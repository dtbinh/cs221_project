Journal Artificial Intelligence Research 32 (2008) 565-606

Submitted 11/07; published 06/08

SATzilla: Portfolio-based Algorithm Selection SAT
Lin Xu
Frank Hutter
Holger H. Hoos
Kevin Leyton-Brown

xulin730@cs.ubc.ca
hutter@cs.ubc.ca
hoos@cs.ubc.ca
kevinlb@cs.ubc.ca

Department Computer Science
University British Columbia
201-2366 Main Mall, BC V6T 1Z4, CANADA

Abstract
widely observed single dominant SAT solver; instead, different
solvers perform best different instances. Rather following traditional approach
choosing best solver given class instances, advocate making decision online per-instance basis. Building previous work, describe SATzilla,
automated approach constructing per-instance algorithm portfolios SAT use socalled empirical hardness models choose among constituent solvers. approach
takes input distribution problem instances set component solvers, constructs portfolio optimizing given objective function (such mean runtime, percent
instances solved, score competition). excellent performance SATzilla
independently verified 2007 SAT Competition, SATzilla07 solvers
three gold, one silver one bronze medal. article, go well beyond SATzilla07
making portfolio construction scalable completely automated, improving
integrating local search solvers candidate solvers, predicting performance score
instead runtime, using hierarchical hardness models take account different types SAT instances. demonstrate effectiveness new techniques
extensive experimental results data sets including instances recent SAT
competition.

1. Introduction
propositional satisfiability problem (SAT) one fundamental problems
computer science. Indeed, entire conferences journals devoted study
problem, long history AI. SAT interesting sake
instances problems N P encoded SAT solved
SAT solvers. approach proven effective tackling many real-world applications,
including planning, scheduling, graph colouring, bounded model checking, formal verification (examples described Kautz & Selman, 1996, 1999; Crawford & Baker,
1994; van Gelder, 2002; Biere, Cimatti, Clarke, Fujita, & Zhu, 1999; Stephan, Brayton, &
Sangiovanni-Vencentelli, 1996).
conceptual simplicity SAT facilitates algorithm development, considerable
research engineering efforts past decades led sophisticated algorithms
highly-optimized implementations. fact, SAT probably N P-complete decision
problem largest amount research effort expended developc
2008
AI Access Foundation. rights reserved.

fiXu, Hutter, Hoos & Leyton-Brown

ment study algorithms. Todays high-performance SAT solvers include tree-search
algorithms (see, e.g., Davis, Logemann, & Loveland, 1962; Zhang, Madigan, Moskewicz, &
Malik, 2001; Zhang, 2002; Kullmann, 2002; Dubois & Dequen, 2001; Heule, Zwieten, Dufour, & Maaren, 2004; Een & Sorensson, 2003), local search algorithms (see, e.g., Selman,
Levesque, & Mitchell, 1992; Selman, Kautz, & Cohen, 1994; Hutter, Tompkins, & Hoos,
2002; Hoos, 2002; Li & Huang, 2005; Ishtaiwi, Thornton, Anbulagan, Sattar, & Pham,
2006; Hoos & Stutzle, 2005), resolution-based approaches (see, e.g., Davis & Putnam,
1960; Dechter & Rish, 1994; Bacchus, 2002b, 2002a; Bacchus & Winter, 2003; Subbarayan
& Pradhan, 2005).
SAT algorithms highly complex, thus largely resisted theoretical average-case analysis. Instead, empirical studies often practical means
assessing comparing performance. one prominent ongoing example,
SAT community holds annual SAT competition (http://www.satcompetition.org; see,
e.g., Le Berre & Simon, 2004). competition intended provide objective assessment SAT algorithms, thus track state art SAT solving, assess
promote new solvers, identify new challenging benchmarks. Solvers judged based
empirical performance three categories instances, divided satisfiable, unsatisfiable mixed instances, speed robustness
taken account. competition serves annual showcase state art
SAT solving; 30 solvers entered 2007.
1.1 Algorithm Selection Problem
One way evaluations like SAT competition useful allow practitioners determine algorithm performs best instances relevant problem
domain. However, choosing single algorithm basis competition performance
always good approachindeed, often case one solver better others solving problem instances given class, dramatically worse
instances. Thus, practitioners hard SAT problems solve face potentially difficult
algorithm selection problem (Rice, 1976): algorithm(s) run order
minimize performance objective, expected runtime?
widely-adopted solution algorithm selection problems measure
every candidate solvers runtime representative set problem instances,
use algorithm offered best (e.g., average median) performance. call
winner-take-all approach. use resulted neglect many algorithms
competitive average nevertheless offer good performance
particular instances.
ideal solution algorithm selection problem, hand, would
consult oracle knows amount time algorithm would take solve
given problem instance, select algorithm best performance. Unfortunately, computationally cheap, perfect oracles nature available SAT
N P-complete problem; cannot precisely determine arbitrary algorithms
runtime arbitrary instance without actually running it. Nevertheless, approach
algorithm selection based idea building approximate runtime predictor,
seen heuristic approximation perfect oracle. Specifically, use machine
566

fiSATzilla: Portfolio-based Algorithm Selection SAT

learning techniques build empirical hardness model, computationally inexpensive
predictor algorithms runtime given problem instance based features
instance algorithms past performance (Nudelman, Leyton-Brown, Hoos, Devkar,
& Shoham, 2004a; Leyton-Brown, Nudelman, & Shoham, 2002). modeling several algorithms and, runtime, choosing algorithm predicted best performance,
empirical hardness models serve basis algorithm portfolio solves
algorithm selection problem automatically (Leyton-Brown, Nudelman, Andrew, McFadden,
& Shoham, 2003b, 2003a).1
work show, believe first time, empirical hardness
models used build algorithm portfolio achieves state-of-the-art performance broad, practical domain. is, evaluated algorithm idiosyncratic conditions narrowly-selected data, rather large, independentlyconducted competition, confronting wide range high-performance algorithms
large set independently-chosen interesting data. Specifically, describe analyze
SATzilla, portfolio-based SAT solver utilizes empirical hardness models perinstance algorithm selection.
1.2 Algorithm Portfolios
term algorithm portfolio introduced Huberman, Lukose, Hogg (1997)
describe strategy running several algorithms parallel, potentially different
algorithms assigned different amounts CPU time. approach also studied
Gomes Selman (2001). Several authors since used term broader way
encompasses strategy leverages multiple black-box algorithms solve
single problem instance. view, space algorithm portfolios spectrum,
approaches use available algorithms one end approaches always
select single algorithm other. advantage using term portfolio
refer broader class algorithms work reasonthey
exploit lack correlation best-case performance several algorithms order
obtain improved performance average case.
clearly describe algorithm portfolios broad sense, introduce
new terminology. define (a, b)-of-n portfolio set n algorithms procedure
selecting among property algorithm terminates early, least
b algorithms executed.2 brevity, also use terms a-of-n
portfolio refer (a, a)-of-n portfolio, n-portfolio n-of-n portfolio.
also useful distinguish solvers run selected. Portfolios parallel
(all algorithms executed concurrently), sequential (the execution one algorithm
begins previous algorithms execution ended), partly sequential (some
1. Similarly, one could predict performance single algorithm different parameter settings
choose best setting per-instance basis. previously demonstrated approach
feasible case number parameters small (Hutter, Hamadi, Hoos, & Leyton-Brown,
2006). Ultimately, conceivable combine two lines research, automatically select
good algorithm along good parameter settings per-instance basis.
2. termination condition somewhat tricky. consider portfolio terminated early
solves problem one solvers chance run, one solvers crashes. Thus,
determining b, consider crash-recovery techniques, using next best
predicted solver (discussed later paper).

567

fiXu, Hutter, Hoos & Leyton-Brown

combination two). Thus classic algorithm portfolios Huberman et al. (1997)
Gomes Selman (2001) described parallel n-portfolios. contrast,
SATzilla solvers present paper sequential 3-of-n portfolios since
sequentially execute two pre-solvers followed one main solver.
range work literature describes algorithm portfolios
broad sense defined here. First, consider work emphasized
algorithm selection (or 1-of-n portfolios). Lobjois Lematre (1998) studied problem
selecting branch-and-bound algorithms based estimate search tree size
due Knuth (1975). Gebruers, Hnich, Bridge, Freuder (2005) employed case-based
reasoning select solution strategy instances constraint programming problem.
Various authors proposed classification-based methods algorithm selection (e.g.,
Guerri & Milano, 2004; Gebruers, Guerri, Hnich, & Milano, 2004; Guo & Hsu, 2004; and,
extent, Horvitz, Ruan, Gomes, Kautz, Selman, & Chickering, 2001). One problem
approaches use error metric penalizes misclassifications
equally, regardless cost. problematic using suboptimal algorithm
acceptable, provided nearly good best algorithm. SATzilla approach
considered classifier error metric depends difference
runtime algorithms.
end spectrum, much work done considers switching
multiple algorithms, terminology building parallel n-portfolios. Gomes
Selman (2001) built portfolio stochastic algorithms quasi-group completion
logistics scheduling problems. Low-knowledge algorithm control Carchrae Beck
(2005) employed portfolio anytime algorithms, prioritizing algorithm according
performance far. Gagliolo Schmidhuber (2006b) learned dynamic algorithm
portfolios also support running several algorithms once, algorithms priority
depends predicted runtime conditioned fact yet found solution.
Streeter, Golovin, Smith (2007) improved average-case performance using black-box
techniques learning interleave execution multiple heuristics based
instance features runtime algorithms.
approaches fall two extremes, making decisions algorithms use flywhile solving problem instanceinstead committing
advance subset algorithms. examples give (1, n)-of-n portfolios.
Lagoudakis Littman (2001) employed reinforcement learning solve algorithm
selection problem decision point DPLL solver SAT order select
branching rule. Similarly, Samulowitz Memisevic (2007) employed classification
switch different heuristics QBF solving search.
Finally, describe ways paper builds past work. LeytonBrown et al. (2002) introduced empirical hardness models. Nudelman et al. (2004a) demonstrated work (uniform-random) SAT introduced features use
here, Hutter et al. (2006) showed apply randomized, incomplete algorithms. Empirical hardness models first used basis algorithm portfolios
Leyton-Brown et al. (2003b, 2003a). idea building algorithm portfolio SAT goes back 2003, submitted first SATzilla solver SAT
competition (Nudelman, Leyton-Brown, Devkar, Shoham, & Hoos, 2004b); version
SATzilla placed 2nd two categories 3rd another. following, describe
568

fiSATzilla: Portfolio-based Algorithm Selection SAT

substantially improved SATzilla solver, entered 2007 SAT Competition
anddespite considerable progress SAT community four year interval
placed 1st three categories, 2nd 3rd two categories. solver
described, along preliminary analysis, conference paper (Xu, Hutter, Hoos,
& Leyton-Brown, 2007c); used hierarchical hardness models, described separately (Xu,
Hoos, & Leyton-Brown, 2007a). work, provide much detailed description
new solver, present several new techniques never previously published
(chiefly introduced Section 5) report new experimental results.
1.3 Overview
Overall, paper divided two parts. first part describes development
SATzilla07, submitted 2007 SAT Competition second part demonstrates recent, improved portfolio algorithms (SATzilla07+ SATzilla07 ).
part subdivided three sections, first describes approach
designing portfolio-based solver high level, second explains lower-level
details portfolio construction, third provides results extensive
experimental evaluation.
Section 2 (Design I) begins general methodology building algorithm portfolios
based empirical hardness models. work, apply general strategies SAT
evaluate experimentally. Section 3 (Construction I) describe architecture
portfolio-based solvers entered 2007 SAT Competition described
previous work (Xu et al., 2007c). addition, constructed new portfolio-based
solver INDUSTRIAL instances analyzed effectively re-running INDUSTRIAL
category 2007 competition portfolio included; results analysis
reported Section 4 (Evaluation I).
move consider ways extending, strengthening, automating
portfolio construction. present five new design ideas Section 5 (Design II),
consider incorporation new solvers training data Section 6 (Construction
II). Finally, evaluated new ideas quantified benefits second set
experiments, describe Section 7 (Evaluation II). Section 8 concludes paper
general observations.

2. Design I: Building Algorithm Portfolios Empirical Hardness
Models
general methodology building algorithm portfolio use work
follows Leyton-Brown et al. (2003b) broad strokes, made significant
extensions here. Portfolio construction happens offline, part algorithm development,
comprises following steps.
1. Identify target distribution problem instances. Practically, means selecting
set instances believed representative underlying distribution, using
instance generator constructs instances represent samples
distribution.
569

fiXu, Hutter, Hoos & Leyton-Brown

2. Select set candidate solvers relatively uncorrelated runtimes
distribution known expected perform well least instances.
3. Identify features characterize problem instances. general cannot done
automatically, rather must reflect knowledge domain expert.
usable effectively automated algorithm selection, features must related
instance hardness relatively cheap compute.
4. training set problem instances, compute features run algorithm
determine running times.
5. Identify one solvers use pre-solving instances. pre-solvers
later run short amount time features computed (step 9 below),
order ensure good performance easy instances allow empirical
hardness models focus exclusively harder instances.
6. Using validation data set, determine solver achieves best performance
instances solved pre-solvers feature
computation times out. refer solver backup solver. absence
sufficient number instances pre-solving feature computation timed
out, employ single best component solver (i.e., winner-take-all choice)
backup solver.
7. Construct empirical hardness model algorithm portfolio, predicts runtime algorithm instance, based instances features.
8. Choose best subset solvers use final portfolio. formalize
automatically solve simple subset selection problem: given solvers,
select subset respective portfolio (which uses empirical hardness
models learned previous step) achieves best performance validation
set. (Observe runtime predictions perfect, dropping solver
portfolio entirely increase portfolios overall performance.)
Then, online, solve given problem instance, following steps performed.
9. Run pre-solver predetermined fixed cutoff time reached.
10. Compute feature values. feature computation cannot completed reason
(error timeout), select backup solver identified step 6 above.
11. Otherwise, predict algorithms runtime using empirical hardness models
step 7 above.
12. Run algorithm predicted best. solver fails complete run (e.g.,
crashes), run algorithm predicted next best.
effectiveness algorithm portfolio built using approach depends
ability learn empirical hardness models accurately predict solvers runtime
given instance using efficiently computable features. experiments presented
570

fiSATzilla: Portfolio-based Algorithm Selection SAT

paper, use ridge regression method previously proven
successful predicting runtime uniform random k-SAT, structured SAT instances,
combinatorial auction winner determination problems (Nudelman et al., 2004a;
Hutter et al., 2006; Leyton-Brown et al., 2002).3
2.1 Ridge Regression Feature Selection
explain construction empirical hardness models described Step 7 above.
predict runtime algorithm instance distribution D, first draw n
instances uniformly random. (In article, distributions given implicitly
benchmark set instances, simply use instances benchmark set.)
instance i, compute set features xi = [xi,1 , . . . , xi,m ] characterize
instance. also run algorithm instance, recording runtime ri .
computed features runtimes n instances, fit function f (x) that,
given features xi instance i, yields prediction, yi , logarithm runtime
yi = log ri . experience, found log transformation runtime
important due large variation runtimes hard combinatorial problems. Unfortunately, performance learning algorithms deteriorate features
uninformative highly correlated features; difficult construct features
suffer problems. Therefore, first reduce set features performing feature selection, case forward selection (see e.g., Guyon, Gunn, Nikravesh,
& Zadeh, 2006), simple iterative method starts empty feature set greedily adds one feature time, aiming reduce cross-validation error much possible
every added feature. Next, add additional pairwise product features xi,j xi,k
j = 1 . . . k = j . . . m; widely used method typically referred quadratic
basis function expansion. Finally, perform another pass forward selection
extended set determine final set basis functions, instance obtain
expanded feature vector = (xi ) = [1 (xi ), . . . , (xi )], final number
basis functions used model.
use ridge regression (see, e.g., Bishop, 2006) fit free parameters w
function fw (x). Ridge regression works follows. Let n matrix containing
vectors instance training set, let vector log runtimes, let
identity matrix. Finally, let small constant penalize large coefficients
w thereby increase numerical stability (we used = 103 experiments). Then,
compute w = (I + > )1 > y, > denotes transpose matrix .
previously unseen instance i, obtain prediction log runtime computing
instance features xi evaluating fw (xi ) = w> (xi ).
3. noted portfolio methodology make use regression approach provides
sufficiently accurate estimates algorithms runtime computationally efficient enough
time spent making prediction compensated performance gain obtained
improved algorithm selection. example, similar settings, previously explored many
learning techniques, lasso regression, SVM regression, Gaussian process regression (LeytonBrown et al., 2002; Hutter et al., 2006). techniques computationally expensive
ridge regression, previous experiments found improve predictive
performance enough justify additional cost.

571

fiXu, Hutter, Hoos & Leyton-Brown

2.2 Accounting Censored Data
common heuristic algorithms solving N P-complete problems, SAT algorithms
tend solve instances quickly, taking extremely long amount time
solve instances. Hence, runtime data costly gather, individual
runs take literally weeks complete, even runs instances
size take milliseconds. common solution problem censor runs
terminating fixed cutoff time.
question fit good models presence censored data extensively studied survival analysis literature statistics, originated actuarial
questions estimating persons lifespan given mortality data well ages
characteristics people still alive. Observe problem ours,
except case, data points always censored value, subtlety
turns matter.
best approach know dealing censored data build models
use available information censored runs using censored runtimes lower
bounds actual runtimes. knowledge, technique first used
context SAT Gagliolo Schmidhuber (2006a). chose simple, yet effective
method Schmee Hahn (1979) deal censored samples. brief, method
first trains hardness model treating cutoff time true (uncensored) runtime
censored samples, repeats following steps convergence.
1. Estimate expected runtime censored runs using hardness model. Since
ridge regression, predictions fact normal distributions (with fixed variance),
expected runtime conditional runtime exceeding cutoff time mean
corresponding normal distribution truncated cutoff time.
2. Train new hardness model using true runtimes uncensored instances
predictions generated previous step censored instances.
earlier work (Xu, Hutter, Hoos, & Leyton-Brown, 2007b), experimentally compared
approach two approaches dealing censored data: dropping data
entirely, treating censored runs though finished cutoff threshold.
demonstrated empirically methods significantly worse
method presented above. Intuitively, methods introduce bias empirical hardness
models, whereas method Schmee Hahn (1979) unbiased.
2.3 Using Hierarchical Hardness Models
previous research empirical hardness models SAT showed achieve
better prediction accuracy even simpler models restrict satisfiable unsatisfiable instances (Nudelman et al., 2004a). course, practice
interested making accurate predictions even know whether instance
satisfiable. recent work (Xu et al., 2007a), introduced hierarchical hardness models
method solving problem. define subjective probability instance
features x satisfiable probability instance chosen random
underlying instance distribution features matching x satisfiable. Hierarchical
572

fiSATzilla: Portfolio-based Algorithm Selection SAT

hardness models first use classifier predict subjective probability satisfiability
use probability, well features x, combine predictions so-called
conditional models, trained satisfiable instances unsatisfiable instances, respectively. previous work conducted extensive experiments
various types SAT instances found hierarchical hardness models achieve
better runtime prediction accuracies traditional empirical hardness models (Xu et al.,
2007a).
Specifically, begin predicting instances satisfiability using classification
algorithm depends instance features used empirical hardness
models. chose Sparse Multinomial Logistic Regression, SMLR (Krishnapuram, Carin,
Figueiredo, & Hartemink, 2005), classification algorithm returns
probability instance belongs class could used instead. Then, train
conditional empirical hardness models (Msat , Munsat ) using quadratic basis-function regression satisfiable unsatisfiable training instances.
Next, must decide combine predictions two models.4 observe
set instance features x classifier prediction s; task predict expected
value algorithms runtime given information. introduce additional
random variable z {sat, unsat}, represents subjective belief oracles
choice conditional model perform best given instance. (Observe
may always correspond model trained data satisfiability
status instance.) express conditional dependence relationships
random variables using graphical model, illustrated Figure 1.

x,

z



features &
probability
satisfiable

model
selection
oracle

runtime

Figure 1: Graphical model mixture-of-experts approach.

write expression probability distribution instances
runtime given features x
P (y | x, s) =

X

P (z = k | x, s) PMk (y | x, s),

(1)

k{sat,unsat}

PMk (y | x, s) probability evaluated according model Mk (see Figure 1).
Since models fitted using ridge regression, rewrite Equation (1)
4. Note classifiers output used directly select modeldoing would mean ignoring
cost making mistake. Instead, use classifiers output feature upon hierarchical
model depend.

573

fiXu, Hutter, Hoos & Leyton-Brown

P (y | x, s) =



X

P (z = k | x, s)

k{sat,unsat}

wk> (x)
k


,

(2)

() denotes probability distribution function Normal distribution mean
zero unit variance, wk weights model Mk , (x) quadratic basis function
expansion x, k fixed standard deviation.
particular, interested mean predicted runtime, is, expectation
P (y | x, s):
X
P (z = k | x, s) wk> (x).
(3)
E(y | x, s) =
k{sat,unsat}

Evaluating expression would easy knew P (z = k | x, s); course,
not. approach learn weighting functions P (z = k | x, s) minimize following
loss function:
2
n
X
L=
yi E(y | x, s) ,
(4)
i=1

yi true log runtime instance n number training instances.
hypothesis space weighting functions chose softmax function
(see, e.g., Bishop, 2006)
>

ev [x;s]
P (z = sat | x, s) =
,
1 + ev> [x;s]

(5)

v vector free parameters set minimize loss function (4).
functional form frequently used probabilistic classification tasks: v > [x; s] large
>
positive, ev [x;s] much larger 1 resulting probability close 1;
large negative, result close zero; zero, resulting probability
exactly 0.5.
seen mixture-of-experts problem (see, e.g., Bishop, 2006) experts fixed Msat Munsat . (In traditional mixture-of-experts methods, experts
allowed vary training process described below.) implementation convenience, used existing mixture experts implementation optimize v, built
around expectation maximization (EM) algorithm performs iterative reweighted
least squares step (Murphy, 2001). modified code slightly fix
experts initialized choice expert classifiers output setting initial
values P (z | x, s) s. Note numerical optimization procedures could used
minimize loss function (4) respect v.
optimized v, obtain runtime prediction unseen test instance
simply compute instances features x classifiers output s, compute
expected runtime evaluating Equation (3).
Finally, note techniques require us restrict conditional models Msat Munsat , even use two models. Section 7,
describe hierarchical hardness models rely six conditional models, trained
satisfiable unsatisfiable instances different data sets.
574

fiSATzilla: Portfolio-based Algorithm Selection SAT

3. Construction I: Building SATzilla07 2007 SAT Competition
section, describe SATzilla07 solvers entered 2007 SAT Competition, whichlike previous events seriesfeatured three main categories instances,
RANDOM, HANDMADE (also known CRAFTED) INDUSTRIAL. submitted three versions SATzilla07 competition. Two versions specifically targeted RANDOM
HANDMADE instance categories trained data target category.
order allow us study SATzilla07s performance even heterogeneous
instance distribution, third version SATzilla07 trained data three
categories competition; call new category ALL. construct version
SATzilla07 INDUSTRIAL category, time constraints limit
three submissions per team. However, built version submission deadline
report results below.
solvers built using design methodology detailed Section 2.
following subsections corresponds one step methodology.
3.1 Selecting Instances
order train empirical hardness models scenarios, needed
instances would similar used real competition. purpose
used instances respective categories previous SAT competitions (2002,
2003, 2004, 2005), well 2006 SAT Race (which featured INDUSTRIAL
instances). Instances repeated previous competitions also repeated
data sets. Overall, 4 811 instances: 2 300 instances category RANDOM, 1 490
category HANDMADE 1 021 category INDUSTRIAL; course, category included
instances. 68% instances solved within 1 200 CPU seconds
reference machine least one seven solvers used (see Section 3.2 below;
computational infrastructure used experiments described Section 3.4).
instances solved solvers dropped data set.
randomly split data set training, validation test sets ratio
40:30:30. parameter tuning intermediate model testing performed validation set; test set used generate final results reported here.
section, use SATzilla07 methodology building multiple portfolios
different sets benchmark instances. order avoid confusion changes
overall methodology discussed later differences training data, treat
data set input parameter SATzilla. data set comprising previously
mentioned RANDOM instances, write Dr ; similarly, write Dh HANDMADE Di
INDUSTRIAL; ALL, simply write D.
3.2 Selecting Solvers
decide algorithms include portfolio, considered wide variety solvers
entered previous SAT competitions 2006 SAT Race.
manually analyzed results competitions, identifying algorithms yielded
best performance subset instances. Since focus satisfiable
unsatisfiable instances, since concerned cost misclassifications,
575

fiXu, Hutter, Hoos & Leyton-Brown

choose incomplete algorithms stage; however, revisit issue
Section 5.4. end, selected seven high-performance solvers shown Table 1
candidates SATzilla07 portfolio. Like data set used training, treat
component solvers input SATzilla, denote set solvers Table 1
S.
Solver

Reference

Eureka
Kcnfs06
March dl04
Minisat 2.0
Rsat 1.03
Vallst
Zchaff Rand

Nadel, Gordon, Palti, Hanna (2006)
Dubois Dequen (2001)
Heule et al. (2004)
Een Sorensson (2006)
Pipatsrisawat Darwiche (2006)
Vallstrom (2005)
Mahajan, Fu, Malik (2005)

Table 1: seven solvers SATzilla07; refer set solvers S.

previous work (Xu et al., 2007b), considered using Hypre preprocessor (Bacchus & Winter, 2003) applying one SATzillas component solvers; effectively
doubled number component solvers. work, re-evaluated option
found performance basically remain unchanged without preprocessing (performance differences terms instances solved, runtime, SAT competition score smaller
1%, even small difference consistently favor using Hypre
preprocessor). reason, dropped preprocessing work reported here.
3.3 Choosing Features
choice instance features significant impact performance empirical
hardness models. Good features need correlate well (solver-specific) instance hardness need cheap compute, since feature computation time counts part
SATzilla07s runtime.
Nudelman et al. (2004a) introduced 84 features SAT instances. features
classified nine categories: problem size, variable-clause graph, variable graph, clause
graph, balance, proximity Horn formulae, LP-based, DPLL probing local search
probing features code last group features based UBCSAT (Tompkins
& Hoos, 2004). order limit time spent computing features, slightly modified
feature computation code Nudelman et al. (2004a). SATzilla07, excluded
number computationally expensive features, LP-based clause graph features.
computation time local search DPLL probing features limited
1 CPU second, total feature computation time per instance limited 60
CPU seconds. eliminating features value across instances
unstable given 1 CPU second local search probing, ended
using 48 raw features summarized Figure 2.
576

fiSATzilla: Portfolio-based Algorithm Selection SAT

Proximity Horn Formula:
28. Fraction Horn clauses
29-33. Number occurrences Horn clause
variable: mean, variation coefficient, min, max
entropy.

Problem Size Features:
1. Number clauses: denoted c
2. Number variables: denoted v
3. Ratio: c/v
Variable-Clause Graph Features:
4-8.
Variable nodes degree statistics: mean,
variation coefficient, min, max entropy.
9-13. Clause nodes degree statistics: mean, variation coefficient, min, max entropy.

DPLL Probing Features:
34-38. Number unit propagations: computed
depths 1, 4, 16, 64 256.
39-40. Search space size estimate: mean depth
contradiction, estimate log number nodes.

Variable Graph Features:
14-17. Nodes degree statistics: mean, variation
coefficient, min max.

Local Search Probing Features:
41-44. Number steps best local minimum
run: mean, median, 10th 90th percentiles
SAPS.
45. Average improvement best run: mean
improvement per step best solution SAPS.
46-47. Fraction improvement due first local
minimum: mean SAPS GSAT.
48. Coefficient variation number unsatisfied clauses local minimum: mean
runs SAPS.

Balance Features:
18-20. Ratio positive negative literals
clause: mean, variation coefficient entropy.
21-25. Ratio positive negative occurrences
variable: mean, variation coefficient, min, max
entropy.
26-27. Fraction binary ternary clauses

Figure 2: features used building SATzilla07; originally introduced described
detail Nudelman et al. (2004a).

3.4 Computing Features Runtimes
experiments performed using computer cluster consisting 55 machines
dual Intel Xeon 3.2GHz CPUs, 2MB cache 2GB RAM, running Suse Linux 10.1.
SAT competition, runs solver exceeded certain runtime aborted
(censored) recorded such. order keep computational cost manageable,
chose cutoff time 1 200 CPU seconds.
3.5 Identifying Pre-solvers
described Section 2, order solve easy instances quickly without spending time
computation features, use one pre-solvers: algorithms run
unconditionally briefly features computed. Good algorithms pre-solving
solve large proportion instances quickly. Based examination training
runtime data, chose March dl04 local search algorithm SAPS (Hutter et al., 2002)
pre-solvers RANDOM, HANDMADE ALL; SAPS, used UBCSAT implementation
(Tompkins & Hoos, 2004) best fixed parameter configuration identified Hutter
et al. (2006). (Note consider incomplete algorithms inclusion
portfolio, use one here.)
Within 5 CPU seconds reference machine, March dl04 solved 47.8%, 47.7%,
43.4% instances RANDOM, HANDMADE data sets, respectively.
remaining instances, let SAPS run 2 CPU seconds, found runtime
almost completely uncorrelated March dl04 (Pearson correlation coefficient r = 0.118
577

fiXu, Hutter, Hoos & Leyton-Brown

Solver
Eureka
Kcnfs06
March dl04
Minisat 2.0
Rsat 1.03
Vallst
Zchaff Rand

RANDOM
Time Solved

HANDMADE
Time Solved

INDUSTRIAL
Time Solved

Time


Solved

770
319
269
520
522
757
802

561
846
311
411
412
440
562

330
1050
715
407
345
582
461

598
658
394
459
445
620
645

57%
50%
73%
69%
70%
54%
51%

40%
81%
85%
62%
62%
40%
36%

59%
33%
80%
73%
72%
67%
58%

84%
13%
42%
76%
81%
59%
71%

Table 2: Average runtime (in CPU seconds reference machine) percentage instances
solved solver instances solved least one seven component solvers within cutoff time 1 200 seconds.

487 remaining instances solved solvers). time, SAPS solved 28.8%,
5.3%, 14.5% remaining RANDOM, HANDMADE instances, respectively.
INDUSTRIAL category, chose run Rsat 1.03 2 CPU seconds pre-solver,
resulted 32.0% instances INDUSTRIAL set solved. Since SAPS
solved less 3% remaining instances within 2 CPU seconds, used
pre-solver category.
3.6 Identifying Backup Solver
performance solvers Table 1 reported Table 2. computed
average runtime (here remainder work) counting timeouts runs
completed cutoff time 1 200 CPU seconds. seen data,
best single solver ALL, RANDOM HANDMADE always March dl04. categories
RANDOM HANDMADE, encounter instances feature computation
timed out. Thus, employed winner-take-all solver March dl04 backup solver
domains. categories INDUSTRIAL ALL, Eureka performed best
instances remained unsolved pre-solving feature computation
timed out; thus chose Eureka backup solver.
3.7 Learning Empirical Hardness Models
learned empirical hardness models predicting solvers runtime described
Section 2, using procedure Schmee Hahn (1979) dealing censored data
also employing hierarchical hardness models.
3.8 Solver Subset Selection
performed automatic exhaustive subset search outlined Section 2 determine
solvers include SATzilla07. Table 3 describes solvers selected
four data sets.
578

fiSATzilla: Portfolio-based Algorithm Selection SAT

Data Set
RANDOM
HANDMADE
INDUSTRIAL


Solvers used SATzilla07
March dl04, Kcnfs06, Rsat 1.03
Kcnfs06, March dl04, Minisat 2.0, Rsat 1.03, Zchaff Rand
Eureka, March dl04, Minisat 2.0, Rsat 1.03
Eureka, Kcnfs06, March dl04, Minisat 2.0, Zchaff Rand

Table 3: Results subset selection SATzilla07.

4. Evaluation I: Performance Analysis SATzilla07
section, evaluate SATzilla07 four data sets. Since use SAT Competition running example throughout paper, start describing SATzilla07
fared 2007 SAT Competition. describe comprehensive evaluations
SATzilla07 version compared greater detail component
solvers.
4.1 SATzilla07 2007 SAT Competition
submitted three versions SATzilla07 2007 SAT Competition, namely
SATzilla07(S,Dr ) (i.e., SATzilla07 using seven component solvers Table 1 trained RANDOM instances) SATzilla07(S,Dh ) (trained HANDMADE),
SATzilla07(S,D) (trained ALL). Table 4 shows results 2007 SAT Competition RANDOM HANDMADE categories. RANDOM category, SATzilla07(S,Dr )
gold medal subcategory SAT+UNSAT, came third UNSAT subcategory. SAT subcategory dominated local search solvers. HANDMADE
category, SATzilla07(S,Dh ) showed excellent performance, winning SAT+UNSAT
UNSAT subcategories, placing second SAT subcategory.
Category

Rank

SAT & UNSAT

SAT

UNSAT

RANDOM

1st
2nd
3rd

SATzilla07(S,Dr )
March ks
Kcnfs04

Gnovelty+
Ag2wsat0
Ag2wsat+

March ks
Kcnfs04
SATzilla07(S,Dr )

HANDMADE

1st
2nd
3rd

SATzilla07(S,Dh )
Minisat07
MXC

March ks
SATzilla07(S,Dh )
Minisat07

SATzilla07(S,Dh )
TTS
Minisat07

Table 4: Results 2007 SAT Competition. 30 solvers competed
category.

Since general portfolio SATzilla07(S,D) included Eureka (whose source code
publicly available) run demonstration division only. official competition
results (available http://www.cril.univ-artois.fr/SAT07/) show solver,
trained instances three categories, performed well, solving
579

fiXu, Hutter, Hoos & Leyton-Brown

Solver

Average runtime [s]

Solved percentage

Performance score

Picosat
TinisatElite
Minisat07
Rsat 2.0

398
494
484
446

82
71
72
75

31484
21630
34088
23446

SATzilla07(S,Di )

346

87

29552

Table 5: Performance comparison SATzilla07(S,Di ) winners 2007 SAT Competition
INDUSTRIAL category. performance scores computed using 2007 SAT
Competition scoring function cutoff time 1 200 CPU seconds. SATzilla07(S,Di )
exactly solver shown Figure 6 trained without reference
2007 SAT Competition data.

instances union three categories solver (including two
versions SATzilla07).
submit version SATzilla07 2007 SAT Competition
specifically trained instances INDUSTRIAL category. However, constructed
version, SATzilla07(S,Di ), submission deadline report
performance. Although SATzilla07(S,Di ) compete actual 2007 SAT Competition, approximate well would performed simulation
competition, using scoring function competition (described detail
Section 5.3), based large number competitors, namely 19 solvers listed
Tables 1, 9 10, plus SATzilla07(S,Di ).
Table 5 compares performance SATzilla07 simulation competition
solvers least one medal INDUSTRIAL category 2007
SAT Competition: Picosat, TinisatElite, Minisat07 Rsat 2.0.
differences test environment used real SAT competition:
simulation ran different machines different operating system; also used
shorter cutoff time fewer competitors evaluate solvers performance scores. differences, ranking solvers simulation necessarily
would actual competition. Nevertheless, results leave doubt
SATzilla07 compete state-of-the-art SAT solvers INDUSTRIAL
category.
4.2 Feature Computation
actual time required compute features varied instance instance.
following, report runtimes computing features instance sets defined Section
3.1. Typically, feature computation took least 3 CPU seconds: 1 second local
search probing SAPS GSAT, 1 second DPLL probing. However,
small instances, limit 300 000 local search steps reached one CPU
second passed, resulting feature computation times lower 3 CPU seconds.
instances RANDOM HANDMADE categories, computation
features took insignificant amount time, resulting feature computation times
580

fi100

100

80

80

% Instances Finished

% Instances Finished

SATzilla: Portfolio-based Algorithm Selection SAT

60
40
20
0
0

1

2
3
Feature Time [CPU sec]

4

5

60
40
20
0
0

10

20
30
40
Feature Time [CPU sec]

50

60

Figure 3: Variability feature computation times. y-axis denotes percentage instances
feature computation finished time given x-axis. Left:
RANDOM, right: INDUSTRIAL. Note different scales x-axes.

3 CPU seconds. However, many instances INDUSTRIAL category,
feature computation quite expensive, times 1 200 CPU seconds
instances. limited feature computation time 60 CPU seconds, resulted
time-outs 19% instances INDUSTRIAL category (but instances
categories). instances feature computation timed out,
backup solver used.
Figure 3 illustrates variation feature computation time. category RANDOM
(left pane), feature computation never took significantly longer three CPU seconds.
contrast, category INDUSTRIAL, fairly high variation, feature
computation reaching cut-off time 60 CPU seconds 19% instances.
average total feature computation times categories RANDOM, HANDMADE, INDUSTRIAL
3.01, 4.22, 14.4 CPU seconds, respectively.
4.3 RANDOM Category
category, evaluated SATzilla portfolios comparing
best solvers Table 1 respective category. Note solvers table
exactly candidate solvers used SATzilla.
Figure 4 shows performance SATzilla07 top three single solvers
Table 1 category RANDOM. Note count runtime pre-solving, well
feature computation time, part SATzillas runtime. Oracle(S) provides upper
bound performance could achieved SATzilla: runtime
hypothetical version SATzilla makes every decision optimal fashion,
without time spent computing features. Furthermore, also choose run
pre-solvers instance. Essentially, Oracle(S) thus indicates performance
would achieved running best algorithm single instance. Figure 4
(right), horizontal line near bottom plot shows time SATzilla07(S,Dr )
allots pre-solving (on average) feature computation.
581

fiXu, Hutter, Hoos & Leyton-Brown

100

500

% Instances Solved

Average Runtime [CPU sec]

600

400
300
200

90

Oracle(S)
SATzilla07(S,Dr)

80

Kcnfs06
March_dl04
Rsat1.03

70
60
50
40
30
20

100

10
Presolving

0

K

06

fs
cn

dl0
h_

4

rc


03
t1.
sa

R

07
illa
Tz



ac


le

AvgFeature

0 1
10

0

10

1

10

2

10

3

10

Runtime [CPU sec]

Figure 4: Left: Average runtime, right: runtime cumulative distribution function (CDF) dif-

500

100

450

90

Oracle(S)
SATzilla07(S,Dh)

400

80

March_dl04
Minisat2.0
Vallst

% Instances Solved

Average Runtime [CPU sec]

ferent solvers RANDOM; average feature computation time 2.3 CPU seconds
(too insignificant visible SATzilla07s runtime bar). solvers CDFs
ones shown (i.e., given runtime maximum CDFs
selected solvers upper bound CDF solvers considered
experiments).

350
300
250
200
150
100

70
60
50
40
30
20

50

10

0

h_

4
dl0

rc


nis
Mi

.0
at2

V

st


SA




zill

07

0 1
10

cle



ra

Presolving

AvgFeature
0

10

1

10

2

10

3

10

Runtime [CPU sec]

Figure 5: Left: Average runtime, right: runtime CDF different solvers HANDMADE; average feature computation time 4.5 CPU seconds (shown white box top
SATzilla07s runtime bar). solvers CDFs ones shown here.

Overall, SATzilla07(S,Dr ) achieved good performance data set RANDOM:
three times faster average best component solver, March dl04 (see
Figure 4, left), also dominated terms fraction instances solved, solving 20%
instances within cutoff time (see Figure 4, left). runtime CDF plot also shows
local-search-based pre-solver SAPS helped considerably solving 20%
instances within 2 CPU seconds (this reflected sharp increase solved instances
feature computation begins).
582

fi450

100

400

90

350

80

% Instances Solved

Average Runtime [CPU sec]

SATzilla: Portfolio-based Algorithm Selection SAT

300
250
200
150
100

70

Oracle(S)
SATzilla07(S,D)


Eureka
Minisat2.0
Rsat1.03

60
50
40
30
20

50

10
Presolving

0

ka
ure

E

nis

.0
at2

Mi

.03

Rs

at1

SA



7
a0
zill

cle

0 1
10




AvgFeature
0

10

1

10

2

10

Figure 6: Left: Average runtime, right: runtime CDF different solvers INDUSTRIAL;
average feature computation time 14.1 CPU seconds (shown white box top
SATzilla07s runtime bar). solvers CDFs ones shown here.

4.4 HANDMADE Category
SATzilla07s performance results HANDMADE category also good. Using
five component solvers listed Table 3, average runtime 45% less
best single component solver (see Figure 5, left). CDF plot Figure 5 (right)
shows SATzilla07 dominated components solved 13% instances
best non-portfolio solver.
4.5 INDUSTRIAL Category
performed experiment INDUSTRIAL instances categories
order study SATzilla07s performance compared component solvers. SATzilla07
23% faster average best component solver, Eureka (see Figure 6
(left)). Moreover, Figure 6 (right) shows SATzilla07 also solved 9% instances
Eureka within cutoff time 1 200 CPU seconds. Note category,
feature computation timed 15.5% test instances 60 CPU seconds; Eureka
used backup solver cases.
4.6
final category, ALL, heterogeneous category included instances
categories, portfolio approach especially appealing. SATzilla07 performed
well category, average runtime less half best single solver,
March dl04 (159 vs. 389 CPU seconds, respectively). also solved 20% instances
non-portfolio solver within given time limit 1 200 CPU seconds (see Figure 7).
583

3

10

Runtime [CPU sec]

fi500

100

450

90

400

80

% Instances Solved

Average Runtime [CPU sec]

Xu, Hutter, Hoos & Leyton-Brown

350
300
250
200
150
100

70
60
50
40
30
20

50

10
Presolving

0

4
dl0

_
rch



Oracle(S)
SATzilla07(S,D)
March_dl04
Minisat2.0
Rsat1.03

nis

.0
at2

Mi

Rs

.03
at1

SA

07
illa
Tz

le
rac

0 1
10



AvgFeature
0

10

1

10

2

10

3

10

Runtime [CPU sec]

Figure 7: Left: Average runtime; right: runtime CDF different solvers ALL; average feature computation time 6.7 CPU seconds (shown white box top
SATzilla07s runtime bar). solvers CDFs ones shown here.

4.7 Classifier Accuracy
satisfiability status classifiers trained various data sets surprisingly effective
predicting satisfiability RANDOM INDUSTRIAL instances, reached classification accuracies 94% 92%, respectively. HANDMADE ALL, classification
accuracy still considerably better random guessing, 70% 78%, respectively.
Interestingly, classifiers often misclassified unsatisfiable instances SAT
satisfiable instances UNSAT. effect seen confusion matrices
Table 6; pronounced HANDMADE category, overall classification
quality also lowest: 40% HANDMADE instances classified SAT
fact unsatisfiable, 18% instances classified UNSAT fact
satisfiable.

5. Design II: SATzilla Beyond 2007
Despite SATzilla07s success 2007 SAT Competition, still room improvement. section describes number design enhancements SATzilla07
underly new SATzilla versions, SATzilla07+ SATzilla07 , describe
detail Section 6 evaluate experimentally Section 7.
5.1 Automatically Selecting Pre-solvers
SATzilla07, identified pre-solvers cutoff times manually. several
limitations approach. First foremost, manual pre-solver selection
scale well. many candidate solvers, manually finding best combination
pre-solvers cutoff times difficult requires significant amounts valuable
human time. addition, manual pre-solver selection performed SATzilla07
concentrated solely solving large number instances quickly take
584

fiSATzilla: Portfolio-based Algorithm Selection SAT

satisfiable

unsatisfiable

classified SAT

91%

9%

classified UNSAT

5%

95%

satisfiable

unsatisfiable

classified SAT

60%

40%

classified UNSAT

18%

82%

RANDOM data set

HANDMADE data set

satisfiable

unsatisfiable

classified SAT

81%

19%

classified UNSAT

5%

95%

INDUSTRIAL data set

satisfiable

unsatisfiable

classified SAT

65%

35%

classified UNSAT

12%

88%
data set

Table 6: Confusion matrices satisfiability status classifier data sets RANDOM, HANDMADE,
INDUSTRIAL ALL.

account pre-solvers effect model learning. fact, three consequences
pre-solving.
1. Pre-solving solves instances quickly features computed. context
SAT competition, improves SATzillas scores easy problem instances
due speed purse component SAT competition scoring function. (See
Section 5.3 below.)
2. Pre-solving increases SATzillas runtime instances solved pre-solving
adding pre-solvers time every instance. Like feature computation itself,
additional cost reduces SATzillas scores.
3. Pre-solving filters easy instances, allowing empirical hardness models
trained exclusively harder instances.
considered (1) (2) manual selection pre-solvers, consider
(3), namely fact use different pre-solvers and/or cutoff times results different training data hence different learned models, also affect portfolios
effectiveness.
new automatic pre-solver selection technique works follows. committed
advance using maximum two pre-solvers: one three complete search algorithms
one three local search algorithms. three candidates search
approaches automatically determined data set highest score
validation set run maximum 10 CPU seconds. also use number
possible cutoff times, namely 2, 5 10 CPU seconds, well 0 seconds (i.e.,
pre-solver run all) consider orders two pre-solvers
run. resulting 288 possible combinations two pre-solvers cutoff times,
SATzillas performance validation data evaluated performing steps 6, 7 8
general methodology presented Section 2:
585

fiXu, Hutter, Hoos & Leyton-Brown

6. determine backup solver selection features time out;
7. construct empirical hardness model algorithm;
8. automatically select best subset algorithms use part SATzilla.
best-performing subset found last stepevaluated validation datais selected algorithm portfolio given combination pre-solver / cutoff time
pairs. Overall, method aims choose pre-solver configuration yields
best-performing portfolio.
5.2 Randomized Solver Subset Selection Large Set Component Solvers
methodology Section 2 relied exhaustive subset search choosing best
combination component solvers. large number component solvers, impossible (N component solvers would require consideration 2N solver sets,
model would trained). automatic pre-solver selection methods described
Section 5.1 worsens situation: solver selection must performed
every candidate configuration pre-solvers, new pre-solver configurations induce
new models.
alternative exhaustively considering subsets, implemented randomized
iterative improvement procedure search good subset solvers. local search
neighbourhood used procedure consists subsets solvers reached
adding dropping single component solver. Starting randomly selected subset
solvers, search step, consider neighbouring solver subset selected uniformly
random accept validation set performance increases; otherwise, accept
solver subset anyway probability 5%. 100 steps performed
improving step, new run started re-initializing search random. 10
runs, search terminated best subset solvers encountered
search process returned. Preliminary evidence suggests local search procedure
efficiently finds good subsets solvers.
5.3 Predicting Performance Score Instead Runtime
general portfolio methodology based empirical hardness models, predict
algorithms runtime. However, one might simply interested using portfolio pick
solver lowest expected runtime. example, SAT competition, solvers
evaluated based complex scoring function depends partly solvers
runtime. Although idiosyncracies scoring function somewhat particular
SAT competition, idea portfolio built optimize performance score
complex runtime wide applicability. section describe techniques
building models predict performance score directly.
One key issue thatas long depend standard supervised learning methods
require independent identically distributed training datawe deal easily
scoring functions actually associate score single instance combine
partial scores instances compute overall score. Given training data labeled
scoring function, SATzilla simply learn model score (rather
586

fiSATzilla: Portfolio-based Algorithm Selection SAT

runtime) choose solver highest predicted score. Unfortunately, scoring
function used 2007 SAT Competition satisfy independence property:
score solver attains solving given instance depends part (and, indeed,
solvers) performance other, similar instances. specifically, SAT competition
instance P solution purse SolutionP speed purse SpeedP ; instances
given series (typically 540 similar instances) share one series purse SeriesP . Algorithms
ranked summing three partial scores derived purses.
1. problem instance P , solution purse equally distributed
solvers Si solve instance within cutoff time (thereby rewarding robustness
solver).
2. speed purse P divided among set solvers solved instance
SF(P,Si )
timeLimit(P )
Score(P, Si ) = SpeedP
, speed factor SF(P, S) = 1+timeUsed(P,S)

j SF(P,Sj )
measure speed discounts small absolute differences runtime.
3. series purse series divided equally distributed solvers
Si solved least one instance series.5
Si partial score problem P solution speed purses solely depends solvers
runtime P runtime competing solvers P . Thus, given runtimes
competing solvers part training data, compute score contributions
solution speed purses instance P , two components
independent across instances. contrast, since solvers share series purse
depend performance instances series, partial score received
series purse solving one instance independent performance
instances.
solution problem approximate instances share series purse
score independent score. N instances series solved SATzillas
component solvers, n solvers solve least one instances series, assign
partial score SeriesP/(N n) solver Si (where = 1, . . . , n) instance
series solved. approximation non-independent score independent
always perfect, conservative defines lower-bound partial
score series purse. Predicted scores used SATzilla choose
different solvers per-instance basis. Thus, partial score solver
instance reflect much would contribute SATzillas score. SATzilla
perfect (i.e., instance, always selected best algorithm) score approximation
would correct: SATzilla would solve N instances series component
solver solve, thus would actually achieve series score SeriesP/(N n) N =
SeriesP/n. SATzilla performed poorly solve instance series,
approximation would also exact, since would estimate partial series score
zero. Finally, SATzilla pick successful solvers (say, )
instances series solved component solvers (i.e., < N ), would
underestimate partial series purse, since SeriesP/(N n) < SeriesP/n.
5. See http://www.satcompetition.org/2007/rules07.html details.

587

fiXu, Hutter, Hoos & Leyton-Brown

learning techniques require approximation performance score
independent score, experimental evaluations solvers scores employ actual
SAT competition scoring function. explained previously, SAT competition,
performance score solver depends score solvers competition.
order simulate competition, select large number solvers pretend
reference solvers SATzilla solvers competition; throughout
analysis use 19 solvers listed Tables 1, 9 10. perfect simulation,
since scores change somewhat different solvers added removed
competition. However, obtain much better approximations performance score
following methodology outlined using cruder measures, learning
models predict mean runtime numbers benchmark instances solved.
Finally, predicting performance score instead runtime number implications
components SATzilla. First, notice compute exact score
algorithm instance, even algorithm times unsuccessfully crashesin
cases, score three components simply zero. predicting scores instead
runtimes, thus need rely censored sampling techniques (see Section 2.2)
anymore. Secondly, notice oracles maximizing SAT competition score
minimizing runtime identical, since always using solver smallest runtime
guarantees highest values three components obtained.
5.4 Introducing Local Search Solvers SATzilla
SAT solvers based local search well known effective certain classes
satisfiable instances. fact, classes hard random satisfiable instances
local search solvers solve reasonable amount time (Hoos & Stutzle, 2005).
However, high-performance local-search-based SAT solvers incomplete cannot
solve unsatisfiable instances. previous versions SATzilla avoided using local search
algorithms risk would select unsatisfiable instances,
would run uselessly reaching cutoff time.
shift predicting optimizing performance score instead runtime,
issue turns matter anymore. Treating every solver black box, local search
solvers always get score exactly zero unsatisfiable instances since guaranteed
solve within cutoff time. (Of course, need run
instance training instance known unsatisfiable.) Hence, build
models predicting score local search solvers using exactly methods
complete solvers.
5.5 General Hierarchical Hardness Models
benchmark set consists instances categories RANDOM, HANDMADE
INDUSTRIAL. order improve performance heterogeneous instance
distribution, extend previous hierarchical hardness model approach (predicting satisfiability status using mixture two conditional models) general
scenario six underlying empirical hardness models (one combination category
satisfiability status). output general hierarchical model linear weighted
combination output component. described Section 2.3, approx588

fiSATzilla: Portfolio-based Algorithm Selection SAT

Old instances 2007

New instances 2007

(1 925 instances)
Vo (1 443 instances)
Eo (1 443 instances)

Tn (347 instances)
Vn (261 instances)
En (261 instances)

Training (40%)
Validation (30%)
Test (30%)

Table 7: Instances 2007 2007 randomly split training (T), validation (V)
test (E) data sets. sets include instances categories: RANDOM, HANDMADE
INDUSTRIAL.

Data set

Training

Validation

Test


D0
D+



Tn

Vo
Vo
Vo Vn

Eo
Eo En
Eo En

Table 8: Data sets used experiments. used first series experiments
Section 4, D0 D+ used second series experiments. Note data sets
D0 use identical training validation data, different test data.

imate model selection oracle softmax function whose parameters estimated
using EM.

6. Construction II: Building Improved SATzilla Versions
section describe construction new SATzilla versions incorporate new
design elements previous section. also describe two versions based old
design, use evaluate impact changes.
6.1 Benchmark Instances
addition instances used Section 3.1, added 869 instances 2007
SAT Competition four data sets. Overall, resulted 5 680 instances: 2 811
instances category RANDOM, 1 676 category HANDMADE 1 193 category INDUSTRIAL.
Recall Section 3.1 dropped instances could solved seven
solvers Table 1. follow methodology here, extend solver set
12 solvers Tables 9 10. Now, 71.8% instances solved least one
19 solvers within cutoff time 1 200 CPU seconds reference machine;
remaining instances excluded analysis.
randomly split benchmark sets training, validation test sets,
described Table 7. parameter tuning intermediate testing performed
validation sets, test sets used generate final results reported here.
interested analyzing SATzillas performance vary data
used train it. make easy refer different data sets, describe
assign names (D, D0 , D+ ) them. Table 7 shows division data
589

fiXu, Hutter, Hoos & Leyton-Brown

Solver
Kcnfs04
TTS
Picosat
MXC
March ks
TinisatElite
Minisat07
Rsat 2.0

Reference
Dequen Dubois (2007)
Spence (2007)
Biere (2007)
Bregman Mitchell (2007)
Heule v. Maaren (2007)
Huang (2007)
Sorensson Een (2007)
Pipatsrisawat Darwiche (2007)

Table 9: Eight complete solvers 2007 SAT Competition.
Solver
Ranov
Ag2wsat0
Ag2wsat+
Gnovelty+

Reference
Pham Anbulagan (2007)
C. M. Li Zhang (2007)
Wei, Li, Zhang (2007)
Pham Gretton (2007)

Table 10: Four local search solvers 2007 SAT Competition.
old (pre-2007) new (2007) instances. Table 8 shows combined data
construct three data sets use evaluation. Data set one introduced
used Section 3.1: uses pre-2007 instances training, validation testing.
Data set D0 uses training validation data sets, differs test sets,
include old new instances. Data set D+ combines old new instances
training, validation test sets.
Thus, note data sets D0 D+ use test sets, meaning performance portfolios trained using different sets compared directly. However,
expect portfolio trained using D+ least slightly better, access
data. before, want refer RANDOM instances D+ ,
write Dr+ ; likewise, write Dh+ HANDMADE, Di+ INDUSTRIAL, etc.
6.2 Extending Set Component Solvers
addition seven old solvers used SATzilla07 (previously described Table 1),
considered eight new complete solvers four local search solvers 2007 SAT
Competition inclusion portfolio; solvers described Tables 9 10.
training instances, treat sets candidate solvers input parameter
SATzilla. sets candidate solvers used experiments detailed Table 11.
6.3 Different SATzilla Versions
introduced new design ideas SATzilla (Section 5), new training data (Section 6.1) new solvers (Section 6.2), interested evaluating much portfolio improved result. order gain insights much performance improvement
590

fiSATzilla: Portfolio-based Algorithm Selection SAT

Name Set

Solvers Set


S+
S++

7 solvers Table 1
15 solvers Tables 1 9
19 solvers Tables 1, 9 10

Table 11: Solver sets used second series experiments.
SATzilla version
SATzilla07(S,D0 )
SATzilla07(S+ ,D+ )
SATzilla07+ (S++ ,D+ )
SATzilla07 (S++ ,D+ )

Description
version entered 2007 SAT Competition (Section 3),
evaluated extended test set.
version built using design described Section 3,
includes new complete solvers (Table 9) new data (Section 6.1).
addition new complete solvers data, version uses local search
solvers (Table 10) new design elements Section 5 except
general hierarchical hardness models (Section 5.5).
version uses solvers, data new design elements. Unlike
versions, trained one variant solver use
data set categories.

Table 12: different SATzilla versions evaluated second set experiments.

achieved different changes, studied several intermediate SATzilla solvers,
summarized Table 12.
Observe solvers built using identical test data thus directly
comparable. generally expected solver outperform predecessors list.
exception SATzilla07 (S++ ,D+ ): instead aiming increased performance,
last solver designed achieve good performance across broader range instances.
Thus, expected SATzilla07 (S++ ,D+ ) outperform others category ALL,
outperform SATzilla07+ (S++ ,D+ ) specific categories.
6.4 Constructing SATzilla07+ (S++ ,D+ ) SATzilla07 (S++ ,D+ )
construction SATzilla07(S,D) already described Section 3;
SATzilla07(S,D) differed test set used evaluate it, otherwise identical. construction SATzilla07(S+ ,D+ )
SATzilla07(S,D), except relied different solvers corresponding training
data. SATzilla07+ (S++ ,D+ ) SATzilla07 (S++ ,D+ ) incorporated new techniques introduced Section 5. section briefly describe solvers
constructed.
used set features SATzilla07 (see Section 3.3). also used
execution environment cutoff times. Pre-solvers identified automatically
described Section 5.1, using (automatically determined) candidate solvers listed
Table 13. final sets pre-solvers selected version SATzilla listed
Section 7 (Tables 14, 17, 20 23). Based solvers scores validation data sets,
591

fiXu, Hutter, Hoos & Leyton-Brown

RANDOM

HANDMADE

INDUSTRIAL



Complete
Pre-solver
Candidates

Kcnfs06
March dl04
March ks

March dl04
Vallst
March ks

Rsat 1.03
Picosat
Rsat 2.0

Minisat07
March ks
March dl04

Local Search
Pre-solver
Candidates

Ag2wsat0
Gnovelty+
SAPS

Ag2wsat0
Ag2wsat+
Gnovelty+

Ag2wsat0
Ag2wsat+
Gnovelty+

SAPS
Ag2wsat0
Gnovelty+

Table 13: Pre-solver candidates four data sets. candidates automatically
chosen based scores validation data achieved running respective
algorithms maximum 10 CPU seconds.

automatically determined backup solvers RANDOM, HANDMADE, INDUSTRIAL
March ks, March dl04, Eureka Eureka, respectively.
built models predict performance score algorithm. score well
defined even case timeouts crashes; thus, need deal censored
data. Like SATzilla07, SATzilla07+ used hierarchical empirical hardness models (Xu
et al., 2007a) two underlying models (Msat Munsat ) predicting solvers score.
SATzilla07 , built general hierarchical hardness models predicting scores
described Section 5.5; models based six underlying empirical hardness
models (Msat Munsat trained data SAT competition category).
chose solver subsets based results local search procedure subset search outlined Section 5.2. resulting final components SATzilla07,
SATzilla07+ SATzilla07 category described detail following
section.

7. Evaluation II: Performance Analysis Improved SATzilla Versions
section, investigate effectiveness new techniques evaluating
four SATzilla versions listed Table 12: SATzilla07(S,D0 ), SATzilla07(S+ ,D+ ),
SATzilla07+ (S++ ,D+ ) SATzilla07 (S++ ,D+ ). evaluate performance,
constructed simulated SAT competition using scoring function 2007
SAT Competition, differing number important aspects. participants
competition 19 solvers listed Tables 1, 9, 10 (all solvers considered
categories), test instances Eo En described Tables 7 8. Furthermore, computational infrastructure (see Section 3.4) differed 2007 competition,
also used shorter cutoff times 1200 seconds. reasons solvers ranked
slightly differently simulated competition 2007 competition.
7.1 RANDOM Category
Table 14 shows configuration three different SATzilla versions designed
RANDOM category. Note automatic solver selection SATzilla07+ (S++ ,D+
r )
included different solvers ones used SATzilla07(S+ ,D+
);

particular,

chose
r
592

fiSATzilla: Portfolio-based Algorithm Selection SAT

SATzilla version

Pre-Solvers (time)

Component solvers

SATzilla07(S,D0r )

March dl04(5); SAPS(2)

Kcnfs06, March dl04, Rsat 1.03

SATzilla07(S+ ,D+
r )

March dl04(5); SAPS(2)

SATzilla07+ (S++ ,D+
r )

SAPS(2); Kcnfs06(2)

Kcnfs06, March dl04, March ks,
Minisat07
Kcnfs06, March ks, Minisat07, Ranov,
Ag2wsat+, Gnovelty+

Table 14: SATzillas configuration RANDOM category; cutoff times pre-solvers specified CPU seconds.

three local search solvers, Ranov, Ag2wsat+, Gnovelty+, available
SATzilla07. Also, automatic pre-solver selection chose different order cutoff
time pre-solvers manual selection: chose first run SAPS two CPU
seconds, followed two CPU seconds Kcnfs06. Even though running local search
algorithm SAPS help solving unsatisfiable instances, see Figure 8 (left)
SAPS solved many instances March dl04 first seconds.
Table 15 shows performance different versions SATzilla compared best
solvers RANDOM category. versions SATzilla outperformed every non-portfolio
solver terms average runtime number instances solved. SATzilla07+
SATzilla07 , variants optimizing score rather another objective function, also
clearly achieved higher scores non-portfolio solvers. always case
versions; example, SATzilla07(S+ ,D+
r ) achieved 86.6% score
best solver, Gnovelty+ (where scores computed based reference set 20
reference solvers: 19 solvers Tables 1, 9, 10, well SATzilla07(S+ ,D+
r )).
Table 15 Figure 8 show adding complete solvers training data improve
SATzilla07 much. time, substantial improvements achieved
new mechanisms SATzilla07+ , leading 11% instances solved, reduction
average runtime half, increase score 50%. Interestingly,
performance general SATzilla07 (S++ ,D+ ) trained instance mix
tested RANDOM category quite close best version SATzilla specifically
designed RANDOM instances, SATzilla07+ (S++ ,D+
r ). Note due excellent
performance satisfiable instances, local search solvers Table 15 (Gnovelty+
Ag2wsat variants) tended higher overall scores complete solvers (Kcnfs04
March ks) even though solved fewer instances particular could solve
unsatisfiable instance. 2007 SAT Competition, however, winners random
SAT+UNSAT category complete solvers, lead us speculate local search
solvers considered category (in random SAT category, winners
indeed local search solvers).
Figure 8 presents CDFs summarizing performance best non-portfolio solvers,
SATzilla solvers two oracles. non-portfolio solvers omitted CDFs
shown. Section 4, oracles represent ideal versions SATzilla choose among
component solvers perfectly without computational cost. specifically, given
instance, oracle picks fastest algorithm; allowed consider SAPS (with
593

fiXu, Hutter, Hoos & Leyton-Brown

Solver

Avg. runtime [s]

Solved [%]

Performance score

Kcnfs04
March ks
Ag2wsat0
Ag2wsat+
Gnovelty+

852
351
479
510
410

32.1
78.4
62.0
59.1
67.4

38309
113666
119919
110218
131703

SATzilla07(S,D0r )
SATzilla07(S+ ,D+
r )
SATzilla07+ (S++ ,D+
r )
SATzilla07 (S++ ,D+ )

231
218
84
113

85.4
86.5
97.8
95.8

(86.6%)
(88.7%)
189436 (143.8%)
(137.8%)

Table 15: performance SATzilla compared best solvers RANDOM. cutoff time
1 200 CPU seconds; SATzilla07 (S++ ,D+ ) trained ALL. Scores computed based 20 reference solvers: 19 solvers Tables 1, 9, 10, well one
version SATzilla. compute score non-SATzilla solver, SATzilla
version used member set reference solvers SATzilla07+ (S++ ,D+
r ).
)


Since include SATzilla versions SATzilla07+ (S++ ,D+
r
set reference solvers, scores solvers incomparable scores given
here, therefore report them. Instead, SATzilla solver, indicate
parentheses performance score percentage highest score achieved
non-portfolio solver, given reference set appropriate SATzilla solver took
place SATzilla07+ (S++ ,D+
r ).

100

% Instances Solved

80
70

90

March_dl04
March_ks
Gnovelty+

80

% Instances Solved

90

100

Oracle(S++)
SATzilla07+(S++,D+r)

60
50
40
30
20

70
60

Oracle(S++)
Oracle(S)
+ ++ +
SATzilla07 (S ,Dr )
SATzilla07(S+,D+r)
SATzilla07(S,Dr)
SATzilla07*(S++,D+)

50
40
30
20

Presolving(07(S+,D+r),07(S,D ))
r

10
0 1
10

Presolving(07+(S++,D+r))
0

10

+

AvgFeature(07 (S

+
,Dr ))

1

10

AvgFeature(07(S+,D+r),07(S,D ))
r

10

++

Presolving(others)
2

10

3

10

Runtime [CPU sec]

0 1
10

AvgFeature(others)
0

10

1

10

2

10

Figure 8: Left: CDFs SATzilla07+ (S++ ,D+
r ) best non-portfolio solvers RANDOM;
right: CDFs different versions SATzilla RANDOM shown Table 14,
SATzilla07 (S++ ,D+ ) trained ALL. solvers CDFs ones
shown here.

maximum runtime 10 CPU seconds) solver given set (S one oracle
S++ other).
Table 16 indicates often component solver SATzilla07+ (S++ ,D+
r )
selected, often successful, amount average runtime. found
594

3

10

Runtime [CPU sec]

fiSATzilla: Portfolio-based Algorithm Selection SAT

Pre-Solver (Pre-Time)

Solved [%]

Avg. Runtime [CPU sec]

SAPS(2)
March dl04(2)

52.2
9.6

1.1
1.68

Selected Solver

Selected [%]

Success [%]

Avg. Runtime [CPU sec]

March dl04
Gnovelty+
March ks
Minisat07
Ranov
Ag2wsat+

34.8
28.8
23.9
4.4
4.0
4.0

96.2
93.9
92.6
100
100
77.8

294.8
143.6
213.3
61.0
6.9
357.9

Table 16: solvers selected SATzilla07+ (S++ ,D+
r ) RANDOM category. Note
column Selected [%] shows percentage instances remaining pre-solving
algorithm selected, sums 100%. Cutoff times pre-solvers
specified CPU seconds.

solvers picked SATzilla07+ (S++ ,D+
r ) solved given instance cases.
Another interesting observation solvers success ratio high, average
runtime tended lower.
7.2 HANDMADE Category
configurations three SATzilla versions designed HANDMADE category
shown Table 17. Again, SATzilla07+ (S++ ,D+
h ) included three local search solvers,
Ranov, Ag2wsat+ Gnovelty+, available SATzilla07. Like
manual choice SATzilla07, automatic pre-solver selection chose run March dl04
five CPU seconds. Unlike manual selection, abstained using SAPS (or indeed
solver) second pre-solver. Table 18 shows performance different
versions SATzilla compared best solvers category HANDMADE. Here, half
observed performance improvement achieved using solvers
training data; half due improvements SATzilla07+ . Note
HANDMADE category, SATzilla07 (S++ ,D+ ) performed quite poorly. attribute
weakness feature-based classifier HANDMADE instances, issue discuss
Section 7.4.
Table 19 indicates often component solver SATzilla07+ (S++ ,D+
h )
selected, many problem instances solved, average runtime runs.
many solvers SATzilla07+ (S++ ,D+
h ) picked quite rarely; however,
cases, success ratios close 100%, average runtimes low.
7.3 INDUSTRIAL Category
Table 20 shows configuration three different SATzilla versions designed
INDUSTRIAL category. Local search solvers performed quite poorly instances
category, best local search solver, Ag2wsat0, solving 23% instances
595

fiXu, Hutter, Hoos & Leyton-Brown

SATzilla

Pre-Solver (time)

Components

SATzilla07(S,D0h )

March dl04(5); SAPS(2)

SATzilla07(S+ ,D+
h)

March dl04(5); SAPS(2)

SATzilla07+ (S++ ,D+
h)

March ks(5)

Kcnfs06, March dl04, Minisat 2.0,
Rsat 1.03
Vallst, Zchaff rand, TTS, MXC,
March ks, Minisat07, Rsat 2.0
Eureka, March dl04; Minisat 2.0,
Rsat 1.03, Vallst, TTS, Picosat, MXC,
March ks, TinisatElite, Minisat07,
Rsat 2.0, Ranov, Ag2wsat0, Gnovelty+

Table 17: SATzillas configuration HANDMADE category.
Solver

Avg. runtime [s]

Solved [%]

Performance score

TTS
MXC
March ks
Minisat07
March dl04

729
527
494
438
408

41.1
61.9
63.9
68.9
72.4

40669
43024
68859
59863
73226

SATzilla07(S,D0h )
SATzilla07(S+ ,D+
h)
SATzilla07+ (S++ ,D+
h)
SATzilla07 (S++ ,D+ )

284
203
131
215

80.4
87.4
95.6
88.0

(93.5%)
(118.8%)
112287 (153.3%)
(110.5%)

Table 18: performance SATzilla compared best solvers HANDMADE. Scores nonportfolio solvers computed using reference set SATzilla solver

++
SATzilla07+ (S++ ,D+
,D+ )
h ). Cutoff time: 1 200 CPU seconds; SATzilla07 (S
trained ALL.
100

100
AvgFeature(07+(S++,D+h))

90

70
60
50
40

++

Oracle(S )
SATzila07+(S++,D+h)

30

Presolving(07+(S++,D+h))

0

10

1

10

2

10

AvgFeature(07(S+,D+h),07(S,Dh))
AvgFeature(07+(S++,D+h))

AvgFeature(others)

70
60
50
Oracle(S++)
Oracle(S)
+ ++ +
SATzilla07 (S ,Dh)

40
30

SATzilla07(S+,D+h)

20

March_dl04
March_ks
Minisat07

20
10 1
10

Presolving(07(S+,D+h),07(S,Dh))

80 Presolving(others)

80

% Instances Solved

% Instances Solved

90

Presolving(07+(S++,D+h))

SATzilla07(S,D )
h

10
3

10

Runtime [CPU sec]

0 1
10

SATzilla07*(S++,D+)
0

10

1

10

2

10

Runtime [CPU sec]

Figure 9: Left: CDFs SATzilla07+ (S++ ,D+
h ) best non-portfolio solvers HANDMADE;
right: CDFs different versions SATzilla HANDMADE shown Table 17,
SATzilla07 (S++ ,D+ ) trained ALL. solvers CDFs ones
shown here.

596

3

10

fiSATzilla: Portfolio-based Algorithm Selection SAT

Pre-Solver (Pre-Time)

Solved [%]

Avg. Runtime [CPU sec]

March ks(5)

39.0

3.2

Selected Solver

Selected [%]

Success [%]

Avg. Runtime [CPU sec]

Minisat07
TTS
MXC
March ks
Eureka
March dl04
Rsat 1.03
Picosat
Ag2wsat0
TinisatElite
Ranov
Minisat 2.0
Rsat 2.0
Gnovelty+
Vallst

40.4
11.5
7.2
7.2
5.8
5.8
4.8
3.9
3.4
2.9
2.9
1.4
1.4
1.0
0.5

89.3
91.7
93.3
100
100
91.7
100
100
100
100
83.3
66.7
100
100
100

205.1
133.2
310.5
544.7
0.34
317.6
185.1
1.7
0.5
86.5
206.1
796.5
0.9
3.2
<0.01

Table 19: solvers selected SATzilla07+ (S++ ,D+
h ) HANDMADE category.
SATzilla

Pre-Solver (time)

Components

SATzilla07(S,Di )

Rsat 1.03 (2)

SATzilla07(S+ ,D+
)

Rsat 2.0 (2)

Eureka, March dl04, Minisat 2.0,
Rsat 1.03
Eureka, March dl04, Minisat 2.0,
Zchaff Rand, TTS, Picosat, March ks

SATzilla07+ (S++ ,D+
)

Rsat 2.0 (10); Gnovelty+(2)

Eureka, March dl04, Minisat 2.0,
Rsat 1.03, TTS, Picosat, Minisat07,
Rsat 2.0

Table 20: SATzillas configuration INDUSTRIAL category.
within cutoff time. Consequently, local search solver selected automatic
solver subset selection SATzilla07+ (S++ ,D+
). However, automatic pre-solver selection
include local search solver Gnovelty+ second pre-solver, run 2 CPU
seconds 10 CPU seconds running Rsat 2.0.
Table 21 compares performance different versions SATzilla best
solvers INDUSTRIAL instances. surprising training data
solvers helped SATzilla07 improve terms metrics (avg. runtime, percentage
solved score). somewhat bigger improvement due new mechanisms
SATzilla07+ led SATzilla07+ (S++ ,D+
) outperforming every non-portfolio solver
respect every metric, specially terms performance score. Note general
SATzilla version SATzilla07 (S++ ,D+ ) trained achieved performance close
SATzilla07+ (S++ ,D+
) INDUSTRIAL data set terms average runtime
percentage solved instances.
597

fiXu, Hutter, Hoos & Leyton-Brown

Solver

Avg. runtime [s]

Solved [%]

Performance score

Rsat 1.03
Rsat 2.0
Picosat
TinisatElite
Minisat07
Eureka

353
365
282
452
372
349

80.8
80.8
85.9
70.8
76.6
83.2

52740
51299
66561
40867
60002
71505

SATzilla07(S,D0i )
SATzilla07(S+ ,D+
)
SATzilla07+ (S++ ,D+
)
SATzilla07 (S++ ,D+ )

298
262
233
239

87.6
89.0
93.1
92.7

(91.3%)
(98.2%)
79724 (111.5%)
(104.8%)

Table 21: performance SATzilla compared best solvers INDUSTRIAL. Scores
non-portfolio solvers computed using reference set
SATzilla solver SATzilla07+ (S++ ,D+
Cutoff time: 1 200 CPU seconds;
).
SATzilla07 (S++ ,D+ ) trained ALL.

Pre-Solver (Pre-Time)

Solved [%]

Avg. Runtime [CPU sec]

Rsat 2.0(10)
Gnovelty+ (2)

38.1
0.3

6.8
2.0

Selected Solver

Selected [%]

Success [%]

Avg. Runtime [CPU sec]

Eureka (BACKUP)
Eureka
Picosat
Minisat07
Minisat 2.0
March dl04
TTS
Rsat 2.0
Rsat 1.03

29.1
15.1
14.5
14.0
12.3
8.4
3.9
1.7
1.1

88.5
100
96.2
84.0
68.2
86.7
100
100
100

385.4
394.2
179.6
306.3
709.2
180.8
0.7
281.6
10.6

Table 22: solvers selected SATzilla07+ (S++ ,D+
) INDUSTRIAL category.
seen Figure 10, performance improvements achieved SATzilla
non-portfolio solvers smaller INDUSTRIAL category categories. Note best INDUSTRIAL solver performed well, solving 85.9%
instances within cutoff time 1 200 CPU seconds.6 Still, SATzilla07+ (S++ ,D+
)
significantly smaller average runtime (17%) solved 7.2% instances best
component solver, Picosat. Likewise, score SATzilla07+ (S++ ,D+
) 11.5%
higher top-ranking component solver (in terms score), Eureka.
6. Recall number means solver solved 85.9% instances could solved least
one solver. Compared data sets, seems either solvers exhibited similar behavior
INDUSTRIAL instances instances category exhibited greater variability hardness.

598

fiSATzilla: Portfolio-based Algorithm Selection SAT

100

100
+

90

++

+

Presolving(07 (S ,Di ))

+

++

+

AvgFeature(07 (S ,Di ))

90
80

% Instances Solved

% Instances Solved

80
70
60
50
40
30
20
10 1
10

0

10

1

10

70

Presolving(others)

AvgFeature(07*(S++,D+))
AvgFeature(others)

50
++

Oracle(S )
Oracle(S)
+ ++ +
SATzilla07 (S ,Di ))

40
30

Rsat1.03
Picosat

20

2

Presolving(07*(S++,D+))

+ ++ +
AvgFeature(07 (S ,Di ))

60

Oracle(S++)
SATzilla07+(S++,D+i)

10

Presolving(07+(S++,D+i))

SATzilla07(S+,D+i)
SATzilla07(S,Di)
*

++

+

SATzilla07 (S ,D )
3

10

Runtime [CPU sec]

10 1
10

0

10

1

10

2

10

3

10

Runtime [CPU sec]

CDFs SATzilla07+ (S++ ,D+
) best non-portfolio solvers
INDUSTRIAL; right: CDFs different versions SATzilla INDUSTRIAL shown
Table 20, SATzilla07 (S++ ,D+ ) trained ALL. solvers CDFs
(including Eurekas) ones shown here.

Figure 10: Left:

Table 22 indicates often component solver SATzilla07+ (S++ ,D+
) selected, many problem instances solved, average runtime runs.
case, backup solver Eureka used problem instances feature computation
timed pre-solvers produce solution.
7.4
four versions SATzilla specialized category ALL. detailed configurations listed Table 23. results automatic pre-solver selection identical
SATzilla07+ SATzilla07 : chose first run local search solver SAPS
two CPU seconds, followed two CPU seconds March ks. solvers similar manual selection, order reversed. solver subset selection,
SATzilla07+ SATzilla07 yielded somewhat different results, kept
two local search algorithms, Ag2wsat+ & Ranov, Ag2wsat+ & Gnovelty+, respectively.
Table 24 compares performance four versions SATzilla category.
Roughly equal improvements terms performance metrics due
training data solvers one hand, improvements SATzilla07+
hand. best performance terms performance metrics obtained
SATzilla07 (S++ ,D+ ). Recall difference SATzilla07+ (S++ ,D+ )
SATzilla07 (S++ ,D+ ) use general hierarchical hardness models,
described Section 5.5.
Note using classifier course good using oracle determining
distribution instance comes from; thus, success ratios solvers selected
SATzilla07 instances test set distribution (see Table 25)
slightly lower solvers picked SATzilla07+ distributions
individually (see Tables 16, 19, 22). However, compared SATzilla07+
distribution ALL, SATzilla07 performed significantly better: achieving overall performance
599

fiXu, Hutter, Hoos & Leyton-Brown

SATzilla

Pre-Solver (time)

Components

SATzilla07(S,D)

March dl04(5); SAPS(2)

Eureka, Kcnfs06, March dl04, Minisat
2.0,Zchaff rand

SATzilla07(S+ ,D+ )

March dl04(5); SAPS(2)

SATzilla07+ (S++ ,D+ )

SAPS(2); March ks(2)

SATzilla07 (S++ ,D+ )

SAPS(2); March ks(2)

Eureka, March dl04, Zchaff rand,
Kcnfs04, TTS, Picosat, March ks,
Minisat07
Eureka,
Kcnfs06,
Rsat 1.03,
Zchaff rand, TTS, MXC, TinisatElite,
Rsat 2.0, Ag2wsat+, Ranov
Eureka, Kcnfs06, March dl04, Minisat
2.0,
Rsat 1.03,
Picosat,
MXC,
Minisat07,
Ag2wsat+,
March ks,
Gnovelty+

Table 23: SATzillas configuration category.
Solver

Avg. runtime [s]

Solved [%]

Performance score

Rsat 1.03
Kcnfs04
TTS
Picosat
March ks
TinisatElite
Minisat07
Gnovelty+
March dl04

542
969
939
571
509
690
528
684
509

61.1
21.3
22.6
57.7
62.9
47.3
61.8
43.9
62.7

131399
46695
74616
135049
202133
93169
162987
156365
205592

SATzilla07(S,D)
SATzilla07(S+ ,D+ )
SATzilla07+ (S++ ,D+ )
SATzilla07 (S++ ,D+ )

282
224
194
172

83.1
87.0
91.1
92.9

(125.0%)
(139.2%)
(158%)
344594 (167.6%)

Table 24: performance SATzilla compared best solvers ALL. Scores nonportfolio solvers computed using reference set SATzilla solver
SATzilla07 (S++ ,D+ ). Cutoff time: 1 200 CPU seconds.

improvements 11.3% lower average runtime, 1.8% solved instances 9.6% higher
score. supports initial hypothesis SATzilla07 would perform slightly worse
specialized versions SATzilla07+ single category, yet would yield best
result applied broader heterogeneous set instances.
runtime cumulative distribution function (Figure 11, right) shows
SATzilla07 (S++ ,D+ ) dominated versions SATzilla solved
30% instances best non-portfolio solver, March dl04 (Figure 11, left).
Table 26 shows performance general classifier SATzilla07 (S++ ,D+ ).
note several patterns: Firstly, classification performance RANDOM INDUSTRIAL in600

fiSATzilla: Portfolio-based Algorithm Selection SAT

100

100

Oracle(S++)
Oracle(S)
SATzilla07+(S++,D+)
SATzilla07(S+,D+)
SATzilla07(S,D)
SATzilla07*(S++,D+)

++

Oracle(S )
SATzilla07*(S++,D+)
March_dl04
Gnovelty+

90

80

% Instances Solved

% Instances Solved

80

90

70
60
50
40
30

70
60
50
40
30
20

20

+

+

Presolving(07(S ,D ),07(S,D))

10

++

+

Presolving(07*(S ,D ))

0 1
10

0

10

++

1

10

+

+

AvgFeature(07(S ,D ),07(S,D))

10

+

AvgFeature(07*(S ,D ))

Presolving(others)
2

0 1
10

3

10

10

AvgFeature(others)
0

10

1

2

10

10

3

10

Runtime [CPU sec]

Runtime [CPU sec]

Figure 11: Left: CDF SATzilla07 (S++ ,D+ ) best non-portfolio solvers ALL; right:
CDFs different versions SATzilla shown Table 23. solvers
CDFs ones shown here.

Pre-Solver (Pre-Time)

Solved [%]

Avg. Runtime [CPU sec]

SAPS(2)
March ks (2)

33.0
13.9

1.4
1.6

Selected Solver

Selected [%]

Success [%]

Avg. Runtime [CPU sec]

Minisat07
March dl04
Gnovelty+
March ks
Eureka (BACKUP)
Eureka
Picosat
Kcnfs06
MXC
Rsat 1.03
Minisat 2.0
Ag2wsat+

21.2
14.5
12.5
9.1
8.9
7.2
6.6
6.5
5.5
4.0
3.5
0.5

85.5
84.0
85.2
89.8
89.7
97.9
90.7
95.2
88.9
80.8
56.5
33.3

247.5
389.5
273.2
305.6
346.1
234.6
188.6
236.3
334.0
364.9
775.7
815.7

Table 25: solvers selected SATzilla07 (S++ ,D+ ) category.

stances much better HANDMADE instances. Secondly, HANDMADE instances,
misclassifications due misclassification type instance,
rather satisfiability status. Finally, see RANDOM instances almost perfectly classified RANDOM instances classified
RANDOM, HANDMADE INDUSTRIAL instances confused somewhat often.
comparably poor classification performance HANDMADE instances partly explains
SATzilla07 (S++ ,D+ ) perform well HANDMADE category others.
601

fiXu, Hutter, Hoos & Leyton-Brown

R, sat

R, unsat

H, sat

H, unsat

I, sat

I, unsat

92%

5%

1%



1%

1%

4%

94%



1%



1%

classified H, sat





57%

38%



5%

classified H, unsat



1%

23%

71%

1%

4%

classified I, sat





8%



81%

11%

classified I, unsat







5%

6%

89%

classified R, sat
classified R, unsat

Table 26: Confusion matrix 6-way classifier data set ALL.

8. Conclusions
Algorithms combined portfolios build whole greater sum
parts. significantly extended earlier work algorithm portfolios SAT
select solvers per-instance basis using empirical hardness models runtime prediction. demonstrated effectiveness general portfolio construction method,
SATzilla07, four large sets SAT competition instances. experiments show
SATzilla07 portfolio solvers always outperform components. Furthermore,
SATzilla07s excellent performance recent 2007 SAT Competition demonstrates
practical effectiveness approach.
work, pushed SATzilla approach beyond SATzilla07.
first time, showed portfolios optimize complex scoring functions integrate
local search algorithms component solvers. Furthermore, showed automate
process pre-solver selection, one last aspects approach previously
based manual engineering. demonstrated extensive computational experiments,
enhancements improved SATzilla07s performance substantially.
SATzilla stage applied box given set
possible component solvers along representative training validation instances.
automated built-in meta-optimization process, component solvers used
solvers used pre-solvers automatically determined given set
solvers, without human effort. computational bottleneck execute possible
component solvers representative set instances order obtain enough runtime
data build reasonably accurate empirical hardness models. However, computations
parallelized easily require human intervention, computer time,
becomes ever cheaper. Matlab code building empirical hardness models
C++ code building SATzilla portfolios use models available online
http://www.cs.ubc.ca/labs/beta/Projects/SATzilla.
interesting note use local search methods significant impact
performance SATzilla. preliminary experiments, observed overall performance SATzilla07 significantly weaker local search solvers
local-search-based features excluded. Specifically, availability local search
602

fiSATzilla: Portfolio-based Algorithm Selection SAT

components substantially boosted SATzilla07 performance RANDOM instance category also led improvements INDUSTRIAL, resulted weaker performance
HANDMADE instances. Generally, believe better understanding impact
features runtime predictions instance categorizations allow us
improve SATzilla, therefore begun in-depth investigation direction.
SATzillas performance ultimately depends power component solvers
automatically gets better improved. Furthermore, SATzilla takes advantage solvers competitive certain kinds instances perform poorly
otherwise, thus SATzillas success demonstrates value solvers. Indeed,
identification solvers, otherwise easily overlooked, still
potential improve SATzillas performance substantially.

Acknowledgments
work builds contributions wide range past co-authors, colleagues,
members SAT community. First, many colleagues thank contributions work described article. Eugene Nudelman, Alex Devkar Yoav
Shoham Kevin Holgers co-authors papers first introduced SATzilla
(Nudelman et al., 2004a, 2004b); work grew project automated algorithm
selection involved Galen Andrew Jim McFadden, addition Kevin, Eugene
Yoav (Leyton-Brown et al., 2003b, 2003a). Nando de Freitas, Bart Selman, Kevin
Murphy gave useful suggestions machine learning algorithms, SAT instance features,
mixtures experts, respectively. Second, academic research always builds
previous work, especially indebted authors dozens SAT solvers
discuss paper, particularly commitment furthering scientific understanding making code publicly available. Without researchers considerable
efforts, SATzilla could never built.

References
Bacchus, F. (2002a). Enhancing Davis Putnam extended binary clause reasoning. Proceedings
Eighteenth National Conference Artificial Intelligence (AAAI02), pp. 613619.
Bacchus, F. (2002b). Exploring computational tradeoff reasoning less searching.
Proceedings Fifth International Conference Theory Applications Satisfiability Testing (SAT02), pp. 716.
Bacchus, F., & Winter, J. (2003). Effective preprocessing hyper-resolution equality reduction.
Proceedings Sixth International Conference Theory Applications Satisfiability
Testing (SAT03), pp. 341355.
Biere, A. (2007). Picosat version 535. Solver description, SAT competition 2007.
Biere, A., Cimatti, A., Clarke, E. M., Fujita, M., & Zhu, Y. (1999). Symbolic model checking using SAT
procedures instead BDDs. Proceedings Design Automation Conference (DAC99), pp. 317320.
Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.
Bregman, D. R., & Mitchell, D. G. (2007). SAT solver MXC, version 0.5. Solver description, SAT
competition 2007.
C. M. Li, W. W., & Zhang, H. (2007). Combining adaptive noise promising decreasing variables local
search SAT. Solver description, SAT competition 2007.

603

fiXu, Hutter, Hoos & Leyton-Brown

Carchrae, T., & Beck, J. C. (2005). Applying machine learning low-knowledge control optimization
algorithms. Computational Intelligence, 21 (4), 372387.
Crawford, J. M., & Baker, A. B. (1994). Experimental results application satisfiability algorithms scheduling problems. Proceedings Twelfth National Conference Artificial Intelligence (AAAI94), pp. 10921097.
Davis, M., Logemann, G., & Loveland, D. (1962). machine program theorem proving. Communications
ACM, 5 (7), 394397.
Davis, M., & Putnam, H. (1960). computing procedure quantification theory. Journal ACM,
7 (1), 201215.
Dechter, R., & Rish, I. (1994). Directional resolution: Davis-Putnam procedure, revisited. Principles
Knowledge Representation Reasoning (KR94), pp. 134145.
Dequen, G., & Dubois, O. (2007). kcnfs. Solver description, SAT competition 2007.
Dubois, O., & Dequen, G. (2001). backbone-search heuristic efficient solving hard 3-SAT formulae.
Proceedings Seventeenth International Joint Conference Artificial Intelligence (IJCAI01),
pp. 248253.
Een, N., & Sorensson, N. (2003). extensible SAT-solver. Proceedings Sixth International
Conference Theory Applications Satisfiability Testing (SAT03), pp. 502518.
Een, N., & Sorensson, N. (2006). Minisat v2.0 (beta). Solver description, SAT Race 2006.
Gagliolo, M., & Schmidhuber, J. (2006a). Impact censored sampling performance restart
strategies. Twelfth Internatioal Conference Principles Practice Constraint Programming (CP06), pp. 167181.
Gagliolo, M., & Schmidhuber, J. (2006b). Learning dynamic algorithm portfolios. Annals Mathematics
Artificial Intelligence, 47 (3-4), 295328.
Gebruers, C., Hnich, B., Bridge, D., & Freuder, E. (2005). Using CBR select solution strategies
constraint programming. Proceedings Sixth International Conference Case-Based Reasoning (ICCBR05), pp. 222236.
Gebruers, C., Guerri, A., Hnich, B., & Milano, M. (2004). Making choices using structure instance
level within case based reasoning framework. International Conference Integration AI
Techniques Constraint Programming Combinatorial Optimization Problems (CPAIOR-04),
pp. 380386.
Gomes, C. P., & Selman, B. (2001). Algorithm portfolios. Artificial Intelligence, 126(1-2), 4362.
Guerri, A., & Milano, M. (2004). Learning techniques automatic algorithm portfolio selection.
Proceedings 16th European Conference Artificial Intelligence (ECAI-04), pp. 475479.
Guo, H., & Hsu, W. H. (2004). learning-based algorithm selection meta-reasoner real-time MPE
problem. Proceedings Seventeenth Australian Conference Artificial Intelligence, pp. 307
318.
Guyon, I., Gunn, S., Nikravesh, M., & Zadeh, L. (2006). Feature Extraction, Foundations Applications.
Springer.
Heule, M., & v. Maaren, H. (2007). march ks. Solver description, SAT competition 2007.
Heule, M., Zwieten, J., Dufour, M., & Maaren, H. (2004). March eq: implementing additional reasoning
efficient lookahead SAT solver. Proceedings Seventh International Conference Theory
Applications Satisfiability Testing (SAT04), pp. 345359.
Hoos, H. H. (2002). adaptive noise mechanism WalkSAT. Proceedings Eighteenth National
Conference Artificial Intelligence (AAAI02), pp. 655660.
Hoos, H. H., & Stutzle, T. (2005). Stochastic Local Search - Foundations & Applications. Morgan Kaufmann
Publishers, San Francisco, CA, USA.
Horvitz, E., Ruan, Y., Gomes, C. P., Kautz, H., Selman, B., & Chickering, D. M. (2001). Bayesian
approach tackling hard computational problems. Proceedings Seventeenth Conference
Uncertainty Artificial Intelligence (UAI01), pp. 235244.
Huang, J. (2007). TINISAT SAT competition 2007. Solver description, SAT competition 2007.
Huberman, B., Lukose, R., & Hogg, T. (1997). economics approach hard computational problems.
Science, 265, 5154.

604

fiSATzilla: Portfolio-based Algorithm Selection SAT

Hutter, F., Hamadi, Y., Hoos, H. H., & Leyton-Brown, K. (2006). Performance prediction automated
tuning randomized parametric algorithms. Twelfth Internatioal Conference Principles
Practice Constraint Programming (CP06), pp. 213228.
Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling probabilistic smoothing: Efficient dynamic
local search SAT. Proceedings Eighth International Conference Principles Practice
Constraint Programming, pp. 233248.
Ishtaiwi, A., Thornton, J., Anbulagan, Sattar, A., & Pham, D. N. (2006). Adaptive clause weight redistribution. Twelfth Internatioal Conference Principles Practice Constraint Programming (CP06), pp. 229243.
Kautz, H., & Selman, B. (1996). Pushing envelope: Planning, propositional logic, stochastic search.
Proceedings Thirteenth National Conference Artificial Intelligence Eighth Innovative
Applications Artificial Intelligence Conference, pp. 11941201.
Kautz, H. A., & Selman, B. (1999). Unifying SAT-based graph-based planning. Proceedings
Sixteenth International Joint Conference Artificial Intelligence (IJCAI99), pp. 318325.
Knuth, D. (1975). Estimating efficiency backtrack programs. Mathematics Computation, 29 (129),
121136.
Krishnapuram, B., Carin, L., Figueiredo, M., & Hartemink, A. (2005). Sparse multinomial logistic regression:
Fast algorithms generalization bounds. IEEE Transactions Pattern Analysis Machine
Intelligence, pp. 957968.
Kullmann, O. (2002). Investigating behaviour SAT solver random formulas. http://cssvr1.swan.ac.uk/csoliver/Artikel/OKsolverAnalyse.html.
Lagoudakis, M. G., & Littman, M. L. (2001). Learning select branching rules DPLL procedure
satisfiability. LICS/SAT, pp. 344359.
Le Berre, D., & Simon, L. (2004). Fifty-five solvers Vancouver: SAT 2004 competition. Proceedings
Seventh International Conference Theory Applications Satisfiability Testing (SAT04),
pp. 321344.
Leyton-Brown, K., Nudelman, E., Andrew, G., McFadden, J., & Shoham, Y. (2003a). Boosting metaphor
algorithm design. Ninth Internatioal Conference Principles Practice Constraint
Programming (CP03), pp. 899903.
Leyton-Brown, K., Nudelman, E., Andrew, G., McFadden, J., & Shoham, Y. (2003b). portfolio approach
algorithm selection. Proceedings Eighteenth International Joint Conference Artificial
Intelligence (IJCAI03), pp. 15421543.
Leyton-Brown, K., Nudelman, E., & Shoham, Y. (2002). Learning empirical hardness optimization
problems: case combinatorial auctions. Eighth Internatioal Conference Principles
Practice Constraint Programming (CP02), pp. 556572.
Li, C., & Huang, W. (2005). Diversification determinism local search satisfiability. Proceedings
Eighth International Conference Theory Applications Satisfiability Testing (SAT05),
pp. 158172.
Lobjois, L., & Lematre, M. (1998). Branch bound algorithm selection performance prediction.
Proceedings Fifteenth National Conference Artificial Intelligence (AAAI98), pp. 353358.
Mahajan, Y. S., Fu, Z., & Malik, S. (2005). Zchaff2004: efficient SAT solver. Proceedings Eighth
International Conference Theory Applications Satisfiability Testing (SAT05), pp. 360375.
Murphy, K. (2001). Bayes Net Toolbox Matlab. Computing Science Statistics, Vol. 33.
http://bnt.sourceforge.net/.
Nadel, A., Gordon, M., Palti, A., & Hanna, Z. (2006). Eureka-2006 SAT solver. Solver description, SAT
Race 2006.
Nudelman, E., Leyton-Brown, K., Hoos, H. H., Devkar, A., & Shoham, Y. (2004a). Understanding random
SAT: Beyond clauses-to-variables ratio. Tenth Internatioal Conference Principles
Practice Constraint Programming (CP04), pp. 438452.
Nudelman, E., Leyton-Brown, K., Devkar, A., Shoham, Y., & Hoos, H. (2004b). Satzilla: algorithm
portfolio SAT. Solver description, SAT competition 2004.
Pham, D. N., & Anbulagan (2007). Resolution enhanced SLS solver: R+AdaptNovelty+. Solver description,
SAT competition 2007.

605

fiXu, Hutter, Hoos & Leyton-Brown

Pham, D. N., & Gretton, C. (2007). gNovelty+. Solver description, SAT competition 2007.
Pipatsrisawat, K., & Darwiche, A. (2006). Rsat 1.03: SAT solver description. Tech. rep. D-152, Automated
Reasoning Group, UCLA.
Pipatsrisawat, K., & Darwiche, A. (2007). Rsat 2.0: SAT solver description. Solver description, SAT
competition 2007.
Rice, J. R. (1976). algorithm selection problem. Advances Computers, 15, 65118.
Samulowitz, H., & Memisevic, R. (2007). Learning solve QBF. Proceedings Twentysecond
National Conference Artificial Intelligence (AAAI07), pp. 255260.
Schmee, J., & Hahn, G. J. (1979). simple method regression analysis censored data. Technometrics,
21 (4), 417432.
Selman, B., Kautz, H., & Cohen, B. (1994). Noise strategies improving local search. Proceedings
Twelfth National Conference Artificial Intelligence (AAAI94), pp. 337343.
Selman, B., Levesque, H., & Mitchell, D. (1992). new method solving hard satisfiability problems.
Proceedings Tenth National Conference Artificial Intelligence (AAAI92), pp. 440446.
Sorensson, N., & Een, N. (2007). Minisat2007. http://www.cs.chalmers.se/Cs/Research/FormalMethods/MiniSat/.
Spence, I. (2007). Ternary tree solver (tts-4-0). Solver description, SAT competition 2007.
Stephan, P., Brayton, R., & Sangiovanni-Vencentelli, A. (1996). Combinational test generation using satisfiability. IEEE Transactions Computer-Aided Design Integrated Circuits Systems, 15,
11671176.
Streeter, M., Golovin, D., & Smith, S. F. (2007). Combining multiple heuristics online. Proceedings
Twentysecond National Conference Artificial Intelligence (AAAI07), pp. 11971203.
Subbarayan, S., & Pradhan, D. (2005). Niver: Non-increasing variable elimination resolution preprocessing sat instances. Lecture Notes Computer Science,Springer, 3542/2005, 276291.
Tompkins, D. A. D., & Hoos, H. H. (2004). UBCSAT: implementation experimentation environment
SLS algorithms SAT & MAX-SAT.. Proceedings Seventh International Conference
Theory Applications Satisfiability Testing (SAT04).
Vallstrom, D. (2005). Vallst documentation. http://vallst.satcompetition.org/index.html.
van Gelder, A. (2002). Another look graph coloring via propositional satisfiability. Proceedings
Computational Symposium Graph Coloring Generalizations (COLOR-02), pp. 4854.
Wei, W., Li, C. M., & Zhang, H. (2007). Deterministic random selection variables local search
SAT. Solver description, SAT competition 2007.
Xu, L., Hoos, H. H., & Leyton-Brown, K. (2007a). Hierarchical hardness models SAT. Thirteenth
Internatioal Conference Principles Practice Constraint Programming (CP07), pp. 696711.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2007b). Satzilla-07: design analysis
algorithm portfolio SAT. Thirteenth Internatioal Conference Principles Practice
Constraint Programming (CP07), pp. 712727.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2007c). Satzilla2007: new & improved algorithm
portfolio SAT. Solver description, SAT competition 2007.
Zhang, L., Madigan, C. F., Moskewicz, M. W., & Malik, S. (2001). Efficient conflict driven learning Boolean
satisfiability solver. Proceedings International Conference Computer Aided Design, pp.
279285.
Zhang, L. (2002). quest efficient Boolean satisfiability solvers. Proceedings 8th International
Conference Computer Aided Deduction (CADE-02), pp. 313331.

606

fiJournal Artificial Intelligence Research 32 (2008) 663-704

Submitted 03/08; published 07/08

Online Planning Algorithms POMDPs
Stephane Ross
Joelle Pineau

stephane.ross@mail.mcgill.ca
jpineau@cs.mcgill.ca

School Computer Science
McGill University, Montreal, Canada, H3A 2A7

Sebastien Paquet
Brahim Chaib-draa

spaquet@damas.ift.ulaval.ca
chaib@damas.ift.ulaval.ca

Department Computer Science Software Engineering
Laval University, Quebec, Canada, G1K 7P4

Abstract
Partially Observable Markov Decision Processes (POMDPs) provide rich framework
sequential decision-making uncertainty stochastic domains. However, solving
POMDP often intractable except small problems due complexity. Here,
focus online approaches alleviate computational complexity computing
good local policies decision step execution. Online algorithms generally consist lookahead search find best action execute time step
environment. objectives survey various existing online POMDP
methods, analyze properties discuss advantages disadvantages;
thoroughly evaluate online approaches different environments various metrics (return, error bound reduction, lower bound improvement). experimental results
indicate state-of-the-art online heuristic search methods handle large POMDP
domains efficiently.

1. Introduction
Partially Observable Markov Decision Process (POMDP) general model sequential decision problems partially observable environments. Many planning control problems modeled POMDPs, solved exactly
computational complexity: finite-horizon POMDPs PSPACE-complete (Papadimitriou & Tsitsiklis, 1987) infinite-horizon POMDPs undecidable (Madani, Hanks,
& Condon, 1999).
last years, POMDPs generated significant interest AI community many approximation algorithms developed (Hauskrecht, 2000; Pineau,
Gordon, & Thrun, 2003; Braziunas & Boutilier, 2004; Poupart, 2005; Smith & Simmons,
2005; Spaan & Vlassis, 2005). methods offline algorithms, meaning
specify, prior execution, best action execute possible situations.
approximate algorithms achieve good performance, often take significant time (e.g. hour) solve large problems, many
possible situations enumerate (let alone plan for). Furthermore, small changes
environments dynamics require recomputing full policy, may take hours days.
c
2008
AI Access Foundation. rights reserved.

fiRoss, Pineau, Paquet, & Chaib-draa

hand, online approaches (Satia & Lave, 1973; Washington, 1997; Barto,
Bradtke, & Singhe, 1995; Paquet, Tobin, & Chaib-draa, 2005; McAllester & Singh, 1999;
Bertsekas & Castanon, 1999; Shani, Brafman, & Shimony, 2005) try circumvent complexity computing policy planning online current information state. Online algorithms sometimes also called agent-centered search algorithms (Koenig, 2001).
Whereas offline search would compute exponentially large contingency plan considering possible happenings, online search considers current situation small
horizon contingency plans. Moreover, approaches handle environment
changes without requiring computation, allows online approaches applicable many contexts offline approaches applicable, instance,
task accomplish, defined reward function, changes regularly environment.
One drawback online planning generally needs meet real-time constraints,
thus greatly reducing available planning time, compared offline approaches.
Recent developments online POMDP search algorithms (Paquet, Chaib-draa, & Ross,
2006; Ross & Chaib-draa, 2007; Ross, Pineau, & Chaib-draa, 2008) suggest combining
approximate offline online solving approaches may efficient way tackle
large POMDPs. fact, generally compute rough policy offline using existing
offline value iteration algorithms, use approximation heuristic function
guide online search algorithm. combination enables online search algorithms
plan shorter horizons, thereby respecting online real-time constraints retaining
good precision. exact online search fixed horizon, guarantee
reduction error approximate offline value function. overall time (offline
online) required obtain good policy dramatically reduced combining
approaches.
main purpose paper draw attention AI community online
methods viable alternative solving large POMDP problems. support this,
first survey various existing online approaches applied POMDPs,
discuss strengths drawbacks. present various combinations online algorithms
various existing offline algorithms, QMDP (Littman, Cassandra, & Kaelbling,
1995), FIB (Hauskrecht, 2000), Blind (Hauskrecht, 2000; Smith & Simmons, 2005)
PBVI (Pineau et al., 2003). compare empirically different online approaches
two large POMDP domains according different metrics (average discounted return, error
bound reduction, lower bound improvement). also evaluate available online
planning time offline planning time affect performance different algorithms.
results experiments show many state-of-the-art online heuristic search methods
tractable large state observation spaces, achieve solution quality stateof-the-art offline approaches fraction computational cost. best methods
achieve focusing search relevant future outcomes current
decision, e.g. likely high uncertainty (error) longterm values, minimize quickly possible error bound performance
best action found. tradeoff solution quality computing time offered
combinations online offline approaches attractive tackling increasingly
large domains.
664

fiOnline Planning Algorithms POMDPs

2. POMDP Model
Partially observable Markov decision processes (POMDPs) provide general framework
acting partially observable environments (Astrom, 1965; Smallwood & Sondik, 1973;
Monahan, 1982; Kaelbling, Littman, & Cassandra, 1998). POMDP generalization
MDP model planning uncertainty, gives agent ability
effectively estimate outcome actions even cannot exactly observe state
environment.
Formally, POMDP represented tuple (S, A, T, R, Z, O) where:
set environment states. state description environment
specific moment capture information relevant agents
decision-making process.
set possible actions.
: [0, 1] transition function, (s, a, s0 ) = Pr(s0 |s, a)
represents probability ending state s0 agent performs action state
s.
R : R reward function, R(s, a) reward obtained
executing action state s.
Z set possible observations.
: Z [0, 1] observation function, O(s0 , a, z) = Pr(z|a, s0 ) gives
probability observing z action performed resulting state s0 .
assume paper S, Z finite R bounded.
key aspect POMDP model assumption states directly
observable. Instead, given time, agent access observation
z Z gives incomplete information current state. Since states
observable, agent cannot choose actions based states. consider
complete history past actions observations choose current action.
history time defined as:
ht = {a0 , z1 , . . . , zt1 , at1 , zt }.

(1)

explicit representation past typically memory expensive. Instead,
possible summarize relevant information previous actions observations
probability distribution state space S, called belief state (Astrom, 1965).
belief state time defined posterior probability distribution
state, given complete history:
bt (s) = Pr(st = s|ht , b0 ).

(2)

belief state bt sufficient statistic history ht (Smallwood & Sondik, 1973),
therefore agent choose actions based current belief state bt instead
past actions observations. Initially, agent starts initial belief state b0 ,
665

fiRoss, Pineau, Paquet, & Chaib-draa

representing knowledge starting state environment. Then, time
t, belief state bt computed previous belief state bt1 , using previous
action at1 current observation zt . done belief state update function
(b, a, z), bt = (bt1 , at1 , zt ) defined following equation:
bt (s0 ) = (bt1 , at1 , zt )(s0 ) =

1
Pr(zt |bt1 , at1 )

O(s0 , at1 , zt )

X

(s, at1 , s0 )bt1 (s), (3)

sS

Pr(z|b, a), probability observing z action belief b, acts
normalizing constant bt remains probability distribution:
Pr(z|b, a) =

X

O(s0 , a, z)

s0

X

(s, a, s0 )b(s).

(4)

sS

agent way computing belief, next interesting question
choose action based belief state.
action determined agents policy , specifying probability
agent execute action given belief state, i.e. defines agents strategy
possible situations could encounter. strategy maximize amount
reward earned finite infinite time horizon. article, restrict attention
infinite-horizon POMDPs optimality criterion maximize expected
sum discounted rewards (also called return discounted return). formally,
optimal policy defined following equation:
#
"
X
X X
(5)
bt (s)
R(s, a)(bt , a) |b0 ,
= argmax E



t=0

sS

aA

[0, 1) discount factor (bt , a) probability action
performed belief bt , prescribed policy .
return obtained following specific policy , certain belief state b,
defined value function equation V :
"
#
X
X


V (b) =
(b, a) RB (b, a) +
Pr(z|b, a)V ( (b, a, z)) .
(6)
aA

zZ

function RB (b, a) specifies immediate expected reward executing action
belief b according reward function R:
RB (b, a) =

X

b(s)R(s, a).

(7)

sS

sum Z Equation 6 interpreted expected future return infinite
horizon executing action a, assuming policy followed afterwards.
Note definitions RB (b, a), Pr(z|b, a) (b, a, z), one view
POMDP MDP belief states (called belief MDP), Pr(z|b, a) specifies
probability moving b (b, a, z) action a, RB (b, a) immediate
reward obtained action b.
666

fiOnline Planning Algorithms POMDPs

optimal policy defined Equation 5 represents action-selection strategy
maximize equation V (b0 ). Since always exists deterministic policy
maximizes V belief states (Sondik, 1978), generally consider deterministic policies (i.e. assign probability 1 specific action every belief
state).
value function V optimal policy fixed point Bellmans equation
(Bellman, 1957):
"
#
X
V (b) = max RB (b, a) +
Pr(z|b, a)V ( (b, a, z)) .
(8)
aA

zZ

Another useful quantity value executing given action belief state b,
denoted Q-value:
Q (b, a) = RB (b, a) +

X

Pr(z|b, a)V ( (b, a, z)).

(9)

zZ

difference definition V max operator omitted. Notice
Q (b, a) determines value assuming optimal policy followed
every step action a.
review different offline methods solving POMDPs. used guide
online heuristic search methods discussed later, cases form
basis online solutions.
2.1 Optimal Value Function Algorithm
One solve optimally POMDP specified finite horizon H using value
iteration algorithm (Sondik, 1971). algorithm uses dynamic programming compute
increasingly accurate values belief state b. value iteration algorithm
begins evaluating value belief state immediate horizon = 1. Formally,
let V value function takes belief state parameter returns numerical
value R belief state. initial value function is:
V1 (b) = max RB (b, a).
aA

(10)

value function horizon constructed value function horizon 1
using following recursive equation:
"
#
X
Vt (b) = max RB (b, a) +
Pr(z|b, a)Vt1 ( (b, a, z)) .
(11)
aA

zZ

value function Equation 11 defines discounted sum expected rewards
agent receive next time steps, belief state b. Therefore, optimal
policy finite horizon simply choose action maximizing Vt (b):
"
#
X
Pr(z|b, a)Vt1 ( (b, a, z)) .
(12)
(b) = argmax RB (b, a) +
aA

zZ

667

fiRoss, Pineau, Paquet, & Chaib-draa

last equation associates action specific belief state, therefore must
computed possible belief states order define full policy.
key result Smallwood Sondik (1973) shows optimal value function
finite-horizon POMDP represented hyperplanes, therefore convex
piecewise linear. means value function Vt horizon represented
set |S|-dimensional hyperplanes: = {0 , 1 , . . . , }. hyperplanes often
called -vectors. defines linear value function belief state space associated
action A. value belief state maximum value returned one
-vectors belief state. best action one associated -vector
returned best value:
X
(s)b(s).
(13)
Vt (b) = max


sS

number exact value function algorithms leveraging piecewise-linear convex
aspects value function proposed POMDP literature (Sondik, 1971;
Monahan, 1982; Littman, 1996; Cassandra, Littman, & Zhang, 1997; Zhang & Zhang,
2001). problem exact approaches number -vectors
needed represent value function grows exponentially number observations
iteration, i.e. size set O(|A||t1 ||Z| ). Since new -vector
requires computation time O(|Z||S|2 ), resulting complexity iteration exact
approaches O(|A||Z||S|2 |t1 ||Z| ). work exact approaches focused
finding efficient ways prune set , effectively reduce computation.
2.2 Offline Approximate Algorithms
Due high complexity exact solving approaches, many researchers worked
improving applicability POMDP approaches developing approximate offline
approaches applied larger problems.
online methods review below, approximate offline algorithms often used
compute lower upper bounds optimal value function. bounds
leveraged orient search promising directions, apply branch-and-bound pruning
techniques, estimate long term reward belief states, show Section
3. However, generally want use approximate methods require low
computational cost. particularly interested approximations use
underlying MDP1 compute lower bounds (Blind policy) upper bounds (MDP, QMDP,
FIB) exact value function. also investigate usefulness using precise
lower bounds provided point-based methods. briefly review offline methods
featured empirical investigation. recent publications provide
comprehensive overview offline approximate algorithms (Hauskrecht, 2000; Pineau,
Gordon, & Thrun, 2006).
2.2.1 Blind policy
Blind policy (Hauskrecht, 2000; Smith & Simmons, 2005) policy
action always executed, regardless belief state. value function Blind
1. MDP defined (S, A, T, R) components POMDP model.

668

fiOnline Planning Algorithms POMDPs

policy obviously lower bound V since corresponds value one specific
policy agent could execute environment. resulting value function
specified set |A| -vectors, -vector specifies long term expected
reward following corresponding blind policy. -vectors computed using
simple update rule:
at+1 (s) = R(s, a) +

X

(s, a, s0 )at (s),

(14)

s0

a0 = minsS R(s, a)/(1). -vectors computed, use Equation 13
obtain lower bound value belief state. complexity iteration
O(|A||S|2 ), far less exact methods. lower bound
computed quickly, usually tight thus informative.
2.2.2 Point-Based Algorithms
obtain tighter lower bounds, one use point-based methods (Lovejoy, 1991; Hauskrecht,
2000; Pineau et al., 2003). popular approach approximates value function updating selected belief states. point-based methods sample belief
states simulating random interactions agent POMDP environment,
update value function gradient sampled beliefs. approaches circumvent complexity exact approaches sampling small set beliefs
maintaining one -vector per sampled belief state. Let B represent set
sampled beliefs, set -vectors time obtained follows:
(s)
a,z

bt


=
=
=
=

R(s, a),
P
a,z
0 , a, z)0 (s0 ), 0
{a,z
s0 (s, a, s0 )O(s
t1 },


|i (s) = P
P



a,z
{b |b = + zZ argmaxt
sS (s)b(s), A},
P
{b |b = argmaxb sS b(s)(s), b B}.

(15)



ensure gives lower bound, 0 initialized single -vector 0 (s) =

mins0 S,aA R(s0 ,a)
.
1

Since |t1 | |B|, iteration complexity O(|A||Z||S||B|(|S|+
|B|)), polynomial time, compared exponential time exact approaches.
Different algorithms developed using point-based approach: PBVI (Pineau
et al., 2003), Perseus (Spaan & Vlassis, 2005), HSVI (Smith & Simmons, 2004, 2005)
recent methods. methods differ slightly choose
belief states update value function chosen belief states.
nice property approaches one tradeoff complexity
algorithm precision lower bound increasing (or decreasing) number
sampled belief points.
2.2.3 MDP
MDP approximation consists approximating value function V POMDP
value function underlying MDP (Littman et al., 1995). value function
upper bound value function POMDP computed using Bellmans
equation:
669

fiRoss, Pineau, Paquet, & Chaib-draa

"

DP
Vt+1
(s) = max R(s, a) +
aA

X

#

(s, a, s0 )VtM DP (s0 ) .

s0

(16)

P
value V (b) belief state b computed V (b) = sS V DP (s)b(s).
computed quickly, iteration Equation 16 done O(|A||S|2 ).
2.2.4 QMDP
QMDP approximation slight variation MDP approximation (Littman et al.,
1995). main idea behind QMDP consider partial observability disappear
single step. assumes MDP solution computed generate VtM DP (Equation
16). Given this, define:
DP
QM
t+1 (s, a) = R(s, a) +

X

(s, a, s0 )VtM DP (s0 ).

(17)

s0

approximation defines -vector action, gives upper bound V
tighter V DP ( i.e. VtQM DP (b) VtM DP (b) belief b). Again,
obtain value belief state, use Equation 13, contain one -vector
DP (s, a) A.
(s) = QM

2.2.5 FIB
two upper bounds presented far, QMDP MDP, take account
partial observability environment. particular, information-gathering actions
may help identify current state always suboptimal according bounds.
address problem, Hauskrecht (2000) proposed new method compute upper bounds,
called Fast Informed Bound (FIB), able take account (to degree)
partial observability environment. -vector update process described
follows:
at+1 (s) = R(s, a) +

X

zZ

a0

max



X

O(s0 , a, z)T (s, a, s0 )t (s0 ).

(18)

s0

initialized -vectors found QMDP convergence, i.e.
-vectors
a0 (s) = QM DP (s, a). FIB defines single -vector action value belief
state computed according Equation 13. FIB provides tighter upper bound
QMDP ( i.e. VtF IB (b) VtQM DP (b) b ). complexity algorithm remains
acceptable, iteration requires O(|A|2 |S|2 |Z|) operations.

3. Online Algorithms POMDPs
offline approaches, algorithm returns policy defining action execute
every possible belief state. approaches tend applicable dealing
small mid-size domains, since policy construction step takes significant time. large
POMDPs, using rough value function approximation (such ones presented
Section 2.2) tends substantially hinder performance resulting approximate
670

fiOnline Planning Algorithms POMDPs

Offline Approaches
Policy Construction

Policy Execution

Online Approaches

Small policy construction step policy execution steps

Figure 1: Comparison offline online approaches.
policy. Even recent point-based methods produce solutions limited quality
large domains (Paquet et al., 2006).
Hence large POMDPs, potentially better alternative use online approach,
tries find good local policy current belief state agent.
advantage approach needs consider belief states reachable current belief state. focuses computation small set beliefs.
addition, since online planning done every step (and thus generalization beliefs
required), sufficient calculate maximal value current belief
state, full optimal -vector. setting, policy construction steps
execution steps interleaved one another shown Figure 1. cases, online
approaches may require extra execution steps (and online planning), since policy
locally constructed therefore always optimal. However policy construction time
often substantially shorter. Consequently, overall time policy construction
execution normally less online approaches (Koenig, 2001). practice, potential
limitation online planning need meet short real-time constraints.
case, time available construct plan small compared offline algorithms.
3.1 General Framework Online Planning
subsection presents general framework online planning algorithms POMDPs.
Subsequently, discuss specific approaches literature describe vary
tackling various aspects general framework.
online algorithm divided planning phase, execution phase,
applied alternately time step.
planning phase, algorithm given current belief state agent
computes best action execute belief. usually achieved two steps.
First tree reachable belief states current belief state built looking
several possible sequences actions observations taken current
belief. tree, current belief root node subsequent reachable beliefs (as
calculated (b, a, z) function Equation 3) added tree child nodes
immediate previous belief. Belief nodes represented using OR-nodes (at
must choose action) actions included layer belief nodes using
AND-nodes (at must consider possible observations lead subsequent
671

fiRoss, Pineau, Paquet, & Chaib-draa

b0

[14.4, 18.7]

1

3

[14.4, 17.9]

[12, 18.7]

a1

a2

0.7

z1
[13.7, 16.9]

b1

[15, 20]

b2

z1
[6, 14] b
5

b3

0.5

z2
[9, 15]

b4

[10, 18]

4

a1
0.6

z1

z2

-1

[5.8, 11.5]

0.5

0.3

a2
0.2

0.4

z2
b6

[9, 12]

z1

[13.7, 16.9]
0.8

[11, 20]

z2
b8

b7

[10, 12]

Figure 2: AND-OR tree constructed search process POMDP 2 actions 2 observations. belief states OR-nodes represented triangular nodes action AND-nodes
circular nodes. rewards RB (b, a) represented values outgoing arcs
OR-nodes probabilities Pr(z|b, a) shown outgoing arcs AND-nodes.
values inside brackets represent lower upper bounds computed according
Equations 19 - 22, assuming discount factor = 0.95. Also notice example
action a1 belief state b1 could pruned since upper bound (= 11.5) lower
lower bound (= 13.7) action a2 b1 .

beliefs). value current belief estimated propagating value estimates
fringe nodes, ancestors, way root, according Bellmans
equation (Equation 8). long-term value belief nodes fringe usually estimated
using approximate value function computed offline. methods also maintain
lower bound upper bound value node. example
tree contructed evaluated presented Figure 2.
planning phase terminates, execution phase proceeds executing best
action found current belief environment, updating current belief
tree according observation obtained.
Notice general, belief MDP could graph structure cycles.
online algorithms handle structure unrolling graph tree. Hence,
reach belief already elsewhere tree, duplicated. algorithms could
always modified handle generic graph structures using technique proposed
LAO* algorithm (Hansen & Zilberstein, 2001) handle cycles. However
advantages disadvantages this. in-depth discussion issue
presented Section 5.4.
generic online algorithm implementing planning phase (lines 5-9) execution
phase (lines 10-13) presented Algorithm 3.1. algorithm first initializes tree
contain initial belief state (line 2). given current tree, planning phase
algorithm proceeds first selecting next fringe node (line 6)
pursue search (construction tree). Expand function (line 7) constructs
672

fiOnline Planning Algorithms POMDPs

1: Function OnlinePOMDPSolver()
Static: bc : current belief state agent.
: AND-OR tree representing current search tree.
D: Expansion depth.
L: lower bound V .
U : upper bound V .

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

bc b0
Initialize contain bc root
ExecutionTerminated()
PlanningTerminated()
b ChooseNextNodeToExpand()
Expand(b , D)
UpdateAncestors(b )
end
Execute best action bc
Perceive new observation z
bc (bc , a, z)
Update tree bc new root
end

Algorithm 3.1: Generic Online Algorithm.
next reachable beliefs (using Equation 3) selected leaf pre-determined
expansion depth evaluates approximate value function newly created
nodes. new approximate value expanded node propagated ancestors
via UpdateAncestors function (line 8). planning phase conducted
terminating condition met (e.g. planning time available -optimal action
found).
execution phase algorithm executes best action found planning
(line 10) gets new observation environment (line 11). Next, algorithm
updates current belief state search tree according recent action
observation z (lines 12-13). online approaches reuse previous computations
keeping subtree new belief resuming search subtree
next time step. cases, algorithm keeps nodes tree
new belief bc deletes nodes tree. algorithm loops back
planning phase next time step, task terminated.
side note, online planning algorithm also useful improve precision
approximate value function computed offline. captured Theorem 3.1.
Theorem 3.1. (Puterman, 1994; Hauskrecht, 2000) Let V approximate value function = supb |V (b) V (b)|. approximate value V (b) returned Dstep lookahead belief b, using V estimate fringe node values, error bounded
|V (b) V (b)| .
notice [0, 1), error converges 0 depth search tends
. indicates online algorithm effectively improve performance obtained
approximate value function computed offline, find action arbitrarily close
optimal current belief. However, evaluating tree reachable beliefs
within depth complexity O((|A||Z|)D |S|2 ), exponential D.
becomes quickly intractable large D. Furthermore, planning time available
execution may short exploring beliefs depth may infeasible.
673

fiRoss, Pineau, Paquet, & Chaib-draa

Hence motivates need efficient online algorithms guarantee similar
better error bounds.
efficient, online algorithms focus limiting number reachable beliefs explored tree (or choose relevant ones). approaches
generally differ subroutines ChooseNextNodeToExpand Expand
implemented. classify approaches three categories : Branch-and-Bound
Pruning, Monte Carlo Sampling Heuristic Search. present survey
approaches discuss strengths drawbacks. online algorithms
proceed via tree search; approaches discussed Section 3.5.
3.2 Branch-and-Bound Pruning
Branch-and-Bound pruning general search technique used prune nodes
known suboptimal search tree, thus preventing expansion unnecessary
lower nodes. achieve AND-OR tree, lower bound upper bound
maintained value Q (b, a) action a, every belief b tree.
bounds computed first evaluating lower upper bound fringe nodes
tree. bounds propagated parent nodes according following
equations:

L(b),
b F(T )
LT (b) =
(19)
maxaA LT (b, a), otherwise
X
LT (b, a) = RB (b, a) +
Pr(z|b, a)LT ( (b, a, z)),
(20)
zZ



U (b),
b F(T )
maxaA UT (b, a), otherwise
X
UT (b, a) = RB (b, a) +
Pr(z|b, a)UT ( (b, a, z)),
UT (b) =

(21)
(22)

zZ

F(T ) denotes set fringe nodes tree , UT (b) LT (b) represent upper
lower bounds V (b) associated belief state b tree , UT (b, a) LT (b, a)
represent corresponding bounds Q (b, a), L(b) U (b) bounds used fringe
nodes, typically computed offline. equations equivalent Bellmans equation
(Equation 8), however use lower upper bounds children, instead V .
Several techniques presented Section 2.2 used quickly compute lower bounds
(Blind policy) upper bounds (MDP, QMDP, FIB) offline.
Given bounds, idea behind Branch-and-Bound pruning relatively simple:
given action belief b upper bound UT (b, a) lower another action
lower bound LT (b, a), know guaranteed value Q (b, a) Q (b, a).
Thus suboptimal belief b. Hence branch pruned belief reached
taking action b considered.
3.2.1 RTBSS
Real-Time Belief Space Search (RTBSS) algorithm uses Branch-and-Bound approach
compute best action take current belief (Paquet et al., 2005, 2006). Starting
674

fiOnline Planning Algorithms POMDPs

1: Function Expand(b, d)
Inputs: b:
d:
Static: :
L:
U:

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

belief node want expand.
depth expansion b.
AND-OR tree representing current search tree.
lower bound V .
upper bound V .

= 0
LT (b) L(b)
else
Sort actions {a1 , a2 , . . . , a|A| } U (b, ai ) U (b, aj ) j
i1
LT (b)
|A| U (b, ai ) >
PLT (b)
LT (b, ai ) RB (b, ai ) + zZ Pr(z|b, ai )Expand( (b, ai , z), 1)
LT (b) max{LT (b), LT (b, ai )}
ii+1
end
end
return LT (b)

Algorithm 3.2: Expand subroutine RTBSS.
current belief, expands AND-OR tree depth-first search fashion,
pre-determined search depth D. leaves tree evaluated using lower
bound computed offline, propagated upwards lower bound maintained
node tree.
limit number nodes explored, Branch-and-Bound pruning used along way
prune actions known suboptimal, thus excluding unnecessary nodes
actions. maximize pruning, RTBSS expands actions descending order
upper bound (first action expanded one highest upper bound). expanding
actions order, one never expands action could pruned actions
expanded different order. Intuitively, action higher upper bound
actions, cannot pruned actions since lower
bound never exceed upper bound. Another advantage expanding actions
descending order upper bound soon find action
pruned, also know remaining actions pruned, since upper
bounds necessarily lower. fact RTBSS proceeds via depth-first search also
increases number actions pruned since bounds expanded actions
become precise due search depth.
terms framework Algorithm 3.1, RTBSS requires ChooseNextNodeToExpand subroutine simply return current belief bc . UpdateAncestors function need perform operation since bc ancestor (root tree
). Expand subroutine proceeds via depth-first search fixed depth D, using
Branch-and-Bound pruning, mentioned above. subroutine detailed Algorithm
3.2. expansion performed, PlanningTerminated evaluates true
best action found executed. end time step, tree simply reinitialized
contain new current belief root node.
efficiency RTBSS depends largely precision lower upper bounds
computed offline. bounds tight, pruning possible, search
efficient. algorithm unable prune many actions, searching
675

fiRoss, Pineau, Paquet, & Chaib-draa

limited short horizons order meet real-time constraints. Another drawback
RTBSS explores observations equally. inefficient since algorithm
could explore parts tree small probability occurring thus
small effect value function. result, number observations large,
algorithm limited exploring short horizon.
final note, since RTBSS explores reacheable beliefs within depth (except
reached suboptimal actions), guarantee error bound D-step
lookahead (see Theorem 3.1). Therefore, online search directly improves precision
original (offline) value bounds factor . aspect confirmed empirically
different domains RTBSS authors combined online search bounds given
various offline algorithms. cases, results showed tremendous improvement
policy given offline algorithm (Paquet et al., 2006).
3.3 Monte Carlo Sampling
mentioned above, expanding search tree fully large set observations
infeasible except shallow depths. cases, better alternative may sample
subset observations expansion consider beliefs reached sampled
observations. reduces branching factor search allows deeper search
within set planning time. strategy employed Monte Carlo algorithms.
3.3.1 McAllester Singh
approach presented McAllester Singh (1999) adaptation online MDP
algorithm presented Kearns, Mansour, Ng (1999). consists depth-limited
search AND-OR tree certain fixed horizon instead exploring
observations action choice, C observations sampled generative model.
probabilities Pr(z|b, a) approximated using observed frequencies sample.
advantage approach sampling observation distribution
Pr(z|b, a) achieved efficiently O(log |S| + log |Z|), computing exact
probabilities Pr(z|b, a) O(|S|2 ) observation z. Thus sampling useful
alleviate complexity computing Pr(z|b, a), expense less precise estimate.
Nevertheless, samples often sufficient obtain good estimate observations
effect Q (b, a) (i.e. occur high probability)
likely sampled. authors also apply belief state factorization Boyen
Koller (1998) simplify belief state calculations.
implementation algorithm, Expand subroutine expands tree
fixed depth D, using Monte Carlo sampling observations, mentioned (see
Algorithm 3.3). end time step, tree reinitialized contain
new current belief root.
Kearns et al. (1999) derive bounds depth number samples C needed
obtain -optimal policy high probability show number samples
required grows exponentially desired accuracy. practice, number samples
required infeasible given realistic online time constraints. However, performance terms
returns usually good even many fewer samples.
676

fiOnline Planning Algorithms POMDPs

1: Function Expand(b, d)
Inputs: b:
d:
Static: :
C:

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

belief node want expand.
depth expansion b.
AND-OR tree representing current search tree.
number observations sample.

= 0
LT (b) maxaA RB (b, a)
else
LT (b)

Sample Z = {z1 , z2 , . . . zC } distribution Pr(z|b, a)
P
N (Z)
LT (b, a) RB (b, a) + zZ|Nz (Z)>0 zC Expand( (b, a, z), 1)
LT (b) max{LT (b), LT (b, a)}
end
end
return LT (b)

Algorithm 3.3: Expand subroutine McAllester Singhs Algorithm.

One inconvenience method action pruning done since Monte
Carlo estimation guaranteed correctly propagate lower (and upper) bound
property tree. article, authors simply approximate value
fringe belief states immediate reward RB (b, a); could improved using
good estimate V computed offline. Note also approach may difficult
apply domains number actions |A| large. course may
impact performance.
3.3.2 Rollout
Another similar online Monte Carlo approach Rollout algorithm (Bertsekas & Castanon, 1999). algorithm requires initial policy (possibly computed offline).
time step, estimates future expected value action, assuming initial policy followed future time steps, executes action highest estimated value.
estimates obtained computing average discounted return obtained
set sampled trajectories depth D. trajectories generated first taking
action evaluated, following initial policy subsequent belief states,
assuming observations sampled generative model. Since approach
needs consider different actions root belief node, number actions |A|
influences branching factor first level tree. Consequently, generally
scalable McAllester Singhs approach. Bertsekas Castanon (1999) also
show enough sampling, resulting policy guaranteed perform least
well initial policy high probability. However, generally requires many sampled
trajectories provide substantial improvement initial policy. Furthermore,
initial policy significant impact performance approach. particular,
cases might impossible improve return initial policy changing
immediate action (e.g. several steps need changed reach specific subgoal
higher rewards associated). cases, Rollout policy never improve
initial policy.
677

fiRoss, Pineau, Paquet, & Chaib-draa

1: Function Expand(b, d)
Inputs: b: belief node want expand.
d: depth expansion b.
Static: : AND-OR tree representing current search tree.
: set initial policies.
: number trajectories depth sample.

2: LT (b)
3:
4:
5:
Q (b, a) 0
6:
= 1
7:
b b
8:

9:
j = 0
1 j
10:
Q (b, a) Q (b, a) +
RB (b, a)
11:
z SampleObservation(b, a)
12:
b (b, a, z)
13:
(b)
14:
end
15:
end
16: end
17: LT (b, a) = max Q (b, a)
18: end

Algorithm 3.4: Expand subroutine Parallel Rollout Algorithm.
address issue relative initial policy, Chang, Givan, Chong (2004)
introduced modified version algorithm, called Parallel Rollout. case,
algorithm starts set initial policies. algorithm proceeds Rollout
initial policies set. value considered immediate action
maximum set initial policies, action highest value executed.
algorithm, policy obtained guaranteed perform least well best
initial policy high probability, given enough samples. Parallel Rollout handle
domains large number actions observations, perform well
set initial policies contain policies good different regions belief space.
Expand subroutine Parallel Rollout algorithm presented Algorithm 3.4.
original Rollout algorithm Bertsekas Castanon (1999) algorithm
special case set initial policies contains one policy.
subroutines proceed McAllester Singhs algorithm.
3.4 Heuristic Search
Instead using Branch-and-Bound pruning Monte Carlo sampling reduce branching factor search, heuristic search algorithms try focus search relevant reachable beliefs using heuristics select best fringe beliefs node expand.
relevant reachable beliefs ones would allow search algorithm
make good decisions quickly possible, i.e. expanding nodes possible.
three different online heuristic search algorithms POMDPs
proposed past: Satia Lave (1973), BI-POMDP (Washington, 1997) AEMS
(Ross & Chaib-draa, 2007). algorithms maintain lower upper bounds
value node tree (using Equations 19 - 22) differ
specific heuristic used choose next fringe node expand AND/OR tree.
678

fiOnline Planning Algorithms POMDPs

first present common subroutines algorithms, discuss different
heuristics.
Recalling general framework Algorithm 3.1, three steps interleaved several
times heuristic search algorithms. First, best fringe node expand (according
heuristic) current search tree found. tree expanded
node (usually one level). Finally, ancestor nodes values updated;
values must updated choose next node expand, since heuristic
value usually depends them. general, heuristic search algorithms slightly
computationally expensive standard depth- breadth-first search algorithms, due
extra computations needed select best fringe node expand, need
update ancestors iteration. required previous methods using
Branch-and-Bound pruning and/or Monte Carlo sampling. complexity extra
steps high, benefit expanding relevant nodes might
outweighed lower number nodes expanded (assuming fixed planning time).
heuristic search algorithms, particular heuristic value associated every fringe
node tree. value indicate important expand node
order improve current solution. iteration algorithm, goal find
fringe node maximizes heuristic value among fringe nodes.
achieved efficiently storing node tree reference best fringe node
expand within subtree, well associated heuristic value. particular,
root node always contains reference best fringe node whole tree.
node expanded, ancestors nodes tree best fringe node
reference, corresponding heuristic value, need updated. updated
efficiently using references heuristic values stored lower nodes via
dynamic programming algorithm, described formally Equations 23 24. HT (b)
denotes highest heuristic value among fringe nodes subtree b, bT (b)
reference fringe node, HT (b) basic heuristic value associated fringe node b,
HT (b, a) HT (b, a, z) factors weigh basic heuristic value level
tree . example, HT (b, a, z) could Pr(z|b, a) order give higher weight
(and hence favor) fringe nodes reached likely observations.

HT (b)
b F(T )

HT (b) =

maxaA HT (b, a)HT (b, a) otherwise
(23)
HT (b, a) = maxzZ HT (b, a, z)HT ( (b, a, z))

b
b F(T )

bT (b) =
bT (b, aTb ) otherwise
))
bT (b, a) = bT ( (b, a, zb,a
(24)

ab = argmaxaA HT (b, a)HT (b, a)

zb,a
= argmaxzZ HT (b, a, z)HT ( (b, a, z))
procedure finds fringe node b F(T ) maximizes overall heuristic value
QdT (b)
HT (bi , ai )HT (bi , ai , zi ), bi , ai zi represent ith belief,
HT (bc , b) = HT (b) i=1
action observation path bc b , dT (b) depth fringe
node b. Note HT bT updated ancestor nodes last expanded
node. reusing previously computed values nodes, procedure
679

fiRoss, Pineau, Paquet, & Chaib-draa

1: Function Expand(b)
Inputs: b:
Static: bc :
T:
L:
U:

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

OR-Node want expand.
current belief state agent.
AND-OR tree representing current search tree.
lower bound V .
upper bound V .


z Z
b0 (b, a, z)
UT (b0 ) U (b0 )
LT (b0 ) L(b0 )
HT (b0 ) HT (b0 )
bT (b0 ) b0
end
P
LT (b, a) RB (b, a) + P zZ Pr(z|b, a)LT ( (b, a, z))
UT (b, a) RB (b, a) + zZ Pr(z|b, a)UT ( (b, a, z))

argmax
zb,a
zZ HT (b, a, z)HT ( (b, a, z))

))


HT (b, a) = HT (b, a, zb,a )HT ( (b, a, zb,a



bT (b, a) bT ( (b, a, zb,a ))
end
LT (b) max{maxaA LT (b, a), LT (b)}
UT (b) min{maxaA UT (b, a), UT (b)}


b argmaxaA HT (b, a)HT (b, a)
(b, )
)H
HT (b) HT (b,
b

b
bT (b) bT (b,
b )

Algorithm 3.5: Expand : Expand subroutine heuristic search algorithms.
find best fringe node expand tree time linear depth
tree (versus exponential depth tree exhaustive search fringe
nodes). updates performed Expand UpdateAncestors
subroutines, described detail below. iteration,
ChooseNextNodeToExpand subroutine simply returns reference best fringe
node stored root tree, i.e. bT (bc ).
Expand subroutine used heuristic search methods presented Algorithm 3.5.
performs one-step lookahead fringe node b. main difference respect
previous methods Sections 3.2 3.3 heuristic value best fringe node
expand new nodes computed lines 7-8 12-14. best leaf node
bs subtree heuristic value computed according Equations 23 24
(lines 18-20).
UpdateAncestors function presented Algorithm 3.6. goal function update bounds ancestor nodes, find best fringe node expand
next. Starting given OR-Node b0 , function simply updates recursively ancestor nodes b0 bottom-up fashion, using Equations 19-22 update bounds
Equations 23-24 update reference best fringe expand heuristic value.
Notice UpdateAncestors function reuse information already stored
node objects, need recompute (b, a, z), Pr(z|b, a) RB (b, a).
However may need recompute HT (b, a, z) HT (b, a) according new bounds,
depending heuristic defined.
Due anytime nature heuristic search algorithms, search usually keeps
going -optimal action found current belief bc , available planning
680

fiOnline Planning Algorithms POMDPs

1: Function UpdateAncestors(b0 )

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

Inputs: b0 : OR-Node want update ancestors.
Static: bc : current belief state agent.
: AND-OR tree representing current search tree.
L: lower bound V .
U : upper bound V .
b0 6= bc
Set (b, a) action aPin belief b parent node belief node b0
LT (b, a) RB (b, a) + P zZ Pr(z|b, a)LT ( (b, a, z))
UT (b, a) RB (b, a) + zZ Pr(z|b, a)UT ( (b, a, z))

argmax
zb,a
zZ HT (b, a, z)HT ( (b, a, z))
)H ( (b, a, z ))
HT (b, a) HT (b, a, zb,a

b,a
))
bT (b, a) bT ( (b, a, zb,a
LT (b) maxa0 LT (b, a0 )
UT (b) maxa0 UT (b, a0 )
0
0


b argmaxa0 HT (b, )HT (b, )
(b, )
)H
HT (b) HT (b,

b
b
bT (b) bT (b,
b )
b0 b
end

Algorithm 3.6: UpdateAncestors : Updates bounds ancestors ancestors
OR-Node
time elapsed. -optimal action found whenever UT (bc ) LT (bc ) LT (bc )
UT (bc , a0 ), a0 6= argmaxaA LT (bc , a) (i.e. actions pruned, case
optimal action found).
covered basic subroutines, present different heuristics
proposed Satia Lave (1973), Washington (1997) Ross Chaib-draa (2007).
begin introducing useful notation.
Given graph structure G, let us denote F(G) set fringe nodes G
HG (b, b0 ) set sequences actions observations lead belief node b0
belief node b search graph G. tree , HT (b, b0 ) contain
0
0
single sequence denote hb,b
. given sequence h HG (b, b ), define
Pr(hz |b, ha ) probability observe whole sequence observations hz h, given
start belief node b perform whole sequence actions ha h. Finally,
define Pr(h|b, ) probability follow entire action/observation sequence
h start belief b behave according policy . Formally, probabilities
computed follows:
d(h)

Pr(hz |b, ha ) =



Pr(hiz |bhi1 , hia ),

(25)

i=1

d(h)

Pr(h|b, ) =



Pr(hiz |bhi1 , hia )(bhi1 , hia ),

(26)

i=1

d(h) represents depth h (number actions sequence h), hia denotes
ith action sequence h, hiz ith observation sequence h, bhi belief state
obtained taking first actions observations sequence h b. Note
bh0 = b.
681

fiRoss, Pineau, Paquet, & Chaib-draa

3.4.1 Satia Lave
approach Satia Lave (1973) follows heuristic search framework presented
above. main feature approach explore, iteration, fringe node b
current search tree maximizes following term:
bc ,b

HT (bc , b) = d(hT

)

c ,b
c ,b
Pr(hbT,z
|bc , hbT,a
)(UT (b) LT (b)),

(27)

b F(T ) bc root node . intuition behind heuristic simple:
recalling definition V , note weight value V (b) fringe node b
bc ,b
c ,b
c ,b
c ,b
sequence optimal
), provided hbT,a
|bc , hbT,a
V (bc ) would exactly d(hT ) Pr(hbT,z
actions. fringe nodes weight high effect estimate
V (bc ). Hence one try minimize error nodes first. term
UT (b) LT (b) included since upper bound (unknown) error V (b) LT (b).
Thus heuristic focuses search areas tree affect value V (bc )
error possibly large. approach also uses Branch-and-Bound pruning,
fringe node reached action dominated parent belief b
never going expanded. Using notation Algorithms 3.5 3.6,
heuristic implemented defining HT (b), HT (b, a) HT (b, a, z), follows:
HT (b) =
UT (b) LT (b),
1 UT (b, a) > LT (b),
HT (b, a) =
0 otherwise,
HT (b, a, z) = Pr(z|b, a),

(28)

condition UT (b, a) > LT (b) ensures global heuristic value HT (bc , b0 ) = 0
bc ,b0
dominated (pruned). guarantees fringe
action sequence hT,a
nodes never expanded.
Satia Laves heuristic focuses search towards beliefs likely
reached future, error large. heuristic likely efficient
domains large number observations, probability distribution
observations concentrated observations. term UT (b) LT (b)
heuristic also prevents search unnecessary computations areas tree
already good estimate value function. term efficient
bounds computed offline, U L, sufficiently informative. Similarly, node pruning
going efficient U L sufficiently tight, otherwise actions
pruned.
3.4.2 BI-POMDP
Washington (1997) proposed slightly different approach inspired AO algorithm
(Nilsson, 1980), search conducted best solution graph. case
online POMDPs, corresponds subtree belief nodes reached
sequences actions maximizing upper bound parent beliefs.
b
set fringe nodes best solution graph G, denote F(G),

b
defined formally F(G) = {b F(G)|h HG (bc , b), Pr(h|b, G ) > 0}, G (b, a) = 1
= argmaxa0 UG (b, a0 ) G (b, a) = 0 otherwise. AO algorithm simply specifies
682

fiOnline Planning Algorithms POMDPs

expanding fringe nodes. Washington (1997) recommends exploring fringe
node Fb(G) (where G current acyclic search graph) maximizes UG (b) LG (b).
Washingtons heuristic implemented defining HT (b), HT (b, a) HT (b, a, z),
follows:
HT (b) =
UT (b) LT (b),
1 = argmaxa0 UT (b, a0 ),
HT (b, a) =
0 otherwise,
HT (b, a, z) = 1.

(29)

heuristic tries guide search towards nodes reachable promising
actions, especially loose bounds values (possibly large error). One
nice property approach expanding fringe nodes best solution graph
way reduce upper bound root node bc . case
Satia Laves heuristic. However, Washingtons heuristic take account
probability Pr(hz |b, ha ), discount factor d(h) , may end exploring
nodes small probability reached future, thus
little effect value V (bc ). Hence, may explore relevant nodes
optimizing decision bc . heuristic appropriate upper bound U
computed offline sufficiently informative, actions highest upper bound
would also usually tend highest Q-value. cases, algorithm focus
search actions thus find optimal action quickly
explored actions equally. hand, consider observation
probabilities, approach may scale well large observation sets,
able focus search towards relevant observations.
3.4.3 AEMS
Ross Chaib-draa (2007) introduced heuristic combines advantages BIPOMDP, Satia Laves heuristic. based theoretical error analysis tree
search POMDPs, presented Ross et al. (2008).
core idea expand tree reduce error V (bc ) quickly
possible. achieved expanding fringe node b contributes
error V (bc ). exact error contribution eT (bc , b) fringe node b bc tree
defined following equation:
bc ,b

eT (bc , b) = d(hT

)

Pr(hbTc ,b |bc , )(V (b) LT (b)).

(30)

expression requires V computed exactly. practice, Ross Chaibdraa (2007) suggest approximating exact error (V (b) LT (b)) (UT (b) LT (b)),
done Satia Lave, Washington. also suggest approximating
policy , (b, a) represents probability action optimal
parent belief b, given lower upper bounds tree . particular, Ross et al. (2008)
considered two possible approximations . first one based uniformity
assumption distribution Q-values lower upper bounds,
yields:
683

fiRoss, Pineau, Paquet, & Chaib-draa

(b, a) =

(

2

(b,a)LT (b))
(U
UT (b,a)LT (b,a)
0

UT (b, a) > LT (b),
otherwise,

(31)

normalization constant sum probabilities (b, a)
actions equals 1.
second inspired AO BI-POMDP, assumes action maximizing
upper bound fact optimal action:

1 = argmaxa0 UT (b, a0 ),
(32)
(b, a) =
0 otherwise.
Given approximation , AEMS heuristic explore fringe node b
maximizes:
bc ,b
)

HT (bc , b) = d(hT

Pr(hbTc ,b |bc , )(UT (b) LT (b)).

(33)

implemented defining HT (b), HT (b, a) HT (b, a, z) follows:
HT (b) = UT (b) LT (b),
HT (b, a) = (b, a),
HT (b, a, z) = Pr(z|b, a).

(34)

refer heuristic AEMS1 defined Equation 31, AEMS2
defined Equation 32.2
Let us examine AEMS combines advantages Satia Lave,
BI-POMDP heuristics. First, AEMS encourages exploration nodes loose bounds
possibly large error considering term UT (b) LT (b) previous heuristics.
Moreover, Satia Lave, focuses exploration towards belief states likely
encountered future. good two reasons. mentioned before, belief
state low probability occurrence future, limited effect value
V (bc ) thus necessary know value precisely. Second, exploring highly
probable belief states increases chance able reuse computations
future. Hence, AEMS able deal efficiently large observation sets,
assuming distribution observations concentrated observations. Finally,
BI-POMDP, AEMS favors exploration fringe nodes reachable actions
seem likely optimal (according ). useful handle large action
sets, focuses search actions look promising. promising actions
optimal, quickly become apparent. work well best
actions highest probabilities . Furthermore, possible define
automatically prunes dominated actions ensuring (b, a) = 0 whenever
UT (b, a) < LT (b). cases, heuristic never choose expand fringe node
reached dominated action.
final note, Ross et al. (2008) determined sufficient conditions
search algorithm using heuristic guaranteed find -optimal action within finite
time. stated Theorem 3.2.
2. AEMS2 heuristic also used policy search algorithm Hansen (1998).

684

fiOnline Planning Algorithms POMDPs

Theorem 3.2. (Ross et al., 2008) Let > 0 bc current belief. tree
parent belief b UT (b) LT (b) > , (b, a) > 0 = argmaxa0 UT (b, a0 ),
AEMS algorithm guaranteed find -optimal action bc within finite time.
observe theorem possible define many different policies
AEMS heuristic guaranteed converge. AEMS1 AEMS2
satisfy condition.
3.4.4 HSVI
heuristic similar AEMS2 also used Smith Simmons (2004) offline
value iteration algorithm HSVI way pick next belief point perform
-vector backups. main difference HSVI proceeds via greedy search
descends tree root node b0 , going towards action maximizes
upper bound observation maximizes Pr(z|b, a)(U ( (b, a, z))L( (b, a, z)))
level, reaches belief b depth (U (b) L(b)) < .
heuristic could used online heuristic search algorithm instead stopping
greedy search process reaches fringe node tree selecting node
one expanded next. setting, HSVIs heuristic would return greedy
approximation AEMS2 heuristic, may find fringe node actually
bc ,b
maximizes d(hT ) Pr(hbTc ,b |bc , )(UT (b) LT (b)). consider online version
HSVI heuristic empirical study (Section 4). refer extension HSVI-BFS.
Note complexity greedy search finding best fringe node
via dynamic programming process updates HT bT UpdateAncestors
subroutine.
3.5 Alternatives Tree Search
present two alternative online approaches proceed via lookahead
search belief MDP. online approaches presented far, one problem
learning achieved time, i.e. everytime agent encounters belief,
recompute policy starting initial upper lower bounds computed offline.
two online approaches presented next address problem presenting alternative
ways updating initial value functions computed offline performance
agent improves time stores updated values computed time step.
However, argued discussion (Section 5.2), techniques lead
disadvantages terms memory consumption and/or time complexity.
3.5.1 RTDP-BEL
alternative approach searching AND-OR graphs RTDP algorithm (Barto
et al., 1995) adapted solve POMDPs Geffner Bonet (1998).
algorithm, called RTDP-BEL, learns approximate values belief states visited
successive trials environment. belief state visited, agent evaluates
possible actions estimating expected reward taking action current belief
685

fiRoss, Pineau, Paquet, & Chaib-draa

1: Function OnlinePOMDPSolver()
Static: bc : current belief state agent.
V0 : Initial approximate value function (computed offline).
V : hashtable beliefs approximate value.
k: Discretization resolution.

2: Initialize bc initial belief state V empty hashtable.
3: ExecutionTerminated()
P
4: A: Evaluate Q(bc , a) = RB (b, a) + zZ Pr(z|b, a)V (Discretize( (b, a, z), k))
5: argmaxaA Q(bc , a)
6: Execute best action bc
7: V (Discretize(bc , k)) Q(bc , a)
8: Perceive new observation z
9: bc (bc , a, z)
10: end

Algorithm 3.7: RTDP-Bel Algorithm.
state b approximate Q-value equation:
X
Q(b, a) = RB (b, a) +
Pr(z|b, a)V ( (b, a, z)),

(35)

zZ

V (b) value learned belief b.
belief state b value table, initialized heuristic value.
authors suggest using MDP approximation initial value belief state.
agent executes action returned greatest Q(b, a) value. Afterwards,
value V (b) table updated Q(b, a) value best action. Finally,
agent executes chosen action makes new observation, ending new
belief state. process repeated new belief.
RTDP-BEL algorithm learns heuristic value belief state visited.
maintain estimated value belief state memory, needs discretize
belief state space finite number belief states. also allows generalization
value function unseen belief states. However, might difficult find best
discretization given problem. practice, algorithm needs substantial amounts
memory (greater 1GB cases) store learned belief state values,
especially POMDPs large state spaces. implementation RTDP-Bel
algorithm presented Algorithm 3.7.
function Discretize(b, k) returns discretized belief b0 b0 (s) = round(kb(s))/k
states S, V (b) looks value belief b hashtable. b present
hashtable, value V0 (b) returned V . Supported experimental data, Geffner
Bonet (1998) suggest choosing k [10, 100], usually produces best results.
Notice discretization resolution k O((k + 1)|S| ) possible discretized
beliefs. implies memory storage required maintain V exponential |S|,
becomes quickly intractable, even mid-size problems. Furthermore, learning good
estimates exponentially large number beliefs usually requires large number
trials, might infeasible practice. technique sometimes applied
large domains factorized representation available. cases, belief
maintained set distributions (one subset conditionaly independent state
variables) discretization applied seperately distribution. greatly
reduce possible number discretized beliefs.
686

fiOnline Planning Algorithms POMDPs

Algorithm
RTBSS
McAllester
Rollout
Satia Lave
Washington
AEMS
HSVI-BFS
RTDP-Bel
SOVI

-optimal
yes
high probability

yes
acyclic graph
yes
yes

yes

Anytime



yes
yes
yes
yes

yes

Branch &
Bound
yes


yes
implicit
implicit
implicit



Monte
Carlo

yes
yes







Heuristic



yes
yes
yes
yes



Learning







yes
yes

Table 1: Properties various online methods.

3.5.2 SOVI
recent online approach, called SOVI (Shani et al., 2005), extends HSVI (Smith &
Simmons, 2004, 2005) online value iteration algorithm. approach maintains
priority queue belief states encountered execution proceeds
-vector updates current belief state k belief states highest priority
time step. priority belief state computed according much value
function changed successor belief states, since last time updated. authors
also propose improvements HSVI algorithm improve scalability,
efficient -vector pruning technique, avoiding use linear programs update
evaluate upper bound. main drawback approach hardly applicable
large environments short real-time constraints, since needs perform value
iteration update -vectors online, high complexity number
-vectors representing value function increases (i.e. O(k|S||A||Z|(|S| + |t1 |))
compute ).
3.6 Summary Online POMDP Algorithms
summary, see online POMDP approaches based lookahead search.
improve scalability, different techniques used: branch-and-bound pruning, search
heuristics, Monte Carlo sampling. techniques reduce complexity different angles. Branch-and-bound pruning lowers complexity related action space
size. Monte Carlo sampling used lower complexity related observation space size, could also potentially used reduce complexity related
action space size (by sampling subset actions). Search heuristics lower complexity
related actions observations orienting search towards relevant actions observations. appropriate, factored POMDP representations used
reduce complexity related state. summary different properties
online algorithm presented Table 1.
687

fiRoss, Pineau, Paquet, & Chaib-draa

4. Empirical Study
section, compare several online approaches two domains found POMDP
literature: Tag (Pineau et al., 2003) RockSample (Smith & Simmons, 2004). consider modified version RockSample, called FieldVisionRockSample (Ross & Chaib-draa,
2007), higher observation space original RockSample. environment
introduced means test compare different algorithms environments
large observation spaces.
4.1 Methodology
environment, first compare real-time performance different heuristics
presented Section 3.4 limiting planning time 1 second per action. heuristics
given lower upper bounds results would comparable.
objective evaluate search heuristic efficient different types
environments. end, implemented different search heuristics (Satia
Lave, BI-POMDP, HSVI-BFS AEMS) best-first search algorithm,
directly measure efficiency heuristic itself. Results also obtained
different lower bounds (Blind PBVI) verify choice affects heuristics
efficiency. Finally, compare online offline times affect performance
approach. Except stated otherwise, experiments run Intel Xeon 2.4
Ghz 4GB RAM; processes limited 1GB RAM.
4.1.1 Metrics compare online approaches
compare performance first foremost terms average discounted return execution time. However, really seek online approaches guarantee better
solution quality provided original bounds. words, seek
reduce error original bounds much possible. suggests good
metric efficiency online algorithms compare improvement terms
error bounds current belief online search. Hence, define
error bound reduction percentage be:
UT (b) LT (b)
,
(36)
U (b) L(b)
UT (b), LT (b), U (b) L(b) defined Section 3.2. best online algorithm
provide highest error bound reduction percentage, given initial bounds
real-time constraint.
EBR metric necessarily reflect true error reduction, also compare return guarantees provided algorithm, i.e. lower bounds expected
return provided computed policies current belief. improvement
lower bound compared initial lower bound computed offline direct indicator
true error reduction, best online algorithm provide greatest lower bound
improvement current belief, given initial bounds real-time constraint.
Formally, define lower bound improvement be:
EBR(b) = 1

LBI(b) = LT (b) L(b).
688

(37)

fiOnline Planning Algorithms POMDPs

experiments, EBR LBI metrics evaluated time step
current belief. interested seeing approach provides highest EBR
LBI average.
also consider metrics pertaining complexity efficiency. particular,
report average number belief nodes maintained search tree. Methods
lower complexity generally able maintain bigger trees, results
show always relate higher error bound reduction returns.
also measure efficiency reusing part search tree recording percentage
belief nodes reused one time step next.
4.2 Tag
Tag initially introduced Pineau et al. (2003). environment also
used recently work several authors (Poupart & Boutilier, 2003; Vlassis &
Spaan, 2004; Pineau, 2004; Spaan & Vlassis, 2004; Smith & Simmons, 2004; Braziunas &
Boutilier, 2004; Spaan & Vlassis, 2005; Smith & Simmons, 2005). environment,
approximate POMDP algorithm necessary large size (870 states, 5 actions
30 observations). Tag environment consists agent catch (Tag)
another agent moving 29-cell grid domain. reader referred work
Pineau et al. (2003) full description domain. Note results presented
below, belief state represented factored form. domain exact
factorization possible.
obtain results Tag, run algorithm starting configuration 5 times,
( i.e. 5 runs 841 different starting joint positions, excluding 29 terminal
states ). initial belief state runs consists uniform distribution
possible joint agent positions.
Table 2 compares different heuristics presenting 95% confidence intervals
average discounted return per run (Return), average error bound reduction percentage per
time step (EBR), average lower bound improvement per time step (LBI), average belief
nodes search tree per time step (Belief Nodes), average percentage belief nodes
reused per time step (Nodes Reused), average online planning time used per time step
(Online Time). cases, use FIB upper bound Blind lower bound. Note
average online time slightly lower 1 second per step algorithms
sometimes find -optimal solutions less second.
observe efficiency HSVI-BFS, BI-POMDP AEMS2 differs slightly
environment outperform three heuristics: RTBSS, Satia
Lave, AEMS1. difference explained fact latter three
methods restrict search best solution graph. consequence,
explore many irrelevant nodes, shown lower error bound reduction percentage,
lower bound improvement, nodes reused. poor reuse percentage explains
Satia Lave, AEMS1 limited lower number belief nodes search
tree, compared methods reached averages around 70K. results
three heuristics differ much three heuristics differ
way choose observations explore search. Since two observations
possible first action observation, one observations leads directly
689

fiRoss, Pineau, Paquet, & Chaib-draa

Heuristic
RTBSS(5)
Satia Lave
AEMS1
HSVI-BFS
BI-POMDP
AEMS2

Return
-10.31 0.22
-8.35 0.18
-6.73 0.15
-6.22 0.19
-6.22 0.15
-6.19 0.15

EBR (%)
22.3 0.4
22.9 0.2
49.0 0.3
75.7 0.4
76.2 0.5
76.3 0.5

LBI
3.03 0.07
2.47 0.04
3.92 0.03
7.69 0.06
7.81 0.06
7.81 0.06

Belief
Nodes
45066 701
36908 209
43693 314
64870 947
79508 1000
80250 1018

Nodes
Reused (%)
0
10.0 0.2
25.1 0.3
54.1 0.7
54.6 0.6
54.8 0.6

Online
Time (ms)
580 9
856 4
814 4
673 5
622 4
623 4

Table 2: Comparison different search heuristics Tag environment using Blind
policy lower bound.

EXIT



Figure 3: RockSample[7,8].
terminal belief state, possibility heuristics differed significantly
limited. Due limitation Tag domain, compare online algorithms
larger complex domain: RockSample.
4.3 RockSample
RockSample problem originally presented Smith Simmons (2004).
domain, agent explore environment sample rocks (see Figure 3),
similarly real robot would planet Mars. agent receives rewards
sampling rocks leaving environment (at extreme right environment).
rock scientific value not, agent sample good rocks.
define RockSample[n, k] instance RockSample problem n n
grid k rocks. state characterized k + 1 variables: XP , defines position
robot take values {(1, 1), (1, 2), . . . , (n, n)} k variables, X1R XkR ,
representing rock, take values {Good, Bad}.
agent perform k + 5 actions: {N orth, South, East, W est, Sample, Check1 , . . . ,
Checkk }. four motion actions deterministic. Sample action samples
rock agents current location. Checki action returns noisy observation
{Good, Bad} rock i.
belief state represented factored form known position set k
probabilities, namely probability rock good. Since observation rock
690

fiOnline Planning Algorithms POMDPs

Heuristic
Satia Lave
AEMS1
RTBSS(2)
BI-POMDP
HSVI-BFS
AEMS2
AEMS1
Satia Lave
RTBSS(2)
BI-POMDP
AEMS2
HSVI-BFS

Belief
Nodes
EBR (%)
LBI
Nodes
Reused (%)
Blind: Return:7.35, || = 1, Time:4s
7.35 0
3.64 0
00
509 0
8.92 0
10.30 0.08
9.50 0.11
0.90 0.03
579 2
5.31 0.03
10.30 0.15
9.65 0.02
1.00 0.04
439 0
00
18.43 0.14
33.3 0.5
4.33 0.06
2152 71
29.9 0.6
20.53 0.31
51.7 0.7
5.25 0.07
2582 72
36.5 0.5
20.75 0.15
52.4 0.6
5.30 0.06
3145 101
36.4 0.5
PBVI: Return:5.93, |B| = 64, || = 54, Time:2418s
17.10 0.28
26.1 0.4
1.39 0.03
1461 28
12.2 0.1
19.09 0.21
16.9 0.1
1.17 0.01
2311 25
13.5 0.1
19.45 0.30
22.4 0.3
1.37 0.04
426 1
00
21.36 0.22
49.5 0.2
2.73 0.02
2781 38
32.2 0.2
21.37 0.22
57.7 0.2
3.08 0.02
2910 46
38.2 0.2
21.46 0.22
56.3 0.2
3.03 0.02
2184 33
37.3 0.2
Return

Online
Time (ms)
900
916
886
953
885
859








0
1
2
2
5
6

954
965
540
892
826
826








2
1
7
2
3
2

Table 3: Comparison different search heuristics RockSample[7,8] environment, using
Blind policy PBVI lower bound.

state independent rock states (it depends known robot position),
complexity computing Pr(z|b, a) (b, a, z) greatly reduced. Effectively,
computation Pr(z|b, Checki ) reduces to: Pr(z|b, Checki ) = Pr(Accurate|XP , Checki )
Pr(XiR = z) + (1 Pr(Accurate|XP , Checki )) (1 Pr(XiR = z)). probability
1+(Xp ,i)
, (Xp , i) =
sensor accurate rock i, Pr(Accurate|XP , Checki ) =
2
d(X
p ,i)/d0
2
, d(Xp , i) euclidean distance position Xp position rock i,
d0 constant specifying half efficiency distance. Pr(XiR = z) obtained directly
probability (stored b) rock good. Similarly, (b, a, z) computed
quite easily move actions deterministically affect variable XP , Checki action
changes probability associated XiR according sensors accuracy.
obtain results RockSample, run algorithm starting rock configuration 20 times (i.e. 20 runs 2k different joint rock states). initial
belief state runs consists 0.5 rock good, plus
known initial robot position.
4.3.1 Real-Time Performance Online Search
Table 3, present 95% confidence intervals mean metrics interest,
RockSample[7,8] (12545 states, 13 actions, 2 observations), real-time contraints 1
second per action. compare performance using two different lower bounds, Blind
policy PBVI, use QMDP upper bound cases. performance
policy defined lower bound shown comparison header. RTBSS,
notation RTBSS(k) indicates k-step lookahead; use depth k yields average
online time closest 1 second per action.
Return terms return, first observe AEMS2 HSVI-BFS heuristics
obtain similar results. obtains highest return slight margin
one lower bounds. BI-POMDP obtains similar return combined
691

fiRoss, Pineau, Paquet, & Chaib-draa

PBVI lower bound, performs much worse Blind lower bound. two
heuristics, Satia Lave, AEMS1, perform considerably worse terms return
either lower bound.
EBR LBI terms error bound reduction lower bound improvement, AEMS2
obtains best results lower bounds. HSVI-BFS close second. indicates AEMS2 effectively reduce true error heuristics,
therefore, guarantees better performance. BI-POMDP tends less efficient
AEMS2 HSVI-BFS, significantly better RTBSS, Satia Lave,
AEMS1, slightly improve bounds case. Satia Lave unable
increase Blind lower bound, explains obtains return
Blind policy. also observe higher error bound reduction lower bound
improvement, higher average discounted return usually is. confirms intuition guiding search minimize error current belief bc good
strategy obtain better return.
Nodes Reused terms percentage nodes reused, AEMS2 HVSI-BFS
generally obtain best scores. allows algorithms maintain higher number
nodes trees, could also partly explain outperform
heuristics terms return, error bound reduction lower bound improvement. Note
RTBSS reuse node tree algorithm store
tree memory. consequence, reuse percentage always 0.
Online Time Finally, also observe AEMS2 requires less average online time per
action algorithms attain performance. general, lower average
online time means heuristic efficient finding -optimal actions small amount
time. running time RTBSS determined chosen depth, cannot stop
completing full lookahead search.
Summary Overall, see AEMS2 HSVI-BFS obtain similar results. However
AEMS2 seems slightly better HSVI-BFS, provides better performance guarantees
(lower error) within shorter period time. difference significant.
may due small number observations environment, case two
heuristics expand tree similar ways. next section, explore domain
many observations evaluate impact factor.
lower performances three heuristics explained various reasons.
case BI-POMDP, due fact take account
observation probabilities Pr(z|b, a) discount factor heuristic value. Hence
tend expand fringe nodes affect significantly value current
belief. Satia Lave, poor performance case Blind policy
explained fact fringe nodes maximize heuristic always leaves
reached sequence Move actions. Due deterministic nature Move actions
(Pr(z|b, a) = 1 actions, whereas Check actions Pr(z|b, a) = 0.5 initially),
heuristic value fringe nodes reached Move actions much higher error
reduced significantly. result, algorithm never explores nodes Check
actions, robot always follows Blind policy (moving east, never checking
sampling rocks). demonstrates importance restricting choice
692

fiOnline Planning Algorithms POMDPs

30

25

V(b0)

20

15

AEMS2
AEMS1
BIPOMDP
HSVIBFS
Satia

10

5 2
10

1

10

0

1

10

10

2

10

3

10

Time (s)

Figure 4: Evolution upper lower bounds RockSample[7,8].

leaves explore reached sequence actions maximizing upper bound,
done AEMS2, HSVI-BFS BI-POMDP. case AEMS1, probably behaves
less efficiently term uses estimate probability certain action
optimal good approximation environment. Moreover, AEMS1
restrict exploration best solution graph, probably also suffers, part,
problems Satia Lave heuristic. RTBSS also perform
well Blind lower bound. due short depth allowed search
tree, required running time 1 second/action. confirms
significantly better exhaustive search good heuristics guide search.
4.3.2 Long-Term Error Reduction Online Heuristic Search
compare long term performance different heuristics, let algorithms run
offline mode initial belief state environment, log changes lower
upper bound values initial belief state 1000 seconds. Here, initial lower
upper bounds provided Blind policy QMDP respectively. see
Figure 4 Satia Lave, AEMS1 BI-POMDP efficient HSVI-BFS
AEMS2 reducing error bounds. One interesting thing note
upper bound tends decrease slowly continuously, whereas lower bound often
increases stepwise manner. believe due fact upper bound
much tighter lower bound. also observe error bound reduction
happens first seconds search. confirms nodes expanded earlier
tree much impact error bc expanded far
tree (e.g. hundreds seconds). important result support using online
(as opposed offline) methods.
693

fi22

22

20

20

Average Discounted Return

Average Discounted Return

Ross, Pineau, Paquet, & Chaib-draa

18
16
AEMS2
HSVIBFS
BIPOMDP

14
12
10
8
6 1
10

0

10

18
16

12
10
8
6 1
10

1

10

AEMS2 & Blind
AEMS2 & PBVI(8)
AEMS2 & PBVI(16)

14

Online Time (s)

0

10

1

10

Online Time (s)

Figure 5: Comparison return Figure 6: Comparison return
function online time
function online time
RockSample(10,10) different
RockSample(10,10) different
online methods.
offline lower bounds.

4.3.3 Influence Offline Online Time
compare performance online approaches influenced available
online offline times. allows us verify particular method better
available online time shorter (or longer), whether increasing offline time could
beneficial.
consider three approaches shown best overall performance far (BIPOMDP, HSVI-BFS AEMS2) compare average discounted return function online time constraint per action. Experiments run RockSample[10,10]
(102,401 states, 15 actions, 2 observations) following online time constraints:
0.1s, 0.2s, 0.5s, 1s, 2s, 5s 10s. vary offline time, used 3 different lower
bounds: Blind policy, PBVI 8 belief points, PBVI 16 belief points, taking
respectively 15s, 82s, 193s. upper bound used QMDP cases. results
obtained Intel Xeon 3.0 Ghz processor.
Figure 5, observe AEMS2 fares significantly better HSVI-BFS
BI-POMDP short time constraints. time constraint increases, AEMS2
HSVI-BFS performs similarly (no significant statistical difference). also notice
performance BI-POMDP stops improving 1 second planning time.
explained fact take account observation probabilities
Pr(z|b, a), discount factor. search tree grows bigger, fringe
nodes small probability reached future, becomes
important take probabilities account order improve performance.
Otherwise, observe case BI-POMDP, expanded nodes affect
quality solution found.
Figure 6, observe increasing offline time beneficial effect mostly
short real-time constraints. online planning time available,
694

fiOnline Planning Algorithms POMDPs

difference performances AEMS2 Blind lower bound, AEMS2
PBVI becomes insignificant. However, online time constraints smaller one
second, difference performance large. Intuitively, short real-time
constraints algorithm enough time expand lot nodes,
policy found relies much bounds computed offline. hand,
longer time constraints, algorithm enough time significantly improve bounds
computed offline, thus policy found rely much offline bounds.
4.4 FieldVisionRockSample
seems results presented thus far HSVI-BFS AEMS2 comparable
performance standard domains. note however environments
small observation sets (assuming observations zero probability removed).
believe AEMS2 especially well suited domains large observation spaces. However,
standard problems literature. therefore consider modified
version RockSample environment, called FieldVisionRockSample (Ross & Chaib-draa,
2007), observation space size exponential number rocks.
FieldVisionRockSample (FVRS) problem differs RockSample problem
way robot able perceive rocks environment. Recall
RockSample, agent Check action specific rock observe state
noisy sensor. FVRS, robot observes state rocks,
noisy sensor, action conducted environment. Consequently,
eliminates use Check actions, remaining actions robot include
four move actions {North, East, South, West} Sample action. robot
perceive rock either Good Bad, thus observation space size 2k
instance problem k rocks. RockSample, efficiency sensor
defined parameter = 2d/d0 , distance rock d0
half efficiency distance. assume sensors observations independent rock.
FVRS, partial observability environment directly proportional
parameter d0 : d0 increases, sensor becomes accurate uncertainty
state environment decreases. value d0 defined different instances
RockSample work Smith Simmons (2004) high FVRS problem
(especially bigger instances RockSample), making almost completely observable.
Consequently, re-define value d0 different instances FieldVisionRockSample according size grid (n). considering fact
p n n grid,
largest possible distance rock robot (n 1) (2), seems reasonable distance, probability observing real state rock
close 50%
p problem remain partially observable. Consequently, define
d0 = (n 1) (2)/4.
obtain results FVRS domain, run algorithm starting rock
configurations 20 times (i.e. 20 runs 2k different joint rock states).
initial belief state runs corresponds probability 0.5
rock good, well known initial position robot.
695

fiRoss, Pineau, Paquet, & Chaib-draa

Heuristic
RTBSS(2)
AEMS1
Satia Lave
HSVI-BFS
AEMS2
BI-POMDP
RTBSS(1)
BI-POMDP
Satia Lave
AEMS1
AEMS2
HSVI-BFS

Belief
Nodes
Return
EBR (%)
LBI
Nodes
Reused (%)
FVRS[5,5] [Blind: Return:8.15, || = 1, Time=170ms]
16.54 0.37
18.4 1.1
2.80 0.19
18499 102
00
16.88 0.36
17.1 1.1
2.35 0.16
8053 123
1.19 0.07
18.68 0.39
15.9 1.2
2.17 0.16
7965 118
0.88 0.06
20.27 0.44
23.8 1.4
2.64 0.14
4494 105
4.50 0.80
21.18 0.45
31.5 1.5
3.11 0.15
12301 440
3.93 0.22
22.75 0.47
31.1 1.2
3.30 0.17
12199 427
2.26 0.44
FVRS[5,7] [Blind: Return:8.15, || = 1, Time=761ms]
20.57 0.23
7.72 0.13
2.07 0.11
516 1
00
22.75 0.25
11.1 0.4
2.08 0.07
4457 61
0.37 0.11
22.79 0.25
11.1 0.4
2.05 0.08
3683 52
0.36 0.07
23.31 0.25
12.4 0.4
2.24 0.08
3856 55
1.36 0.13
23.39 0.25
13.3 0.4
2.35 0.08
4070 58
1.64 0.14
23.40 0.25
13.0 0.4
2.30 0.08
3573 52
1.69 0.27

Online
Time (ms)
3135 27
876 5
878 4
857 12
854 13
782 12
254
923
947
942
944
946








1
2
3
3
2
3

Table 4: Comparison different search heuristics different instances FieldVisionRockSample environment.

4.4.1 Real-Time Performance Online Search
Table 4, present 95% confidence intervals mean metrics interest.
consider two instances environment, FVRS[5,5] (801 states, 5 actions, 32 observations) FVRS[5,7] (3201 states, 5 actions, 128 observations). cases, use
QMDP upper bound Blind lower bound, real-time constraints 1 second per
action.
Return terms return, observe clear winner. BI-POMDP performs surpringly well FVRS[5,5] significantly worse AEMS2 HSVI-BFS FVRS[5,7].
hand, AEMS2 significantly better HSVI-BFS FVRS[5,5]
get similar performances FVRS[5,7]. Satia Lave performs better environment RockSample. likely due fact transitions belief
space longer deterministic (as case Move actions RockSample).
FVRS[5,5], also observe even RTBSS given 3 seconds per action
perform two-step lookahead, performance worse heuristic search
methods. clearly shows expanding observations equally search
good strategy, many observations negligible impact current
decision.
EBR LBI terms error bound reduction lower bound improvement, observe AEMS2 performs much better HSVI-BFS FVRS[5,5], significantly
better FVRS[5,7]. hand, BI-POMDP obtains similar results AEMS2
FVRS[5,5] significantly worse terms EBR LBI FVRS[5,7].
suggests AEMS2 consistently effective reducing error, even environments
large branching factors.
Nodes Reused percentage belief nodes reused much lower FVRS due
much higher branching factor. observe HSVI-BFS best reuse percentage
696

fiOnline Planning Algorithms POMDPs

26

35

24
30

22
20

16
14

25
AEMS2
AEMS1
BIPOMDP
HSVIBFS
Satia

V(b0)

V(b0)

18

20

AEMS2
AEMS1
BIPOMDP
HSVIBFS
Satia

15

12
10

10

8
6 2
10

1

10

0

1

10

10

2

10

5 1
10

3

10

Time (s)

0

10

1

10
Time (s)

2

10

3

10

Figure 7: Evolution upper lower Figure 8: Evolution upper lower
bounds FieldVisionRockSambounds FieldVisionRockSample[5,5].
ple[5,7].

environments, however significantly higher AEMS2. methods
reuse significantly larger portion tree methods. confirms
two methods able guide search towards likely beliefs.
4.4.2 Long-Term Error Reduction Online Heuristic Search
Overall, Table 4 confirms consistent performance HSVI-BFS AEMS2,
difference heuristics modest. Considering complexity environment,
may due fact algorithms enough time expand
significant number nodes within 1 second. long-term analysis bounds evolution
Figures 7 8 confirms this. observe figures lower bound converges
slightly rapidly AEMS2 heuristics. AEMS1 heuristic also
performs well long run problem, seems second best heuristic,
Satia Lave far behind. hand, HSVI-BFS heuristic far
worse problem RockSample. seems part due fact
heuristic takes time find next node expand others, thus
explores fewer belief states.

5. Discussion
previous sections presented evaluated several online POMDP algorithms.
discuss important issues arise applying online methods practice, summarize
advantages disadvantages. help researchers decide whether
online algorithms good approach solving given problem.
697

fiRoss, Pineau, Paquet, & Chaib-draa

5.1 Lower Upper Bound Selection
Online algorithms combined many valid lower upper bounds. However,
properties bounds satisfy online search perform efficiently practice. One desired properties lower upper
bound functions

property states b : L(b)


P monotone. monotone
maxaA RB (b, a) + PzZ Pr(z|b, a)L( (b, a, z)) lower bound b : U (b)
maxaA RB (b, a) + zZ Pr(z|b, a)U ( (b, a, z)) upper bound. property
guarantees certain fringe node expanded, lower bound non-decreasing
upper bound non-increasing. sufficient guarantee error bound
UT (b) LT (b) b non-increasing expansion b, error bound
given algorithm value root belief state bc , cannot worse
error bound defined initial bounds given. Note however monotonicity
necessary AEMS converge -optimal solution, shown previous work (Ross
et al., 2008); boundedness sufficient.
5.2 Improving Bounds Time
mentioned survey online algorithms, one drawback many online approaches store improvements made offline bounds
online search, that, belief state encountered again, computations need performed again, restarting offline bounds. trivial way
improve maintain large hashtable (or database) belief states
improved lower upper bounds previous search, associated new
bounds. however many drawbacks this. First every time want
evaluate lower upper bound fringe belief, search hashtable needs
performed check better bounds available. may require significant
time hashtable large (e.g. millions beliefs). Furthermore, experiments conducted
RTDP-Bel large domains, RockSample[7,8], shown process
usually runs memory (i.e. requires 1 GB) good performance
achieved requires several thousands episodes performing well (Paquet, 2006).
authors RTBSS also tried combining search algorithm RTDPBel preserve improvements made search (Paquet, 2006).
combination usually performed better learned faster RTDP-Bel alone, found
domains, thousand episodes still required improvement
seen (in terms return). Hence, point updates offline bounds tend
useful large domains task accomplish repeated large number
times.
better strategy improve lower bound might save time perform
-vector updates beliefs expanded search, offline
lower bound improves time. updates advantage improving lower
bound whole belief space, instead single belief state. However
time consuming, especially large domains. Hence, need act within short
time constraints, approach infeasible. However several seconds planning time
available per action, might advantageous use time perform
-vector updates, rather use available time search tree. good
698

fiOnline Planning Algorithms POMDPs

idea would perform -vector updates subset beliefs search tree,
lower bound improves.
5.3 Factored POMDP Representations
efficiency online algorithms relies heavily ability quickly compute (b, a, z)
Pr(z|b, a), must computed evey belief state search tree. Using
factored POMDP representations effective way reduce time complexity computing quantities. Since environments large state spaces structured
described sets features, obtaining factored representation complex systems
issue cases. However, domains significant dependencies
state features, may useful use algorithms proposed Boyen Koller
(1998) Poupart (2005) find approximate factored representations features
independent, minimal degradation solution quality. upper
lower bounds might hold anymore computed approximate factored
representation, usually may still yield good results practice.
5.4 Handling Graph Structure
mentioned before, general tree search algorithm used online algorithms
duplicate belief states whenever multiple paths leading posterior
belief current belief bc . greatly simplifies complexity related updating
values ancestor nodes, also reduces complexity related finding
best fringe node expand (using technique Section 3.4 valid
trees). disadvantage using tree structure inevitably, computations
redundant, algorithm potentially expand subtree every
duplicate belief. avoid this, could use LAO algorithm proposed Hansen
Zilberstein (2001) extension AO handle generic graph structure, including
cyclic graphs. expansion, runs value (or policy) iteration algorithm
convergence among ancestor nodes order update values.
heuristics surveyed Section 3.4 generalized guide best-first search
algorithms handle graph structure, like LAO . first thing notice that,
graph, fringe node reached multiple paths, error contributes multiple
times error value bc . error contribution perspective, heuristic
value fringe node sum heuristic values paths reaching
it. instance, case AEMS heuristic, using notation defined
Section 3.4, global heuristic value given fringe node b, current belief state
bc graph G, computed follows:
HG (bc , b) = (U (b) L(b))

X

d(h) Pr(h|bc , G ).

(38)

hHG (bc ,b)

Notice cyclic graphs, infinitely many paths HG (bc , b).
case, could use dynamic programming estimate heuristic value.
solving HG (bc , b) fringe nodes b graph G require lot time
practice, especially many fringe nodes, experimented
method Section 4. However, would practical use heuristic could find
699

fiRoss, Pineau, Paquet, & Chaib-draa

alternative way determine best fringe node without computing HG (bc , b) separately
fringe node b performing exhaustive search fringe nodes.
5.5 Online vs. Offline Time
One important aspect determining efficiency applicability online algorithms
amount time available execution planning. course often taskdependent. real-time problems like robot navigation, amount time may
short, e.g. 0.1 1 second per action. hand tasks like portfolio
management, acting every second necessary, several minutes could easily
taken plan stock buying/selling action. seen experiments,
shorter available online planning time, greater importance good
offline value function start with. case, often necessary reserve sufficient
time compute good offline policy. planning time available online,
influence offline value function becomes negligible, rough offline
value function sufficient obtain good performance. best trade-off online
offline time often depends large problem is. branching factor
(|A||Z|) large and/or computing successor belief states takes long time, online
time required achieve significant improvement offline value function.
However, small problems, online time 0.1 second per action may sufficient
perform near-optimally even rough offline value function.
5.6 Advantages Disadvantages Online Algorithms
discuss advantages disadvantages online planning algorithms general.
5.6.1 Advantages
online algorithms combined offline solving algorithm, assuming
provides lower bound upper bound V , improve quality
policy found offline.
Online algorithms require little offline computation executable
environment, perform well even using loose bounds, quick
compute.
Online methods exploit knowledge current belief focus computation
relevant future beliefs current decision, scale well
large action observation spaces.
Anytime online methods applicable real-time environments,
stopped whenever planning time runs out, still provide best solution found
far.
5.6.2 Disadvantages
branching factor depends number actions observations. Thus
many observations and/or actions, might impossible search deep
700

fiOnline Planning Algorithms POMDPs

enough, provide significant improvement offline policy. cases, sampling methods designed reduce branching factor could useful.
cannot guarantee lower upper bounds still valid sampling
used, guarantee valid high probability, given enough
samples drawn.
online algorithms store improvements made offline policy
online search, algorithm plan bounds time
environment restarted. time available, could advantageous add
-vector updates belief states explored tree, offline bounds
improve time.

6. Conclusion
POMDPs provide rich elegant framework planning stochastic partially observable domains, however time complexity major issue preventing
application complex real-world systems. paper thoroughly surveys various existing online algorithms key techniques approximations used solve POMDPs
efficiently. empirically compare online approaches several POMDP domains different metrics: average discounted return, average error bound reduction
average lower bound improvement, using different lower upper bounds: PBVI,
Blind, FIB QMDP.
empirical results, observe heuristic search methods, namely
AEMS2 HSVI-BFS, obtain good performances, even domains large branching factors large state spaces. two methods similar perform well
orient search towards nodes improve current approximate
value function quickly possible; i.e. belief nodes largest error
likely reached future promising actions. However, environments
large branching factors, may time expand nodes turn.
Hence, would interesting develop approximations reduce branching
factor cases.
conclusion, believe online approaches important role play
improving scalability POMDP solution methods. good example succesful
applications RTBSS algorithm RobocupRescue simulation Paquet et al.
(2005). environment challenging state space orders magnitude
beyond scope current algorithms. Offline algorithms remain important obtain
tight lower upper bounds value function. interesting question whether
online offline approaches better, improve kinds approaches,
synergy exploited solve complex real-world problems.

Acknowledgments
research supported Natural Sciences Engineering Council Canada
Fonds Quebecois de la Recherche sur la Nature et les Technologies. would also
like thank anonymous reviewers helpful comments suggestions.
701

fiRoss, Pineau, Paquet, & Chaib-draa

References
Astrom, K. J. (1965). Optimal control Markov decision processes incomplete state
estimation. Journal Mathematical Analysis Applications, 10, 174205.
Barto, A. G., Bradtke, S. J., & Singhe, S. P. (1995). Learning act using real-time dynamic
programming. Artificial Intelligence, 72 (1), 81138.
Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ,
USA.
Bertsekas, D. P., & Castanon, D. A. (1999). Rollout algorithms stochastic scheduling
problems. Journal Heuristics, 5 (1), 89108.
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.
Proceedings Fourteenth Conference Uncertainty Artificial Intelligence
(UAI-98), pp. 3342.
Braziunas, D., & Boutilier, C. (2004). Stochastic local search POMDP controllers.
Nineteenth National Conference Artificial Intelligence (AAAI-04), pp. 690696.
Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple, fast,
exact method partially observable Markov decision processes. Proceedings
Thirteenth Conference Uncertainty Artificial Intelligence (UAI-97), pp. 5461.
Chang, H. S., Givan, R., & Chong, E. K. P. (2004). Parallel rollout online solution
partially observable Markov decision processes. Discrete Event Dynamic Systems,
14 (3), 309341.
Geffner, H., & Bonet, B. (1998). Solving large POMDPs using real time dynamic programming. Proceedings Fall AAAI symposium POMDPs, pp. 6168.
Hansen, E. A. (1998). Solving POMDPs searching policy space. Fourteenth Conference Uncertainty Artificial Intelligence (UAI-98), pp. 211219.
Hansen, E. A., & Zilberstein, S. (2001). LAO * : heuristic search algorithm finds
solutions loops. Artificial Intelligence, 129 (1-2), 3562.
Hauskrecht, M. (2000). Value-function approximations partially observable Markov
decision processes. Journal Artificial Intelligence Research, 13, 3394.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting
partially observable stochastic domains. Artificial Intelligence, 101, 99134.
Kearns, M. J., Mansour, Y., & Ng, A. Y. (1999). sparse sampling algorithm nearoptimal planning large markov decision processes. Proceedings Sixteenth
International Joint Conference Artificial Intelligence (IJCAI-99), pp. 13241331.
Koenig, S. (2001). Agent-centered search. AI Magazine, 22 (4), 109131.
Littman, M. L. (1996). Algorithms sequential decision making. Ph.D. thesis, Brown
University.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995). Learning policies partially observable environments: scaling up. Proceedings 12th International
Conference Machine Learning (ICML-95), pp. 362370.
702

fiOnline Planning Algorithms POMDPs

Lovejoy, W. S. (1991). Computationally feasible bounds POMDPs. Operations Research,
39 (1), 162175.
Madani, O., Hanks, S., & Condon, A. (1999). undecidability probabilistic planning
infinite-horizon partially observable Markov decision problems. Proceedings
Sixteenth National Conference Artificial Intelligence. (AAAI-99), pp. 541548.
McAllester, D., & Singh, S. (1999). Approximate Planning Factored POMDPs using Belief State Simplification. Proceedings 15th Annual Conference Uncertainty
Artificial Intelligence (UAI-99), pp. 409416.
Monahan, G. E. (1982). survey partially observable Markov decision processes: theory,
models algorithms. Management Science, 28 (1), 116.
Nilsson, N. (1980). Principles Artificial Intelligence. Tioga Publishing.
Papadimitriou, C., & Tsitsiklis, J. N. (1987). complexity Markov decision processes.
Mathematics Operations Research, 12 (3), 441450.
Paquet, S. (2006). Distributed Decision-Making Task Coordination Dynamic, Uncertain Real-Time Multiagent Environments. Ph.D. thesis, Laval University.
Paquet, S., Chaib-draa, B., & Ross, S. (2006). Hybrid POMDP algorithms. Proceedings
Workshop Multi-Agent Sequential Decision Making Uncertain Domains
(MSDM-06), pp. 133147.
Paquet, S., Tobin, L., & Chaib-draa, B. (2005). online POMDP algorithm complex
multiagent environments. Proceedings fourth International Joint Conference
Autonomous Agents Multi Agent Systems (AAMAS-05), pp. 970977.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: anytime algorithm POMDPs. Proceedings International Joint Conference Artificial
Intelligence (IJCAI-03), pp. 10251032.
Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations large
POMDPs. Journal Artificial Intelligence Research, 27, 335380.
Pineau, J. (2004). Tractable planning uncertainty: exploiting structure. Ph.D. thesis,
Carnegie Mellon University.
Poupart, P. (2005). Exploiting structure efficiently solve large scale partially observable
Markov decision processes. Ph.D. thesis, University Toronto.
Poupart, P., & Boutilier, C. (2003). Bounded finite state controllers. Advances Neural
Information Processing Systems 16 (NIPS).
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.
Ross, S., & Chaib-draa, B. (2007). Aems: anytime online search algorithm approximate policy refinement large POMDPs. Proceedings 20th International
Joint Conference Artificial Intelligence (IJCAI-07), pp. 25922598.
Ross, S., Pineau, J., & Chaib-draa, B. (2008). Theoretical analysis heuristic search
methods online POMDPs. Advances Neural Information Processing Systems
20 (NIPS).
703

fiRoss, Pineau, Paquet, & Chaib-draa

Satia, J. K., & Lave, R. E. (1973). Markovian decision processes probabilistic observation states. Management Science, 20 (1), 113.
Shani, G., Brafman, R., & Shimony, S. (2005). Adaptation changing stochastic environments online POMDP policy learning. Proceedings Workshop
Reinforcement Learning Non-Stationary Environments, ECML 2005, pp. 6170.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable
Markov processes finite horizon. Operations Research, 21 (5), 10711088.
Smith, T., & Simmons, R. (2004). Heuristic search value iteration POMDPs. Proceedings 20th Conference Uncertainty Artificial Intelligence (UAI-04), pp.
520527.
Smith, T., & Simmons, R. (2005). Point-based POMDP algorithms: improved analysis
implementation. Proceedings 21th Conference Uncertainty Artificial
Intelligence (UAI-05), pp. 542547.
Sondik, E. J. (1971). optimal control partially observable Markov processes. Ph.D.
thesis, Stanford University.
Sondik, E. J. (1978). optimal control partially observable Markov processes
infinite horizon: Discounted costs. Operations Research, 26 (2), 282304.
Spaan, M. T. J., & Vlassis, N. (2004). point-based POMDP algorithm robot planning.
Proceedings IEEE International Conference Robotics Automation
(ICRA-04), pp. 23992404.
Spaan, M. T. J., & Vlassis, N. (2005). Perseus: randomized point-based value iteration
POMDPs. Journal Artificial Intelligence Research, 24, 195220.
Vlassis, N., & Spaan, M. T. J. (2004). fast point-based algorithm POMDPs.
Benelearn 2004: Proceedings Annual Machine Learning Conference Belgium
Netherlands, pp. 170176.
Washington, R. (1997). BI-POMDP: bounded, incremental partially observable Markov
model planning. Proceedings 4th European Conference Planning, pp.
440451.
Zhang, N. L., & Zhang, W. (2001). Speeding convergence value iteration partially observable Markov decision processes. Journal Artificial Intelligence Research,
14, 2951.

704

fiJournal Artificial Intelligence Research 32 (2008) 169-202

Submitted 10/07; published 05/08

Communication-Based Decomposition Mechanisms
Decentralized MDPs
Claudia V. Goldman

c.goldman@samsung.com

Samsung Telecom Research Israel
Yakum, Israel

Shlomo Zilberstein

shlomo@cs.umass.edu

Department Computer Science
University Massachusetts, Amherst, 01003 USA

Abstract
Multi-agent planning stochastic environments framed formally decentralized Markov decision problem. Many real-life distributed problems arise manufacturing, multi-robot coordination information gathering scenarios formalized
using framework. However, finding optimal solution general case hard,
limiting applicability recently developed algorithms. paper provides practical approach solving decentralized control problems communication among
decision makers possible, costly. develop notion communication-based
mechanism allows us decompose decentralized MDP multiple single-agent
problems. framework, referred decentralized semi-Markov decision process
direct communication (Dec-SMDP-Com), agents operate separately communications. show finding optimal mechanism equivalent solving optimally
Dec-SMDP-Com. also provide heuristic search algorithm converges optimal decomposition. Restricting decomposition specific types local behaviors
reduces significantly complexity planning. particular, present polynomialtime algorithm case individual agents perform goal-oriented behaviors
communications. paper concludes additional tractable algorithm
enables introduction human knowledge, thereby reducing overall problem
finding best time communicate. Empirical results show approaches
provide good approximate solutions.

1. Introduction
decentralized Markov decision process become common formal tool study
multi-agent planning control decision-theoretic perspective (Bernstein, Givan,
Immerman, & Zilberstein, 2002; Becker, Zilberstein, Lesser, & Goldman, 2004; Guestrin &
Gordon, 2002; Guestrin, Koller, & Parr, 2001; Nair, Tambe, Yokoo, Pynadath, & Marsella,
2003; Petrik & Zilberstein, 2007; Peshkin, Kim, Meuleau, & Kaelbling, 2000). Seuken
Zilberstein (2008) provide comprehensive comparison existing formal models
algorithms. Decentralized MDPs complement existing approaches coordination
multiple agents based on-line learning heuristic approaches (Wolpert, Wheeler, &
Tumer, 1999; Schneider, Wong, Moore, & Riedmiller, 1999; Xuan, Lesser, & Zilberstein,
2001; Ghavamzadeh & Mahadevan, 2004; Nair, Tambe, Roth, & Yokoo, 2004).
Many challenging real-world problems formalized instances decentralized
MDPs. problems, exchanging information constantly decision makers
c
2008
AI Access Foundation. rights reserved.

fiGoldman & Zilberstein

either undesirable impossible. Furthermore, processes controlled group
decision makers must act based different partial views global state. Thus,
centralized approach action selection infeasible. example, exchanging information
single central controller lead saturation communication network. Even
transitions observations agents independent, global problem
may decompose separate, individual problems, thus simple parallel algorithm
may sufficient. Choosing different local behaviors could lead different global
rewards. Therefore, agents may need exchange information periodically revise
local behaviors. One important point understand model propose although
eventually agent behave following local behavior, choosing among possible
behaviors requires information agents. focus situations
information freely available, obtained via communication.
Solving optimally general decentralized control problem shown computationally hard (Bernstein et al., 2002; Pynadath & Tambe, 2002). worst case,
general problem requires double-exponential algorithm1 . difficulty due two
main reasons: 1) none decision-makers full-observability global system
2) global performance system depends global reward, affected
agents behaviors. previous work (Goldman & Zilberstein, 2004a), studied complexity solving optimally certain classes Dec-MDPs Dec-POMDPs2 .
example, shown decentralized problems independent transitions
observations considerably easier solve, namely, NP-complete. Even
cases, agents behaviors dependent global reward function, may
decompose separate local reward functions. latter case studied within
context auction mechanisms weakly coupled MDPs Bererton et al. (2003).
paper, solution type complex decentralized problems includes
temporally abstracted actions combined communication actions. Petrik Zilberstein (2007) recently presented improved solution previous Coverage Set
algorithm (Becker et al., 2004), solve decentralized problems optimally. However, technique suitable communication agents possible.
Another recent study Seuken Zilberstein (2007a, 2007b) produced general
approximation technique based dynamic programming heuristic search.
approach shows better scalability, remains limited relatively small problems compared
decomposition method presented here.
propose approach approximate optimal solutions decentralized problems
off-line. main idea compute multiagent macro actions necessarily end
communication. Assuming communication incurs cost, communication policy
computed optimally, algorithms proposed paper compute
best time agents exchange information. time points, agents attain full
knowledge current global state. algorithms also compute agent
domain actions perform communication, temporally abstracted actions
interrupted time. Since behaviors computed agent
1. Unless NEXP different EXP, cannot prove super-exponential complexity. But,
generally believed NEXP-complete problems require double-exponential time solve optimally.
2. Dec-MDPs, observations agents sufficient determine global state,
Dec-POMDPs global state cannot fully determined observations.

170

fiCommunication-Based Decomposition Mechanism

separately independently other, final complete solution communication
action policies guaranteed globally optimal. refer approach
communication-based decomposition mechanism: algorithms proposed compute
mechanisms decompose global behavior agents local behaviors
coordinated communication. Throughout paper, algorithms differ space
behaviors search: solutions range general search space
available (leading optimal mechanism) restricted sets behaviors.
contribution paper provide tractable method, namely communicationbased decomposition mechanisms, solve decentralized problems, efficient
algorithms currently exist. general decentralized problems, approach serves
practical way approximate solution systematic way. also provide analysis
bounds approximations local transitions independent.
specific cases, like independent transitions observations, show
compute optimal decompositions local behaviors optimal policies communication coordinate agents behaviors global level.
Section 3 introduces notion communication-based mechanisms. formally frame
approach decentralized semi-Markov decision process direct communication
(Dec-SMDP-Com) Section 4. Section 5 presents decentralized multi-step backup
policy-iteration algorithm returns optimal decomposition mechanism restrictions imposed individual behaviors agents. Due generality,
algorithm applicable limited domains. Section 6 presents practical
solution, considering agent assigned local goal states. Assuming local
goal-oriented behavior reduces complexity problem polynomial number states. Empirical results (Section 6.2) support claims. approximation
mechanism also applied range possible local behaviors provided
design time. Since predetermined local behaviors alone may sufficient
achieve coordination, agents still need decide communicate. Section 7 presents
polynomial-time algorithm computes policy communication, given local policies domain actions. closer human-designed local plans local optimal
behaviors, closer solution optimal joint solution. Empirical results
Meeting Uncertainty scenario (also known Gathering Problem robotics,
Suzuki Yamashita, 1999) presented Section 7.1. conclude discussion
contributions work Section 8.

2. Dec-MDP model
Previous studies shown decentralized MDPs general hard solve
optimally off-line even direct communication allowed (Bernstein et al., 2002;
Pynadath & Tambe, 2002; Goldman & Zilberstein, 2004a). comprehensive complexity
analysis solving optimally decentralized control problems revealed sources difficulty solving problems (Goldman & Zilberstein, 2004a). algorithms
proposed actually solve classes problems optimally efficiently.
define general underlying process allows agents exchange messages directly
decentralized POMDP direct communication:

171

fiGoldman & Zilberstein

Definition 1 (Dec-POMDP-Com) decentralized partially-observable Markov decision
process direct communication, Dec-POMDP-Com given following tuple:
=< S, A1 , A2 , , C , P, R, 1 , 2 , O, >,
finite set world states, factored include distinguished initial
state s0 .
A1 A2 finite sets actions. ai denotes action performed agent i.
denotes alphabet messages represents atomic message sent
agent (i.e., letter language).
C cost transmitting atomic message: C : <. cost transmitting null message zero.
P transition probability function. P (s0 |s, a1 , a2 ) probability moving
state state s0 agents 1 2 perform actions a1 a2
respectively. transition model stationary, i.e., independent time.
R global reward function. R(s, a1 , a2 , s0 ) represents reward obtained
system whole, agent 1 executes action a1 agent 2 executes action a2
state resulting transition state s0 .
1 2 finite sets observations.
observation function. O(o1 , o2 |s, a1 , a2 , s0 ) probability observing o1
o2 (respectively two agents) state agent 1 takes action a1
agent 2 takes action a2 , resulting state s0 .
Dec-POMDP finite horizon, represented positive integer .
notation represents set discrete time points process.
optimal solution decentralized problem joint policy maximizes
criteriain case, expected accumulated reward system. joint policy
tuple composed local policies agent, composed policy action
policy communication: i.e., joint policy = (1 , 2 ), iA : Ai
: . is, local policy action assigns action possible
sequence local observations messages received. local policy communication
assigns message possible sequence observations messages received.
cycle, agents perform domain action, perceive observation send
message.
assume system independent observations transitions (see Section 6.3
discussion general case). Given factored system states = (s1 , s2 ) S,
domain actions ai observations oi agent, formal definitions3
decentralized processes independent transitions, observations follow. note
class problems trivial since reward system necessarily
independent. simplicity, present definitions case two agents. However,
approach presented paper applicable systems n agents.

3. definitions based Goldman Zilberstein (2004a). include make
paper self-contained.

172

fiCommunication-Based Decomposition Mechanism

Definition 2 (A Dec-POMDP Independent Transitions) Dec-POMDP
independent transitions set states factored two components = S1 S2
that:
s1 , s01 S1 , s2 , s02 S2 , a1 A1 , a2 A2 ,
P r(s01 |(s1 , s2 ), a1 , a2 , s02 ) = P r(s01 |s1 , a1 )
P r(s02 |(s1 , s2 ), a1 , a2 , s01 ) = P r(s02 |s2 , a2 ).
words, transition probability P Dec-POMDP represented
P = P1 P2 , P1 = P r(s01 |s1 , a1 ) P2 = P r(s02 |s2 , a2 ).
Definition 3 (A Dec-POMDP Independent Observations) Dec-POMDP
independent observations set states factored two components =
S1 S2 that:
o1 1 , o2 2 , = (s1 , s2 ), s0 = (s01 , s02 ) S, a1 A1 , a2 A2 ,
P r(o1 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o2 ) = P r(o1 |s1 , a1 , s01 )
P r(o2 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o1 ) = P r(o2 |s2 , a2 , s02 )
O(o1 , o2 |(s1 , s2 ), a1 , a2 , (s01 , s02 )) = P r(o1 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o2 )P r(o2 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o1 ).
words, observation probability Dec-POMDP decomposed
two observation probabilities O1 O2 , O1 = P r(o1 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o2 )
O2 = P r(o2 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o1 ).
Definition 4 (Dec-MDP) decentralized Markov decision process (Dec-MDP) DecPOMDP, jointly fully observable, i.e., combination agents observations
determine global state system.
previous work (Goldman & Zilberstein, 2004a), proved Dec-MDPs independent transitions observations locally fully-observable. particular, showed
exchanging last observation sufficient obtain complete information
current global state guarantees optimality solution.
focus computation individual behaviors agents taking
account exchange information time time. following sections
present communication-based decomposition approximation method solve Dec-MDPs
direct communication independent transitions observations.

3. Communication-based Decomposition Mechanism
interested creating mechanism tell us individual behaviors
beneficial sense behaviors taken jointly result good
approximation optimal decentralized solution global system. Notice
even system global objective, straightforward compute
individual behaviors. decision problem requires achievement global
objective tell us local goals decision maker needs reach order
173

fiGoldman & Zilberstein

maximize value joint policy reaches global objective. Therefore, propose
communication-based decomposition mechanisms practical approach approximating
optimal joint policy decentralized control problems. approach produce two
results: 1) set temporarily abstracted actions global state agent
2) policy communication, aimed synchronizing agents partial information
time beneficial system.
Formally, communication-based decomposition mechanism CDM function
global state decentralized problem two single agent behaviors policies:
CDM : (Opt1 , Opt2 ). general, mechanism applied systems n
agents, case decomposition decentralized process n individual behaviors. order study communication-based mechanisms, draw analogy
temporary local policies actions options. Options defined
Sutton et al. (1999) temporally abstracted actions, formalized triplets including
stochastic single-agent policy, termination condition, set states
initiated: opt =< : [0, 1], : + [0, 1], >. option available
state I.
approach considers options terminal actions (instead terminal states). Terminal actions also considered Hansen Zhou (2003) framework indefinite
POMDPs. denote domain actions agent Ai . set terminal actions
includes messages . one agent, option given following tuple:

opti =< : Si Ai , Si >, i.e., option non-stochastic policy
agents partial view (local states) time set primitive domain actions
terminal actions. local states Si given factored representation
Dec-MDP independent transitions observations. Similarly, transitions
local states known since P (s0 |s, a1 , a2 ) = P1 (s01 |s1 , a1 ) P2 (s02 |s2 , a2 ).
paper, concentrate terminal actions necessarily communication
actions. assume options terminated whenever least one agents
initiates communication (i.e., option message sender terminates communicates hearers option terminates due external event). also assume
joint exchange messages, i.e., whenever one agent initiates communication,
global state system revealed agents receiving messages:
agent 1 sends observation o1 agent 2, also receive agent 2s observation o2 .
exchange messages cost system once. Since focus finite-horizon
processes, options may also artificially terminated time limit problem
reached. cost communication C may include, addition actual transmission
cost, cost resulting time takes compute agents local policies.
Communication-based decomposition mechanisms enable agents operate separately certain periods time. question, then, design mechanisms
approximate best optimal joint policy decentralized problem. distinguish
three cases: general options, restricted options, predefined options.
General options built primitive domain action communication action
given model problem. Searching possible pairs local single-agent
policies communication policies built general options lead best
approximation. obtained compute optimal mechanism among possible
mechanisms. Restricted options limit space feasible options much smaller set de174

fiCommunication-Based Decomposition Mechanism

fined using certain behavior characteristics. Consequently, obtain mechanisms
lower complexity. tractable mechanisms provide approximation solutions decentralized problems efficient algorithms currently exist. Obtaining optimal
mechanism certain set restricted options (e.g., goal-oriented options) becomes feasible, show Sections 4-6. Furthermore, sometimes, may consider options
pre-defined. example, knowledge effective individual procedures may already exist. mechanism approach allows us combine domain knowledge solution
decentralized problem. situations, mapping global states
single-agent behaviors already exists, computation mechanism returns policy
communication meta-level control synchronizes agents partial information. Section 7, study greedy approach computing policy communication
knowledge local behaviors given.
Practical concerns lead us study communication-based decomposition mechanisms. order design applicable mechanisms, two desirable properties need
considered:
Computational complexity whole motivation behind mechanism approach based idea mechanism low computational complexity. Therefore, computation CDM mapping practical
sense individual behaviors agent complexity lower
complexity decentralized problem free communication.
trade-off complexity computing mechanism global reward
system. may simple way split decentralized process
separate local behaviors. complexity characteristic taken account
designing mechanism; different mechanisms computed different levels
difficulty.
Dominance mechanism CDM1 dominates another mechanism CDM2
global reward attained CDM1 policy communication larger
global reward attained CDM2 communication policy. mechanism
optimal certain problem mechanism dominates it.

4. Decentralized Semi-Markov Decision Problems
Solving decentralized MDP problems communication-based decomposition mechanism translates computing set individual temporally abstracted actions
agent perform together policy communication stipulates
exchange information. Hereafter, show problem computing mechanism
formalized semi-Markov decision problem. particular, set basic actions
process composed temporally abstracted actions together communication actions. rest paper presents three algorithms aimed solving
semi-Markov problem optimally. algorithms differ sets actions available
decision-makers, affecting significantly complexity finding decentralized solution.
noted optimality mechanism computed conditioned
assumptions algorithm (i.e., first algorithm provides optimal mechanism
possible options, second algorithm provides optimal mechanism local goals
assumed, last algorithm computes optimal policy communication assum175

fiGoldman & Zilberstein

ing local behaviors given). Formally, decentralized semi-Markov decision
problem direct communication (Dec-SMDP-Com) given follows:
Definition 5 (Dec-SMDP-Com) factored, finite-horizon Dec-SMDP-Com underlying Dec-MDP-Com tuple
< , Opt1 , Opt2 , P N , RN > where:
S, , C , 1 , 2 , P , components underlying process defined
definitions 4 1.
Opti set actions available agent i. comprises possible options
agent choose perform, terminate necessarily communication act:

opti =< : Si Ai , Si >.
P N (s0 , t+N |s, t, opt1 , opt2 ) probability system reaching state s0 exactly
N time units, least one option terminates (necessarily communication
act). probability function given part model every value N ,
+ N . framework, N time steps least one agent initiates
communication (for first time since time t) interrupts option
hearer agent. Then, agents get full observability synchronized state. Since
decentralized process independent transitions observations, P N probability either agent communicated have. probability
agent terminated option exactly time + N , PiN , given follows:

1




0




0
0
PiN (s0i , t+N |si , t, opti ) =








P






(opti (si , t) ) (N = 1) (s0i = si ))
(opti (si , t) ) (N = 1) (s0i 6= si ))
(opti (si , t) A) (N = 1))
(opti (si , t) ) (N > 1))

(opti (si , t) A) (N > 1))
N 0
P
(q
|s


, opti (si , t))Pi (si , (t+1)+(N 1)|qi , t+1, opti )
qi Si

single-agent probability one policy option instructs agent
communicate (i.e., opti (si , t) ), case local process remains
local state.
use notation = (s1 , s2 ) s0 = (s01 , s02 ) refer agents local state.
N
Then, denote P (s0i , t+N |si , t, opti ) probability agent reach state
0
si N time steps follows option opti . refers probability
reaching state s0i without terminated option necessarily
state reached. transition probability computed recursively since
transition probability underlying Dec-MDP known:
N
P (s0i , t+N |si , t, opti )

=


0
Pi (si |si , opti (si , t)))

N = 1
otherwise
P
N 0
0
qi Si P (si |qi , opti (qi , t))P (si , (t+1)+(N 1)|si , t+1, opti )

Finally, obtain that:
N

P N (s0 , t+N |s, t, opt1 , opt2 ) = P1N (s01 , t+N |s1 , t, opt1 ) P 2 (s02 , t+N |s2 , t, opt2 )+
N

P2N (s02 , t+N |s2 , t, opt2 ) P 1 (s01 , t+N |s1 , t, opt1 )
P1N (s01 , t+N |s1 , t, opt1 ) P2N (s02 , t+N |s2 , t, opt2 )
176

fiCommunication-Based Decomposition Mechanism

RN (s, t, opt1 , opt2 , s0 , t+N ) expected reward obtained system N time steps
agents started options opt1 opt2 respectively state time t,
least one terminated option communication act (resulting
termination agents option). reward computed t+N .
(
0

N

R (s, t, opt1 , opt2 , , t+N ) =

C(opt1 , opt2 , s, s0 , N )
t+N =
0
C(opt1 , opt2 , s, , N ) + C otherwise

C(opt1 , opt2 , s, s0 , N ) expected cost incurred system transitions
states s0 least one agent communicates N time steps.
define probability certain sequence global states transitioned
system agent follows corresponding option P (< s0 , s1 , . . . , sN >):
0

1

N

P (< , , . . . , >) =

N1


P (sj+1 |sj , opt1 (sj1 ), opt2 (sj2 ))

j=0

normalizing factor makes sure possible sequences, probability adds one given s0 , sN N steps going intermediate steps s1 , . . . , sN 1 . Then, denote Rseq reward attained system traverses certain sequence states. Formally, Rseq (< s0 , . . . , sN >
P
j
j
j
j
j+1 )
) = N1
opti (si )) refers primitive action
j=0 R(s , opt1 (s1 ), opt2 (s2 ),
chosen option local state sji . Finally, define expected
cost C(opt1 , opt2 , s, s0 , N ) follows:
C(opt1 , opt2 , s, s0 , N ) =

X

P (< s, q 1 , . . . , q N1 , s0 >)Rseq (< s, q 1 , . . . , q N1 , s0 >)

q 1 ,...,q N1

dynamics semi-Markov decentralized process follows. agent performs option starting global state fully observed. agents option
mapping local states actions, agent starts option state si time
terminates state s0i , k time steps later. Whenever options terminated,
agents fully observe global state due terminal communication actions.
reach state s0 time t+k < , joint policy chooses possible different pair
options state s0 time t+k process continues.
Communication model leads joint exchange messages. Therefore
agents observe global state system information exchanged. means
states decentralized semi-Markov process fully-observable (as opposed
jointly fully-observable states classical Dec-MDP-Com).
local policy agent Dec-SMDP-Com mapping global states
options (as opposed mapping sequences observations general
Dec-POMDP case, mapping local state Dec-MDPs independent
transitions observations):
: Opti
177

fiGoldman & Zilberstein

joint policy tuple local policies, one agent, i.e., joint policy instructs
agent choose option global state. Thus, solving optimal mechanism equivalent solving optimally decentralized semi-Markov decision problems
temporally abstracted actions.
Lemma 1 Dec-SMDP-Com equivalent multi-agent MDP.
Proof. Multiagent MDPs (MMDPs) represent Markov decision process controlled
several agents (Boutilier, 1999). One important feature model
agents central view global state. Formally, MMDPs tuples form
< Ag, {Ai }iAg , S, P r, R >, where:





Ag finite collection n agents.
{Ai }iAg represents joint action space.
finite set system states.
P r(s0 |s, a1 , . . . , ) transition probability global states s0
agents perform joint action.
R : < reward system obtains global state reached.
decentralized semi-Markov problem direct communication solved optimally solving corresponding MMDP. simpicity exposition show proof
systems two agents. Following Definition 5, 2-agent Dec-SMDP-Com given
tuple: < , Opt1 , Opt2 , P N , RN >. mapping two models
follows: Ag finite collection agents control MMDP semiMarkov process. set set states world cases. MMDP
model, states fully observable definition. semi-Markov decentralized
model global states also fully observable agents always exchange information end option perform. set joint actions {Ai }iAg given
semi-Markov process set options available agent (e.g., n = 2
{Ai } = {Opt1 , Opt2 }). difference joint actions chosen primitive
domain actions MMDP options temporarily abstracted actions
terminate communication act. probability transition reward functions
easily mapped models matching P N P r RN R.
solution MMDP (or Dec-SMDP-Com) problem strategy assigns
joint action (or set options) global state. Solving MMDP actions
given options solves semi-Markov problem. Solving semi-Markov problem
options length two, i.e., option composed exactly one primitive action
followed communication action tells agent communicate observation
solves corresponding MMDP problem.
2
Solving decentralized semi-Markov process communication P-complete
Lemma 1 polynomial complexity single agent MDPs (Papadimitriou & Tsitsiklis, 1987). However, input problem includes states also
double exponential number domain actions agent. explained next
section, option represented tree, where: 1) depth option limited finite horizon 2) branching factor option constrained
number states S. Therefore, maximal number leaves option might
178

fiCommunication-Based Decomposition Mechanism



bounded |S|T . Consequently, |A||S| assignments primitive domain
communication acts leaves possible option.
naive solution Dec-SMDP-Com problem search space possible
pairs options find pair maximizes value global state. multistep policy-iteration algorithm, presented Section 5, implements heuristic version
search converges optimal mechanism. resulting search space (after pruning)
become intractable even simple small problems. Therefore, propose
apply communication-based decomposition mechanisms restricted sets options.
Solving Dec-SMDP-Com restricted set options means find optimal policy
attains maximal value possible options restricted set (Sutton et al.,
1999; Puterman, 1994). Sections 6 7 present two additional algorithms solve
Dec-SMDP-Com problems options considered goal-oriented options, i.e.,
mechanism assigns local goals one agents global state, allowing
communicate reached local goals.

5. Multi-step Backup Policy-Iteration Dec-SMDP-Com
Solving Dec-SMDP problem optimally means computing optimal pair options
fully-observable global state. options instruct agents act independently information exchanged. order find options
global state, apply adapted extended version multi-step backup policyiteration algorithm heuristic search (Hansen, 1997). show decentralized
version algorithm converges optimal policy decentralized case
temporally abstracted actions.
extend model single-agent POMDP observations costs DecSMDP-Com model. global perspective, agent follows option
without knowing global state system, following open-loop policy. However,
locally, agent following option, depend agents local observations. first define multi-step backup options, s0 global states
decentralized problem: V (s, t, ) =
min{b,Tt}

max
{
opt1 ,opt2 OPTb

X

X

k=1

s0

P N (s0 , t+k|s, t, opt1 , opt2 )[RN (s, t, opt1 , opt2 , s0 , t+k) + V (s0 , t+k, )]}

OPTb set options length b, length defined follows:
Definition 6 (The length Option) length option k option
perform k domain actions one execution.
Hansens work, b bound length options (k b). Here, finite
horizon Dec-SMDP-Com case analyzed, therefore b . P N (s0 , t+k|s, t, opt1 , opt2 )
RN (s, t, opt1 , opt2 , s0 , t+k) taken Dec-SMDP-Com model (Definition 5).
apply multi-step backup policy-iteration algorithm (see Figure 2) using
pruning rule introduced Hansen (1997), adapt work pairs policies
instead linear sequences actions. resulting optimal multi-step backup policy
equivalent optimal policy MMDP (Lemma 1), i.e., equivalent optimal
decentralized policy Dec-SMDP-Com temporally abstracted actions. order
179

fiGoldman & Zilberstein

s1

a1

s1

s2

s3

a1

!2

!3

s1

s4

a2

a3

Figure 1: policy tree size k=3.
explain pruning rule decentralized case temporally abstracted actions,
define policy-tree structures are.
Definition 7 (Policy-tree) policy-tree tree structure, composed local state nodes
corresponding action nodes level. Communication actions assigned
leaves tree. edges connecting action (taken parent state si )
resulting state s0i transition probability Pi (s0i |si , a) assigned them.
Figure 1 shows possible policy-tree. option represented policy-tree
leaves assigned communication actions. denote policy-tree sroot , state
assigned root (e.g., sroot ), assignment domain actions local states
rest nodes (e.g., ). size policy-tree defined follows:
Definition 8 (Size Policy-tree) size policy-tree k longest branch
tree, starting root composed k 1 edges (counting edges
actions resulting states). policy-tree size one includes root state action
taken state.
(k ) policy induced assignment k actions implementation. expected cost g policy-tree sroot k expected cost incurred
agent follows policy (k ). denote set nodes tree
correspond leaves N L set states assigned SN L . notation
\ n refers assignment excluding node n. expected cost tree, g(sroot k ),
computed follows:
(

g(sroot k ) =

C(aroot )
k = 1
P
C(aroot ) + s0 SN L [P r(s0i |sroot , aroot )g(s0i ( \ root)k1 )] 1 < k


Since decentralized process factored states, write global state
pair (s1 , s2 ). agent act independently period time k
performs option. Therefore, refer information state system
k time steps s1 ks2 k , s1 k s2 k correspond agents policy tree
size k. assume least one agent communicates time t+k. necessarily
180

fiCommunication-Based Decomposition Mechanism

interrupt agents option time t+k. Therefore, sufficient look
pairs trees size k. information state refers belief agent
forms world based partial information available operates
locally. model, agents get full observability communicate exchange
observations.
heuristic function used search optimal decentralized joint
policy Dec-SMDP-Com follows traditional notation, i.e., f (s) = g(s) + h(s).
case, functions defined pairs policy-trees, i.e., f (sk k ) =
G(sk k ) + H(sk k ). f value denotes backed-up value implementing policies
(k ) (k ), respectively two agents, starting state time t. expected
value state time horizon given multi-step backup state
follows:
V (s, t, ) = max {f (s)}.
||,|| b

Note policy-trees corresponding assignments size
b . define expected cost implementing pair policy-trees, G, sum
expected costs one separately. leaves communication actions,
cost communication taken account g functions. Hansens work,
leaves assigned communication action, assume agents sense
cost compute f function.
G(s1 ks2 k ) = g(s1 k ) + g(s2 k ).
option policy-tree communication actions assigned leaves.
option denoted opt1 (k ) (or opt2 (k )). message associated leaf corresponds
local state assigned leaf (or ). define expected value
perfect information information state k time steps:
H(sk k ) =

X

P N (s0 , t+k|s, t, opt1 (k ), opt2 (k ))V (s0 , t+k, )

s0

multi-step backup policy-iteration algorithm adapted Hansen decentralized control case appears Figure 2. Intuitively, heuristic search possible
options unfolds follows: node search space composed two policy-trees,
representing local policy one agent. search advances nodes whose f
value (considering trees) greater value global root state (composed
roots policy-trees). nodes whose f value follow inequality
actually pruned used updating joint policy. policy updated
node, composed two options found f > V . leaves
options (at possible depths) include communication acts. updated policy 0 maps
global state two options. leaves one policy-tree current
depth communication actions assigned, algorithm assigns communication acts
leaves policy-tree depth. change policies correct joint exchange information (i.e., actions interrupted
least one agent communicates). notice, though, may leaves
policy-trees depths lower may still domain actions assigned. Therefore,
policy-trees cannot considered options yet remain stack. leaves
181

fiGoldman & Zilberstein

1. Initialization: Start initial joint policy assigns pair options global state s.
2.
Evaluation: S, V (s, t, ) =
PTPolicy
P
P N (s0 , t+k|s, t, opt1 (s1 , t), opt2 (s2 , t))[RN (s, t, opt1 (s1 , t), opt2 (s2 , t), s0 , t+k) + V (s0 , t+k, )]
k=1
s0
3. Policy Improvement: state = (s1 , s2 ) :
a. Set-up:
Create search node possible pair policy-trees length 1: (s1 1 , s2 1 ).
Compute f (s1 1 ) = G(s1 1 ) + H(s1 1 ).
Push search node onto stack.
b. search stack empty, do:
i. Get next pair policy-trees:
Pop search node stack let (s1 , s2 )
(the policy-trees length starting state = (s1 , s2 ))
Let f (si ) estimated value.
ii. Possibly update policy:
(f (si ) = G(si ) + H(si )) > V (s, t, ),
leaves depth either communication action assigned,
Assign communication action leaves policy-tree depth
leaves depths communication action assigned,
Denote new two options opti1 opti2 .
Let 0 (s) = (opti1 , opti2 ) V (s, t, ) = f (si ).
iii. Possibly expand node:
(f (si ) = G(si ) + H(si )) > V (s, t, ),
((some leaves either domain actions assigned)
((i+2) ))
/*At t+1 new action taken transition another state t+2*/
Create successor node two policy-trees length i,
adding possible transition states actions leaf tree
communication action assigned it.
Calculate f value new node (i.e., either f (si+1 i+1 ) policy
trees expanded, recalculate f (si ) one communication
actions leaves depth i)
Push node onto stack.
/*All nodes f < V pruned pushed stack.*/
4. Convergence test:
= 0
return 0
else set = 0 , GOTO 2.

Figure 2: Multi-step Backup Policy-iteration (MSBPI) using depth-first branch-and-bound.

remain assigned domain actions expanded algorithm. expansion
requires addition possible next states, reachable performing
domain-actions leaves, addition possible action state.
leaves depth one policy-tree already assigned communication acts,
algorithm expands leaves domain actions lower levels policy-trees.
leaf expanded beyond level corresponding time one agent
going initiate communication option going interrupted anyways.
next section, show convergence Multi-step Backup Policy-iteration
(MSBPI) algorithm optimal decentralized solution Dec-SMDP-Com,
agents follow temporally abstracted actions horizon finite.
182

fiCommunication-Based Decomposition Mechanism

5.1 Optimal Decentralized Solution Multi-step Backups
section, prove MSBPI algorithm presented Figure 2 converges
optimal decentralized control joint policy temporally abstracted actions direct
communication. first show policy improvement step algorithm based
heuristic multi-step backups improves value current policy sub-optimal.
Finally, policy iteration algorithm iterates improving policies known
converge.
Theorem 1 current joint policy optimal, policy improvement step
multi-step backup policy-iteration algorithm always finds improved joint policy.
Proof. adapt Hansens proof decentralized control problem, policies
represented policy-trees. Algorithm MSBPI Figure 2 updates current policy
new policy assigns pair options yield greater value certain global state.
show induction size options, least one state, new option
found improvement step (step 3.b.ii).
value state improved two policy-trees size one, improved
joint policy found policy-trees size one evaluated. initialized
policy-trees. assume improved joint policy found policytrees size k. show improved joint policy found policy-trees
size k. Lets assume k policy tree size k, f (sk ) > V (s)
communication actions assigned leaves. case policy followed
agent 2 interrupted time k latest. One possibility sk
evaluated algorithm. Then, improved joint policy indeed found. pair
policy-trees evaluated algorithm, means pruned earlier.
assume happened level i. means f (si ) < V (s). assumed
f (sk ) > V (s) obtain that: f (sk ) > f (si ).
expand f values inequality, obtain following:
g(si )+g(s)+

X

P N (s0 , t+i|s, opt1 (i ), opt2 ())[g(s0 (i, k))+g(s0 )+

X

P N (s00 , t+i+ki)V (s00 )] >

s00

s0

g(si ) + g(s) +

X

P N (s0 , t+i|s, opt1 (k ), opt2 ())V (s0 , t+i)

s0

g(s0 (i, k)) refers expected cost subtree starting level ending
level k starting s0 . simplification obtain:
X

P N (s0 , t+i|s, opt1 (i ), opt2 ())[g(s0 (i, k)) + g(s0 ) +

s0

X

P N (s00 , t+i+ki)V (s00 )] >

s00

X

P N (s0 , t+i|s, opt1 (k ), opt2 ())V (s0 , t+i)

s0

is, exists state s0 f (s0 (i, k)) > V (s0 ). Since policy-tree
(i, k) size less k, induction assumption obtain exists
state s0 multi-step backed-up value increased. Therefore, policy found
step 3.b.ii indeed improved policy.
2
183

fiGoldman & Zilberstein

Lemma 2 complexity computing optimal mechanism general options
(T 1)
MSBPI algorithm O(((|A1 | + ||)(|A2 | + ||))|S|
). (General options based
possible primitive domain action model, communication act).

Proof. agent perform primitive domain actions Ai communicate possible message . |S|(T 1) leaves policy tree
horizon |S| possible resulting states transition. Therefore, time
MSBPI algorithm expands policy tree (step 3.b.iii Figure 2), number result(T 1)
ing trees ((|A1 | + ||)(|A2 | + ||))|S|
. worst case, number trees
algorithm develop one iteration. Therefore, size search space
function number times number iterations convergence.
2
Solving optimally Dec-MDP-Com independent transitions observations
shown NP (Goldman & Zilberstein, 2004a). show here, solving
optimal mechanism harder, although solution may optimal. due
main difference two problems. Dec-MDP-Com, know
since transitions observations independent, local state sufficient statistic
history observations. However, order compute optimal mechanism
need search space options, is, single local state sufficient statistic.
options allowed general, search space larger since possible
option needs considered arbitrarily large (with length branch
bounded ). example, Meeting Uncertainty scenario (presented
Section 7.1), agents aim meeting stochastic environment shortest time
possible. agent choose perform anyone six primitive actions (four move
actions, one stay action communication action). Even small world composed
100 possible locations, implementing MSBPI algorithm intractable. require
expansion possible combinations pairs policy-trees leading possible
(T 1)
addition 36100
nodes search space iteration. Restricting mechanism
certain set possible options, example goal-oriented options leads significant
reduction complexity algorithm shown following two sections.

6. Dec-SMDP-Com Local Goal-oriented Behavior
previous section provided algorithm computes optimal mechanism, searching possible combinations domain communication actions agent.
one hand, solution general restrict individual
behaviors aspect. hand, solution may require search
large space, even space pruned heuristic search technique. Therefore,
order provide practical decomposition mechanism algorithm, reasonable restrict
mechanism certain sets individual behaviors. section, concentrate
goal-oriented options propose algorithm computes optimal mechanism
respect set options: i.e., algorithm finds mapping global state
set locally goal oriented behaviors highest value. algorithm proposed
structure MSBPI algorithm; main difference options
built.
184

fiCommunication-Based Decomposition Mechanism

Definition 9 (Goal-oriented Options) goal-oriented option local policy achieves
given local goal.
study locally goal-oriented mechanisms, map global state pair
goal-oriented options period time k. assume set local goals Gi
provided problem. local goal, local policy achieve
considered goal-oriented option. mechanism applied, agent follows
policy corresponding local goal k time steps. time k + 1, agents exchange
information stop acting (even though may reached local goal).
agents, then, become synchronized assigned possibly different local goals
working period k 0 .
algorithm presented section, LGO-MSBPI, solves decentralized control
problem communication finding optimal mapping global states local
goals periods time (the algorithm finds best solution given agents
act individually periods time). start arbitrary joint policy
assigns one pair local goal states number k global state. current joint
policy evaluated set current best known mechanism. Given joint policy
: G1 G2 , (Gi Si S), value state time t,
finite horizon given Equation 1: (this value computed states
t+k ).

0
=
P
0
0
N
N 0
(s
,
t+k|s,
t,

(s
),

(s
P
V (s, t, ) =
g1 1
g2 2 ))[Rg (s, t, g1 , g2 , , k)+V (s , t+k, )] (1)
s0 g

s.t. (s, t) = (g1 , g2 , k)

Notice RgN (s, g1 ,g2 , s0 , k) defined similarly RN () (see Definition 5), taking
account options aimed reaching certain local goal state (g1
g2 aimed reaching local goal states g1 g2 , respectively).
RgN (s, t, g1 , g2 , s0 , k) = C(g1 , g2 , s, s0 , k)+C =
C +

X

P (< s, q 1 , . . . , q k1 , s0 >) Cseq (< s, q 1 , . . . , sk1 , s0 >

q 1 ,...,q k1

one-to-one mapping goals goal-oriented options. is,
policy gi assigned found agent independently solving optimally
agents local process DPi = (Si , Pi , Ri , Gi , ): set global states factored
agent set local states. process independent transitions,
Pi primitive transition probability known described options framework.
Ri cost incurred agent performs primitive action ai zero
agent reaches goal state Gi . finite horizon global problem.
PgN (with goal g subscript) different probability function P N appears
Section 4. PgN probability reaching global state s0 k time steps,
trying reach g1 g2 respectively following corresponding optimal local policies.
PgN (s0 , t+k|s, t+i, g1 (s1 ), g2 (s2 )) =
185

fiGoldman & Zilberstein

= k = s0
= k =
6 s0
< k


1




0






P


N 0





P (s |s, g1 (s1 ), g2 (s2 )) Pg (s , t+k|s , t+i+1, g1 (s1 ), g2 (s2 ))


s.t. (s, t+i) = (g1 , g2 , k)

iteration LGO-MSBPI algorithm (shown Figure 3) tries improve
value state testing possible pairs local goal states increasing number
time steps allowed communication. value f computed mapping
states assignments local goals periods time. f function given
global state, current time, pair local goals given period time k expresses cost
incurred agents acted k time steps communicated time
k+1, expected value reachable states k time steps (these states
reached agents following corresponding optimal local policies towards g1
g2 respectively). current joint policy updated f value state
s, time t, local goals g1 g2 period k greater value V (s, t, ) computed
current best known assignment local goals period time. Formally:
f (s, t, g1 , g2 , k) = G(s, t, g1 , g2 , k) + H(s, t, g1 , g2 , k)

(2)

G(s, t, g1 , g2 , k) = C(g1 , g2 , s, t, k) + C

(3)

H(s, t, g1 , g2 , k) =



0

=
<

P 0 P N (s0 , + k|s, t, (s ), (s ))V (s0 , + k, )
g1 1
g2 2
g

(4)

C(g1 , g2 , s, t, k) expected cost incurred agents following corresponding options k time steps starting state s. defined similarly
expected cost explained Definition 5. notice computation f refers
goals evaluated algorithm, evaluation policy (step 2) refers
goals assigned current best policy.
6.1 Convergence Algorithm Complexity
Lemma 3 algorithm LGO-MSBPI Figure 3 converges optimal solution.
Proof. set global states set local goal states Gi finite.
horizon also finite. Therefore, step 3 algorithm terminate. Like
classical policy-iteration algorithm, LGO-MSBPI algorithm also converge
finite numbers calls step 3 policy improve value one
iteration another.
2
Lemma 4 complexity computing optimal mechanism based local goal-oriented
behavior following LGO-MSBPI algorithm polynomial size state space.
Proof. Step 2 LGO-MSBPI algorithm computed dynamic programming
polynomial time (the value state computed backwards manner finite
horizon ). complexity improving policy Step 3 polynomial time,
186

fiCommunication-Based Decomposition Mechanism

1. Initialization: Start initial joint policy assigns local goals
gi Gi time periods k N
S, : (s, t) = (g1 , g2 , k)
2. Policy Evaluation: S, Compute V (s, t, ) based Equation 1.
3. Policy Improvement:
a. k = 1
b. (k < )
i. s, t, g1 , g2 : Compute f (s, t, g1 , g2 , k) based Equations 2,3 4.
ii. Possible update policy
f (s, t, g1 , g2 , k) > V (s, t, )
(s, t) (g1 , g2 , k) / Communicate k + 1 /
V (s, t, ) f (s, t, g1 , g2 , k)
iii. Test joint policies next extended period time
k k+1
4. Convergence test:
change Step 3
return
else GOTO 2.

Figure 3: Multi-step Backup
(LGO-MSBPI).

Policy-iteration



local

goal-oriented

behavior

number states number goal states, i.e., O(T 2 |S||G|). worst case, every
component global state local goal state. However, cases, |Gi |
much smaller |Si | Gi strict subset Si , decreasing even running
time algorithm.
2
6.2 Experiments - Goal-oriented Options
illustrate LGO-MSBPI decomposition mechanism production control scenario.
assume two machines, control production boxes
cereals: machine M1 produce two types boxes b. amount boxes type
produced machine denoted Ba (Bb represents amount boxes type b
produced respectively). Machine M2 produce two kinds cereals b. Ca (and Cb
respectively) denotes number bags cereals type (we assume one bag
cereals sold one box type). boxes differ presentation
boxes type advertise content type boxes type b advertise content
type b. assume discrete time t, machine M1 may produce one box
boxes all, machine may produce one bag cereals may produce
cereal all. production process stochastic sense machines
perfect: probability PM1 , machine one succeeds producing intended box
(either b) probability 1 PM1 , machine produce box
particular time unit. Similarly, assume PM2 expresses probability machine two
producing one bag cereals type b required selling one box.
example, reward attained system equal number products ready
187

fiGoldman & Zilberstein

sale, i.e., min{Ba , Ca } + min{Bb , Cb }. product sold composed one
box together one bag cereals corresponding type advertised box.
goal-oriented option given number products machine
produce. Therefore, option opti scenario described pair numbers (Xa , Xb )
(when machine one X refers boxes machine two, X refers bags
cereals). is, machine instructed produce Xa items type a, followed
Xb items type b, followed Xa items type forth either time limit
anyone machines decides communicate. machines exchange
information, global state revealed, i.e., current number boxes cereals
produced far known. Given set goal-oriented options, LGO-MSBPI algorithm
returned optimal joint policy action communication solves problem.
counted time units takes produce boxes cereals. compared
locally goal oriented multi-step backup policy iteration algorithm (LGO-MSBPI)
two approaches: 1) Ideal case machines exchange information
state production time cost. idealized case, since
reality exchanging information incur cost, example changing setting
machine takes valuable time 2) Always Communicate ad-hoc case,
machines exchange information time step also incur cost
it. Tables 1, 2, 3 present average utility obtained production system
cost communication set 0.1, 1 10 respectively, cost
domain action set 1 joint utility averaged 1000 experiments.
state represented tuple (Ba , Bb , Ca , Cb ). initial state set (0,0,0,8),
boxes produced already 8 bags cereals type B.
finite horizon set 10. set goal-oriented options (Xa , Xb ) tested included
(0,1),(1,4),(2,3),(1,1),(3,2),(4,1) (1,0).

PM1 , PM2
0.2, 0.2
0.2, 0.8
0.8, 0.8

Ideal C = 0
-17.012
-16.999
-11.003

Average Utility
Always Communicate
-18.017
-17.94
-12.01

LGO-MSBPI
-17.7949
-18.0026
-12.446

Table 1: C = 0.10, Ra = 1.0.

PM1 , PM2
0.2, 0.2
0.2, 0.8
0.8, 0.8

Ideal C = 0
-17.012
-16.999
-11.003

Average Utility
Always Communicate
-26.99
-26.985
-20.995

LGO-MSBPI
-19.584
-25.294
-17.908

Table 2: C = 1.0, Ra = 1.0.
LGO-MSBPI algorithm computed mechanism resulted three products
average uncertainty least one machine set 0.2 1000 tests
run, ten time units. number products increased average 8
9 products machines succeeded 80% cases. numbers products
188

fiCommunication-Based Decomposition Mechanism

PM1 , PM2
0.2, 0.2
0.2, 0.8
0.8, 0.8

Ideal C = 0
-17.012
-16.999
-11.003

Average Utility
Always Communicate
-117
-117.028
-110.961

LGO-MSBPI
-17.262
-87.27
-81.798

Table 3: C = 10.0, Ra = 1.0.
always attained either decomposition mechanism implemented
ad-hoc approaches tested. Ideal Always Communicate algorithms differ
respect cost communication, differ actual policies
action. Although machines incur higher cost mechanism applied compared
ideal case (due cost communication), number final products ready
sell almost amount. is, take time order
produce right amount products policies implemented computed
locally goal oriented multi-step backup policy iteration algorithm. cost
communication scenario capture cost changing setting one machine
one production program another. Therefore, result significant cost
communication high compared time whole process takes.
decomposition mechanism finds times beneficial synchronize information
constant communication feasible desirable due high cost.
6.3 Generalization LGO-MSBPI Algorithm
mechanism approach assumes agents operate independent
period time. However, decentralized process kind dependency
observations transitions, assumption violated, i.e., plans reach
local goals interfere (the local goals may compatible).
LGO-MSBPI algorithm presented paper applied Dec-MDPs
transitions observations assumed independent. section, bound
error utilities options computed LGO-MSBPI algorithms
dependencies exist. define independent decentralized processes refer nearlyindependent processes whose dependency quantified cost marginal
interactions.
Definition 10 (independent Process) Let C Ai (s gk |gj ) expected cost incurred agent following optimal local policy reach local goal state gk
state s, agent following optimal policy reach gj . decentralized
control process independent = max{1 , 2 }, 1 2 defined
follows: g1 , g10 G1 S1 , g2 , g20 G2 S2 S:
1 = max{max{max0 {C A1 (s0 g1 |g20 ) C A1 (s0 g1 |g2 )}}}


g1

g2 ,g2

2 = max{max{max0 {C A2 (s0 g2 |g10 ) C A2 (s0 g2 |g1 )}}}


g2

g1 ,g1

189

fiGoldman & Zilberstein

is, maximal difference cost agent may incur trying
reach one local goal state interferes possible local goal reached
agent.
computation cost function C Ai (s gk |gj ) domain dependent.
address issue compute cost provide condition. individual
costs one agent affected interference exists pair local
goals. example, assume 2D grid scenario: one agent move four directions
(north, south, east west) needs reach location (9,9) (0,0). second agent
able moving also collecting rocks blocking squares grid. Assuming
second agent assigned task blocking squares even rows,
first agents solution task constrained squares free cross.
case, agent ones cost reach (9,9) depends path choose depends
strongly state grid resulting second agents actions.
value denotes amount interference might occur agents
locally goal-oriented behaviors. Dec-MDP independent transitions observations, value zero. LGO-MSBPI algorithm proposed paper
computes mechanism global state mapping states pairs local
goal states ignoring potential interference. Therefore, difference actual
cost incurred options found algorithm optimal options
. Since mechanism applied global state time steps
loss cost occur worst case agents, algorithm presented
2T optimal general case.

7. Myopic-greedy Approach Direct Communication
cases, reasonable assume single-agent behaviors already known
fixed, ahead time possible global state. example, may occur
settings individual agents designed ahead coordination time (e.g., agents
manufacturing line represent machines, built specifically implement
certain procedures). achieve coordination, though, additional method may
needed synchronize individual behaviors. section, present apply
communication-based decomposition approach compute policy communication
synchronize given goal-oriented options. take myopic-greedy approach
runs polynomial-time: i.e., time agent makes decision, chooses
action maximal expected accumulated reward assuming agents able
communicate along whole process. Notice LGO-MSBPI
general sense also computed local goals pursued
agent together communication policy synchronizes individual behaviors.
Here, time agents exchange information, mechanism applied inducing two
individual behaviors (chosen given mapping states individual behaviors).
given optimal policies action (with communication actions) denoted 1A
2A respectively.
expected global reward system, given agents communicate
follows corresponding optimal policy iA given value
initial state s0 : nc (s0 , 1A , 2A ). value computed summing possible
190

fiCommunication-Based Decomposition Mechanism

next states computing probability agent reaching it, reward obtained
recursive value computed next states.
nc (s0 , 1A , 2A ) =
X

P1 (s01 |s01 , 1A (s01 )) P2 (s02 |s02 , 2A (s02 ))(R(s0 , 1A (s01 ), 2A (s02 ), s0 ) + nc (s0 , 1A , 2A ))

(s01 ,s02 )

denote expected cost system computed agent i, last synchronized
state s0 , agents communicate state continue without
communication, c (s0 , si , 1A , 2A ):
c (s0 , s1 , 1A , 2A ) =
X

P2 (s2 |s02 , 2A )(R(s0 , 1A (s01 ), 2A (s02 ), (s1 , s2 )) + nc ((s1 , s2 ), 1A , 2A ) + C F lag)

s2

Flag zero agents reached global goal state reached state s.
time stamp state denoted t(s). P (s|, s0 , 1A , 2A ) probability reaching state
state s0 , following given policies action.

1


P (s0 |s, (s ), (s ))
1
2
1
2
P (s0 |s, 1A , 2A ) =
0P

00

0 00

s00 P (s |s , 1 , 2 ) P (s |s, 1 , 2 )

= s0
t(s0 ) = t(s) + 1
t(s0 ) < t(s) + 1
otherwise

Similarly, P1 (P2 ) defined probability reaching s01 (s02 ), given agent 1 (2)s
current partial view s1 (s2 ) policy action 1A (2A ). accumulated reward
attained agents move state s0 state given follows:

R(s0 , 1A (s01 ), 2A (s02 ), s)
t(s) = t(s0 ) + 1



t(s) > t(s0 ) + 1
P
00
00
R(s0 , 1A , 2A , s) =
0

2 , )

s00 P (s |1 , 200 , ) P00(s|1 , 00

00

0

(R(s , 1 , 2 , ) + R(s , 1 (s1 ), 2A (s2 ), s))

state, agent decides whether communicate partial view based
whether expected cost following policies action, communicated
larger smaller expected cost following policies action
communicated.
Lemma 5 Deciding Dec-MDP-Com myopic-greedy approach direct communication P class.
Proof. agent executes known policy iA mechanism applied. local
goals provided instead actual policies, finding optimal single-agent policies
reach goal states done polynomial time. complexity finding
communication policy dynamic programming (based formulas above),
therefore computing policy communication also P. |S| states
nc c need computed, one formulas solved time
polynomial |S|.
2
previous work, also studied set monotonic goal-oriented Dec-MDPs,
provide algorithm finds optimal policy communication assuming
set individual behaviors provided (Goldman & Zilberstein, 2004b).
191

fiGoldman & Zilberstein

7.1 Meeting Uncertainty Example
present empirical results obtained myopic-greedy approach applied
Meeting Uncertainty example4 . testbed consider sample problem DecMDP-Com involving two agents meet location early possible.
scenario also known gathering problem robotics (Suzuki & Yamashita, 1999).
environment represented 2D grid discrete locations. example,
global state occupied agents considered global goal state. set
control actions includes moving North, South, East West, staying
location. agents partial view (which locally fully-observable) corresponds
agents location coordinates. observations transitions independent.
outcomes agents actions uncertain: is, probability Pi , agent arrives
desired location taken move action, probability 1 Pi agent
remains location. Due uncertainty effects agents actions,
clear setting predetermined meeting point best strategy designing
agents. Agents may able meet faster change meeting place
realizing actual locations. achieved exchanging information
locations agents, otherwise observable. showed exchanging
last observation guarantees optimality Dec-MDP-Com process constant message
costs (Goldman & Zilberstein, 2004a). example tested, messages exchanged
correspond agents observations, i.e., location coordinates.
implemented locally goal-oriented mechanism assigns single local
goal agent synchronized state. local goals chosen location
middle shortest Manhattan path agents locations (this distance
revealed information exchanged).
Intuitively, desirable mechanism set meeting place middle
shortest Manhattan path connects two agents absence communication, cost meet point minimal. shown computing
expected time meet, nc , pair possible distances two agents
location grid, communication possible. simplify exposition,
use function takes advantage specific characteristics example.
notation follows: agent 1 distance d1 meeting location, agent 2
distance d2 location, system incurs cost one time period
agents met yet. agents meeting location, expected time
meet zero, nc (0, 0) = 0. agent 2 meeting location, agent 1
reached location yet, expected time meet given
nc (d1 , 0) = P1 (1 + nc (d1 1, 0)) + (1P1 ) (1 + nc (d1 , 0)) =
= P1 nc (d1 1, 0)) + (1P1 ) nc (d1 , 0)) 1

is, probability P1 agent 1 succeeds decreasing distance meeting location one, probability 1 P1 fails remains location. Recursively, compute remaining expected time meet updated parameters.
Similarly agent 2: nc (0, d2 ) = P2 (1 + nc (0, d2 1)) + (1P2 ) (1+nc (0, d2 )).
none agents reached meeting place yet, four different cases
either both, one, none succeeded moving right direction either
4. empirical results section described first Goldman Zilberstein (2003).

192

fiCommunication-Based Decomposition Mechanism

decreased distances meeting location respectively:
nc (d1 , d2 ) = P1 P2 (1 + nc (d1 1, d2 1)) + P1 (1P2 ) (1 + nc (d1 1, d2 ))+
+(1P1 ) P2 (1 + nc (d1 , d2 1)) + (1P1 ) (1P2 ) (1 + nc (d1 , d2 )) =
= P1 P2 nc (d1 1, d2 1) + P1 (1P2 ) nc (d1 1, d2 ) + (1P1 ) P2 nc (d1 , d2 1)+
+(1P1 ) (1P2 ) nc (d1 , d2 ) 1

value nc (d1 , d2 ) computed possible distances d1 d2 2D grid
size 10 10. minimal expected time meet obtained d1 = d2 = 9
expected cost 12.16.
summary, approximating optimal solution Meeting Uncertainty
example direct communication possible mechanism applied one described unfold follows: time t0 , initial state system s0 fully
observable agents. agents set meeting point middle Manhattan path connects them. Denote d0 distance agents t0
gt0 = (gt10 , gt20 ) goal state set t0 . one agents move optimally towards
corresponding component gt0 . agent moves independently environment
transitions observations independent. time t, policy
communication instructs agent initiate exchange information, current Manhattan distance agents dt revealed both. Then, mechanism applied,
setting possibly new goal state gt , decomposes two components one
agent. goal state gt middle Manhattan path connects agents
length dt revealed communication.
7.2 Experiments - Myopic-greedy Approach
following experiments, assumed transition probabilities P1 P2
equal. uncertainties specified parameter Pu . mechanism
applied whenever agents communicate time results agent adopting local
goal state, set location middle Manhattan path connecting
agents (the Manhattan distance agents revealed time t). compare
joint utility attained system following four different scenarios:
1. No-Communication meeting point fixed time t0 remains fixed along
simulation. located middle Manhattan path connects
agents, known time t0 . agent follows optimal policy action
location without communicating.
2. Ideal case assumes agents communicate freely (C = 0) every
time step resulting highest global utility agents attain. Notice,
though, optimal solution looking for, assume
communication free. Nevertheless, difference utility obtained
first two cases shed light trade-off achieved implementing
non-free communication policies.
3. Communicate SubGoals heuristic solution problem, assumes
agents notion sub-goals. notify sub-goals
achieved, eventually leading agents meet.
193

fiGoldman & Zilberstein

A1
A1

A2

A2
Time

new subgoal set agent 2 arrived
subgoal set time t.

Figure 4: Goal decomposition sub-goal areas.
4. Myopic-greedy Approach Agents act myopically optimizing choice
send message, assuming additional communication possible. possible
distance agents, policy communication computed
stipulates best time send message. iterating policy agents
able communicate thus approximate optimal solution
decentralized control problem direct communication. agents continue
moving meet.
solution No-Communication case solved analytically Meeting
Uncertainty example, computing expected cost nc (d1 , d2 ) incurred two
agents located distances d1 d2 respectively goal state time t0 (the complete
mathematical solution appears Section 7.1). Ideal case, set 1000 experiments
run cost communication set zero. Agents communicate locations
every time instance, update location meeting place accordingly. Agents move
optimally last synchronized meeting location.
third case tested (Communicate SubGoals) sub-goal defined cells
grid distance equal p d/2 center located d/2 one
agents. p parameter problem determines radius circle
considered sub-goal. time agent reaches cell inside area defined
sub-goal, initiates exchange information (therefore, p induces communication
strategy). expresses Manhattan distance two agents, value accurate
agents synchronize knowledge. is, time t0 agents determine
first sub-goal area bounded radius p d0 /2 and, center located
d0 /2 one agents. time agents synchronize information
communication, new sub-goal determined p dt /2. Figure 4 shows
new sub-goals set agents transmit actual location reached
sub-goal area. meeting point dynamically set center sub-goal area.
Experiments run Communicate SubGoals case different uncertainty values, values parameter p costs communication (for case, 1000 experiments
run averaged). results show agents obtain higher utility adjusting meeting point dynamically rather set one fixed meeting point. Agents
synchronize knowledge thus set new meeting location instead
acting two independent MDPs communicate move towards fixed meeting point (see Figure 5). Nevertheless, certain values p, joint utility agents
194

fiCommunication-Based Decomposition Mechanism

p ratio
-20.5
0

0.1

0.2

0.3
-20.768
-20.852

-21

-21.038
-21.13
-21.15

-21.132

0.4

0.5

-20.808
-20.8586 -20.824
-21.0888

0.6
-20.726

0.7
-20.758

-21.3665
-21.5

0.9
-20.804

-20.9302
-21.0255

-21.0433

-21.26

-21.2636

-21.284

0.8
-20.742

-21.3405

-21.4254
-21.5715

Avg. Joint Utlity

-21.7747
-21.9355

-22

-21.9794

-22.308
-22.414
-22.5

-22.581
-22.771

-23

Pu=0.8 Cost Com.=0
Pu=0.8 Cost Com.=0.1
Pu=08 Cost Com.=0.3
Pu=0.8 Cost Com.=0.5

-23.5

2MDPs Pu=0.8

-23.739
-24

Figure 5: average joint utility obtained sub-goals communicated.
actually smaller joint utility achieved No-Communication case (2 MDPs
figure). points need empirically tune parameters required
heuristic.
Myopic-greedy case, design agents optimize time
send message, assuming communicate once. off-line planning
stage, agents compute expected joint cost meet possible state
system (s0 ) time (included local state si ), c (s0 , si , 1A , 2A ). global states
revealed communication correspond possible distances agents.
time agents get synchronized, mechanism applied assigning local goals
instructing agents follow optimal local policies achieve them. Meeting
Uncertainty scenario study, c expected joint cost incurred taking control
actions time steps, communicating time + 1 agents met
far, continuing optimal policy control actions without communicating
towards goal state (the meeting location agreed upon + 1) expected cost
nc (d1 , d2 ) computed No-Communication case. agents meet
time steps elapsed, incur cost time act met.
time t, one agents knows meeting location, goal location
computed last exchange information. Consequently, agent moves optimally
towards goal state. addition, myopic-greedy policy communication found
computing earliest time t, c (d1 + d2 , s1 , 1A , 2A ) < nc (d1 , d2 ), is,
best time communicate expected cost meet least.
myopic-greedy policy communication vector states time communicate
possible distance agents.
Tables 4, 5, 6 present myopic-greedy communication policies computed
Meeting Uncertainty problem Pu values taken {0.2, 0.4, 0.6, 0.8}. cost
taking control action Ra = 1.0 costs communicating C tested
195

fiGoldman & Zilberstein

{0.1, 1.0, 10.0}. row corresponds configuration tested different statetransition uncertainties. column corresponds synchronized state, given
possible Manhattan distance agents moving 2D grid size 10x10. Given
certain value Pu certain global distance, agent interprets value
entry next time communicate position. Time reset zero
agents exchange information. long distance agents larger
communication cost increases, policy instructs agents communicate later, i.e.,
agents keep operating information exchanged better effect
rescheduling meeting place.
Pu
0.2
0.4
0.6
0.8

1
2
2
2
2

d0 =distance agents last synchronized, g
2 3 4 5 6 7 8 9 10 11 12 13 14
3 2 3 2 3 2 3 2 3
2
3
2
3
2 2 3 2 3 2 3 2 3
2
3
2
3
2 2 3 2 3 2 3 2 3
2
3
2
3
2 2 3 2 4 2 4 2 4
2
4
2
4

located
15 16
2
3
2
3
2
3
2
4

d0 /2
17
2
2
2
2

18
3
3
3
4

Table 4: Myopic-greedy policy communication: C = 0.1, Ra = 1.0.

Pu
0.2
0.4
0.6
0.8

1
3
2
2
2

d0 =distance agents last synchronized, g
2 3 4 5 6 7 8 9 10 11 12 13 14
4 3 5 3 6 4 7 4 7
5
7
5
8
3 3 4 4 5 4 6 5 7
5
7
6
8
2 3 4 4 5 5 6 6 7
6
8
7
8
2 3 3 4 4 5 5 6 6
7
7
8
8

located
15 16
5
8
6
8
7
9
9
9

d0 /2
17
6
7
8
10

18
9
9
10
10

Table 5: Myopic-greedy policy communication: C = 1.0, Ra = 1.0.

Pu
0.2
0.4
0.6
0.8

1
9
5
4
3

2
9
6
4
3

d0 =distance agents last
3
4
5
6
7
8
9 10
11 13 14 17 18 20 21 23
7
8
9
10 11 12 13 14
5
6
6
7
8
9
9
10
4
4
5
5
6
7
7
8

synchronized, g
11 12 13
25 27 28
15 16 17
11 12 12
8
9
10

located d0 /2
14 15 16 17
30 32 34 35
18 19 20 21
13 14 15 15
10 11 11 12

18
37
22
16
12

Table 6: Myopic-greedy policy communication: C = 10.0, Ra = 1.0.
smallest cost communication tested, always beneficial communicate
rather early, matter uncertainty environment, almost matter
d0 is. larger costs communication given Pu , larger distance
agents, later communicate (e.g., Pu = 0.4, C = 1 = 5,
agents communicate time 4, C = 10, communicate time
9). given C , larger distance agents is, later agents
communicate (e.g., Pu = 0.4, C = 10 = 5, agents communicate
time 9, = 12, communicate time 16). results averaging
1000 runs show given cost C long Pu decreases (the agent
uncertain actions outcomes), agents communicate times.
196

fiCommunication-Based Decomposition Mechanism

1000 experiments run, agents exchange information actual locations best time myopically found d0 (known time t0 ).
communicate, know actual distance dt , them. agents follow
myopic-greedy communication policy find next time communicate meet already. time best time found myopic-greedy
algorithm given distance agents dt . Iteratively, agents approximate optimal solution decentralized control problem direct communication
following independent optimal policies action, myopic-greedy policy
communication. Results obtained averaging global utility attained 1000
experiments show myopic-greedy agents perform better agents communicate sub-goals (that shown already efficient communicating
all).

Pu
0.2
0.4
0.6
0.8

No-Comm.
-104.925
-51.4522
-33.4955
-24.3202

Average Joint Utility
Ideal C = 0 SubGoals5
-62.872
-64.7399
-37.33
-38.172
-26.444
-27.232
-20.584
-20.852

Myopic-Greedy
-63.76
-37.338
-26.666
-20.704

Table 7: C = 0.10, Ra = 1.0.
Myopic-greedy approach attained utilities statistically significantly greater
obtained heuristic case C = 0.1 (see Table 7)6 . C = 0.1
Pu = 0.4, Myopic-greedy even attained utilities significantly different (with significance
level 98%) Ideal.

Pu
0.2
0.4
0.6
0.8

No-Comm.
-104.925
-51.4522
-33.4955
-24.3202

Average Joint Utility
Ideal C = 0 Comm. SubGoals
-62.872
-65.906
-37.33
-39.558
-26.444
-27.996
-20.584
-21.05

Best p
0.3
0.2
0.2
0.1

Myopic-greedy
-63.84
-37.774
-27.156
-21.3

Table 8: C = 1.0 SubGoals Myopic-greedy, Ra = 1.0.

Pu
0.2
0.4
0.6
0.8

No-Comm.
-104.925
-51.4522
-33.4955
-24.3202

Average Joint Utility
Ideal C = 0 Comm. SubGoals
-62.872
-69.286
-37.33
-40.516
-26.444
-28.192
-20.584
-21.118

Best p
0.1
0.1
0.1
0.1

Myopic-greedy
-68.948
-40.594
-28.908
-22.166

Table 9: C = 10.0 SubGoals Myopic-greedy, Ra = 1.0.
5. results presented best p, found empirically.
6. Statistical significance established t-test.

197

fiGoldman & Zilberstein

C = 1 (see Table 8) utilities attained Myopic-greedy approach
Pu < 0.8 significantly greater results obtained heuristic case.
Pu = 0.8, heuristic case found better Myopic-greedy best
choice p (Myopic-greedy obtained -21.3, SubGoals p = 0.1 attained -21.05
(variance=2.18)). utilities attained Myopic-greedy agents, C = 10 (see
Table 9) Pu {0.2, 0.4}, significantly different SubGoals case
best p significance levels 61% 82%, respectively. However, heuristic case
yielded smaller costs values Pu = {0.6, 0.8}. One important point notice
results consider best p found heuristic trying set discrete
values p (see x-axis Figure 5). general trying tuning heuristic parameter
time consuming best choice may known ahead time
designer. hand, Myopic-greedy approach require tuning
parameter. settings tested, Myopic-greedy always attain utilities higher
attained SubGoals case worst p.

Pu
0.2
0.4
0.6
0.8

Average Communication Acts Performed
No-Comm. Ideal C = 0 SubGoals Myopic-greedy
0
31.436
5.4
21.096
0
18.665
1
11.962
0
13.426
1
8.323
0
10.292
1
4.579

Table 10: C = 0.10, Ra = 1.0.

Pu
0.2
0.4
0.6
0.8

No-Comm.
0
0
0
0

Average Communication Acts Performed
Ideal C = 0 Comm. SubGoals Myopic-greedy
31.436
1.194
6.717
18.665
1
3.904
13.426
1
2.036
10.292
0
1.296

Table 11: C = 1.0 Myopic-greedy SubGoals, Ra = 1.0.

Pu
0.2
0.4
0.6
0.8

No-Comm.
0
0
0
0

Average Communication Acts Performed
Ideal C = 0 Comm. SubGoals Myopic-greedy
31.436
0
0.416
18.665
0
0.417
13.426
0
0.338
10.292
0
0.329

Table 12: C = 10.0 Myopic-greedy SubGoals, Ra = 1.0.
Tables 10, 11 12 present average number communication acts performed
one cases.
198

fiCommunication-Based Decomposition Mechanism

[Bernstein et al.
2002]

NEXP-C

NP-C
[GZ 2004]

Dec-MDP

IO

NP-C

Goaloriented

Poly-C

Poly-C
|G|=1

[GZ 2004]

[GZ 2004]

Goaloriented

Information
Sharing

|G| > 1
Information
Sharing

[GZ 2004]

Optimal
Mechanism
NEXP

NEXP-C
[GZ 2004]

[Rabinovich et al.
2003]

Approx.
NEXP-C

P

LGO
Mechanism
Myopic
Greedy

P

P

Backward
Induction

Figure 6: complexity solving Dec-MDPs.

8. Discussion
Solving optimally decentralized control problems known hard. Figure 6 summarizes complexity results (the rectangles stand optimal solutions parallelograms stand solutions proposed framework communication-based decomposition mechanisms). taxonomy helps us understand characteristics different
classes decentralized control problems effect complexity problems.
Coverage-set (Becker et al., 2004), Opt1Goal OptNGoals (Goldman & Zilberstein,
2004a) first algorithms solve optimally non-trivial classes Dec-MDPs.
paper presents communication-based decomposition mechanisms way approximate optimal joint solution decentralized control problems. approach
based two key ideas: (1) separating questions communicate
communications, (2) exploiting full observability global states
communication generate individual behaviors agents follow
communications. Communication decision makers serves synchronization
point local information exchanged order assign individual behavior
controller. addresses effectively applications constant communication
desirable feasible. Many practical reasons could prevent agents communication
constantly. Communication actions may incur costs reflect complexity
transmitting information, utilization limited bandwidth may shared
applications, risk revealing information competitive parties operating
environment. communication-based decomposition mechanism divides
global problem individual behaviors combined communication acts overcome
lack global information.
199

fiGoldman & Zilberstein

formalized communication-based decomposition mechanisms decentralized semiMarkov process communication (Dec-SMDP-Com). proved solving optimally problems temporally abstracted actions equivalent solving optimally
multi-agent MDP (MMDP).We adapted multi-step backup policy iteration algorithm
decentralized case solve Dec-SMDP-Com problem optimally. algorithm produces optimal communication-based decomposition mechanism. provide
tractable algorithm, restrict set individual behaviors allowed.
proposed LGO-MSBPI polynomial-time algorithm computes assignment
pair local goals period time k highest value possible global state.
Adding local goals model seems natural intuitive computing local
behaviors based general options. easier state local behavior completed
local goal reached, rather stating sequences local actions eventually achieve desired global behavior. Furthermore, unrestricted set
options larger therefore computationally cheaper compute decomposition
mechanism local goals assumed. intuition confirmed experiments.
general question remain open, namely determine beneficial compute
local behaviors general options rather assuming local goals.
paper also presents simpler approximation method. assumes certain
mechanism given, i.e., human knowledge incorporated model provide agents
individual policies actions (not including communication acts). greedy-approach
presented computes best time communicate assuming one opportunity exchanging information. paper concludes empirical assessment
approaches.
summary, paper contributes communication-based decomposition mechanism
applied many hard decentralized control problems shown Figure 6.
approach enables us compute tractable individual behaviors agent together
beneficial time communicate change local behaviors. analytical results paper support validity approach respect Dec-MDPs
independent transitions observations. However, straightforward apply
approach general Dec-POMDPs Dec-MDPs dependent transitions observations, believe offers viable approximation technique problems well.
approach scalable respect number agents since complexity
results presented increase linearly agents added system. Exchange
information assumed via broadcast agents. interesting future
extension study agents efficiently choose partners communication avoid
global broadcasting.

Acknowledgments
work supported part National Science Foundation grants IIS0219606, Air Force Office Scientific Research grants F49620-03-1-0090
FA9550-08-1-0181, NASA grant NCC 2-1311. opinions, findings,
conclusions recommendations expressed material authors
reflect views NSF, AFOSR NASA.
200

fiCommunication-Based Decomposition Mechanism

References
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004). Solving transition independent decentralized MDPs. Journal Artificial Intelligence Research, 22, 423455.
Bererton, C., Gordon, G., Thrun, S., & Khosla, P. (2003). Auction mechanism design
multi-robot coordination. Proceedings Seventeenth Annual Conference
Neural Information Processing Systems, Whistler, BC, Canada.
Bernstein, D., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity decentralized control Markov decision processes. Mathematics Operations Research,
27 (4), 819840.
Boutilier, C. (1999). Sequential optimality coordination multiagent systems.
Proceedings Sixteenth International Joint Conference Artificial Intelligence,
pp. 478485, Stockholm, Sweden.
Ghavamzadeh, M., & Mahadevan, S. (2004). Learning act communicate cooperative multiagent systems using hierarchical reinforcement learning. Proceedings
Third International Joint Conference Autonomous Agents Multi-Agent
Systems, pp. 11141121, New York City, NY.
Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange cooperative
multi-agent systems. Proceedings Second International Joint Conference
Autonomous Agents Multi-Agent Systems, pp. 137144, Melbourne, Australia.
Goldman, C. V., & Zilberstein, S. (2004a). Decentralized control cooperative systems:
Categorization complexity analysis. Journal Artificial Intelligence Research,
22, 143174.
Goldman, C. V., & Zilberstein, S. (2004b). Goal-oriented Dec-MDPs direct communication. Technical Report 0444, University Massachusetts Amherst, Computer
Science Department.
Guestrin, C., & Gordon, G. (2002). Distributed planning hierarchical factored MDPs.
Proceedings Eighteenth Conference Uncertainty Artificial Intelligence,
pp. 197206, Edmonton, Canada.
Guestrin, C., Koller, D., & Parr, R. (2001). Multiagent planning factored MDPs.
Advances Neural Information Processing Systems, pp. 15231530, Vancouver,
British Columbia.
Hansen, E. A. (1997). Markov decision processes observation costs. Technical Report
97-01, University Massachusetts Amherst, Computer Science Department.
Hansen, E. A., & Zhou, R. (2003). Synthesis hierarchical finite-state controllers
POMDPs. Proceedings Thirteenth International Conference Automated
Planning Scheduling, Trento, Italy.
Nair, R., Tambe, M., Roth, M., & Yokoo, M. (2004). Communication improving policy
computation distributed POMDPs. Proceedings Third International Joint
Conference Autonomous Agents Multi-Agent Systems, pp. 10981105, New
York City, NY.
201

fiGoldman & Zilberstein

Nair, R., Tambe, M., Yokoo, M., Pynadath, D., & Marsella, S. (2003). Taming decentralized
POMDPs: Towards efficient policy computation multiagent settings. Proceedings
Eighteenth International Joint Conference Artificial Intelligence, pp. 705
711, Acapulco, Mexico.
Papadimitriou, C. H., & Tsitsiklis, J. (1987). complexity Markov decision processes.
Mathematics Operations Research, 12 (3), 441450.
Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning cooperate via
policy search. Proceedings Sixteenth Conference Uncertainty Artificial
Intelligence, pp. 489496, Stanford, CA.
Petrik, M., & Zilberstein, S. (2007). Anytime coordination using separable bilinear programs. Proceedings Twenty Second Conference Artificial Intelligence, pp.
750755, Vancouver, BC, Canada.
Puterman, M. L. (1994). Markov Decision Processes Discrete Stochastic Dynamic Programming. Wiley & Sons, Inc., New York.
Pynadath, D. V., & Tambe, M. (2002). communicative multiagent team decision
problem: Analyzing teamwork theories models. Journal Artificial Intelligence
Research, 16, 389423.
Schneider, J., Wong, W.-K., Moore, A., & Riedmiller, M. (1999). Distributed value functions. Proceedings Sixteenth International Conference Machine Learning,
pp. 371378, Bled, Slovenia.
Seuken, S., & Zilberstein, S. (2007a). Improved memory-bounded dynamic programming
Dec-POMDPs. Proceedings Twenty Third Conference Uncertainty
Artificial Intelligence, Vancouver, BC, Canada.
Seuken, S., & Zilberstein, S. (2007b). Memory-bounded dynamic programming DecPOMDPs. Proceedings Twentieth International Joint Conference Artificial Intelligence, pp. 20092015, Hyderabad, India.
Seuken, S., & Zilberstein, S. (2008). Formal models algorithms decentralized decision
making uncertainty. Autonomous Agents Multi-agent Systems. appear.
Sutton, R. S., Precup, D., & Singh, S. (1999). MDPs semi-MDPs: framework
temporal abstraction reinforcement learning. Artificial Intelligence, 112, 181
211.
Suzuki, I., & Yamashita, M. (1999). Distributed anonymous mobile robots: Formation
geometric patterns. SIAM Journal Computing, 28 (4), 13471363.
Wolpert, D. H., Wheeler, K. R., & Tumer, K. (1999). General principles learning-based
multi-agent systems. Proceedings Third International Conference Autonomous Agents, pp. 7783, Seattle, Washington.
Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions multi-agent cooperation: Model experiments. Proceedings Fifth International Conference
Autonomous Agents, pp. 616623, Montreal, Canada.

202

fiJournal Artificial Intelligence Research 32 (2008) 453486

Submitted 02/08; published 06/08

Adaptive Stochastic Resource Control:
Machine Learning Approach
Balazs Csanad Csaji

balazs.csaji@sztaki.hu

Computer Automation Research Institute,
Hungarian Academy Sciences
Kende utca 1317, Budapest, H1111, Hungary

Laszlo Monostori

laszlo.monostori@sztaki.hu

Computer Automation Research Institute,
Hungarian Academy Sciences;
Faculty Mechanical Engineering,
Budapest University Technology Economics

Abstract
paper investigates stochastic resource allocation problems scarce, reusable
resources non-preemtive, time-dependent, interconnected tasks. approach
natural generalization several standard resource management problems, scheduling transportation problems. First, reactive solutions considered defined
control policies suitably reformulated Markov decision processes (MDPs). argue
reformulation several favorable properties, finite state action
spaces, aperiodic, hence policies proper space control policies
safely restricted. Next, approximate dynamic programming (ADP) methods, fitted
Q-learning, suggested computing efficient control policy. order compactly
maintain cost-to-go function, two representations studied: hash tables support
vector regression (SVR), particularly, -SVRs. Several additional improvements,
application limited-lookahead rollout algorithms initial phases, action space
decomposition, task clustering distributed sampling investigated, too. Finally,
experimental results benchmark industry-related data presented.

1. Introduction
Resource allocation problems (RAPs) high practical importance, since arise
many diverse fields, manufacturing production control (e.g., production scheduling),
warehousing (e.g., storage allocation), fleet management (e.g., freight transportation), personnel management (e.g., office), scheduling computer programs (e.g., massively
parallel GRID systems), managing construction project controlling cellular mobile
network. RAPs also central management science (Powell & Van Roy, 2004).
paper consider optimization problems include assignment finite set
reusable resources non-preemtive, interconnected tasks stochastic durations
effects. objective investigate efficient reactive (closed-loop) decision-making
processes deal allocation scarce resources time goal
optimizing objectives. real world applications, important solution
able deal large-scale problems handle environmental changes, well.
c
2008
AI Access Foundation. rights reserved.

fiCsaji & Monostori

1.1 Industrial Motivations
One main motivations investigating RAPs enhance manufacturing production
control. Regarding contemporary manufacturing systems, difficulties arise unexpected
tasks events, non-linearities, multitude interactions attempting control
various activities dynamic shop floors. Complexity uncertainty seriously limit
effectiveness conventional production control approaches (e.g., deterministic scheduling).
paper apply mathematical programming machine learning (ML) techniques achieve suboptimal control general class stochastic RAPs,
vital intelligent manufacturing system (IMS). term IMS attributed
tentative forecast Hatvany Nemes (1978). early 80s IMSs outlined
next generation manufacturing systems utilize results artificial intelligence
research expected solve, within certain limits, unprecedented, unforeseen problems basis even incomplete imprecise information. Naturally, applicability
different solutions RAPs limited industrial problems.
1.2 Curse(s) Dimensionality
Different kinds RAPs huge number exact approximate solution methods,
example, case scheduling problems (Pinedo, 2002). However, methods
primarily deal static (and often strictly deterministic) variants various problems and, mostly, aware uncertainties changes. Special (deterministic)
RAPs appear field combinatorial optimization, traveling salesman problem (TSP) job-shop scheduling problem (JSP), strongly NP-hard and,
moreover, good polynomial-time approximation, either.
stochastic case RAPs often formulated Markov decision processes
(MDPs) applying dynamic programming (DP) methods, theory,
solved optimally. However, due phenomenon named curse dimensionality
Bellman (1961), methods highly intractable practice. curse refers
combinatorial explosion required computation size problem increases. authors, e.g., Powell Van Roy (2004), talk even three types
curses concerning DP algorithms. motivated approximate approaches require
tractable computation, often yield suboptimal solutions (Bertsekas, 2005).
1.3 Related Literature
beyond scope give general overview different solutions RAPs, hence,
concentrate part literature closely related approach.
solution belongs class approximate dynamic programming (ADP) algorithms
constitute broad class discrete-time control techniques. Note ADP methods
take actor-critic point view often called reinforcement learning (RL).
Zhang Dietterich (1995) first apply RL technique special RAP.
used D() method iterative repair solve static scheduling problem,
namely, NASA space shuttle payload processing problem. Since then, number
papers published suggested using RL different RAPs. first reactive
(closed-loop) solution scheduling problems using ADP algorithms briefly described
454

fiAdaptive Stochastic Resource Control

Schneider, Boyan, Moore (1998). Riedmiller Riedmiller (1999) used multilayer perceptron (MLP) based neural RL approach learn local heuristics. Aydin
Oztemel (2000) applied modified version Q-learning learn dispatching rules production scheduling. Multi-agent versions ADP techniques solving dynamic scheduling
problems also suggested (Csaji, Kadar, & Monostori, 2003; Csaji & Monostori, 2006).
Powell Van Roy (2004) presented formal framework RAPs applied
ADP give general solution problem. Later, parallelized solution
previously defined problem given Topaloglu Powell (2005). RAP framework, presented Section 2, differs approaches, since system goal
accomplish set tasks widely different stochastic durations
precedence constraints them, Powell Van Roys (2004) approach concerns satisfying many similar demands arriving stochastically time demands
unit durations precedence constraints. Recently, support vector machines
(SVMs) applied Gersmann Hammer (2005) improve iterative repair (local
search) strategies resource constrained project scheduling problems (RCPSPs). agentbased resource allocation system MDP-induced preferences presented Dolgov
Durfee (2006). Finally, Beck Wilson (2007) gave proactive solutions job-shop
scheduling problems based combination Monte Carlo simulation, solutions
associated deterministic problem, either constraint programming tabu-search.
1.4 Main Contributions
summary main contributions paper, highlighted that:
1. propose formal framework investigating stochastic resource allocation problems scarce, reusable resources non-preemtive, time-dependent, interconnected tasks. approach constitutes natural generalization several standard
resource management problems, scheduling problems, transportation problems, inventory management problems maintenance repair problems.
general RAP reformulated stochastic shortest path problem (a special
MDP) favorable properties, as, aperiodic, state action spaces
finite, policies proper space control policies safely restricted. Reactive solutions defined policies reformulated problem.
2. order compute good approximation optimal control policy, ADP methods suggested, particularly, fitted Q-learning. Regarding value function representations ADP, two approaches studied: hash tables SVRs. latter,
samples regression generated Monte Carlo simulation
cases inputs suitably defined numerical feature vectors.
Several improvements speed calculation ADP-based solution suggested: application limited lookahead rollout algorithms initial phases
guide exploration provide first samples approximator; decomposing action space decrease number available actions states;
clustering tasks reduce length trajectories variance
cumulative costs; well two methods distribute proposed algorithm among
several processors either shared distributed memory architecture.
455

fiCsaji & Monostori

3. paper also presents several results numerical experiments benchmark
industry-related problems. First, performance algorithm measured
hard benchmark flexible job-shop scheduling datasets. scaling properties
approach demonstrated experiments simulated factory producing
mass-products. effects clustering depending size clusters
speedup relative number processors case distributed sampling
studied, well. Finally, results adaptive features algorithm case
disturbances, resource breakdowns new task arrivals, also shown.

2. Markovian Resource Control
section aims precisely defining RAPs reformulating way would
allow effectively solved ML methods presented Section 3. First, brief
introduction RAPs given followed formulation general resource allocation
framework. start deterministic variants extend definition
stochastic case. Afterwards, give short overview Markov decision processes (MDPs),
constitute fundamental theory approach. Next, reformulate reactive
control problem RAPs stochastic shortest path (SSP) problem (a special MDP).
2.1 Classical Problems
section give brief introduction RAPs two strongly NP-hard combinatorial optimization problems: job-shop scheduling problem traveling salesman
problem. Later, apply two basic problems demonstrate results.
2.1.1 Job-Shop Scheduling
First, consider classical job-shop scheduling problem (JSP) standard deterministic RAP (Pinedo, 2002). set jobs, J = {J1 , . . . , Jn }, processed
set machines, = {M1 , . . . , Mk }. j J consists sequence nj
tasks, task tji , {1, . . . , nj }, machine mji
process task, processing time pji N. aim optimization find
feasible schedule minimizes given performance measure. solution, i.e., schedule,
suitable task starting time assignment. Figure 1 visualizes example schedule
using Gantt chart. Note Gantt chart (Pinedo, 2002) figure using bars,
order illustrate starting finishing times tasks resources.
concept feasibility defined Section 2.2.1. case JSP feasible
schedule associated ordering tasks, i.e., order
executed machines. many types performance measures available
JSP, probably commonly applied one maximum completion time
tasks, also called makespan. case applying makespan, JSP interpreted
problem finding schedule completes tasks every job soon possible.
Later, study extension JSP, called flexible job-shop scheduling problem
(FJSP) arises machines interchangeable, i.e., may tasks
executed several machines. case processing times given
partial function, p : , N. Note partial function binary relation
456

fiAdaptive Stochastic Resource Control

associates elements domain set one element range set.
Throughout paper use , denote partial function type binary relations.

Figure 1: possible solution JSP, presented Gantt chart. Tasks
color belong job processed given order.
vertical gray dotted line indicates maximum completion time tasks.

2.1.2 Traveling Salesman
One basic logistic problems traveling salesman problem (TSP)
stated follows. Given number cities costs travelings them,
least-cost round-trip route visits city exactly returns
starting city. Several variants TSP known, standard one
formally characterized connected, undirected, edge-weighted graph G = hV, E, wi,
V = {1, . . . , n} vertex set corresponding set cities,E V V
set edges represents roads cities, w : E N defines
weights edges: durations trips. aim optimization find
Hamilton-circuit smallest possible weight. Note Hamilton-circuit graph
cycle starts vertex, passes every vertex exactly once, returns
starting vertex. Take look Figure 2 example Hamilton-circuit.

Figure 2: possible solution TSP, closed path graph. black edges constitute
Hamilton-circuit given connected, undirected, edge-weighted graph.
457

fiCsaji & Monostori

2.2 Deterministic Framework
Now, present general framework model resource allocation problems. framework treated generalization several classical RAPs, JSP TSP.
First, deterministic resource allocation problem considered: instance
problem characterized 8-tuple hR, S, O, , C, d, e, ii. details problem
consists set reusable resources R together corresponds set
possible resource states. set allowed operations also given subset
denotes target operations tasks. R, supposed finite
pairwise disjoint. precedence constrains tasks,
represented partial ordering C . durations operations depending
state executing resource defined partial function : , N,
N set natural numbers, thus, discrete-time model. Every operation
affect state executing resource, well, described e : ,
also partial function. assumed dom(d) = dom(e), dom() denotes
domain set function. Finally, initial states resources given : R S.
state resource contain relevant information it, example, type
current setup (scheduling problems), location load (transportation problems)
condition (maintenance repair problems). Similarly, operation affect state
many ways, e.g., change setup resource, location condition.
system must allocate task (target operation) resource, however, may
cases first state resource must modified order able execute
certain task (e.g., transporter may need, first, travel loading/source point,
machine may require repair setup). cases non-task operations may applied.
modify states resources without directly serving demand (executing
task). possible resource allocation process non-task operation
applied several times, non-task operations completely avoided (for example,
high cost). Nevertheless, finally, tasks must completed.
2.2.1 Feasible Resource Allocation
solution deterministic RAP partial function, resource allocator function,
% : R N , assigns starting times operations resources. Note
operations supposed non-preemptive (they may interrupted).
solution called feasible following four properties satisfied:
1. tasks associated exactly one (resource, time point) pair:
v : ! hr, ti dom(%) : v = %(r, t).
2. resource executes, most, one operation time:
u, v : u = %(r, t1 ) v = %(r, t2 ) t1 t2 < t1 + d(s(r, t1 ), u).
3. precedence constraints tasks satisfied:
hu, vi C : [u = %(r1 , t1 ) v = %(r2 , t2 )] [t1 + d(s(r1 , t1 ), u) t2 ] .
4. Every operation-to-resource assignment valid:
hr, ti dom(%) : hs(r, t), %(r, t)i dom(d),
458

fiAdaptive Stochastic Resource Control

: R N describes states

i(r)
s(r, 1)
s(r, t) =

e(s(r, 1), %(r, t))

resources given times
= 0
hr, ti
/ dom(%)
otherwise

RAP called correctly specified exists least one feasible solution.
follows assumed problems correctly specified. Take look Figure 3.

Figure 3: Feasibility illustration four forbidden properties, using JSP
example. presented four cases excluded set feasible schedules.

2.2.2 Performance Measures
set feasible solutions denoted S. performance (or cost) associated
solution, defined performance measure : R often depends
task completion times, only. Typical performance measures appear practice
include: maximum completion time mean flow time. aim resource allocator
system compute feasible solution maximal performance (or minimal cost).
Note performance measure assign penalties violating release due
dates (if available) even reflect priority tasks. possible generalization given problem case operations may require resources simultaneously, important model, e.g., resource constrained project scheduling problems. However, straightforward extend framework case: definition
e changed : hki N e : hki hki , hki = ki=1
k |R|. Naturally, assume hs, oi dom(e) : dim(e(s, o)) = dim(s).
Although, managing tasks multiple resource requirements may important
cases, keep analysis simple possible, deal paper.
Nevertheless, results easily generalized case, well.
459

fiCsaji & Monostori

2.2.3 Demonstrative Examples
Now, demonstrative examples, reformulate (F)JSP TSP given framework.
straightforward formulate scheduling problems, JSP, presented
resource allocation framework: tasks JSP directly associated tasks
framework, machines associated resources processing times durations. precedence constraints determined linear ordering tasks
job. Note one possible resource state every machine. Finally, feasible
schedules associated feasible solutions. setup-times problem, well, would several states resource (according current
setup) set-up procedures could associated non-task operations.
RAP formulation TSP given follows. set resources consists
one element, namely salesman, therefore, R = {r}. possible states
resource r (the salesman) = {s1 , . . . , sn }. state (of r) si , indicates
salesman city i. allowed operations allowed tasks,
= = {t1 , . . . , tn }, execution task ti symbolizes salesman travels
city current location. constraints C = {ht2 , t1 , ht3 , t1 . . . , htn , t1 i} used
forcing system end whole round-tour city 1, also starting city,
thus, i(r) = s1 . si tj : hsi , tj dom(d) hi, ji E.
hsi , tj dom(d) : d(si , tj ) = wij e(si , tj ) = sj . Note dom(e) = dom(d) first
feasibility requirement guarantees city visited exactly once. performance
measure latest arrival time, (%) = max {t + d(s(r, t), %(r, t)) | hr, ti dom(%)}.
2.2.4 Computational Complexity
use performance measure property solution precisely
defined bounded sequence operations (which includes tasks) assignment
resources and, additionally, among solutions generated way optimal
one found, RAP becomes combinatorial optimization problem.
performance measure monotone completion times, called regular, property.
defined RAP generalization of, e.g., JSP TSP, strongly
NP-hard and, furthermore, good polynomial-time approximation optimal resource
allocating algorithm exits, either (Papadimitriou, 1994).
2.3 Stochastic Framework
far model deterministic, turn stochastic RAPs. stochastic
variant described general class RAPs defined randomizing functions d,
e i. Consequently, operation durations become random, : (N),
(N) space probability distributions N. Also effects operations
uncertain, e : (S) initial states resources stochastic,
well, : R (S). Note ranges functions d, e contain probability
distributions, denote corresponding random variables D, E I, respectively.
notation X f indicate random variable X probability distribution f . Thus,
D(s, o) d(s, o), E(s, o) e(s, o) I(r) i(r) S, r R. Take
look Figure 4 illustration stochastic variants JSP TSP problems.
460

fiAdaptive Stochastic Resource Control

2.3.1 Stochastic Dominance
stochastic RAPs performance solution also random variable. Therefore,
order compare performance different solutions, compare random
variables. Many ways known make comparison. may say, example,
random variable stochastic dominance another random variable almost
surely, likelihood ratio sense, stochastically, increasing convex sense
expectation. different applications different types comparisons may suitable,
however, probably natural one based upon expected values random
variables. paper applies kind comparison stochastic RAPs.

Figure 4: Randomization case JSP (left) case TSP (right). latter,
initial state, durations arrival vertex could uncertain, well.

2.3.2 Solution Classification
Now, classify basic types resource allocation techniques. First, order give
proper classification begin recalling concepts open-loop closed-loop
controllers. open-loop controller, also called non-feedback controller, computes
input system using current state model system. Therefore,
open-loop controller use feedback determine input achieved
desired goal, observe output process controlled. Conversely,
closed-loop controller uses feedback control states outputs dynamical system
(Sontag, 1998). Closed-loop control significant advantage open-loop solutions
dealing uncertainties. Hence, improved reference tracking performance,
stabilize unstable processes reduced sensitivity parameter variations.
deterministic RAPs significant difference open- closed-loop
controls. case safely restrict open-loop methods. solution
aimed generating resource allocation off-line advance, called predictive.
Thus, predictive solutions perform open-loop control assume deterministic environment. stochastic resource allocation data (e.g., actual durations)
available execution plan. Based usage
information, identify two basic types solution techniques. open-loop solution
deal uncertainties environment called proactive. proactive solution
allocates operations resources defines orders operations, but,
durations uncertain, determine precise starting times. kind
461

fiCsaji & Monostori

technique applied durations operations stochastic, but,
states resources known perfectly (e.g., stochastic JSP). Finally, stochastic
case closed-loop solutions called reactive. reactive solution allowed make
decisions on-line, process actually evolves providing information. Naturally,
reactive solution simple sequence, rather resource allocation policy (to
defined later) controls process. paper focuses reactive solutions, only.
formulate reactive solution stochastic RAP control policy suitably
defined Markov decision process (specially, stochastic shortest path problem).
2.4 Markov Decision Processes
Sequential decision-making presence uncertainties often modeled MDPs
(Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998; Feinberg & Shwartz, 2002).
section contains basic definitions, notations applied preliminaries.
(finite, discrete-time, stationary, fully observable) Markov decision process (MDP)
mean stochastic system characterized 6-tuple hX, A, A, p, g, i, components follows: X finite set discrete states finite set control actions.
Mapping : X P(A) availability function renders state set actions
available state P denotes power set. transition-probability function
given p : X (X), (X) space probability distributions X.
Let p(y | x, a) denote probability arrival state executing action A(x)
state x. immediate-cost function defined g : X R, g(x, a)
cost taking action state x. Finally, constant [0, 1] denotes discount rate
discount factor. = 1, MDP called undiscounted, otherwise discounted.
possible extend theory general state action spaces,
expense increased mathematical complexity. Finite state action sets mostly
sufficient digitally implemented controls and, therefore, restrict case.

Figure 5: Markov decision processes interaction decision-maker uncertain environment (left); temporal progress system (right).
interpretation MDP given consider agent acts
uncertain environment, viewpoint often taken RL. agent receives information
state environment, x, state x agent allowed choose
action A(x). action selected, environment moves next state
according probability distribution p(x, a), decision-maker collects one462

fiAdaptive Stochastic Resource Control

step cost, g(x, a), illustrated Figure 5. aim agent find optimal
behavior (policy) applying strategy minimizes expected cumulative costs.
stochastic shortest path (SSP) problem special MDP aim find
control policy reaches pre-defined terminal state starting given initial
state, additionally, minimizes expected total costs path, well. policy
called proper reaches terminal state probability one. usual assumption
dealing SSP problems policies proper, abbreviated APP.
2.4.1 Control Policies
(stationary, Markov) control policy determines action take possible state.
deterministic policy, : X A, simply function states control actions.
randomized policy, : X (A), function states probability distributions
actions. denote probability executing action state x (x)(a) or,
short, (x, a). Naturally, deterministic policies special cases randomized ones and,
therefore, unless indicated otherwise, consider randomized control policies.
x
e0 (X) initial probability distribution states, transition probabilities p together control policy completely determine progress system
stochastic sense, namely, define homogeneous Markov chain X,
x
et+1 = P ()e
xt ,

x
et state probability distribution vector system time t, P ()
denotes probability transition matrix induced control policy , formally defined
X
[P ()]x,y =
p(y | x, a) (x, a).
aA

2.4.2 Value Functions
value cost-to-go function policy function states costs. defined
state: J : X R. Function J (x) gives expected value cumulative
(discounted) costs system state x follows policy thereafter,
J (x) = E

"

N
X
t=0

#
fi
fi
g(Xt , ) fifi X0 = x ,

(1)

Xt random variables, selected according control policy
distribution Xt+1 p(Xt , ). horizon problem denoted N N {}.
Unless indicated otherwise, always assume horizon infinite, N = .
Similarly definition J , one define action-value functions control polices,
" N
#
fi
X
fi



fi
Q (x, a) = E
g(Xt , ) fi X0 = x, A0 = ,
t=0

notations equation (1). Action-value functions especially
important model-free approaches, classical Q-learning algorithm.
463

fiCsaji & Monostori

2.4.3 Bellman Equations
say 1 2 if, x X, J 1 (x) J 2 (x). control policy
(uniformly) optimal less equal control policies.
always exists least one optimal policy (Sutton & Barto, 1998). Although
may many optimal policies, share unique optimal cost-to-go function,
denoted J . function must satisfy Bellman optimality equation (Bertsekas &
Tsitsiklis, 1996), J = J , Bellman operator, defined x X,
(T J)(x) = min

aA(x)

h

g(x, a) +

X

yX


p(y | x, a)J(y) .

(2)

Bellman equation arbitrary (stationary, Markov, randomized) policy
(T J)(x) =

X

h

X
(x, a) g(x, a) +
p(y | x, a)J(y) ,
yX

aA(x)

notations equation (2) also J = J .
given value function J, straightforward get policy, e.g., applying
greedy deterministic policy (w.r.t. J) always selects actions minimal costs,
h

X
p(y | x, a)J(y) .
(x) arg min g(x, a) +
aA(x)

yX

MDPs extensively studied theory exist lot exact approximate solution methods, e.g., value iteration, policy iteration, Gauss-Seidel method,
Q-learning, Q(), SARSA TD() - temporal difference learning (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998; Feinberg & Shwartz, 2002). reinforcement
learning algorithms work iteratively approximating optimal value function.
2.5 Reactive Resource Control
section formulate reactive solutions stochastic RAPs control policies
suitably reformulated SSP problems. current task durations resource states
incrementally available resource allocation control process.
2.5.1 Problem Reformulation
state x X defined 4-tuple x = h, , %, i, N current time
function : R determines current states resources. partial functions %
store past process, namely, % : RN 1 , contains resources
times operation started : R N 1 , N describes finish times
already completed operations, N = {0, . . . , }. Naturally, dom() dom(%).
TS (x) denote set tasks started state x (before
current time ) TF (x) TS (x) set tasks finished already
state x. easy see TS (x) = rng(%) TF (x) = rng(%|dom() ) ,
rng() denotes range image set function. process starts initial state
464

fiAdaptive Stochastic Resource Control

xs = h0, , , i, corresponds situation time zero none operations
started. initial probability distribution, , calculated follows
(xs ) = P ((r1 ) = I(r1 ), . . . , (rn ) = I(rn )) ,
I(r) i(r) denotes random variable determines initial state resource
r R n = |R|. Thus, renders initial states resources according (multivariate)
probability distribution component RAP. introduce set terminal
states, well. state x considered terminal state (x T) two cases. First,
tasks finished state, formally, TF (x) = reached state
x, TF (x) 6= . Second, system reached state tasks operations
executed, words, allowed set actions empty, A(x) = .
easy see that, theory, aggregate terminal states global unique
terminal state introduce new unique initial state, x0 , one available
action takes us randomly (with distribution) real initial states. Then,
problem becomes stochastic shortest path problem aim described finding
routing minimal expected cost new initial state goal state.
every time system informed finished operations, decide
operations apply (and resources). control action space contains
operation-resource assignments avr A, v r R, special await
control corresponds action system start new operation
current time. non-terminal state x = h, , %, available actions
await A(x) TS (x) \ TF (x) 6=
v : r R : avr A(x) (v \ TS (x) hr, ti dom(%) \ dom() : r 6= r
h(r), vi dom(d) v (u : hu, vi C u TF (x)))
Thus, action await available every state unfinished operation; action avr
available states resource r idle, process operation v, additionally, v
task, executed earlier precedence constraints satisfied.
action avr A(x) executed state x = h, , %, i, system moves
probability one new state x = h, , %, i, % = % {hhr, ti , vi}. Note
treat functions sets ordered pairs. resulting x corresponds state
operation v started resource r previous state environment x.
effect await action x = h, , %, takes x = h + 1, , %, i,
unfinished operation %(r, t) started r finishes probability
P(hr, ti dom() | x, hr, ti dom(%) \ dom()) =

P(D((r), %(r, t)) + = )
,
P(D((r), %(r, t)) + )

D(s, v) d(s, v) random variable determines duration operation v
executed resource state s. quantity called completion rate
stochastic scheduling theory hazard rate reliability theory. remark
operations continuous durations, quantity defined f (t)/(1 F (t)), f
denotes density function F distribution random variable determines
duration operation. operation v = %(r, t) finished (hr, ti dom()),
465

fiCsaji & Monostori

(r, t) = (r) = E(r, v), E(r, v) e(r, v) random variable determines
new state resource r executed operation v. Except extension
domain set, values function change, consequently, hr, ti dom() :
(r, t) = (r, t). words, conservative extension , formally, .
immediate-cost function g given performance measure defined follows.
Assume depends operation-resource assignments completion
times. Let x = h, , %, x = h , , %, i. Then, general, system arrives
state x executing control action state x, incurs cost (%, ) (%, ).
Note that, though, Section 2.2.2 performance measures defined complete
solutions, measures applied practice, total completion time weighted
total lateness, straightforward generalize performance measure partial solutions, well. One may, example, treat partial solution problem complete
solution smaller (sub)problem, namely, problem fewer tasks completed.
control process failed, precisely, possible finish tasks,
immediate-cost function render penalties (depending specific problem) regarding non-completed tasks proportional number failed tasks.
2.5.2 Favorable Features
Let us call introduced SSPs, describe stochastic RAPs, RAP-MDPs.
section overview basic properties RAP-MDPs. First, straightforward see
MDPs finite action spaces, since |A| |R| |O| + 1 always holds.
Though, state space RAP-MDP denumerable general, allowed number non-task operations bounded random variables describing operation
durations finite, state space reformulated MDP becomes finite, well.
may also observe RAP-MDPs acyclic (or aperiodic), viz., none states
appear multiple times, resource allocation process dom(%)
non-decreasing and, additionally, time state changes, quantity + |dom(%)|
strictly increases. Therefore, system cannot reach state twice. immediate
consequence, policies eventually terminate (if MDP finite) and, thus, proper.
effective computation good control policy, important try reduce
number states. recognizing performance measure
non-decreasing completion times, optimal control policy reformulated
RAP-MDP found among policies start new operations times
another operation finished initial state. statement supported
fact without increasing cost ( non-decreasing) every operation
shifted earlier resource assigned reaches another operation,
reaches time one preceding tasks finished (if operation
task precedence constrains), or, ultimately, time zero. Note
performance measures used practice (e.g., makespan, weighted completion time, average
tardiness) non-decreasing. consequence, states operation
finished omitted, except initial states. Therefore, await action may lead
state operation finished. may consider it, system executes
automatically await action omitted states. way, state space
decreased and, therefore, good control policy calculated effectively.
466

fiAdaptive Stochastic Resource Control

2.5.3 Composable Measures
large class performance measures, state representation simplified
leaving past process. order so, must require performance
measure composable suitable function. general, call function f : P(X)
R -composable A, B X, AB = holds (f (A), f (B)) = f (AB),
: R R R called composition function, X arbitrary set. definition
directly applied performance measures. performance measure, example,
-composable, indicates value complete solution computed
values disjoint subsolutions (solutions subproblems) function . practical
situations composition function often max, min + function.
performance measure -composable, past omitted
state representation, performance calculated incrementally. Thus,
state described x = h , , , TU i, N, previously, current
time, R contains performance current (partial) solution TU set
unfinished tasks. function : R (O {}) N determines current states
resources together operations currently executed (or resource
idle) starting times operations (needed compute completion rates).
order keep analysis simple possible, restrict composable
functions, since almost performance measures appear practice -composable
suitable (e.g., makespan total production time max-composable).
2.5.4 Reactive Solutions
Now, position define concept reactive solutions stochastic RAPs.
reactive solution (stationary, Markov) control policy reformulated SSP problem.
reactive solution performs closed-loop control, since time step controller
informed current state system choose control action based upon
information. Section 3 deals computation effective control policies.

3. Solution Methods
section aim giving effective solution large-scale RAPs uncertain
changing environments help different machine learning approaches. First,
overview approximate dynamic programming methods compute good policy.
Afterwards, investigate two function approximation techniques enhance solution.
Clustering, rollout algorithm action space decomposition well distributed sampling also considered, speedup computation good control policy
considerably and, therefore, important additions face large-scale problems.
3.1 Approximate Dynamic Programming
previous sections formulated RAPs acyclic (aperiodic) SSP problems.
Now, face challenge finding good policy. theory, optimal value function
finite MDP exactly computed dynamic programming (DP) methods, value
iteration Gauss-Seidel method. Alternatively, exact optimal policy directly
calculated policy iteration. However, due curse dimensionality, computing
467

fiCsaji & Monostori

exact optimal solution methods practically infeasible, e.g., typically
required amount computation needed storage space, viz., memory, grows
quickly size problem. order handle curse, apply
approximate dynamic programming (ADP) techniques achieve good approximation
optimal policy. Here, suggest using sampling-based fitted Q-learning (FQL).
trial Monte-Carlo estimate value function computed projected onto suitable
function space. methods described section (FQL, MCMC Boltzmann
formula) applied simultaneously, order achieve efficient solution.
3.1.1 Fitted Q-learning
Watkins Q-learning popular off-policy model-free reinforcement learning algorithm (Even-Dar & Mansour, 2003). works action-value functions iteratively
approximates optimal value function. one-step Q-learning rule defined follows
Qi+1 (x, a) = (1 (x, a))Qi (x, a) + (x, a)(Tei Qi )(x, a),
(Tei Qi )(x, a) = g(x, a) + min Qi (Y, B),
BA(Y )

(x, a) learning rates random variable representing state generated pair (x, a) simulation, is, according probability distribution
p(x, a). known (Bertsekas & Tsitsiklis, 1996) (x) [0, 1] satisfy

X

(x, a) =




X

i2 (x, a) < ,

i=0

i=0

Q-learning algorithm converges probability one optimal action-value
function, Q , case lookup table representation state-action value
stored independently. speak method fitted Q-learning (FQL)
value function represented (typically parametric) function suitable function
space, F, iteration, updated value function projected back onto F.
useful observation need learning rate parameters overcome
effects random disturbances. However, deal deterministic problems,
part method simplified. resulting algorithm simply updates Q(x, a)
minimum previously stored estimation current outcome simulation,
also core idea LRTA* algorithm (Bulitko & Lee, 2006). dealt
deterministic resource allocation problems, applied simplification, well.
3.1.2 Evaluation Simulation
Naturally, large-scale problems cannot update states once. Therefore, perform Markov chain Monte Carlo (MCMC) simulations (Hastings, 1970; Andrieu, Freitas,
Doucet, & Jordan, 2003) generate samples model, used computing new approximation estimated cost-to-go function. Thus, set states
updated episode i, namely Xi , generated simulation. RAP-MDPs
acyclic, apply prioritized sweeping, means iteration, cost-to-go
estimations updated reverse order appeared simulation.
468

fiAdaptive Stochastic Resource Control



Assume, example, Xi = xi1 , xi2 , . . . , xiti set states update
value function iteration i, j < k implies xij appeared earlier
simulation xik . case order updates performed, xiti , . . . , xi1 .
Moreover, need uniformly optimal value function, enough good
approximation optimal cost-to-go function relevant states. state called
relevant appear positive probability application optimal policy. Therefore, sufficient consider case xi1 = x0 , xi1 first state
episode x0 (aggregated) initial state SSP problem.
3.1.3 Boltzmann Formula
order ensure convergence FQL algorithm, one must guarantee
cost-to-go estimation continuously updated. technique used often balance
exploration exploitation Boltzmann formula (also called softmin action selection):
(x, a) =

exp(Qi (x, a)/ )
,
P
exp(Qi (x, b)/ )

bA(x)

0 Boltzmann (or Gibbs) temperature, episode number. easy
see high temperatures cause actions (nearly) equiprobable, low ones cause
greater difference selection probability actions differ value estimations.
Note applied Boltzmann formula minimization, viz., small values result
high probability. advised extend approach variant simulated annealing
(Kirkpatrick, Gelatt, & Vecchi, 1983) Metropolis algorithm (Metropolis, Rosenbluth,
Rosenbluth, Teller, & Teller, 1953), means decreased time,
suitable, e.g., logarithmic, rate (Singh, Jaakkola, Littman, & Szepesvari, 2000).
3.2 Cost-to-Go Representations
Section 3.1 suggested FQL iteratively approximating optimal value function.
However, question suitable function space, onto resulted value functions
effectively projected, remained open. order deal large-scale problems (or
problems continuous state spaces) question crucial. section, first,
suggest features stochastic RAPs, describe two methods applied
compactly represent value functions. first simpler one applies hash tables
second, sophisticated one, builds upon theory support vector machines.
3.2.1 Feature Vectors
order efficiently apply function approximator, first, states actions
reformulated MDP associated numerical vectors representing, e.g., typical
features system. case stochastic RAPs, suggest using features follows:
resource R, resource state id, operation id operation
currently processed resource (could idle), well starting time
last (and currently unfinished) operation feature. model available
system, expected ready time resource stored instead.
469

fiCsaji & Monostori

task , task state id could treated feature assume
one following values: available (e.g., precedence constraints
satisfied), ready execution, processed finished. also advised
apply 1-out-of-n coding, viz., value associated separate bit.
case use action-value functions, action (resource-operation assignment)
resource id operation id could stored. model available,
expected finish time operation also taken account.
case model-free approach applies action-value functions, example,
feature vector would 3|R|+|T |+2 components. Note features representing
temporal values, advised use relative time values instead absolute ones.
3.2.2 Hash Tables
Suppose vector w = hw1 , w2 , . . . , wk i, component wi corresponds
feature state action. Usually, value estimations vectors
cannot stored memory. case one simplest methods applied
represent estimations hash table. hash table is, basically, dictionary
keys mapped array positions hash functions. components assume finite
values, e.g., finite-state, discrete-time case, key could generated follows.
Let us suppose wi 0 wi < mi , w seen number
mixed radix numeral system and, therefore, unique key calculated
(w) =

k
X

wi

i=1

i1


mj ,

j=1

(w) denotes key w, value empty product treated one.
hash function, , maps feature vector keys memory positions. precisely,
memory storing value estimations, hash function takes
form : rng() {0, . . . , 1}, rng() denotes range set function.
advised apply prime. case usual hashing function choice
(x) = x (mod d), namely, congruent x modulo d.
keys one item map position called collision.
case RAP-MDPs suggest collision resolution method follows. Suppose
value update feature vector state (or state-action pair) maps position
already occupied another estimation corresponding another item (which
detected, e.g., storing keys). collision estimation new
item overwrite old estimation MDP state corresponding
new item appears higher probability execution starting (aggregated)
initial state one corresponding old item. case model-free approach,
item state smaller current time component kept.
Despite simplicity, hash table representation several disadvantages, e.g.,
still needs lot memory work efficiently, cannot easily handle continuous values
and, stores individual data, moreover, generalize similar items.
next section present statistical approach deal issues, well.
470

fiAdaptive Stochastic Resource Control

3.2.3 Support Vector Regression
promising choice compactly representing cost-to-go function use support vector
regression (SVR) statistical learning theory. maintaining value function estimations, suggest using -SVRs proposed Scholkopf, Smola, Williamson,
Bartlett (2000). advantage classical -SVRs according which,
new parameter , number support vectors controlled. Additionally, parameter eliminated. First, core ideas -SVRs presented.
general, SVR addresses problem follows. given sample, set data
points {hx1 , y1 , . . . , hxl , yl i}, xi X input, X measurable space,
yi R target output. simplicity, shall assume X Rk , k N.
aim learning process find function f : X R small risk
Z
l(f, x, y)dP (x, y),
(3)
R[f ] =
X

P probability measure, responsible generation observations
l loss function, l(f, x, y) = (f (x) y)2 . common error function used
SVRs so-called -insensitive loss function, |f (x) y| = max {0, |f (x) y| }.
Unfortunately, cannot minimize (3) directly, since know P , given
sample, (generated, e.g., simulation). try obtain small risk minimizing
regularized risk functional average training sample
1

kwk2 + C Remp
[f ],
2

(4)

where, kwk2 term characterizes model complexity C > 0 constant
determines trade-off flatness regression amount

[f ] defined follows
deviations larger tolerated. function Remp
l


Remp
[f ] =

1X
|f (xi ) yi | .
l
i=1

measures -insensitive average training error. problem arises
try minimize (4) called empirical risk minimization (ERM). regression problems
usually Hilbert space F, containing X R type (typically non-linear) functions,
aim find function f close yi xi takes form
X
wj j (x) + b = wT (x) + b,
f (x) =
j

j F, wj R b R. Using Lagrange multiplier techniques, rewrite
regression problem dual form (Scholkopf et al., 2000) arrive final -SVR
optimization problem. resulting regression estimate takes form follows
f (x) =

l
X

(i )K(xi , x) + b,

i=1

Lagrange multipliers, K denotes inner product kernel
defined K(x, y) = h(x), (y)i, h, denotes inner product. Note , 6= 0
471

fiCsaji & Monostori

holds usually small subset training samples, furthermore, parameter b (and )
determined well, applying Karush-Kuhn-Tucker (KKT) conditions.
Mercers theorem functional analysis characterizes functions correspond
inner product space F. Basic kernel types include linear, polynomial, Gaussian
sigmoid functions. experiments RAP-MDPs used Gaussian kernels
also called radial basis function (RBF) kernels. RBF kernels defined
K(x, y) = exp( kx yk2 /(2 2 )), > 0 adjustable kernel parameter.
variant fitted Q-learning algorithm combined regression softmin action
selection described Table 1. simulates state-action trajectory model
updates estimated values state-action pairs appeared
simulation. RAP solutions described paper based algorithm.
notations pseudocode shown Table 1 follows. Variable contains
episode number, ti length episode j parameter time-steps inside
episode. Boltzmann temperature denoted , control policy applied
episode x0 (aggregated) initial state. State xij action aij correspond step
j episode i. Function h computes features state-action pairs denotes learning
rates. Finally, Li denotes regression sample Qi fitted value function.
Although, support vector regression offers elegant efficient solution value
function representation problem, presented hash table representation possibility
much easier implement, also requires less computation,
thus, provides faster solutions. Moreover, values hash table could accessed
independently; one reasons applied hash tables dealt
distributed solutions, e.g., architectures uniform memory access. Nevertheless,
SVRs advantages, importantly, generalize similar data.

1.
2.
3.
4.
5.
6.
7.

Regression Based Q-learning
Initialize Q0 , L0 , let = 1.
Repeat (for episode)
Set soft semi-greedyh policy w.r.t. Qi1 , e.g.,

P
exp(Q
(x,
b)/
)
.
(x, a) = exp(Qi1 (x, a)/ )/
i1
bA(x)

Simulate state-action trajectory x0 using policy .
j = ti 1 (for state-action pair episode)
Determine features state-action pair, yji = h(xij , aij ).


Compute new action-value
h estimation xj aj , e.g.,
zji = (1 )Qi1 (xij , aij ) + g(xij , aij ) + minbA(xi

j+1

8.
9.
10.
11.
12.



,
b)
.
Q
(x
i1
)
j+1

End loop (end state-action

processing)

ff
Update sample set Li1 yji , zji : j = 1, . . . , ti , result Li .
Calculate Qi fitting smooth regression function sample Li .
Increase episode number, i, decrease temperature, .
terminating conditions met, e.g., reaches limit
estimated approximation error Q gets sufficiently small.
Output: action-value function Qi (or (Qi ), e.g., greedy policy w.r.t. Qi ).
Table 1: Pseudocode regression-based Q-learning softmin action selection.
472

fiAdaptive Stochastic Resource Control

3.3 Additional Improvements
Computing (close-to) optimal solution RL methods, (fitted) Q-learning, could
inefficient large-scale systems, even apply prioritized sweeping capable
representation. section present additional improvements order speed
optimization process, even expense achieving suboptimal solutions.
3.3.1 Rollout Algorithms
experiments, presented Section 4, turned using suboptimal base policy, greedy policy respect immediate costs, guide
exploration, speeds optimization considerably. Therefore, initial stage
suggest applying rollout policy, limited lookahead policy, optimal
cost-to-go approximated cost-to-go base policy (Bertsekas, 2001). order
introduce concept precisely, let greedy policy w.r.t. immediate-costs,
(x) arg min g(x, a).
aA(x)

value function denoted J . one-step lookahead rollout policy based
policy , improvement (cf. policy iteration), calculated
h

(x) arg min E G(x, a) + J (Y ) ,
aA(x)

random variable representing state generated pair (x, a) simulation, is, according probability distribution p(x, a). expected value (viz.,
expected costs cost-to-go base policy) approximated Monte Carlo simulation several trajectories start current state. problem deterministic,
single simulation trajectory suffices, calculations greatly simplified.
Take look Figure 6 illustration. scheduling theory, similar (but simplified)
concept found rollout policy would called dispatching rule.

Figure 6: evaluation state x rollout algorithms deterministic (left)
stochastic (right) case. Circles denote states rectangles represent actions.
two main issues suggest application rollout algorithms initial
stages value function approximation-based reinforcement learning follows:
473

fiCsaji & Monostori

1. need several initial samples first application approximation techniques
first samples generated simulations guided rollout policy.
2. General reinforcement learning methods perform quite poorly practice without
initial guidance. However, learning algorithm start improving rollout
policy , especially, case apply (fitted) Q-learning, learn directly
trajectories generated rollout policy, since off-policy learning method.
numerical experiments showed rollout algorithms provide significant speedup.
3.3.2 Action Space Decomposition
large-scale problems set available actions state may large,
slow system significantly. current formulation RAP number
available actions state O(|T | |R|). Though, even real world situations |R|
is, usually, large, could contain thousands tasks. Here, suggest
decomposing action space shown Figure 7. First, system selects task, only,
moves new state task fixed executing resource
selected. case state description extended new variable {},
denotes case task selected yet. every case
system select executing resource selected task. Consequently, new
action space = A1 A2 , A1 = { av | v } {a } A2 = { ar | r R }.
result, radically decreased number available actions, however, number
possible states increased. experiments showed reasonable trade-off.

Figure 7: Action selection (up) (down) action space decomposition.
474

fiAdaptive Stochastic Resource Control

3.3.3 Clustering Tasks
idea divide-and-conquer widely used artificial intelligence recently
appeared theory dealing large-scale MDPs. Partitioning problem
several smaller subproblems also often applied decrease computational complexity
combinatorial optimization problems, example, scheduling theory.
propose simple still efficient partitioning method practically important class performance measures. real world situations tasks often
release dates due dates, performance measure, e.g., total lateness number
tardy tasks, depends meeting deadlines. Note measures regular.
denote (possibly randomized) functions defining release due dates
tasks : N B : N, respectively. section restrict
performance measures regular depend due dates. order cluster
tasks, need definition weighted expected slack time given follows
h

X
w(s) E B(v) A(v) D(s, v) ,
Sw (v) =
s(v)

(v) = { | hs, vi dom(D) } denotes set resource states task v
processed, w(s) weights corresponding, example, likelihood
resource state appears execution, simply w(s) = 1/ |(v)|.

Figure 8: Clustering tasks according slack times precedence constraints.
order increase computational speed, suggest clustering tasks successive disjoint subsets T1 , . . . , Tk according precedence constraints expected
slack times; take look Figure 8 illustration. basic idea behind approach
handle constrained tasks first. Therefore, ideally, Ti Tj
two clusters < j, tasks Ti expected slack times smaller tasks Tj .
However, cases clustering simple, since precedence constraints
must also taken account clustering criterion priority. Thus,
hu, vi C, u Ti v Tj , j must hold. learning, first, tasks T1
allocated resources, only. episodes, fix allocation policy concerning
tasks T1 start sampling achieve good policy tasks T2 , on.
Naturally, clustering tasks two-edged weapon, making small clusters may
seriously decrease performance best achievable policy, making large clusters
475

fiCsaji & Monostori

may considerably slow system. technique, however, several advantages,
e.g., (1) effectively decreases search space; (2) reduces number available
actions states; and, additionally (3) speeds learning, since sample trajectories become smaller (only small part tasks allocated trial and, consequently,
variance total costs decreased). effects clustering relative size
clusters analyzed experimentally presented Section 4.5.
3.3.4 Distributed Sampling
Finally, argue presented approach easily modified order allow
computing policy several processors distributed way. Parallel computing
speed calculation solution. consider extensions algorithm
using shared memory distributed memory architectures. Let us suppose
k processors, denote set processors P = {p1 , p2 , . . . , pk }.
case parallel system shared memory architecture, e.g., UMA (uniform
memory access), straightforward parallelize computation control policy.
Namely, processor p P sample search space independently, using
same, shared value function. (joint) policy calculated using common,
global value function, e.g., greedy policy w.r.t. function applied.
Parallelizing solution using architecture distributed memory
challenging. Probably simplest way parallelize approach several processors
distributed memory let processors search independently letting
working own, local value functions. given time number iterations,
may treat best achieved solution joint policy. precisely, denote
aggregated initial state x0 , joint control policy defined follows
arg min J p (x0 )



arg min min Qp (x0 , a),
p (pP) aA(x0 )

p (pP)

J p Qp (approximate) state- action-value functions calculated processor p P. Control policy p solution processor p given number
iterations. numerical experiments usually applied 104 iterations.
Naturally, could many (more sophisticated) ways parallelize computation
using several processors distributed memory. example, time time
processors could exchange best episodes (trajectories lowest costs)
learn experiments others. way, could help improve
value functions other. numerical experiments, presented Section 4.3, showed
even simplest case, distributing calculation speeds optimization
considerably. Moreover, case shared memory speedup almost linear.
parallel computing represents promising way deal large-scale systems,
theoretical experimental investigation would important. example, harmonizing exploration processors, speedup could improved.

4. Experimental Results
section experimental results benchmark industry-related problems
presented. experiments highlight characteristics solution.
476

fiAdaptive Stochastic Resource Control

4.1 Testing Methodology
order experimentally study resource control approach, simulation environment
developed C++. applied FQL and, cases, SVRs
realized LIBSVM free library support vector machines (Chang & Lin, 2001).
centering scaling data interval [0, 1], used Gaussian kernels
shrinking techniques. always applied rollout algorithms action decomposition,
clustering used tests presented Section 4.5, furthermore, distributed sampling
applied test shown Section 4.3. latter cases (tests clustering
distributed sampling) used hash tables approximately 256Mb hash memory.
performance algorithm measured follows. Testing took place two
main fields: first one benchmark scheduling dataset hard problems,
one simulation real world production control problem. first case best
solution, viz., optimal value (aggregated) initial state, J (x0 ) = mina Q (x0 , a),
known test instances. hard instances occurred
lower upper bounds known, e.g., J1 (x0 ) J (x0 ) J2 (x0 ). cases
assumed J (x0 ) (J1 (x0 ) + J2 (x0 ))/2. Since estimations good (viz.,
length intervals short), simplification might introduce considerable
error performance estimations. latter test case generated problems
generator way J (x0 ) known concerning constructed problems.
performance presented tables section, precisely average, E ,
standard deviation, (Ei ), error iteration computed follows
v
u
N
N h
i2
X
u1 X


1


Gj J (x0 ) ,

(Ei ) =
Gji J (x0 ) E ,
Ei =
N
N
j=1

j=1

Gji denotes cumulative incurred costs iteration sample j N
sample size. Unless indicated otherwise, sample contained results 100 simulation
trials parameter configuration (which associated rows tables).
shown Section 2.5.2, RAP-MDPs aperiodic, moreover,
APP property, therefore, discounting necessary achieve well-defined problem.
However, order enhance learning, still advised apply discounting and, therefore,
give less credit events farther current decision point. Heuristically,
suggest applying = 0.95 middle-sized RAPs (e.g., hundreds tasks),
problems benchmark dataset, = 0.99 large-scale RAPs (e.g.,
thousands tasks), problems industry-related experiments.
4.2 Benchmark Datasets
ADP based resource control approach tested Hurinks benchmark dataset
(Hurink, Jurisch, & Thole, 1994). contains flexible job-shop scheduling problems (FJSPs)
630 jobs (30225 tasks) 515 machines. applied performance measure
maximum completion time tasks (makespan). problems hard,
means, e.g., standard dispatching rules heuristics perform poorly them.
dataset consists four subsets, subset contains 60 problems. subsets (sdata,
edata, rdata, vdata) differ ratio machine interchangeability (flexibility),
477

fiCsaji & Monostori

shown flex(ib) columns Tables 3 2. columns label n iters (and
avg err) show average error carrying altogether n iterations. std
dev columns tables section contain standard deviation sample.
Table 2 illustrates performance typical dataset instances also gives
details them, e.g., number machines jobs (columns labels mcs
jbs). Table 3 summarized performance benchmark datasets shown.
benchmark configuration
dataset inst mcs jbs flex
sdata
mt06
6
6
1
sdata
mt10
10
10
1
sdata
la09
5
15
1
sdata
la19
10
10
1
sdata
la39
15
15
1
sdata
la40
15
15
1
edata
mt06
6
6 1.15
edata
mt10
10
10 1.15
edata
la09
5
15 1.15
edata
la19
10
10 1.15
edata
la39
15
15 1.15
edata
la40
15
15 1.15
rdata
mt06
6
6
2
rdata
mt10
10
10
2
rdata
la09
5
15
2
rdata
la19
10
10
2
rdata
la39
15
15
2
rdata
la40
15
15
2
vdata
mt06
6
6
3
vdata
mt10
10
10
5
vdata
la09
5
15
2.5
vdata
la19
10
10
5
vdata
la39
15
15
7.5
vdata
la40
15
15
7.5

average error (standard deviation)
1000 iters
5000 iters
10 000 iters
1.79 (1.01) %
0.00 (0.00) %
0.00 (0.00) %
9.63 (4.59) %
8.83 (4.37) %
7.92 (4.05) %
5.67 (2.41) %
3.87 (1.97) %
3.05 (1.69) %
11.65 (5.21) %
6.44 (3.41) %
3.11 (1.74) %
14.61 (7.61) % 12.74 (5.92) % 11.92 (5.63) %
10.98 (5.04) %
8.87 (4.75) %
8.39 (4.33) %
0.00 (0.00) %
0.00 (0.00) %
0.00 (0.00) %
18.14 (8.15) % 12.51 (6.12) %
9.61 (4.67) %
7.51 (3.33) %
5.23 (2.65) %
2.73 (1.89) %
8.04 (4.64) %
4.14 (2.81) %
1.38 (1.02) %
22.80 (9.67) % 17.32 (8.29) % 12.41 (6.54) %
14.78 (7.14) %
8.08 (4.16) %
6.68 (4.01) %
6.03 (3.11) %
0.00 (0.00) %
0.00 (0.00) %
17.21 (8.21) % 12.68 (6.81) %
7.87 (4.21) %
7.08 (3.23) %
6.15 (2.92) %
3.80 (2.17) %
18.03 (8.78) % 11.71 (5.78) %
8.87 (4.38) %
24.55 (9.59) % 18.90 (8.05) % 13.06 (7.14) %
23.90 (7.21) % 18.91 (6.92) % 14.08 (6.68) %
0.00 (0.00) %
0.00 (0.00) %
0.00 (0.00) %
8.76 (4.65) %
4.73 (2.23) %
0.45 (0.34) %
9.92 (5.32) %
7.97 (3.54) %
4.92 (2.60) %
14.42 (7.12) % 11.61 (5.76) %
6.54 (3.14) %
16.16 (7.72) % 12.25 (6.08) %
9.02 (4.48) %
5.86 (3.11) %
4.08 (2.12) %
2.43 (1.83) %

Table 2: Performance (average error deviation) typical benchmark problems.
Simple dispatching rules (which often applied practice), greedy ones,
perform poorly benchmark datasets. average error around 2530 %.
contrast, Table 3 demonstrates using method, average error less 5 %
10 000 iterations. shows learning beneficial type problems.
best performance benchmark datasets achieved Mastrolilli
Gambardella (2000). Though, algorithm performs slightly better ours,
solution exploits (unrealistic) specialties dataset, e.g., durations depend
resources; tasks linearly ordered jobs; job consists
478

fiAdaptive Stochastic Resource Control

number tasks. Moreover, cannot easily generalized stochastic resource control
problem algorithm faces. Therefore, comparison solutions hard.
benchmark
dataset flexib
sdata
1.0
edata
1.2
rdata
2.0
vdata
5.0
average
2.3

1000 iterations
avg err std dev
8.54 %
5.02 %
12.37 %
8.26 %
16.14 %
7.98 %
10.18 %
5.91 %
11.81 %
6.79 %

5000 iterations
avg err std dev
5.69 %
4.61 %
8.03 %
6.12 %
11.41 %
7.37 %
7.73 %
4.73 %
8.21 %
5.70 %

10 000 iterations
avg err std dev
3.57 %
4.43 %
5.26 %
4.92 %
7.14 %
5.38 %
3.49 %
3.56 %
4.86 %
4.57 %

Table 3: Summarized performance (average error deviation) benchmark datasets.
4.3 Distributed Sampling
possible parallelizations presented method also investigated, i.e., speedup
system relative number processors (in practise, multiprocessor environment emulated single processor, only). average number iterations
studied, system could reach solution less 5% error Hurinks dataset.
average speed single processor treated unit, comparison.
Figure 9 two cases shown: first case (rear dark bars) processor could
access common global value function. means processor could read write
global value function, otherwise, searched (sampled search space)
independently. Figure 9 demonstrates case speedup almost linear.
second case (front light bars) processor (local) value function
(which realistic strongly distributed system, GRID) and,
search finished, individual value functions compared. Therefore,
processors estimations own, search, local solution
best performing processor selected. Figure 9 shows achieved speedup case
stopped simulation processors achieved solution less 5 % error.

Figure 9: Average speedup relative number processors.
experiments show computation resource allocator function
effectively distributed, even commonly accessible value function available.
479

fiCsaji & Monostori

4.4 Industry Related Tests
also initiated experiments simulated factory modeling structure real
plant producing customized mass-products, especially, light bulbs. industrial data
came huge national industry-academia project, research development solutions support manufacturing enterprises coping requirements adaptiveness, realtimeness cooperativeness (Monostori, Kis, Kadar, Vancza, & Erdos, 2008).
optimal
slack ratio
50 %
40 %
30 %
20 %
10 %
0%

1000 iterations
avg err std dev
0.00 %
0.00 %
0.12 %
0.10 %
0.52 %
0.71 %
1.43 %
1.67 %
5.28 %
3.81 %
8.89 %
5.17 %

5000 iterations
avg err std dev
0.00 %
0.00 %
0.00 %
0.00 %
0.24 %
0.52 %
1.11 %
1.58 %
4.13 %
3.53 %
7.56 %
5.04 %

10 000 iterations
avg err std dev
0.00 %
0.00 %
0.00 %
0.00 %
0.13 %
0.47 %
1.05 %
1.49 %
3.91 %
3.48 %
6.74 %
4.83 %

Table 4: Summarized performance relative optimal slack ratio system.
Since, access historical data concerning past orders, used randomly
generated orders (jobs) random due dates. tasks process-plans jobs,
however, covered real products; well as, resources covered real machine types.
plant machines require product-type dependent setup times, special
tasks durations require resources processed, example,
cooling down. Another feature plant previously given time points
preemptions allowed, e.g., end work shift. applied performance measure
minimize number late jobs, viz., jobs finished due dates,
additional secondary measure minimize total cumulative lateness,
applied compare two schedules number late jobs.
experiments jobs due dates generated special
parameterizable generator way optimally none jobs late. Therefore,
known J (x0 ) = 0 error algorithm computed accordingly.
first case, shown Table 4, applied 16 machines 100 random jobs,
altogether contained 200 tasks. convergence properties studied relative
optimal slack ratio. deterministic case, e.g., slack ratio solution
n

1 X B(Ji ) F (Ji )
(%) =
,
n
B(Ji ) A(Ji )
i=1

n number jobs; A(J) B(J) denote release due date job J,
respectively; F (J) finish time job J relative solution %, namely, latest finish
time tasks job. Roughly, slack ratio measures tightness solution,
example, (%) > 0, shows jobs were, average, finished
due dates (%) < 0, indicates that, approximately, many jobs late.
(%) = 0, shows jobs meet due dates, job finished
480

fiAdaptive Stochastic Resource Control

time, spare (residual) times. optimal slack ratio mean
maximal achievable slack ratio (by optimal solution). experiments
values known special construction test problem instances.
applied optimal slack ratio measure hard problem is. first column
Table 4 shows optimal slack ratio percentage, e.g., 30 % means 0.3 slack ratio.
configuration
machs
tasks
6
30
16
140
25
280
30
560
50
2000
100
10000

1000 iterations
avg err std dev
4.01 %
2.24 %
4.26 %
2.32 %
7.05 %
2.55 %
7.56 %
3.56 %
8.69 %
7.11 %
15.07 % 11.89 %

5000 iterations
avg err std dev
3.03 %
1.92 %
3.28 %
2.12 %
4.14 %
2.16 %
5.96 %
2.47 %
7.24 %
5.08 %
10.31%
7.97 %

10 000 iterations
avg err std dev
2.12 %
1.85 %
2.45 %
1.98 %
3.61 %
2.06 %
4.57 %
2.12 %
6.04 %
4.53 %
9.11 %
7.58 %

Table 5: Summarized performance relative number machines tasks.
second case, shown Table 5, fixed optimal slack ratio
system 10 % investigated convergence speed relative plant size (number
machines) number tasks. last two experiments (configuration
2000 10 000 tasks) 10 samples generated, long runtime.
computation 10 000 iterations took approximately 30 minutes 50 machines & 2000
tasks configuration 3 hours 100 machines & 10000 tasks configuration1 .
results demonstrate ADP adaptive sampling based solution scales well
slack ratio size (the number machines task) problem.
4.5 Clustering Experiments
effectiveness clustering industry-related data also studied. considered
system 60 resources 1000 random tasks distributed among 400500 jobs (there
approximately 10002000 precedence constraints). tasks generated way
that, optimal case, none late slack ratio 20 %.
First, tasks ordered according slack times clustered.
applied 104 iterations cluster. computational time case using
one cluster treated unit. Table 6 average standard deviation
error computational speedup shown relative number tasks cluster.
results demonstrate partitioning search space results greater
speed, often accompanied better solutions. latter phenomenon explained fact using smaller sample trajectories generates smaller variance
preferable learning. hand, making small clusters may decrease performance (e.g., making 50 clusters 20 tasks current case). particular case
applying 20 clusters approximately 50 tasks cluster balances good performance
(3.02 % error average) remarkable speedup (approximately 3.28 ).
1. tests performed Centrino (Core-Duo) 1660Mhz CPU ( P4 3GHz) 1Gb RAM.

481

fiCsaji & Monostori

configuration
clusters
tasks
1
1000
5
200
10
100
20
50
30
33
40
25
50
20

performance 10 000 iterations per cluster
late jobs avg error
std dev
speed speedup
28.1
6.88 %
2.38 %
423
1.00
22.7
5.95 %
2.05 %
275
1.54
20.3
4.13 %
1.61 %
189
2.24
13.9
3.02 %
1.54 %
104
3.28
14.4
3.15 %
1.51 %
67
6.31
16.2
3.61 %
1.45 %
49
8.63
18.7
4.03 %
1.43 %
36
11.65

Table 6: Speedup performance relative number tasks cluster.
clustering tasks represents considerable help dealing large-scale RAPs,
theoretical experimental investigations promising.
4.6 Adaptation Disturbances
order verify proposed algorithm changing environments, experiments
initiated carried random JSPs aim minimizing makespan.
adaptive features system tested confronting unexpected events,
as: resource breakdowns, new resource availability (Figure 10), new job arrivals job
cancellations (Figure 11). Figures 10 11 horizontal axis represents time,
vertical one, achieved performance measure. figures made averaging
hundred random samples. tests 20 machines used dozens jobs.
test episode unexpected event (disturbance) time = 100.
change took place, considered two possibilities: either restarted iterative
scheduling process scratch continued learning, using current (obsolete) value
function. experienced latter approach much efficient.
explanation phenomenon value functions control policies
optimal value function Lipschitz continuously depend transition-probability
immediate-cost functions MDP (Csaji, 2008). Therefore, small changes
environmental dynamics cannot cause arbitrary large changes value function.
results numerical experiments, shown Figures 10 11, indicative
phenomenon average change value function large. Consequently,
applying obsolete value function change took place MDP preferable
restarting whole optimization process scratch. adaptive feature makes
ADP/RL based approaches even attractive practical applications.
results, black curves, show case obsolete value function approximation
applied change took place. performance would arise system
recomputed whole schedule scratch drawn gray part (a) Figure 10.
One notice even problem became easier change environment (at time = 100), example, new resource available (part (b) Figure 10)
job cancelled (part (b) Figure 11), performance started slightly decrease
( started slightly increase) event. phenomenon explained
fact even special cases system explore new configuration.
482

fiAdaptive Stochastic Resource Control

Figure 10: black curves, (t), show performance measure case resource breakdown (a) new resource availability (b) = 100; gray
curve, (t), demonstrates case policy would recomputed scratch.

Figure 11: black curves, (t), show performance measure resource control
case new job arrival (a) job cancellation (b) time = 100.

5. Concluding Remarks
Efficient allocation scarce, reusable resources time uncertain dynamic environments important problem arises many real world domains, production
control. paper took machine learning (ML) approach problem. First, general
resource allocation framework presented and, order define reactive solutions,
reformulated stochastic shortest path problem, special Markov decision process
(MDP). core idea solution application approximate dynamic programming (ADP) reinforcement learning (RL) techniques Monte Carlo simulation
stochastic resource allocation problems (RAPs). Regarding compact value function representations, two approaches studied: hash table support vector regression (SVR),
specially -SVRs. Afterwards, several additional improvements, application
limited-lookahead rollout algorithms initial phases, action space decomposition, task
clustering distributed sampling, suggested speeding computation
good control policy. Finally, effectiveness approach demonstrated results
numerical simulation experiments benchmark industry-related data.
experiments also supported adaptive capabilities proposed method.
483

fiCsaji & Monostori

several advantages ML based resource allocation preferable
kinds RAP solutions, e.g., classical approaches. favorable features follows:
1. presented RAP framework general, model several resource management problems appear practice, scheduling problems, transportation
problems, inventory management problems maintenance repair problems.
2. ADP/RL based methods essentially face problem presence uncertainties, since theoretical foundation provided MDPs. Moreover,
adapt unexpected changes environmental dynamics, breakdowns.
3. Additionally, algorithms theoretical guarantees finding (approximately)
optimal solutions, least limit, known. demonstrated experiments, actual convergence speed RAPs usually high, especially case
applying described improvements, clustering distributed sampling.
4. simulation experiments industrial data also demonstrate ADP/RL based
solutions scale well workload size problem and, therefore,
effectively applied handle real world RAPs, production scheduling.
5. Domain specific knowledge also incorporated solution. base policy
rollout algorithm, example, reflect priori knowledge structure
problem; later knowledge may appear exploration strategy.
6. Finally, proposed method constitutes any-time solution, since sampling
stopped number iterations. way, amount computational
time controlled, also important practical advantage.
Consequently, ML approaches great potentials dealing real world RAPs, since
handle large-scale problems even dynamic uncertain environments.
Several research directions possible. Now, conclusion paper,
highlight them. suggested improvements, clustering distributed
sampling, investigated since resulted considerable speedup.
guidance reinforcement learning rollout algorithms might effectively applied
applications, well. theoretical analysis average effects environmental
changes value functions could result new approaches handle disturbances.
Another promising direction would extend solution way also takes risk
account and, e.g., minimizes expected value total costs also
deviation, secondary criterion. Finally, trying apply solution pilot project
control real plant would interesting could motivate research directions.

6. Acknowledgments
work supported Hungarian Scientific Research Fund (OTKA), Grant No.
T73376, EU-funded project Coll-Plexity, Grant No. 12781 (NEST). Balazs
Csanad Csaji greatly acknowledges scholarship Hungarian Academy Sciences.
authors express thanks Tamas Kis contribution related tests
industrial data Csaba Szepesvari helpful discussions machine learning.
484

fiAdaptive Stochastic Resource Control

References
Andrieu, C., Freitas, N. D., Doucet, A., & Jordan, M. I. (2003). introduction MCMC
(Markov Chain Monte Carlo) machine learning. Machine Learning, 50, 543.
Aydin, M. E., & Oztemel, E. (2000). Dynamic job-shop scheduling using reinforcement
learning agents. Robotics Autonomous Systems, 33, 169178.
Beck, J. C., & Wilson, N. (2007). Proactive algorithms job shop scheduling probabilistic durations. Journal Artificial Intelligence Research, 28, 183232.
Bellman, R. E. (1961). Adaptive Control Processes. Princeton University Press.
Bertsekas, D. P. (2005). Dynamic programming suboptimal control: survey
ADP MPC. European Journal Control, 11 (45), 310334.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, Massachusetts.
Bertsekas, D. P. (2001). Dynamic Programming Optimal Control (2nd edition). Athena
Scientific, Belmont, Massachusetts.
Bulitko, V., & Lee, G. (2006). Learning real-time search: unifying framework. Journal
Artificial Intelligence Research, 25, 119157.
Chang, C. C., & Lin, C. J. (2001). LIBSVM: library support vector machines. Software
available on-line http://www.csie.ntu.edu.tw/cjlin/libsvm.
Csaji, B. Cs. (2008). Adaptive Resource Control: Machine Learning Approaches Resource Allocation Uncertain Changing Environments. Ph.D. thesis, Faculty
Informatics, Eotvos Lorand University, Budapest.
Csaji, B. Cs., Kadar, B., & Monostori, L. (2003). Improving multi-agent based scheduling
neurodynamic programming. Proceedings 1st International Conference
Holonic Mult-Agent Systems Manufacturing, September 13, Prague, Czech
Republic, Vol. 2744 Lecture Notes Artificial Intelligence, pp. 110123.
Csaji, B. Cs., & Monostori, L. (2006). Adaptive sampling based large-scale stochastic resource control. Proceedings 21st National Conference Artificial Intelligence
(AAAI 2006), July 1620, Boston, Massachusetts, pp. 815820.
Dolgov, D. A., & Durfee, E. H. (2006). Resource allocation among agents MDP-induced
preferences. Journal Artificial Intelligence Research, 27, 505549.
Even-Dar, E., & Mansour, Y. (2003). Learning rates Q-learning. Journal Machine
Learning Research, 5, 125.
Feinberg, E. A., & Shwartz, A. (Eds.). (2002). Handbook Markov Decision Processes:
Methods Applications. Kluwer Academic Publishers.
Gersmann, K., & Hammer, B. (2005). Improving iterative repair strategies scheduling
SVM. Neurocomputing, 63, 271292.
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains
application. Biometrika, 57, 97109.
485

fiCsaji & Monostori

Hatvany, J., & Nemes, L. (1978). Intelligent manufacturing systems - tentative forecast.
Niemi, A. (Ed.), link science applications automatic control;
Proceedings 7th IFAC World Congress, Vol. 2, pp. 895899.
Hurink, E., Jurisch, B., & Thole, M. (1994). Tabu search job shop scheduling problem
multi-purpose machines. Operations Research Spektrum, 15, 205215.
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization simulated annealing.
Science, 220 (4598), 671680.
Mastrolilli, M., & Gambardella, L. M. (2000). Effective neighborhood functions
flexible job shop problem. Journal Scheduling, 3 (1), 320.
Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., & Teller, E. (1953). Equation
state calculations fast computing machines. Journal Chemical Physics, 21,
10871092.
Monostori, L., Kis, T., Kadar, B., Vancza, J., & Erdos, G. (2008). Real-time cooperative enterprises mass-customized production. International Journal Computer
Integrated Manufacturing, (to appear).
Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.
Pinedo, M. (2002). Scheduling: Theory, Algorithms, Systems. Prentice-Hall.
Powell, W. B., & Van Roy, B. (2004). Handbook Learning Approximate Dynamic
Programming, chap. Approximate Dynamic Programming High-Dimensional Resource Allocation Problems, pp. 261283. IEEE Press, Wiley-Interscience.
Riedmiller, S., & Riedmiller, M. (1999). neural reinforcement learning approach learn
local dispatching policies production scheduling. Proceedings 16th International Joint Conference Artificial Intelligence, Stockholm, Sweden, pp. 764771.
Schneider, J. G., Boyan, J. A., & Moore, A. W. (1998). Value function based production
scheduling. Proceedings 15th International Conference Machine Learning,
pp. 522530. Morgan Kaufmann, San Francisco, California.
Scholkopf, B., Smola, A., Williamson, R. C., & Bartlett, P. L. (2000). New support vector
algorithms. Neural Computation, 12, 12071245.
Singh, S., Jaakkola, T., Littman, M., & Szepesvari, Cs. (2000). Convergence results
single-step on-policy reinforcement-learning algorithms. Machine Learning, 38 (3),
287308.
Sontag, E. D. (1998). Mathematical Control Theory: Deterministic Finite Dimensional
Systems. Springer, New York.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning. MIT Press.
Topaloglu, H., & Powell, W. B. (2005). distributed decision-making structure dynamic resource allocation using nonlinear function approximators. Operations Research, 53 (2), 281297.
Zhang, W., & Dietterich, T. (1995). reinforcement learning approach job-shop scheduling. Proceedings 14th International Joint Conference Artificial Intelligence, pp. 11141120. Morgan Kauffman.

486

fiJournal Artificial Intelligence Research 32 (2008) 607629

Submitted 01/08; published 06/08

Unifying Framework Structural Properties CSPs:
Definitions, Complexity, Tractability
Lucas Bordeaux

lucasb@microsoft.com

Microsoft Research
7 J J Thomson Avenue
Cambridge, CB3 0FB, United Kingdom

Marco Cadoli
Toni Mancini

cadoli@dis.uniroma1.it
tmancini@dis.uniroma1.it

Dipartimento di Informatica e Sistemistica
Sapienza Universita di Roma
Via Ariosto 25, I-00185 Roma, Italy

Abstract
Literature Constraint Satisfaction exhibits definition several structural properties possessed CSPs, like (in)consistency, substitutability interchangeability. Current tools constraint solving typically detect properties efficiently
means incomplete yet effective algorithms, use reduce search space
boost search.
paper, provide unifying framework encompassing properties known
far, CSP fields literature, shed light semantical relationships among them. gives unified comprehensive view topic, allows new,
unknown, properties emerge, clarifies computational complexity various
detection problems.
particular, among others, two new concepts, fixability removability emerge,
come ideal characterisations values may safely assigned removed variables domain, preserving problem satisfiability. two notions
subsume large number known properties, including inconsistency, substitutability
others.
computational intractability property-detection problems, following CSP approach determine number relaxations provide sufficient conditions tractability. particular, exploit forms language restrictions
local reasoning.

1. Introduction
Many Constraint Satisfaction Problems (CSPs) arise modelling real-life
applications exhibit structural properties distinguish random instances.
Detecting properties widely recognised effective way improving
solving process. end, several already identified, different
techniques developed order exploit them, goal reducing
search space explored. Good examples value inconsistency (Mackworth, 1977;
Montanari, 1974), substitutability interchangeability (Freuder, 1991), general
c
2008
AI Access Foundation. rights reserved.

fiBordeaux, Cadoli, & Mancini

forms symmetries (Crawford, Ginsberg, Luks, & Roy, 1996; Gent & Smith, 2000),
functional dependencies among variables (Li, 2000; Mancini & Cadoli, 2007).
Unfortunately, checking whether properties hold, (or thought be) often
computationally hard. example, let us consider interchangeability. Value said
interchangeable value b variable x every solution assigns x
remains solution x changed b, vice versa (Freuder, 1991). problem
checking interchangeability coNP-complete (cf. Proposition 4). Analogously, detecting
forms symmetry reduces graph automorphism problem (Crawford,
1992) (for known polynomial time algorithm, even evidence
NP-complete, Kobler, Schoning, & Toran, 1993).
end, order allow general algorithms exploit properties efficiently,
different approaches followed. First all, syntactic restrictions constraint
languages enforced, order allow efficient verification properties
interest. Alternatively, local versions properties defined,
used infer global counterparts, verified polynomial time.
instance local reasoning approach, instead checking whether value fully
interchangeable variable, Freuder (1991) proposes check whether value neighbourhood, k-interchangeable. task involves considering bounded-sized subsets
constraints, hence performed polynomial time. Neighbourhood
k-interchangeability sufficient (but necessary) conditions full interchangeability,
proven highly effective practice (cf., e.g., Choueiry & Noubir, 1998;
Lal, Choueiry, & Freuder, 2005).
paper give formal characterisation several properties CSPs
exploited order save search, present unifying framework allows
semantical connections emerge. properties well-known Constraint Programming literature, others used contexts (as databases),
others are, best knowledge, original, and, opinion, play
key role allowing deep understanding topic. particular, reconsider
notions inconsistency, substitutability interchangeability, propose fixable,
removable, implied value given variable, instances general
definition satisfiability-preserving transformation determined, dependent,
irrelevant variable. properties make possible transform problem simpler
one. Depending case, transformation guaranteed preserve solutions,
satisfiability problem, i.e., least solution, one exists. general,
properties detected either statically, preprocessing stage input
CSP (cf., e.g., Cadoli & Mancini, 2007), dynamically, search (since may arise
time). Moreover, cases dont even need explicitly verify whether
properties hold, guaranteed intrinsic characteristics problem.
instance, problems guaranteed unique solution. cases referred promise problems literature (Even, Selman, & Yacobi, 1984), meaning
addition problem description informed certain properties verifies
(cf. forthcoming Example 1 example).
formal characterisation properties connections allow us shed light
computational complexity recognition task elegant way, proving
608

fiA Unifying Framework Structural Properties CSPs

that, worst case, detection complex original problem.
particular, see Section 3.1, detecting proposed properties
coNP-complete task. holds also Freuders substitutability interchangeability
(this result is, best knowledge, original). Hence, order able
practically make relevant checks preprocessing search, investigate two
different approaches efficient verification proposed properties: additions
suitable restrictions constraint language, exploitation efficient forms local
reasoning, i.e., checking single constraints.
outline paper follows: giving intuitive example recalling
preliminaries, Section 2 present properties interested in, discuss
connections. Then, Section 3 focus complexity various propertydetection tasks. particular, Section 3.1 prove intractable; hence,
Section 3.2 focus relaxations guarantee tractability reasoning, investigating
two aforementioned approaches. Finally, Section 4 draw conclusions address
future work.

2. Hierarchy Properties
section, formally define several structural properties CSPs, discuss semantical relationships hold among them, show exploited
constraint solving.
2.1 Intuitive Example
order allow gentle introduction main properties investigated forthcoming sections, first introduce following example.
Example 1 (Factoring, Lenstra & Lenstra, 1990; Pyhala, 2004). problem simplified version one important problems public-key cryptography. Given
(large) positive integer Z fact product two different unknown prime
numbers X (different 1), goal identify two numbers.
intuitive formulation instance problem (i.e., given Z)
CSP, adequate arbitrarily large numbers, amounts encode combinatorial circuit
integer multiplication, follows: assuming Z n digits (in base b) z1 , . . . , zn ,
consider 2n variables x1 , . . . , xn y1 , . . . , yn one digit (in base b) X
(x1 y1 least significant ones). domain variables [0, b 1].
order maintain information carries, n + 1 additional variables c1 , . . . , cn+1
must considered, domain [0..(b 1)2 n/b].1
constraints (cf. Figure 1 intuition, x4 , x5 , x6 , y4 , y5 , y6
equal 0, omitted readability), following:
1. Constraints factors:
1. intuitive example, little abuse respect permitted forthcoming
Definition 1, allow, enhance readability, different variables defined different domains.
However, observe easy recover using standard well-known techniques
(e.g., adding new monadic constraints model smaller domains).

609

fiBordeaux, Cadoli, & Mancini

7 8 7
7 9 7=
0 6 13 18 12

4

0

49
63 72
49 56 49
6 2 7 2

56
63

3

49


9

c7

c6

c5

x3
y3

x2
y2

x1
y1 =

c3

c2

c1

c4

x3 y1 x2 y1 x1 y1
x3 y2 x2 y2 x1 y2

x3 y3 x2 y3 x1 y3


z6 z5 z4 z3 z2 z1

Figure 1: Factoring instance 627239, n = 6, b = 10

(a) Factors must different 1, or, equivalently, X 6= Z 6= Z must hold;
P
(b) every digit [1, n]: zi = ci + j,k[1,n]:j+k=i+1 (xj yk mod b);
2. Constraints carries:
(a) Carry least significant digit 0: c1 = 0;
(b) Carries digits: [2, n + 1], ci = ci1 +

P

j,k[1,n]:j+k=i

xj yk
b ;

(c) Carry significant digit 0: cn+1 = 0.

guess two factors X (i.e., variables x1 , . . . , xn y1 , . . . , yn )
made, values variables c1 , . . . , cn+1 completely determined, since follow
semantics multiplication. called functional dependence among
variables.
Functional dependencies arise often in, e.g., problems intermediate
state maintained, detection exploitation recognized
great importance efficiency point view, since lead significant reductions
search space (cf., e.g., Giunchiglia, Massarotto, & Sebastiani, 1998; Mancini & Cadoli,
2007; Cadoli & Mancini, 2007).
Moreover, presence functional dependencies among variables CSP highlights
second interesting problem, i.e., computing values dependent variables
choice defining ones made. problem, always subproblem
CSP dependencies, exactly one solution, hence, knowledge promise
useful solver. worth noting also problems intrinsically
exhibit promises. case of, e.g., Factoring add symmetry-breaking
constraint forcing x1 , . . . , xn lexicographically less equal y1 , . . . , yn .
new formulation guaranteed exactly one solution (since X prime).
Factoring problem exhibits also interesting properties: let us consider
instance Z given binary notation (i.e., b = 2) least significant
digit z1 equal 1. implies last digit factors X must 1.
Hence, say value 1 implied variables x1 y1 , 0 removable
and, precisely inconsistent. Moreover, problem, which, symmetry
broken, unique solution, also know variables x1 , . . . , xn y1 , . . . , yn
610

fiA Unifying Framework Structural Properties CSPs

determined (cf. forthcoming Definition 2), regardless instance,
functional dependence already discussed, variables encoding carries, i.e.,
ci (i [1, n]), dependent {x1 , . . . , xn , y1 , . . . , yn }.
problems unique solutions, known that, unfortunately, resolution remains intractable (cf., e.g., Papadimitriou, 1994; Valiant & Vijay V. Vazirani, 1986; Calabro,
Impagliazzo, Kabanets, & Paturi, 2003). However, exclude possibility
find good heuristics instances promise, look properties
implied existence unique solutions, exploited order improve
search process. particular, determined implied values play important role
classes problems. Factoring example shows, problems arise
frequently practice, either subproblems CSPs, presence functional dependencies (cf. also Mancini & Cadoli, 2007; Mancini, Cadoli, Micaletto, & Patrizi, 2008,
examples), intrinsic characteristics problem hand. general,
problem unique solution, variables determined value.
Another central role played removability property, characterises precisely
case value safely removed domain variable, preserving
satisfiability. property course weaker inconsistency (since solutions may
lost), safely used place interested finding solution
CSP, one exists, them.
2.2 Preliminaries
Definition 1 (Constraint Satisfaction Problem (CSP), Dechter, 1992). Constraint Satisfaction Problem triple hX, D, Ci where:
X finite set variables;
finite set values, representing domain variable;
C finite set constraints {c1 , . . . c|C| }, form
ci = ri (Vi ), Vi list k |X| variables X (the constraint scope),
ri Dk k-ary relation (the constraint relation). sometimes
denote set Vi variables constraint ci var(ci ).

Given set variables V domain D, V -tuple mapping
associates value tx every x V . value called x-component t. Given
V -tuple subset U V variables, denote t|U restriction
U , value variables U undefined elsewhere.
explicit assignment value x-component V -tuple (x V ) written
t[x := a].
Given CSP hX, D, Ci, X-tuple satisfies constraint ci =
ri (Vi ) C t|Vi ri .
denote Sol(ci ) set X-tuples satisfy ci . set cC Sol(c) X-tuples
satisfy constraints called solution space, denoted Sol(C).
solving CSP mean decide whether set Sol(C) non-empty and, so,
compute one (or all) solutions.
611

fiBordeaux, Cadoli, & Mancini

'$
'$

1

2

'$ '$
PP
&%
PP
...
&%
P
..

4

...
...
...

5

...
...
"
...
" &% &%
'$
.
"
"
"

3

&%

Figure 2: graph 3-colored

set X-tuples called search space denoted SD , simply
domain implicit context. relational operators selection, projection
complement useful: given V -relation c, subset U V value D,
denote x=a (ci ) (resp. x6=a (ci )) V -relation contains tuples ci whose
value x (resp. different a), U (ci ) set restrictions U tuples
ci (i.e., set U -tuples {t | t0 ci (t = t0 |U )}) ci set V -tuples {t | 6 ci }.
Note x=a (S) denotes search space obtained fixing value x a.
sake simplicity, sets X C considered globally defined shall
therefore omitted parameters definitions; search space
explicitly mentioned.
Example 2. Consider CSP hX, D, Ci modeling 3-coloring problem graph
Figure 2. that:
X = {x1 , . . . , x5 } set variables (one node),
= {R, G, B} set colors,
C following finite set constraints, one edge:
C = {N E(x2 , x3 ), N E(x3 , x4 ), N E(x2 , x4 ), N E(x4 , x5 )},
N E (not-equal) binary relation defined
({R, G, B} {R, G, B}) \ {hR, Ri, hG, Gi, hB, Bi}.

2.3 Definitions
section, formally present properties already introduced Section 2.1,
show applicability examples.
Definition 2. following properties defined search space S, variables x y,
values b, set variables V :
612

fiA Unifying Framework Structural Properties CSPs

fixable(S, x, a)
substitutable(S, x, a, b)

(t Sol(C) t[x := a] Sol(C))




tx = Sol(C)
t[x := b] Sol(C)



tx = Sol(C)
b 6= (t[x := b] Sol(C))

removable(S, x, a)



inconsistent(S, x, a)



Sol(C) tx 6=



implied(S, x, a)



Sol(C) tx =



determined(S, x)







Sol(C)
b 6= tx (t[x := b] 6 Sol(C))







Sol(C)
ty = t0y
t, t0 t0 Sol(C)
0
x V (tx = tx )


dependent(S, V, y)

irrelevant(S, x)





Sol(C)
(t[x := a] Sol(C))



interchangeability, well-known (Freuder, 1991) defined terms
substitutability:
interchangeable(S, x, a, b)

substitutable(S, x, a, b) substitutable(S, x, b, a)

cases ambiguity arises considered set constraints,
indicate using subscript (e.g., irrelevantC (S, x)). Note definitions
last three value-oriented, properties specific values domain.
contrary, determinacy, irrelevance, dependence variable-oriented properties
directly express results particular values domains important
relations value-oriented notions (cf. forthcoming Section 2.4).
already claimed Section 1, properties Definition 2 already
known, well beneficial effects search. particular, notion consistency
proposed Montanari (1974) Mackworth (1977), one best-studied
notions CSP. Substitutability interchangeability introduced Freuder (1991).
Implied values, also known literature backbones, seemingly first
studied explicitly Monasson, Zecchina, Kirkpatrick, Selman, Troyansky (1999).
best knowledge, notions removable fixable values (which, show
Section 2.4 play key role unifying framework proposed paper)
contrary considered. Determined, irrelevant dependent variables
studied number contexts logic, SAT, databases, cf., e.g., Beth definability (Chang & Keisler, 1990), dont care variables propositional formulae (Thiffault,
613

fiBordeaux, Cadoli, & Mancini

Bacchus, & Walsh, 2004; Safarpour, Veneris, Drechsler, & Lee, 2004), Audit problem (Jonsson & Krokhin, 2008), aware little work concerning application
context CSPs.
following examples illustrate properties.
Example 3 (Example 2 continued). Consider CSP modeling coloring problem
graph Figure 2. Let denote search space five variables x1 , . . . , x5
domain {R, G, B}. following properties hold:
fixable(,x1 ,R), since every solution t, t[x1 := R] solution well;
substitutable(,x1 ,R,G), since every solution tx1 = R, t[x1 := G]
solution well;
interchangeable(,x1 ,R,G),
substitutable(,x1 ,G,R) also holds;



previous

point





fact



removable(,x1 ,G), every solution tx1 = G, exists
different color K {R, B} x1 t[x1 := K] solution well;
irrelevant(,x1 ), actually replace x1 -component solution
value, since x1 denotes disconnected node graph.
properties holding variable x1 encodes disconnected node give
initial suggestions relationships exist among different notions.
nodes, have, example:
removable(,x5 ,G), every solution tx5 = G, exists
different color K {R, B} x5 t[x5 := K] solution well.
due fact node 5 connected node 4.
another, complex, example, consider following:
Example 4. Let CSP given boolean variables x, y, z, w, p, q, r, whose constraints
encoded formula below:
x (x y) ((z w) p) ((z y) (q r))
Denoting search space variables range {true, false}, have,
among others:
inconsistent(,x,false),

determined(,y),

fixable(,x,true),

dependent(,{z, w},p),

implied(,x,true),

dependent(,{z, y},q),

implied(,y,true),

dependent(,{z, y},r),

inconsistent(,y,false),

fixable(,q,true),
614

fiA Unifying Framework Structural Properties CSPs

implied(,q,true),

implied(,r,true).

fixable(,r,true),
definitions Definition 2 used construct solution-preserving transformations, i.e., mappings transform solutions solutions.
Definition 3 (solution-preserving transformation). solution-preserving transformation
total mapping
(t Sol(C) (t) Sol(C))
understand connection solution-preserving transformations aforementioned properties, consider following mappings:
1 (t) = t[x := a]
(
t[x := b]
2 (t) =




t[x := b] tx =
3 (t) = t[x := a] tx = b



otherwise

tx =
otherwise

Checking whether value fixable variable x, whether value substitutable value
b variable x, whether values b interchangeable variable x amounts
check whether mappings 1 , 2 3 (respectively) solution-preserving.
Solution-preserving transformations interesting allow us remove values search space preserving satisfiability problem. Moreover,
correspondence properties existence particular solution-preserving
mappings shows interesting connections hold among properties concepts, like symmetries. example, Mancini Cadoli (2005) give logical characterisation symmetries problem specifications, similar to, fact stronger
than, Definition 3. addition, general forms solution-preserving transformations could defined, that, e.g., allow also modification constraints, i.e.,
pairs (, ) (t Sol(C) (t) Sol((t, C))). interesting topic,
may lead definition general properties CSP, left
future research.
2.4 Semantical Relationships
already observed (cf. also Examples 3 4), several semantical relationships exist
among notions presented Definition 2, appear weaker,
others stronger. main connections clarified following theorem.
Theorem 1. relationships shown Figure 3 hold properties Definition 2.
Proof.
Dependence-determinacy. dependent(S, {x1 , . . . , xi }, y) iff every solution
value given function f values assigns x1 . . . xi , iff search
615

fiBordeaux, Cadoli, & Mancini

irrelevance

dependence

dependent(S, {x1 , . . . , xi }, y)
a1 . . . ai determined(x1 =a1 ,...,xi =ai (S), y)
determinacy
irrelevant(S, x)
fixable(S, x, a)

determined(S, x)
b implied(S, x, b)

implication
implied(S, x, b)
fixable(S, x, b)

implied(S, x, a)
b \ {a} inconsistent(S, x, b)

fixability

inconsistency

fixable(S, x, b)
substitutable(S, x, a, b)

inconsistent(S, x, a)
b substitutable(S, x, a, b)

substitutability

inconsistent(S, x, a)
removable(S, x, a)
removable(S, x, a)
b \ {a} substitutable(S, x, a, b)
removability

Figure 3: Semantical relationships among properties.

space x1 =a1 ,...,xi =ai (S) (where variables receive fixed value), solutions assign
value f (a1 , . . . , ) y.
Irrelevance-fixability. Sol(C) (t[x := a] Sol(C)) rewrites (t
Sol(C) t[x := a] Sol(C)).
Determinacy-implication. implied(S, x, b) holds b, 6= b
t[x := a] 6 Sol(C).
Implication-fixability. implied(S, x, b) means every Sol(C) tx = b. Hence
every Sol(C), t[x := b] = Sol(C).
Implication-inconsistency. implied(S, x, a) holds iff (tx 6= 6 Sol(C)), i.e., iff
b \ {a} (tx = b 6 Sol(C)). rewrites b \ {a} inconsistent(S, x, b).
V
Fixability-substitutability. Let = {a1 , .., ad }. i1..d substitutable(S, x, ai , b) iff
((tx = a1 tx = ad ) Sol(C) t[x := b] Sol(C)), rewrites
fixable(S, x, b).
Inconsistency-substitutability. Suppose inconsistent(S, x, a) holds. solution tx =
exists, hence implication tx = Sol(C) t[x := b] Sol(C) true choices
b.
Inconsistency-removability. argument inconsistency-substitutability.
Substitutability-removability. Suppose substitutable(S, x, a, b) holds value b 6= a.
written b (tx = Sol(C) t[x := b] Sol(C)), implies
b(tx = Sol(C) t[x := b] Sol(C)). latter rewrites (tx =
Sol(C) b t[x := b] Sol(C)).
616

fiA Unifying Framework Structural Properties CSPs

Note also implied values determined variables strongly related problems
unique solution: problem unique solution, variables
implied value (cf. Example 1), hence determined.
2.5 Exploiting Properties Constraint Solving
important reason aforementioned properties interesting that, detected, allow us reduce search space removing values active domains
variables. particular, several properties successfully exploited
purpose, like inconsistency (Montanari, 1974; Mackworth, 1977), substitutability (Freuder,
1991), irrelevance (Thiffault et al., 2004; Safarpour et al., 2004), implication (Monasson
et al., 1999), dependence (Mancini & Cadoli, 2007). However, thanks unifying framework Figure 3, show wide interest aforementioned properties
essentially relies relations two fundamental properties removability
fixability.
Theorem 2. Let CSP hX, D, Ci. value fixable variable x X,
satisfiable CSP 0 = hX, D, C {x = a}i obtained instantiating
variable x value satisfiable.
Proof. Assume 0 satisfiable. exists X-tuple satisfies
constraints. Since constraints 0 superset , also solution .
direction, assume satisfiable. exists solution
t. Since value fixable x, follows t[x := a] solution well. t[x := a]
satisfies also additional constraint x = 0 , hence latter problem satisfiable
well.
Theorem 3. Let CSP hX, D, Ci. value removable variable x X,
satisfiable CSP 0 = hX, D, C {x 6= a}i obtained removing
value domain variable x satisfiable.
Proof. 0 satisfiable, then, arguments proof Theorem 2,
satisfiable well.
direction, assume satisfiable. exists solution t.
tx 6= a, course also solution 0 . hand, tx = a, since value
removable x , follows exists b 6= t[x := b] solution
well. t[x := b] satisfies also additional constraint x 6= 0 , hence latter problem
also satisfiable.
results show key roles played fixability removability.
ideal properties checked order reduce domain variable.
interest notions essentially relies relationships fixability
removability. example, implied value interest essentially fixable,
irrelevant variable interest essentially fixable value domain,
substitutable value interest essentially removable, etc.
Also interest inconsistent values relies fact removable. However, inconsistency much stronger removability, removing inconsistent values
617

fiBordeaux, Cadoli, & Mancini

guarantees solutions (and satisfiability problem) preserved.
Hence removability plays exactly role inconsistency case want
find solutions problem simply want find one. situations removability
ideal property use.
properties, worth noting that, although may appear strong
unlikely first sight, still play precious role detected dynamically
search. example, Thiffault et al. (2004) show dynamically detecting
variables become irrelevant search (called dont care variables paper)
greatly speed-up non-CNF SAT solvers, actually separating problems independent
components.

3. Complexity reasoning
section show problem checking whether properties Definition 2 hold
coNP-complete. Hence, Section 3.2, try determine special cases checking
done efficiently (i.e., polynomial time).
3.1 Intractability Results
on, assume input given set constraints C set
variables X. also assume problem checking whether Sol(C) polynomial
size representation input. properties hold propositional logic
CSPs, sense Dechter (1992).
note problem checking whether properties Definition 2 hold
coNP, because, possible find counter-example guessing tuple
(two, case dependency) non-deterministic polynomial time, checking,
polynomial time, whether negation subformula parentheses holds (as
interchangeability, note logical two properties coNP still
coNP). Alternatively, coNP-membership follows observing succinct certificates exist
proving various properties hold (as example, certificate proving
variable x fixable V -tuple Sol(C) t[x := a] 6 Sol(C)).
rest section, proofs therefore restricted coNP-hardness part.
Theorem 4 (coNP-completeness properties Definition 2). Given CSP, following
tasks coNP-complete:
Checking whether value fixable, removable, inconsistent, implied, determined
variable x X;
Checking whether value substitutable to, interchangeable value b
variable x X;
Checking whether variable X dependent variables set V X;
Checking whether variable x X irrelevant.
Proof. prove checking properties hard coNP, reduce coNP-complete
problem, i.e., checking arbitrary CSP unsatisfiable, problem
618

fiA Unifying Framework Structural Properties CSPs

checking properties. particular, proofs hold even domain boolean,
case CSP written propositional formula , e.g., CNF. Hence,
let arbitrary propositional formula CNF set variables X, let x
variable x 6 X: unsatisfiability problem reduced
problem checking various properties. Moreover, semantical relationships defined
Theorem 1 allow infer elegant way hardness results several properties starting
others.
Irrelevance. Consider defined x. unsatisfiable unsatisfiable.
show unsatisfiable x irrelevant formula . Let us first
assume unsatisfiable. follows x irrelevant , models.
hand, let model . Interpretation {x true} model ,
{x false} not, implying x irrelevant .
Fixability, Substitutability. Results follow irrelevance, combined semantical relationships define irrelevance terms fixability, fixability terms
substitutability.
Dependence. Consider defined (x x). unsatisfiable
unsatisfiable, x dependent X unsatisfiable.
Determinacy. result follows dependence, combined semantical
relationship defines dependence terms determinacy.
Implication. Consider defined x. Value false implied x
unsatisfiable.
Inconsistency. result follows implication, combined semantical
relationship defines implication terms inconsistency.
Removability. Consider defined x. Value true removable x
unsatisfiable.
proof Theorem 4, observed intractability checking
properties holds also binary CSPs (i.e., CSPs constraints relate
two variables).
Theorem 5 (coNP-completeness properties Definition 2 binary CSPs). Given
CSP binary constraints domain size greater two, checking
properties Definition 2 coNP-complete.
Proof. give proof irrelevance only: others derived similarly.
Let = hX, D, Ci binary CSP. Consider arbitrary variable x 6 X let b
arbitrary distinct values D. Let denote CSP hX 0 , D, C 0 X 0 = X {x},
C 0 = C {x = a}. binary and, similarly proof Theorem 4, unsatisfiable
variable x irrelevant .
observation CSP encoding graph 3-colourability problem
made using binary constraints, thesis follows, since checking unsatisfiability
problem (which coNP-hard) reduced checking irrelevance binary CSP.
619

fiBordeaux, Cadoli, & Mancini

3.2 Tractability Results
Since detecting properties interested computationally hard problem,
natural question determine special cases checking done efficiently.
end, investigate two approaches: exhibit syntactical restrictions make
problem tractable, study local relaxations definitions polynomialtime checkable, therefore provide incomplete algorithms detecting various
properties.
3.2.1 Tractability Restricted Constraint Languages
number syntactical restrictions constraint satisfaction problem known
make tractable. instance, case boolean constraints, i.e., propositional formulae, satisfiability problem becomes tractable instance expressed using
Horn clauses, dual Horn clauses (i.e., clauses one negative literal),
clauses size 2, affine constraints (i.e., formulae built using XOR).
known Schaefers (1978) classes. natural wonder properties
identified Definition 2 also easy determine classes formulae. indeed case them, give general condition tractable
classes consistency property also tractable properties framework.
note recent paper (Jonsson & Krokhin, 2004) gives complete characterisation
tractable cases related property.
follows interested classes CSPs. end, define constraint
language domain finite set relations (of finite arity) elements
D, denote CSP(D ) set CSPs form hX, D, Ci every element
ci = ri (Vi ) C Vi X ri . (We observe constraint
language fixed, domain instances CSP(D ) fixed well.)
constraint language said closed instantiation (resp. complementation) whenever constraint ci = ri (Vi ) expressible language (i.e., ri ),
relation X\{x} (x=a (ci )), (resp. complementation ci ) represented
conjunction constraints language. means exist constraints
c01 = r10 (V10 ), . . . , c0k = rk0 (Vk0 ), Vj0 X rj0 j, X\{x} (x=a (ci ))
(resp. ci ) equivalent c01 c0k .2 Well known constraint languages boolean domains
closed instantiation complementation Horn clauses, dual
Horn clauses, 2CNF clauses affine constraints (since instantiation complement
Horn /dual Horn/2CNF clause affine constraint expressed conjunction
Horn /dual Horn/2CNF clauses affine constraints).
Theorem 6. Given constraint language , satisfiability problem CSP() tractable
closed instantiation, problem checking determinacy CSPs
class CSP() tractable well.
2. Note define closure respect complementation slightly different non-standard
meaning, negation constraint needs expressible conjunction constraints.
definitions impose definable single constraint language.

620

fiA Unifying Framework Structural Properties CSPs

Proof. Let us consider arbitrary instance hX, D, Ci CSP(). Variable x X
determined exist two different domain values, b D,
X\{x} (x=a (Sol(C))) X\{x} (x=b (Sol(C)))
empty, i.e., one CSPs hX, D, Ca,b i, with:
X = X \ {x},
Ca,b = {X\{x} (x=a (c)) | c C} {X\{x} (x=b (c)) | c C},
satisfiable. closed instantiation, constraints Ca,b written
conjunctions constraints . Hence, reduced determinacy-testing problem
solving O(|D|2 ) instances CSP(), tractable.
Theorem 7. Given constraint language , satisfiability problem CSP() tractable
closed instantiation complementation, problem checking
property among fixability, substitutability, interchangeability, inconsistency irrelevance
CSPs class CSP() tractable well.
Proof. start substitutability property note that, given arbitrary instance
hX, D, Ci CSP(), value substitutable b variable x X
X\{x} (x=a (Sol(C))) X\{x} (x=b (Sol(C))).
inclusion false, i.e., substitutability hold, set
X\{x} (x=a (Sol(C))) X\{x} (x=b (Sol(C)))

(1)



non-empty. Since x=b (Sol(C)) = x=b ( cC Sol(c)) =
cC (x=b (Sol(c))), have:
X\{x} (x=b (Sol(C))) = X\{x}



cC (x=b (Sol(c)))



.

Although projection intersection relations equal intersection
projections general, latter rewrites to:
\
X\{x} (x=b (Sol(c))).
cC

due fact select x eliminating byTprojection. prove
inclusion hold general: suppose cC X\{x} (x=b (Sol(c))).
means c C, exists tuple tc tc |X\{x} = tc x=b (Sol(c)).
follows tcx = b indeed unique
tx = b tc |X\{x} =
c C (t x=b (Sol(c))), i.e., X\{x} ( cC (x=b (Sol(c)))).
Formula (1) therefore equivalent to:
\

X\{x} (x=a (Sol(c)))

cC

[
cC

621

X\{x} (x=b (Sol(c)))

fiBordeaux, Cadoli, & Mancini

solution exists (and therefore substitutability hold) one sets
\
X\{x} (x=a (Sol(c))) X\{x} (x=b (Sol(c)))
cC

obtained every c C solution. language closed instantiation complement, express new constraint X\{x} (x=b (Sol(c))) conjunction C 0 constraints . sets solution iff CSP hX \ {x}, D, {X\{x} (x=a (c)) | c
C} C 0 satisfiable. reduced substitutability testing problem solving |C|
instances constraint satisfaction problem whose constraints language ,
tractable.
results fixability, interchangeability irrelevance follow directly,
semantical relationships shown Figure 3. Consistency value variable x
directly expressed satisfiability X\{x} (x=a (Sol(C))), expressed
, proof implication follows result.
slightly different closure property needed removability value variable x
since expressed X\{x} (Sol(C)) X\{x} (x6=a (Sol(C))).
Nevertheless, since boolean domains value v removable v substitutable
v, remarks closure properties Schaefers classes previous
theorem, obtain that:
Corollary 1. Testing fixability, substitutability, interchangeability, inconsistency, determinacy, irrelevance removability tractable boolean CSP constraints
either Horn clauses, dual Horn clauses, clauses size two affine constraints.
Unfortunately, dont tractability results dependence.
Table summarizes Theorems 6 7, Corollary 1:

Property
Determinacy
Fixability, substitutability,
interchangeability, inconsistency, irrelevance
Removability

Polynomial
Tractable closed instantiation
Tractable closed inst. compl.

Boolean Schaefer

worth noting conditions become restrictive reading table top-down.
Moreover, cases, observed tractability various propertydetection problems derives tractability constraint language . leads
serious concerns practical applicability results: CSP() tractable,
worrying identifying properties? Actually, preliminary studies show better results unlikely hold: example, proven constraint
language intractable, hope detecting properties like fixability, irrelevance, substitutability, inconsistency polynomial time. results become
622

fiA Unifying Framework Structural Properties CSPs

interest, suggesting two main directions research: first course
investigating practical benefit detecting properties real cases; second
exploit sufficient efficiently evaluable conditions properties hold,
regarded form approximate reasoning. One used forms kind
reasoning called local reasoning, addressed below.
3.2.2 Tractability Locality
important class incomplete criteria determine polynomial time whether complex property holds based local reasoning. approach proved extremely
successful consistency (Mackworth, 1977) interchangeability (Freuder, 1991) (cf.
also Choueiry & Noubir, 1998, classification different local forms interchangeability studied classified). propose section systematic investigation
whether local approach used value-based properties.
Verifying property P (C) set constraints C locally means verify
property well-chosen number sub-problems. must ensure approach
sound considered property:
Definition 4 (soundness local reasoning). say local reasoning
property P
sound if, subsets constraints C1 C, . . . , Ck C i1..k Ci = C,
(depending property):


W
V
P (C).
P (C)
i1..k P (Ci )
i1..k P (Ci )
typical choice granularity simply consider Ci contains one constraints C done, instance, arc-consistency. extreme, take
unique C1 = C, global checking. two extremes, wide range
intermediate levels defined (cf., e.g., Freuder, 1978, 1991).
Example 5. Consider CSP hX, D, Ci X = {x, y, z}, = {0, 1, 2} C =
{c1 , c2 , c3 }, whose elements defined follows:
x
0 1
1 2
2 1
2 2
c1 (x, y)

x z
1 0
1 2
2 0
2 2
c2 (x, z)

z
1 1
1 2
2 1
2 2
c3 (y, z)

observed value 1 substitutable 2 variable x. order check
property locally, consider suitable covering C1 , . . . , Ck set C, verify
induced subproblems. example, taking C1 = {c1 }, C2 = {c2 }, C3 = {c3 },
substitutableCi (S, x, 1, 2) every {1, 2, 3}. Since local reasoning sound substitutability (cf. Freuder, 1991), infer global property substitutableC (S, x, 1, 2)
holds.
Reasoning locally typically tractable focus moderate number subsets C,
condition bound complexity reasoning
623

fiBordeaux, Cadoli, & Mancini

subsets. typical assumption CSP bound arity constraints,
every constraint instance binary. case, cost determining
property constraint polynomial; choose reason locally considering
constraint separately, taking groups constraints bounded size, local
checking tractable.
Theorem 8. Local reasoning sound properties substitutability, interchangeability, fixability, inconsistency, implication, irrelevance,
determinacy, dependence. particuS
lar, C1 C, . . . , Ck C i1..k Ci = C:

V

substitutableC (S, x, a, b);
i1..k substitutableCi (S, x, a, b)

V
interchangeableC (S, x, a, b);

i1..k interchangeableCi (S, x, a, b)


fixableC (S, x, b);

W
inconsistentC (S, x, a);

i1..k inconsistentCi (S, x, a)
W


impliedC (S, x, a);
i1..k impliedCi (S, x, a)
V

i1..k

fixableCi (S, x, b)



irrelevantC (S, x);

W

determinedC (S, x);
i1..k determinedCi (S, x)

W
dependentC (S, V, y).

i1..k dependentCi (S, V, y)


V

i1..k

irrelevantCi (S, x)



Proof. result well-known consistency (Mackworth, 1977), substitutability
interchangeability (Freuder, 1991). Fixability variable x value b expressed
6= b (substitutableC (S, x, a, b))
V
V V
Therefore,
(S,
x,
b)
(which

equivalent

C
i1..k fixable
a6=b
V
Vi
substitutable
(S,
x,
a,
b)


substitutable
(S,
x,
a,
b)),



Ci
Ci
a6=b
V
substitutable
(S,
x,
a,
b),

means
fixable
(S,
x,
b).

similar
argument
works
C
C
a6=b
irrelevance, analogously defined terms fixability (cf. Figure 3).
implication, value implied variable x Ci , tuples tx 6=
violate constraints Ci fortiori belong Sol(C). Similarly determinacy: variable x determined Ci , tuples Sol(Ci )
t[x := b] 6 Sol(Ci ) b 6= tx . Hence cannot solution Sol(C)
t[x := b] Sol(C) b 6= tx . Finally, dependence, exists Ci
dependentC (S, V, y) holds (it enough consider sets constraints Ci

V var(Ci ) 6= var(Ci ) dependentC (S, V var(Ci ), y)),

have, definition, t, t0 Sol(Ci ) (x V var(Ci ) (tx = t0x )) ty = t0y ,
dependentC (S, V, y) also holds, since solution whole problem must satisfy also
Ci .
Example 6 (Example 5 continued). Value 2 fixable z. inferred
performing local reasoning follows:
624

fiA Unifying Framework Structural Properties CSPs

fixableC1 (S, z, 2) holds, since z occur scope c1 ;
fixableC2 (S, z, 2) holds, since tuple belongs c2 , also t[z := 2] belongs
c2 ;
fixableC3 (S, z, 2) holds, argument.
Since Theorem 8 local reasoning sound fixability, infer fixableC (S, z, 2)
holds.
one (value-based) property, namely removability, local approach
unfortunately sound:
Theorem 9. Local reasoning sound removability property.
Proof. Take C = C1 C2 , C1 defined x C2 x y. Suppose
domain values {1, 2, 3}. Value 2 x removable constraints considered
independently since, cases, change value solution assigns 2
x another value. Still, value 2 removable x conjunction. see
why, consider solution hx, yi = h2, 2i, observe neither h1, 2i h3, 2i
solutions.
Note removing values shown removable locally even make
satisfiable problem unsatisfiable: furthermore add constraints C3 , defined
x 6= 1 C4 , defined x 6= 3, value 2 x removable constraint,
(global) solution actually assigns value 2 x.
result, although negative, fact interesting, gives ex-post justification extensive use made last decades stronger notions, like
inconsistency substitutability, imply removability (cf. Figure 3). main
reason current tools frameworks CP try detect properties order
remove values active domain variables. naturally relies removability property (cf. Section 2.5). However, reason removability directly
used intractable. reason, stronger notions like consistency substitutability forms removability commonly used. Actually,
unlike full-fledged removability (cf. Theorem 9), properties detected efficiently,
incompletely, local reasoning. Hence, raises interesting open issue:
exist new (i.e., substitutability inconsistency) properties
local reasoning sound imply removability?
end section noting local version fixability property indeed
generalisation arbitrary domains pure literal rule (Davis & Putnam, 1960)
well-known case boolean constraints conjunctive normal form. pure
literal rule exploits cases constraint (clause) problem positive
(resp. negative) occurrence variable x. case, assigning value 0 (resp. 1) x
preserves satisfiability problem: solution tx = 1 exists, t[x := 0]
also solution since clause constrains x value 1.
625

fiBordeaux, Cadoli, & Mancini

Example 7. Consider following propositional formula CNF:
(x z) (x z) (y z)
Since x occur clause, assign x 1, maintain satisfiability
formula: solution , assignment t[x := 1] solution well.
clear pure literal rule detects fixability based reasoning local
clause (a variable x fixable to, say, 1 clause iff clause contain literal
x, pure literal rule checks condition holds every constraint).
generalisation pure literal rule has, best knowledge, proposed
CSP, generalisation pure literal rule QBF applied solvers
Quantified CSP name pure value rule (Gent, Nightingale, & Stergiou, 2005).
observed proposal fact local relaxation generalisation
quantified constraints fixability. shortly discussed Section 4,
properties presented paper generalized Quantified CSP elegant way,
many local relaxations remain valid.

4. Conclusions Perspectives
paper reconsidered structural properties CSPs extensively studied
exploited order simplify search. properties may course useful also
tasks, e.g., classification update solutions, compacting solution
space, supporting explanation interaction users.
provided unifying framework properties, clarifies semantical
relationships allows new ones emerge. argued new notions,
namely fixability removability play key role deep understanding topic,
ideal characterisations values fixed removed preserving
satisfiability problem. Known properties, like inconsistency substitutability
thus suitable specialisations them.
tackled questions related automated detection different properties exploitation solving engine simplifying problems. particular,
showed detecting proposed properties general hard original
CSP. Hence, order find efficient ways verification, investigated, according
CSP approach, two main lines: addition suitable restrictions constraint language approximation reasoning task exploiting local versions various
notions. Moreover, discussed cases properties may arise explicit
promises made users. case problems properties functional
dependencies unique solutions.
Two perspectives raised work concern new central properties
emerged it. identified removability property ideal characterisation values removed preserving satisfiability. Unfortunately,
negative results (coNP-completeness detection property impossibility local reasoning) make impossible directly use removability property practice.
gives ex-post justification extensive use made last decades
stronger notions (like inconsistency substitutability) imply removability, yet
626

fiA Unifying Framework Structural Properties CSPs

checked tractable means (of course price losing completeness). interesting
problem thus determine new cases removability-checking tractable.
Also, benefits fixability long known boolean case, since
property used form pure literal rule many SAT solvers. However,
generalisation property CSPs considered far.
Finally, proposed framework allows natural elegant generalisation case
Quantified CSP. particular, related work (Bordeaux, Cadoli, & Mancini, 2008)
propose new notion outcome natural counterpart quantified level
concept solution CSP. notion mind, properties studied
paper straightforwardly restated Quantified CSP, well local
relaxations, new, even general concepts emerge (the so-called shallow properties,
may impact also pure existential CSP level). opens important
new horizons, allowing QCSP solvers perform smarter reasoning input problem,
taking proper account quantifiers prefix, today usually ignored.

Acknowledgments
paper extended revised version Bordeaux, Cadoli, Mancini (2004).

References
Bordeaux, L., Cadoli, M., & Mancini, T. (2004). Exploiting fixable, substitutable determined values constraint satisfaction problems. Baader, F., & Voronkov, A.
(Eds.), Proceedings Eleventh International Conference Logic Programming Automated Reasoning (LPAR 2004), Vol. 3452 Lecture Notes Computer
Science, pp. 270284, Montevideo, Uruguay. Springer.
Bordeaux, L., Cadoli, M., & Mancini, T. (2008). Generalizing consistency constraint properties quantified constraints. ACM Transactions Computational
Logic. appear.
Cadoli, M., & Mancini, T. (2007). Using theorem prover reasoning constraint
problems. Applied Artificial Intelligence, 21 (4/5), 383404.
Calabro, C., Impagliazzo, R., Kabanets, V., & Paturi, R. (2003). complexity Unique
k-SAT: isolation lemma k-CNFs. Proceedings Eighteenth IEEE Conference Computational Complexity (CCC 2003), p. 135 ff., Aarhus, Denmark. IEEE
Computer Society Press.
Chang, C. C., & Keisler, H. J. (1990). Model Theory, 3rd ed. North-Holland.
Choueiry, B. Y., & Noubir, G. (1998). computation local interchangeability
Discrete Constraint Satisfaction Problems. Proceedings Fifteenth National
Conference Artificial Intelligence (AAAI98), pp. 326333, Madison, WI, USA.
AAAI Press/The MIT Press.
Crawford, J. M. (1992). theoretical analysis reasoning symmetry first-order
logic (extended abstract). Proceedings Workshop Tractable Reasoning,
conjunction Tenth National Conference Artificial Intelligence (AAAI92),
San Jose, CA, USA.
627

fiBordeaux, Cadoli, & Mancini

Crawford, J. M., Ginsberg, M. L., Luks, E. M., & Roy, A. (1996). Symmetry-breaking
predicates search problems. Proceedings Fifth International Conference
Principles Knowledge Representation Reasoning (KR96), pp. 148159,
Cambridge, MA, USA. Morgan Kaufmann, Los Altos.
Davis, M., & Putnam, H. (1960). computing procedure Quantification Theory. Journal
ACM, 7 (3), 201215.
Dechter, R. (1992). Constraint networks (survey). Encyclopedia Artificial Intelligence,
2nd edition, pp. 276285. John Wiley & Sons.
Even, S., Selman, A., & Yacobi, Y. (1984). complexity promise problems
applications public-key cryptography. Information Control, 61 (2), 159173.
Freuder, E. C. (1978). Synthesizing constraint expressions. Communications ACM,
21 (11), 958966.
Freuder, E. C. (1991). Eliminating interchangeable values Constraint Satisfaction Problems. Proceedings Ninth National Conference Artificial Intelligence
(AAAI91), pp. 227233, Anaheim, CA, USA. AAAI Press/The MIT Press.
Gent, I. P., & Smith, B. (2000). Symmetry breaking search constraint programming. Proceedings Fourteenth European Conference Artificial Intelligence
(ECAI 2000), pp. 599603, Berlin, Germany.
Gent, I., Nightingale, P., & Stergiou, K. (2005). QCSP-Solve: solver Quantified
Constraint Satisfaction Problems. Proceedings Nineteenth International Joint
Conference Artificial Intelligence (IJCAI 2005), pp. 138143, Edinburgh, Scotland.
Morgan Kaufmann, Los Altos.
Giunchiglia, E., Massarotto, A., & Sebastiani, R. (1998). Act, rest follow:
Exploiting determinism planning satisfiability. Proceedings Fifteenth
National Conference Artificial Intelligence (AAAI98), pp. 948953, Madison, WI,
USA. AAAI Press/The MIT Press.
Jonsson, P., & Krokhin, A. (2004). Recognizing frozen variables constraint satisfaction
problems. Theoretical Computer Science, 329 (13), 93113.
Jonsson, P., & Krokhin, A. (2008). Computational complexity auditing discrete attributes
statistical databases. Journal Computer System Sciences. appear.
Kobler, J., Schoning, U., & Toran, J. (1993). graph isomorphism problem: computational complexity. Birkhauser Press.
Lal, A., Choueiry, B., & Freuder, E. C. (2005). Interchangeability dynamic bundling
non-binary finite CSPs. Proceedings Twentieth National Conference
Artificial Intelligence (AAAI 2005), pp. 397404, Pittsburgh, PA, USA. AAAI
Press/The MIT Press.
Lenstra, A., & Lenstra, H. W. (1990). Algorithms number theory. van Leeuwen, J.
(Ed.), Handbook Theoretical Computer Science, vol. 1: Algorithms Complexity. MIT Press.
628

fiA Unifying Framework Structural Properties CSPs

Li, C. M. (2000). Integrating equivalency reasoning Davis-Putnam procedure.
Proceedings Seventeenth National Conference Artificial Intelligence
(AAAI 2000), pp. 291296, Austin, TX, USA. AAAI Press/The MIT Press.
Mackworth, A. K. (1977). Consistency networks relations. Artificial Intelligence, 8,
99118.
Mancini, T., & Cadoli, M. (2005). Detecting breaking symmetries reasoning
problem specifications. Proceedings Sixth International Symposium Abstraction, Reformulation Approximation (SARA 2005), Vol. 3607 Lecture Notes
Artificial Intelligence, pp. 165181, Airth Castle, Scotland, UK. Springer.
Mancini, T., & Cadoli, M. (2007). Exploiting functional dependencies declarative problem
specifications. Artificial Intelligence, 171 (1617), 9851010.
Mancini, T., Cadoli, M., Micaletto, D., & Patrizi, F. (2008). Evaluating ASP commercial
solvers CSPLib. Constraints, 13 (4).
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determining computational complexity characteristic phase transitions. Nature, 400,
133137.
Montanari, U. (1974). Networks constraints: Fundamental properties applications
picture processing. Information Sciences, 7 (2), 85132.
Papadimitriou, C. H. (1994). Computational Complexity. Addison Wesley Publishing Company, Reading, Massachussetts, Reading, MA.
Pyhala, T. (2004). Factoring benchmarks SAT solvers. Tech. rep., Helsinki university
technology.
Safarpour, S., Veneris, A., Drechsler, R., & Lee, J. (2004). Managing dont cares Boolean
Satisfiability. Proceedings Design Automation Test Conference Europe
(DATE 2004), pp. 260265, Paris, France. IEEE Computer Society Press.
Schaefer, T. J. (1978). complexity satisfiability problems. Proceedings Tenth
ACM Symposium Theory Computing (STOC78), pp. 216226, San Diego, CA,
USA. ACM Press.
Thiffault, C., Bacchus, F., & Walsh, T. (2004). Solving non-clausal formulas DPLL
search. Proceedings Tenth International Conference Principles Practice Constraint Programming (CP 2004), Vol. 3258 Lecture Notes Computer
Science, pp. 663678, Toronto, Canada. Springer.
Valiant, L. G., & Vijay V. Vazirani, V. V. (1986). NP easy detecting unique
solutions. Theoretical Computer Science, 47 (3), 8593.

629

fiJournal Artificial Intelligence Research 32 (2008) 419 - 452

Submitted 11/07; published 06/08

Dynamic Control Real-Time Heuristic Search
Vadim Bulitko

BULITKO @ UALBERTA . CA

Department Computing Science, University Alberta
Edmonton, Alberta, T6G 2E8, CANADA

Mitja Lustrek

MITJA . LUSTREK @ IJS . SI

Department Intelligent Systems, Jozef Stefan Institute
Jamova 39, 1000 Ljubljana, SLOVENIA

Jonathan Schaeffer

JONATHAN @ CS . UALBERTA . CA

Department Computing Science, University Alberta
Edmonton, Alberta, T6G 2E8, CANADA

Yngvi Bjornsson

YNGVI @ RU .

School Computer Science, Reykjavik University
Kringlan 1, IS-103 Reykjavik, ICELAND

Sverrir Sigmundarson

SVERRIR . SIGMUNDARSON @ LANDSBANKI .

Landsbanki London Branch, Beaufort House,
15 St Botolph Street, London EC3A 7QR, GREAT BRITAIN

Abstract
Real-time heuristic search challenging type agent-centered search agents
planning time per action bounded constant independent problem size. common problem
imposes restrictions pathfinding modern computer games large number
units must plan paths simultaneously large maps. Common search algorithms (e.g., A*,
IDA*, D*, ARA*, AD*) inherently real-time may lose completeness constant
bound imposed per-action planning time. Real-time search algorithms retain completeness
frequently produce unacceptably suboptimal solutions. paper, extend classic
modern real-time search algorithms automated mechanism dynamic depth subgoal
selection. new algorithms remain real-time complete. large computer game maps,
find paths within 7% optimal average expanding roughly single state per action.
nearly three-fold improvement suboptimality existing state-of-the-art algorithms
and, time, 15-fold improvement amount planning per action.

1. Introduction
paper study problem agent-centered real-time heuristic search (Koenig, 2001).
distinctive property search agent must repeatedly plan execute actions
within constant time interval independent size problem solved.
restriction severely limits range applicable heuristic search algorithms. instance, static
search algorithms A* (Hart, Nilsson, & Raphael, 1968) IDA* (Korf, 1985), re-planning
algorithms D* (Stenz, 1995), anytime algorithms ARA* (Likhachev, Gordon, &
Thrun, 2004) anytime re-planning algorithms AD* (Likhachev, Ferguson, Gordon,
Stentz, & Thrun, 2005) cannot guarantee constant bound planning time per action. LRTA*
c
2008
AI Access Foundation. rights reserved.

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

can, potentially low solution quality due need fill heuristic depressions (Korf,
1990; Ishida, 1992).
motivating example, consider autonomous surveillance aircraft context disaster response (Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjou, & Shimada, 1999).
surveying disaster site, locating victims, assessing damage, aircraft ordered fly
particular location. Radio interference may make remote control unreliable thereby requiring
certain degree autonomy aircraft using AI. task presents two challenges. First,
due flight dynamics, AI must control aircraft real time, producing minimum number
actions per second. Second, aircraft needs reach target location quickly due limited
fuel supply need find rescue potential victims promptly.
study simplified version problem captures two AI challenges abstracting away robot-specific details. Specifically, line work real-time heuristic
search (e.g., Furcy & Koenig, 2000; Shimbo & Ishida, 2003; Koenig, 2004; Botea, Muller, & Schaeffer, 2004; Hernandez & Meseguer, 2005a, 2005b; Likhachev & Koenig, 2005; Sigmundarson &
Bjornsson, 2006; Koenig & Likhachev, 2006) consider agent finite search graph
task traveling path current state given goal state. Within context measure
amount planning agent conducts per action length path traveled
start goal locations. two measures antagonistic reducing amount planning per action leads suboptimal actions results longer paths. Conversely, shorter paths
require better actions obtained larger planning effort per action.
use navigation grid world maps derived computer games testbed. games,
agent tasked go location map current location. Examples include
real-time strategy games (e.g., Blizzard, 2002), first-person shooters (e.g., id Software, 1993),
role-playing games (e.g., BioWare Corp., 1998). Size complexity game maps well
number simultaneously moving units maps continues increase every new generation games. Nevertheless, game unit agent must react quickly users command
regardless maps size complexity. Consequently, game companies impose time-peraction limit pathfinding algorithms. instance, Bioware Corp., major game company
collaborate with, sets limit 1-3 ms units computing paths time.
Search algorithms produce entire solution agent takes first action (e.g., A*
Hart et al., 1968) lead increasing action delays map size increases. Numerous optimizations
suggested remedy problems decrease delays (for recent example deployed forthcoming computer game refer Sturtevant, 2007). Real-time search addresses
problem fundamentally different way. Instead computing complete, possibly abstract, solution first action taken, real-time search algorithms compute (or plan)
first actions agent take. usually done conducting lookahead search fixed
depth (also known search horizon, search depth lookahead depth) around agents
current state using heuristic (i.e., estimate remaining travel cost) select next
actions. actions taken planning-execution cycle repeats (e.g., Korf, 1990).
Since goal state reached local searches, agent runs risks heading
dead end or, generally, selecting suboptimal actions. address problem, real-time
heuristic search algorithms update (or learn) heuristic function experience. existing
algorithms constant amount planning (i.e., lookahead search) per action. result,
tend waste CPU cycles heuristic function fairly accurate and, conversely, plan
enough heuristic function particularly inaccurate. Additionally, compute heuris420

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

tic respect distant global goal state put unrealistic requirements heuristic
accuracy demonstrate paper.
paper address problems making following three contributions. First,
propose two ways selecting lookahead search depth dynamically, per action basis. Second,
propose way selecting intermediate subgoals per action basis. Third, apply
extensions classic LRTA* (Korf, 1990) state-of-the-art real-time PR LRTS (Bulitko,
Sturtevant, Lu, & Yau, 2007) demonstrate improvements performance. resulting
algorithms new state art real-time search. illustrate, large computer game
maps new algorithms find paths within 7% optimal expanding single state
action. comparison, previous state-of-the-art, PR LRTS, 15 times slower per
action finding paths two three times suboptimal. Furthermore,
dynamically controlled LRTA* PR LRTS one two orders magnitude faster per action
A*, weighted A* state-of-the-art Partial Refinement A* (PRA*) (Sturtevant & Buro,
2005). Finally, unlike A* modern extensions used games, new algorithms provably
real-time slow maps become larger.
rest paper organized follows. Section 2 formulate problem real-time
heuristic search show core LRTA* algorithm extended dynamic lookahead
subgoal selection. Section 3 analyzes related research. Section 4 provides intuition dynamic
control search. Section 5 describe two approaches dynamic lookahead selection: one
based induction decision-tree classifiers (Section 5.1) one based precomputing depth
table using state abstraction (Section 5.2). Section 6 present approach selecting subgoals
dynamically. Section 7 evaluates efficiency extensions domain pathfinding.
conclude discussion applicability new approach general planning.
paper extends conference publication (Bulitko, Bjornsson, Lustrek, Schaeffer, & Sigmundarson, 2007) new set features decision tree approach, new way selecting
subgoals, additional real-time heuristic search algorithm (PR LRTA*) extended dynamic
control, numerous additional experiments detailed presentation.

2. Problem Formulation
define heuristic search problem directed graph containing finite set states weighted
edges, single state designated goal state. every time step, search agent single
current state, vertex search graph, takes action traversing out-edge current
state. edge positive cost associated it. total cost edges traversed agent
start state arrives goal state called solution cost. require algorithms
complete produce path start goal finite amount time path exists.
order guarantee completeness real-time heuristic search make assumption safe
explorability search problems. Namely, costs finite goal state reachable
state agent possibly reach start state.
Formally, algorithms discussed paper applicable heuristic search problem. keep presentation focused intuitive well afford large-scale empirical
evaluation, use particular type heuristic search problems, pathfinding grid worlds,
rest paper. However, discuss applicability new methods suggest
heuristic search problems Section 5.3 general planning problems Section 9.
421

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

computer-game map settings, states vacant square grid cells. cell connected
four cardinally (i.e., west, north, east, south) four diagonally neighboring cells. Outbound
edges vertex moves available corresponding cell rest paper

use terms action move interchangeably. edge costs 1 cardinal moves
2 diagonal moves. agent plans next action considering states local search space
surrounding current position. heuristic function (or simply heuristic) estimates (remaining)
travel cost state goal. used agent rank available actions select
promising one. paper consider admissible heuristic functions
overestimate actual remaining cost goal. agent modify heuristic function
state avoid getting stuck local minima heuristic function, well improve action
selection experience.
defining property real-time heuristic search amount planning agent
per action upper bound depend problem size. enforce property
setting real-time cut-off amount planning action. algorithm exceeds
cut-off discarded. Fast planning preferred guarantees agents quick reaction
new goal specification changes environment. measure mean planning time per action
terms CPU time well machine-independent measure number states expanded
planning. state called expanded successor states considered/generated
search. second performance measure study sub-optimality defined ratio
solution cost found agent minimum solution cost. Ratios close one indicate
near-optimal solutions.
core real-time heuristic search algorithms algorithm called Learning RealTime A* (LRTA*) (Korf, 1990). shown Figure 1 operates follows. long goal
state sglobal goal reached, algorithm interleaves planning execution lines 4 7.
generalized version added new step line 3 selecting search depth goal sgoal
individually execution step (the original algorithm uses fixed sglobal goal planning
searches). line 4, d-ply breadth-first search duplicate detection used find frontier states
precisely actions away current state s. frontier state s, value sum
cost shortest path s, denoted g(s, s), estimated cost shortest path
sgoal (i.e., heuristic value h(s, sgoal )). use standard path-max technique (Mero,
1984) deal possible inconsistencies heuristic function computing g + h values.
result, g + h values never decrease along branch lookahead tree. state
minimizes sum identified sfrontier line 5. heuristic value current state
updated line 6 (we keep separate heuristic tables different goals). Finally, take one step
towards promising frontier state sfrontier line 7.

3. Related Research
algorithms single-agent real-time heuristic search use fixed search depth, notable
exceptions. Russell Wefald (1991) proposed estimate utility expanding state use
control lookahead search on-line. one needs estimate likely additional search
change actions estimated value. Inaccuracies estimates overhead metalevel control led reasonable unexciting benefits combinatorial puzzle pathfinding.
additional problem relatively low branching factor combinatorial puzzles makes
difficult eliminate parts search space early on. problem likely occur grid422

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

LRTA*(sstart , sglobal goal )
1 sstart
2 6= sglobal goal
3
select search depth goal sgoal
4
expand successor states actions away, generating frontier
5
find frontier state sfrontier lowest g(s, sfrontier ) + h(sfrontier , sgoal )
6
update h(s, sgoal ) g(s, sfrontier ) + h(sfrontier , sgoal )
7
change one step towards sfrontier
8 end
Figure 1: LRTA* algorithm dynamic control.
based pathfinding. Finally, method adds substantial implementation complexity requires
non-trivial changes underlying search algorithm. contrast, approach search depth
selection easily interfaced real-time search algorithm search depth parameter
without modifying existing code.
Ishida (1992) observed LRTA*-style algorithms tend get trapped local minima
heuristic function, termed heuristic depressions. proposed remedy switch limited
A* search heuristic depression detected use results A* search
correct depression once. different approach two ways: first,
need mechanism decide switch real-time A* search thus avoid
need hand-tune control parameters Ishidas control module. Instead, employ automated
approach decide search horizon depth every action. Additionally, spend extra
time filling heuristic values within heuristic depression A* estimates.
Bulitko (2003a) showed optimal search depth selection highly beneficial realtime heuristic search. linked benefits avoiding so-called lookahead pathologies
deeper lookahead leads worse moves suggest practical way selecting lookahead depth dynamically. way proposed 2004 via use generalized definition
heuristic depressions (Bulitko, 2004). proposed algorithm extends search horizon incrementally search finds way depression. actions leading found
frontier state executed. cap search horizon depth set user. idea precomputing depth table heuristic values real-time pathfinding first suggested Lustrek
Bulitko (2006). paper extends work follows: (i) introduce intermediate goals,
(ii) propose alternative approach require map-specific pre-computation (iii)
extend evaluate state-of-the-art algorithm addition classic LRTA*.
long tradition search control two-player search. High-performance game-playing
programs games like chess checkers rely extensively search decide actions
take. search performed strict real-time constraints programs typically
minutes seconds deliberating next action. Instead using fixed-depth lookahead strategy programs employ sophisticated search control mechanisms maximizing
quality action decisions within given time constraints. search control techniques
coarsely divided three main categories: move ordering, search extensions/reductions,
time allotment. One earlier works dynamic move ordering history heuristic technique (Schaeffer, 1989), recent attempts include work training neural networks (Kocsis, 2003). exist large variety techniques adjusting search horizon
423

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

different branches within game tree; interesting continuations explored deeply
less promising ones terminated prematurely. Whereas early techniques
static, research focus shifted towards dynamic control well using machine-learning
approaches automatic parameterization (Buro, 2000; Bjornsson & Marsland, 2003). best
knowledge, none techniques applied single-agent real-time search.

4. Intuition Dynamic Search Control
observed literature common heuristic functions uniformly inaccurate (Pearl, 1984). Namely, tend accurate closer goal state less accurate
farther away. intuition fact follows: heuristic functions usually ignore certain constraints search space. instance, Manhattan distance heuristic sliding tile puzzle
would perfectly accurate tiles could pass other. Likewise, Euclidian distance
map ignores obstacles. closer state goal fewer constraints heuristic function
likely ignore and, result, accurate (i.e., closer optimal solution cost)
heuristic likely be.
intuition motivates adaptive search control real-time heuristic search. First, heuristic values inaccurate, agent conduct deeper lookahead search compensate
inaccuracies maintain quality actions. Deeper lookaheads generally found
beneficial real-time heuristic search (Korf, 1990), though lookahead pathologies (i.e., detrimental
effects deeper lookaheads action quality) observed well (Bulitko, Li, Greiner, &
Levner, 2003; Bulitko, 2003b; Lustrek, 2005; Lustrek & Bulitko, 2006). illustration, consider
Figure 2. Every state map shaded according minimum lookahead depth
LRTA* agent use select optimal action. Darker shades correspond deeper lookahead
depths. Notice many areas bright white, indicating shallowest lookahead depth
one sufficient. use intuition first control mechanism: dynamic selection
lookahead depth Section 5.

Figure 2: partial grid world map computer game Baldurs Gate (BioWare Corp., 1998).
Shades grey indicate optimal search depth values white representing one ply.
Completely black cells impassable obstacles (e.g., walls).
424

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Dynamic search depth selection helps eliminate wasted computation switching shallower
lookahead heuristic function fairly accurate. Unfortunately, help
heuristic function grossly inaccurate. Instead, calls deep lookahead order select
optimal action. deep search tremendously increases planning time and, sometimes, leads
violating real-time cut-off planning time per move. address issue, Section 6
propose second control mechanism: dynamic selection subgoals. idea straightforward:
far goal leads grossly inaccurate heuristic values, let us move goal closer
agent, thereby improving heuristic accuracy. computing heuristic function
respect intermediate, thus nearby, goal opposed distant global goal
final destination agent. Since intermediate goal closer global goal, heuristic
values states around agent likely accurate thus search depth picked
first control mechanism likely shallower. agent gets intermediate goal,
next intermediate goal selected agent makes progress towards actual global goal.

5. Dynamic Search Depth Selection
First, define optimal search depth follows. (s, sglobal goal ) state pair, true optimal action (s, sglobal goal ) take edge lies optimal path sglobal goal (there
one optimal action). (s, sglobal goal ) known, run series progressively
deeper LRTA* searches state s. shallowest search depth yields (s, sglobal goal )
optimal search depth (s, sglobal goal ). may search depth forfeit LRTA*s real-time
property also impractical compute. Thus, following subsections present two
different practical approaches approximating optimal search depth. equips LRTA*
dynamic search depth selection (i.e., realizing first part line 3 Figure 1). first
approach uses decision-tree classifier select search depth based features agents
current state recent history. second approach uses pre-computed depth database based
automatically built state abstraction.
5.1 Decision-Tree Classifier Approach
effective classifier needs input features useful predicting optimal search
depth, also efficiently computable agent real time. features use
classifier selected compromise two considerations, well domain independent. features calculated based properties states agent recently
visited, well features gathered shallow pre-search agents current state. Example
features are: distance state agent n steps ago, estimate distance
agents goal, number states visited pre-search phase updated heuristics.
Appendix features listed rationale behind explained.
classifier predicts optimal search depth current state. optimal depth
shallowest search depth returns optimal action. training classifier must thus label
training states optimal search depths. However, avoid pre-computing optimal actions,
make simplifying assumption deeper search always yields better action. Consequently,
training phase agent first conducts lookahead search pre-defined maximum depth, dmax ,
derive optimal action (under assumption). choice maximum depth domain
dependent would typically set largest depth still guarantees search return
within acceptable real-time requirement task hand. series progressively
425

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

shallower searches performed determine shallowest search depth, dDT , still returns
optimal action. process, given depth action returned differs
optimal action, progression stopped. enforces depths dDT dmax
agree best action. important improving overall robustness classification,
classifier must generalize large set states. depth dDT set class label
vector features describing current state.
classifier choosing lookahead depth, LRTA* augmented
(line 3 Figure 1). overhead using classifier consists time required collecting
features running classifier. overhead negligible classifier
implemented handful nested conditional statements. Collecting features takes
somewhat time but, careful implementation, overhead made negligible
well. Indeed, four history-based features efficiently computed small constant time,
keeping lookahead depth pre-search small (e.g., one two) overhead collecting
pre-search features usually dwarfed time planning phase (i.e., lookahead search)
takes. process gathering training data building classifier carried off-line
time overhead thus lesser concern.
5.2 Pattern Database Approach
nave approach would precompute optimal depth (s, sgoal ) state pair.
two problems approach. First, (s, sgoal ) priori upper-bounded independently
map size, thereby forfeiting LRTA*s real-time property. Second, pre-computing (s, sgoal )
(s, sgoal ) pairs (s, sgoal ) states on, instance, 512 512 cell computer game map
prohibitive time space complexity. solve first problem capping (s, sgoal )
fixed constant c 1 (henceforth called cap). solve second problem using automatically built abstraction original search space. entire map partitioned regions (or
abstract states) single search depth value pre-computed pair abstract states. run-time single search depth value shared children abstract state pair (Figure 3).
search depth values stored table refer pattern database PDB
short. past, pattern databases used store approximate heuristic values (Culberson
& Schaeffer, 1998) important board features (Schaeffer, 2000). work appears first
use pattern databases store search depth values.
Computing search depths abstract states speeds pre-computation reduces memory
overhead (both important considerations commercial computer games). paper use
previously published clique abstraction (Sturtevant & Buro, 2005). preserves overall topology
map requires storing abstraction links explicitly.1 clique abstraction works
finding fully connected subgraphs (i.e., cliques) original graph abstracting states
within clique single abstract state. Two abstract states connected abstract
action single original action leads state first clique
state single clique (Figure 4). costs abstract actions computed Euclidean
distances average coordinates states cliques.
typical grid world computer-game maps, single application clique abstraction reduces
number states factor two four. average, abstraction level five (i.e.,
five applications abstraction procedure), region contains one hundred original
1. alternative use regular rectangular tiles (e.g., Botea et al., 2004).

426

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Figure 3: single optimal lookahead depth value shared among children abstract state.
memory-efficient approximation true per-ground-state values Figure 2.

Level 0 (original graph)

Level 1

Level 2

Figure 4: Two iterations clique abstraction procedure produce two abstract levels
ground-level search graph.
(or ground-level) states. Thus, single search depth value shared among ten thousand
state pairs. result, five-level clique abstraction yields four orders magnitude reduction
memory two orders magnitude reduction pre-computation time (as analyzed later).
downside, higher levels abstraction effectively make search depth selection less
less dynamic depth value shared among progressively states. abstraction
level pattern database control parameter trades pre-computation time pattern
database size on-line performance algorithm uses database.
Two alternatives storing optimal search depth store optimal action optimal
heuristic value. combination abstraction real-time search precludes them. Indeed,
sharing optimal action computed single ground-level representative abstract region
among states region may cause agent run wall (Figure 5, left). Likewise,
sharing single heuristic value among states region leaves agent without sense
427

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON


G

5.84

5.84

5.84

5.84

5.84

5.84

5.84

5.84

5.84

5.84



5.84

5.84

5.84

G

Figure 5: Goal shown G, agent A. Abstract states four tiles separated dashed lines.
Diamonds indicate representative states tile. Left: Optimal actions shown
representative abstract tile; applying optimal action agents tile
agents current location leads wall. Right: Optimal heuristic value (h ) lower
left tiles representative state (5.84) shared among states tile. result,
agent preference among three legal actions shown.
direction states vicinity would look equally close goal (Figure 5, right).
contrast sharing heuristic value among states within abstract state (known pattern)
using optimal non-real-time search algorithms A* IDA* (Culberson & Schaeffer,
1996). case real-time search, agents using either alternative guaranteed reach
goal, let alone minimize travel. contrary, sharing search depth among number
ground-level states safe LRTA* complete search depth.
compute single depth table per map off-line (Figure 6). line 1 state space abstracted ` times. Lines 2 7 iterate pairs abstract states. pair (s0 , s0goal ),
representative ground-level states sgoal (i.e., ground-level states closest centroids regions) picked optimal search depth value calculated them. this, Dijkstras
algorithm (Dijkstra, 1959) run ground-level search space (V, E) compute true
minimal distances state sgoal . distances known successors s,
optimal action (s, sgoal ) computed greedily. optimal search depth (s, sgoal )
computed previously described capped c (line 5). resulting value stored pair
abstract states (s0 , s0goal ) line 6. Figures 2 3 show optimal search depth values single
goal state grid world game map without abstraction respectively.
run-time, LRTA* agent going state state sgoal takes search depth
depth table value pair (s0 , s0goal ), s0 s0goal images sgoal `-level
abstraction. additional run-time complexity minimal s0 , s0goal , d(s0 , s0goal ) computed
small constant-time overhead action.
building pattern database Dijkstras algorithm run V` times2 graph (V, E)
time complexity O(V` (V log V + E)) sparse graphs (i.e., E = O(V )). optimal
search depth computed V`2 times. time, c LRTA* invocations total
2. brevity, use V E mean sets vertices/edges sizes (i.e., |V | |E|).

428

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

BuildPatternDatabase(V, E, c, `)
1 apply abstraction procedure ` times (V, E) compute abstract space S` = (V` , E` )
2 pair states (s0 , s0goal ) V` V`
3
select V representative s0 V`
4
select sgoal V representative s0goal V`
5
compute c-capped optimal search depth value state respect goal sgoal
6
store capped pair (s0 , s0goal )
7 end
Figure 6: Pattern database construction.
complexity O(bc ) b maximum degree V . Thus, overall time complexity
O(V` (V log V + E + V` bc )). space complexity lower store optimal search depth
values pairs abstract states: O(V`2 ). Table 1 lists bounds sparse graphs.
Table 1: Reduction complexity due state abstraction.

time
space

abstraction
O(V 2 log V )
O(V 2 )

`-level abstraction
O(V` V log V )
O(V`2 )

reduction
V /V`
(V /V` )2

5.3 Discussion Two Approaches
Selecting search depth pattern database two advantages. First, search depth values
stored pair abstract states optimal non-abstract representatives, unless either
value capped states local search space visited heuristic values modified. (conditional) optimality contrast classifier approach
optimal actions ever computed deeper searches merely assumed lead
better action. assumption always hold phenomenon known lookahead pathology, found abstract graphs (Bulitko et al., 2003) well grid-based pathfinding (Lustrek &
Bulitko, 2006). second advantage need features current state, recent
history pre-search. search depth retrieved depth table simply basis
current states identifier, coordinates.
decision-tree classifier approach two advantages depth table approach. First,
classifier training need happen search space agent operates in.
long training maps used collect features build decision tree representative
run-time maps, approach run never-before-seen maps (e.g., user-created maps
computer game). Second, much smaller memory overhead method
classifier specified procedurally pattern database needs loaded memory.
Note approaches assume structure heuristic search problem
hand. Namely, pattern database approach shares single search depth value across region
states. works effectively states region indeed lookahead
depth best them. abstraction mechanism forms regions basis search
graph structure, regard search depth. empirical study show, clique abstraction
429

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

seems right choice pathfinding. However, choice best abstraction technique
general heuristic search problem open question.
Similarly, decision-tree approach assumes states share similar feature values
also share best search depth value. appears hold large extent pathfinding domain
feature selection arbitrary heuristic search problems open question well.

6. Dynamic Goal Selection
two methods described allow agent select individual search depth state.
However, original LRTA*, heuristic still computed respect global goal
sgoal . illustrate: Figure 7, map partitioned eight abstract states (in case, 4 4
square tiles) whose representative states shown diamonds (18). optimal path
agent (A) goal (G) shown well. straight-line distance heuristic ignore
wall agent goal lead agent south-western direction. LRTA*
search depth 11 higher needed produce optimal action (such ). Thus,
cap value 11, agent left suboptimal action spend long time
horizontal wall raising heuristic values. Spending large amounts time corners
heuristic depressions primary weakness real-time heuristic search agents and,
example, remedied dynamic search depth selection due cap.

1

2

3

4



5

G

7

6

8

Figure 7: Goal shown G, agent A. Abstract states eight tiles separated dashed
lines. Diamonds indicate ground-level representative tile. optimal path
shown. Entry points path abstract states marked circles.

5a compute sintermediate goal goal (s, sgoal )
5b compute capped optimal search depth value respect sintermediate goal
6 store (d , sintermediate goal ) pair (s0 , s0goal )
Figure 8: Switching sgoal sintermediate goal ; replaces lines 56 Figure 6.
430

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Figure 9: three maps used experiments.
address issue, switch intermediate goals pattern-database construction well
on-line LRTA* operation. example Figure 7 compute heuristic around
respect intermediate goal marked double-border circle map. Consequently,
eleven times shallower search depth needed optimal action towards next abstract state
(right-most upper tile). approach replaces lines 5 - 6 Figure 6 Figure 8. line
5a, compute intermediate goal sintermediate goal ground-level state optimal path
sgoal enters next abstract state. entry points marked circles Figure 7.
compared entry states centroids abstract states intermediate goals (Bulitko et al., 2007)
found former superior terms algorithms performance. Note optimal path
easily available off-line run Dijkstras algorithm (Section 5.2).
intermediate goal computed, line 5b computes capped optimal search depth
respect intermediate goal sintermediate goal . depth computation done described
Section 5.2. search depth intermediate goal added pattern database
line 6. run-time, agent executes LRTA* stored search depth computes
heuristic h respect stored goal (i.e., sgoal set sintermediate goal line 3 Figure 1).
words, search depth agents goal selected dynamically, per action.
approach works heuristic functions used practice tend become accurate states closer goal state. Therefore, switching distant global goal nearby
intermediate goal makes heuristics around current state accurate leads shallower search depth necessary achieve optimal action. result, algorithm
run quickly shallower search per move also search depth cap reached less
frequently therefore search depth values actually result optimal moves.

7. Empirical Evaluation
section presents results empirical evaluation algorithms dynamic control search
depth goals classic state-of-the-art published algorithms. algorithms avoid reexpanding states planning move via transposition table. report sub-optimality
solution found average amount computation per action, expressed number
states expanded. believe algorithms implemented way single
expanded state takes amount time. case testbed code
optimized other. reason avoid clutter, report CPU times
Section 7.7. used fixed tie-breaking scheme real-time algorithms.
431

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

use grid world maps computer game testbed. Game maps provide realistic
challenging environment real-time search seen number recent publications (e.g., Nash, Daniel, & Felner, 2007; Hernandez & Meseguer, 2007). original maps
sized 161161 193193 cells (Figure 9). line Sturtevant Buro (2005) Sturtevant
Jansen (2007), also experimented maps upscaled 512 512 closer size
maps used modern computer games. Note three maps depicted figure
outdoor-type maps, also ran preliminary experiments indoor-type game maps (e.g., one
shown Figure 2). trends similar decided focus larger outdoor maps.
100 search problems defined three original size maps. start
goal locations chosen randomly, although constrained optimal solution paths cost
90 100 order generate difficult instances. upscaled maps 300
problems upscaled well. data point plots average 300 problems (3
maps 100 runs each). different legend entry used algorithm, multiple points
legend entry represent alternative parameter instantiation algorithm.
heuristic function used octile distance natural extension Manhattan distance maps
diagonal actions. enforce real-time constraint disqualified parameter settings
caused algorithm expand 1000 states move problem. points
excluded empirical evaluation. Maps known priori off-line order build
decision-tree classifiers pattern databases.
use following notation identify algorithms variants: AlgorithmName
(X, Y) X defined follows. X denotes search depth control: F fixed search
depth, DT search depth selected dynamically decision tree, ORACLE search depth
selected decision-tree oracle (see next section details) PDB search depth
selected dynamically pattern databases. denotes goal state selection: G heuristic
computed respect single global goal, PDB heuristic computed respect
intermediate goal pattern databases. instance, classic LRTA* LRTA* (F, G).
empirical evaluation organized eight parts follows. Section 7.1 describes six
algorithms compute heuristic respect global goal discusses performance.
Section 7.2 describes five algorithms use intermediate goals. Section 7.3 compares global
intermediate goals. Section 7.4 studies effects path-refinement without dynamic
control. Secton 7.5 pits new algorithms state-of-the-art real-time non-real-time
algorithms. provide algorithm selection guide different time limits planning per
move Section 7.6. Finally, Section 7.7 considers issue amortizing off-line pattern-database
build time on-line pathfinding.
7.1 Algorithms Global Goals
subsection describe following algorithms compute heuristic respect
single global goal (i.e., use intermediate goals):
1. LRTA* (F, G) Learning Real-Time A* (Korf, 1990). action conducts breadthfirst search fixed depth around agents current state. first move towards
best depth state taken heuristic agents previous state updated using
Korfs mini-min rule.3 used {4, 5, . . . , 20}.
3. Instead using LRTA* could used RTA*. experiments showed grid pathfinding
significant performance difference two search depth beyond one. Indeed deeper searches

432

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

2. LRTA* (DT, G) LRTA* search depth dynamically controlled decision
tree described Section 5.1. used following parameters: dmax {5, 10, 15, 20}
history trace length n = 60. building decision-tree classifier WEKA (Witten & Frank, 2005) pruning factor set 0.05 minimum number data items
per leaf 100 original size maps 25 upscaled ones. opposed learning
tailor-made classifier game map, single common decision-tree classifier built
based data collected maps (using 10-fold cross-validation). done
demonstrate ability classifier generalize across maps.
3. LRTA* (ORACLE, G) LRTA* search depth dynamically controlled
oracle. oracle always selects best search depth produce move given
LRTA* (F, G) fixed lookahead depth dmax (Bulitko et al., 2007). words,
oracle acts perfect decision-tree thus sets upper bound LRTA* (DT, G)
performance. oracle run dmax {5, 10, 15, 20}, original size
maps proved prohibitively expensive compute upscaled maps. Note
practical real-time algorithm used reference point experiments.
4. LRTA* (PDB, G) LRTA* search depth dynamically controlled
pattern database described Section 5.2. original size maps, used abstraction
level ` {0, 1, . . . , 5} depth cap c {10, 20, 30, 40, 50, 1000}. upscaled maps,
used abstraction level ` {3, 4, . . . , 7} depth cap c {20, 30, 40, 50, 80, 3000}.
Considering size maps, cap value 1000 3000 means virtually capless search.
5. K LRTA* (F, G) variant LRTA* proposed Koenig (2004). Unlike original
LRTA*, uses A*-shaped lookahead search space updates heuristic values states
within using Dijkstras algorithm.4 number states K LRTA* expands per move
took values: {10, 20, 30, 40, 100, 250, 500, 1000}.
6. P LRTA* (F, G) Prioritized LRTA* variant LRTA* proposed Rayner, Davison,
Bulitko, Anderson, Lu (2007). uses lookahead depth 1 moves. However,
every state whose heuristic value updated, neighbors put onto update queue,
sorted magnitude update. Thus, algorithm propagates heuristic function
updates space fashion Prioritized Sweeping (Moore & Atkeson, 1993).
control parameter (queue size) set {10, 20, 30, 40, 100, 250, 500, 1000} original
size maps {10, 20, 30, 40, 100, 250} upscaled maps.
Figure 10 evaluate performance new dynamic depth selection algorithms
original size maps. see decision-tree pattern-database approach improve
significantly upon LRTA* algorithm, expanding two three times fewer states generating
solutions comparable quality. Furthermore, perform par current state-of-the-art realtime search algorithms without abstraction, seen compared K LRTA* (F, G).
solutions generated acceptable quality domain (e.g., 50% suboptimal), even
expanding 100 states per action. Also interest decision-tree approach performs
likelihood multiple actions equally low g + h cost high, reducing distinction RTA*
LRTA*. using LRTA* agents learn repeated trials.
4. also experimented A*-shaped lookahead new algorithms found inferior breadth-first lookahead deeper searches.

433

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Original size maps

Realtime cutoff: 1000

4
LRTA* (F, G)
LRTA* (ORACLE, G)
LRTA* (DT, G)
LRTA* (PDB, G)
P LRTA* (F, G)
K LRTA* (F, G)

Suboptimality (times)

3.5
3
2.5
2
1.5

1

0

100

200
300
400
Mean number states expanded per move

500

600

Figure 10: Global-goal algorithms original size maps.
quite close theoretical best case, seen compared LRTA* (ORACLE, G).
shows features use, although seemingly simplistic, good job predicting
appropriate search depth.
ran similar sets experiments upscaled maps. However, none global goal
algorithms generated solutions acceptable quality given real-time cut-off (the solutions
300 1700% suboptimal). experimental results upscaled maps provided
Appendix B. shows inherent limitations global goal approaches; large search
spaces cannot compete equal footing abstraction-based methods. brings us
intermediate goal selection methods.
7.2 Algorithms Intermediate Goals
section describe algorithms use intermediate goals search. best
knowledge, one previously published real-time heuristic search algorithm
so. Thus, compare new algorithms proposed paper. Given intermediate
goals increase performance algorithms significantly, present results
challenging upscaled maps. full roster algorithms used section follows:
1. PR LRTA* (F, G) Path Refinement Learning Real-Time Search (Bulitko et al., 2007).
algorithm two components: runs LRTA* fixed search depth global
goal abstract space (abstraction level ` clique abstraction hierarchy) refines
first move using corridor-constrained A* running original ground-level map.5
Constraining A* small set states, collectively called corridor Sturtevant Buro
5. algorithm actually called PR LRTS (Bulitko et al., 2007). Based findings Lustrek Bulitko (2006),
modified refine single abstract action order reduce susceptibility lookahead pathologies.
modification equivalent substituting LRTS component LRTA*. Hence, rest paper,
call PR LRTA*.

434

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

(2005) tunnel Furcy (2006), speeds makes real-time corridor size
independent map size (Bulitko, Sturtevant, & Kazakevich, 2005). heuristic
computed abstract space respect fixed global goal, A* component
computes path current state intermediate goal. qualifies PR LRTA*
enter section empirical evaluation. control parameters follows: abstraction
level ` {3, 4, . . . , 7}, LRTA* lookahead depth {1, 3, 5, 10, 15} LRTA* heuristic
weight {0.2, 0.4, 0.6, 1.0} ( imposed g line 5 Figure 1).
2. LRTA* (F, PDB) LRTA* fixed search depth uses pattern database select
intermediate goals. control parameters follows: abstraction level ` {3, 4, . . . , 7}
search depth {1, 2, . . . , 9, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30}.
3. LRTA* (PDB, PDB) LRTA* generalized dynamic search depth intermediate goal selection pattern databases presented Sections 5.2 6. control parameters follows: abstraction level ` {3, 4, . . . , 7} lookahead cap
c {20, 30, 40, 50, 80, 3000}.
4. PR LRTA* (PDB, G) PR LRTA* whose LRTA* component equipped dynamic search depth uses global (abstract) goal respect computes abstract heuristic. pattern database search depth constructed
abstraction level ` LRTA* component runs on, making component optimal lookahead cap allows. used abstraction level `
{3, 4, . . . , 7} lookahead cap c {5, 10, 15, 20, 1000}.
also ran version PR LRTA* (PDB, G) pattern database constructed abstraction level `2 level ` LRTA* operates (Table 2). used (`, `2 )
{(1, 3), (2, 4), (3, 5), (4, 6), (5, 7), (1, 4), (2, 6), (3, 7), (4, 8), (5, 9)}.
5. PR LRTA* (PDB, PDB) two-database version PR LRTA* (PDB, G)
except uses second database goal selection well depth selection. used
(`, `2 ) {(1, 3), (2, 4), (3, 5), (4, 6), (5, 7), (1, 4), (2, 6), (3, 7), (4, 8), (5, 9)} (Table 2).
Table 2: PR LRTA* (PDB, G PDB) uses LRTA* abstraction level ` define corridor within
refines path using A*. Dynamic depth (and goal) selection performed either
abstraction level ` `2 > `.
Abstraction level
`2
`
0

Single abstraction PR LRTA*(PDB,G)
abstract-level LRTA*
dynamic depth selection
corridor-constrained ground-level A*

Dual abstraction PR LRTA*(PDB,{G,PDB})
dynamic depth (and goal) selection
abstract-level LRTA*
corridor-constrained ground-level A*

pattern database algorithms presented stores depth value intermediate
ground-level goal pair abstract states. present performance results algorithms
intermediate goals Sections 7.37.6 analyze complexity pattern database
computation effects performance Section 7.7.
435

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Upscaled maps

Realtime cutoff: 10000

20
LRTA* (F, G)
LRTA* (F, PDB)

18

Suboptimality (times)

16
14
12
10
8
6
4
2
0

200

400

600
800
1000
1200
Mean number states expanded per move

1400

1600

Figure 11: Effects intermediate goals: LRTA* (F, G) versus LRTA* (F, PDB).

7.3 Global versus Intermediate Goals
Sections 7.1 7.2 presented algorithms global intermediate goals respectively.
section compare algorithms across two groups. include LRTA* (PDB, G), increased
real-time cut-off 1000 10000 graphs section. start baseline LRTA* fixed lookahead. effects adding intermediate goal selection dramatic:
LRTA* intermediate goals (F, PDB) finds five times better solutions three orders
magnitude faster LRTA* global goals (F, G) (see Figure 11). believe
result octile distance heuristic substantially accurate around goal. Consequently,
LRTA* (F, PDB) benefiting much better heuristic function.
second experiment, equip versions dynamic search depth control compare LRTA* (PDB, G) LRTA* (PDB, PDB) Figure 12. performance gap less
dramatic: planning speed-up still around three orders magnitude, suboptimality
advantage went five two times. Again, note increase real-time
cut-off order magnitude get points plot.
Finally, evaluate beneficial: dynamic depth control dynamic goal control
comparing baseline LRTA* (F, G) LRTA* (PDB, G) LRTA* (F, PDB) Figure 13.
clear dynamic goal selection much stronger addition baseline LRTA* dynamic
search depth selection. Dynamic depth selection sometimes actually performs worse fixed
depth, evidenced data points LRTA* (F, G) line. happens primarily
high abstraction levels small caps. optimal lookahead depth computed high
abstraction level, depth value shared among many ground-level states. selected
depth value beneficial near entry point abstract state, abstract state
large, depth likely become inappropriate ground-level states away.
example, optimal depth entry point 1, worse moderate fixed depth
436

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Upscaled maps

Realtime cutoff: 10000
LRTA* (PDB, G)
LRTA* (PDB, PDB)

14

Suboptimality (times)

12
10
8
6
4
2
0

100

200

300
400
500
600
Mean number states expanded per move

700

800

Figure 12: Effects intermediate goals: LRTA* (PDB, G) versus LRTA* (PDB, PDB).
Upscaled maps

Realtime cutoff: 10000

20
LRTA* (F, G)
LRTA* (F, PDB)
LRTA* (PDB, G)

18

Suboptimality (times)

16
14
12
10
8
6
4
2
0

200

400

600
800
1000
1200
Mean number states expanded per move

1400

1600

Figure 13: Dynamic search depth control versus dynamic goal control.

ground-level states far entry point. Small caps compound problem sometimes
preventing selection optimal depth even entry point.
shown plot, running (i.e., LRTA* (PDB, PDB)) leads marginal improvements. best parameterizations LRTA* (F, PDB) already expands
single state per move virtually times. Consequently, benefit adding dynamic
depth control slight improvement suboptimality next section.
437

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Upscaled maps

Realtime cutoff: 1000

1.5
LRTA* (F, PDB)
LRTA* (PDB, PDB)
PR LRTA* (F, G)
PR LRTA* (F, PDB)
PR LRTA* (PDB, PDB)
PR LRTA* (PDB, G)

Suboptimality (times)

1.4

1.3

1.2

1.1

1

0

5

10
15
Mean number states expanded per move

20

25

Figure 14: Effects path refinement: LRTA* versus PR LRTA*.
7.4 Effects Path Refinement
Path-refinement algorithms (denoted PR prefix) run learning real-time search (LRTA*)
abstract space refine path running A* ground level. Non-PR algorithm run
A* real-time search happens ground-level space. examine effects pathrefinement comparing LRTA* PR LRTA*. Note even statically controlled baseline
PR LRTA* (F, G) uses intermediate goals refining abstract actions. match using
dynamic intermediate goal selection LRTA*. Thus, compare four versions PR LRTA*: (F,
G), (PDB, G), (F, PDB) (PDB, PDB) two versions LRTA*: (F, PDB) (PDB, PDB).
results found Figure 14. sake clarity, show high performance area
capping number states expanded per move 25 suboptimality 1.5.
best parameterizations LRTA* find near-optimal solutions expanding one state
per move virtually times. astonishing performance one state expansion per
move corresponds search depth one fastest possible operation algorithm
framework. Thus, LRTA* (F, PDB) LRTA* (PDB, PDB) virtually unbeatable terms
planning time. hand, PR LRTA* incurs planning overhead due path-refinement
component (i.e., running corridor-constrained A*). result, PR LRTA* also finds nearlyoptimal solutions incurs least five times higher planning cost per move. Dynamic control
PR LRTA* results moderate performance gains.
7.5 Comparison Existing State Art
Traditionally, computer games used A* pathfinding needs (Stout, 2000). map size
number simultaneously planning agents increase, game developers find even highly optimized
implementations A* insufficient. result, variants A* use state abstraction
used (Sturtevant, 2007). Another way speeding A* introduce weight computing travel
cost state. done f = g + h, 0 values 1 make agent
438

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

greedy (more weight put h) usually leads fewer states expanded price
suboptimal solutions. section, compare new algorithms weighted A* (Korf, 1993)
state-of-the-art Partial Refinement A* (PRA*) (Sturtevant & Buro, 2005). Note neither
algorithm real-time and, thus, planning times per move map-size specific. is,
larger maps, A*s PRA*s planning times per move increase algorithms compute
complete (abstract) path start goal states take first move. instance,
maps used PRA* expands 3454 states expensive move. Weighted A*
= 15 expands 40734 states classic A* expands 88138 states worst moves. Thus,
include two algorithms comparison effectively remove real-time cut-off.
results found Table 3. Dynamically controlled LRTA* one two orders magnitude faster average planning time per move. produces shorter paths existing stateof-the-art real-time algorithm (PR LRTA*) fastest weighted A* tried. original A*
provably optimal solution quality PRA* nearly optimal. argue hundreds
units simultaneously planning paths computer game, LRTA* (PDB, PDB)s low planning time per move real-time guarantees worth 6.1% path-length suboptimality (e.g., 106
screen pixels versus optimal 100 screen pixels).

Table 3: Comparison high-performance algorithms, best values bold. Standard errors
reported .
Algorithm, parameters
PR LRTA* (F, G), ` = 4, = 5, = 1.0
LRTA* (PDB, PDB), ` = 3, c = 3000
A*
weighted A*, f = 15 g + h
PRA*

Planning per move
15.06 0.0722
1.032 0.0054
119.8 3.5203
24.86 1.4404
10.83 0.0829

Suboptimality (times)
1.161 0.0177
1.061 0.0027
1 0.00
1.146 0.0072
1.001 0.0003

7.6 Best Solution Quality Time Limit
section identify algorithms deliver best solution quality time limit.
Specifically, impose hard limit planning time per move, expressed number states
expanded. algorithm exceeds limit even single move made 300
problems upscaled maps excluded consideration. Among remaining algorithms,
select one highest solution quality (i.e., lowest suboptimality). results found
Table 4. algorithms expand least one state per move move, leaving first row
empty. LRTA* (F, PDB) = 1, ` = 3 best choice time limit one
eight states expanded per move. limit rises, expensive optimal algorithms
become affordable. Note best choices dynamically controlled algorithms
time limit rises 3454 states. point, non-real-time PRA* takes ending domain
real-time algorithms. cross-over point specific problem map sizes. larger
problems/maps, PRA*s maximum planning time per move necessarily increase, making
best choice progressively higher planning-time-per-move limits.
439

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Table 4: Best solution quality strict limit planning time per move. Planning time
states expanded per move. sake readability, suboptimality shown
percentage (e.g., 1.102267 = 10.2267%).
Planning time limit
0
[1, 8]
[9, 24]
[25, 48]
[49, 120]
[121, 728]
[729, 753]
[754, 1223]
[1224, 1934]
[1935, 3453]
[3454, 88137]
[88138, )

Algorithm, parameters
LRTA* (F, PDB) = 1, ` = 3
LRTA* (F, PDB) = 2, ` = 3
LRTA* (F, PDB) = 3, ` = 3
LRTA* (F, PDB) = 4, ` = 3
LRTA* (F, PDB) = 6, ` = 3
LRTA* (F, PDB) = 14, ` = 4
PR LRTA* (PDB, G) c = 15, = 1.0, ` = 3
PR LRTA* (PDB, G) c = 20, = 1.0, ` = 3
PR LRTA* (PDB, G) c = 1000, = 1.0, ` = 3
PRA*
A*

Suboptimality (%)
10.2267%
8.6692%
5.6793%
5.6765%
5.6688%
5.6258%
4.2074%
3.6907%
3.5358%
0.1302%
0%

7.7 Amortization Pattern-database Build Time
pattern-database approach invests time computing PDB map. section
study amortization off-line investment multiple problem instances. PDB build
times 3 GHz Pentium CPU listed Table 5 single map. Consider algorithm LRTA*
(PDB, PDB) cap c = 20 pattern databases built level ` = 3. average,
solution suboptimality 1.058 expanding 1.536 states per move 31.065 microseconds.
closest statically controlled competitor PR LRTA* (F, G) ` = 4, = 15, = 0.6
suboptimality 1.059 expanding average 28.63 states per move 131.128 microseconds. Thus, LRTA* (PDB, PDB) 100 microseconds faster move. Consequently,
4.7 108 moves necessary recoup off-line PDB build time 13 hours. move
taking 31 microseconds, LRTA* lower total run-time first four hours
pathfinding. computed recoup times parameterizations LRTA* (PDB, PDB)
whose closest statically controlled competitor slower per move. results found Table 6
demonstrate LRTA* (PDB, PDB) recoups PDB build time first 1.4 27 hours
pathfinding time. Note numbers highly implementation domain-specific. particular, code building PDBs leaves substantial room optimization. completeness
sake, report detailed times Appendix C.

8. Discussion Empirical Results
section recap trends observed previous sections. Dynamic selection
lookahead either decision-tree PDB approach helps reduce planning time per move
well solution suboptimality (Section 7.1). result, LRTA* becomes competitive
modern algorithms Koenigs LRTA*. However, real-time search algorithms global goals
scale well large maps.
440

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Table 5: Pattern database average 512512 map, computed intermediate goals. Database
size listed number abstract state pairs. Suboptimality planning per move
listed representative algorithm: LRTA* (PDB, PDB) cap c = 20.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 2 years
est. 1.5 months
est. 4 days
13 hours
3 hours
1 hour
24 minutes
10 minutes

Planning per move
1.5
3.2
41.3
104.4
169.3

Suboptimality (times)
1.058
1.059
1.535
2.315
2.284

Table 6: Amortization PDB build times. dynamically controlled LRTA*, list
statically controlled PR LRTA* closest terms solution suboptimality.
LRTA* (PDB, PDB)
c = 20, ` = 3
c = 20, ` = 4
c = 30, ` = 3
c = 40, ` = 3
c = 40, ` = 4
c = 50, ` = 3
c = 50, ` = 4
c = 80, ` = 3

PR LRTA* (F, G)
` = 4, = 15, = 0.6
` = 4, = 15, = 0.6
` = 4, = 15, = 0.6
` = 4, = 15, = 0.6
` = 4, = 15, = 0.4
` = 4, = 15, = 0.6
` = 4, = 15, = 0.6
` = 4, = 15, = 0.6

Amortization moves
4.7 108
1.2 108
5.1 108
5.3 108
3.4 108
6.2 108
6.7 108
1.1 109

Amortization run-time
4 hours
1.4 hours
5.1 hours
6 hours
9.3 hours
9 hours
21.1 hours
27 hours

Adding intermediate goals brings even classic LRTA* par previous state-of-theart real-time search algorithm PR LRTA* much stronger addition dynamic lookahead
depth selection (Section 7.3). Using dynamic lookahead depth subgoals brings
improvements. Section 7.5 details, LRTA* equipped dynamic lookahead depth
subgoal selection expands barely state per move less 7% solution suboptimality.
better previous state-of-the-art algorithms PR LRTA*, PRA* A*
solution quality planning time per move, believe trade-offs makes appealing
practice. aid practitioners further, provide algorithm selection guide Section 7.6
makes clear LRTA* dynamic subgoal selection best algorithms time
per move severely limited. speed advantage deliver state-of-the-art PR LRTA*
algorithm allows recoup PDB build time several hours pathfinding.

9. Current Limitations Future Work
project opens several interesting avenues future research. particular, would worthwhile investigate performance algorithms paper dynamic environments (e.g.,
bridge gets destroyed real-time strategy game goal moves away agent).
441

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Another area future research application proposed algorithms general planning.
Heuristic search successful approach planning planners ASP (Bonet,
Loerincs, & Geffner, 1997), HSP-family (Bonet & Geffner, 2001), FF (Hoffmann, 2000),
SHERPA (Koenig, Furcy, & Bauer, 2002) LDFS (Bonet & Geffner, 2006). line recent
planning work (Likhachev & Koenig, 2005) Bonet Geffner (2006), evaluate
proposed algorithms general STRIPS-style planning problem. Nevertheless, believe
new real-time heuristic search algorithms may also offer benefits wider range planning
problems. Indeed, core heuristic search algorithm extended paper (LRTA*) previously applied general planning (Bonet et al., 1997). extensions introduced may
beneficial effect similar way B-LRTA* improved performance ASP planner.
Subgoal selection long studied planning central part intermediate-goal
depth-table approach. Decision trees search depth selection induced sample trajectories space appear scalable general planning problems. part
approach requires solving numerous ground-level problems optimally pre-computation
optimal search depth PDB approach. conjecture approach still effective if,
instead computing optimal search depth based optimal action , one solve
relaxed planning problem use resulting action place . idea deriving heuristic
guidance solving relaxed problems quite common planning heuristic search
community.

10. Conclusions
Real-time pathfinding non-trivial problem algorithms must trade solution quality
amount planning per move. two measures antagonistic thus interested
Pareto optimal algorithms outperformed measures algorithms.
classic LRTA* provides smooth trade-off curve, parameterized lookahead depth. Since
introduction 1990, variety extensions proposed. recent extension,
PR LRTS (Bulitko et al., 2005) first application automatic state abstraction real-time
search. large-scale empirical study pathfinding game maps, PR LRTS outperformed
many algorithms respect several antagonistic measures (Bulitko et al., 2007).
paper also employ automatic state abstraction instead using pathrefinement, pre-compute pattern databases use select amount planning
intermediate goals dynamically, per move. Several mechanisms dynamic control proposed used virtually existing real-time search algorithm. demonstration,
equip classic LRTA* state-of-the-art PR LRTS dynamic control.
resulting improvements substantial. instance, LRTA* equipped PDB-based control
lookahead intermediate goal selection significantly outperforms existing state art (PR
LRTS) simultaneously planning per move solution quality. Furthermore, average expands little one state per move minimum amount planning
LRTA*-based algorithm.
new algorithms compare favorably A* state-of-the-art extension, PRA*,
presently popular industrial choices pathfinding computer games (Stout, 2000; Sturtevant,
2007). First, per-move planning time algorithms provably unaffected increase
map size. Second, two orders magnitude faster A* one order magnitude
faster PRA* planning time per move. improvements come price 7%
442

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

suboptimality, likely unnoticed computer game player scenarios. Thus appears
new algorithms redefine state art real-time search arena also
well-suited industrial applications.

Acknowledgments
Sverrir Sigmundarson School Computer Science, Reykjavik University
project. appreciate consultation Robert C. Holte detailed feedback anonymous
reviewers. research supported grants National Science Engineering Research Council Canada (NSERC); Albertas Informatics Circle Research Excellence (iCORE);
Slovenian Ministry Higher Education, Science Technology; Icelandic Centre Research
(RANNIS); Marie Curie Fellowship European Community programme Structuring
ERA contract number MIRG-CT-2005-017284. Special thanks Nathan Sturtevant
development support HOG.

Appendix A. Decision-Tree Features
devised two different categories classifier features: first consists features based
agents recent history, whereas second contains features sampled shallow pre-search
agents current state. Thus, collectively, features two categories make predictions
based agents recent history well current situation.
first category four features listed Table 7. features computed
execution step. aggregated recent states agent in,
done incremental fashion improved performance. parameter n set user
controls long history aggregate over. use notation s1 refer state agent
one step ago, s2 state two steps ago, etc.; agent thus aggregates states s1 ,
..., sn . Feature f1 provides rough estimate location agent relative goal.
distance goal state affect required lookahead depth, example heuristics
closer goal usually accurate. feature makes possible classifier make
decisions based deemed necessary. Features f2 (known mobility) f3 provide
measure much progress agent made towards reaching goal past steps.
Frequent state revisits may indicate heuristic depression deeper search usually beneficial
situations (Ishida, 1992). Feature f4 measure inaccuracies inconsistencies
heuristic around agent; again, many heuristic updates may warrant deeper search.
features second category listed Table 8. also computed execution step. planning phase starts, shallow lookahead pre-search performed gather
information nearby part search space. types features category
coarsely divided features (i) compute fraction states pre-search lookahead
frontier satisfy property, (ii) compare action chosen pre-search previous
actions (either previous state taken last time current state visited), (iii)
check heuristic estimates immediate successors current state. Feature f5 rough
measure density obstacles agents vicinity: obstacles are,
beneficial deeper search might be. Feature f6 indicator difficulty traversing
local area. proportion high, many states updated, possibly suggesting heuristic
depression. feature f7 , pre-search selects action might indicate
443

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Table 7: History based classifier features.
Feature
f1
f2
f3
f4

Description
initial heuristic estimate distance current state goal:
hoctile (s, sglobal goal ).
heuristic estimate distance current state state
agent n steps ago: h(s, sn ).
number distinct states agent visited last n steps:
|{s1 , s2 , ..., sn }|.
P
total volume heuristic updates last n steps: ni=1 hafter update (si )
hbefore update (si ) (line 6 Figure 1).
Table 8: Pre-search based classifier features.

Feature
f5
f6
f7

f8
f9
f10
f11

Description
ratio actual number states pre-search frontier expected
number states obstacles map.
fraction frontier states updated heuristic value.
boolean feature telling whether action chosen pre-search
action chosen planning phase last time state visited.
first time state visited feature false.
boolean feature telling whether direction suggested pre-search
direction agent took previous step.
ratio current states heuristic best successor state suggested
pre-search: h(s, sgoal )/h(s, sgoal ).
boolean feature telling whether best action proposed pre-search phase
would lead successor state updated heuristic value.
boolean feature telling whether heuristic value current state larger
heuristic value best immediate successor found pre-search.

heuristic values part search space already mutually consistent thus
shallow lookahead needed; applies feature f8 . Features f9 f11 compare current
state successor state suggested pre-search.

Appendix B. Experiments Upscaled Maps Using Global Goals
Empirical results running global-goal algorithms upscaled maps shown Figure 15.
LRTA* (DT, G) shows significant improvement LRTA* (F, G), making comparable
quality existing state-of-the-art algorithms: par P LRTA* (F, G) slightly better
K LRTA* (F, G) allowed expand 200 states per move. also worth noting
LRTA* (PDB, G) longer competitive algorithms and, fact, make
real-time cut-off 1000 states parameters combinations (and thus shown
plot). reason lies fact problems simply difficult LRTA* find
optimal move small lookahead depth. instance, abstraction level ` = 3 cap
444

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

c = 80, LRTA* (PDB, G) suboptimality 1.36. Unfortunately, lookahead depth hits cap
11% visited states. result, algorithm expands average 1214 states per move
disqualifies cut-off 1000.
Upscaled maps

Realtime cutoff: 1000

20
LRTA* (F, G)
LRTA* (DT, G)
P LRTA* (F, G)
K LRTA* (F, G)

18

Suboptimality (times)

16
14
12
10
8
6
4
2
0

100

200

300
400
500
600
Mean number states expanded per move

700

800

Figure 15: Performance global-goal algorithms upscaled maps.
Looking collectively small upscaled map results, LRTA* (DT, G) demonstrates
excellent performance among global goal algorithms robust respect map
upscaling one efficient ones (the comparable algorithm K LRTA* (F, G)).
However, within provided 1000 states cut-off limit, none real-time global-goal algorithms
returned solutions would considered acceptable quality pathfinding. Indeed, even
best solutions found approximately four times worse optimal.

Appendix C. Pattern Database Build Times
order operate LRTA* PR LRTA* use lookahead depth intermediate goals controlled dynamically, build pattern databases. pattern database built off-line contains
single entry pair abstract states. three types entries: (i) intermediate goal
ground-level entry state next abstract state; (ii) capped optimal lookahead depth
respect intermediate goal (iii) optimal lookahead depth respect global
goal. running algorithms capped lookaheads (i.e., c < 1000) need two databases
per map: one containing intermediate goals one containing capped optimal lookahead depths.
running effectively uncapped algorithms (i.e., c = 1000 c = 3000) also need third
database lookahead depths global goals (see Appendix discussion). Tables 5
912 report build times LRTA* (PDB, PDB) performance capped (i.e.,
build two pattern databases). Tables 13 14 report build times performance
effectively cap (i.e., built three pattern databases).
Finally, interest speeding experiments fact compute pattern databases
pairs abstract states. Instead, took advantage prior benchmark problem availability
445

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Table 9: Pattern databases average 512 512 map, computed intermediate goals.
Database size listed number abstract state pairs. Suboptimality planning
per move listed LRTA* (PDB, PDB) cap c = 30.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 2 years
est. 1.5 months
est. 4 days
13.4 hours
3.1 hours
1.1 hours
24 minutes
11 minutes

Planning per move
2.1
12.3
60.7
166.6
258.8

Suboptimality (times)
1.058
1.083
1.260
1.843
1.729

Table 10: Pattern databases average 512 512 map, computed intermediate goals.
Database size listed number abstract state pairs. Suboptimality planning
per move listed LRTA* (PDB, PDB) cap c = 40.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 2 years
est. 1.5 months
est. 4 days
13.1 hours
3.1 hours
1.0 hours
24 minutes
10 minutes

Planning per move
2.7
10.2
53.4
217.3
355.4

Suboptimality (times)
1.058
1.060
1.102
1.474
1.490

computed PDBs abstract goal states come play problems agents
solve. Thus, times tables estimates possible pairs.

Appendix D. Intermediate Goals Loops
shown Korf original paper, LRTA* complete lookahead depth
heuristic taken respect single global goal. completeness guarantee lost one
uses intermediate goals (i.e., LRTA* (F, PDB), LRTA* (PDB, PDB) well PR LRTA*
counter-parts). Indeed, abstract tile A, dynamic goal control module guide
agent towards entry state tile B. However, way, agent may stumble different
abstract tile C. soon happens, dynamic control module may select entry state tile
new intermediate goal. unsuspecting agent heads back everything repeats.
combat loops equipped algorithms use intermediate goals state reentrance detector. Namely, soon agent re-visits ground-level state, dynamic control
switches intermediate goal global goal. Additionally, new lookahead depth selected. Ideally, lookahead depth optimal depth respect global goal,
446

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Table 11: Pattern databases average 512 512 map, computed intermediate goals.
Database size listed number abstract state pairs. Suboptimality planning
per move listed LRTA* (PDB, PDB) cap c = 50.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 2 years
est. 1.5 months
est. 4 days
13.6 hours
3.1 hours
1.0 hours
24 minutes
11 minutes

Planning per move
3.5
11.1
68.5
279.4
452.3

Suboptimality (times)
1.058
1.059
1.098
1.432
1.386

Table 12: Pattern databases average 512 512 map, computed intermediate goals.
Database size listed number abstract state pairs. Suboptimality planning
per move listed LRTA* (PDB, PDB) cap c = 80.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 2 years
est. 1.5 months
est. 4 days
13.5 hours
3.2 hours
1.0 hours
25 minutes
10 minutes

Planning per move
6.6
22.9
109.7
523.3
811.5

Suboptimality (times)
1.058
1.059
1.087
1.411
1.301

capped c. Unfortunately, computing optimal lookahead depths global goals quite expensive
off-line (Tables 13 14). Given loops occur fairly infrequently, normally compute
optimal lookahead depths global goals. Instead, state re-visit detected, switch
global goals simply set lookahead cap c. saves off-line PDB computation time
sometimes causes agent conduct deeper search (c plies) really necessary.6
alternative solution investigated future research progressively increase lookahead on-line re-visits detected (i.e., every time re-visit occurs, lookahead depth
state increased certain number plies).

6. exception practice cases c = 1000 c = 3000 setting lookahead depth c
would immediately disqualified algorithm, provided reasonable real-time cut-off. Consequently,
two cap values, invest large amount time computed effectively uncapped optimal lookahead depth
respect global goals.

447

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Table 13: Pattern databases average 512 512 map, computed global goals. Database
size listed number abstract state pairs. Suboptimality planning per move
listed LRTA* (PDB, PDB) cap c = 3000.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 350 years
est. 25 years
est. 2 years
73 days
10.3 days
1.7 days
9.8 hours
2.5 hours

Planning per move
1.0
4.8
27.9
86.7
174.1

Suboptimality (times)
1.061
1.062
1.133
3.626
3.474

Table 14: Pattern databases average 512 512 map, computed global goals. Database
size listed number abstract state pairs. Suboptimality planning per move
listed LRTA* (PDB, G) cap c = 20.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 12 years
est. 6 months
est. 13 days
38.0 hours
7.5 hours
2.3 hours
52 minutes
21 minutes

448

Planning per move
349.9
331.6
281.0
298.1
216.1

Suboptimality (times)
6.468
8.766
10.425
8.155
14.989

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

References
BioWare Corp. (1998). Baldurs Gate., Published Interplay, http://www.bioware.com/bgate/,
November 30, 1998.
Bjornsson, Y., & Marsland, T. A. (2003). Learning extension parameters game-tree search. Inf.
Sci, 154(34), 95118.
Blizzard (2002). Warcraft 3: Reign chaos. http://www.blizzard.com/war3.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(12),
533.
Bonet, B., & Geffner, H. (2006). Learning depth-first search: unified approach heuristic search
deterministic non-deterministic settings, application MDPs. Proceedings
International Conference Automated Planning Scheduling (ICAPS), pp. 142
151, Cumbria, UK.
Bonet, B., Loerincs, G., & Geffner, H. (1997). fast robust action selection mechanism
planning.. Proceedings National Conference Artificial Intelligence (AAAI), pp.
714719, Providence, Rhode Island. AAAI Press / MIT Press.
Botea, A., Muller, M., & Schaeffer, J. (2004). Near optimal hierarchical path-finding. Journal
Game Development, 1(1), 728.
Bulitko, V. (2003a). Lookahead pathologies meta-level control real-time heuristic search.
Proceedings 15th Euromicro Conference Real-Time Systems, pp. 1316, Porto,
Portugal.
Bulitko, V. (2003b). Lookahead pathologies meta-level control real-time heuristic search.
Proceedings 15th Euromicro Conference Real-Time Systems, pp. 1316.
Bulitko, V. (2004). Learning adaptive real-time search. Tech. rep. http://arxiv.org/abs/cs.AI/
0407016, Computer Science Research Repository (CoRR).
Bulitko, V., Bjornsson, Y., Lustrek, M., Schaeffer, J., & Sigmundarson, S. (2007). Dynamic Control Path-Planning Real-Time Heuristic Search. Proceedings International
Conference Automated Planning Scheduling (ICAPS), pp. 4956, Providence, RI.
Bulitko, V., Li, L., Greiner, R., & Levner, I. (2003). Lookahead pathologies single agent search.
Proceedings International Joint Conference Artificial Intelligence (IJCAI), pp.
15311533, Acapulco, Mexico.
Bulitko, V., Sturtevant, N., & Kazakevich, M. (2005). Speeding learning real-time search via
automatic state abstraction. Proceedings National Conference Artificial Intelligence (AAAI), pp. 13491354, Pittsburgh, Pennsylvania.
Bulitko, V., Sturtevant, N., Lu, J., & Yau, T. (2007). Graph Abstraction Real-time Heuristic
Search. Journal Artificial Intelligence Research (JAIR), 30, 51100.
Buro, M. (2000). Experiments Multi-ProbCut new high-quality evaluation function
Othello. van den Herik, H. J., & Iida, H. (Eds.), Games AI Research, pp. 7796. U.
Maastricht.
Culberson, J., & Schaeffer, J. (1996). Searching pattern databases. CSCI (Canadian AI
Conference), Advances Artificial Intelligence, pp. 402416. Springer-Verlag.
449

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Culberson, J., & Schaeffer, J. (1998). Pattern Databases. Computational Intelligence, 14(3), 318
334.
Dijkstra, E. W. (1959). note two problems connexion graphs.. Numerische Mathematik,
1, 269271.
Furcy, D. (2006). ITSA*: Iterative tunneling search A*. Proceedings National
Conference Artificial Intelligence (AAAI), Workshop Heuristic Search, Memory-Based
Heuristics Applications, Boston, Massachusetts.
Furcy, D., & Koenig, S. (2000). Speeding convergence real-time search. Proceedings
National Conference Artificial Intelligence (AAAI), pp. 891897.
Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determination
minimum cost paths. IEEE Transactions Systems Science Cybernetics, 4(2), 100107.
Hernandez, C., & Meseguer, P. (2005a). Improving convergence LRTA*(k). Proceedings
International Joint Conference Artificial Intelligence (IJCAI), Workshop Planning
Learning Priori Unknown Dynamic Domains, Edinburgh, UK.
Hernandez, C., & Meseguer, P. (2005b). LRTA*(k). Proceedings 19th International Joint
Conference Artificial Intelligence (IJCAI), Edinburgh, UK.
Hernandez, C., & Meseguer, P. (2007). Improving real-time heuristic search initially unknown
maps. Proceedings International Conference Automated Planning Scheduling
(ICAPS), Workshop Planning Games, p. 8, Providence, Rhode Island.
Hoffmann, J. (2000). heuristic domain independent planning use enforced hillclimbing algorithm. Proceedings 12th International Symposium Methodologies
Intelligent Systems (ISMIS), pp. 216227.
id Software (1993). Doom., Published id Software, http://en.wikipedia.org/ wiki/Doom, December 10, 1993.
Ishida, T. (1992). Moving target search intelligence. Proceedings National Conference
Artificial Intelligence (AAAI), pp. 525532.
Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjou, A., & Shimada, S. (1999).
Robocup rescue: Search rescue large-scale disasters domain autonomous agents
research. Man, Systems, Cybernetics, pp. 739743.
Kocsis, L. (2003). Learning Search Decisions. Ph.D. thesis, University Maastricht.
Koenig, S. (2004). comparison fast search methods real-time situated agents. Proceedings International Joint Conference Autonomous Agents Multiagent Systems
(AAMAS), pp. 864871.
Koenig, S. (2001). Agent-centered search. AI Magazine, 22(4), 109132.
Koenig, S., Furcy, D., & Bauer, C. (2002). Heuristic search-based replanning. Proceedings
Int. Conference Artificial Intelligence Planning Scheduling, pp. 294301.
Koenig, S., & Likhachev, M. (2006). Real-time adaptive A*. Proceedings International
Joint Conference Autonomous Agents Multiagent Systems, pp. 281288.
Korf, R. (1985). Depth-first iterative deepening : optimal admissible tree search. Artificial
Intelligence, 27(3), 97109.
450

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42(23), 189211.
Korf, R. (1993). Linear-space best-first search. Artificial Intelligence, 62, 4178.
Likhachev, M., Ferguson, D., Gordon, G., Stentz, A., & Thrun, S. (2005). Anytime dynamic A*:
anytime, replanning algorithm. Proceedings International Conference Automated
Planning Scheduling (ICAPS).
Likhachev, M., Gordon, G. J., & Thrun, S. (2004). ARA*: Anytime A* provable bounds
sub-optimality. Thrun, S., Saul, L., & Scholkopf, B. (Eds.), Advances Neural Information Processing Systems 16. MIT Press, Cambridge, MA.
Likhachev, M., & Koenig, S. (2005). generalized framework lifelong planning A*. Proceedings International Conference Automated Planning Scheduling (ICAPS),
pp. 99108.
Lustrek, M. (2005). Pathology single-agent search. Proceedings Information Society Conference, pp. 345348, Ljubljana, Slovenia.
Lustrek, M., & Bulitko, V. (2006). Lookahead pathology real-time path-finding. Proceedings
National Conference Artificial Intelligence (AAAI), Workshop Learning Search,
pp. 108114, Boston, Massachusetts.
Mero, L. (1984). heuristic search algorithm modifiable estimate. Artificial Intelligence, 23,
1327.
Moore, A., & Atkeson, C. (1993). Prioritized sweeping: Reinforcement learning less data
less time. Machine Learning, 13, 103130.
Nash, A., Daniel, K., & Felner, S. K. A. (2007). Theta*: Any-angle path planning grids.
Proceedings National Conference Artificial Intelligence, pp. 11771183.
Pearl, J. (1984). Heuristics. Addison-Wesley.
Rayner, D. C., Davison, K., Bulitko, V., Anderson, K., & Lu, J. (2007). Real-time heuristic search
priority queue. Proceedings International Joint Conference Artificial
Intelligence (IJCAI), pp. 23722377, Hyderabad, India.
Russell, S., & Wefald, E. (1991). Right Thing: Studies Limited Rationality. MIT Press.
Schaeffer, J. (1989). history heuristic alpha-beta search enhancements practice. IEEE
Transactions Pattern Analysis Machine Intelligence, PAMI-11(1), 12031212.
Schaeffer, J. (2000). Search ideas Chinook. van den Herik, H. J., & Iida, H. (Eds.), Games
AI Research, pp. 1930. U. Maastricht.
Shimbo, M., & Ishida, T. (2003). Controlling learning process real-time heuristic search.
Artificial Intelligence, 146(1), 141.
Sigmundarson, S., & Bjornsson, Y. (2006). Value Back-Propagation vs. Backtracking RealTime Search. Proceedings National Conference Artificial Intelligence (AAAI),
Workshop Learning Search, pp. 136141, Boston, Massachusetts, USA.
Stenz, A. (1995). focussed D* algorithm real-time replanning. Proceedings
International Joint Conference Artificial Intelligence (IJCAI), pp. 16521659.
Stout, B. (2000). basics A* path planning. Game Programming Gems. Charles River
Media.
451

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Sturtevant, N. (2007). Memory-efficient abstractions pathfinding. Proceedings third
conference Artificial Intelligence Interactive Digital Entertainment, pp. 3136, Stanford, California.
Sturtevant, N., & Buro, M. (2005). Partial pathfinding using map abstraction refinement.
Proceedings National Conference Artificial Intelligence, pp. 13921397.
Sturtevant, N., & Jansen, R. (2007). analysis map-based abstraction refinement.
Proceedings 7th International Symposium Abstraction, Reformulation Approximation, Whistler, British Columbia.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical machine learning tools techniques
(2nd edition). Morgan Kaufmann, San Fransisco.

452

fiJournal Artificial Intelligence Research 32 (2008) 289353

Submitted 09/07; published 05/08

Optimal Approximate Q-value Functions
Decentralized POMDPs
Frans A. Oliehoek

f.a.oliehoek@uva.nl

Intelligent Systems Lab Amsterdam, University Amsterdam
Amsterdam, Netherlands

Matthijs T.J. Spaan

mtjspaan@isr.ist.utl.pt

Institute Systems Robotics, Instituto Superior Tecnico
Lisbon, Portugal

Nikos Vlassis

vlassis@dpem.tuc.gr
Department Production Engineering Management, Technical University Crete
Chania, Greece

Abstract
Decision-theoretic planning popular approach sequential decision making problems, treats uncertainty sensing acting principled way. single-agent
frameworks like MDPs POMDPs, planning carried resorting Q-value
functions: optimal Q-value function Q computed recursive manner dynamic
programming, optimal policy extracted Q . paper study
whether similar Q-value functions defined decentralized POMDP models (DecPOMDPs), policies extracted value functions. define two
forms optimal Q-value function Dec-POMDPs: one gives normative description Q-value function optimal pure joint policy another one
sequentially rational thus gives recipe computation. computation, however,
infeasible smallest problems. Therefore, analyze various approximate
Q-value functions allow efficient computation. describe relate,
prove provide upper bound optimal Q-value function Q . Finally,
unifying previous approaches solving Dec-POMDPs, describe family algorithms extracting policies Q-value functions, perform experimental
evaluation existing test problems, including new firefighting benchmark problem.

1. Introduction
One main goals artificial intelligence (AI) development intelligent agents,
perceive environment sensors influence environment
actuators. setting, essential problem agent decide
action perform certain situation. work, focus planning: constructing
plan specifies action take situation agent might encounter
time. particular, focus planning cooperative multiagent system (MAS):
environment multiple agents coexist interact order perform joint
task. adopt decision-theoretic approach, allows us tackle uncertainty
sensing acting principled way.
Decision-theoretic planning roots control theory operations research.
control theory, one controllers control stochastic system specific output
c
2008
AI Access Foundation. rights reserved.

fiOliehoek, Spaan & Vlassis

goal. Operations research considers tasks related scheduling, logistics work flow
tries optimize concerning systems. Decision-theoretic planning problems
formalized Markov decision processes (MDPs), frequently employed
control theory well operations research, also adopted AI
planning stochastic environments. fields goal find (conditional)
plan, policy, optimal respect desired behavior. Traditionally, main
focus systems one agent controller, last decade interest
systems multiple agents decentralized control grown.
different, also related field game theory. Game theory considers agents,
called players, interacting dynamic, potentially stochastic process, game. goal
find optimal strategies agents, specify play
therefore correspond policies. contrast decision-theoretic planning, game theory
always considered multiple agents, consequence several ideas concepts
game theory applied decentralized decision-theoretic planning.
work apply game-theoretic models decision-theoretic planning multiple agents.
1.1 Decision-Theoretic Planning
last decades, Markov decision process (MDP) framework gained popularity
AI community model planning uncertainty (Boutilier, Dean, & Hanks,
1999; Guestrin, Koller, Parr, & Venkataraman, 2003). MDPs used formalize
discrete time planning task single agent stochastically changing environment,
condition agent observe state environment. Every time step
state changes stochastically, agent chooses action selects particular
transition function. Taking action particular state time step induces
probability distribution states time step + 1.
agents objective formulated several ways. first type objective
agent reaching specific goal state, example maze agents
goal reach exit. different formulation given associating certain cost
execution particular action particular state, case goal
minimize expected total cost. Alternatively, one associate rewards actions
performed certain state, goal maximize total reward.
agent knows probabilities state transitions, i.e., knows
model, contemplate expected transitions time construct plan
likely reach specific goal state, minimizes expected costs maximizes
expected reward. stands contrast reinforcement learning (RL) (Sutton &
Barto, 1998), agent model environment, learn
good behavior repeatedly interacting environment. Reinforcement learning
seen combined task learning model environment planning,
although practice often necessary explicitly recover environment model.
article focus planning, consider two factors complicate computing
successful plans: inability agent observe state environment well
presence multiple agents.
real world agent might able determine state environment exactly is, agents sensors noisy and/or limited. sensors
290

fiOptimal Approximate Q-Value Functions Dec-POMDPs

noisy, agent receive faulty inaccurate observations probability.
sensors limited agent unable observe differences states cannot
detected sensor, e.g., presence absence object outside laser rangefinders field view. sensor reading might require different action choices,
phenomenon referred perceptual aliasing. order deal introduced
sensor uncertainty, partially observable Markov decision process (POMDP) extends
MDP model incorporating observations probability occurrence conditional
state environment (Kaelbling, Littman, & Cassandra, 1998).
complicating factor consider presence multiple agents. Instead
planning single agent plan team cooperative agents. assume
communication within team possible.1 major problem setting
agents coordinate actions. Especially, agents assumed
observe stateeach agent knows observations received actions taken
common signal condition actions on. Note problem
addition problem partial observability, substitution it; even
agents could freely instantaneously communicate individual observations,
joint observations would disambiguate true state.
One option consider agent separately, agent maintain
explicit model agents. approach chosen Interactive
POMDP (I-POMDP) framework (Gmytrasiewicz & Doshi, 2005). problem approach, however, agents also model considered agent, leading
infinite recursion beliefs regarding behavior agents. adopt decentralized
partially observable Markov decision process (Dec-POMDP) model class problems
(Bernstein, Givan, Immerman, & Zilberstein, 2002). Dec-POMDP generalization
multiple agents POMDP used model team cooperative agents
situated stochastic, partially observable environment.
single-agent MDP setting received much attention, many results known.
particular known optimal plan, policy, extracted optimal
action-value, Q-value, function Q (s,a), latter calculated efficiently.
POMDPs, similar results available, although finding optimal solution harder
(PSPACE-complete finite-horizon problems, Papadimitriou & Tsitsiklis, 1987).
hand, Dec-POMDPs relatively little known except
provably intractable (NEXP-complete, Bernstein et al., 2002). particular, outstanding
issue whether Q-value functions defined Dec-POMDPs (PO)MDPs,
whether policies extracted Q-value functions. Currently algorithms planning Dec-POMDPs based version policy search (Nair,
Tambe, Yokoo, Pynadath, & Marsella, 2003b; Hansen, Bernstein, & Zilberstein, 2004; Szer,
Charpillet, & Zilberstein, 2005; Varakantham, Marecki, Yabu, Tambe, & Yokoo, 2007),
proper theory Q-value functions Dec-POMDPs still lacking. Given wide range
applications value functions single-agent decision-theoretic planning, expect
theory Dec-POMDPs great benefits, terms providing insight
well guiding design solution algorithms.
1. turns out, framework consider also model communication particular cost
subject minimization (Pynadath & Tambe, 2002; Goldman & Zilberstein, 2004). noncommunicative setting interpreted special case infinite cost.

291

fiOliehoek, Spaan & Vlassis

1.2 Contributions
paper develop theory Q-value functions Dec-POMDPs, showing
optimal Q-function Q defined Dec-POMDP. define two forms optimal
Q-value function Dec-POMDPs: one gives normative description Q-value
function optimal pure joint policy another one sequentially rational
thus gives recipe computation. also show given Q , optimal policy
computed forward-sweep policy computation, solving sequence Bayesian games
forward time (i.e., first last time step), thereby extending
solution technique Emery-Montemerlo, Gordon, Schneider, Thrun (2004)
exact setting.
Computation Q infeasible smallest problems. Therefore, analyze
three different approximate Q-value functions QMDP , QPOMDP QBG
efficiently computed constitute upper bounds Q . also describe generalized form QBG includes QPOMDP , QBG Q . used prove hierarchy
upper bounds: Q QBG QPOMDP QMDP .
Next, show approximate Q-value functions used compute optimal
sub-optimal policies. describe generic policy search algorithm, dub
Generalized MAA (GMAA ) generalization MAA algorithm Szer et al.
(2005), used extracting policy approximate Q-value function.
varying implementation sub-routine algorithm, algorithm unifies MAA
forward-sweep policy computation thus approach Emery-Montemerlo et al.
(2004).
Finally, experimental evaluation examine differences QMDP ,
QPOMDP , QBG Q several problems. also experimentally verify potential
benefit tighter heuristics, testing different settings GMAA well known
test problems new benchmark problem involving firefighting agents.
article based previous work Oliehoek Vlassis (2007)abbreviated OV
herecontaining several new contributions: (1) Contrary OV work, current work
includes section sequential rational description Q suggests way compute
Q practice (OV provided normative description Q ). (2) current work
provides formal proof hierarchy upper bounds Q (which qualitatively
argued OV paper). (3) current article additionally contains proof
solutions Bayesian games identical payoffs given equation (4.2) constitute
Pareto optimal Nash equilibria game (which proven OV paper). (4)
article contains extensive experimental evaluation derived bounds
Q , introduces new benchmark problem (firefighting). (5) Finally, current article
provides complete introduction Dec-POMDPs existing solution methods,
well Bayesian games, hence serve self-contained introduction Dec-POMDPs.
1.3 Applications
Although field multiagent systems stochastic, partially observable environment
seems quite specialized thus narrow, application area actually broad.
real world practically always partially observable due sensor noise perceptual
aliasing. Also, domains communication free, consumes resources
292

fiOptimal Approximate Q-Value Functions Dec-POMDPs

thus particular cost. Therefore models Dec-POMDPs, consider
partially observable environments relevant essentially teams embodied agents.
Example applications type given Emery-Montemerlo (2005), considered multi-robot navigation team agents noisy sensors act
find/capture goal. Becker, Zilberstein, Lesser, Goldman (2004b) use multi-robot
space exploration example. Here, agents Mars rovers decide
proceed mission: whether collect particular samples specific sites not.
rewards particular samples sub- super-additive, making task non-trivial.
overview application areas cooperative robotics presented Arai, Pagello,
Parker (2002), among robotic soccer, applied RoboCup (Kitano, Asada, Kuniyoshi, Noda, & Osawa, 1997). Another application investigated within project
crisis management: RoboCup Rescue (Kitano, Tadokoro, Noda, Matsubara, Takahashi,
Shinjoh, & Shimada, 1999) models situation rescue teams perform search
rescue task crisis situation. task also modeled partially observable system (Nair, Tambe, & Marsella, 2002, 2003, 2003a; Oliehoek & Visser, 2006; Paquet,
Tobin, & Chaib-draa, 2005).
also many types applications. Nair, Varakantham, Tambe,
Yokoo (2005), Lesser, Ortiz Jr., Tambe (2003) give applications distributed sensor
networks (typically used surveillance). example load balancing among queues
presented Cogill, Rotkowitz, Roy, Lall (2004). agents represent queues
observe queue sizes immediate neighbors. decide
whether accept new jobs pass another queue. Another frequently considered
application domain communication networks. Peshkin (2001) treated packet routing
application agents routers minimize average transfer time
packets. connected immediate neighbors decide time step
neighbor send packet. approaches communication networks using
decentralized, stochastic, partially observable systems given Ooi Wornell (1996),
Tao, Baxter, Weaver (2001), Altman (2002).
1.4 Overview Article
rest article organized follows. Section 2 first formally introduce
Dec-POMDP model provide background components. existing solution
methods treated Section 3. Then, Section 4 show Dec-POMDP
modeled series Bayesian games constitutes theory Q-value functions
BGs. also treat two forms optimal Q-value functions, Q , here. Approximate
Q-value functions described Section 5 one applications discussed
Section 6. Section 7 presents results experimental evaluation. Finally, Section 8
concludes.

2. Decentralized POMDPs
section define Dec-POMDP model discuss properties. Intuitively, Dec-POMDP models number agents inhabit particular environment,
considered discrete time steps, also referred stages (Boutilier et al., 1999)
(decision) epochs (Puterman, 1994). number time steps agents interact
293

fiOliehoek, Spaan & Vlassis

environment called horizon decision problem, denoted h.
paper horizon assumed finite. stage = 0,1,2, . . . ,h 1 every agent
takes action combination actions influences environment, causing
state transition. next time step, agent first receives observation environment, take action again. probabilities state transitions
observations specified Dec-POMDP model, rewards received particular actions particular states. transition- observation probabilities specify
dynamics environment, rewards specify behavior desirable. Hence,
reward model defines agents goal task: agents come plan
maximizes expected long term reward signal. work assume planning
takes place off-line, computed plans distributed agents,
merely execute plans on-line. is, computation plan centralized,
execution decentralized. centralized planning phase, entire model detailed
available. execution agent knows joint policy found
planning phase individual history actions observations.
2.1 Formal Model
section formally treat basic components Dec-POMDP. start
giving mathematical definition components.
Definition 2.1. decentralized
partially observable
Markov decision process (Dec

ff
POMDP) defined tuple n,S,A,T,R,O,O,h,b0 where:
n number agents.
finite set states.
set joint actions.
transition function.
R immediate reward function.
set joint observations.
observation function.
h horizon problem.
b0 P(S), initial state distribution time = 0.2
Dec-POMDP model extends single-agent (PO)MDP models considering joint
actions observations. particular, define = Ai set joint actions. Here,
Ai set actions available agent i. Every time step, one joint action = ha1 ,...,an
taken. Dec-POMDP, agents know individual action; observe
others actions. assume action ai Ai selected time.
set Ai depend stage state environment. general,
2. P() denotes set probability distributions ().

294

fiOptimal Approximate Q-Value Functions Dec-POMDPs

denote stage using superscripts, denotes joint action taken stage t, ati
individual action agent taken stage t. Also, write a6=i = ha1 , . . . ,ai1 ,ai+1 , . . . ,an
profile actions agents i.
Similarly set joint actions, = Oi set joint observations,
Oi set observations available agent i. Every time step environment emits one
joint observation = ho1 ,...,on i, agent observes component
oi , illustrated Figure 1. Notation respect time indices observations
analogous notation actions. paper, assume actionand observation sets finite. Infinite action- observation sets difficult
deal even single-agent case, authors knowledge research
performed topic partially observable, multiagent case.
Actions observations interface agents environment.
Dec-POMDP framework describes environment states transitions.
means rather considering complex, typically domain-dependent model
environment explains environment works, descriptive stance taken:

Dec-POMDP specifies environment model simply set states = s1 ,...,s|S|
environment in, together probabilities state transitions
dependent executed joint actions. particular, transition state next
state depends stochastically past states actions. probabilistic dependence
models outcome uncertainty: fact outcome action cannot predicted
full certainty.
important characteristic Dec-POMDPs states possess Markov
property. is, probability particular next state depends current state
joint action, whole history:
P (st+1 |st ,at ,st1 ,at1 ,...,s0 ,a0 ) = P (st+1 |st ,at ).

(2.1)

Also, assume transition probabilities stationary, meaning
independent stage t.
way similar transition model describes stochastic influence
actions environment, observation model describes state environment perceived agents. Formally, observation function, mapping
joint actions successor states probability distributions joint observations:
: P(O). I.e., specifies
P (ot |at1 ,st ).

(2.2)

implies observation model also satisfies Markov property (there
dependence history). Also observation model assumed stationary:
dependence stage t.
Literature identified different categories observability (Pynadath & Tambe, 2002;
Goldman & Zilberstein, 2004). observation function individual
observation agents always uniquely identify true state, problem
considered fully- individually observable. case, Dec-POMDP effectively
reduces multiagent MDP (MMDP) described Boutilier (1996). extreme
problem non-observable, meaning none agents observes useful
295

fiOliehoek, Spaan & Vlassis

actions

a0n

a1n

h1


..
.

..
.

..
.

a01

a11

a1h1

o01

o0n

o1n

observations

o0

states

s0



0

o11

h1


o1

a0

s1

oh1

...

a1

...

o1h1

ah2

sh1

ah1

h1

1

Figure 1: illustration dynamics Dec-POMDP. every stage environment
particular state. state emits joint observation, agent
observes individual observation. agent selects action forming
joint action.

information.
modeled fact agents always receive null-observation,
Oi = oi, . non-observability agents employ open-loop plan.
two extremes partially observable problems. One special case
identified, namely case individual, joint observation identifies
true state. case referred jointly- collectively observable. jointly observable
Dec-POMDP referred Dec-MDP.
reward function R(s,a) used specify goal agents function states joint actions. particular, desirable sequence joint actions
correspond high long-term reward, formalized return.
Definition 2.2. Let return cumulative reward Dec-POMDP defined total
rewards received execution:
r(0) + r(1) + + r(h 1),

(2.3)

r(t) reward received time step t.
When, stage t, state st taken joint action , r(t) =
R(st ,a). Therefore, given sequence states taken joint actions, straightforward
determine return substitution r(t) R(st ,a) (2.3).
296

fiOptimal Approximate Q-Value Functions Dec-POMDPs

paper consider optimality criterion expected cumulative reward,
expectation refers expectation sequences states executed joint actions.
planning problem find conditional plan, policy, agent maximize
optimality criterion. Dec-POMDP case amounts finding tuple policies,
called joint policy maximizes expected cumulative reward.
Note that, Dec-POMDP, agents assumed observe immediate
rewards: observing immediate rewards could convey information regarding true state
present received observations, undesirable information
available agents modeled observations. planning DecPOMDPs thing matters expectation cumulative future reward
available off-line planning phase, actual reward obtained. Indeed,
even assumed actual reward observed end episode.
Summarizing, work consider Dec-POMDPs finite actions observation
sets finite planning horizon. Furthermore, consider general Dec-POMDP setting, without simplifying assumptions observation, transition, reward models.
2.2 Example: Decentralized Tiger Problem
describe decentralized tiger problem introduced Nair et al. (2003b).
test problem frequently used (Nair et al., 2003b; Emery-Montemerlo et al.,
2004; Emery-Montemerlo, Gordon, Schneider, & Thrun, 2005; Szer et al., 2005)
modification (single-agent) tiger problem (Kaelbling et al., 1998). concerns two
agents standing hallway two doors. Behind one doors tiger,
behind treasure. Therefore two states: tiger behind left door
(sl ) behind right door (sr ). agents 3 actions disposal: open
left door (aOL ), open right door (aOR ) listen (aLi ). cannot observe
others actions. fact, receive 2 observations. Either hear sound left
(oHL ) right (oHR ).
= 0 state sl sr probability 0.5. long agent opens door
state doesnt change, door opened, state resets sl sr probability 0.5.
full transition, observation reward model listed Nair et al. (2003b).
observation probabilities independent, identical agents. instance,
state sl perform action aLi , agents 85% chance observing
oHL , probability hearing tiger left 0.85 0.85 = 0.72.
agents open door treasure receive positive reward,
receive penalty opening wrong door. opening wrong door jointly,
penalty less severe. Opening correct door jointly leads higher reward.
Note that, wrong door opened one agents, attacked
tiger receive penalty. However, neither agents observe attack
penalty episode continues. Arguably, natural representation would
episode end door opened let agents observe whether
encountered tiger treasure, however considered test problem.
297

fiOliehoek, Spaan & Vlassis

2.3 Histories
mentioned, goal planning Dec-POMDP find (near-) optimal tuple
policies, policies specify agent act specific situation.
Therefore, define policy, first need define exactly specific
situations are. essence situations parts history process
agents observe.
Let us first consider history process is. Dec-POMDP horizon h
specifies h time steps stages = 0,...,h 1. stages, state st ,
joint observation ot joint action . Therefore, agents select
k-th actions (at = k 1), history process sequence states, joint
observations joint actions, following form:


s0 ,o0 ,a0 ,s1 ,o1 ,a1 ,...,sk1 ,ok1 .

s0 initial state, drawn according initial state distribution b0
. initial
ff
joint observation o0 assumed empty joint observation: o0 = = o1, ,...,on, .
history process, states remain unobserved agent
observe actions observations. Therefore agent base decision
regarding action select sequence actions observations observed
point.
Definition 2.3. define action-observation history agent i, ~i , sequence
actions taken observations received agent i. specific time step t, is:


~it = o0i ,a0i ,o1i . . . ,at1
,oi .

~ action-observation history agents:
joint action-observation history, ,
~ = h~1t , . . . ,~nt i.
~ = (Oi Ai ).
Agent set possible action-observation histories time

~ = h1
~t 3
set possible action-observation histories agent
t=0 . Finally set
h1 ~
~
~ tn ).
possible joint action-observation histories given = t=0 (1 ...
= 0, action-observation history empty, denoted ~ 0 = ~ .
also use notion history using observations agent.
Definition 2.4. Formally, define observation history agent i, ~oi , sequence
observations agent received. specific time step t, is:

~oit = o0i ,o1i , . . . ,oti .
joint observation history, ~o, observation history agents:
~o = h~o1t , . . . ,~ont i.
3. Note particular Dec-POMDP, may case histories actually
realized, probabilities specified transition observation model.

298

fiOptimal Approximate Q-Value Functions Dec-POMDPs

~ = Oi . Similar
set observation histories agent time denoted

~
~ empty observation
notation action-observation histories, also use
history denoted ~o .
Similarly define action history follows.
Definition 2.5. action history agent i, ~ai , sequence actions agent
performed. specific time step t, write:

~ait = a0i ,a1i , . . . ,at1
.


Notation joint action histories sets analogous observation histories. Also write ~o6=i ,~6=i , etc. denote tuple observation-, action-observation histories,
etc. agents except i. Finally note that, clearly, (joint) action-observation


ff
history consists (joint) action- (joint) observation history: ~ = ~o ,~a .

2.4 Policies

discussed previous section, action-observation history agent specifies
information agent decide upon action. moment
assume individual policy agent deterministic mapping actionobservation sequences actions.
number possible action-observation histories usually large set
grows exponentially horizon problem. time step t, (|Ai | |Oi |)t
action-observation histories agent i. consequence total
h1
X

(|Ai | |Oi |)t =

t=0

(|Ai | |Oi |)h 1
(|Ai | |Oi |) 1

sequences agent i. Therefore number policies agent becomes:
|Ai

(|Ai ||Oi |)h 1
| (|Ai ||Oi |)1

,

(2.4)

doubly exponential horizon h.
2.4.1 Pure Stochastic Policies
possible reduce number policies consideration realizing lot
policies specify behavior. illustrated left side Figure 2,
clearly shows deterministic policy subset possible action-observation
histories reached. Policies differ respect action-observation history
reached first place, manifest behavior. consequence
order specify deterministic policy, observation history suffices: agent
takes action deterministically, able infer action took
observation history illustrated right side Figure 2.
Definition 2.6. pure deterministic policy, , agent Dec-POMDP
~ Ai . set pure policies
mapping observation histories actions, :
agent denoted .
299

fiOliehoek, Spaan & Vlassis

act.-obs. history
aLi
oHL

oHR

aOL

aOR
aLi

aOL
aLi

aOR

oHL

aOL

aOR
aOL

oHR

aLi

aLi

oHL

oHR

oHL

oHR

aOL

aLi

aLi

aOR

Figure 2: deterministic policy represented tree. Left: tree actionobservation histories ~i one agents Dec-Tiger problem.
arbitrary deterministic policy highlighted. Clearly shown
reaches subset histories ~i . (~i reached expanded.) Right: policy shown simplified policy tree.

Note also pure policies sometimes write (~i ). case mean
action specifies observation history contained ~i . instance, let ~i =
h~oi ,~ai i, (~i ) = (~oi ). use = h1 ,...,n denote joint policy, profile
specifying policy agent. say pure joint policy induced implicit
~ A. is, mapping
mapping joint observation histories joint actions :
induced individual policies make joint policy. Also use 6=i =
h1 , . . . ,i1 ,i+1 , . . . ,n i, denote profile policies agents i.
Apart pure policies, also possible agents execute randomized
policies, i.e., policies always specify action situation,
element chance decides action performed.
two types randomized policies: mixed policies stochastic policies.
Definition 2.7. mixed policy, , agent set pure policies, , along
probability distribution set. Thus mixed policy P(M) element
set probability distributions M.
Definition 2.8. stochastic behavioral policy, , agent mapping action~ P(Ai ).
observation histories probability distributions actions, :
considering stochastic policies, keeping track observations insufficient, general action-observation histories realized. stochastic
policies defined mapping full space action-observation histories probability distributions actions. Note use denote policy (space)
general, also randomized policies. use , need
discriminate different types policies.
300

fiOptimal Approximate Q-Value Functions Dec-POMDPs

common way represent temporal structure policy split decision
rules specify policy stage. individual policy represented
sequence decision rules = (i0 , . . . ,ih1 ). case deterministic policy, form
decision rule stage mapping length-t observation histories actions
~ Ai .
:

2.4.2 Special Cases Simpler Policies.
special cases Dec-POMDPs policy specified
simpler way. treat three cases: case state observable,
single-agent case case combines previous two: single agent
environment observe state.
last case, single agent fully observable environment, corresponds regular
MDP setting. agent observe state, Markovian, agent
need remember
history, simply specify decision rules policy

0
h1
= , . . . ,
mappings states actions: : A. complexity
policy representation reduces even infinite-horizon case, optimal
policy known stationary. such, one decision rule , used
stages.
true multiple agents observe state, i.e., fully observable
Dec-POMDP defined Section 2.1. essentially setting multiagent
Markov decision process (MMDP) introduced Boutilier (1996). case, decision
rules agent policy mappings states actions : Ai , although
case care needs taken make sure coordination errors occur
searching individual policies.
POMDP, Dec-POMDP single agent, agent cannot observe state,
possible specify policy mapping states actions. However,
turns maintaining probability distribution states, called belief, b P(S),
Markovian signal:
P (st+1 |at ,ot ,at1 ,ot1 , . . . ,a0 ,o0 ) = P (st+1 |bt ,at ),
belief bt defined
st

bt (st ) P (st |ot ,at1 ,ot1 , . . . ,a0 ,o0 ) = P (st |bt1 ,at1 ,ot ).

result, single agent partially observable environment specify policy
series mappings set beliefs actions : P(S) A.
Unfortunately, general case consider, space-saving simplifications
policy possible. Even though transition observation model used
compute joint belief, computation requires knowledge joint actions
observations. execution, agents simply access information
thus compute joint belief.
2.4.3 Quality Joint Policies
Clearly, policies differ much reward expect accumulate, serve
criterion joint policys quality. Formally, consider expected cumulative
reward joint policy, also referred value.
301

fiOliehoek, Spaan & Vlassis

Definition 2.9. value V () joint policy defined
V () E

h1
hX
t=0

fi

fi
R(st ,at )fi,b0 ,

(2.5)

expectation states, observations andin case randomized
actions.
particular calculate expectation
V () =

h1 X X
X

P (st ,~ |,b0 )

t=0 ~
~ st

X

R(st ,at )P (at |~ ),

(2.6)



P (at |~ ) probability specified , P (st ,~ |,b0 ) recursively defined
X
P (st ,~ |,b0 ) =
P (st ,~ |st1 ,~ t1 ,)P (st1 ,~ t1 |,b0 ),
(2.7)
st1


P (st ,~ |st1 ,~ t1 ,) = P (ot |at1 ,st )P (st |st1 ,at1 )P (at1 |~ t1 )

(2.8)

term completely specified transition observation model joint
policy. stage 0 P (s0 ,~ |,b0 ) = b0 (s0 ).
recursive nature P (st ,~ |,b0 ) intuitive specify value
recursively:


X
X X
V (st ,~ ) =
P (at |~ ) R(st ,at ) +
P (st+1 ,ot+1 |st ,at )V (st+1 ,~ t+1 ) ,


st+1 ot+1

(2.9)

~ t+1 = (~ ,at ,ot+1 ). value joint policy given
X
V () =
V (s0 ,~ )b0 (s0 ).

(2.10)

s0

special case evaluating pure joint policy , eq. (2.6) written as:
V () =

h1 X
X

P (~ |,b0 )R(~ ,(~ )),

(2.11)

t=0 ~
~t


R(~ ,at ) =

X

R(st ,at )P (st |~ ,b0 )

(2.12)

st

denotes expected immediate reward. case, recursive formulation (2.9) reduces

X X

Vt (st ,~o ) = R st ,(~o ) +
P (st+1 ,ot+1 |st ,(~o ))Vt+1 (st+1 ,~o t+1 ).
(2.13)
st+1 ot+1

302

fiOptimal Approximate Q-Value Functions Dec-POMDPs

Note that, performing computation value joint policy recursively,
intermediate results cached. particular (st+1 ,~o t+1 )-pair (or (st+1 ,~ t+1 )-pair
stochastic joint policy) reached |S| states st previous stage. value
Vt+1 (st+1 ,~o t+1 ) same, however, computed once.
2.4.4 Existence Optimal Pure Joint Policy
Although randomized policies may useful, restrict attention pure policies
without sacrificing optimality, shown following.
Proposition 2.1. Dec-POMDP least one optimal pure joint policy.
Proof. See appendix A.1.

3. Overview Dec-POMDP Solution Methods
order provide background solving Dec-POMDPs, section gives overview
recently proposed methods. limit review number finite-horizon
methods general Dec-POMDPs related approach.
review work performed infinite-horizon Dec-POMDPs,
work Peshkin, Kim, Meuleau, Kaelbling (2000), Bernstein, Hansen, Zilberstein
(2005), Szer Charpillet (2005), Amato, Bernstein, Zilberstein (2006, 2007a).
setting policies usually represented finite state controllers (FSCs). Since infinitehorizon Dec-POMDP undecidable (Bernstein et al., 2002), line work, focuses
finding -approximate solutions (Bernstein, 2005) (near-) optimal policies given
particular controller size.
also substantial amount work methods exploiting particular independence assumptions. particular, transition observation independent Dec-MDPs
(Becker et al., 2004b; Wu & Durfee, 2006) Dec-POMDPs (Kim, Nair, Varakantham,
Tambe, & Yokoo, 2006; Varakantham et al., 2007) received quite attention.
models assume agent individual state space Si actions one agent influence transitions local states another agent.
Although models easier solve, independence assumptions severely restrict
applicability. special cases considered are, instance, goal
oriented Dec-POMDPs (Goldman & Zilberstein, 2004), event-driven Dec-MDPs (Becker,
Zilberstein, & Lesser, 2004a), Dec-MDPs time resource constraints (Beynier &
Mouaddib, 2005, 2006; Marecki & Tambe, 2007), Dec-MDPs local interactions (Spaan
& Melo, 2008) factored Dec-POMDPs additive rewards (Oliehoek, Spaan, Whiteson, & Vlassis, 2008).
final body related work beyond scope article models
techniques explicit communication Dec-POMDP settings (Ooi & Wornell, 1996; Pynadath & Tambe, 2002; Goldman & Zilberstein, 2003; Nair, Roth, & Yohoo, 2004; Becker,
Lesser, & Zilberstein, 2005; Roth, Simmons, & Veloso, 2005; Oliehoek, Spaan, & Vlassis,
2007b; Roth, Simmons, & Veloso, 2007; Goldman, Allen, & Zilberstein, 2007). DecPOMDP model model communication actions regular actions, case
semantics communication actions becomes part optimization problem (Xuan,
Lesser, & Zilberstein, 2001; Goldman & Zilberstein, 2003; Spaan, Gordon, & Vlassis, 2006).
303

fiOliehoek, Spaan & Vlassis

contrast, approaches mentioned typically assume communication happens outside Dec-POMDP model pre-defined semantics. typical assumption
every time step agents communicate individual observations selecting
action. Pynadath Tambe (2002) showed that, assumptions instantaneous
cost-free communication, sharing individual observations way optimal.
3.1 Brute Force Policy Evaluation
exists optimal pure joint policy finite-horizon Dec-POMDP,
theory possible enumerate different pure joint policies, evaluate using equations
(2.10) (2.13) choose best one. number pure joint policies evaluated
is:


n(|O |h 1)
|A | |O |1
,
(3.1)
|A | |O | denote largest individual
action observation sets. cost

evaluating policy |S| |O|h . resulting total cost brute-force policy
evaluation


n(|O |h 1)
nh
|A | |O |1 |S| |O |
,
(3.2)
doubly exponential horizon h.
3.2 Alternating Maximization
Nair et al. (2003b) introduced Joint Equilibrium based Search Policies (JESP).
method guarantees find locally optimal joint policy, specifically, Nash equilibrium: tuple policies agent policy best response
policies employed agents 6=i . relies process refer alternating
maximization. procedure computes policy agent maximizes
joint reward, keeping policies agents fixed. Next, another agent
chosen maximize joint reward finding best-response fixed policies
agents. process repeated joint policy converges Nash equilibrium,
local optimum. main idea fixing agents others improve
policy presented Chades, Scherrer, Charpillet (2002), used
heuristic approach memory-less agents. process alternating maximization
also referred hill-climbing coordinate ascent.
Nair et al. (2003b) describe two variants JESP, first which, Exhaustive-JESP,
implements idea straightforward fashion: Starting random joint
policy, first agent chosen. agent selects best-response policy evaluating
joint reward obtained individual policies agents follow
fixed policy.
second variant, DP-JESP, uses dynamic programming approach compute
best-response policy selected agent i. essence, fixing policies agents
allows reformulation problem augmented POMDP. augmented
POMDP state = hs,~o6=i consists nominal state observation histories
304

fiOptimal Approximate Q-Value Functions Dec-POMDPs

agents ~o6=i . Given fixed deterministic policies agents 6=i ,
augmented state Markovian state, transition observation probabilities
easily derived 6=i .
Like methods proposed Dec-POMDPs, JESP exploits knowledge
initial belief b0 considering reachable beliefs b(s) solution POMDP.
However, cases initial belief might available. demonstrated
Varakantham, Nair, Tambe, Yokoo (2006), JESP extended plan entire
space initial beliefs, overcoming problem.
3.3 MAA
Szer et al. (2005) introduced heuristically guided policy search method called multiagent
A* (MAA ). performs guided A*-like search partially specified joint policies,
pruning joint policies guaranteed worse best (fully specified) joint
policy found far admissible heuristic.
particular MAA considers joint policies partially specified respect
time: partial joint policy = ( 0 , 1 , . . . , t1 ) specifies joint decision rules
first stages. partial joint policy heuristic value Vb (t ) calculated
taking V 0...t1 (t ), actual expected reward achieves first stages,
adding Vb t...h1 , heuristic value remaining h stages. Clearly Vb t...h1
admissible heuristica guaranteed overestimationso Vb (t ).
MAA starts placing completely unspecified joint policy 0 open list.
Then, proceeds selecting partial joint policies = ( 0 , 1 , . . . , t1 ) list
expanding them: generating t+1 = ( 0 , 1 , . . . , t1 , ) appending possible joint
decision rules next time step (t). left side Figure (3) illustrates expansion
process. expansion, created children heuristically valuated placed
open list, partial joint policies t+1 Vb (t+1 ) less expected value V ()
earlier found (fully specified) joint policy , pruned. search ends
list becomes empty, point found optimal fully specified joint policy.
3.4 Dynamic Programming Dec-POMDPs
MAA incrementally builds policies first stage = 0 last = h 1. Prior
work, Hansen et al. (2004) introduced dynamic programming (DP) Dec-POMDPs,
constructs policies way around: starting set 1-step policies
(actions) executed last stage, construct set 2-step policies
executed h 2, etc.
stressed policies maintained quite different used
MAA . particular partial policy MAA form = ( 0 , 1 , . . . , t1 ).
policies maintained DP correspondence decision rules. define
time-to-go stage
= h t.
(3.3)
qi =k denotes k-steps-to-go sub-tree policy agent i. is, qi =k policy
tree form full policy horizon-k problem. Within original
horizon-h problem qi =k candidate execution starting stage = hk. set ksteps-to-go sub-tree policies maintained agent denoted Qi =k . Dynamic programming
305

fiOliehoek, Spaan & Vlassis




















































Figure 3: Difference policy construction MAA (left) dynamic programming
(right) agent actions a,a observations o,o. dashed components newly generated, dotted components result previous iteration.
MAA expands partial policy leaves, dynamic programming
backs set sub-tree policies forming new ones.

Dec-POMDPs based backup operations: constructing Qi =k+1 set sub-tree
policies qi =k+1 set Qi =k . instance, right side Figure 3 shows qi =3 ,
3-steps-to-go sub-tree policy, constructed two qi =2 Qi =2 . Also illustrated
difference process MAA expansion (on left side).
Dynamic programming consecutively constructs Qi =1 ,Qi =2 , . . . ,Qi =h agents i.
However, size set Qi =k+1 given
|Qi =k+1 | = |Ai | |Qi =k ||Oi | ,
result sizes maintained sets grow doubly exponential k. counter
source intractability, Hansen et al. (2004) propose eliminate dominated sub-tree
policies. expected reward particular sub-tree policy qi =k depends probability
states qi =k started (at stage = h k) well probability
=k denote
agents j 6= select sub-tree policies qj =k Qj =k . let q6=


=k
sub-tree profile agents i, Q6=i set profiles, say qi =k
dominated maximizing point multiagent belief space: simplex
=k . Hansen et al. test dominance entire multiagent belief space
Q6=

linear programming. Removal dominated sub-tree policy qi =k agent may cause
sub-tree policy qj =k agent j become dominated. Therefore Hansen et al.
propose iterate agents pruning possible, procedure known
iterated elimination dominated policies (Osborne & Rubinstein, 1994).
Finally, last backup step completed optimal policy found
evaluating joint policies Q1 =h Qn =h initial belief b0 .
306

fiOptimal Approximate Q-Value Functions Dec-POMDPs

3.5 Extensions DP Dec-POMDPs
last years several extensions dynamic programming algorithm DecPOMDPs proposed. first extensions due Szer Charpillet
(2006). Rather testing dominance entire multiagent belief space, Szer
Charpillet propose perform point-based dynamic programming (PBDP). order
prune set sub-tree policies Qi =k , set belief points Bi,reachable
=k ) possibly reached deterministic joint policies generated.
P(S Q6=

sub-tree policies qi =k maximize value bi Bi,reachable kept.
proposed algorithm optimal, intractable needs generate
multiagent belief points reachable joint policies. overcome
bottleneck, Szer Charpillet propose randomly sample one joint policies
use generate Bi,reachable .
Seuken Zilberstein (2007b) also proposed point-based extension DP algorithm, called memory-bounded dynamic programming (MBDP). Rather using
randomly selected policy generate belief points, propose use heuristic policies. important difference, however, lies pruning step. Rather pruning
dominated sub-tree policies qi =k , MBDP prunes sub-tree policies except
iteration. specifically, agent maxTrees sub-tree policies retained,
parameter planning method. result, MBDP linear space time
complexity respect horizon. MBDP algorithm still depends exhaus|Oi | sub-tree policies.
tive generation sets Qi =k+1 contain
n |Ai | maxT rees
|O
|
Moreover, iteration |A | maxT rees
joint sub-tree policies evaluated sampled belief points. counter growth, Seuken Zilberstein
(2007a) proposed extension limits considered observations backup
step maxObs likely observations.
Finally, extension DP Dec-POMDPs algorithm given Amato,
Carlin, Zilberstein (2007b). approach, bounded DP (BDP), establishes bound
used memory, quality approximation. particular, BDP uses
-pruning iteration. is, qi =k maximizing region
multiagent belief space, improves value region , also pruned.
iterated elimination using - pruning still lead unbounded reduction
value, Amato et al. propose perform one iteration -pruning, followed iterated
elimination using normal pruning.
3.6 Approaches Finite-Horizon Dec-POMDPs
approaches finite-horizon Dec-POMDPs, briefly
describe here. Aras, Dutech, Charpillet (2007) proposed mixed integer linear programming formulation optimal solution finite-horizon Dec-POMDPs. approach based representing set possible policies agent sequence form
(Romanovskii, 1962; Koller, Megiddo, & von Stengel, 1994; Koller & Pfeffer, 1997). sequence form, single policy agent represented subset set sequences
(roughly corresponding action-observation histories) agent. problem
interpreted combinatorial optimization problem, Aras et al. propose
solve mixed integer linear program.
307

fiOliehoek, Spaan & Vlassis

Oliehoek, Kooij, Vlassis (2007a) also recognize finding solution DecPOMDPs essence combinatorial optimization problem propose apply
Cross-Entropy method (de Boer, Kroese, Mannor, & Rubinstein, 2005), method combinatorial optimization recently become popular ability find
near-optimal solutions large optimization problems. resulting algorithm performs
sampling-based policy search approximately solving Dec-POMDPs. operates sampling pure policies appropriately parameterized stochastic policy, evaluates
policies either exactly approximately order define next stochastic policy
sample from, convergence.
Finally, Emery-Montemerlo et al. (2004, 2005) proposed approximate Dec-POMDPs
series Bayesian games. Since work article based
representation, defer detailed explanation next section. mention
Emery-Montemerlo et al. assume algorithm run on-line (interleaving
planning execution), assumption necessary. Rather apply
framework off-line planning phase, like algorithms covered
overview.

4. Optimal Q-value Functions
section show Dec-POMDP modeled series Bayesian
games (BGs). BG game-theoretic model deal uncertainty (Osborne
& Rubinstein, 1994). Bayesian games similar well-known normal form,
matrix games, allow model agents private information. section
introduce Bayesian games show Dec-POMDP modeled series
Bayesian games (BGs). idea using series BGs find policies DecPOMDP proposed approximate setting Emery-Montemerlo et al. (2004).
particular, showed using series BGs approximate payoff function,
able obtain approximate solutions Dec-Tiger problem, comparable
results JESP (see Section 3.2).
main result section optimal Dec-POMDP policy computed
solution sequence Bayesian games, payoff function games
coincides Q-value function optimal policy , i.e., optimal Qvalue function Q . Thus, extend results Emery-Montemerlo et al. (2004)
include optimal setting. Also, conjecture form Q computed
without already knowing optimal policy . transferring game-theoretic concept
sequential rationality Dec-POMDPs, find description Q computable
without knowing front.
4.1 Game-Theoretic Background
explain Dec-POMDPs modeled using Bayesian games,
first introduce together necessary game theoretic background.
308

fiOptimal Approximate Q-Value Functions Dec-POMDPs


C


1, 1
0, + 2

C
+2,0
+1, + 1


B


+2
0

B
0
+2

Figure 4: Left: game Chicken. players option (D)rive (C)hicken
out. Right: meeting location problem. game identical
payoffs, entry contains one number.

4.1.1 Strategic Form Games Nash Equilibria
basis concept Bayesian game lies simpler form game: strategic-
normal form game. strategic game consists set agents players,
set actions (or strategies). combination selected actions specifies particular
outcome. strategic game consists two agents, visualized matrix
shown Figure 4. first game shown called Chicken involves two teenagers
driving head on. option drive chicken out. teenagers
payoff maximal (+2) drives opponent chickens out. However,
drive on, collision follows giving payoff 1. second game meeting
location problem. agents want meet location B. preference
location, long pick location. game fully cooperative,
modeled fact agents receive identical payoffs.
Definition 4.1. Formally, strategic game tuple hn,A,ui, n number
agents, = Ai set joint actions, u = hu1 , . . . ,un ui : R
payoff function agent i.
Game theory tries specify agent play. is, game-theoretic
solution suggest policy agent. strategic game write denote
policy agent joint policy. policy agent simply one actions
= ai Ai (i.e., pure policy), probability distribution actions P(Ai )
(i.e., mixed policy). Also, policy suggested agent rational given
policies suggested agent; would undesirable suggest particular
policy agent, get better payoff switching another policy. Rather,
suggested policies form equilibrium, meaning profitable agent
unilaterally deviate suggested policy. notion formalized concept
Nash equilibrium.
Definition 4.2. pure policy profile = h1 , . . . ,i , . . . ,n specifying pure policy
agent Nash Equilibrium (NE)
ff


(4.1)
ui (h1 , . . . ,i , . . . ,n i) ui ( 1 , . . . ,i , . . . ,n ), i:1in , Ai .
definition easily extended incorporate mixed policies defining
ui (h1 , . . . ,n i) =

X

ui (ha1 , . . . ,an i)

ha1 ,...,,an

309

n

i=1

Pi (ai ).

fiOliehoek, Spaan & Vlassis

Nash (1950) proved allowing mixed policies, every (finite) strategic game contains
least one NE, making proper solution game. However, unclear
NE found. particular, may multiple NEs game, making unclear
one select. order make discrimination Nash equilibria,
consider NEs NE better everyone.
Definition 4.3. Nash Equilibrium = h1 , . . . ,i , . . . ,n referred Pareto Optimal (PO) NE specifies least payoff agents
higher payoff least one agent:



ui ( ) ui () ui ( ) > ui () .

case multiple Pareto optimal Nash equilibria exist, agents agree
beforehand particular ordering, ensure NE chosen.
4.1.2 Bayesian Games
Bayesian game (Osborne & Rubinstein, 1994) augmented normal form game
players hold private information. private information defines type
agent, i.e., particular type agent corresponds agent knowing
particular information. payoff agents receive longer depends
actions, also private information. Formally, BG defined follows:
Definition 4.4. Bayesian game (BG) tuple hn,A,,P (), hu1 ,...un ii, n
number agents, set joint actions, = set joint types
probability function P () specified, ui : R payoff function
agent i.
normal form game agents select action. Now, BG agents
condition action private information. means BGs agents
use different type policies. BG, denote joint policy = h1 ,...,n i,
individual policies mappings types actions: : Ai . case
identical payoffs agents, solution BG given following theorem:
Theorem 4.1. BG identical payoffs, i.e., i,j ui (,a) = uj (,a), solution
given by:
X
= arg max
P ()u(,()),
(4.2)




() = h1 (1 ),...,n (n )i joint action specified joint type .
solution constitutes Pareto optimal Nash equilibrium.
Proof. proof consists two parts: first shows Nash equilibrium,
second shows Pareto optimal.
Nash equilibrium proof. clear satisfying 4.2 Nash equilibrium
rewriting perspective arbitrary agent follows:
310

fiOptimal Approximate Q-Value Functions Dec-POMDPs



"

= arg max max


"

6=i

= arg max max


"

6=i

= arg max max


= arg max


6=i

X

X

#

P ()u(,()) ,



XX
X

P (i )



P (i )

X
6=i



P (6=i |i )

6=i



X

"
|

X
6=i

#

#

P (hi ,6=i i) u(,()) ,
{z

P (i )

#

}

P (6=i |i )u(,()) ,

6=i



ff

P (6=i |i )u(hi ,6=i , (i ),6=
(6=i ) ),

. Since special assumptions made
means best response 6=

i, follows Nash equilibrium.

Pareto optimality proof. Let us write Vi (ai ,6=i ) payoff agent expects
performing ai agents use policy profile 6=i .
Vi (ai ,6=i ) =

X

P (6=i |i )u(hi ,6=i , hai ,6=i (6=i )i).

6=i

Now, joint policy satisfying (4.2) Pareto optimal another
Nash equilibrium attains least payoff agents types
strictly least one agent type. Formally Pareto optimal
that:









Vi (i (i ),6=
) Vi (i (i ),6=i ) Vi (i (i ),6=i ) < Vi (i (i ),6=i ). (4.3)


prove exist contradiction. Suppose = hi ,6=

NE (4.3) holds (and thus Pareto optimal). satisfies (4.2)
know that:
X
X
P ()u(, ()),
(4.4)
P ()u(, ())




therefore, agents




P (i,1 )Vi,1 (i (i,1 ),6=
) + ... + P (i,|i | )Vi,|i | (i (i,|i | ),6=i )



P (i,1 )Vi,1 (i (i,1 ),6=
) + ... + P (i,|i | )Vi,|i | (i (i,|i | ),6=i )

holds. However, assumption satisfies (4.3) get
j




Vi,j (i (i,j ),6=
) < Vi,j (i (i,j ),6=i ).

Therefore must
X
X


P (i,k )Vi,k (i (i,k ),6=
P (i,k )Vi,k (i (i,k ),6=
),
i) >
k6=j

k6=j

311

fiOliehoek, Spaan & Vlassis

t=0

joint actions
joint observations
joint act.-obs. history

ha1 ,a2

ha1 ,a2
ha1 ,a2
t=1
ho1 ,o2

ha1 ,a2
ho1 ,o2

ho1 ,o2
ho1 ,o2

Figure 5: Dec-POMDP seen tree joint actions observations.
indicated planes correspond Bayesian games first two stages.

thus
k




Vi,k (i (i,k ),6=
) > Vi,k (i (i,k ),6=i ),

contradicting assumption satisfies (4.3).
4.2 Modeling Dec-POMDPs Series Bayesian Games
discuss Bayesian games used model Dec-POMDPs. Essentially,
Dec-POMDP seen tree nodes joint action-observation histories
edges represent joint actions observations, illustrated Figure 5. specific
stage Dec-POMDP, main difficulty coordinating action selection presented
fact agent individual action-observation history. is,
global signal agents use coordinate actions. situation
conveniently modeled Bayesian game discuss.
time step t, one directly associate primitives Dec-POMDP
BG identical payoffs: actions agents cases,
~ . Figure 6 shows
types agent correspond action-observation histories

Bayesian games = 0 = 1 fictitious Dec-POMDP 2 agents.
denote payoff function BG models stage Dec-POMDP
~
Q( ,a). payoff function naturally defined accordance value
function planning task. instance, Emery-Montemerlo et al. (2004) define Q(~ ,a)
312

fiOptimal Approximate Q-Value Functions Dec-POMDPs

QMDP -value underlying MDP. extensively discuss payoff
function Section 4.3.
probability P () equal probability joint action-observation history
corresponds depends past joint policy = ( 0 , . . . , t1 )
initial state distribution. calculated marginal (2.7):
P () = P (~ |t ,b0 ) =

X

P (st ,~ |t ,b0 ).

(4.5)

st

~
considering pure joint policies , action probability component P (a|)
(2.7) 1 joint action-observation histories ~ consistent past joint
policy 0 otherwise. say action-observation ~i history consistent
pure policy occur executing , i.e., actions ~i would
selected . Let us formally define consistency follows.

Definition 4.5 (Consistency). Let us write ~it restriction ~it stage 0, . . . ,t
(with 0 < t). action-observation history ~it agent consistent pure
policy time step 0 <



(~it ) = (~oit ) = ati

(t + 1)-th action ~it . joint action-observation history ~ = h~1t , . . . ,~nt consistent pure joint policy = h1 , . . . ,n individual ~it consistent
corresponding individual policy . C indicator function consistency. instance
C(~ ,) filters action-observation histories ~ inconsistent joint
pure policy :
C(~ ,) =

(

1 , ~ = o0 ,(o0 ),o1 ,(o0 ,o1 ),... )
0 , otherwise.

(4.6)

~ {~ | C(~ ,) = 1} set ~ consistent .
also write
definition allows us write
P (~ |t ,b0 ) = C(~ ,t )

X

P (st ,~ |b0 )

(4.7)

st


P (st ,~ |b0 ) =

X

P (ot |at1 ,st )P (st |st1 ,at1 )P (st1 ,~ t1 |b0 ).

(4.8)

st1

Figure 6 illustrates indicator function filters policies, t=0 (~ t=0 ) =
ha1 ,a2 i, non-shaded part BG = 1 reached (has positive probability).
313

fiOliehoek, Spaan & Vlassis

~2t=0
~1t=0
()
~2t=1
~1t=1
(a1 ,o1 )
(a1 ,o1 )
(a1 ,o1 )
(a1 ,o1 )

a1
a1
a1
a1
a1
a1
...

a1
a1

()
a2
a2
+2.75 4.1
0.9
+0.3

(a2 ,o2 )
a2
a2
0.3
+0.6
0.6
+2.0
+3.1
+4.4
+1.1
2.9
0.4
0.9
0.9
4.5
...
...

(a2 ,o2 )
a2
a2
0.6
+4.0
1.3
+3.6
1.9
+1.0
+2.0
0.4
0.5
1.0
1.0
+3.5
...
...

...
...
...
...
...
...
...

Figure 6: Bayesian game first second time step (top: = 0, bottom: = 1).
entries ~ , given payoff function Q(~ ,at ). Light shaded entries
indicate solutions. Dark entries realized given ha1 ,a2 solution
BG = 0.

4.3 Q-value Function Optimal Joint Policy
Given perspective Dec-POMDP interpreted series BGs outlined
previous section, solution BG stage joint decision rule . payoff
function BG chosen well, quality high. Emery-Montemerlo
et al. (2004) try find good joint policy = ( 0 , . . . , h1 ) procedure refer
forward-sweep policy computation (FSPC): one sweep forward time, BG
stage = 0,1, . . . ,h 1 consecutively solved. such, payoff function
BGs constitute call Q-value function Dec-POMDP.
Here, show optimal Q-value function Q : using Q
payoff functions BGs, forward-sweep policy computation lead optimal
joint policy = ( 0, , . . . , h1, ). first give derivation Q . Next,
discuss Q indeed used calculate , computing Q seems impractical
without already knowing optimal joint policy . issue addressed
Section 4.4.
4.3.1 Existence Q
state theorem identifying normative description Q Q-value function
optimal joint policy.
Theorem 4.2. expected cumulative reward stages t, . . . ,h 1 induced ,
optimal joint policy Dec-POMDP, given by:
V ( ) =

X

P (~ |b0 )Q (~ , (~ )),

~t
~


314

(4.9)

fiOptimal Approximate Q-Value Functions Dec-POMDPs



ff
~ = ~o ,~a , (~ ) = (~o ) denotes joint action pure joint policy
specifies ~o ,
X
Q (~ ,a) = R(~ ,a) +
P (ot+1 |~ ,a)Q (~ t+1 , (~ t+1 ))
(4.10)
ot+1

Q-value function , gives expected cumulative future reward
taking joint action ~ given optimal joint policy followed hereafter.
Proof. filling (2.11) optimal pure joint
policy , obtain expected
fi

cumulative reward summation E R(st ,at )fi expected rewards yields
time step:


V ( ) =

h1
X
t=0

h1 X
fi X

fi
P (~ | ,b0 )R(~ , (~ )).
E R(s ,a ) =

(4.11)

t=0 ~
~t

equation, P (~ | ,b0 ) given (4.7). result, influence P (~ | ,b0 )
C. I.e., used filter inconsistent histories. Therefore
write:
X
fi

P (~ |b0 )R(~ , (~ )),
(4.12)
E R(st ,at )fi =
~t
~


P (~ |b0 ) given directly taking marginal (4.8). Now, let us define
value starting time step t:

X
fi

P (~ |b0 )R(~ , (~ )) + V t+1 ( ). (4.13)
V ( ) = E R(st ,at )fi + V t+1 ( ) =
~t
~


last time step h 1 expected future reward, get:
X
P (~ h1 |b0 ) R(~ h1 , (~ h1 )) .
V h1 ( ) =
|
{z
}
h1
~
~ h1


(4.14)

Q (~ h1 , (~ h1 ))

time step h 2 becomes:

h
fi
V h2 ( ) E R(sh2 ,ah2 )fi + V h1 ( ) =
X
X
P (~ h2 |b0 )R(~ h2 , (~ h2 )) +

P (~ h1 |b0 )Q (~ h1 , (~ h1 )).

~ h1
~ h1


~ h2
~ h2


(4.15)
P (~ h1 ) = P (~ h2 )P (oh1 |~ h2 , (~ h2 )), (4.15) rewritten to:
V h2 ( ) =

X

P (~ h2 |b0 )Q (~ h2 , (~ h2 )),

~ h2
~ h2


315

(4.16)

fiOliehoek, Spaan & Vlassis


Q (~ h2 , (~ h2 )) = R(~ h2 , (~ h2 ))+
X
P (oh1 |~ h2 , (~ h2 ))Q (~ h1 , (~ h1 )). (4.17)
oh1

Reasoning way see (4.9) (4.10) constitute generic expression
expected cumulative future reward starting time step t.
Note derivation, explicitly included b0 one given arguments.
rest text, always assume b0 given therefore omit it, unless
necessary.
4.3.2 Deriving Optimal Joint Policy Q
point derived Q , Q-value function optimal joint policy. Now,
extend results Emery-Montemerlo et al. (2004) exact setting:
Theorem 4.3. Applying forward-sweep policy computation using Q defined (4.10)
yields optimal joint policy.
Proof. Note that, per definition, optimal Dec-POMDP policy maximizes expected
future reward V ( ) specified (4.9). Therefore t, , optimal decision rule stage t,
identical optimal joint policy t, Bayesian game time step t, payoff
function BG given Q , is:
X

t, t, = arg max


P (~ )Q (~ , (~ )).

(4.18)

~t
~


Equation (4.18) tells us t, t, . means possible construct
complete optimal Dec-POMDP policy = ( 0, , . . . , h1, ), computing t, t.
subtlety calculation (4.18) dependent optimal joint
~ {~ | C(~ , ) = 1}. resolved
policy, summation ~

realizing past actions influence action-observation histories
reached time step t. Formally, let = ( 0, , . . . , t1, ) denote past joint policy,
partial joint policy specified stages 0,...,t 1. denote optimal past joint
~t =
~ t, , therefore that:
policy t, ,


t, = arg max


X

P (~ )Q (~ , (~ )).

(4.19)

~t
~
t,


solved forward manner time steps = 0,1,2,...,h 1, every
time step t, = ( 0, , . . . , t1, ) available: specified ( 0, ,..., t1, )
solutions previously solved BGs.
316

fiOptimal Approximate Q-Value Functions Dec-POMDPs

4.3.3 Computing Q
far discussed Q used find optimal joint policy . Unfortunately,
optimal joint policy known, computing Q impractical,
discuss here. contrast (fully observable) single-agent case
optimal Q-values found relatively easily single sweep backward time.
MDPs POMDPs compute Q-values time step
+ 1 applying backup operator. possible single agent
perceives Markovian signal. allows agent (1) select optimal action (policy)
next time step (2) determine expected future reward given optimal
action (policy) found step 1. instance, backup operator POMDP given
by:
X
Q (bt ,a) = R(bt ,a) +
P (o|bt ,a) max Q (bt+1 ,a),




rewritten 2-step procedure:

1. t+1, (bt+1 ) = arg maxa Q (bt+1 ,a )
P
2. Q (bt ,a) = R(bt ,a) + P (o|bt ,a)Q (bt+1 , t+1, (bt+1 )).

case Dec-POMDPs, step 2 would correspond calculating Q using (4.10)
thus depends t+1, optimal joint policy next stage. However, step 1
calculates t+1, , corresponds (4.19) therefore dependent t+1, (an optimal
joint policy time steps 0,...,t). calculate Qt, optimal Q-value function
specified (4.10) stage t, optimal joint policy including stage
needed. Effectively, dependence future past optimal policy,
rather future optimal policies single agent case. clear
solution seems evaluation possible past policies, detailed next. conjecture
problem encountered inherent decentralized decision making
imperfect information. example, also observe exact point-based dynamic
programming Dec-POMDPs, described Section 3.5, necessary
generate (multiagent belief points generated all) possible past policies.
4.4 Sequential Rationality Dec-POMDPs
conjectured computing Q introduced Section 4.3 seems impractical without
knowing . relate concepts game theory. particular,
discuss different formulation Q based principle sequential rationality, i.e.,
also considering joint action-observation histories realized given optimal
joint policy. formulation Q computable without knowing optimal joint policy
advance, present dynamic programming algorithm perform computation.
4.4.1 Sub-game Perfect Sequential Equilibria
problem facing much related notion sub-game perfect equilibria
game theory. sub-game perfect Nash equilibrium = h1 , . . . ,n characteristic contained policies specify optimal action possible situationseven
317

fiOliehoek, Spaan & Vlassis

situations occur following . commonly given rationale behind
concept that, mistake one agents execution, situations
occur according , occur, also situations agents act optimally. different rationale given Binmore (1992), remarks tempting
shrug ones shoulders difficulties [because] rational players stray
equilibrium path, would clearly mistake, agents remain
equilibrium path anticipate would happen
deviate. implies agents decide upon Nash equilibrium analyzing
expected outcome would following policies: is, acting optimally
situations. perform similar reasoning Dec-POMDPs,
similar fashionwill result description allows deduce optimal Q-value
function thus joint policy.
Dec-POMDP modeled extensive form game imperfect information
(Oliehoek & Vlassis, 2006). games, notion sub-game perfect equilibria
inadequate; type games often contain proper sub-games, every Nash
equilibrium trivially sub-game perfect.4 overcome problem different refinements
Nash equilibrium concept defined, mention assessment
equilibrium (Binmore, 1992) closely related, stronger sequential equilibrium
(Osborne & Rubinstein, 1994). equilibria based concept assessment, pair h,bi consisting joint policy belief system b. belief
system maps possible situation, information set, agentalso ones
reachable given probability distribution possible joint histories. Roughly
speaking, assessment equilibrium requires sequential rationality belief consistency.5
former entails joint policy specifies optimal actions information
set given b. Belief consistency means beliefs assigned b Bayes
rational given specified joint policy . instance, context Dec-POMDPs b
would prescribe, particular ~it agent i, belief joint histories P (~ |~it ).
beliefs prescribed belief system b Bayes-rational (i.e., computed appropriate
conditionals (4.5)), b called belief consistent.6
4.4.2 Sequential Rationality Optimal Q-value Function
dependence sequential rationality belief system b indicates optimal
action particular point dependent probability distribution histories.
Section 4.3.3 encountered similar dependence history specified t+1, .
make dependence exact.
particular stage t, policy optimal or, game-theoretic terms, rational
maximizes expected return point on. Section 4.3.1, able
express expected return Q (~ ,a) assuming optimal joint policy followed
4. extensive form Dec-POMDP indeed contain proper sub-games, agent
never discriminate agents observations.
5. Osborne Rubinstein (1994) refer second requirement simply consistency. order
avoid confusion definition 4.5 use term belief consistency.
6. sequential equilibrium includes technical part definition belief consistency addresses beliefs held information sets reached according .
information refer Osborne Rubinstein (1994).

318

fiOptimal Approximate Q-Value Functions Dec-POMDPs

current stage t. However, previous policy assumed, maximal
expected return defined.
Proposition 4.1. pair (~ ,at ) < h 1 optimal value Q (~ ,at ) cannot


defined without assuming (possibly randomized) past policy t+1 = 0 , . . . , .
last stage = h 1 expected reward defined
Q (~ h1 ,ah1 ) R(~ h1 ,ah1 )
without assuming past policy.
Proof. Let us try deduce Q (~ ,at ) optimal value particular ~ assuming
Q -values next time step + 1 known. Q (~ ,at )-values
possible joint actions evaluated follows


Q (~ ,at ) = R(~ ,at ) +

X

P (ot+1 |~ ,at )Q (~ t+1 , t+1, (~ t+1 )).

ot+1

t+1, optimal decision rule next stage. t+1, be?
assume stage + 1 followed particular (possibly randomized) t+1 ,
t+1, = arg max
t+1

X

P (~ t+1 |t+1 ,b0 )Q (~ t+1 , t+1 (~ t+1 )).

~ t+1
~ t+1

optimal. However, many pure infinite randomized past policies t+1
consistent ~ ,at , leading many t+1, might optimal. conclusion
draw Q (~ ,at ) ill-defined without P (~ t+1 |t+1 ,b0 ), probability distribution
(belief) joint action-observation histories, induced t+1 , policy followed
stages 0, . . . ,t.
Let us illustrate reviewing optimal Q-value function defined Section 4.3.1. Consider (~ t+1 ) (4.10). optimal policy mapping observation
~ induced individual policies observation histories.
histories actions :
means two joint action-observation histories joint observation
history results joint action. ~a,~o,~a (h~a,~oi) = (h~a ,~oi). Effectively
~ , say mistake7 , continues
means reach ~ 6

specify actions mistake ever happened: is, still assuming
~ .
followed stage t. fact, (~ ) might even optimal ~ 6


t1

~
~
turn means Q ( ,a), Q-values predecessors , might optimal
expected reward.
demonstrated optimal Q-value function Dec-POMDP welldefined without assuming past joint policy. propose new definition Q
explicitly incorporates t+1 .
7. question mistake one agent detected another agent different
matter altogether beyond scope text.

319

fiOliehoek, Spaan & Vlassis

t=0

21

a1
o1

t=1

t=2

a2
o2

o1

a1

22

a2

a1

2

o2
a2

o1

o1

o1

o1

o2

o2

o2

o2

a1

a1

a1

a1

a2

a2

a2

a2

2,

Figure 7: Computation sequential rational Q . 2, optimal decision rule stage
= 2, given 2 followed first two stages. Q (~ 1 ,2 ) entries
computed propagating relevant Q -values next stage. instance,
highlighted joint history ~ 1 = h(a1 ,o1 ),(a2 ,o2 )i, Q -value 2
computed propagating values four successor joint histories, per
(4.20).

Theorem 4.4 (Sequentially rational Q ). optimal Q-value function properly defined
function joint action-observation histories past joint policies, Q (~ ,t+1 ).
Q specifies optimal value given (~ ,t+1 ), even ~ reached
execution optimal policy , therefore referred sequentially rational.
Proof. ~ ,t+1 , optimal expected return given

t=h1
R(~ ,t+1 (~ )),
X
~ t+1
t+1 ~ t+1 ~
~ t+1 t+2,
Q ( , ) = R(~ ,t+1 (~ )) +
P (o | , ( ))Q ( ,
), 0 < h 1

ot+1



t+2,

=



t+1 ,t+1,



(4.20)



t+1, = arg max
t+1

X

~ t+1
~ t+1


P (~ t+1 |t+1 ,b0 )Q (~ t+1 , t+1 , t+1 ).

(4.21)

well-defined.
equations constitute dynamic program. assuming pure
joint past policies used, (4.21) transforms
X

(4.22)
P (~ t+1 )Q (~ t+1 , t+1 , t+1 )
t+1, = arg max
t+1

~ t+1
~ t+1


320

fiOptimal Approximate Q-Value Functions Dec-POMDPs

~
(,)
~ consistent dynamic program evaluated
end (t = h 1) begin (t = 0). Figure 7 illustrates computation Q .
arriving stage 0, 1 reduce joint actions possible select
0, = arg max Q (~ ,a) = arg max Q (~ 0 ,1 ).


1

given 1 = 0, determine 1, = 1, using (4.22), etc. essentially
forward-sweep policy computation using optimal Q-value function Q (~ ,t+1 )
defined (4.20).
computation Q also closely related point-based dynamic programming
Dec-POMDPs discussed section 3.5. Suppose = 2 Figure 7 last stage
(i.e., h = 3). 2, 2 computed, easy construct sets
non-dominated action agent: every action ai agent specified i2,
non-dominated. computed values (~ 1 ,2 ) = 1, 2
associated optimal future policy 2, . means individual history ~i1
associated sub-tree policy qi =2 2 (~ 1 ,2 )-pair associated
joint sub-tree policy (e.g, shaded trees Figure 7). Clearly, Q (~ 1 ,2 ) corresponds
expected value associated joint sub-tree policy. Rather keeping track
sub-trees policies, however, algorithm presented keeps track values.
advantage description Q using (4.21) rather (4.10) twofold. First
description treated describes way actually compute values
used construct , latter gives normative description needs
order compute Q-values.
Second, Q (~ ,t+1 ) describes sequential rationality Dec-POMDPs.
past policy (and corresponding consistent belief system) optimal future policy
computed. variation might even applied on-line. Suppose agent makes
mistake stage t, executing action prescribed , assuming agents
execute policy 6=i without mistakes, agent knows actually executed previous
policy t+1 . Therefore compute new individual policy


E
X
t+1,
t+1
P (~ t+1 )Q (~ t+1 , t+1 , t+1 ,6=
).
i,
t+1 = arg max

t+1

~ t+1
~ t+1


4.4.3 Complexity Computing Sequentially Rational Q
Although found way compute Q , computation intractable
P
|Oi |t 1

smallest problems, show. stage t1 t1
=0 |Oi | = |Oi |1
observation histories agent i, leading
|A |

(

n |O |t 1
|O |1

)

~ | = |O|t1 consistent joint actionpure joint past policies . |O
observation histories (for observation history ~o t1 , specifies actions forming
~ t1 ). means stage h 2 (for h 1, Q-values easily calculated),
321

fiOliehoek, Spaan & Vlassis

number entries computed number joint past policies h1 times number
joint histories
!
n(|O |h1 1)
|A | |O |1
|O|h2 ,
indicating computation function doubly exponential, brute force policy
evaluation. Also, joint past policy h1 , need compute h = (h1 ,h1, )
solving next-stage BG:


X
P (~ h1 )Q (~ h1 , h1 , h1 ).
h1, = arg max
h1

h1
~
~ h1

authors knowledge, method optimally solve BGs evaluation



h1
|A |n|O |

joint BG-polices, also doubly exponential horizon.

5. Approximate Q-value Functions
indicated previous section, although optimal Q-value function Q exists,
costly compute thus impractical. section, review Q-value
b used approximation Q . discuss underlying
functions, Q,
assumptions, computation, computational complexity properties, thereby providing taxonomy approximate Q-value functions Dec-POMDPs. particular
treat two well-known approximate Q-value functions, QMDP QPOMDP , QBG
recently introduced Oliehoek Vlassis (2007).
5.1 QMDP
QMDP originally proposed approximately solve POMDPs Littman, Cassandra,
Kaelbling (1995), also applied Dec-POMDPs (Emery-Montemerlo
et al., 2004; Szer et al., 2005). idea Q approximated using stateaction values QM (s,a) found solving underlying MDP Dec-POMDP.
underlying MDP horizon-h MDP defined single agent takes joint actions
observes nominal state transition model reward
model R original Dec-POMDP. Solving underlying MDP efficiently done
using dynamic programming techniques (Puterman, 1994), resulting optimal nonstationary MDP Q-value function:
X


(st+1 ,a).
(5.1)
P (st+1 |st ,a) max Qt+1,
Qt,

(s ,a) = R(s ,a) +


st+1

t+1,
equation, maximization implicit selection
, optimal MDP policy
next time step, explained Section 4.3.3. Note Qt,
also optimal Qvalue function, MDP setting. article Q always denote optimal

value function (original) Dec-POMDP. order transform Qt,
(s ,a)-values
b (~ ,a)-values used original Dec-POMDP, compute:
approximate Q

322

fiOptimal Approximate Q-Value Functions Dec-POMDPs

b (~ ,a) =
Q

X

~t
Qt,
(s,a)P (s| ),

(5.2)

sS

P (s|~ ) computed (4.8). Combining (5.1) (5.2) making
t+1,
selection
explicit get:
b (~ ,a) = R(~ ,a) +
Q

X

P (st+1 |~ ,a)

st+1

max

t+1 t+1

(s
)

t+1 t+1
Qt+1,
(st+1 ,M
(s )),


(5.3)

defines approximate Q-value function used payoff function
b consistent established definition
various BGs Dec-POMDP. Note Q
Q-value functions since defined expected immediate reward performing
(joint) action plus value following optimal joint policy (in case optimal
MDP-policy) thereafter.
calculation QtM (s,a)-values dynamic programming (which cost
O(|S|h) performed separate phase, cost computation QMDP
dependent cost evaluation (5.3), O(|S|). want evaluate
P
(|A||O|)h 1

QMDP h1
t=0 (|A| |O|) = (|A||O|)1 joint action-observation histories is, total
computational cost becomes:
!
(|A| |O|)h 1

|A||S| .
(5.4)
(|A| |O|) 1
However, applying QMDP forward-sweep policy computation,
consider action-observation histories, consistent policy
found earlier stages. Effectively evaluate (5.3) observation histories
joint actions, leading to:
!
(|O|)h 1
|A||S| .
(5.5)

(|O|) 1
used context Dec-POMDPs, QMDP solutions known undervalue
actions gain information (Fernandez, Sanz, Simmons, & Dieguez, 2006). explained realizing QMDP solution assumes state fully observable
next time step. Therefore actions provide information state,
thus lead high future reward (but might low immediate reward),
undervalued. applying QMDP Dec-POMDP setting, effect also
expected. Another consequence simplifying assumption QMDP -value function upper bound optimal value function used approximate POMDP
(Hauskrecht, 2000), consequence also upper bound optimal value function
Dec-POMDP. intuitively clear, Dec-POMDP POMDP
additional difficulty decentralization. formal argument presented Section 5.4.
5.2 QPOMDP
Similar underlying MDP, one define underlying POMDP Dec-POMDP
POMDP , R, single agent
323

fiOliehoek, Spaan & Vlassis

takes joint actions receives joint observations O. QPOMDP approximates Q
using solution underlying POMDP (Szer et al., 2005; Roth et al., 2005).
particular, optimal QPOMDP value function underlying POMDP satisfies:
~t

~t

QP (b ,a) = R(b ,a) +

X

~t

P (ot+1 |b ,a)

ot+1

max

t+1
t+1
P
(b~
)

~ t+1

QP (b

~ t+1

,Pt+1 (b

)),

(5.6)

~t

b joint belief single agent selects joint actions receives joint
observations time step t,
X
~t
~t
R(b ,a) =
R(s,a)b (s)
(5.7)
sS

~ t+1

~t

immediate reward, b
joint belief resulting b action
t+1
joint observation , calculated
P
~t
P (o|a,s ) sS P (s |s,a)b (s)
~ t+1
b
(s ) = P
.
(5.8)
P

~

sS P (s |s,a)b (s)
P (o|a,s )

~t
~ one joint belief b , corresponds P (s|~ ) derived
(4.8). Therefore possible directly use computed QPOMDP values payoffs
BGs Dec-POMDP, is, define:

b P (~ ,a) QP (b~ ,a).
Q

(5.9)

maximization (5.6) stated explicit form: maximization time step
+ 1 POMDP policies. However, clear maximization effectively one
joint actions, conditional received joint observation ot+1 thus
~ t+1
resulting belief b .
finite horizon, QP computed generating possible joint beliefs
solving belief MDP. Generating possible beliefs easy: starting b0 corresponding empty joint action-observation history ~ t=0 , calculate
~1
resulting ~ t=1 corresponding belief b continue recursively. Solving belief
MDP amounts recursively applying (5.6).
computation QMDP could restrict attention (~ ,a)-pairs
b (~ ,a)-values
specified forward-sweep policy computation, Q
t+1
~
b
depend values successor-histories QM ( ,a). QPOMDP , however,
dependence, meaning necessary evaluate ~ ,a. particular,
cost calculating QPOMDP divided cost calculating expected
immediate reward ~ ,a, cost evaluating future reward ~ ,a,
= 0,...,h 2. former operation given (5.7) cost O(|S|) per ~ ,a
thus total cost equal (5.4). latter requires selecting maximizing joint action
joint observation ~ ,a = 0,...,h 2, leading
!
(|A| |O|)h1 1

|A| (|A| |O|) .
(5.10)
(|A| |O|) 1
324

fiOptimal Approximate Q-Value Functions Dec-POMDPs

~2t=1
~1t=1
~2t=0
~1t=0
()

a1
a1

()
a2
+3.1
0.9

(a1 ,o1 )
a2
4.1
+0.3

(a1 ,o1 )
(a1 ,o1 )
(a1 ,o1 )

a1
a1
a1
a1
a1
a1
...

(a2 ,o2 )
a2
a2
0.3
+0.6
0.6
+2.0
+3.1
+4.4
+1.1
2.9
0.4
0.9
0.9
4.5
...
...

(a2 ,o2 )
a2
a2
0.6
+4.0
1.3
+3.6
1.9
+1.0
+2.0
0.4
0.5
1.0
1.0
+3.5
...
...

...
...
...
...
...
...
...

Figure 8: Backward calculation QPOMDP -values. Note solutions (the highlighted
entries) different Figure 6: QPOMDP assumes actions
conditioned joint action-observation history. highlighted +3.1
entry Bayesian game = 0 calculated expected immediate
reward (= 0) plus weighted sum maximizing entry (joint action) per
next joint observation history. assuming uniform distribution joint
observations given ha1 ,a2 future reward given by: +3.1 = 0 + 0.25 2.0 +
0.25 4.0 + 0.25 4.4 + 0.25 2.0.

Therefore total complexity computing QPOMDP becomes


!
(|A| |O|)h 1
(|A| |O|)h1 1
|A| (|A| |O|) +
|A||S| .
(|A| |O|) 1
(|A| |O|) 1

(5.11)

~ done single
Evaluating (5.6) joint action-observation histories ~
backward sweep time, mentioned Section 4.3.3. also visualized
Bayesian games illustrated Figure 8; expected future reward calculated
maximizing weighted sum entries next time step BG.
Nevertheless, solving POMDP optimally also known intractable problem.
result, POMDP research last decade focused approximate solutions
POMDPs. particular, known value function POMDP piecewise-linear
convex (PWLC) (joint) belief space (Sondik, 1971). property exploited
many approximate POMDP solution methods (Pineau, Gordon, & Thrun, 2003; Spaan
& Vlassis, 2005). Clearly methods also used calculate approximate
QPOMDP -value function use Dec-POMDPs.
intuitively clear QPOMDP also admissible heuristic Dec-POMDPs,
still assumes information available actually case (again formal
proof given Section 5.4). Also clear that, fewer assumptions
made, QPOMDP yield less over-estimation QMDP . I.e., QPOMDP -values
lie QMDP optimal Q -values.
contrast QMDP , QPOMDP assume full observability nominal states.
result latter share drawback undervaluing actions gain
information regarding nominal state. applied Dec-POMDP setting, however,
QPOMDP share assumption centralized control. assumption might also
cause relative undervaluation: might situations action might gain
325

fiOliehoek, Spaan & Vlassis

information regarding joint (i.e., others) observation history. QPOMDP
considered redundant, decentralized execution might beneficial,
allows better coordination.
5.3 QBG
QMDP approximates Q assuming state becomes fully observable next
time step, QPOMDP assumes every time step agents know joint
action-observation history ~ . present new approximate Q-value function, called
QBG , relaxes assumptions further: assumes agents know ~ t1 , joint
action-observation history time step 1, joint action at1 taken
previous time step. means agents uncertain regarding others
last observation, effectively defines BG ~ t1 ,a. Note, BGs
different BGs used Section 4.2: BGs types correspond
single observations, whereas BGs 4.2 types correspond complete actionobservation histories. Hence, BGs QBG much smaller size thus easier
solve. Formally QBG defined as:
QB (~ ,a) = R(~ ,a) + max


X

P (ot+1 |~ ,a)QB (~ t+1 ,(ot+1 )),

(5.12)

ot+1

t+1
= h1 (ot+1
1 ),...,n (on )i tuple individual policies : Oi Ai BG
constructed ~ ,a.
Note difference (5.12) (5.6) position argument
maximization operator: (5.12) maximizes (conditional) BG-policy,
maximization (5.6) effectively unconditional joint actions.
BG representation fictitious Dec-POMDP Figure 6 illustrates com~1
putation QBG .8 probability distribution P (
ha1 ,a2 ) joint action-observation
histories reached given ha1 ,a2 = 0 uniform immediate reward
ha1 ,a2 0. Therefore, 2.75 = 0.25 2.0 + 0.25 3.6 + 0.25 4.4 + 0.25 1.0.
cost computing QBG ~ ,a split cost computing
immediate reward (see (5.4)) cost computing future reward (solving BG
last received observation),
!
(|A| |O|)h1 1

|A| |A |n|O | ,
(|A| |O|) 1

leading total cost of:


!
(|A| |O|)h1 1
(|A| |O|)h 1
n|O |
|A| |A |
+
|A||S| .
(|A| |O|) 1
(|A| |O|) 1

(5.13)

Comparing cost computing QPOMDP , contains additional exponential term,
term depend horizon problem.
8. BG representing = 1 Dec-POMDP also involves observation histories length 1,
illustration BG corresponds BGs considered QBG . stages
case.

326

fiOptimal Approximate Q-Value Functions Dec-POMDPs

mentioned Section 5.2, QPOMDP approximated exploiting PWLCproperty value function. turns QBG -value function corresponds
optimal value function situation agents communicate freely onestep delay (Oliehoek et al., 2007b). Hsu Marcus (1982) showed complex dynamic
program constructed settings resulting value function also
preserves PWLC property. surprisingly, QBG -value function also piecewiselinear convex joint belief space and, result, approximation methods
POMDPs transferred computation QBG (Oliehoek et al., 2007b).
5.4 Generalized QBG Bounds
think extension QBG -value function framework case k-steps delayed communication, agent perceives joint action-observation history
k stages delay. is, stage t, agent knows ~ tk joint action-observation history k stages addition current action-observation history ~it . Similar
k-step delayed observation models decentralized control previously proposed
Aicardi, Davoli, Minciardi (1987) Ooi Wornell (1996). particular Aicardi
et al. consider Dec-MDP setting agent observations local states si
joint observation identifies state = hs1 , . . . ,sn i. Ooi Wornell examine
decentralized control broadcast channel infinite horizon, allow
local observations arbitrary, still require joint state observed
k-steps delay. assumption less strong, require observation ~ tk
assume general Dec-POMDP (not Dec-MDP) setting.
k-step delayed communication model Dec-POMDP setting allows expressing different Q-value functions defined article optimal value functions
appropriate k-step delay models. importantly, resorting k-step delay
model prove hierarchy bounds hold various Q-functions defined
article:
Theorem 5.1 (Hierarchy upper bounds). approximate Q-value functions QBG
QPOMDP correspond optimal Q-value functions appropriately defined k-step delayed
communication models. Moreover Q-value functions form hierarchy upper bounds
optimal Q Dec-POMDP:
Q QBG QPOMDP QMDP .

(5.14)

Proof. See appendix.
idea POMDP corresponds system (0-steps) delayed communication, QBG -setting corresponds 1-step delayed communication system.
appendix shows Q-value function system k steps delay forms
upper bound decentralized system k + 1 steps delay. note last
inequality (5.14) well-known result (Hauskrecht, 2000).

6. Generalized Value-Based Policy Search
hierarchy approximate Q-value functions implies Q-value functions
used admissible heuristics MAA policy search, treated Section 3.3.
327

fiOliehoek, Spaan & Vlassis

Algorithm 1 GMAA
1: v
2: P {0 = ()}
3: repeat
4:
Select(P)
5:
Next Next(t )
6:
Next contains subset full policies Next Next
7:
arg maxNext V ()
8:
V ( ) > v
9:
v V ( )
10:
n

11:
P P | Vb () > v {prune policy pool}
12:
end
13:
Next Next \ Next {remove full policies}
14:
end
n

15:
P (P \ ) Next | Vb () > v {remove processed/add new partial policies}
16: P empty
section present general heuristic policy search framework
call Generalized MAA (GMAA ), show unifies solution methods
proposed Dec-POMDPs.
GMAA generalizes MAA (Szer et al., 2005) making explicit different procedures
implicit MAA : (1) iterating pool partial joint policies, pruning pool
whenever possible, (2) selecting partial joint policy policy pool, (3) finding
new partial and/or full joint policies given selected policy. first procedure
core GMAA fixed, two procedures performed many
ways.
second procedure, Select, chooses policy process next thus determines type search (e.g., depth-first, breadth-first, A*-like) (Russell & Norvig, 2003;
Bertsekas, 2005). third procedure, refer Next, determines
set next (partial) joint policies constructed, given previous partial joint policy.
original MAA seen instance generalized case particular
Next-operator, namely shown algorithm 2.
6.1 GMAA Algorithm
GMAA refer policy pool P rather open list, neutral
word imply ordering. policy pool P initialized completely
unspecified joint policy 0 = () maximum lower bound (found far) v set
. denotes best joint policy found far.
point GMAA starts. First, selection operator, Select, selects partial joint
policy P. assume that, accordance MAA , partial policy
highest heuristic value selected. general, however, kind selection algorithm
may used. Next, selected policy processed policy search operator Next,
328

fiOptimal Approximate Q-Value Functions Dec-POMDPs

Algorithm 2 Next(t ) MAA

n
ff t+1

t+1

, , :

~
|

=

1: t+1 t+1 = 1 ,...,t+1

n




fi t+1

t+1
0...t1


b
fi
+ Vb (t+1)...h (t+1 )
2: t+1 t+1 V (
)V
( ) + E R(s ,a)
t+1
3: return
returns set (partial) joint policies Next heuristic values. Next
returns one full policies Next , provided values Vb () = V () lower
bound optimal joint policy, used prune search space. found
partial joint policies Next heuristic value Vb () > v added P.
process repeated policy pool empty.
6.2 Next Operator
describe different choices Next-operator correspond
existing Dec-POMDP solution methods.
6.2.1 MAA
GMAA reduces standard MAA using Next-operator described Algorithm 2.
Line 1 expands forming t+1 set partial joint policies one extra stage. Line 2
valuates child policies,
fi


V 0...t (t+1 ) = V 0...t1 (t ) + E R(st ,a)fit+1

gives true expected reward first + 1 stages. Vb (t+1)...h (t+1 ) heuristic
value stages (t + 1)...h given t+1 followed first + 1 stages.
using admissible heuristic, GMAA never prune partial policy
expanded optimal policy. combining fact MAA Next operator returns possible t+1 , clear P becomes empty
optimal policy found.
6.2.2 Forward-Sweep Policy Computation
Forward-sweep policy computation, introduced Section 4.3.1, described algorithms 1 3 jointly. Given partial joint policy , Next operator constructs
solves BG time step t. Next algorithm 3 returns best-ranked
policy, P never contain 1 joint policy whole search process reduces
solving BGs time steps 0,...,h 1.
approach Emery-Montemerlo et al. (2004) identical forward-sweep policy
computation, except 1) smaller BGs created discarding clustering low probability action-observation histories, 2) BGs approximately solved alternating
maximization. Therefore approach also incorporated GMAA policy
search framework making appropriate modifications Algorithm 3.
329

fiOliehoek, Spaan & Vlassis

Algorithm 3 Next(t ) Forward-sweep policy computation
E

bt
~ ),Q
~ ,P (
1: BG A,


~ Ai
2: = h1 ,...,n s.t. :

P




~
~
b
3:
Vb () ~
P
(

)
Q
(

,(
~ ))
~t


4:
t+1 ,
5:
Vb (t+1 ) V 0...t1 (t ) + Vb ()
6: end
b (t+1 )
7: return arg maxt+1 V
6.2.3 Unification
give unified perspective MAA forward-sweep policy computation
examining relation corresponding Next-operators. particular
show that, using approximate Q-value functions described Section 5
heuristic, sole difference two FSPC returns joint policy
highest heuristic value.
b following form
Proposition 6.1. heuristic Q
X
b (~ ,a) = R(~ ,a) +
P (ot+1 |~ ,a)Vb t+1 (~ t+1 ),
Q

(6.1)

ot+1


partial policy t+1 = ,
X
fi


b (~ ,(~ )) = E R(st ,a)fit+1 + Vb (t+1)...h (t+1 )
P (~ )Q

(6.2)

~t
~


holds.

Proof. expectation Rt given t+1 written
X
X
X
fi


P (~ )R(~ ,t+1 (~ )).
P (~ )
R(s,t+1 (~ ))P (s|~ ) =
E R(st ,a)fit+1 =
sS

~t
~


~t
~


Also, rewrite Vb (t+1)...h (t+1 )
X
X
P (ot+1 |~ ,t+1 (~ ))Vb (t+1)...h (~ t+1 ),
P (~ )
Vb (t+1)...h (t+1 ) =
ot+1

~t
~



X
fi


P (~ )
E R(st ,a)fit+1 + Vb (t+1)...h (t+1 ) =
"

~t
~


~t

t+1

R( ,

~t

( )) +

X

ot+1

Therefore, assuming (6.1) yields (6.2).
330

P (o

t+1

~t

t+1

| ,

~t

b (t+1)...h

( ))V

~ t+1

(

#

)

(6.3)

fiOptimal Approximate Q-Value Functions Dec-POMDPs

~o go house 3
flames go house 3
flames go house 1
flames, flames go house 1
flames, flames go house 1
flames, flames go house 2
flames, flames go house 2

~o go house 2
flames go house 2
flames go house 2
flames, flames go house 1
flames, flames go house 1
flames, flames go house 1
flames, flames go house 1

Figure 9: Optimal policy FireFighting hnh = 3,nf = 3i, horizon 3. left
policy first agent, right second agents policy.

means heuristic satisfies (6.1), case Q-value functions
discussed paper, Next operators algorithms 2 3 evaluate expanded
policies same. I.e., algorithms 2 3 calculate identical heuristic values
next time step joint policies. Also expanded policies t+1 formed way:
considering possible respectively extend . Therefore, sole difference
case latter returns joint policy highest heuristic value.
Clearly computation time/quality trade-off MAA FSPC: MAA
guaranteed find optimal policy (given admissible heuristic), FSPC
guaranteed finish one forward sweep. propose generalization, returns
k-best ranked policies. refer k-best joint BG policies GMAA variant,
k-GMAA . way, k-GMAA reduces forward-sweep policy computation k = 1
MAA k = .

7. Experiments
order compare different approximate Q-value functions discussed work,
well show flexibility GMAA algorithm, performed several
experiments. use QMDP , QPOMDP QBG heuristic estimates Q .
provide qualitative insight different Q-value functions considered, well
results computing optimal policies using MAA , performance forwardsweep policy computation. First describe problem domains,
standard test problems, others introduced work.
7.1 Problem Domains
Section 2.2 discussed decentralized tiger (Dec-Tiger) problem introduced
Nair et al. (2003b). Apart standard Dec-Tiger domain, consider modified
version, called Skewed Dec-Tiger, start distribution uniform. Instead,
initially tiger located left probability 0.8. also include results
BroadcastChannel problem, introduced Hansen et al. (2004), models two nodes
cooperate maximize throughput shared communication channel.
Furthermore, test problem called Meeting Grid provided Bernstein et al.
(2005), two robots navigate two-by-two grid. consider version 2
observations per agent (Amato et al., 2006).
331

fiOliehoek, Spaan & Vlassis

introduce new benchmark problem, models team n fire fighters
extinguish fires row nh houses. house characterized integer
parameter f , fire level. indicates degree house burning, nf
different values, 0 f < nf . minimum value 0, indicating house burning.
every time step, agents receive reward f house agent choose
move houses fight fires location. house burning (f > 0)
fire fighting agent present, fire level increase one point probability 0.8
neighboring houses burning, probability 0.4 none neighbors
fire. house burning catch fire probability 0.8 one
neighbors fire. two agents house, extinguish
present fire completely, setting houses fire level 0. single agent present house
lower fire level one point probability 1 neighbors burning,
probability 0.6 otherwise. agent observe whether flames
location. Flames observed probability 0.2 f = 0, probability 0.5
f = 1, probability 0.8 otherwise. Initially, agents start outside
houses, fire level f house drawn uniform distribution.
test different variations problems, number agents always
2, differ number houses fire levels. particular, consider
hnh = 3,nf = 3i hnh = 4,nf = 3i. Figure 9 shows optimal joint policy horizon 3
former variation. One agent initially moves middle house fight fires there,
helps prevent fire spreading two neighbors. agent moves
house 3, stays observes fire, moves house 1 observe
flames. well optimal, joint policy makes sense intuitively speaking.
7.2 Comparing Q-value Functions
providing comparison performance approximate Q-value functions
described work, first give insights actual values.
h = 4 Dec-Tiger problem, generated possible ~ corresponding P (sl |~ ),
according (4.8). these, maximal Q(~ ,a)-value plotted Figure 10.
Apart three approximate Q-value functions, also plotted optimal value
joint action-observation history ~ realized using . Note
different ~ different optimal values, induce P (sl |~ ), demonstrated
figure: multiple Q -values plotted P (sl |~ ). horizon 3
Meeting Grid problem also collected ~ visited optimal
policy, Figure 11 plotted maximal Q(~ ,a)-values. problem
many states, representation Figure 10 possible. Instead, ordered
~
~ according optimal value. see bounds tight ,

others quite loose. However, used GMAA framework,
~ shown
actual performance heuristic also depends valuation ~
Figure 11, namely visited optimal policy: especially
overestimated, GMAA first examine sub-optimal branch search tree.
tighter upper bound speed computation large extent, allows
algorithm prune policy pool more, reducing number Bayesian games need
332

fiOptimal Approximate Q-Value Functions Dec-POMDPs

Qheuristics horizon=4 DecTiger t=0
60

Qheuristics horizon=4 DecTiger t=1
60

Q

Q

BG
POMDP

Qmax = maxa Q(t,a)

MDP

Q*

40
30
20
10
0
0

QPOMDP

50

Q



Qmax = max Q(t,a)

BG

Q

50

Q

MDP
*

40

Q

30
20
10
0



P(sl | )

10
0

1

Qheuristics horizon=4 DecTiger t=2
40

P(sl | )

1

Qheuristics horizon=4 DecTiger t=3
20

Q

QBG

BG

QPOMDP
Q

*

Q

20



20

POMDP

QMDP



MDP
*

Q

0

Q

Qmax = max Q( ,a)

Qmax = maxa Q(t,a)

30

10
0
10
20
0

40
60
80

P(sl | )

100
0

1



P(sl | )

1

Figure 10: Q-values horizon 4 Dec-Tiger. ~ , corresponding P (sl |~ ),
maximal Q(~ ,a)-value plotted.

solved. figures clearly illustrate main property upper bounds
discussed, namely Q QBG QPOMDP QMDP (see Theorem 5.1).
7.3 Computing Optimal Policies
shown above, hierarchy upper bounds Q QBG QPOMDP QMDP
theoretical construct, differences value specified significant
particular problems. order evaluate impact differences
approximate Q-value functions, performed several experiments. describe
evaluation MAA number test problems using QBG , QPOMDP QMDP
heuristic. timing results paper CPU times resolution 0.01s,
obtained 3.4GHz Intel Xeon processors.
333

fiOliehoek, Spaan & Vlassis

2
1.8

Qmax = maxa Q(t,a)

1.6
1.4
1.2
1
0.8
Q

MDP

0.6

Q

POMDP

Q

BG
*

0.4

Q

0.2





Figure 11: Comparison maximal Q(~ ,a)-values Meeting Grid. plot
value reached optimal policy, ordered according
optimal value.

h

V

3

5.1908

4

4.8028

QMDP
QPOMDP
QBG
QMDP
QPOMDP
QBG

n
105,228
6,651
6,651
37,536,938,118
559,653,390
301,333,698

TGMAA
0.31
0.02
0.02
431,776
5,961
3,208

TQ
0s
0s
0.02
0s
0.13
0.94

Table 1: MAA results Dec-Tiger.

h

V
QMDP

3

5.8402

4

11.1908

QPOMDP
QBG
QMDP
QPOMDP
QBG

n
151,236
19,854
13,212
33,921,256,149
774,880,515
86,106,735

TGMAA
0.46
0.06
0.04
388,894
8,908
919

Table 2: MAA results Skewed Dec-Tiger.

334

TQ
0s
0.01
0.03
0s
0.13
0.92

fiOptimal Approximate Q-Value Functions Dec-POMDPs

h

V
QMDP

4

3.8900

5

4.7900

QPOMDP
QBG
QMDP
QPOMDP
QBG

n
328,212
531
531
N/A
196,883
196,883

TGMAA
3.54
0s
0s
> 4.32e5
5.30
5.15

TQ
0s
0.01
0.03
0s
0.20
0.53

Table 3: MAA results BroadcastChannel.

h

V
QMDP

2

0.9100

3

1.5504

QPOMDP
QBG
QMDP
QPOMDP
QBG

n
1,275
1,275
194
29,688,775
3,907,525
1,563,775

TGMAA
0s
0s
0s
81.93
10.80
4.44

TQ
0s
0s
0s
0s
0.15
1.37

Table 4: MAA results Meeting Grid.

Table 1 shows results MAA obtained original Dec-Tiger problem horizon
3 4. shows heuristic number partial joint policies evaluated n , CPU
time spent GMAA phase TGMAA , CPU time spent calculating heuristic
TQ . QBG , QPOMDP QMDP upper bounds Q , MAA guaranteed find
optimal policy using heuristic, however timing results may differ.
h = 3 see using QPOMDP QBG fraction number policies
evaluated compared QMDP reflects proportionally time spent
GMAA . horizon QPOMDP QBG perform same, time needed
compute QBG heuristic long GMAA -phase, therefore QPOMDP outperforms
QBG here. h = 4, impact using tighter heuristics becomes even pronounced.
case computation time heuristic negligible, QBG outperforms both,
able prune much partial joint policies policy pool. Table 2 shows
results Skewed Dec-Tiger. problem QMDP QBG results roughly
original Dec-Tiger problem; h = 3 timings bit slower,
h = 4 faster. QPOMDP , however, see h = 4 results slower
well QBG outperforms QPOMDP order magnitude.
Results Broadcast Channel (Table 3), Meeting Grid (Table 4) Fire
fighting problem (Table 5) similar. N/A entry Table 3 indicates QMDP
able compute solution within 5 days. problems also see
performance QPOMDP QBG roughly equal. Meeting Grid problem,
QBG yields significant speedup QPOMDP .
335

fiOliehoek, Spaan & Vlassis

h

V

3

5.7370

4

6.5788

QMDP
QPOMDP
QBG
QMDP
QPOMDP
QBG

n
446,724
26,577
26,577
25,656,607,368
516,587,229
516,587,229

TGMAA
1.58
0.08
0.08
309,235
5,730
5,499

TQ
0.56
0.21
0.33
0.85
7.22
11.72

Table 5: MAA results Fire Fighting hnh = 3, nf = 3i.
~o aLi
oHL aLi
oHR aLi
oHL , oHL aLi
oHL , oHR aLi
oHR , oHL aLi
oHR , oHR aLi
oHL , oHL , oHL aOR
oHL , oHL , oHR aLi
oHL , oHR , oHL aLi
oHL , oHR , oHR aLi
oHR , oHL , oHL aLi
oHR , oHL , oHR aLi
oHR , oHR , oHL aLi
oHR , oHR , oHR aOL

~o aLi
oHL aLi
oHR aLi
oHL , oHL aOR
oHL , oHR aLi
oHR , oHL aLi
oHR , oHR aOL
oHL , oHL , oHL aLi
oHL , oHL , oHR aLi
oHL , oHR , oHL aLi
oHL , oHR , oHR aLi
oHR , oHL , oHL aLi
oHR , oHL , oHR aLi
oHR , oHR , oHL aLi
oHR , oHR , oHR aLi

Figure 12: Policies found using forward-sweep policy computation (i.e., k = 1) h = 4
Dec-Tiger problem. Left: policy resulting QMDP . Right: optimal
policy calculated QPOMDP QBG . framed entries highlight
crucial differences.

7.4 Forward-Sweep Policy Computation
MAA results described indicate use tighter heuristic yield
substantial time savings. section, approximate Q-value functions used
forward-sweep policy computation. would expect using Q-value function
closely resembles Q , quality resulting policy higher. also
tested whether k-GMAA k > 1 improved quality computed policies.
particular, tested k = 1,2, . . . ,5.
Dec-Tiger problem, k-GMAA k = 1 (and thus also 2 k 5) found
optimal policy (with V ( ) = 5.19) horizon 3 using approximate Q-value functions. horizon h = 4, also different values k produced result
approximate Q-value function. case, however, QMDP found policy expected
return 3.19. QPOMDP QBG find optimal policy (V ( ) = 4.80). Figure 12
illustrates optimal policy (right) one found QMDP (left). shows QMDP
overestimates value opening door stage = 2.
336

fiOptimal Approximate Q-Value Functions Dec-POMDPs

6

15
10

4

V

V

5
Q

MDP

Q

MDP

5

Q

3

Q

POMDP

POMDP

Q

Q

BG

2

1

2

3

k

4

BG

0

5

(a) Skewed Dec-Tiger, h = 3.

2

3

k

4

6.575

1.5505

6.58

V

1.55

Q

MDP

Q

MDP

6.585

Q

1.5495

Q

POMDP

POMDP

Q

Q

BG

1.549

5

(b) Skewed Dec-Tiger, h = 4.

1.551

V

1

1

2

3

k

4

BG

6.59

5

(c) GridSmall, h = 3.

1

2

3

k

4

5

(d) FireFighting hnh = 3,nf = 3i, h = 4.

11.05

14

11.1

14.1

Q

MDP

Q

POMDP

Q

V

V

BG

11.15

Q

14.2

MDP

Q

11.2

14.3

POMDP

Q

BG

11.25

1

2

3

k

4

14.4

5

(e) FireFighting hnh = 4,nf = 3i, h = 3.

1

2

3

k

4

5

(f) FireFighting hnh = 4,nf = 3i, h = 4.

Figure 13: k-GMAA results different problems horizons. y-axis indicates value
initial joint belief, x-axis denotes k.

Skewed Dec-Tiger problem, different values k produce different results.
particular, h = 3 QBG finds optimal policy (and thus attains optimal value)
values k, shown Figure 13(a). QPOMDP find starting k = 2,
QMDP k = 5. Figure 13(b) shows somewhat unexpected result h = 4:
k = 1 QMDP QBG find optimal policy, QPOMDP doesnt. clearly
illustrates tighter approximate Q-value function guarantee better joint
policy, also illustrated results GridSmall Figure 13(c).
also performed experiment two settings FireFighting problem.
hnh = 3,nf = 3i h = 3 Q-value functions found optimal policy (with value
5.7370) k, horizon 4 shown Figure 13(d). Figures 13(e) 13(f) show
results hnh = 4,nf = 3i. h = 4, QMDP finish k 3 within 5 days.
encouraging experiments k-GMAA using QBG QPOMDP k 2
found optimal policy. Using QMDP optimal policy also always found k 5,
except horizon 4 Dec-Tiger hnh = 4,nf = 3i FireFighting problem. results
seem indicate type approximation might likely produce (near-) optimal
results domains well.
337

fiOliehoek, Spaan & Vlassis

8. Conclusions
large body work single-agent decision-theoretic planning based value functions,
theory lacking thus far Dec-POMDPs. Given large impact value
functions single-agent planning uncertainty, expect thorough study
value functions Dec-POMDPs greatly benefit multiagent planning certainty.
work, presented framework Q-value functions Dec-POMDPs, providing
significant contribution fill gap Dec-POMDP theory. theoretical contributions
lead new insights, applied improve extend solution methods.
shown optimal joint policy induces optimal Q-value function

Q (~ ,a), possible construct optimal policy using forward-sweep
policy computation. entails solving Bayesian games time steps = 0 ,..., h 1
use Q (~ ,a) payoff function. clear way compute
Q (~ ,a), introduced different description optimal Q-value function Q (~ ,t+1 )
based sequential rationality. new description Q computed using
dynamic programming used construct .
calculating Q computationally expensive, examined approximate Q-value
functions calculated efficiently discussed relate Q .
covered QMDP , QPOMDP , QBG , recently proposed approximate Q-value function.
Also, established decreasing communication delays decentralized systems cannot
decrease expected value thus Q QBG QPOMDP QMDP . Experimental
evaluation indicated upper bounds theoretical interest,
significant differences exist tightness various approximate Q-value functions.
Additionally showed approximate Q-value functions used heuristics generalized policy search method GMAA , thereby unifying forward-sweep policy
computation recent Dec-POMDP solution techniques Emery-Montemerlo et al.
(2004) Szer et al. (2005). Finally, performed empirical evaluation GMAA
showing significant reductions computation time using tighter heuristics calculate
optimal policies. Also QBG generally found better approximate solutions forward-sweep
policy computation k-best joint BG policies GMAA variant, k-GMAA .
quite directions future research. One try extend results
paper partially observable stochastic games (POSGs) (Hansen et al., 2004),
Dec-POMDPs individual reward function agent. Since dynamics
POSG model identical Dec-POMDP, similar modeling via Bayesian
games possible. interesting question whether also case, optimal (i.e.,
rational) joint policy found forward-sweep policy computation.
Staying within context Dec-POMDPs, research direction could
generalize GMAA , defining Next Select operators, hope
resulting algorithms able scale larger problems. Also important establish
bounds performance learning curves GMAA combination different
Next operators heuristics. different direction experimentally evaluate use
even tighter heuristics Q-value functions case observations delayed
multiple time steps. research paired methods efficiently find
Q-value functions. Finally, future research examine Bayesian games.
particular, work Emery-Montemerlo et al. (2005) could used starting point
338

fiOptimal Approximate Q-Value Functions Dec-POMDPs

research approximately modeling Dec-POMDPs using BGs. Finally,
need efficient approximate methods solving Bayesian games.

Acknowledgments
thank anonymous reviewers useful comments. research reported
part Interactive Collaborative Information Systems (ICIS) project, supported
Dutch Ministry Economic Affairs, grant nr: BSIK03024. work supported
Fundacao para Ciencia e Tecnologia (ISR/IST pluriannual funding)
POS Conhecimento Program includes FEDER funds, grant PTDC/EEAACR/73266/2006.

Appendix A. Proofs
A.1 Least One Optimal Pure Joint Policy
Proposition (2.1). Dec-POMDP least one optimal pure joint policy.
Proof. proof follows proof Schoute (1978). possible convert Dec-POMDP
extensive game thus strategic game, actions pure policies
Dec-POMDP (Oliehoek & Vlassis, 2006). strategic game, least one
maximizing entry corresponding pure joint policy denote max . Now, assume
joint stochastic policy = h1 , . . . ,n attains higher payoff. Kuhn
(1953) showed stochastic policy, corresponding mixed policy .
Therefore corresponds joint mixed policy = h1 , . . . ,n i. Let us write i,i
support . induces probability distribution P set joint policies
= 1,1 n,n subset set joint policies. expected
payoff written
V () = EP (V ()| ) max V () = V (max ),


contradicting joint stochastic policy attains higher payoff.
A.2 Hierarchy Q-value Functions
section lists proof theorem 5.1. ordered follows. First, Section A.2.1
presents model resulting value functions Dec-POMDPs k-steps delayed communication. Next, Section A.2.2 shows QPOMDP , QBG Q correspond
case k respectively 0, 1 h. Finally, Section A.2.3 shows communication delay k increases, optimal expected return cannot decrease, thereby proving
theorem 5.1.
A.2.1 Modeling Dec-POMDPs k-Steps Delayed Communication
present augmented MDP used find optimal solution
Dec-POMDPs k steps delayed communication. reformulation work
Aicardi et al. (1987) Ooi Wornell (1996), extended Dec-POMDP setting.
339

fiOliehoek, Spaan & Vlassis

q1t

q2t

a1
o1

o1

o1

a1

a1 ...

a...1

t+k1

a2

o2

o1

a1

o2

a2

a1
o1



o2

o1

a1

a2

o2
a...2

a2

o2

o2

a2

a2

t+k
t+k

ot+1 = ho1 ,o2

q1t+1

q2t+1

a1
o1

o2

o1

a1

a1

a2

a2

t+1
o2
a2

t+k

Figure 14: Policies specified states augmented MDP k = 2. Top: policies
st . policy extended augmented MDP action = shown dashed.
Bottom: resulting policies joint observation ho1 ,o2 i.

define augmented MDP = hS,A,T ,Ri, augmented MDP stages
indicated t.
state space = (S t=0 , . . . ,S t=h1 ). augmented state composed joint
action-observation history, joint policy tree q .
st=t =

(

h~ ,q
h~ ,q =ht,t

,0 h k 1
.
,h k h 1


q joint depth-k (specifying actions k stages) joint policy tree q =
ff

contained
q1 ,...,qnt , used starting stage t. last k stages, contained joint policy
q =ht,t specifies = h k stages.
set augmented actions. 0 h k 1, action joint
policy at=t = t+k = h1t+k . . . nt+k implicitly mapping length-k observation histories
~ k At+k . last k stages
joint actions taken stage + k. I.e., it+k :


h k h 1 one empty action influence whatsoever.
augmented actions used expand joint policy trees. appending
policy t+k q form depth k + 1 policy, denote q =k+1,t = hq t+k i.
execution initial joint action q =k+1,t (~o ) receiving particular joint observation o,
340

fiOptimal Approximate Q-Value Functions Dec-POMDPs

st
a1
~ ,

o1

a2
o2

o1

a1


o2

a2

a1

a2

t+1

ot+1 = ho1 ,o2
st+1
a2

a1
~ t+1 ,

o1
a1

o2

o1
a1

a2

t+1
o2
a2

t+k

Figure 15: illustration augmented MDP k = 2, showing transition
st st+1 action = . example ~ tk+1 = (~ tk+1 , ha1 ,a2 , ho1 ,o2 i).
actions specified stage given (ho1 ,o2 i) depicted Figure 14.

q =k+1,t reduces depth k sub-tree policy particular joint observation, denoted
q tk+1 = q =k+1,t (o) = hq tk i(o). illustrated Figure 14.
transition model. probability P (st+1 |st ,at ) stage = translates follows
0 h k 1
(
P (ot+1 |~ ,q (~o )) conditions hold,
t+1
t+1


t+k
P (h~ ,q i|h~ ,q i, ) =
(A.1)
0
otherwise,
conditions are: 1) q t+1 = hq t+k i(ot+1 ), 2) ~ t+1 = (~ ,at ,ot+1 ).
h k h 1, t+k (A.1) reduces . probabilities unaffected,
first condition changes q =ht1,t+1 = q =ht,t (ot+1 ).
Finally, R reward model, specified follows:
0th1

R(st=t ) = R(h~ ,q i) = R(~ ,q (~o )),

(A.2)

q (~o ) initial joint action specified q . R(~ ,a) defined (2.12).
resulting optimality equations Qt (s,a) augmented MDP follows.
write Qk optimal Q-value function k-steps delayed communication system.
also refer k-QBG value function.
X
P (ot+1 |~ ,q (~o ))Qk (~ t+1 ,q t+1 ), (A.3)
0thk1 Qk (~ ,q , t+k ) = R(~ ,q (~o ))+
ot+1

341

fiOliehoek, Spaan & Vlassis

q t+1 = hq t+k i(ot+1 )
Qk (~ ,q ) max Qk (~ ,q , t+k ).

(A.4)

t+k

last k stages, h k h 1, = h stages go get




Qk (~ ,q = ,t ) = R(~ ,q = ,t (~o )) +

X



P (ot+1 |~ ,q = ,t (~o ))Qk (~ t+1 ,q =

1,t+1

). (A.5)

ot+1

Note (A.5) include augmented actions at=t = t+k . Therefore, last
k stages interpreted Markov chain. Standard dynamic programming
applied calculate Q (~ ,q )-values.
A.2.2 Relation k-QBG Approximate Q-value Functions
briefly show k-QBG fact reduces cases treated earlier.
k = 0, k-QBG (A.3) reduces QPOMDP . k = 0 case, q tk becomes depth0, i.e. empty, policy. Also, becomes mapping length-0 observation histories
actions, i.e., becomes joint action. Substitution (A.3) yields
E
E


X
Q0 ( ~ , ,at ) = R(~ ,at ) +
P (ot+1 |~ ,at ) max Q0 ( ~ t+1 , ,at+1 ).
at+1

ot+1

b P (~ ,a) Q (b~ ,a), clearly corresponds QPOMDP -value function
Now, Q
P
(5.6).
1-QBG reduces regular QBG . Notice k = 1, q =k,t reduces . Filling
yields:
E

E

X
Q1 ( ~ ,at , t+1 ) = R(~ ,at ) +
P (ot+1 |~ ,at ) max Q1 ( ~ t+1 , t+1 (ot+1 ) , t+2 ).
t+2

ot+1

using (A.4) obtain QBG -value function (5.12).
Dec-POMDP identical h-steps delayed communication system. Augmented
states form st=0 = h~ ,q 0 i, q 0 = specifies full length h joint policy.
first stage = 0 augmented MDP, also one last k (= h) stages. Therefore,
applied Q-function (A.5), means Markov chain evaluation starts immediately. Effectively boils evaluation joint policies (corresponding
augmented start states). maximizing one specifies value function optimal
joint policy Q .
A.2.3 Shorter Communication Delays cannot Decrease Value
First, introduce notation. Let us write Po observation probabilities given
~ ,q sequence intermediate observations (ot+1 , . . . ,ot+l1 )
h

Po (ot+l ) P ot+l |(~ ,q (~o ),ot+1 ,q (ot+1 ), . . . ,ot+l1 ),q (ot+l1 ) ,
342

l k.

(A.6)

fiOptimal Approximate Q-Value Functions Dec-POMDPs

policy implicitly maps k-length
order avoid confusion, write |k|

observation histories actions, |k+1|
one mapping length (k + 1)
observation-histories actions.
~t
~t
give reformulation Qk . Qt,
k ( ,q ) specifies expected return ,q
stages t,t + 1, . . . ,h 1. Here, split

Qk (~ ,q ) = Kk (~ ,q ) + Fkt, (~ ,q )

(A.7)

Kk (~ ,q ), expected k-step reward, i.e., expected return stages t, . . . ,t+k1
Fkt, (~ ,q ), expected return stages + k,t + k + 1, . . . ,h 1, referred
k-steps expected return.
former defined
Kk (~ ,q ) E

"t+k1
X
=t

#
fi
fi
R(~ ,a ) fi ~ ,q .




(A.8)

Let us define K =i (~ ,q =i,t ) expected reward next stages, i.e.,
Kk (~ ,q ) = K =k (~ ,q ).

(A.9)

K =1 (~ ,at ) = R(~ ,at )
K =i (~ ,q =i,t ) = R(~ ,q =i,t (~o ))+
X
P (ot+1 |~ ,q =i,t (~o ))K =i1 (~ t+1 ,q =i1,t+1 (ot+1 )), (A.10)
ot+1

q =i1,t+1 (ot+1 ) depth-(i 1) joint policy results q =i,t observation ot+1 .
t+k
define Fk =i,t (~ ,q ,|k|
) expected reward stages + i,t + + 1, . . . ,h 1.
is, time-to-go = denotes much time-to-go start accumulating
expected reward. k-steps expected return given
t+k
t+k
Fkt (~ ,q ,|k|
) = Fk =k,t (~ ,q ,|k|
).

evaluation performed
t+k
t+k
)
) = Qk (~ ,q ,|k|
Fk =0,t (~ ,q ,|k|
X
t+k
Po (ot+1 )Fk =i1,t+1, (~ t+1 ,q t+1 ),
Fk =i,t (~ ,q ,|k|
) =

(A.11)
(A.12)

ot+1

t+k
i(ot+1 ),
q t+1 = hq t+1 |k|
t+k
).
Fk =i,t, (~ ,q ) = max Fk =i,t (~ ,q ,|k|
t+k
|k|

343

(A.13)

fiOliehoek, Spaan & Vlassis

Theorem A.1 (Shorter communication delays cannot decrease value). optimal
Q-value function Qk finite horizon Dec-POMDP k-steps delayed communication
upper bound Qk+1 , k + 1-steps delayed communication system.
~ q =k,t , t+k
|k|

t+k
t+k+1
t+k
i,|k+1|
).
Qk (~ ,q =k,t ,|k|
) max Qk+1 (~ ,hq =k,t |k|
t+k+1
|k+1|

(A.14)

Proof. proof induction. base case (A.14) holds stages h (k +
1) h 1, shown lemma A.1. induction hypothesis states that, assuming
(A.14) holds stage + k, also holds stage t. induction step proven
lemma A.2.
Lemma A.1 (Base case). h k 1 h 1, expected cumulative future
reward k steps delay equal k + 1 steps delay policies
followed point. is,
hkth1 ~ q =ht,t

Qk (~ ,q =ht,t ) = Qk+1 (~ ,q =ht,t ),

(A.15)

~ hk1 q =k,hk1 , h1
|k|

h1
h1
Qk (~ hk1 ,q =k,hk1 ,|k|
) = Qk+1 (~ hk1 ,hq =k,hk1 |k|
i).

(A.16)


Proof. particular stage = h h k h 1 arbitrary ~ ,q = ,t ,
write


Qk (~ ,q = ,t ) = Qk+1 (~ ,q = ,t ),

given evaluation (A.5), evaluation involves actions:
Basically (A.5) reduced Markov chain, Markov chain Qk
Qk+1 . conclude
hkth1 ~ ,q = ,t



Qk (~ ,q = ,t ) = Qk+1 (~ ,q = ,t ).

prove (A.16). left side (A.16) given application (A.3)
h1
Qk (~ hk1 ,q =k,hk1 ,|k|
) = R(~ hk1 ,q =k,hk1 (~o ))+
X
P (ohk |~ hk1 ,q =k,hk1 (~o ))Qk (~ hk ,q =k,hk ),
ohk

h1
q =k,hk = hq =k,hk1 |k|
i(ohk ). right side given application (A.5)
h1
Qk+1 (~ hk1 ,hq =k,hk1 |k|
i) = R(~ hk1 ,q =k,hk1 (~o ))+
X
P (ohk |~ hk1 ,q =k,hk1 (~o ))Qk+1 (~ hk ,q =k,hk )
ohk

h1
q =k,hk = hq =k,hk1 |k|
i(ohk ). Now, policies q =k,hk
same, get
Qk (~ hk ,q =k,hk ) = Qk+1 (~ hk ,q =k,hk )

thus (A.16) holds.
344

fiOptimal Approximate Q-Value Functions Dec-POMDPs

Lemma A.2 (Induction step). Given
~ q =k,t , +k
|k|















+k
+k
+k+1
Qk (~ ,q =k,t ,|k|
) max
Qk+1 (~ ,hq =k,t |k|
i,|k+1|
) (A.17)

+k+1
|k+1|

holds = + (k + 1),
~ q =k,t , t+k
|k|

t+k
t+k+1
t+k
i,|k+1|
)
Qk (~ ,q =k,t ,|k|
) max Qk+1 (~ ,hq =k,t |k|

(A.18)

t+k+1
|k+1|

holds stage t.
Proof. k-steps delay Q-function, write
t+k
Qk (~ ,q =k,t ,|k|
) = R(~ ,q =k,t (~o ))+
h

X
t+k+1
) (A.19)
Po (ot+1 |~ ,q =k,t (~o )) max Kk (~ t+1 ,q t+1 ) + Fkt+1 (~ t+1 ,q =k,t+1 ,|k|
t+k+1
|k|

ot+1

t+k
t+k+1
q =k,t+1 = hq =k,t |k|
i(ot+1 ). Kk independent |k|
, regroup
terms get

~t

Qk ( ,q

=k,t

t+k
,|k|
)

"

~t

= R( ,q

=k,t

"

(~o )) +

X

Po (o

t+1

)Kk (

ot+1

X

ot+1

~ t+1

,q

=k,t+1

#

) +
#

t+k+1
) . (A.20)
Po (ot+1 ) max Fkt+1 (~ t+1 ,q =k,t+1 ,|k|
t+k+1
|k|

case k + 1-steps delay, write
t+k
t+k
t+k
t+k
t+k+1

Qk+1 (~ ,hq =k,t |k|
i,|k+1|
) = Kk+1 (~ ,hq =k,t |k|
i) + Fk+1
(~ ,hq =k,t |k+1|
i,|k+1|
)
(A.21)
where, per definition (by (A.9) (A.10))
t+k
Kk+1 (~ ,hq =k,t |k|
i) = R(~ ,q =k,t (~o )) +

X

Po (ot+1 )K =k (~ t+1 ,q =k,t+1 ),

ot+1

= R(~ ,q =k,t (~o )) +

X

Po (ot+1 )Kk (~ t+1 ,q =k,t+1 ),

(A.22)

ot+1

q =k,t+1 = hq =k,t t+k i(ot+1 ).
Equation (A.22) equal first part (A.20). Therefore, arbitrary ~ ,q =k,t
t+k
, know (A.18) holds
|k|
X

ot+1

t+k
t+k+1
t+k+1

(~ ,hq =k,t |k|
i,|k+1|
)
) max Fk+1
Po (ot+1 ) max Fkt+1 (~ t+1 ,q =k,t+1 ,|k|
t+k+1
|k+1|

t+k+1
|k|

(A.23)
345

fiOliehoek, Spaan & Vlassis

t+k

q =k,t+1 = hq =k,t |k|
i(ot+1 ). filling expanding Fk+1
using
(A.12) get

X

t+k
t+k+1
i(ot+1 ),|k|
)
Po (ot+1 ) max Fk =k,t+1 (~ t+1 ,hq =k,t |k|
t+k+1
|k|

ot+1

max

t+k+1
|k+1|

X

=k,t+1, ~ t+1
t+k
t+k+1
Po (ot+1 )Fk+1
( ,hhq =k,t |k|
|k+1|
i(ot+1 )). (A.24)

ot+1

clearly holds
X
t+k
t+k+1
i(ot+1 ),|k|
)
Po (ot+1 ) max Fk =k,t+1 (~ t+1 ,hq =k,t |k|
t+k+1
|k|

ot+1

X

=k,t+1, ~ t+1
t+k
t+k+1
( ,hhq =k,t |k|
i(ot+1 ) |k|
i), (A.25)
Po (ot+1 ) max Fk+1
t+k+1
|k|

ot+1

second part (A.25) upper bound second part (A.24). Therefore,
induction step proved show
ot+1 t+k+1
|k|

t+k
t+k+1
i(ot+1 ),|k|
)
Fk =k,t+1 (~ t+1 ,hq =k,t |k|
=k,t+1, ~ t+1
t+k
t+k+1
Fk+1
( ,hhq =k,t |k|
i(ot+1 ) |k|
i). (A.26)

t+k
(A.13) q =k,t+1 = hq =k,t |k|
i(ot+1 ) transforms

q =k,t+1 t+k+1
|k|

t+k+1
Fk =k,t+1 (~ t+1 ,q =k,t+1 ,|k|
)
=k,t+1 ~ t+1
t+k+1
t+k+2
( ,hq =k,t+1 |k|
i,|k+1|
). (A.27)
max Fk+1

t+k+2
|k+1|

Now, apply (A.11) induction hypothesis (A.17) yield
~ q =k,t , +k
|k|





=0,t ~
+k
+k
+k+1
Fk =0,t (~ ,q =k,t ,|k|
) max
Fk+1
( ,hq =k,t |k|
i,|k+1|
).

+k+1
|k+1|

(A.28)
Application lemma A.4 transformed induction hypothesis asserts (A.27)
thereby proves lemma.
Auxiliary Lemmas.
Lemma A.3. If, stage t, i-steps expected return k-steps delayed system
higher (k + 1)-steps delayed system, 1 (i + 1)-steps expected return
k-steps delayed system higher (k + 1)-steps delayed system. is,
t1+k
particular q =k,t = hq =k,t1 |k|
i(ot )
t+k ot
|k|

t+k
t1+k
t+k
Fk =i,t (~ ,q =k,t ,|k|
) = Fk =i,t (~ ,hq =k,t1 |k|
i(ot ),|k|
)
=i,t ~
t+k
t+k
t+k+1
( ,hhq =k,t1 |k|
i(ot ) |k|
i,|k+1|
) (A.29)
max Fk+1

t+k+1
|k+1|

346

fiOptimal Approximate Q-Value Functions Dec-POMDPs

holds,
=i+1,t1 ~ t1
t1+k
t+k
t1+k
( ,hq =k,t1 |k|
i,|k+1|
).
Fk =i+1,t1 (~ t1 ,q =k,t1 ,|k|
) max Fk+1
t+k
|k+1|

(A.30)
Proof. following derivation
t1+k
Fk =i+1,t1 (~ t1 ,q =k,t1 ,|k|
)
h

X
t1+k
t+k
Po (ot ) max Fk =i,t (~ ,hq =k,t1 |k|
i(ot ),|k|
)
=
t+k
|k|

ot



X



Po (o ) max
t+k
|k|

ot

max

t+k
|k+1|

= max

t+k
|k+1|

X

"

Po (ot )

=i,t ~
( ,hhq =k,t1
max Fk+1
t+k+1
|k+1|

"

=i,t ~
( ,hhq =k,t1
max Fk+1
t+k+1
|k+1|

ot
=i+1,t1 ~ t1
Fk+1
( ,hq =k,t1



t1+k
|k|
i(ot )



#

t+k
t+k+1
|k|
i,|k+1|
)

#

t1+k
t+k
t+k+1
|k|
|k+1|
i(ot ),|k+1|
)

t1+k
t+k
|k|
i,|k+1|
)

proves lemma.
Lemma A.4. If, stage
~ ,q =k,t , t+k
|k|

=0,t ~
t+k
t+k+1
t+k
( ,hq =k,t |k|
i,|k+1|
)
Fk =0,t (~ ,q =k,t ,|k|
) max Fk+1
t+k+1
|k+1|

(A.31)

holds, ~ ti ,q =k,ti , ti+k
|k|

=i,ti ~ ti
ti+k
ti+k+1
ti+k
( ,hq =k,ti |k|
i,|k+1|
).
Fk =i,ti (~ ti ,q =k,ti ,|k|
) max Fk+1
ti+k+1
|k+1|

(A.32)
t+k
Proof. (A.31) holds ~ , q =k,t , |k|
, eq. (A.29) satisfied ~ , q =k,t ,
t+k
, lemma (A.3) yields ~ t1 ,q =k,t1 , t1+k
|k|
=1,t1 ~ t1
t1+k
t+k
Fk =1,t1 (~ t1 ,q =k,t1 ,|k|
) max Fk+1
( ,hq =k,t1 t+k1 i,|k+1|
). (A.33)
t+k
|k+1|

point apply lemma again, etc. i-th application lemma yields
(A.32).

347

fiOliehoek, Spaan & Vlassis

References
Aicardi, M., Davoli, F., & Minciardi, R. (1987). Decentralized optimal control Markov
chains common past information set. IEEE Transactions Automatic Control,
32 (11), 10281031.
Altman, E. (2002). Applications Markov decision processes communication networks.
Feinberg, E. A., & Shwartz, A. (Eds.), Handbook Markov Decision Processes:
Methods Applications. Kluwer Academic Publishers.
Amato, C., Bernstein, D. S., & Zilberstein, S. (2006). Optimal fixed-size controllers
decentralized POMDPs. Proc. AAMAS Workshop Multi-Agent Sequential
Decision Making Uncertain Domains (MSDM).
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007a). Optimizing memory-bounded controllers decentralized POMDPs. Proc. Uncertainty Artificial Intelligence.
Amato, C., Carlin, A., & Zilberstein, S. (2007b). Bounded dynamic programming decentralized POMDPs. Proc. AAMAS Workshop Multi-Agent Sequential
Decision Making Uncertain Domains (MSDM).
Arai, T., Pagello, E., & Parker, L. (2002). Editorial: Advances multirobot systems. IEEE
Transactions Robotics Automation, 18 (5), 655661.
Aras, R., Dutech, A., & Charpillet, F. (2007). Mixed integer linear programming exact finite-horizon planning decentralized POMDPs. Proc. International
Conference Automated Planning Scheduling.
Becker, R., Lesser, V., & Zilberstein, S. (2005). Analyzing myopic approaches multiagent communication. Proc. International Conference Intelligent Agent
Technology, pp. 550557.
Becker, R., Zilberstein, S., & Lesser, V. (2004a). Decentralized Markov decision processes
event-driven interactions. Proc. International Joint Conference
Autonomous Agents Multi Agent Systems, pp. 302309.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004b). Solving transition independent decentralized Markov decision processes. Journal Artificial Intelligence
Research, 22, 423455.
Bernstein, D. S. (2005). Complexity Analysis Optimal Algorithms Decentralized
Decision Making. Ph.D. thesis, University Massachusets Amherst.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity
decentralized control Markov decision processes. Mathematics Operations
Research, 27 (4), 819840.
Bernstein, D. S., Hansen, E. A., & Zilberstein, S. (2005). Bounded policy iteration
decentralized POMDPs. Proc. International Joint Conference Artificial
Intelligence, pp. 12871292.
Bertsekas, D. P. (2005). Dynamic Programming Optimal Control (3rd edition)., Vol. I.
Athena Scientific.
348

fiOptimal Approximate Q-Value Functions Dec-POMDPs

Beynier, A., & Mouaddib, A.-I. (2005). polynomial algorithm decentralized Markov
decision processes temporal constraints. Proc. International Joint
Conference Autonomous Agents Multi Agent Systems, pp. 963969.
Beynier, A., & Mouaddib, A.-I. (2006). iterative algorithm solving constrained
decentralized Markov decision processes. Proc. National Conference
Artificial Intelligence.
Binmore, K. (1992). Fun Games. D.C. Heath Company.
de Boer, P.-T., Kroese, D. P., Mannor, S., & Rubinstein, R. Y. (2005). tutorial
cross-entropy method. Annals Operations Research, 134 (1), 1967.
Boutilier, C. (1996). Planning, learning coordination multiagent decision processes.
Proc. 6th Conference Theoretical Aspects Rationality Knowledge,
pp. 195210.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 194.
Chades, I., Scherrer, B., & Charpillet, F. (2002). heuristic approach solving
decentralized-POMDP: assessment pursuit problem. Proc. 2002 ACM
Symposium Applied Computing, pp. 5762.
Cogill, R., Rotkowitz, M., Roy, B. V., & Lall, S. (2004). approximate dynamic programming approach decentralized control stochastic systems. Proc. 2004
Allerton Conference Communication, Control, Computing.
Emery-Montemerlo, R. (2005). Game-Theoretic Control Robot Teams. Ph.D. thesis,
Carnegie Mellon University.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2004). Approximate solutions partially observable stochastic games common payoffs. Proc.
International Joint Conference Autonomous Agents Multi Agent Systems, pp.
136143.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2005). Game theoretic
control robot teams. Proc. IEEE International Conference Robotics
Automation, pp. 11751181.
Fernandez, J. L., Sanz, R., Simmons, R. G., & Dieguez, A. R. (2006). Heuristic anytime
approaches stochastic decision processes. Journal Heuristics, 12 (3), 181209.
Gmytrasiewicz, P. J., & Doshi, P. (2005). framework sequential planning multiagent settings. Journal Artificial Intelligence Research, 24, 4979.
Goldman, C. V., Allen, M., & Zilberstein, S. (2007). Learning communicate decentralized environment. Autonomous Agents Multi-Agent Systems, 15 (1), 4790.
Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange cooperative
multi-agent systems. Proc. International Joint Conference Autonomous
Agents Multi Agent Systems, pp. 137144.
349

fiOliehoek, Spaan & Vlassis

Goldman, C. V., & Zilberstein, S. (2004). Decentralized control cooperative systems:
Categorization complexity analysis.. Journal Artificial Intelligence Research,
22, 143174.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms
factored MDPs. Journal Artificial Intelligence Research, 19, 399468.
Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming partially observable stochastic games. Proc. National Conference Artificial
Intelligence, pp. 709715.
Hauskrecht, M. (2000). Value-function approximations partially observable Markov
decision processes.. Journal Artificial Intelligence Research, 13, 3394.
Hsu, K., & Marcus, S. (1982). Decentralized control finite state Markov processes. IEEE
Transactions Automatic Control, 27 (2), 426431.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting
partially observable stochastic domains. Artificial Intelligence, 101 (1-2), 99134.
Kim, Y., Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2006). Exploiting locality interaction networked distributed POMDPs. Proc. AAAI Spring
Symposium Distributed Plan Schedule Management.
Kitano, H., Asada, M., Kuniyoshi, Y., Noda, I., & Osawa, E. (1997). RoboCup: robot
world cup initiative. Proc. International Conference Autonomous Agents.
Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjoh, A., & Shimada,
S. (1999). Robocup rescue: Search rescue large-scale disasters domain
autonomous agents research. Proc. International Conference Systems,
Man Cybernetics, pp. 739743.
Koller, D., Megiddo, N., & von Stengel, B. (1994). Fast algorithms finding randomized strategies game trees. Proc. 26th ACM Symposium Theory
Computing, pp. 750759.
Koller, D., & Pfeffer, A. (1997). Representations solutions game-theoretic problems.
Artificial Intelligence, 94 (1-2), 167215.
Kuhn, H. (1953). Extensive games problem information. Annals Mathematics
Studies, 28, 193216.
Lesser, V., Ortiz Jr., C. L., & Tambe, M. (Eds.). (2003). Distributed Sensor Networks:
Multiagent Perspective, Vol. 9. Kluwer Academic Publishers.
Littman, M., Cassandra, A., & Kaelbling, L. (1995). Learning policies partially observable environments: Scaling up. Proc. International Conference Machine
Learning, pp. 362370.
Marecki, J., & Tambe, M. (2007). opportunistic techniques solving decentralized
Markov decision processes temporal constraints. Proc. International
Joint Conference Autonomous Agents Multi Agent Systems, pp. 18.
Nair, R., Tambe, M., & Marsella, S. (2003). Team formation reformation multiagent
domains like RoboCupRescue. Proc. RoboCup-2002 International Symposium.
350

fiOptimal Approximate Q-Value Functions Dec-POMDPs

Nair, R., Roth, M., & Yohoo, M. (2004). Communication improving policy computation distributed POMDPs. Proc. International Joint Conference
Autonomous Agents Multi Agent Systems, pp. 10981105.
Nair, R., Tambe, M., & Marsella, S. (2002). Team formation reformation. Proc.
AAAI Spring Symposium Intelligent Distributed Embedded Systems.
Nair, R., Tambe, M., & Marsella, S. (2003a). Role allocation reallocation multiagent
teams: towards practical analysis. Proc. International Joint Conference
Autonomous Agents Multi Agent Systems, pp. 552559.
Nair, R., Tambe, M., Yokoo, M., Pynadath, D. V., & Marsella, S. (2003b). Taming decentralized POMDPs: Towards efficient policy computation multiagent settings.
Proc. International Joint Conference Artificial Intelligence, pp. 705711.
Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed
POMDPs: synthesis distributed constraint optimization POMDPs. Proc.
National Conference Artificial Intelligence, pp. 133139.
Nash, J. F. (1950). Equilibrium points N-person games. Proc. National Academy
Sciences United States America, 36, 4849.
Oliehoek, F., & Vlassis, N. (2006). Dec-POMDPs extensive form games: equivalence
models algorithms. Ias technical report IAS-UVA-06-02, University Amsterdam, Intelligent Systems Lab, Amsterdam, Netherlands.
Oliehoek, F. A., Kooij, J. F., & Vlassis, N. (2007a). cross-entropy approach solving
Dec-POMDPs. Proc. International Symposium Intelligent Distributed
Computing, pp. 145154.
Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. (2007b). Dec-POMDPs delayed communication. Proc. AAMAS Workshop Multi-Agent Sequential Decision
Making Uncertain Domains (MSDM).
Oliehoek, F. A., Spaan, M. T. J., Whiteson, S., & Vlassis, N. (2008). Exploiting locality
interaction factored Dec-POMDPs. Proc. International Joint Conference
Autonomous Agents Multi Agent Systems.
Oliehoek, F. A., & Visser, A. (2006). hierarchical model decentralized fighting large
scale urban fires. Proc. AAMAS06 Workshop Hierarchical Autonomous
Agents Multi-Agent Systems (H-AAMAS), pp. 1421.
Oliehoek, F. A., & Vlassis, N. (2007). Q-value functions decentralized POMDPs.
Proc. International Joint Conference Autonomous Agents Multi Agent
Systems, pp. 833840.
Ooi, J. M., & Wornell, G. W. (1996). Decentralized control multiple access broadcast channel: Performance bounds. Proc. 35th Conference Decision
Control, pp. 293298.
Osborne, M. J., & Rubinstein, A. (1994). Course Game Theory. MIT Press.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). complexity Markov decision processes. Mathematics Operations Research, 12 (3), 441451.
351

fiOliehoek, Spaan & Vlassis

Paquet, S., Tobin, L., & Chaib-draa, B. (2005). online POMDP algorithm complex multiagent environments. Proc. International Joint Conference
Autonomous Agents Multi Agent Systems.
Peshkin, L. (2001). Reinforcement Learning Policy Search. Ph.D. thesis, Brown University.
Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning cooperate via
policy search. Proc. Uncertainty Artificial Intelligence, pp. 307314.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: anytime
algorithm POMDPs. Proc. International Joint Conference Artificial
Intelligence, pp. 10251032.
Puterman, M. L. (1994). Markov Decision ProcessesDiscrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.
Pynadath, D. V., & Tambe, M. (2002). communicative multiagent team decision
problem: Analyzing teamwork theories models. Journal Artificial Intelligence
Research, 16, 389423.
Romanovskii, I. (1962). Reduction game complete memory matrix game.
Soviet Mathematics, 3, 678681.
Roth, M., Simmons, R., & Veloso, M. (2005). Reasoning joint beliefs executiontime communication decisions. Proc. International Joint Conference
Autonomous Agents Multi Agent Systems, pp. 786793.
Roth, M., Simmons, R., & Veloso, M. (2007). Exploiting factored representations decentralized execution multi-agent teams. Proc. International Joint Conference
Autonomous Agents Multi Agent Systems, pp. 467463.
Russell, S., & Norvig, P. (2003). Artificial Intelligence: Modern Approach (2nd edition).
Pearson Education.
Schoute, F. C. (1978). Symmetric team problems multi access wire communication.
Automatica, 14, 255269.
Seuken, S., & Zilberstein, S. (2007a). Improved memory-bounded dynamic programming
decentralized POMDPs. Proc. Uncertainty Artificial Intelligence.
Seuken, S., & Zilberstein, S. (2007b). Memory-bounded dynamic programming DECPOMDPs.. Proc. International Joint Conference Artificial Intelligence,
pp. 20092015.
Sondik, E. J. (1971). optimal control partially observable Markov decision processes.
Ph.D. thesis, Stanford University.
Spaan, M. T. J., Gordon, G. J., & Vlassis, N. (2006). Decentralized planning uncertainty teams communicating agents. Proc. International Joint
Conference Autonomous Agents Multi Agent Systems, pp. 249256.
Spaan, M. T. J., & Melo, F. S. (2008). Interaction-driven Markov games decentralized
multiagent planning uncertainty. Proc. International Joint Conference
Autonomous Agents Multi Agent Systems.
352

fiOptimal Approximate Q-Value Functions Dec-POMDPs

Spaan, M. T. J., & Vlassis, N. (2005). Perseus: Randomized point-based value iteration
POMDPs. Journal Artificial Intelligence Research, 24, 195220.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT
Press.
Szer, D., & Charpillet, F. (2005). optimal best-first search algorithm solving infinite
horizon DEC-POMDPs. Proc. European Conference Machine Learning,
pp. 389399.
Szer, D., & Charpillet, F. (2006). Point-based dynamic programming DEC-POMDPs..
Proc. National Conference Artificial Intelligence.
Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithm
solving decentralized POMDPs. Proc. Uncertainty Artificial Intelligence, pp.
576583.
Tao, N., Baxter, J., & Weaver, L. (2001). multi-agent policy-gradient approach network
routing. Proc. International Conference Machine Learning, pp. 553560.
Varakantham, P., Marecki, J., Yabu, Y., Tambe, M., & Yokoo, M. (2007). Letting loose
SPIDER network POMDPs: Generating quality guaranteed policies.
Proc. International Joint Conference Autonomous Agents Multi Agent
Systems.
Varakantham, P., Nair, R., Tambe, M., & Yokoo, M. (2006). Winning back cup
distributed POMDPs: planning continuous belief spaces. Proc. International Joint Conference Autonomous Agents Multi Agent Systems, pp.
289296.
Wu, J., & Durfee, E. H. (2006). Mixed-integer linear programming transitionindependent decentralized MDPs. Proc. International Joint Conference
Autonomous Agents Multi Agent Systems, pp. 10581060.
Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions multi-agent
cooperation: Model experiments. Proc. International Conference
Autonomous Agents.

353

fiJournal Artificial Intelligence Research 32 (2008) 525-564

Submitted 10/07; published 06/08

Efficiency Envy-freeness Fair Division Indivisible
Goods: Logical Representation Complexity
Sylvain Bouveret

sylvain.bouveret@onera.fr

ONERA Centre de Toulouse.
2, avenue Edouard Belin, BP74025.
31055 Toulouse cedex 4, FRANCE.

Jerome Lang

lang@irit.fr

IRIT-CNRS. 118, route de Narbonne.
31062 Toulouse cedex, FRANCE.

Abstract
consider problem allocating fairly set indivisible goods among agents
point view compact representation computational complexity. start
assuming agents dichotomous preferences expressed propositional formulae. express efficiency envy-freeness logical setting, reveals unexpected
connections nonmonotonic reasoning. identify complexity determining
whether exists efficient envy-free allocation, several notions efficiency,
preferences represented succinct way (as well restrictions problem).
first study problem assumption preferences dichotomous,
general case.

1. Introduction
Allocating goods agents important issue considered different perspectives economics (especially social choice theory) computer science (especially
Artificial Intelligence Operations Research), arises various real-world settings:
auctions, divorce settlements, electronic spectrum frequency allocation, airport traffic management, fair efficient exploitation Earth Observation Satellites (see
survey Chevaleyre, Dunne, Endriss, Lang, Lematre, Maudet, Padget, Phelps,
Rodrguez-Aguilar, & Sousa, 2006, detailed description). general issue also covers
huge variety allocation problems, depending following parameters (see
work Chevaleyre et al., 2006, detailed taxonomy):
nature resources allocated (are divisible not? single-unit
multi-unit?);
nature preferences agents (are numerical simply ordinal?
preferential dependencies goods?)
nature permitted allocations (can goods shared among several agents?
goods allocated? allocations accompanied side payments?);
evaluation quality allocation (Pareto-efficiency, utilitarian egalitarian social welfare etc.);
c
2008
AI Access Foundation. rights reserved.

fiBouveret & Lang

nature process leads allocation (centralized decentralized).
instance, standard combinatorial auctions (Cramton, Shoham, & Steinberg, 2005)
typically correspond indivisible goods (possible multi-unit), numerical preferences
possible dependencies goods, monetary payments, maximization total value
sold items, centralized computation.
paper focus fair division indivisible goods without money transfers1 . Fair
division makes prominent use ex-post fairness criteria equity envy-freeness,
point departs auctions, rather focus kinds fairness (as
well efficiency procedure), truthful mechanisms, fairness
procedure (Brams & Taylor, 1996; Young, 1995). key concept literature
fair division envy-freeness: allocation envy-free agent likes
share least much share agent. Ensuring envy-freeness considered
desirable property; however, envy-freeness alone suffice criterion finding
satisfactory allocations (this especially obvious compulsory allocate
goods: case, allocating anything anyone results envy-free allocation, yet
totally unsatisfactory), therefore paired efficiency criterion,
Pareto optimality maximum social welfare. However, known reasonable
notion efficiency, profiles efficient envy-free allocation exists2
(Brams, Edelman, & Fishburn, 2003). even trivial every good must assigned
someone: case, profiles even envy-free allocation exists.
Another well-known notion fairness (that consider paper, except one
result) Rawlsian egalitarianism, says allocation equitable maximizes
satisfaction least satisfied agent. Unlike envy-freeness, egalitarianism requires
interpersonal comparability preferences.
Whereas social choice theory developed important literature fair division,
computational issues rarely considered. hand, artificial intelligence
studied issues extensively, focused mainly combinatorial
auctions related classical utilitarian problems. Combinatorial auctions, aiming maximizing auctioneers revenue (consisting sum prices paid agents),
specific form allocation process, namely pure utilitarian form money
transfers considerations equity fairness relevant. literature
combinatorial auctions related problems investigating issues compact representation (so allow agents express bids concise way see
work Nisan, 2005, overview) well computational complexity, algorithms,
tractable subclasses approximation. Complexity issues negotiation (where agents
exchange goods means deals) also studied (e.g. Dunne, Wooldridge, &
Laurence, 2005; Chevaleyre, Endriss, Estivie, & Maudet, 2004).
discussion reveals existence gap (summarized Table 1): compact
representation complexity issues fair division received little attention
now, apart recent work (Lipton, Markakis, Mossel, & Saberi, 2004)
1. Note possibility money transfers reintroduces divisibility extent, considering money
particular divisible good.
2. Consider example situation single item two agents want it:
case, allocation either efficient envy-free (when item given one two
agents), envy-free efficient (if item allocated anyone).

526

fiEfficiency Envy-freeness Fair Division Indivisible Goods

studies approximation schemes envy-freeness. need compact representation
arises following dilemma, formulated several social choice theorists: either (a)
allow agents express possible preference relation set subsets items,
end exponentially large representation (which actually happens example3
work Herreiner & Puppe, 2002); (b) severely restrict set expressible
preferences, typically assuming additive independence items, design
procedures agents express preferences single items, preferences
extended sets items assuming additivity, thus giving possibility
expressing preferential dependencies complementarity substitutability effects
among items; path followed Brams et al. (2003), Brams King (2005)
Demko Hill (1998). Yet, advocate paper, conciliating conciseness
expressivity possible, means compact representation.
axiomatic study
auctions
(and related problems)

economics

fair division

economics
(especially social choice)

computational study
computer science
(especially AI)
?

Table 1: Computational issues fair division.
works fair allocation indivisible items, focus joint search
envy-freeness efficiency. impossibility guarantee existence efficient
envy-free allocation implies determining whether exists allocation
crucial task, since positive answer leads choose allocation whereas negative
answer calls relaxation one criteria, investigated papers
(Lipton et al., 2004; Chevaleyre, Endriss, & Maudet, 2007b).
consider problem determining whether exists efficient envy-free
allocation point view compact representation computational complexity.
First, since cases agents preferential dependencies (or synergies)
goods, raise issue fair division problem indivisible goods
expressed. focus first simple case agents dichotomous preferences,
is, simply express partition satisfactory unsatisfactory shares.
interest restriction spite expressivity loss imposes,
shown less complex general case, much simpler expose.
Dichotomous preferences considered social choice contexts,
Bogomolnaia, Moulin, Stong (2005) fair division context, course approval
voting (Brams & Fishburn, 1978), every voter specifies dichotomous preference
set candidates. natural representation dichotomous preference (with
preferential dependencies formulae otherwise problem trivial) single
3. Quoting work Brams et al. (2003): Herreiner Puppe (...) assume person
linear preference order 2B . allows complementarity substitutability effects among items
(...). view interdependencies may beset subset evaluations (...), procedures Herreiner
Puppe offer creative way dealing subset preference. hand, sheer number
subsets (more million n = 20) presumption clear preference subsets,
could detract practicability procedures.

527

fiBouveret & Lang

propositional formula, variables correspond goods. Expressing envy-freeness
efficiency within logical representation reveals unexpected connections nonmonotonic
reasoning; issue addressed Section 3.
following Sections devoted detailed complexity study following problem: given fair division problem, exist efficient end envy-free allocation?. latter problem studied different notions efficiency various
restrictions. start (in Sections 3 4) assuming preferences dichotomous,
identify complexity existence envy-free Pareto-efficient allocation, turns p2 -complete. consider several restrictions
latter problem, namely, (a) fixing number agents two; (b) forcing agents
identical preferences; (c) restricting syntax propositional formulae expressing preferences agents. study variations problem obtained
replacing Pareto-efficiency notions efficiency, namely: (a) asking complete
allocations (such every good allocated agent); (b) requiring maximum
number agents satisfied. Section 5 consider general problem
obtained removing assumption preferences dichotomous. problem
becomes dependent choice particular language compact preference
representation, pick particular one (weighted propositional formulae) extends
simple way pure propositional representation considered Section 3, identify
complexity existence envy-free efficient allocations, several notions
efficiency. Finally, Section 6 sum contributions discuss related work
issues.

2. Background
section provide basic concepts definitions use along
paper.
2.1 Fair Division Problems
Definition 1 (Fair division problem) fair division problem4 tuple P = hI, X, Ri

= {1, . . . , N } set agents;
X = {x1 , . . . , xp } set indivisible goods;
R = hR1 , . . . , RN preference profile, Ri reflexive, transitive
complete relation 2X .
Ri preference relation agent i. ARi B alternatively denoted Ri (A, B)
B; write B (strict preference) (A B B A) B
(indifference) (A B B A).
addition, Ri said monotonic A, B, B X implies
B A. R = hR1 , . . . , RN monotonic Ri monotonic every i.
4. following, use indifferently terms fair division resource allocation.

528

fiEfficiency Envy-freeness Fair Division Indivisible Goods

Definition 2 (Allocation) allocation P = hI, X, Ri mapping : 2X
j 6= i, (i) (j) = . every x X exists
x (i) complete allocation.
words, possible framework give good different
agents time, possible throw away goods. paper,
focus especially two desirable properties allocations: Pareto-efficiency
envy-freeness.
Definition 3 Let , 0 two allocations. dominates 0 (a) i, (i)
0 (i) (b) exists (i) 0 (i). (Pareto-) efficient
0 0 dominates .
Definition 4 allocation envy-free (i) (j) holds
j 6= i.
2.2 Propositional Logic
Let V finite set propositional variables. LV propositional language generated
V , usual connectives , Boolean constants > usual
way5 .
interpretation LV element 2V , i.e., truth assignment symbols:
x V , x (resp. x 6 ) means assigns x true (resp. false).
od() = {M 2V | |= } set models (the satisfaction relation |=
defined usual, well satisfiability logical consequence).
literal formula LV form x form x, x V . formula
negative normal form (or NNF) negation symbol appears
literals. formula turned polynomial time equivalent NNF
formula. instance, (b c) NNF equivalent NNF formula
(b c).
formula positive contains occurrence negation symbol. instance,
(b c) (a b) positive, whereas (b c) (a c) (a b) are.
> considered positive well.
Let LV . V ar() V set propositional variables appearing .
instance, V ar((a c) (a b)) = {a, b, c} V ar(>) = .
V
Lastly, = {1 , . . . , n } finite
W set formulae = 1 . . . n
conjunction formulae S, = 1 . . . n disjunction formulae
S.
2.3 Computational Complexity
paper refer complexity classes located polynomial hierarchy.
BH2 (also referred DP) class languages form L1 L2 L1
NP L2 coNP. p2 = PNP class languages recognizable deterministic
Turing machine working polynomial time using NP oracles. Likewise, p2 = NPNP . p2 =
5. Note connectives allowed; important definition positive formulae
(to come).

529

fiBouveret & Lang

p2 [O(log n)] subclass p2 problems need logarithmic number
oracles. See instance book Papadimitriou (1994) details.

3. Fair Division Problems Dichotomous Preferences: Logical
Representation
start considering full detail case preferences dichotomous.
Definition 5 Ri dichotomous exists subset Goodi 2X
A, B X, B Goodi B
6 Goodi . R = hR1 , . . . , RN
dichotomous every Ri dichotomous.
obvious way representing dichotomous preferences compactly, namely
propositional formula (for agent i) language LX (a propositional symbol
vx good x) od() = Goodi . Formally:
Definition 6 Let Ri dichotomous preference 2X , Goodi associated subset
2X , propositional formula propositional language LX . say
represents Ri od(i ) = Goodi .
Clearly,
preference Ri formula representing Ri : =
Vfor dichotomous
V
v

v
x . Furthermore, formula unique logical equivAGoodi
xA x
x6A
alence.
W

Example 1 X = {a, b, c} Goodi = {{a, b}, {b, c}}. Note Ri monotonic.
= (a b c) (a b c) represents Ri . 0i = b ((a c) (a c)),
logically equivalent .
easy yet useful result:
Proposition 1 Let Ri dichotomous preference 2X . following statements
equivalent:
1. Ri monotonic;
2. Goodi upward closed, is, Goodi B imply B Goodi .
3. Ri representable positive propositional formula.
Proof (1) (2). Suppose Ri monotonic, let Goodi B A. must
B A, therefore B Goodi (since Goodi ).
(2) (3). Suppose Goodi upward closed,
consider
`V set
min (Goodi ) inclusionW
minimal sets Goodi . formula = Amin (Goodi )
xA vx represents Ri following
reasons. B Goodi , set min (Goodi ) B. Thus corresponding
conjunction satisfied, satisfied. Conversely, set B 6 Goodi ,
min (Goodi ) B. Therefore, none terms satisfied,
satisfied. Moreover, clearly positive propositional formula.
(3) (1). Suppose Ri representable positive propositional formula , let
B two sets items B. 6 Goodi clearly B A. Goodi ,
od(i ). Since positive, B od(i ) also. Therefore B Goodi , finally B A.


530

fiEfficiency Envy-freeness Fair Division Indivisible Goods

on, assume allocation problems P represented propositional
form, namely, instead I, X R specify h1 , . . . , N i. X obviously
determined h1 , . . . , N i. following, also write propositional variables
corresponding items x instead vx short, since often unambiguous.
Let P = h1 , . . . , N allocation problem dichotomous preferences;
N , rewrite obtained replacing every variable x
new symbol xi . instance, 1 = (b c) 2 = 1 = a1 (b1 c1 )
2 = a2 d2 .
Example 2 Consider following allocation problem: 1 = (b c), 2 =
3 = ab (therefore = {1, 2, 3} X = {a, b, c}). formulae positive, therefore
preferences monotonic. instance, Good1 set composed supersets {a}
supersets {b, c}.
1 = a1 (b1 c1 );
2 = a2 ;
3 = a3 b3
N , let Xi = {xi , x X}. allocation standard allocation problem
P corresponds model V = X1 . . . XN satisfying one xi x X.
terms, bijective mapping
set possible allocations
V
V
models following formula: P = xX i6=j (xi xj ).
V
allocation required complete, P replaced C
P = P xX (x1
. . . xn ). rest unchanged.
Let V = {xi | = 1, . . . , N, x X}. interpretation od(P )
never case xi xj simultaneously true 6= j, therefore map
od(P ) allocation simply defined (i) = {x | |= xi }. mapping
obviously bijective; write F () model od(P ) corresponding allocation
, course F 1 (M ) allocation corresponding interpretation od(P ).
Example 2 (continued) allocation problem Example 2, have:
P = (a1 a2 ) (a1 a3 ) (a2 a3 )
(b1 b2 ) (b1 b3 ) (b2 b3 )
(c1 c2 ) (c1 c3 ) (c2 c3 )
interpretation sets a1 , b3 , c1 true clearly model
P . corresponds allocation F 1 (M ) = , (1) = {a, c}, (2) =
(3) = {c}.
3.1 Envy-freeness
show search envy-free allocation mapped model finding
problem. Let j|i formula obtained substituting every symbol xi
xj : instance, 1 = a1 (b1 c1 ) 2|1 = a2 (b2 c2 ). Notice obviously,
i|i = .
first give following lemma, easy yet useful:
531

fiBouveret & Lang

Lemma 1 i, j, (j) Goodi F () |= j|i .
particular, = j (i) Goodi F () |= .
Proof definition F , (j) Goodi {x | F () |= xj } Goodi , is, {x | F () |=
xj } |= . latter relation equivalent {xi | F () |= xj } |= , finally {xj | F () |=
xj } |= j|i definition j|i , deduce result.


Using lemma, map envy-freeness property satisfiability
logical formula:
Proposition 2 Let P = h1 , . . . , N allocation problem dichotomous preferences propositional form, formulae j|i mapping F defined above.
Let



^
^

P =
j|i
i=1,...,N

j6=i

envy-free F () |= P .
Proof Let allocation. envy-free pair (i, j), 6= j
(j) (i), (j) Goodi (i) 6 Goodi , turn equivalent F () |= j|i

F () 6|= lemma 1. Therefore envy-free F () |= P .

intuitive meaning result allocation envy-free if,
every agent i, either satisfied share (i), envies one,
is, every j, would satisfied js share either.
search envy-free allocations thus reduced model finding problem:
{F 1 (M ) | |= P P } set envy-free allocations P. Note that, importantly,
P P polynomial size (precisely, quadratic) size input data.
problem existence envy-free allocation without property
interesting, allocation always exists : suffices consider allocation gives empty share everyone. However,
problem deciding whether exists envy-free allocation satisfying
property expressible polynomial size formula (e.g. completeness) reduced
satisfiability problem;
problem finding (resp. counting) envy-free allocations comes
problem finding (resp. counting) models P P .
Example 2 (continued) allocation problem Example 2, have:
P =

((a1 (b1 c1 )) ((a2 (b2 c2 )) (a3 (b3 c3 ))))
(a2 (a1 a3 ))
((a3 b3 ) ((a1 b1 ) (a2 b2 )))

od(P P ) = {{c1 }, {c1 , b3 }, {c2 , b3 }, {c2 }, {b3 }, {c3 }, }.
therefore 7 envy-free allocations, namely (c, , ), (c, , b), (, c, b), (, c, ),
(, , b), (, , c) (, , ). Note none complete.
532

fiEfficiency Envy-freeness Fair Division Indivisible Goods

3.2 Efficient Allocations
Similarly envy-freeness property, Pareto-efficiency expressed logical
property. logical expression property requires definition maximal consistent subset set formulae.
Definition 7 Let = {1 , . . . , } set formulae
formula.
V
maximal -consistent subset Vif (a) consistent (b)
0 0 0 consistent. Let axCons(, ) set
maximal -consistent subsets . Moreover, write axCons() set
maximal-consistent subsets , is, set axCons(, >).
Proposition 3 Let P = h1 , . . . , N allocation problem. Let P = {1 , . . . , N }.
Pareto-efficient P {i | F () |= } maximal P -consistent
subset P .
Proof Let allocation. Let Sat() set agents satisfied , is, Lemma 1,
Sat(), F () |= P
|= definition V
set {i | F () |= }. Sat(),
V F ()

|


Sat()}
=

{i | F () |= }. Therefore
definition

F
().
Therefore
F
()
|=

{
P
P

V

{i | F () |= } P consistent.
definition, Pareto-dominated allocation 0 Sat( 0 ) )
Sat(). Therefore, Pareto-dominated consistent subset P (corresponding

V
{i | F ( 0 ) |= }) {i | F () |= } S. Moreover, since 0 allocation, P
consistent.
V
Conversely,
{i | F () |= } P consistent,
V let P
0
1
model P . definition, = F (M ) well-defined allocation, 0 (i) Goodi
S. Since {i | F () |= } S, Sat( 0 ) ) Sat(). Therefore Pareto-dominated.

simple result suggests efficient allocations computed logical
expression P problem, namely, computing maximal
P -consistent subsets
V
P ; call {S1 , . . . , Sq }. Si , let Mi = od( Si P ) let = qi=1 Mi .
F 1 (M ) set efficient allocations P . Note general
exponentially many maximal P -consistent subsets P (and therefore exponentially many
efficient allocations). tempered (a) many practical cases
number maximal consistent subsets small; (b) generally asked look
efficient allocations; look one, done computing one
maximal P -consistent subset P .
Example 2 (continued) maximal
P -consistent subsets P S1 =V{1 , 2 },
V




S2 = {1 , 3 } S3 = {2 , 3 }.
S1 VP one model: {b1 , c1 , a2 }.
2 P
two models: {a1 , b3 } {b1 , c1 , a3 }. S3 P one model: {a2 , b3 }. Therefore
four efficient allocations P (bc, a, ), (a, , b), (bc, , a) (, a, b). None
envy-free.
3.3 Efficient Envy-free Allocations
position putting things together. Since envy-free allocations correspond
models P efficient allocations models maximal P -consistent subsets
P , existence efficient envy-free (EEF) allocation equivalent
V
following condition: exists maximal P -consistent subset P
533

fiBouveret & Lang

P P consistent. case, models latter formula EEF allocations.
Interestingly, instance well-known problem nonmonotonic reasoning:
Definition 8 supernormal default theory6 pair = h, = {1 , . . . , },
1 , . . . , propositional formulae. propositional formula skeptical
consequence D, denoted | , axCons(, )
V
|= .
Proposition 4 Let P = h1 , . . . , N fair division problem. Let DP = hP , P i.
exists efficient envy-free allocation P DP 6| P .
Proof Let P = h1 , . . . , N fair division problem. Let Pareto-efficient envy-free
V
allocation, = {i | F () |= }.
2. also F () |=
V F () |= P Proposition
V
definition
S, F () |= P , proves P consistent, or,
V
terms, 6|= P . Moreover,
V maximal P -consistent subset P Proposition 3. Thus
axCons(P , P ), P 6|= P , implies hP , P 6| P definition 6| .

set axCons(P , P )
V Conversely, suppose hP , P 6| P .
P V
P model . Proposition 3, F 1 (M ) Pareto-efficient allocation (since
model P ), Proposition 2, F 1 (M ) envy-free (since model P ).


somewhat unexpected connection nonmonotonic reasoning
several impliV
cations. First, EEF allocations correspond models
P P
axCons(P , P ); however, axCons(P , P ) may exponentially large, argues avoiding start computing efficient allocations filtering
envy-free, rather compute EEF allocations single step, using default reasoning algorithms. Thus, fair division may benefit computational work default logic
connected domains belief revision answer set programming (Baral, 2003;
Gebser, Liu, Namasivayam, Neumann, Schaub, & Truszczynski, 2007). Moreover, alternative criteria selecting extensions default reasoning (such cardinality, weights
priorities) correspond alternative efficiency criteria allocation problems.

4. Allocation Problems Dichotomous Preferences: Complexity
study section complexity problem existence EEF
allocation restrictions, case preferences dichotomous,
several notions efficiency.
4.1 Complexity General Problem
known skeptical inference p2 -complete (Gottlob, 1992); now, Proposition
4, problem existence EEF allocation reduced complement
skeptical inference problem, immediately tells p2 . Less obviously,
show complete class, even preferences required monotonic.
prove hardness, use following restriction skeptical inference problem:
Problem 1: restricted skeptical inference (rsi)
INSTANCE: set propositional formulae = {1 , . . . , n }.
QUESTION: maximal-consistent subsets contain 1 ?
6. Supernormal defaults also called normal defaults without prerequisites (e.g. Reiter, 1980).

534

fiEfficiency Envy-freeness Fair Division Indivisible Goods

Proposition 5 problem rsi p2 -complete.
Proof Membership p2 comes easily fact problem rsi restriction
skeptical inference problem, formula infer simply 1 . Hardness comes fact
instance h, , skeptical inference problem, h, |
h{} , | , h{ } {1 , . . . , n }, >i | , maximal
-consistent subsets {, 1 , . . . , n } contain , instance rsi.


Proposition 6 problem eef existence determining whether exists efficient envy-free allocation given problem P monotonic, dichotomous preferences logical form p2 -complete.
show hardness following reduction rsi (the complement problem rsi,
is, one maximal-consistent subset contain 1 ?) eef
existence. Given finite set propositional formulae, let V = V ar() set
propositional symbols appearing , let P() = hI, X, P() following
instance eef existence:
1. = {1, 2, ..., n + 3};
2. X = {v |v V , 1...n} {v |v V , 1...n} {xi |i 1...n + 1} {y};
3. = 1, . . . , n, let obtained following sequence operations:
(i) put NNF form (let i0 result); (ii) every v V , replace, i0 ,
(positive) occurrence v v occurrence v v ; let
formula obtained. Then:
= 1, . . . , n, = xi ,
V



Vn
Vn


n+1 y,
n+1 =
v
v

x
vV
i=1
i=1
n+2 = y,
n+3 = 1 .
prove Proposition 6 using several lemmas.
Lemma 2 allocation P said regular n + 3,
(i) (i),


n, (i) = vV {v , v } {xi };


(n + 1) = vV ,i=1,...,n {v , v } {xn+1 , y};
(n + 2) = {y}.
(n + 3) = (1).
Given allocation , let R defined R (i) = (i) (i).
1. R regular;
2. efficient R efficient;
535

fiBouveret & Lang

3. envy-free R envy-free.
Proof (1) obvious. i, goods outside (i) influence satisfaction
(since appear ), therefore R (i) (i), (2) follows. formulae
positive, preference relations monotonic, therefore (j) R (j) holds i, j.
Now, envy-free i, j (i) (j), therefore R (i) (i) (j) R (j)
therefore R envy-free, (3) follows.


Lemma 3 regular
1. 1 envy n + 3;
2. n + 3 envy 1;
3. 2, . . . , n envy one;
4. n + 1 envy n + 2;
5. n + 2 envy n + 1;
Proof First, note i, j 6= i, envies j (i) |= (j) |= .
1. Let = 1 j {2, . . . , n, n + 2}. 1 envies j, x1 (j). regular, x1 6 (j),
therefore cannot envy j.
2. Since n+3 = 1 , holds agent n + 3.
3. Let {2, . . . , n} j 6= i. envies j (j) |= xi , impossible
xi 6 (j), due regularity .
4. Assume n + 1 envies j j {1, . . . , n,n + 3}. (j) |= n+1 . Since
(j) |= impossible
`V n
`V n

V
n+1


(because regular), (j) |=

x
, thus (j) |= xn+1
v

v
vV
i=1
i=1
impossible well, since regular.
5. Let = n + 2 j {1, . . . , n, n + 3}. envies j (j) |= y, impossible
regular.


Lemma 4 Let regular allocation satisfying n + 1 n + 2 leaving 1 n + 3
unsatisfied. Let () interpretation V obtained by: v V ,
() |= v (i.e., v ()) n + 1 receives v 1 , . . . , v n , () |= v otherwise, i.e.,
n + 1 receives v 1 , . . . , v n . efficient envy-free () 6|= 1 .
Proof Let regular allocation satisfying n + 1 n + 2. Since satisfies n + 2, (n + 2).
Now, satisfies n + 1 without giving y, therefore, v, n + 1 receives either v
v s. shows definition () well-founded.
assume efficient envy-free, suppose () |= 1 . One
agents 1
1
n +
3

satisfiable
without
spoiling

agent
j

6
{1,
n
+
3},

giving

{x
}

(v | () |=



v ) (v | () |= v ). since efficient must satisfy least one 1 n + 3. cannot
satisfy simultaneously (because x1 ). Thus one among 1 n + 3 satisfied ,
one envying her. contradicts envy-freeness , thus proving () 6|= 1 .


Lemma 5 interpretation V , let us define : 2X by:
1, . . . , n, (i) = {v | |= v} {v | |= v} {xi };
(n + 1) = {xn+1 } {v | |= v, = 1, . . . , n} {v | |= v, = 1, . . . , n};
(n + 2) = {y}
536

fiEfficiency Envy-freeness Fair Division Indivisible Goods

(n + 3) = .
Then:
1. well-defined regular allocation satisfying n + 1 n + 2;
2. (M ) = (M (M ) obtained Lemma 4).
3. 1, . . . , n, satisfies |= .
4. efficient satisfies maximal consistent subset .
Proof
1. One easily check give good one individual,
give agent set items (i). Therefore
well-defined regular allocation. allocation obviously satisfies n + 1 n + 2.
2. |= v (n + 1) contains {v | = 1, . . . , n} therefore (M ) |= v. case
|= v similar.
3. Let 1, . . . , n. Since gives xi i, satisfies F (M (i)) |= ,
equivalent |= .
4. point 3, {i | satisfies i} = {i | |= } {n + 1, n + 2} (obviously, n + 3
satisfied). Now, since preferences dichotomous, allocation efficient
set individuals satisfies maximal respect inclusion. Therefore, efficient
satisfies maximal consistent subset .


Lemma 6 Let regular efficient allocation satisfying n + 1 n + 2. ()
satisfies maximal consistent subset .
Proof regular satisfies n+1 n+2, therefore obviously (n+2) = {y} lemma 4 ()
well-defined. consider allocation () , defined like previous lemmas.
() (n + 1) = {xn+1 } {v |M () |= v} {v |M () |= v} = {xn+1 } {v |{v 1 , . . . , v n }
(n + 1)} {v |{v 1 , . . . , v n } (n + 1)}. Since n + 1 satisfied , (n + 1) must contain {xn+1 },
assert () (n + 1) (n + 1).
Let {2, . . . , n}. Since regular, (i) (i). Since () complete allocation
definition, regular lemma 5, (i) () (i) () (n + 1). Since () (n + 1) (n + 1)
(i) () (i) (n + 1), thus (i) () (i) (n + 1). allocation,
course (i) (n + 1) = , follows (i) () (i).
regular, (1)(n+3) (1)(n+3). Since (1) = (n+3), latter inclusion
comes (1) (n + 3) (1). Now, (1) () (1) () (n + 1) () (n + 3)
similar reasons {2, . . . , n}, which, together () (n+1) (n+1) () (n+3) = ,
comes (1) () (1) (n + 1), and, (1) (n + 1) = , deduce inclusion
(1) (n + 3) () (1).
prove () efficient. Since preferences monotonic, individuals n + 3
satisfied satisfied 0 well (since 6= n + 3, (i) () (i)).
n + 3 satisfied , immediately deduce () efficient.
n + 3 satisfied , suppose () efficient. case, allocation
0 i, () satisfies implies 0 satisfies particular j 6= 1 0
satisfies j () satisfy j. Clearly, 0 satisfies 1 (since () does), thus j 6= n + 3
(satisfying simultaneously 1 n + 3 impossible). Consider allocation 00 deduced
0 swapping shares 1 n + 3. have, {2, . . . , n + 2}, satisfies
implies () satisfies implies 0 satisfies implies 00 satisfies i. also satisfies n + 3
satisfy 1, 00 . Thus satisfies implies 00 satisfies
i. Moreover, 00 satisfies j {2, . . . , n + 2} (the j above) () not,
therefore neither . proves Pareto-dominated, contradictory
hypotheses.

537

fiBouveret & Lang

Therefore () efficient, conclude, together lemma 5 (point 4), ()
satisfies maximal consistent subset .


Lemma 7 envy-free efficient allocation P() satisfies n + 1 n + 2,
leaves 1 n + 3 unsatisfied.
Proof Suppose satisfy n + 1; 6 (n + 1); now, (n + 2) n + 1 envies
n + 2; 6 (n + 2) efficient giving n + 2 would satisfy n + 2 thus lead
better allocation .
Now, suppose satisfy n + 2, is, 6 (n + 2); (n + 1) n + 2 envies n + 1;
6 (n + 1) again, efficient giving n + 2 would satisfy n + 2 thus lead
better allocation .
Concerning agents 1 n + 3, one notice since identical preferences,
envy-free allocation must either satisfy both, leave unsatisfied. Since cannot
simultaneously satisfied (because x1 ), envy-free allocation leaves unsatisfied.

Lemma 8 exists EEF allocation, exists maximal consistent subset
contain 1 .
Proof Let efficient envy-free allocation. Lemma 2, R regular, efficient envyfree. Lemma 7, R satisfies n + 1 n + 2 leaves 1 n + 3 unsatisfied. Lemma 6,
(R ) satisfies maximal consistent subset , Lemma 4, (R ) 6|= 1 . Therefore {i
| (R ) |= } maximal consistent subset contain 1 .


Lemma 9 exists maximal consistent subset contain 1
exists EEF allocation.
Proof Assume
V exists maximal consistent subset contain 1 , let
model S. point 4 Lemma 5, efficient.
point 1 Lemma 5, regular; Lemma 3, envy-free (i) 1
envy n + 3, (ii) n + 3 envy 1 (iii) n + 1 envy n + 2 (iv) n + 2 envy
n + 1. definition , satisfy n + 3, hence (i) holds. point 3 Lemma 5, 6|= 1
implies satisfy 1, therefore (ii) holds well. finally, point 1 lemma 4, n + 1
n + 2 satisfied , thus (iii) (iv) also hold. Therefore envy-free.


position putting things together proving Proposition 6:
Proof (Proposition 6) Lemmas 8 9, existence maximal consistent subset
contain 1 existence efficient envy-free allocation P() equivalent.
Clearly, P() computed polynomial time. Therefore, P polynomial reduction rsi eef
existence, shows latter problem p2 -hard, therefore p2 -complete.


corollary, p2 -completeness result holds general (not necessarily monotonic)
dichotomous preferences:
Corollary 1 eef existence general, dichotomous preference logical form p2 complete.
4.2 Restrictions Language
consequence high complexity, worth studying restrictions variants
latter problem complexity may fall down. investigate three kind
intuitive restrictions eef existence problem, defined by:
fixing number agents, especially, restricting problem case
2 agents;
538

fiEfficiency Envy-freeness Fair Division Indivisible Goods

forcing agents identical preferences;
restricting syntax agents goals, limiting expression subclasses propositional formulae (e.g. clauses, cubes, . . . ).
Contrary general eef existence problem, complexity restrictions
may sensitive whether preferences monotonic not.
4.2.1 Identical Preferences
start considering identical dichotomous preference profiles, is, agents
preference, i.e. formula .
Proposition 7 eef existence N identical dichotomous, monotonic preferences
NP-complete. result holds even fixed number agents N 2.
Proof preferences identical, envy-free allocation satisfies either agents none; now,
preferences monotonic, always possible satisfy least one agent (by giving items).
Therefore, allocation EEF satisfies agents. Clearly, checked
polynomial time given allocation satisfies agents, hence membership NP.
Hardness comes simple reduction set splitting:
Problem 2: set splitting
INSTANCE: collection C = {X1 , . . . , Xn } subsets finite set S.
QUESTION: partition hS1 , S2 subset C entirely contained either
S1 S2 ?
Given instance hC, Si set splitting, let P(C, S) following eef existence instance:
Agents:
Objects:
Preferences:

2 agents,
one object x(a) per element S,
V
W
V
W
1 = 2 = Xi C aXi x(a) (and usual k = Xi C aXi xk (a)): agent
wants least one object set.

easy see set splitting hS1 , S2 hC, Si, possible find allocation
satisfy two agents, giving respectively x(S1 ) x(S2 ). Conversely, suppose
efficient envy-free allocation , allocation must satisfy two agents. Let
1
hS1 , S2 = hx1 ((1)),
Wx ((2))i. Suppose Xi C Xi S1 Xi S2 (say
e.g Xi S1 ). aXi x2 (a) false, thus making 2 false, contradictory initial
hypothesis. Therefore hS1 , S2 set splitting hC, Si.
clearly polynomial-time reduction, hence NP-hardness eef existence 2 agents
identical dichotomous monotonic preferences.


Unlike Proposition 6, Proposition 7 sensitive whether preferences required
monotonic not.
Proposition 8 eef existence N identical dichotomous preferences coBH2 -complete.
result holds even fixed number agents N 2.
Proof preferences identical, envy-free allocation satisfies either agents none. Let
formula representing one agents preferences (of course identical agents).
satisfiable possible satisfy least one agent. case, allocation EEF
satisfies agents. satisfiable, every allocation EEF. Therefore,
exists EEF allocation 1 . . . . . . N satisfiable unsatisfiable. shows
membership coBH2 .
Hardness comes simple reduction sat-or-unsat. Let = h, pair propositional formulae, assumed w.l.o.g. variables common; map following allocation
problem:

539

fiBouveret & Lang

Agents:
Objects:
Preferences:

2 agents;
2 objects v v 0 per propositional variable v appearing , one object w per propositional variable w appearing , one object y;
1 = 2 = 0 (y ) 0 denotes formula variable v
replaced v 0 .

1. Suppose satisfiable, (corresponding negative instance sat-orunsat). possible satisfy least one agent giving objects v
corresponding variables assigned true model . However possible
satisfy simultaneously second agent unsatisfiable (and way 0 ),
first agent already taken y. Therefore case EEF allocation.
2. Suppose satisfiable unsatisfiable (corresponding positive instance
sat-or-unsat). two cases:
satisfiable. case, matter whether satisfiable not, possible satisfy
two agents satisfying simultaneously first one 0 second one.
Consequently EEF allocation.
unsatisfiable (recall case unsatisfiable satisfiable covered
point 1). case clearly impossible satisfy agent. case, empty
allocation efficient envy-free.
Therefore EEF allocation satisfiable unsatisfiable, proves
proposition.


4.2.2 Two Agents
Note two previous results, hardness result holds fixed number
agents ( 2). Things different Proposition 6, hardness hold
N fixed. Namely, following results.
Proposition 9 eef existence two agents monotonic dichotomous preferences
NP-complete.
Proof Hardness corollary Proposition 7. Membership obtained follows. Let h1 , 2
preference profile, 1 , 2 positive. formulae , formulae defined
earlier (see section 3), well F () allocation . efficient either (a)
satisfies agents, (b) satisfies one agent, 1 2 unsatisfiable, (c)
impossible satisfy even one agent, i.e., 1 2 unsatisfiable. (c) impossible
1 , 2 positive. Now, envy-free |= . Therefore, EEF (a)
|= 1 2 (b) |= (1 2 ) . Thus, exists EEF allocation
( 1 2 ) ( (1 2 ) ) satisfiable, hence membership NP.


Proposition 10 eef existence 2 agents dichotomous preferences coBH2 complete.
Proof Membership comes following reduction sat-or-unsat. Consider instance P
EEF problem 2 agents respectively preferences 1 2 . translate instance
instance h, 0 sat-or-unsat defined follows ( defined usual): = ( 1 2 )
(1 2 ) (1 2 ) 0 = (1 2 ). prove satisfiable 0 unsatisfiable
EEF allocation P.
1. Suppose satisfiable, 0 is. Since satisfiable, valid allocation
satisfy agents (because 1 2 satisfiable). Since 0 satisfiable, possible
satisfy least one agent (because 1 2 satisfiable). deduce every
efficient allocation satisfies exactly one agent. Since (1 2 ) (1 2 ) satisfiable,
od(1 ) = od(2 ) (in words, 1 2 logically equivalent). Let allocation

540

fiEfficiency Envy-freeness Fair Division Indivisible Goods

satisfying agent 1 (the case similar agent 2), F () |= 1 , thus F () |= 2|1 .
Since F () 6|= 2 (because impossible satisfy agents), F () 6|= thus
envy-free. Hence every efficient allocation raises envy: EEF allocation.
2. Suppose satisfiable 0 satisfiable.
0 satisfiable. case easy possible satisfy even one agent.
Therefore every allocation efficient envy free.
satisfiable. two cases:
1 2 satisfiable. case, allocation, corresponding model
1 2 , satisfies agents. allocation clearly EEF.
1 2 satisfiable 1 2 satisfiable (the case 2 1 similar).
case possible satisfy agents. However, since satisfiable,
possible satisfy least one, and, like point 1, every efficient allocation satisfies
exactly one agent. Since 1 2 satisfiable, model 1
model 2 . allocation corresponding model agent 1
satisfied agent 2, latter agent cannot envy first one.
finally proves correctness reduction, clearly polynomial.
Hardness comes directly Proposition 8.



4.2.3 Restriction Propositional Language
previous results, made specific assumptions formulae expressing
agents preferences, except (sometimes) monotonicity. However, restrict set possible propositional formulae, decrease complexity eef existence problem.
investigate first two natural restrictions propositional language: first case
restrict formulae set clauses (disjunctions literals), second case
preferences expressed using cubes (conjunctions literals). restrictions
match two different kinds real-world problems.
case, agents preferences represented clauses, corresponds
kind problems agent wants one single object certain class. One
consider example set patients waiting kidney transplant.
patient needs one kidney, several ones may compatible.
case, agents preferences represented cubes, corresponds
kind problems agent needs single bundle objects. typical
kind problems agents build object want set basic
material (or virtual) components: set objects stand basic components,
cube one agent stands complete device wants.
Making one two assumptions actually decreases complexity eef existence problem: even renders tractable case clauses objects.
Proposition 11 eef existence agents dichotomous preferences restricted
clauses objects solved polynomial time.
Proof first make two additional assumptions prove complexity problem
decrease two assumptions. (1) suppose first agents preferences
monotonic. one agent non-monotonic preferences, means one negative literal
clause. Giving empty share satisfy without spoiling another agent, thus
safely removed problem. (2) also suppose agent wants least one object. one

541

fiBouveret & Lang

agent empty clause goal, means matter gets, cannot satisfied.
Thus also safely removed problem. rest proof, consider
problems verify assumptions (1) (2).
proof based following result: agents preferences disjunction objects
assumptions (1) (2), allocation Pareto-efficient envy-free satisfies
every agent. implication immediate. prove implication need notice
agent satisfied receives least one object clause. take allocation
agent satisfied . either possible satisfy without
spoiling another agent, one object clause given agent want it,
already satisfied another agent: case Pareto-efficient. possible
objects agent disjunction given agents truly
want them: case, agent envies agents, thus envy-free.
Therefore, finding Pareto-efficient envy-free allocation comes finding allocation gives agent one object wants. Thus, instance P eef existence
problem reduced maximal matching problem, bipartite graph GP one
node per agent one side, one node per object side, edge
agent-node object-node agent clause. easily
checked Pareto-efficient envy-free allocation matching size
n GP . latter problem solved time O(nm) (Ford & Fulkerson, 1962),
size biggest disjunction.


investigate case agents preferences cubes objects.
surprisingly, case harder previous one, remains NP.
Proposition 12 eef existence agents dichotomous preferences restricted
cubes objects NP-complete. result holds require preferences
monotonic.
Proof proof organized follows. first prove membership NP without assumption
monotonicity preferences. show hardness monotonic case.
first introduce additional notations. following, denote Obj + (i) (resp.

Obj (i)) set objects appearing positive (resp. negative) literals agent cube. Let
allocation. said minimally regular i, either (i) = Obj + (i) (i) = .
given allocation , denote R corresponding minimally regular allocation, is,
allocation i, R (i) = Obj + (i) 6 (i), R (i) = Obj + (i) Obj + (i) (i).
+
also write
Sat() set agents satisfied : Sat(M R ) = {i | R (i) = Obj (i)}),
Allocated() = iI (i) (the set objects allocated agent).
following result:
Lemma 10 Let allocation. have:
R minimally regular;
Sat() Sat(M R );
Pareto-efficient, R also Pareto-efficient.
Proof
agent i, R (i) = R (i) = Obj + (i) definition R . Therefore
R minimally regular.
Let allocation, let agent. satisfied , Obj + (i) (i)
Obj (i) (i) = . definition R , R (i) = Obj + (i), thus still
Obj + (i) (i) Obj (i) (i) = , therefore agent still satisfied R .
proves Sat() Sat(M R ).
Suppose Pareto-efficient, suppose 0 Pareto-dominates
R . Sat() Sat(M R ) ( Sat( 0 ), contradicts fact
Pareto-efficient. proves third point.


542

fiEfficiency Envy-freeness Fair Division Indivisible Goods

Lemma 11 minimally regular allocation R Pareto-efficient
(a) 6 Sat(M R ) (b) Obj + (i) Allocated(M R ) = .
Proof Let R minimally regular allocation, suppose
6 Sat(M R ) Obj + (i) X \ Allocated(M R ). allocation 0 j 6=
0 (j) = R (j) 0 (i) = Obj + (i) well-defined (since Obj + (i) among set unallocated
objects R ), Pareto-dominates R , since agents satisfied R also satisfied
0 , satisfied 0 whereas R .
Conversely, suppose R Pareto-efficient, let 0 Pareto-efficient alloca0
tion Pareto-dominates R . Lemma 10,
R Pareto-efficient, also Pareto0
dominates R . Sat(M R ), R (i) = R (i) = Obj + (i) since two allocations
0
minimally regular, agent satisfied R also satisfied
R . Moreover,
0
+
j 6 Sat(M R ) R (j) = Obj (j). Since Allocated(M R ) = iSat(M R ) Obj + (i)

0
+
+
since
R (j) X \
iSat(M R ) Obj (i), Obj (j) X \ Allocated(M R ),
finally proves lemma.

two lemmas provide procedure check given allocation Pareto-efficient: first
compute R (which done polynomial time). Lemma 10, Sat() Sat(M R ).
inclusion strict (that is, Sat() ( Sat(M R )), obviously Pareto-efficient since R
Pareto-dominates it. Otherwise, checking Pareto-efficient comes checking R
Pareto-efficient, comes down, according Lemma 11, n set inclusion tests. Now, checking
envy-free still polynomial. Hence problem NP.
prove hardness problem focusing monotonic preferences (that is,
Obj (i) = i). hardness proof, need additional lemmas.
Lemma 12 Let allocation suppose agents monotonic preferences.
envy-free R envy-free.
Proof Let allocation. Lemma 10, Sat() Sat(M R ). Conversely, let Sat(M R ).
R (i) (i), proves Sat(), deal monotonic preferences.
suppose envies j R . satisfied R , thus neither satisfied
. Since R (j) (j) agent preferences monotonic, still envy j . Thus
envy-free, R .

important corollary lemma deal monotonic cubes, existence
Pareto-efficient envy-free allocation equivalent existence minimally regular Paretoefficient envy-free allocation. Therefore restrict existence problem minimally
regular allocations.
Lemma 13 Let j two different agents (and still suppose agents monotonic
preferences). (there exists minimally regular allocation R envies j)
Obj + (i) Obj + (j).
Proof Let R minimally regular allocation, suppose envies j. obviously
satisfied j is; hence R (j) = Obj + (j). Since envies j, thus directly
Obj + (i) Obj + (j).
Conversely, suppose Obj + (i) Obj + (j). allocation R gives Obj + (j)
j nothing agents clearly minimally regular, also obviously
envies j.

introduce NP-complete problem use prove NP-hardness (in monotonic
case):
Problem 3: exact cover 3-sets (Karp, 1972)
INSTANCE: set size 3q, collection C = hS1 , . . . , S|C| 3-element subsets
QUESTION: C contain exact cover S, i.e. sub-collection C 0 C every element
occur exactly one member C 0 ?
Given instance hS, C = hS1 , . . . , S|C| ii exact cover 3-sets (we assume w.l.o.g.
Si different), let P(S, C) following eef instance:

543

fiBouveret & Lang

Agents:

set |C| + 2|S| agents = I1 I2 , I1 = {1, . . . , |C|} I2 = {|C| + 1, . . . , |C| +
2|S|},

Objects:

set 2|S| items X = X1 X2 , X = {x1 , . . . , x|S| } X 0 = {x01 , . . . , x0|S| },
pair (xi , x0i ) corresponding different element ai S,
V
agent I1 , = aj Si xj , k {1, . . . , |S|}, |C|+2k1 =
|C|+2k = xk x0k .

Preferences:

words, first |C| agents preferences correspond sets collection C, last
2|S| agents gathered pairs, member pair preferences
member.
Since Si different size 3, 6= j, Si 6 Sj , thus Obj + (i) 6 Obj + (j).
definition preferences, also Obj + (i) 6 Obj + (j) (i, j) I1 I2
(i, j) I2 I1 well. Hence, Lemma 13, potential source envy instance
comes agent I2 envying partner. Since impossible satisfy two agents
pair time, allocation envy-free satisfy agent
I2 .
Lemma 11, minimally regular allocation R Pareto-efficient
6 Sat(M R ) Obj + (i) X \ Allocated(M R ). Therefore minimally regular allocation R
Pareto-efficient envy-free k {1, . . . , |S|} R (|C| + 2k 1) =
{xk , x0k } R (|C| + 2k) = {xk0 , x0k0 }, k0 {1, . . . , |S|} {xk0 , x0k0 }
X \ Allocated(M R ) (this last condition comes xk0 6 Allocated(M R ), since xk0 x0k0 must
allocated together minimally regular allocation). Finally, R Pareto-efficient envy-free

kS {1, . . . , |S|}, I1 xk R (i), is,
iI1 R (i) =
aj {xj }.
Let R minimally regular allocation. define sub-collection g(M R )
g(M R ) = {Si C | R (i) = Obj + (i)}. mapping g clearly defines bijection set
non-overlapping
sub-collections


set minimally regular allocations, one notice
iI1 R (i) =
Sj g(M R )
ak Sj {xk }.
1
0
Let C 0 CSis exact cover
S.
g (C ) exists
valid minimally regular allocation.
1
0
also iI1 g (C )(i) = Sj C ak Sj {xk } = aj {xj } C 0 cover. Therefore

g 1 (C 0 ) Pareto-efficient envy-free previous result.
Conversely, suppose minimally regular Pareto-efficient andSenvy-free allocation
R .

g(M R ) non-overlapping sub-collection C, Si g(M R ) aj Si {aj } =



{aj } = xj {M R (i) | iI1 } {aj }. R Pareto-efficient envy-free,
SiI1 xj R (i)S


iI1 R (i) =
aj {xj }, hence
xj {M R (i) | iI1 } {aj } =
xj {xj | aj S} {aj } = S. proves
g(M R ) exact cover S.
reduction clearly polynomial; hence NP-hardness.


previous proof (and especially Lemma 13) sheds light hard case
eef existence problem conjunctive preferences. instance problem,
possible source envy comes Obj + (i) Obj + (j) deal minimally
regular allocations. case, cannot satisfy j without raising envy i.
Obj + (i) ( Obj + (j), one remove j instance, satisfied,
envy (note true non-monotonic preferences, one
give object agent want prevent envy another agent,
thus cannot restrict problem minimally regular allocations).
pair (i, j), 6= j, Obj + (i) = Obj + (j), one remove every
agent j 6= Obj + (j) ( Obj + (i). that, easy
see every minimally regular allocation envy-free. Since least one Paretoefficient minimally regular allocation, guarantees existence Pareto-efficient
envy-free allocation case, may preferences monotonic not. formally:
544

fiEfficiency Envy-freeness Fair Division Indivisible Goods

Proposition 13 always exists efficient envy-free allocation instance
eef existence problem agents dichotomous preferences restricted
cubes objects following condition holds:

(i, j) 2 , 6= j, (i = j ) k k 6= i, k 6= j Obj + (k) ( Obj + (i) . (1)
course, equivalence condition 1 existence Paretoefficient envy-free allocation7 , may happen that, given two agents j
preferences, satisfaction one two agents prevented
another agent k Obj + (i) Obj + (k) 6= , Obj + (k) 6 Obj + (i).
hard case: two agents j identical preferences, agent k
Obj + (k) ( Obj + (i), may however possible prevent j satisfied
satisfying another agent k 0 Obj + (i) Obj + (k 0 ) 6= , following example
shows: 1 = 2 = x1 x2 , 2 = x2 x3 . Satisfying agent 2 leads efficient
envy-free allocation, whereas condition 1 hold.
Proof (Proposition 13) following denote I1 set agents whose preferences inclusion-minimal, is, I1 = {i | @j Obj + (j) Obj + (i)}. denote
I2 agents: I2 = \ I1 .
simple procedure finding Pareto-efficient envy-free allocation: greedily select
maximal set agents I1 , agent receives Obj + (i) (until becomes
impossible select another unsatisfied agent I1 ).
allocation resulting procedure minimally regular, Lemma 13 clearly
envy-free (by definition I1 ). Moreover, suppose 6 Sat() Obj + (i) X \
Allocated(). 6 I1 , since case, procedure would selected therefore
satisfied. also 6 I2 , case, j I1
Obj + (j) Obj + (i), therefore Obj + (j) X \ Allocated(), impossible
reasons above. Therefore, also Pareto-efficient Lemma 11.


investigated two natural restrictions propositional language used
dichotomous preferences, introduce general result, based fact
hardness result Proposition 6 clearly linked NP-completeness sat
problem. happens restrict expression preferences certain class C
sat(C) solved polynomial time ? general case, additional
assumption made C cannot say anything complexity eef
existence problem complexity general problem. However, C
also closed conjunction, complexity falls NP:
Proposition 14 Let C class propositional formulae closed conjunction
sat(C) P. eef existence agents dichotomous preferences
expressed formulae class C NP.
Proof Membership NP comes fact that, non-deterministically guessed
allocation , checking envy-free Pareto-efficient done polynomial time. Given
allocation, checking envy-free done time O(nm) (where length biggest
CNF), checking, unsatisfied agent, would satisfied share
another agent. Given set Sat() ofVthe agents satisfied , checking Pareto-efficiency
comes check \ Sat() jsatI j unsatisfiable. done making
linear number calls sat(C) oracle, since preferences C, since class closed
conjunction. thus proves eef existence formulae C NP.

7. Otherwise, Proposition 12 would false, would proved P = NP.

545

fiBouveret & Lang

corollary, classes propositional formulae sat(C) polynomial,
contain cubes, eef existence problem NP-complete. applies
example class 2-CNF formulae class Horn clauses.
4.3 Alternative Efficiency Criteria
main reason high complexity eef existence problem Paretoefficiency allocation hard check. consequence, complexity decrease
choose alternative notion efficiency. investigate two alternative efficiency
criteria: completeness allocation, maximal number satisfied agents.
First, weaken Pareto-efficiency requiring allocations complete.
Unsurprisingly, makes complexity fall NP.
Proposition 15 problem deciding whether complete envy-free allocation agents
dichotomous preferences exists NP-complete, even 2 agents identical preferences.
Proof Since checking allocation complete done polynomial time, membership
NP straightforward.
prove hardness reduction sat problem. Let propositional formula.
create following instance resource allocation problem : map propositional variable
different object add another object ; two agents preferences,
represented formula y. Obviously, every complete allocation satisfies least one agent (the
one receives y). satisfiable, possible satisfy agent well share
corresponds model : thus case exists complete envy-free allocation.
Conversely, suppose exists complete envy-free allocation. one two agents
must satisfied thanks (since cannot given agents), hence proving satisfiable.


Notice hardness proof longer valid require preferences
monotonic. anonymous referee pointed out, proved reduction
exact-cover-by-3-sets result holds require monotonicity, relaxing
restriction number agents. However, know NP-hardness holds
two agents monotonic preferences (we conjecture does).
Secondly, think looking cardinality-maximal subsets satisfied agents,
instead inclusion-maximal subsets like Pareto-efficiency does.
Proposition 16 problem deciding whether envy-free allocation satisfying maximal number agents monotonic dichotomous preferences exists p2 -complete.
Proof Checking whether exists envy-free allocation satisfying least k agents NP;
therefore, maximal number agents satisfied simultaneously computed
dichotomy within log n NP oracles. suffices, step done, guess allocation
check envy-free satisfies maximal number agents, adding one NP oracle.
Hence membership p2 .
Hardness obtained reduction following problem8 :
Problem 4: max-index-satodd (Wagner, 1990)
INSTANCE: sequence propositional formulae h1 , . . . , n (i unsatisfiable) (i+1
unsatisfiable.)
QUESTION: maximum index satisfiable odd number ?
8. problem referred several times literature, seem name.

546

fiEfficiency Envy-freeness Fair Division Indivisible Goods

First notice complexity latter problem decrease following assumptions:
n even (if not, add formula end sequence);
sets propositional variables formulae pairwise-disjoint (if two formulae
i+1 share variables, transform variable v say copy v 0 :
change (un)satisfiability set propositional variables i+1
disjoint).
Let h1 , . . . , n instance max-index-satodd two additional latter assumptions,
let Vi denote set propositional variables appearing . translate instance
following instance P(1 , . . . , n ):
Agents:
Objects:
Preferences:

2n agents : I1,2 I3,4 In1,n , group I2i1,2i contains four agents
{4i 3, 4i 2, 4i 1, 4i}.
create v Vi (for {1, . . . , n}) four objects xv , xv , yv , yv ,
add n dummy objects dk (k {1, . . . , n});
group I2i1,2i (i {1, . . . , n/2}), preferences agents are:
4i3 = 4i2 = (02i1 d2i1 ) (02i d2i ),
V
4i1 = vV2i1 V2i xv xv ,
V
4i = vV2i1 V2i yv v ,
0k formula k v replaced xv yv , v
replaced xv v .

proof proposition primarily based fact problem split n/2
subproblems concerning agents I2i1,2i :
Lemma 14 denote P|i restriction P(1 , . . . , n ) set agents I2i1,2i
objects want. allocation said splittable 6= j, (I2i1,2i ) (I2j1,2j ) = .
restriction splittable allocation (I2i1,2i ) written |i .
exists envy-free allocation satisfying maximal number agents P(1 , . . . , n )
exists splittable allocation {1, . . . , n/2}, |i envy-free satisfies
maximal number agents P|i .
Proof First, restrict attention regular allocations, regular means, like Lemma 2,
allocation gives object agent wants it. safe reasons
Lemma 2: existence envy-free allocation satisfying maximal number agents
equivalent existence regular envy-free allocation satisfying maximal number
agents. Since sets Vi pairwise-disjoint, two different subproblems P|i P|j share
object, therefore regular allocation also splittable.
0
Let regular allocation. Suppose allocation |i
problem P|i
satisfies agents |i . splittable allocation made sub-allocations |j
0
j 6= |i
valid, regular, satisfies agents . Conversely, suppose
regular allocation 0 satisfies agents . least one
0
strictly agents I2i1,2i satisfied |i
|i . proves regular
allocation satisfies maximal number agents i, |i satisfies maximal
number agents.
Suppose envy-free. obviously |i well. Conversely, suppose
|i envy-free. envy-free, (1) agent envy another agent
group, |i envy-free, (2) agent group envy
agent another group j, since (I2i1,2i ) (I2j1,2j ) = .

interpretation Intk Vk , define following sets objects:
f (Intk ) = {xv | Intk |= v} {xv | Intk 6|= v};
g(Intk ) = {yv | Intk |= v} {yv | Intk 6|= v};
f (Intk ) = {xv | Intk 6|= v} {xv | Intk |= v};

547

fiBouveret & Lang

g(Intk ) = {yv | Intk 6|= v} {yv | Intk |= v}.
Moreover, given two interpretations Int2i1 Int2i respectively V2i1 V2i , write
Int2i1 ,Int2i following allocation P|i :
Int2i1 ,Int2i (4i 3) = f (Int2i1 ) g(Int2i1 ) {d2i1 };
Int2i1 ,Int2i (4i 2) = f (Int2i ) g(Int2i ) {d2i };
Int2i1 ,Int2i (4i 1) = f (Int2i1 ) f (Int2i );
Int2i1 ,Int2i (4i) = g(Int2i1 ) g(Int2i ).
Lemma 15 Let Int2i1 Int2i two respective interpretations V2i1 V2i .
Int2i1 ,Int2i satisfies agents 4i 1 4i;
Int2i1 ,Int2i satisfies 4i 3 Int2i1 |= 2i1 , Int2i1 ,Int2i satisfies 4i 2
Int2i |= 2i ;
Proof Let Int2i1 Int2i two respective interpretations V2i1 V2i .
definition, f (Intk ) contains xv xv v Vk , thus Int2i1 ,Int2i (4i1) contains xv
xv v V2i1 V2i . Therefore agent 4i 1 satisfied Int2i1 ,Int2i (4i 1).
reasoning holds agent 4i.
definition, 2i1 satisfied Int2i1 02i1 satisfied interpretation
defined setting true xv yv (resp. xv yv ) Int2i1 |= v
(resp. Int2i1 6|= v). Thus, Int2i1 |= 2i1 , Int2i1 ,Int2i satisfied 02i1 . Since also
satisfies d2i1 , 4i 3 satisfied Int2i1 ,Int2i . Conversely, 4i 3 satisfied
Int2i1 ,Int2i , obviously 02i1 must satisfied Int2i1 ,Int2i (because 4i 3
receive d2i ), proves 2i1 satisfied Int2i1 . result holds
2i agent 4i 2.

Lemma 16 Consider restricted problem P|i .
neither 2i1 2i satisfiable, interpretations Int2i1 Int2i V2i1
V2i respectively, Int2i1 ,Int2i envy-free satisfies maximal number agents.
2i1 satisfiable, M2i1 model 2i1 , M2i1 ,Int2i satisfies maximal
number agents. Moreover, envy-free allocation satisfying maximal number
agents case.
2i1 2i satisfiable, M2i1 M2i respective models 2i1
2i , M2i1 ,M2i satisfies maximal number agents envy-free.
Proof Suppose neither 2i1 2i satisfiable. allocation |i satisfying 4i 3
(resp. 4i 2) must least one v V2i1 V2i {xv , xv , yv , yv }
|i (4i 3) (resp. |i (4i 2)), otherwise one could deduce model 2i1 2i
|i (4i 3) (resp. |i (4i 2)). Thus agents 4i 4i 1 satisfied case:
maximal number agents possible satisfy 2. Since every allocation form
Int2i1 ,Int2i satisfies agents 4i 1 4i, satisfies maximal number agents
case. also obviously envy-free, since neither d2i d2i1 shares agents 4i 1
4i, thus 2 agents cannot envy them.
Suppose 2i1 satisfiable. allocation satisfying agents 4i 3
4i 2 must satisfy 02i1 one two agents, 02i one (because d2i
d2i1 ). Since 2i satisfiable, case neither 4i 1 4i satisfied |i ,
reasons above. deduce possible satisfy 4
agents. Neither possible satisfy 3 agents 4i 3 4i 2 satisfied. consider
allocation M2i1 ,Int2i , M2i1 model 2i1 . Lemma 15, M2i1 ,Int2i satisfies
3 agents: 4i 3, 4i 1 4i. allocation envy-free, allocation satisfying
maximal number agents case (because either 4i 3 4i 2 remains unsatisfied
allocation, envying partner).
Lastly, suppose 2i1 2i satisfiable, let M2i1 M2i models.
Lemma 15, M2i1 ,M2i satisfies 4 agents, thus satisfying maximal number agents
obviously envy-free.


548

fiEfficiency Envy-freeness Fair Division Indivisible Goods

conclude proof. Lemma 14, exists envy-free allocation satisfying
maximal number agents P(1 , . . . , n ) exists splittable allocation
{1, . . . , n/2}, |i envy-free satisfies maximal number agents P|i . Lemma 15,
exists envy-free allocation |i satisfying maximal number agents P|i either
none two formulae 2i1 2i satisfiable, are. suppose maximum
index j j satisfiable odd number (say 2i 1). case, envyfree allocation satisfying maximal number agents P|i since 2i1 satisfiable 2i not.
Conversely, suppose maximum index j j satisfiable even number (say 2i).
case, envy-free allocation satisfying maximal number agents P|k , since
P|k either two formulae 2k1 2k satisfiable (if k i), none (if k > i).
thus reduction latter problem problem existence envy-free
allocation satisfying maximal number agents. proves p2 -completeness.


5. Non-dichotomous Preferences
consider case preferences longer dichotomous.
5.1 General Logical Preferences
Again, since explicit description preferences exponentially large, need
compact description thereof clear. Many languages exist succinct representation
preference. limit investigation following class languages:
Definition 9 (Compact language logical form) Let L language representing set preference relations set alternatives 2X . L said compact
language logical form :
(a) able express dichotomous preference compactly previous language
introduced, is, language expressing dichotomous preferences propositional
form polynomially reduced L;
(b) comparing two sets goods done polynomial time.
two previous conditions practise restricting, met many
languages succinct representation representation. See instance paper Lang
(2004) survey logical languages compact preference representation. Note
several widely studied representation languages, CP-nets graphical languages, logical form, fail represent preferences expressed
logical formulas within polynomial space9 . Interestingly, Proposition 6 extends
compact representation language logical form:
Corollary 2 eef existence monotonic compact preference logical form p2 complete.
Proof eef existence problem solved using following algorithm:
1. non-deterministically guess allocation ;
2. check envy-free;
3. check Pareto-efficient.
9. natural question complexity fair division problems preferences expressed
languages. left study.

549

fiBouveret & Lang

condition (b), step two done polynomial time, since requires quadratic number
polynomial oracles. condition (b) also, problem checking whether given allocation
Pareto-efficient co-NP. Therefore, previous non-deterministic algorithm uses 1 NP oracle,
runs polynomial-time. Hence membership p2 .
Hardness corollary Proposition 6 together condition (a).


5.2 Numerical Preferences Logical Form
latter result preferences numerical since Pareto-efficiency
envy-freeness purely ordinal notions. Now, preferences numerical, implies
possibility intercomparing aggregating preferences several agents, then, besides
Pareto-efficiency, may consider efficiency based social welfare functions. consider
two classical way aggregating collection utility functions
social welfare function:
Definition 10 (Classical utilitarianism egalitarianism) Given collection individual utility functions hu1 , . . . , un i, i, ui : 2X Z:
?

P classical utilitarian social welfare function function defined sw : 7
ui ((i));

egalitarian social welfare function function defined sw(e) : 7 mini ui ((i));
Maximizing egalitarian social welfare function often viewed alternative criterion fairness, encoding Rawlsian egalitarian point view (Rawls, 1971). However,
see Proposition 17, egalitarianism (as well classical utilitarianism)
always compatible envy-freeness. link two alternative points
view fairness deeply investigated Brams King (2005).
Since deal anymore ordinal (or dichotomous) preferences,
define precisely mean compact representation numerical preferences.
pick basic numerical language one simple compact languages,
consisting associating numerical weights propositional formulae see e.g. (Chevaleyre,
Endriss, & Lang, 2006), (Ieong & Shoham, 2005) context coalitional games:
Definition 11 (Weighted propositional language) Given set goods X, weighted
propositional language associated X set possible subsets LX Z.
Given set weighted propositional formulae = {h1 , w1 i, . . . , hr , wr i}, utility
function associated is:

1 |= k
X
u : 2
Z
, k =
Pr
0 otherwise.

7
k=1 wk k
Using language, preferences monotonic formulae positive weights
positive.
Now, define notion compact numerical language:
Definition 12 (Compact numerical language logical form) Let L language representing set utility functions set alternatives 2X . L said
compact numerical language logical form :
550

fiEfficiency Envy-freeness Fair Division Indivisible Goods

(a) able express utility function compactly weighted propositional
language, is, weighted propositional language polynomially reduced
L;
(b) computing utility one set goods done polynomial time.
course, since compact numerical language logical form also compact
language logical form, complexity result form Corollary 2 still holds. However,
appears complexity problem deciding whether efficient envy-free
allocation exists decreases Pareto-efficiency replaced weaker notion:
maximization one two latter social welfare functions.
Proposition 17 Given collection utility functions 2R given compact numerical
language logical form:
problem deciding whether exists envy-free allocation among
maximize utilitarian social welfare p2 -complete, even N = 2, even
agents identical preferences.
problem deciding whether exists envy-free allocation among
maximize egalitarian social welfare p2 -complete, even N = 2.
Proof results, membership comes easily fact maximum value social
welfare computed dichotomy set possible social welfare values;
exponentially many, therefore need polynomial number NP oracles this; step
done, suffices guess allocation check envy-free maximizes social
welfare, adding one NP oracle.
Hardness obtained utilitarian egalitarian cases simple reduction instance
fair division problem preferences expressed weighted propositional language
following problem:
Problem 5: max-sat-asgeven (Wagner, 1987)
INSTANCE: propositional formula Conjunctive Normal Form, set propositional
variables V = {v1 , . . . , vn }, weight function w interpretations : V {0, 1},
def P
i1
defined w(I) =
.
I(vi ) 2
QUESTION: maxM model w(M ) even number (in words, v1 falsified model
maximal weight) ?
suppose formula least model 6|= v1 .
change complexity, v1 verified every model , clearly answer problem
max-sat-asgeven no: consequence, every instance h, V without assumption
solved first checking v1 unsatisfiable (which coNP-complete problem), then,
not, solving unsat-or-max-sat-asgeven problem instance least one model
falsifying v1 .
Utilitarian social welfare: instance h, V max-sat-asgeven , create following
instance P(, V ):
Agents:
Objects:
Preferences:

2 agents;
literal vi , create two objects xi x0i , except v1 ,
one object x1 created, add two objects 0 ;
agents 1 2 identical preferences, ask h( y) ( 0
0 ), 2n+1 i, hx1 y, 1i, . . . , hxn y, 2n1 i, hx02 0 , 2i, . . . , hx0n 0 , 2n1 i, formula symbol vi replaced xi , 0 formula
symbol vi (except v1 replaced x1 ) replaced x0i .

551

fiBouveret & Lang

Let (M1 , M2 ) pair models (with possibly M1 = M2 ) M2 6|= v1 . define
allocation M1 ,M2 by: M1 ,M2 (1) = {y} {xi |M1 |= vi } M1 ,M2 (2) = {y 0 } {x0i |M2 |= vi }.
proof based following lemma:
Lemma 17 exists envy-free allocation among maximize utilitarian social welfare
two models M1 M2 (possibly M1 = M2 ) M2 6|= v1 , M1 ,M2
envy-free maximizes utilitarian social welfare.
Proof Let allocation maximizing utilitarian social welfare. Let model
falsifying v1 (our hypothesis least one). F (M,M (1)) |=
F (M,M (2)) |= 0 0 , proves individual utility two agents least 2n+1 .
Hence least one allocation whose utilitarian social welfare greater equal
2n+2 . Therefore, allocation maximizing utilitarian social welfare must
F ((1)) |= F ((2)) |= 0 0 , vice versa. Moreover, either x1 6 (1), x1 6 (2).
Suppose x1 (2): swapping shares agents leads allocation 0
completely equivalent respect Pareto-efficiency envy-freeness, due identical
preferences. therefore assume w.l.o.g x1 6 (2)
Since F ((1)) |= , model M1 (1) = {y} {xi |M1 |= vi } S1 ,
S1 {x1 , x02 , . . . , x0n }. Similarly, model M2 M2 6|= v1 , (2) =
{y 0 } {x0i |i > 1, M2 |= vi } S2 , S2 {x1 , . . . , xn }. consider allocation M1 ,M2 ,
well-defined since M2 6|= v1 . u1 () = u1 (M1 ,M2 ), since x0i satisfy
formula preferences agent 1 without 0 (given agent 2), u2 () = u2 (M1 ,M2 )
reasons. terms, M1 ,M2 gives utility agents. M1 ,M2
thus envy-free maximizes utilitarian social welfare.

Lemma 17, thus restrict problem allocations form M1 ,M2 .
have, M1 M2 defined earlier, u1 (M1 ,M2 ) = 2n+1 + w(M1 ) u2 (M1 ,M2 ) = 2n+1 +
w(M2 ); thus sw? (M1 ,M2 ) = 2n+2 + w(M1 ) + w(M2 ). have: argmaxM ,M sw? (M1 ,M2 ) =
1
2
argmax(M ,M ) {w(M1 )+w(M2 )|M1 6|=v1 M2 6|=v1 } . Given symmetry problem, assume
1
2
M2 satisfy M2 6|= v1 , thus latter allocation becomes: Mopt ,argmaxM {w(M2 )|M2 6|=v1 } ,
2
Mopt model maximal weight.
Suppose Mopt 6|= v1 , allocation maximizing utilitarian social welfare Mopt ,Mopt
clearly envy-free, agents utility. suppose Mopt |= v1 ,
allocation maximizing utilitarian social welfare Mopt ,Mopt0 , Mopt0 model
maximal weight assigns v1 false. w(Mopt0 ) < w(Mopt ), thus u1 (Mopt ,Mopt0 ) >
u2 (Mopt ,Mopt0 ), hence allocation envy-free.
latter reduction clearly polynomial (recall weights 2n+1 encoded using
linear space). proves proposition utilitarian case.
Egalitarian social welfare: instance h, V max-sat-asgeven , create following
instance P(, V ):
Agents:
Objects:
Preferences:

2 agents;
literal vi , create two objects xi x0i , add two objects
y0 ;
preferences agent 1 hx1 , 1i, . . . , hxn , 2n1 i, h y, 2n i, preferences
agent 2 hy 0 , 22n i, hx1 , 1i, formula symbol vi
replaced xi .

Every allocation maximizes egalitarian social welfare must give least utility 22n
agent 2. case, egalitarian social welfare given utility agent 1,
utility cannot greater 22n . Therefore, maximizing social welfare comes maximizing
utility agent 1, words give items corresponding model maximum
weight. v1 set true model, x1 given agent 1, since also given
agent 1, agent 2 could get strictly higher utility agent 1s share. Therefore allocation
envy-free. v1 set false latter model, x1 given agent 1, thus given
agent 2, producing envy-free allocation. Since reduction polynomial, proves hardness
egalitarian case.


552

fiEfficiency Envy-freeness Fair Division Indivisible Goods

notice combination envy-freeness numerical criterion
classical utilitarianism egalitarianism induces complexity gap, since, stated
Bouveret, Fargier, Lang, Lematre (2005), complexity problems maximizing classical utilitarian egalitarian collective utility functions, agents
weighted logical preferences, NP-complete.
previous proof utilitarian case, notice hardness result still
holds require allocation Pareto-efficient instead maximizing utilitarian
social welfare. suggests case language extending weighted propositional
formulae, eef existence problem identical preferences much harder
case agents identical dichotomous preferences. actually following
result:
Proposition 18 Given collection N identical utility functions 2R given compact
numerical language logical form problem deciding whether Pareto-efficient
envy-free allocation exists p2 -complete, even N = 2, even preferences
monotonic.
Proof Since preferences identical, envy-free allocation satisfies agents equally. Thus,
Pareto-efficient envy-free allocation, one, allocation gives everyone utility
, maximal among set allocations satisfy everyone equally. value computed
using polynomial number NP oracles (like previous proof). value
, checking Pareto-efficient envy-free allocation comes check
allocation giving least agents, least + 1 least one agent,
problem coNP, hence adds one call NP oracle.
hardness proof, one may notice reduction one used utilitarian
case proof Proposition 17 works case, allocation Pareto-efficient
envy-free particular problem envy-free maximizes utilitarian social
welfare.


5.3 Additive Numerical Preferences
last case consider case additive numerical preferences. Additive
numerical preferences degenerate case weighted logical preferences,
formulae single positive literals. words, preferences agent given
set pairs hxk , wk i, xk object wk weight (possibly 0)
associated object. utility function associated preferences thus
following:
u : 2X Z
P

7
xk wk .
Notice agents preferences monotonic numbers wk
positive.
preference representation language natural one dealing
resource allocation problems; however, unable express compactly kind dependencies (superadditivity subadditivity) objects. particular,
extend dichotomous preferences. Hence previous hardness results extend
additive preferences. However, since still able compare two alternatives
polynomial time, membership p2 guaranteed.
553

fiBouveret & Lang

intuition problem hard eef existence problem
dichotomous preferences:
Conjecture 1 eef existence additive numerical preferences p2 -complete, even
preferences monotonic.
know problem NP-hard (this implied Proposition 20
presented later) p2 , precise complexity remains open. However, things become
much easier require allocation complete, instead Pareto-efficient.
case already investigated Lipton et al. (2004), following result:
Proposition 19 (Lipton et al., 2004) problem deciding whether exists
complete envy-free allocation agents additive preferences NP-complete, even
preferences monotonic.
restrictions eef existence problem additive preferences worth
studied. First, study dichotomous case restriction identical additive
preferences:
Proposition 20 eef existence N identical additive numerical preferences NPcomplete, fixed N 2. result holds require preferences
monotonic.
Proof Membership easy prove. Since preferences identical (we write hu(x1 ), . . . , u(xp )i
utility vector associated set object), allocation Pareto-efficient gives
object xj u(xj ) > 0 one agent, trashes object xj u(xj ) 0.
Moreover, allocation envy-free agents utility. two latter
properties checked polynomial-time, hence membership NP.
Hardness comes reduction partition:
Problem 6: partition
INSTANCE: finite set size s(a) N
P S.P
QUESTION: subset 0 s(a) = aS\S 0 s(a) ?
given instance hS, si partition problem, create following instance P(S, s)
eef existence problem:
Agents:
Objects:
Preferences:

2 agents;
S, associate object xa ;
two agents preferences identical defined size elements
initial set: u(x(a)) = s(a).

isP
Pareto-efficient
P envy-free allocation P(S, s) allocation
x(1) u(x) = x(2) u(x), is, partition problem returns true. reduction
clearly done polynomial time, proves proposition.


another interesting case case preferences necessary identical,
atomic utilities ui (xj ) either 0 1. words, agent either
wants object want it, agent wants maximize number
desired objects gets.
Proposition 21 eef existence additive 01-preferences (i.e. i, j, ui (xj ) {0, 1})
NP-complete.
554

fiEfficiency Envy-freeness Fair Division Indivisible Goods

Proof Pareto-efficiency easy check case. first safely remove objects
appear anywhere preferences. Afterwards, allocation Pareto-efficient
object xj given agent ui (xj ) = 1, following reasons. () Let
Pareto-efficient allocation, suppose xj given agent,
given agent ui (xj ) = 0. Let k agent uk (xj ) = 1 (there
one since previously trashed undesired objects). giving xj agent k increases
ks utility others utilities remains same. Thus Pareto-dominated. () Let
allocation object xj given
P
ui (xj ) = 1,
P agent iP
P suppose
Pareto-dominated allocation 0 . iI ui ( 0 (i)) = iI xj 0 (i) ui (xj ) > iI ui ((i)) =
P P
iI
xj 0 (i) ui (xj ) = p. Therefore least one ui (xj ) ui (xj ) > 1,
possible due restriction 01-preferences.
thus give simple way check Pareto-efficiency, checking sum utilities equal
number objects p desired least one agent. usual, envy-freeness verified
polynomial time; therefore eef existence additive 01-preferences NP.
Hardness proved polynomial reduction exact cover 3-sets (problem 3).
Given instance hS, C = hS1 , . . . , S|C| ii exact cover 3-sets, create following eef existence instance P(C, S) (we suppose elements written ai , {1, . . . , |S|}):
Agents:
Objects:
Preferences:

set 3|C| agents gathered triples {3i 2, 3i 1, 3i};
set |S| + 3|C| items X =SM (M main, dummy),
= {m1 , . . . , m|S| }, = i{1,...,|C|},j{1,2,3} {di,j };

agents {3i 2, 3i 1, 3i} desire set objects ak Si {mk }
{di,1 , di,2 , di,3 } (the three objects corresponding Si plus three dummy objects
di,j ).

exact cover C 0 instance hC, Si, consider following allocation:
agent triple {3i 2, 3i 1, 3i} gets respectively di,1 , di,2 , di,3 , Si C 0 ,
one three agents gets one three objects mk corresponding elements set
Si . allocation admissible Pareto-efficient (because objects allocated). also
envy-free, following reasons:
agents triple cannot envy other, equally satisfied.
agent k1 cannot envy agent k2 another triple, objects k1 could
envy k2 share mi . k2 one mi , k1 utility least
one, k1 cannot envy k2 .
rest proof based following result: allocation
Pareto-efficient
envy-free P(C, S) must (3i 2) (3i 1) (3i) = ak Si {mk } {di,1 , di,2 , di,3 }
(3i) (3i 1) (3i) = {di,1 , di,2 , di,3 }. easy show. Since agents triple
{3i 2, 3i 1, 3i} ones desire objects di,k , three objects must given
three agents, allocation efficient. Since three agents preferences,
allocation satisfy equally order envy-free. Thus number objects allocated
three agents must divisible 3, gives two possible numbers, 3 6, hence
two possible allocations.
Suppose Pareto-efficient envy-free allocation P(C, S). Consider subcollection C 0
= hS1 , . . . , S|C 0 | made triples Si collection C (3i 2) (3i
1) (3i) = ak Si {mk } {di,1 , di,2 , di,3 }. following results.
Si pairwise disjoints. Suppose pair (i, j) 6= j
element ak belonging Si Sj . mk allocated two different agents: one
member triple {3i 2, 3i 1, 3i}, one member triple {3j + 1, 3j + 2, 3j + 3},
impossible.

i{1,...,|C 0 |} Si = S. Let ak element S. Since Pareto-efficient, mk must
allocated one agent wants (say
agent belongs triple {3j + 1, 3j + 2, 3j + 3}),
unless one wants it, occurs i{1,...,|C 0 |} Si 6= S. Then, previous result,

objects al Sj {ml } must allocated triple. Consequently, Sj C 0 . Since ak Sj ,
ak belongs least one set collection C 0 .

555

fiBouveret & Lang

Therefore, C 0 exact cover S, finally proves proposition.



see Proposition 21 Conjecture 1 huge complexity gap
problem allow weights freely given problem
require weights 0 1 (at least conjecture true). natural question
raises know complexity fall specific 01-preferences, occurs
fixed upper bound weights.
Conjecture 2 complexity eef existence problem additive 01. . . k
preferences k 2 fixed hard general problem unbounded additive preferences.
precise complexity problem remains open problem, stated
conjecture, intuition hard eef existence problem
unbounded additive preferences.
Another natural problem raised Proposition 21: complexity
eef existence problem stratified 01-preferences ? stratified 01-preferences,
mean preferences given set pairs hxk , pi, xk object p
priority level. Comparing two sets objects comes compare lexicographically
vectors component index number objects priority
share agent. Notice problem instance eef existence
problem additive preferences, instance eef existence problem
logical numerical preferences logical form. However easy see remains
p2 , precise complexity remains unknown.
Finally, investigate case number objects less number
agents. One could think intuitively problem trivial case. However,
always case, see. begin with, following results shows
problem easy monotonic preferences:
Proposition 22 Let P allocation problem n agents additive monotonic
numerical preferences wanting least one object, p objects desired
least one agent.
p < n, Pareto-efficient envy-free allocation.
p = n, problem deciding whether exists efficient envy-free allocation agents monotonic additive preferences P.
Proof
object desired least one agent, every Pareto-efficient allocation complete. number objects p strictly lower number agents N , least one
agent unsatisfied. Consequently, agent j obtains object wanted i, hence
creating envy. Thus Pareto-efficient allocation envy-free.
Since many objects agents, agent receive one object values
(that is, ui ({x}) maximal), allocation Pareto-efficient
envy-free. Indeed, agent receives object preferred one, means
another agent j receives (otherwise allocation would Pareto-efficient), creating envy
i. Therefore, checking existence Pareto-efficient envy-free allocation comes
case checking possible give every agent one best-valued objects.
comes checking perfect matching bipartite graph made one
node per agent one side, one node per object side, connecting

556

fiEfficiency Envy-freeness Fair Division Indivisible Goods

agent object x x among best-valued objects agent preferences.
perfect matching computing polynomial time, hence result.


Interestingly, latter result hold allow non-monotonic preferences.
case, complexity increases complexity general eef existence
problem additive preferences.
Proposition 23 eef existence problem additive numerical preferences
number objects less number agents complexity
eef existence problem additive numerical preferences, assumption
number objects.
Proof Let us consider instance hI, X, h. . . , ui (xj ), . . . eef existence problem additive numerical preferences, N agents p objects (p > N ). create following instance
P(hI, X, h. . . , ui (xj ), . . . i):
Agents:
Objects:
Preferences:

p + 3 agents (the number agents important, greater
number objects initial number agents);
p initial objects xi plus two dummy ones d1 d2 ;
preferences N first agents hI, X, h. . . , ui (xj ), . . . i;
preferences (N +1)st agent uN +1 ({d1 }) = uN +1 ({d2 }) = 1 u{N +1} (xj ) =
0 items xj ; preferences remaining agents uN +1 ({d1 }) =
1, uN +1 ({d2 }) = 2 uN +1 (xj ) = 0 remaining objects.

efficient envy free allocation hI, X, h. . . , ui (xj ), . . . easily
checked allocation gives items N first agents P(hI, X, h. . . , ui (xj ), . . . i),
{d1 , d2 } (N + 1)st agent, nothing remaining ones efficient envy-free. Conversely,
Pareto-efficient envy-free allocation P(hI, X, h. . . , ui (xj ), . . . i) yields Pareto-efficient
envy-free allocation hI, X, h. . . , ui (xj ), . . . restricting N first agents objects
two dummy ones.


6. Related Work Discussion
already argued Introduction, computational studies resource allocation either
concern divisible goods, focus classical utilitarianism combinatorial auctions.
Existing work fair division indivisible goods, hand, mainly axiomatic,
computational aspects neglected, except papers
mentioning below.
results course lot common complexity results combinatorial
auctions. all, structure problems are, extent, similar: items
indivisible, allocations preemptive10 , agent preferences sets items
expressed compact representation language. Logical bidding languages also
designed (Boutilier & Hoos, 2001). However, complexity results completely differ:
standard decision problem combinatorial auctions NP-complete (Rothkopf, Pekec,
& Harstad, 1998) decision problems considered typically located
second level polynomial hierarchy, even degenerate case preferences
dichotomous. explained fact combinatorial auctions care
efficiency, envy-freeness. Requiring criteria together (efficiency
10. Here, preemptive means object cannot allocated one agent. assumption
absent problems implying example virtual objects, software licences.

557

fiBouveret & Lang

envy-freeness) makes things much difficult: while, usual assumption
preferences monotonic, efficiency monotonic property (allocating goods never
makes allocation less efficient), envy-freeness (allocating goods agent
may generate envy)11 . reason may exist EEF allocation,
also source high complexity problem.
Moreover, due failure monotonicity envy-freeness, searching EEF
allocation cannot simply formulated maximization minimization simple
criterion, problematic designing local search algorithms approximation
schemes. authors (Lipton et al., 2004; Chevaleyre, Endriss, Estivie, & Maudet,
2007a; Brams, Jones, & Klamler, 2007) suggested relax envy-freeness criterion
make gradual notion, defining measure envy-freeness. Lipton et al. (2004)
assume input consists numerical utility functions sets goods; degree
agent envies agent j allocation either defined envy difference
di,j () = max(0, ui ((j)) ui ((i))) envy ratio rij () = max 1, uuii((j))
((i)) .
cases, global degree envy maximum degree envy pair
players. Alternative definitions degree envy society proposed
Chevaleyre et al. (2007a), number envious agents, number pairs (i, j)
envies j, sum local degrees envy, relatively similar
measure envy considered Brams et al. (2007), based maximum number
agents single agent may envy. Chevaleyre et al. (2007b) suggest radically
different way relaxing envy-freeness: society comes along undirected graph
(which reflects acquaintance agents), allocation envy-free
agent envies agent connected.
Then, Lipton et al. (2004) focus search complete allocation minimum
envy; moreover, case additive utilities, provide approximation schemes. Another work approximation algorithms fair allocation indivisible goods
one Asadpour Saberi (2007), assume beginning utilities linear
consider problem finding maximally equitable allocation, is, allocation maximizing utility least satisfied agent (cf. problem considered
Proposition 17); consider envy-freeness all12 .
approaches considered far (including ours) assume allocation
computed centralized way neutral, objective agent. contexts,
centralized approach possible realistic, allocation obtained
decentralized way, successive negotiations groups agents. approach
initiated Sandholm (1998), studies convergence properties allocation
depending structural restrictions made exchanges goods may occur.
investigated Dunne (2005) Dunne et al. (2005), study
computational complexity negotiation problems, Endriss, Maudet, Sadri,
Toni (2006). approaches, optimality criterion classical utilitarian
11. Note antimonotonic either: allocating less goods agent may generate envy well.
12. Note alternative ways relaxing search EEF allocations exist. One may instance
keep envy-freeness hard requirement relax efficiency, allow relaxing look
allocation shows good trade-off efficiency envy-freeness (considering
problem two-criteria optimization problem).

558

fiEfficiency Envy-freeness Fair Division Indivisible Goods

social welfare. Envy-freeness considered distributed setting Chevaleyre et al.
(2007a, 2007b).
far, computational issues referred design study algorithms
run computers find allocation, designing studying
protocols query agents interactively gather enough information solution
determined. procedural issues, although extensively studied literature
fair division divisible goods (see below), well voting (Conitzer & Sandholm, 2005)
rarely considered allocating indivisible goods, notable exception
Herreiner Puppe (2002), study properties interactive protocols
two agents enumerate preferred bundles one one, allocation found.
mentioned Introduction, drawback protocols exponentially
long, henceforth infeasible soon number objects units.
Beyond works computational aspects fair division indivisible goods,
much consider computational procedural aspects fair division
divisible goods (or, least, assume least one good divisible e.g., money).
literature subject vast techniques quite far used
allocating indivisible goods (see particular literature cake-cutting algorithms, e.g.
Brams & Taylor, 1996; Robertson & Webb, 1998) find relevant give
detailed bibliography subject. interested reader refer book Brams
(2008) covers fair resource allocation indivisible divisible case.

7. Conclusion
studied several computational aspects search efficient envy-free allocations fair division problems indivisible goods. results Section 3 show
case dichotomous preferences, search allocations reduced
search preferred models prerequisite-free default logic. connection
rather unexpected, implies practical search EEF allocations done
using existing algorithms default logic. However search likely timeconsuming, due complexity results: indeed, extensive study complexity
deciding whether efficient envy-free allocation exists, various restrictions (dichotomous preferences not, two agents more, agents identical preferences
not, monotonic preferences not) various notions efficiency (Pareto-efficiency,
completeness, maximum number satisfied agents, maximum social welfare classical
utilitarian egalitarian), seems show problem intrinsically difficult,
since lies second level polynomial hierarchy, even preferences dichotomous monotonic. implies designing fast algorithms solving problem
general case reach. may first focus restrictions
problem NP-complete. Unfortunately, restrictions (agents identical
preferences; two agents; purely conjunctive, purely disjunctive 2-CNF preferences;
search complete allocations without efficiency requirement; additive 0-1 preferences) compelling imply huge loss generality.
complexity results introduced paper summed-up Figure 1 Table 2.
Several issues research remain explored.
559

fiBouveret & Lang

12
12

1

16

?

1

p2

22

13

?

p2

15

14

15

p2

p2

7
7

p2

p2
5

3

coBH2

coBH2
4
11

NP

2

11
6

9

9

6

?

18
6?

17

18
17

19

NP

8
8

P

10

21

10

P

20

O(1)

O(1)
? Proof included paper.

1

problem whose complexity proved paper (the mapping
numbers problems specified table 2).

17

problem whose complexity already known literature.

22



?

problem whose complexity remains unknown.

j

intersection problems corresponding outgoing edges.
Problem included problem j. Arcs obtained transitivity
omitted.

Figure 1: different problems complexity classes inclusion relations.

560

fiEfficiency Envy-freeness Fair Division Indivisible Goods

Efficiency

number agents

preferences

monotonicity

comp.



yes (1) (1)

p2 -c.

identical

yes

NP-c.

identical



coBH2 -c.




yes


NP-c.
coBH2 -c.

ident.

yes (6) (6, 6)

NP-c.



yes (7) (7)

p2 -c.

disjunctions
conjunctions
conj.
condition 1
C st sat(C) P
closed

yes (8) (8)
yes (9) (9)

P
NP-c.

yes (10) (10)

O(1)

yes (11) (11)

NP-c.

numerical

yes (12) (12)

p2 -c.

numerical



p2 -c.

numerical



p2 -c.

Dichotomous preferences
1, 1

Pareto-eff.

fixed

2

Pareto-eff.

3

Pareto-eff.

4
5

Pareto-eff.
Pareto-eff.

6, 6, 6

complete all.

7, 7

max nb ag.

fixed (6, 6, 6)
fixed N 2
(6, 6)
fixed

8, 8
9, 9

Pareto-eff.
Pareto-eff.




10, 10

Pareto-eff.



11, 11

Pareto-eff.



12, 12

Pareto-eff.

fixed

13

utilitarian sw

14

egalitarian sw

15, 15

Pareto-eff.

16
17, 17

Pareto-eff.
complete all.

18, 18

Pareto-eff.

19
20
21
22

Pareto-eff.
Pareto-eff.
Pareto-eff.
Pareto-eff.

fixed fixed
N 2
fixed fixed
N 2
2 agents
2 agents

Non-dichotomous preferences
fixed fixed
N 2
fixed fixed
N 2
fixed fixed
N 2
fixed
fixed
fixed fixed
N 2

> Nb objects
= Nb objects
Nb objects

numerical,
identical
additive
additive

yes (15) (15)

p2 -c.


yes (17) (17)

p2 -c. ?
NP-c.

additive ident.

yes (17) (17)

NP-c.

additive 01
additive
additive
additive

yes
yes
yes


NP-c.
O(1)
P
p2 -c. ?

Table 2: set resource allocation problems studied paper. complexity
classes represented figure 1.

561

fiBouveret & Lang

First, knowing efficient envy-free allocation given problem
helpful practice allocation found anyway. solution
consists defining functions return allocation cases, even envyfreeness efficiency cannot jointly met. way addressing issue consists
defining suitable relaxations problem, as: (a) using measure envy instead
seeing envy-freeness strict criterion (as suggested Lipton et al., 2004; Chevaleyre
et al., 2007b); (b) make envy-freeness relative notion, instance introducing
acquaintance graph agents (Chevaleyre et al., 2007b); (c) keeping envy-freeness
strict criterion relaxing efficiency. cases, new problems arise, complexity
identified.
Second, results mostly negative, since interesting problems studied NP-hard (and often even worse), therefore, would worth pursuing work
design practical algorithms problems. likely would require coming
appropriate optimization criteria either (a) giving polynomial algorithms
approximate desired objective (Lipton et al., 2004) (b) implementing
experimenting local search algorithms relevant heuristics.
Third, throughout paper assumed everyones preferences completely
known. reality, presumably agents need report preferences, introduces
issue strategic misreporting (manipulation). One direction future research would
investigate prevent this, is, mechanism design aspects.

Acknowledgments
thank Michel Lematre stimulating discussions fair division compact
representation; Thibault Gajdos stimulating discussions envy-freeness
pointing us relevant papers; Steven Brams, giving us feedback
earlier version paper pointing us relevant references; participants
AgentLink Technical Forum Group Multiagent Resource Allocation. work
partly supported project ANR-05-BLAN-0384 Preference Handling
Aggregation Combinatorial Domains, funded Agence Nationale de la Recherche.

References
Asadpour, A., & Saberi, A. (2007). approximation algorithm max-min fair allocation
indivisible goods. Tech. rep., Department Management Science Engineering,
Stanford University, Stanford.
Baral, C. (2003). Knowledge Representation, Reasoning Declarative Problem Solving.
Cambridge University Press.
Bogomolnaia, A., Moulin, H., & Stong, R. (2005). Collective choice dichotomous
preferences. Journal Economic Theory, 122, 165184.
Boutilier, C., & Hoos, H. H. (2001). Bidding languages combinatorial auctions. Proc.
17th International Joint Conference Artificial Intelligence (IJCAI-01), pp.
12111217, Seattle, Washington, USA.
562

fiEfficiency Envy-freeness Fair Division Indivisible Goods

Bouveret, S., Fargier, H., Lang, J., & Lematre, M. (2005). Allocation indivisible goods:
general model complexity results. Dignum, F., Dignum, V., Koenig, S.,
Kraus, S., Singh, M. P., & Wooldridge, M. (Eds.), Proceedings AAMAS05, Utrecht,
Nederlands. ACM Press.
Brams, S., & Fishburn, P. (1978). Approval voting. American Political Science Review,
72 (3), 831857.
Brams, S. J. (2008). Mathematics Democracy: Designing Better Voting FairDivision Procedures. Princeton University Press.
Brams, S. J., Edelman, P. H., & Fishburn, P. C. (2003). Fair division indivisible items.
Theory Decision, 55 (2), 147180.
Brams, S. J., Jones, M. A., & Klamler, C. (2007). Divide-and-conquer: proportional,
minimal-envy cake-cutting procedure. Tech. rep., NYU Department Politics.
Brams, S. J., & King, D. L. (2005). Efficient fair division: Help worst avoid envy?.
Rationality Society, 17 (4), 387421.
Brams, S. J., & Taylor, A. (1996). Fair Division: Cake-Cutting Dispute Resolution.
Cambridge Univ. Press.
Chevaleyre, Y., Dunne, P. E., Endriss, U., Lang, J., Lematre, M., Maudet, N., Padget, J.,
Phelps, S., Rodrguez-Aguilar, J. A., & Sousa, P. (2006). Issues multiagent resource
allocation. Informatica, 30, 331. Survey paper.
Chevaleyre, Y., Endriss, U., Estivie, S., & Maudet, N. (2004). Multiagent resource allocation k-additive utility functions. Proc. DIMACS-LAMSADE Workshop
Computer Science Decision Theory, Vol. 3 Annales du LAMSADE, pp. 83100.
Chevaleyre, Y., Endriss, U., Estivie, S., & Maudet, N. (2007a). Reaching envy-free states
distributed negotiation settings. Veloso, M. (Ed.), Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI-2007), pp. 12391244.
Chevaleyre, Y., Endriss, U., & Lang, J. (2006). Expressive power weighted propositional
formulas cardinal preference modelling. Proceedings 10th International
Conference Principles Knowledge Representation Reasoning (KR), pp. 145
152, Lake District, UK. AAAI Press.
Chevaleyre, Y., Endriss, U., & Maudet, N. (2007b). Allocating goods graph eliminate
envy. Proceedings AAAI-07, pp. 700705.
Conitzer, V., & Sandholm, T. (2005). Communication complexity common votiong rules.
Proceedings EC-05.
Cramton, P., Shoham, Y., & Steinberg, R. (Eds.). (2005). Combinatorial Auctions. MIT
Press.
Demko, S., & Hill, T. P. (1998). Equitable distribution indivisible items. Mathematical
Social Sciences, 16, 145158.
Dunne, P. E. (2005). Extremal behaviour multiagent contract negotiation. Journal
Artificial Intelligence Research, 23, 4178.
563

fiBouveret & Lang

Dunne, P. E., Wooldridge, M., & Laurence, M. (2005). complexity contract negotiation. Artificial Intelligence, 164 (1-2), 2346.
Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2006). Negotiating socially optimal allocations resources. Journal Artificial Intelligence Research, 25, 315348.
Ford, L. R., & Fulkerson, D. R. (1962). Flows Networks. Princeton University Press.
Gebser, M., Liu, L., Namasivayam, G., Neumann, A., Schaub, T., & Truszczynski, M.
(2007). first answer set programming system competition. Baral, C., Brewka,
G., & Schlipf, J. (Eds.), Proceedings Ninth International Conference Logic
Programming Nonmonotonic Reasoning (LPNMR07), Vol. 4483 Lecture Notes
Artificial Intelligence, pp. 317. Springer-Verlag.
Gottlob, G. (1992). Complexity results nonmonotonic logics. Journal Logic
Computation, 2, 397425.
Herreiner, D. K., & Puppe, C. (2002). simple procedure finding equitable allocations
indivisible goods. Social Choice Welfare, 19, 415430.
Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: compact representation
scheme coalitional games. Proceedings EC05.
Karp, R. M. (1972). Reducibility among combinatorial problems. Miller, R. E., &
Watcher, J. W. (Eds.), Complexity Computer Computations, pp. 85103, New York.
Plenum Press.
Lang, J. (2004). Logical preference representation combinatorial vote. Annals Mathematics Artificial Intelligence, 42 (1), 3771.
Lipton, R. J., Markakis, E., Mossel, E., & Saberi, A. (2004). approximately fair allocations indivisible goods. EC 04: Proceedings 5th ACM conference
Electronic commerce, pp. 125131, New York, NY, USA. ACM Press.
Nisan, N. (2005). Bidding Languages Combinatorial Auctions. MIT Press.
Papadimitriou, C. H. (1994). Computational complexity. AddisonWesley.
Rawls, J. (1971). Theory Justice. Harvard University Press, Cambridge, Mass.
Reiter, R. (1980). logic default reasoning. Artificial Intelligence, 13, 81132.
Robertson, J., & Webb, W. (1998). Cake-Cutting Algorithms. A.K. Peters.
Rothkopf, M., Pekec, A., & Harstad, R. (1998). Computationally manageable combinational
auctions. Management Science, 8 (44), 11311147.
Sandholm, T. (1998). Contract types satisficing task allocation: theoretical results.
Proc. AAAI Spring Symposium: Satisficing Models.
Wagner, K. W. (1987). complicated questions maxima minima,
closures NP. Theoretical Computer Science, 51, 5380.
Wagner, K. W. (1990). Bounded query classes. SIAM Journal Computing, 19 (5), 833
846.
Young, H. P. (1995). Equity Theory Practice. Princeton University Press.

564

fiJournal Artificial Intelligence Research 32 (2008) 1-36

Submitted 05/07; published 5/08

Enhancing Cooperative Search Concurrent Interactions
MANISTER @ BIU .013. NET. IL

Efrat Manisterski
Department Computer Science,
Bar-Ilan University, Ramat Gan, 52900 Israel

SARNED @ CS . BIU . AC . IL

David Sarne
Department Computer Science,
Bar-Ilan University, Ramat Gan, 52900 Israel

SARIT @ CS . BIU . AC . IL

Sarit Kraus
Department Computer Science,
Bar-Ilan University, Ramat Gan, 52900 Israel

Abstract
paper show taking advantage autonomous agents capability maintain
parallel interactions others, incorporating cooperative economic search model
results new search strategy outperforms current strategies use. framework
analysis use electronic marketplace, buyer agents incentive search
cooperatively. new search technique quite intuitive, however analysis process
extracting optimal search strategy associated several significant complexities.
difficulties derived mainly unbounded search space simultaneous dual affects
decisions taken along search. provide comprehensive analysis model, highlighting,
demonstrating proving important characteristics optimal search strategy. Consequently,
manage come efficient modular algorithm extracting optimal cooperative
search strategy given environment. computational based comparative illustration
system performance using new search technique versus traditional methods given, emphasizing main differences optimal strategys structure advantage using
proposed model.

1. Introduction
Coalition formation well recognized key process multi-agent systems, mostly desirable
environments group agents perform task efficiently single agent
(Lermann & Shehory, 2000). recent years many coalition formation models suggested,
various domains (Talukdar, Baerentzen, Gove, & de Souza, 1998; Dias, 2004), particularly
electronic commerce (Tsvetovat, Sycara, Chen, & Ying, 2000; Yamamoto & Sycara, 2001; Sarne
& Kraus, 2005). latter context, common coalition coalition buyers, derived
mainly potential obtaining volume discounts (Tsvetovat et al.; Sarne & Kraus, 2003)
ability search cooperatively market opportunities efficient manner (Sarne &
Kraus, 2005).

c
2008
AI Access Foundation. rights reserved.

fiM ANISTERSKI , ARNE , & K RAUS

cooperative economic search1 incentive derives principally existence search
costs found MAS. costs reflect resources (not necessarily monetary) need
invested consumed searching opportunities environment (Sarne & Kraus, 2005)
(e.g., searching opportunity buy product context electronic marketplace).
scenario search costs common MAS agent needs (for decision
making process) immediate information concerning market opportunities. Given richness
opportunities dynamic open nature environments, central mechanisms usually incapable supplying information level completeness accuracy required
agent, certainly without cost. Thus agent needs spend resources
search related activities. Despite reduction magnitude search costs electronic commerce era, continuous growth number retailers virtual stores
Internet, followed phenomenal increase number opportunities available, makes
overall search cost important parameter affecting buyers search strategy (Choi & Liu, 2000;
Kephart & Greenwald, 2002; Sarne & Kraus, 2005; Bakos, 1997).
context, cooperative search offers advantage sharing, reusing re-allocating
opportunities among coalition members (Sarne & Kraus, 2005). example, using cooperative
search agents exploit opportunities would discarded otherwise
agents would conducted alternative separate search. Nevertheless, process forming
maintaining coalition induces overhead, derived mainly communication
coordination activities (Sarne & Kraus, 2003), thus coalition set search strategy
cost/effective manner.
classic example traditional markets procurement management officer
corporation. Instead individual cooperation spend time resources
locating specific requested supplies, task delegated procurement management
officer. Here, addition price discounts obtained aggregated demands identical items,
procurement management officer becomes highly updated different offers specific
supplies available different merchants markets. result cost locating
best deal request becomes significantly smaller (in comparison equivalent search
conducted individuals).
basic concepts coalition manage cooperative search, including
analysis computational means extracting optimal search strategies, given Sarne
Kraus (2005). Nevertheless, assumption used model constructing coalitions
strategy coalition interacts one seller agent time. assumption ignores
inherent strength autonomous agents, capability efficiently interact
several agents parallel. capability derives primarily improved communication capabilities ability process enormous amount information short time
compared people. paper, take advantage capability incorporate
cooperative economic search model, supplying comprehensive analysis resulting parallel
cooperative search variant. show throughout paper, parallel model weakly dominates
existing sequential cooperative search model described Sarne Kraus: potential
significantly improving searchers performance various environments, always guarantees reaching least performance existing cooperative search model. particular,
parallel interaction preferable whenever agents search cost non-linear combines fixed
1. opposed classical AI search (Hart, Nilsson, & Raphael, 1968) agent seeks sequence actions
bring initial state desired goal state.

2

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

components (e.g. operational costs), depending number interactions maintained (e.g. advantage size). cases, adoption parallel technique coalition suggests
reduction average cost per interaction seller agents.
Moreover, improvement achieved using parallel technique increases finite decision horizon (i.e., whenever coalition deadline finishing search). addition
advantage reducing average cost per interaction, finite horizon settings coalition
benefits fact increase intensity search and, thus, scan opportunities (in comparison sequential search model described Sarne & Kraus, 2005) prior
reaching deadline.
integration parallel interactions technique single agents search process
quite intuitive, finding optimal (overall utility maximization) strategy cooperative search
case trivial all. major difficulty derives fact different coalition members
may heterogeneous multi-attribute utility functions. Therefore, extracting value encapsulated future streams opportunities complex. overcome difficulty present
algorithm extracting coalition strategy. algorithm facilitates calculation process
coalition strategy polynomial number parallel interactions. significant improvement brute-force algorithm exponential number parallel
interactions.
Similar model introduced Sarne Kraus (2005), apply multi-attribute utility
theory (MAUT) (Keeney & Raiffa, 1976) analyze preferences multiple attributes agent
based search mechanism. enables set preferences represented numerical utility
function. consider agents heterogeneous, i.e. utility function.
model general, though emphasize several specific implementation aspects relating B2C
market (businesses selling products services end-user consumers), sellers supply
almost demanded volume, C2C market (transactions consumers),
sellers offer single items sale. Based proposed analysis, coalition calculate
optimal strategy given utility functions coalition members specific environment
operates.
Three basic stages common coalition formation models (Sandholm, Larson, Andersson, Shehory, & Tohme, 1999; Tsvetovat et al., 2000): coalition structure generation (where
agents form/join coalition), executing coalition task, dividing generated value among
coalition members. Among three stages focus finding optimal search strategy
coalition, given structure opportunity distribution. suggested Sarne
Kraus (2005), coalition operates environment alongside many coalitions differing
size, members utility functions products seeking. coalitions,
well different individual utility functions play important role studying stability
coalition issue revealing true utility function (truth telling). analysis
important issues based ability properly derive coalitions utility given specific
self structure environment within operates. 2 paper aims supply functionality, laying foundation enabling research many important aspects coalition
formation given context cooperative search (truth telling, stability, payoff division,
etc.).
2. utility considered agents reported, necessarily true, utility function, since goal extract
optimal strategy given input.

3

fiM ANISTERSKI , ARNE , & K RAUS

main contributions work fourfold: First, formally model analyze
parallel cooperative search problem agents operating costly environment. parallel cooperative search model general search model applied various domains addition
electronic marketplace used framework work. Second, show
many environments parallel cooperative search outperforms previous search strategies (either
agent searches using cooperative sequential search). Furthermore,
draw attention scenarios sequential cooperative search proven non-beneficial,
however parallel cooperative search favorable technique. Third, supply algorithm
facilitates calculation coalitions optimal strategy, significantly reduces complexities associated attempt extract strategy appropriate set equations. Finally
provide comprehensive analysis parallel model finite decision horizon. draw
attention significant improvement achieved integrating parallel technique
cooperative search finite decision horizon.
rest paper organized follows. Section 2 reviews related work, emphasizing
uniqueness proposed parallel cooperative search model. model description
underlying assumptions given section 3. section 4, formally describe performance
coalition using parallel cooperative search function strategy used
present complexities associated extracting optimal search strategy. exploring
unique characteristics coalitions optimal strategy using cooperative parallel search
manage overcome computational complexity. process described section 5. Consequently, present efficient algorithm extracting optimal cooperative search strategy.
interesting properties new search model, regard market takes
place, illustrated section 6. section 7, finite decision horizon variant model
discussed. parallel cooperative search performance advantages current search
models, infinite finite decision horizons, illustrated section 8. Finally, conclude
discussion suggested future research directions section 9.

2. Related Work
many scenarios autonomous agents multi-agent environments may cooperate order perform tasks. need cooperation may arise either agent incapable completing
task operating group improve overall performance (Breban & Vassileva, 2001; Lermann & Shehory, 2000; Tsvetovat et al., 2000). Group based cooperative behavior
found various domains, solving complex optimization problems (Talukdar et al.,
1998), military rescue domains (Dias, 2004), e-business applications (Tsvetovat et al., 2000;
Yamamoto & Sycara, 2001) many more. recognition advantages encapsulated
teamwork cooperative behaviors, main driving force many coalition formation models
area cooperative game theory MAS (Lermann & Shehory, 2000; Li, Rajan, Chawla, &
Sycara, 2003; Shehory & Kraus, 1998). Many examples extensive literature coalition formation found books journals (Kahan & Rapoport, 1984). electronic
market domain, authors focus coalitions formed obtain volume discounts Tsvetovat
et al., Yamamoto Sycara. Additional coalition formation models electronic marketplace
consider extensions transaction-oriented coalitions long-term ones Breban Vassileva,
large-scale electronic markets Lermann Shehory.

4

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

Traditionally, majority research effort focused issues concerning optimal
division agents disjoint exhaustive coalitions (Sandholm et al., 1999; Yamamoto & Sycara,
2001), division coalition payoffs Yamamoto Sycara enforcement methods interaction
protocols. authors considered coalitions problem determining strategy
electronic commerce domain, coalition formed (Ito, Ochi, & Shintani, 2002).
Nevertheless, single exception (Sarne & Kraus, 2005), none proposed models
considered coalition search costly environment, particular none (including
Sarne & Kraus, 2005) made use capabilities maintain parallel interactions.
problem searcher operating costly environment, seeking maximize long term
utility addressed classical search theory (Lippman & McCall, 1976; McMillan & Rothschild,
1994, references therein). three main search models found literature.
first search model Fixed Sample Size (FSS) model, introduced Stigler (1961).
model searcher first chooses sample size draws single sample observations made simultaneously. second model Single Agent Sequential Search (SASS)
strategy (Rothschild, 1974; Lippman McCall). model searcher draws exactly one observation time. Based value observations drawn till time, searcher decides
whether draw another observation. decision depends upon observed. Attempts
adopt sequential search model agent-based electronic trading environments associated
search costs suggested Choi Liu (2000), Kephart Greenwald (2002), though
main focus establishing appropriate characteristics environment search strategy
rather computational aspects extracting it. last search method, Single Agent Parallel Search (SAPS) (Benhabib & Bull, 1983; Gal, Landsberger, & Levykson, 1981; Morgan, 1983;
Morgan & Manning, 1985), encompasses search models special cases. model
searcher may choose number samples taken sample size period.
latter method, outperforms two, fact single agents equivalent parallel
cooperative search model considered paper. Nevertheless, search theory mainly focused
single searcher, looking single opportunity, either one sided (taking environments
reaction search strategy used agent static) two sided (as matching model,
analyzed equilibrium perspective) model. analysis cooperative search lacking.
This, in-spite fact cooperative search proven (Sarne & Kraus, 2005)
inherently different single agents search relation complexity, strategy structure
solution methodology.
Several extensions economic search theory suggested case consumer
searching multiple different commodities, facing imperfect information prices (Gatti,
1999; Carlson & McAfee, 1984; Burdett & Malueg, 1981). Here, find different variants
consumer visits one stores order minimize total expenditure. Nevertheless, attempt adjust proposed methods suggested models support parallel
cooperative search process results solution complexity exponential number parallel interactions. contrast algorithm extracting optimal search strategy polynomial
number parallel interactions.

3. Parallel Cooperative Search Model
base model description formulation definitions given Sarne Kraus (2005)
extend reflect agents parallel search capabilities. consider electronic mar5

fiM ANISTERSKI , ARNE , & K RAUS

ketplace numerous buyer seller agents found. agent interested buying
offering sell well defined product. product offered many different seller agents
various terms policies (including price). assume buyer agents ignorant
individual seller agents offers, acquainted overall distribution opportunities
(where opportunity defined option buy product specific terms policies)
marketplace.
Assuming central mechanisms mediators supply agents full
immediate information concerning current market opportunities, agent needs search
appropriate opportunities buy requested product. Throughout search buyer agents
locate seller agents learn offers interacting them. buyer agent evaluates
opportunities using multi-attribute utility function. Buyer agents may heterogeneous
preferences thus utility given opportunity differs according evaluating buyer
agent.
basic form, buyer agent interacts several sellers parallel stage
search thus learns new set opportunities. Based agents evaluation utility
gained opportunity set, agent makes decision whether exploit
opportunities encountered throughout search (i.e. buy sellers)
resume search similar method. decision resume search always accompanied
number parallel interactions conducted next.
search activity assumed costly (Choi & Liu, 2000; Kephart & Greenwald, 2002;
Sarne & Kraus, 2005; Bakos, 1997). search stage buyer agent locates, interacts
evaluates seller agents, process induces search cost. cost function number
parallel interactions initiated maintained agent. search cost structure principally
parameter markets liquidity volatility, thus assumed shared buyer
agents operating specific marketplace. Recognizing benefits cooperative search,
buyer agents, interested similar products interchangeable products, may form coalitions (Sarne
& Kraus, 2005). various methods coalition members coordinate
cooperative search (e.g. assign representative agent search behalf coalition simply
take turns searching), deriving different search cost overhead structure. coalitions search
costs assumed increase function number parallel interactions forms
number buyer agents coalition. 3 assume buyer agents utility given
opportunity may interpreted monetary terms. Thus utilities additive total
search utility obtained subtracting search cost process induces
value.
part search process, coalition needs set strategy determining, given set
known opportunities, whether terminate resume search. latter case, coalition
also needs determine number parallel interactions used next search round.
optimal strategy one maximizing expected total search utility (opportunities utility
minus search costs). discussed detail cooperative search model (Sarne & Kraus, 2005),
given option side-payments overall utility maximization strategy taken coalition
always preferred one coalition members (i.e. conflict interests), regardless
pre-set coalitions payoff division protocol. Given coalitions goal maximizing overall coalition utility, decision influenced payoff division protocol, coalition
3. reason correlating coalitions search cost number coalition members mainly associated
coordination overhead (Sarne & Kraus, 2005).

6

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

stability considerations, rather influences two factors (Sarne & Kraus, 2005).
agents pre-determined portion coalitions utility increase absolute value along
increase net coalition utility, thus overall utility maximization strategy preferred
strategy agents every stage search.

4. Parallel Cooperative Search (PCS) Analysis
following section formally defines search environment coalitions search strategy.
convenience, notations given, meanings, summarized table end
paper.
Let B = (B1 , B2 , ..., Bk ) set attributes defining potentially available
opportunities market, attribute B assigned value finite set
(bimin , ..., bimax ). opportunitys type defined vector ~o = (b1 , b2 , ...bk ), assigning value
bi specific attribute Bi .4 use p denote space potential opportunity types
coalition may encounter. opportunity types distribution marketplace denoted
probability function p(~o), ~oO p p(~o) = 1. consider coalition = {a 1 , a2 , ..., a|A| } general
size, j j th buyer agent coalition. buyer agent, j , evaluates opportunities
using utility function U j : p R, U j (~o) agents utility opportunity type ~o.
denote search cost associated coalition n agents maintaining w simultaneous
interactions seller agents search round function c(w, n).
Let collection possible sets opportunities environment
agents reside. Given set known opportunities known coalition needs determine
strategy (whether terminate/resume search number parallel interactions
later case). Similar analysis suggested Sequential Cooperative Search (SCS) model
(Sarne & Kraus, 2005) reduce large number world states coalition
be, adopting representation states sets effective known opportunities.
purpose consider function alloc : np maps given set opportunities
coalition members (i.e. allocation) way aggregated agents utility using
allocation maximized.5 B2C markets opportunity may allocated
one agent, C2C markets opportunity restricted one agent. Given coalition

/ represent allocation resulting applying
A, use alloc() = (~y1 , ...
yn ), ~yi ( {0})
function alloc set , ~y denotes opportunity associated agent (yi = 0/
denotes opportunity allocated agent ).
computation method used function alloc market-dependent. B2C markets
function assigns agent opportunity maximizes utility, ~y j = arg max~y U j (~y), j =
1, ..., n, C2C markets alloc computed solving maximum weighted matching
bipartite graph (Avis & Lai, 1988). Specifically set opportunities found C2C
market construct graph G = (V1 ,V2 , E), vertex V1 corresponds agent
vertex V2 corresponds opportunity ~o . edge connects agent j V1
opportunity ~o V2 (each member two groups edges connecting members
4. Notice ~o noted vector since assigns specific value different attributes, terms
conditions associated specific opportunity. example, specific opportunity buy calculator
represented vector ~o = (scienti f ic, 20$, small display, pocket, 1Y R warranty).
5. one allocation maximizes overall coalition utility function alloc chooses one
according pre-defined order.

7

fiM ANISTERSKI , ARNE , & K RAUS

group). weight edge utility agent j opportunity ~o, U j (~o).
alloc() = (~y1 , .., y~n ), {(a1 , y~1 ), .., (an , y~n )} maximum weighted matching G .
illustrate computation used function alloc, use following example:
Example 1. Consider following environment:
Environment 1. three agents, 1 , a2 a3 , searching product (e.g., memory chip)
characterized two attributes, B 1 (e.g., quality) B2 (e.g., store rating). attribute
either value 1 2, equal probability 1/2. means four possible
opportunities ~o1 = (1, 1) (both attributes values equal 1), ~o 2 = (1, 2), ~o3 = (2, 1), ~o4 =
(2, 2). utility function agents 1 , a2 a3 U1 (~o) = 9B1 + B2 , U2 (~o) = 4B1 + 5B2 ,
U3 (~o) = B1 + 10B2 , respectively. Table 1 summarizes environments setting.
Opportunity
~o1
~o2
~o3
~o4

(Attribute1,
Attribute2)
(1,1)
(1,2)
(2,1)
(2,2)

Probability
Agent a1
10
11
19
20

1
4
1
4
1
4
1
4

Utility
Agent a2
9
14
13
18

Agent a3
11
21
12
22

Table 1: Agents utilities four opportunities Environment 1
Assume coalition already interacted 4 sellers, encountering two opportunities
type ~o1 two single opportunities types ~o 3 ~o4 . Here, set known opportunities
known = {~o1 ,~o1 ,~o3 ,~o4 }.
first calculate alloc(known ) coalition operating B2C market. market
assume sellers supply demanded volume. Therefore allocation maximizes
coalitions overall utility assigning agent opportunity maximizes utility
known . Since opportunity ~o4 maximizes utility agents, obtain alloc( known ) =
(~o4 ,~o4 ,~o4 ).
C2C markets allocation impossible, since opportunity ~o 4 assigned
one agents (each seller offers single item sale). case optimal allocation
calculated solving maximum matching problem resulting assignment alloc( known ) =
(~o3 ,~o1 ,~o4 ).
Given set known opportunities known , use function alloc consequent
allocation alloc(known ) calculate immediate utility coalition terminates search
current time point. utility, defined aggregated agents utility
agents allocated according allocation alloc( known ), denoted Vt (known ) (abbreviation
Vterminate ) calculated using:
n

Vt (known ) =

U j (~y j )

j=1

/ = 0, j.
alloc(known ) = (~y1 , ...~yn ) U j (0)
8

(1)

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

Notice point, world state space upon coalition defines strategy
defined set opportunities known known coalition. space potentially
large. order reduce strategys space, introduce concept equivalence
different sets opportunities within context cooperative search. consider two sets
opportunities 0 , 00 equivalent (0 00 ), following hold: (a) Vt (00 ) = Vt (0 );
(b) Vt (0 ) = Vt (00 ) set opportunities coalition may encounter
future. two equivalent sets 0 , 00 , coalition indifferent knowing opportunities
0 opportunities 00 . set opportunities coalition encounter
future results similar utility, thus coalitions overall utility
cases. Moreover, since coalitions decisions merely determined overall utility
cases similar utilities reached similar probabilities, coalition uses
search strategy (either terminates search uses number parallel interactions
subsequent search stage) opportunities sets.
Notice according definition equivalent transitive relation ( 0 00 , 00
000
0 000 ). Moreover, 0 00 implies (0 ) (00 ), . Given allocation
` = (~y1 , ..., y~n ) set , use {`} denote set opportunities appear `.
Similar former cooperative search models (Sarne & Kraus, 2005; Manisterski, 2007)
following theorem holds cooperative search model (and proven similar manner).
Theorem 1. set opportunities equivalent set opportunities returned
function alloc(). Formally stated: {alloc()}.
Theorem 1 enables us reduce set known opportunities affect coalitions strategy.
immediate implication Theorem 1 coalitions strategy affected
subset known defined {alloc(known )}. Thus coalition need keep known
opportunities. reduce set known opportunities determines strategy
subset, s. Given result, define state set opportunities members
{alloc(known )}. Formally, calculate state coalition acquainted set
known known opportunities using function = state( known ) = {alloc(known )}. use
SA denote set possible states coalition A. definition significantly simplifies
analysis enables coalition calculate optimal strategy exclusively based sets
opportunities SA . following example illustrates computation state.
Example 2. Consider environment set known opportunities described Example
1. computed Example 1, allocation maximizes coalitions overall utility
B2C market alloc(known ) = (~o4 ,~o4 ,~o4 ). Thus coalitions current state B2C market
state(known ) = {~o4 }. result coalition ignore opportunities encountered
{~o1 ,~o1 ,~o3 } strategy (terminate resume search number parallel interactions
later case) strategy known = {~o4 }. Similarly, coalitions state
C2C market includes opportunities alloc( known ) thus state(known ) = {~o3 ,~o1 ,~o4 }.
coalitions transition one state another search B2C C2C
markets described directed acyclic graph (DAG). vertices graph present
potential coalition states. directed edge (s, 0 ) connects two states, s0 ,
opportunity ~o p changes current coalitions state 0 (i.e ~o, s.t state(s ~o) = s0 ).
better understand use DAG markets, use following two environments:
9

fiM ANISTERSKI , ARNE , & K RAUS

Environment 2. two agents 1 a2 searching product (e.g., computer mouse)
B2C market associated 3 types opportunities (e.g., 3 models): o~ 1 , o~2 o~3 . Table 2
summarizes environments setting.
Opportunity
~o1
~o2
~o3

Utility
Agent a1 Agent a2
5
10
10
5
20
21

Table 2: Agents utilities three opportunities Environment 2
Environment 3. two agents 1 a2 searching product (e.g., used book)
C2C market associated two types opportunities (e.g., English edition American edition):
o~1 o~2 . Table 3 summarizes environments setting.
Opportunity
~o1
~o2

Utility
Agent a1 Agent a2
5
10
10
5

Table 3: Agents utilities two opportunities Environment 3
Figure 1(a) Figure 1(b) show DAG states Environment 2 (the B2C market)
Environment 3 (the C2C market) described below, respectively. Notice possible
opportunities change coalitions current state (since opportunities increase coalitions overall utility). simplify graph mark opportunities.
notable C2C market sets include n opportunities feasible states (state
feasible belongs SA , i.e. set state() = s). hold
B2C market. example, set {~o 1 ,~o3 } feasible state since ~o3 maximizes agents
utility. Thus coalition encounters opportunity opportunities ignored.
coalition reaches state along search change state 0
sequence directed edges state state 0 . coalition conduct parallel interactions,
thus transition within single search round state 0 directly connected
state s. example, Environment 3, state = {} coalition change state
s0 = {~o1 ,~o2 } one search round (even though directed edge them).
happen coalition conducts two parallel interactions encounters
opportunities ~o1 ~o2 . transition new state suggests new state coalition
termination utility equal higher utility current state.
define strategy function x : N, x(s) = 0 agent decides terminate
search; otherwise x(s) number parallel interactions coalition maintain next,
state s. denote optimal strategy x .
define V (s, w) coalitions expected utility using w parallel interactions
state (assuming search decision taken future state 0 6= make use optimal
number parallel interactions). term V (s, 0) denotes immediate utility obtained,
10

fiE NHANCING C OOPERATIVE EARCH

{o1}

o1
{o1,o2}

o3

{}

o1

o2

o2
o3

C ONCURRENT NTERACTIONS

o3

{}

o1



o2

{o1}

{o2}

{o2}

o1
o3

o1

o2

{o1,o1}
o2

{o3}

(a)

o2
{o2,o2}

{o1,o2}

o1

(b)

Figure 1: States Diagram (a) Environment 2 (B2C market); (b) Environment 3 (C2C market).

coalition decides terminate search state s, thus: V (s, 0) = Vt (s). value w (w N,
w 0) maximizes coalitions expected utility V (s, w), equal x (s):
x (s) = arg max V (s, w)
w

(2)

coalitions expected utility point onwards using optimal strategy, denoted
V (s), expressed as:
V (s) = maxwV (s, w) = V (s, x (s))

(3)

order formulate appropriate equation V (s, w) (from x (s) V (s)
derived) make use several additional notations definitions. Consider search round
coalition interacts simultaneously w seller agents, yielding set w = {~
o1 , ..., o~w },
~oi p opportunities. Let w collection w-sized sets opportunities
produced environment coalition operates. denote p w (w ) probability
encountering specific set opportunities w , maintaining w random interactions
seller agents.
Similar basic cooperative search (Sarne & Kraus, 2005) divide w-sized opportunities space, w , two sub-spaces, containing improving non-improving w-sized sets opportunities coalitions utility, respectively. Nevertheless, definition needs extended
fit scenario parallel search follows. Given number simultaneous interactions,
w, state s, let simprovew collection w-size sets opportunities, w , change
coalitions current state (formally stated as: simprovew = {w |w w state(s w ) 6= s}).
denote complementary set simprovew sstayw (the set includes w-size sets
opportunities w change coalitions current state).
Therefore coalition encounters set opportunities w , distinguish two
possible scenarios:
(1) w belongs sstayw coalitions current state still s. case coalitions future
expected utility (i.e., point on) remains V (s, w). derived stationary nature 6
6. Stationary strategy strategy player chooses moves every structurally equivalent subgame
(Baron & Ferejohn, 1989).

11

fiM ANISTERSKI , ARNE , & K RAUS

problem - better state reached, search resumes using number
parallel interactions w, yielding expected utility.
(2) w belongs simprovew coalitions current state changes 0 = state(s w ) 6= s. Since
assume coalitions decision taken future state 0 6= make use coalitions
optimal strategy x (s0 ), coalitions expected utility expressed V (state(s w )).
using analysis, expected utility using w parallel interactions
state s, V (s, w), expressed (w > 0):
V (s, w) =c(w, n)+



pw (w )V
w simprovew



(s0 )+



pw (w )V (s, w)
w sstayw

(4)

s0 coalitions new state encounters set opportunities w , s0 = state(s w ).
applying basic mathematical manipulations term obtain:
V (s, w) =

c(w, n) + w simprove pw (w )V (s0 )
w

1 w sstayw pw (w )

(5)

Since 1 w sstayw pw (w ) = w simprove pw (w ) obtain:
w

V (s, w) =

c(w, n) + w simprove pw (w )V (s0 )
w

w simprovew pw (w )

(6)

Notice case new better state reached, denominator becomes zero,
V (s, w) = . quite straightforward since coalition maintains endless costly search
(trying reach better state actually exist). Here, inevitably coalitions optimal
strategy terminate search. important characteristic used later design proposed
algorithms extracting optimal search strategy.
point, one may attempt compute coalitions strategy x solving set equations
types 1, 3 6. Nevertheless, straightforward solution approach accompanied many
inherent complexities, derived structure equations. First, notice Equation 6
recursive equation one needs know optimal strategy taken future states 0
extracting optimal strategy given state s. Second, computation V (s, w) Equation
6 exponential number parallel interactions, w, used (affecting number sets
simprovew , denominator numerator). Last, according formulation,
potential number parallel interactions may used bounded 7 , thus reaching local
maximum guarantee higher utility cannot obtained. Therefore, algorithmic
approach reduce complexity extracting optimal cooperative parallel search
favorable.

5. Algorithmic Approach
section present comprehensive analysis problem, emphasizing unique
characteristics coalitions optimal strategy. findings lead algorithm computing
V (s, w) polynomial complexity number potential interactions, w (which key
component computing x (s) V (s)).
7. number opportunities theoretically infinite due high arrival rate new opportunities derived
dynamic environment.

12

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

5.1 Reducing Calculation Complexity
Recall attempting solve problem set equations (see section 4) potential
number parallel interactions may used unbounded. Nevertheless, order extract
x (s) essential supply coalition upper bound, w smax , optimal number
parallel interactions used state s. order overcome difficulty suggest
upper bound wsmax make use following notation. use = (s1 , ...s|SA | ) denote
states constituting SA , sorted termination utilities Vt (s)8 , s1 state
highest expected utility Vt (s) SA .
following proposition suggests efficient upper bound x (s).


Proposition 1. state si upper bound wsmax
, x (si ) calculated using wsmax
= dwe,
w solution following equation:

(7)

c(w, n) = Vt (s1 ) Vt (si )


suggested bound valid simply every value w greater w smax
search cost
associated following immediate search round c(w, n) greater possible future
improvement coalitions utility Vt (s1 ) Vt (si ) (since maximum additional utility
coalition gain bounded difference coalitions overall maximum utility
Vt (s1 ) immediate utility current state Vt (si )). Later on, show upper
bound value byproduct main loop proposed algorithm, thus even need
directly calculated.
upper bound x (s) important step towards solution, however calculation
V (s, w) (from x (s) derived) still exponential number parallel interactions
used, w. analysis, based restructuring different elements composing
V (s, w), allows us bypass complexity introduction finite algorithm
polynomial computational complexity w inevitably identify optimal strategy
coalition.
order reduce complexity computing coalitions best strategy, make use
following notations definitions:

use pstay (s, w) denote probability coalition remain state
conducting w parallel interactions. calculated probability none
encountered w opportunities changing coalitions state:
pstay (s, w) = (pstay (s, 1)) = (
w



p(~o))w

(8)

{~o}sstay1

term 1(pstay (s, 1))w used better structured representation element
w simprovew pw (w ) Equation 6.
use V new (s, k) denote coalitions expected utility obtained potentially reaching
new states (e.g. different s) maintaining k parallel interactions, using
optimal strategy x (s0 ) new future state s0 . term V new (s, k) take
account cost associated current k interactions. However consider
8. several states equal utility sorted according pre-defined order.

13

fiM ANISTERSKI , ARNE , & K RAUS

search costs associated search stages, originating new states. Notice
V new (s, k) equal zero coalition remains state k interactions. term
V new (s, k) expressed as:
(
)
(state(s )) k > 0

p
(
)V

k
k improve k k
k
V new (s, k) =
.
(9)
0
k=0
Note V new (s, w) actually one elements Equation 6. Therefore, Equation 6
formulated (w > 0):
V (s, w) =

c(w, n) +V new (s, w)
1 pstay (s, 1)w

(10)

calculation V new (s, w) using Equation 9 still exponential number parallel
interactions. order efficiently compute V new (s, w) Equation 10 consider w simultaneous interactions w sequential interactions, associated search costs. fully complies
definition V new (s, w) given (as cost w interactions already considered). justification representation method given Lemma 1 follows
directly Theorem 1.
Lemma 1. new state reached obtaining new set opportunities equivalent state
reached sequentially obtaining pairwise disjoint subsets set. Formally stated, given set
/ 6= j, 1w1 ... rwr = w
w number subsets 1w1 , ..., rwr , iwi w , iwi wj j = 0,
state s, following holds:
r
state(s, w ) state(state(...state(state(s 1w1 ) 2w2 )... r1
wr1 ) wr )

(11)

Proof. begin proving following supporting lemma:
Lemma 2. Let 0 , 00 two sets opportunities. 0 00 , state(0 )
state(00 ).
Proof. Since 0 00 , definition equivalence relation follows 0 00
. Theorem 1 state definition follows 0 {alloc(0 )} = state(0 )
00 {alloc(00 )} = state(00 ). transitive characteristic equivalence
relation follows state(0 ) state(00 ).
Let state let w set opportunities. prove Lemma 1 induction
number disjoint sets r. r = 2, let 0w0 00w00 sets satisfy 0w0 00w00 = w
/ Theorem 1 state definition obtain:
0w0 00w00 = 0.
0w0 {alloc(s 0w0 )} = state(s 0w0 )

(12)

12 Lemma 2 follows
state(s w ) = state(s 0w0 00w00 ) state(state(s, 0w0 ) 00w00 )

(13)

assume number disjoint sets r r > 2, 1w1 , ..., rwr , iwi w , iwi wj j =
/ 6= j, 1w1 ... rwr = w Equation 11 holds prove Equation 11 holds r=M+1.
0,
14

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

Given w sets 1w1 , ..., rwr , decompose w two disjoint sets wwr rwr ,
wwr = w \ rwr . Therefore proved r = 2 following holds:
state(s w ) state(state(s, wwr ) rwr ).

(14)

addition decompose set wwr r 1 sets 1w1 , ..., r1
wr1 . Therefore using
induction assumption:
state(s wwr ) state(...state(state(s, 1w1 ) 2w2 )... r1
wr1 )

(15)

Lemma 2 15 obtain:
r
state(state(s wwr ) rwr ) state(state(...state(state(s 1w1 ) 2w2 )... r1
wr1 ) wr )

(16)

14, 16 transitive characteristic equivalence relation follows that:
r
state(s w ) state(state(...state(state(s 1w1 ) 2w2 )... r1
wr1 ) wr )
specific case Lemma 1 subset consists single opportunity. Thus
calculation V new (s, k) recursively rely values V new (s0 , k 1), s0 represents
new states reached obtaining one additional opportunity p . Therefore
order compute V new (s, k), merely consider expected utility conducting one interaction
planned k interactions. Here, probability p stay (s, 1) coalition remains
state s, expected utility (having k 1 additional interactions go) V new (s, k 1)
(recall V new (s, 0) = 0 according Equation 9). Otherwise, new state 0 reached
single interaction, possibility reaching new states remaining
k 1 interactions (taking states utility consideration term V new (s0 , k 1))
probability pstay (s0 , k 1) remaining new state s0 even additional k 1 interactions
(in case utility one obtained resuming search state using
optimal strategy, V (s0 )). description encapsulated following recursive equation:
V

new

(s, k) =

(

pstay (s, 1)V new (s, k 1) + {~o}simprove p(~o)(V new (s0 , k 1) + pstay(s0 , k 1)V (s0 )) k > 0
1
0
k=0
(17)

s0 = state(s ~o ). Notice repeating calculation using equation
increasing k value starting k = 1, iteration includes single unknown parameter,
(V new (s, k)). addition, values V new (s0 , k 1) V (s0 ) depend values
computed state (s0 precedes set Ss ).
5.2 Algorithm Computing Coalitions Optimal Strategy
analysis leads algorithm computing coalitions optimal strategy (Algorithm
1). algorithm computes coalitions strategy expected utility possible states
init
SA . execution time polynomial w init
max (for convenience use wmax denote bound
number parallel interactions w, coalition begins search, i.e., coalitions state
= sinit = {}).
Notice stage execution, algorithm 1 reuses components computed earlier
stages. example, V new (s, w) appears computation V (s, w) (using Equation 10),
computation V new (s0 , w + 1) (using Equation 17, ~o0 p state(s0 ~o0 ) = s)
15

)

fiM ANISTERSKI , ARNE , & K RAUS

Algorithm 1 algorithm computing optimal search strategy x
Input: p - set potential opportunity types market; p(~o) - opportunity types probability
function; n - coalitions size; U j (), j = 1, ..., n - coalition members utility functions; c(w, n) search cost function; SA - set possible states coalition A.
Output: x (s) SA - coalitions optimal strategy.
1: Build set ordered states
2: i=1 |Ss |
3:
Set V (si , 0) = Vt (si ) using Equation 1
4:
Set V new (si , 0) = 0
5:
Set w = 1
6:
c(w, n) (Vt (s1 ) Vt (sinit ))
7:
Compute V new (si , w) using Equation 17
8:
Compute V (si , w) using Equation 10
9:
w++;
10:
end
11:
Set x (si ) = arg maxw0 (0,...,w1) V (si , w0 ), V (si ) = V (si , x (si ))
12: end
13: Return (x (si ), = 1, ..., s|SA | )
computation V new (s00 , w + 1) (using Equation 17, ~o00 p state(s00
~o00 ) = s). Storing result computational element memory, purpose
reusing later stages, significantly improves efficiency algorithm. accomplished
using two matrixes V V new size |SA | (winit
max + 1), corresponding V (s, w)
V new (s, w) values stored pair (s, w), representing state correlated result
number parallel interactions used calculations. Additionally, store x V
values using two arrays size |S | reusing x (s0 ) V (s) computation V new (s, w).
Theorem 2. Algorithm 1 returns optimal strategy coalition polynomial time w init
max .
Proof. proof, assume use matrices V V new storing values computed along execution algorithm described above. order build set ordered
states Ss , algorithm needs compute coalitions termination utility states
SA , according equation 1. Computing coalitions termination utility given state
takes O(|A|3 ) C2C markets9 O(|A|2 ) B2C markets10 . coalitions termination utility calculated state . Thus overall complexity computing coalitions
termination utility states O(|S ||A|2 ) O(|SA ||A|3 ) B2C C2C markets, respectively (the sets Ss SA size). Sorting members order build Ss takes
O(|SA | log |SA |). computation step 3 takes constant time assuming stored
9. mentioned previously, computing coalitions termination utility C2C markets equivalent finding
maximum matching weighted bipartite graph. Finding maximum matching weighted bipartite graph
done O(n(n log n + m)), n number vertices number edges (Wang, Makedon, & Ford,
2004). Since graph agent connected opportunities, number opportunities given
state bounded number agents, process computing coalitions termination utility takes O(|A|3 ).
10. mentioned previously, computing coalitions termination utility B2C markets done finding
opportunity agent coalition maximizes utility. number opportunities given state
bounded number agents, computing coalitions termination utility takes O(|A|2 ).

16

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

state coalitions termination utility built. Steps 3-5 performed states,
thus, overall complexity O(|S |). loop step 6 performed w init
max times.
init
reaching value w = w max + 1 step 6, loop condition longer holds (i.e.,
c(w, n) > (Vt (s1 ) Vt (sinit ))). Notice first elements calculated state 1 (i.e. one
maximum utility) according loop step 2. Here, explained section 4, expected utility strategy search resumed (i.e., using w 1) V (s, w) =
1
(formally, since simprove
= 0/ w pstay (s1 , w) = 1 V new (s1 , w) = 0 w (1, .., winit
max ), thus,
w
init
V (s1 , w) = w (1, .., wmax )). Therefore, optimal strategy state 1 terminate
search, i.e. x (s1 ) = 0 V (s1 , 0) = Vt (s1 ). state, s, reaching step 7,
algorithm already computed V (s0 ) V new (s0 , w) w (0, ..winit
max ) potential future new state s0 originating state s. due fact future states state
appear set ordered states (either higher utility, equal utility
yet sorted according function alloc). addition, number parallel interactions w 1, algorithm already computed V new (s, w 1). Therefore, computation
time step 7 sums order |O p | components already computed. Then,
reaching step 8, value V new (which part numerator equation 10)
already computed. Therefore, ignore, now, time computing coalitions new
state s0 encounters opportunity ~o given state time computing p stay
values, computation step 7 takes O(|O p |) computation step 8 takes constant
time. Since steps performed w init
max |SA | times, overall complexity computing steps O(|O p |winit
|S
|).

step
11,

algorithm chooses maximum value among

max
winit
values


already

computed.
Since
step performed state ,
max
overall complexity performing step O(w init
max |SA |). complexity computing
coalitions new state s0 encounters opportunity ~o given current state depends
market type. B2C markets, complexity O(|A|) store state also
alloc(s) value (we compute value compute coalitions termination utility).
C2C markets, computing new state takes O(|A| 3 )11 . store values using matrix f uture size |SA | |O p | new state s0 = state(s ~o) stored pair (s,~o).
Therefore, overall complexity computing future states states O(|S ||A||O p |) B2C
markets, O(|SA ||A|3 |O p |) C2C markets. Computing pstay (s, w) done constant
time based pstay (s, w 1). Moreover, store values using matrix Pstay size
stay (s, w) stored pair (s, w). Therefore, total com|SAg | winit
max value p
plexity computing pstay values O(|SA | winit
max ). Given analysis, overall complexity
algorithm O(|SA ||A|2 + |SA | log |SA | + |SA ||O p |winit
max + |SA ||A||O p |) B2C markets,
O(|SA | log |SA | + |SA ||A|3 |O p | + |SA ||O p |winit
)

C2C
markets.
Hence algorithm polynomial
max
init
wmax .

Note algorithm uses winit
max upper bound optimal number interactions x (si ),

= 1, ..., |Ss | . bound valid since according Equation 7 value w smax
increases
Vt (si ) decreases coalitions termination utility reaches minimum initial state
(Vt (sinit ) = Vt ({}) = 0). suggests significant improvement algorithm.
si
Instead using winit
max states, calculate specific upper bound, w max , state si
step 6, according Equation 7. point may require re-computation V (s , w) w

11. Computing coalitions new state C2C markets done finding maximum matching weighted bipartite
graph (see footnote 10 complexity analysis).

17

fiM ANISTERSKI , ARNE , & K RAUS

values used previous execution stages algorithm; however, total number
calculations state si many cases significantly decrease 12 .

6. Properties Parallel Cooperative Search (PCS) Model
Prior extending analysis scenarios coalition faces finite decision horizon,
emphasize interested unique characteristics parallel cooperative search derived
general analysis. First wish emphasize PCS model generalization
Single Agent Parallel Search (SAPS) Sequential Cooperative Search (SCS) model stated
following proposition.
Proposition 2. cooperative parallel search generalization single agent parallel
search cooperative sequential search models 13 .
Proof. analysis given section 4 clear Algorithm 1 results strategy
used cooperative sequential search single agents parallel search, specific

cases si wsmax
= 1 parallel interactions n = 1 agents, respectively. 14
Furthermore, emphasize coalitions expected utility never decreases using
proposed mechanism comparison sequential cooperative search. Indeed case
maintaining single interaction favorable strategy Algorithm 1 results one
interaction time strategy, used sequential cooperative search. Obviously using
parallel interaction decrease search cost (i.e. search cost conducting number
interactions sequentially equal smaller conducting interactions parallel),
sequential cooperative search dominating strategy stated next proposition.
Proposition 3. cost conducting number parallel interactions equal higher
cost conducting interactions sequentially (i.e w, w c(1, n) c(w, n)) use
parallel search match expected utility sequential cooperative search.
Proof. Consider optimal cooperative parallel search strategy x par . Obviously state
SA satisfying: (1) coalitions strategy conduct one interaction, x par (s) > 1
(2) future state s0 coalition conducts one interaction (or terminates search),
i.e., xpar (s0 ) 1. replace parallel search strategy state sequential
search state expected utility increase. worst
case scenario coalition execute w interactions incurring cost x par ,
ending expected termination utility. case, whenever coalition
reaches state s0 strategy according xpar terminate search prior completing w
interactions, expected utility least expected utility achieved using x par (otherwise
coalitions strategy s0 according xpar resume search) . Therefore use
new strategy improve expected utility. Using backward induction apply
logic former states x par implies one interaction parallel.
12. extent achieved improvement highly correlated specific environment coalition
operates.
13. Notice context single agent sequential search specific case single agent parallel search,
agent interacts single seller agent time.
14. Assuming similar cost structures three models.

18

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

Consequently, obtain sequential strategy results least large expected utility
xpar .
Next consider case agents fully homogeneous (in terms utility
functions) operate B2C markets. Here, prove optimal strategy coalition
stationary (i.e. change according current state). Furthermore, show
stationary strategy characteristic holds fully homogeneous agents also
agents correlated preferences. Two agents j correlated preferences
agent ai prefers ~o0 ~o00 agent j prefers opportunity ~o0 ~o00 (i.e ~o0 ,~o00 p ,
U j (~o0 ) U j (~o00 ) Ui (~o0 ) Ui (~o00 )) vice versa.
Theorem 3. B2C market, agents correlated preferences, search strategy
based reservation value15 Urv . scenario number parallel interactions
coalition uses according optimal strategy (in case resumes search) fixed
search (s SA Vt (s) < Urv exists x (s) = w f ixed ).
Proof. scenario, search problem equivalent problem single agent
utility function equal sum different agents utilities, U = U 1 +U2 + ... +Un .
case terminating search, coalition members always assigned
opportunity. Therefore, search strategy reservation value based, search terminated
upon reaching opportunity utility exceeding pre-set reservation value U rv . Since
probability reaching opportunity depend coalitions state, number
parallel interactions used throughout search fixed.
Nevertheless Theorem 3 hold C2C market even agents homogeneous next lemma states.
Lemma 3. C2C market even fully homogeneous agents (all agents utility
functions) search strategy always stationary optimal number parallel interactions changed search according coalitions current state.
Proof. order prove lemma, consider coalition operates C2C market
following environment:
Environment 4. Assume coalition two agents, 1 a2 , searching product (e.g., used
book) characterized one attribute (e.g., indicating whether book signed author)
two possible values, 1 (signed) 2 (not signed). results two opportunity types, o~ 1 = (1)
o~2 = (2). Assume opportunity o~1 rare found probability 1/100,
opportunity o~2 common (a probability 99/100). agents utilities 100 rare
opportunity 1 common opportunity. Consider cost conducting w parallel
interactions equal cost conducting w interactions sequentially, c(w, n) = wc(1, n),
cost conducting single interaction c(1, n) = 0.1 + 0.05n, (n > 1). Table 4 summarizes
environments setting.
consider two following states: (1) coalition starts search (the coalitions current
state initial state = {}) (2) coalition encounters opportunity o~ 1 (the coalitions current
15. reservation value value coalition sets a-priori terminates search reached opportunity
associated utility greater equal value (Sarne & Kraus, 2005).

19

fiM ANISTERSKI , ARNE , & K RAUS

Opportunity

Attribute

Probability

~o1
~o2

signed
signed

0.01
0.99

Utility
Agent a1 Agent a2
100
100
1
1

Table 4: Agents utilities four opportunities Environment 4
state s0 = {~
o1 }).
order coalition terminate search members exploit opportunity o~ 1 (otherwise expected utility resuming search exceeds search cost). Thus order
coalition terminate search former case (the coalitions state = {}) must encounter
opportunity o~1 twice latter case (the coalitions state 0 = {~
o1 }) encounter
opportunity o~1 once. Thus expect number optimal interactions, coalitions
state s, different number optimal interactions, state 0 . Indeed
computation coalitions optimal number interactions (using Algorithm 1) results
x (s) = 299 x (s0 ) = 161.

7. Finite Decision Horizon
important variant cooperative parallel search one agents forming
coalition restricted deadline finalizing search. example, consider coalition
searches costumes members wear costume party. case coalition
search costumes forever since customs value coalition members
purchased prior party. type environment often referred search theory
finite decision horizon environment. Specifically, within context paper consider
finite decision horizon environments coalition whole terminate search prior
within next r search rounds. Note sequential search definition equivalent
definition stating coalition conduct r additional interactions.
addition general advantages recognized parallel cooperative search, finite decision horizon environments model enable coalition conduct maximum
r interactions facilitated sequential model (whenever necessary). Moreover, environments coalition cannot improve performance interacting several sellers
parallel (e.g., cases described Proposition 3), introduction finite decision
horizon constraint creates strong incentive interact single seller time.
illustrated following example:
Example 3. Consider coalition operates B2C market, characteristics
Environment 4, introduced section 6. environment coalitions optimal
strategy infinite decision horizon search sequentially (see Proposition 3). Nevertheless, various finite decision horizon values coalition benefit using parallel
search technique. trivial example case coalition terminate
search within next search round. case, coalition conducts several interactions
next round, probability encounter opportunity o~1 increases comparison case
conducts single search. example coalition conducts 100 interactions
probability 0.99100 encounter opportunity o~2 search probability
20

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

1 0.99100 encounter opportunity o~1 search expected utility expressed
as: (1 0.99100 ) Vt ({~
o1 }) + 0.99100Vt ({~
o2 }) c(100, 2) = (1 0.99100 ) 200 + (0.99)100 2
0.2 100 = 107.52. value significantly greater expected utility using sequential cooperative search 0.01 Vt ({~
o1 }) + 0.99Vt ({~
o2 }) c(1, 2) = 3.78.
order compute coalitions strategy finite decision horizon model variant extend
definitions include number remaining search rounds, r. use V (s, w, r) denote
coalitions expected utility, conducts w interactions next round terminate
search within next r rounds. term V (s, 0, r) denotes immediate utility obtained,
coalition decides terminate search state s, thus: V (s, 0, r) = Vt (s). denote x (s, r)
V (s, r) coalitions optimal strategy expected utility (when using optimal strategy)
state s, terminate search within next r rounds.
V (s, r) x (s, r) calculation similar infinite decision horizon case, except
whenever coalition reaches decision deadline inevitably terminates search. Namely,
r = 0, x (s, 0) = 0 coalitions expected utility V (s, 0) equal Vt (s).

arg maxw V (s, w, r) r > 0
(18)
x (s, r) =
0
r=0
(19)

V (s, r) = V (s, x (s), r)

begin computing V (s, w, r), r > 0 (since V (s, w, 0) = 0, w). Here, apply
analysis methodology used section 4. However, expected utility coalition
resuming search following current search stage reflect change decision
horizon. Therefore, instead using Equation 4, use following modification:



V (s, w, r) = c(w, n) +

w simprovew

pw (w )V (s0 , r 1) + pstay (s, w)V (s, r 1),

w > 0, r > 0
(20)

= state(s w ).
computation Equation 20 exponential number parallel interactions. order
reduce complexity V(s,w,r) r > 0 consider two following cases:
s0

w = 1: coalition encounters single opportunity thus Equation 20 expressed as:
V (s, 1, r) = c(1, n) +



{~o}simprove

p(~o)V (state(s ~o ), r 1) +



p(~o)V (s, r 1),

r > 0

{~o}sstay1

1

(21)
case, additional computational complexity introduced, comparison SCS
model (Sarne & Kraus, 2005).
w > 1: attempt find computational means extracting value term
w simprovew V (s0 , r 1) + pstay (s, w)V (s, r 1) (in Equation 20) complexity polynomial
w. expression denotes coalitions expected utility conducts w interactions
without considering cost associated conducting w interactions. order efficiently compute value consider coalitions expected utility conducting one
w interactions obtaining opportunity ~o. case, coalitions expected utility
21

fiM ANISTERSKI , ARNE , & K RAUS

equal coalitions expected utility starting state state(s ~o) conducting
w 1 interactions plus cost equivalent cost maintaining w 1 interactions,
c(w 1, n) (these added since already subtracted expected utility
Equation 20 V (state(s ~o), w 1, r) subtracts again):
V (s, w, r) = c(w, n) +



p(~o)(V (state(s ~o), w 1, r) + c(w 1, n)),

~oO p

w > 1, r > 0
(22)

sum Equation 22 represented sum expected utility opportunities change coalitions current state sum opportunities change
coalitions current state:
V (s, w, r) = c(w, n)+



p(~o)V (s0 , w1, r)+

{~o}simprove
1

p(~o)V (s, w1, r)+c(w1, n),

{~o}sstay1

w > 1, r > 0
(23)

= state(s ~o).
Equations 23 21 facilitate calculation V (s, w, r) polynomial time w. order
find coalitions optimal strategy, efficient bound optimal number interactions
given Equation 7 also valid finite decision horizon variant.
analysis leads Algorithm 2 modification Algorithm 1. algorithm
computes coalitions strategy expected utility states . order compute
V (s, r), algorithm uses backward induction. starts computing V (si , 0) states.
Here, coalition forced terminate search thus algorithm sets expected utility
equal termination utility: V (si , 0) = Vt (si ). Then, algorithm computes V (s , r, w) r > 0
using Equation 21 23 starting w = 1 till c(w, n) Vt (s1 ) Vt (sinit ). algorithm sets
coalitions optimal strategy number interactions w maximizes V (s , r, w).
Similar infinite decision horizon case, Algorithm 2 reuses components computed earlier
stages stage execution. Nevertheless, case order store results
memory reuse later, need three dimensional matrix V size |S | (winit
max + 1) (rmax + 1)
rmax initial decision horizon. corresponding V (s, w, r) values stored
triplet (s, w, r), representing coalitions state, number simultaneous interactions used
limit number rounds coalition conduct. Additionally, store V (s, r)
x (s, r) values two matrixes V X size |S | (rmax + 1) reusing x (s0 , r 1)
V (s0 , r 1) computation V (s, w, r).
following theorem prove algorithm 2 returns coalitions optimal strategy
expected utility, polynomial time w init
max (the bound number parallel interactions
computed using Equation 7) rmax .
s0

Theorem 4. Algorithm 2 returns optimal strategy coalition expected utility
following strategy, terminate search within next r max rounds, polynomial time winit
max rmax .
Proof. proof, assume use matrices storing computed values described
above. process extracting Ss done exactly finite decision horizon. Thus, step
1 complexity O(|SA ||A|2 + |SA | log |SA |) O(|SA ||A|3 + |SA | log |SA |) B2C markets,
22

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

Algorithm 2 algorithm computing optimal search strategy x coalition
terminate search within next r max rounds
Input: p - set potential opportunity types market; p(~o) - opportunity types probability
function; n - coalitions size; U j (), j = 1, ..., n - coalition members utility functions; r max -
maximum number search rounds terminating search (the decision horizon); c(w, n)
- search cost function; SA - set possible states coalition A.
Output: x (s, r),V (s, r) Ss , 0 r rmax - coalitions optimal strategy expected
utility.
1: Build set ordered states
2: = 1 |Ss |
3:
Set x (si , 0) = 0
4:
Set V (si , 0) = Vt (si ) using Equation 1
5: end
6: r = 1 rmax
7:
i=1 |SA |
8:
Set V (si , 0, r) = Vt (si ) using Equation 1
9:
Compute V (si , 1, r) using Equation 21
10:
Set w = 2
11:
c(w, n) (Vt (s1 ) Vt (sinit ))
12:
Compute V (si , w, r) using Equation 23
13:
w++;
14:
end
15:
Set x (si , r) = arg maxw0 (0,...,w1) V (si , w0 , r), V (si , r) = V (si , x (si , r), r)
16:
end
17: end
18: Return (x (si , r), = 1, ..., s|SA | , r = 0, ..., rmax )
C2C markets, respectively. computation steps 3 4 takes constant time, assuming
stored state coalitions termination utility built . steps performed
states, thus, overall complexity computing steps throughout loop O(|S |).
Again, step 8 performed constant time, assuming coalitions termination utility
stored state. state, s, limit number search rounds r > 0
reaching step 9 algorithm, algorithm already computed V (s0 , r 1) potential
future new state s0 originating state s. Therefore, computation step 9 simply summing order |O p | components already computed. Consequently, ignore
time computing coalitions new states 0 (we add time computing values
later), computation step takes O(|O p |). Since steps 8-9 performed rmax |SA | times,
overall complexity computing steps (when ignoring time computing new
states) O(|O p |rmax |SA |). Notice w = winit
max + 1 obtain c(w, n) > (Vt (s1 ) Vt (sinit )).
Thus, loop step 11 performed w init
max times. w > 0 reaching step 12,
0
algorithm already computed V (s , w 1, r) potential future new state 0 originating
state s. Therefore, computation step 12 takes O(|O p |) ignoring time computing new states. Since step performed r max |SA |winit
max times, overall complexity
computing step (when ignoring time computing new states) O(|O p |rmax |SA |winit
max ).
23

fiM ANISTERSKI , ARNE , & K RAUS

step 15, coalition chooses maximum value among w init
max values already
computed. Since step performed state , r {0, ..., rmax },
overall complexity performing step O(w init
max |SA |rmax ). Computing coalitions new state
s0 encounters opportunity ~o given current state done described
infinite decision horizon case. Thus, overall complexity computing future states states
O(|SA ||A||O p |) B2C markets, O(|SA ||A|3 |O p |) C2C markets. Given analysis,
2
overall complexity algorithm O(|S | log |SA | + |SA ||O p |winit
max rmax + |SA ||A| + |SA ||A||O p |)
3
B2C markets, O(|SA | log |SA | + |SA ||O p |winit
max rmax + |SA ||A| |O p |) C2C markets. Hence
16
init
algorithm polynomial wmax rmax .
Similar infinite decision horizon significant improvement algorithms peri , state,
formance achieved calculating using specific upper bound, w smax

step 11, according Equation 7.
following section illustrate properties PCS model infinite
finite decision horizons model variants.

8. Illustrative Examples PCS Model
efficient means calculating coalitions optimal strategy using parallel cooperative search, demonstrate specific properties search method. reference
use Single Agents Parallel Search (SAPS), Single Agents Sequential Search (SASS)
Sequential Cooperative Search (SCS) models.
8.1 Infinite Decision Horizon
begin illustrating parallel cooperative search infinite decision horizon. First
demonstrate influence level heterogeneity utility functions different coalition members coalitions performance (in terms expected utility achieved). order
demonstrate use following environment, used originally evaluating
performance SCS model (Sarne & Kraus, 2005):
Environment 5. coalition two agents, 1 a2 , searching opportunities defined two
attributes, B1 (e.g., quality) B2 (e.g., store rating), attribute value
discrete range (1, ..., 5) equal probability values. agents
heterogeneous respect way evaluate potential opportunity. Agent 1 associated utility function U1 (~o) = B1 + B2 , agent a2 associated utility function
U2 (~o) = 2(1 )B1 + 2B2 . Thus, parameter indicates level agents similarity/heterogeneity. search cost single agent conducting single interaction c base
w parallel interactions (w > 1): c(w, 1) = c base + c parallel (w 1). search cost coalition
size n c(w, n) = c(w, 1)ln(n + 1), (n > 1).
Figure 2 depicts expected utility per agent using different search methods 17
C2C market (left hand-side) B2C market (right hand-side) parameter similarity
level, , utility functions agents constituting coalition.

16. algorithm uses winit
max upper bound optimal number interactions x (si ), = 1, ..., |SA |. bound
si
valid since according Equation 7 value wmax increases Vt (si ) decreases coalitions termination
utility reaches minimum initial state (Vt ({}) = 0).
17. cooperative models average expected utility per coalition member measure used.

24

fiE NHANCING C OOPERATIVE EARCH

utility
9.3

utility
9.3
9.1

1



9.1

PCS

C ONCURRENT NTERACTIONS

PCS
1

8.9

8.9

2

2
SAPS
8.5
8.3

SAPS

8.7

8.7

8.5

3

SCS

8.3

4

8.1

SASS

7.9
7.7

3
SCS

8.1
7.9

4

SASS

7.7

0.01 0.11 0.21 0.31 0.41 0.51 0.61 0.71 0.81 0.91

0.01 0.11 0.21 0.31 0.41 0.51 0.61 0.71 0.81 0.91

similarity level ( )

Figure 2: Average expected utility per buyer agent function similarity level different models
different markets

Curve 1 graph depicts average expected utility two agents form coalition function similarity level agents utility functions, making use
suggested parallel cooperative search. Here, search cost conducting single interaction
set 0.2 (cbase = 0.2) search cost conduct additional interactions set 0.05
(c parallel = 0.05). expected model (represented curve 1) outperforms SCS model (represented curve 3) terms expected utility agents. two curves describe
average expected utility agents searches separately using Single Agent
Parallel Search (SAPS) (represented curve 2) Single Agent Sequential Search (SASS)
(represented curve 4) models. specific environment use cooperative parallel
search also outperforms single agent parallel search model though always case.
Notice results obtained cooperative parallel search consistent general
characteristic cooperative search (Sarne & Kraus, 2005) use method
B2C market results better expected utility C2C market. case separated
single searches (SAPS SASS models) market type affect strategy structure
expected utility since agent searches single opportunity benefit.
Figure 2 also reflects interesting insight contradicts important strategy domination
relationship found single cooperative sequential search techniques fully homogeneous agents (i.e. utility function, case = 0.5 example)
operating C2C markets. sequential search use single agent search always outperforms cooperative search C2C markets (when considering fully homogeneous
agents) (Sarne & Kraus, 2005), actual evidence parallel search concerned,
cooperative search technique may outperform aggregated result single homogeneous
agents search.
Figure 3 shows expected utility per agent using different search methods
C2C market (left hand-side) B2C market (right hand-side) parameter cost
conducting additional interaction c parallel (notice agents performance affected
value SASS SCS models). results based Environment 5 (the
environment used Figure 2), = 0.1 c base = 0.2. expected cost c parallel
25

fiM ANISTERSKI , ARNE , & K RAUS

decreases average expected utility using parallel models (PSC SAPS models) increases
(and thus greater improvement comparison sequential models). Note whenever
c parallel cbase performance converges one achieved obtaining one observation
time, used sequential models. behavior correlated Proposition 3.

0.17

0.21

9.7

utility

utility

9.7

9.5

9.5

9.3

9.3

9.1

1

9.1

PCS

1
8.9

8.9

SAPS

8.7

2

2
3

8.5

SCS

8.3

4

8.1

SAPS

8.7

8.5

3

PCS

SCS

8.3

4

SASS

SASS

8.1

0.01

0.05

0.09

0.13

0.17

0.21

0.01

0.05

0.09

0.13

0.17

0.21

cost parallel interaction

Figure 3: Average expected utility per buyer agent function c parallel using different search
methods different markets

cost conducting additional interaction c parallel also influences optimal number
interactions coalition conduct. Figure 4 shows optimal number parallel
interactions coalition conduct beginning search, x ({}), function
c parallel environment used Figure 3 (Environment 5). expected,
optimal number interactions, x ({}), increases c parallel decreases equal 1,
c parallel cbase .
Figure 4 coalitions optimal number interactions using PCS model higher
coalition operates C2C market B2C market.
intuitive explanation (in C2C market opportunity exploited one agents
coalition needs encounter opportunities C2C market) cannot generalized.
following example illustrates scenario optimal number interactions actually
greater operating B2C market.
Environment 6. coalition two agents, 1 a2 , searching product (e.g., computer
game) associated 4 types opportunities {~o 1 ,~o2 ,~o3 ,~o4 } (e.g., representing different configurations). agents utilities opportunities distribution given Table 5. search cost
single agent conduct single interaction c base = 0.2, search cost conducting
additional search c parallel = 0.1, c(w, 1) = cbase + c parallel (w 1). search cost
coalition c(w, n) = c(w, 1)ln(n + 1), (n > 1).
Figure 5 shows optimal number interactions used coalition (in SCS model)
agent (in SAPS model) beginning search, operating Environment
6. one see coalitions optimal number interactions using PCS model B2C
26

fiE NHANCING C OOPERATIVE EARCH

optimal # interactions

32



C ONCURRENT NTERACTIONS

(1) Coalition B2C

28
(2) Coalition C2C
24

1
(3) Agent A1

20
4

(4) Agent A2

16
12

3

2

8
4
0
0.01

0.06

0.11

cost parallel interaction

0.16

0.21

Figure 4: Coalitions agents optimal number parallel interactions beginning search
function c parallel

Opportunity

Probability

~o1
~o2
~o3
~o4

0.499
0.25
0.25
0.001

Utility
Agent a1 Agent a2
0
0
0
1
1
0
100
100

Table 5: Agents utilities four opportunities Environment 6
market (44) larger coalitions optimal number parallel interactions C2C market
(8). Moreover figure contradicts two additional hypothesis one may presume
optimal number coalitions parallel interactions. first coalitions optimal number
parallel interactions equal overall number parallel interactions,
agents conducts search autonomously. seen Figure 5, optimal number
parallel interactions B2C market greater total number parallel interactions,
agents conducts search autonomously (10 + 10 = 20 < 44). second hypothesis
suggests coalitions optimal number parallel interactions least number parallel
interactions, agents conducts search autonomously. This, proven
wrong Figure 5, coalitions optimal number parallel interactions C2C market
(8) smaller number parallel interactions agent 1 (10), number parallel
interactions agent a2 (10).
Next introduce make use simpler sample environment demonstrating
additional properties cooperative parallel search.
Environment 7. coalition two agents, 1 a2 , searching opportunities defined two
attributes, B1 (e.g., quality) B2 (e.g., store rating), attribute value
discrete range (1, 2) equal probability values. utility functions
used U1 (~o) = 1.9B1 + 0.1B2 U2 (~o) = 0.1B1 + 1.9B2 . search cost single agent
27

fiM ANISTERSKI , ARNE , & K RAUS

x*({})
45

36
27
18
9
0
PCS
C2C

PCS
B2C

SAPS
Agent 1

SAPS
Agent 2

Figure 5: Coalitions optimal number parallel interactions beginning search
c(w, 1) = 0.5 + 0.05w, coalition c(w, n) = c(w, 1) ln(n + 1), (n > 1). Table 6
summarizes environments setting.
Opportunity
~o1
~o2
~o3
~o4

(Attribute1,
Attribute2)
(1,1)
(1,2)
(2,1)
(2,2)

Probability
1
4
1
4
1
4
1
4

Utility
Agent a1 Agent a2
2
2
2.1
3.9
3.9
2.1
4
4

Table 6: Agents utilities four opportunities Environment 7

7.1
utility
7
6.9
6.8
6.7
6.6
6.5
6.4
1

2

3

4

5

6

7

8

9

10

# parallel interactions (w)

Figure 6: Coalitions overall expected utility function w
Figure 6 depicts expected coalitions overall utility respect number interactions
conducted beginning search (i.e. first search stage, coalition knows
opportunities), assuming states coalition uses optimal number
28

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

parallel interactions, x (s). Here, see effect two conflicting forces: number
parallel interactions coalition uses stage increases, probability associating better
opportunity two coalition members increases. However overall search cost
associated search stage increases. figure, conclude optimal number
parallel interactions used stage x ({}) = 5.
additional important characteristic cooperative parallel search wish emphasize
concerns number parallel interactions used part optimal search strategy along
search. single agents parallel search search strategy stationary (i.e. number
parallel interactions used change along search process) model number
parallel interactions along search needs maintained depends coalitions state
(i.e. set opportunities known coalition). demonstrated directed acyclic
graph (DAG) given Figure 7, describes search process Environment 7. vertices
graph present potential coalitions states (the state determined according relevant
set known opportunities, correlated definition given section 4). edge connects
two states s0 possibility reach state following search round
coalition conducts search according optimal search strategy. example directed
edge connects = {} s0 = {(1, 2), (2, 1)}, since coalition proceed 0
conducts five parallel interactions according optimal search strategy encounters
opportunities (1, 2) (2, 1). Notice reaching states {(1, 2), (2, 1)} {(2, 2), (2, 2)}
optimal strategy coalition terminate search. Therefore edge originating
states. illustrated Figure 7, number parallel interactions coalition
use according optimal search strategy (denoted x (s)) depends coalitions state.
comparison purposes, notice single agents separate search (i.e. single agent
parallel search model) optimal strategy constantly use 4 parallel interactions (as long
agents strategy resume search).

x*(s)=4
{(1,2)}
x*(s)=5

{}
Vt(s)=0

x*(s)=5

Vt(s)=6

{(1,1)}
Vt(s)=4

x*(s)=0
{(1,2),(2,1)}
Vt(s)=7.8
x*(s)=0

x*(s)=4
{(2,1)}
Vt(s)=6

{(2,2)}
Vt(s)=8

Figure 7: Optimal strategy potential transitions states simple B2C market

8.2 Finite Decision Horizon
section demonstrate properties PCS model variant coalition
given finite decision horizon. First explore influences cost conducting addi29

fiM ANISTERSKI , ARNE , & K RAUS

tional interaction, c parallel , agents expected utility. Figure 8 depicts results obtained
varying c parallel 0.01 0.3. figure use environment used Figure
3 (Environment 5), = 0.1, cbase = 0.2 decision horizon two search rounds.
cost conducting additional interaction c parallel decreases, coalitions expected utility
using parallel models PSC SAPS increases superiority sequential models
increases. Moreover shown Figure 8 parallel model outperforms sequential model even
c parallel cbase (e.g., c parallel = 0.2). Recall c parallel cbase parallel technique improve expected utility infinite decision horizon (as suggested
Proposition 3).
9.6

utility

utility

9.6

9.1

9.1

8.6

1

2
8.1

PCS

SAPS
8.1

SAPS

7.6
7.1

1 PCS
2

8.6

7.6

4

SASS

3

7.1

SCS
4

6.6

6.6

3
6.1
0.01

SASS

SCS
6.1
0.06

0.11

0.16

0.21

0.26

0.01

0.06

0.11

0.16

0.21

0.26

cost parallel search

Figure 8: average expected utility per buyer agent function c parallel coalition
terminate search within two next rounds.

Another factor affects expected utility decision horizon, represented value
parameter r. Figure 9 presents coalitions expected utility function r. figure
used environment used Figure 8, set cost additional
search, c parallel = 0.2. observed Figure 9, search models, earlier coalition
terminate search smaller expected utility. Moreover, expected utility improvement
obtained PCS (in comparison SCS) SAPS (in comparison SASS) models
increases r decreases. case ability conduct parallel interactions compensates
small number search rounds coalition conduct. Note r increases, average
expected utility per buyer agent converges average expected utility per buyer agent
infinite decision horizon model.
Finally, Figure 10 depicts optimal number interactions coalition (in PCS model)
agent (in SAPS model) beginning search, function r. figure
used environment parameters used Figure 9. expected number
parallel interactions increases r decreases.
demonstrated throughout section, PCS outperforms sequential cooperative search. Generally, magnitude improvement depends size domain, search
cost structure, different utility functions used.

30

fiE NHANCING C OOPERATIVE EARCH

utility
2

7.7

utility

PCS

1

8.2



SAPS

4

SASS

PCS
1
3

8.3

SCS

SAPS
2

7.2
6.7

C ONCURRENT NTERACTIONS

4

7.8

SASS

6.2
7.3

5.7
5.2

6.8

3

4.7

SCS

4.2

6.3

3.7
3.2

5.8
1

5

9

13

1

4

7

10

13

limit number search rounds- r

Figure 9: average expected utility per buyer agent function limit number search

optimal # interactions

rounds, r

(1) Coalition B2C

8

(2) Coalition C2C
6

3

(3) Agent A1
2
(4) Agent A2

4

4
1
2

0

1

5

9

limit number search rounds- r

13

Figure 10: Optimal number interactions function decision horizon, r

9. Discussion Conclusions
capability using parallel interactions part search process inherent infrastructure autonomous information agents. using cooperative parallel search, coalition
use new strategy, different structure comparison optimal strategy used
cooperative sequential search single agents parallel search. expected, use new
model potential significantly improving coalitions expected utility demonstrated
previous section. Furthermore, emphasize coalitions expected utility never
decrease using proposed mechanism comparison pure sequential cooperative search.
mainly suggested algorithm converge one interaction time strategy,
used sequential cooperative search (which specific case model) case
31

fiM ANISTERSKI , ARNE , & K RAUS

maintaining single interaction world states favorable. Obviously
search cost linear depends solely number interactions maintained,
use coalition increase number sellers interacts given
search round. Nevertheless, scenarios coalitions search cost combines additional fixed
components non-linear dependency number interactions maintained much realistic (Sarne & Kraus, 2005). scenarios parallel cooperative search yields large benefits
searchers.
parallel cooperative search weakly dominates sequential cooperative search,
necessarily dominate autonomous search (where agents search instead
cooperatively). decision whether use parallel cooperative search autonomously
depends amount coalition overhead costs induced cooperative search. Therefore,
computational means developed work enables agents identify fruitful opportunities searching cooperatively. Generally, introduction parallel cooperative search
model substantially increases number scenarios agents prefer search cooperatively.
novelty analysis given paper threefold. First supplies us better
understanding space opportunities, dividing improving non-improving areas.
Thus, instead dual simultaneous dependencies states define single
directional dependency pair states. Second, supplies bound optimum number
parallel interactions coalition uses state optimal strategy. Third represent parallel search sequential process, without breaching model assumptions.
three features allow us overcome main complexity associated attempt solve
problem set equations proffer finite algorithm polynomial number
parallel interactions (rather brute-force algorithm exponential number
parallel interactions) inevitably reaches optimal strategy. Moreover, provide comprehensive analysis parallel model extracting optimal search strategy given finite
decision horizon. Here, illustrate, coalition benefit significantly infinite
decision horizon integration parallel interactions cooperative search. PCS
model (both finite infinite decision horizon forms) general applied
coalition regardless cost function preferences used agents. adaptation
additional markets (other C2C B2C) achieved appropriately modifying
allocation function used assigning agent one opportunities found along
search.
focus research finding optimal search strategy coalition, given
structure, opportunity distribution, reported members preferences. treats coalition unified entity sharing common goal (to maximize sum members utilities).
Nevertheless, various, important aspects coalition formation, context cooperative search, always correlate assumptions used therefore
addressed. example, may incentive coalition members misreport preferences side-payments used. Alternatively, agents may able form side coalitions,
free-ride actively searching coalitions single agent member actively
searching coalition (that member would act spy side coalition). Moreover,
environments agents may face tight budget constraints could violated within
coalition side-payments. analysis important issues based ability properly
derive coalitions utility given specific setting. paper supplies functionality, laying
32

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

foundation enabling research. Future work encompass extend scope
research include additional topics associated coalition formation process,
coalition stability division payoffs coalition members. important
extensions include relaxation assumptions underlying opportunity model
(e.g., different deadlines different opportunities).

Acknowledgments
work supported part NSF grant IIS0705587 ISF. Kraus also affiliated
UMIACS.

Appendix A. Summary Notations
Notation

Meaning

B = (B1 , B2 , ..., Bk )

set attributes defining potentially available opportunities market, attribute B
assigned value finite set (b imin , ..., bimax ).
space potential opportunity types coalition may encounter.
coalition agents.
Agent j utility opportunity type ~o.
search cost associated coalition n agents
maintaining w simultaneous interactions seller agents.
function maps given set opportunities known
coalition members way aggregated agents utility
maximized
state coalition acquainted set known known
opportunities.
set possible states coalition throughout search.
immediate utility coalition terminates search
given set known opportunities known .
coalitions expected utility using w parallel interactions
state s.
immediate utility obtained, coalition decides terminate search state s.
number parallel interactions coalition conducts
state according optimal strategy.
collection w-sized sets opportunities produced environment coalition operates.

Op

U j (~o)
c(w, n)
alloc(known )

state(known )
SA
Vt (known ):
V (s, w)
V (s, 0)
x (s)
w

33

fiM ANISTERSKI , ARNE , & K RAUS

Notation
pw (w )
simprovew
sstayw
Ss
pstay (s, w)
wsmax
winit
max
V new (s, k)

r
x (s, r)
V (s, r)

V (s, w, r)

Meaning
probability encountering specific set opportunities w ,
maintaining w random interactions seller agents.
collection w-sized sets opportunities, w , change
coalitions current state s.
collection w-sized sets opportunities, w ,
change coalitions current state s.
set states belonging SA , sorted according
termination utilities Vt (s).
probability agent stays state conducting
w parallel interactions.
upper bound optimal number parallel interactions
used coalition state s.
upper bound optimal number parallel interactions
used coalition begins search (s = {}).
coalitions expected utility obtained potentially reaching
new states (e.g. different s) executing k parallel interactions (without incorporating search cost conducting
k interactions), assuming future strategy uses x (s0 ) new
future state s0 .
Finite Decision Horizon
maximum number search rounds coalition
conduct.
coalitions optimal strategy state s, needs terminate
search within next r search rounds.
expected utility coalition restricted maximum r
search rounds, reaching state assuming acts optimally
throughout search.
coalitions expected utility conducts w interactions
next round terminate search within next r
rounds.

References
Avis, D., & Lai, C. (1988). probabilistic analysis heuristic assignment problem.
SIAM J. Comput., 17(4), 732741.
Bakos, Y. (1997). Reducing buyer search costs: Implications electronic marketplaces. Management Science, 42(12), 167692.
Baron, D. P., & Ferejohn, J. A. (1989). Bargaining legislatures. American Political Science
Review, 83(4), 11811206.
Benhabib, J., & Bull, C. (1983). Job search: choice intensity. J. Political Economy, 91(5),
747764.

34

fiE NHANCING C OOPERATIVE EARCH



C ONCURRENT NTERACTIONS

Breban, S., & Vassileva, J. (2001). Long-term coalitions electronic marketplace. B.
Spencer, ed., Proceedings E-Commerce Applications Workshop.
Burdett, K., & Malueg, D. A. (1981). theory search several goods. Journal Economic
Theory, 24, 362376.
Carlson, J. A., & McAfee, R. P. (1984). Joint search several goods. Journal Economic Theory,
32, 337345.
Choi, S., & Liu, J. (2000). Optimal time-constrained trading strategies autonomous agents.
Proceedings MAMA-2000, pp. 1113.
Dias, M. (2004). TraderBots: New Paradigm Robust Efficient Multirobot Coordination
Dynamic Environments. Ph.D. thesis, Robotics Institute, Carnegie Mellon University.
Gal, S., Landsberger, M., & Levykson, B. (1981). compound strategy search labor
market. International Economic Review, 22(3), 597608.
Gatti, J. (1999). Multi-commodity consumer search. Journal Economic Theory, 86(2), 219244.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determination
minimum cost paths. IEEE Transactions Systems, Science Cybernetics, 4(2), 100107.
Ito, T., Ochi, H., & Shintani, T. (2002). group-buy protocol based coalition formation
agent-mediated e-commerce. International Journal Computer Information Science
(IJCIS), 3(1), 1120.
Kahan, J., & Rapoport, A. (1984). Theories Coalition Formation. Hillsdale, NJ:Lawrence Erlbaum Associates.
Keeney, R., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences Value Tradeoffs. New York, US:John Wiley & Sons.
Kephart, J., & Greenwald, A. (2002). Shopbot economics. JAAMAS, 5(3), 255287.
Lermann, K., & Shehory, O. (2000). Coalition formation large scale electronic markets.
Proceedings ICMAS-00, pp. 167174.
Li, C., Rajan, U., Chawla, S., & Sycara, K. (2003). Mechanisms coalition formation cost
sharing electronic marketplace. Proceedings ICEC-03, pp. 68 77.
Lippman, S., & McCall, J. (1976). economics job search: survey. Economic Inquiry,
14(3), 155189.
Manisterski, E. (2007). Protocols Strategies Agents Teamwork. Ph.D. thesis, Department
Computing Science, Bar Ilan University.
McMillan, J., & Rothschild, M. (1994). Search. Aumann, R. J., & Sergiu Hart, A. (Eds.),
Handbook Game Theory Economic Applications, pp. 905927. Elsevier.
Morgan, P. (1983). Search optimal sample size. Review Economic Studies, 50(4), 659675.
Morgan, P., & Manning, R. (1985). Optimal search. Econometrica, 53(4), 923944.
Rothschild, M. (1974). Searching lowest price distribution prices unknown.
Journal Political Economy, 82(4), 689711.
Sandholm, T., Larson, K., Andersson, M., Shehory, O., & Tohme, F. (1999). Coalition structure
generation worst case guarantees. Artificial Intelligence, 111(1-2), 209238.
35

fiM ANISTERSKI , ARNE , & K RAUS

Sarne, D., & Kraus, S. (2003). search coalition formation costly environments.
Proceedings CIA-03, pp. 117136.
Sarne, D., & Kraus, S. (2005). Cooperative exploration electronic marketplace. Proceedings AAAI-05, pp. 158163.
Shehory, O., & Kraus, S. (1998). Methods task allocation via agent coalition formation. Artificial
Intelligence, 101(1-2), 165200.
Stigler, G. (1961). economics information. Journal Political Economy, 69(3), 213225.
Talukdar, S., Baerentzen, L., Gove, A., & de Souza, P. S. (1998). Asynchronous teams: Cooperation
schemes autonomous agents. Journal Heuristics, 4(4), 295321.
Tsvetovat, N., Sycara, K., Chen, Y., & Ying, J. (2000). Customer coalitions electronic markets.
Proceedings AMEC-00, pp. 121138.
Wang, Y., Makedon, F., & Ford, J. (2004). bipartite graph matching framework finding correspondences structural elements two proteins. Proceedings EMBC-04, Vol. 42,
pp. 297275.
Yamamoto, J., & Sycara, K. (2001). stable efficient buyer coalition formation scheme
e-marketplaces. Proceedings Agents-01, pp. 576583.

36

fiJournal Artificial Intelligence Research 32 (2008) 757791

Submitted 01/08; published 08/08

Compositional Belief Update
James Delgrande
Yi Jin

jim@cs.sfu.ca
yij@cs.sfu.ca

School Computing Science
Simon Fraser University, Burnaby, BC, Canada V5A 1S6

Francis Jeffry Pelletier

jeffpell@sfu.ca

Departments Philosophy Linguistics
Simon Fraser University, Burnaby, BC, Canada V5A 1S6

Abstract
paper explore class belief update operators, denition
operator compositional respect sentence added. goal provide
update operator intuitive, denition based recursive decomposition
update sentences structure, may reasonably implemented. addressing
update, rst provide denition phrased terms models knowledge base.
operator satises core group benchmark Katsuno-Mendelzon update
postulates, postulates satised. Katsuno-Mendelzon postulates
obtained suitably restricting syntactic form sentence update, show.
restricting syntactic form sentence update, also obtain hierarchy
update operators Winsletts standard semantics basic interesting approach
captured. subsequently give algorithm captures approach; general
case algorithm exponential, not-unreasonable assumptions obtain
algorithm linear size knowledge base. Hence resulting approach
much better complexity characteristics operators situations.
also explore compositional belief change operators: erasure developed dual
operator update; show forget operator denable terms update;
give denition compositional revision operator. obtain compositional
revision, natural denition, yields Satoh revision operator.

1. Introduction
knowledge base typically static entity, rather evolves time. New information may added, old out-of-date information may removed. fundamental
issue concerns change managed. major body research addresses
question via specication rationality postulates, standards change operator
satisfy. postulates describe belief change knowledge level, independent
beliefs represented manipulated. various rationales motivating change evolving knowledge base, diering rationales seen
calling dierences background knowledge-level postulates. example, one
may think alteration world occurred, result
update knowledge bases representation world appropriate way. Or,
may think previous sources information fallible incomplete
better, accurate information world. So, case
revise beliefs. Another motivation might merge already-existing stores beliefs,
c
2008
AI Access Foundation. rights reserved.

fiDelgrande, Jin, & Pelletier

without giving priori preference one belief sets, aiming
achieve balanced resolution conicts. merging might used combine
belief states dierent agents, come joint course action based
sort things considered assimilation knowledge preferences
agents involved. also imagine linguistic reform, concept (or
rather, associated word) longer used. case one might say
users forgot concept/word.
dierences motivation led specic dierences sorts postulates
associated dierent motivations. Initially, AGM approach (Alchourron, Gardenfors, & Makinson, 1985; Gardenfors, 1988), standards belief revision
contraction functions given, wherein assumed knowledge base
receiving information concerning static1 domain, increased amount
accuracy information responsible changes knowledge base. Subsequently, Katsuno Mendelzon (1992) explored distinct notion belief change,
functions belief update erasure, wherein agent changes beliefs response
perceives changes environment. concept forget goes back George
Boole (1854), reintroduced work Lin Reiter (1994) Lin (2001)
way characterize agent may bring knowledge base up-to-date, forgetting
facts longer relevant way aect possible
future actions. approach syntactic nature: deals issue removing
facts removing ability describe facts.2 Finally, notion knowledge base
merging introduced generalization long-standing problem information
sharing databases, dierent databases might contain conicting information
(see Bright, Hurson, & Pakzad, 1992, survey). work Revesz (1993),
came interest constructing merged knowledge base best represents information set knowledge bases. One use thought way
determining course action best represents desires goals divergent
set knowledge bases, thereby forming group-level, all-things-considered knowledge base.
formal properties merging discussed previous works (e.g., see Lin &
Mendelzon, 1998; Konieczny & Pino Perez, 1998; Everaere, Konieczny, & Marquis, 2007).
distinctions formal properties dierent types change
brought papers initial AGM publications; instance, Katsuno
Mendelzon (1992) compared update revision; Konieczny Pino Perez (1998)
compared merging revision; Nayak et al. (2006) compared forgetting update.
postulates suggested initial authors dierent conceptions
belief change challenged writers. since approach towards
update conicts Katsuno Mendelzons postulates, wish show
1. Note static imply mention time. example, one could information
knowledge base state world different points time, revise information
points time. Thus, belief revision also applicable situation agent investigates
past event tries reason real state world event took place.
considerations revision update interrelated work Lang (2006).
2. Nayak, Chen, Lin (2006) described difference thus: belief erasure purports answer
question believe longer support belief cook killed Cock Robin?,
forgetting purports answer question believe Killing concept afforded
language?.

758

fiCompositional Belief Update

not, itself, reason reject theory every theory met objurgation
concerning foundational postulates.
Although focus paper update hence postulates given
Katsuno Mendelzon (1992) objections related postulates
believe considerations similar ones bring forward arena would hold
respect sorts belief change postulates. is, think
rationale imposing compositionality constraint belief update
brought bear cases belief revision, belief merging, forgetting.
knowledge level specications types belief change allow dierent ways
implement them. Various researchers proposed specic change operators
belief revision (Borgida, 1985; Dalal, 1988; Satoh, 1988), belief update (Forbus, 1989; Weber, 1986; Winslett, 1988), belief merging (Subrahmanian, 1994; Konieczny, 2000; Everaere,
Konieczny, & Marquis, 2005), forgetting (Lang, Liberatore, & Marquis, 2003; Nayak
et al., 2006). approaches formulated terms distance models
knowledge base models sentence revision update. general
less work dealing systems may readily implementable (but see, e.g.,
Williams, 1996; Delgrande & Schaub, 2003).
paper develop specic update operator operator intended
compositional, update expressed recursively terms syntactic
structure . Thus, knowledge base updated disjunction = b,
idea update function update certain combination
update b. update knowledge base conjunction = b
also function (a dierent one) update combination update
b. goal arrive operator whose results intuitive, denition
based recursive decomposition formula; hence (generally abstract) notion
update anchored part familiar computational setting. Second,
hope operators eciently implementable, least cases,
exploiting restrictions syntactic form formula. focus form
formula update; presumably approach described may combined one
knowledge base divided relevant irrelevant parts update
(Parikh, 1999).
goals generally realised. First, operators reasonable properties: many
Katsuno Mendelzon benchmark properties satised, including deemed
essential Herzig Ri (1999). dont obtain full irrelevance syntax,
obtain weaker results regard; well show irrelevance syntax
obtained restricting syntactic form sentence update. approach
also related approaches literature, hence serves establish links
approaches. fact, family compositional update operators obtained
imposing various syntactic restrictions regarded constituting family operators
Winsletts standard semantics makes basic nontrivial approach.
well, general approach update presented capture forget operator (Lin &
Reiter, 1994; Lang et al., 2003; Nayak et al., 2006), certain sense regarded
generalizing forget. also dene revision operator using obvious denition
operator; proves case operator corresponds revision
operator work Satoh (1988).
759

fiDelgrande, Jin, & Pelletier

approach leads straightforward algorithm implementing operators.
algorithm ecient, compared model-based denition distancebased operators. knowledge base disjunctive normal form, size knowledge
base contributes linear factor overall complexity. well, eciency
obtained size input sentence bounded constant.
next section reviews belief revision, update, forgetting, merging, describes
two specic approaches update. section following describes approach,
which, next section, give discussion analysis. last section contains
concluding remarks; proofs theorems given Appendix.

2. Background
described, goal introduce compositional method carrying belief change.
since part overall goal also examine place compositional belief change
operation various arenas take place, start outlining
details dierent conceptions motivate belief change, along
motivational considerations areas dierent types belief change
part ways. operators introduced implicitly, means set postulates
legitimate operator required obey. However, areas
dispute concerning correctness various postulates, mention
proceed, since approach case update obey
standard postulates update. start historically earlier case revision
moving central concern update. followed short expositions
concerning forgetting merging.
2.1 Formal Preliminaries
consider propositional language L, nite set atoms, is, propositional
letters, P = {, a, b, c, . . . }, truth-functional connectives , , . convenient, also used, considered introduced denition.
use logical equivalence; is, abbreviation ( ). Lits
set literals P {l | l P}. particular, also denoted . set literals
consistent 6 atom p P p, p . literal
l, use l denote l l P, l P l = l. Similarly, set literals ,
use denote set {l | l }. expression atom() denotes set atoms
formula . interpretation L maximal consistent set literals, i.e.,
every p P precisely one p , p holds. model sentence
interpretation makes true, according usual denition truth. od() denotes set models sentence . also make use notation odL() denote
set models sentence language (that say, language
atom().) interpretation write |= mean true . interpretation
set literals , dene = \ ( ). is, set literals
containing neither l l l . example, = {a, b, c}
{b, c} = {a}.
denote negation-normal form (in negation applies atoms only)
sentence nnf (). Similarly, denote conjunctive normal form disjunctive
760

fiCompositional Belief Update

3
normal form cnf ()
W dnf () respectively. setVof sentences (which
always nite), use denote disjunction conjunction
sentences . Proofs often based structure formula, specically
depth formula; formula , depth , depth() maximum nesting
connectives . Hence depth(a (b c)) = 3.
Later make use notion prime implicants sentence. consistent set
literals prime implicant i: 6 .4
limiting case , take (sole) prime implicant {}.

2.2 Belief Revision Contraction
seminal approach AGM (Alchourron et al., 1985), postulates proposed
constrain belief revision. approach, knowledge base K assumed belief
set, set sentences closed logical consequence. revision belief set
formula, K , new belief set formula believed. interesting case
initially believed, attain consistent belief set (assuming
satisable), beliefs dropped. Exactly beliefs must dropped
stipulated AGM approach; however, constraints form postulates
govern seen legitimate revision operators given. contrast,
development belief update Katsuno Mendelzon (1992) represented knowledge
base formula language L. Hence, paper also express things terms
postulates phrased terms formulas, rather belief sets.
following R-postulates comprise Katsuno Mendelzons reformulation
AGM revision postulates, function L L L.
(R1) .
(R2) satisable, .
(R3) satisable also satisable.
(R4) 1 2 1 2 1 1 2 2 .
(R5) ( ) ( ).
(R6) ( ) satisable ( ) ( ) .
dual operation, called contraction also dened, formula deleted
knowledge base. operation seen governed C-postulates,
using Katsuno Mendelzon formulation terms function L L L.
3. course formula , many different logically equivalent ways express cnf ()
dnf (). assume fixed procedure converting cnf (or dnf), converting negation normal
form, distributing disjunctions conjunctions (or vice versa dnf), hence justifying use
term conjunctive (disjunctive) normal form formula, rather (disjunctive) normal
form.
4. notion prime implicant confused dual notion prime implicate.
prime implicate formula clause, disjunction literals, , proper
subclause , 6 .

761

fiDelgrande, Jin, & Pelletier

(C1) .
(C2) 6 .
(C3) 6 6 .
(C4) 1 2 1 2 1 1 2 2 .
(C5) ( ) .
Revision contraction related AGM approach come
known Levi Harper identities. may expressed follows (using formulas
rather belief sets):
( )

(1)

( ).

(2)

rst case asserts revising corresponds contraction conjoined . second asserts contracting corresponds disjunction
result updated .
Although makes nice picture, various objections
presuppositions AGM model (e.g., representation belief states theories,
is, innite sets formulas) postulates said govern
operations revision contraction (especially (C5), postulate recovery). Issues
involved (C5) discussed Fuhrmann, 1991; Tennant, 1997; Hansson &
Rott, 1998; Rott & Pagnucco, 1999, others.
2.3 Belief Update Erasure
account revision contraction described preceding subsection usually seen
applying straightforwardly case one store information
unchanging, static world new information world received
agent, thereby forcing change representation unchanging, static world.
dierent picture put forward Katsuno Mendelzon (1992),
changing, dynamic world. conception, new information gathered
agent reects idea world dierent knowledge
base previously constructed. sorts changes knowledge base
required type new information seen dierent sorts envisaged
thought changes knowledge base going make contents
successively accurate. Although simplistic distinction
dierences two pictures (as mentioned Footnote 1), led large
body work point dierent conception. Distinct operations change
knowledge bases proposed: update, makes changes knowledge base
given information concerning change state world, erasure, removing
out-of-date information.
formula said complete implies truth falsity every
formula. approach (Katsuno & Mendelzon, 1992), update function
L L L satisfying following U-postulates.
762

fiCompositional Belief Update

(U1) .
(U2) ( ) .
(U3) satisable .
(U4) 1 2 1 2 (1 1 ) (2 2 ).
(U5) ( ) ( ).
(U6) 1 2 2 1 ( 1 ) ( 2 ).
(U7) complete ( 1 ) ( 2 ) (1 2 ).
(U8) (1 2 ) (1 ) (2 )
postulates not, however, uncontentious. Herzig Ri (1999) discussed
plausibility postulates given; assert U2, U5, U6 undesirable,
U7 unimportant. leaves (according authors) U1, U3, U4, U8
desirable.
Erasure also dened, manner analogous way described contraction
related belief revision. cases, specied formula believed
result. erasure denoted , formula believed
resulting state. operations, set postulates characterizing
erasure (given Katsuno & Mendelzon, 1992). Update erasure also interdenable
means identities, analogous Levi Harper identities, related revision
contraction:
( )

(3)

( ).

(4)

rst case asserts update corresponds erasing along conjunction
. second asserts erasing corresponds disjoining result
updated .
various specic update (and revision) operators proposed based
distance interpretations. focus two update operators, due Winslett.
rst, Possible Models Approach (PMA) (Winslett, 1988) well-known example update operator satisfying Katsuno Mendelzon update postulates.
second, standard semantics (Winslett, 1990) weak (in fact, arguably weakest
reasonable) approach update. denote operators pma ss respectively.
pma , that, interpretation w , pma selects interpretations closest w. update determined set
closest interpretations. notion closeness two interpretations w1 w2
Hamming distance, given follows:
Definition 1 diff (w1 , w2 ) = set propositional letters w1 w2 differ.
763

fiDelgrande, Jin, & Pelletier

Interpretation w1 less close w w2 , w1 w w2 , diff (w, w1 ) diff (w, w2 ).
follows w partial order interpretations. w -minimal set respect
designated in(M od(), w). specify PMA update operator:
[
od( pma ) =
in(M od(), w).
wM od()

update operator ss dened model , models
retain truth values atoms chosen. is:
[
od( ss ) =
{w2 od() | diff (w1 , w2 ) atom()}
w1 od()

operator ss weakest reasonable update operator following sense
(Winslett, 1990): First, update ss , true every model ss . Second,
every model language excluding atoms model ss (again
restricted language). Moreover, ss consists maximal set interpretations
satises preceding two properties. Hence update , truth values
atoms unaected update.
Example 1 (Katsuno & Mendelzon, 1992) Let L = {b, m} language discourse. Let = (b m) (b m), = b. interpretations w1 = (b, m),
w2 = (b, m); interpretations are: w1 = (b, m), w2 = (b, m). Thus
diff (w1 , w1 ) = {b} diff (w1 , w2 ) = {b, m}, hence w1 w1 w2 w2 6w1 w1 ,
in(M od(), w1 ) = {w1 }. Similarly, in(M od(), w2 ) = {w2 }. Hence, ( pma ) b.
result obtains ss .
concreteness, take b mean book oor, mean magazine
oor. means either book magazine oor,
both. robot ordered put book oor. Intuitively, end action
book oor, location magazine unknown.
operators give result.
Example 2 Let = (b m) = (b m). ( pma ) (b m), whereas
( ss ) (b m).
Here, neither book magazine oor. robot ordered put
least one oor. According pma operator, exactly one
oor action, according ss operator, least one oor.
2.4 Forget
focus specic approach update erasure, also relate approach
forget operator. notion forgetting goes back George Boole (1854),
though received recent attention Articial Intelligence by, e.g., Lin & Reiter,
1994; Lin, 2001; Lang et al., 2003; Nayak et al., 2006. propositional context, forget
atom, set atoms, remove information concerning atom set atoms.
764

fiCompositional Belief Update

suggested (in Nayak et al., 2006) forgetting corresponds removal
literals atoms language discourse case propositional forgetting (i.e.,
0-place predicate forgetting). general case, seen removing predicate
relation language, hence removing consequences might
due predicates presence.
Let [p/q] denote formula occurrences atom p replaced q.
usual denition forgetting (again, going back Boole) atom p given
[p/] [p/]. order forget set atoms , one takes disjunction
substitution 2|| combinations , elements .
following denitions. single atoms basically follow Nayak et al.
(2006); sets atoms use denition (Lin & Reiter, 1994). begin,
p-dual interpretation interpretation like truth value assigned
p changed negation. set interpretations closed p-duals if,
interpretation set, p-dual also set.
Definition 2 Given set interpretations atom p, operator
least set interpretations containing closed p-duals.

U

(, p) yields

Given this, dene forget atom set atoms, latter dened
recursively terms former:
Definition 3 Basis Case: Let formula p atom. forget p respect
given by:
]
od( p) =
(M od(), p)
= od([p/] [p/]).

Inductive Case: Let formula = {p1 , . . . , pn } set atoms. forget
respect given by:
= ( ( \ {pn })) pn .
example, (b c) (b c) b c. (Given knowledge base
stored Alberta Canada also either Vancouver British Columbia
Charlottetown Ontario, forgetting Alberta Canada would yield either
Vancouver British Columbia Charlottetown Ontario. would
result initial knowledge base Alberta Canada, either
Vancouver British Columbia Charlottetown Ontario.) another example,
(a b) . last example illustrates forget distinct erasure, since
property erasure imply ( ) (Katsuno & Mendelzon,
1992).
2.5 Belief Merging
Merging diers formally preceding three pictures knowledge bases
changed. preceding operators knowledge base sentence may need
occasion change knowledge base. one rephrases terms agents,
765

fiDelgrande, Jin, & Pelletier

types change postulate agent, store beliefs, faced
new belief needs accommodated. case merging, however, start
many belief sets need dealt way yields best, overall
single belief state. terms agents, again, number agents,
belief set, trying construct belief set best represents total
beliefs community agents. So, rather function maps belief set
sentence onto belief set, instead function maps number belief sets
single one. Following earlier practice representing belief sets single formula
(in manner Katsuno & Mendelzon, 1992), see earlier rationales
belief change envision function L L L, whereas merging envisions function
L L . . . L L. Note general case allows knowledge bases
list identical one another, thus list actually multi-set (bag).
goal merging, then, construct, nite list knowledge bases E,
appropriate, single merged knowledge base. Despite formal dierence
earlier three types belief change, nevertheless include discussion
conceptual similarities hold merging versions belief
change. Indeed, seems plausible suggest merging might denable terms
others, maybe sort generalization others. cases,
considerations compositionality belief change operators may relevant.
Definition 4 knowledge set multi-set (bag) knowledge bases.
V
Definition 5 E knowledge set, E conjunction formulas representing knowledge bases E.
Konieczny Pino Perez (1998, 2002) proposed following M-principles govern
merging operators. merge function function knowledge set E knowledge
base (E) satisfying following postulates, multiset union.5
(M1) (E) consistent
V
V
(M2) E consistent (E) = E.

(M3) E1 E2 knowledge sets E1 E2 , (E1 ) (E2 )
(M4) K1 K2 knowledge bases mutually consistent, (K1
K2 ) 6 K1
(M5) (E1 ) (E2 ) (E1 E2 )
(M6) (E1 ) (E2 ) consistent, (E1 E2 ) (E1 ) (E2 )
merging postulates contested: example, Meyer (2000) argued
M4 M6 rejected. (He argues grounds many
plausible merging operations obey postulates).
5. simplicity, list postulates (Konieczny & Pino Perez, 1998), include integrity
constraints.

766

fiCompositional Belief Update

natural method determining whether formula merged knowledge base determine whether appears majority members knowledge set merged (the merged knowledge base allow opinion
majority prevail). Liberatore Schaerf (1998) introduced method arbitration,
whereby goal adopt many dierent opinions possible members
knowledge set (try take many diering opinions possible account). Konieczny
Pino Perez (1998) proved arbitration operator (at least, sort
characterize) obeys M1 M6.6 interplay various merging
operations ability agent hide, lie, otherwise camouage preferences
agents try construct merged knowledge base surveyed
Everaere et al. (2007).

3. Approach
section discusses approach. Following intuitions motivation formal
approach, introduce compositional update and, subsequently, erasure. also consider
notion compositional belief revision, conclude that, least respect
specic approach, separate, distinct, notion compositional revision. Analysis
properties operators covered next section.
3.1 Intuitions
goal dene update operators compositional fashion that, updating
formula , update dened terms syntactic components . general idea
behind update , model replaced closest model(s)
(Katsuno & Mendelzon, 1992). approach, notion close model
determined part syntactic structure . is, recursively decomposed;
resulting (base case) literals used determine models update sets literals;
results combined depending connective(s) .
Consider may carried out. given knowledge base sentence
, wish determine new knowledge base believed. base case,
= l literal, wish update knowledge base literal l. implies l
need nothing. imply l, wish arrive knowledge base
l believed. is, want change knowledge base enough
entails l. Clearly, replacing model interpretation
= ( {l}) {l}.7 Thus, would every resulting interpretation entails l.
Consider next updating knowledge base conjunction literals = l1 l2 .
knowledge base l1 l2 believed will, obviously, one every model
knowledge base entails l1 l2 . carry replacing interpretation
od() interpretation = ( {l1 , l2 }) {l1 , l2 }. limiting case
needs taken care of, l1 l2 . situation, interpretation
l1 , l2 true, case exist, reecting attempt update
inconsistent formula.
6. forms part rationale Meyer (2000, 2001) deny M4 M6.
7. clear, |= l = ; 6|= l like l replacing complement.

767

fiDelgrande, Jin, & Pelletier

update knowledge base disjunction literals = l1 l2 , want
modify models least one l1 l2 true. Consider od()
6|= l1 l2 . 1 = ( {l1 }) {l1 } interpretation involves least change
l1 true, 2 = ( {l2 }) {l2 } l2 . Arguably
replaced 1 2 .
Last, generalize considerations deal arbitrary formulas.
update disjunction formulas, recursively determine update given
individual disjuncts return union resulting sets interpretations.
3.2 Compositional Update Operator
Based preceding intuitions, dene update operator c . begin
preliminary denitions. following, UL function interpretation
nite set formulas set interpretations. Informally, model knowledge
base set formulas resulting partial decomposition formula
update. value UL set interpretations closest , according . ease
notation, case single formula sometimes write UL(, ) UL(, {}).
Definition 6 interpretation finite L, define UL(, ) follows:
1. Lits
UL(, ) =



{( ) } 6

otherwise

2. = { } UL(, ) = UL(, {, } )
3. = { } UL(, ) = UL(, {} ) UL(, {} )
4. = {( )} UL(, ) = UL(, {, } )
5. = {( )} UL(, ) = UL(, {} ) UL(, {} )
6. = {} UL(, ) = UL(, {} )
worth noting recursion steps denition resemble closely
procedure use convert formula disjunctive normal form. dening
update terms operator, rst investigate properties. Foremost,
need show UL well-dened. is, specifying UL(, ), denition
phrased terms member ; needs shown order
elements selected recursion aect result.
Theorem 1 UL well-defined.
next two results reect inuence structure formula recursive
decomposition denition UL.
V
Theorem 2 UL(, ) = UL(, nnf ( )).

V
Theorem 3 UL(, ) = UL(, dnf ( )).

768

fiCompositional Belief Update

Note similar result extend conjunctive normal form. counterexample
given following:
UL(, {a (b c)}) = UL(, {a}) UL(, {b, c})
6= UL(, {a}) UL(, {a, c}) UL(, {b, a}) UL(, {b, c})
= UL(, {a, c}) UL(, {b, c})
= UL(, {(a b), (a c)})
= UL(, {(a b) (a c)}).
consider next couple fundamental properties UL:
Theorem 4 every w UL(, ) w |= .
Theorem 5 UL(, ) = iff .
next dene update operator directly terms UL.
Definition 7
od( c ) = { | UL(, {}), od()}.
Recall Example 1 = b = (b m) (b m). od( c
) = { | UL(, {}), od()} = {( {b}) {b} | od()}. Thus,
od( c ) = {{b, m}, {b, m}}, ( c ) b. result obtain
Winsletts approaches.8
Example 2, = b = (b m), obtain od( c ) =
{{b, m}, {b, m}}. case, update operator behaves pma , differently ss .
similarly dene erasure operator via UL. erase , analogy
Harper Identity, one update add result . Thus:
Definition 8
od( c ) = od() { | UL(, {}), od()}.
get results:
Theorem 6
c ( c )
c ( c ).
8. note however approaches differ. Specifically, PMA update operator satisfies
KM postulates, whereas operator not; see Section 4 details.

769

fiDelgrande, Jin, & Pelletier

3.3 Erasure
Denition 8 dened dual update, called erasure, directly terms UL.
equally well dene function analogous UL, call EL, directly dene erasure
operator rst principles. now, toward denition erasure. Briey,
motivation is: want erase consequence , semantically want
add interpretations models . corresponds single literal,
model would want add interpretation l replaced l.
corresponds conjunction, erased erasing either conjuncts;
corresponds disjunction, erase disjuncts must erased. continuing
fashion obtain following denition:
Definition 9 interpretation finite L, define EL(, ) follows:
1. Lits
EL(, ) =



W
6
{( ) }

otherwise

2. = { } EL(, ) = EL(, {} ) EL(, {} )
3. = { } EL(, ) = EL(, {, } )
4. = {( )} EL(, ) = EL(, {} ) EL(, {} )
5. = {( )} EL(, ) = EL(, {, } )
6. = {} EL(, ) = EL(, {} )
following results analogous Theorems 2 3; note occurrence cnf
Theorem 8, contrast dnf Theorem 3.
V
Theorem 7 EL(, ) = EL(, nnf ( )).
V
Theorem 8 EL(, ) = EL(, cnf ( )).
directly dene erasure operator

Definition 10 od(


c )


c

terms EL:

= od() { | EL(, {}), od()}

Unsurprisingly, notion erasure given Denition 8 equivalent.
show rst establishing following result:
V
Lemma 1 interpretation L, EL(, ) = UL(, { })

this, follows notions erasure given via Harper Identity,
direct denition via Denition 9 coincide:
Theorem 9 c


c .

Hence use symbol
well-denedness c .9

c

erasure. corollary, Theorem 9 also establishes

9. is, since UL well-defined (Theorem 1),
equivalence.

770

c

(Definition 8) hence


c



fiCompositional Belief Update

3.4 Revision
section consider extending compositional approach belief revision.
begin, might pointed nothing underlying motivation
makes c update operator, point suggests c might also regarded
revision operator, albeit weak properties. However, regardless intuitions, recursive decomposition implicit Denition 6 yields operator update-like properties,
sentence update , one eectively deals models disjuncts
dnf (). revision, contrast, intuition one deals models
(in sense) closest knowledge base . Hence, operator c
really appropriate revision operator.
suggests possibly-feasible approach dening compositional revision: dene
revision , one rst uses operator c nd candidate set models ,
employs distance function determine subset models closest
models whole. is, formulas , , update dened (in one
fashion another) respect models . revision contrast, denition
revision makes reference subset models ,
closest (in sense) models . sense then, update logically weaker
operator revision. Thus revision operator dened respect
rst applying (compositional) update operator get candidate set models .
set ltered, removing models minimal distance
closest models . depending notion distance employed, one might
expect obtain dierent revision operators given compositional update operator.
two common notions distance used model-based belief change,
one based set containment cardinality. rst case, formulas
, , dene
min (, )

=

min ({M1 M2 | M1 od(), M2 od()}),

sets B, AB symmetric dierence B. Satohs (1988)
revision operator dened follows.
Definition 11
od( ) = {w od() | w od(), ww min (, )}.
example, let = abc let = a(bc). = (abc)(abc).
dene corresponding compositional revision operator follows:
Definition 12
od( ) = {w | w UL(w, {}), w od(), ww min (, )}.
However, turns revision operator fact coincides Satoh revision operator:
Theorem 10 .
771

fiDelgrande, Jin, & Pelletier

follows straightforward corollary use distance metric based number diering propositional symbols two interpretations, obtain revision
operator (Dalal, 1988).10 obvious approaches compositional revision,
obtain new revision operators; say, recursive decomposition
denition UL serve select among models interesting sense
respect revision.
However, considerations lead one interesting result, point
way algorithms may eciently compute Satoh Dalal revision:
compute Satoh revision example, one use Denition 6 determine relevant
subset models , use min (, ) determine closest subset
models set models . discuss Section 5, initial ltering models
may done eciently certain syntactically-restricted cases.

4. Analysis Compositional Update Erasure
start, consider Katsuno-Mendelzon update postulates operator
satises. consider set corresponding compositional erasure postulates,
since results analogous update postulates, limited
additional interest. considering update postulates, explore update
erasure operators, including properties resulting restriction syntactic
form formula update, comparison related approaches.
Theorem 11 c satisfies U1, U3, U5, U7, U8.
counterexample U2, consider rst example given above, illustrating
approaches Winslett, c = (b m) (b m) = b m.
approach, update (as given Denition 7) rst disjunct viz., b,
yields interpretations {b, m} {b, m} update second disjunct, m, gives
interpretations {b, m} {b, m}. Hence c (bm) characterized interpretations
{b, m}, {b, m}, {b, m} get c (b m) (b m). U2 would dictate
result ; however, example suggests U2 problematic
context update. borrow example works (Herzig & Ri, 1999; Brewka
& Herzberg, 1993), suppose agent believes p (that certain coin shows heads).
world changes toss coin (where agent see result).
Letting p coin shows tails, note agent believe (p p). Yet
note p (p p); U2 would stipulate p (p p) p, contrary
want. operator c , hand, includes additional model. appears
make sense, updating b really telling knowledge base
world changed one b b b true. Thus, case
update operator behaves like Gricean belief change operator Delgrande, Nayak,
Pagnucco (2005), goal incorporate new information.
10. is, fixed formulas, model Dalal revision model Satoh revision. Rephrasing
Definitions 11 12 cardinality-based distance gives result analogous Theorem 10 Dalal
revision. omit details.

772

fiCompositional Belief Update

note modify c operator simple fashion satisfy U2 follows:11




c =
c otherwise
purposes, although U2 indeed satised, modication sheds light
original goal investigating ramications developing compositional update
operator, pursue modication.
next consider counterexample U4. Although ((a b) b) b, nonetheless
od(ac ((ab)b)) = {{a, b}, {a, b}} od(ac b) = {{a, b}}. U4 satised
since compositional approach parts sentence may provide implicit results
explicit sentence. Consider (ab)(bc) illustrate point. Updating
sentence eected updating individual components, viz., (a b)
(b c). However, implicit parts fact (a c) also true,
addition (implied) sentence would aect result update. consider
behaviour below.
counterexample U6 given following. Let
= ab
1 = (a a)
2 =

od((a b) c (a a)) = od()
also have:
od((a b) c ) = od(a b)
case c 1 2 also c 2 1 . Thus antecedent conditions
U6 satised, c 1 c 2 .
c satisfy U4 (substitution logical equivalents) general,
satisfy weaker conditions. First, update obviously satises substitution logical
equivalents rst argument c . well, light Theorems 2 3, 1
2 share negation normal form disjunctive normal form, may
substituted one formula update. summarize results
follows:
Observation 1
1. 1 2 (1 c ) (2 c ).
2. nnf (1 ) = nnf (2 ) ( c 1 ) ( c 2 ).
3. dnf (1 ) = dnf (2 ) ( c 1 ) ( c 2 ).
11. Borgida (1985) employed similar definition respect revision operator.

773

fiDelgrande, Jin, & Pelletier

Despite failing satisfy postulates (which, noted, overlap
postulates Herzig & Ri, 1999, think undesirable), c exhibit nice property,
reecting compositional nature operator, operators appearing
literature satisfying Katsuno Mendelzon postulates fail satisfy. following
version disjunction property holds.
Theorem 12 c (1 2 ) ( c 1 ) ( c 2 )
Corollary 1 ( c 1 ) ( c 2 ) implies c (1 2 ).
corollary observed strengthening U7.
update operator satises postulates deemed desirable Herzig Ri
(1999), exception U4. discussed above, U4 satised due
interaction parts sentence. would seem could compile implicit
information sentence would obtain full substitution equivalents,
expressed U4. So, one way satisfy U4 redene c rst get
information implicit interaction compositionally distinct parts update
sentence. dening operators consider set prime implicants
sentence. call modied operator pi
c . Let P I() set prime implicants .
W
Definition 13 pi
P I()
c = c
Theorem 13 pi
c satisfies U4

Although pi
c satises U4, lose U7. counter-example U7 given
= abcd
1 = (a d) (c d)
2 = (a d) (c d).

od( pi
c 1 ) = {{a, b, c, d}, {a, b, c, d}}
od(

pi
c



2 ) = {{a, b, c, d}, {a, b, c, d}}

pi
Hence od( pi
c 1 ) od( c 2 ) = {{a, b, c, d}}. hand
pi
od( pi
c (1 2 )) = od( c ((a d) (c d) (a d) (c d)))

= od( pi
c d)
= {{a, b, c, d}}.
Conversion prime implicants eect removes irrelevant redundant syntactic information, illustrated preceding example 1 2 fact equivalent
atom d. pursue line inquiry considering, formula update
, syntactic representation proposition expressed language .
given formula
WV , recall odL() set models , language .
formula
odL() would formula expressed disjunctive normal form;
example odL((a b) c) would expressed (a b c) (a b c) (a b c).
dene update operator follows:
774

fiCompositional Belief Update

WV
Definition 14 od( ss
odL())).
c ) = od( c (

obtain update operator fact Winsletts standard
semantics:
Theorem 14 ss
c ss .
pursue direction one step further, dene update operator
update formula characterized models expressed dnf. dene:
WV
) = od( c (
od())).
od( triv
c

Denition 14, except models , rather models
language . However easily shown interesting operator, since
removes old information have:
) .
Observation 2 ( triv
c

next proceed slightly dierent direction compare update operator
forget operator. Recall forgetting set atoms formula basically removes
information set atoms; sense forgetting analogous
decreasing language set atoms.
begin with, noted update operator contraction-like
properties similar forget. example, (a b) c (a a) readily shown equivalent
b. again, sense, update read updating precisely a,
case would indicate tautologous information concerning a. fact,
following result (recall use denote forget):
Theorem 15 Let L let P.


^
= c (p p).
p

Hence forgetting set atoms special case update operator. (Given Theorem 6, forget course also expressible via erasure analogous fashion.) Last,
establish result forget operator Winsletts standard semantics.
hindsight obvious, result appear previously noted.
V
Theorem 16 formula P, let = l (l l).
= ss
c .

summary, observed discussion obtained
hierarchy operators, based extent information made explicit.
basic case, triv
, update formula syntactic
c
representation models language; trivial update operator results.
basic interesting operator given ss
c , Winsletts standard
pi
semantics, followed c c . well, introducing tautologies, also
capture notion forgetting atoms.
already noted update operator c distinct Winslett PMA
approach. best knowledge also distinct specic approaches
appearing literature, including surveyed (Herzig & Ri, 1999).
775

fiDelgrande, Jin, & Pelletier

5. Algorithms Complexity
section present syntactic characterization well algorithm computing
compositional update. also analyze complexity algorithm variety
assumptions. Specically, analyze complexity algorithm applied
propositional sentences general, sentences disjunctive normal form,
sentences whose sizes bounded specied constant.
start background notions. Recall [p/q] denotes formula
occurrences atom p replaced q. write p. denote formula
[p/] [p/]. P = {p1 , , pn } set atoms P., called eliminant
P , stands p1 .( (pn .)) (Brown, 1990). Intuitively, eliminant P
viewed formula representing knowledge concerned
atoms P . eliminated information members P replacing
two possible values, , thus leaving information .
shown Winsletts standard semantics syntactically captured
based notion eliminant (Doherty, Lukaszewicz, & Madalinska-Bugaj, 1998). Let
P = atom(),
ss ( P.)
(5)
5.1 Syntactic Characterization Algorithms
ready provide syntactical characterization compositional update.
idea quite similar (Doherty et al., 1998). However, approach rst converts
update formula disjunctive normal form, deals disjunct.
_
U pdate(, ) = {( P.) | dnf (), P = atom(t)}

following results establish correspondence semantical denition
syntactic characterizations compositional update.
Lemma 2 Suppose term (a conjunction literals) P = atom(t).
od(( P.) t) = {w | w UL(w, t), w od()}
Theorem 17 od( c ) = od(U pdate(, )).
Corollary 2 c U pdate(, )
need compute compositional update, therefore, ability compute
eliminants. proposed Brown (1990), eliminant P. constructed follows.
1. Convert dnf t1 tn (each ti conjunction literals)
2. Replace ti ti P .
ready provide algorithms compositional update. assume
dnf () refers disjunctive normal form represented clause form,
formula represented sets sets literals. case, members dnf ()
implicitly disjoined, set literals making member dnf () implicitly
conjoined.
following algorithms, let , L P set atoms:
776

fiCompositional Belief Update

Algorithm Eliminant(P, )
1.

2.
term dnf ()
3.

4.
literal l
/P
5.
l
/ P l
6.
l
7.

8.
return
Algorithm U pdate(, )
1.

2.
term dnf ()
3.
P = atom(t)
4.
(Eliminant(P, ) t)
5.
return
Lets consider Example 1 = b = (b m) (b m).
U pdate(, ) = (Eliminant({b}, ) b). Since Eliminant({b}, ) =
( m) ( m), equivalent , obtain U pdate(, ) b. Thus,
U pdate(, ) c , already shown c b.
Example 2, = b = (b m), obtain U pdate(, ) =
(b m) (b m). Again, result obtain c .
5.2 Complexity
sequel, analyze space complexity update algorithm; is,
interested large updated knowledge base could be. Unfortunately, applied
arbitrary formulas, algorithm U pdate may cause exponential space blowup,
disjunctive normal form formula could exponentially large.
Theorem 18 space complexity U pdate(, ) O(2(||+||) ) , L;
However, able show exponential space blowup inevitable
algorithm compositional update. end, need introduce so-called advice-taking
Turing machine (TM) non-uniform complexity class, see (Johnson, 1990).
advice-taking TM TM advice oracle, considered
function positive integers strings. input x, machine loads string a(|x|)
continues usual based two inputs x a(|x|). Note oracle string
a(|x|) depends size input x. call advice oracle polynomial
|a(n)| < p(n) xed polynomial p positive integers n. X usual
complexity class dened terms resource-bounded machines (e.g., P NP) X/poly
class problem decided machines resource bound
augmented polynomial advice oracles. class X/poly also known non-uniform
X; particular, P/poly appears much powerful P. However,
shown unlikely NP P/poly, otherwise polynomial hierarchy would collapse
777

fiDelgrande, Jin, & Pelletier

p2 (Karp & Lipton, 1980). result used show unlikely exists
algorithm compositional update polynomial space bound.
Theorem 19 Assume exist polynomial p algorithm Update compositional
update U pdate(, ) c |U pdate(, )| p(|| + ||), belief
base formula . NP P/poly.
pursue result one step further, show algorithms sensible update operators cause exponential blowup. Formally, say update operator
sensible consistent set literals :
^
od(
) = { | (w ) , od()}

Arguably, condition intuitive natural (cf. discussions Section 3).
fact, almost update operators literature sensible.
Theorem 20 exists polynomially space bounded algorithm sensible update
operator, NP P/poly.

remark result also proves Winsletts conjecture stating
exist polynomially space bounded algorithm standard semantics (see Winslett,
1990).
algorithm becomes tractably better applied formulas disjunctive normal
form, update formulas whose sizes bounded.
Theorem 21 space complexity U pdate(, ) is:
1. O(|| ||) , dnf;
2. O(||) dnf || < k constant k.
Arguably, practice, update formula (representing changes world)
relatively small. Therefore, relatively easy convert dnf, also reasonable
assume size bounded. usually restrict size belief
base , converting dnf could computationally much expensive. Fortunately,
need compile (o-line) original belief base dnf, output
U pdate algorithm automatically dnf updated belief base. considerably
facilitate update belief base.

6. Conclusion
presented belief change operators updating knowledge base denition operators compositional respect sentence added. intent
provide operators transparent denitions, based structure formula
belief change. result lose standard postulates update, although
satisfy core group standard postulate set. achieve full irrelevance
syntax sentence update replaced disjunction prime implicants.
778

fiCompositional Belief Update

approach interesting rst, founded diering intuitions
operators, based decomposition formula rather models
formula, second, allows straightforward (under reasonable assumptions)
ecient implementation. distinct previous update operators appeared
literature, capture Winsletts standard semantics approach update
restriction approach. fact, update operator, dierent syntactic restrictions, may regarded constituting family update operators Winsletts
standard semantics weakest interesting approach. turn update revision, discover new, interesting compositional revision operator; nevertheless,
results indicate rst computing compositional update, one implement
Satoh Dalal revision operator eciently, consider subset
models formula revision, certain cases signicant
speedup naive algorithm.
open question concerns combining approach one designed exploit
structure knowledge base (such discussed Parikh, 1999 characterized
terms PMA updates Peppas, Chopra, & Foo, 2004). second, technical question
fully explored concerns behaviour c erasure operator. example,
let = (a b) (a b). Then, get c (a b) b. So, updating
knowledge base formula already implied knowledge base, actually
removed information. This, discussed earlier, quite reasonable one considers
update (in contrast revision) b asserts world changed
one {a, b}, {a, b}, {a, b} true. Finally, would interest apply
compositional approach merging knowledge bases.

Acknowledgments
early precursor paper presented FLAIRS 2007 (Delgrande, Pelletier,
& Suderman, 2007). grateful audience comments; also
beneted perceptive comments three JAIR referees. Delgrande Pelletier
also acknowledge support Canadian NSERC granting agency.

779

fiDelgrande, Jin, & Pelletier

Appendix A. Proof Theorems
Proof 1.
Observe UL associative commutative respect top-level conjunctions
top-level disjunctions. is, example
UL(, { ( )} ) = UL(, {( ) } ).
similar observation made negations top-level conjunctions
disjunctions; example
UL(, {( ( ))} ) = UL(, {(( ) )} ).
use basic facts without comment sequel.
means particular showing order-independence UL
respect second argument, need consider general caseVof UL(, )
= {1 } {2 }, since UL(, {1 , . . . , n }) = UL(, {1 , ni=2 }).
Given preamble, need show formulas 1 2 ,
UL(, {1 } {2 }) independent whether initial recursion terms 1
2 .
proof depth formula.
BASE:
Assume depth(1 ) 1 depth(2 ) 1:
1. depth(1 ) = depth(2 ) = 0 1 , 2 atoms result follows trivially.
2. connective 1 , 2 negation 1 , 2 literals
result follows trivially.
3. connectives 1 , 2 {, }, 1 , 2 reduce sets literals,
previous case applies.
4. 1 a1 a2 , 1 literal, Step 3 denition applies,
result obtains easily. converse 2 a1 a2 , 2 literal course
yields result.
5. 1 a1 a2 2 b1 b2 ,
UL(, {a1 a2 } {2 }) = UL(, {a1 , a2 } {2 })
= UL(, {a1 , a2 } {b1 b2 })
= UL(, {b1 b2 } {a1 , a2 })
= UL(, {b1 } {a1 , a2 }) UL(, {b2 } {a1 , a2 })
= UL(, {b1 } {a1 a2 }) UL(, {b2 } {a1 a2 })
= UL(, {b1 b2 } {a1 a2 })
= UL(, {b1 b2 } {1 })
780

fiCompositional Belief Update

6. 1 a1 a2 2 b1 b2 ,
UL(, {a1 a2 } {2 }) = UL(, {a1 } {2 }) UL(, {a2 } {2 })
= UL(, {a1 } {b1 b2 }) UL(, {a2 } {b1 b2 })
= UL(, {b1 b2 } {a1 }) UL(, {b1 b2 } {a2 })
= (UL(, {b1 } {a1 }) UL(, {b2 } {a1 }))
(UL(, {b1 } {a2 }) UL(, {b2 } {a2 }))
= UL(, {b1 , a1 }) UL(, {b2 , a1 })
UL(, {b1 , a2 }) UL(, {b2 , a2 })
Analogous manipulations show UL(, {b1 b2 } {1 }) yields result.
STEP:
induction hypothesis, assume result holds depth(1 ) n
depth(2 ) n. show desired result obtains depth(1 ) (n + 1)
depth(2 ) (n + 1).
A: Consider rst depth(1 ) n depth(2 ) = n + 1.
1. 2 form :
UL(, {1 } {2 }) UL(, {1 } {}), result follows
induction hypothesis.
2. 2 :
UL(, {1 } {2 }) = UL(, {1 } { }) = UL(, {1 , , }) = UL(, { }
{1 }).
3. 2 :
UL(, {1 } {2 }) = UL(, {1 } { }) = UL(, {1 , }) UL(, {1 , })
UL(, {2 } {1 }) = UL(, { } {1 }) = UL(, {1 , }) UL(, {1 , }).
4. 2 ( ) 2 ( ):
handled respectively.
B: Consider next depth(1 ) = n + 1 depth(2 ) = n + 1.
1. 1 2 :
1 2 , result holds via induction
hypothesis.
2. 1 2 1 1 :
Assume without loss generality 1 1 1 .
(a) 2 2 2 :
UL(, {1 } {2 }) = UL(, {1 1 } {2 2 }) = UL(, {1 , 1 , 2 , 2 }) =
UL(, {2 } {1 }).
781

fiDelgrande, Jin, & Pelletier

(b) 2 2 2 :
case handled base case, 1 , 1 , 2 , 2 atoms.
(c) 2 (2 2 ) 2 (2 2 ):
handled 2 2 2 2 respectively.
3. 1 2 :
proof base case, , atoms.
4. 1 (2 ) ( ) 1 (2 ) ( ):
handled respectively.
Since covers cases, result follows induction.



Proof 2.
proof follows straightforwardly observations arbitrary , ,
have:
UL(, {} ) = UL(, {} )
UL(, {( )} ) = UL(, { } )
UL(, {( )} ) = UL(, { } )
induction argument establishes value UL doesnt change conversion
elements second argument negation normal form.

Proof 3.
result follows preceding, plus fact arbitrary ,
that: UL(, { ( )} ) = UL(, {( ) ( )} ); is, UL invariant
distribution conjunction disjunction.

Proof 4.
Proof induction maximum depth formula .
maximum depth 0, members literals, result immediate
Denition 6. Otherwise induction hypothesis result holds
maximum depth formula n, step easily shown appeal truth
conditions classical propositional logic.

Proof 5.
Right-to-left: corollary Theorem 4.
Left-to-right:
arbitrary Theorem 3 UL(, ) = UL(, dnf ()).
Let dnf () = 1 n conjunction literals.
Via Denition 6 UL(, dnf ()) = UL(, {1 }) UL(, {n }).
, contains complementary pair literals UL(, {i }) = ; otherwise
UL(, {i }) 6= .
782

fiCompositional Belief Update

assume 6 , complementary literals, consequently UL(, {i }) 6= =
6 UL(, dnf ()) = UL(, ).
Thus 6 implies UL(, ) 6|= , shown.


Lemma 3 c .
Proof Lemma 3. result immediate.
Consequently assume satisable, let od( ). show
od( c ). Given Denition 7, since already od(), need
show UL(, {}).
assumption od(), whence (Theorem 3) od(dnf ()).
Since od(dnf ()) = od(dnf (1 )) od(dnf (n )) dnf () = 1 n
get od(dnf (i )) disjunct dnf ().
Since od(dnf (i )) follows denition UL = UL(, {i }); hence
od(dnf ()) UL(, {}), shown.

Proof 6. second part theorem follows immediately Denitions 7 8.
rst part: Since c (Lemma 3),
c ( ) ( c )
( ) (( c ) )
( ( c ))
( c )
last step applies part theorem, established above.



Proof 7.
proof Theorem 2 minor modications.



Proof 8.
proof analogous Theorem 3, omitted.



Proof Lemma 1.
proof straightforward, except setting induction bit ddly. induction based maximum depth formula . L, let depth() =
max depth(). stipulate precedes ordering induction
depth() < depth( ), depth() = depth( ) = n number formulas
depth n less .
BASE:
Let set literals.
W
EL(, ) = = UL(, { }).
783

fiDelgrande, Jin, & Pelletier



W

6 then:
EL(, ) = ( )
= ( )
= UL(, )
n ^
= UL ,

STEP:
Assume result holds rst n sets formulas ordering, let
formula maximum depth . Let = \ {}.
= ,
EL(, ) = EL(, { } )
= EL(, {} ) EL(, {} )
= EL(, {({} )}) EL(, {({} )})
= UL(, { ({} )}) UL(, { ({} )})
= UL(, { ( )}) UL(, { ( )})
= UL(, { ( ) ( )})
= UL(, { ( )})
= UL(, {( ( ))})
= UL(, {()}).
change EL UL justied induction hypothesis; otherwise
steps denition UL EL, simple manipulation.
= ,
EL(, ) = EL(, { } )
= EL(, {, } )
= UL(, { ({, } )})
= UL(, {()}).
Again, change EL UL justied induction hypothesis.
cases handled analogously; proofs omitted.
Hence result follows induction.



Proof 9.
od( c ) = od() { | UL(, {}), od()}
= od() { | EL(, {}), od()}
= od(


c )

rst last steps justied Denitions 8 10 respectively; middle
step follows Lemma 1.

784

fiCompositional Belief Update

Proof 10. follows immediately Denitions 11 12 od( ) od(
).
show converse, let w od( ) show w od( ).
Given Satoh revision operator satises irrelevance syntax (R4),
assume without loss generality dnf; i.e. = 1 n
conjunction literals.
assumption w od( ); hence w od() ww
min
(, ).
Since w od( ), w od() = od(1 n ). Thus
clause , , w od(i ). Assume without loss generality
subset-minimal among sets literals making disjuncts .
show w UL(w, {i }) shown w satises conditions
member od( ). show follows. Let Ui set literals .
l 6 Ui l w l w (since otherwise would contradict
ww min (, )).
follows (w Ui ) Ui = w . means w UL(w, Ui ), w
UL(w, {ui }) w UL(w, {}).
Hence w od( ), shown.

Proof 11.
U1: Theorem 4 , UL(, {}) |= every od(), whence od(c ) od()
c .
U3: assumption 6 , od() 6= . result follows immediately
Theorem 5 Denition 7.
U5: od( c ) od() = result follows vacuously.
Otherwise, let od( c ) od().
Since V
od( c ) then, Theorem 3, exists od() Lits
disjunct dnf () = ( ) .
V
od() set literals disjunct dnf ()
.
V
denition, clause dnf (). note = ( ( ))( )
Denition 7 od( c ( )).
U7:
od( c 1 ) od( c 2 )
od( c 1 ) od( c 2 )
od( c (1 2 ))
last step follows Theorem 3, using fact dnf () = dnf ()dnf ().
Hence ( c 1 ) ( c 2 ) implies c (1 2 ).
785

fiDelgrande, Jin, & Pelletier

U8:
od((1 2 ) c ) = { | UL(, {}), od(1 2 )}
= { | UL(, {}), od(1 ) od(2 )}
= { | UL(, {}), od(1 )}
{ UL(, {}), od(2 )}
= od(1 c ) od(2 c ).
od((1 2 )c ) = od(1 c )M od(2 c ) follows ((1 2 )c )
(1 c ) (2 c ).

Proof 12.
od( c (1 2 )) = { | UL(, {1 2 }), od()}
= { | UL(, {1 }), od()}
{ | UL(, {2 }), od()}
= od( c 1 ) od( c 2 )
= od(( c 1 ) ( c 2 ))

Proof 13.
need show 1 2 1 2 (1 c 1 ) W
(2 c 1 ).
12 Since (
Since W



assumption,



P
I(
)
=
P
I(
).
1
2
1W
2
1 c W P I(1 ))
W
(2 c W P I(1 )) (2 c P I(1 )) (2 c P I(2 )), (1 c P I(1 ))
pi
(2 c P I(2 )), whence ( pi

c 1 ) ( c 2 ).

Proof 14.
WV
od( ss
c (V ( odL()))). Using Denition 7,
c ) = od( W
right hand side equal { | UL(, { ( odL())}), od()}. Hence,
od() replaced set interpretations = {( ) |
odL()}. say, od() replaced set interpretations
dier language . denition

od( ss
c ).
Proof 15.
Let = {p1 , . . . , pn }.
= {p1 pn }
= [( {p1 pn }) c pn ] [( {p1 pn }) c pn ]

12. Equality isnt quite right here. Rather equality modulo associativity commutativity,
need result.

786

fiCompositional Belief Update

second step Denition 3 forget expressed terms update. Denition 3 successively reapplied eventually terminate disjunction 2n
disjuncts, disjunct sequence n updates literals . Moreover, every
maximum consistent set literals appears disjunct.
easy result, state without proof, disjoint sets literals 1 ,
2 , ( c (1 )) c (2 ) = ( c (1 2 )).
Hence get nally
^
=
c (() ( \ ))




= c

^



(p p).

p



Proof 16.
Theorem 15
= c

^

!

(a a).



= c .

Observation
1 get c = c dnf (). easy argument shows
WV
dnf () =
odL(), Denition 14 yields c dnf () = ss
c . Theorem 14
ss
ss
c = ss . Putting together get () = c .

Proof Lemma 2.
Equation (5) implies od(( P.) t) = od( W
t). According Theorem 14
ss V
Denition 14, od( ss t) = od( c ( ( odL(t)))) = od( c t).
Denition 7, follows od(( P.) t) = {w | w UL(w, t), w od()}.
Proof 17.
According Denition 7, od(c ) = {w | w UL(w, {}), w od()}. Theorem 3, {w | w UL(w, {}), w od()} = {w | w UL(w, {dnf ()}), w
od()}. Denition 6, follows od( c ) = {w | w UL(w, t),
dnf (), w od()}. According Lemma 2, thus od( c ) = od(U pdate(, )).
Proof 18.
size dnf () O(2|| ). Hence, size Eliminant(P, ) also O(2|| ). Similarly, size dnf () O(2|| ). Therefore, |U pdate(, )| = O(2|| 2|| ) = O(2||+|| ).

Proof 19.
proof inspired ideas (Cadoli, Donini, Liberatore, & Schaerf, 1995),
shown many revision operators cause exponential blowup. show
787

fiDelgrande, Jin, & Pelletier

exists polynomially space bounded algorithm compositional update, 3SAT
P/poly.13 proof consists two steps.
STEP 1:
integer n, rst construct belief base n formula n , whose sizes
polynomial wrt. n. Let X = {x1 , , xn } = {y1 , , yn } two disjoint set
atoms let C set new atoms 3-literal clause X, i.e., C = {ci |
3-literal clause X}. obtain n n follows:
n = V
{i ci | 3-literal clause X}
n = ni=1 (x1 yi )

easy see |n | O(n3 ) |n | O(n).
show 3CNF size n, exists interpretation (on
atoms X C) |= n c n satisable. assume, without loss
generality, atom() X; otherwise, always substitute atoms respectively
elements X obtain new sentence X satisable X satisable.
w obtained follows:
w = {ci C | clause } {ci C | clause } X
show satisable |= n c n .
Assume satisable. Let model . construct another interpretation
= UL(, {ci C | clause }). easy see |= n
= UL( , {xi , yi | 1 n}). follows |= n c n .
Assume |= n c n . exists interpretation |= n
= UL(, {xi , yi | 1 n}). claim |= . Assume 6|= .
exists 3-literal clause 6|= . = UL(, {xi , yi |
1 n}) ci , follows ci . implies 6|= ci ,
contradicts |= n . Thus, indeed satisable.
STEP 2:
Suppose U pdate polynomial space bounded algorithm compositional update.
3SAT solved advice taking TM follows: Given arbitrary 3CNF
size n, machine rst loads advice string U pdate(n , n ) computes (in polynomial
time) ; veries |= U pdate(n , n ). Since |n | O(n3 ), |n | O(n),
|U pdate(n , n )| p(|n | + |n |), verication polynomial time. Since
U pdate(n , n ) n c n , |= n c n |= U pdate(n , n ). Therefore,
satisable |= U pdate(n , n ). shows 3SAT P/poly. 3SAT
NP-complete, NP P/poly.

Proof 20.
13. 3-literal clause clause consists precisely 3 literals 3CNF conjunction 3-literal clauses.
3SAT satisfiability problem 3CNFs, shown NP-complete.

788

fiCompositional Belief Update

proof exactly Theorem 19, update formula n used
consistent conjunction literals.

Proof 21. Since , dnf, || = |dnf ()| || = dnf (). Thus |Eliminant(P, )| =
O||. Therefore |U pdate(, )| = O(|| ||).
case || < k, |U pdate(, )| = O(|| k) = O().


References
Alchourron, C., Gardenfors, P., & Makinson, D. (1985). logic theory change:
Partial meet functions contraction revision. Journal Symbolic Logic, 50 (2),
510530.
Boole, G. (1854). Investigation Laws Thought. Walton, London. (Reprinted
Dover Books, New York, 1954).
Borgida, A. (1985). Language features exible handling exceptions information
systems. ACM Transactions Database Systems, 10.
Brewka, G., & Herzberg, J. (1993). things worlds: formalizing actions
plans. J. Logic Computation, 3, 517532.
Bright, M., Hurson, A., & Pakzad, S. (1992). taxonomy current issues multidatabase systems. Computer, 25, 5059.
Brown, F. (1990). Boolean Reasoning. Kluwer Academic Publishers.
Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1995). size revised
knowledge base. PODS 95: Proceedings fourteenth ACM SIGACT-SIGMODSIGART symposium Principles database systems, pp. 151162, New York, NY,
USA. ACM Press.
Dalal, M. (1988). Investigations theory knowledge base revision.. Proceedings
AAAI National Conference Artificial Intelligence, pp. 449479, St. Paul,
Minnesota.
Delgrande, J., Nayak, A., & Pagnucco, M. (2005). Gricean belief change. Studia Logica, 79,
97113.
Delgrande, J., & Schaub, T. (2003). consistency-based approach belief change. Artificial Intelligence, 151 (1-2), 141.
Delgrande, J., Pelletier, F., & Suderman, M. (2007). Compositional belief update.
Proceedings FLAIRS-20, pp. 6873, Key West.
Doherty, P., Lukaszewicz, W., & Madalinska-Bugaj, E. (1998). PMA relativizing
change action update. Cohn, A. G., Schubert, L., & Shapiro, S. C. (Eds.),
Proceedings International Conference Principles Knowledge Representation Reasoning, pp. 258269. Morgan Kaufmann, San Francisco, California.
Everaere, P., Konieczny, S., & Marquis, P. (2005). Quota Gmin merging operators.
Proceedings International Joint Conference Artificial Intelligence, pp.
424429, Edinburgh.
789

fiDelgrande, Jin, & Pelletier

Everaere, P., Konieczny, S., & Marquis, P. (2007). strategy-proofness landscape
merging. Journal Artificial Intelligence Research, 28, 49105.
Forbus, K. (1989). Introducing actions qualitative simulation. Proceedings
International Joint Conference Artificial Intelligence, pp. 12731278.
Fuhrmann, A. (1991). Theory contraction base contraction. Journal Philosophical Logic, 20, 175203.
Gardenfors, P. (1988). Knowledge Flux: Modelling Dynamics Epistemic States.
MIT Press, Cambridge, MA.
Hansson, S. O., & Rott, H. (1998). Beyond recovery. Erkenntnis, 49, 387392.
Herzig, A., & Ri, O. (1999). Propositional belief update minimal change. Artificial
Intelligence, 115 (1), 107138.
Johnson, D. (1990). catalog complexity classes. van Leeuwen, J. (Ed.), Handbook
Theoretical Computer Science: Volume A: Algorithms Complexity, pp. 67161.
Elsevier, Amsterdam.
Karp, R. M., & Lipton, R. J. (1980). connections non-uniform uniform
complexity classes. Proc. 12th ACM sym. Theory Computing (STOC80), pp. 302309.
Katsuno, H., & Mendelzon, A. (1992). dierence updating knowledge
base revising it. Gardenfors, P. (Ed.), Belief Revision, pp. 183203. Cambridge
University Press.
Konieczny, S., & Pino Perez, R. (2002). Merging information constraints: logical
framework. Journal Logic Computation, 12 (5), 773808.
Konieczny, S. (2000). dierence merging knowledge bases combining
them. Cohn, A. G., Giunchiglia, F., & Selman, B. (Eds.), KR2000: Principles
Knowledge Representation Reasoning, pp. 135144, San Francisco. Morgan
Kaufmann.
Konieczny, S., & Pino Perez, R. (1998). logic merging. Cohn, A. G., Schubert, L., & Shapiro, S. C. (Eds.), KR98: Principles Knowledge Representation
Reasoning, pp. 488498. Morgan Kaufmann, San Francisco, California.
Lang, J. (2006). time, revision, update. Dix, J., & Hunter, A. (Eds.),
Proceedings Eleventh International Workshop Non-Monotonic Reasoning
(NMR 2006).
Lang, J., Liberatore, P., & Marquis, P. (2003). Propositional independence : Formulavariable independence forgetting. Journal Artificial Intelligence Research, 18,
391443.
Liberatore, P., & Schaerf, M. (1998). Arbitration (or merge knowledge bases). IEEE
Transactions Knowledge Data Engineering, 10 (1), 7690.
Lin, F., & Reiter, R. (1994). Forget it!. AAAI Fall Symposium Relevance, New
Orleans.
790

fiCompositional Belief Update

Lin, F. (2001). strongest neccessary weakest sucient conditions. Artificial Intelligence, 128 (1-2), 143159.
Lin, J., & Mendelzon, A. O. (1998). Merging databases constraints. International
Journal Cooperative Information Systems, 7 (1), 5576.
Meyer, T. (2000). Merging epistemic states. Pacific Rim International Conference
Artificial Intelligence, pp. 286296.
Meyer, T. (2001). semantics combination operations. Journal Applied NonClassical Logics, 11 (1-2), 5984.
Nayak, A., Chen, Y., & Lin, F. (2006). Forgetting knowledge update. Sattar,
A., & Kang, B. (Eds.), Proceedings Nineteenth Australian Joint Conference
Artificial Intelligence (AI-06), Vol. 4304 Lecture Notes Artificial Intelligence, pp.
131140. Springer Verlag.
Parikh, R. (1999). Beliefs, belief revision, splitting languages. Moss, L., Ginzburg, J.,
& de Rijke, M. (Eds.), Logic, Language Computation, Vol 2, pp. 266278. CSLI
Publications.
Peppas, P., Chopra, S., & Foo, N. (2004). Distance semantics relevance-sensitive belief
revision. KR2004: Principles Knowledge Representation Reasoning, San
Francisco. Morgan Kaufmann.
Revesz, P. (1993). semantics theory change: Arbitration old new
information. Beeri, C. (Ed.), Proceedings Twelth ACM Symposium Principles Database Systems, pp. 7182, Washington D.C.
Rott, H., & Pagnucco, M. (1999). Severe withdrawal (and recovery). Journal Philosophical
Logic, 28 (5), 501547.
Satoh, K. (1988). Nonmonotonic reasoning minimal belief revision. Proceedings
International Conference Fifth Generation Computer Systems, pp. 455462,
Tokyo.
Subrahmanian, S. (1994). Amalgamating knowledge bases. ACM Transactions Database
Systems, 19, 291331.
Tennant, N. (1997). bad contractions or: room recovery. Journal
Applied Non-Classical Logic, 7, 241266.
Weber, A. (1986). Updating propositional formulas. Proc. First Conference Expert
Database Systems, pp. 487500.
Williams, M.-A. (1996). Towards practical approach belief revision: Reason-based
change. Aiello, L., Doyle, J., & Shapiro, S. (Eds.), Proceedings Fifth International Conference Principles Knowledge Representation Reasoning,
pp. 412421, Cambridge, MA.
Winslett, M. (1988). Reasoning action using possible models approach. Proceedings AAAI National Conference Artificial Intelligence, pp. 8993, St. Paul,
Minnesota.
Winslett, M. (1990). Updating Logical Databases. Cambridge University Press, Cambridge.

791

fiJournal Artificial Intelligence Research 32 (2008) 631-662

Submitted 11/07; published 06/08

General Theory Additive State Space Abstractions
Fan Yang
Joseph Culberson
Robert Holte

fyang@cs.ualberta.ca
joe@cs.ualberta.ca
holte@cs.ualberta.ca

Computing Science Department, University Alberta
Edmonton, Alberta T6G 2E8 Canada

Uzi Zahavi

zahaviu@cs.biu.ac.il

Computer Science Department, Bar-Ilan University
Ramat-Gan, Israel 92500

Ariel Felner

felner@bgu.ac.il

Information Systems Engineering Department,
Deutsche Telekom Labs,
Ben-Gurion University.
Beer-Sheva, Israel 85104

Abstract
Informally, set abstractions state space additive distance
two states always greater equal sum corresponding distances abstract spaces. first known additive abstractions, called disjoint pattern
databases, experimentally demonstrated produce state art performance
certain state spaces. However, previous applications restricted state spaces
special properties, precludes disjoint pattern databases defined several commonly used testbeds, Rubiks Cube, TopSpin Pancake puzzle.
paper give general definition additive abstractions applied
state space prove heuristics based additive abstractions consistent well
admissible. use new definition create additive abstractions testbeds
show experimentally well chosen additive abstractions reduce search time
substantially (18,4)-TopSpin puzzle three orders magnitude state
art methods 17-Pancake puzzle. also derive way testing heuristic
value returned additive abstractions provably low show use
test reduce search time 15-puzzle TopSpin roughly factor two.

1. Introduction
purest form, single-agent heuristic search concerned problem finding
least-cost path two states (start goal) state space given heuristic function
h(t, g) estimates cost reach goal state g state t. Standard algorithms
single-agent heuristic search IDA (Korf, 1985) guaranteed find optimal
paths h(t, g) admissible, i.e. never overestimates actual cost goal state
t, efficiency heavily influenced accuracy h(t, g). Considerable research
therefore investigated methods defining accurate, admissible heuristics.
common method defining admissible heuristics, led major advances
combinatorial problems (Culberson & Schaeffer, 1998; Hernadvolgyi, 2003; Korf, 1997;
Korf & Taylor, 1996) planning (Edelkamp, 2001), abstract original state
c
2008
AI Access Foundation. rights reserved.

fiYang, Culberson, Holte, Zahavi & Felner

space create new, smaller state space key property path p~
original space corresponding abstract path whose cost exceed cost
p~. Given abstraction, h(t, g) defined cost least-cost abstract path
abstract state corresponding abstract state corresponding g.
best heuristic functions defined abstraction typically based several abstractions,
equal either maximum, sum, costs returned abstractions
(Korf & Felner, 2002; Felner, Korf, & Hanan, 2004; Holte, Felner, Newton, Meshulam, &
Furcy, 2006).
sum costs returned set abstractions always admissible.
is, set abstractions said additive. main contribution paper
identify general conditions abstractions additive. new conditions subsume
previous notions additive special cases. greater generality allows additive
abstractions defined state spaces additive abstractions according
previous definitions, Rubiks Cube, TopSpin, Pancake puzzle, related realworld problems genome rearrangement problem described Erdem Tillier
(2005). definitions fully formal, enabling rigorous proofs admissibility
consistency heuristics defined abstractions. Heuristic h(t, g) consistent
states t, g u, h(t, g) cost(t, u) + h(u, g), cost(t, u) cost least-cost
path u.
usefulness general definitions demonstrated experimentally defining
additive abstractions substantially reduce CPU time needed solve TopSpin
Pancake puzzle. example, use additive abstractions allows 17-Pancake
puzzle solved three orders magnitude faster previous state-of-the-art methods.
Additional experiments show additive abstractions always best abstraction method. main reason solution cost calculated individual
additive abstraction sometimes low. extreme case, actually arises
practice, problems abstract solutions cost 0. final contribution
paper introduce technique sometimes able identify sum
costs additive abstractions provably small (infeasible).
remainder paper organized follows. informal introduction abstraction given Section 2. Section 3 presents formal general definitions abstractions
extend general additive abstractions. provide lemmas proving admissibility
consistency standard additive heuristics based abstractions.
section also discusses relation previous definitions. Section 4 describes successful applications additive abstractions TopSpin Pancake puzzle. Section 5 discusses
negative results. Section 6 introduces infeasibility presents experimental results
showing effectiveness sliding tile puzzle TopSpin. Conclusions presented
Section 7.

2. Heuristics Defined Abstraction
illustrate idea abstraction used define heuristics, consider
well-known 8-puzzle (the 3 3 sliding tile puzzle). puzzle 9 locations
form 3 3 grid 8 tiles, numbered 18, 9th location empty (or
blank). tile adjacent empty location moved empty location;
632

fiA General Theory Additive State Space Abstractions

every move cost 1. common way abstracting state space treat
several tiles indistinguishable instead distinct (Culberson &
Schaeffer, 1996). extreme version type abstraction shown Figure 1.
tiles indistinguishable other, abstract state entirely defined
position blank. therefore 9 abstract states, connected shown
Figure 1. goal state original puzzle blank upper left corner,
abstract goal state shown top figure. number beside
abstract state distance abstract state abstract goal. example,
Figure 1, abstract state e 2 moves abstract goal. heuristic function h(t, g)
distance state g original space computed two steps: (1) compute
abstract state corresponding (in example, done determining
location blank state t); (2) determine distance abstract
state abstract goal. calculation abstract distance either done
preprocessing step create heuristic lookup table called pattern database (Culberson
& Schaeffer, 1994, 1996) time needed (Holte, Perez, Zimmer, & MacDonald,
1996; Holte, Grajkowski, & Tanner, 2005; Felner & Adler, 2005).
abstract goal

id: distance goal

a:0

b: 1

c: 1
e: 2

f: 2

d: 2
h: 4
g: 3

i: 3

Figure 1: abstraction 8-puzzle. white square state blank
non-white squares tiles, indistinguishable
abstraction.

Given several abstractions state space, heuristic hmax (t, g) defined
maximum abstract distances given abstractions individually.
standard method defining heuristic function given multiple abstractions (Holte et al.,
2006). example, consider state 3 3 sliding tile puzzle shown top left
Figure 2 goal state shown it. middle column shows abstraction
two states (A1 g1 ) tiles 1, 3, 5, 7, blank, distinct
tiles indistinguishable other. refer distinct tiles
distinguished tiles indistinguishable tiles dont care tiles. right column
633

fiYang, Culberson, Holte, Zahavi & Felner

shows complementary abstraction, tiles 1, 3, 5, 7 dont cares
tiles 2, 4, 6, 8 distinguished. arrows figure trace least-cost
path reach abstract goal gi state Ai abstraction. cost solving A1
16 cost solving A2 12. Therefore, hmax (A, g) 16, maximum
two abstract distances.
abstract state A2

abstract state A1

state

1

2

4

8

5

6

7

3

7

h=max(16,12)

1

2

3

4

5

6

7

8

goal state g

2

1
5

4

3

6

8

16 moves

12 moves

2

1
3

4

5
7
abstract goal g1

6

8
abstract goal g 2

Figure 2: Computation hmax (A, g), standard, maximum-based heuristic value
state (top left) using two abstractions shown middle right
columns. Solid arrows denote distinguished moves, dashed arrows denote dont
care moves.

2.1 Additive Abstractions
Figure 3 illustrates additive abstractions defined sliding tile puzzle (Korf
& Felner, 2002; Felner et al., 2004; Korf & Taylor, 1996). State abstractions
Figure 2, costs operators abstract spaces defined
differently. Instead abstract operators cost 1, case previously,
operator cost 1 moves distinguished tile; moves called
distinguished moves shown solid arrows Figures 2 3. operator
moves dont care tile (a dont care move) cost 0 shown dashed
arrow figures. Least-cost paths abstract spaces defined way therefore minimize
number distinguished moves without considering many dont care moves
made. example, least-cost path A1 Figure 3 contains fewer distinguished
moves (9 compared 10) least-cost path A1 Figure 2and therefore
lower cost according cost function describedbut contains moves total
(18 compared 16) dont care moves (9 compared 6). Figure
3 shows, 9 distinguished moves needed solve A1 5 distinguished moves needed
solve A2 . tile distinguished abstractions, move cost
1 one space cost 0 space, therefore admissible add
634

fiA General Theory Additive State Space Abstractions

two distances. heuristic calculated using additive abstractions referred hadd ;
example, hadd (A, g) = 9 + 5 = 14. Note hadd (A, g) less hmax (A, g)
example, showing heuristics based additive abstractions always superior
standard, maximum-based method combining multiple abstractions even though
general proven effective sliding tile puzzles (Korf & Felner, 2002;
Felner et al., 2004; Korf & Taylor, 1996).
abstract state A2

abstract state A1

state

1

2

4

8

5

6

7

3

7

h=9+5=14

1

2

3

4

5

6

7

8

goal state g

2

1
5

4

3

6

8

5 distinguished moves

9 distinguished moves

2

1
3

4

5
7
abstract goal g1

6

8
abstract goal g 2

Figure 3: Computation hadd (A, g), additive heuristic value state A. Solid arrows
denote distinguished moves, dashed arrows denote dont care moves.

general method defined Korf, Felner, colleagues (Korf & Felner, 2002; Felner
et al., 2004; Korf & Taylor, 1996) creates set k additive abstractions partitioning
tiles k disjoint groups defining one abstraction group making
tiles group distinguished abstraction. important limitation
existing methods defining additive abstractions apply
spaces operator move one tile time, unless way
guarantee tiles moved operator group.
example state space additive abstractions according previous
definitions Pancake puzzle. N -Pancake puzzle, state permutation N
tiles (0, 1, ..., N 1) N 1 successors, lth successor formed reversing
order first l + 1 positions permutation (1 l N 1). example,
4-Pancake puzzle shown Figure 4, state top figure three successors,
formed reversing order first two tiles, first three tiles, four
tiles, respectively. operators move one tile tile appear
location non-trivial way partition tiles tiles moved
operator distinguished one abstraction. common state spaces
additive abstractions according previous definitionsfor similar reasonsare
Rubiks Cube TopSpin.
635

fiYang, Culberson, Holte, Zahavi & Felner

Figure 4: 4-Pancake puzzle state three successors.

general definition additive abstractions presented next section overcomes
limitations previous definitions. Intuitively, abstractions additive provided
cost operator divided among abstract spaces. definition provides
formal basis intuition. numerous ways even operators
move many tiles (or, words, make changes many state variables). example,
operator cost might divided proportionally across abstractions based
percentage tiles moved operator distinguished abstraction.
call method defining abstract costs cost-splitting. example, consider two
abstractions 4-Pancake puzzle, one tiles 0 1 distinguished,
tiles 2 3 distinguished. middle operator Figure 4 would
cost 23 first abstract space 13 second abstract space,
three tiles operator moves, two distinguished first abstraction one
distinguished second abstraction.
different method dividing operator costs among abstractions focuses specific location (or locations) puzzle assigns full cost operator
abstraction tile moves location distinguished. call
location-based cost definition. Pancake puzzle natural use leftmost
location special location since every operator changes tile location.
middle operator Figure 4 would cost 0 abstract space tiles 0
1 distinguished cost 1 abstract space tiles 2 3
distinguished operator moves tile 2 leftmost location.
methods apply Rubiks Cube TopSpin, many state spaces
addition Pancake puzzle, hadd heuristics produce always superior
hmax heuristics based tile partitions. theory experiments
remainder paper shed light general question hadd preferable
hmax .

3. Formal Theory Additive Abstractions
section, give formal definitions lemmas related state spaces, abstractions,
heuristics defined them, discuss meanings relation previous
work. definitions state space etc. Section 3.1 standard, definition
state space abstraction Section 3.2 differs previous definitions one important
detail: state transition abstract space two costs associated instead
one. main new contribution definition additive abstractions Section
3.3.
636

fiA General Theory Additive State Space Abstractions

underlying structure abstraction definition directed graph (digraph)
homomorphism. easy reference, quote standard definitions digraph
digraph homomorphism (Hell & Nesetril, 2004).
Definition 3.1 digraph G finite set V = V (G) vertices, together binary
relation E = E(G) V. elements (u, v) E called arcs G.
Definition 3.2 Let G H digraphs. homomorphism G H, written
f : G H mapping f : V (G) V (H) (f (u), f (v)) E(H) whenever
(u, v) E(G).
Note digraphs G H may self-loops, (u, u), homomorphism
required surjective either vertices arcs. typically refer arcs edges,
kept mind that, general, directed edges, ordered pairs.
3.1 State Space
Definition 3.3 state space weighted directed graph = hT, , Ci finite
set states, set directed edges (ordered pairs states) representing state
transitions, C : N = {0, 1, 2, 3, . . . } edge cost function.
typical practice, defined implicitly. Usually distinct state corresponds
assignment values set state variables. C derive successor function,
set planing operators. cases, restricted set states reachable
given state. example, 8-puzzle, set edges defined rule
tile adjacent empty location moved empty location,
set states defined one two ways: either set states reachable
goal state, set permutations tiles blank, case consists
two components connected one another. standard cost function C
8-puzzle assigns cost 1 edges, easy imagine cost functions
8-puzzle depend tile moved locations involved move.
path state state g sequence edges beginning ending
g. Formally, p~ path state state g p~ = h 1 , . . . , n i, j j =
(tj1 , tj ), j {1, . . . , n} t0 = t, tn = g. Note use superscripts rather
subscripts distinguish states edges
P within state space. length p~
number edges n cost C(~
p) = nj=1 C( j ). use P aths(S, t, g) denote
set paths g S.
Definition 3.4 optimal (minimum) cost path state state g defined

OPT(t, g) =

min

p
~P aths(S,t,g)

C(~
p)

pathfinding problem triple hS, t, gi, state space t, g ,
objective finding minimum cost path g, cases finding
minimum cost path p~ P aths(S, t, g) C(~
p) = OPT(t, g). one goal
state may seem restrictive, problems set goal states accommodated
definition adding virtual goal state state space zero-cost edges
actual goal states virtual goal state.
637

fiYang, Culberson, Holte, Zahavi & Felner

3.2 State Space Abstraction
Definition 3.5 Abstraction System pair hS, = hT, , Ci state space
= {hAi , | : Ai , 1 k} set abstractions, abstraction
pair consisting abstract state space abstraction mapping, abstract
state space abstraction mapping defined below.
Note abstractions intended form hierarchy considered set independent abstractions.
Definition 3.6 abstract state space directed graph two weights per edge,
defined four-tuple Ai = hTi , , Ci , Ri i.
Ti set abstract states set abstract edges, definition
state space. abstract space two costs associated ,
primary cost Ci : N residual cost Ri : N . idea
two costs per abstract edge, instead one, inspired practice, illustrated
Figure 3, two types edges abstract space counting distinguished
moves differently dont care moves. example, primary cost cost
associated distinguished moves, residual cost cost associated
dont care moves. usefulness considering cost dont care moves arises
abstraction system additive, suggested Lemmas 3.6 3.10 below.
indicate additive heuristic infeasible improved, effectiveness
become apparent experiments reported Section 6.
Like edges, abstract path p~i = hi1 , . . . , Ai primary residual cost:
P
P
pi ) = nj=1 Ri (ij ).
Ci (~
pi ) = nj=1 Ci (ij ), Ri (~
Definition 3.7 abstraction mapping : Ai state space abstract
state space Ai defined mapping states states Ai , :
Ti , satisfies two following conditions.
first condition mapping homomorphism thus connectivity
original space preserved, i.e.,

(1)

(u, v) , (i (u), (v))

words, edge original space corresponding edge
abstract space Ai . Note u 6= v (u) = (v) non-identity edge gets
mapped identity edge (self-loop) Ai . use shorthand notation tji = (tj )
abstract state Ti corresponding tj , ij = ( j ) = (i (uj ), (v j ))
abstract edge corresponding j = (uj , v j ) .
second condition state mapping must satisfy abstract edges must
cost edges correspond original state space, i.e.,

(2)

, Ci (i ) + Ri (i ) C()
638

fiA General Theory Additive State Space Abstractions

consequence, multiple edges original space map abstract edge
, usually case, Ci () + Ri () must less equal them, i.e.,
, Ci () + Ri ()

min

C()

,i ()=

Note edge maps edge abstract space, bound cost
edge imposed.
example, state mapping used define abstraction middle column
Figure 3 maps 8-puzzle state abstract state renaming tiles 2, 4, 6, 8
dont care. mapping satisfies condition (1) dont care tiles
exchanged blank whenever regular tiles can. satisfies condition (2)
move either distinguished move (Ci (i ) = 1 Ri (i ) = 0) dont care move
(Ci (i ) = 0 Ri (i ) = 1) cases Ci (i ) + Ri (i ) = 1, cost edge
original space.
set abstract states Ti usually equal (T ) = {i (t) | },
superset, case abstraction said non-surjective (Hernadvolgyi & Holte,
2000). Likewise, set abstract edges usually equal () = {i () | }
superset even Ti = (T ). cases, one deliberately chooses
abstract space states edges counterpart original space.
example, methods define abstractions dropping operator preconditions must,
design, create abstract spaces edges correspond edge
original space (e.g. Pearl, 1984). cases, non-surjectivity inadvertent
consequence abstract space defined implicitly set states reachable
abstract goal state applying operator inverses. example, tile
2 2 sliding tile puzzle mapped blank abstract space, puzzle
two blanks states reachable abstract space counterpart
original space (Hernadvolgyi & Holte, 2000). additional examples extensive
discussion non-surjectivity see previous paper Holte Hernadvolgyi (2004).
lemmas definitions follow assume abstraction system hS, containing k abstractions given. Conditions (1) (2) guarantee following.
Lemma 3.1 path p~ P aths(S, u1 , u2 ) S, corresponding abstract path
(~
p) u1i u2i Ai Ci (i (~
p)) + Ri (i (~
p)) C(~
p).
Proof: definition, p~ P aths(S, u1 , u2 ) sequence edges h 1 , . . . , n i, j
j = (tj1 , tj ), j {1, . . . , n} t0 = u1 , tn = u2 . (),
corresponding abstract edges exists (ij ). i1 = (u1i , t1i ) = (tin1 , u2i ),
sequence (~
p) = hi1 , . P
. . , path u1i u2i .
definition, C(~
p) = nj=1 C( j ). j , Condition (2) ensures C( j )
P
P
P
Ci (ij )+Ri (ij ), therefore C(~
p) nj=1 (Ci (ij )+Ri (ij )) = nj=1 Ci (ij )+ nj=1 Ri (ij ) =
Ci (i (~
p)) + Ri (i (~
p)).
example, consider state goal g Figure 3. condition (1), path
state g original space also path abstract state A1 abstract goal
state g1 abstract state A2 g2 abstract spaces. condition (2),
cost path original space greater equal sum primary
cost residual cost corresponding abstract path abstract space.
639

fiYang, Culberson, Holte, Zahavi & Felner

use P aths(Ai , u, v) mean set paths u v space Ai .
Definition 3.8 optimal abstract cost abstract state u abstract state v Ai
defined
OPTi (u, v) =
min
Ci (~q) + Ri (~q)
q~P aths(Ai ,u,v)

Definition 3.9 define heuristic obtained abstract space Ai cost
state g
hi (t, g) = OPTi (ti , gi ).
Note definitions, path minimizing cost required image,
(~
p), path p~ S.
following prove heuristic generated individual abstraction admissible (Lemma 3.2) consistent (Lemma 3.3).
Lemma 3.2 hi (t, g) OPT(t, g) t, g {1, . . . , k}.
Proof: Lemma 3.1, C(~
p) Ci (i (~
p)) + Ri (i (~
p)), therefore
min

p
~P aths(S,t,g)

C(~
p)

min

p
~P aths(S,t,g)

Ci (i (~
p)) + Ri (i (~
p)).

left hand side inequality OPT(t, g) definition, right hand side
proved following Claim 3.2.1 greater equal hi (t, g). Therefore,
OPT(t, g) hi (t, g).
Claim 3.2.1 minp~P aths(S,t,g) Ci (i (~
p)) + Ri (i (~
p)) hi (t, g) t, g .
Proof Claim 3.2.1:
Lemma 3.1 every path p~ corresponding abstract path. may also additional paths abstract space, is,
{i (~
p) | p~ P aths(S, t, g)} P aths(Ai , ti , gi ). follows {Ci (i (~
p)) + Ri (i (~
p)) | p~
P aths(S, t, g)} {Ci (~q) + Ri (~q) | ~q P aths(Ai , ti , gi )}. Therefore,
min

p
~P aths(S,t,g)

Ci (i (~
p)) + Ri (i (~
p))

min

q~P aths(Ai ,ti ,gi )

Ci (~q) + Ri (~q) = OPTi (ti , gi ) = hi (t, g)

Lemma 3.3 hi (t1 , g) OPT(t1 , t2 ) + hi (t2 , g) t1 , t2 , g {1, . . . , k}.
Proof: definition OPTi minimization definition hi (t, g), follows
hi (t1 , g) = OPTi (t1i , gi ) OPTi (t1i , t2i ) + OPTi (t2i , gi ) = OPTi (t1i , t2i ) + hi (t2 , g).
complete proof, observe Lemma 3.2, OPT(t1 , t2 ) hi (t1 , t2 ) =
OPTi (t1i , t2i ).
Definition 3.10 hmax heuristic state state g defined abstraction system
hS,
k

hmax (t, g) = max hi (t, g)
i=1

Lemmas 3.2 3.3 immediately follows hmax admissible consistent.
640

fiA General Theory Additive State Space Abstractions

3.3 Additive Abstractions
section, formalize notion additive abstraction introduced intuitively Section 2.1. example showed hadd (t, g), sum heuristics
state defined multiple abstractions, admissible provided cost functions
abstract spaces counted distinguished moves. formal framework,
cost distinguished moves captured notion primary cost.
Definition 3.11 pair states t, g additive heuristic given abstraction
system defined
hadd (t, g) =

k
X

Ci (ti , gi ).

i=1


Ci (ti , gi ) =

min

q~P aths(Ai ,ti ,gi )

Ci (~q)

minimum primary cost path abstract space ti gi .
Figure 3, example, C1 (A1 , g1 ) = 9 C2 (A2 , g2 ) = 5 minimum
number distinguished moves reach g1 A1 9 minimum number
distinguished moves reach g2 A2 5.
Intuitively, hadd admissible cost edge original space divided
among abstract edges correspond , done cost-splitting
location-based methods defining abstract costs introduced end
Section 2.1. leads following formal definition.
P
Definition 3.12 abstraction system hS, additive , ki=1 Ci (i ) C().
following prove hadd admissible (Lemma 3.4) consistent (Lemma 3.5)
abstraction system hS, additive.
Lemma 3.4 hS, additive hadd (t, g) OPT(t, g) t, g .
Proof: Assume
g) = C(~
p), p~ = h 1 , . . . , n P aths(S, t, g). Therefore,
Pnthat OPT(t,
j
OPT(t, g) = j=1 C( ). Since hS, additive, follows definition
n
X

C( j )

j=1

n X
k
X

Ci (ij ) =

j=1 i=1



k
X

k X
n
X

Ci (ij )

i=1 j=1

Ci (ti , gi ) = hadd (t, g)

i=1

last line follows definitions Ci hadd .
Lemma 3.5 hS, additive hadd (t1 , g) OPT(t1 , t2 )+hadd (t2 , g) t1 , t2 , g
T.
641

fiYang, Culberson, Holte, Zahavi & Felner

Proof: Ci (t1i , gi ) obeys triangle inequality: Ci (t1i , gi ) Ci (t1i , t2i ) + Ci (t2i , gi )
P
P
P
t1 , t2 , g . follows ki=1 Ci (t1i , gi ) ki=1 Ci (t1i , t2i ) + ki=1 Ci (t2i , gi ).
Pk
P
i=1 Ci (t1i , gi ) = hadd (t1 , g) ki=1 Ci (t2i , gi ) = hadd (t2 , g), follows
P
hadd (t1 , g) ki=1 Ci (t1i , t2i ) + hadd (t2 , g).
P
Since hS, additive, Lemma 3.4, OPT(t1 , t2 ) ki=1 Ci (t1i , t2i ).
Hence hadd (t1 , g) OPT(t1 , t2 ) + hadd (t2 , g) t1 , t2 , g .
develop simple test important consequences additive heuristics.
Define P~i (ti , gi ) = {~q | ~q P aths(Ai , ti , gi ) Ci (~q) = Ci (ti , gi )}, set abstract
paths ti gi whose primary cost minimal.
Definition 3.13 conditional optimal residual cost minimum residual cost among
paths P~i (ti , gi ):
Ri (ti , gi ) = min Ri (~q)
~i (ti ,gi )
q~P

Note value (Ci (ti , gi ) + Ri (ti , gi )) sometimes, always, equal
optimal abstract cost OPTi (ti , gi ). Figure 3, example, OPT1 (A1 , g1 ) = 16 (a path
cost shown Figure 2) C1 (A1 , g1 ) + R1 (A1 , g1 ) = 18, C2 (A2 , g2 ) +
R2 (A2 , g2 ) = OPT2 (A2 , g2 ) = 12. following lemmas show, possible draw
important conclusions hadd comparing value (Ci (ti , gi ) + Ri (ti , gi )).
Lemma 3.6 Let hS, additive abstraction system let t, g states.
hadd (t, g) Cj (tj , gj ) + Rj (tj , gj ) j {1, . . . , k}, hadd (t, g) hmax (t, g).
Proof: definition OPTi (ti , gi ), j {1, . . . , k}, Cj (tj , gj )+Rj (tj , gj ) OPTj (tj , gj ).
Therefore, j {1, . . . , k}, hadd (t, g) Cj (tj , gj ) + Rj (tj , gj ) OPTj (tj , gj ) hadd (t, g)
max1ik OPTi (ti , gi ) = hmax (t, g).
Lemma 3.7 additive hS, path p~ P aths(S, t, g) C(~
p) =
Cj (j (~
p)) = Cj (tj , gj ) j {1, . . . , k}.

Pk


i=1 Ci (ti , gi ),

Proof: Suppose contradictionP
exists i1 , Ci1 (i1 (~
p)) >
k


Ci1 (ti1 , gi1 ). C(~
p) =
i=1 Ci (ti , gi ), must exist i2 ,
Ci2 (i2 (~
p)) < Ci2 (ti2 , gi2 ), contradicts definition Ci . Therefore, i1
exist Cj (j (~
p)) = Cj (tj , gj ) j {1, . . . , k}.
Lemma 3.8 additive hS, path p~ P aths(S, t, g) C(~
p) =
Ri (i (~
p)) Ri (ti , gi ) {1, . . . , k}.

Pk


i=1 Ci (ti , gi ),

Proof: Following Lemma 3.7 definition P~i (ti , gi ), (~
p) P~i (ti , gi )

{1, . . . , k}. Ri (ti , gi ) smallest residual cost paths P~i (ti , gi ), follows
Ri (i (~
p)) Ri (ti , gi ).
Lemma 3.9 additive hS, path p~ P aths(S, t, g) C(~
p) =
Pk
(t , g ) C (t , g ) + R (t , g ) j {1, . . . , k}.
C
j j j
j j j
i=1
642

Pk


i=1 Ci (ti , gi ),

fiA General Theory Additive State Space Abstractions

Proof: Lemma 3.1, C(~
p) Cj (j (~
p)) + Rj (j (~
p)) j {1, . . . , k}. Lemma 3.7
Cj (j (~
p)) = Cj (tj , gj ), Lemma 3.8 Rj (j (~
p)) Rj (tj , gj ). Therefore C(~
p)
Pk



Cj (tj , gj ) + Rj (tj , gj ), lemma follows premise C(~
p) = i=1 Ci (ti , gi ).

Lemma 3.10 Let hS, additive abstraction system let t, g states.
hadd (t, g) < Cj (tj , gj ) + Rj (tj , gj ) j {1, . . . , k}, hadd (t, g) 6= OP (t, g).
Proof: lemma follows directly contrapositive Lemma 3.9.
Lemma 3.6 gives condition hadd guaranteed least large
hmax specific states g. condition holds large fraction state
space , one would expect search using hadd least fast as, possibly
faster than, search using hmax . seen experiments reported Section 4.
opposite true general, i.e., failing condition imply hmax
result faster search hadd . However, Lemma 3.10 shows, interesting
consequence condition fails state t: know value returned hadd
true cost reach goal t. Detecting useful allows
heuristic value increased without risking becoming inadmissible. Section 6
explores detail.
3.4 Relation Previous Work
aim preceding formal definitions identify fundamental properties guarantee abstractions give rise admissible, consistent heuristics. shown
following two conditions guarantee heuristic defined abstraction
admissible consistent
(P 1)

(u, v) , (i (u), (v))

(P 2)

, C() Ci (i ) + Ri (i )

third condition
(P 3)

, C()

k
X

Ci (i )

i=1

guarantees hadd (t, g) admissible consistent.
Previous work focused defining abstraction additivity specific ways
representing states transition functions. important contributions
ultimately one needs computationally effective ways defining abstract state spaces,
abstraction mappings, cost functions theory takes given. importance
contribution make future proofs admissibility, consistency,
additivity easier, one need show particular method defining
abstractions satisfies three preceding conditions. generally simple conditions demonstrate, several methods defining abstractions
additivity currently exist literature.
643

fiYang, Culberson, Holte, Zahavi & Felner

3.4.1 Previous Definitions Abstraction
use abstraction create heuristics began late 1970s popularized
Pearls landmark book heuristics (Pearl, 1984). Two abstraction methods identified time: relaxing state space definition dropping operator preconditions
(Gaschnig, 1979; Guida & Somalvico, 1979; Pearl, 1984; Valtorta, 1984), homomorphic abstractions (Banerji, 1980; Kibler, 1982). early notions abstraction
unified extended Mostow Prieditis (1989) Prieditis (1993), producing
formal definition important respects except concept
residual cost introduced.1
Todays two commonly used abstraction methods among ones implemented
Prieditiss Absolver II system (Prieditis, 1993). first domain abstraction,
independently introduced seminal work pattern databases (Culberson &
Schaeffer, 1994, 1998) generalized (Hernadvolgyi & Holte, 2000). assumes
state represented set state variables, set possible values
called domain. abstraction states defined specifying mapping
original domains new, smaller domains. example, 8-puzzle state typically
represented 9 variables, one location puzzle, domain
9 elements, one tile one blank. domain abstraction
maps elements representing tiles new element (dont care)
blank different element would produce abstract space shown Figure 1.
reason particular example satisfies property (P1) explained Section 3.2. general,
domain abstraction satisfy property (P1) long conditions define
state transitions occur (e.g. operator preconditions) guaranteed satisfied
dont care symbol whenever satisfied one domain elements
map dont care. Property (P2) follows immediately fact state
transitions original abstract spaces primary cost 1.
major type abstraction used today, called drop Prieditis (1993),
independently introduced abstracting planning domains represented grounded (or
propositional) STRIPS operators (Edelkamp, 2001). STRIPS representation, state
represented set logical atoms true state, directed edges
states represented set operators, operator described
three sets atoms, P (a), A(a), D(a). P (a) lists preconditions: applied
state atoms P (a) true (i.e., P (a) t). A(a) D(a) specify
effects operator a, A(a) listing atoms become true applied (the
add list) D(a) listing atoms become false applied (the delete
list). Hence operator applicable state t, state u = a(t) produces applied
set atoms u = (t D(a)) A(a).
setting, Edelkamp defined abstraction given state space specifying
subset atoms restricting abstract state descriptions operator definitions
include atoms subset. Suppose Vi subset atoms underlying
abstraction mapping : Ai , original state space Ai abstract
state space based Vi . Two states mapped abstract state
1. Prieditiss definition allows abstraction expand set goals. achieved
definition mapping non-goal states original space abstract state goal.

644

fiA General Theory Additive State Space Abstractions

contain subset atoms Vi , i.e., (t) = (u) iff Vi = u Vi .
satisfies property (P1) operator applicable state (P (a) t)
implies abstract operator ai = (a) applicable abstract state ti (P (a) Vi Vi )
resulting state a(t) = (t D(a)) A(a) mapped ai (i (t))
set intersection distributes across set subtraction union (Vi ((t D(a)) A(a)) =
((Vi t) (Vi D(a))) (Vi A(a))). Again, property (P2) follows immediately
fact operators original abstract spaces primary cost 1.
Recently, Helmert et al. (2007) described general approach defining abstractions planning based transition graph abstractions. transition graph directed
graph arcs labels, transition graph abstraction directed graph
homomorphism preserves labels.2 Hence, Helmert et al.s method restricted
version definition abstraction therefore satisfies properties (P1) (P2).
Helmert et al. make following interesting observations true general
definition abstractions:
composition two abstractions abstraction. words, :
abstraction : B abstraction A, () : B
abstraction S. property abstractions exploited Prieditis (1993).
product A1 A2 two abstractions, A1 A2 , abstraction S,
state space product Cartesian product two abstract
state spaces, edge 12 product space state (t1 , t2 ) state
(u1 , u2 ) edge 1 t1 u1 A1 edge 2 t2
u2 A2 . primary cost 12 minimum C1 (1 ) C2 (2 )
residual cost 12 taken space primary cost.
working labelled edges Helmert et al. require edge connecting t1 u1
label edge connecting t2 u2 ; called synchronized
product denoted A1 A2 (refer Definition 6 defined Helmert et al. (2007)
exact definition synchronized product).
Figure 5 shows synchronized product, B, two abstractions, A1 A2 ,
3-state space edge labels b. A1 derived mapping
states s1 s2 state (s1,2 ), A2 derived mapping states s2
s3 state (s2,3 ). Note B contains four states, original
space. abstraction mapping original states s1 , s2 , s3
states (s1,2 , s1 ) (s1,2 , s2,3 ) (s3 , s2,3 ), respectively, satisfies property (P1), property
(P2) satisfied automatically edges cost 1. point view
fourth state B, (s3 , s1 ), redundant state (s1,2 , s1 ). Nevertheless distinct
state product space.
Haslum et al. (2005) introduce family heuristics, called hm (for fixed
{1, 2, ...}), based abstraction, covered definition
value heuristic state t, hm (t), defined distance abstraction
abstract goal state. Instead takes advantage special monotonicity property
2. Homomorphism means standard definition digraph homomorphism (Definition 3.2),
permits non-surjectivity (as discussed Section 3.2), opposed Helmert et al.s definition
homomorphism, allow non-surjectivity.

645

fiYang, Culberson, Holte, Zahavi & Felner




s1

b

s2

s3





b

s1, 2

s3



b


s2 , 3

s1

B=



( s3 , s2 , 3 )

b






( s1, 2 , s1 ) ( s1, 2 , s2, 3 )

( s3 , s1 )

Figure 5: original state space. A1 A2 abstractions S. B = A1 A2
synchronized product A1 A2 .

costs planning problems: cost achieving subset atoms defining goal
lower bound cost achieving goal. searching backwards
goal start state, Haslum et al. do, allows admissible heuristic defined
following recursive minimax fashion (|t| denotes number atoms state t):

0,
start



min C(s, t) + hm (s), |t|

h (t) =
(s,t)


|t| >
max hm (s),
st,|s|m

first two lines definition standard method calculating cost
least-cost path. third line uses fact cost achieving subset
atoms lower bound cost achieving entire set atoms t.
recursive calculation alternates min max calculation depending
number atoms state currently considered recursive calculation,
therefore different shortest path calculation taking maximum set
shortest path calculations.
3.4.2 Previous definitions additive abstractions
Prieditis (1993) included method (Factor) Absolver II system creating additive
abstractions, present formal definitions theory.
first thorough discussion additive abstractions due Korf Taylor (1996).
observed sliding tile puzzles Manhattan Distance heuristic, several
enhancements, sum distances set abstract spaces small
number tiles distinguished. explained Section 2.1, allowed abstract
distances added still lower bound distances original space
moves distinguished tiles counted towards abstract distance tile
646

fiA General Theory Additive State Space Abstractions

distinguished one abstraction. idea later developed series
papers (Korf & Felner, 2002; Felner et al., 2004), extended application
domains, 4-peg Towers Hanoi puzzle.
planning literature, idea proposed Haslum et al. (2005),
described partitioning operators disjoint sets B1 , ...Bk counting cost
operators set Bi abstract space Ai . example give
Blocks World operators move block would set Bi , effectively defining set
additive abstractions Blocks World exactly analogous Korf Taylor
abstractions define Manhattan Distance sliding tile puzzle.
Edelkamp (2001) took different approach defining additive abstractions STRIPS
planning representations. method involves partitioning atoms disjoint sets
V1 , ...Vk operator changes atoms one group. abstract space
Ai retains atoms set Vi operators affect atoms Vi
effect abstract space Ai naturally cost 0 Ai . Since
operator affects atoms one group, operator non-zero cost
one abstract space distances abstract spaces safely added. Haslum et
al. (2007) extended idea representations state variables could multiple
values. subsequent paper Edelkamp (2002) remarks partitioning
atoms induces partitioning operators described, additivity could
enforced assigning operator cost zero one abstract spacesa
return Korf Taylor idea.
methods described might called all-or-nothing methods defining
abstract costs, cost edge C() fully assigned cost
corresponding abstract edge Ci (i ) one abstractions corresponding edges
abstractions assigned cost zero. method obviously satisfies
property (P3) therefore additive.
theory additivity require abstract methods defined all-ornothing manner, allows C() divided way whatsoever among abstractions
long property (P3) satisfied. possibility recognized one recent publication (Katz & Domshlak, 2007), report experimental results.
generalization important eliminates requirement operators must move
one tile change atoms/variables one group, related requirement
tiles/atoms distinguished/represented exactly one abstract spaces. requirement restricted application previous methods defining additive abstractions,
precluding application state spaces Rubiks Cube, Pancake puzzle,
TopSpin. following sections show, definition, additive abstractions
defined state space, including three mentioned.
Finally, Helmert et al. (2007) showed synchronized product additive abstractions produces heuristic hsprod dominates hadd , sense hsprod (s) hadd (s)
states s. happens synchronized product forces path
used abstract spaces, whereas calculation Ci hadd based
different path. discussion negative results infeasibility highlight
problems arise Ci calculated independently.
647

fiYang, Culberson, Holte, Zahavi & Felner

4. New Applications Additive Abstractions
section next section report results applying general definition
additive abstraction given previous section three benchmark state spaces: TopSpin,
Pancake puzzle Rubiks Cube. additional experimental results may found
previous paper Yang, Culberson, Holte (2007). experiments edges
original state spaces cost 1 define Ri (i ) = 1 Ci (i ), maximum
permitted value edges cost 1. use pattern databases store heuristic values.
pre-processing time required compute pattern databases excluded
times reported results, PDB needs calculated
overhead amortized solving many problem instances.

4.1 Methods Defining Costs
investigate two general methods defining primary cost abstract state
transition Ci (i ), call cost-splitting location-based costs. illustrate
generality methods define two common ways
representing statesas vector state variables, method implemented
experiments, set logical atoms STRIPS representation planning
problems.
state variable representation state represented vector state variables,
domain possible values Dj , i.e., = (t(0), ..., t(m 1)),
t(j) Dj value assigned j th state variable state t. example, puzzles
Pancake puzzle sliding tile puzzles, typically one variable
physical location puzzle, value t(j) indicates tile location
j state t. case domain variables same. State space abstractions
defined abstracting domains. particular, setting domain abstraction
leave specific domain values unchanged (the distinguished values according )
map rest special value, dont care. abstract state corresponding
according ti =(ti (0), ..., ti (m 1)) ti (j) = (t(j)). previous research
state spaces set abstractions defined partitioning domain values
disjoint sets E1 , ..., Ek Ei set distinguished values abstraction i. Note
theory developed previous section require distinguished values
different abstractions mutually exclusive; allows value distinguished
number abstract spaces provided abstract costs defined appropriately.
mentioned previously, STRIPS representation state represented
set logical atoms true state. state variable representation
converted STRIPS representation variety ways, simplest define
atom possible variable-value combination. state variable j value v
state variable representation state atom variable-j-has-value-v true
STRIPS representation t. exact equivalent domain abstraction achieved
defining Vi , set atoms used abstraction i, atoms variable-jhas-value-v v Ei , set distinguished values domain abstraction i.
648

fiA General Theory Additive State Space Abstractions

4.1.1 Cost-splitting
state variable representation, cost-splitting method defining primary costs works
follows. state transition changes b state variables cost, C(), split
among corresponding abstract state transitions 1 , . . . , k proportion number
distinguished values assign variables, i.e., abstraction
Ci (i ) =

bi C()
b

changes b variables bi assigned distinguished values .3
example, 3 3 3 Rubiks cube composed twenty little moveable cubies
operator moves eight cubies, four corner cubies four edge cubies. Hence b = 8
state transitions . particular state transition moves three cubies
distinguished according abstraction , corresponding abstract state transition, ,
would cost 38 . Strictly speaking, require abstract edge costs integers,
fractional edge costs produced cost-splitting must scaled appropriately become
integers. implementation cost-splitting actually scaling simplify
presentation cost-splitting talk edge costs fractional.
domain value distinguished one abstraction (e.g. abstractions
defined byPpartitioning domain values) cost-splitting produces additive abstractions,
i.e., C() ki=1 Ci (i ) . C() known integer, hadd
P
defined ceiling sum abstract distances, ki=1 Ci (i )e, instead
sum.
STRIPS representation, cost-splitting could defined identically, b
number atoms changed (added deleted) operator original space bi
number atoms changed corresponding operator abstraction i.
4.1.2 Location-based Costs
location-based cost definition state variable representation, state variable loc
associated state transition full cost C() assigned abstract state
transition changes value variable loc value distinguished according
.4 Formally:

C(), = (t1i , t2i ), t1i (loc ) 6= t2i (loc ),
t2i (loc ) distinguished value according .
Ci (i ) =

0,
otherwise.
Instead focusing value assigned variable loc , location-based costs
defined equally well value variable loc changed.
either case, domain value distinguished one abstraction location-based
3. might correspond several edges original space, different cost
moving different set tiles, technically correct definition is:
Ci (i ) =

min
,i ()=i

bi C()
b

4. footnote 3, technically correct definition min,i ()=i C() instead C().

649

fiYang, Culberson, Holte, Zahavi & Felner

costs produce additive abstractions. name location-based based typical
representations used puzzles, state variable physical location
puzzle. example, Rubiks Cube one could choose reference variables
ones representing two diagonally opposite corner locations puzzle. Note
possible Rubiks cube operator changes exactly one locations. abstract
state transition would primary cost 1 cubie moved one
locations distinguished cubie abstraction, primary cost 0 otherwise.
STRIPS representation states, location-based costs defined choosing
atom Add list operator assigning full cost C() abstraction
appears Add list . atoms partitioned atom appears
one abstraction, method define additive costs.
Although cost-splitting location-based methods defining costs applied
wide range state spaces, guaranteed define heuristics superior
heuristics given state space. determined experimentally heuristics
based cost-splitting substantially improve performance sufficiently large versions
TopSpin heuristics based location-based costs vastly improve state
art 17-Pancake puzzle. experiments additive heuristics improve
state art Rubiks Cube. following subsections describe positive results
detail. negative results discussed Section 5.
4.2 TopSpin Cost-Splitting
(N, K)-TopSpin puzzle (see Figure 6) N tiles (numbered 1, . . . , N ) arranged
circular track, two physical movements possible: (1) entire set tiles may
rotated around track, (2) segment consisting K adjacent tiles track
may reversed. previous formulations puzzle state space (Felner, Zahavi,
Schaeffer, & Holte, 2005; Holte et al., 2005; Holte, Newton, Felner, Meshulam, & Furcy,
2004), represent first physical movement operator, instead designate
one tiles (tile 1) reference tile goal get tiles increasing
order starting tile (regardless position). state space therefore N
operators (numbered 1, . . . , N ), operator reversing segment length K starting
position relative current position tile 1. certain combinations N K
possible permutations generated standard goal state operators,
general space consists connected components states reachable
(Chen & Skiena, 1996). experiments section, K = 4 N varied.
sets abstractions used experiments described using tuple written
a1 a2 . . . , indicating set contains abstractions, tiles 1 . . . (a1 ) distinguished first abstraction, tiles (a1 + 1) . . . (a1 + a2 ) distinguished second
abstraction, on. example, 6-6-6 denotes set three abstractions
distinguished tiles (1 . . . 6), (7 . . . 12), (13 . . . 18) respectively.
experiments compare hadd , additive use set abstractions, hmax ,
standard use abstractions, which, described Section 2, full cost
state transition counted abstraction heuristic returns maximum
distance goal returned different abstractions. Cost-splitting used define
operator costs abstract spaces hadd . K = 4, operator moves 4 tiles.
650

fiA General Theory Additive State Space Abstractions

3

4

5

6

7

2

8

9

1

10
11

20
19

12
18

17

16

15

14

13

Figure 6: TopSpin puzzle.

bi distinguished tiles operator op applied state si abstraction i,
applying op si primary cost b4i abstraction i.
experiments heuristic defined abstraction stored pattern
database (PDB). abstraction would normally used define PDB,
set abstractions would require PDBs. However, TopSpin, two (or more)
abstractions number distinguished tiles distinguished tiles
adjacent, one PDB used suitably renaming tiles
PDB lookup. 6-6-6 abstractions, example, one PDB needed,
three lookups would done it, one abstraction. position tile 1
effectively fixed, PDB N times smaller would normally be. example,
N = 18, PDB 6-6-6 abstractions contains 17 16 . . . 13 entries.
memory needed entry hadd PDBs twice memory needed entry
hmax PDBs need represent fractional values.
ran experiments values N sets abstractions shown first two
columns Table 1. Start states generated random walk 150 moves
goal state. 1000, 50 20 start states N = 12, 16 18, respectively.
average solution length start states shown third column Table 1.
average number nodes generated average CPU time (in seconds) IDA
solve given start states shown Nodes Time columns hmax
hadd . Nodes Ratio column gives ratio Nodes using hadd Nodes using
hmax . ratio less one (highlighted bold) indicates hadd , heuristic based
additive abstractions cost-splitting, superior hmax , standard heuristic using
set abstractions.
N = 12 N = 16 best performance achieved hmax based pair
abstractions N2 distinguished tiles. N increases advantage hmax
decreases and, N = 18, hadd outperforms hmax abstractions used. Moreover,
even smaller values N hadd outperforms hmax set four abstractions
N4 distinguished tiles used. important N increases, memory
limitations preclude using abstractions N2 distinguished tiles option
use abstractions fewer distinguished tiles each. results Table 1
show hadd method choice situation.
651

fiYang, Culberson, Holte, Zahavi & Felner

N
12
12
12
16
16
18
18

Abs
6-6
4-4-4
3-3-3-3
8-8
4-4-4-4
9-9
6-6-6

Average
Solution
Length
9.138
9.138
9.138
14.040
14.040
17.000
17.000

hmax
Nodes
14,821
269,974
1,762,262
1,361,042
4,494,414,929
38,646,344
18,438,031,512

Time
0.05
1.10
8.16
3.42
13,575.00
165.42
108,155.00

hadd based
cost-splitting
Nodes
Time
53,460
0.16
346,446
1.33
1,388,183
6.44
2,137,740
4.74
251,946,069
851.00
21,285,298
91.76
879,249,695 4,713.00

Nodes
Ratio
3.60
1.28
0.78
1.57
0.056
0.55
0.04

Table 1: (N, 4)-TopSpin results using cost-splitting.
4.3 Pancake Puzzle Location-based Costs
section, present experimental results 17-Pancake puzzle using locationbased costs. notation previous section used denote sets abstractions, e.g. 5-6-6 denotes set three abstractions, first tiles (0 . . . 4)
distinguished tiles, second tiles (5 . . . 10) distinguished tiles,
third tiles (11 . . . 16) distinguished tiles. Also before, heuristic
abstraction precomputed stored pattern database (PDB). Unlike TopSpin,
symmetries Pancake puzzle enable different abstractions make use
PDB, set abstractions Pancake puzzle requires different
PDBs.
Additive abstractions defined using location-based method one reference location, leftmost position. position chosen tile
position changes whenever operator applied state original state space.
means every edge cost original space fully counted abstract
space long tile distinguished tile abstraction. before, use hadd
denote heuristic defined adding values returned individual additive
abstractions.
first experiment compares IDA using hadd best results known
17-Pancake puzzle (Zahavi, Felner, Holte, & Schaeffer, 2006) (shown Table 2),
obtained using single abstraction rightmost seven tiles (1016)
distinguished tiles advanced search technique called Dual IDA (DIDA ).5 DIDA
extension IDA exploits fact that, states permutations tiles
Pancake puzzle, state easily computable dual state sd
special property inverses paths goal paths sd goal.
paths inverses cost same, DIDA defines heuristic value state
maximum h(s) h(sd ), sometimes decide search least-cost path
sd goal looking path goal.
5. particular, DIDA jump larger (JIL) policy bidirectional pathmax method
(BPMX) propagate inconsistent heuristic values arise dual search. Zahavi et al.
(2006) provided details. BPMX first introduced Felner et al. (2005).

652

fiA General Theory Additive State Space Abstractions

results experiment shown top three rows Table 3. Algorithm column indicates heuristic search algorithm. Abs column shows set
abstractions used generate heuristics. Nodes column shows average number
nodes generated solving 1000 randomly generated start states. start states
average solution length 15.77. Time column gives average number CPU
seconds needed solve start states AMD Athlon(tm) 64 Processor 3700+
2.4 GHz clock rate 1GB memory. Memory column indicates total size
set PDBs.
N

Algorithm

17

DIDA

Abs
rightmost-7

Average
Solution
Length
15.77

h based
single large PDB
Nodes
Time
Memory
124,198,462 37.713 98,017,920

Table 2: best results known 17-Pancake puzzle (Zahavi et al., 2006),
obtained using single abstraction rightmost seven tiles (10 16)
distinguished tiles advanced search technique called Dual IDA (DIDA ).

N

Algorithm

17
17
17
17
17
17

IDA
IDA
IDA
DIDA
DIDA
DIDA

Abs
4-4-4-5
5-6-6
3-7-7
4-4-4-5
5-6-6
3-7-7

Average
Solution
Length
15.77
15.77
15.77
15.77
15.77
15.77

hadd based
Location-based Costs
Nodes Time
Memory
14,610,039 4.302
913,920
1,064,108 0.342
18,564,000
1,061,383 0.383 196,039,920
368,925 0.195
913,920
44,618 0.028
18,564,000
37,155 0.026 196,039,920

Table 3: 17-Pancake puzzle results using hadd based location-based costs.
Clearly, use hadd based location-based costs results significant reduction nodes generated compared using single large PDB, even latter
advantage used sophisticated search algorithm. Note
total memory needed 4-4-4-5 PDBs one percent memory needed
rightmost-7 PDB, yet IDA 4-4-4-5 generates 8.5 times fewer nodes
DIDA rightmost-7 PDB. Getting excellent search performance small
PDB especially important situations cost computing PDBs must
taken account addition cost problem-solving (Holte et al., 2005).
memory requirements increase significantly abstractions contain distinguished tiles, experiment improvement running time increase
accordingly. example, 3-7-7 PDBs use ten times memory 5-6-6 PDBs,
running time almost same. 5-6-6 PDBs accurate
653

fiYang, Culberson, Holte, Zahavi & Felner

little room improve them. average heuristic value start states using
5-6-6 PDBs 13.594, 2.2 less actual average solution length. average
heuristic value using 3-7-7 PDBs slightly higher (13.628).
last three rows Table 3 show results hadd location-based costs
used conjunction DIDA . results show combining additive abstractions state-of-the-art search techniques results significant reductions
nodes generated CPU time. example, 5-6-6 PDBs use 1/5 memory
rightmost-7 PDB reduce number nodes generated DIDA factor
2783 CPU time factor 1347.
compare hadd hmax ran plain IDA hmax 1000 start states,
time limit start state ten times greater time needed solve
start state using hadd . time limit 63 1000 start states could
solved hmax using 3-7-7 abstraction, 5 could solved hmax using
5-6-6 abstraction, 3 could solved hmax using 4-4-4-5 abstraction.
determine hadd superiority hmax location-based costs puzzle could
predicted using Lemma 3.6, generated 100 million random 17-Pancake puzzle
states tested many satisfied requirements Lemma 3.6. 98%
states satisfied requirements 3-7-7 abstraction, 99.8% states
satisfied requirements 5-6-6 4-4-4-5 abstractions.

5. Negative Results
experiments yielded positive results. explore trials
additive approaches perform well. examining cases closely,
shed light conditions might indicate approaches useful.
5.1 TopSpin Location-Based Costs
experiment, used 6-6-6 abstraction (18, 4)-TopSpin Section 4.2
location-based costs instead cost-splitting. primary cost operator a,
operator reverses segment consisting locations + 3 (modulo 18), 1
abstract space tile location operator applied distinguished
according abstraction 0 otherwise.
definition costs disastrous, resulting Ci (ti , gi ) = 0 abstract states
abstractions. words, finding least-cost path never necessary
use operator distinguished tile location a. always possible
move towards goal applying another operator, a0 , primary cost 0.
illustrate possible, consider state 0 4 5 6 3 2 1 (7, 4)-T opSpin.
state transformed goal single move: operator reverses
four tiles starting tile 3 produces state 3 4 5 6 0 1 2 equal
goal state cyclically shifted put 0 leftmost position. 4-3
abstraction move primary cost 0 abstract space based tiles 4...6,
would primary cost 1 abstract space based tiles 0...3 (because tile 3
leftmost location changed operator). However following sequence maps
tiles 0...3 goal locations primary cost 0 abstract space (because
dont care tile always moved reference location):
654

fiA General Theory Additive State Space Abstractions

0

*

*

*

3

2

1

0

*

*

1

2

3

*

0

*

3

2

1

*

*

0

1

2

3

*

*

*

5.2 Rubiks Cube
success cost-splitting (18,4)-TopSpin suggested might also provide improved heuristic Rubiks Cube, viewed 3-dimensional version
(20,8)-TopSpin. used standard method partitioning cubies create three
abstractions, one based 8 corner cubies, others based 6 edge cubies each.
standard heuristic based partitioning, hmax , expanded approximately three
times fewer nodes hadd based partitioning primary costs defined costsplitting. result similar whether 24 symmetries Rubiks Cube used
define multiple heuristic lookups not.
believe reason cost-splitting working well (18,4)-TopSpin Rubiks
Cube operator Rubiks Cube moves cubies number tiles
moved operator (18,4)-TopSpin. test operators moving tiles reduces
effectiveness cost-splitting solved 1000 instances (12,K)-TopSpin various values
K, using 3-3-3-3 abstraction. results shown Table 4. Nodes Ratio
column gives ratio Nodes using hadd Nodes using hmax . ratio less one
(highlighted bold) indicates hadd superior hmax . results clearly show
hadd based cost-splitting superior hmax small K steadily loses advantage
K increases. phenomenon also seen Table 1, increasing N
relative K increases effectiveness additive heuristics based cost-splitting.

K
3
4
5
6

hmax
Nodes
486,515
1,762,262
8,978
193,335,181

Time
2.206
8.164
0.043
901.000

hadd based
cost-splitting
Nodes
Time
207,479
0.952
1,388,183
6.437
20,096
0.095
2,459,204,715 11,457.000

Nodes
Ratio
0.42
0.78
2.23
12.72

Table 4: (12, K)-TopSpin results using cost-splitting.
also investigated location-based costs Rubiks Cube. cubies partitioned
four groups, containing three edge cubies two corner cubies, abstraction
defined using group. Two diagonally opposite corner positions used
reference locations (as noted above, Rubiks Cube operator changes exactly one
locations). resulting hadd heuristic weak could solve random instances
puzzle it.
655

fiYang, Culberson, Holte, Zahavi & Felner

5.3 Pancake Puzzle Cost-Splitting
Table 5 compares hadd hmax 13-Pancake puzzle costs defined using
cost-splitting. memory greater hadd hmax fractional entries
cost-splitting produces require bits per entry small integer values stored
hmax PDB. terms run-time number nodes generated, hadd inferior
hmax costs, opposite seen Section 4.3 using location-based
costs.
N
13

Abs
6-7

Average
Solution
Length
11.791

hmax
Nodes
166,479

Time
0.0466

hadd based
costing-splitting
Nodes
Time
1,218,903 0.3622

Table 5: hadd vs. hmax 13-Pancake puzzle.
Cost-splitting, defined Pancake puzzle, adversely affects hadd enables individual abstraction get artificially low estimates cost
solving distinguished tiles increasing number dont care tiles moved.
example, cost-splitting least-cost sequence operators get tile 0
goal position abstract state * 0 * * * obvious single move
reversing first two positions. move costs 21 , whereas 2-move sequence
reverses entire state reverses first four positions costs 15 + 14 .
specific example, consider state 7 4 5 6 3 8 0 10 9 2 1 11
12-Pancake puzzle. Using 6-6 abstractions, minimum number moves get
tiles 05 goal positions 8, 611 7, case ignore
final locations tiles. Thus, hmax 8. contrast, hadd 6.918, less
even smaller two numbers used define hmax . two move sequences
whose costs added compute hadd state slightly moves
corresponding sequences hmax based (10 9 compared 8 7),
involve twice many dont care tiles (45 44 compared 11 17)
less costly.
hope pathological situation detected, least sometimes,
inspecting residual costs. residual costs defined complementary
primary costs (i.e. Ri (i ) = C() Ci (i )), done, decreasing primary
cost increases residual cost. residual cost sufficiently large one abstract
spaces conditions Lemma 3.10 satisfied, signalling value returned
hadd provably low. subject next section, infeasibility.

6. Infeasible Heuristic Values
section describes way increase heuristic values defined additive abstractions
circumstances. key approach identify infeasible valuesones
cannot possibly optimal solution cost. identified infeasible values
increased give better estimate solution cost. example infeasibility occurs
656

fiA General Theory Additive State Space Abstractions

Manhattan Distance (MD) heuristic sliding tile puzzle. well-known
parity MD(t) parity optimal solution cost state t.
heuristic sliding tile puzzle returns value opposite parity,
safely increased correct parity. example relies specific properties
MD heuristic puzzle. Lemma 3.10 gives problem-independent method
testing infeasibility, use.
illustrate infeasibility detected using Lemma 3.10 consider example
Figure 3. solution abstract problem shown middle part figure
requires 9 distinguished moves, C1 (A1 ) = 9. abstract paths solve problem
9 distinguished moves require, minimum, 9 dont care moves, R1 (A1 ) = 9.
similar calculation abstract space right figure yields C2 (A2 ) = 5
R2 (A2 ) = 7. value hadd (A, g) therefore C1 (A1 ) + C2 (A2 ) = 9 + 5 = 14.
value based assumption path original space makes
C1 (A1 ) = 9 moves tiles 1, 3, 5, 7, C2 (A2 ) = 5 moves tiles. However,
value R1 (A1 ) tells us path uses 9 moves tiles 1, 3, 5, 7
put goal locations must make least 9 moves tiles, cannot
possibly make 5 moves. Therefore exist solution costing little
C1 (A1 ) + C2 (A2 ) = 14.
illustrate potential method improving additive heuristics, Table 6
shows average results IDA solving 1000 test instances 15-puzzle using two
different tile partitionings (shown Figure 7) costs defined method described
Section 2.1. additive heuristics parity property Manhattan
Distance, infeasibility detected 2 added value. hadd columns
show average heuristic value 1000 start states. seen infeasibility
checking increases initial heuristic value 0.5 reduces number nodes
generated CPU time factor 2. However, space penalty
improvement, R values must stored pattern database addition
normal C values. doubles amount memory required, clear
storing R best way use extra memory. experiment merely shows
infeasibility checking one way use extra memory speed search problems.

1 2 3
4 5 6 7
8 9 10 11
12 13 14 15

4

1
5

2
6

3
7

8 9 10 11
12 13 14 15

Figure 7: Different tile partitionings 15-puzzle (left: 5-5-5; right: 6-6-3).
Slightly stronger results obtained (N, 4)-TopSpin puzzle costs defined
cost-splitting, described Section 4.2. Infeasibility Check columns Table
7 hadd based cost-splitting columns corresponding rows
Table 1. Comparing Infeasibility Check columns shows cases
infeasibility checking reduces number nodes generated CPU time roughly
factor 2.
657

fiYang, Culberson, Holte, Zahavi & Felner

N

Abs

15
15

5-5-5
6-6-3

Average
Solution
Length
52.522
52.522

hadd based zero-one cost-splitting
Infeasibility Check
Infeasibility Check
hadd
Nodes Time
hadd
Nodes Time
41.56 3,186,654 0.642 42.10 1,453,358 0.312
42.13 1,858,899 0.379 42.78
784,145 0.171

Table 6: effect infeasibility checking 15-puzzle.

location-based costs used TopSpin infeasibility checking adds one
heuristic value almost every state. However, simply means states
heuristic value 1 instead 0 (recall discussion Section 5.1), still
poor heuristic.

N

Abs

12
12
12
16
16
18

6-6
4-4-4
3-3-3-3
8-8
4-4-4-4
6-6-6

Average
Solution
Length
9.138
9.138
9.138
14.040
14.040
17.000

hadd based costing-splitting
Infeasibility Check
Infeasibility Check
Nodes
Time
Nodes
Time
53,460
0.16
20,229
0.07
346,446
1.33
174,293
0.62
1,388,183
6.44
1,078,853
4.90
2,137,740
4.74
705,790
1.80
251,946,069
851.00 203,213,736
772.04
879,249,695 4,713.00 508,851,444 2,846.52

Table 7: effect infeasibility checking (N, 4)-TopSpin using cost-splitting.

Infeasibility checking produces almost benefit 17-Pancake puzzle locationbased costs conditions Lemma 3.10 almost never satisfied. experiment discussed end Section 4.3 showed fewer 2% states satisfy
conditions Lemma 3.10 3-7-7 abstraction, fewer 0.2% states
satisfy conditions Lemma 3.10 5-6-6 4-4-4-5 abstractions.
Infeasibility checking 13-Pancake puzzle cost-splitting also produces
little benefit, different reason. example, Table 8 shows effect infeasibility
checking 13-Pancake puzzle; results shown averages 1000 start states.
1
Cost-splitting state space produces fractional edge costs multiples 360360
(360360 Least Common Multiple integers 1 13), therefore
1
infeasibility detected amount added 360360
. recall hadd , cost-splitting,
Pk
defined ceiling i=1 Ci (i ). value hadd therefore same, whether
1
360360 added not, unless sum Ci (i ) exactly integer. Table 8 shows,
happen rarely.
658

fiA General Theory Additive State Space Abstractions

N

Abs

13

6-7

Average
Solution
Length
11.791

hadd based costing-splitting
Infeasibility Check Infeasibility Check
Nodes
Time
Nodes
Time
1,218,903
0.3622 1,218,789 0.4453

Table 8: effect infeasibility checking 13-Pancake puzzle using cost-splitting.

7. Conclusions
paper presented formal, general definition additive abstractions
removes restrictions previous definitions, thereby enabling additive abstractions defined state space. proven heuristics based additive
abstractions consistent well admissible. definition formalizes intuitive
idea abstractions additive provided cost operator divided among
abstract spaces, presented two specific, practical methods defining abstract costs, cost-splitting location-based costs. methods applied three
standard state spaces additive abstractions according previous definitions: TopSpin, Rubiks Cube, Pancake puzzle. Additive abstractions using
cost-splitting reduce search time substantially (18,4)-TopSpin additive abstractions
using location-based costs reduce search time 17-Pancake puzzle three orders
magnitude state art. also report negative results, example Rubiks
Cube, demonstrating additive abstractions always superior standard,
maximum-based method combining multiple abstractions.
distinctive feature definition edge abstract space two costs
instead one. inspired previous definitions treating distinguished moves
differently dont care moves calculating least-cost abstract paths. Formalizing
idea two costs per edge enabled us develop way testing heuristic value
returned additive abstractions provably low (infeasible). test produced
speedup applied Pancake puzzle, roughly halved search time
15-puzzle experiments TopSpin.

8. Acknowledgments
research supported part funding Canadas Natural Sciences Engineering Research Council (NSERC). Sandra Zilles Jonathan Schaeffer suggested useful
improvements drafts paper. research also supported Israel Science
Foundation (ISF) grant number 728/06 Ariel Felner.

References
Banerji, R. B. (1980). Artificial Intelligence: Theoretical Approach. North Holland.
Chen, T., & Skiena, S. (1996). Sorting fixed-length reversals. Discrete Applied Mathematics, 71 (13), 269295.
659

fiYang, Culberson, Holte, Zahavi & Felner

Culberson, J. C., & Schaeffer, J. (1996). Searching pattern databases. Advances
Artificial Intelligence (Lecture Notes Artificial Intelligence 1081), pp. 402416.
Springer.
Culberson, J. C., & Schaeffer, J. (1994). Efficiently searching 15-puzzle. Tech. rep.
TR94-08, Department Computing Science, University Alberta.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,
14(3), 318334.
Edelkamp, S. (2001). Planning pattern databases. Proceedings 6th European
Conference Planning, pp. 1324.
Edelkamp, S. (2002). Symbolic pattern databases heuristic search planning. Proceedings Sixth International Conference AI Planning Scheduling (AIPS-02), pp.
274283.
Erdem, E., & Tillier, E. (2005). Genome rearrangement planning. Proceedings
Twentieth National Conference Artificial Intelligence (AAAI-05), pp. 11391144.
Felner, A., & Adler, A. (2005). Solving 24 puzzle instance dependent pattern
databases. Proc. SARA-2005, Lecture Notes Artificial Intelligence, 3607, 248260.
Felner, A., Korf, E., & Hanan, S. (2004). Additive pattern database heuristics. Journal
Artificial Intelligence Research, 22, 279318.
Felner, A., Zahavi, U., Schaeffer, J., & Holte, R. (2005). Dual lookups pattern databases.
Proceedings Nineteenth International Joint Conference Artificial Intelligence (IJCAI-05), pp. 103108.
Gaschnig, J. (1979). problem similarity approach devising heuristics: First results.
Proceedings Sixth International Joint Conference Artificial Intelligence
(IJCAI-79), pp. 301307.
Guida, G., & Somalvico, M. (1979). method computing heuristics problem solving.
Information Sciences, 19, 251259.
Haslum, P., Bonet, B., & Geffner, H. (2005). New admissible heurisitics domainindependent planning. Proceedings Twentieth National Conference Artificial Intelligence (AAAI-05), pp. 11631168.
Haslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independent
construction pattern database heuristics cost-optimal planning. Proceedings
Twenty-Second National Conference Artificial Intelligence (AAAI-07), pp.
10071012.
Hell, P., & Nesetril, J. (2004). Graphs homomorphisms. Oxford Lecture Series
Mathematics Applications, 28.
Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics optimal
sequential planning. Proceedings 17th International Conference Automated
Planning Scheduling (ICAPS-07), pp. 176183.
Hernadvolgyi, I., & Holte, R. C. (2000). Experiments automatically created memorybased heuristics. Proc. SARA-2000, Lecture Notes Artificial Intelligence, 1864,
281290.
660

fiA General Theory Additive State Space Abstractions

Hernadvolgyi, I. T. (2003). Solving sequential ordering problem automatically
generated lower bounds. Proceedings Operations Research 2003 (Heidelberg,
Germany), pp. 355362.
Holte, R. C., Perez, M. B., Zimmer, R. M., & MacDonald, A. J. (1996). Hierarchical A*:
Searching abstraction hierarchies efficiently. Proceedings Thirteenth National
Conference Artificial Intelligence (AAAI-96), pp. 530535.
Holte, R. C., Felner, A., Newton, J., Meshulam, R., & Furcy, D. (2006). Maximizing
multiple pattern databases speeds heuristic search. Artificial Intelligence, 170,
11231136.
Holte, R. C., Grajkowski, J., & Tanner, B. (2005). Hierarchical heuristic search revisited.
Proc. SARA-2005, Lecture Notes Artificial Intelligence, 3607, 121133.
Holte, R. C., & Hernadvolgyi, I. T. (2004). Steps towards automatic creation search
heuristics. Tech. rep. TR04-02, Computing Science Department, University Alberta,
Edmonton, Canada T6G 2E8.
Holte, R. C., Newton, J., Felner, A., Meshulam, R., & Furcy, D. (2004). Multiple pattern
databases. Proceedings Fourteenth International Conference Automated
Planning Scheduling (ICAPS-04), pp. 122131.
Katz, M., & Domshlak, C. (2007). Structural patterns heuristics: Basic idea concrete instance. Proceedings ICAPS-07 Workshop Heuristics Domain-independent
Planning: Progress, Ideas, Limitations, Challenges.
Kibler, D. (1982). Natural generation admissible heuristics. Tech. rep. TR-188, University
California Irvine.
Korf, R. E. (1985). Depth-first iterative-deepening: optimal admissible tree search.
Artificial Intelligence, 27 (1), 97109.
Korf, R. E. (1997). Finding optimal solutions Rubiks Cube using pattern databases.
Proceedings Fourteenth National Conference Artificial Intelligence (AAAI97), pp. 700705.
Korf, R. E., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence,
134, 922.
Korf, R. E., & Taylor, L. A. (1996). Finding optimal solutions twenty-four puzzle.
Proceedings Thirteenth National Conference Artificial Intelligence (AAAI96), pp. 12021207.
Mostow, J., & Prieditis, A. (1989). Discovering admissible heuristics abstracting
optimizing: transformational approach. Proceedings International Joint
Conference Artificial Intelligence (IJCAI-89), pp. 701707.
Pearl, J. (1984). Heuristics. Addison Wesley, Reading, MA.
Prieditis, A. E. (1993). Machine discovery effective admissible heuristics. Machine Learning, 12, 117141.
Valtorta, M. (1984). result computational complexity heuristic estimates
A* algorithm. Information Science, 34, 4859.
661

fiYang, Culberson, Holte, Zahavi & Felner

Yang, F., Culberson, J., & Holte, R. (2007). general additive search abstraction. Tech. rep.
TR07-06, Computing Science Department, University Alberta, Edmonton, Canada
T6G 2E8.
Zahavi, U., Felner, A., Holte, R., & Schaeffer, J. (2006). Dual search permutation
state spaces. Proceedings Twenty-First Conference Artificial Intelligence
(AAAI-06), pp. 10761081.

662

fiJournal Artificial Intelligence Research 32 (2008) 901-938

Submitted 08/07; published 08/08

Ultrametric Constraint
Application Phylogenetics
Neil C.A. Moore

ncam@cs.st-andrews.ac.uk

Computer Science, University St. Andrews, Scotland

Patrick Prosser

pat@dcs.gla.ac.uk

Computing Science, Glasgow University, Scotland

Abstract
phylogenetic tree shows evolutionary relationships among species. Internal nodes
tree represent speciation events leaf nodes correspond species. goal
phylogenetics combine trees larger trees, called supertrees, whilst respecting
relationships original trees. rooted tree exhibits ultrametric property;
is, three leaves tree must one pair deeper recent common
ancestor pairs, three recent common ancestor.
inspires constraint programming encoding rooted trees. present efficient
constraint enforces ultrametric property symmetric array constrained
integer variables, inevitable property lower bounds three variables
mutually supportive. show allows efficient constraint-based solution
supertree construction problem. demonstrate versatility constraint
programming exploited allow solutions variants supertree construction
problem.

1. Introduction
One grand challenges phylogenetics build Tree Life (ToL), representation evolutionary history every living thing. date, biologists catalogued
1.7 million species, yet estimates total number species range 4 100
million. 1.7 million species identified 80,000 placed ToL
far (Pennisi, 2003). applications ToL: help understand pathogens
become virulent time, new diseases emerge, recognise species risk
extinction (Pennisi, 2003; Mace, Gittleman, & Purvis, 2003). One approach building
ToL divide conquer: combining smaller trees available TreeBase
(TreeBASE, 2003) so-called supertrees (Bininda-Emonds, 2004) approach
complete ToL.
date, supertree construction dominated imperative techniques (Semple
& Steel, 2000; Semple, Daniel, Hordijk, Page, & Steel, 2004; Daniel, 2003; Bordewich,
Evans, & Semple, 2006; Ng & Wormald, 1996; Bryant & Steel, 1995; Page, 2002)
recently new declarative approaches emerged using constraint programming (Gent,
Prosser, Smith, & Wei, 2003; Prosser, 2006; Beldiceanu, Flener, & Lorca, 2008) answer
set programming (Wu, You, & Lin, 2007). One properties rooted trees suits
approaches trees nature ultrametric: rooted trees root node
depth 0, depth nodes 1 plus depth parent. Taking
c
2008
AI Access Foundation. rights reserved.

fiMoore & Prosser

three leaves a, b c pairs must one pair deeper recent common
ancestor (mrca) pairs, three pairs mrca.
mean ultrametric, three tie minimum. fact,
know depth mrca pairs leaves structure tree uniquely
determined. inspires constraint programming encoding rooted trees, using
ultrametric constraint define later. explore solutions phylogenetic
supertree problem variants. doing, show practicality ultrametric
encoding rooted tree problems, well arguing valuable addition
set techniques supertree problems.
paper organised follows. First, introduce constraint programming
supertree construction problem. propose specialised ultrametric constraint,
terms propagation procedures, maintains bounds(Z)-consistency (Bessiere, 2006)
three variables. show specialised constraint required models
use toolkit primitives cannot guarantee ultrametric property supertree problem via propagation alone. Furthermore, space complexity models becomes
prohibitive. ultrametric constraint extended maintain property
symmetric matrix variables. go show constraint efficiently
applied problem supertree construction, particular applying propagation
model gives polynomial time procedure supertree construction. demonstrate real data give justification improvement time
space previous constraint encodings. One benefits constraint programming approach variants supertree problem addressed within
one model. justify assertion proposing constraint solution finding essential
relations supertree (Daniel, 2003), addressing ancestral divergence dates (Semple
et al., 2004; Bryant, Semple, & Steel, 2004), modelling nested taxa (Page, 2004; Daniel &
Semple, 2004) coping conflicting data.

2. Background
section give necessary definitions descriptions Constraint Satisfaction
Problem (Tsang, 1993), Constraint Programming, Supertree problem.
2.1 Constraint Programming CSP
Constraint Programming (CP) (Rossi, van Beek, & Walsh, 2007) declarative style
programming problems modelled CSP, i.e., set variables
assigned values variables domains satisfy set constraints. Values
might typically integers drawn finite domains, real numbers ranges,
complex entities like sets graphs. considering integers.
Definition 1. constraint satisfaction problem (CSP) triple (V, D, C) V set
n variables {v1 , . . . , vn }; = {dom(v1 ), . . . , dom(vn )} collection domains,
totally ordered set integer values; C = {c1 , . . . , ce } set e constraints,
scope variables scope(c) = (vc1 , . . . , vck ) relation rel(c) dom(vc1 ). . .dom(vck ).
assignment value x dom(v) variable vi V denoted (vi , x). constraint
c C satisfied assignment {(vc1 , xc1 ), . . . , (vck , xck )} scope(c) = (vc1 , . . . , vck )
902

fiThe Ultrametric Constraint Phylogenetics

(xc1 , . . . , xck ) rel(c). set assignments {(v1 , x1 ), . . . , (vn , xn )} involving every
variable problem solution satisfies constraints C.
constraint solver finds solution CSP via process constraint propagation
search. Constraint propagation inferencing process takes place variable
initialised loses values. Propagation maintains level consistency, arcconsistency (Mackworth, 1977), across variables, removing values domains
cannot occur solution (i.e., removing unsupported values). use definitions
(generalized) arc-consistency ((G)AC) due Bessiere (2006):
Definition 2. Given CSP (V, D, C), constraint c C scope(c) = (vc1 , . . . , vck ),
variable v scope(c), value x dom(v) consistent respect c (alternatively,
supported c) iff exists satisfying assignment = {(vc1 , a1 ), . . . , (vck , ak )} c
(v, x) i, ai dom(vci ). domain dom(v) (generalized) arcconsistent c iff values dom(v) consistent respect c, CSP
(generalized) arc-consistent variable domains (generalized) arc-consistent
constraints C.
Arc-consistency established CSP using algorithm AC3 (Mackworth, 1977). sake exposition assume constraints C binary
constraint c counterpart c scope(c) = (va , vb )
scope(c ) = (vb , va ) rel(c ) = rel1 (c). example, constraint
cxy = x < also constraint cyx = > x. heart AC3 revise
function, takes binary constraint c argument delivers Boolean result.
function removes dom(va ) values support dom(vb ) w.r.t.
constraint c, returns true removals take place. Initially constraints added
set S. Constraints iteratively removed revised. revise(ckm ) returns
true becomes {cik |cik C 6= k 6= m}. step considered
propagation domain reduction variable vk variables constrained vk .
iteration terminates empty variables domain becomes empty.
empty arc-consistency algorithm reached fixed point (i.e., application
arc-consistency process effect domains variables)
problem made arc-consistent. domain empties, shown
solutions globally hence stop. AC3 algorithm O(e d3 ) time
complexity, e number constraints size largest domain, however algorithms achieve time bound O(e d2 ) (Yuanlin & Yap, 2001; Bessiere
& Regin, 2001).
demonstrate arc-consistency example Figure 1 Smith (1995).
three constrained integer variables x, z, integer domain {1..5},
binary constraints cxy : x < 2, cyz : + z even, czx : z < 2x + 1. Since
constraints binary represent problem constraint graph, nodes
vertices edges constraints. Initially constraint cxy revised respect x
values {3..5} removed dom(x). cxy revised w.r.t. dom(y)
becomes {4, 5}. cyz revised w.r.t. effect revised w.r.t. z,
effect. Revising czx w.r.t. z reduces dom(z) becomes {1..4}, consequently
constraint cyz added set constraints pending revision. Constraint czx
903

fiMoore & Prosser

{1..5}

se



zi

2

y+
n

x<



x
{1..5}

z {1..5}

z < 2x + 1

Figure 1: binary constraint satisfaction problem.
import choco.Problem;
import choco.ContradictionException;
import choco.integer.*;
public class BMStut {
public static void main(String[] args) throws ContradictionException {
Problem pb
= new Problem();
IntDomainVar x
= pb.makeEnumIntVar("x",1,5); // x {1..5}
IntDomainVar
= pb.makeEnumIntVar("y",1,5); // {1..5}
IntDomainVar z
= pb.makeEnumIntVar("z",1,5); // {1..5}
IntDomainVar even = pb.makeEnumIntVar("even",new int[] {2,4,6,8,10});
pb.post(pb.gt(pb.minus(y,2),x));
// x < - 2
pb.post(pb.gt(pb.plus(pb.mult(2,x),1),z)); // z < 2x + 1
pb.post(pb.eq(even,pb.plus(y,z)));
// + z even
pb.solve();

// solve using MAC

}
}

Figure 2: JChoco constraint program CSP Figure 1.
revised w.r.t. x cyz w.r.t. y, effect. revision set point
empty arc-consistency established variable domains dom(x) = {1, 2},
dom(y) = {4, 5}, dom(z) = {1..4}.
Solving CSP may involve search, i.e., might need try different values variables order determine solution exists. Typically constraint solver begin
establishing arc-consistency, repeatedly select variable assign value
domain (instantiate it). effectively reduces variables domain singleton,
arc-consistency re-established. succeeds another instantiation made,
fails backtrack undoing recent instantiation. called MAC,
maintaining arc-consistency (Sabin & Freuder, 1994).
Figure 2 shows constraint program problem Figure 1 using choco constraint programming toolkit Java language (Choco, 2008), finds solution
x = 1, = 4, z = 2 first.
Constraint toolkits tend based around AC5 algorithm (van Hentenryck, Deville,
& Teng, 1992), allowing propagators specialised specific constraints resulting
improved efficiency adaptability. AC5 amends set AC3 contain triples
form (v, c, ) v scope(c) set values lost v, consequently
904

fiThe Ultrametric Constraint Phylogenetics

revision efficient propagation focus values may lost support,
rather check every value support. object-oriented toolkit language
constraint associated propagation methods implemented,
methods activated domain event occurs variable involved constraint.
Domain events initialisation variable, increase lower bound,
decrease upper bound, removal value bounds, instantiation
variable. exhaustive list, however toolkits allow one event:
one values lost propagator writer must determine
action take. give examples using toolkit specialised constraints,
modelling routing problem might constrained integer variable location
visited, domain values corresponding index next destination (the
so-called single-successor model). subtour elimination constraint (Caseau & Laburthe,
1997) might used ensure legal tours produced, Regins alldifferent constraint (Regin, 1994) could added increase domain filtering. pick
delivery variant, side constraints could added ensure locations
visited others. job shop scheduling problem might model uses
0/1 variables decide relative order pairs activities share resource,
might increase propagation adding Carlier Pinsons edge finding constraint (1994).
constraint programming approach general practical modelling solving
problems, provides framework combination problem specific algorithms
one solver. allows us solve many classes problems efficiently model even
problems via addition side constraints.
2.2 Supertree Problem
Supertree construction problem phylogenetics combine leaf-labelled
species trees, sets leaf labels intersect, single tree respects
arboreal relationships input tree (Bininda-Emonds, 2004). Species trees describe
part evolutionary history set species. Labels leaves correspond existing
species internal nodes represent divergence events evolutionary history one
species split least two species. Species trees may also annotated dates
internal nodes, representing time divergence event happened.
define term displays, makes precise mean respects
arboreal relationships: supertree T1 displays tree T2 T2 equivalent T4
(i.e. induce hierarchy leaf labels) T4 obtained following
steps (Semple & Steel, 2000):
1. Let L set leaves T1 T2 .
2. Let T3 unique subtree T1 connects leaves L.
3. obtain T4 : wherever subpath (p1 , . . . , pk ) path root
leaf T3 p2 , . . . , pk1 interior nodes degree 2, contract
single edge.
problem produce rooted species tree forest input trees F,
contains species F displays every tree F . Figures 3 4 illustrate
displays property.
905

fiMoore & Prosser

1
0

T1

1
0

1
0

T3

1
0

b

c



1
0









e

1 0
0
1



2

0
1
1
0
1 0
0
1 0
1

1
0

e

0
1
1
0
1 0
0
1 0
1



=
4
2

1
0

1
0

1 0
0
1 0
1 0
1 0
1



1
0

L={a,d,e}



e

e

Figure 3: example tree T1 displays tree T2

1
0

T1

1
0

L={a,d,e}

1
0

1
0

1 0
0
1 0
1 0
1 0
1





c

b

e

T4 = 2

1
0

1
0
0
1
1 0
0
1 0
1



1 0
0
1


T2

1
0

T3

1
0





e

1
0

e

0
1
1
0
1 0
0
1 0
1





e

Figure 4: example tree T1 display tree T2

906

fiThe Ultrametric Constraint Phylogenetics

say two trees T1 T2 compatible (incompatible) exists (doesnt
exist) third tree T3 displays T1 T2 . Variants supertree problem
previously published solved specialist bioinformatics literature include
finding solutions1 , counting solutions, finding conserved relationships supertrees
(Daniel, 2003), incorporating nested taxa (Semple et al., 2004), incorporating ancestral divergence dates (Semple et al., 2004) possibility contradictory input data (Semple
& Steel, 2000).

3. Ultrametric Constraint
ultrametric constraint first proposed Gent et al. (2003) within context
supertree construction (Bininda-Emonds, 2004), implemented using toolkit primitives. review encoding show constraint toolkits inefficient
terms space time. motivates creation specialised ultrametric
propagator three variables, maintains ultrametric property bounds
variables. presented describing necessary propagation methods.
extend specialised propagator maintains ultrametric property
symmetric matrix variables.
3.1 Previous Work Ultrametric Constraint
First, give definition ultrametric constraint.
Definition 3. ultrametric constraint three variables (henceforth, Um-3) x, z
constrains that:
(x > = z) (y > x = z) (z > x = y) (x = = z)

(1)

constraint ensures tie least element three, i.e., either
three same, two greater. constraint
proposed Gent et al. (2003), used Prosser (2006) times implemented
literal translation Equation 1 using toolkit primitives. Evidence obtained
JChoco, ECLiPSe ILog constraint programming toolkits shows propagation
done lower bounds combination primitive constraints. due disjunctive constraints since many constraint programming toolkits propagation delayed
one disjuncts true, known delayed-disjunction consistency
(van Hentenryck, Saraswat, & Deville, 1998). Consequently, encoding values
cannot occur satisfying assignment might pruned domain
variable. Consider case three variables: x {1, 2, 3}, {2, 3} z {3}.
domains variables already fixed point respect delayed-disjunction
consistency ultrametric assignment x takes value 1, i.e., delayeddisjunction propagation achieve arc-consistency. shall see later, finding
solution supertree problem using toolkit constraints result backtracking
search, prefer avoid this. course, higher levels consistency would overcome
this, constructive-disjunction consistency (van Hentenryck et al., 1998), singleton
1. may multiple supertrees set input trees.

907

fiMoore & Prosser

arc-consistency (Debruyne & Bessiere, 1997) filtering algorithm Lhomme (2003).
However, cost greater average case delayed-disjunction, preventing use toolkits. fact Um-3 constraint especially unfortunate
lower bounds may trimmed properly:
Lemma 1. Um-3 constraint, lower bounds supported (i.e., form ultrametric instantiation values constrained variables), support other.
Proof. Consider three supported lower bounds. Suppose contradiction two
least distinct. One distinct lowest cannot supported
account fact equal anything larger anything. Therefore
contradiction two least must equal. However lower bound least
large these, lower bounds mutually supportive.
Lemma important implications species tree model presented
detail Section 4: particular, lower bounds bounds(Z)-consistency model
form solution, bounds(Z)-consistency (Bessiere, 2006) defined follows
Definition 4. Given (V, D, C) constraint c C scope(c) = (vc1 , . . . , vck ), tuple
= (xc1 , . . . , xck ) bound support rel(c) xci , min(dom(vci ))
xci max(dom(vci )). constraint c bounds(Z)-consistent vci scope(c)
exist bound supports involving min(dom(vci )) max(dom(vci )). CSP bounds(Z)consistent every constraint c C bounds(Z)-consistent.
henceforth abbreviate bound support support bounds(Z)-consistency
BC(Z). BC(Z) differs AC puts weaker conditions values comprise
support: rather domain, need
lower upper bounds domain. means BC(Z) prunes subset
values AC can, general. Weaker levels consistency BC(Z) useful
certain problems prune number values AC easily,
fewer values much quickly. case, BC(Z) interesting level
consistency enough ensure problem solved propagation
search, shall see.
3.2 Design BC(Z) UM-3 Propagator
Section describe Um-3 propagator enforces BC(Z), namely UM-3-BCZ.
3.2.1 Analysis Lower Upper Bounds
section dont take account domains becoming empty. Since analyse lower
upper bounds isolation lower bound may pass upper bound vice-versa,
thereby emptying domain. happens propagator described Section 3.2.2
enforce BC(Z), rather terminate. problem, domain
becoming empty means solution continue would waste time.
Concordantly, section, analyse lower bounds assume upper
bound domain cannot become null reason, vice-versa.
908

fiThe Ultrametric Constraint Phylogenetics

Code style: Variables v1 , v2 , v3 , S, L constrained integer variables, synonymous domains. Consequently variable x considered domain x.lb
x.ub return references lower upper bounds respectively; SortOnLowerBounds(x,y,z)
returns tuple references variables x, z non-decreasing order lower bounds;
SortOnUpperBounds analogous; let (S,M ,L) . . . names references S, L;
result S.lb .lb, lower bound assigned equal value lower bound ,
although .lb subsequently changes distinct again; expression x b returns
intersection domains [max(x.lb,y.lb) . . . min(x.up,y.up)].

Algorithm UM-3-BCZ
LBFix(v1 ,v2 ,v3 )
A1
let (S, M, L) SortOnLowerBounds(v1 , v2 , v3 )
A2
(S.lb < .lb)
A2.1
S.lb .lb
UBFix(v1 ,v2 ,v3 )
A3
let (S, M, L) SortOnUpperBounds(v1 , v2 , v3 )
A4
(S.ub < .ub)
A4.1
(S b L = )
A4.2
.ub S.ub
A4.3
else (S b = )
A4.4
L.ub S.ub
Min event(v1 ,v2 ,v3 )
A5
LBFix(v1 ,v2 ,v3 )
A6
domains non-empty
A6.1
UBFix(v1 ,v2 ,v3 )
Max event(v1 ,v2 ,v3 )
A7
UBFix(v1 ,v2 ,v3 )
Fix event(v1 ,v2 ,v3 )
A8
LBFix(v1 ,v2 ,v3 )
A9
domains non-empty
A9.1
UBFix(v1 ,v2 ,v3 )

Figure 5: Algorithm UM-3-BCZ propagator

909

fiMoore & Prosser

(1)

(2)
l



(3)

(4)
l
s=m

s=m=l

m=l


Figure 6: Cases analysis LBFix

L
(1)

l



L
(3)

(2)



(4)

Figure 7: Cases analysis UBFix
procedure LBFix Figure 5 takes input three variables removes unsupported values lower bounds domain. intuition algorithm achieving
one needs involved tie least element, hence smallest
lower bound strictly less others must unsupported.
possible states lower bounds LBFix invoked summarised Figure 6.
Either three different (case 1), three (case 2) two
one different (cases 3 4). give relationships bounds point time
lower bound may unsupported. boxes Figure 6 shaded follows:
regions shaded black removed propagation whereas gray regions supported.
diagrams supposed suggest that, example case 1, bounds differ
1. Rather two bounds lined bound one
different another different non-zero unspecified amount. Hence
describe relationships actual values.
following shows LBFix removes unsupported values remove
supported values.
Lemma 2. LBFix invoked lower bounds argument variables supported
w.r.t. Um-3 constraint supported values removed.
Proof. cases 1 4 (Figure 6) condition line A2 satisfied, line A2.1
executed, results removal unsupported range inspection
remaining bounds mutually supportive. cases 2 3, condition line A2
failed changes made domains; bounds mutually supportive.
procedure UBFix Figure 5 job upper bounds LBFix
lower bounds. following Lemma justifies assertion cases used
proof shown Figure 7:
Lemma 3. UBFix invoked either
910

fiThe Ultrametric Constraint Phylogenetics

upper bounds argument variables supported w.r.t. UM-3-BCZ constraint supported values removed,
domain null result removing unsupported values.

Proof. Let S, L domains smallest, middle largest upper bounds,
breaking ties arbitrarily. Let s, l upper bounds.
First prove case 1 (Figure 7) shaded region supported
L b 6= : Potentially, bound supported
equal values L least small (i.e., b L 6= ),
equal value either L, value least large remaining domain.
However, notice latter impossible due fact L contains equal
value, value large this.
Similar arguments establish shaded regions L case 1, case 3
L case 3 supported b 6= , L b 6= b 6= , respectively.
establish Lemma cases 1, 2, 3 4:
Cases 2 4 condition line A4 false, domains changed.
upper bounds mutually supportive case.
Case 1 shaded region unsupported L b = . Hence
UBFix line A4.2 executed unsupported region removed. upper
bounds l L, mutually supportive.
Case 1 shaded region supported Lb 6= . shaded
region L also supported b 6= neither line A4.2 A4.4 executed
changes made domains. upper bound also supported,
L, S. shaded region L supported b =
line A4.4 executed resulting removal region. new bounds S,
L mutually supportive.
Case 3 shaded regions L supported above, L b 6=
b 6= . Hence domain changes result executing UBFix. l L
supported l L, S. supported L, S.
Case 3 shaded regions L unsupported above, L b =
b = line A4.2 UBFix executed results becoming null.
Case 3, shaded region supported shaded region L supported
Lb 6= b = , A4.4 executed remove unsupported
region. new bounds S, L mutually supportive.
Case 3, shaded region L supported shaded region supported
Symmetric previous case.

Note analog Lemma 1 upper bounds since, example, bounds
x = {1, 2, 3}, = {1, 2} z = {1} supported, mutually supportive.
911

fiMoore & Prosser

3.2.2 Propagation Algorithm
presented LBFix UBFix position present complete propagation algorithm. propagator works arbitrary domains enforces BC(Z),
except domain becomes empty, case work. algorithm
described action taken three domain events occur:
min domain lost lower bound since propagator last invoked.
max domain lost upper bound since propagator last invoked.
fix domain singleton, i.e., variable instantiated upper lower bounds
equal.
i.e. consider events bounds variables. algorithm listed lines
A5-A9 Figure 5. Intuitively procedures work because, show, change
upper bound affect support upper bounds, change lower
bound affect support lower upper bounds. Hence need run
LBFix lower bound may changed, UBFix must run change either
lower upper bounds. Whilst would correct cycle trimming upper
lower bounds fixed point reached (i.e. changes occur), guarantee
fixed point easily.
Lemma 4. possible change lower bound result loss support
another lower bound.
Proof. bounds diagram support, black shaded lower
bound lost dark gray shaded lower bound loses support.

Lemma 5. possible change lower bound result loss support
upper bound.
Proof. bounds diagram support, black shaded lower
bound lost dark gray shaded upper bound loses support.

Lemma 6. possible loss upper bound cause loss support
another upper bound.
912

fiThe Ultrametric Constraint Phylogenetics

Proof. bounds diagram support, black shaded upper
bound lost dark gray shaded upper bound loses support.

Corollary 1. impossible change upper bound result loss support
lower bound.
Proof. Lemma 1 lower bound retains support long lower bounds
intact, hence losing upper bound effect.
asymmetry upper lower bounds? due asymmetry
definition UM-3-BCZ practical repercussion BC(Z) lower bounds
must mutually supportive whereas BC(Z) upper bounds may may require
support values including lower bounds.
Corollary 1 suggests improvement algorithm Figure 5
execute Line A6 A9 lower bound lost remaining support
upper bound. However, conditionals intrinsic UBFix amount much
thing little point repeating them.
point theorems build complete proof correctness
BC(Z) status:
Theorem 1. code min, max fix events listed Figure 5 remove
values involved bound supports UM-3-BCZ constraint, and, domains
non-null propagation, resultant domains BC(Z).
Proof. First must establish values removed propagation could
involved support, result domains subsets input domains.
former immediate Lemmas 2 3, values removed result
executing LBFix UBFix. latter immediate inspection LBFix UBFix,
ever make lower bounds larger upper bounds smaller.
final thing establish BC(Z) enforced, unless domain becomes null.
domain becomes empty result running algorithm Theorem
trivially true.
domain becomes empty must show bounds supported.
lower bounds, Lemma 4 Corollary 1 know loss lower bound
result need change lower bound propagation. Lower bounds
change result either fix min events, hence propagator Figure 5 runs LBFix
either event. LBFix runs leaves lower bounds supported, shown
Lemma 2. upper bounds, Lemmas 5 6 know loss either lower
upper bound result loss upper bound. Hence upper lower bounds
change result event, lower bounds also change result LBFix,
hence propagator runs UBFix events, runs LBFix finished,
necessary. UBFix runs leaves upper bounds supported provided domain
becomes empty, shown Lemma 3.
913

fiMoore & Prosser

1
0
0
1
0
1
0
1
0
1
0
1

(a)

1
0
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1

(b)

Figure 8: Propagation done 2 domains singleton
propagation algorithm runs (1) time. operations
LBFix, UBFix, min, max fix events (1), provided domain representation
allows access upper lower bounds (1). guaranteed domain
reductions occur bounds, case here, domains represented using
one structures proposed van Hentenryck et al. (1992).
3.3 Entailment
Schulte Carlsson (2006) define entailment possible constractions
domains constraints scope consistent. detect happened
stop running propagator henceforth, since cannot prune values.
Definition 5. propagator entailed domains = {d1 , . . . , dn } set
domains subsets these, i.e., E = {e1 , . . . , en } s.t. i.ei di , fixed
point.
describe sufficient condition UM-3-BCZ constraint entailed, i.e.,
UM-3-BCZ constraint becomes entailed soon two variables singleton domains:
Theorem 2. UM-3-BCZ becomes entailed soon two variables singleton domains.
Proof. Consider possible scenarios: either two singletons (case (a)
Figure 8) distinct (case (b) Figure 8). domains propagation
shown Figure 8 boxes; domains propagation shaded gray. Clearly
remaining choices third variable valid instantiations since propagation
algorithm safe cannot removed propagation definition
propagation fixed point.
3.4 Ultrametric Matrix Constraint
supertree model presented Section 4 makes use ultrametric constraint, however
context desired end product constrain whole matrix ultrametric
matrix, merely constrain three variables.
914

fiThe Ultrametric Constraint Phylogenetics

Code style: let (i, j) index(v) declares j indices variable v matrix
constraint over.

Algorithm UM-Matrix-BCZ
Min event(v)
A1
let (i, j) index(v)
A2
k 1 . . . n
A2.1
k 6= k 6= j
A2.2
Min event(Mij , Mik , Mjk )
A2.3
Max event(Mij , Mik , Mjk )
Max event(v)
A3
let (i, j) index(v)
A4
k 1 . . . n
A4.1
k 6= k 6= j
A4.2
Max event(Mij , Mik , Mjk )
Fix event(v)
A5
let (i, j) index(v)
A6
k 1 . . . n
A6.1
k 6= k 6= j
A6.2
Min event(Mij , Mik , Mjk )
A6.3
Max event(Mij , Mik , Mjk )

Figure 9: Algorithm UM-Matrix-BCZ propagator

Definition 6. symmetric matrix ultrametric matrix every set
three distinct indices i, j k, tie minimum Mij , Mik Mjk ;
Mii = 0 i.
ultrametric matrix constraint achieved matrix posting constraint UM-3-BCZ(M
ij , Mik , Mjk ) choices distinct i, j k, cost

introducing n3 constraints. practical constraint solvers model containing constraint (n3 ) space complexity, since solver must list (n3 )
constraints stored somewhere. However, domain event occurs matrix variable Mij straightforward iterate k indices propagation
UM-Matrix-BCZ Figure 5. replaces (n3 ) space list representation set
UM-3-BCZ constraints (1) code representation. Hence propose ultrametric
matrix constraint propagator UM-Matrix-BCZ Figure 9.
propagator mimics part AC3 algorithm (Mackworth, 1977) since (a)
receives propagation event variable, (b) identifies constraints
variable, (c) arranges propagation carried out. events caused
result queued dispatched underlying propagator normal may
cause UM-Matrix-BCZ run again. variable involved n 2
constraints, since variable two indices matrix constraint
involving choice three different indices.
n
3

algorithm propagates (n) time, expensive per event using
Um-3 constraints, factor n fewer propagators wake result event.
915

fiMoore & Prosser



b

c



c

b

b

c





b

c

Figure 10: four possible relationships three leaf nodes tree: i.e. three
triples (ab)c, (ac)b, (bc)a, fan (abc).

4. Supertree Construction
review imperative solutions supertree construction problem, review first
constraint programing solution (Gent et al., 2003), present new encoding exploits
specialised UM-Matrix-BCZ constraint.
4.1 Imperative Solutions Supertree Problem
earliest imperative techniques due Bryant Steel (1995) Ng Wormald
(1996). present OneTree algorithm based Build algorithm
Aho, Sagiv, Szymanski, Ullman (1981). OneTree based observation
tree three leaf nodes define unique relation respect recent
common ancestor (mrca), mrca(a, b) interior node furthest root
leaf nodes b descendants. abuse notation writing mrca(a, b) >
mrca(c, d) former greater depth latter, similarly mrca(a, b) =
mrca(c, d) depth. Given three different leaf nodes/species (labelled
a, b, c) one following four relations must hold:
(1)

mrca(a, b) > mrca(a, c) = mrca(b, c)

(2)

mrca(a, c) > mrca(a, b) = mrca(c, b)

(3)

mrca(b, c) > mrca(b, a) = mrca(c, a)

(4)

mrca(a, b) = mrca(a, c) = mrca(b, c)

say (1), (2) (3) triples (ab)c, (ac)b, (bc)a (where
(xy)z read x closer z) (4) fan (abc), i.e.,
fan relationship species unresolved dont specify pair
closely related. shown Figure 10. Prior applying OneTree algorithm two
(or more) species trees broken triples fans using BreakUp algorithm
(Ng & Wormald, 1996), resulting linear sized encoding trees. supertree
constructed (if possible) using encoding input.
Figure 11 shows example BreakUp algorithm process. Two variants
process shown; top hard breakup, fans considered hard
evidence must respected (hard polytomies described Ng Wormald, 1996)
soft breakup fans taken lack evidence (soft polytomies
described Bryant Steel, 1995). hard breakup algorithm modified
916

fiThe Ultrametric Constraint Phylogenetics

e



b

c

b

c

g

BREAK

BREAK

BREAK

(a b c)
(a b d)
(a c d)
(b c d)

(cd)e

(de)f

(ef)g





e

c

e



f

BREAK

f

f

g



e

f

g

e

f

g



BREAK

BREAK

BREAK

BREAK

(ab)e,
(bc)e

(cd)e

(de)f

(ef)g

e

g

c

{(abc),(abd),
}
(bcd),(cd)e,
(de)f,(ef)g}

f

g



e

f

g

e

f

{(ab)e,(bc)e,
}
(cd)e,(de)f,
(ef)g}

g



Figure 11: Example execution BreakUp algorithm. top, hard breakup,
soft breakup (no fans produced)

Code style: function sortedInteriorNodes(T ) delivers set interior nodes tree nonincreasing order depth tree; degree(v) delivers degree node v; function child(v, i)
delivers ith child interior node v; uncleOrCousin(l) delivers leaf node descended
sibling parent leaf node l; function becomesLeaf(v, l) transforms interior node v leaf
node labelled l; removeChild(l, v) removes leaf node l list children interior node v.

Algorithm HardBreakup
HardBreakup(T )
1
let V sortedInteriorNodes(T )
2
let
3
let 0
4
notRoot(V [i]) degree(V [i]) > 2
5
let v V [i]
6
let c0 child(v, 0)
7
degree(v) = 2
8
let c1 child(v, 1)
9
let c2 uncleOrCousinOf(c0 )
10
{triple(c0 , c1 , c2 )}
11
v becomesLeaf(v, c0 )
12
ii+1
13
else j 1 degree(v) 2
14
k j + 1 degree(v) 1
15
let c1 child(v, j)
16
let c2 child(v, k)
17
{fan(c0 , c1 , c2 )}
18
v removeChild(c0 , v)
19 return

Figure 12: Hard breakup tree , producing triples fans.

encountering kfan broken n3 3-fans, soft breakup
fan broken linear number rooted triples. Algorithms hard soft breakups
given Figures 12 13, used imperative OneTree algorithm
constraint programming models.
917

fiMoore & Prosser

Algorithm SoftBreakup
SoftBreakup(T )
1
let V sortedInteriorNodes(T )
2
let
3
let 0
4
notRoot(V [i])
5
let v V [i]
6
let c0 child(v, 0)
7
let c1 child(v, 1)
8
let c2 uncleOrCousinOf(c0 )
9
{triple(c0 , c1 , c2 )}
10
degree(v) = 2
11
v becomesLeaf(v, c0 )
12
ii+1
13
else v removeChild(c0 , v)
14 return

Figure 13: Soft breakup tree , producing triples.

c



b

c

e

c

b



c





e

b

e

Figure 14: toy input (left) single solution supertree problem (right). Input
trees distorted make relationships resultant supertree obvious.

toy set input triples single solution shown Figure 14. triples
drawn reflect solution compatible them.
Ng Wormald (1996) give complexity OneTree O(h(n)) h(n) =
n(n + + bn)(n + + f ), n number labels, number triples, f number
fans, b sum squares number leaves fans, inverse
Ackermann function (and less 4 conceivable inputs behaves like
constant). Therefore input trees fully resolved (i.e., fans) running
918

fiThe Ultrametric Constraint Phylogenetics

time complexity O(n2 ) worst case complexity grows O(n4 ).
contrasted O(t n) complexity Bryant Steels OneTree (1995).
4.2 Constraint Encoding using Toolkit Constraints
second stage, i.e., OneTree equivalent, first solved constraint program
Gent et al. (2003). encoding takes advantage equivalence ultrametric
trees ultrametric matrices:
Definition 7. Let real symmetric n n matrix. ultrametric tree
rooted tree that:
1. n leaves, corresponding unique row M;
2. internal node least 2 children;
3. two leaves j, Mij label recent common ancestor
j;
4. along path root leaf, labels strictly increase.
Theorem 3. symmetric matrix ultrametric tree
ultrametric matrix. Furthermore, tree uniquely determines matrix
matrix uniquely determines tree .
Proof. proof given Gusfield (1997).
clear correspondence Definition 7 description species
tree given Section 4: species tree ultrametric tree matrix , Mij
depth mrca species j Mij divergence date two species.
reason use ultrametric matrix model solve supertree problem.
4.2.1 Model Gent et al.
Given input forest F n distinct leaf labels, symmetric
constrained integer variables created domains {1, . . . , n 1}
diagonal. Variable Mij depth mrca species j.
posted make whole matrix ultrametric thus ensuring
ultrametric:
Mij > Mik = Mjk
Mik > Mij = Mjk
Mjk > Mij = Mik
Mij = Mik = Mjk

n n matrix
{0} main
Initially, constraints
resulting tree

(2)

< j < k. input trees broken triples fans using either
breakup algorithms Figures 12 13. triple (ij)k produced constraint
Mij > Mik = Mjk
919

(3)

fiMoore & Prosser

1

b
c

e
f
g

3
{e,g}
5
{c,d}



b c e f g
0 5 3 3 1 5 1
5 0
3
0
3
0
1
0
5
0
0
1

{b,f}

Figure 15: One iteration algorithm convert ultrametric matrix tree
posted 3-fan (ijk)
Mij = Mik = Mjk

(4)

posted. constraints break disjunctions Equation 2. model (n2 n)/2
variables
t+f +


n
= O(n3 ) + O(n3 ) + (n3 ) = (n3 )
3

(5)

constraints, number triples f number fans. O(n3 )
one breaks disjunction one constraint Equation 2,
(n3 ) those.
4.2.2 Converting Back Tree Representation
final step use algorithm based constructive proof Gusfield (1997)
direction Theorem 3 build tree matrix produced constraint
solver. describe algorithm detail, sake intuition works
follows
Pick arbitrary leaf s. Let number distinct entries row d.
Partition leaves sets p1 , . . . , pd based entry row s.
Solve problem recursively pi ignoring rows columns
matrix pi .
Combine overall solution attaching subproblem solutions correct depth
path s.
Figure 15 shows one recursion algorithm choice leaf shows
row fully describes path corresponding tree.
920

fiThe Ultrametric Constraint Phylogenetics

Algorithm CPBuild
CPBuild(F )
1 let (V, D, C) CPModel(F )
2 F
3
BreakUp(T ) C post(t, C)
4 propagate(V,D,C) return UMToTree(V,D)
5 else fail()

Figure 16: Build supertree forest F using ultrametric constraint model.
4.2.3 Time Complexity Model Gent et al.
BreakUp, procedures build constraint model convert ultrametric
matrix tree polynomial time. However complexity backtracking search
2
O(n2 ) variables O(n)-size domains worst case O(nn ). upper bound
time taken solve supertree problem. attempted derive lesser
upper bound time complexity, since, show following section,
new model provably achieved polynomial time bound.
4.3 Constraint Encoding Using New Propagator Design
issue potentially exponential solution time model Section 4.2 worrying,
experiments time taken solve instances major issue. Conversely
memory requirements problem practice, theory! model requires
(n3 ) space n species, constant factor inhibiting. Posting constraint
Equation 1 literally (using toolkit propagators) described Section 3 uses 23 propagators JChoco toolkit. requires roughly 23 times runtime memory
single propagator, since corresponds single Java object,
comparable footprints. show empirical study Section 5, prevents
modest instances loaded typical current workstations.

Using new propagator Section 3 replace n3 propagators single
compact propagator result memory usage reduced asymptotically (n3 )
(n2 ) since model memory dominated (n2 ) space needed
matrix . Also reducing amount space initialised delivers proportional
saving build time. importantly, using new constraint provides solution
exponential time complexity, enforcing BC(Z) model allows solution
read lower bound domain. Theorem 4 proof correctness
algorithm.
Figure 16 gives schema constraint programming algorithm supertree construction, CPBuild. algorithm takes input forest F trees. line 1 constraint
model produced, i.e., n n symmetric array constrained integers variables created, n unique species forest, UM-Matrix-BCZ constraint
posted variables. Lines 2 3 breaks input trees triples
fans using either breakup algorithms given Figures 12 13, posts
model constraints. Propagators constraints executed fixed point
line 4; succeeds tree created lower bounds ultrametric matrix
otherwise fail.
921

fiMoore & Prosser

Lemma 7. propagator every constraint model enforces BC(Z) furthermore lower bounds mutually supportive, executing propagators
fixed point, either lower bounds solution, empty domain fail.
Proof. reduce domain lower bound fixed point obtained
bound supported mutually supportive supposition. Hence
every constraint simultaneously satisfied singleton domains and, definition,
solution.
Theorem 4. CPBuild polynomial time solution supertree problem.
Proof. constraints involved model triples fans ultrametric constraints. Theorem 1 know lower bounds supported
propagators run, Lemma 1 know lower bounds mutually supportive. true disjunction-breaking propagators. Hence Lemma 7,
shown Figure 16, either read solution fail. enforce BC(Z)
problem polynomial time shown below.
Immediate preserve polynomial time solution
addition polynomial number side-constraints, long additional constraints
preserve property lower bounds mutually supportive. fact, CSPs
ordered domains constraints property lower bounds
mutually supportive belong known tractable class called min-closed (Jeavons & Cooper,
1995).
4.3.1 Time Complexity CPBuild
algorithm implemented run O(n4 ) time using variation AC3
algorithm. AC3 (Mackworth, 1977) begins queue containing constraints.
repeatedly removes constraint none remain runs associated propagator.
constraints affected variables re-queued, necessary. queue empties,
propagators fixed point. need O(n3 ) constraints worst case complexity

O(n3 )
| {z }

+

build initial Q

O(n)O(n3 )
| {z }

worst case re-queues 1 value removed time

.

O(1)
| {z }

propagation time

O(n4 ) overall. matches worst case complexity OneTree (Ng & Wormald,
1996). constraint solution worst case problem unsolvable, since
unsolvable domains emptied propagation, whereas solvable instances
propagation reaches fixed point sooner.

5. Empirical Study
present empirical study determine practical improvements
achieved constraint solutions supertree problem and, so, size improvement. Experiments run using 1.7GHz Pentium 4 processor 768MB memory,
using Sun Java build 1.5.0 06-b05. constraint toolkit used JChoco version 1.1.04.
922

fiThe Ultrametric Constraint Phylogenetics

Input trees broken using hard breakup, consequently cases fans treated
hard polytomies2 .
benchmark real-life seabird data previously used Kennedy Page (2002)
Beldiceanu et al. (2008) present statistics various techniques producing supertrees, namely OneTree, CP solutions Section 4 (entries Toolkit
CPBuild). completeness reproduce results Beldiceanu et al. (2008)
data set, tabulate TreeCon. TreeCon uses single-successor model,
constrained integer variables represent nodes within tree, domains correspond
possible successors3 . unique variable represents root loops (i.e.,
vroot = root), leaf nodes indegree zero. Precedence incomparability
constraints generated input trees.
TreeCon results encoded constraint programming toolkit
processor approximately twice fast (3GHz). correct times
compensate factor. mark bold results differ significantly
CPBuild TreeCon results, specifically whose runtimes would undoubtedly
factor 10 different processor. Results reported combinations
seabird trees (input trees named G) following data tabulated below:
Data combination attempted.
n Total distinct species input trees
Sol iff supertree possible
Technique Type algorithm used solve
Build Time milliseconds initialise CP model
Solve Time milliseconds first solution,
Total = Build + Solve
Nodes Number nodes search tree
Mem Model memory MB
table, DNL means model could loaded (as large) DNF
means could solved within 30 mins, succeeded loading.
provided memory usage OneTree; however smaller
constraint encodings.
obvious thing note much faster imperative approach compared
constraint techniques. this? Primarily due lower complexity
OneTree absence fans (we investigated benefit this),
partly due generality constraint programming approach. imperative
approach highly specialised one class problem whereas constraint approach
sits within toolkit, runs top general purpose constraint maintenance system.
expect constraint approach compete raw speed
later demonstrate (in Section 6) approach benefits versatility, i.e.,
2. later section use soft breakup.
3. alternative constraint model tree might use 0/1 variables corresponding potential edges
within adjacency matrix (Prosser & Unsworth, 2006), indeed CP(Graph) computation domain
(Dooms, 2006).

923

fiMoore & Prosser

costs space time repaid ease accommodating variants problem
model.
Data
AB

n
23

Sol


AC

32

F

AD

47



AE

95

F

AF

31



AG

46



BC

29

F

BD

42





94

F

BF

30



BG

40



CD

45



CE

68



CF

34



CG

44

F

Technique
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon

Build
2056
183

Solve
374
131

2670
189

327
153

8235
220

946
248

DNL
340

DNL
1477

2497
188

379
137

7671
222

871
252

2056
171

21931
107

5833
201

930
251

DNL
335

DNL
16340

2405
174

343
99

5098
203

651
353

10056
224

1134
276

DNL
516

DNL
1451

3101
180

563
133

6683
210

587
215

924

Total
2430
314
302
13
2997
342
406
12
9181
468
398
22
DNL
1817
10393
37
2876
325
127
20
8542
474
409
21
23987
278
32
8
6763
452
301
17
DNL
16675
892
11
2748
273
144
8
5749
556
1440
13
11190
500
630
14
DNL
1967
27180
36
3662
313
393
11
7270
425
1530

Nodes
23
23

Mem
26.92
0.24

0
0

36.34
0.34

38
38

118.51
0.70

DNL
0

> 629
2.79

19
18

32.99
0.32

31
31

111.07
0.68

171
0

26.90
0.27

33
33

84.26
0.55

DNL
0

> 629
2.71

29
29

29.83
0.28

30
30

72.71
0.51

45
45

143.91
0.77

DNL
68

> 629
2.72

30
30

43.72
0.36

0
0

97.10
0.61

fiThe Ultrametric Constraint Phylogenetics

DE

104

F

DF

44



DG

56

F

EF

94

F

EG

97

F

FG

38

F

ABDF

72



ABDG

78

F

ACDF

72

F

ACDG

81

F

ACE

97

F

OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
OneTree

DNL
360

DNL
2021

6613
203

987
250

14090
252

2280
640

DNL
331

DNL
9546

DNL
344

DNL
8900

4299
195

DNL
212

27032
277

5291
722

60847
301

DNF
3633

31067
286

1931
649

DNL
307

DNL
1711

DNL
737

DNL
1632

14
DNL
2381
1126
34
7600
453
630
17
16370
892
910
19
DNL
9877
1035
12
DNL
9244
1211
15
DNL
407
62
10
32323
999
8139
34
DNF
3934
347
29
32998
935
8690
28
DNL
2018
12650
35
DNL
2369
38

DNL
0

> 629
3.31

37
37

97.10
0.60

1
0

201.42
0.99

DNL
0

> 629
2.71

DNL
0

> 629
2.89

DNL
0

61.41
0.46

63
59

382.52
1.48

DNF
0

553.49
1.91

0
0

434.84
1.61

DNL
0

> 629
2.06

DNL
0

> 629
2.91

impressive aspect matrix model Section 4.3 Section 4.2
improvement memory requirements, instances loaded comfortably. also dramatic impact build time. improvements dominate
reduction solve time practice. toolkit model outperformed CPBuild
order magnitude instance; moreover, two cases search occurring
toolkit model (on data sets BC ABDG) whereas CPBuild never search.
polynomial time complexity due provable absence search.
results also compare well Beldiceanu et al. (2008). one
case, BE, CPBuild order magnitude slower TreeCon; far
explanation this. four cases TreeCon significantly worse
CPBuild. results available TreeCon data set ACE.
noted Beldiceanu et al. yet complete filtering algorithm problem
based constraint model and, personal communication, although TreeCon
925

fiMoore & Prosser

Figure 17: Supertree largest compatible data sets birds ABDF. took 737ms
model 270ms solve using cpbuild.

model never backtracked birds data set yet proof complexity
model polynomial. also noted see CPBuild taking time
unsolvable instances solvable instances, predicted.
Figure 17 shows supertree, displayed treeView (Page, 1996), produced
largest compatible forest {A, B, D, F }. supertree 72 leaves takes
1 second produce. Although result printed table, finding forest
{A, B, C, D, E, F, G} incompatible takes 12 seconds total (1.5 seconds build
model 10 seconds determine incompatibility).
926

fiThe Ultrametric Constraint Phylogenetics

6. Versatility Constraint Model
One strengths constraint programming versatility: given constraint model
core problem model enhanced address variants original pure
problem. demonstrate versatility respect ultrametric model, presenting
four variants supertree problem (a) incorporating ancestral divergence dates
model, (b) nested taxa, (c) determining induced triple fan common
supertrees, (d) coping incompatibilities.
6.1 Ancestral Divergence Dates
Semple et al. (2004) Bryant et al. (2004) add temporal information input trees.
Interior nodes may labelled integer ranks interior node v2 proper
descendant v1 rank(v1 ) < rank(v2 ), resulting ranked phylogenetic tree. Additionally relative divergence dates may expressed form div(c,d) predates div(a,b)
interpreted divergence species c predates species b.
RankedTree algorithm (Bryant et al., 2004) takes input collection precedence
constraints derived input ranked species trees predates relations. algorithm
outputs ranked tree respects relations returns compatible.
trivial incorporate constraint model. trees ranked
pair species (i, j) leaf set instantiate constrained integer variable
Mij value mrca(i, j). predates relation div(c,d) predates div(a,b) post
constraint Mcd < Mab . done step 4 CPBuild (Figure 16), i.e., ranks
predates relations become side constraints. Similarly time bounds speciation events
posted unary constraints, i.e. dated phylogenetic tree upper lower divergence
bounds given interior nodes, l(a, b) u(a, b) give respectively lower
upper bounds divergence dates species b. constraint program
following two side constraints posted (again, step 4): l(a, b) Mab
Mab u(a, b).
demonstration ranked trees given Figure 18. left two ranked
species trees cats used recently Semple et al. (2004) originally Janczewski,
Modi, Stephens, OBrien (1995). branch lengths source trees
translated rankings added interior vertices trees. right
one 17 possible resultant supertrees. total, 7 17 solutions contain
interior nodes ranges. interior nodes labelled specific values rather
ranges 30 solutions produced, structurally identical. goes
way addressing issue enumerating supertrees compactly, raised
challenge Semple et al. (2004). Figure 19 show effect adding predates
constraint supertree construction. data previously used Bryant et al.
(2004) Figures 5 6.
6.2 Nested Taxa
taxon (plural, taxa) group organisms comprising single common ancestor
descendents (Dawkins & Wong, 2004). example species lion class birds
taxa. far, species trees leaf-labelled, however restrictive
927

fiMoore & Prosser

Figure 18: Two ranked trees cats. right one 17 possible supertrees produced
CPBuild. Displayed using Pages treeView.

Figure 19: Two input trees T1 = ((a, c), x) T2 = (b, x) resultant supertree shown
3rd position. tree far right supertree T1 T2
side constraint div(a,c) predates div(a,b), produced CPBuild
displayed using Pages treeView.

928

fiThe Ultrametric Constraint Phylogenetics

P

P

Q

Q

P

Q



b

c



e

g

b

f



e



g

b

c

f



e

Figure 20: Two input rooted X-trees T1 T2 (left) output tree T3 (right)
perfectly displays them.

trees may annotated taxa names leaves internal nodes, giving
nested taxa. example, Figure 20 shows tree T1 internal node labelled P
descendents b, i.e., b taxa nested within P taxon. Problems
related creating compatible supertrees type data raised Page (2004)
defined solved Daniel Semple (2004). set input trees possible
solution problem shown Figure 20: notice labels conserved
solution, ancestral relationships conserved and, labels li lj
input tree, li ancestor lj input tree li ancestor lj solution
tree. instance problem Higher Taxa Compatibility defined Daniel
Semple (2004) Semple et al. (2004), result tree must perfectly display
input trees. define problem formally.
Definition 8. rooted X-tree (Daniel & Semple, 2004) species tree internal
nodes well leaves may labelled set X.
following slightly loose may use label l identify labelled node,
well label itself, e.g., descendants l means descendants
node labelled l.
Definition 9. rooted X-tree perfectly displays rooted X -tree
1. X X;
2. displays , neglecting internal labels;
3. descendant b descendant b ;
4. descendant b descendant b .
rooted X-tree perfectly displays forest phylogenetic trees F = {T1 , . . . , Tn }
perfectly displays every Ti .
6.2.1 Constraint Encoding
constraint encoding implemented addition variables side constraints
standard model Section 4. describe transform input make
constraint solution simpler, describe variables constraints needed.
929

fiMoore & Prosser

T1

T2

T2

P

Q

P

Q

Q


b

c



e

f

P

e

g

f



b

c

e

g

f

Figure 21: Two input trees T1 T2 enclosing taxon P . process substitution T2 replaced T2 .

Substitution Taxon P Figure 21 appears internal node T1 , call
label enclosing taxon. Note also appears leaf T2 . input trees
preprocessed replace tree enclosing taxon P leaf tree
single subtree rooted P substituted place. must subtree
elsewhere input forest, contradiction enclosing taxon.
process add remove information, since relationships
everything tree still holds, new relationships taxa subtree
rest tree always implicit input.
aim process obtain set inputs enclosing taxa appear
internal nodes only, without loss generality constraint encoding assumes
case. Figure 21 shows example substitution process applied
trees T1 T2 , T2 would replaced T2 .
Variables constraints variables added one integer variable vl per enclosing
taxa/label l, domain {1, . . . , n 1}. value variable solution
tree depth internal node labels, labels position final
tree determined must unique node depth path one
nested taxa root. See Figure 15 suppose sake argument
enclosing taxa labels b, variable lM = 1 solution.
unique location label go root node.
Properties (1) (2) Definition 9 immediate properties
earlier model foundation one. explaining enforce
property (3) introduce notations convenience. Function desc(l, F ) returns
set descendants label l tree forest F , notDesc(l, ) returns
set labels descendants l tree .
first need constraint l must label every single species labels input.
every enclosing label l, post following set constraints:
{vl Mij | desc(l, F ) j desc(l, F ) 6= j}

(6)

label must settle least shallow mrca descendants input,
hence must remain descendants. Notice necessary consider pairs species
distinct input trees. alternative taking pairs tree work,
necessary pairs internal node l, rather two
930

fiThe Ultrametric Constraint Phylogenetics

distinct nodes happen correct depth. Next, label must constrained
label already descendant becomes one. X-tree
enclosing label l X, post following set constraints:
{vl > Mij | desc(l, {T }) j notDesc(l, )}

(7)

label l must placed strictly deeper mrca descendant
something thats descendant, i.e., non-descendents l descendent
result. illustration list generated constraints example Figure 20.
1. Equation 6 l = P : {vP Mab , vP Mag , vP Mbg }
2. Equation 6 l = Q: {vQ Mde , vQ Mdf , vQ Mef }
3. Equation 7, l = P {T1 , T2 }: {vP > Mac , vP > Mad , vP > Mae , vP >
Mbc , vP > Mbd , vP > Mbe , vP > Mgf , vP > Mgd , vP > Mge , vP > Mbf }
4. Equation 7, l = Q {T1 , T2 }: {vQ > Mad , vQ > Mae , vQ > Mbd , vQ >
Mbe , vP > Mcd , vQ > Mce , vQ > Mgf , vP > Mbf , vP > Mgd , vP > Mge }
number new constraints created Equations 6 7 bounded
number distinct pairs species, i.e. (n2 ) new constraints.
6.3 Necessity
may many possible supertrees given input forest. One question then,
relationships common supertrees? problem determining derived
induced triple (or fan) supertree necessary (i.e., common possible supertrees)
introduced (Daniel, 2003) along polynomial time decision procedure Necessity.
algorithm Necessity Figure 22 takes arguments forest F trees, assumed
compatible, rooted triple fan determines occurs every supertree
displays trees F . algorithm simple modification CPBuild,
lines 1 3 essentially same. line 4 negation triple posted
problem, posted
Mik 6= Mjk Mij Mik Mij Mik

(8)

= (ij)k posted
Mij 6= Mik Mij 6= Mjk Mik 6= Mjk

(9)

= (ijk). call made propagate make problem arc-consistent (line
5), fails necessary, otherwise necessary. algorithm
complexity CPBuild.
6.4 Coping Conflict
supertree cannot produced pair trees input triples fans
must conflict one another, either directly indirectly. Junkers quickXPlain
method (Junker, 2004) discovers minimal subset constraints posted
propagated result failure. set necessarily smallest possible set
931

fiMoore & Prosser

Algorithm Necessity
Necessity(F, )
1 let (V, D, C) CPModel(F )
2 F
3
BreakUp(T ) C post(t, C)
4 C post(, C)
5 return propagate(V, D, C)

Figure 22: triple/fan occur every supertree displays trees F ?
minimal sense removal element set constitute
sound explanation, addition constraint would redundant. set
constraints input triples fans, minimal set semantically collection input
data incompatible. Junker (2004) state method achieved worst
case 2k log2 (n/k) + 2k propagations4 , k size minimal explanation
found n number constraints.
alternative approach satisfy many input triples fans possible
within reasonable amount time, i.e., polynomial time. Semple Steel propose
algorithm, MinCutSupertree (2000), refined Page (2002).
propose similar scheme within constraint programming framework. call
algorithm GreedyBuild works follows. associate constrained integer
variable x, domain {0, 1}, triple fan. variable assigned
value 0 triple (or fan) respected, otherwise ignored. Therefore triple
(ij)k post constraint equation 10 3-fan (ijk) constraint equation
11.

(x = 0 Mij > Mik = Mjk ) (x = 1 (Mik 6= Mjk Mij Mik Mij Mik ))

(10)

(x = 0 Mij = Mik = Mjk ) (x = 1 (Mij 6= Mik Mij 6= Mjk Mik 6= Mjk ))

(11)

GreedyBuild instantiates turn x variables, i.e. decision variables,
preferring value 0 value 1, instantiation problem made arcconsistent. algorithm shown Figure 23. line 1 constraint model produced,
i.e., n n symmetric array constrained integers variables created, n
unique species forest, UM-Matrix-BCZ constraint posted
variables. variable X line 2 set decision variables. input trees
broken before, new variable x created triple fan. line
6 constraints equations 10 11 posted model. loop lines 10
12 turn select decision variable, set lowest possible value, make
problem arc-consistent. might turn cause uninstantiated variables
value 0 removed domain associated triple fan conflicts triple
4. BC(Z) propagator O(1), however, time complexity number
propagations.

932

fiThe Ultrametric Constraint Phylogenetics

Algorithm GreedyBuild
GreedyBuild(F )
1
let (V, D, C) CPModel(F )
2
let X
3
F
4
BreakUp(T )
5
let x newV ar(0, 1)
6
let c newConstraint((t x = 0) (t x = 1))
7
X X {x}
8
V V {x}
9
C post(c, C)
10 x X
11
instantiate(x)
12
propagate(V, D, C)
13 return UMToTree(V, D)

Figure 23: Greedily Build supertree forest F using ultrametric constraint model.

fan enforced. process terminates without failure,
conflicting triples fans essentially ignored. line 13 ultrametric matrix
converted tree. complexity GreedyBuild O((t + f ) n4 )
triples f fans.
GreedyBuild applied forest bird data {A, B, C, D, E, F, G} section 5, using soft breakup. data incompatible use CPBuild, however
GreedyBuild produces supertree Figure 24. supertree contains 121 species.
SoftBreakup produced 201 triples, 17 rejected. took less 2
seconds build model 100 seconds solve model.
compared CPBuild data set, taking 1.5 seconds build model
10 seconds determine incompatibility. GreedyBuild also applied data set
ABDF, producing identical supertree CPBuild, comparable time (890ms build
model 578ms solve).
executed GreedyBuild decision variables set X (lines 2, 7, 10
11) analysed identify set triples fans excluded
supertree, i.e., x variable instantiated value 1 corresponding
triple fan ignored.
Note claim biological significance arbitrary order use
suppress triples. GreedyBuild could amended follow order MinCutSupertree investigated this. GreedyBuild also enhanced
follows. Currently triple fan exists multiple input trees occurs
constraint. information could exploited weighting decision variables
take consideration relative weight evidence triple fan, e.g., number
times triple fan occurs input. decision variables instantiated
non-increasing order weight, i.e., variable ordering heuristic used.
extreme GreedyBuild modified become OptBuild full backtracking
search performed objective minimising sum decision variables,
933

fiMoore & Prosser

Figure 24: Supertree largest data set birds, ABCDEFG, 121 species.
took 2 seconds model 100 seconds solve using GreedyBuild.
Displayed using Rod Pages treeView.

934

fiThe Ultrametric Constraint Phylogenetics

potentially exponential cost time. would return tree fewest possible
input triples suppressed.
6.5 Summary
little effort, constraint model adapted deal ancestral divergence
dates nested taxa. achieved adding side constraints.
added advantage respect ancestral divergence result compact
enumeration output trees interior nodes labelled ranges rather specific
values.
input trees conflict propose two options: use quickXPlain determine
cause conflict greedily build supertree using GreedyBuild. Bryant et al. (2004)
state essentially all-or-nothing approach supertree construction
using RankedTree needed something akin MinCutSupertree, i.e.
trees incompatible build supertree violates minimum number triples
fans, polynomial time (Page, 2002; Semple & Steel, 2000). since
done Bordewich et al. (2006) also done constraint model
incorporating constraints identified section 6.1 GreedyBuild.
Although shown, obvious ancestral divergence data nested taxa
combined one model, simply adding necessary constraint auxiliary
variables variants one model. This, again, could done GreedyBuild,
would require heuristic rule used deciding constraints ignore
input trees side constraints incompatible.
opinion, deriving, combining analysing results imperative algorithms
supertree problems much difficult above. algorithms variants
required far complex data structures tailored algorithms processing them.
Moreover, combine algorithms produced seems practically impossible account
intricacy. Finally constraint programming provides various generic methods like
quickXPlain box turn interest supertree problems.

7. Conclusion
presented new constraint propagator ultrametric constraint three
integer variables, shown extended symmetric matrix constrained
integer variables. bounds(Z)-consistency established symmetric array
lower bounds variables give mutual support. sufficient modelling solving
supertree construction problem O(n4 ) time O(n2 ) space, comparable complexity OneTree (Ng & Wormald, 1996) inferior algorithm Bryant
Steel (1995). So, bother CPBuild approach efficient imperative
approaches already exist? answer lies versatility constraint programming.
Rather develop new algorithm new variant supertree problem add
side constraints base model, shown polynomial time bound
often achieved. done ancestral divergence dates nested taxa,
shown model used deliver necessary triples fans,
proposed GreedyBuild way dealing incompatible trees.
935

fiMoore & Prosser

Acknowledgments
would like thank Pierre Flener Xavier Lorca; Barbara Smith, Ian Gent Christine Wei Wu; Charles Semple, Mike Steel Rod Page; Muffy Calder Joe Sventek;
Stanislav Zivny; Chris Unsworth; three anonymous reviewers/co-authors.

References
Aho, A., Sagiv, Y., Szymanski, T., & Ullman, J. (1981). Inferring tree lowest
common ancestors application optimization relational expressions.
SIAM J. Comput, 10 (3), 405421.
Beldiceanu, N., Flener, P., & Lorca, X. (2008). Combining tree partitioning, precedence,
incompatibility constraints. Constraints, 13, 131.
Bessiere, C., & Regin, J.-C. (2001). Refining basic constraint propagation algorithm.
IJCAI, pp. 309315.
Bessiere, C. (2006). Constraint propagation. Handbook constraint programming. Elsevier. Chapter 3.
Bininda-Emonds, O. (2004). Phylogenetic Supertrees: Combining information reveal
tree life. Springer.
Bordewich, M., Evans, G., & Semple, C. (2006). Extending limits supertree methods.
Annals combinatorics, 10, 3151.
Bryant, D., & Steel, M. (1995). Extension Operations Sets Leaf-labeled Trees. Advances Applied Mathematics, 16, 425453.
Bryant, D., Semple, C., & Steel, M. (2004). Supertree methods ancestral divergence
dates applications. Bininda-Emonds, O. (Ed.), Phylogenetic Supertrees:
Combining information reveal tree life, pp. 151171. Computational Biology
Series Kluwer.
Carlier, J., & Pinson, E. (1994). Adjustment heads tails jobshop scheduling
problem. European Journal Operational Research, 78, 146161.
Caseau, Y., & Laburthe, F. (1997). Solving small TSPs constraints. Proceedings
International Conference Logic Programming, pp. 115.
Choco (2008). http://www.choco-constraints.net/ home choco constraint programming system..
Daniel, P. (2003). Supertree methods: new approaches. Masters thesis, Department
Mathematics Statistics, University Canterbury.
Daniel, P., & Semple, C. (2004). Supertree algorithms nested taxa. Bininda-Emonds,
O. (Ed.), Phylogenetic Supertrees: Combining information reveal tree life,
pp. 151171. Computational Biology Series Kluwer.
936

fiThe Ultrametric Constraint Phylogenetics

Dawkins, R., & Wong, Y. (2004). Ancestors Tale. Weidenfeld Nicholson.
Debruyne, R., & Bessiere, C. (1997). practicable filtering techniques constraint
satisfaction problem. Proceedings IJCAI97, pp. 412417.
Dooms, G. (2006). CP(Graph) Computation Domain Constraint Programming.
Ph.D. thesis, Universite catholique de Louvain, Faculte des sciences appliquees.
Gent, I., Prosser, P., Smith, B., & Wei, W. (2003). Supertree construction constraint
programming. Principles Practice Constraint Programming, pp. 837841.
Springer.
Gusfield, D. (1997). Algorithms strings, trees, sequences: computer science
computational biology. Cambridge University Press, New York, NY, USA.
Janczewski, D., Modi, W., Stephens, J., & OBrien, S. (1995). Molecular evolution
mitochondrial 12S RNA Cytochrome b sequences pantherine lineage
Felidae. Mol. Biol. Evol., 12, 690707.
Jeavons, P. G., & Cooper, M. C. (1995). Tractable constraints ordered domains. Artif.
Intell., 79 (2), 327339.
Junker, U. (2004). QUICKXPLAIN: Preferred Explanations Relaxations OverConstrained Problems. Proceedings AAAI2004, pp. 167172.
Kennedy, M., & Page, R. (2002). Seabird supertrees: Combining partial estimates procellariiform phylogeny. Auk, 69, 88108.
Lhomme, O. (2003). efficient filtering algorithm disjunction constraints.
Principles Practice Constraint Programming, pp. 904908. Springer.
Mace, G. M., Gittleman, J. L., & Purvis, A. (2003). Preserving Tree Life. Science,
300, 17071709.
Mackworth, A. (1977). Consistency networks relations. Artificial Intelligence, 8,
99118.
Ng, M. P., & Wormald, N. C. (1996). Reconstruction rooted trees subtrees. Discrete
Appl. Math., 69 (1-2), 1931.
Page, R. (1996). TREEVIEW: application display phylogenetic trees personal
computers. Computer Applications Biosciences, 12, 357358.
Page, R. (2004). Taxonomy, supertrees, tree life. Bininda-Emonds, O. (Ed.),
Phylogenetic Supertrees: Combining information reveal tree life, pp. 247265.
Computational Biology Series Kluwer.
Page, R. D. M. (2002). Modified mincut supertrees. WABI 02: Proceedings Second
International Workshop Algorithms Bioinformatics, pp. 537552 London, UK.
Springer-Verlag.
937

fiMoore & Prosser

Pennisi, E. (2003). Modernizing Tree Life. Science, 300, 16921697.
Prosser, P. (2006). Supertree construction constraint programming: recent progress
new challenges. WCB06 - Workshop Constraint Based Methods Bioinformatics, pp. 7582.
Prosser, P., & Unsworth, C. (2006). Rooted Tree Spanning Tree Constraints. 17th
ECAI Workshop Modelling Solving Problems Constraints.
Regin, J.-C. (1994). filtering algorithm constraints difference CSPs. Proceedings AAAI94, pp. 362367.
Rossi, F., van Beek, P., & Walsh, T. (2007). Handbook Constraint Programming. Elsevier.
Sabin, D., & Freuder, E. (1994). Contradicting conventional wisdom constraint satisfaction. Proceedings ECAI-94, pp. 125129.
Schulte, C., & Carlsson, M. (2006). Finite domain constraint programming systems.
Handbook constraint programming. Elsevier. Chapter 14.
Semple, C., Daniel, P., Hordijk, W., Page, R., & Steel, M. (2004). Supertree algorithms
ancestral divergence dates nested taxa. Bioinformatics, 20 (15), 23552360.
Semple, C., & Steel, M. (2000). supertree method rooted trees. Discrete Appl. Math.,
105 (1-3), 147158.
Smith, B. M. (1995). Tutorial Constraint Programming. Technical Report 95.14,
University Leeds.
TreeBASE (2003). http://www.treebase.org/ TreeBASE: database phylogenetic knowledge..
Tsang, E. (1993). Foundations Constraint Satisfaction. Academic Press.
van Hentenryck, P., Deville, Y., & Teng, C.-M. (1992). generic arc-consistency algorithm
specializations. Artificial Intelligence, 57, 291321.
van Hentenryck, P., Saraswat, V., & Deville, Y. (1998). Design, implementation,
evaluation constraint language cc(fd). Journal Logic Programming, 37, 139
164.
Wu, G., You, J.-H., & Lin, G. (2007). Quartet-based phylogeny reconstruction answer
set programming. IEEE/ACM Transactions Computational Biology Bioinformatics, 4, 139152.
Yuanlin, Z., & Yap, R. H. C. (2001). Making AC-3 optimal algorithm. IJCAI, pp.
316321.

938

fiJournal Artificial Intelligence Research 32 (2008) 793-824

Submitted 12/07; published 08/08

Analogical Dissimilarity: Definition, Algorithms
Two Experiments Machine Learning
Laurent Miclet
Sabri Bayoudh
Arnaud Delhay

LAURENT. MICLET @ UNIV- RENNES 1. FR
SABRI . BAYOUDH @ UNIV- ST- ETIENNE . FR
ARNAUD . DELHAY @ UNIV- RENNES 1. FR

IRISA/CORDIAL, 6, rue de Kerampont
BP 80518 - F-22305 Lannion Cedex, France

Abstract
paper defines notion analogical dissimilarity four objects, special
focus objects structured sequences. Firstly, studies case four objects
null analogical dissimilarity, i.e. analogical proportion. Secondly, one objects
unknown, gives algorithms compute it. Thirdly, tackles problem defining analogical
dissimilarity, measure far four objects analogical proportion.
particular, objects sequences, gives definition algorithm based optimal
alignment four sequences. gives also learning algorithms, i.e. methods find triple
objects learning sample least analogical dissimilarity given object.
Two practical experiments described: first classification problem benchmarks
binary nominal data, second shows generation sequences solving analogical
equations enables handwritten character recognition system rapidly adapted new writer.

1. Introduction
Analogy way reasoning studied throughout history philosophy
widely used Artificial Intelligence Linguistics. focus paper restricted
concept analogy called analogical proportion.
1.1 Analogical Proportion Four Elements
analogical proportion four elements A, B, C universe usually
expressed follows: B C D. Depending elements, analogical proportions1
different meanings. example, natural language analogical proportions could be:
crow raven merlin peregrine vinegar wine
sloe cherry. based semantics words. contrast, formal
universe sequences, analogical proportions abcd abc abbd abb
g gt gg ggt morphological.
Whether morphological not, examples show intrinsic ambiguity
defining analogical proportion.
could well accept, good reasons:
g gt gg ggtt vinegar wine vulgar wul. Obviously,
ambiguities inherent semantic analogies, since related meaning words
(the concepts expressed natural language). Hence, seems important, first step,
focus formal morphological properties. Moreover, solving analogies sequences
1. ambiguity, may use analogy short instead analogical proportion.
c
2008
AI Access Foundation. rights reserved.

fiM ICLET, BAYOUDH & ELHAY

operational problem several fields linguistics, morphology syntax, provides
basis learning data mining analogy universe sequences.
paper, firstly consider analogical proportions sets objects secondly present may transferred sequences elements sets.
1.2 Solving Analogical Equations
one four elements unknown, analogical proportion turns equation.
instance, sequences letters, analogical proportion wolf leaf wolves x
corresponds equation = {x | wolf leaf wolves x}. Resolving equation consists computing (possibly empty) set sequences x satisfy analogy.
sequence leaves exact semantic morphological solution. shall see that, however,
straightforward design algorithm able solve kind equation, particular
looking approximate solution necessary.
Solving analogical equations sequences useful linguistic analysis tasks
applied (with empirical resolution techniques, simple cases) mainly lexical analysis tasks.
example, Yvon (1999) presents analogical approach grapheme-to-phoneme conversion,
text-to-speech synthesis purposes. generally, resolution analogical equations
also seen basic component learning analogy systems, part lazy
learning techniques (Daelemans, 1996).
1.3 Using Analogical Proportions Machine Learning
Let = {(x, u(x))} finite set training examples, x description example
(x may sequence vector Rn , instance) u(x) label finite set. Given
description new pattern, would like assign label u(y), based
knowledge S. problem inductive learning classification rule examples,
consists finding value u point (Mitchell, 1997). nearest neighbor method,
popular lazy learning technique, simply finds description x
minimizes distance hypothesizes u(x ), label x , label y.
Moving one step further, learning analogical proportions consists searching
triple (x , z , ) x z predicts label u(y)
solution equation u(x ) u(z ) u(t ) u(y). one triple found,
voting procedure used. learning technique based resolution analogical
equations. Pirrelli Yvon (1999) discuss relevance learning procedure various
linguistic analysis tasks. important notice u(y) different domains:
example, simple case learning classification rule, may sequence whereas u
class label.
next step learning analogical proportions is, given y, find triple (x , z , )
x z holds almost true, or, closeness measure defined,
triple closest term analogical proportion. study article
quantify measure, order provide flexible method learning analogy.
794

fiA NALOGICAL ISSIMILARITY

1.4 Related Work
paper related several domains artificial intelligence. Obviously, first one
reasoning analogy. Much work done subject cognitive science point
view, led computational models reasoning analogy: see example,
classical paper (Falkenhainer, Forbus, & Gentner, 1989), book (Gentner, Holyoak, & Kokinov,
2001) recent survey (Holyoak, 2005). Usually, works use notion transfer,
within scope article. means knowledge solving problem
domain transported another domain. Since work four objects
space, implicitly ignore notion transfer different domains. Technically speaking,
restriction allows us use axiom called exchange means define analogical
proportion (see Definition 2.1). However, share works following idea: may
similar relation two couples structured objects even objects apparently
quite different. interested giving formal algorithmic definition relation.
work also aims define supervised machine learning process (Mitchell, 1997; Cornujols & Miclet, 2002), spirit lazy learning. seek extract model
learning data, merely conclude class, generally supervision, new
object inspecting (a part of) learning data. Usually, lazy learning, like k-nearest neighbors
technique, makes use unstructured objects, vectors. Since distance measures also
defined strings, trees even graphs, technique also used structured objects,
framework structural pattern recognition (see example work Bunke & Caelli, 2004;
Blin & Miclet, 2000; Basu, Bunke, & Del Bimbo, 2005). extend search nearest
neighbor learning set best triple (when combined new object,
closest make analogical proportion). requires defining analogical proportion
structured objects, like sequences, also give definition far 4-tuple objects
analogy (that call analogical dissimilarity).
Learning analogy sequences already studied, restricted manner,
linguistic data (Yvon, 1997, 1999; Itkonen & Haukioja, 1997). Reasoning learning analogy proven useful tasks like grapheme phoneme conversion, morphology translation.
Sequences letters and/or phonemes natural application work, also interested type data, structured sequences trees, prosodic representations
speech synthesis, biochemical sequences, online handwriting recognition, etc.
Analogical proportions four structured objects universe, mainly strings,
studied mathematical algorithmic approach, like ours, Mitchell (1993)
Hofstadter et al. (1994), Dastani et al. (2003), Schmid et al. (2003). best knowledge
proposition original: give formal definition analogical dissimilarity
four objects, particular sequences, produce algorithms enable
efficient use concept machine learning practical problems. already discussed
compute exact analogical proportions sequences paper Yvon et al. (2004)
given preliminary attempt compute analogical dissimilarity sequences paper
Delhay Miclet (2004). Excerpts present article presented conferences
(Bayoudh, Miclet, & Delhay, 2007a; Bayoudh, Mouchre, Miclet, & Anquetil, 2007b).
connect another field A.I., let us quote Aamodt Plaza (1994) use
term analogy Case-Based Reasoning (CBR): Analogy-based reasoning: term sometimes used, synonym case-based reasoning, describe typical case-based approach.
795

fiM ICLET, BAYOUDH & ELHAY

However, also often used characterize methods solve new problems based past cases
different domain, typical case-based methods focus indexing matching strategies single-domain cases. According authors, use word analogy broader
meaning, typical CBR deals single domain problems, analogical proportions also do.
sense, study could seen particular case CBR, applied paper supervised
learning classification rules.
1.5 Organization Paper
paper organized six sections. introduction, present section 2 general
principles govern definition analogical proportion four objects
set define analogical equation set. apply definitions Rn
{0, 1}n . Finally, section defines analogical proportion four sequences alphabet
analogy defined, using optimal alignment method four sequences.
Sections 3 introduces new concept analogical dissimilarity (AD) four objects,
measuring way much objects analogy. particular, must equivalent
say four objects analogy analogical dissimilarity null. extend
sequences. end section gives two algorithms: SEQUANA4 computes value
AD four sequences SOLVANA solves analogical equations generalized manner:
produce approximate solutions (i.e. strictly positive AD).
Section 4 begins explore use concept analogical dissimilarity supervised
machine learning. give algorithm (FADANA) fast search k-best analogical
3-tuples learning set.
Section 5 presents two applications concepts algorithms real problems.
firstly apply FADANA objects described binary nominal features. Experiments conducted classical benchmarks favorably compared standard classification techniques.
Secondly, make use SOLVANA produce new examples handwritten recognition system. allows training classifier small number learning patterns.
last section presents work done, particularly discussing real world application
learning analogy, especially universe sequences.

2. Analogical Proportions Equations
section, give formal definition analogical proportion four objects
explain solve analogical equation. Instanciations general definitions given
objects either finite sets (or equivalently binary vectors), vectors real numbers
sequences finite alphabets.
2.1 Axioms Analogical Proportion
meaning analogical proportion : B :: C : four objects set X depends
nature X, relations defined. However, general
properties required, according usual meaning word analogy philosophy
linguistics. According Lepage (2003) three basic axioms given:
Definition 2.1 (Analogical proportion) analogical proportion set X relation X 4 ,
i.e. subset X 4 . (A, B, C, D) A, four elements A, B, C said
796

fiA NALOGICAL ISSIMILARITY

analogical proportion, write: analogical proportion : B :: C : holds true,
simply : B :: C : , reads B C D. every 4-tuple analogical
proportion, following equivalences must hold true:
Symmetry relation: : B :: C : C : :: : B
Exchange means: : B :: C : : C :: B :
third axiom (determinism) requires one two following implications holds true
(the consequence):
: :: B : x
: B :: : x




x=B
x=B

According first two axioms, five formulations equivalent canonical form
: B :: C : :
B : :: : C
: C :: B :

: B :: C :


C : :: : B
B : :: : C

Consequently, three different possible analogical proportions four objects,
canonical forms:
: B :: C :

: C :: : B

: :: B : C

2.2 Analogical Equations
solve analogical equation consists finding fourth term analogical proportion,
first three known.
Definition 2.2 (Analogical equation) solution analogical equation : B :: C : x
: B :: C : .
already know previous sections that, depending nature objects definition analogy, analogical equation may either solution unique solution several
solutions. study sequel solve analogical equations different sets.
2.3 Analogical Proportion Finite Sets Binary Objects
relation equality sets, Lepage given definition analogical
proportion sets coherent axioms. useful section 2.3.2
objects described sets binary features.
2.3.1 N NALOGICAL P ROPORTION F INITE ETS
Definition 2.3 (Analogical proportion finite sets) Four sets A, B, C analogical proportion : B :: C : transformed B, C D,
adding subtracting elements C.
case, example, four sets: = {t1 , t2 , t3 , t4 , }, B = {t1 , t2 , t3 , t5 } C =
{t1 , t4 , t6 , t7 }, = {t1 , t5 , t6 , t7 }, t4 taken from, t5 added
C, giving B D.
797

fiM ICLET, BAYOUDH & ELHAY

2.3.2 OLVING NALOGICAL E QUATIONS F INITE ETS
Considering analogy sets, Lepage (2003) shown following theorem, respect
axioms analogy (section 2.1):
Theorem 2.4 (Solution analogical equation sets) Let A, B C three sets. analogical equation : B :: C : unknown solution following
conditions hold true:
B C B C
solution unique, given by:
= ((B C)\A) (B C)
2.3.3 NALOGICAL P ROPORTIONS {0, 1}n
Let X set {0, 1}n . x X [1, n], fi (x) = 1 (resp. fi (x) = 0)
means binary feature fi takes value RU E (resp. F ALSE) object x.
Let : B :: C : analogical equation. feature fi , eight different
possibilities values A, B C. derive solutions definition properties
analogy sets, two following principles:
feature fi (D) computed independently.
following table gives solution fi (D):
fi (A)
fi (B)
fi (C)
fi (D)

0
0
0
0

0
0
1
1

0
1
0
1

0
1
1
?

1
0
0
?

1
0
1
0

1
1
0
0

1
1
1
1

two cases among eight, fi (D) exists. derives defining X binary
features, equivalent defining X finite set. Theorem 2.4 imposes conditions
resolution analogical equations finite sets, results fact two binary analogical
equations solution.
2.4 Analogical Proportion Rn
2.4.1 EFINITION
Let origin Rn . Let = (a1 , a2 , . . . , ) vector Rn , defined n
coordinates. Let a, b, c four vectors Rn . interpretation analogical proportion
: b :: c : usually a, b, c, corners parallelogram, opposite
corners (see Figure 1).
Definition 2.5 (Analogical proportion Rn ) Four elements Rn analogical propor


tion (a : b :: c : d) form parallelogram, Oa + Od = Ob + Oc






equivalently ab = cd equivalently
ac = bd
straightforward axioms analogy, given section 2.1 verified definition.
798

fiA NALOGICAL ISSIMILARITY


b
c

Figure 1: Analogical parallelogram Rn .

2.4.2 OLVING NALOGICAL E QUATIONS Rn
Solving analogical equation : b :: c : x , a, b c vectors Rn x
unknown derives directly definition analogy vector spaces: four vectors must
form parallelogram. always one one solution given equation:



Ox = Ob + Oc Oa
2.5 Analogical Proportion Sequences
2.5.1 N OTATIONS
sequence2 finite series symbols finite alphabet . set sequences denoted
. x, , xy denotes concatenation x y. also denote | x | = n length
x, write x x = x1 . . . x|x| x = x[1] . . . x[n], xi x[i] . denote
empty word, null length, + = \{}.
factor (or subword) f sequence x sequence exists two sequences
u v with: x = uf v. example, abb bbac factors abbacbbaba.
subsequence sequence x = x1 . . . x|x| composed letters x indices
i1 . . . ik , i1 < i2 . . . < ik . example, ca aaa two subsequences abbacbaba.
2.5.2 EFINITION
Let alphabet. add new letter , denote , giving augmented alphabet
. interpretation new letter simply empty symbol, need
subsequent sections.
Definition 2.6 (Semantic equivalence) Let x sequence sequence . x
semantically equivalent subsequence composed letters x. denote
relation .
example, ab abaa.
Let us assume analogy , i.e. every 4-tuple a, b, c, letters ,
relation : b :: c : defined either RU E F ALSE.
Definition 2.7 (Alignment two sequences) alignment two sequences x,
, lengths n, word z alphabet ( ) ( ){(, )} whose first projection
semantically equivalent x whose second projection semantically equivalent y.
2. classically language theory, word sentence.

799

fiM ICLET, BAYOUDH & ELHAY

Informally, alignment represents one-to-one letter matching two sequences,
letters may inserted. matching (, ) permitted. alignment
presented array two rows, one x one y, word completed ,
resulting two words length.
instance, alignment x = abgef = acde :
x

=



=


|


b
|
c


|


g
|


e
|
e

f
|


extend definition alignment four sequences.
Definition 2.8 (Alignment four sequences) alignment four sequences
u, v, w, x , word z alphabet ( {})4 {(, , , )} whose projection
first, second, third fourth component respectively semantically equivalent
u, v, w x.
following definition uses alignments four sequences.
Definition 2.9 (Analogical proportion sequences) Let u, v, w x four sequences
, analogy defined. say u, v, w x analogical proportion
exists four sequences u , v , w x length n , following properties:
1. u u, v v, w w x x.
2. [1, n] analogies ui : vi :: wi : xi hold true .
One note Lepage (2001) Stroppa Yvon (2004) already proposed definition analogical proportion sequences applications linguistic data. Basically,
difference accept trivial analogies alphabet (such : b :: : b
::: :).
example, let = {a, b, , , B, C, } non trivial analogies : b :: : B ,
: :: b : : :: B : . following alignment four sequences aBA,
bBA, ba ba analogical proportion :


b


B
B




b

b






3. Analogical Dissimilarity
3.1 Motivation
section, interested defining could relaxed analogy, linguistic
expression would b almost c d. remain coherent previous definitions,
measure term almost positive real value, equal 0 analogy stands true,
increasing four objects less likely analogy. also want value,
call analogical dissimilarity (AD), good properties respect analogy. want
800

fiA NALOGICAL ISSIMILARITY

symmetrical, stay unchanged permute mean terms analogy finally
respect triangle inequality. requirements allow us, section 4, generalize
classical fast nearest neighbor search algorithm exhibit algorithmic learning process
principle extract, learning set, 3-tuple objects least AD
combined another unknown object. lazy learning technique therefore generalization
nearest neighbor method.
firstly study definition analogical dissimilarity structured sets
previous sections, secondly extend sequences.
3.2 Definition {0, 1}n
Definition 3.1 (Analogical dissimilarity {0, 1}) analogical dissimilarity four binary values given following table:
u
v
w
x
AD(u, v, w, t)

0
0
0
0
0

0
0
0
1
1

0
0
1
0
1

0
0
1
1
0

0
1
0
0
1

0
1
0
1
0

0
1
1
0
2

0
1
1
1
1

1
0
0
0
1

1
0
0
1
2

1
0
1
0
0

1
0
1
1
1

1
1
0
0
0

1
1
0
1
1

1
1
1
0
1

1
1
1
1
0

words, AD four binary values minimal number bits
switched order produce analogical proportion. seen extension edit
distance four dimensions supports coherence analogy.
Definition 3.2 (Analogical dissimilarity {0, 1}n ) analogical dissimilarity AD(u, v, w, t) four objects u, v, w finite set X defined binary features sum values
analogical dissimilarities features.
3.2.1 P ROPERTIES
definition, analogical dissimilarity following properties:
Property 3.1 (Properties AD {0, 1}n )
Coherence analogy.
(AD(u, v, w, x) = 0) u : v :: w : x
Symmetry as. AD(u, v, w, x) = AD(w, x, u, v)
Exchange medians. AD(u, v, w, x) = AD(u, w, v, x)
Triangle inequality. AD(u, v, z, t) AD(u, v, w, x) + AD(w, x, z, t)
Asymmetry to. general: AD(u, v, w, x) 6= AD(v, u, w, x)
first properties quite straightforward definition. demonstration third one
simple well. property
AD(fi (u), fi (v), fi (z), fi (t)) AD(fi (u), fi (v), fi (w), fi (x))
+ AD(fi (w), fi (x), fi (z), fi (t))
801

fiM ICLET, BAYOUDH & ELHAY


w
b

x

AD(u, v, w, x) = 2 (t, x)

v
u
Figure 2: Analogical dissimilarity vector spaces distance 2 .

holds true every 6-tuple elements every feature fi , property (4) true. demonstration done examining possible cases: impossible find 6 binary features a, b,
c, d, e, f AD(a, b, e, f ) = 2 AD(a, b, c, d) + AD(c, d, e, f ) < 2. precisely,
AD(a, b, e, f ) = 2, AD(a, b, c, d) + AD(c, d, e, f ) also equal 2 four values
(c, d) take.
3.3 Analogical Dissimilarity Rn
analogical dissimilarity four vectors must reflect way far
constructing parallelogram. Four vectors u, v, w x analogical proportion (i.e., form



parallelogram) opposite sides
uv
wx
Ou + Ox = Ov + Ow, equivalently
u + x = v + w, chosen following definition (see Figure 2):
Definition 3.3 (Analogical dissimilarity vectors) analogical dissimilarity
four vectors u, v, w x Rn defined norm k kp corresponding distance
p given real positive value AD(u, v, w, x) = p (u + x, v + w) = k(u + x) (v + w)kp .
also equal p (t, x), solution analogical equation u : v :: w : t.
Property 3.2 (Properties AD vectors) definition analogical dissimilarity
Rn guarantees following properties hold true: coherence analogy, symmetry as,
exchange medians, triangle inequality asymmetry to.
first two properties quite straightforward definition. Since k kp norm,
respects triangle inequality involves third property:
AD(u, v, z, t) AD(u, v, w, x) + AD(w, x, z, t)
3.4 Analogical Dissimilarity Sequences
present following definition two algorithms. Firstly, extend notion analogical dissimilarity sequences. first algorithm, called SEQUANA4, computes analogical
dissimilarity four sequences . second one, called SOLVANA, given analogical
equation sequences, produces Directed Acyclic Graph (DAG) solutions.
solution, gives DAG sentences least analogical dissimilarity
associated three known sentences equation.
802

fiA NALOGICAL ISSIMILARITY

two algorithms quite general, since make particular assumption alphabet sequences. alphabet simply augmented = {} produce
alignments described section 2.5. analogical dissimilarity must that:
AD(, , a, a) = 0, AD(, a, b, c) > 0 every a, b, c , constraint
required.
3.4.1 EFINITION
Let set defined analogical dissimilarity AD. augment adding
special symbol . assume analogical dissimilarity AD .
Definition 3.4 (Analogical dissimilarity four sequences) cost alignment four sequences sum analogical dissimilarities 4-tuples letters
given alignment.
analogical dissimilarity AD(u, v, w, x) four sequences cost
alignment minimal cost four sequences.
definition ensures following properties hold true: coherence analogy, symmetry
as, exchange medians asymmetry to3 .
Depending looking for, many methods developed multiples alignment bio-informatics (Needleman & Wunsch, 1970; Smith & Waterman, 1981) :
1. structure functional similarity like protein modelization, pattern identification
structure prediction DNA, methods using simultaneous alignment like MSA (Wang &
Jiang, 1994) DCA (Dress, Fllen, & Perrey, 1995), iterative alignment like MUSCLE
(Edgar, 2004) best.
2. Evolutionary similarity like phylogenic classification, methods using progressive alignment tree structure, like ClustalW (Thompson, Higgins, & Gibson, 1994),
fitted.
However, alignment methods (global local) heuristic algorithms overcome
problem time space complexity introduced first length sequences second
number sequences align. generation problem neither sequence length
around 30 characters number sequences align always four analogy need
heuristic alignment speed algorithm. techniques used bio-informatics compute
automatically substitution matrix could helpful interesting handwritten characters
recognition. Introducing Gap (Gep, Gop) penalties like DNA protein sequences also
interesting idea explore.
3.5 Computing Analogical Dissimilarity Four Sequences: SEQUANA4
Algorithm
compute AD(u, v, w, x) dynamic programming algorithm, called SEQUANA4, progresses synchronicity four sequences build optimal alignment.
3. definition AD, triangle inequality property always true sequences.

803

fiM ICLET, BAYOUDH & ELHAY

input algorithm augmented alphabet analogical dissimilarity AD(a, b, c, d). output analogical dissimilarity four sentences ,
namely AD(u, v, w, x).
give basics formulas recurrence. implementing computation, one
check correct progression indexes i, j, k l.
Initialisation
Cwu00vx00 0 ;
u

v

0
+ AD(ui , , , ) done ;
= 1, |u| Cwui0vx00 Cwi1
0 x0

u v

u v

j = 1, |v| Cw00 xj0 Cw00 xj1
+ AD(, vj , , ) done ;
0
v0
k = 1, |w| Cwu0kvx00 Cwu0k1
x0 + AD(, , wk , ) done ;

l = 1, |x| Cwu00vx0l Cwu00vx0l1 + AD(, , , xl ) done ;
Recurrence

uv

Cwik xjl

u v

C i1 j1 + AD(ui , vj , wk , xl )

wu k1vxl1

j1


Cwi1
+ AD(ui , vj , wk , )

k1 xl

ui1 vj1


Cwk xl1 + AD(ui , vj , , xl )


ui1 vj1


+ AD(ui , vj , , )
Cwk xl



u
v

j1


Cwk1 xl1 + AD(, vj , wk , xl )



uv


Cwik xj1
l1 + AD(, vj , , xl )



u
v

j1

Cwk1 xl + AD(, vj , wk , )

uv
= Cwik xj1
+ AD(, vj , , )
l


ui1 vj

Cwk1 xl1 + AD(ui , , wk , xl )




u
vj

Cwi1

k xl1 + AD(ui , , , xl )



u
vj

Cwi1

k1 xl + AD(ui , , wk , )


u
vj


+ AD(ui , , , )
Cwi1

k xl


u vj


Cwk1 xl1 + AD(, , wk , xl )



uv


Cwik xjl1 + AD(, , , xl )



u vj
Cwk1 xl + AD(, , wk , )



+ 1; j j + 1; k k + 1; l l + 1


+ 1; j j + 1; k k + 1


+ 1; j j + 1; l l + 1


+ 1; j j + 1


j j + 1; k k + 1; l l + 1


j j + 1; l l + 1


+ 1; k k + 1


j j+1


+ 1; j j + 1; l l + 1


+ 1; l l + 1


+ 1; k k + 1


ii+1


k k + 1; l l + 1


l l+1


k k+1

End
= |u| j = |v| k = |w| l = |x|.
Result
u v|v|
Cw|u|
AD(u, v, w, x) .
|w| x|x|
Complexity

algorithms runs time complexity |u|.|v|.|w|.|x| .
Correctness
correctness algorithm demonstrated recurrence, since uses dynamic programming principles. requires analogical dissimilarity properties
called: coherence analogy, symmetry exchange medians. triangle
inequality property necessary.
804

fiA NALOGICAL ISSIMILARITY

3.6 Generalized Resolution Analogical Equations Sequences: SOLVANA Algorithm
3.6.1 PPROXIMATE OLUTIONS NALOGICAL E QUATION
now, considered analogical equation either one (or several) exact solutions,
solution. latter case, concept analogical dissimilarity useful define
approximate solution.

Definition 3.5 (Best approximate solution analogical equation) Let X set
defined analogy analogical dissimilarity AD. Let : b :: c : x analogical
equation X. set best approximate solutions equation given by:



: arg min AD(a, b, c, y)
yX

words, best approximate solutions objects X closest
analogical proportion a, b c. Obviously, definition generalizes solution
analogical equation given section 2.2. Since defined AD good properties
several alphabets sequences alphabets, compute approximate solution
analogical equations domains.
easily enlarge concept define set k-best solutions analogical
equation : b :: c : x . Informally, subset k elements X minimal AD
associated fourth position a, b c.
Rn {0, 1}n , one best approximate solution analogical equation,
easily computed (see sections 3.2 3.3). Finding set k-best solutions also
simple problem.
Let us turn algorithm finds set best approximate solutions
equation u : v :: w : x objects sequences alphabet AD
defined. also make comments extend capacity find set k-best
solutions.
3.6.2 SOLVANA LGORITHM
algorithm uses dynamic programming construct 3-dimensional array. construction finished, backtracking performed produce DAG best solutions.
alignment four sequences different lengths realized inserting letters
four sequences length. done, consider column
alignment analogical dissimilarity augmented alphabet.
construct three dimensional n1 n2 n3 matrix (respectively length first,
second third sequences A, B C analogical equation B C x).
find fourth sequence, fill following recurrence:
805

fiM ICLET, BAYOUDH & ELHAY

[i, j, k]
1i,j,kn1 ,n2 ,n3



[i 1, j 1, k 1] + AD(ai , bj , ck , x)


x



[i, j 1, k 1] + AD(, bj , ck , x)



x




[i,
j,
k

1]
+


AD(, , ck , x)


x

[i, j 1, k] + AD(, bj , , x)
=
x


[i 1, j, k 1] + AD(ai , , ck , x)


x



[i 1, j 1, k] + AD(ai , bj , , x)


x



[i 1, j, k] + AD(ai , , , x)
x

ai

ith

object sequence A. = {}.

step, save cell [i, j, k] cost also letter(s) found analogical
resolution along optimal way progression. completed, backward propagation
gives us optimal generated sequences optimal analogical dissimilarity, strucured DAG.
computational complexity algorithm O(m n3 ), = Card( ) n
maximum length sequences
3.6.3 E XAMPLE
Let = {a, b, c, A, B, C} alphabet defined 5 binary features, follows:


b
c

B
C


f1
1
0
0
1
0
0
0

f2
0
1
0
0
1
0
0

f3
0
0
1
0
0
1
0

f4
1
1
1
0
0
0
0

f5
0
0
0
1
1
1
0

first three features indicates letter (for example, f1 true only)
last two indicate case letter (f4 holds true lower case letters, f5 upper case letters).
example, let ab : Bc :: Bc : x analogical equation. exact solution, six
best approximate solutions AD(ab, Bc, Bc, y) = 4, example = BB = Cc.
Figure 3 displays DAG results produced SOLVANA example.

4. Analogical Dissimilarity Machine Learning
4.1 Motivation
assume exists analogy defined set X analogical dissimilarity
AD following properties: coherence analogy, symmetry as, triangle inequality,
exchange medians asymmetry to.
Let set elements X, cardinality m, let another element X
6 S. problem tackle section find triple objects (u, v, w)
806

fiA NALOGICAL ISSIMILARITY

Figure 3: Result SOLVANA: DAG best approximate solutions analogical equation
sequences. path displays different alignment optimal cost.

that:
AD(u, v, w, y) = arg min AD(t1 , t2 , t3 , y)
t1 ,t2 ,t3

directly lead us use notion AD supervised machine learning, e.g. classification rule.
4.2 Brute Force Solution
obvious solution examine triples S. brute force method requires m3 calls
procedure computing analogical dissimilarity four objects X. According
properties analogical dissimilarity, number actually divided 8,
change theoretical practical complexity search.
situation similar search nearest neighbor Machine Learning,
naive algorithm requires distance computations. Many proposals made
decrease complexity (see example work Chvez, Navarro, Baeza-Yates, & Marroqun,
2001). chosen focus extension AESA algorithm, based property
triangle inequality distances (Mic, Oncina, & Vidal, 1994). Since defined
concept analogical dissimilarity similar property, natural explore extend
algorithm.
807

fiM ICLET, BAYOUDH & ELHAY

4.3 FADANA: FAst search least Dissimilar ANAlogy
section describes fast algorithm find, given set objects cardinalty
object y, three objects (z , , x ) analogical dissimilarity AD(z , , x , y)
minimal. based AESA technique, extended analogical dissimilarity.
Thanks properties, analogical dissimilarity AD(z, t, x, y) seen distance
two couples (z, t) (x, y), consequently basically work couples objects.
use equivalently paragraph terms (analogical) distance two couples (u, v)
(w, x) (analogical) dissimilarity four elements u, v, w x describe
AD(u, v, w, x).
4.3.1 P RELIMINARY C OMPUTATION
part, done line, compute analogical dissimilarity every
four objects data base. step complexity time space O(m4 ),
size S. come back point section 4.4, progress
AESA-like LAESA-like technique reduce computational complexity.
4.3.2 P RINCIPLE LGORITHM
basic operation compose couple objects adding object xi =
1, m. goal find couple objects lowest distance (xi , y),
change xi xi+1 . Looping times AESA-like select eliminate technique insures
finally find triple lowest analogical dissimilarity associated y.
4.3.3 N OTATIONS
Let us denote:
C set couples (u, v) distance (xi , y) already computed.
= arg min(AD(z, t, xi , y))
(z,t)U

=

arg min (AD(z, t, xi , y))
(z,t)U ,1ji

Dist = {AD(z, t, xi , y), (z, t) C}
Dist(j) j th element Dist
QuadU = {(z, t, xi , y), (z, t) C}
QuadU (j) j th element QuadU
algorithm constructed three following phases:
4.3.4 NITIALIZATION
time xi changes (when increased 1), set U refilled possible
couples objects S.
808

fiA NALOGICAL ISSIMILARITY

set C Dist contain respectively couples distances (xi , y)
measured one loop, initialized empty sets.
local minimum in, containing minimum analogical dissimilarities one loop
set infinity.
k = Card(C) represents number couples distance computed
(xi , y) current loop. k initialized zero.
Algorithm 1 Algorithm FADANA: initialization.
begin
U {(xi , xj ), = 1, j = 1, m};
C ;
+;
Dist ;
k 0;
end

4.3.5 ELECTION
goal function extract set U couple (zz, tt) promising
terms minimum analogical dissimilarity (xi , y), using criterion:
fi
fi
(zz, tt) = arg min ax fi AD(u, v, z, t) AD(z, t, xi , y) fi
(u,v)U

(z,t)C

Algorithm 2 Algorithm FADANA: selection promising couple.
selection(U, C, (xi , y), Dist)
begin
s0
= 1,P
Card(U)
PjC |AD(zj , tj , ui , vi ) Dist(j)|
jC |AD(zj , tj , ui , vi ) Dist(j)|;
arg min i;
end
end
Return (uarg min , varg min );
end

4.3.6 E LIMINATION
section couples (u, v) U analogical distance (xi , y)
less already found eliminated thanks two criteria below:
AD(u, v, z, t) AD(z, t, y, xi ) AD(u, v, xi , y)

AD(u, v, z, t) AD(z, t, y, xi ) + AD(u, v, xi , y)
809

fiM ICLET, BAYOUDH & ELHAY

= AD(z , , x , y) represents minimum analogical dissimilarity found (see
figure 4). Note updated whole algorithm never reinitialized
increased.
Algorithm 3 Algorithm FADANA: elimination useless couples.
eliminate(U, C, (xi , y), , k)
(zk , tk ) k th element QuadU
begin
= 1, Card(U)
AD(zk , tk , ui , vi ) Dist(k) +
U U {(ui , vi )};
C C {(ui , vi )};
else AD(zk , tk , ui , vi ) Dist(k)
U U {(ui , vi )};
C C {(ui , vi )};
end
end
end

Algorithm 4 Algorithm FADANA: main procedure.
begin
{xi , = 1, m};
AD +;
= Card(S)
Initialize;
U =
6
(z, t) selection(U, C, (xi , y), Dist);
Dist(k) AD(z, t, xi , y);
k = k + 1;
U U {(z, t)};
C C {(z, t)};
Dist(k)
eliminate(U, C, (xi , y), , k)
else
Dist(k);
Dist(k) < AD
AD Dist(k);
z z, t, x xi ;
end
k = 1, Card(C)
eliminate(U, C, (xi , y), , k)
end
end
end
end
best triple (z , , x ) ;
least analogical dissimilarity AD = AD(z , , x , y) ;
end

810

fiA NALOGICAL ISSIMILARITY

(u2 , v2 )
(z, t)

(y, xi )

b

b

(u1 , v1 )

b

(z , )

(u3 , v3 )
Figure 4: Elimination process FADANA.

4.4 Selection Base Prototypes FADANA
far, FADANA drawback requiring precomputing time storage O(m4 ),
practice impossible handle > 100.
go further, devised ameliorated version FADANA algorithm,
preliminary computation storage limited N.m2 , N certain number couples
objects. principle similar LAESA (Mic et al., 1994). N base prototypes couples
selected among m2 possibilities greedy process, first one chosen
random, second one far possible first one, on. distance
couples objects is, according definition analogical dissimilarity:

(x, y), (z, t) = AD(z, t, x, y)
4.5 Efficiency FADANA
conducted experiments measure efficiency FADANA. tested
algorithm four databases UCI Repository (Newman, Hettich, Blake, & Merz, 1998),
noting time percentage AD computed in-line different numbers base prototypes
compared made naive method (see Figure 5, scales logarithmic). number
base prototypes expressed percentage learning set. Obviously, learning set
contains elements, number possible 3-tuples built m3 . point explains
percentage base prototypes compared size learning set rise 100%.
number in-line computations AD mean test set.
observe results optimal number base prototypes 10% 20%
aim optimize computation time performance.

5. Two Applications Machine Learning Problems
5.1 Classification Objects Described Binary Nominal Features
purpose first experiment measure benefit analogical dissimilarity applied
basic problem classification, compared standard classifiers k-nearest neighbors, neural
networks, decision trees. benchmarking, yet interested classifying sequences, merely investigate basic concept analogical dissimilarity bring
learning classification rule symbolic objects.
811

fiM ICLET, BAYOUDH & ELHAY

Figure 5: Efficiency FADANA w.r.t. number base prototypes
5.1.1 ETHOD ESCRIPTION



Let = oi , h(oi ) | 1 learning set, h(oi ) class object oi .
objects defined binary attributes. Let x object S. learning problem find
class new object x, using learning set S. this, define learning rule based
concept analogical dissimilarity depending integer k, could called k least
dissimilar 3-tuple rule.
basic principle following: among 3-tuples (a, b, c) 3 , consider subset
produce least analogical dissimilarity associated x (the FADANA
algorithm used here). part them, analogical equation h(a) : h(b) :: h(c) : g
exact solution finite set classes. keep 3-tuples choose class
takes majority among values g class x.
precisely, procedure follows:
1. Compute analogical dissimilarity x n 3-tuples produce
solution class x.
2. Sort n 3-tuples increasing value AD associated x.
3. k-th object value p, let k greatest integer k -th object
value p.
4. Solve k analogical equations label class. Take winner votes among
k results.
explain, firstly consider case two classes 0 1 . example
3 classes follow.
Point 1 means retain 3-tuples one four4 configurations
class displayed Table 1. ignore 3-tuples lead equation trivial
solution classes:
4. actually two more, one equivalent one four (by exchange means objects).

812

fiA NALOGICAL ISSIMILARITY

h(a)
0
1
1
0

h(b)
0
0
1
1

:
:
:
:
:

::
::
::
::
::

h(c)
0
1
1
0

: h(x)
: ?
: ?
: ?
: ?

resolution
h(x) = 0
h(x) = 1

Table 1: Possible configurations 3-tuple
1 2 o3
b
b e
c e
b
c e
c e
b c
c e
c c
b e
b e
b c
c c c
c
... ... ...

h(o1 ) h(o2 ) h(o3 )
0
0
1
0
1
2
1
1
2
0
0
1
1
0
2
1
1
2
1
0
1
0
1
2
0
1
1
0
0
2
0
0
2
0
1
1
1
1
1
0
0
1
...
...
...

h(x)
1

2
1

2
0


2
2

1
1
...

AD
0
1
1
1
2
2
2
2
3
3
3
3
4
4
...

k
1
2
3
4
5

6
7
8
9
...

Table 2: example classification analogical dissimilarity. Analogical proportions whose
analogical resolution classes solution (represented ) taken
account. AD short AD(o1 , o2 , o3 , x).

h(a) : h(b) :: h(c) : h(x)
0 : 1 :: 1 : ?
1 : 0 :: 0 : ?
Example
Let = {(a, 0 ), (b, 0 ), (c, 1 ), (d, 1 ), (e, 2 )} set five labelled objects let x 6
object classified. According analogical proportion axioms, 75
(= (Card(S)3 + Card(S)2 )/2) non-equivalent analogical equations among 125(= Card(S)3 )
equations formed three objects x. Table (2) shows first 14
lines sorting regard arbitrarily analogical dissimilarity. following table gives
classification object x according k:
k
k
classification x

813

1 2 3 4 5 6 7
1 3 3 5 5 7 7
1 1 1 ? ? 2 2

fiM ICLET, BAYOUDH & ELHAY

5.1.2 W EIGHTING



ATTRIBUTES

basic idea weighting attributes importance classification, importance given discriminative. idea selecting
enhancing interesting attributes classical Machine Learning, quite new framework analogy. paper Turney (2005), discrimination done keeping frequent
patterns words. Therefore, greater importance given attributes actually discriminant. However, analogical classification system, several ways find class
unknown element. Let us take preceding two class problem example (see table 1) focus
point.
notice two ways decide class 0 class 1 (there also
third possible configuration equivalent second exchange means).
therefore take account equation used find class. define set
weights attribute, depending number classes. sets stored
call analogical weighting matrix.
Definition 5.1 analogical weighting matrix (W ) three dimensional array. first dimension attributes, second one class first element analogical
proportion third one class last element analogical proportion.
analogical proportion weighting matrix C C matrix, number attributes
C number classes.
given attribute ak rank k, element Wkij matrix indicates weight must
given ak encountered analogical proportion classes whose first element ,
j computed solution.
Hence, attribute ak :

First element

Last element (decision)
class class j
class
Wkii
Wkij
class j
Wkji
Wkjj

Since take account 3-tuples give solution class decision, possible
situations one three patterns:
Possible patterns
: :: j : j
: j :: : j
: :: :

First
element




Decision
class
j
j


observation gives us way compute values Wkij learning set.
5.1.3 L EARNING W EIGHTING ATRIX RAINING AMPLE
goal fill three dimensional analogical weighting matrix using learning set.
estimate Wkij frequency attribute k analogical proportion first
element class , solves class j .
Firstly, tabulate splitting every attribute ak classes :
814

fiA NALOGICAL ISSIMILARITY

ak = 0
ak = 1

. . . class . . .
...
n0i . . .
...
n1i . . .

ak attribute k n0i (resp. n1i ) theP
number
Pof objects class
value 0 (resp. 1) binary attribute k. Hence, 1k=0 C
i=1 nki = (the number objects
training set). Secondly, compute Wkij estimating probability find correct
analogical proportion attribute k first element class solves class j .
following table show possible ways analogical proportion
binary attribute k. 0i (resp. 1i ) 0 (resp. 1) value attribute k class .
1st
2sd
3rd

4th
5th
6th

0i : 0i :: 0j : 0j
0i : 1i :: 0j : 1j
0i : 0i :: 1j : 1j

1i : 1i :: 1j : 1j
1i : 0i :: 1j : 0j
1i : 1i :: 0j : 0j

Pk (1st ) estimates probability first analogical proportion table occurs.
Pk (1st ) = n0i n0i n0j n0j /m4
..
.
Wkij = Pk (1st ) + + Pk (6th ), compute

Wkij = (n20i + n21i )(n20j + n21j ) + 2 n0i n0j n1i n1j /(6 m4 )
decision algorithm section 5.1.1 modified point 1, turns Weighted
Analogical Proportion Classifier (W AP C):
Given x, find n 3-tuples produce solution class x. every
3-tuple among n, say (a, b, c), consider class first element class
j solution. Compute analogical dissimilarity x 3-tuple
weighted AD:

X
Wkij AD(ak , bk , ck , xk )
AD(a, b, c, x) =
k=1

Otherwise, point 1 modified, method called Analogical Proportion Classifier (AP C).
5.1.4 E XPERIMENTS



R ESULTS

applied weighted analogical proportion classifier (W AP C) eight classical data
bases, binary nominal attributes, UCI Repository.
MONK 1,2 3 Problems (MO.1, MO.2 MO.3), MONK3 problem noise added.
SPECT heart data (SP.). Balance-Scale (B.S) Hayes Roth (H.R) database, multiclass
database. Breast-W (Br.) Mushroom (Mu.), data sets contain missing values. kr-vs-kp
Kasparov vs Karpov (k.k.).
order measure efficiency W AP C, applied standard classifiers
databases, also applied AP C point contribution weighting matrix
(Sect.5.1.2). give parameters used comparison method Table 3:
815

fiM ICLET, BAYOUDH & ELHAY

Decision Table: number non improving decision tables consider abandoning
search 5.
Id3: unpruned decision tree, missing values allowed.
Part: partial C4.5 decision tree iteration turns best leaf rule, One-pervalue encoding.
Multi layer Perceptron: back propagation training, One-per-value encoding, one hidden
layer (# classes + # attributes)/2 nodes.
LMT (logistic model trees): classification trees logistic regression functions
leaves, One-per-value encoding.
IB1: Nearest-neighbor classifier normalized Euclidean distance, better results IB10.
JRip: propositional rule learner, Repeated Incremental Pruning Produce Error Reduction
(RIPPER), optimized version IREP. .
worked WEKA package (Witten & Frank, 2005), choosing 6 different classification rules data. well fit binary data, like ID3, PART, Decision Table.
Others, like IB1 Multilayer Perceptron, adapted numerical noisy data.
results given Table 3. arbitrarily taken k = 100 two rules. value
k sensitive case nominal binary data small databases ones
used experiments (see Figure 6). However, possible set k using validation
set.

Recognition rates %

100
rs
ut

ut

rs
ut

rs

rs
rs
ut

ut

rs

rs
rs

rs

ut
ut

ut
ut

80

60
100

101

102

103

value k
Figure 6: Modification recognition rate subject k. Full line dotted line respectively
recognition rates database breast-w vote.

draw following conclusions study: firstly, according good classification
rate W AP C Br. Mu. databases, say W AP C handles missing values
well. Secondly, W AP C seems belong best classifiers B.S H.R databases,
816

fiA NALOGICAL ISSIMILARITY

Methods
MO.1 MO.2 MO.3 SP. B.S Br. H.R Mu. k.k.
nb. nominal atts.
7
7
7
22
4
9
4
22
34
nb. binary atts.
15
15
15
22
4
9
4
22
38
nb. train instances
124
169 122 80 187 35
66
81
32
nb. test instances
432
432 432 172 438 664 66 8043 3164
nb. classes
2
2
2
2
3
2
4
2
2
WAPC (k = 100)
98% 100% 96% 79% 86% 96% 82% 98% 61%
APC (k = 100)
98% 100% 96% 58% 86% 91% 74% 97% 61%
Decision Table
100% 64% 97% 65% 67% 86% 42% 99% 72%
Id3
78% 65% 94% 71% 54% 71% 71%
PART
93% 78% 98% 81% 76% 88% 82% 94% 61%
Multi layer Perceptron 100% 100% 94% 73% 89% 96% 77% 96% 76%
LMT
94% 76% 97% 77% 89% 88% 83% 94% 81%
IB1
79% 74% 83% 80% 62% 96% 56% 98% 71%
IBk (k = 10)
81% 79% 93% 57% 82% 86% 61% 91%
IB1 (k = 5)
73% 59% 97% 65% 78% 95% 80% 97%
JRip
75% 62.5% 88% 80% 69% 86% 85% 97% 94%

Table 3: Comparison Table W AP C classical classifiers eight data sets. Best
classifiers database bold significance level equal 5%.

confirms W AP C deals well multiclass problems. Thirdly, shown good
classification rate W AP C MO.3 problem, W AP C handles well noisy data. Finally,
results MO. B.S database exactly weighted decision rule W AP C
AP C. due fact AD computed k = 100 null value.
data bases, weighting quite effective. Unfortunatly, last database show
W AP C poor recognition rate databases, means analogy fit
classification problems.
5.2 Handwritten Character Recognition: Generation New Examples
5.2.1 NTRODUCTION
number Pattern Recognition systems, acquisition labeled data expensive user
unfriendly process. example, buying smartphone equipped handwritten recognition system, customer likely write dozens examples every letter digit order
provide system consequent learning sample. However, efficient, statistical
classification system retrained new personal writing style new patterns
many examples possible, least sufficient number well chosen examples.
overcome paradox, hence make possible learning classifier
examples, straightforward idea generate new examples randomly adding noise
elements small learning sample. recent book, Bishop (2007) gives theoretical coverage
procedure, rather draws pragmatic conclusion: . . . addition random noise
inputs . . . shown improve generalization appropriate circumstances.
far character recognition concerned, generating synthetic data learning
recognition system mainly used offline systems (which process image char817

fiM ICLET, BAYOUDH & ELHAY

u=
v=
w=
x0 =

9 9 99999 E
1 L 8 9 9 9 9 9 10 E
9 8 9 9 9 9 9 10 E
1 L 8 9 9 9 9 9 10 E
(a)

1 24L 6 99 9
2 2 4L88 9
2 2 3 3L 8 99
2 2 3 3L 8 88

(b)
Figure 7: (a) Resolution Freeman direction sequences AP. (b) corresponding characters
representation.

acter). offline character recognition, several image distortions however used (Cano,
Prez-Cortes, Arlandis, & Llobet, 2002): slanting, shrinking, ink erosion ink dilatation.
online character recognition, several online distortions used, speed variation
angular variation (Mouchre, Anquetil, & Ragot, 2007).
therefore interested quick tuning handwritten character recognition new
user, consider small set examples character (typically 2 3)
required new user. learn writer-dependent system, synthetic data
keep handwriting style original data.
5.2.2 NALOGY BASED G ENERATION
second experiment, interested handwritten characters, captured online.
represented sequence letters , = {1, 2, ..., 16, 0, C, ..., N } alphabet Freeman symbols code augmented symbols anchorage points. anchorage
points come analysis stable handwriting properties, defined (Mouchre et al.,
2007): pen-up/down, y-extrema, angular points in-loop y-extrema.
learning set contains examples letter, generate synthetic examples
analogical proportion described section 3.6 (see Figure 7). Hence, generating artificial
examples letter f analogical proportion using three instances augment learning
set new different examples shown following pictures.

| {z }

Original letters

=

|

{z

}

Analogy based generated letters

5.2.3 E XPERIMENTS
section show generation strategies improves recognition rate three classical
classifiers learned data.
Experimental Protocol data base use (Mouchre et al., 2007), twelve different
writers written 40 times 26 lowercase letters (1040 characters) PDA. use 4-fold
818

fiA NALOGICAL ISSIMILARITY

stratified cross validation. experiments composed two phases three writerdependent recognition systems learned: Radial Basis Function Network (RBFN), K-Nearest
Neighbor (K-NN) one-against-all Support Vector Machine (SVM).
Firstly, compute two Reference recognition Rates without data generation: RR10
recognition rate achievable 10 original characters without character generation RR30
gives idea achievable recognition rates original data. Practically speaking,
context fly learning phase ask user input 10 characters per
class.
Secondly artificial character generation strategies tested. given writer, one ten
characters per class randomly chosen. 300 synthetic characters per class generated
make synthetic learning database. experiment done 3 times per cross validation split
per writer (12 times per user). mean standard deviation 12 performance rates
computed. Finally means measurements computed give writer dependent
mean recognition rate associated standard deviation.
study three different strategies generation synthetic learning databases. strategy Image Distortions chooses randomly generation one among several image distortions. way strategy Online Image Distortions chooses randomly one distortion
among image distortions online distortions. Analogy Distortions strategy generates two-thirds base previous strategy remaining third AP generation.
Results Figure 8 compares recognition rates achieved three generation strategies
three classifiers. Firstly note global behavior three classifiers.
Thus following conclusions depend classifier type. Secondly three generation
strategies complementary using Online Image Distortions better Image
Distortions alone Analogy Distortions better using distortions. Furthermore using four original character complete generation strategy better RR10.
RR30 achieved using 9 10 original characters. Thus conclude using generation strategies learns classifier original data efficiently using original data
long input phase : need three times fewer original data achieve recognition
rate.
Comparing Image Distortions, Online Distortions Analogy alone shows Analogy
less efficient ad-hoc methods. Nevertheless, generating sequences approximate analogical proportion meaningful somewhat independant classical distorsions. words,
analogy character shapes, used natural intelligence, somehow captured
definition algorithms.
aim know difference average three methods significant.
performed two methods validation evaluate difference two stategies. first
method parametric: T-T EST (Gillick & Cox, 1989). second method non-parametric:
IGN EST (Hull, 1993). methods, comparaison first second
strategy second third strategy number original characters.
T-T EST compares value difference two generation methods regarding
variation differences. assumption errors normal distribution
errors independent. mean difference large comparing standard
deviation, two strategies statistically different. case, probability results
random artefact less 1012 .
819

fiM ICLET, BAYOUDH & ELHAY

IGN EST non-parametric comparison method. benefit avoid assumptions
normal distribution observations errors. test replaces difference
sign difference. sum occurrences compared value hypothesis H0
(H0 : difference methods significant). Thus strategy frequently better
expected mean, strategy significantly better. case, probability
hypothesis H0 true less 1030 . Hence, difference significantly better.
96

RR30

95

RR30

94

Recognition rate (%)

Recognition rate (%)

90

RR10

92

RBFN

85

RR10

KNN

90

80

88

Image Distortions

Image Distortions
Online Image Distortions
Distortions Analogy

Online Image Distortions

75

86

Distortions Analogy
2

3

4

5

6

7

Number original characters

8

9

10

2

3

4

5

6

7

Number original characters

8

9

10

RR30

Recognition rate (%)

96

94

RR10
92

SVM

90

88

Image Distortions
Online Image Distortions

86

Distortions Analogy
2

3

4

5

6

7

8

9

10

Number original characters

Figure 8: Writer-dependent recognition rates (mean standard deviation) depending number used original characters compared reference rates using 10 30 characters per
class RBFN, KNN SVM classifiers.

6. Conclusions Future Work
article, investigated formal notion analogy four objects
universe. given definitions analogy, formulas algorithms solving analogical
equations particular sets. given special focus objects structured sequences,
original definition analogy based optimal alignments. also introduced,
coherent manner, new notion analogical dissimilarity, quantifies far four objects
analogy. notion useful lazy supervised learning: shown
time consuming brute force algorithm could ameliorated generalizing fast nearest
neighbor search algorithm, given preliminary experiments. However, much left
done, want especially explore following questions:
820

fiA NALOGICAL ISSIMILARITY

sort data particularly suited lazy learning analogy? know
bibliography linguistic data successfully processed learning analogy
techniques, fields grapheme phoneme transcription, morphology, translation.
currently working experiments phoneme grapheme transcription,
useful special cases speech recognition (for proper names, example).
also interested sequential real data, biosequences, analogical
reasoning technique (rather unformally) presently already used. selection data
supervision equally important, since search less dissemblant
analogic triple labelling process based concept analogy.
sort structured data processed? Sequences naturally extended ordered trees, several generalizations alignments already defined.
could useful, example, extending nearest neighbor technique learning prosodic
trees speech synthesis (Blin & Miclet, 2000). could also imagine sequences models, like Hidden Markov Models (HMM) could combined analogical construction.
sort algorithms devised let large amount data processed
techniques? given first answer FADANA algorithm, believe
quality results still increased. experiments remain done
type algorithm. notice also properties analogical dissimilarity
used far. believe algorithm precomputing storage
O(m) devised, currently working it.
conclusion, confident fact new notion analogical dissimilarity
lazy learning technique associated extended real data,
structures data larger problems.

Acknowledgments
authors would like thank anonymous referrees constructive detailed comments first version article.

References
Aamodt, A., & Plaza, E. (1994). Case-based reasoning: Foundational issues, methodological variations, system approaches. Artificial Intelligence Communications, 7(1), 3959.
Basu, M., Bunke, H., & Del Bimbo, A. (Eds.). (2005). Syntactic Structural Pattern Recognition,
Vol. 27 Special Section IEEE Trans. Pattern Analysis Machine Intelligence. IEEE
Computer Society.
Bayoudh, S., Miclet, L., & Delhay, A. (2007a). Learning analogy : classification rule
binary nominal data. Veloso, M. M. (Ed.), International Joint Conference Artificial
Intelligence, Vol. 20, pp. 678683. AAAI Press.
Bayoudh, S., Mouchre, H., Miclet, L., & Anquetil, E. (2007b). Learning classifier
examples: analogy based knowledge based generation new examples character
821

fiM ICLET, BAYOUDH & ELHAY

recognition.. European Conference Machine Learning, Vol. 18. Springer Verlag LNAI
4701.
Bishop, C. (2007). Pattern Recognition Machine Learning. Springer.
Blin, L., & Miclet, L. (2000). Generating synthetic speech prosody lazy learning tree structures. Proceedings CoNLL-2000 : 4th Conference Computational Natural Language
Learning, pp. 8790, Lisboa, Portugal.
Bunke, H., & Caelli, T. (Eds.). (2004). Graph Matching Pattern Recognition Machine Vision, Special Issue International Journal Pattern Recognition Artificial Intelligence.
World Scientific.
Cano, J., Prez-Cortes, J., Arlandis, J., & Llobet, R. (2002). Training set expansion handwritten
character recognition.. 9th Int. Workshop Structural Syntactic Pattern Recognition,
pp. 548556.
Chvez, E., Navarro, G., Baeza-Yates, R., & Marroqun, J.-L. (2001). Searching metric spaces.
ACM Comput. Surv., 33(3), 273321.
Cornujols, A., & Miclet, L. (2002). Apprentissage artificiel : concepts et algorithmes. Eyrolles,
Paris.
Daelemans, W. (1996). Abstraction considered harmful: lazy learning language processing.
den Herik, H. J. V., & Weijters, A. (Eds.), Proceedings sixth Belgian-Dutch Conference
Machine Learning, pp. 312, Maastricht, Nederlands.
Dastani, M., Indurkhya, B., & Scha, R. (2003). Analogical projection pattern perception. Journal
Experimental Theoretical Artificial Intelligence, 15(4).
Delhay, A., & Miclet, L. (2004). Analogical equations sequences : Definition resolution..
International Colloquium Grammatical Induction, pp. 127138, Athens, Greece.
Dress, A. W. M., Fllen, G., & Perrey, S. (1995). divide conquer approach multiple
alignment. ISMB, pp. 107113.
Edgar, R. (2004). Muscle: multiple sequence alignment method reduced time space
complexity. BMC Bioinformatics, 5(1), 113.
Falkenhainer, B., Forbus, K., & Gentner, D. (1989). structure-mapping engine: Algorithm
examples. Artificial Intelligence, 41, 163.
Gentner, D., Holyoak, K. J., & Kokinov, B. (2001). analogical mind: Perspectives cognitive science. MIT Press.
Gillick, L., & Cox, S. (1989). statistical issues comparison speech recognition
algorithms.. IEEE Conference Acoustics, Speech Signal Processing, pp. 532535,
Glasgow, UK.
Hofstadter, D., & Fluid Analogies Research Group (1994). Fluid Concepts Creative Analogies. Basic Books, New York.
Holyoak, K. (2005). Analogy. Cambridge Handbook Thinking Reasoning, chap. 6.
Cambridge University Press.
Hull, D. (1993). Using statistical testing evaluation retrieval experiments. Research
Development Information Retrieval, pp. 329338.
822

fiA NALOGICAL ISSIMILARITY

Itkonen, E., & Haukioja, J. (1997). rehabilitation analogy syntax (and elsewhere), pp. 131
177. Peter Lang.
Lepage, Y. (2001). Apparatus method producing analogically similar word based pseudodistances words..
Lepage, Y. (2003). De lanalogie rendant compte de la commutation en linguistique. Universit
Joseph Fourier, Grenoble. Habilitation diriger les recherches.
Mic, L., Oncina, J., & Vidal, E. (1994). new version nearest-neighbour approximating
eliminating search algorithm aesa linear preprocessing-time memory requirements.
Pattern Recognition Letters, 15, 917.
Mitchell, M. (1993). Analogy-Making Perception. MIT Press.
Mitchell, T. (1997). Machine Learning. McGraw-Hill.
Mouchre, H., Anquetil, E., & Ragot, N. (2007). Writer style adaptation on-line handwriting
recognizers fuzzy mechanism approach: adapt method. Int. Journal Pattern
Recognition Artificial Intelligence, 21(1), 99116.
Needleman, S. B., & Wunsch, C. D. (1970). general method applicable search similarities amino acid sequence two proteins.. J Mol Biol, 48(3), 443453.
Newman, D., Hettich, S., Blake, C., & Merz, C. (1998). UCI repository machine learning
databases..
Pirrelli, V., & Yvon, F. (1999). Analogy lexicon: probe analogy-based machine learning
language. Proceedings 6th International Symposium Human Communication,
Santiago de Cuba, Cuba.
Schmid, U., Gust, H., Khnberger, K.-U., & Burghardt, J. (2003). algebraic framework
solving proportional predictive analogies. F. Schmalhofer, R. Y., & Katz, G. (Eds.),
Proceedings European Conference Cognitive Science (EuroCogSci 2003), pp. 295
300, Osnabrck, Germany. Lawrence Erlbaum.
Smith, T. F., & Waterman, M. S. (1981). Identification common molecular subsequences. Journal
Molecular Biology, 147, 195197.
Stroppa, N., & Yvon, F. (2004). Analogie dans les squences : un solveur tats finis. TALN
2004.
Thompson, J. D., Higgins, D. G., & Gibson, T. J. (1994). Improved sensitivity profile searches
use sequence weights gap excision. Computer Applications Biosciences, 10(1), 1929.
Turney, P. D. (2005). Measuring semantic similarity latent relational analysis. Proceedings
Nineteenth International Joint Conference Artificial Intelligence (IJCAI-05), 05, 1136.
Wang, L., & Jiang, T. (1994). complexity multiple sequence alignment. Journal
Computational Biology, 1(4), 337348.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical machine learning tools techniques,
2nd Edition. Morgan Kaufmann Publishers.
823

fiM ICLET, BAYOUDH & ELHAY

Yvon, F. (1997). Paradigmatic cascades: linguistically sound model pronunciation analogy.
Proceedings 35th annual meeting Association Computational Linguistics
(ACL), Madrid, Spain.
Yvon, F. (1999). Pronouncing unknown words using multi-dimensional analogies. Proceeding
European conference Speech Application Technology (Eurospeech), Vol. 1, pp.
199202, Budapest, Hungary.
Yvon, F., Stroppa, N., Delhay, A., & Miclet, L. (2004). Solving analogical equations words.
Tech. rep. ENST2004D005, cole Nationale Suprieure des Tlcommunications.

824

fiJournal Artificial Intelligence Research 32 (2008)

Submitted 11/07; published 07/08

M-DPOP: Faithful Distributed Implementation
Efficient Social Choice Problems
Adrian Petcu
Boi Faltings

ADRIAN . PETCU @ EPFL . CH
BOI . FALTINGS @ EPFL . CH

Artificial Intelligence Lab, Ecole Polytechnique Federale de Lausanne,
Station 14, 1015 Lausanne, Switzerland

David C. Parkes

PARKES @ EECS . HARVARD . EDU

School Engineering Applied Sciences, Harvard University
33 Oxford Street, Cambridge, 02138 USA

Abstract
efficient social choice problem, goal assign values, subject side constraints,
set variables maximize total utility across population agents, agent
private information utility function. paper model social choice problem
distributed constraint optimization problem (DCOP), agent communicate
agents share interest one variables. Whereas existing DCOP algorithms
easily manipulated agent, either misreporting private information deviating
algorithm, introduce M-DPOP, first DCOP algorithm provides faithful distributed
implementation efficient social choice. provides concrete example methods
mechanism design unified distributed optimization. Faithfulness ensures
agent benefit unilaterally deviating aspect protocol, neither informationrevelation, computation, communication, whatever private information agents.
allow payments agents central bank, central authority
require. achieve faithfulness, carefully integrate Vickrey-Clarke-Groves (VCG) mechanism DPOP algorithm, agent asked perform computation, report
information, send messages best interest. Determining agent payment
requires solving social choice problem without agent i. Here, present method reuse
computation performed solving main problem way robust manipulation
excluded agent. Experimental results structured problems show much 87%
computation required solving marginal problems avoided re-use, providing
good scalability number agents. unstructured problems, observe sensitivity
M-DPOP density problem, show reusability decreases almost
100% sparse problems around 20% highly connected problems. close discussion features DCOP enable faithful implementations problem, challenge
reusing computation main problem marginal problems algorithms
ADOPT OptAPO, prospect methods avoid welfare loss occur
transfer payments bank.

1. Introduction
Distributed optimization problems model environments set agents must agree
set decisions subject side constraints. consider settings agent
preferences subsets decisions. agents self interested, one would like
obtain decision maximizes utility. However, system whole agrees (or
social designer determines) solution selected maximize total utility across
c
2008
AI Access Foundation. rights reserved.

fiP ETCU , FALTINGS , & PARKES

agents. Thus, problem efficient social choice. motivation, mind massively
distributed problems meeting scheduling, decisions
hold meeting, allocating airport landing slots airlines, decisions
airline allocated slot, scheduling contractors construction projects.
One approach solve problems central authority computes optimal solution. combination incentive mechanism Vickrey-Clarke-Groves (VCG)
mechanism (Jackson, 2000), also prevent manipulation misreporting preferences. However, many practical settings hard bound problem central
authority feasible. Consider meeting scheduling: agent participates
meetings, general possible find set meetings constraints
meetings thus optimized separately. Similarly, contractors construction
project simultaneously work projects, creating web dependencies hard
optimize centralized fashion. Privacy concerns also favor decentralized solutions (Greenstadt,
Pearce, & Tambe, 2006).
Algorithms distributed constraint reasoning, ABT AWC (Yokoo & Hirayama,
2000), AAS (Silaghi, Sam-Haroud, & Faltings, 2000), DPOP (Petcu & Faltings, 2005b)
ADOPT (Modi, Shen, Tambe, & Yokoo, 2005), deal large problems long influence agent solution limited bounded number variables. However,
current techniques assume cooperative agents, provide robustness misreports
preferences deviations algorithm self-interested agents. major limitation.
recent years, faithful distributed implementation (Parkes & Shneidman, 2004) proposed
framework within achieve synthesis methods (centralized) MD distributed
problem solving. Faithfulness ensures agent benefit unilaterally deviating
aspect protocol, neither information-revelation, computation, communication, whatever private information agents. now, distributed implementation applied
lowest-cost routing (Shneidman & Parkes, 2004; Feigenbaum, Papadimitriou, Sami, & Shenker,
2002), policy-based routing (Feigenbaum, Ramachandran, & Schapira, 2006), Internet,
efficient social choice, problem broad applicability.
paper, make following contributions:
show model problem efficient social choice DCOP, adapt
DPOP algorithm exploit local structure distributed model achieve
scalability would possible solving problem centralized problem graph.
provide algorithm whose first stage faithfully generate DCOP representation
underlying social choice problem. DCOP representation generated,
next stages M-DPOP algorithm also faithful, form ex post Nash equilibrium
induced non-cooperative game.
establishing DCOP models social choice problems solved faithfully,
observe communication information structure problem
agent prevent rest system, aggregate, correctly determining marginal
impact allowing agents (reported) preferences total utility achieved
agents. provides generality techniques DCOP algorithms.
Part achieving faithfulness requires solving DCOP agents (reported) preferences ignored turn, without agent able interfere computational
706

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

process. provide algorithm robustness property, nevertheless able
reuse, possible, intermediate results computation solving main problem
agents.
experimental analysis, structured meeting scheduling problems common
benchmark literature, demonstrate much 87% computation required
solving marginal problems avoided reuse. Results also provided
unstructured resource allocation problems 1 , show M-DPOP sensitive problem
density: loose problems, around 80% computation reused,
decreases highly connected problems.
M-DPOP algorithm defines strategy agent extensive-form game induced
DCOP efficient social choice. particular, M-DPOP algorithm defines messages
agent send, computation agent perform, response messages
received agents. proving M-DPOP forms game-theoretic equilibrium, show
agent benefit unilaterally deviating, whatever utility functions agents
whatever constraints. Although robust dominant strategy equilibrium, (ex
post) equilibrium requires every agent follow algorithm, Parkes Shneidman (2004)
earlier commented appears necessary cost decentralization.
total payment made agent bank always non-negative M-DPOP never
runs deficit (i.e. bank always receives non-negative net payment agents).
settings, transfer utility bank undesirable would best avoided. provide
statistics problem domains studied show loss represent much
35% total utility achieved solution problems studied. payments
cannot naively redistributed back agents without breaking faithfulness, extant work redistribution mechanisms VCG payments suggests mitigated (Guo & Conitzer, 2007;
Faltings, 2004; Cavallo, 2006; Moulin, 2007; Bailey, 1997). defer extension M-DPOP,
details surprisingly involved interesting right, future work.
reuse computation, solving marginal problems agent removed turn,
especially important settings distributed optimization motivating scenarios
problem size massive, perhaps spanning multiple organizations encompassing
thousands decisions. example, consider project scheduling, inter-firm logistics, intra-firm
meeting scheduling, etc. appropriate problem structure, DCOP algorithms problems
scale linearly size problem. instance, DPOP able solve problems
single back-and-forth traversal problem graph. without re-use additional
cost solving marginal problem would make computational cost quadratic rather
linear number agents, could untenable massive-scale applications.
rest paper organized follows: preliminaries (Section 2), Section 3
describe DPOP (Petcu & Faltings, 2005b) algorithm distributed constraint optimization,
focus study. Section 4 introduces model self-interested agents defines
(centralized) VCG mechanism. Section 4.4 provides simple method, Simple M-DPOP make
DPOP faithful serves illustrate excellent fit information communication
structure DCOPs faithful VCG mechanisms. Section 5 describe main algorithm, MDPOP, computation re-used solving marginal problems agent removed
1. consider distributed combinatorial auctions, instances randomly generated using distribution CATS
problem suite (Leyton-Brown & Shoham, 2006).

707

fiP ETCU , FALTINGS , & PARKES

turn. present experimental results Section 6. Section 7 discuss adapting DCOP
algorithms social choice (ADOPT OptAPO, see Section 7.2), waste due
payments Section 7.3. conclude Section 8.
1.1 Related Work
work draws two research areas: distributed algorithms constraint satisfaction optimization, mechanism design coordinated decision making multi-agent systems
self-interested agents. briefly overview relevant results areas.
1.1.1 C ONSTRAINT ATISFACTION



PTIMIZATION

Constraint satisfaction optimization powerful paradigms model wide range
tasks like scheduling, planning, optimal process control, etc. Traditionally, problems
gathered single place, centralized algorithm applied find solution. However,
social choice problems naturally distributed, often preclude use centralized entity
gather information compute solutions.
Distributed Constraint Satisfaction (DisCSP) (Yokoo, Durfee, Ishida, & Kuwabara, 1992;
Sycara, Roth, Sadeh-Koniecpol, & Fox, 1991; Collin, Dechter, & Katz, 1991, 1999; Solotorevsky,
Gudes, & Meisels, 1996) Distributed Constraint Optimization (DCOP) (Modi et al., 2005;
Zhang & Wittenburg, 2003; Petcu & Faltings, 2005b; Gershman, Meisels, & Zivan, 2006) formalisms introduced enable distributed solutions. agents involved problems
must communicate find solution overall problem (unknown one
them). Briefly, problems consist individual subproblems (each agent holds subproblem), connected (some of) peers subproblems via constraints limit
individual agent do. goal find feasible solutions overall problem (in
case DisCSP), optimal ones case DCOP.
Many distributed algorithms DCOP introduced, none deals selfinterested agents. well known ones ADOPT, DPOP OptAPO:
ADOPT (Modi et al., 2005) backtracking based, bound propagation algorithm. ADOPT
completely decentralized message passing asynchronous. ADOPT
advantage requiring linear memory, linear-size messages, applicability large
problems 2 questionable due fact produces number messages
exponential depth DFS tree chosen.
OptAPO (Mailler & Lesser, 2005) centralized-distributed hybrid uses mediator nodes
centralize subproblems solve dynamic asynchronous mediation sessions.
authors show message complexity significantly smaller ADOPTs. However, designed cooperative settings, settings self-interested agents like
social choice problem, unclear whether agents would agree revealing constraints
utility functions (possibly many) agents, solve partially
centralized subproblems.
DPOP (Petcu & Faltings, 2005b) complete algorithm based dynamic programming
generates linear number messages. DPOP, size messages depends
2. largest ADOPT experiments aware comprise problems around 20 agents 40 variables.

708

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

structure problem: largest message exponential induced width
problem (see Section 3.1.4) ADOPT, DPOP maintains full distribution
problem. features suggest DPOP good foundation efficient distributed
implementation VCG-based mechanism social choice problems.
discussion features algorithms applicability social choice
problems provided Section 7. paper, focus DPOP provide appropriate
modifications payments effective environments self-interested agents.
Section 7.2 also provide brief discussion opportunites challenges
applying methodology ADOPT OptAPO.
1.1.2 ECHANISM ESIGN



ISTRIBUTED MPLEMENTATION

long tradition using centralized incentive mechanisms within Distributed AI, going
back least work Ephrati Rosenschein (1991) considered use VCG
mechanism compute joint plans; see also work Sandholm (1996) Parkes et al. (2001)
recent discussions. Also noteworthy work Rosenschein Zlotkin (1994, 1996)
rules encounter, provided non-VCG based approaches task allocation systems
two agents.
hand, known methods distributed problem solving
presence self-interested agents. example, RACO N ET (Sandholm, 1993) improved
upon C ONTRACT N ET system (Davis & Smith, 1983) negotiation-based, distributed task reallocation, providing better economic realism, RACO N ET nevertheless studied simple,
myopically-rational agent behaviors performance game-theoretic agents never analyzed; remains true recent works (Endriss, Maudet, Sadri, & Toni, 2006; Dunne,
Wooldridge, & Laurence, 2005; Dunne, 2005). Similarly, Wellmans work market-oriented programming (Wellman, 1993, 1996) considers role virtual markets support optimal
resource allocation, developed model price-taking agents (i.e. agents treat
current prices though final), rather game-theoretic agents.
first step providing satisfactory synthesis distributed algorithms MD
provided agenda distributed algorithmic mechanism design (DAMD), due work
Feigenbaum colleagues (Feigenbaum et al., 2002; Feigenbaum & Shenker, 2002).
authors (FPSS) provided efficient algorithm lowest-cost interdomain routing Internet,
terminating optimal routes payments VCG mechanism. up-shot
agents case autonomous systems running network domains could benefit misreporting information transit costs. missing analysis consideration
robustness algorithm manipulation. Distributed implementation (Parkes
& Shneidman, 2004) introduces additional requirement. algorithm faithful agent
cannot benefit deviating required actions, including information-revelation, computation message passing. number principles achieving faithfulness ex post Nash
equilibrium provided Parkes Shneidman (2004). careful incentive design small
amount cryptography able remove remaining opportunities manipulation
lowest-cost routing algorithm FPSS. Building this, Feigenbaum et al. (2006) recently provide faithful method policy-based interdomain routing, better capturing typical business
agreements Internet domains.
709

fiP ETCU , FALTINGS , & PARKES

first work achieve faithfulness general DCOP algorithms, demonstrated
via application efficient social choice. work, Monderer Tennenholtz (1999) consider distributed single item allocation problem, focus (faithful) communication
provide distributed computation. Izmalkov et al. (2005) adopt cryptographic primitives
ballot boxes show convert centralized mechanisms DI fully connected
communication graph. interest demonstrating theoretical possibility ideal mechanism design without trusted center. work different focus: seek computational
tractability, require fully connected communication graphs, make appeal cryptographic primitives. hand, content retain desired behavior equilibrium
(remaining consistent MD literature) Izmalkov et al. avoid introduction
additional equilibria beyond exist centralized mechanism.
briefly mention two related topics. note well established literature iterative
VCG mechaisms (Mishra & Parkes, 2007; Ausubel, Cramton, & Milgrom, 2006; Bikhchandani,
de Vries, Schummer, & Vohra, 2002). provide partially distributed implementation
combinatorial allocation problems, center typically issuing demand queries agents
via prices, prices triggering computation part agents generating demand set
response. auctions often interpreted decentralized primal-dual algorithms (Parkes
& Ungar, 2000; de Vries & Vohra, 2003). setting differs remains center
performs computation, solving winner determination problem round, agent
communicates directly center peer-to-peer. Mualem (2005) initiates orthogonal
direction within computer science related topic Nash implementation (Jackson, 2001)
economics, approach relies information part private part common knowledge,
one agent entirely private information preferences.

2. Preliminaries: Modeling Social Choice
assume social choice problem consists finite possibly large number decisions
made time. decision modeled variable take
values discrete finite domain. agent private information variables
places relations. relation associated agent defines utility agent
possible assignment values variables domain relation. may also
hard constraints restrict space feasible joint assignments subsets variables.
Definition 1 (Social Choice Problem - SCP) efficient social choice problem modeled
tuple < A, X , D, C, R > that:
X = {X1 , ..., Xm } set public decision variables (e.g. hold
meetings, resources allocated, etc);
= {d1 , ..., dm } set finite public domains variables X (e.g. list possible
time slots venues, list agents eligible receive resource, etc);
C = {c1 , ..., cq } set public constraints specify feasible combinations values
variables involved. constraint cj function cj : dj1 .. djk {, 0}
returns 0 allowed combinations values involved variables,
disallowed ones. denote scope(cj ) set variables associated constraint cj ;
710

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

= {A1 , ..., } set self-interested agents involved optimization problem;
X(Ai ) X (privately known) 3 set variables agent Ai interested
relations.
R = {R1 , ..., Rn } set private relations, Ri set relations specified
agent Ai relation rij Ri function rij : dj1 .. djk R specified agent
Ai , denotes utility Ai receives possible values involved variables
{j1 , . . . , jk } (negative values mean costs). denote scope(rij ) domain variables
rij defined on.
private relations agent may, themselves, induced solution local optimization problems additional, private decision variables additional, private constraints.
kept local agent part SCP definition.
optimal solution SCP complete instantiation X variables X , s.t.
X
X
Ri (X) +
cj (X),
(1)
X arg max
XD

i{1,..,n}

cj C

rj (X) agent Ai total utility assignment X. natural
Ri (X) =
rij Ri
problem social choice: goal find solution maximizes total utility agents,
respecting hard constraints; notice second sum X infeasible precludes
outcome. assume throughout feasible solution. introducing VCG
mechanism require solution SCP influence agents relations
removed turn. this, let SCP (A) denote main problem
Eq. (1)
Pand SCP (Ai ) denote
P
marginal problem without agent Ai , i.e. maxXD j6=i Rj (X) + cj C cj (X). Note
decision variables remain. difference SCP (A) SCP (Ai )
preferences agent Ai ignored solving SCP (Ai ).
variable Xj , refer agents Ai Xj X(Ai ) forming community
Xj . choose emphasize following assumptions:
P

agent knows variables interested, together domain
variable hard constraints involve variable.
decision variable supported community mechanism allows interested
agents report interest learn other. example, mechanism
implemented using bulletin board.
constraint cj C, every agent Ak community Xl scope(cj ), i.e.
Xl X(Ak ), read membership lists communities Xm scope(cj )
Xm 6= Xl . words, every agent involved hard constraint knows
agents involved hard constraint.
agent communicate directly agents communities
member, agents involved shared hard constraints.
communication agents required.
3. Note private knowledge variables interest requirement; algorithms present work
public private knowledge variables interest. required agents interested
variable know - see assumptions below.

711

fiP ETCU , FALTINGS , & PARKES

Figure 1: operator placement problem: (a) centralized model (each variable server load possible
values feasible combinations services run server , edges correspond
relations represent agent preferences). (b) decentralized (DCOP) model replicated
variables. agent local replica variables interest inter-agent edges denote
equality constraints ensure agreement. preferences modeled relations hyperedges local respective agents.

Section 4 establish step identifying SCP, via community mechanism, faithful self-interested agents choose volunteer communities
member (and communities.)
2.1 Modeling Social Choice Constraint Optimization
first introduce centralized, constraint optimization problem (COP) model efficient social choice problem. model represented centralized problem graph. Given this,
model distributed constraint optimization problem (DCOP), along associated
distributed problem graph. distributed problem graph makes explicit control structure
distributed algorithm ultimately used multi-agent system solve problem.
sections illustrated reference overlay network optimization problem (Huebsch,
Hellerstein, Lanham, et al., 2003; Faltings, Parkes, Petcu, & Shneidman, 2006; Pietzuch, Ledlie,
Shneidman, Roussopoulos, Welsh, & Seltzer, 2006):
OVERLAY N ETWORK PTIMIZATION Consider problem optimal placement data aggregation processing operators overlay network large-scale sensor network (Huebsch
et al., 2003; Pietzuch et al., 2006). application, multiple users multiple servers.
user associated query client machine located particular node
overlay network. query associated set data producers, known user located
712

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

nodes network. query also requires set data aggregation processing operators, placed server nodes nodes data producers
users node. user assigns utility different assignments operators servers represent preferences different kinds data aggregation. Examples in-network operators
data aggregation include database style join operators; e.g., user may desire volcano data X
earthquake data joined sent them. address this, specific operator call
VolcanoXEarthquakeY Join created put network. Naturally, user prefers
operators placed best servers network, without regard costs incurred, overloading servers, denying service users, etc. problem find optimal
allocation operators servers, subject capacity compatibility constraints.
Faltings et al. (2006) model problem one efficient social choice. distributed algorithm, executed user clients situated network nodes, used determine assignment
data aggregation processing operators server nodes.
2.1.1 C ENTRALIZED COP ODEL ULTI G RAPH
Viewed centralized problem, SCP defined constraint optimization problem
multigraph, i.e. graph several distinct edges connect set nodes. denote
COP (A), provide illustration Figure 1(a). decision variables nodes,
relations defined subsets variables form edges multigraph; hyperedges connect
two vertices case relation involving two variables.
multiple edges involve set variables, edge corresponding
relations distinct agent set variables. hard constraints also represented
edges graph.
Example 1 (Centralized Model Overlay Optimization) example Figure 1(a) contains
3 users Ai 3 servers Sj . simplicity reasons, assume user Ai one single operator oi want executed server. According prerequisites compatibility
issues, assume S1 execute o1 o2 , o3 . Similarly, assume S2 execute
o2 o3 , o1 , S3 execute combination two three operators. Agents preferences operators executed (e.g. proximity
data sources, computational capabilities servers, cost electricity, etc). example, A1
extracts utility 10 o1 executed S1 , utility 5 o1 executed S3 .
model problem optimization problem, use following:
1. variables: server Si , create variable Si denotes set operators Si
execute.
2. values: variable Si take values set possible combinations operators
server execute. example, S1 = {null, o1 , o2 , o1 + o2 }, null means
server executes operator, oi executes operator oi , o1 + o2 executes
o1 o2 .
3. constraints: restrict possible combinations assignments. Example: two servers
execute operator.
4. relations: allow agents express preferences combinations assignments. A1 models
preference placement o1 using relation r10 , defined variables S1
713

fiP ETCU , FALTINGS , & PARKES

S3 . relation associates utility value combination assignments S1
S3 (in total 4 8 = 32 combinations) follows:
0 combinations o1 executed neither S1 , S3 (e.g. hS1 = o2 , S3 =
o3 i)
10 combinations o1 executed S1 (e.g. hS1 = o1 , S3 = o2 + o3 i)
5 combinations o1 executed S3 (e.g. hS1 = o2 , S3 = o1 i)
depict variables nodes graph, constraints relations (hyper)edges (see
Figure 1(a)). problem get arbitrarily complex, multiple operators per agent, groups
servers able execute certain groups compatible operators, etc.
2.1.2 ECENTRALIZED COP (DCOP) ODEL U SING R EPLICATED VARIABLES
useful define alternate graphical representation SCP, centralized problem
graph replaced distributed problem graph. distributed problem graph direct correspondence DPOP algorithm solving DCOPs. denote DCOP (A) problem
agents included, corresponds main social choice problem, SCP (A). Similarly,
DCOP (Ai ) problem agent Ai removed, corresponds SCP (Ai ).
distributed model, agent local replica variables interested.4
public variable, Xv X(Ai ), agent Ai interested, agent local replica, denoted
Xvi . Agent Ai models local problem COP (X(Ai ), Ri ), specifying relations rij Ri
locally replicated variables.
Refer Figure 1(b) translation centralized problem Figure 1(a) DCOP
model. agent local variables loads servers interest itself, i.e.
servers execute one operators (e.g. S12 represents A2 local replica variable
representing server S1 ). Local edges correspond local all-different constraints agents
variables ensure execute operator several servers time. Equality
constraints local replicas value ensure global agreement operators
run servers.
Agents specify relations via local edges local replicas. example, agent A1
relation load servers S1 S3 express preference placement
operator o1 relation r10 , assign e.g. utility 5 S3 executing o1 , utility 10 S1
executing o1 .
begin understand potential manipulation self-interested agents
example. Notice although globally optimal solution may require assigning o1 S3 ,
less preferable A1 , providing utility 5 instead 10. Therefore, absence incentive
mechanism, A1 could benefit simple manipulation: declare utility + hS1 = o1 i, thus
changing final assignment suboptimal one nevertheless better itself.
4. alternate model designates owner agent decision variable. owner agent would centralize
aggregate preferences agents interested variable. Subsequently, owner agents would use
distributed optimization algorithm find optimal solution. model limits reusability computation
main problem solving marginal problems agent removed turn excluding
owner agent variable, one needs assign ownership another agent restart computational process
regards variable connected variables. reuse computation important making M-DPOP
scalable. approach disaggregated facilitates greater reuse.

714

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

neighborhood local copy Xvi variable composed three kinds variables:
Neighbors(Xvi ) = Siblings(Xvi ) Local neighbors(Xvi ) Hard neighbors(Xvi ).

(2)

siblings local copies Xv belong agents Aj 6= Ai also interested Xv :
Siblings(Xvi ) = {Xvj | Aj 6= Ai Xv X(Aj )}

(3)

siblings Xvi connected pairwise equality constraint. ensures
agents eventually consistent value variable. second set variables
local neighbors Xvi local optimization problem Ai . local copies
variables agent Ai interested in, connected Xvi via relations Ai local
problem:
Local neighbors(Xvi ) = {Xui | Xu X(Ai ), rij Ri s.t. Xui scope(ri )}

(4)

must also consider set hard constraints contain scope variable Xv
public variables: Hard (Xv ) = {cs C|Xv scope(cs )}. constraints connect
Xv variables Xu appear scope, may interest
agents well. Consequently, Xvi connected local copies Xtj
variables Xt appear hard constraints:
Hard neighbors(Xvi ) = {Xtj |cs Hard (Xv ) s.t. Xt scope(cs ), Xt X(Aj )}

(5)

general, agent also private variables, relations constraints involve
private variables, link public decision variables. example, consider meeting
scheduling application employees company. Apart work-related meetings
schedule together, one employees also personal items agenda, like appointments doctor, etc. Decisions values private variables information
local relations constraints remain private. provide additional complications
discussed paper.
2.2 Example Social Choice Problems
continuing present main results describe three additional problems social choice
serve motivate work. fact, problem efficient social choice fundamental
microeconomics political science (Mas-Colell, Whinston, & Green, 1995). problem
present large scale distributed, involves actors system businesses
cannot expected cooperate, either revealing preferences following rules
distributed algorithm.
IRPORT LOT LLOCATION . airports become congested, governments turning
market-based approaches allocate landing takeoff slots. instance, U.S. Federal Aviation Administration recently commisioned study use auction allocate slots
New Yorks congested LaGuardia airport (Ball, Donohue, & Hoffman, 2006). problem large
scale expands include airports throughout U.S., eventually World, exhibits
self-interest (airlines profit-maximizing agents private information utilities
715

fiP ETCU , FALTINGS , & PARKES

different slot allocations), one privacy major concern competitiveness airline industry. typical policy goal maximize total utility allocation,
i.e. one efficient social choice. problem motivates study combinatorial auctions
Section 6. combinatorial auction (CA) one set heterogeneous, indivisible goods
allocated agents, values expressed sets goods; e.g., want
9am slot also get 10am slot indifferent 9am 9:05am slot.
airport slot allocation problem motivated first paper CAs (Rassenti, Smith, & Bulfin,
1982), recognized airlines would likely need express utilities sets slots
correspond right fly schedule airport.
PEN -ACCESS W IRELESS N ETWORKS . wireless spectrum today owned operated
closed networks, example cellular companies T-Mobile AT&T. However
plenty debate creating open-access wireless networks bandwidth must available
use phone software.5 recently proposed using auction protocol
allow service providers bid dynamic auction right use spectrum given period
time deliver services.6 Taken logical conclusion, idea anticipated Rosenschein
Zlotkin (1994) wired telephony, suggests secondary market wireless spectrum
corresponds problem efficient social choice: allocate spectrum maximize total utility
consumers. problem large scale, exhibits self-interest, inherently decentralized.
EETING CHEDULING P ROBLEM . Consider large organization dozens departments, spread across dozens sites, employing tens thousands people. Employees
different sites departments want setup thousands meetings week. Due privacy
concerns among different departments, centralized problem solving desirable. Furthermore,
although organization whole desires minimize cost whole process, department employee self interested wishes maximize utility. artificial
currency created purpose weekly assignment made employee. Employees
express preferences meeting schedules units currency.
Refer Figure 2 example problem, 3 agents want setup 3 meetings.
Figure 2(b) shows agent local variables time slots corresponding meetings
participates (e.g. M12 represents A2 local replica variable representing meeting M1 ).
Local edges correspond local all-different constraints agents variables ensure
participate several meetings time. Equality constraints local
replicas value ensure global agreement. Agents specify relations via local edges
local replicas. example, agent A1 relation time meeting M1 express
preference meeting later day relation r10 , assign low utilities morning
time slots high utilities afternoon time slots. Similarly, A2 prefers holding meeting M2
meeting M1 , use local relation r20 assign high utilities satisfactory
combinations timeslots low utility otherwise. example, hM1 = 9AM, M2 = 11AM
gets utility 10, hM1 = 9AM, M2 = 8AM gets utility 2.
5. breakthrough ruling, U.S. Federal Communications Commission (FCC) require open access
around one-third spectrum auctioned early 08.
stopped short mandating spectrum made available wholesale market would service providers.
See
http://www.fcc.gov/073107/700mhz news release 073107.pdf
6. Google proposed auction filing made FCC May 21st, 2007.
See
http://gullfoss2.fcc.gov/prod/ecfs/retrieve.cgi?native pdf=pdf&id document=6519412647.

716

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

Figure 2: meeting scheduling problem. (a) centralized model (each vertex meeting variable, red
edges correspond hard constraints non-overlap meetings share participant (that
agent A2 hyperedge particpates every meeting), blue edges correspond
relations represent agent preferences). (b) decentralized (DCOP) model replicated
variables. agent local replica variables interest inter-agent edges denote
equality constraints ensure agreement. hard constraint non-overlap meetings
M1 , M2 M3 local hyperedge agent A2 . (c) DFS arrangement decentralized
problem graph. Used DPOP algorithm control order problem solving.

experimental results presented Section 6 adopt meeting scheduling prototypical
structured social choice problems problem instances associated organizational
hierarchy. Meeting scheduling introduced Section 2.1. second set experiments
consider combinatorial auctions (CAs), agents bid bundles goods,
consider set problem instances unstructured provide comparison point
meeting scheduling. CAs provide nice abstraction kinds allocation problems exist
airport wireless network domains.

3. Cooperative Case: Efficient Social Choice via DPOP
section, review DPOP (Petcu & Faltings, 2005b), general purpose distributed
optimization algorithm. DPOP (Distributed Pseudotree Optimization Protocol) based dynamic
programming adapts Dechters (Dechter, 2003) general bucket elimination scheme distributed case. main advantage generates linear number messages.
contrast optimization algorithms like ADOPT (Modi et al., 2005) ensures minimal network overhead produced message exchange. hand, concern DPOP
size individual messages since grows exponentially parameter constraint graph
called induced width (see Section 3.1.4). Nevertheless, problems exhibit local structure,
DPOP typically scales much larger problems, orders magnitude efficient,
717

fiP ETCU , FALTINGS , & PARKES

techniques (Petcu & Faltings, 2005b, 2007). simplify exposition, first illustrate
DPOP general DCOP context, show instantiate DPOP social choice problems. particular, explain leverage structure provided local replicas. consider
cooperative agents throughout section.
3.1 DPOP Algorithm DCOPs
section presents DPOP algorithm generic DCOPs. simplify exposition,
assume section agent Ai represents single variable Xi ,
constraint graph given.
DPOP composed three phases:
Phase one constructs DFS arrangement, DFS (A), defines control flow message passing computation DPOP.
Phase two bottom-up utility propagation along tree constructed phase 1.
phase utilities different values variables aggregated reflect optimal decisions
made subtrees rooted node tree.
Phase three top-down value assignment propagation along tree constructed phase
1. phase decisions made based aggregate utility information phase 2.
describing phases refer Figure 3 running example. also introduce
explicit numerical example illustrate phases two three detail.
3.1.1 DPOP P HASE NE : DFS REE G ENERATION
first phase performs depth-first search (DFS) traversal problem graph, thereby constructing DFS arrangement problem graph. DFS arrangement subsequently used
provide control flow DPOP guide variable elimination order. underlying problem graph tree DFS arrangement also tree. general, DFS arrangement
graph define union set tree edges additional back edges, connect
nodes ancestors.7
Definition 2 (DFS arrangement) DFS arrangement graph G defines rooted tree
subset edges (the tree edges) remaining edges included back edges. tree
edges defined adjacent nodes G fall branch tree.
Figure 3 shows example DFS arrangement. tree edges shown solid lines (e.g.
1 3) back edges shown dashed lines (e.g. 12 2, 4 0). Two nodes Xi Xv
said branch DFS arrangement path higher node
lower node along tree edges; e.g., nodes X0 X11 Figure 3. DFS arrangements already
investigated means boost search constraint optimization problems (Freuder & Quinn,
1985; Modi et al., 2005; Dechter & Mateescu, 2006). advantage allow algorithms
exploit relative independence nodes lying different branches DFS arrangement
7. simplicity, assume follows original problem connected. However difficulty
applying DPOP disconnected problems. DFS arrangement becomes DFS forest, agents
connected component simply execute DPOP parallel separate control thread. solution overall
problem union optimal solutions independent subproblem.

718

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

Figure 3: DFS arrangement problem graph. Tree edges shown solid back edges
dashed. DFS arrangement constructed initializing token-passing X0 . k-ary
constraints, C4 , treated cliques.

(i.e. nodes direct descendants ancestors one-another), possible
perform search parallel independent branches combine results.
introduce definitions related DFS arrangements:
Definition 3 (DFS concepts) Given node Xi DFS arrangement, define:
parent Pi / children Ci : Xi ancestor/descendants connected Xi via tree-edges (e.g.
P4 = X1 , C4 = {X9 , X10 }).
pseudo-parents PP : Xi ancestors connected Xi via back-edges (PP 5 = {X0 }).
pseudo-children PC : Xi descendants connected Xi via back-edges (e.g. PC 1 =
{X8 }).
separator Sep Xi : ancestors Xi directly connected Xi descendants Xi (e.g. Sep 3 = {X1 } Sep 11 = {X0 , X2 , X5 }).
tree neighbors TN Xi nodes linked Xi via tree edges, TN = Pi Ci
(e.g. TN 4 = {X1 , X9 , X10 }).
Removing nodes Sep completely disconnects subtree rooted Xi rest
problem. case problem tree, Sep = {Pi }, Xi X . general case, Sep
contains Pi , PP pseudoparents descendants Xi pseudoparents
also ancestors Xi . example, Figure 3, separator node X4 contains parent X1 ,
pseudoparent X0 . necessary sufficient values variables {X0 , X1 }
set problem rooted node X4 independent rest problem. Separators
play important role DPOP contingent solutions must maintained propagating
utility information DFS arrangement different possible assignments separator variables.
Constructing DFS Tree Generating DFS trees distributed manner task
received lot attention, many algorithms available: example Collin
Dolev (1994), Barbosa (1996), Cidon (1988), Cheung (1983) name few. purposes executing DPOP, assume example algorithm Cheung (1983),
briefly outline below. instantiate DPOP SCPs, present adaptation
DFS generation algorithm exploit particulars SCP.
simple DFS construction algorithm starts agents labeling internally neighbors
not-visited. One agents graph designated root, using example leader
719

fiP ETCU , FALTINGS , & PARKES

election algorithm Abu-Amara (1988),8 simply picking agent
lowest ID. root initiates propagation token, unique message
circulated agents graph, thus visiting them. Initially, token contains
ID root. root sends one neighbors, waits return sending
one (still) unvisited neighbors. agent Xi first receives token, marks
sender parent. neighbors Xi contained token marked Xi pseudoparents
(PP ).
this, Xi adds ID token, sends token turn one notvisited neighbors Xj , become children. Every time agent receives token one
neighbors, marks sender visited. token return either Xj (the child
Xi sent first place), another neighbor, Xk . latter case, means
cycle subtree, Xk marked pseudochild.
dead end reached, last agent backtracks sending token back parent.
neighbors marked visited, Xi finished exploring subtree. Xi removes
ID token, sends token back parent; process finished Xi .
root marked neighbors visited, entire DFS construction process over.
Handling Non-binary Constraints. special treatment required construct neighbors
variable correspond k-ary constraints, k > 2. example, Figure 3 (left),
4-ary constraint C4 involving {X0 , X2 , X5 , X11 }. Eq. 2, implies {X0 , X2 , X5 , X11 }
neighbors, DFS construction process appear along branch
tree. produces result Figure 3 (right).
3.1.2 DPOP P HASE WO : UTIL P ROPAGATION (I NFERENCE )
Phase two bottom-to-top pass DFS arrangement utility information aggregated
propagated leaves towards root node parent tree edges
back edges. high level, leaves start computing sending UTIL messages
parents, UTIL message informs parent local utility solutions
rest problem, minimally specified terms local utility different value assignments
separator variables. Subsequently node propagates UTIL message represents
contingent utility subtree rooted node assignments values separator variables.
detail, nodes perform following steps:
1. Wait UTIL messages children, store them.
2. Perform aggregation: join messages children, also relations
parents pseudoparents.
3. Perform optimization: project resulting join picking optimal
values combination values variables join.
4. Send result parent new UTIL message.
8. cases problem initially disconnected, required choose multiple roots, one connected component. standard leader election algorithm, executed agents problem, elect
exactly many leaders connected components.

720

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

UTIL message sent node Xi parent Pi multidimensional matrix informs
Pi much utility, ui (Sep ) subtree rooted Xi receives different assignments values
variables define separator Sep subtree. One variables, definition,
variable managed parent Pi . UTIL message already represents result optimization,
variables local subtree optimized different assignments separator
variables. compute UTIL message node uses two operations: aggregation optimization.
Aggregations apply JOIN operator optimizations apply PROJECT operator described
Petcu Faltings (2005b), briefly summarized here.
Let UTILij UTILkj denote UTIL messages sent nodes Xi Xk parent
node Xj . denote dim(UTILkj ) set dimensions matrix, i.e. set
variables separator sending node Xk . Assuming Xj node receiving messages,
define:
Definition 4 (JOIN operator) operator (join): UTILij UTILkj join two
UTIL matrices. also matrix dim(UTILij ) dim(UTILkj ) dimensions.
value cell join sum corresponding cells two source matrices.
Definition 5 (PROJECT operator) operator (projection): Xj dim(UTILij ),
UTILij Xj projection optimization UTILij matrix along Xj axis:
instantiation variables {dim(UTILij ) \ Xj }, corresponding values
UTILij (one value Xj ) tried, maximal one chosen. result matrix
one less dimension (Xj ).
Notice subtree rooted Xi influenced rest problem Xi
separator variables. Therefore, UTIL message contains optimal utility obtained subtree
instantiation variables Sep separator size plays crucial role bounding
message size.
Example 2 (UTIL propagation) Figure 4 shows simple example UTIL propagation.
problem tree structure (Figure 4(a)), 3 relations r31 , r21 , r10 detailed Figure 4(b).
relations variables (X3 , X1 ), (X2 , X1 ) (X1 , X0 ) respectively.
individual variables local replicas. UTIL phase X2 X3 project r21 r31 , respectively. results highlighed cells r21 r31 Figure 4(b).
instance, optimal value X2 given X1 := assign X2 := c utility
5. projections define UTIL messages send X1 . X1 receives messages X2
X3 , joins together relation X0 (adds utilities messages
corresponding cells r10 ). projects join. instance, optimal value
X1 given X0 := b X1 := 2 + 5 + 6 max{3 + 4 + 4, 3 + 6 + 3}. result
depicted Figure 4(d). UTIL message X0 receives X1 . value
message represents total utility entire problem value X0 . return
example context third phase value propagation.
Non-binary Relations Constraints. binary constraints/relations, k-ary constraint
introduced UTIL propagation once, lowest node DFS arrangement
part scope constraint. example, Figure 3, constraint C4 introduced
UTIL propagation once, X11 , computing message parent, X5 .
721

fiP ETCU , FALTINGS , & PARKES

Figure 4: Numerical example UTIL propagation. (a) simple DCOP problem three
relations r31 , r21 r10 (X3 , X1 ), (X2 , X1 ) (X1 , X0 ) respectively. (b) Projections
X2 X3 relations X1 . results sent X1 UTIL21 , UTIL31
respectively. (c) X1 joins UTIL21 UTIL31 relation X0 . (d) X1 projects
join sends result X0 .

3.1.3 DPOP P HASE HREE : VALUE P ROPAGATION
Phase three top-to-bottom pass assigns values variables, decisions made recursively
root leaves. VALUE propagation phase initiated root agent
X0 received UTIL messages children. Based UTIL messages,
root assigns variable X0 value v maximizes sum utility
communicated subtrees. sends VALUE(X0r v ) message every child.
process continues recursively leaves, agents Xi assigning optimal values
variables. end phase, algorithm finishes, variables assigned
optimal values.
Example 3 (Value propagation) Return example Figure 4. X0 receives UTIL
message node X1 simply choose value X0 produces largest utility
whole problem: X0 = (X0 = X0 = c produce result example, either one
chosen). value-assignment propagation phase X0 informs X1 choice via
message VALUE (X0 a). Node X1 assigns optimal value X1 = c process continues
message V ALU E(X1 c) sent children, X2 X3 . children assign X2 = b
X3 = algorithm terminates optimal solution hX0 = a, X1 = c, X2 = b, X3 = ai
total utility 15.
3.1.4 C OMPLEXITY NALYSIS DPOP
DPOP produces number messages scales linearly size problem graph, i.e.
linearly number nodes edges DCOP model (Petcu & Faltings, 2005b).
complexity DPOP lies size UTIL messages (note tokens passed around
722

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

constructing DFS(A) VALUE messages size linear problem graph). Petcu
Faltings (2005b) show size largest UTIL message exponential parameter
called induced width (Kloks, 1994; Dechter, 2003).
induced width, denoted w, constraint graph given chosen DFS arrangement
structural parameter equals size largest separator node DFS arrangement
(see Definition 3.):
w = max |Sep |.
Xi X

(6)

example Figure 3, induced width graph given particular DFS ordering
w = 3, given Sep11 = {X0 , X2 , X5 }. Intuitively, problem tree-like structure,
lower induced width. particular, problem graph tree induced
width equal 1 DFS arrangement always tree. Problem graphs cliques,
hand, induced width equal number nodes minus 1, irrespective
DFS-tree arrangement.
Proposition 1 (DPOP Complexity) (Petcu & Faltings, 2005b) number messages passed
DPOP 2m, (n 1) (n 1) phases one, two three respectively, n
number nodes edges DCOP model replicated variables. maximal number
utility values computed node DPOP O(Dw+1 ), largest UTIL message
O(Dw ) entries, w induced width DFS ordering used.
case trees, DPOP generates UTIL messages dimension equal domain size
variable defining parent node. case cliques, maximal message size
DPOP exponential n 1. DFS arrangements yield width, desirable
construct DFS arrangements provide low induced width. However, finding tree arrangement
lowest induced width NP-hard optimization problem (Arnborg, 1985). Nevertheless,
good heuristics identified finding tree arrangements low width (Kloks, 1994;
Bayardo & Miranker, 1995; Bidyuk & Dechter, 2004; Petcu & Faltings, 2007, 2005b). Although
designed explored centralized context, (notably max-degree
maximum cardinality set) easily amenable distributed environment.
3.2 DPOP Applied Social Choice Problems
section, instantiate DPOP efficient social choice problems. Specifically, first show
optimization problem constructed agents preferences potential variables
interest. Subsequently, show changes make DPOP adapt SCP domain.
prominent adaptation exploits fact several variables represent local replicas
variable, treated UTIL VALUE phases.
adaptation improves efficiency significantly, allows complexity claims stated terms
induced width centralized COP problem graph rather distributed COP problem
graph (see Section 3.2.5).
3.2.1 NITIALIZATION : C OMMUNITY F ORMATION
initialize algorithm, agent first forms communities around variables interest,
X(Ai ), defines local optimization problem COP (X(Ai ), Ri ) replicated variable Xvi
723

fiP ETCU , FALTINGS , & PARKES

Xv X(Ai ). Shorthand Xvi COP denotes agent Ai local replica variable
Xv . agent owns multiple nodes conceptualize node associated virtual agent operated owning agent. virtual agent responsible
associated variable.
agents subscribe communities interested, learn
agents belong communities. Neighboring relations established local variable
according Eq. 2, follows: agents community Xv connect corresponding local
copies Xv equality constraints. so, local problems COP (X(Ai ), Ri )
connected according interests owning agents. Local relations
COP (X(Ai ), Ri ) connect corresponding local variables. Hard constraints connect local copies
variables involve. Thus, overall problem graph formed.
example, consider Figure 2(b). decision variables start times three
meetings. agent models local optimization problem creating local copies variables
interested expressing preferences local relations. Formally, initialization
process described Algorithm 1.
Algorithm 1: DPOP init: community formation building DCOP (A).
DPOP init(A, X , D, C, R):
1 agent Ai models interests COP (X(Ai ), Ri ): set relations Ri imposed
set X(Ai ) variables Xvi replicate public variable Xv X(Ai )
2 agent Ai subscribes communities Xv X(Ai )
3 agent Ai connects local copies Xvi X(Ai ) corresponding local copies
agents via equality constraints

3.2.2 DFS RAVERSAL
method DFS traversal described Algorithm 2. algorithm starts choosing one
variables, X0 , root. done randomly, example using distributed algorithm
random number generation, leader election algorithm like Ostrovski (1994), simply
picking variable lowest ID. agents involved community X0 randomly
choose one them, Ar leader. local copy X0r variable X0 becomes root
DFS. Making assumption virtual agents act behalf variable problem,
functioning token passing mechanism similar described Section 3.1.1,
additional consideration given community structure. root chosen, agents
participate distributed depth-first traversal problem graph. convenience, describe
DFS process token-passing algorithm members within community observe
release pick token agents. neighbors node sorted (in
line 7) prioritize copies variables held agents, local variables,
finally variables linked hard constraints.
Example 4 Consider meeting scheduling example Figure 2. Assume M3 chosen
start community A2 chosen within community leader. A2 creates empty
token DFS = adds M32 ID token (DFS = {M32 }). Eq. 2, Neighbors(M32 ) =
{M33 , M31 , M12 , M22 }. A2 sends token DFS = {M32 } first unvisited neighbor
724

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

Algorithm 2: DPOP Phase One: DFS construction.
Inputs: Ai knows COP , Neighbors(Xvi ), Xvi COP
Outputs: Ai knows P (Xvi ), PP (Xvi ), C(Xvi ), PC (Xvi ), Xvi COP .
1
2
3

4
5
6
7

8
9
10

Procedure Initialization
agents choose one variables, X0 , root.
Agents X0 community elect leader, Ar .
Ar initiates token passing X0r construct DFS
Procedure Token Passing (performed virtual agent Xvi COP )
Xvi root P (Xvi ) = null; create empty token DFS :=
else DFS :=Handle incoming tokens()
Let DFS := DFS {Xvi }
Sort Neighbors(Xvi ) Siblings(Xvi ), Local neighbors(Xvi ),
Hard neighbors(Xvi ). Set C(Xvi ) := null.
forall Xl Neighbors(Xvi ) s.t. Xl visited yet
C(Xvi ) := C(Xvi ) Xl . Send DFS Xl wait DFS token return.
Send DFS token back P (Xvi ).
Procedure Handle incoming tokens() //run virtual agent Xvi COP

11
12
13
14

15

Wait incoming DFS message; let Xl sender
Mark Xl visited.
first DFS message (i.e. Xl parent)
P (Xvi ) := Xl ; PP (Xvi ) := {Xk 6= Xl |Xk Neighbors(Xvi ) DFS }; PP (Xvi ) :=
else
Xl
/ C(Xvi ) (i.e. DFS coming pseudochild)
PC (Xvi ) := PC (Xvi ) Xl

list, i.e. M33 , belongs A3 . A3 receives token adds copy M3 (now DFS =
{M32 , M33 }). A3 sends token M33 first unvisited neighbor, M31 (which belongs A1 ).
Agent A1 receives token adds copy M3 (now DFS = {M32 , M33 , M31 }).
M31 neighbor list Neighbors(M31 ) = {M32 , M33 , M11 }. Since token A1 received
already contains M32 M33 , means already visited. Thus, next variable
visit M11 , happens variable also belongs A1 . token passed
M11 internally (no message exchange required), M11 added token (now DFS =
{M32 , M33 , M31 , M11 }).
process continues, exploring sibling variables community turn, passing another community, on. Eventually replicas variable arranged
chain equality constraints (back-edges) predecessors replicas
variable. dead end reached, last agent backtracks sending token back
parent. example, happens A3 receives token A2 M2 community.
Then, A3 sends back token A2 on. Eventually, token returns path
way root process completes.
725

fiP ETCU , FALTINGS , & PARKES

3.2.3 H ANDLING



P UBLIC H ARD C ONSTRAINTS .

Social choice problems, defined Definition 1 contain side constraints, form publicly known hard constraints, represent domain knowledge resource allocated
once, hotel accomodate 100 people, person one meeting
time. etc. constraints owned agent, available agents
interested variable involved domain constraint. Handling constraints
essentially unchanged handling non-binary constraints standard DPOP, described
Section 3.1.1 DFS construction phase, Section 3.1.2 UTIL phase. Specifically:
DFS Construction: Neighboring relationships defined Eq. 2 require local variable
local copies share hard constraint considered neighbors.
prioritization line 7 Algorithm 2 (for DFS construction), DFS traversal mostly made
according structure defined relations agents hard constraints
appear backedges DFS arrangement problem graph.
UTIL Propagation: Hard constraints introduced UTIL propagation phase lowest
agent community variable scope hard constraint, i.e. agent
variable lowest DFS ordering. example, constraint M2
M3 Figure 2 specify M2 occur M3 becomes backedge
2 communities would assigned A3 handling.
3.2.4 H ANDLING R EPLICA VARIABLES
distributed model SCP replicates decision variable every interested agent connects copies equality constraints. handling replica variables carefully
avoid increasing induced width k DCOP model compared induced width
w centralized model. adaptation, UTIL messages DPOP distributed problem graph would conditioned many variables local copies
original variable. However, local copies represent variable must assigned
value; thus, sending many combinations different local copies variable take
different values wasteful. Therefore, handle multiple replicas variable UTIL
propagation though single, original variable, condition relations one
value. realized updating JOIN operator follows:
Definition 6 (Updated JOIN operator SCP) Defined two steps:
Step 1: Consider UTIL messages received input. one, consider variable
Xvi message conditioned, also local copy original variable Xv .
Rename Xvi input UTIL message Xv , i.e. corresponding name original
problem.
Step 2: Apply normal JOIN operator DPOP.
Applying updated JOIN operator makes local copies variable become indistinguishable other, merges single dimension UTIL message
avoids exponential blow-up.
Example 5 Consider meeting scheduling example Figure 2. centralized model Figure 2(a) DFS arrangement yields induced width 2 clique 3 nodes.
726

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

Nevertheless, corresponding DCOP model Figure 2(b) induced width 3, seen
DFS arrangement Figure 2(c), Sep M22 = {M32 , M33 , M12 }. Applying DPOP
DFS arrangement, M22 would condition UTIL message UTILM22 M12 variables
separator: {M32 , M33 , M12 }. However, M32 M33 represent variable, M3 . Therefore, M22 apply updated JOIN operator, leverages equality constraint
two local replicas collapse single dimension (called M3 ) message M12 .
result outgoing message 2 dimensions: {M3 , M12 }, takes much less
space. possible 3 agents involved, i.e. A1 , A2 A3 know M31 , M32
M33 represent variable.
change, VALUE propagation phase modified top local
copy variable solve optimization problem compute best value, announcing
result local copies assume value.
3.2.5 C OMPLEXITY NALYSIS DPOP PPLIED OCIAL C HOICE
special handling replica variables, DPOP applied SCPs scale induced
width centralized problem graph, independently number agents involved
number local replica variables.
Consider DFS arrangement centralized model SCP equivalent
DFS arrangement DCOP model. Equivalent means original variables
SCP visited order corresponding communities visited
distributed DFS construction. (Recall distributed DFS traversal described Section 3.1.1
visits local copies community DCOP moving next community). Let
w denote induced width DFS arrangement centralized SCP. Similarly, let k denote
induced width DFS arrangement distributed model. Let = maxm |dm | denote
maximal domain variable. Then, following:
Theorem 1 (DPOP Complexity SCP) number messages passed DPOP solving
SCP 2m, (n 1) (n 1) phases one, two three respectively, n
number nodes edges DCOP model replicated variables. maximal number
utility values computed node DPOP O(Dw+1 ), largest UTIL message
O(Dw+1 ) entries, w induced width centralized problem graph.
P ROOF. first part claim (number messages) follows trivially Proposition 1.
second part (message size computation): given DFS arrangement DCOP, applying
Proposition 1 trivially gives basic DPOP algorithm, maximal amount computation
node O(Dk+1 ), largest UTIL message O(Dk ) entries, k induced width DCOP problem graph. improve analysis need consider special
handling replica variables.
Consider UTIL messages travel along DFS tree, whose sets dimensions
contain separators sending nodes. Recall updated JOIN collapses local replicas
original variables. union dimensions UTIL messages join DPOP
DCOP model becomes identical set dimensions nodes DPOP
centralized model. Thus, node DCOP model performs amount computation
counterpart centralized model. follows computation required DPOP scales
O(Dw+1 ) rather O(Dk+1 ) special handling.
727

fiP ETCU , FALTINGS , & PARKES

remains one additional difference DPOP DFS arrangement centralized SCP versus DPOP DFS arrangement DCOP. variable Xv replicated
across multiple agents projected UTIL propagation local optimization top-most agent handling local replica Xv . first node
relevant information place support optimization step. particular, whenever node
maximal separator set also associated top-most replica variable
must retain dependence value assigned variable UTIL message sends
parent. increases worst case message size DPOP O(Dw+1 ), opposed O(Dw )
normal DPOP. Computation remains O(Dw+1 ) utility determined
value Xv anyway, projecting Xv out. 2
see effect message size described proof, local variable cannot
immediately removed UTIL propagation, consider problem Figure 2. Suppose agent A3 also involved meeting M1 . introduces additional back-edge
M23 M13 DFS arrangement decentralized model shown Figure 2(c). DFS
arrangement COP model corresponds decentralized model simply traversal
COP order communities visited distributed DFS construction. corresponds chain: M3 M1 M2 . introduction additional back-edge
M23 M13 distributed DFS arrangement change DFS COP model,
width remains w = 2. However, M23 top copy M2 , agent A3 cannot project
M2 outgoing UTIL message. result sends UTIL message w + 1 = 3
dimensions, opposed w = 2.

4. Handling Self-interest: Faithful Algorithm Social Choice
adapted DPOP remain efficient SCPs, turn issue self-interest. Without modification, agent manipulate DPOP misreporting private relations
deviating algorithm various ways. setting meeting scheduling, example,
agent might benefit misrepresenting local preferences (I massively utility
meeting occurring 2pm 9am), incorrectly propagating utility information
(competing) agents (The person team high utility meeting 2pm),
incorrectly propagating value decisions (It already decided meeting
involving person team 9am meeting must 2pm.)
introducing carefully crafted payments, leveraging information communication
structure inherent DCOPs social choice, careful partitioning computation
agent asked reveal information, perform optimization, send messages
interest, able achieve faithfulness. mean agent choose,
even self-interested, follow modified algorithm. first define VCG mechanism
social choice illustrate ability prevent manipulation centralized problem solving
simple example. place, next review definitions faithful distributed implementation results useful principle, partition principle. describe Simple
M-DPOP algorithm without reuse computation prove faithfulness.
728

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

4.1 Review: Mechanism Design VCG Mechanism
Mechanism design (MD) addresses problem optimizing criteria, frequently social welfare, presence self-interested agents private information relevant
problem hand. standard story, agents report private information center, solves
optimization problem enforces outcome.
second-price, sealed-bid (Vickrey) auction simple example mechanism: agent
makes claim value item auctioneer, allocates item highest
bidder second-highest price (Krishna, 2002). Vickrey auction useful
non-manipulable, weakly dominant strategy agent report true value,
efficient, item allocated agent highest value.
setting efficient social choice, assume existence currency agents
make payments, make standard assumption quasilinear utility functions, agent
Ai net utility is,
ui (X, p) = Ri (X) p,

(7)

assignment X variables X payment p
center, i.e., net utility
P R
j
(X),
minus amount
r
defined utility assignment, Ri (X) =
j
ri Ri
payment. One celebrated results MD provided Vickrey-Clarke-Groves
(VCG) mechanism, generalizes Vickreys second price auction problem efficient
social choice:
Definition 7 (VCG mechanism Efficient Social Choice) Given knowledge public constraints C, public decision variables X , Vickrey-Clarke-Groves (VCG) mechanism works
follows:
agent, Ai , makes report Ri private relations.
centers decision, X , solves SCP (A) given reports R = (R1 , . . . , Rn ).
agent Ai , makes payment
Tax (Ai ) =

X



Rj (Xi
) Rj (X ) ,

(8)

j6=i

, , solution SCP (A ) given reports R
center, Xi


=
(R1 , . . . , Ri1 , Ri+1 , . . . , Rn ).

agent makes payment equals negative marginal externality presence
imposes rest system, terms impact preferences solution
SCP.
VCG mechanism number useful properties:
Strategyproofness: agents weakly dominant strategy, i.e. utility-maximizing strategy whatever strategies whatever private information agents, truthfully report preferences center. sense VCG mechanism
non-manipulable.
729

fiP ETCU , FALTINGS , & PARKES

Efficiency: equilibrium, mechanism makes decision maximizes total utility
agents feasible solutions SCP.
Participation:
P
equilibrium, utility agent Ai , Ri (X ) Tax (Ai ) = (Ri (X ) +
P


j6=i Rj (Xi ), non-negative, principle optimality, therej6=i Rj (X ))
fore agents choose participate.
No-Deficit:
payment
made agent Ai non-negative SCP,
P
P


j6=i Rj (X ) principle optimality, therefore entire
j6=i Rj (Xi )
mechanism runs budget surplus.
begin understand VCG mechanism strategyproof, notice first term
Tax (Ai ) independent Ai report. second term, taken together
agents
P

true utility decision, provides Ai net utility Ri (X ) + j6=i Rj (X ).
total utility agents, maximize agent simply report true preference
information, center explicitly solve problem picking X .
Example 6 Return example Figure 4. make SCP associating agents
A1 , A2 A3 relations r10 , r21 r31 variables {X0 , X1 }, {X1 , X2 }, {X1 , X3 } respectively. Breaking ties before, solution SCP (A) < X0 = a, X1 = c, X2 = b, X3 =
> utility < 6, 6, 3 > agents A1 , A2 A3 respectively. Removing agent A1 , solution
would < X0 =?, X1 = a, X2 = c, X3 = > utility < 5, 6 > agents A2 A3 . ?
indicates agents A2 A3 indifferent value X0 . Removing agent A2 , solution
would < X0 = c, X1 = b, X2 =?, X3 = c >, utility < 7, 4 > agents A1 A3 . Removing agent A3 , solution would < X0 = a, X1 = c, X2 = b, X3 =? >, utility < 6, 6 >
agents A1 A2 . VCG mechanism would assign < X0 = a, X1 = c, X2 = b, X3 = >,
payments (5 + 6) (6 + 3) = 2, (7 + 4) (6 + 3) = 2, (6 + 6) (6 + 6) = 0 collected
agents A1 , A2 A3 respectively. A3 negative impact agents A1 A2
incur payment. agents make payments: presence A1 helps A2 hurts A3
more, presence A2 hurts A1 A3 . conflict problem
value assigned variable X1 . Agents A1 , A2 A3 prefer X1 assigned b, c
respectively. chosen solution, agent A2 gets best outcome. Considering case
A3 , force either b selected reporting suitably high utility choice,
X1 = must pay 4 X1 = b must pay 1, either case weakly prefers
current outcome makes zero payment.
introduced VCG mechanism, important realize VCG mechanism
provides known, general purpose, method exists solve optimization problems
presence self-interest private information. positive side, straightforward
extend VCG mechanism (and techniques paper) maximize linear weighted sum
utility agent, weights fixed known, instance social
planner (Jackson, 2000). Roberts (1979) hand, established Groves mechanisms VCG mechanism important special case non-trivial
strategyproof mechanisms domain social choice unless known structure
agent preferences; e.g., everyone prefers earlier meetings, resource always weakly
preferred less. Together another technical assumption, Roberts theorem also extended Lavi et al. (2003) domains kind structure, instance combinatorial
730

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

auctions. see real sense possible address self-interested
DCOPs maximizing something like total utility participants.
4.2 Faithful Distributed Implementation
goal faithful distributed implementation distribute computation required solve
SCP determine payments population agents, retaining
analog strategyproofness. challenging opens additional opportunities
manipulation beyond centralized VCG mechanism.
presenting results, introduce following additional assumptions over-and-above
made far:
Agents rational helpful, meaning although self-interested, follow protocol whenever deviation make strictly better (given behavior
agents).
agent prevented posing several independent agents external technique
(perhaps cryptographic) providing strong (perhaps pseudonymous) identities.
Catastrophic failure occur agents community variable eventually
choose value variable.
trusted bank, connected trusted communication channel agent,
authority collect payments agent.
property rational helpful required able rely upon agents compute
payments agents make. Strong identities required avoid known vulnerabilities
VCG mechanism shown Yokoo, Sakurai Matsubara (2004), wherein agents
sometimes better participating multiple identities. Catastrophic failure ensures
decision determined protocol actually executed. prevents hold-out problem,
unhappy agent refuses adopt consensus decision. alternative solution would
agents report final decision trusted party, responsible enforcement.
trusted communication channel, mean agent send message bank without
interference agent. messages sent agent upon termination
M-DPOP, inform bank agents payments. bank also assumed work
distributed MD (Feigenbaum et al., 2002, 2006; Shneidman & Parkes, 2004),
trusted entity require. purpose ensure payments used align incentives.
provide formal definition distributed implementation need concept local
state. local state agent Ai corresponds sequence messages agent
received sent, together initial information available agent (including
relations, public information constraints). Given this, distributed implementation,
dM =< g, , >, defined terms three components (Shneidman & Parkes, 2004; Parkes &
Shneidman, 2004):
Strategy space, , defines set feasible strategies available agent Ai ,
strategy defines message(s) agent Ai send every possible local state.
Suggested protocol, = (s1 , . . . , sn ), defines strategy parameterized
private relations Ri agent Ai .
731

fiP ETCU , FALTINGS , & PARKES

Outcome rule, g = (g1 , g2 ), g1 : n defines assignment values, g1 () D,
variables X given joint strategy, = (1 , . . . , n ) n , g2 : n Rn defines
payment g2,i () R made agent Ai given joint strategy n .
defining message(s) sent every state, strategy encompasses
computation performed internally agent, information agent reveals private
inputs (e.g. relations), decisions agent makes propagate information
received messages agents.9 suggested protocol si corresponds algorithm,
takes input private information available agent relevant details
agents local state, generates message messages send neighbors network.
applied distributed input R = (R1 , . . . , Rn ) known parts input hard
constraints C, protocol induces particular execution trace algorithm. turn
induces outcome g(), = s(R), g1 () final assignment values (information
distributed across agents) g2 () vector payments bank
collect agents.10
main question ask, given distributed algorithm corresponding suggested
protocol, whether suggested protocol forms ex post Nash equilibrium induced game:
Definition 8 (Ex post Nash equilibrium.) Given distributed implementation dM =< g, , >,
suggested protocol = (s1 , . . . , sn ) ex post Nash equilibrium (EPNE) if, agents
Ai , relations Ri , relations agents Ri , alternate strategies ,
Ri (g1 (si (Ri ), si (Ri ))) g2 (si (Ri ), si (Ri )) Ri (g1 (i , si (Ri ))) g2 (i , si (Ri ))
(9)
EPNE, agent Ai benefit deviating protocol, si , whatever particular
instance DCOP (i.e. private relations R = (R1 , . . . , Rn )), long agents also
choose follow protocol. latter requirement makes EPNE weaker dominantstrategy equilibrium, si would best protocol agent even agents
followed arbitrary protocol.
Definition 9 (Faithfulness) Distributed implementation, dM = < g, , >, ex post faithful
suggested protocol ex post Nash equilibrium.
is, suggested protocol, s, said ex post faithful (or simply faithful)
best interest every agent Ai follow aspects algorithm information revelation,
computation message-passing whatever private inputs agents, long every
agent follows algorithm.
9. idea agent limited set possible messages sent local state implied
notion (restricted) strategy space justified following sense. Agents model autonomous
self-interested and, course, free send message state. hand, suggested
protocol followed every agent, messages semantically meaningful recipient
agent(s) trigger meaningful change local state recipient agent(s); i.e. change local state
changes future (external) behavior recipient agent. way, strategy space characterizes complete
set interesting behaviors available agent given agents follow suggested protocol.
sufficient, technical perspecitve, define ex post Nash equilibrium.
10. outcome rule must well-defined unilateral deviation s, i.e. one agent deviates
follow suggested protocol. Either protocol still reaches terminal state decisions payments
defined, protocol reaches bad state suitably negative utility participants, livelock
deadlock. neglect latter possibility rest analysis, easily treated introducing
special notation bad outcome.

732

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

4.3 Partition Principle Applied Efficient Social Choice
One cannot achieve faithful DI efficient SCP simply running DPOP, n + 1 times
problem graph, main problem agents effect nullified turn
asking simply propagate messages. Agent Ai would seek following: (a) interfere
computational process SCP (Ai ), make solution close possible
SCP (A), marginal impact appears small; (b) otherwise decrease payment,
example increasing apparent utility agents solution SCP (A), turn
increases value second term VCG payment (Eq. 8).
opportunity manipulation recognized Parkes Shneidman (2004)
general setting, proposed partition principle method achieving faithfulness distributed VCG mechanisms, instantiated context efficient SCPs:
Definition 10 (partition principle) distributed algorithm, corresponding suggested protocol
s, satisfies partition principle application efficient social choice, if:
1. (Correctness) optimal solution obtained SCP (A) SCP (Ai ) every agent
follows s, bank receives messages instruct collect correct VCG payment
every agent.
2. (Robustness) Agent Ai cannot influence solution SCP (Ai ), report(s)
bank receives negative externality Ai imposes rest system
conditioned solutions SCP (A) SCP (Ai ).
3. (Enforcement) decision corresponds SCP (A) enforced, bank collects
payments instructed.
Theorem 2 (Parkes & Shneidman, 2004) distributed algorithm efficient social choice
satisfies partition principle ex post faithful distributed implementation.
intuition behind result, note opportunity manipulation agent
Ai restricted to: (a) influencing solution computed SCP (A); (b) influencing
payments made agents. Agent Ai cannot prevent agents correctly solving
SCP (Ai ) correctly reporting negative externality Ai imposes agents
presence. long agents follow algorithm, ex post faithfulness follows
strategyproofness VCG mechanism additional opportunity manipulation, available misreporting preferences centralized context,
change (either increase reduce) amount agents payment. opportunity
(b). Opportunity (a) new. agent always influence solution context
centralized VCG mechanism misreporting preferences.
Remark: suggested previous work, weakening dominant-strategy equilibrium centralized VCG mechanism, ex post Nash equilibrium distributed implementation, viewed cost decentralization. incentive properties necessarily rely
payments collected rely turn computation performed agents
turn strategy followed agents.11
11. exception provided Izmalkov et al. (2005), able avoid use cryptographic
primitives, case best thought physical devices ballot boxes.

733

fiP ETCU , FALTINGS , & PARKES

4.4 Simple M-DPOP
Algorithm 3 describes simple-M-DPOP. variation main problem, SCP (A) solved,
followed social choice problem, SCP (Ai ) agent removed turn.12

n + 1 problems solved, every agent Aj knows local part solution X Xi
Ai 6= Aj , part solution affects utility. provides enough
information allow system agents without agent Ai , Ai , send message
bank component payment agent Ai make.
Algorithm 3: Simple-M-DPOP.
1 Run DPOP DCOP (A) DFS (A); find X
2 forall Ai

3
Build DFS (Ai ); run DPOP DCOP (Ai ) DFS (Ai ); find Xi
) R (X ) report bank.
4
agents Aj P
6= Ai compute Tax j (Ai ) = Rj (Xi
j
5
Bank deducts j6=i Tax j (Ai ) Ai account
6

Ai assigns values X solution local COPi

computation payments
P disaggregated across agents. tax payment collected
agent Ai Tax (Ai ) = j6=i Tax j (Ai ),

Tax j (Ai ) = Rj (Xi
) Rj (X ),

(10)

component payment occurs negative effect agent Ai
utility agent Aj . information communicated bank agent Aj equilibrium.
important observation, able satisfy partition principle, components Ai payment satisfy locality property, agent Aj compute component Ai payment private information relations local information
affect utility. information availabout parts solutions X Xi
able upon termination simple-M-DPOP. Correctly determining payment, condition
, rely aspect agents algorithm, including
solutions X Xi
13
Ai .
Figure 5 provides illustration Simple M-DPOP earlier meeting scheduling example,
shows marginal problems (and DFS arrangements problem)
related main problem.
Theorem 3 simple-M-DPOP algorithm faithful distributed implementation efficient social choice terminates outcome VCG mechanism.
P ROOF. prove establish simple-M-DPOP satisfies partition principle
appeal Theorem 2. First, DPOP computes optimal solutions SCP (A) SCP (Ai )
12. Simple M-DPOP presented setting main problem subproblems connected extends
immediately disconnected problems. Indeed, may main problem connected one
subproblems disconnected. see additional incentive concerns notice sufficient
recognize correctness robustness properties partition principle would retained case.
13. similar disaggregation identified Feigenbaum et al. (2002) lowest-cost interdomain routing
Internet. Shneidman Parkes (2004) subsequently modified protocol authors agents
Ai enough information report payments made agent Ai .

734

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

Figure 5: Simple M-DPOP: agent Ai excluded turn optimization DCOP (Ai ).
illustrated meeting scheduling example.

Ai every agent follows protocol. immediate correctness
DCOP model SCP correctness DPOP. correct VCG payments collected
every agent follows algorithm correctness disaggregation VCG payments
Eq. 10. Second, agent Ai cannot influence solution SCP (Ai ) involved
computation way. DFS arrangement constructed, problem solved,
agents, completely ignore Ai messages agent Ai might send. (Any hard
constraints Ai may handled SCP (A) reassigned automatically agent
SCP (Ai ) consequence fact DFS arrangement reconstructed). DPOP
still solves SCP (Ai ) correctly case problem graph corresponding SCP (Ai )
becomes disconnected (in case DFS arrangement forest). robustness value
reports agents 6= Ai negative externality imposed Ai , conditioned solutions
SCP (A) SCP (Ai ), follows locality property payment terms Tax j (Ai )
Aj 6= Ai . enforcement, bank trusted empowered collect payments, agents
finally set local copies variables X prevent catastrophic failure. Agent Ai
deviate long agents deviate. Moreover, agent Ai agent
interested variable value already optimal agent Ai anyway. 2
partition principle, faithfulness, sweeping implications. agent
follow subtantive aspects simple-M-DPOP, agent also choose faithfully participate community discovery phase, algorithm choosing root community,
selecting leader agent Phase one DPOP.14
14. One also observe useful agent misreport local utility another agent Aj sending
UTIL messages around system. one hand, deviation could course change selection X

Xk
k 6= {i, j} thus payments agents solution ultimately selected. But, deviating

735

fiP ETCU , FALTINGS , & PARKES

Remark Antisocial Behavior: Note reporting exaggerated taxes hurts agents
increase ones utility excluded assumption agents selfinterested helpful.

5. M-DPOP: Reusing Computation Retaining Faithfulness
section, present main result, M-DPOP algorithm. simple-M-DPOP,
computation solve main problem completely isolated computation solve
marginal problems. comparison, M-DPOP re-use computation already performed
solving main problem solving marginal problems. enables algorithm scale well
problems agents influence limited small part entire problem
little additional computation required beyond DPOP. problems agents
influence limited precisely interest also induced
tree width small DPOP scales.
challenge face, facilitating re-use computation, retain incentive
properties provided partition principle. possible new manipulation agent
Ai deviate computation DCOP (A), intended effect change solution
DCOP (Ai ) via indirect impact computation performed DCOP (A)
reused solving DCOP (Ai ). prevent this, determine UTIL messages
DCOP (A) could influenced agent Ai .
Example 7 Refer Figure 6. agent Ai controls X3 X10 . way
influencing messages sent subtrees rooted {X14 , X15 , X2 , X7 , X5 , X11 }. want
able reuse many UTIL messages possible. solving problem agent
Ai removed strive construct DFS arrangement problem DCOP (Ai )
similar possible DFS main problem. done goal maximizing
re-use computation across problems. See Figure 6(b). Notice DFS forest,
three distinct connected components. UTIL messages sent shaded nodes
re-used solving DCOP (Ai ). UTIL messages sent nodes subtrees
influenced agent Ai except {X14 , X15 , X5 } also X9 , different
local DFS arrangement.
M-DPOP uses safe reusability idea suggested example. See Algorithm 4. first
stage, M-DPOP solves main problem Simple-M-DPOP. complete,
marginal problem DCOP (Ai ) solved parallel. solve DCOP (Ai ), DFS forest (it
forest case DCOP (Ai ) becomes disconnected) constructed modification
DFS (A), retaining much structure DFS (A) possible. new DPOP (Ai )
execution performed DFS U IL messages determined either reusable
reusable sender message based differences DFS DFS (A).
explain DFS constructed.
way agent cannot change utility information finally used determining payments.
agent Aj computes marginal effect agent Ai local solution, component
Tax j (Ai ) agent Ai payment.

736

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

Figure 6: Reconstructing DFS (Ai ) DFS (A) M-DPOP. result general DFS forest.
bold nodes main DFS initiate DFS propagation. one initiated X5 redundant
eventually stopped X9 . ones X4 X15 useful, subtrees become
really disconnected removing Ai . X14 initiate propagation since X1
pseudoparent. X1 controlled Ai , eventually connect X14 . Notice
X0 X9 X1 X14 turned tree edges.

5.1 Phase One M-DPOP Marginal Problem: Constructing DFS
Given graph DCOP (A) DFS arrangement DFS (A) DCOP (A), one removes set
nodes X(Ai ) DCOP (A) (the ones belong Ai ), need algorithm constructs
DFS arrangement, DFS , DCOP (A) \ X(Ai ). want achieve following properties:
1. DFS must represent correct DFS arrangement graph DCOP (Ai ) (a DFS forest
case DCOP (Ai ) becomes disconnected).
2. DFS must constructed way non-manipulable Ai , i.e. without allowing
agent Ai interfere construction.
3. DFS similar possible DFS (A). allows reusing UTIL messages
DPOP (A), saves computation communication.
main difficulty stems fact removing nodes represent variables interest agent Ai DFS (A) create disconnected subtrees. need reconnect possibly
rearrange (now disconnected) subtrees DFS (A) whenever possible. Return example Figure 6. Removing agent Ai nodes X3 X10 disrupts tree two ways:
subtrees become completely disconnected rest problem (e.g. X15 X18 X19 );
ones remain connected via back-edges, thus forming invalid DFS arrangement
737

fiP ETCU , FALTINGS , & PARKES

Algorithm 4: M-DPOP: faithfully reuses computation main problem.
1 Run DPOP DCOP (A) DFS (A); find X
2 forall Ai
parallel
3

Create DFS Algorithm 5 adjusting DFS (A)

4

Run DPOP DCOP (Ai ) DFS :
leaves DFS observe changes DFS
send null UTILi messages

5

6

7
8

else compute UTILi messages anew, DPOP
subsequently, nodes Xk DF do:
Xk receives null UTILi msgs (Pk = Pki P Pk = P Pki Ck = Cki )
Xk sends null UTILi message
else
node Xk computes UTILi message, reusing:
forall Xl N eighbors(Xk ) s.t. Xl sent UTILi = null
Xk reuses UTIL message Xl sent DCOP (A)
Compute levy taxes simple-M-DPOP;
Ai assigns values X solution local COPi ;

(e.g. X5 X8 X9 ). basic principle use reconnect disconnected parts via back-edges
DFS (A) whenever possible. intended preserve much structure possible. example, Figure 6, back edge X0 X9 turned tree edge, X5 becomes
X9 child. Node X8 remains X5 child.
DFS reconstruction algorithm presented Algorithm 5. high-level overview
follows (in bold state purpose step):
1. (Similarity DFS (A) :) nodes retain DFS data structures constructing
DFS (A); i.e., lists children, pseudo parents/children, parents
DFS (A). use data starting point building DFS arrangements,
DFS (Ai ), marginal problems.
2. (At least one traversal connected component DFS forest:) root
DFS (A) children15 removed nodes initiate DFS token passing
DFS (A), except changes:
node Xk sends token neighbors owned Ai .
order Xk sends token neighbors based DFS (A): First Xk
children DFS (A), pseudochildren, pseudoparents,
parent. order helps preserve structure DFS (A) DFS (Ai ).
15. Children pseudoparents excluded node, instance X14 Figure 6, initiate DFS token
passing would redundant: would eventually receive DFS token pseudoparent.

738

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

Algorithm 5: Reconstruction DFS DFS (A).
data structures DFS denoted superscript .

Procedure Token passing DFS (executed nodes Xk
/ X(Ai )) :
1
2

3
4
5

forall Xl Neighbors(Xk ) s.t. Xl belongs Ai
Remove Xl Neighbors(Xk ) Ck , PC k , PP k //i.e. send nothing Ai
Sort Neighbors(Xk ) order: Ck , PC k , PP k , Pk //mimic DFS (A)
Xk root, Pk X(Ai ) (i.e. executed root children Ai )
Initiate DFS normal DFS (Algorithm 2)
else Process incoming tokens()
Send DFS (Xk ) back Pki // Xk subtree completely explored
Procedure Process incoming tokens()

6
7
8
9
10
11

Wait incoming DFS token; Let Xl sender
Xl Ai ignore message
else
first token received


Pki = Xl ; PP
k = {Xj 6= Pk |Xj Neighbors(Xi ) DFS }


rootk = first node token DFS

17

else
let Xr first node DFS

traversal
Xr 6= rooti
k //i.e. another DFS
depth Xr DFS (A) < depth root
k DFS (A)




Reset Pk , PP k , Ck , PC k //override redundant DFS lower root


Pki = Xl ; PP
k = {Xj 6= Pk |Xj Neighbors(Xi ) DFS }

root k = Xr

18

Continue Algorithm 2

12
13
14
15
16

3. (Unique traversal connected component DFS forest:) node Xk retains
root path DFS (A) knows depth DFS arrangement. new token
DFS arrives:
first DFS token arrives, sender (let Xl ) marked
parent Xk DFS : Pki = Xl . Notice Xl could different
parent Xk DFS (A). Xk stores first node received token DFS ,
root
k : (provisional) root connected component Xk belongs
DCOP (Ai ).
first DFS token arrives, two possibilities:
token received part DFS traversal process. Xk recognizes
fact first node newly received token
previously stored root
k . case, Xk proceeds normal, Algorithm 2:
marks sender pseudochild, etc.
739

fiP ETCU , FALTINGS , & PARKES

token received part another DFS traversal process, initiated another
node root
k (see text could happen). Let Xr first
node newly received token. Xk recognizes situation fact Xr

previously stored rooti
traversal
k . case, DFS
initiated higher node DFS (A) prevails, one dropped.
determine traversal pursue one drop, Xk compares depths

rooti
k Xr DFS (A). Xr higher, becomes new rootk . Xk
overrides previous DFS information one new token.
continues token passing new token Algorithm 2.
see necessary also start propagations children removed nodes (step
2), consider example Figure 6. Removing X10 X3 completely disconnects
subtree {X4 , X6 , X11 , X7 , X12 , X13 }. X4 started propagation, subtree would
visited since connections rest problem
nodes subtree.16 17
Lemma 1 (DFS correctness) Algorithm 5 constructs correct DFS arrangement (or forest),
DFS DCOP (Ai ) given correct DFS arrangement DFS (A) DCOP (A).
P ROOF. First, since DFS started child node controlled Ai , also
root, ensured connected component DFS-traversed least (follows
Step 2). Second, DFS process similar normal DFS construction, node
sends token neighbors (except ones controlled Ai );
pre-specified order (the one given DFS (A)). follows nodes connected component
eventually visited (follows Step 3). Third, higher-priority DFS traversals override
lower priority ones (i.e. DFS traversals initiated nodes higher tree priority),
Step 3. Eventually one single DFS-traversal performed single connected component. 2
Lemma 2 (DFS robustness) DFS arrangement, DFS , constructed Algorithm 5 nonmanipulable agent Ai , input DFS arrangement solution phase DCOP (A).
P ROOF. follows directly Step 3, since Ai participate process all:
neighbors send messages (see Algorithm 5, line 1), messages may send
simply ignored (see Algorithm 5, line 7) 2
fact, additional links created constructing DFS . possible changes
edges reverse direction (parents/children pseudoparents-pseudochildren
16. DFS traversals initiated Step 2 redundant part problem graph visited
once. simple overriding rule Step 3 ensures single DFS tree eventually adopted
connected component, namely one initiated highest node original DFS (A).
example, Figure 6, X5 starts unnecessary DFS propagation, eventually stopped X9 ,
receives higher priority DFS token X0 . Since X9 knows X0 higher DFS (A) X5 , drops
propagation initiated X5 , relays one initiated X0 . sending X5 token
DFS received X0 adds itself. Upon receiving new token X9 , node X5 realizes
X9 new parent DFS . Thus, redundant propagation initiated X5 eliminated result
consistent DFS subtree single connected component P1 .
17. simple time-out mechanism used ensure agent knows provisional DFS ordering final
(i.e. higher priority DFS traversals arrive future).

740

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

switch places), existing back-edges turn tree edges. Again, one see
Figure 6.18
5.2 Phase Two M-DPOP Marginal Problem: UTILi propagations
DFS built, marginal problem without Ai solved DFS . Utility propagation proceeds normal DPOP except nodes determine whether UTIL message
sent DPOP (A) reused. signaled parent sending special null UTIL
message. specifically, process follows:
leaves DFS initiate UTILi propagations:
1. leaves DFS observe changes local DFS arrangement compared DFS (A) UTIL message sent DCOP (A) remains valid
announce parents sending instead null UTILi message.
2. Otherwise, leaf node computes UTIL message anew sends (new)
parent DFS .
nodes wait incoming UTILi messages and:
1. every incoming messages node Xk receives children null
changes parent/pseudoparents propagate null UTILi message
parent.
2. Otherwise, Xk recompute UTILi message. reusing UTIL
messages received DCOP (A) children sent null messages
DCOP (Ai ) joining new UTIL messages received.
example, consider DCOP (Ai ) Figure 6, X16 X17 children X14 . X14
recompute UTIL message send new parent X1 . this, reuse
messages sent X16 X17 DCOP (A), neither sending subtrees contain Ai .
16
so, X14 reuses effort spent DCOP (A) compute messages UTIL16
20 , UTIL21 ,
14
UTIL14
16 UTIL17 .
Theorem 4 M-DPOP algorithm faithful distributed implementation efficient social
choice terminates outcome VCG mechanism.
P ROOF. partition principle appeal Theorem 3 (and turn Theorem 2). First,
agent Ai cannot prevent construction valid DFS DCOP (Ai ) (Lemmas 1 2).
Second, agent Ai cannot influence execution DPOP DCOP (Ai ) messages
Ai influenced main problem DCOP (A) recomputed system without Ai .
rest proof follows simple-M-DPOP, leveraging locality tax payment messages
enforcement provided bank via catastrophic failure assumption. 2
18. simple alternative children nodes Xki belong Ai , create bypass link first ancestor
Xki belong Ai . example, Figure 6, X4 X5 could create link X1 bypass X3
completely DFS (Ai ). However, additional communication links may required approach.

741

fiP ETCU , FALTINGS , & PARKES

6. Experimental Evaluation: Understanding Effectiveness M-DPOP
present results experimental evaluation DPOP, Simple M-DPOP M-DPOP
two different domains: distributed meeting scheduling problems (MS), combinatorial auctions
(CAs). first set experiments investigate performance M-DPOP structured
constraint optimization problem (MS) received lot attention cooperative distributed
constraint optimization. second set experiments (CAs), investigate unstructured domains, observe performance specifically ability re-use computation computing
payments M-DPOP respect problem density. CAs provide abstract model many
real world allocation problems much studied mechanism design (Cramton, Shoham, &
Steinberg, 2006).
6.1 Distributed Meeting Scheduling
distributed meeting scheduling, consider set agents working large organization
representing individuals, groups individuals, engaged scheduling meetings
upcoming period time. Although agents self interested, organization
whole requires optimal overall schedule, minimizes cost (alternatively, maximizes
utility agents). makes necessary use faithful distributed implementation
M-DPOP. enabling this, suppose organization distributes virtual currency
agent (perhaps using currency allocation prioritize particular participants.) relations held
agents defining agents utility solution scheduling problem thus stated
units currency.
agent Ai set local replicate variables Xji meeting Mj
involved. domain variable Xj (and thus local replicas Xji ) represents feasible
time slots meeting. equality constraint included replica variables ensure
meeting times aligned across agents. Since agent cannot participate one
meeting all-different constraint variables Xij belonging agent.
modeled clique constraint meeting variables. agent assigns utility
possible time meeting imposing unary relation variable Xji .
relation private Ai , denotes much utility Ai associates starting meeting Mj
time dj , dj domain meeting Mj . social objective find schedule
total utility maximized satisfying all-different constraints agent.
Following Maheswaran et al. (2004), model organization providing hierarchical
structure. realistic organization, majority interactions within departments,
small number across departments even interactions typically take place
two departments adjacent hierarchy. hierarchical organization provides structure
test instances: high probability (around 70%) generate meetings within departments,
lower probability (around 30%) generate meetings agents belonging
parent-child departments. generated random problems structure, increasing
number agents: 10 100 agents. agent participates 1 5 meetings,
uniform random utility 0 10 possible schedule meeting
participates. problems generated feasible solutions.19
19. test instances found http://liawww.epfl.ch/People/apetcu/research/mdpop/MSexperiments.tgz

742

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

problem size, averaged results 100 different instances. solved main
problems using DPOP marginal ones using simple-M-DPOP, M-DPOP respectively.
experiments performed FRODO multiagent simulation environment (Petcu, 2006),
1.6Ghz/1GB RAM laptop. FRODO simulated multiagent system, agent executes
asynchronously thread, communicates peers via message exchange.
experiments geared towards showing much effort M-DPOP able reuse
main marginal problems. Figure 6.1 shows absolute computational effort terms
number messages (Figure 6.1(a)), terms total size messages exchanged,
bytes (Figure 6.1(b)). curves DPOP represent number messages (total size
messages, respectively) required solving cooperative problem. curves simpleM-DPOP M-DPOP represent total number (size, respectively) UTIL messages,
main marginal economies.
notice several interesting facts. First, number messages required DPOP increases
linearly number agents DPOPs complexity terms number messages
always linear size problem. hand, number messages simple-MDPOP increases roughly quadratically number agents, since solves linear number
marginal economies scratch using DPOP, requiring linear number messages.
performance M-DPOP lies somewhere DPOP simple-M-DPOP
advantage achieved simple-M-DPOP size problem increases, culminating
almost order magnitude improvement Simple M-DPOP largest problem sizes (i.e.
100 agents problem). Similar observations made total size UTIL
messages, also good measure computation, traffic memory requirements, inspecting
Figure 6.1(b). metrics find performance M-DPOP slightly superlinear size problem.
Figure 8 shows percentage additional effort required solving marginal problems
reused main problem, i.e. probability UTIL message required solving marginal problem taken directly message already used main problem.
clearly see problem size increases actually reuse computation
main problem. intuition behind large problems, individual agent
localized particular area problem. translates agent localized
specific branch tree, thus rendering computation performed branches reusable
marginal problem corresponds respective agent. Looking also percentage
reuse defined terms message size rather number messages see
also trending upwards size problem increases.
6.2 Combinatorial Auctions
Combinatorial Auctions (CAs) popular means allocate resources multiple agents. CAs,
bidders bid bundles goods (as opposed bidding single goods). Combinatorial bids
model complementarity substitutability among goods, i.e. valuation
bundle more, respectively less sum valuations individual items.
setting agents distributed (geographically logically), form problem graph
neighbors agents bids overlap. objective find feasible solution (i.e.
declare bids winning losing two winning bids share good) maximizes
total utility agents.
743

fiP ETCU , FALTINGS , & PARKES

100000

1e+07

10000
# messages

DPOP
simple_M-DPOP
M-DPOP

Total Size UTIL Messages

DPOP
simple_M-DPOP
M-DPOP

1000

100

1e+06

100000

10000

10

1000
10

20

30

40

50

60

70

80

90 100

10

20

Number agents

30

40

50

60

70

80

90 100

Number agents

(a) Number messages

(b) Total size UTIL messages (in valuations)

Figure 7: Meeting scheduling problem: measures absolute computational effort (in terms number

% effort marginals reused main

messages sent total size UTIL messages) DPOP, simple-M-DPOP MDPOP. curves DPOP represent effort spent main problem, ones
simple-M-DPOP M-DPOP represent total effort main marginal problems.

90
85
80
75
70
65
60
55

Total information
Number messages

50
10

20

30

40

50

60

70

80

90

100

Number agents

Figure 8: Meeting scheduling problem: Percentage effort required marginal problems
reused M-DPOP main problem. Reuse measured terms percentage
UTIL messages reused (dashed) also terms total size UTIL
messages reused fraction total UTIL message size (solid).

744

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

CAs adopted stylized model distributed allocation problems airport slot
allocation wireless spectrum allocation discussed Introduction. CA instances also
provide counterpoint meeting scheduling problems represent problems
less structure. DCOP model, agent holds variable one bids, two
possible values: 0 bid rejected, 1 bid accepted. pair overlapping
bids (bids share least one good) connected one constraint specifies
cannot accepted. multiple bids submitted agent
connected additional constraints capture bid logic, instance exclusive-or constraints
one bid accepted.
generated random problems using CATS (Leyton-Brown, Pearson, & Shoham, 2000), using
L3 distribution Sandholm (2002). L3 Constant distribution agent
demands bundle 3 goods, selected uniformly random, value distributed uniformly
[0, 1]. simulations consider market 50 goods vary number agents
5 40. recorded performance DPOP, simple-MDPOP M-DPOP
graphs Figures 9 10. Figure 9 shows density problems increase, three
algorithms require effort solving (both terms number messages, terms
total information exchange).
Figure 10 shows reusability varies problem density: one see loose problems reusability good, close 100% problems 5 agents. density
problems increases number agents, reusability decreases well, around 20%
dense problems, 40 agents. explain phenomenon follows: loose
problems (many goods bidders), bids mostly non-overlapping, turn ensures
removing individual agents solving marginal problems affect computation
performed solving main problem. end spectrum, dense problems
tend highly connected, produces DFS trees similar chains.
case, removing agents close bottom chain invalidates much computation performed solving main problem. Therefore, limited amount computation
reused.
noting L3 recognized one hardest problem distributions CATS
suite (Leyton-Brown et al., 2000), remark need limit experiments distribution problems large induced tree width (and high density problem graphs).
Consider example problem every agent bids bundle overlaps every
agent. problem graph clique DPOP scale. leave detailed
examination future work, recent extension DPOP H-DPOP (Kumar, Petcu, & Faltings,
2007) immediately address issue. H-DPOP, consistency techniques used order
compactly represent UTIL messages, tightly constrained problems, orders magnitude
improvements DPOP reported (see Section 7.1).

7. Discussion
section discuss alternatives improving computational performance M-DPOP,
possibility faithful variations DCOP algorithms (ADOPT (Modi et al., 2005)
OptAPO (Mailler & Lesser, 2004)), loss utility agents occur due
transfer payments bank, mentioning approach address problem.
745

fiP ETCU , FALTINGS , & PARKES

10000

1e+08
Total Size UTIL Messages

DPOP
simple_M-DPOP
M-DPOP
# messages

1000

100

10

DPOP
simple_M-DPOP
M-DPOP

1e+07
1e+06
100000
10000
1000
100
10

1

1
5

10

15
20
25
30
Number agents

35

40

5

(a) Number messages

10

15
20
25
30
Number agents

35

40

(b) Total size UTIL messages (in valuations)

Figure 9: Combinatorial Auctions problems: measures absolute computational effort (in terms

% effort marginals reused main

number messages sent total size UTIL messages) DPOP, simple-M-DPOP
M-DPOP. curves DPOP represent effort spent main problem, ones
simple-M-DPOP M-DPOP represent effort main marginal problems.
higher number agents (and thus bids, thus constraints problem graph
problem density), greater computational effort solve problem.

100
90
80
70
60
50
40
30
Total information
Number messages

20
10
5

10

15

20

25

30

35

40

Number agents

Figure 10: Combinatorial Auctions problems: Percentage effort required marginal problems
reused M-DPOP main problem. Reuse measured terms percentage
UTIL messages reused (dashed) also terms total size UTIL
messages reused fraction total UTIL message size (solid).

746

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

7.1 Algorithmic Alternatives Improved Performance
M-DPOP scales well problem size long induced width problem remains
low. characteristic M-DPOP inherits DPOP, based. problems
high induced width, DPOP/M-DPOP require producing, sending storing large messages,
may unfeasible undesirable. mitigate problem, several advances basic
DPOP algorithm recently proposed. new algorithms sacrifice optimality
return computational tractability, makes difficult combine VCG payment
mechanism way faithfulness guaranteed. Nevertheless, H-DPOP (Kumar et al.,
2007) MB-DPOP (Petcu & Faltings, 2007) employ two different techniques preserve
optimality guarantees, fitted M-DPOP.
H-DPOP leverages observation many real problems contain hard constraints significantly reduce space feasible assignments. example, auctions, possible
allocate item one bidder. meeting scheduling, possible set two different start times given meeting. Unfortunately, DPOP take advantage pruning
power hard constraints, sends messages explicitly represent value combinations, including many infeasible ones. H-DPOP addresses issue using Constraint Decision
Diagrams (CDD) introduced Cheng Yap (2005) compactly represent UTIL messages
excluding unfeasible combinations. Performance improvements several orders magnitude
achieved, especially highly constrained problems (Kumar et al., 2007).
MB-DPOP (Petcu & Faltings, 2007) uses idea cycle cutsets (Dechter, 2003) explore
parts search space sequentially. Dense parts problem explored iterating
assignments subset nodes designated cycle cuts, assignment performing
limited UTIL propagation similar one DPOP. Easy parts problem explored
one-shot UTIL messages, exactly DPOP. MB-DPOP offers thus configurable tradeoff
number messages exchanged, size messages memory
requirements.
7.2 Achieving Faithfulness DCOP Algorithms
partition principle, described Section 4.3, algorithm independent. question
whether another, optimal DCOP algorithm made faithful therefore revolves, critically, around
whether algorithm satisfy robustness requirement partition priciple. make
following observations:
Robustness first sense, i.e. agent Ai influence solution efficient SCP without agent Ai , always achievable cost restarting computation
marginal problem agent removed turn, proposed simple-M-DPOP.
Robustness second sense, i.e. agent Ai influence report(s) bank
receives negative externality Ai imposes rest system, conditioning
solutions main problem problem without Ai , requires DCOP
algorithm terminates every agent knowing part solution relevant
defining utility; robustness property follows disaggregation payments.
Thus, one content restart DCOP algorithm multiple times, kinds
results provide simple-M-DPOP generally available. possible
747

fiP ETCU , FALTINGS , & PARKES

already mentioned locality property payments, follows disaggregation
VCG payment across agents Eq. (10) information communication
structure DCOP.
useful property DCOP context self-interested agents, worth reemphasizing, possible retain faithfulness even one agent plays pivotal role
connecting problem graph. Suppose problem, DCOP (Ai ), becomes disconnected without Ai . But, case optimal solution represented union optimal
solutions connected subcomponent problem, information needs flow disconnected components either purpose solving problem purpose
reporting components agent Ai tax.
discuss following two sections adaptation two prominent complete DCOP algorithms: ADOPT (Modi et al., 2005) OptAPO (Mailler & Lesser, 2004).
discuss following two sections adaptation two prominent complete DCOP algorithms: ADOPT (Modi et al., 2005) OptAPO (Mailler & Lesser, 2004).
consider computational aspects making algorithms faithful, specifically issues related efficient handling replica variables providing reusability main
marginal problems.
7.2.1 U SING ADOPT



FAITHFUL , E FFICIENT OCIAL C HOICE

ADOPT polynomial-space search algorithm DCOP guaranteed find globally
optimal solution allowing agents execute asynchronously parallel. agents
ADOPT make local decisions based conservative cost estimates. ADOPT also works DFS
arrangement, constructed detailed Section 3.1.1. Roughly speaking, main process
executed ADOPT backtrack search DFS tree.
Adaptation ADOPT DCOP Model Replicated Variables. ADOPTs complexity
given number messages, exponential height DFS tree. Similar
DPOP, using DCOP model replicated variables could artificially increase complexity
solving process. Specifically, height DFS tree increased using replicated
variables compared centralized problem graph. ADOPT modified exploit special
structure replicated local variables similar way DPOP. Specifically, ADOPT
explore sequentially values original variable, ignore assignments replicas
variable take different values. works allowing agent owns
highest replica variable freely choose values variable. agent announces
new value variable agents owning replicas variable. agents
would consider announced value replicas, add corresponding utilities, continue search process. Using special handling replica variables,
resulting complexity longer exponential height distributed DFS tree,
height DFS tree obtained traversing original problem graph. example, Figure 2, sufficient explore values M32 , directly assign values M33 M31
via VALUE messages, without trying combinations values. reduces ADOPTs
complexity exponential 6, exponential 3.
Reusability Computation ADOPT. Turning re-use computation main
marginal problems, note ADOPT uses DFS arrangement easy
identify parts DFS arrangement main problem impossible agent
748

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

manipulate, therefore reused computing solution marginal problem
agent removed. However, major difference DPOP ADOPT DPOP,
agent stores outgoing UTIL message, thus available utilities contingent
assignments variables agents separator. makes possible agent
simply reuse information marginal economies structure DFS proves
safe. contrast, ADOPT store information linear memory
policy. turn makes impossible reuse computation main problem marginal
problems. marginal problems solved scratch, thus performance would
scale poorly problem size increases even structured problems meeting scheduling.
see two alternatives addressing problem: (a) renounce linear memory guarantees,
use caching scheme like example NCBB (Chechetka & Sycara, 2006): would allow
similar reusability M-DPOP, previously computated utilities extracted
cache instead recomputed. Alternatively, (b) one devise scheme
previously computed best solution saved reference, subsequently used
approximation solving marginal problems. could possibly provide better bounds
thus allow better pruning, computation could saved. alternatives
outside scope paper, considered future work.
7.2.2 U SING PTAPO



FAITHFUL , E FFICIENT OCIAL C HOICE

OptAPO (Mailler & Lesser, 2004) popular algorithm DCOP. Similar
adaptations DPOP ADOPT social choice, OptAPO also made take advantage
special features DCOP model replicated variables. complexity would
artificially increased use DCOP model. OptAPO particularity uses
mediator agents centralize subproblems solve dynamic asynchronous mediation sessions, i.e. partial centralization. mediator agents announce results
agents, previously sent subproblems mediators. process alone would
introduce additional possibility manipulation setting self interested agents. However,
using VCG mechanism addresses concern agents choose behave correctly according protocol.
ADOPT, main issue using OptAPO faithful social choice reusability
computation main marginal problems. Specifically, consider solving
main problem, mediator agent Ai centralized aggregated preferences number
agents, solving mediation problems dictated OptAPO protocol. Subsequently,
trying compute solution marginal problem without agent Ai , computation
go waste, could manipulated Ai solving main problem. Furthermore, since OptAPOs centralization process asynchronous conflict-driven opposed
structure-driven M-DPOP, unclear whether computation main problem
could safely reused marginal problems. make matters worse, experimental studies (Davin & Modi, 2005; Petcu & Faltings, 2006) show many situations, OptAPO ends
relying single agent system centralize solve whole problem. implies
solving marginal problem without agent, one reuse zero effort main
problem.
749

fiP ETCU , FALTINGS , & PARKES

7.3 Loss Utility due Wasting VCG Taxes
VCG mechanism, agents net utility difference utility derives
optimal solution VCG tax pay. net utility whole group agents
sum individual net utilities agents, i.e. total utility assignment values
variables net total payment made agents bank. loss utility
using M-DPOP great 35% total utility optimal solution meeting
scheduling domain. problem size increases, money burnt
form VCG taxes. Similar waste observed others; e.g., Faltings (2004), also
context efficient social choice.
One cannot naively redistribute payment back agents, instance sharing payments equally across agents would break faithfulness. example, agent Ai would prefer
agents make greater payments, order receive larger repayment bank.
faithfulness properties M-DPOP would unravel. hand, problem
inherent structure possible redistribute fraction payments back agents.
idea careful redistribution suggested Bailey (1997), subsequently extended
Cavallo (2006), Guo Conitzer (2007) Moulin (2007). Another approach, advocated example Faltings (2004), simply preclude agent problem transfer payments
agent. work centralized context.
important issue future work, then, study budget surplus accrues bank
M-DPOP seek mitigate welfare loss setting distributed implementation.
defer discussion topic future work, investigate methods
leverage structure problem redistributing majority payments back agents
without compromising either efficiency faithfulness.

8. Conclusions
developed M-DPOP, faithful, distributed algorithm solve efficient
social choice problems multi-agent systems private information self-interest. agent
improve utility either misreporting local information deviating aspect
algorithm (e.g., computation, message-passing, information revelation.) centralized
component bank able receive messages payments collect payments.
addition promoting efficient decisions, minimize amount additional computational
effort required computing VCG payments reusing effort main problem. first
set experimental results shows significant amount computation required
marginal problems reused main problem, sometimes 87%. provides
near-linear scalability massive, distributed social choice problems local structure
maximal induced tree width small. second set experiments performed problems
without local structure shows problem density increases, amount effort required increases, reusability computation decreases. results suggest M-DPOP
good candidate solving loose problems exhibit local structure induced width
remains small. addition addressing need reduce total payments made agents
bank, one issue future work relates need provide robustness faced adversarial
faulty agents: current solution fragile sense, equilibrium properties relying
agents following protocol. papers (Lysyanskaya & Triandopoulos, 2006; Aiyer,
Alvisi, Clement, Dahlin, Martin, & Porth, 2005; Shneidman & Parkes, 2003) provide robustness
750

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

mixture models (e.g. rational, adversarial) aware work
mixture models context efficient social choice. Another interesting direction find ways
allow approximate social choice, example memory-limited DPOP variations (Petcu
& Faltings, 2005a) retaining incentive properties, perhaps approximate equilibria. Future
research also consider design distributed protocols robust false-name
manipulations agents participate multiple pseudonyms (Yokoo et al., 2004),
seek mitigate opportunities collusive behavior possibility multiple equilibria exist incentive mechanisms (Ausubel & Milgrom, 2006; Andelman, Feldman, &
Mansour, 2007; Katz & Gordon, 2006).

Acknowledgments
Parkes supported part National Science Foundation grants IIS-0238147, IIS-0534620
Alfred P. Sloan Foundation award. Petcu supported Swiss National Science Foundation
grant 200020-103421/1. authors would like thank Wei Xue valuable feedback several
parts paper. thank Jeffrey Shneidman feedback early version paper.
also thank Aaron Bernstein valuable insights DFS reconstruction process. three
anonymous reviewers also provided excellent suggestions improving exposition work.
earlier version paper appeared Proc. Fifth International Joint Conference
Autonomous Agents Multiagent Systems (AAMAS), 2006.

References
Abu-Amara, H. H. (1988). Fault-tolerant distributed algorithm election complete networks. IEEE
Trans. Comput., 37(4), 449453.
Aiyer, A. S., Alvisi, L., Clement, A., Dahlin, M., Martin, J.-P., & Porth, C. (2005). Bar fault tolerance
cooperative services. 20th ACM Symposium Operating Systems Principles.
Andelman, N., Feldman, M., & Mansour, Y. (2007). Strong price anarchy. ACM-SIAM Symposium
Discrete Algorithms 2007 (SODA07).
Arnborg, S. (1985). Efficient algorithms combinatorial problems graphs bounded decomposability
- survey. BIT, 25(1), 223.
Ausubel, L., Cramton, P., & Milgrom, P. (2006). clock-proxy auction: practical combinatorial auction
design. Cramton et al. (Cramton et al., 2006), chap. 5.
Ausubel, L., & Milgrom, P. (2006). lovely lonely Vickrey auction. Cramton et al. (Cramton et al.,
2006), chap. 1.
Bailey, M. J. (1997). demand revealing process: distribute surplus. PublicChoice, 107126.
Ball, M., Donohue, G., & Hoffman, K. (2006). Auctions safe, efficient, equitable allocation
airspace system resources. Cramton, Shoham, S. (Ed.), Combinatorial Auctions. MIT Press.
Barbosa, V. (1996). Introduction Distributed Algorithms. MIT Press.
Bayardo, R., & Miranker, D. (1995). space-time trade-off solving constraint satisfaction problems.. Proceedings 15th International Joint Conference Artificial Intelligence, IJCAI-95,
Montreal, Canada.
Bidyuk, B., & Dechter, R. (2004). finding minimal w-cutset. AUAI 04: Proceedings 20th
conference Uncertainty artificial intelligence, pp. 4350, Arlington, Virginia, United States.
AUAI Press.
751

fiP ETCU , FALTINGS , & PARKES

Bikhchandani, S., de Vries, S., Schummer, J., & Vohra, R. V. (2002). Linear programming Vickrey
auctions. Dietrich, B., & Vohra, R. (Eds.), Mathematics Internet: E-Auction Markets, pp.
75116. IMA Volumes Mathematics Applications, Springer-Verlag.
Cavallo, R. (2006). Optimal decision-making minimal waste: Strategyproof redistribution vcg payments. Proc. 5th Int. Joint Conf. Autonomous Agents Multi Agent Systems (AAMAS06).
Chechetka, A., & Sycara, K. (2006). any-space algorithm distributed constraint optimization.
Proceedings AAAI Spring Symposium Distributed Plan Schedule Management.
Cheng, K. C. K., & Yap, R. H. C. (2005). Constrained decision diagrams.. Proceedings National
Conference Artificial Intelligence, AAAI-05, pp. 366371, Pittsburgh, USA.
Cheung, T.-Y. (1983). Graph traversal techniques maximum flow problem distributed computation..
IEEE Trans. Software Eng., 9(4), 504512.
Cidon, I. (1988). Yet another distributed depth-first-search algorithm. Inf. Process. Letters, 26(6), 301305.
Collin, Z., Dechter, R., & Katz, S. (1991). Feasibility Distributed Constraint Satisfaction.
Proceedings 12th International Joint Conference Artificial Intelligence, IJCAI-91, pp. 318
324, Sidney, Australia.
Collin, Z., Dechter, R., & Katz, S. (1999). Self-stabilizing distributed constraint satisfaction. Chicago Journal
Theoretical Computer Science.
Collin, Z., & Dolev, S. (1994). Self-stabilizing depth-first search. Information Processing Letters, 49(6),
297301.
Cramton, P., Shoham, Y., & Steinberg, R. (Eds.). (2006). Combinatorial Auctions. MIT Press.
Davin, J., & Modi, P. J. (2005). Impact problem centralization distributed constraint optimization
algorithms. AAMAS 05: Proceedings fourth international joint conference Autonomous
agents multiagent systems, pp. 10571063, New York, NY, USA. ACM Press.
Davis, R., & Smith, R. G. (1983). Negotiation metaphor distributed problem solving. Artificial
Intelligence, 63109.
de Vries, S., & Vohra, R. V. (2003). Combinatorial auctions: survey. Informs Journal Computing, 15(3),
284309.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
Dechter, R., & Mateescu, R. (2006). AND/OR search spaces graphical models. Artificial Intelligence.
appear.
Dunne, P. E. (2005). Extremal behaviour multiagent contract negotiation. Journal Artificial Intelligence
Research (JAIR), 23, 4178.
Dunne, P. E., Wooldridge, M., & Laurence, M. (2005). complexity contract negotiation. Artificial
Intelligence Journal, 164(1-2), 2346.
Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2006). Negotiating socially optimal allocations resources.
Journal Artificial Intelligence Research, 25, 315348.
Ephrati, E., & Rosenschein, J. (1991). Clarke tax consensus mechanism among automated agents.
Proceedings National Conference Artificial Intelligence, AAAI-91, pp. 173178, Anaheim,
CA.
Faltings, B. (2004). budget-balanced, incentive-compatible scheme social choice. Workshop
Agent-mediated E-commerce (AMEC) VI. Springer Lecture Notes Computer Science.
Faltings, B., Parkes, D., Petcu, A., & Shneidman, J. (2006). Optimizing streaming applications selfinterested users using M-DPOP. COMSOC06: International Workshop Computational Social
Choice, pp. 206219, Amsterdam, Netherlands.
752

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

Feigenbaum, J., Papadimitriou, C., Sami, R., & Shenker, S. (2002). BGP-based mechanism lowest-cost
routing. Proceedings 2002 ACM Symposium Principles Distributed Computing, pp.
173182.
Feigenbaum, J., Ramachandran, V., & Schapira, M. (2006). Incentive-compatible interdomain routing.
Proceedings 7th Conference Electronic Commerce, pp. 130139.
Feigenbaum, J., & Shenker, S. (2002). Distributed Algorithmic Mechanism Design: Recent Results
Future Directions. Proceedings 6th International Workshop Discrete Algorithms
Methods Mobile Computing Communications, pp. 113.
Freuder, E. C., & Quinn, M. J. (1985). Taking advantage stable sets variables constraint satisfaction
problems. Proceedings 9th International Joint Conference Artificial Intelligence, IJCAI85, pp. 10761078, Los Angeles, CA.
Gershman, A., Meisels, A., & Zivan, R. (2006). Asynchronous forward-bounding distributed constraints
optimization. Proceedings 17th European Conference Artificial Intelligence (ECAI-06),
Riva del Garda, Italy.
Greenstadt, R., Pearce, J. P., & Tambe, M. (2006). Analysis privacy loss distributed constraint optimization. Proc. Twenty-First National Conference Artificial Intelligence (AAAI-06).
Guo, M., & Conitzer, V. (2007). Worst-case optimal redistribution vcg payments. Proceedings
8th ACM Conference Electronic Commerce (EC-07), pp. 3039.
Huebsch, R., Hellerstein, J. M., Lanham, N., et al. (2003). Querying Internet PIER. VLDB.
Izmalkov, S., Micali, S., & Lepinski, M. (2005). Rational secure computation ideal mechanism design.
FOCS 05: Proceedings 46th Annual IEEE Symposium Foundations Computer Science,
pp. 585595, Washington, DC, USA. IEEE Computer Society.
Jackson, M. O. (2000). Mechanism theory. Encyclopedia Life Support Systems. EOLSS Publishers.
Jackson, M. O. (2001). crash course Implementation theory. Social Choice Welfare, 18(4), 655708.
Katz, J., & Gordon, S. D. (2006). Rational secret sharing, revisited. Proc. Security Cryptography
Networks.
Kloks, T. (1994). Treewidth, Computations Approximations, Vol. 842 Lecture Notes Computer
Science. Springer.
Krishna, V. (2002). Auction Theory. Academic Press.
Kumar, A., Petcu, A., & Faltings, B. (2007). H-DPOP: Using hard constraints prune search space.
IJCAI07 - Distributed Constraint Reasoning workshop, DCR07, pp. 4055, Hyderabad, India.
Lavi, R., Mualem, A., & Nisan, N. (2003). Towards characterization truthful combinatorial auctions.
Proc. 44th Annual Symposium Foundations Computer Science.
Leyton-Brown, K., Pearson, M., & Shoham, Y. (2000). Towards universal test suite combinatorial
auction algorithms. Proceedings ACM Conference Electronic Commerce, EC-00, pp. 235
245.
Leyton-Brown, K., & Shoham, Y. (2006). test suite combinatorial auctions. Cramton, P., Shoham,
Y., & Steinberg, R. (Eds.), Combinatorial Auctions, chap. 18. MIT Press.
Lysyanskaya, A., & Triandopoulos, N. (2006). Rationality adversarial behavior multi-party computation. 26th Annual Int. Cryptology Conference (CRYPTO06).
Maheswaran, R. T., Tambe, M., Bowring, E., Pearce, J. P., & Varakantham, P. (2004). Taking DCOP
real world: Efficient complete solutions distributed multi-event scheduling. AAMAS-04.
Mailler, R., & Lesser, V. (2004). Solving distributed constraint optimization problems using cooperative mediation. Proceedings Third International Joint Conference Autonomous Agents MultiAgent
Systems (AAMAS 2004), 1, 438445.
753

fiP ETCU , FALTINGS , & PARKES

Mailler, R., & Lesser, V. (2005). Asynchronous partial overlay: new algorithm solving distributed
constraint satisfaction problems. Journal Artificial Intelligence Research (JAIR).
Mas-Colell, A., Whinston, M. D., & Green, J. R. (1995). Microeconomic Theory. Oxford University Press.
Mishra, D., & Parkes, D. (2007). Ascending price Vickrey auctions general valuations. Journal
Economic Theory, 132, 335366.
Modi, P. J., Shen, W.-M., Tambe, M., & Yokoo, M. (2005). ADOPT: Asynchronous distributed constraint
optimization quality guarantees. AI Journal, 161, 149180.
Monderer, D., & Tennenholtz, M. (1999). Distributed games: mechanisms protocols. Proc. 16th
National Conference Artificial Intelligence (AAAI-99), pp. 3237.
Moulin, H. (2007). Efficient, strategy-proof almost budget-balanced assignment. Tech. rep., Rice University.
Mualem, A. (2005). decentralized incentive compatible mechanisms partially informed environments.
Proc. ACM Conf. Electronic Commerce (EC).
Ostrovsky, R., Rajagopalan, S., & Vazirani, U. (1994). Simple efficient leader election full information model. STOC 94: Proceedings twenty-sixth annual ACM symposium Theory
computing, pp. 234242, New York, NY, USA. ACM Press.
Parkes, D. C., Kalagnanam, J. R., & Eso, M. (2001). Achieving budget-balance Vickrey-based payment
schemes exchanges. Proc. 17th International Joint Conference Artificial Intelligence (IJCAI01), pp. 11611168.
Parkes, D. C., & Shneidman, J. (2004). Distributed implementations Vickrey-Clarke-Groves mechanisms.
Proc. 3rd Int. Joint Conf. Autonomous Agents Multi Agent Systems, pp. 261268.
Parkes, D. C., & Ungar, L. H. (2000). Iterative combinatorial auctions: Theory practice. Proc. 17th
National Conference Artificial Intelligence (AAAI-00), pp. 7481.
Petcu, A. (2006). FRODO: FRamework Open Distributed constraint Optimization. Technical
report no. 2006/001, Swiss Federal Institute Technology (EPFL), Lausanne, Switzerland. http:
//liawww.epfl.ch/frodo/.
Petcu, A., & Faltings, B. (2005a). A-DPOP: Approximations distributed optimization. Proceedings
Eleventh International Conference Principles Practice Constraint Programming
(CP05), pp. 802806, Sitges, Spain.
Petcu, A., & Faltings, B. (2005b). DPOP: scalable method multiagent constraint optimization.
Proceedings 19th International Joint Conference Artificial Intelligence, IJCAI-05, pp. 266
271, Edinburgh, Scotland.
Petcu, A., & Faltings, B. (2006). PC-DPOP: partial centralization extension DPOP. Proceedings
Second International Workshop Distributed Constraint Satisfaction Problems, ECAI06, Riva
del Garda, Italy.
Petcu, A., & Faltings, B. (2007). MB-DPOP: new memory-bounded algorithm distributed optimization. Proceedings 20th International Joint Conference Artificial Intelligence, IJCAI-07,
Hyderabad, India.
Pietzuch, P., Ledlie, J., Shneidman, J., Roussopoulos, M., Welsh, M., & Seltzer, M. (2006). Network-Aware
Operator Placement Stream-Processing Systems. ICDE.
Rassenti, S. J., Smith, V. L., & Bulfin, R. L. (1982). combinatorial mechanism airport time slot allocation. Bell Journal Economics, 13, 402417.
Roberts, K. (1979). characterization implementable rules. Laffont, J.-J. (Ed.), Aggregation
Revelation Preferences, pp. 321348. North-Holland, Amsterdam.
Rosenschein, J. S., & Zlotkin, G. (1994). Designing conventions automated negotiation. AI Magazine.
Fall.
754

fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS

Sandholm, T. (2002). Algorithm optimal winner determination combinatorial auctions. Artificial
Intelligence, 135, 154.
Sandholm, T. W. (1993). implementation Contract Net Protocol based marginal-cost calculations. Proc. 11th National Conference Artificial Intelligence (AAAI-93), pp. 256262.
Sandholm, T. W. (1996). Limitations Vickrey auction computational multiagent systems. Second
International Conference Multiagent Systems (ICMAS-96), pp. 299306.
Shneidman, J., & Parkes, D. C. (2003). Rationality self-interest peer peer networks. 2nd Int.
Workshop Peer-to-Peer Systems (IPTPS03).
Shneidman, J., & Parkes, D. C. (2004). Specification faithfulness networks rational nodes. Proc.
23rd ACM Symp. Principles Distributed Computing (PODC04), St. Johns, Canada.
Silaghi, M.-C., Sam-Haroud, D., & Faltings, B. (2000). Asynchronous search aggregations.
AAAI/IAAI, pp. 917922, Austin, Texas.
Solotorevsky, G., Gudes, E., & Meisels, A. (1996). Modeling Solving Distributed Constraint Satisfaction
Problems (DCSPs). Proceedings Second International Conference Principles Practice
Constraint Programming (CP96), pp. 561562, Cambridge, Massachusetts, USA.
Sycara, K., Roth, S. F., Sadeh-Koniecpol, N., & Fox, M. S. (1991). Distributed constrained heuristic search.
IEEE Transactions Systems, Man, Cybernetics, 21(6), 14461461.
Wellman, M. P. (1993). market-oriented programming environment application distributed multicommodity flow problems. Journal Artificial Intelligence Research, 1, 123.
Wellman, M. P. (1996). Market-oriented programming: early lessons. Clearwater, S. H. (Ed.),
Market-Based Control: Paradigm Distributed Resource Allocation, chap. 4, pp. 7495. World
Scientific.
Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1992). Distributed constraint satisfaction formalizing distributed problem solving. International Conference Distributed Computing Systems, pp.
614621.
Yokoo, M., & Hirayama, K. (2000). Algorithms distributed constraint satisfaction: review. Autonomous
Agents Multi-Agent Systems, 3(2), 185207.
Yokoo, M., Sakurai, Y., & Matsubara, S. (2004). effect false-name bids combinatorial auctions:
New Fraud Internet Auctions. Games Economic Behavior, 46(1), 174188.
Zhang, W., & Wittenburg, L. (2003). Distributed breakout algorithm distributed constraint optimization
problems - DBArelax. Proceedings International Joint Conference Autonomous Agents
Multi Agent Systems (AAMAS-03), Melbourne, Australia.
Zlotkin, G., & Rosenschein, J. S. (1996). Mechanisms automated negotiation state oriented domains.
Journal Artificial Intelligence Research, 5, 163238.

755

fiJournal Artificial Intelligence Research 32 (2008) 879-900

Submitted 01/08; published 08/08

Latent Tree Models
Approximate Inference Bayesian Networks
Yi Wang
Nevin L. Zhang
Tao Chen

wangyi@cse.ust.hk
lzhang@cse.ust.hk
csct@cse.ust.hk

Department Computer Science Engineering
Hong Kong University Science Technology
Clear Water Bay Road, Kowloon, Hong Kong, China

Abstract
propose novel method approximate inference Bayesian networks (BNs).
idea sample data BN, learn latent tree model (LTM) data
oine, online, make inference LTM instead original BN.
LTMs tree-structured, inference takes linear time. meantime, represent
complex relationship among leaf nodes hence approximation accuracy often good.
Empirical evidence shows method achieve good approximation accuracy
low online computational cost.

1. Introduction
Latent tree models (LTMs) tree-structured Bayesian networks leaf nodes represent
manifest variables observed, internal nodes represent latent variables
hidden. previously known hierarchical latent class models (Zhang, 2004).
paper, distinguish variables nodes, assume variables
categorical.
Pearl (1988) rst identify LTMs potentially useful class models.
two reasons. First, inference LTMs takes time linear number nodes,
intractable general BNs. Second, latent variables capture complex relationships
among manifest variables. LTM, manifest variables mutually independent
given latent variables, eliminating latent variables results completely
connected BN.
study possibility exploiting two properties approximate inference
BNs. natural idea:
1. Oine: Obtain LTM approximates BN N sense joint
distribution manifest variables approximately equals joint distribution
variables N .
2. Online: Use instead N compute answers probabilistic queries.
cardinalities latent variables play crucial role approximation scheme.
determine inferential complexity inuence approximation accuracy. one extreme, represent BN exactly using LTM setting cardinalities latent
variables large enough. case, inferential complexity high.
c
2008
AI Access Foundation. rights reserved.

fiWang, Zhang, & Chen

extreme, set cardinalities latent variables 1. case, manifest
variables become mutually independent. inferential complexity lowest
approximation quality poorest. seek appropriate middle point
two extremes.
assume predetermined constraint cardinalities latent
variables control inferential complexity. develop algorithm nding LTM
satises constraint approximates original BN well. idea sample
data BN, learn LTM data. model structure determined
using hierarchical clustering manifest variables. step, two closely correlated sets
manifest variables grouped, new latent variable introduced account
relationship them. cardinalities latent variables set
predetermined value. parameters optimized using Expectation-Maximization
(EM) algorithm (Dempster, Laird, & Rubin, 1977).
empirically evaluated inference method array networks. possibility tradeo inferential complexity approximation accuracy
demonstrated adjusting cardinality constraints. turns method
able achieve good approximation accuracy cardinality becomes high.
compared method loopy belief propagation (LBP) (Pearl, 1988), standard approximate inference method successfully used many real world domains
(Frey & MacKay, 1997; Murphy, Weiss, & Jordan, 1999). Given amount time,
method achieves signicantly higher accuracy LBP cases. achieve
accuracy, LBP needs one three orders magnitude time method.
inference method fast LTM tree-structured. One also construct
Chow-Liu tree (Chow & Liu, 1968) approximate original BN use inference.
refer approach CL-based method. comparison method, CLbased method always faster, accurate method.
scheme exploits strong expressive capability latent variable models. One
course use latent variable models instead LTMs scheme. straightforward
choice latent class model (LCM) (Hagenaars & McCutcheon, 2002). LCM
LTM one latent variable1 . assumes local independence, is, manifest
variables mutually independent conditioning latent variable. also compare
method alternative. results show that, inferential complexity
constraints, method accurate LCM-based method.
noted approximate scheme needs lot time oine phase.
EM usually takes long time converge. Moreover, time complexity EM scales linearly sample size, set large possible
achieve high-quality approximation. Therefore, method suitable applications
allow long oine phase.
remainder paper organized follows. Section 2, review LTMs.
Section 3, describe method constructing LTMs approximate BNs. Section 4,
describe scheme approximate inference formally. Section 5 reports empirical results. Section 6 discusses relationship approach existing work. Finally,
Section 7, conclude paper point future directions.
1. machine learning community, LCM also referred naive Bayes model latent variable.

880

fiLatent Tree Models Approximate Inference Bayesian Networks

2. Latent Tree Model
LTM pair = (m, ). rst component denotes rooted tree set
cardinalities latent variables. refer model, rooted tree
model structure. second component denotes collection parameters
M. contains conditional probability table node given parent.
Let X set manifest variables set latent variables M,
respectively. use P (X, Y|m, ), PM (X, Y) short, denote joint distribution
represented M. Two LTMs marginally equivalent share
set manifest variables X PM (X) = PM (X). model includes another model
exists (m, ) (m , ) marginally equivalent.
Two models marginally equivalent includes vice versa.
Let |Z| denote cardinality variable Z. node Z m, use nb(Z)
denote set neighbors. model regular latent node ,
1. two neighbors, least one neighbors must latent node


Znb(Y ) |Z|
.
|Y | <
maxZnb(Y ) |Z|
2. two neighbors,

|Y |

Znb(Y ) |Z|

maxZnb(Y ) |Z|

.

model irregular, over-complicated. reduced regular model
marginally equivalent contains fewer parameters
(Zhang, 2004).
Q
|Z|

regular model, latent node saturated |Y | = maxZnb(Y ) |Z| . case, say
Znb(Y )
subsumes neighbors except one largest cardinality.

3. Approximating Bayesian Networks Latent Tree Models
section, study problem approximating BN LTM. Let N
BN approximated. Let X set variables N . LTM
approximation N , use X manifest variables, cardinalities
latent variables exceed predetermined threshold C. Figure 1(b), 1(c), 1(d)
show three example LTMs approximate BN Figure 1(a). used
illustrate various steps method.
Let PN (X) joint distribution represented N . approximation high
quality PM (X) close PN (X). measure quality approximation
KL divergence (Cover & Thomas, 1991)
D[PN (X)PM (X)] =


X

PN (X) log

PN (X)
.
PM (X)

objective nd LTM minimizes KL divergence, i.e.,
= arg min D[PN (X)PM (X)].


881

fiWang, Zhang, & Chen

X1 (2)

X2 (2)

Y5 (8)

Y4 (8)

X3 (2)

X4 (2)

Y2 (8)

Y1 (8)

X5 (2)

X1 (2)

X6 (2)

X4 (2)

(a) Bayesian network N

Y3 (8)

X5 (2)

X2 (2)

X3 (2)

X6 (2)

(b) Latent tree model

Y4 (8)

Y2 (8)

Y1 (4)

X4 (2)

X1 (2)

X5 (2)

Y2 (8)

Y3 (4)

X2 (2)

Y4 (8)

X3 (2)

X4 (2)

X6 (2)

(c) Regularized model

X5 (2)

X6 (2)

X1 (2)

X2 (2)

X3 (2)

(d) Simplified model

Figure 1: illustrative example. numbers within parentheses cardinalities
variables.

LTM consists two components, model parameters . Therefore,
optimization problem naturally decomposed two subproblems.

1. Find optimal model .

2. Optimize parameters given model m.

remainder section, discuss two subproblems details.
3.1 Parameter Optimization
start addressing second subproblem. Given model m, target nd
= arg min D[PN (X)P (X|m, )].


882

fiLatent Tree Models Approximate Inference Bayesian Networks

turns that, due presence latent variables, KL divergence dicult
directly minimize. seen expanding KL divergence follows,


PN (X)
P (X|m, )
X


=
PN (X) log PN (X)
PN (X) log P (X|m, )

D[PN (X)P (X|m, )] =

PN (X) log

X

=



X

PN (X) log PN (X)

X



PN (X) log

X



P (X, Y|m, ).



rst term last line neglected independent .
diculty lies maximizing second term. summation latent variables
appearing inside logarithm makes term indecomposable. Therefore, closed-form
solution obtained taking derivative term respect
setting zero.
transform problem asymptotically equivalent maximum likelihood estimation (MLE) problem. idea follows.
1. Generate data set N independently identically distributed samples
PN (X).
2. Find MLE respect D, i.e.,
= arg max P (D|m, ).


well known converges almost surely sample size N approaches
innity (Huber, 1967).
discuss implementation solution. start generating
PN (X). Since PN (X) represented BN N , use logic sampling (Henrion, 1988)
task. Specically, generate piece sample PN (X), process nodes
topological ordering2 . handling node X, sample value according
conditional distribution P (X|(X) = j), (X) denotes set parents X j
denote values sampled earlier. obtain D, repeat procedure
N times.
Given D, next step nd MLE. Note values latent variables
missing D. thus use EM algorithm (Dempster et al., 1977). Starting
random guess, EM algorithm iteratively improves estimate change
loglikelihoods two consecutive iterations smaller predetermined threshold.
practical issue EM converge local maxima likelihood surface.
local maxima far global maxima, thus poor approximations
. Fortunately, local maxima issue severe LTMs (Wang & Zhang, 2006).
practice, one also use various techniques multiple restart (Chickering &
Heckerman, 1997) data permutation (Elidan et al., 2002) alleviate issue.
Note EM takes long time converge, especially sample size N large.
algorithm expensive oine phase.
2. topological ordering sorts nodes DAG node always precedes children.

883

fiWang, Zhang, & Chen

3.2 Exhaustive Search Optimal Model
consider rst subproblem, i.e., nd best model . straightforward
way solve problem exhaust possible models, nd optimal parameters
model m, compute KL divergence D[PN (X)P (X|m, )], return
model minimum KL divergence.
problem solution high computational complexity. Given set
manifest variable X, innitely many models. One always obtain new models
inserting latent variables existing model. show Section 3.5,
sucient consider nite subspace, i.e., subspace regular models. However,
still super-exponentially many regular models (Zhang, 2004). model, need
optimize parameters running EM, time-consuming process. Therefore,
exhaustive search computationally infeasible. following 4 subsections,
present heuristic method.
3.3 Heuristic Construction Model Structure
rst present heuristic determining model structure. LTM, two manifest
variables called siblings share parent. heuristic based two
ideas: (1) LTM M, siblings generally closely correlated variables
located far apart; (2) good approximation N , two variables Xi
Xj closely correlated closely correlated N .
examine pair variables N , pick two variables closely correlated,
introduce latent variable parent M.
measure strength correlation pair variables Xi Xj
mutual information (Cover & Thomas, 1991)
(Xi ; Xj ) =



PN (Xi , Xj ) log

Xi ,Xj

PN (Xi , Xj )
.
PN (Xi )PN (Xj )

compute (Xi ; Xj ), one need make inference N . could computationally
hard rst place. use sampling technique address issue. Specically,
generate data set N samples BN N , compute empirical mutual
; Xj ) using empirical distribution P (Xi , Xj ) based D. strong
information I(X
; Xj ) almost surely converge (Xi ; Xj ) sample
law large numbers, I(X
size N goes innity.
use BN shown Figure 1(a) example illustrate idea.
contains 6 binary variables X1 , X2 , . . ., X6 . Suppose empirical mutual information
based data set presented Table 1. discussed above, regard
approximation mutual information variables N hence regard
approximation mutual information variables nal LTM
construct. nd X4 X6 pair largest mutual information.
Therefore, create latent variable Y1 make parent X4 X6 .
next step nd, among Y1 , X1 , X2 , X3 , X5 , pair variables
largest mutual information M. one diculty: Y1 original Bayesian
network hence observed data set. mutual information Y1
884

fiLatent Tree Models Approximate Inference Bayesian Networks

X1
X2
X3
X4
X5
X6

X1
0.0000
0.0003
0.0015
0.0017
0.0102

X2
0.0971
0.0654
0.0311
0.0252

X3
0.0196
0.0086
0.0080

X4
0.1264
0.1817

X5
0.0486

Table 1: Empirical mutual information manifest variables
variables cannot computed directly. hence seek approximation.
nal model M, Y1 would d-separate X4 X6 variables. Therefore,
X {X1 , X2 , X3 , X5 },
IM (Y1 ; X) IM (X4 ; X), IM (Y1 ; X) IM (X6 ; X).
hence approximate IM (Y1 ; X) using lower bound
max{IM (X4 ; X), IM (X6 ; X)}.
Back running example, estimated mutual information Y1 X1 ,
X2 , X3 , X5 presented Table 2. see next pair pick Y1 X5 .
introduce latent variable Y2 parent Y1 X5 . process continues. nal
model structure binary tree shown Figure 1(b).
Y1

X1
0.0102

X2
0.0654

X3
0.0196

X5
0.1264

Table 2: Estimated mutual information Y1 manifest variables
3.4 Cardinalities Latent Variables
obtaining model structure, next step determine cardinalities
latent variables. set cardinalities latent variables predetermined value
C. following, discuss choice C inuences quality approximation
inferential eciency.
rst discuss impact value C
approximation quality. start
considering case C equals Cmax = XX |X|, i.e., product
cardinalities manifest variables. case, latent variable viewed
joint variable manifest variables. therefore set parameters
P (X|m, ) = PN (X). is, capture interactions among manifest
variables.
happens decrease C? shown approximation quality
degrade. Let model obtained value C another model obtained
smaller value C . easy see includes . following lemma states
approximation quality better m.
885

fiWang, Zhang, & Chen

Lemma 1 Let P (X) joint probability distribution X. Let two models
manifest variables X. includes ,
min D[P (X)P (X|m, )] min D[P (X)P (X|m , )].


Proof: Dene



= arg min D[P (X)P (X|m , )].


includes , must parameters
P (X|m, ) = P (X|m , ).
Therefore,
min D[P (X)P (X|m, )] D[P (X)P (X|m, )]


= D[P (X)P (X|m , )]

= min D[P (X)P (X|m , )]


Q.E.D.
mentioned earlier, C large enough, model capture interactions
among manifest variables hence represent joint distribution PN (X) exactly.
C large enough, represent PN (X) approximately. According
previous discussion, C decreases, approximation accuracy (in terms KL divergence)
gradually degrade, indicating model capture less less interactions among
manifest variables. worst case occurs C = 1. case, interactions
lost. approximation accuracy poorest.
parameter C also determines computational cost making inference m.
use clique tree propagation (CTP) algorithm inference. measure cost
inferential complexity, dened sum clique sizes clique
tree m. given

|X| C.
(1)
(|X| 2) C 2 +
XX

Note |X| number manifest variables, |X| cardinality manifest
variable X. Therefore, one control inferential complexity changing value
C. smaller value C, lower complexity.
summary, one achieve tradeo approximation quality
inferential complexity resultant model tuning parameter C. Figure 1(b),
set C = 8.
3.5 Model Regularization
Suppose obtained model using technique described Section 3.3
setting cardinalities latent variables certain value. following two
subsections, show sometimes possible simplify without compromising
approximation quality.
886

fiLatent Tree Models Approximate Inference Bayesian Networks

rst notice could irregular. example, let us consider model
Figure 1(b). constructed approximation BN N Figure 1(a)
C = 8. checking latent variables, nd Y5 violates regularity condition. two neighbors |Y5 | |X1 ||Y4 |/ max{|X1 |, |Y4 |}. Y1 Y3 also
violate regularity condition |Y1 | > |X4 ||X6 ||Y2 |/ max{|X4 |, |X6 |, |Y2 |}
|Y3 | > |X2 ||X3 ||Y4 |/ max{|X2 |, |X3 |, |Y4 |}. following proposition suggests irregular models always simplied become regular.
Proposition 1 irregular model, must exists model lower
inferential complexity
min D[PN (X)P (X|m, )] = min D[PN (X)P (X|m , )].




(2)

Proof: Let latent variable violates regularity condition. Denote
neighbors Z1 , Z2 , . . . , Zk . dene another model follows:
1. two neighbors, remove connect Z1 Z2 .
2. Otherwise, replace saturated latent variable , i.e.,
k

i=1 |Zi |
|Y | =
.
maxki=1 |Zi |
shown Zhang (2004), parameters m, exists parameters
(m, ) (m , ) marginally equivalent. reverse also true.
Therefore, marginally equivalent. Equation 2 thus follows Lemma 1.
show inferential complexity lower m, compare
clique trees . Consider aforementioned two cases:
1. two neighbors. case, cliques {Y, Z1 } {Y, Z2 } clique tree
replaced {Z1 , Z2 } clique tree . Assume |Z2 | |Z1 |.
dierence sum clique sizes
sum(m) sum(m ) = |Y ||Z1 | + |Y ||Z2 | |Z1 ||Z2 |
|Z1 ||Z1 | + |Z1 ||Z2 | |Z1 ||Z2 |
= |Z1 ||Z1 |
> 0.
2. two neighbors. case, = 1, 2, . . . , k, clique {Y, Zi }
clique tree replaced smaller clique {Y , Zi } clique tree .
cases, inferential complexity lower m.
Q.E.D.
proof Proposition 1 presents way handle latent variable violates
regularity condition, i.e., either eliminating decreasing cardinality. regularize
irregular model, handle latent variables order created
887

fiWang, Zhang, & Chen

Section 3.3. following, use irregular model Figure 1(b) demonstrate
regularization process.
begin latent variable Y1 . three neighbors violates regularity
condition. decrease cardinality |X4 ||X6 ||Y2 |/ max{|X4 |, |X6 |, |Y2 |} = 4.
consider Y2 . satises regularity condition hence changes made.
next latent variable examine Y3 . violates regularity condition. decrease
cardinality |X2 ||X3 ||Y4 |/ max{|X2 |, |X3 |, |Y4 |} = 4. change Y4
cause irregularity. last, remove Y5 , two neighbors
violates regularity condition, connect Y4 X1 . end regular
model shown Figure 1(c).
3.6 Simplifications
regularization, sometimes still opportunities model simplication.
Take model Figure 1(c) example. contains two adjacent latent variables
Y1 Y2 . variables saturated. Y1 subsumes X4 X6 , Y2 subsumes Y1
X5 . Y2 viewed joint variable Y1 X5 , Y1 turn viewed
joint variable X4 X6 . Intuitively, eliminate Y1 directly make Y2 joint
variable X4 , X5 , X6 . intuition formalized following proposition.
Proposition 2 Let model one latent node. Let Y1 Y2 two
adjacent latent nodes. Y1 Y2 saturated Y2 subsumes Y1 , exist
another model marginally equivalent lower inferential complexity
m. Therefore,
min D[PN (X)P (X|m, )] = min D[PN (X)P (X|m , )].




Proof: enumerate neighbors Y1 Y2 , Z11 , Z12 , . . . , Z1k , neighbors
Y2 Y1 , Z21 , Z22 , . . . , Z2l . Dene another model removing Y1 connecting Z11 , Z12 , . . . , Z1k Y2 . See Figure 2. prove marginally
equivalent, inferential complexity lower m.
start proving marginal equivalence. technical convenience, work
unrooted models. unrooted model model directions edges
dropped. Parameters unrooted model include potential edge model.
potential non-negative function two variables connected edge.
concept marginal equivalence dened way rooted models.
shown Zhang (2004), model marginally equivalent unrooted version.
Therefore, prove marginal equivalence , sucient show
unrooted versions marginally equivalent. simplicity, abuse
denote unrooted models. also use f () denote potential ,
g() denote potential .
Note Y1 Y2 saturated, Y2 subsumes Y1 . variables
less two states, implies that:
1. Y1 subsumes Z11 , Z12 , . . . , Z1k .
2. Suppose |Z2l | = maxlj=1 |Z2j |. Y2 subsumes Z21 , Z22 , . . . , Z2l1 .
888

fiLatent Tree Models Approximate Inference Bayesian Networks

Z11

Z12

Z11

Z22

Z12

Z21

Z22

Y2

...

...

Z1k

...

Y2

...

Y1

Z21

Z1k

Z2l

Z2l

(a)

(b)

Figure 2: simplication. (a) part model contains two adjacent
saturated latent nodes Y1 Y2 , Y2 subsuming Y1 . (b) Simplied model
Y1 eliminated.

Therefore, state Y1 written y1 =< z11 , z12 , . . . , z1k >, state Y2
written y2 =< y1 , z21 , z22 , . . . , z2l1 >. latter expanded
y2 =< z11 , z12 , . . . , z1k , z21 , z22 , . . . , z2l1 >.
rst show includes m. Let parameters m. dene parameters
follows:
Potential edge Y2 Z2l :
g(Y2 =< z11 , z12 , . . . , z1k , z21 , z22 , . . . , z2l1 >, Z2l = z2l )


=

f (Y1 , Y2 )

k


f (Y1 , Z1i = z1i )

i=1

Y1 ,Y2

l


f (Y2 , Z2j = z2j ).

j=1

Potential edge Y2 Z1i , = 1, 2, . . . , k:
g(Y2 =< z11 , z12 , . . . , z1k , z21 , z22 , . . . , z2l1 >, Z1i =


z1i
)


=


1 z1i = z1i
0 otherwise

Potential edge Y2 Z2j , j = 1, 2, . . . , l 1:
g(Y2 =< z11 , z12 , . . . , z1k , z21 , z22 , . . . , z2l1 >, Z2j =


z2j
)


=


1 z2j = z2j
0 otherwise

Set potentials .
easy verify

Y1 ,Y2

Therefore,

f (Y1 , Y2 )

k

i=1

f (Y1 , Z1i )

l


f (Y2 , Z2j ) =

j=1

k

Y2

g(Y2 , Z1i )

i=1

P (X|m, ) = P (X|m , ).
889

l


g(Y2 , Z2j ).

(3)

j=1

(4)

fiWang, Zhang, & Chen

Next, prove includes . Given parameters , dene parameters
follows:
Potential edge Y1 Y2 :
f (Y1 =< z11 , z12 , . . . , z1k >, Y2 = y2 ) =

k


g(Y2 = y2 , Z1i = z1i ).

i=1

Potential edge Y1 Z1i , = 1, 2, . . . , k:
f (Y1 =< z11 , z12 , . . . , z1k >, Z1i =


z1i
)


=


1 z1i = z1i
0 otherwise

Set potentials .
veried Equation 3 4 also hold. Therefore, marginally
equivalent.
compare inferential complexity . According construction
, clique tree dierent clique tree contains one less
clique {Y1 , Y2 } replaces clique {Y1 , Z1i } {Y2 , Z1i } = 1, 2, . . . , k. Therefore,
dierence sum clique sizes


|Y1 ||Z1i |
|Y2 ||Z1i |
sum(m) sum(m ) = |Y1 ||Y2 | +
= |Y2 |





|Z1i | +







|Y1 ||Z1i |





|Y2 ||Z1i |






|Z1i |) +
|Y1 ||Z1i |.
= |Y2 |( |Z1i |







rst term last line non-negative |Z1i | |Z1i | |Z1i | 2
= 1, 2, . . . , k. Therefore, inferential complexity always lower
Z1i nontrivial.
Q.E.D.
Given regularized model, check pair adjacent latent variables apply
Proposition 2 eliminate redundant latent variables. use model Figure 1(c)
example demonstrate process. rst pair check Y1 Y2 .
saturated Y2 subsumes Y1 . thus remove Y1 connect Y2 X4
X6 . check Y3 Y4 . turns Y3 redundant. Therefore, remove
connect Y4 X2 X3 . last pair check Y2 Y4 .
saturated, neither subsumes other. Hence, cannot removed.
nal model shown Figure 1(d).


3.7 Algorithm LTAB
summarize, outlined algorithm approximating BNs using LTMs. call
algorithm LTAB, shorthand Latent Tree Approximation Bayesian network.
3 inputs: BN N , predetermined cardinality C latent variables, sample
890

fiLatent Tree Models Approximate Inference Bayesian Networks

size N . output LTAB LTM approximates PN (X), joint probability
distribution represented N . LTAB briey described follows.
1. Generate data set N i.i.d. samples PN (X). (Section 3.1)
2. Obtain LTM structure performing hierarchical clustering variables, using
empirical mutual information based similarity measure. (Section 3.3)
3. Set cardinalities latent variables C simplify model. (Section 3.4 3.6)
4. Optimize parameters running EM. (Section 3.1)
5. Return resultant LTM.

4. LTM-based Approximate Inference
focus paper approximate inference Bayesian networks. propose
following two-phase method:
1. Oine: Given BN N , use LTAB construct approximation M. sample size
N set large possible, cardinality C determined
meet requirement inferential complexity.
2. Online: Make inference instead N . specically, given piece evidence E = e querying variable Q, return PM (Q|E = e) approximation
PN (Q|E = e).

5. Empirical Results
section, empirically evaluate approximate inference method. rst examine
impact sample size N cardinality C performance method.
compare method CTP, LBP, CL-based method, LCM-based
method.
used 8 networks experiments. listed Table 3. CPCS54
subset CPCS network (Pradhan et al., 1994). networks available
http://www.cs.huji.ac.il/labs/compbio/Repository/. Table 3 also reports characteristics networks, including number nodes, average/max indegree
cardinality nodes, inferential complexity (i.e., sum clique sizes
clique tree). networks sorted ascending order respect inferential
complexity.
network, simulated 500 pieces evidence. piece evidence set
leaf nodes sampling based joint probability distribution.
used CTP algorithm approximate inference methods compute posterior
distribution non-leaf node conditioned piece evidence. accuracy
approximate method measured average KL divergence exact
approximate posterior distributions query nodes evidence.
algorithms experiments implemented Java run machine
Intel Pentium IV 3.2GHz CPU 1GB RAM.
891

fiWang, Zhang, & Chen

Network
ALARM
WIN95PTS
HAILFINDER
INSURANCE
CPCS54
WATER
MILDEW
BARLEY

Number
Nodes
37
76
56
27
54
32
35
48

Average/Max
Indegree
1.24/4
1.47/7
1.18/4
1.93/3
2/9
2.06/5
1.31/3
1.75/4

Average/Max
Cardinality
2.84/4
2/2
3.98/11
3.3/5
2/2
3.62/4
17.6/100
8.77/67

Inferential
Complexity
1,038
2,684
9,706
29,352
109,208
3,028,305
3,400,464
17,140,796

Table 3: Networks characteristics.
5.1 Impact N C
discussed impact N C performance method Section 3.
subsection empirically veries claims.
Three sample sizes chosen experiments: 1k, 10k, 100k. network,
also chose set C. LTMs learned using LTAB dierent combination
values N C. parameter learning, terminated EM either
improvement loglikelihoods smaller 0.1, algorithm ran two months.
multiple restarting strategy Chickering Heckerman (1997) used avoid
local maxima. number starting points set 16.
running time LTAB plotted Figure 3. y-axes denote time hours,
x-axes denote parameter C LTAB. three curves correspond dierent
values N . general, running time increases N C, ranging seconds
weeks. settings, EM failed converge two months. settings indicated
arrows plots. emphasize LTAB executed oine running time
confused time online inference, reported next.
obtaining LTMs, used clique tree propagation make inference.
approximation accuracy shown Figure 4. y-axes denote average KL divergence, x-axes still denote parameter C LTAB. curves one
horizontal line plot. three curves labeled LTM method,
correspond three sample sizes used. remaining two curves horizontal
line approximate inference methods. discuss Sections 5.3
5.5.
rst examine impact sample size comparing corresponding curves
plot. nd that, general, curves larger samples located
smaller ones. shows approximation accuracy increases size
training data.
see impact C, examine individual curve left right. According
discussion, curve expected drop monotonically C increases.
generally true results sample size 100k. sample sizes 1k 10k, however,
cases approximation becomes poorer C increases. See Figure
4(e) 4(f). phenomenon conict claims. C increases,
892

fiLatent Tree Models Approximate Inference Bayesian Networks

Time (hour)

10
10
10
10
10

2

2

10
N=1k
N=10k
N=100k

1

1

10
Time (hour)

10

0

1

2

0

10

1

10

2

10

3

3

1

2

4

10

8

1

2

4
C

C

(a) ALARM
10

10

8

16

(b) WIN95PTS

4

3

10

2

10

10

10

Time (hour)

Time (hour)

1

0

10

1

10

2

4

3

1

4

16

10

32

1

4

C

(c) HAILFINDER

Time (hour)

10

10

10

10

4

4

10

2

2

10

0

2

2

10

4

1

0

10

4

2

4
C

8

10

16

1

4

10

10

10

64

16

64

(f) WATER

4

4

10

2

2

10
Time (hour)

Time (hour)

10

16
C

(e) CPCS54
10

32

(d) INSURANCE

Time (hour)

10

16
C

0

2

2

10

4

1

0

10

4

4

16

10

64

C

1

4
C

(g) MILDEW

(h) BARLEY

Figure 3: Running time LTAB. Settings EM converge indicated
arrows.
893

fiWang, Zhang, & Chen

10

10

0

10

Average KL divergence

Average KL divergence

10

1

2

LTM (1k)
LTM (10k)
LTM (100k)
LBP
CL (100k)
LCM (100k)

1

2

4

10

10

10

8

0

1

2

3

1

C

(a) ALARM

10

10

10

10

0

10

1

2

3

4

1

4
C

4

16

10

10

10

32

1

2

3

1

4

0

2

Average KL divergence

Average KL divergence

10

3

4

5

1

2

4
C

8

10

10

10

10

16

1

2

3

4

1

4

10

64

16

64

(f) WATER

0

10

Average KL divergence

Average KL divergence

10

16
C

(e) CPCS54
10

32

(d) INSURANCE
10

10

16
C

(c) HAILFINDER

10

16

0

C

10

8

(b) WIN95PTS

Average KL divergence

Average KL divergence

10

2

1

10

0

1

2

1

4

16

10

64

2

1

C

4
C

(g) MILDEW

(h) BARLEY

Figure 4: Approximation accuracy.
894

fiLatent Tree Models Approximate Inference Bayesian Networks

expressive power learned LTM increases. tends overt data.
hand, empirical distribution small set data may signicantly deviate
joint distribution BN. also suggests sample size set
large possible.
Finally, let us examine impact N C inferential complexity. Figure 5
plots running time dierent methods answer queries. now,
consider three curves labeled LTM. seen three curves
overlap plots. implies running time independent sample size
N . hand, curves monotonically increasing. conrms claim
inferential complexity positively dependent C.
following subsections, stated explicitly otherwise, consider
results N = 100k largest C. settings, method achieves
highest accuracy.
5.2 Comparison CTP
compare method CTP, state-of-the-art exact inference algorithm.
rst concern that, accurate method. examining Figure 4, argue
method always achieves good approximation accuracy: HAILFINDER, CPCS54, WATER,
average KL divergence method around less 103 ; networks,
average KL divergence around less 102 .
next compare inferential eciency method CTP algorithm.
running time CTP denoted dashed horizontal lines plots Figure 5.
seen method ecient CTP algorithm. particular,
networks highest inferential complexity, method faster CTP
two three orders magnitude.
summarize, results suggest method achieve good approximation
accuracy low computational cost.
5.3 Comparison LBP
compare method LBP. latter iterative algorithm. used
anytime inference method running specic number iterations. rst set
experiments, let LBP run long method compare approximation
accuracy. network value C. accuracy LBP
denoted curves labeled LBP Figure 4. comparing curves LTM
curves N = 100k, see method achieves signicantly higher accuracy
LBP cases: WATER, dierence average KL divergence three orders
magnitude; networks, dierence one order magnitude.
HAILFINDER C = 32, LBP two times accurate method. However,
method also achieves good approximation accuracy case. average KL divergence
smaller 103 . Finally, noticed LBP curves horizontal lines CPCS54,
MILDEW, BARLEY. investigation cases shows LBP nished one
iteration given time period.
next examine much time takes LBP achieve level accuracy
method. piece evidence, ran LBP average KL divergence
895

fiWang, Zhang, & Chen

10

10

1

10
LTM (1k)
LTM (10k)
LTM (100k)
CTP
LBP
CL (100k)
LCM (100k)

0

Time (second)

Time (second)

10

10

10

2

1

0

1

1

2

4

10

8

1

1

C

(a) ALARM
10

2

10

1

Time (second)

Time (second)

10

0

10
10
10

10

1

1

4

4
C

16

10

32

2

1

0

1

2

1

4

3

10

4

2

Time (second)

Time (second)

10

1

0

10

10

2

0

1

1

2

4
C

8

10

16

2

1

4

5

10

3

Time (second)

Time (second)

10

1

10

10

10

1

1

4

64

16

64

(f) WATER

10
10

16
C

(e) CPCS54
10

32

(d) INSURANCE

10

10

16
C

(c) HAILFINDER

10

16

3

C

10

8

(b) WIN95PTS

10
10

2

16

10

64

C

6

4

2

0

2

1

4
C

(g) MILDEW

(h) BARLEY

Figure 5: Running time online inference.
896

fiLatent Tree Models Approximate Inference Bayesian Networks

comparable method number iterations exceeds 100.
running time LBP denoted curves labeled LBP Figure 5. Comparing
curves LTM curves, found LBP takes much time
method: MILDEW, LBP slower method three orders magnitude;
networks except HAILFINDER, LBP slower one two orders magnitude;
HAILFINDER C = 32, running time two methods similar. results
show method compares favorably LBP networks examined.
5.4 Comparison CL-based Method
subsection, compare method CL-based method. specically,
network, learn tree model 100k samples using maximum spanning
tree algorithm developed Chow Liu (1968). use learned tree model
answer queries.
approximation accuracy CL-based method shown solid horizontal lines
plots Figure 4. Comparing CL-based method, method achieves higher
accuracy networks except MILDEW. INSURANCE, WATER, BARLEY, differences signicant. MILDEW, method competitive CL-based method.
meantime, notice CL-based method achieves good approximations
networks except BARLEY. average KL divergence around less 102 .
obvious advantage CL-based method high eciency. seen
plots Figure 5. plots, CL line locates second data point
LTM curve. exception MILDEW, running time CL-based
method long method C = 16.
summary, results suggest CL-based method good choice approximate inference online inference time limited. Otherwise, method
attractive able produce accurate results time allowed.
5.5 Comparison LCM-based Method
Lowd Domingos (2005) previously investigated use LCM density estimation. Given data set, determine cardinality latent variable using
hold-out validation, optimize parameters using EM. shown learned
LCM achieves good model separate testing set. LCM also used answer
simulated probabilistic queries results turn good.
Inspired work, also learned set LCMs 100k samples
compared LTMs approximate inference task. learning strategy
slightly dierent. Since LCM special case LTM, inferential complexity also
controlled changing cardinality latent variable. experiments, set
cardinality sum clique sizes clique tree LCM roughly
LTM learned chosen C. way, inferential complexity
two models comparable. veried examining LCM curves
Figure 5. optimize parameters LCM using EM setting
case LTM.
shown Figure 4, ALARM, WIN95PTS, CPCS54, WATER, BARLEY, LCM
curves located LTM curves. is, method consistently outperforms
897

fiWang, Zhang, & Chen

LCM-based method C. HAILFINDER MILDEW, method worse
LCM-based method C small. C becomes large, method begins
win. INSURANCE, performance two methods close. results
suggest unrestricted LTM suitable approximation inference LCM does.

6. Related Work
idea approximating complex BNs simple models using latter make inference investigated previously. existing work mainly falls two categories.
work rst category approximates joint distributions BNs uses
approximation answer probabilistic queries. contrast, work second
category query-specic. assumes evidence known directly approximates
posterior distribution querying nodes.
method falls rst category. investigate use LTMs
framework. possibility also studied Pearl (1988) Sarkar (1995). Pearl
(1988) develops algorithm constructing LTM marginally equivalent
joint distribution P (X), assuming LTM exists. Sarkar (1995) studies build
good LTMs approximations amenable. methods, however,
deal cases binary variables.
Researchers also explored use models. Chow Liu (1968) consider
tree-structured BNs without latent variables. develop maximum spanning tree
algorithm eciently construct tree model closest original BN terms
KL divergence. Lowd Domingos (2005) learn LCM summarize data set.
cardinality latent variable determined logscore hold-out set
maximized. show learned model achieves good model separate testing
set, provide accurate answers simulated probabilistic queries. work,
approximation quality inferential complexity learned model xed.
method, hand, provides parameter C let users make tradeo
approximation quality inferential complexity.
work second category mainly carried variational framework.
mean eld method (Saul, Jaakkola, & Jordan, 1996) assumes querying nodes
mutually independent. constructs independent model close posterior distribution. improvement mean eld method, structured mean eld
method (Saul & Jordan, 1996) preserves tractable substructure among querying nodes,
rather neglecting interactions. Bishop et al. (1997) consider another improvement,
i.e., mixtures mean eld distributions. essentially ts LCM posterior distribution. methods directly approximate posterior distributions. Therefore,
might accurate method used make inference. However,
methods evidence-specic construct approximations online. Moreover, involve
iterative process optimizing variational parameters. Consequently, online running time unpredictable. method, contrast, one determine inferential
complexity beforehand.
898

fiLatent Tree Models Approximate Inference Bayesian Networks

7. Concluding Remarks
propose novel scheme BN approximate inference using LTMs. scheme
one trade approximation accuracy inferential complexity.
scheme achieves good accuracy low costs networks examined.
particular, consistently outperforms LBP. also show LTMs superior LCMs
used approximate inference.
current bottleneck oine phase parameter learning. used EM algorithm
optimize parameters, known time consuming. problem especially
severe parameter C sample size large. One way speed parameter
learning adapt agglomerative clustering technique learning cardinality
latent variable data (Elidan & Friedman, 2001). basic idea complete
training data setting cardinality latent variable large enough assigning
record latent state. step, one selects two states latent variable
merge. process repeats (penalized) likelihood ceases improve.
parameter learning problem, terminate process desired cardinality
C achieved. also need deal multiple latent variables. Since data set
completed, expect method yield good starting point EM short
time, turn drastically shorten oine phase.

Acknowledgments
thank Haipeng Guo Yiping Ke insightful discussions. also grateful
anonymous reviewers valuable comments suggestions earlier version
paper. Research work supported Hong Kong Grants Council Grants
#622105 #622307, National Basic Research Program China (aka 973
Program) project No. 2003CB517106. work completed rst author
leave HKUST Fok Ying Tung Graduate School, Guangzhou, China.

References
Bishop, C. M., Lawrence, N., Jaakkola, T., & Jordan, M. I. (1997). Approximating posterior
distributions belief networks using mixtures. Proceedings 10th Conference
Advances Neural Information Processing Systems, pp. 416422.
Chickering, D. M., & Heckerman, D. (1997). Ecient approximations marginal
likelihood Bayesian networks hidden variables. Machine Learning, 29, 181
212.
Chow, C. K., & Liu, C. N. (1968). Approximating discrete probability distributions
dependence trees. IEEE Transactions Information Theory, 14 (3), 462467.
Cover, T. M., & Thomas, J. A. (1991). Elements Information Theory. Wiley-Interscience,
New York.
Dempster, A. P., Laird, N. M., & Rubin, D. R. (1977). Maximum likelihood incomplete data via EM algorithm. Journal Royal Statistical Society. Series B
(Methodological), 39 (1), 138.
899

fiWang, Zhang, & Chen

Elidan, G., & Friedman, N. (2001). Learning dimensionality hidden variables.
Proceedings 17th Conference Uncertainty Artificial Intelligence.
Elidan, G., Ninio, M., Friedman, N., & Schuurmans, D. (2002). Data perturbation
escaping local maxima learning. Proceedings 18th National Conference
Artificial Intelligence, pp. 132139.
Frey, B. J., & MacKay, D. J. C. (1997). revolution: belief propagation graphs
cycles. Advances Neural Information Processing Systems, Vol. 10.
Hagenaars, J. A., & McCutcheon, A. L. (2002). Applied Latent Class Analysis. Cambridge
University Press, Cambridge, UK.
Henrion, M. (1988). Propagating uncertainty Bayesian networks probabilistic logic
sampling. Uncertainty Artificial Intelligence 2, pp. 317324.
Huber, P. J. (1967). behavior maximum likelihood estimates nonstandard
conditions. Proceedings 5th Berkeley Symposium Mathematical Statistics
Probability, pp. 221233.
Lowd, D., & Domingos, P. (2005). Naive Bayes models probability estimation.
Proceedings 22nd International Conference Machine Learning, pp. 529536.
Murphy, K. P., Weiss, Y., & Jordan, M. I. (1999). Loopy belief propagation approximate
inference: empirical study. Proceedings 15th Conference Uncertainty
Artificial Intelligence, pp. 467475.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann Publishers, San Mateo, CA.
Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering
large belief networks. Proceedings 10th Conference Uncertainty
Artificial Intelligence, pp. 484490.
Sarkar, S. (1995). Modeling uncertainty using enhanced tree structures expert systems.
IEEE Transactions Systems, Man, Cybernetics, 25 (4), 592604.
Saul, L. K., Jaakkola, T., & Jordan, M. I. (1996). Mean eld theory sigmoid belief
networks. Journal Artificial Intelligence Research, 4, 6176.
Saul, L. K., & Jordan, M. I. (1996). Exploiting tractable substructures intractable
networks. Advances Neural Information Processing Systems, Vol. 8, pp. 486
492.
Wang, Y., & Zhang, N. L. (2006). Severity local maxima em algorithm: Experiences
hierarchical latent class models. Proceedings 3rd European Workshop
Probabilistic Graphical Models, pp. 301308.
Zhang, N. L. (2004). Hierarchical latent class models cluster analysis. Journal Machine
Learning Research, 5 (6), 697723.

900

fiJournal Artificial Intelligence Research 32 (2008) 825877

Submitted 06/07; published 08/08

Qualitative System Identification Imperfect Data
George M. Coghill

g.coghill@abdn.ac.uk

School Natural Computing Sciences
University Aberdeen, Aberdeen, AB24 3UE. UK.

Ashwin Srinivasan

ashwin.srinivasan@in.ibm.com

IBM India Research Laboratory
4, Block C, Institutional Area
Vasant Kunj Phase II, New Delhi 110070, India.

Department CSE Centre Health Informatics
University New South Wales, Kensington
Sydney, Australia.

Ross D. King

rdk@aber.ac.uk

Deptartment Computer Science
University Wales, Aberystwyth, SY23 3DB. UK.

Abstract
Experience physical sciences suggests realistic means understanding complex systems use mathematical models. Typically,
come mean identification quantitative models expressed differential equations.
Quantitative modelling works best structure model (i.e., form
equations) known; primary concern one estimating values parameters model. complex biological systems, model-structure rarely known
modeler deal model-identification parameter-estimation.
paper concerned providing automated assistance first problems. Specifically, examine identification machine structural relationships
experimentally observed variables. relationship expressed
form qualitative abstractions quantitative model. qualitative models may
provide clues precise quantitative model, also assist understanding essence model. position paper background knowledge
incorporating system modelling principles used constrain effectively set
good qualitative models. Utilising model-identification framework provided Inductive Logic Programming (ILP) present empirical support position using series
increasingly complex artificial datasets. results obtained qualitative
quantitative data subject varying amounts noise different degrees sparsity.
results also point presence set qualitative states, term kernel
subsets, may necessary qualitative model-learner learn correct models.
demonstrate scalability method biological system modelling identification
glycolysis metabolic pathway data.

1. Introduction
growing recognition research life sciences increasingly concerned ways relating large amounts biological physical data structure
function higher-level biological systems. Experience physical sciences suggests
c
2008
AI Access Foundation. rights reserved.

fiCoghill, Srinivasan, & King

realistic means understanding complex systems use
mathematical models. topical example provided Physiome Project seeks
utilise data obtained sequencing human genome understand describe
human organism using models that: . . . include everything diagrammatic schema,
suggesting relationships among elements composing system, fully quantitative, computational models describing behaviour physiological systems organisms
response environmental change (see http://www.physiome.org/). paper concerned computational tool aims assist identification mathematical
models complex systems.
Broadly speaking, system identification viewed field modelling dynamic systems experimental data (Soderstrom & Stoica, 1989). distinguish
between: (a) classical system identification techniques, developed control engineers econometricians; (b) machine learning techniques, developed computer
scientists. two main aspects activity. First, appropriate structure
determined (the model-identification problem). Second, acceptably accurate values
parameters model obtained (the parameter-estimation problem). Classical
system identification usually (but always) used possible model structure
known priori. Machine learning methods, hand, interest little
nothing known model structure. tool described machine learning
technique identifies qualitative models observational data. Qualitative models
non-parametric; therefore computational effort focussed model-identification
(there parameters estimated). task therefore somewhat easier
ambitious machine learning programs attempt identify parametric quantitative models (Bradley, Easley, & Stolle, 2000; Dzeroski, 1992; Dzeroski & Todorovski, 1995;
Todorovski, Srinivasan, Whiteley, & Gavaghan, 2000). Qualitative model-learning
number advantages: models quite comprehensible; system-dynamics
obtained relatively easily; space possible models finite; noise-resistance
fairly high. down-side, qualtitative model-learners often produced models
under- over-constrained; models provide clues precise mathematical structure; models largely restricted abstractions ordinary
differential equations (ODEs). attempt mitigate first shortcomings
adopting framework Inductive Logic Programming (ILP) (see Bergadano & Gunetti,
1996; Muggleton & Raedt, 1994). Properly constrained models identified using library syntactic semantic constraintspart background knowledge ILP
systemon physically meaningful models. Like ILP systems, library relatively
easily extendable. position paper that:
Background knowledge incorporating physical (and later, biological) system modelling principles used constrain set good qualitative models.
Using classical physical systems test-beds demonstrate empirically that:
small set constraints, conjunction Bayesian scoring function, sufficient
identify correct models.
Correct models identified qualitative quantitative data need
contain measurements variables model; learned
826

fiQualitative System Identification

sparse data large amounts noise. is, correct models identified
input data incomplete, incorrect, both.
closer examination performance test systems led discovery
term kernel subsets: minimal qualitative states present guarantee
implementation identify model correctly. concept may value model
identification systems.
primary interests, made clear outset, lie biological system identification.
completion sequencing key model genomes rise new technologies
opened prospect modelling cells silico unprecedented detail. models essential integrate ever-increasing store biological knowledge,
potential transform medicine biotechnology. key task emerging field
systems biology identify cellular models directly experimental data. applying qualitative system identification systems biology focus models metabolism:
interaction small molecules enzymes (the domain classical biochemistry).
models best established systems biology. end, demonstrate
approach scales identify core well-known, complex biological system
(glycolysis) qualitative data. system modelled set 25 qualitative relations, several unmeasured variables. scale-up achieved augmenting
background knowledge incorporate general chemical biological constraints
enzymes metabolites.
rest paper organised follows. next section present learning
approach ILP-QSI means example: u-tube. also describe details
learning algorithm section. Section 3 apply learning experiments
number systems class u-tube, present results obtained,
discuss results experiments reported thus far. Section 4 extends work
learning qualitative data set proof-of-concept experiments assess
ability ILP-QSI learn quantitative data. scalability method tested
Section 5 application large scale metabolic pathway: glycolysis. Section 6
discuss related work; finally Section 7 provide general discussion research
draw general conclusions.

2. Qualitative System Identification Using ILP
order aid understanding method presented paper first present
detailed description process applied illustrative system: u-tube.
u-tube chosen well understood system, one
used literature (Muggleton & Feng, 1990; Say & Kuru, 1996). results emerging
set experiments allow us draw tentative conclusions regarding
qualitative systems identification.
subsequent sections present results applying method described
section examples class system; enable us
generalise tentative conclusions. also apply method large scale biological
system demonstrate scalability method.
827

fiCoghill, Srinivasan, & King

State
1
2
3
4
5
6

h1
< 0, std >
< 0, inc >
< (0, ), dec >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >

h2
< 0, std >
< (0, ), dec >
< 0, inc >
< (0, ), inc >
< (0, ), std >
< (0, ), dec >

qx
< 0, std >
< (, 0), inc >
< (0, ), dec >
< (0, ), dec >
< 0, std >
< (, 0), inc >

qx
< 0, std >
< (0, ), dec >
< (, 0), inc >
< (, 0), inc >
< 0, std >
< (0, ), dec >

Table 1: envisionment states used u-tube experiments. qualitative values
standard form used QSIM. Positive values magnitude
represented interval (0, ), negative values interval (, 0) zero
0. directions change self explanatory increasing represented
inc, decreasing dec steady std.
2.1 Illustrative System: U-tube
u-tube system (Fig. 1) closed system consisting two tanks containing (or potentially containing) fluid, joined together base pipe. Assuming fluid
system passes one tank via pipe tank higher
fluid level tank lower fluid level (as function difference height).
height fluid tanks system equilibrium
fluid flow.
u-tube represented system ordinary differential equations follows:
dh1
dt
dh2
dt



= k (h1 h2 )


(1)


= k (h2 h1 )

qualitative model may obtained simply abstracting real numbers,
would normally associated Equation 1, quantity space signs.
common formalism used represent qualitative models QSIM (Kuipers, 1994).
representation models conjunctions constraints, two three place
predicates representing abstractions real valued arithmetic functional operations.
variables model values represented two element vectors consisting (in
abstract case) sign magnitude direction change variable.
order accommodate restriction number variables constraint may
rewrite Equation 1 follows:

h = (h1 h2 )



qx = k h
(2)
dh1


dt = qx


dh2
dt = qx

h1 h2 height fluid Tank 1 Tank 2 respectively; h
difference height fluid tanks; q x flow fluid tanks.
converted directly QSIM constraints shown Fig. 1.

828

fiQualitative System Identification

Tank 2

Tank 1

Delta_h

h1

+

h

h2
dt

dt

h1
h2

-qx

-

qx

M+

DERIV(h1,qx ),
DERIV(h2,qx ),
ADD(h2,Delta h, h1 ),
M+ (Delta h,qx ),
MINUS(qx,qx ).

qx

Figure 1: u-tube: (left) physical; (middle) QSIM diagrammatic; (right) QSIM constraints. QSIM version model Delta h corresponds h
physical model. QSIM, M+ (, ) qualitative version functional relation
indicates monotonically increasing relation two
variables arguments. + (, ) constraint represents family
functions includes linear non-linear relations.

2

6
5

3

1

4

Figure 2: u-tube envisionment graph.
Appropriate qualitative analysis u-tube produce states shown Table 1,
states envisionment. represent distinct qualitative states
u-tube may exist Fig. 2 depicts possible behaviours (in terms
transitions states)1 . figure represents complete envisionment system,
graph containing qualitative states system transitions
particular input value. case u-tube presented
input (which equivalent value zero). hand behaviours
u-tube may observed number experimental (initial) conditions,
measurements taken height fluid tank flow
tanks. converted (by means quantitative-to-qualitative converter)
set qualitative observations (or states). sufficient temporal information available
enable calculation qualitative derivatives, observation tuple stating
magnitude direction change measured variable. observations also
contain states complete envisionment Table 1 (or subset thereof).
1. State 1 represents situation fluid system, nothing happens
interesting.

829

fiCoghill, Srinivasan, & King

u-tube member large class dynamic systems defined
states: state systems. systems values variables future times
defined current state system regardless state achieved (Healey,
1975). means simulation, system state act initial state.
current context means order learn structure systems need
focus states may ignore transitions states.
enables us explore power set envisionment ascertain conditions
system identification possible. Given qualitative observations examples,
background knowledge consisting constraints models (described later) QSIM
relations, learning system (which name ILP-QSI) performs search acceptable
models. suitable first approximation, basic task viewed discrete
optimisation problem finding lowest cost elements amongst finite set alternatives.
is, given finite discrete set models real-valued cost-function f : <,
find subset H H = {H|H f (H) = min Hi f (Hi )}. problem
may solved employing procedure searches directed acyclic graph
representation possible models. representation, pair models connected
graph one transformed another operation called refinement. Fig. 3
shows parts graph u-tube model refined another
addition qualitative constraint. optimal search procedure (the branch-and-bound
procedure) traverses graph order, times keeping cost best nodes
far. Whenever node reached certain descendents
cost higher best nodes, node descendents removed
search.
number features apparent u-tube model relevant
learning method utilised work (and discussed Section 2.3) described
since regard general modelling issues relevant learning qualitative models
dynamic systems.
first thing may noted regard expressions Equation
2 resulting qualitative constraints ordered; is, given values
exogenous variables magnitude state variables (the height fluid
tanks case) equations placed order variables
left hand side may values calculated appear right hand
side equation2 . particular form ordering known causal ordering (Iwasaki
& Simon, 1986). causally ordered system depicted graphically shown Fig. 4.
causally ordered model contains algebraic loops. quantitative systems one tries
avoid algebraic loops hard simulate, requiring additional simultaneous
equation solvers used.
qualitative model combined Qualitative Reasoning (QR) inference engine
provide envisionment system interest. is, generate qualitatively distinct states system may exist. case u-tube six
states given Table 1. Example behaviours resulting states shown
Fig. 2.
2. ordering required QSIM order preform qualitative simulation. However, ability
order equations manner utilised filter learning system order eliminate
models containing algebraic loops.

830

fiQualitative System Identification

ADD( h1, h 2, h1)

ADD( h1, h 2, h1)
DERIV( h2,h1 )
MPLUS( h2,f12)

ADD(h 2, h2, h1)

ADD( h1, h 2, x1)

ADD( h1, h 2, h1)
DERIV( h2,h1 )
MPLUS( h2,f12)
MINUS( x8,f12 )
MPLUS( x8,h1 )

ADD( h1, h 2, x1)
MPLUS( x1,f12 )
MPLUS( h2,x4)

DERIV(h1,x2 )

DERIV( h1,x2 )

[]

DERIV( h1,h2 )

DERIV( h1,f12)

DERIV( h1,f12)
DERIV( h2,x5)
ADD( h2,x6,h1 )

MPLUS( h1,f12)

DERIV( h1,f12)
DERIV( h2,x5)
ADD( h2,x6,h1 )
MPLUS( x6,f12 )
MINUS( f12,x6 )

MPLUS( h1,f12)

MPLUS( h1,f12)

MPLUS( h1,f12)
ADD( f12,x7,h2 )
DERIV( h2,f12)

Figure 3: portions u-tube lattice (with target model box).
may noted differential equation model captures essence explanation given first paragraph section. sufficient explain operation
system, well predict way behave, contains
variables constants necessary achieve task - i.e. model parsimonious. 3
Furthermore, examination causal diagram Fig. 4 indicates causal
ordering particular direction magnitudes state variables
derivatives. link derivatives magnitudes state variables
integration time. integral causality preferred kind
3. possible didactic purposes may want include detail, example relation
intertank flow pressure difference, height fluid pressure.
reason would expect relations found; although context
adequate theoretical framework model fits, model provides pointers direction.
hand, one envisage simpler models existing may suitable prediction
inadequate required kind explanation. See Section 6 this.

831

fiCoghill, Srinivasan, & King

k

h1

h1

h

qx

h2

h2

Figure 4: causal ordering u-tube model given Equation 2.
causality systems engineering modelling; simulation generally.
integration smooths noise whereas differentiation introduces it.
variables either endogenous exogenous. Exogenous variables influence system influenced it. Well posed models flapping variables;
is, endogenous variables appear one constraint. QSIM includes
DERIV constraint linking state variables directly derivatives, systems interested regulatory, containing feedback paths, endogenous
variables must appear least two constraints.
Well posedness parsimony mandatory properties model, properties desirable always achievable may relaxed. However,
systems examined paper properties holds.
final feature u-tube model represents single system.
assumption implicit learning experiments described paper data
measured belongs single coherent system. keeping general experimental
approaches assumed measurements related way
part system. course may get wrong relax requirement
discover thought related cannot actually brought together
single model. generalises requirement parsimony line Einsteins
adage model simple possible simpler. case
translates minimising number disjoint sub-systems identified.
2.2 Qualitative Solution Space
Section 2.3 shall present algorithm automatically constructing models
data. method utilise background knowledge consisting QSIM modelling
primitives combined systems theory meta-knowledge (e.g. parsimony causality).
Later shall also provide analysis models learned states utilised learn
order ascertain which, any, states important successful learning.
One way facilitate analysis make use solution space relate qualitative
states critical points relevant class systems (via isoclines system) 4
4. critical points dynamic system points one derivatives state
variables zero. isoclines contours critical points.

832

fiQualitative System Identification

(Coghill, 2003; Coghill, Asbury, van Rijsbergen, & Gray, 1992). stated previously,
qualitative analysis u-tube generate envisionment containing six states,
shown Table 1, depicted envisionment graph given Fig. 2. Continuing
h2
h1
h2

height

6

5

5

f12 = 0

2

4
2

time

1

3

h1

Figure 5: qualitative states u-tube system presented representative time
courses (left) solution space (right). state numbers refer
states u-tube described above. (State 5 represents steady state
strictly speaking reached = , practice taken occur
two trajectories sufficiently close, shown here.)

u-tube; two ways behave (ignoring state 1), captured Fig. 5. Either
head fluid tank 1 greater tank 2 (state 4) (in extreme tank
1 empty state 3), head greater tank 2 tank 1 (state 6). Fig. 5 (left)
shows transient behaviour extreme case tank 1 empty (state 2);
seen diagram head starts condition eventual end
equilibrium (state 5). state Equation 1 rewritten as:


0 = k (h1 h2 )


(3)


0 = k (h2 h1 )

definition k must non-zero; solution pair equations is:
h2 = h 1
relation plotted graph shown right hand side Fig. 5.
qualitative states u-tube may placed solution space graph relation
equilibrium line. representation (similar form phase space diagram)
useful provides global picture location qualitative states
envisionment relative equilibria critical points system. also
utilised construction diagnostic expert systems (Warren, Coghill, & Johnstone,
2004). details means analysing envisionments see work Coghill
(2003) Coghill et al. (1992).
833

fiCoghill, Srinivasan, & King

bb(i, , f ) : Given initial element discrete set S; successor function : 2 ; cost function
f : <, return H H contains set cost-minimal models. h i,j H, f (hi ) =
f (hj ) = fmin s0 S\H f (s0 ) > fmin .
1. Active := h(i, )i.
2. worst :=
3. selected :=
4. Active 6= hi
5. begin
(a) remove element (k, costk ) Active
(b) costk < worst
(c) begin
i. worst := costk
ii. selected := {k}
iii. let P rune1 Active s.t. j P rune1 , f (j) > worst f (j) lowest cost
possible j successors
iv. remove elements P rune1 Active
(d) end
(e) elseif costk = worst
i. selected := selected {k}
(f) Branch := (k)
(g) let P rune2 Branch s.t. j P rune2 , fmin (j) > best fmin (j) lowest cost
possible j successors
(h) Bound := Branch\P rune2
(i) x Bound
i. add (x, f (x)) Active
6. end
7. return selected

Figure 6: basic branch-and-bound algorithm. type Active determines specialised
variants: Active stack (elements added removed front)
depth-first branch-and-bound results; Active queue (elements added
end removed front) breadth-first branch-and-bound
results; Active prioritised queue best-first branch-and-bound results.

2.3 Algorithm
ILP learner used research multistage procedure, addresses
discrete optimisation problem. general terms, posed follows: given finite
discrete set cost-function f : <, find subset H H =
{s|s f (s) = minsi f (si )}. optimal algorithm solving problems
branch-and-bound algorithm, shown Fig. 6 (the correctness, complexity optimality
properties algorithm presented paper Papadimitriou & Steiglitz, 1982).
specific variant algorithm available within software environment comprising
Aleph (Srinivasan, 1999). modified procedure Fig. 7. principal differences
Fig. 6 are:
1. procedure given set starting points H 0 , instead single one (i Fig. 6);
834

fiQualitative System Identification

2. limitation number nodes explored (n Fig. 7);
3. use boolean function acceptable : H B E {F ALSE, RU E}.
acceptable(k,B,E) TRUE, if: (a) Hypothesis k explains examples
E, given B usual sense understood ILP (that is, B k |= E absence
noise); (b) Hypothesis k consistent constraints contained
background knowledge (that B kI 6|= 2). practice, possible merge
requirements encoding requirement entailing examples
constraint B;
4. Inclusion background knowledge examples (B E Fig. 7).
arguments refinement operator (the reason become apparent
shortly) cost function f .
following points relevant implementation used here:
qualitative model represented single definite clause. Given definite
clause C, qualitative constraints model (the size model) obtained
counting number qualitative constraints C. also called
size C.
Constraints, restriction well-posed models (described below), assumed encoded background knowledge;
initial set H0 Fig. 7 consists empty clause denoted . is,
H0 = {};
acceptable(C, B, E) RU E qualitative model C consistent
constraints B, given E.
Active prioritised queue sorted f ;
successor function used . defined follows. Let size
acceptable model C qualitative model size 0 n = 0 .
assume B contains set mode declarations form described (Muggleton,
1995). Then, given definite clause C, obtain definite C 0 (C, B, E) =
n1
nA = hD 0 |
(C, B, E) s.t. 0 1A (D, B, E)i, (n 2). C 0 1A (C, B, E)
obtained adding literal L C, that:
argument mode +t L substituted input variable type
appears positive literal C variable type
occurs negative literal C;
argument mode L substituted variable C type
appears argument new variable type t;
argument mode #t L substituted ground term type t.
assumes availability generator elements Herbrand universe
terms;
acceptable(C 0 , B, E) RU E.
835

fiCoghill, Srinivasan, & King

bbA (B, E, H0 , , f, n) : Given background knowledge B B; examples E E; non-empty set initial elements
H0 discrete set possible hypotheses H; successor function : H B E 2 H ; cost function
f : H B E <; maximum number nodes n N (n 0) explored, return H H
H contains set cost-minimal models models explored.
1. Active = hi
2. H0
(a) add (i, ) Active
3. worst :=
4. selected :=
5. explored := 0
6. (explored < n Active 6= hi)
7. begin
(a)
(b)
(c)
(d)

(e)
(f)
(g)
(h)
(i)

remove element (k, costk ) Active
increment explored
acceptable(k, B, E)
begin
i. costk < worst
ii. begin
A. worst := cost
B. selected := {k}
C. let P rune1 Active s.t. j P rune1 , f (j, B, E) > worst f (j, B, E)
lowest cost possible j successors
D. remove elements P rune1 Active
iii. end:
iv. elseif costk = worst
A. selected := selected {k}
end
Branch := (k, B, E)
let P rune2 Branch s.t. j P rune2 , f (j, B, E) > worst f (j, B, E) lowest
cost possible j successors
Bound := Branch\P rune2
x Bound
i. add (x, f (x, B, E)) Active

8. end
9. return selected

Figure 7: variant basic branch-and-bound algorithm, implemented within
Aleph system. B E sets logic programs; N set
natural numbers.

following properties 1A (and, turn ) shown hold (Riguzzi,
2005):
locally finite. is, 1A (C, B, E) finite computable (assuming
constraints B computable);
weakly complete. is, clause containing n literals obtained
n refinement steps empty clause;
836

fiQualitative System Identification

proper. is, C 0 equivalent C;
optimal. is, C 0 obtained multiply refining different
clauses.
addition, clear definition given qualitative model C, acceptable(C 0 , B, E)
RU E model C 0 1A (C, B, E). turn, follows acceptable(C 0 , B, E)
RU E C 0 (C, B, E).
cost function used (following Muggleton, 1996) f Bayes (C, B, E) = P(C|B, E)
P(C|B, E) Bayesian posterior probability estimate clause C, given
background knowledge B positive examples E. Finding model maximal posterior probability (that is, lowest cost) involves maximising function (McCreath, 1999):
1
Q(C) = logDH (C) + p log
g(C)
DH prior probability measure space possible models; p
number positive examples (that is, p = |E|); g generality model.
use approach used ILP system C-Progol obtain values two
functions. is, prior probability related complexity models (more
complex models taken less probable, priori); generality model
estimated using number random examples entailed model, given
background knowledge B (the details presented Muggleton paper
1996).
selected Bayesian function score hypotheses since represents,
best knowledge, one ILP literature explicitly developed
case data consist positive examples (as situation
paper, examples observations system behaviour: system identification
non-behaviour represent usual understanding task
attempting here).
evident choices make branch-and-bound procedure simple generateand-score approach. Clearly, approach scalable constraints encoding
well-posed models sufficient restrict acceptable models reasonable number:
describe set constraints sufficient models examined
paper. rest paper, term ILP-QSI taken mean Aleph
branch-and-bound algorithm specific choices above.
2.3.1 Well-posed models
Well-posed models introduced Section 2.1; current implementation
defined satisfying least following syntactic constraints:
1. Size. model must particular size (measured number qualitative
relations physical models Sections 2.4 3 number metabolites
biological model Section 5). size pre-specified.
2. Complete. model must contain measured variables.
837

fiCoghill, Srinivasan, & King

3. Determinate. model must contain many relations variables (a basic principle
systems theorythe reader may recall version school algebra, system
equations contains many equations unknowns).
4. Language. number instances qualitative relation model must
pre-specified limit. kind restriction studied greater
detail work Camacho (2000).
least following semantic constraints:
5. Sufficient. model must adequately explain observed data. adequate,
intend acknowledge due noise measurements, observations
may logical consequences model 5 . percentage observations must
explainable sense user-defined value.
6. Redundant. model must contain relations redundant. example, relation ADD(inf low, outf low, x1) redundant model already
ADD(outf low, inf low, x1).
7. Contradictory. model must contain relations contradictory given
relations present model.
8. Dimensional. model must contain relations respect dimensional constraints.
prevents, example, addition relations like ADD(inf low, outf low, amount)
perform arithmetic variables different units measurement.
following additional constraints incorporated algorithm could
ignored (because preferences rather absolute rules), results presented
paper require satisfied:
9. Single. model must contain two disjoint models. assumption
set measurements made within particular context
user desires single model includes measurement variables.
10. Connected. intermediate variables appear least two relations.
11. Causal. model must causally ordered (Iwasaki & Simon, 1986) integral
causality (Gawthrop & Smith, 1996). is, causality runs algebraic
constraints model magnitudes state variables derivatives;
derivatives magnitudes DERIV constraint only.
list intended exhaustive: fully expect would need augmented domain-specific constraints (the biological system identification problem
described Section 5 provides instance this). advantage using ILP
augmentation possible relatively straightforward manner.
5. Strictly speaking, model conjunction background knowledge.

838

fiQualitative System Identification

2.4 Experimental Investigation Learning U-tube System
section present comprehensive experimental test learning algorithm
described previous section. focus u-tube illustrate approach
explain results obtained. subsequent section present results
applying ILP-QSI learning structure number different systems similar
kind. data utilised experiments qualitative. assumed either
measurements yield qualitative values quantitative time series
converted qualitative values. latter may necessary situations
quantitative time series data available sufficient quantity permit
quantitative system identification performed.
following general method applied learning systems studied
paper.
2.4.1 Experimental Aim
Using u-tube system, investigate model identification capabilities ILP-QSI using
qualitative data subject increasing amounts noise made increasingly sparse order ascertain circumstances target system may
accurately identified, function number qualitative observations available.
2.4.2 Materials Method
model learning system ILP-QSI seeks learn qualitative structural models qualitative data; therefore focus experiments learning qualitative data.
Data inputs (exogenous variables) system. data required
learning combinations qualitative states (of 6) envisionment shown Table 1.
Method two distinct sets experiments reported here: based noise
free data based noisy data. former assume data provided
correct used test capability ILP-QSI handling sparse data. latter set
experiments captures situation qualitative data may incorrect
measurement errors due noise original signal, errors introduced
quantitative qualitative transformation (which may occur cases original
data numerical).
Noise-free data. use following method evaluating ILP-QSIs system-identification
performance noise-free data:
system investigation:
1. Obtain complete envisionment specific values exogeneous variables.
(In particular case u-tube discussed section exogenous
variables envisionment states shown Table 1, stated above.)
839

fiCoghill, Srinivasan, & King

2. non-empty subsets states envisionment training data construct
set models using ILP-QSI record precision result. 6 number
possible non-empty sets states different test scenarios u-tube 63.
(2N 1, N number states complete envisionment)
3. Plot learning curves showing average precision versus size training data.
Noisy data. use following method evaluating ILP-QSIs system-identification
performance noisy qualitative data:
system investigation:
1. Obtain complete envisionment specific values exogeneous variables.
2. Replace non-empty subsets states envisionment randomly generated
noise states. combination correct random states, training
data construct set models using ILP-QSI record precision result. 7
Given complete envisionment N states, replacing random subset k > 0
random states result noisy envisionment consisting N k noisefree states k random states. Step 2 noise-free data, exhaustive
replacement possible subsets complete envisionment random states
result 2N 1 noisy test sets.
3. Plot learning curves showing average precision versus size training data.
2.4.3 Results
results performing experiments, showing precision learning target
model versus number states used (for noise-free noisy data) shown
Fig. 8. evident situations precision improves number states
used results experiments noisy data lower precision
noise-free data (though curves general shape).
results one would expect.
noise-free data find possible identify target model using
one state data. However possible identify target model using pairs
states 53% cases. states are:
[2, 3], [2, 4], [2, 5], [3, 5], [3, 6], [4, 5], [4, 6], [5, 6]
refer Kernel sets. time merely report finding delay
discussion significance reporting results experiments
systems class.
6. proportion models result equivalent correct model. Thus,
training data set, result returned ILP-QSI precision 0.0 1.0. term
precision used meaning usually associated Machine Learning community
rather familiar Qualitative Reasoning.
7. non-noisy data, training data set, result returned ILP-QSI
precision 0.0 1.0.

840

fiQualitative System Identification

1

0.9

0.8

Precision

0.7

0.6

0.5

0.4

0.3

0.2

Clean
Noisy

0.1

0
1

2

3

4

5

6

Number States

Figure 8: Precision models obtained u-tube.

3. Experiments Systems
section present experimental setup applied number systems:
coupled tanks, cascaded tanks mass spring damper. systems representative
class system appearing industrial contexts (e.g. cascaded tanks system
used model diagnosis industrial Ammonia Washer system Warren
et al., 2004) well useful analogs metabolic compartmental systems.
case experimental method identical utilised u-tube
described Section 2.4. system give description system target
model, envisionment associated system, statement data used
experiments, summary results obtained experiments.
3.1 Experimental Aim
three physical systems: coupled tanks, cascaded tanks mass-spring-damper (a well
known example servomechanism), investigate model identification capabilities
ILP-QSI using qualitative data subject increasing amounts noise
made increasingly sparse.
3.2 Materials Method
Data Qualitative data available consist complete envisionment arising specific
values input variables. precise details data given experiment.
Method

method used u-tube described Section 2.4.
841

fiCoghill, Srinivasan, & King

h1
< 0, std >
< 0, inc >
< (0, ), dec >
< (0, ), dec >
< (0, ), inc >
< (0, ), dec >
< (0, ), std >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >

State
1
2
3
4
6
7
8
9
10
11

h2
< 0, std >
< (0, ), dec >
< 0, inc >
< (0, ), inc >
< (0, ), dec >
< (0, ), std >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >

qx
< 0, std >
< (, 0), inc >
< (0, ), dec >
< (0, ), dec >
< (, 0), inc >
< (0, ), dec >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >

qo
< 0, std >
< (0, ), dec >
< 0, inc >
< (0, ), inc >
< (0, ), dec >
< (0, ), std >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >

Table 2: envisionment states used coupled tanks experiments. (The states
labeled accord u-tube; since state 5 u-tube
appear coupled tanks envisionment state labeled 5
table.)
3.3 Coupled Tanks
open system consisting two reservoirs shown Fig. 9. Essentially,
seen u-tube input output. input, q , flows top tank
1 output, qo , flows base tank 2 (see Fig. 9).
Tank 2

Tank 1
qi

h2

h1

+
h

h

M+

dt

h1

h 1

h2
qi
qx

+

M+

dt
h 2

qx

+

qo

DERIV(h1,h01 ),
DERIV(h2,h02 ),
ADD(h2 ,Delta h, h1 ),
M+ (Delta h,qx ),
M+ (h2 ,qo ),
ADD(h02 ,qo ,qx ),
ADD(qx,h01 ,qi ).

qo

Figure 9: coupled tanks: (left) physical; (middle) QSIM diagram; (right) QSIM relations.
experiments assume observe: q , qx , h1 , h2 , qo . Thus system
identification must discover model three intermediate variables, h 01 , h02 h.
Data one exogenous variable, namely flow liquid tank 1 (q ).
experiments described input flow kept zero (that is, q = h0, stdi), making
system particular case moderately complex u-tube.
complete envisionment consists 10 states, shown Table 2 Fig. 10, means
1024 experiments set.

842

fiQualitative System Identification

1

8

9

3

6

7

10

4

11

2

Figure 10: Coupled tanks envisionment graph.
3.3.1 Results
precision graphs coupled tanks experiments shown Fig. 11.
results show improvement precision number states used increases also
deterioration precision noise added. effect noise worse fewer states
used case u-tube, though effect nullified states
used.
1

0.9

0.8

Precision

0.7

0.6

0.5

0.4

0.3

Clean
Noisy

0.2

0.1

0

1

2

3

4

5

6

7

8

9

10

Number States

Figure 11: Coupled tanks precision graphs.
noise free data possible identify models using single
datum utilising pairs states yielded target model 11% cases. relevant
pairs states (kernel sets) are:
[2, 7], [3, 8], [4, 8], [6, 7], [7, 8]
843

fiCoghill, Srinivasan, & King

Whereas u-tube experiments states target model successfully
learned supersets pairs, coupled tanks case sets three states
(which supersets pairs listed above) result successful identification
target model:
[2, 3, 9], [2, 3, 10], [2, 3, 11]
[2, 4, 9], [2, 4, 10], [2, 4, 11]
[3, 6, 9], [3, 6, 10], [3, 6, 11]
[4, 6, 9], [4, 6, 10], [4, 6, 11]
3.4 Cascaded Tanks
system also open system. However, flow system always unidirectional (unlike coupled tanks system). principle, system broken
two sub-systems containing one reservoir, input output.
example system shown Fig. 12. Liquid flows tank 1, unidirectionally tank 1 tank 2. apparent figure, flow
top tank 1 base tank 2.
qi

h1

h1
qx

h2
M+

dt
h 1

Tank

M+

dt
h 2

qo

h2

qi

+

qx

+

qo

DERIV(h1,h01 ),
DERIV(h2,h02 ),
M+ (h1 ,qx ),
M+ (h2 ,qo ),
ADD(h02 ,qo ,qx ),
ADD(qx,h01 ,qi ).

Tank B

Figure 12: cascaded tanks: (left) physical; (middle) QSIM diagrammatic; (right) QSIM
relations.
assume observe: qi , h1 , h2 , qx . Thus system identification must
discover model two intermediate variables, h 01 h02 . numbered list states
(or complete envisionment) case shown Fig. 13 Table 3.

Data one exogenous variable, namely flow liquid tank 1 (q ).
increase complexity allowing steady positive input flow (that is, q = h(0, ), stdi).
complete envisionment consists 14 states, shown Fig. 13 Table 3
means 16,383 experiments required .
3.4.1 Results
precision graphs cascaded tanks shown Fig. 14. graphs similar
shape coupled systems, showing generally lower precision noisy-data.
examination shows unable identify target model fewer
three states. subset triples (which form kernel sets case)
844

fiQualitative System Identification

1

12

8

4

2

13

9

5

11

14

10

6

7

3

Figure 13: Cascaded tanks envisionment graph.
State
1
2
3
4
5
6
7
8
9
10
11
12
13
14

h1
< 0, inc >
< 0, inc >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), std >
< (0, ), std >
< (0, ), std >
< (0, ), std >
< (0, ), inc >
< (0, ), inc >
< (0, ), inc >
< (0, ), inc >

h2
< 0, std >
< (0, ), dec >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >

qx
< 0, inc >
< 0, inc >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), std >
< (0, ), std >
< (0, ), std >
< (0, ), std >
< (0, ), inc >
< (0, ), inc >
< (0, ), inc >
< (0, ), inc >

qo
< 0, std >
< (0, ), dec >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >

Table 3: envisionment states used cascaded tanks experiments.

target model identified are:
[1, 3, 4], [1, 3, 5], [1, 3, 8], [1, 3, 9], [1, 7, 4], [1, 7, 5], [1, 7, 8], [1, 7, 9]

845

fiCoghill, Srinivasan, & King

1

Clean
Noisy

0.9

0.8

Precision

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

2

4

6

8

10

12

14

Number States

Figure 14: Cascaded tanks precision graph.
3.5 Mass-Spring-Damper
final physical system considered abstraction wide variety servomechanisms
displacing force. example system shown Fig. 15. situation,
mass held equilibrium two forces. equilibrium disturbed, oscillatory
behaviour observed. motion mass damped oscillations
continue indefinitely, eventually return original equilibrium position.
external force applied (for example pulling mass down) final resting place
displaced natural equilibrium point (see Fig. 15). mass displacement
dispM rest position, time, t, moving velocity vel
accelerating rate accM . assume observe variables: disp , velM , accM ,
disp M+ H 1
+

dt
Damping

Spring

vel M+

H2

H
+

force

dt
vel
disp

a=v/sec

accM M+

H3

DERIV(dispM ,velM ),
DERIV(velM ,accM ),
M+ (dispM ,H1 ),
M+ (velM ,H2 ),
M+ (accM ,H3 ),
ADD(H1,H2 ,H4 ),
ADD(H3,H4 ,force).

force

Figure 15: spring system (a) physical; (b) QSIM diagrammatic; (c) QSIM relations
f orce. Qualitative system identification must find model four intermediate
variables, H1 , H2 , H3 H4 ; well intermediate relation ADD(H 1 ,H2 ,H4 ),
846

fiQualitative System Identification

three variables. input force, force, exogenous. experiments here,
consider case steady force applied system (that
is, ForceA = h(0, ), stdi). complete envisionment case shown Fig. 16,
equilibrium point represented state 2. precision graphs shown
2
19

24

29

17

0

3

18

22

27

16

23

28

20

21

26

12

7

5

6

10

8

9

15

13

14

11

4

1

30

25

Figure 16: Mass-spring-damper envisionment graph
Fig. 17. system envisionment contains 31 states, makes exhaustive testing
unrealistic. Instead sets clean noisy states randomly selected space
possible experiments. Nonetheless observed average precision graphs
in-line obtained tanks experiments. However, actual precision values
suggest sparse data noise less effect systems.
may due tight relationship two derivatives spring model,
making system extremely constrained.
3.6 Discussion Results
inspection experimental results reveals expected pattern: cases
precision curves (for noisy noise free experiments) general shape.
Experiments utilise fewer states identify target model less often
greater number states used. However, closer examination results reveals
even states used (pairs triples) target model may consistently
found particular combinations states used. order understand
requires us look solution spaces systems concerned. 8
examine u-tube coupled tanks together closely
related systems zero input. cascaded tanks system slightly different
non-zero input discussed later section.
8. discuss spring system complexity.

847

fiCoghill, Srinivasan, & King

1

0.9

0.8

Precision

0.7

0.6

0.5

0.4

0.3

0.2

Clean data
Noisy data

0.1

0
1

4

7

10

13

16

19

22

25

28

31

Number States

Figure 17: Mass-spring-damper precision graph
3.6.1 U-tube Coupled Tanks
bare results, interesting, give indication particular pairs
triples highlighted precisely identify target model. order ascertain why?
must examine envisionment states given Tables 1 2, itemise
relevant features sets states follows:
u-tube coupled tanks least one critical point pair.
systems pair states contains one state branch
envisionment graph (Fig. 2 & Fig. 10); least one extreme
branch. is, states one tank either empty state immediately
succeeding this, tank relatively full derivatives
height tank opposite signs.
systems supersets minimal sets precisely learn target
model.
observations lead us suggest coupled systems ability learning
system identify structure model dependent data used including
critical points data covers different types starting point
system behaviours have. keeping systems theory would lead
us expect (Gawthrop & Smith, 1996).
order properly appreciate indicated kernel sets relation
systems need look solution spaces (Coghill et al., 1992;
848

fiQualitative System Identification

Coghill, 2003) two systems. shown Fig. 18 (and derivation
similar given Section 2.2 detailed Appendix A). get
clear picture kernel pairs triples lie respect critical points
system.
h1=h2=0

h2

h1=0

h2

5

8

6

6

2

11

2

10

4

h2=0

9
7
4

1

3

h1

1

3

h1

Figure 18: solution spaces u-tube coupled tanks systems.
system diagrams provided Fig. 1 Fig. 9 seen u-tube
coupled tanks systems differ fact coupled tanks outlet orifice,
whereas u-tube not. accounts major difference solution spaces;
namely coupled tanks two critical points (states 7 9) whereas u-tube
one (state 5) actually steady state. gives rise additional
states: 9, 10 11 lie critical points. 9 observed
outlet orifice tank 2 coupled tanks system decreases size space
isoclines solution space become narrower disappears orifice
closes. seen formally comparing equations 6 10 Appendix A.
clear k2 approaches zero, equation 6 approximates equation 10 (and
k2 = 0 two equations same).
look sets pairs observe related ways
reflect relationship two coupled systems. Firstly, looking pairs.
u-tube 4 pairs include critical point (steady state), state 5: [2,
5], [3, 5], [4, 5], [5, 6]. noting discussion state 5 u-tube
relates either states 7 8 coupled tanks find analogous pairs
exist kernel set coupled tanks: [2, 7], [3,8], [4, 8], [6, 7]. leaves one
pair coupled tanks pairs unaccounted for: [7, 8]. However, surprise since
pair taken map state 5 u-tube; consistent finding
singleton state sufficient learn model system.
9. three states differ magnitude qx qdir h01 h02 ,
neither appear explicitly solution space. Readers may convince
comparing Table 1 envisionment Table 2.

849

fiCoghill, Srinivasan, & King

still 4 pairs u-tube experiments able learn reliably
target model corresponding coupled tanks pair. are: [2, 3],
[2, 4], [3, 6], [4, 6]. comparison triples learning coupled tanks
model reveals states pairs conjoined either state 9, 10
11 make triples. inclusion states warrants explanation since
states distinguish closed u-tube open coupled tanks.
three states state variables value h(0, ), dec)i; situation
cannot occur u-tube. Combining fact four pairs listed
contain critical point qualitatively identical systems leads one
conclusion additional information contained triple kernel sets enables
one distinguish u-tube coupled tanks case.
results extend, strengthen deepen reported Coghill et al. (2004)
Garrett et al. (2007).
3.6.2 Cascaded Tanks
cascaded tanks system asymmetrical flow possible one direction. fact input positive steady flow makes setup marginally
complex regard coupled systems, input flow.
kernel sets model system may learned (presented Section
3.4.1 depicted schematically Fig. 19 order explain results obtained.
look first middle columns diagram ignore, time being,
downstream tank, see represented two pairs states: tank
empty combined tank steady state, tank empty combined
state amount fluid tank greater steady state. confirmed
experimentally kernel sets single tank model learned.
ignore upstream tank (apart outflow) examine middle
third columns diagram see divide two groups according
whether input downstream tank steady decaying (positive decreasing).
two pairs states, upstream tank:
tank empty combined tank steady state, tank empty combined
state amount fluid tank greater steady state.
case tank seen cross product states appear kernel sets
case represents valid possible situation.
results lead two major conclusions regard cascaded tanks system:
1. ILP-QSI effectively identifies individual components cascade combines
cascade point.
2. situation downstream tank, input either steady flow
decreasing flow, indicates utilising variety inputs aid identification
process.
former conclusion may serve pointer possibility incremental learning
cascaded systems.
850

fiQualitative System Identification

1

3

8

4

5

9

7

8

4

5

9

Figure 19: schematic representation triples states target model
cascaded tanks systems learned.

851

fiCoghill, Srinivasan, & King

4. Experiments Quantitative Data
part experimental testing system proof-of-concept test.
stated system designed learn qualitative models qualitative data.
assumed conversion quantitative data already performed,
least needed qualitative data analysis would require another research project
beyond scope paper. However, order test usability system
quantitative data test ability go whole process receiving
data producing model, implemented rudimentary data analysis package
facilitate this. course exhaustive, permit us test results
produced via process consistency produced experiments
qualitative data.
4.1 Experimental Aim
Using four physical systems, investigate model identification capabilities ILPQSI using numeric traces system behaviour subject increasing amounts
noise.
4.1.1 Quantitative Qualitative Conversion
proceeding describe experiments carried out, present method used
convert numerical data qualitative form required ILP-QSI
utilised set experiments.
adopted straightforward simple approach performing conversion.
quantitative variable x, values N real-valued time series steps numerically
differentiated means central difference approach (Shoup, 1979) that,
(xi xi1 )+(xi+1 xi )
2





dxi
dt

=

d2 x
dt2


= (xi xi1 ) (xi+1 xi )

= 2N 1

quantitative variable x converted qualitative variable q = hqmag, qdiri,
qmag {(-,0), 0, (0,)} generated x, qdir {dec, std, inc} generated
dx/dt. qualitative derivative q, q, obtained similar manner generated
dx/dt d2 x/dt2 respectively.
data typically noisyeither inherently, process differentiation
perform simple smoothing first second derivatives using Blackman
filter (a relative moving average filter see Blackman & Tukey, 1958). case,
filter actually applied result Fast Fourier Transform (FFT) result obtained taking real part inverse FFT. note form
smoothing appropriate sufficient number time steps present.
obtained (smoothed) numerical value x variable x instant i, qualitative magnitude qmag(xi ) is, principle, simply obtained following:
qmag(xi ) =



(, 0)

xi < 0
0
xi = 0


(0, +) otherwise
852

fiQualitative System Identification

practice, since floating-point values unlikely exactly zero, found
advantageous re-apply filtering process data straddling zero eliminate small
fluctuations around value. Despite measures, addition generating correct
qualitative states (true positives) conversion produce errors: states generated may
correspond true states (false positives); true states may generated
(false negatives). Fig. 20 shows example (the problem is, course, exacerbated
original quantitative data noisy). reason imperfect results
noise free quantitative data twofold: one smoothing process small fluctuations around zero; main reason that, discussed above, creating full qualitative
state involves numerical differentiation introduces noise data derivatives affects ability process convert quantitative qualitative
absolute accuracy.
System
u-tube
Coupled
Cascaded
Spring

True
States
6
10
14
33

Generated
States
6
14
8
33

True
Positives
4
6
5
20

False
Positives
2
8
3
13

False
Negatives
0
0
6
0

Figure 20: example errors resulting generating qualitative states traces
system behaviour. Here, traces generated following initial
conditions: h1 = 2.0, h2 = 0.0 three tank systems; disp = 2.0,
velM = 0.0 spring.

4.2 Materials Method
Numerical simulations four physical systems constructed using general
relations qualitative models. experiments carried utilising
noise free noisy data, described rest section.
4.2.1 Data
models used numerical simulations structure qualitative
models, substitution real valued parameter monotonic function
relation. gives linear relation two variables; complex, non-linear
functions might used, linear functions provided suitable approximation
known behavior systems, shown graphically Fig. 21 (a)(d);
much required proof-of-concept study.
given set function parameter values, initial conditions, input value,
quantitative model produces single quantitative behaviour (this contrasts qualitative
models produce list possible behaviours model). Parameter values
chosen models approached steady state time period
test. models implemented Matlab 5.3 using ODE15s ODE solver.
853

fiCoghill, Srinivasan, & King

3

3

Tank
Tank B

2.9

Tank
Tank B
2.5

2.8

2.7
2

Level

Level

2.6

2.5

1.5

2.4
1
2.3

2.2
0.5
2.1

2

0

50

100

150

200

250
300
Time seconds

350

400

450

0

500

3

0

50

100

150

200

250
300
Time seconds

350

400

450

500

20

Tank
Tank B

Displacement
Velocity

15

2.5
10

Displacement/Velocity

Level

2

1.5

5

0

5

1
10

0.5
15

0

0

50

100

150

200

250
300
Time seconds

350

400

450

20

500

0

50

100

150

200

250
300
Time seconds

350

400

450

500

Figure 21: graph example numeric behavior (a) u-tube, top left; (b) coupled
tanks, top right; (c) cascaded tanks, bottom left, (d) damped spring,
bottom right.

time point generated simulation made available part sampled data.
ensures sampling rate suitably fast respect Nyquist criterion.
also guarantees sufficient number data points available required
Beckman filter.
4.2.2 Method
Noise-free data. use following method evaluating ILP-QSIs system-identification
performance noise-free data:
four test-systems:
(a) Obtain system behaviour test system number different initial
conditions input values. Convert qualitative states using
procedure Section 4.1.1.
(b) Using qualitative states obtained training data construct set models
using ILP-QSI record precision result (this proportion
854

fiQualitative System Identification

models result equivalent correct model). Thus,
training data set, result returned ILP-QSI precision
0.0 1.0.
following details relevant: (a) quantitative models put three
separate initial conditions, magnitude two state variables set (2, 0),
(0, 3) (2, 3). Specifically, initial values two tank levels three
tank systems, displacement velocity spring. values
crucial chosen initial conditions caused numerical models
converge steady state system reasonable number iterations; (b)
initial condition gave rise system behaviour hence set qualitative states.
second step above, qualitative states behaviours used training data.
kernel subsets necessary correct system identification usually contain
qualitative states multiple quantitative behaviours. (c) conversion process results
erroneous qualitative states (see Section 4.1.1). Thus, training data used contain
false positives false negatives.
Noisy data. use following method evaluating ILP-QSIs system-identification
performance noisy quantitative data:
four test-systems:
(a) Obtain system behaviour test system number different initial
conditions input values.
(b) Corrupt system behaviour additive noise;
(c) Convert corrupted behaviour qualitative states using procedure
Section 4.1.1.
(d) Using qualitative states obtained training data construct set models
using ILP-QSI record precision result (this proportion
models result equivalent correct model). Thus,
training data set, result returned ILP-QSI precision
0.0 1.0.
second step noise added numerical data sets follows. Gaussian noise
signal (with 0.0 1.0) generated using built-in Matlab function
normrnd scaled three orders magnitude original noise, namely 0.01, 0.1
1.0 (representing low, medium high amounts noise respectively). scaled
noise variants added numerical values system behaviour obtained
initial condition.
4.3 Quantitative Experimental Results
process converting quantitative qualitative states introduces errors, even
noise free data. Table 4 shows proportion correct qualitative states total
number qualitative states obtained numerical signal, including noisy
states. table shows proportion four systems, different degrees
855

fiCoghill, Srinivasan, & King

Model
u-tube

Coupled

Cascaded

Spring

Noise level
0
0.01
0.1
1
0
0.01
0.1
1
0
0.01
0.1
1
0
0.01
0.1
1

Initial States
(2,0)
(0,3)
(2,3)
4/6
4/6
3/5
2/8
2/8
2/9
2/10
2/10
2/15
2/37
2/37
2/53
6/14
5/13
5/14
6/16
4/14
4/12
6/16
5/25
4/15
6/58
6/61
4/46
5/8
5/8
3/8
4/10
4/12
2/9
4/17
4/21
2/13
4/39
4/49
4/38
20/33 19/35 22/39
23/48 20/36 18/41
23/49 20/38 18/44
20/65 20/52 19/53

Table 4: input data numeric experiments, described proportion
number clean states / total number converted states different systems
degrees noise.

noise. numerical simulations intended exhaustive cover
every possible behaviour; surprising observe case
states complete envisionment generated.
results qualitative experiments detailed previous section indicate
order successfully learn target model data branches envisionment
required, greater number states used greater liklihood
learning target model structure. Therefore experiments utilised states
generated numerical simulations.
results numerical data experiments shown Fig. 22. experiments show possible learn models clean noisy numerical data even
qualitative states generated clean numerical data contain number
unavoidable data transformation errors. results systems used
follows:
spring system system 31 states complete envisionment Table 4
shows quantitative qualitative conversion process yields around 20
states. seen Fig. 17 learning 20 states gives 100% precision
learning target model, even presence noise. surprising,
therefore, find learning precision perfect highest noise level.
Since qualitative experiments done sampling, slight downturn
highest noise level could due large number noisy states generated
856

fiQualitative System Identification

1
0.9
0.8

Utube
Coupled Tanks
Cascaded Tanks
Spring

Precision

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
0.01
0.1
1.0

Amount Noise

Figure 22: comparison results numerical learning tests, averaged
three initial conditions. Tests attempted learn states,
cascaded tanks, likely fail large numbers
states, spring. consistent kernel subset principle
introduced Section 2.4 since model-learning precise without presence
certain key states input data.

experiment. Hence say results keeping qualitative
experiments.
u-tube & coupled tanks complete envisonments systems contain 6 10
states respectively. Table 4 shows number true states generated less
complete envisionment, significantly less number noisy states
case. one would expect results presented Fig. 8 u-tube gives
better results coupled tanks (having higher proportion envisionment
states present). Ultimately, ability learn model completely curtailed
noise; though sooner case coupled tanks (which qualitative
experiments show sensitive presence noise). consistent
results qualitative experiments.
cascaded tanks qualitative learning experiments kernel subset triples
state 0 included. state representing situation tanks
empty begin with, one initial states included numerical
857

fiCoghill, Srinivasan, & King

simulations. Also, perusal Fig. 14 reveals introduction noise radically
reduces learning precision, 4 states (the average number true states
generated qualitative quantitative conversion process) zero. Taking
account facts expected cascaded tanks model would
successfully identified experiments. consistent
findings qualitative experiments.

5. Application Biological System Identification
work reported thus far aimed demonstrating viability ILP-QSI
identifying bounds operation approach. section examine scalability
method identify complex real world biological network. use glycolysis
pathway test case identification.
5.1 Test System: Glycolysis
chose study metabolic pathway glycolysis test case. Glycolysis one
important ubiquitous biology, also historically one first
discovered, still presents challenge model accurately.
QSIM primitives sufficient model adequately qualitative behaviour
glycolysis pathway. are, however, two problems. First, biologists would
understand model, would reason much higher level abstraction. Second, computational complexity corresponding system identification task
glycolsis (a qualitative model 100 QSIM relations) is, least currently,
intractable. address modelling metabolic pathways abstract
manner using biologically meaningful metabolic components (MC) (a similar approach
constructing complex qualitative models human heart used Bratko, Mozetic,
& Lavrac, 1989). Specifically, note metabolic pathways, essentially two
types object: metabolites (small molecules) enzymes (larger molecules catalyze
reactions). use component models objects described (King,
Garrett, & Coghill, 2005).
5.1.1 Modelling Metabolites Enzymes
concentrations metabolites vary time synthesised utilised
enzymatically catalysed reactions. result concentration given time-point
function of: (a) concentration previous time-point; (b) degree
used created various enzyme reactions.
modelling enzymes, enzyme assumed two substrates
two products. two substrates products considered form
substrate product complex, amount complex proportional
amount substrates products multiplied together. models probability
substrates (or products) collide enzyme sufficient timeliness
catalysed product complex (or substrate complex). substrate complex
converted product complex, disassociates product metabolites,
vice versa. shall use phrase flow enzyme denote amount
858

fiQualitative System Identification

substrate complex formed minus amount product complex formed. (See work
Voit Radivoyevitch (2000) details enzyme kinetics.)

Metabolite

Metabolites1

Metabolites2
E nzm

Metab

*
S_for
w

dt
Mtb

M_dt

M+

-

M+

P_rev

Sum

-

FlowFlow+

*
Flow1 Flown

Metabolitep1

Metabolitep2

Figure 23: Metabolic Components (MCs) used biological experiments,
underlying QSIM primitives.
Quantitative, corresponding qualitative representations metabolite enzymes using QSIM relations, therefore:
Enzymes:

Metabolites:
n

X
dM
=
F lowi
dt
i=1
DERIV(M etabolite,Mdt),
SUM(F low1 , . . ., F lown ,
Md t).

(4)

F lowi = f (




s=1

etabolites) g(

P


etabolitep)

(5)

p=1

PROD(M etabolite1, . . ., etabolites, S-for),
PROD(M etabolite1, . . ., etabolitep, P-rev),
M+(S-for, Ds),
M+(P-rev, Dp),
SUB(Ds, Dp, F low),
MINUS(F low,F lowminus).

Here, refers input metabolites enzymatic reaction, substrates, P
refers products enzymatic reaction. SUM() PROD() predicates simply
extensions ADD() MULT() predicates, number inputs. Fig. 23 shows
constraints grouped together metabolic components (MCs). permits
us create general constraints representing metabolite enzyme components
follows:
ENZYME((S1 , S2 ) (P1 , P2 ) enzymeF low)
METABOLITE(metaboliteConc metaboliteF low (enzymeF low 1 . . . enzymeF lown ))
859

fiCoghill, Srinivasan, & King

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

(Hexokinase):
(Phosphoglucose isomerase):
(Phosphofructokinase):
(Aldolase):
(Triose phosphate isomerase):
(Glyceraldehyde 3-phosphate dehydrogenase):
(Phosphoglycerate kinase):
(Phosphoglycerate mutase):
(Enolase):
(Pyruvate kinase):

Glc + ATP G6P + ADP.
G6P F6P.
F6P + ATP F16BP + ADP
F16BP DHAP + G3P
DHAP G3P
G3P + NAD 13BP + NADH.
13BP + ADP 3PG + ATP.
3PG 2PG.
2PG PEP
PEP + ADP Pyr + ATP.

Figure 24: reactions included qualitative model glycolysis. reactions
consume ATP NADH explicitly included.

ENZYME predicate identifies substrates products (the first argument)
returns single variable representing flow enzyme (the second argument).
METABOLITE predicate relates level flow metabolites (the first second
arguments) flow enzymes (the third argument).
5.1.2 Modelling Glycolysis
Using qualitative components representing metabolites enzymes, construct qualitative model glycolysis. model uses 15 metabolites, namely: pyruvate (Pyr), glucose (Glc), phosphoenolpyruvate (PEP), fructose 6-phosphate (F6P), glucose 6-phosphate
(G6P), dihydroxyacetone phosphate (DHAP), 3-phosphoglycerate (3PG), 1,3-bisphosphoglycerate
(13BP), fructose 1,6-biphosphate (F16BP), 2-phosphoglycerate (2PG), glyceraldehyde 3phosphate (G3P), ADP, ATP, NAD, NADH. included H+, H 2 O, Orthophosphate assumed ubiquitous (in addition, restriction substrates
products three number prevents inclusion).
qualitative state glycolysis defined set qualitative states 15
metabolites. Table 5 representation one qualitative state. understand
state consider first entry intended represent qualitative state NAD (that is,
NAD concentration: < 0, ), dec >, NAD flow: < (, 0), dec >). meaning
concentration NAD positive (0, ) decreasing (dec),
rate change concentration NAD (in analogy physical systems, flow
NAD) negative (, 0) decreasing (dec). Similar meanings apply
metabolites. Note metabolic concentrations must 0 ; cannot
negative, 0 state uninteresting.

Using representation, possible model glycolysis shown Fig. 25. model describes constraints levels flows metabolites. Thus, constraint enzyme((G3Pc,
NADc), (13BPc, NADHc), Enz6f) states flow enzyme 6 (Enz6f) controls transformation concentrations G3Pc NADc levels 13BPc
NADHc; whereas constraint metabolite(NADc, NADc, (Enz6f, -)) states
860

fiQualitative System Identification

Metabolite
NAD
NADH
ATP
ADP
Pyr
Glc
PEP
F6P
G6P
DHAP
3PG
13BP
F16PB
2PG
G3P

Concentration
< (0, ), dec >
< (0, ), inc >
< (0, ), dec >
< (0, ), dec >
< (0, ), inc >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), inc >
< (0, ), std >
< (0, ), inc >
< (0, ), dec >
< (0, ), inc >

Flow
< (, 0), dec >
< (0, ), inc >
< (, 0), dec >
< (, 0), dec >
< (0, ), dec >
< (, 0), inc >
< (, 0), dec >
< (, 0), dec >
< (, 0), dec >
< (, 0), dec >
< (0, ), std >
< 0, inc >
< (0, ), dec >
< (, 0), dec >
< (0, ), inc >

Table 5: legal qualitative state 15 metabolites observed glycolysis.
ENZYME((Glcc, ATPc),(G6Pc,ADPc),Enz1f),
ENZYME((G6Pc),(F6Pc),Enz2f),
ENZYME((F6Pc,ATPc),(F16BPc,ADPc),Enz3f),
ENZYME((F16BPc),(G3Pc,DHAPc),Enz4f),
ENZYME((DHAPc),(G3Pc),Enz5f),
ENZYME((G3Pc,NADc),(13BPc,NADHc),Enz6f),
ENZYME((13BPc,ADPc),(3PGc,ATPc),Enz7f),
ENZYME((3PGc),(2PGc),Enz8f),
ENZYME((2PGc),(PEPc),Enz9f),
ENZYME((PEPc,ADPc),(Pyrc,ATPc),Enz10f),
METABOLITE(ATPc,ATPf, (Enz10f +), (Enz7f, +),(Enz1f, -),(Enz3f, -)),
METABOLITE(ADPc,ADPf,(Enz1f, +),(Enz3f, +),(Enz10f, -)(Enz7f, -)),
METABOLITE(NADc,NADf,(Enz6f, -)),
METABOLITE(NADHc,NADHf,(Enz6f, +)),
METABOLITE(Pyrc,Pyrf,(Enz10f, +)),
METABOLITE(Glcc,Glcf,(Enz1f, -)),
METABOLITE(PEPc,PEPf,(Enz9f, +),(Enz10f, -)),
METABOLITE(F6Pc,F6Pf,(Enz2f, +),(Enz3f, -)),
METABOLITE(G6Pc,G6Pf,(Enz1f, +),(Enz2f, -)),
METABOLITE(DHAPc,DHAPf,(Enz4f, +),(Enz5f, -)),
METABOLITE(3PGc,3PGf,(Enz7f, +),(Enz8f, -)),
METABOLITE(13BPc,13BPf,(Enz6f, +),(Enz7f, -)),
METABOLITE(F16BPc,F16BPf,(Enz3f, +), (Enz4f, -)),
METABOLITE(2PGc,2PGf,(Enz8f, +),(Enz9f, -)),
METABOLITE(G3Pc,G3Pf,(Enz5f, +),(Enz4f, +),(Enz6f, -)).

Figure 25: representation qualitative model glycolysis (see text details).

concentration (NADc) flow (NADf) metabolite NAD controlled flow
single enzyme number 6 (Enz6f : Glyceraldehyde 3-phosphate dehydrogenase),
enzyme removes (signified -) NAD (+ would mean enzyme flow adds
corresponding metabolite).
861

fiCoghill, Srinivasan, & King

5.2 Experimental Aim
specific system identification task interested is: Given qualitative observations metabolic states, ILP-QSI identify correct qualitative model glycolysis?
5.3 Materials Method
methodology depicted Fig. 26, describe two separate ways identifying
biochemical pathways. make following assumptions:
1. data sparse necessarily measured part continuous time series.
realistic given current experimental limitations metabolomics, rules
possibility numerical system identification approaches.
2. metabolites known structure involved model. reason
employ chemoinformatic heuristic decrease number possible
reactions. heuristic based reasonable assumption chemical
reaction catalysed enzyme breaks chemical bonds. Full details
paper King et al. (2005). strongest assumption make. Even given
rapid advance metabolomics (NMR, mass-spectroscopy, etc.), currently
realistic assume relevant metabolites pathway observed
structure determined.
3. metabolites known structure involved particular pathway.
restriction current metabolomics technology observe compounds
structurally identified.
4. reactions involve three substrates three products.
5. qualitative states: measure direction change metabolite level
first-derivative. requires sampling level least three times succession.
5.3.1 Logical/Graph-based Constraints
first considered logical/graph-based (LG) nature problem. specific domain metabolism imposes strong constraints possible LG based models. used
constraints following way:
1. Chemical reactions conserve matter atom type (Valdes-Perez, 1994). glycolysis generated possible ways combining 18 metabolites form matter
atom type balance reactions ( 3 reactants 3 products). produced 172
possible reactions substrates balanced products number type
element. number compares well 2,300,000 possible reactions
would naively possible.
2. Typical biochemical reactions make/break bonds, cannot arbitrarily
rearrange atoms make new compounds. reaction considered plausible
broke 1 bond per reactant. analysis done originally hand,
subsequently developed general computer program automate task.
862

fiQualitative System Identification

LG Modelling

possible
combinations
metabolites
Conservation mass
element type.

QR Modelling

Bonds broken

Typical
biochemistry

QR System
Identification

QR

2

Glycolysis

Glycolysis

Figure 26: Metabolic System Identification methodology.
172 balanced reactions 18 considered chemically plausible. 18
reactions, 10 actual reactions glycolysis 8 decoy reactions.
5.3.2 Qualitative Reasoning Constraints
used simple generate test approach learning. first computational
experiment used 10 reactions glycolysis 8 decoy reactions
considered chemically feasible, see Fig. 24. reactions, absence evidence
contrary, considered irreversible. first generated possible ways
combining 18 reactions connected 15 main substrates glycolysis (models
non-disjoint). generated 27,254 possible models 10 reactions -
necessary look models reactions target (parsimony),
models generated size order. smallest number reactions necessary
include 15 metabolites size 5. 27,254 models involved reaction:
glyceraldehyde 3-phosphate + NAD 1,3-bisphosphoglycerate + NADH (reaction 6);
could immediately conclude reaction occurred glycolysis.
863

fiCoghill, Srinivasan, & King

formed example qualitative states glycolysis using QR simulator (in pseudo
random manner) test models. states thus generated contain noise.
27,254 possible models tested states, model could
generate particular state removed consideration (accuracy constraint).
Note flows metabolites enzyme observed -
intermediate variables. observe overall levels flows metabolites.
makes system identification task much harder.
efficiency, used fast YAP Prolog compiler. also formed compiled
versions enzyme metabolite MCs (input/output look-up tables), compiled
parts QSIM. also adopted resource allocation method employed increasingly computationally expensive tests: i.e. forming filter tests exponentially increasing
numbers example states.
5.3.3 Results
several months compute time 65 node Beowulf cluster reduced 27,254
possible models 35 (a 736 fold reduction). models included target model
(glycolysis), plus 34 models could qualitatively distinguished it.
35 models included following six reactions (see Fig. 24):
3. F6P + ATP F16BP + ADP
4. F16BP DHAP + G3P
5. DHAP G3P
6. G3P + NAD 13BP + NADH
8. 3PG 2PG
9. 2PG PEP
reactions form core glycolysis.
Examining 35 models also revealed correct model fewest cycles,
however know general phenomenon.
attempted use Progol positive compression measure distinguish models. based comparing models randomly generated states.
However, unsuccessful model covered 100,000 random states
generated! believe due extremely large state space. However, simple
modification approach work. produce random states glucogenesis
(glycolysis driven reverse direction), true model glycolysis covers fewer
examples 34 alternatives, identified true model. Note
approach, unlike use Progol positive compression measure, requires
new experimental data obtained.
Thus demonstrated method learning qualitative models dynamic
systems scalable handle relatively large metabolic system. achieved
means MCs represent meaningful units domain, map directly
QSIM constraints abstracted. also enable us present
complex models user friendly manner, removing need understand
structure high order differential equations.
864

fiQualitative System Identification

6. Related Work
System identification long history within machine learning: present
important signposts directly relevent work here. focus
strand research deals learning qualitative models dynamic systems.
earliest description aware concerning computer program identifying quantitative model explain experimental data work Collins (1968).
procedure described heuristically searches equation structures,
linear combination functions observed variables. Better known Bacon
system (Langley, 1981), early versions largely concentrated parameter estimation problem, particular selecting appropriate values exponents
equations. example, given class algebraic equation structures Bacon.1 able
reconstruct Keplers model planetary motion data. later work (for example work Nordhausen & Langley, 1993) attempted extend work deal
identifying algebraic structure relevant parameters, Bacon highlighted
importance bias (Mitchell, Keller, & Kedar-Cabelli, 1986) machine learning,
constraining possible model structures space possible models conforming structures. quantitative equation discovery systems lineage are:
Coper (Kokar, 1985), uses dimensional analysis restrict space equations;
Fahrenheit/EF (Langley & Zytkow, 1989) E examine space bivariate equations; Abacus (Falkenhainer & Michalski, 1986) identify piecewise
equations; Sds uses type dimensionality restrictions constrain space
equations; Lagrange family equation finders (Dzeroski & Todorovski, 1993; Todorovski & Dzeroski, 1997; Todorovski et al., 2000; Todorovski, 2003) attempt identify
models form ordinary partial differential equations; IPM (Langley, George,
Bay, & Saito, 2003) extensions developments, Prometheus/RPM (Bridewell,
Sandy, & Langley, 2004; Asgharbeygi, Bay, Langley, & Arrigo, 2006), incorporate
process descriptions (Forbus, 1984) aid construction revision quantitative
dynamic models.
Focussing specifically non-classical system identification metabolic models, perhaps notable work identification Arkin, Shen, Ross (1997)
identified graphical model reactions part glycolysis experimental data.
work Reiser, King, Kell, Muggleton, Bryant, Oliver (2001) presents unified
logical approach simulation (deduction) system identification (induction abduction). interesting recent approach, presented Koza, Mydlowec, Lanza, Yu, Keane
(2000), examines identification metabolic ODE models using genetic programming
techniques. this, cellular system viewed electrical circuit space
possible circuits searched means genetic programming approach.
earliest reported work identification qualitative models Mozetic,
1987, colleagues, identified model electrical activity heart. work,
reported fully (Bratko et al., 1989) remains landmark effort qualitative
modelling complex biological system. However, researchers noted (Bratko,
Muggleton, & Varsek, 1992), results obtained static models
provide insight models dynamic systems identified.
865

fiCoghill, Srinivasan, & King

first machine learning system learning qualitative models dynamic systems
Genmodel (Coiera, 1989a, 1989b). Genmodel need negative examples
system behaviour models learned restricted qualitative relationships amongst
observed variables (that is, intermediate, hidden, variables hypothesized). model, obtained using notion specific generalization observed
variables (in sense Plotkin, 1971), usually over-constrained. is, contained
constraints necessary characterize fully dynamics system
modeled. updated version Genmodel developed Hau Coiera (1993) showed
dimensional analysis (Bhaskhar & Nigam, 1990) could used form directed
negative example generation. new version could learn real-valued experimental data (which converted internally qualitative form), still required
variables known measured outset. system MISQ, entirely similar
complexity abilities earlier version Genmodel developed Kraan,
Richards, Kuipers (1991). later re-implemented general-purpose relational learning program Forte (Richards & Mooney, 1995), allowed hypothesis
intermediate variables (Richards, Kraan, & Kuipers, 1992). relational pathfinding
approach used MISQ (through auspices Forte) special form Inductive Logic
Programming, general framework much powerful
Bratko colleagues first view problem learning dynamic qualitative
models explicitly exercise Inductive Logic Programming (ILP) first demonstrated possibility introducing intermediate (unobserved) variables models.
used ILP system Golem (Muggleton & Feng, 1990) along QSIM representation produce model u-tube system. model identified equivalent
accepted model (in sense predicted behaviour) structure
generated form could help explain behaviour (Coghill & Shen, 2001).
Like Genmodel, model produced constrained. Unlike Genmodel, Golem
required positive negative examples system behaviour shown Hau
Coiera (1993) sensitive actual negative examples used.
Say Kuru (1996) describe program system identification qualitative data
called QSI. QSI first finds correlations variables, iteratively introduces
new relations (and intermediate variables), building model comparing output
model known states satisfactory model found. Say Kuru
characterized approach one diminishing oscillation approaches correct
model. Like Genmodel MISQ, QSI require negative observations system
behaviour. Unlike systems, use dimensional analysis
appear mechanism incorportating constraints easily within program.
importance dimensional analysis recognised though: authors suggest
central search procedure.
Thus, identification quantitative models longer history machine
learning, learning qualitative models also subject notable research efforts.
view, MISQ (the version implemented within Forte) QSI probably represent
state-of-the-art area. primary shortcomings these:
866

fiQualitative System Identification

apparent description experimental evaluation MISQ whether
able handle imperfect data (the correctness theorem presented
applies complete, noise-free data).
MISQ seeks constrained model consistent data. Often,
exactly opposite sought (that is, want parsiomonius model).
QSI deals qualitative data appear include easy mechanism incorporation new constraints guide search.

7. General Discussion
paper presented method learning qualitative models dynamic systems
time-series data (both qualitative quantitative). section discuss
general findings limitations, well suggesting number directions developing
research theme.
7.1 Computational Limitations
major limitation ILP-QSI system identifying glycolysis time taken
(several months Beowulf cluster) reduce models 27,000 possible ones
generated using chemoinformatic constraints, single correct one using qualitative
state constraints. would preferable process faster, important
note identifying system 10 reactions 15 metabolites scratch
extremely hard identification task. doubt human could achieve it,
believe would challenge system identification methods aware of.
difficult compare system identification methods believe need
competitions run KDD compare methods.
computational time identification dominated time taken test
particular model produce certain observed states: examining 27,000 models
unusual machine learning program, unusual program take hours
test individual examples covered. slow speed identification method
therefore problem normally considered learning method (i.e.
search space possible models done), rather, intrinsic complex
relationship model states defines. cover-test method is, worst
case, exponential maximum size model. Note lack efficient, i.e.
polynomial algorithm, determine cover using qualitative states.
believe inherent difficulty task applies quantitative qualitative
models. areas mathematics moving discrete real domain
simplify problems - basis much power analysis. However,
currently little evidence case system identification, quantitative
models would seem aggravate problem. cover tests essentially deductions:
set axioms rules (computer program/model) produce particular logical sentence
(observed state); general non-computable. However, real scientific systems,
bounded space time, non-computability problem, however
expect system identification methods struggle task (Sadot, Fisher, Barak,
Admanit, Stern, Hubbard, & Harel, 2008).
867

fiCoghill, Srinivasan, & King

7.2 Kernel Subsets
presentation results experimentation clear certain subsets
states (termed kernel subsets) guarantee target model learned.
analysis kernel subsets state sets, hypothesise kernel sets reflect
qualitative structure system interest.
coupled system, order learn structure system high degree
precision, data used come tests yielding qualitatively different behaviors:
i.e. behaviors would appear distinct branches envisionment graph. However,
hypothesis provides necessary, sufficient, condition learning
identify states branch suitable starting points experiment.
example consider coupled tanks system. One could select states 9 11;
different branches yet form kernel subset. hand,
noted presenting results system key states kernel subsets
states 7 8. states different branches represent critical points
first derivative state variables system. indicates importance
states definition system.
test set state variables critical points
test could run short time correct model structure identified. However,
probably impossible practice set test; especially situation
structure system completely unknown. alternative set multiple tests
state variables set extrema: initial conditions states
envisionment eventually passed through. However still may difficult
set tests, could take long time complete. two scenarios form
ends spectrum within practical experimental setting lie.
identification best strategies important area research present
work clearly relevant.
hand, cascaded systems kernel sets capture asymmetry
structure. extrema critical points play important role;
case subordinate fact ILP-QSI automatically decomposes system
constituent parts learning. fact points important conclusion learning
larger scale complex systems; namely learning facilitated by, possible,
decomposing system cascaded subsystems.
7.3 Future Work
validated ILP-QSI data derived real biological systems, next step
explore successful modelling real experimental data. would relatively
straightforward obtain data water tanks springs, would much
interesting work real biological data. work successful likely
quantitative qualitative conversion process need improved. Although
focus work here, developing rigorous approach would crucial using
ILP-QSI laboratory setting (Narasimhan, Mosterman, & Biswas, 1998).
done much easier use real experimental data analysis ILP-QSI.
Specifically, improvements required ability extract qualitative states
868

fiQualitative System Identification

passed numerical simulation, whilst minimizing noise. Nevertheless,
direct limitation ILP-QSI method.
following possibilities would benefit investigation:
QR representation used could changed QSIM detailed
flexible one Morven (Coghill & Chantler, 1994; Coghill, 1996).
hypotheses presented kernel subsets, formed
states others, need confirmed analyzed further.
ability map explore features model space would great
use planning enhancements and, alongside kernel subsets, help give
understanding exactly states allow reliable learning.
Large scale complex systems generally identified piece piece. results
cascaded tanks experiments indicate circumstances may
easily facilitatied. investigation warranted.
alternative methods described paper, incremental approach
identifies subsystems complete system interesting avenue investigation
(Srinivasan & King, 2008).

8. Summary Conclusions
paper presented novel system, named ILP-QSI, learns qualitative
models dynamic processes. system stands squarely strand research
integrates Machine Learning Qualitative Reasoning extends work area
following ways:
ILP-QSI algorithm extends work; branch bound algorithm
makes use background knowledge (at least) three kinds order focus guide
search well posed models dynamic processes.
Syntactic Constraints: model size prespecified; models must complete
determinate; must proliferate instances qualitative relations.
Semantic Constraints: model must adequately explain data; must contain
relations redundant contradictory; relations model must
respect dimensional constraints.
System Theoretic Constraints: model singular disjoint; endogenous variables must appear least two relations; model
causally ordered.
thoroughly tested system number well known dynamic processes.
enabled us ascertain ILP-QSI capable learning variety
conditions noisy noise free data. testing also allowed us identify
conditions possible learn appropriate model dynamic system.
conclusions aspect work are:
869

fiCoghill, Srinivasan, & King

Learning precision related richness (or sparcity) noisiness data
learning performed.
target model precisely learnt data used kernel subset.
kernels made states different branches envisionment graph.
system critical points play important role identifying model structure.
spectrum possibilities regard setting suitable experiments garner data learn models physical biological systems
interest.
Cascaded parts systems help identify suitable points decomposition model
learning.
ILP-QSI designed learn qualitative structural model qualitative data,
sometimes case original measurements quantitative (albeit sparse
possibly noisy). order ascertain ILP-QSI would cope qualitative data generated quantitative measurements carried proof-of-concept set experiments
physical process models previously utilised. results
keeping results obtained qualitative experiments. adds weight
conclusions regarding viability approach learning structural models
dynamic systems adverse conditions.
Finally, order test scalability method, applied ILP-QSI large
scale metabolic pathway: glycolysis. case search space deemed large
attempt learning QSIM primitives alone. However, knowledge domain enabled us
group primitives set Metabolic Components models metabolic
pathways easily constructed. Also, part research Logical graph
based models used represent background domain knowledge. Utilising these,
able identify 35 possible structures glycolysis pathway (out possible
27,254); target model fewest cycles (though know
general phenomenon) minimally covered data generated reverse pathway
glucogenesis.
overall conclusions work qualitative reasoning methods combined
machine learning (specifically ILP) successfully learn qualitative structural models
systems high complexity number adverse circumstances. However, work
reported herein constitutes step line research recently begun; and,
interesting lines research, raises turn interesting questions need
addressed.
Acknowledgments
work supported part BBSRC/EPSRC grant BIO10479. authors would
like thank Stephen Oliver Douglas Kell helpful discussions biological
aspects paper. would also like thank Simon Garrett many interesting
fruitful interactions.
870

fiQualitative System Identification

Appendix A. Derivation Solution Space Tanks Systems
appendix provide summary whence solution spaces tanks systems
utilised project constructed. details regarding envisionments
associated solution spaces may found work Coghill et al. (1992) Coghill
(2003).
order facilitate analysis need make use quantitative version
system models. ease exposition make additional assumption
systems linear.10
A.1 U-tube
quantitative model u-tube system
dh1
= k(h1 h2 )
dt
dh2
= k(h2 h1 )
dt
inspection two equations easy see (ignoring trivial case
k = 0) derivatives two equations zero when:
h1 = h 2

(6)

is:
dh2
dh1
=
=0
dt
dt
accounts relationship, depicted Fig. 18, h 1 h2
derivatives zero. envisionment table u-tube (Table 1 Section 2.2)
see state zero derivatives state 5; hence represented line.
h1 = h 2

A.2 Coupled Tanks
quantitative model coupled tanks system
dh1
= qi k1 (h2 h1 )
dt
dh2
= k1 (h2 h1 ) k2 h2
dt


dh1
dt

(7)
(8)

= 0 Equation 7 rewritten as:
0 = qi k1 (h2 h1 )
= q k 1 h2 k 1 h1

10. fact types non-linearity normally associated systems kind solution spaces
qualitatively identical described here, although analysis required construct
slightly complicated.

871

fiCoghill, Srinivasan, & King

re-arranged give
h2 =

qi
h1
k1

qi zero reduces
h2 = h 1


dh2
dt

(9)

= 0 Equation 8 rewritten as:
0 = k1 (h2 h1 ) k2 h2
= k 1 h1 k 1 h1 k 2 h2
= (k1 k2 )h2 k1 h1


(k1 k2 )h2 = k1 h1
Re-arranging gives
h2 =

k1
h1
(k1 k2 )

(10)

accounts relations h 1 h2 depicted solution space Fig.
18.
A.3 Cascaded Tanks
quantitative model cascaded tanks system is:



dh1
dt

dh1
= q k 1 h1
dt

(11)

dh2
= k 1 h1 k 2 h2
dt

(12)

= 0 Equation 11 re-arranged as:
qi = k 1 h1


h1 =


dh2
dt

qi
k1

= 0 Equation 12 rewritten as:
k2 h2 = k 1 h1


872

fiQualitative System Identification

h2 =

k1
h1
k2

accounts relations h 1 h2 depicted solution space Fig.
27.

h2

h'1 = 0

3

7

11

1

h'2 = 0

4
12

8
5
9

13

h1

0

6

10

2

Figure 27: solution space cascaded tanks system.

References
Arkin, A., Shen, P., & Ross, J. (1997). test case correlation metric construction
reaction pathway measurements. Science, 277, 12751279.
Asgharbeygi, N., Bay, S., Langley, P., & Arrigo, K. (2006). Inductive revision quantitative
process models. Ecological modelling, 194, 7079.
Bergadano, F., & Gunetti, D. (1996). Inductive Logic Programming: Machine Learning
Software Engineering. MIT Press.
Bhaskhar, R., & Nigam, A. (1990). Qualitative physics using dimensional analysis. Artificial
Intelligence, 45, 73111.
Blackman, R. B., & Tukey, J. W. (1958). Measurement Power Spectra. John Wiley
Sons, New York.
Bradley, E., Easley, M., & Stolle, R. (2000). Reasoning nonlinear system identification. Tech. rep. CU-CS-894-99, University Colorado.
Bratko, I., Mozetic, I., & Lavrac, N. (1989). KARDIO: Study Deep Qualitative
Knowledge Expert Systems. MIT Press, Cambridge, Massachusetts.
873

fiCoghill, Srinivasan, & King

Bratko, I., Muggleton, S., & Varsek, A. (1992). Learning qualitative models dynamic
systems. Muggleton, S. (Ed.), Inductive Logic Programming, pp. 437452. Academic
Press.
Bridewell, W., Sandy, J., & Langley, P. (2004). interactive environment modeling
discovery scientific knowledge.. Tech. rep., Institute Study Learning
Expertise, Palo Alto, CA.
Camacho, R. (2000). Inducing Models Human Control Skills using Machine Learning
Algorithms. Ph.D. thesis, University Porto.
Coghill, G. M. (1996). Mycroft: Framework Constraint-Based Fuzzy Qualitative Reasoning. Ph.D. thesis, Heriot-Watt University.
Coghill, G. M. (2003). Fuzzy envisionment. Proc. Third International Workshop
Hybrid Methods Adaptive Systems, Oulu, Finland.
Coghill, G. M., Asbury, A. J., van Rijsbergen, C. J., & Gray, W. M. (1992). application vector envisionment compartmental systems.. Proceedings first
international conference Intelligent Systems Engineering, pp. 123128, Edinburgh,
Scotland.
Coghill, G. M., & Chantler, M. J. (1994). Mycroft: framework qualitative reasoning. Proceedings Second International Conference Intelligent Systems
Engineering, pp. 4955, Hamburg, Germany.
Coghill, G. M., Garret, S. M., & King, R. D. (2004). Learning qualitative models
metabolic systems. Proceedings European Conference Artificial Intelligence ECAI-04, Valencia, Spain.
Coghill, G. M., & Shen, Q. (2001). specification multiple models diagnosis
dynamic systems. AI Communications, 14 (2), 93104.
Coiera, E. W. (1989a). Generating qualitative models example behaviours. Tech. rep.
8901, University New South Wales, Deptartment Computer Science.
Coiera, E. W. (1989b). Learning qualitative models example behaviours. Proc.
Third Workshop Qualitative Physics, Stanford.
Collins, J. (1968). regression analysis program incorporating heuristic term selection.
Dale, E., & Michie, D. (Eds.), Machine Intelligence 2. Oliver Boyd.
Dzeroski, S. (1992). Learning qualitative models inductive logic programming. Informatica, 16 (4), 3041.
Dzeroski, S., & Todorovski, L. (1993). Discovering dynamics. International Conference
Machine Learning, pp. 97103.
Dzeroski, S., & Todorovski, L. (1995). Discovering dynamics: inductive logic programming machine discovery. J. Intell. Information Syst., 4, 89108.
Falkenhainer, B., & Michalski, R. S. (1986). Integrating quantitative qualitative discovery: abacus system. Machine Learning, 1 (4), 367401.
Forbus, K. D. (1984). Qualitative process theory. Artificial Intelligence, 24, 169204.
874

fiQualitative System Identification

Garrett, S. M., Coghill, G. M., Srinivasan, A., & King, R. D. (2007). Learning qualitative
models physical biological systems. Dzeroski, S., & Todorovski, L. (Eds.),
Computational discovery communicable knowledge, pp. 248272. Springer.
Gawthrop, P. J., & Smith, L. P. S. (1996). Metamodelling: Bond Graphs Dynamic
Systems. Prentice Hall, Hemel Hempstead, Herts, England.
Hau, D. T., & Coiera, E. W. (1993). Learning qualitative models dynamic systems.
Machine Learning, 26, 177211.
Healey, M. (1975). Principles Automatic Control. Hodder Stoughton.
Iwasaki, Y., & Simon, H. A. (1986). Causality device behavior. Artificial Intelligence,
29, 332. See also De Kleer Browns rebuttal Iwasaki Simons reply
rebuttal volume journal.
King, R. D., Garrett, S. M., & Coghill, G. M. (2005). use qualitative reasoning
simulate identify metabolic pathways.. Bioinformatics, 21 (9), 2017 2026.
Kokar, M. M. (1985). Coper: methodology learning invariant functional descriptions.
Mitchell, T., Carbonell, J., & Michalski, R. (Eds.), Machine Learning: Guide
Current Research, pp. 151154. Kluwer Academic Press.
Koza, J. R., Mydlowec, W., Lanza, G., Yu, J., & Keane, M. A. (2000). Reverse engineering
automatic synthesis metabolic pathways observed data using genetic
programming.. Tech. rep. SMI-2000-0851, Stanford University.
Kraan, I. C., Richards, B. L., & Kuipers, B. J. (1991). Automatic abduction qualitative
models. Proceedings Qualitative Reasoning 1991 (QR91).
Kuipers, B. (1994). Qualitative Reasoning. MIT Press.
Langley, P. (1981). Data-driven discovery physical laws. Cognitive Science, 5, 3154.
Langley, P., George, D., Bay, S., & Saito, K. (2003). Robust induction process models
time series data.. Proc. twentieth International Conference Machine Learning,
pp. 432439, Washington, DC. AAAI Press.
Langley, P., & Zytkow, J. (1989). Data-driven approaches empirical discovery. Artificial
Intelligence, 40, 283312.
McCreath, E. (1999). Induction first order logic noisy training examples fixed
example set sizes. Ph.D. thesis, University New South Wales.
Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. (1986). Explanation-based generalization: unifying view. Machine Learning, 1, 4780.
Mozetic, I. (1987). Learning qualitative models. Bratko, I., & Lavrac, N. (Eds.),
Progress Machine Learning: Proceedings EWSL 87: 2nd European Working Session Learning, pp. 201217. Sigma Press.
Muggleton, S. (1995). Inverse Entailment Progol. New Gen. Comput., 13, 245286.
Muggleton, S. (1996). Learning positive data. Lecture Notes AI, 1314, 358376.
Muggleton, S., & Feng, C. (1990). Efficient induction logic programs. Proc.
First Conf. Algorithmic Learning Theory. OHMSHA, Tokyo.
875

fiCoghill, Srinivasan, & King

Muggleton, S., & Raedt, L. D. (1994). Inductive logic programming: Theory methods.
Journal Logic Programming, 19,20, 629679.
Narasimhan, S., Mosterman, P., & Biswas, G. (1998). systematic analysis measurement selection algorithms fault isolation dynamic systems. Proc. Ninth Intl.
Workshop Principles Diagnosis (DX-98), pp. 94101, Cape Cod, MA.
Nordhausen, B., & Langley, P. (1993). integrated framework empirical discovery.
Machine Learning, 12, 1747.
Papadimitriou, C., & Steiglitz, K. (1982). Combinatorial Optimisation. Prentice-Hall,
Edgewood-Cliffs, NJ.
Plotkin, G. (1971). Automatic Methods Inductive Inference. Ph.D. thesis, Edinburgh
University.
Reiser, P., King, R., Kell, D., Muggleton, S., Bryant, C., & Oliver, S. (2001). Developing
logical model yeast metabolism. Electronic Transactions Artificial Intelligence,
5, 233244.
Richards, B. L., Kraan, I., & Kuipers, B. J. (1992). Automatic abduction qualitative
models. Proc. Tenth National Conference Artificial Intelligence (AAAI92), pp. 723728. MIT Press.
Richards, B. L., & Mooney, R. J. (1995). Automated refinement first-order horn-clause
domain theories. Machine Learning, 19 (2), 95131.
Riguzzi, F. (2005). Two results regarding refinement operators. Kramer, S., & Pfahringer,
B. (Eds.), Late Breaking Papers, 15th International Workshop Inductive Logic
Programming (ILP05), August 1013, 2005, pp. 5358, Munich, Germany.
Sadot, A., Fisher, J., Barak, D., Admanit, Y., Stern, M. J., Hubbard, E. J. A., & Harel, D.
(2008). Towards verified biological models.. IEEE/ACM Trans. Comput. Biology
Bioinformatics., 5(2), 112.
Say, A. C. C., & Kuru, S. (1996). Qualitative system identification: deriving structure
behavior. Artificial Intelligence, 83, 75141.
Shoup, T. E. (1979). Practical Guide Computer Methods Engineers. Prentice-Hall
Inc., Englewood Cliffs, N. J. 07632.
Soderstrom, T., & Stoica, P. (1989). System Identification. Prentice Hall.
Srinivasan, A. (1999). Aleph Manual. Available http://www.comlab.ox.ac.uk/oucl/
research/areas/machlearn/Aleph/.
Srinivasan, A., & King, R. D. (2008). Incremental identification qualitative models
biological systems using inductive logic programming. J. Machine Learning Research
appear.
Todorovski, L. (2003). Using domain knowledge automated modeling dynamic systems
equation discovery. Ph.D. thesis, Faculty Electrical Engineering Computer
Science, University Ljubljana, Slovenia.
Todorovski, L., Srinivasan, A., Whiteley, J., & Gavaghan, D. (2000). Discovering structure partial differential equations example behavior. Proceedings
876

fiQualitative System Identification

Seventeenth International Conference Machine Learning, pp. 991998, San Francisco.
Todorovski, L., & Dzeroski, S. (1997). Declarative bias equation discovery. Proc. 14th
International Conference Machine Learning, pp. 376384. Morgan Kaufmann.
Valdes-Perez, R. E. (1994). Heuristics systematic elucidation reaction pathways.. J.
Chem. Informat. Comput. Sci., 34, 976983.
Voit, E. O., & Radivoyevitch, T. (2000). Biochemical systems analysis genome-wide
expression data. Bioinformatics, 16 (11), 10231037.
Warren, P., Coghill, G. M., & Johnstone, A. (2004). Top botton development
fuzzy rule-based diagnostic system. Proc. Fourth International Workshop
Hybrid Methods Adaptive Systems, Aachen, Germany.

877

fiJournal Artificial Intelligence Research 32 (2008) 385-417

Submitted 12/07; published 05/08

Qualitative Comparison Decisions Positive
Negative Features
Didier Dubois
Helene Fargier

dubois@irit.fr
fargier@irit.fr

Universite de Toulouse
IRIT- CNRS, 118 route de Narbonne
31062 Toulouse Cedex, France

Jean-Francois Bonnefon

bonnefon@univ-tlse2.fr

Universite de Toulouse
CLLE (CNRS, UTM, EPHE)
Maison de la Recherche, 5 al. Machado
31058 Toulouse Cedex 9, France

Abstract
Making decision often matter listing comparing positive negative
arguments. cases, evaluation scale decisions considered bipolar,
is, negative positive values explicitly distinguished. done,
example, Cumulative Prospect Theory. However, contrary latter framework
presupposes genuine numerical assessments, human agents often decide basis
ordinal ranking pros cons, focusing salient arguments.
terms, decision process qualitative well bipolar. article, based
bipolar extension possibility theory, define axiomatically characterize several
decision rules tailored joint handling positive negative arguments ordinal
setting. simplest rules viewed extensions maximin maximax
criteria bipolar case, consequently suffer poor decisive power. decisive
rules refine former also proposed. refinements agree principles
efficiency spirit order-of-magnitude reasoning, prevails qualitative
decision theory. refined decision rule uses leximin rankings pros
cons, ideas counting arguments equal strength cancelling pros cons.
shown come special case Cumulative Prospect Theory, subsume
Take Best heuristic studied cognitive psychologists.

1. Introduction
personal experience, also psychological experiments suggest making
decision often matter listing comparing positive negative features
alternatives (Cacioppo & Berntson, 1994; Osgood, Suci, & Tannenbaum, 1957; Slovic,
Finucane, Peters, & MacGregor, 2002). Individuals evaluate alternatives objects
considering positive negative aspects parallel instance, choosing
movie, presence good actress positive argument; noisy theater bad critiques
negative arguments. bipolar perspective, comparing two decisions amounts
comparing two pairs sets, sets pros cons attached one decision
sets pros cons attached other. Cumulative Prospect Theory (Tversky &
Kahneman, 1992) explicit attempt accounting positive negative arguments
c
!2008
AI Access Foundation. rights reserved.

fiDubois, Fargier, Bonnefon

numerical setting. proposes compute net predisposition decision,
difference two set functions (capacities) taking values positive real line,
first one measuring importance group positive features, second one
importance group negative features. general numerical models, namely
bi-capacities (Grabisch & Labreuche, 2005) bipolar capacities (Greco, Matarazzo, &
Slowinski, 2002) encompass sophisticated situations criteria independent
other. numerical approaches bipolar decision contrast standard
decision theory, account bipolarity phenomenon. Indeed, utility
functions defined increasing affine transformation, preserve
value 0.
However, cognitive psychologists claimed arguments featured decision process different strengths, decision-makers likely consider
degrees strength ordinal level rather cardinal level (Gigerenzer, Todd,
& ABC group, 1999). Individuals appear consider arguments (i.e.,
salient ones) making choice, rather attempt exact numerical computation merits decision (Brandstatter, Gigerenzer, & Hertwig, 2006). sum,
cognitive psychologists claimed human decision processes likely largely
qualitative well bipolar.
last 10 years also witnessed emergence qualitative decision theory Artificial Intelligence (Doyle & Thomason, 1999). instance, qualitative criteria like Walds
rule (Wald, 1950/1971) axiomatized along line decision theory (Brafman
& Tennenholtz, 2000) well variants extensions thereof; see survey Dubois
Fargier (2003). So-called conditional preference networks (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004) introduced easier representation preference
relations multidimensional sets alternatives, using local preference statements interpreted ceteris paribus style. recent emergence qualitative decision methods
Artificial Intelligence partly motivated traditional stress qualitative representations reasoning methods. also due fact numerical data available
many AI applications, example point requiring precise
evaluations user (e.g., recommender systems).
models use preference relations express statements like decision better decision b agent. However, preference relations cannot deal
bipolaritymore precisely, cannot express simple notion people know
good bad them, judgments orthogonal judgments
best given situation. Sometimes, best available choice
detrimental anywayand yet, occasions, even worst option still somewhat
desirable. (Note best worst option statu quo option, i.e.,
option making active choice.) fully capture situations, necessary
absolute landmark reference point model, expresses neutrality
separates positive values negative values. emphasizing qualitative
approaches decision choice, Artificial Intelligence literature area somewhat neglected fact preference orderings enough express fact
option good bad per se.
qualitative formalisms capable representing preference exploit value scales
include reference points, e.g., fuzzy constraint satisfaction problems (Dubois, Fargier,
386

fiQualitative Bipolarity Decision

& Prade, 1996) possibilistic logic (Benferhat, Dubois, & Prade, 2001). There, merit
decision evaluated different criteria means kind utility functions
mapping bounded ordinal scale whose bottom value expresses unacceptable degree
violation, whose top value expresses absence violation. Decisions
ranked according merit worst evaluation, following pessimistic attitude.
kind approach bipolar, handles negative arguments: absolute
landmark expresses unacceptability, neutrality. Neutrality present omission,
constraint violated, decision exists could better neutral.
Another kind bipolarity accounted Benferhat, Dubois, Kaci, Prade
(2006), distinguish prioritized constraints one hand, goals desires
hand. Constraints (expressed logical formulas) given prominent role:
guide initial selection tolerated decisions. Positive preferences (goals
desires) taken consideration discriminate among set tolerated
decisions. consequence, positive evaluations (no matter positive) never trump
negative evaluations (no matter negative).
Finally, topic argumentation reasoning gained considerable interest
artificial intelligence last ten years so. natural way coping
inconsistency knowledge bases (Besnard & Hunter, 2008). Argumentation naturally
bipolar nature, since construction arguments consists collecting reasons
deriving proposition reasons deriving negation, proceeding arbitration
arguments various weights (Cayrol & Lagasquie-Schiex, 2005). However
clear strength argument inconsistency-tolerant reasoning defined
number. sounds natural adopt qualitative approach bipolar nature
argument-based reasoning. Moreover AI-based decision procedures also naturally rely
argumentation, facilitate process explicating merits decision
(Amgoud & Prade, 2004, 2006).
present paper,1 aim proposing bipolar qualitative setting, equipped
family decision rules, decision based comparison positive
negative arguments whose strength assessed ordinal fashion.
insist assumption positive negative evaluations share common scale;
evaluations one polarity trump evaluations polarity, e.g.,
strong positive (resp. negative) argument win weaker negative (resp. positive)
argument. Indeed, precisely idea behind intuitive procedure weighing
pros cons, i.e., finding heavier side.This cannot done without
common importance scale. also adopt systematic approach, formalize
axiomatically characterize set procedures simultaneously ordinal bipolar.
paper structured follows. Section 2 introduces framework qualitative
bipolar choice. Then, Section 3 presents two basic qualitative bipolar decision rules.
Section 4, show basic properties bipolar reasoning expressed axiomatically, and, taking one step further, axioms capture principles qualitative
bipolar decision-making. Section 5 studies decision rules decisive
basic rules Section 3, without giving qualitative nature. Finally, Sections 6
1. paper extended version work Dubois Fargier (2006). analyzes additional
decision rules, provides full axiomatization cardinality based rule.

387

fiDubois, Fargier, Bonnefon

7 relate rules range approaches, identify avenues future
research. Proofs properties representation theorems provided appendix.

2. Framework Qualitative Bipolar Decisions
formal framework qualitative bipolar multicriteria decision consist finite
set potential decisions a, b, c, . . . ; set X criteria arguments, viewed attributes
ranging bipolar scale, say V ; totally ordered scale L expressing relative
importance criteria groups criteria. article, use simplest possible
bipolar scale V = {, 0, +}, whose elements reflect negativity, neutrality, positivity,
respectively. scale, argument X either completely against, totally
irrelevant, totally favor decision D. focus bipolarity,
simpler approach many multi-criteria decision-making frameworks
criterion x X rated numerical scale, like multi-attribute utility theory. However,
qualitative evaluations often closer human capabilities numerical ones.
basis useful see far go rough modelling preference
bipolar situation. Indeed, problem solved rigorous way without resorting
numerical evaluations, sophisticated techniques needed.
Let = {x, x(a) #= 0} set relevant arguments decision a. contains
arguments matter a, either good things
bad things. let = {x, x(a) = } set arguments decision a,
A+ = {x, x(a) = +} set arguments favor a. Considering sets A+
amounts enumerating pros cons a. Thus, comparing decisions b
amounts comparing pairs disjoint sets (A , A+ ) (B , B + ). Obviously,
B , B + A+ , clearly preferred b wide sense.
basic property bivariate monotony bipolar decision rule obey.
sake simplicity, assume following X divided two subsets.
+
X set positive arguments taking value {0, +}; X set
negative arguments taking value {, 0}. simplified model, longer
possible B + #= A+ B #= . This, however, done without loss
generality affect validity results ordinal setting. Indeed,
x whose evaluation may range full domain {, 0, +} duplicated, leading
attribute x+ X + attribute x X . Furthermore, transformation
generalize framework arguments positive negative side (e.g.,
eating chocolate).
scale L measuring importance arguments top 1L (full importance)
bottom 0L (no importance). Within qualitative approach, L finite.
common set functions, make hypothesis importance group
arguments depends importance individual arguments group.
assumption independence, levels importance directly attached
elements X function : X ' L, importance group
arguments also derived. (x) = 0L means decision-maker indifferent
argument x; (x) = 1L means argument possesses highest level attraction
repulsion (according whether applies positive negative argument). supposed
non trivial, is, least one x positive importance level.
388

fiQualitative Bipolarity Decision

Example 1 (Lucs Holidays). Luc provide us one running examples.
considering two holiday destinations, listed pros cons each. Option [a]
scenic landscapes (a strong pro), expensive, local airline
terrible reputation (two strong cons). Option [b] non-democratic region, Luc
considers strong con. hand, option [b] tennis court, disco,
swimming pool. three pros, decisive. matter,
much arguments. Note Luc give rough evaluation strong
pro con is. say gorgeous landscapes, indecent price,
terrible reputation airline company, non-democratic governance four arguments comparable importance; swimming pool, tennis disco three
arguments comparable importance, important previous ones. Formally,
let:
X + = {landscape ++ , tennis + , pool + , disco + }
X = {price , airline , governance }

subset pros,
subset cons.

Strong arguments landscape ++ , price , airline , governance . arguments
tennis + , pool + , disco + weaker. Thus, letting > > 0L , have:
(lanscape ++ ) = (price ) = (airline ) = (governance ) =
(tennis + )
= (pool + )
= (disco + )
= .
Finally options [a] [b] described following sets arguments:
Options [a] : A+ = {landscape ++ }
= {airline , price }
+
+
+
+
Options [b] : B = {tennis , pool , disco } B = {governance }.
sum, attribute x X Boolean (presence vs. absence), polarity (its
presence either good bad, absence always neutral), importance (x)
L. Now, since interested qualitative decision rules, approach relies two
modelling assumptions:
Qualitative Scale: L, big step one level merit next one.
Arguments ranked terms order magnitude figure importance
means mapping .
Focalization: order magnitude importance group arguments
prescribed polarity one important arguments group.
assumption suits use qualitative scale, means weak arguments
negligible compared stronger ones.
Technically, assumptions suggest use following measure importance
set arguments
OM(A) = max (x).
xA

terms, simply use qualitative possibility measures (Lewis, 1973; Dubois,
1986) interpreted term order magnitude importance.
next sections examine several decision rules relying use OM(A),
defined balancing pros cons. see Luc example,
389

fiDubois, Fargier, Bonnefon

prefer option [a], prefer option [b], regard two options equally
attractive, find impossible make decision. begin Section 3
two basic rules take account important arguments.
corresponding ordering complete partial, usually weakly discriminant.
due immediate use order-of-magnitude evaluations, leads
rough decision rules. Refinements basic rules proposed Section 5.
apply elementary principles simplification (discarding arguments relevant
decisions, sometimes cancelling opposite arguments strength) making
choice. refinements thus obey form preferential independence.

3. Elementary Qualitative Bipolar Decision Rules
two elementary decision rules Section differ one basic feature: first one
treats positive negative arguments separately; second one allows comparison
relative strengths positive negative arguments, one side possibly overriding
other.
decision rule defines preference relation. Since relations presented
necessarily complete transitive, let us recall definitions, prior presenting
two decision rules.
Definition 1. relation ), one define:
symmetric part:
B ) B B )
asymmetric part:
, B ) B not(B ) A)
incomparability relation: ! B not(A ) B) not(B ) A)

) said quasi-transitive , transitive. transitivity ) obviously
implies quasi-transitivity, whether complete. converse implication generally hold. relation complete, ! empty. ) said weak
order complete (and thus reflexive) transitive.
following, also use notion refinement relation:
Definition 2. )$ refines ) A, B : , B ,$ B

refined relation )$ thus follows strict preference ) any, also
make difference decisions case ) cannotthat is, may happen ,$ B
B ! B.

3.1 Bipolar Qualitative Pareto Dominance Rule
order magnitude bipolar set longer unique value L like unipolar
case, pair (OM(A+ ), OM(A )). Pairs generally vectors evaluations
easily compared others using classical principle Pareto comparison. yields
following rule, assume commensurateness evaluations
positive negative arguments:
Definition 3. )Pareto B OM(A+ ) OM(B + ) OM(A ) OM(B ).

would ,Pareto conclude Lucs example? Luc strong argument option
[a], weak arguments option [b] : OM(A+ ) > OM(B + ). parallel, Luc
390

fiQualitative Bipolarity Decision

strong arguments option [a] option [b]: OM(A ) = OM(B ).
consequence, ,Pareto B, Luc choose option [a].
Let us lay bare details cases Pareto B, ,Pareto B, !Pareto B.
B indifferent salient positive aspects well
salient negative aspects share order magnitude, i.e., OM(A+ ) = OM(B + )
OM(A ) = OM(B );
B negligible compared (A ,Pareto B) two cases: either OM(A+ ) OM(B + )
OM(A ) < OM(B ), OM(A+ ) > OM(B + ) OM(A ) OM(B ).
case B ,Pareto described symmetrically.
cases, conflict comparable B (A !Pareto B).
)Pareto obviously reflexive transitive. collapses Walds pessimistic ordering
(Wald, 1950/1971) X = X , optimistic max-based counterpart
X = X + . Note )Pareto partial maybe partial. example,
decision pros cons, )Pareto concludes incomparable decision
pro con. quite counter-intuitive, shown following
example.
Example 2 (Lucy Riviera estate). short money, Lucy planning
spend summer home. offered spend part summer
brothers paradisiacal estate Riviera. inconvenient arrangement
Lucy finds sister-in-law mildly annoying.
Formally, let: X = {estate ++ , inlaw }, (estate ++ ) > (inlaw ).
two options, going riviera staying home described follows:
Option [a]: A+ = {estate ++ } = {inlaw }.
Option [b]: B + = {}
B = {}
common intuition Lucy case really sister-in-law
mildly annoying, estate fantastic, Lucy likely prefer go rather
stay home. However, )Pareto cannot predict preference finds two
options incomparable: OM(A+ ) > OM(B + ), OM(A ) > OM(B ).
Another drawback )Pareto becomes clear two decisions order
magnitude one two dimensions:
Example 3 (Luka gyms). Luka considering buying membership one two
gyms. Option [a] expensive, strong con. Option [b] also expensive,
comes small bonus squash court (this small bonus
Luka sure yet want use court). hand, option [b] also
drawback medium importance, is, inconveniently located. Location
important price Lukas mind, still important presence
squash court. Formally:

X = {squash + , location , price }
+


).
(squash ) < (location ) < (price
391

fiDubois, Fargier, Bonnefon

options described follows:
Option [a]: A+ = {squash + } = {location , price },
Option [b]: B + =
B = {price }.
follows Definition 3 Luka prefer option [a], OM(A+ ) > OM(B + )
whilst OM(A ) = OM(B ). intuitively unsatisfying, however, would expect
Luka examine carefully fact option [a] inconveniently located,
moderately strong con, rather decide basis weak argument,
is, squash court. terms, )Pareto completely obey principle
Focalization discussed introduction: argument lower level (the squash court)
determine choice even though argument higher level (the location) would
pointed opposite direction.
problem )Pareto partly rooted fact capture
assumption positive negative evaluations share common scale. fact
argument may stronger argument opposite polarity never taken
account. propose realistic rule captures assumption.
3.2 Bipolar Possibility Relation
section, propose decision rule comparing B focuses arguments
maximal strength B, i.e., level = maxyAB (y) = OM(A B).
principle underlying rule simple: argument (resp. B)
argument pro B (resp. pro A), conversely. supported decision
preferred.
Definition 4. )BiPoss B OM(A+ B ) OM(B + ).
rule decides least good B iff, highest level importance,
arguments favor arguments attacking B. Clearly ,BiPoss B iff,
highest level, least positive element element B,
element element pro B. Obviously, )BiPoss collapses Walds pessimistic
ordering X = X , optimistic counterpart X = X + . sense,
comparison yields straightforward way generalizing possibility orderings
bipolar case.
Proposition 1. )BiPoss complete quasi-transitive.
terms, strict part relation )BiPoss transitive, associated
indifference relation generally not: BiPoss B B BiPoss C imply BiPoss
C. instance, let us denote a+ = OM(A+ ), = OM(A ), b+ = OM(B + ), b =
OM(B ), c+ = OM(C + ), c = OM(C ). Assume max(a+ , b ) = max(a , b+ )
max(b+ , c ) = max(b , c+ ). Assume b+ = b = 1L . two equalities hold regardless
values a+ , , c+ , c . values max(a+ , c ) max(a , c+ ) anything.
Similarly ,BiPoss B B BiPoss C imply ,BiPoss C counter examples
built setting c+ = c = 1L .
case Luc (Example 1), = {landscape ++ , airline , price }, B =
{governance , tennis + , swimming + , disco + }. perspective )BiPoss , options equivalently bad, since OM(A+ B ) = OM(B + ). Likewise, case
392

fiQualitative Bipolarity Decision

Luka (Example 3), )BiPoss regard two gyms equivalently bad
high price. Now, case Lucy (Example 2), remember )Pareto regarded options
[a] [b] incomparable, = {estate ++ , inlaws } B empty set.
contrast, )BiPoss consider, line common intuition, Lucy prefer go
Riviera estate, since OM(A+ B ) > OM(B + ).
)BiPoss different arguably less dubious )Pareto . But, shown
Luc Lukass examples, rough rule may decisive enough.
weakness )BiPoss rooted usual drowning effect possibility theory:
argument high importance attached decisions (e.g., ludicrous price
two gyms), trump arguments lesser importance (e.g., squash court,
also location).2 Variants )BiPoss presented Section 5 overcome
difficulty. Nevertheless, rule )BiPoss alone merit capturing essence
ordinal decision-making, shown axiomatic study presented next section.

4. Axioms Ordinal Comparison Bipolar Scale
previous sections, proposed framework decision rules intend
capture essence qualitative bipolar decision-making. present section,
adopt opposite strategy, is, formalize natural properties qualitative bipolar preference relation obeyand show framework
sound (it obeys aforementioned properties) also complete. main result
representation theorem stating preference relation satisfies properties
equivalent )BiPoss .
Let ) abstract preference relation 2X , ) B meaning decision
least good decision B. ) compares sets decisions, call set-relation.
First all, introduce general properties (e.g., reflexivity monotony) ) sensibly
obey well-behaved bipolar set-relation, qualitative not. Then, introduce
axioms characterize qualitative bipolar set-relations.
4.1 Axioms Monotonic Bipolar Set-Relations
First all, preference relation, shall assume minimal working conditions
sensible framework, reflexivity (R) quasi-transitivity (QT). 3
basic notion bipolar reasoning sets arguments separation X
good bad arguments. first axiom thus states argument either positive
negative wide sense, i.e., either worse better worse nothing:
Clarity Arguments (CA) x X, {x} ) ) {x}.
2. drowning effect also work !Pareto , within comparison A+ B + , within
comparison B . Ceteris paribus, destination gorgeous landscapes plus swimming
pool preferred destination gorgeous landscapes pool.
3. Although weak, assumptions relaxed relational approaches multicriteria evaluation,
aggregation process produces cycles preference relation. Nevertheless, methods,
even resulting relation transitive, next step build transitive approximation
it.

393

fiDubois, Fargier, Bonnefon

One partition X, differentiating positive, negative null arguments:
X + = {x, {x} , }
X = {x, , {x}}
X 0 = {x, {x}}

Now, arguments decision-maker indifferent obviously affect
preference. meaning next axiom, allows forget
X 0 without loss generality:
Status Quo Consistency (SQC)
{x} A, B : ) B {x} ) B ) B {x} .
Let us discuss property monotony. Monotony sense inclusion
(A B = B ) A) obviously obeyed bipolar framework. Indeed,
B set negative arguments, generally holds , B. rather need axioms monotony specific positive negative argumentsbasically, one bipolar
capacities (Greco et al., 2002), expressed comparative way.
Positive
Negative

Monotony
Monotony

C, C $ X + , A, B :
C, C $ X , A, B :

A)B
A)B




C
C \A

)
)

B \ C $.
B C $.

see bivariate monotony property captured pair axioms.
Now, another assumption positive side negative side
B taken account comparing them: least good B
positive negative sides, least good B. expressed
axiom weak unanimity.
Weak Unanimity A, B, A+ ) B + ) B ) B.
set-relations presented previous Section obviously satisfy weak unanimity.
Finally, add classical axiom non triviality:
Non-Triviality: X + , X .
leads following generalization comparative capacities:
Definition 5. relation power set 2X monotonic bipolar set-relation
reflexive, quasi-transitive satisfies properties CA, SQC, Non-Triviality, Weak
unanimity, Positive Negative Monotony.
expected, monotonic bipolar set-relation safisfies bivariate monotony property:
using conventions positive negative arguments subsets B,
B + A+ (resp. B ), using Clarity Arguments Positive (resp. Negative)
Monotony, follows A+ ) B + (resp. ) B ), hence ) B due weak unanimity.
Proposition 2. )BiPoss monotonic bipolar set-relation.
394

fiQualitative Bipolarity Decision

present work, interested set-relations entirely determined
strength polarity individual arguments X. denote X = X {0}
set individual arguments X, adding element 0 keep track polarity
arguments. basic information arguments captured restriction
) X. Formally defined by:
x )X {x} ){ y}
x )X 0 {x} )
0 )X x ) {x}

on, call )X ground relation ).
agreement existence totally ordered scale weighting arguments,
ground relation )X supposed weak order. consequence, minimal condition
coherence )X preference cannot reversed argument preferred set (resp., least preferred set) replaced even better one (resp., worse
one). viewed condition monotony respect )X :
Monotony w.r.t. )X X-monotony
A, B, x, x$ {x, x$ } = x$ )X x:
{x} , B
{x} B
B , {x$ }
B {x$ }






{x$ } , B
{x$ } ) B
B , {x}
B ) {x}

natural axiom richer seems. example, implies property
substitutability equally strong arguments polaritya kind property
often called anonymity social choice decision theory. kindred property
anonymity also required, positive arguments block negative arguments
strength. blocking effect depend arguments themselves,
position scale. Hence axioms positive negative cancellation:
Positive Cancellation(POSC)
x, z X + , X , {x, y} {z, y} x X z.
Negative Cancellation (NEGC)
x, z X , X + , {x, y} {z, y} x X z.
makes sense summarize requirements single axiom, call
Simple Grounding:
Simple Grounding
bipolar set-relation ) said simply grounded )X weak order, )
monotonic respect )X satisfies positive negative cancellation.
395

fiDubois, Fargier, Bonnefon

Proposition 3. set-relation )BiPoss simply grounded.4
4.2 Axiomatizing Qualitative Bipolar Set-Relations
definition monotonic bipolar set-relation (Definition 5) general encompasses numerous models, qualitative (e.g., two rules section 3)
(e.g., cumulative prospect theory full generality). interested preference
rules derive principles ordinal reasoning only, focus axioms
account ordinality.
ordinal comparison sets extensively used, especially Artificial Intelligence. basic principle qualitative reasoning Negligibility, assumes
level importance interpreted order magnitude, much higher next
lower level.
Negligibility (NEG)
A, B, C X + : , B , C , B C.
Axiom NEG already featured around AI, directly form
demanding versions. Let us mention union property nonmonotonic reasoning,
Halperns (1997) Qualitativeness axioms (see Dubois & Fargier, 2004, discussion).
Lehmann (1996) introduced axiom negligibility inside Savage decision theory axiomatics.
Qualitative reasoning generally also comes along notion closeness preservation
(which away notion counting):
Closeness Preservation (CLO)
A, B, C X + : B C B C
B, C X + :
B )C B BC

axioms proposed justified ordinal reasoning one scale needs
considered (namely, positive one). sufficient negative
scale also needs taken account. need example express
bad consequence B, bad , B C , B, whatever negative
arguments C, B still worse C:
A, B, C : , B C , B C , B.

property meaningful negative sets arguments, trivial X + ; hence
introduced soundly framework.
cases sets negative positive elements compared
also encompassed. example, good cope globally negative
B also win comparison C, B still better C:
A, B, C : B , , C B , C.

4. Technically, !Pareto also monotonic bipolar simply grounded set-relation. result quite
irrelevant, since rule never compares strengths positive negative arguments.
terms, never happens arguments opposite polarities cancel other, condition parts
POSC NEGC thus never fulfilled.

396

fiQualitative Bipolarity Decision

similarly, globally negative (A ) bad outperformed C
(C , A) cannot enhanced B ( , B), C , B, i.e.:
A, B, C : , B C , C , B.
properties expressed following axiom global negligibility:
Global Negligibility (GNEG)
A, B, C, : , B C , C , B
classic property purely positive qualitative scalesin case,
consequence NEG positive monotony. usually explicitly required
positive frameworks. framework positive negative scale
needed, NEG condition longer sufficient getting GNEG. So, order keep
property foundation pure order-of-magnitude reasoning, bipolar qualitative
frameworks must explicitly require GNEG.
similar argument applies axiom CLO. must expicitly require property
general usual unipolar qualitative scales:
Global Closeness Preservation (GCLO)
A, B, C, : ) B C ) C ) B
Proposition 4. set-relation )BiPoss satisfies GNEG GCLO.
Propositions 2, 3 4 show bipolar possibility relation simply grounded
monotonic bipolar set-relation satisfying GNEG GCLO. Applying principles
qualitative bipolar reasoning described previous axioms also lead many different less intuitive qualititative rules, instance Pareto rule (see Dubois
Fargier (2006) full characterization rule). looking simple
complete decision rule, axioms provide full characterization Biposs preference
relation. Note strict part rule governed axiom GNEG. indifference
part includes pure case indifference also cases one would expect incomparability
decisions rather indifference proper. debatable part
Biposs rule, refined sequel.
Theorem 1. following propositions equivalent:
1. ) simply grounded complete monotonic bipolar set-relation 2X satisfies
GNEG GCLO.
2. exists mapping : X ' [0L , 1L ] ) )BiPoss .
detailed proof Theorem 1 provided Appendix A. short, show that,
) complete simply grounded, ranking built ranks arguments
respect strength. Within X + within X , simply obeys information
captured )X :
x, X + X 0 , x x )X y;
397

fiDubois, Fargier, Bonnefon

x, X X 0 , x x 5X y.

relative strength elements different signs deduced blocking effects. Indeed,
{x, y} preferred empty set, positive argument must stronger
negative one (and symmetrically). two arguments equivalent strength
none win: {x, y} . Formally:
x X + , X , x {x, y} ) ;
x X + , X , x ) {x, y}.

condition simple grounding ensures weak order. thus
encoded mapping : X ' [0L , 1L ] (x) = 0L x X 0 . Note
construction valid simple grounded complete bipolar relation,
qualitative ones. sequel proof uses axioms closeness negligibility
show ))BiPoss .
4.3 Principles Efficiency Preferential Independence
summary, previous Section shown )BiPoss natural model preferences
based bipolar orders magnitude. particular, rule accordance GNEG
follow strict preference prescribed ,BiPoss .
Nonetheless, seen Section 3.2 )BiPoss suffers drowning effect,
usual standard possibility theory. instance, B included even
elements positive, necessarily strictly preferred B. problem
rooted fact CLO concludes indifference even cases would
like appeal so-called principle efficiency make decision. like
monotony principle, axiom well known positive sets. proper extension
bipolar framework obviously one positive one negative side:
Positive efficiency B \ B , , B
Negative efficiency B \ B B
set-relations )BiPoss )Pareto also fail obey classical condition preferential independence, also called principle additivity. condition simply states
arguments present B influence decision:
Preferential Independence: A, B, C, (A B) C = : ) B C ) B C
axiom well known uncertain reasoning, one fundamental axioms
comparative probabilities (see Fishburn, 1986, survey). Note implies
conditions efficiency (provided completeness holds).
Except special cases arguments different levels importance
(when )X linear order), new axioms incompatible axioms ordinality
completeness transitivity enforced. already true purely positive
case, i.e. X empty (Fargier & Sabbadin, 2005). impossibility result
insuperable, shown next Section.
398

fiQualitative Bipolarity Decision

5. Refining Basic Order-of-Magnitude Comparison
order overcome lack decisiveness )BiPoss propose comparison principles
refine it, is, decisive set-relations ) still compatible )BiPoss ,
,BiPoss B , B. following, shall consider refining )Pareto
important drawbacks.
5.1 Implicative Bipolar Decision Rule
implicative decision rule (Dubois & Fargier, 2005) follows basic focalization principle
)BiPoss : comparing B, focuses arguments maximal strength OM(A
B) = maxxAB (x) B. adds principle following simple existential
principle: least good B iff, level existence arguments favor
B counterbalanced existence arguments favor A, existence
arguments counterbalanced existence arguments B. Formally,
implicative bipolar rule described follows:
Definition 6. Let = maxxAB (x);
)Impl B

OM(B + ) =
OM(A ) =

= OM(A+ ) =
= OM(B ) =

Let us go back examples Luc, Lucy, Luka. Luc case (Example 1),
)BiPoss concluded indifference, )Impl rather select Option [a],
important arguments decisions, whilst Option [a] supported
important pro (there important pro supporting option [b]). Lucy case
(Example 2), )Impl follow strict preference )BiPoss send Lucy Riviera.
Finally, Luka case (Example 3), highest level argument importance
features one con pro, sides, )Impl opine )BiPoss conclude
indifference.
Let us lay bare cases )Impl B. again, let us denote a+ = OM(A+ ),
= OM(A ), b+ = OM(B + ), b = OM(B ). definition, )Impl B four
following situations:
1. a+ = b+ = = b ;
2. a+ = b+ max(a , b ) > min(a , b );
3. = b max(a+ , b+ ) > min(a+ , b+ );
4. max(a+ , b ) > max(a , b+ ).
thus get following decomposition )Impl rule:
Proposition 5.
Impl B either a+ = b+ = = b , a+ = b+ > max(a , b ), yet
= b > max(a+ , b+ ).
!Impl B either a+ = > max(b , b+ ), b+ = b > max(a , a+ ).
399

fiDubois, Fargier, Bonnefon

,Impl B either max(a+ , b ) > max(a , b+ ), a+ = = b > b+ , yet
b+ = b = a+ > .
easy check )Impl bipolar monotonic set-relation. Like )BiPoss ,
positive arguments only, set-relation )Impl collapses max rule.
also obeys principle weak unanimity. Moreover:
Proposition 6. set-relation )Impl transitive.
Since max(a+ , b ) > max(a , b+ ), i.e., ,Biposs B, one conditions ,Impl
B, obviously follows that:
Proposition 7. Relation )Impl refinement )BiPoss .
Finally, situation incomparability !Impl B arises two cases only,
a+ = > max(b , b+ ), symmetric case b+ = b > max(a , a+ ). terms,
incomparability occurs one two sets displays internal contradiction
highest level, arguments set weak matter. particular,
a+ = > 0L ! . instance, dangerous travel exceptional,
mysterious part far tropical forest displays internal conflict, know
whether prefer stay home not. conflict appears also Lucs first option
(which attractive high priced). hand, considering non conflicting
(and non-empty set) A, either OM(A+ ) > OM(A ) ,Impl (well, travel
reasonably dangerous, prefer go), OM(A ) > OM(A+ ) ,Impl
(there war near border prefer stay home); two latter cases,
Pareto rule would concluded incomparability. range incompleteness
)Impl thus different one )Pareto , account notion
internal conflict.
)Impl rule interesting theoretic descriptive point view,
way handles conflict, fact refines )BiPoss . However,
)Impl decisive enough fully overcome drowning effect: salient
arguments taken account. instance, expensive hotel without swimming
pool undistinguishable expensive hotel include swimming pool.
Even decisive )BiPoss )Impl rule satisfy Preferential
Independence: like previous rules, collapses Walds criterion (resp., max
rule) negative (resp., positive) sub-scale X (resp., X + ), thus suffer
drowning effect. solve problem, leave family set-relations
focus set refinements satisfy Preferential Independence, thus efficient
positively negatively.
5.2 Efficient Refinements )BiPoss

following so-called Discri rule adds principle preferential independence
ones proposed )BiPoss , cancelling elements appear sets applying
)BiPoss rule:
Definition 7. )Discri B \ B )BiPoss B \ A.
400

fiQualitative Bipolarity Decision

Note simplification options B (by cancellation common aspects) inconsistent focalization assumption (i.e., importance group
arguments important argument group). Rather, focalization
applies options simplified delete arguments that,
common options, contribute making difference them.
Luka case (Example 3) typical example way )Discri outperforms )BiPoss .
case, A+ = {squash + }, = {location , price }, B + empty, B =
{price }. Recall )BiPoss concludes indifference two options
common strong cons. However, cancelling away price argument
present options, )Discri choose B, longer pros cons,
described moderately strong con weak pro. Luc
Lucy cases (Examples 1 2), two options feature common argument,
)Discri therefore share preferences )BiPoss (indifference Luc case, going
Riviera Lucy case).
)Discri complete quasi-transitive (its strict part, ,Discri transitive symmetric part necessarily transitive). Unsurprisingly, X = X + , )Discri
collapse max rule, rather discrimax procedure (Brewka, 1989; Dubois
& Fargier, 2004), is, comparison OM(A \ B) OM(B \ A).
already noted, )Discri simply cancels argument appearing
B. One could accept cancellation positive (resp. negative) argument
another positive (resp. negative) argument B, long two arguments
share order magnitude. yields following )BiLexi rule (and, later on,
)Lexi rule), based levelwise comparison cardinality. arguments
B scanned top down, level reached numbers positive
negative arguments pertaining two alternatives different; then, set
least number negative arguments greatest number positive ones preferred.
Note perfectly legitimate qualitative decision rule count arguments
strength. simply means one argument one side cancels one argument
strength side. people seem do. must
remain qualitative scale expressing importance arguments
decision based; would unreasonable consider number arguments
given importance level not, indeed, number, is, natural integer.
Let us first define -section set arguments:
Definition 8. level L:
= {x A, (x) = } -section A.

+

A+
= X (resp. = X ) positive (resp. negative) -section.

Now, lexicographic two-sided partial ordering (called Levelwise Bivariate Tallying
Bonnefon et al.(in press)) introduced:

Definition 9.
+


)BiLexi B |A+
| |B | |A | |B |,
+


= Argmax{ : |A+
| #= |B | |A | #= |B |}.

easy show )BiLexi reflexive, transitive, complete. Indeed,
decisive level ( ) one set wins positive side set wins
negative side, conflict revealed procedure concludes incomparability.
401

fiDubois, Fargier, Bonnefon

)BiLexi rule well-behaved respect Lucy Luka examples. Lucy
go Riviera, Luka chose best located gym. Luka case, {price }
preferred {price , location , squash + }: price argument featured
options cancelled away, {location , squash + } moderately strong con judged
worse . Luc story, difficulty dilemma clearly pointed
rule. Remember option [a] involves three arguments highest level importance,
landscape ++ , airline , price , whilst option [b] involves one, governance .
Since |{governance }| < |{airline , price }| |{landscape ++ }| > ||, )BiLexi concludes incomparability, reflecting difficulty decision.5 generally,
)BiLexi rule concludes incomparability conflict positive side negative side decisive level. descriptive point view,
range incomparability necessarily shortcoming )BiLexi .
Now, one assume form compensation positive negative arguments
level within given option, following refinement )BiLexi obtained,
take care complex cases Lucs:
Definition 10.


)Lexi

B L \ 0L

!


+

> , |A+
| | | = |B | |B |
+

+

|A | | | > |B | |B |.

Let us look one time Lucs dilemma. One strong pros Option [a]
cancelled one strong cons, discarded. remains one strong con
option: option wins level. Therefore, procedure examines
next lower importance level. level, three weak pros con option
[b], neither pro con option [a]: Option [b] winsand first time
article decision rule yields strict preference Luc case. Lucy
Luka examples, )Lexi breaks ties )BiPoss )BiLexi )Discri did: Lucy go
riviera, Luka chose best located gym.
three rules proposed Section obviously define monotonic bipolar set-relations.
refines )BiPoss satisfies Preferential Independence. ranked
least decisive (,Lexi ), moreover complete transitive.
Proposition 8. ,BiPoss B ,Discri B ,BiLexi B ,Lexi B

decision rules ,BiLexi ,Lexi satisfy strong form unanimity, namely,
A+ ) B + ) B moreover least one , B A+ , B + holds,
, B follows. However kind strong unanimity alone strong enough
achieve good sound discrimination among options (for instance, key-feature
)Pareto ). fact, ,BiLexi ,Lexi satisfy much stronger efficiency properties, since
BiLexi B B number positive negative arguments
level importance, Lexi B B number
additional arguments polarity level importance cancellation
equally important opposite arguments inside option.
5. take opportunity insist fact incomparability confused indifference, signals complex situation. situations incomparability, decision-maker
perplex alternative look better other, choosing random would
satisfactory way out, choice leads reason regret. case indifference,
alternatives equally attractive (or repulsive), random choice makes sense.

402

fiQualitative Bipolarity Decision

Notice restriction )BiLexi )Lexi X + amounts leximax
preference relation (Deschamps & Gevers, 1978). thus use classical encoding
leximax (unipolar) procedure sum finite case (Moulin, 1988). capacity
set-function monotonic inclusion. easy show that:
Proposition 9. exist two capacities + 2X

+



2X that:

)Lexi B + (A+ )! (A ) + (B + ) (B )
+ (A+ ) (B + )
)BiLexi B
+ (B ) (A )
example, denoting 1 = 0L < 2 < < l = 1L l elements L, use
(integer-valued) capacity:
"
+ (A+ ) =
|A + | |X|i .
L

#
set-function said big-stepped capacity |X|i > j<i |X|j (see Dubois
& Fargier, 2004). similarly use similar big-stepped capacity represent
importance sets negative arguments:
"

(A ) =
|A
| |X| .
L

proposition means )Lexi )BiLexi rankings particular cases (using
big-stepped probabilities) Cumulative Prospect Theory decision rule (Tversky &
Kahneman, 1992). See Section 6 discussion.
summary, )Lexi complies spirit qualitative bipolar reasoning
efficient. meantime, advantages numerical measures (transitivity
representability pair functions). Finally, order make full case attractive
)Lexi , show )Lexi rule time (i) refines )BiPoss ,
(ii) weak order, (iii) satisfies principle preferential independence without
introducing bias order X.
Definition 11. refinement )$ monotonic bipolar set relation ) unbiased
preserves ground relation latter: )$X )X .
order prove claim, let us first establish following proposition applies
transitive relation satisfying preferential independence (note completeness
required). proposition establishes principle anonymity, according two
equivalent subsets interchangeable provided overlap sets
arguments:
Proposition 10. Let ) transitive monotonic bipolar set-relation satisfies preferential independence. A, B, C, (C D) = C D:
C ) B ) B;
B ) C B ) D.
403

fiDubois, Fargier, Bonnefon

direct consequence property set arguments cancel
added another set without changing preferences involving latter (just let
):
Corollary 1. Let ) transitive monotonic bipolar set-relation satisfies preferential
independence. A, B, C C = , C ,
) B C ) B;
B ) B ) C.
Another noteworthy consequence Proposition 10 extended principle preferential independence:
Corollary 2. Let ) transitive monotonic bipolar set-relation satisfies preferential
independence. A, B, C, (A B) (C D) = C D:
) B C ) B
Finally, Proposition 10 allows show that6 :
Corollary 3. Let ) transitive monotonic bipolar set-relation satisfies preferential
independence. A, B X, x, X x
/ A,
/ B, {x} { y}:
) B {x} ) B {y}
three results instrumental proving third representation result
paper:
Theorem 2. Let ) monotonic bipolar set-relation order-of-magnitude
distribution elements X. following propositions equivalent:
1. ) complete, transitive, satisfies preferential independence unbiased refinement )BiPoss ;
2. ) )Lexi .
theorem concludes argumentation favor )Lexi , shown
rule satisfy natural principles order-of-magnitude reasoning decisive practical (e.g., completeness, transitivity, representability pair functions).

6. Related Works
section, relate decision rules general approach several traditions
research, Cognitive Psychology well Artificial Intelligence.
interesting connection made decision rules Take
Best heuristic extensively studied psychologists since introduction
6. Note that, Corollary 3, neither condition {x} B = condition {y} = required;
main difference Corollary 2; hand, Corollary 3 restricted addition
singletons.

404

fiQualitative Bipolarity Decision

Gigerenzer Goldstein (1996). Take Best so-called fast frugal heuristic
comparing two objects based values series binary cues. fast
frugal focuses limited subset available information order make
decision; heuristic value shows reasonable accuracy compared less
frugal comparison rules (see Gigerenzer et al., 1999; Katsikopoulos & Martignon, 2006,
simulations empirical tests). consequence, evolutionary psychologists argued
Take Best adapted strategy kind binary cue-based comparison
investigating.
Take Best requires arguments consideration different orders
magnitude, ranked lexicographically: exist x,
(x) = (y). Furthermore, argument considered generate polar opposite:
pro x+ attached option [a], con x (x ) = (x+ )
automatically attached options feature x+ (and reciprocally).
applying Take Best amounts scanning arguments top-down, starting
important, stop soon argument found favors one option
other. Interestingly, applied linearly ranked arguments, )Discri ,
)BiLexi , )Lexi rules coincide Take Best heuristic. new decision
rules proposed able account choice situations Take Best
heuristice.g., several criteria share degree importance. sense,
natural extensions Take Best qualitative rule advocated psychologists.
Originally descriptive model risky decisions, Cumulative Prospect Theory (Tversky
& Kahneman, 1992) provides another psychological account decisions involving positive
negative arguments. Contrary Take Best approach, oriented towards
quantitative evaluation decisions. key notions Cumulative Prospect Theory
individuals assess outcomes relatively reference point, rather absolutely; concerned losses gains, ceteris paribus;
individuals overweight extreme outcomes occur small probability.
technically, Cumulative Prospect Theory assumes reasons supporting
decision reasons measured means two capacities +
; + reflects importance group positive arguments, importance
group negative arguments. higher + , convincing set positive
arguments; conversely, higher , deterring set negative arguments. Furthermore, Cumulative Prospect Theory assumes possible map
evaluations so-called net predisposition score, expressed single scale:
X, NP(A) = + (A+ ) (A ),
A+ = X + , = X . Alternatives ranked according net
predisposition: )NP B + (A+ ) (A ) + (B + ) (B ). Proposition 9
(Section 5) actually shows )Lexi particular case rule (using big-stepped
capacities). Interestingly, )BiPoss rule foreign comparison net predispositions. precisely, viewed ordinal counterpart net predisposition
comparison. Let us first write )NP B + (A+ ) + (B ) + (B + ) + (A ). Now,
immediate changing + max, changing + inequality
possibility measures, yields )BiPoss rule.
405

fiDubois, Fargier, Bonnefon

Cumulative Prospect Theory variants assume kind independence X + X , assumption always hold. Bi-capacities (Grabisch &
Labreuche, 2002, 2005) introduced handle non-separable bipolar preferences:
measure defined Q(X) := {(U, V ) 2X , U V = }, increases (resp., decreases) addition elements U (resp., V )7 . Bi-capacities originally stemmed
bi-cooperative games (Bilbao, Fernandez, Jimenez Losada, & Lebron, 2000),
players divided two groups, pros cons: player x sometimes
pro, sometimes con, cannot simultaneously. context bipolar
decision, typically set U = A+ V = measure attractivity
(A+ , ). net predisposition Cumulative Prospect Theory recovered letting
(A+ , ) = + (A+ ) (A ) = NP(A).

Since comparison net predispositions (and, generally, bi-capacities bicooperative games) systematically provides complete transitive preference, fail
capture large range decision-making attitudes: contrasting affects make decision
difficult, comparison objects bipolar evaluations systematically
yield complete relation? might imply incompatibilities. bi-capacities
generalized means bipolar capacities (Greco et al., 2002). idea underlying
bipolar capacities use two measures, measure positiveness (that increases
addition positive arguments deletion negative arguments) measure
negativeness (that increases addition negative arguments deletion
positive arguments), combine them. terms, bipolar capacity
equivalently defined pair bi-capacites + , namely by: (A) =
( + (A+ , ), (A , A+ )). preferred B respect
preferred B respect + is, according sole Pareto
principle. allows representation conflicting evaluations leads partial
order.
approach provides clear qualitative counterparts Cumulative Prospect Theory,
bi-capacities, bipolar capacities certain extent. Indeed, )Lexi belongs
Cumulative Prospect Theory family, thus represented bi-capacity,
generally bipolar capacity. contrast, )Pareto obviously belongs family
bipolar capacities. rules )BiPoss , )Discri, )BiLexi )Impl spirit
bi-capacities bi-cooperative games. First all, contrary models,
decision rules provide complete preorder alternatives argue
contrary conflicts positive negative arguments strength
lead conflict, thus incomparable alternatives. decision rules, like )BiPoss
cannot understood comparison (A+ , ) (B + , B ); rule
indeed compares max(OM(A+ ), OM(B )) max(OM(B + ), OM(A )). )BiPoss ,
right scheme would rather comparison (A+ , B ) (B + , ).
7. bibliography bi-capacities bi-cooperative games see work Grabisch colleagues
(Grabisch & Labreuche, 2005; Grabisch & Lange, 2007). developments notions
concern computation Shapley value, definition Choquet integrals. apart
Cumulative Prospect Theory, instances general framework presented. point
probably that, provides complete transitive comparison, cannot highlight presence
conflicting information.

406

fiQualitative Bipolarity Decision

suggests neither framework bi-cooperative games, ones bi-capacities
bipolar capacities yet general enough.
Finally, notice couched results terminology borrowing argumentation decision theories, indeed consider relevant both.
paper also relevant argumentative reasoning evaluation sets arguments
inference processes (Cayrol & Lagasquie-Schiex, 2005). connection reasoning
decision setting argumentation laid bare Amgoud, Bonnefon, Prade
(2005). propose extensive framework arguments constructed
knowledge base base logically defined criteria. Arguments evaluated terms
certainty, strength degree attainment corresponding criterion. Assuming
common scale three aspects, separately compare individual positive arguments, negative ones, using simple aggregation weights, never compare
arguments different polarity. idea using logical arguments compare decisions
first discussed setting possibilistic logic Amgoud Prade (2004). Amgoud
Prade (2006) elaborated framework, explicit use knowledge base
goal base construction arguments builds bridge argumentative
reasoning qualitative decision uncertainty.

7. Conclusion
paper focused particular class bipolar decision making situations, namely
qualitative rather quantitative. proposed extension
possibility theory handling sets containing arguments considered positive
negative. laid bare importance )Lexi )BiLexi decision rules
qualitative bipolar decision-making. rules separately evaluate positive negative sets arguments, means big-stepped capacities + . Then, )Lexi
rule aggregates two measures agreement Cumulative Prospect Theorys net
predisposition. contrast, )BiLexi rule merge positive negative
measures, allowing expression conflict incomparability. sense, two
rules combine best two worlds: agree spirit order magnitude
reasoning, decisive efficient basic rules )BiPoss ,
offer practical advantages quantitative Cumulative Prospect Theorye.g.,
transitivity representability pair functions.
paper adopted prescriptive point view sense rules
studied respect properties qualitative theory bipolar decision-making
obey. representation theorems Sections 4 5 show use )BiP oss
)Lexi well-behaved ones dealing qualitative bipolar information.
parallel, tested descriptive power rules, i.e., accuracy predicting
behavior human decision makers. experimental results (Bonnefon et al., press;
Bonnefon & Fargier, 2006) confirm ,BiPoss basic ordinal decision-making rule:
,BiPoss yields strict preference, uncommon human decision-makers disagree
preference. Furthermore, results strongly suggest )Lexi decision rule
generally followed decision-makers: )Lexi accurately predicted nearly 80% 2,000
decisions collected, always one decision-makers individually agreed
407

fiDubois, Fargier, Bonnefon

most. Finally, results suggest human decision-makers sometimes find decisions
incomparable; do, usually situations )BiLexi would predict so.
present paper, address question computational complexity
comparing two objects bipolar decision rule. Actually, presence bipolar
information change range complexity comparison objects. detailed
complexity study scope paper, least say
rules presented paper, comparison two alternatives polynomial
number criteria X. comparison indeed relies computation strength
four subsets X computation sometimes preceded simple deletion step
(for rules based discri- lexi- comparison). computation strength
set linear, since performed means aggregation operator. complexity
comparing two alternatives )BiPoss )Lexi rules instance O(Card(X))
(it computed simple aggregation individual strengths). Finally, notice
rules provide transitive strict preference, cycle appear may
case CP-nets. combinatorial alternatives considered, could easily
use branch bound algorithm looking best alternative(s); problem
optimization bipolar aggregation harder unipolar one:
corresponding decision problem remains NP-complete (Fargier & Wilson, 2007).
concludes study qualitative bipolar reasoning. However, aware
unresolved issues paper raised. example, Section 6 suggests
framework bi-capacities, even bipolar capacities, rich enough accommodate
full range qualitative bipolar rules introduced. Secondly, results
established within restricted framework, relevant criterion either complete
pro complete opponent w.r.t. decision, spirit bi-cooperative games.
clearly simpler approach usual multi-criteria decision-making frameworks,
x X full-fledged criterion rated bipolar utility scale like Lx = [1x , +1x ],
containing neutral value 0x . Thus, natural extension present work would
address qualitative bipolar criteria whose satisfaction matter degree.

Appendix A. Proofs
simplify notations, let a+ = OM(A+ ), = OM(A ), b+ = OM(B + ), b = OM(B ),
c+ = OM(C + ), c = OM(C ). Hence OM(A) = max(a+ , ), OM(A+ B ) = max(a+ , b )
on.
Proposition 1. proof completeness trivial, OM(A+ B ) OM(B + )
always compared. prove transitivity ,BiPoss , let us assume max(a+ , b ) >
max(a , b+ ) max(b+ , c ) > max(b , a+ ). Then, letting b = max(b+ , b ), get
max(a+ , b, c ) > max(a , b, c+ ). consequence, max(a+ , c ) > b. Hence, max(a+ , c ) >
max(a , b, c+ ) max(a , c+ ).
Proposition 2. Quasi-transitivity proved Proposition 1. Positive negative monotony,
well SQC, follow monotony OM, possibility measure: i.e.,
U, V, OM(U V ) OM(V ). Non-triviality )BiPoss obtained non-triviality
(there exists x (x) = 0), implies OM((X + )+ (X ) ) = OM(X +
408

fiQualitative Bipolarity Decision

X ) > 0 OM((X + ) (X )+ ) = OM() = 0. Clarity argument also trivial: x X + OM({x}+ () ) = (x) OM({x} ()+ ) = OM() = 0:
{x} )BiPoss . x con, get way {x} 5BiPoss . x null importance, get OM({x}+ () ) = OM({x} ()+ ) = OM() = 0: {x} BiPoss .
prove weak unanimity, suppose A+ )BiPoss B + )BiPoss B . Obviously
A+ )BiPoss B + OM(A+ ) OM(B + ) )BiPoss B OM(B ) OM(A ).
Hence OM(A+ B ) OM(B + ) : )BiPoss B.
Proposition 3. restricted singletons, )BiPoss weak order ranks positive
arguments decreasing values , null arguments ( = 0), negative
arguments increasing value . ranking defines complete transitive relation.
proves relation )X induced )BiPoss weak order. Axioms POSC easy
check, since {x+ , } (resp., {z + , } BiPoss ) (x+ ) = (y ) (resp.
(z + ) = (y )). {x+ , } BiPoss {z + , } imply (x+ ) = (z + ). So,
{x+ } BiPoss {z + }, i.e., x+ X z + . proof NEGC similar. X-monotony
)BiPoss shown follows. Let A, x, x$ {x, x$ } = x$ )X x. Three
cases possible:
1. x X + . x$ X + (x$ ) (x).
{x} ,BiPoss B: OM(A+ {x} B ) > OM(A B + ). Since (x$ )
(x), get OM(A+ {x$ } B )OM(A B + ): {x$ } ,BiPoss B.
{x} BiPoss B: OM(A+ {x} B ) = OM(A B + ). Replacing x
x$ , i.e., (x) (x$ ), first OM level increases, get OM(A+ {x$ }
B ) OM(A B + ): {x$ } )BiPoss B.
B , {x$ }, OM(B + ) > OM(A+ {x$ } B ). Replacing x$
x, i.e., (x$ ) (x), second OM level decreases, get OM(B + ) >
OM(A+ {x} B ), i.e. B ,BiPoss {x}.
B {x$ }, OM(B + ) = OM(A+ {x$ } B ). Replacing x$
x, i.e. (x$ ) (x), second OM level decreases, OM(B + )
OM(A+ {x} B ), i.e. B )BiPoss {x}.
2. x$ X . x X (x) (x$ ). kind four-case proof
carried out.
3. x$ X + x X .
{x} ,BiPoss B, i.e., OM(A+ B ) > OM(A B + {x}), follows
OM(A+ B {x$ }) > OM(A B + ), {x$ } ,BiPoss B.
Similarly, A{x} BiPoss B, i.e., OM(A+ B ) = OM(A B + {x}), follows
OM(A+ B {x$ }) OM(A B + {x}), OM(A+ B {x$ })
OM(A B + {x}), implies OM(A+ B {x$ }) OM(A B + ). Hence
(x$ positive) {x$ } )BiPoss B.
B , {x$ }, means OM(B + ) > OM(A+ {x$ } B ) since x$
positive. Obviously, OM(B + {x}) OM(B + ) OM(A+
{x$ } B ) OM(A+ B ). Hence OM(B + {x}) > OM(A+ B ):
B ,BiPoss {x}.
409

fiDubois, Fargier, Bonnefon

Similarly, B BiPoss {x$ } have: OM(B + {x}) OM(B + ) =
OM(A+ {x$ } B ) OM(A+ B ): B )BiPoss {x}.
Proposition 4.
)BiPoss satisfies GCLO Recall )BiPoss B max(a+ , b ) max(b+ , )
C )BiPoss max(c+ , ) max(d+ , c ). Hence, max(a+ , b , c+ , )
max(b+ , , d+ , c ), i.e., C )BiPoss B D.
)BiPoss satisfies GNEG Recall ,BiPoss B max(a+ , b ) > max(b+ , )
C ,BiPoss max(c+ , ) > max(d+ , c ). Hence: max(a+ , b , c+ , ) >
max(b+ , , d+ , c ), i.e., C ,BiPoss B D.
Theorem 1. Let us first build . CA, singleton {x} comparable . X + =
{x, {x} , }, X = {x, {x} } X 0 = {x, {x} } soundly defined.
Let us define relation X follows:
x, X 0 : x x.

x, X + : x {x} ) {y}
x, X : x {y} ){ x}
x X + , X : x not( , {x, y})
x X +, X 0 : x >
x X , X 0 : x >

axiom CA, X + , X X 0 disjoint. previous definition thus well
founded. complete definition. prove transitive. Suppose
x z let us perform following case analysis:
x, y, z X + : x z trivial )X transitive identified
within X + .
x, y, z X : x z trivial )X transitive identified
within X .
x X 0 : x means STQ also X 0 , turn implies z X 0 .
So, STQ again, x z.
X 0 : z implies STQ z also X 0 . So, STQ again, x z.
z X 0 : x z always true (by status quo consistency again).
x X + , X z X . definition, x means {x, y} )
z means {z} ){ y}. X-monotony replace z without reversing
preference: {x, z} ) , i.e. x z.
410

fiQualitative Bipolarity Decision

x X + , X + z X : proof x z similar one previous
item.
X + x X z X . Suppose {x, y} 5 (x y), {z, y} ) (y z)
{x} > {z} (z > x negative arguments). {z, y} , , X-monotony implies
{x, y} , (thus contradiction). , {x, y} X-monotony implies , {z, y}
(second contradiction). Last case, {x, y} {z, y} , POSC implies
{x} { z} (last contradiction). So, z > x hold thus, completeness
, x z.
X x X + z X + . proof x z similar one previous
item, using NEGC instead POSC.
So, weak order. encoded distribution : X ' [0L , 1L ], [0L , 1L ]
totally ordered scale. Level 0L mapped elements X0 . Now, show
equivalence ) relation )BiPoss induced , namely prove
) B OM(A+ B ) OM(A B + ). Since ) complete, amounts showing
OM(A+ B ) = OM(A B + ) implies B OM(A+ B )OM(A B + )
implies , B. Let us first prove OM(A+ B )OM(A B + ) implies , B. Suppose
element highest value A+ B x A+ . So, , {x, y} ,
w B + , {x} ,{ w}. GNEG get {x}A , B + positive negative monotony
imply A+ , B + B . element highest value A+ B
v B w B + , {w, v} , {y} , {v}. GNEG
get {v} B + positive negative monotony imply A+ , B + B .
Let us prove OM(A+ B ) OM(A B + ) implies ) B. Suppose
element highest value A+ B x A+ . So, , {x, y} )
w B + , {x} ) {w}. GCLO get {x} ) B + positive negative monotony
imply A+ ) B + B . element highest value A+ B
v B w B + , {w, v} 5 , {y} ){ v}. GCLO get
{v} B + 5 positive negative monotony implies A+ ) B + B .
Proposition 5. Consider four cases identified text. Clearly situation 1 corresponds
case Impl B. case 4, strict dominance ,Impl B prevails. Case 2 may
lead three different conclusions. Equivalence arises a+ = b+ max(a , b ):
cons low level w.r.t pros, taken account indifference
prevails based pros. a+ = b+ = b > , )Impl B holds
B )Impl A. So, ,Impl B since arguments weak. symmetry,
a+ = b+ = > b+ concludes B ,Impl A. Case 3 handled similar manner:
equivalence arises = b > max(a+ , b+ ); = b = a+ > b+ , ,Impl B
holds since arguments B weak; = b = b+ > a+ , B ,Impl Finally,
"Impl B concluded neither )Impl B B )Impl A. arises two cases
only, a+ = > max(b , b+ ) symmetric case b+ = b max(a , a+ ).
Proposition 6. Assume )Impl B B )Impl C. Using conventions, consider:
1. following situations ensuring )Impl B:
411

fiDubois, Fargier, Bonnefon

(a) a+ = b+ = = b ;
(b) a+ = b+ max(a , b );
(c) = b max(a+ , b+ );

(d) max(a+ , b ) > max(a , b+ ).
2. following situations ensuring B )Impl C following group:
(a) b+ = c+ = b = c ;
(b) b+ = c+ max(b , c );
(c) b = c max(b+ , c+ );

(d) max(b+ , c ) > max(b , c+ ).
Combining one condition first group one second group yields one
corresponding conditions ensuring )Impl C. instance,
Combining conditions 1b 2b yields a+ = c+ max(a , b , c ) max(a , c ).
Combining conditions 1b 2c yields a+ max(a , c ) c max(a+ , c+ ).
Hence a+ max(a , a+ , c+ ), c max(a , c , c+ ) a+ = c . Hence a+ = c
max(a , c+ ). inequality strict condition d. a+ = c = max(a , c+ ),
condition b c.
Combining condition 1d condition 2b yields max(a+ , b ) > max(a , c+ )
c+ max(b , c ). Hence a+ > max(a , c+ ).
cases handled similarly.
Proposition 7. ,BiPoss B max(a+ , b ) > max(a , b+ ) thanks
Proposition 5 implies ,Impl+ B.
Proposition 8.
Suppose ,BiPoss B: x A+ B x B + , (x )(x).
many arguments satisfy property, let x one maximizing . x thus
B +. So, either A+ \ B + B \ . Since A+ \ B + = (A \ B)+
B \ = (B \ A) , write x (A \ B)+ (B \ A) . hand
(B \ A)+ (A \ B) B + element B + higher degree
x . So, OM((B \ A)+ (A \ B) ) < (x ). So, ,Discri B. proves )Discri
refines )BiPoss .
Suppose ,Discri B: x A+ \ B + B \ x B + \ A+
\ B , (x ) > (x). many elements satisfy property, let x one
maximizing . So, level > (x ), A+ \ B + B \ empty; thus
+


levels, A+
= B = B , imply equality cardinalities.

hand, level = (x ), element B + \ A+ \ B .
So, |(B + \ A+ ) | = 0 |(A \ B ) | = 0. least one element
A+ \ B + B \ |(A+ \ B + ) | 1 |(B \ ) | 1 (or even both).
412

fiQualitative Bipolarity Decision

Suppose |(A+ \ B + ) | 1: |(B + \ A+ ) | = 0 adding common
+
elements, get |B+ | = |(A+ B + ) |. Since |(A+ \ B + ) | 1 get |A+
||B |.
|(A \ B ) | = 0 adding common element get |A | = |B |
thus |A | |B |. ,BiLexi B. get ,BiLexi B way
|(B \ ) | 1. Hence )BiLexi refines )Discri .
+
Finally, suppose ,BiLexi B, i.e., level higher , |A+
| = |B |



|A | = |B | level , difference favor A. necessarily,

+

level higher , |A+
||A | = |B ||B |. Let first suppose level
+

+

, difference made positive scale, i.e., |A+
| > |B | |A | |B |.
+

+

Summing inequalities get: |A | |A ||B | |B |. ,Lexi B.
difference rather made negative side, get ,Lexi B similar way.
So, ,Lexi B. Hence )Lexi refines )BiLexi .

Proposition 10 . C (C D) = , preferential independence implies
C D. Then, transitivity: C ) B implies ) B; ) B implies
C ) B; B ) C implies B ) D; B ) implies B ) C.
Corollary 1 . Since C C = , apply principle preferential
independence get C A. Then, transitivity, ) B implies C ) B.
Conversely, C ) B C implies, also transitivity, ) B. Similarly,
B ) AC A, transitivity implies B ) AC; B ) AC AC A,
transitivity implies B ) A.
Corollary 2 . axiom preferential independence, (A B) C = :
) B C ) B C. applying Proposition 10 B (C D) = ,
) B C ) B D.
Corollary 3.
Case x B: Proposition 10 replace x B get ) B
) (B \ {x}) {y}. Let us apply preferential independence add x
sides ): ) (B \ {x}) {y} {x} ) (B \ {x}) {y} {x}, i.e.,
) (B \ {x}) {y} {x} ) B {y}, thus ) B {x} ) B {y}.
Case x # B: preferential independence means ) B {x} ) B {x}
Proposition 10 used get ) B {x} ) B {y}.
Theorem 2 . easy show item 2 implies item 1. already seen )Lexi
complete, transitive, satisfies preferential independence, refines )BiPoss . also
easy show grounding relations X induced relations equivalent,
i.e., )Lexi unbiased refinement )BiPoss .
Let us prove item 1 implies item 2. Let ) complete transitive
monotonic bipolar set-relation satisfies preferential independence, unbiased
.
refinement )BiPoss . Since ) unbiased refinement )BiPoss , )X )BiPoss
X
413

fiDubois, Fargier, Bonnefon

equivalently defined . notions -section, positive -section negative section thus well defined.

+

1. Let us first suppose that, , |A+
| |A | = |B | | B |. show
, B . Two cases possible:



Case |A+
| |A | 0: possible partition n = |A | pairs
+

+

+
{x , x } n = |A | |A | singletons {x }. Similarly, possible partition B nB = |B | pairs {y + , } n singletons {y + }the n

+

A, since |A+
| | | = |B | |B |. So, = , use Proposition
+
3 add n singletons {x } n singletons {y + } side
equivalence (left side x+ , right side + ). Corollary 1 allows
us add pairs {x+ , x } left side, pairs {y + , } right side.
arrive B .



Case |A+
| |A | 0: proof similar. possible partition n =
+
+


|A
| pairs {x , x } n = |A | | | singletons {x }. Similarly,
B

+

possible partition B n = |B | pairs {y , } n singletons {y }.
So, = , use Proposition 3 Corollary 1 allow us add pairs
{x+ , x } remaining singletons {x } left side, pairs {y + , }
remaining singletons {y } right side. arrive B .

-sections B pairwise disjoint.
Thanks
Corollary 2
$
$
sum equivalences B get B , i.e., B.


2. Let us suppose > 0L (i) > , |A+
| |A | =
+

+

|B+ | | B |
$ (ii) |A | $|A | > |B | |B |. proof similar one
before, get > > B . Three cases possible:




Case |A+
| |A | 0: possible partition n = |A | pairs
+

+

+
{x , x } n = |A | | | singletons {x }. Similarly, possible
partition B nB = min(|B+ |, |B |) pairs {y + , } singletons
{y + } number m$ singletons {y }, max(m, m$ ) = 0.


|B+ | | B | < |A+
| | | = n, < n. Let x one n positive
BiPoss
+
singletons partition . Obviously, x <
< B
,

BiPoss

thus monotony x < ,
< B . x < , < B
(this refinement hypothesis). add n singletons x+
n singleton + left right side strict preference without modifying (this Proposition 3). monotony, add singletons
, any, remaining n 1 singletons x+ , any. Since pairs
{x+ , x } (resp., {y + , }) equivalent , added left
(resp., right) side without modifying inequality (we use Corollary 1). get
, B .

Case |B+ | | B | 0: proof similar previous one, negative
. partition B maximum number pairs, given number n
negative singletons. also partitioned maximum number pairs,
414

fiQualitative Bipolarity Decision

possibly < n negative singleton (the two conditions exclusive)
positive singletons show < , < B {y }, add
negative singletons x negative singletons respective sides,
add remaining negative remaining positive x+ . pairs {x+ , x }
(resp., {y + , }) equivalent , added left (resp., right)
side without modifying inequality. get , B .


Finally, |B+ | |B | > 0 |A+
| | | < 0 incompatible fact
+

+

|A | |A | > |B | |B |.

thus get , B . -sections B pairwise disjoint.
Thanks Corollary 2 we$can then$sum equivalences B , >
string preference get , B , i.e. , B.

3. shown ,lexi B = , B lexi B = B.
relations complete, means equivalent: )lexi B ) B.

References
Amgoud, L., Bonnefon, J. F., & Prade, H. (2005). argumentation-based approach
multiple criteria decision. Lecture Notes Computer Science, 3571, 269280.
Amgoud, L., & Prade, H. (2004). Using arguments making decisions: possibilistic
logic approach. M. Chickering & J. Halpern (Eds.), Proc. 20th Conference
Uncertainty Artificial Intelligence (UAI04) (pp. 1017). Menlo Park, CA: AUAI
Press.
Amgoud, L., & Prade, H. (2006). Explaining qualitative decision uncertainty
argumentation. Proc. 21st National Conf. Artificial Intelligence. Menlo
Park, Ca: AAAI Press.
Benferhat, S., Dubois, D., Kaci, S., & Prade, H. (2006). Bipolar possibility theory
preference modeling: Representation, fusion optimal solutions. International
Journal Information Fusion, 7, 135150.
Benferhat, S., Dubois, D., & Prade, H. (2001). Towards possibilistic logic handling
preferences. Applied Intelligence, 14 (3), 303317.
Besnard, P., & Hunter, A. (2008). Elements Argumentation. Cambridge, Mass.:
MIT Press.
Bilbao, J. M., Fernandez, J. R., Jimenez Losada, A., & Lebron, E. (2000). Bicooperative
games. J. M. Bilbao (Ed.), Cooperative games combinatorial structures (p.
23-26). Dordrecht: Kluwer Academic Publishers.
Bonnefon, J. F., Dubois, D., Fargier, H., & Leblois, S. (in press). Qualitative heuristics
balancing pros cons. Theory & Decision.
Bonnefon, J. F., & Fargier, H. (2006). Comparing sets positive negative arguments: Empirical assessment seven qualitative rules. G. Brewka, S. Coradeschi,
A. Perini, & P. Traverso (Eds.), Proceedings 17th European Conference
Artificial Intelligence (ECAI2006) (pp. 1620). Zurich: IOS Press.
415

fiDubois, Fargier, Bonnefon

Boutilier, C., Brafman, R. I., Domshlak, C., Hoos, H. H., & Poole, D. (2004). CP-nets:
tool representing reasoning conditional ceteris paribus preference statements. J. Artif. Intell. Res. (JAIR), 21, 135-191.
Brafman, R., & Tennenholtz, M. (2000). axiomatic treatment three qualitative
decision criteria. Journal ACM, 47 (3), 452482.
Brandstatter, E., Gigerenzer, G., & Hertwig, R. (2006). priority heuristic: Making
choices without trade-offs. Psychological Review, 113, 409432.
Brewka, G. (1989). Preferred subtheories: extended logical framework default
reasoning. Int. Joint Conf. Artificial Intelligence (p. 1043-1048). Menlo Park,
Ca: AAAI Press.
Cacioppo, J. T., & Berntson, G. G. (1994). Relationship attitudes evaluative
space: critical review, emphasis separability positive negative
substrates. Psychological Bulletin, 115, 401423.
Cayrol, C., & Lagasquie-Schiex, M.-C. (2005). Graduality argumentation. Journal
Artificial Intelligence Research, 23, 245297.
Deschamps, R., & Gevers, L. (1978). Leximin utilitarian rules: joint characterization.
Journal Economic Theory, 17, 143163.
Doyle, J., & Thomason, R. (1999). Background qualitative decision theory. AI
Magazine, 20 (2), 5568.
Dubois, D. (1986). Belief structures, possibility theory decomposable confidence measures finite sets. Computers Artificial Intelligence, 5 (5), 403416.
Dubois, D., & Fargier, H. (2003). Qualitative decision rules uncertainty.
G. Della Riccia, D. Dubois, R. Kruse, & H.-J. Lenz (Eds.), Planning Based Decision Theory (Vol. 472, pp. 326). Wien: Springer.
Dubois, D., & Fargier, H. (2004). axiomatic framework order magnitude confidence
relations. M. Chickering & J. Halpern (Eds.), Proceedings 20th Conference
Uncertainty Artificial Intelligence (UAI04) (pp. 138145). Menlo Park, CA:
AUAI Press.
Dubois, D., & Fargier, H. (2005). qualitative comparison sets positive
negative affects. Lecture Notes Computer Science, 3571, 305316.
Dubois, D., & Fargier, H. (2006). Qualitative decision making bipolar information.
P. Doherty, J. Mylopoulos, & C. Welty (Eds.), Proceedings 10th International
Conference Principles Knowledge Representation Reasoning (pp. 175186).
Menlo Park, CA: AAAI Press.
Dubois, D., Fargier, H., & Prade, H. (1996). Possibility theory constraint satisfaction
problems: Handling priority, preference uncertainty. Applied Intelligence, 6 (4),
287-309.
Fargier, H., & Sabbadin, R. (2005). Qualitative decision uncertainty: back expected utility. Artificial Intelligence, 164, 245280.
Fargier, H., & Wilson, N. (2007). Algebraic structures bipolar constraint-based reasoning. Symbolic quantitative approaches reasoning uncertainty, 9th european conference, ecsqaru 2007, proceedings (Vol. 4724, p. 623-634). Berlin: Springer.
Fishburn, P. (1986). axioms subjective probabilities. Statistical Science, 1 (3),
335345.
416

fiQualitative Bipolarity Decision

Gigerenzer, G., & Goldstein, D. (1996). Reasoning fast frugal way: Models
bounded rationality. Psychological Review, 103, 650669.
Gigerenzer, G., Todd, P. M., & ABC group. (1999). Simple heuristics make us
smart. New York: Oxford University Press.
Grabisch, M., & Labreuche, C. (2002). Bi-capacities decision making bipolar scales.
Eurofuse02 Workshop Information Systems (p. 185-190).
Grabisch, M., & Labreuche, C. (2005). Bi-capacities parts II. Fuzzy Sets
Systems, 151 (2), 211260.
Grabisch, M., & Lange, F. (2007). Games lattices, multichoice games Shapley
value: new approach. Mathematical Methods Operations Research, 65 (1), 153
167.
Greco, S., Matarazzo, B., & Slowinski, R. (2002). Bipolar Sugeno Choquet integrals.
B. De Baets, J. Fodor, & G. Pasi (Eds.), Proceedings 7th Meeting
EURO Working Group Fuzzy Sets (EUROFUSE 2002 (pp. 191196).
Halpern, J. Y. (1997). Defining relative likelihood partially-ordered structures. Journal
Artificial Intelligence Research, 7, 124.
Katsikopoulos, K. V., & Martignon, L. (2006). Nave heuristics paired comparisons:
results relative accuracy. Journal Mathematical Psychology, 50,
488494.
Lehmann, D. J. (1996). Generalized qualitative probability: Savage revisited. E. Horvitz
& F. Jensen (Eds.), Proceedings 12th Conference Uncertainty Artificial
Intelligence (UAI-96) (pp. 381388). San Francisco, CA: Morgan Kaufman.
Lewis, D. L. (1973). Counterfactuals comparative possibility. Journal Philosophical
Logic, 2, 418446.
Moulin, H. (1988). Axioms cooperative decision making. New-York: Wiley.
Osgood, C. E., Suci, G., & Tannenbaum, P. H. (1957). Measurement Meaning.
Chicago: University Illinois Press.
Slovic, P., Finucane, M., Peters, E., & MacGregor, D. G. (2002). Rational actors rational
fools? Implications affect heuristic behavioral economics. Journal
Socio-Economics, 31, 329342.
Tversky, A., & Kahneman, D. (1992). Advances prospect theory: Cumulative representation uncertainty. Journal Risk Uncertainty, 5, 297323.
Wald, A. (1971). Statistical Decision Functions. New York: Wiley. (Original work published
1950)

417

fiJournal Artificial Intelligence Research 32 (2008) 95-122

Submitted 10/07; published 05/08

Graphical Model Inference Optimal Control Stochastic
Multi-Agent Systems
Bart van den Broek
Wim Wiegerinck
Bert Kappen

B.vandenBroek@science.ru.nl
W.Wiegerinck@science.ru.nl
B.Kappen@science.ru.nl

SNN, Radboud University Nijmegen, Geert Grooteplein 21,
Nijmegen, Netherlands

Abstract
article consider issue optimal control collaborative multi-agent
systems stochastic dynamics. agents joint task
reach number target states. dynamics agents contains additive control
additive noise, autonomous part factorizes agents. Full observation
global state assumed. goal minimize accumulated joint cost, consists
integrated instantaneous costs joint end cost. joint end cost expresses joint
task agents. instantaneous costs quadratic control factorize
agents. optimal control given weighted linear combination single-agent
single-target controls. single-agent single-target controls expressed terms
diffusion processes. controls, closed form expressions, formulated
terms path integrals, calculated approximately Metropolis-Hastings
sampling. weights control interpreted marginals joint distribution
agent target assignments. structure latter represented graphical
model, marginals obtained graphical model inference. Exact inference
graphical model break large systems, approximate inference methods
needed. use naive mean field approximation belief propagation approximate
optimal control systems linear dynamics. compare approximate inference
methods exact solution, show accurately compute optimal
control. Finally, demonstrate control method multi-agent systems nonlinear
dynamics consisting 80 agents reach equal number target states.

1. Introduction
topic control multi-agent systems characterized many issues, originating
various sources, including wide variety possible execution plans, uncertainties
interaction environment, limited operation time supporting resources,
demand robustness joint performance agents. issues encountered
in, example, air traffic management (Tomlin, Pappas, & Sastry, 1998; van Leeuwen,
Hesseling, & Rohling, 2002), formation flight (Ribichini & Frazzoli, 2003; Hu, Prandini,
& Tomlin, 2007), radar avoidance unmanned air vehicles fighter aircraft (Pachter &
Pachter, 2001; Kamal, Gu, & Postlethwaite, 2005; Larson, Pachter, & Mears, 2005; Shi,
Wang, Liu, Wang, & Zu, 2007), persistent area denial (Subramanian & Cruz, 2003;
Liu, Cruz, & Schumacher, 2007; Castanon, Pachter, & Chandler, 2004).
many control approaches multi-agent systems, stochastic influences dynamics
agents taken account assumed negligible, dynamics
c
2008
AI Access Foundation. rights reserved.

fivan den Broek, Wiegerinck & Kappen

modeled deterministically. system truly deterministic, agents
optimally controlled open loop controls. However, stochastic influences
dynamics large ignored, open loop controls become far optimal,
multi-agent system longer modeled deterministically.
usual
approach control multi-agent systems stochastic dynamics model system
Markov Decision Processes (MDP) (Boutilier, 1996; Sadati & Elhamifar, 2006).
principle, solved discrete space time backward dynamic programming.
However, discretization make joint state space multi-agent system increase
exponentially number agents, basic dynamic programming approach
generally infeasible (Boutilier, 1996). attempt overcome exploit structures
problem describe system factored MDP. general structures
conserved value functions, exact computations remain exponential
system size. Guestrin, Koller, Parr (2002a) Guestrin, Venkataraman, Koller
(2002b) assumed predefined approximate structure value functions, thereby
provided efficient approximate MDP model multi-agent systems. similar approach
taken Becker, Zilberstein, Lesser, Goldman (2003, 2004), assuming independent
collaboration agents global reward function, resulting transition-independent
decentralized MDPs.
paper concentrate multi-agent systems agents joint task
reach number target states. model multi-agent system
continuous space time, following approach Wiegerinck, van den Broek,
Kappen (2006). make following assumptions. agents assumed
complete accurate knowledge global state system (assumption 1).
dynamics agent additive control disturbed additive Wiener noise
(assumption 2). performance agents valued global cost function,
integral instantaneous costs plus end cost. joint task agents modeled
end cost. instantaneous costs assumed quadratic control
(assumption 3). noise level dynamics agents inversely proportional
control cost (assumption 4). Finally, assume autonomous dynamics
instantaneous costs factorize agents (assumption 5).
assumptions 1 2, optimal control problem partially solved finding
optimal expected cost-to-go, satisfies so-called stochastic Hamilton-JacobiBellman (SHJB) equation. optimal expected cost-to-go given, optimal
control provided gradient optimal expected cost-to-go adopting assumption 3. SHJB equation nonlinear partial differential equation (PDE),
nonlinearity makes difficult solve. common approach solving SHJB equation
assume, addition assumption 3, instantaneous costs end cost
cost function quadratic state, dynamics linear
state wellthis known linear-quadratic control. optimal expected cost-to-go
quadratic state time-varying coefficients, problem reduces
solving Riccati equations coefficients satisfy (Stengel, 1993; ksendal, 1998).
Otherwise, approximation methods needed. approximate approach given
iterative linear-quadratic Gaussian method (Todorov & Li, 2005); yields locally optimal feedback control, valid case little noise. instead follow approach
Fleming (1978) adopt assumption 4. assumption SHJB equation
96

fiGraphical Model Inference MAS Optimal Control

transformed linear PDE performing logarithmic transformation. solution
equals expectation value stochastic integral diffusion process. general,
closed form expression. paper estimate expression formulating
path integral (Kappen, 2005a, 2005b), estimate latter using MetropolisHastings sampling. several ways estimate path integral,
Hamilton Monte Carlo sampling Laplace approximation, covered
paper.
structure optimal expected cost-to-go generally complex due
dynamic couplings agents. adopting assumption 5, agents
coupled joint end cost, solely determines structure
optimal expected cost-to-go. result state transition probabilities factorize
agents. follows optimal control becomes weighted combination
single-agent single-target controls. weights given joint distribution
agent target assignments. joint distribution structure joint end
cost. structure joint distribution representable factor graph,
optimal control problem becomes graphical model inference problem (Wiegerinck et al.,
2006). complexity graphical model inference exponential tree width
factor graph. Exact inference possible using junction tree algorithm,
given graph sufficiently sparse number agents large.
complex situations approximate inference methods necessary, show
optimal control accurately approximated polynomial time, using naive mean
field (MF) approximation belief propagation (BP). makes distributed coordination
possible multi-agent systems much larger could treated
exact inference.
paper organized follows. Sections 2 3, provide review
single multi-agent stochastic optimal control framework, developed Kappen (2005a, 2005b) Wiegerinck et al. (2006). example, rederive linear
quadratic control. general solution given terms path integral, explain
approximated Metropolis-Hastings sampling.
Section 4, give factor graph representation end cost function. discuss two graphical model approximate inference methods: naive mean field approximation
belief propagation. show approximation optimal control
methods obtained replacing exact weights controls respective
approximations.
Section 5, present numerical results. make comparison approximate
optimal controls, infered naive mean field approximation, belief propagation
greedy method, exact optimal control; multi-agent system
18 agents linear dynamics two-dimensional state space, two target
states. Furthermore, present results control multi-agent systems nonlinear
dynamics four-dimensional state space, agents control forward velocity
driving direction. controls approximated combination MetropolisHastings sampling, infer path integrals, naive mean field approximation, infer
agent target assignments. allowed us control systems 80 agents
80 target states. results regarding nonlinear dynamics illustrative
purpose.
97

fivan den Broek, Wiegerinck & Kappen

2. Stochastic Optimal Control Single Agent
consider agent k-dimensional continuous state space Rk , state x(t) evolving
time according controlled stochastic differential equation
dx(t) = b(x(t), t)dt + u(x(t), t)dt + dw(t),

(1)

accordance assumptions 1 2 introduction. control agent
Rk -valued function u x(t) t. noise dynamics modeled Wiener
process w(t), i.e., normally distributed k-dimensional stochastic process continuous
time mean 0 variance t, k k matrix represents variance
noise. autonomous dynamics modeled b, Rk -valued function
x(t) t. state change dx(t) sum noisy control autonomous
dynamics.
behavior agent valued cost function. Given agents state x(t) = x
present time t, control u, expected future cost agent:



Z
1

C u (x, t) = Eux,t (x(T )) +
kRu(x(), )k2 + V (x(), ) .
(2)
2

expectation Eux,t taken respect probability measure x(t)
solution (1) given control law u condition x(t) = x. cost
combination end cost (x(T )), function end state x(T ),
integral instantaneous costs. instantaneous cost sum state control
dependent term. state dependent term V (x(), ) cost state x()
time . function V arbitrary, represents environment agent.
control dependent term 12 kRu(x(), )k2 cost control state x() time ,
kzk2 = z z Euclidean norm, R full rank k k matrix. quadratic
control, accordance assumption 3 introduction, assumption 4,
R related variance noise control via relation
= (R R)1 ,

(3)

scalar.
expected cost-to-go time minimized controls u defines optimal
expected cost-to-go
J(x, t) = min C u (x, t).
(4)
u

Appendix A, explained due linear-quadratic form optimization
problemthe dynamics (1) linear action u, cost function (2) quadratic
actionthe minimization performed explicitly, yielding nonlinear partial differential equation J, so-called stochastic Hamilton-Jacobi-Bellman (SHJB) equation.
minimum attained
u(x, t) = (R R)1 x J(x, t).

(5)

optimal control. Note explicitely depends state x agent
time t, making feedback control.
98

fiGraphical Model Inference MAS Optimal Control

optimal expected cost-to-go re-expressed terms diffusion process (for
derivation, refer Appendix A):
J(x, t) = log Z(x, t)

(6)

Z(x, t) expectation value



Z
1
1
Z(x, t) = Ex,t exp (y(T ))
V (y(), )



(7)

y() diffusion process y(t) = x satisfying uncontrolled dynamics:
dy() = b(y(), )d + dw().

(8)

Substituting relations (3) (6) (5), find optimal control terms Z(x, t):
u(x, t) = x log Z(x, t).

(9)

Example 1. Consider agent one dimension state x(t) described dynamical equation (1) without autonomous dynamics (b = 0). instantaneous cost V zero,
end cost quadratic function around target state :
(y) =


|y |2 .
2

diffusion process y() satisfies uncontrolled dynamics (8) normally distributed
around agents state x = y(t) time variance 2 ( t), hence state
transition probability agent go (x, t) (y, ) space-time given
Gaussian density


|y x|2
(y, |x, t) = p
.
exp 2
2 (T t)
2 2 (T t)
1

expectation value (7) given integral

Z(x, t) =

Z

1

(y)

dy(y, |x, t)e

=





R2 /
|x |2
exp 2
,
+ R2 /
2 (T + R2 /)

relation (3) used. optimal control follows (6) (9) reads
u(x, t) =

x
.
+ R2 /

result well known (Stengel, 1993).
99

(10)

fivan den Broek, Wiegerinck & Kappen

2.1 Path Integral Formulation
Example 1 shows simple system autonomous dynamics (b = 0) costs
due environment (V = 0), write control explicitly.
uncontrolled dynamics normally distributed, consequently expectation value
(7) quadratic end cost closed form expression. general situation b
V arbitrary, longer exists explicit expression expectation value,
optimal control obtained approximation. discuss
done taking path integral approach (Kleinert, 2006). detailed derivation
expressions presented given Appendix B.
path integral approach, write expectation value (7) path integral:
Z(x, t) = lim Z (x(t0 ), t0 )

(11)

0

x(t0 ) = x, t0 =
1

Z (x(t0 ), t0 ) = p
det(2 2 )N

Z

dx(t1 ) . . .

Z

1

dx(tN ) e (x(t0 ),...,x(tN ),t0 ) .

integral paths (x(t0 ), . . . , x(tN )) discrete time, start x(t0 ) kept fixed
N = t, taken continuous time limit sending length time steps
= ti+1 ti zero. Note limit N goes infinity paths become infinite
dimensional objects. function exponent cost path:
(x(t0 ), . . . , x(tN ), t0 ) =
(x(T )) +

N
1
X

V (x(ti ), ti ) +

N
1
X
i=0

i=0


2

1
x(ti+1 ) x(ti )

R
b(x(ti ), ti )
,
2


optimal control becomes weighted average controls derived single
path:
u(x(t0 ), t0 ) = lim
0

Z

dx(t1 ) . . .

Z

dx(tN ) p(x(t0 ), . . . , x(tN ), t0 ) u(x(t0 ), . . . , x(tN ), t0 ). (12)

weights given
1

p(x(t0 ), . . . , x(tN ), t0 ) = p

e (x(t0 ),...,x(tN ),t0 )
det(2 2 )N Z (x(t0 ), t0 )

.

control derived path (x(t0 ), . . . , x(tN )) reads
u(x(t0 ), . . . , x(tN ), t0 ) =

x(t1 ) x(t0 )
b(x(t0 ), t0 ).


Note depends first two entries x(t0 ) x(t1 ) path.
100

(13)

fiGraphical Model Inference MAS Optimal Control

2.2 Path Integration Metropolis-Hastings Sampling
path integral formulation (12) optimal control generally computed,
integral uncountably many paths, exist several ways
approximate it. natural approach goes stochastic sampling paths. Several methods
stochastic sampling exist, one use known Metropolis-Hastings
sampling (Hastings, 1970). implementation time discretized: take
limit (12) decreasing zero, instead keep fixed value. sample path
sequence (xs (t0 ), . . . , xs (tN )) vectors state space Rk , x(t0 ) = x
current state agent current time t0 = t. According equation (13),
need xs (t0 ) xs (t1 ) derive control sample path (x(t0 ), . . . , x(tN )).
Metropolis-Hastings sampling ensures different paths properly weighted, hence
optimal control approximated follows:
u(x(t0 ), t0 )

hx(t1 )i x(t0 )
b(x(t0 ), t0 ),
t1 t0

(14)

hx(t1 )i mean value xs (t1 ) taken sample paths. Pseudo-code
algorithm given Algorithm 1.
Algorithm 1: Metropolis-Hastings sampling
Input: initial path (x(t0 ), . . . , x(tN ))
1: = 1
2: repeat times:
3: define Gaussian proposal distribution centered around (x(t1 ), . . . , x(tN ))
variance equal noise
4: draw sample path (x (t1 ), . . . , x (tN )) proposal distribution

5: = exp 1 (x(t0 ), x(t1 ), . . . , x(tN ), t0 ) 1 (x(t0 ), x (t1 ), . . . , x (tN ), t0 )
6: 1
7:
set (x(t1 ), . . . , x(tN )) = (x (t1 ), . . . , x (tN ))
8: else
9:
set (x(t1 ), . . . , x(tN )) = (x (t1 ), . . . , x (tN )) probability
10: end
11: (xs (t0 ), . . . , xs (tN )) = (x(t0 ), . . . , x(tN ))
12: = + 1
13: end repeat
14: compute approximate control equation (14)

3. Stochastic Optimal Control Multi-Agent System
turn issue optimally controlling multi-agent system n agents.
principle, theory developed single agent straightforwardly generalizes multiagent situation. agent k-dimensional state xa satisfies dynamics similar
(1):
dxa (t) = ba (xa (t), t)dt + ua (x(t), t)dt + dwa (t),
(15)
accordance assumptions 1, 2 5 introduction. Note control
agent depends state xa , joint state x = (x1 , . . . , xn )
101

fivan den Broek, Wiegerinck & Kappen

system. system joint cost function similar (2), depending joint
state x joint control u = (u1 , . . . , un ) system:



n Z
X
1
u
u
2

C (x, t) = Ex,t (x(T )) +
kRa ua (x(), )k + V (xa (), ) .
2

a=1

expectation Eux,t taken respect probability measure x(t)
solution (15) given control law u condition x(t) = x. cost
combination joint end cost (x(T )), function joint end state x(T ),
integral instantaneous costs. instantaneous cost factorizes agents,
accordance assumption 5 introduction. agent, sum state
dependent term V (xa (), ) control dependent term 12 kRa ua (xa (), )k2 , similar
single agent case. accordance assumption 4 introduction, control cost
agent related noise agents dynamics via relation
= (Ra Ra )1 ,
agent. joint cost function minimized joint
control, yielding optimal expected cost-to-go J. optimal expected cost-to-go
expressed terms diffusion process via relation
J(x, t) = log Z(x, t),
Z(x, t) joint expectation value
!#
"
n Z
1X
1
V (ya (), )
Z(x, t) = Ex,t exp (y(T ))




(16)

a=1

y1 (t), . . . , yn (t) diffusion processes, = (y1 , . . . , yn ) y(t) = x, satisfying
uncontrolled dynamics
dya () = ba (ya (), )d + dwa (),

= 1, . . . , n.

(17)

multi-agent equivalent optimal control (9) reads
ua (x, t) = xa log Z(x, t).

(18)

show optimal control agent understood expected
control, is, integral target states ya transition probability target
times optimal control target. end, write expectation (16)
integral end state:
Z
n

1
Z(x, t) = dye (y)
Za (ya , ; xa , t),
(19)
a=1

Za (ya , ; xa , t) implicitly defined



Z
Z
1
V (ya (), )
dya Za (ya , ; xa , t)f (ya ) = Exa ,t f (ya (T )) exp

102

fiGraphical Model Inference MAS Optimal Control

arbitrary functions f . Substituting (19) (18) yields
Z
ua (x, t) = dya pa (ya |x, t) ua (ya ; xa , t)

(20)


ua (ya ; xa , t) = xa log Za (ya , ; xa , t)

(21)

optimal control agent go state xa current time state ya
end time , pa (ya |x, t) marginal
n


1
1
p(y|x, t) =
Za (ya , ; xa , t).
e (y)
Z(x, t)
a=1

3.1 Discrete End States
agents fulfill task arriving number target states end time
according initially specified way: example, arrive
target, arrive different targets. targets considered regions
G1 , . . . , Gm state space, end cost modeled follows:
1

e (y) =

X


w(s)

n


wa (ya ; sa ),

1

wa (ya ; sa ) = e (ya ;sa ) ,

(22)

a=1

sum runs assignments = (s1 , . . . , sn ) agents regions Gsa . (ya ; sa )
cost function associated region Gsa , returning low cost end state ya agent
lies region Gsa high cost otherwise. w(s) weight, grading assignments
thereby specifying joint task agents. Assignments result better
fulfillment task higher weight. situation agents go
target, example, vector assigns agent different target
low weight w(s).
choice end cost, equation (19) factorizes
Z(x, t) =

X



Za (sa ; xa , t) =

Z

w(s)

n


Za (sa ; xa , t)

a=1

dya Za (ya , ; xa , t)wa (ya ; sa ).

(23)

interpretation Za (sa ; xa , t) log Za (sa ; xa , t) expected cost agent
move xa target sa . optimal control (20) single agent becomes
ua (x, t) =


X

p(sa |x, t)ua (sa ; xa , t),

(24)

sa =1


ua (sa ; xa , t) = xa log Za (sa ; xa , t)
103

(25)

fivan den Broek, Wiegerinck & Kappen

control agent go target sa , weights p(sa |x, t) single-agent
marginals
X
p(s|x, t)
(26)
p(sa |x, t) =
s\sa

joint distribution
n


1
p(s|x, t) =
Za (sa ; xa , t).
(27)
w(s)
Z(x, t)
a=1


1
1
weight p(s|x,
Pnt) equals ratio exp J(s; x, t) / exp J(x, t) , J(s; x, t) =
log w(s) a=1 log Za (sa ; x, t) optimal expected cost-to-go case agents
predetermined targets specified assignment s; assignment agents
targets low expected cost J(s; x, t) yield high weight p(s|x, t),
associated single-agent single-target controls ua (sa ; xa , t) predominant
optimal controls ua (x, t).
3.2 Metropolis-Hastings Sampling Multi-Agent Systems
general, controls ua (sa ; xa , t) marginals p(sa |x, t) optimal control (24) closed form solution, inferred approximately.
controls ua (sa ; xa , t) approximated Metropolis-Hastings sampling discussed
Section 2.2. Inference marginals involves inference path integral formulations Za (sa ; xa , t):
Z
Z
1
1
Za (sa ; xa , t) = lim p
dxa (t1 ) . . . dxa (tN )e (xa (t0 ),...,xa (tN ),t0 ;sa )
2
N
0
det(2 )
xa (t0 ) = xa , t0 =

S(xa (t0 ), . . . , xa (tN ), t0 ; sa ) = (xa (T ); sa )
+

N
1
X

V (xa (ti ), ti ) +

i=0

N
1
X
i=0


2

1
xa (ti+1 ) xa (ti )

Ra
ba (xa (ti ), ti )
.
2


value Za (sa ; xa , t) generally hard determine (MacKay, 2003). Possible approximations include maximum posteriori (MAP) estimate inclusion variance
sample paths. third approximation take average path costs
estimate log Za (sa ; xa , t); means entropy distribution path
integral neglected.

4. Graphical Model Inference
additional computational effort multi-agent control compared single-agent control
lies computation marginals p(sa |x, t) joint distribution p(s|x, t),
involves sum mn assignments s. small systems feasible, large
systems summation performed efficiently. efficient approach
provided graphical model inference, relies factor graph representation
joint distribution.
104

fiGraphical Model Inference MAS Optimal Control

1,4

1,2

1

2,4

4

3,4

2

2,3

3

Figure 1: Example factor graph multi-agent system four agents. couplings
represented factors A, = {1, 4}, {1, 2}, {2, 4}, {3, 4}, {2, 3}.

4.1 Factor Graph Representation Joint Distribution
complexity joint distribution part determined weights w(s) end
cost function (22). weights determine agents consider states
agents. complex case, way one agent takes state another agent
account depend states agents. situation less complicated
agent considers states agents independently states others.
means joint end cost factorized form:

w(s) =
wA (sA ),
(28)


subsets agents. structure represented graphically so-called factor
graph (Kschischang, Frey, & Loeliger, 2001). See Figure 1 example. agents
factors nodes factor graph, represented circles squares respectively,
edge agent factor member subset A,
is, wA factorization w depends sa . (27) immediate
joint distribution p(s|x, t) factorizes according factor graph.
4.2 Junction Tree Algorithm
Efficient inference distribution p(s|x, t) means factor graph representation
accomplished using junction tree algorithm (Lauritzen & Spiegelhalter, 1988).
complexity algorithm exponential induced tree width graph. small
tree width expected systems factor graph sparse, case
agents take states account limited number agents.
implies multi-agent systems sparse graphs limited number targets
tractable (Wiegerinck et al., 2006). factor graph Figure 1 example sparse
graph. hand, agent take state agent account,
junction tree algorithm really help: underlying factor graph fully
connected tree width graph equals number agents system.
Exact computation optimal control intractable large complex multiagent systems, since junction tree algorithm requires memory exponential tree
width factor graph. Instead use graphical model approximate inference
methods approximately infer marginals (26). proceed discussion
two methods: naive mean field (MF) approximation (Jordan, Ghahramani, Jaakkola,
& Saul, 1999) belief propagation (BP) (Kschischang et al., 2001; Yedidia, Freeman, &
Weiss, 2001).
105

fivan den Broek, Wiegerinck & Kappen

4.3 Naive Mean Field Approximation
starting point note optimal expected cost-to-go log partition sum,
also known free energy. Consider variational free energy
X
F (q) = h log wiq
hlog Za iqa H(q),


h iq h iqa denote expectation values respect distribution q marginals
qa respectively, H(q) entropy q:
X
H(q) =
q(s) log q(s).


optimal expected cost-to-go equals variational free energy minimized distributions q. naive mean field approximation
one considers variational free energy
Q
restricted factorized distributions q(s) = qa (sa ). minimum
F (q)
JMF = min
Q
q=

qa

upper bound optimal expected cost-to-go J, equals J case agents
uncoupled. F zero gradient local minima, is,
0=

F (q1 (s1 ) qn (sn ))
qa (sa )

= 1, . . . , n,

(29)

additional constraints normalization distributions qa . Solutions set
equations implicitly given mean field equations
Za (sa )hw|sa iq
qa (sa ) = Pn


sa =1 Za (sa )hw|sa iq

(30)

hw|sa iq conditional expectation w q given sa :

X
qa (sa ) w(s1 , . . . , sn ).
hw|sa iq =
s1 ,...,sn \sa

6=a

mean field equations solved means iteration; procedure results
convergence local minimum free energy.
mean field approximation optimal control found taking gradient
respect x minimum JMF free energy. similar exact case
optimal control gradient optimal expected cost-to-go, equation (18).
Using (29), find
X
1
ua (x, t) = xa JMF (x, t) =
qa (sa )ua (xa , t; sa ).




Similar exact case, average single-agent single-target optimal controls
ua (xa , t; sa ) given equation (25), average taken respect mean
field approximate marginal qa (sa ) agent a.
106

fiGraphical Model Inference MAS Optimal Control

4.4 Belief Propagation
belief propagation, approximate free energy Bethe free energy,
minimize latter. Bethe free energy defined
FBethe ({qa , qA }) =

X

h log wA iqA



X

h log Za iqa



X

H(qA ) +



X

(na 1)H(qa ).



(31)
function beliefs qa (sa ) qA (sA ), non-negative normalized functions
satisfy consistency relations:
:

X

qA (sA ) = qa (sa ).

sA\a

H(qa ) H(qA ) entropies beliefs qa qA , na denotes number
neighbors node factor graph.
Belief propagation algorithm computes beliefs (Kschischang et al., 2001).
case joint distribution p factor graph representation tree, belief propagation converge beliefs exact marginals p, Bethe free energy
beliefs equals optimal expected cost-to-go J. factor graph representation
p contains cycles, may still apply belief propagation. Yedidia et al. (2001) showed
fixed points algorithm correspond local extrema Bethe free energy.
particular, advanced variations algorithm (Heskes, Albers, & Kappen, 2003;
Teh & Welling, 2001; Yuille, 2002) guaranteed converge local minima Bethe
free energy (Heskes, 2003).
find BP approximation optimal control taking gradient
minimum JBethe Bethe free energy:
X
1
ua (x, t) = xa JBethe (x, t) =
qa (sa )ua (xa , t; sa ),




ua (xa , t; sa ) given equation (25). Similar exact case mean field
approximation, BP approximation optimal control average single-agent
single-target optimal controls, average taken respect belief qa (sa ).

5. Numerical Results
section, present numerical results simulations optimal control multiagent systems. problem computing optimal controls (24) consists two parts:
inference single-agent single-target controls (25), inference
marginals (26) global distribution agent target assignments. dynamics linear, instantaneous costs V zero, single-agent single-target
controls given closed form. multi-agent systems therefore know issue
infering marginal distributions. Section 5.1 consider multi-agent systems
kind. Section 5.2 deals general problem infering optimal controls
dynamics nonlinear instantaneous costs V nonzero. sections
107

fiExpected Target

van den Broek, Wiegerinck & Kappen

Position

1
0
1
0

0.5

1
Time

1.5

2

1
0
1
0

(a) Positions

0.5

1
Time

1.5

2

(b) Expected Targets

Figure 2: Two agents, noise control positions, need reach target locations -1 1 end time = 2, agent different target location.
positions (a) expected targets (b) time.

joint end cost given equation (22),
w(s) =

n

a,b

wa,b (sa , sb ),

c

wa,b (sa , sb ) = exp sa ,sb ,
n

(32)




1
(33)
(ya ; sa ) = |ya sa |2 ,
wa (ya ; sa ) = exp (ya ; sa ) ,

2
c determines coupling strength agents, sa target
states.
5.1 Linear Dynamics
begin illustration optimal control showing simulation exactly
solvable stochastic multi-agent system. system two agents one dimension,
agents satisfy dynamics (15) ba equal zero. two target states, x = 1 = 1
x = 2 = 1. task agents one go different target.
instantaneous costs V cost function zero, end cost function given
equations (22), (32) (33) = 20 c = 4. negative sign coupling
strength c implies repulsion agents. control cost parameter R equals 1,
noise level 2 lies 0.5. agents start x = 0 time = 0, end time lies
= 2. prevent overshooting targets, udt small compared distance
target states. done choosing dt = 0.05(T + 0.05).
P
Figure 2 shows agents positions expected targets
sa =1,2 p(sa |x, t)sa
time. see time = 1, agents decided target
go, remain two targets. Then, = 1, final decision
seems made. delayed choice due symmetry breaking cost-togo time increases. symmetry breaking, better keep options open,
see effect noise is. symmetry breaking, time short wait
longer choice made. phenomenon typical multi-modal problems.
proceed quantitative comparison different control methods arise
exact approximate inferences marginals joint distribution (27).
108

fiGraphical Model Inference MAS Optimal Control

3

10

4

2

CPU Time

Cost Difference

5
3
2
1
0
1
0

10

1

10

0

10

1

0.2

0.4 0.6
Noise

0.8

10

1

(a) Costs

0

0.2

0.4 0.6
Noise

0.8

1

(b) CPU Time

Figure 3: deviation optimal cost (a) required CPU Time seconds
(b) functions noise. lines represent exact ( ), Greedy ( ), MF
() BP () control.

example consider multi-agent system n = 18 agents two-dimensional
state space zero instantaneous costs (V = 0) autonomous dynamics (ba = 0).
end cost function given equations (22), (32) (33). two targets located
1 = (1, 0) 2 = (1, 0). = 20 c = 0.5. control cost matrix R equals
identity matrix. agents start (0, 0) time = 0, end time lies = 2,
time steps size dt = 0.05(T + 0.05).
approximations naive mean field approximation belief propagation, described Section 4, greedy control. greedy control mean time step
agent chooses go nearest target. include approximation
simple requires little computation time, reasons obvious choice
naive approximation. greedy control policy neglects choices
agents, expect give inferior performance.
approximation, Figure 3(a) shows cost approximate (optimal)
control minus cost exact (optimal) control, averaged 100 simulations,
different noise levels. noise samples used approximate exact
control. see naive mean field approximation belief propagation yield
costs average coincide cost exact control: average cost difference
methods significantly differ zero. Greedy control,
hand, yields costs significantly higher costs exact control;
deterministic limit converge cost exact control, controls
coincide. Figure 3(b) shows CPU time required calculation controls
different control methods. average CPU time entire simulation.
simulation consists 73 time steps, time step control calculated
agent. observe greedy control least 10 times faster methods,
exact control nearly 100 times time consuming methods. Belief
propagation gives performance considered noise levels bit quicker
naive mean field approximation, may result implementation details.
also done simulations attractive coupling c = 0.5; returned results similar
ones repulsive coupling c = 0.5 presented here.
109

fiCumulative Control Cost

van den Broek, Wiegerinck & Kappen

20
15
10
5
0
0

0.5

1
Time

1.5

2

Figure 4: cumulative control cost time, case strong repulsive coupling
c = 2 low noise level 2 = 0.1. curves represent exact ( ), MF
(), BP control ().

Although Figure 3 suggests belief propagation naive mean field approximation
perform equally well, always case, since certain combinations noise
level coupling strength BP control costly MF control exact
control. origin difference lies symmetry breaking, tends occur
later BP earlier MF compared exact control. observe
Figure 4, shows cumulative cost time control methods
multi-agent system, coupling strength c = 2 fixed noise level 2 = 0.1.
cumulative costs averages 100 simulations. cost MF control lies
bit higher cost exact control, whereas cost BP control initially
lower cost control methods, = 1.7 starts increase
much faster eventually ends higher. Including end costs, found total costs
26.13 0.12 exact control, 26.19 0.12 MF control, 35.5 0.4 BP
control. suggests better early symmetry breaking late
symmetry breaking.
time required computing control various methods depends
number agents multi-agent system. Figure 5 shows required CPU time
function number agents n two-dimensional multi-agent system considered
above. see exact method requires CPU time increases exponentially
number agents. may expected theory,
exact method uses junction tree algorithm complexity exponential
tree width underlying graph, i.e., exponential n. greedy method,
CPU time increases linearly number agents, agreement
fact greedy control coupling agents. required CPU
time increases polynomially mean field approximation belief propagation.
5.2 Nonlinear Dynamics
turn multi-agent systems nonlinear dynamics. control systems,
must approximate graphical model inference well single-agent singletarget control problem (12). consider multi-agent system agents move
110

fiGraphical Model Inference MAS Optimal Control

3

10
CPU Time

2

10

1

10

0

10

1

10

10

15

20
Agents

25

30

Figure 5: required CPU time seconds calculation controls different
number agents. Exact ( ), greedy ( ), MF (), BP control ().

two dimensions four-dimensional state specified agents location
(xa , ya ), forward velocity va , driving direction . dynamics agent
given equations
dxa = va cos dt
dya = va sin dt
dva = ua dt + dwa
da = dt + da .
first two equations model kinematics agents position given forward
velocity driving direction. last two equations describe control speed
driving direction application forward acceleration ua angular velocity
. noise control modeled standard normal Wiener processes wa
noise level parameters . Note noise act dimensions
control. Although control space counts less dimensions
state space, example fit general framework: refer Appendix C
details.
look two different tasks. first task obstacle avoidance multiagent system three agents. agents reach one three target locations
avoid obstacles environment. target location reached precisely
one agent; model end cost function, given equations (22), (32) (33),
= c = 0.5. targets located (10, 15), (45, 12) (26, 45),
agents arrive zero velocity. control cost matrix R identity matrix.
= 0.1. instantaneous cost V equaled 1000 locations obstacles, zero
otherwise. agents start time = 0, end time lies = 20, time steps dt
size 0.2. starting locations agents (18, 31), (25, 12) (39, 33),
agents start zero velocity. sample paths discrete time paths twodimensional space forward velocity v driving direction . specified
values times ti = + , = 0, . . . , N 1, = NT
1 N = 7, value
time t0 equals current state one agents, value time tN equals
one target end states. control agent one targets computed
111

fivan den Broek, Wiegerinck & Kappen

50

50

40

40

30

30

20

20

10

10

0

0

10

20

30

40

50

0

(a) Trajectories

0

10

20

30

40

50

(b) Sample paths

Figure 6: Three agents, noise control forward velocities driving directions, reach three targets (marked X) environment containing
also number walls. agent starts different location (marked O)
zero forward velocity, agent arrive different target
zero velocity without hitting walls. (a) trajectories agents
followed reach targets. (b) Sample paths.

Metropolis-Hastings sampling paths, according Subsection 3.2. proposal
distribution 2N -dimensional Gaussian, centered around agents current planned
path, variance equal noise level agents dynamics. expectation
values Za (sa ; xa , t) estimated average costs sample paths. also
tried MAP estimation Za (sa ; xa , t) inclusion variance sample paths,
former show significant difference, latter returned estimates
fluctuated heavily. Figure 6(a) shows environment trajectories agents
starting locations targets. agent manages avoid obstacles
arrive one targets zero velocity, target reached different
agent.
second task coordination multi-agent system shown Figure 7(a). system instantaneous costs (V = 0). agents move
initial positions number target locations. arrive
locations zero velocity horizontal driving direction. equal number
agents target locations, agent reach different target. initial
locations aligned vertically, target locations, vertical displacement two. Thus agents coordinate movements order
reach targets satisfactory way.
agents start time 0, end time lies 100, make time steps size

dt = 2(N
1) , N = 7, dt < 0.01. time step controls computed
Metropolis-Hastings sampling paths naive mean field approximation infer
marginals pa (sa |x, t) weigh single-agent single-target controls, equations (24)
(26). sample paths discretized seven equidistant time points
present time end time. proposal distribution taken Gaussian,
112

fiGraphical Model Inference MAS Optimal Control

centered around agents current planned path variance equal noise
level agents dynamics. Figure 7(a) shows example trajectories system
10 agents. obtained 10 sample paths per agent-target combination.
observe agents reach targets, target reached precisely one
agent, required. Due noise second order dynamics agents, takes
agents less effort approach target remain there, since former allows
exploitation noise latter requires constant correction state changes
caused noise. result trajectories agents curved
elongated would expected situation without noise. simulation
carried well larger number agents. Figure 7(b) shows required CPU time
function number agents, exact MF inference marginals
agents. Note complexity graphical model inference problem scales
nn , n number agents. Exact inference using junction tree algorithm
feasible n < 10.

6. Discussion
studied use graphical model inference methods optimal control stochastic
multi-agent systems continuous space time agents joint task
reach number target states. Rather discretizing, commonly done typically
makes large systems intractable due curse dimensionality, followed approach
developed Wiegerinck et al. (2006), modeling system continuous space time.
certain assumptions dynamics cost function, solution given
terms path integral.
path integral computed closed form special cases,
linear-quadratic case, general approximated. done
variety methods. method considered paper MCMC sampling.
dimension sample paths kept low (N = 7) limit curvature sample
paths. gain limiting curvature variance samples reduced
less samples needed. limiting curvature, however, introduce bias.
addition, presence obstacles insufficient curvature would make sampler return
sample paths run obstacles. believe advanced MCMC
methods Hybrid MC sampling (Duane, Kennedy, Pendleton, & Roweth, 1987)
overrelaxation (Neal, 1998) improve inference path integrals.
Apart MCMC sampling, approximation methods one could
consider, Laplace approximation variational approximation. Laplace
approximation becomes exact noiseless limit could useful low noise regimes
well. variational approximation approximates path integral (11) Gaussian
process (Archambeau, Opper, Shen, Cornford, & Shawe-Taylor, 2007), could particularly useful high noise regime. drawback variational approach, however,
cannot straightforwardly applied situations infinite instantaneous costs,
like hard obstacles environment considered here.
Wiegerinck et al. (2006) showed systems sufficiently sparse
single-agent single-target controls determined closed form, e.g. linearquadratic control time-independent coefficients, exact inference achieved using
113

fivan den Broek, Wiegerinck & Kappen

50

40

30

20

10

0

10

20

30

40

50
20

15

10

5

0

5

10

15

(a) Trajectories
5

10

4

CPU time

10

3

10

2

10

1

10

0

20

40
60
Number Agents

80

(b) CPU time

Figure 7: (a) trajectories 10 agents starting locations 10 targets X. (b)
required CPU time seconds function number agents,
number targets equal number agents. lines represent exact
( ) MF () inference marginals.

114

fiGraphical Model Inference MAS Optimal Control

junction tree algorithm. Van den Broek, Wiegerinck, Kappen (2007) considered
multi-agent system second-order dynamics, linear autonomous dynamics zero
instantaneous costs, showed graphical model inference naive mean field approximation significantly outperformed greedy inference. showed close
optimal result achieved well dense systems, using graphical model approximate
inference methods. approximation methods considered naive mean field
approximation belief propagation. demonstrated performances example
system exact inference significantly time consuming. Mean field approximation showed work well, returning costs control equal optimal ones, belief
propagation performed similarly. certain value ratio coupling strength
noise level, symmetry breaking control process takes place earlier
mean field approximation compared exact inference, later belief propagation. early symmetry breaking increase costs coordination much,
however, late symmetry breaking does, making performance belief propagation
suboptimal.
variations considered case also possible within general framework.
Wiegerinck, van den Broek, Kappen (2007) discuss situations agents sequentially
visit number targets, end time fixed. focusses prefered
trajectories state space time, instead prefered states end time;
achieved modeling path cost way similar modeled end cost.
problem agents intercept moving target noisy dynamics also
covered there.
control formalism developed Kappen (2005a, 2005b) applied multi-agent
coordination Wiegerinck et al. (2006) article, demands noise
control act dimensions. One way satisfy constraint assume
agents identical. addition, single agent dynamics
noise control act dimensions. saw two-dimensional
second order system Section 5.2 condition satisfied natural way. However,
general one think examples control problems equation (3) violated.
interesting future direction research investigate extend path integral
approach used approximation cases.
paper assumes joint state space agents observable agents.
large multi-agent systems, however, realistic agent observes
state states agents physically nearby. approach
directly apply situations. Depending joint task agents, may
valid approximation optimal control sub-system consisting agents
one agent observe. task agents avoid collisions,
sufficient consider states agents nearby, task go
target crucial information states
agents. natural alternative deal partial observability describe multi-agent
system decentralized POMDP (Seuken & Zilberstein, 2008). clear however,
approach would combine path integral formalism.
topic learning addressed paper, clearly great
interest. However, one could argue sampling procedure compute path integral
115

fivan den Broek, Wiegerinck & Kappen

corresponds learning environment. discussion line thought
found (Kappen, 2007).
many possible model extensions worthwhile exploring future research.
Obvious examples bounded controls, limited observation global state
system; issues already interest study single agent situation. Others
apply typically multi-agent situation. context physical agents, introducing penalties collisions agents would become relevant. Typically, types
model extensions solution closed form, require additional
approximate numerical methods. suggestions given Kappen (2005a, 2005b).
Acknowledgments
thank reviewers useful comments. thank Joris Mooij making
available useful software (www.mbfys.ru.nl/~jorism/libDAI/). research part
Interactive Collaborative Information Systems (ICIS) project, supported Dutch
Ministry Economic Affairs, grant BSIK03024.

Appendix A. Stochastic Optimal Control
appendix give derivation (5), (6) (7), starting (1), (2), (3)
(4). Detailed discussions found many works stochastic optimal control,
example Kushner (1967), Fleming Rishel (1975), Fleming (1978), ksendal
(1998), Stengel (1993), Kappen (2005a, 2005b).
optimal expected cost-to-go J state x time defined
J(x, t) = min C u (x, t),
u


u

C (x, t) =

Eux,t


Z
(x(T )) +










1
2
kRu(x(), )k + V (x(), )
2

(34)

(35)

expected cost given control law u. equations (4) (2)
main text. first show J satisfies stochastic Hamilton-Jacobi-Bellman (SHJB)
equation


1
1 2
2

J = min
kRuk + (b + u) x J + Tr x J + V ,
(36)
u
2
2

boundary condition J(x, ) = (x). equation derived following way.
moment time holds


Z
1
2
u
ds
kRu(x(s), s)k + V (x(s), s)
J(x, t) =
C (x(), ) +
2



Z
1
2
u
kRu(x(s), s)k + V (x(s), s) .
ds
= min Ex,t J(x(), ) +
u
2

min Eux,t
u

first line follows dividing integral two integrals, one
one , using definition cost function C, second line
116

fiGraphical Model Inference MAS Optimal Control

follows definition J. rewriting yields


Z
J(x(), ) J(x, t)
1
1
u
2
0 = min Ex,t
ds
+
kRu(x(s), s)k + V (x(s), s) .
u


2
Taking limit obtain


dJ(x(t), t) 1
2
u
+ kRu(x(t), t)k + V (x(t), t) .
0 = min Ex,t
u
dt
2

(37)

Subsequently, apply dJ(x(t), t) well known chain rule diffusion processes:
dJ(x(t), t) =

X J(x(t), t)


xi

dxi (t) +

J(x(t), t)
1 X 2 J(x(t), t)
dt +
dxi (t)dxj (t). (38)

2
xi xj
i,j

differs chain rule deterministic processes also contains term
quadratic dx. extra term vanish, Wiener process appearing
dynamics (1) quadratic variation increases linear time:
Eux,t [dwi (t)dwj (t)] = ij dt.

(39)

follows expectation dxi (t)dxj (t) equal ( )ij dt. substituting dynamics (1) (38), taking expectation values, using (39), obtain


2
J(x, t)
J(x, t)
J(x, t)
u
dt + (b(x, t) + u(x, t))
dt + Tr
dt.
Ex,t [dJ(x(t), t)] =

x
xx
Substitution equation (37) yields equation (36).
minimum right-hand side equation (36) given
u = (R R)1 x J.
optimal control.
minimization (36) removed inserting optimal control. yields
nonlinear equation J. remove nonlinearity using logarithmic transformation: introduce constant , define Z(x, t) J(x, t) = log Z(x, t),

1
1
u R Ru + u x J = 2 Z 2 (x Z) (R R)1 x Z,
2
2


1 2
1
1 2
Tr x J
=
Z (x Z) x Z Z 1 Tr x2 Z .
2
2
2

terms quadratic x Z vanish R related via equation (3),
= (R R)1 .
relation satisfied, SHJB equation becomes



V
1

2
Z =
b x Tr x Z

2
= HZ,
117

(40)

fivan den Broek, Wiegerinck & Kappen

H linear operator acting function Z.
Equation (40) must solved backwards time boundary condition Z(x, ) =
1

e (x) . present solution terms forward diffusion process. common approach theory stochastic processes give solutions partial differential equations
terms diffusion processes. solution equation (40) expectation value



Z
1
1
Z(x, t) = Ex,t exp (y(T ))
V (y(), ) ,



(41)

y() process satisfies uncontrolled dynamics
dy() = b(y(), )d + dw(),
y(t) = x. expectation Ex,t taken respect probability measure
y() satisfies uncontrolled dynamics condition y(t) = x. clear (41)
matches boundary condition. verify satisfies equation (40), let


Z
1
V (y(), ) .
I(t) = exp

see

1
V (y(t), t)I(t)dt.


Let f function f (y) = exp 1 (y) . use chain rule stochastic
processes apply f (y(T )) find
dI(t) =

k
k
X
f (y(T ))
1 X 2 f (y(T ))
dyi (T ) +
dyi (T )dyj (T )
df (y(T )) =
yi
2
yi yj
i=1
i,j=1


f (y(T ))
(b(y(T ), )d + dw(T ))
=



2
1
f (y(T ))
Tr
d.
2
yy

choose = 0 = dt combine identity previous one
obtain
df (y(T ))I(t) = f (y(T ))dI(t) + I(t)df (y(T ))
= Hf (y(T ))I(t)dt + f (y(T ))I(t)dw(T ).
Taking expectation value sides makes term f (y(T ))I(t)dw(T ) disappear,
remaining part,
dE [f (y(T ))I(t)] = [f (y(T ))I(t)] dt,
equation (40).
118

fiGraphical Model Inference MAS Optimal Control

Appendix B. Path Integral Formulation
going write expectation value (7) path integral. Partitioning time
interval N intervals equal length , = t0 < t1 < . . . < tN = ,
expectation value written follows:
Z(x, t) =

Z

dx1 . . .

Z

1

dxN e (xN )

N
1


Z(xi+1 , ti+1 ; xi , ti )

(42)

i=0

x0 = x Z(xi+1 , ti+1 ; xi , ti ) implicitly defined



fi
Z
Z
fi
1 ti+1
fi
dxi+1 Z(xi+1 , ti+1 ; xi , ti )f (xi+1 ) = E f (xi+1 ) exp
V (y(), ) fiy(ti ) = xi
ti

arbitrary functions f . limit infinitesimal , Z(xi+1 , ti+1 ; xi , ti ) satisfy


1
Z(xi+1 , ti+1 ; xi , ti ) = (xi+1 , ti+1 |xi , ti ) exp V (xi , ti ) ,
(43)


(xi+1 , ti+1 |xi , ti ) transition probability uncontrolled dynamics (8) go
(xi , ti ) (xi+1 , ti+1 ) space-time. transition probability given


1
k 1 (xi+1 xi b(xi , ti ))k2
.
(xi+1 , ti+1 |xi , ti ) = p
exp
2
det(2 2 )
follows dynamics

xi+1 xi = b(xi , ti ) + w
infinitesimal time interval observation Wiener process w normally distributed around zero variance . Using equation (3), may rewrite
transition probability

2 !

1
1
x

x
i+1

exp
(xi+1 , ti+1 |xi , ti ) = p
R
b(xi , ti )
(44)

.
2
2

det(2 )

obtain path integral representation Z(x, t) combining equations (42), (43)
(44) limit going zero:
Z(x, t) = lim Z (x0 , t0 )

(45)

0

x0 = x, t0 = t,
1



Z (x0 , t0 ) = p
det(2 2 )N

(x0 , . . . , xN , t0 ) = (xN ) +

N
1
X

Z

dx1 . . .

V (xi , ti ) +

Z

N
1
X
i=0

i=0

119

1

dxN e (x0 ,...,xN ,t0 )


2

xi+1 xi
1

b(xi , ti )
R
.
2


fivan den Broek, Wiegerinck & Kappen

optimal control given equation (9) proportional gradient
log Z(x, t). Substituting path integral representation (45) Z(x, t), find
u(x0 , t0 ) = lim
0

= lim
0

Z

Z

dx1 . . .
dx1 . . .

Z

Z

1

e (x0 ,...,xN ,t0 )

dxN p
x0
det(2 2 )N Z (x, t0 )




1
(x0 , . . . , xN , t0 )


dxN p(x0 , . . . , xN , t0 )u(x0 , . . . , xN , t0 )


u(x0 , . . . , xN , t0 ) =




x1 x0
b(x0 , t0 )

1

e (x0 ,...,xN ,t0 )
p(x0 , . . . , xN , t0 ) = p
.
det(2 2 )N Z (x0 , t0 )

Note control u(x0 , . . . , xN , t0 ) results path (x0 , . . . , xN ) depends
first two entries x0 x1 path.

Appendix C. Dimension Reduction
derivation path integral Appendix B given case
state control k-dimensional. particular case dimensions
state controlled deduced taking limit infinite control cost along
dimensions without control. control along latter dimensions becomes zero,
seen equation (5). noise dimensions equal zero accordance
relation (3). path integral formalism transition probabilities (44)
reduce delta functions along dimensions without control. implications
MCMC sampling dimension space sample also reduced,
since sampling performed dimensions noise.

References
Archambeau, C., Opper, M., Shen, Y., Cornford, D., & Shawe-Taylor, J. (2007). Variational inference
diffusion processes. Advances Neural Information Processing Systems.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-independent decentralized Markov decision processes. Proceedings Second International Joint Conference
Autonomous Agents Multiagent Systems, pp. 4148.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004). Solving transition independent
decentralized Markov decision processes. Journal Artificial Intelligence Research, 22, 423
455.
Boutilier, C. (1996). Planning, learning coordination multiagent decision processes.
Proceedings Sixth Conference Theoretical Aspects Rationality Knowledge, pp.
195210.
Castanon, D. A., Pachter, M., & Chandler, P. R. (2004). game deception. Proceedings
43rd IEEE Conference Decision Control, pp. 33643369.
Duane, S., Kennedy, A., Pendleton, B., & Roweth, D. (1987). Hybrid Monte Carlo. Physics Letters
B, 195 (2), 216222.
120

fiGraphical Model Inference MAS Optimal Control

Fleming, W. H. (1978). Exit probabilities optimal stochastic control. Applied Mathematics
Optimization, 4, 329346.
Fleming, W. H., & Rishel, R. W. (1975). Deterministic Stochastic Optimal Control. SpringerVerlag, New York.
Guestrin, C., Koller, D., & Parr, R. (2002a). Multiagent planning factored MDPs. Advances
Neural Information Processing Systems, Vol. 14, pp. 15231530.
Guestrin, C., Venkataraman, S., & Koller, D. (2002b). Context-specific multiagent coordination
planning factored MDPs. Eighteenth National Conference Artificial Intelligence,
pp. 253259.
Hastings, W. (1970). Monte Carlo sampling methods using Markov chains applications.
Biometrika, 57 (1), 97109.
Heskes, T. (2003). Stable fixed points loopy belief propagation minima Bethe free
energy. Advances Neural Information Processing Systems, Vol. 15, pp. 343350.
Heskes, T., Albers, K., & Kappen, B. (2003). Approximate inference constrained optimization.
Proceedings 19th Conference Uncertainty Artificial Intelligence, pp. 313320.
Hu, J., Prandini, M., & Tomlin, C. (2007). Conjugate points formation constrained optimal
multi-agent coordination: case study. SIAM Journal Control Optimization, 45 (6),
21192137.
Jordan, M., Ghahramani, Z., Jaakkola, T., & Saul, L. (1999). introduction variational methods
graphical models. Learning Graphical Models. MIT Press, Cambridge.
Kamal, W. A., Gu, D.-W., & Postlethwaite, I. (2005). Real time trajectory planning UAVs using
MILP. Proceedings 4th IEEE Conference Decision Control, European
Control Conference 2005, pp. 33813386.
Kappen, H. J. (2005a). Path integrals symmetry breaking optimal control theory. Journal
statistical mechanics: theory experiment, P11011.
Kappen, H. J. (2005b). Linear theory control nonlinear stochastic systems. Physical Review
Letters, 95 (20), 200201.
Kappen, H. J. (2007). introduction stochastic control theory, path integrals reinforcement
learning. AIP conference proceedings, Vol. 887, pp. 149181.
Kleinert, H. (2006). Path Integrals Quantum Mechanics, Statistics, Polymer Physics, Financial Markets. World Scientific, Singapore.
Kschischang, F. R., Frey, B. J., & Loeliger, H.-A. (2001). Factor graphs sum-product
algorithm. IEEE Transactions Information Theory, 47 (2), 498519.
Kushner, H. J. (1967). Stochastic Stability Control. Academic Press Inc., New York.
Larson, R. A., Pachter, M., & Mears, M. (2005). Path planning unmanned air vehicles
engaging integrated radar network. Proceedings AIAA Guidance, Navigation,
Control Conference Exhibit.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphical structures application expert systems (with discussion). J. Royal Statistical Society
Series B, 50, 157224.
Liu, Y., Cruz, J. B., & Schumacher, C. J. (2007). Pop-up threat models persistent area denial.
IEEE Transactions Aerospace Electronic Systems, 43 (2), 509521.
MacKay, D. J. (2003). Information Theory, Inference, Learning Algorithms. Cambridge University Press.
Neal, R. M. (1998). Learning Graphical Models, pp. 205225. Kluwer Academic Publishers.
121

fivan den Broek, Wiegerinck & Kappen

ksendal, B. (1998). Stochastic Differential Equations: Introduction Applications. SpringerVerlag.
Pachter, L., & Pachter, M. (2001). Optimal paths avoiding radiating source. Proceedings
40th IEEE Conference Decision Control, pp. 35813586.
Ribichini, G., & Frazzoli, E. (2003). Efficient coordination multiple-aircraft systems. Proceedings
42nd IEEE Conference Decision Control, Vol. 1, pp. 10351040.
Sadati, N., & Elhamifar, E. (2006). Semi-decentralized control multi-agent systems based
redundant manipulator optimization methods. Proceedings 9th IEEE International
Workshop Advanced Motion Control, pp. 278283.
Seuken, S., & Zilberstein, S. (2008). Formal models algorithms decentralized decision making
uncertainty. Journal Autonomous Agents Multi-Agent Systems.
Shi, X., Wang, X., Liu, Y., Wang, C., & Zu, C. (2007). Optimization fighter aircraft evasive trajectories radar threats avoidance. Proceedings 2007 IEEE International Conference
Control Automation, pp. 303307.
Stengel, R. (1993). Optimal Control Estimation. Dover Publications, New York.
Subramanian, S. K., & Cruz, J. B. (2003). Adaptive models pop-up threats multi-agent
persistent area denial. Proceedings 42nd IEEE Conference Decision Control,
pp. 510515.
Teh, Y., & Welling, M. (2001). unified propagation scaling algorithm. Advances
Neural Information Processing Systems, Vol. 14, pp. 953960.
Todorov, E., & Li, W. (2005). generalized iterative LQG method locally-optimal feedback
control constrained nonlinear stochastic systems. Proceedings American Control
Conference, pp. 300306.
Tomlin, C., Pappas, G. J., & Sastry, S. (1998). Conflict resolution air traffic management: study
multiagent hybrid systems. IEEE Transactions Automatic Control, 43 (4), 509521.
van den Broek, B., Wiegerinck, W., & Kappen, B. (2007). Optimal control large stochastic multiagent systems. Proceedings Seventh Symposium Adaptive Learning Agents
Multi-Agent Systems, pp. 920.
van Leeuwen, P., Hesseling, H., & Rohling, J. (2002). Scheduling aircraft using constraint satisfaction.
Electronic Notes Theoretical Computer Science, 76, 252268.
Wiegerinck, W., van den Broek, B., & Kappen, B. (2006). Stochastic optimal control continuous
space-time multi-agent systems. Proceedings 22nd Conference Uncertainty
Artificial Intelligence, pp. 528535.
Wiegerinck, W., van den Broek, B., & Kappen, B. (2007). Optimal on-line scheduling stochastic
multi-agent systems continuous space-time. Proceedings Sixth International Joint
Conference Autonomous Agents Multiagent Systems, pp. 744751.
Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. Advances Neural
Information Processing Systems, Vol. 13, pp. 689695.
Yuille, A. (2002). CCCP algorithms minimize Bethe Kikuchi free energies: Convergent
alternatives belief propagation. Neural Computation, 14 (7), 16911722.

122

fiJournal Artificial Intelligence Research 32 (2008) 939-982

Submitted 01/31; published 08/08

Optimal Strategies Bidding Agents Participating
Simultaneous Vickrey Auctions Perfect Substitutes
Enrico H. Gerding
Rajdeep K. Dash
Andrew Byde
Nicholas R. Jennings

eg@ecs.soton.ac.uk
rkd@ecs.soton.ac.uk
ab06v@ecs.soton.ac.uk
nrj@ecs.soton.ac.uk

Intelligence, Agents, Multimedia Group
School Electronics Computer Science
University Southampton, Southampton, UK

Abstract
derive optimal strategies bidding agent participates multiple, simultaneous
second-price auctions perfect substitutes. prove that, everyone else bids locally
single auction, global bidder always place non-zero bids available
auctions, provided budget constraints. budget, however, optimal
strategy bid locally budget equal less valuation. Furthermore,
wide range valuation distributions, prove problem finding optimal
bids reduces two dimensions auctions identical. Finally, address markets
sequential simultaneous auctions, non-identical auctions, allocative
efficiency market.

1. Introduction
recent years, surge application auctions, online
within multi-agent systems (Wellman, Greenwald, & Stone, 2007; Clearwater, 1996; Gerding, Rogers, Dash, & Jennings, 2007b; Rogers, David, & Jennings, 2005; Rosenthal & Wang,
1996; Roth & Ockenfels, 2002; Dash, Parkes, & Jennings, 2003). result,
increasing number auctions offering similar even identical goods services.1
take advantage fact, automated support needs developed monitor, bid
in, make decisions across large set possibilities. software, form
intelligent bidding agents (hereafter shortened bidder), increase likelihood winning item lower price, thus result considerable advantage buyer.
Now, whereas participating many auctions arduous task done manually,
problem ideally suited autonomous agents execute proper actions
buyers behalf (Stone, Schapire, Littman, Csirik, & McAllester, 2003). end,
paper devise analyse optimal bidding strategies one auction setting
namely, bidder participates multiple, simultaneous second-price auctions
goods perfect substitutes. show, however, analysis also applies
wider context markets consisting sequential (i.e., auctions close one
1. eBay alone, example, often hundreds sometimes even thousands concurrent auctions
running worldwide selling substitutable items. illustrate, time writing, 1600 eBay
auctions selling Apple iPhone worldwide.
c
2008
AI Access Foundation. rights reserved.

fiGerding, Dash, Byde & Jennings

other), well simultaneous (i.e., auctions close time)
auctions.
date, much existing literature multiple auctions focuses either sequential
auctions (Krishna, 2002) simultaneous auctions complementarities,
value items together greater sum individual items (see Section 2
related research simultaneous auctions). contrast, consider bidding strategies
case simultaneous auctions perfect substitutes. particular, focus
Vickrey second-price sealed bid auctions. choose low
communication overhead (in terms number required interactions) well
known capacity induce truthful bidding. result, type auction
generalisations, Vickrey-Clarke-Groves mechanism, used
number multi-agent system settings (Dash, Rogers, Reece, Roberts, & Jennings, 2005;
Varian, 1995; Mes, van der Heijden, & van Harten, 2007; Dash, Vytelingum, Rogers, David,
& Jennings, 2007). Moreover, auctions (weakly) strategically equivalent
widely used English auctions.2 However, find that, multiple auctions
running simultaneously, truthful bidding longer optimal. Given this, characterise,
first time, bidding agents utility-maximising strategy bidding number
auctions type bidder valuation distribution.
detail, consider market single bidder, called global bidder,
bid number auctions, whereas bidders, called local bidders,
assumed bid single auction. distinguish two types settings
market: one auctions identical global bidder indifferent
auctions, one global bidder prefers auctions others.
settings main results follows:
Whereas case single second-price auction bidder weakly dominant
strategy bid true value, longer case several simultaneous
auctions. best strategy global bidder bid true value.
prove that, even global bidder requires one item assuming free disposal,
expected utility maximised participating (i.e., bidding non-zero amount)
auctions selling desired item.
Finding optimal bid auction arduous task considering
possible combinations. However, global bidder indifferent auctions, able significantly reduce search space common bidder valuation
distributions. result, optimal bids efficiently calculated number
auctions. Although setting global bidder preferences auctions
involved, still apply analytical methods obtain tractable optimal results.
prove that, auctions identical, bidders expected utility maximised
bidding either uniformly across auctions, relatively high one auctions,
same, low value others (which two behaviours optimal depends
bidder valuation market conditions number bidders).
2. specifically, Vickrey English auction strategically equivalent assuming private valuations
good, i.e., bidders value item remains unchanged bidding process.

940

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

case global bidder different preferences auctions,
show optimal bid relatively higher auctions preferred
auctions.
argue that, even though global bidder significantly higher expected utility
local one, bidders necessarily bid globally. example, bidders
budget considerations constrain amount bid, show analytically
that, budget equal less valuation, optimal bid single
auction certain conditions.
Finally, consider issue market efficiency simultaneous
auctions. Efficiency important system-wide consideration within area multiagent systems since characterises well allocations system maximise
overall utility (Dash et al., 2003). Now, efficiency maximised goods allocated
value most. However, certain amount inefficiency inherent
distributed market auctions held separately. Given this, paper,
measure efficiency markets local bidders consider impact
global bidders inefficiency. doing, find presence global bidder
generally positive impact efficiency.
remainder paper structured follows. first discuss related work
Section 2. Section 3 describe bidders auctions detail. Section 4
characterise optimal bidding behaviour base setting auctions
identical, Section 5 explore number extensions: non-identical auctions, budget
constraints sequential auctions. Section 6 address market efficiency
impact global bidder. Finally, Section 7 concludes. proofs placed
appendix.

2. Related Work
Research area simultaneous auctions segmented along two broad lines.
one hand, game-theoretic analysis simultaneous auctions concentrates studying equilibrium strategy rational agents (Engelbrecht-Wiggans &
Weber, 1979; Krishna & Rosenthal, 1996; Lang & Rosenthal, 1991; Rosenthal & Wang,
1996; Szentes & Rosenthal, 2003). analyses typically used auction format employed simultaneous auctions (e.g., Vickrey auctions
first-price auctions). hand, heuristic strategies developed
complex settings sellers offer different types auctions buyers
need buy bundles goods distributed auctions (Stone et al., 2003; Byde, Preist, &
Jennings, 2002; Yuen, Byde, & Jennings, 2006; Greenwald, Kirby, Reiter, & Boyan, 2001;
Greenwald & Boyan, 2004; Wellman, Reeves, Lochner, & Vorobeychik, 2004). paper
adopts former approach studying market simultaneous Vickrey auctions since
approach yields provably optimal bidding strategies.
Related approach seminal paper Engelbrecht-Wiggans Weber (1979),
provides one starting points game-theoretic analysis distributed
markets buyers substitutable goods. work analyses market consisting
couples equal valuations want bid dresser. Thus, couples bid
941

fiGerding, Dash, Byde & Jennings

space contain two bids since husband wife two
geographically distributed auctions simultaneously. derive mixed strategy Nash
equilibrium special case number buyers large. analysis differs
use decision-theoretic approach opposed game-theoretic
one. argued among others Rothkopf (2007), Jiang Leyton-Brown (2007),
decision-theoretic analysis often sufficient practice case auctions.3 Moreover,
decision-theoretic framework allows us study complex setting bidders
different valuations global bidder bid auctions simultaneously
(which entirely possible online auctions).
Subsequently, Krishna Rosenthal (1996) studied case simultaneous auctions
complementary goods. analyse case local global bidders
characterise bidding buyers resultant market efficiency. setting provided
Krishna Rosenthal (1996) extended case common values
Rosenthal Wang (1996). However, neither works extend easily case
substitutable goods consider. case studied Szentes Rosenthal (2003),
scenario considered restricted three sellers two global bidders
bidder value (and thereby knowing value bidders).
space symmetric mixed equilibrium strategies derived special case. However,
mentioned earlier, results based decision theory, rather game theory,
setting general (i.e., consider arbitrary number auctions). number
authors study settings bidders face multiple simultaneous sealed-bid auctions,
e.g. McAfee (1993), Peters Severinov (1997), Gerding et al. (2007b), Leyton-Brown,
Shoham, Tennenholtz (2000). papers, however, assume bidders bid
single auction choose auction randomly. show here, however,
optimal. Finally, Shehory (2002) considers case concurrent English auctions,
bidding algorithms developed buyers different risk attitudes. However,
setting auctions never close time, forces bids
across auctions. Although strategy may effective described setting,
show paper, always optimal auctions close simultaneously (and
buyers bid late auctions).
Related paper also literature considers bidding budget constraints. Although beyond scope paper provide full literature review
topic, highlight relevant work. extensive recent overview
presented Pitchik (2006). number papers, Rothkopf (1977)
Palfrey (1980), study optimal bidding simultaneous first-price auctions bidders constraints exposure, refers sum bids. papers, however,
make strong assumption value accrued one auction independent
others. words, winning losing one auction affect expected
utility auctions. contrast, consider complete substitutes bidders
require single item. result, one auctions won, value accrued
auctions zero. interdependency auctions makes analysis sig3. Essentially, decision-theoretic setup requires (1) information distribution best
competitive bid, (2) bidders optimize decision given assessment (Rothkopf, 2007).
information distribution obtained observing previous bids, e.g., using learning
approach described Jiang Leyton-Brown (2007).

942

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

nificantly difficult. Krishna Benoit (2001) also consider interdependency,
valuations budget constraints assumed common knowledge (and
problem would thus trivial without budget constraints). Others, Che
Gale (1998), consider effect budget constraints single auctions, bidder types
consist two dimensions: valuation budget. Sequential auctions also
extensively studied regards budget constraints, e.g., Krishna Benoit
(2001), Pitchik (2006). Furthermore, addition constraints exposure, types
budget constraints considered, particular limits expected expenditures (Engelbrecht-Wiggans, 1987). None papers, however, address particular
setting consider here.
Finally, number researchers investigated online auctions eBay.
one hand, Hendricks, Onur, Wiseman (2005), Stryszowska (2004), Zeithammer (2005),
Peters Severinov (2006), Rogers, David, Schiff, Jennings (2007) sought
explain bidding behaviour buyers online auctions, include multiple bidding
(i.e., successively increasing amount placed maximum bid eBay proxy agent,
opposed bidding true value) sniping (i.e., starting bid near end
auction). fact, result sniping behaviour agents, English auctions
run eBay approximated Vickrey auctions since bidders longer much
information status bidding auction. papers, however, focus
sequential auction problem, discuss bidding strategies
multiple auctions close simultaneously. hand, Boutilier, Goldszmidt,
Sabata (1999), Gopal, Thompson, Tung, Whinston (2005), Juda Parkes (2006)
explored means improve efficiency simultaneous auctions either proposing
sequential auction mechanisms use options.4 Whereas works aimed
changing augmenting auction mechanism improve outcome, case
take auctions given focus strategic aspects bidders perspective.

3. Bidding Multiple Vickrey Auctions
model consists sellers, acts auctioneer. seller auctions
one item; items complete substitutes (i.e., equal terms value
bidder obtains additional benefit winning one them). auctions
executed simultaneously; is, end simultaneously information
outcome auctions becomes available bids placed.5 However,
Section 5.3 briefly investigate markets sequential simultaneous auctions
since markets common practice, especially online.
Furthermore, generally assume auctions identical (i.e., bidder
indifferent them), relax assumption Section 5.2 auctions
4. notion similar financial options, auction options provide buyer right,
obligation, buy specific item specified price (the strike price) specified
period time. However, analysis options differs financial perspective due
different assumptions modelling them.
5. note that, although paper focuses sealed-bid auctions, case, conditions
similar last-minute bidding sniping iterative auctions eBay (Roth & Ockenfels,
2002); auctions close almost time, due network delays may
insufficient time obtain results one auction proceeding bid next one.

943

fiGerding, Dash, Byde & Jennings

different valuation distributions and/or numbers participating bidders global
bidder may thus prefer one auction another. Finally, assume free disposal
bidders maximise expected profit. two assumptions, together
assumption complete substitutes implicit throughout paper.
3.1 Auctions
sellers auction implemented second-price sealed bid auction, highest
bidder wins pays second-highest price. format several advantages agentbased settings. Firstly, communication efficient terms number interactions,
since requires bidder place single bid once, auctioneer respond
bidder outcome. contrast, iterative auction English auction
usually requires several interactions typically time consuming. Secondly,
single-auction case (i.e., bidder places bid one auction), optimal
strategy bid true value thus requires computation (once valuation
item known). strategy also weakly dominant (i.e., independent
bidders decisions), therefore requires information preferences
agents (such distribution valuations).
3.2 Global Local Bidders
distinguish global local bidders. former bid number
auctions, whereas latter bid single one. Local bidders assumed bid
according weakly dominant strategy bid true valuation.6 consider two
ways modelling local bidders: static dynamic. first model, number local
bidders assumed known equal n auction. latter model,
hand, average number bidders equal n, exact number unknown
may vary auction. uncertainty modelled using Poisson distribution
(more details provided Section 4.1).
later show, global bidder bids optimally higher expected utility
compared local bidder, even though items complete substitutes bidder
requires one them. Nevertheless, identify number compelling reasons
bidders may choose bid globally:
Participation Costs. Although bidding may automated autonomous
agent, still takes time and/or money, entry fees time setup account,
participate new auction. case, marginal benefits bidding two
auctions instead single auctions less participation costs, buyer
better choosing bidding one auctions.
Information. Bidders may simply aware auctions selling type
item. Even known, however, bidder may sufficient information
distribution valuations bidders number participating bidders.
Whereas information required bidding single auction (because
6. Note that, since bidding true value optimal local bidders irrespective others bidding,
strategy affected presence global bidders.

944

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

dominance property second-price auction), important bidding multiple
simultaneous auctions. information obtained expert user learned
time, often available novice.
Risk Attitude. Although global bidder obtains higher utility average,
bidder runs risk incurring loss (i.e., negative utility) winning multiple
auctions. risk averse bidder may willing take chance, may choose
participate fewer even single auction avoid potential loss. Whether
bidder chooses reduce number auctions single one depends degree
risk aversion. general, would expect agent take less risk bid fewer
auctions stakes higher, i.e., case high-value transactions. global
bidding agent, hand, could representing large firm sufficient funds
cover losses, agents likely risk neutral.
Budget Constraints. Related previous point, bidder may sufficient
funds take loss case wins one auction. detail, fixed
budget b, sum bids exceed b, thereby limiting number auctions
bidder participate and/or lowering actual bids placed auctions
(see also Section 5.1 investigate budget constraints detail).
Bounded Rationality. become clear paper, optimal strategy
global bidder harder compute local one. bidder therefore
bid globally costs computing optimal strategy outweigh benefits
additional utility. Moreover, concept local bidder also captures notion
manual bidder use intelligent bidding agent compute optimal
bid. human bidder clearly easier bid true value single auction
requires calculation (again, given value known).

4. Identical Simultaneous Auctions
section, provide theoretical analysis optimal bidding strategy
global bidder participates identical simultaneous auctions. particular, address
case bidders local simply bid true valuation. However,
analysis general applies setting distribution best
competitive bid known satisfies certain properties. describe global
bidders expected utility Section 4.1, show Section 4.2 always optimal
global bidder participate maximum number simultaneous auctions available.
Subsequently, Section 4.3 significantly reduce complexity finding optimal
bids multi-auction problem case auctions equivalent, apply
methods find optimal strategies specific examples.
4.1 Global Bidders Expected Utility
follows, number sellers (auctions) 2 = {1, . . . , m} denotes
set available auctions. Let G denote cumulative distribution function best
competitive bid particular auction g corresponding density function. Equiva945

fiGerding, Dash, Byde & Jennings

lently, G(b) probability winning specific auction conditional placing bid b
auction. introduce following assumptions regarding function G:
Assumption 1. cumulative distribution G(x) bounded support [0, vmax ], continuous within range, strictly increasing 0 < x < vmax .
global bidder valuation v (0, vmax ] (NB. consider trivial case
v = 0, assume v within bounds G) places global bid b,
vector specifies (possibly different) bid bi [0, vmax ] auction . Now,
given setting described Section 3 identical auctions, expected utility U
global bidder global bid b valuation v given by:
"

U (b, v) = v 1



#

(1 G(bi ))

iM

XZ

iM

bi

yg(y)dy.

(1)

0

Here, left part equation valuation multiplied probability
global bidder wins least one auctions thus corresponds expected
benefit. Q
detail, note (1 G(bi )) probability winning auction
whenQbidding bi , iM (1 G(bi )) probability winning auction, thus
[1 iM (1 G(bi ))] probability winning least one auction. right part
Equation (1) corresponds total expected costs payments. see latter,
expected payment single second-price auction bidding b equals
Rnote
b
0 yg(y)dy independent expected payments auctions.
Now, Equation (1) used address setting bidders except
global bidder local. done follows. Let number local bidders n
1. local bidders valuation v [0, vmax ] independently drawn cumulative
distribution F probability density f , F (x) properties G(x) (i.e.,
support [0, vmax ], continuous within range, strictly positive 0 < x <
vmax ). Note that, since dominant strategy local bidder bid true value,
additional assumptions needed knowledge local bidders regarding
distributions bidders . case local bidders static, i.e., exactly n
local bidders equal distributions, G simply highest-order statistic G(b) = F (b)n ,
g(b) = dG(b)/db = nF (b)n1 f (b). However, use equation model
dynamic local bidders following way:
Lemma 1. replacing highest-order statistic G(y) corresponding density
function g(y) with:
G(y) = en(F (y)1) ,
g(y) = dG(y)/dy = n f (y)en(F (y)1) ,
Equation (1) becomes expected utility number local bidders auction
described Poisson distribution average n (i.e., probability n local
bidders participate given P (n) = nn en /n!).
Proof prove this, first show G() F () modified
number bidders per auction given binomial distribution (where bidders decision
946

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

participate given Bernoulli trial) follows:
G (y) = F (y)N = (1 p + p F (y))N ,

(2)

p probability bidder participates auction, N total
number bidders. see this, note participating equivalent bidding zero.
result, F (0) = 1 p since 1 p probability bidder bids zero
specific auction, F (y) = F (0) + p F (y) since probability p bidder
bids according original distribution F (y). Now, average number participating
bidders given n = p N . replacing p n/N , Equation (2) becomes G (y) =
(1 n/N + (n/N )F (y))N . Note Poisson distribution given limit
binomial distribution. keeping n constant taking limit N , obtain
G (y) = en(F (y)1) = G(y).

results follow apply static dynamic model unless stated otherwise.
4.2 Participation Multiple Auctions
show that, valuation 0 < v < vmax , utility-maximising global bidder
always place non-zero bids available auctions.7 prove this, show
expected utility increases placing arbitrarily small bid compared participating auction. holds even auctions identical. following
let bbj =bj denote global bid b j th bid bj = bj . formally,
Theorem 1. Assumption 1 given global bidder valuation 0 <
v < vmax , consider global bid b bi v . Suppose bj = 0
auction j , Equation (1) maximised, i.e., exists bj > 0
U (bbj =bj , v) > U (b, v).
Proof need show exists bj > 0 U (bbj =bj , v) U (b, v) >
0. Using Equation (1), marginal expected utility participating auction j w.r.t.
bidding zero auction written as:


U (bbj =bj , v) U (b, v) = vG(bj )

(1 G(bi ))

iM \{j}

Now, using integration parts,
equation rewritten as:


U (bbj =bj , v) U (b, v) = G(bj ) v

R bj
0

yg(y) = bj G(bj )



iM \{j}

Z

bj

yg(y)dy.

0

R bj
0



G(y)dy

(1 G(bi )) bj +

Z

bj

G(y)dy.

(3)

0

7. note necessarily hold boundary case v = vmax . However, practice,
find even optimal strategy bid true value multiple auctions instead
true value single auction. especially case number auctions large (see
also Section 4.3.4).

947

fiGerding, Dash, Byde & Jennings

Clearly, since bj > 0 g(x) > 0 x > 0 (due Assumption 1) G(bj )
R bj
positive. Moreover, given bi v < vmax
0 G(y)dy strictly
Q
Q
v > 0, follows v iM \{j} (1 G(bi )) > 0. Now, suppose set bj = 12 v iM \{j} (1
h Q
R
b
G(bi )), U (bbj =bj , v) U (b, v) = G(bj ) 12 v iM \{j} (1 G(bi )) + 0 j G(y)dy > 0

thus SU (bbj =bj , v) > U (b, v).
proof applies setting auctions identical. However, easy
see argument holds G differs auction.
result states that, even though risk winning pay
one item (and buyer disposes additional items won), best strategy
participate auctions. Therefore, expectation, increasing probability winning
single item outweighs possible loss incurred winning one them.
obtain better understanding true, consider following intuitive
argument. Suppose global bidder bids k < auctions. case,
non-zero probability bidder wins none auctions, thus
non-zero expected demand least one items remaining auctions. Since
argument holds k < m, induction global bidder bid auctions.
Note Theorem 1 holds v strictly smaller vmax . case
v = vmax two possibilities: either optimal bid vmax one auction
case bids auctions zero (since bidder guaranteed win),
optimal bid vmax strictly positive auctions. show
Section 4.3.4 find emperically first true number auctions
small, latter case large numbers auctions.
4.3 Optimal Global Bid
general solution optimal global bid requires maximisation Equation (1)
dimensions, arduous task, even applying numerical methods. section,
however, show reduce entire bid space single dimension cases,
thereby significantly simplifying problem hand. First, however, order find
optimal solutions Equation (1), set partial derivatives zero:



U
(1 G(bj )) bi = 0.
(4)
= g(bi ) v
bi
jM \{i}

Q
Now, equality (4) holds either g(bi ) = 0 bj b\{bi } (1 G(bj ))v bi = 0.
model dynamic local bidders, g(bi ) always greater zero, therefore
ignored (since g(0) = nf (0)en assume f (y) > 0). cases, g(bi ) = 0
bi = 0. However, Theorem 1 shows optimal bid non-zero 0 < v < vmax .
Therefore, ignore first part, second part yields:
bi = v



(1 G(bj )).

(5)

jM \{i}

words, optimal bid auction equal bidders valuation multiplied
probability winning auctions. straightforward show
948

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

second partial derivative negative, confirming solution indeed maximum
keeping bids constant. Moreover, since optimal bid requires 0 < bi <
vmax v < vmax (due Theorem 1 since bidding valuation clearly
suboptimal), need consider interior solutions. Thus, Equation (5) provides
means derive optimal bid auction i, given bids auctions.
Now, taking partial derivatives auction rewriting Equation (5),
optimal global bid must obey following relationship: b1 (1 G(b1 )) = b2 (1 G(b2 )) =
. . . = bm (1 G(bm )). defining H(b) = b(1 G(b)) rewrite equation to:

H(b1 ) = H(b2 ) = . . . = H(bm ) = v
(1 G(bj )).
(6)
jM

follows, apply Equation (6) reduce search space, first show that,
large class probability distributions, optimal global bid consists two
different values, thereby reducing search space two dimensions. Section 4.3.2
reduce single dimension. Section 4.3.3 consider limit results
number auctions goes infinity. Finally, Section 4.3.4 perform numerical
analysis optimal bidding strategy specific cases number auctions
finite, consider extent global bidder benefits compared bidding locally.
4.3.1 Reducing Search Space
section, first show optimal global bids consists two different
values function H(b) = b(1 G(b)) unique critical point. formally,
introduce following requirement:
Assumption 2. H(b) = b(1 G(b)) unique critical point bf , i.e., exists bf


s.t. db
H(bf ) = 0 b 6= bf , db
H(b) 6= 0.
go show requirement met wide class distributions
characterised non-decreasing hazard rate. Let two bid values denoted b
b+ , b b+ . Formally:
Theorem 2. Assumptions 1 2, global bid b maximising Equation (1)
contains two distinct bid values: b b+ . addition, b bf b+ , bf
unique critical point H(b) = b(1 G(b)).
Proof Suppose H unique critical point, bf . H(0) = H(vmax ) = 0, H(b) > 0
(0, vmax ), bf must fact global maximum H. Furthermore, b < bf
H strictly increasing b > bf H strictly decreasing. also implies
H(x) = two solutions: > H(bf ) solutions, since bf
global maximiser H; = H(bf ) unique solution, namely bf ; < H(bf )
applying intermediate value theorem H interval [0, bf ] gives one solution,
namely b bf , interval [bf , vmax ] gives other, namely b+ bf .
one solution interval H strictly monotonic each.
Now, Equation (6) implies H(bi ) equal . Therefore, given
H(x) = two solutions, two distinct interior bids bi ,
949

fiGerding, Dash, Byde & Jennings

namely b b+ . mentioned before, Theorem 1 utility maximizing solution
0 < bi < vmax v < vmax , therefore solutions interior ones. case
v = vmax either b+ = vmax b = 0, b+ < vmax
b > 0.

show unique critical point H guaranteed G non-increasing
hazard rate within interval [0, vmax ]. choose property since encompasses
large number distributions, including log-concave density functions
uniform, normal exponential (we refer work Barlow, Marshall, Proschan,
1993 Bergstrom Bagnoli, 2005 list functions). Formally, hazard rate
(see e.g., Krishna, 2002) cumulative distribution function F denoted F
defined by:
F (x)

f (x)
.
1 F (x)

following result:
Lemma 2. Assumption 1, G (b) non-decreasing (0, vmax ), function
H(b) = b(1 G(b)) unique critical point bf interval.
Proof See Appendix A.1.
extend proof distribution functions local bidders F showing
that, hazard rate F non-decreasing, hazard rate G also non-decreasing,
thus reduction also applies. holds local static bidders. Formally:
Theorem 3. F (b) non-decreasing, G (b) Gb (b) also non-decreasing,
b = en(F (y)1) n 2.
G(b) = F (b)n G

Proof See Appendix A.1.

4.3.2 Characterising Optimal Bids
Using results able reduce optimal global bid two values, high
bid, b+ bf , low bid, b bf . However, know number auctions
bid high low. section show optimal bid high
one auction, low auctions. Using result, write
high bid terms low bids, reducing search space even further.
Theorem 4. Assumptions 1 2, global bid b maximises Equation 1
one high bid, i.e., one bi > bf , bf unique
critical point H(b) = b(1 G(b)).
Proof See Appendix A.1.
950

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

Together Lemma 2 implies that, distributions non-decreasing hazard
rates, either exists one high bid 1 low bids, bids low. Note
calculate value high bid analytically given low-bid value using
Equation (5). Consequently, finding optimal global bid reduces optimising single
variable (i.e., value low bids). value computed numerically
using standard optimisation techniques Quasi-Newton method, or, alternatively,
bids discretised brute-force search applied find optimum.
Whatever method selected important notice that, due reduction
search space, computational complexity calculating optimal outcome numerically
independent number auctions (or indeed number bidders).
result Theorem 4 suggests optimal restrict attention single
auction bidding high auction, using remaining auctions backup
case high-bid auction fails. show Section 4.3.4 often case
practise number auctions small. number auctions large,
however, find optimal bid low auctions, irrespective
bidders valuation. derive theoretical results limit case number
auctions goes infinity Section 4.3.3, consider empirical results finite
case Section 4.3.4.
4.3.3 Limit Results
section investigate optimal bidding changes number auctions
becomes large consider whether general patterns characterise
optimal strategy. first, basic result that, number auctions increases,
agent able extract increasingly greater utility approaches maximum
possible utility, v. prove this, without loss generality restrict strategy
global bidder considering uniform bidding (i.e., bids equal). Let bu

denote optimal global bid bidding auctions bidder confined
using uniform bids:
Theorem 5. Assumption 1, expected utility defined Equation 1
playing optimal uniform bid bu
converges v, sense > 0
constant > implies U (bu
) > v .
Proof See Appendix A.2.

Note that, since v upper bound utility achieved global
bidder, result implies that, number auctions increases, eventually uniform
bidding always superior non-uniform bidding. formally:
Corollary 1. Assumption 1, sufficiently large globally optimal bid b
maximising Equation (1) equal optimal uniform bid bu
, independent v.

951

fiGerding, Dash, Byde & Jennings

R bf
Proof corollary follows Theorem 5 = EP (bf ) = 0 xg(x) dx: >
uniform bidding gives utility least v , whereas non-uniform bidding gives strictly less:
U (b ) < v EP (b+ ) < v EP (bf ) = v .

practice, find necessary number auctions particularly
large uniform bidding optimal v. end, next section provide
examples optimal bidding strategy specific settings.
4.3.4 Empirical Evaluation
section, present results empirical study characterise optimal
global bid specific cases finite numbers auctions. Furthermore, measure
actual utility improvement obtained using global strategy.
results presented based uniform distribution valuations vmax = 1,
static local bidder model, generalise dynamic model
distributions. Figure 1 illustrates optimal global bids corresponding expected
utility various n = 5, bid curves different values n
follow similar pattern.
shown Figure 1, bidders relatively low valuation, optimal strategy
submit equal bids at, close to, true value. valuation reaches
certain point, however, placing equal bids longer optimal strategy
number auctions small (in Figure 1 occurs = 4 = 6). point,
so-called pitchfork bifurcation observed optimal bids split two values:
single high bid 1 low ones. experiments, however, consistently observe
optimal strategy always place uniform bids valuation relatively
low. Moreover, bifurcation point moves right increases, disappears
altogether becomes sufficiently large (m 10 Figure 1) point
optimal bids uniform (note holds even v = vmax ). Note also
uniform optimal bids move closer zero tends infinity.
illustrated Figure 1, utility global bidder becomes progressively higher
auctions. Note that, consistent limit results Section 4.3.3,
utility approaches upper bound v number auctions becomes large. absolute
terms, improvement especially high bidders average valuation,
close vmax . bidders range thus benefit bidding globally.
bidders low valuations small chance winning
auction, whereas bidders high valuation high probability winning
single auction benefit less participating auctions. contrast, shown
Figure 1, consider utility relative bidding single auction, much higher
bidders relatively low valuations. particular, notice global bidder
low valuation improve utility times expected utility bidding
locally. Intuitively, chance winning one auctions increases
factor m, whereas increase expected cost negligible. high valuation
buyers, however, benefit obvious chances winning relatively
high even case single auction.
952

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

1

0.8

bid

0.6

m=1
m=4
m=6
= 10
= 102
= 106
= 1010

0.4

0.2

0
0

0.2

0.4

0.6

0.8

1

0.8

1

valuation (v)
1

expected utility

0.8

0.6

0.4

m=1
m=4
m=6
= 10
= 102
= 106
= 1010


0.2

0
0

0.2

0.4

0.6

valuation (v)
10

m=1
m=4
m=6
= 10

relative utility

8

6

4

2

0
0

0.2

0.4

0.6

0.8

1

valuation (v)

Figure 1: optimal global bid, corresponding expected utility, expected
utility proportional local bidder setting n = 5 static local
bidders varying number auctions (m). Note results = 1
correspond local bidder. comparison, expected utility also
shown number auctions approaches infinity.
953

fiGerding, Dash, Byde & Jennings

5. Extensions: Budgets, Non-Identical Sequential Auctions
previous section, considered best response strategy global bidder faces
multiple identical simultaneous auctions financial constraints. section,
generalise results settings. particular, investigate three important
extensions:
1. Section 5.1 investigate setting global bidder limited budget
constrains sum bids exposure.
2. Section 5.2 investigate setting auctions differ probability
winning. differences arise, example, auctions different numbers
local bidders participating.
3. Section 5.3 extend results sequential auctions consider setting
resources interest auctioned simultaneously sequentially.
5.1 Budget-Constrained Bidding
derivation optimal global bid shown optimal strategy
global bidder bid simultaneous auctions. Now, strategy implicitly
assumes bidder face financial constraints (i.e., bidder pay
items won). However, often bidder limited resources may restrict bidding
strategy. section study budget limit space possible strategies
available global bidder affect optimal strategy. particular, consider
case budget constrains exposure, i.e., sum bids.8 occurs,
example, global bidder limited liquidity faces negative consequences
cannot pay items wins (e.g., going bankrupt thrown
system).
Now, importance taking budget constraints account becomes even
pronounced following result shows that, number auctions increases,
required budget exposure unconstrained case exceed given limit:9
Theorem 6. Assumption 1 assuming g(b) bounded throughout [0, vmax ],
v > 0 C, exists forP
exposure optimal global

bid b maximising Equation (1) exceeds C, i.e., iM bi > C.
Proof See Appendix A.3.

Thus, despite low probability bidder pay sum bids (especially,
given auction second-price auction), practice bidder may still want
limit amount.
8. note that, although practice budget constraint payments rather bids, even
second-price auctions worst-case outcome bidder pays bids auctions,
therefore hard budget constraint equivalent constraining sum bids.
9. Note occurs despite fact bids tend zero goes infinity. However, exposure
(see proof Theorem 5).

954

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

detail, formulate budget-constrained problem faced bidder
as:
max U (b, v)

s.t.

b[0,v]m

X

bi C,

(7)

iM

C budget limit U (b, v) utility global bidder given
Equation (1). consider budget constraints distinguishing three cases.
following, b = (b1 , . . . , bm ) refers unconstrained optimal solution (i.e., optimal
c
solution absence budget constraints), whereas bc = (bc
1 , . . . , bm ) refers
optimal global bid subject budget constraints.
P

Case (1)
iM bi > C v C.

Here, sum unconstrained optimal bids exceeds budget
therefore required recompute optimal bid given constraint. Moreover,
budget constraint equal less valuation. case,
able show best strategy bid single auction place bid
bi = C auction fairly general probability density functions g.
doing, provides one justifications existence local bidders (as
outlined Section 3.2). Although result intuitive, straightforward
since bidder may still decide divide budget across several auctions.
provide proof result.
P

Case (2)
iM bi > C v < C.

previous case, need recompute optimal bid. However,
show, optimal strategy global bidder case bid multiple
auctions (as unconstrained case). Moreover, contrast unconstrained
case, global bid may consist two different values.
P

Case (3)
iM bi C.
case, sum unconstrained optimal bids less budget.
Thus result trivial b = bc .

Examples three different cases depicted Figure 2. figure shows
optimal strategy bidding 4 simultaneous auctions local bidders uniformly
distributed valuations within range [0, 1] global bidder budget constraint
C = 0.8.10 Clearly, global bidder low valuation budget constraint
affect optimal bidding strategy (case 3). Case 1 occurs bidder valuation
0.8, case 2 occurs in-between range. figure shows, optimal
strategy latter two cases qualitatively different unconstrained case;
whereas unconstrained optimal strategy bid high single auction,
consists single low bid, several high bids, placing zero remaining
auctions. budget becomes tighter relative valuation, number auctions
bidder participates decreases single auction remains (see Figure 2, case
1). follows, first consider conditions behaviour optimal
10. Similar patterns observed optimal strategy varying number auctions budget.
patterns always grouped three cases.

955

fiGerding, Dash, Byde & Jennings

1

0.8

1

bid

0.6

case (1)
0.4

2

case (3)

3

0.2

case (2)
4

0
0

1
0.2

1
0.4

1
0.6

0.8

1

valuation (v)
Figure 2: solid lines denote optimal bids global bidder budget C = 0.8
setting n=5 m=4. Here, numbers indicate number
auctions certain bid placed. valuation increases (and
budget remains constant), bids auctions taper one one,
single high bid remains bids zero. dotted line represents
unconstrained solution.

case 1, subsequently address case 2 detail. Case 3 trivial therefore
considered further.
provide formal result optimal strategy case 1 show that,
budget constraint imposed global bidder equal less value
attaches item wishes acquire, best strategy act local bidder
following conditions:
Assumption 3. probability density function best competitive bid g(x) convex
g(0) = 0.
result stated formally follows:
Theorem 7. Assumptions 1 3, global bidder budget C v,
Equation (7) satisfied bc = (C, 0, . . . , 0), i.e., optimal bid C exactly one
auction.
Proof See Appendix A.3.
Intuitively, convexity g(x) necessary ensure probability winning
auction increases sufficiently quickly bid increases. way, higher bid
956

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

single auction results higher probability winning compared dividing
amount several auctions. Although placing higher bid single auction may lead
higher expected payment, proof shows utility obtained increased
probability winning outweighs expected payment increase. time,
condition g(0) = 0 important, otherwise probability winning increases sufficiently
quickly optimal spread budget multiple auctions. importance
two conditions becomes apparent following corollary shows that,
special case C = v, condition g(0) = 0 fact necessary condition
bidding single auction optimal, and, furthermore, strategy longer
optimal case g(x) concave.
Corollary 2. Assumption 1, either g(0) > 0 g(x) strictly concave,
global bidder budget C = v, Equation (7) satisfied bidding strictly positive
least two auctions.
Proof See Appendix A.3.
Note that, case static local bidders, condition g(x) convex holds,
example, f (x) convex, increasing, non-negative (and thus F (x) also convex),
since g(x) = nF n1 (x)f (x). However, condition holds general cases
well; especially number local bidders high, concave local bidder distribution
easily result convex g(x). Also note condition g(0) = 0 holds case
static local bidders n 2, even f (0) > 0 (but case dynamic local
bidders unless f (0) = 0). Finally, note convexity g(x) implies non-decreasing
hazard rate therefore conditions stronger imposed Assumption 2
Section 4.3.
move consider cases 2 3. Case 2 cannot analysed easily
case 1. However, good insight constrained optimal strategy obtained
considering Lagrangian Equation (7). Since budget constraint inequality
constraint, addition regular Langrangian multiplier need introduce slack
variable convert inequality equality. variable first P
squared ensure
positive value added constraint becomes C iM bi + 2 = 0.
Langrange function becomes follows:
!
X
(B, , ) = U (b, v) +
bi C + 2 .
(8)
iM

setting partial derivatives zero, results following + 2 equations
solved:
X
(.)
=
bi C + 2 = 0,

iM

(.)
= 2 = 0,

(.)
U (b, v)
=
+ = 0 M.
bi
bi
957

(9)

fiGerding, Dash, Byde & Jennings

2.5

1

2

sum bids

0.8

Case 2

Constrained
Unconstrained

bid

0.6

0.4

=0

1

0.5

0.2

0
0

1.5

0.2

0.4

0.6

0.8

1

0
0

0.2

0.4

0.6

0.8

1

valuation (v)

valuation (v)
(a) Optimal bids

(b) Sum bids

Figure 3: optimal global bid corresponding sum bids unconstrained case case budget constraint C = 1.5. Here, n=10
m=3.

equations, readily observed case 1, = 0 thereby leading

P case solution Equation (9) forces total budget spent, i.e.,
iM bi = C.

case 2, either = 0 corresponding local maximum Equation (4),
total budget spent = 0 whereby total budget spent. two
possible situations highlighted Figure 3, showing optimal global bidding strategy
n = 10, = 3 budget C = 1.5, corresponding sum bids (note
case 1 arise since budget exceeds highest valuation). unconstrained
solution also provided comparison. example shows total amount spent
necessarily equal available budget, even unconstrained optimal solution
exceeds budget. Thus, case 2 cannot solely consider solutions whereby sum
bids equal budget since may case bidding less budget
yields greater utility (i.e., = 0).
Furthermore, observed Figures 2 3(a), introduction budget
constraint changes shape optimal strategy cases 1 2. Recall Section 4.3 optimal strategy unconstrained case either bid equally
auctions bid high one auction low equally remaining ones.
However, budget constraint, region best strategy
bid high one auction low remaining ones. Moreover, parts
solution consist three different bids: high, low, zero. Hence, Equation (5)
longer holds case budget constraints, structure bids observed Section 4.3
need satisfied. means cannot apply reduction search space
efficiently compute optimal strategy case budget constraints case 2,
complexity computing optimum using brute force increases exponentially
number auctions.
958

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

5.2 Non-Identical Auctions
Whereas previously assumed auctions equivalent, relax assumption
general case non-identical auctions. Here, assume auctions
differ global bidders valuation item sold, rather probability obtaining item given bid. differences arise, example,
number bidders and/or local bidders valuation distribution vary one auction
another. used practice specific information individual
auctions available.11 detail, assume auction individual
cumulative distribution function, denoted Gi (b), corresponding density function
gi (b). Now, expected utility given by:
"
#

X Z bi
ygi (y)dy,
(10)
U (b, v) = v 1
(1 Gi (bi ))
iM

iM

0

b = (b1 , . . . , bm ) global bid, specifying bid auction before.
easy see Theorem 1 Section 4.2 extends setting non-identical
auctions optimal strategy bid strictly positive amount auction,
provided Gi (b) strictly positive M, 0 < b vmax . But, auctions
non-identical complicates bidding space. revisit search space problem
non-identical auctions Section 5.2.1 different method employed reduce
bidding space. Section 5.2.2, show global bidder always bid higher
preferred auction. optimal bidding strategy number non-identical
auctions empirically evaluated Section 5.2.3.
5.2.1 Calculating Optimal Global Bid
Although reduction search space described Section 4.3 longer possible
auctions identical, still significantly reduce computation needed
find optimal bid. end, first set partial derivatives U/bi zero
. Given gi (bi ) > 0 have:

bi = v
(1 Gk (bk )).
(11)
kM \{i}

combining partial derivatives, obtain following relationship:

(1 Gk (bk )).
b1 (1 G1 (b1 )) = b2 (1 G2 (b2 )) = . . . = bm (1 Gm (bm )) = v

(12)

kM

Generally, possible find optimal global bid analytically. Using expression (12),
however, confine search one bids then, value
bid, determine values 2 bids computationally less demanding onedimensional root-finding operations (see e.g., Burden & Faires, 2004). remaining bid
value found analytically using Equation (11).
11. example, eBay auctions reveal number visits web pages particular items,
used estimate number participating bidders individual auctions (see http://www.ebay.com
examples).

959

fiGerding, Dash, Byde & Jennings

clarify reduction search space, show applied discrete
bid space using brute-force search approach. find optimal (discrete) global
bid, iterate discrete space one bids, say b1 . value b1
find corresponding m1 bid values expression (12) satisfied follows.
first calculate b1 (1 G1 (b1 )) search discrete values b2
b1 (1 G1 (b1 )) = b2 (1 G2 (b2 )).12 Typically least two values b2
equality holds.13 solutions stored memory repeated
bi < bm . value bm calculated using Equation (11). way
calculate expected utility given b1 expression (12) satisfied. optimal
solution found maximising expected utility across possible values b1
combinations solutions stored memory. Note amount
computation required find expected utility single value b1 increases linearly
number auctions. However, number combinations increases exponentially
number auctions. Nevertheless, since base exponential typically
2 (in particular case non-decreasing hazard rates, see Footnote 13), computation
remains tractable number auctions relatively small becomes intractable
large settings.14 specific cases, however, one auction clearly better
another auction precise sense, possible reduce number combinations
thus required computation becomes linear number auctions. issue
addressed next section.
5.2.2 Preferred Auctions Optimal Bids
many cases, possible find auctions favourable others
terms expected utility. example, else equal, bidder expected
better average auctions fewer bidders. bidder would therefore prefer
auctions less profitable ones, irrespective bidders valuation.
section, formalise notion preferred auction, investigate optimal bids
bidding multiple auctions respect preferences auctions.
use concept stochastic orders (Shaked & Shanthikumar, 1994) rank
auctions terms global bidders preferences. Formally, auction j stochastically dominates auction Gj (b) Gi (b) 0 b vmax (Krishna, 2002, Appendix B).
expected utility least high bidding auction compared
bidding amount auction j. shown
R bas follows. write expected
utility U auction i/j as: Ui/j = (v b)Gi/j (b) + 0 Gi/j (x)dx. Since Gi (b) Gj (b)
Rb
Rb
0 Gi (x)dx 0 Gj (x)dx auction j dominates auction i, expected utility
bidding auction least high auction j. therefore refer auction
12. case discrete bids equality rarely holds exactly resolved minimising
difference instead, i.e., minimising b1 (1 G1 (b1 )) b2 (1 G2 (b2 )).
13. precisely, function b (1 G(b)) single critical value, two
solutions. shown earlier Lemma 2 case G non-decreasing hazard rate. Note
that, case discrete bids, solutions local minima taking difference.
14. provide idea means practice, implemented brute-force search Java
finds optimal global bid 100 discrete valuations and, so, searches 100 bids per
valuation per auction. Using settings, 1.66GHz Intel Centrino optimal solution
found within 4 seconds = 10, takes 35 seconds = 15.

960

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

(weakly) preferred auction. Furthermore, say auction strictly preferred
auction j b Gi (b) > Gj (b).
bidding multiple auctions, show optimal bid higher
preferred auctions. Intuitively, because, case second-price auctions, preferred
auctions give agent higher probability winning good lower price.
observation reduce computation required since bid values higher
bids less preferred auctions need considered. practice usually
reduces number solutions auction satisfy expression (12) single
one (in case computation optimal global bid becomes linear number
auctions). addition computational benefit, observation also provides
guidelines intuition strategies. relationship preferred auctions
optimal global bid precisely described follows:
Theorem 8. Assumption 1, i, j M, 6= j: auction (weakly) preferred
auction j, i.e., Gi (b) Gj (b) 0 b vmax , auction strictly preferred
bi bj , bi bj maximise Equation (10), bi > bj v > 0.
Proof first prove bi bj contradiction, go show bi 6= bj .
Suppose opposite holds exist i, j bi , bj b optimal
bi < bj . show expected utility strictly higher interchanging
two bids, i.e., bidding bi auction j bj auction i. Let bbi bj denote global
bid b two bids bi , bj interchanged. show U (bbi bj , v) U (b, v)
strictly positive 0 v vmax . Using Equation (10) itegration parts obtain
following:
U (bbi bj , v) U (b, v) = (v c bi )(Gj (bi ) Gi (bi )) + (v c bj )(Gi (bj ) Gj (bj ))
Z b
j
Gi (y) Gj (y)dy, (13)
v c Gj (bi )Gi (bj ) + v c Gj (bj )Gi (bi ) +
bi

Q
c = kM \{i,j} (1 Gk (bk )). First note c > 0 strict preference
auctions bi bj requires 0 < bi < vmax ,0 < bj < vmax (since Gi (0) = Gj (0) =
Gi (vmax ) = Gj (vmax ) = 0), therefore due Theorem 7 holds 0 < bk < vmax
k \{i, j}. Next, note last term Equation (13) always positive since
bj > bi Gi (b) Gj (b). therefore ignore term remaining part also
positive. Let term denoted following. Since assumed bi bj
optimal, Equation (11) following holds:
bi = v c (1 Gj (bj )) v c bi = v c Gj (bj )),
bj = v c (1 Gi (bi )) v c bj = v c Gi (bi )).

(14)

replacing v c bi v c bj Equation (13) v c Gj (bj ) v c Gi (bi ) respectively,
rearranging terms, obtain following:
U (bbi bj , v) U (b, v) = v c (Gi (bi ) Gj (bi ))(Gi (bj ) Gj (bj )) + .
961

(15)

fiGerding, Dash, Byde & Jennings

1
0.8
bid fraction x = b/v

bid fraction x = b/v

1
0.8
n1
n1
n1
n1
n1

0.6
0.4
0.6

= 6, n2 = 7
= 6, n2 = 8
= 6, n2 = 9
= 6, n2 = 15
= n2 = 6

0.7

0.8

0.6
0.4
0.2
0
0

0.9

valuation (v)

n = {5, 6, 7, 8, 9, 10, 11}
n = {5, 5, 5, 7, 7, 7, 7}

0.5

1

valuation (v)

(a) 2 auctions

(b) 7 auctions

Figure 4: Optimal global bid fraction valuation (a) two (b) seven simultaneous auctions various settings. auctions differ number local
bidders present auction.

Now, since Gi (bi ) > Gj (bi ), Gi (bj ) > Gj (bj ), v > 0, c > 0, 0, follows
U (bbi bj , v) > U (b, v). result, bi bj cannot optimal bids auctions j
respectively, contradicts initial proposition.
proves bi bj . show bi must strictly higher

bj . follows directly Equation (14). Suppose bi = bj , Equation (14)
follows Gi (bi ) = Gj (bi ) Gi (bj ) = Gj (bj ). However, conflicts
requirement auction must strictly preferred bi bj , thereby completing
proof.

proof based requirement auction strictly preferred
auction j least two optimal bids auctions. Although requires
optimal bid known advance, proof also applies cases auction strictly
preferred auction j entire range 0 < b < vmax . case, example,
number bidders auction strictly less auction j. Furthermore,
note similar results hold slightly different conditions. Specifically, auction
strictly preferred set points non-zero measure anywhere within range
[bj , bi ], bi bj since > 0 Equation (14). Consequently, condition bi bj
holds auction preferred auction j Gi , Gj described analytic
functions.
5.2.3 Empirical Evaluation
section, examine bidding strategy global bidder affected
auctions different numbers local bidders. end, find optimal bids
maximise utility given Equation (10) using standard optimisation technique (Press,
Flannery, Teukolsky, & Vetterling, 1992). numerical results shown assume
962

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

bidder valuation distribution uniform, although similar results obtained
commonly used distributions. Two different scenarios evaluated. first,
investigate optimal bids change increasing differences local bidder
numbers case two simultaneous auctions (see Figure 4(a)). Here, number
local bidders first auction (n1 ) fixed 6 whereas number local bidders
second auction (n2 ) varied 6 15 second (note depict
optimal strategy fraction valuation (bi /v) rather actual bid value since
clearly demonstrates effect preferred auctions). Figure 4(a) shows
global bidder bids higher preferred auction (i.e., one fewer bidders),
consistent Lemma 8. valuation increases, bid preferred auction first
decreases relative valuation, increases, similar case
number local bidders. However, higher bid much closer true valuation,
especially difference auctions large.
simulation extended auctions second scenario. results
two different settings shown Figure 4(b). one setting (solid lines),
seven auctions different number local bidders. settings (dashed lines)
results shown 3 auctions 5 local bidders 4 auctions
7 local bidders. before, observe global bidder always bids higher
auctions fewer local bidders. Although appearance bifurcation point
common simultaneous auctions number bidders, case
number bidders identical. global bidder always bids
different amounts auctions different numbers bidders. result, bifurcation
phenomenon indicates transition equal bids high-low bids occur.
5.3 Sequential Simultaneous Auctions
section extend analysis optimal bidding strategy sequential auctions.
Specifically, auction process consists R rounds, round number
auctions running simultaneously. combination sequential simultaneous
auctions common practice, especially online.15 turns analysis
case simultaneous auctions easily extended include sequential auctions.
following, set simultaneous auctions round r denoted r , vector
bids round br . before, analysis assumes bidders
local bid single auction. initially assume global bidders complete
knowledge number rounds number auctions round
relax assumptions.
expected utility round r, denoted U r , similar (Equation (1)
Section 4.1) except additional benefit obtained future auctions
desired item one current set simultaneous auctions. convenience,
U r (br , r ) abbreviated U r following. expected utility thus becomes:
15. Rather purely sequential nature, online auctions also often overlap (i.e., new auctions
start others still ongoing). case, however, optimal wait bid
new auctions outcome earlier auctions known, thereby reducing chance
unwittingly winning multiple items. Using strategy, overlapping auctions effectively become
sequential thus analysed using results section.

963

fiGerding, Dash, Byde & Jennings

r

r

r

U = v P (b )

X Z

iM r

=U

r+1

+ (v U

r+1

bri



yg(y)dy + U r+1 (1 P r (br ))

0

r

r

)P (b )

X Z

iM r

bri



yg(y)dy ,
0

(16)

Q
P r (br ) = 1 iM r (1 G(bri )) probability winning least one auction
round r. Now, take partial derivative Equation (16) order find optimal
bid brj auction j round r:


r

U
(1 G(bri )) brj .
(17)
= g(brj ) (v U r+1 )
brj
r
iM \{j}

Note Equation (17) almost identical Equation (4) Section 4.3, except
valuation v replaced v U r+1 . optimal bidding strategy thus
found backward induction (where U R+1 = 0) using procedure outlined Section 4.3.
Now, first relax assumption global bidder complete knowledge
number auctions future rounds. Let p(m) denote probability
auctions next round let mmax denote maximum number auctions. Furthermore, let Mj set j auctions U r (br , Mj ) expected utility round r
j auctions round. uncertainty number
inPmmaxof auctions
r+1
r+1
corporated Equations (16) (17) replacing U
j=0 p(j)U
(br+1 , Mj ).
Furthermore, uncertainty number rounds addressed adding discount factor 0 < 1 represents probability auctions
round. Finally, note equation similarly extended non-identical
auctions settings G depends number auctions and/or round.

6. Market Efficiency
Efficiency important system-wide property since characterises extent
market maximises social welfare (i.e., sum utilities agents market).
end, section study efficiency markets either static dynamic
local bidders, impact global bidder efficiency markets.
Specifically, efficiency context maximised bidders highest
valuations (recall denotes number auctions, also total number
items) entire market obtain single item each. simplicity assume
total number bidders market least m. formally, define efficiency
allocation as:
Definition 1. Efficiency Allocation. efficiency K allocation K
obtained social welfare proportional maximum social welfare achieved
type market given by:
PnT
vi (K)
K = Pni=1
,
(18)


i=1 vi (K )
964

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

P
K = arg maxKK ni=1
vi (K) efficient allocation, K set possible
allocations (assuming bidders allocated auction), vi (K) bidder utility
allocation K K, nT total number bidders participating market.

Now, order measure efficiency market impact global bidder,
run simulations markets without global bidder different
types local bidders. experiments carried follows. bidders valuation
(both local global) independently drawn uniform distribution support
[0, 1]. experiments without global bidder, additional local bidder placed one
auctions overall number bidders average compared
case global bidder. local bidders bid true valuations, whereas
global bidder bids optimally auction described Section 4.3. experiments
repeated 10000 times order get statistically significant results 99% confidence
interval.
results experiments shown Figure 5. Note degree inefficiency
inherent multi-auction market local bidders.16 example,
two auctions selling one item each, two bidders highest valuations
bid locally auction, bidder second-highest value
obtain good. Thus, allocation items bidders inefficient, inefficiency
increases number auctions increases (keeping average number bidders per
auction equal). observed Figure 5, however, efficiency increases
n becomes larger. differences bidders highest
valuations become smaller, thereby decreasing loss efficiency.
Furthermore, Figure 5 shows one local bidders replaced global
bidder generally creates positive effect efficiency number bidders
small, significant change occurs many local bidders (this
holds static dynamic local bidders). latter comes surprise since
impact single bidder diminishes bidders competing auction.
former, hand, obvious; introduction global bidder potentially
leads decrease efficiency since bidder unwittingly win one item.
Furthermore, global bidder generally bid true valuation also result
inefficient outcomes. However, results show that, average, opposite occurs.
because, global bidder local bidders, high
probability local bidder low valuation win item. global bidder,
hand, likely win auction sufficiently high valuation (even
though global bids true valuation, shown Section 4.3.4 bids
often uniform fairly close true value). effect particularly pronounced
case dynamic local bidders since may occur auction local bidder
whatsoever, case global bidder wins item sure.
16. exception n = 1 bidders static, since market completely efficient without
global bidder. However, since special case apply settings,
discuss here.

965

fiGerding, Dash, Byde & Jennings

efficiency (K )

1

0.95

2
1
4
6

0.9



5
3

1
2
3
4
5
6
7
8

8

0.85
7

0.8
2

Local
Bidders

Global
Bidder

2
2
2
2

Static
Static
Dynamic
Dynamic


Yes

Yes

6
6
6
6

Static
Static
Dynamic
Dynamic


Yes

Yes

4
6
8
10
12
(average) number local bidders (n)

Figure 5: Average efficiency different market settings shown legend,
number auctions (items), n average number local bidders
per auction. error-bars indicate 99% confidence intervals.

7. Conclusions
paper, derive utility-maximising strategies agent bid multiple,
simultaneous second-price auctions. first analyse case single global bidder
bids auctions, whereas bidders local bid single auction.
setting, find optimal place non-zero bids auctions sell desired
item, even bidder requires single item derives additional benefit
more. Thus, potential buyer achieve considerable benefit participating
multiple auctions employing optimal bidding strategy.
common valuation distributions, show analytically optimal bids
identical auctions consist two values, high bid low bid,
optimal bid high one auction. Moreover, writing high bid terms
low ones, problem finding optimal bids reduces optimising single
variable. considerably simplifies original optimisation problem thus
used practice compute optimal bids number auctions. Furthermore,
show that, number auctions becomes large, optimal bid uniformly across
auctions. also analyse setting auctions identical differ
probability winning. Although still optimal participate auctions, find
best strategy bid relatively high favourable auctions (i.e.,
probability winning highest). investigate practical considerations well.
show budget constraints limit number auctions bidders participate in.
Specifically, global bidders budget equal less valuation, optimal
strategy reverts bidding single auction certain conditions, thereby justifying
966

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

presence local bidders. Furthermore, consider sequential auctions show
results readily applied markets auctions occur sequentially
simultaneously. Finally, compare efficiency market multiple simultaneous
auctions without global bidder. show that, local bidder replaced
global one, increases average efficiency thus social welfare sysstem
number bidders small, significant effect found many local
bidders.
number interesting directions future work. First all, whereas
paper focuses buyer, intend extend analysis consider revenue
sellers point view. particularly relevant sellers proactive role set auction parameters reserve price try maximise
expected revenue. closely related research competing sellers,
shown optimal auction parameters depend competition
sellers, since affects number potential buyers attracted particular
auction (McAfee, 1993; Peters & Severinov, 1997; Burguet & Sakovics, 1999; Gerding et al.,
2007b). However, current literature competing sellers assumes buyers
participate single auction, shown suboptimal. case
sellers optimise auction parameters buyers participate number auctions
simultaneously far investigated. Furthermore, paper taken
decision-theoretic approach analysing case single global bidder. interesting
open problem characterising game-theoretic solution case multiple global
bidders interact strategically.
Acknowledgments
paper significantly extended preliminary version published previously (Gerding, Dash, Yuen, & Jennings, 2007a).
research undertaken part EPSRC (Engineering Physical Research
Council) funded project Market-Based Control (GR/T10664/01). collaborative
project involving Universities Birmingham, Liverpool Southampton BAE
Systems, BT HP. research also undertaken part ALADDIN (Autonomous Learning Agents Decentralised Data Information Systems) project
jointly funded BAE Systems EPSRC strategic partnership. addition, would
like thank Alex Rogers, Ioannis Vetsikas, Wenming Bian, Florin Constantin
input. authors also grateful Adam Prugel-Bennett valuable help
proofs, Richard Engelbrecht-Wiggans useful discussions
comments. Finally, would like thank reviewers thorough detailed
feedback.

Appendix A. Proofs
A.1 Reduction Search Space
Lemma 2. Assumption 1, G (b) non-decreasing (0, vmax ), function
H(b) = b(1 G(b)) unique critical point bf interval.
967

fiGerding, Dash, Byde & Jennings

Proof critical point H following equation must hold:


H(b) =
[b(1 G(b))] = 1 bg(b) G(b)
db
db



1 G(b)
1
= g(b)
b = g(b)
b = 0.
g(b)
G (b)

(19)

Now, since g(b) > 0 (due Assumption 1) , equality (19) holds 1/G (b) b = 0.
Therefore, order show (19) one solution, sufficient show
1/G (b) b either strictly increasing strictly decreasing. However, since
boundaries b = 0 b = vmax h(0) = 1 h(vmax ) = vmax g(vmax ), since
g(b) > 0, need consider latter. Since b strictly decreasing, sufficient
show 1/G (b) non-increasing. assumption G (b) non-decreasing,
thus 1/G (b) non-increasing.


Theorem 3. F (b) non-decreasing, G (b) Gb (b) also non-decreasing,
b = en(F (y)1) n 2.
G(b) = F (b)n G

Proof case static local bidders prove
thus showing
1G(b)
g(b)

g(b)
1G(b)

1G(b)
g(b)

non-increasing function,

non-decreasing function. detail, refactor

follows:
1 F n (b)
1 G(b)
=
g(b)
nF n1 (b)f (b)



1 F (b) 1 + F (b) + . . . + F n1 (b)
=
f (b)
nF n1 (b)


1 1 F (b)
=
[1 + F 1 (b) + . . . + F 1n (b)].
n
f (b)

(20)

1F (b)
f (b)

non-increasing

Due non-decreasing hazard rate f (b), derive
follows:

b



1 F (b)
f (b)




=
v



f (b)
1 F (b)

h



1 F (b)
f (b)

2

i2
1F (b)
0. Furthermore, first order condition
f (b)
[1 + F 1 (b) + . . . + F 1n (b)] F 2 (b) . . . (n 1)F n (b),

since

0

second part equation

20,
negative b. Thus,
second part strictly decreasing. Therefore, given first part non-increasing
second part strictly decreasing, implies overall 1G(b)
g(b) strictly decreasing.
Hence G (b) non-decreasing.
968

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

case dynamic local bidders, rewrite Gb (b) as:

g(b)
nf (b)en(F (b)1)
=
b
1 en(F (b)1)
1 G(b)
#

"
nf (b)
(1 F (b))en(F (b)1)
=
1 F (b)
1 en(F (b)1)



1 F (b)
nf (b)
.
=
1 F (b) en(1F (b)) 1

(21)

Since F non-decreasing, clearly first part equation (21) non-decreasing. shall
1F (b)
prove second part equation (21), (b) = en(1F
(b)) 1 , also non-decreasing.
first order condition (b) given by:
f (b)(en(1F (b)) 1) + (1 F (b))(nf (b)en(1F (b)) )
(b)
=
.
b
(en(1F (b)) 1)2

(22)

Since denominator equation (22) always non-negative, thus sufficient show
numerator always non-negative. numerator equation (22) rewritten
as:
h

f (b)en(1F (b)) n 1 nF (b) + en(F (b)1) ,

first second terms > 0 b. Thus, order prove
numerator iis
h
always non-negative, remains shown third term, n1nF (b)+en(F (b)1) ,

non-negative. term equal n 1 + en > 0 (since n > 1) 0 extremums
bmin bmax respectively. Furthermore, first order differential term yields
nf (b)[1 en(F (b)1) ] < 0, b. Hence third term also > 0, b.

Lemma 3. H(b) = b(1 G(b)) unique critical point
b > bf



g(b) > (1 G(b))/b,

b < bf



g(b) < (1 G(b))/b.

similarly

Proof saw already, H unique critical point b > bf iff H decreasing
b. Therefore
b > bf H (b) < 0 1 G(b) bg(b) < 0 g(b) > (1 G(b))/b.
result follows analagously.



Theorem 4. Assumptions 1 2, global bid b maximises Equation 1
one high bid, i.e., one bi > bf , bf unique
critical point H(b) = b(1 G(b)).
969

fiGerding, Dash, Byde & Jennings

Proof second derivatives U interior critical point b follows:
2U
b2i
2U
bi bj



= g (bi ) v (1 G(bj )) bi g(bi ) = g(bi ),
j6=i

= g(bi )g(bj )v



(1 G(bk ))

k6=i,j

g(bj )bj
= g(bi )
.
1 G(bi )

(23)

proof contradiction: show two high bids critical
point cannot local maximum U . done showing two high
bids b, Hessian matrix entries (23) positive eigenvalue, equivalently
exists vector = (a1 , a2 , . . . , )
X

ai aj

i,j

2U
(b) > 0.
bi bj

(24)

show Hessian matrix b positive eigenvalue, means b
either local minimum saddle point (Magnus & Neudecker, 1999, Chapter 6),
turn means small enough displacement direction leads increase
U , contradicting local optimality b.
Assume without loss generality auctions rearranged b1 = b2 =
b+ > bf . Lemma 3 implies g(b+ ) > (1 G(b+ ))/b+ . choose
= (1, 1, 0, . . . , 0), (24) becomes:
X
i,j

ai aj

2U
(b) =
bi bj

2U
2U
2U
(b)
+
(b)

2
(b)
b2
b1 b2
b21


g(b+ )b+
= 2g(b+ )
1
1 G(b+ )
> 0.


A.2 Limit Cases
section examine global bidders expected utility number auctions
goes infinity. particular, prove large enough number auctions,
optimal behaviour always bid uniformly.
First all, note (5) definition optimal uniform bid bu
,
u m1
bu
.
= v(1 G(bm ))

(25)

Lemma 4. smallest bid b optimal (possibly non-uniform) bid vector b tends
0 uniformly v .
Rb
Proof follows fact expected payment function EP (b) = 0 xg(x) dx
strictly monotonically increasing continuous function b, hence monotonically
970

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

increasing continuous inverse, EP 1 : [0, EP (vmax )] [0, vmax ] EP 1 (0) = 0.
0 < U (b)
=

b

<
<
<

v mEP (b )

EP 1 v/m


EP 1 vmax /m

0



first step towards proving uniform bidding eventually globally optimal
show utility optimal bidding converges maximum value v:

Theorem 5. Assumption 1, expected utility defined Equation 1
playing optimal uniform bid bu
converges v, sense > 0
constant > implies U (bu
) > v .
Proof First note result trivial v < since U (bu
) 0 always.
definition utility (1), using (25),


u
U (bu
)
=
v
1

(1

G(b
))
mEP (bu


m)
u
u
= v bu
(1 G(bm )) mEP (bm ).

(26)

u
u
Lemma 4 know bu
0, implies bm (1 G(bm )) 0, turn
attention expected payment term, showing tends zero .
definition (25) bu
m,
ln(bm /v)
m=
+ 1,
ln(1 G(bm ))



ln(bu
/v)
u
lim mEP (bm ) = lim
+ 1 EP (bu
m)

ln(1 G(bu
))

u
ln(bu
/v)EP (bm )
.
(27)
= lim
ln(1 G(bu
))

prove limit zero demonstrate stronger limit
ln(b/v)EP (b)
= 0,
b0 ln(1 G(b))
lim

(28)

applying LHopitals rule multiple times. First apply numerator
(28):


EP (b)
lim ln(b/v)EP (b)
= lim
b0
b0 (ln(b/v))1
bg(b)
= lim 1
b0 b (ln(b/v))2


= lim bg(b) b(ln(b/v))2
b0

= 0,
971

fiGerding, Dash, Byde & Jennings

last limit holds uniformly v vmax .
numerator shown converge zero, possible apply
LHopitals rule directly (28),
ln(b/v)EP (b)
b0 ln(1 G(b))
lim

b1 EP (b) + g(b)b ln(b/v)
b0
g(b)(1 G(b))1


EP (b)
+ b ln(b/v)
= lim
b0
bg(b)


bg(b)
= lim
+ b ln(b/v) .
b0 g(b) + bg (b)
= lim

(29)
(30)

g (b) 0 small enough b order g(b) 0 true everywhere, implies

bg(b)
< b 0,
g(b) + bg (b)
limit b ln(b/v) 0 obvious, limit (30) must zero. turn implies
expected payment uniform bidding tends zero, expected value
tends v, thus theorem proved.

A.3 Budget Constraints
follows provide proof proposition optimal strategy bid
single auction certain conditions, exposure constrained
valuation. proof number auctions complex. Therefore, first provide
formal proof case two auctions, generalise result two.
proof two-auction case also provides building block inductive proofs
contained general case.
First show unconstrained optimum bid eventually exceeds given budget:

Theorem 6. Assumption 1 assuming g(b) bounded throughout [0, vmax ],
v > 0 C, exists forP
exposure optimal global
bid b maximising Equation (1) exceeds C, i.e., iM bi > C.

Proof begin with, note conditions Corollary 1 met, fact
restrict attention uniform bidding sufficiently large, particular
. show v > V > 0 C > 0
mbu
> C.
proof contradiction. Assume constant C bu
mm < C
u < KC/m, (using
m. bounded density assumption, G(bu
)

Kb


Equation (25)):
bu


=

m1
v(1 G(bu
))

>

v(1 KC/m)m1

>

V (1 KC/m)m1

V eKC
972

.

(31)

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

u
fact bu
thus bounded contradicts bound bm < C.



following Lemma characterises bidding strategy budget C v
= 2. result used Theorem 7 prove general case 2.
Lemma 5. = 2 optimal bidding strategy global bidder budget C v
given bc = (C, 0) probability density function g(x) convex g(0) = 0.
Proof order prove lemma, consider difference optimal bid
bc arbitrary bid b = b1 , b2 b 6= bc b1 + b2 = C:
Z C
h

yg(y)dy v 1 (1 G(b1 ))(1 G(b2 ))
U (bc , v) U (b , v) = vG(C)
0
Z b1
Z b2


yg(y)dy
yg(y)dy .
0

0

Hence,
U (bc , v) U (b , v) = (v C)G(C) +

Z

C

G(y)dy vG(b1 ) vG(b2 ) + b1 G(b1 )
Z b1
Z b2
+ vG(b1 )G(b2 )
G(y)dy + b2 G(b2 )
G(y)dy
0
0
Z C
= (v C)(G(C) G(b1 ) G(b2 )) +
G(y)dy
0

b2

(C b1 )G(b1 ) (C b2 )G(b2 )

Z

b1

G(y)dy

(32)

0

+ vG(b1 )(G(b2 )
= (v C)(G(C) G(b1 ) G(b2 )) +
b1 G(b2 )

Z

b1

Z

C

G(y)dy b2 G(b1 )
b2

G(y)dy + vG(b1 )G(b2 ).

0

Now, prove bc indeed optimal bid, need show difference
positive b satisfies budget constraint. RIn order so, first separate
C
equation two parts, namely, X(b1 , b2 ) = b2 G(y)dy b2 G(b1 ) b1 G(b2 )
R b1
0 G(y)dy (v C)Y (b1 , b2 ) = (G(C) G(b1 ) G(b2 )). show
X positive; thus implying that, since (v C) positive, U (bc , v) U (b , v)
also positive.
Now, rewrite X terms b1 replacing b2 C b1, X becomes:
X(b1 ) =

Z

C

G(y)dy (C b1 )G(b1 ) b1 G(C b1 )
Cb1

Z

0

973

b1

G(y)dy.

(33)

fiGerding, Dash, Byde & Jennings

order find local maxima minima set derivative

X
b1

zero:

X
= G(C b1 ) (C b1 )g(b1 ) + G(b1 ) + b1 g(C b1 ) G(C b1 ) G(b1 )
b1
= b1 g(C b1 ) (C b1 )g(b1 ) = 0.

(34)

Since g(0) = 0, easy see least three solutions Equation (34),
namely b1 = {0, C, C/2}. shall show first derivative X always
non-negative within range b1 = [0, C/2], implying X increases within range.
Since X(0) = 0 solution symmetric around C/2, follows X always
non-negative. formally, need show that:
X
0, b1 [0, C/2].
b1
prove holds g(x) convex. Now, definition convex
function have:
g(x1 + (1 )x2 )) g(x1 ) + (1 )g(x2 ), 1.
Now, let x2 = 0. Then:
g(x1 ) g(x1 ) + (1 )g(0).
Since g(0) = 0, becomes:
g(x1 ) g(x1 )
1
g(x1 ) g(x1 ).

Let x1 = (C b1 ) =

b1
Cb1 .

(35)

Then, Equation (35) becomes:

C b1
g(b1 ) g(C b1 )
b1
b1 g(C b1 ) (C b1 )g(b1 ) 0.

(36)

X
Thus, shows b
0, which, turn, proves X 0.
1
prove = G(C) G(b1 ) G(b2 ) 0. Since assume g(x) convex
positive Rin interval [0, 1] g(0) = 0, follows g(x) increasing
x
thus G(x) = 0 g(x)dx also convex. result, use condition Equation (35)
x
1 +b2 )
G(x). Now, replacing b1 +b
x1 b1 + b2 , G(x) G(b
b1 +b2 x
2
x b1 + b2 follows that:

G(b1 + b2 )
G(b1 + b2 )
+ b2

b1 + b2
b1 + b2


b1
b2
G(b1 ) + G(b2 ) G(b1 + b2 )
+

b1 + b2 b1 + b2
G(b1 ) + G(b2 ) G(b1 + b2 ).

G(b1 ) + G(b2 ) b1

974

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

Hence, = G(C) G(b1 ) G(b2 ) 0. result, U (b , v) U (b , v) 0
b1 + b2 = C. Note far assumed sum bids equal budget
constraint, mentioned case b1 + b2 < C. show
optimal bid full budget (i.e., b1 + b2 = C).
Consider arbitrary B = b1 , b2 b1 + b2 = C. Then, replacing C
above, know U ((S, 0), v) U ((b1 , b2 ), v). Hence, remains shown
U ((C, 0), v) U ((S, 0), v). consider difference two bids,
Equation (1) is:
Z
Z C
yg(y)dy
yg(y)dy vG(S) +
U ((C, 0), v) U ((S, 0), v) = vG(C)
0
0


Z C
G(y)dy
= v (G(C) G(S)) CG(C)
0


Z
+ SG(S)
G(y)dy
0

= (v C)(G(C) G(S)) +
(C S)G(S).

Z

C

G(y)dy


Now, G(C) G(S) positive since C definition G(x) non decreasing and,
RC
therefore, G(y)dy (C S)G(S) also positive. Hence, U ((C, 0), v) U ((S, 0), v) 0.

thus shown global bidder bid one auction budget
constrained C v bidding two simultaneous auctions, provided
g(x) convex g(0) = 0 results generalised case 2.
detail, following theorem holds:
Theorem 7. Assumptions 1 3, global bidder budget C v,
Equation (7) satisfied bc = (C, 0, . . . , 0), i.e., optimal bid C exactly one
auction.
c
Proof before, consider difference
P optimal bid b another bid


c
b = (b1 , . . . , bm ) b 6= b C = iM bi , show positive:

U (bc , v) U (b , v) =vG(C)


Z

XZ

iM

C

yg(y)dy

0
bi

yg(y)dy
0

=(v C)G(C) +

Z

!

C

+

bi G(bi )

iM

975

iM

h

G(y)dy v 1

0

X

h


v 1
(1 G(bi ))

Z

bi



G(y)dy .
0



iM


(1 G(bi ))

(37)

fiGerding, Dash, Byde & Jennings

inserting (v C)
rewritten as:

P

G(bi ) (v C)

iM

c

P

iM

G(bi ) Equation (37)
X



U (b , v) U (b , v) = (v C) G(C)

!

G(bi )

iM

C

X

iM

h

v 1

= (v C) G(C)


X

X Z

G(bi )


(1 G(bi ))

iM

X

G(bi )

iM

(C bi )G(bi ) +

Z

+

Z

X


G(bi )

C

G(y)dy

0

bi

G(y)dy

0

iM

G(y)dy

0

iM

!

C


G(y)dy bi G(bi )

bi

0

iM

+

Z



h


X
v 1
(1 G(bi ))
G(bi ) .
iM

iM

Now, define variables Xm Zm as:
Z
Z C
X
G(y)dy
(C bi )G(bi ) +
Xm =
0

h

Xi
Zm = 1
(1 G(bi ))
.



G(y)dy ,

0

iM

iM

bi

iM

Then, rewrite Equation (37) as:
c



U (b , v) U (b , v) = (v C) G(C)

X

!

G(bi )

iM

+ Xm + vZm .

(38)

P
prove part equation positive
P (i.e., (v C)(G(C) iM G(bi ))
0, Xm 0, vZ 0). first prove G(C) iM G(bi ) 0 using relationship
P
Equation (35) convex functions. Specifically, taking = bCi since C = iM bi
yields:
C
G(bi )
bi
X
X
bi G(C)
CG(bi )
G(C)

P

iM

iM

X
iM bi
G(C)
G(bi )
C
iM
X
G(C)
G(bi ).
iM

976

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

Since, definition case studied, v C, (vC)(G(C)
0.

P

iM

G(bi ))

P prove Xm 0 using inductive argument. detail, let =
iM \{m} bi = C bm . Xm written terms Xm1 follows:
Xm =
=

Z

Z

X

C

G(y)dy
0

(C bi )G(bi ) +

iM


G(y)dy
0

X

iM \{m}

bi

G(y)dy

0


Z
(S bi )G(bi ) +

(C bm )G(bm ) (C S)
= Xm1 +

Z

X

C

0

Z

Z

C

P

iM \{m} G(bi ),

G(y)dy



bm

G(y)dy
0

X

G(bi )

iM \{m}

Since G(S) = G(C bm )
Z



G(y)dy +

G(y)dy (C bm )G(bm ) bm



Xm Xm1 +

bi

G(bi )

iM \{m}

Z



Z

bm

G(y)dy.

0

:

C

G(y)dy (C bm )G(bm ) bm G(C bm )

Cbm

Z

bm

G(y)dy.
0

RC
Lemma 5, shown Cbm G(y)dy (C bm )G(bm ) bm G(C bm )
R bm
0 G(y)dy 0 therefore Xm Xm1 . base case X2 shown
within proof Lemma 5 positive. Hence Xm 0.
use inductive argument finally prove Zm 0. base case Zm
= 2 yields:
h


X
Z2 = 1
(1 G(bi ))
G(bi )
iM

iM

h

= 1 (1 G(b1 ))(1 G(b2 )) G(b1 ) G(b2 )

= G(b1 )G(b2 )
0.

inductive hypothesis formulated Zm 0 Zm1 0. order prove
hypothesis, express Zm function Zm1 :
977

fiGerding, Dash, Byde & Jennings

Zm =



(1 G(bi )) +

iM

X

G(bi ) 1

iM

= (1 G(bm ))



(1 G(bi )) +

iM

iM \{m}



= (1 G(bm )) Zm1

X

iM \{m}

= Zm1 G(bm )Zm1 + G(bm )


= Zm1 + G(bm )


X

iM \{m}

= Zm1 + G(bm ) 1



G(bi ) + 1 +
X

iM \{m}

X

G(bi ) 1
X

G(bi ) 1

iM

G(bi )


G(bi ) Zm1



iM \{m}

Zm1 .



(1 G(bi ))

thus proves inductive step, along base case, thereby proves
Zm 0. Hence third part Equation (38) also positive. Since three parts
equation positive, impliesPthat U (bc , v) indeed optimal bc =
(C, 0, . . . , 0). Note assumed C = iM bi . However, using argument
Lemma 5 easy see indeed optimal bid full budget.


Corollary 2. Assumption 1, either g(0) > 0 g(x) strictly concave,
global bidder budget C = v, Equation (7) satisfied bidding strictly positive
least two auctions.
Proof proof largely based reverse arguments Lemma 5. Without loss
generality take = 2. Let bL = {C, 0} denote single-auction bid b = {b1 , b2 }
arbitrary bid b 6= bL b1 + b2 = C, b1 , b2 0 before. Since v C = 0
U (bL , v) U (b , v) = X(b1 ), X given Equation (33). Furthermore,
X
b1 = b1 g(C b1 ) (C b1 )g(b1 ) (see Equation (34)). Now, order prove
optimal bid strictly positive auctions, sufficient show exists
C > b1 > 0 X(b1 ) < 0 one two conditions holds.
X
first show holds g(0) > 0. easy see b
(0) = Cg(0) < 0
1
X(0) thus strictly decreasing. Since X(0) = 0 proves X(b1 ) < 0 b1
slightly larger zero. Since b2 = C b1 follows b2 > 0 long b1 < C,
agent thus better bidding auctions.
consider case g(0) = 0 g(x) strictly concave. replacing
condition convex functions Lemma 5 strictly concave functions follows
b1
1 g(x1 ) > g(x1 ) 12 C > x1 > 0. setting x1 = (C b1 ) = Cb

1
978

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

before, gives (see Equations (35) (36))
b1 (0, C/2).

X
b1

= b1 g(C b1 ) (C b1 )g(b1 ) < 0


References
Barlow, R. E., Marshall, A. W., & Proschan, F. (1963). Properties probability distributions monotone hazard rate. Annals Mathematical Statistics, 34 (2),
375389.
Bergstrom, T., & Bagnoli, M. (2005). Log-concave probability applications. Economic Theory, 26 (2), 445469.
Boutilier, C., Goldszmidt, M., & Sabata, B. (1999). Sequential auctions allocation
resources complementarities. Proceedings 16th International Joint
Conference Artificial Intelligence, pp. 527523.
Burden, R. L., & Faires, J. D. (2004). Numerical Analysis, 8th edition. Brooks Cole.
Burguet, R., & Sakovics, J. (1999). Imperfect competition auction design. International
Economic Review, 40 (1), 231247.
Byde, A., Preist, C., & Jennings, N. R. (2002). Decision procedures multiple auctions.
Proceedings 1st International Joint Conference Autonomous Agents
Multi-Agent Systems, pp. 613620.
Che, Y. K., & Gale, I. (1998). Standard auctions financially constrained bidders.
Review Economic Studies, 65 (1), 121.
Clearwater, S. H. (Ed.). (1996). Market-Based Control: Paradigm Distributed Resource Allocation. World Scientific Publishing.
Dash, R. K., Parkes, D. C., & Jennings, N. R. (2003). Computational mechanism design:
call arms. IEEE Intelligent Systems, 18 (6), 4047.
Dash, R. K., Rogers, A., Reece, S., Roberts, S., & Jennings, N. R. (2005). Constrained
bandwidth allocation multi-sensor information fusion: mechanism design approach. Proceedings 8th International Conference Information Fusion,
pp. 11851192.
Dash, R. K., Vytelingum, P., Rogers, A., David, E., & Jennings, N. R. (2007). Marketbased task allocation mechanisms limited capacity suppliers. IEEE Transactions
Systems, Man Cybernetics: Part A, 37 (3), 391405.
Engelbrecht-Wiggans, R. (1987). Optimal constrained bidding. International Journal
Game Theory, 16 (2), 115121.
Engelbrecht-Wiggans, R., & Weber, R. (1979). example multiobject auction game.
Management Science, 25, 12721277.
979

fiGerding, Dash, Byde & Jennings

Gerding, E. H., Dash, R. K., Yuen, D. C. K., & Jennings, N. R. (2007a). Bidding optimally
concurrent second-price auctions perfectly substitutable goods. Proceedings
6th International Joint Conference Autonomous Agents Multi-Agent
Systems, pp. 267274.
Gerding, E. H., Rogers, A., Dash, R. K., & Jennings, N. R. (2007b). Sellers competing
buyers online markets: Reserve prices, shill bids, auction fees. Proceedings
20th International Joint Conference Artificial Intelligence, pp. 12871293.
Gopal, R., Thompson, S., Tung, Y. A., & Whinston, A. B. (2005). Managing risks
multiple online auctions: options approach. Decision Sciences, 36 (3), 397425.
Greenwald, A., & Boyan, J. (2004). Bidding uncertainty: Theory experiments.
Proceedings 20th Conference Uncertainty Artificial Intelligence, 209216.
Greenwald, A., Kirby, R. M., Reiter, J., & Boyan, J. (2001). Bid determination simultaneous auctions: case study. Proceedings 3rd ACM Conference
Electronic Commerce, pp. 115124.
Hendricks, K., Onur, I., & Wiseman, T. (2005). Preemption delay eBay auctions.
Working Paper, University Texas Austin.
Jiang, A. X., & Leyton-Brown, K. (2007). Bidding agents online auctions hidden
bids. Machine Learning, 67 (1), 117143.
Juda, A. I., & Parkes, D. C. (2006). sequential auction problem eBay: empirical
analysis solution. Proceedings 7th ACM Conference Electronic
Commerce, pp. 180189.
Krishna, V. (2002). Auction Theory. Academic Press.
Krishna, V., & Benoit, J. P. (2001). Multiple object auctions budget constrained
bidders. Review Economic Studies, 68 (1), 155179.
Krishna, V., & Rosenthal, R. (1996). Simultaneous auctions synergies. Games
Economic Behavior, 17, 131.
Lang, K., & Rosenthal, R. (1991). contractors game. RAND Journal Economics,
22, 329338.
Leyton-Brown, K., Shoham, Y., & Tennenholtz, M. (2000). Bidding clubs: Institutionalized collusion auctions. Proceedings 2nd ACM Conference Electronic
Commerce, pp. 253259. ACM Press New York, NY, USA.
Magnus, J. R., & Neudecker, H. (1999). Matrix Differential Calculus Applications
Statistics Econometrics (2nd edition). John Wiley & Sons.
McAfee, R. P. (1993). Mechanism design competing sellers. Econometrica, 61 (6), 1281
1312.
980

fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes

Mes, M., van der Heijden, M., & van Harten, A. (2007). Comparison agent-based scheduling look-ahead heuristics real-time transportation problems. European Journal
Operational Research, 181 (1), 5975.
Palfrey, T. R. (1980). Multi-object, discriminatory auctions bidding constraints:
game-theoretic analysis. Management Science, 26 (9), 935946.
Peters, M., & Severinov, S. (1997). Competition among sellers offer auctions instead
prices. Journal Economic Theory, 75, 141179.
Peters, M., & Severinov, S. (2006). Internet auctions many traders. Journal Economic Theory, 130 (1), 220245.
Pitchik, C. (2006). Budget-constrained sequential auctions incomplete information.
Working Paper 230, Department Economics, University Toronto.
Press, W. H., Flannery, B. P., Teukolsky, S. A., & Vetterling, W. T. (1992). Numerical
Recipes C: Art Scientific Computing (2nd edition). Cambridge University
Press.
Rogers, A., David, E., & Jennings, N. R. (2005). Self organised routing wireless microsensor networks. IEEE Transactions Systems, Man Cybernetics: Part A,
35 (3), 349359.
Rogers, A., David, E., Schiff, J., & Jennings, N. R. (2007). effects proxy bidding
minimum bid increments within eBay auctions. ACM Transactions Web,
1 (2), article 9, 28 pages.
Rosenthal, R., & Wang, R. (1996). Simultaneous auctions synergies common
values. Games Economic Behavior, 17 (1), 3255.
Roth, A. E., & Ockenfels, A. (2002). Last-minute bidding rules ending secondprice auctions: Evidence eBay Amazon auctions Internet. American Economic Review, 92 (4), 10931103.
Rothkopf, M. H. (1977). Bidding simultaneous auctions constraint exposure.
Operations Research, 25 (4), 620629.
Rothkopf, M. H. (2007). Decision analysis: right tool auctions. Decision Analysis,
4 (3), 167172.
Shaked, M., & Shanthikumar, J. G. (1994). Stochastic Orders Applications.
Academic Press.
Shehory, O. (2002). Optimal bidding multiple concurrent auctions. International Journal
Cooperative Information Systems, 11, 315327.
Stone, P., Schapire, R. E., Littman, M. L., Csirik, J. A., & McAllester, D. (2003). Decisiontheoretic bidding based learned density models simultaneous, interacting auctions. Journal Artificial Intelligence Research, 19, 513567.
981

fiGerding, Dash, Byde & Jennings

Stryszowska, M. (2004).
Late multiple bidding competing second price.
Working paper 2004.16, Fondazione Eni Enrico Mattei.
Available
http://ideas.repec.org/p/fem/femwpa/2004.16.html.
Szentes, B., & Rosenthal, R. (2003). Three-object two-bidder simultaneous auctions: Chopsticks tetrahedra. Games Economic Behavior, 44, 114133.
Varian, H. R. (1995). Economic mechanism design computerized agents. Proceedings
1st USENIX Workshop Electronic Commerce, pp. 1321.
Wellman, M. P., Greenwald, A., & Stone, P. (2007). Autonomous Bidding Agents: Strategies
Lessons Trading Agent Competition. MIT Press.
Wellman, M. P., Reeves, D. M., Lochner, K. M., & Vorobeychik, Y. (2004). Price prediction
trading agent competition. Journal Artificial Intelligence Research, 21, 1936.
Yuen, D., Byde, A., & Jennings, N. R. (2006). Heuristic bidding strategies multiple
heterogeneous auctions. Proceedings 17th European Conference Artificial
Intelligence, pp. 300304.
Zeithammer, R. (2005). equilibrium model dynamic auction marketplace. Working
Paper, University Chicago.

982

fiJournal Artificial Intelligence Research 32 (2008) 203-288

Submitted 11/07; published 05/08

New Islands Tractability Cost-Optimal Planning
Michael Katz,
Carmel Domshlak,

dugi@tx.technion.ac.il,
dcarmel@ie.technion.ac.il

Faculty Industrial Engineering Management,
Technion - Israel Institute Technology, Haifa, Israel

Abstract
study complexity cost-optimal classical planning propositional state
variables unary-effect actions. discover novel problem fragments optimization tractable, identify certain conditions differentiate tractable
intractable problems. results based exploiting structural syntactic characteristics planning problems. Specifically, following Brafman Domshlak
(2003), relate complexity planning topology causal graph.
main results correspond tractability cost-optimal planning propositional problems
polytree causal graphs either O(1)-bounded in-degree, induced
actions one prevail condition each. Almost tractability results
based constructive proof technique connects certain tools planning
tractable constraint optimization, believe technique interest
due clear evidence robustness.

1. Precis
AI problem solving inherently facing computational paradox. one hand,
general tasks AI reasoning known hard, degree membership NP sometimes perceived good news. hand,
intelligence somehow modeled computation, computation delegated
computers, artificial intelligence escape traps intractability much
possible. Planning one reasoning tasks, corresponding finding sequence
state-transforming actions achieve goal given initial state. well known
planning intractable general (Chapman, 1987), even simple classical
planning propositional state variables PSPACE-complete (Bylander, 1994).
ups downs interest planning community formal
complexity analysis planning problems, growing understanding days
computational tractability fundamental issue problem solving. pragmatic
reasons twofold.
1. Many planning problems manufacturing process controlling systems
believed highly structured, thus potential allow efficient planning exploiting structure (Klein, Jonsson, & Backstrom, 1998). fact,
structure accounted explicitly, general-purpose planner likely
go tour exponential search space even tractable problems. Moreover,
since intractable theories provide guarantees performance engineering systems, cases guarantees required unavoidable design
c
2008
AI Access Foundation. rights reserved.

fiKatz & Domshlak

controlled system complexity-aware manner planning
provably tractable (Williams & Nayak, 1996, 1997).
2. Computational tractability invaluable tool even dealing problems
fall outside known tractable fragments planning. instance, tractable
fragments planning provide foundations (if all) rigorous heuristic
estimates employed planning heuristic search (Bonet & Geffner, 2001; Hoffmann,
2003; Helmert, 2006; Hoffmann & Nebel, 2001; Edelkamp, 2001). particular
true admissible heuristic functions planning typically defined
optimal cost achieving goals over-approximating abstraction planning problem hand. abstraction obtained relaxing certain constraints
specification original problem, purpose abstraction
provide us provably tractable abstract problem (Haslum, 2006; Haslum &
Geffner, 2000; Haslum, Bonet, & Geffner, 2005).
Unfortunately, palette known tractable fragments planning still limited,
situation even severe tractable optimal planning. knowledge,
less handful non-trivial fragments optimal planning known
tractable. difference theoretical complexity regular optimal
planning general case (Bylander, 1994), many classical planning domains
provably easy solve, hard solve optimally (Helmert, 2003). Practice also provides
clear evidence strikingly different scalability satisficing optimal general-purpose
planners (Hoffmann & Edelkamp, 2005).
work show search new islands tractability optimal classical
planning far exhausted. Specifically, study complexity optimal
planning problems specified terms propositional state variables, actions
changes value single variable. sense, continue line complexity
analysis suggested Brafman Domshlak (2003), extend satisficing
optimal planning. results first time provide dividing line tractable
intractable problems.
1.1 UB (Optimal) Planning Problems
Problems classical planning correspond reachability analysis state models
deterministic actions complete information. work focus state models
describable certain fragment SAS+ formalism (Backstrom & Nebel, 1995)
allows propositional state variables unary-effect actions. Following Backstrom
Nebel (1995), follows refer subclass SAS+ UB (short
unary-effect, binary-valued). Somewhat surprisingly, even non-optimal planning UB
PSPACE-complete, is, hard general propositional planning (Bylander, 1994).
Definition 1 SAS+ problem instance given quadruple = hV, A, I, Gi, where:
V = {v1 , . . . , vn } set state variables, associated finite domain
Dom(vi ); initial state complete assignment, goal G partial
assignment V , respectively.
204

fiTractable Cost-Optimal Planning

= {a1 , . . . , } finite set actions, action pair hpre(a), eff(a)i
partial assignments V called preconditions effects, respectively. action
associated non-negative real-valued cost C(a). action applicable
state Dom(V ) iff s[v] = pre(a)[v] whenever pre(a)[v] specified. Applying
applicable action changes value variable v eff(a)[v] eff(a)[v]
specified.
SAS+ problem instance belongs fragment UB SAS+ iff state
variables V binary-valued, action changes value exactly one variable,
is, A, |eff(a)| = 1.
Different sub-fragments UB defined placing syntactic structural restrictions actions sets problems. instance, Bylander (1994) shows
planning UB domains action restricted positive preconditions
tractable, yet optimal planning UB fragment hard. general, seminal
works Bylander (1994) Erol, Nau, Subrahmanian (1995) indicate extremely
severe syntactic restrictions single actions required guarantee tractability, even
membership NP. Backstrom Klein (1991) consider syntactic restrictions
global nature, show UB planning tractable two actions effect,
preconditions two actions require different values variables
affected actions. Interestingly, fragment UB, known PUBS, remains
tractable optimal planning well. characterizing properties PUBS
restrictive, result Backstrom Klein provided important milestone
research planning tractability.
Given limitations syntactic restrictions observed Bylander (1994), Erol et al.
(1995), Backstrom Klein (1991), recent works studied impact
posing structural mixed structural/syntactic restrictions action sets.
scope UB, works relate complexity planning topological
properties problems causal graph structure.
Definition 2 causal graph CG() SAS+ problem = hV, A, I, Gi digraph
nodes V . arc (v, v ) belongs CG() iff v 6= v exists action
changing value v preconditioned value v, is,
eff(a)[v ] pre(a)[v] specified.
Informally, immediate predecessors v CG() variables directly
affect ability change value v, evident constructing causal
graph CG() given UB planning problem straightforward. instance, consider
action set depicted Figure 1a. easy verify actions set
unary effect. causal graph induced action set depicted Figure 1b.
actions a1 a2 actions change values v1 v2 , respectively,
actions preconditions outside affected variables. Hence, causal graph
contains arcs incoming nodes v1 v2 . hand, actions changing
v3 v4 preconditioned (in cases) values v1 v2 , thus v3
v4 incoming arcs v1 v2 .
Way used complexity analysis, causal graphs (sometimes
indirectly) considered scope hierarchically decomposing planning tasks (Newell &
205

fiKatz & Domshlak


a1
a2
a3
a4
a5

v1
0

pre(a)
v2 v3

v4

v1
1

eff(a)
v2 v3

v4

0
0
1

1
0
1

v2

BB
BB |||
B|
|| BBB
~||


1
0
0

v1

v3

1
0

(a)

v4

v1

v2

||
||
|
|
|~ |


v3

(b)

v4

(c)

Figure 1: Example two simple action sets fit characteristics UB fragment.
(a) Unary-effect action set propositional variables V = {v1 , . . . , v4 }, (b)
Causal graph induced A, (c) Causal graph induced \ {a4 }.

Simon, 1963; Sacerdoti, 1974; Knoblock, 1994; Tenenberg, 1991; Bacchus & Yang, 1994).
first result relating complexity UB planning structure
causal graph due Backstrom Jonsson (1995, 1998b) identify fragment
UB, called 3S, interesting property inducing tractable plan existence
yet intractable plan generation. One key characteristics 3S acyclicity
causal graphs. special case 3S also independently studied Williams
Nayak (1997) scope incremental planning general SAS+ problems.
recently, Brafman Domshlak (2003) provide detailed account complexity finding plans UB problems acyclic causal graphs. results
closely related problems examined paper, thus survey
details. ease presentation, introduce certain notation heavily used
throughout paper.
node v CG(), In(v) Out(v) denote in- out-degrees v,
respectively, In(CG())/Out(CG()) stand maximal in-degree/out-degree
CG() nodes.
Assuming CG() connected1 , provide special notation following topologies acyclic causal graphs, also depicted Figure 2. causal CG()
tree In(CG()) 1, exists v V In(v) = 0.
inverted tree Out(CG()) 1, exists v V Out(v) = 0.
P polytree CG() contains undirected cycles. (For example polytree
neither tree inverted tree see Figure 1c Figure 2.)
directed-path singly connected one directed path node
v CG() node v CG(). (For example directed-path
singly connected DAG see Figure 1b Figure 2.)
1. CG() consists connected components, components identify independent subproblems easily identified treated separately.

206

fiTractable Cost-Optimal Planning

Figure 2: Examples causal graphs topologies considered paper, along
inclusion relations induced fragments UB.

follows, use T, I, P, refer corresponding fragments UB,
use subscript/superscript b refer fragment induced additional constraint
in-degree/out-degree bounded constant. hard verify
T, P S, Pb Pb ; complete inclusion hierarchy
sub-fragments UB shown Figure 3a.
key tractability result Brafman Domshlak (2003) corresponds polynomial time plan generation procedure Pb , is, UB problems inducing polytree
causal graphs nodes O(1)-bounded indegree. addition, Brafman
Domshlak show plan generation NP-complete fragment S, note
proof claim easily modified hold Sbb . results tractability
hardness (as well immediate implications) depicted Figure 3b
shaded bottom-most transparent top-most free-shaped regions. empty freeshaped region corresponds gap left Brafman Domshlak (2003).
gap recently closed Gimenez Jonsson (2008) prove NP-completeness
plan generation P. note proof Gimenez Jonsson actually carries
fragment well, gap left Brafman Domshlak (2003)
entirely closed.
1.2 Summary Results
complexity results Brafman Domshlak (2003) Gimenez Jonsson
(2008) correspond satisficing planning, distinguish plans
207

fiKatz & Domshlak

(a)

(b)

Figure 3: Inclusion-based hierarchy complexity plan generation UB problems acyclic causal graphs. (a) hierarchy STRIPS fragments corresponding tree, inverted tree, polytree, directed-path singly connected
topologies causal graph, (possibly) O(1) bounds causal graph
in-degree and/or out-degree. (b) Plan generation tractable fragments
(bottom-most) shaded region, NP-complete depicted fragments. top-most intermediate (transparent) regions correspond
results Brafman Domshlak (2003) Gimenez Jonsson (2008), respectively.

basis quality. contrast, study complexity optimal plan generation
UB, focusing (probably canonical) cost-optimal (also known sequentiallyoptimal) planning. Cost-optimal
planning corresponds task finding plan
P
minimizes C() = C(a). provide novel tractability results cost-optimal
planning UB, draw dividing line tractable intractable
problems. Almost tractability results based proof technique connects
certain tools planning tractable constraint optimization. strongly
believe proof-technical contribution paper interest due
clear evidence robustnessour different algorithms exploit proof technique,
much different manners.
rest section aim providing adequate description results
readers want delve formal details, prefer first
reading paper.2 Hence, formal definitions, constructions, proofs underlying
results given later, starting Section 2.

2. adopted format seminal paper Bylander (1994) feel format contributed
something making paper extremely enjoyable read.

208

fiTractable Cost-Optimal Planning

1.2.1 Cost-Optimal Planning Pb
Following Brafman Domshlak (2003), relate complexity (cost-optimal) UB
planning topology causal graph. consider structural hierarchy depicted Figure 3a. begin considering cost-optimal planning Pb
apparent Figure 3b expressive fragment hierarchy
still candidate tractable cost-optimal planning. first positive result affirms
possibility, showing complexity map cost-optimal planning UB
fragments Figure 3a identical satisficing planning (that is, Figure 3b).
algorithm Pb based compiling given Pb problem constraint
optimization problemP
COP = (X , F) variables X , functional components F,
global objective min F (X )
(I) COP constructed time polynomial description size ,

(II) tree-width cost network COP bounded constant, optimal
tree-decomposition network given compilation process,
(III) unsolvable assignments X evaluate objective function
, otherwise, optimum global objective obtained
assignments X correspond cost-optimal plans ,
(IV) given optimal solution COP , corresponding cost-optimal plan
reconstructed former polynomial time.
compilation scheme, solve COP using standard, poly-time algorithm constraint optimization trees (Dechter, 2003), find optimal solution
. compilation based certain property cost-optimal plans Pb
allows conveniently bounding number times state variable changes value
along optimal plan. Given property Pb , state variable v compiled
single COP variable xv , domain COP variable corresponds
possible sequences value changes v may undergo along cost-optimal plan.
functional components F defined one COP variable xv , scope
function captures family original state variable v causal graph,
is, v immediate predecessors CG(). illustration, Figure 4a
depicts causal graph P problem , family state variable v4
depicted shaded region, Figure 4b shows cost network induced compiling
Pb problem, dashed line surrounding scope functional component
induced family v4 . hard verify cost network induces tree
variable-families cliques, Pb problem, size clique bounded
constant. Hence, tree-width cost-network bounded constant well.
1.2.2 Causal Graphs k-Dependence
causal graphs provide important information structure planning
problems, closer look definition reveals information used defining
causal graphs actually gets hidden structure. start example, let us
consider multi-valued encoding Logistics-style problems (Helmert, 2006).
209

fiKatz & Domshlak

problems, variable representing location package parents
causal graph variables representing alternative transportation means (i.e.,
tracks, planes, etc.), yet, individual action affecting location package
preconditioned one parent variable. (You cannot load/unload package
into/from one vehicle.) exemplifies fact that, even in-degree
causal graph proportional problem domains parameters, number variables
determine applicability action may still bounded constant.
words, causal graph provides aggregate view independence
relationships problem variables, individual dependencies problem
actions unaffected variables suppressed view. Targeting actual
individual dependencies actions, define (tangential causal graphs
topology) classification UB problems, study connection classification computational tractability general cost-optimal plan generation
UB.
Definition 3 k Z , SAS+ problem instance = (V, A, I, G), say
k-dependent satisfies
max |{v V | pre(a)[v] 6= u eff(a)[v] = u}| k,
aA

= u standing unspecified.
words, SAS+ problem k-dependent action action set depends
k unaffected variables. Combining two classifications problems,
structural fragment F UB (such as, e.g., Figure 3), k Z ,
F(k) denote set k-dependent problems within F.
Recall fragment P UB NP-hard even satisficing planning (Gimenez
& Jonsson, 2008). main result positiveat least extreme (yet, says
Logistics example above, unrealistic) setting k = 1, satisfying k-dependence
bring us island tractability P(1).
Similarly treatment Pb , algorithm P(1) exploits idea compiling
planning problem tractable constraint optimization problem COP . However,
planning-to-COP compilation scheme P(1) much different devised
Pb . fact, difference unavoidable since construction Pb heavily relies
assumption In(CG()) = O(1), luxury P(1). Instead,
identify certain properties cost-optimal plan sets P(1) problems, exploit
properties devising suitable planning-to-COP compilation schemes.
begin considering P(1) problems uniform-cost actions; cost
plan problem proportional length plan3 . show
solvable problem cost-optimal plan makes changes variable
certain value using exactly (type of) action. devising correct
tractable planning-to-COP compilation scheme step away identifying
property P(1), latter provides critical brick everything else relies upon.
Relying property P(1), state variable v edge (v, v ) uniquely
3. probably origin term sequential optimality.

210

fiTractable Cost-Optimal Planning

(a)

(b)

(c)

Figure 4: Cost networks induced planning-to-COP compilation schemes P. (a)
Causal graph P problem , family state variable v4
depicted shaded region. (b) Cost network induced compiling Pb
problem, dashed line surrounding scope functional component
induced family v4 . (c) Cost network induced compiling P(1)
problem, dashed lines surrounding scopes four functional
components induced family v4 .

compiled COP variables xv xvv (see Figure 4c). certain set functional components defined COP variable xv . domains COP variables
specification functional components technically involved, thus relegated later paper. important, however, note already cost
networks COPs guaranteed induce trees cliques size 3, thus
tree-width bounded constant. reader get intuition
cliques size 3 coming looking example depicted Figure 4c.
Unfortunately, aforementioned helpful property P(1) problems uniformcost actions hold general action-cost schemes P(1). Turns out, however, problems P(1) satisfy another property still allows devising
general, correct, tractable planning-to-COP scheme P(1). Specifically, show
solvable problem P(1) cost-optimal plan makes changes
variable using three types action. algorithm resulting exploiting
property complex costly devised P(1) problems
uniform-cost actions, yet still poly-time. Interestingly, cost networks COPs
topologically identical problems uniform-cost actions, difference domains COP variables, specification functional
components.
read far, reader may rightfully wonder whether O(1)-dependence
strong enough property make cost-optimal planning tractable even
complex polytree forms causal graph. Turns dividing line
211

fiKatz & Domshlak

Pb
P(k)
Sbb

k=1

k=2

k=3

k = (n)


P
NPC









P
NPC
NPC

Pb
P(k)
Sbb

(a)

k=1

k=2

k=3

k = (n)


P





NPC



P
NPC
NPC

(b)

Figure 5: Complexity (a) cost-optimal (b) satisficing plan generation fragments
UB. mark indicates complexity implied results
row. shaded regular cells correspond results obtained
work past, respectively. Empty cells correspond open questions.
Note difference understanding cost-optimal satisficing
planning fragments question complexity planning S(1).

tractable intractable problems much delicate. Figure 5 summarizes current
understanding time complexity cost-optimal satisficing plan generation
P fragments UB. First, paper show even satisficing planning
directed-path singly connected, bounded in- out-degree causal graphs hard
2-dependence, cost-optimal planning structural fragment UB hard
even 1-dependent problems. Note complexity (both cost-optimal
satisficing) plan generation P(k) k = O(1) remains interesting open problem.
additional question remains open complexity satisficing plan generation
S(1).
1.3 Remarks
goal work identifying new islands tractability cost-optimal planning, improving understanding makes planning problems hard
easy solve. lesser interest make poly-time algorithms practically
efficient reducing (quite prohibitive) polynomial time complexity. fact,
places intentionally sacrificed possible optimizations keep already involved
constructions apprehensible possible. Therefore, likely time
complexity planning-to-COP algorithms improved, conceptually different algorithmic ideas found appropriate problems question.
addition, much efficient algorithms may work special cases general
tractable families. instance, paper illustrate possibility presenting
low poly-time algorithm UB problems tree causal graphs (that is, fragment)
uniform-cost actions.
course, reader may ask whether striving practical efficiency solving various
special fragments planning motivated. discussion beginning
paper suggests, believe answer question yes.
research AI planning rightfully devoted solving general planning problems, many
tools developed employed research rely tractable fragments planning.
212

fiTractable Cost-Optimal Planning

instance, one works devising effective heuristic estimator planning problem
projecting (or embedding in) another relaxed problem,
happy know latter solved low poly-time. hand, making
tractable fragment also efficiently solvable practical terms probably worth effort
face concrete customer fragment practice.

2. Definitions Notation
Starting Definitions 1-3 previous section, section introduce
additional definitions notation used throughout paper.
contrast well-known STRIPS formalism propositional planning, assume
actions value changing, contrast value setting.
is, eff(a)[v] specified pre(a)[v] also specified, case
eff(a)[v] 6= pre(a)[v]. general assumption requires exponential time
translation, case unary-effect actions translation takes linear time. Given
UB problem = hV, A, I, Gi, Av denote actions change value
v. Note unary-effectness implies Av1 , . . . , Avn partition problem
actions A. Considering applicability actions, SAS+ also helps give special
attention notation action preconditions left unaffected action.
customary name preconditions prevail conditions (Backstrom & Klein,
1991). example, truck package P location L preconditions
loading P L, former prevail condition action
truck still L loading P , P longer (but inside ).
Given UB problem = hV, A, I, Gi, variable subset V V , arbitrary
sequence
actions , V denote order-preserving restriction
actions vV Av . restriction respect singleton set V = {v},
allow writing {v} simply v . One key properties cost-optimal plans
UB problems directed-path singly connected causal graphs immediately derivable
Lemma 1 Brafman Domshlak (2003), given Corollary 1 below.
Henceforth, valid plan given problem called irreducible subplan
plan , following sense4 : Removal subset (not necessarily
subsequent) actions makes resulting plan either illegal, initial state
I, end state one states specified G.
Lemma 1 (Brafman Domshlak, 2003) solvable problem n
state variables, irreducible plan , state variable v , number
value changes v along n, is, |v | n.
Corollary 1 solvable problem n state variables, cost-optimal plan
, state variable v , |v | n.
4. notion irreducible plans introduced Kambhampati (1995), called minimal
plans, exploited admissible pruning partial plans search. adopt terminology
suggested Brafman Domshlak (2003) prevent ambiguity minimal irreducible
minimal optimal.

213

fiKatz & Domshlak

Given problem = hV, A, I, Gi, denote initial value I[v] variable
v V bv , opposite value wv (short for, black/white). Using notation
exploiting Corollary 1, (v) denote longest possible sequence values
obtainable v along cost-optimal plan , |(v)| = n + 1, bv occupying
odd positions (v), wv occupying even positions (v). addition, (v)
denote per-value time-stamping (v)
(
b1v wv1 b2v wv2 bj+1
v , n = 2j,
(v) =
, j N.
n = 2j 1,
b1v wv1 b2v wv2 wvj ,
sequences (v) (v) play important role constructions via prefixes suffixes. general, sequence seq, [seq]
[seq] denote set non-empty prefixes suffixes seq, respectively.
context, prefix [(v)] called goal-valid either goal value G[v] unspecified,
last element equals G[v]. set goal-valid prefixes (v) denoted
[(v)] [(v)]. notion goal-valid prefixes also similarly specified (v).
Finally, given SAS+ problem = hV, A, I, Gi, subset state variables V V ,
action sequence , say applicable respect V restricting
preconditions effects actions variables V makes applicable I.

3. Cost-Optimal Planning Pb
section devoted proof tractability cost-optimal planning problem
fragment Pb . begin describing planning-to-COP scheme Pb , prove
correctness complexity. Finally, present interesting subset Pb costoptimal planning tractable, also provably solvable low polynomial time.
3.1 Construction
proceed details construction, make assumption
actions fully specified terms variables parents causal graph. pred(v)
V denotes set immediate predecessors v causal graph CG(),
assume that, action Av , pre(a)[w] specified w pred(v).
general assumption requires exponential translation, case
Pb . Let translation original problem actions A. obtain ,
every variable v V , every action Av represented set actions
preconditioned complete assignments pred(v). |pred(v)| = k, precondition
specified terms 0 k k parents v, represented
set actions, extending precondition pre(a) certain instantiation
previously unspecified k k parents v, cost C(a ) = C(a). Note
expansions two original actions may overlap, thus may contain
syntactically identical yet differently priced actions. Without loss generality, assume
minimally-priced clone kept . key point compiling
Pb problems poly-time, procedure linear |A | = O(n2In()+1 ).
Finally, (straightforward prove) Proposition 1 summarizes correctness
assumption respect cost-optimal planning UB.
214

fiTractable Cost-Optimal Planning

Proposition 1 UB problem = hV, A, I, Gi, cost optimal plans
equal = hV, , I, Gi, optimal plans reconstructible
linear time optimal plans vice versa.
specify compilation given Pb problem constraint optimization
problem COP . COP variable set X contains variable xv planning variable
v V , domain Dom(xv ) consists valid prefixes (v). is,
X = {xv | v V }
Dom(xv ) = [ (v)]

(1)

Informally, domain variable xv contains possible sequences values
planning variable v may undergo along cost-optimal plan. Now, planning
variable v parents pred(v) = {w1 , . . . , wk }, set COP functions F contains
single non-negative, real-valued function v scope
Qv = {xv , xw1 , . . . , xwk }

(2)

purpose functions connect value-changing sequences v
parents pred(v). specification functions involved part
compilation.
First, planning variable v pred(v) = , goal-valid (timestamped) value-changing sequences [ (v)], set


| | = 1

0,
| | = 2
(3)
v ( ) = C(a
j k
j wkv ),


| | C(awv ) + | |1 C(abv ), otherwise
2

2

eff(awv )[v] = {wv }, eff(abv )[v] = {bv }, C(a) = C(a) A, , otherwise.
hard verify v ( ) corresponds optimal cost performing | | 1
value changes v .
Now, non-root variable v pred(v) = {w1 , . . . , wk }, k 1, specify
function v follows. goal-valid value-changing sequence [ (v)] v,
set goal-valid value-changing sequences {1 [ (w1 )], . . . , k [ (wk )]}
vs parents, want set v ( , 1 , . . . , k ) optimal cost performing | | 1
value changes v, given w1 , . . . , wk change values |1 | 1, . . . , |k | 1 times,
respectively. follows, reduce setting value v ( , 1 , . . . , k ) solving
single-source shortest path problem edge-weighted digraph Ge (v) slightly enhances similarly-named graphical structure suggested Brafman Domshlak (2003).
Despite substantial similarity, provide construction Ge (v) full details
save reader patching essential differences.
Given value-changing sequences 1 , . . . , k above, digraph Ge (v) created
three steps. First, construct labeled directed graph G(v) capturing information
sequences assignments pred(v) enable n less value flips v. graph
G(v) defined follows:
1. G(v) consist = max [ (v)] | | nodes.
215

fiKatz & Domshlak

2. G(v) forms 2-colored multichain, i.e., (i) nodes graph colored
black (b) white (w), starting black; (ii) two subsequent nodes
color; (iii) 1 1, edges node node
+ 1.
Observe construction G(v) promises color last node
consistent goal value G[v] specified.
3. nodes G(v) denoted precisely elements longest goal-valid
value-changing sequence [ (v)], is, biv stands ith black node
G(v).
4. Suppose operators Av that, different preconditions, change
value v bv wv . case, i, edges biv
wvi , |Av | edges wvi bi+1
v . edge e labeled cost
corresponding action, well prevail conditions action,
k-tuple values w1 , . . . , wk . compound label e denoted l(e),
prevail condition cost parts l(e) henceforth denoted prv(e)
C(e), respectively.
formal definition G(v) somewhat complicated, provide illustrating
example. Suppose given Pb problem 5 variables, consider
variable v pred(v) = {u, w}, I[v] = bv , G[v] = wv . Let

a1 : pre(a1 ) = {bv , bu , ww }, eff(a1 ) = {wv }, C(a1 ) = 1
Av =
: pre(a2 ) = {wv , bu , bw }, eff(a2 ) = {bv }, C(a2 ) = 2
2
a3 : pre(a3 ) = {wv , wu , ww }, eff(a3 ) = {bv }, C(a3 ) = 3

corresponding graph G(v) depicted Figure 6a. Informally, graph G(v)
captures information potential executions actions Av along cost-optimal
plan . path source node G(v) uniquely corresponds one
execution. Although number alternative executions may exponential
n, graphical representation via G(v) compactthe number edges G(v)
O(n |Av |). Note information number times action Av
executed captured G(v). following two steps add essential information
graphical structure.
second step, digraph G(v) = (V, E) expanded digraph G (v) =

(V , E ) substituting edge e E set edges (between nodes),
labels corresponding possible assignments elements 1 , . . . , k
prv(e). example, edge e E labeled kbw1 bw2 , 10k might substituted
E edges labeled {kb1w1 b1w2 , 10k, kb1w1 b2w2 , 10k, kb2w1 b1w2 , 10k, . . . }. Finally, set
V = V {sv , tv }, add single edge labeled first elements 1 , . . . , k
zero cost (that is, kb1w1 b1wk , 0k) sv original source node b1v , plus single edge
labeled last elements 1 , . . . , k zero cost original sink node G(v)
tv . Informally, digraph G (v) viewed projection value-changing
sequences 1 , . . . , k base digraph G(v). Figure 6b illustrates G (v) example
1 b2 w2 .
above, assuming u = b1u wu1 b2u wu2 b3u w = b1w ww
w
w



third step, digraph Ge (v) = (Ve , Ee ) constructed G (v) follows.
216

fiTractable Cost-Optimal Planning

bu bw ,2
bu ww ,1 1
/w
b1v
v

$
:

bu bw ,2
bu ww ,1 2
/w
b2v
v

wu ww ,3

$

bu ww ,1 3
/w
v

3
: bv

wu ww ,3

(a)

sv

b3u b1w ,2

b3u b2w ,2

b3u b2w ,2

b2u b1w ,2

b2u b1w ,2

1 ,
b3u ww
1

b2u b2w ,2

1 ,
b3u ww
1

b2u b2w ,2

1 ,
b3u ww
1

1 ,
b2u ww
1

b1u b1w ,2

1 ,
b2u ww
1

b1u b1w ,2

1 ,
b2u ww
1

1 ,
b1u ww
1

b1u b1w ,0

b3u b1w ,2

/ b1
v

&
1
8 wH K v

b1u b2w ,2

%
2
9 bH K L v

1 ,
b1u ww
1

&

b1u b2w ,2

2
8 wv

%

1 ,
b1u ww
1

3
9 bv

HK

3
8 wv

H KL

HK

2 ,
b1u ww
1

1 w1 ,
wu
w 3

2 ,
b1u ww
1

1 w1 ,
wu
w 3

2 ,
b1u ww
1

2 ,
b2u ww
1

1 w2 ,
wu
w 3

2 ,
b2u ww
1

1 w2 ,
wu
w 3

2 ,
b2u ww
1

2 ,
b3u ww
1

2 w1 ,
wu
w 3

2 ,
b3u ww
1

2 w1 ,
wu
w 3

2 ,
b3u ww
1

2 w2 ,
wu
w 3

&

2 ,0
b3u ww

/ tv

2 w2 ,
wu
w 3

(b)
Figure 6: Example graphs (a) G(v), (b) G (v).
(i) nodes correspond edges G (v).
(ii) edges (ve , ) Ee correspond pairs immediately consecutive edges
e, e E that, 1 k, either prv(e)[wi ] = prv(e )[wi ], prv(e )[wi ]
appears prv(e)[wi ] along .
(iii) edge (ve , ) Ee weighted C(e ).
Figure 7 depicts graph Ge (v) example.
Assuming 3 2 , dashed edges correspond minimal-cost path length 5
dummy source node b1u b1w . Note that, costs actions Av care
about, path corresponds cost-optimal sequence 5 value changes v starting
initial value bv . fact, path corresponds cost-optimal
sequence, also explicitly describes underlying sequence actions Av ,
well total cost actions. Finally, 0 n, minimal-cost paths
length determined running Ge (v) low-polynomial single-source shortest
217

fiKatz & Domshlak

b3u b2w , 2

b3 b2 ,

b1u b1w , 2

b1u b1w , 2

2
u Fw
F J K 999
J K 999






99
99


99
99




9
9
3 1
3 1


9
bubw , 2
bubw , 2 999
99


99LLL 99
99LLL 99




99 LLL 99
99 LLL 99






99 LLL 99
99 LLL 99




L
L%


99
99
%




2 2

9
9


3
2
2
2
2 ,
3
2
_
_
/

/

99 bu ww , 1
99 b3u ww
w ,
w
bu ww , 1 w
w ,

C
uB F H J w 3
uB F H J w 3
99rr9 B F H J K
99rr9 B FH J K 88 1





r 9
r 9
88





rrr 9 9
rrr 9 9
88


r
r


r
r





r

9
88
9



rr

r






2 1
/ 3 1
2 1
8
/ 3 1
3 1








b
w
,

w
w
,

uww , 1 w
uww , 1 88
u ww , 3 b
1 u B H w
3 b
u9 w




B
J
H
B
B

H
H
K
K



















JJ 88

JJ 88


tttt






JJ 8
















tt


JJ 8











%



/ 2 2 2 2
/ 2 2
2 2



2
2
1
1
/
/ b3 w2 , 0


bu ww , 1 b u bw , 2 b u ww , 1 b u bw , 2 b u ww , 1
bu bw , 0
//7 JJ
9 uC G w
r r9 F
9 B F H J
9 B F H J 9 F
tt
rr r r
rr r r rrr r

//7 JJJJ
rr rr




r
r
r








r
rr
rr rr
JJ
// 7
tt
J%
tt
rrr
rrr
rrr rrr
// 7

1 , b2 b1 , / b2 w1 , b2 b1 , / b2 w1 ,
// 7 7 b2u ww
1
1
2
1
2

// 7
u w uB H w u w uB H w


// 7












// 7



//










2 ,
2 , b1 w2 ,
/ w1 w2 ,
b1 w2 ,
b//1u ww


1 _ _/ w9u1 ww

9u w 3 uB F w 1
3 uB F w 1
//
r
r

r
r
rrrr

//

rrr
r









r
r



rrr


/ rrr



1 ,
1 w1 ,
1 w1 ,
b1 w1 ,
b1 w1 ,
/
/
b1u ww
w
w


1
u w 3 u B w L1
u w 3 u B w
L1LL
LLL


LLL
L








LLL
LLL



L%

%



b1u b2w ,
b1u b2w ,
2
2







Figure 7: graph Ge (v) constructed graph G (v) Fugure 6b.

paths algorithm Dijkstra (Cormen, Leiserson, & Rivest, 1990). property
graph Ge (v) provides us last building block algorithm cost-optimal
planning Pb .
overall algorithm cost-optimal planning Pb based construction depicted Figure 8. Given problem Pb , algorithm compiles
constraint optimization problem COP , solves using standard algorithm
constraint optimization tree constraint networks (Dechter, 2003). specification
COP already explained inline. believe already intuitive compilation takes time polynomial description size , next section also
prove formally. Solving COP using algorithm tree-structured constraint networks
done time polynomial description size COP
218

fiTractable Cost-Optimal Planning

procedure polytree-k-indegree( = (V, A, I, G))
takes problem Pb
returns optimal plan solvable, fails otherwise
create set variables X set domains Eq. 1
create set functions F = {v | v V } scopes Eq. 2
v V
pred(v) =
specify v according Eq. 3
elseif pred(v) = {w1 , . . . , wk }
construct graph G(v)
k-tuple 1 [ (w1 )], . . . , k [ (wk )]
construct graph G (v) graph G(v) sequences 1 , . . . , k
construct graph Ge (v) graph G (v)
goal-valid sequence [ (v)]
:= minimal-cost path | | 1 edges
source node hbw1 bwk Ge (v)
returned
v ( , 1 , . . . , k ) := C()
else
v ( , 1 , . . . , k ) :=
endif
endfor
endfor
endif
endfor
P
set COP := (X , F) global objective min F (X )
x :=
P solve-tree-cop(COP )
F (x) = return failure
P
extract plan x C() = F (x)
return
Figure 8: Algorithm cost-optimal planning Pb .
(i) tree-width cost network COP bounded constant
bounds in-degree causal graph,
(ii) optimal tree-decomposition COP cost network given topological
ordering causal graph.
3.2 Correctness Complexity
proceed proving correctness polynomial time complexity
algorithm Pb . begin proving Theorem 1 rather general property
polytrees helps us analyzing Pb fragment question, well P(1)
fragment considered later paper. note special case property
219

fiKatz & Domshlak

already exploited past proof Lemma 2 Brafman Domshlak
(2003), but, knowledge, property never formulated generic claim
Theorem 1. Throughout paper demonstrate generic claim
helpful numerous situations; proof Theorem 1 Appendix A, p. 245.
Theorem 1 Let G polytree vertices V = {1, . . . , n}, pred(i) V denote
immediate predecessors G. V , let Oi finite set objects associated
vertex i, sets O1 , . . . , pairwise disjoint. V , let >i
strict partial order Oi , and, j pred(i), let >i,j strict partial order
Oi Oj .
If, V, j pred(i), transitively closed >i >i,j >j >i,j induce
(strict) partial orders Oi Oj , transitively closed


[
[
>i
>i,j
> =
iV

=



iV

jpred(i)

Oi .

Using Theorem 1 proceed proving correctness complexity
polytree-k-indegree algorithm.
Theorem 2 Let planning problem Pb , COP = (X , F) corresponding
constraint
optimization problem, x Dom(X ) optimal solution COP
P
F (x) = .
(I) < , plan cost reconstructed x time polynomial
description size .

(II) plan, < .
Proof Sketch: proof Theorem 2Pis Appendix A. p. 247. prove (I), given
COP solution x = {v1 , . . . , vn } F (x) = < , construct plan
C() = . done constructing action sequences v v V ,
well constructing partial orders elements sequences variable
parents. orders combined linearized
1)
P (using Theorem
P

P action sequence valid plan C() = vV C(v ) = vV v (x) =
problem irreducible
F (x) = . prove (II), given solvable
P
Pplan ,
construct COP assignment x F (x ) = C(). Then, F (x )
C() < , obtain claimed < .

Theorem 3 Cost-optimal planning Pb tractable.
Proof: Given planning problem Pb , show corresponding constraint optimization problem COP constructed solved time polynomial description
size .
Let n number state variables , maximal node in-degree
causal graph CG(). polytree-k-indegree, planning variable v V
pred(v) = {w1 , . . . , wk }, k-tuple 1 [ (w1 )], . . . , k [ (wk )],
220

fiTractable Cost-Optimal Planning

(i) construct graph Ge (v),
(ii) use Dijkstra algorithm compute shortest paths source node Ge (v)
nodes graph.
wi , (wi ) = n, thus number k-tuples v V
O(nk ). k-tuple, corresponding graph Ge (v) constructed
time linear number edges = O(n2k+2 |Av |2 ) = O(n2k+2 22k+2 ) (Brafman &
Domshlak, 2003). time complexity Dijkstra algorithm digraph
G = (N, E)
O(E log (N )), Ge (v) gives us n2k+2 22k+2 log nk+1 2k+1 . Putting things
together, complexity constructing COP

n3+3 22+2 log n+1 2+1 .
(4)

Applying tree-decomposition COP along scopes functional components
arrive equivalent, tree-structured constraint optimization problem n variables
domains size O(n+1 ). COP defined hard binary compatibility
constraints variables, costs associated variables values.
tree-structured COP solved time O(xy 2 ) x number variables
upper bound size variables domain (Dechter, 2003). Therefore, solving
COP done time O(n2+3 ). expression Eq. 4 dominates O(n2+3 ),
time complexity extracting plan optimal solution COP (see
proof (I) Theorem 2), overall complexity algorithm polytree-k-indegree
given Eq. 4. since Pb = O(1), conclude complexity
polytree-k-indegree polynomial description size .

3.3 Towards Practically Efficient Special Cases

polytree-k-indegree algorithm Pb polynomial, rather involved complexity exponential In(CG()). quite possible efficient algorithms
Pb , fragments devised. Indeed, show simple algorithm Pb problems already appeared literature different context,
never checked (if all) provides cost-optimal solutions.
TreeDT algorithm preferential reasoning tree-structured CP-nets (Boutilier,
Brafman, Domshlak, Hoos, & Poole, 2004), turns straightforward adaptation planning problems always provides cost-optimal solutions problems
uniform-cost actions. algorithm depicted Figure 9, hard verify
time complexity linear length generated plan iteratively
removing parts problem safely ignored later steps,
applying value-changing action lowest (in causal graph) variable
action exists.
Theorem 4 Given problem uniform-cost actions n state variables,
(I) algorithm tree-uniform-cost finds plan solvable,
(II) algorithm tree-uniform-cost finds plan , plan cost-optimal,

221

fiKatz & Domshlak

procedure tree-uniform-cost( = (V, A, I, G))
takes problem uniform-cost actions
returns cost-optimal plan solvable, fails otherwise
= hi, := I, V := V
loop
V := remove-solved-leafs(s, V )
V = return
else
find v V , Av A(s)
u Desc(v, V ) : Au A(s) =
found return failure
:= hai, := (s \ pre(a)) eff(a)
Figure 9: simple algorithm cost-optimal planning problems uniform-cost
actions. notation Desc(v, V ) stands subset V containing
descendants v CG(), A(s) stands set actions applicable
state s.

(III) time complexity tree-uniform-cost (n2 ).
Proof: Without loss generality, follows assume actions
unit-cost, is, plan , C() = ||.
(I) Straightforward reusing proof Theorem 11 Boutilier et al. (2004).
(II) Assume contrary plan provided tree-uniform-cost optimal,
is, exists plan | | < ||. particular, implies existence
variable v | v | < |v |. semantics planning implies
| v | |v | (v + 1)

(5)

v = 1 G[v] specified, 0 otherwise. Likewise, since causal graph CG()
forms directed tree, exists variable v satisfying Eq. 5 that, descendants u v CG() holds:
| u | |u |
(6)
Let Ch(v) set immediate descendants v CG(). construction
tree-uniform-cost, that:
1. Ch(v) = , |v | v , contradicts Eq. 5 | v | non-negative
quantity definition.
2. Otherwise, Ch(v) 6= , then, construction tree-uniform-cost, exists
u Ch(v) changing value |u | times requires changing value v
least |v | v times. words, action sequence applicable
222

fiTractable Cost-Optimal Planning

|u | |u | |v | < |v | v . However, Eq. 6
| u | |u |, thus | v | least |v | v . This, however, contradicts
Eq. 5.
Hence, proved | v | |v |, contradicting assumption | | < ||.
(III) Implied Theorems 12 13 Boutilier et al. (2004).



requirement Theorem 4 actions cost essential.
example shows general case algorithm tree-uniform-cost longer
cost-optimal. Consider = (V, A, I, G) V = {v, u}, = {bv , bu }, G = {bv , wu },
= {a1 , a2 , a3 , a4 }
eff(a1 ) = {wv }, pre(a1 ) = {bv }
eff(a2 ) = {bv }, pre(a2 ) = {wv }
eff(a3 ) = {wu }, pre(a3 ) = {bu , wv }
eff(a4 ) = {wu }, pre(a4 ) = {bu , bv }
C(a1 ) = C(a2 ) = C(a3 ) = 1
C(a4 ) = 4
problem, tree-uniform-cost algorithm returns = ha4 C() = 4,
optimal plan = ha1 , a3 , a2 C( ) = 3.

4. Cost-Optimal Planning P(1) Uniform-Cost Actions
section provide polynomial time algorithm cost-optimal planning P(1)
problems uniform-cost actions. begin showing problems exhibit
interesting property, exploit property devising planning-to-COP scheme
problems, prove correctness complexity algorithm.
begin providing useful notation. Given P(1) problem = (V, A, I, G),
v V , w pred(v), {bv , wv }, {bw , ww }, a| denote
action eff(a)[v] = pre(a)[w] = . Since 1-dependent, applicability
a| prevailed value w. important keep mind a|
notation; action a| may belong action set .
4.1 Post-Unique Plans P(1) Problems
proceed introducing notion post-unique action sequences plays
key role planning-to-COP compilation here.
Definition 4 Let = (V, A, I, G) UB problem instance. action sequence
called post-unique if, pair actions a, , eff(a) = eff(a )
= . is, changes variable certain value along performed
(type ) action. (possibly empty) set post-unique plans
denoted P pu () (or simply P pu , identity clear context).
223

fiKatz & Domshlak

notion post-unique action sequences closely related notion postunique planning problems (Backstrom & Klein, 1991; Backstrom & Nebel, 1995),
considerably weaker latter. action sets post-unique planning problems
allowed contain two actions effect, Definition 4 poses similar
restriction action sequences, underlying planning problems. Still,
property post-uniqueness plans strong. general, solvable problems UB
may exhibit post-unique plans all. Turns out, however, problems P(1)
much case.
Theorem 5 solvable P(1) problem = (V, A, I, G), P pu () 6= . Moreover, actions uniform-cost, P pu () contains least one cost-optimal
plan.
Proof: correctness second claim immediately implies correctness
first one, focus proof second claim. Given P(1) problem = (V, A, I, G)
uniform-cost actions, plan = ha1 , . . . , , construct sequence
actions that:
post-unique plan ,
C( ) = C().
construction two-step. First, v V , map subsequence v =
hai1 , . . . , aik post-unique sequence actions v = hai1 , . . . , aik i. Note
indexes i1 , . . . , ik action elements v global indexes actions
along , exactly indexes used marking elements constructed
sequences v . constructed sequences v1 , . . . , vn , merge
single actions sequence , show valid plan . two properties
required hold immediately | | = ||, post-uniqueness
implied individual post-uniqueness per-variable components v .
mapping subsequences v desired sequences v variables v
performed top-down, consistently topological ordering causal graph CG().
top-down processing allows us assume that, constructing v , subsequences
w w pred(v) already constructed. Given that, mapping v =
hai1 , . . . , aik corresponding v , distinguish following three cases.
(1) subsequence v already post-unique.
case, simply set v v . addition, construct following sets
ordering constraints. First, set binary relation >v action elements
v = hai1 , . . . , aik
>v = {ai < aj | ai , aj v , < j}.
(7)
immediate Eq. 7 >v strict total order elements v >v
simply follows action indexing inherited v plan via v .
Now, w pred(v), set binary relation >v,w elements v
w
>v,w =

(S




v ,aj w

{ai < aj | < j} {aj < ai | j < i},

,

pre(a)[w] specified v
otherwise

.

(8)

224

fiTractable Cost-Optimal Planning

w pred(v), relation >v,w defined Eq. 8 strict total order
domain ordering constraints elements v w subset
constraints induced total-order plan (corresponding) actions
v w . reason, Eqs. 7 8, that,
w pred(v), >v >v,w strict total order union elements v
w .

Eqs. 7-8 derive linearization >v wpred(v) >v,w defines
sequence actions applicable respect {v} pred(v). addition,
|v | = |v | implies action sequence provides v value G[v] latter
specified.
(2) subsequence v post-unique, actions v prevailed
value single parent w pred(v).
Since v post-unique, v case contain instances least three
action types {abv |bw , abv |ww , awv |bw , awv |ww }. Thus, particular, must
(a) |w | 1,
(b) {bw , ww }, awv | , abv | v .
Given that, set v = hai1 , . . . , aik
1 j k :

aij

(
awv | ,
=
abv | ,

j odd
.
j even

post-uniqueness v , well applicability respect v straightforward. ordering constraints >v set according Eq. 7. Likewise,
w = haj1 , . . . , ajl i, set



= bw

Sai v {ai < aj1 },

>v,w =
(9)
= ww , l = 1
ai v {ai > aj1 },



{a > aj } {a < aj }, = ww , l > 1
ai v



1



2

Finally, ordering constraints >v,w rest parents w pred(v) \ {w}
set empty sets.

relation >v identical case (1), thus strict total order
elements v . Eq. 9, easy verify >v,w also strict partial
order union elements v w . Finally, elements v
identically constrained respect elements w , >v >v,w
forming strict partial order union elements v w . (For
parents w pred(v), simply >v >v,w = >v .)

Eqs. 7 9 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v}pred(v). addition,
|v | = |v | implies action sequence provides v value G[v] latter
specified.
225

fiKatz & Domshlak

(3) subsequence v post-unique, actions v prevailed
one parent v.
setting case particular implies pair vs parents {u, w}
pred(v) awv | , abv | v {bu , wu }, {bw , ww }. Given that,
set v
(
awv | , j odd
,
1 j k : aij =
abv | , j even
and, similarly case (2), post-uniqueness v , applicability respect
v straightforward.
well, ordering constraints >v set according Eq. 7. Likewise,
w = haj1 , . . . , ajl i, u = haj1 , . . . , aj i, set >v,w according Eq. 9 above,
l
>v,u according Eq. 10 below.

>v,u




= bu

Sai v {ai < aj1 },

= wu , l = 1
=
ai v {ai > aj1 },



{a > } {a < }, = wu , l > 1
j1
j2


v

(10)



Finally, ordering constraints >v,w rest parents w pred(v) \ {u, w}
set empty sets.
relation >v identical cases (1-2), relations >v,u >v,w
effectively identical relation >v,w case (2). Thus, >v >v,u
>v >v,w forming strict partial orders unions elements v u ,
v w , respectively.

Eqs. 7, 9, 10 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v}pred(v). addition,
|v | = |v | implies action sequence provides v value G[v] latter
specified.
last step, prove that, v V w pred(v),
>w >v,w strict partial order union elements w v .
- >v,w constructed according Eq. 8, >w >v,w subset constraints
induced plan (corresponding v w ) actions v w .
- Otherwise, >v,w constructed according Eq. 9 (in case, equivalent)
Eq. 10, >v,w (i) addresses two elements w , (ii) orders elements
consistently >w .
cases, argued properties >w >v,w implies forms strict partial order
union elements v w .
now, specified sequences v , orders >v induced sequences,
orders >v,w , proved >v >v,w >w >v,w form strict partial orders
226

fiTractable Cost-Optimal Planning

domains. construction allows us apply Theorem 1 (considered
sets) sequences v orders >v >v,w , proving
[
[
>=
(>v
>v,w )
vV

wpred(v)

forms strict partial order union v1 , . . . , vn . Putting thing together,
implies linearization > plan , post-uniqueness
subsequences v1 , . . . , vn implies P pu (). Moreover, optimal plan
, | | = || implies optimality .

4.2 Construction, Correctness, Complexity
main impact Theorem 5 planning-to-COP scheme uniform-cost P(1)
restrict attention post-unique plans only. Given that, constraint
optimization problem COP = (X , F) uniform-cost problem = (V, A, I, G) P(1)
specified follows.
variable set X contains variable xv planning variable v V ,
variable xw
v edge (w, v) CG(). is,
X = X V X E,
X V = {xv | v V }

(11)

X E = {xw
v | (w, v) CG()}
variable xv X V , domain Dom(xv ) consists goal-valid prefixes
E
w
(v). variable xw
v X , domain Dom(xv ) consists triples integers
[w , b , ]] satisfying Eq. 12.
Dom(xv ) = [(v)]
Dom(xw
v ) = {[[w , b , ]] | w , b {0, 1}, 0 n}

(12)

semantics Eq. 12 follows. Let {w1 , . . . , wk } arbitrary fixed ordering
pred(v). xv takes value v Dom(xv ), v forced provide sequence

values v . turn, xw
v takes value [w , b , ]], corresponds number
value changes v, w = 1 (b = 1) forces subset parents {w1 , . . . , wi } pred(v)
support (that is, prevail) changes v wv (respectively, bv ), w = 0
(b = 0) relieves subset parents {w1 , . . . , wi } responsibility.
variable x X , set F contains non-negative, real-valued function x
scope


{xv },
x = xv , k = 0



{x , xwk },
x = xv , k > 0
v v
(13)
Qx =
w1 , k > 0
w
1

},
x
=
x
,
x
{x
w

v
1
v


{xwj , xwj1 , x }, x = xwj , 1 < j k
v
v
v
wj
pred(v) = {w1 , . . . , wk } (and k = 0 means pred(v) = ). Proceeding
specifying functional components F COP , first, xv pred(v) = ,
227

fiKatz & Domshlak

v [(v)], set xv (v )


0,



1,
xv (v ) =

|v | 1,



,

|v | = 1,
(|v | = 2) (awv Av ),
(|v | > 2) (awv , abv Av ),
otherwise

(14)

turn, planning variable v V pred(v) = {w1 , . . . , wk }, k > 0,
function xv set


0,



1,
xv (v , [w , b , ]]) =

|v | 1,



,

(|v | = 1) ([[w , b , ]] = [0, 0, 0]]),
(|v | = 2) ([[w , b , ]] = [1, 0, 1]]),
(|v | > 2) ([[w , b , ]] = [1, 1, |v | 1]]),
otherwise

(15)

functions xv capture the, marginal actions Av , cost providing sequence
v value changes v , given (in case Eq. 15) parents v ready
support value changes. specifying remaining functional components use
indicator function specified Eq. 16.
8
>
>0,
>
>
>
>
0,
>
>
>
>
>
<0,
([[w , b , ]] , w ) = 0,
>
>
>
0,
>
>
>
>
>
0,
>
>
>
:,

w = 0, b = 0,
w = 1, b = 0, (awv |bw Av ) ((|w | > 1) (awv |ww Av )),
w = 0, b = 1, (abv |bw Av ) ((|w | > 1) (abv |ww Av )),
w = 1, b = 1, (awv |bw , abv |bw Av ) ((|w | > 1) (awv |ww , abv |ww Av )),
w = 1, b = 1, |w | , awv |bw , abv |ww Av ,
w = 1, b = 1, |w | > , awv |ww , abv |bw Av ,
otherwise
(16)

semantics that, planning variable v V , w pred(v),
([[w , b , ]] , w ) Dom(xw
v ) Dom(xw ), ([[w , b , ]] , w ) = 0 value sequence
w w support changes v wv (if w = 1) changes v bv (if
b = 1), value changes v . Given indicator function , v V ,
functional component xwv 1 specified
xwv 1 ([[w , b , ]] , w1 ) = ([[w , b , ]] , w1 ) ,

(17)

rest functions xwv 2 , . . . , xwv k specified follows. 2 k,




value function xwj combination [w , b , ]] Dom(xw
v ), [w , b , ]
v
w
Dom(xv i1 ), wi Dom(xwi ) = [(wi )] specified
`



] , w , b , , wj =
xw
[w , b , ]
v

( `

[w w , b b , ]] , wj ,


228

= w w b b
otherwise

(18)

fiTractable Cost-Optimal Planning

procedure polytree-1-dep-uniform( = (V, A, I, G))
takes problem P(1) uniform-cost actions
returns cost-optimal plan solvable, fails otherwise
create set variables X Eqs. 11-12
create set functions F = {x | x X } scopes Eq. 13
x X
specify x according Eqs. 14-18
endfor
P
set COP := (X , F) global objective min F (X )
x :=
P solve-tree-cop(COP )
F (x) = return failure
P
extract plan x C() = F (x)
return
Figure 10: Algorithm cost-optimal planning P(1) problems uniform-cost actions.

finalizes construction COP , construction constitutes first three
steps algorithm polytree-1-dep-uniform Figure 10(a). subsequent steps
algorithm conceptually similar polytree-k-indegree algorithm Section 3,
major difference plan reconstruction routines. hard verify
Eqs. 11-13, fact causal graph P(1) forms polytree
(i) variable x X , |Dom(x)| = poly(n),
(ii) tree-width cost network F 3,
(iii) optimal tree-decomposition COP cost network given topological
ordering causal graph consistent (arbitrary yet fixed time
COP construction) orderings planning variables parents causal
graph.
illustration, refer reader Figure 4 (p. 211) Figure 4(a) depicts
causal graph problem P(1), Figure 4(c) depicts cost network
corresponding COP . top-most variables cliques cost network correspond
functional components COP .
proceed proving correctness complexity polytree-1-depuniform algorithm.
Theorem 6 Let P(1) problem uniform-costs actions, COP = (X , F)
corresponding constraint optimization problem, x optimal assignment X
P
F (x) = .
(I) < , plan cost reconstructed x time polynomial
description size .

(II) plan, < .
229

fiKatz & Domshlak

Proof Sketch: proof Theorem 6 Appendix A. p. 249. overall flow
proof similar proof Theorem 2, yet details much different.
main source proofs complexity that, proving (I), must distinguish
several cases based roles taken (up to) two parents supporting
value changes variable question.

Theorem 7 Cost-optimal planning P(1) uniform cost actions tractable.
Proof: Given planning problem P(1) uniform cost actions, show
corresponding constraint optimization problem COP constructed solved time
polynomial description size .
Let n number state variables . polytree-1-dep-uniform, first construct constraint optimization problem COP (n2 ) variables X domain sizes
bounded O(n), (n2 ) functional components F, defined three
COP variables. construction linear size resulting COP, thus
accomplished time O(n5 ).
Applying COP tree-decomposition along scopes functional components F, arrive equivalent, tree-structured constraint optimization problem
(n2 ) variables domains size O(n3 ). tree-structured COP solved
time O(xy 2 ) x number variables upper bound size
variables domain (Dechter, 2003). Therefore, solving COP done time
O(n8 ). dominates time complexity constructing COP , time
complexity extracting plan optimal solution COP (see proof (I)
Theorem 6), overall complexity algorithm polytree-1-dep-uniform O(n8 ),
therefore polynomial description size .


5. Cost-Optimal Planning P(1) General Action Costs
consider cost-optimal planning P(1) problems without constraints actions
cost. Theorem 5 Section 4 shows solvable P(1)
problem least one post-unique plan, possible plan cost-optimal
, Example 1 affirms possibility.
Example 1 Let = hV, A, I, Gi P(1) problem instance variables V = {v1 , . . . , v5 },
= {0, 0, 0, 0, 0}, G = {v1 = 0, v2 = 1, v3 = 1}, actions depicted Figure 11a.
polytree causal graph shown Figure 11b, easy verify table
Figure 11a P(1).
Ignoring non-uniformity action costs, problem post-unique costoptimal plan = ha4 a2 a1 a5 a3 a4 i. However, considering action costs last
column Figure 11a, cost optimal plan, = ha4 a2 a1 a5 a3 a7 a6
C( ) = 16 < 24 = C(). Note plan post-unique changes
value v3 1 using actions a4 a6 . fact, plan problem
least two action instances change value v3 1. However, cheap
action a6 cannot applied twice requires v4 = 1, action a5
sets v3 = 0 cannot applied a6 a5 requires v4 = 0, action
230

fiTractable Cost-Optimal Planning


a1
a2
a3
a4
a5
a6
a7

v1

v2

1
0
1

0

pre(a)
v3 v4

v5

v1

v2

eff(a)
v3 v4

1
1
0
0
1
0

1
0
0

1
0
1

0
1
0

1
(a)

v5

C(a)
1.0
1.0
1.0
10.0
1.0
1.0
1.0

v4

>>
>>
>>


v5
v3


v1


v2
(b)

Figure 11: Action set causal graph problem Example 1.
effect. Therefore, post-unique plan problem invoke twice
action a4 , thus cost least 2 C(a4 ) = 20.
Fortunately, show solvable P(1) problem guaranteed costoptimal plan satisfying certain relaxation action sequence post-uniqueness still
allows us devise (more costly polytree-1-dep-uniform) planning-to-COP scheme
general P(1) problems.
5.1 Post-3/2 Plans P(1) Problems
proceed introducing notion post-3/2 property action sequences
relaxes post-uniqueness property exploited previous section.
Definition 5 Let = (V, A, I, G) UB problem instance. action sequence
called post-3/2 if, v V , v , exist 6= {bv , wv }, parent
w pred(v), , {bw , ww }, {bu , wu | u pred(v)}, {a| , a| , a| }.
is, changes variable done using three types actions
prevailed two parents, u different w, different actions
prevailed w perform different value changes v.
(possibly empty) set post-3/2 plans denoted P 3/2 () (or simply
P 3/2 , identity clear context).
illustrate rather involved definition post-3/2 plans, consider following four
action sequences four actions each. value changes made sequences
changes variable v pred(v) = {x, y, z}.
hawv |bx abv |wx awv |bx abv |by post-3/2 uses three types actions,
prevailed one two parents x, y.
hawv |bx abv |by awv |bx abv |by post-3/2 uses two types actions,
prevailed one two parents x, y.
231

fiKatz & Domshlak

hawv |bx abv |by awv |bx abv |bz post-3/2 uses three types actions,
prevailed one three parents x, y, z.
hawv |bx abv |wx awv |by abv |by post-3/2 uses four types actions,
prevailed one two parents x, y.
hard verify post-3/2 relaxation post-uniquenessif plan
post-unique, post-3/2, necessarily way around. Turns that,
P(1) problem , relaxed property guaranteed satisfied least one
cost-optimal plan .
Theorem 8 every solvable P(1) problem = (V, A, I, G), plan set P 3/2 () contains least one cost-optimal plan.
Proof Sketch: proof Theorem 8 Appendix A, pp. 255-278. proof
flow-wise similar proof Theorem 5, technically much involved.
provide sketch proof. note, however, many building blocks
proof used correctness proof planning-to-COP algorithm (notably,
Theorem 9).
Given P(1) problem = (V, A, I, G), cost-optimal plan , construct
post-3/2 plan , C( ) = C(). nutshell, first, v V , map
subsequence v = ha1 , . . . , ak sequence actions v = ha1 , . . . , ak
(i) satisfy post-3/2 property, (ii) C(v ) C(v ). Then, merge constructed
sequences {v }vV , show valid plan . two properties
required hold immediately C( ) = C(), post-3/2
implied per-variable components v post-3/2.
variable v V , pred(v) = , set v = v . turn, variable
v V pred(v) 6= , given {w }wpred(v) , |w | = |w | + 1, let ai ith
cheapest action changes variable v {bv , wv } prevailed value
{w }wpred(v) (that is, applicable given sequences values {w }wpred(v) respectively
obtained parents v). proof considers (in groups) possible settings
b
b
aw
aw
2 = awv | , a1 = abv | , a2 = abv | (that is, possible combinations
1 = awv | ,
{, , , } wpred(v) {bw , ww }) case-by-case basis. Specifically, cases correspond

(I) = {bw , ww }.
(II) {bw , ww }, {bu , wu }, w 6= u.
(III) = bw , = ww ; distinguish cases based w v .
(1) |v | = 2y + 1, |w | = 2x, |w | |v |.
(2) |v | = 2y + 1, |w | = 2x, |w | > |v |.
(3) |v | = 2y, |w | = 2x, |w | < |v |.
(4) |v | = 2y, |w | = 2x, |w | |v |.
(5) |v | = 2y + 1, |w | = 2x + 1, |w | < |v |.
(6) |v | = 2y + 1, |w | = 2x + 1, |w | |v |.
232

fiTractable Cost-Optimal Planning

(7) |v | = 2y, |w | = 2x + 1, |w | |v |.
(8) |v | = 2y, |w | = 2x + 1, |w | > |v |.
(IV) = ww , = bw ; well distinguish eight cases specification-wise
almost identical (III), lead different settings v .

5.2 Construction, Correctness, Complexity
Given post-3/2 action sequence variable v V , distinguish
following exhaustive roles parent w pred(v) respect v along .
R1 actions change value v supported value w.
is, {bw , ww }, v , {abv | , awv | }.
R2 actions change value v wv supported value
w, actions change value v bv supported another
value w.
is, 6= {bw , ww }, v , {abv | , awv | }.
R3 actions change value v wv supported value
w, none actions change value v bv supported w.
is, {bw , ww } 6 {bw , ww }, v , {abv | , awv | }.
R4 actions change value v bv supported value
w, none actions change value v wv supported w.
is, {bw , ww } 6 {bw , ww }, v , {abv | , awv | }.
R5 actions change value v wv supported value
w, actions change value v bv supported two values
w.
is, 6= {bw , ww }, v , {awv | , abv | , abv | }.
R6 actions change value v bv supported value
w, actions change value v wv supported two
values w.
is, 6= {bw , ww }, v , {abv | , awv | , awv | }.
R7 actions change value v wv supported value
w, actions change value v bv supported
another value w others supported another parent.
is, 6= {bw , ww } 6 {bw , ww }, v , {awv | , abv | , abv | }.
R8 actions change value v bv supported value
w, actions change value v wv supported
another value w others supported another parent.
is, 6= {bw , ww } 6 {bw , ww }, v , {abv | , awv | , awv | }.
233

fiKatz & Domshlak

R9 Part actions change value v bv supported
value w, none actions change value v wv supported
value w.
R10 Part actions change value v wv supported
value w, none actions change value v bv supported
value w.
R11 None actions supported w.
is, a| , 6 {bw , ww }.
given post-3/2 action sequence variable v V , parent v
performs one roles R1-R11 respect v along , roles R1-R10
performed one parents v. addition, sets roles
cannot simultaneously performed parents v respect v
action sequence , roles performed pairs. Specifically,
one roles {R1,R2,R5,R6} played parent w pred(v), R11
must played parents w pred(v) \ {w }.
R3/R7/R8 played parent w1 pred(v), R4/R9/R10, respectively,
must played parent w2 pred(v) \ {w1 }, R11 must played
parents w pred(v) \ {w1 , w2 }.
Considering variable v parents pred(v) lens eleven roles,
suppose aim assigning roles pred(v) considering one another
arbitrary order. Given aforementioned constraints role assignment,
step sequential process one following eight states,
whole process described state machine depicted Fig. 12.
S1 roles R1-R11 still available (to assigned parents v).
S2 roles {R3,R11} still available.
S3 roles {R4,R11} available.
S4 roles {R7,R11} available.
S5 roles {R8,R11} available.
S6 roles {R9,R11} available.
S7 roles {R10,R11} available.
S8 role R11 available.
Given language roles states, proceed specifying constraint optimization problem COP = (X , F) problem = (V, A, I, G) P(1).
follows, variable v V , assume fixed (arbitrarily chosen) numbering
{w1 , . . . , wk } pred(v) respect v.
234

fiTractable Cost-Optimal Planning

R11

start



@ABC
GFED
p7 S2 NNNN
ppp
R11NNN
p
p
NNN
p


p
p
NNN
pp
@ABC
GFED
p
NNN
p

5
S3
TTTT
p
jj
p
j
NNNR3
p
j

j
R4 pp

j
R11

j
TTTT
NN
jj
pp
j
p
j



j
p
TTTTR4 NNNN
p R3 jjjj
p
p
@ABC
GFED

j
TTTT NNNN

pppjjjjjj
eeeeee2 S4 YYYYYYYYYYYY
e
TT N
p
e
e
e
p
e
j
e
R9
YYYR7
R11pppjjjj
YYYYYYY TTTTNTNTNNN
eeee
e
e
e
e
e
p
j
e
YYYYYY TTTNN


ppjpjjjeeeeeee
YYYYT, ) '
R1,R2,R5,R6
jeeee
@ABC
@ABC
89:;
?>=<
/ GFED
/ GFED
S1 NTYNTYTYTYYYYYY
ejejejpejp2 5 7 S8
e
e
e
e
e
NNNTTT YYYYYYYR10
e
e
j
p
e
e
R8
R11
YYYYYY
NNNTTTT
eeeeee
jjjj pp
YYYYYYY


NNN TTTTT
eeeeeee R9 jjjjjjppppp
e
e
e

e

e

R7
e

, GFED
e
NNN TTTT
@ABC
jjjj ppppp
S5
TTTT
NNN
jjjR10
j
j
TTTT
pp
NNR8
j
R11 jjj
NNN
TTTT
ppp
p
p
NNN
TT)
jjjjjj
ppp
NNN
@ABC
GFED
S6
ppp
NNN
p
p
NNN
pp
NNN
pR11
ppp
'
p
@ABC
GFED
S7

Figure 12: State machine describing process sequential role assignment parents v (with respect v). transition labeled set roles, one
getting assigned parent v corresponding step.

1. Similarly uniform-cost case, variable set X contains variable xv
planning variable v V , variable xw
v edge (w, v) CG(). is,
X = XV XE
X V = {xv | v V }

(19)

X E = {xw
v | (w, v) CG()}
2. variable xv X V , domain Dom(xv ) consists possible valid prefixes
E
wi

(v). variable xw
v X , domain Dom(xv ) consists possible
quadruples satisfying Eq. 20.
Dom(xv ) = {v [(v)]}

wi
Dom(xv ) = [S, #w , #b , ]]

fi

fi 0 n, 0 #w , #b
2
fi
fi {S1, . . . , S8}

(20)

semantics Eq. 20 follows. Let {w1 , . . . , wk } arbitrary fixed ordering
pred(v). xv takes value v Dom(xv ), v forced provide sequence

values v . turn, xw
v takes value [S, #w , #b , ]], corresponds number
value changes v, #w #b correspond number value changes v wv
bv , respectively, performed actions prevailed values
235

R11

fiKatz & Domshlak

{w1 , . . . , wi }, state-component captures roles assigned
parents {w1 , . . . , wi }.
3. Similarly uniform-cost case, variable x X , set F contains nonnegative, real-valued function x scope


{xv },



{x , xwk },
v v
Qx =
w

{xv 1 , xw1 },



{xwj , xwj1 , x },
v
v
wj

x = xv , k = 0
x = xv , k > 0
1
x = xw
v ,k > 0
wj
x = xv , 1 < j k

(21)

pred(v) = {w1 , . . . , wk } (with k = 0 meaning pred(v) = ).
Proceeding specifying functional components F COP , first,
xv pred(v) = , v [v], set xv (v ) according Eq. 22.


0,
|v | = 1,



C(a ),
|v | = 2, awv Av ,
wv
xv (v ) =
(22)
|
|1
|
|1
v
v

2 C(awv ) + 2 C(abv ), |v | > 2, awv , abv Av ,



,
otherwise

turn, planning variable v V pred(v) = {w1 , . . . , wk }, k > 0, function
xv set Eq. 23.


|v | = 1, [S, #w , #b , ]] = [hhS8, 0, 0, 0]] ,
0,

ii
xv (v , [S, #w , #b , ]]) = 0,
|v | > 1, [S, #w , #b , ]] = S1, |v2|1 , |v2|1 , |v | 1 ,


, otherwise

(23)
semantics Eq. 23 simpleif value changes v required, trivially
support pred(v) v needed; otherwise, possible roles pred(v)
considered.
Now, proceed specifying generic function that, v V ,
w pred(v), (R, [S, #w , #b , ]] , w ) {R1, . . . , R10} Dom(xw
v ) Dom(xw ),
provides marginal actions Av cost w taking role R, role,
supporting #w changes v wv #b changes v bv , total changes v
needed. ease presentation, let (x1 , x2 , y1 , y2 ) denote cost action sequence
consisting x1 actions type awv |bw , x2 actions type awv |ww , y1 actions type abv |ww ,
y2 actions type abv |bw ,
(x1 , x2 , y1 , y2 ) = x1 C(awv |bw ) + x2 C(awv |ww ) + y1 C(abv |ww ) + y2 C(abv |bw )

(24)

notation v,w probably appropriate semantics , adopt
latter shortness identity v w always clear context.
Eqs. 25-34 specify (R, [S, #w , #b , ]] , w ) R {R1, . . . , R10}. semantics (R, [S, #w , #b , ]] , w ) capture minimal cumulative cost actions Av achieve #w #b (out ) value changes v support
236

fiTractable Cost-Optimal Planning

parent w playing role R respect v. example, role R3 means supporting
actions change value v wv , Eq. 27 gives us minimal cost
support terms cumulative cost supported actions Av . minimal
costs taken relevant cases proof Theorem 8, notably
(Eq. 25) Case (I).
(Eq. 26) Cases {(III), (IV )}.{2, 4, 6, 8}.
(Eq. 27) Case (II), cost actions change value v wv .
(Eq. 28) Case (II), cost actions change value v bv .
(Eq. 29) Cases {(III), (IV )}.{1, 3, 5, 7}.a, minimal cost.
(Eq. 30) Cases {(III), (IV )}.{1, 3, 5, 7}.b, minimal cost.
(Eq. 31) Cases {(III), (IV )}.{1, 3, 5, 7}.a, cost actions prevailed one parent.
(Eq. 32) Cases {(III), (IV )}.{1, 3, 5, 7}.b, cost actions prevailed one parent.
(Eq. 33) residue cases {(III), (IV )}.{1, 3, 5, 7}.a. (Together Eq. 31 gives
us full cost changing v required.)
(Eq. 34) residue cases {(III), (IV )}.{1, 3, 5, 7}.b. (Together Eq. 31 gives
us full cost changing v required.)
8
>
(#w , 0, 0, #b ),
>
>
)
(
>
<
(#w , 0, 0, #b ),
(R1, [S, #w , #b , ]] , w ) = min
,
>
(0, #w , #b , 0)
>
>
>
:,

8
>
(#w , 0, #b , 0),
>
>
)
(
>
<
(#w , 0, #b , 0),
(R2, [S, #w , #b , ]] , w ) = min
,
>
(0, #w , 0, #b )
>
>
>
:,

|w | > 1, #w = 2 , #b = 2

(25)

otherwise

|w | = 2, #w = 2 , #b = 2
|w | > 2, #w = 2 , #b = 2

(26)

otherwise

8
#w (
C(awv |bw ),
>
>
)
>
<
#w C(awv |bw ),
,
(R3, [S, #w , #b , ]] , w ) = min
>
#w C(awv |ww )
>
>
:
,
8
C(abv |bw ),
>
>#b (
)
>
<
#b C(abv |bw ),
(R4, [S, #w , #b , ]] , w ) = min
,
>
#b C(abv |ww )
>
>
:
,

237

|w | = 1, #w = 2 , #b = 2

|w | = 1, #w = 2 , #b = 0
|w | > 1, #w = 2 , #b = 0

(27)

otherwise
|w | = 1, #w = 0, #b = 2
|w | > 1, #w = 0, #b = 2
otherwise

(28)

fiKatz & Domshlak

8
(
)
>
(y + 1, 0, x 1, x + 1),
>
>
min
,
>
>
>
(0, + 1, x + 1, x 1)
>
>
>
>
>
(
)
>
>
>
(y,
0,
x,


x),
>
>
>
min
,
>
>
(0, y, x + 1, x 1)
>
>
>
>
>
>
>
>
>
>
(y, 0, 1, 1),
>
>
>
>
>
>
>
>
>
>
>
(0, y, 1, 1),
>
>
>
>
>
<
(
)
(R5, [S, #w , #b , ]] , w ) =
(y + 1, 0, x, x),
>min
,
>
>
>
(0, + 1, x + 1, x 1)
>
>
>
>
>
>
>
>
>
(y + 1, 0, 1, 1),
>
>
>
>
>
>
>
>
>
>
>
>
(0, + 1, 1, 1),
>
>
>
>
>
>
>
(
)
>
>
>
(y, 0, x, x),
>
>
min
,
>
>
>
(0, y, x, x)
>
>
>
>
>
:,

(R6, [S, #w , #b , ]] , w ) =

(
)
8
>
(x, + 1 x, y, 0),
>
>
min
,
>
>
(y + 1 x, x, 0, y)
>
>
>
>
(
)
>
>
>
>
(x, x, y, 0),
>
>
min
,
>
>
>
(y x + 1, x 1, 0, y)
>
>
>
>
>
>
>
>
>
(1, 1, y, 0),
>
>
>
>
>
>
>
>
>
>
>
<(1, 1, 0, y),
>
(
)
>
>
>
(x + 1, x, y, 0),
>
>
>
min
,
>
>
(y x + 1, x, 0, y)
>
>
>
>
>
>
>
>
>
>
(1, y, 0, y),
>
>
>
>
>
>
(
)
>
>
>
>
(x, x, y, 0),
>
>min
,
>
>
(y x, x, 0, y)
>
>
>
>
>
:
,

238

= 2y + 1, |w | = 2x, 1 < x y,
#w = + 1, #b =
= 2y, |w | = 2x, 1 < x < y,
#w = # b =
= 2y, |w | = 2, 1 < y,
#w = # b =
= |w | = 2y, 1 < y,
#w = # b =
= 2y + 1, |w | = 2x + 1, 1 < x < y,
#w = + 1, #b =
= 2y + 1, |w | = 3, 1 < y,
#w = + 1, #b =
= |w | = 2y + 1, 1 < y,
#w = + 1, #b =
= 2y, |w | = 2x + 1, 1 x < y,
#w = # b =
otherwise
(29)

= 2y + 1, |w | = 2x, 1 x y,
#w = + 1, #b =
= 2y, |w | = 2x, 1 < x < y,
#w = # b =
= 2y, |w | = 2, 1 < y,
#w = # b =
= |w | = 2y, 1 < y,
#w = # b =
= 2y + 1, |w | = 2x + 1, 1 x < y,
#w = + 1, #b =
= |w | = 2y + 1, 1 y,
#w = + 1, #b =
= 2y, |w | = 2x + 1, 1 x < y,
#w = # b =
otherwise
(30)

fiTractable Cost-Optimal Planning

(R7, [S, #w , #b , ]] , w ) =

(
)
8
>
(y + 1, 0, x 1, 0),
>
>
min
,
>
>
>
(0, + 1, 0, x 1)
>
>
>
>
>
>
>
>
>
>
>
>
>
(y, 0, x, 0),
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
(0, y, 0, x 1),
>
>
>
>
>
>
>
>
>
<
>
>
>
(y + 1, 0, x, 0),
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>(0, + 1, 0, x 1),
>
>
>
>
>
>
>
>
>
>
(
)
>
>
>
(y, 0, x, 0),
>
>
>
min
,
>
>
(0, y, 0, x)
>
>
>
>
>
>
:
,

(R8, [S, #w , #b , ]] , w ) =

(
)
8
>
(x, 0, y, 0),
>
>
min
,
>
>
>
(0, x, 0, y)
>
>
>
>
>
>
>
>
>
>
>
>
(x, 0, y, 0),
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>(0, x 1, 0, y),
>
>
>
<
>
>
>
>
>
>
(x + 1, 0, y, 0),
>
>
>
>
>
>
>
>
>
>
>
>
>
(0, x, 0, y),
>
>
>
>
>
>
>
(
)
>
>
>
(x, 0, y, 0),
>
>min
>
,
>
>
(0, x, 0, y)
>
>
>
>
>
>
:
,

239

= 2y + 1, |w | = 2x, 1 < x y,
#w = + 1, #b = x 1
C(awv |bw ) < C(awv |ww ),
= 2y, |w | = 2x, 1 x < y,
#w = y, #b = x
C(awv |bw ) C(awv |ww ),
= 2y, |w | = 2x, 1 < x y,
#w = y, #b = x 1
C(awv |bw ) < C(awv |ww ),
= 2y + 1, |w | = 2x + 1, 1 x < y,
#w = + 1, #b = x
C(awv |bw ) C(awv |ww ),
= 2y + 1, |w | = 2x + 1, 1 < x y,
#w = + 1, #b = x 1
= 2y, |w | = 2x + 1, 1 x < y,
#w = y, #b = x
otherwise
(31)

= 2y + 1, |w | = 2x, 1 x y,
#w = x, #b =
C(awv |bw ) < C(awv |ww ),
= 2y, |w | = 2x, 1 x < y,
#w = x, #b =
C(awv |bw ) C(awv |ww ),
= 2y, |w | = 2x, 1 < x y,
#w = x 1, #b =
= 2y + 1, |w | = 2x + 1, 1 x < y,
#w = x + 1, #b =
= 2y + 1, |w | = 2x + 1, 1 x y,
#w = x, #b =
= 2y, |w | = 2x + 1, 1 x < y,
#w = x, #b =
otherwise

(32)

fiKatz & Domshlak

8
>
C(abv |ww ),
|w | = 1, #w = 0, #b < 2
>
>#b (
)
>
<
#b C(abv |ww ),
(R9, [S, #w , #b , ]] , w ) = min
,
|w | > 1, #w = 0, #b < 2
>
#b C(abv |bw )
>
>
>
:,
otherwise
8
>
#w C(awv |bw ),
|w | = 1, #w < 2 , #b = 0
>
>
(
)
>
<
#w C(awv |bw ),
(R10, [S, #w , #b , ]] , w ) = min
,
|w | > 1, #w < 2 , #b = 0
>
#
w C(awv |ww )
>
>
>
:,
otherwise

(33)

(34)

specified function , use it, particular, specifying functional
component xwv 1 Eq. 35. equation actually emulates movements state
machine v Figure 12 terminal state S8.

xwv 1 ([[S, #w , #b , ]] , w1 ) =




(R1, [S, #w , #b , ]] , w1 ),







(R2, [S, # , # , ]] , ),


w
w1
b


min



(R5, [S, #w , #b , ]] , w1 ),







(R6, [S, # , # , ]] , )


w
w1
b








(R3, [S, #w , #b , ]] , w1 ),










(R4, [S, #w , #b , ]] , w1 ),









(R7, [S, #w , #b , ]] , w1 ),





(R8, [S, #w , #b , ]] , w1 ),










(R9, [S, #w , #b , ]] , w1 ),










(R10, [S, #w , #b , ]] , w1 ),















0,













,













, = S1,

= S2,
= S3,
= S4,
= S5,

(35)

= S6,
= S7,
= S8,
#w = 0,
#b = 0
otherwise

proceed rest functional components xwv 2 , . . . , xwv k .
w
w
2 j k, [S, #w , #b , ]] Dom(xv j ), [S , #w , #b , ] Dom(xv j1 ),
w Dom(xwj ) = [wj ], value xwj set according Eq. 36. equation also
v

240

fiTractable Cost-Optimal Planning

emulates movements state machine v Figure 12each sub-case Eq. 36
deals certain transition state machine.


xwj ( [S, #w , #b , ]] , , #w , #b , , wj ) =
v
8
8
(R1, [S, #w #w , #b #b , ]] , wj ),
>
>
>
>
>
>
>
< (R2, [S, # # , # # , ]] , ),
>
>
w
wj
b
w
b
>
>
min
>


>
>
(R5,
[
S,
#

#
,
#

#
,
]
]
,

>
w
wj ),
b
>
w
b
>
>
>
:
>


>
(R6,
[
S,
#

#
,
#

#
,
]
]
,

w
wj )
b
>
w
b
>
>
>
>
>
>
>
>
(R4, [S, #w #w , #b #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>


>
>
>(R3, [S, #w #w , #b #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
(R9, [S, #w #w , #b #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>(R10, [S, # # , # # , ]] , ),
>
wj
w
b
>
w
b
>
>
>
>
>
>
>
>
>
>
>
(R7, [S, #w #w , #b #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>(R8, [S, #w # , #b # , ]] , w ),
>
w
<
b
j
>
>
>
>
>
>
(R3, [S, #w #w , #b #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>
>
(R4, [S, #w #w , #b #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>
(R7, [S, #w #w , #b #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>
>
(R8, [S, #w #w , #b #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>


>
>(R9, [S, #w #w , #b #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>(R10, [S, # # , # # , ]] , ),
>
w
wj
>
b
w
b
>
>
>
>
>
>
>
>
>
>
>
0,
>
>
>
>
>
>
>
:
,

9
>
>
>
=
>
>
>
;

,

= S1, = S8, = ,
#w #w , #b #b
= S1, = S2, = ,
#w #w , #b #b
= S1, = S3, = ,
#w #w , #b #b
= S1, = S4, = ,
#w #w , #b #b
= S1, = S5, = ,
#w #w , #b #b
= S1, = S6, = ,
#w #w , #b #b
= S1, = S7, = ,
#w #w , #b #b

(36)

= S2, = S8, = ,
#w #w , #b #b
= S3, = S8, = ,
#w #w , #b #b
= S4, = S8, = ,
#w #w , #b #b
= S5, = S8, = ,
#w #w , #b #b
= S6, = S8, = ,
#w #w , #b #b
= S7, = S8, = ,
#w #w , #b #b
= , = ,
#w = #w , #b = #b
otherwise

finalizes construction COP , construction constitutes first three
steps algorithm polytree-1-dep Figure 13(a). subsequent steps algorithm
conceptually similar polytree-1-dep-uniform algorithm Section 4.
241

fiKatz & Domshlak

procedure polytree-1-dep( = (V, A, I, G))
takes problem P(1)
returns cost-optimal plan solvable, fails otherwise
create set variables X Eqs. 19-20
create set functions F = {x | x X } scopes Eq. 21
x X
specify x according Eqs. 22-36
endfor
P
set COP := (X , F) global objective min F (X )
x :=
P solve-tree-cop(COP )
F (x) = return failure
P
extract plan x C() = F (x)
return
Figure 13: Algorithm cost-optimal planning P(1) problems.
hard verify Eqs. 19-21, fact causal graph P(1) forms
polytree
(i) variable x X , |Dom(x)| = poly(n),
(ii) tree-width cost network F 3,
(iii) optimal tree-decomposition COP cost network given topological
ordering causal graph consistent (arbitrary yet fixed time
COP construction) orderings planning variables parents causal
graph.
Theorem 9 Let P(1) problem, COP = (X , F) P
corresponding constraint
optimization problem, x optimal assignment X F (x) = .
(I) < , plan cost reconstructed x time polynomial
description size .

(II) plan, < .
Proof Sketch: proof Theorem 9 Appendix A, pp. 278-286; provide
aPsketch proofs skeleton. prove (I), given COP solution x = {v1 , . . . , vn }
F (x) = < , construct plan C() = . done
constructing action sequences v v V , well constructing partial orders
elements sequences variable parents. construction distinguishes numerous possibilities (joint) role taken parents
variable question. constructed orders local actions sequences
combined linearized
1) P
action sequence valid
P (using Theorem
P
=
plan C() =
C(
)
=

(x)
v
v
F (x) = . prove (II),
vV
vV
given solvable problem P
irreducible post-3/2 plan P
, construct COP
assignment x F (x ) = C(). Then, F (x ) C() < ,
obtain claimed < .

242

fiTractable Cost-Optimal Planning

Theorem 10 Cost-optimal planning P(1) tractable.
Proof: Given planning problem P(1), show corresponding constraint
optimization problem COP constructed solved time polynomial description size . Let n number state variables . polytree-1-dep, first
construct constraint optimization problem COP (n2 ) variables X domain
sizes bounded either O(n) O(n3 ) (for COP variables representing state variables causal graph edges, respectively). number functional components COP
(n2 ), defined one variable domain size O(n) either one two
variables domain sizes O(n3 ). construction linear size resulting
COP, thus accomplished time O(n9 ).
Applying COP tree-decomposition clusters scopes functional
components F, arrive equivalent, tree-structured constraint optimization problem
(n2 ) variables domains size O(n7 ). tree-structured COP solved
time O(xy 2 ) x number variables upper bound size
variables domain (Dechter, 2003). Therefore, solving COP done time O(n16 ).
dominates time complexity constructing COP , time complexity
extracting plan optimal solution COP (see proof (I) Theorem 9),
overall complexity algorithm polytree-1-dep O(n16 ), therefore polynomial
description size .


6. Drawing Limits k-Dependence
read far, reader may wonder whether 1-dependence strong enough
property make cost-planning tractable even complex polytree
forms causal graph. last technical section discuss limits power
k-dependence (and, particular, 1-dependence), present negative results
draw boundary tractable intractable k-dependent UB problems.
Theorem 11 show cost-optimal planning already hard Sbb (1)
problem class, is, class 1-dependent UB problems inducing directed-path
singly connected causal graphs in- out-degrees bounded
constant. result stresses connection undirected cycles
causal graph complexity various planning tasks, first
discussed Brafman Domshlak (2003).
Theorem 12 show even non-optimal planning hard Sbb (2) problem
class. results suggests 1-dependence rather special case k-dependence
terms connection computational tractability. However, given (still)
empty entries Figures 5a 5b, analysis criticality 1-dependence
needed.
Theorem 11 Cost-optimal planning Sbb (1) NP-complete.
Proof: membership NP implied Theorem 2 Brafman Domshlak (2003).
proof NP-hardness polynomial reduction well-known Vertex Cover
problem (Garey & Johnson, 1978). problem Vertex Cover is: given undirected
243

fiKatz & Domshlak

graph G = (V, E), find minimal-size subset V V edge E least
one two end-nodes V . Given undirected graph G = (V, E), let planning
problem G = hVG , AG , IG , GG defined follows.
VG = {v1 , . . . , v|V| , u1 , . . . , u|E| }, and, vi , uj , Dom(vi ) = Dom(uj ) = {T, F },

IG = {vi = F | vi VG } {ui = F | ui VG },
GG = {ui = | ui VG },

Actions AG = AV AE , AV = {av1 , . . . , av|V| }
pre(avi ) = {vi = F }, eff(avi ) = {vi = }, C(avi ) = 1

(37)

AE = {au1 , au1 , . . . , au|E| , au|E| }
pre(aui ) = {ui = F, vi1 = },
pre(aui ) = {ui = F, vi2 = },
eff(aui ) = eff(aui ) = {ui = },

(38)

C(aui ) = C(aui ) = 0
variables vi1 , vi2 correspond endpoints edge corresponding
variable ui .
Given construction G , easy see (i) plan G provides
us vertex cover V G |V | = C() vice versa, thus (ii) costoptimal plans G (and plans G ) provide us minimal vertex covers
G. topology causal graph G required, 1-dependence G
immediate Eqs. 37-38. finalizes proof NP-completeness cost-optimal

planning Sbb (1).
Theorem 12 Planning Sbb (2) NP-complete.
Proof: proof basically given construction proof Brafman
Domshlak (2003) Theorem 2. polynomial reduction 3-SAT
planning S. Observing 3-SAT remains hard even variable participates
five clauses formula (Garey & Johnson, 1978), reduction
Brafman Domshlak 3-SAT formulas effectively planning Sbb (2),
accomplishes proof claim.


7. Conclusion Future Work
One key conclusions Bylander (1994) seminal article planning complexity
. . . analysis strongly suggests thing set generallyapplicable domain independent properties lead efficient planning. later works
by, e.g., Backstrom Nebel (1995), Jonsson Backstrom (1998a), Brafman
Domshlak (2003, 2006) shown conclusion pessimistic. considering
local restrictions actions, also global restrictions action sets, well
244

fiTractable Cost-Optimal Planning

structural properties problems, works managed identify numerous
domain-independent tractable fragments classical planning. said that, palette
known tractable fragments planning remains limited, even less known
tractable optimal planning. difference theoretical complexity regular
optimal planning general case (Bylander, 1994), many classical planning
domains provably easy solve, hard solve optimally (Helmert, 2003).
work studied complexity cost-optimal classical planning
propositional state variables unary-effect actions. discovered novel problem fragments optimization tractable, identified certain conditions differentiate tractable intractable problems. results based exploiting certain structural syntactic characteristics planning problems. Almost
tractability results based proof technique connects certain tools
planning tractable constraint optimization, believe technique
interesting due clear evidence robustnessour different algorithms exploit
proof technique, much different manners.
results suggest discovering new islands tractability optimal planning
hopeless, strongly believe indeed case. particular, ongoing
work devoted questions left open paper (see Figure 5),
well planning problems simple causal graphs multi-valued (in contrast
propositional) state variables. fact, recently reported preliminary positive
results latter direction (Katz & Domshlak, 2007). Interestingly, recent results
presented context potential customer tractability results,
namely, context homomorphism abstractions admissible heuristics general
planning heuristic search.
Acknowledgments
research supported part Israel Science Foundations grants 2008100
2009589, well C. Wellner Research Fund. thank Adele Howe anonymous reviewers whose attentive comments helpful suggestions greatly improved
paper.

Appendix A. Proofs
Theorem 1 Let G polytree vertices V = {1, . . . , n}, pred(i) V denote
immediate predecessors G. V , let Oi finite set objects associated
vertex i, sets O1 , . . . , pairwise disjoint. V , let >i
strict partial order Oi , and, j pred(i), let >i,j strict partial order
Oi Oj .
If, V, j pred(i), transitively closed >i >i,j >j >i,j induce
(strict) partial orders Oi Oj , transitively closed

> =

[

iV



>i
245

[

jpred(i)



>i,j

fiKatz & Domshlak

=



iV

Oi .

Proof: follows, oi denote arbitrary object Oi . Assume
contrary >i >i,j >j >i,j (strict) partial orders, yet > so.
is, exists pair objects oi , oj hold oi > oj oj > oi .
construction >, a, possibly empty, path vertices
j undirected graph induced G. Since G polytree, know
undirected path
= i0 i1 . . . im1 im = j
(39)
j unique. Thus, must cycle >
: oi = o1i0 < . . . < oxi00 < o1i1 < . . . < oxi11 < . . . . . . < o1im < . . . < oximm = oj
: oi = o1i0 > . . . > oyi00 > o1i1 > . . . > oyi11 > . . . . . . > o1im > . . . > oyimm = oj

(40)

where, 0 k m, xk 1 yk 1, step chains
directly implied local relation >l >l,l constructing >.
Without loss generality, assume cycle > induced lengthwise minimal among cycles >. particular, implies
(i) 0 k m, 1 xk , yk 2 (one object Oik required connect
local relations >ik1 >ik+1 , two elements Oik required
>ik transitively closed),
(ii) pair objects , , 6= , unless = = oi = = oj ,
(or otherwise would shorter cycle )
(iii) pair objects , , >l (and >l,l ) implies >l (respectively,
>l,l o), otherwise, again, would shorter cycle .
First, let us show least one chains contains least one internal
element. Assume, contrary, contain internal elements. = j,
oi >i oi (where oi = oj ) oi >i oi , contradicting assumption >i
partial order. (If >i partial order, neither >i >i,j .) Otherwise, 6= j,
either pred(j) j pred(i). Assuming latter, (oi > oj ) (oi > oj ) implies
(oi >i,j oj ) (oi >i,j oj ), contradicting assumption >i,j partial order.
Given that, let us prove oximm 6= oyimm , contradicting assumption
chains Eq. 40 exist. case-by-case basis possible combinations
xm , ym , length-minimality cycle implies four cases
consider.
[xm = 2, ym = 2 ] case, Eq. 40 implies o1im >im o2im = o2im >im o1im . transitivity
>im implies o1im > o1im , contradicting assumption minimality
cycle .
[xm = 1, ym = 1 ] Eq. 39 either im1 pred(im ) im pred(im1 ).
xm1
ym1
.
>im ,im1 o1im = o1im >im ,im1 oim1
im1 pred(im ), Eq. 40 implies oim1
ym1
xm1
transitivity >im ,im1 implies oim1 > oim1 , contradicting assumption
246

fiTractable Cost-Optimal Planning

minimality cycle . Otherwise, im pred(im1 ), Eq. 40 implies
ym1
xm1
oim1
>im1 ,im o1im = o1im >im1 ,im oim1
. Again, transitivity >im1 ,im
ym1
xm1
implies oim1 > oim1 , contradicting assumption minimality cycle .
[xm = 2, ym = 1 ] case well, Eq. 39 implies either im1 pred(im )
ym1
im pred(im1 ). im1 pred(im ), Eq. 40 implies oim1
>im ,im1 o1im =
ym1
> o1im , contrao2im >im o1im . Then, transitivity >im >im ,im1 implies oim1
dicting assumption minimality cycle . Otherwise, im pred(im1 ),
ym1
Eq. 40 implies oim1
>im1 ,im o1im = o2im >im o1im . Then, transitivity
ym1
>im >im1 ,im implies oim1 > o1im , contradicting assumption minimality
cycle .
[xm = 1, ym = 2 ] case similar previous case xm = 2, ym = 1, mutatis
mutandis.

Theorem 2 Let planning problem Pb , COP = (X , F) corresponding
constraint optimization problem, x Dom(X ) optimal solution COP
P
F (x) = .
(I) < , plan cost reconstructed x time polynomial
description size .

(II) plan, < .

Proof:
P
(I) Given COP solution x = {v1 , . . . , vn } F (x) = < , construct
plan C() = .
First, variable v V pred(v) = , let sequence v actions Av
defined
(

|v | = 1
v =
,
(41)
|v |1
1
av . . . av
otherwise
where, 1 j |v | 1,

(
abv , j even
,
ajv =
awv , j odd

(42)

eff(abv ) = {bv }, eff(awv ) = {wv }. Eq. 3 v (v ) < , immediately
(i) {awv } Av |v | 2, {abv , awv } Av |v | > 2, (ii) C(v ) = v (v ).
Now, purpose gets clear below, let binary relation >v action elements
v defined transitive closure {avj1 < ajv | 1 < j |v | 1}. Clearly, >v
constitutes strict total ordering elements v .
Next, non-root variable v V pred(v) = {w1 , . . . , wk }, construct
graph Ge (v) respect w1 , . . . , wk , determine Ge (v) minimal-cost path
247

fiKatz & Domshlak

|v | 1 edges source node hb1w1 b1wk i. existence path implied
v (v , w1 , . . . , wk ) < . construction Ge (v) also know that, 1 j |v |
1, j-th edge path node labeled hw1 [l1j1 ] wk [lkj1 ]i node
labeled hw1 [l1j ] wk [lkj ]i, 1 l k, li0 = 1 lij1 lij .
that, let sequence v actions Av defined Eq. 41, with, 1 j |v | 1,
eff(ajv ) = {v [j + 1]}
n

pre(ajv ) = v [j], w1 [l1j ], w2 [l2j ], . . . , wk [lkj ]

(43)

| |1

Note {a1v , . . . , av v } Av implied construction Ge (v) presence
considered minimal-cost path it.
Now, similarly case root variables, let binary relation >v action
elements v defined transitive closure {avj1 < ajv | 1 < j |v | 1}.
well, >v constitutes strict total ordering elements v . addition,
parent wi v, let binary relation >v,wi union action elements v
+
wi defined transitive closure >
v,wi >v,wi , turn defined
j

li 1
j

j
awi < av | 1 j |v | 1, li > 1
>v,wi =


(44)
lij
j
+
j
>v,wi = av < awi | 1 j |v | 1, li < |wi | .
hard verify Eq. 44 that, v V w pred(v),
>v,w constitutes strict partial ordering, transitively closed >v >v,w
>w >v,w . Given that,
definition w = ha1w . . . alw i, polytree structure causal
graph CG(), restricting preconditions effects aiw variables
{v} pred(v), pre(aiw ) = {bw }, eff(aiw ) = {ww } odd, pre(aiw ) =
{ww }, eff(aiw ) = {bw } even. 1 k, Eq. 43
lj 1

j

eff(a
wi ) pre(av ). Eq. 44 derive linearization >v
wpred(v) >v,w defines sequence actions applicable respect {v}
pred(v). addition, construction graph Ge (v) implies action
sequence provides v value G[v] latter specified.

polytree structure causal graph CG() Theorem 1 together imply
transitively closed relation
[
[
>=
(>v
>v,w )
vV

wpred(v)

strict partial order union action elements v1 , . . . , vn .
Putting thing together, implies linearization > constitutes valid
plan cost
X
X
C() =
C(v ) =
v (x),
vV

vV

248

fiTractable Cost-Optimal Planning

exactly prove. also note plan extraction step
algorithm polytree-k-indegree corresponds exactly construction along Eqs. 4144, providing us polynomial time concrete cost-optimal plan corresponding
optimal solution COP .
(II) prove solvable, must < . Assume
contrary case. Let solvable planning problem, let
irreducible plan . Given , let x = {v1 , . . . , vn } COP assignment
|vi | = |vi | 1. Note x well-defined (that is, 1 n,
vi [ (vi )])
P definition (vi ), Corollary 1, irreducible. Let
us
show

F (x ) C(), contradicting assumption = due
P
F (x ) C() < .
First, variable v pred(v) = , Eq. 3 immediately implies v (x ) C(v ).
Next, non-root variable v V pred(v) = {w1 , . . . , wk }, consider graph
Ge (v) constructed respect w1 , . . . , wk . Let {a1 , . . . , a|v | } actions v
numbered order appearance along v . Let {yw1 (1), . . . , ywk (1)} denote
prevail condition a1 ywi (1) time-stamped earliest appearance
1 }. Now, 2 j | |, set {y (j), . . . , (j)}
along wi , is, ywi (1) {b1wi , ww
v
w1
wk

prevail condition ai ywi (j) time-stamped lowest possible
time index along wi satisfying ywi (j 1) come ywi (j) along wi . Given

(i) v complete order-preserving restriction v-changing actions Av ,
| |

(ii) sequence time-stamped prevail conditions {{yw1 (j), . . . , ywk (j)}}j=1v constructed above,
(iii) |v | = |v | 1 construction x ,
Ge (v) contains path
hb1w1 b1wk hyw1 (1) ywk (1)i . . . hyw1 (|v |) ywk (|v |)i
cost path C(v ) < . However, constructive definition v
algorithm polytree-k-indegree, v (x ) cost minimal-cost path
|v | 1 edges Ge (v) originated hb1w1 b1wk i, thus v (x ) C(v ). latter
argument valid planning variables v V , thus
X
X
C(v ) = C(),
(x )
F

vV

prove.



Theorem 6 Let P(1) problem uniform-costs actions, COP = (X , F)
corresponding
constraint optimization problem, x optimal assignment X
P
(x)
=
.
F
249

fiKatz & Domshlak

(I) < , plan cost reconstructed x time polynomial
description size .
(II) plan, < .

Proof:
P
(I) Given COP solution x F (x) = < , construct plan
C() = . construct plan
1. Traversing planning variables topological ordering causal graph CG(),
associating variable v sequence v Av .
2. Merging constructed sequences v1 , . . . , vn desired plan .
variable xv X , let v denote value provided x xv . First,
variable v V pred(v) = , let sequence v actions Av defined
(

|v | = 1
v =
,
(45)
|v |1
1
otherwise
av . . . av
where, 1 j |v | 1,

(
abv , j even
,
ajv =
awv , j odd

(46)

eff(abv ) = {bv }, eff(awv ) = {wv }. Eq. 14 v (v ) < ,
immediately (i) {awv } Av |v | 2, {abv , awv } Av |v | > 2, (ii)
C(v ) = v (v ). Let binary relation >v action elements v defined
transitive closure {avj1 < ajv | 1 < j |v | 1},


>v = {ajv < ajv | 1 j < j |v | 1}

(47)

Clearly, >v constitutes strict total ordering elements v , making v applicable sequence actions provides v value G[v] latter specified.
Next, variable v V pred(v) 6= , let pred(v) = {w1 , . . . , wk } numbered
according ordering used constructing COP . Likewise, wi pred(v),

let [w (i), b (i), (i)]] value provided x xw
v . Given that, let pair indexes
0 hwi, hbi k defined


0, w (k) = 0,
(48)
hwi = 1, w (1) = 1,


j, w (j 1) < w (j), 2 j k


0, b (k) = 0,
hbi = 1, b (1) = 1,


j, b (j 1) < b (j), 2 j k
250

(49)

fiTractable Cost-Optimal Planning

words, hwi captures smallest 1 j k w (j) = 1, 0,
j all; semantics hbi similar, mutatis mutandis.
Informally, next-coming construction action sequence v state
variable v, hwi hbi indicate parents prevailing value changes v wv
bv , respectively, along v . Note Eqs. 48-49 well-defined because, 2 j k,
Eq. 18 implies
w (j 1) w (j) b (j 1) b (j) (j 1) = (j).
Given notation, action sequence v partial orders >v,w1 , . . . , >v,wk
constructed follows.
[ hwi = 0, hbi = 0 ] case, constructed plan perform value changes
v, thus v set empty action sequence, and, consequently, >v
>v,w set empty sets.
[ hwi > 0, hbi = 0 ] case, constructed plan perform exactly one value
change v (from bv wv ), thus v set contain exactly one action a1v
eff(a) = {wv },
(
{bv , bwhwi }, awv |bw Av
hwi
pre(a1v ) =
(50)
{bv , wwhwi }, otherwise
Note a1v well-defined, < Eq. 16 together imply {awv |bw , awv |bw }
hwi
hwi
Av 6= (see case (2) Eq. 16). outcomes Eq. 50, set >v = .
a1v = awv |bw , set
hwi

>v,whwi = {a1v < a1whwi | a1whwi whwi }

(51)

Otherwise, a1v = awv |ww , case (2) Eq. 16, awv |bw 6 Av , < ,
hwi
hwi
|whwi | > 1, thus |whwi | 1. Given that, set
>v,whwi = {a1whwi < a1v } {a1v < a2whwi | a2whwi whwi }

(52)

cases, easy verify >v >v,whwi >whwi constitutes strict total
order action elements v whwi . (In particular, trivially implies
>v >v,w >v,w >w strict partial orderings domains.)

Eqs. 47, 51, 52 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v). addition, Eq. 12 implies action sequence provides v value G[v] latter
specified.
[ hwi > 0, hbi > 0, hwi = hbi ] case, constructed plan perform
one value change v, value changes performed (a
pair types of) actions prevailed value whwi . < ,
([[w (hwi), b (hwi), ]] , whwi ) = 0. specification case question (that is,
hwi = hbi > 0) thus implies one conditions cases (4-6) Eq. 16
hold. Given that, distinguish following four settings.
251

fiKatz & Domshlak

(1) {awv |bw , abv |bw } Av , v specified according Eq. 45,
hwi
hwi
action elements specified

aw |b
, odd
v whwi
aiv =
.
(53)
abv |bw , even
hwi

relation >v set according Eq. 47, >v,whwi set
>v,whwi = {aiv < a1whwi | aiv v , a1whwi whwi }

(54)

Finally, w pred(v) \ {whwi }, set >v,w = .
(2) Otherwise, {awv |ww , abv |ww } Av |whwi | > 1, |whwi | 1.
hwi
hwi
Given that, set v according Eq. 45, action elements
set

aw |w
, odd
v whwi
aiv =
.
(55)
abv |ww , even
hwi

relation >v set according Eq. 47, >v,whwi set

>v,whwi = {a1whwi < aiv | aiv v } {aiv < a2whwi | aiv v , a2whwi whwi }

(56)

Finally, w pred(v) \ {whwi }, set >v,w = .
(3) Otherwise, {awv |bw , abv |ww } Av , |whwi | |v |1, v specified
hwi
hwi
according Eq. 45, action elements specified

aw |b
, odd
v whwi

av =
.
(57)
abv |ww , even
hwi

relation >v set according Eq. 47, >v,whwi set
[
{aiv < ajwhwi | j} {ajwhwi < aiv | > j}
>v,whwi =

(58)

aiv v ,ajwhwi whwi

w pred(v) \ {whwi }, set >v,w = .
(4) Otherwise, {awv |ww1 , abv |bw1 } Av , |whwi | |v |, v specified
according Eq. 45, action elements specified

aw |w
, odd
v whwi
aiv =
.
(59)
abv |bw , even
hwi

relation >v set according Eq. 47, >v,whwi set
[
{aiv < ajwhwi | < j} {ajwhwi < aiv | j}
>v,whwi =
aiv v ,ajwhwi whwi

w pred(v) \ {whwi }, set >v,w = .
252

(60)

fiTractable Cost-Optimal Planning

four cases above, >v >v,whwi >whwi constitutes strict total order
elements v whwi .

Eqs. 47, 54, 56, 58, 60 derive linearization >v
wpred(v) >v,w defines sequence actions applicable respect {v}
pred(v). addition, Eq. 12 implies action sequence provides v value
G[v] latter specified.
[ hwi > 0, hbi > 0, hwi =
6 hbi ] case, constructed plan perform
one value change v, changes v wv bv performed (a pair
types of) actions prevailed value whwi whbi , respectively. < ,
([[w (hwi), b (hwi), ]] , whwi ) = ([[w (hbi), b (hbi), ]] , whbi ) = 0,
due respective satisfaction conditions cases (2) (3) Eq. 16.
Given that, distinguish following four settings5 .
(1) {awv |bw , abv |bw } Av , v specified according Eq. 45,
hwi
hbi
action elements specified

aw |b
, odd
v whwi
aiv =
.
(61)
abv |bw , even
hbi

relation >v action elements v set according Eq. 47,
relation >v,whwi action elements v whwi set
>v,whwi = {aiv < a1whwi | odd, aiv v , a1whwi whwi }

(62)

relation >v,whbi action elements v whbi set
>v,whbi = {aiv < a1whbi | even, aiv v , a1whbi whbi }

(63)

w pred(v) \ {whwi , whbi }, set >v,w = .
(2) Otherwise, {awv |ww , abv |bw } Av |whwi | > 1, |whwi | 1.
hwi
hbi
Given that, set v according Eq. 45, action elements
set

aw |w
, odd
v whwi
aiv =
.
(64)
abv |bw , even
hbi

relation >v set according Eq. 47, >v,whwi set
[
>v,whwi =
{a1whwi < aiv } {aiv < a2whwi | a2whwi whwi }

(65)

aiv v , odd

>v,whbi set
>v,whbi = {aiv < a1whbi | even, aiv v , a1whbi whbi }

(66)

w pred(v) \ {whwi , whbi }, set >v,w = .
5. details slightly different, four settings case conceptually similar
previously considered case hwi > 0, hbi > 0, hwi = hbi.

253

fiKatz & Domshlak

(3) Otherwise, {abv |ww , awv |bw } Av , |whbi | > 1, |whbi | 1.
hbi
hwi
Given that, v specified according Eq. 45, action elements
specified

aw |b
, odd
v whwi
aiv =
.
(67)
abv |ww , even
hbi

relation >v set according Eq. 47, >v,whwi set

>v,whwi = {aiv < a1whwi | odd, aiv v , a1whwi whwi }

(68)

>v,whbi set
[

>v,whbi =

aiv v ,

{a1whbi < aiv } {aiv < a2whbi | a2whbi whbi }

(69)

even

w pred(v) \ {whwi }, set >v,w = .
(4) Otherwise, {awv |ww , abv |ww } Av , |whwi | > 1, |whbi | > 1,
hwi
hbi
|whwi | 1 |whbi | 1. Given that, set v according
Eq. 45, action elements specified

aw |w
, odd
v whwi
aiv =
.
(70)
abv |ww , even
hbi

relation >v set according Eq. 47, >v,whwi set
>v,whwi =

[

{a1whwi < aiv } {aiv < a2whwi | a2whwi whwi }

(71)

[

{a1whbi < aiv } {aiv < a2whbi | a2whbi whbi }

(72)

aiv v ,

odd

>v,whbi set
>v,whbi =

aiv v , even

w pred(v) \ {whwi }, set >v,w = .
four cases above, >v >v,whwi >whwi >v >v,whbi >whbi
constitute strict total orders respective domains.
Eqs.
47, 62, 63, 65, 66, 68, 69, 71, 72 derive linearization
>v wpred(v) >v,w defines sequence actions applicable respect
{v} pred(v). addition, Eq. 12 implies action sequence provides v
value G[v] latter specified.
now, variable v V , specified action sequence v
order >v elements v . w pred(v), specified order >v,w ,
proved >v >v,w >w >v,w form strict partial orders domains,
254

fiTractable Cost-Optimal Planning


linearization >v wpred(v) >v,w defines sequence actions applicable
respect {v} pred(v) provides v value G[v] latter specified.
construction allows us apply Theorem 1 (considered sets) sequences v
orders >v >v,w , proving
[
[
>=
(>v
>v,w )
vV

wpred(v)

forms strict partial order union v1 , . . . , vn .
also note plan extraction step algorithm polytree-1-dep-uniform
corresponds exactly construction along Eqs. 45-72, providing us polynomial
time concrete cost-optimal plan corresponding optimal solution COP .
(II) prove solvable, must < . Assume contrary
case. Let solvable P(1) problem, let (using Theorem 5)
irreducible, post-unique plan . Given , let COP assignment x defined
follows.
1. COP variable xv , assignment x provides value v [(v)]
|v | = |v | + 1.

w w

2. COP variable xw
v , assignment x provides value wv , bv , |v | 1 ,
wwv = 1 action v changes value v wv (considering
pre-fixed ordering vs parents) preconditioned value
wj , j i, wwv = 0, otherwise. bwv defined similarly wwv , mutatis mutandis.
Eq. 14-18 directly that, v V , xv (x ) = |v |,
w pred(v), xwv (x ) = 0. Therefore,
X
X
C(v ) = C(),
(x ) =
F

vV

prove.



Theorem 8 every solvable P(1) problem = (V, A, I, G), plan set P 3/2 () contains least one cost-optimal plan.
Proof: Given P(1) problem = (V, A, I, G), cost-optimal plan , construct
sequence actions that:
post-3/2 plan ,
C( ) = C().
nutshell, first, v V , map subsequence v = ha1 , . . . , ak
sequence actions v = ha1 , . . . , ak (i) satisfy post-3/2 property, (ii)
C(v ) C(v ). Then, merge constructed sequences {v }vV , show
valid plan . two properties required hold
255

fiKatz & Domshlak

immediately C( ) = C(), post-3/2 implied per-variable
components v post-3/2.
variable v V , pred(v) = , set v = v
>v = {ai < aj | ai , aj v , < j}.

(73)

immediate Eq. 73 >v strict total order elements v .
turn, variable v V pred(v) 6= , given {w }wpred(v) ,
|w | = |w | + 1, let ai ith cheapest action changes variable v {bv , wv }
prevailed value {w }wpred(v) (that is, applicable given sequences
values {w }wpred(v) respectively obtained parents v). Let us focus

6
b
b
w
aw
1 = awv | , a2 = awv | , a1 = abv | , a2 = abv | (that is, {, , , }
wpred(v) {bw , ww }).
(I) = {bw , ww }, set

ai =

(

= 2j 1, j N
otherwise

awv |
abv |

(74)

addition, construct following sets ordering constraints. First, set
binary relation >v action elements v = ha1 , . . . , ak
>v = {ai < aj | ai , aj v , < j}.

(75)

immediate Eq. 75 >v strict total order elements v .
Likewise, w = haj1 , . . . , ajl i, set



= bw

Sai v {ai < aj1 },

(76)
>v,w =
= ww , l = 1
ai v {ai > aj1 },



{a > aj } {a < aj }, = ww , l > 1
ai v



1



2

Finally, ordering constraints >v,w rest parents w pred(v) \ {w}
set empty sets.

w pred(v), easy verify relation >v,w defined Eq. 76
strict total order domain. Also, Eqs. 75 76, that,
w pred(v), >v >v,w strict total order union elements v
w .

Eqs. 75-76 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 74 implies action sequence provides
v value G[v] latter specified.
6. possible actions exist, case case analysis proof transparently takes possibility account. Specifically, aw1 exist, variable v simply
unchangeable, meaning v = . Next, ab1 exist, v changed (from b
w), covered subcase (I). aw2 exist, ab2 exist, sub-cases
(a) cases {(III), (IV )}.{1, 3, 5, 7} possible. Similarly, ab2 exist, aw2 exist,
sub-cases (b) cases {(III), (IV )}.{1, 3, 5, 7} possible. Finally, aw2 ab2
exist, cases {(III), (IV )}.{1, 3, 5, 7} possible all.

256

fiTractable Cost-Optimal Planning

(II) {bw , ww } {bu , wu }, w 6= u, set
(
awv | = 2j 1, j N
ai =
abv | otherwise

(77)

case well, ordering constraints >v set according Eq. 75. Likewise,
w = ha1 , . . . , al i, u = ha1 , . . . , al i, set >v,w according Eq. 76 above,
>v,u according Eq. 78 below.




= bu

Sai v {ai < a1 },


>v,u =
(78)
= wu , l = 1
ai v {ai > a1 },



{a > } {a < }, = wu , l > 1
ai v



1



2

Finally, ordering constraints >v,w rest parents w pred(v) \ {u, w}
set empty sets.

relations >v case identical previous case, relations
>v,u >v,w effectively identical relation >v,w previous case. Thus,
>v >v,u >v >v,w forming strict partial orders unions
elements v u , v w , respectively.

Eqs. 75, 76, 78 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 77 implies action sequence provides
v value G[v] latter specified.
(III) = bw , = ww , distinguish cases based w v .
(1) |v | = 2y + 1, |w | = 2x, |w | |v |, construct two post-3/2
candidates v , assign v cheapest among two, proving
cost lower C(v ).
(a) changes v wv done using action aw
1 , largest
possible number changes bv done using action ab1 , remaining
changes bv done using action ab2 . candidate v , set

b

a1 = 2j, j N, j < x
(79)
ai = ab2 = 2j, j N, x j

w
a1 otherwise
cost case

b
b
(y + 1) C(aw
1 ) + (x 1) C(a1 ) + (y x + 1) C(a2 )

(80)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, u = ha1 , . . . , al i, set >v,w according Eq. 81,
>v,u according Eq. 82.
>v,w =

[

{ai < aj | j < 2x 1} {ai < a2x1 } {aj < ai | j < i, j < 2x 1}




v ,aj w


(81)

257

fiKatz & Domshlak

u pred(v) \ {w} set,


{a < a1 },


Sai v

{a > },
1
>v,u = Sai v





v {ai > a1 } {ai < a2 },



,

= bu
= wu , l = 1
= wu , l > 1

.

(82)

otherwise

hard verify relation >v,w defined Eq. 81 strict
total order domain. Suppose contrary i, j,
aj < ai ai < aj . first inequality either j < 2x 1
j = 2x 1, second j < i, j < 2x 1.
relations >v >v,u effectively identical case (II). Thus,
>v >v,u >v >v,w forming strict partial orders
unions elements v u , v w , respectively.
Eqs. 75, 81, 82 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 79 implies action sequence
provides v value G[v] latter specified.
(b) changes v bv done using action ab1 , largest
possible number changes wv done using action aw
1 , remaining
.


candidate
v , set
changes wv done using action aw
2

w

a1 = 2j 1, j N, j x
(83)
ai = aw
= 2j 1, j N, x < j + 1
2

b
a1 otherwise
cost case

w
b
x C(aw
1 ) + (y + 1 x) C(a2 ) + C(a1 )

(84)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, u = ha1 , . . . , al i, set >v,w according Eq. 85,
>v,u according Eq. 86.
[
{ai < aj | j} {aj < ai | j < i}
(85)
>v,w =
ai v ,aj w

u pred(v) \ {w} set,





ai v {ai < a1 },






{a > a1 },
>v,u = Sai v





ai v {ai > a1 } {ai < a2 },



,

= bu
= wu , l = 1
= wu , l > 1

.

(86)

otherwise

relation >v,w defined Eq. 85 strict total order domain.
relations >v >v,u effectively identical case (II). Thus,
258

fiTractable Cost-Optimal Planning

>v >v,u >v >v,w forming strict partial orders
unions elements v u , v w , respectively.
Eqs. 75, 85, 86 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 83 implies action sequence
provides v value G[v] latter specified.
Now, cost-optimal plan , v cannot contain + x actions
b
types aw
1 a1 totally. Suppose contrary v contain
b
least + x + 1 actions types aw
1 a1 . contains x
actions types. Let bw ww . . . bw sequence 2y + 1 values w
support cost-optimal plan v given w change value number
times. action type decrease needed length
sequence 2. Therefore x actions type
decrease length 2y 2x, left sequence
length 2y + 1 (2y 2x) = 2x + 1. Therefore w cannot support
b
+ x actions types aw
1 a1 . Now, suppose given cost-optimal
b
plan v v actions type aw
1 actions type a1 .
+ y+x

(87)


w
b
b
C(v ) C(aw
1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 )

(88)

(80) (84),
w
C(ab2 ) C(ab1 ) C(aw
2 ) C(a1 )

(89)

suppose contrary plan first case cost-optimal.
Eq. 88
w
b
b
C(aw
1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
b
b
(y + 1) C(aw
1 ) + (x 1) C(a1 ) + (y x + 1) C(a2 )


w
b
b
(y + 1 ) (C(aw
2 ) C(a1 )) < ( x + 1) (C(a2 ) C(a1 ))

(90)

Eq. 87 + 1 x + 1, together Eq. 89 contradicting
Eq. 90.
(84) (80),
w
b
b
C(aw
2 ) C(a1 ) C(a2 ) C(a1 )

(91)

suppose contrary plan second case cost-optimal.
Eq. 88
w
b
b
C(aw
1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
w
b
x C(aw
1 ) + (y x + 1) C(a2 ) + C(a1 )

259

fiKatz & Domshlak


w
(y ) (C(ab2 ) C(ab1 )) < ( x) (C(aw
2 ) C(a1 ))

(92)

Eq. 87 x, together Eq. 91 contradicting Eq. 92.
(2) |v | = 2y + 1, |w | = 2x, |w | > |v |, actions v set
(
awv |bw = 2j 1, j N
ai =
abv |ww otherwise

(93)

case well, ordering constraints >v set according Eq. 75.
Likewise, w = ha1 , . . . , a2x1 i, set >v,w according Eq. 85 above. Finally,
ordering constraints >v,w rest parents w pred(v) \ {u, w}
set empty sets.
relations >v >v,w identical previous case. Thus,
>v >v,w forming strict partial order union elements v
w .

Eqs. 75, 85 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 93 implies action sequence
provides v value G[v] latter specified.
(3) |v | = 2y, |w | = 2x, |w | < |v |, construct two post-3/2 candidates
v , assign v cheapest among two, proving cost
lower C(v ).
(a) changes v wv done using action aw
1 , largest
possible number changes bv done using action ab1 , remaining
changes bv done using action ab2 . candidate v , set

b

a2 = 2j, j N, j x
(94)
ai = ab1 = 2j, j N, x < j

w
a1 otherwise
cost case

b
b
C(aw
1 ) + x C(a1 ) + (y x) C(a2 )

(95)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 96.
[
>v,w =
{ai < aj | 2y 2x + j} {aj < ai | > 2y 2x + j}
ai v ,aj w

(96)
u pred(v) \ {w} set >v,u according Eq. 82. easy
verify relation >v,w defined Eq. 96 strict total order
domain. relations >v >v,u effectively identical previous
260

fiTractable Cost-Optimal Planning

case. Thus, >v >v,u >v >v,w forming strict partial orders
unions elements v u , v w , respectively.

Eqs. 75, 82, 96 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 94 implies action sequence
provides v value G[v] latter specified.
(b) changes v bv done using action ab1 , largest
possible number changes wv done using action aw
1 , remaining
w
changes wv done using action a2 . candidate v , set

w

a1

ai = aw
2

b
a1

= 2j 1, j N, j x
= 2j 1, j N, x < j
otherwise

(97)

cost case

w
b
x C(aw
1 ) + (y x) C(a2 ) + C(a1 )

(98)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 85 above.
u pred(v) \ {w} set >v,u according Eq. 86. relations >v ,
>v,w >v,u effectively identical previous case. Thus, again,
>v >v,u >v >v,w forming strict partial orders unions
elements v u , v w , respectively.

Eqs. 75, 85, 86 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 97 implies action sequence
provides v value G[v] latter specified.
Now, cost-optimal plan , v cannot contain + x actions
b
types aw
1 a1 totally. Suppose contrary v contain least
b
+ x + 1 actions types aw
1 a1 . contains x 1
actions types. Let bw ww . . . ww sequence 2y values w
support cost-optimal plan v given w change value number
times. action type decrease needed length
sequence 2. Therefore x 1 actions type
decrease length 2y 2x 2, left sequence
length 2y (2y 2x 2) = 2x + 2. Therefore w cannot support
b
+ x actions types aw
1 a1 . Now, suppose given cost-optimal
b
plan v v actions type aw
1 actions type a1 .
+ y+x

(99)


w
b
b
C(v ) C(aw
1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 )

261

(100)

fiKatz & Domshlak

(95) (98),
w
C(ab2 ) C(ab1 ) C(aw
2 ) C(a1 )

(101)

suppose contrary plan first case cost-optimal.
Eq. 100
w
b
b
C(aw
1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
b
b
C(aw
1 ) + x C(a1 ) + (y x) C(a2 )


w
b
b
(y ) (C(aw
2 ) C(a1 )) < ( x) (C(a2 ) C(a1 ))

(102)

Eq. 99 x, together Eq. 101 contradicting Eq. 102.
(98) (95),
w
b
b
C(aw
2 ) C(a1 ) C(a2 ) C(a1 )

(103)

suppose contrary plan second case cost-optimal.
Eq. 100
w
b
b
C(aw
1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
w
b
x C(aw
1 ) + (y x) C(a2 ) + C(a1 )


w
(y ) (C(ab2 ) C(ab1 )) < ( x) (C(aw
2 ) C(a1 ))

(104)

Eq. 99 x, together Eq. 103 contradicting Eq. 104.
(4) |v | = 2y, |w | = 2x, |w | |v |, actions v set
(
awv |bw = 2j 1, j N

ai =
abv |ww otherwise

(105)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 85 above. Finally, ordering
constraints >v,w rest parents w pred(v) \ {u, w} set empty
sets. relations >v >v,w identical previous case. Thus,
>v >v,w forming strict partial order union elements v
w .

Eqs. 75, 85 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 105 implies action sequence
provides v value G[v] latter specified.
(5) |v | = 2y + 1, |w | = 2x + 1, |w | < |v |, construct two post-3/2
candidates v , assign v cheapest among two, proving
cost lower C(v ).
262

fiTractable Cost-Optimal Planning

(a) changes v wv done using action aw
1 , largest
possible number changes bv done using action ab1 , remaining
changes bv done using action ab2 . candidate v , set

b

a2 = 2j, j N, j x
(106)
ai = ab1 = 2j, j N, x < j

w
a1 otherwise
cost case

b
b
(y + 1) C(aw
1 ) + x C(a1 ) + (y x) C(a2 )

(107)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x i, set >v,w according Eq. 96 above. u pred(v) \
{w} set >v,u according Eq. 82 above. relations >v , >v,w >v,u
effectively identical previous case. Thus, >v >v,u
>v >v,w forming strict partial orders unions elements v
u , v w , respectively.

Eqs. 75, 82, 96 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 106 implies action sequence provides v value G[v] latter specified.
(b) changes v bv done using action ab1 , largest
possible number changes wv done using action aw
1 , remaining
.


candidate
v , set
changes wv done using action aw
2

w

a1 = 2j 1, j N, j x j = + 1

(108)
ai = aw
= 2j 1, j N, x < j
2

b
a1 otherwise
cost case

w
b
(x + 1) C(aw
1 ) + (y x) C(a2 ) + C(a1 )

(109)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x i, set >v,w according Eq. 110.
>v,w =

[

ai v ,aj w

{ai < aj | j < 2x} {ai < a2x | 2y}
{aj < ai | j < i, j < 2x} {a2x < a2y+1 }

(110)

u pred(v) \ {w} set >v,u according Eq. 86 above.
easy verify relation >v,w defined Eq. 110 strict total
order domain. relations >v >v,u effectively identical
previous case. Thus, >v >v,u >v >v,w forming strict
partial orders unions elements v u , v w ,
respectively.
263

fiKatz & Domshlak


Eqs. 75, 86, 110 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 108 implies action sequence provides v value G[v] latter specified.
Now, cost-optimal plan , v cannot contain + x+ 1 actions
b
types aw
1 a1 totally. Suppose contrary v contain least
b
+ x + 2 actions types aw
1 a1 . contains x 1
actions types. Let bw ww . . . bw sequence 2y + 1 values w
support cost-optimal plan v given w change value number
times. action type decrease needed length
sequence 2. Therefore x 1 actions type
decrease length 2y 2x 2, left sequence
length 2y + 1 (2y 2x 2) = 2x + 3. Therefore w cannot support
b
+ x + 1 actions types aw
1 a1 . Now, suppose given
cost-optimal plan v v actions type aw
1 actions type
b
a1 .
+ y+x+1
(111)

w
b
b
C(v ) C(aw
1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 )

(112)

(107) (109),
w
C(ab2 ) C(ab1 ) C(aw
2 ) C(a1 )

(113)

suppose contrary plan first case cost-optimal.
Eq. 112
w
b
b
C(aw
1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
b
b
(y + 1) C(aw
1 ) + x C(a1 ) + (y x) C(a2 )


w
b
b
(y + 1 ) (C(aw
2 ) C(a1 )) < ( x) (C(a2 ) C(a1 ))

(114)

Eq. 111 + 1 x, together Eq. 113 contradicting
Eq. 114.
(109) (107),
w
b
b
C(aw
2 ) C(a1 ) C(a2 ) C(a1 )

(115)

suppose contrary plan second case cost-optimal.
Eq. 112
w
b
b
C(aw
1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
w
b
(x + 1 C(aw
1 ) + (y x) C(a2 ) + C(a1 )

264

fiTractable Cost-Optimal Planning


w
(y ) (C(ab2 ) C(ab1 )) < ( x 1) (C(aw
2 ) C(a1 ))

(116)

Eq. 111 x 1, together Eq. 115 contradicting
Eq. 116.
(6) |v | = 2y + 1, |w | = 2x + 1, |w | |v |, actions v set
(
awv |bw = 2j 1, j N

ai =
abv |ww otherwise

(117)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x i, set >v,w according Eq. 85 above. Finally, ordering constraints >v,w rest parents w pred(v) \ {u, w} set empty
sets. relations >v >v,w identical previous case. Thus,
>v >v,w forming strict partial order union elements v
w .

Eqs. 75, 85 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 117 implies action sequence
provides v value G[v] latter specified.
(7) |v | = 2y, |w | = 2x + 1, |w | |v |, construct two post-3/2
candidates v , assign v cheapest among two, proving
cost lower C(v ).
(a) changes v wv done using action aw
1 , largest
possible number changes bv done using action ab1 , remaining
changes bv done using action ab2 . candidate v , set

b

a2 = 2j, j N, j x
(118)
ai = ab1 = 2j, j N, x < j

w
a1 otherwise
cost case

b
b
C(aw
1 ) + x C(a1 ) + (y x) C(a2 )

(119)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x i, set >v,w according Eq. 96 above. u pred(v) \
{w} set >v,u according Eq. 82 above. relations >v , >v,w >v,u
effectively identical previous case. Thus, >v >v,u
>v >v,w forming strict partial orders unions elements v
u , v w , respectively.

Eqs. 75, 82, 96 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 118 implies action sequence provides v value G[v] latter specified.
265

fiKatz & Domshlak

(b) changes v bv done using action ab1 , largest
possible number changes wv done using action aw
1 , remaining
w
changes wv done using action a2 . candidate v , set

w

a1

ai = aw
2

b
a1

= 2j 1, j N, j x
= 2j 1, j N, x < j
otherwise

(120)

cost case

b
w
x C(aw
1 ) + (y x) C(a2 ) + C(a1 )

(121)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x i, set >v,w according Eq. 122.
>v,w =

[

{ai < aj | j < 2x} {aj < ai | j < i, j < 2x} {ai < a2x }

ai v ,aj w

(122)

u pred(v) \ {w} set >v,u according Eq. 86 above.
easy verify relation >v,w defined Eq. 122 strict total
order domain. relations >v >v,u effectively identical
previous case. Thus, >v >v,u >v >v,w forming strict
partial orders unions elements v u , v w ,
respectively.

Eqs. 75, 86, 122 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 120 implies action sequence provides v value G[v] latter specified.
Now, cost-optimal plan , v cannot contain + x actions
b
types aw
1 a1 totally. Suppose contrary v contain least
b
+ x + 1 actions types aw
1 a1 . contains x 1
actions types. Let bw ww . . . ww sequence 2y values w
support cost-optimal plan v given w change value number
times. action type decrease needed length
sequence 2. Therefore x 1 actions type
decrease length 2y 2x 2, left sequence
length 2y (2y 2x 2) = 2x + 2. Therefore w cannot support
b
+ x actions types aw
1 a1 . Now, suppose given cost-optimal
b
plan v v actions type aw
1 actions type a1 .
+ y+x

(123)


w
b
b
C(v ) C(aw
1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 )

266

(124)

fiTractable Cost-Optimal Planning

(119) (121),
w
C(ab2 ) C(ab1 ) C(aw
2 ) C(a1 )

(125)

suppose contrary plan first case cost-optimal.
Eq. 124
b
w
b
C(aw
1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
b
b
C(aw
1 ) + x C(a1 ) + (y x) C(a2 )


w
b
b
(y ) (C(aw
2 ) C(a1 )) < ( x) (C(a2 ) C(a1 ))

(126)

Eq. 123 x, together Eq. 125 contradicting Eq. 126.
(121) (119),
w
b
b
C(aw
2 ) C(a1 ) C(a2 ) C(a1 )

(127)

suppose contrary plan second case cost-optimal.
Eq. 124
w
b
b
C(aw
1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
w
b
x C(aw
1 ) + (y x) C(a2 ) + C(a1 )


w
(y ) (C(ab2 ) C(ab1 )) < ( x) (C(aw
2 ) C(a1 ))

(128)

Eq. 123 x, together Eq. 127 contradicting Eq. 128.
(8) |v | = 2y, |w | = 2x + 1, |w | > |v |, actions v set
(
awv |bw = 2j 1, j N

(129)
ai =
abv |ww otherwise
ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x i, set >v,w according Eq. 85 above. Finally, ordering constraints >v,w rest parents w pred(v) \ {u, w} set empty
sets. relations >v >v,w identical previous case. Thus,
>v >v,w forming strict partial order union elements v
w .

Eqs. 75, 85 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 129 implies action sequence
provides v value G[v] latter specified.
(IV) = ww , = bw , distinguish cases based w v .
267

fiKatz & Domshlak

(1) |v | = 2y + 1, |w | = 2x, |w | |v |, construct two post-3/2
candidates v , assign v cheapest among two, proving
cost lower C(v ).
(a) changes v wv done using action aw
1 , largest
possible number changes bv done using action ab1 , remaining
changes bv done using action ab2 . candidate v , set

b

a2 = 2j, j N, j x + 1
(130)
ai = ab1 = 2j, j N, x + 1 < j

w
a1 otherwise
cost case

b
b
(y + 1) C(aw
1 ) + (x 1) C(a1 ) + (y x + 1) C(a2 )

(131)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 132.
[
>v,w =
{ai < aj | < j} {aj < ai | j i}
(132)
ai v ,aj w

u pred(v) \ {w} set >v,u according Eq. 82.
easy verify relation >v,w defined Eq. 132 strict total
order domain. relations >v >v,u effectively identical
previous case. Thus, >v >v,u >v >v,w forming strict
partial orders unions elements v u , v w ,
respectively.

Eqs. 75, 82, 132 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 130 implies action sequence provides v value G[v] latter specified.
(b) changes v bv done using action ab1 , largest
possible number changes wv done using action aw
1 , remaining
.


candidate
v , set
changes wv done using action aw
2

w

a2 = 2j 1, j N, j x + 1
(133)
ai = aw
= 2j 1, j N, x + 1 < j + 1
1

b
a1 otherwise
cost case

w
b
x C(aw
1 ) + (y + 1 x) C(a2 ) + C(a1 )

(134)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 135.
[
>v,w =
{ai < aj | 2y 2x + 1 + j} {aj < ai | > 2y 2x + 1 + j}
ai v ,aj w

(135)

268

fiTractable Cost-Optimal Planning

u pred(v) \ {w} set >v,u according Eq. 86.
easy verify relation >v,w defined Eq. 135 strict total
order domain. relations >v >v,u effectively identical
previous case. Thus, >v >v,u >v >v,w forming strict
partial orders unions elements v u , v w ,
respectively.

Eqs. 75, 86, 135 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 133 implies action sequence provides v value G[v] latter specified.
Now, cost-optimal plan , v cannot contain + x actions
b
types aw
1 a1 totally. Suppose contrary v contain
b
least + x + 1 actions types aw
1 a1 . contains x
actions types. Let ww wb . . . ww sequence 2y + 1 values w
support cost-optimal plan v given w change value number
times. action type decrease needed length
sequence 2. Therefore x actions type
decrease length 2y 2x, left sequence
length 2y + 1 (2y 2x) = 2x + 1. Therefore w cannot support
b
+ x actions types aw
1 a1 . Now, suppose given cost-optimal
b
plan v v actions type aw
1 actions type a1 .
+ y+x

(136)


w
b
b
C(v ) C(aw
1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 )

(137)

(131) (134),
w
C(ab2 ) C(ab1 ) C(aw
2 ) C(a1 )

(138)

suppose contrary plan first case cost-optimal.
Eq. 137
w
b
b
C(aw
1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
b
b
(y + 1) C(aw
1 ) + (x 1) C(a1 ) + (y x + 1) C(a2 )


w
b
b
(y + 1 ) (C(aw
2 ) C(a1 )) < ( x + 1) (C(a2 ) C(a1 ))

(139)

Eq. 136 + 1 x + 1, together Eq. 138 contradicting
Eq. 139.
(134) (131),
w
b
b
C(aw
2 ) C(a1 ) C(a2 ) C(a1 )

269

(140)

fiKatz & Domshlak

suppose contrary plan second case cost-optimal.
Eq. 137
w
b
b
C(aw
1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
w
b
x C(aw
1 ) + (y x + 1) C(a2 ) + C(a1 )


w
(y ) (C(ab2 ) C(ab1 )) < ( x) (C(aw
2 ) C(a1 ))

(141)

Eq. 136 x, together Eq. 140 contradicting Eq. 141.
(2) |v | = 2y + 1, |w | = 2x, |w | > |v |, actions v set
(
awv |ww = 2j 1, j N
(142)
ai =
abv |bw otherwise
ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 132 above. Finally, ordering
constraints >v,w rest parents w pred(v) \ {u, w} set empty
sets. relations >v >v,w identical previous case. Thus,
>v >v,w forming strict partial order union elements v
w .

Eqs. 75, 132 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 142 implies action sequence
provides v value G[v] latter specified.
(3) |v | = 2y, |w | = 2x, |w | |v | + 1, construct two post-3/2
candidates v , assign v cheapest among two, proving
cost lower C(v ).
(a) changes v wv done using action aw
1 , largest
possible number changes bv done using action ab1 , remaining
changes bv done using action ab2 . candidate v , set

b

a1 = 2j, j N, j < x
(143)
ai = ab2 = 2j, j N, x j

w
a1 otherwise
cost case

b
b
C(aw
1 ) + (x 1) C(a1 ) + (y x + 1) C(a2 )

(144)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 132 above.
u pred(v) \ {w} set >v,u according Eq. 82.
relations >v , >v,w >v,u effectively identical previous case.
Thus, >v >v,u >v >v,w forming strict partial orders
unions elements v u , v w , respectively.
270

fiTractable Cost-Optimal Planning


Eqs. 75, 82, 132 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 143 implies action sequence provides v value G[v] latter specified.
(b) changes v bv done using action ab1 , largest
possible number changes wv done using action aw
1 , remaining
w
changes wv done using action a2 . candidate v , set

w

a2
ai = aw
1

b
a1

= 2j 1, j N, j x + 1
= 2j 1, j N, x + 1 < j
otherwise

(145)

cost case
w
b
(x 1) C(aw
1 ) + (y x + 1) C(a2 ) + C(a1 )

(146)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 135 above.
u pred(v) \ {w} set >v,u according Eq. 86.
relations >v , >v,w >v,u effectively identical previous case.
Thus, >v >v,u >v >v,w forming strict partial orders
unions elements v u , v w , respectively.S
Eqs. 75, 86, 135 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 145 implies action sequence provides v value G[v] latter specified.
Now, cost-optimal plan , v cannot contain + x 1
b
actions types aw
1 a1 totally. Suppose contrary v contain
b
least + x actions types aw
1 a1 . contains x
actions types. Let ww bw . . . bw sequence 2y values w
support cost-optimal plan v given w change value number
times. action type decrease needed length
sequence 2. Therefore x actions type
decrease length 2y 2x, left sequence
length 2y (2y 2x) = 2x, subsequence w , contradicting
fact w smaller size starts different
character. Therefore w cannot support + x actions types aw
1
b
a1 . Now, suppose given cost-optimal plan v v
b
actions type aw
1 actions type a1 .
+ y+x

(147)


w
b
b
C(v ) C(aw
1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 )

271

(148)

fiKatz & Domshlak

(144) (146),
w
C(ab2 ) C(ab1 ) C(aw
2 ) C(a1 )

(149)

suppose contrary plan first case cost-optimal.
Eq. 148
w
b
b
C(aw
1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
b
b
C(aw
1 ) + x C(a1 ) + (y x) C(a2 )


w
b
b
(y ) (C(aw
2 ) C(a1 )) < ( x) (C(a2 ) C(a1 ))

(150)

Eq. 147 x, together Eq. 149 contradicting Eq. 150.
(146) (144),
w
b
b
C(aw
2 ) C(a1 ) C(a2 ) C(a1 )

(151)

suppose contrary plan second case cost-optimal.
Eq. 148
w
b
b
C(aw
1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
w
b
x C(aw
1 ) + (y x) C(a2 ) + C(a1 )


w
(y ) (C(ab2 ) C(ab1 )) < ( x) (C(aw
2 ) C(a1 ))

(152)

Eq. 147 x, together Eq. 151 contradicting Eq. 152.
(4) |v | = 2y, |w | = 2x, |w | > |v | + 1, actions v set
(
awv |ww = 2j 1, j N

(153)
ai =
abv |bw otherwise
ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 132 above. Finally, ordering
constraints >v,w rest parents w pred(v) \ {u, w} set empty
sets. relations >v >v,w identical previous case. Thus,
>v >v,w forming strict partial order union elements v
w .

Eqs. 75, 132 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 153 implies action sequence
provides v value G[v] latter specified.
(5) |v | = 2y + 1, |w | = 2x + 1, |w | |v | + 1, construct two post-3/2
candidates v , assign v cheapest among two, proving
cost lower C(v ).
272

fiTractable Cost-Optimal Planning

(a) changes v wv done using action aw
1 , largest
possible number changes bv done using action ab1 , remaining
changes bv done using action ab2 . candidate v , set

b

a1 = 2j, j N, j < x
ai = ab2 = 2j, j N, x j
(154)

w
a1 otherwise
cost case

b
b
(y + 1) C(aw
1 ) + (x 1) C(a1 ) + (y x + 1) C(a2 )

(155)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x i, set >v,w according Eq. 156.
[
>v,w =
{ai < aj | < j < 2x} {aj < ai | j i, j < 2x} {ai < a2x }
ai v ,aj w

(156)

u pred(v) \ {w} set >v,u according Eq. 82.
easy verify relation >v,w defined Eq. 156 strict total
order domain. relations >v >v,u effectively identical
previous case. Thus, >v >v,u >v >v,w forming strict
partial orders unions elements v u , v w ,
respectively.

Eqs. 75, 82, 156 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 154 implies action sequence provides v value G[v] latter specified.
(b) changes v bv done using action ab1 , largest
possible number changes wv done using action aw
1 , remaining
w
changes wv done using action a2 . candidate v , set

w

a2 = 2j 1, j N, j x j = + 1
(157)
ai = aw
= 2j 1, j N, x < j
1

b
a1 otherwise
cost case

w
b
x C(aw
1 ) + (y + 1 x) C(a2 ) + C(a1 )

(158)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 159.
[
>v,w =
{ai < aj | < 2y 2x + j} {aj < ai | 2y 2x + j}
ai v ,aj w

(159)

273

fiKatz & Domshlak

u pred(v) \ {w} set >v,u according Eq. 86.
easy verify relation >v,w defined Eq. 159 strict total
order domain. relations >v >v,u effectively identical
previous case. Thus, >v >v,u >v >v,w forming strict
partial orders unions elements v u , v w ,
respectively.

Eqs. 75, 86, 159 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 157 implies action sequence provides v value G[v] latter specified.
Now, cost-optimal plan , v cannot contain + x+ 1 actions
b
types aw
1 a1 totally. Suppose contrary v contain least
b
+ x + 2 actions types aw
1 a1 . contains x 1
actions types. Let ww bw . . . ww sequence 2y + 1 values w
support cost-optimal plan v given w change value number
times. action type decrease needed length
sequence 2. Therefore x 1 actions type
decrease length 2y 2x 2, left sequence
length 2y + 1 (2y 2x 2) = 2x + 3. Therefore w cannot support
b
+ x + 1 actions types aw
1 a1 . Now, suppose given
cost-optimal plan v v actions type aw
1 actions type
ab1 .
+ y+x+1
(160)

w
b
b
C(v ) C(aw
1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 )

(161)

(155) (158),
w
C(ab2 ) C(ab1 ) C(aw
2 ) C(a1 )

(162)

suppose contrary plan first case cost-optimal.
Eq. 161
w
b
b
C(aw
1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
b
b
(y + 1) C(aw
1 ) + x C(a1 ) + (y x) C(a2 )


w
b
b
(y + 1 ) (C(aw
2 ) C(a1 )) < ( x) (C(a2 ) C(a1 ))

(163)

Eq. 160 + 1 x, together Eq. 162 contradicting
Eq. 163.
(158) (155),
w
b
b
C(aw
2 ) C(a1 ) C(a2 ) C(a1 )

274

(164)

fiTractable Cost-Optimal Planning

suppose contrary plan second case cost-optimal.
Eq. 161
w
b
b
C(aw
1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
w
b
(x + 1 C(aw
1 ) + (y x) C(a2 ) + C(a1 )


w
(y ) (C(ab2 ) C(ab1 )) < ( x 1) (C(aw
2 ) C(a1 ))

(165)

Eq. 160 x 1, together Eq. 164 contradicting
Eq. 165.
(6) |v | = 2y + 1, |w | = 2x + 1, |w | > |v | + 1, actions v set
(
awv |ww = 2j 1, j N
(166)
ai =
abv |bw otherwise
ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 132 above.
Finally, ordering constraints >v,w rest parents w pred(v) \
{u, w} set empty sets.
relations >v >v,w identical previous case. Thus,
>v >v,w forming strict partial order union elements v
w .

Eqs. 75, 132 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 166 implies action sequence
provides v value G[v] latter specified.
(7) |v | = 2y, |w | = 2x + 1, |w | |v |, construct two post-3/2
candidates v , assign v cheapest among two, proving
cost lower C(v ).
(a) changes v wv done using action aw
1 , largest
possible number changes bv done using action ab1 , remaining
changes bv done using action ab2 . candidate v , set

b

a2 = 2j, j N, j x
(167)
ai = ab1 = 2j, j N, x < j

w
a1 otherwise
cost case

b
b
C(aw
1 ) + x C(a1 ) + (y x) C(a2 )

(168)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x i, set >v,w according Eq. 169.
>v,w =

[

{ai < aj | < 2y 2x + j, j > 1} {aj < ai | 2y 2x + j} {a1 < ai }




v ,aj w


(169)

275

fiKatz & Domshlak

u pred(v) \ {w} set >v,u according Eq. 82.
easy verify relation >v,w defined Eq. 169 strict total
order domain. relations >v >v,u effectively identical
previous case. Thus, >v >v,u >v >v,w forming strict
partial orders unions elements v u , v w ,
respectively.

Eqs. 75, 82, 169 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 167 implies action sequence provides v value G[v] latter specified.
(b) changes v bv done using action ab1 , largest
possible number changes wv done using action aw
1 , remaining
changes wv done using action aw
.


candidate
v , set
2

w

a2
ai = aw
1

b
a1

= 2j 1, j N, j x
= 2j 1, j N, x < j
otherwise

(170)

cost case
w
b
x C(aw
1 ) + (y x) C(a2 ) + C(a1 )

(171)

ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 159 above.
u pred(v) \ {w} set >v,u according Eq. 86 above.
relations >v , >v,w >v,u effectively identical previous case.
Thus, >v >v,u >v >v,w forming strict partial orders
unions elements v u , v w , respectively.S
Eqs. 75, 86, 159 derive linearization >v wpred(v) >v,w
defines sequence actions applicable respect {v} pred(v).
addition, |v | = |v | together Eq. 170 implies action sequence provides v value G[v] latter specified.
Now, cost-optimal plan , v cannot contain + x actions
b
types aw
1 a1 totally. Suppose contrary v contain least
b
+ x + 1 actions types aw
1 a1 . contains x 1
actions types. Let ww bw . . . bw sequence 2y values w
support cost-optimal plan v given w change value number
times. action type decrease needed length
sequence 2. Therefore x 1 actions type
decrease length 2y 2x 2, left sequence
length 2y (2y 2x 2) = 2x + 2. Therefore w cannot support
b
+ x actions types aw
1 a1 . Now, suppose given cost-optimal
b
plan v v actions type aw
1 actions type a1 .
+ y+x
276

(172)

fiTractable Cost-Optimal Planning


w
b
b
C(v ) C(aw
1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 )

(173)

(168) (171),
w
C(ab2 ) C(ab1 ) C(aw
2 ) C(a1 )

(174)

suppose contrary plan first case cost-optimal.
Eq. 173
w
b
b
C(aw
1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
b
b
C(aw
1 ) + x C(a1 ) + (y x) C(a2 )


w
b
b
(y ) (C(aw
2 ) C(a1 )) < ( x) (C(a2 ) C(a1 ))

(175)

Eq. 172 x, together Eq. 174 contradicting Eq. 175.
(171) (168),
w
b
b
C(aw
2 ) C(a1 ) C(a2 ) C(a1 )

(176)

suppose contrary plan second case cost-optimal.
Eq. 173
w
b
b
C(aw
1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <
w
b
x C(aw
1 ) + (y x) C(a2 ) + C(a1 )


w
(y ) (C(ab2 ) C(ab1 )) < ( x) (C(aw
2 ) C(a1 ))

(177)

Eq. 172 x, together Eq. 176 contradicting Eq. 177.
(8) |v | = 2y, |w | = 2x + 1, |w | > |v |, actions v set
(
awv |ww = 2j 1, j N
(178)
ai =
abv |bw otherwise
ordering constraints >v set according Eq. 75. Likewise, w =
ha1 , . . . , a2x1 i, set >v,w according Eq. 132 above.
Finally, ordering constraints >v,w rest parents w pred(v) \
{u, w} set empty sets.
relations >v >v,w identical previous case. Thus,
>v >v,w forming strict partial order union elements v
w .
277

fiKatz & Domshlak

now, specified sequences v , orders >v induced sequences,
orders >v,w , proved >v >v,w >w >v,w form strict partial orders
domains. construction allows us apply Theorem 1 (considered
sets) sequences v orders >v >v,w , proving
[
[
>=
(>v
>v,w )
vV

wpred(v)

forms strict partial order union v1 , . . . , vn . Putting thing together,
implies linearization > plan , post-3/2ness
subsequences v1 , . . . , vn implies P 3/2 (). Moreover, optimal plan
, C( ) = C() implies optimality .

Theorem 9 Let P(1) problem, COP = (X , F) corresponding
constraint
P
optimization problem, x optimal assignment X F (x) = .

(I) < , plan cost reconstructed x time polynomial
description size .

(II) plan, < .

Proof:
P
(I) Given COP solution x F (x) = < , construct plan
C() = . construct plan
1. Traversing planning variables topological ordering causal graph CG(),
associating variable v sequence v Av .
2. Merging constructed sequences v1 , . . . , vn desired plan .
v V pred(v) = set v = ha1 . . . al i, l = |xv | 1, ai
defined Eq 179 below.
(
awv , odd,
(179)
ai =
abv , even,
Note Eq. 179 well-definedthe existence essential Eq. 179 actions awv /abv
implied Eq. 22 < .
wk
1
turn, v V pred(v) = {w1 , . . . , wk }, given xw
v , . . . , xv , distinguish
following cases.
[ R1 played ] R1 played one parents, parents play role R11.
[ R1 played w1 ] Eq. 35 implies
w1
1
xwv 1 (xw
v , xw1 ) = (R1, xv , xw1 )
w

j

1
xw
v = S1. Eq. 36 xv = S1 1 < j k,
giving us
w
w
xwj (xv j , xv j1 , xwj ) = 0
v

278

fiTractable Cost-Optimal Planning

[ R1 played wj , j > 1 ] Eq. 36 implies


xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R1, S, #w #w , #b #b , , wj )

and, 1 < 6= j k,

w

i1

xwv (xw
, xwi ) = 0.
v , xv

Eq. 35 also
1
xwv 1 (xw
v , x w1 ) = 0

sub-cases, v , >v >v,w specified proof Theorem 8,
case I.
[ R2 played ] R2 played one parents, parents play role R11.
[ R2 played w1 ] Eq. 35 implies
w1
1
xwv 1 (xw
v , xw1 ) = (R2, xv , xw1 )

and, 1 < j k, Eq. 36 implies
w

w

xwj (xv j , xv j1 , xwj ) = 0
v

(R2, [S, #w , #b , ]] , w1 ) = (#w , 0, #b , 0), v , >v >v,w specified
proof Theorem 8, case III.2, otherwise, case IV.2.
[ R2 played wj , j > 1 ] Eq. 36 implies


xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R2, S, #w #w , #b #b , , wj )

and, 1 < 6= j k,

w

i1

xwv (xw
, xwi ) = 0.
v , xv

Eq. 35 also
1
xwv 1 (xw
v , x w1 ) = 0

(R2, [S, #w #w , #b #b , ]] , wj ) = (#w #w , 0, #b #b , 0), v , >v
>v,w specified proof Theorem 8, case III.2, otherwise,
case IV.2.
[ R3 R4 played ] roles played two parents,
parents play role R11.
279

fiKatz & Domshlak

[ R3 played w1 , R4 played wj , j > 1 ] Eqs. 35 36

w1
1
xwv 1 (xw
v , xw1 ) = (R3, xv , xw1 )



xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R4, S, #w #w , #b #b , , wj )

and, 1 < k, 6= j:

w

i1

, x wi ) = 0
xwv (xw
v , xv

[ R4 played w1 , R3 played wj , j > 1 ] Eqs. 35 36

w1
1
xwv 1 (xw
v , xw1 ) = (R4, xv , xw1 )



xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R3, S, #w #w , #b #b , , wj )

and, 1 < 6= j k,

w

i1

, x wi ) = 0
xwv (xw
v , xv

[ R3 played wj , R4 played wt , j 6= t, j, > 1 ] Eqs. 35 36

1
xwv 1 (xw
v , x w1 ) = 0






xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R3, S, #w #w , #b #b , , wj )


xwv ([[S, #w , #b , ]] , , #w , #b , , wt ) =


(R4, S, #w #w , #b #b , , wt )

and, 1 < k 6 {j, t},
w

i1

xwv (xw
, x wi ) = 0
v , xv

three sub-cases, v , >v >v,w specified proof Theorem 8,
case II.
[ R5 played ] R5 played one parents, parents play role R11.
280

fiTractable Cost-Optimal Planning

[ R5 played w1 ] Eqs. 35 36 imply
w1
1
xwv 1 (xw
v , xw1 ) = (R5, xv , xw1 )

and, 1 < j k,
w

w

xwj (xv j , xv j1 , xwj ) = 0
v

Considering specification function Eq. 29,
first case holds, minimum obtained first expression,
v , >v >v,w defined proof Theorem 8, case III.1.a.
first case holds, minimum obtained second expression,
v , >v >v,w defined proof Theorem 8, case IV.1.a.
second case holds, minimum obtained first expression,
v , >v >v,w defined proof Theorem 8, case III.3.a.
second case holds, minimum obtained second expression, v , >v >v,w defined proof Theorem 8, case
IV.3.a.
third case holds, v , >v >v,w defined proof
Theorem 8, case III.3.a.
forth case holds, v , >v >v,w defined proof
Theorem 8, case IV.3.a.
fifth case holds, minimum obtained first expression,
v , >v >v,w defined proof Theorem 8, case III.5.a.
fifth case holds, minimum obtained second expression,
v , >v >v,w defined proof Theorem 8, case IV.5.a.
sixth case holds, v , >v >v,w defined proof
Theorem 8, case III.5.a.
seventh case holds, v , >v >v,w defined proof
Theorem 8, case IV.5.a.
eighth case holds, minimum obtained first expression,
v , >v >v,w defined proof Theorem 8, case III.7.a.
eighth case holds, minimum obtained second expression, v , >v >v,w defined proof Theorem 8, case
IV.7.a.
[ R5 played wj , j > 1 ] Eq. 36 implies


xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R5, S, #w #w , #b #b , , wj )
and, 1 < 6= j k,

w

i1

, xwi ) = 0.
xwv (xw
v , xv

Eq. 35 also
1
xwv 1 (xw
v , x w1 ) = 0

v , >v >v,w specified exactly previous case.
281

fiKatz & Domshlak

[ R6 played ] R6 played one parents, parents play role R11.
[ R6 played w1 ] Eqs. 35 36 imply
w1
1
xwv 1 (xw
v , xw1 ) = (R6, xv , xw1 )

and, 1 < j k,
w

w

xwj (xv j , xv j1 , xwj ) = 0
v

Considering specification function Eq. 30,
first case holds, minimum obtained first expression,
v , >v >v,w defined proof Theorem 8, case III.1.b.
first case holds, minimum obtained second expression,
v , >v >v,w defined proof Theorem 8, case IV.1.b.
second case holds, minimum obtained first expression,
v , >v >v,w defined proof Theorem 8, case III.3.b.
second case holds, minimum obtained second expression, v , >v >v,w defined proof Theorem 8, case
IV.3.b.
third case holds, v , >v >v,w defined proof
Theorem 8, case III.3.b.
forth case holds, v , >v >v,w defined proof
Theorem 8, case IV.3.b.
fifth case holds, minimum obtained first expression,
v , >v >v,w defined proof Theorem 8, case III.5.b.
fifth case holds, minimum obtained second expression,
v , >v >v,w defined proof Theorem 8, case IV.5.b.
sixth case holds, v , >v >v,w defined proof
Theorem 8, case III.5.b.
seventh case holds, minimum obtained first expression,
v , >v >v,w defined proof Theorem 8, case III.7.b.
seventh case holds, minimum obtained second expression, v , >v >v,w defined proof Theorem 8,
case IV.7.b.
[ R6 played wj , j > 1 ] Eq. 36 implies


xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R6, S, #w #w , #b #b , , wj )
and, 1 < 6= j k,

w

i1

, xwi ) = 0.
xwv (xw
v , xv

Eq. 35 also
1
xwv 1 (xw
v , x w1 ) = 0

v , >v >v,w specified exactly previous case.
282

fiTractable Cost-Optimal Planning

[ R7 R9 played ] roles played two parents,
parents play role R11.
[ R7 played w1 , R9 played wj , j > 1 ] Eqs. 35 36

w1
1
xwv 1 (xw
v , xw1 ) = (R7, xv , xw1 )



xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R9, S, #w #w , #b #b , , wj )

,for 1 < 6= j k,

w

i1

xwv (xw
, x wi ) = 0
v , xv

Considering specification function Eq. 31,
first case holds, minimum obtained first expression,
v , >v >v,w defined proof Theorem 8, case III.1.a.
first case holds, minimum obtained second expression,
v , >v >v,w defined proof Theorem 8, case IV.1.a.
second case holds, v , >v >v,w defined proof
Theorem 8, case III.3.a.
third case holds, v , >v >v,w defined proof
Theorem 8, case IV.3.a.
forth case holds, v , >v >v,w defined proof
Theorem 8, case III.5.a.
fifth case holds, v , >v >v,w defined proof
Theorem 8, case IV.5.a.
sixth case holds, minimum obtained first expression,
v , >v >v,w defined proof Theorem 8, case III.7.a.
sixth case holds, minimum obtained second expression,
v , >v >v,w defined proof Theorem 8, case IV.7.a.
[ R9 played w1 , R7 played wj , j > 1 ] Eqs. 35 36

w1
1
xwv 1 (xw
v , xw1 ) = (R9, xv , xw1 )



xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R7, S, #w #w , #b #b , , wj )

and, 1 < 6= j k,

w

i1

xwv (xw
, x wi ) = 0
v , xv

v , >v >v,w specified exactly previous case.
283

fiKatz & Domshlak

[ R7 played wj , R9 played wt , j 6= t, j, > 1 ] Eqs. 35 36

1
xwv 1 (xw
v , x w1 ) = 0






xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R7, S, #w #w , #b #b , , wj )


xwv ( [S, #w , #b , ]] , , #w , #b , , wt ) =


(R9, S, #w #w , #b #b , , wt )

and, 1 < k, 6 {j, t},
w

i1

xwv (xw
, x wi ) = 0
v , xv

Then, v , >v >v,w specified exactly two previous cases.
[ R8 R10 played ] roles played two parents,
parents play role R11.
[ R8 played w1 , R10 played wj , j > 1 ] Eqs. 35 36

w1
1
xwv 1 (xw
v , xw1 ) = (R8, xv , xw1 )



xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R10, S, #w #w , #b #b , , wj )

and, 1 < 6= j k,

w

i1

xwv (xw
, x wi ) = 0
v , xv

Considering specification function Eq. 32,
first case holds, minimum obtained first expression,
v , >v >v,w defined proof Theorem 8, case III.1.b.
first case holds, minimum obtained second expression,
v , >v >v,w defined proof Theorem 8, case IV.1.b.
second case holds, v , >v >v,w defined proof
Theorem 8, case III.3.b.
third case holds, v , >v >v,w defined proof
Theorem 8, case IV.3.b.
forth case holds, v , >v >v,w defined proof
Theorem 8, case III.5.b.
fifth case holds, v , >v >v,w defined proof
Theorem 8, case IV.5.b.
284

fiTractable Cost-Optimal Planning

sixth case holds, minimum obtained first expression,
v , >v >v,w defined proof Theorem 8, case III.7.b.
sixth case holds, minimum obtained second expression,
v , >v >v,w defined proof Theorem 8, case IV.7.b.
[ R10 played w1 , R8 played wj , j > 1 ] Eqs. 35 36

w1
1
xwv 1 (xw
v , xw1 ) = (R10, x v , xw1 )



xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R8, S, #w #w , #b #b , , wj )

and, 1 < 6= j k,

w

i1

xwv (xw
, x wi ) = 0
v , xv

v , >v >v,w specified exactly previous case.
[ R8 played wj , R10 played wt , j 6= t, j, > 1 ] Eqs. 35 36

1
xwv 1 (xw
v , x w1 ) = 0






xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =
v


(R8, S, #w #w , #b #b , , wj )


xwv ( [S, #w , #b , ]] , , #w , #b , , wt ) =


(R10, S, #w #w , #b #b , , wt )

and, 1 < k, 6 {j, t},
w

i1

, x wi ) = 0
xwv (xw
v , xv

Then, v , >v >v,w specified exactly two previous cases.
now, variable v V , specified action sequence v
order >v elements v . w pred(v), specified order >v,w ,
proved >v >v,w >w >v,w form strict partial orders domains.
Similarly uniform cost case, construction allows us apply Theorem 1
(considered sets) sequences v orders >v >v,w , proving
[
[
>=
(>v
>v,w )
vV

wpred(v)

forms strict partial order union v1 , . . . , vn .
285

fiKatz & Domshlak

Finally, note plan extraction step algorithm polytree-1-dep corresponds exactly construction along Eqs. 74-79, 81-83, 85-86, 93-94, 9697, 105-106, 108, 110, 117-118, 120, 122, 129-130, 132-133, 135, 142-143, 145, 153-154, 156157, 159, 166-167, 169-170, 178, providing us poly-time concrete cost-optimal plan
corresponding optimal solution COP .
(II) prove solvable, must < . Assume contrary
case. Let solvable P(1) problem, let (using Theorem 8)
irreducible, post-3/2 plan . Given , let COP assignment x defined
follows.
1. COP variable xv , assignment x provides value v [(v)]
|v | = |v | + 1.
2. variable v V , pred(v) 6= , find (at two) parents
prevail actions v . Let w parent performs role R
{R1, R2, R3, R5, R6, R7, R8}, w parent performs one
roles R {R4, R9, R10, R11}. (By definition post-3/2 action sequences,
rest parents perform role R11.) Given that, |pred(v)| = k > 0, adopt
k
xw
ordering pred(v) w1 hh= w wk = w . First, assignment
v COP
ii
|
|1
|
|1
v
v
k
variable xw
v provides value S1, 2 , 2 , |v | 1 . Then, 1 < k,
wi

assignment xw
v COP variable xv provides value [S, #w , #b , |v | 1]],


S2, R = R4



S4, R = R9
S=

S5, R = R10



S1, R = R11
#w #b numbers actions v change value v wv
bv , respectively, prevailed value w1 .

Eq. 22-36 that, v V , pred(v) = ,
P xv (xv ) = C(v ).
w
k
Otherwise, pred(v) = {w1 , . . . , wk }, xv (xv , xv ) = 0, wpred(v) xwv (x ) =
C(v ). Therefore,
X
X
(x ) =
C(v ) = C(),
F

vV

prove.



References
Bacchus, F., & Yang, Q. (1994). Downward refinement efficiency hierarchical
problem solving. Artificial Intelligence, 71 (1), 43100.
Backstrom, C., & Klein, I. (1991). Planning polynomial time: SAS-PUBS class.
Computational Intelligence, 7 (3), 181197.
286

fiTractable Cost-Optimal Planning

Backstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. Computational
Intelligence, 11 (4), 625655.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (1
2), 533.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004). CP-nets: tool
representing reasoning conditional ceteris paribus preference statements.
Journal Artificial Intelligence Research, 21, 135191.
Brafman, R. I., & Domshlak, C. (2003). Structure complexity planning unary
operators. Journal Artificial Intelligence Research, 18, 315349.
Brafman, R. I., & Domshlak, C. (2006). Factored planning: How, when, not.
Proceedings 18th National Conference Artificial Intelligence (AAAI), pp.
809814, Boston, MA.
Bylander, T. (1994). computational complexity propositional STRIPS planning.
Artificial Intelligence, 69 (1-2), 165204.
Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32 (3), 333377.
Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1990). Introduction Algorithms. MIT
Press.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
Edelkamp, S. (2001). Planning pattern databases. Proceedings European
Conference Planning (ECP), pp. 1334.
Erol, K., Nau, D. S., & Subrahmanian, V. S. (1995). Complexity, decidability undecidability results domain-independent planning. Artificial Intelligence, Special Issue
Planning, 76 (12), 7588.
Garey, M. R., & Johnson, D. S. (1978). Computers Intractability: Guide Theory
NP-Completeness. W.H. Freeman Company, New-York.
Gimenez, O., & Jonsson, A. (2008). complexity planning problems simple
causal graphs. Journal Artificial Intelligence Research, 31, 319351.
Haslum, P. (2006). Admissible Heuristics Automated Planning. Ph.D. thesis, Linkoping
University, Department Computer Information Science.
Haslum, P., Bonet, B., & Geffner, H. (2005). New admissible heuristics domainindependent planning. Proceedings 20th National Conference Artificial
Intelligence (AAAI), pp. 11631168, Pittsburgh, PA.
Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. Proceedings 15th International Conference Artificial Intelligence Planning Systems
(AIPS), pp. 140149, Breckenridge, CO.
Helmert, M. (2003). Complexity results standard benchmark domains planning.
Artificial Intelligence, 146 (2), 219262.
Helmert, M. (2006). Fast Downward planning system. Journal Artificial Intelligence
Research, 26, 191246.
287

fiKatz & Domshlak

Hoffmann, J. (2003). Utilizing Problem Structure Planning: Local Search Approach.
No. 2854 LNAI. Springer-Verlag.
Hoffmann, J., & Edelkamp, S. (2005). deterministic part IPC-4: overview. Journal
Artificial Intelligence Research, 24, 519579.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 253302.
Jonsson, P., & Backstrom, C. (1995). Incremental planning. New Directions AI
Planning: EWSP95-3rd European Workshop Planning, pp. 7990, Assisi, Italy.
Jonsson, P., & Backstrom, C. (1998a). State-variable planning structural restrictions:
Algorithms complexity. Artificial Intelligence, 100 (12), 125176.
Jonsson, P., & Backstrom, C. (1998b). Tractable plan existence imply tractable
plan generation. Annals Mathematics Artificial Intelligence, 22 (3-4), 281296.
Kambhampati, S. (1995). Admissible pruning strategies based plan minimality planspace planning. Proceedings 14th International Joint Conference Artificial
Intelligence, pp. 16271635, Montreal, Canada.
Katz, M., & Domshlak, C. (2007). Structural patterns heuristics. ICAPS-07 Workshop Heuristics Domain-independent Planning: Progress, Ideas, Limitations,
Challenges, Providence, RI.
Klein, I., Jonsson, P., & Backstrom, C. (1998). Efficient planning miniature assembly
line. Artificial Intelligence Engineering, 13 (1), 6981.
Knoblock, C. (1994). Automatically generating abstractions planning. Artificial Intelligence, 68 (2), 243302.
Newell, A., & Simon, H. A. (1963). GPS: program simulates human thought.
Feigenbaum, E. A., & Feldman, J. (Eds.), Computers Thought, pp. 279293.
Oldenbourg.
Sacerdoti, E. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence,
5, 115135.
Tenenberg, J. D. (1991). Abstraction planning. Allen, J. F., Kautz, H. A., Pelavin,
R. N., & Tenenberg, J. D. (Eds.), Reasoning Plans, chap. 4, pp. 213283. Morgan Kaufmann.
Williams, B., & Nayak, P. (1996). model-based approach reactive self-configuring
systems. Proceedings 13th National Conference Artificial Intelligence
(AAAI), pp. 971977, Portland, OR.
Williams, B., & Nayak, P. (1997). reactive planner model-based executive. Proceedings 15th International Joint Conference Artificial Intelligence (IJCAI),
pp. 11781185, Nagoya, Japan.

288

fiJournal Artificial Intelligence Research 32 (2008) 487-523

Submitted 11/07; published 06/08

Refining Execution Abstract Actions
Learned Action Models
Freek Stulp
Michael Beetz

stulp@cs.tum.edu
beetz@cs.tum.edu

Intelligent Autonomous Systems Group
Technische Universitat Munchen
Boltzmannstrae 3, D-85747 Garching bei Munchen, Germany

Abstract
Robots reason abstract actions, go position l, order decide
generate plans intended course action. use abstract actions
enables robots employ small action libraries, reduces search space decision
making. executing actions, however, robot must tailor abstract actions
specific task situation context hand.
article propose novel robot action execution system learns success
performance models possible specializations abstract actions. execution time,
robot uses models optimize execution abstract actions respective task
contexts. robot use abstract actions efficient reasoning, without compromising performance action execution. show impact action execution
model three robotic domains two kinds action execution problems: (1)
instantiation free action parameters optimize expected performance action sequences; (2) automatic introduction additional subgoals make action sequences
reliable.

1. Introduction
motor control, main challenge map high-level goals plans low-level motor
commands. instance, soccer scenario Figure 1(a), needs done
abstract level informally declared as: achieve scoring opportunity, first
approach ball, dribble towards opponents goal. actually execute
abstract plan, robot must map low-level motor commands, translational
rotational velocities.
Using durative actions bridge gap proven successful approach
nature (Wolpert & Ghahramani, 2000) robotics (Arkin, 1998). Durative parameterizable actions, simply actions 1 , encapsulate knowledge certain goals
achieved certain task contexts, produce streams motor command achieve
goals. instance, human robot soccer players typically dribbling, kicking,
passing actions, relevant context soccer. Also, actions achieves different goals within different soccer contexts. abstract plan executed
mapping sequence actions. running example, plan mapped
action sequence approachBall, dribbleBall.
1. cognitive science, actions known inverse models control rules, robotics also behaviors,
routines, policies, controllers.

2008 AI Access Foundation. rights reserved.

fiStulp & Beetz

(a) Abstract action chain.

(b) Suboptimal execution
abrupt transition.

(c) Time-optimal execution
exhibits smooth motion.

Figure 1: abstract action chain, two alternative executions chain.
center line trajectory represents robots position. lines
drawn perpendicular robots orientation, width represents
translational velocity point. values recorded 10Hz.

Although actions specify achieve goal, often several ways
execute them. Figure 1 depicts two executions action sequence. first,
robot naively executes first action, arrives ball goal back,
depicted Figure 1(b). unfortunate position start dribbling
towards goal. abrupt transition occurs actions, robot needs
brake, carefully maneuver behind ball direction goal. Preferably,
robot go ball order dribble towards goal afterwards.
robot should, depicted Figure 1(c), perform first action sub-optimally order
achieve much better position executing second action. article,
performance measure execution duration. behavior shown Figure 1(c) defined
optimal, minimizes time needed execute action sequence.
reason suboptimal behavior often witnessed robots robot
planners, also designers robot controllers, view actions level abstraction
ignores subtle differences actions. Abstracting away action details
essential keep action selection programming planning tractable. However,
planning system considers actions black boxes performance independent
prior subsequent steps, planning system cannot tailor actions execution
contexts. often yields suboptimal behavior abrupt transitions actions,
Figure 1(b). example, problem abstract view planner,
ball considered sufficient dribbling ball dynamical state
robot arriving ball considered irrelevant dribbling action. Whereas
angle approach relevant validity plan therefore free choose,
must reasoned order optimize plan execution.
So, given abstract plan, concrete action sequence predicted
minimal total cost (execution duration) likely succeed? problem
broken three subproblems: 1) cost success predicted? 2)
action sequences optimized minimal cost? 3) sequences actions
transformed increase successful execution?
488

fiRefining Execution Abstract Actions Learned Action Models

take inspiration findings cognitive science (Wolpert & Flanagan, 2001),
solve first subproblem acquiring applying third kind motor control
knowledge: able predict outcome performance actions. running
example, robot could predict performance alternative executions beforehand,
could choose commit fastest execution. predict execution duration
action sequences, robot must predict execution duration individual actions.
robot learn action models experimentation, observation generalization. observing executions actions various parameterizations,
learning general models tree-based induction.
implemented approach diversity real simulated robotic platforms, introduced next section. Afterwards, discuss related work
Section 3. following four sections organized according flowchart Figure 2.
representation (abstract) actions action models described Section 4, along
generation abstract plans. explain action models learned observed experience Section 5. first applications action models, subgoal refinement,
described Section 6. Section 7, present subgoal assertion, transforms plans
predicted fail successful plans. Throughout article, describe
representations algorithms used implement flowchart Figure 2. Finally,
conclude summary outlook future work Section 8.

Figure 2: Outline article flowchart overall system.

2. Robotic Domains
article, robots learn apply action models three domains: robotic soccer,
service robotics arm control. variety robots domains chosen
emphasize generality approach. Also, different characteristics domains
allow different aspects action model applications investigated.
first domain robotic soccer (Kitano, Asada, Kuniyoshi, Noda, & Osawa, 1997).
adversarial domain, performance efficiency essential achieving goals
team. Tailoring actions perform well within given task context therefore
necessity. use customized Pioneer robots mid-size league team Agilo
RoboCuppers (Beetz, Schmitt, Hanek, Buck, Stulp, Schroter, & Radig, 2004), one
depicted Figure 3(a). robots differential drive, localize
single forward facing camera. Experiments domain conducted real
robots, well simulation (Buck, Beetz, & Schmitt, 2002a).
test approach robots degrees freedom, operating richer environments, included articulated B21 robot simulated kitchen environ489

fiStulp & Beetz

(a) Agilo soccer robot.

(b) B21 kitchen environment.

(c) PowerCube arm.

Figure 3: three robotic domains considered article.
ment (Muller & Beetz, 2006). Household tasks also less reactive robotic soccer,
require complex long-term planning, relevant context
action sequence optimization presented Section 6. simulator based Gazebo
simulator Player Project (Gerkey, Vaughan, & Howard, 2003). environment,
depicted Figure 3(b) typical kitchen scenario, furniture, appliances, cups
cutlery.
scenarios domain related fetching delivering cups one place
another. so, robot navigation actions, actions reaching cup,
putting down.
Finally, evaluate approach articulated manipulator, performs reaching movements. experiments performed PowerCube arm Amtec
Robotics 6 degrees freedom, shown Figure 3(c). experiments, two
joints used.

3. Related Work
section, discuss work related learning predictive models actions, optimal
hybrid control, well learning preconditions.
3.1 Learning Predictive Models Actions
Jordan Rumelhart (1992) introduced Distal Learning, explicitly learns forward
models neural networks. action models used bridge gap
distal target values motor commands, enables motor control learning. Recently,
robotic forward models also learned using Bayesian networks (Dearden & Demiris,
2005). Infantes, Ingrand, Ghallab (2006) present recent work also includes use
dynamic Bayesian networks. work extends approach Fox, Ghallab, Infantes,
Long (2006), action models learned Hidden Markov Models.
general, advantage Bayesian networks allow causal nature
robots control system modelled using probabilistic framework. However,
appropriate predicting outcome motor command next time step,
rather expected performance durative action.
490

fiRefining Execution Abstract Actions Learned Action Models

Modular Selection Identification Control (MOSAIC) architecture (Haruno,
Wolpert, & Kawato, 2001) also integrates forward models computational model
motor control. framework intended model two problems humans must solve:
learn inverse models tasks, select appropriate inverse model,
given certain task. MOSAIC uses multiple pairs forward inverse models so.
architecture designed robot control. aware (robotic)
controllers action models integral central part computational
model, acquired automatically, represented explicitly, used modular
resources different kinds control problems.
Haigh (1998) Belker (2004) also developed robots optimize high-level
plans action models. models used determine best path hallway
environment. approach rather focuses optimizing parameterization subgoals
fixed points plan, limited navigation plans tasks. authors
report good experiences tree-based induction learning robot action models
observed data, also chosen learning algorithm.
3.2 Optimal Control
Optimal control refers use online, optimal trajectory generation part
feedback stabilization typically nonlinear system (Astrom & Murray, 2008). Receding
horizon control subclass optimal control approaches, (optimal) trajectory
planned state x(t + Hp ) lies current state x(t)
goal state xgoal (Astrom & Murray, 2008). planning next Hp steps, steps
trajectory executed (with 1 Hp ), new trajectory computed
new current state x(t + ) x(t + + Hp ). example RHC
work Bellingham, Richards, (2002), apply RHC simulated autonomous
aerial vehicles. rationale behind receding horizon control (RHC)
diminishing return optimizing later parts trajectory beginning execution.
experiments described Section 6.3.1 verify effect. comparison RHC
methods also follow Section 6.3.1.
Todorov, Li, Pan (2005) take hierarchical approach optimal control. highlevel controller uses abstract state command representation, control lowlevel controller, turn controls physical plant. main difference
work purpose low-level controller solve specific subtask,
actions, rather perform instantaneous feedback transformation
abstraction high-level controllers.
Using redundant degrees freedom optimize subordinate criteria well studied context arm control, humans (Schaal & Schweighofer, 2005; Wolpert
& Ghahramani, 2000; Uno, Wolpert, Kawato, & Suzuki, 1989) robots (Simmons &
Demiris, 2004; Nakanishi, Cory, Mistry, Peters, & Schaal, 2005; Bertram, Kuffner, Dillmann, & Asfour, 2006). Arm poses said redundant many arm configurations result equivalent pose. work cited above, configurations
called uncontrolled manifold, motion space, null space, finding best configuration called redundancy resolution null-space optimization. approaches above,
optimization performed analytically, specific arm control domain. Learning
491

fiStulp & Beetz

models observed experience enables whole range different robots apply
variety tasks.
Smooth motion also arises control signals several actions combined using
weighted interpolation scheme. motion blending approaches found robotics (Utz,
Kraetzschmar, Mayer, & Palm, 2005; Jaeger & Christaller, 1998; Saffiotti, Ruspini, & Konolige, 1993), well computer graphics (Perlin, 1995; Kovar & Gleicher, 2003). Since
discrete transitions actions, visible execution. Hoffmann
Duffert (2004) propose another method generate smooth transitions different gaits quadruped robots, based smoothing signals frequency domain. Since
actions use periodic, methods apply. approaches,
assumed smooth motion lead better performance. However, achieving optimal
behavior explicit goal, objective performance measures optimized.

3.3 Reinforcement Learning
Reinforcement Learning (RL) another method seeks optimize performance, specified reward function. RL approaches often model optimal control problem
Markov Decision Process (MDP), usually solved dynamic programming.
Recent attempts combat curse dimensionality RL turned principled
ways exploiting temporal abstraction (Barto & Mahadevan, 2003). sense, Hierarchical Reinforcement Learning algorithms also learn action models, Q-value predicts
future reward executing action. Note Q-values learned one specific
environment, one specific reward function. values learned states,
single goal function. action models general, describe action
independent environment, context called. Therefore, action
models transfered task contexts. Haigh (1998) draws conclusion
comparing action models RL. view, action models also provide
informative performance measures physical meaning, execution time,
scale better continuous complex state spaces.
approach know explicitly combines planning Reinforcement
Learning RL-TOPS (Reinforcement Learning - Teleo Operators) (Ryan & Pendrith, 1998).
approach, sequences actions first generated based preconditions
effects, using Prolog. Reinforcement Learning within action sequence done
HSMQ (Dietterich, 2000). actions, abrupt transitions arise too,
author recognizes cutting corners would improve performance, present
solution.
similar work, point view smoothness emergent property
optimality requirements redundant subgoals, approach Kollar Roy
(2006). work, simulated robot maps environment range measurements
traversing set waypoints. policy minimizes error resulting map
learned RL. side-effect, smooth transitions waypoints arise. approach
tested real robots.
492

fiRefining Execution Abstract Actions Learned Action Models

3.4 Hybrid Control
Problems involving choice actions action chains often regarded planning problems. However, planning systems aim optimizing resources, time.
scheduling systems better representing time constraints resources,
could deal selection actions problem. Systems integrate planning
scheduling, work Smith, Frank, Jonsson (2000), able optimize
resources, ignore interactions actions intermediate dynamical states.
apply well continuous domain problems.
PDDL (Fox & Long, 2003), resource consumption actions represented
abstract level. Planners take resources account generating plans.
contrast planners, system generates action sequences optimized
respect realistic, non-linear, continuous performance models, grounded
real world learned observed experience.
recent approaches using symbolic planning robots focus different problems arise plans executed real robots. instance, Bouguerra
Karlsson (2005) use probabilistic planners abstract planning domain, enables
exploit uncertainties determined probabilistic state estimation. aSyMov reasons
geometric preconditions consequences actions simulated 3-D world (Cambon, Gravot, & Alami, 2004). Note methods complementary rather
incompatible described Bouguerra Karlsson (2005) Cambon et al. (2004),
merging would combine advantages. Cambon et al. (2004) mention
resulting plan improved optimized way, describe how. probabilistic motion planning, post-processing step smoothing generated paths
common procedure. Subgoal refinements might well integrated optimization
step.
Belker (2004) uses action models learned model trees optimize Hierarchical
Transition Network (HTN) plans. HTN plans structured hierarchically high level
goals low level commands. optimize performance, order actions,
actions changed varying levels hierarchy. Rather refining
plans, system modifies HTN plans themselves, therefore applies HTN plans
only. hand, refine existing action chain, planner selected
independently optimization process
Generating collision-free paths initial final robot configuration also known
robot motion planning. common distinction algorithms generate
paths global, local hybrid approaches (Brock & Khatib, 1999). approach presented article global approach, planning performed offline.
One disadvantage offline algorithms guarantee planned trajectory actually succeed execution. respect robotic motion planning, movable
obstacles unforeseen dynamics two examples correctly planned trajectories
fail. holds global approaches, work Bouguerra
Karlsson (2005), Cambon et al. (2004) examples discussed previously. Hybrid motion
planning approaches, compute commit plans offline, leave freedom plans
react unforeseen circumstances (Brock & Khatib, 1999). Whereas hybrid motion
planning approaches freedom subgoals considered, subgoal refinement con493

fiStulp & Beetz

siders freedom subgoal itself. Note implies hybrid approaches
incompatible subgoal refinement, might complement well.
3.5 Learning Preconditions
Section 7, demonstrate failure action models, predict whether action
fail succeed, learned. similar learning preconditions actions.
research learning preconditions, concept induced symbolic.
Furthermore, examples consist symbols grounded real world.
precondition learned examples, instance Inductive Logic
Programming (Benson, 1995) specialized methods logic inference (Shahaf &
Amir, 2006; Clement, Durfee, & Barrett, 2007). approaches applied
robots. believe symbolic representations suffice encapsulate
complex conditions arise robot dynamics action parameterizations.
models best learned experience, described Section 5.
Schmill, Oates, Cohen (2000) present system non-symbolic planning
operators learned actual interaction robot real world. experiences robot first partitioned qualitatively different outcome classes,
clustering approach. learned operators similar previously hand-coded
operators. classes known, robot learns map sensory features
classes decision tree, similar approach. approach aims learning predict robot perceive executing action scratch, whereas condition
refinement aims refining already existing symbolic preconditions based changing
goals.
Buck Riedmiller (2000) propose method learning success rate passing
actions context RoboCup simulation league. neural network trained
8000 examples pass attempted, along success failure pass.
information used manually code action selection rules attempt
pass expected succeed probability > 0.7. also good example
integrating human-specified learned knowledge controller.
Sussman (1973) first realize bugs plans lead failure,
actually opportunity construct improved robust plans. Although
research done highly abstract symbolic blocks world domain, idea
still fundamental transformational planning. XFRMLearn framework proposed
Beetz Belker (2000), human-specified declarative knowledge combined robotlearned knowledge. Navigation plans optimized respect execution time
analyzing, transforming testing structured reactive controllers (SRCs) (Beetz, 2001).
One difference XFRMLearn work, analysis phase learned instead
human-specified. Another difference XFRMLearn improves existing plans, whereas
condition refinement learns adapt changing action requirements, refined
goals.

4. Conceptualization
conceptualization computational problem based dynamic system
model (Dean & Wellmann, 1991). model, world changes inter494

fiRefining Execution Abstract Actions Learned Action Models

action two processes: controlled process controlling process. controlled
process essentially environment, including body sensors robot.
dynamic system model, assumed environment described set
state variables.
controlling process performs two tasks. First, uses state estimation estimate
values state variables environment, based percepts sensors
provide. observable state variables encapsulated belief state. Second,
controller takes belief state input, determines appropriate motor commands
direct state variables certain target values. motor commands dispatched
motor system robot environment. robotic domain, state
estimation methods, state variables belief state, motor commands listed
Table 5 Appendix A. rest section focus concepts used inside
controller.
4.1 Actions Action Libraries
Actions control programs produce streams motor commands, based
current belief state. robotics, actions usually take parameters allow
used wide range situations. Instead programming action dribbleBallToCenter, preferable program action dribbleBall(Pose) dribble ball
location field, including center. actions apply certain task
contexts, easier design learn controller must able deal
possible contexts (Haruno, Wolpert, & Kawato, 1999; Jacobs & Jordan, 1993; Ginsberg,
1989).
One actions used goToPose, navigates robot current state
[x,y,,v] future target state [xg ,yg ,g ,vg ] setting translational rotational
velocity [v,] robot. Table 6 Appendix lists actions used different
robots. action parameters separated values variables current state
target values variables. first so-called observable state variables,
whereas second exist internally controller. action models learned
observed experience, learning applying independent action implementations.
describe hand-coded implementations here, rather refer Stulp (2007,
Appendix A).
Action libraries contain set actions frequently used within given domain.
action designed cover large set tasks, usually small set actions
needed achieve tasks given domain. actions several
advantages: 1) controller less complex, making robust. 2) Fewer interactions
actions need considered, facilitating action selection design autonomous
planning. 3) environment changes, actions need redesigned
relearned, making system adaptive, easier maintain.
article, assume actions innate, change time.
building blocks combined concatenated solve tasks single action
could achieve alone. Actions never modified optimized way.
Rather, parameters chosen expected performance optimal within
given task context.
495

fiStulp & Beetz

4.2 Action Models
Action models predict performance outcome action. action executed certain parameters, corresponding action model called
parameters, instance predict execution duration. Action models discussed
detail Section 5.
4.3 Abstract Actions
achieve complex tasks, actions combined concatenated. long
tradition using symbolic planners generate abstract action chains robot control.
Shakey, one first autonomous mobile robots, used symbolic representations determine action sequences would achieve goal (Nilsson, 1984). recent examples
include work Coradeschi Saffiotti (2001), Beetz (2001), Cambon et al. (2004)
Bouguerra Karlsson (2005). One main reasons symbolic planning
abstract actions interest robotics abstract away many aspects
continuous belief state, planning replanning faster, complex problems dealt with. Planners good fixing general abstract structure
task. Also, action sequences action hierarchies must specified advance,
generated online, depending situation hand, making system adaptive.
Furthermore, robots reason plans offline execution, recognize repair failures advance (Beetz, 2000), preferable encountering task
execution. Finally, symbolic planning robotics enables designers specify constraints
actions symbolically (Cambon et al., 2004).
Abstract actions consist preconditions effects action (Nilsson, 1994).
constitute controllers abstract declarative knowledge. effects represent
intended goal action. specifies region state space goal
satisfied. precondition region respect temporally extended action defined
set world states continuous execution action eventually satisfy
effects. Figure 4 shows two abstract actions, along preconditions effects.

Figure 4: abstract action chain consisting two abstract actions.

system implementation, Planning Domain Description Language (PDDL2.1)
used describe abstract actions, goals, states plans (Fox & Long, 2003). declarative knowledge preconditions effects (add- delete-list) abstract
actions action library specified manually designer. PDDL planner
use Versatile Heuristic Partial Order Planner (Younes & Simmons, 2003)2 . VHPOP is,
PDDL planners, general purpose planner specifically tailored robot planning.
2. VHPOP downloaded free cost http://www.tempastic.org/vhpop/

496

fiRefining Execution Abstract Actions Learned Action Models

work focusses problems need solved exploit symbolic plannning
robotics, uncertainty, failure recovery action monitoring (Bouguerra & Karlsson,
2005), geometric constraints (Cambon et al., 2004), anchoring (Coradeschi & Saffiotti,
2001). system presented section abstracts away problems focus
main contribution: optimization already generated plans. output VHPOP
list symbolic actions causal links, shall see example Figure 10.
4.4 System Overview
Subgoal refinement subgoal assertion operate sequences actions.
article, action sequences derived abstract action chains, generated
symbolic planner. general computational model robot control based symbolic
plans depicted Figure 5, similar models proposed Bouguerra
Karlsson (2005) Cambon et al. (2004).
goal passed input system. Usually, abstract state derived
belief state process called anchoring (Coradeschi & Saffiotti, 2001).
International Planning Competition, consider limited number scenarios, enabling
us specify scenarios PDDL advance. name scenario contained
belief state, corresponding abstract state read PDDL file.

Figure 5: Computational model robot control based symbolic plans.

: pddl goal, (represented PDDL)
beliefState, (belief state state variables)
action lib (library PDDL representations, actions, action models)
output : exe action seq (an optimized sequences executable actions)
input

1
2
3
4
5
6

pddl state = readFromFile (beliefState.scenario name) // Anchoring
pddl plan = makePlan (pddl state, pddl goal, action lib.pddl) // VHPOP
exe action seq = instantiateAction (pddl plan, belief state, action lib.actions) // Section 6.1
exe action seq = assertSubgoals (exe action seq,belief state, action lib) // Section 7
exe action seq = refineSubgoals (exe action seq, action lib.models) // Section 6.2
return exe action seq;

Algorithm 1: System Overview.
497

fiStulp & Beetz

pseudo-code complete system described article listed Algorithm 1.
Data structures abstract declarative planning domain (see Figure 5)
prefix pddl . section, presented implementations functions
lines 1 2. next section, describe action models used
subgoal refinement assertion (line 4 5) learned observed experience.
implementations functions lines 3 5 presented Section 6 7. Note
subgoal refinement assertion modify existing action sequences.
interfere planning instantiation process. means compatible
planning systems.
action models applied online task execution, must learned.
learning phase described next section.

5. Learning Action Models
section, describe robots learn action models observed experience.
advantage learning action models analytical methods 1) based real
experience, therefore takes factors relevant performance account. 2) many
(hand-coded) actions difficult formalize analytically (Beetz & Belker, 2000).
instance, exact interactions ball guide rail difficult predict
model-predictive control, result subtle interplay many complex factors,
robots dynamics, sensing capabilities, smoothness ball guide rail,
parameterizations control program, etc. factors must modeled order
make accurate predictions. 3) analysis sometimes impossible implementation action unknown, instance learned. 4) although
implemented work, learned action models also adapt changing environments
trained online (Dearden & Demiris, 2005; Kirsch & Beetz, 2007). 5) enables robots
learn models robots, observing behavior, demonstrated Stulp, Isik,
Beetz (2006).
5.1 Data Acquisition
Training examples gathered executing action, observing results.
article, robots record learn predict execution duration success actions,
given parameterization action. generate training data action, parameter
values initial goal state sampled uniformly possible values action
parameters take. possible parameter value ranges depend preconditions
effects action, specified semi-automatically. user defines ranges
action parameters ensure preconditions effects action met,
action parameters used uniformly sampled ranges. execution
action initial goal state called episode.
running example section learning predict execution duration
goToPose action simulated B21 Agilo robot. Figure 6(a) displays
concrete example gathered training data simulated B21 robot. Here, 30 2948
executions goToPose random initial goal states shown. total number
executions denoted ne . instance, ne =2948 example above. split
498

fiRefining Execution Abstract Actions Learned Action Models

data training test set. number examples training set N = 43 ne ,
current example yields 2200 episodes.

(a) Visualization 30
2948 goToPose executions.

(b) original state space, two derived feature spaces. top figures
depict features used. lower graphs plot time remaining
reaching goal one features.

Figure 6: Gathering transforming experience goToPose kitchen domain.
learning algorithm trained 2200 examples likely make erroneous predictions previously unseen cases? general, hypothesis consistent
sufficiently large training set deemed probably approximately correct (PAC) (Russell &
Norvig, 2003). obtain model error probability 1 (i.e.
PAC), learning algorithm must trained least N 1 (ln 1 + ln|H|) training
examples, |H| number possible hypotheses. use formula
exactly compute number training examples needed, rather determine strategies
learn accurate models limited amount costly training episodes. use
three approaches:
Reduce number hypotheses |H|. exploiting invariances, map
data original direct state space lower-dimensional derived feature space.
limits number possible hypotheses |H|. navigation actions instance,
translational rotational invariances original 8-D state space (listed Table 6)
exploited transform 5-D features space, depicted Figure 6(b). features
5-D state space correlate better performance measure. Haigh (1998) calls
features projective.
exploiting invariances, reducing dimensionality feature space,
leads decrease |H|. equation specifies lower |H|, fewer training
examples needed learn PAC model. reasoning, accurate models
(i.e. lower ) learned lower dimensional feature spaces, given amount
data.
experimentally verified training model tree learning algorithm (to
presented Section 5.2) data mapped three different feature spaces
499

fiStulp & Beetz

Figure 6(b). feature space, model trained N =2200 ne =2948
executed episodes. Mean Absolute Error (MAE) models determined
separate test containing remaining episodes3 . seen Figure 6(b),
MAE lower lower dimensional feature spaces used. course, lower
dimensionality achieved simply discarding informative features, rather
composing features projective features exploiting invariances.
Increase number training examples N including intermediate examples. Instead using first initial example episode (large red dots
Figure 6(b)), could also include intermediate examples goal state.
recorded 10Hz, yields many examples, higher N . However,
one characteristic projective features pass point origin,
seen right-most graph Figure 6(b). Therefore, including intermediate
examples lead distribution skewed towards origin. violates
stationarity assumption, poses training test set must sampled
probability distribution. learning algorithms trained abundance
data around origin biased towards states close goal,
tend predict states accurately, cost inaccurate prediction states
goal.
Therefore, include first ni examples episode. means
number training examples roughly ne ni instead ne , still represents
original distribution initial states. Since best value ni clear analytically,
determine empirically, described Stulp (2007)
Track error empirically. Instead defining advance, compute
Mean Absolute Error (MAE) time data gathered, determine
stabilizes visually. point, assume N sufficiently large, stop gathering
data.
5.2 Tree-based Induction
Learning algorithms used learning robot action models observed experience
include neural networks (Buck, Beetz, & Schmitt, 2002b), tree-based induction (Belker,
2004; Haigh, 1998). shown significant difference accuracy
action models learned neural networks tree-based induction (Stulp et al., 2006).
However, decision model trees advantage converted sets
rules, visually inspected. Furthermore, model trees optimized
analytically described Section 6.2. reasons, focus decision
model trees article.
Decision trees functions map continuous nominal input features nominal
output value. function learned examples piecewise partitioning
feature space. One class chosen represent data partition. Model trees
generalization decision trees, nominal values leaf nodes replaced
line segments. line fitted data partition linear regression.
3. prefer MAE Root Mean Square Error (RMSE), intuitive understand,
cost prediction error roughly proportional size error. need
weight larger errors more.

500

fiRefining Execution Abstract Actions Learned Action Models

linear function interpolates data partition. enables model trees
approximate continuous functions. information decision model trees,
learned tree-based induction, refer (Quinlan, 1992; Stulp
et al., 2006; Wimmer, Stulp, Pietzsch, & Radig, 2008). implementation, use
WEKA (Witten & Frank, 2005) learn decision model trees, convert output
C++ function.
5.3 Action Model: Execution Duration Prediction
visualize action models learned model trees, example execution duration
prediction specific situation depicted Figure 7. model goToPose
action soccer domain (real robots) learned ne =386 episodes. first ni =20
examples per episode used. features used dist, angle to, angle at, v vg ,
depicted Figure 6(b).
situation depicted Figure 6(b), variables dist, angle to, v, vg set
1.5m, 0 , 0m/s, 0m/s respectively. model much general, predicts
accurate values dist, angle to, v, vg ; variables fixed visualization
purposes only. fixed values, Figure 7 shows predicted time depends
angle at, Cartesian, polar coordinate system.

Figure 7: example situation, two graphs time prediction situation varying
angle at, model tree rule one line segments.

Cartesian plot five line segments. means model tree
partitioned feature space dist=1.5m angle to=0 , v=0m/s, vg =0m/s five
areas, linear model. two plots, one learned model tree
rules applies situation displayed. Arrows graphs indicate linear
model corresponds rule.
501

fiStulp & Beetz

5.3.1 Empirical Evaluation
different domains actions action models execution duration learned
listed first two columns Table 1. subsequent columns list number
episodes executed gather data training set 34 ne , mean execution duration per
episode t, total duration data gathering training set 34 ne , well
models error (MAE) separate test set remaining 41 ne episodes. Note
final model stored action library trained data ne episodes.
next sections, demonstrate action models accurate enough enable
significant improvement action execution.
Domain

Action

3
4 ne

Soccer
(real)
Soccer
(simulated)
Kitchen

goToPose
dribbleBall
goToPose
dribbleBall
goToPose
reach
reach

290
202
750
750
2200
2200
1100

Arm control


(s)
6.4
7.7
6.2
7.4
9.0
2.6
2.9

34 ne
(h:mm)
0:31
0:26
1:18
1:32
5:45
1:38
0:53

MAE
(s)
0.32
0.43
0.22
0.29
0.52
0.10
0.21

Table 1: List actions action model statistics.

5.4 Action Model: Failure Prediction
simulated soccer robots also learn predict failures approaching ball
goToPose action. failure occurs robot collides ball desired state
achieved. robots learn predict failures observed experience. acquire
experience, robot executes goToPose thousand times, random initial goal
states. ball always positioned destination state. initial goal state
stored, along flag set Fail robot collided ball
reaching desired position orientation, Success otherwise. feature space
learning temporal prediction model goToPose, listed Table 7.
learned tree, graphical representation, depicted Figure 8. goal
state represented robot, different areas indicate robot reach
position goToPose without bumping ball first. Remember goToPose
awareness ball all. model simply predicts execution leads
collision not. Intuitively, rules seem correct. coming right,
instance, robot always clumsily stumbles ball, long reaching desired
orientation. Approaching ball fine state green area labeled S.
5.4.1 Empirical Evaluation
evaluate accuracy model, robot executes another thousand runs.
resulting confusion matrix depicted Table 2. decision tree predicts collisions
correctly almost 90% cases. model quite pessimistic, predicts failure
502

fiRefining Execution Abstract Actions Learned Action Models

Figure 8: learned decision tree predicts whether collision occur.

61%, whereas reality 52%. 10% cases, predicts collision
actually happen. preferable optimistic model, better
safe sorry. pessimism actually coincidence; caused cost matrix
penalizes incorrect classification Fail Success passed
decision tree (Witten & Frank, 2005).

Predicted

Fail
Success

Total Observed

Observed
Fail Success
51%
10%
1%
38%


52%
48%

Total
Predicted
61%
39%

89%

Table 2: Confusion matrix ball collision prediction.

6. Subgoal Refinement
comes elegant motion, robots good reputation. Jagged movements actually typical robots people trying imitate robots
executing movements abrupt transitions them. Figure 1(b) demonstrates
abrupt transition arises approaching ball dribble certain location.
jagged motion inefficient aesthetically displeasing, also reveals
fundamental problem inevitably arises way robot controllers actions
designed reasoned about.
Figure 9(a) depicts abstract action chain scenario, preconditions
effects represented subsets entire state space. Note intersection preconditions effects intermediate subgoal contain many intermediate states, eight
also depicted scenario Figure 9(a). set, variables equal,
except angle ball approached. action parameter therefore
called free.
503

fiStulp & Beetz

discussed Section 1, reason abrupt transitions although preconditions effects abstract actions justly abstract away action parameters
influence high-level selection actions, free parameters might influence
performance actually executing them. instance, angle approach abstracted
away selecting actions, although obviously influences execution duration.
first step subgoal refinement therefore determine free action parameters
sequence abstract actions.

(a) Abstract action chain subgoal refinement.

(b) Abstract action chain optimized subgoal refinement.

Figure 9: Computational model subgoal refinement.
contrast robot motion, one impressive capabilities animals humans
ability perform sequences actions efficiently, seamless transitions subsequent actions. assumed typical patterns minimize
certain cost function (Wolpert & Ghahramani, 2000; Schaal & Schweighofer, 2005). So,
nature, fluency motion goal itself, rather emergent property time,
energy accuracy optimization.
Subgoal refinement exploits free action parameters similar way. Since states
intermediate set lead successful execution action sequence, free
choose whichever state want. Execution succeed value free angle
approach. saw Figure 1 values better others, respect
expected execution duration. Therefore, subgoal refinement determines values
free action parameters minimize predicted execution duration entire action
sequence. prediction made action models.
behavior shown applying subgoal refinement Figure 1(c) better performance, achieving ultimate goal less time. pleasing side-effect exhibits
seamless transitions actions.
proceed explaining executable actions instantiated
abstract PDDL plans generated VHPOP, free action parameters arise
process. Then, describe subgoal refinement optimizes free action parameters.
6.1 Action Instantiation Free Action Parameters
Causal links specify action executed previously achieve effect meets
precondition current action. chain abstract actions represents valid
504

fiRefining Execution Abstract Actions Learned Action Models

plan achieve goal, example shown Figure 10. next step
system map plans executable actions action library. process
also known operator instantiation (Schmill et al., 2000).
Initial 0 : (robot pos1) (ball pos2) (final pos3)
Step 2

: (approachball pos1 pos2)
0
-> (ball pos2)
0
-> (robot pos1)

Step 1

: (dribbleball pos2 pos3)
2
-> (atball pos2)

Goal

:
0
1

-> (final pos3)
-> (atball pos3)

Figure 10: output VHPOP PDDL plan causal links.
PDDL plans instantiated executable actions first extracting symbolic actions causal links plan, instantiating symbolic actions one one,
listed Algorithm 2. symbolic action, executable action retrieved
name (line 5), parameters requested (line 6). next step
determine parameter values executable action, considering corresponding
symbolic parameters PDDL plan. correspondence executable action parameter symbolic action parameter determined based index
executable action parameter (line 8).
symbolic parameters meaning belief state.
labels used PDDL plan. However, causal links define predicates labels
meaning belief state. predicates therefore retrieved (line
9), used extract correct values belief state (line 10).
Mapping symbolic predicates continuous values done belief state,
call made line 10. predicate holds current belief state, case
starts 0 (the initial state considered first action), simply retrieves
value. 0robot x would return x-coordinate current position
robot. Predicates hold current state also constrain values.
instance, atball predicate restricts translational velocity 0 0.3m/s.
predicates impose constraints, default values parameter types returned. instance, values x-coordinates must within field, angles
always - . several predicates hold, ranges values return
composed.
6.2 Optimizing Free Action Parameters
Mapping abstract PDDL plans executable actions often leads free action parameters.
Humans also high level redundancy actions. Wolpert Ghahramani
(2000) note, many ways bring glass water lips,
sensible silly. reason typically witness stereotypical sensi505

fiStulp & Beetz

input : pddl output (the output VHPOP, see Figure 10 example)
output : exe actions (a parameterized sequence executable actions)
1 pddl actions = parseActions(pddl output);
2 pddl links = parseCausalLinks(pddl output);

3
4
5
6
7
8
9
10
11
12
13
14

15

// example Figure 10, following holds:
// pddl actions = [(approachball pos1 pos2),(dribbleball pos2 pos3)]
// pddl links = {pos3=[0center,1atball], pos1=[0robot], pos2=[0ball,2atball]}
exe actions = {};
foreach pddl action pddl actions
exe action = getAction(pddl action.name) // e.g. exe action = approachBall
exe params = exe action.getParameters() // exe params = [x0,y0,...]
foreach exe par exe params
pddl par = pddl action.params[exe par.index] ;
// e.g. exe par = x0, exe par.index = 0 pddl par = pos1
pddl predicates = pddl links[pddl par] ;
// e.g. pddl par = pos1, pddl predicates = [0robot]
value = beliefState.getValue(exe par.name, pddl predicates) ;
exe action.setParameter(exe par, value);
end
exe actions.add(exe action);
end
// example Figure 10 Figure 9(a), following holds:
// exe actions = [ approachBall(x=0,y=1,=0,v=0, xg=3,yg=1,g=[-,],vg=[0,0.3]),
// dribbleBall(x=3,y=1,=[-,],v=[0,0.3], xg=1,yg=3,g=2.6,vg=0) ]
return exe actions;

Algorithm 2: Action Instantiation. Implementation line 3 Algorithm 1.

ble fluent movement redundancy exploited optimize subordinate
criteria (Schaal & Schweighofer, 2005), cost functions (Wolpert & Ghahramani, 2000),
energy efficiency variance minimization.
also take approach, optimize free parameters action sequences
respect expected execution duration. optimize action sequence, system
find values free action parameters overall predicted execution
duration sequence minimal. overall performance estimated simply
summing action models actions constitute sequence (Stulp & Beetz,
2005).
Let us take example action sequence line 14 Algorithm 2
example. Figure 11, Figures 1 7 combined. first two polar plots represent
predicted execution duration two individual actions different values
free angle approach. overall duration computed simply adding two,
depicted third polar plot. fastest time execute first approachBall
read first polar plot. 2.5s, angle approach 0.0 degrees, indicated
first plot. However, total time executing approachBall dribbleBall
angle 7.4s, second action takes 4.9s. third plot clearly shows
optimal overall performance. minimum actually 6.5s, angle
50 . Beneath polar plots, situation Figure 1 repeated, time
predicted performance action.
506

fiRefining Execution Abstract Actions Learned Action Models

Figure 11: Selecting optimal subgoal finding optimum summation
action models chain.

reasons clarity, one parameter optimized example, simply
read minima plot. Online however, robots must able determine
minimum automatically, possibly several free action parameters resulting
high-dimensional search spaces. next sections describe two optimization methods.
6.2.1 Analytical Optimization Model Trees
Figure 7, action model clearly consists line segments 1-dimensional feature
space. general, model trees partition d-dimensional feature space k partitions,
represent data partition d-dimensional hyperplane.
representation allows analytical minimization model trees. solution idea
minimum hyperplane found quickly determining values
corners, taking corner minimum value. procedure repeated
k hyperplanes, leads k corner minima. global minimum
determined choosing minimum minimal corners. novel analytical method
complexity O(kd), k number hyperplanes (i.e. number
rules, leaves model tree), number dimensions. Therefore, method
suffer curse dimensionality, complexity linearly depends
number dimensions, instead exponentially.
Determining minimum two model trees done first merging model
trees one, determining minimum new model tree. Unfortunately,
cases model trees cannot merged, instance nonlinear mapping action parameters features derived them.
cases, analytical optimization summations trees possible, genetic algorithm
used. implementation analytical optimization, model tree merging, well
507

fiStulp & Beetz

formalization special cases model trees cannot merged presented
Stulp (2007).
6.2.2 Optimization Genetic Algorithm
cases model trees cannot optimized analytically, use genetic algorithm
optimization. Note problem optimizing action sequence optimization
simplified straightforward function optimization search space
determined free action parameters ranges, target function whose
minimum sought determined model trees, Figure 11. Since model trees
many discontinuities therefore differentiated, chosen genetic algorithm optimization, applied well non-differentiable non-continuous
functions. Figure 12 depicts optimization genetic algorithm (Goldberg, 1989)
integrated overall system. top, instantiated action sequence bound
free action parameters requested optimized.
Note parameters labeled identification number (ID). Action parameters ID symbolic parameters derived
same, see line 8 Algorithm 2. reasons brevity, ID allocation included Algorithm 2. IDs reflect certain parameters different actions always
value, identical. instance, goal orientation (g ) approachBall
equivalent initial orientation () dribbleBall. Therefore share ID 13.
Note free action parameter.
first step partition action parameters action sequence two sets:
one set contains action parameters bound certain value instantiation,
set contains free action parameters, along range values
take. Note action parameters ID stored sets,
value.
free action parameter represented floating point gene chromosome.
number chromosomes population number free parameters multiplied
25. chromosomes initial population initialized random values
respective ranges. standard genetic algorithm (GA) loop started. loop
halts best fitness changed last 50 generations, 500 generations
evaluated. optimal values free parameters also bound action
sequence.
chromosome, predicted execution duration determined calling action
models fixed values set bound parameters, values free
parameters represented chromosome. fitness proportionate selection, fitness
non-negative number larger increased fitness. Therefore,
chromosome c fitness f computed fc = tmax + tmin tc , tmax tmin
maximum minimum execution duration chromosomes respectively.
implementation genetic algorithm uses elitarianism (2% best individuals
passes next generation unmodified), mutation (on remaining 98%), two-point
crossover (on 65% individuals), fitness proportionate selection (the chance
selected crossover proportionate individuals fitness). test evaluate
GA implementation C++, first applied several optimization benchmarks,
508

fiRefining Execution Abstract Actions Learned Action Models

Figure 12: Optimization subgoal refinement genetic algorithm. Implementation
line 5 Algorithm 1.

De Jongs function, Schwefels function Ackleys Path function. results
optimization times reported Koska (2006).
subgoal refinement scenarios presented Section 6.3, optimization time
usually small comparison gain performance. complex kitchen
scenario, 4 actions 10 action parameters optimized,
implementation GA still takes less 0.5s get good result.
6.3 Empirical Evaluation
section, introduce different scenarios results applying
subgoal refinement evaluated. robotic soccer domain, action sequence
optimized approachBall action, followed dribbleBall action, Figure 1.
free action parameters intermediate state angle approach
translational velocity.
evaluate effect subgoal refinement service robotics domain, two scenarios
tested. first scenario, goal put cup one table another,
achieved sequence navigation grasping actions. evaluation episode,
topology environment scenario stays same, initial robot position,
509

fiStulp & Beetz

tables, cups randomly displaced. Scenario 2 variation Scenario 1,
two cups delivered.
kitchen scenarios many free action parameters. preconditions usually
bind either navigation (goToPose) manipulation (grip put) actions never
(they independent), one action parameter sets always free. Furthermore,
distance robot table order grab cup must 40 80cm (as
fixed precondition grip). range another free parameter. soccer
domain, velocity orientation waypoints also fixed, free optimization
well.
arm control domain, sequences reaching movements performed.
particular task require abstract planning, use VHPOP. demonstration purposes, arm draw first letter first name author
paper Stulp, Koska, Maldonado, Beetz (2007), chose 4/5 waypoints accordingly.
Figure 13(a) shows PowerCube arm, attached B21 robot, drawing F.
draw letters, two six degrees freedom arm used. free
action parameters angular velocities waypoints.
Table 3 lists results applying subgoal refinement different domains
scenarios, number actions sequence, n number episodes
tested. baseline subgoal refinement compared greedy approach,
next subgoal optimized respect execution duration
current action. case, say horizon h optimization 1. downside
greedy baseline also depends accuracy action model. However,
chose baseline, setting free action parameters zero certainly leads
worse execution times, optimizing manually introduces human bias.
Scenario
Soccer (real)
Soccer (simulated)
Kitchen (scenario 1)
Kitchen (scenario 2)
Arm control


2
2
4
13
4-5

n
100
1000
100
100
4

t1
10.6s
9.8s
46.5s
91.7s
10.6s

t2
9.9s
9.1s
41.5s
85.4s
10.0s

t2/1
6.1%
6.6%
10.0%
6.6%
5.7%

p
0.00
0.00
0.00
0.00
0.08

Table 3: Subgoal refinement results.

execution time single action denoted teh , two indices referring
horizon episode. instance t64
1 refers total execution time
64th episode, performed horizon 1. mean
P overall execution duration
episodes denoted t1 , computed t1 = n1 ne=1 te1 . Since subgoal refinement
optimizes execution duration current next action, horizon 2.
fourth column lists thePmean overall execution duration subgoal refinement t2 ,
computed t2 = n1 ne=1 te2 .
P
equation t2/1 = n1 ne=1 (1 te2 /te1 ) used compute mean improvement
episodes achieved subgoal refinement. p-value improvement computed
using dependent t-test repeated measures, episode performed twice,
510

fiRefining Execution Abstract Actions Learned Action Models

with, without subgoal refinement. significant improvement occurs one
domain.
visualize qualitative effect applying subgoal refinement, results
arm control domain depicted Figure 13(b). angular velocities set zero
(upper row) optimized subgoal refinement (lower row). axes represent
angles two joints. figure demonstrates trajectories smoother
subgoal refinement: arm mostly draws one long stroke, rather discernible line
segments. Since arm control domain mainly included visualization purposes,
four test runs. reason overall improvement significant
(p > 0.05).

(a) B21 robot drawing F
PowerCube arm.

(b) Drawing letters without (upper row) (lower row)
subgoal refinement. refinement, letters drawn faster
smoother.

Figure 13: Arm control experiment.
Although optimizing execution duration leads smoother motion domain,
smooth human arm motion arises variability rather time optimization (Harris &
Wolpert, 1998; Simmons & Demiris, 2004). article, main goal explain
model human motion, rather demonstrate effects optimizing sequences
actions.
6.3.1 Sequences Actions
far, seen optimization horizons h = 1 (greedy) h = 2. standard
approach h = 2 easily extended, subgoal refinement optimizes
execution duration next h > 2 actions. higher horizon h, subgoal
refinement preparing actions future.
evaluate effect optimizing two actions, sequences four
actions optimized using subgoal refinement different horizons. Two scenarios
used: simulated soccer scenario robot traverse four waypoints
goToPose action, two kitchen scenarios. action execution, subgoals
subsequent actions recomputed horizon, less number
remaining actions smaller h. results summarized Table 4. first
row represents baseline greedy approach h = 1, second row represents
results reported far h = 2. next two rows list results optimizing 3 4
511

fiStulp & Beetz

action execution durations. Again, reported times represent execution duration
entire action sequence, averaged 100 episodes.
horizon
h=1
h=2
h=3
h=4

P

22.7
20.3
20.2
20.2

Soccer
Imp. p-value
10.6%
0.7%
0.2%

0.000
0.001
0.053

Kitchen (Scen.1)
P
Imp. p-value
46.5
41.5 10.0%
0.000
40.6
1.5%
0.041
-

Kitchen (Scen.2)
P
Imp. p-value
91.7
85.4 6.6%
0.041
85.3 0.1%
0.498
-

Table 4: Effect subgoal refinement horizon h performance improvement.

three scenarios, substantial improvement h = 1 h = 2,
h = 2 h = 3 improvement marginal insignificant. executing action
scenarios, apparently beneficial prepare next action, action that. insight also main motivation behind receding horizon control,
described Section 3.2. However, also important differences RHC
subgoal refinement. First all, optimal control plans optimizes motor commands
fixed duration, whereas subgoal refinement durative actions varying duration, higher level temporal abstraction. Furthermore, RHC optimizes first
Hp motor commands, whereas subgoal refinement optimizes action parameters partially
fixed subgoals, concerned actual motor commands needed reach
subgoals. planner therefore fix general structure plan, rather committing first steps. Finally, optimal control assumes precise analytic
models actions systems behavior available. many robotic systems,
well humans, hold. However, lack analytic models keep
us acquiring models experience. learning action models, system also
flexible enough acquire action models changing actions, actions model
acquired analysis.
6.3.2 Predicting Performance Decrease
cases subgoal refinement effect. ball approach
scenario instance, robot, ball final destination perfectly aligned,
much subgoal refinement, greedy approach already delivers
optimal angle approach: straight towards ball. contrary, refining subgoals
cases might put unnecessary constraints execution. Due inaccuracies
action models optimization techniques, sometimes even case
greedy approach slightly better subgoal refinement.
evaluate effects, 1000 episodes executed simulation h = 1
h = 2. Although overall improvement h = 2 6.6% (see Table 3), 160 1000
episodes actually lead increased execution duration. episodes labeled -,
remaining +. trained decision tree predict nominal value.
tree yields four simple rules predict performance difference correctly 87%
given cases. learned decision tree essentially action model too. Rather
512

fiRefining Execution Abstract Actions Learned Action Models

predicting outcome individual action, predicts outcome applying action
models actions.
performed another 1000 test episodes, described above, applied subgoal
refinement decision tree predicted applying would yield better performance.
overall improvement raised 6.6% 8.6%.

7. Subgoal Assertion
practice, learning actions hardly starts scratch, knowledge previously
learned actions transfered novel task contexts. humans robots instance, approaching ball similar navigating without considering ball.
abstract level, involve going state another field, Figure 14(a), implemented execute efficiently fast possible.
However, also slight differences two tasks. approaching
ball important bump achieving desired state, depicted
Figure 14(b).

(a) Original scenario.

(b) Refined goal, learned
condition refinement.

(c) Subgoal assertion based condition refinement.

Figure 14: Computational model condition refinement subgoal assertion.
Figure 14 illustrates small differences. first scenario, ball,
effects goToPose actions satisfy goal, actions achieve goal.
Figure 14(b) basically scenario, added requirement robot
must possession ball goal state. goToPose action used
achieve goals. Since goToPose aware ball, often collides ball
achieving desired state. new goal approaching ball essentially
refined subset former goal simply navigating there. executing goToPose,
robot left succeeds approaching ball, robot right not,
bumps ball beforehand. case effects goToPose
longer satisfy refined goal, depicted scenario.
effects goToPose partitioned subset satisfy new
refined goal, subset not. represented blue (S) red (F)
respectively. Analogously, preconditions partitioned subset Success
leads final state subset effects satisfy refined goal,
subset Fail case. effects, consequently,
preconditions action refined new task, call condition refinement.
513

fiStulp & Beetz

refined precondition novel goal known, easy determine
particular initial state lead successful execution not. does, action
executed is. instance, robot left simply execute goToPose action,
refined precondition. shall see, goToPose action used
approach ball successfully almost half time.
However, robot right cannot simply use goToPose approach ball.
robot needs novel action, e.g. approachBall, enables go
states Fail refined goal. it? Instead, goToPose action used
again, take robot Fail subset Success subset. done,
goToPose action succeed approaching ball executed.
key reuse therefore able predict action fail,
succeed novel task. predicted succeed, action executed
is. action fail, another action executed beforehand,
robot ends state action succeed, depicted Figure 14(c).
intermediate state actions new subgoal. approach therefore called
subgoal assertion.

input : exe actions (a sequence (partially) instantiated actions)
output : exe actions2 (a sequence (partially) instantiated actions asserted subgoals)
1 exe actions2 = {};
2 foreach exe action exe actions
3
switch exe action.name
4
case goToPose
exe actions2.add (exe action) // Subgoal assertion never needed action
5
6
end
7
case approachBall

8
9
10
11
12

13
14
15
16
17
18
19
20
21

// Get parameters related states.
// Uses indexing scheme lines 6-8 Algorithm 2..
exe params0 = exe action.getParameters(0);
exe params1 = exe action.getParameters(1);
goToPose.approachBallSuccess( exe params0, exe params1)
// goToPose job, subgoal assertion needed
exe actions2.add (new goToPose(exe params0, exe params1));
else
// exe params2 set default ranges action parameters goToPose.
// lines 10-11 Algorithm 2.
exe params2 = ...;
exe actions2.add (new goToPose(exe params0, exe params2));
exe actions2.add (new goToPose(exe params2, exe params1));
end
end
...
end
end
return exe actions2;

Algorithm 3: Implementation line 4 Algorithm 1.

514

fiRefining Execution Abstract Actions Learned Action Models

7.1 Integration Overall System
Subgoal assertion takes sequence actions, returns sequence asserted
subgoal needed assure successful execution, listed Algorithm 3. main
loop goes actions, leaves goToPose actions untouched. approachBall
implementation itself, replaced goToPose actions. one goToPose needed
predicted succeed approaching ball. case initial state
Success subset Figure 14(b). Determining subsets manually difficult task,
due complex interactions dynamics shape robot, well
specific characteristics action. Therefore, subsets learned decision
tree, described Section 5.4.
success predicted, one goToPose executed is, parameters
approachBall action. predicted fail, subgoal asserted (exe params2),
inserted two goToPose actions. action parameters exe params2 initially
receive default ranges. parameters exe params2 free, optimized
subgoal refinement. immediately follows subgoal assertion, listed Algorithm 1.
ensures values exe params2 minimize predicted execution duration,
transition two goToPose actions smooth.
One issue remains open. intermediate goal actions must lie within
Success subset Figure 14, ball approach task position green
area left Figure 8. requirement puts constraints values exe params2.
must ensured optimization process subgoal refinement considers
states Success subset refined precondition second goToPose action.
Therefore, action model action modified returns INVALID
flag states. approach chosen requires little modification
optimization module. Chromosomes lead invalid value simply receive low
fitness.
1 goToPose.executionDurationApproachBall (x,y,,v,xg ,yg ,g ,vg ) {
2
goToPose.approachBallSuccess ( x,y,,v,xg ,yg ,g ,vg )
3
return goToPose.executionDuration (x,y,,v,xg ,yg ,g ,vg );
4
else
5
return INVALID;
6
end
7 }

Algorithm 4: Modified goToPose action model approaching ball.
Analogously Figure 11, predicted execution durations two actions, well
summation depicted Figure 15. Invalid values rendered. second
graph depicts function described Algorithm 4. Note due removal invalid
values, shape functions ground plane last two graphs corresponds
Figure 8 16(a).
Figure 16(a), three instances problem depicted. Since robot
left area collision predicted, simply executes goToPose, without
asserting subgoal. model predicts two robots collide ball
515

fiStulp & Beetz

Figure 15: Search space subgoal refinement subgoal assertion.

executing goToPose, subgoal asserted. subgoals, determined subgoal
refinement, depicted well.

Fail
Predicted
Fail
Success

(a) Three examples. Subgoal assertion applied two them.

2%

Observed
Success

(52%50%)

60%

1%

3%

(10%+50%)

37%

97%




62%
38%

(b) Mean results 100 episodes. Almost predicted failures
(50% 52%) transformed yield successful execution.

Figure 16: Results subgoal assertion.

7.2 Empirical Evaluation
evaluate automatic subgoal assertion, hundred random ball approaches executed
simulation, subgoal assertion, without. results summarized
Table 16(b). Without assertion, results similar results reported Table 2.
collision correctly predicted approximately half time: 52% hundred
episodes. Subgoal assertion applied cases, almost always successful: 50%
episodes predicted fail executed successfully. 2% episodes
still collision, despite subgoal assertion. subgoal assertion applied
Success predicted, change lower prediction row. Consciously choosing
apply subgoal assertion applying equivalent.
Subgoal assertion applied unnecessarily 10% episodes, means subgoal assertion applied, even though original sequence would already successful.
However, execution subgoal assertion consequent subgoal refinement significant 8% slower executing one goToPose action. performance loss
cases seems acceptable cost compared pay-off dramatic increase
number successful task completions.
516

fiRefining Execution Abstract Actions Learned Action Models

Summarizing: subgoal assertion necessary, usually applied. subgoal
introduced half time, raises successful task completion 47 97%. Infrequently, subgoals introduced inappropriately, leads small loss performance
terms execution duration.

8. Conclusion Outlook
Durative actions provide conceptual abstraction reasoned either
designer action selection design, or, abstraction explicitly coded
controller, action selection system itself. Action abstractions partially achieve
abstraction ignoring action parameters. Although parameters relevant
action abstract level, often relevant performance success
executing plan.
robots learn predict effects performance actions, use
knowledge optimize behavior subgoal refinement, avoid failures
subgoal assertion. empirical evaluations verify leads efficient, robust,
effective behavior. believe important contributions towards bridging
gap abstract planning systems robot action execution.
multi-robot scenarios robotic soccer, robots provided action
models teammates, enabling robot reason actions others.
shown enables robots implicitly coordinate actions others, without
resort utility communication (Stulp et al., 2006). also successfully
applied approach heterogeneous team robots, robots two different
research groups (Isik, Stulp, Mayer, & Utz, 2006).
Also, preliminary results showing even accurate models learned
data gathered online operation time (Kirsch & Beetz, 2007). Extended
operation times enable robot gather training data, actions also
called parameterizations typical domain context robot
used. contrast current method, generates action parameterizations
randomly choosing possible parameterizations. Given amount
data, model generalize possible parameterizations tend less
accurate model subset parameterizations.
main assumption underlying article human-specified knowledge
robot-learned knowledge complement well robot controllers. exemplary recent winners two well-known robotic benchmarks, RoboCup mid-size
league (Gabel, Hafner, Lange, Lauer, & Riedmiller, 2006) DARPA challenge (Thrun
et al., 2006), emphasize success could achieved combination
manual coding experience-based learning. specifically, believe ideally
human designers specify action abstractions offline. task execution, robot automatically optimize aspects actions relevant
execution learned action models.
Future work includes learning several models different performance measures,
optimizing several performance measures simultaneously. instance, energy consumption
another important performance measure autonomous mobile robots. specifying objective functions consist combinations energy consumption execution
517

fiStulp & Beetz

duration, optimized. weighting individual performance functions differently, function optimized customized specific scenarios. instance,
mid-size league robotic soccer, short operation time 15 minutes, speed far
important energy consumption. service robotics way around.
Also, intend evaluate use learning algorithms predicting failure
actions, instance Neural Networks (Buck & Riedmiller, 2000) Support Vector Machines (Hart, Ou, Sweeney, & Grupen, 2006). learning algorithms predict action
failures accurately, even better results expected subgoal refinement
subgoal assertion.

Acknowledgments
would like thank Andreas Fedrizzi anonymous reviewers valuable remarks suggestions. research described article partly funded
German Research Foundation (DFG) SPP-1125, Cooperating Teams Mobile
Robots Dynamic Environments, also CoTeSys cluster excellence (Cognition Technical Systems, http://www.cotesys.org), part Excellence Initiative
DFG.

Appendix
Robot

State Estimation

Soccer
(real)
Soccer
(simulated)
Kitchen

Probabilistic state estimation based camera
Probabilistic state estimation based camera
Ground truth, noise based noise model
Ground truth field view, noise
Ground truth
Ground truth
Ground truth field view

Arm control

Joint angles read directly motor encoders










Belief State

Motor comm.

x, y, , v,
xball , yball
x, y, , v,
xball , yball
x, y, , v,
ax, ay, az
[xo , yo , zo ]n
, , b , b

v,
v,
v,
ax, ay, az
a, b

Table 5: State variables belief state, state estimation process used acquire them,
motor command domain. x, y, , v dynamic pose robot body,
ax, ay, az relative position arm robot body, [xo , yo , zo ]n absolute positions n objects, v, translational/rotational velocities, current sent
arm joint motor.

518

fiRefining Execution Abstract Actions Learned Action Models

Robot

Action

Soccer

goToPose
dribbleBall
goToPose
reach
reach

Kitchen
Arm control

Action Parameters
Current
Goal
x, y, , v
xg , yg , g , vg
x, y, , v, xball , yball
xg , yg , g , vg
x, y, , v
xg , yg , g , vg
x, y, z, ax, ay, az xg , yg , zg , axg , ayg , azg
, , b , b
ga , ga , gb , gb

Table 6: List actions used application domains. list actions might shorter
expected. instance, doubtful robots could play soccer
navigate certain pose. aim article demonstrate
actions reused customized perform well varying
task contexts.
Robot

Action

Features

Soccer

goToPose
dribbleBall

Kitchen

goToPose

p
v, dist = (x xg )2 + (y yg )2 ,
angle = |angle tosigned |,
angle = sgn(angle tosigned )
norm(g atan2(yg y, xg x))
vg , dist, angle to, angle at,
angle =p
|norm(g )|
distxyz =p (x xg )2 + (y yg )2 + (z zg )2
distxy = (x xg )2 + (y yg )2 , distxz , distyz ,
anglexyq= atan2(yg y, xg x), anglexz , angleyz

reach

Arm control

reach

dist =

(a ga )2 + (b gb )2 ,

angle1 = norm(atan2(gb b , ga ) atan2(b , )),
b b , ) + atan2(b , ))
angle
g
g g
q2 = norm(atan2(
q g
2
2
2
2
v = + b , vg = ga + gb
norm(a): adds subtracts 2 range [, ]
angle tosigned = norm(atan2(yg y, xg x) )
Table 7: feature spaces used learn action models

References
Arkin, R. (1998). Behavior based Robotics. MIT Press, Cambridge, Ma.
Astrom, K. J., & Murray, R. M. (2008). Feedback Systems: Introduction Scientists
Engineers, chap. Supplement optimization-based control. Princeton University Press.
Barto, A., & Mahadevan, S. (2003). Recent advances hierarchical reinforcement learning. Discrete
event systems, 13 (1-2), 4177.
Beetz, M., & Belker, T. (2000). XFRMLearn - system learning structured reactive navigation
plans. Proceedings 8th International Symposium Intelligent Robotic Systems.
519

fiStulp & Beetz

Beetz, M. (2000). Concurrent Reactive Plans: Anticipating Forestalling Execution Failures, Vol.
LNAI 1772 Lecture Notes Artificial Intelligence. Springer Publishers.
Beetz, M. (2001). Structured Reactive Controllers. Journal Autonomous Agents Multi-Agent
Systems. Special Issue: Best Papers International Conference Autonomous Agents
99, 4, 2555.
Beetz, M., Schmitt, T., Hanek, R., Buck, S., Stulp, F., Schroter, D., & Radig, B. (2004). AGILO
robot soccer team experience-based learning probabilistic reasoning autonomous robot
control. Autonomous Robots, 17 (1), 5577.
Belker, T. (2004). Plan Projection, Execution, Learning Mobile Robot Control. Ph.D. thesis,
Department Applied Computer Science, University Bonn.
Bellingham, J., Richards, A., & How, J. P. (2002). Receding horizon control autonomous aerial
vehicles. Proceedings 2002 American Control Conference, Vol. 5, pp. 37413746.
Benson, S. (1995). Inductive learning reactive action models. International Conference
Machine Learning (ICML), pp. 4754.
Bertram, D., Kuffner, J., Dillmann, R., & Asfour, T. (2006). integrated approach inverse
kinematics path planning redundant manipulators. Proceedings IEEE International Conference Robotics Automation (ICRA), pp. 18741879.
Bouguerra, A., & Karlsson, L. (2005). Symbolic probabilistic-conditional plans execution mobile
robot. IJCAI-05 Workshop: Reasoning Uncertainty Robotics (RUR-05).
Brock, O., & Khatib, O. (1999). Elastic Strips: framework integrated planning execution.
Proceedings International Symposium Experimental Robotics, pp. 329338.
Buck, S., Beetz, M., & Schmitt, T. (2002a). M-ROSE: Multi Robot Simulation Environment
Learning Cooperative Behavior. Asama, H., Arai, T., Fukuda, T., & Hasegawa, T. (Eds.),
Distributed Autonomous Robotic Systems 5, Lecture Notes Artificial Intelligence, LNAI.
Springer-Verlag.
Buck, S., Beetz, M., & Schmitt, T. (2002b). Reliable Multi Robot Coordination Using Minimal
Communication Neural Prediction. Beetz, M., Hertzberg, J., Ghallab, M., & Pollack,
M. (Eds.), Advances Plan-based Control Autonomous Robots. Selected Contributions
Dagstuhl Seminar Plan-based Control Robotic Agents, Lecture Notes Artificial
Intelligence. Springer.
Buck, S., & Riedmiller, M. (2000). Learning situation dependent success rates actions
RoboCup scenario. Pacific Rim International Conference Artificial Intelligence, p. 809.
Cambon, S., Gravot, F., & Alami, R. (2004). robot task planner merges symbolic
geometric reasoning.. Proceedings 16th European Conference Artificial Intelligence
(ECAI), pp. 895899.
Clement, B. J., Durfee, E. H., & Barrett, A. C. (2007). Abstract reasoning planning coordination. Journal Artificial Intelligence Research, 28, 453515.
Coradeschi, S., & Saffiotti, A. (2001). Perceptual anchoring symbols action. Proceedings
International Joint Conference Artificial Intelligence (IJCAI), pp. 407416.
Dean, T., & Wellmann, M. (1991). Planning Control. Morgan Kaufmann Publishers.
Dearden, A., & Demiris, Y. (2005). Learning forward models robotics. Proceedings
Nineteenth International Joint Conference Artificial Intelligence (IJCAI), pp. 14401445.
Dietterich, T. G. (2000). Hierarchical reinforcement learning MAXQ value function decomposition. Journal Artificial Intelligence Research, 13, 227303.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planning
domains.. Journal Artificial Intelligence Research, 20, 61124.
520

fiRefining Execution Abstract Actions Learned Action Models

Fox, M., Ghallab, M., Infantes, G., & Long, D. (2006). Robot introspection learned hidden
markov models. Artificial Intelligence, 170 (2), 59113.
Gabel, T., Hafner, R., Lange, S., Lauer, M., & Riedmiller, M. (2006). Bridging gap: Learning
RoboCup simulation midsize league. Proceedings 7th Portuguese Conference
Automatic Control.
Gerkey, B., Vaughan, R. T., & Howard, A. (2003). Player/Stage Project: Tools multirobot distributed sensor systems. Proceedings 11th International Conference
Advanced Robotics (ICAR), pp. 317323.
Ginsberg, M. L. (1989). Universal planning: (almost) universally bad idea. AI Magazine, 10 (4),
4044.
Goldberg, D. E. (1989). Genetic Algorithms Search, Optimization Machine Learning. Kluwer
Academic Publishers.
Haigh, K. Z. (1998). Situation-Dependent Learning Interleaved Planning Robot Execution.
Ph.D. thesis, School Computer Science, Carnegie Mellon University.
Harris, C. M., & Wolpert, D. M. (1998). Signal-dependent noise determines motor planning. Nature,
394 (20), 780784.
Hart, S., Ou, S., Sweeney, J., & Grupen, R. (2006). framework learning declarative structure.
Workshop Manipulation Human Environments, Robotics: Science Systems.
Haruno, M., Wolpert, D. M., & Kawato, M. (2001). MOSAIC model sensorimotor learning
control. Neural Computation, 13, 22012220.
Haruno, M., Wolpert, D. M., & Kawato, M. (1999). Multiple paired forward-inverse models
human motor learning control. Proceedings 1998 conference Advances
neural information processing systems II, pp. 3137, Cambridge, MA, USA. MIT Press.
Hoffmann, J., & Duffert, U. (2004). Frequency space representation transitions quadruped
robot gaits. Proceedings 27th Australasian computer science conference (ACSC), pp.
275278.
Infantes, G., Ingrand, F., & Ghallab, M. (2006). Learning behavior models robot execution
control. Proceedings 17th European Conference Artificial Intelligence (ECAI),
pp. 678682.
Isik, M., Stulp, F., Mayer, G., & Utz, H. (2006). Coordination without negotiation teams
heterogeneous robots. Proceedings RoboCup Symposium, pp. 355362.
Jacobs, R. A., & Jordan, M. I. (1993). Learning piecewise control strategies modular neural
network. IEEE Transactions Systems, Man Cybernetics, 23 (3), 337345.
Jaeger, H., & Christaller, T. (1998). Dual dynamics: Designing behavior systems autonomous
robots. Artificial Life Robotics, 2 (3), 108112.
Jordan, M. I., & Rumelhart, D. E. (1992). Forward models: Supervised learning distal teacher.
Cognitive Science, 16, 307354.
Kirsch, A., & Beetz, M. (2007). Training job collecting experience hierarchical hybrid
automata. Hertzberg, J., Beetz, M., & Englert, R. (Eds.), Proceedings 30th German
Conference Artificial Intelligence (KI-2007), pp. 473476.
Kitano, H., Asada, M., Kuniyoshi, Y., Noda, I., & Osawa, E. (1997). RoboCup: robot world
cup initiative. Proceedings first international conference autonomous agents
(AGENTS), pp. 340347.
Kollar, T., & Roy, N. (2006). Using reinforcement learning improve exploration trajectories error minimization. Proceedings International Conference Robotics Automation
(ICRA), pp. 33383343.
521

fiStulp & Beetz

Koska, W. (2006). Optimizing autonomous service robot plans tuning unbound action parameters.
Masters thesis, Technische Universiat Munchen.
Kovar, L., & Gleicher, M. (2003). Flexible automatic motion blending registration curves.
Proceedings 2003 ACM SIGGRAPH/Eurographics symposium computer animation
(SCA), pp. 214224.
Muller, A., & Beetz, M. (2006). Designing implementing plan library simulated household
robot. Beetz, M., Rajan, K., Thielscher, M., & Rusu, R. B. (Eds.), Cognitive Robotics:
Papers AAAI Workshop, Technical Report WS-06-03, pp. 119128, Menlo Park,
California. American Association Artificial Intelligence.
Nakanishi, J., Cory, R., Mistry, M., Peters, J., & Schaal, S. (2005). Comparative experiments task
space control redundancy resolution. IEEE International Conference Intelligent
Robots Systems (IROS), pp. 39013908.
Nilsson, N. J. (1984). Shakey robot. Tech. rep. 323, AI Center, SRI International.
Nilsson, N. J. (1994). Teleo-reactive programs agent control. Journal Artificial Intelligence
Research, 1, 139158.
Perlin, K. (1995). Real time responsive animation personality. IEEE Transactions Visualization Computer Graphics, 1 (1), 515.
Quinlan, R. (1992). Learning continuous classes. Adams, A., & Sterling, L. (Eds.), Proceedings 5th Australian Joint Conference Artificial Intelligence, pp. 343348.
Russell, S., & Norvig, P. (2003). Artificial Intelligence - Modern Approach. Prentice Hall, Upper
Saddle River, New Jersey.
Ryan, M., & Pendrith, M. (1998). RL-TOPs: architecture modularity re-use reinforcement learning. Proceedings 15th International Conference Machine Learning (ICML),
pp. 481487.
Saffiotti, A., Ruspini, E. H., & Konolige, K. (1993). Blending reactivity goal-directedness
fuzzy controller. Proceedings IEEE International Conference Fuzzy Systems, pp.
134139, San Francisco, California. IEEE Press.
Schaal, S., & Schweighofer, N. (2005). Computational motor control humans robots. Current
Opinion Neurobiology, 15, 675682.
Schmill, M. D., Oates, T., & Cohen, P. R. (2000). Learning planning operators real-world,
partially observable environments. Proceedings 5th International Conference
Artificial Intelligence Planning Systems (ICAPS), pp. 246253.
Shahaf, D., & Amir, E. (2006). Learning partially observable action schemas.. Proceedings
21st National Conference Artificial Intelligence (AAAI).
Simmons, G., & Demiris, Y. (2004). Biologically inspired optimal robot arm control signaldependent noise. Proceedings IEEE International Conference Intelligent Robots
Systems (IROS), pp. 491496.
Smith, D., Frank, J., & Jonsson, A. (2000). Bridging gap planning scheduling.
Knowledge Engineering Review, 15 (1), 4783.
Stulp, F. (2007). Tailoring Robot Actions Task Contexts using Action Models. Ph.D. thesis,
Technische Universitat Munchen.
Stulp, F., & Beetz, M. (2005). Optimized execution action chains using learned performance
models abstract actions. Proceedings Nineteenth International Joint Conference
Artificial Intelligence (IJCAI).
Stulp, F., Isik, M., & Beetz, M. (2006). Implicit coordination robotic teams using learned prediction models. Proceedings IEEE International Conference Robotics Automation
(ICRA), pp. 13301335.
522

fiRefining Execution Abstract Actions Learned Action Models

Stulp, F., Koska, W., Maldonado, A., & Beetz, M. (2007). Seamless execution action sequences.
Proceedings IEEE International Conference Robotics Automation (ICRA),
pp. 36873692.
Sussman, G. J. (1973). computational model skill acquisition. Ph.D. thesis, Massachusetts
Institute Technology.
Thrun, S. et al. (2006). Stanley, robot tFhe DARPA grand challenge. Journal Field
Robotics, 23 (9), 661692.
Todorov, E., Li, W., & Pan, X. (2005). task parameters motor synergies: hierarchical
framework approximately optimal control redundant manipulators. Journal Robotic
Systems, 22 (11), 691710.
Uno, Y., Wolpert, D. M., Kawato, M., & Suzuki, R. (1989). Formation control optimal
trajectory human multijoint arm movement - minimum torque-change model. Biological
Cybernetics, 61 (2), 89101.
Utz, H., Kraetzschmar, G., Mayer, G., & Palm, G. (2005). Hierarchical behavior organization.
Proceedings 2005 International Conference Intelligent Robots Systems (IROS),
pp. 25982605.
Wimmer, M., Stulp, F., Pietzsch, S., & Radig, B. (2008). Learning local objective functions robust
face model fitting. IEEE Transactions Pattern Analysis Machine Intelligence (PAMI),
30 (8). appear.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical machine learning tools techniques
(2nd edition). Morgan Kaufmann, San Francisco.
Wolpert, D., & Ghahramani, Z. (2000). Computational principles movement neuroscience. Nature
Neuroscience Supplement, 3, 12121217.
Wolpert, D. M., & Flanagan, J. (2001). Motor prediction. Current Biology, 11 (18), 729732.
Younes, H. L. S., & Simmons, R. G. (2003). VHPOP: Versatile heuristic partial order planner.
Journal Artificial Intelligence Research, 20, 405430.

523

fiJournal Artificial Intelligence Research 32 (2008) 355-384

Submitted 10/07; published 05/08

Spectrum Variable-Random Trees
Fei Tony Liu
Kai Ming Ting

TONY. LIU @ INFOTECH . MONASH . EDU . AU
KAIMING . TING @ INFOTECH . MONASH . EDU . AU

Gippsland School Information Technology,
Monash University, Australia

Yang Yu
Zhi-Hua Zhou

YUY @ LAMDA . NJU . EDU . CN
ZHOUZH @ LAMDA . NJU . EDU . CN

National Key Laboratory Novel Software Technology,
Nanjing University, China

Abstract
paper, show continuous spectrum randomisation exists, existing tree randomisations operating around two ends spectrum. leaves
huge part spectrum largely unexplored. propose base learner VR-Tree generates
trees variable-randomness. VR-Trees able span conventional deterministic
trees complete-random trees using probabilistic parameter. Using VR-Trees base
models, explore entire spectrum randomised ensembles, together Bagging Random Subspace. discover two halves spectrum distinct characteristics;
understanding allows us propose new approach building better decision
tree ensembles. name approach Coalescence, coalesces number points
random-half spectrum. Coalescence acts committee experts cater unforeseeable conditions presented training data. Coalescence found perform better single
operating point spectrum, without need tune specific level randomness.
empirical study, Coalescence ranks top among benchmarking ensemble methods including
Random Forests, Random Subspace C5 Boosting; Coalescence significantly better
Bagging Max-Diverse Ensemble among methods comparison. Although
Coalescence significantly better Random Forests, identified conditions
one perform better other.

1. Introduction
building ensemble-classifiers, randomisation plays important role forming diverse models generated deterministic algorithms. use ensemble methods, diverse
models aggregated improve generalisation capability resulting classifiers.
Traditionally, ensemble methods based deterministic algorithms randomisations injected produce diverse variants. Representatives Bagging (Breiman, 1996a), Random
Forests (Breiman, 2001), Randomised C4.5 (Dietterich, 2000) Random Subspace (Ho, 1998).
Recently, completely random approach (Fan, Wang, Yu, & Ma, 2003; Fan, 2004; Liu, Ting,
& Fan, 2005) proposed using trees generated without deterministic heuristic;
approach represents departure traditional approaches. paper, show
complete-random approach traditional approaches used two extremes
form continuous spectrum randomisation; better predictive accuracy often found
within spectrum.
c
2008
AI Access Foundation. rights reserved.

fiL IU , ING , U , & Z HOU

paper, propose novel algorithm capable generating range models,
end-to-end continuously completely random purely deterministic. striking fact
though tree-node created either randomly deterministically, resulting randomness
span completely random purely deterministic without modification ensemble method. algorithm enables us explore whole spectrum two extremes
show that, new algorithm easily incorporated existing ensemble methods,
Bagging Random Subspace. Together generate ensembles different degrees
randomness, largely unexplored now.
reveal existing random ensemble methods Bagging Random
Subspace focus deterministic-end spectrum, ignore major part spectrum.
show Bagging, Random Subspace simple complete-random trees find better
counterparts inside spectrum.
known way measure priori level randomness required given
problem, analyse spectrum discover two halves spectrum
distinctive characteristics. new understanding, new ensemble approach proposed
paper, coalesces number points spectrum form final ensembles.
Empirically, find new approach performs better single point spectrum
across wide range data sets. new approach off-the-shelf solution, provides
high level accuracy without need knowing tuning level randomness required.
paper presented follows. brief overview existing decision tree randomisation
methods provided Section 2. serves primer decision tree randomisation. algorithm generate variable-random-trees presented Section 3. Section 4, different
ensemble methods used experiment introduced, followed Section 5, presents
comprehensive empirical evaluation spectrum well proposed ensemble approach.
Section 6.1 details key differences randomisation framework Random Forests
proposed framework variable-randomness. related work provided Section 6.2,
conclude last section.

2. Randomisation Methods Decision Trees
Many randomisation methods proposed produce diverse decision trees ensembleclassifiers. section, general introduction decision tree randomisation, give
overview ways applied. following list decision tree randomisation
meant exhaustive, purpose list demonstrate mechanism side effects
randomisation methods, guides us designing better approaches.
conventional decision tree algorithm, one deterministic model produced given
training set. Randomisation helps produce multiple variants deterministic model fulfil
requirement ensemble learning. common characteristic popular methods
heuristic used every tree node, often restricts possible range randomness
reduces impact performance.
literature, proposed randomisation methods grouped three categories, depending dimension applied. first category randomise
instance dimension. includes (i) Bagging (Breiman, 1996a) Wagging (Bauer & Kohavi, 1999), generate different sets training examples random sampling assigning randomly generated probability distributions given training set; (ii) Output flipping
356

fiS PECTRUM



VARIABLE -R ANDOM REES

(Breiman, 2000) classes training examples flipped randomly according
ratio; (iii) Adding random examples (Melville & Mooney, 2003) diverse classifiers
constructed using additional artificial training examples. conventional decision tree algorithm
used generate model random sample training examples. type (i), user
control degree randomisation applied; types (ii) (iii), randomness
expense data integrity.
second category randomise feature dimension randomly selecting subset
features generating model. representative method Random Subspace
(Ho, 1998) 50% features randomly selected produce models ensemble.
Random Subspace designed adjust level randomness, default setting mentioned
commonly used.
third category randomise test-selection decision node tree growing process. Since meant produce variants deterministic model, randomisation
usually applied small degree node maintaining key deterministic characteristic. Examples category Randomised C4.5 (Dietterich, 2000) Random Forests
(Breiman, 2001). reported Breiman, performance Random Forests significantly
impacted different values parameter used.
methods mentioned above, deterministic models common starting point.
Randomisations injected produce diverse variants deterministic models.
contrary, totally different approach start complete-random models, example,
Random Decision Trees (Fan et al., 2003) Max-Diverse Ensemble (Liu et al., 2005).
distinction two starting points inclusion deterministic heuristic.
method uses deterministic weakened heuristic node, starting point
deterministic models. two starting points seem mutually exclusive, however, provide
way connect order maximize possible range randomness and, turn, predictive
performance gain.
paper, show largely unexplored set randomised models found
extremes deterministic complete-random models. Random Forests provides
mean adjust randomness, degrees randomness constrained number features
mandatory use deterministic heuristic node. Details limitation
discussed Section 6.
next section, propose new algorithm constructs trees controllable randomisation test-selection. allows us explore whole spectrum variable-random trees.

3. Trees Variable Randomness
name tree VR-Tree generated using random test-selection nodes.
section, first describe process random test-selection mechanism
induces trees controllable mix random deterministic test-selections.
framework conventional tree building algorithms, random test-selection used
direct replacement deterministic test-selection. depicted Algorithm 1. First, random test-selection randomly picks feature list available features form decision
node. Then, nominal feature possible values form branches continuous-valued
feature random cut-point form 2 branches. random split-point selection procedure
357

fiL IU , ING , U , & Z HOU

described Algorithm 2. random test-selection becomes alternative deterministic
test-selection mechanism create variable-randomness.
Algorithm 1: VR-Tree(Dt , Q, ) - Building Variable-Random Tree
Input: Dt - Training set, Q - Feature set, - probability using deterministic test-selection
Output: node - tree node
classes Dt Q empty |Dt | < nmin
/* nmin
minimum number instances required split
allowed. */
return leaf class frequency
else
let r randomly generated value, 0 < r 1
r
/* Deterministic Test-Selection. */
node DeterministicT estSelection(Dt , Q)
else
/* Random Test-Selection. */
randomly select Q
construct node test label
continuous-valued feature
/* Handling
continuous-valued feature. */
node.splitpoint RandomSplit(, Dt )
D1 f ilter(Dt , > node.splitpoint)
D2 f ilter(Dt , node.splitpoint)
node.branch(1) VR-Tree(D1 , Q, )
node.branch(2) VR-Tree(D2 , Q, )
else
/* Handling discrete feature. */
let {v1 ...vm } possible values

/* m-ary split. */
Di f ilter(Dt , == vi )
node.branch(i) VR-Tree(Di , Q , )
return node

Algorithm 2: RandomSplit(, Dt ) - Random split point selection
Input: - continuous-valued feature, Dt - training data
Output: split point
r1 randomly select value Dt
r2 randomly select value Dt
r1 == r2
r2 randomly select value
return mid point r1 r2
generate variable-randomness, test-selection process split two stages node.
first stage decides test-selection use, either random deterministic test-selection.
second stage proceeds selected test-selection produce actual test node.
parameter provided control probability choosing deterministic test-selection
358

fiS PECTRUM



VARIABLE -R ANDOM REES

random one, 0 1. also approximates percentage deterministic nodes
generated trees. Note setting = 1, procedure generates trees identical
conventional decision trees; setting = 0, generates complete-random trees. procedure
mechanism found Algorithm 1.
next section, introduced three ensemble methods used experiment based
VR-Trees.

4. Ensemble Methods
Using VR-Tree base learner, explore three ensemble methods employed
investigation. listed follows:
Aggregating, trees generated training data using full set
features.
Subspacing, trees generated subsets randomly selected features. parameter used determine percentage features used.
Bagging, trees generated bootstrap sample using full set features.
details ensemble methods shown Algorithms 3, 4 5.
Algorithm 3: Agg.VR-Trees(Dt , Q, N, )
Input: Dt - Training set, Q - Feature set, N - Number trees, - probability using
deterministic test-selection
Output: E - collection trees
N
E E VR-Tree(Dt , Q, )
return E

Algorithm 4: Subspace.VR-Trees(Dt , Q, N, , )
Input: Dt - Training set, Q - Feature set, N - Number trees, - probability using
deterministic test-selection, - percentage features used, 0 < 1
Output: E - collection trees
N
Qs randomly generate set percentage features Q
E E VR-Tree(Dt , Qs , )
return E
none ensemble methods new, incorporation VR-Tree base learner
help unleash potentials methods. predictive performance gain shown Section
5. Note Subspacing equivalent Random Subspace method (Ho, 1998) =50%.
Since =50% provides maximum number distinct subspaces, important factor
increase diversity, use =50% default setting Subspacing. Also, notice
Bag.VR-Trees = 1 equivalent conventional Bagging method (Breiman, 1996a).
359

fiL IU , ING , U , & Z HOU

Algorithm 5: Bag.VR-Trees(Dt , Q, N, )
Input: Dt - Training set, Q - Feature set, N - Number trees, - probability using
deterministic test-selection
Output: E - collection trees
N
Db generate bootstrap sample Dt
E E VR-Tree(Db , Q, )
return E
use probability averaging combine outputs individual models ensemble.
order predict class given test case, predicted class obtained by:
N
X
ni,y
),
arg max(

ni

(1)

i=1

N number trees ensemble, ni,y number class training instances
ni total number training instances leaf tree test case falls into.

5. Empirical Study Spectrum
design experiment four parts. first part investigates predictive performance spectrum Aggregating, Bagging Subspacing using VR-Trees. use result characterize two-halves spectrum. second part examines diversity base learners generated ensembles using strength correlation curves defined Breiman (2001).
part highlights range randomness one achieve using VR-Trees. third part
explores alternative using single value produce models ensemble. alternative combines number points spectrum, proposed ensemble method
paper. fourth part investigates strengths weaknesses proposed method.
Forty-five data sets UCI repository (Asuncion & Newman, 2007) used paper.
characteristics data sets provided Appendix A. Ten-fold cross-validation
conducted data set average error rate reported. 100 trees used
ensemble. Random Forests C5 Boosting (www.rulequest.com) used benchmarks,
addition Bagging, Subspacing Aggregating VR-Trees. use Friedman test
Bonferroni test post-hoc test 95% confidence level compare classifiers (Demsar, 2006).
Random Forests implementation used paper, default settings mtry =
floor(sqrt(q)) nodesize=1 used, mtry number features randomly
sampled split, q number features nodesize minimum size
terminal nodes. implementation taken R (www.r-project.org).
implementation including VR-Trees based C4.5 (Quinlan, 1993). default
C4.5s stop-splitting rules applied: (i) minimum number training samples nmin = 4
required splitting considered, (ii) deterministic test-selection stops splitting
possible splits report negative scores. Gain ratio used test-selection criterion. Probability
averaging implemented curtailment (Zadrozny & Elkan, 2001), minimum leaf size
probability estimation always greater one. default settings used VR-Trees
paper.
360

fiS PECTRUM



VARIABLE -R ANDOM REES

5.1 Predictive Performance Spectrum Aggregating, Bagging Subspacing

Figure 1: spectrum predictive performance Aggregating, Bagging Subspacing,
well Bagging plus Subspacing: error rates , average forty-five data sets.
21
Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees
Bag-Subspace.VR-Trees

Average Error %

20

19

18

17

16

15
0

0.2

0.4

0.6

0.8

1



Figure 1 shows spectrum performance 1 four ensemble methods using VR-Trees
base models. Note conventional Bagging Random Subspace two points
deterministic-end ( = 1) spectrum Bag.VR-Trees Subspace.VR-Trees; MaxDiverse ensemble complete-random-end ( = 0) Aggregating spectrum. Figure 2
shows results Friedman tests Aggregating, Bagging Subspacing
following observations.
Figure 2, interesting note best operating region Agg.VR-Trees
values 0.1 0.6. shows Max-Diverse ensemble operating =
0 improve performance moving middle spectrum. best
operating region Bag.VR-Trees 0.1 0.6, mainly first half
spectrum. significantly different conventional Bagging normally
applied at, namely = 1. best operating region Subspace.VR-Trees 0.4
0.8. also different Random Subspace normally applied = 1.
ii balanced mix random deterministic heuristics, i.e., = 0.5, produces best ensemble
significantly difference best ensemble one three ensemble methods.
iii four curves shown Figure 1, Agg.VR-Trees largest swing performance,
followed Bag.VR-Trees Subspace.VR-Trees. expected two end-points
Agg.VR-Trees curve represents single deterministic model ensemble completerandom models. decreases 1 0.5, substantial improvement predictive performance Agg.VR-Trees, takes effect due increased diversity ensemble,
reduces average error rate 20.6% 16.0%.
iv Although Bag.VR-Trees Agg.VR-Trees perform best region 0 0.5,
result Figure 1 indicates significant difference terms predictive
1. Averaged forty-five data sets.

361

fiL IU , ING , U , & Z HOU

accuracy. additional computational requirement generate bootstrap samples
Bagging, Aggregating becomes preferred method first half spectrum. Bagging
Subspacing preferred Aggregating second half spectrum
latter uncompetitive region.
v use Bagging Subspacing single ensemble recommended, shown
result Figure 1 Bag-Subspace.VR-Trees always performs worse parents,
Bag.VR-Trees Subspace.VR-Trees.

Figure 2: Friedman test results classifiers produced (a) Aggregating, (b) Bagging, (c) Subspacing VR-Trees, eleven values spectrum forty-five data sets. use Bonferroni test post-hoc test = 0 control condition comparison. vertical
axis indicates values, horizontal axis indicates rank values. circle average rank value bar indicates critical values two-tailed test 95%
significance level. two classifiers overlapping bars, indicates significantly different. Significantly worse results presented dotted bars (coloured red) located
right-hand-side diagram. best results presented solid bars (coloured blue)
left-most bar diagram.



0

0

0.1

0.1

0.2

0.2

0.3

0.3

0.4

0.4

0.5

0.5

0.6

0.6

0.7

0.7

0.8

0.8

0.9

0.9

1

1
2

3

4

5

6

7

8

9

10

11

3

(a) Agg.VR-Trees, = 0.4 highest ranking

4

5

0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
4

4.5

5

5.5

7

8

9

(b) Bag.VR-Trees, = 0.6 highest ranking



3.5

6

6

6.5

7

7.5

8

8.5

(c) Subspace.VR-Trees, = 0.5 highest ranking

362

fiS PECTRUM



VARIABLE -R ANDOM REES

summarise characteristics two halves spectrum Table 1. next subsection, continue analysis various points spectrum relation generalisation
error.
Table 1: Characteristics two halves spectrum.
Complete-Random-end, [0, 0.5]
Models extreme end
generated completely random
fashion.
Candidate models possible
trees larger sizes.

Deterministic-end, (0.5, 1]
Models variants deterministic model.
Candidate models models
smaller sizes (because model
reaches pure leaves early).
Maintaining high predictive accuracy providing degree diversity.
Subspacing Bagging preferred region.

Models higher diversity.

Aggregating preferred
region.

5.2 Strength-Correlation Analysis
section, examine strength correlation profiles produced Aggregating, Bagging
Subspacing. Firstly, illustrate relationship generalisation error, strength
correlation using map. Secondly, plot strength-correlation curves actual data sets
characterise behaviours. Thirdly, continue analysis Section 5.1 explore
two halves spectrum light strength-correlation analysis.
Breimans (2001) inequality, P E (1 s2 )/s2 useful discerning relationship generalisation error P E, strength correlation . Briefly, strength ensemble
measures expected margin function ensemble. margin function probability
correct classification minus maximum probability incorrect classification instance. Correlation ensemble measure similar predictions among individual trees.
high reading correlation indicates individual trees making similar predictions. use
strength correlation estimation procedures Buttrey Kobayashi (2003)
corrected version Breiman (2001). make paper self-contained, estimation procedures
given Appendix C.
Figure 3 shows map strength-correlation profiles grey scale indicating generalisation error. see error rate lower high-strength low-correlation corner
(at bottom-right), error rate lower darker grey level. ensemble
classifiers general, goal get region estimated error rate
low possible. Aggregating different values typically spans fashion either
curve (a) curve (b) shown Figure 3, curve starts = 0 (at bottom-left)
= 1 (at top-right). (a), lowest error rate found = 0. (b), lower error
363

fiL IU , ING , U , & Z HOU

rates found larger value . However, error rates also increase approaches
1. case, search would necessary determine optimal .
Figure 3: Ensemble generalisation error distribution using Breimans inequality strength
correlation. Curves (a) (b) represent two typical spans Agg.VR-Trees 0 1.

=1

(a)
=0

(b)

Figure 4 shows three different ensemble methods provide different ranges strength
correlation profile. effective way use VR-Trees Aggregating.
consistently produces longest span strength-correlation curve data sets
used. Bagging usually second longest span, followed Subspacing. Note Aggregating
usually spans strength correlation dimensions; whereas Bagging Subspacing
significantly smaller span, especially correlation dimension. Table 2 shows range
minimum maximum values strength well correlation, averaged forty-five
data sets. result shows Aggregating produces largest range trees comparison
Bagging Subspacing. interestingly, best Aggregating usually located lower
similar error region Bagging Subspacing.
Table 2: Average ranges strength correlation forty-five data sets.
Strength Correlation
Agg.VR-Trees( [0, 1])
0.135
0.696
0.136
0.160
Bag.VR-Trees( [0, 1])
0.069
0.088
Subspace.VR-Trees( [0, 1])
also interesting note =0.5 always close changing corner
strength-varying leg correlation-varying leg examples shown Figure 4. means
= 0.5 either close lowest generalisation error region, ensemble exhibits
364

fiS PECTRUM



VARIABLE -R ANDOM REES

Figure 4: Strength-Correlation distribution Aggregating, Bagging Subspacing different
values . solid-filled marks represent =0.5. Aggregating, first half range
( [0, 0.5]) characterised lower correlation lower strength, located bottomleft corner diagrams. second half ( [0.5, 1]) characterised higher correlation
higher strength, located top-right corner diagrams.
Dataset: coding

Dataset: credit-a

1

1
Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

0.8

correlation

correlation

0.8

Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

0.6

0.4

0.2

0.6

0.4

0.2

0
0.2

0.25

0.3

0.35

0.4

0.45

0
0.45

0.5

0.5

0.55

strength

Dataset: DNA

Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

0.8

correlation

correlation

0.7

1
Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

0.8

0.6

0.4

0.2

0.6

0.4

0.2

0

0
0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0.5

0.55

0.6

0.65

strength

0.7

0.75

0.8

0.85

strength

Dataset: segment

Dataset: sick

1

1
Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

0.8

correlation

0.8

correlation

0.65

Dataset: ionosphere

1

0.6

0.4

0.2

0.6

0.4

0.2

0
0.65

0.7

0.75

0.8

0.85

0.9

0
0.78

0.95

0.8

0.82

0.84

strength

0.86

0.88

0.9

0.92

0.94

0.96

0.7

0.75

strength

Dataset: threeOf9

Dataset: tic-tac-toe

1

1
Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

0.8

correlation

0.8

correlation

0.6

strength

0.6

0.4

0.2

0.6

0.4

0.2

0
0.4

0.5

0.6

0.7
strength

0.8

0.9

0
0.35

1

365

0.4

0.45

0.5

0.55
0.6
strength

0.65

fiL IU , ING , U , & Z HOU

curve (b) like behaviour Figure 3. Thus, = 0.5 often serves upper limit range
values performs well.
Combining result analysis two halves spectrum Section
5.1, find Aggregating exhibits following characteristics data sets Figure
4:
first-half spectrum, strength increases rapidly = 0 slows
= 0.5; however, correlation varies small degree vary all.
second-half spectrum, correlation increases rapidly = 0.5 peaks
= 1. range, strength correlation high, error rates optimal.
summary, better performing Aggregating models found range 0
0.5. single operating points [0, 0.5] shown work well Section 5.1.
next section, show alternative approach achieves even better result using
range [0, 0.5] identified thus far.
5.3 Coalescence Different Operating Points
section, show combining single models multiple points spectrum
achieve similar better performance compared using single point spectrum.
coalesce VR-Trees sampled fixed interval. example, form 100-model ensemble,
construct trees = {0, 0.005, 0.01, ..., 0.495}. call approach Coalescence.
Coalescence approach appealing need know value produces
accurate ensemble given data. introducing members different talents
random-half spectrum; Coalescence forms committee different members.
committee, far know, members good approximating non-axis-parallel boundary
members good avoiding over-fitting. make-up committee helps handle
unforeseeable conditions arise training sample.
result, Coalescence provides comparable predictive performance search-for-the-best approach (Liu & Ting, 2006) without cost searching optimal value. Figure 5,
Coalescence shown better single operating point first-half spectrum
using Friedman test, forty-five data sets.
additional comparison also conducted well known tree ensemble classifiers. complete result Coalescence, Aggregating VR-Trees (=0) (i.e. Max-Diverse Ensemble), Bagging, Subspacing (=1) (i.e. Random Subspace), C5 Boosting Random Forests
presented Table 3. average error rates forty-five data sets provided last row
table. Figure 6 shows result Friedman test following observations:
Coalescence ranks highest among five benchmarking ensembles Friedman test.
second highest ranking ensembles: Random Forests C5 Boosting almost identical
ranking Friedman test, similar average error rates.
third highest ranking ensemble Random Subspace; lowest ranking ensembles
Bagging Aggregating VR-Trees ( = 0). ensemble methods similar average
error rates.
366

fiS PECTRUM



VARIABLE -R ANDOM REES

Figure 5: Friedman test result comparing Coalescence five operating points Agg.VRTrees first-half spectrum. Horizontal axis indicates rank values. Coalescence ranks
top compared different points first-half spectrum.

Coalescence
Agg
( = 0)
( = 0.1)
( = 0.2)
( = 0.3)
( = 0.4)
( = 0.5)
2.5

3

3.5

4

4.5

5

5.5

6

Figure 6: Friedman test result comparing Coalescence five benchmarking methods. Horizontal axis indicates rank values. Notice Coalescence method significantly
better Agg. = 0 (Max-Diverse Ensemble) Bag. = 1 (Bagging).

Coalescence
Agg
( = 0)
Bag
( = 1)
Subspace
( = 1)
C5 Boost
Random
Forests
2

2.5

3

3.5

367

4

4.5

5

fiL IU , ING , U , & Z HOU

Table 3: Experimental results: Average error rate ten-fold cross-validation.
data sets
abalone
anneal
audiology
autos
balance
breastw
breasty
chess
cleveland
coding
credit-a
credit-g
dna
echo
flare
glass
hayes
hepatitis
horse
hypo
ionosphere
iris
labor
led24
led7
liver
lymph
nursery
pima
post
primary
satimage
segment
sick
solar
sonar
soybean
threeOf9
tic-tac-toe
vehicle
vote
waveform21
waveform40
wine
zoo
mean

Coalescence
30.0
1.5
17.2
18.1
14.4
3.0
28.7
0.7
42.9
16.4
12.3
23.1
5.3
33.5
18.6
21.0
18.1
18.0
13.3
0.8
5.7
4.7
7.0
27.9
26.7
27.0
14.9
0.9
23.4
40.0
55.2
8.8
2.1
2.1
29.4
15.9
5.4
0.0
3.0
24.5
4.1
14.5
15.7
3.4
1.0
15.6

Agg.
VR-Trees
(=0)
30.2
1.4
17.7
22.5
12.3
2.4
25.9
1.6
41.6
16.8
13.0
25.7
26.5
34.2
19.2
22.9
21.9
15.5
17.9
1.7
8.5
4.7
3.3
30.3
26.9
27.9
14.3
2.2
24.6
36.7
57.2
10.4
3.1
5.7
30.3
15.9
6.0
0.6
9.7
27.1
5.3
14.7
17.0
1.1
2.0
16.8

Bag.
VR-Trees
(=1)
30.7
3.5
16.2
19.5
18.1
3.4
27.3
2.3
44.2
24.2
12.8
22.7
6.0
29.0
17.7
21.5
17.5
20.0
15.2
0.8
5.4
4.0
10.7
28.1
26.3
27.3
20.4
3.7
24.0
37.8
56.6
9.0
2.4
2.2
27.2
24.0
6.6
1.4
14.3
24.9
4.6
16.1
16.5
3.4
3.0
16.7

368

Subspace.
VR-Trees
(=1)
30.0
3.2
20.3
14.7
11.4
3.0
25.1
1.8
41.9
16.7
13.0
24.8
3.7
32.0
17.1
21.5
17.5
18.6
16.0
1.3
5.4
6.0
12.0
29.1
27.5
31.9
15.6
5.9
23.2
32.2
51.9
8.3
2.3
4.3
27.8
18.7
5.3
10.0
22.3
25.4
4.6
14.7
15.3
2.3
4.0
16.4

C5
Boost
31.1
5.0
15.0
15.6
18.9
3.1
26.9
0.3
41.6
15.4
14.3
22.4
4.8
37.4
17.5
21.4
16.9
14.1
22.5
0.8
5.4
4.0
15.7
28.1
27.8
29.6
19.1
0.9
25.0
30.0
56.9
8.1
1.8
2.2
25.7
15.9
6.2
0.0
1.2
23.3
4.8
15.6
15.1
5.6
3.0
15.9

Random
Forests
30.9
9.7
20.8
15.2
16.3
3.4
29.7
1.1
41.9
17.5
13.0
23.2
3.4
32.0
18.5
21.0
16.9
17.3
14.1
1.0
6.5
3.3
5.0
28.5
26.3
26.1
16.9
2.3
23.2
32.2
56.9
8.1
2.1
2.1
27.2
13.9
5.7
0.8
1.3
26.1
4.1
14.7
14.9
2.3
5.0
15.6

fiS PECTRUM



VARIABLE -R ANDOM REES

ensembles, Coalescence shown significantly better Aggregating
VR-Trees ( = 0) Bagging Friedman test.
interesting note employing oracle find best , optimal error rate
14.6% 2 (Liu & Ting, 2006). Thus, Coalescence approach comes close optimal result
without using oracle.
Figure 7 shows predictive performance Coalescence relation Aggregating eight
data sets. Coalescence sometimes outperforms parents ( [0, 0.5]), often comes close
best performing Aggregating.
5.4 Strengths Weaknesses
section examine strengths weaknesses VR-Tree. Although Coalescence
significantly better Random Forests Friedman test, Table 3 clearly shows Coalescence better Random Forests data sets worse others. section also
examine conditions Coalescence performs better Random Forests
vice versa.
find ensembles complete-random-trees following strengths: 1) capable
approximating non-axis-parallel boundary, 2) highly stable learners terms
Pointwise Hypothesis Stability (Bousquet & Elisseeff, 2002). analysis based Pointwise
Hypothesis Stability found Appendix B. verify complete-random-trees ability
approximate non-axis-parallel boundary, Figure 8, provide visualization example using
Gaussian mixture data set (Hastie, Tibshirani, & Friedman, 2001). Figure 8 shows ensemble complete-random-trees (using 100 trees) able better approximate non-axis-parallel
boundary compared single deterministic tree.
Moreover, described analysis Appendix B, one strengths complete-random
trees also one weaknesses: complete-random trees trend over-fit; problem
stems mainly ability approximate non-axis-parallel boundary. also find
overfitting problem aggravated irrelevant attributes, class noise small training size
shown following empirical examination.
denote X input space output space. learning algorithm outputs
function approximates underlying true function f inspecting training set. training
set = {zi }m
i=1 , zi = (xi , yi ) yi = f (xi ), contains i.i.d. examples Z = X
drawn uniform distribution instance space. define input space two
variables, xi = hi,a , i,b output space two possible classes = {+1, 1}.
case, instance space square point (a = 1, b = 1) point (a = 1, b = 1).
define two concepts follows:
(
+1 (i,a > i,b )
, xi = hi,a , i,b
Concept A: f (xi ) = yi =
1 else
(
+1 (i,a > 0)
Concept B: f B (xi ) = yi =
, xi = hi,a , i,b
1 else
2. Averaged forty-five data sets used paper.

369

fiL IU , ING , U , & Z HOU

Figure 7: Detail results comparing Coalescence Aggregating eight data sets.
Dataset: abalone
32.5
Agg.VR-Trees
Coalescence

Dataset: balance
24

32

Agg.VR-Trees
Coalescence

Error %

20

31

18
16

30.5
14

30
0

0.2

0.4

0.6

0.8

12

1

0

0.2

0.4

0.6



0.8

1



Dataset: DNA

Dataset: ionosphere

30

9
Agg.VR-Trees
Coalescence

Agg.VR-Trees
Coalescence

8.5

25

8
7.5
Error %

Error %

20
15

7
6.5

10

6
5

5.5

0

5
0

0.2

0.4

0.6

0.8

1

0

0.2

0.4



Dataset: nursery

0.8

1

0.8

1

0.8

1

Dataset: segment
10

Agg.VR-Trees
Coalescence

Agg.VR-Trees
Coalescence

8

8

6

6

Error %

Error %

0.6


10

4

2

4

2

0

0
0

0.2

0.4

0.6

0.8

1

0

0.2

0.4



0.6


Dataset: solar

Dataset: waveform 40
28

Agg.CR-Tree
Coalescence

34

Agg.VR-Trees
Coalescence

26
24
Error %

32
Error %

Error %

22

31.5

30

28

22
20
18
16

26

14
0

0.2

0.4

0.6

0.8

1

0



0.2

0.4

0.6


370

fiS PECTRUM



VARIABLE -R ANDOM REES

Figure 8: Visualisation non-axis-parallel decision boundary Gaussian mixture data using
ensemble complete-random-trees compared single deterministic tree (C4.5).

Error 11.3%

Error 14.2%

(a) Complete-random trees

(b) Single deterministic tree

positive class
negative class

(c) Training sample

(d) Actual decision boundary

Concept useful illustrating ability approximate non-axis-parallel boundary. Concept
B axis-parallel concept, used control condition experiment. conduct
four experiments using (a) training sample k = 1024 instances, (b) training sample
irrelevant attributes, (c) training sample class noise (d) size-reduced training sample
k = 64. train ensemble models using Coalescence, VR-Trees ( = 0, = 0.5) Random
Forests. use 100 trees ensemble. Finally, evaluate models 10000 lattice
samples instance space, average error rate 10 runs reported.
Table 4 shows that:
VR-Trees ( = 0) best performer approximating non-axis-parallel boundary concept
A, even small training size k = 64. However, worst performer axis-parallel
concept B k = 1024 k = 64.
VR-Trees ( = 0.5) performs best class noise irrelevant attribute conditions
concepts B. irrelevant attributes added, error rate VR-Trees ( =
0) increases faster rate compare VR-Trees ( = 0.5). Similarly, class noise affects
complete-random-end ( = 0) spectrum severely middle point
spectrum ( = 0.5).
371

fiL IU , ING , U , & Z HOU

Table 4: Averaged error rates VR-Trees ( = 0, = 0.5), Coalescence Random Forests
Concepts B. default number training samples k = 1024, unless otherwise specified.
best error rate row bold faced.
VR-Trees
VR-Trees
( = 0) Coalescence ( = 0.5) Random Forests

1.4%
1.6%
2.0%
1.9%
A, 8 irr. att.
7.6%
3.4%
3.0%
3.2%
A, 40% class noise
30%
21.9%
14.7%
33.4%
A, k = 64
6.7%
8.9%
10.4%
7.3%
B
0.3%
0%
0%
0%
B, 8 irr. att.
5.2%
0%
0%
0%
B, 40% class noise
30.7%
4.2%
2.6%
32.5%
B, k = 64
2.4%
1.1%
1.1%
0.8%
Coalescence Random Forests, Coalescence performs better class noise condition; Random Forests better performer small training size concepts
B.
empirical results show that, complete-random trees, i.e. VR-Trees ( = 0)
good approximating non-axis-parallel boundary, easily over-fitted irrelevant attributes,
class noise small training sample. Although case, find Coalescence way
manage propensities without search specific value. Table 4, find
Coalescence tends performance closer better performing learner either VR-Trees
( = 0) VR-Trees ( = 0.5); exception learning non-axis-parallel concept
small training sample.
understanding irrelevant attributes affect Coalescence, perform
simple check eighteen data sets Random Forests performs better (ignoring two
artificial data sets: led waveform already known results without
irrelevant attributes).
remove less-important half attributes according Random Forests variableimportance (Breiman, 2001). evaluate eighteen data sets using Coalescence
Random Forests 10-fold cross validation. result Table 5 shows Coalescence performs better ten eighteen data sets; Random Forests performs better
four data sets. result indicates influence irrelevant attributes Coalescence
greater Random Forests. Among eighteen data sets, Coalescences error rates
reduced half labor, hayes tic-tac-toe data sets. result indicates
management irrelevant attributes indeed improve performance Coalescence.

6. Related Work
section, first highlight differences VR-Trees Random Forests
capability vary degree randomness, distinguish VR-Trees others. Then, order
better position VR-Trees, discuss various different decision tree ensembles related
VR-Trees.
372

fiS PECTRUM



VARIABLE -R ANDOM REES

Table 5: Evaluation Coalescence Random Forests respect condition irrelevant attribute: eighteen data sets listed less-important half attributes removed.
Boldfaced indicates improvement error rate using reduced number attributes
Coalescence
Random Forests
data sets Full att. Half att. Full att. Half att.
autos
18.1
13.6
15.2
14.6
cleveland
42.9
45.2
41.9
43.9
dna
5.3
3.9
3.4
3.2
echo
33.5
39.0
32.0
42.8
flare
18.6
19.0
18.5
18.9
hayes
40.6
18.1
41.3
16.9
hepatitis
18.0
16.7
17.3
17.4
iris
4.7
4.0
3.3
3.3
labor
7.0
3.3
5.0
5.0
liver
27.0
33.6
26.1
33.1
pima
23.4
24.5
23.2
24.5
post
40.0
37.8
32.2
36.7
satimage
8.8
9.4
8.1
9.3
solar
29.4
30.3
27.2
28.7
sonar
15.9
13.0
13.9
16.4
tic-tac-toe
18.8
3.0
18.3
1.3
vote
4.1
4.4
4.1
4.4
wine
3.4
1.7
2.3
2.8

6.1 Relationship Random Forests
interesting note Breiman (2001) found Random Forests accuracy overly
sensitive value F , parameter intended vary degree randomness.
F parameter corresponds parameter VR-Tree. Yet, experiments section 5.1 clearly
shows varying degree randomness (using ) significant impact predictive
performance resulting ensembles. thus important identify differences
two ensembles cause different behaviours. following paragraphs.
Algorithm 6: random feature selection framework Random Forests
Input: Dt - Training set, F - number features
Output: node: tree node
randomly select F features available features
0 = subset Dt according F features
node = DeterministicTestSelection(D 0 )
return node
surface, Random Forests similar Agg.VR-Trees use
deterministic random test-selections tree induction process. However, differ
way test-selection applied decision node. randomisation framework
Random Forests described Algorithm 6.
373

fiL IU , ING , U , & Z HOU

applying test-selection, Random Forests applies random feature selection deterministic test-selection node; however VR-Tree applies either random deterministic
test-selection node. controls probability whether deterministic random testselection applied node; whereas mixed application two selection processes
node constrains amount randomness introduced Random Forests.
case Random Forests, F controls number features randomly selected. selected, deterministic test-selection chooses best feature. Thus, best feature readily
selected first place, matter F is, best feature always chosen
deterministic test-selection. agrees Breimans observation error rate
overly sensitive different values F Random Forests.
accessibility different degrees randomness directly affects diversity models
produced, Figure 9 shows strength-correlation curves Random Forests (using
available F values, 19 segment data 21 waveform21 data), comparison Agg.VRTrees using eleven values sampled equal interval. find Random Forests produces
ensembles highly correlated many similar strength.
result also reported Breiman (2001). Note fitted curves Random Forests
visual aids mean represent accessibility points. contrast, Agg.VR-Trees
produces ensembles accessible along curve spread along wider range.
nutshell, randomisation framework used Random Forests significantly limits ability scale different levels randomness total number features small.
hand, VR-Trees able scale different levels randomness regardless number
features.
6.2 Related Work
approach search best value proposed (Liu & Ting, 2006). approach searches
optimal value based average progressive training errors. estimated optimal
b task
generated follows:
N
1 X
err(, i, Dt )]
(2)

b = arg min[
00.5 N
i=1

N total number trees ensemble, err() returns training error rate
ensemble size i, Agg.VR-Trees set training set Dt . obtaining
b, i.e.,
best performing , ensemble employs model
b actual predictive tasks. Note
unpruned tree ensemble stops growing number training examples node
four less (the default setting C4.5.) avoids generating zero training error trees. Though
method comparable prediction performance Coalescence approach, requires
substantial computational overhead ensembles values search must
produced.
contrary common belief, Fan et al (2003) first propose use complete-random trees
produce accurate predictive models. Fan (2004) explains reason combination
complete-random trees probability averaging produces accurate models. Using completerandom trees, Liu et al. (2005) show ensembles perform comparably Bagging
Random Forests.
Cutler Zhao (2001) propose PERT (Perfect Random Tree Ensembles) randomises
test-selection continuous-valued features achieve higher randomisation. potential
374

fiS PECTRUM



VARIABLE -R ANDOM REES

Figure 9: Strength-Correlation plots Random Forests Aggregating VR-Trees different
F values. Aggregating VR-Trees wider range correlation compared Random
Forests.
Dataset: segment
1
Agg.VR-Trees
Random Forests

0.9
0.8
correlation

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.55

0.6

0.65

0.7

0.75
0.8
strength

0.85

0.9

0.95

Dataset: waveform 21
1
Agg.VR-Trees
Random Forests

0.9
0.8
correlation

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.34

0.36

0.38

0.4

0.42 0.44
strength

375

0.46

0.48

0.5

0.52

fiL IU , ING , U , & Z HOU

split, PERT first randomly selects two examples different classes local training set.
feature selected randomly. cut point feature randomly selected values
two samples. leaf formed two examples different classes cannot found ten
trials. PERT also shown competitive Bagging Random Forests. believe
method likely close complete-random-end spectrum. However, unclear
different degrees randomisation introduced PERT framework.
Extra-Trees (Geurts, Ernst, & Wehenkel, 2006) relies framework Algorithm
6. However, random split-selection used instead deterministic split-selection. Different
PERT, Extra-Trees cut points randomly selected maximum minimum
values given samples. compared PERT, Extra-Trees requires additional data-scan
every node tree find maximum minimum values, disadvantage
terms computational complexity. categorical features, random subset split used
Extra-Trees. method also shown competitive Random Forests.
Robnik-Sikonja (2004) reports improved version Random Forests using five different
deterministic test-selection criteria instead one. achieves effect increased diversity
producing different variants deterministic models starting point Random Forests,
Bagging Random Subspace.
MultiBoosting (Webb, 2000; Webb & Zheng, 2004) another approach combines
one type ensembles. MultiBoosting tightly-coupled method incorporates bagging
(random sample weight assignment) main Boosting procedure (the incremental sample
weight modifier) order increase model diversity. Like Bagging Random Forests,
focusing increasing diversity deterministic-end spectrum.
many algorithms reported literature, listed paper, suggesting different ways combine models generated one algorithm different algorithms,
(e.g., see Breiman, 1996b; Ting & Witten, 1999; Perrone & Cooper, 1993) . require
kind learning estimation order either selectively choose available models
build meta-model combine them. Coalescence approach simple
approaches require learn meta-model or/and kind estimation.

7. Conclusions
paper, make following contributions:
Propose new algorithm, generates spectrum VR-Trees span completerandom trees deterministic trees. show different points spectrum
significantly different terms predictive accuracy. opens new opportunities
decision tree ensembles.
Show existing ensemble methods Bagging, Random Subspace, Max-Diverse
Ensemble either end spectrum. performance ensembles
improved moving towards middle spectrum improvements
significant.
Discover two halves spectrum distinctive characteristics, separated
critical point =0.5. point two interesting characteristics. First, produces
equal percentage random deterministic decision nodes VR-Trees. Second, often
376

fiS PECTRUM



VARIABLE -R ANDOM REES

lies lowest generalisation error region (or close it) typical strength-correlation
curve. Ensembles generated [0.0, 0.5] often out-perform generated
[0.5, 1.0].
Propose new approach building better performing ensembles. Coalescence approach
coalesces number points first-half spectrum. show ranks better
single operating point whole spectrum.
Identify key differences ensembles constructed frameworks Random
Forests VR-Tree explain predictive accuracy sensitive parameter
VR-Tree framework, Random Forests framework.
empirical evaluation, Coalescence compared five benchmarking ensemble methods: Max-Diverse Ensemble, Bagging, Random Forests, Random Subspace C5 Boosting.
study reveals Coalescence ranks top ensemble method significantly
better Bagging Max-Diverse Ensemble using Friedman test.
Although Coalescence significantly better Random Forests, identified that:
Random Forests performs better Coalescence conditions irrelevant attribute
small training size, Coalescence performs better learning non-axis-parallel concepts
class noise.

Acknowledgments
Y. Yu Z.-H. Zhou supported National Science Foundation China Grant
Nos. 60635030 60721002. Part research conducted K. M. Ting visiting
LAMDA group, Nanjing University.

377

fiL IU , ING , U , & Z HOU

Appendix A. Data characteristics data sets

Table 6: Data characteristics forty-five data sets used experiments. Data taken
UCI repository (Asuncion & Newman, 2007).
data sets
size
#att. #class description
abalone
4177
1n, 7c
2 Abalone growth
anneal
898 13n, 6c, 19b
6 Steel annealing
audiology
226
8n, 61b
23 Standardised Audiology Database
auto
205
6n, 15c, 4b
7 1985 Auto Imports Database
balance
625
4c
3 Balance Scale weight Distance Database
breast-w
699
10c
2 Winconsin breast cancer database
breast-y
286
6n, 3b
2 Ljubljana Breast cancer database
chess
3196
35n, 1b
2 Chess end games
cleveland
303
4n, 6c, 3b
5 Cleveland heart disease database
coding 20000
15n
2 Coding database
credit-a
690
4n, 6c, 4b
2 Australian Credit database
credit-g
1000
12c, 12b
2 German credit database
dna
3186
60n
3 Primate splice-junction gene sequences
echo
133
6c, 1b
2 Echocardiogram data
flare
1066
3n, 2c, 5b
2 Predicting solar flare
glass
214
9c
7 Glass identification database
hayes
160
4c
3 Hayes-Roth & Hayes-Roth database
hepatitis
155
6c, 13b
2 Hepatitis Domain
horse
368
13n, 7c, 2b
2 Horse colic database
hypo
3163
7c, 18b
2 Thyroid disease database
ionosphere
351
34c
2 Radar returns ionosphere
iris
150
4c
3 Iris plants database
labor
57
5n, 8c, 3b
2 Final settlements labour negotiations
led24
3200
24b
10 LED display + 17 irrelevant attributes
led7
3200
7b
10 LED display irrelevant attribute
liver
345
6c
2 BUPA liver disorder
lymph
148
6n, 3c, 9b
4 Lymphography domain
nursery 12960
8n
5 Nursery database
pima
768
8c
2 Diabetes female Pima Indians
post
90
7n, 1c
3 Postoperative patient data
primary
339
3n, 14b
22 Primary tumor domain
satimage
6435
36c
7 Satellite image data set NASA
segment
2310
19c
7 Image segmentation data
sick
3163
7c, 18b
2 Sick-euthyroid data
solar
323
3n, 3c, 6b
6 Solar data set
sonar
208
60c
2 Classification sonar signals
soybean
683
19n, 16b
19 Soy bean disease diagnosis
threeOf9
512
9b
2 concept three nine
tic-tac-toe
958
9n
2 Tic-Tac-Toe board configurations
vehicle
846
18c
4 Vehicle silhouette data set
vote
435
16n
2 Votes U.S. Congressmen
waveform21
5000
21c
3 Waveform data
waveform40
5000
40c
3 Waveform data 19 noise attributes
wine
178
13c
3 Wine recognition data
zoo
101
1n, 15b
7 Zoo database
Attribute type indicated n: nominal, c: continuous, b: binary.

378

fiS PECTRUM



VARIABLE -R ANDOM REES

Appendix B. Theoretical Analysis VR-Trees ( = 0)
notations similar Bousquet Elisseeff (2002). Denote X input
space output space. learning algorithm function : Z k F, F X
function space. denote f function F. learning algorithm outputs function
approximates underlying true function f inspecting training set. training set
= {zi }ki=1 , zi = (xi , yi ) yi = f (xi ), contains k i.i.d. examples Z = X
drawn unknown distribution D. consider case X bounded Real space
= {1, +1}. Denote training set removing i-th example
\i = {z1 , . . . , zi1 , zi+1 , . . . , zk }
Given learning algorithm trained S, denote generalisation error
R(T, S) = Ez=(x,y)[`(fT,S (x), y)] ,
empirical error

1 Xk
`(fT,S (xi ), yi ) ,
i=1
k
= (S) ` : R loss function.
(T, S) =

fT,S

study generalisation ability viewpoint Pointwise Hypothesis Stability
(Bousquet & Elisseeff, 2002).
Definition 1 (Pointwise Hypothesis Stability)
algorithm pointwise hypothesis
stability respect loss function ` holds {1, . . . , k}
ESD [|`(fT,S (xi ), yi ) `(fT,S \i (xi ), yi )|]
Theorem 11 Bousquet Elisseeff (2002) reveals relationship Pointwise Hypothesis Stability generalisation error. self-contained, write theorem
Lemma 1.
Lemma 1 learning algorithm pointwise hypothesis stability respect loss
function ` , 0 `(, ) , probability 1 ,
r
2 + 12M k
R(T, S) (T, S) +
2k
Assume (i) one attribute X attribute Real values, (ii) every
training example unique attribute values, (iii) internal node non-empty subsets, (iv)
tree building process stops nodes contain one example, (v) output
VR-Tree +1 1 input instance case binary classification. building
ensemble VR-Trees, run VR-Tree algorithm N times produce set trees {fi }N
i=1 .
Given test instance z = (x, y), output ensemble is:
f N (x) =

1 XN
fi (x)
i=1
N
379

fiL IU , ING , U , & Z HOU

whose range [1, +1]. following analysis, let N approach infinity.
order study Pointwise Hypothesis Stability VR-Trees ( = 0) ensembles, need
bound
ESD [|`(fT,S (xi ), yi ) `(fT,S \i (xi ), yi )|].
specify loss function


1,
`(y1 , y2 ) = 0.5,


0,

|y1 y2 | > 1
|y1 y2 | = 1 .
|y1 y2 | < 1

Let denote number class transitions, example pairs (zj , zk )
yj 6= yk example xj xk . used bound later.
Now, training instance z 0 = (x0 , 0 ) held test example
leaving \i , want know VR-Tree ensemble makes predictions z 0 . find
one following four events happen classifying z 0 , illustrated Figure 10.

Figure 10: Illustration four possible places test instance. Circles +1, 1 ? denote
positive, negative test instances, respectively.
?
(a )
+1
(b )

+1

(c)
(d )

?
+1

-1

+1
?

?

+1

a) z 0 duplicate training example zi = (xi , yi ), illustrated Figure 10(a).
f n (x0 ) = yi , every tree leaf pure, means empirical error always zero. Since
assume every instance unique location, ignore case .
b) z 0 located two training examples zi = (xi , yi ) zj = (xj , yj ) yi = yj ,
illustrated Figure 10(b). case, z 0 fall leaf node containing zi zj . Therefore,
f n (x0 ) = yi = yj . z 0 wrongly classified, is, z 0 different label yi
yj , two counts added .
c) z 0 located outside training set zi = (xi , yi ) nearest training example,
illustrated Figure 10(c). case z 0 fall leaf node must contain zi , thus
f n (x0 ) = yi . z 0 wrongly classified, one count added .
d) z 0 located two training examples zi = (xi , yi ) zj = (xj , yj ) yi 6= yj .
z 0 wrongly classified, is, z 0 different label either yi yj , one count added .

380

fiS PECTRUM



VARIABLE -R ANDOM REES

Therefore, given example zi = (xi , yi ) S, `(fT,S (xi ), yi ) = 0, yi
included S, case Figure 10(a). analysis, also number
errors upper-bounded .
= ESD [|`(fT,S (xi ), yi ) `(fT,S \i (xi ), yi )|]
= ESD [`(fT,S \i (xi ), yi )]
ESD [S ]/k .
Note varies training sets, upper bounded class transitions
underlying true function f , is, let
fi
fi
= fi{x X | > 0 x0 : f (x)f (x0 ) < 0 kx x0 k < }fi ,

constant.

: ,

= large enough. Therefore,
/k .
Since constant, ensemble VR-Trees ( = 0) stable learner, whose generalisation
error bounded, probability 1 ,
r
1 + 12
.
R(T, S)
2k
observed features large weaken predictive performance.
Moreover, find irrelevant attributes, class noise insufficient training sample
cause large . irrelevant attributes, perform almost random projection X Y.
f random projection, i.e. P (f (x) = +1) = 0.5, probability class
transitions training set
P (S ) (1/2)|S|/S 1
considering transitions divide equal segments. implies irrelevant
attributes weaken predictive performance VR-Trees ( = 0). class noise insufficiency
training samples, hard see also increase hence also weaken
performance VR-Trees ( = 0).

381

fiL IU , ING , U , & Z HOU

Appendix C. Estimation Strength Correlation
make paper self contained, provide estimation strength correlation
defined Breiman (2001) corrected Kobayashi (2002). Given ensemble N trees
{f1 , ..., fN }, hold-out set Dh = {(x1 , y1 ), ..., (xk , yk )} k number test cases,
estimate followings:
Strength - estimated average margin Dh .
Correlation - estimated taking variance margin standard deviation
random vectors, represent trees.
C.1 Estimation Strength
estimation Strength given by:
k
1X
si
k

(3)

N
N
1 X
1 X
I(fj (xi ) = yi )
I(fj (xi ) = c(i))
N
N

(4)

=

i=1

margin si given by:
si =

j=1

j=1

test case i, c(i) class label receives maximum votes among N trees c(i)
class label true label yi . I(.) indicator function returns 1
true, 0 otherwise.
C.2 Estimation Correlation
estimation Correlation given by:
=

var(s)
N
1 P
sd(j)
N

(5)

j=1

variance margin var(s) given by:

var(s) =

k
1X 2
si s2
k

(6)

i=1

standard deviation random vector sd(j) given by:
sd(j) = [p1 + p2 (p1 p2 )2 ]1/2

(7)

k
1X
p1 =
I(fj (xi ) = yi )
k

(8)

i=1
k

1X
I(fj (xi ) = c(i))
p2 =
k
i=1

382

(9)

fiS PECTRUM



VARIABLE -R ANDOM REES

References
Asuncion, A., & Newman, D. J. (2007).
UCI
http://www.ics.uci.edu/mlearn/MLRepository.html.

machine

learning

repository..

Bauer, E., & Kohavi, R. (1999). empirical comparison voting classification algorithms: Bagging, boosting, variants. Machine Learning, 36(1-2), 105139.
Bousquet, O., & Elisseeff, A. (2002). Stability generalization. Journal Machine Learning
Research, 2, 499526.
Breiman, L. (1996a). Bagging predictors. Machine Learning, 24(2), 123140.
Breiman, L. (1996b). Stacked regressions. Machine Learning, 24(1), 4964.
Breiman, L. (2000). Randomizing outputs increase prediction accuracy. Machine Learning,
40(3), 229242.
Breiman, L. (2001). Random forests. Machine Learning, 45(1), 532.
Buttrey, S. E., & Kobayashi, I. (2003). strength correlation random forests. Proceedings
2003 Joint Statistical Meetings, Section Statistical Computing, San Francisco, CA.
American Statistical Association.
Cutler, A., & Zhao, G. (2001). PERT - perfect random tree ensembles. Computing Science
Statistics, Vol. 33, pp. 490497, Costa Mesa, Orange Country, California.
Demsar, J. (2006). Statistical comparisons classifiers multiple data sets. Journal Machine
Learning Research, 7, 130.
Dietterich, T. G. (2000). experimental comparison three methods constructing ensembles
decision trees: Bagging, boosting, randomization. Machine Learning, 40(2), 139157.
Fan, W. (2004). optimality probability estimation random decision trees. Proceedings Nineteenth National Conference Artificial Intelligence, Sixteenth Conference
Innovative Applications Artificial Intelligence (AAAI), pp. 336341, California, USA.
AAAI Press / MIT Press.
Fan, W., Wang, H., Yu, P. S., & Ma, S. (2003). random model better? accuracy efficiency. ICDM 03: Proceedings Third IEEE International Conferenceon Data Mining,
5158.
Geurts, P., Ernst, D., & Wehenkel, L. (2006). Extremely randomized trees. Machine Learning,
63(1), 342.
Hastie, T., Tibshirani, R., & Friedman, J. (2001). elements statistical learning : Data mining,
Inference, Prediction. Springer-Verlag.
Ho, T. K. (1998). random subspace method constructing decision forests. IEEE Transactions
Pattern Analysis Machine Intelligence, 20(8), 832844.
Kobayashi, I. (2002). Randomized Ensemble Methods Classification Trees. Ph.D. thesis, Naval
Postgraduate School, Monterey, CA.
Liu, F. T., & Ting, K. M. (2006). Variable randomness decision tree ensembles. Advances
Knowledge Discovery Data Mining, 10th Pacific-Asia Conference (PAKDD 2006), pp.
8190, Singapore.
383

fiL IU , ING , U , & Z HOU

Liu, F. T., Ting, K. M., & Fan, W. (2005). Maximizing tree diversity building complete-random
decision trees. Advances Knowledge Discovery Data Mining, 9th Pacific-Asia Conference (PAKDD 2005), pp. 605610, Hanoi, Vietnam.
Melville, P., & Mooney, R. (2003). Constructing diverse classifier ensembles using artificial training examples. Proceedings Eighteenth International Joint Conference Artificial
Intelligence, pp. 505510, Mexico.
Perrone, M. P., & Cooper, L. N. (1993). networks disagree: ensemble methods hybrid
neural networks. Artificial Neural Networks Speech Vision, 126142.
Quinlan, R. J. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann, San Mateo,
Calif.
Robnik-Sikonja, M. (2004). Improving random forests.. Boulicaut, J.-F., Esposito, F., Giannotti,
F., & Pedreschi, D. (Eds.), Proceedings 15th European Conference Machine Learning (ECML 2004), Vol. 3201 Lecture Notes Computer Science, pp. 359370, Pisa, Italy.
Springer.
Ting, K. M., & Witten, I. H. (1999). Issues stacked generalization. Journal Artifical Intelligence
Research (JAIR), 10, 271289.
Webb, G. I. (2000). Multiboosting: technique combining boosting wagging. Machince
Learning, 40(2), 159196.
Webb, G. I., & Zheng, Z. (2004). Multistrategy ensemble learning: Reducing error combining
ensemble learning techniques. IEEE Transactions Knowledge Data Engineering,
16(8), 980991.
Zadrozny, B., & Elkan, C. (2001). Obtaining calibrated probability estimates decision trees
naive bayesian classifiers. ICML 01: Proceedings Eighteenth International Conference
Machine Learning, 609616.

384

fiJournal Artificial Intelligence Research 32 (2008) 123167

Submitted 09/07; published 05/08

Constraint Programming Approach Solving
Queueing Control Problem
Daria Terekhov
J. Christopher Beck

dterekho@mie.utoronto.ca
jcb@mie.utoronto.ca

Department Mechanical & Industrial Engineering
University Toronto, Canada

Abstract
facility front room back room operations, useful switch workers
rooms order cope changing customer demand. Assuming stochastic
customer arrival service times, seek policy switching workers
expected customer waiting time minimized expected back room staffing
sufficient perform work. Three novel constraint programming models several
shaving procedures models presented. Experimental results show model
based closed-form expressions together combination shaving procedures
efficient. model able find prove optimal solutions many problem
instances within reasonable run-time. Previously, available approach
heuristic algorithm. Furthermore, hybrid method combining heuristic best
constraint programming method shown perform well heuristic terms
solution quality time, achieving performance terms proving
optimality pure constraint programming model. first work
aware solves queueing-based problems constraint programming.

1. Introduction
original motivation study scheduling resource allocation problems within
artificial intelligence (AI) constraint programming (CP) that, contrast Operations Research (OR), full richness problem domain could represented
reasoned using techniques knowledge representation (Fox, 1983). much
success constraint-based scheduling due algorithmic advances (Baptiste,
Le Pape, & Nuijten, 2001), recently, interest complex problems
involving uncertainty (Policella, Smith, Cesta, & Oddi, 2004; Sutton, Howe,
& Whitley, 2007; Beck & Wilson, 2007). broader constraint programming community significant work past five years reasoning uncertainty
(Brown & Miguel, 2006). Nonetheless, recognized constraint solving change
uncertainty infancy (Brown & Miguel, 2006, p. 754).
Queueing theory intensively studied design control systems resource
allocation uncertainty (Gross & Harris, 1998). Although much study
descriptive sense developing mathematical models queues, prescriptive
work attempts develop queue designs control policies optimize quantities
interest (e.g., customers expected waiting time) (Tadj & Choudhury, 2005). One
challenges queueing theory, however, analytical models yet extend
c
2008
AI Access Foundation. rights reserved.

fiTerekhov & Beck

richer characteristics encountered real-world problems rostering call centres
(Cezik & LEcuyer, 2008).
long-term goal integrate constraint programming queueing theory two
ends mind: extension constraint programming reason better uncertainty
expansion richness problems queueing theory brought
bear. achieve goal here. Rather, paper represents first step
solve queueing control problem constraint programming techniques. Specifically,
develop constraint programming approach queueing control problem arises
retail facilities, stores banks, back room front room operations.
front room, workers serve arriving customers, customers form queue
wait served workers busy. back room, work directly
depend customer arrivals may include tasks sorting processing paperwork.
workers facility cross-trained assumed able perform back
room tasks equally well serve customers service rate. Therefore, makes
sense managers facility switch workers front room
back room depending number customers front room amount
work performed back room. managers thus interested
finding switching policy minimizes expected customer waiting time front
room, subject constraint expected number workers back room
sufficient complete required work. queueing control problem studied
detail Berman, Wang Sapna (2005), propose heuristic solving it.
contributions twofold. Firstly, constraint programming is, first time,
used solve stochastic queueing control problem. Secondly, complete approach
problem heuristic algorithm existed previously presented.
paper organized follows. Section 2 presents description problem
work done Berman et al. (2005). next section, three CP models
problem proposed. Sections 4 5 present methods improving efficiency
models, focusing dominance rules shaving procedures, respectively. Section 6 shows
experimental results comparing proposed CP models combinations inference
methods. performance CP techniques contrasted heuristic
method Berman et al. Based results, hybrid method proposed evaluated
Section 7. Section 8, discussion results presented. Section 9 describes related
problems states directions future work. Section 10 concludes paper.
appendix containing derivations expressions used paper also included.

2. Problem Description
Let N denote number workers facility, let maximum number
customers allowed front room one time.1 customers present,
arriving customers allowed join front room queue leave without
service. Customers arrive according Poisson process rate . Service times
front room follow exponential distribution rate . minimum expected number
workers required present back room order complete
necessary work assumed known, denoted Bl , l stands lower
1. notation used Berman et al. (2005) adopted throughout paper.

124

fiA CP Approach Queueing Control Problem

bound. one worker allowed switched time, switching time
switching cost assumed negligible. goal problem find optimal
approach switching workers front room back room minimize
expected customer waiting time, denoted Wq , time ensuring
expected number workers back room least Bl . Thus, policy needs
constructed specifies many workers front room back room
particular time switches occur.
2.1 Policy Definition
term policy used queueing control literature describe rule prescribes,
given particular queue state, actions taken order control queue.
research optimal control queues focused determining
particular type policy optimal, rather finding actual optimal values
parameters policy (Gross & Harris, 1998). term optimal policy used
literature mean optimal type policy optimal parameter values
given policy type. distinction two important since showing
particular policy type optimal theoretical question, whereas finding optimal
values specific policy type computational one.
policy type adopted one proposed Berman et al. (2005). policy
defined terms quantities ki , = 0, . . . , N states workers
front room whenever ki1 + 1 ki customers (inclusive)
front room, = 1, 2, . . . , N . consequence interpretation, following
constraints hold: ki ki1 1, k0 0 kN = S. example, consider facility
= 6 N = 3, suppose policy (k0 , k1 , k2 , k3 ) = (0, 2, 3, 6) employed.
policy states k0 + 1 = 1 k1 = 2 customers front room,
one worker front room; 3 customers, 2 workers;
4, 5, 6 customers, 3 workers employed front. Alternatively,
ki interpreted upper bound number customers served
workers given policy. Yet another interpretation type switching
policy comes noticing soon number customers front room
increased 1 particular switching point ki , number workers front
room changes + 1. definition policy forms basis model proposed
Berman et al., switching points ki , = 0, . . . , N 1, decision variables
problem, kN fixed S, capacity system.
policy formulation allow worker permanently assigned back
room: definition ki every worker will, system state,
serving customers. Due definition, exist problem instances infeasible policy type yet feasible reality. Consequently, proposed policy
formulation sub-optimal (Terekhov, 2007). However, goal current work
demonstrate applicability constraint programming computing optimal values
given policy type address theoretical optimality questions. Therefore,
term optimal policy used throughout paper refer optimal numerical
parameters policy type proposed Berman et al. (2005).
125

fiTerekhov & Beck

2.2 Berman et al. Model
order determine expected waiting time expected number workers
back room given policy defined particular values ki , Berman et al. first define set
probabilities, P (j), j = k0 , k0 + 1, . . . , S. P (j) denotes steady-state (longrun) probability queue state j, is, exactly j customers
facility. Based Markovian properties queueing system (exponential
inter-arrival service times), Berman et al. define set detailed balance equations
determination probabilities:
P (j) = P (j + 1)1

j = k0 , k0 + 1, . . . , k1 1

P (j) = P (j + 1)2
..
..
.
.

j = k1 , k1 + 1, . . . , k2 1
..
.

P (j) = P (j + 1)i
..
..
.
.

j = ki1 , ki1 + 1, . . . , ki 1
..
.

P (j) = P (j + 1)N

j = kN 1 , kN 1 + 1, . . . , kN 1.

(1)

PS
probabilities P (j) also satisfy equation
j=k0 P (j) = 1. Intuitively,
steady-state, average flow state j state j + 1 equal average
flow state j + 1 state j (Gross & Harris, 1998). Since P (j) viewed
long-run proportion time system state j, mean flow state j
j + 1 P (j) mean flow state j + 1 j P (j + 1)i 1
N depending values switching points.
equations, Berman et al. derive following expressions P (j):
P (j) = j P (k0 ),

(2)



j

=

Xi =





1 j = k0



jk0


,

1 jki1
Xi


ki1 + 1 j ki = 1, . . . , N

i1 kg kg1

1

g=1

(3)

(4)

g

(X1 1), = 1, . . . , N.
P (k0 ) calculated using following equation, derived summing
sides Equation (2) values j:

P (k0 )


X

j = 1.

j=0

126

(5)

fiA CP Approach Queueing Control Problem

quantities interest expressed terms probabilities P (j). Expected
number workers front room
F

=

N
X

ki
X

iP (j),

(6)

i=1 j=ki1 +1

expected number workers back room
B = N F.

(7)

expected number customers front room
L =


X

jP (j).

(8)

j=k0

Expected waiting time queue expressed
Wq =

L
1
.
(1 P (kN ))

(9)

expression derived using Littles Laws system capacity kN = S.
Given family switching policies K = {K; K = {k0 , k1 , ..., kN 1 , S}, ki integers,
ki ki1 1, k0 0, kN 1 < S}, problem formally stated as:
minimizeKK Wq

(10)

s.t. B Bl
PS
j=k0 P (j) = 1

equations (1), (6), (7), (8), (9).
Berman et al. (2005) refer problem problem P1 . important note B,
F L expected values real-valued. Consequently, constraint B Bl
states expected number workers back room resulting realization
policy greater equal minimum expected number back
room workers needed complete work back room. particular time point,
may, fact, fewer Bl workers back room.
far aware, computational complexity problem P1
determined. Berman et al. (p. 354) state solving problem P1 exactly extremely
difficult since constraints set (the detailed balance equations) changes policy
changes.
2.2.1 Berman et al.s Heuristic
Berman et al. (2005) propose heuristic method solution problem.
method based two corollaries theorem, stated proved
authors. results key understanding problem, are, therefore,
repeated below.
127

fiTerekhov & Beck

Theorem 2.1 (Berman et al.s Theorem 1) Consider two policies K K 0
equal one ki . particular, suppose value kJ0 equals kJ 1,
J set {0, ..., N 1} kJ kJ1 2, ki0 = ki 6= J. (a)
Wq (K) Wq (K 0 ), (b) F (K) F (K 0 ), (c) B(K) B(K 0 ).
result Theorem 2.1, seen two policies exist special
properties. Firstly, consider policy
K = {k0 = 0, k1 = 1, k2 = 2, ..., kN 1 = N 1, kN = S} .
policy results largest possible F , smallest possible B Wq .
policy yields smallest possible expected waiting time, optimal feasible.
hand, smallest possible F largest possible Wq B obtained
applying policy

K = {k0 = N, k1 = N + 1, ..., kN 1 = 1, kN = S} .
Therefore, policy infeasible, problem (10) infeasible also.
Berman et al. propose notions eligible type 1 type 2 components. eligible
type 1 component switching point ki satisfying condition ki ki1 > 1
0 < < N ki > 0 = 0. switching point ki eligible type 2 component
ki+1 ki > 1 0 < N . simply, eligible type 1 component ki variable
which, decreased 1, still greater ki1 , eligible type 2 component
ki variable which, increased 1, remain smaller ki+1 . Eligible type 1
components eligible type 2 components referred simply type 1
type 2 components, respectively.

Based definitions policies K K, notions type 1 2 components,
Theorem 2.1, Berman et al. propose heuristic, name
problem used for, P1 :

1. Start K = K.
2. B(K) < Bl , problem infeasible. Otherwise, let imb Wq = Wq (K)
imb K = K. Set J = N .
3. Find smallest j s.t. 0 j < J kj type 1 component. j
exists, go 5. Otherwise, set kj = kj 1. B(K) < Bl , set J = j go 5.
B(K) Bl , go 4.
4. Wq (K) < imb Wq , let imb Wq = Wq (K) imb K = K. Go 3.
5. Find smallest j s.t. 0 j < J kj type 2 component. j
exists, go 6. Otherwise, set kj = kj + 1. B(K) < Bl , repeat 5. B(K) Bl ,
go 4.
6. Stop return imb K best solution.
128

fiA CP Approach Queueing Control Problem

Parameter

N


Bl

Meaning
front room capacity
number workers facility
arrival rate
service rate
expected number workers required back room
Table 1: Summary problem parameters.

Notation
F
B
L
Wq

Meaning
expected number workers front room
expected number workers back room
expected number customers front room
expected customer waiting time

Definition
Equation (6)
Equation (7)
Equation (8)
Equation (9)

Table 2: Summary quantities interest.

Limiting choice j 0 J, resetting J every time
infeasible policy found, prevents heuristic entering infinite cycle. heuristic

guarantees optimality policy returns K K.
Empirical results regarding performance heuristic P1 presented
paper Berman et al. (2005). particular, clear close policies provided
P1 optimal policies.
2.3 Summary Parameters Quantities Interest
Berman et al.s model problem P1 requires five input parameters (Table 1)
expressions calculating four main quantities interest (Table 2),
non-linear.

3. Constraint Programming Models
work done extending CP stochastic problems (Tarim, Manandhar,
& Walsh, 2006; Tarim & Miguel, 2005; Walsh, 2002). problem different
problems addressed papers stochastic information explicitly
encoded constraints expected values, need either stochastic
variables scenarios.
known creating effective constraint programming model usually requires
one experiment various problem representations (Smith, 2006). Consequently,
present, experiment with, three alternative models problem P1 (Equation (10)).
Although first model based directly formulation Berman et al.,
129

fiTerekhov & Beck

models motivated standard CP modelling techniques, use dual
variables (Smith, 2006). investigate following three CP models:
If-Then model CP version formal definition Berman et al.
PSums model uses slightly different set variables, constraints
based closed-form expressions derived constraints used
If-Then model.
Dual model includes set dual decision variables addition variables
used If-Then PSums models. constraints model
expressed terms dual variables.
3.1 Common Model Components
number modelling components common constraint
models. presenting models, therefore present common aspects.
3.1.1 Decision Variables
three proposed models set decision variables ki , = 0, 1, . . . , N , representing switching policy. ki set domain [i, + 1, . . . , N + i]
satisfy constraint ki < ki+1 (since number workers front room, i,
increases number customers, ki , increases). Due Berman et al.s policy
definition, kN must equal S.
3.1.2 Additional Expressions
models include variables constraints representation balance equations
(Equation (1)), expressions F , expected number workers front room,
L, expected number customers front room. However, representations
differ slightly depending model, noted inP
Sections 3.2, 3.3 3.4.
ki
set auxiliary variables, Sum(ki ), defined
j=ki1 +1 j , 1
N 1, included models (see Equations (2)(4) definition j ).
necessary representing Equation (11), relates variables P (k0 ),
floating point variable domain [0..1] representing probability k0 customers
facility. auxiliary variables constraint ensure assignment
decision variables leads unique solution balance equations. discuss formal
definition auxiliary variables Section 3.2.4.
P (k0 )

N
X

Sum(ki ) = 1

(11)

i=0

back room constraint, B Bl , stated models N F Bl . equation
Wq stated models Equation (9).
3.2 If-Then Model
initial model includes variables P (j) j = k0 , k0 + 1, . . . , k1 , k1 + 1, . . . , kN
1, kN , representing steady-state probability j customers front
130

fiA CP Approach Queueing Control Problem

minimize Wq
subject
ki

<

ki+1 {0, 1, . . . , N 1};

kN

=

S;

(ki j ki+1 1)



P (j) = P (j + 1)(i + 1),
{0, 1, . . . , N 1}, j {0, 1, . . . , 1};

(j < k0 )



(P (j) = 0), j {0, 1, . . . , N 1};

P (j)

=

1;

(k0 = j)



P (j)


X
j=0

N
X

Sum(ki ) = 1,

i=0

j {0, 1, . . . , S};
L

=

(ki1 + 1 j ki )




X

jP (j);

j=0

r(i, j) = iP (j),
{1, 2, . . . , N }, j {0, 1, . . . , S};

(ki1 + 1 > j j > ki )



r(i, j) = 0,
{1, 2, . . . , N }, j {0, 1, . . . , S};

F

=


N X
X

r(i, j);

i=1 j=0

Wq

=

N F



auxiliary

L
1
;
(1 P (kN ))
Bl ;
constraints.

Figure 1: Complete If-Then Model
room. floating point variables domain [0..1] satisfy system balance
equations (Equation (1)) used express L F .
complete If-Then model presented Figure 1.
3.2.1 Balance Equation Constraints
balance equations represented set if-then constraints. example, first
balance equation, P (j) = P (j + 1) j = k0 , k0 + 1, ..., k1 1, represented
constraint (k0 j k1 1) P (j) = P (j + 1). Thus, somewhat inelegantly, if-then
131

fiTerekhov & Beck

constraint kind added j 0 1 (inclusive) order
represent one balance equation. order represent rest equations,
technique applied pair switching points ki , ki+1 0 N 1.
results total N if-then constraints.
P
probabilities P (j) also satisfy constraint Sj=k0 P (j) = 1. difficulty
constraint fact sum starts j = k0 , k0 decision variable.
order deal complication, add meta-constraint ((j < k0 ) (P (j) = 0))
j set {0, 1, . . . , N 1}.2 implies values P (j)
jPless k0 0 allows us express sum-of-probabilities constraint

j=0 P (j) = 1.
3.2.2 Expected Number Workers Constraints
set if-then constraints also included order represent Equation (6)
constraint model. due dependence constraint sums
variables two switching points, decision variables. specifically,
add set variables r(i, j) representing product P (j) j
ki1 + 1 ki , constraints (ki1 + 1 j ki ) r(i, j) = iP (j)
(ki1 + 1 > j j > ki ) r(i, j) = 0 1 N j 0 S.
total number if-then constraints 2N (S + 1). F simply stated
sum indices j variables r(i, j).
3.2.3 Expected Number Customers Constraint
L defined according Equation (8). Since meta-constraint ((j < k0 ) (P (j) = 0))
added model order ensure P (j) = 0 j < k0 , constraint
L simply stated sum products j P (j) j 0 S:

L =


X

jP (j).

(12)

j=0

3.2.4 Auxiliary Variables Constraints
model includes set N 2 auxiliary expressions Xi 3 N (X1
X2 always equal 1). Instead including j variables (refer Equation (2)
definition j ), use N + 1 continuous auxiliary variables Sum(ki ) domain

[0 . . . 1 + ], represent sums j variables j = ki1 + 1 j = ki
(inclusive). Sum(k0 ) constrained equal 1, rest variables defined

2. need add constraint j 0 upper bound domain
k0 N .

132

fiA CP Approach Queueing Control Problem

according Equation (13). validity equation proved appendix.


! ki ki1


ki1 k0 +1 1




1

!






6= 1
X






k




X
1

Sum(ki ) =
j =


j=ki1 +1


ki1 k0 +1




1


Xi
(ki ki1 ) otherwise.



(13)

{1, . . . , N };

P
P N
expressed N
sum kj=k
i=0 Sum(ki ). requirement P (k0 )
0 j
PkN
equal 1 stated set if-then constraints (k0 = j) P (j)
0 j
Pj=k
N
i=0 Sum(ki ) = 1 j {0, 1, . . . , S}.
summary, auxiliary constraints present model are: Sum(k0 ) =
Qi1 1 kg kg1
1, Equation (13) Xi = g=1
{3, . . . , N }.
g

If-Then model includes total 3N + 2N + + 1 if-then constraints,
often ineffective search propagation occurs either left-hand side
satisfied right-hand side becomes false. Consequently, next model attempts
avoid, much possible, use constraints.
3.3 PSums Model

second CP model based closed-form expressions derived balance equations
(the details derivations provided appendix). set P (j) variables
formulation Berman et al. replaced set P Sums(ki ) variables
= 0, . . . , N 1, together set probabilities P (j) j = k0 , k1 , k2 , . . . , kN . Note
P (j) defined switching point only, values {0, 1, . . . , S}.
P Sums(ki ) variable represents sum probabilities ki ki+1 1.
complete PSums model presented Figure 2. remainder section
provides details model.
3.3.1 Probability Constraints
Balance equations explicitly stated model. However, expressions P (ki )
P Sums(ki ) derived way balance equations satisfied.
P Sums(ki ) variables defined Equation (14). Equation (15) recursive formula
computing P (ki+1 ).

ki+1 1

P Sums(ki ) =

X

P (j)

j=ki

133

fiTerekhov & Beck



ki+1 ki



1



(i + 1)


P (ki )
(i+1)
6= 1

=
1

(i + 1)






P (ki )(ki+1 ki )
otherwise.


P (ki+1 ) =


(i + 1)

ki+1 ki

(14)

P (ki ), {0, 1, . . . , N 1}.

(15)

PN 1

P Sums(ki ) +

Additionally, probability variables satisfy constraint
P (kN ) = 1.

i=0

3.3.2 Expected Number Workers Constraint
F , expected number workers front room, expressed terms P (ki )
P Sums(ki ) shown Equation (16):
F

=

N
X

[P Sums(ki1 ) P (ki1 ) + P (ki )].

(16)

i=1

3.3.3 Expected Number Customers Constraints
equation L
L =

N
1
X

L(ki ) + kN P (kN )

(17)

i=0



L(ki ) = ki P Sums(ki ) + P (ki )
(i + 1)


ki+1 ki 1
ki+1 ki



(ki ki+1 ) + (i+1)
(ki+1 ki 1) + 1
(i+1)

.
2


1 (i+1)
3.3.4 Auxiliary Constraints
auxiliary constraintsPare exactly If-Then model.
However,
PN
N



equal
1

stated

P
(k
)
Sum(k
requirement P (k0 ) kj=k
0
) = 1,
i=0
0 j
rather set if-then constraints, model explicit closed-form
expression variable P (k0 ).
3.4 Dual Model
problem alternatively formulated using variables wj , represent number
workers front room j customers present. wj variables
134

fiA CP Approach Queueing Control Problem

minimize Wq
subject

N
1
X

ki

<

ki+1 {0, 1, . . . , N 1};

kN

=

P (ki+1 )

=

S;


P Sums(ki )

=

P Sums(ki )

+

P (k0 )



ki+1 ki

P (ki ),
(i + 1)
{0, 1, . . . , N 1};



ki+1 ki




1


(i + 1)


P (ki )
6= 1
(i+1)

,
1

(i
+
1)






P (ki )(ki+1 ki )
otherwise
{1, 2, . . . , N 1};
P (kN ) = 1;

i=0

F

L

=

=

N
X

i=0
N
X

Sum(ki ) = 1;
[P Sums(ki1 ) P (ki1 ) + P (ki )] ;

i=1
N
1
X

L(ki ) + kN P (kN ),

i=0

L(ki )

=

ki P Sums(ki ) + P (ki )






(i+1)

ki+1 ki 1


(i + 1)

(ki ki+1 ) +

1




(i+1)


(i+1)

{0, 1, . . . , N 1};

ki+1 ki

2

L
1
;
(1 P (kN ))
Bl ;

Wq =
N F
auxiliary

constraints.

Figure 2: Complete PSums Model
135

(ki+1 ki 1) + 1

,

fiTerekhov & Beck

referred dual variables because, compared ki s, roles variables
values reversed (Hnich, Smith, & Walsh, 2004; Smith, 2006). stated Smith,
use dual variables constraint programming model beneficial constraints
problem easier express using new variables. case problem
fact, use dual variables allows us significantly reduce number if-then
constraints necessary stating relations probabilities.
Dual model, + 1 wj variables, domain [0, 1, . . . , N ].
variables satisfy following equations: w0 = 0, wS = N wj wj+1
j 0 1. Additionally, complete set ki variables included model,
since constraints easier express using ki rather wj s.
complete Dual model presented Figure 3, details discussed below.
3.4.1 Probability Constraints
Given dual variables, balance equations restated
P (j) = P (j + 1)wj+1 , j {0, 1, . . . , 1}.

(18)

formulation balance equations avoids inefficient if-then constraints. rest
restrictions probability variables
PSstated terms ki variables,
If-Then model. particular, constraints j=0 P (j) = 1 ((k0 > j) (P (j) = 0))
j {0, . . . , N 1} present model.
3.4.2 Channelling Constraints
order use redundant variables, set channelling constraints added
model ensure assignment values one set variables lead unique
assignment variables set. following channelling constraints included:
wj < wj+1 kwj = j

j {0, 1, . . . , 1},

(19)

wj = wj+1 kwj 6= j

j {0, 1, . . . , 1},

(20)

j {0, 1, . . . , S}, {1, . . . , N }.

(21)

wj = ki1 + 1 j ki

Constraints (19) (20) redundant given constraint wj wj+1 . However,
redundancy often lead increased propagation (Hnich et al., 2004). One direction
future work examining effect removing one constraints may
performance program.
3.4.3 Expected Number Workers Constraint
expression expected number workers front room F =
3.4.4 Expected Number Customers Constraint

PS

j=0 wj P (j).

constraint used express L identical one used If-Then model: L =
P

j=0 jP (j). equation valid P (j) j < k0 constrained 0.
136

fiA CP Approach Queueing Control Problem

minimize Wq
subject
w0 = 0,

wS = N

wj



wj+1 j {0, 1, . . . , 1};

ki

<

ki+1 {0, 1, . . . , N 1};

kN

=

S;

wj < wj+1



kwj = jj {0, 1, . . . , 1};

wj = wj+1



kwj 6= j

wj =



ki1 + 1 j ki

j {0, 1, . . . , 1};
{1, 2, . . . , N }, j {0, 1, . . . , S};

P (j)

=

P (j + 1)wj+1 j {0, 1, . . . , 1};

(k0 = j)



P (j)

N
X

Sum(ki ) = 1,

j {0, 1, . . . , S};

i=0

(j < k0 )

X



(P (j) = 0),

P (j)

=

1;

F

=

j {0, 1, . . . , N 1};

j=0


X

wj P (j);

j=0

L

=


X

jP (j);

j=0

Wq

=

N F



auxiliary

L
1
;
(1 P (kN ))
Bl ;
constraints.
Figure 3: Complete Dual Model

137

fiTerekhov & Beck

Statistic/Model
# decision variables
# probability variables
# probability constraints
# constraints F
# constraints L
# if-then constraints
total # constraints

If-Then
N +1
S+1
N (S 1) + 2S + 2
2N (S + 1) + 1
1
3N + 2N + + 1
3N + 4N + 2S + 6

PSums
N +1
2N + 1
2N + 1
1
N +1
0
6N + 5

Dual
N +S +2
S+1
3S N + 2
1
1
S+1
N + 3N + 6S + 8

Table 3: Summary main characteristics three proposed CP models.

3.4.5 Auxiliary Constraints
auxiliary constraints present
exactly If-Then
Pkmodel
N
model. requirement P (k0 ) j=k0 j = 1 also stated set if-then
P
constraints (k0 = j) P (j) N
i=0 Sum(ki ) = 1 j {0, 1, . . . , S}.
3.5 Summary

Table 3 presents summary number variables constraints three
proposed models. seen PSums model smaller number probability
variables constraints slightly larger number constraints representing L
two models, if-then constraints. Dual model larger number
decision variables If-Then PSums models. imply search
space bigger model two sets variables linked channelling
constraints assigned values via propagation. Dual allows simplest
representations F L, requiring one constraint. number probability
constraints Dual smaller equal number constraints
If-Then model greater PSums model. However, actual representation
constraints straightforward Dual since neither requires if-then
constraints closed-form expressions.
hard determine, simply looking Table 3, models
efficient since, CP, larger number constraints and/or variables may actually lead
propagation effective model (Smith, 2006). However, known
if-then constraints propagate well, and, since difference number
constraints PSums model If-Then model Dual model
quite significant, one may expect PSums model advantage
two models.
fact, preliminary experiments three models showed poor performance (see
Table 4 Section 6). Due complexity constraints relating decision variables
variables representing probabilities, little constraint propagation, and, essentially,
search required explore entire branch-and-bound tree. consequence,
following two sections examine dominance rules (Beck & Prestwich, 2004; Smith, 2005)
shaving (Caseau & Laburthe, 1996; Martin & Shmoys, 1996), two stronger inference
138

fiA CP Approach Queueing Control Problem

forms used CP. Section 8, investigate models without dominance rules
shaving need search whole tree order prove optimality, also discuss
differences performance models based experimental results.

4. Dominance Rules
dominance rule constraint forbids assignments values variables
known sub-optimal (Beck & Prestwich, 2004; Smith, 2005). problem P1 ,
dominance rule states that, given feasible solution, K, solutions
least one switching point assigned lower value value assigned K.
words, given two solutions K K 0 , Wq value resulting policy K 0
smaller equal Wq value resulting K, exist switching
points ki0 ki K 0 K, respectively, satisfying condition ki0 < ki . following
theorem states dominance rule formally.
0 )
Theorem 4.1 (Dominance Rule) Let K = (k0 , k1 , . . . , kN ) K 0 = (k00 , k10 , . . . , kN
0
two policies k0 = k00 = 0, k1 = k10 = 1, . . . , kJ1 = kJ1
= J 1 kJ 6= kJ0
0
(i.e. least one kJ , kJ strictly greater J) J {0, 1, . . . , N 1}. Let
Wq (K) Wq (K 0 ) denote expected waiting times resulting two policies K
K 0 , respectively. Wq (K 0 ) < Wq (K), exists {J, J + 1, J + 2, . . . , N 1}
ki0 < ki .

Proof: [By Contraposition] prove contrapositive statement Theorem
4.1: assume exist {J, J + 1, . . . , N 1} ki0 < ki
show that, given assumption, Wq (K 0 ) greater equal
Wq (K).
Assume {J, J + 1, . . . , N 1} exists ki0 < ki . one
following true:
(a) kn = kn0 n {J, J + 1, . . . , N 1},
(b) exists least one j {J, J + 1, . . . , N 1} kj0 > kj ,
values rest switching points two policies.
Case (a) implies K 0 K policy, Wq (K) = Wq (K 0 ).
prove (b), suppose exists exactly one j {J, J + 1, J + 2, . . . , N 1}
kj0 > kj , kn = kn0 n {J, . . . , N 1} \ {j}. K K 0
different value exactly one switching point. Consequently, Theorem 2.1,
Wq (K 0 ) Wq (K). Similarly, applying Theorem 2.1 several times, result
generalizes cases exists one j kj0 > kj .
Therefore, {J, J + 1, . . . , N 1} exists ki0 < ki , follows
Wq (K 0 ) Wq (K). words, Wq (K 0 ) < Wq (K), exists
{J, J + 1, . . . , N 1} ki0 < ki .

2

, . . . , k
Theorem 4.1 implies that, given feasible policy (0, 1, . . . , ki , ki+1
N 1 , S)
switching points index greater assigned values strictly greater
lower bounds, know solution smaller Wq satisfy constraint
) . . . (k

((ki < ki ) (ki+1 < ki+1
N 1 < kN 1 )). Therefore, order implement

139

fiTerekhov & Beck

dominance rule, add constraint search every time feasible policy
found, lead reduction size search space. Section 6 presents
experimental results regarding usefulness technique.

5. Shaving
Shaving inference method temporarily adds constraints model, performs
propagation, soundly prunes variable domains based resulting state
problem (Demassey, Artigues, & Michelon, 2005; van Dongen, 2006). example,
simple shaving procedure may based assignment value variable x.
propagation following assignment results domain wipe-out variable,
assignment inconsistent, value removed domain x (Demassey
et al., 2005; van Dongen, 2006). general case, temporary constraint
inferences made based complex. Shaving particularly useful
job-shop scheduling domain, used reduce domains start end
times operations (Caseau & Laburthe, 1996; Martin & Shmoys, 1996). problems,
shaving used either domain reduction technique search, incorporated
branch-and-bound search variable domains shaved decision (Caseau
& Laburthe, 1996).
shaving procedure problem, temporarily assign particular value
switching point variable, rest variables assigned either maximum
minimum possible values. Depending whether resulting policies feasible
infeasible, new bounds switching point variables may derived.
instance N = 3, = 6, = 15, = 3, Bl = 0.32 used illustration
purposes. Policy K, always yields smallest possible Wq , instance

(k0 , k1 , k2 , k3 ) = (0, 1, 2, 6) policy K, always yields greatest possible Wq ,
(3, 4, 5, 6). Thus, initial domains switching points [0..3], [1..4], [2..5] [6]
k0 , k1 , k2 k3 , respectively. step, shaving may able reduce domains
one variables.
5.1 Bl -based Shaving Procedure
initial shaving procedure consists two cases either upper lower
bounds variables may modified. first case, constraint ki = min(ki ),
min(ki ) smallest value domain ki , temporarily added problem
particular value 0 N . switching points assigned
maximum possible values using function gMax. Given array variables,
function gMax assigns maximum possible values variables yet
value, returning true resulting assignment feasible, false otherwise.
maximum possible values necessarily upper bound values domains
corresponding variables, rather highest values domains respect
condition kn < kn+1 , n {0, ..., N 1}. example, k1 assigned
value 1, rest variables unbound, gMax would result policy (0, 1, 5, 6),
feasible B value 0.508992, thus true would returned.
140

fiA CP Approach Queueing Control Problem

Recall assignment infeasible yields B value smaller
Bl . policy resulting addition ki = min(ki ) use gMax
infeasible, min(ki ) + 1 max(ki ), constraint ki > min(ki ) added
problem: variables except ki set maximum values, problem
infeasible, feasible policy ki must greater min(ki ). reasoning
valid since Theorem 2.1 states increasing value switching point increase
B. Note solution feasible, recorded best-so-far solution
Wq value smaller Wq value previous best policy. easier reference,
part shaving procedure referred gMax case.
second case, constraint ki = max(ki ) added problem
0 N , max(ki ) maximum value domain ki . rest
variables assigned minimum values domains using function
gMin. assignments made way respects constraints kn < kn+1 ,
n {0, ..., N 1}. resulting policy feasible, constraint ki < max(ki )
permanently added problem, assuming max(ki ) 1 min(ki ). Since variables
except ki minimum values already, ki maximum, must true,
Theorem 2.1, better solution value ki smaller
max(ki ). case referred gMin case.
cases, inferred constraint violates current upper lower bound
ki , best policy found point optimal. Whenever domain
switching point modified result inferences made gMax gMin case,
switching points need re-considered. domain one variable reduced
particular shaving iteration, temporary constraints added next
round shaving different ones used previously, and, consequently, new
inferences may possible. Thus, shaving procedure terminates optimality
proved inferences made.
Consider example mentioned above. Suppose constraint k0 = 0 added
problem, rest variables assigned maximum possible values (using
gMax ). resulting policy (k0 , k1 , k2 , k3 ) = (0, 4, 5, 6). policy yields B value
0.63171, implies policy feasible 0.63171 > 0.32 = Bl ,
domain reductions inferred. constraint k0 = 0 removed. Since
domain k0 modified, procedure considers next variable. Thus,
constraint k1 = 1 added, variables set maximum values.
resulting policy (0, 1, 5, 6), also feasible since B value 0.508992.
constraint k1 = 1 removed, k2 = 2 added. variables
set maximum values, resulting policy (0, 1, 2, 6). policy yields B value
0.1116577, smaller Bl . Thus, policy infeasible, constraint
k2 > 2 added problem. changes domain k2 [3..5]. Whenever
domain variable reduced, next shaving step considers switching point,
next constraint added k2 = 3.
Now, consider gMin case assume variables full initial domains.
Suppose constraint k0 = 3 added problem. rest variables
assigned smallest possible values consistent k0 = 3. Thus, policy (3, 4, 5, 6)

considered. policy B value 0.648305 feasible (it fact K,
infeasible, problem would infeasible). value k0 better solution
141

fiTerekhov & Beck

smaller 3, domains variables become [0..2], [1..4], [2..5],
[6]. constraint k0 = 3 removed, and, since domain k0 modified,
constraint k0 = 2 added next. policy considered (2, 3, 4, 6). policy
also feasible, domains become [0..1], [1..4], [2..5], [6]. temporary constraint
k0 = 2 removed, next one added k0 = 1. corresponding policy assigned
gMin infeasible, domain reductions made. Since addition k0 = 1
result domain reductions, need reconsider variable k0
switching points looked at. Consequently, next temporary constraint
added k1 = 4.
complete Bl -based shaving procedure, start either gMin
gMax case. Since policies considered gMin case generally smaller waiting
time ones considered gMax case, may beneficial start gMin
case. approach take.
complete Bl -based shaving algorithm presented Figure 4. assumed
algorithms presented functions add(constraint) remove(constraint)
add remove constraint model, respectively.
Upon completion shaving procedure, constraint Wq bestWq ,
bestWq value best solution found point, added (Wq bestWq
rather Wq < bestWq added numerical issues testing equality
floating point numbers). However, although constraint rules policies higher
Wq infeasible, results almost propagation domains decision variables
little reduce size search tree. order remedy problem, another
shaving procedure, time based constraint Wq bestWq proposed next
sub-section. issue lack propagation domains ki addition
constraint discussed detail Section 8.1.
5.2 Wq -based Shaving Procedure
Wq -based shaving procedure makes inferences based strictly constraint Wq
bestWq : constraint B Bl removed prior running procedure order
eliminate possibility incorrect inferences. Bl -based shaving, constraint
form ki = max(ki ), max(ki ) maximum value domain ki , added
temporarily, function gMin used assign values rest variables.
Bl constraint removed, reason infeasibility
policy Wq value greater best Wq encountered far.
Since switching points except ki assigned smallest possible values, infeasibility
implies solution smaller expected waiting time, value ki
strictly smaller max(ki ). shaving procedure stated Figure 5.
5.3 Combination Shaving Procedures
Wq -based Bl -based shaving result different domain reductions since
based two different constraints. Moreover, using two together may cause
domain modifications either used itself. Therefore, makes sense run
Bl -based Wq -based shaving procedures alternately (with Wq Bl constraints added
142

fiA CP Approach Queueing Control Problem

Algorithm 1: Bl -based Shaving
Input: S, N , , , Bl (problem instance parameters); bestSolution (best solution found
far)
Output: bestSolution (possibly) modified domains variables ki , bestSolution
proof optimality
(there domain changes)
0 N 1
(shaving successful Domain(ki ))
add( ki = max(Domain(ki )) )
(gMin)
(new best solution found)
bestSolution = currentSolution;
( max(Domain(ki )) 1 min(Domain(ki )) )
add( ki < max(Domain(ki )) )
else
return bestSolution; stop, optimality proved
remove( ki = max(Domain(ki )) )
(shaving successful Domain(ki ))
add( ki = min(Domain(ki )) )
(gMax )
(new best solution found)
bestSolution = currentSolution;
else
( min(Domain(ki )) + 1 max(Domain(ki )) )
add( ki > min(Domain(ki )) )
else
return bestSolution; stop, optimality proved
remove( ki = min(Domain(ki )) )

Figure 4: Bl -based shaving algorithm

143

fiTerekhov & Beck

Algorithm 2: Wq -based Shaving
Input: S, N , , , Bl (problem instance parameters); bestSolution (best solution found
far)
Output: bestSolution (possibly) modified domains variables ki , bestSolution
proof optimality
(there domain changes)
0 N 1
(shaving successful Domain(ki ))
add( ki = max(Domain(ki )) )
(!gMin)
( max(Domain(ki )) 1 min(Domain(ki )) )
add( ki < max(Domain(ki )) )
else
return bestSolution; stop, optimality proved
remove( ki = max(Domain(ki )) )

Figure 5: Wq -based shaving algorithm

removed appropriately) domain pruning possible. combination
two shaving procedures referred AlternatingShaving.
AlternatingShaving procedure effectively combined search following manner. AlternatingShaving run initially, domain modifications
possible. Search performed better solution found, point
AlternatingShaving applied again. Subsequently, search shaving alternate
one proves optimality best solution found. approach may
successful search finds new best solution, new constraint Wq
added, Wq -based shaving may able reduce upper bounds switching
point variables. way combining search shaving referred
AlternatingSearchAndShaving.
variations shaving also possible. particular, Bl -based Wq -based
shaving procedures extended make inferences values two switching points.
example, one assign maximum values pair switching point variables,
assigning minimum values rest. resulting policy feasible, constraint
stating least one variable pair assigned smaller value
added problem. Preliminary experiments indicated shaving procedures based
two switching points not, general, result effective models. procedures
explicitly reduce domains switching point variables rather add set
constraints model appear, practice, significantly reduce size
search space. One possible direction future work may investigate
variations shaving.
144

fiA CP Approach Queueing Control Problem

6. Experimental Results
Several sets experiments3 performed order evaluate efficiency proposed models effectiveness dominance rules shaving procedures, well
compare performance best CP model performance heuristic P1 .
constraint programming models implemented ILOG Solver 6.2, heuristic
Berman et al. implemented using C++.
note numerical results obtained experiments sensitive level
precision set. constraint programming models, set default ILOG Solver
precision 0.000001. implies floating point variables model
considered bound maximum (max) minimum (min) values intervals
((max min)/(max{1, |min|}) 0.000001 (Solver, 2006). order propagate constraints involving floating point variables, Equation (15), ILOG Solver uses
standard interval arithmetic outward rounding.4
6.1 Problem Instances

information gained policies K K explicitly used implementation

three models. K infeasible, program stops feasible solution
instance. Otherwise, K feasible, optimal. two cases K

optimal K infeasible therefore trivial solved easily CP models

Berman et al.s heuristic. Although instances K optimal hard
solve without shaving, using elementary Bl -based shaving procedure always result
(usually fast) proof optimality policy. case also trivial Berman
et al.s heuristic. Consequently, experimental results presented based

instances optimal solution K K.
Preliminary experiments indicated value significant impact
efficiency programs since higher values result larger domains ki variables
models higher number wj variables Dual model. indicated
Table 3 Section 3.5, also big impact number constraints IfThen Dual models. Therefore, consider instances value
set {10, 20, . . . , 100} order gain accurate understanding performance
model heuristic. note instances greater 100, neither
method Berman et al.s heuristic P1 may used due numerical instability.
Thirty instances generated way ensure instance

feasible optimal policy neither K K. created random combinations

parameter values chose instances policy K found
feasible, optimal, K determined infeasible. order check

K optimal, sufficient find feasible solution one switching
point assigned value lower upper bound. generating combinations
parameters, values N chosen uniform probability set {2, . . . , 38},
3. Numerical values results slightly different ones presented previous
work (Terekhov & Beck, 2007) due minor errors discovered publication paper.
main conclusions analysis previous work remain valid, however.
4. Jean-Francois Puget - personal communication.

145

fiTerekhov & Beck

values {5,. . . , 99}, values {1, . . . , 49} values Bl
{1, . . . , 4}. appears easy way determining whether given instance

K K optimal solution based parameter values. Moreover,
preliminary experiments indicated problem difficulty depends combination
problem parameters (especially S, N Bl ) rather one parameter only.
10-minute time limit overall run-time program enforced
experiments. experiments performed Dual Core AMD 270 CPU 1 MB
cache, 4 GB main memory, running Red Hat Enterprise Linux 4.
6.2 Performance Measures
order perform comparisons among CP models, CP models
Berman et al.s heuristic, look mean run-times, number instances
optimal solution found, number instances optimality proved,
number instances best-known solution found mean relative
error (MRE). MRE measure solution quality allows one observe quickly
particular algorithm able find good solution. MRE defined
RE(a, ) =

1 X c(a, m) c (m)
|M |
c (m)

(22)

mM

algorithm used solve problem, set problem instances
algorithm tested, c(a, m) cost solution found instance
algorithm a, c (m) best-known solution instance m. generated
instances, c (m) best solution found experiments.
6.3 Comparison Constraint Programming Models Techniques
CP model tested without shaving dominance rules. total 30
CP-based methods therefore evaluated. model Bl -based shaving model
runs Bl -based shaving procedure domain changes possible, adds
constraint value Wq based best solution found shaving procedure runs search rest time. Similarly, models Wq -based shaving
AlternatingShaving models run Wq -based shaving procedure AlternatingShaving procedure, respectively, longer possible reduce domains
switching point variables, add constraint requiring Wq less expected
waiting time best solution found shaving procedure use search
rest time. described previously, AlternatingSearchAndShaving alternates
search AlternatingShaving procedure. models, search assigns switching points
increasing index order. smallest value domain variable tried first.
6.3.1 Constraint Programming Models
Table 4 presents number instances, 300, optimal solution
found proved 30 proposed CP-based methods. table indicates
PSums model effective three, proving optimality largest
number instances regardless use dominance rules shaving. Alter146

fiA CP Approach Queueing Control Problem

Shaving

If-Then
PSums
Dual


105
126
105

ND
105
126
105

Bl -based
Shaving
ND
192 191
202 201
191 191

Wq -based
Shaving
ND
105 105
126 126
105 105

Alternating
Shaving

ND
219
218
225
225
218
218

AlternatingSearch
AndShaving

ND
234
234
238
238
232
232

Table 4: Number instances optimal solution found optimality
proved within 10 CPU-minutes total 300 problem instances (D -
dominance rules, ND - without dominance rules).

natingSearchAndShaving, PSums proves optimality largest number instances:
79.3% instances, 238 239 instances optimality proved
model.
Figure 6 shows MRE changes first 50 seconds run-time If-Then,
PSums Dual models AlternatingSearchAndShaving, Bermans heuristic
(we comment performance heuristic Section 6.4). PSums is, average,
able find better solutions two models given amount run-time.
Table 5, additional statistics regarding performance three models
AlternatingSearchAndShaving without dominance rules presented (we comment
statistics P1 Section 6.4). particular, model, number
instances finds best solution (out 300), number instances
finds optimal solution (out 239 cases optimality proved)
number times proves optimality (out 300) presented. seen
models find optimal solution 239 instances known. However,
PSums model proves optimality 4 instances If-Then model 6
instances Dual. PSums also finds best-known solution algorithm
97.6% instances considered. detailed discussion differences
performance CP models presented Section 8.2.
6.3.2 Shaving Procedures
Table 4, observed CP models without shaving Wq based shaving procedure prove optimality fewest number cases. similarity
performance models without shaving Wq -based shaving surprising
Wq -based procedure able start pruning domains value
best policy found prior procedure quite good. Wq -based procedure

used alone, one solution base inferences on, namely K. Since policies

result smaller expected waiting time K, procedure useless.
Employing Bl -based shaving procedure substantially improves performance
models: without dominance rules, If-Then, PSums Dual models prove
optimality 86, 75 86 instances, respectively, corresponding models
without shaving Wq -based shaving; dominance rules, situation
147

fi0.10

Terekhov & Beck

0.06
0.04
0.00

0.02

Mean Relative Error

0.08

IfThen Model
PSums Model
Dual Model
Berman et al.s Heuristic P1

0

10

20

30

40

50

Run Time (seconds)

Figure 6: Comparison MRE three CP models AlternatingSearchAndShaving
Bermans heuristic P1 .

equivalent. results imply inferences made based Bl constraint effective
reducing domains decision variables.
Models AlternatingShaving AlternatingSearchAndShaving perform even better models employing Bl -based shaving procedure. real power Wq based shaving becomes apparent combined Bl -based shaving
Bl -based shaving often finds good solution good value bestWq , allowing
Wq -based procedure infer domain reductions. observation explains AlternatingSearchAndShaving performs better AlternatingShaving. particular, AlternatingSearchAndShaving, Wq -based procedure used new best solution
148

fiA CP Approach Queueing Control Problem

PSums
If-Then
Dual
P1

# best found (/300)
293
282
279
282

# opt. found (/239)
239
239
239
238

# opt. proved (/300)
238
234
232
0

Table 5: Comparison three CP models (with AlternatingSearchAndShaving without
dominance rules) Bermans heuristic P1 .

found shaving one found search. Therefore, higher quality
solution found search, used Wq -based procedure prune
domains switching point variables.
Figure 7, average run-times value 10 100 presented
four shaving procedures PSums model. Since run-time limit 600 seconds
used throughout experiments, assumed run-time 600 seconds instances
optimality proved within limit. Therefore, mean run-times
reported throughout paper underestimates true means. Figure 7 shows that,
value S, AlternatingSearchAndShaving procedure gives best performance.
also seen that, increases, becomes increasingly difficult prove optimality
average run-times increase. stated previously, due larger domains
switching point variables. AlternatingSearchAndShaving procedure, however, able
significantly reduce domains ki variables therefore provides effective
method instances higher values well.
6.3.3 Dominance Rules
Table 4 indicates rarely difference number instances solved
optimality models without dominance rules. difference visible
model without shaving, Wq -based shaving AlternatingSearchAndShaving.
Recall dominance rules implemented addition constraint
values switching point variables solution found. constraint
effective switching point variables assigned minimum values
current solution. Usually, policies also ones result smaller
expected waiting time. Similarly, Wq -based shaving useful solution
small expected waiting time found. leads conjecture dominance rules
may effective instances Wq -based shaving procedure
effective. conjecture supported results Table 4. particular,
Wq -based shaving procedure used itself, makes inferences based policy

K, solution generally poorest quality instance. method single
run Wq -based shaving therefore heavily relies search. Since search takes long time
find feasible solution good quality, effectiveness dominance rule-based constraints
also visible within given time limit.
149

fi200

300

400

500

Wqbased Shaving
Blbased Shaving
AlternatingShaving
AlternatingSearchAndShaving

0

100

Average RunTimes (seconds)

600

Terekhov & Beck

20

40

60

80

100

Values

Figure 7: PSums model various shaving techniques: average run-times value
S. Average run-times PSums without shaving shown graph
since resulting curve would indistinguishable one Wq -based
shaving.

hand, AlternatingSearchAndShaving, Wq -based procedure plays
key role makes domain reductions based high quality solutions produced
Bl -based shaving, later, search. Dominance rules play role procedure
since shaving used every new solution found. However, even dominance rule
constraints explicitly incorporated procedure (i.e. added
new run search), would redundant since serve essentially
purpose Wq -based shaving procedure.
shaving used, results equivalent achieved Wq -based
shaving employed. explanation absence difference models
150

fi1 e03
1 e07

1 e05

PSums Model 10 seconds
PSums Model 150 seconds
PSums Model 500 seconds
Berman et al.s Heuristic P1

1 e09

Mean Relative Error

1 e01

CP Approach Queueing Control Problem

20

40

60

80

100

Values

Figure 8: MRE value P1 P Sums model.

without dominance rules therefore also same. particular, takes long time
solution found whose quality allows dominance rule constraint
effectively reduce size search tree.
Bl -based shaving AlternatingShaving used, dominance rules sometimes helpful. cases, two shaving procedures, subsequent
search usually finds good solution quickly, and, since Wq -based shaving used
point, dominance rule constraint added effective reducing
size search tree.
Overall, observed using AlternatingSearchAndShaving without dominance
rules effective using Bl -based shaving AlternatingShaving dominance
151

fiTerekhov & Beck

rules. Therefore, comparisons, focus models
AlternatingSearchAndShaving without dominance rules.
6.4 Heuristic P1 vs. Best Constraint Programming Approach
Empirical results regarding performance heuristic P1 presented Berman
et al. (2005), ability P1 find good switching policies explicitly
evaluated previous work. wanted find well heuristic actually performs
comparing CP methods.
Table 5, present several measures performance three proposed models
AlternatingSearchAndShaving heuristic P1 . heuristic performs well,
finding best-known solution eleven fewer instances PSums model,
three instances Dual model number instances
If-Then model. Moreover, heuristic finds, but, course, cannot prove, optimal
solution 238 239 instances optimal known. three CP models
find optimal solution 239 these. run-time heuristic negligible,
whereas mean run-time PSums model approximately 130 seconds (the mean
run-times two models slightly higher: 141 seconds If-Then model
149 seconds Dual model).
Table 5 also shows PSums model able find best-known solution 11
instances heuristic. Closer examination reveals 275 instances
PSums model P1 find best-known solution, 18 instances
PSums able 7 heuristic finds best-known.
Figure 6, observed heuristic achieves small MRE
negligible amount time. 50 seconds run-time, MRE 300 instances
resulting PSums AlternatingSearchAndShaving becomes comparable
heuristic MRE. Figure 8, MRE 30 instances value presented
heuristic PSums AlternatingSearchAndShaving 10, 150 500
seconds run-time. 10 seconds, performance PSums comparable
heuristic values smaller equal 40, heuristic appears
quite bit better higher values S. 150 seconds, performance PSums
comparable heuristic except values 50 80. 500 seconds,
PSums smaller MRE 300 instances also lower (or equal) MRE
value except 50 100.
Overall, results indicate heuristic performs wellits run-time negligible,
finds optimal solution one cases known, finds
best solution 94% instances. Moreover, results low MRE. Although
PSums AlternatingSearchAndShaving able achieve slightly higher numbers
performance measures, clear improvements small given
PSums run-time much higher run-time heuristic.

7. PSums-P1 Hybrid
Naturally, desirable create method would able find solution high
quality short amount time, Bermans heuristic, would also
high rate able prove optimality within reasonable run-time
152

fiA CP Approach Queueing Control Problem

PSums
P1
PSums-P1
Hybrid

# best found (/300)
293
282
300

# opt. found (/239)
239
238
239

# opt. proved (/300)
238
0
238

Table 6: Comparison PSums model AlternatingSearchAndShaving Berman et
al.s heuristic P1 Hybrid model.

PSums AlternatingSearchAndShaving. therefore worthwhile experiment
PSums-P1 Hybrid, starts running P1 then, assuming instance
feasible, uses PSums model AlternatingSearchAndShaving find better solution
prove optimality solution found P1 (infeasibility instance proved

heuristic determines policy K infeasible).
Since shown heuristic P1 fast, running first incurs almost
overhead. Throughout analysis experimental results, also noted
performance Wq -based shaving procedure depends quality best solution
found used. shown heuristic provides solutions high
quality. Therefore, first iteration Wq -based procedure may able significantly
prune domains switching point variables good-quality solution found
heuristic. Continuing alternating two shaving techniques search,
also shown effective approach, lead good results.
proposed Hybrid algorithm tested set 300 instances
used above. Results illustrating performance Hybrid well performance
P1 PSums AlternatingSearchAndShaving presented Table 6. Hybrid
able find best solution 300 cases: 275 instances heuristic
PSums find best-known solution, 18 PSums finds best-known
7 heuristic so. Hybrid finds optimal solution (for
instances known) proves optimality many instances PSums
model. mean run-time Hybrid essentially identical mean run-time
PSums AlternatingSearchAndShaving, equalling approximately 130 seconds.
Thus, Hybrid best choice solving problem: finds good solution
heuristic little time (close 0 seconds), able prove optimality
many instances best constraint programming method, finds best-known
solution instances considered. Moreover, improvements achieved without
increase average run-time PSums model.

8. Discussion
section, examine reasons poor performance CP models
without shaving, suggest reasons observed differences among CP models, discuss
performance Hybrid present perspectives work.
153

fiTerekhov & Beck

8.1 Lack Back-Propagation
experiments, instances even PSums-P1 Hybrid
AlternatingSearchAndShaving unable find prove optimal solution within
10-minute time limit. fact, many instances, amount time spent
search higher time spent shaving, run-time limit usually reached
search, rather shaving, phase. analysis algorithms
behaviour suggests poor performance search explained lack backpropagation. Back-propagation refers pruning domains decision variables
due addition constraint objective function: objective constraint
propagates back decision variables, removing domain values reducing search.
CP models presented above, little back-propagation.
Consider model without shaving. Throughout search, new best solution found,
constraint Wq bestWq , bestWq new objective value, added
model. However, domains switching point variables usually reduced
way addition constraint. illustrated observing
amount propagation occurs PSums model Wq constrained.
example, consider instance problem = 6, N = 3, = 15, = 3,
Bl = 0.32 (this instance used Section 5 illustrate shaving procedures).
initial domains switching point variables [0..3], [1..4], [2..5] [6]. initial
domains probability variables P (ki ) i, addition Wq bounds

provided K K, listed Table 7. initial domain Wq , also determined

objective function values K K, [0.22225..0.425225]. initial domains L
7
F , [2.8175e ..6] [0..2.68], respectively. Upon addition constraint
Wq 0.306323, 0.306323 known optimal value instance, domain
Wq reduced [0.22225..0.306323], domain L becomes [1.68024..6]
domain F remains [0..2.68]. domains P (ki ) addition listed
Table 7. domains types probability variables reduced addition
new Wq constraint. However, domains switching point variables remain
unchanged. Therefore, even though policies value Wq less 0.306323
infeasible, constraining Wq less equal value result
reduction search space. still necessary search policies order
show better feasible solution exists.
One reasons lack pruning domains ki variables due
Wq constraint likely complexity expression Wq = (1PL(kN )) 1 . example
above, Wq constrained less equal 0.306323, get constraint
1
L
0.306323 15(1P
(kN )) 3 , implies 9.594845(1P (kN )) L. explains
domains L P (kN ) change upon addition model. domains
rest P (ki ) variables change relationships P (ki )s (Equation
(15)) constraint sum probability variables 1.
Similarly, domains P Sums(ki )s change variables expressed
terms P (ki ) (Equation (14)). However, actual ki variables mostly occur
exponents expressions P Sums(ki ), P (ki ), L(ki ), minor changes domains
P Sums(ki ), P (ki ), L(ki ) happen due constraint Wq effect
domains ki . analysis suggests may interesting investigate CP
154

fiA CP Approach Queueing Control Problem

j
k0
k1
k2
k3

addition Wq 0.306323
P (j)
P Sums(j)
[4.40235e6 ..0.979592]
[0..1]
7
[1.76094e ..1]
[0..1]
[2.8175e8 ..0.6]
[2.8175e8 ..1]
[4.6958e8 ..1]
N/A

addition Wq 0.306323
P (j)
P Sums(j)
[4.40235e6 ..0.979592]
[0..0.683666]
[0.000929106..1]
[0..0.683666]
[0.0362932..0.578224] [0.0362932..0.71996]
[0.28004..0.963707]
N/A

Table 7: Domains P (j) P Sums(j) variables j = k0 , k1 , k2 , k3 ,
addition constraint Wq 0.306323.

model based log-probabilities rather probabilities themselves. model
may lead stronger propagation.
Likewise, If-Then Dual models, domains decision variables
reduced bound objective function value added, although domains
probabilities, L F modified. models, constraints relating F , L
probability variables variables ki balance equations, quite
complex. domains probability variables seem reduced significantly
enough due new Wq bound result pruning ki domains
constraints.
observations served motivation proposed shaving techniques.
particular, Wq -based shaving procedure reduces domains switching point variables
shown values necessarily result higher Wq value
best one found point. makes lack back-propagation.
However, even procedure used new best solution found,
AlternatingSearchAndShaving, always able prune enough values domains
ki able prove optimality within 10 minutes run-time.
therefore seen inferences based value Wq limited power
and, therefore, domains switching point variables large shaving,
possible prove optimality short period time.
8.2 Differences Constraint Programming Models
Experimental results demonstrate PSums model best three models
without shaving. section, examine models detail
attempt understand reasons differences.
8.2.1 Comparison PSums two models
order analyze performance models without shaving, look mean
number choice points statistics, give indication size search space
explored. compare three models, look mean number choice
points considered first feasible solution found mean total number
choice points explored within 600 seconds run-time.
155

fiTerekhov & Beck

If-Then
PSums
Dual

21 Instances Solved PSums
First Solution
Total
8234
137592
6415
464928
7528
102408

105 Instances Solved Models
First Solution
Total
1201
37596
1064
36590
1132
36842

Table 8: Mean number choice points explored first solution found mean
total number choice points explored within 600 seconds three models
without shaving without dominance rules. PSums, latter statistic
corresponds total number choice points needed prove optimality.

Table 4, shown that, without shaving, PSums model proves optimality
21 instances two models 105 instances
three models prove optimality. Table 8, present mean number choice points
statistics three models sets instances. seen
mean number choice points need explored PSums model order
find initial solution smaller two models, 105 instances
eventually solved optimality models 21 instances
solved optimality PSums. variable value ordering heuristics
used models, observation implies propagation occurs search
PSums model two models. claim supported
fact mean total number choice points 105 instances solved
models smaller PSums two models.
Table 8 also shows that, 21 instances solved optimality
PSums, mean total number choice points highest PSums model. Since
PSums one three models solve instances, implies
propagation happening faster model. observation confirmed results
105 instances solved three models: instances, Dual
explores average 713 choice points per second, If-Then model explores average
895 choice points per second PSums model explores average 1989 choice
points per second. words, appears propagation PSums model
twice fast two models.
detailed examination results showed 82 105 instances
solved models, number choice points explored, given instance,
models. Moreover, instances value N ,
number choice points explored equal. Figure 9, run-times three models
number choice points increases illustrated. order create graph,
averaged run-times instances number choice points examined
same. points figure labeled (S, N ) order show relationship
number choice points, values N , run-times. note
one instance, = 10 N = 6, number choice points
instances = 10 N = 4. However, instances
156

fiA CP Approach Queueing Control Problem

(70,3)

300

(40,4)
(30,5)

200

(60,3)
(20,9)
(20,8)
100

RunTime (seconds)

400

IfThen Model
PSums Model
Dual Model

(50,3)
(20,6)

0

(70,2)

(20,7)

0

50000

100000

150000

Choice Points

Figure 9: Run-times averaged instances equal number choice points explored, 82 instances number choice points
models. labels points indicate (S, N ) values corresponding
instances.

(out 82), one-to-one correspondence (S, N ) number choice
points.
Several observations made Figure 9. Firstly, graph demonstrates
propagation PSums model faster models. Secondly, behaviour
PSums model appears quite different If-Then
Dual models. run-times PSums model seem significantly influenced
value N . example, = 20, run-times model increase N increases
157

fiTerekhov & Beck

6 9. Moreover, given two instances one high low N ,
low high N , PSums model generally needs longer time
solve instance low high N (e.g., compare points (20, 7) (40, 4),
(20, 7) (70, 3)). If-Then Dual models, several cases
run-times instances high low N higher instances
low high N (e.g., compare run-time (70, 3) (20, 7)), although
opposite happens well (e.g., compare (50, 3) (40, 4)). Thus, appears N
parameter influencing run-times PSums most, two models,
N influential, greater effect. Although characteristics
require additional investigation, one possible reason differences model behaviour
could relationship number constraints models problem
parameters. Table 3, known number constraints mostly determined
value If-Then Dual models (since typically larger N ),
value N PSums model. Combining observations Table 3 Figure
9, appears effect N run-times due influence
number constraints models.
Overall, examination indicates superiority PSums model without
shaving caused stronger propagation (Table 8) fact propagation
faster (Figure 9).
shaving employed, PSums model also performs better Dual
If-Then models, proving optimality greater number instances (Table 4) finding
good-quality solutions faster (Figure 6). models, shaving procedures make
number domain reductions shaving based Wq Bl constraints,
present models. However, time shaving iteration takes
different different models. empirical results show iteration shaving
takes smaller amount time PSums model If-Then Dual
models. Thus, shaving, PSums model performs better two,
shaving faster subsequent search, necessary, faster.
8.2.2 Comparison If-Then Dual models
comparison If-Then model AlternatingSearchAndShaving Dual
AlternatingSearchAndShaving using Figure 6 shows If-Then model usually able
find good solutions smaller amount time. Moreover, shown Table 5,
If-Then model AlternatingSearchAndShaving finds best solution three
instances, proves optimality two instances, Dual model
shaving procedure. shaving procedures without shaving, statistics
show almost difference performance two models. expected
Dual would outperform If-Then model uses simpler representation
balance equations expressions F L, smaller number if-then
constraints. (there Table 8 shows Dual explore smaller number
choice points find initial solution. 105 instances models
solve, total number choice points explored Dual also smaller. However,
If-Then model faster, exploring, average, 895 choice points per second compared
average 713 choice points per second explored Dual. One possible explanation
158

fiA CP Approach Queueing Control Problem

Dual slower fact assign variables (via propagation)
models. particular, order represent switching policy, Dual
assign + 1 wj variables addition N + 1 ki variables, usually much
larger N .
8.3 Performance PSums-P1 Hybrid
Experimental results demonstrate PSums-P1 Hybrid finds good solutions quickly
able prove optimality large number instances. noted however,
synergy results combination: number instances optimality
proved increase run-times decrease. Moreover, hybrid model
finds best-known solution test instances simply cases
PSums model heuristic able so. new best solutions
obtained using PSums-P1 Hybrid solve problem. appears starting
PSums model solution found heuristic lead significant
increase amount propagation. Also, fact heuristic finds good-quality
solution improve overall performance since, search used, placing
constraint Wq requires solutions better quality little effect
domains decision variables. observations imply order create
effective model problem, one would need improve back-propagation
adding new constraints reformulating existing ones. back-propagation improved,
good-quality heuristic solution may result better performance hybrid approach.
8.4 Perspectives
CP methods developed are, ways, non-standard. common
approaches faced poor results three basic CP models (without shaving)
would create better models, develop global constraint could represent
efficiently reason relevant sub-structure problem, and/or invent
sophisticated variable- value-ordering heuristics. Shaving procedural
technique must customized exploit particular problem structure. contrast,
better model creation global constraint in-line declarative goals
CP. decision investigate shaving arose recognition need
tightly link optimization function decision variables clear structure
problem appeared proved ideal shaving.
believe scope better models novel global constraints. Modelling
problem P1 using CP straightforward formulation proposed Berman
et al. contains expressions Equation (6), upper lower limits sum
auxiliary variables decision variables. constraints typical problems
usually modelled solved CP appear existing global constraints
could used facilitate approach. spite issues, models
demonstrate CP flexible enough support queueing constraints. However,
believe likely generalized application CP solve larger class queueing
control problems require global constraints specific expressions commonly occurring
queueing theory. Given back-propagation analysis fact problem
159

fiTerekhov & Beck

find prove optimality, doubtful that, P1 , sophisticated search heuristics
perform significantly better simple heuristics.
first time CP used solve queueing control
problem first time instances P1 provably solved optimality,
work paper viewed somewhat narrow: demonstration
particular queueing control problem solved constraint programming techniques.
work immediately deliver solutions general problems, however,
believe open number directions inquiry problems.
1. appears standard method within queueing theory address queueing control optimization problems. first application opens issue whether
CP become approach choice problems.
2. noted Section 1, increasing interest incorporating reasoning
uncertainty CP-based problem solving. Queueing theory provide formulations allow direct calculation stochastic quantities based expectation.
challenge CP identify common sub-structures formulations
develop modelling, inference, search techniques exploit them.
3. Challenging scheduling problems, staff rostering call centres (Cezik &
LEcuyer, 2008), consist queues well rich resource temporal constraints
(e.g., multiple resource requirements, alternative resources different speeds, task
deadlines, precedence relations tasks). believe integration
CP queueing theory could prove promising approach problems.
4. ability reason resource allocation uncertainty important
component definitions intelligent behaviour bounded rationality (Simon,
1997). cannot claim made significant contribution direction,
perhaps ideas queueing theory serve inspiration contributions
future.

9. Related Work Possible Extensions
Several papers exist deal similar types problems one considered here.
example, Berman Larson (2004) study problem switching workers
two rooms retail facility customers front room divided two
categories, shopping store checkout. Palmer Mitrani
(2004) consider problem switching computational servers different types
jobs randomness user demand may lead unequal utilization resources.
Batta, Berman Wang (2007) study problem assigning cross-trained customer
service representatives different types calls call centre, depending estimated
demand patterns type call. three papers provide examples problems
CP could prove useful approach. Investigating CP solutions problems
therefore one possible direction future work.
work may also include looking extensions problem discussed
paper. example, may consider realistic problem resource
constraints one rooms, workers varying productivity.
160

fiA CP Approach Queueing Control Problem

Another direction work improvement proposed models. particular, models, especially PSums, constraints variable exponents.
One idea improving performance constraints explicitly represent
differences switching points (i.e., ki+1 ki ) variables.5 Another idea investigate model based logarithms probabilities rather probabilities
themselves. Additionally, ways increasing amount back-propagation need
examined.
goal paper demonstrate applicability constraint programming
solving particular queueing control problem. main direction future work is,
therefore, explore possibility integrating CP queueing theory
attempt address stochastic scheduling resource allocation problems. problems
likely involve complex constraints, encoding necessary stochastic information stating typical scheduling requirements task precedences resource
capacities. Combining queueing theory CP may help solving problems.

10. Conclusions
paper, constraint programming approach proposed problem finding
optimal states switch workers front room back room retail facility stochastic customer arrival service times. first work
aware examines solving stochastic queueing control problems using
constraint programming. best pure CP method proposed able prove optimality
large proportion instances within 10-minute time limit. Previously, existed
non-heuristic solution problem aside naive enumeration. result
experiments, hybridized best pure CP model heuristic proposed
problem literature. hybrid technique able achieve performance
equivalent to, better than, individual approaches alone: able
find good solutions negligible amount time due use heuristic,
able prove optimality large proportion problem instances within 10 CPU-minutes
due CP model.
paper demonstrates constraint programming good approach solving queueing control optimization problem. queueing problems optimality
important heuristics perform well, CP may prove effective methodology.

Appendix A. Constraint Derivations
section, derivations constraints PSums model expressions
auxiliary variables constraints presented.
A.1 Closed-form Expressions PSums model
PSums model two sets probability variables, P (ki ), = 0, 1, . . . , N , probability ki customers front room, P Sums(ki ), = 0, 1, . . . , N 1,
sum probabilities two switching point variables. Balance equations
5. Thanks anonymous reviewer suggestion.

161

fiTerekhov & Beck

explicitly stated model. However, expressions P (ki ) P Sums(ki ) derived way balance equations satisfied. technique used
derivations similar used Berman et al. (2005) simplify calculation
probabilities.
Consider balance equation P (j) = P (j + 1)i, true j = ki1 , ki1 +
1, . . . , ki 1 {1, 2 . . . , N }. particular, subset balance equations
P (ki1 ) = P (ki1 + 1)i
P (ki1 + 1) = P (ki1 + 2)i
..
.
P (ki 1) = P (ki )i.
equations imply following expressions:
P (ki1 )

P (ki1 + 1)


= P (ki1 + 1)

(23)

= P (ki1 + 2)
..
.

P (ki 1)


= P (ki ).

Combining together, get P (ki ) =
P (ki+1 ) =




(i + 1)

ki+1 ki

ki ki1



P (ki1 ) 1 N ,

P (ki ), {0, 1, . . . , N 1}.

(24)

equation included PSums model previously stated Equation
(15).

Similarly, Equation (23), see P (ki1 + 1) =
P (ki1 ) 1
N ,
P (ki + 1) =


P (ki ), {0, 1, . . . , N }.
(i + 1)

(25)

Using Equation (25), expression P Sums(ki ), sum probabilities P (j) j
ki ki+1 1, derived follows:
ki+1 1

P Sums(ki ) =

X

P (j)

j=ki

= P (ki ) + P (ki + 1) + P (ki + 2) + . . . + P (ki+1 1)

2


= P (ki ) + P (ki )
+ P (ki )
(i + 1)
(i + 1)
162

fiA CP Approach Queueing Control Problem


ki+1 ki 1

+ . . . + P (ki )
(i + 1)
"


2
ki+1 ki 1 #



+
+ ... +
= P (ki ) 1 +
(i + 1)
(i + 1)
(i + 1)

ki+1 ki





1


(i + 1)


P (ki )
6= 1
(i+1)

=
1

(i + 1)






P (ki )(ki+1 ki )
otherwise.

(26)


+
last step derivation based observation expression [1 + (i+1)

2

ki+1 ki 1



+ . . . + (i+1)
] geometric series common ratio (i+1)
.
(i+1)

1, expression simply sum ki+1 ki ones. expression
(i+1)
P Sums(ki ) previously stated Equation (14).

A.1.1 Expected Number Workers Constraint
F expressed terms P (ki ) P Sums(ki ) using following sequence steps:
F

=

ki
X

N
X

iP (j)

i=1 j=ki1 +1

=

=

N
X

i=1
N
X

[P (ki1 + 1) + P (ki1 + 2) + . . . + P (ki 1) + P (ki )]
[P Sums(ki1 ) P (ki1 ) + P (ki )].

(27)

i=1

A.1.2 Expected Number Customers Constraints
equation L derived similar manner:
L =

kN
X

jP (j)

j=k0

=

kX
1 1
j=k0

jP (j) +

kX
2 1

jP (j) + . . . +

j=k1

kX
N 1

jP (j) + kN P (kN )

j=kN1

= L(k0 ) + L(k1 ) + . . . + L(kN 1 ) + kN P (kN )
=

N
1
X

L(ki ) + kN P (kN )

(28)

i=0

163

fiTerekhov & Beck


L(ki ) = ki P (ki ) + (ki + 1)P (ki + 1) + (ki + 2)P (ki + 2) + . . . + (ki+1 1)P (ki+1 1)
= ki P (ki ) + ki P (ki + 1) + ki P (ki + 2) + . . . + ki P (ki+1 1) + P (ki + 1)
+ 2P (ki + 2) + . . . + (ki+1 ki 1)P (ki+1 1)
= ki [P (ki ) + P (ki + 1) + P (ki + 2) + . . . + P (ki+1 1)] + P (ki + 1)
+ 2P (ki + 2) + . . . + (ki+1 ki 1)P (ki+1 1)

2


+ 2P (ki )
= ki P Sums(ki ) + P (ki )
(i + 1)
(i + 1)
ki+1 ki 1


+ . . . + (ki+1 ki + 1)P (ki )
(i + 1)

= ki P Sums(ki ) + P (ki )
(i + 1)
"

2


1+2
+ ...
+3
(i + 1)
(i + 1)

ki+1 ki 2 #

+ (ki+1 ki 1)P (ki )
(i + 1)

= ki P Sums(ki ) + P (ki )
(i + 1)

ki+1 ki 1

X

n=0




n
(i + 1)

n1

(29)


= ki P Sums(ki ) + P (ki )
(i + 1)


ki+1 ki 1
ki+1 ki



(ki ki+1 ) + (i+1)
(ki+1 ki 1) + 1
(i+1)

.
2


1 (i+1)
A.2 Auxiliary Variables
constraint programming models contain Equation (13) (restated Equation
(30)) defining Sum(ki ) variables, necessary expressing auxiliary
constraint ensures balance equations unique solution. validity
equation proved following derivation, uses formula sum
finite geometric series last step:
Sum(ki ) =

ki
X

j

j=ki1 +1

ki1 +1k0 ki1 +1ki1
1

Xi


ki1 +2k0 ki1 +2ki1
ki k0 ki ki1


1
1
+
Xi + . . . +
Xi




=

164

fiA CP Approach Queueing Control Problem

ki1 k0 +1

1
= Xi


"
2 2
k1 k0 (ki1 k0 +1) ki ki1 1 #


1
1
1
+
+ ... +
1+






ki1 k0 +1 ki k
i1 1
X
1
n

= Xi



n=0



!
ki ki1






1



ki1 k0 +1 1
!






6= 1
X









1

=




ki1 k0 +1




1


Xi
(ki ki1 ) otherwise.



(30)

Acknowledgments research supported part Natural Sciences
Engineering Research Council ILOG, S.A. Thanks Nic Wilson Ken Brown
discussions comments work, Tom Goguen careful proofreading
final copy. preliminary version parts work previously published
(Terekhov & Beck, 2007).

References
Baptiste, P., Le Pape, C., & Nuijten, W. (2001). Constraint-based Scheduling. Kluwer
Academic Publishers.
Batta, R., Berman, O., & Wang, Q. (2007). Balancing staffing switching costs
service center flexible servers. European Journal Operational Research, 177,
924938.
Beck, J. C., & Prestwich, S. (2004). Exploiting dominance three symmetric problems.
Fourth International Workshop Symmetry Constraint Satisfaction Problems.
Beck, J. C., & Wilson, N. (2007). Proactive algorithms job shop schedulng probabilistic durations. Journal Artificial Intelligence Research, 28, 183232.
Berman, O., & Larson, R. (2004). queueing control model retail services back
room operations cross-trained workers. Computers Operations Research,
31 (2), 201222.
Berman, O., Wang, J., & Sapna, K. P. (2005). Optimal management cross-trained workers
services negligible switching costs. European Journal Operational Research,
167 (2), 349369.
165

fiTerekhov & Beck

Brown, K. N., & Miguel, I. (2006). Uncertainty change. Rossi, F., van Beek, P.,
& Walsh, T. (Eds.), Handbook Constraint Programming, chap. 21, pp. 731760.
Elsevier.
Caseau, Y., & Laburthe, F. (1996). Cumulative scheduling task intervals. Proceedings Joint International Conference Symposium Logic Programming,
pp. 363377. MIT Press.
Cezik, M. T., & LEcuyer, P. (2008). Staffing multiskill call centers via linear programming
simulation. Management Science, 54 (2), 310323.
Demassey, S., Artigues, C., & Michelon, P. (2005). Constraint-propagation-based cutting
planes: application resource-constrained project scheduling problem. INFORMS Journal Computing, 17 (1), 5265.
Fox, M. S. (1983). Constraint-Directed Search: Case Study Job-Shop Scheduling. Ph.D.
thesis, Carnegie Mellon University, Intelligent Systems Laboratory, Robotics Institute, Pittsburgh, PA. CMU-RI-TR-85-7.
Gross, D., & Harris, C. (1998). Fundamentals Queueing Theory. John Wiley & Sons,
Inc.
Hnich, B., Smith, B. M., & Walsh, T. (2004). Dual modelling permutation injection
problems. Journal Artificial Intelligence Research, 21, 357391.
Martin, P., & Shmoys, D. B. (1996). new approach computing optimal schedules
job shop scheduling problem. Proceedings Fifth Conference Integer
Programming Combinatorial Optimization, pp. 389403.
Palmer, J., & Mitrani, I. (2004). Optimal server allocation reconfigurable clusters
multiple job types. Proceedings International Conference Computational
Science Applications (ICCSA04), pp. 7686.
Policella, N., Smith, S. F., Cesta, A., & Oddi, A. (2004). Generating robust schedules
temporal flexibility. Proceedings Fourteenth International Conference
Automated Planning Scheduling (ICAPS04), pp. 209218.
Simon, H. A. (1997). Models Bounded Rationality, Vol. 3. MIT Press.
Smith, B. M. (2005). Modelling constraint programming. Lecture Notes
First International Summer School Constraint Programming. Available at:
http://www.math.unipd.it/frossi/cp-school/.
Smith, B. M. (2006). Modelling. Rossi, F., van Beek, P., & Walsh, T. (Eds.), Handbook
Constraint Programming, chap. 11, pp. 377406. Elsevier.
Solver (2006). ILOG Scheduler 6.2 Users Manual Reference Manual. ILOG, S.A.
Sutton, A. M., Howe, A. E., & Whitley, L. D. (2007). Using adaptive priority weighting
direct search probabilistic scheduling. Proceedings Seventeenth International Conference Automated Planning Scheduling, pp. 320327.
Tadj, L., & Choudhury, G. (2005). Optimal design control queues. TOP, 13 (2),
359412.
166

fiA CP Approach Queueing Control Problem

Tarim, S. A., Manandhar, S., & Walsh, T. (2006). Stochastic constraint programming:
scenario-based approach. Constraints, 11 (1), 5380.
Tarim, S. A., & Miguel, I. (2005). hybrid Benders decomposition method solving
stochastic constraint programs linear recourse.. Joint ERCIM/CoLogNET
International Workshop Constraint Solving Constraint Logic Programming,
pp. 133148.
Terekhov, D. (2007). Solving queueing design control problems constraint programming. Masters thesis, Department Mechanical Industrial Engineering,
University Toronto.
Terekhov, D., & Beck, J. C. (2007). Solving stochastic queueing control problem
constraint programming. Proceedings Fourth International Conference
Integration AI Techniques Constraint Programming Combinatorial
Optimization Problems (CPAIOR07), pp. 303317. Springer-Verlag.
van Dongen, M. R. C. (2006). Beyond singleton arc consistency. Proceedings
Seventeenth European Conference Artificial Intelligence (ECAI06), pp. 163167.
Walsh, T. (2002). Stochastic constraint programming. Proceedings Fifteenth
European Conference Artificial Intelligence, pp. 111115.

167

fiJournal Artificial Intelligence Research 32 (2008) 37-94

Submitted 08/07; published 05/08

Extended RDF Semantic Foundation
Rule Markup Languages
Anastasia Analyti

analyti@ics.forth.gr

Institute Computer Science, FORTH-ICS, Crete, Greece

Grigoris Antoniou

antoniou@ics.forth.gr

Institute Computer Science, FORTH-ICS, Crete, Greece
Department Computer Science, University Crete, Greece

Carlos Viegas Damasio

cd@di.fct.unl.pt

Centro de Inteligencia Artificial, Universidade Nova de Lisboa,
Caparica, Portugal

Gerd Wagner

G.Wagner@tu-cottbus.de

Institute Informatics, Brandenburg University
Technology Cottbus, Germany

Abstract
Ontologies automated reasoning building blocks Semantic Web initiative. Derivation rules included ontology define derived concepts, based
base concepts. example, rules allow define extension class property, based
complex relation extensions classes properties.
hand, inclusion negative information form negation-asfailure explicit negative information also needed enable various forms reasoning.
paper, extend RDF graphs weak strong negation, well derivation
rules. ERDF stable model semantics extended framework (Extended RDF)
defined, extending RDF(S) semantics. distinctive feature theory, based
Partial Logic, truth falsity extensions properties classes
considered, allowing truth value gaps. framework supports closed-world
open-world reasoning explicit representation particular closed-world assumptions ERDF ontological categories total properties total classes.

1. Introduction
idea Semantic Web describe meaning web data way suitable
automated reasoning. means descriptive data (meta-data) machine readable form stored web used reasoning. Due distributed
world-wide nature, Web creates new problems knowledge representation research.
Berners-Lee (1998) identifies following fundamental theoretical problems: negation
contradictions, open-world versus closed-world assumptions, rule systems Semantic Web. time being, first two issues circumvented discarding
facilities introduce them, namely negation closed-world assumptions. Though
web ontology language OWL (McGuinness & van Harmelen, 2004), based Description Logics (DLs) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003),
includes form classical negation class complements, form limited.
c
2008
AI Access Foundation. rights reserved.

fiAnalyti, Antoniou, Damasio, & Wagner

because, achieve decidability, classes formed based specific class constructors
negation properties fully considered. Rules constitute next layer
ontology languages Semantic Web and, contrast DL, allow arbitrary interaction
variables body rules. widely recognized need rules Semantic Web, demonstrated Rule Markup Initiative1 , restarted discussion
fundamentals closed-world reasoning appropriate mechanisms implement
rule systems.
RDF(S)2 recommendation (Klyne & Carroll, 2004; Hayes, 2004) provides basic constructs defining web ontologies solid ground discuss issues.
RDF(S) special predicate logical language restricted existentially quantified conjunctions atomic formulas, involving binary predicates only. Due purpose,
RDF(S) number special features distinguish traditional logic languages:
1. uses special jargon, things universe discourse called resources, types called classes, binary predicates called properties. Like
binary relations set theory, properties domain range. Resources
classified help property rdf :type (for stating resource type
c, c class).
2. distinguishes special sort resources, called literal values, denotations
lexical representations strings, numbers, dates, basic datatypes.
3. Properties resources, is, properties also elements universe discourse. Consequently, possible state properties properties, i.e., make statements predicates.
4. resources, except anonymous ones literal values, named help
globally unique reference schema, called Uniform Resource Identifier (URI)3 ,
developed Web.
5. RDF(S) comes non-standard model-theoretic semantics developed Pat Hayes
basis idea Christopher Menzel, allows self-application without
violating axiom foundation. example provable sentence stating
rdfs:Class, class classes, instance itself.
However, RDF(S) support negation rules. Wagner (1991) argues
database, knowledge representation system, needs two kinds negation, namely weak
negation (expressing negation-as-failure non-truth) strong negation (expressing
explicit negative information falsity) able deal partial information.
subsequent paper, Wagner (2003) makes also point Semantic Web,
framework knowledge representation general. present paper, make
argument Semantic Web language RDF show extended
accommodate two negations Partial Logic (Herre, Jaspars, & Wagner, 1999),
well derivation rules. call new language Extended RDF denote ERDF.
1. http://www.ruleml.org/
2. RDF(S) stands Resource Description Framework (Schema).
3. http://gbiv.com/protocols/uri/rfc/rfc3986.html

38

fiExtended RDF Semantic Foundation Rule Markup Languages

model-theoretic semantics ERDF, called ERDF stable model semantics, developed
based Partial Logic (Herre et al., 1999).
Partial Logic, relating strong weak negation interpretation level allows
distinguish four categories properties classes. Partial properties properties p
may truth-value gaps truth-value clashes, p(x, y) possibly neither true
false, true false. Total properties properties p satisfy totalness,
p(x, y) true false (but possibly both). Coherent properties properties p
satisfy coherence, p(x, y) cannot true false. Classical properties total
coherent properties. classical properties p, classical logic law applies: p(x, y)
either true false. Partial, total, coherent, classical classes c defined similarly,
replacing p(x, y) rdf :type(x, c).
Partial logic also allows distinguish properties (and classes) completely represented knowledge base not. classification
property completely represented owner knowledge base:
owner must know properties complete information
not. Clearly, case completely represented (closed ) property p, entailment
p(x, y) allows derive p(x, y), underlying completeness assumption also
called Closed-World Assumption (CWA) AI literature.
completeness assumption closing partial property p default may
expressed ERDF means rule p(?x, ?y) p(?x, ?y) partial class
c, means rule rdf :type(?x, c) rdf :type(?x, c). derivation rules
called default closure rules. case total property p, default closure rules
applicable. because, considered interpretations satisfy p(x, y)
rest p(x, y)4 , preventing preferential entailment p(x, y). Thus, total
properties, Open-World Assumption (OWA) applies. Similarly first-order-logic,
order infer negated statements total properties, explicit negative information
supplied, along ordinary (positive) information.
example, consider ERDF knowledge base KB contains facts:
interestedIn(Anastasia, SemanticWeb)

interestedIn(Grigoris, Robotics)

indicating Anastasia interested SemanticWeb area Grigoris interested
Robotics area. Then, statement interestedIn(Anastasia, Robotics) satisfied
single intended model KB . Thus, KB entails interestedIn(Anastasia, Robotics).
Assume previous list areas interest complete Anastasia
Grigoris. Then, add knowledge base KB statement:
rdf :type(interestedIn, erdf :TotalProperty)
indicating interestedIn total property. case, open-world assumption made interestedIn KB entail interestedIn(Anastasia, Robotics),
longer. particular, intended model revised KB satisfies
interestedIn(Anastasia, Robotics). course, known Anastasia interested
Robotics interestedIn(Anastasia, Robotics) added KB .
Assume add KB following facts:
4. total properties p, Law Excluded Middle p(x, y)p(x, y) applies.

39

fiAnalyti, Antoniou, Damasio, & Wagner

hasCar (Anastasia, Suzuki )

hasCar (Grigoris, Volvo)

assume KB complete knowledge property hasCar , far concerns
elements Herbrand Universe KB . Then, default closure rule hasCar (?x , ?y)
hasCar (?x , ?y) safely added KB . result, hasCar (Anastasia, Volvo)
satisfied intended models KB . Thus, KB entails hasCar (Anastasia, Volvo).
previous example shows need supporting closed-world open-world
reasoning framework. Damasio et al. (2006) Analyti et al. (2004) provide
examples arguments need. Unfortunately, classical logic thus also
OWL support open-world reasoning.
Specifically, paper:
1. extend RDF graphs ERDF graphs inclusion strong negation,
ERDF ontologies (or ERDF knowledge bases) inclusion general
derivation rules. ERDF graphs allow express existential positive negative
information, whereas general derivation rules allow inferences based formulas built
using connectives , , , , quantifiers , .
2. extend vocabulary RDF(S) terms erdf :TotalProperty
erdf :TotalClass, representing metaclasses total properties total classes,
open-world assumption applies.
3. extend RDFS interpretations ERDF interpretations including truth
falsity extensions properties classes. Particularly, consider coherent
ERDF interpretations (imposing coherence properties). Thus, paper,
total properties classes become synonymous classical properties classes.
4. extend RDF graphs ERDF formulas built positive triples, using
connectives , , , , quantifiers , . Then, define ERDF
entailment two ERDF formulas, extending RDFS entailment RDF
graphs.
5. define ERDF models, Herbrand interpretations, minimal Herbrand
models ERDF ontology. Since minimal Herbrand models ERDF
ontology intended, define stable models ERDF ontology. definition
stable model based intuition that:
(a) assertions stating property p class c total accepted,
ontology contains direct support form acceptable
rule sequence,
(b) assertions []p(s, o) []rdf :type(o, c) accepted, (i) ontology contains direct support form acceptable rule
sequence, (ii) property p class c total, respectively.
6. show stable model entailment ERDF ontologies extends ERDF entailment
ERDF graphs, thus also extends RDFS entailment RDF graphs. Moreover, show properties total, (boolean) Herbrand model reasoning
stable model reasoning coincide. case, make open-world assumption
properties classes.
40

fiExtended RDF Semantic Foundation Rule Markup Languages

distinctive feature developed framework respect Partial Logic (Herre
et al., 1999) properties classes declared total selective basis,
extending RDF(S) new built-in classes providing support respective ontological categories. contrast, Partial Logic (Herre et al., 1999), choice partial
total taken complete set predicates. Thus, approach presented
is, respect, flexible general.
work extends conference paper (Analyti, Antoniou, Damasio, & Wagner, 2005)
(i) considering full RDFS model, (ii) providing detailed characterization
properties ERDF interpretations/models, Herbrand interpretations/models, finally
ERDF stable models, (iii) discussing decidability issues, (iv) providing formal proofs
lemmas propositions.
rest paper organized follows: Section 2, extend RDF graphs
ERDF graphs ERDF formulas. Section 3 defines ERDF interpretations ERDF
entailment. show ERDF entailment extends RDFS entailment. Section 4,
define ERDF ontologies Herbrand models ERDF ontology. Section 5,
define stable models ERDF ontology. Section 6 defines stable model entailment,
showing extends ERDF entailment. Section 7, provide brief sketch
ERDF/XML syntax. Decidability issues ERDF stable model semantics discussed
Section 8. Section 9 shows developed ERDF model theory seen
Tarski-style model theory. Section 10 reviews related work Section 11 concludes
paper, including future work. main definitions RDF(S) semantics reviewed
Appendix A. Appendix B includes proofs lemmas propositions, presented
paper.

2. Extending RDF Graphs Negative Information
section, extend RDF graphs ERDF graphs, adding strong negation. Moreover, extend RDF graphs ERDF formulas, built positive ERDF triples,
connectives , , , , , quantifiers , .
According RDF concepts (Klyne & Carroll, 2004; Hayes, 2004), URI references
used globally unique names web resources. RDF URI reference Unicode string represents absolute URI (with optional fragment identifier).
may represented qualified name, colon-separated two-part string consisting namespace prefix (an abbreviated name namespace URI) local
name. example, given namespace prefix ex defined stand namespace URI http://www.example.org/, qualified name ex:Riesling (which stands
http://www.example.org/Riesling) URI reference.
plain literal string s, sequence Unicode characters, pair
string language tag t, denoted s@t. typed literal pair string
datatype URI reference d, denoted sd. example, 27xsd:integer
typed literal.
(Web) vocabulary V set URI references and/or literals (plain typed).
denote set URI references URI, set plain literals PL, set
typed literals L, set literals LIT . holds: URI LIT = .
41

fiAnalyti, Antoniou, Damasio, & Wagner

formalization, consider set Var variable symbols, sets Var ,
URI, LIT pairwise disjoint. main text, variable symbols explicitly indicated,
examples, variable symbols prefixed question mark symbol ?.
RDF triple (Klyne & Carroll, 2004; Hayes, 2004) triple p o.,
URI Var , p URI, URI LIT Var , expressing subject related
object property p. RDF graph set RDF triples.
variable symbols appearing RDF graph called blank nodes, are, intuitively,
existentially quantified variables. paper, denote RDF triple p o.
p(s, o). extend notion RDF triple allow positive negative
information.
Definition 2.1 (ERDF triple) Let V vocabulary. positive ERDF triple V
(also called ERDF sentence atom) expression form p(s, o), s, V Var
called subject 5 object, respectively, p V URI called predicate property.
negative ERDF triple V strong negation p(s, o) positive ERDF triple
p(s, o) V . ERDF triple V (also called ERDF sentence literal ) positive
negative ERDF triple V .
example, ex:likes(ex:Gerd , ex:Riesling) positive ERDF triple, expressing
Gerd likes Riesling, ex:likes(ex:Carlos, ex:Riesling) negative ERDF triple, expressing Carlos dislikes Riesling. Note RDF triple positive ERDF
triple constraint subject triple literal. example,
ex:denotationOf (Grigoris, ex:Grigoris) valid ERDF triple valid RDF triple.
choice allowing literals appearing subject position based intuition
case naturally appear knowledge representation (as previous example). Prudhommeaux & Seaborne (2008) de Bruijn et al. (2005) also consider literals
subject position RDF triples.
Based notion ERDF triple, define ERDF graphs ERDF formulas,
follows:
Definition 2.2 (ERDF graph) ERDF graph G set ERDF triples
vocabulary V . denote variables appearing G Var (G), set URI
references literals appearing G VG .
Note RDF graph set RDF triples (Klyne & Carroll, 2004; Hayes, 2004),
RDF graph also ERDF graph.
Definition 2.3 (ERDF formula) Let V vocabulary. consider logical factors
{, , , , , , }, , , called strong negation, weak negation,
material implication, respectively. denote L(V ) smallest set contains
positive ERDF triples V closed respect following conditions:
F, G L(V ) {F, F, F G, F G, F G, xF, xF } L(V ), x Var .
ERDF formula V element L(V ). denote set variables appearing
5. Opposed pure RDF (Klyne & Carroll, 2004), allow literals subject position ERDF
triple.

42

fiExtended RDF Semantic Foundation Rule Markup Languages

F Var (F ), set free variables6 appearing F FVar (F ). Moreover,
denote set URI references literals appearing F VF .
example, let:
F = ?x ?y (rdf :type(?x, ex:Person) ex:hasChild (?y, ?x)) rdf :type(?z, ex:Person)
Then, F ERDF formula vocabulary V = {rdf :type, ex:Person, ex:hasChild }
Var (F ) = {?x, ?y, ?z} FVar (F ) = {?z}.
denote sublanguages L(V ) formed means subset logical
factors, L(V |S). example, L(V |{}) denotes set (positive negative) ERDF
triples V .

3. ERDF Interpretations
section, extend RDF(S) semantics allowing partial properties classes.
particular, define ERDF interpretations satisfaction ERDF formula, based
notion partial interpretation.
3.1 Partial Interpretations
define partial interpretation extension simple interpretation (Hayes, 2004),
property associated truth extension also falsity
extension allowing partial properties. notation P(S), set, denotes
powerset S.
Definition 3.1 (Partial interpretation) partial interpretation vocabulary V
consists of:
non-empty set resources ResI , called domain universe I.
set properties P ropI .
vocabulary interpretation mapping IV 7 : V URI ResI P ropI .
property-truth extension mapping PT : P ropI P(ResI ResI ).
property-falsity extension mapping PF : P ropI P(ResI ResI ).
mapping ILI : V L ResI .
set literal values LV ResI , contains V PL.

define mapping: : V ResI P ropI , called denotation, that:
I(x) = IV (x), x V URI.
I(x) = x, x V PL.
I(x) = ILI (x), x V L.
6. Without loss generality, assume variable cannot free bound occurrences
F , one bound occurrence.
7. symbol IV , V stands Vocabulary.

43

fiAnalyti, Antoniou, Damasio, & Wagner

Note truth falsity extensions property p according partial interpretation I, P TI (p) P FI (p), sets pairs hsubject, objecti resources.
example, let:
V = {ex:Carlos, ex:Grigoris, ex:Riesling, ex:likes, ex:denotationOf , Grigorisxsd:string}

consider structure consists of:
set resources ResI = {C, G, R, l, d, Grigoris}.
set properties P ropI = {l, d}.
vocabulary interpretation mapping IV : V URI ResI P ropI that:
IV (ex:Carlos) = C, IV (ex:Grigoris) = G, IV (ex:Riesling) = R, IV (ex:likes) = l,
IV (ex:denotationOf ) = d.
property-truth extension mapping PT : P ropI P(ResI ResI ) that:
P TI (d) = {hGrigoris, Gi}.
property-falsity extension mapping PF : P ropI P(ResI ResI ) that:
P FI (l) = {hC, Ri}.
mapping ILI : V L ResI that: ILI (Grigorisxsd :string) = Grigoris.
set literal values LV = {Grigoris}.

easy see partial interpretation V , expressing that: (i) Grigoris
denotation Grigoris (ii) Carlos dislikes Riesling.
Definition 3.2 (Coherent partial interpretation) partial interpretation vocabulary V coherent iff x P ropI , PT (x) PF (x) = .
Coherent partial interpretations enforce constraint pair resources cannot
belong truth falsity extensions property (i.e., properties coherent). Intuitively, means ERDF triple cannot true false.
Continuing previous example, note coherent partial interpretation.
Consider partial interpretation J exactly I, except also holds:
P TJ (l) = {hC, Ri} (expressing Carlos likes Riesling). Then, hC, Ri belongs
truth falsity extension l (i.e., hC, Ri P TJ (l) P FJ (l)). Thus, J coherent.
define satisfaction ERDF formula w.r.t. partial interpretation, need first
following auxiliary definition.
Definition 3.3 (Composition partial interpretation valuation) Let
partial interpretation vocabulary V let v partial function v : Var ResI
(called valuation). define: (i) [I + v](x) = v(x), x Var , (ii) [I + v](x) = I(x),
x V .
Definition 3.4 (Satisfaction ERDF formula w.r.t. partial interpretation
valuation) Let F, G ERDF formulas let partial interpretation
vocabulary V . Additionally, let v mapping v : Var (F ) ResI .
F = p(s, o) I, v |= F iff p V URI, s, V Var , I(p) P ropI ,
h[I + v](s), [I + v](o)i PT (I(p)).
44

fiExtended RDF Semantic Foundation Rule Markup Languages

F = p(s, o) I, v |= F iff p V URI, s, V Var , I(p) P ropI ,
h[I + v](s), [I + v](o)i PF (I(p)).
F = G I, v |= F iff VG V I, v 6|= G.
F = F1 F2 I, v |= F iff I, v |= F1 I, v |= F2 .
F = F1 F2 I, v |= F iff I, v |= F1 I, v |= F2 .
F = F1 F2 then8 I, v |= F iff I, v |= F1 F2 .
F = x G I, v |= F iff exists mapping u : Var (G) ResI u(y) = v(y),
Var (G) {x}, I, u |= G.
F = x G I, v |= F iff mappings u : Var (G) ResI u(y) = v(y),
Var (G) {x}, holds I, u |= G.
cases ERDF formulas treated following DeMorgan-style rewrite rules
expressing falsification compound ERDF formulas:
(F G) F G, (F G) F G, (F ) F, ( F ) F 9 ,
(x F ) x F, (x F ) x F, (F G) F G.

Continuing previous example, let v : {?x, ?y, ?z} ResI v(?x) = C,
v(?y) = R, v(?z) = G. holds:
I, v |= ex:likes(?x, ?y) ex:denotationOf (Grigorisxsd:string, ?z).
Definition 3.5 (Satisfaction ERDF formula w.r.t. partial interpretation)
Let F ERDF formula let partial interpretation vocabulary V . say
satisfies F , denoted |= F , iff every mapping v : Var (F ) ResI , holds
I, v |= F.
Continuing previous example, |= ?x ex:likes(ex:Carlos, ?x).
define ERDF graph satisfaction, extending satisfaction RDF graph
(Hayes, 2004) (see also Appendix A).
Definition 3.6 (Satisfaction ERDF graph w.r.t. partial interpretation) Let
G ERDF graph let partial interpretation vocabulary V . Let v
mapping v : Var (G) ResI . define:
I, v |=GRAPH G iff G, I, v |= t.
satisfies ERDF graph G, denoted |=GRAPH G, iff exists mapping
v : Var (G) ResI I, v |=GRAPH G.
Intuitively, ERDF graph G represents existentially quantified conjunction
ERDF triples. Specifically, let G = {t1 , ..., tn } ERDF graph, let Var (G) =
{x1 , ..., xk }. Then, G represents ERDF formula formula(G) = ?x1 , ..., ?xk t1 ... tn .
shown following lemma.
8. Material implication logical relationship two ERDF formulas either first
non-true second true.
9. transformation expresses false F hold F holds.

45

fiAnalyti, Antoniou, Damasio, & Wagner

Lemma 3.1 Let G ERDF graph let partial interpretation vocabulary
V . holds: |=GRAPH G iff |= formula(G).
Following RDF terminology (Klyne & Carroll, 2004), variables ERDF
graph also called blank nodes intuitively denote anonymous web resources.
example, consider ERDF graph:
G = {rdf :type(?x, ex:EuropeanCountry), rdf :type(?x, ex:EU member)}.
Then, G represents ERDF formula formula(G) =
?x (rdf :type(?x, ex:EuropeanCountry) rdf :type(?x, ex:EU member)),
expressing European country European Union member.
Notational Convention: Let G ERDF graph, let partial interpretation
vocabulary V , let v mapping v : Var (G) ResI . Due Lemma 3.1,
write (by abuse notation) I, v |= G |= G instead I, v |=GRAPH G
|=GRAPH G, respectively.
3.2 ERDF Interpretations Entailment
subsection, define ERDF interpretations entailment extension RDFS
interpretations entailment (Hayes, 2004). First, define vocabularies RDF,
RDFS, ERDF.
vocabulary RDF, VRDF , set URI references rdf : namespace (Hayes,
2004), shown Table 1. vocabulary RDFS, VRDF , set URI references
rdfs: namespace (Hayes, 2004), shown Table 1. vocabulary ERDF , VERDF ,
set URI references erdf : namespace. Specifically, set ERDF predefined
classes CERDF = {erdf :TotalClass, erdf :TotalProperty}. define VERDF = CERDF .
Intuitively, instances metaclass erdf :TotalClass classes c satisfy totalness,
meaning resource belongs truth falsity extension c. Similarly, instances
metaclass erdf :TotalProperty properties p satisfy totalness, meaning
pair resources belongs truth falsity extension p.
ready define ERDF interpretation vocabulary V extension
RDFS interpretation (Hayes, 2004) (see also Appendix A), property
class associated truth extension also falsity extension, allowing
partial properties partial classes. Additionally, ERDF interpretation gives
special semantics terms ERDF vocabulary.
Definition 3.7 (ERDF interpretation) ERDF interpretation vocabulary V
partial interpretation V VRDF VRDF VERDF , extended new ontological
categories ClsI ResI classes, TCls ClsI total classes, TProp P ropI
total properties, well class-truth extension mapping CT : ClsI P(ResI ),
class-falsity extension mapping CF : ClsI P(ResI ), that:
1. x CT (y) iff hx, yi PT (I(rdf :type)),
x CF (y) iff hx, yi PF (I(rdf :type)).
46

fiExtended RDF Semantic Foundation Rule Markup Languages

VRDF
rdf :type
rdf :Property
rdf :XMLLiteral
rdf :nil
rdf :List
rdf :Statement
rdf :subject
rdf :predicate
rdf :object
rdf :first
rdf :rest
rdf :Seq
rdf :Bag
rdf :Alt
rdf : i, {1, 2, ...}
rdf :value

VRDF
rdfs:domain
rdfs:range
rdfs:Resource
rdfs:Literal
rdfs:Datatype
rdfs:Class
rdfs:subClassOf
rdfs:subPropertyOf
rdfs:member
rdfs:Container
rdfs:ContainerMembershipProperty
rdfs:comment
rdfs:seeAlso
rdfs:isDefinedBy
rdfs:label

Table 1: vocabulary RDF RDFS
2. ontological categories defined
P ropI = CT (I(rdf :Property))
ResI = CT (I(rdfs:Resource))
TCls = CT (I(erdf :TotalClass))

follows:
ClsI = CT (I(rdfs:Class))
LV = CT (I(rdfs:Literal))
TProp = CT (I(erdf :TotalProperty)).

3. hx, yi PT (I(rdfs:domain)) hz, wi PT (x) z CT (y).
4. hx, yi PT (I(rdfs:range)) hz, wi PT (x) w CT (y).
5. x ClsI hx, I(rdfs:Resource)i PT (I(rdfs:subClassOf )).
6. hx, yi PT (I(rdfs:subClassOf )) x, ClsI , CT (x) CT (y),
CF (y) CF (x).
7. PT (I(rdfs:subClassOf )) reflexive transitive relation ClsI .
8. hx, yi PT (I(rdfs:subPropertyOf )) x, P ropI , PT (x) PT (y),
PF (y) PF (x).
9. PT (I(rdfs:subPropertyOf )) reflexive transitive relation P ropI .
10. x CT (I(rdfs:Datatype)) hx, I(rdfs:Literal)i PT (I(rdfs:subClassOf )).
11. x CT (I(rdfs:ContainerMembershipProperty))
hx, I(rdfs:member)i PT (I(rdfs:subPropertyOf )).
12. x TCls CT (x) CF (x) = ResI .
13. x TProp PT (x) PF (x) = ResI ResI .

47

fiAnalyti, Antoniou, Damasio, & Wagner

14. srdf :XMLLiteral V well-typed XML literal string,
ILI (srdf :XMLLiteral ) XML value s,
ILI (srdf :XMLLiteral ) CT (I(rdf :XMLLiteral )).
15. srdf :XMLLiteral V ill-typed XML literal string
ILI (srdf :XMLLiteral ) ResI LV ,
ILI (srdf :XMLLiteral ) CF (I(rdfs:Literal)).
16. satisfies RDF RDFS axiomatic triples (Hayes, 2004), shown Table 2 Table 3
Appendix A, respectively.
17. satisfies following triples, called ERDF axiomatic triples:
rdfs:subClassOf (erdf :TotalClass, rdfs:Class).
rdfs:subClassOf (erdf :TotalProperty, rdfs:Class).

Note RDFS intepretations (Hayes, 2004) imply two-valued interpretation
instances rdf :Property, longer case ERDF interpretations.
Specifically, let ERDF interpretation, let p CTI (I (rdf :Property)), let hx, yi
ResI ResI . may case neither hx, yi P TI (p) hx, yi P FI (p).
p(x, y) neither true false.
Semantic conditions ERDF interpretations may impose constraints truth
falsity extensions properties classes. Specifically, consider semantic condition 6
Definition 3.7 assume hx, yi PT (I(rdfs:subClassOf )). Then,
satisfy CT (x) CT (y) (as RDFS interpretation does), also CF (y) CF (x).
latter true certain resource z belong truth
extension class certain z belong truth extension class
x. Thus, falsity extension contained falsity extension x. Similar
case semantic condition 8. Semantic conditions 12 13 represent definition
total classes total properties, respectively. Semantic condition 15 expresses
denotation ill-typed XML literal literal value. Therefore (see semantic condition 2), certain contained truth extension class rdfs:Literal.
Thus, contained falsity extension class rdfs:Literal.
Let coherent ERDF interpretation vocabulary V . Since I(rdf :type) P ropI ,
holds: x ClsI , CT (x) CF (x) = . Thus, properties classes coherent
ERDF interpretations coherent.
Convention: rest document, consider coherent ERDF interpretations.
means referring ERDF interpretation, implicitly mean coherent
one. Moreover, improve readability examples, ignore example
namespace ex:.
According RDFS semantics (Hayes, 2004), source RDFS-inconsistency
appearance ill-typed XML literal l RDF graph, combination
derivation RDF triple x rdf :type rdfs:Literal. RDF RDFS entailment
rules, x blank node allocated l10 . triple called XML clash.
10. RDF(S), literals allowed subject position RDF triples, whereas blank nodes are.
reason, RDF RDFS entailment rules applied RDF graph, literal
replaced unique blank node. way inferences drawn literal value denoted
literal, without concern restriction (Hayes, 2004).

48

fiExtended RDF Semantic Foundation Rule Markup Languages

understand this, note semantic condition 3 Definition A.3 (RDF interpretation,
Appendix A), follows denotation ill-typed XML literal cannot literal
value. Now, semantic conditions 1 2 Definition A.5 (RDFS interpretation,
Appendix A), follows denotation ill-typed XML literal cannot type
rdfs:Literal. Therefore, derivation XML clash RDF graph G
application RDF RDFS entailment rules, indicates RDFS
interpretation satisfies G.
ERDF graph ERDF-inconsistent11 , due appearance
ill-typed XML literal ERDF graph (in combination semantic condition 15
Definition 3.7), also due additional semantic conditions coherent ERDF
interpretations.
example, let p, q, s, URI let G = {p(s, o), rdfs:subPropertyOf (p, q), q(s, o)}.
Then, G ERDF-inconsistent, since (coherent) ERDF interpretation satisfies G.
following proposition shows total properties total classes (coherent)
ERDF interpretations, weak negation strong negation coincide (boolean truth values).
Proposition 3.1 Let ERDF interpretation vocabulary V let V = V
VRDF VRDF VERDF . Then,
1. p, s, V I(p) TProp , holds:
|= p(s, o) iff |= p(s, o) (equivalently, |= p(s, o) p(s, o)).
2. x, c V I(c) TCls , holds:
|= rdf :type(x, c) iff |= rdf :type(x, c)
(equivalently, |= rdf :type(x, c) rdf :type(x, c)).
define ERDF entailment two ERDF formulas ERDF graphs.
Definition 3.8 (ERDF entailment) Let F, F ERDF formulas ERDF graphs.
say F ERDF-entails F (F |=ERDF F ) iff every ERDF interpretation I, |= F
|= F .
example, let:
F = ?x ?y (rdf :type(?x, Person) hasFather (?x, ?y)) rdf :type(John, Person).
Additionally, let F = ?y hasFather (John, ?y) rdf :type(hasFather , rdf :Property).
F |=ERDF F .
following proposition shows ERDF entailment extends RDFS entailment (Hayes,
2004) (see also Appendix A) RDF graphs ERDF formulas. words, ERDF
entailment upward compatible RDFS entailment.
Proposition 3.2 Let G, G RDF graphs VG VERDF = VG VERDF = .
Then, G |=RDF G iff G |=ERDF G .
easily follows Proposition 3.2 RDF graph RDFS satisfiable iff
ERDF satisfiable. Thus, RDF graph ERDF-inconsistent due XML
clash.
11. Meaning (coherent) ERDF interpretation satisfies ERDF graph.

49

fiAnalyti, Antoniou, Damasio, & Wagner

4. ERDF Ontologies & Herbrand Interpretations
section, define ERDF ontology pair ERDF graph G set
P ERDF rules. ERDF rules considered derivation rules allow us
infer ontological information based declarations G. Moreover, define
Herbrand interpretations minimal Herbrand models ERDF ontology.
Definition 4.1 (ERDF rule, ERDF program) ERDF rule r vocabulary V
expression form: G F , F L(V ) {true} called condition
G L(V |{}) {false} called conclusion. assume bound variable F
appears free G. denote set variables set free variables r Var (r)
FVar (r)12 , respectively. Additionally, write Cond(r) = F Concl(r) = G.
ERDF program P set ERDF rules vocabulary V . denote set
URI references literals appearing P VP .
Recall L(V |{}) denotes set ERDF triples V . Therefore, conclusion
ERDF rule, unless false, either positive ERDF triple p(s, o) negative
ERDF triple p(s, o).
example, consider derivation rule r:
allRelated (?P, ?Q) ?p rdf :type(?p, ?P ) ?q (rdf :type(?q, ?Q) related (?p, ?q)),

Then, r ERDF rule, indicating two classes P Q, holds allRelated (P,
Q) instances p class P , instance q class Q
holds related (p, q). Note Var (r) = {?P, ?Q, ?p, ?q} FVar (r) = {?P, ?Q}.
Cond(r) = true Var (r) = {}, rule r called ERDF fact. Concl(r) =
false, rule r called ERDF constraint. assume every partial interpretation
every function v : Var ResI , holds I, v |= true, |= true, I, v 6|= false,
6|= false.
Intuitively, ERDF ontology combination (i) ERDF graph G containing
(implicitly existentially quantified) positive negative information, (ii) ERDF
program P containing derivation rules (whose free variables implicitly universally quantified).
Definition 4.2 (ERDF ontology) ERDF ontology (or ERDF knowledge base)
pair = hG, P i, G ERDF graph P ERDF program.
following definition defines models ERDF ontology.
Definition 4.3 (Satisfaction ERDF rule ERDF ontology) Let
ERDF interpretation vocabulary V .
say satisfies ERDF rule r, denoted |= r, iff mappings
v : Var (r) ResI I, v |= Cond(r), holds I, v |= Concl(r).
say satisfies ERDF ontology = hG, P (also, model O),
denoted |= O, iff |= G |= r, r P .
12. FVar (r) = FVar (F ) FVar (G).

50

fiExtended RDF Semantic Foundation Rule Markup Languages

paper, existentially quantified variables ERDF graphs handled skolemization, syntactic transformation commonly used automatic inference systems removing existentially quantified variables.
Definition 4.4 (Skolemization ERDF graph) Let G ERDF graph.
skolemization function G 1:1 mapping skG : Var (G) URI,
x Var (G), skG (x) artificial URI, denoted G:x. set skG (Var (G)) called
Skolem vocabulary G.
skolemization G, denoted sk(G), ground ERDF graph derived G
replacing variable x Var (G) skG (x).
Intuitively, Skolem vocabulary G (that is, skG (Var (G))) contains artificial URIs
giving arbitrary names anonymous entities whose existence asserted
use blank nodes G.
example, let: G = {rdf :type(?x, EuropeanCountry), rdf :type(?x, EU member)}.
Then,
sk(G) = {rdf :type(skG (?x), EuropeanCountry), rdf :type(skG (?x), EU member)}.

following proposition expresses skolemization ERDF graph
entailments original graph, provided contain URIs
skolemization vocabulary.
Proposition 4.1 Let G ERDF graph let F ERDF formula
VF skG (Var (G)) = . holds: G |=ERDF F iff sk(G) |=ERDF F .
define vocabulary ERDF ontology O.
Definition 4.5 (Vocabulary ERDF ontology) Let = hG, P ERDF ontology. vocabulary defined VO = Vsk(G) VP VRDF VRDF VERDF .

Note vocabulary ontology = hG, P contains skolemization vocabulary G.
Let = hG, P ERDF ontology. denote ResH
union VO set
XML values well-typed XML literals VO minus well-typed XML literals.
following definition defines Herbrand interpretations Herbrand models
ERDF ontology.
Definition 4.6 (Herbrand interpretation, Herbrand model ERDF ontology)
Let = hG, P ERDF ontology let ERDF interpretation VO . say
Herbrand interpretation iff:
ResI = ResH
O.
IV (x) = x, x VO URI.
ILI (x) = x, x typed literal VO well-typed XML literal,
ILI (x) XML value x, x well-typed XML literal VO .
51

fiAnalyti, Antoniou, Damasio, & Wagner

denote set Herbrand interpretations H (O).
Herbrand interpretation Herbrand model iff |= hsk(G), P i. denote
set Herbrand models MH (O).
Note Herbrand interpretation ERDF ontology I(x) = x,
x VO well-typed XML literal.
easy see every Herbrand model ERDF ontology model
O. Moreover, note every Herbrand interpretation ERDF ontology uniquely
identified (i) set properties (ii) property-truth property-falsity extension
mappings.
However, Herbrand models ERDF ontology desirable. example,
let p, s, URI, let G = {p(s, o)}, let = hG, i. Then, Herbrand model
|= p(o, s), whereas want p(o, s) satisfied intended
models O. p total property p(o, s) cannot derived
(negation-as-failure)13 .
define minimal Herbrand interpretations ERDF ontology O, need
define partial ordering Herbrand interpretations O.
Definition 4.7 (Herbrand interpretation ordering) Let = hG, P ERDF ontology. Let I, J H (O). say J extends I, denoted J (or J I), iff
P ropI P ropJ , p P ropI , holds PT (p) PT J (p) PF (p) PF J (p).

easy verify relation reflexive, transitive, antisymmetric. Thus,
partial ordering H (O).
intuition behind Definition 4.7 extending Herbrand interpretation,
extend truth falsity extension properties, thus (since rdf :type
property), classes.
following proposition expresses two Herbrand interpretations I, J ERDF
ontology incomparable, property-truth property-falsity extension total
property p w.r.t. J different.
Proposition 4.2 Let = hG, P ERDF ontology let I, J H (O). Let p
TProp TProp J . PT (p) 6= PT J (p) PF (p) 6= PF J (p) 6 J J 6 I.
Definition 4.8 (Minimal Herbrand interpretations) Let ERDF ontology
let H (O). define minimal(I) = {I | 6 J : J =
6 J I}.
define minimal Herbrand models O, as:
Mmin (O) = minimal(MH (O)).
However minimal Herbrand models give intended semantics ERDF rules.
ERDF rules derivation implication rules. Derivation rules
13. hand, p total property p(o, s)p(o, s) satisfied intended models.
Therefore, case, intended model satisfies p(o, s).

52

fiExtended RDF Semantic Foundation Rule Markup Languages

often identified implications. But, general, two different concepts.
implication expression logical formula language, derivation rule rather
meta-logical expression. logics, implication connective,
derivation rule concept. standard logics (such classical intuitionistic
logic), close relationship derivation rule (also called sequent)
corresponding implicational formula: models. non-monotonic
rules (e.g. negation-as-failure), longer case: intended models
rule are, general, intended models corresponding implication.
easy see help example. Consider rule p q whose model set,
according stable model semantics (Gelfond & Lifschitz, 1988, 1990; Herre & Wagner,
1997; Herre et al., 1999), {{p}}, is, entails p. hand, model set
corresponding implication q p, equivalent disjunction p q,
{{p}, {q}, {p, q}}; consequently, entail p.
Similarly, let = h, P i, P = {p(s, o) q(s, o)} p, q, s, URI.
minimal Herbrand models intended. particular, Mmin (O)
|= q(s, o) p(s, o), whereas want q(s, o) p(s, o) satisfied
intended models O, q total property q(s, o) cannot derived rule
(negation-as-failure).
define intended (stable) models ERDF ontology, need first define
grounding ERDF rules.
Definition 4.9 (Grounding ERDF program) Let V vocabulary let r
ERDF rule. denote [r]V set rules result r replace
variable x FVar (r) v(x), mappings v : FVar (r) V .
Let P ERDF program. define [P ]V =



rP [r]V .



Note rule variable naturally appear subject position ERDF triple.
Since variables instantiated literal, literal naturally appear subject
position ERDF triple grounded version ERDF program. case
supports choice allowing literals subject position ERDF triple.

5. ERDF Stable Models
section, define intended models ERDF ontology O, called stable models
O, based minimal Herbrand interpretations. particular, defining stable models
O, minimal interpretations set Herbrand interpretations satisfy
certain criteria considered.
Below, define stable models ERDF ontology, based coherent stable
models14 Partial Logic (Herre et al., 1999).
Definition 5.1 (ERDF stable model) Let = hG, P ERDF ontology let
H (O). say (ERDF) stable model iff chain
Herbrand interpretations O, I0 ... Ik+1 Ik = Ik+1 = and:
14. Note models extended logic programs equivalent (Herre et al., 1999) Answer Sets
answer set semantics (Gelfond & Lifschitz, 1990).

53

fiAnalyti, Antoniou, Damasio, & Wagner

1. I0 minimal({I H (O) | |= sk(G)}).
2. successor ordinals 0 < k + 1:
minimal({I H (O) | I1 |= Concl(r), r P[I1 ,M ] }),
P[I1 ,M ] = {r [P ]VO | |= Cond(r), H (O) s.t. I1 }.
set stable models denoted Mst (O).
Note I0 minimal Herbrand interpretation = hG, P satisfies sk(G),
Herbrand interpretations I1 , ..., Ik+1 correspond stratified sequence rule applications, applied rules remain applicable throughout generation stable
model . words, stable model generated bottom-up iterative application
rules ERDF program P , starting information ERDF graph G.
Thus, ERDF stable model semantics, refinement minimal model semantics, captures
intuition that:
Assertions rdf :type(p, erdf :TotalProperty) rdf :type(c, erdf :TotalClass)
accepted ontology contains direct support form
acceptable rule sequence (that corresponds proof).
Assertions p(s, o) p(s, o) accepted ontology contains
direct support form acceptable rule sequence,
rdf :type(p, erdf :TotalProperty) accepted.
Assertions rdf :type(o, c) rdf :type(o, c) accepted ontology
contains direct support form acceptable rule sequence,
rdf :type(c, erdf :TotalClass) accepted.
Wine Selection Example: Consider class Wine whose instances wines,
property likes(X, ) indicating person X likes object . Assume want
select wines dinner that, guest, table exactly one wine
he/she likes. Let class Guest indicate persons invited dinner
let class SelectedWine indicate wines chosen served. ERDF program
P describes wine selection problem following (commas , body
rules indicate conjunction ):
id(?x, ?x) rdf :type(?x, rdfs:Resource).
rdf :type(?y, SelectedWine) rdf :type(?x, Guest), rdf :type(?y, Wine), likes(?x, ?y),
?z (rdf :type(?z, SelectedWine), id(?z, ?y) likes(?x, ?z)).

Consider ERDF graph G, containing factual information:
G = { rdf :type(Carlos, Guest), rdf :type(Gerd , Guest), rdf :type(Riesling, Wine),
rdf :type(Retsina, Wine), rdf :type(Chardonnay, Wine), likes(Gerd , Riesling),
likes(Gerd , Retsina), likes(Carlos, Chardonnay), likes(Carlos, Retsina) }.

Then, according Definition 5.1, ERDF ontology = hG, P two stable models,
M1 M2 , that:
54

fiExtended RDF Semantic Foundation Rule Markup Languages

M1 |= rdf :type(Riesling, SelectedWine) rdf :type(Chardonnay, SelectedWine)
rdf :type(Retsina, SelectedWine).
M2 |= rdf :type(Retsina, SelectedWine) rdf :type(Riesling, SelectedWine)
rdf :type(Chardonnay, SelectedWine).

Note that, according stable model M1 , wines selected dinner Riesling
Chardonnay. because, (i) Gerd likes Riesling like Chardonnay,
(ii) Carlos likes Chardonnay like Riesling.
According stable model M2 , Retsina selected dinner. because,
Gerd Carlos like Retsina.
Stable model M1 reached chain I0 M1 M1 , I0 single
Herbrand interpretation minimal({I H (O) | |= sk(G)}). verify this, note that:
P[I0 ,M1 ] = P[M1 ,M1 ] =
[id(?x, ?x) rdf :type(?x, rdfs:Resource)]VO
{rdf :type(Riesling, SelectedWine) rdf :type(Gerd , Guest),
rdf :type(Riesling, Wine), likes(Gerd , Riesling),
?z (rdf :type(?z, SelectedWine), id(?z , Riesling) likes(Gerd , ?z))}
{rdf :type(Chardonnay, SelectedWine) rdf :type(Carlos, Guest),
rdf :type(Chardonnay, Wine), likes(Carlos, Chardonnay),
?z (rdf :type(?z, SelectedWine), id(?z , Chardonnay) likes(Carlos, ?z))}.

Similarly, stable model M2 reached chain I0 M2 M2 . verify this,
note that:
P[I0 ,M2 ] = P[M2 ,M2 ] =
[id(?x, ?x) rdf :type(?x, rdfs:Resource)]VO
{rdf :type(Retsina, SelectedWine) rdf :type(Gerd , Guest),
rdf :type(Retsina, Wine), likes(Gerd , Retsina),
?z (rdf :type(?z, SelectedWine), id(?z, Retsina) likes(Gerd , ?z))}
{rdf :type(Retsina, SelectedWine) rdf :type(Carlos, Guest),
rdf :type(Retsina, Wine), likes(Carlos, Retsina),
?z (rdf :type(?z, SelectedWine), id(?z, Retsina) likes(Carlos, ?z))}.

Assume Retsina one selected wines,
match food. indicate this, add P ERDF constraint:
false rdf :type(Retsina, SelectedWine).
Then, M1 single model modified ontology.
easy verify ERDF ontology exactly O, without
ERDF constraints appearing O, Mst (O) Mst (O ). words, ERDF
constraints appearing ERDF ontology eliminate undesirable stable models.
Paper Assignment Example: Consider class Paper whose instances papers submitted conference, class Reviewer whose instances potential reviewers
55

fiAnalyti, Antoniou, Damasio, & Wagner

submitted papers, property conflict(R, P ) indicating conflict interest reviewer R paper P . Assume want assign papers
reviewers based following criteria: (i) paper assigned one reviewer,
(ii) reviewer assigned one paper, (iii) paper assigned reviewer,
conflict interest. assignment paper P reviewer R indicated
property assign(P, R). ERDF triple allAssigned (Paper , Reviewer ) indicates paper assigned one reviewer. ERDF program P describing
assignment papers following:
id(?x, ?x) true.
assign(?p, ?r) rdf :type(?p, Paper ), rdf :type(?p , Paper ), assign(?p , ?r), id(?p, ?p ).
assign(?p, ?r) rdf :type(?r, Reviewer ), rdf :type(?r , Reviewer ), assign(?p, ?r ), id(?r, ?r ).
assign(?p, ?r) conflict(?r, ?p).
assign(?p, ?r)
rdf :type(?r, Reviewer ), rdf :type(?p, Paper ), assign(?p, ?r).
allAssigned (Paper , Reviewer ) ?p (rdf :type(?p, Paper )
?r (rdf :type(?r, Reviewer ) assign(?p, ?r))).

Consider ERDF graph G, containing factual information:
G = { rdf :type(P 1, Paper ), rdf :type(P 2, Paper ), rdf :type(P 3, Paper ), rdf :type(R1, Reviewer ),
rdf :type(R2, Reviewer ), rdf :type(R3, Reviewer ), conflict(P 1, R3), conflict(P 2, R2),
conflict(P 3, R2) }.

Then, according Definition 5.1, ERDF ontology = hG, P four stable
models, denoted M1 , ..., M4 , that:
M1
M2
M3
M4

|=
|=
|=
|=

assign(P 1, R1) assign(P 2, R3) allAssigned (Paper , Reviewer ),
assign(P 1, R1) assign(P 3, R3) allAssigned (Paper , Reviewer ),
assign(P 1, R2) assign(P 2, R1) assign(P 3, R3) allAssigned (Paper , Reviewer ),
assign(P 1, R2) assign(P 2, R3) assign(P 3, R1) allAssigned (Paper , Reviewer ).

would like note that, contrast previous examples, given ERDF
ontology = hG, P i, possible |minimal ({I H (O) | |= sk(G)})| > 1, due
declaration total properties total classes. Specifically, number
interpretations I0 item 1 Definition 5.1 one iff G contains ERDF triples
form rdf :type(p, erdf :TotalProperty) rdf :type(c, erdf :TotalClass). example, let
= hG, i, where:
G = {authorOf (John, book1 ), authorOf (Peter , book2 ), rdf :type(authorOf , erdf :TotalProperty)}.

Then, I0 , I0 minimal ({I H (O) | |= sk(G)}) that:
I0 |= authorOf (John, book2 ) I0 |= authorOf (John, book2 ).
Note I0 I0 stable models O. However, I0 satisfies authorOf (John, book2 ),
even though evidence John author book2 .
following proposition shows stable model ERDF ontology Herbrand model O.
56

fiExtended RDF Semantic Foundation Rule Markup Languages

Proposition 5.1 Let = hG, P ERDF ontology let Mst (O). holds
MH (O).
hand, properties total, Herbrand model ERDF ontology
= hG, P stable model O15 . Obviously, desirable result since,
case, open-world assumption made properties. Thus, preferential
entailment weak negation, properties. course, term stable model
descriptive, degenerative case.
Proposition 5.2 Let = hG, P ERDF ontology
rdfs:subClassOf (rdf :Property, erdf :TotalProperty) G. Then, Mst (O) = MH (O).
final note that, similarly stable models defined Gelfond & Lifschitz (1988, 1990)
Herre et al. (1999), ERDF stable models preserve Herbrand model satisfiability.
example, let = h, P i, P = {p(s, o) p(s, o)} p, s, URI. Then,
Mst (O) = , whereas Herbrand model satisfies p(s, o).

6. ERDF Stable Model Entailment & Stable Answers
section, define stable model entailment ERDF ontologies, showing extends ERDF entailment ERDF graphs. Moreover, define skeptical credulous
answers ERDF formula (query) F w.r.t. ERDF ontology O.
Definition 6.1 (Stable model entailment) Let = hG, P ERDF ontology
let F ERDF formula ERDF graph. say entails F (ERDF)
stable model semantics, denoted |=st F iff Mst (O), |= F .
example, let = h, P i, P = {p(s, o) q(s, o)} p, q, s, URI. Then,
|=st q(s, o) p(s, o).
Now, let G = {rdfs:subClassOf (rdf :Property, erdf :TotalProperty)} let P
previous example. Then, hG, P |=st q(s, o) p(s, o), hG, P 6|=st q(s, o)
hG, P 6|=st p(s, o). Note desirable result, since q total property
(and thus, open-world assumption made q).
another example, let p, s, URI, let G = {p(s, o)}, let P = {p(?x, ?y)
p(?x, ?y)}. Then, hG, P |=st p(o, s) p(o, s) (note P contains CWA p).
Now, let G = {rdf :type(p, erdf :TotalProperty), p(s, o)} let P previous
example. Then, hG, P |=st ?x ?y (p(?x, ?y) p(?x, ?y)) (see Proposition 3.1),
hG, P 6|=st p(o, s) hG, P 6|=st p(o, s). Indeed, CWA P affect
semantics p, since p total property.
EU Membership Example: Consider following ERDF program P , specifying
rules concluding country member state European Union (EU).
(r1 )
(r2 )

rdf :type(?x, EUMember)
rdf :type(?x, EUMember)

rdf :type(?x, AmericanCountry).
rdf :type(?x, EuropeanCountry),
rdf :type(?x, EUMember).

15. Note that, case, minimal({I H (O) | |= sk(G)}) minimal({I H (O) |
|= Concl(r), r P[M,M ] }).

57

fiAnalyti, Antoniou, Damasio, & Wagner

rather incomplete ERDF ontology = hG, P obtained including following
information ERDF graph G:
rdf :type(Russia, EUMember).
rdf :type(Austria, EUMember).
rdf :type(?x, EuropeanCountry).

rdf :type(Canada, AmericanCountry).
rdf :type(Italy, EuropeanCountry).
rdf :type(?x, EUMember).

Using stable model entailment O, concluded Austria member EU,
Russia Canada members EU, exists European Country
member EU. However, also concluded Italy member EU, wrong statement. G contain complete
information European countries EU members (e.g., contain
rdf :type(Italy, EUMember)). Thus, incorrect information obtained closed-world
assumption expressed rule r2 . case rdf :type(EUMember, erdf :TotalClass)
added G (that is, open-world assumption made class EUMember)
rdf :type(Italy, EUMember) thus, rdf :type(Italy, EUMember) longer entailed.
because, stable model extended ERDF ontology satisfies
rdf :type(Italy, EUMember). Moreover, complete information European countries
members EU included G stable model conclusions also
correct (the closed-world assumption correctly applied). Note that, case,
G include ERDF triple rdf :type(Italy, EUMember).
following proposition follows directly fact stable model
ERDF ontology ERDF interpretation.
Proposition 6.1 Let = hG, P ERDF ontology let F, F ERDF formulas.
|=st F F |=ERDF F |=st F .
ERDF graphs G, G , proved hG, |=st G iff G |=ERDF G (see
below). question arises whether result generalized replacing
ERDF graph G ERDF formula F . following example shows
case. Let G = {p(s, o)} let F = p(o, s), p, s, URI. hG, |=st F ,
whereas G 6|=ERDF F . However, G replaced ERDF d-formula F , defined
follows:
Definition 6.2 (ERDF d-formula) Let F ERDF formula. say F
ERDF d-formula iff (i) F disjunction existentially quantified conjunctions ERDF
triples, (ii) FVar (F ) = .
example, let:
F = (?x rdf :type(?x , Vertex ) rdf :type(?x , Red ))
(?x rdf :type(?x , Vertex ) rdf :type(?x , Blue)).
Then, F ERDF d-formula. easy see G ERDF graph formula(G)
ERDF d-formula.
58

fiExtended RDF Semantic Foundation Rule Markup Languages

Proposition 6.2 Let G ERDF graph let F ERDF formula
VF skG (Var (G)) = . holds:
1. F ERDF d-formula hG, |=st F G |=ERDF F .
2. G |=ERDF F hG, |=st F .
Let G ERDF graph let F ERDF d-formula ERDF graph
VF skG (Var (G)) = . direct consequence Proposition 6.2 that:
hG, |=st F iff G |=ERDF F .
following proposition direct consequence Proposition 3.2 Proposition
6.2, shows stable model entailment extends RDFS entailment RDF graphs
ERDF ontologies.
Proposition 6.3 Let G, G RDF graphs VG VERDF = , VG VERDF = ,
VG skG (Var (G)) = . holds: G |=RDF G iff hG, |=st G .
Recall Skolem vocabulary G (that is, skG (Var (G))) contains artificial URIs
giving arbitrary names anonymous entities whose existence asserted
use blank nodes G. Thus, condition VG skG (Var (G)) = Proposition 6.3
actually trivial.
Definition 6.3 (ERDF query, ERDF stable answers) Let = hG, P ERDF
ontology. (ERDF) query F ERDF formula. (ERDF) stable answers F
w.r.t. defined follows:

FVar (F ) = Mst (O) : |= F
yes
st

FVar (F ) = Mst (O) : 6|= F
Ans (F ) =

{v : FVar (F ) VO | Mst (O), |= v(F )}

FVar (F ) 6= ,

v(F ) formula F replacing free variables x F v(x).
example, let p, q, c, s, URI, let G = {p(s, o), rdf :type(s, c), rdf :type(o, c)},
let P = {q(?x, ?y) rdf :type(?x, c) rdf :type(?y, c) p(?x, ?y)}. Then, stable
answers F = q(?x, ?y) w.r.t. = hG, P Ans st
(F ) = {{?x = o, ?y = o}, {?x =
s, ?y = s}, {?x = o, ?y = s}}.
Let = hG, P i, q, s, URI, G = {rdf :type(p, erdf :TotalProperty), q(s, o)},
st
P = {p(?x, ?y) p(?x, ?y)}. Then, holds Ans st
(p(?x, ?y))= Ans (p(?x, ?y)) =
st
Ans (p(?x, ?y)) = . because, contrast example, p total
property. Thus, mappings v : {?x, ?y} VO , stable model
|= v(p(?x, ?y) p(?x, ?y)), another stable model
|= v(p(?x, ?y) p(?x, ?y)).
Consider ERDF ontology paper assignment example, Definition
st
5.1. Then, Ans st
(assign(P 1, R2)) =yes Ans (assign(P 2, R1)) =no. Though
st
Ans (assign(P 2, R1)) =no, assign(P 2, R1) satisfied stable models O, stable model (M3 ) satisfies assign(P 2, R1). Indeed answers
query assign(?x, ?y) w.r.t. stable models M3 M4 particular interest since
59

fiAnalyti, Antoniou, Damasio, & Wagner

M3 M4 satisfy allAssigned (Paper , Reviewer ), indicating desirable paper
assignment achieved.
following definition defines credulous stable answers query F w.r.t.
ERDF ontology O, answers F w.r.t. particular stable models O.
Definition 6.4 (Credulous ERDF stable answers) Let = hG, P ERDF ontology. credulous (ERDF) stable answers query F w.r.t. defined follows:

yes FVar (F ) = Mst (O) : |= F
st
FVar (F ) = Mst (O) : 6|= F
c-Ans (F ) =

{ans (F ) 6= | Mst (O)}
FVar (F ) 6= ,

ans (F ) = {v : FVar (F ) VO | |= v(F )}.

Continuing paper assignment example, consider query:
F = allAssigned (Paper , Reviewer ).
st
Then, although Ans st
(F ) =no, holds c-Ans (F ) =yes, indicating
least one desirable assignment papers P 1, P 2, P 3 reviewers R1, R2, R3.
Consider query F = allAssigned (Paper , Reviewer ) assign(?x , ?y). Then,

c-Ans st
(F ) = {{{?x = P 1, ?y = R2}, {?x = P 2, ?y = R1}, {?x = P 3, ?y = R3}},
{{?x = P 1, ?y = R2}, {?x = P 2, ?y = R3}, {?x = P 3, ?y = R1}}},

indicating possible desirable assignments papers. Obviously, credulous stable
answers query F provide alternative solutions, useful range
applications, alternative scenarios naturally appear.
Closing section, would like indicate several differences ERDF stable
model semantics w.r.t. first-order logic (FOL). First, semantics domain closure assumption made. due fact domain every Herbrand interpretation
ERDF ontology ResH
, union vocabulary (VO )
set XML values well-typed XML literals VO minus well-typed XML literals.
implies quantified variables always range closed domain. understand
implications assumption, consider ERDF graph:
G = {rdf :type(x, c1) | x {c1, c2} V },

V = (VRDF {rdf : | }) VRDF VERDF . Additionally, consider ERDF
program:
P = { rdf :type(?x, c1) rdf :type(?x, rdfs:ContainerMembershipProperty).
rdf :type(?x, c2) true.}.

Let F = ?x rdf :type(?x, c2) rdf :type(?x, c1). holds hG, P |=st F . However,
G P 6|=F OL F . because, FOL model G P domain
variable assignment v:{?x} M, v |= rdf :type(?x, c2) M, v 6|=
rdf :type(?x, c1).
Another difference due fact definition ERDF stable model
semantics, minimal Herbrand interpretations considered. Let
60

fiExtended RDF Semantic Foundation Rule Markup Languages

G = {teaches(Anne, CS301 ), teaches(Peter , CS505 ), rdf :type(CS505 , GradCourse)}.

Let F = ?x teaches(Peter , ?x) rdf :type(?x, GradCourse). Then, hG, |=st F .
However, G 6|=F OL F . because, FOL model G domain
variable assignment v:{?x} M, v |= teaches(Peter , ?x) M, v 6|=
rdf :type(?x, GradCourse). words, FOL makes open-world assumption
teaches.
Consider G = G {rdf :type(teaches, erdf :TotalProperty)}. Then, similarly
FOL, holds = hG , 6|=st F . teaches total property. Thus,
stable model variable assignment v: {?x} ResH

M, v |= teaches(Peter , ?x) M, v 6|= rdf :type(?x, GradCourse). worlds,
open-world assumption made teaches, FOL. Thus, might exist course
taught Peter , even explicitly indicated G .
example also shows that, contrast FOL, stable model entailment nonmonotonic.
Note previous ERDF graph G also seen Description Logic A-Box
(Baader et al., 2003),
= {teaches(Anne, CS301), teaches(Peter , CS505), GradCourse(CS505)}
Consider T-Box = . Since Description Logics (DLs) fragments first-order
logic, holds L = hA, 6|=DL teaches.GradCourse(Peter ), meaning L
satisfy courses taught Peter graduate courses. interesting approach
supporting non-monotonic conclusions DLs taken Donini et al. (2002), DLs
minimal knowledge negation failure (MKNF-DLs) defined, extending DLs
two modal operators K, A. Intuitively, K expresses minimal knowledge expresses
weak negation. holds L |=MKNF-DL Kteaches.KGradCourse(Peter ), expressing
courses known taught Peter known graduate courses. Note
conclusion non-monotonic, thus cannot derived classical DLs. However,
compared theory, MKNF-DLs support rules closed-world assumptions
properties (i.e., p(?x, ?y) p(?x, ?y)).

7. XML-based Syntax ERDF
natural approach define XML syntax ERDF is: (i) follow RDF/XML
syntax (Beckett, 2004), much possible, (ii) extend suitable way,
necessary. Following approach, briefly present XML syntax ERDF.
Details going given subsequent paper.
Classes properties defined help rdfs:Class rdf:Property
elements RDF/XML syntax. Similarly, total classes total properties defined help erdf:TotalClass erdf:TotalProperty elements
ERDF/XML syntax.
Example 7.1 following ERDF/XML statements:
<rdf:Property rdf:about="#likes">
<rdfs:domain rdf:resource="#Person"/>
61

fiAnalyti, Antoniou, Damasio, & Wagner

</rdf:Property>
<erdf:TotalProperty rdf:about="#authorOf">
<rdfs:domain rdf:resource="#Person"/>
<rdfs:range rdf:resource="#Book"/>
</erdf:TotalProperty>

correspond ERDF graph:
G = { rdf :type(likes, rdf :Property), rdfs:domain(likes, Person),
rdf :type(authorOf , erdf :TotalProperty), rdfs:domain(authorOf , Person),
rdfs:range(authorOf , Book )}.

ERDF triples (and sets ERDF triples sharing subject term) encoded
means erdf:Description element. description contains non-empty list
(possibly negated) property-value slots subject term.
URI references, blank node identifiers, variables appear subject position
ERDF triple expressed values erdf:about attribute, using
SPARQL syntax (Prudhommeaux & Seaborne, 2008) blank node identifiers
variables. hand, literals appear subject position ERDF
triple expressed text content erdf:about subelement.
URI references, blank node identifiers, variables appear object position ERDF triple expressed values attributes rdf:resource,
rdf:nodeID, erdf:variable, respectively. hand, literals appear object position ERDF triple expressed text content
corresponding property subelement.
Example 7.2 following erdf:Description statements:
<erdf:Description erdf:about="#Gerd">
<ex:authorOf rdf:nodeID="x"/>
<ex:likes rdf:resource="#Chicken"/>
<ex:likes erdf:negationMode="Sneg" rdf:resource="#Pork"/>
</erdf:Description>
<erdf:Description>
<erdf:About rdf:datatype="&xsd;string">Grigoris</erdf:About>
<ex:denotationOf rdf:resource="#Grigoris"/>
</erdf:Description>

correspond ERDF graph:
G = { authorOf (Gerd , ?x ), likes(Gerd , Chicken), likes(Gerd , Pork ),
denotationOf (Grigorisxsd :string, Grigoris) }.

Now, order express ERDF rules XML, use rule markup language R2ML
(REWERSE Rule Markup Language) (Wagner, Giurca, & Lukichev, 2006, 2005),
general XML-based markup language representing derivation rules integrity
constraints. demonstrated following example:
62

fiExtended RDF Semantic Foundation Rule Markup Languages

Example 7.3 following erdf:DerivationRule statement:
<r2ml:DerivationRule r2ml:ruleID="R1">
<r2ml:conditions>
<erdf:Description erdf:about="?x">
<rdf:type rdf:resource="#MainDish"/>
</erdf:Description>
<erdf:Description erdf:about="?y">
<rdf:type rdf:resource="#Guest"/>
<ex:likes erdf:variable="x"/>
</erdf:Description>
<r2ml:NegationAsFailure>
<r2ml:ExistentiallyQuantifiedFormula>
<r2ml:GenericVariable r2ml:name="z" r2ml:class="#Guest"/>
<erdf:Description erdf:about="?z">
<ex:likes erdf:negationMode="Sneg" erdf:variable="x"/>
</erdf:Description>
</r2ml:ExistentiallyQuantifiedFormula>
</r2ml:NegationAsFailure>
</r2ml:conditions>
<r2ml:conclusion>
<erdf:Description erdf:about="?x">
<rdf:type rdf:resource="#SelectedMainDish"/>
</erdf:Description>
</r2ml:conclusion>
</r2ml:DerivationRule>

expresses main dish selected dinner, guest likes guest
dislikes it. Specifically, corresponds ERDF rule:
rdf :type(?x, SelectedMainDish) rdf :type(?x, MainDish), rdf :type(?y, Guest), likes(?y, ?x),
(?z rdf :type(?z, Guest), likes(?z, ?x)).

8. Undecidability ERDF Stable Model Semantics
main difficulty computation ERDF stable model semantics fact
VRDF infinite, thus vocabulary ERDF ontology also infinite (note
{rdf : | } VRDF VO ). Due fact, satisfiability entailment
ERDF stable model semantics general undecidable.
proof undecidability exploits reduction unbounded tiling problem.
unbounded tiling problem consists placing tiles infinite grid, satisfying given set
constraints adjacent tiles. Specifically, unbounded tiling problem structure
= hT , H, V i, = {T1 , ..., Tn } finite set tile types H, V specify
tiles adjacent horizontally vertically, respectively. solution
tiling, is, total function : that: ( (i, j), (i + 1, j)) H
( (i, j), (i, j + 1)) V , i, j . existence solution given unbounded
tiling problem known undecidable (Berger, 1966).
Let = hT , H, V instance unbounded tiling problem, = {T1 , ..., Tn }.
construct ERDF ontology OD = hG, P ERDF formula FD
solution iff OD entail FD ERDF stable model semantics.
63

fiAnalyti, Antoniou, Damasio, & Wagner

Consider (i) class Tile whose instances tiles placed infinite grid, (ii)
property right(x , y) indicating tile right next tile x, (iii) property above(x , y)
indicating tile exactly tile x, (iv) class HasRight whose instances
tiles exists tile right next them, (v) class HasAbove whose instances
tiles exists tile exactly them, (vi) property Type(x, ),
indicating type tile x , (vii) property HConstraint(T, ), indicating
(T, ) H, (viii) property VConstraint(T, ), indicating (T, ) V .
Let G ERDF graph:
G=

{rdfs:subClassOf (rdfs:ContainerMembershipProperty, Tile),
rdfs:subClassOf (Tile, rdfs:ContainerMembershipProperty)}
{HConstraint(T, ) | (T, ) H} {VConstraint(T, ) | (T, ) V } .

Let P ERDF program, containing following rules (and constraints):
(1)

Type(?x, T1 ) rdf :type(?x, Tile), Type(?x, T2 ), ..., Type(?x, Tn ).
Type(?x, Ti ) rdf :type(?x, Tile), Type(?x, T1 ), ..., Type(?x, Ti1 ),
Type(?x, Ti+1 ), ..., Type(?x, Tn ), = 2, ..., n 1.
Type(?x, Tn ) rdf :type(?x, Tile), Type(?x, T1 ), ..., Type(?x, Tn1 ).

(2)

right(?x, ?y)
right(?x, ?y)




rdf :type(?x, Tile), rdf :type(?y, Tile), right(?x, ?y).
rdf :type(?x, Tile), rdf :type(?y, Tile), right(?x, ?y).

(3)

above(?x, ?y)
above(?x, ?y)




rdf :type(?x, Tile), rdf :type(?y, Tile), above(?x, ?y).
rdf :type(?x, Tile), rdf :type(?y, Tile), above(?x, ?y).

(4)

rdf :type(?x, HasRight) right(?x, ?y).
rdf :type(?x, HasAbove) above(?x, ?y).
false rdf :type(?x, Tile), rdf :type(?x, HasRight).
false rdf :type(?x, Tile), rdf :type(?x, HasAbove).
id (?x, ?x) rdf :type(?x, rdfs:Resource).
false right(?x, ?y), right(?x, ?y ), id (?y, ?y ).
false above(?x, ?y), above(?x, ?y ), id (?y, ?y ).

(5)

false right(?x, ?y), Type(?x, ?T ), Type(?y, ?T ), HConstraint(?T, ?T ).
false above(?x, ?y), Type(?x, ?T ), Type(?y, ?T ), VConstraint(?T, ?T ).

Note stable models OD = hG, P i, class Tile contains exactly
(infinite mumber) rdf : terms, . because, computing stable models
O, minimal models sk(G) considered (see Definition 5.1, Step 1). Thus,
tile infinite grid represented rdf : term, .
Intuitively, rule set (1) expresses tile exactly one associated type
. Rule set (2) expresses two tiles either horizontally adjacent grid
64

fiExtended RDF Semantic Foundation Rule Markup Languages

horizontally adjacent. Rule set (3) expresses two tiles either vertically adjacent
grid vertically adjacent. Rule set (4) expresses tile exactly
one tile right next exactly one tile right it. Rule set (5) expresses
types horizontally vertically adjacent tiles respect H V relations
D, respectively.
finalize reduction, define:
FD = ?x, ?y, ?x , ?y , ?x right(?x, ?y) above(?y, ?y ) right(?x , ?y ) above(?x , ?x )
id (?x, ?x ).

Formula FD expresses tile x that, starting x, move:
one step right one step one step left one step

meet tile x different x.
Proposition 8.1 Let instance unbounded tiling problem. holds:
1. solution iff OD {false FD } stable model.
2. solution iff OD 6|=st FD .
Since unbounded tiling problem undecidable (Berger, 1966), follows directly
Proposition 8.1 satisfiability entailment ERDF stable model semantics
general undecidable.
previous reduction shows problems remain undecidable ERDF ontology = hG, P i, even (i) body rule P form t1 , ..., tk , tk+1 , ..., tn ,
ti ERDF triple (ii) terms erdf :TotalClass erdf :TotalProperty
appear O, is, (VG VP ) VERDF = . Note since constraint false F
appears ERDF ontology replaced rule F ,
RDF, RDFS, ERDF axiomatic triple, presence constraints affect
decidability.
Future work concerns identification syntactic restrictions ERDF ontology
ERDF stable model entailment decidable.

9. ERDF Model Theory Tarski-style Model Theory
Tarski-style model theory limited classical first-order models, employed
semantics OWL. allows various extensions, relaxing bivalence assumption
(e.g., allowing partial models) allowing higher-order models. also compatible
idea non-monotonic inference, simply considering models rule
intended, models satisfy certain criteria. Thus, stable model
semantics normal (generalized) extended logic programs (Gelfond & Lifschitz, 1988,
1990; Herre & Wagner, 1997; Herre et al., 1999) viewed Tarski-style modeltheoretic semantics non-monotonic derivation rules.
Tarski-style model theory triple hL, I, |=i that:
L set formulas, called language,
65

fiAnalyti, Antoniou, Damasio, & Wagner

set interpretations,
|= relation interpretations formulas, called model relation.
Tarski-style model theory hL, I, |=i, define:
notion derivation rule G F , F L called condition G L
called conclusion,
set derivation rules DRL = {G F | F, G L},
extension model relation |= include also pairs interpretations
derivation rules,
standard model operator M(KB ) = {I | |= X, X KB }, KB
L DRL set formulas and/or derivation rules, called knowledge base.
Notice way define rules also logics contain
implication connective. shows concept rule independent concept
implication.
Typically, knowledge representation theories, models knowledge base
intended models. Except standard model operator M, also non-standard
model operators, provide models knowledge base, special
subset supposed capture intended models according semantics.
particularly important type intended model semantics obtained
basis information ordering , allows compare information content
two interpretations I1 , I2 I. Whenever I1 I2 , say I1 less informative
I2 . information model theory hL, I, |=, Tarski-style model theory, extended
information ordering .
information model theory, define number natural non-standard
model operators, minimal model operator:
Mmin (KB ) = minimal (M(KB ))
various refinements it, like stable generated models (Gelfond & Lifschitz, 1988,
1990; Herre & Wagner, 1997; Herre et al., 1999).
given model operator Mx : P(L DRL ) P(I), knowledge base KB
L DRL , F L, define entailment relation:
KB |=x F

iff Mx (KB ), |= F

non-standard model operators, like minimal stable models, entailment
relation typically non-monotonic, sense extension KB KB may
case KB entails F , KB entail F .
(ERDF) stable model theory seen Tarski-style model theory,
L = L(URI LIT ), set ERDF interpretations vocabulary V
URI LIT , model relation |= defined Definitions 3.5 4.3.
theory, intended model operator (Mst ) assigns ERDF ontology (possibly
empty) set stable models (Definition 5.1).
66

fiExtended RDF Semantic Foundation Rule Markup Languages

10. Related Work
section, briefly review extensions web ontology languages rules.
Ter Horst (2005b, 2004) generalizes RDF graphs generalized RDF graphs, allowing variables property position RDF triples. Additionally, author extends
RDFS semantics datatypes part OWL vocabulary, defining pD semantics, extends if-semantics RDFS weaker iff-semantics
D-entailment (Hayes, 2004) OWL Full (Patel-Schneider, Hayes, & Horrocks, 2004).
sound complete set entailment rules pD entailment also presented.
subsequent work, ter Horst (2005a) considers extension previous framework inclusion rules form G G , G RDF graph without
blank nodes possibly variables G generalized RDF graph, possibly
blank nodes variables. Intuitively, rule variables universally quantified
front rule (like free variables rules) blank nodes head rule
correspond existentially quantified variables (this feature supported model).
Based set rules R datatype map D, R-entailment16 defined two
generalized RDF graphs G G (G |=R G ), set sound complete rules
R-entailment presented. relate work ter Horst (2005a), state
following proposition:
Let datatype map, containing rdf :XMLLiteral , let R set rules form
G G constraints: (i) terms appearing property position URIs, (ii)
G 6= {} blank node appears G , (iii) VR (VpOWL VERDF ) = , VpOWL
denotes part OWL vocabulary, included pD semantics. Let G, G RDF graphs
(VG VG ) (VpOWL VERDF ) = . based G R, define, simple
transformation, ERDF ontology G |=R G iff |=st G .

However, work, weak strong negation considered. Thus, closed-world
reasoning supported. Additionally, theory, condition rule ERDF
formula vocabulary V , (thus, involving logical factors , , , , , ,
), conjunction positive triples.
TRIPLE (Sintek & Decker, 2002) rule language Semantic Web
especially designed querying transforming RDF models (or contexts), supporting
RDF subset OWL Lite. syntax based F-Logic (Kifer, Lausen, & Wu,
1995) supports important fragment first-order logic. triple represented
statement form s[p o] sets statements, sharing subject s,
aggregated using molecules form s[p1 o1 ; p2 o2 ; ....]. variables must
explicitly quantified, either existentially universally. Arbitrary formulas used
body, head rules restricted atoms conjunctions molecules.
interesting relevant feature TRIPLE use models collect sets related
sentences. particular, part semantics RDF(S) vocabulary represented
pre-defined rules (and semantic conditions interpretations), grouped
together module. TRIPLE provides features like path expressions, skolem model
terms, well model intersection difference. Finally, mentioned
queries models compiled XSB Prolog. TRIPLE uses Lloyd-Topor
transformations (Lloyd & Topor, 1984) take care first-order connectives
16. symbol appear explicitly notation R-entailement, reasons simplification.

67

fiAnalyti, Antoniou, Damasio, & Wagner

sentences supports weak negation well-founded semantics (Gelder, Ross, &
Schlipf, 1991). Strong negation used.
Flora-2 (Yang, Kifer, & Zhao, 2003) rule-based object-oriented knowledge base system reasoning semantic information Web. based F-logic (Kifer
et al., 1995) supports metaprogramming, non-monotonic multiple inheritance, logical database updates, encapsulation, dynamic modules, two kinds weak negation.
Specifically, supports Prolog negation well-founded negation (Gelder et al., 1991),
invocation corresponding operators \+ tnot XSB system (Rao,
Sagonas, Swift, Warren, & Freire, 1997). formal semantics non-monotonic multiple inheritance defined Yang & Kifer (2003a). addition, Flora-2 supports reification anonymous resources (Yang & Kifer, 2003b). particular, Flora-2, reified
statements ${s(p o)}$ objects. contrast, RDF(S), referred URI blank node x, associated following RDF triples:
rdf :type(x, rdf :Statement), rdf :subject(x, s), rdf :predicate(x, p), rdf :object(x, o).
RDF(S) model theory (and thus, theory), special semantics given reified
statements. Flora-2, anonymous resources handled skolemization (similarly
theory).
Notation 3 (N3) (Berners-Lee, Connolly, Kagal, Scharf, & Hendler, 2008) provides
human readable syntax RDF also extends RDF adding numerous predefined constructs (built-ins) able express rules conveniently. Remarkably,
N3 contains built-in (log:definitiveDocument) making restricted completeness assumptions another built-in (log:notIncludes) expressing simple negation-as-failure
tests. addition constructs motivated use cases. However, N3
provide strong negation closed-world reasoning fully supported. N3 supported
CWM system17 , forward engine especially designed Semantic Web,
Euler system18 , backward engine relying loop checking techniques guarantee
termination.
Alferes et al. (2003) propose paraconsistent well-founded semantics explicit
negation (WFSXP )19 , appropriate semantics reasoning (possibly, contradictory) information Semantic Web. Supporting arguments include: (i) possible
reasoning, even presence contradiction, (ii) program transformation WFS,
(iii) polynomial time inference procedures. formal model theory explicitly
provided integrated logic.
DR-Prolog (Antoniou, Bikakis, & Wagner, 2004) DR-DEVICE (Bassiliades, Antoniou, & Vlahavas, 2004) two systems integrate RDFS ontologies rules (strict
defeasible), partially ordered superiority relation, based semantics
defeasible logic (Antoniou, Billington, Governatori, & Maher, 2001; Maher, 2002). Defeasible logic contains one kind negation (strong negation) object language20
allows reason presence contradiction incomplete information. supports
17. http://www.w3.org/2000/10/swap/doc/cwm.html.
18. http://www.agfa.com/w3c/euler/.
19. WFSXP (Alferes, Damasio, & Pereira, 1995) extension well-founded semantics explicit
negation (WFSX) extended logic programs (Pereira & Alferes, 1992) and, thus, also wellfounded semantics (WFS) normal logic programs (Gelder et al., 1991).
20. However, defeasible logic, negation-as-failure easily simulated language ingredients.

68

fiExtended RDF Semantic Foundation Rule Markup Languages

monotonic non-monotonic rules, exceptions, default inheritance, preferences.
formal model theory explicitly provided integrated logic.
OWL-DL (McGuinness & van Harmelen, 2004) ontology representation language
Semantic Web, syntactic variant SHOIN (D) description logic
decidable fragment first-order logic (Horrocks & Patel-Schneider, 2003). However,
need extending expressive power OWL-DL rules initiated several studies,
including SWRL (Semantic Web Rule Language) proposal (Horrocks, Patel-Schneider,
Boley, Tabet, Grosof, & Dean, 2004). Horrocks & Patel-Schneider (2004) show
extension general undecidable. AL-log (Donini, Lenzerini, Nardi, & Schaerf, 1998)
one first efforts integrate Description Logics (safe) datalog rules,
achieving decidability. considers basic description logic ALC imposes constraint concept DL-atoms allowed appear body rules, whereas
heads rules always non DL-atoms. Additionally, variable appearing
concept DL atom body rule also appear non DL-atom body
head rule. CARIN (Levy & Rousset, 1998) provides framework studying
effects combining description logic ALCN R (safe) datalog rules. CARIN,
concept role DL-atoms allowed body rules. shown
integration decidable rules non-recursive, certain combinations constructors
allowed DL component, rules role-safe (imposing constraint
variables role DL atoms body rules)21 . Motik et al. (2004) show
integration SHIQ(D) knowledge base L disjunctive datalog program P decidable, P DL-safe, is, variables rule occur least one non DL-atom
body rule. work, contrast AL-log CARIN, tableaux algorithm
employed query answering L translated disjunctive logic program DD(L)
combined P answering ground queries.
category works, entailment DL, extended rules,
based first-order logic. means DL component logic program
viewed set first-order logic statements. Thus, negation-as-failure, closed-worldassumptions, non-monotonic reasoning cannot supported. contrast, work
supports weak strong negation, allows closed-world open-world reasoning
selective basis.
different kind integration achieved Eiter et al. (2004a). work,
SHOIN (D) knowledge base L communicates extended logic program P (possibly
weak strong negation), DL-query atoms body rules.
particular, description logic component L used answering augmented,
input logic program, queries appearing (possibly weakly negated) DL-query
atoms, thus allowing flow knowledge P L vice-versa. answer set semantics hL, P defined, generalization answer set semantics (Gelfond
& Lifschitz, 1990) ordinary extended logic programs. similar kind integration
achieved Eiter et al. (2004b). work, SHOIN (D) knowledge base L communicates normal logic program P (possibly weak negation), DL-query
atoms body rules. well-founded semantics hL, P defined,
21. rule role-safe least one variables x, role DL atom R(x, y) body
rule, appears body atom base predicate, base predicate ordinary predicate
appears facts rule bodies.

69

fiAnalyti, Antoniou, Damasio, & Wagner

generalization well-founded semantics (Gelder et al., 1991) ordinary normal logic
programs. Obviously, works, derived information concerns non DLatoms (that possibly used input DL-query atoms). Thus, rule-based reasoning
supported non DL-atoms. contrast, work, properties classes appearing ERDF graphs freely appear heads bodies rules, allowing
even derivation metalevel statements subclass subproperty relationships,
property transitivity, property class totalness.
Rosati (1999) defines semantics disjunctive AL-log knowledge base, based
stable model semantics disjunctive databases (Gelfond & Lifschitz, 1991), extending
AL-log (Donini et al., 1998). disjunctive AL-log knowledge base integration
ALC knowledge base (safe) disjunctive logic program P allows concept
role DL-atoms body rules (along weak negation non DL-atoms).
safety condition enforces variable head rule also appear
body rule. Additionally, constants P DL-individuals. Similarly
case, defining disjunctive AL-log semantics, grounded versions
rules considered (by instantiating variables DL individuals). However rule-based
reasoning supported non DL-atoms, DL-atoms body rules
mainly express constraints.
subsequent work, Rosati (2005) defines r-hybrid knowledge bases. r-hybrid
knowledge bases, DL-atoms allowed head rules DL component
SHOIN (D) knowledge base. Additionally, constants P necessarily DLindividuals. However, stronger safety condition imposed, rule variable
appear (positive) non DL-atom body rule. Additionally, weak negation
allowed non DL-atoms rule-based meta-reasoning supported. general,
say non DL-atoms, closed-world assumption made, DL-atoms
conform open-world assumption, SHOIN (D) fragment first-order logic.

11. Conclusions
paper, extended RDF graphs ERDF graphs allowing negative triples
representing explicit negative information. Then, proceeded defining ERDF
ontology ERDF graph complemented set derivation rules connectives
(weak negation), (strong negation), (material implication), , , , body
rule, strong negation head rule. Moreover, extended
RDF(S) vocabulary adding predefined vocabulary elements erdf :TotalProperty
erdf :TotalClass, representing metaclasses total properties total classes,
open-world assumption applies.
defined ERDF formulas, ERDF interpretations, ERDF entailment
ERDF formulas, showing conservatively extends RDFS entailment RDF graphs.
developed model-theoretic semantics ERDF ontologies, called ERDF stable
model semantics, showing stable model entailment extends ERDF entailment ERDF
graphs, thus also extends RDFS entailment RDF graphs. ERDF stable model
semantics based Partial Logic and, particular, generalized definition stable
models (Herre & Wagner, 1997; Herre et al., 1999) (which extends answer set semantics
extended logic programs). shown classical (boolean) Herbrand model
70

fiExtended RDF Semantic Foundation Rule Markup Languages

reasoning special case semantics, properties total. case,
similarly classical logic, open-world assumption made properties classes
two negations (weak strong negation) collapse. Allowing (a) totality
properties classes declared selective basis (b) explicit representation
closed-world assumptions (as derivation rules) enables combination open-world
closed-world reasoning framework.
particular, total property p, open-world assumption applies, since
considered Herbrand interpretation I, computation ERDF stable models, satisfies
p(x, y)p(x, y), pair (x, y) ontology vocabulary terms. closed property p,
default closure rule form p(?x, ?y) p(?x, ?y) added, allows infer
falsity p(x, y), evidence p(x, y) holds. However, method
works partial properties. total property p, may happen stable
model, p(x, y) holds, even though evidence (see example
Section 5, Proposition 5.1). fact, p total property, existence
corresponding default closure rule affect ontology semantics.
main advantages ERDF summarized follows:
Tarski-style model theory, desirable feature logic languages
Semantic Web (Bry & Marchiori, 2005).
based Partial Logic (Herre et al., 1999), simplest conservative
extension classical logic supports weak strong negation. Partial
logic also extends Answer Set Programming (ASP)22 (Gelfond & Lifschitz, 1990),
allowing logical factors , , , , , , body rule.
enables combination open-world (monotonic) closed-world (non-monotonic)
reasoning, framework.
extends RDFS ontologies derivation rules integrity constraints.
Satisfiability entailment ERDF stable model semantics general undecidable. subsequent paper, plan identify syntactic restrictions ERDF
ontologies guarantee decidability reasoning elaborate ERDF computability complexity issues.
work, consider coherent ERDF interpretations. However, due
Semantic Webs decentralized distributed nature, contradictory information frequent
(Schaffert, Bry, Besnard, Decker, Decker, Enguix, & Herzig, 2005). Though Partial Logic
allows truth-value clashes, handling inconsistency Semantic Web topic
deserves extended treatment, outside scope paper. future
plans consider general ERDF interpretations extend vocabulary ERDF
terms erdf :CoherentProperty erdf :CoherentClass, whose instances properties
classes satisfy coherence. Thus, coherence decided per property
22. ASP well-known accepted knowledge representation formalism allows (through credulous
reasoning) definition concepts ranging space choices. feature enables compact
representation search optimization problems (Eiter, Ianni, Polleres, & Schindlauer, 2006).

71

fiAnalyti, Antoniou, Damasio, & Wagner

per class basis. Admitting incoherent models interesting combination
second preference criterion minimal incoherence (Herre et al., 1999).
future work also concerns support datatypes, including XSD datatypes,
extension predefined ERDF vocabulary adding useful constructs, possibly
accordance extensions ter Horst (2005b). also plan formally define
ERDF/XML syntax, briefly presented Section 7. Moreover, plan implement
ERDF inference engine.
Finally, would like mention success Semantic Web impossible without support modularity, encapsulation, information hiding, access control.
Modularity mechanisms syntactic restrictions merging knowledge bases Semantic Web explored Damasio et al. (2006). However, work, knowledge bases
expressed extended logic programs. future plans include extension ERDF
mechanisms allowing sharing knowledge different ERDF ontologies, along
lines proposed Damasio et al. (2006).

Acknowledgments
authors would like thank reviewers valuable comments. research
partially funded European Commission Swiss Federal Office Education Science within 6th Framework Programme project REWERSE
num. 506779 (www.rewerse.net).

Appendix A: RDF(S) Semantics
self-containment, Appendix, review definitions simple, RDF,
RDFS interpretations, well definitions satisfaction RDF graph RDFS
entailment. details, see W3C Recommendation RDF semantics (Hayes, 2004).
Let URI denote set URI references, PL denote set plain literals, L
denote set typed literals, respectively. vocabulary V subset URI PL L.
vocabulary RDF, VRDF , vocabulary RDFS, VRDF , shown Table
1 (Section 3).
Definition A.1 (Simple interpretation) simple interpretation vocabulary V
consists of:
non-empty set resources ResI , called domain universe I.
set properties P ropI .
vocabulary interpretation mapping IV : V URI ResI P ropI .
property extension mapping PT : P ropI P(ResI ResI ).
mapping ILI : V L ResI .
set literal values LV ResI , contains V PL.

define mapping: : V ResI P ropI that:
I(x) = IV (x), x V URI.
72

fiExtended RDF Semantic Foundation Rule Markup Languages

I(x) = x, x V PL.
I(x) = ILI (x), x V L.

Definition A.2 (Satisfaction RDF graph w.r.t. simple interpretation) Let
G RDF graph let simple interpretation vocabulary V . Let v
mapping v : Var (G) ResI . x Var (G), define [I + v](x) = v(x). x V ,
define [I + v](x) = I(x). define:
I, v |= G iff p(s, o) G, holds that: p V, s, V Var , I(p) P ropI ,
h[I + v](s), [I + v](o)i PT (I(p)).
satisfies RDF graph G, denoted |= G, iff exists mapping v :
Var (G) ResI I, v |= G.
rdf :type(rdf :type, rdf :Property)
rdf :type(rdf :subject, rdf :Property)
rdf :type(rdf :predicate, rdf :Property)
rdf :type(rdf :object, rdf :Property)
rdf :type(rdf :f irst, rdf :Property)
rdf :type(rdf :rest, rdf :Property)
rdf :type(rdf :value, rdf :Property)
rdf :type(rdf : i, rdf :Property), {1, 2, ...}
rdf :type(rdf :nil, rdf :List)

Table 2: RDF axiomatic triples
Definition A.3 (RDF interpretation) RDF interpretation vocabulary V
simple interpretation V VRDF , satisfies following semantic conditions:
1. x P ropI iff hx, I(rdf :Property)i PT (I(rdf :type)).
2. srdf :XMLLiteral V well-typed XML literal string,
ILI (srdf :XMLLiteral ) XML value s,
ILI (srdf :XMLLiteral ) LV ,
hILI (srdf :XMLLiteral ), I(rdf :XMLLiteral )i PT (I(rdf :type)).
3. srdf :XMLLiteral V ill-typed XML literal string
ILI (srdf :XMLLiteral ) ResI LV ,
hILI (srdf :XMLLiteral ), I(rdf :XMLLiteral )i 6 PT (I(rdf :type)).
4. satisfies RDF axiomatic triples, shown Table 2.

Definition A.4 (RDF entailment) Let G, G RDF graphs. say G RDFentails G (G |=RDF G ) iff every RDF interpretation I, |= G |= G .
Definition A.5 (RDFS interpretation) RDFS interpretation vocabulary V
RDF interpretation V VRDF VRDF , extended new ontological category
ClsI ResI classes, well class extension mapping CT : ClsI P(ResI ),
that:
73

fiAnalyti, Antoniou, Damasio, & Wagner

rdfs:domain(rdf :type, rdfs:Resource)
rdfs:domain(rdfs:domain, rdf :Property)
rdfs:domain(rdfs:range, rdf :Property)
rdfs:domain(rdfs:subPropertyOf , rdf :Property)
rdfs:domain(rdfs:subClassOf , rdfs:Class)
rdfs:domain(rdf :subject, rdf :Statement)
rdfs:domain(rdf :predicate, rdf :Statement)
rdfs:domain(rdf :object, rdf :Statement)
rdfs:domain(rdfs:member, rdfs:Resource)
rdfs:domain(rdf :f irst, rdf :List)
rdfs:domain(rdf :rest, rdf :List)
rdfs:domain(rdfs:seeAlso, rdfs:Resource)
rdfs:domain(rdfs:isDef inedBy, rdfs:Resource)
rdfs:domain(rdfs:comment, rdfs:Resource)
rdfs:domain(rdfs:label, rdfs:Resource)
rdfs:domain(rdfs:value, rdfs:Resource)
rdfs:range(rdf :type, rdfs:Class)
rdfs:range(rdfs:domain, rdfs:Class)
rdfs:range(rdfs:range, rdfs:Class)
rdfs:range(rdfs:subPropertyOf , rdf :Property)
rdfs:range(rdfs:subClassOf , rdfs:Class)
rdfs:range(rdf :subject, rdfs:Resource)
rdfs:range(rdf :predicate, rdfs:Resource)
rdfs:range(rdf :object, rdfs:Resource)
rdfs:range(rdfs:member, rdfs:Resource)
rdfs:range(rdf :f irst, rdfs:Resource)
rdfs:range(rdf :rest, rdf :List)
rdfs:range(rdfs:seeAlso, rdfs:Resource)
rdfs:range(rdfs:isDef inedBy, rdfs:Resource)
rdfs:range(rdfs:comment, rdfs:Literal)
rdfs:range(rdfs:label, rdfs:Literal)
rdfs:range(rdf :value, rdfs:Resource)
rdfs:subClassOf (rdf :Alt, rdfs:Container)
rdfs:subClassOf (rdf :Bag, rdfs:Container)
rdfs:subClassOf (rdf :Seq, rdfs:Container)
rdfs:subClassOf (rdfs:ContainerMembershipProperty, rdf :Property)
rdfs:subPropertyOf (rdfs:isDef inedBy, rdfs:seeAlso)
rdf :type(rdf :XMLLiteral , rdfs:Datatype)
rdfs:subClassOf (rdf :XMLLiteral , rdfs:Literal)
rdfs:subClassOf (rdfs:Datatype, rdfs:Class)
rdf :type(rdf : i, rdfs:ContainerMembershipProperty), {1, 2, ...}
rdfs:domain(rdf : i, rdfs:Resource), {1, 2, ...}
rdfs:range(rdf : i, rdfs:Resource), {1, 2, ...}

Table 3: RDFS axiomatic triples

74

fiExtended RDF Semantic Foundation Rule Markup Languages

1. x CT (y) iff hx, yi PT (I(rdf :type)).
2. ontological categories defined follows:
ClsI = CT (I(rdfs:Class)),
ResI = CT (I(rdfs:Resource)),
LV = CT (I(rdfs:Literal)).
3. hx, yi PT (I(rdfs:domain)) hz, wi PT (x) z CT (y).
4. hx, yi PT (I(rdfs:range)) hz, wi PT (x) w CT (y).
5. x ClsI hx, I(rdfs:Resource)i PT (I(rdfs:subClassOf )).
6. hx, yi PT (I(rdfs:subClassOf )) x, ClsI , CT (x) CT (y).
7. PT (I(rdfs:subClassOf )) reflexive transitive relation ClsI .
8. hx, yi PT (I(rdfs:subPropertyOf )) x, P ropI , PT (x) PT (y).
9. PT (I(rdfs:subPropertyOf )) reflexive transitive relation P ropI .
10. x CT (I(rdfs:Datatype)) hx, I(rdfs:Literal)i PT (I(rdfs:subClassOf )).
11. x CT (I(rdfs:ContainerM embershipP roperty))
hx, I(rdfs:member)i PT (I(rdfs:subPropertyOf )).
12. satisfies RDFS axiomatic triples, shown Table 3.

Definition A.6 (RDFS entailment) Let G, G RDF graphs. say G RDFSentails G (G |=RDF G ) iff every RDFS interpretation I, |= G |= G .


Appendix B: Proofs
Appendix, prove lemmas propositions presented main paper.
addition, provide Lemma B.1, used proofs. reduce size
proofs, eliminated namespace URIs VRDF VRDF VERDF .
Lemma B.1 Let F ERDF formula let partial interpretation
vocabulary V . Let u, u mappings u, u : Var (F ) ResI u(x) = u (x),
x FVar (F ). holds: I, u |= F iff I, u |= F .
Proof: prove proposition induction. Without loss generality, assume
appears front positive ERDF triples. Otherwise apply transformation
rules Definition 3.4, get equivalent formula satisfies assumption.
Let F = p(s, o). holds: I, u |= F iff p V , s, V Var , I(p) P ropI ,
h[I + u](s), [I + u](o)i PT (I(p)) iff p V , s, V Var , I(p) P ropI , h[I +
u ](s), [I + u ](o)i PT (I(p)) iff I, u |= p(s, o).
Let F = p(s, o). holds: I, u |= F iff p V , s, V Var , I(p) P ropI ,
h[I + u](s), [I + u](o)i PF (I(p)) iff p V , s, V Var , I(p) P ropI ,
h[I + u ](s), [I + u ](o)i PF (I(p)) iff I, u |= p(s, o).
Assumption: Assume lemma holds subformulas F .
show lemma holds also F .
Let F = G. holds: I, u |= F iff I, u |= G iff VG V I, u 6|= G iff VG V
I, u 6|= G iff I, u |= G iff I, u |= F .
75

fiAnalyti, Antoniou, Damasio, & Wagner

Let F = F1 F2 . holds: I, u |= F iff I, u |= F1 F2 iff I, u |= F1 I, u |= F2 iff
I, u |= F1 I, u |= F2 iff I, u |= F1 F2 iff I, u |= F .
Let F = x G. show (i) I, u |= F I, u |= F (ii) I, u |= F
I, u |= F .
(i) Let I, u |= F . Then, I, u |= xG. Thus, exists mapping u1 : Var (G) ResI
s.t. u1 (y) = u(y), Var (G) {x}, I, u1 |= G. Let u2 mapping u2 :
Var (G) ResI s.t. u2 (y) = u (y), Var (G) {x}, u2 (x) = u1 (x). Since u(z) =
u (z), z FVar (F ) x FVar (G), follows u1 (z) = u2 (z), z FVar (G).
Thus, I, u2 |= G. Therefore, exists mapping u2 : Var (G) ResI s.t. u2 (y) = u (y),
Var (G) {x}, I, u2 |= G. Thus, I, u |= x G, implies I, u |= F .
(ii) prove statement similarly (i) exchanging u u .
Let F = F1 F2 F = F1 F2 F = xG. prove, similarly
cases, I, u |= F iff I, u |= F .
Lemma 3.1. Let G ERDF graph let partial interpretation vocabulary
V . holds: |=GRAPH G iff |= formula(G).
Proof: Let G = {t1 , ..., tn } F = formula(G).
) Assume |=GRAPH G, show |= F . Since |=GRAPH G, follows
v : Var (G) ResI I, v |= ti , = 1, ..., n. Thus, v : Var (G) ResI
I, v |= t1 ...tn . implies u : Var (G) ResI I, u |= F . Since
FVar (F ) = , follows Lemma B.1 u : Var (G) ResI , holds I, u |= F .
Thus, |= F .
) Assume |= F , show |=GRAPH G. Since |= F , follows
v : Var (G) ResI holds I, v |= F . Thus, v : Var (G) ResI
I, v |= F . implies u : Var (G) ResI I, u |= t1 ...tn . Thus,
u : Var (G) ResI I, u |= ti , = 1, ..., n. Therefore, |=GRAPH G.
Proposition 3.1. Let ERDF interpretation vocabulary V let V =
V VRDF VRDF VERDF . Then,
1. p, s, V I(p) TProp , holds:
|= p(s, o) iff |= p(s, o) (equivalently, |= p(s, o) p(s, o)).
2. x, c V I(c) TCls , holds:
|= rdf :type(x, c) iff |= rdf :type(x, c)
(equivalently, |= rdf :type(x, c) rdf :type(x, c)).
Proof:
1) holds: |= p(s, o) iff 6|= p(s, o) iff hI(s), I(o)i 6 PT (p) iff (since p TProp )
hI(s), I(o)i PF (p) iff |= p(s, o). Therefore, |= p(s, o) iff |= p(s, o).
also show |= p(s, o) p(s, o). holds |= p(s, o) |= p(s, o).
implies |= p(s, o) |= p(s, o), thus, |= p(s, o) p(s, o).
2) proof similar proof 1) replacing p(s, o) type(x, c) TProp
TCls .
Proposition 3.2. Let G, G RDF graphs VG VERDF = VG VERDF =
. Then, G |=RDF G iff G |=ERDF G .
76

fiExtended RDF Semantic Foundation Rule Markup Languages

Proof:
) Let G |=ERDF G . show G |=RDF G . particular, let RDFS
interpretation vocabulary V s.t. |= G, show |= G .
Since |= G, holds v : Var (G) ResI s.t. I, v |= G. goal construct
ERDF interpretation J V s.t. J |= G. consider 1-1 mapping res : VERDF R,
R set disjoint ResI . Additionally, let V = V VRDF VRDF VERDF .
Based mapping res, construct partial interpretation J V follows:
ResJ = ResI res(VERDF ).
JV (x) = IV (x), x (V VERDF ) URI JV (x) = res(x), x VERDF .
define mapping: ILJ : V L ResJ that: ILJ (x) = ILI (x).
define mapping: J : V ResJ that:
J(x) = JV (x), x V URI.
J(x) = x, x V PL.
J(x) = ILJ (x), x V L.
define mapping PT J : ResJ P(ResJ ResJ ) follows:
(PT1) x, y, z ResI hx, yi PT (z) hx, yi PT J (z).
(PT2) hres(TotalClass), J(Class)i PT J (J(subClassOf )).
(PT3) hres(TotalProperty), J(Property)i PT J (J(subClassOf )).
Starting derivations (PT1), (PT2), (PT3), following rules
applied recursively, fixpoint reached:
(PT4) hx, yi PT J (J(domain)) hz, wi PT J (x)
hz, yi PT J (J(type)).
(PT5) hx, yi PT J (J(range)) hz, wi PT J (x)
hw, yi PT J (J(type)).
(PT6) hx, J(Class)i PT J (J(type))
hx, J(Resource)i PT J (J(subClassOf )).
(PT7) hx, yi PT J (J(subClassOf )) hx, J(Class)i PT J (J(type)).
(PT8) hx, yi PT J (J(subClassOf )) hy, J(Class)i PT J (J(type)).
(PT9) hx, yi PT J (J(subClassOf )) hz, xi PT J (J(type))
hz, yi PT J (J(type)).
(PT10) hx, J(Class)i PT J (J(type)) hx, xi PT J (J(subClassOf )).
(PT11) hx, yi PT J (J(subClassOf )) hy, zi PT J (J(subClassOf ))
hx, zi PT J (J(subClassOf )).
(PT12) hx, yi PT J (J(subPropertyOf )) hx, J(Property)i PT J (J(type)).
(PT13) hx, yi PT J (J(subPropertyOf )) hy, J(Property)i PT J (J(type)).
(PT14) hx, yi PT J (J(subPropertyOf )) hz, wi PT J (x)
hz, wi PT J (y).
77

fiAnalyti, Antoniou, Damasio, & Wagner

(PT15) hx, J(Property)i PT J (J(type)) hx, xi PT J (J(subPropertyOf )).
(PT16) hx, yi PT J (J(subPropertyOf )) hy, zi PT J (J(subPropertyOf ))
hx, zi PT J (J(subPropertyOf )).
(PT17) hx, J(Datatype)i PT J (J(type))
hx, J(Literal)i PT J (J(subClassOf )).
(PT18) hx, J(ContainerM embershipP roperty)i PT J (J(type))
hx, J(member)i PT J (J(subPropertyOf )).
reaching fixpoint, nothing else contained PT J (x), x ResJ .
P ropJ = {x ResJ | hx, J(Property)} PT J (J(type))}.
mapping PT J : P ropJ P(ResJ ResJ ) defined follows:
PT J (x) = PT J (x), x P ropJ .
LV J = {x ResJ | hx, J(Literal)i PT J (J(type))}.
mapping PF J : P ropJ P(ResJ ResJ ) defined follows:
(PF1) srdf :XMLLiteral V ill-typed XML-Literal
hILJ (srdf :XMLLiteral ), J(Literal)i PF J (J(type)).
(PF2) hJ(TotalClass), J(TotalClass)i PT J (J(type))
x ResJ {J(TotalClass)}, hx, J(TotalClass)i PF J (J(type)).
(PF3) hJ(TotalProperty), J(TotalProperty)i PT J (J(type))
x, ResJ , hx, yi PF J (J(TotalProperty)).
Starting derivations (PF1), (PF2), (PF3), following rules
applied recursively, fixpoint reached:
(PF4) hx, yi PT J (J(subClassOf )) hz, yi PF J (J(type))
hz, xi PF J (type).
(PF5) hx, yi PT J (J(subPropertyOf )) hz, wi PF J (y)
hz, wi PF J (x).
reaching fixpoint, nothing else contained PF J (x), x P ropJ .
continue, prove following lemma:
Lemma: x, y, x ResJ , hx, yi PT J (z) iff hx, yi PT J (z).
Proof :
) hx, yi PT J (z), definition PT J , follows immediately
hx, yi PT J (z).
) Let hx, yi PT J (z). Then, definition PT J , follows holds (i)
z P ropI (ii) w ResJ , s.t. hw, zi PT J (J(subPropertyOf )).
(i) Assume z P ropI . Then, hz, I(Property)i PT (I(type)). implies
hz, J(Property)i PT (J(type)). (PT1), follows hz, J(v)i PT J (J(type)).
Therefore, z P ropJ . definition PT J , follows hx, yi PT J (z).
78

fiExtended RDF Semantic Foundation Rule Markup Languages

(ii) Assume w ResJ s.t. hw, zi PT J (J(subPropertyOf )). Then, (PT13),
follows hz, J(Property)i PT J (J(type)). Therefore, z P ropJ . definition
PT J , follows hx, yi PT J (z).
End Lemma
Though mentioned explicitly, Lemma used throughout rest
proof.
show J partial interpretation V , enough show V PL LV J .
Let x V PL. Then, x LV . Thus, hx, I(Literal)i PT (I(type)). Due (PT1),
implies hx, J(Literal)i PT J (J(type)). Thus, x LV J .
Now, extend J ontological categories:
ClsJ = {x ResJ | hx, J(Class)i PT J (J(type))},
TCls J = {x ResJ | hx, J(TotalClass)i PT J (J(type))},
TProp J = {x ResJ | hx, J(TotalProperty)i PT J (J(type))}.
define CT J , CF J : ClsJ P(ResJ ) follows:
x CT J (y) iff hx, yi PT J (J(type)),
x CF J (y) iff hx, yi PF J (J(type)).
show J ERDF interpretation V . Specifically, show
J satisfies semantic conditions Definition 3.7 (ERDF interpretation) Definition
3.2 (Coherent ERDF interpretation).
First, show J satisfies semantic condition 2 Definition 3.7. start
proving ResJ = CT J (J(Resource)). Obviously,
CT J (J(Resource)) ResJ . Thus, enough prove ResJ CT J (J(Resource)).
Let x ResJ . Then, distinguish following cases:
Case 1) x ResI . Since RDFS interpretation, holds hx, I(Resource)i
PT (I(type)). Thus, holds hx, J(Resource)i PT J (J(type)), implies x
CT J (J(Resource)).
Case 2) x res(VERDF ). definition PT J , follows
hx, J(Resource)i PT J (J(type)). Thus, hx, J(Resource)i PT J (J(type)), implies
x CT J (J(Resource)).
Thus, ResJ = CT J (J(Resource)).
Additionally, easy see holds P ropJ = CT J (J(Property)), ClsJ =
CT J (J(Class)), LV J = CT J (J(Literal)), TCls J = CT J (J(TotalClass)),
TProp J = CT J (J(TotalProperty)).
show J satisfies semantic condition 3 Definition 3.7. Let hx, yi
PT J (J(domain)) hz, wi PT J (x). Then, (PT4) definition CT J ,
follows z CT J (y).
show J satisfies semantic condition 4 Definition 3.7. Let hx, yi
PT J (J(range)) hz, wi PT J (x). Then, (PT5) definition CT J ,
follows w CT J (y).
show J satisfies semantic condition 5 Definition 3.7. Let x
ClsJ . Thus, holds: hx, J(Class)i PT J (J(type)). (PT6), follows
hx, J(Resource)i PT J (J(subClassOf )).
79

fiAnalyti, Antoniou, Damasio, & Wagner

show J satisfies semantic condition 6 Definition 3.7. Let hx, yi
PT J (J(subClassOf )). Then, (PT7), (PT8), definition CT J , follows
x, ClsJ .
Let hx, yi PT J (J(subClassOf )). show CT J (x) CT J (y). particular,
let z CT J (x). Then, (PT9) definition CT J , follows z CT J (y).
Let hx, yi PT J (J(subClassOf )). show CF J (y) CF J (x). particular,
let z CF J (y). Then, (PF4) definition CF J , follows z CF J (x).
similar manner, prove J also satisfies semantic conditions 7, 8, 9,
10, 11 Definition 3.7.
continue rest proof, need make observations.
Consider mapping h : ResJ ResI , defined follows:

x ResI
x
I(Class)
x = res(TotalClass)
h(x) =

I(Property) x = res(TotalProperty)

Observation 1: hx, yi PT J (z) res(VERDF ) x = y.
Observation 2: x res(VERDF ) x P ropJ PT J (x) = .
Observation 3: hx, yi PT J (z) hh(x), h(y)i PT (h(z)).
Observation 4: x, y, z ResI hx, yi PT J (z) hx, yi PT (z)23 .
proof observations made induction. easy see observations
hold derivations (PT1), (PT2), (PT3). Assume observations
hold derivations obtained step k application fixpoint operator
PT J . Then, observations also hold derivations obtained step k + 1.
show J satisfies semantic condition 12 Definition 3.7. Let x
TCls J . Thus, hx, J(TotalClass)i PT J (J(type)). Observation 1, follows x =
J(TotalClass). (PF2), follows CT J (J(TotalClass))CF J (J(TotalClass)) =
ResJ . Thus, CT J (x) CF J (x) = ResJ .
show J satisfies semantic condition 13 Definition 3.7. Let x
TProp J . Thus, hx, J(TotalProperty)i PT J (J(type)). Observation 1, follows
x = J(TotalProperty). (PF3), follows PT J (J(TotalProperty))
PF J (J(TotalProperty)) = ResJ ResJ . Thus, PT J (x) PF J (x) = ResJ ResJ .
show J satisfies semantic condition 14 Definition 3.7.
Let srdf :XMLLiteral well-typed XML-Literal V ILJ (srdf :XMLLiteral )
= ILI (srdf :XMLLiteral ) XML value s. Additionally, since RDFS
interpretation V , holds: hILI (srdf :XMLLiteral ), I(XMLLiteral )i PT (I(type)).
Therefore, (PT1), follows hILJ (srdf :XMLLiteral ), J(XMLLiteral )i
PT J (J(type)).
show J satisfies semantic condition 15 Definition 3.7. Let
srdf :XMLLiteral V s.t. well-typed XML literal string. Assume
ILJ (srdf :XMLLiteral ) LV J . Then, hILJ (srdf :XMLLiteral ), J(Literal)i
PT J (J(type)). Observation 4, follows hILJ (srdf :XMLLiteral ), J(Literal)i
PT (J(type)). Therefore, follows hILI (srdf :XMLLiteral ), I(Literal)i
23. Note Observation 3 implies Observation 4.

80

fiExtended RDF Semantic Foundation Rule Markup Languages

PT (I(type)). Thus, ILI (srdf :XMLLiteral ) LV , impossible since
RDFS interpretation V . Therefore, ILJ (srdf :XMLLiteral ) ResJ LV J .
Additionally, (PF1), follows hILJ (srdf :XMLLiteral ), J(Literal)i
PF J (J(type)).
J also satisfies semantic condition 16 Definition 3.7, due (PT1). Finally, J satisfies
semantic condition 17, due (PT2) (PT3).
Thus, J ERDF interpretation V .
Now, show J coherent ERDF interpretation (Definition 3.2). Assume
case. Thus, z P ropJ s.t. PT J (z) PF J (z) 6= . Assume
hx, yi PT J (z) PF J (z), z. distinguish following cases:
Case 1) z res(VERDF ). Then, Observation 2, follows PT J (z) = ,
contradiction.
Case 2) res(VERDF ) z ResI . Then, holds:
(i) hz, res(TotalProperty)i PT J (J(subPropertyOf )),
(ii) hz, J(type)i PT J (J(subPropertyOf )) hx, yi PF J (J(type)).
Now, Observation 1 since z ResI , (i) impossible. Thus, hz, J(type)i
PT J (J(subPropertyOf )) hx, yi PF J (J(type)). implies
= res(TotalClass). Observation 1, follows x = res(TotalClass),
impossible since, due (PF2), hres(TotalClass), res(TotalClass)i 6 PF J (J(type)).
Case 3) x res(VERDF ) y, z ResI . Then, holds:
(i) hz, res(TotalProperty)i PT J (J(subPropertyOf )),
(ii) hz, J(type)i PT J (J(subPropertyOf )) hx, yi PF J (J(type)).
Now, Observation 1 since z ResI , (i) impossible. Thus, hz, J(type)i
PT J (J(subPropertyOf )) hx, yi PF J (J(type)). implies
= res(TotalClass), impossible, since ResI .
Case 4) x, y, z ResI . Then, x = ILJ (s), ill-typed XML-Literal
V , hz, J(type)i PT J (J(subPropertyOf )) hy, J(Literal)i PT J (J(subClassOf )).
Since hx, yi PT J (z), follows hx, yi PT J (J(type)). Since hy, J(Literal)i
PT J (J(subClassOf )), follows hx, J(Literal)i PT J (J(type)). Observation 4,
follows hILJ (s), J(Literal)i PT (J(type)). Therefore,
hILI (s), I(Literal)i PT (I(type)). implies ILI (s) LV , impossible since RDFS interpretation V .
Since cases lead contradiction, follows that:
z P ropJ , PT J (z) PF J (z) = .
show J, v |= G. Let p(s, o) G. Since I, v |= G, holds
p V , s, V Var . Note that, due (PT1), holds P ropI P ropJ . Since
p 6 VERDF , holds J(p) = I(p) P ropI P ropJ . Since s, 6 VERDF , holds
[I + v](s) = [J + v](s) [I + v](o) = [J + v](o). Since I, v |= G, holds h[I + v](s), [I +
v](o)i PT (I(p)). Thus, h[J + v](s), [J + v](o)i PT (J(p)). (PT1), follows
h[J + v](s), [J + v](o)i PT J (J(p)). Thus, J, v |= G, implies J |= G.
Since J ERDF interpretation G |=ERDF G , follows J |= G . Thus,
u : Var (G ) ResJ = ResI res(VERDF ) s.t. J, u |= G . define mapping
u : Var (G ) ResI follows:
81

fiAnalyti, Antoniou, Damasio, & Wagner


u(x) ResI
u(x)
I(Class)
u(x) = res(TotalClass)
u (x) =

I(Property) u(x) = res(TotalProperty)

show I, u |= G . Let p(s, o) G . Since J |= G VG VERDF = ,
follows p V VRDF VRDF , s, V VRDF VRDF Var , J(p)
P ropJ . Thus, hJ(p), J(type)i PT J (J(Property)), implies (since p 6 VERDF )
hI(p), I(type)i PT J (I(Property). Due Observation 4, follows hI(p), I(type)i
PT (I(Property). Thus, I(p) P ropI . Additionally, holds: h[J + u](s), [J + u](o)i
PT J (J(p)). want show h[I + u ](s), [I + u ](o)i PT (I(p)).
Case 1) holds: (i) Var (G ) u(s) 6 res(VERDF ) (ii) Var (G )
u(o) 6 res(VERDF ).
Then, [J + u](s) = [J + u ](s) = [I + u ](s) ResI , [J + u](o) = [J + u ](o) = [I +
u ](o) ResI , J(p) = I(p) ResI . Thus, h[J + u](s), [J + u](o)i PT J (J(p)) implies
h[I + u ](s), [I + u ](o)i PT J (I(p)). Observation 4, latter implies
h[I + u ](s), [I + u ](o)i PT (I(p)).
Case 2) holds: (i) Var (G ) u(s) res(VERDF ) (ii) Var (G )
u(o) 6 res(VERDF ).
Assume u(s) = res(TotalClass), [J + u](o) = y, J(p) = z. y, z ResI .
Additionally, I(p) = J(p) = z [I + u ](o) = [J + u](o) = y. Thus, h[I + u ](s), [I +
u ](o)i = hI(Class), yi. holds hres(TotalClass), yi PT J (z). Due Observation 3,
holds hI(Class), yi PT (z). Thus, h[I + u ](s), [I + u ](o)i = hI(Class), yi PT (z) =
PT (I(p)).
Similarly, u(s) = res(TotalProperty), prove h[I+u ](s), [I+u ](o)i PT (I(p)).
Case 3) holds: Var (G ) u(o) res(VERDF ). Then, Observation 1,
follows Var (G ) u(s) = u(o). Assume u(o) = res(TotalClass),
J(p) = z. Then, z ResI I(p) = J(p) = z. Additionally, h[I + u ](s), [I + u ](o)i =
hI(Class), I(Class)i. holds hres(TotalClass), res(TotalClass)i PT J (z). Due Observation 3, follows hI(Class), I(v)i PT (z). Thus, h[I + u ](s), [I + u ](o)i =
hI(Class), I(Class)i PT (z) = PT (I(p)).
Similarly, u(o) = res(TotalProperty), prove h[I+u ](s), [I+u ](o)i PT (I(p)).
cases, holds h[I + u ](s), [I + u ](o)i = PT (I(p)), follows I, u |= G ,
implies |= G .
) Let G |=RDF G . show G |=ERDF G . Let ERDF interpretation
vocabulary V , |= G. Thus, u : Var (G) ResI s.t. I, u |= G.
show |= G .
define V = V VRDF VRDF VERDF . Based I, construct RDFS interpretation J V that: ResJ = ResI , P ropJ = P ropI , LV J = LV , ClsJ =
ClsI , JV (x) = IV (x), x V URI, PT J (x) = PT (x), x P ropJ , ILJ (x) =
ILI (x), x V L, CT J (x) = CT (x), x ClsJ .
show J indeed RDFS interpretation V .
First, show J satisfies semantic condition 1 Definition A.3 (Appendix
A, RDF interpretation). holds: x P ropJ iff x P ropI iff x CT (I(Property)) iff
hx, I(Property)i PT (I(type)) iff hx, J(Property)i PT J (J(type)).
82

fiExtended RDF Semantic Foundation Rule Markup Languages

show J satisfies semantic condition 2 Definition A.3.
Let srdf :XMLLiteral V well-typed XML literal string. Then,
follows definition J fact ERDF interpretation V
ILJ (srdf :XMLLiteral ) XML value s, ILJ (srdf :XMLLiteral )
CT J (J(XMLLiteral )). show ILJ (srdf :XMLLiteral ) LV J . Since
ERDF interpretation, ILI (srdf :XMLLiteral ) CT (I(XMLLiteral )). Additionally,
hI(XMLLiteral ), I(Literal)i PT (I(subClassOf )). Therefore, ILI (srdf :XMLLiteral )
CT (I(Literal)), thus, ILI (srdf :XMLLiteral ) LV . last statement implies
ILJ (srdf :XMLLiteral ) LV J .
show J satisfies semantic condition 3 Definition A.3.
Let srdf :XMLLiteral V ill-typed XML literal string. Then,
follows definition J fact ERDF interpretation V
ILJ (srdf :XMLLiteral ) ResJ LV J . show
hILJ (srdf :XMLLiteral ), J(XMLLiteral )i 6 PT J (J(type)). Assume
hILJ (srdf :XMLLiteral ), J(XMLLiteral )i PT J (J(type)). Then,
hILI (srdf :XMLLiteral ), I(XMLLiteral )i PT (I(type)). Thus,
ILI (srdf :XMLLiteral ) CT (I(XMLLiteral )). Since holds
hI(XMLLiteral ), I(Literal)i PT (I(subClassOf )), follows
ILI (srdf :XMLLiteral ) CT (I(Literal)). Thus, ILI (srdf :XMLLiteral ) LV ,
impossible since ERDF interpretation V . Therefore,
hILJ (srdf :XMLLiteral ), J(XMLLiteral )i 6 PT J (J(type)).
easy see J satisfies semantic condition 4 Definition A.3
semantic conditions Definition A.5 (Appendix A, RDFS Interpretation). Therefore, J
RDFS interpretation V .
show J, u |= G. Let p(s, o) G. Since |= G, holds p V ,
s, V Var , J(p) = I(p) P ropI = P ropJ . holds: h[J + u](s), [J + u](o)i
PT J (J(p)) iff h[I + u](s)), [I + u](o)i P ropI (I(p)), true, since I, u |= G. Thus,
J, u |= G, implies J |= G. Since G |=RDF G , follows J |= G . Thus,
v : Var (G ) ResJ s.t. J, v |= G .
show |= G . Let p(s, o) G . Since J, v |= G , holds p V ,
s, V Var , I(p) = J(p) P ropJ = P ropI . holds: h[I + v](s), [I + v](o)i
PT (I(p)) iff h[J + v](s), [J + v](o)i PT J (J(p)), true, since J, v |= G . Thus,
I, v |= G , implies |= G .
Proposition 4.1. Let G ERDF graph let F ERDF formula
VF skG (Var (G)) = . holds: G |=ERDF F iff sk(G) |=ERDF F .
Proof:
) Let G |=ERDF F . show sk(G) |=ERDF F . Let ERDF interpretation
vocabulary V s.t. |= sk(G). show |= G. define V = V VRDF
VRDF VERDF . Additionally, define total function u : Var (G) ResI s.t. u(x) =
IV (skG (x)), x Var (G). Moreover, define total function u : V Var (G) V s.t.
u (x) = skG (x), x Var (G) u (x) = x, otherwise.
Let p(s, o) G. Then, p V , s, V Var , I(p) P ropI . holds: h[I+u](s), [I+
u](o)i PT (I(p)) iff hI(u (s)), I(u (o))i PT (I(p)), true, since p(u (s), u (o))
sk(G) |= sk(G). Thus, I, u |= p(s, o).
83

fiAnalyti, Antoniou, Damasio, & Wagner

Let p(s, o) G. Then, p V , s, V Var , I(p) P ropI . holds:
h[I + u](s), [I + u](o)i PF (I(p)) iff hI(u (s)), I(u (o))i PF (I(p)), true, since
p(u (s), u (o)) sk(G) |= sk(G). Thus, I, u |= p(s, o).
Therefore, |= G. Since G |=ERDF F , follows |= F .
) Let sk(G) |=ERDF F . show G |=ERDF F . Let ERDF interpretation
vocabulary V |= G. show |= F . Since |= G, total
function u : Var (G) ResI s.t. I, u |= G. define V = V VRDF VRDF VRDF .
construct ERDF interpretation J V skG (Var (G)) follows: ResJ = ResI , P ropJ =
P ropI , LV J = LV , ClsJ = ClsI . define JV : (V skG (Var (G))) URI ResJ ,
1
follows: JV (x) = IV (x), x V URI JV (x) = u(skG
(x)), x skG (Var (G)).
Moreover, PT J (x) = PT (x), x P ropJ , PF J (x) = PF (x), x P ropJ , ILJ (x) =
ILI (x), x V L, CT J (x) = CT (x), x ClsJ , CF J (x) = CF (x), x ClsJ .
Since ERDF interpretation V , easy see J indeed ERDF interpretation V skG (Var (G)). show J |= sk(G). First, define total func1
tion g : V skG (Var (G)) V Var (G) follows: g(x) = skG
(x), x skG (Var (G))
g(x) = x, otherwise. Let p(s, o) sk(G). Since |= G, follows p V ,
s, V Var , J(p) = I(p) P ropI = P ropJ . holds J(s) = [I + u](g(s)),
J(o) = [I + u](g(o)), J(p) = I(p). Therefore, holds: hJ(s), J(o)i PT J (J(p)) iff
h[I + u](g(s)), [I + u](g(o))i PT (I(p)), holds since p(g(s), g(o)) G I, u |= G.
Let v : {} ResJ . follows J, v |= p(s, o). Let p(s, o) sk(G). show
J, v |= p(s, o), similar manner. Therefore, J |= sk(G).
Since sk(G) |=ERDF F , follows J |= F . show |= F . define
V = V VRDF VRDF VERDF . Note ResJ = ResI .


Lemma: every mapping u : Var (F ) ResJ , holds J, u |= F iff I, u |= F .
Proof: prove Lemma induction. Without loss generality, assume
appears front positive ERDF triples. Otherwise apply transformation
rules Definition 3.4, get equivalent formula satisfies assumption.
Let F = p(s, o). Assume J, u |= F . Since VF skG (Var (G)) = , follows
p V , s, V Var , J(p) = I(p) P ropI = P ropJ . Since h[J + u](s), [J + u](o)i
PT J (J(p)), follows h[I + u](s), [I + u](o)i PT (I(p)). Therefore, I, u |= F .
Assume I, u |= F . follows p V , s, V Var , J(p) = I(p) P ropI =
P ropJ . Since h[I + u](s), [I + u](o)i PT (I(p)), follows h[J + u](s), [J + u](o)i
PT J (J(p)). Therefore, J, u |= F .
Let F = p(s, o). Similarly, prove J, u |= F iff I, u |= F .
Assumption: Assume lemma holds subformulas F .
show lemma holds also F .
Let F = G. holds: I, u |= F iff VG V I, u 6|= G iff VG V J, u 6|= G iff
J, u |= F .
Let F = F1 F2 . holds: I, u |= F iff I, u |= F1 I, u |= F2 iff J, u |= F1
J, u |= F2 iff J, u |= F .
84

fiExtended RDF Semantic Foundation Rule Markup Languages

Let F = x G. holds: I, u |= F iff I, u |= x G iff v : Var (G) ResI
s.t. v(y) = u(y), Var (G) {x} I, v |= G iff v : Var (G) ResJ s.t.
v(y) = u(y), Var (G) {x} J, v |= G iff J, u |= x G iff J, u |= F .
Let F = F1 F2 F = F1 F2 F = xG. prove, similarly
cases, I, u |= F iff J, u |= F .
End lemma
Since J |= F , follows every mapping u : Var (F ) ResJ ,
J, u |= F .
Therefore, follows Lemma fact ResJ = ResI every mapping
u : Var (F ) ResI , I, u |= F . Thus, |= F .
Proposition 4.2. Let = hG, P ERDF ontology let I, J H (O). Let
p TProp TProp J . PT (p) 6= PT J (p) PF (p) 6= PF J (p) 6 J J 6 I.
Proof: Assume PT (p) 6= PT J (p). Now, assume J. Then, PT (p) PT J (p)
PF (p) PF J (p). Since I, J H (O) p TProp TProp J , holds
H
PF (p) = ResH
PT (p) PF J (p) = ResO PT J (p). Thus, PF (p) PF J (p),
contradiction. Thus, 6 J. Similarly, prove J 6 I.
Assume PF (p) 6= PF J (p). Then, prove 6 J J 6 I,
similar manner.
Proposition 5.1. Let = hG, P ERDF ontology let Mst (O). holds
MH (O).
Proof: Let Mst (O). Obviously, H (O) |= sk(G). show
|= r, r P . Let r P . Let v mapping v : Var (r) ResH
s.t. M, v |= Cond(r).
enough show M, v |= Concl(r).
mapping u : X ResH (O), X Var , define mapping u : X
VO follows:

u(x) u(x) xml value well-typed XML literal VO
u (x) =

u(x) xml value well-typed XML literal VO




Let x VO , define xu = x. Let x X, define xu = u (x). Let F L(VO )

{true, f alse} FVar (F ) X, define F u formula results F

replacing free variable F u (x). easy see holds: Concl(r)v

Concl(r)v [r]VO [P ]VO .
Lemma: Let F ERDF formula VO let u mapping u : Var (F ) ResH
O.

holds: M, u |= F iff M, u |= F u .
Proof: prove lemma induction. Without loss generality, assume
appears front positive ERDF triples. Otherwise apply transformation
rules Definition 3.4, get equivalent formula satisfies assumption.
Let F = p(s, o). holds: M, u |= F iff M, u |= p(s, o) iff h[M + u](s), [M + u](o)i



PT (M (p)) iff h[M + u](su ), [M + u](ou )i PT (M (p)) iff M, u |= p(s, o)u .
Let F = p(s, o). holds: M, u |= F iff M, u |= p(s, o) iff h[M + u](s), [M + u](o)i



PF (M (p)) iff h[M + u](su ), [M + u](ou )i PF (M (p)) iff M, u |= (p(s, o))u .
Assumption: Assume lemma holds subformulas F .
show lemma holds also F .
85

fiAnalyti, Antoniou, Damasio, & Wagner



Let F = G. holds: M, u |= F iff M, u |= G iff M, u 6|= G iff M, u 6|= Gu iff


M, u |= Gu iff M, u |= F u .
Let F = F1 F2 . holds: M, u |= F iff M, u |= F1 F2 iff M, u |= F1 M, u |= F2 iff




M, u |= F1u M, u |= F2u iff M, u |= (F1 F2 )u iff M, u |= F u .
Let F = xG. holds: M, u |= F iff exists mapping u1 : Var (G) ResH
s.t.
u1 (y) = u(y), Var (G) {x} s.t. M, u1 |= G iff exists mapping u1 : Var (G)
u1 iff exists mapping
ResH
s.t. u1 (y) = u(y), Var (G) {x} s.t. M, u1 |= G

u1 : Var (G) ResH
u1 |= (xG)u1 iff (since
s.t. u1 (y) = u(y), Var(G) {x} s.t. M,

u1 (y) = u (y), FVar (xG)) M, u |= (xG)u iff M, u |= F u .
Let F = F1 F2 F = F1 F2 F = xG. prove, similarly

cases, M, u |= F iff M, u |= F u .
End Lemma
First assume Cond(r) 6= true. Then, Cond(r) L(VO ) thus, Cond(r) ERDF

formula VO . Since M, v |= Cond(r), follows Lemma M, v |= Cond(r)v .


since FVar (Cond(r)v ) = , follows Lemma B.1 |= Cond(r)v . Since

Mst (O), follows |= Concl(r)v . Thus, Concl(r) 6= f alse Concl(r)

L(VO |{}). since FVar (Concl(r)v ) = , follows lemma B.1 M, v |=

Concl(r)v . Since Concl(r) ERDF formula VO , follows Lemma
M, v |= Concl(r).

Assume Cond(r) = true. Then, |= Cond(r)v . Since Mst (O),

follows |= Concl(r)v . Therefore, Concl(r) 6= f alse, prove
M, v |= Concl(r).
Therefore, |= r, r P .
Proposition 5.2. Let = hG, P ERDF ontology,
rdfs:subClassOf (rdf :Property, erdf :TotalProperty) G. Then, Mst (O) = MH (O).
Proof: Proposition 5.1, follows Mst (O) MH (O). show
MH (O) Mst (O). Let MH (O). follows |= sk(G). show
minimal({I H (O) | |= sk(G)}).
Let J H (O) s.t. J |= sk(G) J . show J = . Since J ,
follows P ropJ P ropM p P ropJ , holds PT J (p) PT (p)
PF J (p) PF (p). Let p P ropJ . Since J |= sk(G), follows P ropJ TProp J .
Thus, p TProp J . Assume PT J (p) 6= PT (p). Then, hx, yi PT (p)
s.t. hx, yi 6 PT J (p). Then, hx, yi PF J (p). Thus, hx, yi PF (p), impossible,
since hx, yi PT (p). Thus, PT J (p) = PT (p). Similarly, prove PF J (p) =
PF (p). Therefore, p P ropJ , holds PT J (p) = PT (p) PF J (p) = PF (p).
show P ropJ = P ropM . holds P ropJ ={x ResH
| hx, Propertyi
PT J (type)} = {x ResH
|
hx,
Propertyi

PT
(type)}
=P
rop
.
Based
results,



fact J, H (O), follows J = . Therefore, minimal({I
H (O) | |= sk(G)}).
show minimal({I H (O) | |= Concl(r),
r P[M,M ] }). Since MH (O) follows {I H (O) |
|= Concl(r), r P[M,M ] }. Let J {I H (O) | |= Concl(r),
r P[M,M ] } J . Since J , follows P ropM P ropJ ,
p P ropM , holds PT (p) PT J (p) PF (p) PF J (p). Since J ,
86

fiExtended RDF Semantic Foundation Rule Markup Languages

follows P ropJ P ropM , p P ropJ , holds PT J (p) PT (p)
PF J (p) PF (p). Therefore, follows P ropM = P ropJ , p P ropM ,
holds PT (p) = PT J (p) PF (p) = PF J (p). Based result, fact
J, H (O), follows J = .
Thus, minimal({I H (O) | |= Concl(r), r P[M,M ] }).
Since satisfies conditions Definition 5.1 (Stable Model), follows
st
(O). Thus, holds MH (O) Mst (O).
Therefore, MH (O) = Mst (O).
Proposition 6.2. Let G ERDF graph let F ERDF formula
VF skG (Var (G)) = . holds:
1. F ERDF d-formula hG, |=st F G |=ERDF F .
2. G |=ERDF F hG, |=st F .
Proof:
1) Let hG, |=st F . show sk(G) |=ERDF F . Let ERDF interpretation
vocabulary V s.t. |= sk(G). show |= F . define V = V VRDF
VRDF VERDF .
Let = hG, i. Based I, construct partial interpretation J VO follows:
ResJ = ResH
O.
JV (x) = x, x VO URI.
define mapping: ILJ : VO L ResJ that:
ILJ (x) = x, x typed literal VO well-typed XML literal,
ILI (x) XML value x, x well-typed XML literal VO .
define mapping: J : VO ResJ that:
J(x) = JV (x), x VO URI.
J(x) = x, x VO PL.
J(x) = ILJ (x), x VO L.
P ropJ = {x ResJ | x VO , J(x ) = x I(x ) P ropI }.
mapping PT J : P ropJ P(ResJ ResJ ) defined follows:
x, y, z VO , holds:
hJ(x), J(y)i PT J (J(z)) iff hI(x), I(y)i PT (I(z)).
define mapping PF J : P ropJ P(ResJ ResJ ) follows:
x, y, z VO , holds:
hJ(x), J(y)i PF J (J(z)) iff hI(x), I(y)i PF (I(z)).
LV J = {x ResJ | hx, J(Literal)i PT J (J(type))}.
87

fiAnalyti, Antoniou, Damasio, & Wagner

show J partial interpretation, enough show VO PL LV J .
Let x VO PL. Then, x LV . Thus, hx, I(Literal)i PT (I(type)). implies
hx, J(Literal)i PT J (J(type)). Thus, x LV J .
Now, extend J ontological categories:
ClsJ = {x ResJ | hx, J(Class)i PT J (J(type))},
TCls J = {x ResJ | hx, J(TotalClass)i PT J (J(type))},
TProp J = {x ResJ | hx, J(TotalProperty)i PT J (J(type))}.
define mappings CT J , CF J : ClsJ P(ResJ ) follows:
x CT J (y) iff hx, yi PT J (J(type)),
x CF J (y) iff hx, yi PF J (J(type)).
show J ERDF interpretation VO . First, show
J satisfies semantic condition 2 Definition 3.7 (ERDF Interpretation), number
steps:
Step 1: Here, prove ResJ = CT J (J(Resource)). Obviously, CT J (J(Resource))
ResJ . show ResJ CT J (J(Resource)). Let x ResJ . Then,
x VO J(x ) = x. want show hJ(x ), J(Resource)i PT J (J(type)).
holds: hJ(x ), J(Resource)i PT J (J(type)) iff hI(x ), I(Resource)i PT (I(type)),
true, since ERDF interpretation satisfies sk(G) I(x ) ResI .
Thus, x = J(x ) CT J (J(ResourceResource)).
Therefore, ResJ = CT J (J(Resource)).
Step 2: Here, prove P ropJ = CT J (J(Property)). show P ropJ
CT J (J(Property)). Let x P ropJ . Then, x VO J(x ) = x
I(x ) P ropI . want show hJ(x ), J(Property)i PT J (J(type)). holds:
hJ(x ), J(Property)i PT J (J(type)) iff hI(x ), I(Property)i PT (I(type)),
true, since I(x ) P ropI . Thus, x = J(x ) CT J (J(Property)).
Therefore, P ropJ CT J (J(Property)).
show CT J (J(Property)) P ropJ . Let x CT J (J(Property)). Then,
x VO J(x ) = x. holds hJ(x ), J(Property)i PT J (J(type)), implies
hI(x ), I(Property)i PT (I(type)). Thus, I(x ) P ropI x P ropJ .
Therefore, CT J (J(Property)) P ropJ .
Step 3: definition, holds ClsJ = CT J (J(Class)), LV J = CT J (J(Literal)), TCls J =
CT J (J(TotalClass)) TProp J = CT J (J(TotalProperty)).
show J satisfies semantic condition 3 Definition 3.7 (ERDF Interpretation). Let hx, yi PT J (J(domain)) hz, wi PT J (x). show
z CT J (y). x , VO J(x ) = x, J(y ) = y. Thus,
hJ(x ), J(y )i PT J (J(domain)). Additionally, z , w VO J(z ) =
z, J(w ) = w. Thus, hJ(z ), J(w )i PT J (J(x )). Then, hI(x ), I(y )i PT (I(domain))
hI(z ), I(w )i PT (I(x )). Since ERDF interpretation, hI(z ), I(y )i
PT (I(type)). Thus, hJ(z ), J(y )i PT J (J(type)) z CT J (y).
similar manner, prove J also satisfies rest semantic conditions
Definition 3.7. Thus, J ERDF interpretation VO .
Moreover, show J coherent ERDF interpretation (Definition 3.2).
Assume case. Thus, z P ropJ s.t. PT J (z) PF J (z) 6= .
Thus, x, ResJ s.t. hx, yi PT J (z) PF J (z), z. Then,
88

fiExtended RDF Semantic Foundation Rule Markup Languages

x , , z VO s.t. J(x ) = x, J(y ) = y, J(z ) = z. holds: hJ(x ), J(y )i PT J (J(z ))
hJ(x ), J(y )i PF J (J(z )). Thus, hI(x ), I(y )i PT (I(z )) hI(x ), I(y )i
PF (I(z )). impossible, since (coherent) ERDF interpretation. Therefore,
J also coherent ERDF interpretation.
Thus, J H (O).
show J |= sk(G). Let p(s, o) sk(G). holds p, s, VO . Since
|= sk(G), holds I(p) P ropI . Thus, hI(p), I(Property)i PT (I(type)), implies
hJ(p), J(Property)i PT J (J(type)). this, follows J(p) P ropJ .
holds: hJ(s), J(o)i PT J (J(p)) iff hI(s), I(o)i PT (I(p)). last statement true
since |= sk(G). Let u : {} ResH
. Then, J, u |= p(s, o). Let p(s, o) sk(G).
show J, u |= p(s, o), similar manner. Thus, J |= sk(G).
Now, Definition 5.1 (Stable Model) fact J |= sk(G), follows
K Mst (O) s.t. K J. fact |=st F , follows K |= F .
Since F ERDF d-formula, holds
F = (?x1 , ..., ?xk1 F1 ) ... (?x1 , ..., ?xkn Fn ),
Fi = t1 ... tmi tj , j = 1, ..., mi , ERDF triple. Thus,
{1, ..., n} u : Var (Fi ) ResH
s.t. K, u |= Fi .
show J, u |= Fi .
Let p(s, o) {t1 , ..., tmi }. Since K ERDF interpretation VO , K, u |= Fi ,
P ropK P ropJ , follows p VO , s, VO Var , J(p) = K(p) P ropK
P ropJ . Additionally, h[K +u](s), [K +u](o)i PT K (p). Since h[J +u](s), [J +u](o)i = h[K +
u](s), [K + u](o)i PT K (p) PT J (p), follows h[J + u](s), [J + u](o)i PT J (p).
Thus, J, u |= p(s, o).
Let p(s, o) {t1 , ..., tmi }. Since K ERDF interpretation VO , K, u |= Fi ,
P ropK P ropJ , follows p VO , s, VO Var , J(p) = K(p) P ropK
P ropJ . Additionally, h[K +u](s), [K +u](o)i PF K (p). Since h[J +u](s), [J +u](o)i = h[K +
u](s), [K + u](o)i PF K (p) PF J (p), follows h[J + u](s), [J + u](o)i PF J (p).
Thus, J, u |= p(s, o).
define total function u : VFi Var (Fi ) VO , follows:

u(x) x Var (Fi )




u(x) xml value well-typed XML literal VO


x Var (Fi )
u (x) =


u(x) xml value well-typed XML literal VO



x
otherwise

Moreover, define total function u : Var (Fi ) ResI s.t. u (x) = I(u (x)).
show I, u |= Fi .
Let p(s, o) {t1 , ..., tmi }. Then, p VFi s, VFi Var . Since J, u |= Fi , follows
VFi VO . Therefore, VFi Vsk(G) VRDF VRDF VERDF V . Thus, p V
s, V Var .
show I(p) P ropI . holds:
hI(p), I(Property)i PT (I(type)) iff
hJ(p), J(Property)i PT J (J(type)), holds since J, u |= Fi .
89

fiAnalyti, Antoniou, Damasio, & Wagner

want show h[I +v ](s), [I +v ](o)i PT (I(p)). Note x VFi , holds:
[I + u ](x) = I(u (x)) = I(x) J(u (x)) = [J + u](x) = J(x). Moreover, x Var (Fi ),
holds: [I + u ](x) = I(u (x)) J(u (x)) = [J + u](x) (recall definition J(.)).
Therefore, holds:
h[I + u ](s), [I + u ](o)i PT (I(p)) iff
hI(u (s)), I(u (o))i PT (I(p)) iff
hJ(u (s)), J(u (o))i PT J (J(p)) iff
h[J + u](s), [J + u](o)i PT J (J(p)), true since J, u |= Fi . Thus, I, u |= p(s, o).
Let p(s, o) {t1 , ..., tmi }. show I, u |= p(s, o), similar manner.
Thus, I, u |= Fi , implies I, u |= ?x1 , ..., ?xki Fi . Thus, I, u |= F . Now,
follows Lemma B.1 |= F .
Thus, sk(G) |=ERDF F . Now, follows Proposition 4.1 G |=ERDF F .
2) Let G |=ERDF F . follows Proposition 4.1 sk(G) |=ERDF F . show
hG, |=st F . particular, let = hG, let Mst (O). Note
ERDF interpretation VO , |= sk(G). Since sk(G) |=ERDF F , follows
|= F .
Proposition 8.1 Let instance unbounded tiling problem. holds:
1. solution iff OD {false FD } stable model.
2. solution iff OD 6|=st FD .
Proof:
1) statement follows easily statement 2).
2) ) Let solution D. Since denumerable, exists bijective
function : . Consider Herbrand interpretation OD that:
1. CTI (Tile) = CTI (HasRight) = CTI (HasAbove) = {rdf : | }
CFI (T ile) = CFI (HasRight) = CFI (HasAbove) = .
2. P TI (id ) = {hx, xi | x VO } P FI (id ) = .
3. P TI (HConstraint) = H P FI (HConstraint) = .
4. P TI (VConstraint) = V P FI (VConstraint) = .
5. P TI (Type) = {hrdf : (i, j), (i, j)i | i, j } P FI (Type) = .
6. P TI (right) = {hrdf : (i, j), rdf : (i + 1, j)i | i, j }
P FI (right) = {hrdf : i, rdf : ji | i, j hrdf : i, rdf : ji 6 P TI (right)}.
7. P TI (above) = {hrdf : (i, j), rdf : (i, j + 1)i | i, j }
P FI (above) = {hrdf : i, rdf : ji | i, j hrdf : i, rdf : ji 6 P TI (above)}.

easy see stable model OD 6|= FD . Thus, OD 6|=st FD .
) Let = hT , H, V i, = {T1 , ..., Tn }. Assume OD 6|=st FD let
stable model OD = hG, P 6|= FD . Obviously, CTI (Tile) = {rdf : | }.
Due rule sets (2)-(4) P since OD 6|=st FD , holds starting tile rdf : 0
90

fiExtended RDF Semantic Foundation Rule Markup Languages

placing tiles according P TI (right) P TI (above) relations, grid formed.
define (i, j) = k, i, j, k , iff tile rdf : k placed hi, ji position
previous grid. Note total function. Due rule set (1) P , tile
assigned unique type = {T1 , ..., Tn }. Due rule set (5) P , type assignment
satisfies horizontal vertical adjacency constraints D. Thus, solution
: , (i, j) = iff hrdf : (i, j), P TI (T ype). Since total
function and, k , tile rdf : k assigned unique type , follows
total function.

References
Alferes, J. J., Damasio, C. V., & Pereira, L. M. (1995). Logic Programming System
Non-monotonic Reasoning. Special Issue Journal Automated Reasoning,
14 (1), 93147.
Alferes, J. J., Damasio, C. V., & Pereira, L. M. (2003). Semantic Web Logic Programming Tools. International Workshop Principles Practice Semantic Web
Reasoning (PPSWR03), pp. 1632.
Analyti, A., Antoniou, G., Damasio, C. V., & Wagner, G. (2004). Negation Negative
Information W3C Resource Description Framework. Annals Mathematics,
Computing & Teleinformatics (AMCT), 1 (2), 2534.
Analyti, A., Antoniou, G., Damasio, C. V., & Wagner, G. (2005). Stable Model Theory
Extended RDF Ontologies. 4th International Semantic Web Conference (ISWC2005), pp. 2136.
Antoniou, G., Bikakis, A., & Wagner, G. (2004). System Nonmonotonic Rules
Web. 3rd International Workshop Rules Rule Markup Languages
Semantic Web (RULEML03), pp. 2336.
Antoniou, G., Billington, D., Governatori, G., & Maher, M. J. (2001). Representation
Results Defeasible Logic. ACM Transactions Computational Logic (TOCL),
2 (2), 255287.
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). Description Logic Handbook: Theory, Implementation, Applications.
Cambridge University Press.
Bassiliades, N., Antoniou, G., & Vlahavas, I. P. (2004). DR-DEVICE: Defeasible Logic
System Semantic Web. 2nd International Workshop Principles
Practice Semantic Web Reasoning (PPSWR04), pp. 134148.
Beckett, D. (2004). RDF/XML Syntax Specification (Revised). W3C Recommendation.
Available http://www.w3.org/TR/2004/REC-rdf-syntax-grammar-20040210/.
Berger, R. (1966). Undecidability Dominoe Problem. Memoirs American
Mathematical Society, 66, 172.
Berners-Lee, T. (1998). Design Issues - Architectual Philosophical Points. Personal
notes. Available http://www.w3.org/DesignIssues.
91

fiAnalyti, Antoniou, Damasio, & Wagner

Berners-Lee, T., Connolly, D., Kagal, L., Scharf, Y., & Hendler, J. (2008). N3Logic:
Logical Framework World Wide Web. published Theory Practice
Logic Programming (TPLP), Special Issue Logic Programming Web.
Bry, F., & Marchiori, M. (2005). Ten Theses Logic Languages Semantic Web.
3rd International Workshop Principles Practice Semantic Web Reasoning
(PPSWR-2005), pp. 4249.
Damasio, C. V., Analyti, A., Antoniou, G., & Wagner, G. (2006). Supporting Open
Closed World Reasoning Web. 4th Workshop Principles Practice
Semantic Web Reasoning (PPSWR-2006), pp. 149163.
de Bruijn, J., Franconi, E., & Tessaris, S. (2005). Logical Reconstruction Normative RDF.
OWL: Experiences Directions Workshop (OWLED-2005), Galway, Ireland.
Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1998). AL-log: Integrating Datalog
Description Logics. Journal Intelligent Information Systems, 10 (3), 227252.
Donini, F. M., Nardi, D., & Rosati, R. (2002). Description Logics Minimal Knowledge
Negation Failure. ACM Transactions Computational Logic, 3 (2), 177225.
Eiter, T., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2004a). Combining Answer Set
Programming Description Logics Semantic Web. 9th International
Conference Principles Knowledge Representation Reasoning (KR04), pp.
141151.
Eiter, T., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2004b). Well-Founded Semantics
Description Logic Programs Semantic Web. 3rd International Workshop
Rules Rule Markup Languages Semantic Web (RuleML04), pp. 8197.
Eiter, T., Ianni, G., Polleres, A., & Schindlauer, R. (2006). Answer Set Programming
Semantic Web. Tutorial co-located 3d European Semantic Web Conference
(ESWC-2006).
Gelder, A. V., Ross, K. A., & Schlipf, J. S. (1991). Well-Founded Semantics General
Logic Programs. Journal ACM, 38 (3), 620650.
Gelfond, M., & Lifschitz, V. (1988). Stable Model Semantics Logic Programming.
Kowalski, R., & Bowen, K. A. (Eds.), 5th International Conference Logic Programming, pp. 10701080. MIT Press.
Gelfond, M., & Lifschitz, V. (1990). Logic programs Classical Negation. Warren,
& Szeredi (Eds.), 7th International Conference Logic Programming, pp. 579597.
MIT Press.
Gelfond, M., & Lifschitz, V. (1991). Classical Negation Logic programs Disjunctive
Databases. New Generation Computing, 9, 365385.
Hayes, P. (2004). RDF Semantics. W3C Recommendation. Available http://www.w3.
org/TR/2004/REC-rdf-mt-20040210/.
Herre, H., Jaspars, J., & Wagner, G. (1999). Partial Logics Two Kinds Negation
Foundation Knowledge-Based Reasoning. Gabbay, D. M., & Wansing, H.
(Eds.), Negation? Kluwer Academic Publishers.
92

fiExtended RDF Semantic Foundation Rule Markup Languages

Herre, H., & Wagner, G. (1997). Stable Models Generated Stable Chain. Journal
Logic Programming, 30 (2), 165177.
Horrocks, I., & Patel-Schneider, P. F. (2003). Reducing OWL Entailment Description
Logic Satisfiability. 2nd International Semantic Web Conference (ISWC-2003),
pp. 1729.
Horrocks, I., & Patel-Schneider, P. F. (2004). Proposal OWL Rules Language.
13th International Conference World Wide Web (WWW04), pp. 723731. ACM
Press.
Horrocks, I., Patel-Schneider, P. F., Boley, H., Tabet, S., Grosof, B., & Dean, M.
(2004). SWRL: semantic web rule language combining OWL RuleML.
W3C Member Submission. Available http://www.w3.org/Submission/2004/
SUBM-SWRL-20040521/.
Kifer, M., Lausen, G., & Wu, J. (1995). Logical Foundations Object-Oriented FrameBased Languages. Journal ACM, 42 (4), 741843.
Klyne, G., & Carroll, J. J. (2004). Resource Description Framework (RDF): Concepts
Abstract Syntax. W3C Recommendation. Available http://www.w3.org/TR/
2004/REC-rdf-concepts-20040210/.
Levy, A. Y., & Rousset, M. (1998). Combining Horn Rules Description Logics
CARIN. Artificial Intelligence, 104 (1-2), 165209.
Lloyd, J. W., & Topor, R. W. (1984). Making Prolog Expressive. Journal Logic
Programming, 1 (3), 225240.
Maher, M. J. (2002). Model-Theoretic Semantics Defeasible Logic. ICLP 2002
Workshop Paraconsistent Computational Logic (PCL-2002), pp. 255287.
McGuinness, D. L., & van Harmelen, F. (2004).
OWL Web Ontology Language
Overview. W3C Recommendation. Available http://www.w3.org/TR/2004/
REC-owl-features-20040210/.
Motik, B., Sattler, U., & Studer, R. (2004). Query Answering OWL-DL Rules.
3rd International Semantic Web Conference (ISWC-2004), pp. 549563.
Patel-Schneider, P. F., Hayes, P., & Horrocks, I. (2004). OWL Web Ontology Language
Semantics Abstract Syntax. W3C Recommendation. Available http://www.
w3.org/TR/2004/REC-owl-semantics-20040210/.
Pereira, L. M., & Alferes, J. J. (1992). Well-Founded Semantics Logic Programs
Explicit Negation. Neumann, B. (Ed.), European Conference Artificial Intelligence, pp. 102106. John Wiley & Sons.
Prudhommeaux, E., & Seaborne, A. (2008). SPARQL Query Language RDF. W3C
Recommendation. Available http://www.w3.org/TR/rdf-sparql-query/.
Rao, P., Sagonas, K. F., Swift, T., Warren, D. S., & Freire, J. (1997). XSB: System
Efficiently Computing WFS. Proceedings 4th International Conference Logic
Programming Nonmonotonic Reasoning (LPNMR97), pp. 10701080.
93

fiAnalyti, Antoniou, Damasio, & Wagner

Rosati, R. (1999). Towards Expressive KR Systems Integrating Datalog Description
Logics: Preliminary Report. Proc. 1999 Description Logic Workshop (DL99),
pp. 160164.
Rosati, R. (2005). Decidability Complexity Integrating Ontologies Rules.
Journal Web Semantics, 3, 6173.
Schaffert, S., Bry, F., Besnard, P., Decker, H., Decker, S., Enguix, C. F., & Herzig, A.
(2005). Paraconsistent Reasoning Semantic Web. Workshop Uncertainty
Reasoning Semantic Web, co-located ISWC-2005, pp. 104105.
Sintek, M., & Decker, S. (2002). TRIPLE - Query, Inference, Transformation Language Semantic Web. 1st International Semantic Web Conference (ISWC2002), pp. 364378. Springer-Verlag.
ter Horst, H. J. (2004). Extending RDFS Entailment Lemma. 3rd International
Semantic Web Conference (ISWC-2004), pp. 7791.
ter Horst, H. J. (2005a). Combining RDF Part OWL Rules: Semantics, Decidability, Complexity. 4th International Semantic Web Conference (ISWC-2005),
pp. 668684.
ter Horst, H. J. (2005b). Completeness, Decidability Complexity Entailment
RDF Schema Semantic Extension Involving OWL Vocabulary. Journal
Web Semantics, 3 (2-3), 79115.
Wagner, G. (1991). Database Needs Two Kinds Negation. 3rd Symposium
Mathematical Fundamentals Database Knowledge Base Systems (MFDBS91),
pp. 357371. Springer-Verlag.
Wagner, G. (2003). Web Rules Need Two Kinds Negation. 1st International Workshop Principles Practice Semantic Web Reasoning (PPSWR03), pp. 3350.
Springer-Verlag.
Wagner, G., Giurca, A., & Lukichev, S. (2005). General Markup Framework Integrity
Derivation Rules. Dagstuhl Seminar Proceedings: Principles Practices
Semantic Web Reasoning.
Wagner, G., Giurca, A., & Lukichev, S. (2006). Usable Interchange Format Rich
Syntax Rules Integrating OCL, RuleML SWRL. Workshop Reasoning
Web (RoW-2006), co-located WWW-2006).
Yang, G., & Kifer, M. (2003a). Inheritance Rules Object-Oriented Semantic Web
Languages. 2nd International Workshop Rules Rule Markup Languages
Semantic Web (RULEML03), pp. 95110.
Yang, G., & Kifer, M. (2003b). Reasoning Anonymous Resources Meta Statements Semantic Web. Journal Data Semantics, 1, 6997.
Yang, G., Kifer, M., & Zhao, C. (2003). Flora-2: Rule-Based Knowledge Representation
Inference Infrastructure Semantic Web. 2nd International Conference
Ontologies, DataBases, Applications Semantics Large Scale Information
Systems (ODBASE03), pp. 671688.

94

fi
