Journal Artificial Intelligence Research 32 (2008) 565-606Submitted 11/07; published 06/08SATzilla: Portfolio-based Algorithm Selection SATLin XuFrank HutterHolger H. HoosKevin Leyton-Brownxulin730@cs.ubc.cahutter@cs.ubc.cahoos@cs.ubc.cakevinlb@cs.ubc.caDepartment Computer ScienceUniversity British Columbia201-2366 Main Mall, BC V6T 1Z4, CANADAAbstractwidely observed single dominant SAT solver; instead, differentsolvers perform best different instances. Rather following traditional approachchoosing best solver given class instances, advocate making decision online per-instance basis. Building previous work, describe SATzilla,automated approach constructing per-instance algorithm portfolios SAT use socalled empirical hardness models choose among constituent solvers. approachtakes input distribution problem instances set component solvers, constructs portfolio optimizing given objective function (such mean runtime, percentinstances solved, score competition). excellent performance SATzillaindependently verified 2007 SAT Competition, SATzilla07 solversthree gold, one silver one bronze medal. article, go well beyond SATzilla07making portfolio construction scalable completely automated, improvingintegrating local search solvers candidate solvers, predicting performance scoreinstead runtime, using hierarchical hardness models take account different types SAT instances. demonstrate effectiveness new techniquesextensive experimental results data sets including instances recent SATcompetition.1. Introductionpropositional satisfiability problem (SAT) one fundamental problemscomputer science. Indeed, entire conferences journals devoted studyproblem, long history AI. SAT interesting sakeinstances problems N P encoded SAT solvedSAT solvers. approach proven effective tackling many real-world applications,including planning, scheduling, graph colouring, bounded model checking, formal verification (examples described Kautz & Selman, 1996, 1999; Crawford & Baker,1994; van Gelder, 2002; Biere, Cimatti, Clarke, Fujita, & Zhu, 1999; Stephan, Brayton, &Sangiovanni-Vencentelli, 1996).conceptual simplicity SAT facilitates algorithm development, considerableresearch engineering efforts past decades led sophisticated algorithmshighly-optimized implementations. fact, SAT probably N P-complete decisionproblem largest amount research effort expended developc2008AI Access Foundation. rights reserved.fiXu, Hutter, Hoos & Leyton-Brownment study algorithms. Todays high-performance SAT solvers include tree-searchalgorithms (see, e.g., Davis, Logemann, & Loveland, 1962; Zhang, Madigan, Moskewicz, &Malik, 2001; Zhang, 2002; Kullmann, 2002; Dubois & Dequen, 2001; Heule, Zwieten, Dufour, & Maaren, 2004; Een & Sorensson, 2003), local search algorithms (see, e.g., Selman,Levesque, & Mitchell, 1992; Selman, Kautz, & Cohen, 1994; Hutter, Tompkins, & Hoos,2002; Hoos, 2002; Li & Huang, 2005; Ishtaiwi, Thornton, Anbulagan, Sattar, & Pham,2006; Hoos & Stutzle, 2005), resolution-based approaches (see, e.g., Davis & Putnam,1960; Dechter & Rish, 1994; Bacchus, 2002b, 2002a; Bacchus & Winter, 2003; Subbarayan& Pradhan, 2005).SAT algorithms highly complex, thus largely resisted theoretical average-case analysis. Instead, empirical studies often practical meansassessing comparing performance. one prominent ongoing example,SAT community holds annual SAT competition (http://www.satcompetition.org; see,e.g., Le Berre & Simon, 2004). competition intended provide objective assessment SAT algorithms, thus track state art SAT solving, assesspromote new solvers, identify new challenging benchmarks. Solvers judged basedempirical performance three categories instances, divided satisfiable, unsatisfiable mixed instances, speed robustnesstaken account. competition serves annual showcase state artSAT solving; 30 solvers entered 2007.1.1 Algorithm Selection ProblemOne way evaluations like SAT competition useful allow practitioners determine algorithm performs best instances relevant problemdomain. However, choosing single algorithm basis competition performancealways good approachindeed, often case one solver better others solving problem instances given class, dramatically worseinstances. Thus, practitioners hard SAT problems solve face potentially difficultalgorithm selection problem (Rice, 1976): algorithm(s) run orderminimize performance objective, expected runtime?widely-adopted solution algorithm selection problems measureevery candidate solvers runtime representative set problem instances,use algorithm offered best (e.g., average median) performance. callwinner-take-all approach. use resulted neglect many algorithmscompetitive average nevertheless offer good performanceparticular instances.ideal solution algorithm selection problem, hand, wouldconsult oracle knows amount time algorithm would take solvegiven problem instance, select algorithm best performance. Unfortunately, computationally cheap, perfect oracles nature available SATN P-complete problem; cannot precisely determine arbitrary algorithmsruntime arbitrary instance without actually running it. Nevertheless, approachalgorithm selection based idea building approximate runtime predictor,seen heuristic approximation perfect oracle. Specifically, use machine566fiSATzilla: Portfolio-based Algorithm Selection SATlearning techniques build empirical hardness model, computationally inexpensivepredictor algorithms runtime given problem instance based featuresinstance algorithms past performance (Nudelman, Leyton-Brown, Hoos, Devkar,& Shoham, 2004a; Leyton-Brown, Nudelman, & Shoham, 2002). modeling several algorithms and, runtime, choosing algorithm predicted best performance,empirical hardness models serve basis algorithm portfolio solvesalgorithm selection problem automatically (Leyton-Brown, Nudelman, Andrew, McFadden,& Shoham, 2003b, 2003a).1work show, believe first time, empirical hardnessmodels used build algorithm portfolio achieves state-of-the-art performance broad, practical domain. is, evaluated algorithm idiosyncratic conditions narrowly-selected data, rather large, independentlyconducted competition, confronting wide range high-performance algorithmslarge set independently-chosen interesting data. Specifically, describe analyzeSATzilla, portfolio-based SAT solver utilizes empirical hardness models perinstance algorithm selection.1.2 Algorithm Portfoliosterm algorithm portfolio introduced Huberman, Lukose, Hogg (1997)describe strategy running several algorithms parallel, potentially differentalgorithms assigned different amounts CPU time. approach also studiedGomes Selman (2001). Several authors since used term broader wayencompasses strategy leverages multiple black-box algorithms solvesingle problem instance. view, space algorithm portfolios spectrum,approaches use available algorithms one end approaches alwaysselect single algorithm other. advantage using term portfoliorefer broader class algorithms work reasontheyexploit lack correlation best-case performance several algorithms orderobtain improved performance average case.clearly describe algorithm portfolios broad sense, introducenew terminology. define (a, b)-of-n portfolio set n algorithms procedureselecting among property algorithm terminates early, leastb algorithms executed.2 brevity, also use terms a-of-nportfolio refer (a, a)-of-n portfolio, n-portfolio n-of-n portfolio.also useful distinguish solvers run selected. Portfolios parallel(all algorithms executed concurrently), sequential (the execution one algorithmbegins previous algorithms execution ended), partly sequential (some1. Similarly, one could predict performance single algorithm different parameter settingschoose best setting per-instance basis. previously demonstrated approachfeasible case number parameters small (Hutter, Hamadi, Hoos, & Leyton-Brown,2006). Ultimately, conceivable combine two lines research, automatically selectgood algorithm along good parameter settings per-instance basis.2. termination condition somewhat tricky. consider portfolio terminated earlysolves problem one solvers chance run, one solvers crashes. Thus,determining b, consider crash-recovery techniques, using next bestpredicted solver (discussed later paper).567fiXu, Hutter, Hoos & Leyton-Browncombination two). Thus classic algorithm portfolios Huberman et al. (1997)Gomes Selman (2001) described parallel n-portfolios. contrast,SATzilla solvers present paper sequential 3-of-n portfolios sincesequentially execute two pre-solvers followed one main solver.range work literature describes algorithm portfoliosbroad sense defined here. First, consider work emphasizedalgorithm selection (or 1-of-n portfolios). Lobjois Lematre (1998) studied problemselecting branch-and-bound algorithms based estimate search tree sizedue Knuth (1975). Gebruers, Hnich, Bridge, Freuder (2005) employed case-basedreasoning select solution strategy instances constraint programming problem.Various authors proposed classification-based methods algorithm selection (e.g.,Guerri & Milano, 2004; Gebruers, Guerri, Hnich, & Milano, 2004; Guo & Hsu, 2004; and,extent, Horvitz, Ruan, Gomes, Kautz, Selman, & Chickering, 2001). One problemapproaches use error metric penalizes misclassificationsequally, regardless cost. problematic using suboptimal algorithmacceptable, provided nearly good best algorithm. SATzilla approachconsidered classifier error metric depends differenceruntime algorithms.end spectrum, much work done considers switchingmultiple algorithms, terminology building parallel n-portfolios. GomesSelman (2001) built portfolio stochastic algorithms quasi-group completionlogistics scheduling problems. Low-knowledge algorithm control Carchrae Beck(2005) employed portfolio anytime algorithms, prioritizing algorithm accordingperformance far. Gagliolo Schmidhuber (2006b) learned dynamic algorithmportfolios also support running several algorithms once, algorithms prioritydepends predicted runtime conditioned fact yet found solution.Streeter, Golovin, Smith (2007) improved average-case performance using black-boxtechniques learning interleave execution multiple heuristics basedinstance features runtime algorithms.approaches fall two extremes, making decisions algorithms use flywhile solving problem instanceinstead committingadvance subset algorithms. examples give (1, n)-of-n portfolios.Lagoudakis Littman (2001) employed reinforcement learning solve algorithmselection problem decision point DPLL solver SAT order selectbranching rule. Similarly, Samulowitz Memisevic (2007) employed classificationswitch different heuristics QBF solving search.Finally, describe ways paper builds past work. LeytonBrown et al. (2002) introduced empirical hardness models. Nudelman et al. (2004a) demonstrated work (uniform-random) SAT introduced features usehere, Hutter et al. (2006) showed apply randomized, incomplete algorithms. Empirical hardness models first used basis algorithm portfoliosLeyton-Brown et al. (2003b, 2003a). idea building algorithm portfolio SAT goes back 2003, submitted first SATzilla solver SATcompetition (Nudelman, Leyton-Brown, Devkar, Shoham, & Hoos, 2004b); versionSATzilla placed 2nd two categories 3rd another. following, describe568fiSATzilla: Portfolio-based Algorithm Selection SATsubstantially improved SATzilla solver, entered 2007 SAT Competitionanddespite considerable progress SAT community four year intervalplaced 1st three categories, 2nd 3rd two categories. solverdescribed, along preliminary analysis, conference paper (Xu, Hutter, Hoos,& Leyton-Brown, 2007c); used hierarchical hardness models, described separately (Xu,Hoos, & Leyton-Brown, 2007a). work, provide much detailed descriptionnew solver, present several new techniques never previously published(chiefly introduced Section 5) report new experimental results.1.3 OverviewOverall, paper divided two parts. first part describes developmentSATzilla07, submitted 2007 SAT Competition second part demonstrates recent, improved portfolio algorithms (SATzilla07+ SATzilla07 ).part subdivided three sections, first describes approachdesigning portfolio-based solver high level, second explains lower-leveldetails portfolio construction, third provides results extensiveexperimental evaluation.Section 2 (Design I) begins general methodology building algorithm portfoliosbased empirical hardness models. work, apply general strategies SATevaluate experimentally. Section 3 (Construction I) describe architectureportfolio-based solvers entered 2007 SAT Competition describedprevious work (Xu et al., 2007c). addition, constructed new portfolio-basedsolver INDUSTRIAL instances analyzed effectively re-running INDUSTRIALcategory 2007 competition portfolio included; results analysisreported Section 4 (Evaluation I).move consider ways extending, strengthening, automatingportfolio construction. present five new design ideas Section 5 (Design II),consider incorporation new solvers training data Section 6 (ConstructionII). Finally, evaluated new ideas quantified benefits second setexperiments, describe Section 7 (Evaluation II). Section 8 concludes papergeneral observations.2. Design I: Building Algorithm Portfolios Empirical HardnessModelsgeneral methodology building algorithm portfolio use workfollows Leyton-Brown et al. (2003b) broad strokes, made significantextensions here. Portfolio construction happens offline, part algorithm development,comprises following steps.1. Identify target distribution problem instances. Practically, means selectingset instances believed representative underlying distribution, usinginstance generator constructs instances represent samplesdistribution.569fiXu, Hutter, Hoos & Leyton-Brown2. Select set candidate solvers relatively uncorrelated runtimesdistribution known expected perform well least instances.3. Identify features characterize problem instances. general cannot doneautomatically, rather must reflect knowledge domain expert.usable effectively automated algorithm selection, features must relatedinstance hardness relatively cheap compute.4. training set problem instances, compute features run algorithmdetermine running times.5. Identify one solvers use pre-solving instances. pre-solverslater run short amount time features computed (step 9 below),order ensure good performance easy instances allow empiricalhardness models focus exclusively harder instances.6. Using validation data set, determine solver achieves best performanceinstances solved pre-solvers featurecomputation times out. refer solver backup solver. absencesufficient number instances pre-solving feature computation timedout, employ single best component solver (i.e., winner-take-all choice)backup solver.7. Construct empirical hardness model algorithm portfolio, predicts runtime algorithm instance, based instances features.8. Choose best subset solvers use final portfolio. formalizeautomatically solve simple subset selection problem: given solvers,select subset respective portfolio (which uses empirical hardnessmodels learned previous step) achieves best performance validationset. (Observe runtime predictions perfect, dropping solverportfolio entirely increase portfolios overall performance.)Then, online, solve given problem instance, following steps performed.9. Run pre-solver predetermined fixed cutoff time reached.10. Compute feature values. feature computation cannot completed reason(error timeout), select backup solver identified step 6 above.11. Otherwise, predict algorithms runtime using empirical hardness modelsstep 7 above.12. Run algorithm predicted best. solver fails complete run (e.g.,crashes), run algorithm predicted next best.effectiveness algorithm portfolio built using approach dependsability learn empirical hardness models accurately predict solvers runtimegiven instance using efficiently computable features. experiments presented570fiSATzilla: Portfolio-based Algorithm Selection SATpaper, use ridge regression method previously provensuccessful predicting runtime uniform random k-SAT, structured SAT instances,combinatorial auction winner determination problems (Nudelman et al., 2004a;Hutter et al., 2006; Leyton-Brown et al., 2002).32.1 Ridge Regression Feature Selectionexplain construction empirical hardness models described Step 7 above.predict runtime algorithm instance distribution D, first draw ninstances uniformly random. (In article, distributions given implicitlybenchmark set instances, simply use instances benchmark set.)instance i, compute set features xi = [xi,1 , . . . , xi,m ] characterizeinstance. also run algorithm instance, recording runtime ri .computed features runtimes n instances, fit function f (x) that,given features xi instance i, yields prediction, yi , logarithm runtimeyi = log ri . experience, found log transformation runtimeimportant due large variation runtimes hard combinatorial problems. Unfortunately, performance learning algorithms deteriorate featuresuninformative highly correlated features; difficult construct featuressuffer problems. Therefore, first reduce set features performing feature selection, case forward selection (see e.g., Guyon, Gunn, Nikravesh,& Zadeh, 2006), simple iterative method starts empty feature set greedily adds one feature time, aiming reduce cross-validation error much possibleevery added feature. Next, add additional pairwise product features xi,j xi,kj = 1 . . . k = j . . . m; widely used method typically referred quadraticbasis function expansion. Finally, perform another pass forward selectionextended set determine final set basis functions, instance obtainexpanded feature vector = (xi ) = [1 (xi ), . . . , (xi )], final numberbasis functions used model.use ridge regression (see, e.g., Bishop, 2006) fit free parameters wfunction fw (x). Ridge regression works follows. Let n matrix containingvectors instance training set, let vector log runtimes, letidentity matrix. Finally, let small constant penalize large coefficientsw thereby increase numerical stability (we used = 103 experiments). Then,compute w = (I + > )1 > y, > denotes transpose matrix .previously unseen instance i, obtain prediction log runtime computinginstance features xi evaluating fw (xi ) = w> (xi ).3. noted portfolio methodology make use regression approach providessufficiently accurate estimates algorithms runtime computationally efficient enoughtime spent making prediction compensated performance gain obtainedimproved algorithm selection. example, similar settings, previously explored manylearning techniques, lasso regression, SVM regression, Gaussian process regression (LeytonBrown et al., 2002; Hutter et al., 2006). techniques computationally expensiveridge regression, previous experiments found improve predictiveperformance enough justify additional cost.571fiXu, Hutter, Hoos & Leyton-Brown2.2 Accounting Censored Datacommon heuristic algorithms solving N P-complete problems, SAT algorithmstend solve instances quickly, taking extremely long amount timesolve instances. Hence, runtime data costly gather, individualruns take literally weeks complete, even runs instancessize take milliseconds. common solution problem censor runsterminating fixed cutoff time.question fit good models presence censored data extensively studied survival analysis literature statistics, originated actuarialquestions estimating persons lifespan given mortality data well agescharacteristics people still alive. Observe problem ours,except case, data points always censored value, subtletyturns matter.best approach know dealing censored data build modelsuse available information censored runs using censored runtimes lowerbounds actual runtimes. knowledge, technique first usedcontext SAT Gagliolo Schmidhuber (2006a). chose simple, yet effectivemethod Schmee Hahn (1979) deal censored samples. brief, methodfirst trains hardness model treating cutoff time true (uncensored) runtimecensored samples, repeats following steps convergence.1. Estimate expected runtime censored runs using hardness model. Sinceridge regression, predictions fact normal distributions (with fixed variance),expected runtime conditional runtime exceeding cutoff time meancorresponding normal distribution truncated cutoff time.2. Train new hardness model using true runtimes uncensored instancespredictions generated previous step censored instances.earlier work (Xu, Hutter, Hoos, & Leyton-Brown, 2007b), experimentally comparedapproach two approaches dealing censored data: dropping dataentirely, treating censored runs though finished cutoff threshold.demonstrated empirically methods significantly worsemethod presented above. Intuitively, methods introduce bias empirical hardnessmodels, whereas method Schmee Hahn (1979) unbiased.2.3 Using Hierarchical Hardness Modelsprevious research empirical hardness models SAT showed achievebetter prediction accuracy even simpler models restrict satisfiable unsatisfiable instances (Nudelman et al., 2004a). course, practiceinterested making accurate predictions even know whether instancesatisfiable. recent work (Xu et al., 2007a), introduced hierarchical hardness modelsmethod solving problem. define subjective probability instancefeatures x satisfiable probability instance chosen randomunderlying instance distribution features matching x satisfiable. Hierarchical572fiSATzilla: Portfolio-based Algorithm Selection SAThardness models first use classifier predict subjective probability satisfiabilityuse probability, well features x, combine predictions so-calledconditional models, trained satisfiable instances unsatisfiable instances, respectively. previous work conducted extensive experimentsvarious types SAT instances found hierarchical hardness models achievebetter runtime prediction accuracies traditional empirical hardness models (Xu et al.,2007a).Specifically, begin predicting instances satisfiability using classificationalgorithm depends instance features used empirical hardnessmodels. chose Sparse Multinomial Logistic Regression, SMLR (Krishnapuram, Carin,Figueiredo, & Hartemink, 2005), classification algorithm returnsprobability instance belongs class could used instead. Then, trainconditional empirical hardness models (Msat , Munsat ) using quadratic basis-function regression satisfiable unsatisfiable training instances.Next, must decide combine predictions two models.4 observeset instance features x classifier prediction s; task predict expectedvalue algorithms runtime given information. introduce additionalrandom variable z {sat, unsat}, represents subjective belief oracleschoice conditional model perform best given instance. (Observemay always correspond model trained data satisfiabilitystatus instance.) express conditional dependence relationshipsrandom variables using graphical model, illustrated Figure 1.x,zfeatures &probabilitysatisfiablemodelselectionoracleruntimeFigure 1: Graphical model mixture-of-experts approach.write expression probability distribution instancesruntime given features xP (y | x, s) =XP (z = k | x, s) PMk (y | x, s),(1)k{sat,unsat}PMk (y | x, s) probability evaluated according model Mk (see Figure 1).Since models fitted using ridge regression, rewrite Equation (1)4. Note classifiers output used directly select modeldoing would mean ignoringcost making mistake. Instead, use classifiers output feature upon hierarchicalmodel depend.573fiXu, Hutter, Hoos & Leyton-BrownP (y | x, s) =XP (z = k | x, s)k{sat,unsat}wk> (x)k,(2)() denotes probability distribution function Normal distribution meanzero unit variance, wk weights model Mk , (x) quadratic basis functionexpansion x, k fixed standard deviation.particular, interested mean predicted runtime, is, expectationP (y | x, s):XP (z = k | x, s) wk> (x).(3)E(y | x, s) =k{sat,unsat}Evaluating expression would easy knew P (z = k | x, s); course,not. approach learn weighting functions P (z = k | x, s) minimize followingloss function:2nXL=yi E(y | x, s) ,(4)i=1yi true log runtime instance n number training instances.hypothesis space weighting functions chose softmax function(see, e.g., Bishop, 2006)>ev [x;s]P (z = sat | x, s) =,1 + ev> [x;s](5)v vector free parameters set minimize loss function (4).functional form frequently used probabilistic classification tasks: v > [x; s] large>positive, ev [x;s] much larger 1 resulting probability close 1;large negative, result close zero; zero, resulting probabilityexactly 0.5.seen mixture-of-experts problem (see, e.g., Bishop, 2006) experts fixed Msat Munsat . (In traditional mixture-of-experts methods, expertsallowed vary training process described below.) implementation convenience, used existing mixture experts implementation optimize v, builtaround expectation maximization (EM) algorithm performs iterative reweightedleast squares step (Murphy, 2001). modified code slightly fixexperts initialized choice expert classifiers output setting initialvalues P (z | x, s) s. Note numerical optimization procedures could usedminimize loss function (4) respect v.optimized v, obtain runtime prediction unseen test instancesimply compute instances features x classifiers output s, computeexpected runtime evaluating Equation (3).Finally, note techniques require us restrict conditional models Msat Munsat , even use two models. Section 7,describe hierarchical hardness models rely six conditional models, trainedsatisfiable unsatisfiable instances different data sets.574fiSATzilla: Portfolio-based Algorithm Selection SAT3. Construction I: Building SATzilla07 2007 SAT Competitionsection, describe SATzilla07 solvers entered 2007 SAT Competition, whichlike previous events seriesfeatured three main categories instances,RANDOM, HANDMADE (also known CRAFTED) INDUSTRIAL. submitted three versions SATzilla07 competition. Two versions specifically targeted RANDOMHANDMADE instance categories trained data target category.order allow us study SATzilla07s performance even heterogeneousinstance distribution, third version SATzilla07 trained data threecategories competition; call new category ALL. construct versionSATzilla07 INDUSTRIAL category, time constraints limitthree submissions per team. However, built version submission deadlinereport results below.solvers built using design methodology detailed Section 2.following subsections corresponds one step methodology.3.1 Selecting Instancesorder train empirical hardness models scenarios, neededinstances would similar used real competition. purposeused instances respective categories previous SAT competitions (2002,2003, 2004, 2005), well 2006 SAT Race (which featured INDUSTRIALinstances). Instances repeated previous competitions also repeateddata sets. Overall, 4 811 instances: 2 300 instances category RANDOM, 1 490category HANDMADE 1 021 category INDUSTRIAL; course, category includedinstances. 68% instances solved within 1 200 CPU secondsreference machine least one seven solvers used (see Section 3.2 below;computational infrastructure used experiments described Section 3.4).instances solved solvers dropped data set.randomly split data set training, validation test sets ratio40:30:30. parameter tuning intermediate model testing performed validation set; test set used generate final results reported here.section, use SATzilla07 methodology building multiple portfoliosdifferent sets benchmark instances. order avoid confusion changesoverall methodology discussed later differences training data, treatdata set input parameter SATzilla. data set comprising previouslymentioned RANDOM instances, write Dr ; similarly, write Dh HANDMADE DiINDUSTRIAL; ALL, simply write D.3.2 Selecting Solversdecide algorithms include portfolio, considered wide variety solversentered previous SAT competitions 2006 SAT Race.manually analyzed results competitions, identifying algorithms yieldedbest performance subset instances. Since focus satisfiableunsatisfiable instances, since concerned cost misclassifications,575fiXu, Hutter, Hoos & Leyton-Brownchoose incomplete algorithms stage; however, revisit issueSection 5.4. end, selected seven high-performance solvers shown Table 1candidates SATzilla07 portfolio. Like data set used training, treatcomponent solvers input SATzilla, denote set solvers Table 1S.SolverReferenceEurekaKcnfs06March dl04Minisat 2.0Rsat 1.03VallstZchaff RandNadel, Gordon, Palti, Hanna (2006)Dubois Dequen (2001)Heule et al. (2004)Een Sorensson (2006)Pipatsrisawat Darwiche (2006)Vallstrom (2005)Mahajan, Fu, Malik (2005)Table 1: seven solvers SATzilla07; refer set solvers S.previous work (Xu et al., 2007b), considered using Hypre preprocessor (Bacchus & Winter, 2003) applying one SATzillas component solvers; effectivelydoubled number component solvers. work, re-evaluated optionfound performance basically remain unchanged without preprocessing (performance differences terms instances solved, runtime, SAT competition score smaller1%, even small difference consistently favor using Hyprepreprocessor). reason, dropped preprocessing work reported here.3.3 Choosing Featureschoice instance features significant impact performance empiricalhardness models. Good features need correlate well (solver-specific) instance hardness need cheap compute, since feature computation time counts partSATzilla07s runtime.Nudelman et al. (2004a) introduced 84 features SAT instances. featuresclassified nine categories: problem size, variable-clause graph, variable graph, clausegraph, balance, proximity Horn formulae, LP-based, DPLL probing local searchprobing features code last group features based UBCSAT (Tompkins& Hoos, 2004). order limit time spent computing features, slightly modifiedfeature computation code Nudelman et al. (2004a). SATzilla07, excludednumber computationally expensive features, LP-based clause graph features.computation time local search DPLL probing features limited1 CPU second, total feature computation time per instance limited 60CPU seconds. eliminating features value across instancesunstable given 1 CPU second local search probing, endedusing 48 raw features summarized Figure 2.576fiSATzilla: Portfolio-based Algorithm Selection SATProximity Horn Formula:28. Fraction Horn clauses29-33. Number occurrences Horn clausevariable: mean, variation coefficient, min, maxentropy.Problem Size Features:1. Number clauses: denoted c2. Number variables: denoted v3. Ratio: c/vVariable-Clause Graph Features:4-8.Variable nodes degree statistics: mean,variation coefficient, min, max entropy.9-13. Clause nodes degree statistics: mean, variation coefficient, min, max entropy.DPLL Probing Features:34-38. Number unit propagations: computeddepths 1, 4, 16, 64 256.39-40. Search space size estimate: mean depthcontradiction, estimate log number nodes.Variable Graph Features:14-17. Nodes degree statistics: mean, variationcoefficient, min max.Local Search Probing Features:41-44. Number steps best local minimumrun: mean, median, 10th 90th percentilesSAPS.45. Average improvement best run: meanimprovement per step best solution SAPS.46-47. Fraction improvement due first localminimum: mean SAPS GSAT.48. Coefficient variation number unsatisfied clauses local minimum: meanruns SAPS.Balance Features:18-20. Ratio positive negative literalsclause: mean, variation coefficient entropy.21-25. Ratio positive negative occurrencesvariable: mean, variation coefficient, min, maxentropy.26-27. Fraction binary ternary clausesFigure 2: features used building SATzilla07; originally introduced describeddetail Nudelman et al. (2004a).3.4 Computing Features Runtimesexperiments performed using computer cluster consisting 55 machinesdual Intel Xeon 3.2GHz CPUs, 2MB cache 2GB RAM, running Suse Linux 10.1.SAT competition, runs solver exceeded certain runtime aborted(censored) recorded such. order keep computational cost manageable,chose cutoff time 1 200 CPU seconds.3.5 Identifying Pre-solversdescribed Section 2, order solve easy instances quickly without spending timecomputation features, use one pre-solvers: algorithms rununconditionally briefly features computed. Good algorithms pre-solvingsolve large proportion instances quickly. Based examination trainingruntime data, chose March dl04 local search algorithm SAPS (Hutter et al., 2002)pre-solvers RANDOM, HANDMADE ALL; SAPS, used UBCSAT implementation(Tompkins & Hoos, 2004) best fixed parameter configuration identified Hutteret al. (2006). (Note consider incomplete algorithms inclusionportfolio, use one here.)Within 5 CPU seconds reference machine, March dl04 solved 47.8%, 47.7%,43.4% instances RANDOM, HANDMADE data sets, respectively.remaining instances, let SAPS run 2 CPU seconds, found runtimealmost completely uncorrelated March dl04 (Pearson correlation coefficient r = 0.118577fiXu, Hutter, Hoos & Leyton-BrownSolverEurekaKcnfs06March dl04Minisat 2.0Rsat 1.03VallstZchaff RandRANDOMTime SolvedHANDMADETime SolvedINDUSTRIALTime SolvedTimeSolved770319269520522757802561846311411412440562330105071540734558246159865839445944562064557%50%73%69%70%54%51%40%81%85%62%62%40%36%59%33%80%73%72%67%58%84%13%42%76%81%59%71%Table 2: Average runtime (in CPU seconds reference machine) percentage instancessolved solver instances solved least one seven component solvers within cutoff time 1 200 seconds.487 remaining instances solved solvers). time, SAPS solved 28.8%,5.3%, 14.5% remaining RANDOM, HANDMADE instances, respectively.INDUSTRIAL category, chose run Rsat 1.03 2 CPU seconds pre-solver,resulted 32.0% instances INDUSTRIAL set solved. Since SAPSsolved less 3% remaining instances within 2 CPU seconds, usedpre-solver category.3.6 Identifying Backup Solverperformance solvers Table 1 reported Table 2. computedaverage runtime (here remainder work) counting timeouts runscompleted cutoff time 1 200 CPU seconds. seen data,best single solver ALL, RANDOM HANDMADE always March dl04. categoriesRANDOM HANDMADE, encounter instances feature computationtimed out. Thus, employed winner-take-all solver March dl04 backup solverdomains. categories INDUSTRIAL ALL, Eureka performed bestinstances remained unsolved pre-solving feature computationtimed out; thus chose Eureka backup solver.3.7 Learning Empirical Hardness Modelslearned empirical hardness models predicting solvers runtime describedSection 2, using procedure Schmee Hahn (1979) dealing censored dataalso employing hierarchical hardness models.3.8 Solver Subset Selectionperformed automatic exhaustive subset search outlined Section 2 determinesolvers include SATzilla07. Table 3 describes solvers selectedfour data sets.578fiSATzilla: Portfolio-based Algorithm Selection SATData SetRANDOMHANDMADEINDUSTRIALSolvers used SATzilla07March dl04, Kcnfs06, Rsat 1.03Kcnfs06, March dl04, Minisat 2.0, Rsat 1.03, Zchaff RandEureka, March dl04, Minisat 2.0, Rsat 1.03Eureka, Kcnfs06, March dl04, Minisat 2.0, Zchaff RandTable 3: Results subset selection SATzilla07.4. Evaluation I: Performance Analysis SATzilla07section, evaluate SATzilla07 four data sets. Since use SAT Competition running example throughout paper, start describing SATzilla07fared 2007 SAT Competition. describe comprehensive evaluationsSATzilla07 version compared greater detail componentsolvers.4.1 SATzilla07 2007 SAT Competitionsubmitted three versions SATzilla07 2007 SAT Competition, namelySATzilla07(S,Dr ) (i.e., SATzilla07 using seven component solvers Table 1 trained RANDOM instances) SATzilla07(S,Dh ) (trained HANDMADE),SATzilla07(S,D) (trained ALL). Table 4 shows results 2007 SAT Competition RANDOM HANDMADE categories. RANDOM category, SATzilla07(S,Dr )gold medal subcategory SAT+UNSAT, came third UNSAT subcategory. SAT subcategory dominated local search solvers. HANDMADEcategory, SATzilla07(S,Dh ) showed excellent performance, winning SAT+UNSATUNSAT subcategories, placing second SAT subcategory.CategoryRankSAT & UNSATSATUNSATRANDOM1st2nd3rdSATzilla07(S,Dr )March ksKcnfs04Gnovelty+Ag2wsat0Ag2wsat+March ksKcnfs04SATzilla07(S,Dr )HANDMADE1st2nd3rdSATzilla07(S,Dh )Minisat07MXCMarch ksSATzilla07(S,Dh )Minisat07SATzilla07(S,Dh )TTSMinisat07Table 4: Results 2007 SAT Competition. 30 solvers competedcategory.Since general portfolio SATzilla07(S,D) included Eureka (whose source codepublicly available) run demonstration division only. official competitionresults (available http://www.cril.univ-artois.fr/SAT07/) show solver,trained instances three categories, performed well, solving579fiXu, Hutter, Hoos & Leyton-BrownSolverAverage runtime [s]Solved percentagePerformance scorePicosatTinisatEliteMinisat07Rsat 2.03984944844468271727531484216303408823446SATzilla07(S,Di )3468729552Table 5: Performance comparison SATzilla07(S,Di ) winners 2007 SAT CompetitionINDUSTRIAL category. performance scores computed using 2007 SATCompetition scoring function cutoff time 1 200 CPU seconds. SATzilla07(S,Di )exactly solver shown Figure 6 trained without reference2007 SAT Competition data.instances union three categories solver (including twoversions SATzilla07).submit version SATzilla07 2007 SAT Competitionspecifically trained instances INDUSTRIAL category. However, constructedversion, SATzilla07(S,Di ), submission deadline reportperformance. Although SATzilla07(S,Di ) compete actual 2007 SAT Competition, approximate well would performed simulationcompetition, using scoring function competition (described detailSection 5.3), based large number competitors, namely 19 solvers listedTables 1, 9 10, plus SATzilla07(S,Di ).Table 5 compares performance SATzilla07 simulation competitionsolvers least one medal INDUSTRIAL category 2007SAT Competition: Picosat, TinisatElite, Minisat07 Rsat 2.0.differences test environment used real SAT competition:simulation ran different machines different operating system; also usedshorter cutoff time fewer competitors evaluate solvers performance scores. differences, ranking solvers simulation necessarilywould actual competition. Nevertheless, results leave doubtSATzilla07 compete state-of-the-art SAT solvers INDUSTRIALcategory.4.2 Feature Computationactual time required compute features varied instance instance.following, report runtimes computing features instance sets defined Section3.1. Typically, feature computation took least 3 CPU seconds: 1 second localsearch probing SAPS GSAT, 1 second DPLL probing. However,small instances, limit 300 000 local search steps reached one CPUsecond passed, resulting feature computation times lower 3 CPU seconds.instances RANDOM HANDMADE categories, computationfeatures took insignificant amount time, resulting feature computation times580fi1001008080% Instances Finished% Instances FinishedSATzilla: Portfolio-based Algorithm Selection SAT60402000123Feature Time [CPU sec]456040200010203040Feature Time [CPU sec]5060Figure 3: Variability feature computation times. y-axis denotes percentage instancesfeature computation finished time given x-axis. Left:RANDOM, right: INDUSTRIAL. Note different scales x-axes.3 CPU seconds. However, many instances INDUSTRIAL category,feature computation quite expensive, times 1 200 CPU secondsinstances. limited feature computation time 60 CPU seconds, resultedtime-outs 19% instances INDUSTRIAL category (but instancescategories). instances feature computation timed out,backup solver used.Figure 3 illustrates variation feature computation time. category RANDOM(left pane), feature computation never took significantly longer three CPU seconds.contrast, category INDUSTRIAL, fairly high variation, featurecomputation reaching cut-off time 60 CPU seconds 19% instances.average total feature computation times categories RANDOM, HANDMADE, INDUSTRIAL3.01, 4.22, 14.4 CPU seconds, respectively.4.3 RANDOM Categorycategory, evaluated SATzilla portfolios comparingbest solvers Table 1 respective category. Note solvers tableexactly candidate solvers used SATzilla.Figure 4 shows performance SATzilla07 top three single solversTable 1 category RANDOM. Note count runtime pre-solving, wellfeature computation time, part SATzillas runtime. Oracle(S) provides upperbound performance could achieved SATzilla: runtimehypothetical version SATzilla makes every decision optimal fashion,without time spent computing features. Furthermore, also choose runpre-solvers instance. Essentially, Oracle(S) thus indicates performancewould achieved running best algorithm single instance. Figure 4(right), horizontal line near bottom plot shows time SATzilla07(S,Dr )allots pre-solving (on average) feature computation.581fiXu, Hutter, Hoos & Leyton-Brown100500% Instances SolvedAverage Runtime [CPU sec]60040030020090Oracle(S)SATzilla07(S,Dr)80Kcnfs06March_dl04Rsat1.0370605040302010010Presolving0K06fscndl0h_4rc03t1.saR07illaTzacleAvgFeature0 110010110210310Runtime [CPU sec]Figure 4: Left: Average runtime, right: runtime cumulative distribution function (CDF) dif-50010045090Oracle(S)SATzilla07(S,Dh)40080March_dl04Minisat2.0Vallst% Instances SolvedAverage Runtime [CPU sec]ferent solvers RANDOM; average feature computation time 2.3 CPU seconds(too insignificant visible SATzilla07s runtime bar). solvers CDFsones shown (i.e., given runtime maximum CDFsselected solvers upper bound CDF solvers consideredexperiments).35030025020015010070605040302050100h_4dl0rcnisMi.0at2VstSAzill070 110cleraPresolvingAvgFeature010110210310Runtime [CPU sec]Figure 5: Left: Average runtime, right: runtime CDF different solvers HANDMADE; average feature computation time 4.5 CPU seconds (shown white box topSATzilla07s runtime bar). solvers CDFs ones shown here.Overall, SATzilla07(S,Dr ) achieved good performance data set RANDOM:three times faster average best component solver, March dl04 (seeFigure 4, left), also dominated terms fraction instances solved, solving 20%instances within cutoff time (see Figure 4, left). runtime CDF plot also showslocal-search-based pre-solver SAPS helped considerably solving 20%instances within 2 CPU seconds (this reflected sharp increase solved instancesfeature computation begins).582fi4501004009035080% Instances SolvedAverage Runtime [CPU sec]SATzilla: Portfolio-based Algorithm Selection SAT30025020015010070Oracle(S)SATzilla07(S,D)EurekaMinisat2.0Rsat1.0360504030205010Presolving0kaureEnis.0at2Mi.03Rsat1SA7a0zillcle0 110AvgFeature010110210Figure 6: Left: Average runtime, right: runtime CDF different solvers INDUSTRIAL;average feature computation time 14.1 CPU seconds (shown white box topSATzilla07s runtime bar). solvers CDFs ones shown here.4.4 HANDMADE CategorySATzilla07s performance results HANDMADE category also good. Usingfive component solvers listed Table 3, average runtime 45% lessbest single component solver (see Figure 5, left). CDF plot Figure 5 (right)shows SATzilla07 dominated components solved 13% instancesbest non-portfolio solver.4.5 INDUSTRIAL Categoryperformed experiment INDUSTRIAL instances categoriesorder study SATzilla07s performance compared component solvers. SATzilla0723% faster average best component solver, Eureka (see Figure 6(left)). Moreover, Figure 6 (right) shows SATzilla07 also solved 9% instancesEureka within cutoff time 1 200 CPU seconds. Note category,feature computation timed 15.5% test instances 60 CPU seconds; Eurekaused backup solver cases.4.6final category, ALL, heterogeneous category included instancescategories, portfolio approach especially appealing. SATzilla07 performedwell category, average runtime less half best single solver,March dl04 (159 vs. 389 CPU seconds, respectively). also solved 20% instancesnon-portfolio solver within given time limit 1 200 CPU seconds (see Figure 7).583310Runtime [CPU sec]fi5001004509040080% Instances SolvedAverage Runtime [CPU sec]Xu, Hutter, Hoos & Leyton-Brown3503002502001501007060504030205010Presolving04dl0_rchOracle(S)SATzilla07(S,D)March_dl04Minisat2.0Rsat1.03nis.0at2MiRs.03at1SA07illaTzlerac0 110AvgFeature010110210310Runtime [CPU sec]Figure 7: Left: Average runtime; right: runtime CDF different solvers ALL; average feature computation time 6.7 CPU seconds (shown white box topSATzilla07s runtime bar). solvers CDFs ones shown here.4.7 Classifier Accuracysatisfiability status classifiers trained various data sets surprisingly effectivepredicting satisfiability RANDOM INDUSTRIAL instances, reached classification accuracies 94% 92%, respectively. HANDMADE ALL, classificationaccuracy still considerably better random guessing, 70% 78%, respectively.Interestingly, classifiers often misclassified unsatisfiable instances SATsatisfiable instances UNSAT. effect seen confusion matricesTable 6; pronounced HANDMADE category, overall classificationquality also lowest: 40% HANDMADE instances classified SATfact unsatisfiable, 18% instances classified UNSAT factsatisfiable.5. Design II: SATzilla Beyond 2007Despite SATzilla07s success 2007 SAT Competition, still room improvement. section describes number design enhancements SATzilla07underly new SATzilla versions, SATzilla07+ SATzilla07 , describedetail Section 6 evaluate experimentally Section 7.5.1 Automatically Selecting Pre-solversSATzilla07, identified pre-solvers cutoff times manually. severallimitations approach. First foremost, manual pre-solver selectionscale well. many candidate solvers, manually finding best combinationpre-solvers cutoff times difficult requires significant amounts valuablehuman time. addition, manual pre-solver selection performed SATzilla07concentrated solely solving large number instances quickly take584fiSATzilla: Portfolio-based Algorithm Selection SATsatisfiableunsatisfiableclassified SAT91%9%classified UNSAT5%95%satisfiableunsatisfiableclassified SAT60%40%classified UNSAT18%82%RANDOM data setHANDMADE data setsatisfiableunsatisfiableclassified SAT81%19%classified UNSAT5%95%INDUSTRIAL data setsatisfiableunsatisfiableclassified SAT65%35%classified UNSAT12%88%data setTable 6: Confusion matrices satisfiability status classifier data sets RANDOM, HANDMADE,INDUSTRIAL ALL.account pre-solvers effect model learning. fact, three consequencespre-solving.1. Pre-solving solves instances quickly features computed. contextSAT competition, improves SATzillas scores easy problem instancesdue speed purse component SAT competition scoring function. (SeeSection 5.3 below.)2. Pre-solving increases SATzillas runtime instances solved pre-solvingadding pre-solvers time every instance. Like feature computation itself,additional cost reduces SATzillas scores.3. Pre-solving filters easy instances, allowing empirical hardness modelstrained exclusively harder instances.considered (1) (2) manual selection pre-solvers, consider(3), namely fact use different pre-solvers and/or cutoff times results different training data hence different learned models, also affect portfolioseffectiveness.new automatic pre-solver selection technique works follows. committedadvance using maximum two pre-solvers: one three complete search algorithmsone three local search algorithms. three candidates searchapproaches automatically determined data set highest scorevalidation set run maximum 10 CPU seconds. also use numberpossible cutoff times, namely 2, 5 10 CPU seconds, well 0 seconds (i.e.,pre-solver run all) consider orders two pre-solversrun. resulting 288 possible combinations two pre-solvers cutoff times,SATzillas performance validation data evaluated performing steps 6, 7 8general methodology presented Section 2:585fiXu, Hutter, Hoos & Leyton-Brown6. determine backup solver selection features time out;7. construct empirical hardness model algorithm;8. automatically select best subset algorithms use part SATzilla.best-performing subset found last stepevaluated validation datais selected algorithm portfolio given combination pre-solver / cutoff timepairs. Overall, method aims choose pre-solver configuration yieldsbest-performing portfolio.5.2 Randomized Solver Subset Selection Large Set Component Solversmethodology Section 2 relied exhaustive subset search choosing bestcombination component solvers. large number component solvers, impossible (N component solvers would require consideration 2N solver sets,model would trained). automatic pre-solver selection methods describedSection 5.1 worsens situation: solver selection must performedevery candidate configuration pre-solvers, new pre-solver configurations inducenew models.alternative exhaustively considering subsets, implemented randomizediterative improvement procedure search good subset solvers. local searchneighbourhood used procedure consists subsets solvers reachedadding dropping single component solver. Starting randomly selected subsetsolvers, search step, consider neighbouring solver subset selected uniformlyrandom accept validation set performance increases; otherwise, acceptsolver subset anyway probability 5%. 100 steps performedimproving step, new run started re-initializing search random. 10runs, search terminated best subset solvers encounteredsearch process returned. Preliminary evidence suggests local search procedureefficiently finds good subsets solvers.5.3 Predicting Performance Score Instead Runtimegeneral portfolio methodology based empirical hardness models, predictalgorithms runtime. However, one might simply interested using portfolio picksolver lowest expected runtime. example, SAT competition, solversevaluated based complex scoring function depends partly solversruntime. Although idiosyncracies scoring function somewhat particularSAT competition, idea portfolio built optimize performance scorecomplex runtime wide applicability. section describe techniquesbuilding models predict performance score directly.One key issue thatas long depend standard supervised learning methodsrequire independent identically distributed training datawe deal easilyscoring functions actually associate score single instance combinepartial scores instances compute overall score. Given training data labeledscoring function, SATzilla simply learn model score (rather586fiSATzilla: Portfolio-based Algorithm Selection SATruntime) choose solver highest predicted score. Unfortunately, scoringfunction used 2007 SAT Competition satisfy independence property:score solver attains solving given instance depends part (and, indeed,solvers) performance other, similar instances. specifically, SAT competitioninstance P solution purse SolutionP speed purse SpeedP ; instancesgiven series (typically 540 similar instances) share one series purse SeriesP . Algorithmsranked summing three partial scores derived purses.1. problem instance P , solution purse equally distributedsolvers Si solve instance within cutoff time (thereby rewarding robustnesssolver).2. speed purse P divided among set solvers solved instanceSF(P,Si )timeLimit(P )Score(P, Si ) = SpeedP, speed factor SF(P, S) = 1+timeUsed(P,S)j SF(P,Sj )measure speed discounts small absolute differences runtime.3. series purse series divided equally distributed solversSi solved least one instance series.5Si partial score problem P solution speed purses solely depends solversruntime P runtime competing solvers P . Thus, given runtimescompeting solvers part training data, compute score contributionssolution speed purses instance P , two componentsindependent across instances. contrast, since solvers share series pursedepend performance instances series, partial score receivedseries purse solving one instance independent performanceinstances.solution problem approximate instances share series pursescore independent score. N instances series solved SATzillascomponent solvers, n solvers solve least one instances series, assignpartial score SeriesP/(N n) solver Si (where = 1, . . . , n) instanceseries solved. approximation non-independent score independentalways perfect, conservative defines lower-bound partialscore series purse. Predicted scores used SATzilla choosedifferent solvers per-instance basis. Thus, partial score solverinstance reflect much would contribute SATzillas score. SATzillaperfect (i.e., instance, always selected best algorithm) score approximationwould correct: SATzilla would solve N instances series componentsolver solve, thus would actually achieve series score SeriesP/(N n) N =SeriesP/n. SATzilla performed poorly solve instance series,approximation would also exact, since would estimate partial series scorezero. Finally, SATzilla pick successful solvers (say, )instances series solved component solvers (i.e., < N ), wouldunderestimate partial series purse, since SeriesP/(N n) < SeriesP/n.5. See http://www.satcompetition.org/2007/rules07.html details.587fiXu, Hutter, Hoos & Leyton-Brownlearning techniques require approximation performance scoreindependent score, experimental evaluations solvers scores employ actualSAT competition scoring function. explained previously, SAT competition,performance score solver depends score solvers competition.order simulate competition, select large number solvers pretendreference solvers SATzilla solvers competition; throughoutanalysis use 19 solvers listed Tables 1, 9 10. perfect simulation,since scores change somewhat different solvers added removedcompetition. However, obtain much better approximations performance scorefollowing methodology outlined using cruder measures, learningmodels predict mean runtime numbers benchmark instances solved.Finally, predicting performance score instead runtime number implicationscomponents SATzilla. First, notice compute exact scorealgorithm instance, even algorithm times unsuccessfully crashesincases, score three components simply zero. predicting scores insteadruntimes, thus need rely censored sampling techniques (see Section 2.2)anymore. Secondly, notice oracles maximizing SAT competition scoreminimizing runtime identical, since always using solver smallest runtimeguarantees highest values three components obtained.5.4 Introducing Local Search Solvers SATzillaSAT solvers based local search well known effective certain classessatisfiable instances. fact, classes hard random satisfiable instanceslocal search solvers solve reasonable amount time (Hoos & Stutzle, 2005).However, high-performance local-search-based SAT solvers incomplete cannotsolve unsatisfiable instances. previous versions SATzilla avoided using local searchalgorithms risk would select unsatisfiable instances,would run uselessly reaching cutoff time.shift predicting optimizing performance score instead runtime,issue turns matter anymore. Treating every solver black box, local searchsolvers always get score exactly zero unsatisfiable instances since guaranteedsolve within cutoff time. (Of course, need runinstance training instance known unsatisfiable.) Hence, buildmodels predicting score local search solvers using exactly methodscomplete solvers.5.5 General Hierarchical Hardness Modelsbenchmark set consists instances categories RANDOM, HANDMADEINDUSTRIAL. order improve performance heterogeneous instancedistribution, extend previous hierarchical hardness model approach (predicting satisfiability status using mixture two conditional models) generalscenario six underlying empirical hardness models (one combination categorysatisfiability status). output general hierarchical model linear weightedcombination output component. described Section 2.3, approx588fiSATzilla: Portfolio-based Algorithm Selection SATOld instances 2007New instances 2007(1 925 instances)Vo (1 443 instances)Eo (1 443 instances)Tn (347 instances)Vn (261 instances)En (261 instances)Training (40%)Validation (30%)Test (30%)Table 7: Instances 2007 2007 randomly split training (T), validation (V)test (E) data sets. sets include instances categories: RANDOM, HANDMADEINDUSTRIAL.Data setTrainingValidationTestD0D+TnVoVoVo VnEoEo EnEo EnTable 8: Data sets used experiments. used first series experimentsSection 4, D0 D+ used second series experiments. Note data setsD0 use identical training validation data, different test data.imate model selection oracle softmax function whose parameters estimatedusing EM.6. Construction II: Building Improved SATzilla Versionssection describe construction new SATzilla versions incorporate newdesign elements previous section. also describe two versions based olddesign, use evaluate impact changes.6.1 Benchmark Instancesaddition instances used Section 3.1, added 869 instances 2007SAT Competition four data sets. Overall, resulted 5 680 instances: 2 811instances category RANDOM, 1 676 category HANDMADE 1 193 category INDUSTRIAL.Recall Section 3.1 dropped instances could solved sevensolvers Table 1. follow methodology here, extend solver set12 solvers Tables 9 10. Now, 71.8% instances solved least one19 solvers within cutoff time 1 200 CPU seconds reference machine;remaining instances excluded analysis.randomly split benchmark sets training, validation test sets,described Table 7. parameter tuning intermediate testing performedvalidation sets, test sets used generate final results reported here.interested analyzing SATzillas performance vary dataused train it. make easy refer different data sets, describeassign names (D, D0 , D+ ) them. Table 7 shows division data589fiXu, Hutter, Hoos & Leyton-BrownSolverKcnfs04TTSPicosatMXCMarch ksTinisatEliteMinisat07Rsat 2.0ReferenceDequen Dubois (2007)Spence (2007)Biere (2007)Bregman Mitchell (2007)Heule v. Maaren (2007)Huang (2007)Sorensson Een (2007)Pipatsrisawat Darwiche (2007)Table 9: Eight complete solvers 2007 SAT Competition.SolverRanovAg2wsat0Ag2wsat+Gnovelty+ReferencePham Anbulagan (2007)C. M. Li Zhang (2007)Wei, Li, Zhang (2007)Pham Gretton (2007)Table 10: Four local search solvers 2007 SAT Competition.old (pre-2007) new (2007) instances. Table 8 shows combined dataconstruct three data sets use evaluation. Data set one introducedused Section 3.1: uses pre-2007 instances training, validation testing.Data set D0 uses training validation data sets, differs test sets,include old new instances. Data set D+ combines old new instancestraining, validation test sets.Thus, note data sets D0 D+ use test sets, meaning performance portfolios trained using different sets compared directly. However,expect portfolio trained using D+ least slightly better, accessdata. before, want refer RANDOM instances D+ ,write Dr+ ; likewise, write Dh+ HANDMADE, Di+ INDUSTRIAL, etc.6.2 Extending Set Component Solversaddition seven old solvers used SATzilla07 (previously described Table 1),considered eight new complete solvers four local search solvers 2007 SATCompetition inclusion portfolio; solvers described Tables 9 10.training instances, treat sets candidate solvers input parameterSATzilla. sets candidate solvers used experiments detailed Table 11.6.3 Different SATzilla Versionsintroduced new design ideas SATzilla (Section 5), new training data (Section 6.1) new solvers (Section 6.2), interested evaluating much portfolio improved result. order gain insights much performance improvement590fiSATzilla: Portfolio-based Algorithm Selection SATName SetSolvers SetS+S++7 solvers Table 115 solvers Tables 1 919 solvers Tables 1, 9 10Table 11: Solver sets used second series experiments.SATzilla versionSATzilla07(S,D0 )SATzilla07(S+ ,D+ )SATzilla07+ (S++ ,D+ )SATzilla07 (S++ ,D+ )Descriptionversion entered 2007 SAT Competition (Section 3),evaluated extended test set.version built using design described Section 3,includes new complete solvers (Table 9) new data (Section 6.1).addition new complete solvers data, version uses local searchsolvers (Table 10) new design elements Section 5 exceptgeneral hierarchical hardness models (Section 5.5).version uses solvers, data new design elements. Unlikeversions, trained one variant solver usedata set categories.Table 12: different SATzilla versions evaluated second set experiments.achieved different changes, studied several intermediate SATzilla solvers,summarized Table 12.Observe solvers built using identical test data thus directlycomparable. generally expected solver outperform predecessors list.exception SATzilla07 (S++ ,D+ ): instead aiming increased performance,last solver designed achieve good performance across broader range instances.Thus, expected SATzilla07 (S++ ,D+ ) outperform others category ALL,outperform SATzilla07+ (S++ ,D+ ) specific categories.6.4 Constructing SATzilla07+ (S++ ,D+ ) SATzilla07 (S++ ,D+ )construction SATzilla07(S,D) already described Section 3;SATzilla07(S,D) differed test set used evaluate it, otherwise identical. construction SATzilla07(S+ ,D+ )SATzilla07(S,D), except relied different solvers corresponding trainingdata. SATzilla07+ (S++ ,D+ ) SATzilla07 (S++ ,D+ ) incorporated new techniques introduced Section 5. section briefly describe solversconstructed.used set features SATzilla07 (see Section 3.3). also usedexecution environment cutoff times. Pre-solvers identified automaticallydescribed Section 5.1, using (automatically determined) candidate solvers listedTable 13. final sets pre-solvers selected version SATzilla listedSection 7 (Tables 14, 17, 20 23). Based solvers scores validation data sets,591fiXu, Hutter, Hoos & Leyton-BrownRANDOMHANDMADEINDUSTRIALCompletePre-solverCandidatesKcnfs06March dl04March ksMarch dl04VallstMarch ksRsat 1.03PicosatRsat 2.0Minisat07March ksMarch dl04Local SearchPre-solverCandidatesAg2wsat0Gnovelty+SAPSAg2wsat0Ag2wsat+Gnovelty+Ag2wsat0Ag2wsat+Gnovelty+SAPSAg2wsat0Gnovelty+Table 13: Pre-solver candidates four data sets. candidates automaticallychosen based scores validation data achieved running respectivealgorithms maximum 10 CPU seconds.automatically determined backup solvers RANDOM, HANDMADE, INDUSTRIALMarch ks, March dl04, Eureka Eureka, respectively.built models predict performance score algorithm. score welldefined even case timeouts crashes; thus, need deal censoreddata. Like SATzilla07, SATzilla07+ used hierarchical empirical hardness models (Xuet al., 2007a) two underlying models (Msat Munsat ) predicting solvers score.SATzilla07 , built general hierarchical hardness models predicting scoresdescribed Section 5.5; models based six underlying empirical hardnessmodels (Msat Munsat trained data SAT competition category).chose solver subsets based results local search procedure subset search outlined Section 5.2. resulting final components SATzilla07,SATzilla07+ SATzilla07 category described detail followingsection.7. Evaluation II: Performance Analysis Improved SATzilla Versionssection, investigate effectiveness new techniques evaluatingfour SATzilla versions listed Table 12: SATzilla07(S,D0 ), SATzilla07(S+ ,D+ ),SATzilla07+ (S++ ,D+ ) SATzilla07 (S++ ,D+ ). evaluate performance,constructed simulated SAT competition using scoring function 2007SAT Competition, differing number important aspects. participantscompetition 19 solvers listed Tables 1, 9, 10 (all solvers consideredcategories), test instances Eo En described Tables 7 8. Furthermore, computational infrastructure (see Section 3.4) differed 2007 competition,also used shorter cutoff times 1200 seconds. reasons solvers rankedslightly differently simulated competition 2007 competition.7.1 RANDOM CategoryTable 14 shows configuration three different SATzilla versions designedRANDOM category. Note automatic solver selection SATzilla07+ (S++ ,D+r )included different solvers ones used SATzilla07(S+ ,D+);particular,choser592fiSATzilla: Portfolio-based Algorithm Selection SATSATzilla versionPre-Solvers (time)Component solversSATzilla07(S,D0r )March dl04(5); SAPS(2)Kcnfs06, March dl04, Rsat 1.03SATzilla07(S+ ,D+r )March dl04(5); SAPS(2)SATzilla07+ (S++ ,D+r )SAPS(2); Kcnfs06(2)Kcnfs06, March dl04, March ks,Minisat07Kcnfs06, March ks, Minisat07, Ranov,Ag2wsat+, Gnovelty+Table 14: SATzillas configuration RANDOM category; cutoff times pre-solvers specified CPU seconds.three local search solvers, Ranov, Ag2wsat+, Gnovelty+, availableSATzilla07. Also, automatic pre-solver selection chose different order cutofftime pre-solvers manual selection: chose first run SAPS two CPUseconds, followed two CPU seconds Kcnfs06. Even though running local searchalgorithm SAPS help solving unsatisfiable instances, see Figure 8 (left)SAPS solved many instances March dl04 first seconds.Table 15 shows performance different versions SATzilla compared bestsolvers RANDOM category. versions SATzilla outperformed every non-portfoliosolver terms average runtime number instances solved. SATzilla07+SATzilla07 , variants optimizing score rather another objective function, alsoclearly achieved higher scores non-portfolio solvers. always caseversions; example, SATzilla07(S+ ,D+r ) achieved 86.6% scorebest solver, Gnovelty+ (where scores computed based reference set 20reference solvers: 19 solvers Tables 1, 9, 10, well SATzilla07(S+ ,D+r )).Table 15 Figure 8 show adding complete solvers training data improveSATzilla07 much. time, substantial improvements achievednew mechanisms SATzilla07+ , leading 11% instances solved, reductionaverage runtime half, increase score 50%. Interestingly,performance general SATzilla07 (S++ ,D+ ) trained instance mixtested RANDOM category quite close best version SATzilla specificallydesigned RANDOM instances, SATzilla07+ (S++ ,D+r ). Note due excellentperformance satisfiable instances, local search solvers Table 15 (Gnovelty+Ag2wsat variants) tended higher overall scores complete solvers (Kcnfs04March ks) even though solved fewer instances particular could solveunsatisfiable instance. 2007 SAT Competition, however, winners randomSAT+UNSAT category complete solvers, lead us speculate local searchsolvers considered category (in random SAT category, winnersindeed local search solvers).Figure 8 presents CDFs summarizing performance best non-portfolio solvers,SATzilla solvers two oracles. non-portfolio solvers omitted CDFsshown. Section 4, oracles represent ideal versions SATzilla choose amongcomponent solvers perfectly without computational cost. specifically, giveninstance, oracle picks fastest algorithm; allowed consider SAPS (with593fiXu, Hutter, Hoos & Leyton-BrownSolverAvg. runtime [s]Solved [%]Performance scoreKcnfs04March ksAg2wsat0Ag2wsat+Gnovelty+85235147951041032.178.462.059.167.438309113666119919110218131703SATzilla07(S,D0r )SATzilla07(S+ ,D+r )SATzilla07+ (S++ ,D+r )SATzilla07 (S++ ,D+ )2312188411385.486.597.895.8(86.6%)(88.7%)189436 (143.8%)(137.8%)Table 15: performance SATzilla compared best solvers RANDOM. cutoff time1 200 CPU seconds; SATzilla07 (S++ ,D+ ) trained ALL. Scores computed based 20 reference solvers: 19 solvers Tables 1, 9, 10, well oneversion SATzilla. compute score non-SATzilla solver, SATzillaversion used member set reference solvers SATzilla07+ (S++ ,D+r ).)Since include SATzilla versions SATzilla07+ (S++ ,D+rset reference solvers, scores solvers incomparable scores givenhere, therefore report them. Instead, SATzilla solver, indicateparentheses performance score percentage highest score achievednon-portfolio solver, given reference set appropriate SATzilla solver tookplace SATzilla07+ (S++ ,D+r ).100% Instances Solved807090March_dl04March_ksGnovelty+80% Instances Solved90100Oracle(S++)SATzilla07+(S++,D+r)60504030207060Oracle(S++)Oracle(S)+ ++ +SATzilla07 (S ,Dr )SATzilla07(S+,D+r)SATzilla07(S,Dr)SATzilla07*(S++,D+)50403020Presolving(07(S+,D+r),07(S,D ))r100 110Presolving(07+(S++,D+r))010+AvgFeature(07 (S+,Dr ))110AvgFeature(07(S+,D+r),07(S,D ))r10++Presolving(others)210310Runtime [CPU sec]0 110AvgFeature(others)010110210Figure 8: Left: CDFs SATzilla07+ (S++ ,D+r ) best non-portfolio solvers RANDOM;right: CDFs different versions SATzilla RANDOM shown Table 14,SATzilla07 (S++ ,D+ ) trained ALL. solvers CDFs onesshown here.maximum runtime 10 CPU seconds) solver given set (S one oracleS++ other).Table 16 indicates often component solver SATzilla07+ (S++ ,D+r )selected, often successful, amount average runtime. found594310Runtime [CPU sec]fiSATzilla: Portfolio-based Algorithm Selection SATPre-Solver (Pre-Time)Solved [%]Avg. Runtime [CPU sec]SAPS(2)March dl04(2)52.29.61.11.68Selected SolverSelected [%]Success [%]Avg. Runtime [CPU sec]March dl04Gnovelty+March ksMinisat07RanovAg2wsat+34.828.823.94.44.04.096.293.992.610010077.8294.8143.6213.361.06.9357.9Table 16: solvers selected SATzilla07+ (S++ ,D+r ) RANDOM category. Notecolumn Selected [%] shows percentage instances remaining pre-solvingalgorithm selected, sums 100%. Cutoff times pre-solversspecified CPU seconds.solvers picked SATzilla07+ (S++ ,D+r ) solved given instance cases.Another interesting observation solvers success ratio high, averageruntime tended lower.7.2 HANDMADE Categoryconfigurations three SATzilla versions designed HANDMADE categoryshown Table 17. Again, SATzilla07+ (S++ ,D+h ) included three local search solvers,Ranov, Ag2wsat+ Gnovelty+, available SATzilla07. Likemanual choice SATzilla07, automatic pre-solver selection chose run March dl04five CPU seconds. Unlike manual selection, abstained using SAPS (or indeedsolver) second pre-solver. Table 18 shows performance differentversions SATzilla compared best solvers category HANDMADE. Here, halfobserved performance improvement achieved using solverstraining data; half due improvements SATzilla07+ . NoteHANDMADE category, SATzilla07 (S++ ,D+ ) performed quite poorly. attributeweakness feature-based classifier HANDMADE instances, issue discussSection 7.4.Table 19 indicates often component solver SATzilla07+ (S++ ,D+h )selected, many problem instances solved, average runtime runs.many solvers SATzilla07+ (S++ ,D+h ) picked quite rarely; however,cases, success ratios close 100%, average runtimes low.7.3 INDUSTRIAL CategoryTable 20 shows configuration three different SATzilla versions designedINDUSTRIAL category. Local search solvers performed quite poorly instancescategory, best local search solver, Ag2wsat0, solving 23% instances595fiXu, Hutter, Hoos & Leyton-BrownSATzillaPre-Solver (time)ComponentsSATzilla07(S,D0h )March dl04(5); SAPS(2)SATzilla07(S+ ,D+h)March dl04(5); SAPS(2)SATzilla07+ (S++ ,D+h)March ks(5)Kcnfs06, March dl04, Minisat 2.0,Rsat 1.03Vallst, Zchaff rand, TTS, MXC,March ks, Minisat07, Rsat 2.0Eureka, March dl04; Minisat 2.0,Rsat 1.03, Vallst, TTS, Picosat, MXC,March ks, TinisatElite, Minisat07,Rsat 2.0, Ranov, Ag2wsat0, Gnovelty+Table 17: SATzillas configuration HANDMADE category.SolverAvg. runtime [s]Solved [%]Performance scoreTTSMXCMarch ksMinisat07March dl0472952749443840841.161.963.968.972.44066943024688595986373226SATzilla07(S,D0h )SATzilla07(S+ ,D+h)SATzilla07+ (S++ ,D+h)SATzilla07 (S++ ,D+ )28420313121580.487.495.688.0(93.5%)(118.8%)112287 (153.3%)(110.5%)Table 18: performance SATzilla compared best solvers HANDMADE. Scores nonportfolio solvers computed using reference set SATzilla solver++SATzilla07+ (S++ ,D+,D+ )h ). Cutoff time: 1 200 CPU seconds; SATzilla07 (Strained ALL.100100AvgFeature(07+(S++,D+h))9070605040++Oracle(S )SATzila07+(S++,D+h)30Presolving(07+(S++,D+h))010110210AvgFeature(07(S+,D+h),07(S,Dh))AvgFeature(07+(S++,D+h))AvgFeature(others)706050Oracle(S++)Oracle(S)+ ++ +SATzilla07 (S ,Dh)4030SATzilla07(S+,D+h)20March_dl04March_ksMinisat072010 110Presolving(07(S+,D+h),07(S,Dh))80 Presolving(others)80% Instances Solved% Instances Solved90Presolving(07+(S++,D+h))SATzilla07(S,D )h10310Runtime [CPU sec]0 110SATzilla07*(S++,D+)010110210Runtime [CPU sec]Figure 9: Left: CDFs SATzilla07+ (S++ ,D+h ) best non-portfolio solvers HANDMADE;right: CDFs different versions SATzilla HANDMADE shown Table 17,SATzilla07 (S++ ,D+ ) trained ALL. solvers CDFs onesshown here.596310fiSATzilla: Portfolio-based Algorithm Selection SATPre-Solver (Pre-Time)Solved [%]Avg. Runtime [CPU sec]March ks(5)39.03.2Selected SolverSelected [%]Success [%]Avg. Runtime [CPU sec]Minisat07TTSMXCMarch ksEurekaMarch dl04Rsat 1.03PicosatAg2wsat0TinisatEliteRanovMinisat 2.0Rsat 2.0Gnovelty+Vallst40.411.57.27.25.85.84.83.93.42.92.91.41.41.00.589.391.793.310010091.710010010010083.366.7100100100205.1133.2310.5544.70.34317.6185.11.70.586.5206.1796.50.93.2<0.01Table 19: solvers selected SATzilla07+ (S++ ,D+h ) HANDMADE category.SATzillaPre-Solver (time)ComponentsSATzilla07(S,Di )Rsat 1.03 (2)SATzilla07(S+ ,D+)Rsat 2.0 (2)Eureka, March dl04, Minisat 2.0,Rsat 1.03Eureka, March dl04, Minisat 2.0,Zchaff Rand, TTS, Picosat, March ksSATzilla07+ (S++ ,D+)Rsat 2.0 (10); Gnovelty+(2)Eureka, March dl04, Minisat 2.0,Rsat 1.03, TTS, Picosat, Minisat07,Rsat 2.0Table 20: SATzillas configuration INDUSTRIAL category.within cutoff time. Consequently, local search solver selected automaticsolver subset selection SATzilla07+ (S++ ,D+). However, automatic pre-solver selectioninclude local search solver Gnovelty+ second pre-solver, run 2 CPUseconds 10 CPU seconds running Rsat 2.0.Table 21 compares performance different versions SATzilla bestsolvers INDUSTRIAL instances. surprising training datasolvers helped SATzilla07 improve terms metrics (avg. runtime, percentagesolved score). somewhat bigger improvement due new mechanismsSATzilla07+ led SATzilla07+ (S++ ,D+) outperforming every non-portfolio solverrespect every metric, specially terms performance score. Note generalSATzilla version SATzilla07 (S++ ,D+ ) trained achieved performance closeSATzilla07+ (S++ ,D+) INDUSTRIAL data set terms average runtimepercentage solved instances.597fiXu, Hutter, Hoos & Leyton-BrownSolverAvg. runtime [s]Solved [%]Performance scoreRsat 1.03Rsat 2.0PicosatTinisatEliteMinisat07Eureka35336528245237234980.880.885.970.876.683.2527405129966561408676000271505SATzilla07(S,D0i )SATzilla07(S+ ,D+)SATzilla07+ (S++ ,D+)SATzilla07 (S++ ,D+ )29826223323987.689.093.192.7(91.3%)(98.2%)79724 (111.5%)(104.8%)Table 21: performance SATzilla compared best solvers INDUSTRIAL. Scoresnon-portfolio solvers computed using reference setSATzilla solver SATzilla07+ (S++ ,D+Cutoff time: 1 200 CPU seconds;).SATzilla07 (S++ ,D+ ) trained ALL.Pre-Solver (Pre-Time)Solved [%]Avg. Runtime [CPU sec]Rsat 2.0(10)Gnovelty+ (2)38.10.36.82.0Selected SolverSelected [%]Success [%]Avg. Runtime [CPU sec]Eureka (BACKUP)EurekaPicosatMinisat07Minisat 2.0March dl04TTSRsat 2.0Rsat 1.0329.115.114.514.012.38.43.91.71.188.510096.284.068.286.7100100100385.4394.2179.6306.3709.2180.80.7281.610.6Table 22: solvers selected SATzilla07+ (S++ ,D+) INDUSTRIAL category.seen Figure 10, performance improvements achieved SATzillanon-portfolio solvers smaller INDUSTRIAL category categories. Note best INDUSTRIAL solver performed well, solving 85.9%instances within cutoff time 1 200 CPU seconds.6 Still, SATzilla07+ (S++ ,D+)significantly smaller average runtime (17%) solved 7.2% instances bestcomponent solver, Picosat. Likewise, score SATzilla07+ (S++ ,D+) 11.5%higher top-ranking component solver (in terms score), Eureka.6. Recall number means solver solved 85.9% instances could solved leastone solver. Compared data sets, seems either solvers exhibited similar behaviorINDUSTRIAL instances instances category exhibited greater variability hardness.598fiSATzilla: Portfolio-based Algorithm Selection SAT100100+90+++Presolving(07 (S ,Di ))++++AvgFeature(07 (S ,Di ))9080% Instances Solved% Instances Solved8070605040302010 11001011070Presolving(others)AvgFeature(07*(S++,D+))AvgFeature(others)50++Oracle(S )Oracle(S)+ ++ +SATzilla07 (S ,Di ))4030Rsat1.03Picosat202Presolving(07*(S++,D+))+ ++ +AvgFeature(07 (S ,Di ))60Oracle(S++)SATzilla07+(S++,D+i)10Presolving(07+(S++,D+i))SATzilla07(S+,D+i)SATzilla07(S,Di)*+++SATzilla07 (S ,D )310Runtime [CPU sec]10 110010110210310Runtime [CPU sec]CDFs SATzilla07+ (S++ ,D+) best non-portfolio solversINDUSTRIAL; right: CDFs different versions SATzilla INDUSTRIAL shownTable 20, SATzilla07 (S++ ,D+ ) trained ALL. solvers CDFs(including Eurekas) ones shown here.Figure 10: Left:Table 22 indicates often component solver SATzilla07+ (S++ ,D+) selected, many problem instances solved, average runtime runs.case, backup solver Eureka used problem instances feature computationtimed pre-solvers produce solution.7.4four versions SATzilla specialized category ALL. detailed configurations listed Table 23. results automatic pre-solver selection identicalSATzilla07+ SATzilla07 : chose first run local search solver SAPStwo CPU seconds, followed two CPU seconds March ks. solvers similar manual selection, order reversed. solver subset selection,SATzilla07+ SATzilla07 yielded somewhat different results, kepttwo local search algorithms, Ag2wsat+ & Ranov, Ag2wsat+ & Gnovelty+, respectively.Table 24 compares performance four versions SATzilla category.Roughly equal improvements terms performance metrics duetraining data solvers one hand, improvements SATzilla07+hand. best performance terms performance metrics obtainedSATzilla07 (S++ ,D+ ). Recall difference SATzilla07+ (S++ ,D+ )SATzilla07 (S++ ,D+ ) use general hierarchical hardness models,described Section 5.5.Note using classifier course good using oracle determiningdistribution instance comes from; thus, success ratios solvers selectedSATzilla07 instances test set distribution (see Table 25)slightly lower solvers picked SATzilla07+ distributionsindividually (see Tables 16, 19, 22). However, compared SATzilla07+distribution ALL, SATzilla07 performed significantly better: achieving overall performance599fiXu, Hutter, Hoos & Leyton-BrownSATzillaPre-Solver (time)ComponentsSATzilla07(S,D)March dl04(5); SAPS(2)Eureka, Kcnfs06, March dl04, Minisat2.0,Zchaff randSATzilla07(S+ ,D+ )March dl04(5); SAPS(2)SATzilla07+ (S++ ,D+ )SAPS(2); March ks(2)SATzilla07 (S++ ,D+ )SAPS(2); March ks(2)Eureka, March dl04, Zchaff rand,Kcnfs04, TTS, Picosat, March ks,Minisat07Eureka,Kcnfs06,Rsat 1.03,Zchaff rand, TTS, MXC, TinisatElite,Rsat 2.0, Ag2wsat+, RanovEureka, Kcnfs06, March dl04, Minisat2.0,Rsat 1.03,Picosat,MXC,Minisat07,Ag2wsat+,March ks,Gnovelty+Table 23: SATzillas configuration category.SolverAvg. runtime [s]Solved [%]Performance scoreRsat 1.03Kcnfs04TTSPicosatMarch ksTinisatEliteMinisat07Gnovelty+March dl0454296993957150969052868450961.121.322.657.762.947.361.843.962.7131399466957461613504920213393169162987156365205592SATzilla07(S,D)SATzilla07(S+ ,D+ )SATzilla07+ (S++ ,D+ )SATzilla07 (S++ ,D+ )28222419417283.187.091.192.9(125.0%)(139.2%)(158%)344594 (167.6%)Table 24: performance SATzilla compared best solvers ALL. Scores nonportfolio solvers computed using reference set SATzilla solverSATzilla07 (S++ ,D+ ). Cutoff time: 1 200 CPU seconds.improvements 11.3% lower average runtime, 1.8% solved instances 9.6% higherscore. supports initial hypothesis SATzilla07 would perform slightly worsespecialized versions SATzilla07+ single category, yet would yield bestresult applied broader heterogeneous set instances.runtime cumulative distribution function (Figure 11, right) showsSATzilla07 (S++ ,D+ ) dominated versions SATzilla solved30% instances best non-portfolio solver, March dl04 (Figure 11, left).Table 26 shows performance general classifier SATzilla07 (S++ ,D+ ).note several patterns: Firstly, classification performance RANDOM INDUSTRIAL in600fiSATzilla: Portfolio-based Algorithm Selection SAT100100Oracle(S++)Oracle(S)SATzilla07+(S++,D+)SATzilla07(S+,D+)SATzilla07(S,D)SATzilla07*(S++,D+)++Oracle(S )SATzilla07*(S++,D+)March_dl04Gnovelty+9080% Instances Solved% Instances Solved8090706050403070605040302020++Presolving(07(S ,D ),07(S,D))10+++Presolving(07*(S ,D ))0 110010++110++AvgFeature(07(S ,D ),07(S,D))10+AvgFeature(07*(S ,D ))Presolving(others)20 11031010AvgFeature(others)010121010310Runtime [CPU sec]Runtime [CPU sec]Figure 11: Left: CDF SATzilla07 (S++ ,D+ ) best non-portfolio solvers ALL; right:CDFs different versions SATzilla shown Table 23. solversCDFs ones shown here.Pre-Solver (Pre-Time)Solved [%]Avg. Runtime [CPU sec]SAPS(2)March ks (2)33.013.91.41.6Selected SolverSelected [%]Success [%]Avg. Runtime [CPU sec]Minisat07March dl04Gnovelty+March ksEureka (BACKUP)EurekaPicosatKcnfs06MXCRsat 1.03Minisat 2.0Ag2wsat+21.214.512.59.18.97.26.66.55.54.03.50.585.584.085.289.889.797.990.795.288.980.856.533.3247.5389.5273.2305.6346.1234.6188.6236.3334.0364.9775.7815.7Table 25: solvers selected SATzilla07 (S++ ,D+ ) category.stances much better HANDMADE instances. Secondly, HANDMADE instances,misclassifications due misclassification type instance,rather satisfiability status. Finally, see RANDOM instances almost perfectly classified RANDOM instances classifiedRANDOM, HANDMADE INDUSTRIAL instances confused somewhat often.comparably poor classification performance HANDMADE instances partly explainsSATzilla07 (S++ ,D+ ) perform well HANDMADE category others.601fiXu, Hutter, Hoos & Leyton-BrownR, satR, unsatH, satH, unsatI, satI, unsat92%5%1%1%1%4%94%1%1%classified H, sat57%38%5%classified H, unsat1%23%71%1%4%classified I, sat8%81%11%classified I, unsat5%6%89%classified R, satclassified R, unsatTable 26: Confusion matrix 6-way classifier data set ALL.8. ConclusionsAlgorithms combined portfolios build whole greater sumparts. significantly extended earlier work algorithm portfolios SATselect solvers per-instance basis using empirical hardness models runtime prediction. demonstrated effectiveness general portfolio construction method,SATzilla07, four large sets SAT competition instances. experiments showSATzilla07 portfolio solvers always outperform components. Furthermore,SATzilla07s excellent performance recent 2007 SAT Competition demonstratespractical effectiveness approach.work, pushed SATzilla approach beyond SATzilla07.first time, showed portfolios optimize complex scoring functions integratelocal search algorithms component solvers. Furthermore, showed automateprocess pre-solver selection, one last aspects approach previouslybased manual engineering. demonstrated extensive computational experiments,enhancements improved SATzilla07s performance substantially.SATzilla stage applied box given setpossible component solvers along representative training validation instances.automated built-in meta-optimization process, component solvers usedsolvers used pre-solvers automatically determined given setsolvers, without human effort. computational bottleneck execute possiblecomponent solvers representative set instances order obtain enough runtimedata build reasonably accurate empirical hardness models. However, computationsparallelized easily require human intervention, computer time,becomes ever cheaper. Matlab code building empirical hardness modelsC++ code building SATzilla portfolios use models available onlinehttp://www.cs.ubc.ca/labs/beta/Projects/SATzilla.interesting note use local search methods significant impactperformance SATzilla. preliminary experiments, observed overall performance SATzilla07 significantly weaker local search solverslocal-search-based features excluded. Specifically, availability local search602fiSATzilla: Portfolio-based Algorithm Selection SATcomponents substantially boosted SATzilla07 performance RANDOM instance category also led improvements INDUSTRIAL, resulted weaker performanceHANDMADE instances. Generally, believe better understanding impactfeatures runtime predictions instance categorizations allow usimprove SATzilla, therefore begun in-depth investigation direction.SATzillas performance ultimately depends power component solversautomatically gets better improved. Furthermore, SATzilla takes advantage solvers competitive certain kinds instances perform poorlyotherwise, thus SATzillas success demonstrates value solvers. Indeed,identification solvers, otherwise easily overlooked, stillpotential improve SATzillas performance substantially.Acknowledgmentswork builds contributions wide range past co-authors, colleagues,members SAT community. First, many colleagues thank contributions work described article. Eugene Nudelman, Alex Devkar YoavShoham Kevin Holgers co-authors papers first introduced SATzilla(Nudelman et al., 2004a, 2004b); work grew project automated algorithmselection involved Galen Andrew Jim McFadden, addition Kevin, EugeneYoav (Leyton-Brown et al., 2003b, 2003a). Nando de Freitas, Bart Selman, KevinMurphy gave useful suggestions machine learning algorithms, SAT instance features,mixtures experts, respectively. Second, academic research always buildsprevious work, especially indebted authors dozens SAT solversdiscuss paper, particularly commitment furthering scientific understanding making code publicly available. Without researchers considerableefforts, SATzilla could never built.ReferencesBacchus, F. (2002a). Enhancing Davis Putnam extended binary clause reasoning. ProceedingsEighteenth National Conference Artificial Intelligence (AAAI02), pp. 613619.Bacchus, F. (2002b). Exploring computational tradeoff reasoning less searching.Proceedings Fifth International Conference Theory Applications Satisfiability Testing (SAT02), pp. 716.Bacchus, F., & Winter, J. (2003). Effective preprocessing hyper-resolution equality reduction.Proceedings Sixth International Conference Theory Applications SatisfiabilityTesting (SAT03), pp. 341355.Biere, A. (2007). Picosat version 535. Solver description, SAT competition 2007.Biere, A., Cimatti, A., Clarke, E. M., Fujita, M., & Zhu, Y. (1999). Symbolic model checking using SATprocedures instead BDDs. Proceedings Design Automation Conference (DAC99), pp. 317320.Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.Bregman, D. R., & Mitchell, D. G. (2007). SAT solver MXC, version 0.5. Solver description, SATcompetition 2007.C. M. Li, W. W., & Zhang, H. (2007). Combining adaptive noise promising decreasing variables localsearch SAT. Solver description, SAT competition 2007.603fiXu, Hutter, Hoos & Leyton-BrownCarchrae, T., & Beck, J. C. (2005). Applying machine learning low-knowledge control optimizationalgorithms. Computational Intelligence, 21 (4), 372387.Crawford, J. M., & Baker, A. B. (1994). Experimental results application satisfiability algorithms scheduling problems. Proceedings Twelfth National Conference Artificial Intelligence (AAAI94), pp. 10921097.Davis, M., Logemann, G., & Loveland, D. (1962). machine program theorem proving. CommunicationsACM, 5 (7), 394397.Davis, M., & Putnam, H. (1960). computing procedure quantification theory. Journal ACM,7 (1), 201215.Dechter, R., & Rish, I. (1994). Directional resolution: Davis-Putnam procedure, revisited. PrinciplesKnowledge Representation Reasoning (KR94), pp. 134145.Dequen, G., & Dubois, O. (2007). kcnfs. Solver description, SAT competition 2007.Dubois, O., & Dequen, G. (2001). backbone-search heuristic efficient solving hard 3-SAT formulae.Proceedings Seventeenth International Joint Conference Artificial Intelligence (IJCAI01),pp. 248253.Een, N., & Sorensson, N. (2003). extensible SAT-solver. Proceedings Sixth InternationalConference Theory Applications Satisfiability Testing (SAT03), pp. 502518.Een, N., & Sorensson, N. (2006). Minisat v2.0 (beta). Solver description, SAT Race 2006.Gagliolo, M., & Schmidhuber, J. (2006a). Impact censored sampling performance restartstrategies. Twelfth Internatioal Conference Principles Practice Constraint Programming (CP06), pp. 167181.Gagliolo, M., & Schmidhuber, J. (2006b). Learning dynamic algorithm portfolios. Annals MathematicsArtificial Intelligence, 47 (3-4), 295328.Gebruers, C., Hnich, B., Bridge, D., & Freuder, E. (2005). Using CBR select solution strategiesconstraint programming. Proceedings Sixth International Conference Case-Based Reasoning (ICCBR05), pp. 222236.Gebruers, C., Guerri, A., Hnich, B., & Milano, M. (2004). Making choices using structure instancelevel within case based reasoning framework. International Conference Integration AITechniques Constraint Programming Combinatorial Optimization Problems (CPAIOR-04),pp. 380386.Gomes, C. P., & Selman, B. (2001). Algorithm portfolios. Artificial Intelligence, 126(1-2), 4362.Guerri, A., & Milano, M. (2004). Learning techniques automatic algorithm portfolio selection.Proceedings 16th European Conference Artificial Intelligence (ECAI-04), pp. 475479.Guo, H., & Hsu, W. H. (2004). learning-based algorithm selection meta-reasoner real-time MPEproblem. Proceedings Seventeenth Australian Conference Artificial Intelligence, pp. 307318.Guyon, I., Gunn, S., Nikravesh, M., & Zadeh, L. (2006). Feature Extraction, Foundations Applications.Springer.Heule, M., & v. Maaren, H. (2007). march ks. Solver description, SAT competition 2007.Heule, M., Zwieten, J., Dufour, M., & Maaren, H. (2004). March eq: implementing additional reasoningefficient lookahead SAT solver. Proceedings Seventh International Conference TheoryApplications Satisfiability Testing (SAT04), pp. 345359.Hoos, H. H. (2002). adaptive noise mechanism WalkSAT. Proceedings Eighteenth NationalConference Artificial Intelligence (AAAI02), pp. 655660.Hoos, H. H., & Stutzle, T. (2005). Stochastic Local Search - Foundations & Applications. Morgan KaufmannPublishers, San Francisco, CA, USA.Horvitz, E., Ruan, Y., Gomes, C. P., Kautz, H., Selman, B., & Chickering, D. M. (2001). Bayesianapproach tackling hard computational problems. Proceedings Seventeenth ConferenceUncertainty Artificial Intelligence (UAI01), pp. 235244.Huang, J. (2007). TINISAT SAT competition 2007. Solver description, SAT competition 2007.Huberman, B., Lukose, R., & Hogg, T. (1997). economics approach hard computational problems.Science, 265, 5154.604fiSATzilla: Portfolio-based Algorithm Selection SATHutter, F., Hamadi, Y., Hoos, H. H., & Leyton-Brown, K. (2006). Performance prediction automatedtuning randomized parametric algorithms. Twelfth Internatioal Conference PrinciplesPractice Constraint Programming (CP06), pp. 213228.Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling probabilistic smoothing: Efficient dynamiclocal search SAT. Proceedings Eighth International Conference Principles PracticeConstraint Programming, pp. 233248.Ishtaiwi, A., Thornton, J., Anbulagan, Sattar, A., & Pham, D. N. (2006). Adaptive clause weight redistribution. Twelfth Internatioal Conference Principles Practice Constraint Programming (CP06), pp. 229243.Kautz, H., & Selman, B. (1996). Pushing envelope: Planning, propositional logic, stochastic search.Proceedings Thirteenth National Conference Artificial Intelligence Eighth InnovativeApplications Artificial Intelligence Conference, pp. 11941201.Kautz, H. A., & Selman, B. (1999). Unifying SAT-based graph-based planning. ProceedingsSixteenth International Joint Conference Artificial Intelligence (IJCAI99), pp. 318325.Knuth, D. (1975). Estimating efficiency backtrack programs. Mathematics Computation, 29 (129),121136.Krishnapuram, B., Carin, L., Figueiredo, M., & Hartemink, A. (2005). Sparse multinomial logistic regression:Fast algorithms generalization bounds. IEEE Transactions Pattern Analysis MachineIntelligence, pp. 957968.Kullmann, O. (2002). Investigating behaviour SAT solver random formulas. http://cssvr1.swan.ac.uk/csoliver/Artikel/OKsolverAnalyse.html.Lagoudakis, M. G., & Littman, M. L. (2001). Learning select branching rules DPLL proceduresatisfiability. LICS/SAT, pp. 344359.Le Berre, D., & Simon, L. (2004). Fifty-five solvers Vancouver: SAT 2004 competition. ProceedingsSeventh International Conference Theory Applications Satisfiability Testing (SAT04),pp. 321344.Leyton-Brown, K., Nudelman, E., Andrew, G., McFadden, J., & Shoham, Y. (2003a). Boosting metaphoralgorithm design. Ninth Internatioal Conference Principles Practice ConstraintProgramming (CP03), pp. 899903.Leyton-Brown, K., Nudelman, E., Andrew, G., McFadden, J., & Shoham, Y. (2003b). portfolio approachalgorithm selection. Proceedings Eighteenth International Joint Conference ArtificialIntelligence (IJCAI03), pp. 15421543.Leyton-Brown, K., Nudelman, E., & Shoham, Y. (2002). Learning empirical hardness optimizationproblems: case combinatorial auctions. Eighth Internatioal Conference PrinciplesPractice Constraint Programming (CP02), pp. 556572.Li, C., & Huang, W. (2005). Diversification determinism local search satisfiability. ProceedingsEighth International Conference Theory Applications Satisfiability Testing (SAT05),pp. 158172.Lobjois, L., & Lematre, M. (1998). Branch bound algorithm selection performance prediction.Proceedings Fifteenth National Conference Artificial Intelligence (AAAI98), pp. 353358.Mahajan, Y. S., Fu, Z., & Malik, S. (2005). Zchaff2004: efficient SAT solver. Proceedings EighthInternational Conference Theory Applications Satisfiability Testing (SAT05), pp. 360375.Murphy, K. (2001). Bayes Net Toolbox Matlab. Computing Science Statistics, Vol. 33.http://bnt.sourceforge.net/.Nadel, A., Gordon, M., Palti, A., & Hanna, Z. (2006). Eureka-2006 SAT solver. Solver description, SATRace 2006.Nudelman, E., Leyton-Brown, K., Hoos, H. H., Devkar, A., & Shoham, Y. (2004a). Understanding randomSAT: Beyond clauses-to-variables ratio. Tenth Internatioal Conference PrinciplesPractice Constraint Programming (CP04), pp. 438452.Nudelman, E., Leyton-Brown, K., Devkar, A., Shoham, Y., & Hoos, H. (2004b). Satzilla: algorithmportfolio SAT. Solver description, SAT competition 2004.Pham, D. N., & Anbulagan (2007). Resolution enhanced SLS solver: R+AdaptNovelty+. Solver description,SAT competition 2007.605fiXu, Hutter, Hoos & Leyton-BrownPham, D. N., & Gretton, C. (2007). gNovelty+. Solver description, SAT competition 2007.Pipatsrisawat, K., & Darwiche, A. (2006). Rsat 1.03: SAT solver description. Tech. rep. D-152, AutomatedReasoning Group, UCLA.Pipatsrisawat, K., & Darwiche, A. (2007). Rsat 2.0: SAT solver description. Solver description, SATcompetition 2007.Rice, J. R. (1976). algorithm selection problem. Advances Computers, 15, 65118.Samulowitz, H., & Memisevic, R. (2007). Learning solve QBF. Proceedings TwentysecondNational Conference Artificial Intelligence (AAAI07), pp. 255260.Schmee, J., & Hahn, G. J. (1979). simple method regression analysis censored data. Technometrics,21 (4), 417432.Selman, B., Kautz, H., & Cohen, B. (1994). Noise strategies improving local search. ProceedingsTwelfth National Conference Artificial Intelligence (AAAI94), pp. 337343.Selman, B., Levesque, H., & Mitchell, D. (1992). new method solving hard satisfiability problems.Proceedings Tenth National Conference Artificial Intelligence (AAAI92), pp. 440446.Sorensson, N., & Een, N. (2007). Minisat2007. http://www.cs.chalmers.se/Cs/Research/FormalMethods/MiniSat/.Spence, I. (2007). Ternary tree solver (tts-4-0). Solver description, SAT competition 2007.Stephan, P., Brayton, R., & Sangiovanni-Vencentelli, A. (1996). Combinational test generation using satisfiability. IEEE Transactions Computer-Aided Design Integrated Circuits Systems, 15,11671176.Streeter, M., Golovin, D., & Smith, S. F. (2007). Combining multiple heuristics online. ProceedingsTwentysecond National Conference Artificial Intelligence (AAAI07), pp. 11971203.Subbarayan, S., & Pradhan, D. (2005). Niver: Non-increasing variable elimination resolution preprocessing sat instances. Lecture Notes Computer Science,Springer, 3542/2005, 276291.Tompkins, D. A. D., & Hoos, H. H. (2004). UBCSAT: implementation experimentation environmentSLS algorithms SAT & MAX-SAT.. Proceedings Seventh International ConferenceTheory Applications Satisfiability Testing (SAT04).Vallstrom, D. (2005). Vallst documentation. http://vallst.satcompetition.org/index.html.van Gelder, A. (2002). Another look graph coloring via propositional satisfiability. ProceedingsComputational Symposium Graph Coloring Generalizations (COLOR-02), pp. 4854.Wei, W., Li, C. M., & Zhang, H. (2007). Deterministic random selection variables local searchSAT. Solver description, SAT competition 2007.Xu, L., Hoos, H. H., & Leyton-Brown, K. (2007a). Hierarchical hardness models SAT. ThirteenthInternatioal Conference Principles Practice Constraint Programming (CP07), pp. 696711.Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2007b). Satzilla-07: design analysisalgorithm portfolio SAT. Thirteenth Internatioal Conference Principles PracticeConstraint Programming (CP07), pp. 712727.Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2007c). Satzilla2007: new & improved algorithmportfolio SAT. Solver description, SAT competition 2007.Zhang, L., Madigan, C. F., Moskewicz, M. W., & Malik, S. (2001). Efficient conflict driven learning Booleansatisfiability solver. Proceedings International Conference Computer Aided Design, pp.279285.Zhang, L. (2002). quest efficient Boolean satisfiability solvers. Proceedings 8th InternationalConference Computer Aided Deduction (CADE-02), pp. 313331.606fiJournal Artificial Intelligence Research 32 (2008) 663-704Submitted 03/08; published 07/08Online Planning Algorithms POMDPsStephane RossJoelle Pineaustephane.ross@mail.mcgill.cajpineau@cs.mcgill.caSchool Computer ScienceMcGill University, Montreal, Canada, H3A 2A7Sebastien PaquetBrahim Chaib-draaspaquet@damas.ift.ulaval.cachaib@damas.ift.ulaval.caDepartment Computer Science Software EngineeringLaval University, Quebec, Canada, G1K 7P4AbstractPartially Observable Markov Decision Processes (POMDPs) provide rich frameworksequential decision-making uncertainty stochastic domains. However, solvingPOMDP often intractable except small problems due complexity. Here,focus online approaches alleviate computational complexity computinggood local policies decision step execution. Online algorithms generally consist lookahead search find best action execute time stepenvironment. objectives survey various existing online POMDPmethods, analyze properties discuss advantages disadvantages;thoroughly evaluate online approaches different environments various metrics (return, error bound reduction, lower bound improvement). experimental resultsindicate state-of-the-art online heuristic search methods handle large POMDPdomains efficiently.1. IntroductionPartially Observable Markov Decision Process (POMDP) general model sequential decision problems partially observable environments. Many planning control problems modeled POMDPs, solved exactlycomputational complexity: finite-horizon POMDPs PSPACE-complete (Papadimitriou & Tsitsiklis, 1987) infinite-horizon POMDPs undecidable (Madani, Hanks,& Condon, 1999).last years, POMDPs generated significant interest AI community many approximation algorithms developed (Hauskrecht, 2000; Pineau,Gordon, & Thrun, 2003; Braziunas & Boutilier, 2004; Poupart, 2005; Smith & Simmons,2005; Spaan & Vlassis, 2005). methods offline algorithms, meaningspecify, prior execution, best action execute possible situations.approximate algorithms achieve good performance, often take significant time (e.g. hour) solve large problems, manypossible situations enumerate (let alone plan for). Furthermore, small changesenvironments dynamics require recomputing full policy, may take hours days.c2008AI Access Foundation. rights reserved.fiRoss, Pineau, Paquet, & Chaib-draahand, online approaches (Satia & Lave, 1973; Washington, 1997; Barto,Bradtke, & Singhe, 1995; Paquet, Tobin, & Chaib-draa, 2005; McAllester & Singh, 1999;Bertsekas & Castanon, 1999; Shani, Brafman, & Shimony, 2005) try circumvent complexity computing policy planning online current information state. Online algorithms sometimes also called agent-centered search algorithms (Koenig, 2001).Whereas offline search would compute exponentially large contingency plan considering possible happenings, online search considers current situation smallhorizon contingency plans. Moreover, approaches handle environmentchanges without requiring computation, allows online approaches applicable many contexts offline approaches applicable, instance,task accomplish, defined reward function, changes regularly environment.One drawback online planning generally needs meet real-time constraints,thus greatly reducing available planning time, compared offline approaches.Recent developments online POMDP search algorithms (Paquet, Chaib-draa, & Ross,2006; Ross & Chaib-draa, 2007; Ross, Pineau, & Chaib-draa, 2008) suggest combiningapproximate offline online solving approaches may efficient way tacklelarge POMDPs. fact, generally compute rough policy offline using existingoffline value iteration algorithms, use approximation heuristic functionguide online search algorithm. combination enables online search algorithmsplan shorter horizons, thereby respecting online real-time constraints retaininggood precision. exact online search fixed horizon, guaranteereduction error approximate offline value function. overall time (offlineonline) required obtain good policy dramatically reduced combiningapproaches.main purpose paper draw attention AI community onlinemethods viable alternative solving large POMDP problems. support this,first survey various existing online approaches applied POMDPs,discuss strengths drawbacks. present various combinations online algorithmsvarious existing offline algorithms, QMDP (Littman, Cassandra, & Kaelbling,1995), FIB (Hauskrecht, 2000), Blind (Hauskrecht, 2000; Smith & Simmons, 2005)PBVI (Pineau et al., 2003). compare empirically different online approachestwo large POMDP domains according different metrics (average discounted return, errorbound reduction, lower bound improvement). also evaluate available onlineplanning time offline planning time affect performance different algorithms.results experiments show many state-of-the-art online heuristic search methodstractable large state observation spaces, achieve solution quality stateof-the-art offline approaches fraction computational cost. best methodsachieve focusing search relevant future outcomes currentdecision, e.g. likely high uncertainty (error) longterm values, minimize quickly possible error bound performancebest action found. tradeoff solution quality computing time offeredcombinations online offline approaches attractive tackling increasinglylarge domains.664fiOnline Planning Algorithms POMDPs2. POMDP ModelPartially observable Markov decision processes (POMDPs) provide general frameworkacting partially observable environments (Astrom, 1965; Smallwood & Sondik, 1973;Monahan, 1982; Kaelbling, Littman, & Cassandra, 1998). POMDP generalizationMDP model planning uncertainty, gives agent abilityeffectively estimate outcome actions even cannot exactly observe stateenvironment.Formally, POMDP represented tuple (S, A, T, R, Z, O) where:set environment states. state description environmentspecific moment capture information relevant agentsdecision-making process.set possible actions.: [0, 1] transition function, (s, a, s0 ) = Pr(s0 |s, a)represents probability ending state s0 agent performs action states.R : R reward function, R(s, a) reward obtainedexecuting action state s.Z set possible observations.: Z [0, 1] observation function, O(s0 , a, z) = Pr(z|a, s0 ) givesprobability observing z action performed resulting state s0 .assume paper S, Z finite R bounded.key aspect POMDP model assumption states directlyobservable. Instead, given time, agent access observationz Z gives incomplete information current state. Since statesobservable, agent cannot choose actions based states. considercomplete history past actions observations choose current action.history time defined as:ht = {a0 , z1 , . . . , zt1 , at1 , zt }.(1)explicit representation past typically memory expensive. Instead,possible summarize relevant information previous actions observationsprobability distribution state space S, called belief state (Astrom, 1965).belief state time defined posterior probability distributionstate, given complete history:bt (s) = Pr(st = s|ht , b0 ).(2)belief state bt sufficient statistic history ht (Smallwood & Sondik, 1973),therefore agent choose actions based current belief state bt insteadpast actions observations. Initially, agent starts initial belief state b0 ,665fiRoss, Pineau, Paquet, & Chaib-draarepresenting knowledge starting state environment. Then, timet, belief state bt computed previous belief state bt1 , using previousaction at1 current observation zt . done belief state update function(b, a, z), bt = (bt1 , at1 , zt ) defined following equation:bt (s0 ) = (bt1 , at1 , zt )(s0 ) =1Pr(zt |bt1 , at1 )O(s0 , at1 , zt )X(s, at1 , s0 )bt1 (s), (3)sSPr(z|b, a), probability observing z action belief b, actsnormalizing constant bt remains probability distribution:Pr(z|b, a) =XO(s0 , a, z)s0X(s, a, s0 )b(s).(4)sSagent way computing belief, next interesting questionchoose action based belief state.action determined agents policy , specifying probabilityagent execute action given belief state, i.e. defines agents strategypossible situations could encounter. strategy maximize amountreward earned finite infinite time horizon. article, restrict attentioninfinite-horizon POMDPs optimality criterion maximize expectedsum discounted rewards (also called return discounted return). formally,optimal policy defined following equation:#"XX X(5)bt (s)R(s, a)(bt , a) |b0 ,= argmax Et=0sSaA[0, 1) discount factor (bt , a) probability actionperformed belief bt , prescribed policy .return obtained following specific policy , certain belief state b,defined value function equation V :"#XXV (b) =(b, a) RB (b, a) +Pr(z|b, a)V ( (b, a, z)) .(6)aAzZfunction RB (b, a) specifies immediate expected reward executing actionbelief b according reward function R:RB (b, a) =Xb(s)R(s, a).(7)sSsum Z Equation 6 interpreted expected future return infinitehorizon executing action a, assuming policy followed afterwards.Note definitions RB (b, a), Pr(z|b, a) (b, a, z), one viewPOMDP MDP belief states (called belief MDP), Pr(z|b, a) specifiesprobability moving b (b, a, z) action a, RB (b, a) immediatereward obtained action b.666fiOnline Planning Algorithms POMDPsoptimal policy defined Equation 5 represents action-selection strategymaximize equation V (b0 ). Since always exists deterministic policymaximizes V belief states (Sondik, 1978), generally consider deterministic policies (i.e. assign probability 1 specific action every beliefstate).value function V optimal policy fixed point Bellmans equation(Bellman, 1957):"#XV (b) = max RB (b, a) +Pr(z|b, a)V ( (b, a, z)) .(8)aAzZAnother useful quantity value executing given action belief state b,denoted Q-value:Q (b, a) = RB (b, a) +XPr(z|b, a)V ( (b, a, z)).(9)zZdifference definition V max operator omitted. NoticeQ (b, a) determines value assuming optimal policy followedevery step action a.review different offline methods solving POMDPs. used guideonline heuristic search methods discussed later, cases formbasis online solutions.2.1 Optimal Value Function AlgorithmOne solve optimally POMDP specified finite horizon H using valueiteration algorithm (Sondik, 1971). algorithm uses dynamic programming computeincreasingly accurate values belief state b. value iteration algorithmbegins evaluating value belief state immediate horizon = 1. Formally,let V value function takes belief state parameter returns numericalvalue R belief state. initial value function is:V1 (b) = max RB (b, a).aA(10)value function horizon constructed value function horizon 1using following recursive equation:"#XVt (b) = max RB (b, a) +Pr(z|b, a)Vt1 ( (b, a, z)) .(11)aAzZvalue function Equation 11 defines discounted sum expected rewardsagent receive next time steps, belief state b. Therefore, optimalpolicy finite horizon simply choose action maximizing Vt (b):"#XPr(z|b, a)Vt1 ( (b, a, z)) .(12)(b) = argmax RB (b, a) +aAzZ667fiRoss, Pineau, Paquet, & Chaib-draalast equation associates action specific belief state, therefore mustcomputed possible belief states order define full policy.key result Smallwood Sondik (1973) shows optimal value functionfinite-horizon POMDP represented hyperplanes, therefore convexpiecewise linear. means value function Vt horizon representedset |S|-dimensional hyperplanes: = {0 , 1 , . . . , }. hyperplanes oftencalled -vectors. defines linear value function belief state space associatedaction A. value belief state maximum value returned one-vectors belief state. best action one associated -vectorreturned best value:X(s)b(s).(13)Vt (b) = maxsSnumber exact value function algorithms leveraging piecewise-linear convexaspects value function proposed POMDP literature (Sondik, 1971;Monahan, 1982; Littman, 1996; Cassandra, Littman, & Zhang, 1997; Zhang & Zhang,2001). problem exact approaches number -vectorsneeded represent value function grows exponentially number observationsiteration, i.e. size set O(|A||t1 ||Z| ). Since new -vectorrequires computation time O(|Z||S|2 ), resulting complexity iteration exactapproaches O(|A||Z||S|2 |t1 ||Z| ). work exact approaches focusedfinding efficient ways prune set , effectively reduce computation.2.2 Offline Approximate AlgorithmsDue high complexity exact solving approaches, many researchers workedimproving applicability POMDP approaches developing approximate offlineapproaches applied larger problems.online methods review below, approximate offline algorithms often usedcompute lower upper bounds optimal value function. boundsleveraged orient search promising directions, apply branch-and-bound pruningtechniques, estimate long term reward belief states, show Section3. However, generally want use approximate methods require lowcomputational cost. particularly interested approximations useunderlying MDP1 compute lower bounds (Blind policy) upper bounds (MDP, QMDP,FIB) exact value function. also investigate usefulness using preciselower bounds provided point-based methods. briefly review offline methodsfeatured empirical investigation. recent publications providecomprehensive overview offline approximate algorithms (Hauskrecht, 2000; Pineau,Gordon, & Thrun, 2006).2.2.1 Blind policyBlind policy (Hauskrecht, 2000; Smith & Simmons, 2005) policyaction always executed, regardless belief state. value function Blind1. MDP defined (S, A, T, R) components POMDP model.668fiOnline Planning Algorithms POMDPspolicy obviously lower bound V since corresponds value one specificpolicy agent could execute environment. resulting value functionspecified set |A| -vectors, -vector specifies long term expectedreward following corresponding blind policy. -vectors computed usingsimple update rule:at+1 (s) = R(s, a) +X(s, a, s0 )at (s),(14)s0a0 = minsS R(s, a)/(1). -vectors computed, use Equation 13obtain lower bound value belief state. complexity iterationO(|A||S|2 ), far less exact methods. lower boundcomputed quickly, usually tight thus informative.2.2.2 Point-Based Algorithmsobtain tighter lower bounds, one use point-based methods (Lovejoy, 1991; Hauskrecht,2000; Pineau et al., 2003). popular approach approximates value function updating selected belief states. point-based methods sample beliefstates simulating random interactions agent POMDP environment,update value function gradient sampled beliefs. approaches circumvent complexity exact approaches sampling small set beliefsmaintaining one -vector per sampled belief state. Let B represent setsampled beliefs, set -vectors time obtained follows:(s)a,zbt====R(s, a),Pa,z0 , a, z)0 (s0 ), 0{a,zs0 (s, a, s0 )O(st1 },|i (s) = PPa,z{b |b = + zZ argmaxtsS (s)b(s), A},P{b |b = argmaxb sS b(s)(s), b B}.(15)ensure gives lower bound, 0 initialized single -vector 0 (s) =mins0 S,aA R(s0 ,a).1Since |t1 | |B|, iteration complexity O(|A||Z||S||B|(|S|+|B|)), polynomial time, compared exponential time exact approaches.Different algorithms developed using point-based approach: PBVI (Pineauet al., 2003), Perseus (Spaan & Vlassis, 2005), HSVI (Smith & Simmons, 2004, 2005)recent methods. methods differ slightly choosebelief states update value function chosen belief states.nice property approaches one tradeoff complexityalgorithm precision lower bound increasing (or decreasing) numbersampled belief points.2.2.3 MDPMDP approximation consists approximating value function V POMDPvalue function underlying MDP (Littman et al., 1995). value functionupper bound value function POMDP computed using Bellmansequation:669fiRoss, Pineau, Paquet, & Chaib-draa"DPVt+1(s) = max R(s, a) +aAX#(s, a, s0 )VtM DP (s0 ) .s0(16)Pvalue V (b) belief state b computed V (b) = sS V DP (s)b(s).computed quickly, iteration Equation 16 done O(|A||S|2 ).2.2.4 QMDPQMDP approximation slight variation MDP approximation (Littman et al.,1995). main idea behind QMDP consider partial observability disappearsingle step. assumes MDP solution computed generate VtM DP (Equation16). Given this, define:DPQMt+1 (s, a) = R(s, a) +X(s, a, s0 )VtM DP (s0 ).(17)s0approximation defines -vector action, gives upper bound Vtighter V DP ( i.e. VtQM DP (b) VtM DP (b) belief b). Again,obtain value belief state, use Equation 13, contain one -vectorDP (s, a) A.(s) = QM2.2.5 FIBtwo upper bounds presented far, QMDP MDP, take accountpartial observability environment. particular, information-gathering actionsmay help identify current state always suboptimal according bounds.address problem, Hauskrecht (2000) proposed new method compute upper bounds,called Fast Informed Bound (FIB), able take account (to degree)partial observability environment. -vector update process describedfollows:at+1 (s) = R(s, a) +XzZa0maxXO(s0 , a, z)T (s, a, s0 )t (s0 ).(18)s0initialized -vectors found QMDP convergence, i.e.-vectorsa0 (s) = QM DP (s, a). FIB defines single -vector action value beliefstate computed according Equation 13. FIB provides tighter upper boundQMDP ( i.e. VtF IB (b) VtQM DP (b) b ). complexity algorithm remainsacceptable, iteration requires O(|A|2 |S|2 |Z|) operations.3. Online Algorithms POMDPsoffline approaches, algorithm returns policy defining action executeevery possible belief state. approaches tend applicable dealingsmall mid-size domains, since policy construction step takes significant time. largePOMDPs, using rough value function approximation (such ones presentedSection 2.2) tends substantially hinder performance resulting approximate670fiOnline Planning Algorithms POMDPsOffline ApproachesPolicy ConstructionPolicy ExecutionOnline ApproachesSmall policy construction step policy execution stepsFigure 1: Comparison offline online approaches.policy. Even recent point-based methods produce solutions limited qualitylarge domains (Paquet et al., 2006).Hence large POMDPs, potentially better alternative use online approach,tries find good local policy current belief state agent.advantage approach needs consider belief states reachable current belief state. focuses computation small set beliefs.addition, since online planning done every step (and thus generalization beliefsrequired), sufficient calculate maximal value current beliefstate, full optimal -vector. setting, policy construction stepsexecution steps interleaved one another shown Figure 1. cases, onlineapproaches may require extra execution steps (and online planning), since policylocally constructed therefore always optimal. However policy construction timeoften substantially shorter. Consequently, overall time policy constructionexecution normally less online approaches (Koenig, 2001). practice, potentiallimitation online planning need meet short real-time constraints.case, time available construct plan small compared offline algorithms.3.1 General Framework Online Planningsubsection presents general framework online planning algorithms POMDPs.Subsequently, discuss specific approaches literature describe varytackling various aspects general framework.online algorithm divided planning phase, execution phase,applied alternately time step.planning phase, algorithm given current belief state agentcomputes best action execute belief. usually achieved two steps.First tree reachable belief states current belief state built lookingseveral possible sequences actions observations taken currentbelief. tree, current belief root node subsequent reachable beliefs (ascalculated (b, a, z) function Equation 3) added tree child nodesimmediate previous belief. Belief nodes represented using OR-nodes (atmust choose action) actions included layer belief nodes usingAND-nodes (at must consider possible observations lead subsequent671fiRoss, Pineau, Paquet, & Chaib-draab0[14.4, 18.7]13[14.4, 17.9][12, 18.7]a1a20.7z1[13.7, 16.9]b1[15, 20]b2z1[6, 14] b5b30.5z2[9, 15]b4[10, 18]4a10.6z1z2-1[5.8, 11.5]0.50.3a20.20.4z2b6[9, 12]z1[13.7, 16.9]0.8[11, 20]z2b8b7[10, 12]Figure 2: AND-OR tree constructed search process POMDP 2 actions 2 observations. belief states OR-nodes represented triangular nodes action AND-nodescircular nodes. rewards RB (b, a) represented values outgoing arcsOR-nodes probabilities Pr(z|b, a) shown outgoing arcs AND-nodes.values inside brackets represent lower upper bounds computed accordingEquations 19 - 22, assuming discount factor = 0.95. Also notice exampleaction a1 belief state b1 could pruned since upper bound (= 11.5) lowerlower bound (= 13.7) action a2 b1 .beliefs). value current belief estimated propagating value estimatesfringe nodes, ancestors, way root, according Bellmansequation (Equation 8). long-term value belief nodes fringe usually estimatedusing approximate value function computed offline. methods also maintainlower bound upper bound value node. exampletree contructed evaluated presented Figure 2.planning phase terminates, execution phase proceeds executing bestaction found current belief environment, updating current belieftree according observation obtained.Notice general, belief MDP could graph structure cycles.online algorithms handle structure unrolling graph tree. Hence,reach belief already elsewhere tree, duplicated. algorithms couldalways modified handle generic graph structures using technique proposedLAO* algorithm (Hansen & Zilberstein, 2001) handle cycles. Howeveradvantages disadvantages this. in-depth discussion issuepresented Section 5.4.generic online algorithm implementing planning phase (lines 5-9) executionphase (lines 10-13) presented Algorithm 3.1. algorithm first initializes treecontain initial belief state (line 2). given current tree, planning phasealgorithm proceeds first selecting next fringe node (line 6)pursue search (construction tree). Expand function (line 7) constructs672fiOnline Planning Algorithms POMDPs1: Function OnlinePOMDPSolver()Static: bc : current belief state agent.: AND-OR tree representing current search tree.D: Expansion depth.L: lower bound V .U : upper bound V .2:3:4:5:6:7:8:9:10:11:12:13:14:bc b0Initialize contain bc rootExecutionTerminated()PlanningTerminated()b ChooseNextNodeToExpand()Expand(b , D)UpdateAncestors(b )endExecute best action bcPerceive new observation zbc (bc , a, z)Update tree bc new rootendAlgorithm 3.1: Generic Online Algorithm.next reachable beliefs (using Equation 3) selected leaf pre-determinedexpansion depth evaluates approximate value function newly creatednodes. new approximate value expanded node propagated ancestorsvia UpdateAncestors function (line 8). planning phase conductedterminating condition met (e.g. planning time available -optimal actionfound).execution phase algorithm executes best action found planning(line 10) gets new observation environment (line 11). Next, algorithmupdates current belief state search tree according recent actionobservation z (lines 12-13). online approaches reuse previous computationskeeping subtree new belief resuming search subtreenext time step. cases, algorithm keeps nodes treenew belief bc deletes nodes tree. algorithm loops backplanning phase next time step, task terminated.side note, online planning algorithm also useful improve precisionapproximate value function computed offline. captured Theorem 3.1.Theorem 3.1. (Puterman, 1994; Hauskrecht, 2000) Let V approximate value function = supb |V (b) V (b)|. approximate value V (b) returned Dstep lookahead belief b, using V estimate fringe node values, error bounded|V (b) V (b)| .notice [0, 1), error converges 0 depth search tends. indicates online algorithm effectively improve performance obtainedapproximate value function computed offline, find action arbitrarily closeoptimal current belief. However, evaluating tree reachable beliefswithin depth complexity O((|A||Z|)D |S|2 ), exponential D.becomes quickly intractable large D. Furthermore, planning time availableexecution may short exploring beliefs depth may infeasible.673fiRoss, Pineau, Paquet, & Chaib-draaHence motivates need efficient online algorithms guarantee similarbetter error bounds.efficient, online algorithms focus limiting number reachable beliefs explored tree (or choose relevant ones). approachesgenerally differ subroutines ChooseNextNodeToExpand Expandimplemented. classify approaches three categories : Branch-and-BoundPruning, Monte Carlo Sampling Heuristic Search. present surveyapproaches discuss strengths drawbacks. online algorithmsproceed via tree search; approaches discussed Section 3.5.3.2 Branch-and-Bound PruningBranch-and-Bound pruning general search technique used prune nodesknown suboptimal search tree, thus preventing expansion unnecessarylower nodes. achieve AND-OR tree, lower bound upper boundmaintained value Q (b, a) action a, every belief b tree.bounds computed first evaluating lower upper bound fringe nodestree. bounds propagated parent nodes according followingequations:L(b),b F(T )LT (b) =(19)maxaA LT (b, a), otherwiseXLT (b, a) = RB (b, a) +Pr(z|b, a)LT ( (b, a, z)),(20)zZU (b),b F(T )maxaA UT (b, a), otherwiseXUT (b, a) = RB (b, a) +Pr(z|b, a)UT ( (b, a, z)),UT (b) =(21)(22)zZF(T ) denotes set fringe nodes tree , UT (b) LT (b) represent upperlower bounds V (b) associated belief state b tree , UT (b, a) LT (b, a)represent corresponding bounds Q (b, a), L(b) U (b) bounds used fringenodes, typically computed offline. equations equivalent Bellmans equation(Equation 8), however use lower upper bounds children, instead V .Several techniques presented Section 2.2 used quickly compute lower bounds(Blind policy) upper bounds (MDP, QMDP, FIB) offline.Given bounds, idea behind Branch-and-Bound pruning relatively simple:given action belief b upper bound UT (b, a) lower another actionlower bound LT (b, a), know guaranteed value Q (b, a) Q (b, a).Thus suboptimal belief b. Hence branch pruned belief reachedtaking action b considered.3.2.1 RTBSSReal-Time Belief Space Search (RTBSS) algorithm uses Branch-and-Bound approachcompute best action take current belief (Paquet et al., 2005, 2006). Starting674fiOnline Planning Algorithms POMDPs1: Function Expand(b, d)Inputs: b:d:Static: :L:U:2:3:4:5:6:7:8:9:10:11:12:13:14:belief node want expand.depth expansion b.AND-OR tree representing current search tree.lower bound V .upper bound V .= 0LT (b) L(b)elseSort actions {a1 , a2 , . . . , a|A| } U (b, ai ) U (b, aj ) ji1LT (b)|A| U (b, ai ) >PLT (b)LT (b, ai ) RB (b, ai ) + zZ Pr(z|b, ai )Expand( (b, ai , z), 1)LT (b) max{LT (b), LT (b, ai )}ii+1endendreturn LT (b)Algorithm 3.2: Expand subroutine RTBSS.current belief, expands AND-OR tree depth-first search fashion,pre-determined search depth D. leaves tree evaluated using lowerbound computed offline, propagated upwards lower bound maintainednode tree.limit number nodes explored, Branch-and-Bound pruning used along wayprune actions known suboptimal, thus excluding unnecessary nodesactions. maximize pruning, RTBSS expands actions descending orderupper bound (first action expanded one highest upper bound). expandingactions order, one never expands action could pruned actionsexpanded different order. Intuitively, action higher upper boundactions, cannot pruned actions since lowerbound never exceed upper bound. Another advantage expanding actionsdescending order upper bound soon find actionpruned, also know remaining actions pruned, since upperbounds necessarily lower. fact RTBSS proceeds via depth-first search alsoincreases number actions pruned since bounds expanded actionsbecome precise due search depth.terms framework Algorithm 3.1, RTBSS requires ChooseNextNodeToExpand subroutine simply return current belief bc . UpdateAncestors function need perform operation since bc ancestor (root tree). Expand subroutine proceeds via depth-first search fixed depth D, usingBranch-and-Bound pruning, mentioned above. subroutine detailed Algorithm3.2. expansion performed, PlanningTerminated evaluates truebest action found executed. end time step, tree simply reinitializedcontain new current belief root node.efficiency RTBSS depends largely precision lower upper boundscomputed offline. bounds tight, pruning possible, searchefficient. algorithm unable prune many actions, searching675fiRoss, Pineau, Paquet, & Chaib-draalimited short horizons order meet real-time constraints. Another drawbackRTBSS explores observations equally. inefficient since algorithmcould explore parts tree small probability occurring thussmall effect value function. result, number observations large,algorithm limited exploring short horizon.final note, since RTBSS explores reacheable beliefs within depth (exceptreached suboptimal actions), guarantee error bound D-steplookahead (see Theorem 3.1). Therefore, online search directly improves precisionoriginal (offline) value bounds factor . aspect confirmed empiricallydifferent domains RTBSS authors combined online search bounds givenvarious offline algorithms. cases, results showed tremendous improvementpolicy given offline algorithm (Paquet et al., 2006).3.3 Monte Carlo Samplingmentioned above, expanding search tree fully large set observationsinfeasible except shallow depths. cases, better alternative may samplesubset observations expansion consider beliefs reached sampledobservations. reduces branching factor search allows deeper searchwithin set planning time. strategy employed Monte Carlo algorithms.3.3.1 McAllester Singhapproach presented McAllester Singh (1999) adaptation online MDPalgorithm presented Kearns, Mansour, Ng (1999). consists depth-limitedsearch AND-OR tree certain fixed horizon instead exploringobservations action choice, C observations sampled generative model.probabilities Pr(z|b, a) approximated using observed frequencies sample.advantage approach sampling observation distributionPr(z|b, a) achieved efficiently O(log |S| + log |Z|), computing exactprobabilities Pr(z|b, a) O(|S|2 ) observation z. Thus sampling usefulalleviate complexity computing Pr(z|b, a), expense less precise estimate.Nevertheless, samples often sufficient obtain good estimate observationseffect Q (b, a) (i.e. occur high probability)likely sampled. authors also apply belief state factorization BoyenKoller (1998) simplify belief state calculations.implementation algorithm, Expand subroutine expands treefixed depth D, using Monte Carlo sampling observations, mentioned (seeAlgorithm 3.3). end time step, tree reinitialized containnew current belief root.Kearns et al. (1999) derive bounds depth number samples C neededobtain -optimal policy high probability show number samplesrequired grows exponentially desired accuracy. practice, number samplesrequired infeasible given realistic online time constraints. However, performance termsreturns usually good even many fewer samples.676fiOnline Planning Algorithms POMDPs1: Function Expand(b, d)Inputs: b:d:Static: :C:2:3:4:5:6:7:8:9:10:11:12:belief node want expand.depth expansion b.AND-OR tree representing current search tree.number observations sample.= 0LT (b) maxaA RB (b, a)elseLT (b)Sample Z = {z1 , z2 , . . . zC } distribution Pr(z|b, a)PN (Z)LT (b, a) RB (b, a) + zZ|Nz (Z)>0 zC Expand( (b, a, z), 1)LT (b) max{LT (b), LT (b, a)}endendreturn LT (b)Algorithm 3.3: Expand subroutine McAllester Singhs Algorithm.One inconvenience method action pruning done since MonteCarlo estimation guaranteed correctly propagate lower (and upper) boundproperty tree. article, authors simply approximate valuefringe belief states immediate reward RB (b, a); could improved usinggood estimate V computed offline. Note also approach may difficultapply domains number actions |A| large. course mayimpact performance.3.3.2 RolloutAnother similar online Monte Carlo approach Rollout algorithm (Bertsekas & Castanon, 1999). algorithm requires initial policy (possibly computed offline).time step, estimates future expected value action, assuming initial policy followed future time steps, executes action highest estimated value.estimates obtained computing average discounted return obtainedset sampled trajectories depth D. trajectories generated first takingaction evaluated, following initial policy subsequent belief states,assuming observations sampled generative model. Since approachneeds consider different actions root belief node, number actions |A|influences branching factor first level tree. Consequently, generallyscalable McAllester Singhs approach. Bertsekas Castanon (1999) alsoshow enough sampling, resulting policy guaranteed perform leastwell initial policy high probability. However, generally requires many sampledtrajectories provide substantial improvement initial policy. Furthermore,initial policy significant impact performance approach. particular,cases might impossible improve return initial policy changingimmediate action (e.g. several steps need changed reach specific subgoalhigher rewards associated). cases, Rollout policy never improveinitial policy.677fiRoss, Pineau, Paquet, & Chaib-draa1: Function Expand(b, d)Inputs: b: belief node want expand.d: depth expansion b.Static: : AND-OR tree representing current search tree.: set initial policies.: number trajectories depth sample.2: LT (b)3:4:5:Q (b, a) 06:= 17:b b8:9:j = 01 j10:Q (b, a) Q (b, a) +RB (b, a)11:z SampleObservation(b, a)12:b (b, a, z)13:(b)14:end15:end16: end17: LT (b, a) = max Q (b, a)18: endAlgorithm 3.4: Expand subroutine Parallel Rollout Algorithm.address issue relative initial policy, Chang, Givan, Chong (2004)introduced modified version algorithm, called Parallel Rollout. case,algorithm starts set initial policies. algorithm proceeds Rolloutinitial policies set. value considered immediate actionmaximum set initial policies, action highest value executed.algorithm, policy obtained guaranteed perform least well bestinitial policy high probability, given enough samples. Parallel Rollout handledomains large number actions observations, perform wellset initial policies contain policies good different regions belief space.Expand subroutine Parallel Rollout algorithm presented Algorithm 3.4.original Rollout algorithm Bertsekas Castanon (1999) algorithmspecial case set initial policies contains one policy.subroutines proceed McAllester Singhs algorithm.3.4 Heuristic SearchInstead using Branch-and-Bound pruning Monte Carlo sampling reduce branching factor search, heuristic search algorithms try focus search relevant reachable beliefs using heuristics select best fringe beliefs node expand.relevant reachable beliefs ones would allow search algorithmmake good decisions quickly possible, i.e. expanding nodes possible.three different online heuristic search algorithms POMDPsproposed past: Satia Lave (1973), BI-POMDP (Washington, 1997) AEMS(Ross & Chaib-draa, 2007). algorithms maintain lower upper boundsvalue node tree (using Equations 19 - 22) differspecific heuristic used choose next fringe node expand AND/OR tree.678fiOnline Planning Algorithms POMDPsfirst present common subroutines algorithms, discuss differentheuristics.Recalling general framework Algorithm 3.1, three steps interleaved severaltimes heuristic search algorithms. First, best fringe node expand (accordingheuristic) current search tree found. tree expandednode (usually one level). Finally, ancestor nodes values updated;values must updated choose next node expand, since heuristicvalue usually depends them. general, heuristic search algorithms slightlycomputationally expensive standard depth- breadth-first search algorithms, dueextra computations needed select best fringe node expand, needupdate ancestors iteration. required previous methods usingBranch-and-Bound pruning and/or Monte Carlo sampling. complexity extrasteps high, benefit expanding relevant nodes mightoutweighed lower number nodes expanded (assuming fixed planning time).heuristic search algorithms, particular heuristic value associated every fringenode tree. value indicate important expand nodeorder improve current solution. iteration algorithm, goal findfringe node maximizes heuristic value among fringe nodes.achieved efficiently storing node tree reference best fringe nodeexpand within subtree, well associated heuristic value. particular,root node always contains reference best fringe node whole tree.node expanded, ancestors nodes tree best fringe nodereference, corresponding heuristic value, need updated. updatedefficiently using references heuristic values stored lower nodes viadynamic programming algorithm, described formally Equations 23 24. HT (b)denotes highest heuristic value among fringe nodes subtree b, bT (b)reference fringe node, HT (b) basic heuristic value associated fringe node b,HT (b, a) HT (b, a, z) factors weigh basic heuristic value leveltree . example, HT (b, a, z) could Pr(z|b, a) order give higher weight(and hence favor) fringe nodes reached likely observations.HT (b)b F(T )HT (b) =maxaA HT (b, a)HT (b, a) otherwise(23)HT (b, a) = maxzZ HT (b, a, z)HT ( (b, a, z))bb F(T )bT (b) =bT (b, aTb ) otherwise))bT (b, a) = bT ( (b, a, zb,a(24)ab = argmaxaA HT (b, a)HT (b, a)zb,a= argmaxzZ HT (b, a, z)HT ( (b, a, z))procedure finds fringe node b F(T ) maximizes overall heuristic valueQdT (b)HT (bi , ai )HT (bi , ai , zi ), bi , ai zi represent ith belief,HT (bc , b) = HT (b) i=1action observation path bc b , dT (b) depth fringenode b. Note HT bT updated ancestor nodes last expandednode. reusing previously computed values nodes, procedure679fiRoss, Pineau, Paquet, & Chaib-draa1: Function Expand(b)Inputs: b:Static: bc :T:L:U:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:OR-Node want expand.current belief state agent.AND-OR tree representing current search tree.lower bound V .upper bound V .z Zb0 (b, a, z)UT (b0 ) U (b0 )LT (b0 ) L(b0 )HT (b0 ) HT (b0 )bT (b0 ) b0endPLT (b, a) RB (b, a) + P zZ Pr(z|b, a)LT ( (b, a, z))UT (b, a) RB (b, a) + zZ Pr(z|b, a)UT ( (b, a, z))argmaxzb,azZ HT (b, a, z)HT ( (b, a, z))))HT (b, a) = HT (b, a, zb,a )HT ( (b, a, zb,abT (b, a) bT ( (b, a, zb,a ))endLT (b) max{maxaA LT (b, a), LT (b)}UT (b) min{maxaA UT (b, a), UT (b)}b argmaxaA HT (b, a)HT (b, a)(b, ))HHT (b) HT (b,bbbT (b) bT (b,b )Algorithm 3.5: Expand : Expand subroutine heuristic search algorithms.find best fringe node expand tree time linear depthtree (versus exponential depth tree exhaustive search fringenodes). updates performed Expand UpdateAncestorssubroutines, described detail below. iteration,ChooseNextNodeToExpand subroutine simply returns reference best fringenode stored root tree, i.e. bT (bc ).Expand subroutine used heuristic search methods presented Algorithm 3.5.performs one-step lookahead fringe node b. main difference respectprevious methods Sections 3.2 3.3 heuristic value best fringe nodeexpand new nodes computed lines 7-8 12-14. best leaf nodebs subtree heuristic value computed according Equations 23 24(lines 18-20).UpdateAncestors function presented Algorithm 3.6. goal function update bounds ancestor nodes, find best fringe node expandnext. Starting given OR-Node b0 , function simply updates recursively ancestor nodes b0 bottom-up fashion, using Equations 19-22 update boundsEquations 23-24 update reference best fringe expand heuristic value.Notice UpdateAncestors function reuse information already storednode objects, need recompute (b, a, z), Pr(z|b, a) RB (b, a).However may need recompute HT (b, a, z) HT (b, a) according new bounds,depending heuristic defined.Due anytime nature heuristic search algorithms, search usually keepsgoing -optimal action found current belief bc , available planning680fiOnline Planning Algorithms POMDPs1: Function UpdateAncestors(b0 )2:3:4:5:6:7:8:9:10:11:12:13:14:15:Inputs: b0 : OR-Node want update ancestors.Static: bc : current belief state agent.: AND-OR tree representing current search tree.L: lower bound V .U : upper bound V .b0 6= bcSet (b, a) action aPin belief b parent node belief node b0LT (b, a) RB (b, a) + P zZ Pr(z|b, a)LT ( (b, a, z))UT (b, a) RB (b, a) + zZ Pr(z|b, a)UT ( (b, a, z))argmaxzb,azZ HT (b, a, z)HT ( (b, a, z)))H ( (b, a, z ))HT (b, a) HT (b, a, zb,ab,a))bT (b, a) bT ( (b, a, zb,aLT (b) maxa0 LT (b, a0 )UT (b) maxa0 UT (b, a0 )00b argmaxa0 HT (b, )HT (b, )(b, ))HHT (b) HT (b,bbbT (b) bT (b,b )b0 bendAlgorithm 3.6: UpdateAncestors : Updates bounds ancestors ancestorsOR-Nodetime elapsed. -optimal action found whenever UT (bc ) LT (bc ) LT (bc )UT (bc , a0 ), a0 6= argmaxaA LT (bc , a) (i.e. actions pruned, caseoptimal action found).covered basic subroutines, present different heuristicsproposed Satia Lave (1973), Washington (1997) Ross Chaib-draa (2007).begin introducing useful notation.Given graph structure G, let us denote F(G) set fringe nodes GHG (b, b0 ) set sequences actions observations lead belief node b0belief node b search graph G. tree , HT (b, b0 ) contain00single sequence denote hb,b. given sequence h HG (b, b ), definePr(hz |b, ha ) probability observe whole sequence observations hz h, givenstart belief node b perform whole sequence actions ha h. Finally,define Pr(h|b, ) probability follow entire action/observation sequenceh start belief b behave according policy . Formally, probabilitiescomputed follows:d(h)Pr(hz |b, ha ) =Pr(hiz |bhi1 , hia ),(25)i=1d(h)Pr(h|b, ) =Pr(hiz |bhi1 , hia )(bhi1 , hia ),(26)i=1d(h) represents depth h (number actions sequence h), hia denotesith action sequence h, hiz ith observation sequence h, bhi belief stateobtained taking first actions observations sequence h b. Notebh0 = b.681fiRoss, Pineau, Paquet, & Chaib-draa3.4.1 Satia Laveapproach Satia Lave (1973) follows heuristic search framework presentedabove. main feature approach explore, iteration, fringe node bcurrent search tree maximizes following term:bc ,bHT (bc , b) = d(hT)c ,bc ,bPr(hbT,z|bc , hbT,a)(UT (b) LT (b)),(27)b F(T ) bc root node . intuition behind heuristic simple:recalling definition V , note weight value V (b) fringe node bbc ,bc ,bc ,bc ,bsequence optimal), provided hbT,a|bc , hbT,aV (bc ) would exactly d(hT ) Pr(hbT,zactions. fringe nodes weight high effect estimateV (bc ). Hence one try minimize error nodes first. termUT (b) LT (b) included since upper bound (unknown) error V (b) LT (b).Thus heuristic focuses search areas tree affect value V (bc )error possibly large. approach also uses Branch-and-Bound pruning,fringe node reached action dominated parent belief bnever going expanded. Using notation Algorithms 3.5 3.6,heuristic implemented defining HT (b), HT (b, a) HT (b, a, z), follows:HT (b) =UT (b) LT (b),1 UT (b, a) > LT (b),HT (b, a) =0 otherwise,HT (b, a, z) = Pr(z|b, a),(28)condition UT (b, a) > LT (b) ensures global heuristic value HT (bc , b0 ) = 0bc ,b0dominated (pruned). guarantees fringeaction sequence hT,anodes never expanded.Satia Laves heuristic focuses search towards beliefs likelyreached future, error large. heuristic likely efficientdomains large number observations, probability distributionobservations concentrated observations. term UT (b) LT (b)heuristic also prevents search unnecessary computations areas treealready good estimate value function. term efficientbounds computed offline, U L, sufficiently informative. Similarly, node pruninggoing efficient U L sufficiently tight, otherwise actionspruned.3.4.2 BI-POMDPWashington (1997) proposed slightly different approach inspired AO algorithm(Nilsson, 1980), search conducted best solution graph. caseonline POMDPs, corresponds subtree belief nodes reachedsequences actions maximizing upper bound parent beliefs.bset fringe nodes best solution graph G, denote F(G),bdefined formally F(G) = {b F(G)|h HG (bc , b), Pr(h|b, G ) > 0}, G (b, a) = 1= argmaxa0 UG (b, a0 ) G (b, a) = 0 otherwise. AO algorithm simply specifies682fiOnline Planning Algorithms POMDPsexpanding fringe nodes. Washington (1997) recommends exploring fringenode Fb(G) (where G current acyclic search graph) maximizes UG (b) LG (b).Washingtons heuristic implemented defining HT (b), HT (b, a) HT (b, a, z),follows:HT (b) =UT (b) LT (b),1 = argmaxa0 UT (b, a0 ),HT (b, a) =0 otherwise,HT (b, a, z) = 1.(29)heuristic tries guide search towards nodes reachable promisingactions, especially loose bounds values (possibly large error). Onenice property approach expanding fringe nodes best solution graphway reduce upper bound root node bc . caseSatia Laves heuristic. However, Washingtons heuristic take accountprobability Pr(hz |b, ha ), discount factor d(h) , may end exploringnodes small probability reached future, thuslittle effect value V (bc ). Hence, may explore relevant nodesoptimizing decision bc . heuristic appropriate upper bound Ucomputed offline sufficiently informative, actions highest upper boundwould also usually tend highest Q-value. cases, algorithm focussearch actions thus find optimal action quicklyexplored actions equally. hand, consider observationprobabilities, approach may scale well large observation sets,able focus search towards relevant observations.3.4.3 AEMSRoss Chaib-draa (2007) introduced heuristic combines advantages BIPOMDP, Satia Laves heuristic. based theoretical error analysis treesearch POMDPs, presented Ross et al. (2008).core idea expand tree reduce error V (bc ) quicklypossible. achieved expanding fringe node b contributeserror V (bc ). exact error contribution eT (bc , b) fringe node b bc treedefined following equation:bc ,beT (bc , b) = d(hT)Pr(hbTc ,b |bc , )(V (b) LT (b)).(30)expression requires V computed exactly. practice, Ross Chaibdraa (2007) suggest approximating exact error (V (b) LT (b)) (UT (b) LT (b)),done Satia Lave, Washington. also suggest approximatingpolicy , (b, a) represents probability action optimalparent belief b, given lower upper bounds tree . particular, Ross et al. (2008)considered two possible approximations . first one based uniformityassumption distribution Q-values lower upper bounds,yields:683fiRoss, Pineau, Paquet, & Chaib-draa(b, a) =(2(b,a)LT (b))(UUT (b,a)LT (b,a)0UT (b, a) > LT (b),otherwise,(31)normalization constant sum probabilities (b, a)actions equals 1.second inspired AO BI-POMDP, assumes action maximizingupper bound fact optimal action:1 = argmaxa0 UT (b, a0 ),(32)(b, a) =0 otherwise.Given approximation , AEMS heuristic explore fringe node bmaximizes:bc ,b)HT (bc , b) = d(hTPr(hbTc ,b |bc , )(UT (b) LT (b)).(33)implemented defining HT (b), HT (b, a) HT (b, a, z) follows:HT (b) = UT (b) LT (b),HT (b, a) = (b, a),HT (b, a, z) = Pr(z|b, a).(34)refer heuristic AEMS1 defined Equation 31, AEMS2defined Equation 32.2Let us examine AEMS combines advantages Satia Lave,BI-POMDP heuristics. First, AEMS encourages exploration nodes loose boundspossibly large error considering term UT (b) LT (b) previous heuristics.Moreover, Satia Lave, focuses exploration towards belief states likelyencountered future. good two reasons. mentioned before, beliefstate low probability occurrence future, limited effect valueV (bc ) thus necessary know value precisely. Second, exploring highlyprobable belief states increases chance able reuse computationsfuture. Hence, AEMS able deal efficiently large observation sets,assuming distribution observations concentrated observations. Finally,BI-POMDP, AEMS favors exploration fringe nodes reachable actionsseem likely optimal (according ). useful handle large actionsets, focuses search actions look promising. promising actionsoptimal, quickly become apparent. work well bestactions highest probabilities . Furthermore, possible defineautomatically prunes dominated actions ensuring (b, a) = 0 wheneverUT (b, a) < LT (b). cases, heuristic never choose expand fringe nodereached dominated action.final note, Ross et al. (2008) determined sufficient conditionssearch algorithm using heuristic guaranteed find -optimal action within finitetime. stated Theorem 3.2.2. AEMS2 heuristic also used policy search algorithm Hansen (1998).684fiOnline Planning Algorithms POMDPsTheorem 3.2. (Ross et al., 2008) Let > 0 bc current belief. treeparent belief b UT (b) LT (b) > , (b, a) > 0 = argmaxa0 UT (b, a0 ),AEMS algorithm guaranteed find -optimal action bc within finite time.observe theorem possible define many different policiesAEMS heuristic guaranteed converge. AEMS1 AEMS2satisfy condition.3.4.4 HSVIheuristic similar AEMS2 also used Smith Simmons (2004) offlinevalue iteration algorithm HSVI way pick next belief point perform-vector backups. main difference HSVI proceeds via greedy searchdescends tree root node b0 , going towards action maximizesupper bound observation maximizes Pr(z|b, a)(U ( (b, a, z))L( (b, a, z)))level, reaches belief b depth (U (b) L(b)) < .heuristic could used online heuristic search algorithm instead stoppinggreedy search process reaches fringe node tree selecting nodeone expanded next. setting, HSVIs heuristic would return greedyapproximation AEMS2 heuristic, may find fringe node actuallybc ,bmaximizes d(hT ) Pr(hbTc ,b |bc , )(UT (b) LT (b)). consider online versionHSVI heuristic empirical study (Section 4). refer extension HSVI-BFS.Note complexity greedy search finding best fringe nodevia dynamic programming process updates HT bT UpdateAncestorssubroutine.3.5 Alternatives Tree Searchpresent two alternative online approaches proceed via lookaheadsearch belief MDP. online approaches presented far, one problemlearning achieved time, i.e. everytime agent encounters belief,recompute policy starting initial upper lower bounds computed offline.two online approaches presented next address problem presenting alternativeways updating initial value functions computed offline performanceagent improves time stores updated values computed time step.However, argued discussion (Section 5.2), techniques leaddisadvantages terms memory consumption and/or time complexity.3.5.1 RTDP-BELalternative approach searching AND-OR graphs RTDP algorithm (Bartoet al., 1995) adapted solve POMDPs Geffner Bonet (1998).algorithm, called RTDP-BEL, learns approximate values belief states visitedsuccessive trials environment. belief state visited, agent evaluatespossible actions estimating expected reward taking action current belief685fiRoss, Pineau, Paquet, & Chaib-draa1: Function OnlinePOMDPSolver()Static: bc : current belief state agent.V0 : Initial approximate value function (computed offline).V : hashtable beliefs approximate value.k: Discretization resolution.2: Initialize bc initial belief state V empty hashtable.3: ExecutionTerminated()P4: A: Evaluate Q(bc , a) = RB (b, a) + zZ Pr(z|b, a)V (Discretize( (b, a, z), k))5: argmaxaA Q(bc , a)6: Execute best action bc7: V (Discretize(bc , k)) Q(bc , a)8: Perceive new observation z9: bc (bc , a, z)10: endAlgorithm 3.7: RTDP-Bel Algorithm.state b approximate Q-value equation:XQ(b, a) = RB (b, a) +Pr(z|b, a)V ( (b, a, z)),(35)zZV (b) value learned belief b.belief state b value table, initialized heuristic value.authors suggest using MDP approximation initial value belief state.agent executes action returned greatest Q(b, a) value. Afterwards,value V (b) table updated Q(b, a) value best action. Finally,agent executes chosen action makes new observation, ending newbelief state. process repeated new belief.RTDP-BEL algorithm learns heuristic value belief state visited.maintain estimated value belief state memory, needs discretizebelief state space finite number belief states. also allows generalizationvalue function unseen belief states. However, might difficult find bestdiscretization given problem. practice, algorithm needs substantial amountsmemory (greater 1GB cases) store learned belief state values,especially POMDPs large state spaces. implementation RTDP-Belalgorithm presented Algorithm 3.7.function Discretize(b, k) returns discretized belief b0 b0 (s) = round(kb(s))/kstates S, V (b) looks value belief b hashtable. b presenthashtable, value V0 (b) returned V . Supported experimental data, GeffnerBonet (1998) suggest choosing k [10, 100], usually produces best results.Notice discretization resolution k O((k + 1)|S| ) possible discretizedbeliefs. implies memory storage required maintain V exponential |S|,becomes quickly intractable, even mid-size problems. Furthermore, learning goodestimates exponentially large number beliefs usually requires large numbertrials, might infeasible practice. technique sometimes appliedlarge domains factorized representation available. cases, beliefmaintained set distributions (one subset conditionaly independent statevariables) discretization applied seperately distribution. greatlyreduce possible number discretized beliefs.686fiOnline Planning Algorithms POMDPsAlgorithmRTBSSMcAllesterRolloutSatia LaveWashingtonAEMSHSVI-BFSRTDP-BelSOVI-optimalyeshigh probabilityyesacyclic graphyesyesyesAnytimeyesyesyesyesyesBranch &BoundyesyesimplicitimplicitimplicitMonteCarloyesyesHeuristicyesyesyesyesLearningyesyesTable 1: Properties various online methods.3.5.2 SOVIrecent online approach, called SOVI (Shani et al., 2005), extends HSVI (Smith &Simmons, 2004, 2005) online value iteration algorithm. approach maintainspriority queue belief states encountered execution proceeds-vector updates current belief state k belief states highest prioritytime step. priority belief state computed according much valuefunction changed successor belief states, since last time updated. authorsalso propose improvements HSVI algorithm improve scalability,efficient -vector pruning technique, avoiding use linear programs updateevaluate upper bound. main drawback approach hardly applicablelarge environments short real-time constraints, since needs perform valueiteration update -vectors online, high complexity number-vectors representing value function increases (i.e. O(k|S||A||Z|(|S| + |t1 |))compute ).3.6 Summary Online POMDP Algorithmssummary, see online POMDP approaches based lookahead search.improve scalability, different techniques used: branch-and-bound pruning, searchheuristics, Monte Carlo sampling. techniques reduce complexity different angles. Branch-and-bound pruning lowers complexity related action spacesize. Monte Carlo sampling used lower complexity related observation space size, could also potentially used reduce complexity relatedaction space size (by sampling subset actions). Search heuristics lower complexityrelated actions observations orienting search towards relevant actions observations. appropriate, factored POMDP representations usedreduce complexity related state. summary different propertiesonline algorithm presented Table 1.687fiRoss, Pineau, Paquet, & Chaib-draa4. Empirical Studysection, compare several online approaches two domains found POMDPliterature: Tag (Pineau et al., 2003) RockSample (Smith & Simmons, 2004). consider modified version RockSample, called FieldVisionRockSample (Ross & Chaib-draa,2007), higher observation space original RockSample. environmentintroduced means test compare different algorithms environmentslarge observation spaces.4.1 Methodologyenvironment, first compare real-time performance different heuristicspresented Section 3.4 limiting planning time 1 second per action. heuristicsgiven lower upper bounds results would comparable.objective evaluate search heuristic efficient different typesenvironments. end, implemented different search heuristics (SatiaLave, BI-POMDP, HSVI-BFS AEMS) best-first search algorithm,directly measure efficiency heuristic itself. Results also obtaineddifferent lower bounds (Blind PBVI) verify choice affects heuristicsefficiency. Finally, compare online offline times affect performanceapproach. Except stated otherwise, experiments run Intel Xeon 2.4Ghz 4GB RAM; processes limited 1GB RAM.4.1.1 Metrics compare online approachescompare performance first foremost terms average discounted return execution time. However, really seek online approaches guarantee bettersolution quality provided original bounds. words, seekreduce error original bounds much possible. suggests goodmetric efficiency online algorithms compare improvement termserror bounds current belief online search. Hence, defineerror bound reduction percentage be:UT (b) LT (b),(36)U (b) L(b)UT (b), LT (b), U (b) L(b) defined Section 3.2. best online algorithmprovide highest error bound reduction percentage, given initial boundsreal-time constraint.EBR metric necessarily reflect true error reduction, also compare return guarantees provided algorithm, i.e. lower bounds expectedreturn provided computed policies current belief. improvementlower bound compared initial lower bound computed offline direct indicatortrue error reduction, best online algorithm provide greatest lower boundimprovement current belief, given initial bounds real-time constraint.Formally, define lower bound improvement be:EBR(b) = 1LBI(b) = LT (b) L(b).688(37)fiOnline Planning Algorithms POMDPsexperiments, EBR LBI metrics evaluated time stepcurrent belief. interested seeing approach provides highest EBRLBI average.also consider metrics pertaining complexity efficiency. particular,report average number belief nodes maintained search tree. Methodslower complexity generally able maintain bigger trees, resultsshow always relate higher error bound reduction returns.also measure efficiency reusing part search tree recording percentagebelief nodes reused one time step next.4.2 TagTag initially introduced Pineau et al. (2003). environment alsoused recently work several authors (Poupart & Boutilier, 2003; Vlassis &Spaan, 2004; Pineau, 2004; Spaan & Vlassis, 2004; Smith & Simmons, 2004; Braziunas &Boutilier, 2004; Spaan & Vlassis, 2005; Smith & Simmons, 2005). environment,approximate POMDP algorithm necessary large size (870 states, 5 actions30 observations). Tag environment consists agent catch (Tag)another agent moving 29-cell grid domain. reader referred workPineau et al. (2003) full description domain. Note results presentedbelow, belief state represented factored form. domain exactfactorization possible.obtain results Tag, run algorithm starting configuration 5 times,( i.e. 5 runs 841 different starting joint positions, excluding 29 terminalstates ). initial belief state runs consists uniform distributionpossible joint agent positions.Table 2 compares different heuristics presenting 95% confidence intervalsaverage discounted return per run (Return), average error bound reduction percentage pertime step (EBR), average lower bound improvement per time step (LBI), average beliefnodes search tree per time step (Belief Nodes), average percentage belief nodesreused per time step (Nodes Reused), average online planning time used per time step(Online Time). cases, use FIB upper bound Blind lower bound. Noteaverage online time slightly lower 1 second per step algorithmssometimes find -optimal solutions less second.observe efficiency HSVI-BFS, BI-POMDP AEMS2 differs slightlyenvironment outperform three heuristics: RTBSS, SatiaLave, AEMS1. difference explained fact latter threemethods restrict search best solution graph. consequence,explore many irrelevant nodes, shown lower error bound reduction percentage,lower bound improvement, nodes reused. poor reuse percentage explainsSatia Lave, AEMS1 limited lower number belief nodes searchtree, compared methods reached averages around 70K. resultsthree heuristics differ much three heuristics differway choose observations explore search. Since two observationspossible first action observation, one observations leads directly689fiRoss, Pineau, Paquet, & Chaib-draaHeuristicRTBSS(5)Satia LaveAEMS1HSVI-BFSBI-POMDPAEMS2Return-10.31 0.22-8.35 0.18-6.73 0.15-6.22 0.19-6.22 0.15-6.19 0.15EBR (%)22.3 0.422.9 0.249.0 0.375.7 0.476.2 0.576.3 0.5LBI3.03 0.072.47 0.043.92 0.037.69 0.067.81 0.067.81 0.06BeliefNodes45066 70136908 20943693 31464870 94779508 100080250 1018NodesReused (%)010.0 0.225.1 0.354.1 0.754.6 0.654.8 0.6OnlineTime (ms)580 9856 4814 4673 5622 4623 4Table 2: Comparison different search heuristics Tag environment using Blindpolicy lower bound.EXITFigure 3: RockSample[7,8].terminal belief state, possibility heuristics differed significantlylimited. Due limitation Tag domain, compare online algorithmslarger complex domain: RockSample.4.3 RockSampleRockSample problem originally presented Smith Simmons (2004).domain, agent explore environment sample rocks (see Figure 3),similarly real robot would planet Mars. agent receives rewardssampling rocks leaving environment (at extreme right environment).rock scientific value not, agent sample good rocks.define RockSample[n, k] instance RockSample problem n ngrid k rocks. state characterized k + 1 variables: XP , defines positionrobot take values {(1, 1), (1, 2), . . . , (n, n)} k variables, X1R XkR ,representing rock, take values {Good, Bad}.agent perform k + 5 actions: {N orth, South, East, W est, Sample, Check1 , . . . ,Checkk }. four motion actions deterministic. Sample action samplesrock agents current location. Checki action returns noisy observation{Good, Bad} rock i.belief state represented factored form known position set kprobabilities, namely probability rock good. Since observation rock690fiOnline Planning Algorithms POMDPsHeuristicSatia LaveAEMS1RTBSS(2)BI-POMDPHSVI-BFSAEMS2AEMS1Satia LaveRTBSS(2)BI-POMDPAEMS2HSVI-BFSBeliefNodesEBR (%)LBINodesReused (%)Blind: Return:7.35, || = 1, Time:4s7.35 03.64 000509 08.92 010.30 0.089.50 0.110.90 0.03579 25.31 0.0310.30 0.159.65 0.021.00 0.04439 00018.43 0.1433.3 0.54.33 0.062152 7129.9 0.620.53 0.3151.7 0.75.25 0.072582 7236.5 0.520.75 0.1552.4 0.65.30 0.063145 10136.4 0.5PBVI: Return:5.93, |B| = 64, || = 54, Time:2418s17.10 0.2826.1 0.41.39 0.031461 2812.2 0.119.09 0.2116.9 0.11.17 0.012311 2513.5 0.119.45 0.3022.4 0.31.37 0.04426 10021.36 0.2249.5 0.22.73 0.022781 3832.2 0.221.37 0.2257.7 0.23.08 0.022910 4638.2 0.221.46 0.2256.3 0.23.03 0.022184 3337.3 0.2ReturnOnlineTime (ms)900916886953885859012256954965540892826826217232Table 3: Comparison different search heuristics RockSample[7,8] environment, usingBlind policy PBVI lower bound.state independent rock states (it depends known robot position),complexity computing Pr(z|b, a) (b, a, z) greatly reduced. Effectively,computation Pr(z|b, Checki ) reduces to: Pr(z|b, Checki ) = Pr(Accurate|XP , Checki )Pr(XiR = z) + (1 Pr(Accurate|XP , Checki )) (1 Pr(XiR = z)). probability1+(Xp ,i), (Xp , i) =sensor accurate rock i, Pr(Accurate|XP , Checki ) =2d(Xp ,i)/d02, d(Xp , i) euclidean distance position Xp position rock i,d0 constant specifying half efficiency distance. Pr(XiR = z) obtained directlyprobability (stored b) rock good. Similarly, (b, a, z) computedquite easily move actions deterministically affect variable XP , Checki actionchanges probability associated XiR according sensors accuracy.obtain results RockSample, run algorithm starting rock configuration 20 times (i.e. 20 runs 2k different joint rock states). initialbelief state runs consists 0.5 rock good, plusknown initial robot position.4.3.1 Real-Time Performance Online SearchTable 3, present 95% confidence intervals mean metrics interest,RockSample[7,8] (12545 states, 13 actions, 2 observations), real-time contraints 1second per action. compare performance using two different lower bounds, Blindpolicy PBVI, use QMDP upper bound cases. performancepolicy defined lower bound shown comparison header. RTBSS,notation RTBSS(k) indicates k-step lookahead; use depth k yields averageonline time closest 1 second per action.Return terms return, first observe AEMS2 HSVI-BFS heuristicsobtain similar results. obtains highest return slight marginone lower bounds. BI-POMDP obtains similar return combined691fiRoss, Pineau, Paquet, & Chaib-draaPBVI lower bound, performs much worse Blind lower bound. twoheuristics, Satia Lave, AEMS1, perform considerably worse terms returneither lower bound.EBR LBI terms error bound reduction lower bound improvement, AEMS2obtains best results lower bounds. HSVI-BFS close second. indicates AEMS2 effectively reduce true error heuristics,therefore, guarantees better performance. BI-POMDP tends less efficientAEMS2 HSVI-BFS, significantly better RTBSS, Satia Lave,AEMS1, slightly improve bounds case. Satia Lave unableincrease Blind lower bound, explains obtains returnBlind policy. also observe higher error bound reduction lower boundimprovement, higher average discounted return usually is. confirms intuition guiding search minimize error current belief bc goodstrategy obtain better return.Nodes Reused terms percentage nodes reused, AEMS2 HVSI-BFSgenerally obtain best scores. allows algorithms maintain higher numbernodes trees, could also partly explain outperformheuristics terms return, error bound reduction lower bound improvement. NoteRTBSS reuse node tree algorithm storetree memory. consequence, reuse percentage always 0.Online Time Finally, also observe AEMS2 requires less average online time peraction algorithms attain performance. general, lower averageonline time means heuristic efficient finding -optimal actions small amounttime. running time RTBSS determined chosen depth, cannot stopcompleting full lookahead search.Summary Overall, see AEMS2 HSVI-BFS obtain similar results. HoweverAEMS2 seems slightly better HSVI-BFS, provides better performance guarantees(lower error) within shorter period time. difference significant.may due small number observations environment, case twoheuristics expand tree similar ways. next section, explore domainmany observations evaluate impact factor.lower performances three heuristics explained various reasons.case BI-POMDP, due fact take accountobservation probabilities Pr(z|b, a) discount factor heuristic value. Hencetend expand fringe nodes affect significantly value currentbelief. Satia Lave, poor performance case Blind policyexplained fact fringe nodes maximize heuristic always leavesreached sequence Move actions. Due deterministic nature Move actions(Pr(z|b, a) = 1 actions, whereas Check actions Pr(z|b, a) = 0.5 initially),heuristic value fringe nodes reached Move actions much higher errorreduced significantly. result, algorithm never explores nodes Checkactions, robot always follows Blind policy (moving east, never checkingsampling rocks). demonstrates importance restricting choice692fiOnline Planning Algorithms POMDPs3025V(b0)2015AEMS2AEMS1BIPOMDPHSVIBFSSatia105 210110011010210310Time (s)Figure 4: Evolution upper lower bounds RockSample[7,8].leaves explore reached sequence actions maximizing upper bound,done AEMS2, HSVI-BFS BI-POMDP. case AEMS1, probably behavesless efficiently term uses estimate probability certain actionoptimal good approximation environment. Moreover, AEMS1restrict exploration best solution graph, probably also suffers, part,problems Satia Lave heuristic. RTBSS also performwell Blind lower bound. due short depth allowed searchtree, required running time 1 second/action. confirmssignificantly better exhaustive search good heuristics guide search.4.3.2 Long-Term Error Reduction Online Heuristic Searchcompare long term performance different heuristics, let algorithms runoffline mode initial belief state environment, log changes lowerupper bound values initial belief state 1000 seconds. Here, initial lowerupper bounds provided Blind policy QMDP respectively. seeFigure 4 Satia Lave, AEMS1 BI-POMDP efficient HSVI-BFSAEMS2 reducing error bounds. One interesting thing noteupper bound tends decrease slowly continuously, whereas lower bound oftenincreases stepwise manner. believe due fact upper boundmuch tighter lower bound. also observe error bound reductionhappens first seconds search. confirms nodes expanded earliertree much impact error bc expanded fartree (e.g. hundreds seconds). important result support using online(as opposed offline) methods.693fi22222020Average Discounted ReturnAverage Discounted ReturnRoss, Pineau, Paquet, & Chaib-draa1816AEMS2HSVIBFSBIPOMDP14121086 1100101816121086 110110AEMS2 & BlindAEMS2 & PBVI(8)AEMS2 & PBVI(16)14Online Time (s)010110Online Time (s)Figure 5: Comparison return Figure 6: Comparison returnfunction online timefunction online timeRockSample(10,10) differentRockSample(10,10) differentonline methods.offline lower bounds.4.3.3 Influence Offline Online Timecompare performance online approaches influenced availableonline offline times. allows us verify particular method betteravailable online time shorter (or longer), whether increasing offline time couldbeneficial.consider three approaches shown best overall performance far (BIPOMDP, HSVI-BFS AEMS2) compare average discounted return function online time constraint per action. Experiments run RockSample[10,10](102,401 states, 15 actions, 2 observations) following online time constraints:0.1s, 0.2s, 0.5s, 1s, 2s, 5s 10s. vary offline time, used 3 different lowerbounds: Blind policy, PBVI 8 belief points, PBVI 16 belief points, takingrespectively 15s, 82s, 193s. upper bound used QMDP cases. resultsobtained Intel Xeon 3.0 Ghz processor.Figure 5, observe AEMS2 fares significantly better HSVI-BFSBI-POMDP short time constraints. time constraint increases, AEMS2HSVI-BFS performs similarly (no significant statistical difference). also noticeperformance BI-POMDP stops improving 1 second planning time.explained fact take account observation probabilitiesPr(z|b, a), discount factor. search tree grows bigger, fringenodes small probability reached future, becomesimportant take probabilities account order improve performance.Otherwise, observe case BI-POMDP, expanded nodes affectquality solution found.Figure 6, observe increasing offline time beneficial effect mostlyshort real-time constraints. online planning time available,694fiOnline Planning Algorithms POMDPsdifference performances AEMS2 Blind lower bound, AEMS2PBVI becomes insignificant. However, online time constraints smaller onesecond, difference performance large. Intuitively, short real-timeconstraints algorithm enough time expand lot nodes,policy found relies much bounds computed offline. hand,longer time constraints, algorithm enough time significantly improve boundscomputed offline, thus policy found rely much offline bounds.4.4 FieldVisionRockSampleseems results presented thus far HSVI-BFS AEMS2 comparableperformance standard domains. note however environmentssmall observation sets (assuming observations zero probability removed).believe AEMS2 especially well suited domains large observation spaces. However,standard problems literature. therefore consider modifiedversion RockSample environment, called FieldVisionRockSample (Ross & Chaib-draa,2007), observation space size exponential number rocks.FieldVisionRockSample (FVRS) problem differs RockSample problemway robot able perceive rocks environment. RecallRockSample, agent Check action specific rock observe statenoisy sensor. FVRS, robot observes state rocks,noisy sensor, action conducted environment. Consequently,eliminates use Check actions, remaining actions robot includefour move actions {North, East, South, West} Sample action. robotperceive rock either Good Bad, thus observation space size 2kinstance problem k rocks. RockSample, efficiency sensordefined parameter = 2d/d0 , distance rock d0half efficiency distance. assume sensors observations independent rock.FVRS, partial observability environment directly proportionalparameter d0 : d0 increases, sensor becomes accurate uncertaintystate environment decreases. value d0 defined different instancesRockSample work Smith Simmons (2004) high FVRS problem(especially bigger instances RockSample), making almost completely observable.Consequently, re-define value d0 different instances FieldVisionRockSample according size grid (n). considering factp n n grid,largest possible distance rock robot (n 1) (2), seems reasonable distance, probability observing real state rockclose 50%p problem remain partially observable. Consequently, defined0 = (n 1) (2)/4.obtain results FVRS domain, run algorithm starting rockconfigurations 20 times (i.e. 20 runs 2k different joint rock states).initial belief state runs corresponds probability 0.5rock good, well known initial position robot.695fiRoss, Pineau, Paquet, & Chaib-draaHeuristicRTBSS(2)AEMS1Satia LaveHSVI-BFSAEMS2BI-POMDPRTBSS(1)BI-POMDPSatia LaveAEMS1AEMS2HSVI-BFSBeliefNodesReturnEBR (%)LBINodesReused (%)FVRS[5,5] [Blind: Return:8.15, || = 1, Time=170ms]16.54 0.3718.4 1.12.80 0.1918499 1020016.88 0.3617.1 1.12.35 0.168053 1231.19 0.0718.68 0.3915.9 1.22.17 0.167965 1180.88 0.0620.27 0.4423.8 1.42.64 0.144494 1054.50 0.8021.18 0.4531.5 1.53.11 0.1512301 4403.93 0.2222.75 0.4731.1 1.23.30 0.1712199 4272.26 0.44FVRS[5,7] [Blind: Return:8.15, || = 1, Time=761ms]20.57 0.237.72 0.132.07 0.11516 10022.75 0.2511.1 0.42.08 0.074457 610.37 0.1122.79 0.2511.1 0.42.05 0.083683 520.36 0.0723.31 0.2512.4 0.42.24 0.083856 551.36 0.1323.39 0.2513.3 0.42.35 0.084070 581.64 0.1423.40 0.2513.0 0.42.30 0.083573 521.69 0.27OnlineTime (ms)3135 27876 5878 4857 12854 13782 12254923947942944946123323Table 4: Comparison different search heuristics different instances FieldVisionRockSample environment.4.4.1 Real-Time Performance Online SearchTable 4, present 95% confidence intervals mean metrics interest.consider two instances environment, FVRS[5,5] (801 states, 5 actions, 32 observations) FVRS[5,7] (3201 states, 5 actions, 128 observations). cases, useQMDP upper bound Blind lower bound, real-time constraints 1 second peraction.Return terms return, observe clear winner. BI-POMDP performs surpringly well FVRS[5,5] significantly worse AEMS2 HSVI-BFS FVRS[5,7].hand, AEMS2 significantly better HSVI-BFS FVRS[5,5]get similar performances FVRS[5,7]. Satia Lave performs better environment RockSample. likely due fact transitions beliefspace longer deterministic (as case Move actions RockSample).FVRS[5,5], also observe even RTBSS given 3 seconds per actionperform two-step lookahead, performance worse heuristic searchmethods. clearly shows expanding observations equally searchgood strategy, many observations negligible impact currentdecision.EBR LBI terms error bound reduction lower bound improvement, observe AEMS2 performs much better HSVI-BFS FVRS[5,5], significantlybetter FVRS[5,7]. hand, BI-POMDP obtains similar results AEMS2FVRS[5,5] significantly worse terms EBR LBI FVRS[5,7].suggests AEMS2 consistently effective reducing error, even environmentslarge branching factors.Nodes Reused percentage belief nodes reused much lower FVRS duemuch higher branching factor. observe HSVI-BFS best reuse percentage696fiOnline Planning Algorithms POMDPs263524302220161425AEMS2AEMS1BIPOMDPHSVIBFSSatiaV(b0)V(b0)1820AEMS2AEMS1BIPOMDPHSVIBFSSatia1512101086 2101100110102105 110310Time (s)010110Time (s)210310Figure 7: Evolution upper lower Figure 8: Evolution upper lowerbounds FieldVisionRockSambounds FieldVisionRockSample[5,5].ple[5,7].environments, however significantly higher AEMS2. methodsreuse significantly larger portion tree methods. confirmstwo methods able guide search towards likely beliefs.4.4.2 Long-Term Error Reduction Online Heuristic SearchOverall, Table 4 confirms consistent performance HSVI-BFS AEMS2,difference heuristics modest. Considering complexity environment,may due fact algorithms enough time expandsignificant number nodes within 1 second. long-term analysis bounds evolutionFigures 7 8 confirms this. observe figures lower bound convergesslightly rapidly AEMS2 heuristics. AEMS1 heuristic alsoperforms well long run problem, seems second best heuristic,Satia Lave far behind. hand, HSVI-BFS heuristic farworse problem RockSample. seems part due factheuristic takes time find next node expand others, thusexplores fewer belief states.5. Discussionprevious sections presented evaluated several online POMDP algorithms.discuss important issues arise applying online methods practice, summarizeadvantages disadvantages. help researchers decide whetheronline algorithms good approach solving given problem.697fiRoss, Pineau, Paquet, & Chaib-draa5.1 Lower Upper Bound SelectionOnline algorithms combined many valid lower upper bounds. However,properties bounds satisfy online search perform efficiently practice. One desired properties lower upperbound functionsproperty states b : L(b)P monotone. monotonemaxaA RB (b, a) + PzZ Pr(z|b, a)L( (b, a, z)) lower bound b : U (b)maxaA RB (b, a) + zZ Pr(z|b, a)U ( (b, a, z)) upper bound. propertyguarantees certain fringe node expanded, lower bound non-decreasingupper bound non-increasing. sufficient guarantee error boundUT (b) LT (b) b non-increasing expansion b, error boundgiven algorithm value root belief state bc , cannot worseerror bound defined initial bounds given. Note however monotonicitynecessary AEMS converge -optimal solution, shown previous work (Rosset al., 2008); boundedness sufficient.5.2 Improving Bounds Timementioned survey online algorithms, one drawback many online approaches store improvements made offline boundsonline search, that, belief state encountered again, computations need performed again, restarting offline bounds. trivial wayimprove maintain large hashtable (or database) belief statesimproved lower upper bounds previous search, associated newbounds. however many drawbacks this. First every time wantevaluate lower upper bound fringe belief, search hashtable needsperformed check better bounds available. may require significanttime hashtable large (e.g. millions beliefs). Furthermore, experiments conductedRTDP-Bel large domains, RockSample[7,8], shown processusually runs memory (i.e. requires 1 GB) good performanceachieved requires several thousands episodes performing well (Paquet, 2006).authors RTBSS also tried combining search algorithm RTDPBel preserve improvements made search (Paquet, 2006).combination usually performed better learned faster RTDP-Bel alone, founddomains, thousand episodes still required improvementseen (in terms return). Hence, point updates offline bounds tenduseful large domains task accomplish repeated large numbertimes.better strategy improve lower bound might save time perform-vector updates beliefs expanded search, offlinelower bound improves time. updates advantage improving lowerbound whole belief space, instead single belief state. Howevertime consuming, especially large domains. Hence, need act within shorttime constraints, approach infeasible. However several seconds planning timeavailable per action, might advantageous use time perform-vector updates, rather use available time search tree. good698fiOnline Planning Algorithms POMDPsidea would perform -vector updates subset beliefs search tree,lower bound improves.5.3 Factored POMDP Representationsefficiency online algorithms relies heavily ability quickly compute (b, a, z)Pr(z|b, a), must computed evey belief state search tree. Usingfactored POMDP representations effective way reduce time complexity computing quantities. Since environments large state spaces structureddescribed sets features, obtaining factored representation complex systemsissue cases. However, domains significant dependenciesstate features, may useful use algorithms proposed Boyen Koller(1998) Poupart (2005) find approximate factored representations featuresindependent, minimal degradation solution quality. upperlower bounds might hold anymore computed approximate factoredrepresentation, usually may still yield good results practice.5.4 Handling Graph Structurementioned before, general tree search algorithm used online algorithmsduplicate belief states whenever multiple paths leading posteriorbelief current belief bc . greatly simplifies complexity related updatingvalues ancestor nodes, also reduces complexity related findingbest fringe node expand (using technique Section 3.4 validtrees). disadvantage using tree structure inevitably, computationsredundant, algorithm potentially expand subtree everyduplicate belief. avoid this, could use LAO algorithm proposed HansenZilberstein (2001) extension AO handle generic graph structure, includingcyclic graphs. expansion, runs value (or policy) iteration algorithmconvergence among ancestor nodes order update values.heuristics surveyed Section 3.4 generalized guide best-first searchalgorithms handle graph structure, like LAO . first thing notice that,graph, fringe node reached multiple paths, error contributes multipletimes error value bc . error contribution perspective, heuristicvalue fringe node sum heuristic values paths reachingit. instance, case AEMS heuristic, using notation definedSection 3.4, global heuristic value given fringe node b, current belief statebc graph G, computed follows:HG (bc , b) = (U (b) L(b))Xd(h) Pr(h|bc , G ).(38)hHG (bc ,b)Notice cyclic graphs, infinitely many paths HG (bc , b).case, could use dynamic programming estimate heuristic value.solving HG (bc , b) fringe nodes b graph G require lot timepractice, especially many fringe nodes, experimentedmethod Section 4. However, would practical use heuristic could find699fiRoss, Pineau, Paquet, & Chaib-draaalternative way determine best fringe node without computing HG (bc , b) separatelyfringe node b performing exhaustive search fringe nodes.5.5 Online vs. Offline TimeOne important aspect determining efficiency applicability online algorithmsamount time available execution planning. course often taskdependent. real-time problems like robot navigation, amount time mayshort, e.g. 0.1 1 second per action. hand tasks like portfoliomanagement, acting every second necessary, several minutes could easilytaken plan stock buying/selling action. seen experiments,shorter available online planning time, greater importance goodoffline value function start with. case, often necessary reserve sufficienttime compute good offline policy. planning time available online,influence offline value function becomes negligible, rough offlinevalue function sufficient obtain good performance. best trade-off onlineoffline time often depends large problem is. branching factor(|A||Z|) large and/or computing successor belief states takes long time, onlinetime required achieve significant improvement offline value function.However, small problems, online time 0.1 second per action may sufficientperform near-optimally even rough offline value function.5.6 Advantages Disadvantages Online Algorithmsdiscuss advantages disadvantages online planning algorithms general.5.6.1 Advantagesonline algorithms combined offline solving algorithm, assumingprovides lower bound upper bound V , improve qualitypolicy found offline.Online algorithms require little offline computation executableenvironment, perform well even using loose bounds, quickcompute.Online methods exploit knowledge current belief focus computationrelevant future beliefs current decision, scale welllarge action observation spaces.Anytime online methods applicable real-time environments,stopped whenever planning time runs out, still provide best solution foundfar.5.6.2 Disadvantagesbranching factor depends number actions observations. Thusmany observations and/or actions, might impossible search deep700fiOnline Planning Algorithms POMDPsenough, provide significant improvement offline policy. cases, sampling methods designed reduce branching factor could useful.cannot guarantee lower upper bounds still valid samplingused, guarantee valid high probability, given enoughsamples drawn.online algorithms store improvements made offline policyonline search, algorithm plan bounds timeenvironment restarted. time available, could advantageous add-vector updates belief states explored tree, offline boundsimprove time.6. ConclusionPOMDPs provide rich elegant framework planning stochastic partially observable domains, however time complexity major issue preventingapplication complex real-world systems. paper thoroughly surveys various existing online algorithms key techniques approximations used solve POMDPsefficiently. empirically compare online approaches several POMDP domains different metrics: average discounted return, average error bound reductionaverage lower bound improvement, using different lower upper bounds: PBVI,Blind, FIB QMDP.empirical results, observe heuristic search methods, namelyAEMS2 HSVI-BFS, obtain good performances, even domains large branching factors large state spaces. two methods similar perform wellorient search towards nodes improve current approximatevalue function quickly possible; i.e. belief nodes largest errorlikely reached future promising actions. However, environmentslarge branching factors, may time expand nodes turn.Hence, would interesting develop approximations reduce branchingfactor cases.conclusion, believe online approaches important role playimproving scalability POMDP solution methods. good example succesfulapplications RTBSS algorithm RobocupRescue simulation Paquet et al.(2005). environment challenging state space orders magnitudebeyond scope current algorithms. Offline algorithms remain important obtaintight lower upper bounds value function. interesting question whetheronline offline approaches better, improve kinds approaches,synergy exploited solve complex real-world problems.Acknowledgmentsresearch supported Natural Sciences Engineering Council CanadaFonds Quebecois de la Recherche sur la Nature et les Technologies. would alsolike thank anonymous reviewers helpful comments suggestions.701fiRoss, Pineau, Paquet, & Chaib-draaReferencesAstrom, K. J. (1965). Optimal control Markov decision processes incomplete stateestimation. Journal Mathematical Analysis Applications, 10, 174205.Barto, A. G., Bradtke, S. J., & Singhe, S. P. (1995). Learning act using real-time dynamicprogramming. Artificial Intelligence, 72 (1), 81138.Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ,USA.Bertsekas, D. P., & Castanon, D. A. (1999). Rollout algorithms stochastic schedulingproblems. Journal Heuristics, 5 (1), 89108.Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.Proceedings Fourteenth Conference Uncertainty Artificial Intelligence(UAI-98), pp. 3342.Braziunas, D., & Boutilier, C. (2004). Stochastic local search POMDP controllers.Nineteenth National Conference Artificial Intelligence (AAAI-04), pp. 690696.Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple, fast,exact method partially observable Markov decision processes. ProceedingsThirteenth Conference Uncertainty Artificial Intelligence (UAI-97), pp. 5461.Chang, H. S., Givan, R., & Chong, E. K. P. (2004). Parallel rollout online solutionpartially observable Markov decision processes. Discrete Event Dynamic Systems,14 (3), 309341.Geffner, H., & Bonet, B. (1998). Solving large POMDPs using real time dynamic programming. Proceedings Fall AAAI symposium POMDPs, pp. 6168.Hansen, E. A. (1998). Solving POMDPs searching policy space. Fourteenth Conference Uncertainty Artificial Intelligence (UAI-98), pp. 211219.Hansen, E. A., & Zilberstein, S. (2001). LAO * : heuristic search algorithm findssolutions loops. Artificial Intelligence, 129 (1-2), 3562.Hauskrecht, M. (2000). Value-function approximations partially observable Markovdecision processes. Journal Artificial Intelligence Research, 13, 3394.Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning actingpartially observable stochastic domains. Artificial Intelligence, 101, 99134.Kearns, M. J., Mansour, Y., & Ng, A. Y. (1999). sparse sampling algorithm nearoptimal planning large markov decision processes. Proceedings SixteenthInternational Joint Conference Artificial Intelligence (IJCAI-99), pp. 13241331.Koenig, S. (2001). Agent-centered search. AI Magazine, 22 (4), 109131.Littman, M. L. (1996). Algorithms sequential decision making. Ph.D. thesis, BrownUniversity.Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995). Learning policies partially observable environments: scaling up. Proceedings 12th InternationalConference Machine Learning (ICML-95), pp. 362370.702fiOnline Planning Algorithms POMDPsLovejoy, W. S. (1991). Computationally feasible bounds POMDPs. Operations Research,39 (1), 162175.Madani, O., Hanks, S., & Condon, A. (1999). undecidability probabilistic planninginfinite-horizon partially observable Markov decision problems. ProceedingsSixteenth National Conference Artificial Intelligence. (AAAI-99), pp. 541548.McAllester, D., & Singh, S. (1999). Approximate Planning Factored POMDPs using Belief State Simplification. Proceedings 15th Annual Conference UncertaintyArtificial Intelligence (UAI-99), pp. 409416.Monahan, G. E. (1982). survey partially observable Markov decision processes: theory,models algorithms. Management Science, 28 (1), 116.Nilsson, N. (1980). Principles Artificial Intelligence. Tioga Publishing.Papadimitriou, C., & Tsitsiklis, J. N. (1987). complexity Markov decision processes.Mathematics Operations Research, 12 (3), 441450.Paquet, S. (2006). Distributed Decision-Making Task Coordination Dynamic, Uncertain Real-Time Multiagent Environments. Ph.D. thesis, Laval University.Paquet, S., Chaib-draa, B., & Ross, S. (2006). Hybrid POMDP algorithms. ProceedingsWorkshop Multi-Agent Sequential Decision Making Uncertain Domains(MSDM-06), pp. 133147.Paquet, S., Tobin, L., & Chaib-draa, B. (2005). online POMDP algorithm complexmultiagent environments. Proceedings fourth International Joint ConferenceAutonomous Agents Multi Agent Systems (AAMAS-05), pp. 970977.Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: anytime algorithm POMDPs. Proceedings International Joint Conference ArtificialIntelligence (IJCAI-03), pp. 10251032.Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations largePOMDPs. Journal Artificial Intelligence Research, 27, 335380.Pineau, J. (2004). Tractable planning uncertainty: exploiting structure. Ph.D. thesis,Carnegie Mellon University.Poupart, P. (2005). Exploiting structure efficiently solve large scale partially observableMarkov decision processes. Ph.D. thesis, University Toronto.Poupart, P., & Boutilier, C. (2003). Bounded finite state controllers. Advances NeuralInformation Processing Systems 16 (NIPS).Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.Ross, S., & Chaib-draa, B. (2007). Aems: anytime online search algorithm approximate policy refinement large POMDPs. Proceedings 20th InternationalJoint Conference Artificial Intelligence (IJCAI-07), pp. 25922598.Ross, S., Pineau, J., & Chaib-draa, B. (2008). Theoretical analysis heuristic searchmethods online POMDPs. Advances Neural Information Processing Systems20 (NIPS).703fiRoss, Pineau, Paquet, & Chaib-draaSatia, J. K., & Lave, R. E. (1973). Markovian decision processes probabilistic observation states. Management Science, 20 (1), 113.Shani, G., Brafman, R., & Shimony, S. (2005). Adaptation changing stochastic environments online POMDP policy learning. Proceedings WorkshopReinforcement Learning Non-Stationary Environments, ECML 2005, pp. 6170.Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observableMarkov processes finite horizon. Operations Research, 21 (5), 10711088.Smith, T., & Simmons, R. (2004). Heuristic search value iteration POMDPs. Proceedings 20th Conference Uncertainty Artificial Intelligence (UAI-04), pp.520527.Smith, T., & Simmons, R. (2005). Point-based POMDP algorithms: improved analysisimplementation. Proceedings 21th Conference Uncertainty ArtificialIntelligence (UAI-05), pp. 542547.Sondik, E. J. (1971). optimal control partially observable Markov processes. Ph.D.thesis, Stanford University.Sondik, E. J. (1978). optimal control partially observable Markov processesinfinite horizon: Discounted costs. Operations Research, 26 (2), 282304.Spaan, M. T. J., & Vlassis, N. (2004). point-based POMDP algorithm robot planning.Proceedings IEEE International Conference Robotics Automation(ICRA-04), pp. 23992404.Spaan, M. T. J., & Vlassis, N. (2005). Perseus: randomized point-based value iterationPOMDPs. Journal Artificial Intelligence Research, 24, 195220.Vlassis, N., & Spaan, M. T. J. (2004). fast point-based algorithm POMDPs.Benelearn 2004: Proceedings Annual Machine Learning Conference BelgiumNetherlands, pp. 170176.Washington, R. (1997). BI-POMDP: bounded, incremental partially observable Markovmodel planning. Proceedings 4th European Conference Planning, pp.440451.Zhang, N. L., & Zhang, W. (2001). Speeding convergence value iteration partially observable Markov decision processes. Journal Artificial Intelligence Research,14, 2951.704fiJournal Artificial Intelligence Research 32 (2008) 169-202Submitted 10/07; published 05/08Communication-Based Decomposition MechanismsDecentralized MDPsClaudia V. Goldmanc.goldman@samsung.comSamsung Telecom Research IsraelYakum, IsraelShlomo Zilbersteinshlomo@cs.umass.eduDepartment Computer ScienceUniversity Massachusetts, Amherst, 01003 USAAbstractMulti-agent planning stochastic environments framed formally decentralized Markov decision problem. Many real-life distributed problems arise manufacturing, multi-robot coordination information gathering scenarios formalizedusing framework. However, finding optimal solution general case hard,limiting applicability recently developed algorithms. paper provides practical approach solving decentralized control problems communication amongdecision makers possible, costly. develop notion communication-basedmechanism allows us decompose decentralized MDP multiple single-agentproblems. framework, referred decentralized semi-Markov decision processdirect communication (Dec-SMDP-Com), agents operate separately communications. show finding optimal mechanism equivalent solving optimallyDec-SMDP-Com. also provide heuristic search algorithm converges optimal decomposition. Restricting decomposition specific types local behaviorsreduces significantly complexity planning. particular, present polynomialtime algorithm case individual agents perform goal-oriented behaviorscommunications. paper concludes additional tractable algorithmenables introduction human knowledge, thereby reducing overall problemfinding best time communicate. Empirical results show approachesprovide good approximate solutions.1. Introductiondecentralized Markov decision process become common formal tool studymulti-agent planning control decision-theoretic perspective (Bernstein, Givan,Immerman, & Zilberstein, 2002; Becker, Zilberstein, Lesser, & Goldman, 2004; Guestrin &Gordon, 2002; Guestrin, Koller, & Parr, 2001; Nair, Tambe, Yokoo, Pynadath, & Marsella,2003; Petrik & Zilberstein, 2007; Peshkin, Kim, Meuleau, & Kaelbling, 2000). SeukenZilberstein (2008) provide comprehensive comparison existing formal modelsalgorithms. Decentralized MDPs complement existing approaches coordinationmultiple agents based on-line learning heuristic approaches (Wolpert, Wheeler, &Tumer, 1999; Schneider, Wong, Moore, & Riedmiller, 1999; Xuan, Lesser, & Zilberstein,2001; Ghavamzadeh & Mahadevan, 2004; Nair, Tambe, Roth, & Yokoo, 2004).Many challenging real-world problems formalized instances decentralizedMDPs. problems, exchanging information constantly decision makersc2008AI Access Foundation. rights reserved.fiGoldman & Zilbersteineither undesirable impossible. Furthermore, processes controlled groupdecision makers must act based different partial views global state. Thus,centralized approach action selection infeasible. example, exchanging informationsingle central controller lead saturation communication network. Eventransitions observations agents independent, global problemmay decompose separate, individual problems, thus simple parallel algorithmmay sufficient. Choosing different local behaviors could lead different globalrewards. Therefore, agents may need exchange information periodically reviselocal behaviors. One important point understand model propose althougheventually agent behave following local behavior, choosing among possiblebehaviors requires information agents. focus situationsinformation freely available, obtained via communication.Solving optimally general decentralized control problem shown computationally hard (Bernstein et al., 2002; Pynadath & Tambe, 2002). worst case,general problem requires double-exponential algorithm1 . difficulty due twomain reasons: 1) none decision-makers full-observability global system2) global performance system depends global reward, affectedagents behaviors. previous work (Goldman & Zilberstein, 2004a), studied complexity solving optimally certain classes Dec-MDPs Dec-POMDPs2 .example, shown decentralized problems independent transitionsobservations considerably easier solve, namely, NP-complete. Evencases, agents behaviors dependent global reward function, maydecompose separate local reward functions. latter case studied withincontext auction mechanisms weakly coupled MDPs Bererton et al. (2003).paper, solution type complex decentralized problems includestemporally abstracted actions combined communication actions. Petrik Zilberstein (2007) recently presented improved solution previous Coverage Setalgorithm (Becker et al., 2004), solve decentralized problems optimally. However, technique suitable communication agents possible.Another recent study Seuken Zilberstein (2007a, 2007b) produced generalapproximation technique based dynamic programming heuristic search.approach shows better scalability, remains limited relatively small problems compareddecomposition method presented here.propose approach approximate optimal solutions decentralized problemsoff-line. main idea compute multiagent macro actions necessarily endcommunication. Assuming communication incurs cost, communication policycomputed optimally, algorithms proposed paper computebest time agents exchange information. time points, agents attain fullknowledge current global state. algorithms also compute agentdomain actions perform communication, temporally abstracted actionsinterrupted time. Since behaviors computed agent1. Unless NEXP different EXP, cannot prove super-exponential complexity. But,generally believed NEXP-complete problems require double-exponential time solve optimally.2. Dec-MDPs, observations agents sufficient determine global state,Dec-POMDPs global state cannot fully determined observations.170fiCommunication-Based Decomposition Mechanismseparately independently other, final complete solution communicationaction policies guaranteed globally optimal. refer approachcommunication-based decomposition mechanism: algorithms proposed computemechanisms decompose global behavior agents local behaviorscoordinated communication. Throughout paper, algorithms differ spacebehaviors search: solutions range general search spaceavailable (leading optimal mechanism) restricted sets behaviors.contribution paper provide tractable method, namely communicationbased decomposition mechanisms, solve decentralized problems, efficientalgorithms currently exist. general decentralized problems, approach servespractical way approximate solution systematic way. also provide analysisbounds approximations local transitions independent.specific cases, like independent transitions observations, showcompute optimal decompositions local behaviors optimal policies communication coordinate agents behaviors global level.Section 3 introduces notion communication-based mechanisms. formally frameapproach decentralized semi-Markov decision process direct communication(Dec-SMDP-Com) Section 4. Section 5 presents decentralized multi-step backuppolicy-iteration algorithm returns optimal decomposition mechanism restrictions imposed individual behaviors agents. Due generality,algorithm applicable limited domains. Section 6 presents practicalsolution, considering agent assigned local goal states. Assuming localgoal-oriented behavior reduces complexity problem polynomial number states. Empirical results (Section 6.2) support claims. approximationmechanism also applied range possible local behaviors provideddesign time. Since predetermined local behaviors alone may sufficientachieve coordination, agents still need decide communicate. Section 7 presentspolynomial-time algorithm computes policy communication, given local policies domain actions. closer human-designed local plans local optimalbehaviors, closer solution optimal joint solution. Empirical resultsMeeting Uncertainty scenario (also known Gathering Problem robotics,Suzuki Yamashita, 1999) presented Section 7.1. conclude discussioncontributions work Section 8.2. Dec-MDP modelPrevious studies shown decentralized MDPs general hard solveoptimally off-line even direct communication allowed (Bernstein et al., 2002;Pynadath & Tambe, 2002; Goldman & Zilberstein, 2004a). comprehensive complexityanalysis solving optimally decentralized control problems revealed sources difficulty solving problems (Goldman & Zilberstein, 2004a). algorithmsproposed actually solve classes problems optimally efficiently.define general underlying process allows agents exchange messages directlydecentralized POMDP direct communication:171fiGoldman & ZilbersteinDefinition 1 (Dec-POMDP-Com) decentralized partially-observable Markov decisionprocess direct communication, Dec-POMDP-Com given following tuple:=< S, A1 , A2 , , C , P, R, 1 , 2 , O, >,finite set world states, factored include distinguished initialstate s0 .A1 A2 finite sets actions. ai denotes action performed agent i.denotes alphabet messages represents atomic message sentagent (i.e., letter language).C cost transmitting atomic message: C : <. cost transmitting null message zero.P transition probability function. P (s0 |s, a1 , a2 ) probability movingstate state s0 agents 1 2 perform actions a1 a2respectively. transition model stationary, i.e., independent time.R global reward function. R(s, a1 , a2 , s0 ) represents reward obtainedsystem whole, agent 1 executes action a1 agent 2 executes action a2state resulting transition state s0 .1 2 finite sets observations.observation function. O(o1 , o2 |s, a1 , a2 , s0 ) probability observing o1o2 (respectively two agents) state agent 1 takes action a1agent 2 takes action a2 , resulting state s0 .Dec-POMDP finite horizon, represented positive integer .notation represents set discrete time points process.optimal solution decentralized problem joint policy maximizescriteriain case, expected accumulated reward system. joint policytuple composed local policies agent, composed policy actionpolicy communication: i.e., joint policy = (1 , 2 ), iA : Ai: . is, local policy action assigns action possiblesequence local observations messages received. local policy communicationassigns message possible sequence observations messages received.cycle, agents perform domain action, perceive observation sendmessage.assume system independent observations transitions (see Section 6.3discussion general case). Given factored system states = (s1 , s2 ) S,domain actions ai observations oi agent, formal definitions3decentralized processes independent transitions, observations follow. noteclass problems trivial since reward system necessarilyindependent. simplicity, present definitions case two agents. However,approach presented paper applicable systems n agents.3. definitions based Goldman Zilberstein (2004a). include makepaper self-contained.172fiCommunication-Based Decomposition MechanismDefinition 2 (A Dec-POMDP Independent Transitions) Dec-POMDPindependent transitions set states factored two components = S1 S2that:s1 , s01 S1 , s2 , s02 S2 , a1 A1 , a2 A2 ,P r(s01 |(s1 , s2 ), a1 , a2 , s02 ) = P r(s01 |s1 , a1 )P r(s02 |(s1 , s2 ), a1 , a2 , s01 ) = P r(s02 |s2 , a2 ).words, transition probability P Dec-POMDP representedP = P1 P2 , P1 = P r(s01 |s1 , a1 ) P2 = P r(s02 |s2 , a2 ).Definition 3 (A Dec-POMDP Independent Observations) Dec-POMDPindependent observations set states factored two components =S1 S2 that:o1 1 , o2 2 , = (s1 , s2 ), s0 = (s01 , s02 ) S, a1 A1 , a2 A2 ,P r(o1 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o2 ) = P r(o1 |s1 , a1 , s01 )P r(o2 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o1 ) = P r(o2 |s2 , a2 , s02 )O(o1 , o2 |(s1 , s2 ), a1 , a2 , (s01 , s02 )) = P r(o1 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o2 )P r(o2 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o1 ).words, observation probability Dec-POMDP decomposedtwo observation probabilities O1 O2 , O1 = P r(o1 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o2 )O2 = P r(o2 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o1 ).Definition 4 (Dec-MDP) decentralized Markov decision process (Dec-MDP) DecPOMDP, jointly fully observable, i.e., combination agents observationsdetermine global state system.previous work (Goldman & Zilberstein, 2004a), proved Dec-MDPs independent transitions observations locally fully-observable. particular, showedexchanging last observation sufficient obtain complete informationcurrent global state guarantees optimality solution.focus computation individual behaviors agents takingaccount exchange information time time. following sectionspresent communication-based decomposition approximation method solve Dec-MDPsdirect communication independent transitions observations.3. Communication-based Decomposition Mechanisminterested creating mechanism tell us individual behaviorsbeneficial sense behaviors taken jointly result goodapproximation optimal decentralized solution global system. Noticeeven system global objective, straightforward computeindividual behaviors. decision problem requires achievement globalobjective tell us local goals decision maker needs reach order173fiGoldman & Zilbersteinmaximize value joint policy reaches global objective. Therefore, proposecommunication-based decomposition mechanisms practical approach approximatingoptimal joint policy decentralized control problems. approach produce tworesults: 1) set temporarily abstracted actions global state agent2) policy communication, aimed synchronizing agents partial informationtime beneficial system.Formally, communication-based decomposition mechanism CDM functionglobal state decentralized problem two single agent behaviors policies:CDM : (Opt1 , Opt2 ). general, mechanism applied systems nagents, case decomposition decentralized process n individual behaviors. order study communication-based mechanisms, draw analogytemporary local policies actions options. Options definedSutton et al. (1999) temporally abstracted actions, formalized triplets includingstochastic single-agent policy, termination condition, set statesinitiated: opt =< : [0, 1], : + [0, 1], >. option availablestate I.approach considers options terminal actions (instead terminal states). Terminal actions also considered Hansen Zhou (2003) framework indefinitePOMDPs. denote domain actions agent Ai . set terminal actionsincludes messages . one agent, option given following tuple:opti =< : Si Ai , Si >, i.e., option non-stochastic policyagents partial view (local states) time set primitive domain actionsterminal actions. local states Si given factored representationDec-MDP independent transitions observations. Similarly, transitionslocal states known since P (s0 |s, a1 , a2 ) = P1 (s01 |s1 , a1 ) P2 (s02 |s2 , a2 ).paper, concentrate terminal actions necessarily communicationactions. assume options terminated whenever least one agentsinitiates communication (i.e., option message sender terminates communicates hearers option terminates due external event). also assumejoint exchange messages, i.e., whenever one agent initiates communication,global state system revealed agents receiving messages:agent 1 sends observation o1 agent 2, also receive agent 2s observation o2 .exchange messages cost system once. Since focus finite-horizonprocesses, options may also artificially terminated time limit problemreached. cost communication C may include, addition actual transmissioncost, cost resulting time takes compute agents local policies.Communication-based decomposition mechanisms enable agents operate separately certain periods time. question, then, design mechanismsapproximate best optimal joint policy decentralized problem. distinguishthree cases: general options, restricted options, predefined options.General options built primitive domain action communication actiongiven model problem. Searching possible pairs local single-agentpolicies communication policies built general options lead bestapproximation. obtained compute optimal mechanism among possiblemechanisms. Restricted options limit space feasible options much smaller set de174fiCommunication-Based Decomposition Mechanismfined using certain behavior characteristics. Consequently, obtain mechanismslower complexity. tractable mechanisms provide approximation solutions decentralized problems efficient algorithms currently exist. Obtaining optimalmechanism certain set restricted options (e.g., goal-oriented options) becomes feasible, show Sections 4-6. Furthermore, sometimes, may consider optionspre-defined. example, knowledge effective individual procedures may already exist. mechanism approach allows us combine domain knowledge solutiondecentralized problem. situations, mapping global statessingle-agent behaviors already exists, computation mechanism returns policycommunication meta-level control synchronizes agents partial information. Section 7, study greedy approach computing policy communicationknowledge local behaviors given.Practical concerns lead us study communication-based decomposition mechanisms. order design applicable mechanisms, two desirable properties needconsidered:Computational complexity whole motivation behind mechanism approach based idea mechanism low computational complexity. Therefore, computation CDM mapping practicalsense individual behaviors agent complexity lowercomplexity decentralized problem free communication.trade-off complexity computing mechanism global rewardsystem. may simple way split decentralized processseparate local behaviors. complexity characteristic taken accountdesigning mechanism; different mechanisms computed different levelsdifficulty.Dominance mechanism CDM1 dominates another mechanism CDM2global reward attained CDM1 policy communication largerglobal reward attained CDM2 communication policy. mechanismoptimal certain problem mechanism dominates it.4. Decentralized Semi-Markov Decision ProblemsSolving decentralized MDP problems communication-based decomposition mechanism translates computing set individual temporally abstracted actionsagent perform together policy communication stipulatesexchange information. Hereafter, show problem computing mechanismformalized semi-Markov decision problem. particular, set basic actionsprocess composed temporally abstracted actions together communication actions. rest paper presents three algorithms aimed solvingsemi-Markov problem optimally. algorithms differ sets actions availabledecision-makers, affecting significantly complexity finding decentralized solution.noted optimality mechanism computed conditionedassumptions algorithm (i.e., first algorithm provides optimal mechanismpossible options, second algorithm provides optimal mechanism local goalsassumed, last algorithm computes optimal policy communication assum175fiGoldman & Zilbersteining local behaviors given). Formally, decentralized semi-Markov decisionproblem direct communication (Dec-SMDP-Com) given follows:Definition 5 (Dec-SMDP-Com) factored, finite-horizon Dec-SMDP-Com underlying Dec-MDP-Com tuple< , Opt1 , Opt2 , P N , RN > where:S, , C , 1 , 2 , P , components underlying process defineddefinitions 4 1.Opti set actions available agent i. comprises possible optionsagent choose perform, terminate necessarily communication act:opti =< : Si Ai , Si >.P N (s0 , t+N |s, t, opt1 , opt2 ) probability system reaching state s0 exactlyN time units, least one option terminates (necessarily communicationact). probability function given part model every value N ,+ N . framework, N time steps least one agent initiatescommunication (for first time since time t) interrupts optionhearer agent. Then, agents get full observability synchronized state. Sincedecentralized process independent transitions observations, P N probability either agent communicated have. probabilityagent terminated option exactly time + N , PiN , given follows:1000PiN (s0i , t+N |si , t, opti ) =P(opti (si , t) ) (N = 1) (s0i = si ))(opti (si , t) ) (N = 1) (s0i 6= si ))(opti (si , t) A) (N = 1))(opti (si , t) ) (N > 1))(opti (si , t) A) (N > 1))N 0P(q|s, opti (si , t))Pi (si , (t+1)+(N 1)|qi , t+1, opti )qi Sisingle-agent probability one policy option instructs agentcommunicate (i.e., opti (si , t) ), case local process remainslocal state.use notation = (s1 , s2 ) s0 = (s01 , s02 ) refer agents local state.NThen, denote P (s0i , t+N |si , t, opti ) probability agent reach state0si N time steps follows option opti . refers probabilityreaching state s0i without terminated option necessarilystate reached. transition probability computed recursively sincetransition probability underlying Dec-MDP known:NP (s0i , t+N |si , t, opti )=0Pi (si |si , opti (si , t)))N = 1otherwisePN 00qi Si P (si |qi , opti (qi , t))P (si , (t+1)+(N 1)|si , t+1, opti )Finally, obtain that:NP N (s0 , t+N |s, t, opt1 , opt2 ) = P1N (s01 , t+N |s1 , t, opt1 ) P 2 (s02 , t+N |s2 , t, opt2 )+NP2N (s02 , t+N |s2 , t, opt2 ) P 1 (s01 , t+N |s1 , t, opt1 )P1N (s01 , t+N |s1 , t, opt1 ) P2N (s02 , t+N |s2 , t, opt2 )176fiCommunication-Based Decomposition MechanismRN (s, t, opt1 , opt2 , s0 , t+N ) expected reward obtained system N time stepsagents started options opt1 opt2 respectively state time t,least one terminated option communication act (resultingtermination agents option). reward computed t+N .(0NR (s, t, opt1 , opt2 , , t+N ) =C(opt1 , opt2 , s, s0 , N )t+N =0C(opt1 , opt2 , s, , N ) + C otherwiseC(opt1 , opt2 , s, s0 , N ) expected cost incurred system transitionsstates s0 least one agent communicates N time steps.define probability certain sequence global states transitionedsystem agent follows corresponding option P (< s0 , s1 , . . . , sN >):01NP (< , , . . . , >) =N1P (sj+1 |sj , opt1 (sj1 ), opt2 (sj2 ))j=0normalizing factor makes sure possible sequences, probability adds one given s0 , sN N steps going intermediate steps s1 , . . . , sN 1 . Then, denote Rseq reward attained system traverses certain sequence states. Formally, Rseq (< s0 , . . . , sN >Pjjjjj+1 )) = N1opti (si )) refers primitive actionj=0 R(s , opt1 (s1 ), opt2 (s2 ),chosen option local state sji . Finally, define expectedcost C(opt1 , opt2 , s, s0 , N ) follows:C(opt1 , opt2 , s, s0 , N ) =XP (< s, q 1 , . . . , q N1 , s0 >)Rseq (< s, q 1 , . . . , q N1 , s0 >)q 1 ,...,q N1dynamics semi-Markov decentralized process follows. agent performs option starting global state fully observed. agents optionmapping local states actions, agent starts option state si timeterminates state s0i , k time steps later. Whenever options terminated,agents fully observe global state due terminal communication actions.reach state s0 time t+k < , joint policy chooses possible different pairoptions state s0 time t+k process continues.Communication model leads joint exchange messages. Thereforeagents observe global state system information exchanged. meansstates decentralized semi-Markov process fully-observable (as opposedjointly fully-observable states classical Dec-MDP-Com).local policy agent Dec-SMDP-Com mapping global statesoptions (as opposed mapping sequences observations generalDec-POMDP case, mapping local state Dec-MDPs independenttransitions observations):: Opti177fiGoldman & Zilbersteinjoint policy tuple local policies, one agent, i.e., joint policy instructsagent choose option global state. Thus, solving optimal mechanism equivalent solving optimally decentralized semi-Markov decision problemstemporally abstracted actions.Lemma 1 Dec-SMDP-Com equivalent multi-agent MDP.Proof. Multiagent MDPs (MMDPs) represent Markov decision process controlledseveral agents (Boutilier, 1999). One important feature modelagents central view global state. Formally, MMDPs tuples form< Ag, {Ai }iAg , S, P r, R >, where:Ag finite collection n agents.{Ai }iAg represents joint action space.finite set system states.P r(s0 |s, a1 , . . . , ) transition probability global states s0agents perform joint action.R : < reward system obtains global state reached.decentralized semi-Markov problem direct communication solved optimally solving corresponding MMDP. simpicity exposition show proofsystems two agents. Following Definition 5, 2-agent Dec-SMDP-Com giventuple: < , Opt1 , Opt2 , P N , RN >. mapping two modelsfollows: Ag finite collection agents control MMDP semiMarkov process. set set states world cases. MMDPmodel, states fully observable definition. semi-Markov decentralizedmodel global states also fully observable agents always exchange information end option perform. set joint actions {Ai }iAg givensemi-Markov process set options available agent (e.g., n = 2{Ai } = {Opt1 , Opt2 }). difference joint actions chosen primitivedomain actions MMDP options temporarily abstracted actionsterminate communication act. probability transition reward functionseasily mapped models matching P N P r RN R.solution MMDP (or Dec-SMDP-Com) problem strategy assignsjoint action (or set options) global state. Solving MMDP actionsgiven options solves semi-Markov problem. Solving semi-Markov problemoptions length two, i.e., option composed exactly one primitive actionfollowed communication action tells agent communicate observationsolves corresponding MMDP problem.2Solving decentralized semi-Markov process communication P-completeLemma 1 polynomial complexity single agent MDPs (Papadimitriou & Tsitsiklis, 1987). However, input problem includes states alsodouble exponential number domain actions agent. explained nextsection, option represented tree, where: 1) depth option limited finite horizon 2) branching factor option constrainednumber states S. Therefore, maximal number leaves option might178fiCommunication-Based Decomposition Mechanismbounded |S|T . Consequently, |A||S| assignments primitive domaincommunication acts leaves possible option.naive solution Dec-SMDP-Com problem search space possiblepairs options find pair maximizes value global state. multistep policy-iteration algorithm, presented Section 5, implements heuristic versionsearch converges optimal mechanism. resulting search space (after pruning)become intractable even simple small problems. Therefore, proposeapply communication-based decomposition mechanisms restricted sets options.Solving Dec-SMDP-Com restricted set options means find optimal policyattains maximal value possible options restricted set (Sutton et al.,1999; Puterman, 1994). Sections 6 7 present two additional algorithms solveDec-SMDP-Com problems options considered goal-oriented options, i.e.,mechanism assigns local goals one agents global state, allowingcommunicate reached local goals.5. Multi-step Backup Policy-Iteration Dec-SMDP-ComSolving Dec-SMDP problem optimally means computing optimal pair optionsfully-observable global state. options instruct agents act independently information exchanged. order find optionsglobal state, apply adapted extended version multi-step backup policyiteration algorithm heuristic search (Hansen, 1997). show decentralizedversion algorithm converges optimal policy decentralized casetemporally abstracted actions.extend model single-agent POMDP observations costs DecSMDP-Com model. global perspective, agent follows optionwithout knowing global state system, following open-loop policy. However,locally, agent following option, depend agents local observations. first define multi-step backup options, s0 global statesdecentralized problem: V (s, t, ) =min{b,Tt}max{opt1 ,opt2 OPTbXXk=1s0P N (s0 , t+k|s, t, opt1 , opt2 )[RN (s, t, opt1 , opt2 , s0 , t+k) + V (s0 , t+k, )]}OPTb set options length b, length defined follows:Definition 6 (The length Option) length option k optionperform k domain actions one execution.Hansens work, b bound length options (k b). Here, finitehorizon Dec-SMDP-Com case analyzed, therefore b . P N (s0 , t+k|s, t, opt1 , opt2 )RN (s, t, opt1 , opt2 , s0 , t+k) taken Dec-SMDP-Com model (Definition 5).apply multi-step backup policy-iteration algorithm (see Figure 2) usingpruning rule introduced Hansen (1997), adapt work pairs policiesinstead linear sequences actions. resulting optimal multi-step backup policyequivalent optimal policy MMDP (Lemma 1), i.e., equivalent optimaldecentralized policy Dec-SMDP-Com temporally abstracted actions. order179fiGoldman & Zilbersteins1a1s1s2s3a1!2!3s1s4a2a3Figure 1: policy tree size k=3.explain pruning rule decentralized case temporally abstracted actions,define policy-tree structures are.Definition 7 (Policy-tree) policy-tree tree structure, composed local state nodescorresponding action nodes level. Communication actions assignedleaves tree. edges connecting action (taken parent state si )resulting state s0i transition probability Pi (s0i |si , a) assigned them.Figure 1 shows possible policy-tree. option represented policy-treeleaves assigned communication actions. denote policy-tree sroot , stateassigned root (e.g., sroot ), assignment domain actions local statesrest nodes (e.g., ). size policy-tree defined follows:Definition 8 (Size Policy-tree) size policy-tree k longest branchtree, starting root composed k 1 edges (counting edgesactions resulting states). policy-tree size one includes root state actiontaken state.(k ) policy induced assignment k actions implementation. expected cost g policy-tree sroot k expected cost incurredagent follows policy (k ). denote set nodes treecorrespond leaves N L set states assigned SN L . notation\ n refers assignment excluding node n. expected cost tree, g(sroot k ),computed follows:(g(sroot k ) =C(aroot )k = 1PC(aroot ) + s0 SN L [P r(s0i |sroot , aroot )g(s0i ( \ root)k1 )] 1 < kSince decentralized process factored states, write global statepair (s1 , s2 ). agent act independently period time kperforms option. Therefore, refer information state systemk time steps s1 ks2 k , s1 k s2 k correspond agents policy treesize k. assume least one agent communicates time t+k. necessarily180fiCommunication-Based Decomposition Mechanisminterrupt agents option time t+k. Therefore, sufficient lookpairs trees size k. information state refers belief agentforms world based partial information available operateslocally. model, agents get full observability communicate exchangeobservations.heuristic function used search optimal decentralized jointpolicy Dec-SMDP-Com follows traditional notation, i.e., f (s) = g(s) + h(s).case, functions defined pairs policy-trees, i.e., f (sk k ) =G(sk k ) + H(sk k ). f value denotes backed-up value implementing policies(k ) (k ), respectively two agents, starting state time t. expectedvalue state time horizon given multi-step backup statefollows:V (s, t, ) = max {f (s)}.||,|| bNote policy-trees corresponding assignments sizeb . define expected cost implementing pair policy-trees, G, sumexpected costs one separately. leaves communication actions,cost communication taken account g functions. Hansens work,leaves assigned communication action, assume agents sensecost compute f function.G(s1 ks2 k ) = g(s1 k ) + g(s2 k ).option policy-tree communication actions assigned leaves.option denoted opt1 (k ) (or opt2 (k )). message associated leaf correspondslocal state assigned leaf (or ). define expected valueperfect information information state k time steps:H(sk k ) =XP N (s0 , t+k|s, t, opt1 (k ), opt2 (k ))V (s0 , t+k, )s0multi-step backup policy-iteration algorithm adapted Hansen decentralized control case appears Figure 2. Intuitively, heuristic search possibleoptions unfolds follows: node search space composed two policy-trees,representing local policy one agent. search advances nodes whose fvalue (considering trees) greater value global root state (composedroots policy-trees). nodes whose f value follow inequalityactually pruned used updating joint policy. policy updatednode, composed two options found f > V . leavesoptions (at possible depths) include communication acts. updated policy 0 mapsglobal state two options. leaves one policy-tree currentdepth communication actions assigned, algorithm assigns communication actsleaves policy-tree depth. change policies correct joint exchange information (i.e., actions interruptedleast one agent communicates). notice, though, may leavespolicy-trees depths lower may still domain actions assigned. Therefore,policy-trees cannot considered options yet remain stack. leaves181fiGoldman & Zilberstein1. Initialization: Start initial joint policy assigns pair options global state s.2.Evaluation: S, V (s, t, ) =PTPolicyPP N (s0 , t+k|s, t, opt1 (s1 , t), opt2 (s2 , t))[RN (s, t, opt1 (s1 , t), opt2 (s2 , t), s0 , t+k) + V (s0 , t+k, )]k=1s03. Policy Improvement: state = (s1 , s2 ) :a. Set-up:Create search node possible pair policy-trees length 1: (s1 1 , s2 1 ).Compute f (s1 1 ) = G(s1 1 ) + H(s1 1 ).Push search node onto stack.b. search stack empty, do:i. Get next pair policy-trees:Pop search node stack let (s1 , s2 )(the policy-trees length starting state = (s1 , s2 ))Let f (si ) estimated value.ii. Possibly update policy:(f (si ) = G(si ) + H(si )) > V (s, t, ),leaves depth either communication action assigned,Assign communication action leaves policy-tree depthleaves depths communication action assigned,Denote new two options opti1 opti2 .Let 0 (s) = (opti1 , opti2 ) V (s, t, ) = f (si ).iii. Possibly expand node:(f (si ) = G(si ) + H(si )) > V (s, t, ),((some leaves either domain actions assigned)((i+2) ))/*At t+1 new action taken transition another state t+2*/Create successor node two policy-trees length i,adding possible transition states actions leaf treecommunication action assigned it.Calculate f value new node (i.e., either f (si+1 i+1 ) policytrees expanded, recalculate f (si ) one communicationactions leaves depth i)Push node onto stack./*All nodes f < V pruned pushed stack.*/4. Convergence test:= 0return 0else set = 0 , GOTO 2.Figure 2: Multi-step Backup Policy-iteration (MSBPI) using depth-first branch-and-bound.remain assigned domain actions expanded algorithm. expansionrequires addition possible next states, reachable performingdomain-actions leaves, addition possible action state.leaves depth one policy-tree already assigned communication acts,algorithm expands leaves domain actions lower levels policy-trees.leaf expanded beyond level corresponding time one agentgoing initiate communication option going interrupted anyways.next section, show convergence Multi-step Backup Policy-iteration(MSBPI) algorithm optimal decentralized solution Dec-SMDP-Com,agents follow temporally abstracted actions horizon finite.182fiCommunication-Based Decomposition Mechanism5.1 Optimal Decentralized Solution Multi-step Backupssection, prove MSBPI algorithm presented Figure 2 convergesoptimal decentralized control joint policy temporally abstracted actions directcommunication. first show policy improvement step algorithm basedheuristic multi-step backups improves value current policy sub-optimal.Finally, policy iteration algorithm iterates improving policies knownconverge.Theorem 1 current joint policy optimal, policy improvement stepmulti-step backup policy-iteration algorithm always finds improved joint policy.Proof. adapt Hansens proof decentralized control problem, policiesrepresented policy-trees. Algorithm MSBPI Figure 2 updates current policynew policy assigns pair options yield greater value certain global state.show induction size options, least one state, new optionfound improvement step (step 3.b.ii).value state improved two policy-trees size one, improvedjoint policy found policy-trees size one evaluated. initializedpolicy-trees. assume improved joint policy found policytrees size k. show improved joint policy found policy-treessize k. Lets assume k policy tree size k, f (sk ) > V (s)communication actions assigned leaves. case policy followedagent 2 interrupted time k latest. One possibility skevaluated algorithm. Then, improved joint policy indeed found. pairpolicy-trees evaluated algorithm, means pruned earlier.assume happened level i. means f (si ) < V (s). assumedf (sk ) > V (s) obtain that: f (sk ) > f (si ).expand f values inequality, obtain following:g(si )+g(s)+XP N (s0 , t+i|s, opt1 (i ), opt2 ())[g(s0 (i, k))+g(s0 )+XP N (s00 , t+i+ki)V (s00 )] >s00s0g(si ) + g(s) +XP N (s0 , t+i|s, opt1 (k ), opt2 ())V (s0 , t+i)s0g(s0 (i, k)) refers expected cost subtree starting level endinglevel k starting s0 . simplification obtain:XP N (s0 , t+i|s, opt1 (i ), opt2 ())[g(s0 (i, k)) + g(s0 ) +s0XP N (s00 , t+i+ki)V (s00 )] >s00XP N (s0 , t+i|s, opt1 (k ), opt2 ())V (s0 , t+i)s0is, exists state s0 f (s0 (i, k)) > V (s0 ). Since policy-tree(i, k) size less k, induction assumption obtain existsstate s0 multi-step backed-up value increased. Therefore, policy foundstep 3.b.ii indeed improved policy.2183fiGoldman & ZilbersteinLemma 2 complexity computing optimal mechanism general options(T 1)MSBPI algorithm O(((|A1 | + ||)(|A2 | + ||))|S|). (General options basedpossible primitive domain action model, communication act).Proof. agent perform primitive domain actions Ai communicate possible message . |S|(T 1) leaves policy treehorizon |S| possible resulting states transition. Therefore, timeMSBPI algorithm expands policy tree (step 3.b.iii Figure 2), number result(T 1)ing trees ((|A1 | + ||)(|A2 | + ||))|S|. worst case, number treesalgorithm develop one iteration. Therefore, size search spacefunction number times number iterations convergence.2Solving optimally Dec-MDP-Com independent transitions observationsshown NP (Goldman & Zilberstein, 2004a). show here, solvingoptimal mechanism harder, although solution may optimal. duemain difference two problems. Dec-MDP-Com, knowsince transitions observations independent, local state sufficient statistichistory observations. However, order compute optimal mechanismneed search space options, is, single local state sufficient statistic.options allowed general, search space larger since possibleoption needs considered arbitrarily large (with length branchbounded ). example, Meeting Uncertainty scenario (presentedSection 7.1), agents aim meeting stochastic environment shortest timepossible. agent choose perform anyone six primitive actions (four moveactions, one stay action communication action). Even small world composed100 possible locations, implementing MSBPI algorithm intractable. requireexpansion possible combinations pairs policy-trees leading possible(T 1)addition 36100nodes search space iteration. Restricting mechanismcertain set possible options, example goal-oriented options leads significantreduction complexity algorithm shown following two sections.6. Dec-SMDP-Com Local Goal-oriented Behaviorprevious section provided algorithm computes optimal mechanism, searching possible combinations domain communication actions agent.one hand, solution general restrict individualbehaviors aspect. hand, solution may require searchlarge space, even space pruned heuristic search technique. Therefore,order provide practical decomposition mechanism algorithm, reasonable restrictmechanism certain sets individual behaviors. section, concentrategoal-oriented options propose algorithm computes optimal mechanismrespect set options: i.e., algorithm finds mapping global stateset locally goal oriented behaviors highest value. algorithm proposedstructure MSBPI algorithm; main difference optionsbuilt.184fiCommunication-Based Decomposition MechanismDefinition 9 (Goal-oriented Options) goal-oriented option local policy achievesgiven local goal.study locally goal-oriented mechanisms, map global state pairgoal-oriented options period time k. assume set local goals Giprovided problem. local goal, local policy achieveconsidered goal-oriented option. mechanism applied, agent followspolicy corresponding local goal k time steps. time k + 1, agents exchangeinformation stop acting (even though may reached local goal).agents, then, become synchronized assigned possibly different local goalsworking period k 0 .algorithm presented section, LGO-MSBPI, solves decentralized controlproblem communication finding optimal mapping global states localgoals periods time (the algorithm finds best solution given agentsact individually periods time). start arbitrary joint policyassigns one pair local goal states number k global state. current jointpolicy evaluated set current best known mechanism. Given joint policy: G1 G2 , (Gi Si S), value state time t,finite horizon given Equation 1: (this value computed statest+k ).0=P00NN 0(s,t+k|s,t,(s),(sPV (s, t, ) =g1 1g2 2 ))[Rg (s, t, g1 , g2 , , k)+V (s , t+k, )] (1)s0 gs.t. (s, t) = (g1 , g2 , k)Notice RgN (s, g1 ,g2 , s0 , k) defined similarly RN () (see Definition 5), takingaccount options aimed reaching certain local goal state (g1g2 aimed reaching local goal states g1 g2 , respectively).RgN (s, t, g1 , g2 , s0 , k) = C(g1 , g2 , s, s0 , k)+C =C +XP (< s, q 1 , . . . , q k1 , s0 >) Cseq (< s, q 1 , . . . , sk1 , s0 >q 1 ,...,q k1one-to-one mapping goals goal-oriented options. is,policy gi assigned found agent independently solving optimallyagents local process DPi = (Si , Pi , Ri , Gi , ): set global states factoredagent set local states. process independent transitions,Pi primitive transition probability known described options framework.Ri cost incurred agent performs primitive action ai zeroagent reaches goal state Gi . finite horizon global problem.PgN (with goal g subscript) different probability function P N appearsSection 4. PgN probability reaching global state s0 k time steps,trying reach g1 g2 respectively following corresponding optimal local policies.PgN (s0 , t+k|s, t+i, g1 (s1 ), g2 (s2 )) =185fiGoldman & Zilberstein= k = s0= k =6 s0< k10PN 0P (s |s, g1 (s1 ), g2 (s2 )) Pg (s , t+k|s , t+i+1, g1 (s1 ), g2 (s2 ))s.t. (s, t+i) = (g1 , g2 , k)iteration LGO-MSBPI algorithm (shown Figure 3) tries improvevalue state testing possible pairs local goal states increasing numbertime steps allowed communication. value f computed mappingstates assignments local goals periods time. f function givenglobal state, current time, pair local goals given period time k expresses costincurred agents acted k time steps communicated timek+1, expected value reachable states k time steps (these statesreached agents following corresponding optimal local policies towards g1g2 respectively). current joint policy updated f value states, time t, local goals g1 g2 period k greater value V (s, t, ) computedcurrent best known assignment local goals period time. Formally:f (s, t, g1 , g2 , k) = G(s, t, g1 , g2 , k) + H(s, t, g1 , g2 , k)(2)G(s, t, g1 , g2 , k) = C(g1 , g2 , s, t, k) + C(3)H(s, t, g1 , g2 , k) =0=<P 0 P N (s0 , + k|s, t, (s ), (s ))V (s0 , + k, )g1 1g2 2g(4)C(g1 , g2 , s, t, k) expected cost incurred agents following corresponding options k time steps starting state s. defined similarlyexpected cost explained Definition 5. notice computation f refersgoals evaluated algorithm, evaluation policy (step 2) refersgoals assigned current best policy.6.1 Convergence Algorithm ComplexityLemma 3 algorithm LGO-MSBPI Figure 3 converges optimal solution.Proof. set global states set local goal states Gi finite.horizon also finite. Therefore, step 3 algorithm terminate. Likeclassical policy-iteration algorithm, LGO-MSBPI algorithm also convergefinite numbers calls step 3 policy improve value oneiteration another.2Lemma 4 complexity computing optimal mechanism based local goal-orientedbehavior following LGO-MSBPI algorithm polynomial size state space.Proof. Step 2 LGO-MSBPI algorithm computed dynamic programmingpolynomial time (the value state computed backwards manner finitehorizon ). complexity improving policy Step 3 polynomial time,186fiCommunication-Based Decomposition Mechanism1. Initialization: Start initial joint policy assigns local goalsgi Gi time periods k NS, : (s, t) = (g1 , g2 , k)2. Policy Evaluation: S, Compute V (s, t, ) based Equation 1.3. Policy Improvement:a. k = 1b. (k < )i. s, t, g1 , g2 : Compute f (s, t, g1 , g2 , k) based Equations 2,3 4.ii. Possible update policyf (s, t, g1 , g2 , k) > V (s, t, )(s, t) (g1 , g2 , k) / Communicate k + 1 /V (s, t, ) f (s, t, g1 , g2 , k)iii. Test joint policies next extended period timek k+14. Convergence test:change Step 3returnelse GOTO 2.Figure 3: Multi-step Backup(LGO-MSBPI).Policy-iterationlocalgoal-orientedbehaviornumber states number goal states, i.e., O(T 2 |S||G|). worst case, everycomponent global state local goal state. However, cases, |Gi |much smaller |Si | Gi strict subset Si , decreasing even runningtime algorithm.26.2 Experiments - Goal-oriented Optionsillustrate LGO-MSBPI decomposition mechanism production control scenario.assume two machines, control production boxescereals: machine M1 produce two types boxes b. amount boxes typeproduced machine denoted Ba (Bb represents amount boxes type bproduced respectively). Machine M2 produce two kinds cereals b. Ca (and Cbrespectively) denotes number bags cereals type (we assume one bagcereals sold one box type). boxes differ presentationboxes type advertise content type boxes type b advertise contenttype b. assume discrete time t, machine M1 may produce one boxboxes all, machine may produce one bag cereals may producecereal all. production process stochastic sense machinesperfect: probability PM1 , machine one succeeds producing intended box(either b) probability 1 PM1 , machine produce boxparticular time unit. Similarly, assume PM2 expresses probability machine twoproducing one bag cereals type b required selling one box.example, reward attained system equal number products ready187fiGoldman & Zilbersteinsale, i.e., min{Ba , Ca } + min{Bb , Cb }. product sold composed onebox together one bag cereals corresponding type advertised box.goal-oriented option given number products machineproduce. Therefore, option opti scenario described pair numbers (Xa , Xb )(when machine one X refers boxes machine two, X refers bagscereals). is, machine instructed produce Xa items type a, followedXb items type b, followed Xa items type forth either time limitanyone machines decides communicate. machines exchangeinformation, global state revealed, i.e., current number boxes cerealsproduced far known. Given set goal-oriented options, LGO-MSBPI algorithmreturned optimal joint policy action communication solves problem.counted time units takes produce boxes cereals. comparedlocally goal oriented multi-step backup policy iteration algorithm (LGO-MSBPI)two approaches: 1) Ideal case machines exchange informationstate production time cost. idealized case, sincereality exchanging information incur cost, example changing settingmachine takes valuable time 2) Always Communicate ad-hoc case,machines exchange information time step also incur costit. Tables 1, 2, 3 present average utility obtained production systemcost communication set 0.1, 1 10 respectively, costdomain action set 1 joint utility averaged 1000 experiments.state represented tuple (Ba , Bb , Ca , Cb ). initial state set (0,0,0,8),boxes produced already 8 bags cereals type B.finite horizon set 10. set goal-oriented options (Xa , Xb ) tested included(0,1),(1,4),(2,3),(1,1),(3,2),(4,1) (1,0).PM1 , PM20.2, 0.20.2, 0.80.8, 0.8Ideal C = 0-17.012-16.999-11.003Average UtilityAlways Communicate-18.017-17.94-12.01LGO-MSBPI-17.7949-18.0026-12.446Table 1: C = 0.10, Ra = 1.0.PM1 , PM20.2, 0.20.2, 0.80.8, 0.8Ideal C = 0-17.012-16.999-11.003Average UtilityAlways Communicate-26.99-26.985-20.995LGO-MSBPI-19.584-25.294-17.908Table 2: C = 1.0, Ra = 1.0.LGO-MSBPI algorithm computed mechanism resulted three productsaverage uncertainty least one machine set 0.2 1000 testsrun, ten time units. number products increased average 89 products machines succeeded 80% cases. numbers products188fiCommunication-Based Decomposition MechanismPM1 , PM20.2, 0.20.2, 0.80.8, 0.8Ideal C = 0-17.012-16.999-11.003Average UtilityAlways Communicate-117-117.028-110.961LGO-MSBPI-17.262-87.27-81.798Table 3: C = 10.0, Ra = 1.0.always attained either decomposition mechanism implementedad-hoc approaches tested. Ideal Always Communicate algorithms differrespect cost communication, differ actual policiesaction. Although machines incur higher cost mechanism applied comparedideal case (due cost communication), number final products readysell almost amount. is, take time orderproduce right amount products policies implemented computedlocally goal oriented multi-step backup policy iteration algorithm. costcommunication scenario capture cost changing setting one machineone production program another. Therefore, result significant costcommunication high compared time whole process takes.decomposition mechanism finds times beneficial synchronize informationconstant communication feasible desirable due high cost.6.3 Generalization LGO-MSBPI Algorithmmechanism approach assumes agents operate independentperiod time. However, decentralized process kind dependencyobservations transitions, assumption violated, i.e., plans reachlocal goals interfere (the local goals may compatible).LGO-MSBPI algorithm presented paper applied Dec-MDPstransitions observations assumed independent. section, bounderror utilities options computed LGO-MSBPI algorithmsdependencies exist. define independent decentralized processes refer nearlyindependent processes whose dependency quantified cost marginalinteractions.Definition 10 (independent Process) Let C Ai (s gk |gj ) expected cost incurred agent following optimal local policy reach local goal state gkstate s, agent following optimal policy reach gj . decentralizedcontrol process independent = max{1 , 2 }, 1 2 definedfollows: g1 , g10 G1 S1 , g2 , g20 G2 S2 S:1 = max{max{max0 {C A1 (s0 g1 |g20 ) C A1 (s0 g1 |g2 )}}}g1g2 ,g22 = max{max{max0 {C A2 (s0 g2 |g10 ) C A2 (s0 g2 |g1 )}}}g2g1 ,g1189fiGoldman & Zilbersteinis, maximal difference cost agent may incur tryingreach one local goal state interferes possible local goal reachedagent.computation cost function C Ai (s gk |gj ) domain dependent.address issue compute cost provide condition. individualcosts one agent affected interference exists pair localgoals. example, assume 2D grid scenario: one agent move four directions(north, south, east west) needs reach location (9,9) (0,0). second agentable moving also collecting rocks blocking squares grid. Assumingsecond agent assigned task blocking squares even rows,first agents solution task constrained squares free cross.case, agent ones cost reach (9,9) depends path choose dependsstrongly state grid resulting second agents actions.value denotes amount interference might occur agentslocally goal-oriented behaviors. Dec-MDP independent transitions observations, value zero. LGO-MSBPI algorithm proposed papercomputes mechanism global state mapping states pairs localgoal states ignoring potential interference. Therefore, difference actualcost incurred options found algorithm optimal options. Since mechanism applied global state time stepsloss cost occur worst case agents, algorithm presented2T optimal general case.7. Myopic-greedy Approach Direct Communicationcases, reasonable assume single-agent behaviors already knownfixed, ahead time possible global state. example, may occursettings individual agents designed ahead coordination time (e.g., agentsmanufacturing line represent machines, built specifically implementcertain procedures). achieve coordination, though, additional method mayneeded synchronize individual behaviors. section, present applycommunication-based decomposition approach compute policy communicationsynchronize given goal-oriented options. take myopic-greedy approachruns polynomial-time: i.e., time agent makes decision, choosesaction maximal expected accumulated reward assuming agents ablecommunicate along whole process. Notice LGO-MSBPIgeneral sense also computed local goals pursuedagent together communication policy synchronizes individual behaviors.Here, time agents exchange information, mechanism applied inducing twoindividual behaviors (chosen given mapping states individual behaviors).given optimal policies action (with communication actions) denoted 1A2A respectively.expected global reward system, given agents communicatefollows corresponding optimal policy iA given valueinitial state s0 : nc (s0 , 1A , 2A ). value computed summing possible190fiCommunication-Based Decomposition Mechanismnext states computing probability agent reaching it, reward obtainedrecursive value computed next states.nc (s0 , 1A , 2A ) =XP1 (s01 |s01 , 1A (s01 )) P2 (s02 |s02 , 2A (s02 ))(R(s0 , 1A (s01 ), 2A (s02 ), s0 ) + nc (s0 , 1A , 2A ))(s01 ,s02 )denote expected cost system computed agent i, last synchronizedstate s0 , agents communicate state continue withoutcommunication, c (s0 , si , 1A , 2A ):c (s0 , s1 , 1A , 2A ) =XP2 (s2 |s02 , 2A )(R(s0 , 1A (s01 ), 2A (s02 ), (s1 , s2 )) + nc ((s1 , s2 ), 1A , 2A ) + C F lag)s2Flag zero agents reached global goal state reached state s.time stamp state denoted t(s). P (s|, s0 , 1A , 2A ) probability reaching statestate s0 , following given policies action.1P (s0 |s, (s ), (s ))1212P (s0 |s, 1A , 2A ) =0P000 00s00 P (s |s , 1 , 2 ) P (s |s, 1 , 2 )= s0t(s0 ) = t(s) + 1t(s0 ) < t(s) + 1otherwiseSimilarly, P1 (P2 ) defined probability reaching s01 (s02 ), given agent 1 (2)scurrent partial view s1 (s2 ) policy action 1A (2A ). accumulated rewardattained agents move state s0 state given follows:R(s0 , 1A (s01 ), 2A (s02 ), s)t(s) = t(s0 ) + 1t(s) > t(s0 ) + 1P0000R(s0 , 1A , 2A , s) =02 , )s00 P (s |1 , 200 , ) P00(s|1 , 00000(R(s , 1 , 2 , ) + R(s , 1 (s1 ), 2A (s2 ), s))state, agent decides whether communicate partial view basedwhether expected cost following policies action, communicatedlarger smaller expected cost following policies actioncommunicated.Lemma 5 Deciding Dec-MDP-Com myopic-greedy approach direct communication P class.Proof. agent executes known policy iA mechanism applied. localgoals provided instead actual policies, finding optimal single-agent policiesreach goal states done polynomial time. complexity findingcommunication policy dynamic programming (based formulas above),therefore computing policy communication also P. |S| statesnc c need computed, one formulas solved timepolynomial |S|.2previous work, also studied set monotonic goal-oriented Dec-MDPs,provide algorithm finds optimal policy communication assumingset individual behaviors provided (Goldman & Zilberstein, 2004b).191fiGoldman & Zilberstein7.1 Meeting Uncertainty Examplepresent empirical results obtained myopic-greedy approach appliedMeeting Uncertainty example4 . testbed consider sample problem DecMDP-Com involving two agents meet location early possible.scenario also known gathering problem robotics (Suzuki & Yamashita, 1999).environment represented 2D grid discrete locations. example,global state occupied agents considered global goal state. setcontrol actions includes moving North, South, East West, stayinglocation. agents partial view (which locally fully-observable) correspondsagents location coordinates. observations transitions independent.outcomes agents actions uncertain: is, probability Pi , agent arrivesdesired location taken move action, probability 1 Pi agentremains location. Due uncertainty effects agents actions,clear setting predetermined meeting point best strategy designingagents. Agents may able meet faster change meeting placerealizing actual locations. achieved exchanging informationlocations agents, otherwise observable. showed exchanginglast observation guarantees optimality Dec-MDP-Com process constant messagecosts (Goldman & Zilberstein, 2004a). example tested, messages exchangedcorrespond agents observations, i.e., location coordinates.implemented locally goal-oriented mechanism assigns single localgoal agent synchronized state. local goals chosen locationmiddle shortest Manhattan path agents locations (this distancerevealed information exchanged).Intuitively, desirable mechanism set meeting place middleshortest Manhattan path connects two agents absence communication, cost meet point minimal. shown computingexpected time meet, nc , pair possible distances two agentslocation grid, communication possible. simplify exposition,use function takes advantage specific characteristics example.notation follows: agent 1 distance d1 meeting location, agent 2distance d2 location, system incurs cost one time periodagents met yet. agents meeting location, expected timemeet zero, nc (0, 0) = 0. agent 2 meeting location, agent 1reached location yet, expected time meet givennc (d1 , 0) = P1 (1 + nc (d1 1, 0)) + (1P1 ) (1 + nc (d1 , 0)) == P1 nc (d1 1, 0)) + (1P1 ) nc (d1 , 0)) 1is, probability P1 agent 1 succeeds decreasing distance meeting location one, probability 1 P1 fails remains location. Recursively, compute remaining expected time meet updated parameters.Similarly agent 2: nc (0, d2 ) = P2 (1 + nc (0, d2 1)) + (1P2 ) (1+nc (0, d2 )).none agents reached meeting place yet, four different caseseither both, one, none succeeded moving right direction either4. empirical results section described first Goldman Zilberstein (2003).192fiCommunication-Based Decomposition Mechanismdecreased distances meeting location respectively:nc (d1 , d2 ) = P1 P2 (1 + nc (d1 1, d2 1)) + P1 (1P2 ) (1 + nc (d1 1, d2 ))++(1P1 ) P2 (1 + nc (d1 , d2 1)) + (1P1 ) (1P2 ) (1 + nc (d1 , d2 )) == P1 P2 nc (d1 1, d2 1) + P1 (1P2 ) nc (d1 1, d2 ) + (1P1 ) P2 nc (d1 , d2 1)++(1P1 ) (1P2 ) nc (d1 , d2 ) 1value nc (d1 , d2 ) computed possible distances d1 d2 2D gridsize 10 10. minimal expected time meet obtained d1 = d2 = 9expected cost 12.16.summary, approximating optimal solution Meeting Uncertaintyexample direct communication possible mechanism applied one described unfold follows: time t0 , initial state system s0 fullyobservable agents. agents set meeting point middle Manhattan path connects them. Denote d0 distance agents t0gt0 = (gt10 , gt20 ) goal state set t0 . one agents move optimally towardscorresponding component gt0 . agent moves independently environmenttransitions observations independent. time t, policycommunication instructs agent initiate exchange information, current Manhattan distance agents dt revealed both. Then, mechanism applied,setting possibly new goal state gt , decomposes two components oneagent. goal state gt middle Manhattan path connects agentslength dt revealed communication.7.2 Experiments - Myopic-greedy Approachfollowing experiments, assumed transition probabilities P1 P2equal. uncertainties specified parameter Pu . mechanismapplied whenever agents communicate time results agent adopting localgoal state, set location middle Manhattan path connectingagents (the Manhattan distance agents revealed time t). comparejoint utility attained system following four different scenarios:1. No-Communication meeting point fixed time t0 remains fixed alongsimulation. located middle Manhattan path connectsagents, known time t0 . agent follows optimal policy actionlocation without communicating.2. Ideal case assumes agents communicate freely (C = 0) everytime step resulting highest global utility agents attain. Notice,though, optimal solution looking for, assumecommunication free. Nevertheless, difference utility obtainedfirst two cases shed light trade-off achieved implementingnon-free communication policies.3. Communicate SubGoals heuristic solution problem, assumesagents notion sub-goals. notify sub-goalsachieved, eventually leading agents meet.193fiGoldman & ZilbersteinA1A1A2A2Timenew subgoal set agent 2 arrivedsubgoal set time t.Figure 4: Goal decomposition sub-goal areas.4. Myopic-greedy Approach Agents act myopically optimizing choicesend message, assuming additional communication possible. possibledistance agents, policy communication computedstipulates best time send message. iterating policy agentsable communicate thus approximate optimal solutiondecentralized control problem direct communication. agents continuemoving meet.solution No-Communication case solved analytically MeetingUncertainty example, computing expected cost nc (d1 , d2 ) incurred twoagents located distances d1 d2 respectively goal state time t0 (the completemathematical solution appears Section 7.1). Ideal case, set 1000 experimentsrun cost communication set zero. Agents communicate locationsevery time instance, update location meeting place accordingly. Agents moveoptimally last synchronized meeting location.third case tested (Communicate SubGoals) sub-goal defined cellsgrid distance equal p d/2 center located d/2 oneagents. p parameter problem determines radius circleconsidered sub-goal. time agent reaches cell inside area definedsub-goal, initiates exchange information (therefore, p induces communicationstrategy). expresses Manhattan distance two agents, value accurateagents synchronize knowledge. is, time t0 agents determinefirst sub-goal area bounded radius p d0 /2 and, center locatedd0 /2 one agents. time agents synchronize informationcommunication, new sub-goal determined p dt /2. Figure 4 showsnew sub-goals set agents transmit actual location reachedsub-goal area. meeting point dynamically set center sub-goal area.Experiments run Communicate SubGoals case different uncertainty values, values parameter p costs communication (for case, 1000 experimentsrun averaged). results show agents obtain higher utility adjusting meeting point dynamically rather set one fixed meeting point. Agentssynchronize knowledge thus set new meeting location insteadacting two independent MDPs communicate move towards fixed meeting point (see Figure 5). Nevertheless, certain values p, joint utility agents194fiCommunication-Based Decomposition Mechanismp ratio-20.500.10.20.3-20.768-20.852-21-21.038-21.13-21.15-21.1320.40.5-20.808-20.8586 -20.824-21.08880.6-20.7260.7-20.758-21.3665-21.50.9-20.804-20.9302-21.0255-21.0433-21.26-21.2636-21.2840.8-20.742-21.3405-21.4254-21.5715Avg. Joint Utlity-21.7747-21.9355-22-21.9794-22.308-22.414-22.5-22.581-22.771-23Pu=0.8 Cost Com.=0Pu=0.8 Cost Com.=0.1Pu=08 Cost Com.=0.3Pu=0.8 Cost Com.=0.5-23.52MDPs Pu=0.8-23.739-24Figure 5: average joint utility obtained sub-goals communicated.actually smaller joint utility achieved No-Communication case (2 MDPsfigure). points need empirically tune parameters requiredheuristic.Myopic-greedy case, design agents optimize timesend message, assuming communicate once. off-line planningstage, agents compute expected joint cost meet possible statesystem (s0 ) time (included local state si ), c (s0 , si , 1A , 2A ). global statesrevealed communication correspond possible distances agents.time agents get synchronized, mechanism applied assigning local goalsinstructing agents follow optimal local policies achieve them. MeetingUncertainty scenario study, c expected joint cost incurred taking controlactions time steps, communicating time + 1 agents metfar, continuing optimal policy control actions without communicatingtowards goal state (the meeting location agreed upon + 1) expected costnc (d1 , d2 ) computed No-Communication case. agents meettime steps elapsed, incur cost time act met.time t, one agents knows meeting location, goal locationcomputed last exchange information. Consequently, agent moves optimallytowards goal state. addition, myopic-greedy policy communication foundcomputing earliest time t, c (d1 + d2 , s1 , 1A , 2A ) < nc (d1 , d2 ), is,best time communicate expected cost meet least.myopic-greedy policy communication vector states time communicatepossible distance agents.Tables 4, 5, 6 present myopic-greedy communication policies computedMeeting Uncertainty problem Pu values taken {0.2, 0.4, 0.6, 0.8}. costtaking control action Ra = 1.0 costs communicating C tested195fiGoldman & Zilberstein{0.1, 1.0, 10.0}. row corresponds configuration tested different statetransition uncertainties. column corresponds synchronized state, givenpossible Manhattan distance agents moving 2D grid size 10x10. Givencertain value Pu certain global distance, agent interprets valueentry next time communicate position. Time reset zeroagents exchange information. long distance agents largercommunication cost increases, policy instructs agents communicate later, i.e.,agents keep operating information exchanged better effectrescheduling meeting place.Pu0.20.40.60.812222d0 =distance agents last synchronized, g2 3 4 5 6 7 8 9 10 11 12 13 143 2 3 2 3 2 3 2 323232 2 3 2 3 2 3 2 323232 2 3 2 3 2 3 2 323232 2 3 2 4 2 4 2 42424located15 1623232324d0 /2172222183334Table 4: Myopic-greedy policy communication: C = 0.1, Ra = 1.0.Pu0.20.40.60.813222d0 =distance agents last synchronized, g2 3 4 5 6 7 8 9 10 11 12 13 144 3 5 3 6 4 7 4 757583 3 4 4 5 4 6 5 757682 3 4 4 5 5 6 6 768782 3 3 4 4 5 5 6 67788located15 1658687999d0 /2176781018991010Table 5: Myopic-greedy policy communication: C = 1.0, Ra = 1.0.Pu0.20.40.60.81954329643d0 =distance agents last3456789 1011 13 14 17 18 20 21 2378910 11 12 13 1456678991044556778synchronized, g11 12 1325 27 2815 16 1711 12 128910located d0 /214 15 16 1730 32 34 3518 19 20 2113 14 15 1510 11 11 121837221612Table 6: Myopic-greedy policy communication: C = 10.0, Ra = 1.0.smallest cost communication tested, always beneficial communicaterather early, matter uncertainty environment, almost matterd0 is. larger costs communication given Pu , larger distanceagents, later communicate (e.g., Pu = 0.4, C = 1 = 5,agents communicate time 4, C = 10, communicate time9). given C , larger distance agents is, later agentscommunicate (e.g., Pu = 0.4, C = 10 = 5, agents communicatetime 9, = 12, communicate time 16). results averaging1000 runs show given cost C long Pu decreases (the agentuncertain actions outcomes), agents communicate times.196fiCommunication-Based Decomposition Mechanism1000 experiments run, agents exchange information actual locations best time myopically found d0 (known time t0 ).communicate, know actual distance dt , them. agents followmyopic-greedy communication policy find next time communicate meet already. time best time found myopic-greedyalgorithm given distance agents dt . Iteratively, agents approximate optimal solution decentralized control problem direct communicationfollowing independent optimal policies action, myopic-greedy policycommunication. Results obtained averaging global utility attained 1000experiments show myopic-greedy agents perform better agents communicate sub-goals (that shown already efficient communicatingall).Pu0.20.40.60.8No-Comm.-104.925-51.4522-33.4955-24.3202Average Joint UtilityIdeal C = 0 SubGoals5-62.872-64.7399-37.33-38.172-26.444-27.232-20.584-20.852Myopic-Greedy-63.76-37.338-26.666-20.704Table 7: C = 0.10, Ra = 1.0.Myopic-greedy approach attained utilities statistically significantly greaterobtained heuristic case C = 0.1 (see Table 7)6 . C = 0.1Pu = 0.4, Myopic-greedy even attained utilities significantly different (with significancelevel 98%) Ideal.Pu0.20.40.60.8No-Comm.-104.925-51.4522-33.4955-24.3202Average Joint UtilityIdeal C = 0 Comm. SubGoals-62.872-65.906-37.33-39.558-26.444-27.996-20.584-21.05Best p0.30.20.20.1Myopic-greedy-63.84-37.774-27.156-21.3Table 8: C = 1.0 SubGoals Myopic-greedy, Ra = 1.0.Pu0.20.40.60.8No-Comm.-104.925-51.4522-33.4955-24.3202Average Joint UtilityIdeal C = 0 Comm. SubGoals-62.872-69.286-37.33-40.516-26.444-28.192-20.584-21.118Best p0.10.10.10.1Myopic-greedy-68.948-40.594-28.908-22.166Table 9: C = 10.0 SubGoals Myopic-greedy, Ra = 1.0.5. results presented best p, found empirically.6. Statistical significance established t-test.197fiGoldman & ZilbersteinC = 1 (see Table 8) utilities attained Myopic-greedy approachPu < 0.8 significantly greater results obtained heuristic case.Pu = 0.8, heuristic case found better Myopic-greedy bestchoice p (Myopic-greedy obtained -21.3, SubGoals p = 0.1 attained -21.05(variance=2.18)). utilities attained Myopic-greedy agents, C = 10 (seeTable 9) Pu {0.2, 0.4}, significantly different SubGoals casebest p significance levels 61% 82%, respectively. However, heuristic caseyielded smaller costs values Pu = {0.6, 0.8}. One important point noticeresults consider best p found heuristic trying set discretevalues p (see x-axis Figure 5). general trying tuning heuristic parametertime consuming best choice may known ahead timedesigner. hand, Myopic-greedy approach require tuningparameter. settings tested, Myopic-greedy always attain utilities higherattained SubGoals case worst p.Pu0.20.40.60.8Average Communication Acts PerformedNo-Comm. Ideal C = 0 SubGoals Myopic-greedy031.4365.421.096018.665111.962013.42618.323010.29214.579Table 10: C = 0.10, Ra = 1.0.Pu0.20.40.60.8No-Comm.0000Average Communication Acts PerformedIdeal C = 0 Comm. SubGoals Myopic-greedy31.4361.1946.71718.66513.90413.42612.03610.29201.296Table 11: C = 1.0 Myopic-greedy SubGoals, Ra = 1.0.Pu0.20.40.60.8No-Comm.0000Average Communication Acts PerformedIdeal C = 0 Comm. SubGoals Myopic-greedy31.43600.41618.66500.41713.42600.33810.29200.329Table 12: C = 10.0 Myopic-greedy SubGoals, Ra = 1.0.Tables 10, 11 12 present average number communication acts performedone cases.198fiCommunication-Based Decomposition Mechanism[Bernstein et al.2002]NEXP-CNP-C[GZ 2004]Dec-MDPIONP-CGoalorientedPoly-CPoly-C|G|=1[GZ 2004][GZ 2004]GoalorientedInformationSharing|G| > 1InformationSharing[GZ 2004]OptimalMechanismNEXPNEXP-C[GZ 2004][Rabinovich et al.2003]Approx.NEXP-CPLGOMechanismMyopicGreedyPPBackwardInductionFigure 6: complexity solving Dec-MDPs.8. DiscussionSolving optimally decentralized control problems known hard. Figure 6 summarizes complexity results (the rectangles stand optimal solutions parallelograms stand solutions proposed framework communication-based decomposition mechanisms). taxonomy helps us understand characteristics differentclasses decentralized control problems effect complexity problems.Coverage-set (Becker et al., 2004), Opt1Goal OptNGoals (Goldman & Zilberstein,2004a) first algorithms solve optimally non-trivial classes Dec-MDPs.paper presents communication-based decomposition mechanisms way approximate optimal joint solution decentralized control problems. approachbased two key ideas: (1) separating questions communicatecommunications, (2) exploiting full observability global statescommunication generate individual behaviors agents followcommunications. Communication decision makers serves synchronizationpoint local information exchanged order assign individual behaviorcontroller. addresses effectively applications constant communicationdesirable feasible. Many practical reasons could prevent agents communicationconstantly. Communication actions may incur costs reflect complexitytransmitting information, utilization limited bandwidth may sharedapplications, risk revealing information competitive parties operatingenvironment. communication-based decomposition mechanism dividesglobal problem individual behaviors combined communication acts overcomelack global information.199fiGoldman & Zilbersteinformalized communication-based decomposition mechanisms decentralized semiMarkov process communication (Dec-SMDP-Com). proved solving optimally problems temporally abstracted actions equivalent solving optimallymulti-agent MDP (MMDP).We adapted multi-step backup policy iteration algorithmdecentralized case solve Dec-SMDP-Com problem optimally. algorithm produces optimal communication-based decomposition mechanism. providetractable algorithm, restrict set individual behaviors allowed.proposed LGO-MSBPI polynomial-time algorithm computes assignmentpair local goals period time k highest value possible global state.Adding local goals model seems natural intuitive computing localbehaviors based general options. easier state local behavior completedlocal goal reached, rather stating sequences local actions eventually achieve desired global behavior. Furthermore, unrestricted setoptions larger therefore computationally cheaper compute decompositionmechanism local goals assumed. intuition confirmed experiments.general question remain open, namely determine beneficial computelocal behaviors general options rather assuming local goals.paper also presents simpler approximation method. assumes certainmechanism given, i.e., human knowledge incorporated model provide agentsindividual policies actions (not including communication acts). greedy-approachpresented computes best time communicate assuming one opportunity exchanging information. paper concludes empirical assessmentapproaches.summary, paper contributes communication-based decomposition mechanismapplied many hard decentralized control problems shown Figure 6.approach enables us compute tractable individual behaviors agent togetherbeneficial time communicate change local behaviors. analytical results paper support validity approach respect Dec-MDPsindependent transitions observations. However, straightforward applyapproach general Dec-POMDPs Dec-MDPs dependent transitions observations, believe offers viable approximation technique problems well.approach scalable respect number agents since complexityresults presented increase linearly agents added system. Exchangeinformation assumed via broadcast agents. interesting futureextension study agents efficiently choose partners communication avoidglobal broadcasting.Acknowledgmentswork supported part National Science Foundation grants IIS0219606, Air Force Office Scientific Research grants F49620-03-1-0090FA9550-08-1-0181, NASA grant NCC 2-1311. opinions, findings,conclusions recommendations expressed material authorsreflect views NSF, AFOSR NASA.200fiCommunication-Based Decomposition MechanismReferencesBecker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004). Solving transition independent decentralized MDPs. Journal Artificial Intelligence Research, 22, 423455.Bererton, C., Gordon, G., Thrun, S., & Khosla, P. (2003). Auction mechanism designmulti-robot coordination. Proceedings Seventeenth Annual ConferenceNeural Information Processing Systems, Whistler, BC, Canada.Bernstein, D., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity decentralized control Markov decision processes. Mathematics Operations Research,27 (4), 819840.Boutilier, C. (1999). Sequential optimality coordination multiagent systems.Proceedings Sixteenth International Joint Conference Artificial Intelligence,pp. 478485, Stockholm, Sweden.Ghavamzadeh, M., & Mahadevan, S. (2004). Learning act communicate cooperative multiagent systems using hierarchical reinforcement learning. ProceedingsThird International Joint Conference Autonomous Agents Multi-AgentSystems, pp. 11141121, New York City, NY.Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange cooperativemulti-agent systems. Proceedings Second International Joint ConferenceAutonomous Agents Multi-Agent Systems, pp. 137144, Melbourne, Australia.Goldman, C. V., & Zilberstein, S. (2004a). Decentralized control cooperative systems:Categorization complexity analysis. Journal Artificial Intelligence Research,22, 143174.Goldman, C. V., & Zilberstein, S. (2004b). Goal-oriented Dec-MDPs direct communication. Technical Report 0444, University Massachusetts Amherst, ComputerScience Department.Guestrin, C., & Gordon, G. (2002). Distributed planning hierarchical factored MDPs.Proceedings Eighteenth Conference Uncertainty Artificial Intelligence,pp. 197206, Edmonton, Canada.Guestrin, C., Koller, D., & Parr, R. (2001). Multiagent planning factored MDPs.Advances Neural Information Processing Systems, pp. 15231530, Vancouver,British Columbia.Hansen, E. A. (1997). Markov decision processes observation costs. Technical Report97-01, University Massachusetts Amherst, Computer Science Department.Hansen, E. A., & Zhou, R. (2003). Synthesis hierarchical finite-state controllersPOMDPs. Proceedings Thirteenth International Conference AutomatedPlanning Scheduling, Trento, Italy.Nair, R., Tambe, M., Roth, M., & Yokoo, M. (2004). Communication improving policycomputation distributed POMDPs. Proceedings Third International JointConference Autonomous Agents Multi-Agent Systems, pp. 10981105, NewYork City, NY.201fiGoldman & ZilbersteinNair, R., Tambe, M., Yokoo, M., Pynadath, D., & Marsella, S. (2003). Taming decentralizedPOMDPs: Towards efficient policy computation multiagent settings. ProceedingsEighteenth International Joint Conference Artificial Intelligence, pp. 705711, Acapulco, Mexico.Papadimitriou, C. H., & Tsitsiklis, J. (1987). complexity Markov decision processes.Mathematics Operations Research, 12 (3), 441450.Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning cooperate viapolicy search. Proceedings Sixteenth Conference Uncertainty ArtificialIntelligence, pp. 489496, Stanford, CA.Petrik, M., & Zilberstein, S. (2007). Anytime coordination using separable bilinear programs. Proceedings Twenty Second Conference Artificial Intelligence, pp.750755, Vancouver, BC, Canada.Puterman, M. L. (1994). Markov Decision Processes Discrete Stochastic Dynamic Programming. Wiley & Sons, Inc., New York.Pynadath, D. V., & Tambe, M. (2002). communicative multiagent team decisionproblem: Analyzing teamwork theories models. Journal Artificial IntelligenceResearch, 16, 389423.Schneider, J., Wong, W.-K., Moore, A., & Riedmiller, M. (1999). Distributed value functions. Proceedings Sixteenth International Conference Machine Learning,pp. 371378, Bled, Slovenia.Seuken, S., & Zilberstein, S. (2007a). Improved memory-bounded dynamic programmingDec-POMDPs. Proceedings Twenty Third Conference UncertaintyArtificial Intelligence, Vancouver, BC, Canada.Seuken, S., & Zilberstein, S. (2007b). Memory-bounded dynamic programming DecPOMDPs. Proceedings Twentieth International Joint Conference Artificial Intelligence, pp. 20092015, Hyderabad, India.Seuken, S., & Zilberstein, S. (2008). Formal models algorithms decentralized decisionmaking uncertainty. Autonomous Agents Multi-agent Systems. appear.Sutton, R. S., Precup, D., & Singh, S. (1999). MDPs semi-MDPs: frameworktemporal abstraction reinforcement learning. Artificial Intelligence, 112, 181211.Suzuki, I., & Yamashita, M. (1999). Distributed anonymous mobile robots: Formationgeometric patterns. SIAM Journal Computing, 28 (4), 13471363.Wolpert, D. H., Wheeler, K. R., & Tumer, K. (1999). General principles learning-basedmulti-agent systems. Proceedings Third International Conference Autonomous Agents, pp. 7783, Seattle, Washington.Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions multi-agent cooperation: Model experiments. Proceedings Fifth International ConferenceAutonomous Agents, pp. 616623, Montreal, Canada.202fiJournal Artificial Intelligence Research 32 (2008) 453486Submitted 02/08; published 06/08Adaptive Stochastic Resource Control:Machine Learning ApproachBalazs Csanad Csajibalazs.csaji@sztaki.huComputer Automation Research Institute,Hungarian Academy SciencesKende utca 1317, Budapest, H1111, HungaryLaszlo Monostorilaszlo.monostori@sztaki.huComputer Automation Research Institute,Hungarian Academy Sciences;Faculty Mechanical Engineering,Budapest University Technology EconomicsAbstractpaper investigates stochastic resource allocation problems scarce, reusableresources non-preemtive, time-dependent, interconnected tasks. approachnatural generalization several standard resource management problems, scheduling transportation problems. First, reactive solutions considered definedcontrol policies suitably reformulated Markov decision processes (MDPs). arguereformulation several favorable properties, finite state actionspaces, aperiodic, hence policies proper space control policiessafely restricted. Next, approximate dynamic programming (ADP) methods, fittedQ-learning, suggested computing efficient control policy. order compactlymaintain cost-to-go function, two representations studied: hash tables supportvector regression (SVR), particularly, -SVRs. Several additional improvements,application limited-lookahead rollout algorithms initial phases, action spacedecomposition, task clustering distributed sampling investigated, too. Finally,experimental results benchmark industry-related data presented.1. IntroductionResource allocation problems (RAPs) high practical importance, since arisemany diverse fields, manufacturing production control (e.g., production scheduling),warehousing (e.g., storage allocation), fleet management (e.g., freight transportation), personnel management (e.g., office), scheduling computer programs (e.g., massivelyparallel GRID systems), managing construction project controlling cellular mobilenetwork. RAPs also central management science (Powell & Van Roy, 2004).paper consider optimization problems include assignment finite setreusable resources non-preemtive, interconnected tasks stochastic durationseffects. objective investigate efficient reactive (closed-loop) decision-makingprocesses deal allocation scarce resources time goaloptimizing objectives. real world applications, important solutionable deal large-scale problems handle environmental changes, well.c2008AI Access Foundation. rights reserved.fiCsaji & Monostori1.1 Industrial MotivationsOne main motivations investigating RAPs enhance manufacturing productioncontrol. Regarding contemporary manufacturing systems, difficulties arise unexpectedtasks events, non-linearities, multitude interactions attempting controlvarious activities dynamic shop floors. Complexity uncertainty seriously limiteffectiveness conventional production control approaches (e.g., deterministic scheduling).paper apply mathematical programming machine learning (ML) techniques achieve suboptimal control general class stochastic RAPs,vital intelligent manufacturing system (IMS). term IMS attributedtentative forecast Hatvany Nemes (1978). early 80s IMSs outlinednext generation manufacturing systems utilize results artificial intelligenceresearch expected solve, within certain limits, unprecedented, unforeseen problems basis even incomplete imprecise information. Naturally, applicabilitydifferent solutions RAPs limited industrial problems.1.2 Curse(s) DimensionalityDifferent kinds RAPs huge number exact approximate solution methods,example, case scheduling problems (Pinedo, 2002). However, methodsprimarily deal static (and often strictly deterministic) variants various problems and, mostly, aware uncertainties changes. Special (deterministic)RAPs appear field combinatorial optimization, traveling salesman problem (TSP) job-shop scheduling problem (JSP), strongly NP-hard and,moreover, good polynomial-time approximation, either.stochastic case RAPs often formulated Markov decision processes(MDPs) applying dynamic programming (DP) methods, theory,solved optimally. However, due phenomenon named curse dimensionalityBellman (1961), methods highly intractable practice. curse referscombinatorial explosion required computation size problem increases. authors, e.g., Powell Van Roy (2004), talk even three typescurses concerning DP algorithms. motivated approximate approaches requiretractable computation, often yield suboptimal solutions (Bertsekas, 2005).1.3 Related Literaturebeyond scope give general overview different solutions RAPs, hence,concentrate part literature closely related approach.solution belongs class approximate dynamic programming (ADP) algorithmsconstitute broad class discrete-time control techniques. Note ADP methodstake actor-critic point view often called reinforcement learning (RL).Zhang Dietterich (1995) first apply RL technique special RAP.used D() method iterative repair solve static scheduling problem,namely, NASA space shuttle payload processing problem. Since then, numberpapers published suggested using RL different RAPs. first reactive(closed-loop) solution scheduling problems using ADP algorithms briefly described454fiAdaptive Stochastic Resource ControlSchneider, Boyan, Moore (1998). Riedmiller Riedmiller (1999) used multilayer perceptron (MLP) based neural RL approach learn local heuristics. AydinOztemel (2000) applied modified version Q-learning learn dispatching rules production scheduling. Multi-agent versions ADP techniques solving dynamic schedulingproblems also suggested (Csaji, Kadar, & Monostori, 2003; Csaji & Monostori, 2006).Powell Van Roy (2004) presented formal framework RAPs appliedADP give general solution problem. Later, parallelized solutionpreviously defined problem given Topaloglu Powell (2005). RAP framework, presented Section 2, differs approaches, since system goalaccomplish set tasks widely different stochastic durationsprecedence constraints them, Powell Van Roys (2004) approach concerns satisfying many similar demands arriving stochastically time demandsunit durations precedence constraints. Recently, support vector machines(SVMs) applied Gersmann Hammer (2005) improve iterative repair (localsearch) strategies resource constrained project scheduling problems (RCPSPs). agentbased resource allocation system MDP-induced preferences presented DolgovDurfee (2006). Finally, Beck Wilson (2007) gave proactive solutions job-shopscheduling problems based combination Monte Carlo simulation, solutionsassociated deterministic problem, either constraint programming tabu-search.1.4 Main Contributionssummary main contributions paper, highlighted that:1. propose formal framework investigating stochastic resource allocation problems scarce, reusable resources non-preemtive, time-dependent, interconnected tasks. approach constitutes natural generalization several standardresource management problems, scheduling problems, transportation problems, inventory management problems maintenance repair problems.general RAP reformulated stochastic shortest path problem (a specialMDP) favorable properties, as, aperiodic, state action spacesfinite, policies proper space control policies safely restricted. Reactive solutions defined policies reformulated problem.2. order compute good approximation optimal control policy, ADP methods suggested, particularly, fitted Q-learning. Regarding value function representations ADP, two approaches studied: hash tables SVRs. latter,samples regression generated Monte Carlo simulationcases inputs suitably defined numerical feature vectors.Several improvements speed calculation ADP-based solution suggested: application limited lookahead rollout algorithms initial phasesguide exploration provide first samples approximator; decomposing action space decrease number available actions states;clustering tasks reduce length trajectories variancecumulative costs; well two methods distribute proposed algorithm amongseveral processors either shared distributed memory architecture.455fiCsaji & Monostori3. paper also presents several results numerical experiments benchmarkindustry-related problems. First, performance algorithm measuredhard benchmark flexible job-shop scheduling datasets. scaling propertiesapproach demonstrated experiments simulated factory producingmass-products. effects clustering depending size clustersspeedup relative number processors case distributed samplingstudied, well. Finally, results adaptive features algorithm casedisturbances, resource breakdowns new task arrivals, also shown.2. Markovian Resource Controlsection aims precisely defining RAPs reformulating way wouldallow effectively solved ML methods presented Section 3. First, briefintroduction RAPs given followed formulation general resource allocationframework. start deterministic variants extend definitionstochastic case. Afterwards, give short overview Markov decision processes (MDPs),constitute fundamental theory approach. Next, reformulate reactivecontrol problem RAPs stochastic shortest path (SSP) problem (a special MDP).2.1 Classical Problemssection give brief introduction RAPs two strongly NP-hard combinatorial optimization problems: job-shop scheduling problem traveling salesmanproblem. Later, apply two basic problems demonstrate results.2.1.1 Job-Shop SchedulingFirst, consider classical job-shop scheduling problem (JSP) standard deterministic RAP (Pinedo, 2002). set jobs, J = {J1 , . . . , Jn }, processedset machines, = {M1 , . . . , Mk }. j J consists sequence njtasks, task tji , {1, . . . , nj }, machine mjiprocess task, processing time pji N. aim optimization findfeasible schedule minimizes given performance measure. solution, i.e., schedule,suitable task starting time assignment. Figure 1 visualizes example scheduleusing Gantt chart. Note Gantt chart (Pinedo, 2002) figure using bars,order illustrate starting finishing times tasks resources.concept feasibility defined Section 2.2.1. case JSP feasibleschedule associated ordering tasks, i.e., orderexecuted machines. many types performance measures availableJSP, probably commonly applied one maximum completion timetasks, also called makespan. case applying makespan, JSP interpretedproblem finding schedule completes tasks every job soon possible.Later, study extension JSP, called flexible job-shop scheduling problem(FJSP) arises machines interchangeable, i.e., may tasksexecuted several machines. case processing times givenpartial function, p : , N. Note partial function binary relation456fiAdaptive Stochastic Resource Controlassociates elements domain set one element range set.Throughout paper use , denote partial function type binary relations.Figure 1: possible solution JSP, presented Gantt chart. Taskscolor belong job processed given order.vertical gray dotted line indicates maximum completion time tasks.2.1.2 Traveling SalesmanOne basic logistic problems traveling salesman problem (TSP)stated follows. Given number cities costs travelings them,least-cost round-trip route visits city exactly returnsstarting city. Several variants TSP known, standard oneformally characterized connected, undirected, edge-weighted graph G = hV, E, wi,V = {1, . . . , n} vertex set corresponding set cities,E V Vset edges represents roads cities, w : E N definesweights edges: durations trips. aim optimization findHamilton-circuit smallest possible weight. Note Hamilton-circuit graphcycle starts vertex, passes every vertex exactly once, returnsstarting vertex. Take look Figure 2 example Hamilton-circuit.Figure 2: possible solution TSP, closed path graph. black edges constituteHamilton-circuit given connected, undirected, edge-weighted graph.457fiCsaji & Monostori2.2 Deterministic FrameworkNow, present general framework model resource allocation problems. framework treated generalization several classical RAPs, JSP TSP.First, deterministic resource allocation problem considered: instanceproblem characterized 8-tuple hR, S, O, , C, d, e, ii. details problemconsists set reusable resources R together corresponds setpossible resource states. set allowed operations also given subsetdenotes target operations tasks. R, supposed finitepairwise disjoint. precedence constrains tasks,represented partial ordering C . durations operations dependingstate executing resource defined partial function : , N,N set natural numbers, thus, discrete-time model. Every operationaffect state executing resource, well, described e : ,also partial function. assumed dom(d) = dom(e), dom() denotesdomain set function. Finally, initial states resources given : R S.state resource contain relevant information it, example, typecurrent setup (scheduling problems), location load (transportation problems)condition (maintenance repair problems). Similarly, operation affect statemany ways, e.g., change setup resource, location condition.system must allocate task (target operation) resource, however, maycases first state resource must modified order able executecertain task (e.g., transporter may need, first, travel loading/source point,machine may require repair setup). cases non-task operations may applied.modify states resources without directly serving demand (executingtask). possible resource allocation process non-task operationapplied several times, non-task operations completely avoided (for example,high cost). Nevertheless, finally, tasks must completed.2.2.1 Feasible Resource Allocationsolution deterministic RAP partial function, resource allocator function,% : R N , assigns starting times operations resources. Noteoperations supposed non-preemptive (they may interrupted).solution called feasible following four properties satisfied:1. tasks associated exactly one (resource, time point) pair:v : ! hr, ti dom(%) : v = %(r, t).2. resource executes, most, one operation time:u, v : u = %(r, t1 ) v = %(r, t2 ) t1 t2 < t1 + d(s(r, t1 ), u).3. precedence constraints tasks satisfied:hu, vi C : [u = %(r1 , t1 ) v = %(r2 , t2 )] [t1 + d(s(r1 , t1 ), u) t2 ] .4. Every operation-to-resource assignment valid:hr, ti dom(%) : hs(r, t), %(r, t)i dom(d),458fiAdaptive Stochastic Resource Control: R N describes statesi(r)s(r, 1)s(r, t) =e(s(r, 1), %(r, t))resources given times= 0hr, ti/ dom(%)otherwiseRAP called correctly specified exists least one feasible solution.follows assumed problems correctly specified. Take look Figure 3.Figure 3: Feasibility illustration four forbidden properties, using JSPexample. presented four cases excluded set feasible schedules.2.2.2 Performance Measuresset feasible solutions denoted S. performance (or cost) associatedsolution, defined performance measure : R often dependstask completion times, only. Typical performance measures appear practiceinclude: maximum completion time mean flow time. aim resource allocatorsystem compute feasible solution maximal performance (or minimal cost).Note performance measure assign penalties violating release duedates (if available) even reflect priority tasks. possible generalization given problem case operations may require resources simultaneously, important model, e.g., resource constrained project scheduling problems. However, straightforward extend framework case: definitione changed : hki N e : hki hki , hki = ki=1k |R|. Naturally, assume hs, oi dom(e) : dim(e(s, o)) = dim(s).Although, managing tasks multiple resource requirements may importantcases, keep analysis simple possible, deal paper.Nevertheless, results easily generalized case, well.459fiCsaji & Monostori2.2.3 Demonstrative ExamplesNow, demonstrative examples, reformulate (F)JSP TSP given framework.straightforward formulate scheduling problems, JSP, presentedresource allocation framework: tasks JSP directly associated tasksframework, machines associated resources processing times durations. precedence constraints determined linear ordering tasksjob. Note one possible resource state every machine. Finally, feasibleschedules associated feasible solutions. setup-times problem, well, would several states resource (according currentsetup) set-up procedures could associated non-task operations.RAP formulation TSP given follows. set resources consistsone element, namely salesman, therefore, R = {r}. possible statesresource r (the salesman) = {s1 , . . . , sn }. state (of r) si , indicatessalesman city i. allowed operations allowed tasks,= = {t1 , . . . , tn }, execution task ti symbolizes salesman travelscity current location. constraints C = {ht2 , t1 , ht3 , t1 . . . , htn , t1 i} usedforcing system end whole round-tour city 1, also starting city,thus, i(r) = s1 . si tj : hsi , tj dom(d) hi, ji E.hsi , tj dom(d) : d(si , tj ) = wij e(si , tj ) = sj . Note dom(e) = dom(d) firstfeasibility requirement guarantees city visited exactly once. performancemeasure latest arrival time, (%) = max {t + d(s(r, t), %(r, t)) | hr, ti dom(%)}.2.2.4 Computational Complexityuse performance measure property solution preciselydefined bounded sequence operations (which includes tasks) assignmentresources and, additionally, among solutions generated way optimalone found, RAP becomes combinatorial optimization problem.performance measure monotone completion times, called regular, property.defined RAP generalization of, e.g., JSP TSP, stronglyNP-hard and, furthermore, good polynomial-time approximation optimal resourceallocating algorithm exits, either (Papadimitriou, 1994).2.3 Stochastic Frameworkfar model deterministic, turn stochastic RAPs. stochasticvariant described general class RAPs defined randomizing functions d,e i. Consequently, operation durations become random, : (N),(N) space probability distributions N. Also effects operationsuncertain, e : (S) initial states resources stochastic,well, : R (S). Note ranges functions d, e contain probabilitydistributions, denote corresponding random variables D, E I, respectively.notation X f indicate random variable X probability distribution f . Thus,D(s, o) d(s, o), E(s, o) e(s, o) I(r) i(r) S, r R. Takelook Figure 4 illustration stochastic variants JSP TSP problems.460fiAdaptive Stochastic Resource Control2.3.1 Stochastic Dominancestochastic RAPs performance solution also random variable. Therefore,order compare performance different solutions, compare randomvariables. Many ways known make comparison. may say, example,random variable stochastic dominance another random variable almostsurely, likelihood ratio sense, stochastically, increasing convex senseexpectation. different applications different types comparisons may suitable,however, probably natural one based upon expected values randomvariables. paper applies kind comparison stochastic RAPs.Figure 4: Randomization case JSP (left) case TSP (right). latter,initial state, durations arrival vertex could uncertain, well.2.3.2 Solution ClassificationNow, classify basic types resource allocation techniques. First, order giveproper classification begin recalling concepts open-loop closed-loopcontrollers. open-loop controller, also called non-feedback controller, computesinput system using current state model system. Therefore,open-loop controller use feedback determine input achieveddesired goal, observe output process controlled. Conversely,closed-loop controller uses feedback control states outputs dynamical system(Sontag, 1998). Closed-loop control significant advantage open-loop solutionsdealing uncertainties. Hence, improved reference tracking performance,stabilize unstable processes reduced sensitivity parameter variations.deterministic RAPs significant difference open- closed-loopcontrols. case safely restrict open-loop methods. solutionaimed generating resource allocation off-line advance, called predictive.Thus, predictive solutions perform open-loop control assume deterministic environment. stochastic resource allocation data (e.g., actual durations)available execution plan. Based usageinformation, identify two basic types solution techniques. open-loop solutiondeal uncertainties environment called proactive. proactive solutionallocates operations resources defines orders operations, but,durations uncertain, determine precise starting times. kind461fiCsaji & Monostoritechnique applied durations operations stochastic, but,states resources known perfectly (e.g., stochastic JSP). Finally, stochasticcase closed-loop solutions called reactive. reactive solution allowed makedecisions on-line, process actually evolves providing information. Naturally,reactive solution simple sequence, rather resource allocation policy (todefined later) controls process. paper focuses reactive solutions, only.formulate reactive solution stochastic RAP control policy suitablydefined Markov decision process (specially, stochastic shortest path problem).2.4 Markov Decision ProcessesSequential decision-making presence uncertainties often modeled MDPs(Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998; Feinberg & Shwartz, 2002).section contains basic definitions, notations applied preliminaries.(finite, discrete-time, stationary, fully observable) Markov decision process (MDP)mean stochastic system characterized 6-tuple hX, A, A, p, g, i, components follows: X finite set discrete states finite set control actions.Mapping : X P(A) availability function renders state set actionsavailable state P denotes power set. transition-probability functiongiven p : X (X), (X) space probability distributions X.Let p(y | x, a) denote probability arrival state executing action A(x)state x. immediate-cost function defined g : X R, g(x, a)cost taking action state x. Finally, constant [0, 1] denotes discount ratediscount factor. = 1, MDP called undiscounted, otherwise discounted.possible extend theory general state action spaces,expense increased mathematical complexity. Finite state action sets mostlysufficient digitally implemented controls and, therefore, restrict case.Figure 5: Markov decision processes interaction decision-maker uncertain environment (left); temporal progress system (right).interpretation MDP given consider agent actsuncertain environment, viewpoint often taken RL. agent receives informationstate environment, x, state x agent allowed chooseaction A(x). action selected, environment moves next stateaccording probability distribution p(x, a), decision-maker collects one462fiAdaptive Stochastic Resource Controlstep cost, g(x, a), illustrated Figure 5. aim agent find optimalbehavior (policy) applying strategy minimizes expected cumulative costs.stochastic shortest path (SSP) problem special MDP aim findcontrol policy reaches pre-defined terminal state starting given initialstate, additionally, minimizes expected total costs path, well. policycalled proper reaches terminal state probability one. usual assumptiondealing SSP problems policies proper, abbreviated APP.2.4.1 Control Policies(stationary, Markov) control policy determines action take possible state.deterministic policy, : X A, simply function states control actions.randomized policy, : X (A), function states probability distributionsactions. denote probability executing action state x (x)(a) or,short, (x, a). Naturally, deterministic policies special cases randomized ones and,therefore, unless indicated otherwise, consider randomized control policies.xe0 (X) initial probability distribution states, transition probabilities p together control policy completely determine progress systemstochastic sense, namely, define homogeneous Markov chain X,xet+1 = P ()ext ,xet state probability distribution vector system time t, P ()denotes probability transition matrix induced control policy , formally definedX[P ()]x,y =p(y | x, a) (x, a).aA2.4.2 Value Functionsvalue cost-to-go function policy function states costs. definedstate: J : X R. Function J (x) gives expected value cumulative(discounted) costs system state x follows policy thereafter,J (x) = E"NXt=0#fifig(Xt , ) fifi X0 = x ,(1)Xt random variables, selected according control policydistribution Xt+1 p(Xt , ). horizon problem denoted N N {}.Unless indicated otherwise, always assume horizon infinite, N = .Similarly definition J , one define action-value functions control polices," N#fiXfifiQ (x, a) = Eg(Xt , ) fi X0 = x, A0 = ,t=0notations equation (1). Action-value functions especiallyimportant model-free approaches, classical Q-learning algorithm.463fiCsaji & Monostori2.4.3 Bellman Equationssay 1 2 if, x X, J 1 (x) J 2 (x). control policy(uniformly) optimal less equal control policies.always exists least one optimal policy (Sutton & Barto, 1998). Althoughmay many optimal policies, share unique optimal cost-to-go function,denoted J . function must satisfy Bellman optimality equation (Bertsekas &Tsitsiklis, 1996), J = J , Bellman operator, defined x X,(T J)(x) = minaA(x)hg(x, a) +XyXp(y | x, a)J(y) .(2)Bellman equation arbitrary (stationary, Markov, randomized) policy(T J)(x) =XhX(x, a) g(x, a) +p(y | x, a)J(y) ,yXaA(x)notations equation (2) also J = J .given value function J, straightforward get policy, e.g., applyinggreedy deterministic policy (w.r.t. J) always selects actions minimal costs,hXp(y | x, a)J(y) .(x) arg min g(x, a) +aA(x)yXMDPs extensively studied theory exist lot exact approximate solution methods, e.g., value iteration, policy iteration, Gauss-Seidel method,Q-learning, Q(), SARSA TD() - temporal difference learning (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998; Feinberg & Shwartz, 2002). reinforcementlearning algorithms work iteratively approximating optimal value function.2.5 Reactive Resource Controlsection formulate reactive solutions stochastic RAPs control policiessuitably reformulated SSP problems. current task durations resource statesincrementally available resource allocation control process.2.5.1 Problem Reformulationstate x X defined 4-tuple x = h, , %, i, N current timefunction : R determines current states resources. partial functions %store past process, namely, % : RN 1 , contains resourcestimes operation started : R N 1 , N describes finish timesalready completed operations, N = {0, . . . , }. Naturally, dom() dom(%).TS (x) denote set tasks started state x (beforecurrent time ) TF (x) TS (x) set tasks finished alreadystate x. easy see TS (x) = rng(%) TF (x) = rng(%|dom() ) ,rng() denotes range image set function. process starts initial state464fiAdaptive Stochastic Resource Controlxs = h0, , , i, corresponds situation time zero none operationsstarted. initial probability distribution, , calculated follows(xs ) = P ((r1 ) = I(r1 ), . . . , (rn ) = I(rn )) ,I(r) i(r) denotes random variable determines initial state resourcer R n = |R|. Thus, renders initial states resources according (multivariate)probability distribution component RAP. introduce set terminalstates, well. state x considered terminal state (x T) two cases. First,tasks finished state, formally, TF (x) = reached statex, TF (x) 6= . Second, system reached state tasks operationsexecuted, words, allowed set actions empty, A(x) = .easy see that, theory, aggregate terminal states global uniqueterminal state introduce new unique initial state, x0 , one availableaction takes us randomly (with distribution) real initial states. Then,problem becomes stochastic shortest path problem aim described findingrouting minimal expected cost new initial state goal state.every time system informed finished operations, decideoperations apply (and resources). control action space containsoperation-resource assignments avr A, v r R, special awaitcontrol corresponds action system start new operationcurrent time. non-terminal state x = h, , %, available actionsawait A(x) TS (x) \ TF (x) 6=v : r R : avr A(x) (v \ TS (x) hr, ti dom(%) \ dom() : r 6= rh(r), vi dom(d) v (u : hu, vi C u TF (x)))Thus, action await available every state unfinished operation; action avravailable states resource r idle, process operation v, additionally, vtask, executed earlier precedence constraints satisfied.action avr A(x) executed state x = h, , %, i, system movesprobability one new state x = h, , %, i, % = % {hhr, ti , vi}. Notetreat functions sets ordered pairs. resulting x corresponds stateoperation v started resource r previous state environment x.effect await action x = h, , %, takes x = h + 1, , %, i,unfinished operation %(r, t) started r finishes probabilityP(hr, ti dom() | x, hr, ti dom(%) \ dom()) =P(D((r), %(r, t)) + = ),P(D((r), %(r, t)) + )D(s, v) d(s, v) random variable determines duration operation vexecuted resource state s. quantity called completion ratestochastic scheduling theory hazard rate reliability theory. remarkoperations continuous durations, quantity defined f (t)/(1 F (t)), fdenotes density function F distribution random variable determinesduration operation. operation v = %(r, t) finished (hr, ti dom()),465fiCsaji & Monostori(r, t) = (r) = E(r, v), E(r, v) e(r, v) random variable determinesnew state resource r executed operation v. Except extensiondomain set, values function change, consequently, hr, ti dom() :(r, t) = (r, t). words, conservative extension , formally, .immediate-cost function g given performance measure defined follows.Assume depends operation-resource assignments completiontimes. Let x = h, , %, x = h , , %, i. Then, general, system arrivesstate x executing control action state x, incurs cost (%, ) (%, ).Note that, though, Section 2.2.2 performance measures defined completesolutions, measures applied practice, total completion time weightedtotal lateness, straightforward generalize performance measure partial solutions, well. One may, example, treat partial solution problem completesolution smaller (sub)problem, namely, problem fewer tasks completed.control process failed, precisely, possible finish tasks,immediate-cost function render penalties (depending specific problem) regarding non-completed tasks proportional number failed tasks.2.5.2 Favorable FeaturesLet us call introduced SSPs, describe stochastic RAPs, RAP-MDPs.section overview basic properties RAP-MDPs. First, straightforward seeMDPs finite action spaces, since |A| |R| |O| + 1 always holds.Though, state space RAP-MDP denumerable general, allowed number non-task operations bounded random variables describing operationdurations finite, state space reformulated MDP becomes finite, well.may also observe RAP-MDPs acyclic (or aperiodic), viz., none statesappear multiple times, resource allocation process dom(%)non-decreasing and, additionally, time state changes, quantity + |dom(%)|strictly increases. Therefore, system cannot reach state twice. immediateconsequence, policies eventually terminate (if MDP finite) and, thus, proper.effective computation good control policy, important try reducenumber states. recognizing performance measurenon-decreasing completion times, optimal control policy reformulatedRAP-MDP found among policies start new operations timesanother operation finished initial state. statement supportedfact without increasing cost ( non-decreasing) every operationshifted earlier resource assigned reaches another operation,reaches time one preceding tasks finished (if operationtask precedence constrains), or, ultimately, time zero. Noteperformance measures used practice (e.g., makespan, weighted completion time, averagetardiness) non-decreasing. consequence, states operationfinished omitted, except initial states. Therefore, await action may leadstate operation finished. may consider it, system executesautomatically await action omitted states. way, state spacedecreased and, therefore, good control policy calculated effectively.466fiAdaptive Stochastic Resource Control2.5.3 Composable Measureslarge class performance measures, state representation simplifiedleaving past process. order so, must require performancemeasure composable suitable function. general, call function f : P(X)R -composable A, B X, AB = holds (f (A), f (B)) = f (AB),: R R R called composition function, X arbitrary set. definitiondirectly applied performance measures. performance measure, example,-composable, indicates value complete solution computedvalues disjoint subsolutions (solutions subproblems) function . practicalsituations composition function often max, min + function.performance measure -composable, past omittedstate representation, performance calculated incrementally. Thus,state described x = h , , , TU i, N, previously, currenttime, R contains performance current (partial) solution TU setunfinished tasks. function : R (O {}) N determines current statesresources together operations currently executed (or resourceidle) starting times operations (needed compute completion rates).order keep analysis simple possible, restrict composablefunctions, since almost performance measures appear practice -composablesuitable (e.g., makespan total production time max-composable).2.5.4 Reactive SolutionsNow, position define concept reactive solutions stochastic RAPs.reactive solution (stationary, Markov) control policy reformulated SSP problem.reactive solution performs closed-loop control, since time step controllerinformed current state system choose control action based uponinformation. Section 3 deals computation effective control policies.3. Solution Methodssection aim giving effective solution large-scale RAPs uncertainchanging environments help different machine learning approaches. First,overview approximate dynamic programming methods compute good policy.Afterwards, investigate two function approximation techniques enhance solution.Clustering, rollout algorithm action space decomposition well distributed sampling also considered, speedup computation good control policyconsiderably and, therefore, important additions face large-scale problems.3.1 Approximate Dynamic Programmingprevious sections formulated RAPs acyclic (aperiodic) SSP problems.Now, face challenge finding good policy. theory, optimal value functionfinite MDP exactly computed dynamic programming (DP) methods, valueiteration Gauss-Seidel method. Alternatively, exact optimal policy directlycalculated policy iteration. However, due curse dimensionality, computing467fiCsaji & Monostoriexact optimal solution methods practically infeasible, e.g., typicallyrequired amount computation needed storage space, viz., memory, growsquickly size problem. order handle curse, applyapproximate dynamic programming (ADP) techniques achieve good approximationoptimal policy. Here, suggest using sampling-based fitted Q-learning (FQL).trial Monte-Carlo estimate value function computed projected onto suitablefunction space. methods described section (FQL, MCMC Boltzmannformula) applied simultaneously, order achieve efficient solution.3.1.1 Fitted Q-learningWatkins Q-learning popular off-policy model-free reinforcement learning algorithm (Even-Dar & Mansour, 2003). works action-value functions iterativelyapproximates optimal value function. one-step Q-learning rule defined followsQi+1 (x, a) = (1 (x, a))Qi (x, a) + (x, a)(Tei Qi )(x, a),(Tei Qi )(x, a) = g(x, a) + min Qi (Y, B),BA(Y )(x, a) learning rates random variable representing state generated pair (x, a) simulation, is, according probability distributionp(x, a). known (Bertsekas & Tsitsiklis, 1996) (x) [0, 1] satisfyX(x, a) =Xi2 (x, a) < ,i=0i=0Q-learning algorithm converges probability one optimal action-valuefunction, Q , case lookup table representation state-action valuestored independently. speak method fitted Q-learning (FQL)value function represented (typically parametric) function suitable functionspace, F, iteration, updated value function projected back onto F.useful observation need learning rate parameters overcomeeffects random disturbances. However, deal deterministic problems,part method simplified. resulting algorithm simply updates Q(x, a)minimum previously stored estimation current outcome simulation,also core idea LRTA* algorithm (Bulitko & Lee, 2006). dealtdeterministic resource allocation problems, applied simplification, well.3.1.2 Evaluation SimulationNaturally, large-scale problems cannot update states once. Therefore, perform Markov chain Monte Carlo (MCMC) simulations (Hastings, 1970; Andrieu, Freitas,Doucet, & Jordan, 2003) generate samples model, used computing new approximation estimated cost-to-go function. Thus, set statesupdated episode i, namely Xi , generated simulation. RAP-MDPsacyclic, apply prioritized sweeping, means iteration, cost-to-goestimations updated reverse order appeared simulation.468fiAdaptive Stochastic Resource ControlAssume, example, Xi = xi1 , xi2 , . . . , xiti set states updatevalue function iteration i, j < k implies xij appeared earliersimulation xik . case order updates performed, xiti , . . . , xi1 .Moreover, need uniformly optimal value function, enough goodapproximation optimal cost-to-go function relevant states. state calledrelevant appear positive probability application optimal policy. Therefore, sufficient consider case xi1 = x0 , xi1 first stateepisode x0 (aggregated) initial state SSP problem.3.1.3 Boltzmann Formulaorder ensure convergence FQL algorithm, one must guaranteecost-to-go estimation continuously updated. technique used often balanceexploration exploitation Boltzmann formula (also called softmin action selection):(x, a) =exp(Qi (x, a)/ ),Pexp(Qi (x, b)/ )bA(x)0 Boltzmann (or Gibbs) temperature, episode number. easysee high temperatures cause actions (nearly) equiprobable, low ones causegreater difference selection probability actions differ value estimations.Note applied Boltzmann formula minimization, viz., small values resulthigh probability. advised extend approach variant simulated annealing(Kirkpatrick, Gelatt, & Vecchi, 1983) Metropolis algorithm (Metropolis, Rosenbluth,Rosenbluth, Teller, & Teller, 1953), means decreased time,suitable, e.g., logarithmic, rate (Singh, Jaakkola, Littman, & Szepesvari, 2000).3.2 Cost-to-Go RepresentationsSection 3.1 suggested FQL iteratively approximating optimal value function.However, question suitable function space, onto resulted value functionseffectively projected, remained open. order deal large-scale problems (orproblems continuous state spaces) question crucial. section, first,suggest features stochastic RAPs, describe two methods appliedcompactly represent value functions. first simpler one applies hash tablessecond, sophisticated one, builds upon theory support vector machines.3.2.1 Feature Vectorsorder efficiently apply function approximator, first, states actionsreformulated MDP associated numerical vectors representing, e.g., typicalfeatures system. case stochastic RAPs, suggest using features follows:resource R, resource state id, operation id operationcurrently processed resource (could idle), well starting timelast (and currently unfinished) operation feature. model availablesystem, expected ready time resource stored instead.469fiCsaji & Monostoritask , task state id could treated feature assumeone following values: available (e.g., precedence constraintssatisfied), ready execution, processed finished. also advisedapply 1-out-of-n coding, viz., value associated separate bit.case use action-value functions, action (resource-operation assignment)resource id operation id could stored. model available,expected finish time operation also taken account.case model-free approach applies action-value functions, example,feature vector would 3|R|+|T |+2 components. Note features representingtemporal values, advised use relative time values instead absolute ones.3.2.2 Hash TablesSuppose vector w = hw1 , w2 , . . . , wk i, component wi correspondsfeature state action. Usually, value estimations vectorscannot stored memory. case one simplest methods appliedrepresent estimations hash table. hash table is, basically, dictionarykeys mapped array positions hash functions. components assume finitevalues, e.g., finite-state, discrete-time case, key could generated follows.Let us suppose wi 0 wi < mi , w seen numbermixed radix numeral system and, therefore, unique key calculated(w) =kXwii=1i1mj ,j=1(w) denotes key w, value empty product treated one.hash function, , maps feature vector keys memory positions. precisely,memory storing value estimations, hash function takesform : rng() {0, . . . , 1}, rng() denotes range set function.advised apply prime. case usual hashing function choice(x) = x (mod d), namely, congruent x modulo d.keys one item map position called collision.case RAP-MDPs suggest collision resolution method follows. Supposevalue update feature vector state (or state-action pair) maps positionalready occupied another estimation corresponding another item (whichdetected, e.g., storing keys). collision estimation newitem overwrite old estimation MDP state correspondingnew item appears higher probability execution starting (aggregated)initial state one corresponding old item. case model-free approach,item state smaller current time component kept.Despite simplicity, hash table representation several disadvantages, e.g.,still needs lot memory work efficiently, cannot easily handle continuous valuesand, stores individual data, moreover, generalize similar items.next section present statistical approach deal issues, well.470fiAdaptive Stochastic Resource Control3.2.3 Support Vector Regressionpromising choice compactly representing cost-to-go function use support vectorregression (SVR) statistical learning theory. maintaining value function estimations, suggest using -SVRs proposed Scholkopf, Smola, Williamson,Bartlett (2000). advantage classical -SVRs according which,new parameter , number support vectors controlled. Additionally, parameter eliminated. First, core ideas -SVRs presented.general, SVR addresses problem follows. given sample, set datapoints {hx1 , y1 , . . . , hxl , yl i}, xi X input, X measurable space,yi R target output. simplicity, shall assume X Rk , k N.aim learning process find function f : X R small riskZl(f, x, y)dP (x, y),(3)R[f ] =XP probability measure, responsible generation observationsl loss function, l(f, x, y) = (f (x) y)2 . common error function usedSVRs so-called -insensitive loss function, |f (x) y| = max {0, |f (x) y| }.Unfortunately, cannot minimize (3) directly, since know P , givensample, (generated, e.g., simulation). try obtain small risk minimizingregularized risk functional average training sample1kwk2 + C Remp[f ],2(4)where, kwk2 term characterizes model complexity C > 0 constantdetermines trade-off flatness regression amount[f ] defined followsdeviations larger tolerated. function RemplRemp[f ] =1X|f (xi ) yi | .li=1measures -insensitive average training error. problem arisestry minimize (4) called empirical risk minimization (ERM). regression problemsusually Hilbert space F, containing X R type (typically non-linear) functions,aim find function f close yi xi takes formXwj j (x) + b = wT (x) + b,f (x) =jj F, wj R b R. Using Lagrange multiplier techniques, rewriteregression problem dual form (Scholkopf et al., 2000) arrive final -SVRoptimization problem. resulting regression estimate takes form followsf (x) =lX(i )K(xi , x) + b,i=1Lagrange multipliers, K denotes inner product kerneldefined K(x, y) = h(x), (y)i, h, denotes inner product. Note , 6= 0471fiCsaji & Monostoriholds usually small subset training samples, furthermore, parameter b (and )determined well, applying Karush-Kuhn-Tucker (KKT) conditions.Mercers theorem functional analysis characterizes functions correspondinner product space F. Basic kernel types include linear, polynomial, Gaussiansigmoid functions. experiments RAP-MDPs used Gaussian kernelsalso called radial basis function (RBF) kernels. RBF kernels definedK(x, y) = exp( kx yk2 /(2 2 )), > 0 adjustable kernel parameter.variant fitted Q-learning algorithm combined regression softmin actionselection described Table 1. simulates state-action trajectory modelupdates estimated values state-action pairs appearedsimulation. RAP solutions described paper based algorithm.notations pseudocode shown Table 1 follows. Variable containsepisode number, ti length episode j parameter time-steps insideepisode. Boltzmann temperature denoted , control policy appliedepisode x0 (aggregated) initial state. State xij action aij correspond stepj episode i. Function h computes features state-action pairs denotes learningrates. Finally, Li denotes regression sample Qi fitted value function.Although, support vector regression offers elegant efficient solution valuefunction representation problem, presented hash table representation possibilitymuch easier implement, also requires less computation,thus, provides faster solutions. Moreover, values hash table could accessedindependently; one reasons applied hash tables dealtdistributed solutions, e.g., architectures uniform memory access. Nevertheless,SVRs advantages, importantly, generalize similar data.1.2.3.4.5.6.7.Regression Based Q-learningInitialize Q0 , L0 , let = 1.Repeat (for episode)Set soft semi-greedyh policy w.r.t. Qi1 , e.g.,Pexp(Q(x,b)/).(x, a) = exp(Qi1 (x, a)/ )/i1bA(x)Simulate state-action trajectory x0 using policy .j = ti 1 (for state-action pair episode)Determine features state-action pair, yji = h(xij , aij ).Compute new action-valueh estimation xj aj , e.g.,zji = (1 )Qi1 (xij , aij ) + g(xij , aij ) + minbA(xij+18.9.10.11.12.,b).Q(xi1)j+1End loop (end state-actionprocessing)ffUpdate sample set Li1 yji , zji : j = 1, . . . , ti , result Li .Calculate Qi fitting smooth regression function sample Li .Increase episode number, i, decrease temperature, .terminating conditions met, e.g., reaches limitestimated approximation error Q gets sufficiently small.Output: action-value function Qi (or (Qi ), e.g., greedy policy w.r.t. Qi ).Table 1: Pseudocode regression-based Q-learning softmin action selection.472fiAdaptive Stochastic Resource Control3.3 Additional ImprovementsComputing (close-to) optimal solution RL methods, (fitted) Q-learning, couldinefficient large-scale systems, even apply prioritized sweeping capablerepresentation. section present additional improvements order speedoptimization process, even expense achieving suboptimal solutions.3.3.1 Rollout Algorithmsexperiments, presented Section 4, turned using suboptimal base policy, greedy policy respect immediate costs, guideexploration, speeds optimization considerably. Therefore, initial stagesuggest applying rollout policy, limited lookahead policy, optimalcost-to-go approximated cost-to-go base policy (Bertsekas, 2001). orderintroduce concept precisely, let greedy policy w.r.t. immediate-costs,(x) arg min g(x, a).aA(x)value function denoted J . one-step lookahead rollout policy basedpolicy , improvement (cf. policy iteration), calculatedh(x) arg min E G(x, a) + J (Y ) ,aA(x)random variable representing state generated pair (x, a) simulation, is, according probability distribution p(x, a). expected value (viz.,expected costs cost-to-go base policy) approximated Monte Carlo simulation several trajectories start current state. problem deterministic,single simulation trajectory suffices, calculations greatly simplified.Take look Figure 6 illustration. scheduling theory, similar (but simplified)concept found rollout policy would called dispatching rule.Figure 6: evaluation state x rollout algorithms deterministic (left)stochastic (right) case. Circles denote states rectangles represent actions.two main issues suggest application rollout algorithms initialstages value function approximation-based reinforcement learning follows:473fiCsaji & Monostori1. need several initial samples first application approximation techniquesfirst samples generated simulations guided rollout policy.2. General reinforcement learning methods perform quite poorly practice withoutinitial guidance. However, learning algorithm start improving rolloutpolicy , especially, case apply (fitted) Q-learning, learn directlytrajectories generated rollout policy, since off-policy learning method.numerical experiments showed rollout algorithms provide significant speedup.3.3.2 Action Space Decompositionlarge-scale problems set available actions state may large,slow system significantly. current formulation RAP numberavailable actions state O(|T | |R|). Though, even real world situations |R|is, usually, large, could contain thousands tasks. Here, suggestdecomposing action space shown Figure 7. First, system selects task, only,moves new state task fixed executing resourceselected. case state description extended new variable {},denotes case task selected yet. every casesystem select executing resource selected task. Consequently, newaction space = A1 A2 , A1 = { av | v } {a } A2 = { ar | r R }.result, radically decreased number available actions, however, numberpossible states increased. experiments showed reasonable trade-off.Figure 7: Action selection (up) (down) action space decomposition.474fiAdaptive Stochastic Resource Control3.3.3 Clustering Tasksidea divide-and-conquer widely used artificial intelligence recentlyappeared theory dealing large-scale MDPs. Partitioning problemseveral smaller subproblems also often applied decrease computational complexitycombinatorial optimization problems, example, scheduling theory.propose simple still efficient partitioning method practically important class performance measures. real world situations tasks oftenrelease dates due dates, performance measure, e.g., total lateness numbertardy tasks, depends meeting deadlines. Note measures regular.denote (possibly randomized) functions defining release due datestasks : N B : N, respectively. section restrictperformance measures regular depend due dates. order clustertasks, need definition weighted expected slack time given followshXw(s) E B(v) A(v) D(s, v) ,Sw (v) =s(v)(v) = { | hs, vi dom(D) } denotes set resource states task vprocessed, w(s) weights corresponding, example, likelihoodresource state appears execution, simply w(s) = 1/ |(v)|.Figure 8: Clustering tasks according slack times precedence constraints.order increase computational speed, suggest clustering tasks successive disjoint subsets T1 , . . . , Tk according precedence constraints expectedslack times; take look Figure 8 illustration. basic idea behind approachhandle constrained tasks first. Therefore, ideally, Ti Tjtwo clusters < j, tasks Ti expected slack times smaller tasks Tj .However, cases clustering simple, since precedence constraintsmust also taken account clustering criterion priority. Thus,hu, vi C, u Ti v Tj , j must hold. learning, first, tasks T1allocated resources, only. episodes, fix allocation policy concerningtasks T1 start sampling achieve good policy tasks T2 , on.Naturally, clustering tasks two-edged weapon, making small clusters mayseriously decrease performance best achievable policy, making large clusters475fiCsaji & Monostorimay considerably slow system. technique, however, several advantages,e.g., (1) effectively decreases search space; (2) reduces number availableactions states; and, additionally (3) speeds learning, since sample trajectories become smaller (only small part tasks allocated trial and, consequently,variance total costs decreased). effects clustering relative sizeclusters analyzed experimentally presented Section 4.5.3.3.4 Distributed SamplingFinally, argue presented approach easily modified order allowcomputing policy several processors distributed way. Parallel computingspeed calculation solution. consider extensions algorithmusing shared memory distributed memory architectures. Let us supposek processors, denote set processors P = {p1 , p2 , . . . , pk }.case parallel system shared memory architecture, e.g., UMA (uniformmemory access), straightforward parallelize computation control policy.Namely, processor p P sample search space independently, usingsame, shared value function. (joint) policy calculated using common,global value function, e.g., greedy policy w.r.t. function applied.Parallelizing solution using architecture distributed memorychallenging. Probably simplest way parallelize approach several processorsdistributed memory let processors search independently lettingworking own, local value functions. given time number iterations,may treat best achieved solution joint policy. precisely, denoteaggregated initial state x0 , joint control policy defined followsarg min J p (x0 )arg min min Qp (x0 , a),p (pP) aA(x0 )p (pP)J p Qp (approximate) state- action-value functions calculated processor p P. Control policy p solution processor p given numberiterations. numerical experiments usually applied 104 iterations.Naturally, could many (more sophisticated) ways parallelize computationusing several processors distributed memory. example, time timeprocessors could exchange best episodes (trajectories lowest costs)learn experiments others. way, could help improvevalue functions other. numerical experiments, presented Section 4.3, showedeven simplest case, distributing calculation speeds optimizationconsiderably. Moreover, case shared memory speedup almost linear.parallel computing represents promising way deal large-scale systems,theoretical experimental investigation would important. example, harmonizing exploration processors, speedup could improved.4. Experimental Resultssection experimental results benchmark industry-related problemspresented. experiments highlight characteristics solution.476fiAdaptive Stochastic Resource Control4.1 Testing Methodologyorder experimentally study resource control approach, simulation environmentdeveloped C++. applied FQL and, cases, SVRsrealized LIBSVM free library support vector machines (Chang & Lin, 2001).centering scaling data interval [0, 1], used Gaussian kernelsshrinking techniques. always applied rollout algorithms action decomposition,clustering used tests presented Section 4.5, furthermore, distributed samplingapplied test shown Section 4.3. latter cases (tests clusteringdistributed sampling) used hash tables approximately 256Mb hash memory.performance algorithm measured follows. Testing took place twomain fields: first one benchmark scheduling dataset hard problems,one simulation real world production control problem. first case bestsolution, viz., optimal value (aggregated) initial state, J (x0 ) = mina Q (x0 , a),known test instances. hard instances occurredlower upper bounds known, e.g., J1 (x0 ) J (x0 ) J2 (x0 ). casesassumed J (x0 ) (J1 (x0 ) + J2 (x0 ))/2. Since estimations good (viz.,length intervals short), simplification might introduce considerableerror performance estimations. latter test case generated problemsgenerator way J (x0 ) known concerning constructed problems.performance presented tables section, precisely average, E ,standard deviation, (Ei ), error iteration computed followsvuNN hi2Xu1 X1Gj J (x0 ) ,(Ei ) =Gji J (x0 ) E ,Ei =NNj=1j=1Gji denotes cumulative incurred costs iteration sample j Nsample size. Unless indicated otherwise, sample contained results 100 simulationtrials parameter configuration (which associated rows tables).shown Section 2.5.2, RAP-MDPs aperiodic, moreover,APP property, therefore, discounting necessary achieve well-defined problem.However, order enhance learning, still advised apply discounting and, therefore,give less credit events farther current decision point. Heuristically,suggest applying = 0.95 middle-sized RAPs (e.g., hundreds tasks),problems benchmark dataset, = 0.99 large-scale RAPs (e.g.,thousands tasks), problems industry-related experiments.4.2 Benchmark DatasetsADP based resource control approach tested Hurinks benchmark dataset(Hurink, Jurisch, & Thole, 1994). contains flexible job-shop scheduling problems (FJSPs)630 jobs (30225 tasks) 515 machines. applied performance measuremaximum completion time tasks (makespan). problems hard,means, e.g., standard dispatching rules heuristics perform poorly them.dataset consists four subsets, subset contains 60 problems. subsets (sdata,edata, rdata, vdata) differ ratio machine interchangeability (flexibility),477fiCsaji & Monostorishown flex(ib) columns Tables 3 2. columns label n iters (andavg err) show average error carrying altogether n iterations. stddev columns tables section contain standard deviation sample.Table 2 illustrates performance typical dataset instances also givesdetails them, e.g., number machines jobs (columns labels mcsjbs). Table 3 summarized performance benchmark datasets shown.benchmark configurationdataset inst mcs jbs flexsdatamt06661sdatamt1010101sdatala095151sdatala1910101sdatala3915151sdatala4015151edatamt0666 1.15edatamt101010 1.15edatala09515 1.15edatala191010 1.15edatala391515 1.15edatala401515 1.15rdatamt06662rdatamt1010102rdatala095152rdatala1910102rdatala3915152rdatala4015152vdatamt06663vdatamt1010105vdatala095152.5vdatala1910105vdatala3915157.5vdatala4015157.5average error (standard deviation)1000 iters5000 iters10 000 iters1.79 (1.01) %0.00 (0.00) %0.00 (0.00) %9.63 (4.59) %8.83 (4.37) %7.92 (4.05) %5.67 (2.41) %3.87 (1.97) %3.05 (1.69) %11.65 (5.21) %6.44 (3.41) %3.11 (1.74) %14.61 (7.61) % 12.74 (5.92) % 11.92 (5.63) %10.98 (5.04) %8.87 (4.75) %8.39 (4.33) %0.00 (0.00) %0.00 (0.00) %0.00 (0.00) %18.14 (8.15) % 12.51 (6.12) %9.61 (4.67) %7.51 (3.33) %5.23 (2.65) %2.73 (1.89) %8.04 (4.64) %4.14 (2.81) %1.38 (1.02) %22.80 (9.67) % 17.32 (8.29) % 12.41 (6.54) %14.78 (7.14) %8.08 (4.16) %6.68 (4.01) %6.03 (3.11) %0.00 (0.00) %0.00 (0.00) %17.21 (8.21) % 12.68 (6.81) %7.87 (4.21) %7.08 (3.23) %6.15 (2.92) %3.80 (2.17) %18.03 (8.78) % 11.71 (5.78) %8.87 (4.38) %24.55 (9.59) % 18.90 (8.05) % 13.06 (7.14) %23.90 (7.21) % 18.91 (6.92) % 14.08 (6.68) %0.00 (0.00) %0.00 (0.00) %0.00 (0.00) %8.76 (4.65) %4.73 (2.23) %0.45 (0.34) %9.92 (5.32) %7.97 (3.54) %4.92 (2.60) %14.42 (7.12) % 11.61 (5.76) %6.54 (3.14) %16.16 (7.72) % 12.25 (6.08) %9.02 (4.48) %5.86 (3.11) %4.08 (2.12) %2.43 (1.83) %Table 2: Performance (average error deviation) typical benchmark problems.Simple dispatching rules (which often applied practice), greedy ones,perform poorly benchmark datasets. average error around 2530 %.contrast, Table 3 demonstrates using method, average error less 5 %10 000 iterations. shows learning beneficial type problems.best performance benchmark datasets achieved MastrolilliGambardella (2000). Though, algorithm performs slightly better ours,solution exploits (unrealistic) specialties dataset, e.g., durations dependresources; tasks linearly ordered jobs; job consists478fiAdaptive Stochastic Resource Controlnumber tasks. Moreover, cannot easily generalized stochastic resource controlproblem algorithm faces. Therefore, comparison solutions hard.benchmarkdataset flexibsdata1.0edata1.2rdata2.0vdata5.0average2.31000 iterationsavg err std dev8.54 %5.02 %12.37 %8.26 %16.14 %7.98 %10.18 %5.91 %11.81 %6.79 %5000 iterationsavg err std dev5.69 %4.61 %8.03 %6.12 %11.41 %7.37 %7.73 %4.73 %8.21 %5.70 %10 000 iterationsavg err std dev3.57 %4.43 %5.26 %4.92 %7.14 %5.38 %3.49 %3.56 %4.86 %4.57 %Table 3: Summarized performance (average error deviation) benchmark datasets.4.3 Distributed Samplingpossible parallelizations presented method also investigated, i.e., speedupsystem relative number processors (in practise, multiprocessor environment emulated single processor, only). average number iterationsstudied, system could reach solution less 5% error Hurinks dataset.average speed single processor treated unit, comparison.Figure 9 two cases shown: first case (rear dark bars) processor couldaccess common global value function. means processor could read writeglobal value function, otherwise, searched (sampled search space)independently. Figure 9 demonstrates case speedup almost linear.second case (front light bars) processor (local) value function(which realistic strongly distributed system, GRID) and,search finished, individual value functions compared. Therefore,processors estimations own, search, local solutionbest performing processor selected. Figure 9 shows achieved speedup casestopped simulation processors achieved solution less 5 % error.Figure 9: Average speedup relative number processors.experiments show computation resource allocator functioneffectively distributed, even commonly accessible value function available.479fiCsaji & Monostori4.4 Industry Related Testsalso initiated experiments simulated factory modeling structure realplant producing customized mass-products, especially, light bulbs. industrial datacame huge national industry-academia project, research development solutions support manufacturing enterprises coping requirements adaptiveness, realtimeness cooperativeness (Monostori, Kis, Kadar, Vancza, & Erdos, 2008).optimalslack ratio50 %40 %30 %20 %10 %0%1000 iterationsavg err std dev0.00 %0.00 %0.12 %0.10 %0.52 %0.71 %1.43 %1.67 %5.28 %3.81 %8.89 %5.17 %5000 iterationsavg err std dev0.00 %0.00 %0.00 %0.00 %0.24 %0.52 %1.11 %1.58 %4.13 %3.53 %7.56 %5.04 %10 000 iterationsavg err std dev0.00 %0.00 %0.00 %0.00 %0.13 %0.47 %1.05 %1.49 %3.91 %3.48 %6.74 %4.83 %Table 4: Summarized performance relative optimal slack ratio system.Since, access historical data concerning past orders, used randomlygenerated orders (jobs) random due dates. tasks process-plans jobs,however, covered real products; well as, resources covered real machine types.plant machines require product-type dependent setup times, specialtasks durations require resources processed, example,cooling down. Another feature plant previously given time pointspreemptions allowed, e.g., end work shift. applied performance measureminimize number late jobs, viz., jobs finished due dates,additional secondary measure minimize total cumulative lateness,applied compare two schedules number late jobs.experiments jobs due dates generated specialparameterizable generator way optimally none jobs late. Therefore,known J (x0 ) = 0 error algorithm computed accordingly.first case, shown Table 4, applied 16 machines 100 random jobs,altogether contained 200 tasks. convergence properties studied relativeoptimal slack ratio. deterministic case, e.g., slack ratio solutionn1 X B(Ji ) F (Ji )(%) =,nB(Ji ) A(Ji )i=1n number jobs; A(J) B(J) denote release due date job J,respectively; F (J) finish time job J relative solution %, namely, latest finishtime tasks job. Roughly, slack ratio measures tightness solution,example, (%) > 0, shows jobs were, average, finisheddue dates (%) < 0, indicates that, approximately, many jobs late.(%) = 0, shows jobs meet due dates, job finished480fiAdaptive Stochastic Resource Controltime, spare (residual) times. optimal slack ratio meanmaximal achievable slack ratio (by optimal solution). experimentsvalues known special construction test problem instances.applied optimal slack ratio measure hard problem is. first columnTable 4 shows optimal slack ratio percentage, e.g., 30 % means 0.3 slack ratio.configurationmachstasks630161402528030560502000100100001000 iterationsavg err std dev4.01 %2.24 %4.26 %2.32 %7.05 %2.55 %7.56 %3.56 %8.69 %7.11 %15.07 % 11.89 %5000 iterationsavg err std dev3.03 %1.92 %3.28 %2.12 %4.14 %2.16 %5.96 %2.47 %7.24 %5.08 %10.31%7.97 %10 000 iterationsavg err std dev2.12 %1.85 %2.45 %1.98 %3.61 %2.06 %4.57 %2.12 %6.04 %4.53 %9.11 %7.58 %Table 5: Summarized performance relative number machines tasks.second case, shown Table 5, fixed optimal slack ratiosystem 10 % investigated convergence speed relative plant size (numbermachines) number tasks. last two experiments (configuration2000 10 000 tasks) 10 samples generated, long runtime.computation 10 000 iterations took approximately 30 minutes 50 machines & 2000tasks configuration 3 hours 100 machines & 10000 tasks configuration1 .results demonstrate ADP adaptive sampling based solution scales wellslack ratio size (the number machines task) problem.4.5 Clustering Experimentseffectiveness clustering industry-related data also studied. consideredsystem 60 resources 1000 random tasks distributed among 400500 jobs (thereapproximately 10002000 precedence constraints). tasks generated waythat, optimal case, none late slack ratio 20 %.First, tasks ordered according slack times clustered.applied 104 iterations cluster. computational time case usingone cluster treated unit. Table 6 average standard deviationerror computational speedup shown relative number tasks cluster.results demonstrate partitioning search space results greaterspeed, often accompanied better solutions. latter phenomenon explained fact using smaller sample trajectories generates smaller variancepreferable learning. hand, making small clusters may decrease performance (e.g., making 50 clusters 20 tasks current case). particular caseapplying 20 clusters approximately 50 tasks cluster balances good performance(3.02 % error average) remarkable speedup (approximately 3.28 ).1. tests performed Centrino (Core-Duo) 1660Mhz CPU ( P4 3GHz) 1Gb RAM.481fiCsaji & Monostoriconfigurationclusterstasks110005200101002050303340255020performance 10 000 iterations per clusterlate jobs avg errorstd devspeed speedup28.16.88 %2.38 %4231.0022.75.95 %2.05 %2751.5420.34.13 %1.61 %1892.2413.93.02 %1.54 %1043.2814.43.15 %1.51 %676.3116.23.61 %1.45 %498.6318.74.03 %1.43 %3611.65Table 6: Speedup performance relative number tasks cluster.clustering tasks represents considerable help dealing large-scale RAPs,theoretical experimental investigations promising.4.6 Adaptation Disturbancesorder verify proposed algorithm changing environments, experimentsinitiated carried random JSPs aim minimizing makespan.adaptive features system tested confronting unexpected events,as: resource breakdowns, new resource availability (Figure 10), new job arrivals jobcancellations (Figure 11). Figures 10 11 horizontal axis represents time,vertical one, achieved performance measure. figures made averaginghundred random samples. tests 20 machines used dozens jobs.test episode unexpected event (disturbance) time = 100.change took place, considered two possibilities: either restarted iterativescheduling process scratch continued learning, using current (obsolete) valuefunction. experienced latter approach much efficient.explanation phenomenon value functions control policiesoptimal value function Lipschitz continuously depend transition-probabilityimmediate-cost functions MDP (Csaji, 2008). Therefore, small changesenvironmental dynamics cannot cause arbitrary large changes value function.results numerical experiments, shown Figures 10 11, indicativephenomenon average change value function large. Consequently,applying obsolete value function change took place MDP preferablerestarting whole optimization process scratch. adaptive feature makesADP/RL based approaches even attractive practical applications.results, black curves, show case obsolete value function approximationapplied change took place. performance would arise systemrecomputed whole schedule scratch drawn gray part (a) Figure 10.One notice even problem became easier change environment (at time = 100), example, new resource available (part (b) Figure 10)job cancelled (part (b) Figure 11), performance started slightly decrease( started slightly increase) event. phenomenon explainedfact even special cases system explore new configuration.482fiAdaptive Stochastic Resource ControlFigure 10: black curves, (t), show performance measure case resource breakdown (a) new resource availability (b) = 100; graycurve, (t), demonstrates case policy would recomputed scratch.Figure 11: black curves, (t), show performance measure resource controlcase new job arrival (a) job cancellation (b) time = 100.5. Concluding RemarksEfficient allocation scarce, reusable resources time uncertain dynamic environments important problem arises many real world domains, productioncontrol. paper took machine learning (ML) approach problem. First, generalresource allocation framework presented and, order define reactive solutions,reformulated stochastic shortest path problem, special Markov decision process(MDP). core idea solution application approximate dynamic programming (ADP) reinforcement learning (RL) techniques Monte Carlo simulationstochastic resource allocation problems (RAPs). Regarding compact value function representations, two approaches studied: hash table support vector regression (SVR),specially -SVRs. Afterwards, several additional improvements, applicationlimited-lookahead rollout algorithms initial phases, action space decomposition, taskclustering distributed sampling, suggested speeding computationgood control policy. Finally, effectiveness approach demonstrated resultsnumerical simulation experiments benchmark industry-related data.experiments also supported adaptive capabilities proposed method.483fiCsaji & Monostoriseveral advantages ML based resource allocation preferablekinds RAP solutions, e.g., classical approaches. favorable features follows:1. presented RAP framework general, model several resource management problems appear practice, scheduling problems, transportationproblems, inventory management problems maintenance repair problems.2. ADP/RL based methods essentially face problem presence uncertainties, since theoretical foundation provided MDPs. Moreover,adapt unexpected changes environmental dynamics, breakdowns.3. Additionally, algorithms theoretical guarantees finding (approximately)optimal solutions, least limit, known. demonstrated experiments, actual convergence speed RAPs usually high, especially caseapplying described improvements, clustering distributed sampling.4. simulation experiments industrial data also demonstrate ADP/RL basedsolutions scale well workload size problem and, therefore,effectively applied handle real world RAPs, production scheduling.5. Domain specific knowledge also incorporated solution. base policyrollout algorithm, example, reflect priori knowledge structureproblem; later knowledge may appear exploration strategy.6. Finally, proposed method constitutes any-time solution, since samplingstopped number iterations. way, amount computationaltime controlled, also important practical advantage.Consequently, ML approaches great potentials dealing real world RAPs, sincehandle large-scale problems even dynamic uncertain environments.Several research directions possible. Now, conclusion paper,highlight them. suggested improvements, clustering distributedsampling, investigated since resulted considerable speedup.guidance reinforcement learning rollout algorithms might effectively appliedapplications, well. theoretical analysis average effects environmentalchanges value functions could result new approaches handle disturbances.Another promising direction would extend solution way also takes riskaccount and, e.g., minimizes expected value total costs alsodeviation, secondary criterion. Finally, trying apply solution pilot projectcontrol real plant would interesting could motivate research directions.6. Acknowledgmentswork supported Hungarian Scientific Research Fund (OTKA), Grant No.T73376, EU-funded project Coll-Plexity, Grant No. 12781 (NEST). BalazsCsanad Csaji greatly acknowledges scholarship Hungarian Academy Sciences.authors express thanks Tamas Kis contribution related testsindustrial data Csaba Szepesvari helpful discussions machine learning.484fiAdaptive Stochastic Resource ControlReferencesAndrieu, C., Freitas, N. D., Doucet, A., & Jordan, M. I. (2003). introduction MCMC(Markov Chain Monte Carlo) machine learning. Machine Learning, 50, 543.Aydin, M. E., & Oztemel, E. (2000). Dynamic job-shop scheduling using reinforcementlearning agents. Robotics Autonomous Systems, 33, 169178.Beck, J. C., & Wilson, N. (2007). Proactive algorithms job shop scheduling probabilistic durations. Journal Artificial Intelligence Research, 28, 183232.Bellman, R. E. (1961). Adaptive Control Processes. Princeton University Press.Bertsekas, D. P. (2005). Dynamic programming suboptimal control: surveyADP MPC. European Journal Control, 11 (45), 310334.Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,Belmont, Massachusetts.Bertsekas, D. P. (2001). Dynamic Programming Optimal Control (2nd edition). AthenaScientific, Belmont, Massachusetts.Bulitko, V., & Lee, G. (2006). Learning real-time search: unifying framework. JournalArtificial Intelligence Research, 25, 119157.Chang, C. C., & Lin, C. J. (2001). LIBSVM: library support vector machines. Softwareavailable on-line http://www.csie.ntu.edu.tw/cjlin/libsvm.Csaji, B. Cs. (2008). Adaptive Resource Control: Machine Learning Approaches Resource Allocation Uncertain Changing Environments. Ph.D. thesis, FacultyInformatics, Eotvos Lorand University, Budapest.Csaji, B. Cs., Kadar, B., & Monostori, L. (2003). Improving multi-agent based schedulingneurodynamic programming. Proceedings 1st International ConferenceHolonic Mult-Agent Systems Manufacturing, September 13, Prague, CzechRepublic, Vol. 2744 Lecture Notes Artificial Intelligence, pp. 110123.Csaji, B. Cs., & Monostori, L. (2006). Adaptive sampling based large-scale stochastic resource control. Proceedings 21st National Conference Artificial Intelligence(AAAI 2006), July 1620, Boston, Massachusetts, pp. 815820.Dolgov, D. A., & Durfee, E. H. (2006). Resource allocation among agents MDP-inducedpreferences. Journal Artificial Intelligence Research, 27, 505549.Even-Dar, E., & Mansour, Y. (2003). Learning rates Q-learning. Journal MachineLearning Research, 5, 125.Feinberg, E. A., & Shwartz, A. (Eds.). (2002). Handbook Markov Decision Processes:Methods Applications. Kluwer Academic Publishers.Gersmann, K., & Hammer, B. (2005). Improving iterative repair strategies schedulingSVM. Neurocomputing, 63, 271292.Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chainsapplication. Biometrika, 57, 97109.485fiCsaji & MonostoriHatvany, J., & Nemes, L. (1978). Intelligent manufacturing systems - tentative forecast.Niemi, A. (Ed.), link science applications automatic control;Proceedings 7th IFAC World Congress, Vol. 2, pp. 895899.Hurink, E., Jurisch, B., & Thole, M. (1994). Tabu search job shop scheduling problemmulti-purpose machines. Operations Research Spektrum, 15, 205215.Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization simulated annealing.Science, 220 (4598), 671680.Mastrolilli, M., & Gambardella, L. M. (2000). Effective neighborhood functionsflexible job shop problem. Journal Scheduling, 3 (1), 320.Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., & Teller, E. (1953). Equationstate calculations fast computing machines. Journal Chemical Physics, 21,10871092.Monostori, L., Kis, T., Kadar, B., Vancza, J., & Erdos, G. (2008). Real-time cooperative enterprises mass-customized production. International Journal ComputerIntegrated Manufacturing, (to appear).Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.Pinedo, M. (2002). Scheduling: Theory, Algorithms, Systems. Prentice-Hall.Powell, W. B., & Van Roy, B. (2004). Handbook Learning Approximate DynamicProgramming, chap. Approximate Dynamic Programming High-Dimensional Resource Allocation Problems, pp. 261283. IEEE Press, Wiley-Interscience.Riedmiller, S., & Riedmiller, M. (1999). neural reinforcement learning approach learnlocal dispatching policies production scheduling. Proceedings 16th International Joint Conference Artificial Intelligence, Stockholm, Sweden, pp. 764771.Schneider, J. G., Boyan, J. A., & Moore, A. W. (1998). Value function based productionscheduling. Proceedings 15th International Conference Machine Learning,pp. 522530. Morgan Kaufmann, San Francisco, California.Scholkopf, B., Smola, A., Williamson, R. C., & Bartlett, P. L. (2000). New support vectoralgorithms. Neural Computation, 12, 12071245.Singh, S., Jaakkola, T., Littman, M., & Szepesvari, Cs. (2000). Convergence resultssingle-step on-policy reinforcement-learning algorithms. Machine Learning, 38 (3),287308.Sontag, E. D. (1998). Mathematical Control Theory: Deterministic Finite DimensionalSystems. Springer, New York.Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning. MIT Press.Topaloglu, H., & Powell, W. B. (2005). distributed decision-making structure dynamic resource allocation using nonlinear function approximators. Operations Research, 53 (2), 281297.Zhang, W., & Dietterich, T. (1995). reinforcement learning approach job-shop scheduling. Proceedings 14th International Joint Conference Artificial Intelligence, pp. 11141120. Morgan Kauffman.486fiJournal Artificial Intelligence Research 32 (2008) 607629Submitted 01/08; published 06/08Unifying Framework Structural Properties CSPs:Definitions, Complexity, TractabilityLucas Bordeauxlucasb@microsoft.comMicrosoft Research7 J J Thomson AvenueCambridge, CB3 0FB, United KingdomMarco CadoliToni Mancinicadoli@dis.uniroma1.ittmancini@dis.uniroma1.itDipartimento di Informatica e SistemisticaSapienza Universita di RomaVia Ariosto 25, I-00185 Roma, ItalyAbstractLiterature Constraint Satisfaction exhibits definition several structural properties possessed CSPs, like (in)consistency, substitutability interchangeability. Current tools constraint solving typically detect properties efficientlymeans incomplete yet effective algorithms, use reduce search spaceboost search.paper, provide unifying framework encompassing properties knownfar, CSP fields literature, shed light semantical relationships among them. gives unified comprehensive view topic, allows new,unknown, properties emerge, clarifies computational complexity variousdetection problems.particular, among others, two new concepts, fixability removability emerge,come ideal characterisations values may safely assigned removed variables domain, preserving problem satisfiability. two notionssubsume large number known properties, including inconsistency, substitutabilityothers.computational intractability property-detection problems, following CSP approach determine number relaxations provide sufficient conditions tractability. particular, exploit forms language restrictionslocal reasoning.1. IntroductionMany Constraint Satisfaction Problems (CSPs) arise modelling real-lifeapplications exhibit structural properties distinguish random instances.Detecting properties widely recognised effective way improvingsolving process. end, several already identified, differenttechniques developed order exploit them, goal reducingsearch space explored. Good examples value inconsistency (Mackworth, 1977;Montanari, 1974), substitutability interchangeability (Freuder, 1991), generalc2008AI Access Foundation. rights reserved.fiBordeaux, Cadoli, & Manciniforms symmetries (Crawford, Ginsberg, Luks, & Roy, 1996; Gent & Smith, 2000),functional dependencies among variables (Li, 2000; Mancini & Cadoli, 2007).Unfortunately, checking whether properties hold, (or thought be) oftencomputationally hard. example, let us consider interchangeability. Value saidinterchangeable value b variable x every solution assigns xremains solution x changed b, vice versa (Freuder, 1991). problemchecking interchangeability coNP-complete (cf. Proposition 4). Analogously, detectingforms symmetry reduces graph automorphism problem (Crawford,1992) (for known polynomial time algorithm, even evidenceNP-complete, Kobler, Schoning, & Toran, 1993).end, order allow general algorithms exploit properties efficiently,different approaches followed. First all, syntactic restrictions constraintlanguages enforced, order allow efficient verification propertiesinterest. Alternatively, local versions properties defined,used infer global counterparts, verified polynomial time.instance local reasoning approach, instead checking whether value fullyinterchangeable variable, Freuder (1991) proposes check whether value neighbourhood, k-interchangeable. task involves considering bounded-sized subsetsconstraints, hence performed polynomial time. Neighbourhoodk-interchangeability sufficient (but necessary) conditions full interchangeability,proven highly effective practice (cf., e.g., Choueiry & Noubir, 1998;Lal, Choueiry, & Freuder, 2005).paper give formal characterisation several properties CSPsexploited order save search, present unifying framework allowssemantical connections emerge. properties well-known Constraint Programming literature, others used contexts (as databases),others are, best knowledge, original, and, opinion, playkey role allowing deep understanding topic. particular, reconsidernotions inconsistency, substitutability interchangeability, propose fixable,removable, implied value given variable, instances generaldefinition satisfiability-preserving transformation determined, dependent,irrelevant variable. properties make possible transform problem simplerone. Depending case, transformation guaranteed preserve solutions,satisfiability problem, i.e., least solution, one exists. general,properties detected either statically, preprocessing stage inputCSP (cf., e.g., Cadoli & Mancini, 2007), dynamically, search (since may arisetime). Moreover, cases dont even need explicitly verify whetherproperties hold, guaranteed intrinsic characteristics problem.instance, problems guaranteed unique solution. cases referred promise problems literature (Even, Selman, & Yacobi, 1984), meaningaddition problem description informed certain properties verifies(cf. forthcoming Example 1 example).formal characterisation properties connections allow us shed lightcomputational complexity recognition task elegant way, proving608fiA Unifying Framework Structural Properties CSPsthat, worst case, detection complex original problem.particular, see Section 3.1, detecting proposed propertiescoNP-complete task. holds also Freuders substitutability interchangeability(this result is, best knowledge, original). Hence, order ablepractically make relevant checks preprocessing search, investigate twodifferent approaches efficient verification proposed properties: additionssuitable restrictions constraint language, exploitation efficient forms localreasoning, i.e., checking single constraints.outline paper follows: giving intuitive example recallingpreliminaries, Section 2 present properties interested in, discussconnections. Then, Section 3 focus complexity various propertydetection tasks. particular, Section 3.1 prove intractable; hence,Section 3.2 focus relaxations guarantee tractability reasoning, investigatingtwo aforementioned approaches. Finally, Section 4 draw conclusions addressfuture work.2. Hierarchy Propertiessection, formally define several structural properties CSPs, discuss semantical relationships hold among them, show exploitedconstraint solving.2.1 Intuitive Exampleorder allow gentle introduction main properties investigated forthcoming sections, first introduce following example.Example 1 (Factoring, Lenstra & Lenstra, 1990; Pyhala, 2004). problem simplified version one important problems public-key cryptography. Given(large) positive integer Z fact product two different unknown primenumbers X (different 1), goal identify two numbers.intuitive formulation instance problem (i.e., given Z)CSP, adequate arbitrarily large numbers, amounts encode combinatorial circuitinteger multiplication, follows: assuming Z n digits (in base b) z1 , . . . , zn ,consider 2n variables x1 , . . . , xn y1 , . . . , yn one digit (in base b) X(x1 y1 least significant ones). domain variables [0, b 1].order maintain information carries, n + 1 additional variables c1 , . . . , cn+1must considered, domain [0..(b 1)2 n/b].1constraints (cf. Figure 1 intuition, x4 , x5 , x6 , y4 , y5 , y6equal 0, omitted readability), following:1. Constraints factors:1. intuitive example, little abuse respect permitted forthcomingDefinition 1, allow, enhance readability, different variables defined different domains.However, observe easy recover using standard well-known techniques(e.g., adding new monadic constraints model smaller domains).609fiBordeaux, Cadoli, & Mancini7 8 77 9 7=0 6 13 18 12404963 7249 56 496 2 7 256633499c7c6c5x3y3x2y2x1y1 =c3c2c1c4x3 y1 x2 y1 x1 y1x3 y2 x2 y2 x1 y2x3 y3 x2 y3 x1 y3z6 z5 z4 z3 z2 z1Figure 1: Factoring instance 627239, n = 6, b = 10(a) Factors must different 1, or, equivalently, X 6= Z 6= Z must hold;P(b) every digit [1, n]: zi = ci + j,k[1,n]:j+k=i+1 (xj yk mod b);2. Constraints carries:(a) Carry least significant digit 0: c1 = 0;(b) Carries digits: [2, n + 1], ci = ci1 +Pj,k[1,n]:j+k=ixj ykb ;(c) Carry significant digit 0: cn+1 = 0.guess two factors X (i.e., variables x1 , . . . , xn y1 , . . . , yn )made, values variables c1 , . . . , cn+1 completely determined, since followsemantics multiplication. called functional dependence amongvariables.Functional dependencies arise often in, e.g., problems intermediatestate maintained, detection exploitation recognizedgreat importance efficiency point view, since lead significant reductionssearch space (cf., e.g., Giunchiglia, Massarotto, & Sebastiani, 1998; Mancini & Cadoli,2007; Cadoli & Mancini, 2007).Moreover, presence functional dependencies among variables CSP highlightssecond interesting problem, i.e., computing values dependent variableschoice defining ones made. problem, always subproblemCSP dependencies, exactly one solution, hence, knowledge promiseuseful solver. worth noting also problems intrinsicallyexhibit promises. case of, e.g., Factoring add symmetry-breakingconstraint forcing x1 , . . . , xn lexicographically less equal y1 , . . . , yn .new formulation guaranteed exactly one solution (since X prime).Factoring problem exhibits also interesting properties: let us considerinstance Z given binary notation (i.e., b = 2) least significantdigit z1 equal 1. implies last digit factors X must 1.Hence, say value 1 implied variables x1 y1 , 0 removableand, precisely inconsistent. Moreover, problem, which, symmetrybroken, unique solution, also know variables x1 , . . . , xn y1 , . . . , yn610fiA Unifying Framework Structural Properties CSPsdetermined (cf. forthcoming Definition 2), regardless instance,functional dependence already discussed, variables encoding carries, i.e.,ci (i [1, n]), dependent {x1 , . . . , xn , y1 , . . . , yn }.problems unique solutions, known that, unfortunately, resolution remains intractable (cf., e.g., Papadimitriou, 1994; Valiant & Vijay V. Vazirani, 1986; Calabro,Impagliazzo, Kabanets, & Paturi, 2003). However, exclude possibilityfind good heuristics instances promise, look propertiesimplied existence unique solutions, exploited order improvesearch process. particular, determined implied values play important roleclasses problems. Factoring example shows, problems arisefrequently practice, either subproblems CSPs, presence functional dependencies (cf. also Mancini & Cadoli, 2007; Mancini, Cadoli, Micaletto, & Patrizi, 2008,examples), intrinsic characteristics problem hand. general,problem unique solution, variables determined value.Another central role played removability property, characterises preciselycase value safely removed domain variable, preservingsatisfiability. property course weaker inconsistency (since solutions maylost), safely used place interested finding solutionCSP, one exists, them.2.2 PreliminariesDefinition 1 (Constraint Satisfaction Problem (CSP), Dechter, 1992). Constraint Satisfaction Problem triple hX, D, Ci where:X finite set variables;finite set values, representing domain variable;C finite set constraints {c1 , . . . c|C| }, formci = ri (Vi ), Vi list k |X| variables X (the constraint scope),ri Dk k-ary relation (the constraint relation). sometimesdenote set Vi variables constraint ci var(ci ).Given set variables V domain D, V -tuple mappingassociates value tx every x V . value called x-component t. GivenV -tuple subset U V variables, denote t|U restrictionU , value variables U undefined elsewhere.explicit assignment value x-component V -tuple (x V ) writtent[x := a].Given CSP hX, D, Ci, X-tuple satisfies constraint ci =ri (Vi ) C t|Vi ri .denote Sol(ci ) set X-tuples satisfy ci . set cC Sol(c) X-tuplessatisfy constraints called solution space, denoted Sol(C).solving CSP mean decide whether set Sol(C) non-empty and, so,compute one (or all) solutions.611fiBordeaux, Cadoli, & Mancini'$'$12'$ '$PP&%PP...&%P..4.........5......"..." &% &%'$."""3&%Figure 2: graph 3-coloredset X-tuples called search space denoted SD , simplydomain implicit context. relational operators selection, projectioncomplement useful: given V -relation c, subset U V value D,denote x=a (ci ) (resp. x6=a (ci )) V -relation contains tuples ci whosevalue x (resp. different a), U (ci ) set restrictions U tuplesci (i.e., set U -tuples {t | t0 ci (t = t0 |U )}) ci set V -tuples {t | 6 ci }.Note x=a (S) denotes search space obtained fixing value x a.sake simplicity, sets X C considered globally defined shalltherefore omitted parameters definitions; search spaceexplicitly mentioned.Example 2. Consider CSP hX, D, Ci modeling 3-coloring problem graphFigure 2. that:X = {x1 , . . . , x5 } set variables (one node),= {R, G, B} set colors,C following finite set constraints, one edge:C = {N E(x2 , x3 ), N E(x3 , x4 ), N E(x2 , x4 ), N E(x4 , x5 )},N E (not-equal) binary relation defined({R, G, B} {R, G, B}) \ {hR, Ri, hG, Gi, hB, Bi}.2.3 Definitionssection, formally present properties already introduced Section 2.1,show applicability examples.Definition 2. following properties defined search space S, variables x y,values b, set variables V :612fiA Unifying Framework Structural Properties CSPsfixable(S, x, a)substitutable(S, x, a, b)(t Sol(C) t[x := a] Sol(C))tx = Sol(C)t[x := b] Sol(C)tx = Sol(C)b 6= (t[x := b] Sol(C))removable(S, x, a)inconsistent(S, x, a)Sol(C) tx 6=implied(S, x, a)Sol(C) tx =determined(S, x)Sol(C)b 6= tx (t[x := b] 6 Sol(C))Sol(C)ty = t0yt, t0 t0 Sol(C)0x V (tx = tx )dependent(S, V, y)irrelevant(S, x)Sol(C)(t[x := a] Sol(C))interchangeability, well-known (Freuder, 1991) defined termssubstitutability:interchangeable(S, x, a, b)substitutable(S, x, a, b) substitutable(S, x, b, a)cases ambiguity arises considered set constraints,indicate using subscript (e.g., irrelevantC (S, x)). Note definitionslast three value-oriented, properties specific values domain.contrary, determinacy, irrelevance, dependence variable-oriented propertiesdirectly express results particular values domains importantrelations value-oriented notions (cf. forthcoming Section 2.4).already claimed Section 1, properties Definition 2 alreadyknown, well beneficial effects search. particular, notion consistencyproposed Montanari (1974) Mackworth (1977), one best-studiednotions CSP. Substitutability interchangeability introduced Freuder (1991).Implied values, also known literature backbones, seemingly firststudied explicitly Monasson, Zecchina, Kirkpatrick, Selman, Troyansky (1999).best knowledge, notions removable fixable values (which, showSection 2.4 play key role unifying framework proposed paper)contrary considered. Determined, irrelevant dependent variablesstudied number contexts logic, SAT, databases, cf., e.g., Beth definability (Chang & Keisler, 1990), dont care variables propositional formulae (Thiffault,613fiBordeaux, Cadoli, & ManciniBacchus, & Walsh, 2004; Safarpour, Veneris, Drechsler, & Lee, 2004), Audit problem (Jonsson & Krokhin, 2008), aware little work concerning applicationcontext CSPs.following examples illustrate properties.Example 3 (Example 2 continued). Consider CSP modeling coloring problemgraph Figure 2. Let denote search space five variables x1 , . . . , x5domain {R, G, B}. following properties hold:fixable(,x1 ,R), since every solution t, t[x1 := R] solution well;substitutable(,x1 ,R,G), since every solution tx1 = R, t[x1 := G]solution well;interchangeable(,x1 ,R,G),substitutable(,x1 ,G,R) also holds;previouspointfactremovable(,x1 ,G), every solution tx1 = G, existsdifferent color K {R, B} x1 t[x1 := K] solution well;irrelevant(,x1 ), actually replace x1 -component solutionvalue, since x1 denotes disconnected node graph.properties holding variable x1 encodes disconnected node giveinitial suggestions relationships exist among different notions.nodes, have, example:removable(,x5 ,G), every solution tx5 = G, existsdifferent color K {R, B} x5 t[x5 := K] solution well.due fact node 5 connected node 4.another, complex, example, consider following:Example 4. Let CSP given boolean variables x, y, z, w, p, q, r, whose constraintsencoded formula below:x (x y) ((z w) p) ((z y) (q r))Denoting search space variables range {true, false}, have,among others:inconsistent(,x,false),determined(,y),fixable(,x,true),dependent(,{z, w},p),implied(,x,true),dependent(,{z, y},q),implied(,y,true),dependent(,{z, y},r),inconsistent(,y,false),fixable(,q,true),614fiA Unifying Framework Structural Properties CSPsimplied(,q,true),implied(,r,true).fixable(,r,true),definitions Definition 2 used construct solution-preserving transformations, i.e., mappings transform solutions solutions.Definition 3 (solution-preserving transformation). solution-preserving transformationtotal mapping(t Sol(C) (t) Sol(C))understand connection solution-preserving transformations aforementioned properties, consider following mappings:1 (t) = t[x := a](t[x := b]2 (t) =t[x := b] tx =3 (t) = t[x := a] tx = botherwisetx =otherwiseChecking whether value fixable variable x, whether value substitutable valueb variable x, whether values b interchangeable variable x amountscheck whether mappings 1 , 2 3 (respectively) solution-preserving.Solution-preserving transformations interesting allow us remove values search space preserving satisfiability problem. Moreover,correspondence properties existence particular solution-preservingmappings shows interesting connections hold among properties concepts, like symmetries. example, Mancini Cadoli (2005) give logical characterisation symmetries problem specifications, similar to, fact strongerthan, Definition 3. addition, general forms solution-preserving transformations could defined, that, e.g., allow also modification constraints, i.e.,pairs (, ) (t Sol(C) (t) Sol((t, C))). interesting topic,may lead definition general properties CSP, leftfuture research.2.4 Semantical Relationshipsalready observed (cf. also Examples 3 4), several semantical relationships existamong notions presented Definition 2, appear weaker,others stronger. main connections clarified following theorem.Theorem 1. relationships shown Figure 3 hold properties Definition 2.Proof.Dependence-determinacy. dependent(S, {x1 , . . . , xi }, y) iff every solutionvalue given function f values assigns x1 . . . xi , iff search615fiBordeaux, Cadoli, & Manciniirrelevancedependencedependent(S, {x1 , . . . , xi }, y)a1 . . . ai determined(x1 =a1 ,...,xi =ai (S), y)determinacyirrelevant(S, x)fixable(S, x, a)determined(S, x)b implied(S, x, b)implicationimplied(S, x, b)fixable(S, x, b)implied(S, x, a)b \ {a} inconsistent(S, x, b)fixabilityinconsistencyfixable(S, x, b)substitutable(S, x, a, b)inconsistent(S, x, a)b substitutable(S, x, a, b)substitutabilityinconsistent(S, x, a)removable(S, x, a)removable(S, x, a)b \ {a} substitutable(S, x, a, b)removabilityFigure 3: Semantical relationships among properties.space x1 =a1 ,...,xi =ai (S) (where variables receive fixed value), solutions assignvalue f (a1 , . . . , ) y.Irrelevance-fixability. Sol(C) (t[x := a] Sol(C)) rewrites (tSol(C) t[x := a] Sol(C)).Determinacy-implication. implied(S, x, b) holds b, 6= bt[x := a] 6 Sol(C).Implication-fixability. implied(S, x, b) means every Sol(C) tx = b. Henceevery Sol(C), t[x := b] = Sol(C).Implication-inconsistency. implied(S, x, a) holds iff (tx 6= 6 Sol(C)), i.e., iffb \ {a} (tx = b 6 Sol(C)). rewrites b \ {a} inconsistent(S, x, b).VFixability-substitutability. Let = {a1 , .., ad }. i1..d substitutable(S, x, ai , b) iff((tx = a1 tx = ad ) Sol(C) t[x := b] Sol(C)), rewritesfixable(S, x, b).Inconsistency-substitutability. Suppose inconsistent(S, x, a) holds. solution tx =exists, hence implication tx = Sol(C) t[x := b] Sol(C) true choicesb.Inconsistency-removability. argument inconsistency-substitutability.Substitutability-removability. Suppose substitutable(S, x, a, b) holds value b 6= a.written b (tx = Sol(C) t[x := b] Sol(C)), impliesb(tx = Sol(C) t[x := b] Sol(C)). latter rewrites (tx =Sol(C) b t[x := b] Sol(C)).616fiA Unifying Framework Structural Properties CSPsNote also implied values determined variables strongly related problemsunique solution: problem unique solution, variablesimplied value (cf. Example 1), hence determined.2.5 Exploiting Properties Constraint Solvingimportant reason aforementioned properties interesting that, detected, allow us reduce search space removing values active domainsvariables. particular, several properties successfully exploitedpurpose, like inconsistency (Montanari, 1974; Mackworth, 1977), substitutability (Freuder,1991), irrelevance (Thiffault et al., 2004; Safarpour et al., 2004), implication (Monassonet al., 1999), dependence (Mancini & Cadoli, 2007). However, thanks unifying framework Figure 3, show wide interest aforementioned propertiesessentially relies relations two fundamental properties removabilityfixability.Theorem 2. Let CSP hX, D, Ci. value fixable variable x X,satisfiable CSP 0 = hX, D, C {x = a}i obtained instantiatingvariable x value satisfiable.Proof. Assume 0 satisfiable. exists X-tuple satisfiesconstraints. Since constraints 0 superset , also solution .direction, assume satisfiable. exists solutiont. Since value fixable x, follows t[x := a] solution well. t[x := a]satisfies also additional constraint x = 0 , hence latter problem satisfiablewell.Theorem 3. Let CSP hX, D, Ci. value removable variable x X,satisfiable CSP 0 = hX, D, C {x 6= a}i obtained removingvalue domain variable x satisfiable.Proof. 0 satisfiable, then, arguments proof Theorem 2,satisfiable well.direction, assume satisfiable. exists solution t.tx 6= a, course also solution 0 . hand, tx = a, since valueremovable x , follows exists b 6= t[x := b] solutionwell. t[x := b] satisfies also additional constraint x 6= 0 , hence latter problemalso satisfiable.results show key roles played fixability removability.ideal properties checked order reduce domain variable.interest notions essentially relies relationships fixabilityremovability. example, implied value interest essentially fixable,irrelevant variable interest essentially fixable value domain,substitutable value interest essentially removable, etc.Also interest inconsistent values relies fact removable. However, inconsistency much stronger removability, removing inconsistent values617fiBordeaux, Cadoli, & Manciniguarantees solutions (and satisfiability problem) preserved.Hence removability plays exactly role inconsistency case wantfind solutions problem simply want find one. situations removabilityideal property use.properties, worth noting that, although may appear strongunlikely first sight, still play precious role detected dynamicallysearch. example, Thiffault et al. (2004) show dynamically detectingvariables become irrelevant search (called dont care variables paper)greatly speed-up non-CNF SAT solvers, actually separating problems independentcomponents.3. Complexity reasoningsection show problem checking whether properties Definition 2 holdcoNP-complete. Hence, Section 3.2, try determine special cases checkingdone efficiently (i.e., polynomial time).3.1 Intractability Resultson, assume input given set constraints C setvariables X. also assume problem checking whether Sol(C) polynomialsize representation input. properties hold propositional logicCSPs, sense Dechter (1992).note problem checking whether properties Definition 2 holdcoNP, because, possible find counter-example guessing tuple(two, case dependency) non-deterministic polynomial time, checking,polynomial time, whether negation subformula parentheses holds (asinterchangeability, note logical two properties coNP stillcoNP). Alternatively, coNP-membership follows observing succinct certificates existproving various properties hold (as example, certificate provingvariable x fixable V -tuple Sol(C) t[x := a] 6 Sol(C)).rest section, proofs therefore restricted coNP-hardness part.Theorem 4 (coNP-completeness properties Definition 2). Given CSP, followingtasks coNP-complete:Checking whether value fixable, removable, inconsistent, implied, determinedvariable x X;Checking whether value substitutable to, interchangeable value bvariable x X;Checking whether variable X dependent variables set V X;Checking whether variable x X irrelevant.Proof. prove checking properties hard coNP, reduce coNP-completeproblem, i.e., checking arbitrary CSP unsatisfiable, problem618fiA Unifying Framework Structural Properties CSPschecking properties. particular, proofs hold even domain boolean,case CSP written propositional formula , e.g., CNF. Hence,let arbitrary propositional formula CNF set variables X, let xvariable x 6 X: unsatisfiability problem reducedproblem checking various properties. Moreover, semantical relationships definedTheorem 1 allow infer elegant way hardness results several properties startingothers.Irrelevance. Consider defined x. unsatisfiable unsatisfiable.show unsatisfiable x irrelevant formula . Let us firstassume unsatisfiable. follows x irrelevant , models.hand, let model . Interpretation {x true} model ,{x false} not, implying x irrelevant .Fixability, Substitutability. Results follow irrelevance, combined semantical relationships define irrelevance terms fixability, fixability termssubstitutability.Dependence. Consider defined (x x). unsatisfiableunsatisfiable, x dependent X unsatisfiable.Determinacy. result follows dependence, combined semanticalrelationship defines dependence terms determinacy.Implication. Consider defined x. Value false implied xunsatisfiable.Inconsistency. result follows implication, combined semanticalrelationship defines implication terms inconsistency.Removability. Consider defined x. Value true removable xunsatisfiable.proof Theorem 4, observed intractability checkingproperties holds also binary CSPs (i.e., CSPs constraints relatetwo variables).Theorem 5 (coNP-completeness properties Definition 2 binary CSPs). GivenCSP binary constraints domain size greater two, checkingproperties Definition 2 coNP-complete.Proof. give proof irrelevance only: others derived similarly.Let = hX, D, Ci binary CSP. Consider arbitrary variable x 6 X let barbitrary distinct values D. Let denote CSP hX 0 , D, C 0 X 0 = X {x},C 0 = C {x = a}. binary and, similarly proof Theorem 4, unsatisfiablevariable x irrelevant .observation CSP encoding graph 3-colourability problemmade using binary constraints, thesis follows, since checking unsatisfiabilityproblem (which coNP-hard) reduced checking irrelevance binary CSP.619fiBordeaux, Cadoli, & Mancini3.2 Tractability ResultsSince detecting properties interested computationally hard problem,natural question determine special cases checking done efficiently.end, investigate two approaches: exhibit syntactical restrictions makeproblem tractable, study local relaxations definitions polynomialtime checkable, therefore provide incomplete algorithms detecting variousproperties.3.2.1 Tractability Restricted Constraint Languagesnumber syntactical restrictions constraint satisfaction problem knownmake tractable. instance, case boolean constraints, i.e., propositional formulae, satisfiability problem becomes tractable instance expressed usingHorn clauses, dual Horn clauses (i.e., clauses one negative literal),clauses size 2, affine constraints (i.e., formulae built using XOR).known Schaefers (1978) classes. natural wonder propertiesidentified Definition 2 also easy determine classes formulae. indeed case them, give general condition tractableclasses consistency property also tractable properties framework.note recent paper (Jonsson & Krokhin, 2004) gives complete characterisationtractable cases related property.follows interested classes CSPs. end, define constraintlanguage domain finite set relations (of finite arity) elementsD, denote CSP(D ) set CSPs form hX, D, Ci every elementci = ri (Vi ) C Vi X ri . (We observe constraintlanguage fixed, domain instances CSP(D ) fixed well.)constraint language said closed instantiation (resp. complementation) whenever constraint ci = ri (Vi ) expressible language (i.e., ri ),relation X\{x} (x=a (ci )), (resp. complementation ci ) representedconjunction constraints language. means exist constraintsc01 = r10 (V10 ), . . . , c0k = rk0 (Vk0 ), Vj0 X rj0 j, X\{x} (x=a (ci ))(resp. ci ) equivalent c01 c0k .2 Well known constraint languages boolean domainsclosed instantiation complementation Horn clauses, dualHorn clauses, 2CNF clauses affine constraints (since instantiation complementHorn /dual Horn/2CNF clause affine constraint expressed conjunctionHorn /dual Horn/2CNF clauses affine constraints).Theorem 6. Given constraint language , satisfiability problem CSP() tractableclosed instantiation, problem checking determinacy CSPsclass CSP() tractable well.2. Note define closure respect complementation slightly different non-standardmeaning, negation constraint needs expressible conjunction constraints.definitions impose definable single constraint language.620fiA Unifying Framework Structural Properties CSPsProof. Let us consider arbitrary instance hX, D, Ci CSP(). Variable x Xdetermined exist two different domain values, b D,X\{x} (x=a (Sol(C))) X\{x} (x=b (Sol(C)))empty, i.e., one CSPs hX, D, Ca,b i, with:X = X \ {x},Ca,b = {X\{x} (x=a (c)) | c C} {X\{x} (x=b (c)) | c C},satisfiable. closed instantiation, constraints Ca,b writtenconjunctions constraints . Hence, reduced determinacy-testing problemsolving O(|D|2 ) instances CSP(), tractable.Theorem 7. Given constraint language , satisfiability problem CSP() tractableclosed instantiation complementation, problem checkingproperty among fixability, substitutability, interchangeability, inconsistency irrelevanceCSPs class CSP() tractable well.Proof. start substitutability property note that, given arbitrary instancehX, D, Ci CSP(), value substitutable b variable x XX\{x} (x=a (Sol(C))) X\{x} (x=b (Sol(C))).inclusion false, i.e., substitutability hold, setX\{x} (x=a (Sol(C))) X\{x} (x=b (Sol(C)))(1)non-empty. Since x=b (Sol(C)) = x=b ( cC Sol(c)) =cC (x=b (Sol(c))), have:X\{x} (x=b (Sol(C))) = X\{x}cC (x=b (Sol(c))).Although projection intersection relations equal intersectionprojections general, latter rewrites to:\X\{x} (x=b (Sol(c))).cCdue fact select x eliminating byTprojection. proveinclusion hold general: suppose cC X\{x} (x=b (Sol(c))).means c C, exists tuple tc tc |X\{x} = tc x=b (Sol(c)).follows tcx = b indeed uniquetx = b tc |X\{x} =c C (t x=b (Sol(c))), i.e., X\{x} ( cC (x=b (Sol(c)))).Formula (1) therefore equivalent to:\X\{x} (x=a (Sol(c)))cC[cC621X\{x} (x=b (Sol(c)))fiBordeaux, Cadoli, & Mancinisolution exists (and therefore substitutability hold) one sets\X\{x} (x=a (Sol(c))) X\{x} (x=b (Sol(c)))cCobtained every c C solution. language closed instantiation complement, express new constraint X\{x} (x=b (Sol(c))) conjunction C 0 constraints . sets solution iff CSP hX \ {x}, D, {X\{x} (x=a (c)) | cC} C 0 satisfiable. reduced substitutability testing problem solving |C|instances constraint satisfaction problem whose constraints language ,tractable.results fixability, interchangeability irrelevance follow directly,semantical relationships shown Figure 3. Consistency value variable xdirectly expressed satisfiability X\{x} (x=a (Sol(C))), expressed, proof implication follows result.slightly different closure property needed removability value variable xsince expressed X\{x} (Sol(C)) X\{x} (x6=a (Sol(C))).Nevertheless, since boolean domains value v removable v substitutablev, remarks closure properties Schaefers classes previoustheorem, obtain that:Corollary 1. Testing fixability, substitutability, interchangeability, inconsistency, determinacy, irrelevance removability tractable boolean CSP constraintseither Horn clauses, dual Horn clauses, clauses size two affine constraints.Unfortunately, dont tractability results dependence.Table summarizes Theorems 6 7, Corollary 1:PropertyDeterminacyFixability, substitutability,interchangeability, inconsistency, irrelevanceRemovabilityPolynomialTractable closed instantiationTractable closed inst. compl.Boolean Schaeferworth noting conditions become restrictive reading table top-down.Moreover, cases, observed tractability various propertydetection problems derives tractability constraint language . leadsserious concerns practical applicability results: CSP() tractable,worrying identifying properties? Actually, preliminary studies show better results unlikely hold: example, proven constraintlanguage intractable, hope detecting properties like fixability, irrelevance, substitutability, inconsistency polynomial time. results become622fiA Unifying Framework Structural Properties CSPsinterest, suggesting two main directions research: first courseinvestigating practical benefit detecting properties real cases; secondexploit sufficient efficiently evaluable conditions properties hold,regarded form approximate reasoning. One used forms kindreasoning called local reasoning, addressed below.3.2.2 Tractability Localityimportant class incomplete criteria determine polynomial time whether complex property holds based local reasoning. approach proved extremelysuccessful consistency (Mackworth, 1977) interchangeability (Freuder, 1991) (cf.also Choueiry & Noubir, 1998, classification different local forms interchangeability studied classified). propose section systematic investigationwhether local approach used value-based properties.Verifying property P (C) set constraints C locally means verifyproperty well-chosen number sub-problems. must ensure approachsound considered property:Definition 4 (soundness local reasoning). say local reasoningproperty Psound if, subsets constraints C1 C, . . . , Ck C i1..k Ci = C,(depending property):WVP (C).P (C)i1..k P (Ci )i1..k P (Ci )typical choice granularity simply consider Ci contains one constraints C done, instance, arc-consistency. extreme, takeunique C1 = C, global checking. two extremes, wide rangeintermediate levels defined (cf., e.g., Freuder, 1978, 1991).Example 5. Consider CSP hX, D, Ci X = {x, y, z}, = {0, 1, 2} C ={c1 , c2 , c3 }, whose elements defined follows:x0 11 22 12 2c1 (x, y)x z1 01 22 02 2c2 (x, z)z1 11 22 12 2c3 (y, z)observed value 1 substitutable 2 variable x. order checkproperty locally, consider suitable covering C1 , . . . , Ck set C, verifyinduced subproblems. example, taking C1 = {c1 }, C2 = {c2 }, C3 = {c3 },substitutableCi (S, x, 1, 2) every {1, 2, 3}. Since local reasoning sound substitutability (cf. Freuder, 1991), infer global property substitutableC (S, x, 1, 2)holds.Reasoning locally typically tractable focus moderate number subsets C,condition bound complexity reasoning623fiBordeaux, Cadoli, & Mancinisubsets. typical assumption CSP bound arity constraints,every constraint instance binary. case, cost determiningproperty constraint polynomial; choose reason locally consideringconstraint separately, taking groups constraints bounded size, localchecking tractable.Theorem 8. Local reasoning sound properties substitutability, interchangeability, fixability, inconsistency, implication, irrelevance,determinacy, dependence. particuSlar, C1 C, . . . , Ck C i1..k Ci = C:VsubstitutableC (S, x, a, b);i1..k substitutableCi (S, x, a, b)VinterchangeableC (S, x, a, b);i1..k interchangeableCi (S, x, a, b)fixableC (S, x, b);WinconsistentC (S, x, a);i1..k inconsistentCi (S, x, a)WimpliedC (S, x, a);i1..k impliedCi (S, x, a)Vi1..kfixableCi (S, x, b)irrelevantC (S, x);WdeterminedC (S, x);i1..k determinedCi (S, x)WdependentC (S, V, y).i1..k dependentCi (S, V, y)Vi1..kirrelevantCi (S, x)Proof. result well-known consistency (Mackworth, 1977), substitutabilityinterchangeability (Freuder, 1991). Fixability variable x value b expressed6= b (substitutableC (S, x, a, b))VV VTherefore,(S,x,b)(whichequivalentCi1..k fixablea6=bVVisubstitutable(S,x,a,b)substitutable(S,x,a,b)),CiCia6=bVsubstitutable(S,x,a,b),meansfixable(S,x,b).similarargumentworksCCa6=birrelevance, analogously defined terms fixability (cf. Figure 3).implication, value implied variable x Ci , tuples tx 6=violate constraints Ci fortiori belong Sol(C). Similarly determinacy: variable x determined Ci , tuples Sol(Ci )t[x := b] 6 Sol(Ci ) b 6= tx . Hence cannot solution Sol(C)t[x := b] Sol(C) b 6= tx . Finally, dependence, exists CidependentC (S, V, y) holds (it enough consider sets constraints CiV var(Ci ) 6= var(Ci ) dependentC (S, V var(Ci ), y)),have, definition, t, t0 Sol(Ci ) (x V var(Ci ) (tx = t0x )) ty = t0y ,dependentC (S, V, y) also holds, since solution whole problem must satisfy alsoCi .Example 6 (Example 5 continued). Value 2 fixable z. inferredperforming local reasoning follows:624fiA Unifying Framework Structural Properties CSPsfixableC1 (S, z, 2) holds, since z occur scope c1 ;fixableC2 (S, z, 2) holds, since tuple belongs c2 , also t[z := 2] belongsc2 ;fixableC3 (S, z, 2) holds, argument.Since Theorem 8 local reasoning sound fixability, infer fixableC (S, z, 2)holds.one (value-based) property, namely removability, local approachunfortunately sound:Theorem 9. Local reasoning sound removability property.Proof. Take C = C1 C2 , C1 defined x C2 x y. Supposedomain values {1, 2, 3}. Value 2 x removable constraints consideredindependently since, cases, change value solution assigns 2x another value. Still, value 2 removable x conjunction. seewhy, consider solution hx, yi = h2, 2i, observe neither h1, 2i h3, 2isolutions.Note removing values shown removable locally even makesatisfiable problem unsatisfiable: furthermore add constraints C3 , definedx 6= 1 C4 , defined x 6= 3, value 2 x removable constraint,(global) solution actually assigns value 2 x.result, although negative, fact interesting, gives ex-post justification extensive use made last decades stronger notions, likeinconsistency substitutability, imply removability (cf. Figure 3). mainreason current tools frameworks CP try detect properties orderremove values active domain variables. naturally relies removability property (cf. Section 2.5). However, reason removability directlyused intractable. reason, stronger notions like consistency substitutability forms removability commonly used. Actually,unlike full-fledged removability (cf. Theorem 9), properties detected efficiently,incompletely, local reasoning. Hence, raises interesting open issue:exist new (i.e., substitutability inconsistency) propertieslocal reasoning sound imply removability?end section noting local version fixability property indeedgeneralisation arbitrary domains pure literal rule (Davis & Putnam, 1960)well-known case boolean constraints conjunctive normal form. pureliteral rule exploits cases constraint (clause) problem positive(resp. negative) occurrence variable x. case, assigning value 0 (resp. 1) xpreserves satisfiability problem: solution tx = 1 exists, t[x := 0]also solution since clause constrains x value 1.625fiBordeaux, Cadoli, & ManciniExample 7. Consider following propositional formula CNF:(x z) (x z) (y z)Since x occur clause, assign x 1, maintain satisfiabilityformula: solution , assignment t[x := 1] solution well.clear pure literal rule detects fixability based reasoning localclause (a variable x fixable to, say, 1 clause iff clause contain literalx, pure literal rule checks condition holds every constraint).generalisation pure literal rule has, best knowledge, proposedCSP, generalisation pure literal rule QBF applied solversQuantified CSP name pure value rule (Gent, Nightingale, & Stergiou, 2005).observed proposal fact local relaxation generalisationquantified constraints fixability. shortly discussed Section 4,properties presented paper generalized Quantified CSP elegant way,many local relaxations remain valid.4. Conclusions Perspectivespaper reconsidered structural properties CSPs extensively studiedexploited order simplify search. properties may course useful alsotasks, e.g., classification update solutions, compacting solutionspace, supporting explanation interaction users.provided unifying framework properties, clarifies semanticalrelationships allows new ones emerge. argued new notions,namely fixability removability play key role deep understanding topic,ideal characterisations values fixed removed preservingsatisfiability problem. Known properties, like inconsistency substitutabilitythus suitable specialisations them.tackled questions related automated detection different properties exploitation solving engine simplifying problems. particular,showed detecting proposed properties general hard originalCSP. Hence, order find efficient ways verification, investigated, accordingCSP approach, two main lines: addition suitable restrictions constraint language approximation reasoning task exploiting local versions variousnotions. Moreover, discussed cases properties may arise explicitpromises made users. case problems properties functionaldependencies unique solutions.Two perspectives raised work concern new central propertiesemerged it. identified removability property ideal characterisation values removed preserving satisfiability. Unfortunately,negative results (coNP-completeness detection property impossibility local reasoning) make impossible directly use removability property practice.gives ex-post justification extensive use made last decadesstronger notions (like inconsistency substitutability) imply removability, yet626fiA Unifying Framework Structural Properties CSPschecked tractable means (of course price losing completeness). interestingproblem thus determine new cases removability-checking tractable.Also, benefits fixability long known boolean case, sinceproperty used form pure literal rule many SAT solvers. However,generalisation property CSPs considered far.Finally, proposed framework allows natural elegant generalisation caseQuantified CSP. particular, related work (Bordeaux, Cadoli, & Mancini, 2008)propose new notion outcome natural counterpart quantified levelconcept solution CSP. notion mind, properties studiedpaper straightforwardly restated Quantified CSP, well localrelaxations, new, even general concepts emerge (the so-called shallow properties,may impact also pure existential CSP level). opens importantnew horizons, allowing QCSP solvers perform smarter reasoning input problem,taking proper account quantifiers prefix, today usually ignored.Acknowledgmentspaper extended revised version Bordeaux, Cadoli, Mancini (2004).ReferencesBordeaux, L., Cadoli, M., & Mancini, T. (2004). Exploiting fixable, substitutable determined values constraint satisfaction problems. Baader, F., & Voronkov, A.(Eds.), Proceedings Eleventh International Conference Logic Programming Automated Reasoning (LPAR 2004), Vol. 3452 Lecture Notes ComputerScience, pp. 270284, Montevideo, Uruguay. Springer.Bordeaux, L., Cadoli, M., & Mancini, T. (2008). Generalizing consistency constraint properties quantified constraints. ACM Transactions ComputationalLogic. appear.Cadoli, M., & Mancini, T. (2007). Using theorem prover reasoning constraintproblems. Applied Artificial Intelligence, 21 (4/5), 383404.Calabro, C., Impagliazzo, R., Kabanets, V., & Paturi, R. (2003). complexity Uniquek-SAT: isolation lemma k-CNFs. Proceedings Eighteenth IEEE Conference Computational Complexity (CCC 2003), p. 135 ff., Aarhus, Denmark. IEEEComputer Society Press.Chang, C. C., & Keisler, H. J. (1990). Model Theory, 3rd ed. North-Holland.Choueiry, B. Y., & Noubir, G. (1998). computation local interchangeabilityDiscrete Constraint Satisfaction Problems. Proceedings Fifteenth NationalConference Artificial Intelligence (AAAI98), pp. 326333, Madison, WI, USA.AAAI Press/The MIT Press.Crawford, J. M. (1992). theoretical analysis reasoning symmetry first-orderlogic (extended abstract). Proceedings Workshop Tractable Reasoning,conjunction Tenth National Conference Artificial Intelligence (AAAI92),San Jose, CA, USA.627fiBordeaux, Cadoli, & ManciniCrawford, J. M., Ginsberg, M. L., Luks, E. M., & Roy, A. (1996). Symmetry-breakingpredicates search problems. Proceedings Fifth International ConferencePrinciples Knowledge Representation Reasoning (KR96), pp. 148159,Cambridge, MA, USA. Morgan Kaufmann, Los Altos.Davis, M., & Putnam, H. (1960). computing procedure Quantification Theory. JournalACM, 7 (3), 201215.Dechter, R. (1992). Constraint networks (survey). Encyclopedia Artificial Intelligence,2nd edition, pp. 276285. John Wiley & Sons.Even, S., Selman, A., & Yacobi, Y. (1984). complexity promise problemsapplications public-key cryptography. Information Control, 61 (2), 159173.Freuder, E. C. (1978). Synthesizing constraint expressions. Communications ACM,21 (11), 958966.Freuder, E. C. (1991). Eliminating interchangeable values Constraint Satisfaction Problems. Proceedings Ninth National Conference Artificial Intelligence(AAAI91), pp. 227233, Anaheim, CA, USA. AAAI Press/The MIT Press.Gent, I. P., & Smith, B. (2000). Symmetry breaking search constraint programming. Proceedings Fourteenth European Conference Artificial Intelligence(ECAI 2000), pp. 599603, Berlin, Germany.Gent, I., Nightingale, P., & Stergiou, K. (2005). QCSP-Solve: solver QuantifiedConstraint Satisfaction Problems. Proceedings Nineteenth International JointConference Artificial Intelligence (IJCAI 2005), pp. 138143, Edinburgh, Scotland.Morgan Kaufmann, Los Altos.Giunchiglia, E., Massarotto, A., & Sebastiani, R. (1998). Act, rest follow:Exploiting determinism planning satisfiability. Proceedings FifteenthNational Conference Artificial Intelligence (AAAI98), pp. 948953, Madison, WI,USA. AAAI Press/The MIT Press.Jonsson, P., & Krokhin, A. (2004). Recognizing frozen variables constraint satisfactionproblems. Theoretical Computer Science, 329 (13), 93113.Jonsson, P., & Krokhin, A. (2008). Computational complexity auditing discrete attributesstatistical databases. Journal Computer System Sciences. appear.Kobler, J., Schoning, U., & Toran, J. (1993). graph isomorphism problem: computational complexity. Birkhauser Press.Lal, A., Choueiry, B., & Freuder, E. C. (2005). Interchangeability dynamic bundlingnon-binary finite CSPs. Proceedings Twentieth National ConferenceArtificial Intelligence (AAAI 2005), pp. 397404, Pittsburgh, PA, USA. AAAIPress/The MIT Press.Lenstra, A., & Lenstra, H. W. (1990). Algorithms number theory. van Leeuwen, J.(Ed.), Handbook Theoretical Computer Science, vol. 1: Algorithms Complexity. MIT Press.628fiA Unifying Framework Structural Properties CSPsLi, C. M. (2000). Integrating equivalency reasoning Davis-Putnam procedure.Proceedings Seventeenth National Conference Artificial Intelligence(AAAI 2000), pp. 291296, Austin, TX, USA. AAAI Press/The MIT Press.Mackworth, A. K. (1977). Consistency networks relations. Artificial Intelligence, 8,99118.Mancini, T., & Cadoli, M. (2005). Detecting breaking symmetries reasoningproblem specifications. Proceedings Sixth International Symposium Abstraction, Reformulation Approximation (SARA 2005), Vol. 3607 Lecture NotesArtificial Intelligence, pp. 165181, Airth Castle, Scotland, UK. Springer.Mancini, T., & Cadoli, M. (2007). Exploiting functional dependencies declarative problemspecifications. Artificial Intelligence, 171 (1617), 9851010.Mancini, T., Cadoli, M., Micaletto, D., & Patrizi, F. (2008). Evaluating ASP commercialsolvers CSPLib. Constraints, 13 (4).Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determining computational complexity characteristic phase transitions. Nature, 400,133137.Montanari, U. (1974). Networks constraints: Fundamental properties applicationspicture processing. Information Sciences, 7 (2), 85132.Papadimitriou, C. H. (1994). Computational Complexity. Addison Wesley Publishing Company, Reading, Massachussetts, Reading, MA.Pyhala, T. (2004). Factoring benchmarks SAT solvers. Tech. rep., Helsinki universitytechnology.Safarpour, S., Veneris, A., Drechsler, R., & Lee, J. (2004). Managing dont cares BooleanSatisfiability. Proceedings Design Automation Test Conference Europe(DATE 2004), pp. 260265, Paris, France. IEEE Computer Society Press.Schaefer, T. J. (1978). complexity satisfiability problems. Proceedings TenthACM Symposium Theory Computing (STOC78), pp. 216226, San Diego, CA,USA. ACM Press.Thiffault, C., Bacchus, F., & Walsh, T. (2004). Solving non-clausal formulas DPLLsearch. Proceedings Tenth International Conference Principles Practice Constraint Programming (CP 2004), Vol. 3258 Lecture Notes ComputerScience, pp. 663678, Toronto, Canada. Springer.Valiant, L. G., & Vijay V. Vazirani, V. V. (1986). NP easy detecting uniquesolutions. Theoretical Computer Science, 47 (3), 8593.629fiJournal Artificial Intelligence Research 32 (2008) 419 - 452Submitted 11/07; published 06/08Dynamic Control Real-Time Heuristic SearchVadim BulitkoBULITKO @ UALBERTA . CADepartment Computing Science, University AlbertaEdmonton, Alberta, T6G 2E8, CANADAMitja LustrekMITJA . LUSTREK @ IJS . SIDepartment Intelligent Systems, Jozef Stefan InstituteJamova 39, 1000 Ljubljana, SLOVENIAJonathan SchaefferJONATHAN @ CS . UALBERTA . CADepartment Computing Science, University AlbertaEdmonton, Alberta, T6G 2E8, CANADAYngvi BjornssonYNGVI @ RU .School Computer Science, Reykjavik UniversityKringlan 1, IS-103 Reykjavik, ICELANDSverrir SigmundarsonSVERRIR . SIGMUNDARSON @ LANDSBANKI .Landsbanki London Branch, Beaufort House,15 St Botolph Street, London EC3A 7QR, GREAT BRITAINAbstractReal-time heuristic search challenging type agent-centered search agentsplanning time per action bounded constant independent problem size. common problemimposes restrictions pathfinding modern computer games large numberunits must plan paths simultaneously large maps. Common search algorithms (e.g., A*,IDA*, D*, ARA*, AD*) inherently real-time may lose completeness constantbound imposed per-action planning time. Real-time search algorithms retain completenessfrequently produce unacceptably suboptimal solutions. paper, extend classicmodern real-time search algorithms automated mechanism dynamic depth subgoalselection. new algorithms remain real-time complete. large computer game maps,find paths within 7% optimal average expanding roughly single state per action.nearly three-fold improvement suboptimality existing state-of-the-art algorithmsand, time, 15-fold improvement amount planning per action.1. Introductionpaper study problem agent-centered real-time heuristic search (Koenig, 2001).distinctive property search agent must repeatedly plan execute actionswithin constant time interval independent size problem solved.restriction severely limits range applicable heuristic search algorithms. instance, staticsearch algorithms A* (Hart, Nilsson, & Raphael, 1968) IDA* (Korf, 1985), re-planningalgorithms D* (Stenz, 1995), anytime algorithms ARA* (Likhachev, Gordon, &Thrun, 2004) anytime re-planning algorithms AD* (Likhachev, Ferguson, Gordon,Stentz, & Thrun, 2005) cannot guarantee constant bound planning time per action. LRTA*c2008AI Access Foundation. rights reserved.fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONcan, potentially low solution quality due need fill heuristic depressions (Korf,1990; Ishida, 1992).motivating example, consider autonomous surveillance aircraft context disaster response (Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjou, & Shimada, 1999).surveying disaster site, locating victims, assessing damage, aircraft ordered flyparticular location. Radio interference may make remote control unreliable thereby requiringcertain degree autonomy aircraft using AI. task presents two challenges. First,due flight dynamics, AI must control aircraft real time, producing minimum numberactions per second. Second, aircraft needs reach target location quickly due limitedfuel supply need find rescue potential victims promptly.study simplified version problem captures two AI challenges abstracting away robot-specific details. Specifically, line work real-time heuristicsearch (e.g., Furcy & Koenig, 2000; Shimbo & Ishida, 2003; Koenig, 2004; Botea, Muller, & Schaeffer, 2004; Hernandez & Meseguer, 2005a, 2005b; Likhachev & Koenig, 2005; Sigmundarson &Bjornsson, 2006; Koenig & Likhachev, 2006) consider agent finite search graphtask traveling path current state given goal state. Within context measureamount planning agent conducts per action length path traveledstart goal locations. two measures antagonistic reducing amount planning per action leads suboptimal actions results longer paths. Conversely, shorter pathsrequire better actions obtained larger planning effort per action.use navigation grid world maps derived computer games testbed. games,agent tasked go location map current location. Examples includereal-time strategy games (e.g., Blizzard, 2002), first-person shooters (e.g., id Software, 1993),role-playing games (e.g., BioWare Corp., 1998). Size complexity game maps wellnumber simultaneously moving units maps continues increase every new generation games. Nevertheless, game unit agent must react quickly users commandregardless maps size complexity. Consequently, game companies impose time-peraction limit pathfinding algorithms. instance, Bioware Corp., major game companycollaborate with, sets limit 1-3 ms units computing paths time.Search algorithms produce entire solution agent takes first action (e.g., A*Hart et al., 1968) lead increasing action delays map size increases. Numerous optimizationssuggested remedy problems decrease delays (for recent example deployed forthcoming computer game refer Sturtevant, 2007). Real-time search addressesproblem fundamentally different way. Instead computing complete, possibly abstract, solution first action taken, real-time search algorithms compute (or plan)first actions agent take. usually done conducting lookahead search fixeddepth (also known search horizon, search depth lookahead depth) around agentscurrent state using heuristic (i.e., estimate remaining travel cost) select nextactions. actions taken planning-execution cycle repeats (e.g., Korf, 1990).Since goal state reached local searches, agent runs risks headingdead end or, generally, selecting suboptimal actions. address problem, real-timeheuristic search algorithms update (or learn) heuristic function experience. existingalgorithms constant amount planning (i.e., lookahead search) per action. result,tend waste CPU cycles heuristic function fairly accurate and, conversely, planenough heuristic function particularly inaccurate. Additionally, compute heuris420fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHtic respect distant global goal state put unrealistic requirements heuristicaccuracy demonstrate paper.paper address problems making following three contributions. First,propose two ways selecting lookahead search depth dynamically, per action basis. Second,propose way selecting intermediate subgoals per action basis. Third, applyextensions classic LRTA* (Korf, 1990) state-of-the-art real-time PR LRTS (Bulitko,Sturtevant, Lu, & Yau, 2007) demonstrate improvements performance. resultingalgorithms new state art real-time search. illustrate, large computer gamemaps new algorithms find paths within 7% optimal expanding single stateaction. comparison, previous state-of-the-art, PR LRTS, 15 times slower peraction finding paths two three times suboptimal. Furthermore,dynamically controlled LRTA* PR LRTS one two orders magnitude faster per actionA*, weighted A* state-of-the-art Partial Refinement A* (PRA*) (Sturtevant & Buro,2005). Finally, unlike A* modern extensions used games, new algorithms provablyreal-time slow maps become larger.rest paper organized follows. Section 2 formulate problem real-timeheuristic search show core LRTA* algorithm extended dynamic lookaheadsubgoal selection. Section 3 analyzes related research. Section 4 provides intuition dynamiccontrol search. Section 5 describe two approaches dynamic lookahead selection: onebased induction decision-tree classifiers (Section 5.1) one based precomputing depthtable using state abstraction (Section 5.2). Section 6 present approach selecting subgoalsdynamically. Section 7 evaluates efficiency extensions domain pathfinding.conclude discussion applicability new approach general planning.paper extends conference publication (Bulitko, Bjornsson, Lustrek, Schaeffer, & Sigmundarson, 2007) new set features decision tree approach, new way selectingsubgoals, additional real-time heuristic search algorithm (PR LRTA*) extended dynamiccontrol, numerous additional experiments detailed presentation.2. Problem Formulationdefine heuristic search problem directed graph containing finite set states weightededges, single state designated goal state. every time step, search agent singlecurrent state, vertex search graph, takes action traversing out-edge currentstate. edge positive cost associated it. total cost edges traversed agentstart state arrives goal state called solution cost. require algorithmscomplete produce path start goal finite amount time path exists.order guarantee completeness real-time heuristic search make assumption safeexplorability search problems. Namely, costs finite goal state reachablestate agent possibly reach start state.Formally, algorithms discussed paper applicable heuristic search problem. keep presentation focused intuitive well afford large-scale empiricalevaluation, use particular type heuristic search problems, pathfinding grid worlds,rest paper. However, discuss applicability new methods suggestheuristic search problems Section 5.3 general planning problems Section 9.421fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONcomputer-game map settings, states vacant square grid cells. cell connectedfour cardinally (i.e., west, north, east, south) four diagonally neighboring cells. Outboundedges vertex moves available corresponding cell rest paperuse terms action move interchangeably. edge costs 1 cardinal moves2 diagonal moves. agent plans next action considering states local search spacesurrounding current position. heuristic function (or simply heuristic) estimates (remaining)travel cost state goal. used agent rank available actions selectpromising one. paper consider admissible heuristic functionsoverestimate actual remaining cost goal. agent modify heuristic functionstate avoid getting stuck local minima heuristic function, well improve actionselection experience.defining property real-time heuristic search amount planning agentper action upper bound depend problem size. enforce propertysetting real-time cut-off amount planning action. algorithm exceedscut-off discarded. Fast planning preferred guarantees agents quick reactionnew goal specification changes environment. measure mean planning time per actionterms CPU time well machine-independent measure number states expandedplanning. state called expanded successor states considered/generatedsearch. second performance measure study sub-optimality defined ratiosolution cost found agent minimum solution cost. Ratios close one indicatenear-optimal solutions.core real-time heuristic search algorithms algorithm called Learning RealTime A* (LRTA*) (Korf, 1990). shown Figure 1 operates follows. long goalstate sglobal goal reached, algorithm interleaves planning execution lines 4 7.generalized version added new step line 3 selecting search depth goal sgoalindividually execution step (the original algorithm uses fixed sglobal goal planningsearches). line 4, d-ply breadth-first search duplicate detection used find frontier statesprecisely actions away current state s. frontier state s, value sumcost shortest path s, denoted g(s, s), estimated cost shortest pathsgoal (i.e., heuristic value h(s, sgoal )). use standard path-max technique (Mero,1984) deal possible inconsistencies heuristic function computing g + h values.result, g + h values never decrease along branch lookahead tree. stateminimizes sum identified sfrontier line 5. heuristic value current stateupdated line 6 (we keep separate heuristic tables different goals). Finally, take one steptowards promising frontier state sfrontier line 7.3. Related Researchalgorithms single-agent real-time heuristic search use fixed search depth, notableexceptions. Russell Wefald (1991) proposed estimate utility expanding state usecontrol lookahead search on-line. one needs estimate likely additional searchchange actions estimated value. Inaccuracies estimates overhead metalevel control led reasonable unexciting benefits combinatorial puzzle pathfinding.additional problem relatively low branching factor combinatorial puzzles makesdifficult eliminate parts search space early on. problem likely occur grid422fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHLRTA*(sstart , sglobal goal )1 sstart2 6= sglobal goal3select search depth goal sgoal4expand successor states actions away, generating frontier5find frontier state sfrontier lowest g(s, sfrontier ) + h(sfrontier , sgoal )6update h(s, sgoal ) g(s, sfrontier ) + h(sfrontier , sgoal )7change one step towards sfrontier8 endFigure 1: LRTA* algorithm dynamic control.based pathfinding. Finally, method adds substantial implementation complexity requiresnon-trivial changes underlying search algorithm. contrast, approach search depthselection easily interfaced real-time search algorithm search depth parameterwithout modifying existing code.Ishida (1992) observed LRTA*-style algorithms tend get trapped local minimaheuristic function, termed heuristic depressions. proposed remedy switch limitedA* search heuristic depression detected use results A* searchcorrect depression once. different approach two ways: first,need mechanism decide switch real-time A* search thus avoidneed hand-tune control parameters Ishidas control module. Instead, employ automatedapproach decide search horizon depth every action. Additionally, spend extratime filling heuristic values within heuristic depression A* estimates.Bulitko (2003a) showed optimal search depth selection highly beneficial realtime heuristic search. linked benefits avoiding so-called lookahead pathologiesdeeper lookahead leads worse moves suggest practical way selecting lookahead depth dynamically. way proposed 2004 via use generalized definitionheuristic depressions (Bulitko, 2004). proposed algorithm extends search horizon incrementally search finds way depression. actions leading foundfrontier state executed. cap search horizon depth set user. idea precomputing depth table heuristic values real-time pathfinding first suggested LustrekBulitko (2006). paper extends work follows: (i) introduce intermediate goals,(ii) propose alternative approach require map-specific pre-computation (iii)extend evaluate state-of-the-art algorithm addition classic LRTA*.long tradition search control two-player search. High-performance game-playingprograms games like chess checkers rely extensively search decide actionstake. search performed strict real-time constraints programs typicallyminutes seconds deliberating next action. Instead using fixed-depth lookahead strategy programs employ sophisticated search control mechanisms maximizingquality action decisions within given time constraints. search control techniquescoarsely divided three main categories: move ordering, search extensions/reductions,time allotment. One earlier works dynamic move ordering history heuristic technique (Schaeffer, 1989), recent attempts include work training neural networks (Kocsis, 2003). exist large variety techniques adjusting search horizon423fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONdifferent branches within game tree; interesting continuations explored deeplyless promising ones terminated prematurely. Whereas early techniquesstatic, research focus shifted towards dynamic control well using machine-learningapproaches automatic parameterization (Buro, 2000; Bjornsson & Marsland, 2003). bestknowledge, none techniques applied single-agent real-time search.4. Intuition Dynamic Search Controlobserved literature common heuristic functions uniformly inaccurate (Pearl, 1984). Namely, tend accurate closer goal state less accuratefarther away. intuition fact follows: heuristic functions usually ignore certain constraints search space. instance, Manhattan distance heuristic sliding tile puzzlewould perfectly accurate tiles could pass other. Likewise, Euclidian distancemap ignores obstacles. closer state goal fewer constraints heuristic functionlikely ignore and, result, accurate (i.e., closer optimal solution cost)heuristic likely be.intuition motivates adaptive search control real-time heuristic search. First, heuristic values inaccurate, agent conduct deeper lookahead search compensateinaccuracies maintain quality actions. Deeper lookaheads generally foundbeneficial real-time heuristic search (Korf, 1990), though lookahead pathologies (i.e., detrimentaleffects deeper lookaheads action quality) observed well (Bulitko, Li, Greiner, &Levner, 2003; Bulitko, 2003b; Lustrek, 2005; Lustrek & Bulitko, 2006). illustration, considerFigure 2. Every state map shaded according minimum lookahead depthLRTA* agent use select optimal action. Darker shades correspond deeper lookaheaddepths. Notice many areas bright white, indicating shallowest lookahead depthone sufficient. use intuition first control mechanism: dynamic selectionlookahead depth Section 5.Figure 2: partial grid world map computer game Baldurs Gate (BioWare Corp., 1998).Shades grey indicate optimal search depth values white representing one ply.Completely black cells impassable obstacles (e.g., walls).424fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHDynamic search depth selection helps eliminate wasted computation switching shallowerlookahead heuristic function fairly accurate. Unfortunately, helpheuristic function grossly inaccurate. Instead, calls deep lookahead order selectoptimal action. deep search tremendously increases planning time and, sometimes, leadsviolating real-time cut-off planning time per move. address issue, Section 6propose second control mechanism: dynamic selection subgoals. idea straightforward:far goal leads grossly inaccurate heuristic values, let us move goal closeragent, thereby improving heuristic accuracy. computing heuristic functionrespect intermediate, thus nearby, goal opposed distant global goalfinal destination agent. Since intermediate goal closer global goal, heuristicvalues states around agent likely accurate thus search depth pickedfirst control mechanism likely shallower. agent gets intermediate goal,next intermediate goal selected agent makes progress towards actual global goal.5. Dynamic Search Depth SelectionFirst, define optimal search depth follows. (s, sglobal goal ) state pair, true optimal action (s, sglobal goal ) take edge lies optimal path sglobal goal (thereone optimal action). (s, sglobal goal ) known, run series progressivelydeeper LRTA* searches state s. shallowest search depth yields (s, sglobal goal )optimal search depth (s, sglobal goal ). may search depth forfeit LRTA*s real-timeproperty also impractical compute. Thus, following subsections present twodifferent practical approaches approximating optimal search depth. equips LRTA*dynamic search depth selection (i.e., realizing first part line 3 Figure 1). firstapproach uses decision-tree classifier select search depth based features agentscurrent state recent history. second approach uses pre-computed depth database basedautomatically built state abstraction.5.1 Decision-Tree Classifier Approacheffective classifier needs input features useful predicting optimal searchdepth, also efficiently computable agent real time. features useclassifier selected compromise two considerations, well domain independent. features calculated based properties states agent recentlyvisited, well features gathered shallow pre-search agents current state. Examplefeatures are: distance state agent n steps ago, estimate distanceagents goal, number states visited pre-search phase updated heuristics.Appendix features listed rationale behind explained.classifier predicts optimal search depth current state. optimal depthshallowest search depth returns optimal action. training classifier must thus labeltraining states optimal search depths. However, avoid pre-computing optimal actions,make simplifying assumption deeper search always yields better action. Consequently,training phase agent first conducts lookahead search pre-defined maximum depth, dmax ,derive optimal action (under assumption). choice maximum depth domaindependent would typically set largest depth still guarantees search returnwithin acceptable real-time requirement task hand. series progressively425fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONshallower searches performed determine shallowest search depth, dDT , still returnsoptimal action. process, given depth action returned differsoptimal action, progression stopped. enforces depths dDT dmaxagree best action. important improving overall robustness classification,classifier must generalize large set states. depth dDT set class labelvector features describing current state.classifier choosing lookahead depth, LRTA* augmented(line 3 Figure 1). overhead using classifier consists time required collectingfeatures running classifier. overhead negligible classifierimplemented handful nested conditional statements. Collecting features takessomewhat time but, careful implementation, overhead made negligiblewell. Indeed, four history-based features efficiently computed small constant time,keeping lookahead depth pre-search small (e.g., one two) overhead collectingpre-search features usually dwarfed time planning phase (i.e., lookahead search)takes. process gathering training data building classifier carried off-linetime overhead thus lesser concern.5.2 Pattern Database Approachnave approach would precompute optimal depth (s, sgoal ) state pair.two problems approach. First, (s, sgoal ) priori upper-bounded independentlymap size, thereby forfeiting LRTA*s real-time property. Second, pre-computing (s, sgoal )(s, sgoal ) pairs (s, sgoal ) states on, instance, 512 512 cell computer game mapprohibitive time space complexity. solve first problem capping (s, sgoal )fixed constant c 1 (henceforth called cap). solve second problem using automatically built abstraction original search space. entire map partitioned regions (orabstract states) single search depth value pre-computed pair abstract states. run-time single search depth value shared children abstract state pair (Figure 3).search depth values stored table refer pattern database PDBshort. past, pattern databases used store approximate heuristic values (Culberson& Schaeffer, 1998) important board features (Schaeffer, 2000). work appears firstuse pattern databases store search depth values.Computing search depths abstract states speeds pre-computation reduces memoryoverhead (both important considerations commercial computer games). paper usepreviously published clique abstraction (Sturtevant & Buro, 2005). preserves overall topologymap requires storing abstraction links explicitly.1 clique abstraction worksfinding fully connected subgraphs (i.e., cliques) original graph abstracting stateswithin clique single abstract state. Two abstract states connected abstractaction single original action leads state first cliquestate single clique (Figure 4). costs abstract actions computed Euclideandistances average coordinates states cliques.typical grid world computer-game maps, single application clique abstraction reducesnumber states factor two four. average, abstraction level five (i.e.,five applications abstraction procedure), region contains one hundred original1. alternative use regular rectangular tiles (e.g., Botea et al., 2004).426fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHFigure 3: single optimal lookahead depth value shared among children abstract state.memory-efficient approximation true per-ground-state values Figure 2.Level 0 (original graph)Level 1Level 2Figure 4: Two iterations clique abstraction procedure produce two abstract levelsground-level search graph.(or ground-level) states. Thus, single search depth value shared among ten thousandstate pairs. result, five-level clique abstraction yields four orders magnitude reductionmemory two orders magnitude reduction pre-computation time (as analyzed later).downside, higher levels abstraction effectively make search depth selection lessless dynamic depth value shared among progressively states. abstractionlevel pattern database control parameter trades pre-computation time patterndatabase size on-line performance algorithm uses database.Two alternatives storing optimal search depth store optimal action optimalheuristic value. combination abstraction real-time search precludes them. Indeed,sharing optimal action computed single ground-level representative abstract regionamong states region may cause agent run wall (Figure 5, left). Likewise,sharing single heuristic value among states region leaves agent without sense427fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONG5.845.845.845.845.845.845.845.845.845.845.845.845.84GFigure 5: Goal shown G, agent A. Abstract states four tiles separated dashed lines.Diamonds indicate representative states tile. Left: Optimal actions shownrepresentative abstract tile; applying optimal action agents tileagents current location leads wall. Right: Optimal heuristic value (h ) lowerleft tiles representative state (5.84) shared among states tile. result,agent preference among three legal actions shown.direction states vicinity would look equally close goal (Figure 5, right).contrast sharing heuristic value among states within abstract state (known pattern)using optimal non-real-time search algorithms A* IDA* (Culberson & Schaeffer,1996). case real-time search, agents using either alternative guaranteed reachgoal, let alone minimize travel. contrary, sharing search depth among numberground-level states safe LRTA* complete search depth.compute single depth table per map off-line (Figure 6). line 1 state space abstracted ` times. Lines 2 7 iterate pairs abstract states. pair (s0 , s0goal ),representative ground-level states sgoal (i.e., ground-level states closest centroids regions) picked optimal search depth value calculated them. this, Dijkstrasalgorithm (Dijkstra, 1959) run ground-level search space (V, E) compute trueminimal distances state sgoal . distances known successors s,optimal action (s, sgoal ) computed greedily. optimal search depth (s, sgoal )computed previously described capped c (line 5). resulting value stored pairabstract states (s0 , s0goal ) line 6. Figures 2 3 show optimal search depth values singlegoal state grid world game map without abstraction respectively.run-time, LRTA* agent going state state sgoal takes search depthdepth table value pair (s0 , s0goal ), s0 s0goal images sgoal `-levelabstraction. additional run-time complexity minimal s0 , s0goal , d(s0 , s0goal ) computedsmall constant-time overhead action.building pattern database Dijkstras algorithm run V` times2 graph (V, E)time complexity O(V` (V log V + E)) sparse graphs (i.e., E = O(V )). optimalsearch depth computed V`2 times. time, c LRTA* invocations total2. brevity, use V E mean sets vertices/edges sizes (i.e., |V | |E|).428fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHBuildPatternDatabase(V, E, c, `)1 apply abstraction procedure ` times (V, E) compute abstract space S` = (V` , E` )2 pair states (s0 , s0goal ) V` V`3select V representative s0 V`4select sgoal V representative s0goal V`5compute c-capped optimal search depth value state respect goal sgoal6store capped pair (s0 , s0goal )7 endFigure 6: Pattern database construction.complexity O(bc ) b maximum degree V . Thus, overall time complexityO(V` (V log V + E + V` bc )). space complexity lower store optimal search depthvalues pairs abstract states: O(V`2 ). Table 1 lists bounds sparse graphs.Table 1: Reduction complexity due state abstraction.timespaceabstractionO(V 2 log V )O(V 2 )`-level abstractionO(V` V log V )O(V`2 )reductionV /V`(V /V` )25.3 Discussion Two ApproachesSelecting search depth pattern database two advantages. First, search depth valuesstored pair abstract states optimal non-abstract representatives, unless eithervalue capped states local search space visited heuristic values modified. (conditional) optimality contrast classifier approachoptimal actions ever computed deeper searches merely assumed leadbetter action. assumption always hold phenomenon known lookahead pathology, found abstract graphs (Bulitko et al., 2003) well grid-based pathfinding (Lustrek &Bulitko, 2006). second advantage need features current state, recenthistory pre-search. search depth retrieved depth table simply basiscurrent states identifier, coordinates.decision-tree classifier approach two advantages depth table approach. First,classifier training need happen search space agent operates in.long training maps used collect features build decision tree representativerun-time maps, approach run never-before-seen maps (e.g., user-created mapscomputer game). Second, much smaller memory overhead methodclassifier specified procedurally pattern database needs loaded memory.Note approaches assume structure heuristic search problemhand. Namely, pattern database approach shares single search depth value across regionstates. works effectively states region indeed lookaheaddepth best them. abstraction mechanism forms regions basis searchgraph structure, regard search depth. empirical study show, clique abstraction429fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONseems right choice pathfinding. However, choice best abstraction techniquegeneral heuristic search problem open question.Similarly, decision-tree approach assumes states share similar feature valuesalso share best search depth value. appears hold large extent pathfinding domainfeature selection arbitrary heuristic search problems open question well.6. Dynamic Goal Selectiontwo methods described allow agent select individual search depth state.However, original LRTA*, heuristic still computed respect global goalsgoal . illustrate: Figure 7, map partitioned eight abstract states (in case, 4 4square tiles) whose representative states shown diamonds (18). optimal pathagent (A) goal (G) shown well. straight-line distance heuristic ignorewall agent goal lead agent south-western direction. LRTA*search depth 11 higher needed produce optimal action (such ). Thus,cap value 11, agent left suboptimal action spend long timehorizontal wall raising heuristic values. Spending large amounts time cornersheuristic depressions primary weakness real-time heuristic search agents and,example, remedied dynamic search depth selection due cap.12345G768Figure 7: Goal shown G, agent A. Abstract states eight tiles separated dashedlines. Diamonds indicate ground-level representative tile. optimal pathshown. Entry points path abstract states marked circles.5a compute sintermediate goal goal (s, sgoal )5b compute capped optimal search depth value respect sintermediate goal6 store (d , sintermediate goal ) pair (s0 , s0goal )Figure 8: Switching sgoal sintermediate goal ; replaces lines 56 Figure 6.430fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHFigure 9: three maps used experiments.address issue, switch intermediate goals pattern-database construction wellon-line LRTA* operation. example Figure 7 compute heuristic aroundrespect intermediate goal marked double-border circle map. Consequently,eleven times shallower search depth needed optimal action towards next abstract state(right-most upper tile). approach replaces lines 5 - 6 Figure 6 Figure 8. line5a, compute intermediate goal sintermediate goal ground-level state optimal pathsgoal enters next abstract state. entry points marked circles Figure 7.compared entry states centroids abstract states intermediate goals (Bulitko et al., 2007)found former superior terms algorithms performance. Note optimal patheasily available off-line run Dijkstras algorithm (Section 5.2).intermediate goal computed, line 5b computes capped optimal search depthrespect intermediate goal sintermediate goal . depth computation done describedSection 5.2. search depth intermediate goal added pattern databaseline 6. run-time, agent executes LRTA* stored search depth computesheuristic h respect stored goal (i.e., sgoal set sintermediate goal line 3 Figure 1).words, search depth agents goal selected dynamically, per action.approach works heuristic functions used practice tend become accurate states closer goal state. Therefore, switching distant global goal nearbyintermediate goal makes heuristics around current state accurate leads shallower search depth necessary achieve optimal action. result, algorithmrun quickly shallower search per move also search depth cap reached lessfrequently therefore search depth values actually result optimal moves.7. Empirical Evaluationsection presents results empirical evaluation algorithms dynamic control searchdepth goals classic state-of-the-art published algorithms. algorithms avoid reexpanding states planning move via transposition table. report sub-optimalitysolution found average amount computation per action, expressed numberstates expanded. believe algorithms implemented way singleexpanded state takes amount time. case testbed codeoptimized other. reason avoid clutter, report CPU timesSection 7.7. used fixed tie-breaking scheme real-time algorithms.431fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONuse grid world maps computer game testbed. Game maps provide realisticchallenging environment real-time search seen number recent publications (e.g., Nash, Daniel, & Felner, 2007; Hernandez & Meseguer, 2007). original mapssized 161161 193193 cells (Figure 9). line Sturtevant Buro (2005) SturtevantJansen (2007), also experimented maps upscaled 512 512 closer sizemaps used modern computer games. Note three maps depicted figureoutdoor-type maps, also ran preliminary experiments indoor-type game maps (e.g., oneshown Figure 2). trends similar decided focus larger outdoor maps.100 search problems defined three original size maps. startgoal locations chosen randomly, although constrained optimal solution paths cost90 100 order generate difficult instances. upscaled maps 300problems upscaled well. data point plots average 300 problems (3maps 100 runs each). different legend entry used algorithm, multiple pointslegend entry represent alternative parameter instantiation algorithm.heuristic function used octile distance natural extension Manhattan distance mapsdiagonal actions. enforce real-time constraint disqualified parameter settingscaused algorithm expand 1000 states move problem. pointsexcluded empirical evaluation. Maps known priori off-line order builddecision-tree classifiers pattern databases.use following notation identify algorithms variants: AlgorithmName(X, Y) X defined follows. X denotes search depth control: F fixed searchdepth, DT search depth selected dynamically decision tree, ORACLE search depthselected decision-tree oracle (see next section details) PDB search depthselected dynamically pattern databases. denotes goal state selection: G heuristiccomputed respect single global goal, PDB heuristic computed respectintermediate goal pattern databases. instance, classic LRTA* LRTA* (F, G).empirical evaluation organized eight parts follows. Section 7.1 describes sixalgorithms compute heuristic respect global goal discusses performance.Section 7.2 describes five algorithms use intermediate goals. Section 7.3 compares globalintermediate goals. Section 7.4 studies effects path-refinement without dynamiccontrol. Secton 7.5 pits new algorithms state-of-the-art real-time non-real-timealgorithms. provide algorithm selection guide different time limits planning permove Section 7.6. Finally, Section 7.7 considers issue amortizing off-line pattern-databasebuild time on-line pathfinding.7.1 Algorithms Global Goalssubsection describe following algorithms compute heuristic respectsingle global goal (i.e., use intermediate goals):1. LRTA* (F, G) Learning Real-Time A* (Korf, 1990). action conducts breadthfirst search fixed depth around agents current state. first move towardsbest depth state taken heuristic agents previous state updated usingKorfs mini-min rule.3 used {4, 5, . . . , 20}.3. Instead using LRTA* could used RTA*. experiments showed grid pathfindingsignificant performance difference two search depth beyond one. Indeed deeper searches432fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH2. LRTA* (DT, G) LRTA* search depth dynamically controlled decisiontree described Section 5.1. used following parameters: dmax {5, 10, 15, 20}history trace length n = 60. building decision-tree classifier WEKA (Witten & Frank, 2005) pruning factor set 0.05 minimum number data itemsper leaf 100 original size maps 25 upscaled ones. opposed learningtailor-made classifier game map, single common decision-tree classifier builtbased data collected maps (using 10-fold cross-validation). donedemonstrate ability classifier generalize across maps.3. LRTA* (ORACLE, G) LRTA* search depth dynamically controlledoracle. oracle always selects best search depth produce move givenLRTA* (F, G) fixed lookahead depth dmax (Bulitko et al., 2007). words,oracle acts perfect decision-tree thus sets upper bound LRTA* (DT, G)performance. oracle run dmax {5, 10, 15, 20}, original sizemaps proved prohibitively expensive compute upscaled maps. Notepractical real-time algorithm used reference point experiments.4. LRTA* (PDB, G) LRTA* search depth dynamically controlledpattern database described Section 5.2. original size maps, used abstractionlevel ` {0, 1, . . . , 5} depth cap c {10, 20, 30, 40, 50, 1000}. upscaled maps,used abstraction level ` {3, 4, . . . , 7} depth cap c {20, 30, 40, 50, 80, 3000}.Considering size maps, cap value 1000 3000 means virtually capless search.5. K LRTA* (F, G) variant LRTA* proposed Koenig (2004). Unlike originalLRTA*, uses A*-shaped lookahead search space updates heuristic values stateswithin using Dijkstras algorithm.4 number states K LRTA* expands per movetook values: {10, 20, 30, 40, 100, 250, 500, 1000}.6. P LRTA* (F, G) Prioritized LRTA* variant LRTA* proposed Rayner, Davison,Bulitko, Anderson, Lu (2007). uses lookahead depth 1 moves. However,every state whose heuristic value updated, neighbors put onto update queue,sorted magnitude update. Thus, algorithm propagates heuristic functionupdates space fashion Prioritized Sweeping (Moore & Atkeson, 1993).control parameter (queue size) set {10, 20, 30, 40, 100, 250, 500, 1000} originalsize maps {10, 20, 30, 40, 100, 250} upscaled maps.Figure 10 evaluate performance new dynamic depth selection algorithmsoriginal size maps. see decision-tree pattern-database approach improvesignificantly upon LRTA* algorithm, expanding two three times fewer states generatingsolutions comparable quality. Furthermore, perform par current state-of-the-art realtime search algorithms without abstraction, seen compared K LRTA* (F, G).solutions generated acceptable quality domain (e.g., 50% suboptimal), evenexpanding 100 states per action. Also interest decision-tree approach performslikelihood multiple actions equally low g + h cost high, reducing distinction RTA*LRTA*. using LRTA* agents learn repeated trials.4. also experimented A*-shaped lookahead new algorithms found inferior breadth-first lookahead deeper searches.433fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONOriginal size mapsRealtime cutoff: 10004LRTA* (F, G)LRTA* (ORACLE, G)LRTA* (DT, G)LRTA* (PDB, G)P LRTA* (F, G)K LRTA* (F, G)Suboptimality (times)3.532.521.510100200300400Mean number states expanded per move500600Figure 10: Global-goal algorithms original size maps.quite close theoretical best case, seen compared LRTA* (ORACLE, G).shows features use, although seemingly simplistic, good job predictingappropriate search depth.ran similar sets experiments upscaled maps. However, none global goalalgorithms generated solutions acceptable quality given real-time cut-off (the solutions300 1700% suboptimal). experimental results upscaled maps providedAppendix B. shows inherent limitations global goal approaches; large searchspaces cannot compete equal footing abstraction-based methods. brings usintermediate goal selection methods.7.2 Algorithms Intermediate Goalssection describe algorithms use intermediate goals search. bestknowledge, one previously published real-time heuristic search algorithmso. Thus, compare new algorithms proposed paper. Given intermediategoals increase performance algorithms significantly, present resultschallenging upscaled maps. full roster algorithms used section follows:1. PR LRTA* (F, G) Path Refinement Learning Real-Time Search (Bulitko et al., 2007).algorithm two components: runs LRTA* fixed search depth globalgoal abstract space (abstraction level ` clique abstraction hierarchy) refinesfirst move using corridor-constrained A* running original ground-level map.5Constraining A* small set states, collectively called corridor Sturtevant Buro5. algorithm actually called PR LRTS (Bulitko et al., 2007). Based findings Lustrek Bulitko (2006),modified refine single abstract action order reduce susceptibility lookahead pathologies.modification equivalent substituting LRTS component LRTA*. Hence, rest paper,call PR LRTA*.434fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH(2005) tunnel Furcy (2006), speeds makes real-time corridor sizeindependent map size (Bulitko, Sturtevant, & Kazakevich, 2005). heuristiccomputed abstract space respect fixed global goal, A* componentcomputes path current state intermediate goal. qualifies PR LRTA*enter section empirical evaluation. control parameters follows: abstractionlevel ` {3, 4, . . . , 7}, LRTA* lookahead depth {1, 3, 5, 10, 15} LRTA* heuristicweight {0.2, 0.4, 0.6, 1.0} ( imposed g line 5 Figure 1).2. LRTA* (F, PDB) LRTA* fixed search depth uses pattern database selectintermediate goals. control parameters follows: abstraction level ` {3, 4, . . . , 7}search depth {1, 2, . . . , 9, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30}.3. LRTA* (PDB, PDB) LRTA* generalized dynamic search depth intermediate goal selection pattern databases presented Sections 5.2 6. control parameters follows: abstraction level ` {3, 4, . . . , 7} lookahead capc {20, 30, 40, 50, 80, 3000}.4. PR LRTA* (PDB, G) PR LRTA* whose LRTA* component equipped dynamic search depth uses global (abstract) goal respect computes abstract heuristic. pattern database search depth constructedabstraction level ` LRTA* component runs on, making component optimal lookahead cap allows. used abstraction level `{3, 4, . . . , 7} lookahead cap c {5, 10, 15, 20, 1000}.also ran version PR LRTA* (PDB, G) pattern database constructed abstraction level `2 level ` LRTA* operates (Table 2). used (`, `2 ){(1, 3), (2, 4), (3, 5), (4, 6), (5, 7), (1, 4), (2, 6), (3, 7), (4, 8), (5, 9)}.5. PR LRTA* (PDB, PDB) two-database version PR LRTA* (PDB, G)except uses second database goal selection well depth selection. used(`, `2 ) {(1, 3), (2, 4), (3, 5), (4, 6), (5, 7), (1, 4), (2, 6), (3, 7), (4, 8), (5, 9)} (Table 2).Table 2: PR LRTA* (PDB, G PDB) uses LRTA* abstraction level ` define corridor withinrefines path using A*. Dynamic depth (and goal) selection performed eitherabstraction level ` `2 > `.Abstraction level`2`0Single abstraction PR LRTA*(PDB,G)abstract-level LRTA*dynamic depth selectioncorridor-constrained ground-level A*Dual abstraction PR LRTA*(PDB,{G,PDB})dynamic depth (and goal) selectionabstract-level LRTA*corridor-constrained ground-level A*pattern database algorithms presented stores depth value intermediateground-level goal pair abstract states. present performance results algorithmsintermediate goals Sections 7.37.6 analyze complexity pattern databasecomputation effects performance Section 7.7.435fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONUpscaled mapsRealtime cutoff: 1000020LRTA* (F, G)LRTA* (F, PDB)18Suboptimality (times)161412108642020040060080010001200Mean number states expanded per move14001600Figure 11: Effects intermediate goals: LRTA* (F, G) versus LRTA* (F, PDB).7.3 Global versus Intermediate GoalsSections 7.1 7.2 presented algorithms global intermediate goals respectively.section compare algorithms across two groups. include LRTA* (PDB, G), increasedreal-time cut-off 1000 10000 graphs section. start baseline LRTA* fixed lookahead. effects adding intermediate goal selection dramatic:LRTA* intermediate goals (F, PDB) finds five times better solutions three ordersmagnitude faster LRTA* global goals (F, G) (see Figure 11). believeresult octile distance heuristic substantially accurate around goal. Consequently,LRTA* (F, PDB) benefiting much better heuristic function.second experiment, equip versions dynamic search depth control compare LRTA* (PDB, G) LRTA* (PDB, PDB) Figure 12. performance gap lessdramatic: planning speed-up still around three orders magnitude, suboptimalityadvantage went five two times. Again, note increase real-timecut-off order magnitude get points plot.Finally, evaluate beneficial: dynamic depth control dynamic goal controlcomparing baseline LRTA* (F, G) LRTA* (PDB, G) LRTA* (F, PDB) Figure 13.clear dynamic goal selection much stronger addition baseline LRTA* dynamicsearch depth selection. Dynamic depth selection sometimes actually performs worse fixeddepth, evidenced data points LRTA* (F, G) line. happens primarilyhigh abstraction levels small caps. optimal lookahead depth computed highabstraction level, depth value shared among many ground-level states. selecteddepth value beneficial near entry point abstract state, abstract statelarge, depth likely become inappropriate ground-level states away.example, optimal depth entry point 1, worse moderate fixed depth436fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHUpscaled mapsRealtime cutoff: 10000LRTA* (PDB, G)LRTA* (PDB, PDB)14Suboptimality (times)121086420100200300400500600Mean number states expanded per move700800Figure 12: Effects intermediate goals: LRTA* (PDB, G) versus LRTA* (PDB, PDB).Upscaled mapsRealtime cutoff: 1000020LRTA* (F, G)LRTA* (F, PDB)LRTA* (PDB, G)18Suboptimality (times)161412108642020040060080010001200Mean number states expanded per move14001600Figure 13: Dynamic search depth control versus dynamic goal control.ground-level states far entry point. Small caps compound problem sometimespreventing selection optimal depth even entry point.shown plot, running (i.e., LRTA* (PDB, PDB)) leads marginal improvements. best parameterizations LRTA* (F, PDB) already expandssingle state per move virtually times. Consequently, benefit adding dynamicdepth control slight improvement suboptimality next section.437fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONUpscaled mapsRealtime cutoff: 10001.5LRTA* (F, PDB)LRTA* (PDB, PDB)PR LRTA* (F, G)PR LRTA* (F, PDB)PR LRTA* (PDB, PDB)PR LRTA* (PDB, G)Suboptimality (times)1.41.31.21.11051015Mean number states expanded per move2025Figure 14: Effects path refinement: LRTA* versus PR LRTA*.7.4 Effects Path RefinementPath-refinement algorithms (denoted PR prefix) run learning real-time search (LRTA*)abstract space refine path running A* ground level. Non-PR algorithm runA* real-time search happens ground-level space. examine effects pathrefinement comparing LRTA* PR LRTA*. Note even statically controlled baselinePR LRTA* (F, G) uses intermediate goals refining abstract actions. match usingdynamic intermediate goal selection LRTA*. Thus, compare four versions PR LRTA*: (F,G), (PDB, G), (F, PDB) (PDB, PDB) two versions LRTA*: (F, PDB) (PDB, PDB).results found Figure 14. sake clarity, show high performance areacapping number states expanded per move 25 suboptimality 1.5.best parameterizations LRTA* find near-optimal solutions expanding one stateper move virtually times. astonishing performance one state expansion permove corresponds search depth one fastest possible operation algorithmframework. Thus, LRTA* (F, PDB) LRTA* (PDB, PDB) virtually unbeatable termsplanning time. hand, PR LRTA* incurs planning overhead due path-refinementcomponent (i.e., running corridor-constrained A*). result, PR LRTA* also finds nearlyoptimal solutions incurs least five times higher planning cost per move. Dynamic controlPR LRTA* results moderate performance gains.7.5 Comparison Existing State ArtTraditionally, computer games used A* pathfinding needs (Stout, 2000). map sizenumber simultaneously planning agents increase, game developers find even highly optimizedimplementations A* insufficient. result, variants A* use state abstractionused (Sturtevant, 2007). Another way speeding A* introduce weight computing travelcost state. done f = g + h, 0 values 1 make agent438fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHgreedy (more weight put h) usually leads fewer states expanded pricesuboptimal solutions. section, compare new algorithms weighted A* (Korf, 1993)state-of-the-art Partial Refinement A* (PRA*) (Sturtevant & Buro, 2005). Note neitheralgorithm real-time and, thus, planning times per move map-size specific. is,larger maps, A*s PRA*s planning times per move increase algorithms computecomplete (abstract) path start goal states take first move. instance,maps used PRA* expands 3454 states expensive move. Weighted A*= 15 expands 40734 states classic A* expands 88138 states worst moves. Thus,include two algorithms comparison effectively remove real-time cut-off.results found Table 3. Dynamically controlled LRTA* one two orders magnitude faster average planning time per move. produces shorter paths existing stateof-the-art real-time algorithm (PR LRTA*) fastest weighted A* tried. original A*provably optimal solution quality PRA* nearly optimal. argue hundredsunits simultaneously planning paths computer game, LRTA* (PDB, PDB)s low planning time per move real-time guarantees worth 6.1% path-length suboptimality (e.g., 106screen pixels versus optimal 100 screen pixels).Table 3: Comparison high-performance algorithms, best values bold. Standard errorsreported .Algorithm, parametersPR LRTA* (F, G), ` = 4, = 5, = 1.0LRTA* (PDB, PDB), ` = 3, c = 3000A*weighted A*, f = 15 g + hPRA*Planning per move15.06 0.07221.032 0.0054119.8 3.520324.86 1.440410.83 0.0829Suboptimality (times)1.161 0.01771.061 0.00271 0.001.146 0.00721.001 0.00037.6 Best Solution Quality Time Limitsection identify algorithms deliver best solution quality time limit.Specifically, impose hard limit planning time per move, expressed number statesexpanded. algorithm exceeds limit even single move made 300problems upscaled maps excluded consideration. Among remaining algorithms,select one highest solution quality (i.e., lowest suboptimality). results foundTable 4. algorithms expand least one state per move move, leaving first rowempty. LRTA* (F, PDB) = 1, ` = 3 best choice time limit oneeight states expanded per move. limit rises, expensive optimal algorithmsbecome affordable. Note best choices dynamically controlled algorithmstime limit rises 3454 states. point, non-real-time PRA* takes ending domainreal-time algorithms. cross-over point specific problem map sizes. largerproblems/maps, PRA*s maximum planning time per move necessarily increase, makingbest choice progressively higher planning-time-per-move limits.439fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONTable 4: Best solution quality strict limit planning time per move. Planning timestates expanded per move. sake readability, suboptimality shownpercentage (e.g., 1.102267 = 10.2267%).Planning time limit0[1, 8][9, 24][25, 48][49, 120][121, 728][729, 753][754, 1223][1224, 1934][1935, 3453][3454, 88137][88138, )Algorithm, parametersLRTA* (F, PDB) = 1, ` = 3LRTA* (F, PDB) = 2, ` = 3LRTA* (F, PDB) = 3, ` = 3LRTA* (F, PDB) = 4, ` = 3LRTA* (F, PDB) = 6, ` = 3LRTA* (F, PDB) = 14, ` = 4PR LRTA* (PDB, G) c = 15, = 1.0, ` = 3PR LRTA* (PDB, G) c = 20, = 1.0, ` = 3PR LRTA* (PDB, G) c = 1000, = 1.0, ` = 3PRA*A*Suboptimality (%)10.2267%8.6692%5.6793%5.6765%5.6688%5.6258%4.2074%3.6907%3.5358%0.1302%0%7.7 Amortization Pattern-database Build Timepattern-database approach invests time computing PDB map. sectionstudy amortization off-line investment multiple problem instances. PDB buildtimes 3 GHz Pentium CPU listed Table 5 single map. Consider algorithm LRTA*(PDB, PDB) cap c = 20 pattern databases built level ` = 3. average,solution suboptimality 1.058 expanding 1.536 states per move 31.065 microseconds.closest statically controlled competitor PR LRTA* (F, G) ` = 4, = 15, = 0.6suboptimality 1.059 expanding average 28.63 states per move 131.128 microseconds. Thus, LRTA* (PDB, PDB) 100 microseconds faster move. Consequently,4.7 108 moves necessary recoup off-line PDB build time 13 hours. movetaking 31 microseconds, LRTA* lower total run-time first four hourspathfinding. computed recoup times parameterizations LRTA* (PDB, PDB)whose closest statically controlled competitor slower per move. results found Table 6demonstrate LRTA* (PDB, PDB) recoups PDB build time first 1.4 27 hourspathfinding time. Note numbers highly implementation domain-specific. particular, code building PDBs leaves substantial room optimization. completenesssake, report detailed times Appendix C.8. Discussion Empirical Resultssection recap trends observed previous sections. Dynamic selectionlookahead either decision-tree PDB approach helps reduce planning time per movewell solution suboptimality (Section 7.1). result, LRTA* becomes competitivemodern algorithms Koenigs LRTA*. However, real-time search algorithms global goalsscale well large maps.440fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHTable 5: Pattern database average 512512 map, computed intermediate goals. Databasesize listed number abstract state pairs. Suboptimality planning per movelisted representative algorithm: LRTA* (PDB, PDB) cap c = 20.Abstraction level01234567Size1.1 10107.4 1085.9 1076.1 1068.6 1051.5 1053.1 1046.4 103Timeest. 2 yearsest. 1.5 monthsest. 4 days13 hours3 hours1 hour24 minutes10 minutesPlanning per move1.53.241.3104.4169.3Suboptimality (times)1.0581.0591.5352.3152.284Table 6: Amortization PDB build times. dynamically controlled LRTA*, liststatically controlled PR LRTA* closest terms solution suboptimality.LRTA* (PDB, PDB)c = 20, ` = 3c = 20, ` = 4c = 30, ` = 3c = 40, ` = 3c = 40, ` = 4c = 50, ` = 3c = 50, ` = 4c = 80, ` = 3PR LRTA* (F, G)` = 4, = 15, = 0.6` = 4, = 15, = 0.6` = 4, = 15, = 0.6` = 4, = 15, = 0.6` = 4, = 15, = 0.4` = 4, = 15, = 0.6` = 4, = 15, = 0.6` = 4, = 15, = 0.6Amortization moves4.7 1081.2 1085.1 1085.3 1083.4 1086.2 1086.7 1081.1 109Amortization run-time4 hours1.4 hours5.1 hours6 hours9.3 hours9 hours21.1 hours27 hoursAdding intermediate goals brings even classic LRTA* par previous state-of-theart real-time search algorithm PR LRTA* much stronger addition dynamic lookaheaddepth selection (Section 7.3). Using dynamic lookahead depth subgoals bringsimprovements. Section 7.5 details, LRTA* equipped dynamic lookahead depthsubgoal selection expands barely state per move less 7% solution suboptimality.better previous state-of-the-art algorithms PR LRTA*, PRA* A*solution quality planning time per move, believe trade-offs makes appealingpractice. aid practitioners further, provide algorithm selection guide Section 7.6makes clear LRTA* dynamic subgoal selection best algorithms timeper move severely limited. speed advantage deliver state-of-the-art PR LRTA*algorithm allows recoup PDB build time several hours pathfinding.9. Current Limitations Future Workproject opens several interesting avenues future research. particular, would worthwhile investigate performance algorithms paper dynamic environments (e.g.,bridge gets destroyed real-time strategy game goal moves away agent).441fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONAnother area future research application proposed algorithms general planning.Heuristic search successful approach planning planners ASP (Bonet,Loerincs, & Geffner, 1997), HSP-family (Bonet & Geffner, 2001), FF (Hoffmann, 2000),SHERPA (Koenig, Furcy, & Bauer, 2002) LDFS (Bonet & Geffner, 2006). line recentplanning work (Likhachev & Koenig, 2005) Bonet Geffner (2006), evaluateproposed algorithms general STRIPS-style planning problem. Nevertheless, believenew real-time heuristic search algorithms may also offer benefits wider range planningproblems. Indeed, core heuristic search algorithm extended paper (LRTA*) previously applied general planning (Bonet et al., 1997). extensions introduced maybeneficial effect similar way B-LRTA* improved performance ASP planner.Subgoal selection long studied planning central part intermediate-goaldepth-table approach. Decision trees search depth selection induced sample trajectories space appear scalable general planning problems. partapproach requires solving numerous ground-level problems optimally pre-computationoptimal search depth PDB approach. conjecture approach still effective if,instead computing optimal search depth based optimal action , one solverelaxed planning problem use resulting action place . idea deriving heuristicguidance solving relaxed problems quite common planning heuristic searchcommunity.10. ConclusionsReal-time pathfinding non-trivial problem algorithms must trade solution qualityamount planning per move. two measures antagonistic thus interestedPareto optimal algorithms outperformed measures algorithms.classic LRTA* provides smooth trade-off curve, parameterized lookahead depth. Sinceintroduction 1990, variety extensions proposed. recent extension,PR LRTS (Bulitko et al., 2005) first application automatic state abstraction real-timesearch. large-scale empirical study pathfinding game maps, PR LRTS outperformedmany algorithms respect several antagonistic measures (Bulitko et al., 2007).paper also employ automatic state abstraction instead using pathrefinement, pre-compute pattern databases use select amount planningintermediate goals dynamically, per move. Several mechanisms dynamic control proposed used virtually existing real-time search algorithm. demonstration,equip classic LRTA* state-of-the-art PR LRTS dynamic control.resulting improvements substantial. instance, LRTA* equipped PDB-based controllookahead intermediate goal selection significantly outperforms existing state art (PRLRTS) simultaneously planning per move solution quality. Furthermore, average expands little one state per move minimum amount planningLRTA*-based algorithm.new algorithms compare favorably A* state-of-the-art extension, PRA*,presently popular industrial choices pathfinding computer games (Stout, 2000; Sturtevant,2007). First, per-move planning time algorithms provably unaffected increasemap size. Second, two orders magnitude faster A* one order magnitudefaster PRA* planning time per move. improvements come price 7%442fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHsuboptimality, likely unnoticed computer game player scenarios. Thus appearsnew algorithms redefine state art real-time search arena alsowell-suited industrial applications.AcknowledgmentsSverrir Sigmundarson School Computer Science, Reykjavik Universityproject. appreciate consultation Robert C. Holte detailed feedback anonymousreviewers. research supported grants National Science Engineering Research Council Canada (NSERC); Albertas Informatics Circle Research Excellence (iCORE);Slovenian Ministry Higher Education, Science Technology; Icelandic Centre Research(RANNIS); Marie Curie Fellowship European Community programme StructuringERA contract number MIRG-CT-2005-017284. Special thanks Nathan Sturtevantdevelopment support HOG.Appendix A. Decision-Tree Featuresdevised two different categories classifier features: first consists features basedagents recent history, whereas second contains features sampled shallow pre-searchagents current state. Thus, collectively, features two categories make predictionsbased agents recent history well current situation.first category four features listed Table 7. features computedexecution step. aggregated recent states agent in,done incremental fashion improved performance. parameter n set usercontrols long history aggregate over. use notation s1 refer state agentone step ago, s2 state two steps ago, etc.; agent thus aggregates states s1 ,..., sn . Feature f1 provides rough estimate location agent relative goal.distance goal state affect required lookahead depth, example heuristicscloser goal usually accurate. feature makes possible classifier makedecisions based deemed necessary. Features f2 (known mobility) f3 providemeasure much progress agent made towards reaching goal past steps.Frequent state revisits may indicate heuristic depression deeper search usually beneficialsituations (Ishida, 1992). Feature f4 measure inaccuracies inconsistenciesheuristic around agent; again, many heuristic updates may warrant deeper search.features second category listed Table 8. also computed execution step. planning phase starts, shallow lookahead pre-search performed gatherinformation nearby part search space. types features categorycoarsely divided features (i) compute fraction states pre-search lookaheadfrontier satisfy property, (ii) compare action chosen pre-search previousactions (either previous state taken last time current state visited), (iii)check heuristic estimates immediate successors current state. Feature f5 roughmeasure density obstacles agents vicinity: obstacles are,beneficial deeper search might be. Feature f6 indicator difficulty traversinglocal area. proportion high, many states updated, possibly suggesting heuristicdepression. feature f7 , pre-search selects action might indicate443fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONTable 7: History based classifier features.Featuref1f2f3f4Descriptioninitial heuristic estimate distance current state goal:hoctile (s, sglobal goal ).heuristic estimate distance current state stateagent n steps ago: h(s, sn ).number distinct states agent visited last n steps:|{s1 , s2 , ..., sn }|.Ptotal volume heuristic updates last n steps: ni=1 hafter update (si )hbefore update (si ) (line 6 Figure 1).Table 8: Pre-search based classifier features.Featuref5f6f7f8f9f10f11Descriptionratio actual number states pre-search frontier expectednumber states obstacles map.fraction frontier states updated heuristic value.boolean feature telling whether action chosen pre-searchaction chosen planning phase last time state visited.first time state visited feature false.boolean feature telling whether direction suggested pre-searchdirection agent took previous step.ratio current states heuristic best successor state suggestedpre-search: h(s, sgoal )/h(s, sgoal ).boolean feature telling whether best action proposed pre-search phasewould lead successor state updated heuristic value.boolean feature telling whether heuristic value current state largerheuristic value best immediate successor found pre-search.heuristic values part search space already mutually consistent thusshallow lookahead needed; applies feature f8 . Features f9 f11 compare currentstate successor state suggested pre-search.Appendix B. Experiments Upscaled Maps Using Global GoalsEmpirical results running global-goal algorithms upscaled maps shown Figure 15.LRTA* (DT, G) shows significant improvement LRTA* (F, G), making comparablequality existing state-of-the-art algorithms: par P LRTA* (F, G) slightly betterK LRTA* (F, G) allowed expand 200 states per move. also worth notingLRTA* (PDB, G) longer competitive algorithms and, fact, makereal-time cut-off 1000 states parameters combinations (and thus shownplot). reason lies fact problems simply difficult LRTA* findoptimal move small lookahead depth. instance, abstraction level ` = 3 cap444fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHc = 80, LRTA* (PDB, G) suboptimality 1.36. Unfortunately, lookahead depth hits cap11% visited states. result, algorithm expands average 1214 states per movedisqualifies cut-off 1000.Upscaled mapsRealtime cutoff: 100020LRTA* (F, G)LRTA* (DT, G)P LRTA* (F, G)K LRTA* (F, G)18Suboptimality (times)1614121086420100200300400500600Mean number states expanded per move700800Figure 15: Performance global-goal algorithms upscaled maps.Looking collectively small upscaled map results, LRTA* (DT, G) demonstratesexcellent performance among global goal algorithms robust respect mapupscaling one efficient ones (the comparable algorithm K LRTA* (F, G)).However, within provided 1000 states cut-off limit, none real-time global-goal algorithmsreturned solutions would considered acceptable quality pathfinding. Indeed, evenbest solutions found approximately four times worse optimal.Appendix C. Pattern Database Build Timesorder operate LRTA* PR LRTA* use lookahead depth intermediate goals controlled dynamically, build pattern databases. pattern database built off-line containssingle entry pair abstract states. three types entries: (i) intermediate goalground-level entry state next abstract state; (ii) capped optimal lookahead depthrespect intermediate goal (iii) optimal lookahead depth respect globalgoal. running algorithms capped lookaheads (i.e., c < 1000) need two databasesper map: one containing intermediate goals one containing capped optimal lookahead depths.running effectively uncapped algorithms (i.e., c = 1000 c = 3000) also need thirddatabase lookahead depths global goals (see Appendix discussion). Tables 5912 report build times LRTA* (PDB, PDB) performance capped (i.e.,build two pattern databases). Tables 13 14 report build times performanceeffectively cap (i.e., built three pattern databases).Finally, interest speeding experiments fact compute pattern databasespairs abstract states. Instead, took advantage prior benchmark problem availability445fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONTable 9: Pattern databases average 512 512 map, computed intermediate goals.Database size listed number abstract state pairs. Suboptimality planningper move listed LRTA* (PDB, PDB) cap c = 30.Abstraction level01234567Size1.1 10107.4 1085.9 1076.1 1068.6 1051.5 1053.1 1046.4 103Timeest. 2 yearsest. 1.5 monthsest. 4 days13.4 hours3.1 hours1.1 hours24 minutes11 minutesPlanning per move2.112.360.7166.6258.8Suboptimality (times)1.0581.0831.2601.8431.729Table 10: Pattern databases average 512 512 map, computed intermediate goals.Database size listed number abstract state pairs. Suboptimality planningper move listed LRTA* (PDB, PDB) cap c = 40.Abstraction level01234567Size1.1 10107.4 1085.9 1076.1 1068.6 1051.5 1053.1 1046.4 103Timeest. 2 yearsest. 1.5 monthsest. 4 days13.1 hours3.1 hours1.0 hours24 minutes10 minutesPlanning per move2.710.253.4217.3355.4Suboptimality (times)1.0581.0601.1021.4741.490computed PDBs abstract goal states come play problems agentssolve. Thus, times tables estimates possible pairs.Appendix D. Intermediate Goals Loopsshown Korf original paper, LRTA* complete lookahead depthheuristic taken respect single global goal. completeness guarantee lost oneuses intermediate goals (i.e., LRTA* (F, PDB), LRTA* (PDB, PDB) well PR LRTA*counter-parts). Indeed, abstract tile A, dynamic goal control module guideagent towards entry state tile B. However, way, agent may stumble differentabstract tile C. soon happens, dynamic control module may select entry state tilenew intermediate goal. unsuspecting agent heads back everything repeats.combat loops equipped algorithms use intermediate goals state reentrance detector. Namely, soon agent re-visits ground-level state, dynamic controlswitches intermediate goal global goal. Additionally, new lookahead depth selected. Ideally, lookahead depth optimal depth respect global goal,446fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHTable 11: Pattern databases average 512 512 map, computed intermediate goals.Database size listed number abstract state pairs. Suboptimality planningper move listed LRTA* (PDB, PDB) cap c = 50.Abstraction level01234567Size1.1 10107.4 1085.9 1076.1 1068.6 1051.5 1053.1 1046.4 103Timeest. 2 yearsest. 1.5 monthsest. 4 days13.6 hours3.1 hours1.0 hours24 minutes11 minutesPlanning per move3.511.168.5279.4452.3Suboptimality (times)1.0581.0591.0981.4321.386Table 12: Pattern databases average 512 512 map, computed intermediate goals.Database size listed number abstract state pairs. Suboptimality planningper move listed LRTA* (PDB, PDB) cap c = 80.Abstraction level01234567Size1.1 10107.4 1085.9 1076.1 1068.6 1051.5 1053.1 1046.4 103Timeest. 2 yearsest. 1.5 monthsest. 4 days13.5 hours3.2 hours1.0 hours25 minutes10 minutesPlanning per move6.622.9109.7523.3811.5Suboptimality (times)1.0581.0591.0871.4111.301capped c. Unfortunately, computing optimal lookahead depths global goals quite expensiveoff-line (Tables 13 14). Given loops occur fairly infrequently, normally computeoptimal lookahead depths global goals. Instead, state re-visit detected, switchglobal goals simply set lookahead cap c. saves off-line PDB computation timesometimes causes agent conduct deeper search (c plies) really necessary.6alternative solution investigated future research progressively increase lookahead on-line re-visits detected (i.e., every time re-visit occurs, lookahead depthstate increased certain number plies).6. exception practice cases c = 1000 c = 3000 setting lookahead depth cwould immediately disqualified algorithm, provided reasonable real-time cut-off. Consequently,two cap values, invest large amount time computed effectively uncapped optimal lookahead depthrespect global goals.447fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONTable 13: Pattern databases average 512 512 map, computed global goals. Databasesize listed number abstract state pairs. Suboptimality planning per movelisted LRTA* (PDB, PDB) cap c = 3000.Abstraction level01234567Size1.1 10107.4 1085.9 1076.1 1068.6 1051.5 1053.1 1046.4 103Timeest. 350 yearsest. 25 yearsest. 2 years73 days10.3 days1.7 days9.8 hours2.5 hoursPlanning per move1.04.827.986.7174.1Suboptimality (times)1.0611.0621.1333.6263.474Table 14: Pattern databases average 512 512 map, computed global goals. Databasesize listed number abstract state pairs. Suboptimality planning per movelisted LRTA* (PDB, G) cap c = 20.Abstraction level01234567Size1.1 10107.4 1085.9 1076.1 1068.6 1051.5 1053.1 1046.4 103Timeest. 12 yearsest. 6 monthsest. 13 days38.0 hours7.5 hours2.3 hours52 minutes21 minutes448Planning per move349.9331.6281.0298.1216.1Suboptimality (times)6.4688.76610.4258.15514.989fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHReferencesBioWare Corp. (1998). Baldurs Gate., Published Interplay, http://www.bioware.com/bgate/,November 30, 1998.Bjornsson, Y., & Marsland, T. A. (2003). Learning extension parameters game-tree search. Inf.Sci, 154(34), 95118.Blizzard (2002). Warcraft 3: Reign chaos. http://www.blizzard.com/war3.Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(12),533.Bonet, B., & Geffner, H. (2006). Learning depth-first search: unified approach heuristic searchdeterministic non-deterministic settings, application MDPs. ProceedingsInternational Conference Automated Planning Scheduling (ICAPS), pp. 142151, Cumbria, UK.Bonet, B., Loerincs, G., & Geffner, H. (1997). fast robust action selection mechanismplanning.. Proceedings National Conference Artificial Intelligence (AAAI), pp.714719, Providence, Rhode Island. AAAI Press / MIT Press.Botea, A., Muller, M., & Schaeffer, J. (2004). Near optimal hierarchical path-finding. JournalGame Development, 1(1), 728.Bulitko, V. (2003a). Lookahead pathologies meta-level control real-time heuristic search.Proceedings 15th Euromicro Conference Real-Time Systems, pp. 1316, Porto,Portugal.Bulitko, V. (2003b). Lookahead pathologies meta-level control real-time heuristic search.Proceedings 15th Euromicro Conference Real-Time Systems, pp. 1316.Bulitko, V. (2004). Learning adaptive real-time search. Tech. rep. http://arxiv.org/abs/cs.AI/0407016, Computer Science Research Repository (CoRR).Bulitko, V., Bjornsson, Y., Lustrek, M., Schaeffer, J., & Sigmundarson, S. (2007). Dynamic Control Path-Planning Real-Time Heuristic Search. Proceedings InternationalConference Automated Planning Scheduling (ICAPS), pp. 4956, Providence, RI.Bulitko, V., Li, L., Greiner, R., & Levner, I. (2003). Lookahead pathologies single agent search.Proceedings International Joint Conference Artificial Intelligence (IJCAI), pp.15311533, Acapulco, Mexico.Bulitko, V., Sturtevant, N., & Kazakevich, M. (2005). Speeding learning real-time search viaautomatic state abstraction. Proceedings National Conference Artificial Intelligence (AAAI), pp. 13491354, Pittsburgh, Pennsylvania.Bulitko, V., Sturtevant, N., Lu, J., & Yau, T. (2007). Graph Abstraction Real-time HeuristicSearch. Journal Artificial Intelligence Research (JAIR), 30, 51100.Buro, M. (2000). Experiments Multi-ProbCut new high-quality evaluation functionOthello. van den Herik, H. J., & Iida, H. (Eds.), Games AI Research, pp. 7796. U.Maastricht.Culberson, J., & Schaeffer, J. (1996). Searching pattern databases. CSCI (Canadian AIConference), Advances Artificial Intelligence, pp. 402416. Springer-Verlag.449fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONCulberson, J., & Schaeffer, J. (1998). Pattern Databases. Computational Intelligence, 14(3), 318334.Dijkstra, E. W. (1959). note two problems connexion graphs.. Numerische Mathematik,1, 269271.Furcy, D. (2006). ITSA*: Iterative tunneling search A*. Proceedings NationalConference Artificial Intelligence (AAAI), Workshop Heuristic Search, Memory-BasedHeuristics Applications, Boston, Massachusetts.Furcy, D., & Koenig, S. (2000). Speeding convergence real-time search. ProceedingsNational Conference Artificial Intelligence (AAAI), pp. 891897.Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determinationminimum cost paths. IEEE Transactions Systems Science Cybernetics, 4(2), 100107.Hernandez, C., & Meseguer, P. (2005a). Improving convergence LRTA*(k). ProceedingsInternational Joint Conference Artificial Intelligence (IJCAI), Workshop PlanningLearning Priori Unknown Dynamic Domains, Edinburgh, UK.Hernandez, C., & Meseguer, P. (2005b). LRTA*(k). Proceedings 19th International JointConference Artificial Intelligence (IJCAI), Edinburgh, UK.Hernandez, C., & Meseguer, P. (2007). Improving real-time heuristic search initially unknownmaps. Proceedings International Conference Automated Planning Scheduling(ICAPS), Workshop Planning Games, p. 8, Providence, Rhode Island.Hoffmann, J. (2000). heuristic domain independent planning use enforced hillclimbing algorithm. Proceedings 12th International Symposium MethodologiesIntelligent Systems (ISMIS), pp. 216227.id Software (1993). Doom., Published id Software, http://en.wikipedia.org/ wiki/Doom, December 10, 1993.Ishida, T. (1992). Moving target search intelligence. Proceedings National ConferenceArtificial Intelligence (AAAI), pp. 525532.Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjou, A., & Shimada, S. (1999).Robocup rescue: Search rescue large-scale disasters domain autonomous agentsresearch. Man, Systems, Cybernetics, pp. 739743.Kocsis, L. (2003). Learning Search Decisions. Ph.D. thesis, University Maastricht.Koenig, S. (2004). comparison fast search methods real-time situated agents. Proceedings International Joint Conference Autonomous Agents Multiagent Systems(AAMAS), pp. 864871.Koenig, S. (2001). Agent-centered search. AI Magazine, 22(4), 109132.Koenig, S., Furcy, D., & Bauer, C. (2002). Heuristic search-based replanning. ProceedingsInt. Conference Artificial Intelligence Planning Scheduling, pp. 294301.Koenig, S., & Likhachev, M. (2006). Real-time adaptive A*. Proceedings InternationalJoint Conference Autonomous Agents Multiagent Systems, pp. 281288.Korf, R. (1985). Depth-first iterative deepening : optimal admissible tree search. ArtificialIntelligence, 27(3), 97109.450fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCHKorf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42(23), 189211.Korf, R. (1993). Linear-space best-first search. Artificial Intelligence, 62, 4178.Likhachev, M., Ferguson, D., Gordon, G., Stentz, A., & Thrun, S. (2005). Anytime dynamic A*:anytime, replanning algorithm. Proceedings International Conference AutomatedPlanning Scheduling (ICAPS).Likhachev, M., Gordon, G. J., & Thrun, S. (2004). ARA*: Anytime A* provable boundssub-optimality. Thrun, S., Saul, L., & Scholkopf, B. (Eds.), Advances Neural Information Processing Systems 16. MIT Press, Cambridge, MA.Likhachev, M., & Koenig, S. (2005). generalized framework lifelong planning A*. Proceedings International Conference Automated Planning Scheduling (ICAPS),pp. 99108.Lustrek, M. (2005). Pathology single-agent search. Proceedings Information Society Conference, pp. 345348, Ljubljana, Slovenia.Lustrek, M., & Bulitko, V. (2006). Lookahead pathology real-time path-finding. ProceedingsNational Conference Artificial Intelligence (AAAI), Workshop Learning Search,pp. 108114, Boston, Massachusetts.Mero, L. (1984). heuristic search algorithm modifiable estimate. Artificial Intelligence, 23,1327.Moore, A., & Atkeson, C. (1993). Prioritized sweeping: Reinforcement learning less dataless time. Machine Learning, 13, 103130.Nash, A., Daniel, K., & Felner, S. K. A. (2007). Theta*: Any-angle path planning grids.Proceedings National Conference Artificial Intelligence, pp. 11771183.Pearl, J. (1984). Heuristics. Addison-Wesley.Rayner, D. C., Davison, K., Bulitko, V., Anderson, K., & Lu, J. (2007). Real-time heuristic searchpriority queue. Proceedings International Joint Conference ArtificialIntelligence (IJCAI), pp. 23722377, Hyderabad, India.Russell, S., & Wefald, E. (1991). Right Thing: Studies Limited Rationality. MIT Press.Schaeffer, J. (1989). history heuristic alpha-beta search enhancements practice. IEEETransactions Pattern Analysis Machine Intelligence, PAMI-11(1), 12031212.Schaeffer, J. (2000). Search ideas Chinook. van den Herik, H. J., & Iida, H. (Eds.), GamesAI Research, pp. 1930. U. Maastricht.Shimbo, M., & Ishida, T. (2003). Controlling learning process real-time heuristic search.Artificial Intelligence, 146(1), 141.Sigmundarson, S., & Bjornsson, Y. (2006). Value Back-Propagation vs. Backtracking RealTime Search. Proceedings National Conference Artificial Intelligence (AAAI),Workshop Learning Search, pp. 136141, Boston, Massachusetts, USA.Stenz, A. (1995). focussed D* algorithm real-time replanning. ProceedingsInternational Joint Conference Artificial Intelligence (IJCAI), pp. 16521659.Stout, B. (2000). basics A* path planning. Game Programming Gems. Charles RiverMedia.451fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSONSturtevant, N. (2007). Memory-efficient abstractions pathfinding. Proceedings thirdconference Artificial Intelligence Interactive Digital Entertainment, pp. 3136, Stanford, California.Sturtevant, N., & Buro, M. (2005). Partial pathfinding using map abstraction refinement.Proceedings National Conference Artificial Intelligence, pp. 13921397.Sturtevant, N., & Jansen, R. (2007). analysis map-based abstraction refinement.Proceedings 7th International Symposium Abstraction, Reformulation Approximation, Whistler, British Columbia.Witten, I. H., & Frank, E. (2005). Data Mining: Practical machine learning tools techniques(2nd edition). Morgan Kaufmann, San Fransisco.452fiJournal Artificial Intelligence Research 32 (2008) 289353Submitted 09/07; published 05/08Optimal Approximate Q-value FunctionsDecentralized POMDPsFrans A. Oliehoekf.a.oliehoek@uva.nlIntelligent Systems Lab Amsterdam, University AmsterdamAmsterdam, NetherlandsMatthijs T.J. Spaanmtjspaan@isr.ist.utl.ptInstitute Systems Robotics, Instituto Superior TecnicoLisbon, PortugalNikos Vlassisvlassis@dpem.tuc.grDepartment Production Engineering Management, Technical University CreteChania, GreeceAbstractDecision-theoretic planning popular approach sequential decision making problems, treats uncertainty sensing acting principled way. single-agentframeworks like MDPs POMDPs, planning carried resorting Q-valuefunctions: optimal Q-value function Q computed recursive manner dynamicprogramming, optimal policy extracted Q . paper studywhether similar Q-value functions defined decentralized POMDP models (DecPOMDPs), policies extracted value functions. define twoforms optimal Q-value function Dec-POMDPs: one gives normative description Q-value function optimal pure joint policy another onesequentially rational thus gives recipe computation. computation, however,infeasible smallest problems. Therefore, analyze various approximateQ-value functions allow efficient computation. describe relate,prove provide upper bound optimal Q-value function Q . Finally,unifying previous approaches solving Dec-POMDPs, describe family algorithms extracting policies Q-value functions, perform experimentalevaluation existing test problems, including new firefighting benchmark problem.1. IntroductionOne main goals artificial intelligence (AI) development intelligent agents,perceive environment sensors influence environmentactuators. setting, essential problem agent decideaction perform certain situation. work, focus planning: constructingplan specifies action take situation agent might encountertime. particular, focus planning cooperative multiagent system (MAS):environment multiple agents coexist interact order perform jointtask. adopt decision-theoretic approach, allows us tackle uncertaintysensing acting principled way.Decision-theoretic planning roots control theory operations research.control theory, one controllers control stochastic system specific outputc2008AI Access Foundation. rights reserved.fiOliehoek, Spaan & Vlassisgoal. Operations research considers tasks related scheduling, logistics work flowtries optimize concerning systems. Decision-theoretic planning problemsformalized Markov decision processes (MDPs), frequently employedcontrol theory well operations research, also adopted AIplanning stochastic environments. fields goal find (conditional)plan, policy, optimal respect desired behavior. Traditionally, mainfocus systems one agent controller, last decade interestsystems multiple agents decentralized control grown.different, also related field game theory. Game theory considers agents,called players, interacting dynamic, potentially stochastic process, game. goalfind optimal strategies agents, specify playtherefore correspond policies. contrast decision-theoretic planning, game theoryalways considered multiple agents, consequence several ideas conceptsgame theory applied decentralized decision-theoretic planning.work apply game-theoretic models decision-theoretic planning multiple agents.1.1 Decision-Theoretic Planninglast decades, Markov decision process (MDP) framework gained popularityAI community model planning uncertainty (Boutilier, Dean, & Hanks,1999; Guestrin, Koller, Parr, & Venkataraman, 2003). MDPs used formalizediscrete time planning task single agent stochastically changing environment,condition agent observe state environment. Every time stepstate changes stochastically, agent chooses action selects particulartransition function. Taking action particular state time step inducesprobability distribution states time step + 1.agents objective formulated several ways. first type objectiveagent reaching specific goal state, example maze agentsgoal reach exit. different formulation given associating certain costexecution particular action particular state, case goalminimize expected total cost. Alternatively, one associate rewards actionsperformed certain state, goal maximize total reward.agent knows probabilities state transitions, i.e., knowsmodel, contemplate expected transitions time construct planlikely reach specific goal state, minimizes expected costs maximizesexpected reward. stands contrast reinforcement learning (RL) (Sutton &Barto, 1998), agent model environment, learngood behavior repeatedly interacting environment. Reinforcement learningseen combined task learning model environment planning,although practice often necessary explicitly recover environment model.article focus planning, consider two factors complicate computingsuccessful plans: inability agent observe state environment wellpresence multiple agents.real world agent might able determine state environment exactly is, agents sensors noisy and/or limited. sensors290fiOptimal Approximate Q-Value Functions Dec-POMDPsnoisy, agent receive faulty inaccurate observations probability.sensors limited agent unable observe differences states cannotdetected sensor, e.g., presence absence object outside laser rangefinders field view. sensor reading might require different action choices,phenomenon referred perceptual aliasing. order deal introducedsensor uncertainty, partially observable Markov decision process (POMDP) extendsMDP model incorporating observations probability occurrence conditionalstate environment (Kaelbling, Littman, & Cassandra, 1998).complicating factor consider presence multiple agents. Insteadplanning single agent plan team cooperative agents. assumecommunication within team possible.1 major problem settingagents coordinate actions. Especially, agents assumedobserve stateeach agent knows observations received actions takencommon signal condition actions on. Note problemaddition problem partial observability, substitution it; evenagents could freely instantaneously communicate individual observations,joint observations would disambiguate true state.One option consider agent separately, agent maintainexplicit model agents. approach chosen InteractivePOMDP (I-POMDP) framework (Gmytrasiewicz & Doshi, 2005). problem approach, however, agents also model considered agent, leadinginfinite recursion beliefs regarding behavior agents. adopt decentralizedpartially observable Markov decision process (Dec-POMDP) model class problems(Bernstein, Givan, Immerman, & Zilberstein, 2002). Dec-POMDP generalizationmultiple agents POMDP used model team cooperative agentssituated stochastic, partially observable environment.single-agent MDP setting received much attention, many results known.particular known optimal plan, policy, extracted optimalaction-value, Q-value, function Q (s,a), latter calculated efficiently.POMDPs, similar results available, although finding optimal solution harder(PSPACE-complete finite-horizon problems, Papadimitriou & Tsitsiklis, 1987).hand, Dec-POMDPs relatively little known exceptprovably intractable (NEXP-complete, Bernstein et al., 2002). particular, outstandingissue whether Q-value functions defined Dec-POMDPs (PO)MDPs,whether policies extracted Q-value functions. Currently algorithms planning Dec-POMDPs based version policy search (Nair,Tambe, Yokoo, Pynadath, & Marsella, 2003b; Hansen, Bernstein, & Zilberstein, 2004; Szer,Charpillet, & Zilberstein, 2005; Varakantham, Marecki, Yabu, Tambe, & Yokoo, 2007),proper theory Q-value functions Dec-POMDPs still lacking. Given wide rangeapplications value functions single-agent decision-theoretic planning, expecttheory Dec-POMDPs great benefits, terms providing insightwell guiding design solution algorithms.1. turns out, framework consider also model communication particular costsubject minimization (Pynadath & Tambe, 2002; Goldman & Zilberstein, 2004). noncommunicative setting interpreted special case infinite cost.291fiOliehoek, Spaan & Vlassis1.2 Contributionspaper develop theory Q-value functions Dec-POMDPs, showingoptimal Q-function Q defined Dec-POMDP. define two forms optimalQ-value function Dec-POMDPs: one gives normative description Q-valuefunction optimal pure joint policy another one sequentially rationalthus gives recipe computation. also show given Q , optimal policycomputed forward-sweep policy computation, solving sequence Bayesian gamesforward time (i.e., first last time step), thereby extendingsolution technique Emery-Montemerlo, Gordon, Schneider, Thrun (2004)exact setting.Computation Q infeasible smallest problems. Therefore, analyzethree different approximate Q-value functions QMDP , QPOMDP QBGefficiently computed constitute upper bounds Q . also describe generalized form QBG includes QPOMDP , QBG Q . used prove hierarchyupper bounds: Q QBG QPOMDP QMDP .Next, show approximate Q-value functions used compute optimalsub-optimal policies. describe generic policy search algorithm, dubGeneralized MAA (GMAA ) generalization MAA algorithm Szer et al.(2005), used extracting policy approximate Q-value function.varying implementation sub-routine algorithm, algorithm unifies MAAforward-sweep policy computation thus approach Emery-Montemerlo et al.(2004).Finally, experimental evaluation examine differences QMDP ,QPOMDP , QBG Q several problems. also experimentally verify potentialbenefit tighter heuristics, testing different settings GMAA well knowntest problems new benchmark problem involving firefighting agents.article based previous work Oliehoek Vlassis (2007)abbreviated OVherecontaining several new contributions: (1) Contrary OV work, current workincludes section sequential rational description Q suggests way computeQ practice (OV provided normative description Q ). (2) current workprovides formal proof hierarchy upper bounds Q (which qualitativelyargued OV paper). (3) current article additionally contains proofsolutions Bayesian games identical payoffs given equation (4.2) constitutePareto optimal Nash equilibria game (which proven OV paper). (4)article contains extensive experimental evaluation derived boundsQ , introduces new benchmark problem (firefighting). (5) Finally, current articleprovides complete introduction Dec-POMDPs existing solution methods,well Bayesian games, hence serve self-contained introduction Dec-POMDPs.1.3 ApplicationsAlthough field multiagent systems stochastic, partially observable environmentseems quite specialized thus narrow, application area actually broad.real world practically always partially observable due sensor noise perceptualaliasing. Also, domains communication free, consumes resources292fiOptimal Approximate Q-Value Functions Dec-POMDPsthus particular cost. Therefore models Dec-POMDPs, considerpartially observable environments relevant essentially teams embodied agents.Example applications type given Emery-Montemerlo (2005), considered multi-robot navigation team agents noisy sensors actfind/capture goal. Becker, Zilberstein, Lesser, Goldman (2004b) use multi-robotspace exploration example. Here, agents Mars rovers decideproceed mission: whether collect particular samples specific sites not.rewards particular samples sub- super-additive, making task non-trivial.overview application areas cooperative robotics presented Arai, Pagello,Parker (2002), among robotic soccer, applied RoboCup (Kitano, Asada, Kuniyoshi, Noda, & Osawa, 1997). Another application investigated within projectcrisis management: RoboCup Rescue (Kitano, Tadokoro, Noda, Matsubara, Takahashi,Shinjoh, & Shimada, 1999) models situation rescue teams perform searchrescue task crisis situation. task also modeled partially observable system (Nair, Tambe, & Marsella, 2002, 2003, 2003a; Oliehoek & Visser, 2006; Paquet,Tobin, & Chaib-draa, 2005).also many types applications. Nair, Varakantham, Tambe,Yokoo (2005), Lesser, Ortiz Jr., Tambe (2003) give applications distributed sensornetworks (typically used surveillance). example load balancing among queuespresented Cogill, Rotkowitz, Roy, Lall (2004). agents represent queuesobserve queue sizes immediate neighbors. decidewhether accept new jobs pass another queue. Another frequently consideredapplication domain communication networks. Peshkin (2001) treated packet routingapplication agents routers minimize average transfer timepackets. connected immediate neighbors decide time stepneighbor send packet. approaches communication networks usingdecentralized, stochastic, partially observable systems given Ooi Wornell (1996),Tao, Baxter, Weaver (2001), Altman (2002).1.4 Overview Articlerest article organized follows. Section 2 first formally introduceDec-POMDP model provide background components. existing solutionmethods treated Section 3. Then, Section 4 show Dec-POMDPmodeled series Bayesian games constitutes theory Q-value functionsBGs. also treat two forms optimal Q-value functions, Q , here. ApproximateQ-value functions described Section 5 one applications discussedSection 6. Section 7 presents results experimental evaluation. Finally, Section 8concludes.2. Decentralized POMDPssection define Dec-POMDP model discuss properties. Intuitively, Dec-POMDP models number agents inhabit particular environment,considered discrete time steps, also referred stages (Boutilier et al., 1999)(decision) epochs (Puterman, 1994). number time steps agents interact293fiOliehoek, Spaan & Vlassisenvironment called horizon decision problem, denoted h.paper horizon assumed finite. stage = 0,1,2, . . . ,h 1 every agenttakes action combination actions influences environment, causingstate transition. next time step, agent first receives observation environment, take action again. probabilities state transitionsobservations specified Dec-POMDP model, rewards received particular actions particular states. transition- observation probabilities specifydynamics environment, rewards specify behavior desirable. Hence,reward model defines agents goal task: agents come planmaximizes expected long term reward signal. work assume planningtakes place off-line, computed plans distributed agents,merely execute plans on-line. is, computation plan centralized,execution decentralized. centralized planning phase, entire model detailedavailable. execution agent knows joint policy foundplanning phase individual history actions observations.2.1 Formal Modelsection formally treat basic components Dec-POMDP. startgiving mathematical definition components.Definition 2.1. decentralizedpartially observableMarkov decision process (DecffPOMDP) defined tuple n,S,A,T,R,O,O,h,b0 where:n number agents.finite set states.set joint actions.transition function.R immediate reward function.set joint observations.observation function.h horizon problem.b0 P(S), initial state distribution time = 0.2Dec-POMDP model extends single-agent (PO)MDP models considering jointactions observations. particular, define = Ai set joint actions. Here,Ai set actions available agent i. Every time step, one joint action = ha1 ,...,antaken. Dec-POMDP, agents know individual action; observeothers actions. assume action ai Ai selected time.set Ai depend stage state environment. general,2. P() denotes set probability distributions ().294fiOptimal Approximate Q-Value Functions Dec-POMDPsdenote stage using superscripts, denotes joint action taken stage t, atiindividual action agent taken stage t. Also, write a6=i = ha1 , . . . ,ai1 ,ai+1 , . . . ,anprofile actions agents i.Similarly set joint actions, = Oi set joint observations,Oi set observations available agent i. Every time step environment emits onejoint observation = ho1 ,...,on i, agent observes componentoi , illustrated Figure 1. Notation respect time indices observationsanalogous notation actions. paper, assume actionand observation sets finite. Infinite action- observation sets difficultdeal even single-agent case, authors knowledge researchperformed topic partially observable, multiagent case.Actions observations interface agents environment.Dec-POMDP framework describes environment states transitions.means rather considering complex, typically domain-dependent modelenvironment explains environment works, descriptive stance taken:Dec-POMDP specifies environment model simply set states = s1 ,...,s|S|environment in, together probabilities state transitionsdependent executed joint actions. particular, transition state nextstate depends stochastically past states actions. probabilistic dependencemodels outcome uncertainty: fact outcome action cannot predictedfull certainty.important characteristic Dec-POMDPs states possess Markovproperty. is, probability particular next state depends current statejoint action, whole history:P (st+1 |st ,at ,st1 ,at1 ,...,s0 ,a0 ) = P (st+1 |st ,at ).(2.1)Also, assume transition probabilities stationary, meaningindependent stage t.way similar transition model describes stochastic influenceactions environment, observation model describes state environment perceived agents. Formally, observation function, mappingjoint actions successor states probability distributions joint observations:: P(O). I.e., specifiesP (ot |at1 ,st ).(2.2)implies observation model also satisfies Markov property (theredependence history). Also observation model assumed stationary:dependence stage t.Literature identified different categories observability (Pynadath & Tambe, 2002;Goldman & Zilberstein, 2004). observation function individualobservation agents always uniquely identify true state, problemconsidered fully- individually observable. case, Dec-POMDP effectivelyreduces multiagent MDP (MMDP) described Boutilier (1996). extremeproblem non-observable, meaning none agents observes useful295fiOliehoek, Spaan & Vlassisactionsa0na1nh1.........a01a11a1h1o01o0no1nobservationso0statess00o11h1o1a0s1oh1...a1...o1h1ah2sh1ah1h11Figure 1: illustration dynamics Dec-POMDP. every stage environmentparticular state. state emits joint observation, agentobserves individual observation. agent selects action formingjoint action.information.modeled fact agents always receive null-observation,Oi = oi, . non-observability agents employ open-loop plan.two extremes partially observable problems. One special caseidentified, namely case individual, joint observation identifiestrue state. case referred jointly- collectively observable. jointly observableDec-POMDP referred Dec-MDP.reward function R(s,a) used specify goal agents function states joint actions. particular, desirable sequence joint actionscorrespond high long-term reward, formalized return.Definition 2.2. Let return cumulative reward Dec-POMDP defined totalrewards received execution:r(0) + r(1) + + r(h 1),(2.3)r(t) reward received time step t.When, stage t, state st taken joint action , r(t) =R(st ,a). Therefore, given sequence states taken joint actions, straightforwarddetermine return substitution r(t) R(st ,a) (2.3).296fiOptimal Approximate Q-Value Functions Dec-POMDPspaper consider optimality criterion expected cumulative reward,expectation refers expectation sequences states executed joint actions.planning problem find conditional plan, policy, agent maximizeoptimality criterion. Dec-POMDP case amounts finding tuple policies,called joint policy maximizes expected cumulative reward.Note that, Dec-POMDP, agents assumed observe immediaterewards: observing immediate rewards could convey information regarding true statepresent received observations, undesirable informationavailable agents modeled observations. planning DecPOMDPs thing matters expectation cumulative future rewardavailable off-line planning phase, actual reward obtained. Indeed,even assumed actual reward observed end episode.Summarizing, work consider Dec-POMDPs finite actions observationsets finite planning horizon. Furthermore, consider general Dec-POMDP setting, without simplifying assumptions observation, transition, reward models.2.2 Example: Decentralized Tiger Problemdescribe decentralized tiger problem introduced Nair et al. (2003b).test problem frequently used (Nair et al., 2003b; Emery-Montemerlo et al.,2004; Emery-Montemerlo, Gordon, Schneider, & Thrun, 2005; Szer et al., 2005)modification (single-agent) tiger problem (Kaelbling et al., 1998). concerns twoagents standing hallway two doors. Behind one doors tiger,behind treasure. Therefore two states: tiger behind left door(sl ) behind right door (sr ). agents 3 actions disposal: openleft door (aOL ), open right door (aOR ) listen (aLi ). cannot observeothers actions. fact, receive 2 observations. Either hear sound left(oHL ) right (oHR ).= 0 state sl sr probability 0.5. long agent opens doorstate doesnt change, door opened, state resets sl sr probability 0.5.full transition, observation reward model listed Nair et al. (2003b).observation probabilities independent, identical agents. instance,state sl perform action aLi , agents 85% chance observingoHL , probability hearing tiger left 0.85 0.85 = 0.72.agents open door treasure receive positive reward,receive penalty opening wrong door. opening wrong door jointly,penalty less severe. Opening correct door jointly leads higher reward.Note that, wrong door opened one agents, attackedtiger receive penalty. However, neither agents observe attackpenalty episode continues. Arguably, natural representation wouldepisode end door opened let agents observe whetherencountered tiger treasure, however considered test problem.297fiOliehoek, Spaan & Vlassis2.3 Historiesmentioned, goal planning Dec-POMDP find (near-) optimal tuplepolicies, policies specify agent act specific situation.Therefore, define policy, first need define exactly specificsituations are. essence situations parts history processagents observe.Let us first consider history process is. Dec-POMDP horizon hspecifies h time steps stages = 0,...,h 1. stages, state st ,joint observation ot joint action . Therefore, agents selectk-th actions (at = k 1), history process sequence states, jointobservations joint actions, following form:s0 ,o0 ,a0 ,s1 ,o1 ,a1 ,...,sk1 ,ok1 .s0 initial state, drawn according initial state distribution b0. initialffjoint observation o0 assumed empty joint observation: o0 = = o1, ,...,on, .history process, states remain unobserved agentobserve actions observations. Therefore agent base decisionregarding action select sequence actions observations observedpoint.Definition 2.3. define action-observation history agent i, ~i , sequenceactions taken observations received agent i. specific time step t, is:~it = o0i ,a0i ,o1i . . . ,at1,oi .~ action-observation history agents:joint action-observation history, ,~ = h~1t , . . . ,~nt i.~ = (Oi Ai ).Agent set possible action-observation histories time~ = h1~t 3set possible action-observation histories agentt=0 . Finally seth1 ~~~ tn ).possible joint action-observation histories given = t=0 (1 ...= 0, action-observation history empty, denoted ~ 0 = ~ .also use notion history using observations agent.Definition 2.4. Formally, define observation history agent i, ~oi , sequenceobservations agent received. specific time step t, is:~oit = o0i ,o1i , . . . ,oti .joint observation history, ~o, observation history agents:~o = h~o1t , . . . ,~ont i.3. Note particular Dec-POMDP, may case histories actuallyrealized, probabilities specified transition observation model.298fiOptimal Approximate Q-Value Functions Dec-POMDPs~ = Oi . Similarset observation histories agent time denoted~~ empty observationnotation action-observation histories, also usehistory denoted ~o .Similarly define action history follows.Definition 2.5. action history agent i, ~ai , sequence actions agentperformed. specific time step t, write:~ait = a0i ,a1i , . . . ,at1.Notation joint action histories sets analogous observation histories. Also write ~o6=i ,~6=i , etc. denote tuple observation-, action-observation histories,etc. agents except i. Finally note that, clearly, (joint) action-observationffhistory consists (joint) action- (joint) observation history: ~ = ~o ,~a .2.4 Policiesdiscussed previous section, action-observation history agent specifiesinformation agent decide upon action. momentassume individual policy agent deterministic mapping actionobservation sequences actions.number possible action-observation histories usually large setgrows exponentially horizon problem. time step t, (|Ai | |Oi |)taction-observation histories agent i. consequence totalh1X(|Ai | |Oi |)t =t=0(|Ai | |Oi |)h 1(|Ai | |Oi |) 1sequences agent i. Therefore number policies agent becomes:|Ai(|Ai ||Oi |)h 1| (|Ai ||Oi |)1,(2.4)doubly exponential horizon h.2.4.1 Pure Stochastic Policiespossible reduce number policies consideration realizing lotpolicies specify behavior. illustrated left side Figure 2,clearly shows deterministic policy subset possible action-observationhistories reached. Policies differ respect action-observation historyreached first place, manifest behavior. consequenceorder specify deterministic policy, observation history suffices: agenttakes action deterministically, able infer action tookobservation history illustrated right side Figure 2.Definition 2.6. pure deterministic policy, , agent Dec-POMDP~ Ai . set pure policiesmapping observation histories actions, :agent denoted .299fiOliehoek, Spaan & Vlassisact.-obs. historyaLioHLoHRaOLaORaLiaOLaLiaORoHLaOLaORaOLoHRaLiaLioHLoHRoHLoHRaOLaLiaLiaORFigure 2: deterministic policy represented tree. Left: tree actionobservation histories ~i one agents Dec-Tiger problem.arbitrary deterministic policy highlighted. Clearly shownreaches subset histories ~i . (~i reached expanded.) Right: policy shown simplified policy tree.Note also pure policies sometimes write (~i ). case meanaction specifies observation history contained ~i . instance, let ~i =h~oi ,~ai i, (~i ) = (~oi ). use = h1 ,...,n denote joint policy, profilespecifying policy agent. say pure joint policy induced implicit~ A. is, mappingmapping joint observation histories joint actions :induced individual policies make joint policy. Also use 6=i =h1 , . . . ,i1 ,i+1 , . . . ,n i, denote profile policies agents i.Apart pure policies, also possible agents execute randomizedpolicies, i.e., policies always specify action situation,element chance decides action performed.two types randomized policies: mixed policies stochastic policies.Definition 2.7. mixed policy, , agent set pure policies, , alongprobability distribution set. Thus mixed policy P(M) elementset probability distributions M.Definition 2.8. stochastic behavioral policy, , agent mapping action~ P(Ai ).observation histories probability distributions actions, :considering stochastic policies, keeping track observations insufficient, general action-observation histories realized. stochasticpolicies defined mapping full space action-observation histories probability distributions actions. Note use denote policy (space)general, also randomized policies. use , needdiscriminate different types policies.300fiOptimal Approximate Q-Value Functions Dec-POMDPscommon way represent temporal structure policy split decisionrules specify policy stage. individual policy representedsequence decision rules = (i0 , . . . ,ih1 ). case deterministic policy, formdecision rule stage mapping length-t observation histories actions~ Ai .:2.4.2 Special Cases Simpler Policies.special cases Dec-POMDPs policy specifiedsimpler way. treat three cases: case state observable,single-agent case case combines previous two: single agentenvironment observe state.last case, single agent fully observable environment, corresponds regularMDP setting. agent observe state, Markovian, agentneed rememberhistory, simply specify decision rules policy0h1= , . . . ,mappings states actions: : A. complexitypolicy representation reduces even infinite-horizon case, optimalpolicy known stationary. such, one decision rule , usedstages.true multiple agents observe state, i.e., fully observableDec-POMDP defined Section 2.1. essentially setting multiagentMarkov decision process (MMDP) introduced Boutilier (1996). case, decisionrules agent policy mappings states actions : Ai , althoughcase care needs taken make sure coordination errors occursearching individual policies.POMDP, Dec-POMDP single agent, agent cannot observe state,possible specify policy mapping states actions. However,turns maintaining probability distribution states, called belief, b P(S),Markovian signal:P (st+1 |at ,ot ,at1 ,ot1 , . . . ,a0 ,o0 ) = P (st+1 |bt ,at ),belief bt definedstbt (st ) P (st |ot ,at1 ,ot1 , . . . ,a0 ,o0 ) = P (st |bt1 ,at1 ,ot ).result, single agent partially observable environment specify policyseries mappings set beliefs actions : P(S) A.Unfortunately, general case consider, space-saving simplificationspolicy possible. Even though transition observation model usedcompute joint belief, computation requires knowledge joint actionsobservations. execution, agents simply access informationthus compute joint belief.2.4.3 Quality Joint PoliciesClearly, policies differ much reward expect accumulate, servecriterion joint policys quality. Formally, consider expected cumulativereward joint policy, also referred value.301fiOliehoek, Spaan & VlassisDefinition 2.9. value V () joint policy definedV () Eh1hXt=0fifiR(st ,at )fi,b0 ,(2.5)expectation states, observations andin case randomizedactions.particular calculate expectationV () =h1 X XXP (st ,~ |,b0 )t=0 ~~ stXR(st ,at )P (at |~ ),(2.6)P (at |~ ) probability specified , P (st ,~ |,b0 ) recursively definedXP (st ,~ |,b0 ) =P (st ,~ |st1 ,~ t1 ,)P (st1 ,~ t1 |,b0 ),(2.7)st1P (st ,~ |st1 ,~ t1 ,) = P (ot |at1 ,st )P (st |st1 ,at1 )P (at1 |~ t1 )(2.8)term completely specified transition observation model jointpolicy. stage 0 P (s0 ,~ |,b0 ) = b0 (s0 ).recursive nature P (st ,~ |,b0 ) intuitive specify valuerecursively:XX XV (st ,~ ) =P (at |~ ) R(st ,at ) +P (st+1 ,ot+1 |st ,at )V (st+1 ,~ t+1 ) ,st+1 ot+1(2.9)~ t+1 = (~ ,at ,ot+1 ). value joint policy givenXV () =V (s0 ,~ )b0 (s0 ).(2.10)s0special case evaluating pure joint policy , eq. (2.6) written as:V () =h1 XXP (~ |,b0 )R(~ ,(~ )),(2.11)t=0 ~~tR(~ ,at ) =XR(st ,at )P (st |~ ,b0 )(2.12)stdenotes expected immediate reward. case, recursive formulation (2.9) reducesX XVt (st ,~o ) = R st ,(~o ) +P (st+1 ,ot+1 |st ,(~o ))Vt+1 (st+1 ,~o t+1 ).(2.13)st+1 ot+1302fiOptimal Approximate Q-Value Functions Dec-POMDPsNote that, performing computation value joint policy recursively,intermediate results cached. particular (st+1 ,~o t+1 )-pair (or (st+1 ,~ t+1 )-pairstochastic joint policy) reached |S| states st previous stage. valueVt+1 (st+1 ,~o t+1 ) same, however, computed once.2.4.4 Existence Optimal Pure Joint PolicyAlthough randomized policies may useful, restrict attention pure policieswithout sacrificing optimality, shown following.Proposition 2.1. Dec-POMDP least one optimal pure joint policy.Proof. See appendix A.1.3. Overview Dec-POMDP Solution Methodsorder provide background solving Dec-POMDPs, section gives overviewrecently proposed methods. limit review number finite-horizonmethods general Dec-POMDPs related approach.review work performed infinite-horizon Dec-POMDPs,work Peshkin, Kim, Meuleau, Kaelbling (2000), Bernstein, Hansen, Zilberstein(2005), Szer Charpillet (2005), Amato, Bernstein, Zilberstein (2006, 2007a).setting policies usually represented finite state controllers (FSCs). Since infinitehorizon Dec-POMDP undecidable (Bernstein et al., 2002), line work, focusesfinding -approximate solutions (Bernstein, 2005) (near-) optimal policies givenparticular controller size.also substantial amount work methods exploiting particular independence assumptions. particular, transition observation independent Dec-MDPs(Becker et al., 2004b; Wu & Durfee, 2006) Dec-POMDPs (Kim, Nair, Varakantham,Tambe, & Yokoo, 2006; Varakantham et al., 2007) received quite attention.models assume agent individual state space Si actions one agent influence transitions local states another agent.Although models easier solve, independence assumptions severely restrictapplicability. special cases considered are, instance, goaloriented Dec-POMDPs (Goldman & Zilberstein, 2004), event-driven Dec-MDPs (Becker,Zilberstein, & Lesser, 2004a), Dec-MDPs time resource constraints (Beynier &Mouaddib, 2005, 2006; Marecki & Tambe, 2007), Dec-MDPs local interactions (Spaan& Melo, 2008) factored Dec-POMDPs additive rewards (Oliehoek, Spaan, Whiteson, & Vlassis, 2008).final body related work beyond scope article modelstechniques explicit communication Dec-POMDP settings (Ooi & Wornell, 1996; Pynadath & Tambe, 2002; Goldman & Zilberstein, 2003; Nair, Roth, & Yohoo, 2004; Becker,Lesser, & Zilberstein, 2005; Roth, Simmons, & Veloso, 2005; Oliehoek, Spaan, & Vlassis,2007b; Roth, Simmons, & Veloso, 2007; Goldman, Allen, & Zilberstein, 2007). DecPOMDP model model communication actions regular actions, casesemantics communication actions becomes part optimization problem (Xuan,Lesser, & Zilberstein, 2001; Goldman & Zilberstein, 2003; Spaan, Gordon, & Vlassis, 2006).303fiOliehoek, Spaan & Vlassiscontrast, approaches mentioned typically assume communication happens outside Dec-POMDP model pre-defined semantics. typical assumptionevery time step agents communicate individual observations selectingaction. Pynadath Tambe (2002) showed that, assumptions instantaneouscost-free communication, sharing individual observations way optimal.3.1 Brute Force Policy Evaluationexists optimal pure joint policy finite-horizon Dec-POMDP,theory possible enumerate different pure joint policies, evaluate using equations(2.10) (2.13) choose best one. number pure joint policies evaluatedis:n(|O |h 1)|A | |O |1,(3.1)|A | |O | denote largest individualaction observation sets. costevaluating policy |S| |O|h . resulting total cost brute-force policyevaluationn(|O |h 1)nh|A | |O |1 |S| |O |,(3.2)doubly exponential horizon h.3.2 Alternating MaximizationNair et al. (2003b) introduced Joint Equilibrium based Search Policies (JESP).method guarantees find locally optimal joint policy, specifically, Nash equilibrium: tuple policies agent policy best responsepolicies employed agents 6=i . relies process refer alternatingmaximization. procedure computes policy agent maximizesjoint reward, keeping policies agents fixed. Next, another agentchosen maximize joint reward finding best-response fixed policiesagents. process repeated joint policy converges Nash equilibrium,local optimum. main idea fixing agents others improvepolicy presented Chades, Scherrer, Charpillet (2002), usedheuristic approach memory-less agents. process alternating maximizationalso referred hill-climbing coordinate ascent.Nair et al. (2003b) describe two variants JESP, first which, Exhaustive-JESP,implements idea straightforward fashion: Starting random jointpolicy, first agent chosen. agent selects best-response policy evaluatingjoint reward obtained individual policies agents followfixed policy.second variant, DP-JESP, uses dynamic programming approach computebest-response policy selected agent i. essence, fixing policies agentsallows reformulation problem augmented POMDP. augmentedPOMDP state = hs,~o6=i consists nominal state observation histories304fiOptimal Approximate Q-Value Functions Dec-POMDPsagents ~o6=i . Given fixed deterministic policies agents 6=i ,augmented state Markovian state, transition observation probabilitieseasily derived 6=i .Like methods proposed Dec-POMDPs, JESP exploits knowledgeinitial belief b0 considering reachable beliefs b(s) solution POMDP.However, cases initial belief might available. demonstratedVarakantham, Nair, Tambe, Yokoo (2006), JESP extended plan entirespace initial beliefs, overcoming problem.3.3 MAASzer et al. (2005) introduced heuristically guided policy search method called multiagentA* (MAA ). performs guided A*-like search partially specified joint policies,pruning joint policies guaranteed worse best (fully specified) jointpolicy found far admissible heuristic.particular MAA considers joint policies partially specified respecttime: partial joint policy = ( 0 , 1 , . . . , t1 ) specifies joint decision rulesfirst stages. partial joint policy heuristic value Vb (t ) calculatedtaking V 0...t1 (t ), actual expected reward achieves first stages,adding Vb t...h1 , heuristic value remaining h stages. Clearly Vb t...h1admissible heuristica guaranteed overestimationso Vb (t ).MAA starts placing completely unspecified joint policy 0 open list.Then, proceeds selecting partial joint policies = ( 0 , 1 , . . . , t1 ) listexpanding them: generating t+1 = ( 0 , 1 , . . . , t1 , ) appending possible jointdecision rules next time step (t). left side Figure (3) illustrates expansionprocess. expansion, created children heuristically valuated placedopen list, partial joint policies t+1 Vb (t+1 ) less expected value V ()earlier found (fully specified) joint policy , pruned. search endslist becomes empty, point found optimal fully specified joint policy.3.4 Dynamic Programming Dec-POMDPsMAA incrementally builds policies first stage = 0 last = h 1. Priorwork, Hansen et al. (2004) introduced dynamic programming (DP) Dec-POMDPs,constructs policies way around: starting set 1-step policies(actions) executed last stage, construct set 2-step policiesexecuted h 2, etc.stressed policies maintained quite different usedMAA . particular partial policy MAA form = ( 0 , 1 , . . . , t1 ).policies maintained DP correspondence decision rules. definetime-to-go stage= h t.(3.3)qi =k denotes k-steps-to-go sub-tree policy agent i. is, qi =k policytree form full policy horizon-k problem. Within originalhorizon-h problem qi =k candidate execution starting stage = hk. set ksteps-to-go sub-tree policies maintained agent denoted Qi =k . Dynamic programming305fiOliehoek, Spaan & VlassisFigure 3: Difference policy construction MAA (left) dynamic programming(right) agent actions a,a observations o,o. dashed components newly generated, dotted components result previous iteration.MAA expands partial policy leaves, dynamic programmingbacks set sub-tree policies forming new ones.Dec-POMDPs based backup operations: constructing Qi =k+1 set sub-treepolicies qi =k+1 set Qi =k . instance, right side Figure 3 shows qi =3 ,3-steps-to-go sub-tree policy, constructed two qi =2 Qi =2 . Also illustrateddifference process MAA expansion (on left side).Dynamic programming consecutively constructs Qi =1 ,Qi =2 , . . . ,Qi =h agents i.However, size set Qi =k+1 given|Qi =k+1 | = |Ai | |Qi =k ||Oi | ,result sizes maintained sets grow doubly exponential k. countersource intractability, Hansen et al. (2004) propose eliminate dominated sub-treepolicies. expected reward particular sub-tree policy qi =k depends probabilitystates qi =k started (at stage = h k) well probability=k denoteagents j 6= select sub-tree policies qj =k Qj =k . let q6==ksub-tree profile agents i, Q6=i set profiles, say qi =kdominated maximizing point multiagent belief space: simplex=k . Hansen et al. test dominance entire multiagent belief spaceQ6=linear programming. Removal dominated sub-tree policy qi =k agent may causesub-tree policy qj =k agent j become dominated. Therefore Hansen et al.propose iterate agents pruning possible, procedure knowniterated elimination dominated policies (Osborne & Rubinstein, 1994).Finally, last backup step completed optimal policy foundevaluating joint policies Q1 =h Qn =h initial belief b0 .306fiOptimal Approximate Q-Value Functions Dec-POMDPs3.5 Extensions DP Dec-POMDPslast years several extensions dynamic programming algorithm DecPOMDPs proposed. first extensions due Szer Charpillet(2006). Rather testing dominance entire multiagent belief space, SzerCharpillet propose perform point-based dynamic programming (PBDP). orderprune set sub-tree policies Qi =k , set belief points Bi,reachable=k ) possibly reached deterministic joint policies generated.P(S Q6=sub-tree policies qi =k maximize value bi Bi,reachable kept.proposed algorithm optimal, intractable needs generatemultiagent belief points reachable joint policies. overcomebottleneck, Szer Charpillet propose randomly sample one joint policiesuse generate Bi,reachable .Seuken Zilberstein (2007b) also proposed point-based extension DP algorithm, called memory-bounded dynamic programming (MBDP). Rather usingrandomly selected policy generate belief points, propose use heuristic policies. important difference, however, lies pruning step. Rather pruningdominated sub-tree policies qi =k , MBDP prunes sub-tree policies exceptiteration. specifically, agent maxTrees sub-tree policies retained,parameter planning method. result, MBDP linear space timecomplexity respect horizon. MBDP algorithm still depends exhaus|Oi | sub-tree policies.tive generation sets Qi =k+1 containn |Ai | maxT rees|O|Moreover, iteration |A | maxT reesjoint sub-tree policies evaluated sampled belief points. counter growth, Seuken Zilberstein(2007a) proposed extension limits considered observations backupstep maxObs likely observations.Finally, extension DP Dec-POMDPs algorithm given Amato,Carlin, Zilberstein (2007b). approach, bounded DP (BDP), establishes boundused memory, quality approximation. particular, BDP uses-pruning iteration. is, qi =k maximizing regionmultiagent belief space, improves value region , also pruned.iterated elimination using - pruning still lead unbounded reductionvalue, Amato et al. propose perform one iteration -pruning, followed iteratedelimination using normal pruning.3.6 Approaches Finite-Horizon Dec-POMDPsapproaches finite-horizon Dec-POMDPs, brieflydescribe here. Aras, Dutech, Charpillet (2007) proposed mixed integer linear programming formulation optimal solution finite-horizon Dec-POMDPs. approach based representing set possible policies agent sequence form(Romanovskii, 1962; Koller, Megiddo, & von Stengel, 1994; Koller & Pfeffer, 1997). sequence form, single policy agent represented subset set sequences(roughly corresponding action-observation histories) agent. probleminterpreted combinatorial optimization problem, Aras et al. proposesolve mixed integer linear program.307fiOliehoek, Spaan & VlassisOliehoek, Kooij, Vlassis (2007a) also recognize finding solution DecPOMDPs essence combinatorial optimization problem propose applyCross-Entropy method (de Boer, Kroese, Mannor, & Rubinstein, 2005), method combinatorial optimization recently become popular ability findnear-optimal solutions large optimization problems. resulting algorithm performssampling-based policy search approximately solving Dec-POMDPs. operates sampling pure policies appropriately parameterized stochastic policy, evaluatespolicies either exactly approximately order define next stochastic policysample from, convergence.Finally, Emery-Montemerlo et al. (2004, 2005) proposed approximate Dec-POMDPsseries Bayesian games. Since work article basedrepresentation, defer detailed explanation next section. mentionEmery-Montemerlo et al. assume algorithm run on-line (interleavingplanning execution), assumption necessary. Rather applyframework off-line planning phase, like algorithms coveredoverview.4. Optimal Q-value Functionssection show Dec-POMDP modeled series Bayesiangames (BGs). BG game-theoretic model deal uncertainty (Osborne& Rubinstein, 1994). Bayesian games similar well-known normal form,matrix games, allow model agents private information. sectionintroduce Bayesian games show Dec-POMDP modeled seriesBayesian games (BGs). idea using series BGs find policies DecPOMDP proposed approximate setting Emery-Montemerlo et al. (2004).particular, showed using series BGs approximate payoff function,able obtain approximate solutions Dec-Tiger problem, comparableresults JESP (see Section 3.2).main result section optimal Dec-POMDP policy computedsolution sequence Bayesian games, payoff function gamescoincides Q-value function optimal policy , i.e., optimal Qvalue function Q . Thus, extend results Emery-Montemerlo et al. (2004)include optimal setting. Also, conjecture form Q computedwithout already knowing optimal policy . transferring game-theoretic conceptsequential rationality Dec-POMDPs, find description Q computablewithout knowing front.4.1 Game-Theoretic Backgroundexplain Dec-POMDPs modeled using Bayesian games,first introduce together necessary game theoretic background.308fiOptimal Approximate Q-Value Functions Dec-POMDPsC1, 10, + 2C+2,0+1, + 1B+20B0+2Figure 4: Left: game Chicken. players option (D)rive (C)hickenout. Right: meeting location problem. game identicalpayoffs, entry contains one number.4.1.1 Strategic Form Games Nash Equilibriabasis concept Bayesian game lies simpler form game: strategic-normal form game. strategic game consists set agents players,set actions (or strategies). combination selected actions specifies particularoutcome. strategic game consists two agents, visualized matrixshown Figure 4. first game shown called Chicken involves two teenagersdriving head on. option drive chicken out. teenagerspayoff maximal (+2) drives opponent chickens out. However,drive on, collision follows giving payoff 1. second game meetinglocation problem. agents want meet location B. preferencelocation, long pick location. game fully cooperative,modeled fact agents receive identical payoffs.Definition 4.1. Formally, strategic game tuple hn,A,ui, n numberagents, = Ai set joint actions, u = hu1 , . . . ,un ui : Rpayoff function agent i.Game theory tries specify agent play. is, game-theoreticsolution suggest policy agent. strategic game write denotepolicy agent joint policy. policy agent simply one actions= ai Ai (i.e., pure policy), probability distribution actions P(Ai )(i.e., mixed policy). Also, policy suggested agent rational givenpolicies suggested agent; would undesirable suggest particularpolicy agent, get better payoff switching another policy. Rather,suggested policies form equilibrium, meaning profitable agentunilaterally deviate suggested policy. notion formalized conceptNash equilibrium.Definition 4.2. pure policy profile = h1 , . . . ,i , . . . ,n specifying pure policyagent Nash Equilibrium (NE)ff(4.1)ui (h1 , . . . ,i , . . . ,n i) ui ( 1 , . . . ,i , . . . ,n ), i:1in , Ai .definition easily extended incorporate mixed policies definingui (h1 , . . . ,n i) =Xui (ha1 , . . . ,an i)ha1 ,...,,an309ni=1Pi (ai ).fiOliehoek, Spaan & VlassisNash (1950) proved allowing mixed policies, every (finite) strategic game containsleast one NE, making proper solution game. However, unclearNE found. particular, may multiple NEs game, making unclearone select. order make discrimination Nash equilibria,consider NEs NE better everyone.Definition 4.3. Nash Equilibrium = h1 , . . . ,i , . . . ,n referred Pareto Optimal (PO) NE specifies least payoff agentshigher payoff least one agent:ui ( ) ui () ui ( ) > ui () .case multiple Pareto optimal Nash equilibria exist, agents agreebeforehand particular ordering, ensure NE chosen.4.1.2 Bayesian GamesBayesian game (Osborne & Rubinstein, 1994) augmented normal form gameplayers hold private information. private information defines typeagent, i.e., particular type agent corresponds agent knowingparticular information. payoff agents receive longer dependsactions, also private information. Formally, BG defined follows:Definition 4.4. Bayesian game (BG) tuple hn,A,,P (), hu1 ,...un ii, nnumber agents, set joint actions, = set joint typesprobability function P () specified, ui : R payoff functionagent i.normal form game agents select action. Now, BG agentscondition action private information. means BGs agentsuse different type policies. BG, denote joint policy = h1 ,...,n i,individual policies mappings types actions: : Ai . caseidentical payoffs agents, solution BG given following theorem:Theorem 4.1. BG identical payoffs, i.e., i,j ui (,a) = uj (,a), solutiongiven by:X= arg maxP ()u(,()),(4.2)() = h1 (1 ),...,n (n )i joint action specified joint type .solution constitutes Pareto optimal Nash equilibrium.Proof. proof consists two parts: first shows Nash equilibrium,second shows Pareto optimal.Nash equilibrium proof. clear satisfying 4.2 Nash equilibriumrewriting perspective arbitrary agent follows:310fiOptimal Approximate Q-Value Functions Dec-POMDPs"= arg max max"6=i= arg max max"6=i= arg max max= arg max6=iXX#P ()u(,()) ,XXXP (i )P (i )X6=iP (6=i |i )6=iX"|X6=i##P (hi ,6=i i) u(,()) ,{zP (i )#}P (6=i |i )u(,()) ,6=iffP (6=i |i )u(hi ,6=i , (i ),6=(6=i ) ),. Since special assumptions mademeans best response 6=i, follows Nash equilibrium.Pareto optimality proof. Let us write Vi (ai ,6=i ) payoff agent expectsperforming ai agents use policy profile 6=i .Vi (ai ,6=i ) =XP (6=i |i )u(hi ,6=i , hai ,6=i (6=i )i).6=iNow, joint policy satisfying (4.2) Pareto optimal anotherNash equilibrium attains least payoff agents typesstrictly least one agent type. Formally Pareto optimalthat:Vi (i (i ),6=) Vi (i (i ),6=i ) Vi (i (i ),6=i ) < Vi (i (i ),6=i ). (4.3)prove exist contradiction. Suppose = hi ,6=NE (4.3) holds (and thus Pareto optimal). satisfies (4.2)know that:XXP ()u(, ()),(4.4)P ()u(, ())therefore, agentsP (i,1 )Vi,1 (i (i,1 ),6=) + ... + P (i,|i | )Vi,|i | (i (i,|i | ),6=i )P (i,1 )Vi,1 (i (i,1 ),6=) + ... + P (i,|i | )Vi,|i | (i (i,|i | ),6=i )holds. However, assumption satisfies (4.3) getjVi,j (i (i,j ),6=) < Vi,j (i (i,j ),6=i ).Therefore mustXXP (i,k )Vi,k (i (i,k ),6=P (i,k )Vi,k (i (i,k ),6=),i) >k6=jk6=j311fiOliehoek, Spaan & Vlassist=0joint actionsjoint observationsjoint act.-obs. historyha1 ,a2ha1 ,a2ha1 ,a2t=1ho1 ,o2ha1 ,a2ho1 ,o2ho1 ,o2ho1 ,o2Figure 5: Dec-POMDP seen tree joint actions observations.indicated planes correspond Bayesian games first two stages.thuskVi,k (i (i,k ),6=) > Vi,k (i (i,k ),6=i ),contradicting assumption satisfies (4.3).4.2 Modeling Dec-POMDPs Series Bayesian Gamesdiscuss Bayesian games used model Dec-POMDPs. Essentially,Dec-POMDP seen tree nodes joint action-observation historiesedges represent joint actions observations, illustrated Figure 5. specificstage Dec-POMDP, main difficulty coordinating action selection presentedfact agent individual action-observation history. is,global signal agents use coordinate actions. situationconveniently modeled Bayesian game discuss.time step t, one directly associate primitives Dec-POMDPBG identical payoffs: actions agents cases,~ . Figure 6 showstypes agent correspond action-observation historiesBayesian games = 0 = 1 fictitious Dec-POMDP 2 agents.denote payoff function BG models stage Dec-POMDP~Q( ,a). payoff function naturally defined accordance valuefunction planning task. instance, Emery-Montemerlo et al. (2004) define Q(~ ,a)312fiOptimal Approximate Q-Value Functions Dec-POMDPsQMDP -value underlying MDP. extensively discuss payofffunction Section 4.3.probability P () equal probability joint action-observation historycorresponds depends past joint policy = ( 0 , . . . , t1 )initial state distribution. calculated marginal (2.7):P () = P (~ |t ,b0 ) =XP (st ,~ |t ,b0 ).(4.5)st~considering pure joint policies , action probability component P (a|)(2.7) 1 joint action-observation histories ~ consistent past jointpolicy 0 otherwise. say action-observation ~i history consistentpure policy occur executing , i.e., actions ~i wouldselected . Let us formally define consistency follows.Definition 4.5 (Consistency). Let us write ~it restriction ~it stage 0, . . . ,t(with 0 < t). action-observation history ~it agent consistent purepolicy time step 0 <(~it ) = (~oit ) = ati(t + 1)-th action ~it . joint action-observation history ~ = h~1t , . . . ,~nt consistent pure joint policy = h1 , . . . ,n individual ~it consistentcorresponding individual policy . C indicator function consistency. instanceC(~ ,) filters action-observation histories ~ inconsistent jointpure policy :C(~ ,) =(1 , ~ = o0 ,(o0 ),o1 ,(o0 ,o1 ),... )0 , otherwise.(4.6)~ {~ | C(~ ,) = 1} set ~ consistent .also writedefinition allows us writeP (~ |t ,b0 ) = C(~ ,t )XP (st ,~ |b0 )(4.7)stP (st ,~ |b0 ) =XP (ot |at1 ,st )P (st |st1 ,at1 )P (st1 ,~ t1 |b0 ).(4.8)st1Figure 6 illustrates indicator function filters policies, t=0 (~ t=0 ) =ha1 ,a2 i, non-shaded part BG = 1 reached (has positive probability).313fiOliehoek, Spaan & Vlassis~2t=0~1t=0()~2t=1~1t=1(a1 ,o1 )(a1 ,o1 )(a1 ,o1 )(a1 ,o1 )a1a1a1a1a1a1...a1a1()a2a2+2.75 4.10.9+0.3(a2 ,o2 )a2a20.3+0.60.6+2.0+3.1+4.4+1.12.90.40.90.94.5......(a2 ,o2 )a2a20.6+4.01.3+3.61.9+1.0+2.00.40.51.01.0+3.5...........................Figure 6: Bayesian game first second time step (top: = 0, bottom: = 1).entries ~ , given payoff function Q(~ ,at ). Light shaded entriesindicate solutions. Dark entries realized given ha1 ,a2 solutionBG = 0.4.3 Q-value Function Optimal Joint PolicyGiven perspective Dec-POMDP interpreted series BGs outlinedprevious section, solution BG stage joint decision rule . payofffunction BG chosen well, quality high. Emery-Montemerloet al. (2004) try find good joint policy = ( 0 , . . . , h1 ) procedure referforward-sweep policy computation (FSPC): one sweep forward time, BGstage = 0,1, . . . ,h 1 consecutively solved. such, payoff functionBGs constitute call Q-value function Dec-POMDP.Here, show optimal Q-value function Q : using Qpayoff functions BGs, forward-sweep policy computation lead optimaljoint policy = ( 0, , . . . , h1, ). first give derivation Q . Next,discuss Q indeed used calculate , computing Q seems impracticalwithout already knowing optimal joint policy . issue addressedSection 4.4.4.3.1 Existence Qstate theorem identifying normative description Q Q-value functionoptimal joint policy.Theorem 4.2. expected cumulative reward stages t, . . . ,h 1 induced ,optimal joint policy Dec-POMDP, given by:V ( ) =XP (~ |b0 )Q (~ , (~ )),~t~314(4.9)fiOptimal Approximate Q-Value Functions Dec-POMDPsff~ = ~o ,~a , (~ ) = (~o ) denotes joint action pure joint policyspecifies ~o ,XQ (~ ,a) = R(~ ,a) +P (ot+1 |~ ,a)Q (~ t+1 , (~ t+1 ))(4.10)ot+1Q-value function , gives expected cumulative future rewardtaking joint action ~ given optimal joint policy followed hereafter.Proof. filling (2.11) optimal pure jointpolicy , obtain expectedficumulative reward summation E R(st ,at )fi expected rewards yieldstime step:V ( ) =h1Xt=0h1 Xfi XfiP (~ | ,b0 )R(~ , (~ )).E R(s ,a ) =(4.11)t=0 ~~tequation, P (~ | ,b0 ) given (4.7). result, influence P (~ | ,b0 )C. I.e., used filter inconsistent histories. Thereforewrite:XfiP (~ |b0 )R(~ , (~ )),(4.12)E R(st ,at )fi =~t~P (~ |b0 ) given directly taking marginal (4.8). Now, let us definevalue starting time step t:XfiP (~ |b0 )R(~ , (~ )) + V t+1 ( ). (4.13)V ( ) = E R(st ,at )fi + V t+1 ( ) =~t~last time step h 1 expected future reward, get:XP (~ h1 |b0 ) R(~ h1 , (~ h1 )) .V h1 ( ) =|{z}h1~~ h1(4.14)Q (~ h1 , (~ h1 ))time step h 2 becomes:hfiV h2 ( ) E R(sh2 ,ah2 )fi + V h1 ( ) =XXP (~ h2 |b0 )R(~ h2 , (~ h2 )) +P (~ h1 |b0 )Q (~ h1 , (~ h1 )).~ h1~ h1~ h2~ h2(4.15)P (~ h1 ) = P (~ h2 )P (oh1 |~ h2 , (~ h2 )), (4.15) rewritten to:V h2 ( ) =XP (~ h2 |b0 )Q (~ h2 , (~ h2 )),~ h2~ h2315(4.16)fiOliehoek, Spaan & VlassisQ (~ h2 , (~ h2 )) = R(~ h2 , (~ h2 ))+XP (oh1 |~ h2 , (~ h2 ))Q (~ h1 , (~ h1 )). (4.17)oh1Reasoning way see (4.9) (4.10) constitute generic expressionexpected cumulative future reward starting time step t.Note derivation, explicitly included b0 one given arguments.rest text, always assume b0 given therefore omit it, unlessnecessary.4.3.2 Deriving Optimal Joint Policy Qpoint derived Q , Q-value function optimal joint policy. Now,extend results Emery-Montemerlo et al. (2004) exact setting:Theorem 4.3. Applying forward-sweep policy computation using Q defined (4.10)yields optimal joint policy.Proof. Note that, per definition, optimal Dec-POMDP policy maximizes expectedfuture reward V ( ) specified (4.9). Therefore t, , optimal decision rule stage t,identical optimal joint policy t, Bayesian game time step t, payofffunction BG given Q , is:Xt, t, = arg maxP (~ )Q (~ , (~ )).(4.18)~t~Equation (4.18) tells us t, t, . means possible constructcomplete optimal Dec-POMDP policy = ( 0, , . . . , h1, ), computing t, t.subtlety calculation (4.18) dependent optimal joint~ {~ | C(~ , ) = 1}. resolvedpolicy, summation ~realizing past actions influence action-observation historiesreached time step t. Formally, let = ( 0, , . . . , t1, ) denote past joint policy,partial joint policy specified stages 0,...,t 1. denote optimal past joint~t =~ t, , therefore that:policy t, ,t, = arg maxXP (~ )Q (~ , (~ )).(4.19)~t~t,solved forward manner time steps = 0,1,2,...,h 1, everytime step t, = ( 0, , . . . , t1, ) available: specified ( 0, ,..., t1, )solutions previously solved BGs.316fiOptimal Approximate Q-Value Functions Dec-POMDPs4.3.3 Computing Qfar discussed Q used find optimal joint policy . Unfortunately,optimal joint policy known, computing Q impractical,discuss here. contrast (fully observable) single-agent caseoptimal Q-values found relatively easily single sweep backward time.MDPs POMDPs compute Q-values time step+ 1 applying backup operator. possible single agentperceives Markovian signal. allows agent (1) select optimal action (policy)next time step (2) determine expected future reward given optimalaction (policy) found step 1. instance, backup operator POMDP givenby:XQ (bt ,a) = R(bt ,a) +P (o|bt ,a) max Q (bt+1 ,a),rewritten 2-step procedure:1. t+1, (bt+1 ) = arg maxa Q (bt+1 ,a )P2. Q (bt ,a) = R(bt ,a) + P (o|bt ,a)Q (bt+1 , t+1, (bt+1 )).case Dec-POMDPs, step 2 would correspond calculating Q using (4.10)thus depends t+1, optimal joint policy next stage. However, step 1calculates t+1, , corresponds (4.19) therefore dependent t+1, (an optimaljoint policy time steps 0,...,t). calculate Qt, optimal Q-value functionspecified (4.10) stage t, optimal joint policy including stageneeded. Effectively, dependence future past optimal policy,rather future optimal policies single agent case. clearsolution seems evaluation possible past policies, detailed next. conjectureproblem encountered inherent decentralized decision makingimperfect information. example, also observe exact point-based dynamicprogramming Dec-POMDPs, described Section 3.5, necessarygenerate (multiagent belief points generated all) possible past policies.4.4 Sequential Rationality Dec-POMDPsconjectured computing Q introduced Section 4.3 seems impractical withoutknowing . relate concepts game theory. particular,discuss different formulation Q based principle sequential rationality, i.e.,also considering joint action-observation histories realized given optimaljoint policy. formulation Q computable without knowing optimal joint policyadvance, present dynamic programming algorithm perform computation.4.4.1 Sub-game Perfect Sequential Equilibriaproblem facing much related notion sub-game perfect equilibriagame theory. sub-game perfect Nash equilibrium = h1 , . . . ,n characteristic contained policies specify optimal action possible situationseven317fiOliehoek, Spaan & Vlassissituations occur following . commonly given rationale behindconcept that, mistake one agents execution, situationsoccur according , occur, also situations agents act optimally. different rationale given Binmore (1992), remarks temptingshrug ones shoulders difficulties [because] rational players strayequilibrium path, would clearly mistake, agents remainequilibrium path anticipate would happendeviate. implies agents decide upon Nash equilibrium analyzingexpected outcome would following policies: is, acting optimallysituations. perform similar reasoning Dec-POMDPs,similar fashionwill result description allows deduce optimal Q-valuefunction thus joint policy.Dec-POMDP modeled extensive form game imperfect information(Oliehoek & Vlassis, 2006). games, notion sub-game perfect equilibriainadequate; type games often contain proper sub-games, every Nashequilibrium trivially sub-game perfect.4 overcome problem different refinementsNash equilibrium concept defined, mention assessmentequilibrium (Binmore, 1992) closely related, stronger sequential equilibrium(Osborne & Rubinstein, 1994). equilibria based concept assessment, pair h,bi consisting joint policy belief system b. beliefsystem maps possible situation, information set, agentalso onesreachable given probability distribution possible joint histories. Roughlyspeaking, assessment equilibrium requires sequential rationality belief consistency.5former entails joint policy specifies optimal actions informationset given b. Belief consistency means beliefs assigned b Bayesrational given specified joint policy . instance, context Dec-POMDPs bwould prescribe, particular ~it agent i, belief joint histories P (~ |~it ).beliefs prescribed belief system b Bayes-rational (i.e., computed appropriateconditionals (4.5)), b called belief consistent.64.4.2 Sequential Rationality Optimal Q-value Functiondependence sequential rationality belief system b indicates optimalaction particular point dependent probability distribution histories.Section 4.3.3 encountered similar dependence history specified t+1, .make dependence exact.particular stage t, policy optimal or, game-theoretic terms, rationalmaximizes expected return point on. Section 4.3.1, ableexpress expected return Q (~ ,a) assuming optimal joint policy followed4. extensive form Dec-POMDP indeed contain proper sub-games, agentnever discriminate agents observations.5. Osborne Rubinstein (1994) refer second requirement simply consistency. orderavoid confusion definition 4.5 use term belief consistency.6. sequential equilibrium includes technical part definition belief consistency addresses beliefs held information sets reached according .information refer Osborne Rubinstein (1994).318fiOptimal Approximate Q-Value Functions Dec-POMDPscurrent stage t. However, previous policy assumed, maximalexpected return defined.Proposition 4.1. pair (~ ,at ) < h 1 optimal value Q (~ ,at ) cannotdefined without assuming (possibly randomized) past policy t+1 = 0 , . . . , .last stage = h 1 expected reward definedQ (~ h1 ,ah1 ) R(~ h1 ,ah1 )without assuming past policy.Proof. Let us try deduce Q (~ ,at ) optimal value particular ~ assumingQ -values next time step + 1 known. Q (~ ,at )-valuespossible joint actions evaluated followsQ (~ ,at ) = R(~ ,at ) +XP (ot+1 |~ ,at )Q (~ t+1 , t+1, (~ t+1 )).ot+1t+1, optimal decision rule next stage. t+1, be?assume stage + 1 followed particular (possibly randomized) t+1 ,t+1, = arg maxt+1XP (~ t+1 |t+1 ,b0 )Q (~ t+1 , t+1 (~ t+1 )).~ t+1~ t+1optimal. However, many pure infinite randomized past policies t+1consistent ~ ,at , leading many t+1, might optimal. conclusiondraw Q (~ ,at ) ill-defined without P (~ t+1 |t+1 ,b0 ), probability distribution(belief) joint action-observation histories, induced t+1 , policy followedstages 0, . . . ,t.Let us illustrate reviewing optimal Q-value function defined Section 4.3.1. Consider (~ t+1 ) (4.10). optimal policy mapping observation~ induced individual policies observation histories.histories actions :means two joint action-observation histories joint observationhistory results joint action. ~a,~o,~a (h~a,~oi) = (h~a ,~oi). Effectively~ , say mistake7 , continuesmeans reach ~ 6specify actions mistake ever happened: is, still assuming~ .followed stage t. fact, (~ ) might even optimal ~ 6t1~~turn means Q ( ,a), Q-values predecessors , might optimalexpected reward.demonstrated optimal Q-value function Dec-POMDP welldefined without assuming past joint policy. propose new definition Qexplicitly incorporates t+1 .7. question mistake one agent detected another agent differentmatter altogether beyond scope text.319fiOliehoek, Spaan & Vlassist=021a1o1t=1t=2a2o2o1a122a2a12o2a2o1o1o1o1o2o2o2o2a1a1a1a1a2a2a2a22,Figure 7: Computation sequential rational Q . 2, optimal decision rule stage= 2, given 2 followed first two stages. Q (~ 1 ,2 ) entriescomputed propagating relevant Q -values next stage. instance,highlighted joint history ~ 1 = h(a1 ,o1 ),(a2 ,o2 )i, Q -value 2computed propagating values four successor joint histories, per(4.20).Theorem 4.4 (Sequentially rational Q ). optimal Q-value function properly definedfunction joint action-observation histories past joint policies, Q (~ ,t+1 ).Q specifies optimal value given (~ ,t+1 ), even ~ reachedexecution optimal policy , therefore referred sequentially rational.Proof. ~ ,t+1 , optimal expected return givent=h1R(~ ,t+1 (~ )),X~ t+1t+1 ~ t+1 ~~ t+1 t+2,Q ( , ) = R(~ ,t+1 (~ )) +P (o | , ( ))Q ( ,), 0 < h 1ot+1t+2,=t+1 ,t+1,(4.20)t+1, = arg maxt+1X~ t+1~ t+1P (~ t+1 |t+1 ,b0 )Q (~ t+1 , t+1 , t+1 ).(4.21)well-defined.equations constitute dynamic program. assuming purejoint past policies used, (4.21) transformsX(4.22)P (~ t+1 )Q (~ t+1 , t+1 , t+1 )t+1, = arg maxt+1~ t+1~ t+1320fiOptimal Approximate Q-Value Functions Dec-POMDPs~(,)~ consistent dynamic program evaluatedend (t = h 1) begin (t = 0). Figure 7 illustrates computation Q .arriving stage 0, 1 reduce joint actions possible select0, = arg max Q (~ ,a) = arg max Q (~ 0 ,1 ).1given 1 = 0, determine 1, = 1, using (4.22), etc. essentiallyforward-sweep policy computation using optimal Q-value function Q (~ ,t+1 )defined (4.20).computation Q also closely related point-based dynamic programmingDec-POMDPs discussed section 3.5. Suppose = 2 Figure 7 last stage(i.e., h = 3). 2, 2 computed, easy construct setsnon-dominated action agent: every action ai agent specified i2,non-dominated. computed values (~ 1 ,2 ) = 1, 2associated optimal future policy 2, . means individual history ~i1associated sub-tree policy qi =2 2 (~ 1 ,2 )-pair associatedjoint sub-tree policy (e.g, shaded trees Figure 7). Clearly, Q (~ 1 ,2 ) correspondsexpected value associated joint sub-tree policy. Rather keeping tracksub-trees policies, however, algorithm presented keeps track values.advantage description Q using (4.21) rather (4.10) twofold. Firstdescription treated describes way actually compute valuesused construct , latter gives normative description needsorder compute Q-values.Second, Q (~ ,t+1 ) describes sequential rationality Dec-POMDPs.past policy (and corresponding consistent belief system) optimal future policycomputed. variation might even applied on-line. Suppose agent makesmistake stage t, executing action prescribed , assuming agentsexecute policy 6=i without mistakes, agent knows actually executed previouspolicy t+1 . Therefore compute new individual policyEXt+1,t+1P (~ t+1 )Q (~ t+1 , t+1 , t+1 ,6=).i,t+1 = arg maxt+1~ t+1~ t+14.4.3 Complexity Computing Sequentially Rational QAlthough found way compute Q , computation intractableP|Oi |t 1smallest problems, show. stage t1 t1=0 |Oi | = |Oi |1observation histories agent i, leading|A |(n |O |t 1|O |1)~ | = |O|t1 consistent joint actionpure joint past policies . |Oobservation histories (for observation history ~o t1 , specifies actions forming~ t1 ). means stage h 2 (for h 1, Q-values easily calculated),321fiOliehoek, Spaan & Vlassisnumber entries computed number joint past policies h1 times numberjoint histories!n(|O |h1 1)|A | |O |1|O|h2 ,indicating computation function doubly exponential, brute force policyevaluation. Also, joint past policy h1 , need compute h = (h1 ,h1, )solving next-stage BG:XP (~ h1 )Q (~ h1 , h1 , h1 ).h1, = arg maxh1h1~~ h1authors knowledge, method optimally solve BGs evaluationh1|A |n|O |joint BG-polices, also doubly exponential horizon.5. Approximate Q-value Functionsindicated previous section, although optimal Q-value function Q exists,costly compute thus impractical. section, review Q-valueb used approximation Q . discuss underlyingfunctions, Q,assumptions, computation, computational complexity properties, thereby providing taxonomy approximate Q-value functions Dec-POMDPs. particulartreat two well-known approximate Q-value functions, QMDP QPOMDP , QBGrecently introduced Oliehoek Vlassis (2007).5.1 QMDPQMDP originally proposed approximately solve POMDPs Littman, Cassandra,Kaelbling (1995), also applied Dec-POMDPs (Emery-Montemerloet al., 2004; Szer et al., 2005). idea Q approximated using stateaction values QM (s,a) found solving underlying MDP Dec-POMDP.underlying MDP horizon-h MDP defined single agent takes joint actionsobserves nominal state transition model rewardmodel R original Dec-POMDP. Solving underlying MDP efficiently doneusing dynamic programming techniques (Puterman, 1994), resulting optimal nonstationary MDP Q-value function:X(st+1 ,a).(5.1)P (st+1 |st ,a) max Qt+1,Qt,(s ,a) = R(s ,a) +st+1t+1,equation, maximization implicit selection, optimal MDP policynext time step, explained Section 4.3.3. Note Qt,also optimal Qvalue function, MDP setting. article Q always denote optimalvalue function (original) Dec-POMDP. order transform Qt,(s ,a)-valuesb (~ ,a)-values used original Dec-POMDP, compute:approximate Q322fiOptimal Approximate Q-Value Functions Dec-POMDPsb (~ ,a) =QX~tQt,(s,a)P (s| ),(5.2)sSP (s|~ ) computed (4.8). Combining (5.1) (5.2) makingt+1,selectionexplicit get:b (~ ,a) = R(~ ,a) +QXP (st+1 |~ ,a)st+1maxt+1 t+1(s)t+1 t+1Qt+1,(st+1 ,M(s )),(5.3)defines approximate Q-value function used payoff functionb consistent established definitionvarious BGs Dec-POMDP. Note QQ-value functions since defined expected immediate reward performing(joint) action plus value following optimal joint policy (in case optimalMDP-policy) thereafter.calculation QtM (s,a)-values dynamic programming (which costO(|S|h) performed separate phase, cost computation QMDPdependent cost evaluation (5.3), O(|S|). want evaluateP(|A||O|)h 1QMDP h1t=0 (|A| |O|) = (|A||O|)1 joint action-observation histories is, totalcomputational cost becomes:!(|A| |O|)h 1|A||S| .(5.4)(|A| |O|) 1However, applying QMDP forward-sweep policy computation,consider action-observation histories, consistent policyfound earlier stages. Effectively evaluate (5.3) observation historiesjoint actions, leading to:!(|O|)h 1|A||S| .(5.5)(|O|) 1used context Dec-POMDPs, QMDP solutions known undervalueactions gain information (Fernandez, Sanz, Simmons, & Dieguez, 2006). explained realizing QMDP solution assumes state fully observablenext time step. Therefore actions provide information state,thus lead high future reward (but might low immediate reward),undervalued. applying QMDP Dec-POMDP setting, effect alsoexpected. Another consequence simplifying assumption QMDP -value function upper bound optimal value function used approximate POMDP(Hauskrecht, 2000), consequence also upper bound optimal value functionDec-POMDP. intuitively clear, Dec-POMDP POMDPadditional difficulty decentralization. formal argument presented Section 5.4.5.2 QPOMDPSimilar underlying MDP, one define underlying POMDP Dec-POMDPPOMDP , R, single agent323fiOliehoek, Spaan & Vlassistakes joint actions receives joint observations O. QPOMDP approximates Qusing solution underlying POMDP (Szer et al., 2005; Roth et al., 2005).particular, optimal QPOMDP value function underlying POMDP satisfies:~t~tQP (b ,a) = R(b ,a) +X~tP (ot+1 |b ,a)ot+1maxt+1t+1P(b~)~ t+1QP (b~ t+1,Pt+1 (b)),(5.6)~tb joint belief single agent selects joint actions receives jointobservations time step t,X~t~tR(b ,a) =R(s,a)b (s)(5.7)sS~ t+1~timmediate reward, bjoint belief resulting b actiont+1joint observation , calculatedP~tP (o|a,s ) sS P (s |s,a)b (s)~ t+1b(s ) = P.(5.8)P~sS P (s |s,a)b (s)P (o|a,s )~t~ one joint belief b , corresponds P (s|~ ) derived(4.8). Therefore possible directly use computed QPOMDP values payoffsBGs Dec-POMDP, is, define:b P (~ ,a) QP (b~ ,a).Q(5.9)maximization (5.6) stated explicit form: maximization time step+ 1 POMDP policies. However, clear maximization effectively onejoint actions, conditional received joint observation ot+1 thus~ t+1resulting belief b .finite horizon, QP computed generating possible joint beliefssolving belief MDP. Generating possible beliefs easy: starting b0 corresponding empty joint action-observation history ~ t=0 , calculate~1resulting ~ t=1 corresponding belief b continue recursively. Solving beliefMDP amounts recursively applying (5.6).computation QMDP could restrict attention (~ ,a)-pairsb (~ ,a)-valuesspecified forward-sweep policy computation, Qt+1~bdepend values successor-histories QM ( ,a). QPOMDP , however,dependence, meaning necessary evaluate ~ ,a. particular,cost calculating QPOMDP divided cost calculating expectedimmediate reward ~ ,a, cost evaluating future reward ~ ,a,= 0,...,h 2. former operation given (5.7) cost O(|S|) per ~ ,athus total cost equal (5.4). latter requires selecting maximizing joint actionjoint observation ~ ,a = 0,...,h 2, leading!(|A| |O|)h1 1|A| (|A| |O|) .(5.10)(|A| |O|) 1324fiOptimal Approximate Q-Value Functions Dec-POMDPs~2t=1~1t=1~2t=0~1t=0()a1a1()a2+3.10.9(a1 ,o1 )a24.1+0.3(a1 ,o1 )(a1 ,o1 )(a1 ,o1 )a1a1a1a1a1a1...(a2 ,o2 )a2a20.3+0.60.6+2.0+3.1+4.4+1.12.90.40.90.94.5......(a2 ,o2 )a2a20.6+4.01.3+3.61.9+1.0+2.00.40.51.01.0+3.5...........................Figure 8: Backward calculation QPOMDP -values. Note solutions (the highlightedentries) different Figure 6: QPOMDP assumes actionsconditioned joint action-observation history. highlighted +3.1entry Bayesian game = 0 calculated expected immediatereward (= 0) plus weighted sum maximizing entry (joint action) pernext joint observation history. assuming uniform distribution jointobservations given ha1 ,a2 future reward given by: +3.1 = 0 + 0.25 2.0 +0.25 4.0 + 0.25 4.4 + 0.25 2.0.Therefore total complexity computing QPOMDP becomes!(|A| |O|)h 1(|A| |O|)h1 1|A| (|A| |O|) +|A||S| .(|A| |O|) 1(|A| |O|) 1(5.11)~ done singleEvaluating (5.6) joint action-observation histories ~backward sweep time, mentioned Section 4.3.3. also visualizedBayesian games illustrated Figure 8; expected future reward calculatedmaximizing weighted sum entries next time step BG.Nevertheless, solving POMDP optimally also known intractable problem.result, POMDP research last decade focused approximate solutionsPOMDPs. particular, known value function POMDP piecewise-linearconvex (PWLC) (joint) belief space (Sondik, 1971). property exploitedmany approximate POMDP solution methods (Pineau, Gordon, & Thrun, 2003; Spaan& Vlassis, 2005). Clearly methods also used calculate approximateQPOMDP -value function use Dec-POMDPs.intuitively clear QPOMDP also admissible heuristic Dec-POMDPs,still assumes information available actually case (again formalproof given Section 5.4). Also clear that, fewer assumptionsmade, QPOMDP yield less over-estimation QMDP . I.e., QPOMDP -valueslie QMDP optimal Q -values.contrast QMDP , QPOMDP assume full observability nominal states.result latter share drawback undervaluing actions gaininformation regarding nominal state. applied Dec-POMDP setting, however,QPOMDP share assumption centralized control. assumption might alsocause relative undervaluation: might situations action might gain325fiOliehoek, Spaan & Vlassisinformation regarding joint (i.e., others) observation history. QPOMDPconsidered redundant, decentralized execution might beneficial,allows better coordination.5.3 QBGQMDP approximates Q assuming state becomes fully observable nexttime step, QPOMDP assumes every time step agents know jointaction-observation history ~ . present new approximate Q-value function, calledQBG , relaxes assumptions further: assumes agents know ~ t1 , jointaction-observation history time step 1, joint action at1 takenprevious time step. means agents uncertain regarding otherslast observation, effectively defines BG ~ t1 ,a. Note, BGsdifferent BGs used Section 4.2: BGs types correspondsingle observations, whereas BGs 4.2 types correspond complete actionobservation histories. Hence, BGs QBG much smaller size thus easiersolve. Formally QBG defined as:QB (~ ,a) = R(~ ,a) + maxXP (ot+1 |~ ,a)QB (~ t+1 ,(ot+1 )),(5.12)ot+1t+1= h1 (ot+11 ),...,n (on )i tuple individual policies : Oi Ai BGconstructed ~ ,a.Note difference (5.12) (5.6) position argumentmaximization operator: (5.12) maximizes (conditional) BG-policy,maximization (5.6) effectively unconditional joint actions.BG representation fictitious Dec-POMDP Figure 6 illustrates com~1putation QBG .8 probability distribution P (ha1 ,a2 ) joint action-observationhistories reached given ha1 ,a2 = 0 uniform immediate rewardha1 ,a2 0. Therefore, 2.75 = 0.25 2.0 + 0.25 3.6 + 0.25 4.4 + 0.25 1.0.cost computing QBG ~ ,a split cost computingimmediate reward (see (5.4)) cost computing future reward (solving BGlast received observation),!(|A| |O|)h1 1|A| |A |n|O | ,(|A| |O|) 1leading total cost of:!(|A| |O|)h1 1(|A| |O|)h 1n|O ||A| |A |+|A||S| .(|A| |O|) 1(|A| |O|) 1(5.13)Comparing cost computing QPOMDP , contains additional exponential term,term depend horizon problem.8. BG representing = 1 Dec-POMDP also involves observation histories length 1,illustration BG corresponds BGs considered QBG . stagescase.326fiOptimal Approximate Q-Value Functions Dec-POMDPsmentioned Section 5.2, QPOMDP approximated exploiting PWLCproperty value function. turns QBG -value function correspondsoptimal value function situation agents communicate freely onestep delay (Oliehoek et al., 2007b). Hsu Marcus (1982) showed complex dynamicprogram constructed settings resulting value function alsopreserves PWLC property. surprisingly, QBG -value function also piecewiselinear convex joint belief space and, result, approximation methodsPOMDPs transferred computation QBG (Oliehoek et al., 2007b).5.4 Generalized QBG Boundsthink extension QBG -value function framework case k-steps delayed communication, agent perceives joint action-observation historyk stages delay. is, stage t, agent knows ~ tk joint action-observation history k stages addition current action-observation history ~it . Similark-step delayed observation models decentralized control previously proposedAicardi, Davoli, Minciardi (1987) Ooi Wornell (1996). particular Aicardiet al. consider Dec-MDP setting agent observations local states sijoint observation identifies state = hs1 , . . . ,sn i. Ooi Wornell examinedecentralized control broadcast channel infinite horizon, allowlocal observations arbitrary, still require joint state observedk-steps delay. assumption less strong, require observation ~ tkassume general Dec-POMDP (not Dec-MDP) setting.k-step delayed communication model Dec-POMDP setting allows expressing different Q-value functions defined article optimal value functionsappropriate k-step delay models. importantly, resorting k-step delaymodel prove hierarchy bounds hold various Q-functions definedarticle:Theorem 5.1 (Hierarchy upper bounds). approximate Q-value functions QBGQPOMDP correspond optimal Q-value functions appropriately defined k-step delayedcommunication models. Moreover Q-value functions form hierarchy upper boundsoptimal Q Dec-POMDP:Q QBG QPOMDP QMDP .(5.14)Proof. See appendix.idea POMDP corresponds system (0-steps) delayed communication, QBG -setting corresponds 1-step delayed communication system.appendix shows Q-value function system k steps delay formsupper bound decentralized system k + 1 steps delay. note lastinequality (5.14) well-known result (Hauskrecht, 2000).6. Generalized Value-Based Policy Searchhierarchy approximate Q-value functions implies Q-value functionsused admissible heuristics MAA policy search, treated Section 3.3.327fiOliehoek, Spaan & VlassisAlgorithm 1 GMAA1: v2: P {0 = ()}3: repeat4:Select(P)5:Next Next(t )6:Next contains subset full policies Next Next7:arg maxNext V ()8:V ( ) > v9:v V ( )10:n11:P P | Vb () > v {prune policy pool}12:end13:Next Next \ Next {remove full policies}14:endn15:P (P \ ) Next | Vb () > v {remove processed/add new partial policies}16: P emptysection present general heuristic policy search frameworkcall Generalized MAA (GMAA ), show unifies solution methodsproposed Dec-POMDPs.GMAA generalizes MAA (Szer et al., 2005) making explicit different proceduresimplicit MAA : (1) iterating pool partial joint policies, pruning poolwhenever possible, (2) selecting partial joint policy policy pool, (3) findingnew partial and/or full joint policies given selected policy. first procedurecore GMAA fixed, two procedures performed manyways.second procedure, Select, chooses policy process next thus determines type search (e.g., depth-first, breadth-first, A*-like) (Russell & Norvig, 2003;Bertsekas, 2005). third procedure, refer Next, determinesset next (partial) joint policies constructed, given previous partial joint policy.original MAA seen instance generalized case particularNext-operator, namely shown algorithm 2.6.1 GMAA AlgorithmGMAA refer policy pool P rather open list, neutralword imply ordering. policy pool P initialized completelyunspecified joint policy 0 = () maximum lower bound (found far) v set. denotes best joint policy found far.point GMAA starts. First, selection operator, Select, selects partial jointpolicy P. assume that, accordance MAA , partial policyhighest heuristic value selected. general, however, kind selection algorithmmay used. Next, selected policy processed policy search operator Next,328fiOptimal Approximate Q-Value Functions Dec-POMDPsAlgorithm 2 Next(t ) MAAnff t+1t+1, , :~|=1: t+1 t+1 = 1 ,...,t+1nfi t+1t+10...t1bfi+ Vb (t+1)...h (t+1 )2: t+1 t+1 V ()V( ) + E R(s ,a)t+13: returnreturns set (partial) joint policies Next heuristic values. Nextreturns one full policies Next , provided values Vb () = V () lowerbound optimal joint policy, used prune search space. foundpartial joint policies Next heuristic value Vb () > v added P.process repeated policy pool empty.6.2 Next Operatordescribe different choices Next-operator correspondexisting Dec-POMDP solution methods.6.2.1 MAAGMAA reduces standard MAA using Next-operator described Algorithm 2.Line 1 expands forming t+1 set partial joint policies one extra stage. Line 2valuates child policies,fiV 0...t (t+1 ) = V 0...t1 (t ) + E R(st ,a)fit+1gives true expected reward first + 1 stages. Vb (t+1)...h (t+1 ) heuristicvalue stages (t + 1)...h given t+1 followed first + 1 stages.using admissible heuristic, GMAA never prune partial policyexpanded optimal policy. combining fact MAA Next operator returns possible t+1 , clear P becomes emptyoptimal policy found.6.2.2 Forward-Sweep Policy ComputationForward-sweep policy computation, introduced Section 4.3.1, described algorithms 1 3 jointly. Given partial joint policy , Next operator constructssolves BG time step t. Next algorithm 3 returns best-rankedpolicy, P never contain 1 joint policy whole search process reducessolving BGs time steps 0,...,h 1.approach Emery-Montemerlo et al. (2004) identical forward-sweep policycomputation, except 1) smaller BGs created discarding clustering low probability action-observation histories, 2) BGs approximately solved alternatingmaximization. Therefore approach also incorporated GMAA policysearch framework making appropriate modifications Algorithm 3.329fiOliehoek, Spaan & VlassisAlgorithm 3 Next(t ) Forward-sweep policy computationEbt~ ),Q~ ,P (1: BG A,~ Ai2: = h1 ,...,n s.t. :P~~b3:Vb () ~P()Q(,(~ ))~t4:t+1 ,5:Vb (t+1 ) V 0...t1 (t ) + Vb ()6: endb (t+1 )7: return arg maxt+1 V6.2.3 Unificationgive unified perspective MAA forward-sweep policy computationexamining relation corresponding Next-operators. particularshow that, using approximate Q-value functions described Section 5heuristic, sole difference two FSPC returns joint policyhighest heuristic value.b following formProposition 6.1. heuristic QXb (~ ,a) = R(~ ,a) +P (ot+1 |~ ,a)Vb t+1 (~ t+1 ),Q(6.1)ot+1partial policy t+1 = ,Xfib (~ ,(~ )) = E R(st ,a)fit+1 + Vb (t+1)...h (t+1 )P (~ )Q(6.2)~t~holds.Proof. expectation Rt given t+1 writtenXXXfiP (~ )R(~ ,t+1 (~ )).P (~ )R(s,t+1 (~ ))P (s|~ ) =E R(st ,a)fit+1 =sS~t~~t~Also, rewrite Vb (t+1)...h (t+1 )XXP (ot+1 |~ ,t+1 (~ ))Vb (t+1)...h (~ t+1 ),P (~ )Vb (t+1)...h (t+1 ) =ot+1~t~XfiP (~ )E R(st ,a)fit+1 + Vb (t+1)...h (t+1 ) ="~t~~tt+1R( ,~t( )) +Xot+1Therefore, assuming (6.1) yields (6.2).330P (ot+1~tt+1| ,~tb (t+1)...h( ))V~ t+1(#)(6.3)fiOptimal Approximate Q-Value Functions Dec-POMDPs~o go house 3flames go house 3flames go house 1flames, flames go house 1flames, flames go house 1flames, flames go house 2flames, flames go house 2~o go house 2flames go house 2flames go house 2flames, flames go house 1flames, flames go house 1flames, flames go house 1flames, flames go house 1Figure 9: Optimal policy FireFighting hnh = 3,nf = 3i, horizon 3. leftpolicy first agent, right second agents policy.means heuristic satisfies (6.1), case Q-value functionsdiscussed paper, Next operators algorithms 2 3 evaluate expandedpolicies same. I.e., algorithms 2 3 calculate identical heuristic valuesnext time step joint policies. Also expanded policies t+1 formed way:considering possible respectively extend . Therefore, sole differencecase latter returns joint policy highest heuristic value.Clearly computation time/quality trade-off MAA FSPC: MAAguaranteed find optimal policy (given admissible heuristic), FSPCguaranteed finish one forward sweep. propose generalization, returnsk-best ranked policies. refer k-best joint BG policies GMAA variant,k-GMAA . way, k-GMAA reduces forward-sweep policy computation k = 1MAA k = .7. Experimentsorder compare different approximate Q-value functions discussed work,well show flexibility GMAA algorithm, performed severalexperiments. use QMDP , QPOMDP QBG heuristic estimates Q .provide qualitative insight different Q-value functions considered, wellresults computing optimal policies using MAA , performance forwardsweep policy computation. First describe problem domains,standard test problems, others introduced work.7.1 Problem DomainsSection 2.2 discussed decentralized tiger (Dec-Tiger) problem introducedNair et al. (2003b). Apart standard Dec-Tiger domain, consider modifiedversion, called Skewed Dec-Tiger, start distribution uniform. Instead,initially tiger located left probability 0.8. also include resultsBroadcastChannel problem, introduced Hansen et al. (2004), models two nodescooperate maximize throughput shared communication channel.Furthermore, test problem called Meeting Grid provided Bernstein et al.(2005), two robots navigate two-by-two grid. consider version 2observations per agent (Amato et al., 2006).331fiOliehoek, Spaan & Vlassisintroduce new benchmark problem, models team n fire fightersextinguish fires row nh houses. house characterized integerparameter f , fire level. indicates degree house burning, nfdifferent values, 0 f < nf . minimum value 0, indicating house burning.every time step, agents receive reward f house agent choosemove houses fight fires location. house burning (f > 0)fire fighting agent present, fire level increase one point probability 0.8neighboring houses burning, probability 0.4 none neighborsfire. house burning catch fire probability 0.8 oneneighbors fire. two agents house, extinguishpresent fire completely, setting houses fire level 0. single agent present houselower fire level one point probability 1 neighbors burning,probability 0.6 otherwise. agent observe whether flameslocation. Flames observed probability 0.2 f = 0, probability 0.5f = 1, probability 0.8 otherwise. Initially, agents start outsidehouses, fire level f house drawn uniform distribution.test different variations problems, number agents always2, differ number houses fire levels. particular, considerhnh = 3,nf = 3i hnh = 4,nf = 3i. Figure 9 shows optimal joint policy horizon 3former variation. One agent initially moves middle house fight fires there,helps prevent fire spreading two neighbors. agent moveshouse 3, stays observes fire, moves house 1 observeflames. well optimal, joint policy makes sense intuitively speaking.7.2 Comparing Q-value Functionsproviding comparison performance approximate Q-value functionsdescribed work, first give insights actual values.h = 4 Dec-Tiger problem, generated possible ~ corresponding P (sl |~ ),according (4.8). these, maximal Q(~ ,a)-value plotted Figure 10.Apart three approximate Q-value functions, also plotted optimal valuejoint action-observation history ~ realized using . Notedifferent ~ different optimal values, induce P (sl |~ ), demonstratedfigure: multiple Q -values plotted P (sl |~ ). horizon 3Meeting Grid problem also collected ~ visited optimalpolicy, Figure 11 plotted maximal Q(~ ,a)-values. problemmany states, representation Figure 10 possible. Instead, ordered~~ according optimal value. see bounds tight ,others quite loose. However, used GMAA framework,~ shownactual performance heuristic also depends valuation ~Figure 11, namely visited optimal policy: especiallyoverestimated, GMAA first examine sub-optimal branch search tree.tighter upper bound speed computation large extent, allowsalgorithm prune policy pool more, reducing number Bayesian games need332fiOptimal Approximate Q-Value Functions Dec-POMDPsQheuristics horizon=4 DecTiger t=060Qheuristics horizon=4 DecTiger t=160QQBGPOMDPQmax = maxa Q(t,a)MDPQ*4030201000QPOMDP50QQmax = max Q(t,a)BGQ50QMDP*40Q3020100P(sl | )1001Qheuristics horizon=4 DecTiger t=240P(sl | )1Qheuristics horizon=4 DecTiger t=320QQBGBGQPOMDPQ*Q2020POMDPQMDPMDP*Q0QQmax = max Q( ,a)Qmax = maxa Q(t,a)3010010200406080P(sl | )10001P(sl | )1Figure 10: Q-values horizon 4 Dec-Tiger. ~ , corresponding P (sl |~ ),maximal Q(~ ,a)-value plotted.solved. figures clearly illustrate main property upper boundsdiscussed, namely Q QBG QPOMDP QMDP (see Theorem 5.1).7.3 Computing Optimal Policiesshown above, hierarchy upper bounds Q QBG QPOMDP QMDPtheoretical construct, differences value specified significantparticular problems. order evaluate impact differencesapproximate Q-value functions, performed several experiments. describeevaluation MAA number test problems using QBG , QPOMDP QMDPheuristic. timing results paper CPU times resolution 0.01s,obtained 3.4GHz Intel Xeon processors.333fiOliehoek, Spaan & Vlassis21.8Qmax = maxa Q(t,a)1.61.41.210.8QMDP0.6QPOMDPQBG*0.4Q0.2Figure 11: Comparison maximal Q(~ ,a)-values Meeting Grid. plotvalue reached optimal policy, ordered accordingoptimal value.hV35.190844.8028QMDPQPOMDPQBGQMDPQPOMDPQBGn105,2286,6516,65137,536,938,118559,653,390301,333,698TGMAA0.310.020.02431,7765,9613,208TQ0s0s0.020s0.130.94Table 1: MAA results Dec-Tiger.hVQMDP35.8402411.1908QPOMDPQBGQMDPQPOMDPQBGn151,23619,85413,21233,921,256,149774,880,51586,106,735TGMAA0.460.060.04388,8948,908919Table 2: MAA results Skewed Dec-Tiger.334TQ0s0.010.030s0.130.92fiOptimal Approximate Q-Value Functions Dec-POMDPshVQMDP43.890054.7900QPOMDPQBGQMDPQPOMDPQBGn328,212531531N/A196,883196,883TGMAA3.540s0s> 4.32e55.305.15TQ0s0.010.030s0.200.53Table 3: MAA results BroadcastChannel.hVQMDP20.910031.5504QPOMDPQBGQMDPQPOMDPQBGn1,2751,27519429,688,7753,907,5251,563,775TGMAA0s0s0s81.9310.804.44TQ0s0s0s0s0.151.37Table 4: MAA results Meeting Grid.Table 1 shows results MAA obtained original Dec-Tiger problem horizon3 4. shows heuristic number partial joint policies evaluated n , CPUtime spent GMAA phase TGMAA , CPU time spent calculating heuristicTQ . QBG , QPOMDP QMDP upper bounds Q , MAA guaranteed findoptimal policy using heuristic, however timing results may differ.h = 3 see using QPOMDP QBG fraction number policiesevaluated compared QMDP reflects proportionally time spentGMAA . horizon QPOMDP QBG perform same, time neededcompute QBG heuristic long GMAA -phase, therefore QPOMDP outperformsQBG here. h = 4, impact using tighter heuristics becomes even pronounced.case computation time heuristic negligible, QBG outperforms both,able prune much partial joint policies policy pool. Table 2 showsresults Skewed Dec-Tiger. problem QMDP QBG results roughlyoriginal Dec-Tiger problem; h = 3 timings bit slower,h = 4 faster. QPOMDP , however, see h = 4 results slowerwell QBG outperforms QPOMDP order magnitude.Results Broadcast Channel (Table 3), Meeting Grid (Table 4) Firefighting problem (Table 5) similar. N/A entry Table 3 indicates QMDPable compute solution within 5 days. problems also seeperformance QPOMDP QBG roughly equal. Meeting Grid problem,QBG yields significant speedup QPOMDP .335fiOliehoek, Spaan & VlassishV35.737046.5788QMDPQPOMDPQBGQMDPQPOMDPQBGn446,72426,57726,57725,656,607,368516,587,229516,587,229TGMAA1.580.080.08309,2355,7305,499TQ0.560.210.330.857.2211.72Table 5: MAA results Fire Fighting hnh = 3, nf = 3i.~o aLioHL aLioHR aLioHL , oHL aLioHL , oHR aLioHR , oHL aLioHR , oHR aLioHL , oHL , oHL aORoHL , oHL , oHR aLioHL , oHR , oHL aLioHL , oHR , oHR aLioHR , oHL , oHL aLioHR , oHL , oHR aLioHR , oHR , oHL aLioHR , oHR , oHR aOL~o aLioHL aLioHR aLioHL , oHL aORoHL , oHR aLioHR , oHL aLioHR , oHR aOLoHL , oHL , oHL aLioHL , oHL , oHR aLioHL , oHR , oHL aLioHL , oHR , oHR aLioHR , oHL , oHL aLioHR , oHL , oHR aLioHR , oHR , oHL aLioHR , oHR , oHR aLiFigure 12: Policies found using forward-sweep policy computation (i.e., k = 1) h = 4Dec-Tiger problem. Left: policy resulting QMDP . Right: optimalpolicy calculated QPOMDP QBG . framed entries highlightcrucial differences.7.4 Forward-Sweep Policy ComputationMAA results described indicate use tighter heuristic yieldsubstantial time savings. section, approximate Q-value functions usedforward-sweep policy computation. would expect using Q-value functionclosely resembles Q , quality resulting policy higher. alsotested whether k-GMAA k > 1 improved quality computed policies.particular, tested k = 1,2, . . . ,5.Dec-Tiger problem, k-GMAA k = 1 (and thus also 2 k 5) foundoptimal policy (with V ( ) = 5.19) horizon 3 using approximate Q-value functions. horizon h = 4, also different values k produced resultapproximate Q-value function. case, however, QMDP found policy expectedreturn 3.19. QPOMDP QBG find optimal policy (V ( ) = 4.80). Figure 12illustrates optimal policy (right) one found QMDP (left). shows QMDPoverestimates value opening door stage = 2.336fiOptimal Approximate Q-Value Functions Dec-POMDPs615104VV5QMDPQMDP5Q3QPOMDPPOMDPQQBG2123k4BG05(a) Skewed Dec-Tiger, h = 3.23k46.5751.55056.58V1.55QMDPQMDP6.585Q1.5495QPOMDPPOMDPQQBG1.5495(b) Skewed Dec-Tiger, h = 4.1.551V1123k4BG6.595(c) GridSmall, h = 3.123k45(d) FireFighting hnh = 3,nf = 3i, h = 4.11.051411.114.1QMDPQPOMDPQVVBG11.15Q14.2MDPQ11.214.3POMDPQBG11.25123k414.45(e) FireFighting hnh = 4,nf = 3i, h = 3.123k45(f) FireFighting hnh = 4,nf = 3i, h = 4.Figure 13: k-GMAA results different problems horizons. y-axis indicates valueinitial joint belief, x-axis denotes k.Skewed Dec-Tiger problem, different values k produce different results.particular, h = 3 QBG finds optimal policy (and thus attains optimal value)values k, shown Figure 13(a). QPOMDP find starting k = 2,QMDP k = 5. Figure 13(b) shows somewhat unexpected result h = 4:k = 1 QMDP QBG find optimal policy, QPOMDP doesnt. clearlyillustrates tighter approximate Q-value function guarantee better jointpolicy, also illustrated results GridSmall Figure 13(c).also performed experiment two settings FireFighting problem.hnh = 3,nf = 3i h = 3 Q-value functions found optimal policy (with value5.7370) k, horizon 4 shown Figure 13(d). Figures 13(e) 13(f) showresults hnh = 4,nf = 3i. h = 4, QMDP finish k 3 within 5 days.encouraging experiments k-GMAA using QBG QPOMDP k 2found optimal policy. Using QMDP optimal policy also always found k 5,except horizon 4 Dec-Tiger hnh = 4,nf = 3i FireFighting problem. resultsseem indicate type approximation might likely produce (near-) optimalresults domains well.337fiOliehoek, Spaan & Vlassis8. Conclusionslarge body work single-agent decision-theoretic planning based value functions,theory lacking thus far Dec-POMDPs. Given large impact valuefunctions single-agent planning uncertainty, expect thorough studyvalue functions Dec-POMDPs greatly benefit multiagent planning certainty.work, presented framework Q-value functions Dec-POMDPs, providingsignificant contribution fill gap Dec-POMDP theory. theoretical contributionslead new insights, applied improve extend solution methods.shown optimal joint policy induces optimal Q-value functionQ (~ ,a), possible construct optimal policy using forward-sweeppolicy computation. entails solving Bayesian games time steps = 0 ,..., h 1use Q (~ ,a) payoff function. clear way computeQ (~ ,a), introduced different description optimal Q-value function Q (~ ,t+1 )based sequential rationality. new description Q computed usingdynamic programming used construct .calculating Q computationally expensive, examined approximate Q-valuefunctions calculated efficiently discussed relate Q .covered QMDP , QPOMDP , QBG , recently proposed approximate Q-value function.Also, established decreasing communication delays decentralized systems cannotdecrease expected value thus Q QBG QPOMDP QMDP . Experimentalevaluation indicated upper bounds theoretical interest,significant differences exist tightness various approximate Q-value functions.Additionally showed approximate Q-value functions used heuristics generalized policy search method GMAA , thereby unifying forward-sweep policycomputation recent Dec-POMDP solution techniques Emery-Montemerlo et al.(2004) Szer et al. (2005). Finally, performed empirical evaluation GMAAshowing significant reductions computation time using tighter heuristics calculateoptimal policies. Also QBG generally found better approximate solutions forward-sweeppolicy computation k-best joint BG policies GMAA variant, k-GMAA .quite directions future research. One try extend resultspaper partially observable stochastic games (POSGs) (Hansen et al., 2004),Dec-POMDPs individual reward function agent. Since dynamicsPOSG model identical Dec-POMDP, similar modeling via Bayesiangames possible. interesting question whether also case, optimal (i.e.,rational) joint policy found forward-sweep policy computation.Staying within context Dec-POMDPs, research direction couldgeneralize GMAA , defining Next Select operators, hoperesulting algorithms able scale larger problems. Also important establishbounds performance learning curves GMAA combination differentNext operators heuristics. different direction experimentally evaluate useeven tighter heuristics Q-value functions case observations delayedmultiple time steps. research paired methods efficiently findQ-value functions. Finally, future research examine Bayesian games.particular, work Emery-Montemerlo et al. (2005) could used starting point338fiOptimal Approximate Q-Value Functions Dec-POMDPsresearch approximately modeling Dec-POMDPs using BGs. Finally,need efficient approximate methods solving Bayesian games.Acknowledgmentsthank anonymous reviewers useful comments. research reportedpart Interactive Collaborative Information Systems (ICIS) project, supportedDutch Ministry Economic Affairs, grant nr: BSIK03024. work supportedFundacao para Ciencia e Tecnologia (ISR/IST pluriannual funding)POS Conhecimento Program includes FEDER funds, grant PTDC/EEAACR/73266/2006.Appendix A. ProofsA.1 Least One Optimal Pure Joint PolicyProposition (2.1). Dec-POMDP least one optimal pure joint policy.Proof. proof follows proof Schoute (1978). possible convert Dec-POMDPextensive game thus strategic game, actions pure policiesDec-POMDP (Oliehoek & Vlassis, 2006). strategic game, least onemaximizing entry corresponding pure joint policy denote max . Now, assumejoint stochastic policy = h1 , . . . ,n attains higher payoff. Kuhn(1953) showed stochastic policy, corresponding mixed policy .Therefore corresponds joint mixed policy = h1 , . . . ,n i. Let us write i,isupport . induces probability distribution P set joint policies= 1,1 n,n subset set joint policies. expectedpayoff writtenV () = EP (V ()| ) max V () = V (max ),contradicting joint stochastic policy attains higher payoff.A.2 Hierarchy Q-value Functionssection lists proof theorem 5.1. ordered follows. First, Section A.2.1presents model resulting value functions Dec-POMDPs k-steps delayed communication. Next, Section A.2.2 shows QPOMDP , QBG Q correspondcase k respectively 0, 1 h. Finally, Section A.2.3 shows communication delay k increases, optimal expected return cannot decrease, thereby provingtheorem 5.1.A.2.1 Modeling Dec-POMDPs k-Steps Delayed Communicationpresent augmented MDP used find optimal solutionDec-POMDPs k steps delayed communication. reformulation workAicardi et al. (1987) Ooi Wornell (1996), extended Dec-POMDP setting.339fiOliehoek, Spaan & Vlassisq1tq2ta1o1o1o1a1a1 ...a...1t+k1a2o2o1a1o2a2a1o1o2o1a1a2o2a...2a2o2o2a2a2t+kt+kot+1 = ho1 ,o2q1t+1q2t+1a1o1o2o1a1a1a2a2t+1o2a2t+kFigure 14: Policies specified states augmented MDP k = 2. Top: policiesst . policy extended augmented MDP action = shown dashed.Bottom: resulting policies joint observation ho1 ,o2 i.define augmented MDP = hS,A,T ,Ri, augmented MDP stagesindicated t.state space = (S t=0 , . . . ,S t=h1 ). augmented state composed jointaction-observation history, joint policy tree q .st=t =(h~ ,qh~ ,q =ht,t,0 h k 1.,h k h 1q joint depth-k (specifying actions k stages) joint policy tree q =ffcontainedq1 ,...,qnt , used starting stage t. last k stages, contained joint policyq =ht,t specifies = h k stages.set augmented actions. 0 h k 1, action jointpolicy at=t = t+k = h1t+k . . . nt+k implicitly mapping length-k observation histories~ k At+k . last k stagesjoint actions taken stage + k. I.e., it+k :h k h 1 one empty action influence whatsoever.augmented actions used expand joint policy trees. appendingpolicy t+k q form depth k + 1 policy, denote q =k+1,t = hq t+k i.execution initial joint action q =k+1,t (~o ) receiving particular joint observation o,340fiOptimal Approximate Q-Value Functions Dec-POMDPssta1~ ,o1a2o2o1a1o2a2a1a2t+1ot+1 = ho1 ,o2st+1a2a1~ t+1 ,o1a1o2o1a1a2t+1o2a2t+kFigure 15: illustration augmented MDP k = 2, showing transitionst st+1 action = . example ~ tk+1 = (~ tk+1 , ha1 ,a2 , ho1 ,o2 i).actions specified stage given (ho1 ,o2 i) depicted Figure 14.q =k+1,t reduces depth k sub-tree policy particular joint observation, denotedq tk+1 = q =k+1,t (o) = hq tk i(o). illustrated Figure 14.transition model. probability P (st+1 |st ,at ) stage = translates follows0 h k 1(P (ot+1 |~ ,q (~o )) conditions hold,t+1t+1t+kP (h~ ,q i|h~ ,q i, ) =(A.1)0otherwise,conditions are: 1) q t+1 = hq t+k i(ot+1 ), 2) ~ t+1 = (~ ,at ,ot+1 ).h k h 1, t+k (A.1) reduces . probabilities unaffected,first condition changes q =ht1,t+1 = q =ht,t (ot+1 ).Finally, R reward model, specified follows:0th1R(st=t ) = R(h~ ,q i) = R(~ ,q (~o )),(A.2)q (~o ) initial joint action specified q . R(~ ,a) defined (2.12).resulting optimality equations Qt (s,a) augmented MDP follows.write Qk optimal Q-value function k-steps delayed communication system.also refer k-QBG value function.XP (ot+1 |~ ,q (~o ))Qk (~ t+1 ,q t+1 ), (A.3)0thk1 Qk (~ ,q , t+k ) = R(~ ,q (~o ))+ot+1341fiOliehoek, Spaan & Vlassisq t+1 = hq t+k i(ot+1 )Qk (~ ,q ) max Qk (~ ,q , t+k ).(A.4)t+klast k stages, h k h 1, = h stages go getQk (~ ,q = ,t ) = R(~ ,q = ,t (~o )) +XP (ot+1 |~ ,q = ,t (~o ))Qk (~ t+1 ,q =1,t+1). (A.5)ot+1Note (A.5) include augmented actions at=t = t+k . Therefore, lastk stages interpreted Markov chain. Standard dynamic programmingapplied calculate Q (~ ,q )-values.A.2.2 Relation k-QBG Approximate Q-value Functionsbriefly show k-QBG fact reduces cases treated earlier.k = 0, k-QBG (A.3) reduces QPOMDP . k = 0 case, q tk becomes depth0, i.e. empty, policy. Also, becomes mapping length-0 observation historiesactions, i.e., becomes joint action. Substitution (A.3) yieldsEEXQ0 ( ~ , ,at ) = R(~ ,at ) +P (ot+1 |~ ,at ) max Q0 ( ~ t+1 , ,at+1 ).at+1ot+1b P (~ ,a) Q (b~ ,a), clearly corresponds QPOMDP -value functionNow, QP(5.6).1-QBG reduces regular QBG . Notice k = 1, q =k,t reduces . Fillingyields:EEXQ1 ( ~ ,at , t+1 ) = R(~ ,at ) +P (ot+1 |~ ,at ) max Q1 ( ~ t+1 , t+1 (ot+1 ) , t+2 ).t+2ot+1using (A.4) obtain QBG -value function (5.12).Dec-POMDP identical h-steps delayed communication system. Augmentedstates form st=0 = h~ ,q 0 i, q 0 = specifies full length h joint policy.first stage = 0 augmented MDP, also one last k (= h) stages. Therefore,applied Q-function (A.5), means Markov chain evaluation starts immediately. Effectively boils evaluation joint policies (correspondingaugmented start states). maximizing one specifies value function optimaljoint policy Q .A.2.3 Shorter Communication Delays cannot Decrease ValueFirst, introduce notation. Let us write Po observation probabilities given~ ,q sequence intermediate observations (ot+1 , . . . ,ot+l1 )hPo (ot+l ) P ot+l |(~ ,q (~o ),ot+1 ,q (ot+1 ), . . . ,ot+l1 ),q (ot+l1 ) ,342l k.(A.6)fiOptimal Approximate Q-Value Functions Dec-POMDPspolicy implicitly maps k-lengthorder avoid confusion, write |k|observation histories actions, |k+1|one mapping length (k + 1)observation-histories actions.~t~tgive reformulation Qk . Qt,k ( ,q ) specifies expected return ,qstages t,t + 1, . . . ,h 1. Here, splitQk (~ ,q ) = Kk (~ ,q ) + Fkt, (~ ,q )(A.7)Kk (~ ,q ), expected k-step reward, i.e., expected return stages t, . . . ,t+k1Fkt, (~ ,q ), expected return stages + k,t + k + 1, . . . ,h 1, referredk-steps expected return.former definedKk (~ ,q ) E"t+k1X=t#fifiR(~ ,a ) fi ~ ,q .(A.8)Let us define K =i (~ ,q =i,t ) expected reward next stages, i.e.,Kk (~ ,q ) = K =k (~ ,q ).(A.9)K =1 (~ ,at ) = R(~ ,at )K =i (~ ,q =i,t ) = R(~ ,q =i,t (~o ))+XP (ot+1 |~ ,q =i,t (~o ))K =i1 (~ t+1 ,q =i1,t+1 (ot+1 )), (A.10)ot+1q =i1,t+1 (ot+1 ) depth-(i 1) joint policy results q =i,t observation ot+1 .t+kdefine Fk =i,t (~ ,q ,|k|) expected reward stages + i,t + + 1, . . . ,h 1.is, time-to-go = denotes much time-to-go start accumulatingexpected reward. k-steps expected return givent+kt+kFkt (~ ,q ,|k|) = Fk =k,t (~ ,q ,|k|).evaluation performedt+kt+k)) = Qk (~ ,q ,|k|Fk =0,t (~ ,q ,|k|Xt+kPo (ot+1 )Fk =i1,t+1, (~ t+1 ,q t+1 ),Fk =i,t (~ ,q ,|k|) =(A.11)(A.12)ot+1t+ki(ot+1 ),q t+1 = hq t+1 |k|t+k).Fk =i,t, (~ ,q ) = max Fk =i,t (~ ,q ,|k|t+k|k|343(A.13)fiOliehoek, Spaan & VlassisTheorem A.1 (Shorter communication delays cannot decrease value). optimalQ-value function Qk finite horizon Dec-POMDP k-steps delayed communicationupper bound Qk+1 , k + 1-steps delayed communication system.~ q =k,t , t+k|k|t+kt+k+1t+ki,|k+1|).Qk (~ ,q =k,t ,|k|) max Qk+1 (~ ,hq =k,t |k|t+k+1|k+1|(A.14)Proof. proof induction. base case (A.14) holds stages h (k +1) h 1, shown lemma A.1. induction hypothesis states that, assuming(A.14) holds stage + k, also holds stage t. induction step provenlemma A.2.Lemma A.1 (Base case). h k 1 h 1, expected cumulative futurereward k steps delay equal k + 1 steps delay policiesfollowed point. is,hkth1 ~ q =ht,tQk (~ ,q =ht,t ) = Qk+1 (~ ,q =ht,t ),(A.15)~ hk1 q =k,hk1 , h1|k|h1h1Qk (~ hk1 ,q =k,hk1 ,|k|) = Qk+1 (~ hk1 ,hq =k,hk1 |k|i).(A.16)Proof. particular stage = h h k h 1 arbitrary ~ ,q = ,t ,writeQk (~ ,q = ,t ) = Qk+1 (~ ,q = ,t ),given evaluation (A.5), evaluation involves actions:Basically (A.5) reduced Markov chain, Markov chain QkQk+1 . concludehkth1 ~ ,q = ,tQk (~ ,q = ,t ) = Qk+1 (~ ,q = ,t ).prove (A.16). left side (A.16) given application (A.3)h1Qk (~ hk1 ,q =k,hk1 ,|k|) = R(~ hk1 ,q =k,hk1 (~o ))+XP (ohk |~ hk1 ,q =k,hk1 (~o ))Qk (~ hk ,q =k,hk ),ohkh1q =k,hk = hq =k,hk1 |k|i(ohk ). right side given application (A.5)h1Qk+1 (~ hk1 ,hq =k,hk1 |k|i) = R(~ hk1 ,q =k,hk1 (~o ))+XP (ohk |~ hk1 ,q =k,hk1 (~o ))Qk+1 (~ hk ,q =k,hk )ohkh1q =k,hk = hq =k,hk1 |k|i(ohk ). Now, policies q =k,hksame, getQk (~ hk ,q =k,hk ) = Qk+1 (~ hk ,q =k,hk )thus (A.16) holds.344fiOptimal Approximate Q-Value Functions Dec-POMDPsLemma A.2 (Induction step). Given~ q =k,t , +k|k|+k+k+k+1Qk (~ ,q =k,t ,|k|) maxQk+1 (~ ,hq =k,t |k|i,|k+1|) (A.17)+k+1|k+1|holds = + (k + 1),~ q =k,t , t+k|k|t+kt+k+1t+ki,|k+1|)Qk (~ ,q =k,t ,|k|) max Qk+1 (~ ,hq =k,t |k|(A.18)t+k+1|k+1|holds stage t.Proof. k-steps delay Q-function, writet+kQk (~ ,q =k,t ,|k|) = R(~ ,q =k,t (~o ))+hXt+k+1) (A.19)Po (ot+1 |~ ,q =k,t (~o )) max Kk (~ t+1 ,q t+1 ) + Fkt+1 (~ t+1 ,q =k,t+1 ,|k|t+k+1|k|ot+1t+kt+k+1q =k,t+1 = hq =k,t |k|i(ot+1 ). Kk independent |k|, regroupterms get~tQk ( ,q=k,tt+k,|k|)"~t= R( ,q=k,t"(~o )) +XPo (ot+1)Kk (ot+1Xot+1~ t+1,q=k,t+1#) +#t+k+1) . (A.20)Po (ot+1 ) max Fkt+1 (~ t+1 ,q =k,t+1 ,|k|t+k+1|k|case k + 1-steps delay, writet+kt+kt+kt+kt+k+1Qk+1 (~ ,hq =k,t |k|i,|k+1|) = Kk+1 (~ ,hq =k,t |k|i) + Fk+1(~ ,hq =k,t |k+1|i,|k+1|)(A.21)where, per definition (by (A.9) (A.10))t+kKk+1 (~ ,hq =k,t |k|i) = R(~ ,q =k,t (~o )) +XPo (ot+1 )K =k (~ t+1 ,q =k,t+1 ),ot+1= R(~ ,q =k,t (~o )) +XPo (ot+1 )Kk (~ t+1 ,q =k,t+1 ),(A.22)ot+1q =k,t+1 = hq =k,t t+k i(ot+1 ).Equation (A.22) equal first part (A.20). Therefore, arbitrary ~ ,q =k,tt+k, know (A.18) holds|k|Xot+1t+kt+k+1t+k+1(~ ,hq =k,t |k|i,|k+1|)) max Fk+1Po (ot+1 ) max Fkt+1 (~ t+1 ,q =k,t+1 ,|k|t+k+1|k+1|t+k+1|k|(A.23)345fiOliehoek, Spaan & Vlassist+kq =k,t+1 = hq =k,t |k|i(ot+1 ). filling expanding Fk+1using(A.12) getXt+kt+k+1i(ot+1 ),|k|)Po (ot+1 ) max Fk =k,t+1 (~ t+1 ,hq =k,t |k|t+k+1|k|ot+1maxt+k+1|k+1|X=k,t+1, ~ t+1t+kt+k+1Po (ot+1 )Fk+1( ,hhq =k,t |k||k+1|i(ot+1 )). (A.24)ot+1clearly holdsXt+kt+k+1i(ot+1 ),|k|)Po (ot+1 ) max Fk =k,t+1 (~ t+1 ,hq =k,t |k|t+k+1|k|ot+1X=k,t+1, ~ t+1t+kt+k+1( ,hhq =k,t |k|i(ot+1 ) |k|i), (A.25)Po (ot+1 ) max Fk+1t+k+1|k|ot+1second part (A.25) upper bound second part (A.24). Therefore,induction step proved showot+1 t+k+1|k|t+kt+k+1i(ot+1 ),|k|)Fk =k,t+1 (~ t+1 ,hq =k,t |k|=k,t+1, ~ t+1t+kt+k+1Fk+1( ,hhq =k,t |k|i(ot+1 ) |k|i). (A.26)t+k(A.13) q =k,t+1 = hq =k,t |k|i(ot+1 ) transformsq =k,t+1 t+k+1|k|t+k+1Fk =k,t+1 (~ t+1 ,q =k,t+1 ,|k|)=k,t+1 ~ t+1t+k+1t+k+2( ,hq =k,t+1 |k|i,|k+1|). (A.27)max Fk+1t+k+2|k+1|Now, apply (A.11) induction hypothesis (A.17) yield~ q =k,t , +k|k|=0,t ~+k+k+k+1Fk =0,t (~ ,q =k,t ,|k|) maxFk+1( ,hq =k,t |k|i,|k+1|).+k+1|k+1|(A.28)Application lemma A.4 transformed induction hypothesis asserts (A.27)thereby proves lemma.Auxiliary Lemmas.Lemma A.3. If, stage t, i-steps expected return k-steps delayed systemhigher (k + 1)-steps delayed system, 1 (i + 1)-steps expected returnk-steps delayed system higher (k + 1)-steps delayed system. is,t1+kparticular q =k,t = hq =k,t1 |k|i(ot )t+k ot|k|t+kt1+kt+kFk =i,t (~ ,q =k,t ,|k|) = Fk =i,t (~ ,hq =k,t1 |k|i(ot ),|k|)=i,t ~t+kt+kt+k+1( ,hhq =k,t1 |k|i(ot ) |k|i,|k+1|) (A.29)max Fk+1t+k+1|k+1|346fiOptimal Approximate Q-Value Functions Dec-POMDPsholds,=i+1,t1 ~ t1t1+kt+kt1+k( ,hq =k,t1 |k|i,|k+1|).Fk =i+1,t1 (~ t1 ,q =k,t1 ,|k|) max Fk+1t+k|k+1|(A.30)Proof. following derivationt1+kFk =i+1,t1 (~ t1 ,q =k,t1 ,|k|)hXt1+kt+kPo (ot ) max Fk =i,t (~ ,hq =k,t1 |k|i(ot ),|k|)=t+k|k|otXPo (o ) maxt+k|k|otmaxt+k|k+1|= maxt+k|k+1|X"Po (ot )=i,t ~( ,hhq =k,t1max Fk+1t+k+1|k+1|"=i,t ~( ,hhq =k,t1max Fk+1t+k+1|k+1|ot=i+1,t1 ~ t1Fk+1( ,hq =k,t1t1+k|k|i(ot )#t+kt+k+1|k|i,|k+1|)#t1+kt+kt+k+1|k||k+1|i(ot ),|k+1|)t1+kt+k|k|i,|k+1|)proves lemma.Lemma A.4. If, stage~ ,q =k,t , t+k|k|=0,t ~t+kt+k+1t+k( ,hq =k,t |k|i,|k+1|)Fk =0,t (~ ,q =k,t ,|k|) max Fk+1t+k+1|k+1|(A.31)holds, ~ ti ,q =k,ti , ti+k|k|=i,ti ~ titi+kti+k+1ti+k( ,hq =k,ti |k|i,|k+1|).Fk =i,ti (~ ti ,q =k,ti ,|k|) max Fk+1ti+k+1|k+1|(A.32)t+kProof. (A.31) holds ~ , q =k,t , |k|, eq. (A.29) satisfied ~ , q =k,t ,t+k, lemma (A.3) yields ~ t1 ,q =k,t1 , t1+k|k|=1,t1 ~ t1t1+kt+kFk =1,t1 (~ t1 ,q =k,t1 ,|k|) max Fk+1( ,hq =k,t1 t+k1 i,|k+1|). (A.33)t+k|k+1|point apply lemma again, etc. i-th application lemma yields(A.32).347fiOliehoek, Spaan & VlassisReferencesAicardi, M., Davoli, F., & Minciardi, R. (1987). Decentralized optimal control Markovchains common past information set. IEEE Transactions Automatic Control,32 (11), 10281031.Altman, E. (2002). Applications Markov decision processes communication networks.Feinberg, E. A., & Shwartz, A. (Eds.), Handbook Markov Decision Processes:Methods Applications. Kluwer Academic Publishers.Amato, C., Bernstein, D. S., & Zilberstein, S. (2006). Optimal fixed-size controllersdecentralized POMDPs. Proc. AAMAS Workshop Multi-Agent SequentialDecision Making Uncertain Domains (MSDM).Amato, C., Bernstein, D. S., & Zilberstein, S. (2007a). Optimizing memory-bounded controllers decentralized POMDPs. Proc. Uncertainty Artificial Intelligence.Amato, C., Carlin, A., & Zilberstein, S. (2007b). Bounded dynamic programming decentralized POMDPs. Proc. AAMAS Workshop Multi-Agent SequentialDecision Making Uncertain Domains (MSDM).Arai, T., Pagello, E., & Parker, L. (2002). Editorial: Advances multirobot systems. IEEETransactions Robotics Automation, 18 (5), 655661.Aras, R., Dutech, A., & Charpillet, F. (2007). Mixed integer linear programming exact finite-horizon planning decentralized POMDPs. Proc. InternationalConference Automated Planning Scheduling.Becker, R., Lesser, V., & Zilberstein, S. (2005). Analyzing myopic approaches multiagent communication. Proc. International Conference Intelligent AgentTechnology, pp. 550557.Becker, R., Zilberstein, S., & Lesser, V. (2004a). Decentralized Markov decision processesevent-driven interactions. Proc. International Joint ConferenceAutonomous Agents Multi Agent Systems, pp. 302309.Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004b). Solving transition independent decentralized Markov decision processes. Journal Artificial IntelligenceResearch, 22, 423455.Bernstein, D. S. (2005). Complexity Analysis Optimal Algorithms DecentralizedDecision Making. Ph.D. thesis, University Massachusets Amherst.Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexitydecentralized control Markov decision processes. Mathematics OperationsResearch, 27 (4), 819840.Bernstein, D. S., Hansen, E. A., & Zilberstein, S. (2005). Bounded policy iterationdecentralized POMDPs. Proc. International Joint Conference ArtificialIntelligence, pp. 12871292.Bertsekas, D. P. (2005). Dynamic Programming Optimal Control (3rd edition)., Vol. I.Athena Scientific.348fiOptimal Approximate Q-Value Functions Dec-POMDPsBeynier, A., & Mouaddib, A.-I. (2005). polynomial algorithm decentralized Markovdecision processes temporal constraints. Proc. International JointConference Autonomous Agents Multi Agent Systems, pp. 963969.Beynier, A., & Mouaddib, A.-I. (2006). iterative algorithm solving constraineddecentralized Markov decision processes. Proc. National ConferenceArtificial Intelligence.Binmore, K. (1992). Fun Games. D.C. Heath Company.de Boer, P.-T., Kroese, D. P., Mannor, S., & Rubinstein, R. Y. (2005). tutorialcross-entropy method. Annals Operations Research, 134 (1), 1967.Boutilier, C. (1996). Planning, learning coordination multiagent decision processes.Proc. 6th Conference Theoretical Aspects Rationality Knowledge,pp. 195210.Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,11, 194.Chades, I., Scherrer, B., & Charpillet, F. (2002). heuristic approach solvingdecentralized-POMDP: assessment pursuit problem. Proc. 2002 ACMSymposium Applied Computing, pp. 5762.Cogill, R., Rotkowitz, M., Roy, B. V., & Lall, S. (2004). approximate dynamic programming approach decentralized control stochastic systems. Proc. 2004Allerton Conference Communication, Control, Computing.Emery-Montemerlo, R. (2005). Game-Theoretic Control Robot Teams. Ph.D. thesis,Carnegie Mellon University.Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2004). Approximate solutions partially observable stochastic games common payoffs. Proc.International Joint Conference Autonomous Agents Multi Agent Systems, pp.136143.Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2005). Game theoreticcontrol robot teams. Proc. IEEE International Conference RoboticsAutomation, pp. 11751181.Fernandez, J. L., Sanz, R., Simmons, R. G., & Dieguez, A. R. (2006). Heuristic anytimeapproaches stochastic decision processes. Journal Heuristics, 12 (3), 181209.Gmytrasiewicz, P. J., & Doshi, P. (2005). framework sequential planning multiagent settings. Journal Artificial Intelligence Research, 24, 4979.Goldman, C. V., Allen, M., & Zilberstein, S. (2007). Learning communicate decentralized environment. Autonomous Agents Multi-Agent Systems, 15 (1), 4790.Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange cooperativemulti-agent systems. Proc. International Joint Conference AutonomousAgents Multi Agent Systems, pp. 137144.349fiOliehoek, Spaan & VlassisGoldman, C. V., & Zilberstein, S. (2004). Decentralized control cooperative systems:Categorization complexity analysis.. Journal Artificial Intelligence Research,22, 143174.Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithmsfactored MDPs. Journal Artificial Intelligence Research, 19, 399468.Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming partially observable stochastic games. Proc. National Conference ArtificialIntelligence, pp. 709715.Hauskrecht, M. (2000). Value-function approximations partially observable Markovdecision processes.. Journal Artificial Intelligence Research, 13, 3394.Hsu, K., & Marcus, S. (1982). Decentralized control finite state Markov processes. IEEETransactions Automatic Control, 27 (2), 426431.Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning actingpartially observable stochastic domains. Artificial Intelligence, 101 (1-2), 99134.Kim, Y., Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2006). Exploiting locality interaction networked distributed POMDPs. Proc. AAAI SpringSymposium Distributed Plan Schedule Management.Kitano, H., Asada, M., Kuniyoshi, Y., Noda, I., & Osawa, E. (1997). RoboCup: robotworld cup initiative. Proc. International Conference Autonomous Agents.Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjoh, A., & Shimada,S. (1999). Robocup rescue: Search rescue large-scale disasters domainautonomous agents research. Proc. International Conference Systems,Man Cybernetics, pp. 739743.Koller, D., Megiddo, N., & von Stengel, B. (1994). Fast algorithms finding randomized strategies game trees. Proc. 26th ACM Symposium TheoryComputing, pp. 750759.Koller, D., & Pfeffer, A. (1997). Representations solutions game-theoretic problems.Artificial Intelligence, 94 (1-2), 167215.Kuhn, H. (1953). Extensive games problem information. Annals MathematicsStudies, 28, 193216.Lesser, V., Ortiz Jr., C. L., & Tambe, M. (Eds.). (2003). Distributed Sensor Networks:Multiagent Perspective, Vol. 9. Kluwer Academic Publishers.Littman, M., Cassandra, A., & Kaelbling, L. (1995). Learning policies partially observable environments: Scaling up. Proc. International Conference MachineLearning, pp. 362370.Marecki, J., & Tambe, M. (2007). opportunistic techniques solving decentralizedMarkov decision processes temporal constraints. Proc. InternationalJoint Conference Autonomous Agents Multi Agent Systems, pp. 18.Nair, R., Tambe, M., & Marsella, S. (2003). Team formation reformation multiagentdomains like RoboCupRescue. Proc. RoboCup-2002 International Symposium.350fiOptimal Approximate Q-Value Functions Dec-POMDPsNair, R., Roth, M., & Yohoo, M. (2004). Communication improving policy computation distributed POMDPs. Proc. International Joint ConferenceAutonomous Agents Multi Agent Systems, pp. 10981105.Nair, R., Tambe, M., & Marsella, S. (2002). Team formation reformation. Proc.AAAI Spring Symposium Intelligent Distributed Embedded Systems.Nair, R., Tambe, M., & Marsella, S. (2003a). Role allocation reallocation multiagentteams: towards practical analysis. Proc. International Joint ConferenceAutonomous Agents Multi Agent Systems, pp. 552559.Nair, R., Tambe, M., Yokoo, M., Pynadath, D. V., & Marsella, S. (2003b). Taming decentralized POMDPs: Towards efficient policy computation multiagent settings.Proc. International Joint Conference Artificial Intelligence, pp. 705711.Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributedPOMDPs: synthesis distributed constraint optimization POMDPs. Proc.National Conference Artificial Intelligence, pp. 133139.Nash, J. F. (1950). Equilibrium points N-person games. Proc. National AcademySciences United States America, 36, 4849.Oliehoek, F., & Vlassis, N. (2006). Dec-POMDPs extensive form games: equivalencemodels algorithms. Ias technical report IAS-UVA-06-02, University Amsterdam, Intelligent Systems Lab, Amsterdam, Netherlands.Oliehoek, F. A., Kooij, J. F., & Vlassis, N. (2007a). cross-entropy approach solvingDec-POMDPs. Proc. International Symposium Intelligent DistributedComputing, pp. 145154.Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. (2007b). Dec-POMDPs delayed communication. Proc. AAMAS Workshop Multi-Agent Sequential DecisionMaking Uncertain Domains (MSDM).Oliehoek, F. A., Spaan, M. T. J., Whiteson, S., & Vlassis, N. (2008). Exploiting localityinteraction factored Dec-POMDPs. Proc. International Joint ConferenceAutonomous Agents Multi Agent Systems.Oliehoek, F. A., & Visser, A. (2006). hierarchical model decentralized fighting largescale urban fires. Proc. AAMAS06 Workshop Hierarchical AutonomousAgents Multi-Agent Systems (H-AAMAS), pp. 1421.Oliehoek, F. A., & Vlassis, N. (2007). Q-value functions decentralized POMDPs.Proc. International Joint Conference Autonomous Agents Multi AgentSystems, pp. 833840.Ooi, J. M., & Wornell, G. W. (1996). Decentralized control multiple access broadcast channel: Performance bounds. Proc. 35th Conference DecisionControl, pp. 293298.Osborne, M. J., & Rubinstein, A. (1994). Course Game Theory. MIT Press.Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). complexity Markov decision processes. Mathematics Operations Research, 12 (3), 441451.351fiOliehoek, Spaan & VlassisPaquet, S., Tobin, L., & Chaib-draa, B. (2005). online POMDP algorithm complex multiagent environments. Proc. International Joint ConferenceAutonomous Agents Multi Agent Systems.Peshkin, L. (2001). Reinforcement Learning Policy Search. Ph.D. thesis, Brown University.Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning cooperate viapolicy search. Proc. Uncertainty Artificial Intelligence, pp. 307314.Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: anytimealgorithm POMDPs. Proc. International Joint Conference ArtificialIntelligence, pp. 10251032.Puterman, M. L. (1994). Markov Decision ProcessesDiscrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.Pynadath, D. V., & Tambe, M. (2002). communicative multiagent team decisionproblem: Analyzing teamwork theories models. Journal Artificial IntelligenceResearch, 16, 389423.Romanovskii, I. (1962). Reduction game complete memory matrix game.Soviet Mathematics, 3, 678681.Roth, M., Simmons, R., & Veloso, M. (2005). Reasoning joint beliefs executiontime communication decisions. Proc. International Joint ConferenceAutonomous Agents Multi Agent Systems, pp. 786793.Roth, M., Simmons, R., & Veloso, M. (2007). Exploiting factored representations decentralized execution multi-agent teams. Proc. International Joint ConferenceAutonomous Agents Multi Agent Systems, pp. 467463.Russell, S., & Norvig, P. (2003). Artificial Intelligence: Modern Approach (2nd edition).Pearson Education.Schoute, F. C. (1978). Symmetric team problems multi access wire communication.Automatica, 14, 255269.Seuken, S., & Zilberstein, S. (2007a). Improved memory-bounded dynamic programmingdecentralized POMDPs. Proc. Uncertainty Artificial Intelligence.Seuken, S., & Zilberstein, S. (2007b). Memory-bounded dynamic programming DECPOMDPs.. Proc. International Joint Conference Artificial Intelligence,pp. 20092015.Sondik, E. J. (1971). optimal control partially observable Markov decision processes.Ph.D. thesis, Stanford University.Spaan, M. T. J., Gordon, G. J., & Vlassis, N. (2006). Decentralized planning uncertainty teams communicating agents. Proc. International JointConference Autonomous Agents Multi Agent Systems, pp. 249256.Spaan, M. T. J., & Melo, F. S. (2008). Interaction-driven Markov games decentralizedmultiagent planning uncertainty. Proc. International Joint ConferenceAutonomous Agents Multi Agent Systems.352fiOptimal Approximate Q-Value Functions Dec-POMDPsSpaan, M. T. J., & Vlassis, N. (2005). Perseus: Randomized point-based value iterationPOMDPs. Journal Artificial Intelligence Research, 24, 195220.Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MITPress.Szer, D., & Charpillet, F. (2005). optimal best-first search algorithm solving infinitehorizon DEC-POMDPs. Proc. European Conference Machine Learning,pp. 389399.Szer, D., & Charpillet, F. (2006). Point-based dynamic programming DEC-POMDPs..Proc. National Conference Artificial Intelligence.Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithmsolving decentralized POMDPs. Proc. Uncertainty Artificial Intelligence, pp.576583.Tao, N., Baxter, J., & Weaver, L. (2001). multi-agent policy-gradient approach networkrouting. Proc. International Conference Machine Learning, pp. 553560.Varakantham, P., Marecki, J., Yabu, Y., Tambe, M., & Yokoo, M. (2007). Letting looseSPIDER network POMDPs: Generating quality guaranteed policies.Proc. International Joint Conference Autonomous Agents Multi AgentSystems.Varakantham, P., Nair, R., Tambe, M., & Yokoo, M. (2006). Winning back cupdistributed POMDPs: planning continuous belief spaces. Proc. International Joint Conference Autonomous Agents Multi Agent Systems, pp.289296.Wu, J., & Durfee, E. H. (2006). Mixed-integer linear programming transitionindependent decentralized MDPs. Proc. International Joint ConferenceAutonomous Agents Multi Agent Systems, pp. 10581060.Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions multi-agentcooperation: Model experiments. Proc. International ConferenceAutonomous Agents.353fiJournal Artificial Intelligence Research 32 (2008) 525-564Submitted 10/07; published 06/08Efficiency Envy-freeness Fair Division IndivisibleGoods: Logical Representation ComplexitySylvain Bouveretsylvain.bouveret@onera.frONERA Centre de Toulouse.2, avenue Edouard Belin, BP74025.31055 Toulouse cedex 4, FRANCE.Jerome Langlang@irit.frIRIT-CNRS. 118, route de Narbonne.31062 Toulouse cedex, FRANCE.Abstractconsider problem allocating fairly set indivisible goods among agentspoint view compact representation computational complexity. startassuming agents dichotomous preferences expressed propositional formulae. express efficiency envy-freeness logical setting, reveals unexpectedconnections nonmonotonic reasoning. identify complexity determiningwhether exists efficient envy-free allocation, several notions efficiency,preferences represented succinct way (as well restrictions problem).first study problem assumption preferences dichotomous,general case.1. IntroductionAllocating goods agents important issue considered different perspectives economics (especially social choice theory) computer science (especiallyArtificial Intelligence Operations Research), arises various real-world settings:auctions, divorce settlements, electronic spectrum frequency allocation, airport traffic management, fair efficient exploitation Earth Observation Satellites (seesurvey Chevaleyre, Dunne, Endriss, Lang, Lematre, Maudet, Padget, Phelps,Rodrguez-Aguilar, & Sousa, 2006, detailed description). general issue also covershuge variety allocation problems, depending following parameters (seework Chevaleyre et al., 2006, detailed taxonomy):nature resources allocated (are divisible not? single-unitmulti-unit?);nature preferences agents (are numerical simply ordinal?preferential dependencies goods?)nature permitted allocations (can goods shared among several agents?goods allocated? allocations accompanied side payments?);evaluation quality allocation (Pareto-efficiency, utilitarian egalitarian social welfare etc.);c2008AI Access Foundation. rights reserved.fiBouveret & Langnature process leads allocation (centralized decentralized).instance, standard combinatorial auctions (Cramton, Shoham, & Steinberg, 2005)typically correspond indivisible goods (possible multi-unit), numerical preferencespossible dependencies goods, monetary payments, maximization total valuesold items, centralized computation.paper focus fair division indivisible goods without money transfers1 . Fairdivision makes prominent use ex-post fairness criteria equity envy-freeness,point departs auctions, rather focus kinds fairness (aswell efficiency procedure), truthful mechanisms, fairnessprocedure (Brams & Taylor, 1996; Young, 1995). key concept literaturefair division envy-freeness: allocation envy-free agent likesshare least much share agent. Ensuring envy-freeness considereddesirable property; however, envy-freeness alone suffice criterion findingsatisfactory allocations (this especially obvious compulsory allocategoods: case, allocating anything anyone results envy-free allocation, yettotally unsatisfactory), therefore paired efficiency criterion,Pareto optimality maximum social welfare. However, known reasonablenotion efficiency, profiles efficient envy-free allocation exists2(Brams, Edelman, & Fishburn, 2003). even trivial every good must assignedsomeone: case, profiles even envy-free allocation exists.Another well-known notion fairness (that consider paper, except oneresult) Rawlsian egalitarianism, says allocation equitable maximizessatisfaction least satisfied agent. Unlike envy-freeness, egalitarianism requiresinterpersonal comparability preferences.Whereas social choice theory developed important literature fair division,computational issues rarely considered. hand, artificial intelligencestudied issues extensively, focused mainly combinatorialauctions related classical utilitarian problems. Combinatorial auctions, aiming maximizing auctioneers revenue (consisting sum prices paid agents),specific form allocation process, namely pure utilitarian form moneytransfers considerations equity fairness relevant. literaturecombinatorial auctions related problems investigating issues compact representation (so allow agents express bids concise way seework Nisan, 2005, overview) well computational complexity, algorithms,tractable subclasses approximation. Complexity issues negotiation (where agentsexchange goods means deals) also studied (e.g. Dunne, Wooldridge, &Laurence, 2005; Chevaleyre, Endriss, Estivie, & Maudet, 2004).discussion reveals existence gap (summarized Table 1): compactrepresentation complexity issues fair division received little attentionnow, apart recent work (Lipton, Markakis, Mossel, & Saberi, 2004)1. Note possibility money transfers reintroduces divisibility extent, considering moneyparticular divisible good.2. Consider example situation single item two agents want it:case, allocation either efficient envy-free (when item given one twoagents), envy-free efficient (if item allocated anyone).526fiEfficiency Envy-freeness Fair Division Indivisible Goodsstudies approximation schemes envy-freeness. need compact representationarises following dilemma, formulated several social choice theorists: either (a)allow agents express possible preference relation set subsets items,end exponentially large representation (which actually happens example3work Herreiner & Puppe, 2002); (b) severely restrict set expressiblepreferences, typically assuming additive independence items, designprocedures agents express preferences single items, preferencesextended sets items assuming additivity, thus giving possibilityexpressing preferential dependencies complementarity substitutability effectsamong items; path followed Brams et al. (2003), Brams King (2005)Demko Hill (1998). Yet, advocate paper, conciliating concisenessexpressivity possible, means compact representation.axiomatic studyauctions(and related problems)economicsfair divisioneconomics(especially social choice)computational studycomputer science(especially AI)?Table 1: Computational issues fair division.works fair allocation indivisible items, focus joint searchenvy-freeness efficiency. impossibility guarantee existence efficientenvy-free allocation implies determining whether exists allocationcrucial task, since positive answer leads choose allocation whereas negativeanswer calls relaxation one criteria, investigated papers(Lipton et al., 2004; Chevaleyre, Endriss, & Maudet, 2007b).consider problem determining whether exists efficient envy-freeallocation point view compact representation computational complexity.First, since cases agents preferential dependencies (or synergies)goods, raise issue fair division problem indivisible goodsexpressed. focus first simple case agents dichotomous preferences,is, simply express partition satisfactory unsatisfactory shares.interest restriction spite expressivity loss imposes,shown less complex general case, much simpler expose.Dichotomous preferences considered social choice contexts,Bogomolnaia, Moulin, Stong (2005) fair division context, course approvalvoting (Brams & Fishburn, 1978), every voter specifies dichotomous preferenceset candidates. natural representation dichotomous preference (withpreferential dependencies formulae otherwise problem trivial) single3. Quoting work Brams et al. (2003): Herreiner Puppe (...) assume personlinear preference order 2B . allows complementarity substitutability effects among items(...). view interdependencies may beset subset evaluations (...), procedures HerreinerPuppe offer creative way dealing subset preference. hand, sheer numbersubsets (more million n = 20) presumption clear preference subsets,could detract practicability procedures.527fiBouveret & Langpropositional formula, variables correspond goods. Expressing envy-freenessefficiency within logical representation reveals unexpected connections nonmonotonicreasoning; issue addressed Section 3.following Sections devoted detailed complexity study following problem: given fair division problem, exist efficient end envy-free allocation?. latter problem studied different notions efficiency variousrestrictions. start (in Sections 3 4) assuming preferences dichotomous,identify complexity existence envy-free Pareto-efficient allocation, turns p2 -complete. consider several restrictionslatter problem, namely, (a) fixing number agents two; (b) forcing agentsidentical preferences; (c) restricting syntax propositional formulae expressing preferences agents. study variations problem obtainedreplacing Pareto-efficiency notions efficiency, namely: (a) asking completeallocations (such every good allocated agent); (b) requiring maximumnumber agents satisfied. Section 5 consider general problemobtained removing assumption preferences dichotomous. problembecomes dependent choice particular language compact preferencerepresentation, pick particular one (weighted propositional formulae) extendssimple way pure propositional representation considered Section 3, identifycomplexity existence envy-free efficient allocations, several notionsefficiency. Finally, Section 6 sum contributions discuss related workissues.2. Backgroundsection provide basic concepts definitions use alongpaper.2.1 Fair Division ProblemsDefinition 1 (Fair division problem) fair division problem4 tuple P = hI, X, Ri= {1, . . . , N } set agents;X = {x1 , . . . , xp } set indivisible goods;R = hR1 , . . . , RN preference profile, Ri reflexive, transitivecomplete relation 2X .Ri preference relation agent i. ARi B alternatively denoted Ri (A, B)B; write B (strict preference) (A B B A) B(indifference) (A B B A).addition, Ri said monotonic A, B, B X impliesB A. R = hR1 , . . . , RN monotonic Ri monotonic every i.4. following, use indifferently terms fair division resource allocation.528fiEfficiency Envy-freeness Fair Division Indivisible GoodsDefinition 2 (Allocation) allocation P = hI, X, Ri mapping : 2Xj 6= i, (i) (j) = . every x X existsx (i) complete allocation.words, possible framework give good differentagents time, possible throw away goods. paper,focus especially two desirable properties allocations: Pareto-efficiencyenvy-freeness.Definition 3 Let , 0 two allocations. dominates 0 (a) i, (i)0 (i) (b) exists (i) 0 (i). (Pareto-) efficient0 0 dominates .Definition 4 allocation envy-free (i) (j) holdsj 6= i.2.2 Propositional LogicLet V finite set propositional variables. LV propositional language generatedV , usual connectives , Boolean constants > usualway5 .interpretation LV element 2V , i.e., truth assignment symbols:x V , x (resp. x 6 ) means assigns x true (resp. false).od() = {M 2V | |= } set models (the satisfaction relation |=defined usual, well satisfiability logical consequence).literal formula LV form x form x, x V . formulanegative normal form (or NNF) negation symbol appearsliterals. formula turned polynomial time equivalent NNFformula. instance, (b c) NNF equivalent NNF formula(b c).formula positive contains occurrence negation symbol. instance,(b c) (a b) positive, whereas (b c) (a c) (a b) are.> considered positive well.Let LV . V ar() V set propositional variables appearing .instance, V ar((a c) (a b)) = {a, b, c} V ar(>) = .VLastly, = {1 , . . . , n } finiteW set formulae = 1 . . . nconjunction formulae S, = 1 . . . n disjunction formulaeS.2.3 Computational Complexitypaper refer complexity classes located polynomial hierarchy.BH2 (also referred DP) class languages form L1 L2 L1NP L2 coNP. p2 = PNP class languages recognizable deterministicTuring machine working polynomial time using NP oracles. Likewise, p2 = NPNP . p2 =5. Note connectives allowed; important definition positive formulae(to come).529fiBouveret & Langp2 [O(log n)] subclass p2 problems need logarithmic numberoracles. See instance book Papadimitriou (1994) details.3. Fair Division Problems Dichotomous Preferences: LogicalRepresentationstart considering full detail case preferences dichotomous.Definition 5 Ri dichotomous exists subset Goodi 2XA, B X, B Goodi B6 Goodi . R = hR1 , . . . , RNdichotomous every Ri dichotomous.obvious way representing dichotomous preferences compactly, namelypropositional formula (for agent i) language LX (a propositional symbolvx good x) od() = Goodi . Formally:Definition 6 Let Ri dichotomous preference 2X , Goodi associated subset2X , propositional formula propositional language LX . sayrepresents Ri od(i ) = Goodi .Clearly,preference Ri formula representing Ri : =Vfor dichotomousVvvx . Furthermore, formula unique logical equivAGoodixA xx6Aalence.WExample 1 X = {a, b, c} Goodi = {{a, b}, {b, c}}. Note Ri monotonic.= (a b c) (a b c) represents Ri . 0i = b ((a c) (a c)),logically equivalent .easy yet useful result:Proposition 1 Let Ri dichotomous preference 2X . following statementsequivalent:1. Ri monotonic;2. Goodi upward closed, is, Goodi B imply B Goodi .3. Ri representable positive propositional formula.Proof (1) (2). Suppose Ri monotonic, let Goodi B A. mustB A, therefore B Goodi (since Goodi ).(2) (3). Suppose Goodi upward closed,consider`V setmin (Goodi ) inclusionWminimal sets Goodi . formula = Amin (Goodi )xA vx represents Ri followingreasons. B Goodi , set min (Goodi ) B. Thus correspondingconjunction satisfied, satisfied. Conversely, set B 6 Goodi ,min (Goodi ) B. Therefore, none terms satisfied,satisfied. Moreover, clearly positive propositional formula.(3) (1). Suppose Ri representable positive propositional formula , letB two sets items B. 6 Goodi clearly B A. Goodi ,od(i ). Since positive, B od(i ) also. Therefore B Goodi , finally B A.530fiEfficiency Envy-freeness Fair Division Indivisible Goodson, assume allocation problems P represented propositionalform, namely, instead I, X R specify h1 , . . . , N i. X obviouslydetermined h1 , . . . , N i. following, also write propositional variablescorresponding items x instead vx short, since often unambiguous.Let P = h1 , . . . , N allocation problem dichotomous preferences;N , rewrite obtained replacing every variable xnew symbol xi . instance, 1 = (b c) 2 = 1 = a1 (b1 c1 )2 = a2 d2 .Example 2 Consider following allocation problem: 1 = (b c), 2 =3 = ab (therefore = {1, 2, 3} X = {a, b, c}). formulae positive, thereforepreferences monotonic. instance, Good1 set composed supersets {a}supersets {b, c}.1 = a1 (b1 c1 );2 = a2 ;3 = a3 b3N , let Xi = {xi , x X}. allocation standard allocation problemP corresponds model V = X1 . . . XN satisfying one xi x X.terms, bijective mappingset possible allocationsVVmodels following formula: P = xX i6=j (xi xj ).Vallocation required complete, P replaced CP = P xX (x1. . . xn ). rest unchanged.Let V = {xi | = 1, . . . , N, x X}. interpretation od(P )never case xi xj simultaneously true 6= j, therefore mapod(P ) allocation simply defined (i) = {x | |= xi }. mappingobviously bijective; write F () model od(P ) corresponding allocation, course F 1 (M ) allocation corresponding interpretation od(P ).Example 2 (continued) allocation problem Example 2, have:P = (a1 a2 ) (a1 a3 ) (a2 a3 )(b1 b2 ) (b1 b3 ) (b2 b3 )(c1 c2 ) (c1 c3 ) (c2 c3 )interpretation sets a1 , b3 , c1 true clearly modelP . corresponds allocation F 1 (M ) = , (1) = {a, c}, (2) =(3) = {c}.3.1 Envy-freenessshow search envy-free allocation mapped model findingproblem. Let j|i formula obtained substituting every symbol xixj : instance, 1 = a1 (b1 c1 ) 2|1 = a2 (b2 c2 ). Notice obviously,i|i = .first give following lemma, easy yet useful:531fiBouveret & LangLemma 1 i, j, (j) Goodi F () |= j|i .particular, = j (i) Goodi F () |= .Proof definition F , (j) Goodi {x | F () |= xj } Goodi , is, {x | F () |=xj } |= . latter relation equivalent {xi | F () |= xj } |= , finally {xj | F () |=xj } |= j|i definition j|i , deduce result.Using lemma, map envy-freeness property satisfiabilitylogical formula:Proposition 2 Let P = h1 , . . . , N allocation problem dichotomous preferences propositional form, formulae j|i mapping F defined above.Let^^P =j|ii=1,...,Nj6=ienvy-free F () |= P .Proof Let allocation. envy-free pair (i, j), 6= j(j) (i), (j) Goodi (i) 6 Goodi , turn equivalent F () |= j|iF () 6|= lemma 1. Therefore envy-free F () |= P .intuitive meaning result allocation envy-free if,every agent i, either satisfied share (i), envies one,is, every j, would satisfied js share either.search envy-free allocations thus reduced model finding problem:{F 1 (M ) | |= P P } set envy-free allocations P. Note that, importantly,P P polynomial size (precisely, quadratic) size input data.problem existence envy-free allocation without propertyinteresting, allocation always exists : suffices consider allocation gives empty share everyone. However,problem deciding whether exists envy-free allocation satisfyingproperty expressible polynomial size formula (e.g. completeness) reducedsatisfiability problem;problem finding (resp. counting) envy-free allocations comesproblem finding (resp. counting) models P P .Example 2 (continued) allocation problem Example 2, have:P =((a1 (b1 c1 )) ((a2 (b2 c2 )) (a3 (b3 c3 ))))(a2 (a1 a3 ))((a3 b3 ) ((a1 b1 ) (a2 b2 )))od(P P ) = {{c1 }, {c1 , b3 }, {c2 , b3 }, {c2 }, {b3 }, {c3 }, }.therefore 7 envy-free allocations, namely (c, , ), (c, , b), (, c, b), (, c, ),(, , b), (, , c) (, , ). Note none complete.532fiEfficiency Envy-freeness Fair Division Indivisible Goods3.2 Efficient AllocationsSimilarly envy-freeness property, Pareto-efficiency expressed logicalproperty. logical expression property requires definition maximal consistent subset set formulae.Definition 7 Let = {1 , . . . , } set formulaeformula.Vmaximal -consistent subset Vif (a) consistent (b)0 0 0 consistent. Let axCons(, ) setmaximal -consistent subsets . Moreover, write axCons() setmaximal-consistent subsets , is, set axCons(, >).Proposition 3 Let P = h1 , . . . , N allocation problem. Let P = {1 , . . . , N }.Pareto-efficient P {i | F () |= } maximal P -consistentsubset P .Proof Let allocation. Let Sat() set agents satisfied , is, Lemma 1,Sat(), F () |= P|= definition Vset {i | F () |= }. Sat(),V F ()|Sat()}={i | F () |= }. ThereforedefinitionF().ThereforeF()|={PPV{i | F () |= } P consistent.definition, Pareto-dominated allocation 0 Sat( 0 ) )Sat(). Therefore, Pareto-dominated consistent subset P (correspondingV{i | F ( 0 ) |= }) {i | F () |= } S. Moreover, since 0 allocation, Pconsistent.VConversely,{i | F () |= } P consistent,V let P01model P . definition, = F (M ) well-defined allocation, 0 (i) GoodiS. Since {i | F () |= } S, Sat( 0 ) ) Sat(). Therefore Pareto-dominated.simple result suggests efficient allocations computed logicalexpression P problem, namely, computing maximalP -consistent subsetsVP ; call {S1 , . . . , Sq }. Si , let Mi = od( Si P ) let = qi=1 Mi .F 1 (M ) set efficient allocations P . Note generalexponentially many maximal P -consistent subsets P (and therefore exponentially manyefficient allocations). tempered (a) many practical casesnumber maximal consistent subsets small; (b) generally asked lookefficient allocations; look one, done computing onemaximal P -consistent subset P .Example 2 (continued) maximalP -consistent subsets P S1 =V{1 , 2 },VS2 = {1 , 3 } S3 = {2 , 3 }.S1 VP one model: {b1 , c1 , a2 }.2 Ptwo models: {a1 , b3 } {b1 , c1 , a3 }. S3 P one model: {a2 , b3 }. Thereforefour efficient allocations P (bc, a, ), (a, , b), (bc, , a) (, a, b). Noneenvy-free.3.3 Efficient Envy-free Allocationsposition putting things together. Since envy-free allocations correspondmodels P efficient allocations models maximal P -consistent subsetsP , existence efficient envy-free (EEF) allocation equivalentVfollowing condition: exists maximal P -consistent subset P533fiBouveret & LangP P consistent. case, models latter formula EEF allocations.Interestingly, instance well-known problem nonmonotonic reasoning:Definition 8 supernormal default theory6 pair = h, = {1 , . . . , },1 , . . . , propositional formulae. propositional formula skepticalconsequence D, denoted | , axCons(, )V|= .Proposition 4 Let P = h1 , . . . , N fair division problem. Let DP = hP , P i.exists efficient envy-free allocation P DP 6| P .Proof Let P = h1 , . . . , N fair division problem. Let Pareto-efficient envy-freeVallocation, = {i | F () |= }.2. also F () |=V F () |= P PropositionVdefinitionS, F () |= P , proves P consistent, or,Vterms, 6|= P . Moreover,V maximal P -consistent subset P Proposition 3. ThusaxCons(P , P ), P 6|= P , implies hP , P 6| P definition 6| .set axCons(P , P )V Conversely, suppose hP , P 6| P .P VP model . Proposition 3, F 1 (M ) Pareto-efficient allocation (sincemodel P ), Proposition 2, F 1 (M ) envy-free (since model P ).somewhat unexpected connection nonmonotonic reasoningseveral impliVcations. First, EEF allocations correspond modelsP PaxCons(P , P ); however, axCons(P , P ) may exponentially large, argues avoiding start computing efficient allocations filteringenvy-free, rather compute EEF allocations single step, using default reasoning algorithms. Thus, fair division may benefit computational work default logicconnected domains belief revision answer set programming (Baral, 2003;Gebser, Liu, Namasivayam, Neumann, Schaub, & Truszczynski, 2007). Moreover, alternative criteria selecting extensions default reasoning (such cardinality, weightspriorities) correspond alternative efficiency criteria allocation problems.4. Allocation Problems Dichotomous Preferences: Complexitystudy section complexity problem existence EEFallocation restrictions, case preferences dichotomous,several notions efficiency.4.1 Complexity General Problemknown skeptical inference p2 -complete (Gottlob, 1992); now, Proposition4, problem existence EEF allocation reduced complementskeptical inference problem, immediately tells p2 . Less obviously,show complete class, even preferences required monotonic.prove hardness, use following restriction skeptical inference problem:Problem 1: restricted skeptical inference (rsi)INSTANCE: set propositional formulae = {1 , . . . , n }.QUESTION: maximal-consistent subsets contain 1 ?6. Supernormal defaults also called normal defaults without prerequisites (e.g. Reiter, 1980).534fiEfficiency Envy-freeness Fair Division Indivisible GoodsProposition 5 problem rsi p2 -complete.Proof Membership p2 comes easily fact problem rsi restrictionskeptical inference problem, formula infer simply 1 . Hardness comes factinstance h, , skeptical inference problem, h, |h{} , | , h{ } {1 , . . . , n }, >i | , maximal-consistent subsets {, 1 , . . . , n } contain , instance rsi.Proposition 6 problem eef existence determining whether exists efficient envy-free allocation given problem P monotonic, dichotomous preferences logical form p2 -complete.show hardness following reduction rsi (the complement problem rsi,is, one maximal-consistent subset contain 1 ?) eefexistence. Given finite set propositional formulae, let V = V ar() setpropositional symbols appearing , let P() = hI, X, P() followinginstance eef existence:1. = {1, 2, ..., n + 3};2. X = {v |v V , 1...n} {v |v V , 1...n} {xi |i 1...n + 1} {y};3. = 1, . . . , n, let obtained following sequence operations:(i) put NNF form (let i0 result); (ii) every v V , replace, i0 ,(positive) occurrence v v occurrence v v ; letformula obtained. Then:= 1, . . . , n, = xi ,VVnVnn+1 y,n+1 =vvxvVi=1i=1n+2 = y,n+3 = 1 .prove Proposition 6 using several lemmas.Lemma 2 allocation P said regular n + 3,(i) (i),n, (i) = vV {v , v } {xi };(n + 1) = vV ,i=1,...,n {v , v } {xn+1 , y};(n + 2) = {y}.(n + 3) = (1).Given allocation , let R defined R (i) = (i) (i).1. R regular;2. efficient R efficient;535fiBouveret & Lang3. envy-free R envy-free.Proof (1) obvious. i, goods outside (i) influence satisfaction(since appear ), therefore R (i) (i), (2) follows. formulaepositive, preference relations monotonic, therefore (j) R (j) holds i, j.Now, envy-free i, j (i) (j), therefore R (i) (i) (j) R (j)therefore R envy-free, (3) follows.Lemma 3 regular1. 1 envy n + 3;2. n + 3 envy 1;3. 2, . . . , n envy one;4. n + 1 envy n + 2;5. n + 2 envy n + 1;Proof First, note i, j 6= i, envies j (i) |= (j) |= .1. Let = 1 j {2, . . . , n, n + 2}. 1 envies j, x1 (j). regular, x1 6 (j),therefore cannot envy j.2. Since n+3 = 1 , holds agent n + 3.3. Let {2, . . . , n} j 6= i. envies j (j) |= xi , impossiblexi 6 (j), due regularity .4. Assume n + 1 envies j j {1, . . . , n,n + 3}. (j) |= n+1 . Since(j) |= impossible`V n`V nVn+1(because regular), (j) |=x, thus (j) |= xn+1vvvVi=1i=1impossible well, since regular.5. Let = n + 2 j {1, . . . , n, n + 3}. envies j (j) |= y, impossibleregular.Lemma 4 Let regular allocation satisfying n + 1 n + 2 leaving 1 n + 3unsatisfied. Let () interpretation V obtained by: v V ,() |= v (i.e., v ()) n + 1 receives v 1 , . . . , v n , () |= v otherwise, i.e.,n + 1 receives v 1 , . . . , v n . efficient envy-free () 6|= 1 .Proof Let regular allocation satisfying n + 1 n + 2. Since satisfies n + 2, (n + 2).Now, satisfies n + 1 without giving y, therefore, v, n + 1 receives either vv s. shows definition () well-founded.assume efficient envy-free, suppose () |= 1 . Oneagents 11n +3satisfiablewithoutspoilingagentj6{1,n+3},giving{x}(v | () |=v ) (v | () |= v ). since efficient must satisfy least one 1 n + 3. cannotsatisfy simultaneously (because x1 ). Thus one among 1 n + 3 satisfied ,one envying her. contradicts envy-freeness , thus proving () 6|= 1 .Lemma 5 interpretation V , let us define : 2X by:1, . . . , n, (i) = {v | |= v} {v | |= v} {xi };(n + 1) = {xn+1 } {v | |= v, = 1, . . . , n} {v | |= v, = 1, . . . , n};(n + 2) = {y}536fiEfficiency Envy-freeness Fair Division Indivisible Goods(n + 3) = .Then:1. well-defined regular allocation satisfying n + 1 n + 2;2. (M ) = (M (M ) obtained Lemma 4).3. 1, . . . , n, satisfies |= .4. efficient satisfies maximal consistent subset .Proof1. One easily check give good one individual,give agent set items (i). Thereforewell-defined regular allocation. allocation obviously satisfies n + 1 n + 2.2. |= v (n + 1) contains {v | = 1, . . . , n} therefore (M ) |= v. case|= v similar.3. Let 1, . . . , n. Since gives xi i, satisfies F (M (i)) |= ,equivalent |= .4. point 3, {i | satisfies i} = {i | |= } {n + 1, n + 2} (obviously, n + 3satisfied). Now, since preferences dichotomous, allocation efficientset individuals satisfies maximal respect inclusion. Therefore, efficientsatisfies maximal consistent subset .Lemma 6 Let regular efficient allocation satisfying n + 1 n + 2. ()satisfies maximal consistent subset .Proof regular satisfies n+1 n+2, therefore obviously (n+2) = {y} lemma 4 ()well-defined. consider allocation () , defined like previous lemmas.() (n + 1) = {xn+1 } {v |M () |= v} {v |M () |= v} = {xn+1 } {v |{v 1 , . . . , v n }(n + 1)} {v |{v 1 , . . . , v n } (n + 1)}. Since n + 1 satisfied , (n + 1) must contain {xn+1 },assert () (n + 1) (n + 1).Let {2, . . . , n}. Since regular, (i) (i). Since () complete allocationdefinition, regular lemma 5, (i) () (i) () (n + 1). Since () (n + 1) (n + 1)(i) () (i) (n + 1), thus (i) () (i) (n + 1). allocation,course (i) (n + 1) = , follows (i) () (i).regular, (1)(n+3) (1)(n+3). Since (1) = (n+3), latter inclusioncomes (1) (n + 3) (1). Now, (1) () (1) () (n + 1) () (n + 3)similar reasons {2, . . . , n}, which, together () (n+1) (n+1) () (n+3) = ,comes (1) () (1) (n + 1), and, (1) (n + 1) = , deduce inclusion(1) (n + 3) () (1).prove () efficient. Since preferences monotonic, individuals n + 3satisfied satisfied 0 well (since 6= n + 3, (i) () (i)).n + 3 satisfied , immediately deduce () efficient.n + 3 satisfied , suppose () efficient. case, allocation0 i, () satisfies implies 0 satisfies particular j 6= 1 0satisfies j () satisfy j. Clearly, 0 satisfies 1 (since () does), thus j 6= n + 3(satisfying simultaneously 1 n + 3 impossible). Consider allocation 00 deduced0 swapping shares 1 n + 3. have, {2, . . . , n + 2}, satisfiesimplies () satisfies implies 0 satisfies implies 00 satisfies i. also satisfies n + 3satisfy 1, 00 . Thus satisfies implies 00 satisfiesi. Moreover, 00 satisfies j {2, . . . , n + 2} (the j above) () not,therefore neither . proves Pareto-dominated, contradictoryhypotheses.537fiBouveret & LangTherefore () efficient, conclude, together lemma 5 (point 4), ()satisfies maximal consistent subset .Lemma 7 envy-free efficient allocation P() satisfies n + 1 n + 2,leaves 1 n + 3 unsatisfied.Proof Suppose satisfy n + 1; 6 (n + 1); now, (n + 2) n + 1 enviesn + 2; 6 (n + 2) efficient giving n + 2 would satisfy n + 2 thus leadbetter allocation .Now, suppose satisfy n + 2, is, 6 (n + 2); (n + 1) n + 2 envies n + 1;6 (n + 1) again, efficient giving n + 2 would satisfy n + 2 thus leadbetter allocation .Concerning agents 1 n + 3, one notice since identical preferences,envy-free allocation must either satisfy both, leave unsatisfied. Since cannotsimultaneously satisfied (because x1 ), envy-free allocation leaves unsatisfied.Lemma 8 exists EEF allocation, exists maximal consistent subsetcontain 1 .Proof Let efficient envy-free allocation. Lemma 2, R regular, efficient envyfree. Lemma 7, R satisfies n + 1 n + 2 leaves 1 n + 3 unsatisfied. Lemma 6,(R ) satisfies maximal consistent subset , Lemma 4, (R ) 6|= 1 . Therefore {i| (R ) |= } maximal consistent subset contain 1 .Lemma 9 exists maximal consistent subset contain 1exists EEF allocation.Proof AssumeV exists maximal consistent subset contain 1 , letmodel S. point 4 Lemma 5, efficient.point 1 Lemma 5, regular; Lemma 3, envy-free (i) 1envy n + 3, (ii) n + 3 envy 1 (iii) n + 1 envy n + 2 (iv) n + 2 envyn + 1. definition , satisfy n + 3, hence (i) holds. point 3 Lemma 5, 6|= 1implies satisfy 1, therefore (ii) holds well. finally, point 1 lemma 4, n + 1n + 2 satisfied , thus (iii) (iv) also hold. Therefore envy-free.position putting things together proving Proposition 6:Proof (Proposition 6) Lemmas 8 9, existence maximal consistent subsetcontain 1 existence efficient envy-free allocation P() equivalent.Clearly, P() computed polynomial time. Therefore, P polynomial reduction rsi eefexistence, shows latter problem p2 -hard, therefore p2 -complete.corollary, p2 -completeness result holds general (not necessarily monotonic)dichotomous preferences:Corollary 1 eef existence general, dichotomous preference logical form p2 complete.4.2 Restrictions Languageconsequence high complexity, worth studying restrictions variantslatter problem complexity may fall down. investigate three kindintuitive restrictions eef existence problem, defined by:fixing number agents, especially, restricting problem case2 agents;538fiEfficiency Envy-freeness Fair Division Indivisible Goodsforcing agents identical preferences;restricting syntax agents goals, limiting expression subclasses propositional formulae (e.g. clauses, cubes, . . . ).Contrary general eef existence problem, complexity restrictionsmay sensitive whether preferences monotonic not.4.2.1 Identical Preferencesstart considering identical dichotomous preference profiles, is, agentspreference, i.e. formula .Proposition 7 eef existence N identical dichotomous, monotonic preferencesNP-complete. result holds even fixed number agents N 2.Proof preferences identical, envy-free allocation satisfies either agents none; now,preferences monotonic, always possible satisfy least one agent (by giving items).Therefore, allocation EEF satisfies agents. Clearly, checkedpolynomial time given allocation satisfies agents, hence membership NP.Hardness comes simple reduction set splitting:Problem 2: set splittingINSTANCE: collection C = {X1 , . . . , Xn } subsets finite set S.QUESTION: partition hS1 , S2 subset C entirely contained eitherS1 S2 ?Given instance hC, Si set splitting, let P(C, S) following eef existence instance:Agents:Objects:Preferences:2 agents,one object x(a) per element S,VWVW1 = 2 = Xi C aXi x(a) (and usual k = Xi C aXi xk (a)): agentwants least one object set.easy see set splitting hS1 , S2 hC, Si, possible find allocationsatisfy two agents, giving respectively x(S1 ) x(S2 ). Conversely, supposeefficient envy-free allocation , allocation must satisfy two agents. Let1hS1 , S2 = hx1 ((1)),Wx ((2))i. Suppose Xi C Xi S1 Xi S2 (saye.g Xi S1 ). aXi x2 (a) false, thus making 2 false, contradictory initialhypothesis. Therefore hS1 , S2 set splitting hC, Si.clearly polynomial-time reduction, hence NP-hardness eef existence 2 agentsidentical dichotomous monotonic preferences.Unlike Proposition 6, Proposition 7 sensitive whether preferences requiredmonotonic not.Proposition 8 eef existence N identical dichotomous preferences coBH2 -complete.result holds even fixed number agents N 2.Proof preferences identical, envy-free allocation satisfies either agents none. Letformula representing one agents preferences (of course identical agents).satisfiable possible satisfy least one agent. case, allocation EEFsatisfies agents. satisfiable, every allocation EEF. Therefore,exists EEF allocation 1 . . . . . . N satisfiable unsatisfiable. showsmembership coBH2 .Hardness comes simple reduction sat-or-unsat. Let = h, pair propositional formulae, assumed w.l.o.g. variables common; map following allocationproblem:539fiBouveret & LangAgents:Objects:Preferences:2 agents;2 objects v v 0 per propositional variable v appearing , one object w per propositional variable w appearing , one object y;1 = 2 = 0 (y ) 0 denotes formula variable vreplaced v 0 .1. Suppose satisfiable, (corresponding negative instance sat-orunsat). possible satisfy least one agent giving objects vcorresponding variables assigned true model . However possiblesatisfy simultaneously second agent unsatisfiable (and way 0 ),first agent already taken y. Therefore case EEF allocation.2. Suppose satisfiable unsatisfiable (corresponding positive instancesat-or-unsat). two cases:satisfiable. case, matter whether satisfiable not, possible satisfytwo agents satisfying simultaneously first one 0 second one.Consequently EEF allocation.unsatisfiable (recall case unsatisfiable satisfiable coveredpoint 1). case clearly impossible satisfy agent. case, emptyallocation efficient envy-free.Therefore EEF allocation satisfiable unsatisfiable, provesproposition.4.2.2 Two AgentsNote two previous results, hardness result holds fixed numberagents ( 2). Things different Proposition 6, hardness holdN fixed. Namely, following results.Proposition 9 eef existence two agents monotonic dichotomous preferencesNP-complete.Proof Hardness corollary Proposition 7. Membership obtained follows. Let h1 , 2preference profile, 1 , 2 positive. formulae , formulae definedearlier (see section 3), well F () allocation . efficient either (a)satisfies agents, (b) satisfies one agent, 1 2 unsatisfiable, (c)impossible satisfy even one agent, i.e., 1 2 unsatisfiable. (c) impossible1 , 2 positive. Now, envy-free |= . Therefore, EEF (a)|= 1 2 (b) |= (1 2 ) . Thus, exists EEF allocation( 1 2 ) ( (1 2 ) ) satisfiable, hence membership NP.Proposition 10 eef existence 2 agents dichotomous preferences coBH2 complete.Proof Membership comes following reduction sat-or-unsat. Consider instance PEEF problem 2 agents respectively preferences 1 2 . translate instanceinstance h, 0 sat-or-unsat defined follows ( defined usual): = ( 1 2 )(1 2 ) (1 2 ) 0 = (1 2 ). prove satisfiable 0 unsatisfiableEEF allocation P.1. Suppose satisfiable, 0 is. Since satisfiable, valid allocationsatisfy agents (because 1 2 satisfiable). Since 0 satisfiable, possiblesatisfy least one agent (because 1 2 satisfiable). deduce everyefficient allocation satisfies exactly one agent. Since (1 2 ) (1 2 ) satisfiable,od(1 ) = od(2 ) (in words, 1 2 logically equivalent). Let allocation540fiEfficiency Envy-freeness Fair Division Indivisible Goodssatisfying agent 1 (the case similar agent 2), F () |= 1 , thus F () |= 2|1 .Since F () 6|= 2 (because impossible satisfy agents), F () 6|= thusenvy-free. Hence every efficient allocation raises envy: EEF allocation.2. Suppose satisfiable 0 satisfiable.0 satisfiable. case easy possible satisfy even one agent.Therefore every allocation efficient envy free.satisfiable. two cases:1 2 satisfiable. case, allocation, corresponding model1 2 , satisfies agents. allocation clearly EEF.1 2 satisfiable 1 2 satisfiable (the case 2 1 similar).case possible satisfy agents. However, since satisfiable,possible satisfy least one, and, like point 1, every efficient allocation satisfiesexactly one agent. Since 1 2 satisfiable, model 1model 2 . allocation corresponding model agent 1satisfied agent 2, latter agent cannot envy first one.finally proves correctness reduction, clearly polynomial.Hardness comes directly Proposition 8.4.2.3 Restriction Propositional Languageprevious results, made specific assumptions formulae expressingagents preferences, except (sometimes) monotonicity. However, restrict set possible propositional formulae, decrease complexity eef existence problem.investigate first two natural restrictions propositional language: first caserestrict formulae set clauses (disjunctions literals), second casepreferences expressed using cubes (conjunctions literals). restrictionsmatch two different kinds real-world problems.case, agents preferences represented clauses, correspondskind problems agent wants one single object certain class. Oneconsider example set patients waiting kidney transplant.patient needs one kidney, several ones may compatible.case, agents preferences represented cubes, correspondskind problems agent needs single bundle objects. typicalkind problems agents build object want set basicmaterial (or virtual) components: set objects stand basic components,cube one agent stands complete device wants.Making one two assumptions actually decreases complexity eef existence problem: even renders tractable case clauses objects.Proposition 11 eef existence agents dichotomous preferences restrictedclauses objects solved polynomial time.Proof first make two additional assumptions prove complexity problemdecrease two assumptions. (1) suppose first agents preferencesmonotonic. one agent non-monotonic preferences, means one negative literalclause. Giving empty share satisfy without spoiling another agent, thussafely removed problem. (2) also suppose agent wants least one object. one541fiBouveret & Langagent empty clause goal, means matter gets, cannot satisfied.Thus also safely removed problem. rest proof, considerproblems verify assumptions (1) (2).proof based following result: agents preferences disjunction objectsassumptions (1) (2), allocation Pareto-efficient envy-free satisfiesevery agent. implication immediate. prove implication need noticeagent satisfied receives least one object clause. take allocationagent satisfied . either possible satisfy withoutspoiling another agent, one object clause given agent want it,already satisfied another agent: case Pareto-efficient. possibleobjects agent disjunction given agents trulywant them: case, agent envies agents, thus envy-free.Therefore, finding Pareto-efficient envy-free allocation comes finding allocation gives agent one object wants. Thus, instance P eef existenceproblem reduced maximal matching problem, bipartite graph GP onenode per agent one side, one node per object side, edgeagent-node object-node agent clause. easilychecked Pareto-efficient envy-free allocation matching sizen GP . latter problem solved time O(nm) (Ford & Fulkerson, 1962),size biggest disjunction.investigate case agents preferences cubes objects.surprisingly, case harder previous one, remains NP.Proposition 12 eef existence agents dichotomous preferences restrictedcubes objects NP-complete. result holds require preferencesmonotonic.Proof proof organized follows. first prove membership NP without assumptionmonotonicity preferences. show hardness monotonic case.first introduce additional notations. following, denote Obj + (i) (resp.Obj (i)) set objects appearing positive (resp. negative) literals agent cube. Letallocation. said minimally regular i, either (i) = Obj + (i) (i) = .given allocation , denote R corresponding minimally regular allocation, is,allocation i, R (i) = Obj + (i) 6 (i), R (i) = Obj + (i) Obj + (i) (i).+also writeSat() set agents satisfied : Sat(M R ) = {i | R (i) = Obj (i)}),Allocated() = iI (i) (the set objects allocated agent).following result:Lemma 10 Let allocation. have:R minimally regular;Sat() Sat(M R );Pareto-efficient, R also Pareto-efficient.Proofagent i, R (i) = R (i) = Obj + (i) definition R . ThereforeR minimally regular.Let allocation, let agent. satisfied , Obj + (i) (i)Obj (i) (i) = . definition R , R (i) = Obj + (i), thus stillObj + (i) (i) Obj (i) (i) = , therefore agent still satisfied R .proves Sat() Sat(M R ).Suppose Pareto-efficient, suppose 0 Pareto-dominatesR . Sat() Sat(M R ) ( Sat( 0 ), contradicts factPareto-efficient. proves third point.542fiEfficiency Envy-freeness Fair Division Indivisible GoodsLemma 11 minimally regular allocation R Pareto-efficient(a) 6 Sat(M R ) (b) Obj + (i) Allocated(M R ) = .Proof Let R minimally regular allocation, suppose6 Sat(M R ) Obj + (i) X \ Allocated(M R ). allocation 0 j 6=0 (j) = R (j) 0 (i) = Obj + (i) well-defined (since Obj + (i) among set unallocatedobjects R ), Pareto-dominates R , since agents satisfied R also satisfied0 , satisfied 0 whereas R .Conversely, suppose R Pareto-efficient, let 0 Pareto-efficient alloca0tion Pareto-dominates R . Lemma 10,R Pareto-efficient, also Pareto0dominates R . Sat(M R ), R (i) = R (i) = Obj + (i) since two allocations0minimally regular, agent satisfied R also satisfiedR . Moreover,0+j 6 Sat(M R ) R (j) = Obj (j). Since Allocated(M R ) = iSat(M R ) Obj + (i)0++sinceR (j) X \iSat(M R ) Obj (i), Obj (j) X \ Allocated(M R ),finally proves lemma.two lemmas provide procedure check given allocation Pareto-efficient: firstcompute R (which done polynomial time). Lemma 10, Sat() Sat(M R ).inclusion strict (that is, Sat() ( Sat(M R )), obviously Pareto-efficient since RPareto-dominates it. Otherwise, checking Pareto-efficient comes checking RPareto-efficient, comes down, according Lemma 11, n set inclusion tests. Now, checkingenvy-free still polynomial. Hence problem NP.prove hardness problem focusing monotonic preferences (that is,Obj (i) = i). hardness proof, need additional lemmas.Lemma 12 Let allocation suppose agents monotonic preferences.envy-free R envy-free.Proof Let allocation. Lemma 10, Sat() Sat(M R ). Conversely, let Sat(M R ).R (i) (i), proves Sat(), deal monotonic preferences.suppose envies j R . satisfied R , thus neither satisfied. Since R (j) (j) agent preferences monotonic, still envy j . Thusenvy-free, R .important corollary lemma deal monotonic cubes, existencePareto-efficient envy-free allocation equivalent existence minimally regular Paretoefficient envy-free allocation. Therefore restrict existence problem minimallyregular allocations.Lemma 13 Let j two different agents (and still suppose agents monotonicpreferences). (there exists minimally regular allocation R envies j)Obj + (i) Obj + (j).Proof Let R minimally regular allocation, suppose envies j. obviouslysatisfied j is; hence R (j) = Obj + (j). Since envies j, thus directlyObj + (i) Obj + (j).Conversely, suppose Obj + (i) Obj + (j). allocation R gives Obj + (j)j nothing agents clearly minimally regular, also obviouslyenvies j.introduce NP-complete problem use prove NP-hardness (in monotoniccase):Problem 3: exact cover 3-sets (Karp, 1972)INSTANCE: set size 3q, collection C = hS1 , . . . , S|C| 3-element subsetsQUESTION: C contain exact cover S, i.e. sub-collection C 0 C every elementoccur exactly one member C 0 ?Given instance hS, C = hS1 , . . . , S|C| ii exact cover 3-sets (we assume w.l.o.g.Si different), let P(S, C) following eef instance:543fiBouveret & LangAgents:set |C| + 2|S| agents = I1 I2 , I1 = {1, . . . , |C|} I2 = {|C| + 1, . . . , |C| +2|S|},Objects:set 2|S| items X = X1 X2 , X = {x1 , . . . , x|S| } X 0 = {x01 , . . . , x0|S| },pair (xi , x0i ) corresponding different element ai S,Vagent I1 , = aj Si xj , k {1, . . . , |S|}, |C|+2k1 =|C|+2k = xk x0k .Preferences:words, first |C| agents preferences correspond sets collection C, last2|S| agents gathered pairs, member pair preferencesmember.Since Si different size 3, 6= j, Si 6 Sj , thus Obj + (i) 6 Obj + (j).definition preferences, also Obj + (i) 6 Obj + (j) (i, j) I1 I2(i, j) I2 I1 well. Hence, Lemma 13, potential source envy instancecomes agent I2 envying partner. Since impossible satisfy two agentspair time, allocation envy-free satisfy agentI2 .Lemma 11, minimally regular allocation R Pareto-efficient6 Sat(M R ) Obj + (i) X \ Allocated(M R ). Therefore minimally regular allocation RPareto-efficient envy-free k {1, . . . , |S|} R (|C| + 2k 1) ={xk , x0k } R (|C| + 2k) = {xk0 , x0k0 }, k0 {1, . . . , |S|} {xk0 , x0k0 }X \ Allocated(M R ) (this last condition comes xk0 6 Allocated(M R ), since xk0 x0k0 mustallocated together minimally regular allocation). Finally, R Pareto-efficient envy-freekS {1, . . . , |S|}, I1 xk R (i), is,iI1 R (i) =aj {xj }.Let R minimally regular allocation. define sub-collection g(M R )g(M R ) = {Si C | R (i) = Obj + (i)}. mapping g clearly defines bijection setnon-overlappingsub-collectionsset minimally regular allocations, one noticeiI1 R (i) =Sj g(M R )ak Sj {xk }.10Let C 0 CSis exact coverS.g (C ) existsvalid minimally regular allocation.10also iI1 g (C )(i) = Sj C ak Sj {xk } = aj {xj } C 0 cover. Thereforeg 1 (C 0 ) Pareto-efficient envy-free previous result.Conversely, suppose minimally regular Pareto-efficient andSenvy-free allocationR .g(M R ) non-overlapping sub-collection C, Si g(M R ) aj Si {aj } ={aj } = xj {M R (i) | iI1 } {aj }. R Pareto-efficient envy-free,SiI1 xj R (i)SiI1 R (i) =aj {xj }, hencexj {M R (i) | iI1 } {aj } =xj {xj | aj S} {aj } = S. provesg(M R ) exact cover S.reduction clearly polynomial; hence NP-hardness.previous proof (and especially Lemma 13) sheds light hard caseeef existence problem conjunctive preferences. instance problem,possible source envy comes Obj + (i) Obj + (j) deal minimallyregular allocations. case, cannot satisfy j without raising envy i.Obj + (i) ( Obj + (j), one remove j instance, satisfied,envy (note true non-monotonic preferences, onegive object agent want prevent envy another agent,thus cannot restrict problem minimally regular allocations).pair (i, j), 6= j, Obj + (i) = Obj + (j), one remove everyagent j 6= Obj + (j) ( Obj + (i). that, easysee every minimally regular allocation envy-free. Since least one Paretoefficient minimally regular allocation, guarantees existence Pareto-efficientenvy-free allocation case, may preferences monotonic not. formally:544fiEfficiency Envy-freeness Fair Division Indivisible GoodsProposition 13 always exists efficient envy-free allocation instanceeef existence problem agents dichotomous preferences restrictedcubes objects following condition holds:(i, j) 2 , 6= j, (i = j ) k k 6= i, k 6= j Obj + (k) ( Obj + (i) . (1)course, equivalence condition 1 existence Paretoefficient envy-free allocation7 , may happen that, given two agents jpreferences, satisfaction one two agents preventedanother agent k Obj + (i) Obj + (k) 6= , Obj + (k) 6 Obj + (i).hard case: two agents j identical preferences, agent kObj + (k) ( Obj + (i), may however possible prevent j satisfiedsatisfying another agent k 0 Obj + (i) Obj + (k 0 ) 6= , following exampleshows: 1 = 2 = x1 x2 , 2 = x2 x3 . Satisfying agent 2 leads efficientenvy-free allocation, whereas condition 1 hold.Proof (Proposition 13) following denote I1 set agents whose preferences inclusion-minimal, is, I1 = {i | @j Obj + (j) Obj + (i)}. denoteI2 agents: I2 = \ I1 .simple procedure finding Pareto-efficient envy-free allocation: greedily selectmaximal set agents I1 , agent receives Obj + (i) (until becomesimpossible select another unsatisfied agent I1 ).allocation resulting procedure minimally regular, Lemma 13 clearlyenvy-free (by definition I1 ). Moreover, suppose 6 Sat() Obj + (i) X \Allocated(). 6 I1 , since case, procedure would selected thereforesatisfied. also 6 I2 , case, j I1Obj + (j) Obj + (i), therefore Obj + (j) X \ Allocated(), impossiblereasons above. Therefore, also Pareto-efficient Lemma 11.investigated two natural restrictions propositional language useddichotomous preferences, introduce general result, based facthardness result Proposition 6 clearly linked NP-completeness satproblem. happens restrict expression preferences certain class Csat(C) solved polynomial time ? general case, additionalassumption made C cannot say anything complexity eefexistence problem complexity general problem. However, Calso closed conjunction, complexity falls NP:Proposition 14 Let C class propositional formulae closed conjunctionsat(C) P. eef existence agents dichotomous preferencesexpressed formulae class C NP.Proof Membership NP comes fact that, non-deterministically guessedallocation , checking envy-free Pareto-efficient done polynomial time. Givenallocation, checking envy-free done time O(nm) (where length biggestCNF), checking, unsatisfied agent, would satisfied shareanother agent. Given set Sat() ofVthe agents satisfied , checking Pareto-efficiencycomes check \ Sat() jsatI j unsatisfiable. done makinglinear number calls sat(C) oracle, since preferences C, since class closedconjunction. thus proves eef existence formulae C NP.7. Otherwise, Proposition 12 would false, would proved P = NP.545fiBouveret & Langcorollary, classes propositional formulae sat(C) polynomial,contain cubes, eef existence problem NP-complete. appliesexample class 2-CNF formulae class Horn clauses.4.3 Alternative Efficiency Criteriamain reason high complexity eef existence problem Paretoefficiency allocation hard check. consequence, complexity decreasechoose alternative notion efficiency. investigate two alternative efficiencycriteria: completeness allocation, maximal number satisfied agents.First, weaken Pareto-efficiency requiring allocations complete.Unsurprisingly, makes complexity fall NP.Proposition 15 problem deciding whether complete envy-free allocation agentsdichotomous preferences exists NP-complete, even 2 agents identical preferences.Proof Since checking allocation complete done polynomial time, membershipNP straightforward.prove hardness reduction sat problem. Let propositional formula.create following instance resource allocation problem : map propositional variabledifferent object add another object ; two agents preferences,represented formula y. Obviously, every complete allocation satisfies least one agent (theone receives y). satisfiable, possible satisfy agent well sharecorresponds model : thus case exists complete envy-free allocation.Conversely, suppose exists complete envy-free allocation. one two agentsmust satisfied thanks (since cannot given agents), hence proving satisfiable.Notice hardness proof longer valid require preferencesmonotonic. anonymous referee pointed out, proved reductionexact-cover-by-3-sets result holds require monotonicity, relaxingrestriction number agents. However, know NP-hardness holdstwo agents monotonic preferences (we conjecture does).Secondly, think looking cardinality-maximal subsets satisfied agents,instead inclusion-maximal subsets like Pareto-efficiency does.Proposition 16 problem deciding whether envy-free allocation satisfying maximal number agents monotonic dichotomous preferences exists p2 -complete.Proof Checking whether exists envy-free allocation satisfying least k agents NP;therefore, maximal number agents satisfied simultaneously computeddichotomy within log n NP oracles. suffices, step done, guess allocationcheck envy-free satisfies maximal number agents, adding one NP oracle.Hence membership p2 .Hardness obtained reduction following problem8 :Problem 4: max-index-satodd (Wagner, 1990)INSTANCE: sequence propositional formulae h1 , . . . , n (i unsatisfiable) (i+1unsatisfiable.)QUESTION: maximum index satisfiable odd number ?8. problem referred several times literature, seem name.546fiEfficiency Envy-freeness Fair Division Indivisible GoodsFirst notice complexity latter problem decrease following assumptions:n even (if not, add formula end sequence);sets propositional variables formulae pairwise-disjoint (if two formulaei+1 share variables, transform variable v say copy v 0 :change (un)satisfiability set propositional variables i+1disjoint).Let h1 , . . . , n instance max-index-satodd two additional latter assumptions,let Vi denote set propositional variables appearing . translate instancefollowing instance P(1 , . . . , n ):Agents:Objects:Preferences:2n agents : I1,2 I3,4 In1,n , group I2i1,2i contains four agents{4i 3, 4i 2, 4i 1, 4i}.create v Vi (for {1, . . . , n}) four objects xv , xv , yv , yv ,add n dummy objects dk (k {1, . . . , n});group I2i1,2i (i {1, . . . , n/2}), preferences agents are:4i3 = 4i2 = (02i1 d2i1 ) (02i d2i ),V4i1 = vV2i1 V2i xv xv ,V4i = vV2i1 V2i yv v ,0k formula k v replaced xv yv , vreplaced xv v .proof proposition primarily based fact problem split n/2subproblems concerning agents I2i1,2i :Lemma 14 denote P|i restriction P(1 , . . . , n ) set agents I2i1,2iobjects want. allocation said splittable 6= j, (I2i1,2i ) (I2j1,2j ) = .restriction splittable allocation (I2i1,2i ) written |i .exists envy-free allocation satisfying maximal number agents P(1 , . . . , n )exists splittable allocation {1, . . . , n/2}, |i envy-free satisfiesmaximal number agents P|i .Proof First, restrict attention regular allocations, regular means, like Lemma 2,allocation gives object agent wants it. safe reasonsLemma 2: existence envy-free allocation satisfying maximal number agentsequivalent existence regular envy-free allocation satisfying maximal numberagents. Since sets Vi pairwise-disjoint, two different subproblems P|i P|j shareobject, therefore regular allocation also splittable.0Let regular allocation. Suppose allocation |iproblem P|isatisfies agents |i . splittable allocation made sub-allocations |j0j 6= |ivalid, regular, satisfies agents . Conversely, supposeregular allocation 0 satisfies agents . least one0strictly agents I2i1,2i satisfied |i|i . proves regularallocation satisfies maximal number agents i, |i satisfies maximalnumber agents.Suppose envy-free. obviously |i well. Conversely, suppose|i envy-free. envy-free, (1) agent envy another agentgroup, |i envy-free, (2) agent group envyagent another group j, since (I2i1,2i ) (I2j1,2j ) = .interpretation Intk Vk , define following sets objects:f (Intk ) = {xv | Intk |= v} {xv | Intk 6|= v};g(Intk ) = {yv | Intk |= v} {yv | Intk 6|= v};f (Intk ) = {xv | Intk 6|= v} {xv | Intk |= v};547fiBouveret & Langg(Intk ) = {yv | Intk 6|= v} {yv | Intk |= v}.Moreover, given two interpretations Int2i1 Int2i respectively V2i1 V2i , writeInt2i1 ,Int2i following allocation P|i :Int2i1 ,Int2i (4i 3) = f (Int2i1 ) g(Int2i1 ) {d2i1 };Int2i1 ,Int2i (4i 2) = f (Int2i ) g(Int2i ) {d2i };Int2i1 ,Int2i (4i 1) = f (Int2i1 ) f (Int2i );Int2i1 ,Int2i (4i) = g(Int2i1 ) g(Int2i ).Lemma 15 Let Int2i1 Int2i two respective interpretations V2i1 V2i .Int2i1 ,Int2i satisfies agents 4i 1 4i;Int2i1 ,Int2i satisfies 4i 3 Int2i1 |= 2i1 , Int2i1 ,Int2i satisfies 4i 2Int2i |= 2i ;Proof Let Int2i1 Int2i two respective interpretations V2i1 V2i .definition, f (Intk ) contains xv xv v Vk , thus Int2i1 ,Int2i (4i1) contains xvxv v V2i1 V2i . Therefore agent 4i 1 satisfied Int2i1 ,Int2i (4i 1).reasoning holds agent 4i.definition, 2i1 satisfied Int2i1 02i1 satisfied interpretationdefined setting true xv yv (resp. xv yv ) Int2i1 |= v(resp. Int2i1 6|= v). Thus, Int2i1 |= 2i1 , Int2i1 ,Int2i satisfied 02i1 . Since alsosatisfies d2i1 , 4i 3 satisfied Int2i1 ,Int2i . Conversely, 4i 3 satisfiedInt2i1 ,Int2i , obviously 02i1 must satisfied Int2i1 ,Int2i (because 4i 3receive d2i ), proves 2i1 satisfied Int2i1 . result holds2i agent 4i 2.Lemma 16 Consider restricted problem P|i .neither 2i1 2i satisfiable, interpretations Int2i1 Int2i V2i1V2i respectively, Int2i1 ,Int2i envy-free satisfies maximal number agents.2i1 satisfiable, M2i1 model 2i1 , M2i1 ,Int2i satisfies maximalnumber agents. Moreover, envy-free allocation satisfying maximal numberagents case.2i1 2i satisfiable, M2i1 M2i respective models 2i12i , M2i1 ,M2i satisfies maximal number agents envy-free.Proof Suppose neither 2i1 2i satisfiable. allocation |i satisfying 4i 3(resp. 4i 2) must least one v V2i1 V2i {xv , xv , yv , yv }|i (4i 3) (resp. |i (4i 2)), otherwise one could deduce model 2i1 2i|i (4i 3) (resp. |i (4i 2)). Thus agents 4i 4i 1 satisfied case:maximal number agents possible satisfy 2. Since every allocation formInt2i1 ,Int2i satisfies agents 4i 1 4i, satisfies maximal number agentscase. also obviously envy-free, since neither d2i d2i1 shares agents 4i 14i, thus 2 agents cannot envy them.Suppose 2i1 satisfiable. allocation satisfying agents 4i 34i 2 must satisfy 02i1 one two agents, 02i one (because d2id2i1 ). Since 2i satisfiable, case neither 4i 1 4i satisfied |i ,reasons above. deduce possible satisfy 4agents. Neither possible satisfy 3 agents 4i 3 4i 2 satisfied. considerallocation M2i1 ,Int2i , M2i1 model 2i1 . Lemma 15, M2i1 ,Int2i satisfies3 agents: 4i 3, 4i 1 4i. allocation envy-free, allocation satisfyingmaximal number agents case (because either 4i 3 4i 2 remains unsatisfiedallocation, envying partner).Lastly, suppose 2i1 2i satisfiable, let M2i1 M2i models.Lemma 15, M2i1 ,M2i satisfies 4 agents, thus satisfying maximal number agentsobviously envy-free.548fiEfficiency Envy-freeness Fair Division Indivisible Goodsconclude proof. Lemma 14, exists envy-free allocation satisfyingmaximal number agents P(1 , . . . , n ) exists splittable allocation{1, . . . , n/2}, |i envy-free satisfies maximal number agents P|i . Lemma 15,exists envy-free allocation |i satisfying maximal number agents P|i eithernone two formulae 2i1 2i satisfiable, are. suppose maximumindex j j satisfiable odd number (say 2i 1). case, envyfree allocation satisfying maximal number agents P|i since 2i1 satisfiable 2i not.Conversely, suppose maximum index j j satisfiable even number (say 2i).case, envy-free allocation satisfying maximal number agents P|k , sinceP|k either two formulae 2k1 2k satisfiable (if k i), none (if k > i).thus reduction latter problem problem existence envy-freeallocation satisfying maximal number agents. proves p2 -completeness.5. Non-dichotomous Preferencesconsider case preferences longer dichotomous.5.1 General Logical PreferencesAgain, since explicit description preferences exponentially large, needcompact description thereof clear. Many languages exist succinct representationpreference. limit investigation following class languages:Definition 9 (Compact language logical form) Let L language representing set preference relations set alternatives 2X . L said compactlanguage logical form :(a) able express dichotomous preference compactly previous languageintroduced, is, language expressing dichotomous preferences propositionalform polynomially reduced L;(b) comparing two sets goods done polynomial time.two previous conditions practise restricting, met manylanguages succinct representation representation. See instance paper Lang(2004) survey logical languages compact preference representation. Noteseveral widely studied representation languages, CP-nets graphical languages, logical form, fail represent preferences expressedlogical formulas within polynomial space9 . Interestingly, Proposition 6 extendscompact representation language logical form:Corollary 2 eef existence monotonic compact preference logical form p2 complete.Proof eef existence problem solved using following algorithm:1. non-deterministically guess allocation ;2. check envy-free;3. check Pareto-efficient.9. natural question complexity fair division problems preferences expressedlanguages. left study.549fiBouveret & Langcondition (b), step two done polynomial time, since requires quadratic numberpolynomial oracles. condition (b) also, problem checking whether given allocationPareto-efficient co-NP. Therefore, previous non-deterministic algorithm uses 1 NP oracle,runs polynomial-time. Hence membership p2 .Hardness corollary Proposition 6 together condition (a).5.2 Numerical Preferences Logical Formlatter result preferences numerical since Pareto-efficiencyenvy-freeness purely ordinal notions. Now, preferences numerical, impliespossibility intercomparing aggregating preferences several agents, then, besidesPareto-efficiency, may consider efficiency based social welfare functions. considertwo classical way aggregating collection utility functionssocial welfare function:Definition 10 (Classical utilitarianism egalitarianism) Given collection individual utility functions hu1 , . . . , un i, i, ui : 2X Z:?P classical utilitarian social welfare function function defined sw : 7ui ((i));egalitarian social welfare function function defined sw(e) : 7 mini ui ((i));Maximizing egalitarian social welfare function often viewed alternative criterion fairness, encoding Rawlsian egalitarian point view (Rawls, 1971). However,see Proposition 17, egalitarianism (as well classical utilitarianism)always compatible envy-freeness. link two alternative pointsview fairness deeply investigated Brams King (2005).Since deal anymore ordinal (or dichotomous) preferences,define precisely mean compact representation numerical preferences.pick basic numerical language one simple compact languages,consisting associating numerical weights propositional formulae see e.g. (Chevaleyre,Endriss, & Lang, 2006), (Ieong & Shoham, 2005) context coalitional games:Definition 11 (Weighted propositional language) Given set goods X, weightedpropositional language associated X set possible subsets LX Z.Given set weighted propositional formulae = {h1 , w1 i, . . . , hr , wr i}, utilityfunction associated is:1 |= kXu : 2Z, k =Pr0 otherwise.7k=1 wk kUsing language, preferences monotonic formulae positive weightspositive.Now, define notion compact numerical language:Definition 12 (Compact numerical language logical form) Let L language representing set utility functions set alternatives 2X . L saidcompact numerical language logical form :550fiEfficiency Envy-freeness Fair Division Indivisible Goods(a) able express utility function compactly weighted propositionallanguage, is, weighted propositional language polynomially reducedL;(b) computing utility one set goods done polynomial time.course, since compact numerical language logical form also compactlanguage logical form, complexity result form Corollary 2 still holds. However,appears complexity problem deciding whether efficient envy-freeallocation exists decreases Pareto-efficiency replaced weaker notion:maximization one two latter social welfare functions.Proposition 17 Given collection utility functions 2R given compact numericallanguage logical form:problem deciding whether exists envy-free allocation amongmaximize utilitarian social welfare p2 -complete, even N = 2, evenagents identical preferences.problem deciding whether exists envy-free allocation amongmaximize egalitarian social welfare p2 -complete, even N = 2.Proof results, membership comes easily fact maximum value socialwelfare computed dichotomy set possible social welfare values;exponentially many, therefore need polynomial number NP oracles this; stepdone, suffices guess allocation check envy-free maximizes socialwelfare, adding one NP oracle.Hardness obtained utilitarian egalitarian cases simple reduction instancefair division problem preferences expressed weighted propositional languagefollowing problem:Problem 5: max-sat-asgeven (Wagner, 1987)INSTANCE: propositional formula Conjunctive Normal Form, set propositionalvariables V = {v1 , . . . , vn }, weight function w interpretations : V {0, 1},def Pi1defined w(I) =.I(vi ) 2QUESTION: maxM model w(M ) even number (in words, v1 falsified modelmaximal weight) ?suppose formula least model 6|= v1 .change complexity, v1 verified every model , clearly answer problemmax-sat-asgeven no: consequence, every instance h, V without assumptionsolved first checking v1 unsatisfiable (which coNP-complete problem), then,not, solving unsat-or-max-sat-asgeven problem instance least one modelfalsifying v1 .Utilitarian social welfare: instance h, V max-sat-asgeven , create followinginstance P(, V ):Agents:Objects:Preferences:2 agents;literal vi , create two objects xi x0i , except v1 ,one object x1 created, add two objects 0 ;agents 1 2 identical preferences, ask h( y) ( 00 ), 2n+1 i, hx1 y, 1i, . . . , hxn y, 2n1 i, hx02 0 , 2i, . . . , hx0n 0 , 2n1 i, formula symbol vi replaced xi , 0 formulasymbol vi (except v1 replaced x1 ) replaced x0i .551fiBouveret & LangLet (M1 , M2 ) pair models (with possibly M1 = M2 ) M2 6|= v1 . defineallocation M1 ,M2 by: M1 ,M2 (1) = {y} {xi |M1 |= vi } M1 ,M2 (2) = {y 0 } {x0i |M2 |= vi }.proof based following lemma:Lemma 17 exists envy-free allocation among maximize utilitarian social welfaretwo models M1 M2 (possibly M1 = M2 ) M2 6|= v1 , M1 ,M2envy-free maximizes utilitarian social welfare.Proof Let allocation maximizing utilitarian social welfare. Let modelfalsifying v1 (our hypothesis least one). F (M,M (1)) |=F (M,M (2)) |= 0 0 , proves individual utility two agents least 2n+1 .Hence least one allocation whose utilitarian social welfare greater equal2n+2 . Therefore, allocation maximizing utilitarian social welfare mustF ((1)) |= F ((2)) |= 0 0 , vice versa. Moreover, either x1 6 (1), x1 6 (2).Suppose x1 (2): swapping shares agents leads allocation 0completely equivalent respect Pareto-efficiency envy-freeness, due identicalpreferences. therefore assume w.l.o.g x1 6 (2)Since F ((1)) |= , model M1 (1) = {y} {xi |M1 |= vi } S1 ,S1 {x1 , x02 , . . . , x0n }. Similarly, model M2 M2 6|= v1 , (2) ={y 0 } {x0i |i > 1, M2 |= vi } S2 , S2 {x1 , . . . , xn }. consider allocation M1 ,M2 ,well-defined since M2 6|= v1 . u1 () = u1 (M1 ,M2 ), since x0i satisfyformula preferences agent 1 without 0 (given agent 2), u2 () = u2 (M1 ,M2 )reasons. terms, M1 ,M2 gives utility agents. M1 ,M2thus envy-free maximizes utilitarian social welfare.Lemma 17, thus restrict problem allocations form M1 ,M2 .have, M1 M2 defined earlier, u1 (M1 ,M2 ) = 2n+1 + w(M1 ) u2 (M1 ,M2 ) = 2n+1 +w(M2 ); thus sw? (M1 ,M2 ) = 2n+2 + w(M1 ) + w(M2 ). have: argmaxM ,M sw? (M1 ,M2 ) =12argmax(M ,M ) {w(M1 )+w(M2 )|M1 6|=v1 M2 6|=v1 } . Given symmetry problem, assume12M2 satisfy M2 6|= v1 , thus latter allocation becomes: Mopt ,argmaxM {w(M2 )|M2 6|=v1 } ,2Mopt model maximal weight.Suppose Mopt 6|= v1 , allocation maximizing utilitarian social welfare Mopt ,Moptclearly envy-free, agents utility. suppose Mopt |= v1 ,allocation maximizing utilitarian social welfare Mopt ,Mopt0 , Mopt0 modelmaximal weight assigns v1 false. w(Mopt0 ) < w(Mopt ), thus u1 (Mopt ,Mopt0 ) >u2 (Mopt ,Mopt0 ), hence allocation envy-free.latter reduction clearly polynomial (recall weights 2n+1 encoded usinglinear space). proves proposition utilitarian case.Egalitarian social welfare: instance h, V max-sat-asgeven , create followinginstance P(, V ):Agents:Objects:Preferences:2 agents;literal vi , create two objects xi x0i , add two objectsy0 ;preferences agent 1 hx1 , 1i, . . . , hxn , 2n1 i, h y, 2n i, preferencesagent 2 hy 0 , 22n i, hx1 , 1i, formula symbol vireplaced xi .Every allocation maximizes egalitarian social welfare must give least utility 22nagent 2. case, egalitarian social welfare given utility agent 1,utility cannot greater 22n . Therefore, maximizing social welfare comes maximizingutility agent 1, words give items corresponding model maximumweight. v1 set true model, x1 given agent 1, since also givenagent 1, agent 2 could get strictly higher utility agent 1s share. Therefore allocationenvy-free. v1 set false latter model, x1 given agent 1, thus givenagent 2, producing envy-free allocation. Since reduction polynomial, proves hardnessegalitarian case.552fiEfficiency Envy-freeness Fair Division Indivisible Goodsnotice combination envy-freeness numerical criterionclassical utilitarianism egalitarianism induces complexity gap, since, statedBouveret, Fargier, Lang, Lematre (2005), complexity problems maximizing classical utilitarian egalitarian collective utility functions, agentsweighted logical preferences, NP-complete.previous proof utilitarian case, notice hardness result stillholds require allocation Pareto-efficient instead maximizing utilitariansocial welfare. suggests case language extending weighted propositionalformulae, eef existence problem identical preferences much hardercase agents identical dichotomous preferences. actually followingresult:Proposition 18 Given collection N identical utility functions 2R given compactnumerical language logical form problem deciding whether Pareto-efficientenvy-free allocation exists p2 -complete, even N = 2, even preferencesmonotonic.Proof Since preferences identical, envy-free allocation satisfies agents equally. Thus,Pareto-efficient envy-free allocation, one, allocation gives everyone utility, maximal among set allocations satisfy everyone equally. value computedusing polynomial number NP oracles (like previous proof). value, checking Pareto-efficient envy-free allocation comes checkallocation giving least agents, least + 1 least one agent,problem coNP, hence adds one call NP oracle.hardness proof, one may notice reduction one used utilitariancase proof Proposition 17 works case, allocation Pareto-efficientenvy-free particular problem envy-free maximizes utilitarian socialwelfare.5.3 Additive Numerical Preferenceslast case consider case additive numerical preferences. Additivenumerical preferences degenerate case weighted logical preferences,formulae single positive literals. words, preferences agent givenset pairs hxk , wk i, xk object wk weight (possibly 0)associated object. utility function associated preferences thusfollowing:u : 2X ZP7xk wk .Notice agents preferences monotonic numbers wkpositive.preference representation language natural one dealingresource allocation problems; however, unable express compactly kind dependencies (superadditivity subadditivity) objects. particular,extend dichotomous preferences. Hence previous hardness results extendadditive preferences. However, since still able compare two alternativespolynomial time, membership p2 guaranteed.553fiBouveret & Langintuition problem hard eef existence problemdichotomous preferences:Conjecture 1 eef existence additive numerical preferences p2 -complete, evenpreferences monotonic.know problem NP-hard (this implied Proposition 20presented later) p2 , precise complexity remains open. However, things becomemuch easier require allocation complete, instead Pareto-efficient.case already investigated Lipton et al. (2004), following result:Proposition 19 (Lipton et al., 2004) problem deciding whether existscomplete envy-free allocation agents additive preferences NP-complete, evenpreferences monotonic.restrictions eef existence problem additive preferences worthstudied. First, study dichotomous case restriction identical additivepreferences:Proposition 20 eef existence N identical additive numerical preferences NPcomplete, fixed N 2. result holds require preferencesmonotonic.Proof Membership easy prove. Since preferences identical (we write hu(x1 ), . . . , u(xp )iutility vector associated set object), allocation Pareto-efficient givesobject xj u(xj ) > 0 one agent, trashes object xj u(xj ) 0.Moreover, allocation envy-free agents utility. two latterproperties checked polynomial-time, hence membership NP.Hardness comes reduction partition:Problem 6: partitionINSTANCE: finite set size s(a) NP S.PQUESTION: subset 0 s(a) = aS\S 0 s(a) ?given instance hS, si partition problem, create following instance P(S, s)eef existence problem:Agents:Objects:Preferences:2 agents;S, associate object xa ;two agents preferences identical defined size elementsinitial set: u(x(a)) = s(a).isPPareto-efficientP envy-free allocation P(S, s) allocationx(1) u(x) = x(2) u(x), is, partition problem returns true. reductionclearly done polynomial time, proves proposition.another interesting case case preferences necessary identical,atomic utilities ui (xj ) either 0 1. words, agent eitherwants object want it, agent wants maximize numberdesired objects gets.Proposition 21 eef existence additive 01-preferences (i.e. i, j, ui (xj ) {0, 1})NP-complete.554fiEfficiency Envy-freeness Fair Division Indivisible GoodsProof Pareto-efficiency easy check case. first safely remove objectsappear anywhere preferences. Afterwards, allocation Pareto-efficientobject xj given agent ui (xj ) = 1, following reasons. () LetPareto-efficient allocation, suppose xj given agent,given agent ui (xj ) = 0. Let k agent uk (xj ) = 1 (thereone since previously trashed undesired objects). giving xj agent k increasesks utility others utilities remains same. Thus Pareto-dominated. () Letallocation object xj givenPui (xj ) = 1,P agent iPP supposePareto-dominated allocation 0 . iI ui ( 0 (i)) = iI xj 0 (i) ui (xj ) > iI ui ((i)) =P PiIxj 0 (i) ui (xj ) = p. Therefore least one ui (xj ) ui (xj ) > 1,possible due restriction 01-preferences.thus give simple way check Pareto-efficiency, checking sum utilities equalnumber objects p desired least one agent. usual, envy-freeness verifiedpolynomial time; therefore eef existence additive 01-preferences NP.Hardness proved polynomial reduction exact cover 3-sets (problem 3).Given instance hS, C = hS1 , . . . , S|C| ii exact cover 3-sets, create following eef existence instance P(C, S) (we suppose elements written ai , {1, . . . , |S|}):Agents:Objects:Preferences:set 3|C| agents gathered triples {3i 2, 3i 1, 3i};set |S| + 3|C| items X =SM (M main, dummy),= {m1 , . . . , m|S| }, = i{1,...,|C|},j{1,2,3} {di,j };agents {3i 2, 3i 1, 3i} desire set objects ak Si {mk }{di,1 , di,2 , di,3 } (the three objects corresponding Si plus three dummy objectsdi,j ).exact cover C 0 instance hC, Si, consider following allocation:agent triple {3i 2, 3i 1, 3i} gets respectively di,1 , di,2 , di,3 , Si C 0 ,one three agents gets one three objects mk corresponding elements setSi . allocation admissible Pareto-efficient (because objects allocated). alsoenvy-free, following reasons:agents triple cannot envy other, equally satisfied.agent k1 cannot envy agent k2 another triple, objects k1 couldenvy k2 share mi . k2 one mi , k1 utility leastone, k1 cannot envy k2 .rest proof based following result: allocationPareto-efficientenvy-free P(C, S) must (3i 2) (3i 1) (3i) = ak Si {mk } {di,1 , di,2 , di,3 }(3i) (3i 1) (3i) = {di,1 , di,2 , di,3 }. easy show. Since agents triple{3i 2, 3i 1, 3i} ones desire objects di,k , three objects must giventhree agents, allocation efficient. Since three agents preferences,allocation satisfy equally order envy-free. Thus number objects allocatedthree agents must divisible 3, gives two possible numbers, 3 6, hencetwo possible allocations.Suppose Pareto-efficient envy-free allocation P(C, S). Consider subcollection C 0= hS1 , . . . , S|C 0 | made triples Si collection C (3i 2) (3i1) (3i) = ak Si {mk } {di,1 , di,2 , di,3 }. following results.Si pairwise disjoints. Suppose pair (i, j) 6= jelement ak belonging Si Sj . mk allocated two different agents: onemember triple {3i 2, 3i 1, 3i}, one member triple {3j + 1, 3j + 2, 3j + 3},impossible.i{1,...,|C 0 |} Si = S. Let ak element S. Since Pareto-efficient, mk mustallocated one agent wants (sayagent belongs triple {3j + 1, 3j + 2, 3j + 3}),unless one wants it, occurs i{1,...,|C 0 |} Si 6= S. Then, previous result,objects al Sj {ml } must allocated triple. Consequently, Sj C 0 . Since ak Sj ,ak belongs least one set collection C 0 .555fiBouveret & LangTherefore, C 0 exact cover S, finally proves proposition.see Proposition 21 Conjecture 1 huge complexity gapproblem allow weights freely given problemrequire weights 0 1 (at least conjecture true). natural questionraises know complexity fall specific 01-preferences, occursfixed upper bound weights.Conjecture 2 complexity eef existence problem additive 01. . . kpreferences k 2 fixed hard general problem unbounded additive preferences.precise complexity problem remains open problem, statedconjecture, intuition hard eef existence problemunbounded additive preferences.Another natural problem raised Proposition 21: complexityeef existence problem stratified 01-preferences ? stratified 01-preferences,mean preferences given set pairs hxk , pi, xk object ppriority level. Comparing two sets objects comes compare lexicographicallyvectors component index number objects priorityshare agent. Notice problem instance eef existenceproblem additive preferences, instance eef existence problemlogical numerical preferences logical form. However easy see remainsp2 , precise complexity remains unknown.Finally, investigate case number objects less numberagents. One could think intuitively problem trivial case. However,always case, see. begin with, following results showsproblem easy monotonic preferences:Proposition 22 Let P allocation problem n agents additive monotonicnumerical preferences wanting least one object, p objects desiredleast one agent.p < n, Pareto-efficient envy-free allocation.p = n, problem deciding whether exists efficient envy-free allocation agents monotonic additive preferences P.Proofobject desired least one agent, every Pareto-efficient allocation complete. number objects p strictly lower number agents N , least oneagent unsatisfied. Consequently, agent j obtains object wanted i, hencecreating envy. Thus Pareto-efficient allocation envy-free.Since many objects agents, agent receive one object values(that is, ui ({x}) maximal), allocation Pareto-efficientenvy-free. Indeed, agent receives object preferred one, meansanother agent j receives (otherwise allocation would Pareto-efficient), creating envyi. Therefore, checking existence Pareto-efficient envy-free allocation comescase checking possible give every agent one best-valued objects.comes checking perfect matching bipartite graph made onenode per agent one side, one node per object side, connecting556fiEfficiency Envy-freeness Fair Division Indivisible Goodsagent object x x among best-valued objects agent preferences.perfect matching computing polynomial time, hence result.Interestingly, latter result hold allow non-monotonic preferences.case, complexity increases complexity general eef existenceproblem additive preferences.Proposition 23 eef existence problem additive numerical preferencesnumber objects less number agents complexityeef existence problem additive numerical preferences, assumptionnumber objects.Proof Let us consider instance hI, X, h. . . , ui (xj ), . . . eef existence problem additive numerical preferences, N agents p objects (p > N ). create following instanceP(hI, X, h. . . , ui (xj ), . . . i):Agents:Objects:Preferences:p + 3 agents (the number agents important, greaternumber objects initial number agents);p initial objects xi plus two dummy ones d1 d2 ;preferences N first agents hI, X, h. . . , ui (xj ), . . . i;preferences (N +1)st agent uN +1 ({d1 }) = uN +1 ({d2 }) = 1 u{N +1} (xj ) =0 items xj ; preferences remaining agents uN +1 ({d1 }) =1, uN +1 ({d2 }) = 2 uN +1 (xj ) = 0 remaining objects.efficient envy free allocation hI, X, h. . . , ui (xj ), . . . easilychecked allocation gives items N first agents P(hI, X, h. . . , ui (xj ), . . . i),{d1 , d2 } (N + 1)st agent, nothing remaining ones efficient envy-free. Conversely,Pareto-efficient envy-free allocation P(hI, X, h. . . , ui (xj ), . . . i) yields Pareto-efficientenvy-free allocation hI, X, h. . . , ui (xj ), . . . restricting N first agents objectstwo dummy ones.6. Related Work Discussionalready argued Introduction, computational studies resource allocation eitherconcern divisible goods, focus classical utilitarianism combinatorial auctions.Existing work fair division indivisible goods, hand, mainly axiomatic,computational aspects neglected, except papersmentioning below.results course lot common complexity results combinatorialauctions. all, structure problems are, extent, similar: itemsindivisible, allocations preemptive10 , agent preferences sets itemsexpressed compact representation language. Logical bidding languages alsodesigned (Boutilier & Hoos, 2001). However, complexity results completely differ:standard decision problem combinatorial auctions NP-complete (Rothkopf, Pekec,& Harstad, 1998) decision problems considered typically locatedsecond level polynomial hierarchy, even degenerate case preferencesdichotomous. explained fact combinatorial auctions careefficiency, envy-freeness. Requiring criteria together (efficiency10. Here, preemptive means object cannot allocated one agent. assumptionabsent problems implying example virtual objects, software licences.557fiBouveret & Langenvy-freeness) makes things much difficult: while, usual assumptionpreferences monotonic, efficiency monotonic property (allocating goods nevermakes allocation less efficient), envy-freeness (allocating goods agentmay generate envy)11 . reason may exist EEF allocation,also source high complexity problem.Moreover, due failure monotonicity envy-freeness, searching EEFallocation cannot simply formulated maximization minimization simplecriterion, problematic designing local search algorithms approximationschemes. authors (Lipton et al., 2004; Chevaleyre, Endriss, Estivie, & Maudet,2007a; Brams, Jones, & Klamler, 2007) suggested relax envy-freeness criterionmake gradual notion, defining measure envy-freeness. Lipton et al. (2004)assume input consists numerical utility functions sets goods; degreeagent envies agent j allocation either defined envy differencedi,j () = max(0, ui ((j)) ui ((i))) envy ratio rij () = max 1, uuii((j))((i)) .cases, global degree envy maximum degree envy pairplayers. Alternative definitions degree envy society proposedChevaleyre et al. (2007a), number envious agents, number pairs (i, j)envies j, sum local degrees envy, relatively similarmeasure envy considered Brams et al. (2007), based maximum numberagents single agent may envy. Chevaleyre et al. (2007b) suggest radicallydifferent way relaxing envy-freeness: society comes along undirected graph(which reflects acquaintance agents), allocation envy-freeagent envies agent connected.Then, Lipton et al. (2004) focus search complete allocation minimumenvy; moreover, case additive utilities, provide approximation schemes. Another work approximation algorithms fair allocation indivisible goodsone Asadpour Saberi (2007), assume beginning utilities linearconsider problem finding maximally equitable allocation, is, allocation maximizing utility least satisfied agent (cf. problem consideredProposition 17); consider envy-freeness all12 .approaches considered far (including ours) assume allocationcomputed centralized way neutral, objective agent. contexts,centralized approach possible realistic, allocation obtaineddecentralized way, successive negotiations groups agents. approachinitiated Sandholm (1998), studies convergence properties allocationdepending structural restrictions made exchanges goods may occur.investigated Dunne (2005) Dunne et al. (2005), studycomputational complexity negotiation problems, Endriss, Maudet, Sadri,Toni (2006). approaches, optimality criterion classical utilitarian11. Note antimonotonic either: allocating less goods agent may generate envy well.12. Note alternative ways relaxing search EEF allocations exist. One may instancekeep envy-freeness hard requirement relax efficiency, allow relaxing lookallocation shows good trade-off efficiency envy-freeness (consideringproblem two-criteria optimization problem).558fiEfficiency Envy-freeness Fair Division Indivisible Goodssocial welfare. Envy-freeness considered distributed setting Chevaleyre et al.(2007a, 2007b).far, computational issues referred design study algorithmsrun computers find allocation, designing studyingprotocols query agents interactively gather enough information solutiondetermined. procedural issues, although extensively studied literaturefair division divisible goods (see below), well voting (Conitzer & Sandholm, 2005)rarely considered allocating indivisible goods, notable exceptionHerreiner Puppe (2002), study properties interactive protocolstwo agents enumerate preferred bundles one one, allocation found.mentioned Introduction, drawback protocols exponentiallylong, henceforth infeasible soon number objects units.Beyond works computational aspects fair division indivisible goods,much consider computational procedural aspects fair divisiondivisible goods (or, least, assume least one good divisible e.g., money).literature subject vast techniques quite far usedallocating indivisible goods (see particular literature cake-cutting algorithms, e.g.Brams & Taylor, 1996; Robertson & Webb, 1998) find relevant givedetailed bibliography subject. interested reader refer book Brams(2008) covers fair resource allocation indivisible divisible case.7. Conclusionstudied several computational aspects search efficient envy-free allocations fair division problems indivisible goods. results Section 3 showcase dichotomous preferences, search allocations reducedsearch preferred models prerequisite-free default logic. connectionrather unexpected, implies practical search EEF allocations doneusing existing algorithms default logic. However search likely timeconsuming, due complexity results: indeed, extensive study complexitydeciding whether efficient envy-free allocation exists, various restrictions (dichotomous preferences not, two agents more, agents identical preferencesnot, monotonic preferences not) various notions efficiency (Pareto-efficiency,completeness, maximum number satisfied agents, maximum social welfare classicalutilitarian egalitarian), seems show problem intrinsically difficult,since lies second level polynomial hierarchy, even preferences dichotomous monotonic. implies designing fast algorithms solving problemgeneral case reach. may first focus restrictionsproblem NP-complete. Unfortunately, restrictions (agents identicalpreferences; two agents; purely conjunctive, purely disjunctive 2-CNF preferences;search complete allocations without efficiency requirement; additive 0-1 preferences) compelling imply huge loss generality.complexity results introduced paper summed-up Figure 1 Table 2.Several issues research remain explored.559fiBouveret & Lang1212116?1p22213?p2151415p2p277p2p253coBH2coBH2411NP2116996?186?17181719NP88P102110P20O(1)O(1)? Proof included paper.1problem whose complexity proved paper (the mappingnumbers problems specified table 2).17problem whose complexity already known literature.22?problem whose complexity remains unknown.jintersection problems corresponding outgoing edges.Problem included problem j. Arcs obtained transitivityomitted.Figure 1: different problems complexity classes inclusion relations.560fiEfficiency Envy-freeness Fair Division Indivisible GoodsEfficiencynumber agentspreferencesmonotonicitycomp.yes (1) (1)p2 -c.identicalyesNP-c.identicalcoBH2 -c.yesNP-c.coBH2 -c.ident.yes (6) (6, 6)NP-c.yes (7) (7)p2 -c.disjunctionsconjunctionsconj.condition 1C st sat(C) Pclosedyes (8) (8)yes (9) (9)PNP-c.yes (10) (10)O(1)yes (11) (11)NP-c.numericalyes (12) (12)p2 -c.numericalp2 -c.numericalp2 -c.Dichotomous preferences1, 1Pareto-eff.fixed2Pareto-eff.3Pareto-eff.45Pareto-eff.Pareto-eff.6, 6, 6complete all.7, 7max nb ag.fixed (6, 6, 6)fixed N 2(6, 6)fixed8, 89, 9Pareto-eff.Pareto-eff.10, 10Pareto-eff.11, 11Pareto-eff.12, 12Pareto-eff.fixed13utilitarian sw14egalitarian sw15, 15Pareto-eff.1617, 17Pareto-eff.complete all.18, 18Pareto-eff.19202122Pareto-eff.Pareto-eff.Pareto-eff.Pareto-eff.fixed fixedN 2fixed fixedN 22 agents2 agentsNon-dichotomous preferencesfixed fixedN 2fixed fixedN 2fixed fixedN 2fixedfixedfixed fixedN 2> Nb objects= Nb objectsNb objectsnumerical,identicaladditiveadditiveyes (15) (15)p2 -c.yes (17) (17)p2 -c. ?NP-c.additive ident.yes (17) (17)NP-c.additive 01additiveadditiveadditiveyesyesyesNP-c.O(1)Pp2 -c. ?Table 2: set resource allocation problems studied paper. complexityclasses represented figure 1.561fiBouveret & LangFirst, knowing efficient envy-free allocation given problemhelpful practice allocation found anyway. solutionconsists defining functions return allocation cases, even envyfreeness efficiency cannot jointly met. way addressing issue consistsdefining suitable relaxations problem, as: (a) using measure envy insteadseeing envy-freeness strict criterion (as suggested Lipton et al., 2004; Chevaleyreet al., 2007b); (b) make envy-freeness relative notion, instance introducingacquaintance graph agents (Chevaleyre et al., 2007b); (c) keeping envy-freenessstrict criterion relaxing efficiency. cases, new problems arise, complexityidentified.Second, results mostly negative, since interesting problems studied NP-hard (and often even worse), therefore, would worth pursuing workdesign practical algorithms problems. likely would require comingappropriate optimization criteria either (a) giving polynomial algorithmsapproximate desired objective (Lipton et al., 2004) (b) implementingexperimenting local search algorithms relevant heuristics.Third, throughout paper assumed everyones preferences completelyknown. reality, presumably agents need report preferences, introducesissue strategic misreporting (manipulation). One direction future research wouldinvestigate prevent this, is, mechanism design aspects.Acknowledgmentsthank Michel Lematre stimulating discussions fair division compactrepresentation; Thibault Gajdos stimulating discussions envy-freenesspointing us relevant papers; Steven Brams, giving us feedbackearlier version paper pointing us relevant references; participantsAgentLink Technical Forum Group Multiagent Resource Allocation. workpartly supported project ANR-05-BLAN-0384 Preference HandlingAggregation Combinatorial Domains, funded Agence Nationale de la Recherche.ReferencesAsadpour, A., & Saberi, A. (2007). approximation algorithm max-min fair allocationindivisible goods. Tech. rep., Department Management Science Engineering,Stanford University, Stanford.Baral, C. (2003). Knowledge Representation, Reasoning Declarative Problem Solving.Cambridge University Press.Bogomolnaia, A., Moulin, H., & Stong, R. (2005). Collective choice dichotomouspreferences. Journal Economic Theory, 122, 165184.Boutilier, C., & Hoos, H. H. (2001). Bidding languages combinatorial auctions. Proc.17th International Joint Conference Artificial Intelligence (IJCAI-01), pp.12111217, Seattle, Washington, USA.562fiEfficiency Envy-freeness Fair Division Indivisible GoodsBouveret, S., Fargier, H., Lang, J., & Lematre, M. (2005). Allocation indivisible goods:general model complexity results. Dignum, F., Dignum, V., Koenig, S.,Kraus, S., Singh, M. P., & Wooldridge, M. (Eds.), Proceedings AAMAS05, Utrecht,Nederlands. ACM Press.Brams, S., & Fishburn, P. (1978). Approval voting. American Political Science Review,72 (3), 831857.Brams, S. J. (2008). Mathematics Democracy: Designing Better Voting FairDivision Procedures. Princeton University Press.Brams, S. J., Edelman, P. H., & Fishburn, P. C. (2003). Fair division indivisible items.Theory Decision, 55 (2), 147180.Brams, S. J., Jones, M. A., & Klamler, C. (2007). Divide-and-conquer: proportional,minimal-envy cake-cutting procedure. Tech. rep., NYU Department Politics.Brams, S. J., & King, D. L. (2005). Efficient fair division: Help worst avoid envy?.Rationality Society, 17 (4), 387421.Brams, S. J., & Taylor, A. (1996). Fair Division: Cake-Cutting Dispute Resolution.Cambridge Univ. Press.Chevaleyre, Y., Dunne, P. E., Endriss, U., Lang, J., Lematre, M., Maudet, N., Padget, J.,Phelps, S., Rodrguez-Aguilar, J. A., & Sousa, P. (2006). Issues multiagent resourceallocation. Informatica, 30, 331. Survey paper.Chevaleyre, Y., Endriss, U., Estivie, S., & Maudet, N. (2004). Multiagent resource allocation k-additive utility functions. Proc. DIMACS-LAMSADE WorkshopComputer Science Decision Theory, Vol. 3 Annales du LAMSADE, pp. 83100.Chevaleyre, Y., Endriss, U., Estivie, S., & Maudet, N. (2007a). Reaching envy-free statesdistributed negotiation settings. Veloso, M. (Ed.), Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI-2007), pp. 12391244.Chevaleyre, Y., Endriss, U., & Lang, J. (2006). Expressive power weighted propositionalformulas cardinal preference modelling. Proceedings 10th InternationalConference Principles Knowledge Representation Reasoning (KR), pp. 145152, Lake District, UK. AAAI Press.Chevaleyre, Y., Endriss, U., & Maudet, N. (2007b). Allocating goods graph eliminateenvy. Proceedings AAAI-07, pp. 700705.Conitzer, V., & Sandholm, T. (2005). Communication complexity common votiong rules.Proceedings EC-05.Cramton, P., Shoham, Y., & Steinberg, R. (Eds.). (2005). Combinatorial Auctions. MITPress.Demko, S., & Hill, T. P. (1998). Equitable distribution indivisible items. MathematicalSocial Sciences, 16, 145158.Dunne, P. E. (2005). Extremal behaviour multiagent contract negotiation. JournalArtificial Intelligence Research, 23, 4178.563fiBouveret & LangDunne, P. E., Wooldridge, M., & Laurence, M. (2005). complexity contract negotiation. Artificial Intelligence, 164 (1-2), 2346.Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2006). Negotiating socially optimal allocations resources. Journal Artificial Intelligence Research, 25, 315348.Ford, L. R., & Fulkerson, D. R. (1962). Flows Networks. Princeton University Press.Gebser, M., Liu, L., Namasivayam, G., Neumann, A., Schaub, T., & Truszczynski, M.(2007). first answer set programming system competition. Baral, C., Brewka,G., & Schlipf, J. (Eds.), Proceedings Ninth International Conference LogicProgramming Nonmonotonic Reasoning (LPNMR07), Vol. 4483 Lecture NotesArtificial Intelligence, pp. 317. Springer-Verlag.Gottlob, G. (1992). Complexity results nonmonotonic logics. Journal LogicComputation, 2, 397425.Herreiner, D. K., & Puppe, C. (2002). simple procedure finding equitable allocationsindivisible goods. Social Choice Welfare, 19, 415430.Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: compact representationscheme coalitional games. Proceedings EC05.Karp, R. M. (1972). Reducibility among combinatorial problems. Miller, R. E., &Watcher, J. W. (Eds.), Complexity Computer Computations, pp. 85103, New York.Plenum Press.Lang, J. (2004). Logical preference representation combinatorial vote. Annals Mathematics Artificial Intelligence, 42 (1), 3771.Lipton, R. J., Markakis, E., Mossel, E., & Saberi, A. (2004). approximately fair allocations indivisible goods. EC 04: Proceedings 5th ACM conferenceElectronic commerce, pp. 125131, New York, NY, USA. ACM Press.Nisan, N. (2005). Bidding Languages Combinatorial Auctions. MIT Press.Papadimitriou, C. H. (1994). Computational complexity. AddisonWesley.Rawls, J. (1971). Theory Justice. Harvard University Press, Cambridge, Mass.Reiter, R. (1980). logic default reasoning. Artificial Intelligence, 13, 81132.Robertson, J., & Webb, W. (1998). Cake-Cutting Algorithms. A.K. Peters.Rothkopf, M., Pekec, A., & Harstad, R. (1998). Computationally manageable combinationalauctions. Management Science, 8 (44), 11311147.Sandholm, T. (1998). Contract types satisficing task allocation: theoretical results.Proc. AAAI Spring Symposium: Satisficing Models.Wagner, K. W. (1987). complicated questions maxima minima,closures NP. Theoretical Computer Science, 51, 5380.Wagner, K. W. (1990). Bounded query classes. SIAM Journal Computing, 19 (5), 833846.Young, H. P. (1995). Equity Theory Practice. Princeton University Press.564fiJournal Artificial Intelligence Research 32 (2008) 1-36Submitted 05/07; published 5/08Enhancing Cooperative Search Concurrent InteractionsMANISTER @ BIU .013. NET. ILEfrat ManisterskiDepartment Computer Science,Bar-Ilan University, Ramat Gan, 52900 IsraelSARNED @ CS . BIU . AC . ILDavid SarneDepartment Computer Science,Bar-Ilan University, Ramat Gan, 52900 IsraelSARIT @ CS . BIU . AC . ILSarit KrausDepartment Computer Science,Bar-Ilan University, Ramat Gan, 52900 IsraelAbstractpaper show taking advantage autonomous agents capability maintainparallel interactions others, incorporating cooperative economic search modelresults new search strategy outperforms current strategies use. frameworkanalysis use electronic marketplace, buyer agents incentive searchcooperatively. new search technique quite intuitive, however analysis processextracting optimal search strategy associated several significant complexities.difficulties derived mainly unbounded search space simultaneous dual affectsdecisions taken along search. provide comprehensive analysis model, highlighting,demonstrating proving important characteristics optimal search strategy. Consequently,manage come efficient modular algorithm extracting optimal cooperativesearch strategy given environment. computational based comparative illustrationsystem performance using new search technique versus traditional methods given, emphasizing main differences optimal strategys structure advantage usingproposed model.1. IntroductionCoalition formation well recognized key process multi-agent systems, mostly desirableenvironments group agents perform task efficiently single agent(Lermann & Shehory, 2000). recent years many coalition formation models suggested,various domains (Talukdar, Baerentzen, Gove, & de Souza, 1998; Dias, 2004), particularlyelectronic commerce (Tsvetovat, Sycara, Chen, & Ying, 2000; Yamamoto & Sycara, 2001; Sarne& Kraus, 2005). latter context, common coalition coalition buyers, derivedmainly potential obtaining volume discounts (Tsvetovat et al.; Sarne & Kraus, 2003)ability search cooperatively market opportunities efficient manner (Sarne &Kraus, 2005).c2008AI Access Foundation. rights reserved.fiM ANISTERSKI , ARNE , & K RAUScooperative economic search1 incentive derives principally existence searchcosts found MAS. costs reflect resources (not necessarily monetary) needinvested consumed searching opportunities environment (Sarne & Kraus, 2005)(e.g., searching opportunity buy product context electronic marketplace).scenario search costs common MAS agent needs (for decisionmaking process) immediate information concerning market opportunities. Given richnessopportunities dynamic open nature environments, central mechanisms usually incapable supplying information level completeness accuracy requiredagent, certainly without cost. Thus agent needs spend resourcessearch related activities. Despite reduction magnitude search costs electronic commerce era, continuous growth number retailers virtual storesInternet, followed phenomenal increase number opportunities available, makesoverall search cost important parameter affecting buyers search strategy (Choi & Liu, 2000;Kephart & Greenwald, 2002; Sarne & Kraus, 2005; Bakos, 1997).context, cooperative search offers advantage sharing, reusing re-allocatingopportunities among coalition members (Sarne & Kraus, 2005). example, using cooperativesearch agents exploit opportunities would discarded otherwiseagents would conducted alternative separate search. Nevertheless, process formingmaintaining coalition induces overhead, derived mainly communicationcoordination activities (Sarne & Kraus, 2003), thus coalition set search strategycost/effective manner.classic example traditional markets procurement management officercorporation. Instead individual cooperation spend time resourceslocating specific requested supplies, task delegated procurement managementofficer. Here, addition price discounts obtained aggregated demands identical items,procurement management officer becomes highly updated different offers specificsupplies available different merchants markets. result cost locatingbest deal request becomes significantly smaller (in comparison equivalent searchconducted individuals).basic concepts coalition manage cooperative search, includinganalysis computational means extracting optimal search strategies, given SarneKraus (2005). Nevertheless, assumption used model constructing coalitionsstrategy coalition interacts one seller agent time. assumption ignoresinherent strength autonomous agents, capability efficiently interactseveral agents parallel. capability derives primarily improved communication capabilities ability process enormous amount information short timecompared people. paper, take advantage capability incorporatecooperative economic search model, supplying comprehensive analysis resulting parallelcooperative search variant. show throughout paper, parallel model weakly dominatesexisting sequential cooperative search model described Sarne Kraus: potentialsignificantly improving searchers performance various environments, always guarantees reaching least performance existing cooperative search model. particular,parallel interaction preferable whenever agents search cost non-linear combines fixed1. opposed classical AI search (Hart, Nilsson, & Raphael, 1968) agent seeks sequence actionsbring initial state desired goal state.2fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONScomponents (e.g. operational costs), depending number interactions maintained (e.g. advantage size). cases, adoption parallel technique coalition suggestsreduction average cost per interaction seller agents.Moreover, improvement achieved using parallel technique increases finite decision horizon (i.e., whenever coalition deadline finishing search). additionadvantage reducing average cost per interaction, finite horizon settings coalitionbenefits fact increase intensity search and, thus, scan opportunities (in comparison sequential search model described Sarne & Kraus, 2005) priorreaching deadline.integration parallel interactions technique single agents search processquite intuitive, finding optimal (overall utility maximization) strategy cooperative searchcase trivial all. major difficulty derives fact different coalition membersmay heterogeneous multi-attribute utility functions. Therefore, extracting value encapsulated future streams opportunities complex. overcome difficulty presentalgorithm extracting coalition strategy. algorithm facilitates calculation processcoalition strategy polynomial number parallel interactions. significant improvement brute-force algorithm exponential number parallelinteractions.Similar model introduced Sarne Kraus (2005), apply multi-attribute utilitytheory (MAUT) (Keeney & Raiffa, 1976) analyze preferences multiple attributes agentbased search mechanism. enables set preferences represented numerical utilityfunction. consider agents heterogeneous, i.e. utility function.model general, though emphasize several specific implementation aspects relating B2Cmarket (businesses selling products services end-user consumers), sellers supplyalmost demanded volume, C2C market (transactions consumers),sellers offer single items sale. Based proposed analysis, coalition calculateoptimal strategy given utility functions coalition members specific environmentoperates.Three basic stages common coalition formation models (Sandholm, Larson, Andersson, Shehory, & Tohme, 1999; Tsvetovat et al., 2000): coalition structure generation (whereagents form/join coalition), executing coalition task, dividing generated value amongcoalition members. Among three stages focus finding optimal search strategycoalition, given structure opportunity distribution. suggested SarneKraus (2005), coalition operates environment alongside many coalitions differingsize, members utility functions products seeking. coalitions,well different individual utility functions play important role studying stabilitycoalition issue revealing true utility function (truth telling). analysisimportant issues based ability properly derive coalitions utility given specificself structure environment within operates. 2 paper aims supply functionality, laying foundation enabling research many important aspects coalitionformation given context cooperative search (truth telling, stability, payoff division,etc.).2. utility considered agents reported, necessarily true, utility function, since goal extractoptimal strategy given input.3fiM ANISTERSKI , ARNE , & K RAUSmain contributions work fourfold: First, formally model analyzeparallel cooperative search problem agents operating costly environment. parallel cooperative search model general search model applied various domains additionelectronic marketplace used framework work. Second, showmany environments parallel cooperative search outperforms previous search strategies (eitheragent searches using cooperative sequential search). Furthermore,draw attention scenarios sequential cooperative search proven non-beneficial,however parallel cooperative search favorable technique. Third, supply algorithmfacilitates calculation coalitions optimal strategy, significantly reduces complexities associated attempt extract strategy appropriate set equations. Finallyprovide comprehensive analysis parallel model finite decision horizon. drawattention significant improvement achieved integrating parallel techniquecooperative search finite decision horizon.rest paper organized follows. Section 2 reviews related work, emphasizinguniqueness proposed parallel cooperative search model. model descriptionunderlying assumptions given section 3. section 4, formally describe performancecoalition using parallel cooperative search function strategy usedpresent complexities associated extracting optimal search strategy. exploringunique characteristics coalitions optimal strategy using cooperative parallel searchmanage overcome computational complexity. process described section 5. Consequently, present efficient algorithm extracting optimal cooperative search strategy.interesting properties new search model, regard market takesplace, illustrated section 6. section 7, finite decision horizon variant modeldiscussed. parallel cooperative search performance advantages current searchmodels, infinite finite decision horizons, illustrated section 8. Finally, concludediscussion suggested future research directions section 9.2. Related Workmany scenarios autonomous agents multi-agent environments may cooperate order perform tasks. need cooperation may arise either agent incapable completingtask operating group improve overall performance (Breban & Vassileva, 2001; Lermann & Shehory, 2000; Tsvetovat et al., 2000). Group based cooperative behaviorfound various domains, solving complex optimization problems (Talukdar et al.,1998), military rescue domains (Dias, 2004), e-business applications (Tsvetovat et al., 2000;Yamamoto & Sycara, 2001) many more. recognition advantages encapsulatedteamwork cooperative behaviors, main driving force many coalition formation modelsarea cooperative game theory MAS (Lermann & Shehory, 2000; Li, Rajan, Chawla, &Sycara, 2003; Shehory & Kraus, 1998). Many examples extensive literature coalition formation found books journals (Kahan & Rapoport, 1984). electronicmarket domain, authors focus coalitions formed obtain volume discounts Tsvetovatet al., Yamamoto Sycara. Additional coalition formation models electronic marketplaceconsider extensions transaction-oriented coalitions long-term ones Breban Vassileva,large-scale electronic markets Lermann Shehory.4fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONSTraditionally, majority research effort focused issues concerning optimaldivision agents disjoint exhaustive coalitions (Sandholm et al., 1999; Yamamoto & Sycara,2001), division coalition payoffs Yamamoto Sycara enforcement methods interactionprotocols. authors considered coalitions problem determining strategyelectronic commerce domain, coalition formed (Ito, Ochi, & Shintani, 2002).Nevertheless, single exception (Sarne & Kraus, 2005), none proposed modelsconsidered coalition search costly environment, particular none (includingSarne & Kraus, 2005) made use capabilities maintain parallel interactions.problem searcher operating costly environment, seeking maximize long termutility addressed classical search theory (Lippman & McCall, 1976; McMillan & Rothschild,1994, references therein). three main search models found literature.first search model Fixed Sample Size (FSS) model, introduced Stigler (1961).model searcher first chooses sample size draws single sample observations made simultaneously. second model Single Agent Sequential Search (SASS)strategy (Rothschild, 1974; Lippman McCall). model searcher draws exactly one observation time. Based value observations drawn till time, searcher decideswhether draw another observation. decision depends upon observed. Attemptsadopt sequential search model agent-based electronic trading environments associatedsearch costs suggested Choi Liu (2000), Kephart Greenwald (2002), thoughmain focus establishing appropriate characteristics environment search strategyrather computational aspects extracting it. last search method, Single Agent Parallel Search (SAPS) (Benhabib & Bull, 1983; Gal, Landsberger, & Levykson, 1981; Morgan, 1983;Morgan & Manning, 1985), encompasses search models special cases. modelsearcher may choose number samples taken sample size period.latter method, outperforms two, fact single agents equivalent parallelcooperative search model considered paper. Nevertheless, search theory mainly focusedsingle searcher, looking single opportunity, either one sided (taking environmentsreaction search strategy used agent static) two sided (as matching model,analyzed equilibrium perspective) model. analysis cooperative search lacking.This, in-spite fact cooperative search proven (Sarne & Kraus, 2005)inherently different single agents search relation complexity, strategy structuresolution methodology.Several extensions economic search theory suggested case consumersearching multiple different commodities, facing imperfect information prices (Gatti,1999; Carlson & McAfee, 1984; Burdett & Malueg, 1981). Here, find different variantsconsumer visits one stores order minimize total expenditure. Nevertheless, attempt adjust proposed methods suggested models support parallelcooperative search process results solution complexity exponential number parallel interactions. contrast algorithm extracting optimal search strategy polynomialnumber parallel interactions.3. Parallel Cooperative Search Modelbase model description formulation definitions given Sarne Kraus (2005)extend reflect agents parallel search capabilities. consider electronic mar5fiM ANISTERSKI , ARNE , & K RAUSketplace numerous buyer seller agents found. agent interested buyingoffering sell well defined product. product offered many different seller agentsvarious terms policies (including price). assume buyer agents ignorantindividual seller agents offers, acquainted overall distribution opportunities(where opportunity defined option buy product specific terms policies)marketplace.Assuming central mechanisms mediators supply agents fullimmediate information concerning current market opportunities, agent needs searchappropriate opportunities buy requested product. Throughout search buyer agentslocate seller agents learn offers interacting them. buyer agent evaluatesopportunities using multi-attribute utility function. Buyer agents may heterogeneouspreferences thus utility given opportunity differs according evaluating buyeragent.basic form, buyer agent interacts several sellers parallel stagesearch thus learns new set opportunities. Based agents evaluation utilitygained opportunity set, agent makes decision whether exploitopportunities encountered throughout search (i.e. buy sellers)resume search similar method. decision resume search always accompaniednumber parallel interactions conducted next.search activity assumed costly (Choi & Liu, 2000; Kephart & Greenwald, 2002;Sarne & Kraus, 2005; Bakos, 1997). search stage buyer agent locates, interactsevaluates seller agents, process induces search cost. cost function numberparallel interactions initiated maintained agent. search cost structure principallyparameter markets liquidity volatility, thus assumed shared buyeragents operating specific marketplace. Recognizing benefits cooperative search,buyer agents, interested similar products interchangeable products, may form coalitions (Sarne& Kraus, 2005). various methods coalition members coordinatecooperative search (e.g. assign representative agent search behalf coalition simplytake turns searching), deriving different search cost overhead structure. coalitions searchcosts assumed increase function number parallel interactions formsnumber buyer agents coalition. 3 assume buyer agents utility givenopportunity may interpreted monetary terms. Thus utilities additive totalsearch utility obtained subtracting search cost process inducesvalue.part search process, coalition needs set strategy determining, given setknown opportunities, whether terminate resume search. latter case, coalitionalso needs determine number parallel interactions used next search round.optimal strategy one maximizing expected total search utility (opportunities utilityminus search costs). discussed detail cooperative search model (Sarne & Kraus, 2005),given option side-payments overall utility maximization strategy taken coalitionalways preferred one coalition members (i.e. conflict interests), regardlesspre-set coalitions payoff division protocol. Given coalitions goal maximizing overall coalition utility, decision influenced payoff division protocol, coalition3. reason correlating coalitions search cost number coalition members mainly associatedcoordination overhead (Sarne & Kraus, 2005).6fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONSstability considerations, rather influences two factors (Sarne & Kraus, 2005).agents pre-determined portion coalitions utility increase absolute value alongincrease net coalition utility, thus overall utility maximization strategy preferredstrategy agents every stage search.4. Parallel Cooperative Search (PCS) Analysisfollowing section formally defines search environment coalitions search strategy.convenience, notations given, meanings, summarized table endpaper.Let B = (B1 , B2 , ..., Bk ) set attributes defining potentially availableopportunities market, attribute B assigned value finite set(bimin , ..., bimax ). opportunitys type defined vector ~o = (b1 , b2 , ...bk ), assigning valuebi specific attribute Bi .4 use p denote space potential opportunity typescoalition may encounter. opportunity types distribution marketplace denotedprobability function p(~o), ~oO p p(~o) = 1. consider coalition = {a 1 , a2 , ..., a|A| } generalsize, j j th buyer agent coalition. buyer agent, j , evaluates opportunitiesusing utility function U j : p R, U j (~o) agents utility opportunity type ~o.denote search cost associated coalition n agents maintaining w simultaneousinteractions seller agents search round function c(w, n).Let collection possible sets opportunities environmentagents reside. Given set known opportunities known coalition needs determinestrategy (whether terminate/resume search number parallel interactionslater case). Similar analysis suggested Sequential Cooperative Search (SCS) model(Sarne & Kraus, 2005) reduce large number world states coalitionbe, adopting representation states sets effective known opportunities.purpose consider function alloc : np maps given set opportunitiescoalition members (i.e. allocation) way aggregated agents utility usingallocation maximized.5 B2C markets opportunity may allocatedone agent, C2C markets opportunity restricted one agent. Given coalition/ represent allocation resulting applyingA, use alloc() = (~y1 , ...yn ), ~yi ( {0})function alloc set , ~y denotes opportunity associated agent (yi = 0/denotes opportunity allocated agent ).computation method used function alloc market-dependent. B2C marketsfunction assigns agent opportunity maximizes utility, ~y j = arg max~y U j (~y), j =1, ..., n, C2C markets alloc computed solving maximum weighted matchingbipartite graph (Avis & Lai, 1988). Specifically set opportunities found C2Cmarket construct graph G = (V1 ,V2 , E), vertex V1 corresponds agentvertex V2 corresponds opportunity ~o . edge connects agent j V1opportunity ~o V2 (each member two groups edges connecting members4. Notice ~o noted vector since assigns specific value different attributes, termsconditions associated specific opportunity. example, specific opportunity buy calculatorrepresented vector ~o = (scienti f ic, 20$, small display, pocket, 1Y R warranty).5. one allocation maximizes overall coalition utility function alloc chooses oneaccording pre-defined order.7fiM ANISTERSKI , ARNE , & K RAUSgroup). weight edge utility agent j opportunity ~o, U j (~o).alloc() = (~y1 , .., y~n ), {(a1 , y~1 ), .., (an , y~n )} maximum weighted matching G .illustrate computation used function alloc, use following example:Example 1. Consider following environment:Environment 1. three agents, 1 , a2 a3 , searching product (e.g., memory chip)characterized two attributes, B 1 (e.g., quality) B2 (e.g., store rating). attributeeither value 1 2, equal probability 1/2. means four possibleopportunities ~o1 = (1, 1) (both attributes values equal 1), ~o 2 = (1, 2), ~o3 = (2, 1), ~o4 =(2, 2). utility function agents 1 , a2 a3 U1 (~o) = 9B1 + B2 , U2 (~o) = 4B1 + 5B2 ,U3 (~o) = B1 + 10B2 , respectively. Table 1 summarizes environments setting.Opportunity~o1~o2~o3~o4(Attribute1,Attribute2)(1,1)(1,2)(2,1)(2,2)ProbabilityAgent a11011192014141414UtilityAgent a29141318Agent a311211222Table 1: Agents utilities four opportunities Environment 1Assume coalition already interacted 4 sellers, encountering two opportunitiestype ~o1 two single opportunities types ~o 3 ~o4 . Here, set known opportunitiesknown = {~o1 ,~o1 ,~o3 ,~o4 }.first calculate alloc(known ) coalition operating B2C market. marketassume sellers supply demanded volume. Therefore allocation maximizescoalitions overall utility assigning agent opportunity maximizes utilityknown . Since opportunity ~o4 maximizes utility agents, obtain alloc( known ) =(~o4 ,~o4 ,~o4 ).C2C markets allocation impossible, since opportunity ~o 4 assignedone agents (each seller offers single item sale). case optimal allocationcalculated solving maximum matching problem resulting assignment alloc( known ) =(~o3 ,~o1 ,~o4 ).Given set known opportunities known , use function alloc consequentallocation alloc(known ) calculate immediate utility coalition terminates searchcurrent time point. utility, defined aggregated agents utilityagents allocated according allocation alloc( known ), denoted Vt (known ) (abbreviationVterminate ) calculated using:nVt (known ) =U j (~y j )j=1/ = 0, j.alloc(known ) = (~y1 , ...~yn ) U j (0)8(1)fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONSNotice point, world state space upon coalition defines strategydefined set opportunities known known coalition. space potentiallylarge. order reduce strategys space, introduce concept equivalencedifferent sets opportunities within context cooperative search. consider two setsopportunities 0 , 00 equivalent (0 00 ), following hold: (a) Vt (00 ) = Vt (0 );(b) Vt (0 ) = Vt (00 ) set opportunities coalition may encounterfuture. two equivalent sets 0 , 00 , coalition indifferent knowing opportunities0 opportunities 00 . set opportunities coalition encounterfuture results similar utility, thus coalitions overall utilitycases. Moreover, since coalitions decisions merely determined overall utilitycases similar utilities reached similar probabilities, coalition usessearch strategy (either terminates search uses number parallel interactionssubsequent search stage) opportunities sets.Notice according definition equivalent transitive relation ( 0 00 , 000000 000 ). Moreover, 0 00 implies (0 ) (00 ), . Given allocation` = (~y1 , ..., y~n ) set , use {`} denote set opportunities appear `.Similar former cooperative search models (Sarne & Kraus, 2005; Manisterski, 2007)following theorem holds cooperative search model (and proven similar manner).Theorem 1. set opportunities equivalent set opportunities returnedfunction alloc(). Formally stated: {alloc()}.Theorem 1 enables us reduce set known opportunities affect coalitions strategy.immediate implication Theorem 1 coalitions strategy affectedsubset known defined {alloc(known )}. Thus coalition need keep knownopportunities. reduce set known opportunities determines strategysubset, s. Given result, define state set opportunities members{alloc(known )}. Formally, calculate state coalition acquainted setknown known opportunities using function = state( known ) = {alloc(known )}. useSA denote set possible states coalition A. definition significantly simplifiesanalysis enables coalition calculate optimal strategy exclusively based setsopportunities SA . following example illustrates computation state.Example 2. Consider environment set known opportunities described Example1. computed Example 1, allocation maximizes coalitions overall utilityB2C market alloc(known ) = (~o4 ,~o4 ,~o4 ). Thus coalitions current state B2C marketstate(known ) = {~o4 }. result coalition ignore opportunities encountered{~o1 ,~o1 ,~o3 } strategy (terminate resume search number parallel interactionslater case) strategy known = {~o4 }. Similarly, coalitions stateC2C market includes opportunities alloc( known ) thus state(known ) = {~o3 ,~o1 ,~o4 }.coalitions transition one state another search B2C C2Cmarkets described directed acyclic graph (DAG). vertices graph presentpotential coalition states. directed edge (s, 0 ) connects two states, s0 ,opportunity ~o p changes current coalitions state 0 (i.e ~o, s.t state(s ~o) = s0 ).better understand use DAG markets, use following two environments:9fiM ANISTERSKI , ARNE , & K RAUSEnvironment 2. two agents 1 a2 searching product (e.g., computer mouse)B2C market associated 3 types opportunities (e.g., 3 models): o~ 1 , o~2 o~3 . Table 2summarizes environments setting.Opportunity~o1~o2~o3UtilityAgent a1 Agent a25101052021Table 2: Agents utilities three opportunities Environment 2Environment 3. two agents 1 a2 searching product (e.g., used book)C2C market associated two types opportunities (e.g., English edition American edition):o~1 o~2 . Table 3 summarizes environments setting.Opportunity~o1~o2UtilityAgent a1 Agent a2510105Table 3: Agents utilities two opportunities Environment 3Figure 1(a) Figure 1(b) show DAG states Environment 2 (the B2C market)Environment 3 (the C2C market) described below, respectively. Notice possibleopportunities change coalitions current state (since opportunities increase coalitions overall utility). simplify graph mark opportunities.notable C2C market sets include n opportunities feasible states (statefeasible belongs SA , i.e. set state() = s). holdB2C market. example, set {~o 1 ,~o3 } feasible state since ~o3 maximizes agentsutility. Thus coalition encounters opportunity opportunities ignored.coalition reaches state along search change state 0sequence directed edges state state 0 . coalition conduct parallel interactions,thus transition within single search round state 0 directly connectedstate s. example, Environment 3, state = {} coalition change states0 = {~o1 ,~o2 } one search round (even though directed edge them).happen coalition conducts two parallel interactions encountersopportunities ~o1 ~o2 . transition new state suggests new state coalitiontermination utility equal higher utility current state.define strategy function x : N, x(s) = 0 agent decides terminatesearch; otherwise x(s) number parallel interactions coalition maintain next,state s. denote optimal strategy x .define V (s, w) coalitions expected utility using w parallel interactionsstate (assuming search decision taken future state 0 6= make use optimalnumber parallel interactions). term V (s, 0) denotes immediate utility obtained,10fiE NHANCING C OOPERATIVE EARCH{o1}o1{o1,o2}o3{}o1o2o2o3C ONCURRENT NTERACTIONSo3{}o1o2{o1}{o2}{o2}o1o3o1o2{o1,o1}o2{o3}(a)o2{o2,o2}{o1,o2}o1(b)Figure 1: States Diagram (a) Environment 2 (B2C market); (b) Environment 3 (C2C market).coalition decides terminate search state s, thus: V (s, 0) = Vt (s). value w (w N,w 0) maximizes coalitions expected utility V (s, w), equal x (s):x (s) = arg max V (s, w)w(2)coalitions expected utility point onwards using optimal strategy, denotedV (s), expressed as:V (s) = maxwV (s, w) = V (s, x (s))(3)order formulate appropriate equation V (s, w) (from x (s) V (s)derived) make use several additional notations definitions. Consider search roundcoalition interacts simultaneously w seller agents, yielding set w = {~o1 , ..., o~w },~oi p opportunities. Let w collection w-sized sets opportunitiesproduced environment coalition operates. denote p w (w ) probabilityencountering specific set opportunities w , maintaining w random interactionsseller agents.Similar basic cooperative search (Sarne & Kraus, 2005) divide w-sized opportunities space, w , two sub-spaces, containing improving non-improving w-sized sets opportunities coalitions utility, respectively. Nevertheless, definition needs extendedfit scenario parallel search follows. Given number simultaneous interactions,w, state s, let simprovew collection w-size sets opportunities, w , changecoalitions current state (formally stated as: simprovew = {w |w w state(s w ) 6= s}).denote complementary set simprovew sstayw (the set includes w-size setsopportunities w change coalitions current state).Therefore coalition encounters set opportunities w , distinguish twopossible scenarios:(1) w belongs sstayw coalitions current state still s. case coalitions futureexpected utility (i.e., point on) remains V (s, w). derived stationary nature 66. Stationary strategy strategy player chooses moves every structurally equivalent subgame(Baron & Ferejohn, 1989).11fiM ANISTERSKI , ARNE , & K RAUSproblem - better state reached, search resumes using numberparallel interactions w, yielding expected utility.(2) w belongs simprovew coalitions current state changes 0 = state(s w ) 6= s. Sinceassume coalitions decision taken future state 0 6= make use coalitionsoptimal strategy x (s0 ), coalitions expected utility expressed V (state(s w )).using analysis, expected utility using w parallel interactionsstate s, V (s, w), expressed (w > 0):V (s, w) =c(w, n)+pw (w )Vw simprovew(s0 )+pw (w )V (s, w)w sstayw(4)s0 coalitions new state encounters set opportunities w , s0 = state(s w ).applying basic mathematical manipulations term obtain:V (s, w) =c(w, n) + w simprove pw (w )V (s0 )w1 w sstayw pw (w )(5)Since 1 w sstayw pw (w ) = w simprove pw (w ) obtain:wV (s, w) =c(w, n) + w simprove pw (w )V (s0 )ww simprovew pw (w )(6)Notice case new better state reached, denominator becomes zero,V (s, w) = . quite straightforward since coalition maintains endless costly search(trying reach better state actually exist). Here, inevitably coalitions optimalstrategy terminate search. important characteristic used later design proposedalgorithms extracting optimal search strategy.point, one may attempt compute coalitions strategy x solving set equationstypes 1, 3 6. Nevertheless, straightforward solution approach accompanied manyinherent complexities, derived structure equations. First, notice Equation 6recursive equation one needs know optimal strategy taken future states 0extracting optimal strategy given state s. Second, computation V (s, w) Equation6 exponential number parallel interactions, w, used (affecting number setssimprovew , denominator numerator). Last, according formulation,potential number parallel interactions may used bounded 7 , thus reaching localmaximum guarantee higher utility cannot obtained. Therefore, algorithmicapproach reduce complexity extracting optimal cooperative parallel searchfavorable.5. Algorithmic Approachsection present comprehensive analysis problem, emphasizing uniquecharacteristics coalitions optimal strategy. findings lead algorithm computingV (s, w) polynomial complexity number potential interactions, w (which keycomponent computing x (s) V (s)).7. number opportunities theoretically infinite due high arrival rate new opportunities deriveddynamic environment.12fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONS5.1 Reducing Calculation ComplexityRecall attempting solve problem set equations (see section 4) potentialnumber parallel interactions may used unbounded. Nevertheless, order extractx (s) essential supply coalition upper bound, w smax , optimal numberparallel interactions used state s. order overcome difficulty suggestupper bound wsmax make use following notation. use = (s1 , ...s|SA | ) denotestates constituting SA , sorted termination utilities Vt (s)8 , s1 statehighest expected utility Vt (s) SA .following proposition suggests efficient upper bound x (s).Proposition 1. state si upper bound wsmax, x (si ) calculated using wsmax= dwe,w solution following equation:(7)c(w, n) = Vt (s1 ) Vt (si )suggested bound valid simply every value w greater w smaxsearch costassociated following immediate search round c(w, n) greater possible futureimprovement coalitions utility Vt (s1 ) Vt (si ) (since maximum additional utilitycoalition gain bounded difference coalitions overall maximum utilityVt (s1 ) immediate utility current state Vt (si )). Later on, show upperbound value byproduct main loop proposed algorithm, thus even needdirectly calculated.upper bound x (s) important step towards solution, however calculationV (s, w) (from x (s) derived) still exponential number parallel interactionsused, w. analysis, based restructuring different elements composingV (s, w), allows us bypass complexity introduction finite algorithmpolynomial computational complexity w inevitably identify optimal strategycoalition.order reduce complexity computing coalitions best strategy, make usefollowing notations definitions:use pstay (s, w) denote probability coalition remain stateconducting w parallel interactions. calculated probability noneencountered w opportunities changing coalitions state:pstay (s, w) = (pstay (s, 1)) = (wp(~o))w(8){~o}sstay1term 1(pstay (s, 1))w used better structured representation elementw simprovew pw (w ) Equation 6.use V new (s, k) denote coalitions expected utility obtained potentially reachingnew states (e.g. different s) maintaining k parallel interactions, usingoptimal strategy x (s0 ) new future state s0 . term V new (s, k) takeaccount cost associated current k interactions. However consider8. several states equal utility sorted according pre-defined order.13fiM ANISTERSKI , ARNE , & K RAUSsearch costs associated search stages, originating new states. NoticeV new (s, k) equal zero coalition remains state k interactions. termV new (s, k) expressed as:()(state(s )) k > 0p()Vkk improve k kkV new (s, k) =.(9)0k=0Note V new (s, w) actually one elements Equation 6. Therefore, Equation 6formulated (w > 0):V (s, w) =c(w, n) +V new (s, w)1 pstay (s, 1)w(10)calculation V new (s, w) using Equation 9 still exponential number parallelinteractions. order efficiently compute V new (s, w) Equation 10 consider w simultaneous interactions w sequential interactions, associated search costs. fully compliesdefinition V new (s, w) given (as cost w interactions already considered). justification representation method given Lemma 1 followsdirectly Theorem 1.Lemma 1. new state reached obtaining new set opportunities equivalent statereached sequentially obtaining pairwise disjoint subsets set. Formally stated, given set/ 6= j, 1w1 ... rwr = ww number subsets 1w1 , ..., rwr , iwi w , iwi wj j = 0,state s, following holds:rstate(s, w ) state(state(...state(state(s 1w1 ) 2w2 )... r1wr1 ) wr )(11)Proof. begin proving following supporting lemma:Lemma 2. Let 0 , 00 two sets opportunities. 0 00 , state(0 )state(00 ).Proof. Since 0 00 , definition equivalence relation follows 0 00. Theorem 1 state definition follows 0 {alloc(0 )} = state(0 )00 {alloc(00 )} = state(00 ). transitive characteristic equivalencerelation follows state(0 ) state(00 ).Let state let w set opportunities. prove Lemma 1 inductionnumber disjoint sets r. r = 2, let 0w0 00w00 sets satisfy 0w0 00w00 = w/ Theorem 1 state definition obtain:0w0 00w00 = 0.0w0 {alloc(s 0w0 )} = state(s 0w0 )(12)12 Lemma 2 followsstate(s w ) = state(s 0w0 00w00 ) state(state(s, 0w0 ) 00w00 )(13)assume number disjoint sets r r > 2, 1w1 , ..., rwr , iwi w , iwi wj j =/ 6= j, 1w1 ... rwr = w Equation 11 holds prove Equation 11 holds r=M+1.0,14fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONSGiven w sets 1w1 , ..., rwr , decompose w two disjoint sets wwr rwr ,wwr = w \ rwr . Therefore proved r = 2 following holds:state(s w ) state(state(s, wwr ) rwr ).(14)addition decompose set wwr r 1 sets 1w1 , ..., r1wr1 . Therefore usinginduction assumption:state(s wwr ) state(...state(state(s, 1w1 ) 2w2 )... r1wr1 )(15)Lemma 2 15 obtain:rstate(state(s wwr ) rwr ) state(state(...state(state(s 1w1 ) 2w2 )... r1wr1 ) wr )(16)14, 16 transitive characteristic equivalence relation follows that:rstate(s w ) state(state(...state(state(s 1w1 ) 2w2 )... r1wr1 ) wr )specific case Lemma 1 subset consists single opportunity. Thuscalculation V new (s, k) recursively rely values V new (s0 , k 1), s0 representsnew states reached obtaining one additional opportunity p . Thereforeorder compute V new (s, k), merely consider expected utility conducting one interactionplanned k interactions. Here, probability p stay (s, 1) coalition remainsstate s, expected utility (having k 1 additional interactions go) V new (s, k 1)(recall V new (s, 0) = 0 according Equation 9). Otherwise, new state 0 reachedsingle interaction, possibility reaching new states remainingk 1 interactions (taking states utility consideration term V new (s0 , k 1))probability pstay (s0 , k 1) remaining new state s0 even additional k 1 interactions(in case utility one obtained resuming search state usingoptimal strategy, V (s0 )). description encapsulated following recursive equation:Vnew(s, k) =(pstay (s, 1)V new (s, k 1) + {~o}simprove p(~o)(V new (s0 , k 1) + pstay(s0 , k 1)V (s0 )) k > 010k=0(17)s0 = state(s ~o ). Notice repeating calculation using equationincreasing k value starting k = 1, iteration includes single unknown parameter,(V new (s, k)). addition, values V new (s0 , k 1) V (s0 ) depend valuescomputed state (s0 precedes set Ss ).5.2 Algorithm Computing Coalitions Optimal Strategyanalysis leads algorithm computing coalitions optimal strategy (Algorithm1). algorithm computes coalitions strategy expected utility possible statesinitSA . execution time polynomial w initmax (for convenience use wmax denote boundnumber parallel interactions w, coalition begins search, i.e., coalitions state= sinit = {}).Notice stage execution, algorithm 1 reuses components computed earlierstages. example, V new (s, w) appears computation V (s, w) (using Equation 10),computation V new (s0 , w + 1) (using Equation 17, ~o0 p state(s0 ~o0 ) = s)15)fiM ANISTERSKI , ARNE , & K RAUSAlgorithm 1 algorithm computing optimal search strategy xInput: p - set potential opportunity types market; p(~o) - opportunity types probabilityfunction; n - coalitions size; U j (), j = 1, ..., n - coalition members utility functions; c(w, n) search cost function; SA - set possible states coalition A.Output: x (s) SA - coalitions optimal strategy.1: Build set ordered states2: i=1 |Ss |3:Set V (si , 0) = Vt (si ) using Equation 14:Set V new (si , 0) = 05:Set w = 16:c(w, n) (Vt (s1 ) Vt (sinit ))7:Compute V new (si , w) using Equation 178:Compute V (si , w) using Equation 109:w++;10:end11:Set x (si ) = arg maxw0 (0,...,w1) V (si , w0 ), V (si ) = V (si , x (si ))12: end13: Return (x (si ), = 1, ..., s|SA | )computation V new (s00 , w + 1) (using Equation 17, ~o00 p state(s00~o00 ) = s). Storing result computational element memory, purposereusing later stages, significantly improves efficiency algorithm. accomplishedusing two matrixes V V new size |SA | (winitmax + 1), corresponding V (s, w)V new (s, w) values stored pair (s, w), representing state correlated resultnumber parallel interactions used calculations. Additionally, store x Vvalues using two arrays size |S | reusing x (s0 ) V (s) computation V new (s, w).Theorem 2. Algorithm 1 returns optimal strategy coalition polynomial time w initmax .Proof. proof, assume use matrices V V new storing values computed along execution algorithm described above. order build set orderedstates Ss , algorithm needs compute coalitions termination utility statesSA , according equation 1. Computing coalitions termination utility given statetakes O(|A|3 ) C2C markets9 O(|A|2 ) B2C markets10 . coalitions termination utility calculated state . Thus overall complexity computing coalitionstermination utility states O(|S ||A|2 ) O(|SA ||A|3 ) B2C C2C markets, respectively (the sets Ss SA size). Sorting members order build Ss takesO(|SA | log |SA |). computation step 3 takes constant time assuming stored9. mentioned previously, computing coalitions termination utility C2C markets equivalent findingmaximum matching weighted bipartite graph. Finding maximum matching weighted bipartite graphdone O(n(n log n + m)), n number vertices number edges (Wang, Makedon, & Ford,2004). Since graph agent connected opportunities, number opportunities givenstate bounded number agents, process computing coalitions termination utility takes O(|A|3 ).10. mentioned previously, computing coalitions termination utility B2C markets done findingopportunity agent coalition maximizes utility. number opportunities given statebounded number agents, computing coalitions termination utility takes O(|A|2 ).16fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONSstate coalitions termination utility built. Steps 3-5 performed states,thus, overall complexity O(|S |). loop step 6 performed w initmax times.initreaching value w = w max + 1 step 6, loop condition longer holds (i.e.,c(w, n) > (Vt (s1 ) Vt (sinit ))). Notice first elements calculated state 1 (i.e. onemaximum utility) according loop step 2. Here, explained section 4, expected utility strategy search resumed (i.e., using w 1) V (s, w) =1(formally, since simprove= 0/ w pstay (s1 , w) = 1 V new (s1 , w) = 0 w (1, .., winitmax ), thus,winitV (s1 , w) = w (1, .., wmax )). Therefore, optimal strategy state 1 terminatesearch, i.e. x (s1 ) = 0 V (s1 , 0) = Vt (s1 ). state, s, reaching step 7,algorithm already computed V (s0 ) V new (s0 , w) w (0, ..winitmax ) potential future new state s0 originating state s. due fact future states stateappear set ordered states (either higher utility, equal utilityyet sorted according function alloc). addition, number parallel interactions w 1, algorithm already computed V new (s, w 1). Therefore, computationtime step 7 sums order |O p | components already computed. Then,reaching step 8, value V new (which part numerator equation 10)already computed. Therefore, ignore, now, time computing coalitions newstate s0 encounters opportunity ~o given state time computing p stayvalues, computation step 7 takes O(|O p |) computation step 8 takes constanttime. Since steps performed w initmax |SA | times, overall complexity computing steps O(|O p |winit|S|).step11,algorithm chooses maximum value amongmaxwinitvaluesalreadycomputed.Sincestep performed state ,maxoverall complexity performing step O(w initmax |SA |). complexity computingcoalitions new state s0 encounters opportunity ~o given current state dependsmarket type. B2C markets, complexity O(|A|) store state alsoalloc(s) value (we compute value compute coalitions termination utility).C2C markets, computing new state takes O(|A| 3 )11 . store values using matrix f uture size |SA | |O p | new state s0 = state(s ~o) stored pair (s,~o).Therefore, overall complexity computing future states states O(|S ||A||O p |) B2Cmarkets, O(|SA ||A|3 |O p |) C2C markets. Computing pstay (s, w) done constanttime based pstay (s, w 1). Moreover, store values using matrix Pstay sizestay (s, w) stored pair (s, w). Therefore, total com|SAg | winitmax value pplexity computing pstay values O(|SA | winitmax ). Given analysis, overall complexityalgorithm O(|SA ||A|2 + |SA | log |SA | + |SA ||O p |winitmax + |SA ||A||O p |) B2C markets,O(|SA | log |SA | + |SA ||A|3 |O p | + |SA ||O p |winit)C2Cmarkets.Hence algorithm polynomialmaxinitwmax .Note algorithm uses winitmax upper bound optimal number interactions x (si ),= 1, ..., |Ss | . bound valid since according Equation 7 value w smaxincreasesVt (si ) decreases coalitions termination utility reaches minimum initial state(Vt (sinit ) = Vt ({}) = 0). suggests significant improvement algorithm.siInstead using winitmax states, calculate specific upper bound, w max , state sistep 6, according Equation 7. point may require re-computation V (s , w) w11. Computing coalitions new state C2C markets done finding maximum matching weighted bipartitegraph (see footnote 10 complexity analysis).17fiM ANISTERSKI , ARNE , & K RAUSvalues used previous execution stages algorithm; however, total numbercalculations state si many cases significantly decrease 12 .6. Properties Parallel Cooperative Search (PCS) ModelPrior extending analysis scenarios coalition faces finite decision horizon,emphasize interested unique characteristics parallel cooperative search derivedgeneral analysis. First wish emphasize PCS model generalizationSingle Agent Parallel Search (SAPS) Sequential Cooperative Search (SCS) model statedfollowing proposition.Proposition 2. cooperative parallel search generalization single agent parallelsearch cooperative sequential search models 13 .Proof. analysis given section 4 clear Algorithm 1 results strategyused cooperative sequential search single agents parallel search, specificcases si wsmax= 1 parallel interactions n = 1 agents, respectively. 14Furthermore, emphasize coalitions expected utility never decreases usingproposed mechanism comparison sequential cooperative search. Indeed casemaintaining single interaction favorable strategy Algorithm 1 results oneinteraction time strategy, used sequential cooperative search. Obviously usingparallel interaction decrease search cost (i.e. search cost conducting numberinteractions sequentially equal smaller conducting interactions parallel),sequential cooperative search dominating strategy stated next proposition.Proposition 3. cost conducting number parallel interactions equal highercost conducting interactions sequentially (i.e w, w c(1, n) c(w, n)) useparallel search match expected utility sequential cooperative search.Proof. Consider optimal cooperative parallel search strategy x par . Obviously stateSA satisfying: (1) coalitions strategy conduct one interaction, x par (s) > 1(2) future state s0 coalition conducts one interaction (or terminates search),i.e., xpar (s0 ) 1. replace parallel search strategy state sequentialsearch state expected utility increase. worstcase scenario coalition execute w interactions incurring cost x par ,ending expected termination utility. case, whenever coalitionreaches state s0 strategy according xpar terminate search prior completing winteractions, expected utility least expected utility achieved using x par (otherwisecoalitions strategy s0 according xpar resume search) . Therefore usenew strategy improve expected utility. Using backward induction applylogic former states x par implies one interaction parallel.12. extent achieved improvement highly correlated specific environment coalitionoperates.13. Notice context single agent sequential search specific case single agent parallel search,agent interacts single seller agent time.14. Assuming similar cost structures three models.18fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONSConsequently, obtain sequential strategy results least large expected utilityxpar .Next consider case agents fully homogeneous (in terms utilityfunctions) operate B2C markets. Here, prove optimal strategy coalitionstationary (i.e. change according current state). Furthermore, showstationary strategy characteristic holds fully homogeneous agents alsoagents correlated preferences. Two agents j correlated preferencesagent ai prefers ~o0 ~o00 agent j prefers opportunity ~o0 ~o00 (i.e ~o0 ,~o00 p ,U j (~o0 ) U j (~o00 ) Ui (~o0 ) Ui (~o00 )) vice versa.Theorem 3. B2C market, agents correlated preferences, search strategybased reservation value15 Urv . scenario number parallel interactionscoalition uses according optimal strategy (in case resumes search) fixedsearch (s SA Vt (s) < Urv exists x (s) = w f ixed ).Proof. scenario, search problem equivalent problem single agentutility function equal sum different agents utilities, U = U 1 +U2 + ... +Un .case terminating search, coalition members always assignedopportunity. Therefore, search strategy reservation value based, search terminatedupon reaching opportunity utility exceeding pre-set reservation value U rv . Sinceprobability reaching opportunity depend coalitions state, numberparallel interactions used throughout search fixed.Nevertheless Theorem 3 hold C2C market even agents homogeneous next lemma states.Lemma 3. C2C market even fully homogeneous agents (all agents utilityfunctions) search strategy always stationary optimal number parallel interactions changed search according coalitions current state.Proof. order prove lemma, consider coalition operates C2C marketfollowing environment:Environment 4. Assume coalition two agents, 1 a2 , searching product (e.g., usedbook) characterized one attribute (e.g., indicating whether book signed author)two possible values, 1 (signed) 2 (not signed). results two opportunity types, o~ 1 = (1)o~2 = (2). Assume opportunity o~1 rare found probability 1/100,opportunity o~2 common (a probability 99/100). agents utilities 100 rareopportunity 1 common opportunity. Consider cost conducting w parallelinteractions equal cost conducting w interactions sequentially, c(w, n) = wc(1, n),cost conducting single interaction c(1, n) = 0.1 + 0.05n, (n > 1). Table 4 summarizesenvironments setting.consider two following states: (1) coalition starts search (the coalitions currentstate initial state = {}) (2) coalition encounters opportunity o~ 1 (the coalitions current15. reservation value value coalition sets a-priori terminates search reached opportunityassociated utility greater equal value (Sarne & Kraus, 2005).19fiM ANISTERSKI , ARNE , & K RAUSOpportunityAttributeProbability~o1~o2signedsigned0.010.99UtilityAgent a1 Agent a210010011Table 4: Agents utilities four opportunities Environment 4state s0 = {~o1 }).order coalition terminate search members exploit opportunity o~ 1 (otherwise expected utility resuming search exceeds search cost). Thus ordercoalition terminate search former case (the coalitions state = {}) must encounteropportunity o~1 twice latter case (the coalitions state 0 = {~o1 }) encounteropportunity o~1 once. Thus expect number optimal interactions, coalitionsstate s, different number optimal interactions, state 0 . Indeedcomputation coalitions optimal number interactions (using Algorithm 1) resultsx (s) = 299 x (s0 ) = 161.7. Finite Decision Horizonimportant variant cooperative parallel search one agents formingcoalition restricted deadline finalizing search. example, consider coalitionsearches costumes members wear costume party. case coalitionsearch costumes forever since customs value coalition memberspurchased prior party. type environment often referred search theoryfinite decision horizon environment. Specifically, within context paper considerfinite decision horizon environments coalition whole terminate search priorwithin next r search rounds. Note sequential search definition equivalentdefinition stating coalition conduct r additional interactions.addition general advantages recognized parallel cooperative search, finite decision horizon environments model enable coalition conduct maximumr interactions facilitated sequential model (whenever necessary). Moreover, environments coalition cannot improve performance interacting several sellersparallel (e.g., cases described Proposition 3), introduction finite decisionhorizon constraint creates strong incentive interact single seller time.illustrated following example:Example 3. Consider coalition operates B2C market, characteristicsEnvironment 4, introduced section 6. environment coalitions optimalstrategy infinite decision horizon search sequentially (see Proposition 3). Nevertheless, various finite decision horizon values coalition benefit using parallelsearch technique. trivial example case coalition terminatesearch within next search round. case, coalition conducts several interactionsnext round, probability encounter opportunity o~1 increases comparison caseconducts single search. example coalition conducts 100 interactionsprobability 0.99100 encounter opportunity o~2 search probability20fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONS1 0.99100 encounter opportunity o~1 search expected utility expressedas: (1 0.99100 ) Vt ({~o1 }) + 0.99100Vt ({~o2 }) c(100, 2) = (1 0.99100 ) 200 + (0.99)100 20.2 100 = 107.52. value significantly greater expected utility using sequential cooperative search 0.01 Vt ({~o1 }) + 0.99Vt ({~o2 }) c(1, 2) = 3.78.order compute coalitions strategy finite decision horizon model variant extenddefinitions include number remaining search rounds, r. use V (s, w, r) denotecoalitions expected utility, conducts w interactions next round terminatesearch within next r rounds. term V (s, 0, r) denotes immediate utility obtained,coalition decides terminate search state s, thus: V (s, 0, r) = Vt (s). denote x (s, r)V (s, r) coalitions optimal strategy expected utility (when using optimal strategy)state s, terminate search within next r rounds.V (s, r) x (s, r) calculation similar infinite decision horizon case, exceptwhenever coalition reaches decision deadline inevitably terminates search. Namely,r = 0, x (s, 0) = 0 coalitions expected utility V (s, 0) equal Vt (s).arg maxw V (s, w, r) r > 0(18)x (s, r) =0r=0(19)V (s, r) = V (s, x (s), r)begin computing V (s, w, r), r > 0 (since V (s, w, 0) = 0, w). Here, applyanalysis methodology used section 4. However, expected utility coalitionresuming search following current search stage reflect change decisionhorizon. Therefore, instead using Equation 4, use following modification:V (s, w, r) = c(w, n) +w simprovewpw (w )V (s0 , r 1) + pstay (s, w)V (s, r 1),w > 0, r > 0(20)= state(s w ).computation Equation 20 exponential number parallel interactions. orderreduce complexity V(s,w,r) r > 0 consider two following cases:s0w = 1: coalition encounters single opportunity thus Equation 20 expressed as:V (s, 1, r) = c(1, n) +{~o}simprovep(~o)V (state(s ~o ), r 1) +p(~o)V (s, r 1),r > 0{~o}sstay11(21)case, additional computational complexity introduced, comparison SCSmodel (Sarne & Kraus, 2005).w > 1: attempt find computational means extracting value termw simprovew V (s0 , r 1) + pstay (s, w)V (s, r 1) (in Equation 20) complexity polynomialw. expression denotes coalitions expected utility conducts w interactionswithout considering cost associated conducting w interactions. order efficiently compute value consider coalitions expected utility conducting onew interactions obtaining opportunity ~o. case, coalitions expected utility21fiM ANISTERSKI , ARNE , & K RAUSequal coalitions expected utility starting state state(s ~o) conductingw 1 interactions plus cost equivalent cost maintaining w 1 interactions,c(w 1, n) (these added since already subtracted expected utilityEquation 20 V (state(s ~o), w 1, r) subtracts again):V (s, w, r) = c(w, n) +p(~o)(V (state(s ~o), w 1, r) + c(w 1, n)),~oO pw > 1, r > 0(22)sum Equation 22 represented sum expected utility opportunities change coalitions current state sum opportunities changecoalitions current state:V (s, w, r) = c(w, n)+p(~o)V (s0 , w1, r)+{~o}simprove1p(~o)V (s, w1, r)+c(w1, n),{~o}sstay1w > 1, r > 0(23)= state(s ~o).Equations 23 21 facilitate calculation V (s, w, r) polynomial time w. orderfind coalitions optimal strategy, efficient bound optimal number interactionsgiven Equation 7 also valid finite decision horizon variant.analysis leads Algorithm 2 modification Algorithm 1. algorithmcomputes coalitions strategy expected utility states . order computeV (s, r), algorithm uses backward induction. starts computing V (si , 0) states.Here, coalition forced terminate search thus algorithm sets expected utilityequal termination utility: V (si , 0) = Vt (si ). Then, algorithm computes V (s , r, w) r > 0using Equation 21 23 starting w = 1 till c(w, n) Vt (s1 ) Vt (sinit ). algorithm setscoalitions optimal strategy number interactions w maximizes V (s , r, w).Similar infinite decision horizon case, Algorithm 2 reuses components computed earlierstages stage execution. Nevertheless, case order store resultsmemory reuse later, need three dimensional matrix V size |S | (winitmax + 1) (rmax + 1)rmax initial decision horizon. corresponding V (s, w, r) values storedtriplet (s, w, r), representing coalitions state, number simultaneous interactions usedlimit number rounds coalition conduct. Additionally, store V (s, r)x (s, r) values two matrixes V X size |S | (rmax + 1) reusing x (s0 , r 1)V (s0 , r 1) computation V (s, w, r).following theorem prove algorithm 2 returns coalitions optimal strategyexpected utility, polynomial time w initmax (the bound number parallel interactionscomputed using Equation 7) rmax .s0Theorem 4. Algorithm 2 returns optimal strategy coalition expected utilityfollowing strategy, terminate search within next r max rounds, polynomial time winitmax rmax .Proof. proof, assume use matrices storing computed values describedabove. process extracting Ss done exactly finite decision horizon. Thus, step1 complexity O(|SA ||A|2 + |SA | log |SA |) O(|SA ||A|3 + |SA | log |SA |) B2C markets,22fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONSAlgorithm 2 algorithm computing optimal search strategy x coalitionterminate search within next r max roundsInput: p - set potential opportunity types market; p(~o) - opportunity types probabilityfunction; n - coalitions size; U j (), j = 1, ..., n - coalition members utility functions; r max -maximum number search rounds terminating search (the decision horizon); c(w, n)- search cost function; SA - set possible states coalition A.Output: x (s, r),V (s, r) Ss , 0 r rmax - coalitions optimal strategy expectedutility.1: Build set ordered states2: = 1 |Ss |3:Set x (si , 0) = 04:Set V (si , 0) = Vt (si ) using Equation 15: end6: r = 1 rmax7:i=1 |SA |8:Set V (si , 0, r) = Vt (si ) using Equation 19:Compute V (si , 1, r) using Equation 2110:Set w = 211:c(w, n) (Vt (s1 ) Vt (sinit ))12:Compute V (si , w, r) using Equation 2313:w++;14:end15:Set x (si , r) = arg maxw0 (0,...,w1) V (si , w0 , r), V (si , r) = V (si , x (si , r), r)16:end17: end18: Return (x (si , r), = 1, ..., s|SA | , r = 0, ..., rmax )C2C markets, respectively. computation steps 3 4 takes constant time, assumingstored state coalitions termination utility built . steps performedstates, thus, overall complexity computing steps throughout loop O(|S |).Again, step 8 performed constant time, assuming coalitions termination utilitystored state. state, s, limit number search rounds r > 0reaching step 9 algorithm, algorithm already computed V (s0 , r 1) potentialfuture new state s0 originating state s. Therefore, computation step 9 simply summing order |O p | components already computed. Consequently, ignoretime computing coalitions new states 0 (we add time computing valueslater), computation step takes O(|O p |). Since steps 8-9 performed rmax |SA | times,overall complexity computing steps (when ignoring time computing newstates) O(|O p |rmax |SA |). Notice w = winitmax + 1 obtain c(w, n) > (Vt (s1 ) Vt (sinit )).Thus, loop step 11 performed w initmax times. w > 0 reaching step 12,0algorithm already computed V (s , w 1, r) potential future new state 0 originatingstate s. Therefore, computation step 12 takes O(|O p |) ignoring time computing new states. Since step performed r max |SA |winitmax times, overall complexitycomputing step (when ignoring time computing new states) O(|O p |rmax |SA |winitmax ).23fiM ANISTERSKI , ARNE , & K RAUSstep 15, coalition chooses maximum value among w initmax values alreadycomputed. Since step performed state , r {0, ..., rmax },overall complexity performing step O(w initmax |SA |rmax ). Computing coalitions new states0 encounters opportunity ~o given current state done describedinfinite decision horizon case. Thus, overall complexity computing future states statesO(|SA ||A||O p |) B2C markets, O(|SA ||A|3 |O p |) C2C markets. Given analysis,2overall complexity algorithm O(|S | log |SA | + |SA ||O p |winitmax rmax + |SA ||A| + |SA ||A||O p |)3B2C markets, O(|SA | log |SA | + |SA ||O p |winitmax rmax + |SA ||A| |O p |) C2C markets. Hence16initalgorithm polynomial wmax rmax .Similar infinite decision horizon significant improvement algorithms peri , state,formance achieved calculating using specific upper bound, w smaxstep 11, according Equation 7.following section illustrate properties PCS model infinitefinite decision horizons model variants.8. Illustrative Examples PCS Modelefficient means calculating coalitions optimal strategy using parallel cooperative search, demonstrate specific properties search method. referenceuse Single Agents Parallel Search (SAPS), Single Agents Sequential Search (SASS)Sequential Cooperative Search (SCS) models.8.1 Infinite Decision Horizonbegin illustrating parallel cooperative search infinite decision horizon. Firstdemonstrate influence level heterogeneity utility functions different coalition members coalitions performance (in terms expected utility achieved). orderdemonstrate use following environment, used originally evaluatingperformance SCS model (Sarne & Kraus, 2005):Environment 5. coalition two agents, 1 a2 , searching opportunities defined twoattributes, B1 (e.g., quality) B2 (e.g., store rating), attribute valuediscrete range (1, ..., 5) equal probability values. agentsheterogeneous respect way evaluate potential opportunity. Agent 1 associated utility function U1 (~o) = B1 + B2 , agent a2 associated utility functionU2 (~o) = 2(1 )B1 + 2B2 . Thus, parameter indicates level agents similarity/heterogeneity. search cost single agent conducting single interaction c basew parallel interactions (w > 1): c(w, 1) = c base + c parallel (w 1). search cost coalitionsize n c(w, n) = c(w, 1)ln(n + 1), (n > 1).Figure 2 depicts expected utility per agent using different search methods 17C2C market (left hand-side) B2C market (right hand-side) parameter similaritylevel, , utility functions agents constituting coalition.16. algorithm uses winitmax upper bound optimal number interactions x (si ), = 1, ..., |SA |. boundsivalid since according Equation 7 value wmax increases Vt (si ) decreases coalitions terminationutility reaches minimum initial state (Vt ({}) = 0).17. cooperative models average expected utility per coalition member measure used.24fiE NHANCING C OOPERATIVE EARCHutility9.3utility9.39.119.1PCSC ONCURRENT NTERACTIONSPCS18.98.922SAPS8.58.3SAPS8.78.78.53SCS8.348.1SASS7.97.73SCS8.17.94SASS7.70.01 0.11 0.21 0.31 0.41 0.51 0.61 0.71 0.81 0.910.01 0.11 0.21 0.31 0.41 0.51 0.61 0.71 0.81 0.91similarity level ( )Figure 2: Average expected utility per buyer agent function similarity level different modelsdifferent marketsCurve 1 graph depicts average expected utility two agents form coalition function similarity level agents utility functions, making usesuggested parallel cooperative search. Here, search cost conducting single interactionset 0.2 (cbase = 0.2) search cost conduct additional interactions set 0.05(c parallel = 0.05). expected model (represented curve 1) outperforms SCS model (represented curve 3) terms expected utility agents. two curves describeaverage expected utility agents searches separately using Single AgentParallel Search (SAPS) (represented curve 2) Single Agent Sequential Search (SASS)(represented curve 4) models. specific environment use cooperative parallelsearch also outperforms single agent parallel search model though always case.Notice results obtained cooperative parallel search consistent generalcharacteristic cooperative search (Sarne & Kraus, 2005) use methodB2C market results better expected utility C2C market. case separatedsingle searches (SAPS SASS models) market type affect strategy structureexpected utility since agent searches single opportunity benefit.Figure 2 also reflects interesting insight contradicts important strategy dominationrelationship found single cooperative sequential search techniques fully homogeneous agents (i.e. utility function, case = 0.5 example)operating C2C markets. sequential search use single agent search always outperforms cooperative search C2C markets (when considering fully homogeneousagents) (Sarne & Kraus, 2005), actual evidence parallel search concerned,cooperative search technique may outperform aggregated result single homogeneousagents search.Figure 3 shows expected utility per agent using different search methodsC2C market (left hand-side) B2C market (right hand-side) parameter costconducting additional interaction c parallel (notice agents performance affectedvalue SASS SCS models). results based Environment 5 (theenvironment used Figure 2), = 0.1 c base = 0.2. expected cost c parallel25fiM ANISTERSKI , ARNE , & K RAUSdecreases average expected utility using parallel models (PSC SAPS models) increases(and thus greater improvement comparison sequential models). Note wheneverc parallel cbase performance converges one achieved obtaining one observationtime, used sequential models. behavior correlated Proposition 3.0.170.219.7utilityutility9.79.59.59.39.39.119.1PCS18.98.9SAPS8.72238.5SCS8.348.1SAPS8.78.53PCSSCS8.34SASSSASS8.10.010.050.090.130.170.210.010.050.090.130.170.21cost parallel interactionFigure 3: Average expected utility per buyer agent function c parallel using different searchmethods different marketscost conducting additional interaction c parallel also influences optimal numberinteractions coalition conduct. Figure 4 shows optimal number parallelinteractions coalition conduct beginning search, x ({}), functionc parallel environment used Figure 3 (Environment 5). expected,optimal number interactions, x ({}), increases c parallel decreases equal 1,c parallel cbase .Figure 4 coalitions optimal number interactions using PCS model highercoalition operates C2C market B2C market.intuitive explanation (in C2C market opportunity exploited one agentscoalition needs encounter opportunities C2C market) cannot generalized.following example illustrates scenario optimal number interactions actuallygreater operating B2C market.Environment 6. coalition two agents, 1 a2 , searching product (e.g., computergame) associated 4 types opportunities {~o 1 ,~o2 ,~o3 ,~o4 } (e.g., representing different configurations). agents utilities opportunities distribution given Table 5. search costsingle agent conduct single interaction c base = 0.2, search cost conductingadditional search c parallel = 0.1, c(w, 1) = cbase + c parallel (w 1). search costcoalition c(w, n) = c(w, 1)ln(n + 1), (n > 1).Figure 5 shows optimal number interactions used coalition (in SCS model)agent (in SAPS model) beginning search, operating Environment6. one see coalitions optimal number interactions using PCS model B2C26fiE NHANCING C OOPERATIVE EARCHoptimal # interactions32C ONCURRENT NTERACTIONS(1) Coalition B2C28(2) Coalition C2C241(3) Agent A1204(4) Agent A21612328400.010.060.11cost parallel interaction0.160.21Figure 4: Coalitions agents optimal number parallel interactions beginning searchfunction c parallelOpportunityProbability~o1~o2~o3~o40.4990.250.250.001UtilityAgent a1 Agent a2000110100100Table 5: Agents utilities four opportunities Environment 6market (44) larger coalitions optimal number parallel interactions C2C market(8). Moreover figure contradicts two additional hypothesis one may presumeoptimal number coalitions parallel interactions. first coalitions optimal numberparallel interactions equal overall number parallel interactions,agents conducts search autonomously. seen Figure 5, optimal numberparallel interactions B2C market greater total number parallel interactions,agents conducts search autonomously (10 + 10 = 20 < 44). second hypothesissuggests coalitions optimal number parallel interactions least number parallelinteractions, agents conducts search autonomously. This, provenwrong Figure 5, coalitions optimal number parallel interactions C2C market(8) smaller number parallel interactions agent 1 (10), number parallelinteractions agent a2 (10).Next introduce make use simpler sample environment demonstratingadditional properties cooperative parallel search.Environment 7. coalition two agents, 1 a2 , searching opportunities defined twoattributes, B1 (e.g., quality) B2 (e.g., store rating), attribute valuediscrete range (1, 2) equal probability values. utility functionsused U1 (~o) = 1.9B1 + 0.1B2 U2 (~o) = 0.1B1 + 1.9B2 . search cost single agent27fiM ANISTERSKI , ARNE , & K RAUSx*({})4536271890PCSC2CPCSB2CSAPSAgent 1SAPSAgent 2Figure 5: Coalitions optimal number parallel interactions beginning searchc(w, 1) = 0.5 + 0.05w, coalition c(w, n) = c(w, 1) ln(n + 1), (n > 1). Table 6summarizes environments setting.Opportunity~o1~o2~o3~o4(Attribute1,Attribute2)(1,1)(1,2)(2,1)(2,2)Probability14141414UtilityAgent a1 Agent a2222.13.93.92.144Table 6: Agents utilities four opportunities Environment 77.1utility76.96.86.76.66.56.412345678910# parallel interactions (w)Figure 6: Coalitions overall expected utility function wFigure 6 depicts expected coalitions overall utility respect number interactionsconducted beginning search (i.e. first search stage, coalition knowsopportunities), assuming states coalition uses optimal number28fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONSparallel interactions, x (s). Here, see effect two conflicting forces: numberparallel interactions coalition uses stage increases, probability associating betteropportunity two coalition members increases. However overall search costassociated search stage increases. figure, conclude optimal numberparallel interactions used stage x ({}) = 5.additional important characteristic cooperative parallel search wish emphasizeconcerns number parallel interactions used part optimal search strategy alongsearch. single agents parallel search search strategy stationary (i.e. numberparallel interactions used change along search process) model numberparallel interactions along search needs maintained depends coalitions state(i.e. set opportunities known coalition). demonstrated directed acyclicgraph (DAG) given Figure 7, describes search process Environment 7. verticesgraph present potential coalitions states (the state determined according relevantset known opportunities, correlated definition given section 4). edge connectstwo states s0 possibility reach state following search roundcoalition conducts search according optimal search strategy. example directededge connects = {} s0 = {(1, 2), (2, 1)}, since coalition proceed 0conducts five parallel interactions according optimal search strategy encountersopportunities (1, 2) (2, 1). Notice reaching states {(1, 2), (2, 1)} {(2, 2), (2, 2)}optimal strategy coalition terminate search. Therefore edge originatingstates. illustrated Figure 7, number parallel interactions coalitionuse according optimal search strategy (denoted x (s)) depends coalitions state.comparison purposes, notice single agents separate search (i.e. single agentparallel search model) optimal strategy constantly use 4 parallel interactions (as longagents strategy resume search).x*(s)=4{(1,2)}x*(s)=5{}Vt(s)=0x*(s)=5Vt(s)=6{(1,1)}Vt(s)=4x*(s)=0{(1,2),(2,1)}Vt(s)=7.8x*(s)=0x*(s)=4{(2,1)}Vt(s)=6{(2,2)}Vt(s)=8Figure 7: Optimal strategy potential transitions states simple B2C market8.2 Finite Decision Horizonsection demonstrate properties PCS model variant coalitiongiven finite decision horizon. First explore influences cost conducting addi29fiM ANISTERSKI , ARNE , & K RAUStional interaction, c parallel , agents expected utility. Figure 8 depicts results obtainedvarying c parallel 0.01 0.3. figure use environment used Figure3 (Environment 5), = 0.1, cbase = 0.2 decision horizon two search rounds.cost conducting additional interaction c parallel decreases, coalitions expected utilityusing parallel models PSC SAPS increases superiority sequential modelsincreases. Moreover shown Figure 8 parallel model outperforms sequential model evenc parallel cbase (e.g., c parallel = 0.2). Recall c parallel cbase parallel technique improve expected utility infinite decision horizon (as suggestedProposition 3).9.6utilityutility9.69.19.18.6128.1PCSSAPS8.1SAPS7.67.11 PCS28.67.64SASS37.1SCS46.66.636.10.01SASSSCS6.10.060.110.160.210.260.010.060.110.160.210.26cost parallel searchFigure 8: average expected utility per buyer agent function c parallel coalitionterminate search within two next rounds.Another factor affects expected utility decision horizon, represented valueparameter r. Figure 9 presents coalitions expected utility function r. figureused environment used Figure 8, set cost additionalsearch, c parallel = 0.2. observed Figure 9, search models, earlier coalitionterminate search smaller expected utility. Moreover, expected utility improvementobtained PCS (in comparison SCS) SAPS (in comparison SASS) modelsincreases r decreases. case ability conduct parallel interactions compensatessmall number search rounds coalition conduct. Note r increases, averageexpected utility per buyer agent converges average expected utility per buyer agentinfinite decision horizon model.Finally, Figure 10 depicts optimal number interactions coalition (in PCS model)agent (in SAPS model) beginning search, function r. figureused environment parameters used Figure 9. expected numberparallel interactions increases r decreases.demonstrated throughout section, PCS outperforms sequential cooperative search. Generally, magnitude improvement depends size domain, searchcost structure, different utility functions used.30fiE NHANCING C OOPERATIVE EARCHutility27.7utilityPCS18.2SAPS4SASSPCS138.3SCSSAPS27.26.7C ONCURRENT NTERACTIONS47.8SASS6.27.35.75.26.834.7SCS4.26.33.73.25.8159131471013limit number search rounds- rFigure 9: average expected utility per buyer agent function limit number searchoptimal # interactionsrounds, r(1) Coalition B2C8(2) Coalition C2C63(3) Agent A12(4) Agent A244120159limit number search rounds- r13Figure 10: Optimal number interactions function decision horizon, r9. Discussion Conclusionscapability using parallel interactions part search process inherent infrastructure autonomous information agents. using cooperative parallel search, coalitionuse new strategy, different structure comparison optimal strategy usedcooperative sequential search single agents parallel search. expected, use newmodel potential significantly improving coalitions expected utility demonstratedprevious section. Furthermore, emphasize coalitions expected utility neverdecrease using proposed mechanism comparison pure sequential cooperative search.mainly suggested algorithm converge one interaction time strategy,used sequential cooperative search (which specific case model) case31fiM ANISTERSKI , ARNE , & K RAUSmaintaining single interaction world states favorable. Obviouslysearch cost linear depends solely number interactions maintained,use coalition increase number sellers interacts givensearch round. Nevertheless, scenarios coalitions search cost combines additional fixedcomponents non-linear dependency number interactions maintained much realistic (Sarne & Kraus, 2005). scenarios parallel cooperative search yields large benefitssearchers.parallel cooperative search weakly dominates sequential cooperative search,necessarily dominate autonomous search (where agents search insteadcooperatively). decision whether use parallel cooperative search autonomouslydepends amount coalition overhead costs induced cooperative search. Therefore,computational means developed work enables agents identify fruitful opportunities searching cooperatively. Generally, introduction parallel cooperative searchmodel substantially increases number scenarios agents prefer search cooperatively.novelty analysis given paper threefold. First supplies us betterunderstanding space opportunities, dividing improving non-improving areas.Thus, instead dual simultaneous dependencies states define singledirectional dependency pair states. Second, supplies bound optimum numberparallel interactions coalition uses state optimal strategy. Third represent parallel search sequential process, without breaching model assumptions.three features allow us overcome main complexity associated attempt solveproblem set equations proffer finite algorithm polynomial numberparallel interactions (rather brute-force algorithm exponential numberparallel interactions) inevitably reaches optimal strategy. Moreover, provide comprehensive analysis parallel model extracting optimal search strategy given finitedecision horizon. Here, illustrate, coalition benefit significantly infinitedecision horizon integration parallel interactions cooperative search. PCSmodel (both finite infinite decision horizon forms) general appliedcoalition regardless cost function preferences used agents. adaptationadditional markets (other C2C B2C) achieved appropriately modifyingallocation function used assigning agent one opportunities found alongsearch.focus research finding optimal search strategy coalition, givenstructure, opportunity distribution, reported members preferences. treats coalition unified entity sharing common goal (to maximize sum members utilities).Nevertheless, various, important aspects coalition formation, context cooperative search, always correlate assumptions used thereforeaddressed. example, may incentive coalition members misreport preferences side-payments used. Alternatively, agents may able form side coalitions,free-ride actively searching coalitions single agent member activelysearching coalition (that member would act spy side coalition). Moreover,environments agents may face tight budget constraints could violated withincoalition side-payments. analysis important issues based ability properlyderive coalitions utility given specific setting. paper supplies functionality, laying32fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONSfoundation enabling research. Future work encompass extend scoperesearch include additional topics associated coalition formation process,coalition stability division payoffs coalition members. importantextensions include relaxation assumptions underlying opportunity model(e.g., different deadlines different opportunities).Acknowledgmentswork supported part NSF grant IIS0705587 ISF. Kraus also affiliatedUMIACS.Appendix A. Summary NotationsNotationMeaningB = (B1 , B2 , ..., Bk )set attributes defining potentially available opportunities market, attribute Bassigned value finite set (b imin , ..., bimax ).space potential opportunity types coalition may encounter.coalition agents.Agent j utility opportunity type ~o.search cost associated coalition n agentsmaintaining w simultaneous interactions seller agents.function maps given set opportunities knowncoalition members way aggregated agents utilitymaximizedstate coalition acquainted set known knownopportunities.set possible states coalition throughout search.immediate utility coalition terminates searchgiven set known opportunities known .coalitions expected utility using w parallel interactionsstate s.immediate utility obtained, coalition decides terminate search state s.number parallel interactions coalition conductsstate according optimal strategy.collection w-sized sets opportunities produced environment coalition operates.OpU j (~o)c(w, n)alloc(known )state(known )SAVt (known ):V (s, w)V (s, 0)x (s)w33fiM ANISTERSKI , ARNE , & K RAUSNotationpw (w )simprovewsstaywSspstay (s, w)wsmaxwinitmaxV new (s, k)rx (s, r)V (s, r)V (s, w, r)Meaningprobability encountering specific set opportunities w ,maintaining w random interactions seller agents.collection w-sized sets opportunities, w , changecoalitions current state s.collection w-sized sets opportunities, w ,change coalitions current state s.set states belonging SA , sorted accordingtermination utilities Vt (s).probability agent stays state conductingw parallel interactions.upper bound optimal number parallel interactionsused coalition state s.upper bound optimal number parallel interactionsused coalition begins search (s = {}).coalitions expected utility obtained potentially reachingnew states (e.g. different s) executing k parallel interactions (without incorporating search cost conductingk interactions), assuming future strategy uses x (s0 ) newfuture state s0 .Finite Decision Horizonmaximum number search rounds coalitionconduct.coalitions optimal strategy state s, needs terminatesearch within next r search rounds.expected utility coalition restricted maximum rsearch rounds, reaching state assuming acts optimallythroughout search.coalitions expected utility conducts w interactionsnext round terminate search within next rrounds.ReferencesAvis, D., & Lai, C. (1988). probabilistic analysis heuristic assignment problem.SIAM J. Comput., 17(4), 732741.Bakos, Y. (1997). Reducing buyer search costs: Implications electronic marketplaces. Management Science, 42(12), 167692.Baron, D. P., & Ferejohn, J. A. (1989). Bargaining legislatures. American Political ScienceReview, 83(4), 11811206.Benhabib, J., & Bull, C. (1983). Job search: choice intensity. J. Political Economy, 91(5),747764.34fiE NHANCING C OOPERATIVE EARCHC ONCURRENT NTERACTIONSBreban, S., & Vassileva, J. (2001). Long-term coalitions electronic marketplace. B.Spencer, ed., Proceedings E-Commerce Applications Workshop.Burdett, K., & Malueg, D. A. (1981). theory search several goods. Journal EconomicTheory, 24, 362376.Carlson, J. A., & McAfee, R. P. (1984). Joint search several goods. Journal Economic Theory,32, 337345.Choi, S., & Liu, J. (2000). Optimal time-constrained trading strategies autonomous agents.Proceedings MAMA-2000, pp. 1113.Dias, M. (2004). TraderBots: New Paradigm Robust Efficient Multirobot CoordinationDynamic Environments. Ph.D. thesis, Robotics Institute, Carnegie Mellon University.Gal, S., Landsberger, M., & Levykson, B. (1981). compound strategy search labormarket. International Economic Review, 22(3), 597608.Gatti, J. (1999). Multi-commodity consumer search. Journal Economic Theory, 86(2), 219244.Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determinationminimum cost paths. IEEE Transactions Systems, Science Cybernetics, 4(2), 100107.Ito, T., Ochi, H., & Shintani, T. (2002). group-buy protocol based coalition formationagent-mediated e-commerce. International Journal Computer Information Science(IJCIS), 3(1), 1120.Kahan, J., & Rapoport, A. (1984). Theories Coalition Formation. Hillsdale, NJ:Lawrence Erlbaum Associates.Keeney, R., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences Value Tradeoffs. New York, US:John Wiley & Sons.Kephart, J., & Greenwald, A. (2002). Shopbot economics. JAAMAS, 5(3), 255287.Lermann, K., & Shehory, O. (2000). Coalition formation large scale electronic markets.Proceedings ICMAS-00, pp. 167174.Li, C., Rajan, U., Chawla, S., & Sycara, K. (2003). Mechanisms coalition formation costsharing electronic marketplace. Proceedings ICEC-03, pp. 68 77.Lippman, S., & McCall, J. (1976). economics job search: survey. Economic Inquiry,14(3), 155189.Manisterski, E. (2007). Protocols Strategies Agents Teamwork. Ph.D. thesis, DepartmentComputing Science, Bar Ilan University.McMillan, J., & Rothschild, M. (1994). Search. Aumann, R. J., & Sergiu Hart, A. (Eds.),Handbook Game Theory Economic Applications, pp. 905927. Elsevier.Morgan, P. (1983). Search optimal sample size. Review Economic Studies, 50(4), 659675.Morgan, P., & Manning, R. (1985). Optimal search. Econometrica, 53(4), 923944.Rothschild, M. (1974). Searching lowest price distribution prices unknown.Journal Political Economy, 82(4), 689711.Sandholm, T., Larson, K., Andersson, M., Shehory, O., & Tohme, F. (1999). Coalition structuregeneration worst case guarantees. Artificial Intelligence, 111(1-2), 209238.35fiM ANISTERSKI , ARNE , & K RAUSSarne, D., & Kraus, S. (2003). search coalition formation costly environments.Proceedings CIA-03, pp. 117136.Sarne, D., & Kraus, S. (2005). Cooperative exploration electronic marketplace. Proceedings AAAI-05, pp. 158163.Shehory, O., & Kraus, S. (1998). Methods task allocation via agent coalition formation. ArtificialIntelligence, 101(1-2), 165200.Stigler, G. (1961). economics information. Journal Political Economy, 69(3), 213225.Talukdar, S., Baerentzen, L., Gove, A., & de Souza, P. S. (1998). Asynchronous teams: Cooperationschemes autonomous agents. Journal Heuristics, 4(4), 295321.Tsvetovat, N., Sycara, K., Chen, Y., & Ying, J. (2000). Customer coalitions electronic markets.Proceedings AMEC-00, pp. 121138.Wang, Y., Makedon, F., & Ford, J. (2004). bipartite graph matching framework finding correspondences structural elements two proteins. Proceedings EMBC-04, Vol. 42,pp. 297275.Yamamoto, J., & Sycara, K. (2001). stable efficient buyer coalition formation schemee-marketplaces. Proceedings Agents-01, pp. 576583.36fiJournal Artificial Intelligence Research 32 (2008) 757791Submitted 01/08; published 08/08Compositional Belief UpdateJames DelgrandeYi Jinjim@cs.sfu.cayij@cs.sfu.caSchool Computing ScienceSimon Fraser University, Burnaby, BC, Canada V5A 1S6Francis Jeffry Pelletierjeffpell@sfu.caDepartments Philosophy LinguisticsSimon Fraser University, Burnaby, BC, Canada V5A 1S6Abstractpaper explore class belief update operators, denitionoperator compositional respect sentence added. goal provideupdate operator intuitive, denition based recursive decompositionupdate sentences structure, may reasonably implemented. addressingupdate, rst provide denition phrased terms models knowledge base.operator satises core group benchmark Katsuno-Mendelzon updatepostulates, postulates satised. Katsuno-Mendelzon postulatesobtained suitably restricting syntactic form sentence update, show.restricting syntactic form sentence update, also obtain hierarchyupdate operators Winsletts standard semantics basic interesting approachcaptured. subsequently give algorithm captures approach; generalcase algorithm exponential, not-unreasonable assumptions obtainalgorithm linear size knowledge base. Hence resulting approachmuch better complexity characteristics operators situations.also explore compositional belief change operators: erasure developed dualoperator update; show forget operator denable terms update;give denition compositional revision operator. obtain compositionalrevision, natural denition, yields Satoh revision operator.1. Introductionknowledge base typically static entity, rather evolves time. New information may added, old out-of-date information may removed. fundamentalissue concerns change managed. major body research addressesquestion via specication rationality postulates, standards change operatorsatisfy. postulates describe belief change knowledge level, independentbeliefs represented manipulated. various rationales motivating change evolving knowledge base, diering rationales seencalling dierences background knowledge-level postulates. example, onemay think alteration world occurred, resultupdate knowledge bases representation world appropriate way. Or,may think previous sources information fallible incompletebetter, accurate information world. So, caserevise beliefs. Another motivation might merge already-existing stores beliefs,c2008AI Access Foundation. rights reserved.fiDelgrande, Jin, & Pelletierwithout giving priori preference one belief sets, aimingachieve balanced resolution conicts. merging might used combinebelief states dierent agents, come joint course action basedsort things considered assimilation knowledge preferencesagents involved. also imagine linguistic reform, concept (orrather, associated word) longer used. case one might sayusers forgot concept/word.dierences motivation led specic dierences sorts postulatesassociated dierent motivations. Initially, AGM approach (Alchourron, Gardenfors, & Makinson, 1985; Gardenfors, 1988), standards belief revisioncontraction functions given, wherein assumed knowledge basereceiving information concerning static1 domain, increased amountaccuracy information responsible changes knowledge base. Subsequently, Katsuno Mendelzon (1992) explored distinct notion belief change,functions belief update erasure, wherein agent changes beliefs responseperceives changes environment. concept forget goes back GeorgeBoole (1854), reintroduced work Lin Reiter (1994) Lin (2001)way characterize agent may bring knowledge base up-to-date, forgettingfacts longer relevant way aect possiblefuture actions. approach syntactic nature: deals issue removingfacts removing ability describe facts.2 Finally, notion knowledge basemerging introduced generalization long-standing problem informationsharing databases, dierent databases might contain conicting information(see Bright, Hurson, & Pakzad, 1992, survey). work Revesz (1993),came interest constructing merged knowledge base best represents information set knowledge bases. One use thought waydetermining course action best represents desires goals divergentset knowledge bases, thereby forming group-level, all-things-considered knowledge base.formal properties merging discussed previous works (e.g., see Lin &Mendelzon, 1998; Konieczny & Pino Perez, 1998; Everaere, Konieczny, & Marquis, 2007).distinctions formal properties dierent types changebrought papers initial AGM publications; instance, KatsunoMendelzon (1992) compared update revision; Konieczny Pino Perez (1998)compared merging revision; Nayak et al. (2006) compared forgetting update.postulates suggested initial authors dierent conceptionsbelief change challenged writers. since approach towardsupdate conicts Katsuno Mendelzons postulates, wish show1. Note static imply mention time. example, one could informationknowledge base state world different points time, revise informationpoints time. Thus, belief revision also applicable situation agent investigatespast event tries reason real state world event took place.considerations revision update interrelated work Lang (2006).2. Nayak, Chen, Lin (2006) described difference thus: belief erasure purports answerquestion believe longer support belief cook killed Cock Robin?,forgetting purports answer question believe Killing concept affordedlanguage?.758fiCompositional Belief Updatenot, itself, reason reject theory every theory met objurgationconcerning foundational postulates.Although focus paper update hence postulates givenKatsuno Mendelzon (1992) objections related postulatesbelieve considerations similar ones bring forward arena would holdrespect sorts belief change postulates. is, thinkrationale imposing compositionality constraint belief updatebrought bear cases belief revision, belief merging, forgetting.knowledge level specications types belief change allow dierent waysimplement them. Various researchers proposed specic change operatorsbelief revision (Borgida, 1985; Dalal, 1988; Satoh, 1988), belief update (Forbus, 1989; Weber, 1986; Winslett, 1988), belief merging (Subrahmanian, 1994; Konieczny, 2000; Everaere,Konieczny, & Marquis, 2005), forgetting (Lang, Liberatore, & Marquis, 2003; Nayaket al., 2006). approaches formulated terms distance modelsknowledge base models sentence revision update. generalless work dealing systems may readily implementable (but see, e.g.,Williams, 1996; Delgrande & Schaub, 2003).paper develop specic update operator operator intendedcompositional, update expressed recursively terms syntacticstructure . Thus, knowledge base updated disjunction = b,idea update function update certain combinationupdate b. update knowledge base conjunction = balso function (a dierent one) update combination updateb. goal arrive operator whose results intuitive, denitionbased recursive decomposition formula; hence (generally abstract) notionupdate anchored part familiar computational setting. Second,hope operators eciently implementable, least cases,exploiting restrictions syntactic form formula. focus formformula update; presumably approach described may combined oneknowledge base divided relevant irrelevant parts update(Parikh, 1999).goals generally realised. First, operators reasonable properties: manyKatsuno Mendelzon benchmark properties satised, including deemedessential Herzig Ri (1999). dont obtain full irrelevance syntax,obtain weaker results regard; well show irrelevance syntaxobtained restricting syntactic form sentence update. approachalso related approaches literature, hence serves establish linksapproaches. fact, family compositional update operators obtainedimposing various syntactic restrictions regarded constituting family operatorsWinsletts standard semantics makes basic nontrivial approach.well, general approach update presented capture forget operator (Lin &Reiter, 1994; Lang et al., 2003; Nayak et al., 2006), certain sense regardedgeneralizing forget. also dene revision operator using obvious denitionoperator; proves case operator corresponds revisionoperator work Satoh (1988).759fiDelgrande, Jin, & Pelletierapproach leads straightforward algorithm implementing operators.algorithm ecient, compared model-based denition distancebased operators. knowledge base disjunctive normal form, size knowledgebase contributes linear factor overall complexity. well, eciencyobtained size input sentence bounded constant.next section reviews belief revision, update, forgetting, merging, describestwo specic approaches update. section following describes approach,which, next section, give discussion analysis. last section containsconcluding remarks; proofs theorems given Appendix.2. Backgrounddescribed, goal introduce compositional method carrying belief change.since part overall goal also examine place compositional belief changeoperation various arenas take place, start outliningdetails dierent conceptions motivate belief change, alongmotivational considerations areas dierent types belief changepart ways. operators introduced implicitly, means set postulateslegitimate operator required obey. However, areasdispute concerning correctness various postulates, mentionproceed, since approach case update obeystandard postulates update. start historically earlier case revisionmoving central concern update. followed short expositionsconcerning forgetting merging.2.1 Formal Preliminariesconsider propositional language L, nite set atoms, is, propositionalletters, P = {, a, b, c, . . . }, truth-functional connectives , , . convenient, also used, considered introduced denition.use logical equivalence; is, abbreviation ( ). Litsset literals P {l | l P}. particular, also denoted . set literalsconsistent 6 atom p P p, p . literall, use l denote l l P, l P l = l. Similarly, set literals ,use denote set {l | l }. expression atom() denotes set atomsformula . interpretation L maximal consistent set literals, i.e.,every p P precisely one p , p holds. model sentenceinterpretation makes true, according usual denition truth. od() denotes set models sentence . also make use notation odL() denoteset models sentence language (that say, languageatom().) interpretation write |= mean true . interpretationset literals , dene = \ ( ). is, set literalscontaining neither l l l . example, = {a, b, c}{b, c} = {a}.denote negation-normal form (in negation applies atoms only)sentence nnf (). Similarly, denote conjunctive normal form disjunctive760fiCompositional Belief Update3normal form cnf ()W dnf () respectively. setVof sentences (whichalways nite), use denote disjunction conjunctionsentences . Proofs often based structure formula, specicallydepth formula; formula , depth , depth() maximum nestingconnectives . Hence depth(a (b c)) = 3.Later make use notion prime implicants sentence. consistent setliterals prime implicant i: 6 .4limiting case , take (sole) prime implicant {}.2.2 Belief Revision Contractionseminal approach AGM (Alchourron et al., 1985), postulates proposedconstrain belief revision. approach, knowledge base K assumed beliefset, set sentences closed logical consequence. revision belief setformula, K , new belief set formula believed. interesting caseinitially believed, attain consistent belief set (assumingsatisable), beliefs dropped. Exactly beliefs must droppedstipulated AGM approach; however, constraints form postulatesgovern seen legitimate revision operators given. contrast,development belief update Katsuno Mendelzon (1992) represented knowledgebase formula language L. Hence, paper also express things termspostulates phrased terms formulas, rather belief sets.following R-postulates comprise Katsuno Mendelzons reformulationAGM revision postulates, function L L L.(R1) .(R2) satisable, .(R3) satisable also satisable.(R4) 1 2 1 2 1 1 2 2 .(R5) ( ) ( ).(R6) ( ) satisable ( ) ( ) .dual operation, called contraction also dened, formula deletedknowledge base. operation seen governed C-postulates,using Katsuno Mendelzon formulation terms function L L L.3. course formula , many different logically equivalent ways express cnf ()dnf (). assume fixed procedure converting cnf (or dnf), converting negation normalform, distributing disjunctions conjunctions (or vice versa dnf), hence justifying useterm conjunctive (disjunctive) normal form formula, rather (disjunctive) normalform.4. notion prime implicant confused dual notion prime implicate.prime implicate formula clause, disjunction literals, , propersubclause , 6 .761fiDelgrande, Jin, & Pelletier(C1) .(C2) 6 .(C3) 6 6 .(C4) 1 2 1 2 1 1 2 2 .(C5) ( ) .Revision contraction related AGM approach comeknown Levi Harper identities. may expressed follows (using formulasrather belief sets):( )(1)( ).(2)rst case asserts revising corresponds contraction conjoined . second asserts contracting corresponds disjunctionresult updated .Although makes nice picture, various objectionspresuppositions AGM model (e.g., representation belief states theories,is, innite sets formulas) postulates said governoperations revision contraction (especially (C5), postulate recovery). Issuesinvolved (C5) discussed Fuhrmann, 1991; Tennant, 1997; Hansson &Rott, 1998; Rott & Pagnucco, 1999, others.2.3 Belief Update Erasureaccount revision contraction described preceding subsection usually seenapplying straightforwardly case one store informationunchanging, static world new information world receivedagent, thereby forcing change representation unchanging, static world.dierent picture put forward Katsuno Mendelzon (1992),changing, dynamic world. conception, new information gatheredagent reects idea world dierent knowledgebase previously constructed. sorts changes knowledge baserequired type new information seen dierent sorts envisagedthought changes knowledge base going make contentssuccessively accurate. Although simplistic distinctiondierences two pictures (as mentioned Footnote 1), led largebody work point dierent conception. Distinct operations changeknowledge bases proposed: update, makes changes knowledge basegiven information concerning change state world, erasure, removingout-of-date information.formula said complete implies truth falsity everyformula. approach (Katsuno & Mendelzon, 1992), update functionL L L satisfying following U-postulates.762fiCompositional Belief Update(U1) .(U2) ( ) .(U3) satisable .(U4) 1 2 1 2 (1 1 ) (2 2 ).(U5) ( ) ( ).(U6) 1 2 2 1 ( 1 ) ( 2 ).(U7) complete ( 1 ) ( 2 ) (1 2 ).(U8) (1 2 ) (1 ) (2 )postulates not, however, uncontentious. Herzig Ri (1999) discussedplausibility postulates given; assert U2, U5, U6 undesirable,U7 unimportant. leaves (according authors) U1, U3, U4, U8desirable.Erasure also dened, manner analogous way described contractionrelated belief revision. cases, specied formula believedresult. erasure denoted , formula believedresulting state. operations, set postulates characterizingerasure (given Katsuno & Mendelzon, 1992). Update erasure also interdenablemeans identities, analogous Levi Harper identities, related revisioncontraction:( )(3)( ).(4)rst case asserts update corresponds erasing along conjunction. second asserts erasing corresponds disjoining resultupdated .various specic update (and revision) operators proposed baseddistance interpretations. focus two update operators, due Winslett.rst, Possible Models Approach (PMA) (Winslett, 1988) well-known example update operator satisfying Katsuno Mendelzon update postulates.second, standard semantics (Winslett, 1990) weak (in fact, arguably weakestreasonable) approach update. denote operators pma ss respectively.pma , that, interpretation w , pma selects interpretations closest w. update determined setclosest interpretations. notion closeness two interpretations w1 w2Hamming distance, given follows:Definition 1 diff (w1 , w2 ) = set propositional letters w1 w2 differ.763fiDelgrande, Jin, & PelletierInterpretation w1 less close w w2 , w1 w w2 , diff (w, w1 ) diff (w, w2 ).follows w partial order interpretations. w -minimal set respectdesignated in(M od(), w). specify PMA update operator:[od( pma ) =in(M od(), w).wM od()update operator ss dened model , modelsretain truth values atoms chosen. is:[od( ss ) ={w2 od() | diff (w1 , w2 ) atom()}w1 od()operator ss weakest reasonable update operator following sense(Winslett, 1990): First, update ss , true every model ss . Second,every model language excluding atoms model ss (againrestricted language). Moreover, ss consists maximal set interpretationssatises preceding two properties. Hence update , truth valuesatoms unaected update.Example 1 (Katsuno & Mendelzon, 1992) Let L = {b, m} language discourse. Let = (b m) (b m), = b. interpretations w1 = (b, m),w2 = (b, m); interpretations are: w1 = (b, m), w2 = (b, m). Thusdiff (w1 , w1 ) = {b} diff (w1 , w2 ) = {b, m}, hence w1 w1 w2 w2 6w1 w1 ,in(M od(), w1 ) = {w1 }. Similarly, in(M od(), w2 ) = {w2 }. Hence, ( pma ) b.result obtains ss .concreteness, take b mean book oor, mean magazineoor. means either book magazine oor,both. robot ordered put book oor. Intuitively, end actionbook oor, location magazine unknown.operators give result.Example 2 Let = (b m) = (b m). ( pma ) (b m), whereas( ss ) (b m).Here, neither book magazine oor. robot ordered putleast one oor. According pma operator, exactly oneoor action, according ss operator, least one oor.2.4 Forgetfocus specic approach update erasure, also relate approachforget operator. notion forgetting goes back George Boole (1854),though received recent attention Articial Intelligence by, e.g., Lin & Reiter,1994; Lin, 2001; Lang et al., 2003; Nayak et al., 2006. propositional context, forgetatom, set atoms, remove information concerning atom set atoms.764fiCompositional Belief Updatesuggested (in Nayak et al., 2006) forgetting corresponds removalliterals atoms language discourse case propositional forgetting (i.e.,0-place predicate forgetting). general case, seen removing predicaterelation language, hence removing consequences mightdue predicates presence.Let [p/q] denote formula occurrences atom p replaced q.usual denition forgetting (again, going back Boole) atom p given[p/] [p/]. order forget set atoms , one takes disjunctionsubstitution 2|| combinations , elements .following denitions. single atoms basically follow Nayak et al.(2006); sets atoms use denition (Lin & Reiter, 1994). begin,p-dual interpretation interpretation like truth value assignedp changed negation. set interpretations closed p-duals if,interpretation set, p-dual also set.Definition 2 Given set interpretations atom p, operatorleast set interpretations containing closed p-duals.U(, p) yieldsGiven this, dene forget atom set atoms, latter denedrecursively terms former:Definition 3 Basis Case: Let formula p atom. forget p respectgiven by:]od( p) =(M od(), p)= od([p/] [p/]).Inductive Case: Let formula = {p1 , . . . , pn } set atoms. forgetrespect given by:= ( ( \ {pn })) pn .example, (b c) (b c) b c. (Given knowledge basestored Alberta Canada also either Vancouver British ColumbiaCharlottetown Ontario, forgetting Alberta Canada would yield eitherVancouver British Columbia Charlottetown Ontario. wouldresult initial knowledge base Alberta Canada, eitherVancouver British Columbia Charlottetown Ontario.) another example,(a b) . last example illustrates forget distinct erasure, sinceproperty erasure imply ( ) (Katsuno & Mendelzon,1992).2.5 Belief MergingMerging diers formally preceding three pictures knowledge baseschanged. preceding operators knowledge base sentence may needoccasion change knowledge base. one rephrases terms agents,765fiDelgrande, Jin, & Pelletiertypes change postulate agent, store beliefs, facednew belief needs accommodated. case merging, however, startmany belief sets need dealt way yields best, overallsingle belief state. terms agents, again, number agents,belief set, trying construct belief set best represents totalbeliefs community agents. So, rather function maps belief setsentence onto belief set, instead function maps number belief setssingle one. Following earlier practice representing belief sets single formula(in manner Katsuno & Mendelzon, 1992), see earlier rationalesbelief change envision function L L L, whereas merging envisions functionL L . . . L L. Note general case allows knowledge baseslist identical one another, thus list actually multi-set (bag).goal merging, then, construct, nite list knowledge bases E,appropriate, single merged knowledge base. Despite formal dierenceearlier three types belief change, nevertheless include discussionconceptual similarities hold merging versions beliefchange. Indeed, seems plausible suggest merging might denable termsothers, maybe sort generalization others. cases,considerations compositionality belief change operators may relevant.Definition 4 knowledge set multi-set (bag) knowledge bases.VDefinition 5 E knowledge set, E conjunction formulas representing knowledge bases E.Konieczny Pino Perez (1998, 2002) proposed following M-principles governmerging operators. merge function function knowledge set E knowledgebase (E) satisfying following postulates, multiset union.5(M1) (E) consistentVV(M2) E consistent (E) = E.(M3) E1 E2 knowledge sets E1 E2 , (E1 ) (E2 )(M4) K1 K2 knowledge bases mutually consistent, (K1K2 ) 6 K1(M5) (E1 ) (E2 ) (E1 E2 )(M6) (E1 ) (E2 ) consistent, (E1 E2 ) (E1 ) (E2 )merging postulates contested: example, Meyer (2000) arguedM4 M6 rejected. (He argues grounds manyplausible merging operations obey postulates).5. simplicity, list postulates (Konieczny & Pino Perez, 1998), include integrityconstraints.766fiCompositional Belief Updatenatural method determining whether formula merged knowledge base determine whether appears majority members knowledge set merged (the merged knowledge base allow opinionmajority prevail). Liberatore Schaerf (1998) introduced method arbitration,whereby goal adopt many dierent opinions possible membersknowledge set (try take many diering opinions possible account). KoniecznyPino Perez (1998) proved arbitration operator (at least, sortcharacterize) obeys M1 M6.6 interplay various mergingoperations ability agent hide, lie, otherwise camouage preferencesagents try construct merged knowledge base surveyedEveraere et al. (2007).3. Approachsection discusses approach. Following intuitions motivation formalapproach, introduce compositional update and, subsequently, erasure. also considernotion compositional belief revision, conclude that, least respectspecic approach, separate, distinct, notion compositional revision. Analysisproperties operators covered next section.3.1 Intuitionsgoal dene update operators compositional fashion that, updatingformula , update dened terms syntactic components . general ideabehind update , model replaced closest model(s)(Katsuno & Mendelzon, 1992). approach, notion close modeldetermined part syntactic structure . is, recursively decomposed;resulting (base case) literals used determine models update sets literals;results combined depending connective(s) .Consider may carried out. given knowledge base sentence, wish determine new knowledge base believed. base case,= l literal, wish update knowledge base literal l. implies lneed nothing. imply l, wish arrive knowledge basel believed. is, want change knowledge base enoughentails l. Clearly, replacing model interpretation= ( {l}) {l}.7 Thus, would every resulting interpretation entails l.Consider next updating knowledge base conjunction literals = l1 l2 .knowledge base l1 l2 believed will, obviously, one every modelknowledge base entails l1 l2 . carry replacing interpretationod() interpretation = ( {l1 , l2 }) {l1 , l2 }. limiting caseneeds taken care of, l1 l2 . situation, interpretationl1 , l2 true, case exist, reecting attempt updateinconsistent formula.6. forms part rationale Meyer (2000, 2001) deny M4 M6.7. clear, |= l = ; 6|= l like l replacing complement.767fiDelgrande, Jin, & Pelletierupdate knowledge base disjunction literals = l1 l2 , wantmodify models least one l1 l2 true. Consider od()6|= l1 l2 . 1 = ( {l1 }) {l1 } interpretation involves least changel1 true, 2 = ( {l2 }) {l2 } l2 . Arguablyreplaced 1 2 .Last, generalize considerations deal arbitrary formulas.update disjunction formulas, recursively determine update givenindividual disjuncts return union resulting sets interpretations.3.2 Compositional Update OperatorBased preceding intuitions, dene update operator c . beginpreliminary denitions. following, UL function interpretationnite set formulas set interpretations. Informally, model knowledgebase set formulas resulting partial decomposition formulaupdate. value UL set interpretations closest , according . easenotation, case single formula sometimes write UL(, ) UL(, {}).Definition 6 interpretation finite L, define UL(, ) follows:1. LitsUL(, ) ={( ) } 6otherwise2. = { } UL(, ) = UL(, {, } )3. = { } UL(, ) = UL(, {} ) UL(, {} )4. = {( )} UL(, ) = UL(, {, } )5. = {( )} UL(, ) = UL(, {} ) UL(, {} )6. = {} UL(, ) = UL(, {} )worth noting recursion steps denition resemble closelyprocedure use convert formula disjunctive normal form. deningupdate terms operator, rst investigate properties. Foremost,need show UL well-dened. is, specifying UL(, ), denitionphrased terms member ; needs shown orderelements selected recursion aect result.Theorem 1 UL well-defined.next two results reect inuence structure formula recursivedecomposition denition UL.VTheorem 2 UL(, ) = UL(, nnf ( )).VTheorem 3 UL(, ) = UL(, dnf ( )).768fiCompositional Belief UpdateNote similar result extend conjunctive normal form. counterexamplegiven following:UL(, {a (b c)}) = UL(, {a}) UL(, {b, c})6= UL(, {a}) UL(, {a, c}) UL(, {b, a}) UL(, {b, c})= UL(, {a, c}) UL(, {b, c})= UL(, {(a b), (a c)})= UL(, {(a b) (a c)}).consider next couple fundamental properties UL:Theorem 4 every w UL(, ) w |= .Theorem 5 UL(, ) = iff .next dene update operator directly terms UL.Definition 7od( c ) = { | UL(, {}), od()}.Recall Example 1 = b = (b m) (b m). od( c) = { | UL(, {}), od()} = {( {b}) {b} | od()}. Thus,od( c ) = {{b, m}, {b, m}}, ( c ) b. result obtainWinsletts approaches.8Example 2, = b = (b m), obtain od( c ) ={{b, m}, {b, m}}. case, update operator behaves pma , differently ss .similarly dene erasure operator via UL. erase , analogyHarper Identity, one update add result . Thus:Definition 8od( c ) = od() { | UL(, {}), od()}.get results:Theorem 6c ( c )c ( c ).8. note however approaches differ. Specifically, PMA update operator satisfiesKM postulates, whereas operator not; see Section 4 details.769fiDelgrande, Jin, & Pelletier3.3 ErasureDenition 8 dened dual update, called erasure, directly terms UL.equally well dene function analogous UL, call EL, directly dene erasureoperator rst principles. now, toward denition erasure. Briey,motivation is: want erase consequence , semantically wantadd interpretations models . corresponds single literal,model would want add interpretation l replaced l.corresponds conjunction, erased erasing either conjuncts;corresponds disjunction, erase disjuncts must erased. continuingfashion obtain following denition:Definition 9 interpretation finite L, define EL(, ) follows:1. LitsEL(, ) =W6{( ) }otherwise2. = { } EL(, ) = EL(, {} ) EL(, {} )3. = { } EL(, ) = EL(, {, } )4. = {( )} EL(, ) = EL(, {} ) EL(, {} )5. = {( )} EL(, ) = EL(, {, } )6. = {} EL(, ) = EL(, {} )following results analogous Theorems 2 3; note occurrence cnfTheorem 8, contrast dnf Theorem 3.VTheorem 7 EL(, ) = EL(, nnf ( )).VTheorem 8 EL(, ) = EL(, cnf ( )).directly dene erasure operatorDefinition 10 od(c )cterms EL:= od() { | EL(, {}), od()}Unsurprisingly, notion erasure given Denition 8 equivalent.show rst establishing following result:VLemma 1 interpretation L, EL(, ) = UL(, { })this, follows notions erasure given via Harper Identity,direct denition via Denition 9 coincide:Theorem 9 cc .Hence use symbolwell-denedness c .9cerasure. corollary, Theorem 9 also establishes9. is, since UL well-defined (Theorem 1),equivalence.770c(Definition 8) hencecfiCompositional Belief Update3.4 Revisionsection consider extending compositional approach belief revision.begin, might pointed nothing underlying motivationmakes c update operator, point suggests c might also regardedrevision operator, albeit weak properties. However, regardless intuitions, recursive decomposition implicit Denition 6 yields operator update-like properties,sentence update , one eectively deals models disjunctsdnf (). revision, contrast, intuition one deals models(in sense) closest knowledge base . Hence, operator creally appropriate revision operator.suggests possibly-feasible approach dening compositional revision: denerevision , one rst uses operator c nd candidate set models ,employs distance function determine subset models closestmodels whole. is, formulas , , update dened (in onefashion another) respect models . revision contrast, denitionrevision makes reference subset models ,closest (in sense) models . sense then, update logically weakeroperator revision. Thus revision operator dened respectrst applying (compositional) update operator get candidate set models .set ltered, removing models minimal distanceclosest models . depending notion distance employed, one mightexpect obtain dierent revision operators given compositional update operator.two common notions distance used model-based belief change,one based set containment cardinality. rst case, formulas, , denemin (, )=min ({M1 M2 | M1 od(), M2 od()}),sets B, AB symmetric dierence B. Satohs (1988)revision operator dened follows.Definition 11od( ) = {w od() | w od(), ww min (, )}.example, let = abc let = a(bc). = (abc)(abc).dene corresponding compositional revision operator follows:Definition 12od( ) = {w | w UL(w, {}), w od(), ww min (, )}.However, turns revision operator fact coincides Satoh revision operator:Theorem 10 .771fiDelgrande, Jin, & Pelletierfollows straightforward corollary use distance metric based number diering propositional symbols two interpretations, obtain revisionoperator (Dalal, 1988).10 obvious approaches compositional revision,obtain new revision operators; say, recursive decompositiondenition UL serve select among models interesting senserespect revision.However, considerations lead one interesting result, pointway algorithms may eciently compute Satoh Dalal revision:compute Satoh revision example, one use Denition 6 determine relevantsubset models , use min (, ) determine closest subsetmodels set models . discuss Section 5, initial ltering modelsmay done eciently certain syntactically-restricted cases.4. Analysis Compositional Update Erasurestart, consider Katsuno-Mendelzon update postulates operatorsatises. consider set corresponding compositional erasure postulates,since results analogous update postulates, limitedadditional interest. considering update postulates, explore updateerasure operators, including properties resulting restriction syntacticform formula update, comparison related approaches.Theorem 11 c satisfies U1, U3, U5, U7, U8.counterexample U2, consider rst example given above, illustratingapproaches Winslett, c = (b m) (b m) = b m.approach, update (as given Denition 7) rst disjunct viz., b,yields interpretations {b, m} {b, m} update second disjunct, m, givesinterpretations {b, m} {b, m}. Hence c (bm) characterized interpretations{b, m}, {b, m}, {b, m} get c (b m) (b m). U2 would dictateresult ; however, example suggests U2 problematiccontext update. borrow example works (Herzig & Ri, 1999; Brewka& Herzberg, 1993), suppose agent believes p (that certain coin shows heads).world changes toss coin (where agent see result).Letting p coin shows tails, note agent believe (p p). Yetnote p (p p); U2 would stipulate p (p p) p, contrarywant. operator c , hand, includes additional model. appearsmake sense, updating b really telling knowledge baseworld changed one b b b true. Thus, caseupdate operator behaves like Gricean belief change operator Delgrande, Nayak,Pagnucco (2005), goal incorporate new information.10. is, fixed formulas, model Dalal revision model Satoh revision. RephrasingDefinitions 11 12 cardinality-based distance gives result analogous Theorem 10 Dalalrevision. omit details.772fiCompositional Belief Updatenote modify c operator simple fashion satisfy U2 follows:11c =c otherwisepurposes, although U2 indeed satised, modication sheds lightoriginal goal investigating ramications developing compositional updateoperator, pursue modication.next consider counterexample U4. Although ((a b) b) b, nonethelessod(ac ((ab)b)) = {{a, b}, {a, b}} od(ac b) = {{a, b}}. U4 satisedsince compositional approach parts sentence may provide implicit resultsexplicit sentence. Consider (ab)(bc) illustrate point. Updatingsentence eected updating individual components, viz., (a b)(b c). However, implicit parts fact (a c) also true,addition (implied) sentence would aect result update. considerbehaviour below.counterexample U6 given following. Let= ab1 = (a a)2 =od((a b) c (a a)) = od()also have:od((a b) c ) = od(a b)case c 1 2 also c 2 1 . Thus antecedent conditionsU6 satised, c 1 c 2 .c satisfy U4 (substitution logical equivalents) general,satisfy weaker conditions. First, update obviously satises substitution logicalequivalents rst argument c . well, light Theorems 2 3, 12 share negation normal form disjunctive normal form, maysubstituted one formula update. summarize resultsfollows:Observation 11. 1 2 (1 c ) (2 c ).2. nnf (1 ) = nnf (2 ) ( c 1 ) ( c 2 ).3. dnf (1 ) = dnf (2 ) ( c 1 ) ( c 2 ).11. Borgida (1985) employed similar definition respect revision operator.773fiDelgrande, Jin, & PelletierDespite failing satisfy postulates (which, noted, overlappostulates Herzig & Ri, 1999, think undesirable), c exhibit nice property,reecting compositional nature operator, operators appearingliterature satisfying Katsuno Mendelzon postulates fail satisfy. followingversion disjunction property holds.Theorem 12 c (1 2 ) ( c 1 ) ( c 2 )Corollary 1 ( c 1 ) ( c 2 ) implies c (1 2 ).corollary observed strengthening U7.update operator satises postulates deemed desirable Herzig Ri(1999), exception U4. discussed above, U4 satised dueinteraction parts sentence. would seem could compile implicitinformation sentence would obtain full substitution equivalents,expressed U4. So, one way satisfy U4 redene c rst getinformation implicit interaction compositionally distinct parts updatesentence. dening operators consider set prime implicantssentence. call modied operator pic . Let P I() set prime implicants .WDefinition 13 piP I()c = cTheorem 13 pic satisfies U4Although pic satises U4, lose U7. counter-example U7 given= abcd1 = (a d) (c d)2 = (a d) (c d).od( pic 1 ) = {{a, b, c, d}, {a, b, c, d}}od(pic2 ) = {{a, b, c, d}, {a, b, c, d}}piHence od( pic 1 ) od( c 2 ) = {{a, b, c, d}}. handpiod( pic (1 2 )) = od( c ((a d) (c d) (a d) (c d)))= od( pic d)= {{a, b, c, d}}.Conversion prime implicants eect removes irrelevant redundant syntactic information, illustrated preceding example 1 2 fact equivalentatom d. pursue line inquiry considering, formula update, syntactic representation proposition expressed language .given formulaWV , recall odL() set models , language .formulaodL() would formula expressed disjunctive normal form;example odL((a b) c) would expressed (a b c) (a b c) (a b c).dene update operator follows:774fiCompositional Belief UpdateWVDefinition 14 od( ssodL())).c ) = od( c (obtain update operator fact Winsletts standardsemantics:Theorem 14 ssc ss .pursue direction one step further, dene update operatorupdate formula characterized models expressed dnf. dene:WV) = od( c (od())).od( trivcDenition 14, except models , rather modelslanguage . However easily shown interesting operator, sinceremoves old information have:) .Observation 2 ( trivcnext proceed slightly dierent direction compare update operatorforget operator. Recall forgetting set atoms formula basically removesinformation set atoms; sense forgetting analogousdecreasing language set atoms.begin with, noted update operator contraction-likeproperties similar forget. example, (a b) c (a a) readily shown equivalentb. again, sense, update read updating precisely a,case would indicate tautologous information concerning a. fact,following result (recall use denote forget):Theorem 15 Let L let P.^= c (p p).pHence forgetting set atoms special case update operator. (Given Theorem 6, forget course also expressible via erasure analogous fashion.) Last,establish result forget operator Winsletts standard semantics.hindsight obvious, result appear previously noted.VTheorem 16 formula P, let = l (l l).= ssc .summary, observed discussion obtainedhierarchy operators, based extent information made explicit.basic case, triv, update formula syntacticcrepresentation models language; trivial update operator results.basic interesting operator given ssc , Winsletts standardpisemantics, followed c c . well, introducing tautologies, alsocapture notion forgetting atoms.already noted update operator c distinct Winslett PMAapproach. best knowledge also distinct specic approachesappearing literature, including surveyed (Herzig & Ri, 1999).775fiDelgrande, Jin, & Pelletier5. Algorithms Complexitysection present syntactic characterization well algorithm computingcompositional update. also analyze complexity algorithm varietyassumptions. Specically, analyze complexity algorithm appliedpropositional sentences general, sentences disjunctive normal form,sentences whose sizes bounded specied constant.start background notions. Recall [p/q] denotes formulaoccurrences atom p replaced q. write p. denote formula[p/] [p/]. P = {p1 , , pn } set atoms P., called eliminantP , stands p1 .( (pn .)) (Brown, 1990). Intuitively, eliminant Pviewed formula representing knowledge concernedatoms P . eliminated information members P replacingtwo possible values, , thus leaving information .shown Winsletts standard semantics syntactically capturedbased notion eliminant (Doherty, Lukaszewicz, & Madalinska-Bugaj, 1998). LetP = atom(),ss ( P.)(5)5.1 Syntactic Characterization Algorithmsready provide syntactical characterization compositional update.idea quite similar (Doherty et al., 1998). However, approach rst convertsupdate formula disjunctive normal form, deals disjunct._U pdate(, ) = {( P.) | dnf (), P = atom(t)}following results establish correspondence semantical denitionsyntactic characterizations compositional update.Lemma 2 Suppose term (a conjunction literals) P = atom(t).od(( P.) t) = {w | w UL(w, t), w od()}Theorem 17 od( c ) = od(U pdate(, )).Corollary 2 c U pdate(, )need compute compositional update, therefore, ability computeeliminants. proposed Brown (1990), eliminant P. constructed follows.1. Convert dnf t1 tn (each ti conjunction literals)2. Replace ti ti P .ready provide algorithms compositional update. assumednf () refers disjunctive normal form represented clause form,formula represented sets sets literals. case, members dnf ()implicitly disjoined, set literals making member dnf () implicitlyconjoined.following algorithms, let , L P set atoms:776fiCompositional Belief UpdateAlgorithm Eliminant(P, )1.2.term dnf ()3.4.literal l/P5.l/ P l6.l7.8.returnAlgorithm U pdate(, )1.2.term dnf ()3.P = atom(t)4.(Eliminant(P, ) t)5.returnLets consider Example 1 = b = (b m) (b m).U pdate(, ) = (Eliminant({b}, ) b). Since Eliminant({b}, ) =( m) ( m), equivalent , obtain U pdate(, ) b. Thus,U pdate(, ) c , already shown c b.Example 2, = b = (b m), obtain U pdate(, ) =(b m) (b m). Again, result obtain c .5.2 Complexitysequel, analyze space complexity update algorithm; is,interested large updated knowledge base could be. Unfortunately, appliedarbitrary formulas, algorithm U pdate may cause exponential space blowup,disjunctive normal form formula could exponentially large.Theorem 18 space complexity U pdate(, ) O(2(||+||) ) , L;However, able show exponential space blowup inevitablealgorithm compositional update. end, need introduce so-called advice-takingTuring machine (TM) non-uniform complexity class, see (Johnson, 1990).advice-taking TM TM advice oracle, consideredfunction positive integers strings. input x, machine loads string a(|x|)continues usual based two inputs x a(|x|). Note oracle stringa(|x|) depends size input x. call advice oracle polynomial|a(n)| < p(n) xed polynomial p positive integers n. X usualcomplexity class dened terms resource-bounded machines (e.g., P NP) X/polyclass problem decided machines resource boundaugmented polynomial advice oracles. class X/poly also known non-uniformX; particular, P/poly appears much powerful P. However,shown unlikely NP P/poly, otherwise polynomial hierarchy would collapse777fiDelgrande, Jin, & Pelletierp2 (Karp & Lipton, 1980). result used show unlikely existsalgorithm compositional update polynomial space bound.Theorem 19 Assume exist polynomial p algorithm Update compositionalupdate U pdate(, ) c |U pdate(, )| p(|| + ||), beliefbase formula . NP P/poly.pursue result one step further, show algorithms sensible update operators cause exponential blowup. Formally, say update operatorsensible consistent set literals :^od() = { | (w ) , od()}Arguably, condition intuitive natural (cf. discussions Section 3).fact, almost update operators literature sensible.Theorem 20 exists polynomially space bounded algorithm sensible updateoperator, NP P/poly.remark result also proves Winsletts conjecture statingexist polynomially space bounded algorithm standard semantics (see Winslett,1990).algorithm becomes tractably better applied formulas disjunctive normalform, update formulas whose sizes bounded.Theorem 21 space complexity U pdate(, ) is:1. O(|| ||) , dnf;2. O(||) dnf || < k constant k.Arguably, practice, update formula (representing changes world)relatively small. Therefore, relatively easy convert dnf, also reasonableassume size bounded. usually restrict size beliefbase , converting dnf could computationally much expensive. Fortunately,need compile (o-line) original belief base dnf, outputU pdate algorithm automatically dnf updated belief base. considerablyfacilitate update belief base.6. Conclusionpresented belief change operators updating knowledge base denition operators compositional respect sentence added. intentprovide operators transparent denitions, based structure formulabelief change. result lose standard postulates update, althoughsatisfy core group standard postulate set. achieve full irrelevancesyntax sentence update replaced disjunction prime implicants.778fiCompositional Belief Updateapproach interesting rst, founded diering intuitionsoperators, based decomposition formula rather modelsformula, second, allows straightforward (under reasonable assumptions)ecient implementation. distinct previous update operators appearedliterature, capture Winsletts standard semantics approach updaterestriction approach. fact, update operator, dierent syntactic restrictions, may regarded constituting family update operators Winslettsstandard semantics weakest interesting approach. turn update revision, discover new, interesting compositional revision operator; nevertheless,results indicate rst computing compositional update, one implementSatoh Dalal revision operator eciently, consider subsetmodels formula revision, certain cases signicantspeedup naive algorithm.open question concerns combining approach one designed exploitstructure knowledge base (such discussed Parikh, 1999 characterizedterms PMA updates Peppas, Chopra, & Foo, 2004). second, technical questionfully explored concerns behaviour c erasure operator. example,let = (a b) (a b). Then, get c (a b) b. So, updatingknowledge base formula already implied knowledge base, actuallyremoved information. This, discussed earlier, quite reasonable one considersupdate (in contrast revision) b asserts world changedone {a, b}, {a, b}, {a, b} true. Finally, would interest applycompositional approach merging knowledge bases.Acknowledgmentsearly precursor paper presented FLAIRS 2007 (Delgrande, Pelletier,& Suderman, 2007). grateful audience comments; alsobeneted perceptive comments three JAIR referees. Delgrande Pelletieralso acknowledge support Canadian NSERC granting agency.779fiDelgrande, Jin, & PelletierAppendix A. Proof TheoremsProof 1.Observe UL associative commutative respect top-level conjunctionstop-level disjunctions. is, exampleUL(, { ( )} ) = UL(, {( ) } ).similar observation made negations top-level conjunctionsdisjunctions; exampleUL(, {( ( ))} ) = UL(, {(( ) )} ).use basic facts without comment sequel.means particular showing order-independence ULrespect second argument, need consider general caseVof UL(, )= {1 } {2 }, since UL(, {1 , . . . , n }) = UL(, {1 , ni=2 }).Given preamble, need show formulas 1 2 ,UL(, {1 } {2 }) independent whether initial recursion terms 12 .proof depth formula.BASE:Assume depth(1 ) 1 depth(2 ) 1:1. depth(1 ) = depth(2 ) = 0 1 , 2 atoms result follows trivially.2. connective 1 , 2 negation 1 , 2 literalsresult follows trivially.3. connectives 1 , 2 {, }, 1 , 2 reduce sets literals,previous case applies.4. 1 a1 a2 , 1 literal, Step 3 denition applies,result obtains easily. converse 2 a1 a2 , 2 literal courseyields result.5. 1 a1 a2 2 b1 b2 ,UL(, {a1 a2 } {2 }) = UL(, {a1 , a2 } {2 })= UL(, {a1 , a2 } {b1 b2 })= UL(, {b1 b2 } {a1 , a2 })= UL(, {b1 } {a1 , a2 }) UL(, {b2 } {a1 , a2 })= UL(, {b1 } {a1 a2 }) UL(, {b2 } {a1 a2 })= UL(, {b1 b2 } {a1 a2 })= UL(, {b1 b2 } {1 })780fiCompositional Belief Update6. 1 a1 a2 2 b1 b2 ,UL(, {a1 a2 } {2 }) = UL(, {a1 } {2 }) UL(, {a2 } {2 })= UL(, {a1 } {b1 b2 }) UL(, {a2 } {b1 b2 })= UL(, {b1 b2 } {a1 }) UL(, {b1 b2 } {a2 })= (UL(, {b1 } {a1 }) UL(, {b2 } {a1 }))(UL(, {b1 } {a2 }) UL(, {b2 } {a2 }))= UL(, {b1 , a1 }) UL(, {b2 , a1 })UL(, {b1 , a2 }) UL(, {b2 , a2 })Analogous manipulations show UL(, {b1 b2 } {1 }) yields result.STEP:induction hypothesis, assume result holds depth(1 ) ndepth(2 ) n. show desired result obtains depth(1 ) (n + 1)depth(2 ) (n + 1).A: Consider rst depth(1 ) n depth(2 ) = n + 1.1. 2 form :UL(, {1 } {2 }) UL(, {1 } {}), result followsinduction hypothesis.2. 2 :UL(, {1 } {2 }) = UL(, {1 } { }) = UL(, {1 , , }) = UL(, { }{1 }).3. 2 :UL(, {1 } {2 }) = UL(, {1 } { }) = UL(, {1 , }) UL(, {1 , })UL(, {2 } {1 }) = UL(, { } {1 }) = UL(, {1 , }) UL(, {1 , }).4. 2 ( ) 2 ( ):handled respectively.B: Consider next depth(1 ) = n + 1 depth(2 ) = n + 1.1. 1 2 :1 2 , result holds via inductionhypothesis.2. 1 2 1 1 :Assume without loss generality 1 1 1 .(a) 2 2 2 :UL(, {1 } {2 }) = UL(, {1 1 } {2 2 }) = UL(, {1 , 1 , 2 , 2 }) =UL(, {2 } {1 }).781fiDelgrande, Jin, & Pelletier(b) 2 2 2 :case handled base case, 1 , 1 , 2 , 2 atoms.(c) 2 (2 2 ) 2 (2 2 ):handled 2 2 2 2 respectively.3. 1 2 :proof base case, , atoms.4. 1 (2 ) ( ) 1 (2 ) ( ):handled respectively.Since covers cases, result follows induction.Proof 2.proof follows straightforwardly observations arbitrary , ,have:UL(, {} ) = UL(, {} )UL(, {( )} ) = UL(, { } )UL(, {( )} ) = UL(, { } )induction argument establishes value UL doesnt change conversionelements second argument negation normal form.Proof 3.result follows preceding, plus fact arbitrary ,that: UL(, { ( )} ) = UL(, {( ) ( )} ); is, UL invariantdistribution conjunction disjunction.Proof 4.Proof induction maximum depth formula .maximum depth 0, members literals, result immediateDenition 6. Otherwise induction hypothesis result holdsmaximum depth formula n, step easily shown appeal truthconditions classical propositional logic.Proof 5.Right-to-left: corollary Theorem 4.Left-to-right:arbitrary Theorem 3 UL(, ) = UL(, dnf ()).Let dnf () = 1 n conjunction literals.Via Denition 6 UL(, dnf ()) = UL(, {1 }) UL(, {n })., contains complementary pair literals UL(, {i }) = ; otherwiseUL(, {i }) 6= .782fiCompositional Belief Updateassume 6 , complementary literals, consequently UL(, {i }) 6= =6 UL(, dnf ()) = UL(, ).Thus 6 implies UL(, ) 6|= , shown.Lemma 3 c .Proof Lemma 3. result immediate.Consequently assume satisable, let od( ). showod( c ). Given Denition 7, since already od(), needshow UL(, {}).assumption od(), whence (Theorem 3) od(dnf ()).Since od(dnf ()) = od(dnf (1 )) od(dnf (n )) dnf () = 1 nget od(dnf (i )) disjunct dnf ().Since od(dnf (i )) follows denition UL = UL(, {i }); henceod(dnf ()) UL(, {}), shown.Proof 6. second part theorem follows immediately Denitions 7 8.rst part: Since c (Lemma 3),c ( ) ( c )( ) (( c ) )( ( c ))( c )last step applies part theorem, established above.Proof 7.proof Theorem 2 minor modications.Proof 8.proof analogous Theorem 3, omitted.Proof Lemma 1.proof straightforward, except setting induction bit ddly. induction based maximum depth formula . L, let depth() =max depth(). stipulate precedes ordering inductiondepth() < depth( ), depth() = depth( ) = n number formulasdepth n less .BASE:Let set literals.WEL(, ) = = UL(, { }).783fiDelgrande, Jin, & PelletierW6 then:EL(, ) = ( )= ( )= UL(, )n ^= UL ,STEP:Assume result holds rst n sets formulas ordering, letformula maximum depth . Let = \ {}.= ,EL(, ) = EL(, { } )= EL(, {} ) EL(, {} )= EL(, {({} )}) EL(, {({} )})= UL(, { ({} )}) UL(, { ({} )})= UL(, { ( )}) UL(, { ( )})= UL(, { ( ) ( )})= UL(, { ( )})= UL(, {( ( ))})= UL(, {()}).change EL UL justied induction hypothesis; otherwisesteps denition UL EL, simple manipulation.= ,EL(, ) = EL(, { } )= EL(, {, } )= UL(, { ({, } )})= UL(, {()}).Again, change EL UL justied induction hypothesis.cases handled analogously; proofs omitted.Hence result follows induction.Proof 9.od( c ) = od() { | UL(, {}), od()}= od() { | EL(, {}), od()}= od(c )rst last steps justied Denitions 8 10 respectively; middlestep follows Lemma 1.784fiCompositional Belief UpdateProof 10. follows immediately Denitions 11 12 od( ) od().show converse, let w od( ) show w od( ).Given Satoh revision operator satises irrelevance syntax (R4),assume without loss generality dnf; i.e. = 1 nconjunction literals.assumption w od( ); hence w od() wwmin(, ).Since w od( ), w od() = od(1 n ). Thusclause , , w od(i ). Assume without loss generalitysubset-minimal among sets literals making disjuncts .show w UL(w, {i }) shown w satises conditionsmember od( ). show follows. Let Ui set literals .l 6 Ui l w l w (since otherwise would contradictww min (, )).follows (w Ui ) Ui = w . means w UL(w, Ui ), wUL(w, {ui }) w UL(w, {}).Hence w od( ), shown.Proof 11.U1: Theorem 4 , UL(, {}) |= every od(), whence od(c ) od()c .U3: assumption 6 , od() 6= . result follows immediatelyTheorem 5 Denition 7.U5: od( c ) od() = result follows vacuously.Otherwise, let od( c ) od().Since Vod( c ) then, Theorem 3, exists od() Litsdisjunct dnf () = ( ) .Vod() set literals disjunct dnf ().Vdenition, clause dnf (). note = ( ( ))( )Denition 7 od( c ( )).U7:od( c 1 ) od( c 2 )od( c 1 ) od( c 2 )od( c (1 2 ))last step follows Theorem 3, using fact dnf () = dnf ()dnf ().Hence ( c 1 ) ( c 2 ) implies c (1 2 ).785fiDelgrande, Jin, & PelletierU8:od((1 2 ) c ) = { | UL(, {}), od(1 2 )}= { | UL(, {}), od(1 ) od(2 )}= { | UL(, {}), od(1 )}{ UL(, {}), od(2 )}= od(1 c ) od(2 c ).od((1 2 )c ) = od(1 c )M od(2 c ) follows ((1 2 )c )(1 c ) (2 c ).Proof 12.od( c (1 2 )) = { | UL(, {1 2 }), od()}= { | UL(, {1 }), od()}{ | UL(, {2 }), od()}= od( c 1 ) od( c 2 )= od(( c 1 ) ( c 2 ))Proof 13.need show 1 2 1 2 (1 c 1 ) W(2 c 1 ).12 Since (Since Wassumption,PI()=PI().121W21 c W P I(1 ))W(2 c W P I(1 )) (2 c P I(1 )) (2 c P I(2 )), (1 c P I(1 ))pi(2 c P I(2 )), whence ( pic 1 ) ( c 2 ).Proof 14.WVod( ssc (V ( odL()))). Using Denition 7,c ) = od( Wright hand side equal { | UL(, { ( odL())}), od()}. Hence,od() replaced set interpretations = {( ) |odL()}. say, od() replaced set interpretationsdier language . denitionod( ssc ).Proof 15.Let = {p1 , . . . , pn }.= {p1 pn }= [( {p1 pn }) c pn ] [( {p1 pn }) c pn ]12. Equality isnt quite right here. Rather equality modulo associativity commutativity,need result.786fiCompositional Belief Updatesecond step Denition 3 forget expressed terms update. Denition 3 successively reapplied eventually terminate disjunction 2ndisjuncts, disjunct sequence n updates literals . Moreover, everymaximum consistent set literals appears disjunct.easy result, state without proof, disjoint sets literals 1 ,2 , ( c (1 )) c (2 ) = ( c (1 2 )).Hence get nally^=c (() ( \ ))= c^(p p).pProof 16.Theorem 15= c^!(a a).= c .Observation1 get c = c dnf (). easy argument showsWVdnf () =odL(), Denition 14 yields c dnf () = ssc . Theorem 14ssssc = ss . Putting together get () = c .Proof Lemma 2.Equation (5) implies od(( P.) t) = od( Wt). According Theorem 14ss VDenition 14, od( ss t) = od( c ( ( odL(t)))) = od( c t).Denition 7, follows od(( P.) t) = {w | w UL(w, t), w od()}.Proof 17.According Denition 7, od(c ) = {w | w UL(w, {}), w od()}. Theorem 3, {w | w UL(w, {}), w od()} = {w | w UL(w, {dnf ()}), wod()}. Denition 6, follows od( c ) = {w | w UL(w, t),dnf (), w od()}. According Lemma 2, thus od( c ) = od(U pdate(, )).Proof 18.size dnf () O(2|| ). Hence, size Eliminant(P, ) also O(2|| ). Similarly, size dnf () O(2|| ). Therefore, |U pdate(, )| = O(2|| 2|| ) = O(2||+|| ).Proof 19.proof inspired ideas (Cadoli, Donini, Liberatore, & Schaerf, 1995),shown many revision operators cause exponential blowup. show787fiDelgrande, Jin, & Pelletierexists polynomially space bounded algorithm compositional update, 3SATP/poly.13 proof consists two steps.STEP 1:integer n, rst construct belief base n formula n , whose sizespolynomial wrt. n. Let X = {x1 , , xn } = {y1 , , yn } two disjoint setatoms let C set new atoms 3-literal clause X, i.e., C = {ci |3-literal clause X}. obtain n n follows:n = V{i ci | 3-literal clause X}n = ni=1 (x1 yi )easy see |n | O(n3 ) |n | O(n).show 3CNF size n, exists interpretation (onatoms X C) |= n c n satisable. assume, without lossgenerality, atom() X; otherwise, always substitute atoms respectivelyelements X obtain new sentence X satisable X satisable.w obtained follows:w = {ci C | clause } {ci C | clause } Xshow satisable |= n c n .Assume satisable. Let model . construct another interpretation= UL(, {ci C | clause }). easy see |= n= UL( , {xi , yi | 1 n}). follows |= n c n .Assume |= n c n . exists interpretation |= n= UL(, {xi , yi | 1 n}). claim |= . Assume 6|= .exists 3-literal clause 6|= . = UL(, {xi , yi |1 n}) ci , follows ci . implies 6|= ci ,contradicts |= n . Thus, indeed satisable.STEP 2:Suppose U pdate polynomial space bounded algorithm compositional update.3SAT solved advice taking TM follows: Given arbitrary 3CNFsize n, machine rst loads advice string U pdate(n , n ) computes (in polynomialtime) ; veries |= U pdate(n , n ). Since |n | O(n3 ), |n | O(n),|U pdate(n , n )| p(|n | + |n |), verication polynomial time. SinceU pdate(n , n ) n c n , |= n c n |= U pdate(n , n ). Therefore,satisable |= U pdate(n , n ). shows 3SAT P/poly. 3SATNP-complete, NP P/poly.Proof 20.13. 3-literal clause clause consists precisely 3 literals 3CNF conjunction 3-literal clauses.3SAT satisfiability problem 3CNFs, shown NP-complete.788fiCompositional Belief Updateproof exactly Theorem 19, update formula n usedconsistent conjunction literals.Proof 21. Since , dnf, || = |dnf ()| || = dnf (). Thus |Eliminant(P, )| =O||. Therefore |U pdate(, )| = O(|| ||).case || < k, |U pdate(, )| = O(|| k) = O().ReferencesAlchourron, C., Gardenfors, P., & Makinson, D. (1985). logic theory change:Partial meet functions contraction revision. Journal Symbolic Logic, 50 (2),510530.Boole, G. (1854). Investigation Laws Thought. Walton, London. (ReprintedDover Books, New York, 1954).Borgida, A. (1985). Language features exible handling exceptions informationsystems. ACM Transactions Database Systems, 10.Brewka, G., & Herzberg, J. (1993). things worlds: formalizing actionsplans. J. Logic Computation, 3, 517532.Bright, M., Hurson, A., & Pakzad, S. (1992). taxonomy current issues multidatabase systems. Computer, 25, 5059.Brown, F. (1990). Boolean Reasoning. Kluwer Academic Publishers.Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1995). size revisedknowledge base. PODS 95: Proceedings fourteenth ACM SIGACT-SIGMODSIGART symposium Principles database systems, pp. 151162, New York, NY,USA. ACM Press.Dalal, M. (1988). Investigations theory knowledge base revision.. ProceedingsAAAI National Conference Artificial Intelligence, pp. 449479, St. Paul,Minnesota.Delgrande, J., Nayak, A., & Pagnucco, M. (2005). Gricean belief change. Studia Logica, 79,97113.Delgrande, J., & Schaub, T. (2003). consistency-based approach belief change. Artificial Intelligence, 151 (1-2), 141.Delgrande, J., Pelletier, F., & Suderman, M. (2007). Compositional belief update.Proceedings FLAIRS-20, pp. 6873, Key West.Doherty, P., Lukaszewicz, W., & Madalinska-Bugaj, E. (1998). PMA relativizingchange action update. Cohn, A. G., Schubert, L., & Shapiro, S. C. (Eds.),Proceedings International Conference Principles Knowledge Representation Reasoning, pp. 258269. Morgan Kaufmann, San Francisco, California.Everaere, P., Konieczny, S., & Marquis, P. (2005). Quota Gmin merging operators.Proceedings International Joint Conference Artificial Intelligence, pp.424429, Edinburgh.789fiDelgrande, Jin, & PelletierEveraere, P., Konieczny, S., & Marquis, P. (2007). strategy-proofness landscapemerging. Journal Artificial Intelligence Research, 28, 49105.Forbus, K. (1989). Introducing actions qualitative simulation. ProceedingsInternational Joint Conference Artificial Intelligence, pp. 12731278.Fuhrmann, A. (1991). Theory contraction base contraction. Journal Philosophical Logic, 20, 175203.Gardenfors, P. (1988). Knowledge Flux: Modelling Dynamics Epistemic States.MIT Press, Cambridge, MA.Hansson, S. O., & Rott, H. (1998). Beyond recovery. Erkenntnis, 49, 387392.Herzig, A., & Ri, O. (1999). Propositional belief update minimal change. ArtificialIntelligence, 115 (1), 107138.Johnson, D. (1990). catalog complexity classes. van Leeuwen, J. (Ed.), HandbookTheoretical Computer Science: Volume A: Algorithms Complexity, pp. 67161.Elsevier, Amsterdam.Karp, R. M., & Lipton, R. J. (1980). connections non-uniform uniformcomplexity classes. Proc. 12th ACM sym. Theory Computing (STOC80), pp. 302309.Katsuno, H., & Mendelzon, A. (1992). dierence updating knowledgebase revising it. Gardenfors, P. (Ed.), Belief Revision, pp. 183203. CambridgeUniversity Press.Konieczny, S., & Pino Perez, R. (2002). Merging information constraints: logicalframework. Journal Logic Computation, 12 (5), 773808.Konieczny, S. (2000). dierence merging knowledge bases combiningthem. Cohn, A. G., Giunchiglia, F., & Selman, B. (Eds.), KR2000: PrinciplesKnowledge Representation Reasoning, pp. 135144, San Francisco. MorganKaufmann.Konieczny, S., & Pino Perez, R. (1998). logic merging. Cohn, A. G., Schubert, L., & Shapiro, S. C. (Eds.), KR98: Principles Knowledge RepresentationReasoning, pp. 488498. Morgan Kaufmann, San Francisco, California.Lang, J. (2006). time, revision, update. Dix, J., & Hunter, A. (Eds.),Proceedings Eleventh International Workshop Non-Monotonic Reasoning(NMR 2006).Lang, J., Liberatore, P., & Marquis, P. (2003). Propositional independence : Formulavariable independence forgetting. Journal Artificial Intelligence Research, 18,391443.Liberatore, P., & Schaerf, M. (1998). Arbitration (or merge knowledge bases). IEEETransactions Knowledge Data Engineering, 10 (1), 7690.Lin, F., & Reiter, R. (1994). Forget it!. AAAI Fall Symposium Relevance, NewOrleans.790fiCompositional Belief UpdateLin, F. (2001). strongest neccessary weakest sucient conditions. Artificial Intelligence, 128 (1-2), 143159.Lin, J., & Mendelzon, A. O. (1998). Merging databases constraints. InternationalJournal Cooperative Information Systems, 7 (1), 5576.Meyer, T. (2000). Merging epistemic states. Pacific Rim International ConferenceArtificial Intelligence, pp. 286296.Meyer, T. (2001). semantics combination operations. Journal Applied NonClassical Logics, 11 (1-2), 5984.Nayak, A., Chen, Y., & Lin, F. (2006). Forgetting knowledge update. Sattar,A., & Kang, B. (Eds.), Proceedings Nineteenth Australian Joint ConferenceArtificial Intelligence (AI-06), Vol. 4304 Lecture Notes Artificial Intelligence, pp.131140. Springer Verlag.Parikh, R. (1999). Beliefs, belief revision, splitting languages. Moss, L., Ginzburg, J.,& de Rijke, M. (Eds.), Logic, Language Computation, Vol 2, pp. 266278. CSLIPublications.Peppas, P., Chopra, S., & Foo, N. (2004). Distance semantics relevance-sensitive beliefrevision. KR2004: Principles Knowledge Representation Reasoning, SanFrancisco. Morgan Kaufmann.Revesz, P. (1993). semantics theory change: Arbitration old newinformation. Beeri, C. (Ed.), Proceedings Twelth ACM Symposium Principles Database Systems, pp. 7182, Washington D.C.Rott, H., & Pagnucco, M. (1999). Severe withdrawal (and recovery). Journal PhilosophicalLogic, 28 (5), 501547.Satoh, K. (1988). Nonmonotonic reasoning minimal belief revision. ProceedingsInternational Conference Fifth Generation Computer Systems, pp. 455462,Tokyo.Subrahmanian, S. (1994). Amalgamating knowledge bases. ACM Transactions DatabaseSystems, 19, 291331.Tennant, N. (1997). bad contractions or: room recovery. JournalApplied Non-Classical Logic, 7, 241266.Weber, A. (1986). Updating propositional formulas. Proc. First Conference ExpertDatabase Systems, pp. 487500.Williams, M.-A. (1996). Towards practical approach belief revision: Reason-basedchange. Aiello, L., Doyle, J., & Shapiro, S. (Eds.), Proceedings Fifth International Conference Principles Knowledge Representation Reasoning,pp. 412421, Cambridge, MA.Winslett, M. (1988). Reasoning action using possible models approach. Proceedings AAAI National Conference Artificial Intelligence, pp. 8993, St. Paul,Minnesota.Winslett, M. (1990). Updating Logical Databases. Cambridge University Press, Cambridge.791fiJournal Artificial Intelligence Research 32 (2008) 631-662Submitted 11/07; published 06/08General Theory Additive State Space AbstractionsFan YangJoseph CulbersonRobert Holtefyang@cs.ualberta.cajoe@cs.ualberta.caholte@cs.ualberta.caComputing Science Department, University AlbertaEdmonton, Alberta T6G 2E8 CanadaUzi Zahavizahaviu@cs.biu.ac.ilComputer Science Department, Bar-Ilan UniversityRamat-Gan, Israel 92500Ariel Felnerfelner@bgu.ac.ilInformation Systems Engineering Department,Deutsche Telekom Labs,Ben-Gurion University.Beer-Sheva, Israel 85104AbstractInformally, set abstractions state space additive distancetwo states always greater equal sum corresponding distances abstract spaces. first known additive abstractions, called disjoint patterndatabases, experimentally demonstrated produce state art performancecertain state spaces. However, previous applications restricted state spacesspecial properties, precludes disjoint pattern databases defined several commonly used testbeds, Rubiks Cube, TopSpin Pancake puzzle.paper give general definition additive abstractions appliedstate space prove heuristics based additive abstractions consistent welladmissible. use new definition create additive abstractions testbedsshow experimentally well chosen additive abstractions reduce search timesubstantially (18,4)-TopSpin puzzle three orders magnitude stateart methods 17-Pancake puzzle. also derive way testing heuristicvalue returned additive abstractions provably low show usetest reduce search time 15-puzzle TopSpin roughly factor two.1. Introductionpurest form, single-agent heuristic search concerned problem findingleast-cost path two states (start goal) state space given heuristic functionh(t, g) estimates cost reach goal state g state t. Standard algorithmssingle-agent heuristic search IDA (Korf, 1985) guaranteed find optimalpaths h(t, g) admissible, i.e. never overestimates actual cost goal statet, efficiency heavily influenced accuracy h(t, g). Considerable researchtherefore investigated methods defining accurate, admissible heuristics.common method defining admissible heuristics, led major advancescombinatorial problems (Culberson & Schaeffer, 1998; Hernadvolgyi, 2003; Korf, 1997;Korf & Taylor, 1996) planning (Edelkamp, 2001), abstract original statec2008AI Access Foundation. rights reserved.fiYang, Culberson, Holte, Zahavi & Felnerspace create new, smaller state space key property path p~original space corresponding abstract path whose cost exceed costp~. Given abstraction, h(t, g) defined cost least-cost abstract pathabstract state corresponding abstract state corresponding g.best heuristic functions defined abstraction typically based several abstractions,equal either maximum, sum, costs returned abstractions(Korf & Felner, 2002; Felner, Korf, & Hanan, 2004; Holte, Felner, Newton, Meshulam, &Furcy, 2006).sum costs returned set abstractions always admissible.is, set abstractions said additive. main contribution paperidentify general conditions abstractions additive. new conditions subsumeprevious notions additive special cases. greater generality allows additiveabstractions defined state spaces additive abstractions accordingprevious definitions, Rubiks Cube, TopSpin, Pancake puzzle, related realworld problems genome rearrangement problem described Erdem Tillier(2005). definitions fully formal, enabling rigorous proofs admissibilityconsistency heuristics defined abstractions. Heuristic h(t, g) consistentstates t, g u, h(t, g) cost(t, u) + h(u, g), cost(t, u) cost least-costpath u.usefulness general definitions demonstrated experimentally definingadditive abstractions substantially reduce CPU time needed solve TopSpinPancake puzzle. example, use additive abstractions allows 17-Pancakepuzzle solved three orders magnitude faster previous state-of-the-art methods.Additional experiments show additive abstractions always best abstraction method. main reason solution cost calculated individualadditive abstraction sometimes low. extreme case, actually arisespractice, problems abstract solutions cost 0. final contributionpaper introduce technique sometimes able identify sumcosts additive abstractions provably small (infeasible).remainder paper organized follows. informal introduction abstraction given Section 2. Section 3 presents formal general definitions abstractionsextend general additive abstractions. provide lemmas proving admissibilityconsistency standard additive heuristics based abstractions.section also discusses relation previous definitions. Section 4 describes successful applications additive abstractions TopSpin Pancake puzzle. Section 5 discussesnegative results. Section 6 introduces infeasibility presents experimental resultsshowing effectiveness sliding tile puzzle TopSpin. Conclusions presentedSection 7.2. Heuristics Defined Abstractionillustrate idea abstraction used define heuristics, considerwell-known 8-puzzle (the 3 3 sliding tile puzzle). puzzle 9 locationsform 3 3 grid 8 tiles, numbered 18, 9th location empty (orblank). tile adjacent empty location moved empty location;632fiA General Theory Additive State Space Abstractionsevery move cost 1. common way abstracting state space treatseveral tiles indistinguishable instead distinct (Culberson &Schaeffer, 1996). extreme version type abstraction shown Figure 1.tiles indistinguishable other, abstract state entirely definedposition blank. therefore 9 abstract states, connected shownFigure 1. goal state original puzzle blank upper left corner,abstract goal state shown top figure. number besideabstract state distance abstract state abstract goal. example,Figure 1, abstract state e 2 moves abstract goal. heuristic function h(t, g)distance state g original space computed two steps: (1) computeabstract state corresponding (in example, done determininglocation blank state t); (2) determine distance abstractstate abstract goal. calculation abstract distance either donepreprocessing step create heuristic lookup table called pattern database (Culberson& Schaeffer, 1994, 1996) time needed (Holte, Perez, Zimmer, & MacDonald,1996; Holte, Grajkowski, & Tanner, 2005; Felner & Adler, 2005).abstract goalid: distance goala:0b: 1c: 1e: 2f: 2d: 2h: 4g: 3i: 3Figure 1: abstraction 8-puzzle. white square state blanknon-white squares tiles, indistinguishableabstraction.Given several abstractions state space, heuristic hmax (t, g) definedmaximum abstract distances given abstractions individually.standard method defining heuristic function given multiple abstractions (Holte et al.,2006). example, consider state 3 3 sliding tile puzzle shown top leftFigure 2 goal state shown it. middle column shows abstractiontwo states (A1 g1 ) tiles 1, 3, 5, 7, blank, distincttiles indistinguishable other. refer distinct tilesdistinguished tiles indistinguishable tiles dont care tiles. right column633fiYang, Culberson, Holte, Zahavi & Felnershows complementary abstraction, tiles 1, 3, 5, 7 dont carestiles 2, 4, 6, 8 distinguished. arrows figure trace least-costpath reach abstract goal gi state Ai abstraction. cost solving A116 cost solving A2 12. Therefore, hmax (A, g) 16, maximumtwo abstract distances.abstract state A2abstract state A1state124856737h=max(16,12)12345678goal state g215436816 moves12 moves213457abstract goal g168abstract goal g 2Figure 2: Computation hmax (A, g), standard, maximum-based heuristic valuestate (top left) using two abstractions shown middle rightcolumns. Solid arrows denote distinguished moves, dashed arrows denote dontcare moves.2.1 Additive AbstractionsFigure 3 illustrates additive abstractions defined sliding tile puzzle (Korf& Felner, 2002; Felner et al., 2004; Korf & Taylor, 1996). State abstractionsFigure 2, costs operators abstract spaces defineddifferently. Instead abstract operators cost 1, case previously,operator cost 1 moves distinguished tile; moves calleddistinguished moves shown solid arrows Figures 2 3. operatormoves dont care tile (a dont care move) cost 0 shown dashedarrow figures. Least-cost paths abstract spaces defined way therefore minimizenumber distinguished moves without considering many dont care movesmade. example, least-cost path A1 Figure 3 contains fewer distinguishedmoves (9 compared 10) least-cost path A1 Figure 2and thereforelower cost according cost function describedbut contains moves total(18 compared 16) dont care moves (9 compared 6). Figure3 shows, 9 distinguished moves needed solve A1 5 distinguished moves neededsolve A2 . tile distinguished abstractions, move cost1 one space cost 0 space, therefore admissible add634fiA General Theory Additive State Space Abstractionstwo distances. heuristic calculated using additive abstractions referred hadd ;example, hadd (A, g) = 9 + 5 = 14. Note hadd (A, g) less hmax (A, g)example, showing heuristics based additive abstractions always superiorstandard, maximum-based method combining multiple abstractions even thoughgeneral proven effective sliding tile puzzles (Korf & Felner, 2002;Felner et al., 2004; Korf & Taylor, 1996).abstract state A2abstract state A1state124856737h=9+5=1412345678goal state g21543685 distinguished moves9 distinguished moves213457abstract goal g168abstract goal g 2Figure 3: Computation hadd (A, g), additive heuristic value state A. Solid arrowsdenote distinguished moves, dashed arrows denote dont care moves.general method defined Korf, Felner, colleagues (Korf & Felner, 2002; Felneret al., 2004; Korf & Taylor, 1996) creates set k additive abstractions partitioningtiles k disjoint groups defining one abstraction group makingtiles group distinguished abstraction. important limitationexisting methods defining additive abstractions applyspaces operator move one tile time, unless wayguarantee tiles moved operator group.example state space additive abstractions according previousdefinitions Pancake puzzle. N -Pancake puzzle, state permutation Ntiles (0, 1, ..., N 1) N 1 successors, lth successor formed reversingorder first l + 1 positions permutation (1 l N 1). example,4-Pancake puzzle shown Figure 4, state top figure three successors,formed reversing order first two tiles, first three tiles, fourtiles, respectively. operators move one tile tile appearlocation non-trivial way partition tiles tiles movedoperator distinguished one abstraction. common state spacesadditive abstractions according previous definitionsfor similar reasonsareRubiks Cube TopSpin.635fiYang, Culberson, Holte, Zahavi & FelnerFigure 4: 4-Pancake puzzle state three successors.general definition additive abstractions presented next section overcomeslimitations previous definitions. Intuitively, abstractions additive providedcost operator divided among abstract spaces. definition providesformal basis intuition. numerous ways even operatorsmove many tiles (or, words, make changes many state variables). example,operator cost might divided proportionally across abstractions basedpercentage tiles moved operator distinguished abstraction.call method defining abstract costs cost-splitting. example, consider twoabstractions 4-Pancake puzzle, one tiles 0 1 distinguished,tiles 2 3 distinguished. middle operator Figure 4 wouldcost 23 first abstract space 13 second abstract space,three tiles operator moves, two distinguished first abstraction onedistinguished second abstraction.different method dividing operator costs among abstractions focuses specific location (or locations) puzzle assigns full cost operatorabstraction tile moves location distinguished. calllocation-based cost definition. Pancake puzzle natural use leftmostlocation special location since every operator changes tile location.middle operator Figure 4 would cost 0 abstract space tiles 01 distinguished cost 1 abstract space tiles 2 3distinguished operator moves tile 2 leftmost location.methods apply Rubiks Cube TopSpin, many state spacesaddition Pancake puzzle, hadd heuristics produce always superiorhmax heuristics based tile partitions. theory experimentsremainder paper shed light general question hadd preferablehmax .3. Formal Theory Additive Abstractionssection, give formal definitions lemmas related state spaces, abstractions,heuristics defined them, discuss meanings relation previouswork. definitions state space etc. Section 3.1 standard, definitionstate space abstraction Section 3.2 differs previous definitions one importantdetail: state transition abstract space two costs associated insteadone. main new contribution definition additive abstractions Section3.3.636fiA General Theory Additive State Space Abstractionsunderlying structure abstraction definition directed graph (digraph)homomorphism. easy reference, quote standard definitions digraphdigraph homomorphism (Hell & Nesetril, 2004).Definition 3.1 digraph G finite set V = V (G) vertices, together binaryrelation E = E(G) V. elements (u, v) E called arcs G.Definition 3.2 Let G H digraphs. homomorphism G H, writtenf : G H mapping f : V (G) V (H) (f (u), f (v)) E(H) whenever(u, v) E(G).Note digraphs G H may self-loops, (u, u), homomorphismrequired surjective either vertices arcs. typically refer arcs edges,kept mind that, general, directed edges, ordered pairs.3.1 State SpaceDefinition 3.3 state space weighted directed graph = hT, , Ci finiteset states, set directed edges (ordered pairs states) representing statetransitions, C : N = {0, 1, 2, 3, . . . } edge cost function.typical practice, defined implicitly. Usually distinct state correspondsassignment values set state variables. C derive successor function,set planing operators. cases, restricted set states reachablegiven state. example, 8-puzzle, set edges defined ruletile adjacent empty location moved empty location,set states defined one two ways: either set states reachablegoal state, set permutations tiles blank, case consiststwo components connected one another. standard cost function C8-puzzle assigns cost 1 edges, easy imagine cost functions8-puzzle depend tile moved locations involved move.path state state g sequence edges beginning endingg. Formally, p~ path state state g p~ = h 1 , . . . , n i, j j =(tj1 , tj ), j {1, . . . , n} t0 = t, tn = g. Note use superscripts rathersubscripts distinguish states edgesP within state space. length p~number edges n cost C(~p) = nj=1 C( j ). use P aths(S, t, g) denoteset paths g S.Definition 3.4 optimal (minimum) cost path state state g definedOPT(t, g) =minp~P aths(S,t,g)C(~p)pathfinding problem triple hS, t, gi, state space t, g ,objective finding minimum cost path g, cases findingminimum cost path p~ P aths(S, t, g) C(~p) = OPT(t, g). one goalstate may seem restrictive, problems set goal states accommodateddefinition adding virtual goal state state space zero-cost edgesactual goal states virtual goal state.637fiYang, Culberson, Holte, Zahavi & Felner3.2 State Space AbstractionDefinition 3.5 Abstraction System pair hS, = hT, , Ci state space= {hAi , | : Ai , 1 k} set abstractions, abstractionpair consisting abstract state space abstraction mapping, abstractstate space abstraction mapping defined below.Note abstractions intended form hierarchy considered set independent abstractions.Definition 3.6 abstract state space directed graph two weights per edge,defined four-tuple Ai = hTi , , Ci , Ri i.Ti set abstract states set abstract edges, definitionstate space. abstract space two costs associated ,primary cost Ci : N residual cost Ri : N . ideatwo costs per abstract edge, instead one, inspired practice, illustratedFigure 3, two types edges abstract space counting distinguishedmoves differently dont care moves. example, primary cost costassociated distinguished moves, residual cost cost associateddont care moves. usefulness considering cost dont care moves arisesabstraction system additive, suggested Lemmas 3.6 3.10 below.indicate additive heuristic infeasible improved, effectivenessbecome apparent experiments reported Section 6.Like edges, abstract path p~i = hi1 , . . . , Ai primary residual cost:PPpi ) = nj=1 Ri (ij ).Ci (~pi ) = nj=1 Ci (ij ), Ri (~Definition 3.7 abstraction mapping : Ai state space abstractstate space Ai defined mapping states states Ai , :Ti , satisfies two following conditions.first condition mapping homomorphism thus connectivityoriginal space preserved, i.e.,(1)(u, v) , (i (u), (v))words, edge original space corresponding edgeabstract space Ai . Note u 6= v (u) = (v) non-identity edge getsmapped identity edge (self-loop) Ai . use shorthand notation tji = (tj )abstract state Ti corresponding tj , ij = ( j ) = (i (uj ), (v j ))abstract edge corresponding j = (uj , v j ) .second condition state mapping must satisfy abstract edges mustcost edges correspond original state space, i.e.,(2), Ci (i ) + Ri (i ) C()638fiA General Theory Additive State Space Abstractionsconsequence, multiple edges original space map abstract edge, usually case, Ci () + Ri () must less equal them, i.e.,, Ci () + Ri ()minC(),i ()=Note edge maps edge abstract space, bound costedge imposed.example, state mapping used define abstraction middle columnFigure 3 maps 8-puzzle state abstract state renaming tiles 2, 4, 6, 8dont care. mapping satisfies condition (1) dont care tilesexchanged blank whenever regular tiles can. satisfies condition (2)move either distinguished move (Ci (i ) = 1 Ri (i ) = 0) dont care move(Ci (i ) = 0 Ri (i ) = 1) cases Ci (i ) + Ri (i ) = 1, cost edgeoriginal space.set abstract states Ti usually equal (T ) = {i (t) | },superset, case abstraction said non-surjective (Hernadvolgyi & Holte,2000). Likewise, set abstract edges usually equal () = {i () | }superset even Ti = (T ). cases, one deliberately choosesabstract space states edges counterpart original space.example, methods define abstractions dropping operator preconditions must,design, create abstract spaces edges correspond edgeoriginal space (e.g. Pearl, 1984). cases, non-surjectivity inadvertentconsequence abstract space defined implicitly set states reachableabstract goal state applying operator inverses. example, tile2 2 sliding tile puzzle mapped blank abstract space, puzzletwo blanks states reachable abstract space counterpartoriginal space (Hernadvolgyi & Holte, 2000). additional examples extensivediscussion non-surjectivity see previous paper Holte Hernadvolgyi (2004).lemmas definitions follow assume abstraction system hS, containing k abstractions given. Conditions (1) (2) guarantee following.Lemma 3.1 path p~ P aths(S, u1 , u2 ) S, corresponding abstract path(~p) u1i u2i Ai Ci (i (~p)) + Ri (i (~p)) C(~p).Proof: definition, p~ P aths(S, u1 , u2 ) sequence edges h 1 , . . . , n i, jj = (tj1 , tj ), j {1, . . . , n} t0 = u1 , tn = u2 . (),corresponding abstract edges exists (ij ). i1 = (u1i , t1i ) = (tin1 , u2i ),sequence (~p) = hi1 , . P. . , path u1i u2i .definition, C(~p) = nj=1 C( j ). j , Condition (2) ensures C( j )PPPCi (ij )+Ri (ij ), therefore C(~p) nj=1 (Ci (ij )+Ri (ij )) = nj=1 Ci (ij )+ nj=1 Ri (ij ) =Ci (i (~p)) + Ri (i (~p)).example, consider state goal g Figure 3. condition (1), pathstate g original space also path abstract state A1 abstract goalstate g1 abstract state A2 g2 abstract spaces. condition (2),cost path original space greater equal sum primarycost residual cost corresponding abstract path abstract space.639fiYang, Culberson, Holte, Zahavi & Felneruse P aths(Ai , u, v) mean set paths u v space Ai .Definition 3.8 optimal abstract cost abstract state u abstract state v AidefinedOPTi (u, v) =minCi (~q) + Ri (~q)q~P aths(Ai ,u,v)Definition 3.9 define heuristic obtained abstract space Ai coststate ghi (t, g) = OPTi (ti , gi ).Note definitions, path minimizing cost required image,(~p), path p~ S.following prove heuristic generated individual abstraction admissible (Lemma 3.2) consistent (Lemma 3.3).Lemma 3.2 hi (t, g) OPT(t, g) t, g {1, . . . , k}.Proof: Lemma 3.1, C(~p) Ci (i (~p)) + Ri (i (~p)), thereforeminp~P aths(S,t,g)C(~p)minp~P aths(S,t,g)Ci (i (~p)) + Ri (i (~p)).left hand side inequality OPT(t, g) definition, right hand sideproved following Claim 3.2.1 greater equal hi (t, g). Therefore,OPT(t, g) hi (t, g).Claim 3.2.1 minp~P aths(S,t,g) Ci (i (~p)) + Ri (i (~p)) hi (t, g) t, g .Proof Claim 3.2.1:Lemma 3.1 every path p~ corresponding abstract path. may also additional paths abstract space, is,{i (~p) | p~ P aths(S, t, g)} P aths(Ai , ti , gi ). follows {Ci (i (~p)) + Ri (i (~p)) | p~P aths(S, t, g)} {Ci (~q) + Ri (~q) | ~q P aths(Ai , ti , gi )}. Therefore,minp~P aths(S,t,g)Ci (i (~p)) + Ri (i (~p))minq~P aths(Ai ,ti ,gi )Ci (~q) + Ri (~q) = OPTi (ti , gi ) = hi (t, g)Lemma 3.3 hi (t1 , g) OPT(t1 , t2 ) + hi (t2 , g) t1 , t2 , g {1, . . . , k}.Proof: definition OPTi minimization definition hi (t, g), followshi (t1 , g) = OPTi (t1i , gi ) OPTi (t1i , t2i ) + OPTi (t2i , gi ) = OPTi (t1i , t2i ) + hi (t2 , g).complete proof, observe Lemma 3.2, OPT(t1 , t2 ) hi (t1 , t2 ) =OPTi (t1i , t2i ).Definition 3.10 hmax heuristic state state g defined abstraction systemhS,khmax (t, g) = max hi (t, g)i=1Lemmas 3.2 3.3 immediately follows hmax admissible consistent.640fiA General Theory Additive State Space Abstractions3.3 Additive Abstractionssection, formalize notion additive abstraction introduced intuitively Section 2.1. example showed hadd (t, g), sum heuristicsstate defined multiple abstractions, admissible provided cost functionsabstract spaces counted distinguished moves. formal framework,cost distinguished moves captured notion primary cost.Definition 3.11 pair states t, g additive heuristic given abstractionsystem definedhadd (t, g) =kXCi (ti , gi ).i=1Ci (ti , gi ) =minq~P aths(Ai ,ti ,gi )Ci (~q)minimum primary cost path abstract space ti gi .Figure 3, example, C1 (A1 , g1 ) = 9 C2 (A2 , g2 ) = 5 minimumnumber distinguished moves reach g1 A1 9 minimum numberdistinguished moves reach g2 A2 5.Intuitively, hadd admissible cost edge original space dividedamong abstract edges correspond , done cost-splittinglocation-based methods defining abstract costs introduced endSection 2.1. leads following formal definition.PDefinition 3.12 abstraction system hS, additive , ki=1 Ci (i ) C().following prove hadd admissible (Lemma 3.4) consistent (Lemma 3.5)abstraction system hS, additive.Lemma 3.4 hS, additive hadd (t, g) OPT(t, g) t, g .Proof: Assumeg) = C(~p), p~ = h 1 , . . . , n P aths(S, t, g). Therefore,Pnthat OPT(t,jOPT(t, g) = j=1 C( ). Since hS, additive, follows definitionnXC( j )j=1n XkXCi (ij ) =j=1 i=1kXk XnXCi (ij )i=1 j=1Ci (ti , gi ) = hadd (t, g)i=1last line follows definitions Ci hadd .Lemma 3.5 hS, additive hadd (t1 , g) OPT(t1 , t2 )+hadd (t2 , g) t1 , t2 , gT.641fiYang, Culberson, Holte, Zahavi & FelnerProof: Ci (t1i , gi ) obeys triangle inequality: Ci (t1i , gi ) Ci (t1i , t2i ) + Ci (t2i , gi )PPPt1 , t2 , g . follows ki=1 Ci (t1i , gi ) ki=1 Ci (t1i , t2i ) + ki=1 Ci (t2i , gi ).PkPi=1 Ci (t1i , gi ) = hadd (t1 , g) ki=1 Ci (t2i , gi ) = hadd (t2 , g), followsPhadd (t1 , g) ki=1 Ci (t1i , t2i ) + hadd (t2 , g).PSince hS, additive, Lemma 3.4, OPT(t1 , t2 ) ki=1 Ci (t1i , t2i ).Hence hadd (t1 , g) OPT(t1 , t2 ) + hadd (t2 , g) t1 , t2 , g .develop simple test important consequences additive heuristics.Define P~i (ti , gi ) = {~q | ~q P aths(Ai , ti , gi ) Ci (~q) = Ci (ti , gi )}, set abstractpaths ti gi whose primary cost minimal.Definition 3.13 conditional optimal residual cost minimum residual cost amongpaths P~i (ti , gi ):Ri (ti , gi ) = min Ri (~q)~i (ti ,gi )q~PNote value (Ci (ti , gi ) + Ri (ti , gi )) sometimes, always, equaloptimal abstract cost OPTi (ti , gi ). Figure 3, example, OPT1 (A1 , g1 ) = 16 (a pathcost shown Figure 2) C1 (A1 , g1 ) + R1 (A1 , g1 ) = 18, C2 (A2 , g2 ) +R2 (A2 , g2 ) = OPT2 (A2 , g2 ) = 12. following lemmas show, possible drawimportant conclusions hadd comparing value (Ci (ti , gi ) + Ri (ti , gi )).Lemma 3.6 Let hS, additive abstraction system let t, g states.hadd (t, g) Cj (tj , gj ) + Rj (tj , gj ) j {1, . . . , k}, hadd (t, g) hmax (t, g).Proof: definition OPTi (ti , gi ), j {1, . . . , k}, Cj (tj , gj )+Rj (tj , gj ) OPTj (tj , gj ).Therefore, j {1, . . . , k}, hadd (t, g) Cj (tj , gj ) + Rj (tj , gj ) OPTj (tj , gj ) hadd (t, g)max1ik OPTi (ti , gi ) = hmax (t, g).Lemma 3.7 additive hS, path p~ P aths(S, t, g) C(~p) =Cj (j (~p)) = Cj (tj , gj ) j {1, . . . , k}.Pki=1 Ci (ti , gi ),Proof: Suppose contradictionPexists i1 , Ci1 (i1 (~p)) >kCi1 (ti1 , gi1 ). C(~p) =i=1 Ci (ti , gi ), must exist i2 ,Ci2 (i2 (~p)) < Ci2 (ti2 , gi2 ), contradicts definition Ci . Therefore, i1exist Cj (j (~p)) = Cj (tj , gj ) j {1, . . . , k}.Lemma 3.8 additive hS, path p~ P aths(S, t, g) C(~p) =Ri (i (~p)) Ri (ti , gi ) {1, . . . , k}.Pki=1 Ci (ti , gi ),Proof: Following Lemma 3.7 definition P~i (ti , gi ), (~p) P~i (ti , gi ){1, . . . , k}. Ri (ti , gi ) smallest residual cost paths P~i (ti , gi ), followsRi (i (~p)) Ri (ti , gi ).Lemma 3.9 additive hS, path p~ P aths(S, t, g) C(~p) =Pk(t , g ) C (t , g ) + R (t , g ) j {1, . . . , k}.Cj j jj j ji=1642Pki=1 Ci (ti , gi ),fiA General Theory Additive State Space AbstractionsProof: Lemma 3.1, C(~p) Cj (j (~p)) + Rj (j (~p)) j {1, . . . , k}. Lemma 3.7Cj (j (~p)) = Cj (tj , gj ), Lemma 3.8 Rj (j (~p)) Rj (tj , gj ). Therefore C(~p)PkCj (tj , gj ) + Rj (tj , gj ), lemma follows premise C(~p) = i=1 Ci (ti , gi ).Lemma 3.10 Let hS, additive abstraction system let t, g states.hadd (t, g) < Cj (tj , gj ) + Rj (tj , gj ) j {1, . . . , k}, hadd (t, g) 6= OP (t, g).Proof: lemma follows directly contrapositive Lemma 3.9.Lemma 3.6 gives condition hadd guaranteed least largehmax specific states g. condition holds large fraction statespace , one would expect search using hadd least fast as, possiblyfaster than, search using hmax . seen experiments reported Section 4.opposite true general, i.e., failing condition imply hmaxresult faster search hadd . However, Lemma 3.10 shows, interestingconsequence condition fails state t: know value returned haddtrue cost reach goal t. Detecting useful allowsheuristic value increased without risking becoming inadmissible. Section 6explores detail.3.4 Relation Previous Workaim preceding formal definitions identify fundamental properties guarantee abstractions give rise admissible, consistent heuristics. shownfollowing two conditions guarantee heuristic defined abstractionadmissible consistent(P 1)(u, v) , (i (u), (v))(P 2), C() Ci (i ) + Ri (i )third condition(P 3), C()kXCi (i )i=1guarantees hadd (t, g) admissible consistent.Previous work focused defining abstraction additivity specific waysrepresenting states transition functions. important contributionsultimately one needs computationally effective ways defining abstract state spaces,abstraction mappings, cost functions theory takes given. importancecontribution make future proofs admissibility, consistency,additivity easier, one need show particular method definingabstractions satisfies three preceding conditions. generally simple conditions demonstrate, several methods defining abstractionsadditivity currently exist literature.643fiYang, Culberson, Holte, Zahavi & Felner3.4.1 Previous Definitions Abstractionuse abstraction create heuristics began late 1970s popularizedPearls landmark book heuristics (Pearl, 1984). Two abstraction methods identified time: relaxing state space definition dropping operator preconditions(Gaschnig, 1979; Guida & Somalvico, 1979; Pearl, 1984; Valtorta, 1984), homomorphic abstractions (Banerji, 1980; Kibler, 1982). early notions abstractionunified extended Mostow Prieditis (1989) Prieditis (1993), producingformal definition important respects except conceptresidual cost introduced.1Todays two commonly used abstraction methods among ones implementedPrieditiss Absolver II system (Prieditis, 1993). first domain abstraction,independently introduced seminal work pattern databases (Culberson &Schaeffer, 1994, 1998) generalized (Hernadvolgyi & Holte, 2000). assumesstate represented set state variables, set possible valuescalled domain. abstraction states defined specifying mappingoriginal domains new, smaller domains. example, 8-puzzle state typicallyrepresented 9 variables, one location puzzle, domain9 elements, one tile one blank. domain abstractionmaps elements representing tiles new element (dont care)blank different element would produce abstract space shown Figure 1.reason particular example satisfies property (P1) explained Section 3.2. general,domain abstraction satisfy property (P1) long conditions definestate transitions occur (e.g. operator preconditions) guaranteed satisfieddont care symbol whenever satisfied one domain elementsmap dont care. Property (P2) follows immediately fact statetransitions original abstract spaces primary cost 1.major type abstraction used today, called drop Prieditis (1993),independently introduced abstracting planning domains represented grounded (orpropositional) STRIPS operators (Edelkamp, 2001). STRIPS representation, staterepresented set logical atoms true state, directed edgesstates represented set operators, operator describedthree sets atoms, P (a), A(a), D(a). P (a) lists preconditions: appliedstate atoms P (a) true (i.e., P (a) t). A(a) D(a) specifyeffects operator a, A(a) listing atoms become true applied (theadd list) D(a) listing atoms become false applied (the deletelist). Hence operator applicable state t, state u = a(t) produces appliedset atoms u = (t D(a)) A(a).setting, Edelkamp defined abstraction given state space specifyingsubset atoms restricting abstract state descriptions operator definitionsinclude atoms subset. Suppose Vi subset atoms underlyingabstraction mapping : Ai , original state space Ai abstractstate space based Vi . Two states mapped abstract state1. Prieditiss definition allows abstraction expand set goals. achieveddefinition mapping non-goal states original space abstract state goal.644fiA General Theory Additive State Space Abstractionscontain subset atoms Vi , i.e., (t) = (u) iff Vi = u Vi .satisfies property (P1) operator applicable state (P (a) t)implies abstract operator ai = (a) applicable abstract state ti (P (a) Vi Vi )resulting state a(t) = (t D(a)) A(a) mapped ai (i (t))set intersection distributes across set subtraction union (Vi ((t D(a)) A(a)) =((Vi t) (Vi D(a))) (Vi A(a))). Again, property (P2) follows immediatelyfact operators original abstract spaces primary cost 1.Recently, Helmert et al. (2007) described general approach defining abstractions planning based transition graph abstractions. transition graph directedgraph arcs labels, transition graph abstraction directed graphhomomorphism preserves labels.2 Hence, Helmert et al.s method restrictedversion definition abstraction therefore satisfies properties (P1) (P2).Helmert et al. make following interesting observations true generaldefinition abstractions:composition two abstractions abstraction. words, :abstraction : B abstraction A, () : Babstraction S. property abstractions exploited Prieditis (1993).product A1 A2 two abstractions, A1 A2 , abstraction S,state space product Cartesian product two abstractstate spaces, edge 12 product space state (t1 , t2 ) state(u1 , u2 ) edge 1 t1 u1 A1 edge 2 t2u2 A2 . primary cost 12 minimum C1 (1 ) C2 (2 )residual cost 12 taken space primary cost.working labelled edges Helmert et al. require edge connecting t1 u1label edge connecting t2 u2 ; called synchronizedproduct denoted A1 A2 (refer Definition 6 defined Helmert et al. (2007)exact definition synchronized product).Figure 5 shows synchronized product, B, two abstractions, A1 A2 ,3-state space edge labels b. A1 derived mappingstates s1 s2 state (s1,2 ), A2 derived mapping states s2s3 state (s2,3 ). Note B contains four states, originalspace. abstraction mapping original states s1 , s2 , s3states (s1,2 , s1 ) (s1,2 , s2,3 ) (s3 , s2,3 ), respectively, satisfies property (P1), property(P2) satisfied automatically edges cost 1. point viewfourth state B, (s3 , s1 ), redundant state (s1,2 , s1 ). Nevertheless distinctstate product space.Haslum et al. (2005) introduce family heuristics, called hm (for fixed{1, 2, ...}), based abstraction, covered definitionvalue heuristic state t, hm (t), defined distance abstractionabstract goal state. Instead takes advantage special monotonicity property2. Homomorphism means standard definition digraph homomorphism (Definition 3.2),permits non-surjectivity (as discussed Section 3.2), opposed Helmert et al.s definitionhomomorphism, allow non-surjectivity.645fiYang, Culberson, Holte, Zahavi & Felners1bs2s3bs1, 2s3bs2 , 3s1B=( s3 , s2 , 3 )b( s1, 2 , s1 ) ( s1, 2 , s2, 3 )( s3 , s1 )Figure 5: original state space. A1 A2 abstractions S. B = A1 A2synchronized product A1 A2 .costs planning problems: cost achieving subset atoms defining goallower bound cost achieving goal. searching backwardsgoal start state, Haslum et al. do, allows admissible heuristic definedfollowing recursive minimax fashion (|t| denotes number atoms state t):0,startmin C(s, t) + hm (s), |t|h (t) =(s,t)|t| >max hm (s),st,|s|mfirst two lines definition standard method calculating costleast-cost path. third line uses fact cost achieving subsetatoms lower bound cost achieving entire set atoms t.recursive calculation alternates min max calculation dependingnumber atoms state currently considered recursive calculation,therefore different shortest path calculation taking maximum setshortest path calculations.3.4.2 Previous definitions additive abstractionsPrieditis (1993) included method (Factor) Absolver II system creating additiveabstractions, present formal definitions theory.first thorough discussion additive abstractions due Korf Taylor (1996).observed sliding tile puzzles Manhattan Distance heuristic, severalenhancements, sum distances set abstract spaces smallnumber tiles distinguished. explained Section 2.1, allowed abstractdistances added still lower bound distances original spacemoves distinguished tiles counted towards abstract distance tile646fiA General Theory Additive State Space Abstractionsdistinguished one abstraction. idea later developed seriespapers (Korf & Felner, 2002; Felner et al., 2004), extended applicationdomains, 4-peg Towers Hanoi puzzle.planning literature, idea proposed Haslum et al. (2005),described partitioning operators disjoint sets B1 , ...Bk counting costoperators set Bi abstract space Ai . example giveBlocks World operators move block would set Bi , effectively defining setadditive abstractions Blocks World exactly analogous Korf Taylorabstractions define Manhattan Distance sliding tile puzzle.Edelkamp (2001) took different approach defining additive abstractions STRIPSplanning representations. method involves partitioning atoms disjoint setsV1 , ...Vk operator changes atoms one group. abstract spaceAi retains atoms set Vi operators affect atoms Vieffect abstract space Ai naturally cost 0 Ai . Sinceoperator affects atoms one group, operator non-zero costone abstract space distances abstract spaces safely added. Haslum etal. (2007) extended idea representations state variables could multiplevalues. subsequent paper Edelkamp (2002) remarks partitioningatoms induces partitioning operators described, additivity couldenforced assigning operator cost zero one abstract spacesareturn Korf Taylor idea.methods described might called all-or-nothing methods definingabstract costs, cost edge C() fully assigned costcorresponding abstract edge Ci (i ) one abstractions corresponding edgesabstractions assigned cost zero. method obviously satisfiesproperty (P3) therefore additive.theory additivity require abstract methods defined all-ornothing manner, allows C() divided way whatsoever among abstractionslong property (P3) satisfied. possibility recognized one recent publication (Katz & Domshlak, 2007), report experimental results.generalization important eliminates requirement operators must moveone tile change atoms/variables one group, related requirementtiles/atoms distinguished/represented exactly one abstract spaces. requirement restricted application previous methods defining additive abstractions,precluding application state spaces Rubiks Cube, Pancake puzzle,TopSpin. following sections show, definition, additive abstractionsdefined state space, including three mentioned.Finally, Helmert et al. (2007) showed synchronized product additive abstractions produces heuristic hsprod dominates hadd , sense hsprod (s) hadd (s)states s. happens synchronized product forces pathused abstract spaces, whereas calculation Ci hadd baseddifferent path. discussion negative results infeasibility highlightproblems arise Ci calculated independently.647fiYang, Culberson, Holte, Zahavi & Felner4. New Applications Additive Abstractionssection next section report results applying general definitionadditive abstraction given previous section three benchmark state spaces: TopSpin,Pancake puzzle Rubiks Cube. additional experimental results may foundprevious paper Yang, Culberson, Holte (2007). experiments edgesoriginal state spaces cost 1 define Ri (i ) = 1 Ci (i ), maximumpermitted value edges cost 1. use pattern databases store heuristic values.pre-processing time required compute pattern databases excludedtimes reported results, PDB needs calculatedoverhead amortized solving many problem instances.4.1 Methods Defining Costsinvestigate two general methods defining primary cost abstract statetransition Ci (i ), call cost-splitting location-based costs. illustrategenerality methods define two common waysrepresenting statesas vector state variables, method implementedexperiments, set logical atoms STRIPS representation planningproblems.state variable representation state represented vector state variables,domain possible values Dj , i.e., = (t(0), ..., t(m 1)),t(j) Dj value assigned j th state variable state t. example, puzzlesPancake puzzle sliding tile puzzles, typically one variablephysical location puzzle, value t(j) indicates tile locationj state t. case domain variables same. State space abstractionsdefined abstracting domains. particular, setting domain abstractionleave specific domain values unchanged (the distinguished values according )map rest special value, dont care. abstract state correspondingaccording ti =(ti (0), ..., ti (m 1)) ti (j) = (t(j)). previous researchstate spaces set abstractions defined partitioning domain valuesdisjoint sets E1 , ..., Ek Ei set distinguished values abstraction i. Notetheory developed previous section require distinguished valuesdifferent abstractions mutually exclusive; allows value distinguishednumber abstract spaces provided abstract costs defined appropriately.mentioned previously, STRIPS representation state representedset logical atoms true state. state variable representationconverted STRIPS representation variety ways, simplest defineatom possible variable-value combination. state variable j value vstate variable representation state atom variable-j-has-value-v trueSTRIPS representation t. exact equivalent domain abstraction achieveddefining Vi , set atoms used abstraction i, atoms variable-jhas-value-v v Ei , set distinguished values domain abstraction i.648fiA General Theory Additive State Space Abstractions4.1.1 Cost-splittingstate variable representation, cost-splitting method defining primary costs worksfollows. state transition changes b state variables cost, C(), splitamong corresponding abstract state transitions 1 , . . . , k proportion numberdistinguished values assign variables, i.e., abstractionCi (i ) =bi C()bchanges b variables bi assigned distinguished values .3example, 3 3 3 Rubiks cube composed twenty little moveable cubiesoperator moves eight cubies, four corner cubies four edge cubies. Hence b = 8state transitions . particular state transition moves three cubiesdistinguished according abstraction , corresponding abstract state transition, ,would cost 38 . Strictly speaking, require abstract edge costs integers,fractional edge costs produced cost-splitting must scaled appropriately becomeintegers. implementation cost-splitting actually scaling simplifypresentation cost-splitting talk edge costs fractional.domain value distinguished one abstraction (e.g. abstractionsdefined byPpartitioning domain values) cost-splitting produces additive abstractions,i.e., C() ki=1 Ci (i ) . C() known integer, haddPdefined ceiling sum abstract distances, ki=1 Ci (i )e, insteadsum.STRIPS representation, cost-splitting could defined identically, bnumber atoms changed (added deleted) operator original space binumber atoms changed corresponding operator abstraction i.4.1.2 Location-based Costslocation-based cost definition state variable representation, state variable locassociated state transition full cost C() assigned abstract statetransition changes value variable loc value distinguished according.4 Formally:C(), = (t1i , t2i ), t1i (loc ) 6= t2i (loc ),t2i (loc ) distinguished value according .Ci (i ) =0,otherwise.Instead focusing value assigned variable loc , location-based costsdefined equally well value variable loc changed.either case, domain value distinguished one abstraction location-based3. might correspond several edges original space, different costmoving different set tiles, technically correct definition is:Ci (i ) =min,i ()=ibi C()b4. footnote 3, technically correct definition min,i ()=i C() instead C().649fiYang, Culberson, Holte, Zahavi & Felnercosts produce additive abstractions. name location-based based typicalrepresentations used puzzles, state variable physical locationpuzzle. example, Rubiks Cube one could choose reference variablesones representing two diagonally opposite corner locations puzzle. Notepossible Rubiks cube operator changes exactly one locations. abstractstate transition would primary cost 1 cubie moved onelocations distinguished cubie abstraction, primary cost 0 otherwise.STRIPS representation states, location-based costs defined choosingatom Add list operator assigning full cost C() abstractionappears Add list . atoms partitioned atom appearsone abstraction, method define additive costs.Although cost-splitting location-based methods defining costs appliedwide range state spaces, guaranteed define heuristics superiorheuristics given state space. determined experimentally heuristicsbased cost-splitting substantially improve performance sufficiently large versionsTopSpin heuristics based location-based costs vastly improve stateart 17-Pancake puzzle. experiments additive heuristics improvestate art Rubiks Cube. following subsections describe positive resultsdetail. negative results discussed Section 5.4.2 TopSpin Cost-Splitting(N, K)-TopSpin puzzle (see Figure 6) N tiles (numbered 1, . . . , N ) arrangedcircular track, two physical movements possible: (1) entire set tiles mayrotated around track, (2) segment consisting K adjacent tiles trackmay reversed. previous formulations puzzle state space (Felner, Zahavi,Schaeffer, & Holte, 2005; Holte et al., 2005; Holte, Newton, Felner, Meshulam, & Furcy,2004), represent first physical movement operator, instead designateone tiles (tile 1) reference tile goal get tiles increasingorder starting tile (regardless position). state space therefore Noperators (numbered 1, . . . , N ), operator reversing segment length K startingposition relative current position tile 1. certain combinations N Kpossible permutations generated standard goal state operators,general space consists connected components states reachable(Chen & Skiena, 1996). experiments section, K = 4 N varied.sets abstractions used experiments described using tuple writtena1 a2 . . . , indicating set contains abstractions, tiles 1 . . . (a1 ) distinguished first abstraction, tiles (a1 + 1) . . . (a1 + a2 ) distinguished secondabstraction, on. example, 6-6-6 denotes set three abstractionsdistinguished tiles (1 . . . 6), (7 . . . 12), (13 . . . 18) respectively.experiments compare hadd , additive use set abstractions, hmax ,standard use abstractions, which, described Section 2, full coststate transition counted abstraction heuristic returns maximumdistance goal returned different abstractions. Cost-splitting used defineoperator costs abstract spaces hadd . K = 4, operator moves 4 tiles.650fiA General Theory Additive State Space Abstractions3456728911011201912181716151413Figure 6: TopSpin puzzle.bi distinguished tiles operator op applied state si abstraction i,applying op si primary cost b4i abstraction i.experiments heuristic defined abstraction stored patterndatabase (PDB). abstraction would normally used define PDB,set abstractions would require PDBs. However, TopSpin, two (or more)abstractions number distinguished tiles distinguished tilesadjacent, one PDB used suitably renaming tilesPDB lookup. 6-6-6 abstractions, example, one PDB needed,three lookups would done it, one abstraction. position tile 1effectively fixed, PDB N times smaller would normally be. example,N = 18, PDB 6-6-6 abstractions contains 17 16 . . . 13 entries.memory needed entry hadd PDBs twice memory needed entryhmax PDBs need represent fractional values.ran experiments values N sets abstractions shown first twocolumns Table 1. Start states generated random walk 150 movesgoal state. 1000, 50 20 start states N = 12, 16 18, respectively.average solution length start states shown third column Table 1.average number nodes generated average CPU time (in seconds) IDAsolve given start states shown Nodes Time columns hmaxhadd . Nodes Ratio column gives ratio Nodes using hadd Nodes usinghmax . ratio less one (highlighted bold) indicates hadd , heuristic basedadditive abstractions cost-splitting, superior hmax , standard heuristic usingset abstractions.N = 12 N = 16 best performance achieved hmax based pairabstractions N2 distinguished tiles. N increases advantage hmaxdecreases and, N = 18, hadd outperforms hmax abstractions used. Moreover,even smaller values N hadd outperforms hmax set four abstractionsN4 distinguished tiles used. important N increases, memorylimitations preclude using abstractions N2 distinguished tiles optionuse abstractions fewer distinguished tiles each. results Table 1show hadd method choice situation.651fiYang, Culberson, Holte, Zahavi & FelnerN12121216161818Abs6-64-4-43-3-3-38-84-4-4-49-96-6-6AverageSolutionLength9.1389.1389.13814.04014.04017.00017.000hmaxNodes14,821269,9741,762,2621,361,0424,494,414,92938,646,34418,438,031,512Time0.051.108.163.4213,575.00165.42108,155.00hadd basedcost-splittingNodesTime53,4600.16346,4461.331,388,1836.442,137,7404.74251,946,069851.0021,285,29891.76879,249,695 4,713.00NodesRatio3.601.280.781.570.0560.550.04Table 1: (N, 4)-TopSpin results using cost-splitting.4.3 Pancake Puzzle Location-based Costssection, present experimental results 17-Pancake puzzle using locationbased costs. notation previous section used denote sets abstractions, e.g. 5-6-6 denotes set three abstractions, first tiles (0 . . . 4)distinguished tiles, second tiles (5 . . . 10) distinguished tiles,third tiles (11 . . . 16) distinguished tiles. Also before, heuristicabstraction precomputed stored pattern database (PDB). Unlike TopSpin,symmetries Pancake puzzle enable different abstractions make usePDB, set abstractions Pancake puzzle requires differentPDBs.Additive abstractions defined using location-based method one reference location, leftmost position. position chosen tileposition changes whenever operator applied state original state space.means every edge cost original space fully counted abstractspace long tile distinguished tile abstraction. before, use hadddenote heuristic defined adding values returned individual additiveabstractions.first experiment compares IDA using hadd best results known17-Pancake puzzle (Zahavi, Felner, Holte, & Schaeffer, 2006) (shown Table 2),obtained using single abstraction rightmost seven tiles (1016)distinguished tiles advanced search technique called Dual IDA (DIDA ).5 DIDAextension IDA exploits fact that, states permutations tilesPancake puzzle, state easily computable dual state sdspecial property inverses paths goal paths sd goal.paths inverses cost same, DIDA defines heuristic value statemaximum h(s) h(sd ), sometimes decide search least-cost pathsd goal looking path goal.5. particular, DIDA jump larger (JIL) policy bidirectional pathmax method(BPMX) propagate inconsistent heuristic values arise dual search. Zahavi et al.(2006) provided details. BPMX first introduced Felner et al. (2005).652fiA General Theory Additive State Space Abstractionsresults experiment shown top three rows Table 3. Algorithm column indicates heuristic search algorithm. Abs column shows setabstractions used generate heuristics. Nodes column shows average numbernodes generated solving 1000 randomly generated start states. start statesaverage solution length 15.77. Time column gives average number CPUseconds needed solve start states AMD Athlon(tm) 64 Processor 3700+2.4 GHz clock rate 1GB memory. Memory column indicates total sizeset PDBs.NAlgorithm17DIDAAbsrightmost-7AverageSolutionLength15.77h basedsingle large PDBNodesTimeMemory124,198,462 37.713 98,017,920Table 2: best results known 17-Pancake puzzle (Zahavi et al., 2006),obtained using single abstraction rightmost seven tiles (10 16)distinguished tiles advanced search technique called Dual IDA (DIDA ).NAlgorithm171717171717IDAIDAIDADIDADIDADIDAAbs4-4-4-55-6-63-7-74-4-4-55-6-63-7-7AverageSolutionLength15.7715.7715.7715.7715.7715.77hadd basedLocation-based CostsNodes TimeMemory14,610,039 4.302913,9201,064,108 0.34218,564,0001,061,383 0.383 196,039,920368,925 0.195913,92044,618 0.02818,564,00037,155 0.026 196,039,920Table 3: 17-Pancake puzzle results using hadd based location-based costs.Clearly, use hadd based location-based costs results significant reduction nodes generated compared using single large PDB, even latteradvantage used sophisticated search algorithm. Notetotal memory needed 4-4-4-5 PDBs one percent memory neededrightmost-7 PDB, yet IDA 4-4-4-5 generates 8.5 times fewer nodesDIDA rightmost-7 PDB. Getting excellent search performance smallPDB especially important situations cost computing PDBs musttaken account addition cost problem-solving (Holte et al., 2005).memory requirements increase significantly abstractions contain distinguished tiles, experiment improvement running time increaseaccordingly. example, 3-7-7 PDBs use ten times memory 5-6-6 PDBs,running time almost same. 5-6-6 PDBs accurate653fiYang, Culberson, Holte, Zahavi & Felnerlittle room improve them. average heuristic value start states using5-6-6 PDBs 13.594, 2.2 less actual average solution length. averageheuristic value using 3-7-7 PDBs slightly higher (13.628).last three rows Table 3 show results hadd location-based costsused conjunction DIDA . results show combining additive abstractions state-of-the-art search techniques results significant reductionsnodes generated CPU time. example, 5-6-6 PDBs use 1/5 memoryrightmost-7 PDB reduce number nodes generated DIDA factor2783 CPU time factor 1347.compare hadd hmax ran plain IDA hmax 1000 start states,time limit start state ten times greater time needed solvestart state using hadd . time limit 63 1000 start states couldsolved hmax using 3-7-7 abstraction, 5 could solved hmax using5-6-6 abstraction, 3 could solved hmax using 4-4-4-5 abstraction.determine hadd superiority hmax location-based costs puzzle couldpredicted using Lemma 3.6, generated 100 million random 17-Pancake puzzlestates tested many satisfied requirements Lemma 3.6. 98%states satisfied requirements 3-7-7 abstraction, 99.8% statessatisfied requirements 5-6-6 4-4-4-5 abstractions.5. Negative Resultsexperiments yielded positive results. explore trialsadditive approaches perform well. examining cases closely,shed light conditions might indicate approaches useful.5.1 TopSpin Location-Based Costsexperiment, used 6-6-6 abstraction (18, 4)-TopSpin Section 4.2location-based costs instead cost-splitting. primary cost operator a,operator reverses segment consisting locations + 3 (modulo 18), 1abstract space tile location operator applied distinguishedaccording abstraction 0 otherwise.definition costs disastrous, resulting Ci (ti , gi ) = 0 abstract statesabstractions. words, finding least-cost path never necessaryuse operator distinguished tile location a. always possiblemove towards goal applying another operator, a0 , primary cost 0.illustrate possible, consider state 0 4 5 6 3 2 1 (7, 4)-T opSpin.state transformed goal single move: operator reversesfour tiles starting tile 3 produces state 3 4 5 6 0 1 2 equalgoal state cyclically shifted put 0 leftmost position. 4-3abstraction move primary cost 0 abstract space based tiles 4...6,would primary cost 1 abstract space based tiles 0...3 (because tile 3leftmost location changed operator). However following sequence mapstiles 0...3 goal locations primary cost 0 abstract space (becausedont care tile always moved reference location):654fiA General Theory Additive State Space Abstractions0***3210**123*0*321**0123***5.2 Rubiks Cubesuccess cost-splitting (18,4)-TopSpin suggested might also provide improved heuristic Rubiks Cube, viewed 3-dimensional version(20,8)-TopSpin. used standard method partitioning cubies create threeabstractions, one based 8 corner cubies, others based 6 edge cubies each.standard heuristic based partitioning, hmax , expanded approximately threetimes fewer nodes hadd based partitioning primary costs defined costsplitting. result similar whether 24 symmetries Rubiks Cube useddefine multiple heuristic lookups not.believe reason cost-splitting working well (18,4)-TopSpin RubiksCube operator Rubiks Cube moves cubies number tilesmoved operator (18,4)-TopSpin. test operators moving tiles reduceseffectiveness cost-splitting solved 1000 instances (12,K)-TopSpin various valuesK, using 3-3-3-3 abstraction. results shown Table 4. Nodes Ratiocolumn gives ratio Nodes using hadd Nodes using hmax . ratio less one(highlighted bold) indicates hadd superior hmax . results clearly showhadd based cost-splitting superior hmax small K steadily loses advantageK increases. phenomenon also seen Table 1, increasing Nrelative K increases effectiveness additive heuristics based cost-splitting.K3456hmaxNodes486,5151,762,2628,978193,335,181Time2.2068.1640.043901.000hadd basedcost-splittingNodesTime207,4790.9521,388,1836.43720,0960.0952,459,204,715 11,457.000NodesRatio0.420.782.2312.72Table 4: (12, K)-TopSpin results using cost-splitting.also investigated location-based costs Rubiks Cube. cubies partitionedfour groups, containing three edge cubies two corner cubies, abstractiondefined using group. Two diagonally opposite corner positions usedreference locations (as noted above, Rubiks Cube operator changes exactly onelocations). resulting hadd heuristic weak could solve random instancespuzzle it.655fiYang, Culberson, Holte, Zahavi & Felner5.3 Pancake Puzzle Cost-SplittingTable 5 compares hadd hmax 13-Pancake puzzle costs defined usingcost-splitting. memory greater hadd hmax fractional entriescost-splitting produces require bits per entry small integer values storedhmax PDB. terms run-time number nodes generated, hadd inferiorhmax costs, opposite seen Section 4.3 using location-basedcosts.N13Abs6-7AverageSolutionLength11.791hmaxNodes166,479Time0.0466hadd basedcosting-splittingNodesTime1,218,903 0.3622Table 5: hadd vs. hmax 13-Pancake puzzle.Cost-splitting, defined Pancake puzzle, adversely affects hadd enables individual abstraction get artificially low estimates costsolving distinguished tiles increasing number dont care tiles moved.example, cost-splitting least-cost sequence operators get tile 0goal position abstract state * 0 * * * obvious single movereversing first two positions. move costs 21 , whereas 2-move sequencereverses entire state reverses first four positions costs 15 + 14 .specific example, consider state 7 4 5 6 3 8 0 10 9 2 1 1112-Pancake puzzle. Using 6-6 abstractions, minimum number moves gettiles 05 goal positions 8, 611 7, case ignorefinal locations tiles. Thus, hmax 8. contrast, hadd 6.918, lesseven smaller two numbers used define hmax . two move sequenceswhose costs added compute hadd state slightly movescorresponding sequences hmax based (10 9 compared 8 7),involve twice many dont care tiles (45 44 compared 11 17)less costly.hope pathological situation detected, least sometimes,inspecting residual costs. residual costs defined complementaryprimary costs (i.e. Ri (i ) = C() Ci (i )), done, decreasing primarycost increases residual cost. residual cost sufficiently large one abstractspaces conditions Lemma 3.10 satisfied, signalling value returnedhadd provably low. subject next section, infeasibility.6. Infeasible Heuristic Valuessection describes way increase heuristic values defined additive abstractionscircumstances. key approach identify infeasible valuesonescannot possibly optimal solution cost. identified infeasible valuesincreased give better estimate solution cost. example infeasibility occurs656fiA General Theory Additive State Space AbstractionsManhattan Distance (MD) heuristic sliding tile puzzle. well-knownparity MD(t) parity optimal solution cost state t.heuristic sliding tile puzzle returns value opposite parity,safely increased correct parity. example relies specific propertiesMD heuristic puzzle. Lemma 3.10 gives problem-independent methodtesting infeasibility, use.illustrate infeasibility detected using Lemma 3.10 consider exampleFigure 3. solution abstract problem shown middle part figurerequires 9 distinguished moves, C1 (A1 ) = 9. abstract paths solve problem9 distinguished moves require, minimum, 9 dont care moves, R1 (A1 ) = 9.similar calculation abstract space right figure yields C2 (A2 ) = 5R2 (A2 ) = 7. value hadd (A, g) therefore C1 (A1 ) + C2 (A2 ) = 9 + 5 = 14.value based assumption path original space makesC1 (A1 ) = 9 moves tiles 1, 3, 5, 7, C2 (A2 ) = 5 moves tiles. However,value R1 (A1 ) tells us path uses 9 moves tiles 1, 3, 5, 7put goal locations must make least 9 moves tiles, cannotpossibly make 5 moves. Therefore exist solution costing littleC1 (A1 ) + C2 (A2 ) = 14.illustrate potential method improving additive heuristics, Table 6shows average results IDA solving 1000 test instances 15-puzzle using twodifferent tile partitionings (shown Figure 7) costs defined method describedSection 2.1. additive heuristics parity property ManhattanDistance, infeasibility detected 2 added value. hadd columnsshow average heuristic value 1000 start states. seen infeasibilitychecking increases initial heuristic value 0.5 reduces number nodesgenerated CPU time factor 2. However, space penaltyimprovement, R values must stored pattern database additionnormal C values. doubles amount memory required, clearstoring R best way use extra memory. experiment merely showsinfeasibility checking one way use extra memory speed search problems.1 2 34 5 6 78 9 10 1112 13 14 1541526378 9 10 1112 13 14 15Figure 7: Different tile partitionings 15-puzzle (left: 5-5-5; right: 6-6-3).Slightly stronger results obtained (N, 4)-TopSpin puzzle costs definedcost-splitting, described Section 4.2. Infeasibility Check columns Table7 hadd based cost-splitting columns corresponding rowsTable 1. Comparing Infeasibility Check columns shows casesinfeasibility checking reduces number nodes generated CPU time roughlyfactor 2.657fiYang, Culberson, Holte, Zahavi & FelnerNAbs15155-5-56-6-3AverageSolutionLength52.52252.522hadd based zero-one cost-splittingInfeasibility CheckInfeasibility CheckhaddNodes TimehaddNodes Time41.56 3,186,654 0.642 42.10 1,453,358 0.31242.13 1,858,899 0.379 42.78784,145 0.171Table 6: effect infeasibility checking 15-puzzle.location-based costs used TopSpin infeasibility checking adds oneheuristic value almost every state. However, simply means statesheuristic value 1 instead 0 (recall discussion Section 5.1), stillpoor heuristic.NAbs1212121616186-64-4-43-3-3-38-84-4-4-46-6-6AverageSolutionLength9.1389.1389.13814.04014.04017.000hadd based costing-splittingInfeasibility CheckInfeasibility CheckNodesTimeNodesTime53,4600.1620,2290.07346,4461.33174,2930.621,388,1836.441,078,8534.902,137,7404.74705,7901.80251,946,069851.00 203,213,736772.04879,249,695 4,713.00 508,851,444 2,846.52Table 7: effect infeasibility checking (N, 4)-TopSpin using cost-splitting.Infeasibility checking produces almost benefit 17-Pancake puzzle locationbased costs conditions Lemma 3.10 almost never satisfied. experiment discussed end Section 4.3 showed fewer 2% states satisfyconditions Lemma 3.10 3-7-7 abstraction, fewer 0.2% statessatisfy conditions Lemma 3.10 5-6-6 4-4-4-5 abstractions.Infeasibility checking 13-Pancake puzzle cost-splitting also produceslittle benefit, different reason. example, Table 8 shows effect infeasibilitychecking 13-Pancake puzzle; results shown averages 1000 start states.1Cost-splitting state space produces fractional edge costs multiples 360360(360360 Least Common Multiple integers 1 13), therefore1infeasibility detected amount added 360360. recall hadd , cost-splitting,Pkdefined ceiling i=1 Ci (i ). value hadd therefore same, whether1360360 added not, unless sum Ci (i ) exactly integer. Table 8 shows,happen rarely.658fiA General Theory Additive State Space AbstractionsNAbs136-7AverageSolutionLength11.791hadd based costing-splittingInfeasibility Check Infeasibility CheckNodesTimeNodesTime1,218,9030.3622 1,218,789 0.4453Table 8: effect infeasibility checking 13-Pancake puzzle using cost-splitting.7. Conclusionspaper presented formal, general definition additive abstractionsremoves restrictions previous definitions, thereby enabling additive abstractions defined state space. proven heuristics based additiveabstractions consistent well admissible. definition formalizes intuitiveidea abstractions additive provided cost operator divided amongabstract spaces, presented two specific, practical methods defining abstract costs, cost-splitting location-based costs. methods applied threestandard state spaces additive abstractions according previous definitions: TopSpin, Rubiks Cube, Pancake puzzle. Additive abstractions usingcost-splitting reduce search time substantially (18,4)-TopSpin additive abstractionsusing location-based costs reduce search time 17-Pancake puzzle three ordersmagnitude state art. also report negative results, example RubiksCube, demonstrating additive abstractions always superior standard,maximum-based method combining multiple abstractions.distinctive feature definition edge abstract space two costsinstead one. inspired previous definitions treating distinguished movesdifferently dont care moves calculating least-cost abstract paths. Formalizingidea two costs per edge enabled us develop way testing heuristic valuereturned additive abstractions provably low (infeasible). test producedspeedup applied Pancake puzzle, roughly halved search time15-puzzle experiments TopSpin.8. Acknowledgmentsresearch supported part funding Canadas Natural Sciences Engineering Research Council (NSERC). Sandra Zilles Jonathan Schaeffer suggested usefulimprovements drafts paper. research also supported Israel ScienceFoundation (ISF) grant number 728/06 Ariel Felner.ReferencesBanerji, R. B. (1980). Artificial Intelligence: Theoretical Approach. North Holland.Chen, T., & Skiena, S. (1996). Sorting fixed-length reversals. Discrete Applied Mathematics, 71 (13), 269295.659fiYang, Culberson, Holte, Zahavi & FelnerCulberson, J. C., & Schaeffer, J. (1996). Searching pattern databases. AdvancesArtificial Intelligence (Lecture Notes Artificial Intelligence 1081), pp. 402416.Springer.Culberson, J. C., & Schaeffer, J. (1994). Efficiently searching 15-puzzle. Tech. rep.TR94-08, Department Computing Science, University Alberta.Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,14(3), 318334.Edelkamp, S. (2001). Planning pattern databases. Proceedings 6th EuropeanConference Planning, pp. 1324.Edelkamp, S. (2002). Symbolic pattern databases heuristic search planning. Proceedings Sixth International Conference AI Planning Scheduling (AIPS-02), pp.274283.Erdem, E., & Tillier, E. (2005). Genome rearrangement planning. ProceedingsTwentieth National Conference Artificial Intelligence (AAAI-05), pp. 11391144.Felner, A., & Adler, A. (2005). Solving 24 puzzle instance dependent patterndatabases. Proc. SARA-2005, Lecture Notes Artificial Intelligence, 3607, 248260.Felner, A., Korf, E., & Hanan, S. (2004). Additive pattern database heuristics. JournalArtificial Intelligence Research, 22, 279318.Felner, A., Zahavi, U., Schaeffer, J., & Holte, R. (2005). Dual lookups pattern databases.Proceedings Nineteenth International Joint Conference Artificial Intelligence (IJCAI-05), pp. 103108.Gaschnig, J. (1979). problem similarity approach devising heuristics: First results.Proceedings Sixth International Joint Conference Artificial Intelligence(IJCAI-79), pp. 301307.Guida, G., & Somalvico, M. (1979). method computing heuristics problem solving.Information Sciences, 19, 251259.Haslum, P., Bonet, B., & Geffner, H. (2005). New admissible heurisitics domainindependent planning. Proceedings Twentieth National Conference Artificial Intelligence (AAAI-05), pp. 11631168.Haslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independentconstruction pattern database heuristics cost-optimal planning. ProceedingsTwenty-Second National Conference Artificial Intelligence (AAAI-07), pp.10071012.Hell, P., & Nesetril, J. (2004). Graphs homomorphisms. Oxford Lecture SeriesMathematics Applications, 28.Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics optimalsequential planning. Proceedings 17th International Conference AutomatedPlanning Scheduling (ICAPS-07), pp. 176183.Hernadvolgyi, I., & Holte, R. C. (2000). Experiments automatically created memorybased heuristics. Proc. SARA-2000, Lecture Notes Artificial Intelligence, 1864,281290.660fiA General Theory Additive State Space AbstractionsHernadvolgyi, I. T. (2003). Solving sequential ordering problem automaticallygenerated lower bounds. Proceedings Operations Research 2003 (Heidelberg,Germany), pp. 355362.Holte, R. C., Perez, M. B., Zimmer, R. M., & MacDonald, A. J. (1996). Hierarchical A*:Searching abstraction hierarchies efficiently. Proceedings Thirteenth NationalConference Artificial Intelligence (AAAI-96), pp. 530535.Holte, R. C., Felner, A., Newton, J., Meshulam, R., & Furcy, D. (2006). Maximizingmultiple pattern databases speeds heuristic search. Artificial Intelligence, 170,11231136.Holte, R. C., Grajkowski, J., & Tanner, B. (2005). Hierarchical heuristic search revisited.Proc. SARA-2005, Lecture Notes Artificial Intelligence, 3607, 121133.Holte, R. C., & Hernadvolgyi, I. T. (2004). Steps towards automatic creation searchheuristics. Tech. rep. TR04-02, Computing Science Department, University Alberta,Edmonton, Canada T6G 2E8.Holte, R. C., Newton, J., Felner, A., Meshulam, R., & Furcy, D. (2004). Multiple patterndatabases. Proceedings Fourteenth International Conference AutomatedPlanning Scheduling (ICAPS-04), pp. 122131.Katz, M., & Domshlak, C. (2007). Structural patterns heuristics: Basic idea concrete instance. Proceedings ICAPS-07 Workshop Heuristics Domain-independentPlanning: Progress, Ideas, Limitations, Challenges.Kibler, D. (1982). Natural generation admissible heuristics. Tech. rep. TR-188, UniversityCalifornia Irvine.Korf, R. E. (1985). Depth-first iterative-deepening: optimal admissible tree search.Artificial Intelligence, 27 (1), 97109.Korf, R. E. (1997). Finding optimal solutions Rubiks Cube using pattern databases.Proceedings Fourteenth National Conference Artificial Intelligence (AAAI97), pp. 700705.Korf, R. E., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence,134, 922.Korf, R. E., & Taylor, L. A. (1996). Finding optimal solutions twenty-four puzzle.Proceedings Thirteenth National Conference Artificial Intelligence (AAAI96), pp. 12021207.Mostow, J., & Prieditis, A. (1989). Discovering admissible heuristics abstractingoptimizing: transformational approach. Proceedings International JointConference Artificial Intelligence (IJCAI-89), pp. 701707.Pearl, J. (1984). Heuristics. Addison Wesley, Reading, MA.Prieditis, A. E. (1993). Machine discovery effective admissible heuristics. Machine Learning, 12, 117141.Valtorta, M. (1984). result computational complexity heuristic estimatesA* algorithm. Information Science, 34, 4859.661fiYang, Culberson, Holte, Zahavi & FelnerYang, F., Culberson, J., & Holte, R. (2007). general additive search abstraction. Tech. rep.TR07-06, Computing Science Department, University Alberta, Edmonton, CanadaT6G 2E8.Zahavi, U., Felner, A., Holte, R., & Schaeffer, J. (2006). Dual search permutationstate spaces. Proceedings Twenty-First Conference Artificial Intelligence(AAAI-06), pp. 10761081.662fiJournal Artificial Intelligence Research 32 (2008) 901-938Submitted 08/07; published 08/08Ultrametric ConstraintApplication PhylogeneticsNeil C.A. Moorencam@cs.st-andrews.ac.ukComputer Science, University St. Andrews, ScotlandPatrick Prosserpat@dcs.gla.ac.ukComputing Science, Glasgow University, ScotlandAbstractphylogenetic tree shows evolutionary relationships among species. Internal nodestree represent speciation events leaf nodes correspond species. goalphylogenetics combine trees larger trees, called supertrees, whilst respectingrelationships original trees. rooted tree exhibits ultrametric property;is, three leaves tree must one pair deeper recent commonancestor pairs, three recent common ancestor.inspires constraint programming encoding rooted trees. present efficientconstraint enforces ultrametric property symmetric array constrainedinteger variables, inevitable property lower bounds three variablesmutually supportive. show allows efficient constraint-based solutionsupertree construction problem. demonstrate versatility constraintprogramming exploited allow solutions variants supertree constructionproblem.1. IntroductionOne grand challenges phylogenetics build Tree Life (ToL), representation evolutionary history every living thing. date, biologists catalogued1.7 million species, yet estimates total number species range 4 100million. 1.7 million species identified 80,000 placed ToLfar (Pennisi, 2003). applications ToL: help understand pathogensbecome virulent time, new diseases emerge, recognise species riskextinction (Pennisi, 2003; Mace, Gittleman, & Purvis, 2003). One approach buildingToL divide conquer: combining smaller trees available TreeBase(TreeBASE, 2003) so-called supertrees (Bininda-Emonds, 2004) approachcomplete ToL.date, supertree construction dominated imperative techniques (Semple& Steel, 2000; Semple, Daniel, Hordijk, Page, & Steel, 2004; Daniel, 2003; Bordewich,Evans, & Semple, 2006; Ng & Wormald, 1996; Bryant & Steel, 1995; Page, 2002)recently new declarative approaches emerged using constraint programming (Gent,Prosser, Smith, & Wei, 2003; Prosser, 2006; Beldiceanu, Flener, & Lorca, 2008) answerset programming (Wu, You, & Lin, 2007). One properties rooted trees suitsapproaches trees nature ultrametric: rooted trees root nodedepth 0, depth nodes 1 plus depth parent. Takingc2008AI Access Foundation. rights reserved.fiMoore & Prosserthree leaves a, b c pairs must one pair deeper recent commonancestor (mrca) pairs, three pairs mrca.mean ultrametric, three tie minimum. fact,know depth mrca pairs leaves structure tree uniquelydetermined. inspires constraint programming encoding rooted trees, usingultrametric constraint define later. explore solutions phylogeneticsupertree problem variants. doing, show practicality ultrametricencoding rooted tree problems, well arguing valuable additionset techniques supertree problems.paper organised follows. First, introduce constraint programmingsupertree construction problem. propose specialised ultrametric constraint,terms propagation procedures, maintains bounds(Z)-consistency (Bessiere, 2006)three variables. show specialised constraint required modelsuse toolkit primitives cannot guarantee ultrametric property supertree problem via propagation alone. Furthermore, space complexity models becomesprohibitive. ultrametric constraint extended maintain propertysymmetric matrix variables. go show constraint efficientlyapplied problem supertree construction, particular applying propagationmodel gives polynomial time procedure supertree construction. demonstrate real data give justification improvement timespace previous constraint encodings. One benefits constraint programming approach variants supertree problem addressed withinone model. justify assertion proposing constraint solution finding essentialrelations supertree (Daniel, 2003), addressing ancestral divergence dates (Sempleet al., 2004; Bryant, Semple, & Steel, 2004), modelling nested taxa (Page, 2004; Daniel &Semple, 2004) coping conflicting data.2. Backgroundsection give necessary definitions descriptions Constraint SatisfactionProblem (Tsang, 1993), Constraint Programming, Supertree problem.2.1 Constraint Programming CSPConstraint Programming (CP) (Rossi, van Beek, & Walsh, 2007) declarative styleprogramming problems modelled CSP, i.e., set variablesassigned values variables domains satisfy set constraints. Valuesmight typically integers drawn finite domains, real numbers ranges,complex entities like sets graphs. considering integers.Definition 1. constraint satisfaction problem (CSP) triple (V, D, C) V setn variables {v1 , . . . , vn }; = {dom(v1 ), . . . , dom(vn )} collection domains,totally ordered set integer values; C = {c1 , . . . , ce } set e constraints,scope variables scope(c) = (vc1 , . . . , vck ) relation rel(c) dom(vc1 ). . .dom(vck ).assignment value x dom(v) variable vi V denoted (vi , x). constraintc C satisfied assignment {(vc1 , xc1 ), . . . , (vck , xck )} scope(c) = (vc1 , . . . , vck )902fiThe Ultrametric Constraint Phylogenetics(xc1 , . . . , xck ) rel(c). set assignments {(v1 , x1 ), . . . , (vn , xn )} involving everyvariable problem solution satisfies constraints C.constraint solver finds solution CSP via process constraint propagationsearch. Constraint propagation inferencing process takes place variableinitialised loses values. Propagation maintains level consistency, arcconsistency (Mackworth, 1977), across variables, removing values domainscannot occur solution (i.e., removing unsupported values). use definitions(generalized) arc-consistency ((G)AC) due Bessiere (2006):Definition 2. Given CSP (V, D, C), constraint c C scope(c) = (vc1 , . . . , vck ),variable v scope(c), value x dom(v) consistent respect c (alternatively,supported c) iff exists satisfying assignment = {(vc1 , a1 ), . . . , (vck , ak )} c(v, x) i, ai dom(vci ). domain dom(v) (generalized) arcconsistent c iff values dom(v) consistent respect c, CSP(generalized) arc-consistent variable domains (generalized) arc-consistentconstraints C.Arc-consistency established CSP using algorithm AC3 (Mackworth, 1977). sake exposition assume constraints C binaryconstraint c counterpart c scope(c) = (va , vb )scope(c ) = (vb , va ) rel(c ) = rel1 (c). example, constraintcxy = x < also constraint cyx = > x. heart AC3 revisefunction, takes binary constraint c argument delivers Boolean result.function removes dom(va ) values support dom(vb ) w.r.t.constraint c, returns true removals take place. Initially constraints addedset S. Constraints iteratively removed revised. revise(ckm ) returnstrue becomes {cik |cik C 6= k 6= m}. step consideredpropagation domain reduction variable vk variables constrained vk .iteration terminates empty variables domain becomes empty.empty arc-consistency algorithm reached fixed point (i.e., applicationarc-consistency process effect domains variables)problem made arc-consistent. domain empties, shownsolutions globally hence stop. AC3 algorithm O(e d3 ) timecomplexity, e number constraints size largest domain, however algorithms achieve time bound O(e d2 ) (Yuanlin & Yap, 2001; Bessiere& Regin, 2001).demonstrate arc-consistency example Figure 1 Smith (1995).three constrained integer variables x, z, integer domain {1..5},binary constraints cxy : x < 2, cyz : + z even, czx : z < 2x + 1. Sinceconstraints binary represent problem constraint graph, nodesvertices edges constraints. Initially constraint cxy revised respect xvalues {3..5} removed dom(x). cxy revised w.r.t. dom(y)becomes {4, 5}. cyz revised w.r.t. effect revised w.r.t. z,effect. Revising czx w.r.t. z reduces dom(z) becomes {1..4}, consequentlyconstraint cyz added set constraints pending revision. Constraint czx903fiMoore & Prosser{1..5}sezi2y+nx<x{1..5}z {1..5}z < 2x + 1Figure 1: binary constraint satisfaction problem.import choco.Problem;import choco.ContradictionException;import choco.integer.*;public class BMStut {public static void main(String[] args) throws ContradictionException {Problem pb= new Problem();IntDomainVar x= pb.makeEnumIntVar("x",1,5); // x {1..5}IntDomainVar= pb.makeEnumIntVar("y",1,5); // {1..5}IntDomainVar z= pb.makeEnumIntVar("z",1,5); // {1..5}IntDomainVar even = pb.makeEnumIntVar("even",new int[] {2,4,6,8,10});pb.post(pb.gt(pb.minus(y,2),x));// x < - 2pb.post(pb.gt(pb.plus(pb.mult(2,x),1),z)); // z < 2x + 1pb.post(pb.eq(even,pb.plus(y,z)));// + z evenpb.solve();// solve using MAC}}Figure 2: JChoco constraint program CSP Figure 1.revised w.r.t. x cyz w.r.t. y, effect. revision set pointempty arc-consistency established variable domains dom(x) = {1, 2},dom(y) = {4, 5}, dom(z) = {1..4}.Solving CSP may involve search, i.e., might need try different values variables order determine solution exists. Typically constraint solver beginestablishing arc-consistency, repeatedly select variable assign valuedomain (instantiate it). effectively reduces variables domain singleton,arc-consistency re-established. succeeds another instantiation made,fails backtrack undoing recent instantiation. called MAC,maintaining arc-consistency (Sabin & Freuder, 1994).Figure 2 shows constraint program problem Figure 1 using choco constraint programming toolkit Java language (Choco, 2008), finds solutionx = 1, = 4, z = 2 first.Constraint toolkits tend based around AC5 algorithm (van Hentenryck, Deville,& Teng, 1992), allowing propagators specialised specific constraints resultingimproved efficiency adaptability. AC5 amends set AC3 contain triplesform (v, c, ) v scope(c) set values lost v, consequently904fiThe Ultrametric Constraint Phylogeneticsrevision efficient propagation focus values may lost support,rather check every value support. object-oriented toolkit languageconstraint associated propagation methods implemented,methods activated domain event occurs variable involved constraint.Domain events initialisation variable, increase lower bound,decrease upper bound, removal value bounds, instantiationvariable. exhaustive list, however toolkits allow one event:one values lost propagator writer must determineaction take. give examples using toolkit specialised constraints,modelling routing problem might constrained integer variable locationvisited, domain values corresponding index next destination (theso-called single-successor model). subtour elimination constraint (Caseau & Laburthe,1997) might used ensure legal tours produced, Regins alldifferent constraint (Regin, 1994) could added increase domain filtering. pickdelivery variant, side constraints could added ensure locationsvisited others. job shop scheduling problem might model uses0/1 variables decide relative order pairs activities share resource,might increase propagation adding Carlier Pinsons edge finding constraint (1994).constraint programming approach general practical modelling solvingproblems, provides framework combination problem specific algorithmsone solver. allows us solve many classes problems efficiently model evenproblems via addition side constraints.2.2 Supertree ProblemSupertree construction problem phylogenetics combine leaf-labelledspecies trees, sets leaf labels intersect, single tree respectsarboreal relationships input tree (Bininda-Emonds, 2004). Species trees describepart evolutionary history set species. Labels leaves correspond existingspecies internal nodes represent divergence events evolutionary history onespecies split least two species. Species trees may also annotated datesinternal nodes, representing time divergence event happened.define term displays, makes precise mean respectsarboreal relationships: supertree T1 displays tree T2 T2 equivalent T4(i.e. induce hierarchy leaf labels) T4 obtained followingsteps (Semple & Steel, 2000):1. Let L set leaves T1 T2 .2. Let T3 unique subtree T1 connects leaves L.3. obtain T4 : wherever subpath (p1 , . . . , pk ) path rootleaf T3 p2 , . . . , pk1 interior nodes degree 2, contractsingle edge.problem produce rooted species tree forest input trees F,contains species F displays every tree F . Figures 3 4 illustratedisplays property.905fiMoore & Prosser10T11010T310bc10e1 001201101 001 0110e01101 001 01=4210101 001 01 01 0110L={a,d,e}eeFigure 3: example tree T1 displays tree T210T110L={a,d,e}10101 001 01 01 01cbeT4 = 21010011 001 011 001T210T310e10e01101 001 01eFigure 4: example tree T1 display tree T2906fiThe Ultrametric Constraint Phylogeneticssay two trees T1 T2 compatible (incompatible) exists (doesntexist) third tree T3 displays T1 T2 . Variants supertree problempreviously published solved specialist bioinformatics literature includefinding solutions1 , counting solutions, finding conserved relationships supertrees(Daniel, 2003), incorporating nested taxa (Semple et al., 2004), incorporating ancestral divergence dates (Semple et al., 2004) possibility contradictory input data (Semple& Steel, 2000).3. Ultrametric Constraintultrametric constraint first proposed Gent et al. (2003) within contextsupertree construction (Bininda-Emonds, 2004), implemented using toolkit primitives. review encoding show constraint toolkits inefficientterms space time. motivates creation specialised ultrametricpropagator three variables, maintains ultrametric property boundsvariables. presented describing necessary propagation methods.extend specialised propagator maintains ultrametric propertysymmetric matrix variables.3.1 Previous Work Ultrametric ConstraintFirst, give definition ultrametric constraint.Definition 3. ultrametric constraint three variables (henceforth, Um-3) x, zconstrains that:(x > = z) (y > x = z) (z > x = y) (x = = z)(1)constraint ensures tie least element three, i.e., eitherthree same, two greater. constraintproposed Gent et al. (2003), used Prosser (2006) times implementedliteral translation Equation 1 using toolkit primitives. Evidence obtainedJChoco, ECLiPSe ILog constraint programming toolkits shows propagationdone lower bounds combination primitive constraints. due disjunctive constraints since many constraint programming toolkits propagation delayedone disjuncts true, known delayed-disjunction consistency(van Hentenryck, Saraswat, & Deville, 1998). Consequently, encoding valuescannot occur satisfying assignment might pruned domainvariable. Consider case three variables: x {1, 2, 3}, {2, 3} z {3}.domains variables already fixed point respect delayed-disjunctionconsistency ultrametric assignment x takes value 1, i.e., delayeddisjunction propagation achieve arc-consistency. shall see later, findingsolution supertree problem using toolkit constraints result backtrackingsearch, prefer avoid this. course, higher levels consistency would overcomethis, constructive-disjunction consistency (van Hentenryck et al., 1998), singleton1. may multiple supertrees set input trees.907fiMoore & Prosserarc-consistency (Debruyne & Bessiere, 1997) filtering algorithm Lhomme (2003).However, cost greater average case delayed-disjunction, preventing use toolkits. fact Um-3 constraint especially unfortunatelower bounds may trimmed properly:Lemma 1. Um-3 constraint, lower bounds supported (i.e., form ultrametric instantiation values constrained variables), support other.Proof. Consider three supported lower bounds. Suppose contradiction twoleast distinct. One distinct lowest cannot supportedaccount fact equal anything larger anything. Thereforecontradiction two least must equal. However lower bound leastlarge these, lower bounds mutually supportive.Lemma important implications species tree model presenteddetail Section 4: particular, lower bounds bounds(Z)-consistency modelform solution, bounds(Z)-consistency (Bessiere, 2006) defined followsDefinition 4. Given (V, D, C) constraint c C scope(c) = (vc1 , . . . , vck ), tuple= (xc1 , . . . , xck ) bound support rel(c) xci , min(dom(vci ))xci max(dom(vci )). constraint c bounds(Z)-consistent vci scope(c)exist bound supports involving min(dom(vci )) max(dom(vci )). CSP bounds(Z)consistent every constraint c C bounds(Z)-consistent.henceforth abbreviate bound support support bounds(Z)-consistencyBC(Z). BC(Z) differs AC puts weaker conditions values comprisesupport: rather domain, needlower upper bounds domain. means BC(Z) prunes subsetvalues AC can, general. Weaker levels consistency BC(Z) usefulcertain problems prune number values AC easily,fewer values much quickly. case, BC(Z) interesting levelconsistency enough ensure problem solved propagationsearch, shall see.3.2 Design BC(Z) UM-3 PropagatorSection describe Um-3 propagator enforces BC(Z), namely UM-3-BCZ.3.2.1 Analysis Lower Upper Boundssection dont take account domains becoming empty. Since analyse lowerupper bounds isolation lower bound may pass upper bound vice-versa,thereby emptying domain. happens propagator described Section 3.2.2enforce BC(Z), rather terminate. problem, domainbecoming empty means solution continue would waste time.Concordantly, section, analyse lower bounds assume upperbound domain cannot become null reason, vice-versa.908fiThe Ultrametric Constraint PhylogeneticsCode style: Variables v1 , v2 , v3 , S, L constrained integer variables, synonymous domains. Consequently variable x considered domain x.lbx.ub return references lower upper bounds respectively; SortOnLowerBounds(x,y,z)returns tuple references variables x, z non-decreasing order lower bounds;SortOnUpperBounds analogous; let (S,M ,L) . . . names references S, L;result S.lb .lb, lower bound assigned equal value lower bound ,although .lb subsequently changes distinct again; expression x b returnsintersection domains [max(x.lb,y.lb) . . . min(x.up,y.up)].Algorithm UM-3-BCZLBFix(v1 ,v2 ,v3 )A1let (S, M, L) SortOnLowerBounds(v1 , v2 , v3 )A2(S.lb < .lb)A2.1S.lb .lbUBFix(v1 ,v2 ,v3 )A3let (S, M, L) SortOnUpperBounds(v1 , v2 , v3 )A4(S.ub < .ub)A4.1(S b L = )A4.2.ub S.ubA4.3else (S b = )A4.4L.ub S.ubMin event(v1 ,v2 ,v3 )A5LBFix(v1 ,v2 ,v3 )A6domains non-emptyA6.1UBFix(v1 ,v2 ,v3 )Max event(v1 ,v2 ,v3 )A7UBFix(v1 ,v2 ,v3 )Fix event(v1 ,v2 ,v3 )A8LBFix(v1 ,v2 ,v3 )A9domains non-emptyA9.1UBFix(v1 ,v2 ,v3 )Figure 5: Algorithm UM-3-BCZ propagator909fiMoore & Prosser(1)(2)l(3)(4)ls=ms=m=lm=lFigure 6: Cases analysis LBFixL(1)lL(3)(2)(4)Figure 7: Cases analysis UBFixprocedure LBFix Figure 5 takes input three variables removes unsupported values lower bounds domain. intuition algorithm achievingone needs involved tie least element, hence smallestlower bound strictly less others must unsupported.possible states lower bounds LBFix invoked summarised Figure 6.Either three different (case 1), three (case 2) twoone different (cases 3 4). give relationships bounds point timelower bound may unsupported. boxes Figure 6 shaded follows:regions shaded black removed propagation whereas gray regions supported.diagrams supposed suggest that, example case 1, bounds differ1. Rather two bounds lined bound onedifferent another different non-zero unspecified amount. Hencedescribe relationships actual values.following shows LBFix removes unsupported values removesupported values.Lemma 2. LBFix invoked lower bounds argument variables supportedw.r.t. Um-3 constraint supported values removed.Proof. cases 1 4 (Figure 6) condition line A2 satisfied, line A2.1executed, results removal unsupported range inspectionremaining bounds mutually supportive. cases 2 3, condition line A2failed changes made domains; bounds mutually supportive.procedure UBFix Figure 5 job upper bounds LBFixlower bounds. following Lemma justifies assertion cases usedproof shown Figure 7:Lemma 3. UBFix invoked either910fiThe Ultrametric Constraint Phylogeneticsupper bounds argument variables supported w.r.t. UM-3-BCZ constraint supported values removed,domain null result removing unsupported values.Proof. Let S, L domains smallest, middle largest upper bounds,breaking ties arbitrarily. Let s, l upper bounds.First prove case 1 (Figure 7) shaded region supportedL b 6= : Potentially, bound supportedequal values L least small (i.e., b L 6= ),equal value either L, value least large remaining domain.However, notice latter impossible due fact L contains equalvalue, value large this.Similar arguments establish shaded regions L case 1, case 3L case 3 supported b 6= , L b 6= b 6= , respectively.establish Lemma cases 1, 2, 3 4:Cases 2 4 condition line A4 false, domains changed.upper bounds mutually supportive case.Case 1 shaded region unsupported L b = . HenceUBFix line A4.2 executed unsupported region removed. upperbounds l L, mutually supportive.Case 1 shaded region supported Lb 6= . shadedregion L also supported b 6= neither line A4.2 A4.4 executedchanges made domains. upper bound also supported,L, S. shaded region L supported b =line A4.4 executed resulting removal region. new bounds S,L mutually supportive.Case 3 shaded regions L supported above, L b 6=b 6= . Hence domain changes result executing UBFix. l Lsupported l L, S. supported L, S.Case 3 shaded regions L unsupported above, L b =b = line A4.2 UBFix executed results becoming null.Case 3, shaded region supported shaded region L supportedLb 6= b = , A4.4 executed remove unsupportedregion. new bounds S, L mutually supportive.Case 3, shaded region L supported shaded region supportedSymmetric previous case.Note analog Lemma 1 upper bounds since, example, boundsx = {1, 2, 3}, = {1, 2} z = {1} supported, mutually supportive.911fiMoore & Prosser3.2.2 Propagation Algorithmpresented LBFix UBFix position present complete propagation algorithm. propagator works arbitrary domains enforces BC(Z),except domain becomes empty, case work. algorithmdescribed action taken three domain events occur:min domain lost lower bound since propagator last invoked.max domain lost upper bound since propagator last invoked.fix domain singleton, i.e., variable instantiated upper lower boundsequal.i.e. consider events bounds variables. algorithm listed linesA5-A9 Figure 5. Intuitively procedures work because, show, changeupper bound affect support upper bounds, change lowerbound affect support lower upper bounds. Hence need runLBFix lower bound may changed, UBFix must run change eitherlower upper bounds. Whilst would correct cycle trimming upperlower bounds fixed point reached (i.e. changes occur), guaranteefixed point easily.Lemma 4. possible change lower bound result loss supportanother lower bound.Proof. bounds diagram support, black shaded lowerbound lost dark gray shaded lower bound loses support.Lemma 5. possible change lower bound result loss supportupper bound.Proof. bounds diagram support, black shaded lowerbound lost dark gray shaded upper bound loses support.Lemma 6. possible loss upper bound cause loss supportanother upper bound.912fiThe Ultrametric Constraint PhylogeneticsProof. bounds diagram support, black shaded upperbound lost dark gray shaded upper bound loses support.Corollary 1. impossible change upper bound result loss supportlower bound.Proof. Lemma 1 lower bound retains support long lower boundsintact, hence losing upper bound effect.asymmetry upper lower bounds? due asymmetrydefinition UM-3-BCZ practical repercussion BC(Z) lower boundsmust mutually supportive whereas BC(Z) upper bounds may may requiresupport values including lower bounds.Corollary 1 suggests improvement algorithm Figure 5execute Line A6 A9 lower bound lost remaining supportupper bound. However, conditionals intrinsic UBFix amount muchthing little point repeating them.point theorems build complete proof correctnessBC(Z) status:Theorem 1. code min, max fix events listed Figure 5 removevalues involved bound supports UM-3-BCZ constraint, and, domainsnon-null propagation, resultant domains BC(Z).Proof. First must establish values removed propagation couldinvolved support, result domains subsets input domains.former immediate Lemmas 2 3, values removed resultexecuting LBFix UBFix. latter immediate inspection LBFix UBFix,ever make lower bounds larger upper bounds smaller.final thing establish BC(Z) enforced, unless domain becomes null.domain becomes empty result running algorithm Theoremtrivially true.domain becomes empty must show bounds supported.lower bounds, Lemma 4 Corollary 1 know loss lower boundresult need change lower bound propagation. Lower boundschange result either fix min events, hence propagator Figure 5 runs LBFixeither event. LBFix runs leaves lower bounds supported, shownLemma 2. upper bounds, Lemmas 5 6 know loss either lowerupper bound result loss upper bound. Hence upper lower boundschange result event, lower bounds also change result LBFix,hence propagator runs UBFix events, runs LBFix finished,necessary. UBFix runs leaves upper bounds supported provided domainbecomes empty, shown Lemma 3.913fiMoore & Prosser100101010101(a)100101010101010101010101(b)Figure 8: Propagation done 2 domains singletonpropagation algorithm runs (1) time. operationsLBFix, UBFix, min, max fix events (1), provided domain representationallows access upper lower bounds (1). guaranteed domainreductions occur bounds, case here, domains represented usingone structures proposed van Hentenryck et al. (1992).3.3 EntailmentSchulte Carlsson (2006) define entailment possible constractionsdomains constraints scope consistent. detect happenedstop running propagator henceforth, since cannot prune values.Definition 5. propagator entailed domains = {d1 , . . . , dn } setdomains subsets these, i.e., E = {e1 , . . . , en } s.t. i.ei di , fixedpoint.describe sufficient condition UM-3-BCZ constraint entailed, i.e.,UM-3-BCZ constraint becomes entailed soon two variables singleton domains:Theorem 2. UM-3-BCZ becomes entailed soon two variables singleton domains.Proof. Consider possible scenarios: either two singletons (case (a)Figure 8) distinct (case (b) Figure 8). domains propagationshown Figure 8 boxes; domains propagation shaded gray. Clearlyremaining choices third variable valid instantiations since propagationalgorithm safe cannot removed propagation definitionpropagation fixed point.3.4 Ultrametric Matrix Constraintsupertree model presented Section 4 makes use ultrametric constraint, howevercontext desired end product constrain whole matrix ultrametricmatrix, merely constrain three variables.914fiThe Ultrametric Constraint PhylogeneticsCode style: let (i, j) index(v) declares j indices variable v matrixconstraint over.Algorithm UM-Matrix-BCZMin event(v)A1let (i, j) index(v)A2k 1 . . . nA2.1k 6= k 6= jA2.2Min event(Mij , Mik , Mjk )A2.3Max event(Mij , Mik , Mjk )Max event(v)A3let (i, j) index(v)A4k 1 . . . nA4.1k 6= k 6= jA4.2Max event(Mij , Mik , Mjk )Fix event(v)A5let (i, j) index(v)A6k 1 . . . nA6.1k 6= k 6= jA6.2Min event(Mij , Mik , Mjk )A6.3Max event(Mij , Mik , Mjk )Figure 9: Algorithm UM-Matrix-BCZ propagatorDefinition 6. symmetric matrix ultrametric matrix every setthree distinct indices i, j k, tie minimum Mij , Mik Mjk ;Mii = 0 i.ultrametric matrix constraint achieved matrix posting constraint UM-3-BCZ(Mij , Mik , Mjk ) choices distinct i, j k, costintroducing n3 constraints. practical constraint solvers model containing constraint (n3 ) space complexity, since solver must list (n3 )constraints stored somewhere. However, domain event occurs matrix variable Mij straightforward iterate k indices propagationUM-Matrix-BCZ Figure 5. replaces (n3 ) space list representation setUM-3-BCZ constraints (1) code representation. Hence propose ultrametricmatrix constraint propagator UM-Matrix-BCZ Figure 9.propagator mimics part AC3 algorithm (Mackworth, 1977) since (a)receives propagation event variable, (b) identifies constraintsvariable, (c) arranges propagation carried out. events causedresult queued dispatched underlying propagator normal maycause UM-Matrix-BCZ run again. variable involved n 2constraints, since variable two indices matrix constraintinvolving choice three different indices.n3algorithm propagates (n) time, expensive per event usingUm-3 constraints, factor n fewer propagators wake result event.915fiMoore & ProsserbccbbcbcFigure 10: four possible relationships three leaf nodes tree: i.e. threetriples (ab)c, (ac)b, (bc)a, fan (abc).4. Supertree Constructionreview imperative solutions supertree construction problem, review firstconstraint programing solution (Gent et al., 2003), present new encoding exploitsspecialised UM-Matrix-BCZ constraint.4.1 Imperative Solutions Supertree Problemearliest imperative techniques due Bryant Steel (1995) Ng Wormald(1996). present OneTree algorithm based Build algorithmAho, Sagiv, Szymanski, Ullman (1981). OneTree based observationtree three leaf nodes define unique relation respect recentcommon ancestor (mrca), mrca(a, b) interior node furthest rootleaf nodes b descendants. abuse notation writing mrca(a, b) >mrca(c, d) former greater depth latter, similarly mrca(a, b) =mrca(c, d) depth. Given three different leaf nodes/species (labelleda, b, c) one following four relations must hold:(1)mrca(a, b) > mrca(a, c) = mrca(b, c)(2)mrca(a, c) > mrca(a, b) = mrca(c, b)(3)mrca(b, c) > mrca(b, a) = mrca(c, a)(4)mrca(a, b) = mrca(a, c) = mrca(b, c)say (1), (2) (3) triples (ab)c, (ac)b, (bc)a (where(xy)z read x closer z) (4) fan (abc), i.e.,fan relationship species unresolved dont specify pairclosely related. shown Figure 10. Prior applying OneTree algorithm two(or more) species trees broken triples fans using BreakUp algorithm(Ng & Wormald, 1996), resulting linear sized encoding trees. supertreeconstructed (if possible) using encoding input.Figure 11 shows example BreakUp algorithm process. Two variantsprocess shown; top hard breakup, fans considered hardevidence must respected (hard polytomies described Ng Wormald, 1996)soft breakup fans taken lack evidence (soft polytomiesdescribed Bryant Steel, 1995). hard breakup algorithm modified916fiThe Ultrametric Constraint PhylogeneticsebcbcgBREAKBREAKBREAK(a b c)(a b d)(a c d)(b c d)(cd)e(de)f(ef)gecefBREAKffgefgefgBREAKBREAKBREAKBREAK(ab)e,(bc)e(cd)e(de)f(ef)gegc{(abc),(abd),}(bcd),(cd)e,(de)f,(ef)g}fgefgef{(ab)e,(bc)e,}(cd)e,(de)f,(ef)g}gFigure 11: Example execution BreakUp algorithm. top, hard breakup,soft breakup (no fans produced)Code style: function sortedInteriorNodes(T ) delivers set interior nodes tree nonincreasing order depth tree; degree(v) delivers degree node v; function child(v, i)delivers ith child interior node v; uncleOrCousin(l) delivers leaf node descendedsibling parent leaf node l; function becomesLeaf(v, l) transforms interior node v leafnode labelled l; removeChild(l, v) removes leaf node l list children interior node v.Algorithm HardBreakupHardBreakup(T )1let V sortedInteriorNodes(T )2let3let 04notRoot(V [i]) degree(V [i]) > 25let v V [i]6let c0 child(v, 0)7degree(v) = 28let c1 child(v, 1)9let c2 uncleOrCousinOf(c0 )10{triple(c0 , c1 , c2 )}11v becomesLeaf(v, c0 )12ii+113else j 1 degree(v) 214k j + 1 degree(v) 115let c1 child(v, j)16let c2 child(v, k)17{fan(c0 , c1 , c2 )}18v removeChild(c0 , v)19 returnFigure 12: Hard breakup tree , producing triples fans.encountering kfan broken n3 3-fans, soft breakupfan broken linear number rooted triples. Algorithms hard soft breakupsgiven Figures 12 13, used imperative OneTree algorithmconstraint programming models.917fiMoore & ProsserAlgorithm SoftBreakupSoftBreakup(T )1let V sortedInteriorNodes(T )2let3let 04notRoot(V [i])5let v V [i]6let c0 child(v, 0)7let c1 child(v, 1)8let c2 uncleOrCousinOf(c0 )9{triple(c0 , c1 , c2 )}10degree(v) = 211v becomesLeaf(v, c0 )12ii+113else v removeChild(c0 , v)14 returnFigure 13: Soft breakup tree , producing triples.cbcecbcebeFigure 14: toy input (left) single solution supertree problem (right). Inputtrees distorted make relationships resultant supertree obvious.toy set input triples single solution shown Figure 14. triplesdrawn reflect solution compatible them.Ng Wormald (1996) give complexity OneTree O(h(n)) h(n) =n(n + + bn)(n + + f ), n number labels, number triples, f numberfans, b sum squares number leaves fans, inverseAckermann function (and less 4 conceivable inputs behaves likeconstant). Therefore input trees fully resolved (i.e., fans) running918fiThe Ultrametric Constraint Phylogeneticstime complexity O(n2 ) worst case complexity grows O(n4 ).contrasted O(t n) complexity Bryant Steels OneTree (1995).4.2 Constraint Encoding using Toolkit Constraintssecond stage, i.e., OneTree equivalent, first solved constraint programGent et al. (2003). encoding takes advantage equivalence ultrametrictrees ultrametric matrices:Definition 7. Let real symmetric n n matrix. ultrametric treerooted tree that:1. n leaves, corresponding unique row M;2. internal node least 2 children;3. two leaves j, Mij label recent common ancestorj;4. along path root leaf, labels strictly increase.Theorem 3. symmetric matrix ultrametric treeultrametric matrix. Furthermore, tree uniquely determines matrixmatrix uniquely determines tree .Proof. proof given Gusfield (1997).clear correspondence Definition 7 description speciestree given Section 4: species tree ultrametric tree matrix , Mijdepth mrca species j Mij divergence date two species.reason use ultrametric matrix model solve supertree problem.4.2.1 Model Gent et al.Given input forest F n distinct leaf labels, symmetricconstrained integer variables created domains {1, . . . , n 1}diagonal. Variable Mij depth mrca species j.posted make whole matrix ultrametric thus ensuringultrametric:Mij > Mik = MjkMik > Mij = MjkMjk > Mij = MikMij = Mik = Mjkn n matrix{0} mainInitially, constraintsresulting tree(2)< j < k. input trees broken triples fans using eitherbreakup algorithms Figures 12 13. triple (ij)k produced constraintMij > Mik = Mjk919(3)fiMoore & Prosser1bcefg3{e,g}5{c,d}b c e f g0 5 3 3 1 5 15 03030105001{b,f}Figure 15: One iteration algorithm convert ultrametric matrix treeposted 3-fan (ijk)Mij = Mik = Mjk(4)posted. constraints break disjunctions Equation 2. model (n2 n)/2variablest+f +n= O(n3 ) + O(n3 ) + (n3 ) = (n3 )3(5)constraints, number triples f number fans. O(n3 )one breaks disjunction one constraint Equation 2,(n3 ) those.4.2.2 Converting Back Tree Representationfinal step use algorithm based constructive proof Gusfield (1997)direction Theorem 3 build tree matrix produced constraintsolver. describe algorithm detail, sake intuition worksfollowsPick arbitrary leaf s. Let number distinct entries row d.Partition leaves sets p1 , . . . , pd based entry row s.Solve problem recursively pi ignoring rows columnsmatrix pi .Combine overall solution attaching subproblem solutions correct depthpath s.Figure 15 shows one recursion algorithm choice leaf showsrow fully describes path corresponding tree.920fiThe Ultrametric Constraint PhylogeneticsAlgorithm CPBuildCPBuild(F )1 let (V, D, C) CPModel(F )2 F3BreakUp(T ) C post(t, C)4 propagate(V,D,C) return UMToTree(V,D)5 else fail()Figure 16: Build supertree forest F using ultrametric constraint model.4.2.3 Time Complexity Model Gent et al.BreakUp, procedures build constraint model convert ultrametricmatrix tree polynomial time. However complexity backtracking search2O(n2 ) variables O(n)-size domains worst case O(nn ). upper boundtime taken solve supertree problem. attempted derive lesserupper bound time complexity, since, show following section,new model provably achieved polynomial time bound.4.3 Constraint Encoding Using New Propagator Designissue potentially exponential solution time model Section 4.2 worrying,experiments time taken solve instances major issue. Converselymemory requirements problem practice, theory! model requires(n3 ) space n species, constant factor inhibiting. Posting constraintEquation 1 literally (using toolkit propagators) described Section 3 uses 23 propagators JChoco toolkit. requires roughly 23 times runtime memorysingle propagator, since corresponds single Java object,comparable footprints. show empirical study Section 5, preventsmodest instances loaded typical current workstations.Using new propagator Section 3 replace n3 propagators singlecompact propagator result memory usage reduced asymptotically (n3 )(n2 ) since model memory dominated (n2 ) space neededmatrix . Also reducing amount space initialised delivers proportionalsaving build time. importantly, using new constraint provides solutionexponential time complexity, enforcing BC(Z) model allows solutionread lower bound domain. Theorem 4 proof correctnessalgorithm.Figure 16 gives schema constraint programming algorithm supertree construction, CPBuild. algorithm takes input forest F trees. line 1 constraintmodel produced, i.e., n n symmetric array constrained integers variables created, n unique species forest, UM-Matrix-BCZ constraintposted variables. Lines 2 3 breaks input trees triplesfans using either breakup algorithms given Figures 12 13, postsmodel constraints. Propagators constraints executed fixed pointline 4; succeeds tree created lower bounds ultrametric matrixotherwise fail.921fiMoore & ProsserLemma 7. propagator every constraint model enforces BC(Z) furthermore lower bounds mutually supportive, executing propagatorsfixed point, either lower bounds solution, empty domain fail.Proof. reduce domain lower bound fixed point obtainedbound supported mutually supportive supposition. Henceevery constraint simultaneously satisfied singleton domains and, definition,solution.Theorem 4. CPBuild polynomial time solution supertree problem.Proof. constraints involved model triples fans ultrametric constraints. Theorem 1 know lower bounds supportedpropagators run, Lemma 1 know lower bounds mutually supportive. true disjunction-breaking propagators. Hence Lemma 7,shown Figure 16, either read solution fail. enforce BC(Z)problem polynomial time shown below.Immediate preserve polynomial time solutionaddition polynomial number side-constraints, long additional constraintspreserve property lower bounds mutually supportive. fact, CSPsordered domains constraints property lower boundsmutually supportive belong known tractable class called min-closed (Jeavons & Cooper,1995).4.3.1 Time Complexity CPBuildalgorithm implemented run O(n4 ) time using variation AC3algorithm. AC3 (Mackworth, 1977) begins queue containing constraints.repeatedly removes constraint none remain runs associated propagator.constraints affected variables re-queued, necessary. queue empties,propagators fixed point. need O(n3 ) constraints worst case complexityO(n3 )| {z }+build initial QO(n)O(n3 )| {z }worst case re-queues 1 value removed time.O(1)| {z }propagation timeO(n4 ) overall. matches worst case complexity OneTree (Ng & Wormald,1996). constraint solution worst case problem unsolvable, sinceunsolvable domains emptied propagation, whereas solvable instancespropagation reaches fixed point sooner.5. Empirical Studypresent empirical study determine practical improvementsachieved constraint solutions supertree problem and, so, size improvement. Experiments run using 1.7GHz Pentium 4 processor 768MB memory,using Sun Java build 1.5.0 06-b05. constraint toolkit used JChoco version 1.1.04.922fiThe Ultrametric Constraint PhylogeneticsInput trees broken using hard breakup, consequently cases fans treatedhard polytomies2 .benchmark real-life seabird data previously used Kennedy Page (2002)Beldiceanu et al. (2008) present statistics various techniques producing supertrees, namely OneTree, CP solutions Section 4 (entries ToolkitCPBuild). completeness reproduce results Beldiceanu et al. (2008)data set, tabulate TreeCon. TreeCon uses single-successor model,constrained integer variables represent nodes within tree, domains correspondpossible successors3 . unique variable represents root loops (i.e.,vroot = root), leaf nodes indegree zero. Precedence incomparabilityconstraints generated input trees.TreeCon results encoded constraint programming toolkitprocessor approximately twice fast (3GHz). correct timescompensate factor. mark bold results differ significantlyCPBuild TreeCon results, specifically whose runtimes would undoubtedlyfactor 10 different processor. Results reported combinationsseabird trees (input trees named G) following data tabulated below:Data combination attempted.n Total distinct species input treesSol iff supertree possibleTechnique Type algorithm used solveBuild Time milliseconds initialise CP modelSolve Time milliseconds first solution,Total = Build + SolveNodes Number nodes search treeMem Model memory MBtable, DNL means model could loaded (as large) DNFmeans could solved within 30 mins, succeeded loading.provided memory usage OneTree; however smallerconstraint encodings.obvious thing note much faster imperative approach comparedconstraint techniques. this? Primarily due lower complexityOneTree absence fans (we investigated benefit this),partly due generality constraint programming approach. imperativeapproach highly specialised one class problem whereas constraint approachsits within toolkit, runs top general purpose constraint maintenance system.expect constraint approach compete raw speedlater demonstrate (in Section 6) approach benefits versatility, i.e.,2. later section use soft breakup.3. alternative constraint model tree might use 0/1 variables corresponding potential edgeswithin adjacency matrix (Prosser & Unsworth, 2006), indeed CP(Graph) computation domain(Dooms, 2006).923fiMoore & Prossercosts space time repaid ease accommodating variants problemmodel.DataABn23SolAC32FAD47AE95FAF31AG46BC29FBD4294FBF30BG40CD45CE68CF34CG44FTechniqueToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConBuild2056183Solve37413126701893271538235220946248DNL340DNL1477249718837913776712228712522056171219311075833201930251DNL335DNL163402405174343995098203651353100562241134276DNL516DNL145131011805631336683210587215924Total243031430213299734240612918146839822DNL1817103933728763251272085424744092123987278328676345230117DNL16675892112748273144857495561440131119050063014DNL1967271803636623133931172704251530Nodes2323Mem26.920.240036.340.343838118.510.70DNL0> 6292.79191832.990.323131111.070.68171026.900.27333384.260.55DNL0> 6292.71292929.830.28303072.710.514545143.910.77DNL68> 6292.72303043.720.360097.100.61fiThe Ultrametric Constraint PhylogeneticsDE104FDF44DG56FEF94FEG97FFG38FABDF72ABDG78FACDF72FACDG81FACE97FOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildTreeConOneTreeToolkitCPBuildOneTreeDNL360DNL20216613203987250140902522280640DNL331DNL9546DNL344DNL89004299195DNL21227032277529172260847301DNF3633310672861931649DNL307DNL1711DNL737DNL163214DNL23811126347600453630171637089291019DNL9877103512DNL9244121115DNL407621032323999813934DNF39343472932998935869028DNL20181265035DNL236938DNL0> 6293.31373797.100.6010201.420.99DNL0> 6292.71DNL0> 6292.89DNL061.410.466359382.521.48DNF0553.491.9100434.841.61DNL0> 6292.06DNL0> 6292.91impressive aspect matrix model Section 4.3 Section 4.2improvement memory requirements, instances loaded comfortably. also dramatic impact build time. improvements dominatereduction solve time practice. toolkit model outperformed CPBuildorder magnitude instance; moreover, two cases search occurringtoolkit model (on data sets BC ABDG) whereas CPBuild never search.polynomial time complexity due provable absence search.results also compare well Beldiceanu et al. (2008). onecase, BE, CPBuild order magnitude slower TreeCon; farexplanation this. four cases TreeCon significantly worseCPBuild. results available TreeCon data set ACE.noted Beldiceanu et al. yet complete filtering algorithm problembased constraint model and, personal communication, although TreeCon925fiMoore & ProsserFigure 17: Supertree largest compatible data sets birds ABDF. took 737msmodel 270ms solve using cpbuild.model never backtracked birds data set yet proof complexitymodel polynomial. also noted see CPBuild taking timeunsolvable instances solvable instances, predicted.Figure 17 shows supertree, displayed treeView (Page, 1996), producedlargest compatible forest {A, B, D, F }. supertree 72 leaves takes1 second produce. Although result printed table, finding forest{A, B, C, D, E, F, G} incompatible takes 12 seconds total (1.5 seconds buildmodel 10 seconds determine incompatibility).926fiThe Ultrametric Constraint Phylogenetics6. Versatility Constraint ModelOne strengths constraint programming versatility: given constraint modelcore problem model enhanced address variants original pureproblem. demonstrate versatility respect ultrametric model, presentingfour variants supertree problem (a) incorporating ancestral divergence datesmodel, (b) nested taxa, (c) determining induced triple fan commonsupertrees, (d) coping incompatibilities.6.1 Ancestral Divergence DatesSemple et al. (2004) Bryant et al. (2004) add temporal information input trees.Interior nodes may labelled integer ranks interior node v2 properdescendant v1 rank(v1 ) < rank(v2 ), resulting ranked phylogenetic tree. Additionally relative divergence dates may expressed form div(c,d) predates div(a,b)interpreted divergence species c predates species b.RankedTree algorithm (Bryant et al., 2004) takes input collection precedenceconstraints derived input ranked species trees predates relations. algorithmoutputs ranked tree respects relations returns compatible.trivial incorporate constraint model. trees rankedpair species (i, j) leaf set instantiate constrained integer variableMij value mrca(i, j). predates relation div(c,d) predates div(a,b) postconstraint Mcd < Mab . done step 4 CPBuild (Figure 16), i.e., rankspredates relations become side constraints. Similarly time bounds speciation eventsposted unary constraints, i.e. dated phylogenetic tree upper lower divergencebounds given interior nodes, l(a, b) u(a, b) give respectively lowerupper bounds divergence dates species b. constraint programfollowing two side constraints posted (again, step 4): l(a, b) MabMab u(a, b).demonstration ranked trees given Figure 18. left two rankedspecies trees cats used recently Semple et al. (2004) originally Janczewski,Modi, Stephens, OBrien (1995). branch lengths source treestranslated rankings added interior vertices trees. rightone 17 possible resultant supertrees. total, 7 17 solutions containinterior nodes ranges. interior nodes labelled specific values ratherranges 30 solutions produced, structurally identical. goesway addressing issue enumerating supertrees compactly, raisedchallenge Semple et al. (2004). Figure 19 show effect adding predatesconstraint supertree construction. data previously used Bryant et al.(2004) Figures 5 6.6.2 Nested Taxataxon (plural, taxa) group organisms comprising single common ancestordescendents (Dawkins & Wong, 2004). example species lion class birdstaxa. far, species trees leaf-labelled, however restrictive927fiMoore & ProsserFigure 18: Two ranked trees cats. right one 17 possible supertrees producedCPBuild. Displayed using Pages treeView.Figure 19: Two input trees T1 = ((a, c), x) T2 = (b, x) resultant supertree shown3rd position. tree far right supertree T1 T2side constraint div(a,c) predates div(a,b), produced CPBuilddisplayed using Pages treeView.928fiThe Ultrametric Constraint PhylogeneticsPPQQPQbcegbfegbcfeFigure 20: Two input rooted X-trees T1 T2 (left) output tree T3 (right)perfectly displays them.trees may annotated taxa names leaves internal nodes, givingnested taxa. example, Figure 20 shows tree T1 internal node labelled Pdescendents b, i.e., b taxa nested within P taxon. Problemsrelated creating compatible supertrees type data raised Page (2004)defined solved Daniel Semple (2004). set input trees possiblesolution problem shown Figure 20: notice labels conservedsolution, ancestral relationships conserved and, labels li ljinput tree, li ancestor lj input tree li ancestor lj solutiontree. instance problem Higher Taxa Compatibility defined DanielSemple (2004) Semple et al. (2004), result tree must perfectly displayinput trees. define problem formally.Definition 8. rooted X-tree (Daniel & Semple, 2004) species tree internalnodes well leaves may labelled set X.following slightly loose may use label l identify labelled node,well label itself, e.g., descendants l means descendantsnode labelled l.Definition 9. rooted X-tree perfectly displays rooted X -tree1. X X;2. displays , neglecting internal labels;3. descendant b descendant b ;4. descendant b descendant b .rooted X-tree perfectly displays forest phylogenetic trees F = {T1 , . . . , Tn }perfectly displays every Ti .6.2.1 Constraint Encodingconstraint encoding implemented addition variables side constraintsstandard model Section 4. describe transform input makeconstraint solution simpler, describe variables constraints needed.929fiMoore & ProsserT1T2T2PQPQQbcefPegfbcegfFigure 21: Two input trees T1 T2 enclosing taxon P . process substitution T2 replaced T2 .Substitution Taxon P Figure 21 appears internal node T1 , calllabel enclosing taxon. Note also appears leaf T2 . input treespreprocessed replace tree enclosing taxon P leaf treesingle subtree rooted P substituted place. must subtreeelsewhere input forest, contradiction enclosing taxon.process add remove information, since relationshipseverything tree still holds, new relationships taxa subtreerest tree always implicit input.aim process obtain set inputs enclosing taxa appearinternal nodes only, without loss generality constraint encoding assumescase. Figure 21 shows example substitution process appliedtrees T1 T2 , T2 would replaced T2 .Variables constraints variables added one integer variable vl per enclosingtaxa/label l, domain {1, . . . , n 1}. value variable solutiontree depth internal node labels, labels position finaltree determined must unique node depth path onenested taxa root. See Figure 15 suppose sake argumentenclosing taxa labels b, variable lM = 1 solution.unique location label go root node.Properties (1) (2) Definition 9 immediate propertiesearlier model foundation one. explaining enforceproperty (3) introduce notations convenience. Function desc(l, F ) returnsset descendants label l tree forest F , notDesc(l, ) returnsset labels descendants l tree .first need constraint l must label every single species labels input.every enclosing label l, post following set constraints:{vl Mij | desc(l, F ) j desc(l, F ) 6= j}(6)label must settle least shallow mrca descendants input,hence must remain descendants. Notice necessary consider pairs speciesdistinct input trees. alternative taking pairs tree work,necessary pairs internal node l, rather two930fiThe Ultrametric Constraint Phylogeneticsdistinct nodes happen correct depth. Next, label must constrainedlabel already descendant becomes one. X-treeenclosing label l X, post following set constraints:{vl > Mij | desc(l, {T }) j notDesc(l, )}(7)label l must placed strictly deeper mrca descendantsomething thats descendant, i.e., non-descendents l descendentresult. illustration list generated constraints example Figure 20.1. Equation 6 l = P : {vP Mab , vP Mag , vP Mbg }2. Equation 6 l = Q: {vQ Mde , vQ Mdf , vQ Mef }3. Equation 7, l = P {T1 , T2 }: {vP > Mac , vP > Mad , vP > Mae , vP >Mbc , vP > Mbd , vP > Mbe , vP > Mgf , vP > Mgd , vP > Mge , vP > Mbf }4. Equation 7, l = Q {T1 , T2 }: {vQ > Mad , vQ > Mae , vQ > Mbd , vQ >Mbe , vP > Mcd , vQ > Mce , vQ > Mgf , vP > Mbf , vP > Mgd , vP > Mge }number new constraints created Equations 6 7 boundednumber distinct pairs species, i.e. (n2 ) new constraints.6.3 Necessitymay many possible supertrees given input forest. One question then,relationships common supertrees? problem determining derivedinduced triple (or fan) supertree necessary (i.e., common possible supertrees)introduced (Daniel, 2003) along polynomial time decision procedure Necessity.algorithm Necessity Figure 22 takes arguments forest F trees, assumedcompatible, rooted triple fan determines occurs every supertreedisplays trees F . algorithm simple modification CPBuild,lines 1 3 essentially same. line 4 negation triple postedproblem, postedMik 6= Mjk Mij Mik Mij Mik(8)= (ij)k postedMij 6= Mik Mij 6= Mjk Mik 6= Mjk(9)= (ijk). call made propagate make problem arc-consistent (line5), fails necessary, otherwise necessary. algorithmcomplexity CPBuild.6.4 Coping Conflictsupertree cannot produced pair trees input triples fansmust conflict one another, either directly indirectly. Junkers quickXPlainmethod (Junker, 2004) discovers minimal subset constraints postedpropagated result failure. set necessarily smallest possible set931fiMoore & ProsserAlgorithm NecessityNecessity(F, )1 let (V, D, C) CPModel(F )2 F3BreakUp(T ) C post(t, C)4 C post(, C)5 return propagate(V, D, C)Figure 22: triple/fan occur every supertree displays trees F ?minimal sense removal element set constitutesound explanation, addition constraint would redundant. setconstraints input triples fans, minimal set semantically collection inputdata incompatible. Junker (2004) state method achieved worstcase 2k log2 (n/k) + 2k propagations4 , k size minimal explanationfound n number constraints.alternative approach satisfy many input triples fans possiblewithin reasonable amount time, i.e., polynomial time. Semple Steel proposealgorithm, MinCutSupertree (2000), refined Page (2002).propose similar scheme within constraint programming framework. callalgorithm GreedyBuild works follows. associate constrained integervariable x, domain {0, 1}, triple fan. variable assignedvalue 0 triple (or fan) respected, otherwise ignored. Therefore triple(ij)k post constraint equation 10 3-fan (ijk) constraint equation11.(x = 0 Mij > Mik = Mjk ) (x = 1 (Mik 6= Mjk Mij Mik Mij Mik ))(10)(x = 0 Mij = Mik = Mjk ) (x = 1 (Mij 6= Mik Mij 6= Mjk Mik 6= Mjk ))(11)GreedyBuild instantiates turn x variables, i.e. decision variables,preferring value 0 value 1, instantiation problem made arcconsistent. algorithm shown Figure 23. line 1 constraint model produced,i.e., n n symmetric array constrained integers variables created, nunique species forest, UM-Matrix-BCZ constraint postedvariables. variable X line 2 set decision variables. input treesbroken before, new variable x created triple fan. line6 constraints equations 10 11 posted model. loop lines 1012 turn select decision variable, set lowest possible value, makeproblem arc-consistent. might turn cause uninstantiated variablesvalue 0 removed domain associated triple fan conflicts triple4. BC(Z) propagator O(1), however, time complexity numberpropagations.932fiThe Ultrametric Constraint PhylogeneticsAlgorithm GreedyBuildGreedyBuild(F )1let (V, D, C) CPModel(F )2let X3F4BreakUp(T )5let x newV ar(0, 1)6let c newConstraint((t x = 0) (t x = 1))7X X {x}8V V {x}9C post(c, C)10 x X11instantiate(x)12propagate(V, D, C)13 return UMToTree(V, D)Figure 23: Greedily Build supertree forest F using ultrametric constraint model.fan enforced. process terminates without failure,conflicting triples fans essentially ignored. line 13 ultrametric matrixconverted tree. complexity GreedyBuild O((t + f ) n4 )triples f fans.GreedyBuild applied forest bird data {A, B, C, D, E, F, G} section 5, using soft breakup. data incompatible use CPBuild, howeverGreedyBuild produces supertree Figure 24. supertree contains 121 species.SoftBreakup produced 201 triples, 17 rejected. took less 2seconds build model 100 seconds solve model.compared CPBuild data set, taking 1.5 seconds build model10 seconds determine incompatibility. GreedyBuild also applied data setABDF, producing identical supertree CPBuild, comparable time (890ms buildmodel 578ms solve).executed GreedyBuild decision variables set X (lines 2, 7, 1011) analysed identify set triples fans excludedsupertree, i.e., x variable instantiated value 1 correspondingtriple fan ignored.Note claim biological significance arbitrary order usesuppress triples. GreedyBuild could amended follow order MinCutSupertree investigated this. GreedyBuild also enhancedfollows. Currently triple fan exists multiple input trees occursconstraint. information could exploited weighting decision variablestake consideration relative weight evidence triple fan, e.g., numbertimes triple fan occurs input. decision variables instantiatednon-increasing order weight, i.e., variable ordering heuristic used.extreme GreedyBuild modified become OptBuild full backtrackingsearch performed objective minimising sum decision variables,933fiMoore & ProsserFigure 24: Supertree largest data set birds, ABCDEFG, 121 species.took 2 seconds model 100 seconds solve using GreedyBuild.Displayed using Rod Pages treeView.934fiThe Ultrametric Constraint Phylogeneticspotentially exponential cost time. would return tree fewest possibleinput triples suppressed.6.5 Summarylittle effort, constraint model adapted deal ancestral divergencedates nested taxa. achieved adding side constraints.added advantage respect ancestral divergence result compactenumeration output trees interior nodes labelled ranges rather specificvalues.input trees conflict propose two options: use quickXPlain determinecause conflict greedily build supertree using GreedyBuild. Bryant et al. (2004)state essentially all-or-nothing approach supertree constructionusing RankedTree needed something akin MinCutSupertree, i.e.trees incompatible build supertree violates minimum number triplesfans, polynomial time (Page, 2002; Semple & Steel, 2000). sincedone Bordewich et al. (2006) also done constraint modelincorporating constraints identified section 6.1 GreedyBuild.Although shown, obvious ancestral divergence data nested taxacombined one model, simply adding necessary constraint auxiliaryvariables variants one model. This, again, could done GreedyBuild,would require heuristic rule used deciding constraints ignoreinput trees side constraints incompatible.opinion, deriving, combining analysing results imperative algorithmssupertree problems much difficult above. algorithms variantsrequired far complex data structures tailored algorithms processing them.Moreover, combine algorithms produced seems practically impossible accountintricacy. Finally constraint programming provides various generic methods likequickXPlain box turn interest supertree problems.7. Conclusionpresented new constraint propagator ultrametric constraint threeinteger variables, shown extended symmetric matrix constrainedinteger variables. bounds(Z)-consistency established symmetric arraylower bounds variables give mutual support. sufficient modelling solvingsupertree construction problem O(n4 ) time O(n2 ) space, comparable complexity OneTree (Ng & Wormald, 1996) inferior algorithm BryantSteel (1995). So, bother CPBuild approach efficient imperativeapproaches already exist? answer lies versatility constraint programming.Rather develop new algorithm new variant supertree problem addside constraints base model, shown polynomial time boundoften achieved. done ancestral divergence dates nested taxa,shown model used deliver necessary triples fans,proposed GreedyBuild way dealing incompatible trees.935fiMoore & ProsserAcknowledgmentswould like thank Pierre Flener Xavier Lorca; Barbara Smith, Ian Gent Christine Wei Wu; Charles Semple, Mike Steel Rod Page; Muffy Calder Joe Sventek;Stanislav Zivny; Chris Unsworth; three anonymous reviewers/co-authors.ReferencesAho, A., Sagiv, Y., Szymanski, T., & Ullman, J. (1981). Inferring tree lowestcommon ancestors application optimization relational expressions.SIAM J. Comput, 10 (3), 405421.Beldiceanu, N., Flener, P., & Lorca, X. (2008). Combining tree partitioning, precedence,incompatibility constraints. Constraints, 13, 131.Bessiere, C., & Regin, J.-C. (2001). Refining basic constraint propagation algorithm.IJCAI, pp. 309315.Bessiere, C. (2006). Constraint propagation. Handbook constraint programming. Elsevier. Chapter 3.Bininda-Emonds, O. (2004). Phylogenetic Supertrees: Combining information revealtree life. Springer.Bordewich, M., Evans, G., & Semple, C. (2006). Extending limits supertree methods.Annals combinatorics, 10, 3151.Bryant, D., & Steel, M. (1995). Extension Operations Sets Leaf-labeled Trees. Advances Applied Mathematics, 16, 425453.Bryant, D., Semple, C., & Steel, M. (2004). Supertree methods ancestral divergencedates applications. Bininda-Emonds, O. (Ed.), Phylogenetic Supertrees:Combining information reveal tree life, pp. 151171. Computational BiologySeries Kluwer.Carlier, J., & Pinson, E. (1994). Adjustment heads tails jobshop schedulingproblem. European Journal Operational Research, 78, 146161.Caseau, Y., & Laburthe, F. (1997). Solving small TSPs constraints. ProceedingsInternational Conference Logic Programming, pp. 115.Choco (2008). http://www.choco-constraints.net/ home choco constraint programming system..Daniel, P. (2003). Supertree methods: new approaches. Masters thesis, DepartmentMathematics Statistics, University Canterbury.Daniel, P., & Semple, C. (2004). Supertree algorithms nested taxa. Bininda-Emonds,O. (Ed.), Phylogenetic Supertrees: Combining information reveal tree life,pp. 151171. Computational Biology Series Kluwer.936fiThe Ultrametric Constraint PhylogeneticsDawkins, R., & Wong, Y. (2004). Ancestors Tale. Weidenfeld Nicholson.Debruyne, R., & Bessiere, C. (1997). practicable filtering techniques constraintsatisfaction problem. Proceedings IJCAI97, pp. 412417.Dooms, G. (2006). CP(Graph) Computation Domain Constraint Programming.Ph.D. thesis, Universite catholique de Louvain, Faculte des sciences appliquees.Gent, I., Prosser, P., Smith, B., & Wei, W. (2003). Supertree construction constraintprogramming. Principles Practice Constraint Programming, pp. 837841.Springer.Gusfield, D. (1997). Algorithms strings, trees, sequences: computer sciencecomputational biology. Cambridge University Press, New York, NY, USA.Janczewski, D., Modi, W., Stephens, J., & OBrien, S. (1995). Molecular evolutionmitochondrial 12S RNA Cytochrome b sequences pantherine lineageFelidae. Mol. Biol. Evol., 12, 690707.Jeavons, P. G., & Cooper, M. C. (1995). Tractable constraints ordered domains. Artif.Intell., 79 (2), 327339.Junker, U. (2004). QUICKXPLAIN: Preferred Explanations Relaxations OverConstrained Problems. Proceedings AAAI2004, pp. 167172.Kennedy, M., & Page, R. (2002). Seabird supertrees: Combining partial estimates procellariiform phylogeny. Auk, 69, 88108.Lhomme, O. (2003). efficient filtering algorithm disjunction constraints.Principles Practice Constraint Programming, pp. 904908. Springer.Mace, G. M., Gittleman, J. L., & Purvis, A. (2003). Preserving Tree Life. Science,300, 17071709.Mackworth, A. (1977). Consistency networks relations. Artificial Intelligence, 8,99118.Ng, M. P., & Wormald, N. C. (1996). Reconstruction rooted trees subtrees. DiscreteAppl. Math., 69 (1-2), 1931.Page, R. (1996). TREEVIEW: application display phylogenetic trees personalcomputers. Computer Applications Biosciences, 12, 357358.Page, R. (2004). Taxonomy, supertrees, tree life. Bininda-Emonds, O. (Ed.),Phylogenetic Supertrees: Combining information reveal tree life, pp. 247265.Computational Biology Series Kluwer.Page, R. D. M. (2002). Modified mincut supertrees. WABI 02: Proceedings SecondInternational Workshop Algorithms Bioinformatics, pp. 537552 London, UK.Springer-Verlag.937fiMoore & ProsserPennisi, E. (2003). Modernizing Tree Life. Science, 300, 16921697.Prosser, P. (2006). Supertree construction constraint programming: recent progressnew challenges. WCB06 - Workshop Constraint Based Methods Bioinformatics, pp. 7582.Prosser, P., & Unsworth, C. (2006). Rooted Tree Spanning Tree Constraints. 17thECAI Workshop Modelling Solving Problems Constraints.Regin, J.-C. (1994). filtering algorithm constraints difference CSPs. Proceedings AAAI94, pp. 362367.Rossi, F., van Beek, P., & Walsh, T. (2007). Handbook Constraint Programming. Elsevier.Sabin, D., & Freuder, E. (1994). Contradicting conventional wisdom constraint satisfaction. Proceedings ECAI-94, pp. 125129.Schulte, C., & Carlsson, M. (2006). Finite domain constraint programming systems.Handbook constraint programming. Elsevier. Chapter 14.Semple, C., Daniel, P., Hordijk, W., Page, R., & Steel, M. (2004). Supertree algorithmsancestral divergence dates nested taxa. Bioinformatics, 20 (15), 23552360.Semple, C., & Steel, M. (2000). supertree method rooted trees. Discrete Appl. Math.,105 (1-3), 147158.Smith, B. M. (1995). Tutorial Constraint Programming. Technical Report 95.14,University Leeds.TreeBASE (2003). http://www.treebase.org/ TreeBASE: database phylogenetic knowledge..Tsang, E. (1993). Foundations Constraint Satisfaction. Academic Press.van Hentenryck, P., Deville, Y., & Teng, C.-M. (1992). generic arc-consistency algorithmspecializations. Artificial Intelligence, 57, 291321.van Hentenryck, P., Saraswat, V., & Deville, Y. (1998). Design, implementation,evaluation constraint language cc(fd). Journal Logic Programming, 37, 139164.Wu, G., You, J.-H., & Lin, G. (2007). Quartet-based phylogeny reconstruction answerset programming. IEEE/ACM Transactions Computational Biology Bioinformatics, 4, 139152.Yuanlin, Z., & Yap, R. H. C. (2001). Making AC-3 optimal algorithm. IJCAI, pp.316321.938fiJournal Artificial Intelligence Research 32 (2008) 793-824Submitted 12/07; published 08/08Analogical Dissimilarity: Definition, AlgorithmsTwo Experiments Machine LearningLaurent MicletSabri BayoudhArnaud DelhayLAURENT. MICLET @ UNIV- RENNES 1. FRSABRI . BAYOUDH @ UNIV- ST- ETIENNE . FRARNAUD . DELHAY @ UNIV- RENNES 1. FRIRISA/CORDIAL, 6, rue de KerampontBP 80518 - F-22305 Lannion Cedex, FranceAbstractpaper defines notion analogical dissimilarity four objects, specialfocus objects structured sequences. Firstly, studies case four objectsnull analogical dissimilarity, i.e. analogical proportion. Secondly, one objectsunknown, gives algorithms compute it. Thirdly, tackles problem defining analogicaldissimilarity, measure far four objects analogical proportion.particular, objects sequences, gives definition algorithm based optimalalignment four sequences. gives also learning algorithms, i.e. methods find tripleobjects learning sample least analogical dissimilarity given object.Two practical experiments described: first classification problem benchmarksbinary nominal data, second shows generation sequences solving analogicalequations enables handwritten character recognition system rapidly adapted new writer.1. IntroductionAnalogy way reasoning studied throughout history philosophywidely used Artificial Intelligence Linguistics. focus paper restrictedconcept analogy called analogical proportion.1.1 Analogical Proportion Four Elementsanalogical proportion four elements A, B, C universe usuallyexpressed follows: B C D. Depending elements, analogical proportions1different meanings. example, natural language analogical proportions could be:crow raven merlin peregrine vinegar winesloe cherry. based semantics words. contrast, formaluniverse sequences, analogical proportions abcd abc abbd abbg gt gg ggt morphological.Whether morphological not, examples show intrinsic ambiguitydefining analogical proportion.could well accept, good reasons:g gt gg ggtt vinegar wine vulgar wul. Obviously,ambiguities inherent semantic analogies, since related meaning words(the concepts expressed natural language). Hence, seems important, first step,focus formal morphological properties. Moreover, solving analogies sequences1. ambiguity, may use analogy short instead analogical proportion.c2008AI Access Foundation. rights reserved.fiM ICLET, BAYOUDH & ELHAYoperational problem several fields linguistics, morphology syntax, providesbasis learning data mining analogy universe sequences.paper, firstly consider analogical proportions sets objects secondly present may transferred sequences elements sets.1.2 Solving Analogical Equationsone four elements unknown, analogical proportion turns equation.instance, sequences letters, analogical proportion wolf leaf wolves xcorresponds equation = {x | wolf leaf wolves x}. Resolving equation consists computing (possibly empty) set sequences x satisfy analogy.sequence leaves exact semantic morphological solution. shall see that, however,straightforward design algorithm able solve kind equation, particularlooking approximate solution necessary.Solving analogical equations sequences useful linguistic analysis tasksapplied (with empirical resolution techniques, simple cases) mainly lexical analysis tasks.example, Yvon (1999) presents analogical approach grapheme-to-phoneme conversion,text-to-speech synthesis purposes. generally, resolution analogical equationsalso seen basic component learning analogy systems, part lazylearning techniques (Daelemans, 1996).1.3 Using Analogical Proportions Machine LearningLet = {(x, u(x))} finite set training examples, x description example(x may sequence vector Rn , instance) u(x) label finite set. Givendescription new pattern, would like assign label u(y), basedknowledge S. problem inductive learning classification rule examples,consists finding value u point (Mitchell, 1997). nearest neighbor method,popular lazy learning technique, simply finds description xminimizes distance hypothesizes u(x ), label x , label y.Moving one step further, learning analogical proportions consists searchingtriple (x , z , ) x z predicts label u(y)solution equation u(x ) u(z ) u(t ) u(y). one triple found,voting procedure used. learning technique based resolution analogicalequations. Pirrelli Yvon (1999) discuss relevance learning procedure variouslinguistic analysis tasks. important notice u(y) different domains:example, simple case learning classification rule, may sequence whereas uclass label.next step learning analogical proportions is, given y, find triple (x , z , )x z holds almost true, or, closeness measure defined,triple closest term analogical proportion. study articlequantify measure, order provide flexible method learning analogy.794fiA NALOGICAL ISSIMILARITY1.4 Related Workpaper related several domains artificial intelligence. Obviously, first onereasoning analogy. Much work done subject cognitive science pointview, led computational models reasoning analogy: see example,classical paper (Falkenhainer, Forbus, & Gentner, 1989), book (Gentner, Holyoak, & Kokinov,2001) recent survey (Holyoak, 2005). Usually, works use notion transfer,within scope article. means knowledge solving problemdomain transported another domain. Since work four objectsspace, implicitly ignore notion transfer different domains. Technically speaking,restriction allows us use axiom called exchange means define analogicalproportion (see Definition 2.1). However, share works following idea: maysimilar relation two couples structured objects even objects apparentlyquite different. interested giving formal algorithmic definition relation.work also aims define supervised machine learning process (Mitchell, 1997; Cornujols & Miclet, 2002), spirit lazy learning. seek extract modellearning data, merely conclude class, generally supervision, newobject inspecting (a part of) learning data. Usually, lazy learning, like k-nearest neighborstechnique, makes use unstructured objects, vectors. Since distance measures alsodefined strings, trees even graphs, technique also used structured objects,framework structural pattern recognition (see example work Bunke & Caelli, 2004;Blin & Miclet, 2000; Basu, Bunke, & Del Bimbo, 2005). extend search nearestneighbor learning set best triple (when combined new object,closest make analogical proportion). requires defining analogical proportionstructured objects, like sequences, also give definition far 4-tuple objectsanalogy (that call analogical dissimilarity).Learning analogy sequences already studied, restricted manner,linguistic data (Yvon, 1997, 1999; Itkonen & Haukioja, 1997). Reasoning learning analogy proven useful tasks like grapheme phoneme conversion, morphology translation.Sequences letters and/or phonemes natural application work, also interested type data, structured sequences trees, prosodic representationsspeech synthesis, biochemical sequences, online handwriting recognition, etc.Analogical proportions four structured objects universe, mainly strings,studied mathematical algorithmic approach, like ours, Mitchell (1993)Hofstadter et al. (1994), Dastani et al. (2003), Schmid et al. (2003). best knowledgeproposition original: give formal definition analogical dissimilarityfour objects, particular sequences, produce algorithms enableefficient use concept machine learning practical problems. already discussedcompute exact analogical proportions sequences paper Yvon et al. (2004)given preliminary attempt compute analogical dissimilarity sequences paperDelhay Miclet (2004). Excerpts present article presented conferences(Bayoudh, Miclet, & Delhay, 2007a; Bayoudh, Mouchre, Miclet, & Anquetil, 2007b).connect another field A.I., let us quote Aamodt Plaza (1994) useterm analogy Case-Based Reasoning (CBR): Analogy-based reasoning: term sometimes used, synonym case-based reasoning, describe typical case-based approach.795fiM ICLET, BAYOUDH & ELHAYHowever, also often used characterize methods solve new problems based past casesdifferent domain, typical case-based methods focus indexing matching strategies single-domain cases. According authors, use word analogy broadermeaning, typical CBR deals single domain problems, analogical proportions also do.sense, study could seen particular case CBR, applied paper supervisedlearning classification rules.1.5 Organization Paperpaper organized six sections. introduction, present section 2 generalprinciples govern definition analogical proportion four objectsset define analogical equation set. apply definitions Rn{0, 1}n . Finally, section defines analogical proportion four sequences alphabetanalogy defined, using optimal alignment method four sequences.Sections 3 introduces new concept analogical dissimilarity (AD) four objects,measuring way much objects analogy. particular, must equivalentsay four objects analogy analogical dissimilarity null. extendsequences. end section gives two algorithms: SEQUANA4 computes valueAD four sequences SOLVANA solves analogical equations generalized manner:produce approximate solutions (i.e. strictly positive AD).Section 4 begins explore use concept analogical dissimilarity supervisedmachine learning. give algorithm (FADANA) fast search k-best analogical3-tuples learning set.Section 5 presents two applications concepts algorithms real problems.firstly apply FADANA objects described binary nominal features. Experiments conducted classical benchmarks favorably compared standard classification techniques.Secondly, make use SOLVANA produce new examples handwritten recognition system. allows training classifier small number learning patterns.last section presents work done, particularly discussing real world applicationlearning analogy, especially universe sequences.2. Analogical Proportions Equationssection, give formal definition analogical proportion four objectsexplain solve analogical equation. Instanciations general definitions givenobjects either finite sets (or equivalently binary vectors), vectors real numberssequences finite alphabets.2.1 Axioms Analogical Proportionmeaning analogical proportion : B :: C : four objects set X dependsnature X, relations defined. However, generalproperties required, according usual meaning word analogy philosophylinguistics. According Lepage (2003) three basic axioms given:Definition 2.1 (Analogical proportion) analogical proportion set X relation X 4 ,i.e. subset X 4 . (A, B, C, D) A, four elements A, B, C said796fiA NALOGICAL ISSIMILARITYanalogical proportion, write: analogical proportion : B :: C : holds true,simply : B :: C : , reads B C D. every 4-tuple analogicalproportion, following equivalences must hold true:Symmetry relation: : B :: C : C : :: : BExchange means: : B :: C : : C :: B :third axiom (determinism) requires one two following implications holds true(the consequence):: :: B : x: B :: : xx=Bx=BAccording first two axioms, five formulations equivalent canonical form: B :: C : :B : :: : C: C :: B :: B :: C :C : :: : BB : :: : CConsequently, three different possible analogical proportions four objects,canonical forms:: B :: C :: C :: : B: :: B : C2.2 Analogical Equationssolve analogical equation consists finding fourth term analogical proportion,first three known.Definition 2.2 (Analogical equation) solution analogical equation : B :: C : x: B :: C : .already know previous sections that, depending nature objects definition analogy, analogical equation may either solution unique solution severalsolutions. study sequel solve analogical equations different sets.2.3 Analogical Proportion Finite Sets Binary Objectsrelation equality sets, Lepage given definition analogicalproportion sets coherent axioms. useful section 2.3.2objects described sets binary features.2.3.1 N NALOGICAL P ROPORTION F INITE ETSDefinition 2.3 (Analogical proportion finite sets) Four sets A, B, C analogical proportion : B :: C : transformed B, C D,adding subtracting elements C.case, example, four sets: = {t1 , t2 , t3 , t4 , }, B = {t1 , t2 , t3 , t5 } C ={t1 , t4 , t6 , t7 }, = {t1 , t5 , t6 , t7 }, t4 taken from, t5 addedC, giving B D.797fiM ICLET, BAYOUDH & ELHAY2.3.2 OLVING NALOGICAL E QUATIONS F INITE ETSConsidering analogy sets, Lepage (2003) shown following theorem, respectaxioms analogy (section 2.1):Theorem 2.4 (Solution analogical equation sets) Let A, B C three sets. analogical equation : B :: C : unknown solution followingconditions hold true:B C B Csolution unique, given by:= ((B C)\A) (B C)2.3.3 NALOGICAL P ROPORTIONS {0, 1}nLet X set {0, 1}n . x X [1, n], fi (x) = 1 (resp. fi (x) = 0)means binary feature fi takes value RU E (resp. F ALSE) object x.Let : B :: C : analogical equation. feature fi , eight differentpossibilities values A, B C. derive solutions definition propertiesanalogy sets, two following principles:feature fi (D) computed independently.following table gives solution fi (D):fi (A)fi (B)fi (C)fi (D)000000110101011?100?101011001111two cases among eight, fi (D) exists. derives defining X binaryfeatures, equivalent defining X finite set. Theorem 2.4 imposes conditionsresolution analogical equations finite sets, results fact two binary analogicalequations solution.2.4 Analogical Proportion Rn2.4.1 EFINITIONLet origin Rn . Let = (a1 , a2 , . . . , ) vector Rn , defined ncoordinates. Let a, b, c four vectors Rn . interpretation analogical proportion: b :: c : usually a, b, c, corners parallelogram, oppositecorners (see Figure 1).Definition 2.5 (Analogical proportion Rn ) Four elements Rn analogical proportion (a : b :: c : d) form parallelogram, Oa + Od = Ob + Ocequivalently ab = cd equivalentlyac = bdstraightforward axioms analogy, given section 2.1 verified definition.798fiA NALOGICAL ISSIMILARITYbcFigure 1: Analogical parallelogram Rn .2.4.2 OLVING NALOGICAL E QUATIONS RnSolving analogical equation : b :: c : x , a, b c vectors Rn xunknown derives directly definition analogy vector spaces: four vectors mustform parallelogram. always one one solution given equation:Ox = Ob + Oc Oa2.5 Analogical Proportion Sequences2.5.1 N OTATIONSsequence2 finite series symbols finite alphabet . set sequences denoted. x, , xy denotes concatenation x y. also denote | x | = n lengthx, write x x = x1 . . . x|x| x = x[1] . . . x[n], xi x[i] . denoteempty word, null length, + = \{}.factor (or subword) f sequence x sequence exists two sequencesu v with: x = uf v. example, abb bbac factors abbacbbaba.subsequence sequence x = x1 . . . x|x| composed letters x indicesi1 . . . ik , i1 < i2 . . . < ik . example, ca aaa two subsequences abbacbaba.2.5.2 EFINITIONLet alphabet. add new letter , denote , giving augmented alphabet. interpretation new letter simply empty symbol, needsubsequent sections.Definition 2.6 (Semantic equivalence) Let x sequence sequence . xsemantically equivalent subsequence composed letters x. denoterelation .example, ab abaa.Let us assume analogy , i.e. every 4-tuple a, b, c, letters ,relation : b :: c : defined either RU E F ALSE.Definition 2.7 (Alignment two sequences) alignment two sequences x,, lengths n, word z alphabet ( ) ( ){(, )} whose first projectionsemantically equivalent x whose second projection semantically equivalent y.2. classically language theory, word sentence.799fiM ICLET, BAYOUDH & ELHAYInformally, alignment represents one-to-one letter matching two sequences,letters may inserted. matching (, ) permitted. alignmentpresented array two rows, one x one y, word completed ,resulting two words length.instance, alignment x = abgef = acde :x==|b|c|g|e|ef|extend definition alignment four sequences.Definition 2.8 (Alignment four sequences) alignment four sequencesu, v, w, x , word z alphabet ( {})4 {(, , , )} whose projectionfirst, second, third fourth component respectively semantically equivalentu, v, w x.following definition uses alignments four sequences.Definition 2.9 (Analogical proportion sequences) Let u, v, w x four sequences, analogy defined. say u, v, w x analogical proportionexists four sequences u , v , w x length n , following properties:1. u u, v v, w w x x.2. [1, n] analogies ui : vi :: wi : xi hold true .One note Lepage (2001) Stroppa Yvon (2004) already proposed definition analogical proportion sequences applications linguistic data. Basically,difference accept trivial analogies alphabet (such : b :: : b::: :).example, let = {a, b, , , B, C, } non trivial analogies : b :: : B ,: :: b : : :: B : . following alignment four sequences aBA,bBA, ba ba analogical proportion :bBBbb3. Analogical Dissimilarity3.1 Motivationsection, interested defining could relaxed analogy, linguisticexpression would b almost c d. remain coherent previous definitions,measure term almost positive real value, equal 0 analogy stands true,increasing four objects less likely analogy. also want value,call analogical dissimilarity (AD), good properties respect analogy. want800fiA NALOGICAL ISSIMILARITYsymmetrical, stay unchanged permute mean terms analogy finallyrespect triangle inequality. requirements allow us, section 4, generalizeclassical fast nearest neighbor search algorithm exhibit algorithmic learning processprinciple extract, learning set, 3-tuple objects least ADcombined another unknown object. lazy learning technique therefore generalizationnearest neighbor method.firstly study definition analogical dissimilarity structured setsprevious sections, secondly extend sequences.3.2 Definition {0, 1}nDefinition 3.1 (Analogical dissimilarity {0, 1}) analogical dissimilarity four binary values given following table:uvwxAD(u, v, w, t)00000000110010100110010010101001102011111000110012101001011111000110111110111110words, AD four binary values minimal number bitsswitched order produce analogical proportion. seen extension editdistance four dimensions supports coherence analogy.Definition 3.2 (Analogical dissimilarity {0, 1}n ) analogical dissimilarity AD(u, v, w, t) four objects u, v, w finite set X defined binary features sum valuesanalogical dissimilarities features.3.2.1 P ROPERTIESdefinition, analogical dissimilarity following properties:Property 3.1 (Properties AD {0, 1}n )Coherence analogy.(AD(u, v, w, x) = 0) u : v :: w : xSymmetry as. AD(u, v, w, x) = AD(w, x, u, v)Exchange medians. AD(u, v, w, x) = AD(u, w, v, x)Triangle inequality. AD(u, v, z, t) AD(u, v, w, x) + AD(w, x, z, t)Asymmetry to. general: AD(u, v, w, x) 6= AD(v, u, w, x)first properties quite straightforward definition. demonstration third onesimple well. propertyAD(fi (u), fi (v), fi (z), fi (t)) AD(fi (u), fi (v), fi (w), fi (x))+ AD(fi (w), fi (x), fi (z), fi (t))801fiM ICLET, BAYOUDH & ELHAYwbxAD(u, v, w, x) = 2 (t, x)vuFigure 2: Analogical dissimilarity vector spaces distance 2 .holds true every 6-tuple elements every feature fi , property (4) true. demonstration done examining possible cases: impossible find 6 binary features a, b,c, d, e, f AD(a, b, e, f ) = 2 AD(a, b, c, d) + AD(c, d, e, f ) < 2. precisely,AD(a, b, e, f ) = 2, AD(a, b, c, d) + AD(c, d, e, f ) also equal 2 four values(c, d) take.3.3 Analogical Dissimilarity Rnanalogical dissimilarity four vectors must reflect way farconstructing parallelogram. Four vectors u, v, w x analogical proportion (i.e., formparallelogram) opposite sidesuvwxOu + Ox = Ov + Ow, equivalentlyu + x = v + w, chosen following definition (see Figure 2):Definition 3.3 (Analogical dissimilarity vectors) analogical dissimilarityfour vectors u, v, w x Rn defined norm k kp corresponding distancep given real positive value AD(u, v, w, x) = p (u + x, v + w) = k(u + x) (v + w)kp .also equal p (t, x), solution analogical equation u : v :: w : t.Property 3.2 (Properties AD vectors) definition analogical dissimilarityRn guarantees following properties hold true: coherence analogy, symmetry as,exchange medians, triangle inequality asymmetry to.first two properties quite straightforward definition. Since k kp norm,respects triangle inequality involves third property:AD(u, v, z, t) AD(u, v, w, x) + AD(w, x, z, t)3.4 Analogical Dissimilarity Sequencespresent following definition two algorithms. Firstly, extend notion analogical dissimilarity sequences. first algorithm, called SEQUANA4, computes analogicaldissimilarity four sequences . second one, called SOLVANA, given analogicalequation sequences, produces Directed Acyclic Graph (DAG) solutions.solution, gives DAG sentences least analogical dissimilarityassociated three known sentences equation.802fiA NALOGICAL ISSIMILARITYtwo algorithms quite general, since make particular assumption alphabet sequences. alphabet simply augmented = {} producealignments described section 2.5. analogical dissimilarity must that:AD(, , a, a) = 0, AD(, a, b, c) > 0 every a, b, c , constraintrequired.3.4.1 EFINITIONLet set defined analogical dissimilarity AD. augment addingspecial symbol . assume analogical dissimilarity AD .Definition 3.4 (Analogical dissimilarity four sequences) cost alignment four sequences sum analogical dissimilarities 4-tuples lettersgiven alignment.analogical dissimilarity AD(u, v, w, x) four sequences costalignment minimal cost four sequences.definition ensures following properties hold true: coherence analogy, symmetryas, exchange medians asymmetry to3 .Depending looking for, many methods developed multiples alignment bio-informatics (Needleman & Wunsch, 1970; Smith & Waterman, 1981) :1. structure functional similarity like protein modelization, pattern identificationstructure prediction DNA, methods using simultaneous alignment like MSA (Wang &Jiang, 1994) DCA (Dress, Fllen, & Perrey, 1995), iterative alignment like MUSCLE(Edgar, 2004) best.2. Evolutionary similarity like phylogenic classification, methods using progressive alignment tree structure, like ClustalW (Thompson, Higgins, & Gibson, 1994),fitted.However, alignment methods (global local) heuristic algorithms overcomeproblem time space complexity introduced first length sequences secondnumber sequences align. generation problem neither sequence lengtharound 30 characters number sequences align always four analogy needheuristic alignment speed algorithm. techniques used bio-informatics computeautomatically substitution matrix could helpful interesting handwritten charactersrecognition. Introducing Gap (Gep, Gop) penalties like DNA protein sequences alsointeresting idea explore.3.5 Computing Analogical Dissimilarity Four Sequences: SEQUANA4Algorithmcompute AD(u, v, w, x) dynamic programming algorithm, called SEQUANA4, progresses synchronicity four sequences build optimal alignment.3. definition AD, triangle inequality property always true sequences.803fiM ICLET, BAYOUDH & ELHAYinput algorithm augmented alphabet analogical dissimilarity AD(a, b, c, d). output analogical dissimilarity four sentences ,namely AD(u, v, w, x).give basics formulas recurrence. implementing computation, onecheck correct progression indexes i, j, k l.InitialisationCwu00vx00 0 ;uv0+ AD(ui , , , ) done ;= 1, |u| Cwui0vx00 Cwi10 x0u vu vj = 1, |v| Cw00 xj0 Cw00 xj1+ AD(, vj , , ) done ;0v0k = 1, |w| Cwu0kvx00 Cwu0k1x0 + AD(, , wk , ) done ;l = 1, |x| Cwu00vx0l Cwu00vx0l1 + AD(, , , xl ) done ;RecurrenceuvCwik xjlu vC i1 j1 + AD(ui , vj , wk , xl )wu k1vxl1j1Cwi1+ AD(ui , vj , wk , )k1 xlui1 vj1Cwk xl1 + AD(ui , vj , , xl )ui1 vj1+ AD(ui , vj , , )Cwk xluvj1Cwk1 xl1 + AD(, vj , wk , xl )uvCwik xj1l1 + AD(, vj , , xl )uvj1Cwk1 xl + AD(, vj , wk , )uv= Cwik xj1+ AD(, vj , , )lui1 vjCwk1 xl1 + AD(ui , , wk , xl )uvjCwi1k xl1 + AD(ui , , , xl )uvjCwi1k1 xl + AD(ui , , wk , )uvj+ AD(ui , , , )Cwi1k xlu vjCwk1 xl1 + AD(, , wk , xl )uvCwik xjl1 + AD(, , , xl )u vjCwk1 xl + AD(, , wk , )+ 1; j j + 1; k k + 1; l l + 1+ 1; j j + 1; k k + 1+ 1; j j + 1; l l + 1+ 1; j j + 1j j + 1; k k + 1; l l + 1j j + 1; l l + 1+ 1; k k + 1j j+1+ 1; j j + 1; l l + 1+ 1; l l + 1+ 1; k k + 1ii+1k k + 1; l l + 1l l+1k k+1End= |u| j = |v| k = |w| l = |x|.Resultu v|v|Cw|u|AD(u, v, w, x) .|w| x|x|Complexityalgorithms runs time complexity |u|.|v|.|w|.|x| .Correctnesscorrectness algorithm demonstrated recurrence, since uses dynamic programming principles. requires analogical dissimilarity propertiescalled: coherence analogy, symmetry exchange medians. triangleinequality property necessary.804fiA NALOGICAL ISSIMILARITY3.6 Generalized Resolution Analogical Equations Sequences: SOLVANA Algorithm3.6.1 PPROXIMATE OLUTIONS NALOGICAL E QUATIONnow, considered analogical equation either one (or several) exact solutions,solution. latter case, concept analogical dissimilarity useful defineapproximate solution.Definition 3.5 (Best approximate solution analogical equation) Let X setdefined analogy analogical dissimilarity AD. Let : b :: c : x analogicalequation X. set best approximate solutions equation given by:: arg min AD(a, b, c, y)yXwords, best approximate solutions objects X closestanalogical proportion a, b c. Obviously, definition generalizes solutionanalogical equation given section 2.2. Since defined AD good propertiesseveral alphabets sequences alphabets, compute approximate solutionanalogical equations domains.easily enlarge concept define set k-best solutions analogicalequation : b :: c : x . Informally, subset k elements X minimal ADassociated fourth position a, b c.Rn {0, 1}n , one best approximate solution analogical equation,easily computed (see sections 3.2 3.3). Finding set k-best solutions alsosimple problem.Let us turn algorithm finds set best approximate solutionsequation u : v :: w : x objects sequences alphabet ADdefined. also make comments extend capacity find set k-bestsolutions.3.6.2 SOLVANA LGORITHMalgorithm uses dynamic programming construct 3-dimensional array. construction finished, backtracking performed produce DAG best solutions.alignment four sequences different lengths realized inserting lettersfour sequences length. done, consider columnalignment analogical dissimilarity augmented alphabet.construct three dimensional n1 n2 n3 matrix (respectively length first,second third sequences A, B C analogical equation B C x).find fourth sequence, fill following recurrence:805fiM ICLET, BAYOUDH & ELHAY[i, j, k]1i,j,kn1 ,n2 ,n3[i 1, j 1, k 1] + AD(ai , bj , ck , x)x[i, j 1, k 1] + AD(, bj , ck , x)x[i,j,k1]+AD(, , ck , x)x[i, j 1, k] + AD(, bj , , x)=x[i 1, j, k 1] + AD(ai , , ck , x)x[i 1, j 1, k] + AD(ai , bj , , x)x[i 1, j, k] + AD(ai , , , x)xaiithobject sequence A. = {}.step, save cell [i, j, k] cost also letter(s) found analogicalresolution along optimal way progression. completed, backward propagationgives us optimal generated sequences optimal analogical dissimilarity, strucured DAG.computational complexity algorithm O(m n3 ), = Card( ) nmaximum length sequences3.6.3 E XAMPLELet = {a, b, c, A, B, C} alphabet defined 5 binary features, follows:bcBCf11001000f20100100f30010010f41110000f50001110first three features indicates letter (for example, f1 true only)last two indicate case letter (f4 holds true lower case letters, f5 upper case letters).example, let ab : Bc :: Bc : x analogical equation. exact solution, sixbest approximate solutions AD(ab, Bc, Bc, y) = 4, example = BB = Cc.Figure 3 displays DAG results produced SOLVANA example.4. Analogical Dissimilarity Machine Learning4.1 Motivationassume exists analogy defined set X analogical dissimilarityAD following properties: coherence analogy, symmetry as, triangle inequality,exchange medians asymmetry to.Let set elements X, cardinality m, let another element X6 S. problem tackle section find triple objects (u, v, w)806fiA NALOGICAL ISSIMILARITYFigure 3: Result SOLVANA: DAG best approximate solutions analogical equationsequences. path displays different alignment optimal cost.that:AD(u, v, w, y) = arg min AD(t1 , t2 , t3 , y)t1 ,t2 ,t3directly lead us use notion AD supervised machine learning, e.g. classification rule.4.2 Brute Force Solutionobvious solution examine triples S. brute force method requires m3 callsprocedure computing analogical dissimilarity four objects X. Accordingproperties analogical dissimilarity, number actually divided 8,change theoretical practical complexity search.situation similar search nearest neighbor Machine Learning,naive algorithm requires distance computations. Many proposals madedecrease complexity (see example work Chvez, Navarro, Baeza-Yates, & Marroqun,2001). chosen focus extension AESA algorithm, based propertytriangle inequality distances (Mic, Oncina, & Vidal, 1994). Since definedconcept analogical dissimilarity similar property, natural explore extendalgorithm.807fiM ICLET, BAYOUDH & ELHAY4.3 FADANA: FAst search least Dissimilar ANAlogysection describes fast algorithm find, given set objects cardinaltyobject y, three objects (z , , x ) analogical dissimilarity AD(z , , x , y)minimal. based AESA technique, extended analogical dissimilarity.Thanks properties, analogical dissimilarity AD(z, t, x, y) seen distancetwo couples (z, t) (x, y), consequently basically work couples objects.use equivalently paragraph terms (analogical) distance two couples (u, v)(w, x) (analogical) dissimilarity four elements u, v, w x describeAD(u, v, w, x).4.3.1 P RELIMINARY C OMPUTATIONpart, done line, compute analogical dissimilarity everyfour objects data base. step complexity time space O(m4 ),size S. come back point section 4.4, progressAESA-like LAESA-like technique reduce computational complexity.4.3.2 P RINCIPLE LGORITHMbasic operation compose couple objects adding object xi =1, m. goal find couple objects lowest distance (xi , y),change xi xi+1 . Looping times AESA-like select eliminate technique insuresfinally find triple lowest analogical dissimilarity associated y.4.3.3 N OTATIONSLet us denote:C set couples (u, v) distance (xi , y) already computed.= arg min(AD(z, t, xi , y))(z,t)U=arg min (AD(z, t, xi , y))(z,t)U ,1jiDist = {AD(z, t, xi , y), (z, t) C}Dist(j) j th element DistQuadU = {(z, t, xi , y), (z, t) C}QuadU (j) j th element QuadUalgorithm constructed three following phases:4.3.4 NITIALIZATIONtime xi changes (when increased 1), set U refilled possiblecouples objects S.808fiA NALOGICAL ISSIMILARITYset C Dist contain respectively couples distances (xi , y)measured one loop, initialized empty sets.local minimum in, containing minimum analogical dissimilarities one loopset infinity.k = Card(C) represents number couples distance computed(xi , y) current loop. k initialized zero.Algorithm 1 Algorithm FADANA: initialization.beginU {(xi , xj ), = 1, j = 1, m};C ;+;Dist ;k 0;end4.3.5 ELECTIONgoal function extract set U couple (zz, tt) promisingterms minimum analogical dissimilarity (xi , y), using criterion:fifi(zz, tt) = arg min ax fi AD(u, v, z, t) AD(z, t, xi , y) fi(u,v)U(z,t)CAlgorithm 2 Algorithm FADANA: selection promising couple.selection(U, C, (xi , y), Dist)begins0= 1,PCard(U)PjC |AD(zj , tj , ui , vi ) Dist(j)|jC |AD(zj , tj , ui , vi ) Dist(j)|;arg min i;endendReturn (uarg min , varg min );end4.3.6 E LIMINATIONsection couples (u, v) U analogical distance (xi , y)less already found eliminated thanks two criteria below:AD(u, v, z, t) AD(z, t, y, xi ) AD(u, v, xi , y)AD(u, v, z, t) AD(z, t, y, xi ) + AD(u, v, xi , y)809fiM ICLET, BAYOUDH & ELHAY= AD(z , , x , y) represents minimum analogical dissimilarity found (seefigure 4). Note updated whole algorithm never reinitializedincreased.Algorithm 3 Algorithm FADANA: elimination useless couples.eliminate(U, C, (xi , y), , k)(zk , tk ) k th element QuadUbegin= 1, Card(U)AD(zk , tk , ui , vi ) Dist(k) +U U {(ui , vi )};C C {(ui , vi )};else AD(zk , tk , ui , vi ) Dist(k)U U {(ui , vi )};C C {(ui , vi )};endendendAlgorithm 4 Algorithm FADANA: main procedure.begin{xi , = 1, m};AD +;= Card(S)Initialize;U =6(z, t) selection(U, C, (xi , y), Dist);Dist(k) AD(z, t, xi , y);k = k + 1;U U {(z, t)};C C {(z, t)};Dist(k)eliminate(U, C, (xi , y), , k)elseDist(k);Dist(k) < ADAD Dist(k);z z, t, x xi ;endk = 1, Card(C)eliminate(U, C, (xi , y), , k)endendendendbest triple (z , , x ) ;least analogical dissimilarity AD = AD(z , , x , y) ;end810fiA NALOGICAL ISSIMILARITY(u2 , v2 )(z, t)(y, xi )bb(u1 , v1 )b(z , )(u3 , v3 )Figure 4: Elimination process FADANA.4.4 Selection Base Prototypes FADANAfar, FADANA drawback requiring precomputing time storage O(m4 ),practice impossible handle > 100.go further, devised ameliorated version FADANA algorithm,preliminary computation storage limited N.m2 , N certain number couplesobjects. principle similar LAESA (Mic et al., 1994). N base prototypes couplesselected among m2 possibilities greedy process, first one chosenrandom, second one far possible first one, on. distancecouples objects is, according definition analogical dissimilarity:(x, y), (z, t) = AD(z, t, x, y)4.5 Efficiency FADANAconducted experiments measure efficiency FADANA. testedalgorithm four databases UCI Repository (Newman, Hettich, Blake, & Merz, 1998),noting time percentage AD computed in-line different numbers base prototypescompared made naive method (see Figure 5, scales logarithmic). numberbase prototypes expressed percentage learning set. Obviously, learning setcontains elements, number possible 3-tuples built m3 . point explainspercentage base prototypes compared size learning set rise 100%.number in-line computations AD mean test set.observe results optimal number base prototypes 10% 20%aim optimize computation time performance.5. Two Applications Machine Learning Problems5.1 Classification Objects Described Binary Nominal Featurespurpose first experiment measure benefit analogical dissimilarity appliedbasic problem classification, compared standard classifiers k-nearest neighbors, neuralnetworks, decision trees. benchmarking, yet interested classifying sequences, merely investigate basic concept analogical dissimilarity bringlearning classification rule symbolic objects.811fiM ICLET, BAYOUDH & ELHAYFigure 5: Efficiency FADANA w.r.t. number base prototypes5.1.1 ETHOD ESCRIPTIONLet = oi , h(oi ) | 1 learning set, h(oi ) class object oi .objects defined binary attributes. Let x object S. learning problem findclass new object x, using learning set S. this, define learning rule basedconcept analogical dissimilarity depending integer k, could called k leastdissimilar 3-tuple rule.basic principle following: among 3-tuples (a, b, c) 3 , consider subsetproduce least analogical dissimilarity associated x (the FADANAalgorithm used here). part them, analogical equation h(a) : h(b) :: h(c) : gexact solution finite set classes. keep 3-tuples choose classtakes majority among values g class x.precisely, procedure follows:1. Compute analogical dissimilarity x n 3-tuples producesolution class x.2. Sort n 3-tuples increasing value AD associated x.3. k-th object value p, let k greatest integer k -th objectvalue p.4. Solve k analogical equations label class. Take winner votes amongk results.explain, firstly consider case two classes 0 1 . example3 classes follow.Point 1 means retain 3-tuples one four4 configurationsclass displayed Table 1. ignore 3-tuples lead equation trivialsolution classes:4. actually two more, one equivalent one four (by exchange means objects).812fiA NALOGICAL ISSIMILARITYh(a)0110h(b)0011:::::::::::::::h(c)0110: h(x): ?: ?: ?: ?resolutionh(x) = 0h(x) = 1Table 1: Possible configurations 3-tuple1 2 o3bb ec ebc ec eb cc ec cb eb eb cc c cc... ... ...h(o1 ) h(o2 ) h(o3 )001012112001102112101012011002002011111001.........h(x)121202211...AD01112222333344...k123456789...Table 2: example classification analogical dissimilarity. Analogical proportions whoseanalogical resolution classes solution (represented ) takenaccount. AD short AD(o1 , o2 , o3 , x).h(a) : h(b) :: h(c) : h(x)0 : 1 :: 1 : ?1 : 0 :: 0 : ?ExampleLet = {(a, 0 ), (b, 0 ), (c, 1 ), (d, 1 ), (e, 2 )} set five labelled objects let x 6object classified. According analogical proportion axioms, 75(= (Card(S)3 + Card(S)2 )/2) non-equivalent analogical equations among 125(= Card(S)3 )equations formed three objects x. Table (2) shows first 14lines sorting regard arbitrarily analogical dissimilarity. following table givesclassification object x according k:kkclassification x8131 2 3 4 5 6 71 3 3 5 5 7 71 1 1 ? ? 2 2fiM ICLET, BAYOUDH & ELHAY5.1.2 W EIGHTINGATTRIBUTESbasic idea weighting attributes importance classification, importance given discriminative. idea selectingenhancing interesting attributes classical Machine Learning, quite new framework analogy. paper Turney (2005), discrimination done keeping frequentpatterns words. Therefore, greater importance given attributes actually discriminant. However, analogical classification system, several ways find classunknown element. Let us take preceding two class problem example (see table 1) focuspoint.notice two ways decide class 0 class 1 (there alsothird possible configuration equivalent second exchange means).therefore take account equation used find class. define setweights attribute, depending number classes. sets storedcall analogical weighting matrix.Definition 5.1 analogical weighting matrix (W ) three dimensional array. first dimension attributes, second one class first element analogicalproportion third one class last element analogical proportion.analogical proportion weighting matrix C C matrix, number attributesC number classes.given attribute ak rank k, element Wkij matrix indicates weight mustgiven ak encountered analogical proportion classes whose first element ,j computed solution.Hence, attribute ak :First elementLast element (decision)class class jclassWkiiWkijclass jWkjiWkjjSince take account 3-tuples give solution class decision, possiblesituations one three patterns:Possible patterns: :: j : j: j :: : j: :: :FirstelementDecisionclassjjobservation gives us way compute values Wkij learning set.5.1.3 L EARNING W EIGHTING ATRIX RAINING AMPLEgoal fill three dimensional analogical weighting matrix using learning set.estimate Wkij frequency attribute k analogical proportion firstelement class , solves class j .Firstly, tabulate splitting every attribute ak classes :814fiA NALOGICAL ISSIMILARITYak = 0ak = 1. . . class . . ....n0i . . ....n1i . . .ak attribute k n0i (resp. n1i ) thePnumberPof objects classvalue 0 (resp. 1) binary attribute k. Hence, 1k=0 Ci=1 nki = (the number objectstraining set). Secondly, compute Wkij estimating probability find correctanalogical proportion attribute k first element class solves class j .following table show possible ways analogical proportionbinary attribute k. 0i (resp. 1i ) 0 (resp. 1) value attribute k class .1st2sd3rd4th5th6th0i : 0i :: 0j : 0j0i : 1i :: 0j : 1j0i : 0i :: 1j : 1j1i : 1i :: 1j : 1j1i : 0i :: 1j : 0j1i : 1i :: 0j : 0jPk (1st ) estimates probability first analogical proportion table occurs.Pk (1st ) = n0i n0i n0j n0j /m4...Wkij = Pk (1st ) + + Pk (6th ), computeWkij = (n20i + n21i )(n20j + n21j ) + 2 n0i n0j n1i n1j /(6 m4 )decision algorithm section 5.1.1 modified point 1, turns WeightedAnalogical Proportion Classifier (W AP C):Given x, find n 3-tuples produce solution class x. every3-tuple among n, say (a, b, c), consider class first element classj solution. Compute analogical dissimilarity x 3-tupleweighted AD:XWkij AD(ak , bk , ck , xk )AD(a, b, c, x) =k=1Otherwise, point 1 modified, method called Analogical Proportion Classifier (AP C).5.1.4 E XPERIMENTSR ESULTSapplied weighted analogical proportion classifier (W AP C) eight classical databases, binary nominal attributes, UCI Repository.MONK 1,2 3 Problems (MO.1, MO.2 MO.3), MONK3 problem noise added.SPECT heart data (SP.). Balance-Scale (B.S) Hayes Roth (H.R) database, multiclassdatabase. Breast-W (Br.) Mushroom (Mu.), data sets contain missing values. kr-vs-kpKasparov vs Karpov (k.k.).order measure efficiency W AP C, applied standard classifiersdatabases, also applied AP C point contribution weighting matrix(Sect.5.1.2). give parameters used comparison method Table 3:815fiM ICLET, BAYOUDH & ELHAYDecision Table: number non improving decision tables consider abandoningsearch 5.Id3: unpruned decision tree, missing values allowed.Part: partial C4.5 decision tree iteration turns best leaf rule, One-pervalue encoding.Multi layer Perceptron: back propagation training, One-per-value encoding, one hiddenlayer (# classes + # attributes)/2 nodes.LMT (logistic model trees): classification trees logistic regression functionsleaves, One-per-value encoding.IB1: Nearest-neighbor classifier normalized Euclidean distance, better results IB10.JRip: propositional rule learner, Repeated Incremental Pruning Produce Error Reduction(RIPPER), optimized version IREP. .worked WEKA package (Witten & Frank, 2005), choosing 6 different classification rules data. well fit binary data, like ID3, PART, Decision Table.Others, like IB1 Multilayer Perceptron, adapted numerical noisy data.results given Table 3. arbitrarily taken k = 100 two rules. valuek sensitive case nominal binary data small databases onesused experiments (see Figure 6). However, possible set k using validationset.Recognition rates %100rsututrsutrsrsrsututrsrsrsrsutututut8060100101102103value kFigure 6: Modification recognition rate subject k. Full line dotted line respectivelyrecognition rates database breast-w vote.draw following conclusions study: firstly, according good classificationrate W AP C Br. Mu. databases, say W AP C handles missing valueswell. Secondly, W AP C seems belong best classifiers B.S H.R databases,816fiA NALOGICAL ISSIMILARITYMethodsMO.1 MO.2 MO.3 SP. B.S Br. H.R Mu. k.k.nb. nominal atts.777224942234nb. binary atts.151515224942238nb. train instances124169 122 80 187 35668132nb. test instances432432 432 172 438 664 66 8043 3164nb. classes222232422WAPC (k = 100)98% 100% 96% 79% 86% 96% 82% 98% 61%APC (k = 100)98% 100% 96% 58% 86% 91% 74% 97% 61%Decision Table100% 64% 97% 65% 67% 86% 42% 99% 72%Id378% 65% 94% 71% 54% 71% 71%PART93% 78% 98% 81% 76% 88% 82% 94% 61%Multi layer Perceptron 100% 100% 94% 73% 89% 96% 77% 96% 76%LMT94% 76% 97% 77% 89% 88% 83% 94% 81%IB179% 74% 83% 80% 62% 96% 56% 98% 71%IBk (k = 10)81% 79% 93% 57% 82% 86% 61% 91%IB1 (k = 5)73% 59% 97% 65% 78% 95% 80% 97%JRip75% 62.5% 88% 80% 69% 86% 85% 97% 94%Table 3: Comparison Table W AP C classical classifiers eight data sets. Bestclassifiers database bold significance level equal 5%.confirms W AP C deals well multiclass problems. Thirdly, shown goodclassification rate W AP C MO.3 problem, W AP C handles well noisy data. Finally,results MO. B.S database exactly weighted decision rule W AP CAP C. due fact AD computed k = 100 null value.data bases, weighting quite effective. Unfortunatly, last database showW AP C poor recognition rate databases, means analogy fitclassification problems.5.2 Handwritten Character Recognition: Generation New Examples5.2.1 NTRODUCTIONnumber Pattern Recognition systems, acquisition labeled data expensive userunfriendly process. example, buying smartphone equipped handwritten recognition system, customer likely write dozens examples every letter digit orderprovide system consequent learning sample. However, efficient, statisticalclassification system retrained new personal writing style new patternsmany examples possible, least sufficient number well chosen examples.overcome paradox, hence make possible learning classifierexamples, straightforward idea generate new examples randomly adding noiseelements small learning sample. recent book, Bishop (2007) gives theoretical coverageprocedure, rather draws pragmatic conclusion: . . . addition random noiseinputs . . . shown improve generalization appropriate circumstances.far character recognition concerned, generating synthetic data learningrecognition system mainly used offline systems (which process image char817fiM ICLET, BAYOUDH & ELHAYu=v=w=x0 =9 9 99999 E1 L 8 9 9 9 9 9 10 E9 8 9 9 9 9 9 10 E1 L 8 9 9 9 9 9 10 E(a)1 24L 6 99 92 2 4L88 92 2 3 3L 8 992 2 3 3L 8 88(b)Figure 7: (a) Resolution Freeman direction sequences AP. (b) corresponding charactersrepresentation.acter). offline character recognition, several image distortions however used (Cano,Prez-Cortes, Arlandis, & Llobet, 2002): slanting, shrinking, ink erosion ink dilatation.online character recognition, several online distortions used, speed variationangular variation (Mouchre, Anquetil, & Ragot, 2007).therefore interested quick tuning handwritten character recognition newuser, consider small set examples character (typically 2 3)required new user. learn writer-dependent system, synthetic datakeep handwriting style original data.5.2.2 NALOGY BASED G ENERATIONsecond experiment, interested handwritten characters, captured online.represented sequence letters , = {1, 2, ..., 16, 0, C, ..., N } alphabet Freeman symbols code augmented symbols anchorage points. anchoragepoints come analysis stable handwriting properties, defined (Mouchre et al.,2007): pen-up/down, y-extrema, angular points in-loop y-extrema.learning set contains examples letter, generate synthetic examplesanalogical proportion described section 3.6 (see Figure 7). Hence, generating artificialexamples letter f analogical proportion using three instances augment learningset new different examples shown following pictures.| {z }Original letters=|{z}Analogy based generated letters5.2.3 E XPERIMENTSsection show generation strategies improves recognition rate three classicalclassifiers learned data.Experimental Protocol data base use (Mouchre et al., 2007), twelve differentwriters written 40 times 26 lowercase letters (1040 characters) PDA. use 4-fold818fiA NALOGICAL ISSIMILARITYstratified cross validation. experiments composed two phases three writerdependent recognition systems learned: Radial Basis Function Network (RBFN), K-NearestNeighbor (K-NN) one-against-all Support Vector Machine (SVM).Firstly, compute two Reference recognition Rates without data generation: RR10recognition rate achievable 10 original characters without character generation RR30gives idea achievable recognition rates original data. Practically speaking,context fly learning phase ask user input 10 characters perclass.Secondly artificial character generation strategies tested. given writer, one tencharacters per class randomly chosen. 300 synthetic characters per class generatedmake synthetic learning database. experiment done 3 times per cross validation splitper writer (12 times per user). mean standard deviation 12 performance ratescomputed. Finally means measurements computed give writer dependentmean recognition rate associated standard deviation.study three different strategies generation synthetic learning databases. strategy Image Distortions chooses randomly generation one among several image distortions. way strategy Online Image Distortions chooses randomly one distortionamong image distortions online distortions. Analogy Distortions strategy generates two-thirds base previous strategy remaining third AP generation.Results Figure 8 compares recognition rates achieved three generation strategiesthree classifiers. Firstly note global behavior three classifiers.Thus following conclusions depend classifier type. Secondly three generationstrategies complementary using Online Image Distortions better ImageDistortions alone Analogy Distortions better using distortions. Furthermore using four original character complete generation strategy better RR10.RR30 achieved using 9 10 original characters. Thus conclude using generation strategies learns classifier original data efficiently using original datalong input phase : need three times fewer original data achieve recognitionrate.Comparing Image Distortions, Online Distortions Analogy alone shows Analogyless efficient ad-hoc methods. Nevertheless, generating sequences approximate analogical proportion meaningful somewhat independant classical distorsions. words,analogy character shapes, used natural intelligence, somehow captureddefinition algorithms.aim know difference average three methods significant.performed two methods validation evaluate difference two stategies. firstmethod parametric: T-T EST (Gillick & Cox, 1989). second method non-parametric:IGN EST (Hull, 1993). methods, comparaison first secondstrategy second third strategy number original characters.T-T EST compares value difference two generation methods regardingvariation differences. assumption errors normal distributionerrors independent. mean difference large comparing standarddeviation, two strategies statistically different. case, probability resultsrandom artefact less 1012 .819fiM ICLET, BAYOUDH & ELHAYIGN EST non-parametric comparison method. benefit avoid assumptionsnormal distribution observations errors. test replaces differencesign difference. sum occurrences compared value hypothesis H0(H0 : difference methods significant). Thus strategy frequently betterexpected mean, strategy significantly better. case, probabilityhypothesis H0 true less 1030 . Hence, difference significantly better.96RR3095RR3094Recognition rate (%)Recognition rate (%)90RR1092RBFN85RR10KNN908088Image DistortionsImage DistortionsOnline Image DistortionsDistortions AnalogyOnline Image Distortions7586Distortions Analogy234567Number original characters8910234567Number original characters8910RR30Recognition rate (%)9694RR1092SVM9088Image DistortionsOnline Image Distortions86Distortions Analogy2345678910Number original charactersFigure 8: Writer-dependent recognition rates (mean standard deviation) depending number used original characters compared reference rates using 10 30 characters perclass RBFN, KNN SVM classifiers.6. Conclusions Future Workarticle, investigated formal notion analogy four objectsuniverse. given definitions analogy, formulas algorithms solving analogicalequations particular sets. given special focus objects structured sequences,original definition analogy based optimal alignments. also introduced,coherent manner, new notion analogical dissimilarity, quantifies far four objectsanalogy. notion useful lazy supervised learning: showntime consuming brute force algorithm could ameliorated generalizing fast nearestneighbor search algorithm, given preliminary experiments. However, much leftdone, want especially explore following questions:820fiA NALOGICAL ISSIMILARITYsort data particularly suited lazy learning analogy? knowbibliography linguistic data successfully processed learning analogytechniques, fields grapheme phoneme transcription, morphology, translation.currently working experiments phoneme grapheme transcription,useful special cases speech recognition (for proper names, example).also interested sequential real data, biosequences, analogicalreasoning technique (rather unformally) presently already used. selection datasupervision equally important, since search less dissemblantanalogic triple labelling process based concept analogy.sort structured data processed? Sequences naturally extended ordered trees, several generalizations alignments already defined.could useful, example, extending nearest neighbor technique learning prosodictrees speech synthesis (Blin & Miclet, 2000). could also imagine sequences models, like Hidden Markov Models (HMM) could combined analogical construction.sort algorithms devised let large amount data processedtechniques? given first answer FADANA algorithm, believequality results still increased. experiments remain donetype algorithm. notice also properties analogical dissimilarityused far. believe algorithm precomputing storageO(m) devised, currently working it.conclusion, confident fact new notion analogical dissimilaritylazy learning technique associated extended real data,structures data larger problems.Acknowledgmentsauthors would like thank anonymous referrees constructive detailed comments first version article.ReferencesAamodt, A., & Plaza, E. (1994). Case-based reasoning: Foundational issues, methodological variations, system approaches. Artificial Intelligence Communications, 7(1), 3959.Basu, M., Bunke, H., & Del Bimbo, A. (Eds.). (2005). Syntactic Structural Pattern Recognition,Vol. 27 Special Section IEEE Trans. Pattern Analysis Machine Intelligence. IEEEComputer Society.Bayoudh, S., Miclet, L., & Delhay, A. (2007a). Learning analogy : classification rulebinary nominal data. Veloso, M. M. (Ed.), International Joint Conference ArtificialIntelligence, Vol. 20, pp. 678683. AAAI Press.Bayoudh, S., Mouchre, H., Miclet, L., & Anquetil, E. (2007b). Learning classifierexamples: analogy based knowledge based generation new examples character821fiM ICLET, BAYOUDH & ELHAYrecognition.. European Conference Machine Learning, Vol. 18. Springer Verlag LNAI4701.Bishop, C. (2007). Pattern Recognition Machine Learning. Springer.Blin, L., & Miclet, L. (2000). Generating synthetic speech prosody lazy learning tree structures. Proceedings CoNLL-2000 : 4th Conference Computational Natural LanguageLearning, pp. 8790, Lisboa, Portugal.Bunke, H., & Caelli, T. (Eds.). (2004). Graph Matching Pattern Recognition Machine Vision, Special Issue International Journal Pattern Recognition Artificial Intelligence.World Scientific.Cano, J., Prez-Cortes, J., Arlandis, J., & Llobet, R. (2002). Training set expansion handwrittencharacter recognition.. 9th Int. Workshop Structural Syntactic Pattern Recognition,pp. 548556.Chvez, E., Navarro, G., Baeza-Yates, R., & Marroqun, J.-L. (2001). Searching metric spaces.ACM Comput. Surv., 33(3), 273321.Cornujols, A., & Miclet, L. (2002). Apprentissage artificiel : concepts et algorithmes. Eyrolles,Paris.Daelemans, W. (1996). Abstraction considered harmful: lazy learning language processing.den Herik, H. J. V., & Weijters, A. (Eds.), Proceedings sixth Belgian-Dutch ConferenceMachine Learning, pp. 312, Maastricht, Nederlands.Dastani, M., Indurkhya, B., & Scha, R. (2003). Analogical projection pattern perception. JournalExperimental Theoretical Artificial Intelligence, 15(4).Delhay, A., & Miclet, L. (2004). Analogical equations sequences : Definition resolution..International Colloquium Grammatical Induction, pp. 127138, Athens, Greece.Dress, A. W. M., Fllen, G., & Perrey, S. (1995). divide conquer approach multiplealignment. ISMB, pp. 107113.Edgar, R. (2004). Muscle: multiple sequence alignment method reduced time spacecomplexity. BMC Bioinformatics, 5(1), 113.Falkenhainer, B., Forbus, K., & Gentner, D. (1989). structure-mapping engine: Algorithmexamples. Artificial Intelligence, 41, 163.Gentner, D., Holyoak, K. J., & Kokinov, B. (2001). analogical mind: Perspectives cognitive science. MIT Press.Gillick, L., & Cox, S. (1989). statistical issues comparison speech recognitionalgorithms.. IEEE Conference Acoustics, Speech Signal Processing, pp. 532535,Glasgow, UK.Hofstadter, D., & Fluid Analogies Research Group (1994). Fluid Concepts Creative Analogies. Basic Books, New York.Holyoak, K. (2005). Analogy. Cambridge Handbook Thinking Reasoning, chap. 6.Cambridge University Press.Hull, D. (1993). Using statistical testing evaluation retrieval experiments. ResearchDevelopment Information Retrieval, pp. 329338.822fiA NALOGICAL ISSIMILARITYItkonen, E., & Haukioja, J. (1997). rehabilitation analogy syntax (and elsewhere), pp. 131177. Peter Lang.Lepage, Y. (2001). Apparatus method producing analogically similar word based pseudodistances words..Lepage, Y. (2003). De lanalogie rendant compte de la commutation en linguistique. UniversitJoseph Fourier, Grenoble. Habilitation diriger les recherches.Mic, L., Oncina, J., & Vidal, E. (1994). new version nearest-neighbour approximatingeliminating search algorithm aesa linear preprocessing-time memory requirements.Pattern Recognition Letters, 15, 917.Mitchell, M. (1993). Analogy-Making Perception. MIT Press.Mitchell, T. (1997). Machine Learning. McGraw-Hill.Mouchre, H., Anquetil, E., & Ragot, N. (2007). Writer style adaptation on-line handwritingrecognizers fuzzy mechanism approach: adapt method. Int. Journal PatternRecognition Artificial Intelligence, 21(1), 99116.Needleman, S. B., & Wunsch, C. D. (1970). general method applicable search similarities amino acid sequence two proteins.. J Mol Biol, 48(3), 443453.Newman, D., Hettich, S., Blake, C., & Merz, C. (1998). UCI repository machine learningdatabases..Pirrelli, V., & Yvon, F. (1999). Analogy lexicon: probe analogy-based machine learninglanguage. Proceedings 6th International Symposium Human Communication,Santiago de Cuba, Cuba.Schmid, U., Gust, H., Khnberger, K.-U., & Burghardt, J. (2003). algebraic frameworksolving proportional predictive analogies. F. Schmalhofer, R. Y., & Katz, G. (Eds.),Proceedings European Conference Cognitive Science (EuroCogSci 2003), pp. 295300, Osnabrck, Germany. Lawrence Erlbaum.Smith, T. F., & Waterman, M. S. (1981). Identification common molecular subsequences. JournalMolecular Biology, 147, 195197.Stroppa, N., & Yvon, F. (2004). Analogie dans les squences : un solveur tats finis. TALN2004.Thompson, J. D., Higgins, D. G., & Gibson, T. J. (1994). Improved sensitivity profile searchesuse sequence weights gap excision. Computer Applications Biosciences, 10(1), 1929.Turney, P. D. (2005). Measuring semantic similarity latent relational analysis. ProceedingsNineteenth International Joint Conference Artificial Intelligence (IJCAI-05), 05, 1136.Wang, L., & Jiang, T. (1994). complexity multiple sequence alignment. JournalComputational Biology, 1(4), 337348.Witten, I. H., & Frank, E. (2005). Data Mining: Practical machine learning tools techniques,2nd Edition. Morgan Kaufmann Publishers.823fiM ICLET, BAYOUDH & ELHAYYvon, F. (1997). Paradigmatic cascades: linguistically sound model pronunciation analogy.Proceedings 35th annual meeting Association Computational Linguistics(ACL), Madrid, Spain.Yvon, F. (1999). Pronouncing unknown words using multi-dimensional analogies. ProceedingEuropean conference Speech Application Technology (Eurospeech), Vol. 1, pp.199202, Budapest, Hungary.Yvon, F., Stroppa, N., Delhay, A., & Miclet, L. (2004). Solving analogical equations words.Tech. rep. ENST2004D005, cole Nationale Suprieure des Tlcommunications.824fiJournal Artificial Intelligence Research 32 (2008)Submitted 11/07; published 07/08M-DPOP: Faithful Distributed ImplementationEfficient Social Choice ProblemsAdrian PetcuBoi FaltingsADRIAN . PETCU @ EPFL . CHBOI . FALTINGS @ EPFL . CHArtificial Intelligence Lab, Ecole Polytechnique Federale de Lausanne,Station 14, 1015 Lausanne, SwitzerlandDavid C. ParkesPARKES @ EECS . HARVARD . EDUSchool Engineering Applied Sciences, Harvard University33 Oxford Street, Cambridge, 02138 USAAbstractefficient social choice problem, goal assign values, subject side constraints,set variables maximize total utility across population agents, agentprivate information utility function. paper model social choice problemdistributed constraint optimization problem (DCOP), agent communicateagents share interest one variables. Whereas existing DCOP algorithmseasily manipulated agent, either misreporting private information deviatingalgorithm, introduce M-DPOP, first DCOP algorithm provides faithful distributedimplementation efficient social choice. provides concrete example methodsmechanism design unified distributed optimization. Faithfulness ensuresagent benefit unilaterally deviating aspect protocol, neither informationrevelation, computation, communication, whatever private information agents.allow payments agents central bank, central authorityrequire. achieve faithfulness, carefully integrate Vickrey-Clarke-Groves (VCG) mechanism DPOP algorithm, agent asked perform computation, reportinformation, send messages best interest. Determining agent paymentrequires solving social choice problem without agent i. Here, present method reusecomputation performed solving main problem way robust manipulationexcluded agent. Experimental results structured problems show much 87%computation required solving marginal problems avoided re-use, providinggood scalability number agents. unstructured problems, observe sensitivityM-DPOP density problem, show reusability decreases almost100% sparse problems around 20% highly connected problems. close discussion features DCOP enable faithful implementations problem, challengereusing computation main problem marginal problems algorithmsADOPT OptAPO, prospect methods avoid welfare loss occurtransfer payments bank.1. IntroductionDistributed optimization problems model environments set agents must agreeset decisions subject side constraints. consider settings agentpreferences subsets decisions. agents self interested, one would likeobtain decision maximizes utility. However, system whole agrees (orsocial designer determines) solution selected maximize total utility acrossc2008AI Access Foundation. rights reserved.fiP ETCU , FALTINGS , & PARKESagents. Thus, problem efficient social choice. motivation, mind massivelydistributed problems meeting scheduling, decisionshold meeting, allocating airport landing slots airlines, decisionsairline allocated slot, scheduling contractors construction projects.One approach solve problems central authority computes optimal solution. combination incentive mechanism Vickrey-Clarke-Groves (VCG)mechanism (Jackson, 2000), also prevent manipulation misreporting preferences. However, many practical settings hard bound problem centralauthority feasible. Consider meeting scheduling: agent participatesmeetings, general possible find set meetings constraintsmeetings thus optimized separately. Similarly, contractors constructionproject simultaneously work projects, creating web dependencies hardoptimize centralized fashion. Privacy concerns also favor decentralized solutions (Greenstadt,Pearce, & Tambe, 2006).Algorithms distributed constraint reasoning, ABT AWC (Yokoo & Hirayama,2000), AAS (Silaghi, Sam-Haroud, & Faltings, 2000), DPOP (Petcu & Faltings, 2005b)ADOPT (Modi, Shen, Tambe, & Yokoo, 2005), deal large problems long influence agent solution limited bounded number variables. However,current techniques assume cooperative agents, provide robustness misreportspreferences deviations algorithm self-interested agents. major limitation.recent years, faithful distributed implementation (Parkes & Shneidman, 2004) proposedframework within achieve synthesis methods (centralized) MD distributedproblem solving. Faithfulness ensures agent benefit unilaterally deviatingaspect protocol, neither information-revelation, computation, communication, whatever private information agents. now, distributed implementation appliedlowest-cost routing (Shneidman & Parkes, 2004; Feigenbaum, Papadimitriou, Sami, & Shenker,2002), policy-based routing (Feigenbaum, Ramachandran, & Schapira, 2006), Internet,efficient social choice, problem broad applicability.paper, make following contributions:show model problem efficient social choice DCOP, adaptDPOP algorithm exploit local structure distributed model achievescalability would possible solving problem centralized problem graph.provide algorithm whose first stage faithfully generate DCOP representationunderlying social choice problem. DCOP representation generated,next stages M-DPOP algorithm also faithful, form ex post Nash equilibriuminduced non-cooperative game.establishing DCOP models social choice problems solved faithfully,observe communication information structure problemagent prevent rest system, aggregate, correctly determining marginalimpact allowing agents (reported) preferences total utility achievedagents. provides generality techniques DCOP algorithms.Part achieving faithfulness requires solving DCOP agents (reported) preferences ignored turn, without agent able interfere computational706fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSprocess. provide algorithm robustness property, nevertheless ablereuse, possible, intermediate results computation solving main problemagents.experimental analysis, structured meeting scheduling problems commonbenchmark literature, demonstrate much 87% computation requiredsolving marginal problems avoided reuse. Results also providedunstructured resource allocation problems 1 , show M-DPOP sensitive problemdensity: loose problems, around 80% computation reused,decreases highly connected problems.M-DPOP algorithm defines strategy agent extensive-form game inducedDCOP efficient social choice. particular, M-DPOP algorithm defines messagesagent send, computation agent perform, response messagesreceived agents. proving M-DPOP forms game-theoretic equilibrium, showagent benefit unilaterally deviating, whatever utility functions agentswhatever constraints. Although robust dominant strategy equilibrium, (expost) equilibrium requires every agent follow algorithm, Parkes Shneidman (2004)earlier commented appears necessary cost decentralization.total payment made agent bank always non-negative M-DPOP neverruns deficit (i.e. bank always receives non-negative net payment agents).settings, transfer utility bank undesirable would best avoided. providestatistics problem domains studied show loss represent much35% total utility achieved solution problems studied. paymentscannot naively redistributed back agents without breaking faithfulness, extant work redistribution mechanisms VCG payments suggests mitigated (Guo & Conitzer, 2007;Faltings, 2004; Cavallo, 2006; Moulin, 2007; Bailey, 1997). defer extension M-DPOP,details surprisingly involved interesting right, future work.reuse computation, solving marginal problems agent removed turn,especially important settings distributed optimization motivating scenariosproblem size massive, perhaps spanning multiple organizations encompassingthousands decisions. example, consider project scheduling, inter-firm logistics, intra-firmmeeting scheduling, etc. appropriate problem structure, DCOP algorithms problemsscale linearly size problem. instance, DPOP able solve problemssingle back-and-forth traversal problem graph. without re-use additionalcost solving marginal problem would make computational cost quadratic ratherlinear number agents, could untenable massive-scale applications.rest paper organized follows: preliminaries (Section 2), Section 3describe DPOP (Petcu & Faltings, 2005b) algorithm distributed constraint optimization,focus study. Section 4 introduces model self-interested agents defines(centralized) VCG mechanism. Section 4.4 provides simple method, Simple M-DPOP makeDPOP faithful serves illustrate excellent fit information communicationstructure DCOPs faithful VCG mechanisms. Section 5 describe main algorithm, MDPOP, computation re-used solving marginal problems agent removed1. consider distributed combinatorial auctions, instances randomly generated using distribution CATSproblem suite (Leyton-Brown & Shoham, 2006).707fiP ETCU , FALTINGS , & PARKESturn. present experimental results Section 6. Section 7 discuss adapting DCOPalgorithms social choice (ADOPT OptAPO, see Section 7.2), waste duepayments Section 7.3. conclude Section 8.1.1 Related Workwork draws two research areas: distributed algorithms constraint satisfaction optimization, mechanism design coordinated decision making multi-agent systemsself-interested agents. briefly overview relevant results areas.1.1.1 C ONSTRAINT ATISFACTIONPTIMIZATIONConstraint satisfaction optimization powerful paradigms model wide rangetasks like scheduling, planning, optimal process control, etc. Traditionally, problemsgathered single place, centralized algorithm applied find solution. However,social choice problems naturally distributed, often preclude use centralized entitygather information compute solutions.Distributed Constraint Satisfaction (DisCSP) (Yokoo, Durfee, Ishida, & Kuwabara, 1992;Sycara, Roth, Sadeh-Koniecpol, & Fox, 1991; Collin, Dechter, & Katz, 1991, 1999; Solotorevsky,Gudes, & Meisels, 1996) Distributed Constraint Optimization (DCOP) (Modi et al., 2005;Zhang & Wittenburg, 2003; Petcu & Faltings, 2005b; Gershman, Meisels, & Zivan, 2006) formalisms introduced enable distributed solutions. agents involved problemsmust communicate find solution overall problem (unknown onethem). Briefly, problems consist individual subproblems (each agent holds subproblem), connected (some of) peers subproblems via constraints limitindividual agent do. goal find feasible solutions overall problem (incase DisCSP), optimal ones case DCOP.Many distributed algorithms DCOP introduced, none deals selfinterested agents. well known ones ADOPT, DPOP OptAPO:ADOPT (Modi et al., 2005) backtracking based, bound propagation algorithm. ADOPTcompletely decentralized message passing asynchronous. ADOPTadvantage requiring linear memory, linear-size messages, applicability largeproblems 2 questionable due fact produces number messagesexponential depth DFS tree chosen.OptAPO (Mailler & Lesser, 2005) centralized-distributed hybrid uses mediator nodescentralize subproblems solve dynamic asynchronous mediation sessions.authors show message complexity significantly smaller ADOPTs. However, designed cooperative settings, settings self-interested agents likesocial choice problem, unclear whether agents would agree revealing constraintsutility functions (possibly many) agents, solve partiallycentralized subproblems.DPOP (Petcu & Faltings, 2005b) complete algorithm based dynamic programminggenerates linear number messages. DPOP, size messages depends2. largest ADOPT experiments aware comprise problems around 20 agents 40 variables.708fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSstructure problem: largest message exponential induced widthproblem (see Section 3.1.4) ADOPT, DPOP maintains full distributionproblem. features suggest DPOP good foundation efficient distributedimplementation VCG-based mechanism social choice problems.discussion features algorithms applicability social choiceproblems provided Section 7. paper, focus DPOP provide appropriatemodifications payments effective environments self-interested agents.Section 7.2 also provide brief discussion opportunites challengesapplying methodology ADOPT OptAPO.1.1.2 ECHANISM ESIGNISTRIBUTED MPLEMENTATIONlong tradition using centralized incentive mechanisms within Distributed AI, goingback least work Ephrati Rosenschein (1991) considered use VCGmechanism compute joint plans; see also work Sandholm (1996) Parkes et al. (2001)recent discussions. Also noteworthy work Rosenschein Zlotkin (1994, 1996)rules encounter, provided non-VCG based approaches task allocation systemstwo agents.hand, known methods distributed problem solvingpresence self-interested agents. example, RACO N ET (Sandholm, 1993) improvedupon C ONTRACT N ET system (Davis & Smith, 1983) negotiation-based, distributed task reallocation, providing better economic realism, RACO N ET nevertheless studied simple,myopically-rational agent behaviors performance game-theoretic agents never analyzed; remains true recent works (Endriss, Maudet, Sadri, & Toni, 2006; Dunne,Wooldridge, & Laurence, 2005; Dunne, 2005). Similarly, Wellmans work market-oriented programming (Wellman, 1993, 1996) considers role virtual markets support optimalresource allocation, developed model price-taking agents (i.e. agents treatcurrent prices though final), rather game-theoretic agents.first step providing satisfactory synthesis distributed algorithms MDprovided agenda distributed algorithmic mechanism design (DAMD), due workFeigenbaum colleagues (Feigenbaum et al., 2002; Feigenbaum & Shenker, 2002).authors (FPSS) provided efficient algorithm lowest-cost interdomain routing Internet,terminating optimal routes payments VCG mechanism. up-shotagents case autonomous systems running network domains could benefit misreporting information transit costs. missing analysis considerationrobustness algorithm manipulation. Distributed implementation (Parkes& Shneidman, 2004) introduces additional requirement. algorithm faithful agentcannot benefit deviating required actions, including information-revelation, computation message passing. number principles achieving faithfulness ex post Nashequilibrium provided Parkes Shneidman (2004). careful incentive design smallamount cryptography able remove remaining opportunities manipulationlowest-cost routing algorithm FPSS. Building this, Feigenbaum et al. (2006) recently provide faithful method policy-based interdomain routing, better capturing typical businessagreements Internet domains.709fiP ETCU , FALTINGS , & PARKESfirst work achieve faithfulness general DCOP algorithms, demonstratedvia application efficient social choice. work, Monderer Tennenholtz (1999) consider distributed single item allocation problem, focus (faithful) communicationprovide distributed computation. Izmalkov et al. (2005) adopt cryptographic primitivesballot boxes show convert centralized mechanisms DI fully connectedcommunication graph. interest demonstrating theoretical possibility ideal mechanism design without trusted center. work different focus: seek computationaltractability, require fully connected communication graphs, make appeal cryptographic primitives. hand, content retain desired behavior equilibrium(remaining consistent MD literature) Izmalkov et al. avoid introductionadditional equilibria beyond exist centralized mechanism.briefly mention two related topics. note well established literature iterativeVCG mechaisms (Mishra & Parkes, 2007; Ausubel, Cramton, & Milgrom, 2006; Bikhchandani,de Vries, Schummer, & Vohra, 2002). provide partially distributed implementationcombinatorial allocation problems, center typically issuing demand queries agentsvia prices, prices triggering computation part agents generating demand setresponse. auctions often interpreted decentralized primal-dual algorithms (Parkes& Ungar, 2000; de Vries & Vohra, 2003). setting differs remains centerperforms computation, solving winner determination problem round, agentcommunicates directly center peer-to-peer. Mualem (2005) initiates orthogonaldirection within computer science related topic Nash implementation (Jackson, 2001)economics, approach relies information part private part common knowledge,one agent entirely private information preferences.2. Preliminaries: Modeling Social Choiceassume social choice problem consists finite possibly large number decisionsmade time. decision modeled variable takevalues discrete finite domain. agent private information variablesplaces relations. relation associated agent defines utility agentpossible assignment values variables domain relation. may alsohard constraints restrict space feasible joint assignments subsets variables.Definition 1 (Social Choice Problem - SCP) efficient social choice problem modeledtuple < A, X , D, C, R > that:X = {X1 , ..., Xm } set public decision variables (e.g. holdmeetings, resources allocated, etc);= {d1 , ..., dm } set finite public domains variables X (e.g. list possibletime slots venues, list agents eligible receive resource, etc);C = {c1 , ..., cq } set public constraints specify feasible combinations valuesvariables involved. constraint cj function cj : dj1 .. djk {, 0}returns 0 allowed combinations values involved variables,disallowed ones. denote scope(cj ) set variables associated constraint cj ;710fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS= {A1 , ..., } set self-interested agents involved optimization problem;X(Ai ) X (privately known) 3 set variables agent Ai interestedrelations.R = {R1 , ..., Rn } set private relations, Ri set relations specifiedagent Ai relation rij Ri function rij : dj1 .. djk R specified agentAi , denotes utility Ai receives possible values involved variables{j1 , . . . , jk } (negative values mean costs). denote scope(rij ) domain variablesrij defined on.private relations agent may, themselves, induced solution local optimization problems additional, private decision variables additional, private constraints.kept local agent part SCP definition.optimal solution SCP complete instantiation X variables X , s.t.XXRi (X) +cj (X),(1)X arg maxXDi{1,..,n}cj Crj (X) agent Ai total utility assignment X. naturalRi (X) =rij Riproblem social choice: goal find solution maximizes total utility agents,respecting hard constraints; notice second sum X infeasible precludesoutcome. assume throughout feasible solution. introducing VCGmechanism require solution SCP influence agents relationsremoved turn. this, let SCP (A) denote main problemEq. (1)Pand SCP (Ai ) denotePmarginal problem without agent Ai , i.e. maxXD j6=i Rj (X) + cj C cj (X). Notedecision variables remain. difference SCP (A) SCP (Ai )preferences agent Ai ignored solving SCP (Ai ).variable Xj , refer agents Ai Xj X(Ai ) forming communityXj . choose emphasize following assumptions:Pagent knows variables interested, together domainvariable hard constraints involve variable.decision variable supported community mechanism allows interestedagents report interest learn other. example, mechanismimplemented using bulletin board.constraint cj C, every agent Ak community Xl scope(cj ), i.e.Xl X(Ak ), read membership lists communities Xm scope(cj )Xm 6= Xl . words, every agent involved hard constraint knowsagents involved hard constraint.agent communicate directly agents communitiesmember, agents involved shared hard constraints.communication agents required.3. Note private knowledge variables interest requirement; algorithms present workpublic private knowledge variables interest. required agents interestedvariable know - see assumptions below.711fiP ETCU , FALTINGS , & PARKESFigure 1: operator placement problem: (a) centralized model (each variable server load possiblevalues feasible combinations services run server , edges correspondrelations represent agent preferences). (b) decentralized (DCOP) model replicatedvariables. agent local replica variables interest inter-agent edges denoteequality constraints ensure agreement. preferences modeled relations hyperedges local respective agents.Section 4 establish step identifying SCP, via community mechanism, faithful self-interested agents choose volunteer communitiesmember (and communities.)2.1 Modeling Social Choice Constraint Optimizationfirst introduce centralized, constraint optimization problem (COP) model efficient social choice problem. model represented centralized problem graph. Given this,model distributed constraint optimization problem (DCOP), along associateddistributed problem graph. distributed problem graph makes explicit control structuredistributed algorithm ultimately used multi-agent system solve problem.sections illustrated reference overlay network optimization problem (Huebsch,Hellerstein, Lanham, et al., 2003; Faltings, Parkes, Petcu, & Shneidman, 2006; Pietzuch, Ledlie,Shneidman, Roussopoulos, Welsh, & Seltzer, 2006):OVERLAY N ETWORK PTIMIZATION Consider problem optimal placement data aggregation processing operators overlay network large-scale sensor network (Huebschet al., 2003; Pietzuch et al., 2006). application, multiple users multiple servers.user associated query client machine located particular nodeoverlay network. query associated set data producers, known user located712fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSnodes network. query also requires set data aggregation processing operators, placed server nodes nodes data producersusers node. user assigns utility different assignments operators servers represent preferences different kinds data aggregation. Examples in-network operatorsdata aggregation include database style join operators; e.g., user may desire volcano data Xearthquake data joined sent them. address this, specific operator callVolcanoXEarthquakeY Join created put network. Naturally, user prefersoperators placed best servers network, without regard costs incurred, overloading servers, denying service users, etc. problem find optimalallocation operators servers, subject capacity compatibility constraints.Faltings et al. (2006) model problem one efficient social choice. distributed algorithm, executed user clients situated network nodes, used determine assignmentdata aggregation processing operators server nodes.2.1.1 C ENTRALIZED COP ODEL ULTI G RAPHViewed centralized problem, SCP defined constraint optimization problemmultigraph, i.e. graph several distinct edges connect set nodes. denoteCOP (A), provide illustration Figure 1(a). decision variables nodes,relations defined subsets variables form edges multigraph; hyperedges connecttwo vertices case relation involving two variables.multiple edges involve set variables, edge correspondingrelations distinct agent set variables. hard constraints also representededges graph.Example 1 (Centralized Model Overlay Optimization) example Figure 1(a) contains3 users Ai 3 servers Sj . simplicity reasons, assume user Ai one single operator oi want executed server. According prerequisites compatibilityissues, assume S1 execute o1 o2 , o3 . Similarly, assume S2 executeo2 o3 , o1 , S3 execute combination two three operators. Agents preferences operators executed (e.g. proximitydata sources, computational capabilities servers, cost electricity, etc). example, A1extracts utility 10 o1 executed S1 , utility 5 o1 executed S3 .model problem optimization problem, use following:1. variables: server Si , create variable Si denotes set operators Siexecute.2. values: variable Si take values set possible combinations operatorsserver execute. example, S1 = {null, o1 , o2 , o1 + o2 }, null meansserver executes operator, oi executes operator oi , o1 + o2 executeso1 o2 .3. constraints: restrict possible combinations assignments. Example: two serversexecute operator.4. relations: allow agents express preferences combinations assignments. A1 modelspreference placement o1 using relation r10 , defined variables S1713fiP ETCU , FALTINGS , & PARKESS3 . relation associates utility value combination assignments S1S3 (in total 4 8 = 32 combinations) follows:0 combinations o1 executed neither S1 , S3 (e.g. hS1 = o2 , S3 =o3 i)10 combinations o1 executed S1 (e.g. hS1 = o1 , S3 = o2 + o3 i)5 combinations o1 executed S3 (e.g. hS1 = o2 , S3 = o1 i)depict variables nodes graph, constraints relations (hyper)edges (seeFigure 1(a)). problem get arbitrarily complex, multiple operators per agent, groupsservers able execute certain groups compatible operators, etc.2.1.2 ECENTRALIZED COP (DCOP) ODEL U SING R EPLICATED VARIABLESuseful define alternate graphical representation SCP, centralized problemgraph replaced distributed problem graph. distributed problem graph direct correspondence DPOP algorithm solving DCOPs. denote DCOP (A) problemagents included, corresponds main social choice problem, SCP (A). Similarly,DCOP (Ai ) problem agent Ai removed, corresponds SCP (Ai ).distributed model, agent local replica variables interested.4public variable, Xv X(Ai ), agent Ai interested, agent local replica, denotedXvi . Agent Ai models local problem COP (X(Ai ), Ri ), specifying relations rij Rilocally replicated variables.Refer Figure 1(b) translation centralized problem Figure 1(a) DCOPmodel. agent local variables loads servers interest itself, i.e.servers execute one operators (e.g. S12 represents A2 local replica variablerepresenting server S1 ). Local edges correspond local all-different constraints agentsvariables ensure execute operator several servers time. Equalityconstraints local replicas value ensure global agreement operatorsrun servers.Agents specify relations via local edges local replicas. example, agent A1relation load servers S1 S3 express preference placementoperator o1 relation r10 , assign e.g. utility 5 S3 executing o1 , utility 10 S1executing o1 .begin understand potential manipulation self-interested agentsexample. Notice although globally optimal solution may require assigning o1 S3 ,less preferable A1 , providing utility 5 instead 10. Therefore, absence incentivemechanism, A1 could benefit simple manipulation: declare utility + hS1 = o1 i, thuschanging final assignment suboptimal one nevertheless better itself.4. alternate model designates owner agent decision variable. owner agent would centralizeaggregate preferences agents interested variable. Subsequently, owner agents would usedistributed optimization algorithm find optimal solution. model limits reusability computationmain problem solving marginal problems agent removed turn excludingowner agent variable, one needs assign ownership another agent restart computational processregards variable connected variables. reuse computation important making M-DPOPscalable. approach disaggregated facilitates greater reuse.714fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSneighborhood local copy Xvi variable composed three kinds variables:Neighbors(Xvi ) = Siblings(Xvi ) Local neighbors(Xvi ) Hard neighbors(Xvi ).(2)siblings local copies Xv belong agents Aj 6= Ai also interested Xv :Siblings(Xvi ) = {Xvj | Aj 6= Ai Xv X(Aj )}(3)siblings Xvi connected pairwise equality constraint. ensuresagents eventually consistent value variable. second set variableslocal neighbors Xvi local optimization problem Ai . local copiesvariables agent Ai interested in, connected Xvi via relations Ai localproblem:Local neighbors(Xvi ) = {Xui | Xu X(Ai ), rij Ri s.t. Xui scope(ri )}(4)must also consider set hard constraints contain scope variable Xvpublic variables: Hard (Xv ) = {cs C|Xv scope(cs )}. constraints connectXv variables Xu appear scope, may interestagents well. Consequently, Xvi connected local copies Xtjvariables Xt appear hard constraints:Hard neighbors(Xvi ) = {Xtj |cs Hard (Xv ) s.t. Xt scope(cs ), Xt X(Aj )}(5)general, agent also private variables, relations constraints involveprivate variables, link public decision variables. example, consider meetingscheduling application employees company. Apart work-related meetingsschedule together, one employees also personal items agenda, like appointments doctor, etc. Decisions values private variables informationlocal relations constraints remain private. provide additional complicationsdiscussed paper.2.2 Example Social Choice Problemscontinuing present main results describe three additional problems social choiceserve motivate work. fact, problem efficient social choice fundamentalmicroeconomics political science (Mas-Colell, Whinston, & Green, 1995). problempresent large scale distributed, involves actors system businessescannot expected cooperate, either revealing preferences following rulesdistributed algorithm.IRPORT LOT LLOCATION . airports become congested, governments turningmarket-based approaches allocate landing takeoff slots. instance, U.S. Federal Aviation Administration recently commisioned study use auction allocate slotsNew Yorks congested LaGuardia airport (Ball, Donohue, & Hoffman, 2006). problem largescale expands include airports throughout U.S., eventually World, exhibitsself-interest (airlines profit-maximizing agents private information utilities715fiP ETCU , FALTINGS , & PARKESdifferent slot allocations), one privacy major concern competitiveness airline industry. typical policy goal maximize total utility allocation,i.e. one efficient social choice. problem motivates study combinatorial auctionsSection 6. combinatorial auction (CA) one set heterogeneous, indivisible goodsallocated agents, values expressed sets goods; e.g., want9am slot also get 10am slot indifferent 9am 9:05am slot.airport slot allocation problem motivated first paper CAs (Rassenti, Smith, & Bulfin,1982), recognized airlines would likely need express utilities sets slotscorrespond right fly schedule airport.PEN -ACCESS W IRELESS N ETWORKS . wireless spectrum today owned operatedclosed networks, example cellular companies T-Mobile AT&T. Howeverplenty debate creating open-access wireless networks bandwidth must availableuse phone software.5 recently proposed using auction protocolallow service providers bid dynamic auction right use spectrum given periodtime deliver services.6 Taken logical conclusion, idea anticipated RosenscheinZlotkin (1994) wired telephony, suggests secondary market wireless spectrumcorresponds problem efficient social choice: allocate spectrum maximize total utilityconsumers. problem large scale, exhibits self-interest, inherently decentralized.EETING CHEDULING P ROBLEM . Consider large organization dozens departments, spread across dozens sites, employing tens thousands people. Employeesdifferent sites departments want setup thousands meetings week. Due privacyconcerns among different departments, centralized problem solving desirable. Furthermore,although organization whole desires minimize cost whole process, department employee self interested wishes maximize utility. artificialcurrency created purpose weekly assignment made employee. Employeesexpress preferences meeting schedules units currency.Refer Figure 2 example problem, 3 agents want setup 3 meetings.Figure 2(b) shows agent local variables time slots corresponding meetingsparticipates (e.g. M12 represents A2 local replica variable representing meeting M1 ).Local edges correspond local all-different constraints agents variables ensureparticipate several meetings time. Equality constraints localreplicas value ensure global agreement. Agents specify relations via local edgeslocal replicas. example, agent A1 relation time meeting M1 expresspreference meeting later day relation r10 , assign low utilities morningtime slots high utilities afternoon time slots. Similarly, A2 prefers holding meeting M2meeting M1 , use local relation r20 assign high utilities satisfactorycombinations timeslots low utility otherwise. example, hM1 = 9AM, M2 = 11AMgets utility 10, hM1 = 9AM, M2 = 8AM gets utility 2.5. breakthrough ruling, U.S. Federal Communications Commission (FCC) require open accessaround one-third spectrum auctioned early 08.stopped short mandating spectrum made available wholesale market would service providers.Seehttp://www.fcc.gov/073107/700mhz news release 073107.pdf6. Google proposed auction filing made FCC May 21st, 2007.Seehttp://gullfoss2.fcc.gov/prod/ecfs/retrieve.cgi?native pdf=pdf&id document=6519412647.716fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSFigure 2: meeting scheduling problem. (a) centralized model (each vertex meeting variable, rededges correspond hard constraints non-overlap meetings share participant (thatagent A2 hyperedge particpates every meeting), blue edges correspondrelations represent agent preferences). (b) decentralized (DCOP) model replicatedvariables. agent local replica variables interest inter-agent edges denoteequality constraints ensure agreement. hard constraint non-overlap meetingsM1 , M2 M3 local hyperedge agent A2 . (c) DFS arrangement decentralizedproblem graph. Used DPOP algorithm control order problem solving.experimental results presented Section 6 adopt meeting scheduling prototypicalstructured social choice problems problem instances associated organizationalhierarchy. Meeting scheduling introduced Section 2.1. second set experimentsconsider combinatorial auctions (CAs), agents bid bundles goods,consider set problem instances unstructured provide comparison pointmeeting scheduling. CAs provide nice abstraction kinds allocation problems existairport wireless network domains.3. Cooperative Case: Efficient Social Choice via DPOPsection, review DPOP (Petcu & Faltings, 2005b), general purpose distributedoptimization algorithm. DPOP (Distributed Pseudotree Optimization Protocol) based dynamicprogramming adapts Dechters (Dechter, 2003) general bucket elimination scheme distributed case. main advantage generates linear number messages.contrast optimization algorithms like ADOPT (Modi et al., 2005) ensures minimal network overhead produced message exchange. hand, concern DPOPsize individual messages since grows exponentially parameter constraint graphcalled induced width (see Section 3.1.4). Nevertheless, problems exhibit local structure,DPOP typically scales much larger problems, orders magnitude efficient,717fiP ETCU , FALTINGS , & PARKEStechniques (Petcu & Faltings, 2005b, 2007). simplify exposition, first illustrateDPOP general DCOP context, show instantiate DPOP social choice problems. particular, explain leverage structure provided local replicas. considercooperative agents throughout section.3.1 DPOP Algorithm DCOPssection presents DPOP algorithm generic DCOPs. simplify exposition,assume section agent Ai represents single variable Xi ,constraint graph given.DPOP composed three phases:Phase one constructs DFS arrangement, DFS (A), defines control flow message passing computation DPOP.Phase two bottom-up utility propagation along tree constructed phase 1.phase utilities different values variables aggregated reflect optimal decisionsmade subtrees rooted node tree.Phase three top-down value assignment propagation along tree constructed phase1. phase decisions made based aggregate utility information phase 2.describing phases refer Figure 3 running example. also introduceexplicit numerical example illustrate phases two three detail.3.1.1 DPOP P HASE NE : DFS REE G ENERATIONfirst phase performs depth-first search (DFS) traversal problem graph, thereby constructing DFS arrangement problem graph. DFS arrangement subsequently usedprovide control flow DPOP guide variable elimination order. underlying problem graph tree DFS arrangement also tree. general, DFS arrangementgraph define union set tree edges additional back edges, connectnodes ancestors.7Definition 2 (DFS arrangement) DFS arrangement graph G defines rooted treesubset edges (the tree edges) remaining edges included back edges. treeedges defined adjacent nodes G fall branch tree.Figure 3 shows example DFS arrangement. tree edges shown solid lines (e.g.1 3) back edges shown dashed lines (e.g. 12 2, 4 0). Two nodes Xi Xvsaid branch DFS arrangement path higher nodelower node along tree edges; e.g., nodes X0 X11 Figure 3. DFS arrangements alreadyinvestigated means boost search constraint optimization problems (Freuder & Quinn,1985; Modi et al., 2005; Dechter & Mateescu, 2006). advantage allow algorithmsexploit relative independence nodes lying different branches DFS arrangement7. simplicity, assume follows original problem connected. However difficultyapplying DPOP disconnected problems. DFS arrangement becomes DFS forest, agentsconnected component simply execute DPOP parallel separate control thread. solution overallproblem union optimal solutions independent subproblem.718fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSFigure 3: DFS arrangement problem graph. Tree edges shown solid back edgesdashed. DFS arrangement constructed initializing token-passing X0 . k-aryconstraints, C4 , treated cliques.(i.e. nodes direct descendants ancestors one-another), possibleperform search parallel independent branches combine results.introduce definitions related DFS arrangements:Definition 3 (DFS concepts) Given node Xi DFS arrangement, define:parent Pi / children Ci : Xi ancestor/descendants connected Xi via tree-edges (e.g.P4 = X1 , C4 = {X9 , X10 }).pseudo-parents PP : Xi ancestors connected Xi via back-edges (PP 5 = {X0 }).pseudo-children PC : Xi descendants connected Xi via back-edges (e.g. PC 1 ={X8 }).separator Sep Xi : ancestors Xi directly connected Xi descendants Xi (e.g. Sep 3 = {X1 } Sep 11 = {X0 , X2 , X5 }).tree neighbors TN Xi nodes linked Xi via tree edges, TN = Pi Ci(e.g. TN 4 = {X1 , X9 , X10 }).Removing nodes Sep completely disconnects subtree rooted Xi restproblem. case problem tree, Sep = {Pi }, Xi X . general case, Sepcontains Pi , PP pseudoparents descendants Xi pseudoparentsalso ancestors Xi . example, Figure 3, separator node X4 contains parent X1 ,pseudoparent X0 . necessary sufficient values variables {X0 , X1 }set problem rooted node X4 independent rest problem. Separatorsplay important role DPOP contingent solutions must maintained propagatingutility information DFS arrangement different possible assignments separator variables.Constructing DFS Tree Generating DFS trees distributed manner taskreceived lot attention, many algorithms available: example CollinDolev (1994), Barbosa (1996), Cidon (1988), Cheung (1983) name few. purposes executing DPOP, assume example algorithm Cheung (1983),briefly outline below. instantiate DPOP SCPs, present adaptationDFS generation algorithm exploit particulars SCP.simple DFS construction algorithm starts agents labeling internally neighborsnot-visited. One agents graph designated root, using example leader719fiP ETCU , FALTINGS , & PARKESelection algorithm Abu-Amara (1988),8 simply picking agentlowest ID. root initiates propagation token, unique messagecirculated agents graph, thus visiting them. Initially, token containsID root. root sends one neighbors, waits return sendingone (still) unvisited neighbors. agent Xi first receives token, markssender parent. neighbors Xi contained token marked Xi pseudoparents(PP ).this, Xi adds ID token, sends token turn one notvisited neighbors Xj , become children. Every time agent receives token oneneighbors, marks sender visited. token return either Xj (the childXi sent first place), another neighbor, Xk . latter case, meanscycle subtree, Xk marked pseudochild.dead end reached, last agent backtracks sending token back parent.neighbors marked visited, Xi finished exploring subtree. Xi removesID token, sends token back parent; process finished Xi .root marked neighbors visited, entire DFS construction process over.Handling Non-binary Constraints. special treatment required construct neighborsvariable correspond k-ary constraints, k > 2. example, Figure 3 (left),4-ary constraint C4 involving {X0 , X2 , X5 , X11 }. Eq. 2, implies {X0 , X2 , X5 , X11 }neighbors, DFS construction process appear along branchtree. produces result Figure 3 (right).3.1.2 DPOP P HASE WO : UTIL P ROPAGATION (I NFERENCE )Phase two bottom-to-top pass DFS arrangement utility information aggregatedpropagated leaves towards root node parent tree edgesback edges. high level, leaves start computing sending UTIL messagesparents, UTIL message informs parent local utility solutionsrest problem, minimally specified terms local utility different value assignmentsseparator variables. Subsequently node propagates UTIL message representscontingent utility subtree rooted node assignments values separator variables.detail, nodes perform following steps:1. Wait UTIL messages children, store them.2. Perform aggregation: join messages children, also relationsparents pseudoparents.3. Perform optimization: project resulting join picking optimalvalues combination values variables join.4. Send result parent new UTIL message.8. cases problem initially disconnected, required choose multiple roots, one connected component. standard leader election algorithm, executed agents problem, electexactly many leaders connected components.720fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSUTIL message sent node Xi parent Pi multidimensional matrix informsPi much utility, ui (Sep ) subtree rooted Xi receives different assignments valuesvariables define separator Sep subtree. One variables, definition,variable managed parent Pi . UTIL message already represents result optimization,variables local subtree optimized different assignments separatorvariables. compute UTIL message node uses two operations: aggregation optimization.Aggregations apply JOIN operator optimizations apply PROJECT operator describedPetcu Faltings (2005b), briefly summarized here.Let UTILij UTILkj denote UTIL messages sent nodes Xi Xk parentnode Xj . denote dim(UTILkj ) set dimensions matrix, i.e. setvariables separator sending node Xk . Assuming Xj node receiving messages,define:Definition 4 (JOIN operator) operator (join): UTILij UTILkj join twoUTIL matrices. also matrix dim(UTILij ) dim(UTILkj ) dimensions.value cell join sum corresponding cells two source matrices.Definition 5 (PROJECT operator) operator (projection): Xj dim(UTILij ),UTILij Xj projection optimization UTILij matrix along Xj axis:instantiation variables {dim(UTILij ) \ Xj }, corresponding valuesUTILij (one value Xj ) tried, maximal one chosen. result matrixone less dimension (Xj ).Notice subtree rooted Xi influenced rest problem Xiseparator variables. Therefore, UTIL message contains optimal utility obtained subtreeinstantiation variables Sep separator size plays crucial role boundingmessage size.Example 2 (UTIL propagation) Figure 4 shows simple example UTIL propagation.problem tree structure (Figure 4(a)), 3 relations r31 , r21 , r10 detailed Figure 4(b).relations variables (X3 , X1 ), (X2 , X1 ) (X1 , X0 ) respectively.individual variables local replicas. UTIL phase X2 X3 project r21 r31 , respectively. results highlighed cells r21 r31 Figure 4(b).instance, optimal value X2 given X1 := assign X2 := c utility5. projections define UTIL messages send X1 . X1 receives messages X2X3 , joins together relation X0 (adds utilities messagescorresponding cells r10 ). projects join. instance, optimal valueX1 given X0 := b X1 := 2 + 5 + 6 max{3 + 4 + 4, 3 + 6 + 3}. resultdepicted Figure 4(d). UTIL message X0 receives X1 . valuemessage represents total utility entire problem value X0 . returnexample context third phase value propagation.Non-binary Relations Constraints. binary constraints/relations, k-ary constraintintroduced UTIL propagation once, lowest node DFS arrangementpart scope constraint. example, Figure 3, constraint C4 introducedUTIL propagation once, X11 , computing message parent, X5 .721fiP ETCU , FALTINGS , & PARKESFigure 4: Numerical example UTIL propagation. (a) simple DCOP problem threerelations r31 , r21 r10 (X3 , X1 ), (X2 , X1 ) (X1 , X0 ) respectively. (b) ProjectionsX2 X3 relations X1 . results sent X1 UTIL21 , UTIL31respectively. (c) X1 joins UTIL21 UTIL31 relation X0 . (d) X1 projectsjoin sends result X0 .3.1.3 DPOP P HASE HREE : VALUE P ROPAGATIONPhase three top-to-bottom pass assigns values variables, decisions made recursivelyroot leaves. VALUE propagation phase initiated root agentX0 received UTIL messages children. Based UTIL messages,root assigns variable X0 value v maximizes sum utilitycommunicated subtrees. sends VALUE(X0r v ) message every child.process continues recursively leaves, agents Xi assigning optimal valuesvariables. end phase, algorithm finishes, variables assignedoptimal values.Example 3 (Value propagation) Return example Figure 4. X0 receives UTILmessage node X1 simply choose value X0 produces largest utilitywhole problem: X0 = (X0 = X0 = c produce result example, either onechosen). value-assignment propagation phase X0 informs X1 choice viamessage VALUE (X0 a). Node X1 assigns optimal value X1 = c process continuesmessage V ALU E(X1 c) sent children, X2 X3 . children assign X2 = bX3 = algorithm terminates optimal solution hX0 = a, X1 = c, X2 = b, X3 = aitotal utility 15.3.1.4 C OMPLEXITY NALYSIS DPOPDPOP produces number messages scales linearly size problem graph, i.e.linearly number nodes edges DCOP model (Petcu & Faltings, 2005b).complexity DPOP lies size UTIL messages (note tokens passed around722fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSconstructing DFS(A) VALUE messages size linear problem graph). PetcuFaltings (2005b) show size largest UTIL message exponential parametercalled induced width (Kloks, 1994; Dechter, 2003).induced width, denoted w, constraint graph given chosen DFS arrangementstructural parameter equals size largest separator node DFS arrangement(see Definition 3.):w = max |Sep |.Xi X(6)example Figure 3, induced width graph given particular DFS orderingw = 3, given Sep11 = {X0 , X2 , X5 }. Intuitively, problem tree-like structure,lower induced width. particular, problem graph tree inducedwidth equal 1 DFS arrangement always tree. Problem graphs cliques,hand, induced width equal number nodes minus 1, irrespectiveDFS-tree arrangement.Proposition 1 (DPOP Complexity) (Petcu & Faltings, 2005b) number messages passedDPOP 2m, (n 1) (n 1) phases one, two three respectively, nnumber nodes edges DCOP model replicated variables. maximal numberutility values computed node DPOP O(Dw+1 ), largest UTIL messageO(Dw ) entries, w induced width DFS ordering used.case trees, DPOP generates UTIL messages dimension equal domain sizevariable defining parent node. case cliques, maximal message sizeDPOP exponential n 1. DFS arrangements yield width, desirableconstruct DFS arrangements provide low induced width. However, finding tree arrangementlowest induced width NP-hard optimization problem (Arnborg, 1985). Nevertheless,good heuristics identified finding tree arrangements low width (Kloks, 1994;Bayardo & Miranker, 1995; Bidyuk & Dechter, 2004; Petcu & Faltings, 2007, 2005b). Althoughdesigned explored centralized context, (notably max-degreemaximum cardinality set) easily amenable distributed environment.3.2 DPOP Applied Social Choice Problemssection, instantiate DPOP efficient social choice problems. Specifically, first showoptimization problem constructed agents preferences potential variablesinterest. Subsequently, show changes make DPOP adapt SCP domain.prominent adaptation exploits fact several variables represent local replicasvariable, treated UTIL VALUE phases.adaptation improves efficiency significantly, allows complexity claims stated termsinduced width centralized COP problem graph rather distributed COP problemgraph (see Section 3.2.5).3.2.1 NITIALIZATION : C OMMUNITY F ORMATIONinitialize algorithm, agent first forms communities around variables interest,X(Ai ), defines local optimization problem COP (X(Ai ), Ri ) replicated variable Xvi723fiP ETCU , FALTINGS , & PARKESXv X(Ai ). Shorthand Xvi COP denotes agent Ai local replica variableXv . agent owns multiple nodes conceptualize node associated virtual agent operated owning agent. virtual agent responsibleassociated variable.agents subscribe communities interested, learnagents belong communities. Neighboring relations established local variableaccording Eq. 2, follows: agents community Xv connect corresponding localcopies Xv equality constraints. so, local problems COP (X(Ai ), Ri )connected according interests owning agents. Local relationsCOP (X(Ai ), Ri ) connect corresponding local variables. Hard constraints connect local copiesvariables involve. Thus, overall problem graph formed.example, consider Figure 2(b). decision variables start times threemeetings. agent models local optimization problem creating local copies variablesinterested expressing preferences local relations. Formally, initializationprocess described Algorithm 1.Algorithm 1: DPOP init: community formation building DCOP (A).DPOP init(A, X , D, C, R):1 agent Ai models interests COP (X(Ai ), Ri ): set relations Ri imposedset X(Ai ) variables Xvi replicate public variable Xv X(Ai )2 agent Ai subscribes communities Xv X(Ai )3 agent Ai connects local copies Xvi X(Ai ) corresponding local copiesagents via equality constraints3.2.2 DFS RAVERSALmethod DFS traversal described Algorithm 2. algorithm starts choosing onevariables, X0 , root. done randomly, example using distributed algorithmrandom number generation, leader election algorithm like Ostrovski (1994), simplypicking variable lowest ID. agents involved community X0 randomlychoose one them, Ar leader. local copy X0r variable X0 becomes rootDFS. Making assumption virtual agents act behalf variable problem,functioning token passing mechanism similar described Section 3.1.1,additional consideration given community structure. root chosen, agentsparticipate distributed depth-first traversal problem graph. convenience, describeDFS process token-passing algorithm members within community observerelease pick token agents. neighbors node sorted (inline 7) prioritize copies variables held agents, local variables,finally variables linked hard constraints.Example 4 Consider meeting scheduling example Figure 2. Assume M3 chosenstart community A2 chosen within community leader. A2 creates emptytoken DFS = adds M32 ID token (DFS = {M32 }). Eq. 2, Neighbors(M32 ) ={M33 , M31 , M12 , M22 }. A2 sends token DFS = {M32 } first unvisited neighbor724fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSAlgorithm 2: DPOP Phase One: DFS construction.Inputs: Ai knows COP , Neighbors(Xvi ), Xvi COPOutputs: Ai knows P (Xvi ), PP (Xvi ), C(Xvi ), PC (Xvi ), Xvi COP .12345678910Procedure Initializationagents choose one variables, X0 , root.Agents X0 community elect leader, Ar .Ar initiates token passing X0r construct DFSProcedure Token Passing (performed virtual agent Xvi COP )Xvi root P (Xvi ) = null; create empty token DFS :=else DFS :=Handle incoming tokens()Let DFS := DFS {Xvi }Sort Neighbors(Xvi ) Siblings(Xvi ), Local neighbors(Xvi ),Hard neighbors(Xvi ). Set C(Xvi ) := null.forall Xl Neighbors(Xvi ) s.t. Xl visited yetC(Xvi ) := C(Xvi ) Xl . Send DFS Xl wait DFS token return.Send DFS token back P (Xvi ).Procedure Handle incoming tokens() //run virtual agent Xvi COP1112131415Wait incoming DFS message; let Xl senderMark Xl visited.first DFS message (i.e. Xl parent)P (Xvi ) := Xl ; PP (Xvi ) := {Xk 6= Xl |Xk Neighbors(Xvi ) DFS }; PP (Xvi ) :=elseXl/ C(Xvi ) (i.e. DFS coming pseudochild)PC (Xvi ) := PC (Xvi ) Xllist, i.e. M33 , belongs A3 . A3 receives token adds copy M3 (now DFS ={M32 , M33 }). A3 sends token M33 first unvisited neighbor, M31 (which belongs A1 ).Agent A1 receives token adds copy M3 (now DFS = {M32 , M33 , M31 }).M31 neighbor list Neighbors(M31 ) = {M32 , M33 , M11 }. Since token A1 receivedalready contains M32 M33 , means already visited. Thus, next variablevisit M11 , happens variable also belongs A1 . token passedM11 internally (no message exchange required), M11 added token (now DFS ={M32 , M33 , M31 , M11 }).process continues, exploring sibling variables community turn, passing another community, on. Eventually replicas variable arrangedchain equality constraints (back-edges) predecessors replicasvariable. dead end reached, last agent backtracks sending token backparent. example, happens A3 receives token A2 M2 community.Then, A3 sends back token A2 on. Eventually, token returns pathway root process completes.725fiP ETCU , FALTINGS , & PARKES3.2.3 H ANDLINGP UBLIC H ARD C ONSTRAINTS .Social choice problems, defined Definition 1 contain side constraints, form publicly known hard constraints, represent domain knowledge resource allocatedonce, hotel accomodate 100 people, person one meetingtime. etc. constraints owned agent, available agentsinterested variable involved domain constraint. Handling constraintsessentially unchanged handling non-binary constraints standard DPOP, describedSection 3.1.1 DFS construction phase, Section 3.1.2 UTIL phase. Specifically:DFS Construction: Neighboring relationships defined Eq. 2 require local variablelocal copies share hard constraint considered neighbors.prioritization line 7 Algorithm 2 (for DFS construction), DFS traversal mostly madeaccording structure defined relations agents hard constraintsappear backedges DFS arrangement problem graph.UTIL Propagation: Hard constraints introduced UTIL propagation phase lowestagent community variable scope hard constraint, i.e. agentvariable lowest DFS ordering. example, constraint M2M3 Figure 2 specify M2 occur M3 becomes backedge2 communities would assigned A3 handling.3.2.4 H ANDLING R EPLICA VARIABLESdistributed model SCP replicates decision variable every interested agent connects copies equality constraints. handling replica variables carefullyavoid increasing induced width k DCOP model compared induced widthw centralized model. adaptation, UTIL messages DPOP distributed problem graph would conditioned many variables local copiesoriginal variable. However, local copies represent variable must assignedvalue; thus, sending many combinations different local copies variable takedifferent values wasteful. Therefore, handle multiple replicas variable UTILpropagation though single, original variable, condition relations onevalue. realized updating JOIN operator follows:Definition 6 (Updated JOIN operator SCP) Defined two steps:Step 1: Consider UTIL messages received input. one, consider variableXvi message conditioned, also local copy original variable Xv .Rename Xvi input UTIL message Xv , i.e. corresponding name originalproblem.Step 2: Apply normal JOIN operator DPOP.Applying updated JOIN operator makes local copies variable become indistinguishable other, merges single dimension UTIL messageavoids exponential blow-up.Example 5 Consider meeting scheduling example Figure 2. centralized model Figure 2(a) DFS arrangement yields induced width 2 clique 3 nodes.726fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSNevertheless, corresponding DCOP model Figure 2(b) induced width 3, seenDFS arrangement Figure 2(c), Sep M22 = {M32 , M33 , M12 }. Applying DPOPDFS arrangement, M22 would condition UTIL message UTILM22 M12 variablesseparator: {M32 , M33 , M12 }. However, M32 M33 represent variable, M3 . Therefore, M22 apply updated JOIN operator, leverages equality constrainttwo local replicas collapse single dimension (called M3 ) message M12 .result outgoing message 2 dimensions: {M3 , M12 }, takes much lessspace. possible 3 agents involved, i.e. A1 , A2 A3 know M31 , M32M33 represent variable.change, VALUE propagation phase modified top localcopy variable solve optimization problem compute best value, announcingresult local copies assume value.3.2.5 C OMPLEXITY NALYSIS DPOP PPLIED OCIAL C HOICEspecial handling replica variables, DPOP applied SCPs scale inducedwidth centralized problem graph, independently number agents involvednumber local replica variables.Consider DFS arrangement centralized model SCP equivalentDFS arrangement DCOP model. Equivalent means original variablesSCP visited order corresponding communities visiteddistributed DFS construction. (Recall distributed DFS traversal described Section 3.1.1visits local copies community DCOP moving next community). Letw denote induced width DFS arrangement centralized SCP. Similarly, let k denoteinduced width DFS arrangement distributed model. Let = maxm |dm | denotemaximal domain variable. Then, following:Theorem 1 (DPOP Complexity SCP) number messages passed DPOP solvingSCP 2m, (n 1) (n 1) phases one, two three respectively, nnumber nodes edges DCOP model replicated variables. maximal numberutility values computed node DPOP O(Dw+1 ), largest UTIL messageO(Dw+1 ) entries, w induced width centralized problem graph.P ROOF. first part claim (number messages) follows trivially Proposition 1.second part (message size computation): given DFS arrangement DCOP, applyingProposition 1 trivially gives basic DPOP algorithm, maximal amount computationnode O(Dk+1 ), largest UTIL message O(Dk ) entries, k induced width DCOP problem graph. improve analysis need consider specialhandling replica variables.Consider UTIL messages travel along DFS tree, whose sets dimensionscontain separators sending nodes. Recall updated JOIN collapses local replicasoriginal variables. union dimensions UTIL messages join DPOPDCOP model becomes identical set dimensions nodes DPOPcentralized model. Thus, node DCOP model performs amount computationcounterpart centralized model. follows computation required DPOP scalesO(Dw+1 ) rather O(Dk+1 ) special handling.727fiP ETCU , FALTINGS , & PARKESremains one additional difference DPOP DFS arrangement centralized SCP versus DPOP DFS arrangement DCOP. variable Xv replicatedacross multiple agents projected UTIL propagation local optimization top-most agent handling local replica Xv . first noderelevant information place support optimization step. particular, whenever nodemaximal separator set also associated top-most replica variablemust retain dependence value assigned variable UTIL message sendsparent. increases worst case message size DPOP O(Dw+1 ), opposed O(Dw )normal DPOP. Computation remains O(Dw+1 ) utility determinedvalue Xv anyway, projecting Xv out. 2see effect message size described proof, local variable cannotimmediately removed UTIL propagation, consider problem Figure 2. Suppose agent A3 also involved meeting M1 . introduces additional back-edgeM23 M13 DFS arrangement decentralized model shown Figure 2(c). DFSarrangement COP model corresponds decentralized model simply traversalCOP order communities visited distributed DFS construction. corresponds chain: M3 M1 M2 . introduction additional back-edgeM23 M13 distributed DFS arrangement change DFS COP model,width remains w = 2. However, M23 top copy M2 , agent A3 cannot projectM2 outgoing UTIL message. result sends UTIL message w + 1 = 3dimensions, opposed w = 2.4. Handling Self-interest: Faithful Algorithm Social Choiceadapted DPOP remain efficient SCPs, turn issue self-interest. Without modification, agent manipulate DPOP misreporting private relationsdeviating algorithm various ways. setting meeting scheduling, example,agent might benefit misrepresenting local preferences (I massively utilitymeeting occurring 2pm 9am), incorrectly propagating utility information(competing) agents (The person team high utility meeting 2pm),incorrectly propagating value decisions (It already decided meetinginvolving person team 9am meeting must 2pm.)introducing carefully crafted payments, leveraging information communicationstructure inherent DCOPs social choice, careful partitioning computationagent asked reveal information, perform optimization, send messagesinterest, able achieve faithfulness. mean agent choose,even self-interested, follow modified algorithm. first define VCG mechanismsocial choice illustrate ability prevent manipulation centralized problem solvingsimple example. place, next review definitions faithful distributed implementation results useful principle, partition principle. describe SimpleM-DPOP algorithm without reuse computation prove faithfulness.728fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS4.1 Review: Mechanism Design VCG MechanismMechanism design (MD) addresses problem optimizing criteria, frequently social welfare, presence self-interested agents private information relevantproblem hand. standard story, agents report private information center, solvesoptimization problem enforces outcome.second-price, sealed-bid (Vickrey) auction simple example mechanism: agentmakes claim value item auctioneer, allocates item highestbidder second-highest price (Krishna, 2002). Vickrey auction usefulnon-manipulable, weakly dominant strategy agent report true value,efficient, item allocated agent highest value.setting efficient social choice, assume existence currency agentsmake payments, make standard assumption quasilinear utility functions, agentAi net utility is,ui (X, p) = Ri (X) p,(7)assignment X variables X payment pcenter, i.e., net utilityP Rj(X),minus amountrdefined utility assignment, Ri (X) =jri Ripayment. One celebrated results MD provided Vickrey-Clarke-Groves(VCG) mechanism, generalizes Vickreys second price auction problem efficientsocial choice:Definition 7 (VCG mechanism Efficient Social Choice) Given knowledge public constraints C, public decision variables X , Vickrey-Clarke-Groves (VCG) mechanism worksfollows:agent, Ai , makes report Ri private relations.centers decision, X , solves SCP (A) given reports R = (R1 , . . . , Rn ).agent Ai , makes paymentTax (Ai ) =XRj (Xi) Rj (X ) ,(8)j6=i, , solution SCP (A ) given reports Rcenter, Xi=(R1 , . . . , Ri1 , Ri+1 , . . . , Rn ).agent makes payment equals negative marginal externality presenceimposes rest system, terms impact preferences solutionSCP.VCG mechanism number useful properties:Strategyproofness: agents weakly dominant strategy, i.e. utility-maximizing strategy whatever strategies whatever private information agents, truthfully report preferences center. sense VCG mechanismnon-manipulable.729fiP ETCU , FALTINGS , & PARKESEfficiency: equilibrium, mechanism makes decision maximizes total utilityagents feasible solutions SCP.Participation:Pequilibrium, utility agent Ai , Ri (X ) Tax (Ai ) = (Ri (X ) +Pj6=i Rj (Xi ), non-negative, principle optimality, therej6=i Rj (X ))fore agents choose participate.No-Deficit:paymentmade agent Ai non-negative SCP,PPj6=i Rj (X ) principle optimality, therefore entirej6=i Rj (Xi )mechanism runs budget surplus.begin understand VCG mechanism strategyproof, notice first termTax (Ai ) independent Ai report. second term, taken togetheragentsPtrue utility decision, provides Ai net utility Ri (X ) + j6=i Rj (X ).total utility agents, maximize agent simply report true preferenceinformation, center explicitly solve problem picking X .Example 6 Return example Figure 4. make SCP associating agentsA1 , A2 A3 relations r10 , r21 r31 variables {X0 , X1 }, {X1 , X2 }, {X1 , X3 } respectively. Breaking ties before, solution SCP (A) < X0 = a, X1 = c, X2 = b, X3 => utility < 6, 6, 3 > agents A1 , A2 A3 respectively. Removing agent A1 , solutionwould < X0 =?, X1 = a, X2 = c, X3 = > utility < 5, 6 > agents A2 A3 . ?indicates agents A2 A3 indifferent value X0 . Removing agent A2 , solutionwould < X0 = c, X1 = b, X2 =?, X3 = c >, utility < 7, 4 > agents A1 A3 . Removing agent A3 , solution would < X0 = a, X1 = c, X2 = b, X3 =? >, utility < 6, 6 >agents A1 A2 . VCG mechanism would assign < X0 = a, X1 = c, X2 = b, X3 = >,payments (5 + 6) (6 + 3) = 2, (7 + 4) (6 + 3) = 2, (6 + 6) (6 + 6) = 0 collectedagents A1 , A2 A3 respectively. A3 negative impact agents A1 A2incur payment. agents make payments: presence A1 helps A2 hurts A3more, presence A2 hurts A1 A3 . conflict problemvalue assigned variable X1 . Agents A1 , A2 A3 prefer X1 assigned b, crespectively. chosen solution, agent A2 gets best outcome. Considering caseA3 , force either b selected reporting suitably high utility choice,X1 = must pay 4 X1 = b must pay 1, either case weakly preferscurrent outcome makes zero payment.introduced VCG mechanism, important realize VCG mechanismprovides known, general purpose, method exists solve optimization problemspresence self-interest private information. positive side, straightforwardextend VCG mechanism (and techniques paper) maximize linear weighted sumutility agent, weights fixed known, instance socialplanner (Jackson, 2000). Roberts (1979) hand, established Groves mechanisms VCG mechanism important special case non-trivialstrategyproof mechanisms domain social choice unless known structureagent preferences; e.g., everyone prefers earlier meetings, resource always weaklypreferred less. Together another technical assumption, Roberts theorem also extended Lavi et al. (2003) domains kind structure, instance combinatorial730fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSauctions. see real sense possible address self-interestedDCOPs maximizing something like total utility participants.4.2 Faithful Distributed Implementationgoal faithful distributed implementation distribute computation required solveSCP determine payments population agents, retaininganalog strategyproofness. challenging opens additional opportunitiesmanipulation beyond centralized VCG mechanism.presenting results, introduce following additional assumptions over-and-abovemade far:Agents rational helpful, meaning although self-interested, follow protocol whenever deviation make strictly better (given behavioragents).agent prevented posing several independent agents external technique(perhaps cryptographic) providing strong (perhaps pseudonymous) identities.Catastrophic failure occur agents community variable eventuallychoose value variable.trusted bank, connected trusted communication channel agent,authority collect payments agent.property rational helpful required able rely upon agents computepayments agents make. Strong identities required avoid known vulnerabilitiesVCG mechanism shown Yokoo, Sakurai Matsubara (2004), wherein agentssometimes better participating multiple identities. Catastrophic failure ensuresdecision determined protocol actually executed. prevents hold-out problem,unhappy agent refuses adopt consensus decision. alternative solution wouldagents report final decision trusted party, responsible enforcement.trusted communication channel, mean agent send message bank withoutinterference agent. messages sent agent upon terminationM-DPOP, inform bank agents payments. bank also assumed workdistributed MD (Feigenbaum et al., 2002, 2006; Shneidman & Parkes, 2004),trusted entity require. purpose ensure payments used align incentives.provide formal definition distributed implementation need concept localstate. local state agent Ai corresponds sequence messages agentreceived sent, together initial information available agent (includingrelations, public information constraints). Given this, distributed implementation,dM =< g, , >, defined terms three components (Shneidman & Parkes, 2004; Parkes &Shneidman, 2004):Strategy space, , defines set feasible strategies available agent Ai ,strategy defines message(s) agent Ai send every possible local state.Suggested protocol, = (s1 , . . . , sn ), defines strategy parameterizedprivate relations Ri agent Ai .731fiP ETCU , FALTINGS , & PARKESOutcome rule, g = (g1 , g2 ), g1 : n defines assignment values, g1 () D,variables X given joint strategy, = (1 , . . . , n ) n , g2 : n Rn definespayment g2,i () R made agent Ai given joint strategy n .defining message(s) sent every state, strategy encompassescomputation performed internally agent, information agent reveals privateinputs (e.g. relations), decisions agent makes propagate informationreceived messages agents.9 suggested protocol si corresponds algorithm,takes input private information available agent relevant detailsagents local state, generates message messages send neighbors network.applied distributed input R = (R1 , . . . , Rn ) known parts input hardconstraints C, protocol induces particular execution trace algorithm. turninduces outcome g(), = s(R), g1 () final assignment values (informationdistributed across agents) g2 () vector payments bankcollect agents.10main question ask, given distributed algorithm corresponding suggestedprotocol, whether suggested protocol forms ex post Nash equilibrium induced game:Definition 8 (Ex post Nash equilibrium.) Given distributed implementation dM =< g, , >,suggested protocol = (s1 , . . . , sn ) ex post Nash equilibrium (EPNE) if, agentsAi , relations Ri , relations agents Ri , alternate strategies ,Ri (g1 (si (Ri ), si (Ri ))) g2 (si (Ri ), si (Ri )) Ri (g1 (i , si (Ri ))) g2 (i , si (Ri ))(9)EPNE, agent Ai benefit deviating protocol, si , whatever particularinstance DCOP (i.e. private relations R = (R1 , . . . , Rn )), long agents alsochoose follow protocol. latter requirement makes EPNE weaker dominantstrategy equilibrium, si would best protocol agent even agentsfollowed arbitrary protocol.Definition 9 (Faithfulness) Distributed implementation, dM = < g, , >, ex post faithfulsuggested protocol ex post Nash equilibrium.is, suggested protocol, s, said ex post faithful (or simply faithful)best interest every agent Ai follow aspects algorithm information revelation,computation message-passing whatever private inputs agents, long everyagent follows algorithm.9. idea agent limited set possible messages sent local state impliednotion (restricted) strategy space justified following sense. Agents model autonomousself-interested and, course, free send message state. hand, suggestedprotocol followed every agent, messages semantically meaningful recipientagent(s) trigger meaningful change local state recipient agent(s); i.e. change local statechanges future (external) behavior recipient agent. way, strategy space characterizes completeset interesting behaviors available agent given agents follow suggested protocol.sufficient, technical perspecitve, define ex post Nash equilibrium.10. outcome rule must well-defined unilateral deviation s, i.e. one agent deviatesfollow suggested protocol. Either protocol still reaches terminal state decisions paymentsdefined, protocol reaches bad state suitably negative utility participants, livelockdeadlock. neglect latter possibility rest analysis, easily treated introducingspecial notation bad outcome.732fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS4.3 Partition Principle Applied Efficient Social ChoiceOne cannot achieve faithful DI efficient SCP simply running DPOP, n + 1 timesproblem graph, main problem agents effect nullified turnasking simply propagate messages. Agent Ai would seek following: (a) interferecomputational process SCP (Ai ), make solution close possibleSCP (A), marginal impact appears small; (b) otherwise decrease payment,example increasing apparent utility agents solution SCP (A), turnincreases value second term VCG payment (Eq. 8).opportunity manipulation recognized Parkes Shneidman (2004)general setting, proposed partition principle method achieving faithfulness distributed VCG mechanisms, instantiated context efficient SCPs:Definition 10 (partition principle) distributed algorithm, corresponding suggested protocols, satisfies partition principle application efficient social choice, if:1. (Correctness) optimal solution obtained SCP (A) SCP (Ai ) every agentfollows s, bank receives messages instruct collect correct VCG paymentevery agent.2. (Robustness) Agent Ai cannot influence solution SCP (Ai ), report(s)bank receives negative externality Ai imposes rest systemconditioned solutions SCP (A) SCP (Ai ).3. (Enforcement) decision corresponds SCP (A) enforced, bank collectspayments instructed.Theorem 2 (Parkes & Shneidman, 2004) distributed algorithm efficient social choicesatisfies partition principle ex post faithful distributed implementation.intuition behind result, note opportunity manipulation agentAi restricted to: (a) influencing solution computed SCP (A); (b) influencingpayments made agents. Agent Ai cannot prevent agents correctly solvingSCP (Ai ) correctly reporting negative externality Ai imposes agentspresence. long agents follow algorithm, ex post faithfulness followsstrategyproofness VCG mechanism additional opportunity manipulation, available misreporting preferences centralized context,change (either increase reduce) amount agents payment. opportunity(b). Opportunity (a) new. agent always influence solution contextcentralized VCG mechanism misreporting preferences.Remark: suggested previous work, weakening dominant-strategy equilibrium centralized VCG mechanism, ex post Nash equilibrium distributed implementation, viewed cost decentralization. incentive properties necessarily relypayments collected rely turn computation performed agentsturn strategy followed agents.1111. exception provided Izmalkov et al. (2005), able avoid use cryptographicprimitives, case best thought physical devices ballot boxes.733fiP ETCU , FALTINGS , & PARKES4.4 Simple M-DPOPAlgorithm 3 describes simple-M-DPOP. variation main problem, SCP (A) solved,followed social choice problem, SCP (Ai ) agent removed turn.12n + 1 problems solved, every agent Aj knows local part solution X XiAi 6= Aj , part solution affects utility. provides enoughinformation allow system agents without agent Ai , Ai , send messagebank component payment agent Ai make.Algorithm 3: Simple-M-DPOP.1 Run DPOP DCOP (A) DFS (A); find X2 forall Ai3Build DFS (Ai ); run DPOP DCOP (Ai ) DFS (Ai ); find Xi) R (X ) report bank.4agents Aj P6= Ai compute Tax j (Ai ) = Rj (Xij5Bank deducts j6=i Tax j (Ai ) Ai account6Ai assigns values X solution local COPicomputation paymentsP disaggregated across agents. tax payment collectedagent Ai Tax (Ai ) = j6=i Tax j (Ai ),Tax j (Ai ) = Rj (Xi) Rj (X ),(10)component payment occurs negative effect agent Aiutility agent Aj . information communicated bank agent Aj equilibrium.important observation, able satisfy partition principle, components Ai payment satisfy locality property, agent Aj compute component Ai payment private information relations local informationaffect utility. information availabout parts solutions X Xiable upon termination simple-M-DPOP. Correctly determining payment, condition, rely aspect agents algorithm, includingsolutions X Xi13Ai .Figure 5 provides illustration Simple M-DPOP earlier meeting scheduling example,shows marginal problems (and DFS arrangements problem)related main problem.Theorem 3 simple-M-DPOP algorithm faithful distributed implementation efficient social choice terminates outcome VCG mechanism.P ROOF. prove establish simple-M-DPOP satisfies partition principleappeal Theorem 2. First, DPOP computes optimal solutions SCP (A) SCP (Ai )12. Simple M-DPOP presented setting main problem subproblems connected extendsimmediately disconnected problems. Indeed, may main problem connected onesubproblems disconnected. see additional incentive concerns notice sufficientrecognize correctness robustness properties partition principle would retained case.13. similar disaggregation identified Feigenbaum et al. (2002) lowest-cost interdomain routingInternet. Shneidman Parkes (2004) subsequently modified protocol authors agentsAi enough information report payments made agent Ai .734fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSFigure 5: Simple M-DPOP: agent Ai excluded turn optimization DCOP (Ai ).illustrated meeting scheduling example.Ai every agent follows protocol. immediate correctnessDCOP model SCP correctness DPOP. correct VCG payments collectedevery agent follows algorithm correctness disaggregation VCG paymentsEq. 10. Second, agent Ai cannot influence solution SCP (Ai ) involvedcomputation way. DFS arrangement constructed, problem solved,agents, completely ignore Ai messages agent Ai might send. (Any hardconstraints Ai may handled SCP (A) reassigned automatically agentSCP (Ai ) consequence fact DFS arrangement reconstructed). DPOPstill solves SCP (Ai ) correctly case problem graph corresponding SCP (Ai )becomes disconnected (in case DFS arrangement forest). robustness valuereports agents 6= Ai negative externality imposed Ai , conditioned solutionsSCP (A) SCP (Ai ), follows locality property payment terms Tax j (Ai )Aj 6= Ai . enforcement, bank trusted empowered collect payments, agentsfinally set local copies variables X prevent catastrophic failure. Agent Aideviate long agents deviate. Moreover, agent Ai agentinterested variable value already optimal agent Ai anyway. 2partition principle, faithfulness, sweeping implications. agentfollow subtantive aspects simple-M-DPOP, agent also choose faithfully participate community discovery phase, algorithm choosing root community,selecting leader agent Phase one DPOP.1414. One also observe useful agent misreport local utility another agent Aj sendingUTIL messages around system. one hand, deviation could course change selection XXkk 6= {i, j} thus payments agents solution ultimately selected. But, deviating735fiP ETCU , FALTINGS , & PARKESRemark Antisocial Behavior: Note reporting exaggerated taxes hurts agentsincrease ones utility excluded assumption agents selfinterested helpful.5. M-DPOP: Reusing Computation Retaining Faithfulnesssection, present main result, M-DPOP algorithm. simple-M-DPOP,computation solve main problem completely isolated computation solvemarginal problems. comparison, M-DPOP re-use computation already performedsolving main problem solving marginal problems. enables algorithm scale wellproblems agents influence limited small part entire problemlittle additional computation required beyond DPOP. problems agentsinfluence limited precisely interest also inducedtree width small DPOP scales.challenge face, facilitating re-use computation, retain incentiveproperties provided partition principle. possible new manipulation agentAi deviate computation DCOP (A), intended effect change solutionDCOP (Ai ) via indirect impact computation performed DCOP (A)reused solving DCOP (Ai ). prevent this, determine UTIL messagesDCOP (A) could influenced agent Ai .Example 7 Refer Figure 6. agent Ai controls X3 X10 . wayinfluencing messages sent subtrees rooted {X14 , X15 , X2 , X7 , X5 , X11 }. wantable reuse many UTIL messages possible. solving problem agentAi removed strive construct DFS arrangement problem DCOP (Ai )similar possible DFS main problem. done goal maximizingre-use computation across problems. See Figure 6(b). Notice DFS forest,three distinct connected components. UTIL messages sent shaded nodesre-used solving DCOP (Ai ). UTIL messages sent nodes subtreesinfluenced agent Ai except {X14 , X15 , X5 } also X9 , differentlocal DFS arrangement.M-DPOP uses safe reusability idea suggested example. See Algorithm 4. firststage, M-DPOP solves main problem Simple-M-DPOP. complete,marginal problem DCOP (Ai ) solved parallel. solve DCOP (Ai ), DFS forest (itforest case DCOP (Ai ) becomes disconnected) constructed modificationDFS (A), retaining much structure DFS (A) possible. new DPOP (Ai )execution performed DFS U IL messages determined either reusablereusable sender message based differences DFS DFS (A).explain DFS constructed.way agent cannot change utility information finally used determining payments.agent Aj computes marginal effect agent Ai local solution, componentTax j (Ai ) agent Ai payment.736fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSFigure 6: Reconstructing DFS (Ai ) DFS (A) M-DPOP. result general DFS forest.bold nodes main DFS initiate DFS propagation. one initiated X5 redundanteventually stopped X9 . ones X4 X15 useful, subtrees becomereally disconnected removing Ai . X14 initiate propagation since X1pseudoparent. X1 controlled Ai , eventually connect X14 . NoticeX0 X9 X1 X14 turned tree edges.5.1 Phase One M-DPOP Marginal Problem: Constructing DFSGiven graph DCOP (A) DFS arrangement DFS (A) DCOP (A), one removes setnodes X(Ai ) DCOP (A) (the ones belong Ai ), need algorithm constructsDFS arrangement, DFS , DCOP (A) \ X(Ai ). want achieve following properties:1. DFS must represent correct DFS arrangement graph DCOP (Ai ) (a DFS forestcase DCOP (Ai ) becomes disconnected).2. DFS must constructed way non-manipulable Ai , i.e. without allowingagent Ai interfere construction.3. DFS similar possible DFS (A). allows reusing UTIL messagesDPOP (A), saves computation communication.main difficulty stems fact removing nodes represent variables interest agent Ai DFS (A) create disconnected subtrees. need reconnect possiblyrearrange (now disconnected) subtrees DFS (A) whenever possible. Return example Figure 6. Removing agent Ai nodes X3 X10 disrupts tree two ways:subtrees become completely disconnected rest problem (e.g. X15 X18 X19 );ones remain connected via back-edges, thus forming invalid DFS arrangement737fiP ETCU , FALTINGS , & PARKESAlgorithm 4: M-DPOP: faithfully reuses computation main problem.1 Run DPOP DCOP (A) DFS (A); find X2 forall Aiparallel3Create DFS Algorithm 5 adjusting DFS (A)4Run DPOP DCOP (Ai ) DFS :leaves DFS observe changes DFSsend null UTILi messages5678else compute UTILi messages anew, DPOPsubsequently, nodes Xk DF do:Xk receives null UTILi msgs (Pk = Pki P Pk = P Pki Ck = Cki )Xk sends null UTILi messageelsenode Xk computes UTILi message, reusing:forall Xl N eighbors(Xk ) s.t. Xl sent UTILi = nullXk reuses UTIL message Xl sent DCOP (A)Compute levy taxes simple-M-DPOP;Ai assigns values X solution local COPi ;(e.g. X5 X8 X9 ). basic principle use reconnect disconnected parts via back-edgesDFS (A) whenever possible. intended preserve much structure possible. example, Figure 6, back edge X0 X9 turned tree edge, X5 becomesX9 child. Node X8 remains X5 child.DFS reconstruction algorithm presented Algorithm 5. high-level overviewfollows (in bold state purpose step):1. (Similarity DFS (A) :) nodes retain DFS data structures constructingDFS (A); i.e., lists children, pseudo parents/children, parentsDFS (A). use data starting point building DFS arrangements,DFS (Ai ), marginal problems.2. (At least one traversal connected component DFS forest:) rootDFS (A) children15 removed nodes initiate DFS token passingDFS (A), except changes:node Xk sends token neighbors owned Ai .order Xk sends token neighbors based DFS (A): First Xkchildren DFS (A), pseudochildren, pseudoparents,parent. order helps preserve structure DFS (A) DFS (Ai ).15. Children pseudoparents excluded node, instance X14 Figure 6, initiate DFS tokenpassing would redundant: would eventually receive DFS token pseudoparent.738fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSAlgorithm 5: Reconstruction DFS DFS (A).data structures DFS denoted superscript .Procedure Token passing DFS (executed nodes Xk/ X(Ai )) :12345forall Xl Neighbors(Xk ) s.t. Xl belongs AiRemove Xl Neighbors(Xk ) Ck , PC k , PP k //i.e. send nothing AiSort Neighbors(Xk ) order: Ck , PC k , PP k , Pk //mimic DFS (A)Xk root, Pk X(Ai ) (i.e. executed root children Ai )Initiate DFS normal DFS (Algorithm 2)else Process incoming tokens()Send DFS (Xk ) back Pki // Xk subtree completely exploredProcedure Process incoming tokens()67891011Wait incoming DFS token; Let Xl senderXl Ai ignore messageelsefirst token receivedPki = Xl ; PPk = {Xj 6= Pk |Xj Neighbors(Xi ) DFS }rootk = first node token DFS17elselet Xr first node DFStraversalXr 6= rootik //i.e. another DFSdepth Xr DFS (A) < depth rootk DFS (A)Reset Pk , PP k , Ck , PC k //override redundant DFS lower rootPki = Xl ; PPk = {Xj 6= Pk |Xj Neighbors(Xi ) DFS }root k = Xr18Continue Algorithm 212131415163. (Unique traversal connected component DFS forest:) node Xk retainsroot path DFS (A) knows depth DFS arrangement. new tokenDFS arrives:first DFS token arrives, sender (let Xl ) markedparent Xk DFS : Pki = Xl . Notice Xl could differentparent Xk DFS (A). Xk stores first node received token DFS ,rootk : (provisional) root connected component Xk belongsDCOP (Ai ).first DFS token arrives, two possibilities:token received part DFS traversal process. Xk recognizesfact first node newly received tokenpreviously stored rootk . case, Xk proceeds normal, Algorithm 2:marks sender pseudochild, etc.739fiP ETCU , FALTINGS , & PARKEStoken received part another DFS traversal process, initiated anothernode rootk (see text could happen). Let Xr firstnode newly received token. Xk recognizes situation fact Xrpreviously stored rootitraversalk . case, DFSinitiated higher node DFS (A) prevails, one dropped.determine traversal pursue one drop, Xk compares depthsrootik Xr DFS (A). Xr higher, becomes new rootk . Xkoverrides previous DFS information one new token.continues token passing new token Algorithm 2.see necessary also start propagations children removed nodes (step2), consider example Figure 6. Removing X10 X3 completely disconnectssubtree {X4 , X6 , X11 , X7 , X12 , X13 }. X4 started propagation, subtree wouldvisited since connections rest problemnodes subtree.16 17Lemma 1 (DFS correctness) Algorithm 5 constructs correct DFS arrangement (or forest),DFS DCOP (Ai ) given correct DFS arrangement DFS (A) DCOP (A).P ROOF. First, since DFS started child node controlled Ai , alsoroot, ensured connected component DFS-traversed least (followsStep 2). Second, DFS process similar normal DFS construction, nodesends token neighbors (except ones controlled Ai );pre-specified order (the one given DFS (A)). follows nodes connected componenteventually visited (follows Step 3). Third, higher-priority DFS traversals overridelower priority ones (i.e. DFS traversals initiated nodes higher tree priority),Step 3. Eventually one single DFS-traversal performed single connected component. 2Lemma 2 (DFS robustness) DFS arrangement, DFS , constructed Algorithm 5 nonmanipulable agent Ai , input DFS arrangement solution phase DCOP (A).P ROOF. follows directly Step 3, since Ai participate process all:neighbors send messages (see Algorithm 5, line 1), messages may sendsimply ignored (see Algorithm 5, line 7) 2fact, additional links created constructing DFS . possible changesedges reverse direction (parents/children pseudoparents-pseudochildren16. DFS traversals initiated Step 2 redundant part problem graph visitedonce. simple overriding rule Step 3 ensures single DFS tree eventually adoptedconnected component, namely one initiated highest node original DFS (A).example, Figure 6, X5 starts unnecessary DFS propagation, eventually stopped X9 ,receives higher priority DFS token X0 . Since X9 knows X0 higher DFS (A) X5 , dropspropagation initiated X5 , relays one initiated X0 . sending X5 tokenDFS received X0 adds itself. Upon receiving new token X9 , node X5 realizesX9 new parent DFS . Thus, redundant propagation initiated X5 eliminated resultconsistent DFS subtree single connected component P1 .17. simple time-out mechanism used ensure agent knows provisional DFS ordering final(i.e. higher priority DFS traversals arrive future).740fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSswitch places), existing back-edges turn tree edges. Again, one seeFigure 6.185.2 Phase Two M-DPOP Marginal Problem: UTILi propagationsDFS built, marginal problem without Ai solved DFS . Utility propagation proceeds normal DPOP except nodes determine whether UTIL messagesent DPOP (A) reused. signaled parent sending special null UTILmessage. specifically, process follows:leaves DFS initiate UTILi propagations:1. leaves DFS observe changes local DFS arrangement compared DFS (A) UTIL message sent DCOP (A) remains validannounce parents sending instead null UTILi message.2. Otherwise, leaf node computes UTIL message anew sends (new)parent DFS .nodes wait incoming UTILi messages and:1. every incoming messages node Xk receives children nullchanges parent/pseudoparents propagate null UTILi messageparent.2. Otherwise, Xk recompute UTILi message. reusing UTILmessages received DCOP (A) children sent null messagesDCOP (Ai ) joining new UTIL messages received.example, consider DCOP (Ai ) Figure 6, X16 X17 children X14 . X14recompute UTIL message send new parent X1 . this, reusemessages sent X16 X17 DCOP (A), neither sending subtrees contain Ai .16so, X14 reuses effort spent DCOP (A) compute messages UTIL1620 , UTIL21 ,14UTIL1416 UTIL17 .Theorem 4 M-DPOP algorithm faithful distributed implementation efficient socialchoice terminates outcome VCG mechanism.P ROOF. partition principle appeal Theorem 3 (and turn Theorem 2). First,agent Ai cannot prevent construction valid DFS DCOP (Ai ) (Lemmas 1 2).Second, agent Ai cannot influence execution DPOP DCOP (Ai ) messagesAi influenced main problem DCOP (A) recomputed system without Ai .rest proof follows simple-M-DPOP, leveraging locality tax payment messagesenforcement provided bank via catastrophic failure assumption. 218. simple alternative children nodes Xki belong Ai , create bypass link first ancestorXki belong Ai . example, Figure 6, X4 X5 could create link X1 bypass X3completely DFS (Ai ). However, additional communication links may required approach.741fiP ETCU , FALTINGS , & PARKES6. Experimental Evaluation: Understanding Effectiveness M-DPOPpresent results experimental evaluation DPOP, Simple M-DPOP M-DPOPtwo different domains: distributed meeting scheduling problems (MS), combinatorial auctions(CAs). first set experiments investigate performance M-DPOP structuredconstraint optimization problem (MS) received lot attention cooperative distributedconstraint optimization. second set experiments (CAs), investigate unstructured domains, observe performance specifically ability re-use computation computingpayments M-DPOP respect problem density. CAs provide abstract model manyreal world allocation problems much studied mechanism design (Cramton, Shoham, &Steinberg, 2006).6.1 Distributed Meeting Schedulingdistributed meeting scheduling, consider set agents working large organizationrepresenting individuals, groups individuals, engaged scheduling meetingsupcoming period time. Although agents self interested, organizationwhole requires optimal overall schedule, minimizes cost (alternatively, maximizesutility agents). makes necessary use faithful distributed implementationM-DPOP. enabling this, suppose organization distributes virtual currencyagent (perhaps using currency allocation prioritize particular participants.) relations heldagents defining agents utility solution scheduling problem thus statedunits currency.agent Ai set local replicate variables Xji meeting Mjinvolved. domain variable Xj (and thus local replicas Xji ) represents feasibletime slots meeting. equality constraint included replica variables ensuremeeting times aligned across agents. Since agent cannot participate onemeeting all-different constraint variables Xij belonging agent.modeled clique constraint meeting variables. agent assigns utilitypossible time meeting imposing unary relation variable Xji .relation private Ai , denotes much utility Ai associates starting meeting Mjtime dj , dj domain meeting Mj . social objective find scheduletotal utility maximized satisfying all-different constraints agent.Following Maheswaran et al. (2004), model organization providing hierarchicalstructure. realistic organization, majority interactions within departments,small number across departments even interactions typically take placetwo departments adjacent hierarchy. hierarchical organization provides structuretest instances: high probability (around 70%) generate meetings within departments,lower probability (around 30%) generate meetings agents belongingparent-child departments. generated random problems structure, increasingnumber agents: 10 100 agents. agent participates 1 5 meetings,uniform random utility 0 10 possible schedule meetingparticipates. problems generated feasible solutions.1919. test instances found http://liawww.epfl.ch/People/apetcu/research/mdpop/MSexperiments.tgz742fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSproblem size, averaged results 100 different instances. solved mainproblems using DPOP marginal ones using simple-M-DPOP, M-DPOP respectively.experiments performed FRODO multiagent simulation environment (Petcu, 2006),1.6Ghz/1GB RAM laptop. FRODO simulated multiagent system, agent executesasynchronously thread, communicates peers via message exchange.experiments geared towards showing much effort M-DPOP able reusemain marginal problems. Figure 6.1 shows absolute computational effort termsnumber messages (Figure 6.1(a)), terms total size messages exchanged,bytes (Figure 6.1(b)). curves DPOP represent number messages (total sizemessages, respectively) required solving cooperative problem. curves simpleM-DPOP M-DPOP represent total number (size, respectively) UTIL messages,main marginal economies.notice several interesting facts. First, number messages required DPOP increaseslinearly number agents DPOPs complexity terms number messagesalways linear size problem. hand, number messages simple-MDPOP increases roughly quadratically number agents, since solves linear numbermarginal economies scratch using DPOP, requiring linear number messages.performance M-DPOP lies somewhere DPOP simple-M-DPOPadvantage achieved simple-M-DPOP size problem increases, culminatingalmost order magnitude improvement Simple M-DPOP largest problem sizes (i.e.100 agents problem). Similar observations made total size UTILmessages, also good measure computation, traffic memory requirements, inspectingFigure 6.1(b). metrics find performance M-DPOP slightly superlinear size problem.Figure 8 shows percentage additional effort required solving marginal problemsreused main problem, i.e. probability UTIL message required solving marginal problem taken directly message already used main problem.clearly see problem size increases actually reuse computationmain problem. intuition behind large problems, individual agentlocalized particular area problem. translates agent localizedspecific branch tree, thus rendering computation performed branches reusablemarginal problem corresponds respective agent. Looking also percentagereuse defined terms message size rather number messages seealso trending upwards size problem increases.6.2 Combinatorial AuctionsCombinatorial Auctions (CAs) popular means allocate resources multiple agents. CAs,bidders bid bundles goods (as opposed bidding single goods). Combinatorial bidsmodel complementarity substitutability among goods, i.e. valuationbundle more, respectively less sum valuations individual items.setting agents distributed (geographically logically), form problem graphneighbors agents bids overlap. objective find feasible solution (i.e.declare bids winning losing two winning bids share good) maximizestotal utility agents.743fiP ETCU , FALTINGS , & PARKES1000001e+0710000# messagesDPOPsimple_M-DPOPM-DPOPTotal Size UTIL MessagesDPOPsimple_M-DPOPM-DPOP10001001e+0610000010000101000102030405060708090 1001020Number agents30405060708090 100Number agents(a) Number messages(b) Total size UTIL messages (in valuations)Figure 7: Meeting scheduling problem: measures absolute computational effort (in terms number% effort marginals reused mainmessages sent total size UTIL messages) DPOP, simple-M-DPOP MDPOP. curves DPOP represent effort spent main problem, onessimple-M-DPOP M-DPOP represent total effort main marginal problems.9085807570656055Total informationNumber messages50102030405060708090100Number agentsFigure 8: Meeting scheduling problem: Percentage effort required marginal problemsreused M-DPOP main problem. Reuse measured terms percentageUTIL messages reused (dashed) also terms total size UTILmessages reused fraction total UTIL message size (solid).744fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSCAs adopted stylized model distributed allocation problems airport slotallocation wireless spectrum allocation discussed Introduction. CA instances alsoprovide counterpoint meeting scheduling problems represent problemsless structure. DCOP model, agent holds variable one bids, twopossible values: 0 bid rejected, 1 bid accepted. pair overlappingbids (bids share least one good) connected one constraint specifiescannot accepted. multiple bids submitted agentconnected additional constraints capture bid logic, instance exclusive-or constraintsone bid accepted.generated random problems using CATS (Leyton-Brown, Pearson, & Shoham, 2000), usingL3 distribution Sandholm (2002). L3 Constant distribution agentdemands bundle 3 goods, selected uniformly random, value distributed uniformly[0, 1]. simulations consider market 50 goods vary number agents5 40. recorded performance DPOP, simple-MDPOP M-DPOPgraphs Figures 9 10. Figure 9 shows density problems increase, threealgorithms require effort solving (both terms number messages, termstotal information exchange).Figure 10 shows reusability varies problem density: one see loose problems reusability good, close 100% problems 5 agents. densityproblems increases number agents, reusability decreases well, around 20%dense problems, 40 agents. explain phenomenon follows: looseproblems (many goods bidders), bids mostly non-overlapping, turn ensuresremoving individual agents solving marginal problems affect computationperformed solving main problem. end spectrum, dense problemstend highly connected, produces DFS trees similar chains.case, removing agents close bottom chain invalidates much computation performed solving main problem. Therefore, limited amount computationreused.noting L3 recognized one hardest problem distributions CATSsuite (Leyton-Brown et al., 2000), remark need limit experiments distribution problems large induced tree width (and high density problem graphs).Consider example problem every agent bids bundle overlaps everyagent. problem graph clique DPOP scale. leave detailedexamination future work, recent extension DPOP H-DPOP (Kumar, Petcu, & Faltings,2007) immediately address issue. H-DPOP, consistency techniques used ordercompactly represent UTIL messages, tightly constrained problems, orders magnitudeimprovements DPOP reported (see Section 7.1).7. Discussionsection discuss alternatives improving computational performance M-DPOP,possibility faithful variations DCOP algorithms (ADOPT (Modi et al., 2005)OptAPO (Mailler & Lesser, 2004)), loss utility agents occur duetransfer payments bank, mentioning approach address problem.745fiP ETCU , FALTINGS , & PARKES100001e+08Total Size UTIL MessagesDPOPsimple_M-DPOPM-DPOP# messages100010010DPOPsimple_M-DPOPM-DPOP1e+071e+06100000100001000100101151015202530Number agents35405(a) Number messages1015202530Number agents3540(b) Total size UTIL messages (in valuations)Figure 9: Combinatorial Auctions problems: measures absolute computational effort (in terms% effort marginals reused mainnumber messages sent total size UTIL messages) DPOP, simple-M-DPOPM-DPOP. curves DPOP represent effort spent main problem, onessimple-M-DPOP M-DPOP represent effort main marginal problems.higher number agents (and thus bids, thus constraints problem graphproblem density), greater computational effort solve problem.10090807060504030Total informationNumber messages2010510152025303540Number agentsFigure 10: Combinatorial Auctions problems: Percentage effort required marginal problemsreused M-DPOP main problem. Reuse measured terms percentageUTIL messages reused (dashed) also terms total size UTILmessages reused fraction total UTIL message size (solid).746fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMS7.1 Algorithmic Alternatives Improved PerformanceM-DPOP scales well problem size long induced width problem remainslow. characteristic M-DPOP inherits DPOP, based. problemshigh induced width, DPOP/M-DPOP require producing, sending storing large messages,may unfeasible undesirable. mitigate problem, several advances basicDPOP algorithm recently proposed. new algorithms sacrifice optimalityreturn computational tractability, makes difficult combine VCG paymentmechanism way faithfulness guaranteed. Nevertheless, H-DPOP (Kumar et al.,2007) MB-DPOP (Petcu & Faltings, 2007) employ two different techniques preserveoptimality guarantees, fitted M-DPOP.H-DPOP leverages observation many real problems contain hard constraints significantly reduce space feasible assignments. example, auctions, possibleallocate item one bidder. meeting scheduling, possible set two different start times given meeting. Unfortunately, DPOP take advantage pruningpower hard constraints, sends messages explicitly represent value combinations, including many infeasible ones. H-DPOP addresses issue using Constraint DecisionDiagrams (CDD) introduced Cheng Yap (2005) compactly represent UTIL messagesexcluding unfeasible combinations. Performance improvements several orders magnitudeachieved, especially highly constrained problems (Kumar et al., 2007).MB-DPOP (Petcu & Faltings, 2007) uses idea cycle cutsets (Dechter, 2003) exploreparts search space sequentially. Dense parts problem explored iteratingassignments subset nodes designated cycle cuts, assignment performinglimited UTIL propagation similar one DPOP. Easy parts problem exploredone-shot UTIL messages, exactly DPOP. MB-DPOP offers thus configurable tradeoffnumber messages exchanged, size messages memoryrequirements.7.2 Achieving Faithfulness DCOP Algorithmspartition principle, described Section 4.3, algorithm independent. questionwhether another, optimal DCOP algorithm made faithful therefore revolves, critically, aroundwhether algorithm satisfy robustness requirement partition priciple. makefollowing observations:Robustness first sense, i.e. agent Ai influence solution efficient SCP without agent Ai , always achievable cost restarting computationmarginal problem agent removed turn, proposed simple-M-DPOP.Robustness second sense, i.e. agent Ai influence report(s) bankreceives negative externality Ai imposes rest system, conditioningsolutions main problem problem without Ai , requires DCOPalgorithm terminates every agent knowing part solution relevantdefining utility; robustness property follows disaggregation payments.Thus, one content restart DCOP algorithm multiple times, kindsresults provide simple-M-DPOP generally available. possible747fiP ETCU , FALTINGS , & PARKESalready mentioned locality property payments, follows disaggregationVCG payment across agents Eq. (10) information communicationstructure DCOP.useful property DCOP context self-interested agents, worth reemphasizing, possible retain faithfulness even one agent plays pivotal roleconnecting problem graph. Suppose problem, DCOP (Ai ), becomes disconnected without Ai . But, case optimal solution represented union optimalsolutions connected subcomponent problem, information needs flow disconnected components either purpose solving problem purposereporting components agent Ai tax.discuss following two sections adaptation two prominent complete DCOP algorithms: ADOPT (Modi et al., 2005) OptAPO (Mailler & Lesser, 2004).discuss following two sections adaptation two prominent complete DCOP algorithms: ADOPT (Modi et al., 2005) OptAPO (Mailler & Lesser, 2004).consider computational aspects making algorithms faithful, specifically issues related efficient handling replica variables providing reusability mainmarginal problems.7.2.1 U SING ADOPTFAITHFUL , E FFICIENT OCIAL C HOICEADOPT polynomial-space search algorithm DCOP guaranteed find globallyoptimal solution allowing agents execute asynchronously parallel. agentsADOPT make local decisions based conservative cost estimates. ADOPT also works DFSarrangement, constructed detailed Section 3.1.1. Roughly speaking, main processexecuted ADOPT backtrack search DFS tree.Adaptation ADOPT DCOP Model Replicated Variables. ADOPTs complexitygiven number messages, exponential height DFS tree. SimilarDPOP, using DCOP model replicated variables could artificially increase complexitysolving process. Specifically, height DFS tree increased using replicatedvariables compared centralized problem graph. ADOPT modified exploit specialstructure replicated local variables similar way DPOP. Specifically, ADOPTexplore sequentially values original variable, ignore assignments replicasvariable take different values. works allowing agent ownshighest replica variable freely choose values variable. agent announcesnew value variable agents owning replicas variable. agentswould consider announced value replicas, add corresponding utilities, continue search process. Using special handling replica variables,resulting complexity longer exponential height distributed DFS tree,height DFS tree obtained traversing original problem graph. example, Figure 2, sufficient explore values M32 , directly assign values M33 M31via VALUE messages, without trying combinations values. reduces ADOPTscomplexity exponential 6, exponential 3.Reusability Computation ADOPT. Turning re-use computation mainmarginal problems, note ADOPT uses DFS arrangement easyidentify parts DFS arrangement main problem impossible agent748fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSmanipulate, therefore reused computing solution marginal problemagent removed. However, major difference DPOP ADOPT DPOP,agent stores outgoing UTIL message, thus available utilities contingentassignments variables agents separator. makes possible agentsimply reuse information marginal economies structure DFS provessafe. contrast, ADOPT store information linear memorypolicy. turn makes impossible reuse computation main problem marginalproblems. marginal problems solved scratch, thus performance wouldscale poorly problem size increases even structured problems meeting scheduling.see two alternatives addressing problem: (a) renounce linear memory guarantees,use caching scheme like example NCBB (Chechetka & Sycara, 2006): would allowsimilar reusability M-DPOP, previously computated utilities extractedcache instead recomputed. Alternatively, (b) one devise schemepreviously computed best solution saved reference, subsequently usedapproximation solving marginal problems. could possibly provide better boundsthus allow better pruning, computation could saved. alternativesoutside scope paper, considered future work.7.2.2 U SING PTAPOFAITHFUL , E FFICIENT OCIAL C HOICEOptAPO (Mailler & Lesser, 2004) popular algorithm DCOP. Similaradaptations DPOP ADOPT social choice, OptAPO also made take advantagespecial features DCOP model replicated variables. complexity wouldartificially increased use DCOP model. OptAPO particularity usesmediator agents centralize subproblems solve dynamic asynchronous mediation sessions, i.e. partial centralization. mediator agents announce resultsagents, previously sent subproblems mediators. process alone wouldintroduce additional possibility manipulation setting self interested agents. However,using VCG mechanism addresses concern agents choose behave correctly according protocol.ADOPT, main issue using OptAPO faithful social choice reusabilitycomputation main marginal problems. Specifically, consider solvingmain problem, mediator agent Ai centralized aggregated preferences numberagents, solving mediation problems dictated OptAPO protocol. Subsequently,trying compute solution marginal problem without agent Ai , computationgo waste, could manipulated Ai solving main problem. Furthermore, since OptAPOs centralization process asynchronous conflict-driven opposedstructure-driven M-DPOP, unclear whether computation main problemcould safely reused marginal problems. make matters worse, experimental studies (Davin & Modi, 2005; Petcu & Faltings, 2006) show many situations, OptAPO endsrelying single agent system centralize solve whole problem. impliessolving marginal problem without agent, one reuse zero effort mainproblem.749fiP ETCU , FALTINGS , & PARKES7.3 Loss Utility due Wasting VCG TaxesVCG mechanism, agents net utility difference utility derivesoptimal solution VCG tax pay. net utility whole group agentssum individual net utilities agents, i.e. total utility assignment valuesvariables net total payment made agents bank. loss utilityusing M-DPOP great 35% total utility optimal solution meetingscheduling domain. problem size increases, money burntform VCG taxes. Similar waste observed others; e.g., Faltings (2004), alsocontext efficient social choice.One cannot naively redistribute payment back agents, instance sharing payments equally across agents would break faithfulness. example, agent Ai would preferagents make greater payments, order receive larger repayment bank.faithfulness properties M-DPOP would unravel. hand, probleminherent structure possible redistribute fraction payments back agents.idea careful redistribution suggested Bailey (1997), subsequently extendedCavallo (2006), Guo Conitzer (2007) Moulin (2007). Another approach, advocated example Faltings (2004), simply preclude agent problem transfer paymentsagent. work centralized context.important issue future work, then, study budget surplus accrues bankM-DPOP seek mitigate welfare loss setting distributed implementation.defer discussion topic future work, investigate methodsleverage structure problem redistributing majority payments back agentswithout compromising either efficiency faithfulness.8. Conclusionsdeveloped M-DPOP, faithful, distributed algorithm solve efficientsocial choice problems multi-agent systems private information self-interest. agentimprove utility either misreporting local information deviating aspectalgorithm (e.g., computation, message-passing, information revelation.) centralizedcomponent bank able receive messages payments collect payments.addition promoting efficient decisions, minimize amount additional computationaleffort required computing VCG payments reusing effort main problem. firstset experimental results shows significant amount computation requiredmarginal problems reused main problem, sometimes 87%. providesnear-linear scalability massive, distributed social choice problems local structuremaximal induced tree width small. second set experiments performed problemswithout local structure shows problem density increases, amount effort required increases, reusability computation decreases. results suggest M-DPOPgood candidate solving loose problems exhibit local structure induced widthremains small. addition addressing need reduce total payments made agentsbank, one issue future work relates need provide robustness faced adversarialfaulty agents: current solution fragile sense, equilibrium properties relyingagents following protocol. papers (Lysyanskaya & Triandopoulos, 2006; Aiyer,Alvisi, Clement, Dahlin, Martin, & Porth, 2005; Shneidman & Parkes, 2003) provide robustness750fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSmixture models (e.g. rational, adversarial) aware workmixture models context efficient social choice. Another interesting direction find waysallow approximate social choice, example memory-limited DPOP variations (Petcu& Faltings, 2005a) retaining incentive properties, perhaps approximate equilibria. Futureresearch also consider design distributed protocols robust false-namemanipulations agents participate multiple pseudonyms (Yokoo et al., 2004),seek mitigate opportunities collusive behavior possibility multiple equilibria exist incentive mechanisms (Ausubel & Milgrom, 2006; Andelman, Feldman, &Mansour, 2007; Katz & Gordon, 2006).AcknowledgmentsParkes supported part National Science Foundation grants IIS-0238147, IIS-0534620Alfred P. Sloan Foundation award. Petcu supported Swiss National Science Foundationgrant 200020-103421/1. authors would like thank Wei Xue valuable feedback severalparts paper. thank Jeffrey Shneidman feedback early version paper.also thank Aaron Bernstein valuable insights DFS reconstruction process. threeanonymous reviewers also provided excellent suggestions improving exposition work.earlier version paper appeared Proc. Fifth International Joint ConferenceAutonomous Agents Multiagent Systems (AAMAS), 2006.ReferencesAbu-Amara, H. H. (1988). Fault-tolerant distributed algorithm election complete networks. IEEETrans. Comput., 37(4), 449453.Aiyer, A. S., Alvisi, L., Clement, A., Dahlin, M., Martin, J.-P., & Porth, C. (2005). Bar fault tolerancecooperative services. 20th ACM Symposium Operating Systems Principles.Andelman, N., Feldman, M., & Mansour, Y. (2007). Strong price anarchy. ACM-SIAM SymposiumDiscrete Algorithms 2007 (SODA07).Arnborg, S. (1985). Efficient algorithms combinatorial problems graphs bounded decomposability- survey. BIT, 25(1), 223.Ausubel, L., Cramton, P., & Milgrom, P. (2006). clock-proxy auction: practical combinatorial auctiondesign. Cramton et al. (Cramton et al., 2006), chap. 5.Ausubel, L., & Milgrom, P. (2006). lovely lonely Vickrey auction. Cramton et al. (Cramton et al.,2006), chap. 1.Bailey, M. J. (1997). demand revealing process: distribute surplus. PublicChoice, 107126.Ball, M., Donohue, G., & Hoffman, K. (2006). Auctions safe, efficient, equitable allocationairspace system resources. Cramton, Shoham, S. (Ed.), Combinatorial Auctions. MIT Press.Barbosa, V. (1996). Introduction Distributed Algorithms. MIT Press.Bayardo, R., & Miranker, D. (1995). space-time trade-off solving constraint satisfaction problems.. Proceedings 15th International Joint Conference Artificial Intelligence, IJCAI-95,Montreal, Canada.Bidyuk, B., & Dechter, R. (2004). finding minimal w-cutset. AUAI 04: Proceedings 20thconference Uncertainty artificial intelligence, pp. 4350, Arlington, Virginia, United States.AUAI Press.751fiP ETCU , FALTINGS , & PARKESBikhchandani, S., de Vries, S., Schummer, J., & Vohra, R. V. (2002). Linear programming Vickreyauctions. Dietrich, B., & Vohra, R. (Eds.), Mathematics Internet: E-Auction Markets, pp.75116. IMA Volumes Mathematics Applications, Springer-Verlag.Cavallo, R. (2006). Optimal decision-making minimal waste: Strategyproof redistribution vcg payments. Proc. 5th Int. Joint Conf. Autonomous Agents Multi Agent Systems (AAMAS06).Chechetka, A., & Sycara, K. (2006). any-space algorithm distributed constraint optimization.Proceedings AAAI Spring Symposium Distributed Plan Schedule Management.Cheng, K. C. K., & Yap, R. H. C. (2005). Constrained decision diagrams.. Proceedings NationalConference Artificial Intelligence, AAAI-05, pp. 366371, Pittsburgh, USA.Cheung, T.-Y. (1983). Graph traversal techniques maximum flow problem distributed computation..IEEE Trans. Software Eng., 9(4), 504512.Cidon, I. (1988). Yet another distributed depth-first-search algorithm. Inf. Process. Letters, 26(6), 301305.Collin, Z., Dechter, R., & Katz, S. (1991). Feasibility Distributed Constraint Satisfaction.Proceedings 12th International Joint Conference Artificial Intelligence, IJCAI-91, pp. 318324, Sidney, Australia.Collin, Z., Dechter, R., & Katz, S. (1999). Self-stabilizing distributed constraint satisfaction. Chicago JournalTheoretical Computer Science.Collin, Z., & Dolev, S. (1994). Self-stabilizing depth-first search. Information Processing Letters, 49(6),297301.Cramton, P., Shoham, Y., & Steinberg, R. (Eds.). (2006). Combinatorial Auctions. MIT Press.Davin, J., & Modi, P. J. (2005). Impact problem centralization distributed constraint optimizationalgorithms. AAMAS 05: Proceedings fourth international joint conference Autonomousagents multiagent systems, pp. 10571063, New York, NY, USA. ACM Press.Davis, R., & Smith, R. G. (1983). Negotiation metaphor distributed problem solving. ArtificialIntelligence, 63109.de Vries, S., & Vohra, R. V. (2003). Combinatorial auctions: survey. Informs Journal Computing, 15(3),284309.Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.Dechter, R., & Mateescu, R. (2006). AND/OR search spaces graphical models. Artificial Intelligence.appear.Dunne, P. E. (2005). Extremal behaviour multiagent contract negotiation. Journal Artificial IntelligenceResearch (JAIR), 23, 4178.Dunne, P. E., Wooldridge, M., & Laurence, M. (2005). complexity contract negotiation. ArtificialIntelligence Journal, 164(1-2), 2346.Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2006). Negotiating socially optimal allocations resources.Journal Artificial Intelligence Research, 25, 315348.Ephrati, E., & Rosenschein, J. (1991). Clarke tax consensus mechanism among automated agents.Proceedings National Conference Artificial Intelligence, AAAI-91, pp. 173178, Anaheim,CA.Faltings, B. (2004). budget-balanced, incentive-compatible scheme social choice. WorkshopAgent-mediated E-commerce (AMEC) VI. Springer Lecture Notes Computer Science.Faltings, B., Parkes, D., Petcu, A., & Shneidman, J. (2006). Optimizing streaming applications selfinterested users using M-DPOP. COMSOC06: International Workshop Computational SocialChoice, pp. 206219, Amsterdam, Netherlands.752fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSFeigenbaum, J., Papadimitriou, C., Sami, R., & Shenker, S. (2002). BGP-based mechanism lowest-costrouting. Proceedings 2002 ACM Symposium Principles Distributed Computing, pp.173182.Feigenbaum, J., Ramachandran, V., & Schapira, M. (2006). Incentive-compatible interdomain routing.Proceedings 7th Conference Electronic Commerce, pp. 130139.Feigenbaum, J., & Shenker, S. (2002). Distributed Algorithmic Mechanism Design: Recent ResultsFuture Directions. Proceedings 6th International Workshop Discrete AlgorithmsMethods Mobile Computing Communications, pp. 113.Freuder, E. C., & Quinn, M. J. (1985). Taking advantage stable sets variables constraint satisfactionproblems. Proceedings 9th International Joint Conference Artificial Intelligence, IJCAI85, pp. 10761078, Los Angeles, CA.Gershman, A., Meisels, A., & Zivan, R. (2006). Asynchronous forward-bounding distributed constraintsoptimization. Proceedings 17th European Conference Artificial Intelligence (ECAI-06),Riva del Garda, Italy.Greenstadt, R., Pearce, J. P., & Tambe, M. (2006). Analysis privacy loss distributed constraint optimization. Proc. Twenty-First National Conference Artificial Intelligence (AAAI-06).Guo, M., & Conitzer, V. (2007). Worst-case optimal redistribution vcg payments. Proceedings8th ACM Conference Electronic Commerce (EC-07), pp. 3039.Huebsch, R., Hellerstein, J. M., Lanham, N., et al. (2003). Querying Internet PIER. VLDB.Izmalkov, S., Micali, S., & Lepinski, M. (2005). Rational secure computation ideal mechanism design.FOCS 05: Proceedings 46th Annual IEEE Symposium Foundations Computer Science,pp. 585595, Washington, DC, USA. IEEE Computer Society.Jackson, M. O. (2000). Mechanism theory. Encyclopedia Life Support Systems. EOLSS Publishers.Jackson, M. O. (2001). crash course Implementation theory. Social Choice Welfare, 18(4), 655708.Katz, J., & Gordon, S. D. (2006). Rational secret sharing, revisited. Proc. Security CryptographyNetworks.Kloks, T. (1994). Treewidth, Computations Approximations, Vol. 842 Lecture Notes ComputerScience. Springer.Krishna, V. (2002). Auction Theory. Academic Press.Kumar, A., Petcu, A., & Faltings, B. (2007). H-DPOP: Using hard constraints prune search space.IJCAI07 - Distributed Constraint Reasoning workshop, DCR07, pp. 4055, Hyderabad, India.Lavi, R., Mualem, A., & Nisan, N. (2003). Towards characterization truthful combinatorial auctions.Proc. 44th Annual Symposium Foundations Computer Science.Leyton-Brown, K., Pearson, M., & Shoham, Y. (2000). Towards universal test suite combinatorialauction algorithms. Proceedings ACM Conference Electronic Commerce, EC-00, pp. 235245.Leyton-Brown, K., & Shoham, Y. (2006). test suite combinatorial auctions. Cramton, P., Shoham,Y., & Steinberg, R. (Eds.), Combinatorial Auctions, chap. 18. MIT Press.Lysyanskaya, A., & Triandopoulos, N. (2006). Rationality adversarial behavior multi-party computation. 26th Annual Int. Cryptology Conference (CRYPTO06).Maheswaran, R. T., Tambe, M., Bowring, E., Pearce, J. P., & Varakantham, P. (2004). Taking DCOPreal world: Efficient complete solutions distributed multi-event scheduling. AAMAS-04.Mailler, R., & Lesser, V. (2004). Solving distributed constraint optimization problems using cooperative mediation. Proceedings Third International Joint Conference Autonomous Agents MultiAgentSystems (AAMAS 2004), 1, 438445.753fiP ETCU , FALTINGS , & PARKESMailler, R., & Lesser, V. (2005). Asynchronous partial overlay: new algorithm solving distributedconstraint satisfaction problems. Journal Artificial Intelligence Research (JAIR).Mas-Colell, A., Whinston, M. D., & Green, J. R. (1995). Microeconomic Theory. Oxford University Press.Mishra, D., & Parkes, D. (2007). Ascending price Vickrey auctions general valuations. JournalEconomic Theory, 132, 335366.Modi, P. J., Shen, W.-M., Tambe, M., & Yokoo, M. (2005). ADOPT: Asynchronous distributed constraintoptimization quality guarantees. AI Journal, 161, 149180.Monderer, D., & Tennenholtz, M. (1999). Distributed games: mechanisms protocols. Proc. 16thNational Conference Artificial Intelligence (AAAI-99), pp. 3237.Moulin, H. (2007). Efficient, strategy-proof almost budget-balanced assignment. Tech. rep., Rice University.Mualem, A. (2005). decentralized incentive compatible mechanisms partially informed environments.Proc. ACM Conf. Electronic Commerce (EC).Ostrovsky, R., Rajagopalan, S., & Vazirani, U. (1994). Simple efficient leader election full information model. STOC 94: Proceedings twenty-sixth annual ACM symposium Theorycomputing, pp. 234242, New York, NY, USA. ACM Press.Parkes, D. C., Kalagnanam, J. R., & Eso, M. (2001). Achieving budget-balance Vickrey-based paymentschemes exchanges. Proc. 17th International Joint Conference Artificial Intelligence (IJCAI01), pp. 11611168.Parkes, D. C., & Shneidman, J. (2004). Distributed implementations Vickrey-Clarke-Groves mechanisms.Proc. 3rd Int. Joint Conf. Autonomous Agents Multi Agent Systems, pp. 261268.Parkes, D. C., & Ungar, L. H. (2000). Iterative combinatorial auctions: Theory practice. Proc. 17thNational Conference Artificial Intelligence (AAAI-00), pp. 7481.Petcu, A. (2006). FRODO: FRamework Open Distributed constraint Optimization. Technicalreport no. 2006/001, Swiss Federal Institute Technology (EPFL), Lausanne, Switzerland. http://liawww.epfl.ch/frodo/.Petcu, A., & Faltings, B. (2005a). A-DPOP: Approximations distributed optimization. ProceedingsEleventh International Conference Principles Practice Constraint Programming(CP05), pp. 802806, Sitges, Spain.Petcu, A., & Faltings, B. (2005b). DPOP: scalable method multiagent constraint optimization.Proceedings 19th International Joint Conference Artificial Intelligence, IJCAI-05, pp. 266271, Edinburgh, Scotland.Petcu, A., & Faltings, B. (2006). PC-DPOP: partial centralization extension DPOP. ProceedingsSecond International Workshop Distributed Constraint Satisfaction Problems, ECAI06, Rivadel Garda, Italy.Petcu, A., & Faltings, B. (2007). MB-DPOP: new memory-bounded algorithm distributed optimization. Proceedings 20th International Joint Conference Artificial Intelligence, IJCAI-07,Hyderabad, India.Pietzuch, P., Ledlie, J., Shneidman, J., Roussopoulos, M., Welsh, M., & Seltzer, M. (2006). Network-AwareOperator Placement Stream-Processing Systems. ICDE.Rassenti, S. J., Smith, V. L., & Bulfin, R. L. (1982). combinatorial mechanism airport time slot allocation. Bell Journal Economics, 13, 402417.Roberts, K. (1979). characterization implementable rules. Laffont, J.-J. (Ed.), AggregationRevelation Preferences, pp. 321348. North-Holland, Amsterdam.Rosenschein, J. S., & Zlotkin, G. (1994). Designing conventions automated negotiation. AI Magazine.Fall.754fiM-DPOP: FAITHFUL ISTRIBUTED MPLEMENTATION E FFICIENT OCIAL C HOICE P ROBLEMSSandholm, T. (2002). Algorithm optimal winner determination combinatorial auctions. ArtificialIntelligence, 135, 154.Sandholm, T. W. (1993). implementation Contract Net Protocol based marginal-cost calculations. Proc. 11th National Conference Artificial Intelligence (AAAI-93), pp. 256262.Sandholm, T. W. (1996). Limitations Vickrey auction computational multiagent systems. SecondInternational Conference Multiagent Systems (ICMAS-96), pp. 299306.Shneidman, J., & Parkes, D. C. (2003). Rationality self-interest peer peer networks. 2nd Int.Workshop Peer-to-Peer Systems (IPTPS03).Shneidman, J., & Parkes, D. C. (2004). Specification faithfulness networks rational nodes. Proc.23rd ACM Symp. Principles Distributed Computing (PODC04), St. Johns, Canada.Silaghi, M.-C., Sam-Haroud, D., & Faltings, B. (2000). Asynchronous search aggregations.AAAI/IAAI, pp. 917922, Austin, Texas.Solotorevsky, G., Gudes, E., & Meisels, A. (1996). Modeling Solving Distributed Constraint SatisfactionProblems (DCSPs). Proceedings Second International Conference Principles PracticeConstraint Programming (CP96), pp. 561562, Cambridge, Massachusetts, USA.Sycara, K., Roth, S. F., Sadeh-Koniecpol, N., & Fox, M. S. (1991). Distributed constrained heuristic search.IEEE Transactions Systems, Man, Cybernetics, 21(6), 14461461.Wellman, M. P. (1993). market-oriented programming environment application distributed multicommodity flow problems. Journal Artificial Intelligence Research, 1, 123.Wellman, M. P. (1996). Market-oriented programming: early lessons. Clearwater, S. H. (Ed.),Market-Based Control: Paradigm Distributed Resource Allocation, chap. 4, pp. 7495. WorldScientific.Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1992). Distributed constraint satisfaction formalizing distributed problem solving. International Conference Distributed Computing Systems, pp.614621.Yokoo, M., & Hirayama, K. (2000). Algorithms distributed constraint satisfaction: review. AutonomousAgents Multi-Agent Systems, 3(2), 185207.Yokoo, M., Sakurai, Y., & Matsubara, S. (2004). effect false-name bids combinatorial auctions:New Fraud Internet Auctions. Games Economic Behavior, 46(1), 174188.Zhang, W., & Wittenburg, L. (2003). Distributed breakout algorithm distributed constraint optimizationproblems - DBArelax. Proceedings International Joint Conference Autonomous AgentsMulti Agent Systems (AAMAS-03), Melbourne, Australia.Zlotkin, G., & Rosenschein, J. S. (1996). Mechanisms automated negotiation state oriented domains.Journal Artificial Intelligence Research, 5, 163238.755fiJournal Artificial Intelligence Research 32 (2008) 879-900Submitted 01/08; published 08/08Latent Tree ModelsApproximate Inference Bayesian NetworksYi WangNevin L. ZhangTao Chenwangyi@cse.ust.hklzhang@cse.ust.hkcsct@cse.ust.hkDepartment Computer Science EngineeringHong Kong University Science TechnologyClear Water Bay Road, Kowloon, Hong Kong, ChinaAbstractpropose novel method approximate inference Bayesian networks (BNs).idea sample data BN, learn latent tree model (LTM) dataoine, online, make inference LTM instead original BN.LTMs tree-structured, inference takes linear time. meantime, representcomplex relationship among leaf nodes hence approximation accuracy often good.Empirical evidence shows method achieve good approximation accuracylow online computational cost.1. IntroductionLatent tree models (LTMs) tree-structured Bayesian networks leaf nodes representmanifest variables observed, internal nodes represent latent variableshidden. previously known hierarchical latent class models (Zhang, 2004).paper, distinguish variables nodes, assume variablescategorical.Pearl (1988) rst identify LTMs potentially useful class models.two reasons. First, inference LTMs takes time linear number nodes,intractable general BNs. Second, latent variables capture complex relationshipsamong manifest variables. LTM, manifest variables mutually independentgiven latent variables, eliminating latent variables results completelyconnected BN.study possibility exploiting two properties approximate inferenceBNs. natural idea:1. Oine: Obtain LTM approximates BN N sense jointdistribution manifest variables approximately equals joint distributionvariables N .2. Online: Use instead N compute answers probabilistic queries.cardinalities latent variables play crucial role approximation scheme.determine inferential complexity inuence approximation accuracy. one extreme, represent BN exactly using LTM setting cardinalities latentvariables large enough. case, inferential complexity high.c2008AI Access Foundation. rights reserved.fiWang, Zhang, & Chenextreme, set cardinalities latent variables 1. case, manifestvariables become mutually independent. inferential complexity lowestapproximation quality poorest. seek appropriate middle pointtwo extremes.assume predetermined constraint cardinalities latentvariables control inferential complexity. develop algorithm nding LTMsatises constraint approximates original BN well. idea sampledata BN, learn LTM data. model structure determinedusing hierarchical clustering manifest variables. step, two closely correlated setsmanifest variables grouped, new latent variable introduced accountrelationship them. cardinalities latent variables setpredetermined value. parameters optimized using Expectation-Maximization(EM) algorithm (Dempster, Laird, & Rubin, 1977).empirically evaluated inference method array networks. possibility tradeo inferential complexity approximation accuracydemonstrated adjusting cardinality constraints. turns methodable achieve good approximation accuracy cardinality becomes high.compared method loopy belief propagation (LBP) (Pearl, 1988), standard approximate inference method successfully used many real world domains(Frey & MacKay, 1997; Murphy, Weiss, & Jordan, 1999). Given amount time,method achieves signicantly higher accuracy LBP cases. achieveaccuracy, LBP needs one three orders magnitude time method.inference method fast LTM tree-structured. One also constructChow-Liu tree (Chow & Liu, 1968) approximate original BN use inference.refer approach CL-based method. comparison method, CLbased method always faster, accurate method.scheme exploits strong expressive capability latent variable models. Onecourse use latent variable models instead LTMs scheme. straightforwardchoice latent class model (LCM) (Hagenaars & McCutcheon, 2002). LCMLTM one latent variable1 . assumes local independence, is, manifestvariables mutually independent conditioning latent variable. also comparemethod alternative. results show that, inferential complexityconstraints, method accurate LCM-based method.noted approximate scheme needs lot time oine phase.EM usually takes long time converge. Moreover, time complexity EM scales linearly sample size, set large possibleachieve high-quality approximation. Therefore, method suitable applicationsallow long oine phase.remainder paper organized follows. Section 2, review LTMs.Section 3, describe method constructing LTMs approximate BNs. Section 4,describe scheme approximate inference formally. Section 5 reports empirical results. Section 6 discusses relationship approach existing work. Finally,Section 7, conclude paper point future directions.1. machine learning community, LCM also referred naive Bayes model latent variable.880fiLatent Tree Models Approximate Inference Bayesian Networks2. Latent Tree ModelLTM pair = (m, ). rst component denotes rooted tree setcardinalities latent variables. refer model, rooted treemodel structure. second component denotes collection parametersM. contains conditional probability table node given parent.Let X set manifest variables set latent variables M,respectively. use P (X, Y|m, ), PM (X, Y) short, denote joint distributionrepresented M. Two LTMs marginally equivalent shareset manifest variables X PM (X) = PM (X). model includes another modelexists (m, ) (m , ) marginally equivalent.Two models marginally equivalent includes vice versa.Let |Z| denote cardinality variable Z. node Z m, use nb(Z)denote set neighbors. model regular latent node ,1. two neighbors, least one neighbors must latent nodeZnb(Y ) |Z|.|Y | <maxZnb(Y ) |Z|2. two neighbors,|Y |Znb(Y ) |Z|maxZnb(Y ) |Z|.model irregular, over-complicated. reduced regular modelmarginally equivalent contains fewer parameters(Zhang, 2004).Q|Z|regular model, latent node saturated |Y | = maxZnb(Y ) |Z| . case, sayZnb(Y )subsumes neighbors except one largest cardinality.3. Approximating Bayesian Networks Latent Tree Modelssection, study problem approximating BN LTM. Let NBN approximated. Let X set variables N . LTMapproximation N , use X manifest variables, cardinalitieslatent variables exceed predetermined threshold C. Figure 1(b), 1(c), 1(d)show three example LTMs approximate BN Figure 1(a). usedillustrate various steps method.Let PN (X) joint distribution represented N . approximation highquality PM (X) close PN (X). measure quality approximationKL divergence (Cover & Thomas, 1991)D[PN (X)PM (X)] =XPN (X) logPN (X).PM (X)objective nd LTM minimizes KL divergence, i.e.,= arg min D[PN (X)PM (X)].881fiWang, Zhang, & ChenX1 (2)X2 (2)Y5 (8)Y4 (8)X3 (2)X4 (2)Y2 (8)Y1 (8)X5 (2)X1 (2)X6 (2)X4 (2)(a) Bayesian network NY3 (8)X5 (2)X2 (2)X3 (2)X6 (2)(b) Latent tree modelY4 (8)Y2 (8)Y1 (4)X4 (2)X1 (2)X5 (2)Y2 (8)Y3 (4)X2 (2)Y4 (8)X3 (2)X4 (2)X6 (2)(c) Regularized modelX5 (2)X6 (2)X1 (2)X2 (2)X3 (2)(d) Simplified modelFigure 1: illustrative example. numbers within parentheses cardinalitiesvariables.LTM consists two components, model parameters . Therefore,optimization problem naturally decomposed two subproblems.1. Find optimal model .2. Optimize parameters given model m.remainder section, discuss two subproblems details.3.1 Parameter Optimizationstart addressing second subproblem. Given model m, target nd= arg min D[PN (X)P (X|m, )].882fiLatent Tree Models Approximate Inference Bayesian Networksturns that, due presence latent variables, KL divergence dicultdirectly minimize. seen expanding KL divergence follows,PN (X)P (X|m, )X=PN (X) log PN (X)PN (X) log P (X|m, )D[PN (X)P (X|m, )] =PN (X) logX=XPN (X) log PN (X)XPN (X) logXP (X, Y|m, ).rst term last line neglected independent .diculty lies maximizing second term. summation latent variablesappearing inside logarithm makes term indecomposable. Therefore, closed-formsolution obtained taking derivative term respectsetting zero.transform problem asymptotically equivalent maximum likelihood estimation (MLE) problem. idea follows.1. Generate data set N independently identically distributed samplesPN (X).2. Find MLE respect D, i.e.,= arg max P (D|m, ).well known converges almost surely sample size N approachesinnity (Huber, 1967).discuss implementation solution. start generatingPN (X). Since PN (X) represented BN N , use logic sampling (Henrion, 1988)task. Specically, generate piece sample PN (X), process nodestopological ordering2 . handling node X, sample value accordingconditional distribution P (X|(X) = j), (X) denotes set parents X jdenote values sampled earlier. obtain D, repeat procedureN times.Given D, next step nd MLE. Note values latent variablesmissing D. thus use EM algorithm (Dempster et al., 1977). Startingrandom guess, EM algorithm iteratively improves estimate changeloglikelihoods two consecutive iterations smaller predetermined threshold.practical issue EM converge local maxima likelihood surface.local maxima far global maxima, thus poor approximations. Fortunately, local maxima issue severe LTMs (Wang & Zhang, 2006).practice, one also use various techniques multiple restart (Chickering &Heckerman, 1997) data permutation (Elidan et al., 2002) alleviate issue.Note EM takes long time converge, especially sample size N large.algorithm expensive oine phase.2. topological ordering sorts nodes DAG node always precedes children.883fiWang, Zhang, & Chen3.2 Exhaustive Search Optimal Modelconsider rst subproblem, i.e., nd best model . straightforwardway solve problem exhaust possible models, nd optimal parametersmodel m, compute KL divergence D[PN (X)P (X|m, )], returnmodel minimum KL divergence.problem solution high computational complexity. Given setmanifest variable X, innitely many models. One always obtain new modelsinserting latent variables existing model. show Section 3.5,sucient consider nite subspace, i.e., subspace regular models. However,still super-exponentially many regular models (Zhang, 2004). model, needoptimize parameters running EM, time-consuming process. Therefore,exhaustive search computationally infeasible. following 4 subsections,present heuristic method.3.3 Heuristic Construction Model Structurerst present heuristic determining model structure. LTM, two manifestvariables called siblings share parent. heuristic based twoideas: (1) LTM M, siblings generally closely correlated variableslocated far apart; (2) good approximation N , two variables XiXj closely correlated closely correlated N .examine pair variables N , pick two variables closely correlated,introduce latent variable parent M.measure strength correlation pair variables Xi Xjmutual information (Cover & Thomas, 1991)(Xi ; Xj ) =PN (Xi , Xj ) logXi ,XjPN (Xi , Xj ).PN (Xi )PN (Xj )compute (Xi ; Xj ), one need make inference N . could computationallyhard rst place. use sampling technique address issue. Specically,generate data set N samples BN N , compute empirical mutual; Xj ) using empirical distribution P (Xi , Xj ) based D. stronginformation I(X; Xj ) almost surely converge (Xi ; Xj ) samplelaw large numbers, I(Xsize N goes innity.use BN shown Figure 1(a) example illustrate idea.contains 6 binary variables X1 , X2 , . . ., X6 . Suppose empirical mutual informationbased data set presented Table 1. discussed above, regardapproximation mutual information variables N hence regardapproximation mutual information variables nal LTMconstruct. nd X4 X6 pair largest mutual information.Therefore, create latent variable Y1 make parent X4 X6 .next step nd, among Y1 , X1 , X2 , X3 , X5 , pair variableslargest mutual information M. one diculty: Y1 original Bayesiannetwork hence observed data set. mutual information Y1884fiLatent Tree Models Approximate Inference Bayesian NetworksX1X2X3X4X5X6X10.00000.00030.00150.00170.0102X20.09710.06540.03110.0252X30.01960.00860.0080X40.12640.1817X50.0486Table 1: Empirical mutual information manifest variablesvariables cannot computed directly. hence seek approximation.nal model M, Y1 would d-separate X4 X6 variables. Therefore,X {X1 , X2 , X3 , X5 },IM (Y1 ; X) IM (X4 ; X), IM (Y1 ; X) IM (X6 ; X).hence approximate IM (Y1 ; X) using lower boundmax{IM (X4 ; X), IM (X6 ; X)}.Back running example, estimated mutual information Y1 X1 ,X2 , X3 , X5 presented Table 2. see next pair pick Y1 X5 .introduce latent variable Y2 parent Y1 X5 . process continues. nalmodel structure binary tree shown Figure 1(b).Y1X10.0102X20.0654X30.0196X50.1264Table 2: Estimated mutual information Y1 manifest variables3.4 Cardinalities Latent Variablesobtaining model structure, next step determine cardinalitieslatent variables. set cardinalities latent variables predetermined valueC. following, discuss choice C inuences quality approximationinferential eciency.rst discuss impact value Capproximation quality. startconsidering case C equals Cmax = XX |X|, i.e., productcardinalities manifest variables. case, latent variable viewedjoint variable manifest variables. therefore set parametersP (X|m, ) = PN (X). is, capture interactions among manifestvariables.happens decrease C? shown approximation qualitydegrade. Let model obtained value C another model obtainedsmaller value C . easy see includes . following lemma statesapproximation quality better m.885fiWang, Zhang, & ChenLemma 1 Let P (X) joint probability distribution X. Let two modelsmanifest variables X. includes ,min D[P (X)P (X|m, )] min D[P (X)P (X|m , )].Proof: Dene= arg min D[P (X)P (X|m , )].includes , must parametersP (X|m, ) = P (X|m , ).Therefore,min D[P (X)P (X|m, )] D[P (X)P (X|m, )]= D[P (X)P (X|m , )]= min D[P (X)P (X|m , )]Q.E.D.mentioned earlier, C large enough, model capture interactionsamong manifest variables hence represent joint distribution PN (X) exactly.C large enough, represent PN (X) approximately. Accordingprevious discussion, C decreases, approximation accuracy (in terms KL divergence)gradually degrade, indicating model capture less less interactions amongmanifest variables. worst case occurs C = 1. case, interactionslost. approximation accuracy poorest.parameter C also determines computational cost making inference m.use clique tree propagation (CTP) algorithm inference. measure costinferential complexity, dened sum clique sizes cliquetree m. given|X| C.(1)(|X| 2) C 2 +XXNote |X| number manifest variables, |X| cardinality manifestvariable X. Therefore, one control inferential complexity changing valueC. smaller value C, lower complexity.summary, one achieve tradeo approximation qualityinferential complexity resultant model tuning parameter C. Figure 1(b),set C = 8.3.5 Model RegularizationSuppose obtained model using technique described Section 3.3setting cardinalities latent variables certain value. following twosubsections, show sometimes possible simplify without compromisingapproximation quality.886fiLatent Tree Models Approximate Inference Bayesian Networksrst notice could irregular. example, let us consider modelFigure 1(b). constructed approximation BN N Figure 1(a)C = 8. checking latent variables, nd Y5 violates regularity condition. two neighbors |Y5 | |X1 ||Y4 |/ max{|X1 |, |Y4 |}. Y1 Y3 alsoviolate regularity condition |Y1 | > |X4 ||X6 ||Y2 |/ max{|X4 |, |X6 |, |Y2 |}|Y3 | > |X2 ||X3 ||Y4 |/ max{|X2 |, |X3 |, |Y4 |}. following proposition suggests irregular models always simplied become regular.Proposition 1 irregular model, must exists model lowerinferential complexitymin D[PN (X)P (X|m, )] = min D[PN (X)P (X|m , )].(2)Proof: Let latent variable violates regularity condition. Denoteneighbors Z1 , Z2 , . . . , Zk . dene another model follows:1. two neighbors, remove connect Z1 Z2 .2. Otherwise, replace saturated latent variable , i.e.,ki=1 |Zi ||Y | =.maxki=1 |Zi |shown Zhang (2004), parameters m, exists parameters(m, ) (m , ) marginally equivalent. reverse also true.Therefore, marginally equivalent. Equation 2 thus follows Lemma 1.show inferential complexity lower m, compareclique trees . Consider aforementioned two cases:1. two neighbors. case, cliques {Y, Z1 } {Y, Z2 } clique treereplaced {Z1 , Z2 } clique tree . Assume |Z2 | |Z1 |.dierence sum clique sizessum(m) sum(m ) = |Y ||Z1 | + |Y ||Z2 | |Z1 ||Z2 ||Z1 ||Z1 | + |Z1 ||Z2 | |Z1 ||Z2 |= |Z1 ||Z1 |> 0.2. two neighbors. case, = 1, 2, . . . , k, clique {Y, Zi }clique tree replaced smaller clique {Y , Zi } clique tree .cases, inferential complexity lower m.Q.E.D.proof Proposition 1 presents way handle latent variable violatesregularity condition, i.e., either eliminating decreasing cardinality. regularizeirregular model, handle latent variables order created887fiWang, Zhang, & ChenSection 3.3. following, use irregular model Figure 1(b) demonstrateregularization process.begin latent variable Y1 . three neighbors violates regularitycondition. decrease cardinality |X4 ||X6 ||Y2 |/ max{|X4 |, |X6 |, |Y2 |} = 4.consider Y2 . satises regularity condition hence changes made.next latent variable examine Y3 . violates regularity condition. decreasecardinality |X2 ||X3 ||Y4 |/ max{|X2 |, |X3 |, |Y4 |} = 4. change Y4cause irregularity. last, remove Y5 , two neighborsviolates regularity condition, connect Y4 X1 . end regularmodel shown Figure 1(c).3.6 Simplificationsregularization, sometimes still opportunities model simplication.Take model Figure 1(c) example. contains two adjacent latent variablesY1 Y2 . variables saturated. Y1 subsumes X4 X6 , Y2 subsumes Y1X5 . Y2 viewed joint variable Y1 X5 , Y1 turn viewedjoint variable X4 X6 . Intuitively, eliminate Y1 directly make Y2 jointvariable X4 , X5 , X6 . intuition formalized following proposition.Proposition 2 Let model one latent node. Let Y1 Y2 twoadjacent latent nodes. Y1 Y2 saturated Y2 subsumes Y1 , existanother model marginally equivalent lower inferential complexitym. Therefore,min D[PN (X)P (X|m, )] = min D[PN (X)P (X|m , )].Proof: enumerate neighbors Y1 Y2 , Z11 , Z12 , . . . , Z1k , neighborsY2 Y1 , Z21 , Z22 , . . . , Z2l . Dene another model removing Y1 connecting Z11 , Z12 , . . . , Z1k Y2 . See Figure 2. prove marginallyequivalent, inferential complexity lower m.start proving marginal equivalence. technical convenience, workunrooted models. unrooted model model directions edgesdropped. Parameters unrooted model include potential edge model.potential non-negative function two variables connected edge.concept marginal equivalence dened way rooted models.shown Zhang (2004), model marginally equivalent unrooted version.Therefore, prove marginal equivalence , sucient showunrooted versions marginally equivalent. simplicity, abusedenote unrooted models. also use f () denote potential ,g() denote potential .Note Y1 Y2 saturated, Y2 subsumes Y1 . variablesless two states, implies that:1. Y1 subsumes Z11 , Z12 , . . . , Z1k .2. Suppose |Z2l | = maxlj=1 |Z2j |. Y2 subsumes Z21 , Z22 , . . . , Z2l1 .888fiLatent Tree Models Approximate Inference Bayesian NetworksZ11Z12Z11Z22Z12Z21Z22Y2......Z1k...Y2...Y1Z21Z1kZ2lZ2l(a)(b)Figure 2: simplication. (a) part model contains two adjacentsaturated latent nodes Y1 Y2 , Y2 subsuming Y1 . (b) Simplied modelY1 eliminated.Therefore, state Y1 written y1 =< z11 , z12 , . . . , z1k >, state Y2written y2 =< y1 , z21 , z22 , . . . , z2l1 >. latter expandedy2 =< z11 , z12 , . . . , z1k , z21 , z22 , . . . , z2l1 >.rst show includes m. Let parameters m. dene parametersfollows:Potential edge Y2 Z2l :g(Y2 =< z11 , z12 , . . . , z1k , z21 , z22 , . . . , z2l1 >, Z2l = z2l )=f (Y1 , Y2 )kf (Y1 , Z1i = z1i )i=1Y1 ,Y2lf (Y2 , Z2j = z2j ).j=1Potential edge Y2 Z1i , = 1, 2, . . . , k:g(Y2 =< z11 , z12 , . . . , z1k , z21 , z22 , . . . , z2l1 >, Z1i =z1i)=1 z1i = z1i0 otherwisePotential edge Y2 Z2j , j = 1, 2, . . . , l 1:g(Y2 =< z11 , z12 , . . . , z1k , z21 , z22 , . . . , z2l1 >, Z2j =z2j)=1 z2j = z2j0 otherwiseSet potentials .easy verifyY1 ,Y2Therefore,f (Y1 , Y2 )ki=1f (Y1 , Z1i )lf (Y2 , Z2j ) =j=1kY2g(Y2 , Z1i )i=1P (X|m, ) = P (X|m , ).889lg(Y2 , Z2j ).(3)j=1(4)fiWang, Zhang, & ChenNext, prove includes . Given parameters , dene parametersfollows:Potential edge Y1 Y2 :f (Y1 =< z11 , z12 , . . . , z1k >, Y2 = y2 ) =kg(Y2 = y2 , Z1i = z1i ).i=1Potential edge Y1 Z1i , = 1, 2, . . . , k:f (Y1 =< z11 , z12 , . . . , z1k >, Z1i =z1i)=1 z1i = z1i0 otherwiseSet potentials .veried Equation 3 4 also hold. Therefore, marginallyequivalent.compare inferential complexity . According construction, clique tree dierent clique tree contains one lessclique {Y1 , Y2 } replaces clique {Y1 , Z1i } {Y2 , Z1i } = 1, 2, . . . , k. Therefore,dierence sum clique sizes|Y1 ||Z1i ||Y2 ||Z1i |sum(m) sum(m ) = |Y1 ||Y2 | += |Y2 ||Z1i | +|Y1 ||Z1i ||Y2 ||Z1i ||Z1i |) +|Y1 ||Z1i |.= |Y2 |( |Z1i |rst term last line non-negative |Z1i | |Z1i | |Z1i | 2= 1, 2, . . . , k. Therefore, inferential complexity always lowerZ1i nontrivial.Q.E.D.Given regularized model, check pair adjacent latent variables applyProposition 2 eliminate redundant latent variables. use model Figure 1(c)example demonstrate process. rst pair check Y1 Y2 .saturated Y2 subsumes Y1 . thus remove Y1 connect Y2 X4X6 . check Y3 Y4 . turns Y3 redundant. Therefore, removeconnect Y4 X2 X3 . last pair check Y2 Y4 .saturated, neither subsumes other. Hence, cannot removed.nal model shown Figure 1(d).3.7 Algorithm LTABsummarize, outlined algorithm approximating BNs using LTMs. callalgorithm LTAB, shorthand Latent Tree Approximation Bayesian network.3 inputs: BN N , predetermined cardinality C latent variables, sample890fiLatent Tree Models Approximate Inference Bayesian Networkssize N . output LTAB LTM approximates PN (X), joint probabilitydistribution represented N . LTAB briey described follows.1. Generate data set N i.i.d. samples PN (X). (Section 3.1)2. Obtain LTM structure performing hierarchical clustering variables, usingempirical mutual information based similarity measure. (Section 3.3)3. Set cardinalities latent variables C simplify model. (Section 3.4 3.6)4. Optimize parameters running EM. (Section 3.1)5. Return resultant LTM.4. LTM-based Approximate Inferencefocus paper approximate inference Bayesian networks. proposefollowing two-phase method:1. Oine: Given BN N , use LTAB construct approximation M. sample sizeN set large possible, cardinality C determinedmeet requirement inferential complexity.2. Online: Make inference instead N . specically, given piece evidence E = e querying variable Q, return PM (Q|E = e) approximationPN (Q|E = e).5. Empirical Resultssection, empirically evaluate approximate inference method. rst examineimpact sample size N cardinality C performance method.compare method CTP, LBP, CL-based method, LCM-basedmethod.used 8 networks experiments. listed Table 3. CPCS54subset CPCS network (Pradhan et al., 1994). networks availablehttp://www.cs.huji.ac.il/labs/compbio/Repository/. Table 3 also reports characteristics networks, including number nodes, average/max indegreecardinality nodes, inferential complexity (i.e., sum clique sizesclique tree). networks sorted ascending order respect inferentialcomplexity.network, simulated 500 pieces evidence. piece evidence setleaf nodes sampling based joint probability distribution.used CTP algorithm approximate inference methods compute posteriordistribution non-leaf node conditioned piece evidence. accuracyapproximate method measured average KL divergence exactapproximate posterior distributions query nodes evidence.algorithms experiments implemented Java run machineIntel Pentium IV 3.2GHz CPU 1GB RAM.891fiWang, Zhang, & ChenNetworkALARMWIN95PTSHAILFINDERINSURANCECPCS54WATERMILDEWBARLEYNumberNodes3776562754323548Average/MaxIndegree1.24/41.47/71.18/41.93/32/92.06/51.31/31.75/4Average/MaxCardinality2.84/42/23.98/113.3/52/23.62/417.6/1008.77/67InferentialComplexity1,0382,6849,70629,352109,2083,028,3053,400,46417,140,796Table 3: Networks characteristics.5.1 Impact N Cdiscussed impact N C performance method Section 3.subsection empirically veries claims.Three sample sizes chosen experiments: 1k, 10k, 100k. network,also chose set C. LTMs learned using LTAB dierent combinationvalues N C. parameter learning, terminated EM eitherimprovement loglikelihoods smaller 0.1, algorithm ran two months.multiple restarting strategy Chickering Heckerman (1997) used avoidlocal maxima. number starting points set 16.running time LTAB plotted Figure 3. y-axes denote time hours,x-axes denote parameter C LTAB. three curves correspond dierentvalues N . general, running time increases N C, ranging secondsweeks. settings, EM failed converge two months. settings indicatedarrows plots. emphasize LTAB executed oine running timeconfused time online inference, reported next.obtaining LTMs, used clique tree propagation make inference.approximation accuracy shown Figure 4. y-axes denote average KL divergence, x-axes still denote parameter C LTAB. curves onehorizontal line plot. three curves labeled LTM method,correspond three sample sizes used. remaining two curves horizontalline approximate inference methods. discuss Sections 5.35.5.rst examine impact sample size comparing corresponding curvesplot. nd that, general, curves larger samples locatedsmaller ones. shows approximation accuracy increases sizetraining data.see impact C, examine individual curve left right. Accordingdiscussion, curve expected drop monotonically C increases.generally true results sample size 100k. sample sizes 1k 10k, however,cases approximation becomes poorer C increases. See Figure4(e) 4(f). phenomenon conict claims. C increases,892fiLatent Tree Models Approximate Inference Bayesian NetworksTime (hour)10101010102210N=1kN=10kN=100k1110Time (hour)1001201011021033124108124CC(a) ALARM1010816(b) WIN95PTS43102101010Time (hour)Time (hour)10101102431416103214C(c) HAILFINDERTime (hour)10101010441022100221041010424C8101614101010641664(f) WATER44102210Time (hour)Time (hour)1016C(e) CPCS541032(d) INSURANCETime (hour)1016C022104101044161064C14C(g) MILDEW(h) BARLEYFigure 3: Running time LTAB. Settings EM converge indicatedarrows.893fiWang, Zhang, & Chen1010010Average KL divergenceAverage KL divergence1012LTM (1k)LTM (10k)LTM (100k)LBPCL (100k)LCM (100k)124101010801231C(a) ALARM10101010010123414C416101010321231402Average KL divergenceAverage KL divergence10345124C8101010101612341410641664(f) WATER010Average KL divergenceAverage KL divergence1016C(e) CPCS541032(d) INSURANCE101016C(c) HAILFINDER10160C108(b) WIN95PTSAverage KL divergenceAverage KL divergence1021100121416106421C4C(g) MILDEW(h) BARLEYFigure 4: Approximation accuracy.894fiLatent Tree Models Approximate Inference Bayesian Networksexpressive power learned LTM increases. tends overt data.hand, empirical distribution small set data may signicantly deviatejoint distribution BN. also suggests sample size setlarge possible.Finally, let us examine impact N C inferential complexity. Figure 5plots running time dierent methods answer queries. now,consider three curves labeled LTM. seen three curvesoverlap plots. implies running time independent sample sizeN . hand, curves monotonically increasing. conrms claiminferential complexity positively dependent C.following subsections, stated explicitly otherwise, considerresults N = 100k largest C. settings, method achieveshighest accuracy.5.2 Comparison CTPcompare method CTP, state-of-the-art exact inference algorithm.rst concern that, accurate method. examining Figure 4, arguemethod always achieves good approximation accuracy: HAILFINDER, CPCS54, WATER,average KL divergence method around less 103 ; networks,average KL divergence around less 102 .next compare inferential eciency method CTP algorithm.running time CTP denoted dashed horizontal lines plots Figure 5.seen method ecient CTP algorithm. particular,networks highest inferential complexity, method faster CTPtwo three orders magnitude.summarize, results suggest method achieve good approximationaccuracy low computational cost.5.3 Comparison LBPcompare method LBP. latter iterative algorithm. usedanytime inference method running specic number iterations. rst setexperiments, let LBP run long method compare approximationaccuracy. network value C. accuracy LBPdenoted curves labeled LBP Figure 4. comparing curves LTMcurves N = 100k, see method achieves signicantly higher accuracyLBP cases: WATER, dierence average KL divergence three ordersmagnitude; networks, dierence one order magnitude.HAILFINDER C = 32, LBP two times accurate method. However,method also achieves good approximation accuracy case. average KL divergencesmaller 103 . Finally, noticed LBP curves horizontal lines CPCS54,MILDEW, BARLEY. investigation cases shows LBP nished oneiteration given time period.next examine much time takes LBP achieve level accuracymethod. piece evidence, ran LBP average KL divergence895fiWang, Zhang, & Chen1010110LTM (1k)LTM (10k)LTM (100k)CTPLBPCL (100k)LCM (100k)0Time (second)Time (second)101010210112410811C(a) ALARM102101Time (second)Time (second)100101010101144C161032210121431042Time (second)Time (second)10101010201124C810162145103Time (second)Time (second)101101010114641664(f) WATER101016C(e) CPCS541032(d) INSURANCE101016C(c) HAILFINDER10163C108(b) WIN95PTS10102161064C6420214C(g) MILDEW(h) BARLEYFigure 5: Running time online inference.896fiLatent Tree Models Approximate Inference Bayesian Networkscomparable method number iterations exceeds 100.running time LBP denoted curves labeled LBP Figure 5. Comparingcurves LTM curves, found LBP takes much timemethod: MILDEW, LBP slower method three orders magnitude;networks except HAILFINDER, LBP slower one two orders magnitude;HAILFINDER C = 32, running time two methods similar. resultsshow method compares favorably LBP networks examined.5.4 Comparison CL-based Methodsubsection, compare method CL-based method. specically,network, learn tree model 100k samples using maximum spanningtree algorithm developed Chow Liu (1968). use learned tree modelanswer queries.approximation accuracy CL-based method shown solid horizontal linesplots Figure 4. Comparing CL-based method, method achieves higheraccuracy networks except MILDEW. INSURANCE, WATER, BARLEY, differences signicant. MILDEW, method competitive CL-based method.meantime, notice CL-based method achieves good approximationsnetworks except BARLEY. average KL divergence around less 102 .obvious advantage CL-based method high eciency. seenplots Figure 5. plots, CL line locates second data pointLTM curve. exception MILDEW, running time CL-basedmethod long method C = 16.summary, results suggest CL-based method good choice approximate inference online inference time limited. Otherwise, methodattractive able produce accurate results time allowed.5.5 Comparison LCM-based MethodLowd Domingos (2005) previously investigated use LCM density estimation. Given data set, determine cardinality latent variable usinghold-out validation, optimize parameters using EM. shown learnedLCM achieves good model separate testing set. LCM also used answersimulated probabilistic queries results turn good.Inspired work, also learned set LCMs 100k samplescompared LTMs approximate inference task. learning strategyslightly dierent. Since LCM special case LTM, inferential complexity alsocontrolled changing cardinality latent variable. experiments, setcardinality sum clique sizes clique tree LCM roughlyLTM learned chosen C. way, inferential complexitytwo models comparable. veried examining LCM curvesFigure 5. optimize parameters LCM using EM settingcase LTM.shown Figure 4, ALARM, WIN95PTS, CPCS54, WATER, BARLEY, LCMcurves located LTM curves. is, method consistently outperforms897fiWang, Zhang, & ChenLCM-based method C. HAILFINDER MILDEW, method worseLCM-based method C small. C becomes large, method beginswin. INSURANCE, performance two methods close. resultssuggest unrestricted LTM suitable approximation inference LCM does.6. Related Workidea approximating complex BNs simple models using latter make inference investigated previously. existing work mainly falls two categories.work rst category approximates joint distributions BNs usesapproximation answer probabilistic queries. contrast, work secondcategory query-specic. assumes evidence known directly approximatesposterior distribution querying nodes.method falls rst category. investigate use LTMsframework. possibility also studied Pearl (1988) Sarkar (1995). Pearl(1988) develops algorithm constructing LTM marginally equivalentjoint distribution P (X), assuming LTM exists. Sarkar (1995) studies buildgood LTMs approximations amenable. methods, however,deal cases binary variables.Researchers also explored use models. Chow Liu (1968) considertree-structured BNs without latent variables. develop maximum spanning treealgorithm eciently construct tree model closest original BN termsKL divergence. Lowd Domingos (2005) learn LCM summarize data set.cardinality latent variable determined logscore hold-out setmaximized. show learned model achieves good model separate testingset, provide accurate answers simulated probabilistic queries. work,approximation quality inferential complexity learned model xed.method, hand, provides parameter C let users make tradeoapproximation quality inferential complexity.work second category mainly carried variational framework.mean eld method (Saul, Jaakkola, & Jordan, 1996) assumes querying nodesmutually independent. constructs independent model close posterior distribution. improvement mean eld method, structured mean eldmethod (Saul & Jordan, 1996) preserves tractable substructure among querying nodes,rather neglecting interactions. Bishop et al. (1997) consider another improvement,i.e., mixtures mean eld distributions. essentially ts LCM posterior distribution. methods directly approximate posterior distributions. Therefore,might accurate method used make inference. However,methods evidence-specic construct approximations online. Moreover, involveiterative process optimizing variational parameters. Consequently, online running time unpredictable. method, contrast, one determine inferentialcomplexity beforehand.898fiLatent Tree Models Approximate Inference Bayesian Networks7. Concluding Remarkspropose novel scheme BN approximate inference using LTMs. schemeone trade approximation accuracy inferential complexity.scheme achieves good accuracy low costs networks examined.particular, consistently outperforms LBP. also show LTMs superior LCMsused approximate inference.current bottleneck oine phase parameter learning. used EM algorithmoptimize parameters, known time consuming. problem especiallysevere parameter C sample size large. One way speed parameterlearning adapt agglomerative clustering technique learning cardinalitylatent variable data (Elidan & Friedman, 2001). basic idea completetraining data setting cardinality latent variable large enough assigningrecord latent state. step, one selects two states latent variablemerge. process repeats (penalized) likelihood ceases improve.parameter learning problem, terminate process desired cardinalityC achieved. also need deal multiple latent variables. Since data setcompleted, expect method yield good starting point EM shorttime, turn drastically shorten oine phase.Acknowledgmentsthank Haipeng Guo Yiping Ke insightful discussions. also gratefulanonymous reviewers valuable comments suggestions earlier versionpaper. Research work supported Hong Kong Grants Council Grants#622105 #622307, National Basic Research Program China (aka 973Program) project No. 2003CB517106. work completed rst authorleave HKUST Fok Ying Tung Graduate School, Guangzhou, China.ReferencesBishop, C. M., Lawrence, N., Jaakkola, T., & Jordan, M. I. (1997). Approximating posteriordistributions belief networks using mixtures. Proceedings 10th ConferenceAdvances Neural Information Processing Systems, pp. 416422.Chickering, D. M., & Heckerman, D. (1997). Ecient approximations marginallikelihood Bayesian networks hidden variables. Machine Learning, 29, 181212.Chow, C. K., & Liu, C. N. (1968). Approximating discrete probability distributionsdependence trees. IEEE Transactions Information Theory, 14 (3), 462467.Cover, T. M., & Thomas, J. A. (1991). Elements Information Theory. Wiley-Interscience,New York.Dempster, A. P., Laird, N. M., & Rubin, D. R. (1977). Maximum likelihood incomplete data via EM algorithm. Journal Royal Statistical Society. Series B(Methodological), 39 (1), 138.899fiWang, Zhang, & ChenElidan, G., & Friedman, N. (2001). Learning dimensionality hidden variables.Proceedings 17th Conference Uncertainty Artificial Intelligence.Elidan, G., Ninio, M., Friedman, N., & Schuurmans, D. (2002). Data perturbationescaping local maxima learning. Proceedings 18th National ConferenceArtificial Intelligence, pp. 132139.Frey, B. J., & MacKay, D. J. C. (1997). revolution: belief propagation graphscycles. Advances Neural Information Processing Systems, Vol. 10.Hagenaars, J. A., & McCutcheon, A. L. (2002). Applied Latent Class Analysis. CambridgeUniversity Press, Cambridge, UK.Henrion, M. (1988). Propagating uncertainty Bayesian networks probabilistic logicsampling. Uncertainty Artificial Intelligence 2, pp. 317324.Huber, P. J. (1967). behavior maximum likelihood estimates nonstandardconditions. Proceedings 5th Berkeley Symposium Mathematical StatisticsProbability, pp. 221233.Lowd, D., & Domingos, P. (2005). Naive Bayes models probability estimation.Proceedings 22nd International Conference Machine Learning, pp. 529536.Murphy, K. P., Weiss, Y., & Jordan, M. I. (1999). Loopy belief propagation approximateinference: empirical study. Proceedings 15th Conference UncertaintyArtificial Intelligence, pp. 467475.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann Publishers, San Mateo, CA.Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineeringlarge belief networks. Proceedings 10th Conference UncertaintyArtificial Intelligence, pp. 484490.Sarkar, S. (1995). Modeling uncertainty using enhanced tree structures expert systems.IEEE Transactions Systems, Man, Cybernetics, 25 (4), 592604.Saul, L. K., Jaakkola, T., & Jordan, M. I. (1996). Mean eld theory sigmoid beliefnetworks. Journal Artificial Intelligence Research, 4, 6176.Saul, L. K., & Jordan, M. I. (1996). Exploiting tractable substructures intractablenetworks. Advances Neural Information Processing Systems, Vol. 8, pp. 486492.Wang, Y., & Zhang, N. L. (2006). Severity local maxima em algorithm: Experienceshierarchical latent class models. Proceedings 3rd European WorkshopProbabilistic Graphical Models, pp. 301308.Zhang, N. L. (2004). Hierarchical latent class models cluster analysis. Journal MachineLearning Research, 5 (6), 697723.900fiJournal Artificial Intelligence Research 32 (2008) 825877Submitted 06/07; published 08/08Qualitative System Identification Imperfect DataGeorge M. Coghillg.coghill@abdn.ac.ukSchool Natural Computing SciencesUniversity Aberdeen, Aberdeen, AB24 3UE. UK.Ashwin Srinivasanashwin.srinivasan@in.ibm.comIBM India Research Laboratory4, Block C, Institutional AreaVasant Kunj Phase II, New Delhi 110070, India.Department CSE Centre Health InformaticsUniversity New South Wales, KensingtonSydney, Australia.Ross D. Kingrdk@aber.ac.ukDeptartment Computer ScienceUniversity Wales, Aberystwyth, SY23 3DB. UK.AbstractExperience physical sciences suggests realistic means understanding complex systems use mathematical models. Typically,come mean identification quantitative models expressed differential equations.Quantitative modelling works best structure model (i.e., formequations) known; primary concern one estimating values parameters model. complex biological systems, model-structure rarely knownmodeler deal model-identification parameter-estimation.paper concerned providing automated assistance first problems. Specifically, examine identification machine structural relationshipsexperimentally observed variables. relationship expressedform qualitative abstractions quantitative model. qualitative models mayprovide clues precise quantitative model, also assist understanding essence model. position paper background knowledgeincorporating system modelling principles used constrain effectively setgood qualitative models. Utilising model-identification framework provided Inductive Logic Programming (ILP) present empirical support position using seriesincreasingly complex artificial datasets. results obtained qualitativequantitative data subject varying amounts noise different degrees sparsity.results also point presence set qualitative states, term kernelsubsets, may necessary qualitative model-learner learn correct models.demonstrate scalability method biological system modelling identificationglycolysis metabolic pathway data.1. Introductiongrowing recognition research life sciences increasingly concerned ways relating large amounts biological physical data structurefunction higher-level biological systems. Experience physical sciences suggestsc2008AI Access Foundation. rights reserved.fiCoghill, Srinivasan, & Kingrealistic means understanding complex systems usemathematical models. topical example provided Physiome Project seeksutilise data obtained sequencing human genome understand describehuman organism using models that: . . . include everything diagrammatic schema,suggesting relationships among elements composing system, fully quantitative, computational models describing behaviour physiological systems organismsresponse environmental change (see http://www.physiome.org/). paper concerned computational tool aims assist identification mathematicalmodels complex systems.Broadly speaking, system identification viewed field modelling dynamic systems experimental data (Soderstrom & Stoica, 1989). distinguishbetween: (a) classical system identification techniques, developed control engineers econometricians; (b) machine learning techniques, developed computerscientists. two main aspects activity. First, appropriate structuredetermined (the model-identification problem). Second, acceptably accurate valuesparameters model obtained (the parameter-estimation problem). Classicalsystem identification usually (but always) used possible model structureknown priori. Machine learning methods, hand, interest littlenothing known model structure. tool described machine learningtechnique identifies qualitative models observational data. Qualitative modelsnon-parametric; therefore computational effort focussed model-identification(there parameters estimated). task therefore somewhat easierambitious machine learning programs attempt identify parametric quantitative models (Bradley, Easley, & Stolle, 2000; Dzeroski, 1992; Dzeroski & Todorovski, 1995;Todorovski, Srinivasan, Whiteley, & Gavaghan, 2000). Qualitative model-learningnumber advantages: models quite comprehensible; system-dynamicsobtained relatively easily; space possible models finite; noise-resistancefairly high. down-side, qualtitative model-learners often produced modelsunder- over-constrained; models provide clues precise mathematical structure; models largely restricted abstractions ordinarydifferential equations (ODEs). attempt mitigate first shortcomingsadopting framework Inductive Logic Programming (ILP) (see Bergadano & Gunetti,1996; Muggleton & Raedt, 1994). Properly constrained models identified using library syntactic semantic constraintspart background knowledge ILPsystemon physically meaningful models. Like ILP systems, library relativelyeasily extendable. position paper that:Background knowledge incorporating physical (and later, biological) system modelling principles used constrain set good qualitative models.Using classical physical systems test-beds demonstrate empirically that:small set constraints, conjunction Bayesian scoring function, sufficientidentify correct models.Correct models identified qualitative quantitative data needcontain measurements variables model; learned826fiQualitative System Identificationsparse data large amounts noise. is, correct models identifiedinput data incomplete, incorrect, both.closer examination performance test systems led discoveryterm kernel subsets: minimal qualitative states present guaranteeimplementation identify model correctly. concept may value modelidentification systems.primary interests, made clear outset, lie biological system identification.completion sequencing key model genomes rise new technologiesopened prospect modelling cells silico unprecedented detail. models essential integrate ever-increasing store biological knowledge,potential transform medicine biotechnology. key task emerging fieldsystems biology identify cellular models directly experimental data. applying qualitative system identification systems biology focus models metabolism:interaction small molecules enzymes (the domain classical biochemistry).models best established systems biology. end, demonstrateapproach scales identify core well-known, complex biological system(glycolysis) qualitative data. system modelled set 25 qualitative relations, several unmeasured variables. scale-up achieved augmentingbackground knowledge incorporate general chemical biological constraintsenzymes metabolites.rest paper organised follows. next section present learningapproach ILP-QSI means example: u-tube. also describe detailslearning algorithm section. Section 3 apply learning experimentsnumber systems class u-tube, present results obtained,discuss results experiments reported thus far. Section 4 extends worklearning qualitative data set proof-of-concept experiments assessability ILP-QSI learn quantitative data. scalability method testedSection 5 application large scale metabolic pathway: glycolysis. Section 6discuss related work; finally Section 7 provide general discussion researchdraw general conclusions.2. Qualitative System Identification Using ILPorder aid understanding method presented paper first presentdetailed description process applied illustrative system: u-tube.u-tube chosen well understood system, oneused literature (Muggleton & Feng, 1990; Say & Kuru, 1996). results emergingset experiments allow us draw tentative conclusions regardingqualitative systems identification.subsequent sections present results applying method describedsection examples class system; enable usgeneralise tentative conclusions. also apply method large scale biologicalsystem demonstrate scalability method.827fiCoghill, Srinivasan, & KingState123456h1< 0, std >< 0, inc >< (0, ), dec >< (0, ), dec >< (0, ), std >< (0, ), inc >h2< 0, std >< (0, ), dec >< 0, inc >< (0, ), inc >< (0, ), std >< (0, ), dec >qx< 0, std >< (, 0), inc >< (0, ), dec >< (0, ), dec >< 0, std >< (, 0), inc >qx< 0, std >< (0, ), dec >< (, 0), inc >< (, 0), inc >< 0, std >< (0, ), dec >Table 1: envisionment states used u-tube experiments. qualitative valuesstandard form used QSIM. Positive values magnituderepresented interval (0, ), negative values interval (, 0) zero0. directions change self explanatory increasing representedinc, decreasing dec steady std.2.1 Illustrative System: U-tubeu-tube system (Fig. 1) closed system consisting two tanks containing (or potentially containing) fluid, joined together base pipe. Assuming fluidsystem passes one tank via pipe tank higherfluid level tank lower fluid level (as function difference height).height fluid tanks system equilibriumfluid flow.u-tube represented system ordinary differential equations follows:dh1dtdh2dt= k (h1 h2 )(1)= k (h2 h1 )qualitative model may obtained simply abstracting real numbers,would normally associated Equation 1, quantity space signs.common formalism used represent qualitative models QSIM (Kuipers, 1994).representation models conjunctions constraints, two three placepredicates representing abstractions real valued arithmetic functional operations.variables model values represented two element vectors consisting (inabstract case) sign magnitude direction change variable.order accommodate restriction number variables constraint mayrewrite Equation 1 follows:h = (h1 h2 )qx = k h(2)dh1dt = qxdh2dt = qxh1 h2 height fluid Tank 1 Tank 2 respectively; hdifference height fluid tanks; q x flow fluid tanks.converted directly QSIM constraints shown Fig. 1.828fiQualitative System IdentificationTank 2Tank 1Delta_hh1+hh2dtdth1h2-qx-qxM+DERIV(h1,qx ),DERIV(h2,qx ),ADD(h2,Delta h, h1 ),M+ (Delta h,qx ),MINUS(qx,qx ).qxFigure 1: u-tube: (left) physical; (middle) QSIM diagrammatic; (right) QSIM constraints. QSIM version model Delta h corresponds hphysical model. QSIM, M+ (, ) qualitative version functional relationindicates monotonically increasing relation twovariables arguments. + (, ) constraint represents familyfunctions includes linear non-linear relations.265314Figure 2: u-tube envisionment graph.Appropriate qualitative analysis u-tube produce states shown Table 1,states envisionment. represent distinct qualitative statesu-tube may exist Fig. 2 depicts possible behaviours (in termstransitions states)1 . figure represents complete envisionment system,graph containing qualitative states system transitionsparticular input value. case u-tube presentedinput (which equivalent value zero). hand behavioursu-tube may observed number experimental (initial) conditions,measurements taken height fluid tank flowtanks. converted (by means quantitative-to-qualitative converter)set qualitative observations (or states). sufficient temporal information availableenable calculation qualitative derivatives, observation tuple statingmagnitude direction change measured variable. observations alsocontain states complete envisionment Table 1 (or subset thereof).1. State 1 represents situation fluid system, nothing happensinteresting.829fiCoghill, Srinivasan, & Kingu-tube member large class dynamic systems definedstates: state systems. systems values variables future timesdefined current state system regardless state achieved (Healey,1975). means simulation, system state act initial state.current context means order learn structure systems needfocus states may ignore transitions states.enables us explore power set envisionment ascertain conditionssystem identification possible. Given qualitative observations examples,background knowledge consisting constraints models (described later) QSIMrelations, learning system (which name ILP-QSI) performs search acceptablemodels. suitable first approximation, basic task viewed discreteoptimisation problem finding lowest cost elements amongst finite set alternatives.is, given finite discrete set models real-valued cost-function f : <,find subset H H = {H|H f (H) = min Hi f (Hi )}. problemmay solved employing procedure searches directed acyclic graphrepresentation possible models. representation, pair models connectedgraph one transformed another operation called refinement. Fig. 3shows parts graph u-tube model refined anotheraddition qualitative constraint. optimal search procedure (the branch-and-boundprocedure) traverses graph order, times keeping cost best nodesfar. Whenever node reached certain descendentscost higher best nodes, node descendents removedsearch.number features apparent u-tube model relevantlearning method utilised work (and discussed Section 2.3) describedsince regard general modelling issues relevant learning qualitative modelsdynamic systems.first thing may noted regard expressions Equation2 resulting qualitative constraints ordered; is, given valuesexogenous variables magnitude state variables (the height fluidtanks case) equations placed order variablesleft hand side may values calculated appear right handside equation2 . particular form ordering known causal ordering (Iwasaki& Simon, 1986). causally ordered system depicted graphically shown Fig. 4.causally ordered model contains algebraic loops. quantitative systems one triesavoid algebraic loops hard simulate, requiring additional simultaneousequation solvers used.qualitative model combined Qualitative Reasoning (QR) inference engineprovide envisionment system interest. is, generate qualitatively distinct states system may exist. case u-tube sixstates given Table 1. Example behaviours resulting states shownFig. 2.2. ordering required QSIM order preform qualitative simulation. However, abilityorder equations manner utilised filter learning system order eliminatemodels containing algebraic loops.830fiQualitative System IdentificationADD( h1, h 2, h1)ADD( h1, h 2, h1)DERIV( h2,h1 )MPLUS( h2,f12)ADD(h 2, h2, h1)ADD( h1, h 2, x1)ADD( h1, h 2, h1)DERIV( h2,h1 )MPLUS( h2,f12)MINUS( x8,f12 )MPLUS( x8,h1 )ADD( h1, h 2, x1)MPLUS( x1,f12 )MPLUS( h2,x4)DERIV(h1,x2 )DERIV( h1,x2 )[]DERIV( h1,h2 )DERIV( h1,f12)DERIV( h1,f12)DERIV( h2,x5)ADD( h2,x6,h1 )MPLUS( h1,f12)DERIV( h1,f12)DERIV( h2,x5)ADD( h2,x6,h1 )MPLUS( x6,f12 )MINUS( f12,x6 )MPLUS( h1,f12)MPLUS( h1,f12)MPLUS( h1,f12)ADD( f12,x7,h2 )DERIV( h2,f12)Figure 3: portions u-tube lattice (with target model box).may noted differential equation model captures essence explanation given first paragraph section. sufficient explain operationsystem, well predict way behave, containsvariables constants necessary achieve task - i.e. model parsimonious. 3Furthermore, examination causal diagram Fig. 4 indicates causalordering particular direction magnitudes state variablesderivatives. link derivatives magnitudes state variablesintegration time. integral causality preferred kind3. possible didactic purposes may want include detail, example relationintertank flow pressure difference, height fluid pressure.reason would expect relations found; although contextadequate theoretical framework model fits, model provides pointers direction.hand, one envisage simpler models existing may suitable predictioninadequate required kind explanation. See Section 6 this.831fiCoghill, Srinivasan, & Kingkh1h1hqxh2h2Figure 4: causal ordering u-tube model given Equation 2.causality systems engineering modelling; simulation generally.integration smooths noise whereas differentiation introduces it.variables either endogenous exogenous. Exogenous variables influence system influenced it. Well posed models flapping variables;is, endogenous variables appear one constraint. QSIM includesDERIV constraint linking state variables directly derivatives, systems interested regulatory, containing feedback paths, endogenousvariables must appear least two constraints.Well posedness parsimony mandatory properties model, properties desirable always achievable may relaxed. However,systems examined paper properties holds.final feature u-tube model represents single system.assumption implicit learning experiments described paper datameasured belongs single coherent system. keeping general experimentalapproaches assumed measurements related waypart system. course may get wrong relax requirementdiscover thought related cannot actually brought togethersingle model. generalises requirement parsimony line Einsteinsadage model simple possible simpler. casetranslates minimising number disjoint sub-systems identified.2.2 Qualitative Solution SpaceSection 2.3 shall present algorithm automatically constructing modelsdata. method utilise background knowledge consisting QSIM modellingprimitives combined systems theory meta-knowledge (e.g. parsimony causality).Later shall also provide analysis models learned states utilised learnorder ascertain which, any, states important successful learning.One way facilitate analysis make use solution space relate qualitativestates critical points relevant class systems (via isoclines system) 44. critical points dynamic system points one derivatives statevariables zero. isoclines contours critical points.832fiQualitative System Identification(Coghill, 2003; Coghill, Asbury, van Rijsbergen, & Gray, 1992). stated previously,qualitative analysis u-tube generate envisionment containing six states,shown Table 1, depicted envisionment graph given Fig. 2. Continuingh2h1h2height655f12 = 0242time13h1Figure 5: qualitative states u-tube system presented representative timecourses (left) solution space (right). state numbers referstates u-tube described above. (State 5 represents steady statestrictly speaking reached = , practice taken occurtwo trajectories sufficiently close, shown here.)u-tube; two ways behave (ignoring state 1), captured Fig. 5. Eitherhead fluid tank 1 greater tank 2 (state 4) (in extreme tank1 empty state 3), head greater tank 2 tank 1 (state 6). Fig. 5 (left)shows transient behaviour extreme case tank 1 empty (state 2);seen diagram head starts condition eventual endequilibrium (state 5). state Equation 1 rewritten as:0 = k (h1 h2 )(3)0 = k (h2 h1 )definition k must non-zero; solution pair equations is:h2 = h 1relation plotted graph shown right hand side Fig. 5.qualitative states u-tube may placed solution space graph relationequilibrium line. representation (similar form phase space diagram)useful provides global picture location qualitative statesenvisionment relative equilibria critical points system. alsoutilised construction diagnostic expert systems (Warren, Coghill, & Johnstone,2004). details means analysing envisionments see work Coghill(2003) Coghill et al. (1992).833fiCoghill, Srinivasan, & Kingbb(i, , f ) : Given initial element discrete set S; successor function : 2 ; cost functionf : <, return H H contains set cost-minimal models. h i,j H, f (hi ) =f (hj ) = fmin s0 S\H f (s0 ) > fmin .1. Active := h(i, )i.2. worst :=3. selected :=4. Active 6= hi5. begin(a) remove element (k, costk ) Active(b) costk < worst(c) begini. worst := costkii. selected := {k}iii. let P rune1 Active s.t. j P rune1 , f (j) > worst f (j) lowest costpossible j successorsiv. remove elements P rune1 Active(d) end(e) elseif costk = worsti. selected := selected {k}(f) Branch := (k)(g) let P rune2 Branch s.t. j P rune2 , fmin (j) > best fmin (j) lowest costpossible j successors(h) Bound := Branch\P rune2(i) x Boundi. add (x, f (x)) Active6. end7. return selectedFigure 6: basic branch-and-bound algorithm. type Active determines specialisedvariants: Active stack (elements added removed front)depth-first branch-and-bound results; Active queue (elements addedend removed front) breadth-first branch-and-boundresults; Active prioritised queue best-first branch-and-bound results.2.3 AlgorithmILP learner used research multistage procedure, addressesdiscrete optimisation problem. general terms, posed follows: given finitediscrete set cost-function f : <, find subset H H ={s|s f (s) = minsi f (si )}. optimal algorithm solving problemsbranch-and-bound algorithm, shown Fig. 6 (the correctness, complexity optimalityproperties algorithm presented paper Papadimitriou & Steiglitz, 1982).specific variant algorithm available within software environment comprisingAleph (Srinivasan, 1999). modified procedure Fig. 7. principal differencesFig. 6 are:1. procedure given set starting points H 0 , instead single one (i Fig. 6);834fiQualitative System Identification2. limitation number nodes explored (n Fig. 7);3. use boolean function acceptable : H B E {F ALSE, RU E}.acceptable(k,B,E) TRUE, if: (a) Hypothesis k explains examplesE, given B usual sense understood ILP (that is, B k |= E absencenoise); (b) Hypothesis k consistent constraints containedbackground knowledge (that B kI 6|= 2). practice, possible mergerequirements encoding requirement entailing examplesconstraint B;4. Inclusion background knowledge examples (B E Fig. 7).arguments refinement operator (the reason become apparentshortly) cost function f .following points relevant implementation used here:qualitative model represented single definite clause. Given definiteclause C, qualitative constraints model (the size model) obtainedcounting number qualitative constraints C. also calledsize C.Constraints, restriction well-posed models (described below), assumed encoded background knowledge;initial set H0 Fig. 7 consists empty clause denoted . is,H0 = {};acceptable(C, B, E) RU E qualitative model C consistentconstraints B, given E.Active prioritised queue sorted f ;successor function used . defined follows. Let sizeacceptable model C qualitative model size 0 n = 0 .assume B contains set mode declarations form described (Muggleton,1995). Then, given definite clause C, obtain definite C 0 (C, B, E) =n1nA = hD 0 |(C, B, E) s.t. 0 1A (D, B, E)i, (n 2). C 0 1A (C, B, E)obtained adding literal L C, that:argument mode +t L substituted input variable typeappears positive literal C variable typeoccurs negative literal C;argument mode L substituted variable C typeappears argument new variable type t;argument mode #t L substituted ground term type t.assumes availability generator elements Herbrand universeterms;acceptable(C 0 , B, E) RU E.835fiCoghill, Srinivasan, & KingbbA (B, E, H0 , , f, n) : Given background knowledge B B; examples E E; non-empty set initial elementsH0 discrete set possible hypotheses H; successor function : H B E 2 H ; cost functionf : H B E <; maximum number nodes n N (n 0) explored, return H HH contains set cost-minimal models models explored.1. Active = hi2. H0(a) add (i, ) Active3. worst :=4. selected :=5. explored := 06. (explored < n Active 6= hi)7. begin(a)(b)(c)(d)(e)(f)(g)(h)(i)remove element (k, costk ) Activeincrement exploredacceptable(k, B, E)begini. costk < worstii. beginA. worst := costB. selected := {k}C. let P rune1 Active s.t. j P rune1 , f (j, B, E) > worst f (j, B, E)lowest cost possible j successorsD. remove elements P rune1 Activeiii. end:iv. elseif costk = worstA. selected := selected {k}endBranch := (k, B, E)let P rune2 Branch s.t. j P rune2 , f (j, B, E) > worst f (j, B, E) lowestcost possible j successorsBound := Branch\P rune2x Boundi. add (x, f (x, B, E)) Active8. end9. return selectedFigure 7: variant basic branch-and-bound algorithm, implemented withinAleph system. B E sets logic programs; N setnatural numbers.following properties 1A (and, turn ) shown hold (Riguzzi,2005):locally finite. is, 1A (C, B, E) finite computable (assumingconstraints B computable);weakly complete. is, clause containing n literals obtainedn refinement steps empty clause;836fiQualitative System Identificationproper. is, C 0 equivalent C;optimal. is, C 0 obtained multiply refining differentclauses.addition, clear definition given qualitative model C, acceptable(C 0 , B, E)RU E model C 0 1A (C, B, E). turn, follows acceptable(C 0 , B, E)RU E C 0 (C, B, E).cost function used (following Muggleton, 1996) f Bayes (C, B, E) = P(C|B, E)P(C|B, E) Bayesian posterior probability estimate clause C, givenbackground knowledge B positive examples E. Finding model maximal posterior probability (that is, lowest cost) involves maximising function (McCreath, 1999):1Q(C) = logDH (C) + p logg(C)DH prior probability measure space possible models; pnumber positive examples (that is, p = |E|); g generality model.use approach used ILP system C-Progol obtain values twofunctions. is, prior probability related complexity models (morecomplex models taken less probable, priori); generality modelestimated using number random examples entailed model, givenbackground knowledge B (the details presented Muggleton paper1996).selected Bayesian function score hypotheses since represents,best knowledge, one ILP literature explicitly developedcase data consist positive examples (as situationpaper, examples observations system behaviour: system identificationnon-behaviour represent usual understanding taskattempting here).evident choices make branch-and-bound procedure simple generateand-score approach. Clearly, approach scalable constraints encodingwell-posed models sufficient restrict acceptable models reasonable number:describe set constraints sufficient models examinedpaper. rest paper, term ILP-QSI taken mean Alephbranch-and-bound algorithm specific choices above.2.3.1 Well-posed modelsWell-posed models introduced Section 2.1; current implementationdefined satisfying least following syntactic constraints:1. Size. model must particular size (measured number qualitativerelations physical models Sections 2.4 3 number metabolitesbiological model Section 5). size pre-specified.2. Complete. model must contain measured variables.837fiCoghill, Srinivasan, & King3. Determinate. model must contain many relations variables (a basic principlesystems theorythe reader may recall version school algebra, systemequations contains many equations unknowns).4. Language. number instances qualitative relation model mustpre-specified limit. kind restriction studied greaterdetail work Camacho (2000).least following semantic constraints:5. Sufficient. model must adequately explain observed data. adequate,intend acknowledge due noise measurements, observationsmay logical consequences model 5 . percentage observations mustexplainable sense user-defined value.6. Redundant. model must contain relations redundant. example, relation ADD(inf low, outf low, x1) redundant model alreadyADD(outf low, inf low, x1).7. Contradictory. model must contain relations contradictory givenrelations present model.8. Dimensional. model must contain relations respect dimensional constraints.prevents, example, addition relations like ADD(inf low, outf low, amount)perform arithmetic variables different units measurement.following additional constraints incorporated algorithm couldignored (because preferences rather absolute rules), results presentedpaper require satisfied:9. Single. model must contain two disjoint models. assumptionset measurements made within particular contextuser desires single model includes measurement variables.10. Connected. intermediate variables appear least two relations.11. Causal. model must causally ordered (Iwasaki & Simon, 1986) integralcausality (Gawthrop & Smith, 1996). is, causality runs algebraicconstraints model magnitudes state variables derivatives;derivatives magnitudes DERIV constraint only.list intended exhaustive: fully expect would need augmented domain-specific constraints (the biological system identification problemdescribed Section 5 provides instance this). advantage using ILPaugmentation possible relatively straightforward manner.5. Strictly speaking, model conjunction background knowledge.838fiQualitative System Identification2.4 Experimental Investigation Learning U-tube Systemsection present comprehensive experimental test learning algorithmdescribed previous section. focus u-tube illustrate approachexplain results obtained. subsequent section present resultsapplying ILP-QSI learning structure number different systems similarkind. data utilised experiments qualitative. assumed eithermeasurements yield qualitative values quantitative time seriesconverted qualitative values. latter may necessary situationsquantitative time series data available sufficient quantity permitquantitative system identification performed.following general method applied learning systems studiedpaper.2.4.1 Experimental AimUsing u-tube system, investigate model identification capabilities ILP-QSI usingqualitative data subject increasing amounts noise made increasingly sparse order ascertain circumstances target system mayaccurately identified, function number qualitative observations available.2.4.2 Materials Methodmodel learning system ILP-QSI seeks learn qualitative structural models qualitative data; therefore focus experiments learning qualitative data.Data inputs (exogenous variables) system. data requiredlearning combinations qualitative states (of 6) envisionment shown Table 1.Method two distinct sets experiments reported here: based noisefree data based noisy data. former assume data providedcorrect used test capability ILP-QSI handling sparse data. latter setexperiments captures situation qualitative data may incorrectmeasurement errors due noise original signal, errors introducedquantitative qualitative transformation (which may occur cases originaldata numerical).Noise-free data. use following method evaluating ILP-QSIs system-identificationperformance noise-free data:system investigation:1. Obtain complete envisionment specific values exogeneous variables.(In particular case u-tube discussed section exogenousvariables envisionment states shown Table 1, stated above.)839fiCoghill, Srinivasan, & King2. non-empty subsets states envisionment training data constructset models using ILP-QSI record precision result. 6 numberpossible non-empty sets states different test scenarios u-tube 63.(2N 1, N number states complete envisionment)3. Plot learning curves showing average precision versus size training data.Noisy data. use following method evaluating ILP-QSIs system-identificationperformance noisy qualitative data:system investigation:1. Obtain complete envisionment specific values exogeneous variables.2. Replace non-empty subsets states envisionment randomly generatednoise states. combination correct random states, trainingdata construct set models using ILP-QSI record precision result. 7Given complete envisionment N states, replacing random subset k > 0random states result noisy envisionment consisting N k noisefree states k random states. Step 2 noise-free data, exhaustivereplacement possible subsets complete envisionment random statesresult 2N 1 noisy test sets.3. Plot learning curves showing average precision versus size training data.2.4.3 Resultsresults performing experiments, showing precision learning targetmodel versus number states used (for noise-free noisy data) shownFig. 8. evident situations precision improves number statesused results experiments noisy data lower precisionnoise-free data (though curves general shape).results one would expect.noise-free data find possible identify target model usingone state data. However possible identify target model using pairsstates 53% cases. states are:[2, 3], [2, 4], [2, 5], [3, 5], [3, 6], [4, 5], [4, 6], [5, 6]refer Kernel sets. time merely report finding delaydiscussion significance reporting results experimentssystems class.6. proportion models result equivalent correct model. Thus,training data set, result returned ILP-QSI precision 0.0 1.0. termprecision used meaning usually associated Machine Learning communityrather familiar Qualitative Reasoning.7. non-noisy data, training data set, result returned ILP-QSIprecision 0.0 1.0.840fiQualitative System Identification10.90.8Precision0.70.60.50.40.30.2CleanNoisy0.10123456Number StatesFigure 8: Precision models obtained u-tube.3. Experiments Systemssection present experimental setup applied number systems:coupled tanks, cascaded tanks mass spring damper. systems representativeclass system appearing industrial contexts (e.g. cascaded tanks systemused model diagnosis industrial Ammonia Washer system Warrenet al., 2004) well useful analogs metabolic compartmental systems.case experimental method identical utilised u-tubedescribed Section 2.4. system give description system targetmodel, envisionment associated system, statement data usedexperiments, summary results obtained experiments.3.1 Experimental Aimthree physical systems: coupled tanks, cascaded tanks mass-spring-damper (a wellknown example servomechanism), investigate model identification capabilitiesILP-QSI using qualitative data subject increasing amounts noisemade increasingly sparse.3.2 Materials MethodData Qualitative data available consist complete envisionment arising specificvalues input variables. precise details data given experiment.Methodmethod used u-tube described Section 2.4.841fiCoghill, Srinivasan, & Kingh1< 0, std >< 0, inc >< (0, ), dec >< (0, ), dec >< (0, ), inc >< (0, ), dec >< (0, ), std >< (0, ), dec >< (0, ), dec >< (0, ), dec >State123467891011h2< 0, std >< (0, ), dec >< 0, inc >< (0, ), inc >< (0, ), dec >< (0, ), std >< (0, ), dec >< (0, ), dec >< (0, ), dec >< (0, ), dec >qx< 0, std >< (, 0), inc >< (0, ), dec >< (0, ), dec >< (, 0), inc >< (0, ), dec >< 0, inc >< (0, ), dec >< (0, ), std >< (0, ), inc >qo< 0, std >< (0, ), dec >< 0, inc >< (0, ), inc >< (0, ), dec >< (0, ), std >< (0, ), dec >< (0, ), dec >< (0, ), dec >< (0, ), dec >Table 2: envisionment states used coupled tanks experiments. (The stateslabeled accord u-tube; since state 5 u-tubeappear coupled tanks envisionment state labeled 5table.)3.3 Coupled Tanksopen system consisting two reservoirs shown Fig. 9. Essentially,seen u-tube input output. input, q , flows top tank1 output, qo , flows base tank 2 (see Fig. 9).Tank 2Tank 1qih2h1+hhM+dth1h 1h2qiqx+M+dth 2qx+qoDERIV(h1,h01 ),DERIV(h2,h02 ),ADD(h2 ,Delta h, h1 ),M+ (Delta h,qx ),M+ (h2 ,qo ),ADD(h02 ,qo ,qx ),ADD(qx,h01 ,qi ).qoFigure 9: coupled tanks: (left) physical; (middle) QSIM diagram; (right) QSIM relations.experiments assume observe: q , qx , h1 , h2 , qo . Thus systemidentification must discover model three intermediate variables, h 01 , h02 h.Data one exogenous variable, namely flow liquid tank 1 (q ).experiments described input flow kept zero (that is, q = h0, stdi), makingsystem particular case moderately complex u-tube.complete envisionment consists 10 states, shown Table 2 Fig. 10, means1024 experiments set.842fiQualitative System Identification189367104112Figure 10: Coupled tanks envisionment graph.3.3.1 Resultsprecision graphs coupled tanks experiments shown Fig. 11.results show improvement precision number states used increases alsodeterioration precision noise added. effect noise worse fewer statesused case u-tube, though effect nullified statesused.10.90.8Precision0.70.60.50.40.3CleanNoisy0.20.1012345678910Number StatesFigure 11: Coupled tanks precision graphs.noise free data possible identify models using singledatum utilising pairs states yielded target model 11% cases. relevantpairs states (kernel sets) are:[2, 7], [3, 8], [4, 8], [6, 7], [7, 8]843fiCoghill, Srinivasan, & KingWhereas u-tube experiments states target model successfullylearned supersets pairs, coupled tanks case sets three states(which supersets pairs listed above) result successful identificationtarget model:[2, 3, 9], [2, 3, 10], [2, 3, 11][2, 4, 9], [2, 4, 10], [2, 4, 11][3, 6, 9], [3, 6, 10], [3, 6, 11][4, 6, 9], [4, 6, 10], [4, 6, 11]3.4 Cascaded Tankssystem also open system. However, flow system always unidirectional (unlike coupled tanks system). principle, system brokentwo sub-systems containing one reservoir, input output.example system shown Fig. 12. Liquid flows tank 1, unidirectionally tank 1 tank 2. apparent figure, flowtop tank 1 base tank 2.qih1h1qxh2M+dth 1TankM+dth 2qoh2qi+qx+qoDERIV(h1,h01 ),DERIV(h2,h02 ),M+ (h1 ,qx ),M+ (h2 ,qo ),ADD(h02 ,qo ,qx ),ADD(qx,h01 ,qi ).Tank BFigure 12: cascaded tanks: (left) physical; (middle) QSIM diagrammatic; (right) QSIMrelations.assume observe: qi , h1 , h2 , qx . Thus system identification mustdiscover model two intermediate variables, h 01 h02 . numbered list states(or complete envisionment) case shown Fig. 13 Table 3.Data one exogenous variable, namely flow liquid tank 1 (q ).increase complexity allowing steady positive input flow (that is, q = h(0, ), stdi).complete envisionment consists 14 states, shown Fig. 13 Table 3means 16,383 experiments required .3.4.1 Resultsprecision graphs cascaded tanks shown Fig. 14. graphs similarshape coupled systems, showing generally lower precision noisy-data.examination shows unable identify target model fewerthree states. subset triples (which form kernel sets case)844fiQualitative System Identification1128421395111410673Figure 13: Cascaded tanks envisionment graph.State1234567891011121314h1< 0, inc >< 0, inc >< (0, ), dec >< (0, ), dec >< (0, ), dec >< (0, ), dec >< (0, ), std >< (0, ), std >< (0, ), std >< (0, ), std >< (0, ), inc >< (0, ), inc >< (0, ), inc >< (0, ), inc >h2< 0, std >< (0, ), dec >< 0, inc >< (0, ), dec >< (0, ), std >< (0, ), inc >< 0, inc >< (0, ), dec >< (0, ), std >< (0, ), inc >< 0, inc >< (0, ), dec >< (0, ), std >< (0, ), inc >qx< 0, inc >< 0, inc >< (0, ), dec >< (0, ), dec >< (0, ), dec >< (0, ), dec >< (0, ), std >< (0, ), std >< (0, ), std >< (0, ), std >< (0, ), inc >< (0, ), inc >< (0, ), inc >< (0, ), inc >qo< 0, std >< (0, ), dec >< 0, inc >< (0, ), dec >< (0, ), std >< (0, ), inc >< 0, inc >< (0, ), dec >< (0, ), std >< (0, ), inc >< 0, inc >< (0, ), dec >< (0, ), std >< (0, ), inc >Table 3: envisionment states used cascaded tanks experiments.target model identified are:[1, 3, 4], [1, 3, 5], [1, 3, 8], [1, 3, 9], [1, 7, 4], [1, 7, 5], [1, 7, 8], [1, 7, 9]845fiCoghill, Srinivasan, & King1CleanNoisy0.90.8Precision0.70.60.50.40.30.20.1002468101214Number StatesFigure 14: Cascaded tanks precision graph.3.5 Mass-Spring-Damperfinal physical system considered abstraction wide variety servomechanismsdisplacing force. example system shown Fig. 15. situation,mass held equilibrium two forces. equilibrium disturbed, oscillatorybehaviour observed. motion mass damped oscillationscontinue indefinitely, eventually return original equilibrium position.external force applied (for example pulling mass down) final resting placedisplaced natural equilibrium point (see Fig. 15). mass displacementdispM rest position, time, t, moving velocity velaccelerating rate accM . assume observe variables: disp , velM , accM ,disp M+ H 1+dtDampingSpringvel M+H2H+forcedtveldispa=v/secaccM M+H3DERIV(dispM ,velM ),DERIV(velM ,accM ),M+ (dispM ,H1 ),M+ (velM ,H2 ),M+ (accM ,H3 ),ADD(H1,H2 ,H4 ),ADD(H3,H4 ,force).forceFigure 15: spring system (a) physical; (b) QSIM diagrammatic; (c) QSIM relationsf orce. Qualitative system identification must find model four intermediatevariables, H1 , H2 , H3 H4 ; well intermediate relation ADD(H 1 ,H2 ,H4 ),846fiQualitative System Identificationthree variables. input force, force, exogenous. experiments here,consider case steady force applied system (thatis, ForceA = h(0, ), stdi). complete envisionment case shown Fig. 16,equilibrium point represented state 2. precision graphs shown2192429170318222716232820212612756108915131411413025Figure 16: Mass-spring-damper envisionment graphFig. 17. system envisionment contains 31 states, makes exhaustive testingunrealistic. Instead sets clean noisy states randomly selected spacepossible experiments. Nonetheless observed average precision graphsin-line obtained tanks experiments. However, actual precision valuessuggest sparse data noise less effect systems.may due tight relationship two derivatives spring model,making system extremely constrained.3.6 Discussion Resultsinspection experimental results reveals expected pattern: casesprecision curves (for noisy noise free experiments) general shape.Experiments utilise fewer states identify target model less oftengreater number states used. However, closer examination results revealseven states used (pairs triples) target model may consistentlyfound particular combinations states used. order understandrequires us look solution spaces systems concerned. 8examine u-tube coupled tanks together closelyrelated systems zero input. cascaded tanks system slightly differentnon-zero input discussed later section.8. discuss spring system complexity.847fiCoghill, Srinivasan, & King10.90.8Precision0.70.60.50.40.30.2Clean dataNoisy data0.101471013161922252831Number StatesFigure 17: Mass-spring-damper precision graph3.6.1 U-tube Coupled Tanksbare results, interesting, give indication particular pairstriples highlighted precisely identify target model. order ascertain why?must examine envisionment states given Tables 1 2, itemiserelevant features sets states follows:u-tube coupled tanks least one critical point pair.systems pair states contains one state branchenvisionment graph (Fig. 2 & Fig. 10); least one extremebranch. is, states one tank either empty state immediatelysucceeding this, tank relatively full derivativesheight tank opposite signs.systems supersets minimal sets precisely learn targetmodel.observations lead us suggest coupled systems ability learningsystem identify structure model dependent data used includingcritical points data covers different types starting pointsystem behaviours have. keeping systems theory would leadus expect (Gawthrop & Smith, 1996).order properly appreciate indicated kernel sets relationsystems need look solution spaces (Coghill et al., 1992;848fiQualitative System IdentificationCoghill, 2003) two systems. shown Fig. 18 (and derivationsimilar given Section 2.2 detailed Appendix A). getclear picture kernel pairs triples lie respect critical pointssystem.h1=h2=0h2h1=0h258662112104h2=097413h113h1Figure 18: solution spaces u-tube coupled tanks systems.system diagrams provided Fig. 1 Fig. 9 seen u-tubecoupled tanks systems differ fact coupled tanks outlet orifice,whereas u-tube not. accounts major difference solution spaces;namely coupled tanks two critical points (states 7 9) whereas u-tubeone (state 5) actually steady state. gives rise additionalstates: 9, 10 11 lie critical points. 9 observedoutlet orifice tank 2 coupled tanks system decreases size spaceisoclines solution space become narrower disappears orificecloses. seen formally comparing equations 6 10 Appendix A.clear k2 approaches zero, equation 6 approximates equation 10 (andk2 = 0 two equations same).look sets pairs observe related waysreflect relationship two coupled systems. Firstly, looking pairs.u-tube 4 pairs include critical point (steady state), state 5: [2,5], [3, 5], [4, 5], [5, 6]. noting discussion state 5 u-tuberelates either states 7 8 coupled tanks find analogous pairsexist kernel set coupled tanks: [2, 7], [3,8], [4, 8], [6, 7]. leaves onepair coupled tanks pairs unaccounted for: [7, 8]. However, surprise sincepair taken map state 5 u-tube; consistent findingsingleton state sufficient learn model system.9. three states differ magnitude qx qdir h01 h02 ,neither appear explicitly solution space. Readers may convincecomparing Table 1 envisionment Table 2.849fiCoghill, Srinivasan, & Kingstill 4 pairs u-tube experiments able learn reliablytarget model corresponding coupled tanks pair. are: [2, 3],[2, 4], [3, 6], [4, 6]. comparison triples learning coupled tanksmodel reveals states pairs conjoined either state 9, 1011 make triples. inclusion states warrants explanation sincestates distinguish closed u-tube open coupled tanks.three states state variables value h(0, ), dec)i; situationcannot occur u-tube. Combining fact four pairs listedcontain critical point qualitatively identical systems leads oneconclusion additional information contained triple kernel sets enablesone distinguish u-tube coupled tanks case.results extend, strengthen deepen reported Coghill et al. (2004)Garrett et al. (2007).3.6.2 Cascaded Tankscascaded tanks system asymmetrical flow possible one direction. fact input positive steady flow makes setup marginallycomplex regard coupled systems, input flow.kernel sets model system may learned (presented Section3.4.1 depicted schematically Fig. 19 order explain results obtained.look first middle columns diagram ignore, time being,downstream tank, see represented two pairs states: tankempty combined tank steady state, tank empty combinedstate amount fluid tank greater steady state. confirmedexperimentally kernel sets single tank model learned.ignore upstream tank (apart outflow) examine middlethird columns diagram see divide two groups accordingwhether input downstream tank steady decaying (positive decreasing).two pairs states, upstream tank:tank empty combined tank steady state, tank empty combinedstate amount fluid tank greater steady state.case tank seen cross product states appear kernel setscase represents valid possible situation.results lead two major conclusions regard cascaded tanks system:1. ILP-QSI effectively identifies individual components cascade combinescascade point.2. situation downstream tank, input either steady flowdecreasing flow, indicates utilising variety inputs aid identificationprocess.former conclusion may serve pointer possibility incremental learningcascaded systems.850fiQualitative System Identification13845978459Figure 19: schematic representation triples states target modelcascaded tanks systems learned.851fiCoghill, Srinivasan, & King4. Experiments Quantitative Datapart experimental testing system proof-of-concept test.stated system designed learn qualitative models qualitative data.assumed conversion quantitative data already performed,least needed qualitative data analysis would require another research projectbeyond scope paper. However, order test usability systemquantitative data test ability go whole process receivingdata producing model, implemented rudimentary data analysis packagefacilitate this. course exhaustive, permit us test resultsproduced via process consistency produced experimentsqualitative data.4.1 Experimental AimUsing four physical systems, investigate model identification capabilities ILPQSI using numeric traces system behaviour subject increasing amountsnoise.4.1.1 Quantitative Qualitative Conversionproceeding describe experiments carried out, present method usedconvert numerical data qualitative form required ILP-QSIutilised set experiments.adopted straightforward simple approach performing conversion.quantitative variable x, values N real-valued time series steps numericallydifferentiated means central difference approach (Shoup, 1979) that,(xi xi1 )+(xi+1 xi )2dxidt=d2 xdt2= (xi xi1 ) (xi+1 xi )= 2N 1quantitative variable x converted qualitative variable q = hqmag, qdiri,qmag {(-,0), 0, (0,)} generated x, qdir {dec, std, inc} generateddx/dt. qualitative derivative q, q, obtained similar manner generateddx/dt d2 x/dt2 respectively.data typically noisyeither inherently, process differentiationperform simple smoothing first second derivatives using Blackmanfilter (a relative moving average filter see Blackman & Tukey, 1958). case,filter actually applied result Fast Fourier Transform (FFT) result obtained taking real part inverse FFT. note formsmoothing appropriate sufficient number time steps present.obtained (smoothed) numerical value x variable x instant i, qualitative magnitude qmag(xi ) is, principle, simply obtained following:qmag(xi ) =(, 0)xi < 00xi = 0(0, +) otherwise852fiQualitative System Identificationpractice, since floating-point values unlikely exactly zero, foundadvantageous re-apply filtering process data straddling zero eliminate smallfluctuations around value. Despite measures, addition generating correctqualitative states (true positives) conversion produce errors: states generated maycorrespond true states (false positives); true states may generated(false negatives). Fig. 20 shows example (the problem is, course, exacerbatedoriginal quantitative data noisy). reason imperfect resultsnoise free quantitative data twofold: one smoothing process small fluctuations around zero; main reason that, discussed above, creating full qualitativestate involves numerical differentiation introduces noise data derivatives affects ability process convert quantitative qualitativeabsolute accuracy.Systemu-tubeCoupledCascadedSpringTrueStates6101433GeneratedStates614833TruePositives46520FalsePositives28313FalseNegatives0060Figure 20: example errors resulting generating qualitative states tracessystem behaviour. Here, traces generated following initialconditions: h1 = 2.0, h2 = 0.0 three tank systems; disp = 2.0,velM = 0.0 spring.4.2 Materials MethodNumerical simulations four physical systems constructed using generalrelations qualitative models. experiments carried utilisingnoise free noisy data, described rest section.4.2.1 Datamodels used numerical simulations structure qualitativemodels, substitution real valued parameter monotonic functionrelation. gives linear relation two variables; complex, non-linearfunctions might used, linear functions provided suitable approximationknown behavior systems, shown graphically Fig. 21 (a)(d);much required proof-of-concept study.given set function parameter values, initial conditions, input value,quantitative model produces single quantitative behaviour (this contrasts qualitativemodels produce list possible behaviours model). Parameter valueschosen models approached steady state time periodtest. models implemented Matlab 5.3 using ODE15s ODE solver.853fiCoghill, Srinivasan, & King33TankTank B2.9TankTank B2.52.82.72LevelLevel2.62.51.52.412.32.20.52.12050100150200250300Time seconds35040045005003050100150200250300Time seconds35040045050020TankTank BDisplacementVelocity152.510Displacement/VelocityLevel21.55051100.5150050100150200250300Time seconds35040045020500050100150200250300Time seconds350400450500Figure 21: graph example numeric behavior (a) u-tube, top left; (b) coupledtanks, top right; (c) cascaded tanks, bottom left, (d) damped spring,bottom right.time point generated simulation made available part sampled data.ensures sampling rate suitably fast respect Nyquist criterion.also guarantees sufficient number data points available requiredBeckman filter.4.2.2 MethodNoise-free data. use following method evaluating ILP-QSIs system-identificationperformance noise-free data:four test-systems:(a) Obtain system behaviour test system number different initialconditions input values. Convert qualitative states usingprocedure Section 4.1.1.(b) Using qualitative states obtained training data construct set modelsusing ILP-QSI record precision result (this proportion854fiQualitative System Identificationmodels result equivalent correct model). Thus,training data set, result returned ILP-QSI precision0.0 1.0.following details relevant: (a) quantitative models put threeseparate initial conditions, magnitude two state variables set (2, 0),(0, 3) (2, 3). Specifically, initial values two tank levels threetank systems, displacement velocity spring. valuescrucial chosen initial conditions caused numerical modelsconverge steady state system reasonable number iterations; (b)initial condition gave rise system behaviour hence set qualitative states.second step above, qualitative states behaviours used training data.kernel subsets necessary correct system identification usually containqualitative states multiple quantitative behaviours. (c) conversion process resultserroneous qualitative states (see Section 4.1.1). Thus, training data used containfalse positives false negatives.Noisy data. use following method evaluating ILP-QSIs system-identificationperformance noisy quantitative data:four test-systems:(a) Obtain system behaviour test system number different initialconditions input values.(b) Corrupt system behaviour additive noise;(c) Convert corrupted behaviour qualitative states using procedureSection 4.1.1.(d) Using qualitative states obtained training data construct set modelsusing ILP-QSI record precision result (this proportionmodels result equivalent correct model). Thus,training data set, result returned ILP-QSI precision0.0 1.0.second step noise added numerical data sets follows. Gaussian noisesignal (with 0.0 1.0) generated using built-in Matlab functionnormrnd scaled three orders magnitude original noise, namely 0.01, 0.11.0 (representing low, medium high amounts noise respectively). scalednoise variants added numerical values system behaviour obtainedinitial condition.4.3 Quantitative Experimental Resultsprocess converting quantitative qualitative states introduces errors, evennoise free data. Table 4 shows proportion correct qualitative states totalnumber qualitative states obtained numerical signal, including noisystates. table shows proportion four systems, different degrees855fiCoghill, Srinivasan, & KingModelu-tubeCoupledCascadedSpringNoise level00.010.1100.010.1100.010.1100.010.11Initial States(2,0)(0,3)(2,3)4/64/63/52/82/82/92/102/102/152/372/372/536/145/135/146/164/144/126/165/254/156/586/614/465/85/83/84/104/122/94/174/212/134/394/494/3820/33 19/35 22/3923/48 20/36 18/4123/49 20/38 18/4420/65 20/52 19/53Table 4: input data numeric experiments, described proportionnumber clean states / total number converted states different systemsdegrees noise.noise. numerical simulations intended exhaustive coverevery possible behaviour; surprising observe casestates complete envisionment generated.results qualitative experiments detailed previous section indicateorder successfully learn target model data branches envisionmentrequired, greater number states used greater liklihoodlearning target model structure. Therefore experiments utilised statesgenerated numerical simulations.results numerical data experiments shown Fig. 22. experiments show possible learn models clean noisy numerical data evenqualitative states generated clean numerical data contain numberunavoidable data transformation errors. results systems usedfollows:spring system system 31 states complete envisionment Table 4shows quantitative qualitative conversion process yields around 20states. seen Fig. 17 learning 20 states gives 100% precisionlearning target model, even presence noise. surprising,therefore, find learning precision perfect highest noise level.Since qualitative experiments done sampling, slight downturnhighest noise level could due large number noisy states generated856fiQualitative System Identification10.90.8UtubeCoupled TanksCascaded TanksSpringPrecision0.70.60.50.40.30.20.1000.010.11.0Amount NoiseFigure 22: comparison results numerical learning tests, averagedthree initial conditions. Tests attempted learn states,cascaded tanks, likely fail large numbersstates, spring. consistent kernel subset principleintroduced Section 2.4 since model-learning precise without presencecertain key states input data.experiment. Hence say results keeping qualitativeexperiments.u-tube & coupled tanks complete envisonments systems contain 6 10states respectively. Table 4 shows number true states generated lesscomplete envisionment, significantly less number noisy statescase. one would expect results presented Fig. 8 u-tube givesbetter results coupled tanks (having higher proportion envisionmentstates present). Ultimately, ability learn model completely curtailednoise; though sooner case coupled tanks (which qualitativeexperiments show sensitive presence noise). consistentresults qualitative experiments.cascaded tanks qualitative learning experiments kernel subset triplesstate 0 included. state representing situation tanksempty begin with, one initial states included numerical857fiCoghill, Srinivasan, & Kingsimulations. Also, perusal Fig. 14 reveals introduction noise radicallyreduces learning precision, 4 states (the average number true statesgenerated qualitative quantitative conversion process) zero. Takingaccount facts expected cascaded tanks model wouldsuccessfully identified experiments. consistentfindings qualitative experiments.5. Application Biological System Identificationwork reported thus far aimed demonstrating viability ILP-QSIidentifying bounds operation approach. section examine scalabilitymethod identify complex real world biological network. use glycolysispathway test case identification.5.1 Test System: Glycolysischose study metabolic pathway glycolysis test case. Glycolysis oneimportant ubiquitous biology, also historically one firstdiscovered, still presents challenge model accurately.QSIM primitives sufficient model adequately qualitative behaviourglycolysis pathway. are, however, two problems. First, biologists wouldunderstand model, would reason much higher level abstraction. Second, computational complexity corresponding system identification taskglycolsis (a qualitative model 100 QSIM relations) is, least currently,intractable. address modelling metabolic pathways abstractmanner using biologically meaningful metabolic components (MC) (a similar approachconstructing complex qualitative models human heart used Bratko, Mozetic,& Lavrac, 1989). Specifically, note metabolic pathways, essentially twotypes object: metabolites (small molecules) enzymes (larger molecules catalyzereactions). use component models objects described (King,Garrett, & Coghill, 2005).5.1.1 Modelling Metabolites Enzymesconcentrations metabolites vary time synthesised utilisedenzymatically catalysed reactions. result concentration given time-pointfunction of: (a) concentration previous time-point; (b) degreeused created various enzyme reactions.modelling enzymes, enzyme assumed two substratestwo products. two substrates products considered formsubstrate product complex, amount complex proportionalamount substrates products multiplied together. models probabilitysubstrates (or products) collide enzyme sufficient timelinesscatalysed product complex (or substrate complex). substrate complexconverted product complex, disassociates product metabolites,vice versa. shall use phrase flow enzyme denote amount858fiQualitative System Identificationsubstrate complex formed minus amount product complex formed. (See workVoit Radivoyevitch (2000) details enzyme kinetics.)MetaboliteMetabolites1Metabolites2E nzmMetab*S_forwdtMtbM_dtM+-M+P_revSum-FlowFlow+*Flow1 FlownMetabolitep1Metabolitep2Figure 23: Metabolic Components (MCs) used biological experiments,underlying QSIM primitives.Quantitative, corresponding qualitative representations metabolite enzymes using QSIM relations, therefore:Enzymes:Metabolites:nXdM=F lowidti=1DERIV(M etabolite,Mdt),SUM(F low1 , . . ., F lown ,Md t).(4)F lowi = f (s=1etabolites) g(Petabolitep)(5)p=1PROD(M etabolite1, . . ., etabolites, S-for),PROD(M etabolite1, . . ., etabolitep, P-rev),M+(S-for, Ds),M+(P-rev, Dp),SUB(Ds, Dp, F low),MINUS(F low,F lowminus).Here, refers input metabolites enzymatic reaction, substrates, Prefers products enzymatic reaction. SUM() PROD() predicates simplyextensions ADD() MULT() predicates, number inputs. Fig. 23 showsconstraints grouped together metabolic components (MCs). permitsus create general constraints representing metabolite enzyme componentsfollows:ENZYME((S1 , S2 ) (P1 , P2 ) enzymeF low)METABOLITE(metaboliteConc metaboliteF low (enzymeF low 1 . . . enzymeF lown ))859fiCoghill, Srinivasan, & King1.2.3.4.5.6.7.8.9.10.(Hexokinase):(Phosphoglucose isomerase):(Phosphofructokinase):(Aldolase):(Triose phosphate isomerase):(Glyceraldehyde 3-phosphate dehydrogenase):(Phosphoglycerate kinase):(Phosphoglycerate mutase):(Enolase):(Pyruvate kinase):Glc + ATP G6P + ADP.G6P F6P.F6P + ATP F16BP + ADPF16BP DHAP + G3PDHAP G3PG3P + NAD 13BP + NADH.13BP + ADP 3PG + ATP.3PG 2PG.2PG PEPPEP + ADP Pyr + ATP.Figure 24: reactions included qualitative model glycolysis. reactionsconsume ATP NADH explicitly included.ENZYME predicate identifies substrates products (the first argument)returns single variable representing flow enzyme (the second argument).METABOLITE predicate relates level flow metabolites (the first secondarguments) flow enzymes (the third argument).5.1.2 Modelling GlycolysisUsing qualitative components representing metabolites enzymes, construct qualitative model glycolysis. model uses 15 metabolites, namely: pyruvate (Pyr), glucose (Glc), phosphoenolpyruvate (PEP), fructose 6-phosphate (F6P), glucose 6-phosphate(G6P), dihydroxyacetone phosphate (DHAP), 3-phosphoglycerate (3PG), 1,3-bisphosphoglycerate(13BP), fructose 1,6-biphosphate (F16BP), 2-phosphoglycerate (2PG), glyceraldehyde 3phosphate (G3P), ADP, ATP, NAD, NADH. included H+, H 2 O, Orthophosphate assumed ubiquitous (in addition, restriction substratesproducts three number prevents inclusion).qualitative state glycolysis defined set qualitative states 15metabolites. Table 5 representation one qualitative state. understandstate consider first entry intended represent qualitative state NAD (that is,NAD concentration: < 0, ), dec >, NAD flow: < (, 0), dec >). meaningconcentration NAD positive (0, ) decreasing (dec),rate change concentration NAD (in analogy physical systems, flowNAD) negative (, 0) decreasing (dec). Similar meanings applymetabolites. Note metabolic concentrations must 0 ; cannotnegative, 0 state uninteresting.Using representation, possible model glycolysis shown Fig. 25. model describes constraints levels flows metabolites. Thus, constraint enzyme((G3Pc,NADc), (13BPc, NADHc), Enz6f) states flow enzyme 6 (Enz6f) controls transformation concentrations G3Pc NADc levels 13BPcNADHc; whereas constraint metabolite(NADc, NADc, (Enz6f, -)) states860fiQualitative System IdentificationMetaboliteNADNADHATPADPPyrGlcPEPF6PG6PDHAP3PG13BPF16PB2PGG3PConcentration< (0, ), dec >< (0, ), inc >< (0, ), dec >< (0, ), dec >< (0, ), inc >< (0, ), dec >< (0, ), dec >< (0, ), dec >< (0, ), dec >< (0, ), dec >< (0, ), inc >< (0, ), std >< (0, ), inc >< (0, ), dec >< (0, ), inc >Flow< (, 0), dec >< (0, ), inc >< (, 0), dec >< (, 0), dec >< (0, ), dec >< (, 0), inc >< (, 0), dec >< (, 0), dec >< (, 0), dec >< (, 0), dec >< (0, ), std >< 0, inc >< (0, ), dec >< (, 0), dec >< (0, ), inc >Table 5: legal qualitative state 15 metabolites observed glycolysis.ENZYME((Glcc, ATPc),(G6Pc,ADPc),Enz1f),ENZYME((G6Pc),(F6Pc),Enz2f),ENZYME((F6Pc,ATPc),(F16BPc,ADPc),Enz3f),ENZYME((F16BPc),(G3Pc,DHAPc),Enz4f),ENZYME((DHAPc),(G3Pc),Enz5f),ENZYME((G3Pc,NADc),(13BPc,NADHc),Enz6f),ENZYME((13BPc,ADPc),(3PGc,ATPc),Enz7f),ENZYME((3PGc),(2PGc),Enz8f),ENZYME((2PGc),(PEPc),Enz9f),ENZYME((PEPc,ADPc),(Pyrc,ATPc),Enz10f),METABOLITE(ATPc,ATPf, (Enz10f +), (Enz7f, +),(Enz1f, -),(Enz3f, -)),METABOLITE(ADPc,ADPf,(Enz1f, +),(Enz3f, +),(Enz10f, -)(Enz7f, -)),METABOLITE(NADc,NADf,(Enz6f, -)),METABOLITE(NADHc,NADHf,(Enz6f, +)),METABOLITE(Pyrc,Pyrf,(Enz10f, +)),METABOLITE(Glcc,Glcf,(Enz1f, -)),METABOLITE(PEPc,PEPf,(Enz9f, +),(Enz10f, -)),METABOLITE(F6Pc,F6Pf,(Enz2f, +),(Enz3f, -)),METABOLITE(G6Pc,G6Pf,(Enz1f, +),(Enz2f, -)),METABOLITE(DHAPc,DHAPf,(Enz4f, +),(Enz5f, -)),METABOLITE(3PGc,3PGf,(Enz7f, +),(Enz8f, -)),METABOLITE(13BPc,13BPf,(Enz6f, +),(Enz7f, -)),METABOLITE(F16BPc,F16BPf,(Enz3f, +), (Enz4f, -)),METABOLITE(2PGc,2PGf,(Enz8f, +),(Enz9f, -)),METABOLITE(G3Pc,G3Pf,(Enz5f, +),(Enz4f, +),(Enz6f, -)).Figure 25: representation qualitative model glycolysis (see text details).concentration (NADc) flow (NADf) metabolite NAD controlled flowsingle enzyme number 6 (Enz6f : Glyceraldehyde 3-phosphate dehydrogenase),enzyme removes (signified -) NAD (+ would mean enzyme flow addscorresponding metabolite).861fiCoghill, Srinivasan, & King5.2 Experimental Aimspecific system identification task interested is: Given qualitative observations metabolic states, ILP-QSI identify correct qualitative model glycolysis?5.3 Materials Methodmethodology depicted Fig. 26, describe two separate ways identifyingbiochemical pathways. make following assumptions:1. data sparse necessarily measured part continuous time series.realistic given current experimental limitations metabolomics, rulespossibility numerical system identification approaches.2. metabolites known structure involved model. reasonemploy chemoinformatic heuristic decrease number possiblereactions. heuristic based reasonable assumption chemicalreaction catalysed enzyme breaks chemical bonds. Full detailspaper King et al. (2005). strongest assumption make. Even givenrapid advance metabolomics (NMR, mass-spectroscopy, etc.), currentlyrealistic assume relevant metabolites pathway observedstructure determined.3. metabolites known structure involved particular pathway.restriction current metabolomics technology observe compoundsstructurally identified.4. reactions involve three substrates three products.5. qualitative states: measure direction change metabolite levelfirst-derivative. requires sampling level least three times succession.5.3.1 Logical/Graph-based Constraintsfirst considered logical/graph-based (LG) nature problem. specific domain metabolism imposes strong constraints possible LG based models. usedconstraints following way:1. Chemical reactions conserve matter atom type (Valdes-Perez, 1994). glycolysis generated possible ways combining 18 metabolites form matteratom type balance reactions ( 3 reactants 3 products). produced 172possible reactions substrates balanced products number typeelement. number compares well 2,300,000 possible reactionswould naively possible.2. Typical biochemical reactions make/break bonds, cannot arbitrarilyrearrange atoms make new compounds. reaction considered plausiblebroke 1 bond per reactant. analysis done originally hand,subsequently developed general computer program automate task.862fiQualitative System IdentificationLG ModellingpossiblecombinationsmetabolitesConservation masselement type.QR ModellingBonds brokenTypicalbiochemistryQR SystemIdentificationQR2GlycolysisGlycolysisFigure 26: Metabolic System Identification methodology.172 balanced reactions 18 considered chemically plausible. 18reactions, 10 actual reactions glycolysis 8 decoy reactions.5.3.2 Qualitative Reasoning Constraintsused simple generate test approach learning. first computationalexperiment used 10 reactions glycolysis 8 decoy reactionsconsidered chemically feasible, see Fig. 24. reactions, absence evidencecontrary, considered irreversible. first generated possible wayscombining 18 reactions connected 15 main substrates glycolysis (modelsnon-disjoint). generated 27,254 possible models 10 reactions -necessary look models reactions target (parsimony),models generated size order. smallest number reactions necessaryinclude 15 metabolites size 5. 27,254 models involved reaction:glyceraldehyde 3-phosphate + NAD 1,3-bisphosphoglycerate + NADH (reaction 6);could immediately conclude reaction occurred glycolysis.863fiCoghill, Srinivasan, & Kingformed example qualitative states glycolysis using QR simulator (in pseudorandom manner) test models. states thus generated contain noise.27,254 possible models tested states, model couldgenerate particular state removed consideration (accuracy constraint).Note flows metabolites enzyme observed -intermediate variables. observe overall levels flows metabolites.makes system identification task much harder.efficiency, used fast YAP Prolog compiler. also formed compiledversions enzyme metabolite MCs (input/output look-up tables), compiledparts QSIM. also adopted resource allocation method employed increasingly computationally expensive tests: i.e. forming filter tests exponentially increasingnumbers example states.5.3.3 Resultsseveral months compute time 65 node Beowulf cluster reduced 27,254possible models 35 (a 736 fold reduction). models included target model(glycolysis), plus 34 models could qualitatively distinguished it.35 models included following six reactions (see Fig. 24):3. F6P + ATP F16BP + ADP4. F16BP DHAP + G3P5. DHAP G3P6. G3P + NAD 13BP + NADH8. 3PG 2PG9. 2PG PEPreactions form core glycolysis.Examining 35 models also revealed correct model fewest cycles,however know general phenomenon.attempted use Progol positive compression measure distinguish models. based comparing models randomly generated states.However, unsuccessful model covered 100,000 random statesgenerated! believe due extremely large state space. However, simplemodification approach work. produce random states glucogenesis(glycolysis driven reverse direction), true model glycolysis covers fewerexamples 34 alternatives, identified true model. Noteapproach, unlike use Progol positive compression measure, requiresnew experimental data obtained.Thus demonstrated method learning qualitative models dynamicsystems scalable handle relatively large metabolic system. achievedmeans MCs represent meaningful units domain, map directlyQSIM constraints abstracted. also enable us presentcomplex models user friendly manner, removing need understandstructure high order differential equations.864fiQualitative System Identification6. Related WorkSystem identification long history within machine learning: presentimportant signposts directly relevent work here. focusstrand research deals learning qualitative models dynamic systems.earliest description aware concerning computer program identifying quantitative model explain experimental data work Collins (1968).procedure described heuristically searches equation structures,linear combination functions observed variables. Better known Baconsystem (Langley, 1981), early versions largely concentrated parameter estimation problem, particular selecting appropriate values exponentsequations. example, given class algebraic equation structures Bacon.1 ablereconstruct Keplers model planetary motion data. later work (for example work Nordhausen & Langley, 1993) attempted extend work dealidentifying algebraic structure relevant parameters, Bacon highlightedimportance bias (Mitchell, Keller, & Kedar-Cabelli, 1986) machine learning,constraining possible model structures space possible models conforming structures. quantitative equation discovery systems lineage are:Coper (Kokar, 1985), uses dimensional analysis restrict space equations;Fahrenheit/EF (Langley & Zytkow, 1989) E examine space bivariate equations; Abacus (Falkenhainer & Michalski, 1986) identify piecewiseequations; Sds uses type dimensionality restrictions constrain spaceequations; Lagrange family equation finders (Dzeroski & Todorovski, 1993; Todorovski & Dzeroski, 1997; Todorovski et al., 2000; Todorovski, 2003) attempt identifymodels form ordinary partial differential equations; IPM (Langley, George,Bay, & Saito, 2003) extensions developments, Prometheus/RPM (Bridewell,Sandy, & Langley, 2004; Asgharbeygi, Bay, Langley, & Arrigo, 2006), incorporateprocess descriptions (Forbus, 1984) aid construction revision quantitativedynamic models.Focussing specifically non-classical system identification metabolic models, perhaps notable work identification Arkin, Shen, Ross (1997)identified graphical model reactions part glycolysis experimental data.work Reiser, King, Kell, Muggleton, Bryant, Oliver (2001) presents unifiedlogical approach simulation (deduction) system identification (induction abduction). interesting recent approach, presented Koza, Mydlowec, Lanza, Yu, Keane(2000), examines identification metabolic ODE models using genetic programmingtechniques. this, cellular system viewed electrical circuit spacepossible circuits searched means genetic programming approach.earliest reported work identification qualitative models Mozetic,1987, colleagues, identified model electrical activity heart. work,reported fully (Bratko et al., 1989) remains landmark effort qualitativemodelling complex biological system. However, researchers noted (Bratko,Muggleton, & Varsek, 1992), results obtained static modelsprovide insight models dynamic systems identified.865fiCoghill, Srinivasan, & Kingfirst machine learning system learning qualitative models dynamic systemsGenmodel (Coiera, 1989a, 1989b). Genmodel need negative examplessystem behaviour models learned restricted qualitative relationships amongstobserved variables (that is, intermediate, hidden, variables hypothesized). model, obtained using notion specific generalization observedvariables (in sense Plotkin, 1971), usually over-constrained. is, containedconstraints necessary characterize fully dynamics systemmodeled. updated version Genmodel developed Hau Coiera (1993) showeddimensional analysis (Bhaskhar & Nigam, 1990) could used form directednegative example generation. new version could learn real-valued experimental data (which converted internally qualitative form), still requiredvariables known measured outset. system MISQ, entirely similarcomplexity abilities earlier version Genmodel developed Kraan,Richards, Kuipers (1991). later re-implemented general-purpose relational learning program Forte (Richards & Mooney, 1995), allowed hypothesisintermediate variables (Richards, Kraan, & Kuipers, 1992). relational pathfindingapproach used MISQ (through auspices Forte) special form Inductive LogicProgramming, general framework much powerfulBratko colleagues first view problem learning dynamic qualitativemodels explicitly exercise Inductive Logic Programming (ILP) first demonstrated possibility introducing intermediate (unobserved) variables models.used ILP system Golem (Muggleton & Feng, 1990) along QSIM representation produce model u-tube system. model identified equivalentaccepted model (in sense predicted behaviour) structuregenerated form could help explain behaviour (Coghill & Shen, 2001).Like Genmodel, model produced constrained. Unlike Genmodel, Golemrequired positive negative examples system behaviour shown HauCoiera (1993) sensitive actual negative examples used.Say Kuru (1996) describe program system identification qualitative datacalled QSI. QSI first finds correlations variables, iteratively introducesnew relations (and intermediate variables), building model comparing outputmodel known states satisfactory model found. Say Kurucharacterized approach one diminishing oscillation approaches correctmodel. Like Genmodel MISQ, QSI require negative observations systembehaviour. Unlike systems, use dimensional analysisappear mechanism incorportating constraints easily within program.importance dimensional analysis recognised though: authors suggestcentral search procedure.Thus, identification quantitative models longer history machinelearning, learning qualitative models also subject notable research efforts.view, MISQ (the version implemented within Forte) QSI probably representstate-of-the-art area. primary shortcomings these:866fiQualitative System Identificationapparent description experimental evaluation MISQ whetherable handle imperfect data (the correctness theorem presentedapplies complete, noise-free data).MISQ seeks constrained model consistent data. Often,exactly opposite sought (that is, want parsiomonius model).QSI deals qualitative data appear include easy mechanism incorporation new constraints guide search.7. General Discussionpaper presented method learning qualitative models dynamic systemstime-series data (both qualitative quantitative). section discussgeneral findings limitations, well suggesting number directions developingresearch theme.7.1 Computational Limitationsmajor limitation ILP-QSI system identifying glycolysis time taken(several months Beowulf cluster) reduce models 27,000 possible onesgenerated using chemoinformatic constraints, single correct one using qualitativestate constraints. would preferable process faster, importantnote identifying system 10 reactions 15 metabolites scratchextremely hard identification task. doubt human could achieve it,believe would challenge system identification methods aware of.difficult compare system identification methods believe needcompetitions run KDD compare methods.computational time identification dominated time taken testparticular model produce certain observed states: examining 27,000 modelsunusual machine learning program, unusual program take hourstest individual examples covered. slow speed identification methodtherefore problem normally considered learning method (i.e.search space possible models done), rather, intrinsic complexrelationship model states defines. cover-test method is, worstcase, exponential maximum size model. Note lack efficient, i.e.polynomial algorithm, determine cover using qualitative states.believe inherent difficulty task applies quantitative qualitativemodels. areas mathematics moving discrete real domainsimplify problems - basis much power analysis. However,currently little evidence case system identification, quantitativemodels would seem aggravate problem. cover tests essentially deductions:set axioms rules (computer program/model) produce particular logical sentence(observed state); general non-computable. However, real scientific systems,bounded space time, non-computability problem, howeverexpect system identification methods struggle task (Sadot, Fisher, Barak,Admanit, Stern, Hubbard, & Harel, 2008).867fiCoghill, Srinivasan, & King7.2 Kernel Subsetspresentation results experimentation clear certain subsetsstates (termed kernel subsets) guarantee target model learned.analysis kernel subsets state sets, hypothesise kernel sets reflectqualitative structure system interest.coupled system, order learn structure system high degreeprecision, data used come tests yielding qualitatively different behaviors:i.e. behaviors would appear distinct branches envisionment graph. However,hypothesis provides necessary, sufficient, condition learningidentify states branch suitable starting points experiment.example consider coupled tanks system. One could select states 9 11;different branches yet form kernel subset. hand,noted presenting results system key states kernel subsetsstates 7 8. states different branches represent critical pointsfirst derivative state variables system. indicates importancestates definition system.test set state variables critical pointstest could run short time correct model structure identified. However,probably impossible practice set test; especially situationstructure system completely unknown. alternative set multiple testsstate variables set extrema: initial conditions statesenvisionment eventually passed through. However still may difficultset tests, could take long time complete. two scenarios formends spectrum within practical experimental setting lie.identification best strategies important area research presentwork clearly relevant.hand, cascaded systems kernel sets capture asymmetrystructure. extrema critical points play important role;case subordinate fact ILP-QSI automatically decomposes systemconstituent parts learning. fact points important conclusion learninglarger scale complex systems; namely learning facilitated by, possible,decomposing system cascaded subsystems.7.3 Future Workvalidated ILP-QSI data derived real biological systems, next stepexplore successful modelling real experimental data. would relativelystraightforward obtain data water tanks springs, would muchinteresting work real biological data. work successful likelyquantitative qualitative conversion process need improved. Althoughfocus work here, developing rigorous approach would crucial usingILP-QSI laboratory setting (Narasimhan, Mosterman, & Biswas, 1998).done much easier use real experimental data analysis ILP-QSI.Specifically, improvements required ability extract qualitative states868fiQualitative System Identificationpassed numerical simulation, whilst minimizing noise. Nevertheless,direct limitation ILP-QSI method.following possibilities would benefit investigation:QR representation used could changed QSIM detailedflexible one Morven (Coghill & Chantler, 1994; Coghill, 1996).hypotheses presented kernel subsets, formedstates others, need confirmed analyzed further.ability map explore features model space would greatuse planning enhancements and, alongside kernel subsets, help giveunderstanding exactly states allow reliable learning.Large scale complex systems generally identified piece piece. resultscascaded tanks experiments indicate circumstances mayeasily facilitatied. investigation warranted.alternative methods described paper, incremental approachidentifies subsystems complete system interesting avenue investigation(Srinivasan & King, 2008).8. Summary Conclusionspaper presented novel system, named ILP-QSI, learns qualitativemodels dynamic processes. system stands squarely strand researchintegrates Machine Learning Qualitative Reasoning extends work areafollowing ways:ILP-QSI algorithm extends work; branch bound algorithmmakes use background knowledge (at least) three kinds order focus guidesearch well posed models dynamic processes.Syntactic Constraints: model size prespecified; models must completedeterminate; must proliferate instances qualitative relations.Semantic Constraints: model must adequately explain data; must containrelations redundant contradictory; relations model mustrespect dimensional constraints.System Theoretic Constraints: model singular disjoint; endogenous variables must appear least two relations; modelcausally ordered.thoroughly tested system number well known dynamic processes.enabled us ascertain ILP-QSI capable learning varietyconditions noisy noise free data. testing also allowed us identifyconditions possible learn appropriate model dynamic system.conclusions aspect work are:869fiCoghill, Srinivasan, & KingLearning precision related richness (or sparcity) noisiness datalearning performed.target model precisely learnt data used kernel subset.kernels made states different branches envisionment graph.system critical points play important role identifying model structure.spectrum possibilities regard setting suitable experiments garner data learn models physical biological systemsinterest.Cascaded parts systems help identify suitable points decomposition modellearning.ILP-QSI designed learn qualitative structural model qualitative data,sometimes case original measurements quantitative (albeit sparsepossibly noisy). order ascertain ILP-QSI would cope qualitative data generated quantitative measurements carried proof-of-concept set experimentsphysical process models previously utilised. resultskeeping results obtained qualitative experiments. adds weightconclusions regarding viability approach learning structural modelsdynamic systems adverse conditions.Finally, order test scalability method, applied ILP-QSI largescale metabolic pathway: glycolysis. case search space deemed largeattempt learning QSIM primitives alone. However, knowledge domain enabled usgroup primitives set Metabolic Components models metabolicpathways easily constructed. Also, part research Logical graphbased models used represent background domain knowledge. Utilising these,able identify 35 possible structures glycolysis pathway (out possible27,254); target model fewest cycles (though knowgeneral phenomenon) minimally covered data generated reverse pathwayglucogenesis.overall conclusions work qualitative reasoning methods combinedmachine learning (specifically ILP) successfully learn qualitative structural modelssystems high complexity number adverse circumstances. However, workreported herein constitutes step line research recently begun; and,interesting lines research, raises turn interesting questions needaddressed.Acknowledgmentswork supported part BBSRC/EPSRC grant BIO10479. authors wouldlike thank Stephen Oliver Douglas Kell helpful discussions biologicalaspects paper. would also like thank Simon Garrett many interestingfruitful interactions.870fiQualitative System IdentificationAppendix A. Derivation Solution Space Tanks Systemsappendix provide summary whence solution spaces tanks systemsutilised project constructed. details regarding envisionmentsassociated solution spaces may found work Coghill et al. (1992) Coghill(2003).order facilitate analysis need make use quantitative versionsystem models. ease exposition make additional assumptionsystems linear.10A.1 U-tubequantitative model u-tube systemdh1= k(h1 h2 )dtdh2= k(h2 h1 )dtinspection two equations easy see (ignoring trivial casek = 0) derivatives two equations zero when:h1 = h 2(6)is:dh2dh1==0dtdtaccounts relationship, depicted Fig. 18, h 1 h2derivatives zero. envisionment table u-tube (Table 1 Section 2.2)see state zero derivatives state 5; hence represented line.h1 = h 2A.2 Coupled Tanksquantitative model coupled tanks systemdh1= qi k1 (h2 h1 )dtdh2= k1 (h2 h1 ) k2 h2dtdh1dt(7)(8)= 0 Equation 7 rewritten as:0 = qi k1 (h2 h1 )= q k 1 h2 k 1 h110. fact types non-linearity normally associated systems kind solution spacesqualitatively identical described here, although analysis required constructslightly complicated.871fiCoghill, Srinivasan, & Kingre-arranged giveh2 =qih1k1qi zero reducesh2 = h 1dh2dt(9)= 0 Equation 8 rewritten as:0 = k1 (h2 h1 ) k2 h2= k 1 h1 k 1 h1 k 2 h2= (k1 k2 )h2 k1 h1(k1 k2 )h2 = k1 h1Re-arranging givesh2 =k1h1(k1 k2 )(10)accounts relations h 1 h2 depicted solution space Fig.18.A.3 Cascaded Tanksquantitative model cascaded tanks system is:dh1dtdh1= q k 1 h1dt(11)dh2= k 1 h1 k 2 h2dt(12)= 0 Equation 11 re-arranged as:qi = k 1 h1h1 =dh2dtqik1= 0 Equation 12 rewritten as:k2 h2 = k 1 h1872fiQualitative System Identificationh2 =k1h1k2accounts relations h 1 h2 depicted solution space Fig.27.h2h'1 = 037111h'2 = 041285913h106102Figure 27: solution space cascaded tanks system.ReferencesArkin, A., Shen, P., & Ross, J. (1997). test case correlation metric constructionreaction pathway measurements. Science, 277, 12751279.Asgharbeygi, N., Bay, S., Langley, P., & Arrigo, K. (2006). Inductive revision quantitativeprocess models. Ecological modelling, 194, 7079.Bergadano, F., & Gunetti, D. (1996). Inductive Logic Programming: Machine LearningSoftware Engineering. MIT Press.Bhaskhar, R., & Nigam, A. (1990). Qualitative physics using dimensional analysis. ArtificialIntelligence, 45, 73111.Blackman, R. B., & Tukey, J. W. (1958). Measurement Power Spectra. John WileySons, New York.Bradley, E., Easley, M., & Stolle, R. (2000). Reasoning nonlinear system identification. Tech. rep. CU-CS-894-99, University Colorado.Bratko, I., Mozetic, I., & Lavrac, N. (1989). KARDIO: Study Deep QualitativeKnowledge Expert Systems. MIT Press, Cambridge, Massachusetts.873fiCoghill, Srinivasan, & KingBratko, I., Muggleton, S., & Varsek, A. (1992). Learning qualitative models dynamicsystems. Muggleton, S. (Ed.), Inductive Logic Programming, pp. 437452. AcademicPress.Bridewell, W., Sandy, J., & Langley, P. (2004). interactive environment modelingdiscovery scientific knowledge.. Tech. rep., Institute Study LearningExpertise, Palo Alto, CA.Camacho, R. (2000). Inducing Models Human Control Skills using Machine LearningAlgorithms. Ph.D. thesis, University Porto.Coghill, G. M. (1996). Mycroft: Framework Constraint-Based Fuzzy Qualitative Reasoning. Ph.D. thesis, Heriot-Watt University.Coghill, G. M. (2003). Fuzzy envisionment. Proc. Third International WorkshopHybrid Methods Adaptive Systems, Oulu, Finland.Coghill, G. M., Asbury, A. J., van Rijsbergen, C. J., & Gray, W. M. (1992). application vector envisionment compartmental systems.. Proceedings firstinternational conference Intelligent Systems Engineering, pp. 123128, Edinburgh,Scotland.Coghill, G. M., & Chantler, M. J. (1994). Mycroft: framework qualitative reasoning. Proceedings Second International Conference Intelligent SystemsEngineering, pp. 4955, Hamburg, Germany.Coghill, G. M., Garret, S. M., & King, R. D. (2004). Learning qualitative modelsmetabolic systems. Proceedings European Conference Artificial Intelligence ECAI-04, Valencia, Spain.Coghill, G. M., & Shen, Q. (2001). specification multiple models diagnosisdynamic systems. AI Communications, 14 (2), 93104.Coiera, E. W. (1989a). Generating qualitative models example behaviours. Tech. rep.8901, University New South Wales, Deptartment Computer Science.Coiera, E. W. (1989b). Learning qualitative models example behaviours. Proc.Third Workshop Qualitative Physics, Stanford.Collins, J. (1968). regression analysis program incorporating heuristic term selection.Dale, E., & Michie, D. (Eds.), Machine Intelligence 2. Oliver Boyd.Dzeroski, S. (1992). Learning qualitative models inductive logic programming. Informatica, 16 (4), 3041.Dzeroski, S., & Todorovski, L. (1993). Discovering dynamics. International ConferenceMachine Learning, pp. 97103.Dzeroski, S., & Todorovski, L. (1995). Discovering dynamics: inductive logic programming machine discovery. J. Intell. Information Syst., 4, 89108.Falkenhainer, B., & Michalski, R. S. (1986). Integrating quantitative qualitative discovery: abacus system. Machine Learning, 1 (4), 367401.Forbus, K. D. (1984). Qualitative process theory. Artificial Intelligence, 24, 169204.874fiQualitative System IdentificationGarrett, S. M., Coghill, G. M., Srinivasan, A., & King, R. D. (2007). Learning qualitativemodels physical biological systems. Dzeroski, S., & Todorovski, L. (Eds.),Computational discovery communicable knowledge, pp. 248272. Springer.Gawthrop, P. J., & Smith, L. P. S. (1996). Metamodelling: Bond Graphs DynamicSystems. Prentice Hall, Hemel Hempstead, Herts, England.Hau, D. T., & Coiera, E. W. (1993). Learning qualitative models dynamic systems.Machine Learning, 26, 177211.Healey, M. (1975). Principles Automatic Control. Hodder Stoughton.Iwasaki, Y., & Simon, H. A. (1986). Causality device behavior. Artificial Intelligence,29, 332. See also De Kleer Browns rebuttal Iwasaki Simons replyrebuttal volume journal.King, R. D., Garrett, S. M., & Coghill, G. M. (2005). use qualitative reasoningsimulate identify metabolic pathways.. Bioinformatics, 21 (9), 2017 2026.Kokar, M. M. (1985). Coper: methodology learning invariant functional descriptions.Mitchell, T., Carbonell, J., & Michalski, R. (Eds.), Machine Learning: GuideCurrent Research, pp. 151154. Kluwer Academic Press.Koza, J. R., Mydlowec, W., Lanza, G., Yu, J., & Keane, M. A. (2000). Reverse engineeringautomatic synthesis metabolic pathways observed data using geneticprogramming.. Tech. rep. SMI-2000-0851, Stanford University.Kraan, I. C., Richards, B. L., & Kuipers, B. J. (1991). Automatic abduction qualitativemodels. Proceedings Qualitative Reasoning 1991 (QR91).Kuipers, B. (1994). Qualitative Reasoning. MIT Press.Langley, P. (1981). Data-driven discovery physical laws. Cognitive Science, 5, 3154.Langley, P., George, D., Bay, S., & Saito, K. (2003). Robust induction process modelstime series data.. Proc. twentieth International Conference Machine Learning,pp. 432439, Washington, DC. AAAI Press.Langley, P., & Zytkow, J. (1989). Data-driven approaches empirical discovery. ArtificialIntelligence, 40, 283312.McCreath, E. (1999). Induction first order logic noisy training examples fixedexample set sizes. Ph.D. thesis, University New South Wales.Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. (1986). Explanation-based generalization: unifying view. Machine Learning, 1, 4780.Mozetic, I. (1987). Learning qualitative models. Bratko, I., & Lavrac, N. (Eds.),Progress Machine Learning: Proceedings EWSL 87: 2nd European Working Session Learning, pp. 201217. Sigma Press.Muggleton, S. (1995). Inverse Entailment Progol. New Gen. Comput., 13, 245286.Muggleton, S. (1996). Learning positive data. Lecture Notes AI, 1314, 358376.Muggleton, S., & Feng, C. (1990). Efficient induction logic programs. Proc.First Conf. Algorithmic Learning Theory. OHMSHA, Tokyo.875fiCoghill, Srinivasan, & KingMuggleton, S., & Raedt, L. D. (1994). Inductive logic programming: Theory methods.Journal Logic Programming, 19,20, 629679.Narasimhan, S., Mosterman, P., & Biswas, G. (1998). systematic analysis measurement selection algorithms fault isolation dynamic systems. Proc. Ninth Intl.Workshop Principles Diagnosis (DX-98), pp. 94101, Cape Cod, MA.Nordhausen, B., & Langley, P. (1993). integrated framework empirical discovery.Machine Learning, 12, 1747.Papadimitriou, C., & Steiglitz, K. (1982). Combinatorial Optimisation. Prentice-Hall,Edgewood-Cliffs, NJ.Plotkin, G. (1971). Automatic Methods Inductive Inference. Ph.D. thesis, EdinburghUniversity.Reiser, P., King, R., Kell, D., Muggleton, S., Bryant, C., & Oliver, S. (2001). Developinglogical model yeast metabolism. Electronic Transactions Artificial Intelligence,5, 233244.Richards, B. L., Kraan, I., & Kuipers, B. J. (1992). Automatic abduction qualitativemodels. Proc. Tenth National Conference Artificial Intelligence (AAAI92), pp. 723728. MIT Press.Richards, B. L., & Mooney, R. J. (1995). Automated refinement first-order horn-clausedomain theories. Machine Learning, 19 (2), 95131.Riguzzi, F. (2005). Two results regarding refinement operators. Kramer, S., & Pfahringer,B. (Eds.), Late Breaking Papers, 15th International Workshop Inductive LogicProgramming (ILP05), August 1013, 2005, pp. 5358, Munich, Germany.Sadot, A., Fisher, J., Barak, D., Admanit, Y., Stern, M. J., Hubbard, E. J. A., & Harel, D.(2008). Towards verified biological models.. IEEE/ACM Trans. Comput. BiologyBioinformatics., 5(2), 112.Say, A. C. C., & Kuru, S. (1996). Qualitative system identification: deriving structurebehavior. Artificial Intelligence, 83, 75141.Shoup, T. E. (1979). Practical Guide Computer Methods Engineers. Prentice-HallInc., Englewood Cliffs, N. J. 07632.Soderstrom, T., & Stoica, P. (1989). System Identification. Prentice Hall.Srinivasan, A. (1999). Aleph Manual. Available http://www.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/.Srinivasan, A., & King, R. D. (2008). Incremental identification qualitative modelsbiological systems using inductive logic programming. J. Machine Learning Researchappear.Todorovski, L. (2003). Using domain knowledge automated modeling dynamic systemsequation discovery. Ph.D. thesis, Faculty Electrical Engineering ComputerScience, University Ljubljana, Slovenia.Todorovski, L., Srinivasan, A., Whiteley, J., & Gavaghan, D. (2000). Discovering structure partial differential equations example behavior. Proceedings876fiQualitative System IdentificationSeventeenth International Conference Machine Learning, pp. 991998, San Francisco.Todorovski, L., & Dzeroski, S. (1997). Declarative bias equation discovery. Proc. 14thInternational Conference Machine Learning, pp. 376384. Morgan Kaufmann.Valdes-Perez, R. E. (1994). Heuristics systematic elucidation reaction pathways.. J.Chem. Informat. Comput. Sci., 34, 976983.Voit, E. O., & Radivoyevitch, T. (2000). Biochemical systems analysis genome-wideexpression data. Bioinformatics, 16 (11), 10231037.Warren, P., Coghill, G. M., & Johnstone, A. (2004). Top botton developmentfuzzy rule-based diagnostic system. Proc. Fourth International WorkshopHybrid Methods Adaptive Systems, Aachen, Germany.877fiJournal Artificial Intelligence Research 32 (2008) 385-417Submitted 12/07; published 05/08Qualitative Comparison Decisions PositiveNegative FeaturesDidier DuboisHelene Fargierdubois@irit.frfargier@irit.frUniversite de ToulouseIRIT- CNRS, 118 route de Narbonne31062 Toulouse Cedex, FranceJean-Francois Bonnefonbonnefon@univ-tlse2.frUniversite de ToulouseCLLE (CNRS, UTM, EPHE)Maison de la Recherche, 5 al. Machado31058 Toulouse Cedex 9, FranceAbstractMaking decision often matter listing comparing positive negativearguments. cases, evaluation scale decisions considered bipolar,is, negative positive values explicitly distinguished. done,example, Cumulative Prospect Theory. However, contrary latter frameworkpresupposes genuine numerical assessments, human agents often decide basisordinal ranking pros cons, focusing salient arguments.terms, decision process qualitative well bipolar. article, basedbipolar extension possibility theory, define axiomatically characterize severaldecision rules tailored joint handling positive negative arguments ordinalsetting. simplest rules viewed extensions maximin maximaxcriteria bipolar case, consequently suffer poor decisive power. decisiverules refine former also proposed. refinements agree principlesefficiency spirit order-of-magnitude reasoning, prevails qualitativedecision theory. refined decision rule uses leximin rankings proscons, ideas counting arguments equal strength cancelling pros cons.shown come special case Cumulative Prospect Theory, subsumeTake Best heuristic studied cognitive psychologists.1. Introductionpersonal experience, also psychological experiments suggest makingdecision often matter listing comparing positive negative featuresalternatives (Cacioppo & Berntson, 1994; Osgood, Suci, & Tannenbaum, 1957; Slovic,Finucane, Peters, & MacGregor, 2002). Individuals evaluate alternatives objectsconsidering positive negative aspects parallel instance, choosingmovie, presence good actress positive argument; noisy theater bad critiquesnegative arguments. bipolar perspective, comparing two decisions amountscomparing two pairs sets, sets pros cons attached one decisionsets pros cons attached other. Cumulative Prospect Theory (Tversky &Kahneman, 1992) explicit attempt accounting positive negative argumentsc!2008AI Access Foundation. rights reserved.fiDubois, Fargier, Bonnefonnumerical setting. proposes compute net predisposition decision,difference two set functions (capacities) taking values positive real line,first one measuring importance group positive features, second oneimportance group negative features. general numerical models, namelybi-capacities (Grabisch & Labreuche, 2005) bipolar capacities (Greco, Matarazzo, &Slowinski, 2002) encompass sophisticated situations criteria independentother. numerical approaches bipolar decision contrast standarddecision theory, account bipolarity phenomenon. Indeed, utilityfunctions defined increasing affine transformation, preservevalue 0.However, cognitive psychologists claimed arguments featured decision process different strengths, decision-makers likely considerdegrees strength ordinal level rather cardinal level (Gigerenzer, Todd,& ABC group, 1999). Individuals appear consider arguments (i.e.,salient ones) making choice, rather attempt exact numerical computation merits decision (Brandstatter, Gigerenzer, & Hertwig, 2006). sum,cognitive psychologists claimed human decision processes likely largelyqualitative well bipolar.last 10 years also witnessed emergence qualitative decision theory Artificial Intelligence (Doyle & Thomason, 1999). instance, qualitative criteria like Waldsrule (Wald, 1950/1971) axiomatized along line decision theory (Brafman& Tennenholtz, 2000) well variants extensions thereof; see survey DuboisFargier (2003). So-called conditional preference networks (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004) introduced easier representation preferencerelations multidimensional sets alternatives, using local preference statements interpreted ceteris paribus style. recent emergence qualitative decision methodsArtificial Intelligence partly motivated traditional stress qualitative representations reasoning methods. also due fact numerical data availablemany AI applications, example point requiring preciseevaluations user (e.g., recommender systems).models use preference relations express statements like decision better decision b agent. However, preference relations cannot dealbipolaritymore precisely, cannot express simple notion people knowgood bad them, judgments orthogonal judgmentsbest given situation. Sometimes, best available choicedetrimental anywayand yet, occasions, even worst option still somewhatdesirable. (Note best worst option statu quo option, i.e.,option making active choice.) fully capture situations, necessaryabsolute landmark reference point model, expresses neutralityseparates positive values negative values. emphasizing qualitativeapproaches decision choice, Artificial Intelligence literature area somewhat neglected fact preference orderings enough express factoption good bad per se.qualitative formalisms capable representing preference exploit value scalesinclude reference points, e.g., fuzzy constraint satisfaction problems (Dubois, Fargier,386fiQualitative Bipolarity Decision& Prade, 1996) possibilistic logic (Benferhat, Dubois, & Prade, 2001). There, meritdecision evaluated different criteria means kind utility functionsmapping bounded ordinal scale whose bottom value expresses unacceptable degreeviolation, whose top value expresses absence violation. Decisionsranked according merit worst evaluation, following pessimistic attitude.kind approach bipolar, handles negative arguments: absolutelandmark expresses unacceptability, neutrality. Neutrality present omission,constraint violated, decision exists could better neutral.Another kind bipolarity accounted Benferhat, Dubois, Kaci, Prade(2006), distinguish prioritized constraints one hand, goals desireshand. Constraints (expressed logical formulas) given prominent role:guide initial selection tolerated decisions. Positive preferences (goalsdesires) taken consideration discriminate among set tolerateddecisions. consequence, positive evaluations (no matter positive) never trumpnegative evaluations (no matter negative).Finally, topic argumentation reasoning gained considerable interestartificial intelligence last ten years so. natural way copinginconsistency knowledge bases (Besnard & Hunter, 2008). Argumentation naturallybipolar nature, since construction arguments consists collecting reasonsderiving proposition reasons deriving negation, proceeding arbitrationarguments various weights (Cayrol & Lagasquie-Schiex, 2005). Howeverclear strength argument inconsistency-tolerant reasoning definednumber. sounds natural adopt qualitative approach bipolar natureargument-based reasoning. Moreover AI-based decision procedures also naturally relyargumentation, facilitate process explicating merits decision(Amgoud & Prade, 2004, 2006).present paper,1 aim proposing bipolar qualitative setting, equippedfamily decision rules, decision based comparison positivenegative arguments whose strength assessed ordinal fashion.insist assumption positive negative evaluations share common scale;evaluations one polarity trump evaluations polarity, e.g.,strong positive (resp. negative) argument win weaker negative (resp. positive)argument. Indeed, precisely idea behind intuitive procedure weighingpros cons, i.e., finding heavier side.This cannot done withoutcommon importance scale. also adopt systematic approach, formalizeaxiomatically characterize set procedures simultaneously ordinal bipolar.paper structured follows. Section 2 introduces framework qualitativebipolar choice. Then, Section 3 presents two basic qualitative bipolar decision rules.Section 4, show basic properties bipolar reasoning expressed axiomatically, and, taking one step further, axioms capture principles qualitativebipolar decision-making. Section 5 studies decision rules decisivebasic rules Section 3, without giving qualitative nature. Finally, Sections 61. paper extended version work Dubois Fargier (2006). analyzes additionaldecision rules, provides full axiomatization cardinality based rule.387fiDubois, Fargier, Bonnefon7 relate rules range approaches, identify avenues futureresearch. Proofs properties representation theorems provided appendix.2. Framework Qualitative Bipolar Decisionsformal framework qualitative bipolar multicriteria decision consist finiteset potential decisions a, b, c, . . . ; set X criteria arguments, viewed attributesranging bipolar scale, say V ; totally ordered scale L expressing relativeimportance criteria groups criteria. article, use simplest possiblebipolar scale V = {, 0, +}, whose elements reflect negativity, neutrality, positivity,respectively. scale, argument X either completely against, totallyirrelevant, totally favor decision D. focus bipolarity,simpler approach many multi-criteria decision-making frameworkscriterion x X rated numerical scale, like multi-attribute utility theory. However,qualitative evaluations often closer human capabilities numerical ones.basis useful see far go rough modelling preferencebipolar situation. Indeed, problem solved rigorous way without resortingnumerical evaluations, sophisticated techniques needed.Let = {x, x(a) #= 0} set relevant arguments decision a. containsarguments matter a, either good thingsbad things. let = {x, x(a) = } set arguments decision a,A+ = {x, x(a) = +} set arguments favor a. Considering sets A+amounts enumerating pros cons a. Thus, comparing decisions bamounts comparing pairs disjoint sets (A , A+ ) (B , B + ). Obviously,B , B + A+ , clearly preferred b wide sense.basic property bivariate monotony bipolar decision rule obey.sake simplicity, assume following X divided two subsets.+X set positive arguments taking value {0, +}; X setnegative arguments taking value {, 0}. simplified model, longerpossible B + #= A+ B #= . This, however, done without lossgenerality affect validity results ordinal setting. Indeed,x whose evaluation may range full domain {, 0, +} duplicated, leadingattribute x+ X + attribute x X . Furthermore, transformationgeneralize framework arguments positive negative side (e.g.,eating chocolate).scale L measuring importance arguments top 1L (full importance)bottom 0L (no importance). Within qualitative approach, L finite.common set functions, make hypothesis importance grouparguments depends importance individual arguments group.assumption independence, levels importance directly attachedelements X function : X ' L, importance grouparguments also derived. (x) = 0L means decision-maker indifferentargument x; (x) = 1L means argument possesses highest level attractionrepulsion (according whether applies positive negative argument). supposednon trivial, is, least one x positive importance level.388fiQualitative Bipolarity DecisionExample 1 (Lucs Holidays). Luc provide us one running examples.considering two holiday destinations, listed pros cons each. Option [a]scenic landscapes (a strong pro), expensive, local airlineterrible reputation (two strong cons). Option [b] non-democratic region, Lucconsiders strong con. hand, option [b] tennis court, disco,swimming pool. three pros, decisive. matter,much arguments. Note Luc give rough evaluation strongpro con is. say gorgeous landscapes, indecent price,terrible reputation airline company, non-democratic governance four arguments comparable importance; swimming pool, tennis disco threearguments comparable importance, important previous ones. Formally,let:X + = {landscape ++ , tennis + , pool + , disco + }X = {price , airline , governance }subset pros,subset cons.Strong arguments landscape ++ , price , airline , governance . argumentstennis + , pool + , disco + weaker. Thus, letting > > 0L , have:(lanscape ++ ) = (price ) = (airline ) = (governance ) =(tennis + )= (pool + )= (disco + )= .Finally options [a] [b] described following sets arguments:Options [a] : A+ = {landscape ++ }= {airline , price }++++Options [b] : B = {tennis , pool , disco } B = {governance }.sum, attribute x X Boolean (presence vs. absence), polarity (itspresence either good bad, absence always neutral), importance (x)L. Now, since interested qualitative decision rules, approach relies twomodelling assumptions:Qualitative Scale: L, big step one level merit next one.Arguments ranked terms order magnitude figure importancemeans mapping .Focalization: order magnitude importance group argumentsprescribed polarity one important arguments group.assumption suits use qualitative scale, means weak argumentsnegligible compared stronger ones.Technically, assumptions suggest use following measure importanceset argumentsOM(A) = max (x).xAterms, simply use qualitative possibility measures (Lewis, 1973; Dubois,1986) interpreted term order magnitude importance.next sections examine several decision rules relying use OM(A),defined balancing pros cons. see Luc example,389fiDubois, Fargier, Bonnefonprefer option [a], prefer option [b], regard two options equallyattractive, find impossible make decision. begin Section 3two basic rules take account important arguments.corresponding ordering complete partial, usually weakly discriminant.due immediate use order-of-magnitude evaluations, leadsrough decision rules. Refinements basic rules proposed Section 5.apply elementary principles simplification (discarding arguments relevantdecisions, sometimes cancelling opposite arguments strength) makingchoice. refinements thus obey form preferential independence.3. Elementary Qualitative Bipolar Decision Rulestwo elementary decision rules Section differ one basic feature: first onetreats positive negative arguments separately; second one allows comparisonrelative strengths positive negative arguments, one side possibly overridingother.decision rule defines preference relation. Since relations presentednecessarily complete transitive, let us recall definitions, prior presentingtwo decision rules.Definition 1. relation ), one define:symmetric part:B ) B B )asymmetric part:, B ) B not(B ) A)incomparability relation: ! B not(A ) B) not(B ) A)) said quasi-transitive , transitive. transitivity ) obviouslyimplies quasi-transitivity, whether complete. converse implication generally hold. relation complete, ! empty. ) said weakorder complete (and thus reflexive) transitive.following, also use notion refinement relation:Definition 2. )$ refines ) A, B : , B ,$ Brefined relation )$ thus follows strict preference ) any, alsomake difference decisions case ) cannotthat is, may happen ,$ BB ! B.3.1 Bipolar Qualitative Pareto Dominance Ruleorder magnitude bipolar set longer unique value L like unipolarcase, pair (OM(A+ ), OM(A )). Pairs generally vectors evaluationseasily compared others using classical principle Pareto comparison. yieldsfollowing rule, assume commensurateness evaluationspositive negative arguments:Definition 3. )Pareto B OM(A+ ) OM(B + ) OM(A ) OM(B ).would ,Pareto conclude Lucs example? Luc strong argument option[a], weak arguments option [b] : OM(A+ ) > OM(B + ). parallel, Luc390fiQualitative Bipolarity Decisionstrong arguments option [a] option [b]: OM(A ) = OM(B ).consequence, ,Pareto B, Luc choose option [a].Let us lay bare details cases Pareto B, ,Pareto B, !Pareto B.B indifferent salient positive aspects wellsalient negative aspects share order magnitude, i.e., OM(A+ ) = OM(B + )OM(A ) = OM(B );B negligible compared (A ,Pareto B) two cases: either OM(A+ ) OM(B + )OM(A ) < OM(B ), OM(A+ ) > OM(B + ) OM(A ) OM(B ).case B ,Pareto described symmetrically.cases, conflict comparable B (A !Pareto B).)Pareto obviously reflexive transitive. collapses Walds pessimistic ordering(Wald, 1950/1971) X = X , optimistic max-based counterpartX = X + . Note )Pareto partial maybe partial. example,decision pros cons, )Pareto concludes incomparable decisionpro con. quite counter-intuitive, shown followingexample.Example 2 (Lucy Riviera estate). short money, Lucy planningspend summer home. offered spend part summerbrothers paradisiacal estate Riviera. inconvenient arrangementLucy finds sister-in-law mildly annoying.Formally, let: X = {estate ++ , inlaw }, (estate ++ ) > (inlaw ).two options, going riviera staying home described follows:Option [a]: A+ = {estate ++ } = {inlaw }.Option [b]: B + = {}B = {}common intuition Lucy case really sister-in-lawmildly annoying, estate fantastic, Lucy likely prefer go ratherstay home. However, )Pareto cannot predict preference finds twooptions incomparable: OM(A+ ) > OM(B + ), OM(A ) > OM(B ).Another drawback )Pareto becomes clear two decisions ordermagnitude one two dimensions:Example 3 (Luka gyms). Luka considering buying membership one twogyms. Option [a] expensive, strong con. Option [b] also expensive,comes small bonus squash court (this small bonusLuka sure yet want use court). hand, option [b] alsodrawback medium importance, is, inconveniently located. Locationimportant price Lukas mind, still important presencesquash court. Formally:X = {squash + , location , price }+).(squash ) < (location ) < (price391fiDubois, Fargier, Bonnefonoptions described follows:Option [a]: A+ = {squash + } = {location , price },Option [b]: B + =B = {price }.follows Definition 3 Luka prefer option [a], OM(A+ ) > OM(B + )whilst OM(A ) = OM(B ). intuitively unsatisfying, however, would expectLuka examine carefully fact option [a] inconveniently located,moderately strong con, rather decide basis weak argument,is, squash court. terms, )Pareto completely obey principleFocalization discussed introduction: argument lower level (the squash court)determine choice even though argument higher level (the location) wouldpointed opposite direction.problem )Pareto partly rooted fact captureassumption positive negative evaluations share common scale. factargument may stronger argument opposite polarity never takenaccount. propose realistic rule captures assumption.3.2 Bipolar Possibility Relationsection, propose decision rule comparing B focuses argumentsmaximal strength B, i.e., level = maxyAB (y) = OM(A B).principle underlying rule simple: argument (resp. B)argument pro B (resp. pro A), conversely. supported decisionpreferred.Definition 4. )BiPoss B OM(A+ B ) OM(B + ).rule decides least good B iff, highest level importance,arguments favor arguments attacking B. Clearly ,BiPoss B iff,highest level, least positive element element B,element element pro B. Obviously, )BiPoss collapses Walds pessimisticordering X = X , optimistic counterpart X = X + . sense,comparison yields straightforward way generalizing possibility orderingsbipolar case.Proposition 1. )BiPoss complete quasi-transitive.terms, strict part relation )BiPoss transitive, associatedindifference relation generally not: BiPoss B B BiPoss C imply BiPossC. instance, let us denote a+ = OM(A+ ), = OM(A ), b+ = OM(B + ), b =OM(B ), c+ = OM(C + ), c = OM(C ). Assume max(a+ , b ) = max(a , b+ )max(b+ , c ) = max(b , c+ ). Assume b+ = b = 1L . two equalities hold regardlessvalues a+ , , c+ , c . values max(a+ , c ) max(a , c+ ) anything.Similarly ,BiPoss B B BiPoss C imply ,BiPoss C counter examplesbuilt setting c+ = c = 1L .case Luc (Example 1), = {landscape ++ , airline , price }, B ={governance , tennis + , swimming + , disco + }. perspective )BiPoss , options equivalently bad, since OM(A+ B ) = OM(B + ). Likewise, case392fiQualitative Bipolarity DecisionLuka (Example 3), )BiPoss regard two gyms equivalently badhigh price. Now, case Lucy (Example 2), remember )Pareto regarded options[a] [b] incomparable, = {estate ++ , inlaws } B empty set.contrast, )BiPoss consider, line common intuition, Lucy prefer goRiviera estate, since OM(A+ B ) > OM(B + ).)BiPoss different arguably less dubious )Pareto . But, shownLuc Lukass examples, rough rule may decisive enough.weakness )BiPoss rooted usual drowning effect possibility theory:argument high importance attached decisions (e.g., ludicrous pricetwo gyms), trump arguments lesser importance (e.g., squash court,also location).2 Variants )BiPoss presented Section 5 overcomedifficulty. Nevertheless, rule )BiPoss alone merit capturing essenceordinal decision-making, shown axiomatic study presented next section.4. Axioms Ordinal Comparison Bipolar Scaleprevious sections, proposed framework decision rules intendcapture essence qualitative bipolar decision-making. present section,adopt opposite strategy, is, formalize natural properties qualitative bipolar preference relation obeyand show frameworksound (it obeys aforementioned properties) also complete. main resultrepresentation theorem stating preference relation satisfies propertiesequivalent )BiPoss .Let ) abstract preference relation 2X , ) B meaning decisionleast good decision B. ) compares sets decisions, call set-relation.First all, introduce general properties (e.g., reflexivity monotony) ) sensiblyobey well-behaved bipolar set-relation, qualitative not. Then, introduceaxioms characterize qualitative bipolar set-relations.4.1 Axioms Monotonic Bipolar Set-RelationsFirst all, preference relation, shall assume minimal working conditionssensible framework, reflexivity (R) quasi-transitivity (QT). 3basic notion bipolar reasoning sets arguments separation Xgood bad arguments. first axiom thus states argument either positivenegative wide sense, i.e., either worse better worse nothing:Clarity Arguments (CA) x X, {x} ) ) {x}.2. drowning effect also work !Pareto , within comparison A+ B + , withincomparison B . Ceteris paribus, destination gorgeous landscapes plus swimmingpool preferred destination gorgeous landscapes pool.3. Although weak, assumptions relaxed relational approaches multicriteria evaluation,aggregation process produces cycles preference relation. Nevertheless, methods,even resulting relation transitive, next step build transitive approximationit.393fiDubois, Fargier, BonnefonOne partition X, differentiating positive, negative null arguments:X + = {x, {x} , }X = {x, , {x}}X 0 = {x, {x}}Now, arguments decision-maker indifferent obviously affectpreference. meaning next axiom, allows forgetX 0 without loss generality:Status Quo Consistency (SQC){x} A, B : ) B {x} ) B ) B {x} .Let us discuss property monotony. Monotony sense inclusion(A B = B ) A) obviously obeyed bipolar framework. Indeed,B set negative arguments, generally holds , B. rather need axioms monotony specific positive negative argumentsbasically, one bipolarcapacities (Greco et al., 2002), expressed comparative way.PositiveNegativeMonotonyMonotonyC, C $ X + , A, B :C, C $ X , A, B :A)BA)BCC \A))B \ C $.B C $.see bivariate monotony property captured pair axioms.Now, another assumption positive side negative sideB taken account comparing them: least good Bpositive negative sides, least good B. expressedaxiom weak unanimity.Weak Unanimity A, B, A+ ) B + ) B ) B.set-relations presented previous Section obviously satisfy weak unanimity.Finally, add classical axiom non triviality:Non-Triviality: X + , X .leads following generalization comparative capacities:Definition 5. relation power set 2X monotonic bipolar set-relationreflexive, quasi-transitive satisfies properties CA, SQC, Non-Triviality, Weakunanimity, Positive Negative Monotony.expected, monotonic bipolar set-relation safisfies bivariate monotony property:using conventions positive negative arguments subsets B,B + A+ (resp. B ), using Clarity Arguments Positive (resp. Negative)Monotony, follows A+ ) B + (resp. ) B ), hence ) B due weak unanimity.Proposition 2. )BiPoss monotonic bipolar set-relation.394fiQualitative Bipolarity Decisionpresent work, interested set-relations entirely determinedstrength polarity individual arguments X. denote X = X {0}set individual arguments X, adding element 0 keep track polarityarguments. basic information arguments captured restriction) X. Formally defined by:x )X {x} ){ y}x )X 0 {x} )0 )X x ) {x}on, call )X ground relation ).agreement existence totally ordered scale weighting arguments,ground relation )X supposed weak order. consequence, minimal conditioncoherence )X preference cannot reversed argument preferred set (resp., least preferred set) replaced even better one (resp., worseone). viewed condition monotony respect )X :Monotony w.r.t. )X X-monotonyA, B, x, x$ {x, x$ } = x$ )X x:{x} , B{x} BB , {x$ }B {x$ }{x$ } , B{x$ } ) BB , {x}B ) {x}natural axiom richer seems. example, implies propertysubstitutability equally strong arguments polaritya kind propertyoften called anonymity social choice decision theory. kindred propertyanonymity also required, positive arguments block negative argumentsstrength. blocking effect depend arguments themselves,position scale. Hence axioms positive negative cancellation:Positive Cancellation(POSC)x, z X + , X , {x, y} {z, y} x X z.Negative Cancellation (NEGC)x, z X , X + , {x, y} {z, y} x X z.makes sense summarize requirements single axiom, callSimple Grounding:Simple Groundingbipolar set-relation ) said simply grounded )X weak order, )monotonic respect )X satisfies positive negative cancellation.395fiDubois, Fargier, BonnefonProposition 3. set-relation )BiPoss simply grounded.44.2 Axiomatizing Qualitative Bipolar Set-Relationsdefinition monotonic bipolar set-relation (Definition 5) general encompasses numerous models, qualitative (e.g., two rules section 3)(e.g., cumulative prospect theory full generality). interested preferencerules derive principles ordinal reasoning only, focus axiomsaccount ordinality.ordinal comparison sets extensively used, especially Artificial Intelligence. basic principle qualitative reasoning Negligibility, assumeslevel importance interpreted order magnitude, much higher nextlower level.Negligibility (NEG)A, B, C X + : , B , C , B C.Axiom NEG already featured around AI, directly formdemanding versions. Let us mention union property nonmonotonic reasoning,Halperns (1997) Qualitativeness axioms (see Dubois & Fargier, 2004, discussion).Lehmann (1996) introduced axiom negligibility inside Savage decision theory axiomatics.Qualitative reasoning generally also comes along notion closeness preservation(which away notion counting):Closeness Preservation (CLO)A, B, C X + : B C B CB, C X + :B )C B BCaxioms proposed justified ordinal reasoning one scale needsconsidered (namely, positive one). sufficient negativescale also needs taken account. need example expressbad consequence B, bad , B C , B, whatever negativearguments C, B still worse C:A, B, C : , B C , B C , B.property meaningful negative sets arguments, trivial X + ; henceintroduced soundly framework.cases sets negative positive elements comparedalso encompassed. example, good cope globally negativeB also win comparison C, B still better C:A, B, C : B , , C B , C.4. Technically, !Pareto also monotonic bipolar simply grounded set-relation. result quiteirrelevant, since rule never compares strengths positive negative arguments.terms, never happens arguments opposite polarities cancel other, condition partsPOSC NEGC thus never fulfilled.396fiQualitative Bipolarity Decisionsimilarly, globally negative (A ) bad outperformed C(C , A) cannot enhanced B ( , B), C , B, i.e.:A, B, C : , B C , C , B.properties expressed following axiom global negligibility:Global Negligibility (GNEG)A, B, C, : , B C , C , Bclassic property purely positive qualitative scalesin case,consequence NEG positive monotony. usually explicitly requiredpositive frameworks. framework positive negative scaleneeded, NEG condition longer sufficient getting GNEG. So, order keepproperty foundation pure order-of-magnitude reasoning, bipolar qualitativeframeworks must explicitly require GNEG.similar argument applies axiom CLO. must expicitly require propertygeneral usual unipolar qualitative scales:Global Closeness Preservation (GCLO)A, B, C, : ) B C ) C ) BProposition 4. set-relation )BiPoss satisfies GNEG GCLO.Propositions 2, 3 4 show bipolar possibility relation simply groundedmonotonic bipolar set-relation satisfying GNEG GCLO. Applying principlesqualitative bipolar reasoning described previous axioms also lead many different less intuitive qualititative rules, instance Pareto rule (see DuboisFargier (2006) full characterization rule). looking simplecomplete decision rule, axioms provide full characterization Biposs preferencerelation. Note strict part rule governed axiom GNEG. indifferencepart includes pure case indifference also cases one would expect incomparabilitydecisions rather indifference proper. debatable partBiposs rule, refined sequel.Theorem 1. following propositions equivalent:1. ) simply grounded complete monotonic bipolar set-relation 2X satisfiesGNEG GCLO.2. exists mapping : X ' [0L , 1L ] ) )BiPoss .detailed proof Theorem 1 provided Appendix A. short, show that,) complete simply grounded, ranking built ranks argumentsrespect strength. Within X + within X , simply obeys informationcaptured )X :x, X + X 0 , x x )X y;397fiDubois, Fargier, Bonnefonx, X X 0 , x x 5X y.relative strength elements different signs deduced blocking effects. Indeed,{x, y} preferred empty set, positive argument must strongernegative one (and symmetrically). two arguments equivalent strengthnone win: {x, y} . Formally:x X + , X , x {x, y} ) ;x X + , X , x ) {x, y}.condition simple grounding ensures weak order. thusencoded mapping : X ' [0L , 1L ] (x) = 0L x X 0 . Noteconstruction valid simple grounded complete bipolar relation,qualitative ones. sequel proof uses axioms closeness negligibilityshow ))BiPoss .4.3 Principles Efficiency Preferential Independencesummary, previous Section shown )BiPoss natural model preferencesbased bipolar orders magnitude. particular, rule accordance GNEGfollow strict preference prescribed ,BiPoss .Nonetheless, seen Section 3.2 )BiPoss suffers drowning effect,usual standard possibility theory. instance, B included evenelements positive, necessarily strictly preferred B. problemrooted fact CLO concludes indifference even cases wouldlike appeal so-called principle efficiency make decision. likemonotony principle, axiom well known positive sets. proper extensionbipolar framework obviously one positive one negative side:Positive efficiency B \ B , , BNegative efficiency B \ B Bset-relations )BiPoss )Pareto also fail obey classical condition preferential independence, also called principle additivity. condition simply statesarguments present B influence decision:Preferential Independence: A, B, C, (A B) C = : ) B C ) B Caxiom well known uncertain reasoning, one fundamental axiomscomparative probabilities (see Fishburn, 1986, survey). Note impliesconditions efficiency (provided completeness holds).Except special cases arguments different levels importance(when )X linear order), new axioms incompatible axioms ordinalitycompleteness transitivity enforced. already true purely positivecase, i.e. X empty (Fargier & Sabbadin, 2005). impossibility resultinsuperable, shown next Section.398fiQualitative Bipolarity Decision5. Refining Basic Order-of-Magnitude Comparisonorder overcome lack decisiveness )BiPoss propose comparison principlesrefine it, is, decisive set-relations ) still compatible )BiPoss ,,BiPoss B , B. following, shall consider refining )Paretoimportant drawbacks.5.1 Implicative Bipolar Decision Ruleimplicative decision rule (Dubois & Fargier, 2005) follows basic focalization principle)BiPoss : comparing B, focuses arguments maximal strength OM(AB) = maxxAB (x) B. adds principle following simple existentialprinciple: least good B iff, level existence arguments favorB counterbalanced existence arguments favor A, existencearguments counterbalanced existence arguments B. Formally,implicative bipolar rule described follows:Definition 6. Let = maxxAB (x);)Impl BOM(B + ) =OM(A ) == OM(A+ ) == OM(B ) =Let us go back examples Luc, Lucy, Luka. Luc case (Example 1),)BiPoss concluded indifference, )Impl rather select Option [a],important arguments decisions, whilst Option [a] supportedimportant pro (there important pro supporting option [b]). Lucy case(Example 2), )Impl follow strict preference )BiPoss send Lucy Riviera.Finally, Luka case (Example 3), highest level argument importancefeatures one con pro, sides, )Impl opine )BiPoss concludeindifference.Let us lay bare cases )Impl B. again, let us denote a+ = OM(A+ ),= OM(A ), b+ = OM(B + ), b = OM(B ). definition, )Impl B fourfollowing situations:1. a+ = b+ = = b ;2. a+ = b+ max(a , b ) > min(a , b );3. = b max(a+ , b+ ) > min(a+ , b+ );4. max(a+ , b ) > max(a , b+ ).thus get following decomposition )Impl rule:Proposition 5.Impl B either a+ = b+ = = b , a+ = b+ > max(a , b ), yet= b > max(a+ , b+ ).!Impl B either a+ = > max(b , b+ ), b+ = b > max(a , a+ ).399fiDubois, Fargier, Bonnefon,Impl B either max(a+ , b ) > max(a , b+ ), a+ = = b > b+ , yetb+ = b = a+ > .easy check )Impl bipolar monotonic set-relation. Like )BiPoss ,positive arguments only, set-relation )Impl collapses max rule.also obeys principle weak unanimity. Moreover:Proposition 6. set-relation )Impl transitive.Since max(a+ , b ) > max(a , b+ ), i.e., ,Biposs B, one conditions ,ImplB, obviously follows that:Proposition 7. Relation )Impl refinement )BiPoss .Finally, situation incomparability !Impl B arises two cases only,a+ = > max(b , b+ ), symmetric case b+ = b > max(a , a+ ). terms,incomparability occurs one two sets displays internal contradictionhighest level, arguments set weak matter. particular,a+ = > 0L ! . instance, dangerous travel exceptional,mysterious part far tropical forest displays internal conflict, knowwhether prefer stay home not. conflict appears also Lucs first option(which attractive high priced). hand, considering non conflicting(and non-empty set) A, either OM(A+ ) > OM(A ) ,Impl (well, travelreasonably dangerous, prefer go), OM(A ) > OM(A+ ) ,Impl(there war near border prefer stay home); two latter cases,Pareto rule would concluded incomparability. range incompleteness)Impl thus different one )Pareto , account notioninternal conflict.)Impl rule interesting theoretic descriptive point view,way handles conflict, fact refines )BiPoss . However,)Impl decisive enough fully overcome drowning effect: salientarguments taken account. instance, expensive hotel without swimmingpool undistinguishable expensive hotel include swimming pool.Even decisive )BiPoss )Impl rule satisfy PreferentialIndependence: like previous rules, collapses Walds criterion (resp., maxrule) negative (resp., positive) sub-scale X (resp., X + ), thus sufferdrowning effect. solve problem, leave family set-relationsfocus set refinements satisfy Preferential Independence, thus efficientpositively negatively.5.2 Efficient Refinements )BiPossfollowing so-called Discri rule adds principle preferential independenceones proposed )BiPoss , cancelling elements appear sets applying)BiPoss rule:Definition 7. )Discri B \ B )BiPoss B \ A.400fiQualitative Bipolarity DecisionNote simplification options B (by cancellation common aspects) inconsistent focalization assumption (i.e., importance grouparguments important argument group). Rather, focalizationapplies options simplified delete arguments that,common options, contribute making difference them.Luka case (Example 3) typical example way )Discri outperforms )BiPoss .case, A+ = {squash + }, = {location , price }, B + empty, B ={price }. Recall )BiPoss concludes indifference two optionscommon strong cons. However, cancelling away price argumentpresent options, )Discri choose B, longer pros cons,described moderately strong con weak pro. LucLucy cases (Examples 1 2), two options feature common argument,)Discri therefore share preferences )BiPoss (indifference Luc case, goingRiviera Lucy case).)Discri complete quasi-transitive (its strict part, ,Discri transitive symmetric part necessarily transitive). Unsurprisingly, X = X + , )Discricollapse max rule, rather discrimax procedure (Brewka, 1989; Dubois& Fargier, 2004), is, comparison OM(A \ B) OM(B \ A).already noted, )Discri simply cancels argument appearingB. One could accept cancellation positive (resp. negative) argumentanother positive (resp. negative) argument B, long two argumentsshare order magnitude. yields following )BiLexi rule (and, later on,)Lexi rule), based levelwise comparison cardinality. argumentsB scanned top down, level reached numbers positivenegative arguments pertaining two alternatives different; then, setleast number negative arguments greatest number positive ones preferred.Note perfectly legitimate qualitative decision rule count argumentsstrength. simply means one argument one side cancels one argumentstrength side. people seem do. mustremain qualitative scale expressing importance argumentsdecision based; would unreasonable consider number argumentsgiven importance level not, indeed, number, is, natural integer.Let us first define -section set arguments:Definition 8. level L:= {x A, (x) = } -section A.+A+= X (resp. = X ) positive (resp. negative) -section.Now, lexicographic two-sided partial ordering (called Levelwise Bivariate TallyingBonnefon et al.(in press)) introduced:Definition 9.+)BiLexi B |A+| |B | |A | |B |,+= Argmax{ : |A+| #= |B | |A | #= |B |}.easy show )BiLexi reflexive, transitive, complete. Indeed,decisive level ( ) one set wins positive side set winsnegative side, conflict revealed procedure concludes incomparability.401fiDubois, Fargier, Bonnefon)BiLexi rule well-behaved respect Lucy Luka examples. Lucygo Riviera, Luka chose best located gym. Luka case, {price }preferred {price , location , squash + }: price argument featuredoptions cancelled away, {location , squash + } moderately strong con judgedworse . Luc story, difficulty dilemma clearly pointedrule. Remember option [a] involves three arguments highest level importance,landscape ++ , airline , price , whilst option [b] involves one, governance .Since |{governance }| < |{airline , price }| |{landscape ++ }| > ||, )BiLexi concludes incomparability, reflecting difficulty decision.5 generally,)BiLexi rule concludes incomparability conflict positive side negative side decisive level. descriptive point view,range incomparability necessarily shortcoming )BiLexi .Now, one assume form compensation positive negative argumentslevel within given option, following refinement )BiLexi obtained,take care complex cases Lucs:Definition 10.)LexiB L \ 0L!+> , |A+| | | = |B | |B |++|A | | | > |B | |B |.Let us look one time Lucs dilemma. One strong pros Option [a]cancelled one strong cons, discarded. remains one strong conoption: option wins level. Therefore, procedure examinesnext lower importance level. level, three weak pros con option[b], neither pro con option [a]: Option [b] winsand first timearticle decision rule yields strict preference Luc case. LucyLuka examples, )Lexi breaks ties )BiPoss )BiLexi )Discri did: Lucy goriviera, Luka chose best located gym.three rules proposed Section obviously define monotonic bipolar set-relations.refines )BiPoss satisfies Preferential Independence. rankedleast decisive (,Lexi ), moreover complete transitive.Proposition 8. ,BiPoss B ,Discri B ,BiLexi B ,Lexi Bdecision rules ,BiLexi ,Lexi satisfy strong form unanimity, namely,A+ ) B + ) B moreover least one , B A+ , B + holds,, B follows. However kind strong unanimity alone strong enoughachieve good sound discrimination among options (for instance, key-feature)Pareto ). fact, ,BiLexi ,Lexi satisfy much stronger efficiency properties, sinceBiLexi B B number positive negative argumentslevel importance, Lexi B B numberadditional arguments polarity level importance cancellationequally important opposite arguments inside option.5. take opportunity insist fact incomparability confused indifference, signals complex situation. situations incomparability, decision-makerperplex alternative look better other, choosing random wouldsatisfactory way out, choice leads reason regret. case indifference,alternatives equally attractive (or repulsive), random choice makes sense.402fiQualitative Bipolarity DecisionNotice restriction )BiLexi )Lexi X + amounts leximaxpreference relation (Deschamps & Gevers, 1978). thus use classical encodingleximax (unipolar) procedure sum finite case (Moulin, 1988). capacityset-function monotonic inclusion. easy show that:Proposition 9. exist two capacities + 2X+2X that:)Lexi B + (A+ )! (A ) + (B + ) (B )+ (A+ ) (B + ))BiLexi B+ (B ) (A )example, denoting 1 = 0L < 2 < < l = 1L l elements L, use(integer-valued) capacity:"+ (A+ ) =|A + | |X|i .L#set-function said big-stepped capacity |X|i > j<i |X|j (see Dubois& Fargier, 2004). similarly use similar big-stepped capacity representimportance sets negative arguments:"(A ) =|A| |X| .Lproposition means )Lexi )BiLexi rankings particular cases (usingbig-stepped probabilities) Cumulative Prospect Theory decision rule (Tversky &Kahneman, 1992). See Section 6 discussion.summary, )Lexi complies spirit qualitative bipolar reasoningefficient. meantime, advantages numerical measures (transitivityrepresentability pair functions). Finally, order make full case attractive)Lexi , show )Lexi rule time (i) refines )BiPoss ,(ii) weak order, (iii) satisfies principle preferential independence withoutintroducing bias order X.Definition 11. refinement )$ monotonic bipolar set relation ) unbiasedpreserves ground relation latter: )$X )X .order prove claim, let us first establish following proposition appliestransitive relation satisfying preferential independence (note completenessrequired). proposition establishes principle anonymity, according twoequivalent subsets interchangeable provided overlap setsarguments:Proposition 10. Let ) transitive monotonic bipolar set-relation satisfies preferential independence. A, B, C, (C D) = C D:C ) B ) B;B ) C B ) D.403fiDubois, Fargier, Bonnefondirect consequence property set arguments canceladded another set without changing preferences involving latter (just let):Corollary 1. Let ) transitive monotonic bipolar set-relation satisfies preferentialindependence. A, B, C C = , C ,) B C ) B;B ) B ) C.Another noteworthy consequence Proposition 10 extended principle preferential independence:Corollary 2. Let ) transitive monotonic bipolar set-relation satisfies preferentialindependence. A, B, C, (A B) (C D) = C D:) B C ) BFinally, Proposition 10 allows show that6 :Corollary 3. Let ) transitive monotonic bipolar set-relation satisfies preferentialindependence. A, B X, x, X x/ A,/ B, {x} { y}:) B {x} ) B {y}three results instrumental proving third representation resultpaper:Theorem 2. Let ) monotonic bipolar set-relation order-of-magnitudedistribution elements X. following propositions equivalent:1. ) complete, transitive, satisfies preferential independence unbiased refinement )BiPoss ;2. ) )Lexi .theorem concludes argumentation favor )Lexi , shownrule satisfy natural principles order-of-magnitude reasoning decisive practical (e.g., completeness, transitivity, representability pair functions).6. Related Workssection, relate decision rules general approach several traditionsresearch, Cognitive Psychology well Artificial Intelligence.interesting connection made decision rules TakeBest heuristic extensively studied psychologists since introduction6. Note that, Corollary 3, neither condition {x} B = condition {y} = required;main difference Corollary 2; hand, Corollary 3 restricted additionsingletons.404fiQualitative Bipolarity DecisionGigerenzer Goldstein (1996). Take Best so-called fast frugal heuristiccomparing two objects based values series binary cues. fastfrugal focuses limited subset available information order makedecision; heuristic value shows reasonable accuracy compared lessfrugal comparison rules (see Gigerenzer et al., 1999; Katsikopoulos & Martignon, 2006,simulations empirical tests). consequence, evolutionary psychologists arguedTake Best adapted strategy kind binary cue-based comparisoninvestigating.Take Best requires arguments consideration different ordersmagnitude, ranked lexicographically: exist x,(x) = (y). Furthermore, argument considered generate polar opposite:pro x+ attached option [a], con x (x ) = (x+ )automatically attached options feature x+ (and reciprocally).applying Take Best amounts scanning arguments top-down, startingimportant, stop soon argument found favors one optionother. Interestingly, applied linearly ranked arguments, )Discri ,)BiLexi , )Lexi rules coincide Take Best heuristic. new decisionrules proposed able account choice situations Take Bestheuristice.g., several criteria share degree importance. sense,natural extensions Take Best qualitative rule advocated psychologists.Originally descriptive model risky decisions, Cumulative Prospect Theory (Tversky& Kahneman, 1992) provides another psychological account decisions involving positivenegative arguments. Contrary Take Best approach, oriented towardsquantitative evaluation decisions. key notions Cumulative Prospect Theoryindividuals assess outcomes relatively reference point, rather absolutely; concerned losses gains, ceteris paribus;individuals overweight extreme outcomes occur small probability.technically, Cumulative Prospect Theory assumes reasons supportingdecision reasons measured means two capacities +; + reflects importance group positive arguments, importancegroup negative arguments. higher + , convincing set positivearguments; conversely, higher , deterring set negative arguments. Furthermore, Cumulative Prospect Theory assumes possible mapevaluations so-called net predisposition score, expressed single scale:X, NP(A) = + (A+ ) (A ),A+ = X + , = X . Alternatives ranked according netpredisposition: )NP B + (A+ ) (A ) + (B + ) (B ). Proposition 9(Section 5) actually shows )Lexi particular case rule (using big-steppedcapacities). Interestingly, )BiPoss rule foreign comparison net predispositions. precisely, viewed ordinal counterpart net predispositioncomparison. Let us first write )NP B + (A+ ) + (B ) + (B + ) + (A ). Now,immediate changing + max, changing + inequalitypossibility measures, yields )BiPoss rule.405fiDubois, Fargier, BonnefonCumulative Prospect Theory variants assume kind independence X + X , assumption always hold. Bi-capacities (Grabisch &Labreuche, 2002, 2005) introduced handle non-separable bipolar preferences:measure defined Q(X) := {(U, V ) 2X , U V = }, increases (resp., decreases) addition elements U (resp., V )7 . Bi-capacities originally stemmedbi-cooperative games (Bilbao, Fernandez, Jimenez Losada, & Lebron, 2000),players divided two groups, pros cons: player x sometimespro, sometimes con, cannot simultaneously. context bipolardecision, typically set U = A+ V = measure attractivity(A+ , ). net predisposition Cumulative Prospect Theory recovered letting(A+ , ) = + (A+ ) (A ) = NP(A).Since comparison net predispositions (and, generally, bi-capacities bicooperative games) systematically provides complete transitive preference, failcapture large range decision-making attitudes: contrasting affects make decisiondifficult, comparison objects bipolar evaluations systematicallyyield complete relation? might imply incompatibilities. bi-capacitiesgeneralized means bipolar capacities (Greco et al., 2002). idea underlyingbipolar capacities use two measures, measure positiveness (that increasesaddition positive arguments deletion negative arguments) measurenegativeness (that increases addition negative arguments deletionpositive arguments), combine them. terms, bipolar capacityequivalently defined pair bi-capacites + , namely by: (A) =( + (A+ , ), (A , A+ )). preferred B respectpreferred B respect + is, according sole Paretoprinciple. allows representation conflicting evaluations leads partialorder.approach provides clear qualitative counterparts Cumulative Prospect Theory,bi-capacities, bipolar capacities certain extent. Indeed, )Lexi belongsCumulative Prospect Theory family, thus represented bi-capacity,generally bipolar capacity. contrast, )Pareto obviously belongs familybipolar capacities. rules )BiPoss , )Discri, )BiLexi )Impl spiritbi-capacities bi-cooperative games. First all, contrary models,decision rules provide complete preorder alternatives arguecontrary conflicts positive negative arguments strengthlead conflict, thus incomparable alternatives. decision rules, like )BiPosscannot understood comparison (A+ , ) (B + , B ); ruleindeed compares max(OM(A+ ), OM(B )) max(OM(B + ), OM(A )). )BiPoss ,right scheme would rather comparison (A+ , B ) (B + , ).7. bibliography bi-capacities bi-cooperative games see work Grabisch colleagues(Grabisch & Labreuche, 2005; Grabisch & Lange, 2007). developments notionsconcern computation Shapley value, definition Choquet integrals. apartCumulative Prospect Theory, instances general framework presented. pointprobably that, provides complete transitive comparison, cannot highlight presenceconflicting information.406fiQualitative Bipolarity Decisionsuggests neither framework bi-cooperative games, ones bi-capacitiesbipolar capacities yet general enough.Finally, notice couched results terminology borrowing argumentation decision theories, indeed consider relevant both.paper also relevant argumentative reasoning evaluation sets argumentsinference processes (Cayrol & Lagasquie-Schiex, 2005). connection reasoningdecision setting argumentation laid bare Amgoud, Bonnefon, Prade(2005). propose extensive framework arguments constructedknowledge base base logically defined criteria. Arguments evaluated termscertainty, strength degree attainment corresponding criterion. Assumingcommon scale three aspects, separately compare individual positive arguments, negative ones, using simple aggregation weights, never comparearguments different polarity. idea using logical arguments compare decisionsfirst discussed setting possibilistic logic Amgoud Prade (2004). AmgoudPrade (2006) elaborated framework, explicit use knowledge basegoal base construction arguments builds bridge argumentativereasoning qualitative decision uncertainty.7. Conclusionpaper focused particular class bipolar decision making situations, namelyqualitative rather quantitative. proposed extensionpossibility theory handling sets containing arguments considered positivenegative. laid bare importance )Lexi )BiLexi decision rulesqualitative bipolar decision-making. rules separately evaluate positive negative sets arguments, means big-stepped capacities + . Then, )Lexirule aggregates two measures agreement Cumulative Prospect Theorys netpredisposition. contrast, )BiLexi rule merge positive negativemeasures, allowing expression conflict incomparability. sense, tworules combine best two worlds: agree spirit order magnitudereasoning, decisive efficient basic rules )BiPoss ,offer practical advantages quantitative Cumulative Prospect Theorye.g.,transitivity representability pair functions.paper adopted prescriptive point view sense rulesstudied respect properties qualitative theory bipolar decision-makingobey. representation theorems Sections 4 5 show use )BiP oss)Lexi well-behaved ones dealing qualitative bipolar information.parallel, tested descriptive power rules, i.e., accuracy predictingbehavior human decision makers. experimental results (Bonnefon et al., press;Bonnefon & Fargier, 2006) confirm ,BiPoss basic ordinal decision-making rule:,BiPoss yields strict preference, uncommon human decision-makers disagreepreference. Furthermore, results strongly suggest )Lexi decision rulegenerally followed decision-makers: )Lexi accurately predicted nearly 80% 2,000decisions collected, always one decision-makers individually agreed407fiDubois, Fargier, Bonnefonmost. Finally, results suggest human decision-makers sometimes find decisionsincomparable; do, usually situations )BiLexi would predict so.present paper, address question computational complexitycomparing two objects bipolar decision rule. Actually, presence bipolarinformation change range complexity comparison objects. detailedcomplexity study scope paper, least sayrules presented paper, comparison two alternatives polynomialnumber criteria X. comparison indeed relies computation strengthfour subsets X computation sometimes preceded simple deletion step(for rules based discri- lexi- comparison). computation strengthset linear, since performed means aggregation operator. complexitycomparing two alternatives )BiPoss )Lexi rules instance O(Card(X))(it computed simple aggregation individual strengths). Finally, noticerules provide transitive strict preference, cycle appear maycase CP-nets. combinatorial alternatives considered, could easilyuse branch bound algorithm looking best alternative(s); problemoptimization bipolar aggregation harder unipolar one:corresponding decision problem remains NP-complete (Fargier & Wilson, 2007).concludes study qualitative bipolar reasoning. However, awareunresolved issues paper raised. example, Section 6 suggestsframework bi-capacities, even bipolar capacities, rich enough accommodatefull range qualitative bipolar rules introduced. Secondly, resultsestablished within restricted framework, relevant criterion either completepro complete opponent w.r.t. decision, spirit bi-cooperative games.clearly simpler approach usual multi-criteria decision-making frameworks,x X full-fledged criterion rated bipolar utility scale like Lx = [1x , +1x ],containing neutral value 0x . Thus, natural extension present work wouldaddress qualitative bipolar criteria whose satisfaction matter degree.Appendix A. Proofssimplify notations, let a+ = OM(A+ ), = OM(A ), b+ = OM(B + ), b = OM(B ),c+ = OM(C + ), c = OM(C ). Hence OM(A) = max(a+ , ), OM(A+ B ) = max(a+ , b )on.Proposition 1. proof completeness trivial, OM(A+ B ) OM(B + )always compared. prove transitivity ,BiPoss , let us assume max(a+ , b ) >max(a , b+ ) max(b+ , c ) > max(b , a+ ). Then, letting b = max(b+ , b ), getmax(a+ , b, c ) > max(a , b, c+ ). consequence, max(a+ , c ) > b. Hence, max(a+ , c ) >max(a , b, c+ ) max(a , c+ ).Proposition 2. Quasi-transitivity proved Proposition 1. Positive negative monotony,well SQC, follow monotony OM, possibility measure: i.e.,U, V, OM(U V ) OM(V ). Non-triviality )BiPoss obtained non-triviality(there exists x (x) = 0), implies OM((X + )+ (X ) ) = OM(X +408fiQualitative Bipolarity DecisionX ) > 0 OM((X + ) (X )+ ) = OM() = 0. Clarity argument also trivial: x X + OM({x}+ () ) = (x) OM({x} ()+ ) = OM() = 0:{x} )BiPoss . x con, get way {x} 5BiPoss . x null importance, get OM({x}+ () ) = OM({x} ()+ ) = OM() = 0: {x} BiPoss .prove weak unanimity, suppose A+ )BiPoss B + )BiPoss B . ObviouslyA+ )BiPoss B + OM(A+ ) OM(B + ) )BiPoss B OM(B ) OM(A ).Hence OM(A+ B ) OM(B + ) : )BiPoss B.Proposition 3. restricted singletons, )BiPoss weak order ranks positivearguments decreasing values , null arguments ( = 0), negativearguments increasing value . ranking defines complete transitive relation.proves relation )X induced )BiPoss weak order. Axioms POSC easycheck, since {x+ , } (resp., {z + , } BiPoss ) (x+ ) = (y ) (resp.(z + ) = (y )). {x+ , } BiPoss {z + , } imply (x+ ) = (z + ). So,{x+ } BiPoss {z + }, i.e., x+ X z + . proof NEGC similar. X-monotony)BiPoss shown follows. Let A, x, x$ {x, x$ } = x$ )X x. Threecases possible:1. x X + . x$ X + (x$ ) (x).{x} ,BiPoss B: OM(A+ {x} B ) > OM(A B + ). Since (x$ )(x), get OM(A+ {x$ } B )OM(A B + ): {x$ } ,BiPoss B.{x} BiPoss B: OM(A+ {x} B ) = OM(A B + ). Replacing xx$ , i.e., (x) (x$ ), first OM level increases, get OM(A+ {x$ }B ) OM(A B + ): {x$ } )BiPoss B.B , {x$ }, OM(B + ) > OM(A+ {x$ } B ). Replacing x$x, i.e., (x$ ) (x), second OM level decreases, get OM(B + ) >OM(A+ {x} B ), i.e. B ,BiPoss {x}.B {x$ }, OM(B + ) = OM(A+ {x$ } B ). Replacing x$x, i.e. (x$ ) (x), second OM level decreases, OM(B + )OM(A+ {x} B ), i.e. B )BiPoss {x}.2. x$ X . x X (x) (x$ ). kind four-case proofcarried out.3. x$ X + x X .{x} ,BiPoss B, i.e., OM(A+ B ) > OM(A B + {x}), followsOM(A+ B {x$ }) > OM(A B + ), {x$ } ,BiPoss B.Similarly, A{x} BiPoss B, i.e., OM(A+ B ) = OM(A B + {x}), followsOM(A+ B {x$ }) OM(A B + {x}), OM(A+ B {x$ })OM(A B + {x}), implies OM(A+ B {x$ }) OM(A B + ). Hence(x$ positive) {x$ } )BiPoss B.B , {x$ }, means OM(B + ) > OM(A+ {x$ } B ) since x$positive. Obviously, OM(B + {x}) OM(B + ) OM(A+{x$ } B ) OM(A+ B ). Hence OM(B + {x}) > OM(A+ B ):B ,BiPoss {x}.409fiDubois, Fargier, BonnefonSimilarly, B BiPoss {x$ } have: OM(B + {x}) OM(B + ) =OM(A+ {x$ } B ) OM(A+ B ): B )BiPoss {x}.Proposition 4.)BiPoss satisfies GCLO Recall )BiPoss B max(a+ , b ) max(b+ , )C )BiPoss max(c+ , ) max(d+ , c ). Hence, max(a+ , b , c+ , )max(b+ , , d+ , c ), i.e., C )BiPoss B D.)BiPoss satisfies GNEG Recall ,BiPoss B max(a+ , b ) > max(b+ , )C ,BiPoss max(c+ , ) > max(d+ , c ). Hence: max(a+ , b , c+ , ) >max(b+ , , d+ , c ), i.e., C ,BiPoss B D.Theorem 1. Let us first build . CA, singleton {x} comparable . X + ={x, {x} , }, X = {x, {x} } X 0 = {x, {x} } soundly defined.Let us define relation X follows:x, X 0 : x x.x, X + : x {x} ) {y}x, X : x {y} ){ x}x X + , X : x not( , {x, y})x X +, X 0 : x >x X , X 0 : x >axiom CA, X + , X X 0 disjoint. previous definition thus wellfounded. complete definition. prove transitive. Supposex z let us perform following case analysis:x, y, z X + : x z trivial )X transitive identifiedwithin X + .x, y, z X : x z trivial )X transitive identifiedwithin X .x X 0 : x means STQ also X 0 , turn implies z X 0 .So, STQ again, x z.X 0 : z implies STQ z also X 0 . So, STQ again, x z.z X 0 : x z always true (by status quo consistency again).x X + , X z X . definition, x means {x, y} )z means {z} ){ y}. X-monotony replace z without reversingpreference: {x, z} ) , i.e. x z.410fiQualitative Bipolarity Decisionx X + , X + z X : proof x z similar one previousitem.X + x X z X . Suppose {x, y} 5 (x y), {z, y} ) (y z){x} > {z} (z > x negative arguments). {z, y} , , X-monotony implies{x, y} , (thus contradiction). , {x, y} X-monotony implies , {z, y}(second contradiction). Last case, {x, y} {z, y} , POSC implies{x} { z} (last contradiction). So, z > x hold thus, completeness, x z.X x X + z X + . proof x z similar one previousitem, using NEGC instead POSC.So, weak order. encoded distribution : X ' [0L , 1L ], [0L , 1L ]totally ordered scale. Level 0L mapped elements X0 . Now, showequivalence ) relation )BiPoss induced , namely prove) B OM(A+ B ) OM(A B + ). Since ) complete, amounts showingOM(A+ B ) = OM(A B + ) implies B OM(A+ B )OM(A B + )implies , B. Let us first prove OM(A+ B )OM(A B + ) implies , B. Supposeelement highest value A+ B x A+ . So, , {x, y} ,w B + , {x} ,{ w}. GNEG get {x}A , B + positive negative monotonyimply A+ , B + B . element highest value A+ Bv B w B + , {w, v} , {y} , {v}. GNEGget {v} B + positive negative monotony imply A+ , B + B .Let us prove OM(A+ B ) OM(A B + ) implies ) B. Supposeelement highest value A+ B x A+ . So, , {x, y} )w B + , {x} ) {w}. GCLO get {x} ) B + positive negative monotonyimply A+ ) B + B . element highest value A+ Bv B w B + , {w, v} 5 , {y} ){ v}. GCLO get{v} B + 5 positive negative monotony implies A+ ) B + B .Proposition 5. Consider four cases identified text. Clearly situation 1 correspondscase Impl B. case 4, strict dominance ,Impl B prevails. Case 2 maylead three different conclusions. Equivalence arises a+ = b+ max(a , b ):cons low level w.r.t pros, taken account indifferenceprevails based pros. a+ = b+ = b > , )Impl B holdsB )Impl A. So, ,Impl B since arguments weak. symmetry,a+ = b+ = > b+ concludes B ,Impl A. Case 3 handled similar manner:equivalence arises = b > max(a+ , b+ ); = b = a+ > b+ , ,Impl Bholds since arguments B weak; = b = b+ > a+ , B ,Impl Finally,"Impl B concluded neither )Impl B B )Impl A. arises two casesonly, a+ = > max(b , b+ ) symmetric case b+ = b max(a , a+ ).Proposition 6. Assume )Impl B B )Impl C. Using conventions, consider:1. following situations ensuring )Impl B:411fiDubois, Fargier, Bonnefon(a) a+ = b+ = = b ;(b) a+ = b+ max(a , b );(c) = b max(a+ , b+ );(d) max(a+ , b ) > max(a , b+ ).2. following situations ensuring B )Impl C following group:(a) b+ = c+ = b = c ;(b) b+ = c+ max(b , c );(c) b = c max(b+ , c+ );(d) max(b+ , c ) > max(b , c+ ).Combining one condition first group one second group yields onecorresponding conditions ensuring )Impl C. instance,Combining conditions 1b 2b yields a+ = c+ max(a , b , c ) max(a , c ).Combining conditions 1b 2c yields a+ max(a , c ) c max(a+ , c+ ).Hence a+ max(a , a+ , c+ ), c max(a , c , c+ ) a+ = c . Hence a+ = cmax(a , c+ ). inequality strict condition d. a+ = c = max(a , c+ ),condition b c.Combining condition 1d condition 2b yields max(a+ , b ) > max(a , c+ )c+ max(b , c ). Hence a+ > max(a , c+ ).cases handled similarly.Proposition 7. ,BiPoss B max(a+ , b ) > max(a , b+ ) thanksProposition 5 implies ,Impl+ B.Proposition 8.Suppose ,BiPoss B: x A+ B x B + , (x )(x).many arguments satisfy property, let x one maximizing . x thusB +. So, either A+ \ B + B \ . Since A+ \ B + = (A \ B)+B \ = (B \ A) , write x (A \ B)+ (B \ A) . hand(B \ A)+ (A \ B) B + element B + higher degreex . So, OM((B \ A)+ (A \ B) ) < (x ). So, ,Discri B. proves )Discrirefines )BiPoss .Suppose ,Discri B: x A+ \ B + B \ x B + \ A+\ B , (x ) > (x). many elements satisfy property, let x onemaximizing . So, level > (x ), A+ \ B + B \ empty; thus+levels, A+= B = B , imply equality cardinalities.hand, level = (x ), element B + \ A+ \ B .So, |(B + \ A+ ) | = 0 |(A \ B ) | = 0. least one elementA+ \ B + B \ |(A+ \ B + ) | 1 |(B \ ) | 1 (or even both).412fiQualitative Bipolarity DecisionSuppose |(A+ \ B + ) | 1: |(B + \ A+ ) | = 0 adding common+elements, get |B+ | = |(A+ B + ) |. Since |(A+ \ B + ) | 1 get |A+||B |.|(A \ B ) | = 0 adding common element get |A | = |B |thus |A | |B |. ,BiLexi B. get ,BiLexi B way|(B \ ) | 1. Hence )BiLexi refines )Discri .+Finally, suppose ,BiLexi B, i.e., level higher , |A+| = |B ||A | = |B | level , difference favor A. necessarily,+level higher , |A+||A | = |B ||B |. Let first suppose level++, difference made positive scale, i.e., |A+| > |B | |A | |B |.++Summing inequalities get: |A | |A ||B | |B |. ,Lexi B.difference rather made negative side, get ,Lexi B similar way.So, ,Lexi B. Hence )Lexi refines )BiLexi .Proposition 10 . C (C D) = , preferential independence impliesC D. Then, transitivity: C ) B implies ) B; ) B impliesC ) B; B ) C implies B ) D; B ) implies B ) C.Corollary 1 . Since C C = , apply principle preferentialindependence get C A. Then, transitivity, ) B implies C ) B.Conversely, C ) B C implies, also transitivity, ) B. Similarly,B ) AC A, transitivity implies B ) AC; B ) AC AC A,transitivity implies B ) A.Corollary 2 . axiom preferential independence, (A B) C = :) B C ) B C. applying Proposition 10 B (C D) = ,) B C ) B D.Corollary 3.Case x B: Proposition 10 replace x B get ) B) (B \ {x}) {y}. Let us apply preferential independence add xsides ): ) (B \ {x}) {y} {x} ) (B \ {x}) {y} {x}, i.e.,) (B \ {x}) {y} {x} ) B {y}, thus ) B {x} ) B {y}.Case x # B: preferential independence means ) B {x} ) B {x}Proposition 10 used get ) B {x} ) B {y}.Theorem 2 . easy show item 2 implies item 1. already seen )Lexicomplete, transitive, satisfies preferential independence, refines )BiPoss . alsoeasy show grounding relations X induced relations equivalent,i.e., )Lexi unbiased refinement )BiPoss .Let us prove item 1 implies item 2. Let ) complete transitivemonotonic bipolar set-relation satisfies preferential independence, unbiased.refinement )BiPoss . Since ) unbiased refinement )BiPoss , )X )BiPossX413fiDubois, Fargier, Bonnefonequivalently defined . notions -section, positive -section negative section thus well defined.+1. Let us first suppose that, , |A+| |A | = |B | | B |. show, B . Two cases possible:Case |A+| |A | 0: possible partition n = |A | pairs+++{x , x } n = |A | |A | singletons {x }. Similarly, possible partition B nB = |B | pairs {y + , } n singletons {y + }the n+A, since |A+| | | = |B | |B |. So, = , use Proposition+3 add n singletons {x } n singletons {y + } sideequivalence (left side x+ , right side + ). Corollary 1 allowsus add pairs {x+ , x } left side, pairs {y + , } right side.arrive B .Case |A+| |A | 0: proof similar. possible partition n =++|A| pairs {x , x } n = |A | | | singletons {x }. Similarly,B+possible partition B n = |B | pairs {y , } n singletons {y }.So, = , use Proposition 3 Corollary 1 allow us add pairs{x+ , x } remaining singletons {x } left side, pairs {y + , }remaining singletons {y } right side. arrive B .-sections B pairwise disjoint.ThanksCorollary 2$$sum equivalences B get B , i.e., B.2. Let us suppose > 0L (i) > , |A+| |A | =++|B+ | | B |$ (ii) |A | $|A | > |B | |B |. proof similar onebefore, get > > B . Three cases possible:Case |A+| |A | 0: possible partition n = |A | pairs+++{x , x } n = |A | | | singletons {x }. Similarly, possiblepartition B nB = min(|B+ |, |B |) pairs {y + , } singletons{y + } number m$ singletons {y }, max(m, m$ ) = 0.|B+ | | B | < |A+| | | = n, < n. Let x one n positiveBiPoss+singletons partition . Obviously, x << B,BiPossthus monotony x < ,< B . x < , < B(this refinement hypothesis). add n singletons x+n singleton + left right side strict preference without modifying (this Proposition 3). monotony, add singletons, any, remaining n 1 singletons x+ , any. Since pairs{x+ , x } (resp., {y + , }) equivalent , added left(resp., right) side without modifying inequality (we use Corollary 1). get, B .Case |B+ | | B | 0: proof similar previous one, negative. partition B maximum number pairs, given number nnegative singletons. also partitioned maximum number pairs,414fiQualitative Bipolarity Decisionpossibly < n negative singleton (the two conditions exclusive)positive singletons show < , < B {y }, addnegative singletons x negative singletons respective sides,add remaining negative remaining positive x+ . pairs {x+ , x }(resp., {y + , }) equivalent , added left (resp., right)side without modifying inequality. get , B .Finally, |B+ | |B | > 0 |A+| | | < 0 incompatible fact++|A | |A | > |B | |B |.thus get , B . -sections B pairwise disjoint.Thanks Corollary 2 we$can then$sum equivalences B , >string preference get , B , i.e. , B.3. shown ,lexi B = , B lexi B = B.relations complete, means equivalent: )lexi B ) B.ReferencesAmgoud, L., Bonnefon, J. F., & Prade, H. (2005). argumentation-based approachmultiple criteria decision. Lecture Notes Computer Science, 3571, 269280.Amgoud, L., & Prade, H. (2004). Using arguments making decisions: possibilisticlogic approach. M. Chickering & J. Halpern (Eds.), Proc. 20th ConferenceUncertainty Artificial Intelligence (UAI04) (pp. 1017). Menlo Park, CA: AUAIPress.Amgoud, L., & Prade, H. (2006). Explaining qualitative decision uncertaintyargumentation. Proc. 21st National Conf. Artificial Intelligence. MenloPark, Ca: AAAI Press.Benferhat, S., Dubois, D., Kaci, S., & Prade, H. (2006). Bipolar possibility theorypreference modeling: Representation, fusion optimal solutions. InternationalJournal Information Fusion, 7, 135150.Benferhat, S., Dubois, D., & Prade, H. (2001). Towards possibilistic logic handlingpreferences. Applied Intelligence, 14 (3), 303317.Besnard, P., & Hunter, A. (2008). Elements Argumentation. Cambridge, Mass.:MIT Press.Bilbao, J. M., Fernandez, J. R., Jimenez Losada, A., & Lebron, E. (2000). Bicooperativegames. J. M. Bilbao (Ed.), Cooperative games combinatorial structures (p.23-26). Dordrecht: Kluwer Academic Publishers.Bonnefon, J. F., Dubois, D., Fargier, H., & Leblois, S. (in press). Qualitative heuristicsbalancing pros cons. Theory & Decision.Bonnefon, J. F., & Fargier, H. (2006). Comparing sets positive negative arguments: Empirical assessment seven qualitative rules. G. Brewka, S. Coradeschi,A. Perini, & P. Traverso (Eds.), Proceedings 17th European ConferenceArtificial Intelligence (ECAI2006) (pp. 1620). Zurich: IOS Press.415fiDubois, Fargier, BonnefonBoutilier, C., Brafman, R. I., Domshlak, C., Hoos, H. H., & Poole, D. (2004). CP-nets:tool representing reasoning conditional ceteris paribus preference statements. J. Artif. Intell. Res. (JAIR), 21, 135-191.Brafman, R., & Tennenholtz, M. (2000). axiomatic treatment three qualitativedecision criteria. Journal ACM, 47 (3), 452482.Brandstatter, E., Gigerenzer, G., & Hertwig, R. (2006). priority heuristic: Makingchoices without trade-offs. Psychological Review, 113, 409432.Brewka, G. (1989). Preferred subtheories: extended logical framework defaultreasoning. Int. Joint Conf. Artificial Intelligence (p. 1043-1048). Menlo Park,Ca: AAAI Press.Cacioppo, J. T., & Berntson, G. G. (1994). Relationship attitudes evaluativespace: critical review, emphasis separability positive negativesubstrates. Psychological Bulletin, 115, 401423.Cayrol, C., & Lagasquie-Schiex, M.-C. (2005). Graduality argumentation. JournalArtificial Intelligence Research, 23, 245297.Deschamps, R., & Gevers, L. (1978). Leximin utilitarian rules: joint characterization.Journal Economic Theory, 17, 143163.Doyle, J., & Thomason, R. (1999). Background qualitative decision theory. AIMagazine, 20 (2), 5568.Dubois, D. (1986). Belief structures, possibility theory decomposable confidence measures finite sets. Computers Artificial Intelligence, 5 (5), 403416.Dubois, D., & Fargier, H. (2003). Qualitative decision rules uncertainty.G. Della Riccia, D. Dubois, R. Kruse, & H.-J. Lenz (Eds.), Planning Based Decision Theory (Vol. 472, pp. 326). Wien: Springer.Dubois, D., & Fargier, H. (2004). axiomatic framework order magnitude confidencerelations. M. Chickering & J. Halpern (Eds.), Proceedings 20th ConferenceUncertainty Artificial Intelligence (UAI04) (pp. 138145). Menlo Park, CA:AUAI Press.Dubois, D., & Fargier, H. (2005). qualitative comparison sets positivenegative affects. Lecture Notes Computer Science, 3571, 305316.Dubois, D., & Fargier, H. (2006). Qualitative decision making bipolar information.P. Doherty, J. Mylopoulos, & C. Welty (Eds.), Proceedings 10th InternationalConference Principles Knowledge Representation Reasoning (pp. 175186).Menlo Park, CA: AAAI Press.Dubois, D., Fargier, H., & Prade, H. (1996). Possibility theory constraint satisfactionproblems: Handling priority, preference uncertainty. Applied Intelligence, 6 (4),287-309.Fargier, H., & Sabbadin, R. (2005). Qualitative decision uncertainty: back expected utility. Artificial Intelligence, 164, 245280.Fargier, H., & Wilson, N. (2007). Algebraic structures bipolar constraint-based reasoning. Symbolic quantitative approaches reasoning uncertainty, 9th european conference, ecsqaru 2007, proceedings (Vol. 4724, p. 623-634). Berlin: Springer.Fishburn, P. (1986). axioms subjective probabilities. Statistical Science, 1 (3),335345.416fiQualitative Bipolarity DecisionGigerenzer, G., & Goldstein, D. (1996). Reasoning fast frugal way: Modelsbounded rationality. Psychological Review, 103, 650669.Gigerenzer, G., Todd, P. M., & ABC group. (1999). Simple heuristics make ussmart. New York: Oxford University Press.Grabisch, M., & Labreuche, C. (2002). Bi-capacities decision making bipolar scales.Eurofuse02 Workshop Information Systems (p. 185-190).Grabisch, M., & Labreuche, C. (2005). Bi-capacities parts II. Fuzzy SetsSystems, 151 (2), 211260.Grabisch, M., & Lange, F. (2007). Games lattices, multichoice games Shapleyvalue: new approach. Mathematical Methods Operations Research, 65 (1), 153167.Greco, S., Matarazzo, B., & Slowinski, R. (2002). Bipolar Sugeno Choquet integrals.B. De Baets, J. Fodor, & G. Pasi (Eds.), Proceedings 7th MeetingEURO Working Group Fuzzy Sets (EUROFUSE 2002 (pp. 191196).Halpern, J. Y. (1997). Defining relative likelihood partially-ordered structures. JournalArtificial Intelligence Research, 7, 124.Katsikopoulos, K. V., & Martignon, L. (2006). Nave heuristics paired comparisons:results relative accuracy. Journal Mathematical Psychology, 50,488494.Lehmann, D. J. (1996). Generalized qualitative probability: Savage revisited. E. Horvitz& F. Jensen (Eds.), Proceedings 12th Conference Uncertainty ArtificialIntelligence (UAI-96) (pp. 381388). San Francisco, CA: Morgan Kaufman.Lewis, D. L. (1973). Counterfactuals comparative possibility. Journal PhilosophicalLogic, 2, 418446.Moulin, H. (1988). Axioms cooperative decision making. New-York: Wiley.Osgood, C. E., Suci, G., & Tannenbaum, P. H. (1957). Measurement Meaning.Chicago: University Illinois Press.Slovic, P., Finucane, M., Peters, E., & MacGregor, D. G. (2002). Rational actors rationalfools? Implications affect heuristic behavioral economics. JournalSocio-Economics, 31, 329342.Tversky, A., & Kahneman, D. (1992). Advances prospect theory: Cumulative representation uncertainty. Journal Risk Uncertainty, 5, 297323.Wald, A. (1971). Statistical Decision Functions. New York: Wiley. (Original work published1950)417fiJournal Artificial Intelligence Research 32 (2008) 95-122Submitted 10/07; published 05/08Graphical Model Inference Optimal Control StochasticMulti-Agent SystemsBart van den BroekWim WiegerinckBert KappenB.vandenBroek@science.ru.nlW.Wiegerinck@science.ru.nlB.Kappen@science.ru.nlSNN, Radboud University Nijmegen, Geert Grooteplein 21,Nijmegen, NetherlandsAbstractarticle consider issue optimal control collaborative multi-agentsystems stochastic dynamics. agents joint taskreach number target states. dynamics agents contains additive controladditive noise, autonomous part factorizes agents. Full observationglobal state assumed. goal minimize accumulated joint cost, consistsintegrated instantaneous costs joint end cost. joint end cost expresses jointtask agents. instantaneous costs quadratic control factorizeagents. optimal control given weighted linear combination single-agentsingle-target controls. single-agent single-target controls expressed termsdiffusion processes. controls, closed form expressions, formulatedterms path integrals, calculated approximately Metropolis-Hastingssampling. weights control interpreted marginals joint distributionagent target assignments. structure latter represented graphicalmodel, marginals obtained graphical model inference. Exact inferencegraphical model break large systems, approximate inference methodsneeded. use naive mean field approximation belief propagation approximateoptimal control systems linear dynamics. compare approximate inferencemethods exact solution, show accurately compute optimalcontrol. Finally, demonstrate control method multi-agent systems nonlineardynamics consisting 80 agents reach equal number target states.1. Introductiontopic control multi-agent systems characterized many issues, originatingvarious sources, including wide variety possible execution plans, uncertaintiesinteraction environment, limited operation time supporting resources,demand robustness joint performance agents. issues encounteredin, example, air traffic management (Tomlin, Pappas, & Sastry, 1998; van Leeuwen,Hesseling, & Rohling, 2002), formation flight (Ribichini & Frazzoli, 2003; Hu, Prandini,& Tomlin, 2007), radar avoidance unmanned air vehicles fighter aircraft (Pachter &Pachter, 2001; Kamal, Gu, & Postlethwaite, 2005; Larson, Pachter, & Mears, 2005; Shi,Wang, Liu, Wang, & Zu, 2007), persistent area denial (Subramanian & Cruz, 2003;Liu, Cruz, & Schumacher, 2007; Castanon, Pachter, & Chandler, 2004).many control approaches multi-agent systems, stochastic influences dynamicsagents taken account assumed negligible, dynamicsc2008AI Access Foundation. rights reserved.fivan den Broek, Wiegerinck & Kappenmodeled deterministically. system truly deterministic, agentsoptimally controlled open loop controls. However, stochastic influencesdynamics large ignored, open loop controls become far optimal,multi-agent system longer modeled deterministically.usualapproach control multi-agent systems stochastic dynamics model systemMarkov Decision Processes (MDP) (Boutilier, 1996; Sadati & Elhamifar, 2006).principle, solved discrete space time backward dynamic programming.However, discretization make joint state space multi-agent system increaseexponentially number agents, basic dynamic programming approachgenerally infeasible (Boutilier, 1996). attempt overcome exploit structuresproblem describe system factored MDP. general structuresconserved value functions, exact computations remain exponentialsystem size. Guestrin, Koller, Parr (2002a) Guestrin, Venkataraman, Koller(2002b) assumed predefined approximate structure value functions, therebyprovided efficient approximate MDP model multi-agent systems. similar approachtaken Becker, Zilberstein, Lesser, Goldman (2003, 2004), assuming independentcollaboration agents global reward function, resulting transition-independentdecentralized MDPs.paper concentrate multi-agent systems agents joint taskreach number target states. model multi-agent systemcontinuous space time, following approach Wiegerinck, van den Broek,Kappen (2006). make following assumptions. agents assumedcomplete accurate knowledge global state system (assumption 1).dynamics agent additive control disturbed additive Wiener noise(assumption 2). performance agents valued global cost function,integral instantaneous costs plus end cost. joint task agents modeledend cost. instantaneous costs assumed quadratic control(assumption 3). noise level dynamics agents inversely proportionalcontrol cost (assumption 4). Finally, assume autonomous dynamicsinstantaneous costs factorize agents (assumption 5).assumptions 1 2, optimal control problem partially solved findingoptimal expected cost-to-go, satisfies so-called stochastic Hamilton-JacobiBellman (SHJB) equation. optimal expected cost-to-go given, optimalcontrol provided gradient optimal expected cost-to-go adopting assumption 3. SHJB equation nonlinear partial differential equation (PDE),nonlinearity makes difficult solve. common approach solving SHJB equationassume, addition assumption 3, instantaneous costs end costcost function quadratic state, dynamics linearstate wellthis known linear-quadratic control. optimal expected cost-to-goquadratic state time-varying coefficients, problem reducessolving Riccati equations coefficients satisfy (Stengel, 1993; ksendal, 1998).Otherwise, approximation methods needed. approximate approach giveniterative linear-quadratic Gaussian method (Todorov & Li, 2005); yields locally optimal feedback control, valid case little noise. instead follow approachFleming (1978) adopt assumption 4. assumption SHJB equation96fiGraphical Model Inference MAS Optimal Controltransformed linear PDE performing logarithmic transformation. solutionequals expectation value stochastic integral diffusion process. general,closed form expression. paper estimate expression formulatingpath integral (Kappen, 2005a, 2005b), estimate latter using MetropolisHastings sampling. several ways estimate path integral,Hamilton Monte Carlo sampling Laplace approximation, coveredpaper.structure optimal expected cost-to-go generally complex duedynamic couplings agents. adopting assumption 5, agentscoupled joint end cost, solely determines structureoptimal expected cost-to-go. result state transition probabilities factorizeagents. follows optimal control becomes weighted combinationsingle-agent single-target controls. weights given joint distributionagent target assignments. joint distribution structure joint endcost. structure joint distribution representable factor graph,optimal control problem becomes graphical model inference problem (Wiegerinck et al.,2006). complexity graphical model inference exponential tree widthfactor graph. Exact inference possible using junction tree algorithm,given graph sufficiently sparse number agents large.complex situations approximate inference methods necessary, showoptimal control accurately approximated polynomial time, using naive meanfield (MF) approximation belief propagation (BP). makes distributed coordinationpossible multi-agent systems much larger could treatedexact inference.paper organized follows. Sections 2 3, provide reviewsingle multi-agent stochastic optimal control framework, developed Kappen (2005a, 2005b) Wiegerinck et al. (2006). example, rederive linearquadratic control. general solution given terms path integral, explainapproximated Metropolis-Hastings sampling.Section 4, give factor graph representation end cost function. discuss two graphical model approximate inference methods: naive mean field approximationbelief propagation. show approximation optimal controlmethods obtained replacing exact weights controls respectiveapproximations.Section 5, present numerical results. make comparison approximateoptimal controls, infered naive mean field approximation, belief propagationgreedy method, exact optimal control; multi-agent system18 agents linear dynamics two-dimensional state space, two targetstates. Furthermore, present results control multi-agent systems nonlineardynamics four-dimensional state space, agents control forward velocitydriving direction. controls approximated combination MetropolisHastings sampling, infer path integrals, naive mean field approximation, inferagent target assignments. allowed us control systems 80 agents80 target states. results regarding nonlinear dynamics illustrativepurpose.97fivan den Broek, Wiegerinck & Kappen2. Stochastic Optimal Control Single Agentconsider agent k-dimensional continuous state space Rk , state x(t) evolvingtime according controlled stochastic differential equationdx(t) = b(x(t), t)dt + u(x(t), t)dt + dw(t),(1)accordance assumptions 1 2 introduction. control agentRk -valued function u x(t) t. noise dynamics modeled Wienerprocess w(t), i.e., normally distributed k-dimensional stochastic process continuoustime mean 0 variance t, k k matrix represents variancenoise. autonomous dynamics modeled b, Rk -valued functionx(t) t. state change dx(t) sum noisy control autonomousdynamics.behavior agent valued cost function. Given agents state x(t) = xpresent time t, control u, expected future cost agent:Z1C u (x, t) = Eux,t (x(T )) +kRu(x(), )k2 + V (x(), ) .(2)2expectation Eux,t taken respect probability measure x(t)solution (1) given control law u condition x(t) = x. costcombination end cost (x(T )), function end state x(T ),integral instantaneous costs. instantaneous cost sum state controldependent term. state dependent term V (x(), ) cost state x()time . function V arbitrary, represents environment agent.control dependent term 12 kRu(x(), )k2 cost control state x() time ,kzk2 = z z Euclidean norm, R full rank k k matrix. quadraticcontrol, accordance assumption 3 introduction, assumption 4,R related variance noise control via relation= (R R)1 ,(3)scalar.expected cost-to-go time minimized controls u defines optimalexpected cost-to-goJ(x, t) = min C u (x, t).(4)uAppendix A, explained due linear-quadratic form optimizationproblemthe dynamics (1) linear action u, cost function (2) quadraticactionthe minimization performed explicitly, yielding nonlinear partial differential equation J, so-called stochastic Hamilton-Jacobi-Bellman (SHJB) equation.minimum attainedu(x, t) = (R R)1 x J(x, t).(5)optimal control. Note explicitely depends state x agenttime t, making feedback control.98fiGraphical Model Inference MAS Optimal Controloptimal expected cost-to-go re-expressed terms diffusion process (forderivation, refer Appendix A):J(x, t) = log Z(x, t)(6)Z(x, t) expectation valueZ11Z(x, t) = Ex,t exp (y(T ))V (y(), )(7)y() diffusion process y(t) = x satisfying uncontrolled dynamics:dy() = b(y(), )d + dw().(8)Substituting relations (3) (6) (5), find optimal control terms Z(x, t):u(x, t) = x log Z(x, t).(9)Example 1. Consider agent one dimension state x(t) described dynamical equation (1) without autonomous dynamics (b = 0). instantaneous cost V zero,end cost quadratic function around target state :(y) =|y |2 .2diffusion process y() satisfies uncontrolled dynamics (8) normally distributedaround agents state x = y(t) time variance 2 ( t), hence statetransition probability agent go (x, t) (y, ) space-time givenGaussian density|y x|2(y, |x, t) = p.exp 22 (T t)2 2 (T t)1expectation value (7) given integralZ(x, t) =Z1(y)dy(y, |x, t)e=R2 /|x |2exp 2,+ R2 /2 (T + R2 /)relation (3) used. optimal control follows (6) (9) readsu(x, t) =x.+ R2 /result well known (Stengel, 1993).99(10)fivan den Broek, Wiegerinck & Kappen2.1 Path Integral FormulationExample 1 shows simple system autonomous dynamics (b = 0) costsdue environment (V = 0), write control explicitly.uncontrolled dynamics normally distributed, consequently expectation value(7) quadratic end cost closed form expression. general situation bV arbitrary, longer exists explicit expression expectation value,optimal control obtained approximation. discussdone taking path integral approach (Kleinert, 2006). detailed derivationexpressions presented given Appendix B.path integral approach, write expectation value (7) path integral:Z(x, t) = lim Z (x(t0 ), t0 )(11)0x(t0 ) = x, t0 =1Z (x(t0 ), t0 ) = pdet(2 2 )NZdx(t1 ) . . .Z1dx(tN ) e (x(t0 ),...,x(tN ),t0 ) .integral paths (x(t0 ), . . . , x(tN )) discrete time, start x(t0 ) kept fixedN = t, taken continuous time limit sending length time steps= ti+1 ti zero. Note limit N goes infinity paths become infinitedimensional objects. function exponent cost path:(x(t0 ), . . . , x(tN ), t0 ) =(x(T )) +N1XV (x(ti ), ti ) +N1Xi=0i=021x(ti+1 ) x(ti )Rb(x(ti ), ti ),2optimal control becomes weighted average controls derived singlepath:u(x(t0 ), t0 ) = lim0Zdx(t1 ) . . .Zdx(tN ) p(x(t0 ), . . . , x(tN ), t0 ) u(x(t0 ), . . . , x(tN ), t0 ). (12)weights given1p(x(t0 ), . . . , x(tN ), t0 ) = pe (x(t0 ),...,x(tN ),t0 )det(2 2 )N Z (x(t0 ), t0 ).control derived path (x(t0 ), . . . , x(tN )) readsu(x(t0 ), . . . , x(tN ), t0 ) =x(t1 ) x(t0 )b(x(t0 ), t0 ).Note depends first two entries x(t0 ) x(t1 ) path.100(13)fiGraphical Model Inference MAS Optimal Control2.2 Path Integration Metropolis-Hastings Samplingpath integral formulation (12) optimal control generally computed,integral uncountably many paths, exist several waysapproximate it. natural approach goes stochastic sampling paths. Several methodsstochastic sampling exist, one use known Metropolis-Hastingssampling (Hastings, 1970). implementation time discretized: takelimit (12) decreasing zero, instead keep fixed value. sample pathsequence (xs (t0 ), . . . , xs (tN )) vectors state space Rk , x(t0 ) = xcurrent state agent current time t0 = t. According equation (13),need xs (t0 ) xs (t1 ) derive control sample path (x(t0 ), . . . , x(tN )).Metropolis-Hastings sampling ensures different paths properly weighted, henceoptimal control approximated follows:u(x(t0 ), t0 )hx(t1 )i x(t0 )b(x(t0 ), t0 ),t1 t0(14)hx(t1 )i mean value xs (t1 ) taken sample paths. Pseudo-codealgorithm given Algorithm 1.Algorithm 1: Metropolis-Hastings samplingInput: initial path (x(t0 ), . . . , x(tN ))1: = 12: repeat times:3: define Gaussian proposal distribution centered around (x(t1 ), . . . , x(tN ))variance equal noise4: draw sample path (x (t1 ), . . . , x (tN )) proposal distribution5: = exp 1 (x(t0 ), x(t1 ), . . . , x(tN ), t0 ) 1 (x(t0 ), x (t1 ), . . . , x (tN ), t0 )6: 17:set (x(t1 ), . . . , x(tN )) = (x (t1 ), . . . , x (tN ))8: else9:set (x(t1 ), . . . , x(tN )) = (x (t1 ), . . . , x (tN )) probability10: end11: (xs (t0 ), . . . , xs (tN )) = (x(t0 ), . . . , x(tN ))12: = + 113: end repeat14: compute approximate control equation (14)3. Stochastic Optimal Control Multi-Agent Systemturn issue optimally controlling multi-agent system n agents.principle, theory developed single agent straightforwardly generalizes multiagent situation. agent k-dimensional state xa satisfies dynamics similar(1):dxa (t) = ba (xa (t), t)dt + ua (x(t), t)dt + dwa (t),(15)accordance assumptions 1, 2 5 introduction. Note controlagent depends state xa , joint state x = (x1 , . . . , xn )101fivan den Broek, Wiegerinck & Kappensystem. system joint cost function similar (2), depending jointstate x joint control u = (u1 , . . . , un ) system:n ZX1uu2C (x, t) = Ex,t (x(T )) +kRa ua (x(), )k + V (xa (), ) .2a=1expectation Eux,t taken respect probability measure x(t)solution (15) given control law u condition x(t) = x. costcombination joint end cost (x(T )), function joint end state x(T ),integral instantaneous costs. instantaneous cost factorizes agents,accordance assumption 5 introduction. agent, sum statedependent term V (xa (), ) control dependent term 12 kRa ua (xa (), )k2 , similarsingle agent case. accordance assumption 4 introduction, control costagent related noise agents dynamics via relation= (Ra Ra )1 ,agent. joint cost function minimized jointcontrol, yielding optimal expected cost-to-go J. optimal expected cost-to-goexpressed terms diffusion process via relationJ(x, t) = log Z(x, t),Z(x, t) joint expectation value!#"n Z1X1V (ya (), )Z(x, t) = Ex,t exp (y(T ))(16)a=1y1 (t), . . . , yn (t) diffusion processes, = (y1 , . . . , yn ) y(t) = x, satisfyinguncontrolled dynamicsdya () = ba (ya (), )d + dwa (),= 1, . . . , n.(17)multi-agent equivalent optimal control (9) readsua (x, t) = xa log Z(x, t).(18)show optimal control agent understood expectedcontrol, is, integral target states ya transition probability targettimes optimal control target. end, write expectation (16)integral end state:Zn1Z(x, t) = dye (y)Za (ya , ; xa , t),(19)a=1Za (ya , ; xa , t) implicitly definedZZ1V (ya (), )dya Za (ya , ; xa , t)f (ya ) = Exa ,t f (ya (T )) exp102fiGraphical Model Inference MAS Optimal Controlarbitrary functions f . Substituting (19) (18) yieldsZua (x, t) = dya pa (ya |x, t) ua (ya ; xa , t)(20)ua (ya ; xa , t) = xa log Za (ya , ; xa , t)(21)optimal control agent go state xa current time state yaend time , pa (ya |x, t) marginaln11p(y|x, t) =Za (ya , ; xa , t).e (y)Z(x, t)a=13.1 Discrete End Statesagents fulfill task arriving number target states end timeaccording initially specified way: example, arrivetarget, arrive different targets. targets considered regionsG1 , . . . , Gm state space, end cost modeled follows:1e (y) =Xw(s)nwa (ya ; sa ),1wa (ya ; sa ) = e (ya ;sa ) ,(22)a=1sum runs assignments = (s1 , . . . , sn ) agents regions Gsa . (ya ; sa )cost function associated region Gsa , returning low cost end state ya agentlies region Gsa high cost otherwise. w(s) weight, grading assignmentsthereby specifying joint task agents. Assignments result betterfulfillment task higher weight. situation agents gotarget, example, vector assigns agent different targetlow weight w(s).choice end cost, equation (19) factorizesZ(x, t) =XZa (sa ; xa , t) =Zw(s)nZa (sa ; xa , t)a=1dya Za (ya , ; xa , t)wa (ya ; sa ).(23)interpretation Za (sa ; xa , t) log Za (sa ; xa , t) expected cost agentmove xa target sa . optimal control (20) single agent becomesua (x, t) =Xp(sa |x, t)ua (sa ; xa , t),(24)sa =1ua (sa ; xa , t) = xa log Za (sa ; xa , t)103(25)fivan den Broek, Wiegerinck & Kappencontrol agent go target sa , weights p(sa |x, t) single-agentmarginalsXp(s|x, t)(26)p(sa |x, t) =s\sajoint distributionn1p(s|x, t) =Za (sa ; xa , t).(27)w(s)Z(x, t)a=111weight p(s|x,Pnt) equals ratio exp J(s; x, t) / exp J(x, t) , J(s; x, t) =log w(s) a=1 log Za (sa ; x, t) optimal expected cost-to-go case agentspredetermined targets specified assignment s; assignment agentstargets low expected cost J(s; x, t) yield high weight p(s|x, t),associated single-agent single-target controls ua (sa ; xa , t) predominantoptimal controls ua (x, t).3.2 Metropolis-Hastings Sampling Multi-Agent Systemsgeneral, controls ua (sa ; xa , t) marginals p(sa |x, t) optimal control (24) closed form solution, inferred approximately.controls ua (sa ; xa , t) approximated Metropolis-Hastings sampling discussedSection 2.2. Inference marginals involves inference path integral formulations Za (sa ; xa , t):ZZ11Za (sa ; xa , t) = lim pdxa (t1 ) . . . dxa (tN )e (xa (t0 ),...,xa (tN ),t0 ;sa )2N0det(2 )xa (t0 ) = xa , t0 =S(xa (t0 ), . . . , xa (tN ), t0 ; sa ) = (xa (T ); sa )+N1XV (xa (ti ), ti ) +i=0N1Xi=021xa (ti+1 ) xa (ti )Raba (xa (ti ), ti ).2value Za (sa ; xa , t) generally hard determine (MacKay, 2003). Possible approximations include maximum posteriori (MAP) estimate inclusion variancesample paths. third approximation take average path costsestimate log Za (sa ; xa , t); means entropy distribution pathintegral neglected.4. Graphical Model Inferenceadditional computational effort multi-agent control compared single-agent controllies computation marginals p(sa |x, t) joint distribution p(s|x, t),involves sum mn assignments s. small systems feasible, largesystems summation performed efficiently. efficient approachprovided graphical model inference, relies factor graph representationjoint distribution.104fiGraphical Model Inference MAS Optimal Control1,41,212,443,422,33Figure 1: Example factor graph multi-agent system four agents. couplingsrepresented factors A, = {1, 4}, {1, 2}, {2, 4}, {3, 4}, {2, 3}.4.1 Factor Graph Representation Joint Distributioncomplexity joint distribution part determined weights w(s) endcost function (22). weights determine agents consider statesagents. complex case, way one agent takes state another agentaccount depend states agents. situation less complicatedagent considers states agents independently states others.means joint end cost factorized form:w(s) =wA (sA ),(28)subsets agents. structure represented graphically so-called factorgraph (Kschischang, Frey, & Loeliger, 2001). See Figure 1 example. agentsfactors nodes factor graph, represented circles squares respectively,edge agent factor member subset A,is, wA factorization w depends sa . (27) immediatejoint distribution p(s|x, t) factorizes according factor graph.4.2 Junction Tree AlgorithmEfficient inference distribution p(s|x, t) means factor graph representationaccomplished using junction tree algorithm (Lauritzen & Spiegelhalter, 1988).complexity algorithm exponential induced tree width graph. smalltree width expected systems factor graph sparse, caseagents take states account limited number agents.implies multi-agent systems sparse graphs limited number targetstractable (Wiegerinck et al., 2006). factor graph Figure 1 example sparsegraph. hand, agent take state agent account,junction tree algorithm really help: underlying factor graph fullyconnected tree width graph equals number agents system.Exact computation optimal control intractable large complex multiagent systems, since junction tree algorithm requires memory exponential treewidth factor graph. Instead use graphical model approximate inferencemethods approximately infer marginals (26). proceed discussiontwo methods: naive mean field (MF) approximation (Jordan, Ghahramani, Jaakkola,& Saul, 1999) belief propagation (BP) (Kschischang et al., 2001; Yedidia, Freeman, &Weiss, 2001).105fivan den Broek, Wiegerinck & Kappen4.3 Naive Mean Field Approximationstarting point note optimal expected cost-to-go log partition sum,also known free energy. Consider variational free energyXF (q) = h log wiqhlog Za iqa H(q),h iq h iqa denote expectation values respect distribution q marginalsqa respectively, H(q) entropy q:XH(q) =q(s) log q(s).optimal expected cost-to-go equals variational free energy minimized distributions q. naive mean field approximationone considers variational free energyQrestricted factorized distributions q(s) = qa (sa ). minimumF (q)JMF = minQq=qaupper bound optimal expected cost-to-go J, equals J case agentsuncoupled. F zero gradient local minima, is,0=F (q1 (s1 ) qn (sn ))qa (sa )= 1, . . . , n,(29)additional constraints normalization distributions qa . Solutions setequations implicitly given mean field equationsZa (sa )hw|sa iqqa (sa ) = Pnsa =1 Za (sa )hw|sa iq(30)hw|sa iq conditional expectation w q given sa :Xqa (sa ) w(s1 , . . . , sn ).hw|sa iq =s1 ,...,sn \sa6=amean field equations solved means iteration; procedure resultsconvergence local minimum free energy.mean field approximation optimal control found taking gradientrespect x minimum JMF free energy. similar exact caseoptimal control gradient optimal expected cost-to-go, equation (18).Using (29), findX1ua (x, t) = xa JMF (x, t) =qa (sa )ua (xa , t; sa ).Similar exact case, average single-agent single-target optimal controlsua (xa , t; sa ) given equation (25), average taken respect meanfield approximate marginal qa (sa ) agent a.106fiGraphical Model Inference MAS Optimal Control4.4 Belief Propagationbelief propagation, approximate free energy Bethe free energy,minimize latter. Bethe free energy definedFBethe ({qa , qA }) =Xh log wA iqAXh log Za iqaXH(qA ) +X(na 1)H(qa ).(31)function beliefs qa (sa ) qA (sA ), non-negative normalized functionssatisfy consistency relations::XqA (sA ) = qa (sa ).sA\aH(qa ) H(qA ) entropies beliefs qa qA , na denotes numberneighbors node factor graph.Belief propagation algorithm computes beliefs (Kschischang et al., 2001).case joint distribution p factor graph representation tree, belief propagation converge beliefs exact marginals p, Bethe free energybeliefs equals optimal expected cost-to-go J. factor graph representationp contains cycles, may still apply belief propagation. Yedidia et al. (2001) showedfixed points algorithm correspond local extrema Bethe free energy.particular, advanced variations algorithm (Heskes, Albers, & Kappen, 2003;Teh & Welling, 2001; Yuille, 2002) guaranteed converge local minima Bethefree energy (Heskes, 2003).find BP approximation optimal control taking gradientminimum JBethe Bethe free energy:X1ua (x, t) = xa JBethe (x, t) =qa (sa )ua (xa , t; sa ),ua (xa , t; sa ) given equation (25). Similar exact case mean fieldapproximation, BP approximation optimal control average single-agentsingle-target optimal controls, average taken respect belief qa (sa ).5. Numerical Resultssection, present numerical results simulations optimal control multiagent systems. problem computing optimal controls (24) consists two parts:inference single-agent single-target controls (25), inferencemarginals (26) global distribution agent target assignments. dynamics linear, instantaneous costs V zero, single-agent single-targetcontrols given closed form. multi-agent systems therefore know issueinfering marginal distributions. Section 5.1 consider multi-agent systemskind. Section 5.2 deals general problem infering optimal controlsdynamics nonlinear instantaneous costs V nonzero. sections107fiExpected Targetvan den Broek, Wiegerinck & KappenPosition10100.51Time1.521010(a) Positions0.51Time1.52(b) Expected TargetsFigure 2: Two agents, noise control positions, need reach target locations -1 1 end time = 2, agent different target location.positions (a) expected targets (b) time.joint end cost given equation (22),w(s) =na,bwa,b (sa , sb ),cwa,b (sa , sb ) = exp sa ,sb ,n(32)1(33)(ya ; sa ) = |ya sa |2 ,wa (ya ; sa ) = exp (ya ; sa ) ,2c determines coupling strength agents, sa targetstates.5.1 Linear Dynamicsbegin illustration optimal control showing simulation exactlysolvable stochastic multi-agent system. system two agents one dimension,agents satisfy dynamics (15) ba equal zero. two target states, x = 1 = 1x = 2 = 1. task agents one go different target.instantaneous costs V cost function zero, end cost function givenequations (22), (32) (33) = 20 c = 4. negative sign couplingstrength c implies repulsion agents. control cost parameter R equals 1,noise level 2 lies 0.5. agents start x = 0 time = 0, end time lies= 2. prevent overshooting targets, udt small compared distancetarget states. done choosing dt = 0.05(T + 0.05).PFigure 2 shows agents positions expected targetssa =1,2 p(sa |x, t)satime. see time = 1, agents decided targetgo, remain two targets. Then, = 1, final decisionseems made. delayed choice due symmetry breaking cost-togo time increases. symmetry breaking, better keep options open,see effect noise is. symmetry breaking, time short waitlonger choice made. phenomenon typical multi-modal problems.proceed quantitative comparison different control methods ariseexact approximate inferences marginals joint distribution (27).108fiGraphical Model Inference MAS Optimal Control31042CPU TimeCost Difference53210101011001010.20.4 0.6Noise0.8101(a) Costs00.20.4 0.6Noise0.81(b) CPU TimeFigure 3: deviation optimal cost (a) required CPU Time seconds(b) functions noise. lines represent exact ( ), Greedy ( ), MF() BP () control.example consider multi-agent system n = 18 agents two-dimensionalstate space zero instantaneous costs (V = 0) autonomous dynamics (ba = 0).end cost function given equations (22), (32) (33). two targets located1 = (1, 0) 2 = (1, 0). = 20 c = 0.5. control cost matrix R equalsidentity matrix. agents start (0, 0) time = 0, end time lies = 2,time steps size dt = 0.05(T + 0.05).approximations naive mean field approximation belief propagation, described Section 4, greedy control. greedy control mean time stepagent chooses go nearest target. include approximationsimple requires little computation time, reasons obvious choicenaive approximation. greedy control policy neglects choicesagents, expect give inferior performance.approximation, Figure 3(a) shows cost approximate (optimal)control minus cost exact (optimal) control, averaged 100 simulations,different noise levels. noise samples used approximate exactcontrol. see naive mean field approximation belief propagation yieldcosts average coincide cost exact control: average cost differencemethods significantly differ zero. Greedy control,hand, yields costs significantly higher costs exact control;deterministic limit converge cost exact control, controlscoincide. Figure 3(b) shows CPU time required calculation controlsdifferent control methods. average CPU time entire simulation.simulation consists 73 time steps, time step control calculatedagent. observe greedy control least 10 times faster methods,exact control nearly 100 times time consuming methods. Beliefpropagation gives performance considered noise levels bit quickernaive mean field approximation, may result implementation details.also done simulations attractive coupling c = 0.5; returned results similarones repulsive coupling c = 0.5 presented here.109fiCumulative Control Costvan den Broek, Wiegerinck & Kappen2015105000.51Time1.52Figure 4: cumulative control cost time, case strong repulsive couplingc = 2 low noise level 2 = 0.1. curves represent exact ( ), MF(), BP control ().Although Figure 3 suggests belief propagation naive mean field approximationperform equally well, always case, since certain combinations noiselevel coupling strength BP control costly MF control exactcontrol. origin difference lies symmetry breaking, tends occurlater BP earlier MF compared exact control. observeFigure 4, shows cumulative cost time control methodsmulti-agent system, coupling strength c = 2 fixed noise level 2 = 0.1.cumulative costs averages 100 simulations. cost MF control liesbit higher cost exact control, whereas cost BP control initiallylower cost control methods, = 1.7 starts increasemuch faster eventually ends higher. Including end costs, found total costs26.13 0.12 exact control, 26.19 0.12 MF control, 35.5 0.4 BPcontrol. suggests better early symmetry breaking latesymmetry breaking.time required computing control various methods dependsnumber agents multi-agent system. Figure 5 shows required CPU timefunction number agents n two-dimensional multi-agent system consideredabove. see exact method requires CPU time increases exponentiallynumber agents. may expected theory,exact method uses junction tree algorithm complexity exponentialtree width underlying graph, i.e., exponential n. greedy method,CPU time increases linearly number agents, agreementfact greedy control coupling agents. required CPUtime increases polynomially mean field approximation belief propagation.5.2 Nonlinear Dynamicsturn multi-agent systems nonlinear dynamics. control systems,must approximate graphical model inference well single-agent singletarget control problem (12). consider multi-agent system agents move110fiGraphical Model Inference MAS Optimal Control310CPU Time210110010110101520Agents2530Figure 5: required CPU time seconds calculation controls differentnumber agents. Exact ( ), greedy ( ), MF (), BP control ().two dimensions four-dimensional state specified agents location(xa , ya ), forward velocity va , driving direction . dynamics agentgiven equationsdxa = va cos dtdya = va sin dtdva = ua dt + dwada = dt + da .first two equations model kinematics agents position given forwardvelocity driving direction. last two equations describe control speeddriving direction application forward acceleration ua angular velocity. noise control modeled standard normal Wiener processes wanoise level parameters . Note noise act dimensionscontrol. Although control space counts less dimensionsstate space, example fit general framework: refer Appendix Cdetails.look two different tasks. first task obstacle avoidance multiagent system three agents. agents reach one three target locationsavoid obstacles environment. target location reached preciselyone agent; model end cost function, given equations (22), (32) (33),= c = 0.5. targets located (10, 15), (45, 12) (26, 45),agents arrive zero velocity. control cost matrix R identity matrix.= 0.1. instantaneous cost V equaled 1000 locations obstacles, zerootherwise. agents start time = 0, end time lies = 20, time steps dtsize 0.2. starting locations agents (18, 31), (25, 12) (39, 33),agents start zero velocity. sample paths discrete time paths twodimensional space forward velocity v driving direction . specifiedvalues times ti = + , = 0, . . . , N 1, = NT1 N = 7, valuetime t0 equals current state one agents, value time tN equalsone target end states. control agent one targets computed111fivan den Broek, Wiegerinck & Kappen505040403030202010100010203040500(a) Trajectories01020304050(b) Sample pathsFigure 6: Three agents, noise control forward velocities driving directions, reach three targets (marked X) environment containingalso number walls. agent starts different location (marked O)zero forward velocity, agent arrive different targetzero velocity without hitting walls. (a) trajectories agentsfollowed reach targets. (b) Sample paths.Metropolis-Hastings sampling paths, according Subsection 3.2. proposaldistribution 2N -dimensional Gaussian, centered around agents current plannedpath, variance equal noise level agents dynamics. expectationvalues Za (sa ; xa , t) estimated average costs sample paths. alsotried MAP estimation Za (sa ; xa , t) inclusion variance sample paths,former show significant difference, latter returned estimatesfluctuated heavily. Figure 6(a) shows environment trajectories agentsstarting locations targets. agent manages avoid obstaclesarrive one targets zero velocity, target reached differentagent.second task coordination multi-agent system shown Figure 7(a). system instantaneous costs (V = 0). agents moveinitial positions number target locations. arrivelocations zero velocity horizontal driving direction. equal numberagents target locations, agent reach different target. initiallocations aligned vertically, target locations, vertical displacement two. Thus agents coordinate movements orderreach targets satisfactory way.agents start time 0, end time lies 100, make time steps sizedt = 2(N1) , N = 7, dt < 0.01. time step controls computedMetropolis-Hastings sampling paths naive mean field approximation infermarginals pa (sa |x, t) weigh single-agent single-target controls, equations (24)(26). sample paths discretized seven equidistant time pointspresent time end time. proposal distribution taken Gaussian,112fiGraphical Model Inference MAS Optimal Controlcentered around agents current planned path variance equal noiselevel agents dynamics. Figure 7(a) shows example trajectories system10 agents. obtained 10 sample paths per agent-target combination.observe agents reach targets, target reached precisely oneagent, required. Due noise second order dynamics agents, takesagents less effort approach target remain there, since former allowsexploitation noise latter requires constant correction state changescaused noise. result trajectories agents curvedelongated would expected situation without noise. simulationcarried well larger number agents. Figure 7(b) shows required CPU timefunction number agents, exact MF inference marginalsagents. Note complexity graphical model inference problem scalesnn , n number agents. Exact inference using junction tree algorithmfeasible n < 10.6. Discussionstudied use graphical model inference methods optimal control stochasticmulti-agent systems continuous space time agents joint taskreach number target states. Rather discretizing, commonly done typicallymakes large systems intractable due curse dimensionality, followed approachdeveloped Wiegerinck et al. (2006), modeling system continuous space time.certain assumptions dynamics cost function, solution giventerms path integral.path integral computed closed form special cases,linear-quadratic case, general approximated. donevariety methods. method considered paper MCMC sampling.dimension sample paths kept low (N = 7) limit curvature samplepaths. gain limiting curvature variance samples reducedless samples needed. limiting curvature, however, introduce bias.addition, presence obstacles insufficient curvature would make sampler returnsample paths run obstacles. believe advanced MCMCmethods Hybrid MC sampling (Duane, Kennedy, Pendleton, & Roweth, 1987)overrelaxation (Neal, 1998) improve inference path integrals.Apart MCMC sampling, approximation methods one couldconsider, Laplace approximation variational approximation. Laplaceapproximation becomes exact noiseless limit could useful low noise regimeswell. variational approximation approximates path integral (11) Gaussianprocess (Archambeau, Opper, Shen, Cornford, & Shawe-Taylor, 2007), could particularly useful high noise regime. drawback variational approach, however,cannot straightforwardly applied situations infinite instantaneous costs,like hard obstacles environment considered here.Wiegerinck et al. (2006) showed systems sufficiently sparsesingle-agent single-target controls determined closed form, e.g. linearquadratic control time-independent coefficients, exact inference achieved using113fivan den Broek, Wiegerinck & Kappen5040302010010203040502015105051015(a) Trajectories5104CPU time103102101100204060Number Agents80(b) CPU timeFigure 7: (a) trajectories 10 agents starting locations 10 targets X. (b)required CPU time seconds function number agents,number targets equal number agents. lines represent exact( ) MF () inference marginals.114fiGraphical Model Inference MAS Optimal Controljunction tree algorithm. Van den Broek, Wiegerinck, Kappen (2007) consideredmulti-agent system second-order dynamics, linear autonomous dynamics zeroinstantaneous costs, showed graphical model inference naive mean field approximation significantly outperformed greedy inference. showed closeoptimal result achieved well dense systems, using graphical model approximateinference methods. approximation methods considered naive mean fieldapproximation belief propagation. demonstrated performances examplesystem exact inference significantly time consuming. Mean field approximation showed work well, returning costs control equal optimal ones, beliefpropagation performed similarly. certain value ratio coupling strengthnoise level, symmetry breaking control process takes place earliermean field approximation compared exact inference, later belief propagation. early symmetry breaking increase costs coordination much,however, late symmetry breaking does, making performance belief propagationsuboptimal.variations considered case also possible within general framework.Wiegerinck, van den Broek, Kappen (2007) discuss situations agents sequentiallyvisit number targets, end time fixed. focusses preferedtrajectories state space time, instead prefered states end time;achieved modeling path cost way similar modeled end cost.problem agents intercept moving target noisy dynamics alsocovered there.control formalism developed Kappen (2005a, 2005b) applied multi-agentcoordination Wiegerinck et al. (2006) article, demands noisecontrol act dimensions. One way satisfy constraint assumeagents identical. addition, single agent dynamicsnoise control act dimensions. saw two-dimensionalsecond order system Section 5.2 condition satisfied natural way. However,general one think examples control problems equation (3) violated.interesting future direction research investigate extend path integralapproach used approximation cases.paper assumes joint state space agents observable agents.large multi-agent systems, however, realistic agent observesstate states agents physically nearby. approachdirectly apply situations. Depending joint task agents, mayvalid approximation optimal control sub-system consisting agentsone agent observe. task agents avoid collisions,sufficient consider states agents nearby, task gotarget crucial information statesagents. natural alternative deal partial observability describe multi-agentsystem decentralized POMDP (Seuken & Zilberstein, 2008). clear however,approach would combine path integral formalism.topic learning addressed paper, clearly greatinterest. However, one could argue sampling procedure compute path integral115fivan den Broek, Wiegerinck & Kappencorresponds learning environment. discussion line thoughtfound (Kappen, 2007).many possible model extensions worthwhile exploring future research.Obvious examples bounded controls, limited observation global statesystem; issues already interest study single agent situation. Othersapply typically multi-agent situation. context physical agents, introducing penalties collisions agents would become relevant. Typically, typesmodel extensions solution closed form, require additionalapproximate numerical methods. suggestions given Kappen (2005a, 2005b).Acknowledgmentsthank reviewers useful comments. thank Joris Mooij makingavailable useful software (www.mbfys.ru.nl/~jorism/libDAI/). research partInteractive Collaborative Information Systems (ICIS) project, supported DutchMinistry Economic Affairs, grant BSIK03024.Appendix A. Stochastic Optimal Controlappendix give derivation (5), (6) (7), starting (1), (2), (3)(4). Detailed discussions found many works stochastic optimal control,example Kushner (1967), Fleming Rishel (1975), Fleming (1978), ksendal(1998), Stengel (1993), Kappen (2005a, 2005b).optimal expected cost-to-go J state x time definedJ(x, t) = min C u (x, t),uuC (x, t) =Eux,tZ(x(T )) +12kRu(x(), )k + V (x(), )2(34)(35)expected cost given control law u. equations (4) (2)main text. first show J satisfies stochastic Hamilton-Jacobi-Bellman (SHJB)equation11 22J = minkRuk + (b + u) x J + Tr x J + V ,(36)u22boundary condition J(x, ) = (x). equation derived following way.moment time holdsZ12udskRu(x(s), s)k + V (x(s), s)J(x, t) =C (x(), ) +2Z12ukRu(x(s), s)k + V (x(s), s) .ds= min Ex,t J(x(), ) +u2min Eux,tufirst line follows dividing integral two integrals, oneone , using definition cost function C, second line116fiGraphical Model Inference MAS Optimal Controlfollows definition J. rewriting yieldsZJ(x(), ) J(x, t)11u20 = min Ex,tds+kRu(x(s), s)k + V (x(s), s) .u2Taking limit obtaindJ(x(t), t) 12u+ kRu(x(t), t)k + V (x(t), t) .0 = min Ex,tudt2(37)Subsequently, apply dJ(x(t), t) well known chain rule diffusion processes:dJ(x(t), t) =X J(x(t), t)xidxi (t) +J(x(t), t)1 X 2 J(x(t), t)dt +dxi (t)dxj (t). (38)2xi xji,jdiffers chain rule deterministic processes also contains termquadratic dx. extra term vanish, Wiener process appearingdynamics (1) quadratic variation increases linear time:Eux,t [dwi (t)dwj (t)] = ij dt.(39)follows expectation dxi (t)dxj (t) equal ( )ij dt. substituting dynamics (1) (38), taking expectation values, using (39), obtain2J(x, t)J(x, t)J(x, t)udt + (b(x, t) + u(x, t))dt + Trdt.Ex,t [dJ(x(t), t)] =xxxSubstitution equation (37) yields equation (36).minimum right-hand side equation (36) givenu = (R R)1 x J.optimal control.minimization (36) removed inserting optimal control. yieldsnonlinear equation J. remove nonlinearity using logarithmic transformation: introduce constant , define Z(x, t) J(x, t) = log Z(x, t),11u R Ru + u x J = 2 Z 2 (x Z) (R R)1 x Z,221 211 2Tr x J=Z (x Z) x Z Z 1 Tr x2 Z .222terms quadratic x Z vanish R related via equation (3),= (R R)1 .relation satisfied, SHJB equation becomesV12Z =b x Tr x Z2= HZ,117(40)fivan den Broek, Wiegerinck & KappenH linear operator acting function Z.Equation (40) must solved backwards time boundary condition Z(x, ) =1e (x) . present solution terms forward diffusion process. common approach theory stochastic processes give solutions partial differential equationsterms diffusion processes. solution equation (40) expectation valueZ11Z(x, t) = Ex,t exp (y(T ))V (y(), ) ,(41)y() process satisfies uncontrolled dynamicsdy() = b(y(), )d + dw(),y(t) = x. expectation Ex,t taken respect probability measurey() satisfies uncontrolled dynamics condition y(t) = x. clear (41)matches boundary condition. verify satisfies equation (40), letZ1V (y(), ) .I(t) = expsee1V (y(t), t)I(t)dt.Let f function f (y) = exp 1 (y) . use chain rule stochasticprocesses apply f (y(T )) finddI(t) =kkXf (y(T ))1 X 2 f (y(T ))dyi (T ) +dyi (T )dyj (T )df (y(T )) =yi2yi yji=1i,j=1f (y(T ))(b(y(T ), )d + dw(T ))=21f (y(T ))Trd.2yychoose = 0 = dt combine identity previous oneobtaindf (y(T ))I(t) = f (y(T ))dI(t) + I(t)df (y(T ))= Hf (y(T ))I(t)dt + f (y(T ))I(t)dw(T ).Taking expectation value sides makes term f (y(T ))I(t)dw(T ) disappear,remaining part,dE [f (y(T ))I(t)] = [f (y(T ))I(t)] dt,equation (40).118fiGraphical Model Inference MAS Optimal ControlAppendix B. Path Integral Formulationgoing write expectation value (7) path integral. Partitioning timeinterval N intervals equal length , = t0 < t1 < . . . < tN = ,expectation value written follows:Z(x, t) =Zdx1 . . .Z1dxN e (xN )N1Z(xi+1 , ti+1 ; xi , ti )(42)i=0x0 = x Z(xi+1 , ti+1 ; xi , ti ) implicitly definedfiZZfi1 ti+1fidxi+1 Z(xi+1 , ti+1 ; xi , ti )f (xi+1 ) = E f (xi+1 ) expV (y(), ) fiy(ti ) = xitiarbitrary functions f . limit infinitesimal , Z(xi+1 , ti+1 ; xi , ti ) satisfy1Z(xi+1 , ti+1 ; xi , ti ) = (xi+1 , ti+1 |xi , ti ) exp V (xi , ti ) ,(43)(xi+1 , ti+1 |xi , ti ) transition probability uncontrolled dynamics (8) go(xi , ti ) (xi+1 , ti+1 ) space-time. transition probability given1k 1 (xi+1 xi b(xi , ti ))k2.(xi+1 , ti+1 |xi , ti ) = pexp2det(2 2 )follows dynamicsxi+1 xi = b(xi , ti ) + winfinitesimal time interval observation Wiener process w normally distributed around zero variance . Using equation (3), may rewritetransition probability2 !11xxi+1exp(xi+1 , ti+1 |xi , ti ) = pRb(xi , ti )(44).22det(2 )obtain path integral representation Z(x, t) combining equations (42), (43)(44) limit going zero:Z(x, t) = lim Z (x0 , t0 )(45)0x0 = x, t0 = t,1Z (x0 , t0 ) = pdet(2 2 )N(x0 , . . . , xN , t0 ) = (xN ) +N1XZdx1 . . .V (xi , ti ) +ZN1Xi=0i=01191dxN e (x0 ,...,xN ,t0 )2xi+1 xi1b(xi , ti )R.2fivan den Broek, Wiegerinck & Kappenoptimal control given equation (9) proportional gradientlog Z(x, t). Substituting path integral representation (45) Z(x, t), findu(x0 , t0 ) = lim0= lim0ZZdx1 . . .dx1 . . .ZZ1e (x0 ,...,xN ,t0 )dxN px0det(2 2 )N Z (x, t0 )1(x0 , . . . , xN , t0 )dxN p(x0 , . . . , xN , t0 )u(x0 , . . . , xN , t0 )u(x0 , . . . , xN , t0 ) =x1 x0b(x0 , t0 )1e (x0 ,...,xN ,t0 )p(x0 , . . . , xN , t0 ) = p.det(2 2 )N Z (x0 , t0 )Note control u(x0 , . . . , xN , t0 ) results path (x0 , . . . , xN ) dependsfirst two entries x0 x1 path.Appendix C. Dimension Reductionderivation path integral Appendix B given casestate control k-dimensional. particular case dimensionsstate controlled deduced taking limit infinite control cost alongdimensions without control. control along latter dimensions becomes zero,seen equation (5). noise dimensions equal zero accordancerelation (3). path integral formalism transition probabilities (44)reduce delta functions along dimensions without control. implicationsMCMC sampling dimension space sample also reduced,since sampling performed dimensions noise.ReferencesArchambeau, C., Opper, M., Shen, Y., Cornford, D., & Shawe-Taylor, J. (2007). Variational inferencediffusion processes. Advances Neural Information Processing Systems.Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-independent decentralized Markov decision processes. Proceedings Second International Joint ConferenceAutonomous Agents Multiagent Systems, pp. 4148.Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004). Solving transition independentdecentralized Markov decision processes. Journal Artificial Intelligence Research, 22, 423455.Boutilier, C. (1996). Planning, learning coordination multiagent decision processes.Proceedings Sixth Conference Theoretical Aspects Rationality Knowledge, pp.195210.Castanon, D. A., Pachter, M., & Chandler, P. R. (2004). game deception. Proceedings43rd IEEE Conference Decision Control, pp. 33643369.Duane, S., Kennedy, A., Pendleton, B., & Roweth, D. (1987). Hybrid Monte Carlo. Physics LettersB, 195 (2), 216222.120fiGraphical Model Inference MAS Optimal ControlFleming, W. H. (1978). Exit probabilities optimal stochastic control. Applied MathematicsOptimization, 4, 329346.Fleming, W. H., & Rishel, R. W. (1975). Deterministic Stochastic Optimal Control. SpringerVerlag, New York.Guestrin, C., Koller, D., & Parr, R. (2002a). Multiagent planning factored MDPs. AdvancesNeural Information Processing Systems, Vol. 14, pp. 15231530.Guestrin, C., Venkataraman, S., & Koller, D. (2002b). Context-specific multiagent coordinationplanning factored MDPs. Eighteenth National Conference Artificial Intelligence,pp. 253259.Hastings, W. (1970). Monte Carlo sampling methods using Markov chains applications.Biometrika, 57 (1), 97109.Heskes, T. (2003). Stable fixed points loopy belief propagation minima Bethe freeenergy. Advances Neural Information Processing Systems, Vol. 15, pp. 343350.Heskes, T., Albers, K., & Kappen, B. (2003). Approximate inference constrained optimization.Proceedings 19th Conference Uncertainty Artificial Intelligence, pp. 313320.Hu, J., Prandini, M., & Tomlin, C. (2007). Conjugate points formation constrained optimalmulti-agent coordination: case study. SIAM Journal Control Optimization, 45 (6),21192137.Jordan, M., Ghahramani, Z., Jaakkola, T., & Saul, L. (1999). introduction variational methodsgraphical models. Learning Graphical Models. MIT Press, Cambridge.Kamal, W. A., Gu, D.-W., & Postlethwaite, I. (2005). Real time trajectory planning UAVs usingMILP. Proceedings 4th IEEE Conference Decision Control, EuropeanControl Conference 2005, pp. 33813386.Kappen, H. J. (2005a). Path integrals symmetry breaking optimal control theory. Journalstatistical mechanics: theory experiment, P11011.Kappen, H. J. (2005b). Linear theory control nonlinear stochastic systems. Physical ReviewLetters, 95 (20), 200201.Kappen, H. J. (2007). introduction stochastic control theory, path integrals reinforcementlearning. AIP conference proceedings, Vol. 887, pp. 149181.Kleinert, H. (2006). Path Integrals Quantum Mechanics, Statistics, Polymer Physics, Financial Markets. World Scientific, Singapore.Kschischang, F. R., Frey, B. J., & Loeliger, H.-A. (2001). Factor graphs sum-productalgorithm. IEEE Transactions Information Theory, 47 (2), 498519.Kushner, H. J. (1967). Stochastic Stability Control. Academic Press Inc., New York.Larson, R. A., Pachter, M., & Mears, M. (2005). Path planning unmanned air vehiclesengaging integrated radar network. Proceedings AIAA Guidance, Navigation,Control Conference Exhibit.Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphical structures application expert systems (with discussion). J. Royal Statistical SocietySeries B, 50, 157224.Liu, Y., Cruz, J. B., & Schumacher, C. J. (2007). Pop-up threat models persistent area denial.IEEE Transactions Aerospace Electronic Systems, 43 (2), 509521.MacKay, D. J. (2003). Information Theory, Inference, Learning Algorithms. Cambridge University Press.Neal, R. M. (1998). Learning Graphical Models, pp. 205225. Kluwer Academic Publishers.121fivan den Broek, Wiegerinck & Kappenksendal, B. (1998). Stochastic Differential Equations: Introduction Applications. SpringerVerlag.Pachter, L., & Pachter, M. (2001). Optimal paths avoiding radiating source. Proceedings40th IEEE Conference Decision Control, pp. 35813586.Ribichini, G., & Frazzoli, E. (2003). Efficient coordination multiple-aircraft systems. Proceedings42nd IEEE Conference Decision Control, Vol. 1, pp. 10351040.Sadati, N., & Elhamifar, E. (2006). Semi-decentralized control multi-agent systems basedredundant manipulator optimization methods. Proceedings 9th IEEE InternationalWorkshop Advanced Motion Control, pp. 278283.Seuken, S., & Zilberstein, S. (2008). Formal models algorithms decentralized decision makinguncertainty. Journal Autonomous Agents Multi-Agent Systems.Shi, X., Wang, X., Liu, Y., Wang, C., & Zu, C. (2007). Optimization fighter aircraft evasive trajectories radar threats avoidance. Proceedings 2007 IEEE International ConferenceControl Automation, pp. 303307.Stengel, R. (1993). Optimal Control Estimation. Dover Publications, New York.Subramanian, S. K., & Cruz, J. B. (2003). Adaptive models pop-up threats multi-agentpersistent area denial. Proceedings 42nd IEEE Conference Decision Control,pp. 510515.Teh, Y., & Welling, M. (2001). unified propagation scaling algorithm. AdvancesNeural Information Processing Systems, Vol. 14, pp. 953960.Todorov, E., & Li, W. (2005). generalized iterative LQG method locally-optimal feedbackcontrol constrained nonlinear stochastic systems. Proceedings American ControlConference, pp. 300306.Tomlin, C., Pappas, G. J., & Sastry, S. (1998). Conflict resolution air traffic management: studymultiagent hybrid systems. IEEE Transactions Automatic Control, 43 (4), 509521.van den Broek, B., Wiegerinck, W., & Kappen, B. (2007). Optimal control large stochastic multiagent systems. Proceedings Seventh Symposium Adaptive Learning AgentsMulti-Agent Systems, pp. 920.van Leeuwen, P., Hesseling, H., & Rohling, J. (2002). Scheduling aircraft using constraint satisfaction.Electronic Notes Theoretical Computer Science, 76, 252268.Wiegerinck, W., van den Broek, B., & Kappen, B. (2006). Stochastic optimal control continuousspace-time multi-agent systems. Proceedings 22nd Conference UncertaintyArtificial Intelligence, pp. 528535.Wiegerinck, W., van den Broek, B., & Kappen, B. (2007). Optimal on-line scheduling stochasticmulti-agent systems continuous space-time. Proceedings Sixth International JointConference Autonomous Agents Multiagent Systems, pp. 744751.Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. Advances NeuralInformation Processing Systems, Vol. 13, pp. 689695.Yuille, A. (2002). CCCP algorithms minimize Bethe Kikuchi free energies: Convergentalternatives belief propagation. Neural Computation, 14 (7), 16911722.122fiJournal Artificial Intelligence Research 32 (2008) 939-982Submitted 01/31; published 08/08Optimal Strategies Bidding Agents ParticipatingSimultaneous Vickrey Auctions Perfect SubstitutesEnrico H. GerdingRajdeep K. DashAndrew BydeNicholas R. Jenningseg@ecs.soton.ac.ukrkd@ecs.soton.ac.ukab06v@ecs.soton.ac.uknrj@ecs.soton.ac.ukIntelligence, Agents, Multimedia GroupSchool Electronics Computer ScienceUniversity Southampton, Southampton, UKAbstractderive optimal strategies bidding agent participates multiple, simultaneoussecond-price auctions perfect substitutes. prove that, everyone else bids locallysingle auction, global bidder always place non-zero bids availableauctions, provided budget constraints. budget, however, optimalstrategy bid locally budget equal less valuation. Furthermore,wide range valuation distributions, prove problem finding optimalbids reduces two dimensions auctions identical. Finally, address marketssequential simultaneous auctions, non-identical auctions, allocativeefficiency market.1. Introductionrecent years, surge application auctions, onlinewithin multi-agent systems (Wellman, Greenwald, & Stone, 2007; Clearwater, 1996; Gerding, Rogers, Dash, & Jennings, 2007b; Rogers, David, & Jennings, 2005; Rosenthal & Wang,1996; Roth & Ockenfels, 2002; Dash, Parkes, & Jennings, 2003). result,increasing number auctions offering similar even identical goods services.1take advantage fact, automated support needs developed monitor, bidin, make decisions across large set possibilities. software, formintelligent bidding agents (hereafter shortened bidder), increase likelihood winning item lower price, thus result considerable advantage buyer.Now, whereas participating many auctions arduous task done manually,problem ideally suited autonomous agents execute proper actionsbuyers behalf (Stone, Schapire, Littman, Csirik, & McAllester, 2003). end,paper devise analyse optimal bidding strategies one auction settingnamely, bidder participates multiple, simultaneous second-price auctionsgoods perfect substitutes. show, however, analysis also applieswider context markets consisting sequential (i.e., auctions close one1. eBay alone, example, often hundreds sometimes even thousands concurrent auctionsrunning worldwide selling substitutable items. illustrate, time writing, 1600 eBayauctions selling Apple iPhone worldwide.c2008AI Access Foundation. rights reserved.fiGerding, Dash, Byde & Jenningsother), well simultaneous (i.e., auctions close time)auctions.date, much existing literature multiple auctions focuses either sequentialauctions (Krishna, 2002) simultaneous auctions complementarities,value items together greater sum individual items (see Section 2related research simultaneous auctions). contrast, consider bidding strategiescase simultaneous auctions perfect substitutes. particular, focusVickrey second-price sealed bid auctions. choose lowcommunication overhead (in terms number required interactions) wellknown capacity induce truthful bidding. result, type auctiongeneralisations, Vickrey-Clarke-Groves mechanism, usednumber multi-agent system settings (Dash, Rogers, Reece, Roberts, & Jennings, 2005;Varian, 1995; Mes, van der Heijden, & van Harten, 2007; Dash, Vytelingum, Rogers, David,& Jennings, 2007). Moreover, auctions (weakly) strategically equivalentwidely used English auctions.2 However, find that, multiple auctionsrunning simultaneously, truthful bidding longer optimal. Given this, characterise,first time, bidding agents utility-maximising strategy bidding numberauctions type bidder valuation distribution.detail, consider market single bidder, called global bidder,bid number auctions, whereas bidders, called local bidders,assumed bid single auction. distinguish two types settingsmarket: one auctions identical global bidder indifferentauctions, one global bidder prefers auctions others.settings main results follows:Whereas case single second-price auction bidder weakly dominantstrategy bid true value, longer case several simultaneousauctions. best strategy global bidder bid true value.prove that, even global bidder requires one item assuming free disposal,expected utility maximised participating (i.e., bidding non-zero amount)auctions selling desired item.Finding optimal bid auction arduous task consideringpossible combinations. However, global bidder indifferent auctions, able significantly reduce search space common bidder valuationdistributions. result, optimal bids efficiently calculated numberauctions. Although setting global bidder preferences auctionsinvolved, still apply analytical methods obtain tractable optimal results.prove that, auctions identical, bidders expected utility maximisedbidding either uniformly across auctions, relatively high one auctions,same, low value others (which two behaviours optimal dependsbidder valuation market conditions number bidders).2. specifically, Vickrey English auction strategically equivalent assuming private valuationsgood, i.e., bidders value item remains unchanged bidding process.940fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutescase global bidder different preferences auctions,show optimal bid relatively higher auctions preferredauctions.argue that, even though global bidder significantly higher expected utilitylocal one, bidders necessarily bid globally. example, biddersbudget considerations constrain amount bid, show analyticallythat, budget equal less valuation, optimal bid singleauction certain conditions.Finally, consider issue market efficiency simultaneousauctions. Efficiency important system-wide consideration within area multiagent systems since characterises well allocations system maximiseoverall utility (Dash et al., 2003). Now, efficiency maximised goods allocatedvalue most. However, certain amount inefficiency inherentdistributed market auctions held separately. Given this, paper,measure efficiency markets local bidders consider impactglobal bidders inefficiency. doing, find presence global biddergenerally positive impact efficiency.remainder paper structured follows. first discuss related workSection 2. Section 3 describe bidders auctions detail. Section 4characterise optimal bidding behaviour base setting auctionsidentical, Section 5 explore number extensions: non-identical auctions, budgetconstraints sequential auctions. Section 6 address market efficiencyimpact global bidder. Finally, Section 7 concludes. proofs placedappendix.2. Related WorkResearch area simultaneous auctions segmented along two broad lines.one hand, game-theoretic analysis simultaneous auctions concentrates studying equilibrium strategy rational agents (Engelbrecht-Wiggans &Weber, 1979; Krishna & Rosenthal, 1996; Lang & Rosenthal, 1991; Rosenthal & Wang,1996; Szentes & Rosenthal, 2003). analyses typically used auction format employed simultaneous auctions (e.g., Vickrey auctionsfirst-price auctions). hand, heuristic strategies developedcomplex settings sellers offer different types auctions buyersneed buy bundles goods distributed auctions (Stone et al., 2003; Byde, Preist, &Jennings, 2002; Yuen, Byde, & Jennings, 2006; Greenwald, Kirby, Reiter, & Boyan, 2001;Greenwald & Boyan, 2004; Wellman, Reeves, Lochner, & Vorobeychik, 2004). paperadopts former approach studying market simultaneous Vickrey auctions sinceapproach yields provably optimal bidding strategies.Related approach seminal paper Engelbrecht-Wiggans Weber (1979),provides one starting points game-theoretic analysis distributedmarkets buyers substitutable goods. work analyses market consistingcouples equal valuations want bid dresser. Thus, couples bid941fiGerding, Dash, Byde & Jenningsspace contain two bids since husband wife twogeographically distributed auctions simultaneously. derive mixed strategy Nashequilibrium special case number buyers large. analysis differsuse decision-theoretic approach opposed game-theoreticone. argued among others Rothkopf (2007), Jiang Leyton-Brown (2007),decision-theoretic analysis often sufficient practice case auctions.3 Moreover,decision-theoretic framework allows us study complex setting biddersdifferent valuations global bidder bid auctions simultaneously(which entirely possible online auctions).Subsequently, Krishna Rosenthal (1996) studied case simultaneous auctionscomplementary goods. analyse case local global bidderscharacterise bidding buyers resultant market efficiency. setting providedKrishna Rosenthal (1996) extended case common valuesRosenthal Wang (1996). However, neither works extend easily casesubstitutable goods consider. case studied Szentes Rosenthal (2003),scenario considered restricted three sellers two global biddersbidder value (and thereby knowing value bidders).space symmetric mixed equilibrium strategies derived special case. However,mentioned earlier, results based decision theory, rather game theory,setting general (i.e., consider arbitrary number auctions). numberauthors study settings bidders face multiple simultaneous sealed-bid auctions,e.g. McAfee (1993), Peters Severinov (1997), Gerding et al. (2007b), Leyton-Brown,Shoham, Tennenholtz (2000). papers, however, assume bidders bidsingle auction choose auction randomly. show here, however,optimal. Finally, Shehory (2002) considers case concurrent English auctions,bidding algorithms developed buyers different risk attitudes. However,setting auctions never close time, forces bidsacross auctions. Although strategy may effective described setting,show paper, always optimal auctions close simultaneously (andbuyers bid late auctions).Related paper also literature considers bidding budget constraints. Although beyond scope paper provide full literature reviewtopic, highlight relevant work. extensive recent overviewpresented Pitchik (2006). number papers, Rothkopf (1977)Palfrey (1980), study optimal bidding simultaneous first-price auctions bidders constraints exposure, refers sum bids. papers, however,make strong assumption value accrued one auction independentothers. words, winning losing one auction affect expectedutility auctions. contrast, consider complete substitutes biddersrequire single item. result, one auctions won, value accruedauctions zero. interdependency auctions makes analysis sig3. Essentially, decision-theoretic setup requires (1) information distribution bestcompetitive bid, (2) bidders optimize decision given assessment (Rothkopf, 2007).information distribution obtained observing previous bids, e.g., using learningapproach described Jiang Leyton-Brown (2007).942fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutesnificantly difficult. Krishna Benoit (2001) also consider interdependency,valuations budget constraints assumed common knowledge (andproblem would thus trivial without budget constraints). Others, CheGale (1998), consider effect budget constraints single auctions, bidder typesconsist two dimensions: valuation budget. Sequential auctions alsoextensively studied regards budget constraints, e.g., Krishna Benoit(2001), Pitchik (2006). Furthermore, addition constraints exposure, typesbudget constraints considered, particular limits expected expenditures (Engelbrecht-Wiggans, 1987). None papers, however, address particularsetting consider here.Finally, number researchers investigated online auctions eBay.one hand, Hendricks, Onur, Wiseman (2005), Stryszowska (2004), Zeithammer (2005),Peters Severinov (2006), Rogers, David, Schiff, Jennings (2007) soughtexplain bidding behaviour buyers online auctions, include multiple bidding(i.e., successively increasing amount placed maximum bid eBay proxy agent,opposed bidding true value) sniping (i.e., starting bid near endauction). fact, result sniping behaviour agents, English auctionsrun eBay approximated Vickrey auctions since bidders longer muchinformation status bidding auction. papers, however, focussequential auction problem, discuss bidding strategiesmultiple auctions close simultaneously. hand, Boutilier, Goldszmidt,Sabata (1999), Gopal, Thompson, Tung, Whinston (2005), Juda Parkes (2006)explored means improve efficiency simultaneous auctions either proposingsequential auction mechanisms use options.4 Whereas works aimedchanging augmenting auction mechanism improve outcome, casetake auctions given focus strategic aspects bidders perspective.3. Bidding Multiple Vickrey Auctionsmodel consists sellers, acts auctioneer. seller auctionsone item; items complete substitutes (i.e., equal terms valuebidder obtains additional benefit winning one them). auctionsexecuted simultaneously; is, end simultaneously informationoutcome auctions becomes available bids placed.5 However,Section 5.3 briefly investigate markets sequential simultaneous auctionssince markets common practice, especially online.Furthermore, generally assume auctions identical (i.e., bidderindifferent them), relax assumption Section 5.2 auctions4. notion similar financial options, auction options provide buyer right,obligation, buy specific item specified price (the strike price) specifiedperiod time. However, analysis options differs financial perspective duedifferent assumptions modelling them.5. note that, although paper focuses sealed-bid auctions, case, conditionssimilar last-minute bidding sniping iterative auctions eBay (Roth & Ockenfels,2002); auctions close almost time, due network delays mayinsufficient time obtain results one auction proceeding bid next one.943fiGerding, Dash, Byde & Jenningsdifferent valuation distributions and/or numbers participating bidders globalbidder may thus prefer one auction another. Finally, assume free disposalbidders maximise expected profit. two assumptions, togetherassumption complete substitutes implicit throughout paper.3.1 Auctionssellers auction implemented second-price sealed bid auction, highestbidder wins pays second-highest price. format several advantages agentbased settings. Firstly, communication efficient terms number interactions,since requires bidder place single bid once, auctioneer respondbidder outcome. contrast, iterative auction English auctionusually requires several interactions typically time consuming. Secondly,single-auction case (i.e., bidder places bid one auction), optimalstrategy bid true value thus requires computation (once valuationitem known). strategy also weakly dominant (i.e., independentbidders decisions), therefore requires information preferencesagents (such distribution valuations).3.2 Global Local Biddersdistinguish global local bidders. former bid numberauctions, whereas latter bid single one. Local bidders assumed bidaccording weakly dominant strategy bid true valuation.6 consider twoways modelling local bidders: static dynamic. first model, number localbidders assumed known equal n auction. latter model,hand, average number bidders equal n, exact number unknownmay vary auction. uncertainty modelled using Poisson distribution(more details provided Section 4.1).later show, global bidder bids optimally higher expected utilitycompared local bidder, even though items complete substitutes bidderrequires one them. Nevertheless, identify number compelling reasonsbidders may choose bid globally:Participation Costs. Although bidding may automated autonomousagent, still takes time and/or money, entry fees time setup account,participate new auction. case, marginal benefits bidding twoauctions instead single auctions less participation costs, buyerbetter choosing bidding one auctions.Information. Bidders may simply aware auctions selling typeitem. Even known, however, bidder may sufficient informationdistribution valuations bidders number participating bidders.Whereas information required bidding single auction (because6. Note that, since bidding true value optimal local bidders irrespective others bidding,strategy affected presence global bidders.944fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutesdominance property second-price auction), important bidding multiplesimultaneous auctions. information obtained expert user learnedtime, often available novice.Risk Attitude. Although global bidder obtains higher utility average,bidder runs risk incurring loss (i.e., negative utility) winning multipleauctions. risk averse bidder may willing take chance, may chooseparticipate fewer even single auction avoid potential loss. Whetherbidder chooses reduce number auctions single one depends degreerisk aversion. general, would expect agent take less risk bid fewerauctions stakes higher, i.e., case high-value transactions. globalbidding agent, hand, could representing large firm sufficient fundscover losses, agents likely risk neutral.Budget Constraints. Related previous point, bidder may sufficientfunds take loss case wins one auction. detail, fixedbudget b, sum bids exceed b, thereby limiting number auctionsbidder participate and/or lowering actual bids placed auctions(see also Section 5.1 investigate budget constraints detail).Bounded Rationality. become clear paper, optimal strategyglobal bidder harder compute local one. bidder thereforebid globally costs computing optimal strategy outweigh benefitsadditional utility. Moreover, concept local bidder also captures notionmanual bidder use intelligent bidding agent compute optimalbid. human bidder clearly easier bid true value single auctionrequires calculation (again, given value known).4. Identical Simultaneous Auctionssection, provide theoretical analysis optimal bidding strategyglobal bidder participates identical simultaneous auctions. particular, addresscase bidders local simply bid true valuation. However,analysis general applies setting distribution bestcompetitive bid known satisfies certain properties. describe globalbidders expected utility Section 4.1, show Section 4.2 always optimalglobal bidder participate maximum number simultaneous auctions available.Subsequently, Section 4.3 significantly reduce complexity finding optimalbids multi-auction problem case auctions equivalent, applymethods find optimal strategies specific examples.4.1 Global Bidders Expected Utilityfollows, number sellers (auctions) 2 = {1, . . . , m} denotesset available auctions. Let G denote cumulative distribution function bestcompetitive bid particular auction g corresponding density function. Equiva945fiGerding, Dash, Byde & Jenningslently, G(b) probability winning specific auction conditional placing bid bauction. introduce following assumptions regarding function G:Assumption 1. cumulative distribution G(x) bounded support [0, vmax ], continuous within range, strictly increasing 0 < x < vmax .global bidder valuation v (0, vmax ] (NB. consider trivial casev = 0, assume v within bounds G) places global bid b,vector specifies (possibly different) bid bi [0, vmax ] auction . Now,given setting described Section 3 identical auctions, expected utility Uglobal bidder global bid b valuation v given by:"U (b, v) = v 1#(1 G(bi ))iMXZiMbiyg(y)dy.(1)0Here, left part equation valuation multiplied probabilityglobal bidder wins least one auctions thus corresponds expectedbenefit. Qdetail, note (1 G(bi )) probability winning auctionwhenQbidding bi , iM (1 G(bi )) probability winning auction, thus[1 iM (1 G(bi ))] probability winning least one auction. right partEquation (1) corresponds total expected costs payments. see latter,expected payment single second-price auction bidding b equalsRnoteb0 yg(y)dy independent expected payments auctions.Now, Equation (1) used address setting bidders exceptglobal bidder local. done follows. Let number local bidders n1. local bidders valuation v [0, vmax ] independently drawn cumulativedistribution F probability density f , F (x) properties G(x) (i.e.,support [0, vmax ], continuous within range, strictly positive 0 < x <vmax ). Note that, since dominant strategy local bidder bid true value,additional assumptions needed knowledge local bidders regardingdistributions bidders . case local bidders static, i.e., exactly nlocal bidders equal distributions, G simply highest-order statistic G(b) = F (b)n ,g(b) = dG(b)/db = nF (b)n1 f (b). However, use equation modeldynamic local bidders following way:Lemma 1. replacing highest-order statistic G(y) corresponding densityfunction g(y) with:G(y) = en(F (y)1) ,g(y) = dG(y)/dy = n f (y)en(F (y)1) ,Equation (1) becomes expected utility number local bidders auctiondescribed Poisson distribution average n (i.e., probability n localbidders participate given P (n) = nn en /n!).Proof prove this, first show G() F () modifiednumber bidders per auction given binomial distribution (where bidders decision946fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutesparticipate given Bernoulli trial) follows:G (y) = F (y)N = (1 p + p F (y))N ,(2)p probability bidder participates auction, N totalnumber bidders. see this, note participating equivalent bidding zero.result, F (0) = 1 p since 1 p probability bidder bids zerospecific auction, F (y) = F (0) + p F (y) since probability p bidderbids according original distribution F (y). Now, average number participatingbidders given n = p N . replacing p n/N , Equation (2) becomes G (y) =(1 n/N + (n/N )F (y))N . Note Poisson distribution given limitbinomial distribution. keeping n constant taking limit N , obtainG (y) = en(F (y)1) = G(y).results follow apply static dynamic model unless stated otherwise.4.2 Participation Multiple Auctionsshow that, valuation 0 < v < vmax , utility-maximising global bidderalways place non-zero bids available auctions.7 prove this, showexpected utility increases placing arbitrarily small bid compared participating auction. holds even auctions identical. followinglet bbj =bj denote global bid b j th bid bj = bj . formally,Theorem 1. Assumption 1 given global bidder valuation 0 <v < vmax , consider global bid b bi v . Suppose bj = 0auction j , Equation (1) maximised, i.e., exists bj > 0U (bbj =bj , v) > U (b, v).Proof need show exists bj > 0 U (bbj =bj , v) U (b, v) >0. Using Equation (1), marginal expected utility participating auction j w.r.t.bidding zero auction written as:U (bbj =bj , v) U (b, v) = vG(bj )(1 G(bi ))iM \{j}Now, using integration parts,equation rewritten as:U (bbj =bj , v) U (b, v) = G(bj ) vR bj0yg(y) = bj G(bj )iM \{j}Zbjyg(y)dy.0R bj0G(y)dy(1 G(bi )) bj +ZbjG(y)dy.(3)07. note necessarily hold boundary case v = vmax . However, practice,find even optimal strategy bid true value multiple auctions insteadtrue value single auction. especially case number auctions large (seealso Section 4.3.4).947fiGerding, Dash, Byde & JenningsClearly, since bj > 0 g(x) > 0 x > 0 (due Assumption 1) G(bj )R bjpositive. Moreover, given bi v < vmax0 G(y)dy strictlyQQv > 0, follows v iM \{j} (1 G(bi )) > 0. Now, suppose set bj = 12 v iM \{j} (1h QRbG(bi )), U (bbj =bj , v) U (b, v) = G(bj ) 12 v iM \{j} (1 G(bi )) + 0 j G(y)dy > 0thus SU (bbj =bj , v) > U (b, v).proof applies setting auctions identical. However, easysee argument holds G differs auction.result states that, even though risk winning payone item (and buyer disposes additional items won), best strategyparticipate auctions. Therefore, expectation, increasing probability winningsingle item outweighs possible loss incurred winning one them.obtain better understanding true, consider following intuitiveargument. Suppose global bidder bids k < auctions. case,non-zero probability bidder wins none auctions, thusnon-zero expected demand least one items remaining auctions. Sinceargument holds k < m, induction global bidder bid auctions.Note Theorem 1 holds v strictly smaller vmax . casev = vmax two possibilities: either optimal bid vmax one auctioncase bids auctions zero (since bidder guaranteed win),optimal bid vmax strictly positive auctions. showSection 4.3.4 find emperically first true number auctionssmall, latter case large numbers auctions.4.3 Optimal Global Bidgeneral solution optimal global bid requires maximisation Equation (1)dimensions, arduous task, even applying numerical methods. section,however, show reduce entire bid space single dimension cases,thereby significantly simplifying problem hand. First, however, order findoptimal solutions Equation (1), set partial derivatives zero:U(1 G(bj )) bi = 0.(4)= g(bi ) vbijM \{i}QNow, equality (4) holds either g(bi ) = 0 bj b\{bi } (1 G(bj ))v bi = 0.model dynamic local bidders, g(bi ) always greater zero, thereforeignored (since g(0) = nf (0)en assume f (y) > 0). cases, g(bi ) = 0bi = 0. However, Theorem 1 shows optimal bid non-zero 0 < v < vmax .Therefore, ignore first part, second part yields:bi = v(1 G(bj )).(5)jM \{i}words, optimal bid auction equal bidders valuation multipliedprobability winning auctions. straightforward show948fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutessecond partial derivative negative, confirming solution indeed maximumkeeping bids constant. Moreover, since optimal bid requires 0 < bi <vmax v < vmax (due Theorem 1 since bidding valuation clearlysuboptimal), need consider interior solutions. Thus, Equation (5) providesmeans derive optimal bid auction i, given bids auctions.Now, taking partial derivatives auction rewriting Equation (5),optimal global bid must obey following relationship: b1 (1 G(b1 )) = b2 (1 G(b2 )) =. . . = bm (1 G(bm )). defining H(b) = b(1 G(b)) rewrite equation to:H(b1 ) = H(b2 ) = . . . = H(bm ) = v(1 G(bj )).(6)jMfollows, apply Equation (6) reduce search space, first show that,large class probability distributions, optimal global bid consists twodifferent values, thereby reducing search space two dimensions. Section 4.3.2reduce single dimension. Section 4.3.3 consider limit resultsnumber auctions goes infinity. Finally, Section 4.3.4 perform numericalanalysis optimal bidding strategy specific cases number auctionsfinite, consider extent global bidder benefits compared bidding locally.4.3.1 Reducing Search Spacesection, first show optimal global bids consists two differentvalues function H(b) = b(1 G(b)) unique critical point. formally,introduce following requirement:Assumption 2. H(b) = b(1 G(b)) unique critical point bf , i.e., exists bfs.t. dbH(bf ) = 0 b 6= bf , dbH(b) 6= 0.go show requirement met wide class distributionscharacterised non-decreasing hazard rate. Let two bid values denoted bb+ , b b+ . Formally:Theorem 2. Assumptions 1 2, global bid b maximising Equation (1)contains two distinct bid values: b b+ . addition, b bf b+ , bfunique critical point H(b) = b(1 G(b)).Proof Suppose H unique critical point, bf . H(0) = H(vmax ) = 0, H(b) > 0(0, vmax ), bf must fact global maximum H. Furthermore, b < bfH strictly increasing b > bf H strictly decreasing. also impliesH(x) = two solutions: > H(bf ) solutions, since bfglobal maximiser H; = H(bf ) unique solution, namely bf ; < H(bf )applying intermediate value theorem H interval [0, bf ] gives one solution,namely b bf , interval [bf , vmax ] gives other, namely b+ bf .one solution interval H strictly monotonic each.Now, Equation (6) implies H(bi ) equal . Therefore, givenH(x) = two solutions, two distinct interior bids bi ,949fiGerding, Dash, Byde & Jenningsnamely b b+ . mentioned before, Theorem 1 utility maximizing solution0 < bi < vmax v < vmax , therefore solutions interior ones. casev = vmax either b+ = vmax b = 0, b+ < vmaxb > 0.show unique critical point H guaranteed G non-increasinghazard rate within interval [0, vmax ]. choose property since encompasseslarge number distributions, including log-concave density functionsuniform, normal exponential (we refer work Barlow, Marshall, Proschan,1993 Bergstrom Bagnoli, 2005 list functions). Formally, hazard rate(see e.g., Krishna, 2002) cumulative distribution function F denoted Fdefined by:F (x)f (x).1 F (x)following result:Lemma 2. Assumption 1, G (b) non-decreasing (0, vmax ), functionH(b) = b(1 G(b)) unique critical point bf interval.Proof See Appendix A.1.extend proof distribution functions local bidders F showingthat, hazard rate F non-decreasing, hazard rate G also non-decreasing,thus reduction also applies. holds local static bidders. Formally:Theorem 3. F (b) non-decreasing, G (b) Gb (b) also non-decreasing,b = en(F (y)1) n 2.G(b) = F (b)n GProof See Appendix A.1.4.3.2 Characterising Optimal BidsUsing results able reduce optimal global bid two values, highbid, b+ bf , low bid, b bf . However, know number auctionsbid high low. section show optimal bid highone auction, low auctions. Using result, writehigh bid terms low bids, reducing search space even further.Theorem 4. Assumptions 1 2, global bid b maximises Equation 1one high bid, i.e., one bi > bf , bf uniquecritical point H(b) = b(1 G(b)).Proof See Appendix A.1.950fiOptimal Strategies Simultaneous Vickrey Auctions Perfect SubstitutesTogether Lemma 2 implies that, distributions non-decreasing hazardrates, either exists one high bid 1 low bids, bids low. Notecalculate value high bid analytically given low-bid value usingEquation (5). Consequently, finding optimal global bid reduces optimising singlevariable (i.e., value low bids). value computed numericallyusing standard optimisation techniques Quasi-Newton method, or, alternatively,bids discretised brute-force search applied find optimum.Whatever method selected important notice that, due reductionsearch space, computational complexity calculating optimal outcome numericallyindependent number auctions (or indeed number bidders).result Theorem 4 suggests optimal restrict attention singleauction bidding high auction, using remaining auctions backupcase high-bid auction fails. show Section 4.3.4 often casepractise number auctions small. number auctions large,however, find optimal bid low auctions, irrespectivebidders valuation. derive theoretical results limit case numberauctions goes infinity Section 4.3.3, consider empirical results finitecase Section 4.3.4.4.3.3 Limit Resultssection investigate optimal bidding changes number auctionsbecomes large consider whether general patterns characteriseoptimal strategy. first, basic result that, number auctions increases,agent able extract increasingly greater utility approaches maximumpossible utility, v. prove this, without loss generality restrict strategyglobal bidder considering uniform bidding (i.e., bids equal). Let budenote optimal global bid bidding auctions bidder confinedusing uniform bids:Theorem 5. Assumption 1, expected utility defined Equation 1playing optimal uniform bid buconverges v, sense > 0constant > implies U (bu) > v .Proof See Appendix A.2.Note that, since v upper bound utility achieved globalbidder, result implies that, number auctions increases, eventually uniformbidding always superior non-uniform bidding. formally:Corollary 1. Assumption 1, sufficiently large globally optimal bid bmaximising Equation (1) equal optimal uniform bid bu, independent v.951fiGerding, Dash, Byde & JenningsR bfProof corollary follows Theorem 5 = EP (bf ) = 0 xg(x) dx: >uniform bidding gives utility least v , whereas non-uniform bidding gives strictly less:U (b ) < v EP (b+ ) < v EP (bf ) = v .practice, find necessary number auctions particularlylarge uniform bidding optimal v. end, next section provideexamples optimal bidding strategy specific settings.4.3.4 Empirical Evaluationsection, present results empirical study characterise optimalglobal bid specific cases finite numbers auctions. Furthermore, measureactual utility improvement obtained using global strategy.results presented based uniform distribution valuations vmax = 1,static local bidder model, generalise dynamic modeldistributions. Figure 1 illustrates optimal global bids corresponding expectedutility various n = 5, bid curves different values nfollow similar pattern.shown Figure 1, bidders relatively low valuation, optimal strategysubmit equal bids at, close to, true value. valuation reachescertain point, however, placing equal bids longer optimal strategynumber auctions small (in Figure 1 occurs = 4 = 6). point,so-called pitchfork bifurcation observed optimal bids split two values:single high bid 1 low ones. experiments, however, consistently observeoptimal strategy always place uniform bids valuation relativelylow. Moreover, bifurcation point moves right increases, disappearsaltogether becomes sufficiently large (m 10 Figure 1) pointoptimal bids uniform (note holds even v = vmax ). Note alsouniform optimal bids move closer zero tends infinity.illustrated Figure 1, utility global bidder becomes progressively higherauctions. Note that, consistent limit results Section 4.3.3,utility approaches upper bound v number auctions becomes large. absoluteterms, improvement especially high bidders average valuation,close vmax . bidders range thus benefit bidding globally.bidders low valuations small chance winningauction, whereas bidders high valuation high probability winningsingle auction benefit less participating auctions. contrast, shownFigure 1, consider utility relative bidding single auction, much higherbidders relatively low valuations. particular, notice global bidderlow valuation improve utility times expected utility biddinglocally. Intuitively, chance winning one auctions increasesfactor m, whereas increase expected cost negligible. high valuationbuyers, however, benefit obvious chances winning relativelyhigh even case single auction.952fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes10.8bid0.6m=1m=4m=6= 10= 102= 106= 10100.40.2000.20.40.60.810.81valuation (v)1expected utility0.80.60.4m=1m=4m=6= 10= 102= 106= 10100.2000.20.40.6valuation (v)10m=1m=4m=6= 10relative utility8642000.20.40.60.81valuation (v)Figure 1: optimal global bid, corresponding expected utility, expectedutility proportional local bidder setting n = 5 static localbidders varying number auctions (m). Note results = 1correspond local bidder. comparison, expected utility alsoshown number auctions approaches infinity.953fiGerding, Dash, Byde & Jennings5. Extensions: Budgets, Non-Identical Sequential Auctionsprevious section, considered best response strategy global bidder facesmultiple identical simultaneous auctions financial constraints. section,generalise results settings. particular, investigate three importantextensions:1. Section 5.1 investigate setting global bidder limited budgetconstrains sum bids exposure.2. Section 5.2 investigate setting auctions differ probabilitywinning. differences arise, example, auctions different numberslocal bidders participating.3. Section 5.3 extend results sequential auctions consider settingresources interest auctioned simultaneously sequentially.5.1 Budget-Constrained Biddingderivation optimal global bid shown optimal strategyglobal bidder bid simultaneous auctions. Now, strategy implicitlyassumes bidder face financial constraints (i.e., bidder payitems won). However, often bidder limited resources may restrict biddingstrategy. section study budget limit space possible strategiesavailable global bidder affect optimal strategy. particular, considercase budget constrains exposure, i.e., sum bids.8 occurs,example, global bidder limited liquidity faces negative consequencescannot pay items wins (e.g., going bankrupt thrownsystem).Now, importance taking budget constraints account becomes evenpronounced following result shows that, number auctions increases,required budget exposure unconstrained case exceed given limit:9Theorem 6. Assumption 1 assuming g(b) bounded throughout [0, vmax ],v > 0 C, exists forPexposure optimal globalbid b maximising Equation (1) exceeds C, i.e., iM bi > C.Proof See Appendix A.3.Thus, despite low probability bidder pay sum bids (especially,given auction second-price auction), practice bidder may still wantlimit amount.8. note that, although practice budget constraint payments rather bids, evensecond-price auctions worst-case outcome bidder pays bids auctions,therefore hard budget constraint equivalent constraining sum bids.9. Note occurs despite fact bids tend zero goes infinity. However, exposure(see proof Theorem 5).954fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutesdetail, formulate budget-constrained problem faced bidderas:max U (b, v)s.t.b[0,v]mXbi C,(7)iMC budget limit U (b, v) utility global bidder givenEquation (1). consider budget constraints distinguishing three cases.following, b = (b1 , . . . , bm ) refers unconstrained optimal solution (i.e., optimalcsolution absence budget constraints), whereas bc = (bc1 , . . . , bm ) refersoptimal global bid subject budget constraints.PCase (1)iM bi > C v C.Here, sum unconstrained optimal bids exceeds budgettherefore required recompute optimal bid given constraint. Moreover,budget constraint equal less valuation. case,able show best strategy bid single auction place bidbi = C auction fairly general probability density functions g.doing, provides one justifications existence local bidders (asoutlined Section 3.2). Although result intuitive, straightforwardsince bidder may still decide divide budget across several auctions.provide proof result.PCase (2)iM bi > C v < C.previous case, need recompute optimal bid. However,show, optimal strategy global bidder case bid multipleauctions (as unconstrained case). Moreover, contrast unconstrainedcase, global bid may consist two different values.PCase (3)iM bi C.case, sum unconstrained optimal bids less budget.Thus result trivial b = bc .Examples three different cases depicted Figure 2. figure showsoptimal strategy bidding 4 simultaneous auctions local bidders uniformlydistributed valuations within range [0, 1] global bidder budget constraintC = 0.8.10 Clearly, global bidder low valuation budget constraintaffect optimal bidding strategy (case 3). Case 1 occurs bidder valuation0.8, case 2 occurs in-between range. figure shows, optimalstrategy latter two cases qualitatively different unconstrained case;whereas unconstrained optimal strategy bid high single auction,consists single low bid, several high bids, placing zero remainingauctions. budget becomes tighter relative valuation, number auctionsbidder participates decreases single auction remains (see Figure 2, case1). follows, first consider conditions behaviour optimal10. Similar patterns observed optimal strategy varying number auctions budget.patterns always grouped three cases.955fiGerding, Dash, Byde & Jennings10.81bid0.6case (1)0.42case (3)30.2case (2)40010.210.410.60.81valuation (v)Figure 2: solid lines denote optimal bids global bidder budget C = 0.8setting n=5 m=4. Here, numbers indicate numberauctions certain bid placed. valuation increases (andbudget remains constant), bids auctions taper one one,single high bid remains bids zero. dotted line representsunconstrained solution.case 1, subsequently address case 2 detail. Case 3 trivial thereforeconsidered further.provide formal result optimal strategy case 1 show that,budget constraint imposed global bidder equal less valueattaches item wishes acquire, best strategy act local bidderfollowing conditions:Assumption 3. probability density function best competitive bid g(x) convexg(0) = 0.result stated formally follows:Theorem 7. Assumptions 1 3, global bidder budget C v,Equation (7) satisfied bc = (C, 0, . . . , 0), i.e., optimal bid C exactly oneauction.Proof See Appendix A.3.Intuitively, convexity g(x) necessary ensure probability winningauction increases sufficiently quickly bid increases. way, higher bid956fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutessingle auction results higher probability winning compared dividingamount several auctions. Although placing higher bid single auction may leadhigher expected payment, proof shows utility obtained increasedprobability winning outweighs expected payment increase. time,condition g(0) = 0 important, otherwise probability winning increases sufficientlyquickly optimal spread budget multiple auctions. importancetwo conditions becomes apparent following corollary shows that,special case C = v, condition g(0) = 0 fact necessary conditionbidding single auction optimal, and, furthermore, strategy longeroptimal case g(x) concave.Corollary 2. Assumption 1, either g(0) > 0 g(x) strictly concave,global bidder budget C = v, Equation (7) satisfied bidding strictly positiveleast two auctions.Proof See Appendix A.3.Note that, case static local bidders, condition g(x) convex holds,example, f (x) convex, increasing, non-negative (and thus F (x) also convex),since g(x) = nF n1 (x)f (x). However, condition holds general caseswell; especially number local bidders high, concave local bidder distributioneasily result convex g(x). Also note condition g(0) = 0 holds casestatic local bidders n 2, even f (0) > 0 (but case dynamic localbidders unless f (0) = 0). Finally, note convexity g(x) implies non-decreasinghazard rate therefore conditions stronger imposed Assumption 2Section 4.3.move consider cases 2 3. Case 2 cannot analysed easilycase 1. However, good insight constrained optimal strategy obtainedconsidering Lagrangian Equation (7). Since budget constraint inequalityconstraint, addition regular Langrangian multiplier need introduce slackvariable convert inequality equality. variable first Psquared ensurepositive value added constraint becomes C iM bi + 2 = 0.Langrange function becomes follows:!X(B, , ) = U (b, v) +bi C + 2 .(8)iMsetting partial derivatives zero, results following + 2 equationssolved:X(.)=bi C + 2 = 0,iM(.)= 2 = 0,(.)U (b, v)=+ = 0 M.bibi957(9)fiGerding, Dash, Byde & Jennings2.512sum bids0.8Case 2ConstrainedUnconstrainedbid0.60.4=010.50.2001.50.20.40.60.81000.20.40.60.81valuation (v)valuation (v)(a) Optimal bids(b) Sum bidsFigure 3: optimal global bid corresponding sum bids unconstrained case case budget constraint C = 1.5. Here, n=10m=3.equations, readily observed case 1, = 0 thereby leadingP case solution Equation (9) forces total budget spent, i.e.,iM bi = C.case 2, either = 0 corresponding local maximum Equation (4),total budget spent = 0 whereby total budget spent. twopossible situations highlighted Figure 3, showing optimal global bidding strategyn = 10, = 3 budget C = 1.5, corresponding sum bids (notecase 1 arise since budget exceeds highest valuation). unconstrainedsolution also provided comparison. example shows total amount spentnecessarily equal available budget, even unconstrained optimal solutionexceeds budget. Thus, case 2 cannot solely consider solutions whereby sumbids equal budget since may case bidding less budgetyields greater utility (i.e., = 0).Furthermore, observed Figures 2 3(a), introduction budgetconstraint changes shape optimal strategy cases 1 2. Recall Section 4.3 optimal strategy unconstrained case either bid equallyauctions bid high one auction low equally remaining ones.However, budget constraint, region best strategybid high one auction low remaining ones. Moreover, partssolution consist three different bids: high, low, zero. Hence, Equation (5)longer holds case budget constraints, structure bids observed Section 4.3need satisfied. means cannot apply reduction search spaceefficiently compute optimal strategy case budget constraints case 2,complexity computing optimum using brute force increases exponentiallynumber auctions.958fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes5.2 Non-Identical AuctionsWhereas previously assumed auctions equivalent, relax assumptiongeneral case non-identical auctions. Here, assume auctionsdiffer global bidders valuation item sold, rather probability obtaining item given bid. differences arise, example,number bidders and/or local bidders valuation distribution vary one auctionanother. used practice specific information individualauctions available.11 detail, assume auction individualcumulative distribution function, denoted Gi (b), corresponding density functiongi (b). Now, expected utility given by:"#X Z biygi (y)dy,(10)U (b, v) = v 1(1 Gi (bi ))iMiM0b = (b1 , . . . , bm ) global bid, specifying bid auction before.easy see Theorem 1 Section 4.2 extends setting non-identicalauctions optimal strategy bid strictly positive amount auction,provided Gi (b) strictly positive M, 0 < b vmax . But, auctionsnon-identical complicates bidding space. revisit search space problemnon-identical auctions Section 5.2.1 different method employed reducebidding space. Section 5.2.2, show global bidder always bid higherpreferred auction. optimal bidding strategy number non-identicalauctions empirically evaluated Section 5.2.3.5.2.1 Calculating Optimal Global BidAlthough reduction search space described Section 4.3 longer possibleauctions identical, still significantly reduce computation neededfind optimal bid. end, first set partial derivatives U/bi zero. Given gi (bi ) > 0 have:bi = v(1 Gk (bk )).(11)kM \{i}combining partial derivatives, obtain following relationship:(1 Gk (bk )).b1 (1 G1 (b1 )) = b2 (1 G2 (b2 )) = . . . = bm (1 Gm (bm )) = v(12)kMGenerally, possible find optimal global bid analytically. Using expression (12),however, confine search one bids then, valuebid, determine values 2 bids computationally less demanding onedimensional root-finding operations (see e.g., Burden & Faires, 2004). remaining bidvalue found analytically using Equation (11).11. example, eBay auctions reveal number visits web pages particular items,used estimate number participating bidders individual auctions (see http://www.ebay.comexamples).959fiGerding, Dash, Byde & Jenningsclarify reduction search space, show applied discretebid space using brute-force search approach. find optimal (discrete) globalbid, iterate discrete space one bids, say b1 . value b1find corresponding m1 bid values expression (12) satisfied follows.first calculate b1 (1 G1 (b1 )) search discrete values b2b1 (1 G1 (b1 )) = b2 (1 G2 (b2 )).12 Typically least two values b2equality holds.13 solutions stored memory repeatedbi < bm . value bm calculated using Equation (11). waycalculate expected utility given b1 expression (12) satisfied. optimalsolution found maximising expected utility across possible values b1combinations solutions stored memory. Note amountcomputation required find expected utility single value b1 increases linearlynumber auctions. However, number combinations increases exponentiallynumber auctions. Nevertheless, since base exponential typically2 (in particular case non-decreasing hazard rates, see Footnote 13), computationremains tractable number auctions relatively small becomes intractablelarge settings.14 specific cases, however, one auction clearly betteranother auction precise sense, possible reduce number combinationsthus required computation becomes linear number auctions. issueaddressed next section.5.2.2 Preferred Auctions Optimal Bidsmany cases, possible find auctions favourable othersterms expected utility. example, else equal, bidder expectedbetter average auctions fewer bidders. bidder would therefore preferauctions less profitable ones, irrespective bidders valuation.section, formalise notion preferred auction, investigate optimal bidsbidding multiple auctions respect preferences auctions.use concept stochastic orders (Shaked & Shanthikumar, 1994) rankauctions terms global bidders preferences. Formally, auction j stochastically dominates auction Gj (b) Gi (b) 0 b vmax (Krishna, 2002, Appendix B).expected utility least high bidding auction comparedbidding amount auction j. shownR bas follows. write expectedutility U auction i/j as: Ui/j = (v b)Gi/j (b) + 0 Gi/j (x)dx. Since Gi (b) Gj (b)RbRb0 Gi (x)dx 0 Gj (x)dx auction j dominates auction i, expected utilitybidding auction least high auction j. therefore refer auction12. case discrete bids equality rarely holds exactly resolved minimisingdifference instead, i.e., minimising b1 (1 G1 (b1 )) b2 (1 G2 (b2 )).13. precisely, function b (1 G(b)) single critical value, twosolutions. shown earlier Lemma 2 case G non-decreasing hazard rate. Notethat, case discrete bids, solutions local minima taking difference.14. provide idea means practice, implemented brute-force search Javafinds optimal global bid 100 discrete valuations and, so, searches 100 bids pervaluation per auction. Using settings, 1.66GHz Intel Centrino optimal solutionfound within 4 seconds = 10, takes 35 seconds = 15.960fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutes(weakly) preferred auction. Furthermore, say auction strictly preferredauction j b Gi (b) > Gj (b).bidding multiple auctions, show optimal bid higherpreferred auctions. Intuitively, because, case second-price auctions, preferredauctions give agent higher probability winning good lower price.observation reduce computation required since bid values higherbids less preferred auctions need considered. practice usuallyreduces number solutions auction satisfy expression (12) singleone (in case computation optimal global bid becomes linear numberauctions). addition computational benefit, observation also providesguidelines intuition strategies. relationship preferred auctionsoptimal global bid precisely described follows:Theorem 8. Assumption 1, i, j M, 6= j: auction (weakly) preferredauction j, i.e., Gi (b) Gj (b) 0 b vmax , auction strictly preferredbi bj , bi bj maximise Equation (10), bi > bj v > 0.Proof first prove bi bj contradiction, go show bi 6= bj .Suppose opposite holds exist i, j bi , bj b optimalbi < bj . show expected utility strictly higher interchangingtwo bids, i.e., bidding bi auction j bj auction i. Let bbi bj denote globalbid b two bids bi , bj interchanged. show U (bbi bj , v) U (b, v)strictly positive 0 v vmax . Using Equation (10) itegration parts obtainfollowing:U (bbi bj , v) U (b, v) = (v c bi )(Gj (bi ) Gi (bi )) + (v c bj )(Gi (bj ) Gj (bj ))Z bjGi (y) Gj (y)dy, (13)v c Gj (bi )Gi (bj ) + v c Gj (bj )Gi (bi ) +biQc = kM \{i,j} (1 Gk (bk )). First note c > 0 strict preferenceauctions bi bj requires 0 < bi < vmax ,0 < bj < vmax (since Gi (0) = Gj (0) =Gi (vmax ) = Gj (vmax ) = 0), therefore due Theorem 7 holds 0 < bk < vmaxk \{i, j}. Next, note last term Equation (13) always positive sincebj > bi Gi (b) Gj (b). therefore ignore term remaining part alsopositive. Let term denoted following. Since assumed bi bjoptimal, Equation (11) following holds:bi = v c (1 Gj (bj )) v c bi = v c Gj (bj )),bj = v c (1 Gi (bi )) v c bj = v c Gi (bi )).(14)replacing v c bi v c bj Equation (13) v c Gj (bj ) v c Gi (bi ) respectively,rearranging terms, obtain following:U (bbi bj , v) U (b, v) = v c (Gi (bi ) Gj (bi ))(Gi (bj ) Gj (bj )) + .961(15)fiGerding, Dash, Byde & Jennings10.8bid fraction x = b/vbid fraction x = b/v10.8n1n1n1n1n10.60.40.6= 6, n2 = 7= 6, n2 = 8= 6, n2 = 9= 6, n2 = 15= n2 = 60.70.80.60.40.2000.9valuation (v)n = {5, 6, 7, 8, 9, 10, 11}n = {5, 5, 5, 7, 7, 7, 7}0.51valuation (v)(a) 2 auctions(b) 7 auctionsFigure 4: Optimal global bid fraction valuation (a) two (b) seven simultaneous auctions various settings. auctions differ number localbidders present auction.Now, since Gi (bi ) > Gj (bi ), Gi (bj ) > Gj (bj ), v > 0, c > 0, 0, followsU (bbi bj , v) > U (b, v). result, bi bj cannot optimal bids auctions jrespectively, contradicts initial proposition.proves bi bj . show bi must strictly higherbj . follows directly Equation (14). Suppose bi = bj , Equation (14)follows Gi (bi ) = Gj (bi ) Gi (bj ) = Gj (bj ). However, conflictsrequirement auction must strictly preferred bi bj , thereby completingproof.proof based requirement auction strictly preferredauction j least two optimal bids auctions. Although requiresoptimal bid known advance, proof also applies cases auction strictlypreferred auction j entire range 0 < b < vmax . case, example,number bidders auction strictly less auction j. Furthermore,note similar results hold slightly different conditions. Specifically, auctionstrictly preferred set points non-zero measure anywhere within range[bj , bi ], bi bj since > 0 Equation (14). Consequently, condition bi bjholds auction preferred auction j Gi , Gj described analyticfunctions.5.2.3 Empirical Evaluationsection, examine bidding strategy global bidder affectedauctions different numbers local bidders. end, find optimal bidsmaximise utility given Equation (10) using standard optimisation technique (Press,Flannery, Teukolsky, & Vetterling, 1992). numerical results shown assume962fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutesbidder valuation distribution uniform, although similar results obtainedcommonly used distributions. Two different scenarios evaluated. first,investigate optimal bids change increasing differences local biddernumbers case two simultaneous auctions (see Figure 4(a)). Here, numberlocal bidders first auction (n1 ) fixed 6 whereas number local bidderssecond auction (n2 ) varied 6 15 second (note depictoptimal strategy fraction valuation (bi /v) rather actual bid value sinceclearly demonstrates effect preferred auctions). Figure 4(a) showsglobal bidder bids higher preferred auction (i.e., one fewer bidders),consistent Lemma 8. valuation increases, bid preferred auction firstdecreases relative valuation, increases, similar casenumber local bidders. However, higher bid much closer true valuation,especially difference auctions large.simulation extended auctions second scenario. resultstwo different settings shown Figure 4(b). one setting (solid lines),seven auctions different number local bidders. settings (dashed lines)results shown 3 auctions 5 local bidders 4 auctions7 local bidders. before, observe global bidder always bids higherauctions fewer local bidders. Although appearance bifurcation pointcommon simultaneous auctions number bidders, casenumber bidders identical. global bidder always bidsdifferent amounts auctions different numbers bidders. result, bifurcationphenomenon indicates transition equal bids high-low bids occur.5.3 Sequential Simultaneous Auctionssection extend analysis optimal bidding strategy sequential auctions.Specifically, auction process consists R rounds, round numberauctions running simultaneously. combination sequential simultaneousauctions common practice, especially online.15 turns analysiscase simultaneous auctions easily extended include sequential auctions.following, set simultaneous auctions round r denoted r , vectorbids round br . before, analysis assumes bidderslocal bid single auction. initially assume global bidders completeknowledge number rounds number auctions roundrelax assumptions.expected utility round r, denoted U r , similar (Equation (1)Section 4.1) except additional benefit obtained future auctionsdesired item one current set simultaneous auctions. convenience,U r (br , r ) abbreviated U r following. expected utility thus becomes:15. Rather purely sequential nature, online auctions also often overlap (i.e., new auctionsstart others still ongoing). case, however, optimal wait bidnew auctions outcome earlier auctions known, thereby reducing chanceunwittingly winning multiple items. Using strategy, overlapping auctions effectively becomesequential thus analysed using results section.963fiGerding, Dash, Byde & JenningsrrrU = v P (b )X ZiM r=Ur+1+ (v Ur+1briyg(y)dy + U r+1 (1 P r (br ))0rr)P (b )X ZiM rbriyg(y)dy ,0(16)QP r (br ) = 1 iM r (1 G(bri )) probability winning least one auctionround r. Now, take partial derivative Equation (16) order find optimalbid brj auction j round r:rU(1 G(bri )) brj .(17)= g(brj ) (v U r+1 )brjriM \{j}Note Equation (17) almost identical Equation (4) Section 4.3, exceptvaluation v replaced v U r+1 . optimal bidding strategy thusfound backward induction (where U R+1 = 0) using procedure outlined Section 4.3.Now, first relax assumption global bidder complete knowledgenumber auctions future rounds. Let p(m) denote probabilityauctions next round let mmax denote maximum number auctions. Furthermore, let Mj set j auctions U r (br , Mj ) expected utility round rj auctions round. uncertainty numberinPmmaxof auctionsr+1r+1corporated Equations (16) (17) replacing Uj=0 p(j)U(br+1 , Mj ).Furthermore, uncertainty number rounds addressed adding discount factor 0 < 1 represents probability auctionsround. Finally, note equation similarly extended non-identicalauctions settings G depends number auctions and/or round.6. Market EfficiencyEfficiency important system-wide property since characterises extentmarket maximises social welfare (i.e., sum utilities agents market).end, section study efficiency markets either static dynamiclocal bidders, impact global bidder efficiency markets.Specifically, efficiency context maximised bidders highestvaluations (recall denotes number auctions, also total numberitems) entire market obtain single item each. simplicity assumetotal number bidders market least m. formally, define efficiencyallocation as:Definition 1. Efficiency Allocation. efficiency K allocation Kobtained social welfare proportional maximum social welfare achievedtype market given by:PnTvi (K)K = Pni=1,(18)i=1 vi (K )964fiOptimal Strategies Simultaneous Vickrey Auctions Perfect SubstitutesPK = arg maxKK ni=1vi (K) efficient allocation, K set possibleallocations (assuming bidders allocated auction), vi (K) bidder utilityallocation K K, nT total number bidders participating market.Now, order measure efficiency market impact global bidder,run simulations markets without global bidder differenttypes local bidders. experiments carried follows. bidders valuation(both local global) independently drawn uniform distribution support[0, 1]. experiments without global bidder, additional local bidder placed oneauctions overall number bidders average comparedcase global bidder. local bidders bid true valuations, whereasglobal bidder bids optimally auction described Section 4.3. experimentsrepeated 10000 times order get statistically significant results 99% confidenceinterval.results experiments shown Figure 5. Note degree inefficiencyinherent multi-auction market local bidders.16 example,two auctions selling one item each, two bidders highest valuationsbid locally auction, bidder second-highest valueobtain good. Thus, allocation items bidders inefficient, inefficiencyincreases number auctions increases (keeping average number bidders perauction equal). observed Figure 5, however, efficiency increasesn becomes larger. differences bidders highestvaluations become smaller, thereby decreasing loss efficiency.Furthermore, Figure 5 shows one local bidders replaced globalbidder generally creates positive effect efficiency number bidderssmall, significant change occurs many local bidders (thisholds static dynamic local bidders). latter comes surprise sinceimpact single bidder diminishes bidders competing auction.former, hand, obvious; introduction global bidder potentiallyleads decrease efficiency since bidder unwittingly win one item.Furthermore, global bidder generally bid true valuation also resultinefficient outcomes. However, results show that, average, opposite occurs.because, global bidder local bidders, highprobability local bidder low valuation win item. global bidder,hand, likely win auction sufficiently high valuation (eventhough global bids true valuation, shown Section 4.3.4 bidsoften uniform fairly close true value). effect particularly pronouncedcase dynamic local bidders since may occur auction local bidderwhatsoever, case global bidder wins item sure.16. exception n = 1 bidders static, since market completely efficient withoutglobal bidder. However, since special case apply settings,discuss here.965fiGerding, Dash, Byde & Jenningsefficiency (K )10.9521460.9531234567880.8570.82LocalBiddersGlobalBidder2222StaticStaticDynamicDynamicYesYes6666StaticStaticDynamicDynamicYesYes4681012(average) number local bidders (n)Figure 5: Average efficiency different market settings shown legend,number auctions (items), n average number local biddersper auction. error-bars indicate 99% confidence intervals.7. Conclusionspaper, derive utility-maximising strategies agent bid multiple,simultaneous second-price auctions. first analyse case single global bidderbids auctions, whereas bidders local bid single auction.setting, find optimal place non-zero bids auctions sell desireditem, even bidder requires single item derives additional benefitmore. Thus, potential buyer achieve considerable benefit participatingmultiple auctions employing optimal bidding strategy.common valuation distributions, show analytically optimal bidsidentical auctions consist two values, high bid low bid,optimal bid high one auction. Moreover, writing high bid termslow ones, problem finding optimal bids reduces optimising singlevariable. considerably simplifies original optimisation problem thusused practice compute optimal bids number auctions. Furthermore,show that, number auctions becomes large, optimal bid uniformly acrossauctions. also analyse setting auctions identical differprobability winning. Although still optimal participate auctions, findbest strategy bid relatively high favourable auctions (i.e.,probability winning highest). investigate practical considerations well.show budget constraints limit number auctions bidders participate in.Specifically, global bidders budget equal less valuation, optimalstrategy reverts bidding single auction certain conditions, thereby justifying966fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutespresence local bidders. Furthermore, consider sequential auctions showresults readily applied markets auctions occur sequentiallysimultaneously. Finally, compare efficiency market multiple simultaneousauctions without global bidder. show that, local bidder replacedglobal one, increases average efficiency thus social welfare sysstemnumber bidders small, significant effect found many localbidders.number interesting directions future work. First all, whereaspaper focuses buyer, intend extend analysis consider revenuesellers point view. particularly relevant sellers proactive role set auction parameters reserve price try maximiseexpected revenue. closely related research competing sellers,shown optimal auction parameters depend competitionsellers, since affects number potential buyers attracted particularauction (McAfee, 1993; Peters & Severinov, 1997; Burguet & Sakovics, 1999; Gerding et al.,2007b). However, current literature competing sellers assumes buyersparticipate single auction, shown suboptimal. casesellers optimise auction parameters buyers participate number auctionssimultaneously far investigated. Furthermore, paper takendecision-theoretic approach analysing case single global bidder. interestingopen problem characterising game-theoretic solution case multiple globalbidders interact strategically.Acknowledgmentspaper significantly extended preliminary version published previously (Gerding, Dash, Yuen, & Jennings, 2007a).research undertaken part EPSRC (Engineering Physical ResearchCouncil) funded project Market-Based Control (GR/T10664/01). collaborativeproject involving Universities Birmingham, Liverpool Southampton BAESystems, BT HP. research also undertaken part ALADDIN (Autonomous Learning Agents Decentralised Data Information Systems) projectjointly funded BAE Systems EPSRC strategic partnership. addition, wouldlike thank Alex Rogers, Ioannis Vetsikas, Wenming Bian, Florin Constantininput. authors also grateful Adam Prugel-Bennett valuable helpproofs, Richard Engelbrecht-Wiggans useful discussionscomments. Finally, would like thank reviewers thorough detailedfeedback.Appendix A. ProofsA.1 Reduction Search SpaceLemma 2. Assumption 1, G (b) non-decreasing (0, vmax ), functionH(b) = b(1 G(b)) unique critical point bf interval.967fiGerding, Dash, Byde & JenningsProof critical point H following equation must hold:H(b) =[b(1 G(b))] = 1 bg(b) G(b)dbdb1 G(b)1= g(b)b = g(b)b = 0.g(b)G (b)(19)Now, since g(b) > 0 (due Assumption 1) , equality (19) holds 1/G (b) b = 0.Therefore, order show (19) one solution, sufficient show1/G (b) b either strictly increasing strictly decreasing. However, sinceboundaries b = 0 b = vmax h(0) = 1 h(vmax ) = vmax g(vmax ), sinceg(b) > 0, need consider latter. Since b strictly decreasing, sufficientshow 1/G (b) non-increasing. assumption G (b) non-decreasing,thus 1/G (b) non-increasing.Theorem 3. F (b) non-decreasing, G (b) Gb (b) also non-decreasing,b = en(F (y)1) n 2.G(b) = F (b)n GProof case static local bidders provethus showing1G(b)g(b)g(b)1G(b)1G(b)g(b)non-increasing function,non-decreasing function. detail, refactorfollows:1 F n (b)1 G(b)=g(b)nF n1 (b)f (b)1 F (b) 1 + F (b) + . . . + F n1 (b)=f (b)nF n1 (b)1 1 F (b)=[1 + F 1 (b) + . . . + F 1n (b)].nf (b)(20)1F (b)f (b)non-increasingDue non-decreasing hazard rate f (b), derivefollows:b1 F (b)f (b)=vf (b)1 F (b)h1 F (b)f (b)2i21F (b)0. Furthermore, first order conditionf (b)[1 + F 1 (b) + . . . + F 1n (b)] F 2 (b) . . . (n 1)F n (b),since0second part equation20,negative b. Thus,second part strictly decreasing. Therefore, given first part non-increasingsecond part strictly decreasing, implies overall 1G(b)g(b) strictly decreasing.Hence G (b) non-decreasing.968fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutescase dynamic local bidders, rewrite Gb (b) as:g(b)nf (b)en(F (b)1)=b1 en(F (b)1)1 G(b)#"nf (b)(1 F (b))en(F (b)1)=1 F (b)1 en(F (b)1)1 F (b)nf (b).=1 F (b) en(1F (b)) 1(21)Since F non-decreasing, clearly first part equation (21) non-decreasing. shall1F (b)prove second part equation (21), (b) = en(1F(b)) 1 , also non-decreasing.first order condition (b) given by:f (b)(en(1F (b)) 1) + (1 F (b))(nf (b)en(1F (b)) )(b)=.b(en(1F (b)) 1)2(22)Since denominator equation (22) always non-negative, thus sufficient shownumerator always non-negative. numerator equation (22) rewrittenas:hf (b)en(1F (b)) n 1 nF (b) + en(F (b)1) ,first second terms > 0 b. Thus, order provenumerator iishalways non-negative, remains shown third term, n1nF (b)+en(F (b)1) ,non-negative. term equal n 1 + en > 0 (since n > 1) 0 extremumsbmin bmax respectively. Furthermore, first order differential term yieldsnf (b)[1 en(F (b)1) ] < 0, b. Hence third term also > 0, b.Lemma 3. H(b) = b(1 G(b)) unique critical pointb > bfg(b) > (1 G(b))/b,b < bfg(b) < (1 G(b))/b.similarlyProof saw already, H unique critical point b > bf iff H decreasingb. Thereforeb > bf H (b) < 0 1 G(b) bg(b) < 0 g(b) > (1 G(b))/b.result follows analagously.Theorem 4. Assumptions 1 2, global bid b maximises Equation 1one high bid, i.e., one bi > bf , bf uniquecritical point H(b) = b(1 G(b)).969fiGerding, Dash, Byde & JenningsProof second derivatives U interior critical point b follows:2Ub2i2Ubi bj= g (bi ) v (1 G(bj )) bi g(bi ) = g(bi ),j6=i= g(bi )g(bj )v(1 G(bk ))k6=i,jg(bj )bj= g(bi ).1 G(bi )(23)proof contradiction: show two high bids criticalpoint cannot local maximum U . done showing two highbids b, Hessian matrix entries (23) positive eigenvalue, equivalentlyexists vector = (a1 , a2 , . . . , )Xai aji,j2U(b) > 0.bi bj(24)show Hessian matrix b positive eigenvalue, means beither local minimum saddle point (Magnus & Neudecker, 1999, Chapter 6),turn means small enough displacement direction leads increaseU , contradicting local optimality b.Assume without loss generality auctions rearranged b1 = b2 =b+ > bf . Lemma 3 implies g(b+ ) > (1 G(b+ ))/b+ . choose= (1, 1, 0, . . . , 0), (24) becomes:Xi,jai aj2U(b) =bi bj2U2U2U(b)+(b)2(b)b2b1 b2b21g(b+ )b+= 2g(b+ )11 G(b+ )> 0.A.2 Limit Casessection examine global bidders expected utility number auctionsgoes infinity. particular, prove large enough number auctions,optimal behaviour always bid uniformly.First all, note (5) definition optimal uniform bid bu,u m1bu.= v(1 G(bm ))(25)Lemma 4. smallest bid b optimal (possibly non-uniform) bid vector b tends0 uniformly v .RbProof follows fact expected payment function EP (b) = 0 xg(x) dxstrictly monotonically increasing continuous function b, hence monotonically970fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutesincreasing continuous inverse, EP 1 : [0, EP (vmax )] [0, vmax ] EP 1 (0) = 0.0 < U (b)=b<<<v mEP (b )EP 1 v/mEP 1 vmax /m0first step towards proving uniform bidding eventually globally optimalshow utility optimal bidding converges maximum value v:Theorem 5. Assumption 1, expected utility defined Equation 1playing optimal uniform bid buconverges v, sense > 0constant > implies U (bu) > v .Proof First note result trivial v < since U (bu) 0 always.definition utility (1), using (25),uU (bu)=v1(1G(b))mEP (bum)uu= v bu(1 G(bm )) mEP (bm ).(26)uuLemma 4 know bu0, implies bm (1 G(bm )) 0, turnattention expected payment term, showing tends zero .definition (25) bum,ln(bm /v)m=+ 1,ln(1 G(bm ))ln(bu/v)ulim mEP (bm ) = lim+ 1 EP (bum)ln(1 G(bu))uln(bu/v)EP (bm ).(27)= limln(1 G(bu))prove limit zero demonstrate stronger limitln(b/v)EP (b)= 0,b0 ln(1 G(b))lim(28)applying LHopitals rule multiple times. First apply numerator(28):EP (b)lim ln(b/v)EP (b)= limb0b0 (ln(b/v))1bg(b)= lim 1b0 b (ln(b/v))2= lim bg(b) b(ln(b/v))2b0= 0,971fiGerding, Dash, Byde & Jenningslast limit holds uniformly v vmax .numerator shown converge zero, possible applyLHopitals rule directly (28),ln(b/v)EP (b)b0 ln(1 G(b))limb1 EP (b) + g(b)b ln(b/v)b0g(b)(1 G(b))1EP (b)+ b ln(b/v)= limb0bg(b)bg(b)= lim+ b ln(b/v) .b0 g(b) + bg (b)= lim(29)(30)g (b) 0 small enough b order g(b) 0 true everywhere, impliesbg(b)< b 0,g(b) + bg (b)limit b ln(b/v) 0 obvious, limit (30) must zero. turn impliesexpected payment uniform bidding tends zero, expected valuetends v, thus theorem proved.A.3 Budget Constraintsfollows provide proof proposition optimal strategy bidsingle auction certain conditions, exposure constrainedvaluation. proof number auctions complex. Therefore, first provideformal proof case two auctions, generalise result two.proof two-auction case also provides building block inductive proofscontained general case.First show unconstrained optimum bid eventually exceeds given budget:Theorem 6. Assumption 1 assuming g(b) bounded throughout [0, vmax ],v > 0 C, exists forPexposure optimal globalbid b maximising Equation (1) exceeds C, i.e., iM bi > C.Proof begin with, note conditions Corollary 1 met, factrestrict attention uniform bidding sufficiently large, particular. show v > V > 0 C > 0mbu> C.proof contradiction. Assume constant C bumm < Cu < KC/m, (usingm. bounded density assumption, G(bu)KbEquation (25)):bu=m1v(1 G(bu))>v(1 KC/m)m1>V (1 KC/m)m1V eKC972.(31)fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutesufact buthus bounded contradicts bound bm < C.following Lemma characterises bidding strategy budget C v= 2. result used Theorem 7 prove general case 2.Lemma 5. = 2 optimal bidding strategy global bidder budget C vgiven bc = (C, 0) probability density function g(x) convex g(0) = 0.Proof order prove lemma, consider difference optimal bidbc arbitrary bid b = b1 , b2 b 6= bc b1 + b2 = C:Z Chyg(y)dy v 1 (1 G(b1 ))(1 G(b2 ))U (bc , v) U (b , v) = vG(C)0Z b1Z b2yg(y)dyyg(y)dy .00Hence,U (bc , v) U (b , v) = (v C)G(C) +ZCG(y)dy vG(b1 ) vG(b2 ) + b1 G(b1 )Z b1Z b2+ vG(b1 )G(b2 )G(y)dy + b2 G(b2 )G(y)dy00Z C= (v C)(G(C) G(b1 ) G(b2 )) +G(y)dy0b2(C b1 )G(b1 ) (C b2 )G(b2 )Zb1G(y)dy(32)0+ vG(b1 )(G(b2 )= (v C)(G(C) G(b1 ) G(b2 )) +b1 G(b2 )Zb1ZCG(y)dy b2 G(b1 )b2G(y)dy + vG(b1 )G(b2 ).0Now, prove bc indeed optimal bid, need show differencepositive b satisfies budget constraint. RIn order so, first separateCequation two parts, namely, X(b1 , b2 ) = b2 G(y)dy b2 G(b1 ) b1 G(b2 )R b10 G(y)dy (v C)Y (b1 , b2 ) = (G(C) G(b1 ) G(b2 )). showX positive; thus implying that, since (v C) positive, U (bc , v) U (b , v)also positive.Now, rewrite X terms b1 replacing b2 C b1, X becomes:X(b1 ) =ZCG(y)dy (C b1 )G(b1 ) b1 G(C b1 )Cb1Z0973b1G(y)dy.(33)fiGerding, Dash, Byde & Jenningsorder find local maxima minima set derivativeXb1zero:X= G(C b1 ) (C b1 )g(b1 ) + G(b1 ) + b1 g(C b1 ) G(C b1 ) G(b1 )b1= b1 g(C b1 ) (C b1 )g(b1 ) = 0.(34)Since g(0) = 0, easy see least three solutions Equation (34),namely b1 = {0, C, C/2}. shall show first derivative X alwaysnon-negative within range b1 = [0, C/2], implying X increases within range.Since X(0) = 0 solution symmetric around C/2, follows X alwaysnon-negative. formally, need show that:X0, b1 [0, C/2].b1prove holds g(x) convex. Now, definition convexfunction have:g(x1 + (1 )x2 )) g(x1 ) + (1 )g(x2 ), 1.Now, let x2 = 0. Then:g(x1 ) g(x1 ) + (1 )g(0).Since g(0) = 0, becomes:g(x1 ) g(x1 )1g(x1 ) g(x1 ).Let x1 = (C b1 ) =b1Cb1 .(35)Then, Equation (35) becomes:C b1g(b1 ) g(C b1 )b1b1 g(C b1 ) (C b1 )g(b1 ) 0.(36)XThus, shows b0, which, turn, proves X 0.1prove = G(C) G(b1 ) G(b2 ) 0. Since assume g(x) convexpositive Rin interval [0, 1] g(0) = 0, follows g(x) increasingxthus G(x) = 0 g(x)dx also convex. result, use condition Equation (35)x1 +b2 )G(x). Now, replacing b1 +bx1 b1 + b2 , G(x) G(bb1 +b2 x2x b1 + b2 follows that:G(b1 + b2 )G(b1 + b2 )+ b2b1 + b2b1 + b2b1b2G(b1 ) + G(b2 ) G(b1 + b2 )+b1 + b2 b1 + b2G(b1 ) + G(b2 ) G(b1 + b2 ).G(b1 ) + G(b2 ) b1974fiOptimal Strategies Simultaneous Vickrey Auctions Perfect SubstitutesHence, = G(C) G(b1 ) G(b2 ) 0. result, U (b , v) U (b , v) 0b1 + b2 = C. Note far assumed sum bids equal budgetconstraint, mentioned case b1 + b2 < C. showoptimal bid full budget (i.e., b1 + b2 = C).Consider arbitrary B = b1 , b2 b1 + b2 = C. Then, replacing Cabove, know U ((S, 0), v) U ((b1 , b2 ), v). Hence, remains shownU ((C, 0), v) U ((S, 0), v). consider difference two bids,Equation (1) is:ZZ Cyg(y)dyyg(y)dy vG(S) +U ((C, 0), v) U ((S, 0), v) = vG(C)00Z CG(y)dy= v (G(C) G(S)) CG(C)0Z+ SG(S)G(y)dy0= (v C)(G(C) G(S)) +(C S)G(S).ZCG(y)dyNow, G(C) G(S) positive since C definition G(x) non decreasing and,RCtherefore, G(y)dy (C S)G(S) also positive. Hence, U ((C, 0), v) U ((S, 0), v) 0.thus shown global bidder bid one auction budgetconstrained C v bidding two simultaneous auctions, providedg(x) convex g(0) = 0 results generalised case 2.detail, following theorem holds:Theorem 7. Assumptions 1 3, global bidder budget C v,Equation (7) satisfied bc = (C, 0, . . . , 0), i.e., optimal bid C exactly oneauction.cProof before, consider differenceP optimal bid b another bidcb = (b1 , . . . , bm ) b 6= b C = iM bi , show positive:U (bc , v) U (b , v) =vG(C)ZXZiMCyg(y)dy0biyg(y)dy0=(v C)G(C) +Z!C+bi G(bi )iM975iMhG(y)dy v 10Xhv 1(1 G(bi ))ZbiG(y)dy .0iM(1 G(bi ))(37)fiGerding, Dash, Byde & Jenningsinserting (v C)rewritten as:PG(bi ) (v C)iMcPiMG(bi ) Equation (37)XU (b , v) U (b , v) = (v C) G(C)!G(bi )iMCXiMhv 1= (v C) G(C)XX ZG(bi )(1 G(bi ))iMXG(bi )iM(C bi )G(bi ) +Z+ZXG(bi )CG(y)dy0biG(y)dy0iMG(y)dy0iM!CG(y)dy bi G(bi )bi0iM+ZhXv 1(1 G(bi ))G(bi ) .iMiMNow, define variables Xm Zm as:ZZ CXG(y)dy(C bi )G(bi ) +Xm =0hXiZm = 1(1 G(bi )).G(y)dy ,0iMiMbiiMThen, rewrite Equation (37) as:cU (b , v) U (b , v) = (v C) G(C)X!G(bi )iM+ Xm + vZm .(38)Pprove part equation positiveP (i.e., (v C)(G(C) iM G(bi ))0, Xm 0, vZ 0). first prove G(C) iM G(bi ) 0 using relationshipPEquation (35) convex functions. Specifically, taking = bCi since C = iM biyields:CG(bi )biXXbi G(C)CG(bi )G(C)PiMiMXiM biG(C)G(bi )CiMXG(C)G(bi ).iM976fiOptimal Strategies Simultaneous Vickrey Auctions Perfect SubstitutesSince, definition case studied, v C, (vC)(G(C)0.PiMG(bi ))P prove Xm 0 using inductive argument. detail, let =iM \{m} bi = C bm . Xm written terms Xm1 follows:Xm ==ZZXCG(y)dy0(C bi )G(bi ) +iMG(y)dy0XiM \{m}biG(y)dy0Z(S bi )G(bi ) +(C bm )G(bm ) (C S)= Xm1 +ZXC0ZZCPiM \{m} G(bi ),G(y)dybmG(y)dy0XG(bi )iM \{m}Since G(S) = G(C bm )ZG(y)dy +G(y)dy (C bm )G(bm ) bmXm Xm1 +biG(bi )iM \{m}ZZbmG(y)dy.0:CG(y)dy (C bm )G(bm ) bm G(C bm )CbmZbmG(y)dy.0RCLemma 5, shown Cbm G(y)dy (C bm )G(bm ) bm G(C bm )R bm0 G(y)dy 0 therefore Xm Xm1 . base case X2 shownwithin proof Lemma 5 positive. Hence Xm 0.use inductive argument finally prove Zm 0. base case Zm= 2 yields:hXZ2 = 1(1 G(bi ))G(bi )iMiMh= 1 (1 G(b1 ))(1 G(b2 )) G(b1 ) G(b2 )= G(b1 )G(b2 )0.inductive hypothesis formulated Zm 0 Zm1 0. order provehypothesis, express Zm function Zm1 :977fiGerding, Dash, Byde & JenningsZm =(1 G(bi )) +iMXG(bi ) 1iM= (1 G(bm ))(1 G(bi )) +iMiM \{m}= (1 G(bm )) Zm1XiM \{m}= Zm1 G(bm )Zm1 + G(bm )= Zm1 + G(bm )XiM \{m}= Zm1 + G(bm ) 1G(bi ) + 1 +XiM \{m}XG(bi ) 1XG(bi ) 1iMG(bi )G(bi ) Zm1iM \{m}Zm1 .(1 G(bi ))thus proves inductive step, along base case, thereby provesZm 0. Hence third part Equation (38) also positive. Since three partsequation positive, impliesPthat U (bc , v) indeed optimal bc =(C, 0, . . . , 0). Note assumed C = iM bi . However, using argumentLemma 5 easy see indeed optimal bid full budget.Corollary 2. Assumption 1, either g(0) > 0 g(x) strictly concave,global bidder budget C = v, Equation (7) satisfied bidding strictly positiveleast two auctions.Proof proof largely based reverse arguments Lemma 5. Without lossgenerality take = 2. Let bL = {C, 0} denote single-auction bid b = {b1 , b2 }arbitrary bid b 6= bL b1 + b2 = C, b1 , b2 0 before. Since v C = 0U (bL , v) U (b , v) = X(b1 ), X given Equation (33). Furthermore,Xb1 = b1 g(C b1 ) (C b1 )g(b1 ) (see Equation (34)). Now, order proveoptimal bid strictly positive auctions, sufficient show existsC > b1 > 0 X(b1 ) < 0 one two conditions holds.Xfirst show holds g(0) > 0. easy see b(0) = Cg(0) < 01X(0) thus strictly decreasing. Since X(0) = 0 proves X(b1 ) < 0 b1slightly larger zero. Since b2 = C b1 follows b2 > 0 long b1 < C,agent thus better bidding auctions.consider case g(0) = 0 g(x) strictly concave. replacingcondition convex functions Lemma 5 strictly concave functions followsb11 g(x1 ) > g(x1 ) 12 C > x1 > 0. setting x1 = (C b1 ) = Cb1978fiOptimal Strategies Simultaneous Vickrey Auctions Perfect Substitutesbefore, gives (see Equations (35) (36))b1 (0, C/2).Xb1= b1 g(C b1 ) (C b1 )g(b1 ) < 0ReferencesBarlow, R. E., Marshall, A. W., & Proschan, F. (1963). Properties probability distributions monotone hazard rate. Annals Mathematical Statistics, 34 (2),375389.Bergstrom, T., & Bagnoli, M. (2005). Log-concave probability applications. Economic Theory, 26 (2), 445469.Boutilier, C., Goldszmidt, M., & Sabata, B. (1999). Sequential auctions allocationresources complementarities. Proceedings 16th International JointConference Artificial Intelligence, pp. 527523.Burden, R. L., & Faires, J. D. (2004). Numerical Analysis, 8th edition. Brooks Cole.Burguet, R., & Sakovics, J. (1999). Imperfect competition auction design. InternationalEconomic Review, 40 (1), 231247.Byde, A., Preist, C., & Jennings, N. R. (2002). Decision procedures multiple auctions.Proceedings 1st International Joint Conference Autonomous AgentsMulti-Agent Systems, pp. 613620.Che, Y. K., & Gale, I. (1998). Standard auctions financially constrained bidders.Review Economic Studies, 65 (1), 121.Clearwater, S. H. (Ed.). (1996). Market-Based Control: Paradigm Distributed Resource Allocation. World Scientific Publishing.Dash, R. K., Parkes, D. C., & Jennings, N. R. (2003). Computational mechanism design:call arms. IEEE Intelligent Systems, 18 (6), 4047.Dash, R. K., Rogers, A., Reece, S., Roberts, S., & Jennings, N. R. (2005). Constrainedbandwidth allocation multi-sensor information fusion: mechanism design approach. Proceedings 8th International Conference Information Fusion,pp. 11851192.Dash, R. K., Vytelingum, P., Rogers, A., David, E., & Jennings, N. R. (2007). Marketbased task allocation mechanisms limited capacity suppliers. IEEE TransactionsSystems, Man Cybernetics: Part A, 37 (3), 391405.Engelbrecht-Wiggans, R. (1987). Optimal constrained bidding. International JournalGame Theory, 16 (2), 115121.Engelbrecht-Wiggans, R., & Weber, R. (1979). example multiobject auction game.Management Science, 25, 12721277.979fiGerding, Dash, Byde & JenningsGerding, E. H., Dash, R. K., Yuen, D. C. K., & Jennings, N. R. (2007a). Bidding optimallyconcurrent second-price auctions perfectly substitutable goods. Proceedings6th International Joint Conference Autonomous Agents Multi-AgentSystems, pp. 267274.Gerding, E. H., Rogers, A., Dash, R. K., & Jennings, N. R. (2007b). Sellers competingbuyers online markets: Reserve prices, shill bids, auction fees. Proceedings20th International Joint Conference Artificial Intelligence, pp. 12871293.Gopal, R., Thompson, S., Tung, Y. A., & Whinston, A. B. (2005). Managing risksmultiple online auctions: options approach. Decision Sciences, 36 (3), 397425.Greenwald, A., & Boyan, J. (2004). Bidding uncertainty: Theory experiments.Proceedings 20th Conference Uncertainty Artificial Intelligence, 209216.Greenwald, A., Kirby, R. M., Reiter, J., & Boyan, J. (2001). Bid determination simultaneous auctions: case study. Proceedings 3rd ACM ConferenceElectronic Commerce, pp. 115124.Hendricks, K., Onur, I., & Wiseman, T. (2005). Preemption delay eBay auctions.Working Paper, University Texas Austin.Jiang, A. X., & Leyton-Brown, K. (2007). Bidding agents online auctions hiddenbids. Machine Learning, 67 (1), 117143.Juda, A. I., & Parkes, D. C. (2006). sequential auction problem eBay: empiricalanalysis solution. Proceedings 7th ACM Conference ElectronicCommerce, pp. 180189.Krishna, V. (2002). Auction Theory. Academic Press.Krishna, V., & Benoit, J. P. (2001). Multiple object auctions budget constrainedbidders. Review Economic Studies, 68 (1), 155179.Krishna, V., & Rosenthal, R. (1996). Simultaneous auctions synergies. GamesEconomic Behavior, 17, 131.Lang, K., & Rosenthal, R. (1991). contractors game. RAND Journal Economics,22, 329338.Leyton-Brown, K., Shoham, Y., & Tennenholtz, M. (2000). Bidding clubs: Institutionalized collusion auctions. Proceedings 2nd ACM Conference ElectronicCommerce, pp. 253259. ACM Press New York, NY, USA.Magnus, J. R., & Neudecker, H. (1999). Matrix Differential Calculus ApplicationsStatistics Econometrics (2nd edition). John Wiley & Sons.McAfee, R. P. (1993). Mechanism design competing sellers. Econometrica, 61 (6), 12811312.980fiOptimal Strategies Simultaneous Vickrey Auctions Perfect SubstitutesMes, M., van der Heijden, M., & van Harten, A. (2007). Comparison agent-based scheduling look-ahead heuristics real-time transportation problems. European JournalOperational Research, 181 (1), 5975.Palfrey, T. R. (1980). Multi-object, discriminatory auctions bidding constraints:game-theoretic analysis. Management Science, 26 (9), 935946.Peters, M., & Severinov, S. (1997). Competition among sellers offer auctions insteadprices. Journal Economic Theory, 75, 141179.Peters, M., & Severinov, S. (2006). Internet auctions many traders. Journal Economic Theory, 130 (1), 220245.Pitchik, C. (2006). Budget-constrained sequential auctions incomplete information.Working Paper 230, Department Economics, University Toronto.Press, W. H., Flannery, B. P., Teukolsky, S. A., & Vetterling, W. T. (1992). NumericalRecipes C: Art Scientific Computing (2nd edition). Cambridge UniversityPress.Rogers, A., David, E., & Jennings, N. R. (2005). Self organised routing wireless microsensor networks. IEEE Transactions Systems, Man Cybernetics: Part A,35 (3), 349359.Rogers, A., David, E., Schiff, J., & Jennings, N. R. (2007). effects proxy biddingminimum bid increments within eBay auctions. ACM Transactions Web,1 (2), article 9, 28 pages.Rosenthal, R., & Wang, R. (1996). Simultaneous auctions synergies commonvalues. Games Economic Behavior, 17 (1), 3255.Roth, A. E., & Ockenfels, A. (2002). Last-minute bidding rules ending secondprice auctions: Evidence eBay Amazon auctions Internet. American Economic Review, 92 (4), 10931103.Rothkopf, M. H. (1977). Bidding simultaneous auctions constraint exposure.Operations Research, 25 (4), 620629.Rothkopf, M. H. (2007). Decision analysis: right tool auctions. Decision Analysis,4 (3), 167172.Shaked, M., & Shanthikumar, J. G. (1994). Stochastic Orders Applications.Academic Press.Shehory, O. (2002). Optimal bidding multiple concurrent auctions. International JournalCooperative Information Systems, 11, 315327.Stone, P., Schapire, R. E., Littman, M. L., Csirik, J. A., & McAllester, D. (2003). Decisiontheoretic bidding based learned density models simultaneous, interacting auctions. Journal Artificial Intelligence Research, 19, 513567.981fiGerding, Dash, Byde & JenningsStryszowska, M. (2004).Late multiple bidding competing second price.Working paper 2004.16, Fondazione Eni Enrico Mattei.Availablehttp://ideas.repec.org/p/fem/femwpa/2004.16.html.Szentes, B., & Rosenthal, R. (2003). Three-object two-bidder simultaneous auctions: Chopsticks tetrahedra. Games Economic Behavior, 44, 114133.Varian, H. R. (1995). Economic mechanism design computerized agents. Proceedings1st USENIX Workshop Electronic Commerce, pp. 1321.Wellman, M. P., Greenwald, A., & Stone, P. (2007). Autonomous Bidding Agents: StrategiesLessons Trading Agent Competition. MIT Press.Wellman, M. P., Reeves, D. M., Lochner, K. M., & Vorobeychik, Y. (2004). Price predictiontrading agent competition. Journal Artificial Intelligence Research, 21, 1936.Yuen, D., Byde, A., & Jennings, N. R. (2006). Heuristic bidding strategies multipleheterogeneous auctions. Proceedings 17th European Conference ArtificialIntelligence, pp. 300304.Zeithammer, R. (2005). equilibrium model dynamic auction marketplace. WorkingPaper, University Chicago.982fiJournal Artificial Intelligence Research 32 (2008) 203-288Submitted 11/07; published 05/08New Islands Tractability Cost-Optimal PlanningMichael Katz,Carmel Domshlak,dugi@tx.technion.ac.il,dcarmel@ie.technion.ac.ilFaculty Industrial Engineering Management,Technion - Israel Institute Technology, Haifa, IsraelAbstractstudy complexity cost-optimal classical planning propositional statevariables unary-effect actions. discover novel problem fragments optimization tractable, identify certain conditions differentiate tractableintractable problems. results based exploiting structural syntactic characteristics planning problems. Specifically, following Brafman Domshlak(2003), relate complexity planning topology causal graph.main results correspond tractability cost-optimal planning propositional problemspolytree causal graphs either O(1)-bounded in-degree, inducedactions one prevail condition each. Almost tractability resultsbased constructive proof technique connects certain tools planningtractable constraint optimization, believe technique interestdue clear evidence robustness.1. PrecisAI problem solving inherently facing computational paradox. one hand,general tasks AI reasoning known hard, degree membership NP sometimes perceived good news. hand,intelligence somehow modeled computation, computation delegatedcomputers, artificial intelligence escape traps intractability muchpossible. Planning one reasoning tasks, corresponding finding sequencestate-transforming actions achieve goal given initial state. well knownplanning intractable general (Chapman, 1987), even simple classicalplanning propositional state variables PSPACE-complete (Bylander, 1994).ups downs interest planning community formalcomplexity analysis planning problems, growing understanding dayscomputational tractability fundamental issue problem solving. pragmaticreasons twofold.1. Many planning problems manufacturing process controlling systemsbelieved highly structured, thus potential allow efficient planning exploiting structure (Klein, Jonsson, & Backstrom, 1998). fact,structure accounted explicitly, general-purpose planner likelygo tour exponential search space even tractable problems. Moreover,since intractable theories provide guarantees performance engineering systems, cases guarantees required unavoidable designc2008AI Access Foundation. rights reserved.fiKatz & Domshlakcontrolled system complexity-aware manner planningprovably tractable (Williams & Nayak, 1996, 1997).2. Computational tractability invaluable tool even dealing problemsfall outside known tractable fragments planning. instance, tractablefragments planning provide foundations (if all) rigorous heuristicestimates employed planning heuristic search (Bonet & Geffner, 2001; Hoffmann,2003; Helmert, 2006; Hoffmann & Nebel, 2001; Edelkamp, 2001). particulartrue admissible heuristic functions planning typically definedoptimal cost achieving goals over-approximating abstraction planning problem hand. abstraction obtained relaxing certain constraintsspecification original problem, purpose abstractionprovide us provably tractable abstract problem (Haslum, 2006; Haslum &Geffner, 2000; Haslum, Bonet, & Geffner, 2005).Unfortunately, palette known tractable fragments planning still limited,situation even severe tractable optimal planning. knowledge,less handful non-trivial fragments optimal planning knowntractable. difference theoretical complexity regular optimalplanning general case (Bylander, 1994), many classical planning domainsprovably easy solve, hard solve optimally (Helmert, 2003). Practice also providesclear evidence strikingly different scalability satisficing optimal general-purposeplanners (Hoffmann & Edelkamp, 2005).work show search new islands tractability optimal classicalplanning far exhausted. Specifically, study complexity optimalplanning problems specified terms propositional state variables, actionschanges value single variable. sense, continue line complexityanalysis suggested Brafman Domshlak (2003), extend satisficingoptimal planning. results first time provide dividing line tractableintractable problems.1.1 UB (Optimal) Planning ProblemsProblems classical planning correspond reachability analysis state modelsdeterministic actions complete information. work focus state modelsdescribable certain fragment SAS+ formalism (Backstrom & Nebel, 1995)allows propositional state variables unary-effect actions. Following BackstromNebel (1995), follows refer subclass SAS+ UB (shortunary-effect, binary-valued). Somewhat surprisingly, even non-optimal planning UBPSPACE-complete, is, hard general propositional planning (Bylander, 1994).Definition 1 SAS+ problem instance given quadruple = hV, A, I, Gi, where:V = {v1 , . . . , vn } set state variables, associated finite domainDom(vi ); initial state complete assignment, goal G partialassignment V , respectively.204fiTractable Cost-Optimal Planning= {a1 , . . . , } finite set actions, action pair hpre(a), eff(a)ipartial assignments V called preconditions effects, respectively. actionassociated non-negative real-valued cost C(a). action applicablestate Dom(V ) iff s[v] = pre(a)[v] whenever pre(a)[v] specified. Applyingapplicable action changes value variable v eff(a)[v] eff(a)[v]specified.SAS+ problem instance belongs fragment UB SAS+ iff statevariables V binary-valued, action changes value exactly one variable,is, A, |eff(a)| = 1.Different sub-fragments UB defined placing syntactic structural restrictions actions sets problems. instance, Bylander (1994) showsplanning UB domains action restricted positive preconditionstractable, yet optimal planning UB fragment hard. general, seminalworks Bylander (1994) Erol, Nau, Subrahmanian (1995) indicate extremelysevere syntactic restrictions single actions required guarantee tractability, evenmembership NP. Backstrom Klein (1991) consider syntactic restrictionsglobal nature, show UB planning tractable two actions effect,preconditions two actions require different values variablesaffected actions. Interestingly, fragment UB, known PUBS, remainstractable optimal planning well. characterizing properties PUBSrestrictive, result Backstrom Klein provided important milestoneresearch planning tractability.Given limitations syntactic restrictions observed Bylander (1994), Erol et al.(1995), Backstrom Klein (1991), recent works studied impactposing structural mixed structural/syntactic restrictions action sets.scope UB, works relate complexity planning topologicalproperties problems causal graph structure.Definition 2 causal graph CG() SAS+ problem = hV, A, I, Gi digraphnodes V . arc (v, v ) belongs CG() iff v 6= v exists actionchanging value v preconditioned value v, is,eff(a)[v ] pre(a)[v] specified.Informally, immediate predecessors v CG() variables directlyaffect ability change value v, evident constructing causalgraph CG() given UB planning problem straightforward. instance, consideraction set depicted Figure 1a. easy verify actions setunary effect. causal graph induced action set depicted Figure 1b.actions a1 a2 actions change values v1 v2 , respectively,actions preconditions outside affected variables. Hence, causal graphcontains arcs incoming nodes v1 v2 . hand, actions changingv3 v4 preconditioned (in cases) values v1 v2 , thus v3v4 incoming arcs v1 v2 .Way used complexity analysis, causal graphs (sometimesindirectly) considered scope hierarchically decomposing planning tasks (Newell &205fiKatz & Domshlaka1a2a3a4a5v10pre(a)v2 v3v4v11eff(a)v2 v3v4001101v2BBBB |||B||| BBB~||100v1v310(a)v4v1v2|||||||~ |v3(b)v4(c)Figure 1: Example two simple action sets fit characteristics UB fragment.(a) Unary-effect action set propositional variables V = {v1 , . . . , v4 }, (b)Causal graph induced A, (c) Causal graph induced \ {a4 }.Simon, 1963; Sacerdoti, 1974; Knoblock, 1994; Tenenberg, 1991; Bacchus & Yang, 1994).first result relating complexity UB planning structurecausal graph due Backstrom Jonsson (1995, 1998b) identify fragmentUB, called 3S, interesting property inducing tractable plan existenceyet intractable plan generation. One key characteristics 3S acyclicitycausal graphs. special case 3S also independently studied WilliamsNayak (1997) scope incremental planning general SAS+ problems.recently, Brafman Domshlak (2003) provide detailed account complexity finding plans UB problems acyclic causal graphs. resultsclosely related problems examined paper, thus surveydetails. ease presentation, introduce certain notation heavily usedthroughout paper.node v CG(), In(v) Out(v) denote in- out-degrees v,respectively, In(CG())/Out(CG()) stand maximal in-degree/out-degreeCG() nodes.Assuming CG() connected1 , provide special notation following topologies acyclic causal graphs, also depicted Figure 2. causal CG()tree In(CG()) 1, exists v V In(v) = 0.inverted tree Out(CG()) 1, exists v V Out(v) = 0.P polytree CG() contains undirected cycles. (For example polytreeneither tree inverted tree see Figure 1c Figure 2.)directed-path singly connected one directed path nodev CG() node v CG(). (For example directed-pathsingly connected DAG see Figure 1b Figure 2.)1. CG() consists connected components, components identify independent subproblems easily identified treated separately.206fiTractable Cost-Optimal PlanningFigure 2: Examples causal graphs topologies considered paper, alonginclusion relations induced fragments UB.follows, use T, I, P, refer corresponding fragments UB,use subscript/superscript b refer fragment induced additional constraintin-degree/out-degree bounded constant. hard verifyT, P S, Pb Pb ; complete inclusion hierarchysub-fragments UB shown Figure 3a.key tractability result Brafman Domshlak (2003) corresponds polynomial time plan generation procedure Pb , is, UB problems inducing polytreecausal graphs nodes O(1)-bounded indegree. addition, BrafmanDomshlak show plan generation NP-complete fragment S, noteproof claim easily modified hold Sbb . results tractabilityhardness (as well immediate implications) depicted Figure 3bshaded bottom-most transparent top-most free-shaped regions. empty freeshaped region corresponds gap left Brafman Domshlak (2003).gap recently closed Gimenez Jonsson (2008) prove NP-completenessplan generation P. note proof Gimenez Jonsson actually carriesfragment well, gap left Brafman Domshlak (2003)entirely closed.1.2 Summary Resultscomplexity results Brafman Domshlak (2003) Gimenez Jonsson(2008) correspond satisficing planning, distinguish plans207fiKatz & Domshlak(a)(b)Figure 3: Inclusion-based hierarchy complexity plan generation UB problems acyclic causal graphs. (a) hierarchy STRIPS fragments corresponding tree, inverted tree, polytree, directed-path singly connectedtopologies causal graph, (possibly) O(1) bounds causal graphin-degree and/or out-degree. (b) Plan generation tractable fragments(bottom-most) shaded region, NP-complete depicted fragments. top-most intermediate (transparent) regions correspondresults Brafman Domshlak (2003) Gimenez Jonsson (2008), respectively.basis quality. contrast, study complexity optimal plan generationUB, focusing (probably canonical) cost-optimal (also known sequentiallyoptimal) planning. Cost-optimalplanning corresponds task finding planPminimizes C() = C(a). provide novel tractability results cost-optimalplanning UB, draw dividing line tractable intractableproblems. Almost tractability results based proof technique connectscertain tools planning tractable constraint optimization. stronglybelieve proof-technical contribution paper interest dueclear evidence robustnessour different algorithms exploit proof technique,much different manners.rest section aim providing adequate description resultsreaders want delve formal details, prefer firstreading paper.2 Hence, formal definitions, constructions, proofs underlyingresults given later, starting Section 2.2. adopted format seminal paper Bylander (1994) feel format contributedsomething making paper extremely enjoyable read.208fiTractable Cost-Optimal Planning1.2.1 Cost-Optimal Planning PbFollowing Brafman Domshlak (2003), relate complexity (cost-optimal) UBplanning topology causal graph. consider structural hierarchy depicted Figure 3a. begin considering cost-optimal planning Pbapparent Figure 3b expressive fragment hierarchystill candidate tractable cost-optimal planning. first positive result affirmspossibility, showing complexity map cost-optimal planning UBfragments Figure 3a identical satisficing planning (that is, Figure 3b).algorithm Pb based compiling given Pb problem constraintoptimization problemPCOP = (X , F) variables X , functional components F,global objective min F (X )(I) COP constructed time polynomial description size ,(II) tree-width cost network COP bounded constant, optimaltree-decomposition network given compilation process,(III) unsolvable assignments X evaluate objective function, otherwise, optimum global objective obtainedassignments X correspond cost-optimal plans ,(IV) given optimal solution COP , corresponding cost-optimal planreconstructed former polynomial time.compilation scheme, solve COP using standard, poly-time algorithm constraint optimization trees (Dechter, 2003), find optimal solution. compilation based certain property cost-optimal plans Pballows conveniently bounding number times state variable changes valuealong optimal plan. Given property Pb , state variable v compiledsingle COP variable xv , domain COP variable correspondspossible sequences value changes v may undergo along cost-optimal plan.functional components F defined one COP variable xv , scopefunction captures family original state variable v causal graph,is, v immediate predecessors CG(). illustration, Figure 4adepicts causal graph P problem , family state variable v4depicted shaded region, Figure 4b shows cost network induced compilingPb problem, dashed line surrounding scope functional componentinduced family v4 . hard verify cost network induces treevariable-families cliques, Pb problem, size clique boundedconstant. Hence, tree-width cost-network bounded constant well.1.2.2 Causal Graphs k-Dependencecausal graphs provide important information structure planningproblems, closer look definition reveals information used definingcausal graphs actually gets hidden structure. start example, let usconsider multi-valued encoding Logistics-style problems (Helmert, 2006).209fiKatz & Domshlakproblems, variable representing location package parentscausal graph variables representing alternative transportation means (i.e.,tracks, planes, etc.), yet, individual action affecting location packagepreconditioned one parent variable. (You cannot load/unload packageinto/from one vehicle.) exemplifies fact that, even in-degreecausal graph proportional problem domains parameters, number variablesdetermine applicability action may still bounded constant.words, causal graph provides aggregate view independencerelationships problem variables, individual dependencies problemactions unaffected variables suppressed view. Targeting actualindividual dependencies actions, define (tangential causal graphstopology) classification UB problems, study connection classification computational tractability general cost-optimal plan generationUB.Definition 3 k Z , SAS+ problem instance = (V, A, I, G), sayk-dependent satisfiesmax |{v V | pre(a)[v] 6= u eff(a)[v] = u}| k,aA= u standing unspecified.words, SAS+ problem k-dependent action action set dependsk unaffected variables. Combining two classifications problems,structural fragment F UB (such as, e.g., Figure 3), k Z ,F(k) denote set k-dependent problems within F.Recall fragment P UB NP-hard even satisficing planning (Gimenez& Jonsson, 2008). main result positiveat least extreme (yet, saysLogistics example above, unrealistic) setting k = 1, satisfying k-dependencebring us island tractability P(1).Similarly treatment Pb , algorithm P(1) exploits idea compilingplanning problem tractable constraint optimization problem COP . However,planning-to-COP compilation scheme P(1) much different devisedPb . fact, difference unavoidable since construction Pb heavily reliesassumption In(CG()) = O(1), luxury P(1). Instead,identify certain properties cost-optimal plan sets P(1) problems, exploitproperties devising suitable planning-to-COP compilation schemes.begin considering P(1) problems uniform-cost actions; costplan problem proportional length plan3 . showsolvable problem cost-optimal plan makes changes variablecertain value using exactly (type of) action. devising correcttractable planning-to-COP compilation scheme step away identifyingproperty P(1), latter provides critical brick everything else relies upon.Relying property P(1), state variable v edge (v, v ) uniquely3. probably origin term sequential optimality.210fiTractable Cost-Optimal Planning(a)(b)(c)Figure 4: Cost networks induced planning-to-COP compilation schemes P. (a)Causal graph P problem , family state variable v4depicted shaded region. (b) Cost network induced compiling Pbproblem, dashed line surrounding scope functional componentinduced family v4 . (c) Cost network induced compiling P(1)problem, dashed lines surrounding scopes four functionalcomponents induced family v4 .compiled COP variables xv xvv (see Figure 4c). certain set functional components defined COP variable xv . domains COP variablesspecification functional components technically involved, thus relegated later paper. important, however, note already costnetworks COPs guaranteed induce trees cliques size 3, thustree-width bounded constant. reader get intuitioncliques size 3 coming looking example depicted Figure 4c.Unfortunately, aforementioned helpful property P(1) problems uniformcost actions hold general action-cost schemes P(1). Turns out, however, problems P(1) satisfy another property still allows devisinggeneral, correct, tractable planning-to-COP scheme P(1). Specifically, showsolvable problem P(1) cost-optimal plan makes changesvariable using three types action. algorithm resulting exploitingproperty complex costly devised P(1) problemsuniform-cost actions, yet still poly-time. Interestingly, cost networks COPstopologically identical problems uniform-cost actions, difference domains COP variables, specification functionalcomponents.read far, reader may rightfully wonder whether O(1)-dependencestrong enough property make cost-optimal planning tractable evencomplex polytree forms causal graph. Turns dividing line211fiKatz & DomshlakPbP(k)Sbbk=1k=2k=3k = (n)PNPCPNPCNPCPbP(k)Sbb(a)k=1k=2k=3k = (n)PNPCPNPCNPC(b)Figure 5: Complexity (a) cost-optimal (b) satisficing plan generation fragmentsUB. mark indicates complexity implied resultsrow. shaded regular cells correspond results obtainedwork past, respectively. Empty cells correspond open questions.Note difference understanding cost-optimal satisficingplanning fragments question complexity planning S(1).tractable intractable problems much delicate. Figure 5 summarizes currentunderstanding time complexity cost-optimal satisficing plan generationP fragments UB. First, paper show even satisficing planningdirected-path singly connected, bounded in- out-degree causal graphs hard2-dependence, cost-optimal planning structural fragment UB hardeven 1-dependent problems. Note complexity (both cost-optimalsatisficing) plan generation P(k) k = O(1) remains interesting open problem.additional question remains open complexity satisficing plan generationS(1).1.3 Remarksgoal work identifying new islands tractability cost-optimal planning, improving understanding makes planning problems hardeasy solve. lesser interest make poly-time algorithms practicallyefficient reducing (quite prohibitive) polynomial time complexity. fact,places intentionally sacrificed possible optimizations keep already involvedconstructions apprehensible possible. Therefore, likely timecomplexity planning-to-COP algorithms improved, conceptually different algorithmic ideas found appropriate problems question.addition, much efficient algorithms may work special cases generaltractable families. instance, paper illustrate possibility presentinglow poly-time algorithm UB problems tree causal graphs (that is, fragment)uniform-cost actions.course, reader may ask whether striving practical efficiency solving variousspecial fragments planning motivated. discussion beginningpaper suggests, believe answer question yes.research AI planning rightfully devoted solving general planning problems, manytools developed employed research rely tractable fragments planning.212fiTractable Cost-Optimal Planninginstance, one works devising effective heuristic estimator planning problemprojecting (or embedding in) another relaxed problem,happy know latter solved low poly-time. hand, makingtractable fragment also efficiently solvable practical terms probably worth effortface concrete customer fragment practice.2. Definitions NotationStarting Definitions 1-3 previous section, section introduceadditional definitions notation used throughout paper.contrast well-known STRIPS formalism propositional planning, assumeactions value changing, contrast value setting.is, eff(a)[v] specified pre(a)[v] also specified, caseeff(a)[v] 6= pre(a)[v]. general assumption requires exponential timetranslation, case unary-effect actions translation takes linear time. GivenUB problem = hV, A, I, Gi, Av denote actions change valuev. Note unary-effectness implies Av1 , . . . , Avn partition problemactions A. Considering applicability actions, SAS+ also helps give specialattention notation action preconditions left unaffected action.customary name preconditions prevail conditions (Backstrom & Klein,1991). example, truck package P location L preconditionsloading P L, former prevail condition actiontruck still L loading P , P longer (but inside ).Given UB problem = hV, A, I, Gi, variable subset V V , arbitrarysequenceactions , V denote order-preserving restrictionactions vV Av . restriction respect singleton set V = {v},allow writing {v} simply v . One key properties cost-optimal plansUB problems directed-path singly connected causal graphs immediately derivableLemma 1 Brafman Domshlak (2003), given Corollary 1 below.Henceforth, valid plan given problem called irreducible subplanplan , following sense4 : Removal subset (not necessarilysubsequent) actions makes resulting plan either illegal, initial stateI, end state one states specified G.Lemma 1 (Brafman Domshlak, 2003) solvable problem nstate variables, irreducible plan , state variable v , numbervalue changes v along n, is, |v | n.Corollary 1 solvable problem n state variables, cost-optimal plan, state variable v , |v | n.4. notion irreducible plans introduced Kambhampati (1995), called minimalplans, exploited admissible pruning partial plans search. adopt terminologysuggested Brafman Domshlak (2003) prevent ambiguity minimal irreducibleminimal optimal.213fiKatz & DomshlakGiven problem = hV, A, I, Gi, denote initial value I[v] variablev V bv , opposite value wv (short for, black/white). Using notationexploiting Corollary 1, (v) denote longest possible sequence valuesobtainable v along cost-optimal plan , |(v)| = n + 1, bv occupyingodd positions (v), wv occupying even positions (v). addition, (v)denote per-value time-stamping (v)(b1v wv1 b2v wv2 bj+1v , n = 2j,(v) =, j N.n = 2j 1,b1v wv1 b2v wv2 wvj ,sequences (v) (v) play important role constructions via prefixes suffixes. general, sequence seq, [seq][seq] denote set non-empty prefixes suffixes seq, respectively.context, prefix [(v)] called goal-valid either goal value G[v] unspecified,last element equals G[v]. set goal-valid prefixes (v) denoted[(v)] [(v)]. notion goal-valid prefixes also similarly specified (v).Finally, given SAS+ problem = hV, A, I, Gi, subset state variables V V ,action sequence , say applicable respect V restrictingpreconditions effects actions variables V makes applicable I.3. Cost-Optimal Planning Pbsection devoted proof tractability cost-optimal planning problemfragment Pb . begin describing planning-to-COP scheme Pb , provecorrectness complexity. Finally, present interesting subset Pb costoptimal planning tractable, also provably solvable low polynomial time.3.1 Constructionproceed details construction, make assumptionactions fully specified terms variables parents causal graph. pred(v)V denotes set immediate predecessors v causal graph CG(),assume that, action Av , pre(a)[w] specified w pred(v).general assumption requires exponential translation, casePb . Let translation original problem actions A. obtain ,every variable v V , every action Av represented set actionspreconditioned complete assignments pred(v). |pred(v)| = k, preconditionspecified terms 0 k k parents v, representedset actions, extending precondition pre(a) certain instantiationpreviously unspecified k k parents v, cost C(a ) = C(a). Noteexpansions two original actions may overlap, thus may containsyntactically identical yet differently priced actions. Without loss generality, assumeminimally-priced clone kept . key point compilingPb problems poly-time, procedure linear |A | = O(n2In()+1 ).Finally, (straightforward prove) Proposition 1 summarizes correctnessassumption respect cost-optimal planning UB.214fiTractable Cost-Optimal PlanningProposition 1 UB problem = hV, A, I, Gi, cost optimal plansequal = hV, , I, Gi, optimal plans reconstructiblelinear time optimal plans vice versa.specify compilation given Pb problem constraint optimizationproblem COP . COP variable set X contains variable xv planning variablev V , domain Dom(xv ) consists valid prefixes (v). is,X = {xv | v V }Dom(xv ) = [ (v)](1)Informally, domain variable xv contains possible sequences valuesplanning variable v may undergo along cost-optimal plan. Now, planningvariable v parents pred(v) = {w1 , . . . , wk }, set COP functions F containssingle non-negative, real-valued function v scopeQv = {xv , xw1 , . . . , xwk }(2)purpose functions connect value-changing sequences vparents pred(v). specification functions involved partcompilation.First, planning variable v pred(v) = , goal-valid (timestamped) value-changing sequences [ (v)], set| | = 10,| | = 2(3)v ( ) = C(aj kj wkv ),| | C(awv ) + | |1 C(abv ), otherwise22eff(awv )[v] = {wv }, eff(abv )[v] = {bv }, C(a) = C(a) A, , otherwise.hard verify v ( ) corresponds optimal cost performing | | 1value changes v .Now, non-root variable v pred(v) = {w1 , . . . , wk }, k 1, specifyfunction v follows. goal-valid value-changing sequence [ (v)] v,set goal-valid value-changing sequences {1 [ (w1 )], . . . , k [ (wk )]}vs parents, want set v ( , 1 , . . . , k ) optimal cost performing | | 1value changes v, given w1 , . . . , wk change values |1 | 1, . . . , |k | 1 times,respectively. follows, reduce setting value v ( , 1 , . . . , k ) solvingsingle-source shortest path problem edge-weighted digraph Ge (v) slightly enhances similarly-named graphical structure suggested Brafman Domshlak (2003).Despite substantial similarity, provide construction Ge (v) full detailssave reader patching essential differences.Given value-changing sequences 1 , . . . , k above, digraph Ge (v) createdthree steps. First, construct labeled directed graph G(v) capturing informationsequences assignments pred(v) enable n less value flips v. graphG(v) defined follows:1. G(v) consist = max [ (v)] | | nodes.215fiKatz & Domshlak2. G(v) forms 2-colored multichain, i.e., (i) nodes graph coloredblack (b) white (w), starting black; (ii) two subsequent nodescolor; (iii) 1 1, edges node node+ 1.Observe construction G(v) promises color last nodeconsistent goal value G[v] specified.3. nodes G(v) denoted precisely elements longest goal-validvalue-changing sequence [ (v)], is, biv stands ith black nodeG(v).4. Suppose operators Av that, different preconditions, changevalue v bv wv . case, i, edges bivwvi , |Av | edges wvi bi+1v . edge e labeled costcorresponding action, well prevail conditions action,k-tuple values w1 , . . . , wk . compound label e denoted l(e),prevail condition cost parts l(e) henceforth denoted prv(e)C(e), respectively.formal definition G(v) somewhat complicated, provide illustratingexample. Suppose given Pb problem 5 variables, considervariable v pred(v) = {u, w}, I[v] = bv , G[v] = wv . Leta1 : pre(a1 ) = {bv , bu , ww }, eff(a1 ) = {wv }, C(a1 ) = 1Av =: pre(a2 ) = {wv , bu , bw }, eff(a2 ) = {bv }, C(a2 ) = 22a3 : pre(a3 ) = {wv , wu , ww }, eff(a3 ) = {bv }, C(a3 ) = 3corresponding graph G(v) depicted Figure 6a. Informally, graph G(v)captures information potential executions actions Av along cost-optimalplan . path source node G(v) uniquely corresponds oneexecution. Although number alternative executions may exponentialn, graphical representation via G(v) compactthe number edges G(v)O(n |Av |). Note information number times action Avexecuted captured G(v). following two steps add essential informationgraphical structure.second step, digraph G(v) = (V, E) expanded digraph G (v) =(V , E ) substituting edge e E set edges (between nodes),labels corresponding possible assignments elements 1 , . . . , kprv(e). example, edge e E labeled kbw1 bw2 , 10k might substitutedE edges labeled {kb1w1 b1w2 , 10k, kb1w1 b2w2 , 10k, kb2w1 b1w2 , 10k, . . . }. Finally, setV = V {sv , tv }, add single edge labeled first elements 1 , . . . , kzero cost (that is, kb1w1 b1wk , 0k) sv original source node b1v , plus single edgelabeled last elements 1 , . . . , k zero cost original sink node G(v)tv . Informally, digraph G (v) viewed projection value-changingsequences 1 , . . . , k base digraph G(v). Figure 6b illustrates G (v) example1 b2 w2 .above, assuming u = b1u wu1 b2u wu2 b3u w = b1w wwwwthird step, digraph Ge (v) = (Ve , Ee ) constructed G (v) follows.216fiTractable Cost-Optimal Planningbu bw ,2bu ww ,1 1/wb1vv$:bu bw ,2bu ww ,1 2/wb2vvwu ww ,3$bu ww ,1 3/wv3: bvwu ww ,3(a)svb3u b1w ,2b3u b2w ,2b3u b2w ,2b2u b1w ,2b2u b1w ,21 ,b3u ww1b2u b2w ,21 ,b3u ww1b2u b2w ,21 ,b3u ww11 ,b2u ww1b1u b1w ,21 ,b2u ww1b1u b1w ,21 ,b2u ww11 ,b1u ww1b1u b1w ,0b3u b1w ,2/ b1v&18 wH K vb1u b2w ,2%29 bH K L v1 ,b1u ww1&b1u b2w ,228 wv%1 ,b1u ww139 bvHK38 wvH KLHK2 ,b1u ww11 w1 ,wuw 32 ,b1u ww11 w1 ,wuw 32 ,b1u ww12 ,b2u ww11 w2 ,wuw 32 ,b2u ww11 w2 ,wuw 32 ,b2u ww12 ,b3u ww12 w1 ,wuw 32 ,b3u ww12 w1 ,wuw 32 ,b3u ww12 w2 ,wuw 3&2 ,0b3u ww/ tv2 w2 ,wuw 3(b)Figure 6: Example graphs (a) G(v), (b) G (v).(i) nodes correspond edges G (v).(ii) edges (ve , ) Ee correspond pairs immediately consecutive edgese, e E that, 1 k, either prv(e)[wi ] = prv(e )[wi ], prv(e )[wi ]appears prv(e)[wi ] along .(iii) edge (ve , ) Ee weighted C(e ).Figure 7 depicts graph Ge (v) example.Assuming 3 2 , dashed edges correspond minimal-cost path length 5dummy source node b1u b1w . Note that, costs actions Av careabout, path corresponds cost-optimal sequence 5 value changes v startinginitial value bv . fact, path corresponds cost-optimalsequence, also explicitly describes underlying sequence actions Av ,well total cost actions. Finally, 0 n, minimal-cost pathslength determined running Ge (v) low-polynomial single-source shortest217fiKatz & Domshlakb3u b2w , 2b3 b2 ,b1u b1w , 2b1u b1w , 22u FwF J K 999J K 99999999999993 13 19bubw , 2bubw , 2 9999999LLL 9999LLL 9999 LLL 9999 LLL 9999 LLL 9999 LLL 99LL%9999%2 29932222 ,32__//99 bu ww , 199 b3u www ,wbu ww , 1 ww ,CuB F H J w 3uB F H J w 399rr9 B F H J K99rr9 B FH J K 88 1r 9r 988rrr 9 9rrr 9 988rrrrr9889rrr2 1/ 3 12 18/ 3 13 1bw,ww,uww , 1 wuww , 1 88u ww , 3 b1 u B H w3 bu9 wBJHBBHHKKJJ 88JJ 88ttttJJ 8ttJJ 8%/ 2 2 2 2/ 2 22 22211// b3 w2 , 0bu ww , 1 b u bw , 2 b u ww , 1 b u bw , 2 b u ww , 1bu bw , 0//7 JJ9 uC G wr r9 F9 B F H J9 B F H J 9 Fttrr r rrr r r rrr r//7 JJJJrr rrrrrrrrrr rrJJ// 7ttJ%ttrrrrrrrrr rrr// 71 , b2 b1 , / b2 w1 , b2 b1 , / b2 w1 ,// 7 7 b2u ww11212// 7u w uB H w u w uB H w// 7// 7//2 ,2 , b1 w2 ,/ w1 w2 ,b1 w2 ,b//1u ww1 _ _/ w9u1 ww9u w 3 uB F w 13 uB F w 1//rrrrrrrr//rrrrrrrrr/ rrr1 ,1 w1 ,1 w1 ,b1 w1 ,b1 w1 ,//b1u wwww1u w 3 u B w L1u w 3 u B wL1LLLLLLLLLLLLLLLL%%b1u b2w ,b1u b2w ,22Figure 7: graph Ge (v) constructed graph G (v) Fugure 6b.paths algorithm Dijkstra (Cormen, Leiserson, & Rivest, 1990). propertygraph Ge (v) provides us last building block algorithm cost-optimalplanning Pb .overall algorithm cost-optimal planning Pb based construction depicted Figure 8. Given problem Pb , algorithm compilesconstraint optimization problem COP , solves using standard algorithmconstraint optimization tree constraint networks (Dechter, 2003). specificationCOP already explained inline. believe already intuitive compilation takes time polynomial description size , next section alsoprove formally. Solving COP using algorithm tree-structured constraint networksdone time polynomial description size COP218fiTractable Cost-Optimal Planningprocedure polytree-k-indegree( = (V, A, I, G))takes problem Pbreturns optimal plan solvable, fails otherwisecreate set variables X set domains Eq. 1create set functions F = {v | v V } scopes Eq. 2v Vpred(v) =specify v according Eq. 3elseif pred(v) = {w1 , . . . , wk }construct graph G(v)k-tuple 1 [ (w1 )], . . . , k [ (wk )]construct graph G (v) graph G(v) sequences 1 , . . . , kconstruct graph Ge (v) graph G (v)goal-valid sequence [ (v)]:= minimal-cost path | | 1 edgessource node hbw1 bwk Ge (v)returnedv ( , 1 , . . . , k ) := C()elsev ( , 1 , . . . , k ) :=endifendforendforendifendforPset COP := (X , F) global objective min F (X )x :=P solve-tree-cop(COP )F (x) = return failurePextract plan x C() = F (x)returnFigure 8: Algorithm cost-optimal planning Pb .(i) tree-width cost network COP bounded constantbounds in-degree causal graph,(ii) optimal tree-decomposition COP cost network given topologicalordering causal graph.3.2 Correctness Complexityproceed proving correctness polynomial time complexityalgorithm Pb . begin proving Theorem 1 rather general propertypolytrees helps us analyzing Pb fragment question, well P(1)fragment considered later paper. note special case property219fiKatz & Domshlakalready exploited past proof Lemma 2 Brafman Domshlak(2003), but, knowledge, property never formulated generic claimTheorem 1. Throughout paper demonstrate generic claimhelpful numerous situations; proof Theorem 1 Appendix A, p. 245.Theorem 1 Let G polytree vertices V = {1, . . . , n}, pred(i) V denoteimmediate predecessors G. V , let Oi finite set objects associatedvertex i, sets O1 , . . . , pairwise disjoint. V , let >istrict partial order Oi , and, j pred(i), let >i,j strict partial orderOi Oj .If, V, j pred(i), transitively closed >i >i,j >j >i,j induce(strict) partial orders Oi Oj , transitively closed[[>i>i,j> =iV=iVjpred(i)Oi .Using Theorem 1 proceed proving correctness complexitypolytree-k-indegree algorithm.Theorem 2 Let planning problem Pb , COP = (X , F) correspondingconstraintoptimization problem, x Dom(X ) optimal solution COPPF (x) = .(I) < , plan cost reconstructed x time polynomialdescription size .(II) plan, < .Proof Sketch: proof Theorem 2Pis Appendix A. p. 247. prove (I), givenCOP solution x = {v1 , . . . , vn } F (x) = < , construct planC() = . done constructing action sequences v v V ,well constructing partial orders elements sequences variableparents. orders combined linearized1)P (using TheoremPP action sequence valid plan C() = vV C(v ) = vV v (x) =problem irreducibleF (x) = . prove (II), given solvablePPplan ,construct COP assignment x F (x ) = C(). Then, F (x )C() < , obtain claimed < .Theorem 3 Cost-optimal planning Pb tractable.Proof: Given planning problem Pb , show corresponding constraint optimization problem COP constructed solved time polynomial descriptionsize .Let n number state variables , maximal node in-degreecausal graph CG(). polytree-k-indegree, planning variable v Vpred(v) = {w1 , . . . , wk }, k-tuple 1 [ (w1 )], . . . , k [ (wk )],220fiTractable Cost-Optimal Planning(i) construct graph Ge (v),(ii) use Dijkstra algorithm compute shortest paths source node Ge (v)nodes graph.wi , (wi ) = n, thus number k-tuples v VO(nk ). k-tuple, corresponding graph Ge (v) constructedtime linear number edges = O(n2k+2 |Av |2 ) = O(n2k+2 22k+2 ) (Brafman &Domshlak, 2003). time complexity Dijkstra algorithm digraphG = (N, E)O(E log (N )), Ge (v) gives us n2k+2 22k+2 log nk+1 2k+1 . Putting thingstogether, complexity constructing COPn3+3 22+2 log n+1 2+1 .(4)Applying tree-decomposition COP along scopes functional componentsarrive equivalent, tree-structured constraint optimization problem n variablesdomains size O(n+1 ). COP defined hard binary compatibilityconstraints variables, costs associated variables values.tree-structured COP solved time O(xy 2 ) x number variablesupper bound size variables domain (Dechter, 2003). Therefore, solvingCOP done time O(n2+3 ). expression Eq. 4 dominates O(n2+3 ),time complexity extracting plan optimal solution COP (seeproof (I) Theorem 2), overall complexity algorithm polytree-k-indegreegiven Eq. 4. since Pb = O(1), conclude complexitypolytree-k-indegree polynomial description size .3.3 Towards Practically Efficient Special Casespolytree-k-indegree algorithm Pb polynomial, rather involved complexity exponential In(CG()). quite possible efficient algorithmsPb , fragments devised. Indeed, show simple algorithm Pb problems already appeared literature different context,never checked (if all) provides cost-optimal solutions.TreeDT algorithm preferential reasoning tree-structured CP-nets (Boutilier,Brafman, Domshlak, Hoos, & Poole, 2004), turns straightforward adaptation planning problems always provides cost-optimal solutions problemsuniform-cost actions. algorithm depicted Figure 9, hard verifytime complexity linear length generated plan iterativelyremoving parts problem safely ignored later steps,applying value-changing action lowest (in causal graph) variableaction exists.Theorem 4 Given problem uniform-cost actions n state variables,(I) algorithm tree-uniform-cost finds plan solvable,(II) algorithm tree-uniform-cost finds plan , plan cost-optimal,221fiKatz & Domshlakprocedure tree-uniform-cost( = (V, A, I, G))takes problem uniform-cost actionsreturns cost-optimal plan solvable, fails otherwise= hi, := I, V := VloopV := remove-solved-leafs(s, V )V = returnelsefind v V , Av A(s)u Desc(v, V ) : Au A(s) =found return failure:= hai, := (s \ pre(a)) eff(a)Figure 9: simple algorithm cost-optimal planning problems uniform-costactions. notation Desc(v, V ) stands subset V containingdescendants v CG(), A(s) stands set actions applicablestate s.(III) time complexity tree-uniform-cost (n2 ).Proof: Without loss generality, follows assume actionsunit-cost, is, plan , C() = ||.(I) Straightforward reusing proof Theorem 11 Boutilier et al. (2004).(II) Assume contrary plan provided tree-uniform-cost optimal,is, exists plan | | < ||. particular, implies existencevariable v | v | < |v |. semantics planning implies| v | |v | (v + 1)(5)v = 1 G[v] specified, 0 otherwise. Likewise, since causal graph CG()forms directed tree, exists variable v satisfying Eq. 5 that, descendants u v CG() holds:| u | |u |(6)Let Ch(v) set immediate descendants v CG(). constructiontree-uniform-cost, that:1. Ch(v) = , |v | v , contradicts Eq. 5 | v | non-negativequantity definition.2. Otherwise, Ch(v) 6= , then, construction tree-uniform-cost, existsu Ch(v) changing value |u | times requires changing value vleast |v | v times. words, action sequence applicable222fiTractable Cost-Optimal Planning|u | |u | |v | < |v | v . However, Eq. 6| u | |u |, thus | v | least |v | v . This, however, contradictsEq. 5.Hence, proved | v | |v |, contradicting assumption | | < ||.(III) Implied Theorems 12 13 Boutilier et al. (2004).requirement Theorem 4 actions cost essential.example shows general case algorithm tree-uniform-cost longercost-optimal. Consider = (V, A, I, G) V = {v, u}, = {bv , bu }, G = {bv , wu },= {a1 , a2 , a3 , a4 }eff(a1 ) = {wv }, pre(a1 ) = {bv }eff(a2 ) = {bv }, pre(a2 ) = {wv }eff(a3 ) = {wu }, pre(a3 ) = {bu , wv }eff(a4 ) = {wu }, pre(a4 ) = {bu , bv }C(a1 ) = C(a2 ) = C(a3 ) = 1C(a4 ) = 4problem, tree-uniform-cost algorithm returns = ha4 C() = 4,optimal plan = ha1 , a3 , a2 C( ) = 3.4. Cost-Optimal Planning P(1) Uniform-Cost Actionssection provide polynomial time algorithm cost-optimal planning P(1)problems uniform-cost actions. begin showing problems exhibitinteresting property, exploit property devising planning-to-COP schemeproblems, prove correctness complexity algorithm.begin providing useful notation. Given P(1) problem = (V, A, I, G),v V , w pred(v), {bv , wv }, {bw , ww }, a| denoteaction eff(a)[v] = pre(a)[w] = . Since 1-dependent, applicabilitya| prevailed value w. important keep mind a|notation; action a| may belong action set .4.1 Post-Unique Plans P(1) Problemsproceed introducing notion post-unique action sequences playskey role planning-to-COP compilation here.Definition 4 Let = (V, A, I, G) UB problem instance. action sequencecalled post-unique if, pair actions a, , eff(a) = eff(a )= . is, changes variable certain value along performed(type ) action. (possibly empty) set post-unique plansdenoted P pu () (or simply P pu , identity clear context).223fiKatz & Domshlaknotion post-unique action sequences closely related notion postunique planning problems (Backstrom & Klein, 1991; Backstrom & Nebel, 1995),considerably weaker latter. action sets post-unique planning problemsallowed contain two actions effect, Definition 4 poses similarrestriction action sequences, underlying planning problems. Still,property post-uniqueness plans strong. general, solvable problems UBmay exhibit post-unique plans all. Turns out, however, problems P(1)much case.Theorem 5 solvable P(1) problem = (V, A, I, G), P pu () 6= . Moreover, actions uniform-cost, P pu () contains least one cost-optimalplan.Proof: correctness second claim immediately implies correctnessfirst one, focus proof second claim. Given P(1) problem = (V, A, I, G)uniform-cost actions, plan = ha1 , . . . , , construct sequenceactions that:post-unique plan ,C( ) = C().construction two-step. First, v V , map subsequence v =hai1 , . . . , aik post-unique sequence actions v = hai1 , . . . , aik i. Noteindexes i1 , . . . , ik action elements v global indexes actionsalong , exactly indexes used marking elements constructedsequences v . constructed sequences v1 , . . . , vn , mergesingle actions sequence , show valid plan . two propertiesrequired hold immediately | | = ||, post-uniquenessimplied individual post-uniqueness per-variable components v .mapping subsequences v desired sequences v variables vperformed top-down, consistently topological ordering causal graph CG().top-down processing allows us assume that, constructing v , subsequencesw w pred(v) already constructed. Given that, mapping v =hai1 , . . . , aik corresponding v , distinguish following three cases.(1) subsequence v already post-unique.case, simply set v v . addition, construct following setsordering constraints. First, set binary relation >v action elementsv = hai1 , . . . , aik>v = {ai < aj | ai , aj v , < j}.(7)immediate Eq. 7 >v strict total order elements v >vsimply follows action indexing inherited v plan via v .Now, w pred(v), set binary relation >v,w elements vw>v,w =(Sv ,aj w{ai < aj | < j} {aj < ai | j < i},,pre(a)[w] specified votherwise.(8)224fiTractable Cost-Optimal Planningw pred(v), relation >v,w defined Eq. 8 strict total orderdomain ordering constraints elements v w subsetconstraints induced total-order plan (corresponding) actionsv w . reason, Eqs. 7 8, that,w pred(v), >v >v,w strict total order union elements vw .Eqs. 7-8 derive linearization >v wpred(v) >v,w definessequence actions applicable respect {v} pred(v). addition,|v | = |v | implies action sequence provides v value G[v] latterspecified.(2) subsequence v post-unique, actions v prevailedvalue single parent w pred(v).Since v post-unique, v case contain instances least threeaction types {abv |bw , abv |ww , awv |bw , awv |ww }. Thus, particular, must(a) |w | 1,(b) {bw , ww }, awv | , abv | v .Given that, set v = hai1 , . . . , aik1 j k :aij(awv | ,=abv | ,j odd.j evenpost-uniqueness v , well applicability respect v straightforward. ordering constraints >v set according Eq. 7. Likewise,w = haj1 , . . . , ajl i, set= bwSai v {ai < aj1 },>v,w =(9)= ww , l = 1ai v {ai > aj1 },{a > aj } {a < aj }, = ww , l > 1ai v12Finally, ordering constraints >v,w rest parents w pred(v) \ {w}set empty sets.relation >v identical case (1), thus strict total orderelements v . Eq. 9, easy verify >v,w also strict partialorder union elements v w . Finally, elements videntically constrained respect elements w , >v >v,wforming strict partial order union elements v w . (Forparents w pred(v), simply >v >v,w = >v .)Eqs. 7 9 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v}pred(v). addition,|v | = |v | implies action sequence provides v value G[v] latterspecified.225fiKatz & Domshlak(3) subsequence v post-unique, actions v prevailedone parent v.setting case particular implies pair vs parents {u, w}pred(v) awv | , abv | v {bu , wu }, {bw , ww }. Given that,set v(awv | , j odd,1 j k : aij =abv | , j evenand, similarly case (2), post-uniqueness v , applicability respectv straightforward.well, ordering constraints >v set according Eq. 7. Likewise,w = haj1 , . . . , ajl i, u = haj1 , . . . , aj i, set >v,w according Eq. 9 above,l>v,u according Eq. 10 below.>v,u= buSai v {ai < aj1 },= wu , l = 1=ai v {ai > aj1 },{a > } {a < }, = wu , l > 1j1j2v(10)Finally, ordering constraints >v,w rest parents w pred(v) \ {u, w}set empty sets.relation >v identical cases (1-2), relations >v,u >v,weffectively identical relation >v,w case (2). Thus, >v >v,u>v >v,w forming strict partial orders unions elements v u ,v w , respectively.Eqs. 7, 9, 10 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v}pred(v). addition,|v | = |v | implies action sequence provides v value G[v] latterspecified.last step, prove that, v V w pred(v),>w >v,w strict partial order union elements w v .- >v,w constructed according Eq. 8, >w >v,w subset constraintsinduced plan (corresponding v w ) actions v w .- Otherwise, >v,w constructed according Eq. 9 (in case, equivalent)Eq. 10, >v,w (i) addresses two elements w , (ii) orders elementsconsistently >w .cases, argued properties >w >v,w implies forms strict partial orderunion elements v w .now, specified sequences v , orders >v induced sequences,orders >v,w , proved >v >v,w >w >v,w form strict partial orders226fiTractable Cost-Optimal Planningdomains. construction allows us apply Theorem 1 (consideredsets) sequences v orders >v >v,w , proving[[>=(>v>v,w )vVwpred(v)forms strict partial order union v1 , . . . , vn . Putting thing together,implies linearization > plan , post-uniquenesssubsequences v1 , . . . , vn implies P pu (). Moreover, optimal plan, | | = || implies optimality .4.2 Construction, Correctness, Complexitymain impact Theorem 5 planning-to-COP scheme uniform-cost P(1)restrict attention post-unique plans only. Given that, constraintoptimization problem COP = (X , F) uniform-cost problem = (V, A, I, G) P(1)specified follows.variable set X contains variable xv planning variable v V ,variable xwv edge (w, v) CG(). is,X = X V X E,X V = {xv | v V }(11)X E = {xwv | (w, v) CG()}variable xv X V , domain Dom(xv ) consists goal-valid prefixesEw(v). variable xwv X , domain Dom(xv ) consists triples integers[w , b , ]] satisfying Eq. 12.Dom(xv ) = [(v)]Dom(xwv ) = {[[w , b , ]] | w , b {0, 1}, 0 n}(12)semantics Eq. 12 follows. Let {w1 , . . . , wk } arbitrary fixed orderingpred(v). xv takes value v Dom(xv ), v forced provide sequencevalues v . turn, xwv takes value [w , b , ]], corresponds numbervalue changes v, w = 1 (b = 1) forces subset parents {w1 , . . . , wi } pred(v)support (that is, prevail) changes v wv (respectively, bv ), w = 0(b = 0) relieves subset parents {w1 , . . . , wi } responsibility.variable x X , set F contains non-negative, real-valued function xscope{xv },x = xv , k = 0{x , xwk },x = xv , k > 0v v(13)Qx =w1 , k > 0w1},x=x,x{xwv1v{xwj , xwj1 , x }, x = xwj , 1 < j kvvvwjpred(v) = {w1 , . . . , wk } (and k = 0 means pred(v) = ). Proceedingspecifying functional components F COP , first, xv pred(v) = ,227fiKatz & Domshlakv [(v)], set xv (v )0,1,xv (v ) =|v | 1,,|v | = 1,(|v | = 2) (awv Av ),(|v | > 2) (awv , abv Av ),otherwise(14)turn, planning variable v V pred(v) = {w1 , . . . , wk }, k > 0,function xv set0,1,xv (v , [w , b , ]]) =|v | 1,,(|v | = 1) ([[w , b , ]] = [0, 0, 0]]),(|v | = 2) ([[w , b , ]] = [1, 0, 1]]),(|v | > 2) ([[w , b , ]] = [1, 1, |v | 1]]),otherwise(15)functions xv capture the, marginal actions Av , cost providing sequencev value changes v , given (in case Eq. 15) parents v readysupport value changes. specifying remaining functional components useindicator function specified Eq. 16.8>>0,>>>>0,>>>>><0,([[w , b , ]] , w ) = 0,>>>0,>>>>>0,>>>:,w = 0, b = 0,w = 1, b = 0, (awv |bw Av ) ((|w | > 1) (awv |ww Av )),w = 0, b = 1, (abv |bw Av ) ((|w | > 1) (abv |ww Av )),w = 1, b = 1, (awv |bw , abv |bw Av ) ((|w | > 1) (awv |ww , abv |ww Av )),w = 1, b = 1, |w | , awv |bw , abv |ww Av ,w = 1, b = 1, |w | > , awv |ww , abv |bw Av ,otherwise(16)semantics that, planning variable v V , w pred(v),([[w , b , ]] , w ) Dom(xwv ) Dom(xw ), ([[w , b , ]] , w ) = 0 value sequencew w support changes v wv (if w = 1) changes v bv (ifb = 1), value changes v . Given indicator function , v V ,functional component xwv 1 specifiedxwv 1 ([[w , b , ]] , w1 ) = ([[w , b , ]] , w1 ) ,(17)rest functions xwv 2 , . . . , xwv k specified follows. 2 k,value function xwj combination [w , b , ]] Dom(xwv ), [w , b , ]vwDom(xv i1 ), wi Dom(xwi ) = [(wi )] specified`] , w , b , , wj =xw[w , b , ]v( `[w w , b b , ]] , wj ,228= w w b botherwise(18)fiTractable Cost-Optimal Planningprocedure polytree-1-dep-uniform( = (V, A, I, G))takes problem P(1) uniform-cost actionsreturns cost-optimal plan solvable, fails otherwisecreate set variables X Eqs. 11-12create set functions F = {x | x X } scopes Eq. 13x Xspecify x according Eqs. 14-18endforPset COP := (X , F) global objective min F (X )x :=P solve-tree-cop(COP )F (x) = return failurePextract plan x C() = F (x)returnFigure 10: Algorithm cost-optimal planning P(1) problems uniform-cost actions.finalizes construction COP , construction constitutes first threesteps algorithm polytree-1-dep-uniform Figure 10(a). subsequent stepsalgorithm conceptually similar polytree-k-indegree algorithm Section 3,major difference plan reconstruction routines. hard verifyEqs. 11-13, fact causal graph P(1) forms polytree(i) variable x X , |Dom(x)| = poly(n),(ii) tree-width cost network F 3,(iii) optimal tree-decomposition COP cost network given topologicalordering causal graph consistent (arbitrary yet fixed timeCOP construction) orderings planning variables parents causalgraph.illustration, refer reader Figure 4 (p. 211) Figure 4(a) depictscausal graph problem P(1), Figure 4(c) depicts cost networkcorresponding COP . top-most variables cliques cost network correspondfunctional components COP .proceed proving correctness complexity polytree-1-depuniform algorithm.Theorem 6 Let P(1) problem uniform-costs actions, COP = (X , F)corresponding constraint optimization problem, x optimal assignment XPF (x) = .(I) < , plan cost reconstructed x time polynomialdescription size .(II) plan, < .229fiKatz & DomshlakProof Sketch: proof Theorem 6 Appendix A. p. 249. overall flowproof similar proof Theorem 2, yet details much different.main source proofs complexity that, proving (I), must distinguishseveral cases based roles taken (up to) two parents supportingvalue changes variable question.Theorem 7 Cost-optimal planning P(1) uniform cost actions tractable.Proof: Given planning problem P(1) uniform cost actions, showcorresponding constraint optimization problem COP constructed solved timepolynomial description size .Let n number state variables . polytree-1-dep-uniform, first construct constraint optimization problem COP (n2 ) variables X domain sizesbounded O(n), (n2 ) functional components F, defined threeCOP variables. construction linear size resulting COP, thusaccomplished time O(n5 ).Applying COP tree-decomposition along scopes functional components F, arrive equivalent, tree-structured constraint optimization problem(n2 ) variables domains size O(n3 ). tree-structured COP solvedtime O(xy 2 ) x number variables upper bound sizevariables domain (Dechter, 2003). Therefore, solving COP done timeO(n8 ). dominates time complexity constructing COP , timecomplexity extracting plan optimal solution COP (see proof (I)Theorem 6), overall complexity algorithm polytree-1-dep-uniform O(n8 ),therefore polynomial description size .5. Cost-Optimal Planning P(1) General Action Costsconsider cost-optimal planning P(1) problems without constraints actionscost. Theorem 5 Section 4 shows solvable P(1)problem least one post-unique plan, possible plan cost-optimal, Example 1 affirms possibility.Example 1 Let = hV, A, I, Gi P(1) problem instance variables V = {v1 , . . . , v5 },= {0, 0, 0, 0, 0}, G = {v1 = 0, v2 = 1, v3 = 1}, actions depicted Figure 11a.polytree causal graph shown Figure 11b, easy verify tableFigure 11a P(1).Ignoring non-uniformity action costs, problem post-unique costoptimal plan = ha4 a2 a1 a5 a3 a4 i. However, considering action costs lastcolumn Figure 11a, cost optimal plan, = ha4 a2 a1 a5 a3 a7 a6C( ) = 16 < 24 = C(). Note plan post-unique changesvalue v3 1 using actions a4 a6 . fact, plan problemleast two action instances change value v3 1. However, cheapaction a6 cannot applied twice requires v4 = 1, action a5sets v3 = 0 cannot applied a6 a5 requires v4 = 0, action230fiTractable Cost-Optimal Planninga1a2a3a4a5a6a7v1v21010pre(a)v3 v4v5v1v2eff(a)v3 v41100101001010101(a)v5C(a)1.01.01.010.01.01.01.0v4>>>>>>v5v3v1v2(b)Figure 11: Action set causal graph problem Example 1.effect. Therefore, post-unique plan problem invoke twiceaction a4 , thus cost least 2 C(a4 ) = 20.Fortunately, show solvable P(1) problem guaranteed costoptimal plan satisfying certain relaxation action sequence post-uniqueness stillallows us devise (more costly polytree-1-dep-uniform) planning-to-COP schemegeneral P(1) problems.5.1 Post-3/2 Plans P(1) Problemsproceed introducing notion post-3/2 property action sequencesrelaxes post-uniqueness property exploited previous section.Definition 5 Let = (V, A, I, G) UB problem instance. action sequencecalled post-3/2 if, v V , v , exist 6= {bv , wv }, parentw pred(v), , {bw , ww }, {bu , wu | u pred(v)}, {a| , a| , a| }.is, changes variable done using three types actionsprevailed two parents, u different w, different actionsprevailed w perform different value changes v.(possibly empty) set post-3/2 plans denoted P 3/2 () (or simplyP 3/2 , identity clear context).illustrate rather involved definition post-3/2 plans, consider following fouraction sequences four actions each. value changes made sequenceschanges variable v pred(v) = {x, y, z}.hawv |bx abv |wx awv |bx abv |by post-3/2 uses three types actions,prevailed one two parents x, y.hawv |bx abv |by awv |bx abv |by post-3/2 uses two types actions,prevailed one two parents x, y.231fiKatz & Domshlakhawv |bx abv |by awv |bx abv |bz post-3/2 uses three types actions,prevailed one three parents x, y, z.hawv |bx abv |wx awv |by abv |by post-3/2 uses four types actions,prevailed one two parents x, y.hard verify post-3/2 relaxation post-uniquenessif planpost-unique, post-3/2, necessarily way around. Turns that,P(1) problem , relaxed property guaranteed satisfied least onecost-optimal plan .Theorem 8 every solvable P(1) problem = (V, A, I, G), plan set P 3/2 () contains least one cost-optimal plan.Proof Sketch: proof Theorem 8 Appendix A, pp. 255-278. proofflow-wise similar proof Theorem 5, technically much involved.provide sketch proof. note, however, many building blocksproof used correctness proof planning-to-COP algorithm (notably,Theorem 9).Given P(1) problem = (V, A, I, G), cost-optimal plan , constructpost-3/2 plan , C( ) = C(). nutshell, first, v V , mapsubsequence v = ha1 , . . . , ak sequence actions v = ha1 , . . . , ak(i) satisfy post-3/2 property, (ii) C(v ) C(v ). Then, merge constructedsequences {v }vV , show valid plan . two propertiesrequired hold immediately C( ) = C(), post-3/2implied per-variable components v post-3/2.variable v V , pred(v) = , set v = v . turn, variablev V pred(v) 6= , given {w }wpred(v) , |w | = |w | + 1, let ai ithcheapest action changes variable v {bv , wv } prevailed value{w }wpred(v) (that is, applicable given sequences values {w }wpred(v) respectivelyobtained parents v). proof considers (in groups) possible settingsbbawaw2 = awv | , a1 = abv | , a2 = abv | (that is, possible combinations1 = awv | ,{, , , } wpred(v) {bw , ww }) case-by-case basis. Specifically, cases correspond(I) = {bw , ww }.(II) {bw , ww }, {bu , wu }, w 6= u.(III) = bw , = ww ; distinguish cases based w v .(1) |v | = 2y + 1, |w | = 2x, |w | |v |.(2) |v | = 2y + 1, |w | = 2x, |w | > |v |.(3) |v | = 2y, |w | = 2x, |w | < |v |.(4) |v | = 2y, |w | = 2x, |w | |v |.(5) |v | = 2y + 1, |w | = 2x + 1, |w | < |v |.(6) |v | = 2y + 1, |w | = 2x + 1, |w | |v |.232fiTractable Cost-Optimal Planning(7) |v | = 2y, |w | = 2x + 1, |w | |v |.(8) |v | = 2y, |w | = 2x + 1, |w | > |v |.(IV) = ww , = bw ; well distinguish eight cases specification-wisealmost identical (III), lead different settings v .5.2 Construction, Correctness, ComplexityGiven post-3/2 action sequence variable v V , distinguishfollowing exhaustive roles parent w pred(v) respect v along .R1 actions change value v supported value w.is, {bw , ww }, v , {abv | , awv | }.R2 actions change value v wv supported valuew, actions change value v bv supported anothervalue w.is, 6= {bw , ww }, v , {abv | , awv | }.R3 actions change value v wv supported valuew, none actions change value v bv supported w.is, {bw , ww } 6 {bw , ww }, v , {abv | , awv | }.R4 actions change value v bv supported valuew, none actions change value v wv supported w.is, {bw , ww } 6 {bw , ww }, v , {abv | , awv | }.R5 actions change value v wv supported valuew, actions change value v bv supported two valuesw.is, 6= {bw , ww }, v , {awv | , abv | , abv | }.R6 actions change value v bv supported valuew, actions change value v wv supported twovalues w.is, 6= {bw , ww }, v , {abv | , awv | , awv | }.R7 actions change value v wv supported valuew, actions change value v bv supportedanother value w others supported another parent.is, 6= {bw , ww } 6 {bw , ww }, v , {awv | , abv | , abv | }.R8 actions change value v bv supported valuew, actions change value v wv supportedanother value w others supported another parent.is, 6= {bw , ww } 6 {bw , ww }, v , {abv | , awv | , awv | }.233fiKatz & DomshlakR9 Part actions change value v bv supportedvalue w, none actions change value v wv supportedvalue w.R10 Part actions change value v wv supportedvalue w, none actions change value v bv supportedvalue w.R11 None actions supported w.is, a| , 6 {bw , ww }.given post-3/2 action sequence variable v V , parent vperforms one roles R1-R11 respect v along , roles R1-R10performed one parents v. addition, sets rolescannot simultaneously performed parents v respect vaction sequence , roles performed pairs. Specifically,one roles {R1,R2,R5,R6} played parent w pred(v), R11must played parents w pred(v) \ {w }.R3/R7/R8 played parent w1 pred(v), R4/R9/R10, respectively,must played parent w2 pred(v) \ {w1 }, R11 must playedparents w pred(v) \ {w1 , w2 }.Considering variable v parents pred(v) lens eleven roles,suppose aim assigning roles pred(v) considering one anotherarbitrary order. Given aforementioned constraints role assignment,step sequential process one following eight states,whole process described state machine depicted Fig. 12.S1 roles R1-R11 still available (to assigned parents v).S2 roles {R3,R11} still available.S3 roles {R4,R11} available.S4 roles {R7,R11} available.S5 roles {R8,R11} available.S6 roles {R9,R11} available.S7 roles {R10,R11} available.S8 role R11 available.Given language roles states, proceed specifying constraint optimization problem COP = (X , F) problem = (V, A, I, G) P(1).follows, variable v V , assume fixed (arbitrarily chosen) numbering{w1 , . . . , wk } pred(v) respect v.234fiTractable Cost-Optimal PlanningR11start@ABCGFEDp7 S2 NNNNpppR11NNNppNNNpppNNNpp@ABCGFEDpNNNp5S3TTTTpjjpjNNNR3pjjR4 ppjR11jTTTTNNjjppjpjjpTTTTR4 NNNNp R3 jjjjpp@ABCGFEDjTTTT NNNNpppjjjjjjeeeeee2 S4 YYYYYYYYYYYYeTT NpeeepejeR9YYYR7R11pppjjjjYYYYYYY TTTTNTNTNNNeeeeeeeeepjeYYYYYY TTTNNppjpjjjeeeeeeeYYYYT, ) 'R1,R2,R5,R6jeeee@ABC@ABC89:;?>=</ GFED/ GFEDS1 NTYNTYTYTYYYYYYejejejpejp2 5 7 S8eeeeeNNNTTT YYYYYYYR10eejpeeR8R11YYYYYYNNNTTTTeeeeeejjjj ppYYYYYYYNNN TTTTTeeeeeee R9 jjjjjjpppppeeeeeR7e, GFEDeNNN TTTT@ABCjjjj pppppS5TTTTNNNjjjR10jjTTTTppNNR8jR11 jjjNNNTTTTpppppNNNTT)jjjjjjpppNNN@ABCGFEDS6pppNNNppNNNppNNNpR11ppp'p@ABCGFEDS7Figure 12: State machine describing process sequential role assignment parents v (with respect v). transition labeled set roles, onegetting assigned parent v corresponding step.1. Similarly uniform-cost case, variable set X contains variable xvplanning variable v V , variable xwv edge (w, v) CG(). is,X = XV XEX V = {xv | v V }(19)X E = {xwv | (w, v) CG()}2. variable xv X V , domain Dom(xv ) consists possible valid prefixesEwi(v). variable xwv X , domain Dom(xv ) consists possiblequadruples satisfying Eq. 20.Dom(xv ) = {v [(v)]}wiDom(xv ) = [S, #w , #b , ]]fifi 0 n, 0 #w , #b2fifi {S1, . . . , S8}(20)semantics Eq. 20 follows. Let {w1 , . . . , wk } arbitrary fixed orderingpred(v). xv takes value v Dom(xv ), v forced provide sequencevalues v . turn, xwv takes value [S, #w , #b , ]], corresponds numbervalue changes v, #w #b correspond number value changes v wvbv , respectively, performed actions prevailed values235R11fiKatz & Domshlak{w1 , . . . , wi }, state-component captures roles assignedparents {w1 , . . . , wi }.3. Similarly uniform-cost case, variable x X , set F contains nonnegative, real-valued function x scope{xv },{x , xwk },v vQx =w{xv 1 , xw1 },{xwj , xwj1 , x },vvwjx = xv , k = 0x = xv , k > 01x = xwv ,k > 0wjx = xv , 1 < j k(21)pred(v) = {w1 , . . . , wk } (with k = 0 meaning pred(v) = ).Proceeding specifying functional components F COP , first,xv pred(v) = , v [v], set xv (v ) according Eq. 22.0,|v | = 1,C(a ),|v | = 2, awv Av ,wvxv (v ) =(22)||1||1vv2 C(awv ) + 2 C(abv ), |v | > 2, awv , abv Av ,,otherwiseturn, planning variable v V pred(v) = {w1 , . . . , wk }, k > 0, functionxv set Eq. 23.|v | = 1, [S, #w , #b , ]] = [hhS8, 0, 0, 0]] ,0,iixv (v , [S, #w , #b , ]]) = 0,|v | > 1, [S, #w , #b , ]] = S1, |v2|1 , |v2|1 , |v | 1 ,, otherwise(23)semantics Eq. 23 simpleif value changes v required, triviallysupport pred(v) v needed; otherwise, possible roles pred(v)considered.Now, proceed specifying generic function that, v V ,w pred(v), (R, [S, #w , #b , ]] , w ) {R1, . . . , R10} Dom(xwv ) Dom(xw ),provides marginal actions Av cost w taking role R, role,supporting #w changes v wv #b changes v bv , total changes vneeded. ease presentation, let (x1 , x2 , y1 , y2 ) denote cost action sequenceconsisting x1 actions type awv |bw , x2 actions type awv |ww , y1 actions type abv |ww ,y2 actions type abv |bw ,(x1 , x2 , y1 , y2 ) = x1 C(awv |bw ) + x2 C(awv |ww ) + y1 C(abv |ww ) + y2 C(abv |bw )(24)notation v,w probably appropriate semantics , adoptlatter shortness identity v w always clear context.Eqs. 25-34 specify (R, [S, #w , #b , ]] , w ) R {R1, . . . , R10}. semantics (R, [S, #w , #b , ]] , w ) capture minimal cumulative cost actions Av achieve #w #b (out ) value changes v support236fiTractable Cost-Optimal Planningparent w playing role R respect v. example, role R3 means supportingactions change value v wv , Eq. 27 gives us minimal costsupport terms cumulative cost supported actions Av . minimalcosts taken relevant cases proof Theorem 8, notably(Eq. 25) Case (I).(Eq. 26) Cases {(III), (IV )}.{2, 4, 6, 8}.(Eq. 27) Case (II), cost actions change value v wv .(Eq. 28) Case (II), cost actions change value v bv .(Eq. 29) Cases {(III), (IV )}.{1, 3, 5, 7}.a, minimal cost.(Eq. 30) Cases {(III), (IV )}.{1, 3, 5, 7}.b, minimal cost.(Eq. 31) Cases {(III), (IV )}.{1, 3, 5, 7}.a, cost actions prevailed one parent.(Eq. 32) Cases {(III), (IV )}.{1, 3, 5, 7}.b, cost actions prevailed one parent.(Eq. 33) residue cases {(III), (IV )}.{1, 3, 5, 7}.a. (Together Eq. 31 givesus full cost changing v required.)(Eq. 34) residue cases {(III), (IV )}.{1, 3, 5, 7}.b. (Together Eq. 31 givesus full cost changing v required.)8>(#w , 0, 0, #b ),>>)(><(#w , 0, 0, #b ),(R1, [S, #w , #b , ]] , w ) = min,>(0, #w , #b , 0)>>>:,8>(#w , 0, #b , 0),>>)(><(#w , 0, #b , 0),(R2, [S, #w , #b , ]] , w ) = min,>(0, #w , 0, #b )>>>:,|w | > 1, #w = 2 , #b = 2(25)otherwise|w | = 2, #w = 2 , #b = 2|w | > 2, #w = 2 , #b = 2(26)otherwise8#w (C(awv |bw ),>>)><#w C(awv |bw ),,(R3, [S, #w , #b , ]] , w ) = min>#w C(awv |ww )>>:,8C(abv |bw ),>>#b ()><#b C(abv |bw ),(R4, [S, #w , #b , ]] , w ) = min,>#b C(abv |ww )>>:,237|w | = 1, #w = 2 , #b = 2|w | = 1, #w = 2 , #b = 0|w | > 1, #w = 2 , #b = 0(27)otherwise|w | = 1, #w = 0, #b = 2|w | > 1, #w = 0, #b = 2otherwise(28)fiKatz & Domshlak8()>(y + 1, 0, x 1, x + 1),>>min,>>>(0, + 1, x + 1, x 1)>>>>>()>>>(y,0,x,x),>>>min,>>(0, y, x + 1, x 1)>>>>>>>>>>(y, 0, 1, 1),>>>>>>>>>>>(0, y, 1, 1),>>>>><()(R5, [S, #w , #b , ]] , w ) =(y + 1, 0, x, x),>min,>>>(0, + 1, x + 1, x 1)>>>>>>>>>(y + 1, 0, 1, 1),>>>>>>>>>>>>(0, + 1, 1, 1),>>>>>>>()>>>(y, 0, x, x),>>min,>>>(0, y, x, x)>>>>>:,(R6, [S, #w , #b , ]] , w ) =()8>(x, + 1 x, y, 0),>>min,>>(y + 1 x, x, 0, y)>>>>()>>>>(x, x, y, 0),>>min,>>>(y x + 1, x 1, 0, y)>>>>>>>>>(1, 1, y, 0),>>>>>>>>>>><(1, 1, 0, y),>()>>>(x + 1, x, y, 0),>>>min,>>(y x + 1, x, 0, y)>>>>>>>>>>(1, y, 0, y),>>>>>>()>>>>(x, x, y, 0),>>min,>>(y x, x, 0, y)>>>>>:,238= 2y + 1, |w | = 2x, 1 < x y,#w = + 1, #b == 2y, |w | = 2x, 1 < x < y,#w = # b == 2y, |w | = 2, 1 < y,#w = # b == |w | = 2y, 1 < y,#w = # b == 2y + 1, |w | = 2x + 1, 1 < x < y,#w = + 1, #b == 2y + 1, |w | = 3, 1 < y,#w = + 1, #b == |w | = 2y + 1, 1 < y,#w = + 1, #b == 2y, |w | = 2x + 1, 1 x < y,#w = # b =otherwise(29)= 2y + 1, |w | = 2x, 1 x y,#w = + 1, #b == 2y, |w | = 2x, 1 < x < y,#w = # b == 2y, |w | = 2, 1 < y,#w = # b == |w | = 2y, 1 < y,#w = # b == 2y + 1, |w | = 2x + 1, 1 x < y,#w = + 1, #b == |w | = 2y + 1, 1 y,#w = + 1, #b == 2y, |w | = 2x + 1, 1 x < y,#w = # b =otherwise(30)fiTractable Cost-Optimal Planning(R7, [S, #w , #b , ]] , w ) =()8>(y + 1, 0, x 1, 0),>>min,>>>(0, + 1, 0, x 1)>>>>>>>>>>>>>(y, 0, x, 0),>>>>>>>>>>>>>>>>>(0, y, 0, x 1),>>>>>>>>><>>>(y + 1, 0, x, 0),>>>>>>>>>>>>>>>>>>(0, + 1, 0, x 1),>>>>>>>>>>()>>>(y, 0, x, 0),>>>min,>>(0, y, 0, x)>>>>>>:,(R8, [S, #w , #b , ]] , w ) =()8>(x, 0, y, 0),>>min,>>>(0, x, 0, y)>>>>>>>>>>>>(x, 0, y, 0),>>>>>>>>>>>>>>>>>>>(0, x 1, 0, y),>>><>>>>>>(x + 1, 0, y, 0),>>>>>>>>>>>>>(0, x, 0, y),>>>>>>>()>>>(x, 0, y, 0),>>min>,>>(0, x, 0, y)>>>>>>:,239= 2y + 1, |w | = 2x, 1 < x y,#w = + 1, #b = x 1C(awv |bw ) < C(awv |ww ),= 2y, |w | = 2x, 1 x < y,#w = y, #b = xC(awv |bw ) C(awv |ww ),= 2y, |w | = 2x, 1 < x y,#w = y, #b = x 1C(awv |bw ) < C(awv |ww ),= 2y + 1, |w | = 2x + 1, 1 x < y,#w = + 1, #b = xC(awv |bw ) C(awv |ww ),= 2y + 1, |w | = 2x + 1, 1 < x y,#w = + 1, #b = x 1= 2y, |w | = 2x + 1, 1 x < y,#w = y, #b = xotherwise(31)= 2y + 1, |w | = 2x, 1 x y,#w = x, #b =C(awv |bw ) < C(awv |ww ),= 2y, |w | = 2x, 1 x < y,#w = x, #b =C(awv |bw ) C(awv |ww ),= 2y, |w | = 2x, 1 < x y,#w = x 1, #b == 2y + 1, |w | = 2x + 1, 1 x < y,#w = x + 1, #b == 2y + 1, |w | = 2x + 1, 1 x y,#w = x, #b == 2y, |w | = 2x + 1, 1 x < y,#w = x, #b =otherwise(32)fiKatz & Domshlak8>C(abv |ww ),|w | = 1, #w = 0, #b < 2>>#b ()><#b C(abv |ww ),(R9, [S, #w , #b , ]] , w ) = min,|w | > 1, #w = 0, #b < 2>#b C(abv |bw )>>>:,otherwise8>#w C(awv |bw ),|w | = 1, #w < 2 , #b = 0>>()><#w C(awv |bw ),(R10, [S, #w , #b , ]] , w ) = min,|w | > 1, #w < 2 , #b = 0>#w C(awv |ww )>>>:,otherwise(33)(34)specified function , use it, particular, specifying functionalcomponent xwv 1 Eq. 35. equation actually emulates movements statemachine v Figure 12 terminal state S8.xwv 1 ([[S, #w , #b , ]] , w1 ) =(R1, [S, #w , #b , ]] , w1 ),(R2, [S, # , # , ]] , ),ww1bmin(R5, [S, #w , #b , ]] , w1 ),(R6, [S, # , # , ]] , )ww1b(R3, [S, #w , #b , ]] , w1 ),(R4, [S, #w , #b , ]] , w1 ),(R7, [S, #w , #b , ]] , w1 ),(R8, [S, #w , #b , ]] , w1 ),(R9, [S, #w , #b , ]] , w1 ),(R10, [S, #w , #b , ]] , w1 ),0,,, = S1,= S2,= S3,= S4,= S5,(35)= S6,= S7,= S8,#w = 0,#b = 0otherwiseproceed rest functional components xwv 2 , . . . , xwv k .ww2 j k, [S, #w , #b , ]] Dom(xv j ), [S , #w , #b , ] Dom(xv j1 ),w Dom(xwj ) = [wj ], value xwj set according Eq. 36. equation alsov240fiTractable Cost-Optimal Planningemulates movements state machine v Figure 12each sub-case Eq. 36deals certain transition state machine.xwj ( [S, #w , #b , ]] , , #w , #b , , wj ) =v88(R1, [S, #w #w , #b #b , ]] , wj ),>>>>>>>< (R2, [S, # # , # # , ]] , ),>>wwjbwb>>min>>>(R5,[S,##,##,]],>wwj ),b>wb>>>:>>(R6,[S,##,##,]],wwj )b>wb>>>>>>>>(R4, [S, #w #w , #b #b , ]] , wj ),>>>>>>>>>>>>>>(R3, [S, #w #w , #b #b , ]] , wj ),>>>>>>>>>>(R9, [S, #w #w , #b #b , ]] , wj ),>>>>>>>>>>>(R10, [S, # # , # # , ]] , ),>wjwb>wb>>>>>>>>>>>(R7, [S, #w #w , #b #b , ]] , wj ),>>>>>>>>>>>(R8, [S, #w # , #b # , ]] , w ),>w<bj>>>>>>(R3, [S, #w #w , #b #b , ]] , wj ),>>>>>>>>>>>>(R4, [S, #w #w , #b #b , ]] , wj ),>>>>>>>>>>>(R7, [S, #w #w , #b #b , ]] , wj ),>>>>>>>>>>>>(R8, [S, #w #w , #b #b , ]] , wj ),>>>>>>>>>>>>>(R9, [S, #w #w , #b #b , ]] , wj ),>>>>>>>>>>(R10, [S, # # , # # , ]] , ),>wwj>bwb>>>>>>>>>>>0,>>>>>>>:,9>>>=>>>;,= S1, = S8, = ,#w #w , #b #b= S1, = S2, = ,#w #w , #b #b= S1, = S3, = ,#w #w , #b #b= S1, = S4, = ,#w #w , #b #b= S1, = S5, = ,#w #w , #b #b= S1, = S6, = ,#w #w , #b #b= S1, = S7, = ,#w #w , #b #b(36)= S2, = S8, = ,#w #w , #b #b= S3, = S8, = ,#w #w , #b #b= S4, = S8, = ,#w #w , #b #b= S5, = S8, = ,#w #w , #b #b= S6, = S8, = ,#w #w , #b #b= S7, = S8, = ,#w #w , #b #b= , = ,#w = #w , #b = #botherwisefinalizes construction COP , construction constitutes first threesteps algorithm polytree-1-dep Figure 13(a). subsequent steps algorithmconceptually similar polytree-1-dep-uniform algorithm Section 4.241fiKatz & Domshlakprocedure polytree-1-dep( = (V, A, I, G))takes problem P(1)returns cost-optimal plan solvable, fails otherwisecreate set variables X Eqs. 19-20create set functions F = {x | x X } scopes Eq. 21x Xspecify x according Eqs. 22-36endforPset COP := (X , F) global objective min F (X )x :=P solve-tree-cop(COP )F (x) = return failurePextract plan x C() = F (x)returnFigure 13: Algorithm cost-optimal planning P(1) problems.hard verify Eqs. 19-21, fact causal graph P(1) formspolytree(i) variable x X , |Dom(x)| = poly(n),(ii) tree-width cost network F 3,(iii) optimal tree-decomposition COP cost network given topologicalordering causal graph consistent (arbitrary yet fixed timeCOP construction) orderings planning variables parents causalgraph.Theorem 9 Let P(1) problem, COP = (X , F) Pcorresponding constraintoptimization problem, x optimal assignment X F (x) = .(I) < , plan cost reconstructed x time polynomialdescription size .(II) plan, < .Proof Sketch: proof Theorem 9 Appendix A, pp. 278-286; provideaPsketch proofs skeleton. prove (I), given COP solution x = {v1 , . . . , vn }F (x) = < , construct plan C() = . doneconstructing action sequences v v V , well constructing partial orderselements sequences variable parents. construction distinguishes numerous possibilities (joint) role taken parentsvariable question. constructed orders local actions sequencescombined linearized1) Paction sequence validP (using TheoremP=plan C() =C()=(x)vvF (x) = . prove (II),vVvVgiven solvable problem Pirreducible post-3/2 plan P, construct COPassignment x F (x ) = C(). Then, F (x ) C() < ,obtain claimed < .242fiTractable Cost-Optimal PlanningTheorem 10 Cost-optimal planning P(1) tractable.Proof: Given planning problem P(1), show corresponding constraintoptimization problem COP constructed solved time polynomial description size . Let n number state variables . polytree-1-dep, firstconstruct constraint optimization problem COP (n2 ) variables X domainsizes bounded either O(n) O(n3 ) (for COP variables representing state variables causal graph edges, respectively). number functional components COP(n2 ), defined one variable domain size O(n) either one twovariables domain sizes O(n3 ). construction linear size resultingCOP, thus accomplished time O(n9 ).Applying COP tree-decomposition clusters scopes functionalcomponents F, arrive equivalent, tree-structured constraint optimization problem(n2 ) variables domains size O(n7 ). tree-structured COP solvedtime O(xy 2 ) x number variables upper bound sizevariables domain (Dechter, 2003). Therefore, solving COP done time O(n16 ).dominates time complexity constructing COP , time complexityextracting plan optimal solution COP (see proof (I) Theorem 9),overall complexity algorithm polytree-1-dep O(n16 ), therefore polynomialdescription size .6. Drawing Limits k-Dependenceread far, reader may wonder whether 1-dependence strong enoughproperty make cost-planning tractable even complex polytreeforms causal graph. last technical section discuss limits powerk-dependence (and, particular, 1-dependence), present negative resultsdraw boundary tractable intractable k-dependent UB problems.Theorem 11 show cost-optimal planning already hard Sbb (1)problem class, is, class 1-dependent UB problems inducing directed-pathsingly connected causal graphs in- out-degrees boundedconstant. result stresses connection undirected cyclescausal graph complexity various planning tasks, firstdiscussed Brafman Domshlak (2003).Theorem 12 show even non-optimal planning hard Sbb (2) problemclass. results suggests 1-dependence rather special case k-dependenceterms connection computational tractability. However, given (still)empty entries Figures 5a 5b, analysis criticality 1-dependenceneeded.Theorem 11 Cost-optimal planning Sbb (1) NP-complete.Proof: membership NP implied Theorem 2 Brafman Domshlak (2003).proof NP-hardness polynomial reduction well-known Vertex Coverproblem (Garey & Johnson, 1978). problem Vertex Cover is: given undirected243fiKatz & Domshlakgraph G = (V, E), find minimal-size subset V V edge E leastone two end-nodes V . Given undirected graph G = (V, E), let planningproblem G = hVG , AG , IG , GG defined follows.VG = {v1 , . . . , v|V| , u1 , . . . , u|E| }, and, vi , uj , Dom(vi ) = Dom(uj ) = {T, F },IG = {vi = F | vi VG } {ui = F | ui VG },GG = {ui = | ui VG },Actions AG = AV AE , AV = {av1 , . . . , av|V| }pre(avi ) = {vi = F }, eff(avi ) = {vi = }, C(avi ) = 1(37)AE = {au1 , au1 , . . . , au|E| , au|E| }pre(aui ) = {ui = F, vi1 = },pre(aui ) = {ui = F, vi2 = },eff(aui ) = eff(aui ) = {ui = },(38)C(aui ) = C(aui ) = 0variables vi1 , vi2 correspond endpoints edge correspondingvariable ui .Given construction G , easy see (i) plan G providesus vertex cover V G |V | = C() vice versa, thus (ii) costoptimal plans G (and plans G ) provide us minimal vertex coversG. topology causal graph G required, 1-dependence Gimmediate Eqs. 37-38. finalizes proof NP-completeness cost-optimalplanning Sbb (1).Theorem 12 Planning Sbb (2) NP-complete.Proof: proof basically given construction proof BrafmanDomshlak (2003) Theorem 2. polynomial reduction 3-SATplanning S. Observing 3-SAT remains hard even variable participatesfive clauses formula (Garey & Johnson, 1978), reductionBrafman Domshlak 3-SAT formulas effectively planning Sbb (2),accomplishes proof claim.7. Conclusion Future WorkOne key conclusions Bylander (1994) seminal article planning complexity. . . analysis strongly suggests thing set generallyapplicable domain independent properties lead efficient planning. later worksby, e.g., Backstrom Nebel (1995), Jonsson Backstrom (1998a), BrafmanDomshlak (2003, 2006) shown conclusion pessimistic. consideringlocal restrictions actions, also global restrictions action sets, well244fiTractable Cost-Optimal Planningstructural properties problems, works managed identify numerousdomain-independent tractable fragments classical planning. said that, paletteknown tractable fragments planning remains limited, even less knowntractable optimal planning. difference theoretical complexity regularoptimal planning general case (Bylander, 1994), many classical planningdomains provably easy solve, hard solve optimally (Helmert, 2003).work studied complexity cost-optimal classical planningpropositional state variables unary-effect actions. discovered novel problem fragments optimization tractable, identified certain conditions differentiate tractable intractable problems. results based exploiting certain structural syntactic characteristics planning problems. Almosttractability results based proof technique connects certain toolsplanning tractable constraint optimization, believe techniqueinteresting due clear evidence robustnessour different algorithms exploitproof technique, much different manners.results suggest discovering new islands tractability optimal planninghopeless, strongly believe indeed case. particular, ongoingwork devoted questions left open paper (see Figure 5),well planning problems simple causal graphs multi-valued (in contrastpropositional) state variables. fact, recently reported preliminary positiveresults latter direction (Katz & Domshlak, 2007). Interestingly, recent resultspresented context potential customer tractability results,namely, context homomorphism abstractions admissible heuristics generalplanning heuristic search.Acknowledgmentsresearch supported part Israel Science Foundations grants 20081002009589, well C. Wellner Research Fund. thank Adele Howe anonymous reviewers whose attentive comments helpful suggestions greatly improvedpaper.Appendix A. ProofsTheorem 1 Let G polytree vertices V = {1, . . . , n}, pred(i) V denoteimmediate predecessors G. V , let Oi finite set objects associatedvertex i, sets O1 , . . . , pairwise disjoint. V , let >istrict partial order Oi , and, j pred(i), let >i,j strict partial orderOi Oj .If, V, j pred(i), transitively closed >i >i,j >j >i,j induce(strict) partial orders Oi Oj , transitively closed> =[iV>i245[jpred(i)>i,jfiKatz & Domshlak=iVOi .Proof: follows, oi denote arbitrary object Oi . Assumecontrary >i >i,j >j >i,j (strict) partial orders, yet > so.is, exists pair objects oi , oj hold oi > oj oj > oi .construction >, a, possibly empty, path verticesj undirected graph induced G. Since G polytree, knowundirected path= i0 i1 . . . im1 im = j(39)j unique. Thus, must cycle >: oi = o1i0 < . . . < oxi00 < o1i1 < . . . < oxi11 < . . . . . . < o1im < . . . < oximm = oj: oi = o1i0 > . . . > oyi00 > o1i1 > . . . > oyi11 > . . . . . . > o1im > . . . > oyimm = oj(40)where, 0 k m, xk 1 yk 1, step chainsdirectly implied local relation >l >l,l constructing >.Without loss generality, assume cycle > induced lengthwise minimal among cycles >. particular, implies(i) 0 k m, 1 xk , yk 2 (one object Oik required connectlocal relations >ik1 >ik+1 , two elements Oik required>ik transitively closed),(ii) pair objects , , 6= , unless = = oi = = oj ,(or otherwise would shorter cycle )(iii) pair objects , , >l (and >l,l ) implies >l (respectively,>l,l o), otherwise, again, would shorter cycle .First, let us show least one chains contains least one internalelement. Assume, contrary, contain internal elements. = j,oi >i oi (where oi = oj ) oi >i oi , contradicting assumption >ipartial order. (If >i partial order, neither >i >i,j .) Otherwise, 6= j,either pred(j) j pred(i). Assuming latter, (oi > oj ) (oi > oj ) implies(oi >i,j oj ) (oi >i,j oj ), contradicting assumption >i,j partial order.Given that, let us prove oximm 6= oyimm , contradicting assumptionchains Eq. 40 exist. case-by-case basis possible combinationsxm , ym , length-minimality cycle implies four casesconsider.[xm = 2, ym = 2 ] case, Eq. 40 implies o1im >im o2im = o2im >im o1im . transitivity>im implies o1im > o1im , contradicting assumption minimalitycycle .[xm = 1, ym = 1 ] Eq. 39 either im1 pred(im ) im pred(im1 ).xm1ym1.>im ,im1 o1im = o1im >im ,im1 oim1im1 pred(im ), Eq. 40 implies oim1ym1xm1transitivity >im ,im1 implies oim1 > oim1 , contradicting assumption246fiTractable Cost-Optimal Planningminimality cycle . Otherwise, im pred(im1 ), Eq. 40 impliesym1xm1oim1>im1 ,im o1im = o1im >im1 ,im oim1. Again, transitivity >im1 ,imym1xm1implies oim1 > oim1 , contradicting assumption minimality cycle .[xm = 2, ym = 1 ] case well, Eq. 39 implies either im1 pred(im )ym1im pred(im1 ). im1 pred(im ), Eq. 40 implies oim1>im ,im1 o1im =ym1> o1im , contrao2im >im o1im . Then, transitivity >im >im ,im1 implies oim1dicting assumption minimality cycle . Otherwise, im pred(im1 ),ym1Eq. 40 implies oim1>im1 ,im o1im = o2im >im o1im . Then, transitivityym1>im >im1 ,im implies oim1 > o1im , contradicting assumption minimalitycycle .[xm = 1, ym = 2 ] case similar previous case xm = 2, ym = 1, mutatismutandis.Theorem 2 Let planning problem Pb , COP = (X , F) correspondingconstraint optimization problem, x Dom(X ) optimal solution COPPF (x) = .(I) < , plan cost reconstructed x time polynomialdescription size .(II) plan, < .Proof:P(I) Given COP solution x = {v1 , . . . , vn } F (x) = < , constructplan C() = .First, variable v V pred(v) = , let sequence v actions Avdefined(|v | = 1v =,(41)|v |11av . . . avotherwisewhere, 1 j |v | 1,(abv , j even,ajv =awv , j odd(42)eff(abv ) = {bv }, eff(awv ) = {wv }. Eq. 3 v (v ) < , immediately(i) {awv } Av |v | 2, {abv , awv } Av |v | > 2, (ii) C(v ) = v (v ).Now, purpose gets clear below, let binary relation >v action elementsv defined transitive closure {avj1 < ajv | 1 < j |v | 1}. Clearly, >vconstitutes strict total ordering elements v .Next, non-root variable v V pred(v) = {w1 , . . . , wk }, constructgraph Ge (v) respect w1 , . . . , wk , determine Ge (v) minimal-cost path247fiKatz & Domshlak|v | 1 edges source node hb1w1 b1wk i. existence path impliedv (v , w1 , . . . , wk ) < . construction Ge (v) also know that, 1 j |v |1, j-th edge path node labeled hw1 [l1j1 ] wk [lkj1 ]i nodelabeled hw1 [l1j ] wk [lkj ]i, 1 l k, li0 = 1 lij1 lij .that, let sequence v actions Av defined Eq. 41, with, 1 j |v | 1,eff(ajv ) = {v [j + 1]}npre(ajv ) = v [j], w1 [l1j ], w2 [l2j ], . . . , wk [lkj ](43)| |1Note {a1v , . . . , av v } Av implied construction Ge (v) presenceconsidered minimal-cost path it.Now, similarly case root variables, let binary relation >v actionelements v defined transitive closure {avj1 < ajv | 1 < j |v | 1}.well, >v constitutes strict total ordering elements v . addition,parent wi v, let binary relation >v,wi union action elements v+wi defined transitive closure >v,wi >v,wi , turn definedjli 1jjawi < av | 1 j |v | 1, li > 1>v,wi =(44)lijj+j>v,wi = av < awi | 1 j |v | 1, li < |wi | .hard verify Eq. 44 that, v V w pred(v),>v,w constitutes strict partial ordering, transitively closed >v >v,w>w >v,w . Given that,definition w = ha1w . . . alw i, polytree structure causalgraph CG(), restricting preconditions effects aiw variables{v} pred(v), pre(aiw ) = {bw }, eff(aiw ) = {ww } odd, pre(aiw ) ={ww }, eff(aiw ) = {bw } even. 1 k, Eq. 43lj 1jeff(awi ) pre(av ). Eq. 44 derive linearization >vwpred(v) >v,w defines sequence actions applicable respect {v}pred(v). addition, construction graph Ge (v) implies actionsequence provides v value G[v] latter specified.polytree structure causal graph CG() Theorem 1 together implytransitively closed relation[[>=(>v>v,w )vVwpred(v)strict partial order union action elements v1 , . . . , vn .Putting thing together, implies linearization > constitutes validplan costXXC() =C(v ) =v (x),vVvV248fiTractable Cost-Optimal Planningexactly prove. also note plan extraction stepalgorithm polytree-k-indegree corresponds exactly construction along Eqs. 4144, providing us polynomial time concrete cost-optimal plan correspondingoptimal solution COP .(II) prove solvable, must < . Assumecontrary case. Let solvable planning problem, letirreducible plan . Given , let x = {v1 , . . . , vn } COP assignment|vi | = |vi | 1. Note x well-defined (that is, 1 n,vi [ (vi )])P definition (vi ), Corollary 1, irreducible. LetusshowF (x ) C(), contradicting assumption = duePF (x ) C() < .First, variable v pred(v) = , Eq. 3 immediately implies v (x ) C(v ).Next, non-root variable v V pred(v) = {w1 , . . . , wk }, consider graphGe (v) constructed respect w1 , . . . , wk . Let {a1 , . . . , a|v | } actions vnumbered order appearance along v . Let {yw1 (1), . . . , ywk (1)} denoteprevail condition a1 ywi (1) time-stamped earliest appearance1 }. Now, 2 j | |, set {y (j), . . . , (j)}along wi , is, ywi (1) {b1wi , wwvw1wkprevail condition ai ywi (j) time-stamped lowest possibletime index along wi satisfying ywi (j 1) come ywi (j) along wi . Given(i) v complete order-preserving restriction v-changing actions Av ,| |(ii) sequence time-stamped prevail conditions {{yw1 (j), . . . , ywk (j)}}j=1v constructed above,(iii) |v | = |v | 1 construction x ,Ge (v) contains pathhb1w1 b1wk hyw1 (1) ywk (1)i . . . hyw1 (|v |) ywk (|v |)icost path C(v ) < . However, constructive definition valgorithm polytree-k-indegree, v (x ) cost minimal-cost path|v | 1 edges Ge (v) originated hb1w1 b1wk i, thus v (x ) C(v ). latterargument valid planning variables v V , thusXXC(v ) = C(),(x )FvVprove.Theorem 6 Let P(1) problem uniform-costs actions, COP = (X , F)correspondingconstraint optimization problem, x optimal assignment XP(x)=.F249fiKatz & Domshlak(I) < , plan cost reconstructed x time polynomialdescription size .(II) plan, < .Proof:P(I) Given COP solution x F (x) = < , construct planC() = . construct plan1. Traversing planning variables topological ordering causal graph CG(),associating variable v sequence v Av .2. Merging constructed sequences v1 , . . . , vn desired plan .variable xv X , let v denote value provided x xv . First,variable v V pred(v) = , let sequence v actions Av defined(|v | = 1v =,(45)|v |11otherwiseav . . . avwhere, 1 j |v | 1,(abv , j even,ajv =awv , j odd(46)eff(abv ) = {bv }, eff(awv ) = {wv }. Eq. 14 v (v ) < ,immediately (i) {awv } Av |v | 2, {abv , awv } Av |v | > 2, (ii)C(v ) = v (v ). Let binary relation >v action elements v definedtransitive closure {avj1 < ajv | 1 < j |v | 1},>v = {ajv < ajv | 1 j < j |v | 1}(47)Clearly, >v constitutes strict total ordering elements v , making v applicable sequence actions provides v value G[v] latter specified.Next, variable v V pred(v) 6= , let pred(v) = {w1 , . . . , wk } numberedaccording ordering used constructing COP . Likewise, wi pred(v),let [w (i), b (i), (i)]] value provided x xwv . Given that, let pair indexes0 hwi, hbi k defined0, w (k) = 0,(48)hwi = 1, w (1) = 1,j, w (j 1) < w (j), 2 j k0, b (k) = 0,hbi = 1, b (1) = 1,j, b (j 1) < b (j), 2 j k250(49)fiTractable Cost-Optimal Planningwords, hwi captures smallest 1 j k w (j) = 1, 0,j all; semantics hbi similar, mutatis mutandis.Informally, next-coming construction action sequence v statevariable v, hwi hbi indicate parents prevailing value changes v wvbv , respectively, along v . Note Eqs. 48-49 well-defined because, 2 j k,Eq. 18 impliesw (j 1) w (j) b (j 1) b (j) (j 1) = (j).Given notation, action sequence v partial orders >v,w1 , . . . , >v,wkconstructed follows.[ hwi = 0, hbi = 0 ] case, constructed plan perform value changesv, thus v set empty action sequence, and, consequently, >v>v,w set empty sets.[ hwi > 0, hbi = 0 ] case, constructed plan perform exactly one valuechange v (from bv wv ), thus v set contain exactly one action a1veff(a) = {wv },({bv , bwhwi }, awv |bw Avhwipre(a1v ) =(50){bv , wwhwi }, otherwiseNote a1v well-defined, < Eq. 16 together imply {awv |bw , awv |bw }hwihwiAv 6= (see case (2) Eq. 16). outcomes Eq. 50, set >v = .a1v = awv |bw , sethwi>v,whwi = {a1v < a1whwi | a1whwi whwi }(51)Otherwise, a1v = awv |ww , case (2) Eq. 16, awv |bw 6 Av , < ,hwihwi|whwi | > 1, thus |whwi | 1. Given that, set>v,whwi = {a1whwi < a1v } {a1v < a2whwi | a2whwi whwi }(52)cases, easy verify >v >v,whwi >whwi constitutes strict totalorder action elements v whwi . (In particular, trivially implies>v >v,w >v,w >w strict partial orderings domains.)Eqs. 47, 51, 52 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v). addition, Eq. 12 implies action sequence provides v value G[v] latterspecified.[ hwi > 0, hbi > 0, hwi = hbi ] case, constructed plan performone value change v, value changes performed (apair types of) actions prevailed value whwi . < ,([[w (hwi), b (hwi), ]] , whwi ) = 0. specification case question (that is,hwi = hbi > 0) thus implies one conditions cases (4-6) Eq. 16hold. Given that, distinguish following four settings.251fiKatz & Domshlak(1) {awv |bw , abv |bw } Av , v specified according Eq. 45,hwihwiaction elements specifiedaw |b, oddv whwiaiv =.(53)abv |bw , evenhwirelation >v set according Eq. 47, >v,whwi set>v,whwi = {aiv < a1whwi | aiv v , a1whwi whwi }(54)Finally, w pred(v) \ {whwi }, set >v,w = .(2) Otherwise, {awv |ww , abv |ww } Av |whwi | > 1, |whwi | 1.hwihwiGiven that, set v according Eq. 45, action elementssetaw |w, oddv whwiaiv =.(55)abv |ww , evenhwirelation >v set according Eq. 47, >v,whwi set>v,whwi = {a1whwi < aiv | aiv v } {aiv < a2whwi | aiv v , a2whwi whwi }(56)Finally, w pred(v) \ {whwi }, set >v,w = .(3) Otherwise, {awv |bw , abv |ww } Av , |whwi | |v |1, v specifiedhwihwiaccording Eq. 45, action elements specifiedaw |b, oddv whwiav =.(57)abv |ww , evenhwirelation >v set according Eq. 47, >v,whwi set[{aiv < ajwhwi | j} {ajwhwi < aiv | > j}>v,whwi =(58)aiv v ,ajwhwi whwiw pred(v) \ {whwi }, set >v,w = .(4) Otherwise, {awv |ww1 , abv |bw1 } Av , |whwi | |v |, v specifiedaccording Eq. 45, action elements specifiedaw |w, oddv whwiaiv =.(59)abv |bw , evenhwirelation >v set according Eq. 47, >v,whwi set[{aiv < ajwhwi | < j} {ajwhwi < aiv | j}>v,whwi =aiv v ,ajwhwi whwiw pred(v) \ {whwi }, set >v,w = .252(60)fiTractable Cost-Optimal Planningfour cases above, >v >v,whwi >whwi constitutes strict total orderelements v whwi .Eqs. 47, 54, 56, 58, 60 derive linearization >vwpred(v) >v,w defines sequence actions applicable respect {v}pred(v). addition, Eq. 12 implies action sequence provides v valueG[v] latter specified.[ hwi > 0, hbi > 0, hwi =6 hbi ] case, constructed plan performone value change v, changes v wv bv performed (a pairtypes of) actions prevailed value whwi whbi , respectively. < ,([[w (hwi), b (hwi), ]] , whwi ) = ([[w (hbi), b (hbi), ]] , whbi ) = 0,due respective satisfaction conditions cases (2) (3) Eq. 16.Given that, distinguish following four settings5 .(1) {awv |bw , abv |bw } Av , v specified according Eq. 45,hwihbiaction elements specifiedaw |b, oddv whwiaiv =.(61)abv |bw , evenhbirelation >v action elements v set according Eq. 47,relation >v,whwi action elements v whwi set>v,whwi = {aiv < a1whwi | odd, aiv v , a1whwi whwi }(62)relation >v,whbi action elements v whbi set>v,whbi = {aiv < a1whbi | even, aiv v , a1whbi whbi }(63)w pred(v) \ {whwi , whbi }, set >v,w = .(2) Otherwise, {awv |ww , abv |bw } Av |whwi | > 1, |whwi | 1.hwihbiGiven that, set v according Eq. 45, action elementssetaw |w, oddv whwiaiv =.(64)abv |bw , evenhbirelation >v set according Eq. 47, >v,whwi set[>v,whwi ={a1whwi < aiv } {aiv < a2whwi | a2whwi whwi }(65)aiv v , odd>v,whbi set>v,whbi = {aiv < a1whbi | even, aiv v , a1whbi whbi }(66)w pred(v) \ {whwi , whbi }, set >v,w = .5. details slightly different, four settings case conceptually similarpreviously considered case hwi > 0, hbi > 0, hwi = hbi.253fiKatz & Domshlak(3) Otherwise, {abv |ww , awv |bw } Av , |whbi | > 1, |whbi | 1.hbihwiGiven that, v specified according Eq. 45, action elementsspecifiedaw |b, oddv whwiaiv =.(67)abv |ww , evenhbirelation >v set according Eq. 47, >v,whwi set>v,whwi = {aiv < a1whwi | odd, aiv v , a1whwi whwi }(68)>v,whbi set[>v,whbi =aiv v ,{a1whbi < aiv } {aiv < a2whbi | a2whbi whbi }(69)evenw pred(v) \ {whwi }, set >v,w = .(4) Otherwise, {awv |ww , abv |ww } Av , |whwi | > 1, |whbi | > 1,hwihbi|whwi | 1 |whbi | 1. Given that, set v accordingEq. 45, action elements specifiedaw |w, oddv whwiaiv =.(70)abv |ww , evenhbirelation >v set according Eq. 47, >v,whwi set>v,whwi =[{a1whwi < aiv } {aiv < a2whwi | a2whwi whwi }(71)[{a1whbi < aiv } {aiv < a2whbi | a2whbi whbi }(72)aiv v ,odd>v,whbi set>v,whbi =aiv v , evenw pred(v) \ {whwi }, set >v,w = .four cases above, >v >v,whwi >whwi >v >v,whbi >whbiconstitute strict total orders respective domains.Eqs.47, 62, 63, 65, 66, 68, 69, 71, 72 derive linearization>v wpred(v) >v,w defines sequence actions applicable respect{v} pred(v). addition, Eq. 12 implies action sequence provides vvalue G[v] latter specified.now, variable v V , specified action sequence vorder >v elements v . w pred(v), specified order >v,w ,proved >v >v,w >w >v,w form strict partial orders domains,254fiTractable Cost-Optimal Planninglinearization >v wpred(v) >v,w defines sequence actions applicablerespect {v} pred(v) provides v value G[v] latter specified.construction allows us apply Theorem 1 (considered sets) sequences vorders >v >v,w , proving[[>=(>v>v,w )vVwpred(v)forms strict partial order union v1 , . . . , vn .also note plan extraction step algorithm polytree-1-dep-uniformcorresponds exactly construction along Eqs. 45-72, providing us polynomialtime concrete cost-optimal plan corresponding optimal solution COP .(II) prove solvable, must < . Assume contrarycase. Let solvable P(1) problem, let (using Theorem 5)irreducible, post-unique plan . Given , let COP assignment x definedfollows.1. COP variable xv , assignment x provides value v [(v)]|v | = |v | + 1.w w2. COP variable xwv , assignment x provides value wv , bv , |v | 1 ,wwv = 1 action v changes value v wv (consideringpre-fixed ordering vs parents) preconditioned valuewj , j i, wwv = 0, otherwise. bwv defined similarly wwv , mutatis mutandis.Eq. 14-18 directly that, v V , xv (x ) = |v |,w pred(v), xwv (x ) = 0. Therefore,XXC(v ) = C(),(x ) =FvVprove.Theorem 8 every solvable P(1) problem = (V, A, I, G), plan set P 3/2 () contains least one cost-optimal plan.Proof: Given P(1) problem = (V, A, I, G), cost-optimal plan , constructsequence actions that:post-3/2 plan ,C( ) = C().nutshell, first, v V , map subsequence v = ha1 , . . . , aksequence actions v = ha1 , . . . , ak (i) satisfy post-3/2 property, (ii)C(v ) C(v ). Then, merge constructed sequences {v }vV , showvalid plan . two properties required hold255fiKatz & Domshlakimmediately C( ) = C(), post-3/2 implied per-variablecomponents v post-3/2.variable v V , pred(v) = , set v = v>v = {ai < aj | ai , aj v , < j}.(73)immediate Eq. 73 >v strict total order elements v .turn, variable v V pred(v) 6= , given {w }wpred(v) ,|w | = |w | + 1, let ai ith cheapest action changes variable v {bv , wv }prevailed value {w }wpred(v) (that is, applicable given sequencesvalues {w }wpred(v) respectively obtained parents v). Let us focus6bbwaw1 = awv | , a2 = awv | , a1 = abv | , a2 = abv | (that is, {, , , }wpred(v) {bw , ww }).(I) = {bw , ww }, setai =(= 2j 1, j Notherwiseawv |abv |(74)addition, construct following sets ordering constraints. First, setbinary relation >v action elements v = ha1 , . . . , ak>v = {ai < aj | ai , aj v , < j}.(75)immediate Eq. 75 >v strict total order elements v .Likewise, w = haj1 , . . . , ajl i, set= bwSai v {ai < aj1 },(76)>v,w == ww , l = 1ai v {ai > aj1 },{a > aj } {a < aj }, = ww , l > 1ai v12Finally, ordering constraints >v,w rest parents w pred(v) \ {w}set empty sets.w pred(v), easy verify relation >v,w defined Eq. 76strict total order domain. Also, Eqs. 75 76, that,w pred(v), >v >v,w strict total order union elements vw .Eqs. 75-76 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 74 implies action sequence providesv value G[v] latter specified.6. possible actions exist, case case analysis proof transparently takes possibility account. Specifically, aw1 exist, variable v simplyunchangeable, meaning v = . Next, ab1 exist, v changed (from bw), covered subcase (I). aw2 exist, ab2 exist, sub-cases(a) cases {(III), (IV )}.{1, 3, 5, 7} possible. Similarly, ab2 exist, aw2 exist,sub-cases (b) cases {(III), (IV )}.{1, 3, 5, 7} possible. Finally, aw2 ab2exist, cases {(III), (IV )}.{1, 3, 5, 7} possible all.256fiTractable Cost-Optimal Planning(II) {bw , ww } {bu , wu }, w 6= u, set(awv | = 2j 1, j Nai =abv | otherwise(77)case well, ordering constraints >v set according Eq. 75. Likewise,w = ha1 , . . . , al i, u = ha1 , . . . , al i, set >v,w according Eq. 76 above,>v,u according Eq. 78 below.= buSai v {ai < a1 },>v,u =(78)= wu , l = 1ai v {ai > a1 },{a > } {a < }, = wu , l > 1ai v12Finally, ordering constraints >v,w rest parents w pred(v) \ {u, w}set empty sets.relations >v case identical previous case, relations>v,u >v,w effectively identical relation >v,w previous case. Thus,>v >v,u >v >v,w forming strict partial orders unionselements v u , v w , respectively.Eqs. 75, 76, 78 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 77 implies action sequence providesv value G[v] latter specified.(III) = bw , = ww , distinguish cases based w v .(1) |v | = 2y + 1, |w | = 2x, |w | |v |, construct two post-3/2candidates v , assign v cheapest among two, provingcost lower C(v ).(a) changes v wv done using action aw1 , largestpossible number changes bv done using action ab1 , remainingchanges bv done using action ab2 . candidate v , setba1 = 2j, j N, j < x(79)ai = ab2 = 2j, j N, x jwa1 otherwisecost casebb(y + 1) C(aw1 ) + (x 1) C(a1 ) + (y x + 1) C(a2 )(80)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, u = ha1 , . . . , al i, set >v,w according Eq. 81,>v,u according Eq. 82.>v,w =[{ai < aj | j < 2x 1} {ai < a2x1 } {aj < ai | j < i, j < 2x 1}v ,aj w(81)257fiKatz & Domshlaku pred(v) \ {w} set,{a < a1 },Sai v{a > },1>v,u = Sai vv {ai > a1 } {ai < a2 },,= bu= wu , l = 1= wu , l > 1.(82)otherwisehard verify relation >v,w defined Eq. 81 stricttotal order domain. Suppose contrary i, j,aj < ai ai < aj . first inequality either j < 2x 1j = 2x 1, second j < i, j < 2x 1.relations >v >v,u effectively identical case (II). Thus,>v >v,u >v >v,w forming strict partial ordersunions elements v u , v w , respectively.Eqs. 75, 81, 82 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 79 implies action sequenceprovides v value G[v] latter specified.(b) changes v bv done using action ab1 , largestpossible number changes wv done using action aw1 , remaining.candidatev , setchanges wv done using action aw2wa1 = 2j 1, j N, j x(83)ai = aw= 2j 1, j N, x < j + 12ba1 otherwisecost casewbx C(aw1 ) + (y + 1 x) C(a2 ) + C(a1 )(84)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, u = ha1 , . . . , al i, set >v,w according Eq. 85,>v,u according Eq. 86.[{ai < aj | j} {aj < ai | j < i}(85)>v,w =ai v ,aj wu pred(v) \ {w} set,ai v {ai < a1 },{a > a1 },>v,u = Sai vai v {ai > a1 } {ai < a2 },,= bu= wu , l = 1= wu , l > 1.(86)otherwiserelation >v,w defined Eq. 85 strict total order domain.relations >v >v,u effectively identical case (II). Thus,258fiTractable Cost-Optimal Planning>v >v,u >v >v,w forming strict partial ordersunions elements v u , v w , respectively.Eqs. 75, 85, 86 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 83 implies action sequenceprovides v value G[v] latter specified.Now, cost-optimal plan , v cannot contain + x actionsbtypes aw1 a1 totally. Suppose contrary v containbleast + x + 1 actions types aw1 a1 . contains xactions types. Let bw ww . . . bw sequence 2y + 1 values wsupport cost-optimal plan v given w change value numbertimes. action type decrease needed lengthsequence 2. Therefore x actions typedecrease length 2y 2x, left sequencelength 2y + 1 (2y 2x) = 2x + 1. Therefore w cannot supportb+ x actions types aw1 a1 . Now, suppose given cost-optimalbplan v v actions type aw1 actions type a1 .+ y+x(87)wbbC(v ) C(aw1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 )(88)(80) (84),wC(ab2 ) C(ab1 ) C(aw2 ) C(a1 )(89)suppose contrary plan first case cost-optimal.Eq. 88wbbC(aw1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <bb(y + 1) C(aw1 ) + (x 1) C(a1 ) + (y x + 1) C(a2 )wbb(y + 1 ) (C(aw2 ) C(a1 )) < ( x + 1) (C(a2 ) C(a1 ))(90)Eq. 87 + 1 x + 1, together Eq. 89 contradictingEq. 90.(84) (80),wbbC(aw2 ) C(a1 ) C(a2 ) C(a1 )(91)suppose contrary plan second case cost-optimal.Eq. 88wbbC(aw1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <wbx C(aw1 ) + (y x + 1) C(a2 ) + C(a1 )259fiKatz & Domshlakw(y ) (C(ab2 ) C(ab1 )) < ( x) (C(aw2 ) C(a1 ))(92)Eq. 87 x, together Eq. 91 contradicting Eq. 92.(2) |v | = 2y + 1, |w | = 2x, |w | > |v |, actions v set(awv |bw = 2j 1, j Nai =abv |ww otherwise(93)case well, ordering constraints >v set according Eq. 75.Likewise, w = ha1 , . . . , a2x1 i, set >v,w according Eq. 85 above. Finally,ordering constraints >v,w rest parents w pred(v) \ {u, w}set empty sets.relations >v >v,w identical previous case. Thus,>v >v,w forming strict partial order union elements vw .Eqs. 75, 85 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 93 implies action sequenceprovides v value G[v] latter specified.(3) |v | = 2y, |w | = 2x, |w | < |v |, construct two post-3/2 candidatesv , assign v cheapest among two, proving costlower C(v ).(a) changes v wv done using action aw1 , largestpossible number changes bv done using action ab1 , remainingchanges bv done using action ab2 . candidate v , setba2 = 2j, j N, j x(94)ai = ab1 = 2j, j N, x < jwa1 otherwisecost casebbC(aw1 ) + x C(a1 ) + (y x) C(a2 )(95)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 96.[>v,w ={ai < aj | 2y 2x + j} {aj < ai | > 2y 2x + j}ai v ,aj w(96)u pred(v) \ {w} set >v,u according Eq. 82. easyverify relation >v,w defined Eq. 96 strict total orderdomain. relations >v >v,u effectively identical previous260fiTractable Cost-Optimal Planningcase. Thus, >v >v,u >v >v,w forming strict partial ordersunions elements v u , v w , respectively.Eqs. 75, 82, 96 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 94 implies action sequenceprovides v value G[v] latter specified.(b) changes v bv done using action ab1 , largestpossible number changes wv done using action aw1 , remainingwchanges wv done using action a2 . candidate v , setwa1ai = aw2ba1= 2j 1, j N, j x= 2j 1, j N, x < jotherwise(97)cost casewbx C(aw1 ) + (y x) C(a2 ) + C(a1 )(98)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 85 above.u pred(v) \ {w} set >v,u according Eq. 86. relations >v ,>v,w >v,u effectively identical previous case. Thus, again,>v >v,u >v >v,w forming strict partial orders unionselements v u , v w , respectively.Eqs. 75, 85, 86 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 97 implies action sequenceprovides v value G[v] latter specified.Now, cost-optimal plan , v cannot contain + x actionsbtypes aw1 a1 totally. Suppose contrary v contain leastb+ x + 1 actions types aw1 a1 . contains x 1actions types. Let bw ww . . . ww sequence 2y values wsupport cost-optimal plan v given w change value numbertimes. action type decrease needed lengthsequence 2. Therefore x 1 actions typedecrease length 2y 2x 2, left sequencelength 2y (2y 2x 2) = 2x + 2. Therefore w cannot supportb+ x actions types aw1 a1 . Now, suppose given cost-optimalbplan v v actions type aw1 actions type a1 .+ y+x(99)wbbC(v ) C(aw1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 )261(100)fiKatz & Domshlak(95) (98),wC(ab2 ) C(ab1 ) C(aw2 ) C(a1 )(101)suppose contrary plan first case cost-optimal.Eq. 100wbbC(aw1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <bbC(aw1 ) + x C(a1 ) + (y x) C(a2 )wbb(y ) (C(aw2 ) C(a1 )) < ( x) (C(a2 ) C(a1 ))(102)Eq. 99 x, together Eq. 101 contradicting Eq. 102.(98) (95),wbbC(aw2 ) C(a1 ) C(a2 ) C(a1 )(103)suppose contrary plan second case cost-optimal.Eq. 100wbbC(aw1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <wbx C(aw1 ) + (y x) C(a2 ) + C(a1 )w(y ) (C(ab2 ) C(ab1 )) < ( x) (C(aw2 ) C(a1 ))(104)Eq. 99 x, together Eq. 103 contradicting Eq. 104.(4) |v | = 2y, |w | = 2x, |w | |v |, actions v set(awv |bw = 2j 1, j Nai =abv |ww otherwise(105)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 85 above. Finally, orderingconstraints >v,w rest parents w pred(v) \ {u, w} set emptysets. relations >v >v,w identical previous case. Thus,>v >v,w forming strict partial order union elements vw .Eqs. 75, 85 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 105 implies action sequenceprovides v value G[v] latter specified.(5) |v | = 2y + 1, |w | = 2x + 1, |w | < |v |, construct two post-3/2candidates v , assign v cheapest among two, provingcost lower C(v ).262fiTractable Cost-Optimal Planning(a) changes v wv done using action aw1 , largestpossible number changes bv done using action ab1 , remainingchanges bv done using action ab2 . candidate v , setba2 = 2j, j N, j x(106)ai = ab1 = 2j, j N, x < jwa1 otherwisecost casebb(y + 1) C(aw1 ) + x C(a1 ) + (y x) C(a2 )(107)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x i, set >v,w according Eq. 96 above. u pred(v) \{w} set >v,u according Eq. 82 above. relations >v , >v,w >v,ueffectively identical previous case. Thus, >v >v,u>v >v,w forming strict partial orders unions elements vu , v w , respectively.Eqs. 75, 82, 96 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 106 implies action sequence provides v value G[v] latter specified.(b) changes v bv done using action ab1 , largestpossible number changes wv done using action aw1 , remaining.candidatev , setchanges wv done using action aw2wa1 = 2j 1, j N, j x j = + 1(108)ai = aw= 2j 1, j N, x < j2ba1 otherwisecost casewb(x + 1) C(aw1 ) + (y x) C(a2 ) + C(a1 )(109)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x i, set >v,w according Eq. 110.>v,w =[ai v ,aj w{ai < aj | j < 2x} {ai < a2x | 2y}{aj < ai | j < i, j < 2x} {a2x < a2y+1 }(110)u pred(v) \ {w} set >v,u according Eq. 86 above.easy verify relation >v,w defined Eq. 110 strict totalorder domain. relations >v >v,u effectively identicalprevious case. Thus, >v >v,u >v >v,w forming strictpartial orders unions elements v u , v w ,respectively.263fiKatz & DomshlakEqs. 75, 86, 110 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 108 implies action sequence provides v value G[v] latter specified.Now, cost-optimal plan , v cannot contain + x+ 1 actionsbtypes aw1 a1 totally. Suppose contrary v contain leastb+ x + 2 actions types aw1 a1 . contains x 1actions types. Let bw ww . . . bw sequence 2y + 1 values wsupport cost-optimal plan v given w change value numbertimes. action type decrease needed lengthsequence 2. Therefore x 1 actions typedecrease length 2y 2x 2, left sequencelength 2y + 1 (2y 2x 2) = 2x + 3. Therefore w cannot supportb+ x + 1 actions types aw1 a1 . Now, suppose givencost-optimal plan v v actions type aw1 actions typeba1 .+ y+x+1(111)wbbC(v ) C(aw1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 )(112)(107) (109),wC(ab2 ) C(ab1 ) C(aw2 ) C(a1 )(113)suppose contrary plan first case cost-optimal.Eq. 112wbbC(aw1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <bb(y + 1) C(aw1 ) + x C(a1 ) + (y x) C(a2 )wbb(y + 1 ) (C(aw2 ) C(a1 )) < ( x) (C(a2 ) C(a1 ))(114)Eq. 111 + 1 x, together Eq. 113 contradictingEq. 114.(109) (107),wbbC(aw2 ) C(a1 ) C(a2 ) C(a1 )(115)suppose contrary plan second case cost-optimal.Eq. 112wbbC(aw1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <wb(x + 1 C(aw1 ) + (y x) C(a2 ) + C(a1 )264fiTractable Cost-Optimal Planningw(y ) (C(ab2 ) C(ab1 )) < ( x 1) (C(aw2 ) C(a1 ))(116)Eq. 111 x 1, together Eq. 115 contradictingEq. 116.(6) |v | = 2y + 1, |w | = 2x + 1, |w | |v |, actions v set(awv |bw = 2j 1, j Nai =abv |ww otherwise(117)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x i, set >v,w according Eq. 85 above. Finally, ordering constraints >v,w rest parents w pred(v) \ {u, w} set emptysets. relations >v >v,w identical previous case. Thus,>v >v,w forming strict partial order union elements vw .Eqs. 75, 85 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 117 implies action sequenceprovides v value G[v] latter specified.(7) |v | = 2y, |w | = 2x + 1, |w | |v |, construct two post-3/2candidates v , assign v cheapest among two, provingcost lower C(v ).(a) changes v wv done using action aw1 , largestpossible number changes bv done using action ab1 , remainingchanges bv done using action ab2 . candidate v , setba2 = 2j, j N, j x(118)ai = ab1 = 2j, j N, x < jwa1 otherwisecost casebbC(aw1 ) + x C(a1 ) + (y x) C(a2 )(119)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x i, set >v,w according Eq. 96 above. u pred(v) \{w} set >v,u according Eq. 82 above. relations >v , >v,w >v,ueffectively identical previous case. Thus, >v >v,u>v >v,w forming strict partial orders unions elements vu , v w , respectively.Eqs. 75, 82, 96 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 118 implies action sequence provides v value G[v] latter specified.265fiKatz & Domshlak(b) changes v bv done using action ab1 , largestpossible number changes wv done using action aw1 , remainingwchanges wv done using action a2 . candidate v , setwa1ai = aw2ba1= 2j 1, j N, j x= 2j 1, j N, x < jotherwise(120)cost casebwx C(aw1 ) + (y x) C(a2 ) + C(a1 )(121)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x i, set >v,w according Eq. 122.>v,w =[{ai < aj | j < 2x} {aj < ai | j < i, j < 2x} {ai < a2x }ai v ,aj w(122)u pred(v) \ {w} set >v,u according Eq. 86 above.easy verify relation >v,w defined Eq. 122 strict totalorder domain. relations >v >v,u effectively identicalprevious case. Thus, >v >v,u >v >v,w forming strictpartial orders unions elements v u , v w ,respectively.Eqs. 75, 86, 122 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 120 implies action sequence provides v value G[v] latter specified.Now, cost-optimal plan , v cannot contain + x actionsbtypes aw1 a1 totally. Suppose contrary v contain leastb+ x + 1 actions types aw1 a1 . contains x 1actions types. Let bw ww . . . ww sequence 2y values wsupport cost-optimal plan v given w change value numbertimes. action type decrease needed lengthsequence 2. Therefore x 1 actions typedecrease length 2y 2x 2, left sequencelength 2y (2y 2x 2) = 2x + 2. Therefore w cannot supportb+ x actions types aw1 a1 . Now, suppose given cost-optimalbplan v v actions type aw1 actions type a1 .+ y+x(123)wbbC(v ) C(aw1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 )266(124)fiTractable Cost-Optimal Planning(119) (121),wC(ab2 ) C(ab1 ) C(aw2 ) C(a1 )(125)suppose contrary plan first case cost-optimal.Eq. 124bwbC(aw1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <bbC(aw1 ) + x C(a1 ) + (y x) C(a2 )wbb(y ) (C(aw2 ) C(a1 )) < ( x) (C(a2 ) C(a1 ))(126)Eq. 123 x, together Eq. 125 contradicting Eq. 126.(121) (119),wbbC(aw2 ) C(a1 ) C(a2 ) C(a1 )(127)suppose contrary plan second case cost-optimal.Eq. 124wbbC(aw1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <wbx C(aw1 ) + (y x) C(a2 ) + C(a1 )w(y ) (C(ab2 ) C(ab1 )) < ( x) (C(aw2 ) C(a1 ))(128)Eq. 123 x, together Eq. 127 contradicting Eq. 128.(8) |v | = 2y, |w | = 2x + 1, |w | > |v |, actions v set(awv |bw = 2j 1, j N(129)ai =abv |ww otherwiseordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x i, set >v,w according Eq. 85 above. Finally, ordering constraints >v,w rest parents w pred(v) \ {u, w} set emptysets. relations >v >v,w identical previous case. Thus,>v >v,w forming strict partial order union elements vw .Eqs. 75, 85 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 129 implies action sequenceprovides v value G[v] latter specified.(IV) = ww , = bw , distinguish cases based w v .267fiKatz & Domshlak(1) |v | = 2y + 1, |w | = 2x, |w | |v |, construct two post-3/2candidates v , assign v cheapest among two, provingcost lower C(v ).(a) changes v wv done using action aw1 , largestpossible number changes bv done using action ab1 , remainingchanges bv done using action ab2 . candidate v , setba2 = 2j, j N, j x + 1(130)ai = ab1 = 2j, j N, x + 1 < jwa1 otherwisecost casebb(y + 1) C(aw1 ) + (x 1) C(a1 ) + (y x + 1) C(a2 )(131)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 132.[>v,w ={ai < aj | < j} {aj < ai | j i}(132)ai v ,aj wu pred(v) \ {w} set >v,u according Eq. 82.easy verify relation >v,w defined Eq. 132 strict totalorder domain. relations >v >v,u effectively identicalprevious case. Thus, >v >v,u >v >v,w forming strictpartial orders unions elements v u , v w ,respectively.Eqs. 75, 82, 132 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 130 implies action sequence provides v value G[v] latter specified.(b) changes v bv done using action ab1 , largestpossible number changes wv done using action aw1 , remaining.candidatev , setchanges wv done using action aw2wa2 = 2j 1, j N, j x + 1(133)ai = aw= 2j 1, j N, x + 1 < j + 11ba1 otherwisecost casewbx C(aw1 ) + (y + 1 x) C(a2 ) + C(a1 )(134)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 135.[>v,w ={ai < aj | 2y 2x + 1 + j} {aj < ai | > 2y 2x + 1 + j}ai v ,aj w(135)268fiTractable Cost-Optimal Planningu pred(v) \ {w} set >v,u according Eq. 86.easy verify relation >v,w defined Eq. 135 strict totalorder domain. relations >v >v,u effectively identicalprevious case. Thus, >v >v,u >v >v,w forming strictpartial orders unions elements v u , v w ,respectively.Eqs. 75, 86, 135 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 133 implies action sequence provides v value G[v] latter specified.Now, cost-optimal plan , v cannot contain + x actionsbtypes aw1 a1 totally. Suppose contrary v containbleast + x + 1 actions types aw1 a1 . contains xactions types. Let ww wb . . . ww sequence 2y + 1 values wsupport cost-optimal plan v given w change value numbertimes. action type decrease needed lengthsequence 2. Therefore x actions typedecrease length 2y 2x, left sequencelength 2y + 1 (2y 2x) = 2x + 1. Therefore w cannot supportb+ x actions types aw1 a1 . Now, suppose given cost-optimalbplan v v actions type aw1 actions type a1 .+ y+x(136)wbbC(v ) C(aw1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 )(137)(131) (134),wC(ab2 ) C(ab1 ) C(aw2 ) C(a1 )(138)suppose contrary plan first case cost-optimal.Eq. 137wbbC(aw1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <bb(y + 1) C(aw1 ) + (x 1) C(a1 ) + (y x + 1) C(a2 )wbb(y + 1 ) (C(aw2 ) C(a1 )) < ( x + 1) (C(a2 ) C(a1 ))(139)Eq. 136 + 1 x + 1, together Eq. 138 contradictingEq. 139.(134) (131),wbbC(aw2 ) C(a1 ) C(a2 ) C(a1 )269(140)fiKatz & Domshlaksuppose contrary plan second case cost-optimal.Eq. 137wbbC(aw1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <wbx C(aw1 ) + (y x + 1) C(a2 ) + C(a1 )w(y ) (C(ab2 ) C(ab1 )) < ( x) (C(aw2 ) C(a1 ))(141)Eq. 136 x, together Eq. 140 contradicting Eq. 141.(2) |v | = 2y + 1, |w | = 2x, |w | > |v |, actions v set(awv |ww = 2j 1, j N(142)ai =abv |bw otherwiseordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 132 above. Finally, orderingconstraints >v,w rest parents w pred(v) \ {u, w} set emptysets. relations >v >v,w identical previous case. Thus,>v >v,w forming strict partial order union elements vw .Eqs. 75, 132 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 142 implies action sequenceprovides v value G[v] latter specified.(3) |v | = 2y, |w | = 2x, |w | |v | + 1, construct two post-3/2candidates v , assign v cheapest among two, provingcost lower C(v ).(a) changes v wv done using action aw1 , largestpossible number changes bv done using action ab1 , remainingchanges bv done using action ab2 . candidate v , setba1 = 2j, j N, j < x(143)ai = ab2 = 2j, j N, x jwa1 otherwisecost casebbC(aw1 ) + (x 1) C(a1 ) + (y x + 1) C(a2 )(144)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 132 above.u pred(v) \ {w} set >v,u according Eq. 82.relations >v , >v,w >v,u effectively identical previous case.Thus, >v >v,u >v >v,w forming strict partial ordersunions elements v u , v w , respectively.270fiTractable Cost-Optimal PlanningEqs. 75, 82, 132 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 143 implies action sequence provides v value G[v] latter specified.(b) changes v bv done using action ab1 , largestpossible number changes wv done using action aw1 , remainingwchanges wv done using action a2 . candidate v , setwa2ai = aw1ba1= 2j 1, j N, j x + 1= 2j 1, j N, x + 1 < jotherwise(145)cost casewb(x 1) C(aw1 ) + (y x + 1) C(a2 ) + C(a1 )(146)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 135 above.u pred(v) \ {w} set >v,u according Eq. 86.relations >v , >v,w >v,u effectively identical previous case.Thus, >v >v,u >v >v,w forming strict partial ordersunions elements v u , v w , respectively.SEqs. 75, 86, 135 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 145 implies action sequence provides v value G[v] latter specified.Now, cost-optimal plan , v cannot contain + x 1bactions types aw1 a1 totally. Suppose contrary v containbleast + x actions types aw1 a1 . contains xactions types. Let ww bw . . . bw sequence 2y values wsupport cost-optimal plan v given w change value numbertimes. action type decrease needed lengthsequence 2. Therefore x actions typedecrease length 2y 2x, left sequencelength 2y (2y 2x) = 2x, subsequence w , contradictingfact w smaller size starts differentcharacter. Therefore w cannot support + x actions types aw1ba1 . Now, suppose given cost-optimal plan v vbactions type aw1 actions type a1 .+ y+x(147)wbbC(v ) C(aw1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 )271(148)fiKatz & Domshlak(144) (146),wC(ab2 ) C(ab1 ) C(aw2 ) C(a1 )(149)suppose contrary plan first case cost-optimal.Eq. 148wbbC(aw1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <bbC(aw1 ) + x C(a1 ) + (y x) C(a2 )wbb(y ) (C(aw2 ) C(a1 )) < ( x) (C(a2 ) C(a1 ))(150)Eq. 147 x, together Eq. 149 contradicting Eq. 150.(146) (144),wbbC(aw2 ) C(a1 ) C(a2 ) C(a1 )(151)suppose contrary plan second case cost-optimal.Eq. 148wbbC(aw1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <wbx C(aw1 ) + (y x) C(a2 ) + C(a1 )w(y ) (C(ab2 ) C(ab1 )) < ( x) (C(aw2 ) C(a1 ))(152)Eq. 147 x, together Eq. 151 contradicting Eq. 152.(4) |v | = 2y, |w | = 2x, |w | > |v | + 1, actions v set(awv |ww = 2j 1, j N(153)ai =abv |bw otherwiseordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 132 above. Finally, orderingconstraints >v,w rest parents w pred(v) \ {u, w} set emptysets. relations >v >v,w identical previous case. Thus,>v >v,w forming strict partial order union elements vw .Eqs. 75, 132 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 153 implies action sequenceprovides v value G[v] latter specified.(5) |v | = 2y + 1, |w | = 2x + 1, |w | |v | + 1, construct two post-3/2candidates v , assign v cheapest among two, provingcost lower C(v ).272fiTractable Cost-Optimal Planning(a) changes v wv done using action aw1 , largestpossible number changes bv done using action ab1 , remainingchanges bv done using action ab2 . candidate v , setba1 = 2j, j N, j < xai = ab2 = 2j, j N, x j(154)wa1 otherwisecost casebb(y + 1) C(aw1 ) + (x 1) C(a1 ) + (y x + 1) C(a2 )(155)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x i, set >v,w according Eq. 156.[>v,w ={ai < aj | < j < 2x} {aj < ai | j i, j < 2x} {ai < a2x }ai v ,aj w(156)u pred(v) \ {w} set >v,u according Eq. 82.easy verify relation >v,w defined Eq. 156 strict totalorder domain. relations >v >v,u effectively identicalprevious case. Thus, >v >v,u >v >v,w forming strictpartial orders unions elements v u , v w ,respectively.Eqs. 75, 82, 156 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 154 implies action sequence provides v value G[v] latter specified.(b) changes v bv done using action ab1 , largestpossible number changes wv done using action aw1 , remainingwchanges wv done using action a2 . candidate v , setwa2 = 2j 1, j N, j x j = + 1(157)ai = aw= 2j 1, j N, x < j1ba1 otherwisecost casewbx C(aw1 ) + (y + 1 x) C(a2 ) + C(a1 )(158)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 159.[>v,w ={ai < aj | < 2y 2x + j} {aj < ai | 2y 2x + j}ai v ,aj w(159)273fiKatz & Domshlaku pred(v) \ {w} set >v,u according Eq. 86.easy verify relation >v,w defined Eq. 159 strict totalorder domain. relations >v >v,u effectively identicalprevious case. Thus, >v >v,u >v >v,w forming strictpartial orders unions elements v u , v w ,respectively.Eqs. 75, 86, 159 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 157 implies action sequence provides v value G[v] latter specified.Now, cost-optimal plan , v cannot contain + x+ 1 actionsbtypes aw1 a1 totally. Suppose contrary v contain leastb+ x + 2 actions types aw1 a1 . contains x 1actions types. Let ww bw . . . ww sequence 2y + 1 values wsupport cost-optimal plan v given w change value numbertimes. action type decrease needed lengthsequence 2. Therefore x 1 actions typedecrease length 2y 2x 2, left sequencelength 2y + 1 (2y 2x 2) = 2x + 3. Therefore w cannot supportb+ x + 1 actions types aw1 a1 . Now, suppose givencost-optimal plan v v actions type aw1 actions typeab1 .+ y+x+1(160)wbbC(v ) C(aw1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 )(161)(155) (158),wC(ab2 ) C(ab1 ) C(aw2 ) C(a1 )(162)suppose contrary plan first case cost-optimal.Eq. 161wbbC(aw1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <bb(y + 1) C(aw1 ) + x C(a1 ) + (y x) C(a2 )wbb(y + 1 ) (C(aw2 ) C(a1 )) < ( x) (C(a2 ) C(a1 ))(163)Eq. 160 + 1 x, together Eq. 162 contradictingEq. 163.(158) (155),wbbC(aw2 ) C(a1 ) C(a2 ) C(a1 )274(164)fiTractable Cost-Optimal Planningsuppose contrary plan second case cost-optimal.Eq. 161wbbC(aw1 ) + (y + 1 ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <wb(x + 1 C(aw1 ) + (y x) C(a2 ) + C(a1 )w(y ) (C(ab2 ) C(ab1 )) < ( x 1) (C(aw2 ) C(a1 ))(165)Eq. 160 x 1, together Eq. 164 contradictingEq. 165.(6) |v | = 2y + 1, |w | = 2x + 1, |w | > |v | + 1, actions v set(awv |ww = 2j 1, j N(166)ai =abv |bw otherwiseordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 132 above.Finally, ordering constraints >v,w rest parents w pred(v) \{u, w} set empty sets.relations >v >v,w identical previous case. Thus,>v >v,w forming strict partial order union elements vw .Eqs. 75, 132 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 166 implies action sequenceprovides v value G[v] latter specified.(7) |v | = 2y, |w | = 2x + 1, |w | |v |, construct two post-3/2candidates v , assign v cheapest among two, provingcost lower C(v ).(a) changes v wv done using action aw1 , largestpossible number changes bv done using action ab1 , remainingchanges bv done using action ab2 . candidate v , setba2 = 2j, j N, j x(167)ai = ab1 = 2j, j N, x < jwa1 otherwisecost casebbC(aw1 ) + x C(a1 ) + (y x) C(a2 )(168)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x i, set >v,w according Eq. 169.>v,w =[{ai < aj | < 2y 2x + j, j > 1} {aj < ai | 2y 2x + j} {a1 < ai }v ,aj w(169)275fiKatz & Domshlaku pred(v) \ {w} set >v,u according Eq. 82.easy verify relation >v,w defined Eq. 169 strict totalorder domain. relations >v >v,u effectively identicalprevious case. Thus, >v >v,u >v >v,w forming strictpartial orders unions elements v u , v w ,respectively.Eqs. 75, 82, 169 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 167 implies action sequence provides v value G[v] latter specified.(b) changes v bv done using action ab1 , largestpossible number changes wv done using action aw1 , remainingchanges wv done using action aw.candidatev , set2wa2ai = aw1ba1= 2j 1, j N, j x= 2j 1, j N, x < jotherwise(170)cost casewbx C(aw1 ) + (y x) C(a2 ) + C(a1 )(171)ordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 159 above.u pred(v) \ {w} set >v,u according Eq. 86 above.relations >v , >v,w >v,u effectively identical previous case.Thus, >v >v,u >v >v,w forming strict partial ordersunions elements v u , v w , respectively.SEqs. 75, 86, 159 derive linearization >v wpred(v) >v,wdefines sequence actions applicable respect {v} pred(v).addition, |v | = |v | together Eq. 170 implies action sequence provides v value G[v] latter specified.Now, cost-optimal plan , v cannot contain + x actionsbtypes aw1 a1 totally. Suppose contrary v contain leastb+ x + 1 actions types aw1 a1 . contains x 1actions types. Let ww bw . . . bw sequence 2y values wsupport cost-optimal plan v given w change value numbertimes. action type decrease needed lengthsequence 2. Therefore x 1 actions typedecrease length 2y 2x 2, left sequencelength 2y (2y 2x 2) = 2x + 2. Therefore w cannot supportb+ x actions types aw1 a1 . Now, suppose given cost-optimalbplan v v actions type aw1 actions type a1 .+ y+x276(172)fiTractable Cost-Optimal PlanningwbbC(v ) C(aw1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 )(173)(168) (171),wC(ab2 ) C(ab1 ) C(aw2 ) C(a1 )(174)suppose contrary plan first case cost-optimal.Eq. 173wbbC(aw1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <bbC(aw1 ) + x C(a1 ) + (y x) C(a2 )wbb(y ) (C(aw2 ) C(a1 )) < ( x) (C(a2 ) C(a1 ))(175)Eq. 172 x, together Eq. 174 contradicting Eq. 175.(171) (168),wbbC(aw2 ) C(a1 ) C(a2 ) C(a1 )(176)suppose contrary plan second case cost-optimal.Eq. 173wbbC(aw1 ) + (y ) C(a2 ) + C(a1 ) + (y ) C(a2 ) <wbx C(aw1 ) + (y x) C(a2 ) + C(a1 )w(y ) (C(ab2 ) C(ab1 )) < ( x) (C(aw2 ) C(a1 ))(177)Eq. 172 x, together Eq. 176 contradicting Eq. 177.(8) |v | = 2y, |w | = 2x + 1, |w | > |v |, actions v set(awv |ww = 2j 1, j N(178)ai =abv |bw otherwiseordering constraints >v set according Eq. 75. Likewise, w =ha1 , . . . , a2x1 i, set >v,w according Eq. 132 above.Finally, ordering constraints >v,w rest parents w pred(v) \{u, w} set empty sets.relations >v >v,w identical previous case. Thus,>v >v,w forming strict partial order union elements vw .277fiKatz & Domshlaknow, specified sequences v , orders >v induced sequences,orders >v,w , proved >v >v,w >w >v,w form strict partial ordersdomains. construction allows us apply Theorem 1 (consideredsets) sequences v orders >v >v,w , proving[[>=(>v>v,w )vVwpred(v)forms strict partial order union v1 , . . . , vn . Putting thing together,implies linearization > plan , post-3/2nesssubsequences v1 , . . . , vn implies P 3/2 (). Moreover, optimal plan, C( ) = C() implies optimality .Theorem 9 Let P(1) problem, COP = (X , F) correspondingconstraintPoptimization problem, x optimal assignment X F (x) = .(I) < , plan cost reconstructed x time polynomialdescription size .(II) plan, < .Proof:P(I) Given COP solution x F (x) = < , construct planC() = . construct plan1. Traversing planning variables topological ordering causal graph CG(),associating variable v sequence v Av .2. Merging constructed sequences v1 , . . . , vn desired plan .v V pred(v) = set v = ha1 . . . al i, l = |xv | 1, aidefined Eq 179 below.(awv , odd,(179)ai =abv , even,Note Eq. 179 well-definedthe existence essential Eq. 179 actions awv /abvimplied Eq. 22 < .wk1turn, v V pred(v) = {w1 , . . . , wk }, given xwv , . . . , xv , distinguishfollowing cases.[ R1 played ] R1 played one parents, parents play role R11.[ R1 played w1 ] Eq. 35 impliesw11xwv 1 (xwv , xw1 ) = (R1, xv , xw1 )wj1xwv = S1. Eq. 36 xv = S1 1 < j k,giving uswwxwj (xv j , xv j1 , xwj ) = 0v278fiTractable Cost-Optimal Planning[ R1 played wj , j > 1 ] Eq. 36 impliesxwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R1, S, #w #w , #b #b , , wj )and, 1 < 6= j k,wi1xwv (xw, xwi ) = 0.v , xvEq. 35 also1xwv 1 (xwv , x w1 ) = 0sub-cases, v , >v >v,w specified proof Theorem 8,case I.[ R2 played ] R2 played one parents, parents play role R11.[ R2 played w1 ] Eq. 35 impliesw11xwv 1 (xwv , xw1 ) = (R2, xv , xw1 )and, 1 < j k, Eq. 36 implieswwxwj (xv j , xv j1 , xwj ) = 0v(R2, [S, #w , #b , ]] , w1 ) = (#w , 0, #b , 0), v , >v >v,w specifiedproof Theorem 8, case III.2, otherwise, case IV.2.[ R2 played wj , j > 1 ] Eq. 36 impliesxwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R2, S, #w #w , #b #b , , wj )and, 1 < 6= j k,wi1xwv (xw, xwi ) = 0.v , xvEq. 35 also1xwv 1 (xwv , x w1 ) = 0(R2, [S, #w #w , #b #b , ]] , wj ) = (#w #w , 0, #b #b , 0), v , >v>v,w specified proof Theorem 8, case III.2, otherwise,case IV.2.[ R3 R4 played ] roles played two parents,parents play role R11.279fiKatz & Domshlak[ R3 played w1 , R4 played wj , j > 1 ] Eqs. 35 36w11xwv 1 (xwv , xw1 ) = (R3, xv , xw1 )xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R4, S, #w #w , #b #b , , wj )and, 1 < k, 6= j:wi1, x wi ) = 0xwv (xwv , xv[ R4 played w1 , R3 played wj , j > 1 ] Eqs. 35 36w11xwv 1 (xwv , xw1 ) = (R4, xv , xw1 )xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R3, S, #w #w , #b #b , , wj )and, 1 < 6= j k,wi1, x wi ) = 0xwv (xwv , xv[ R3 played wj , R4 played wt , j 6= t, j, > 1 ] Eqs. 35 361xwv 1 (xwv , x w1 ) = 0xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R3, S, #w #w , #b #b , , wj )xwv ([[S, #w , #b , ]] , , #w , #b , , wt ) =(R4, S, #w #w , #b #b , , wt )and, 1 < k 6 {j, t},wi1xwv (xw, x wi ) = 0v , xvthree sub-cases, v , >v >v,w specified proof Theorem 8,case II.[ R5 played ] R5 played one parents, parents play role R11.280fiTractable Cost-Optimal Planning[ R5 played w1 ] Eqs. 35 36 implyw11xwv 1 (xwv , xw1 ) = (R5, xv , xw1 )and, 1 < j k,wwxwj (xv j , xv j1 , xwj ) = 0vConsidering specification function Eq. 29,first case holds, minimum obtained first expression,v , >v >v,w defined proof Theorem 8, case III.1.a.first case holds, minimum obtained second expression,v , >v >v,w defined proof Theorem 8, case IV.1.a.second case holds, minimum obtained first expression,v , >v >v,w defined proof Theorem 8, case III.3.a.second case holds, minimum obtained second expression, v , >v >v,w defined proof Theorem 8, caseIV.3.a.third case holds, v , >v >v,w defined proofTheorem 8, case III.3.a.forth case holds, v , >v >v,w defined proofTheorem 8, case IV.3.a.fifth case holds, minimum obtained first expression,v , >v >v,w defined proof Theorem 8, case III.5.a.fifth case holds, minimum obtained second expression,v , >v >v,w defined proof Theorem 8, case IV.5.a.sixth case holds, v , >v >v,w defined proofTheorem 8, case III.5.a.seventh case holds, v , >v >v,w defined proofTheorem 8, case IV.5.a.eighth case holds, minimum obtained first expression,v , >v >v,w defined proof Theorem 8, case III.7.a.eighth case holds, minimum obtained second expression, v , >v >v,w defined proof Theorem 8, caseIV.7.a.[ R5 played wj , j > 1 ] Eq. 36 impliesxwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R5, S, #w #w , #b #b , , wj )and, 1 < 6= j k,wi1, xwi ) = 0.xwv (xwv , xvEq. 35 also1xwv 1 (xwv , x w1 ) = 0v , >v >v,w specified exactly previous case.281fiKatz & Domshlak[ R6 played ] R6 played one parents, parents play role R11.[ R6 played w1 ] Eqs. 35 36 implyw11xwv 1 (xwv , xw1 ) = (R6, xv , xw1 )and, 1 < j k,wwxwj (xv j , xv j1 , xwj ) = 0vConsidering specification function Eq. 30,first case holds, minimum obtained first expression,v , >v >v,w defined proof Theorem 8, case III.1.b.first case holds, minimum obtained second expression,v , >v >v,w defined proof Theorem 8, case IV.1.b.second case holds, minimum obtained first expression,v , >v >v,w defined proof Theorem 8, case III.3.b.second case holds, minimum obtained second expression, v , >v >v,w defined proof Theorem 8, caseIV.3.b.third case holds, v , >v >v,w defined proofTheorem 8, case III.3.b.forth case holds, v , >v >v,w defined proofTheorem 8, case IV.3.b.fifth case holds, minimum obtained first expression,v , >v >v,w defined proof Theorem 8, case III.5.b.fifth case holds, minimum obtained second expression,v , >v >v,w defined proof Theorem 8, case IV.5.b.sixth case holds, v , >v >v,w defined proofTheorem 8, case III.5.b.seventh case holds, minimum obtained first expression,v , >v >v,w defined proof Theorem 8, case III.7.b.seventh case holds, minimum obtained second expression, v , >v >v,w defined proof Theorem 8,case IV.7.b.[ R6 played wj , j > 1 ] Eq. 36 impliesxwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R6, S, #w #w , #b #b , , wj )and, 1 < 6= j k,wi1, xwi ) = 0.xwv (xwv , xvEq. 35 also1xwv 1 (xwv , x w1 ) = 0v , >v >v,w specified exactly previous case.282fiTractable Cost-Optimal Planning[ R7 R9 played ] roles played two parents,parents play role R11.[ R7 played w1 , R9 played wj , j > 1 ] Eqs. 35 36w11xwv 1 (xwv , xw1 ) = (R7, xv , xw1 )xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R9, S, #w #w , #b #b , , wj ),for 1 < 6= j k,wi1xwv (xw, x wi ) = 0v , xvConsidering specification function Eq. 31,first case holds, minimum obtained first expression,v , >v >v,w defined proof Theorem 8, case III.1.a.first case holds, minimum obtained second expression,v , >v >v,w defined proof Theorem 8, case IV.1.a.second case holds, v , >v >v,w defined proofTheorem 8, case III.3.a.third case holds, v , >v >v,w defined proofTheorem 8, case IV.3.a.forth case holds, v , >v >v,w defined proofTheorem 8, case III.5.a.fifth case holds, v , >v >v,w defined proofTheorem 8, case IV.5.a.sixth case holds, minimum obtained first expression,v , >v >v,w defined proof Theorem 8, case III.7.a.sixth case holds, minimum obtained second expression,v , >v >v,w defined proof Theorem 8, case IV.7.a.[ R9 played w1 , R7 played wj , j > 1 ] Eqs. 35 36w11xwv 1 (xwv , xw1 ) = (R9, xv , xw1 )xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R7, S, #w #w , #b #b , , wj )and, 1 < 6= j k,wi1xwv (xw, x wi ) = 0v , xvv , >v >v,w specified exactly previous case.283fiKatz & Domshlak[ R7 played wj , R9 played wt , j 6= t, j, > 1 ] Eqs. 35 361xwv 1 (xwv , x w1 ) = 0xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R7, S, #w #w , #b #b , , wj )xwv ( [S, #w , #b , ]] , , #w , #b , , wt ) =(R9, S, #w #w , #b #b , , wt )and, 1 < k, 6 {j, t},wi1xwv (xw, x wi ) = 0v , xvThen, v , >v >v,w specified exactly two previous cases.[ R8 R10 played ] roles played two parents,parents play role R11.[ R8 played w1 , R10 played wj , j > 1 ] Eqs. 35 36w11xwv 1 (xwv , xw1 ) = (R8, xv , xw1 )xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R10, S, #w #w , #b #b , , wj )and, 1 < 6= j k,wi1xwv (xw, x wi ) = 0v , xvConsidering specification function Eq. 32,first case holds, minimum obtained first expression,v , >v >v,w defined proof Theorem 8, case III.1.b.first case holds, minimum obtained second expression,v , >v >v,w defined proof Theorem 8, case IV.1.b.second case holds, v , >v >v,w defined proofTheorem 8, case III.3.b.third case holds, v , >v >v,w defined proofTheorem 8, case IV.3.b.forth case holds, v , >v >v,w defined proofTheorem 8, case III.5.b.fifth case holds, v , >v >v,w defined proofTheorem 8, case IV.5.b.284fiTractable Cost-Optimal Planningsixth case holds, minimum obtained first expression,v , >v >v,w defined proof Theorem 8, case III.7.b.sixth case holds, minimum obtained second expression,v , >v >v,w defined proof Theorem 8, case IV.7.b.[ R10 played w1 , R8 played wj , j > 1 ] Eqs. 35 36w11xwv 1 (xwv , xw1 ) = (R10, x v , xw1 )xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R8, S, #w #w , #b #b , , wj )and, 1 < 6= j k,wi1xwv (xw, x wi ) = 0v , xvv , >v >v,w specified exactly previous case.[ R8 played wj , R10 played wt , j 6= t, j, > 1 ] Eqs. 35 361xwv 1 (xwv , x w1 ) = 0xwj ([[S, #w , #b , ]] , , #w , #b , , wj ) =v(R8, S, #w #w , #b #b , , wj )xwv ( [S, #w , #b , ]] , , #w , #b , , wt ) =(R10, S, #w #w , #b #b , , wt )and, 1 < k, 6 {j, t},wi1, x wi ) = 0xwv (xwv , xvThen, v , >v >v,w specified exactly two previous cases.now, variable v V , specified action sequence vorder >v elements v . w pred(v), specified order >v,w ,proved >v >v,w >w >v,w form strict partial orders domains.Similarly uniform cost case, construction allows us apply Theorem 1(considered sets) sequences v orders >v >v,w , proving[[>=(>v>v,w )vVwpred(v)forms strict partial order union v1 , . . . , vn .285fiKatz & DomshlakFinally, note plan extraction step algorithm polytree-1-dep corresponds exactly construction along Eqs. 74-79, 81-83, 85-86, 93-94, 9697, 105-106, 108, 110, 117-118, 120, 122, 129-130, 132-133, 135, 142-143, 145, 153-154, 156157, 159, 166-167, 169-170, 178, providing us poly-time concrete cost-optimal plancorresponding optimal solution COP .(II) prove solvable, must < . Assume contrarycase. Let solvable P(1) problem, let (using Theorem 8)irreducible, post-3/2 plan . Given , let COP assignment x definedfollows.1. COP variable xv , assignment x provides value v [(v)]|v | = |v | + 1.2. variable v V , pred(v) 6= , find (at two) parentsprevail actions v . Let w parent performs role R{R1, R2, R3, R5, R6, R7, R8}, w parent performs oneroles R {R4, R9, R10, R11}. (By definition post-3/2 action sequences,rest parents perform role R11.) Given that, |pred(v)| = k > 0, adoptkxwordering pred(v) w1 hh= w wk = w . First, assignmentv COPii||1||1vvkvariable xwv provides value S1, 2 , 2 , |v | 1 . Then, 1 < k,wiassignment xwv COP variable xv provides value [S, #w , #b , |v | 1]],S2, R = R4S4, R = R9S=S5, R = R10S1, R = R11#w #b numbers actions v change value v wvbv , respectively, prevailed value w1 .Eq. 22-36 that, v V , pred(v) = ,P xv (xv ) = C(v ).wkOtherwise, pred(v) = {w1 , . . . , wk }, xv (xv , xv ) = 0, wpred(v) xwv (x ) =C(v ). Therefore,XX(x ) =C(v ) = C(),FvVprove.ReferencesBacchus, F., & Yang, Q. (1994). Downward refinement efficiency hierarchicalproblem solving. Artificial Intelligence, 71 (1), 43100.Backstrom, C., & Klein, I. (1991). Planning polynomial time: SAS-PUBS class.Computational Intelligence, 7 (3), 181197.286fiTractable Cost-Optimal PlanningBackstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. ComputationalIntelligence, 11 (4), 625655.Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (12), 533.Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004). CP-nets: toolrepresenting reasoning conditional ceteris paribus preference statements.Journal Artificial Intelligence Research, 21, 135191.Brafman, R. I., & Domshlak, C. (2003). Structure complexity planning unaryoperators. Journal Artificial Intelligence Research, 18, 315349.Brafman, R. I., & Domshlak, C. (2006). Factored planning: How, when, not.Proceedings 18th National Conference Artificial Intelligence (AAAI), pp.809814, Boston, MA.Bylander, T. (1994). computational complexity propositional STRIPS planning.Artificial Intelligence, 69 (1-2), 165204.Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32 (3), 333377.Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1990). Introduction Algorithms. MITPress.Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.Edelkamp, S. (2001). Planning pattern databases. Proceedings EuropeanConference Planning (ECP), pp. 1334.Erol, K., Nau, D. S., & Subrahmanian, V. S. (1995). Complexity, decidability undecidability results domain-independent planning. Artificial Intelligence, Special IssuePlanning, 76 (12), 7588.Garey, M. R., & Johnson, D. S. (1978). Computers Intractability: Guide TheoryNP-Completeness. W.H. Freeman Company, New-York.Gimenez, O., & Jonsson, A. (2008). complexity planning problems simplecausal graphs. Journal Artificial Intelligence Research, 31, 319351.Haslum, P. (2006). Admissible Heuristics Automated Planning. Ph.D. thesis, LinkopingUniversity, Department Computer Information Science.Haslum, P., Bonet, B., & Geffner, H. (2005). New admissible heuristics domainindependent planning. Proceedings 20th National Conference ArtificialIntelligence (AAAI), pp. 11631168, Pittsburgh, PA.Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. Proceedings 15th International Conference Artificial Intelligence Planning Systems(AIPS), pp. 140149, Breckenridge, CO.Helmert, M. (2003). Complexity results standard benchmark domains planning.Artificial Intelligence, 146 (2), 219262.Helmert, M. (2006). Fast Downward planning system. Journal Artificial IntelligenceResearch, 26, 191246.287fiKatz & DomshlakHoffmann, J. (2003). Utilizing Problem Structure Planning: Local Search Approach.No. 2854 LNAI. Springer-Verlag.Hoffmann, J., & Edelkamp, S. (2005). deterministic part IPC-4: overview. JournalArtificial Intelligence Research, 24, 519579.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generationheuristic search. Journal Artificial Intelligence Research, 14, 253302.Jonsson, P., & Backstrom, C. (1995). Incremental planning. New Directions AIPlanning: EWSP95-3rd European Workshop Planning, pp. 7990, Assisi, Italy.Jonsson, P., & Backstrom, C. (1998a). State-variable planning structural restrictions:Algorithms complexity. Artificial Intelligence, 100 (12), 125176.Jonsson, P., & Backstrom, C. (1998b). Tractable plan existence imply tractableplan generation. Annals Mathematics Artificial Intelligence, 22 (3-4), 281296.Kambhampati, S. (1995). Admissible pruning strategies based plan minimality planspace planning. Proceedings 14th International Joint Conference ArtificialIntelligence, pp. 16271635, Montreal, Canada.Katz, M., & Domshlak, C. (2007). Structural patterns heuristics. ICAPS-07 Workshop Heuristics Domain-independent Planning: Progress, Ideas, Limitations,Challenges, Providence, RI.Klein, I., Jonsson, P., & Backstrom, C. (1998). Efficient planning miniature assemblyline. Artificial Intelligence Engineering, 13 (1), 6981.Knoblock, C. (1994). Automatically generating abstractions planning. Artificial Intelligence, 68 (2), 243302.Newell, A., & Simon, H. A. (1963). GPS: program simulates human thought.Feigenbaum, E. A., & Feldman, J. (Eds.), Computers Thought, pp. 279293.Oldenbourg.Sacerdoti, E. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence,5, 115135.Tenenberg, J. D. (1991). Abstraction planning. Allen, J. F., Kautz, H. A., Pelavin,R. N., & Tenenberg, J. D. (Eds.), Reasoning Plans, chap. 4, pp. 213283. Morgan Kaufmann.Williams, B., & Nayak, P. (1996). model-based approach reactive self-configuringsystems. Proceedings 13th National Conference Artificial Intelligence(AAAI), pp. 971977, Portland, OR.Williams, B., & Nayak, P. (1997). reactive planner model-based executive. Proceedings 15th International Joint Conference Artificial Intelligence (IJCAI),pp. 11781185, Nagoya, Japan.288fiJournal Artificial Intelligence Research 32 (2008) 487-523Submitted 11/07; published 06/08Refining Execution Abstract ActionsLearned Action ModelsFreek StulpMichael Beetzstulp@cs.tum.edubeetz@cs.tum.eduIntelligent Autonomous Systems GroupTechnische Universitat MunchenBoltzmannstrae 3, D-85747 Garching bei Munchen, GermanyAbstractRobots reason abstract actions, go position l, order decidegenerate plans intended course action. use abstract actionsenables robots employ small action libraries, reduces search space decisionmaking. executing actions, however, robot must tailor abstract actionsspecific task situation context hand.article propose novel robot action execution system learns successperformance models possible specializations abstract actions. execution time,robot uses models optimize execution abstract actions respective taskcontexts. robot use abstract actions efficient reasoning, without compromising performance action execution. show impact action executionmodel three robotic domains two kinds action execution problems: (1)instantiation free action parameters optimize expected performance action sequences; (2) automatic introduction additional subgoals make action sequencesreliable.1. Introductionmotor control, main challenge map high-level goals plans low-level motorcommands. instance, soccer scenario Figure 1(a), needs doneabstract level informally declared as: achieve scoring opportunity, firstapproach ball, dribble towards opponents goal. actually executeabstract plan, robot must map low-level motor commands, translationalrotational velocities.Using durative actions bridge gap proven successful approachnature (Wolpert & Ghahramani, 2000) robotics (Arkin, 1998). Durative parameterizable actions, simply actions 1 , encapsulate knowledge certain goalsachieved certain task contexts, produce streams motor command achievegoals. instance, human robot soccer players typically dribbling, kicking,passing actions, relevant context soccer. Also, actions achieves different goals within different soccer contexts. abstract plan executedmapping sequence actions. running example, plan mappedaction sequence approachBall, dribbleBall.1. cognitive science, actions known inverse models control rules, robotics also behaviors,routines, policies, controllers.2008 AI Access Foundation. rights reserved.fiStulp & Beetz(a) Abstract action chain.(b) Suboptimal executionabrupt transition.(c) Time-optimal executionexhibits smooth motion.Figure 1: abstract action chain, two alternative executions chain.center line trajectory represents robots position. linesdrawn perpendicular robots orientation, width representstranslational velocity point. values recorded 10Hz.Although actions specify achieve goal, often several waysexecute them. Figure 1 depicts two executions action sequence. first,robot naively executes first action, arrives ball goal back,depicted Figure 1(b). unfortunate position start dribblingtowards goal. abrupt transition occurs actions, robot needsbrake, carefully maneuver behind ball direction goal. Preferably,robot go ball order dribble towards goal afterwards.robot should, depicted Figure 1(c), perform first action sub-optimally orderachieve much better position executing second action. article,performance measure execution duration. behavior shown Figure 1(c) definedoptimal, minimizes time needed execute action sequence.reason suboptimal behavior often witnessed robots robotplanners, also designers robot controllers, view actions level abstractionignores subtle differences actions. Abstracting away action detailsessential keep action selection programming planning tractable. However,planning system considers actions black boxes performance independentprior subsequent steps, planning system cannot tailor actions executioncontexts. often yields suboptimal behavior abrupt transitions actions,Figure 1(b). example, problem abstract view planner,ball considered sufficient dribbling ball dynamical staterobot arriving ball considered irrelevant dribbling action. Whereasangle approach relevant validity plan therefore free choose,must reasoned order optimize plan execution.So, given abstract plan, concrete action sequence predictedminimal total cost (execution duration) likely succeed? problembroken three subproblems: 1) cost success predicted? 2)action sequences optimized minimal cost? 3) sequences actionstransformed increase successful execution?488fiRefining Execution Abstract Actions Learned Action Modelstake inspiration findings cognitive science (Wolpert & Flanagan, 2001),solve first subproblem acquiring applying third kind motor controlknowledge: able predict outcome performance actions. runningexample, robot could predict performance alternative executions beforehand,could choose commit fastest execution. predict execution durationaction sequences, robot must predict execution duration individual actions.robot learn action models experimentation, observation generalization. observing executions actions various parameterizations,learning general models tree-based induction.implemented approach diversity real simulated robotic platforms, introduced next section. Afterwards, discuss related workSection 3. following four sections organized according flowchart Figure 2.representation (abstract) actions action models described Section 4, alonggeneration abstract plans. explain action models learned observed experience Section 5. first applications action models, subgoal refinement,described Section 6. Section 7, present subgoal assertion, transforms planspredicted fail successful plans. Throughout article, describerepresentations algorithms used implement flowchart Figure 2. Finally,conclude summary outlook future work Section 8.Figure 2: Outline article flowchart overall system.2. Robotic Domainsarticle, robots learn apply action models three domains: robotic soccer,service robotics arm control. variety robots domains chosenemphasize generality approach. Also, different characteristics domainsallow different aspects action model applications investigated.first domain robotic soccer (Kitano, Asada, Kuniyoshi, Noda, & Osawa, 1997).adversarial domain, performance efficiency essential achieving goalsteam. Tailoring actions perform well within given task context thereforenecessity. use customized Pioneer robots mid-size league team AgiloRoboCuppers (Beetz, Schmitt, Hanek, Buck, Stulp, Schroter, & Radig, 2004), onedepicted Figure 3(a). robots differential drive, localizesingle forward facing camera. Experiments domain conducted realrobots, well simulation (Buck, Beetz, & Schmitt, 2002a).test approach robots degrees freedom, operating richer environments, included articulated B21 robot simulated kitchen environ489fiStulp & Beetz(a) Agilo soccer robot.(b) B21 kitchen environment.(c) PowerCube arm.Figure 3: three robotic domains considered article.ment (Muller & Beetz, 2006). Household tasks also less reactive robotic soccer,require complex long-term planning, relevant contextaction sequence optimization presented Section 6. simulator based Gazebosimulator Player Project (Gerkey, Vaughan, & Howard, 2003). environment,depicted Figure 3(b) typical kitchen scenario, furniture, appliances, cupscutlery.scenarios domain related fetching delivering cups one placeanother. so, robot navigation actions, actions reaching cup,putting down.Finally, evaluate approach articulated manipulator, performs reaching movements. experiments performed PowerCube arm AmtecRobotics 6 degrees freedom, shown Figure 3(c). experiments, twojoints used.3. Related Worksection, discuss work related learning predictive models actions, optimalhybrid control, well learning preconditions.3.1 Learning Predictive Models ActionsJordan Rumelhart (1992) introduced Distal Learning, explicitly learns forwardmodels neural networks. action models used bridge gapdistal target values motor commands, enables motor control learning. Recently,robotic forward models also learned using Bayesian networks (Dearden & Demiris,2005). Infantes, Ingrand, Ghallab (2006) present recent work also includes usedynamic Bayesian networks. work extends approach Fox, Ghallab, Infantes,Long (2006), action models learned Hidden Markov Models.general, advantage Bayesian networks allow causal naturerobots control system modelled using probabilistic framework. However,appropriate predicting outcome motor command next time step,rather expected performance durative action.490fiRefining Execution Abstract Actions Learned Action ModelsModular Selection Identification Control (MOSAIC) architecture (Haruno,Wolpert, & Kawato, 2001) also integrates forward models computational modelmotor control. framework intended model two problems humans must solve:learn inverse models tasks, select appropriate inverse model,given certain task. MOSAIC uses multiple pairs forward inverse models so.architecture designed robot control. aware (robotic)controllers action models integral central part computationalmodel, acquired automatically, represented explicitly, used modularresources different kinds control problems.Haigh (1998) Belker (2004) also developed robots optimize high-levelplans action models. models used determine best path hallwayenvironment. approach rather focuses optimizing parameterization subgoalsfixed points plan, limited navigation plans tasks. authorsreport good experiences tree-based induction learning robot action modelsobserved data, also chosen learning algorithm.3.2 Optimal ControlOptimal control refers use online, optimal trajectory generation partfeedback stabilization typically nonlinear system (Astrom & Murray, 2008). Recedinghorizon control subclass optimal control approaches, (optimal) trajectoryplanned state x(t + Hp ) lies current state x(t)goal state xgoal (Astrom & Murray, 2008). planning next Hp steps, stepstrajectory executed (with 1 Hp ), new trajectory computednew current state x(t + ) x(t + + Hp ). example RHCwork Bellingham, Richards, (2002), apply RHC simulated autonomousaerial vehicles. rationale behind receding horizon control (RHC)diminishing return optimizing later parts trajectory beginning execution.experiments described Section 6.3.1 verify effect. comparison RHCmethods also follow Section 6.3.1.Todorov, Li, Pan (2005) take hierarchical approach optimal control. highlevel controller uses abstract state command representation, control lowlevel controller, turn controls physical plant. main differencework purpose low-level controller solve specific subtask,actions, rather perform instantaneous feedback transformationabstraction high-level controllers.Using redundant degrees freedom optimize subordinate criteria well studied context arm control, humans (Schaal & Schweighofer, 2005; Wolpert& Ghahramani, 2000; Uno, Wolpert, Kawato, & Suzuki, 1989) robots (Simmons &Demiris, 2004; Nakanishi, Cory, Mistry, Peters, & Schaal, 2005; Bertram, Kuffner, Dillmann, & Asfour, 2006). Arm poses said redundant many arm configurations result equivalent pose. work cited above, configurationscalled uncontrolled manifold, motion space, null space, finding best configuration called redundancy resolution null-space optimization. approaches above,optimization performed analytically, specific arm control domain. Learning491fiStulp & Beetzmodels observed experience enables whole range different robots applyvariety tasks.Smooth motion also arises control signals several actions combined usingweighted interpolation scheme. motion blending approaches found robotics (Utz,Kraetzschmar, Mayer, & Palm, 2005; Jaeger & Christaller, 1998; Saffiotti, Ruspini, & Konolige, 1993), well computer graphics (Perlin, 1995; Kovar & Gleicher, 2003). Sincediscrete transitions actions, visible execution. HoffmannDuffert (2004) propose another method generate smooth transitions different gaits quadruped robots, based smoothing signals frequency domain. Sinceactions use periodic, methods apply. approaches,assumed smooth motion lead better performance. However, achieving optimalbehavior explicit goal, objective performance measures optimized.3.3 Reinforcement LearningReinforcement Learning (RL) another method seeks optimize performance, specified reward function. RL approaches often model optimal control problemMarkov Decision Process (MDP), usually solved dynamic programming.Recent attempts combat curse dimensionality RL turned principledways exploiting temporal abstraction (Barto & Mahadevan, 2003). sense, Hierarchical Reinforcement Learning algorithms also learn action models, Q-value predictsfuture reward executing action. Note Q-values learned one specificenvironment, one specific reward function. values learned states,single goal function. action models general, describe actionindependent environment, context called. Therefore, actionmodels transfered task contexts. Haigh (1998) draws conclusioncomparing action models RL. view, action models also provideinformative performance measures physical meaning, execution time,scale better continuous complex state spaces.approach know explicitly combines planning ReinforcementLearning RL-TOPS (Reinforcement Learning - Teleo Operators) (Ryan & Pendrith, 1998).approach, sequences actions first generated based preconditionseffects, using Prolog. Reinforcement Learning within action sequence doneHSMQ (Dietterich, 2000). actions, abrupt transitions arise too,author recognizes cutting corners would improve performance, presentsolution.similar work, point view smoothness emergent propertyoptimality requirements redundant subgoals, approach Kollar Roy(2006). work, simulated robot maps environment range measurementstraversing set waypoints. policy minimizes error resulting maplearned RL. side-effect, smooth transitions waypoints arise. approachtested real robots.492fiRefining Execution Abstract Actions Learned Action Models3.4 Hybrid ControlProblems involving choice actions action chains often regarded planning problems. However, planning systems aim optimizing resources, time.scheduling systems better representing time constraints resources,could deal selection actions problem. Systems integrate planningscheduling, work Smith, Frank, Jonsson (2000), able optimizeresources, ignore interactions actions intermediate dynamical states.apply well continuous domain problems.PDDL (Fox & Long, 2003), resource consumption actions representedabstract level. Planners take resources account generating plans.contrast planners, system generates action sequences optimizedrespect realistic, non-linear, continuous performance models, groundedreal world learned observed experience.recent approaches using symbolic planning robots focus different problems arise plans executed real robots. instance, BouguerraKarlsson (2005) use probabilistic planners abstract planning domain, enablesexploit uncertainties determined probabilistic state estimation. aSyMov reasonsgeometric preconditions consequences actions simulated 3-D world (Cambon, Gravot, & Alami, 2004). Note methods complementary ratherincompatible described Bouguerra Karlsson (2005) Cambon et al. (2004),merging would combine advantages. Cambon et al. (2004) mentionresulting plan improved optimized way, describe how. probabilistic motion planning, post-processing step smoothing generated pathscommon procedure. Subgoal refinements might well integrated optimizationstep.Belker (2004) uses action models learned model trees optimize HierarchicalTransition Network (HTN) plans. HTN plans structured hierarchically high levelgoals low level commands. optimize performance, order actions,actions changed varying levels hierarchy. Rather refiningplans, system modifies HTN plans themselves, therefore applies HTN plansonly. hand, refine existing action chain, planner selectedindependently optimization processGenerating collision-free paths initial final robot configuration also knownrobot motion planning. common distinction algorithms generatepaths global, local hybrid approaches (Brock & Khatib, 1999). approach presented article global approach, planning performed offline.One disadvantage offline algorithms guarantee planned trajectory actually succeed execution. respect robotic motion planning, movableobstacles unforeseen dynamics two examples correctly planned trajectoriesfail. holds global approaches, work BouguerraKarlsson (2005), Cambon et al. (2004) examples discussed previously. Hybrid motionplanning approaches, compute commit plans offline, leave freedom plansreact unforeseen circumstances (Brock & Khatib, 1999). Whereas hybrid motionplanning approaches freedom subgoals considered, subgoal refinement con493fiStulp & Beetzsiders freedom subgoal itself. Note implies hybrid approachesincompatible subgoal refinement, might complement well.3.5 Learning PreconditionsSection 7, demonstrate failure action models, predict whether actionfail succeed, learned. similar learning preconditions actions.research learning preconditions, concept induced symbolic.Furthermore, examples consist symbols grounded real world.precondition learned examples, instance Inductive LogicProgramming (Benson, 1995) specialized methods logic inference (Shahaf &Amir, 2006; Clement, Durfee, & Barrett, 2007). approaches appliedrobots. believe symbolic representations suffice encapsulatecomplex conditions arise robot dynamics action parameterizations.models best learned experience, described Section 5.Schmill, Oates, Cohen (2000) present system non-symbolic planningoperators learned actual interaction robot real world. experiences robot first partitioned qualitatively different outcome classes,clustering approach. learned operators similar previously hand-codedoperators. classes known, robot learns map sensory featuresclasses decision tree, similar approach. approach aims learning predict robot perceive executing action scratch, whereas conditionrefinement aims refining already existing symbolic preconditions based changinggoals.Buck Riedmiller (2000) propose method learning success rate passingactions context RoboCup simulation league. neural network trained8000 examples pass attempted, along success failure pass.information used manually code action selection rules attemptpass expected succeed probability > 0.7. also good exampleintegrating human-specified learned knowledge controller.Sussman (1973) first realize bugs plans lead failure,actually opportunity construct improved robust plans. Althoughresearch done highly abstract symbolic blocks world domain, ideastill fundamental transformational planning. XFRMLearn framework proposedBeetz Belker (2000), human-specified declarative knowledge combined robotlearned knowledge. Navigation plans optimized respect execution timeanalyzing, transforming testing structured reactive controllers (SRCs) (Beetz, 2001).One difference XFRMLearn work, analysis phase learned insteadhuman-specified. Another difference XFRMLearn improves existing plans, whereascondition refinement learns adapt changing action requirements, refinedgoals.4. Conceptualizationconceptualization computational problem based dynamic systemmodel (Dean & Wellmann, 1991). model, world changes inter494fiRefining Execution Abstract Actions Learned Action Modelsaction two processes: controlled process controlling process. controlledprocess essentially environment, including body sensors robot.dynamic system model, assumed environment described setstate variables.controlling process performs two tasks. First, uses state estimation estimatevalues state variables environment, based percepts sensorsprovide. observable state variables encapsulated belief state. Second,controller takes belief state input, determines appropriate motor commandsdirect state variables certain target values. motor commands dispatchedmotor system robot environment. robotic domain, stateestimation methods, state variables belief state, motor commands listedTable 5 Appendix A. rest section focus concepts used insidecontroller.4.1 Actions Action LibrariesActions control programs produce streams motor commands, basedcurrent belief state. robotics, actions usually take parameters allowused wide range situations. Instead programming action dribbleBallToCenter, preferable program action dribbleBall(Pose) dribble balllocation field, including center. actions apply certain taskcontexts, easier design learn controller must able dealpossible contexts (Haruno, Wolpert, & Kawato, 1999; Jacobs & Jordan, 1993; Ginsberg,1989).One actions used goToPose, navigates robot current state[x,y,,v] future target state [xg ,yg ,g ,vg ] setting translational rotationalvelocity [v,] robot. Table 6 Appendix lists actions used differentrobots. action parameters separated values variables current statetarget values variables. first so-called observable state variables,whereas second exist internally controller. action models learnedobserved experience, learning applying independent action implementations.describe hand-coded implementations here, rather refer Stulp (2007,Appendix A).Action libraries contain set actions frequently used within given domain.action designed cover large set tasks, usually small set actionsneeded achieve tasks given domain. actions severaladvantages: 1) controller less complex, making robust. 2) Fewer interactionsactions need considered, facilitating action selection design autonomousplanning. 3) environment changes, actions need redesignedrelearned, making system adaptive, easier maintain.article, assume actions innate, change time.building blocks combined concatenated solve tasks single actioncould achieve alone. Actions never modified optimized way.Rather, parameters chosen expected performance optimal withingiven task context.495fiStulp & Beetz4.2 Action ModelsAction models predict performance outcome action. action executed certain parameters, corresponding action model calledparameters, instance predict execution duration. Action models discusseddetail Section 5.4.3 Abstract Actionsachieve complex tasks, actions combined concatenated. longtradition using symbolic planners generate abstract action chains robot control.Shakey, one first autonomous mobile robots, used symbolic representations determine action sequences would achieve goal (Nilsson, 1984). recent examplesinclude work Coradeschi Saffiotti (2001), Beetz (2001), Cambon et al. (2004)Bouguerra Karlsson (2005). One main reasons symbolic planningabstract actions interest robotics abstract away many aspectscontinuous belief state, planning replanning faster, complex problems dealt with. Planners good fixing general abstract structuretask. Also, action sequences action hierarchies must specified advance,generated online, depending situation hand, making system adaptive.Furthermore, robots reason plans offline execution, recognize repair failures advance (Beetz, 2000), preferable encountering taskexecution. Finally, symbolic planning robotics enables designers specify constraintsactions symbolically (Cambon et al., 2004).Abstract actions consist preconditions effects action (Nilsson, 1994).constitute controllers abstract declarative knowledge. effects representintended goal action. specifies region state space goalsatisfied. precondition region respect temporally extended action definedset world states continuous execution action eventually satisfyeffects. Figure 4 shows two abstract actions, along preconditions effects.Figure 4: abstract action chain consisting two abstract actions.system implementation, Planning Domain Description Language (PDDL2.1)used describe abstract actions, goals, states plans (Fox & Long, 2003). declarative knowledge preconditions effects (add- delete-list) abstractactions action library specified manually designer. PDDL planneruse Versatile Heuristic Partial Order Planner (Younes & Simmons, 2003)2 . VHPOP is,PDDL planners, general purpose planner specifically tailored robot planning.2. VHPOP downloaded free cost http://www.tempastic.org/vhpop/496fiRefining Execution Abstract Actions Learned Action Modelswork focusses problems need solved exploit symbolic plannningrobotics, uncertainty, failure recovery action monitoring (Bouguerra & Karlsson,2005), geometric constraints (Cambon et al., 2004), anchoring (Coradeschi & Saffiotti,2001). system presented section abstracts away problems focusmain contribution: optimization already generated plans. output VHPOPlist symbolic actions causal links, shall see example Figure 10.4.4 System OverviewSubgoal refinement subgoal assertion operate sequences actions.article, action sequences derived abstract action chains, generatedsymbolic planner. general computational model robot control based symbolicplans depicted Figure 5, similar models proposed BouguerraKarlsson (2005) Cambon et al. (2004).goal passed input system. Usually, abstract state derivedbelief state process called anchoring (Coradeschi & Saffiotti, 2001).International Planning Competition, consider limited number scenarios, enablingus specify scenarios PDDL advance. name scenario containedbelief state, corresponding abstract state read PDDL file.Figure 5: Computational model robot control based symbolic plans.: pddl goal, (represented PDDL)beliefState, (belief state state variables)action lib (library PDDL representations, actions, action models)output : exe action seq (an optimized sequences executable actions)input123456pddl state = readFromFile (beliefState.scenario name) // Anchoringpddl plan = makePlan (pddl state, pddl goal, action lib.pddl) // VHPOPexe action seq = instantiateAction (pddl plan, belief state, action lib.actions) // Section 6.1exe action seq = assertSubgoals (exe action seq,belief state, action lib) // Section 7exe action seq = refineSubgoals (exe action seq, action lib.models) // Section 6.2return exe action seq;Algorithm 1: System Overview.497fiStulp & Beetzpseudo-code complete system described article listed Algorithm 1.Data structures abstract declarative planning domain (see Figure 5)prefix pddl . section, presented implementations functionslines 1 2. next section, describe action models usedsubgoal refinement assertion (line 4 5) learned observed experience.implementations functions lines 3 5 presented Section 6 7. Notesubgoal refinement assertion modify existing action sequences.interfere planning instantiation process. means compatibleplanning systems.action models applied online task execution, must learned.learning phase described next section.5. Learning Action Modelssection, describe robots learn action models observed experience.advantage learning action models analytical methods 1) based realexperience, therefore takes factors relevant performance account. 2) many(hand-coded) actions difficult formalize analytically (Beetz & Belker, 2000).instance, exact interactions ball guide rail difficult predictmodel-predictive control, result subtle interplay many complex factors,robots dynamics, sensing capabilities, smoothness ball guide rail,parameterizations control program, etc. factors must modeled ordermake accurate predictions. 3) analysis sometimes impossible implementation action unknown, instance learned. 4) althoughimplemented work, learned action models also adapt changing environmentstrained online (Dearden & Demiris, 2005; Kirsch & Beetz, 2007). 5) enables robotslearn models robots, observing behavior, demonstrated Stulp, Isik,Beetz (2006).5.1 Data AcquisitionTraining examples gathered executing action, observing results.article, robots record learn predict execution duration success actions,given parameterization action. generate training data action, parametervalues initial goal state sampled uniformly possible values actionparameters take. possible parameter value ranges depend preconditionseffects action, specified semi-automatically. user defines rangesaction parameters ensure preconditions effects action met,action parameters used uniformly sampled ranges. executionaction initial goal state called episode.running example section learning predict execution durationgoToPose action simulated B21 Agilo robot. Figure 6(a) displaysconcrete example gathered training data simulated B21 robot. Here, 30 2948executions goToPose random initial goal states shown. total numberexecutions denoted ne . instance, ne =2948 example above. split498fiRefining Execution Abstract Actions Learned Action Modelsdata training test set. number examples training set N = 43 ne ,current example yields 2200 episodes.(a) Visualization 302948 goToPose executions.(b) original state space, two derived feature spaces. top figuresdepict features used. lower graphs plot time remainingreaching goal one features.Figure 6: Gathering transforming experience goToPose kitchen domain.learning algorithm trained 2200 examples likely make erroneous predictions previously unseen cases? general, hypothesis consistentsufficiently large training set deemed probably approximately correct (PAC) (Russell &Norvig, 2003). obtain model error probability 1 (i.e.PAC), learning algorithm must trained least N 1 (ln 1 + ln|H|) trainingexamples, |H| number possible hypotheses. use formulaexactly compute number training examples needed, rather determine strategieslearn accurate models limited amount costly training episodes. usethree approaches:Reduce number hypotheses |H|. exploiting invariances, mapdata original direct state space lower-dimensional derived feature space.limits number possible hypotheses |H|. navigation actions instance,translational rotational invariances original 8-D state space (listed Table 6)exploited transform 5-D features space, depicted Figure 6(b). features5-D state space correlate better performance measure. Haigh (1998) callsfeatures projective.exploiting invariances, reducing dimensionality feature space,leads decrease |H|. equation specifies lower |H|, fewer trainingexamples needed learn PAC model. reasoning, accurate models(i.e. lower ) learned lower dimensional feature spaces, given amountdata.experimentally verified training model tree learning algorithm (topresented Section 5.2) data mapped three different feature spaces499fiStulp & BeetzFigure 6(b). feature space, model trained N =2200 ne =2948executed episodes. Mean Absolute Error (MAE) models determinedseparate test containing remaining episodes3 . seen Figure 6(b),MAE lower lower dimensional feature spaces used. course, lowerdimensionality achieved simply discarding informative features, rathercomposing features projective features exploiting invariances.Increase number training examples N including intermediate examples. Instead using first initial example episode (large red dotsFigure 6(b)), could also include intermediate examples goal state.recorded 10Hz, yields many examples, higher N . However,one characteristic projective features pass point origin,seen right-most graph Figure 6(b). Therefore, including intermediateexamples lead distribution skewed towards origin. violatesstationarity assumption, poses training test set must sampledprobability distribution. learning algorithms trained abundancedata around origin biased towards states close goal,tend predict states accurately, cost inaccurate prediction statesgoal.Therefore, include first ni examples episode. meansnumber training examples roughly ne ni instead ne , still representsoriginal distribution initial states. Since best value ni clear analytically,determine empirically, described Stulp (2007)Track error empirically. Instead defining advance, computeMean Absolute Error (MAE) time data gathered, determinestabilizes visually. point, assume N sufficiently large, stop gatheringdata.5.2 Tree-based InductionLearning algorithms used learning robot action models observed experienceinclude neural networks (Buck, Beetz, & Schmitt, 2002b), tree-based induction (Belker,2004; Haigh, 1998). shown significant difference accuracyaction models learned neural networks tree-based induction (Stulp et al., 2006).However, decision model trees advantage converted setsrules, visually inspected. Furthermore, model trees optimizedanalytically described Section 6.2. reasons, focus decisionmodel trees article.Decision trees functions map continuous nominal input features nominaloutput value. function learned examples piecewise partitioningfeature space. One class chosen represent data partition. Model treesgeneralization decision trees, nominal values leaf nodes replacedline segments. line fitted data partition linear regression.3. prefer MAE Root Mean Square Error (RMSE), intuitive understand,cost prediction error roughly proportional size error. needweight larger errors more.500fiRefining Execution Abstract Actions Learned Action Modelslinear function interpolates data partition. enables model treesapproximate continuous functions. information decision model trees,learned tree-based induction, refer (Quinlan, 1992; Stulpet al., 2006; Wimmer, Stulp, Pietzsch, & Radig, 2008). implementation, useWEKA (Witten & Frank, 2005) learn decision model trees, convert outputC++ function.5.3 Action Model: Execution Duration Predictionvisualize action models learned model trees, example execution durationprediction specific situation depicted Figure 7. model goToPoseaction soccer domain (real robots) learned ne =386 episodes. first ni =20examples per episode used. features used dist, angle to, angle at, v vg ,depicted Figure 6(b).situation depicted Figure 6(b), variables dist, angle to, v, vg set1.5m, 0 , 0m/s, 0m/s respectively. model much general, predictsaccurate values dist, angle to, v, vg ; variables fixed visualizationpurposes only. fixed values, Figure 7 shows predicted time dependsangle at, Cartesian, polar coordinate system.Figure 7: example situation, two graphs time prediction situation varyingangle at, model tree rule one line segments.Cartesian plot five line segments. means model treepartitioned feature space dist=1.5m angle to=0 , v=0m/s, vg =0m/s fiveareas, linear model. two plots, one learned model treerules applies situation displayed. Arrows graphs indicate linearmodel corresponds rule.501fiStulp & Beetz5.3.1 Empirical Evaluationdifferent domains actions action models execution duration learnedlisted first two columns Table 1. subsequent columns list numberepisodes executed gather data training set 34 ne , mean execution duration perepisode t, total duration data gathering training set 34 ne , wellmodels error (MAE) separate test set remaining 41 ne episodes. Notefinal model stored action library trained data ne episodes.next sections, demonstrate action models accurate enough enablesignificant improvement action execution.DomainAction34 neSoccer(real)Soccer(simulated)KitchengoToPosedribbleBallgoToPosedribbleBallgoToPosereachreach290202750750220022001100Arm control(s)6.47.76.27.49.02.62.934 ne(h:mm)0:310:261:181:325:451:380:53MAE(s)0.320.430.220.290.520.100.21Table 1: List actions action model statistics.5.4 Action Model: Failure Predictionsimulated soccer robots also learn predict failures approaching ballgoToPose action. failure occurs robot collides ball desired stateachieved. robots learn predict failures observed experience. acquireexperience, robot executes goToPose thousand times, random initial goalstates. ball always positioned destination state. initial goal statestored, along flag set Fail robot collided ballreaching desired position orientation, Success otherwise. feature spacelearning temporal prediction model goToPose, listed Table 7.learned tree, graphical representation, depicted Figure 8. goalstate represented robot, different areas indicate robot reachposition goToPose without bumping ball first. Remember goToPoseawareness ball all. model simply predicts execution leadscollision not. Intuitively, rules seem correct. coming right,instance, robot always clumsily stumbles ball, long reaching desiredorientation. Approaching ball fine state green area labeled S.5.4.1 Empirical Evaluationevaluate accuracy model, robot executes another thousand runs.resulting confusion matrix depicted Table 2. decision tree predicts collisionscorrectly almost 90% cases. model quite pessimistic, predicts failure502fiRefining Execution Abstract Actions Learned Action ModelsFigure 8: learned decision tree predicts whether collision occur.61%, whereas reality 52%. 10% cases, predicts collisionactually happen. preferable optimistic model, bettersafe sorry. pessimism actually coincidence; caused cost matrixpenalizes incorrect classification Fail Success passeddecision tree (Witten & Frank, 2005).PredictedFailSuccessTotal ObservedObservedFail Success51%10%1%38%52%48%TotalPredicted61%39%89%Table 2: Confusion matrix ball collision prediction.6. Subgoal Refinementcomes elegant motion, robots good reputation. Jagged movements actually typical robots people trying imitate robotsexecuting movements abrupt transitions them. Figure 1(b) demonstratesabrupt transition arises approaching ball dribble certain location.jagged motion inefficient aesthetically displeasing, also revealsfundamental problem inevitably arises way robot controllers actionsdesigned reasoned about.Figure 9(a) depicts abstract action chain scenario, preconditionseffects represented subsets entire state space. Note intersection preconditions effects intermediate subgoal contain many intermediate states, eightalso depicted scenario Figure 9(a). set, variables equal,except angle ball approached. action parameter thereforecalled free.503fiStulp & Beetzdiscussed Section 1, reason abrupt transitions although preconditions effects abstract actions justly abstract away action parametersinfluence high-level selection actions, free parameters might influenceperformance actually executing them. instance, angle approach abstractedaway selecting actions, although obviously influences execution duration.first step subgoal refinement therefore determine free action parameterssequence abstract actions.(a) Abstract action chain subgoal refinement.(b) Abstract action chain optimized subgoal refinement.Figure 9: Computational model subgoal refinement.contrast robot motion, one impressive capabilities animals humansability perform sequences actions efficiently, seamless transitions subsequent actions. assumed typical patterns minimizecertain cost function (Wolpert & Ghahramani, 2000; Schaal & Schweighofer, 2005). So,nature, fluency motion goal itself, rather emergent property time,energy accuracy optimization.Subgoal refinement exploits free action parameters similar way. Since statesintermediate set lead successful execution action sequence, freechoose whichever state want. Execution succeed value free angleapproach. saw Figure 1 values better others, respectexpected execution duration. Therefore, subgoal refinement determines valuesfree action parameters minimize predicted execution duration entire actionsequence. prediction made action models.behavior shown applying subgoal refinement Figure 1(c) better performance, achieving ultimate goal less time. pleasing side-effect exhibitsseamless transitions actions.proceed explaining executable actions instantiatedabstract PDDL plans generated VHPOP, free action parameters ariseprocess. Then, describe subgoal refinement optimizes free action parameters.6.1 Action Instantiation Free Action ParametersCausal links specify action executed previously achieve effect meetsprecondition current action. chain abstract actions represents valid504fiRefining Execution Abstract Actions Learned Action Modelsplan achieve goal, example shown Figure 10. next stepsystem map plans executable actions action library. processalso known operator instantiation (Schmill et al., 2000).Initial 0 : (robot pos1) (ball pos2) (final pos3)Step 2: (approachball pos1 pos2)0-> (ball pos2)0-> (robot pos1)Step 1: (dribbleball pos2 pos3)2-> (atball pos2)Goal:01-> (final pos3)-> (atball pos3)Figure 10: output VHPOP PDDL plan causal links.PDDL plans instantiated executable actions first extracting symbolic actions causal links plan, instantiating symbolic actions one one,listed Algorithm 2. symbolic action, executable action retrievedname (line 5), parameters requested (line 6). next stepdetermine parameter values executable action, considering correspondingsymbolic parameters PDDL plan. correspondence executable action parameter symbolic action parameter determined based indexexecutable action parameter (line 8).symbolic parameters meaning belief state.labels used PDDL plan. However, causal links define predicates labelsmeaning belief state. predicates therefore retrieved (line9), used extract correct values belief state (line 10).Mapping symbolic predicates continuous values done belief state,call made line 10. predicate holds current belief state, casestarts 0 (the initial state considered first action), simply retrievesvalue. 0robot x would return x-coordinate current positionrobot. Predicates hold current state also constrain values.instance, atball predicate restricts translational velocity 0 0.3m/s.predicates impose constraints, default values parameter types returned. instance, values x-coordinates must within field, anglesalways - . several predicates hold, ranges values returncomposed.6.2 Optimizing Free Action ParametersMapping abstract PDDL plans executable actions often leads free action parameters.Humans also high level redundancy actions. Wolpert Ghahramani(2000) note, many ways bring glass water lips,sensible silly. reason typically witness stereotypical sensi505fiStulp & Beetzinput : pddl output (the output VHPOP, see Figure 10 example)output : exe actions (a parameterized sequence executable actions)1 pddl actions = parseActions(pddl output);2 pddl links = parseCausalLinks(pddl output);3456789101112131415// example Figure 10, following holds:// pddl actions = [(approachball pos1 pos2),(dribbleball pos2 pos3)]// pddl links = {pos3=[0center,1atball], pos1=[0robot], pos2=[0ball,2atball]}exe actions = {};foreach pddl action pddl actionsexe action = getAction(pddl action.name) // e.g. exe action = approachBallexe params = exe action.getParameters() // exe params = [x0,y0,...]foreach exe par exe paramspddl par = pddl action.params[exe par.index] ;// e.g. exe par = x0, exe par.index = 0 pddl par = pos1pddl predicates = pddl links[pddl par] ;// e.g. pddl par = pos1, pddl predicates = [0robot]value = beliefState.getValue(exe par.name, pddl predicates) ;exe action.setParameter(exe par, value);endexe actions.add(exe action);end// example Figure 10 Figure 9(a), following holds:// exe actions = [ approachBall(x=0,y=1,=0,v=0, xg=3,yg=1,g=[-,],vg=[0,0.3]),// dribbleBall(x=3,y=1,=[-,],v=[0,0.3], xg=1,yg=3,g=2.6,vg=0) ]return exe actions;Algorithm 2: Action Instantiation. Implementation line 3 Algorithm 1.ble fluent movement redundancy exploited optimize subordinatecriteria (Schaal & Schweighofer, 2005), cost functions (Wolpert & Ghahramani, 2000),energy efficiency variance minimization.also take approach, optimize free parameters action sequencesrespect expected execution duration. optimize action sequence, systemfind values free action parameters overall predicted executionduration sequence minimal. overall performance estimated simplysumming action models actions constitute sequence (Stulp & Beetz,2005).Let us take example action sequence line 14 Algorithm 2example. Figure 11, Figures 1 7 combined. first two polar plots representpredicted execution duration two individual actions different valuesfree angle approach. overall duration computed simply adding two,depicted third polar plot. fastest time execute first approachBallread first polar plot. 2.5s, angle approach 0.0 degrees, indicatedfirst plot. However, total time executing approachBall dribbleBallangle 7.4s, second action takes 4.9s. third plot clearly showsoptimal overall performance. minimum actually 6.5s, angle50 . Beneath polar plots, situation Figure 1 repeated, timepredicted performance action.506fiRefining Execution Abstract Actions Learned Action ModelsFigure 11: Selecting optimal subgoal finding optimum summationaction models chain.reasons clarity, one parameter optimized example, simplyread minima plot. Online however, robots must able determineminimum automatically, possibly several free action parameters resultinghigh-dimensional search spaces. next sections describe two optimization methods.6.2.1 Analytical Optimization Model TreesFigure 7, action model clearly consists line segments 1-dimensional featurespace. general, model trees partition d-dimensional feature space k partitions,represent data partition d-dimensional hyperplane.representation allows analytical minimization model trees. solution ideaminimum hyperplane found quickly determining valuescorners, taking corner minimum value. procedure repeatedk hyperplanes, leads k corner minima. global minimumdetermined choosing minimum minimal corners. novel analytical methodcomplexity O(kd), k number hyperplanes (i.e. numberrules, leaves model tree), number dimensions. Therefore, methodsuffer curse dimensionality, complexity linearly dependsnumber dimensions, instead exponentially.Determining minimum two model trees done first merging modeltrees one, determining minimum new model tree. Unfortunately,cases model trees cannot merged, instance nonlinear mapping action parameters features derived them.cases, analytical optimization summations trees possible, genetic algorithmused. implementation analytical optimization, model tree merging, well507fiStulp & Beetzformalization special cases model trees cannot merged presentedStulp (2007).6.2.2 Optimization Genetic Algorithmcases model trees cannot optimized analytically, use genetic algorithmoptimization. Note problem optimizing action sequence optimizationsimplified straightforward function optimization search spacedetermined free action parameters ranges, target function whoseminimum sought determined model trees, Figure 11. Since model treesmany discontinuities therefore differentiated, chosen genetic algorithm optimization, applied well non-differentiable non-continuousfunctions. Figure 12 depicts optimization genetic algorithm (Goldberg, 1989)integrated overall system. top, instantiated action sequence boundfree action parameters requested optimized.Note parameters labeled identification number (ID). Action parameters ID symbolic parameters derivedsame, see line 8 Algorithm 2. reasons brevity, ID allocation included Algorithm 2. IDs reflect certain parameters different actions alwaysvalue, identical. instance, goal orientation (g ) approachBallequivalent initial orientation () dribbleBall. Therefore share ID 13.Note free action parameter.first step partition action parameters action sequence two sets:one set contains action parameters bound certain value instantiation,set contains free action parameters, along range valuestake. Note action parameters ID stored sets,value.free action parameter represented floating point gene chromosome.number chromosomes population number free parameters multiplied25. chromosomes initial population initialized random valuesrespective ranges. standard genetic algorithm (GA) loop started. loophalts best fitness changed last 50 generations, 500 generationsevaluated. optimal values free parameters also bound actionsequence.chromosome, predicted execution duration determined calling actionmodels fixed values set bound parameters, values freeparameters represented chromosome. fitness proportionate selection, fitnessnon-negative number larger increased fitness. Therefore,chromosome c fitness f computed fc = tmax + tmin tc , tmax tminmaximum minimum execution duration chromosomes respectively.implementation genetic algorithm uses elitarianism (2% best individualspasses next generation unmodified), mutation (on remaining 98%), two-pointcrossover (on 65% individuals), fitness proportionate selection (the chanceselected crossover proportionate individuals fitness). test evaluateGA implementation C++, first applied several optimization benchmarks,508fiRefining Execution Abstract Actions Learned Action ModelsFigure 12: Optimization subgoal refinement genetic algorithm. Implementationline 5 Algorithm 1.De Jongs function, Schwefels function Ackleys Path function. resultsoptimization times reported Koska (2006).subgoal refinement scenarios presented Section 6.3, optimization timeusually small comparison gain performance. complex kitchenscenario, 4 actions 10 action parameters optimized,implementation GA still takes less 0.5s get good result.6.3 Empirical Evaluationsection, introduce different scenarios results applyingsubgoal refinement evaluated. robotic soccer domain, action sequenceoptimized approachBall action, followed dribbleBall action, Figure 1.free action parameters intermediate state angle approachtranslational velocity.evaluate effect subgoal refinement service robotics domain, two scenariostested. first scenario, goal put cup one table another,achieved sequence navigation grasping actions. evaluation episode,topology environment scenario stays same, initial robot position,509fiStulp & Beetztables, cups randomly displaced. Scenario 2 variation Scenario 1,two cups delivered.kitchen scenarios many free action parameters. preconditions usuallybind either navigation (goToPose) manipulation (grip put) actions never(they independent), one action parameter sets always free. Furthermore,distance robot table order grab cup must 40 80cm (asfixed precondition grip). range another free parameter. soccerdomain, velocity orientation waypoints also fixed, free optimizationwell.arm control domain, sequences reaching movements performed.particular task require abstract planning, use VHPOP. demonstration purposes, arm draw first letter first name authorpaper Stulp, Koska, Maldonado, Beetz (2007), chose 4/5 waypoints accordingly.Figure 13(a) shows PowerCube arm, attached B21 robot, drawing F.draw letters, two six degrees freedom arm used. freeaction parameters angular velocities waypoints.Table 3 lists results applying subgoal refinement different domainsscenarios, number actions sequence, n number episodestested. baseline subgoal refinement compared greedy approach,next subgoal optimized respect execution durationcurrent action. case, say horizon h optimization 1. downsidegreedy baseline also depends accuracy action model. However,chose baseline, setting free action parameters zero certainly leadsworse execution times, optimizing manually introduces human bias.ScenarioSoccer (real)Soccer (simulated)Kitchen (scenario 1)Kitchen (scenario 2)Arm control224134-5n10010001001004t110.6s9.8s46.5s91.7s10.6st29.9s9.1s41.5s85.4s10.0st2/16.1%6.6%10.0%6.6%5.7%p0.000.000.000.000.08Table 3: Subgoal refinement results.execution time single action denoted teh , two indices referringhorizon episode. instance t641 refers total execution time64th episode, performed horizon 1. meanP overall execution durationepisodes denoted t1 , computed t1 = n1 ne=1 te1 . Since subgoal refinementoptimizes execution duration current next action, horizon 2.fourth column lists thePmean overall execution duration subgoal refinement t2 ,computed t2 = n1 ne=1 te2 .Pequation t2/1 = n1 ne=1 (1 te2 /te1 ) used compute mean improvementepisodes achieved subgoal refinement. p-value improvement computedusing dependent t-test repeated measures, episode performed twice,510fiRefining Execution Abstract Actions Learned Action Modelswith, without subgoal refinement. significant improvement occurs onedomain.visualize qualitative effect applying subgoal refinement, resultsarm control domain depicted Figure 13(b). angular velocities set zero(upper row) optimized subgoal refinement (lower row). axes representangles two joints. figure demonstrates trajectories smoothersubgoal refinement: arm mostly draws one long stroke, rather discernible linesegments. Since arm control domain mainly included visualization purposes,four test runs. reason overall improvement significant(p > 0.05).(a) B21 robot drawing FPowerCube arm.(b) Drawing letters without (upper row) (lower row)subgoal refinement. refinement, letters drawn fastersmoother.Figure 13: Arm control experiment.Although optimizing execution duration leads smoother motion domain,smooth human arm motion arises variability rather time optimization (Harris &Wolpert, 1998; Simmons & Demiris, 2004). article, main goal explainmodel human motion, rather demonstrate effects optimizing sequencesactions.6.3.1 Sequences Actionsfar, seen optimization horizons h = 1 (greedy) h = 2. standardapproach h = 2 easily extended, subgoal refinement optimizesexecution duration next h > 2 actions. higher horizon h, subgoalrefinement preparing actions future.evaluate effect optimizing two actions, sequences fouractions optimized using subgoal refinement different horizons. Two scenariosused: simulated soccer scenario robot traverse four waypointsgoToPose action, two kitchen scenarios. action execution, subgoalssubsequent actions recomputed horizon, less numberremaining actions smaller h. results summarized Table 4. firstrow represents baseline greedy approach h = 1, second row representsresults reported far h = 2. next two rows list results optimizing 3 4511fiStulp & Beetzaction execution durations. Again, reported times represent execution durationentire action sequence, averaged 100 episodes.horizonh=1h=2h=3h=4P22.720.320.220.2SoccerImp. p-value10.6%0.7%0.2%0.0000.0010.053Kitchen (Scen.1)PImp. p-value46.541.5 10.0%0.00040.61.5%0.041-Kitchen (Scen.2)PImp. p-value91.785.4 6.6%0.04185.3 0.1%0.498-Table 4: Effect subgoal refinement horizon h performance improvement.three scenarios, substantial improvement h = 1 h = 2,h = 2 h = 3 improvement marginal insignificant. executing actionscenarios, apparently beneficial prepare next action, action that. insight also main motivation behind receding horizon control,described Section 3.2. However, also important differences RHCsubgoal refinement. First all, optimal control plans optimizes motor commandsfixed duration, whereas subgoal refinement durative actions varying duration, higher level temporal abstraction. Furthermore, RHC optimizes firstHp motor commands, whereas subgoal refinement optimizes action parameters partiallyfixed subgoals, concerned actual motor commands needed reachsubgoals. planner therefore fix general structure plan, rather committing first steps. Finally, optimal control assumes precise analyticmodels actions systems behavior available. many robotic systems,well humans, hold. However, lack analytic models keepus acquiring models experience. learning action models, system alsoflexible enough acquire action models changing actions, actions modelacquired analysis.6.3.2 Predicting Performance Decreasecases subgoal refinement effect. ball approachscenario instance, robot, ball final destination perfectly aligned,much subgoal refinement, greedy approach already deliversoptimal angle approach: straight towards ball. contrary, refining subgoalscases might put unnecessary constraints execution. Due inaccuraciesaction models optimization techniques, sometimes even casegreedy approach slightly better subgoal refinement.evaluate effects, 1000 episodes executed simulation h = 1h = 2. Although overall improvement h = 2 6.6% (see Table 3), 160 1000episodes actually lead increased execution duration. episodes labeled -,remaining +. trained decision tree predict nominal value.tree yields four simple rules predict performance difference correctly 87%given cases. learned decision tree essentially action model too. Rather512fiRefining Execution Abstract Actions Learned Action Modelspredicting outcome individual action, predicts outcome applying actionmodels actions.performed another 1000 test episodes, described above, applied subgoalrefinement decision tree predicted applying would yield better performance.overall improvement raised 6.6% 8.6%.7. Subgoal Assertionpractice, learning actions hardly starts scratch, knowledge previouslylearned actions transfered novel task contexts. humans robots instance, approaching ball similar navigating without considering ball.abstract level, involve going state another field, Figure 14(a), implemented execute efficiently fast possible.However, also slight differences two tasks. approachingball important bump achieving desired state, depictedFigure 14(b).(a) Original scenario.(b) Refined goal, learnedcondition refinement.(c) Subgoal assertion based condition refinement.Figure 14: Computational model condition refinement subgoal assertion.Figure 14 illustrates small differences. first scenario, ball,effects goToPose actions satisfy goal, actions achieve goal.Figure 14(b) basically scenario, added requirement robotmust possession ball goal state. goToPose action usedachieve goals. Since goToPose aware ball, often collides ballachieving desired state. new goal approaching ball essentiallyrefined subset former goal simply navigating there. executing goToPose,robot left succeeds approaching ball, robot right not,bumps ball beforehand. case effects goToPoselonger satisfy refined goal, depicted scenario.effects goToPose partitioned subset satisfy newrefined goal, subset not. represented blue (S) red (F)respectively. Analogously, preconditions partitioned subset Successleads final state subset effects satisfy refined goal,subset Fail case. effects, consequently,preconditions action refined new task, call condition refinement.513fiStulp & Beetzrefined precondition novel goal known, easy determineparticular initial state lead successful execution not. does, actionexecuted is. instance, robot left simply execute goToPose action,refined precondition. shall see, goToPose action usedapproach ball successfully almost half time.However, robot right cannot simply use goToPose approach ball.robot needs novel action, e.g. approachBall, enables gostates Fail refined goal. it? Instead, goToPose action usedagain, take robot Fail subset Success subset. done,goToPose action succeed approaching ball executed.key reuse therefore able predict action fail,succeed novel task. predicted succeed, action executedis. action fail, another action executed beforehand,robot ends state action succeed, depicted Figure 14(c).intermediate state actions new subgoal. approach therefore calledsubgoal assertion.input : exe actions (a sequence (partially) instantiated actions)output : exe actions2 (a sequence (partially) instantiated actions asserted subgoals)1 exe actions2 = {};2 foreach exe action exe actions3switch exe action.name4case goToPoseexe actions2.add (exe action) // Subgoal assertion never needed action56end7case approachBall89101112131415161718192021// Get parameters related states.// Uses indexing scheme lines 6-8 Algorithm 2..exe params0 = exe action.getParameters(0);exe params1 = exe action.getParameters(1);goToPose.approachBallSuccess( exe params0, exe params1)// goToPose job, subgoal assertion neededexe actions2.add (new goToPose(exe params0, exe params1));else// exe params2 set default ranges action parameters goToPose.// lines 10-11 Algorithm 2.exe params2 = ...;exe actions2.add (new goToPose(exe params0, exe params2));exe actions2.add (new goToPose(exe params2, exe params1));endend...endendreturn exe actions2;Algorithm 3: Implementation line 4 Algorithm 1.514fiRefining Execution Abstract Actions Learned Action Models7.1 Integration Overall SystemSubgoal assertion takes sequence actions, returns sequence assertedsubgoal needed assure successful execution, listed Algorithm 3. mainloop goes actions, leaves goToPose actions untouched. approachBallimplementation itself, replaced goToPose actions. one goToPose neededpredicted succeed approaching ball. case initial stateSuccess subset Figure 14(b). Determining subsets manually difficult task,due complex interactions dynamics shape robot, wellspecific characteristics action. Therefore, subsets learned decisiontree, described Section 5.4.success predicted, one goToPose executed is, parametersapproachBall action. predicted fail, subgoal asserted (exe params2),inserted two goToPose actions. action parameters exe params2 initiallyreceive default ranges. parameters exe params2 free, optimizedsubgoal refinement. immediately follows subgoal assertion, listed Algorithm 1.ensures values exe params2 minimize predicted execution duration,transition two goToPose actions smooth.One issue remains open. intermediate goal actions must lie withinSuccess subset Figure 14, ball approach task position greenarea left Figure 8. requirement puts constraints values exe params2.must ensured optimization process subgoal refinement considersstates Success subset refined precondition second goToPose action.Therefore, action model action modified returns INVALIDflag states. approach chosen requires little modificationoptimization module. Chromosomes lead invalid value simply receive lowfitness.1 goToPose.executionDurationApproachBall (x,y,,v,xg ,yg ,g ,vg ) {2goToPose.approachBallSuccess ( x,y,,v,xg ,yg ,g ,vg )3return goToPose.executionDuration (x,y,,v,xg ,yg ,g ,vg );4else5return INVALID;6end7 }Algorithm 4: Modified goToPose action model approaching ball.Analogously Figure 11, predicted execution durations two actions, wellsummation depicted Figure 15. Invalid values rendered. secondgraph depicts function described Algorithm 4. Note due removal invalidvalues, shape functions ground plane last two graphs correspondsFigure 8 16(a).Figure 16(a), three instances problem depicted. Since robotleft area collision predicted, simply executes goToPose, withoutasserting subgoal. model predicts two robots collide ball515fiStulp & BeetzFigure 15: Search space subgoal refinement subgoal assertion.executing goToPose, subgoal asserted. subgoals, determined subgoalrefinement, depicted well.FailPredictedFailSuccess(a) Three examples. Subgoal assertion applied two them.2%ObservedSuccess(52%50%)60%1%3%(10%+50%)37%97%62%38%(b) Mean results 100 episodes. Almost predicted failures(50% 52%) transformed yield successful execution.Figure 16: Results subgoal assertion.7.2 Empirical Evaluationevaluate automatic subgoal assertion, hundred random ball approaches executedsimulation, subgoal assertion, without. results summarizedTable 16(b). Without assertion, results similar results reported Table 2.collision correctly predicted approximately half time: 52% hundredepisodes. Subgoal assertion applied cases, almost always successful: 50%episodes predicted fail executed successfully. 2% episodesstill collision, despite subgoal assertion. subgoal assertion appliedSuccess predicted, change lower prediction row. Consciously choosingapply subgoal assertion applying equivalent.Subgoal assertion applied unnecessarily 10% episodes, means subgoal assertion applied, even though original sequence would already successful.However, execution subgoal assertion consequent subgoal refinement significant 8% slower executing one goToPose action. performance losscases seems acceptable cost compared pay-off dramatic increasenumber successful task completions.516fiRefining Execution Abstract Actions Learned Action ModelsSummarizing: subgoal assertion necessary, usually applied. subgoalintroduced half time, raises successful task completion 47 97%. Infrequently, subgoals introduced inappropriately, leads small loss performanceterms execution duration.8. Conclusion OutlookDurative actions provide conceptual abstraction reasoned eitherdesigner action selection design, or, abstraction explicitly codedcontroller, action selection system itself. Action abstractions partially achieveabstraction ignoring action parameters. Although parameters relevantaction abstract level, often relevant performance successexecuting plan.robots learn predict effects performance actions, useknowledge optimize behavior subgoal refinement, avoid failuressubgoal assertion. empirical evaluations verify leads efficient, robust,effective behavior. believe important contributions towards bridginggap abstract planning systems robot action execution.multi-robot scenarios robotic soccer, robots provided actionmodels teammates, enabling robot reason actions others.shown enables robots implicitly coordinate actions others, withoutresort utility communication (Stulp et al., 2006). also successfullyapplied approach heterogeneous team robots, robots two differentresearch groups (Isik, Stulp, Mayer, & Utz, 2006).Also, preliminary results showing even accurate models learneddata gathered online operation time (Kirsch & Beetz, 2007). Extendedoperation times enable robot gather training data, actions alsocalled parameterizations typical domain context robotused. contrast current method, generates action parameterizationsrandomly choosing possible parameterizations. Given amountdata, model generalize possible parameterizations tend lessaccurate model subset parameterizations.main assumption underlying article human-specified knowledgerobot-learned knowledge complement well robot controllers. exemplary recent winners two well-known robotic benchmarks, RoboCup mid-sizeleague (Gabel, Hafner, Lange, Lauer, & Riedmiller, 2006) DARPA challenge (Thrunet al., 2006), emphasize success could achieved combinationmanual coding experience-based learning. specifically, believe ideallyhuman designers specify action abstractions offline. task execution, robot automatically optimize aspects actions relevantexecution learned action models.Future work includes learning several models different performance measures,optimizing several performance measures simultaneously. instance, energy consumptionanother important performance measure autonomous mobile robots. specifying objective functions consist combinations energy consumption execution517fiStulp & Beetzduration, optimized. weighting individual performance functions differently, function optimized customized specific scenarios. instance,mid-size league robotic soccer, short operation time 15 minutes, speed farimportant energy consumption. service robotics way around.Also, intend evaluate use learning algorithms predicting failureactions, instance Neural Networks (Buck & Riedmiller, 2000) Support Vector Machines (Hart, Ou, Sweeney, & Grupen, 2006). learning algorithms predict actionfailures accurately, even better results expected subgoal refinementsubgoal assertion.Acknowledgmentswould like thank Andreas Fedrizzi anonymous reviewers valuable remarks suggestions. research described article partly fundedGerman Research Foundation (DFG) SPP-1125, Cooperating Teams MobileRobots Dynamic Environments, also CoTeSys cluster excellence (Cognition Technical Systems, http://www.cotesys.org), part Excellence InitiativeDFG.AppendixRobotState EstimationSoccer(real)Soccer(simulated)KitchenProbabilistic state estimation based cameraProbabilistic state estimation based cameraGround truth, noise based noise modelGround truth field view, noiseGround truthGround truthGround truth field viewArm controlJoint angles read directly motor encodersBelief StateMotor comm.x, y, , v,xball , yballx, y, , v,xball , yballx, y, , v,ax, ay, az[xo , yo , zo ]n, , b , bv,v,v,ax, ay, aza, bTable 5: State variables belief state, state estimation process used acquire them,motor command domain. x, y, , v dynamic pose robot body,ax, ay, az relative position arm robot body, [xo , yo , zo ]n absolute positions n objects, v, translational/rotational velocities, current sentarm joint motor.518fiRefining Execution Abstract Actions Learned Action ModelsRobotActionSoccergoToPosedribbleBallgoToPosereachreachKitchenArm controlAction ParametersCurrentGoalx, y, , vxg , yg , g , vgx, y, , v, xball , yballxg , yg , g , vgx, y, , vxg , yg , g , vgx, y, z, ax, ay, az xg , yg , zg , axg , ayg , azg, , b , bga , ga , gb , gbTable 6: List actions used application domains. list actions might shorterexpected. instance, doubtful robots could play soccernavigate certain pose. aim article demonstrateactions reused customized perform well varyingtask contexts.RobotActionFeaturesSoccergoToPosedribbleBallKitchengoToPosepv, dist = (x xg )2 + (y yg )2 ,angle = |angle tosigned |,angle = sgn(angle tosigned )norm(g atan2(yg y, xg x))vg , dist, angle to, angle at,angle =p|norm(g )|distxyz =p (x xg )2 + (y yg )2 + (z zg )2distxy = (x xg )2 + (y yg )2 , distxz , distyz ,anglexyq= atan2(yg y, xg x), anglexz , angleyzreachArm controlreachdist =(a ga )2 + (b gb )2 ,angle1 = norm(atan2(gb b , ga ) atan2(b , )),b b , ) + atan2(b , ))anglegg gq2 = norm(atan2(q g2222v = + b , vg = ga + gbnorm(a): adds subtracts 2 range [, ]angle tosigned = norm(atan2(yg y, xg x) )Table 7: feature spaces used learn action modelsReferencesArkin, R. (1998). Behavior based Robotics. MIT Press, Cambridge, Ma.Astrom, K. J., & Murray, R. M. (2008). Feedback Systems: Introduction ScientistsEngineers, chap. Supplement optimization-based control. Princeton University Press.Barto, A., & Mahadevan, S. (2003). Recent advances hierarchical reinforcement learning. Discreteevent systems, 13 (1-2), 4177.Beetz, M., & Belker, T. (2000). XFRMLearn - system learning structured reactive navigationplans. Proceedings 8th International Symposium Intelligent Robotic Systems.519fiStulp & BeetzBeetz, M. (2000). Concurrent Reactive Plans: Anticipating Forestalling Execution Failures, Vol.LNAI 1772 Lecture Notes Artificial Intelligence. Springer Publishers.Beetz, M. (2001). Structured Reactive Controllers. Journal Autonomous Agents Multi-AgentSystems. Special Issue: Best Papers International Conference Autonomous Agents99, 4, 2555.Beetz, M., Schmitt, T., Hanek, R., Buck, S., Stulp, F., Schroter, D., & Radig, B. (2004). AGILOrobot soccer team experience-based learning probabilistic reasoning autonomous robotcontrol. Autonomous Robots, 17 (1), 5577.Belker, T. (2004). Plan Projection, Execution, Learning Mobile Robot Control. Ph.D. thesis,Department Applied Computer Science, University Bonn.Bellingham, J., Richards, A., & How, J. P. (2002). Receding horizon control autonomous aerialvehicles. Proceedings 2002 American Control Conference, Vol. 5, pp. 37413746.Benson, S. (1995). Inductive learning reactive action models. International ConferenceMachine Learning (ICML), pp. 4754.Bertram, D., Kuffner, J., Dillmann, R., & Asfour, T. (2006). integrated approach inversekinematics path planning redundant manipulators. Proceedings IEEE International Conference Robotics Automation (ICRA), pp. 18741879.Bouguerra, A., & Karlsson, L. (2005). Symbolic probabilistic-conditional plans execution mobilerobot. IJCAI-05 Workshop: Reasoning Uncertainty Robotics (RUR-05).Brock, O., & Khatib, O. (1999). Elastic Strips: framework integrated planning execution.Proceedings International Symposium Experimental Robotics, pp. 329338.Buck, S., Beetz, M., & Schmitt, T. (2002a). M-ROSE: Multi Robot Simulation EnvironmentLearning Cooperative Behavior. Asama, H., Arai, T., Fukuda, T., & Hasegawa, T. (Eds.),Distributed Autonomous Robotic Systems 5, Lecture Notes Artificial Intelligence, LNAI.Springer-Verlag.Buck, S., Beetz, M., & Schmitt, T. (2002b). Reliable Multi Robot Coordination Using MinimalCommunication Neural Prediction. Beetz, M., Hertzberg, J., Ghallab, M., & Pollack,M. (Eds.), Advances Plan-based Control Autonomous Robots. Selected ContributionsDagstuhl Seminar Plan-based Control Robotic Agents, Lecture Notes ArtificialIntelligence. Springer.Buck, S., & Riedmiller, M. (2000). Learning situation dependent success rates actionsRoboCup scenario. Pacific Rim International Conference Artificial Intelligence, p. 809.Cambon, S., Gravot, F., & Alami, R. (2004). robot task planner merges symbolicgeometric reasoning.. Proceedings 16th European Conference Artificial Intelligence(ECAI), pp. 895899.Clement, B. J., Durfee, E. H., & Barrett, A. C. (2007). Abstract reasoning planning coordination. Journal Artificial Intelligence Research, 28, 453515.Coradeschi, S., & Saffiotti, A. (2001). Perceptual anchoring symbols action. ProceedingsInternational Joint Conference Artificial Intelligence (IJCAI), pp. 407416.Dean, T., & Wellmann, M. (1991). Planning Control. Morgan Kaufmann Publishers.Dearden, A., & Demiris, Y. (2005). Learning forward models robotics. ProceedingsNineteenth International Joint Conference Artificial Intelligence (IJCAI), pp. 14401445.Dietterich, T. G. (2000). Hierarchical reinforcement learning MAXQ value function decomposition. Journal Artificial Intelligence Research, 13, 227303.Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planningdomains.. Journal Artificial Intelligence Research, 20, 61124.520fiRefining Execution Abstract Actions Learned Action ModelsFox, M., Ghallab, M., Infantes, G., & Long, D. (2006). Robot introspection learned hiddenmarkov models. Artificial Intelligence, 170 (2), 59113.Gabel, T., Hafner, R., Lange, S., Lauer, M., & Riedmiller, M. (2006). Bridging gap: LearningRoboCup simulation midsize league. Proceedings 7th Portuguese ConferenceAutomatic Control.Gerkey, B., Vaughan, R. T., & Howard, A. (2003). Player/Stage Project: Tools multirobot distributed sensor systems. Proceedings 11th International ConferenceAdvanced Robotics (ICAR), pp. 317323.Ginsberg, M. L. (1989). Universal planning: (almost) universally bad idea. AI Magazine, 10 (4),4044.Goldberg, D. E. (1989). Genetic Algorithms Search, Optimization Machine Learning. KluwerAcademic Publishers.Haigh, K. Z. (1998). Situation-Dependent Learning Interleaved Planning Robot Execution.Ph.D. thesis, School Computer Science, Carnegie Mellon University.Harris, C. M., & Wolpert, D. M. (1998). Signal-dependent noise determines motor planning. Nature,394 (20), 780784.Hart, S., Ou, S., Sweeney, J., & Grupen, R. (2006). framework learning declarative structure.Workshop Manipulation Human Environments, Robotics: Science Systems.Haruno, M., Wolpert, D. M., & Kawato, M. (2001). MOSAIC model sensorimotor learningcontrol. Neural Computation, 13, 22012220.Haruno, M., Wolpert, D. M., & Kawato, M. (1999). Multiple paired forward-inverse modelshuman motor learning control. Proceedings 1998 conference Advancesneural information processing systems II, pp. 3137, Cambridge, MA, USA. MIT Press.Hoffmann, J., & Duffert, U. (2004). Frequency space representation transitions quadrupedrobot gaits. Proceedings 27th Australasian computer science conference (ACSC), pp.275278.Infantes, G., Ingrand, F., & Ghallab, M. (2006). Learning behavior models robot executioncontrol. Proceedings 17th European Conference Artificial Intelligence (ECAI),pp. 678682.Isik, M., Stulp, F., Mayer, G., & Utz, H. (2006). Coordination without negotiation teamsheterogeneous robots. Proceedings RoboCup Symposium, pp. 355362.Jacobs, R. A., & Jordan, M. I. (1993). Learning piecewise control strategies modular neuralnetwork. IEEE Transactions Systems, Man Cybernetics, 23 (3), 337345.Jaeger, H., & Christaller, T. (1998). Dual dynamics: Designing behavior systems autonomousrobots. Artificial Life Robotics, 2 (3), 108112.Jordan, M. I., & Rumelhart, D. E. (1992). Forward models: Supervised learning distal teacher.Cognitive Science, 16, 307354.Kirsch, A., & Beetz, M. (2007). Training job collecting experience hierarchical hybridautomata. Hertzberg, J., Beetz, M., & Englert, R. (Eds.), Proceedings 30th GermanConference Artificial Intelligence (KI-2007), pp. 473476.Kitano, H., Asada, M., Kuniyoshi, Y., Noda, I., & Osawa, E. (1997). RoboCup: robot worldcup initiative. Proceedings first international conference autonomous agents(AGENTS), pp. 340347.Kollar, T., & Roy, N. (2006). Using reinforcement learning improve exploration trajectories error minimization. Proceedings International Conference Robotics Automation(ICRA), pp. 33383343.521fiStulp & BeetzKoska, W. (2006). Optimizing autonomous service robot plans tuning unbound action parameters.Masters thesis, Technische Universiat Munchen.Kovar, L., & Gleicher, M. (2003). Flexible automatic motion blending registration curves.Proceedings 2003 ACM SIGGRAPH/Eurographics symposium computer animation(SCA), pp. 214224.Muller, A., & Beetz, M. (2006). Designing implementing plan library simulated householdrobot. Beetz, M., Rajan, K., Thielscher, M., & Rusu, R. B. (Eds.), Cognitive Robotics:Papers AAAI Workshop, Technical Report WS-06-03, pp. 119128, Menlo Park,California. American Association Artificial Intelligence.Nakanishi, J., Cory, R., Mistry, M., Peters, J., & Schaal, S. (2005). Comparative experiments taskspace control redundancy resolution. IEEE International Conference IntelligentRobots Systems (IROS), pp. 39013908.Nilsson, N. J. (1984). Shakey robot. Tech. rep. 323, AI Center, SRI International.Nilsson, N. J. (1994). Teleo-reactive programs agent control. Journal Artificial IntelligenceResearch, 1, 139158.Perlin, K. (1995). Real time responsive animation personality. IEEE Transactions Visualization Computer Graphics, 1 (1), 515.Quinlan, R. (1992). Learning continuous classes. Adams, A., & Sterling, L. (Eds.), Proceedings 5th Australian Joint Conference Artificial Intelligence, pp. 343348.Russell, S., & Norvig, P. (2003). Artificial Intelligence - Modern Approach. Prentice Hall, UpperSaddle River, New Jersey.Ryan, M., & Pendrith, M. (1998). RL-TOPs: architecture modularity re-use reinforcement learning. Proceedings 15th International Conference Machine Learning (ICML),pp. 481487.Saffiotti, A., Ruspini, E. H., & Konolige, K. (1993). Blending reactivity goal-directednessfuzzy controller. Proceedings IEEE International Conference Fuzzy Systems, pp.134139, San Francisco, California. IEEE Press.Schaal, S., & Schweighofer, N. (2005). Computational motor control humans robots. CurrentOpinion Neurobiology, 15, 675682.Schmill, M. D., Oates, T., & Cohen, P. R. (2000). Learning planning operators real-world,partially observable environments. Proceedings 5th International ConferenceArtificial Intelligence Planning Systems (ICAPS), pp. 246253.Shahaf, D., & Amir, E. (2006). Learning partially observable action schemas.. Proceedings21st National Conference Artificial Intelligence (AAAI).Simmons, G., & Demiris, Y. (2004). Biologically inspired optimal robot arm control signaldependent noise. Proceedings IEEE International Conference Intelligent RobotsSystems (IROS), pp. 491496.Smith, D., Frank, J., & Jonsson, A. (2000). Bridging gap planning scheduling.Knowledge Engineering Review, 15 (1), 4783.Stulp, F. (2007). Tailoring Robot Actions Task Contexts using Action Models. Ph.D. thesis,Technische Universitat Munchen.Stulp, F., & Beetz, M. (2005). Optimized execution action chains using learned performancemodels abstract actions. Proceedings Nineteenth International Joint ConferenceArtificial Intelligence (IJCAI).Stulp, F., Isik, M., & Beetz, M. (2006). Implicit coordination robotic teams using learned prediction models. Proceedings IEEE International Conference Robotics Automation(ICRA), pp. 13301335.522fiRefining Execution Abstract Actions Learned Action ModelsStulp, F., Koska, W., Maldonado, A., & Beetz, M. (2007). Seamless execution action sequences.Proceedings IEEE International Conference Robotics Automation (ICRA),pp. 36873692.Sussman, G. J. (1973). computational model skill acquisition. Ph.D. thesis, MassachusettsInstitute Technology.Thrun, S. et al. (2006). Stanley, robot tFhe DARPA grand challenge. Journal FieldRobotics, 23 (9), 661692.Todorov, E., Li, W., & Pan, X. (2005). task parameters motor synergies: hierarchicalframework approximately optimal control redundant manipulators. Journal RoboticSystems, 22 (11), 691710.Uno, Y., Wolpert, D. M., Kawato, M., & Suzuki, R. (1989). Formation control optimaltrajectory human multijoint arm movement - minimum torque-change model. BiologicalCybernetics, 61 (2), 89101.Utz, H., Kraetzschmar, G., Mayer, G., & Palm, G. (2005). Hierarchical behavior organization.Proceedings 2005 International Conference Intelligent Robots Systems (IROS),pp. 25982605.Wimmer, M., Stulp, F., Pietzsch, S., & Radig, B. (2008). Learning local objective functions robustface model fitting. IEEE Transactions Pattern Analysis Machine Intelligence (PAMI),30 (8). appear.Witten, I. H., & Frank, E. (2005). Data Mining: Practical machine learning tools techniques(2nd edition). Morgan Kaufmann, San Francisco.Wolpert, D., & Ghahramani, Z. (2000). Computational principles movement neuroscience. NatureNeuroscience Supplement, 3, 12121217.Wolpert, D. M., & Flanagan, J. (2001). Motor prediction. Current Biology, 11 (18), 729732.Younes, H. L. S., & Simmons, R. G. (2003). VHPOP: Versatile heuristic partial order planner.Journal Artificial Intelligence Research, 20, 405430.523fiJournal Artificial Intelligence Research 32 (2008) 355-384Submitted 10/07; published 05/08Spectrum Variable-Random TreesFei Tony LiuKai Ming TingTONY. LIU @ INFOTECH . MONASH . EDU . AUKAIMING . TING @ INFOTECH . MONASH . EDU . AUGippsland School Information Technology,Monash University, AustraliaYang YuZhi-Hua ZhouYUY @ LAMDA . NJU . EDU . CNZHOUZH @ LAMDA . NJU . EDU . CNNational Key Laboratory Novel Software Technology,Nanjing University, ChinaAbstractpaper, show continuous spectrum randomisation exists, existing tree randomisations operating around two ends spectrum. leaveshuge part spectrum largely unexplored. propose base learner VR-Tree generatestrees variable-randomness. VR-Trees able span conventional deterministictrees complete-random trees using probabilistic parameter. Using VR-Trees basemodels, explore entire spectrum randomised ensembles, together Bagging Random Subspace. discover two halves spectrum distinct characteristics;understanding allows us propose new approach building better decisiontree ensembles. name approach Coalescence, coalesces number pointsrandom-half spectrum. Coalescence acts committee experts cater unforeseeable conditions presented training data. Coalescence found perform better singleoperating point spectrum, without need tune specific level randomness.empirical study, Coalescence ranks top among benchmarking ensemble methods includingRandom Forests, Random Subspace C5 Boosting; Coalescence significantly betterBagging Max-Diverse Ensemble among methods comparison. AlthoughCoalescence significantly better Random Forests, identified conditionsone perform better other.1. Introductionbuilding ensemble-classifiers, randomisation plays important role forming diverse models generated deterministic algorithms. use ensemble methods, diversemodels aggregated improve generalisation capability resulting classifiers.Traditionally, ensemble methods based deterministic algorithms randomisations injected produce diverse variants. Representatives Bagging (Breiman, 1996a), RandomForests (Breiman, 2001), Randomised C4.5 (Dietterich, 2000) Random Subspace (Ho, 1998).Recently, completely random approach (Fan, Wang, Yu, & Ma, 2003; Fan, 2004; Liu, Ting,& Fan, 2005) proposed using trees generated without deterministic heuristic;approach represents departure traditional approaches. paper, showcomplete-random approach traditional approaches used two extremesform continuous spectrum randomisation; better predictive accuracy often foundwithin spectrum.c2008AI Access Foundation. rights reserved.fiL IU , ING , U , & Z HOUpaper, propose novel algorithm capable generating range models,end-to-end continuously completely random purely deterministic. striking factthough tree-node created either randomly deterministically, resulting randomnessspan completely random purely deterministic without modification ensemble method. algorithm enables us explore whole spectrum two extremesshow that, new algorithm easily incorporated existing ensemble methods,Bagging Random Subspace. Together generate ensembles different degreesrandomness, largely unexplored now.reveal existing random ensemble methods Bagging RandomSubspace focus deterministic-end spectrum, ignore major part spectrum.show Bagging, Random Subspace simple complete-random trees find bettercounterparts inside spectrum.known way measure priori level randomness required givenproblem, analyse spectrum discover two halves spectrumdistinctive characteristics. new understanding, new ensemble approach proposedpaper, coalesces number points spectrum form final ensembles.Empirically, find new approach performs better single point spectrumacross wide range data sets. new approach off-the-shelf solution, provideshigh level accuracy without need knowing tuning level randomness required.paper presented follows. brief overview existing decision tree randomisationmethods provided Section 2. serves primer decision tree randomisation. algorithm generate variable-random-trees presented Section 3. Section 4, differentensemble methods used experiment introduced, followed Section 5, presentscomprehensive empirical evaluation spectrum well proposed ensemble approach.Section 6.1 details key differences randomisation framework Random Forestsproposed framework variable-randomness. related work provided Section 6.2,conclude last section.2. Randomisation Methods Decision TreesMany randomisation methods proposed produce diverse decision trees ensembleclassifiers. section, general introduction decision tree randomisation, giveoverview ways applied. following list decision tree randomisationmeant exhaustive, purpose list demonstrate mechanism side effectsrandomisation methods, guides us designing better approaches.conventional decision tree algorithm, one deterministic model produced giventraining set. Randomisation helps produce multiple variants deterministic model fulfilrequirement ensemble learning. common characteristic popular methodsheuristic used every tree node, often restricts possible range randomnessreduces impact performance.literature, proposed randomisation methods grouped three categories, depending dimension applied. first category randomiseinstance dimension. includes (i) Bagging (Breiman, 1996a) Wagging (Bauer & Kohavi, 1999), generate different sets training examples random sampling assigning randomly generated probability distributions given training set; (ii) Output flipping356fiS PECTRUMVARIABLE -R ANDOM REES(Breiman, 2000) classes training examples flipped randomly accordingratio; (iii) Adding random examples (Melville & Mooney, 2003) diverse classifiersconstructed using additional artificial training examples. conventional decision tree algorithmused generate model random sample training examples. type (i), usercontrol degree randomisation applied; types (ii) (iii), randomnessexpense data integrity.second category randomise feature dimension randomly selecting subsetfeatures generating model. representative method Random Subspace(Ho, 1998) 50% features randomly selected produce models ensemble.Random Subspace designed adjust level randomness, default setting mentionedcommonly used.third category randomise test-selection decision node tree growing process. Since meant produce variants deterministic model, randomisationusually applied small degree node maintaining key deterministic characteristic. Examples category Randomised C4.5 (Dietterich, 2000) Random Forests(Breiman, 2001). reported Breiman, performance Random Forests significantlyimpacted different values parameter used.methods mentioned above, deterministic models common starting point.Randomisations injected produce diverse variants deterministic models.contrary, totally different approach start complete-random models, example,Random Decision Trees (Fan et al., 2003) Max-Diverse Ensemble (Liu et al., 2005).distinction two starting points inclusion deterministic heuristic.method uses deterministic weakened heuristic node, starting pointdeterministic models. two starting points seem mutually exclusive, however, provideway connect order maximize possible range randomness and, turn, predictiveperformance gain.paper, show largely unexplored set randomised models foundextremes deterministic complete-random models. Random Forests providesmean adjust randomness, degrees randomness constrained number featuresmandatory use deterministic heuristic node. Details limitationdiscussed Section 6.next section, propose new algorithm constructs trees controllable randomisation test-selection. allows us explore whole spectrum variable-random trees.3. Trees Variable Randomnessname tree VR-Tree generated using random test-selection nodes.section, first describe process random test-selection mechanisminduces trees controllable mix random deterministic test-selections.framework conventional tree building algorithms, random test-selection useddirect replacement deterministic test-selection. depicted Algorithm 1. First, random test-selection randomly picks feature list available features form decisionnode. Then, nominal feature possible values form branches continuous-valuedfeature random cut-point form 2 branches. random split-point selection procedure357fiL IU , ING , U , & Z HOUdescribed Algorithm 2. random test-selection becomes alternative deterministictest-selection mechanism create variable-randomness.Algorithm 1: VR-Tree(Dt , Q, ) - Building Variable-Random TreeInput: Dt - Training set, Q - Feature set, - probability using deterministic test-selectionOutput: node - tree nodeclasses Dt Q empty |Dt | < nmin/* nminminimum number instances required splitallowed. */return leaf class frequencyelselet r randomly generated value, 0 < r 1r/* Deterministic Test-Selection. */node DeterministicT estSelection(Dt , Q)else/* Random Test-Selection. */randomly select Qconstruct node test labelcontinuous-valued feature/* Handlingcontinuous-valued feature. */node.splitpoint RandomSplit(, Dt )D1 f ilter(Dt , > node.splitpoint)D2 f ilter(Dt , node.splitpoint)node.branch(1) VR-Tree(D1 , Q, )node.branch(2) VR-Tree(D2 , Q, )else/* Handling discrete feature. */let {v1 ...vm } possible values/* m-ary split. */Di f ilter(Dt , == vi )node.branch(i) VR-Tree(Di , Q , )return nodeAlgorithm 2: RandomSplit(, Dt ) - Random split point selectionInput: - continuous-valued feature, Dt - training dataOutput: split pointr1 randomly select value Dtr2 randomly select value Dtr1 == r2r2 randomly select valuereturn mid point r1 r2generate variable-randomness, test-selection process split two stages node.first stage decides test-selection use, either random deterministic test-selection.second stage proceeds selected test-selection produce actual test node.parameter provided control probability choosing deterministic test-selection358fiS PECTRUMVARIABLE -R ANDOM REESrandom one, 0 1. also approximates percentage deterministic nodesgenerated trees. Note setting = 1, procedure generates trees identicalconventional decision trees; setting = 0, generates complete-random trees. proceduremechanism found Algorithm 1.next section, introduced three ensemble methods used experiment basedVR-Trees.4. Ensemble MethodsUsing VR-Tree base learner, explore three ensemble methods employedinvestigation. listed follows:Aggregating, trees generated training data using full setfeatures.Subspacing, trees generated subsets randomly selected features. parameter used determine percentage features used.Bagging, trees generated bootstrap sample using full set features.details ensemble methods shown Algorithms 3, 4 5.Algorithm 3: Agg.VR-Trees(Dt , Q, N, )Input: Dt - Training set, Q - Feature set, N - Number trees, - probability usingdeterministic test-selectionOutput: E - collection treesNE E VR-Tree(Dt , Q, )return EAlgorithm 4: Subspace.VR-Trees(Dt , Q, N, , )Input: Dt - Training set, Q - Feature set, N - Number trees, - probability usingdeterministic test-selection, - percentage features used, 0 < 1Output: E - collection treesNQs randomly generate set percentage features QE E VR-Tree(Dt , Qs , )return Enone ensemble methods new, incorporation VR-Tree base learnerhelp unleash potentials methods. predictive performance gain shown Section5. Note Subspacing equivalent Random Subspace method (Ho, 1998) =50%.Since =50% provides maximum number distinct subspaces, important factorincrease diversity, use =50% default setting Subspacing. Also, noticeBag.VR-Trees = 1 equivalent conventional Bagging method (Breiman, 1996a).359fiL IU , ING , U , & Z HOUAlgorithm 5: Bag.VR-Trees(Dt , Q, N, )Input: Dt - Training set, Q - Feature set, N - Number trees, - probability usingdeterministic test-selectionOutput: E - collection treesNDb generate bootstrap sample DtE E VR-Tree(Db , Q, )return Euse probability averaging combine outputs individual models ensemble.order predict class given test case, predicted class obtained by:NXni,y),arg max(ni(1)i=1N number trees ensemble, ni,y number class training instancesni total number training instances leaf tree test case falls into.5. Empirical Study Spectrumdesign experiment four parts. first part investigates predictive performance spectrum Aggregating, Bagging Subspacing using VR-Trees. use result characterize two-halves spectrum. second part examines diversity base learners generated ensembles using strength correlation curves defined Breiman (2001).part highlights range randomness one achieve using VR-Trees. third partexplores alternative using single value produce models ensemble. alternative combines number points spectrum, proposed ensemble methodpaper. fourth part investigates strengths weaknesses proposed method.Forty-five data sets UCI repository (Asuncion & Newman, 2007) used paper.characteristics data sets provided Appendix A. Ten-fold cross-validationconducted data set average error rate reported. 100 trees usedensemble. Random Forests C5 Boosting (www.rulequest.com) used benchmarks,addition Bagging, Subspacing Aggregating VR-Trees. use Friedman testBonferroni test post-hoc test 95% confidence level compare classifiers (Demsar, 2006).Random Forests implementation used paper, default settings mtry =floor(sqrt(q)) nodesize=1 used, mtry number features randomlysampled split, q number features nodesize minimum sizeterminal nodes. implementation taken R (www.r-project.org).implementation including VR-Trees based C4.5 (Quinlan, 1993). defaultC4.5s stop-splitting rules applied: (i) minimum number training samples nmin = 4required splitting considered, (ii) deterministic test-selection stops splittingpossible splits report negative scores. Gain ratio used test-selection criterion. Probabilityaveraging implemented curtailment (Zadrozny & Elkan, 2001), minimum leaf sizeprobability estimation always greater one. default settings used VR-Treespaper.360fiS PECTRUMVARIABLE -R ANDOM REES5.1 Predictive Performance Spectrum Aggregating, Bagging SubspacingFigure 1: spectrum predictive performance Aggregating, Bagging Subspacing,well Bagging plus Subspacing: error rates , average forty-five data sets.21Agg.VR-TreesBag.VR-TreesSubspace.VR-TreesBag-Subspace.VR-TreesAverage Error %20191817161500.20.40.60.81Figure 1 shows spectrum performance 1 four ensemble methods using VR-Treesbase models. Note conventional Bagging Random Subspace two pointsdeterministic-end ( = 1) spectrum Bag.VR-Trees Subspace.VR-Trees; MaxDiverse ensemble complete-random-end ( = 0) Aggregating spectrum. Figure 2shows results Friedman tests Aggregating, Bagging Subspacingfollowing observations.Figure 2, interesting note best operating region Agg.VR-Treesvalues 0.1 0.6. shows Max-Diverse ensemble operating =0 improve performance moving middle spectrum. bestoperating region Bag.VR-Trees 0.1 0.6, mainly first halfspectrum. significantly different conventional Bagging normallyapplied at, namely = 1. best operating region Subspace.VR-Trees 0.40.8. also different Random Subspace normally applied = 1.ii balanced mix random deterministic heuristics, i.e., = 0.5, produces best ensemblesignificantly difference best ensemble one three ensemble methods.iii four curves shown Figure 1, Agg.VR-Trees largest swing performance,followed Bag.VR-Trees Subspace.VR-Trees. expected two end-pointsAgg.VR-Trees curve represents single deterministic model ensemble completerandom models. decreases 1 0.5, substantial improvement predictive performance Agg.VR-Trees, takes effect due increased diversity ensemble,reduces average error rate 20.6% 16.0%.iv Although Bag.VR-Trees Agg.VR-Trees perform best region 0 0.5,result Figure 1 indicates significant difference terms predictive1. Averaged forty-five data sets.361fiL IU , ING , U , & Z HOUaccuracy. additional computational requirement generate bootstrap samplesBagging, Aggregating becomes preferred method first half spectrum. BaggingSubspacing preferred Aggregating second half spectrumlatter uncompetitive region.v use Bagging Subspacing single ensemble recommended, shownresult Figure 1 Bag-Subspace.VR-Trees always performs worse parents,Bag.VR-Trees Subspace.VR-Trees.Figure 2: Friedman test results classifiers produced (a) Aggregating, (b) Bagging, (c) Subspacing VR-Trees, eleven values spectrum forty-five data sets. use Bonferroni test post-hoc test = 0 control condition comparison. verticalaxis indicates values, horizontal axis indicates rank values. circle average rank value bar indicates critical values two-tailed test 95%significance level. two classifiers overlapping bars, indicates significantly different. Significantly worse results presented dotted bars (coloured red) locatedright-hand-side diagram. best results presented solid bars (coloured blue)left-most bar diagram.000.10.10.20.20.30.30.40.40.50.50.60.60.70.70.80.80.90.9112345678910113(a) Agg.VR-Trees, = 0.4 highest ranking4500.10.20.30.40.50.60.70.80.9144.555.5789(b) Bag.VR-Trees, = 0.6 highest ranking3.5666.577.588.5(c) Subspace.VR-Trees, = 0.5 highest ranking362fiS PECTRUMVARIABLE -R ANDOM REESsummarise characteristics two halves spectrum Table 1. next subsection, continue analysis various points spectrum relation generalisationerror.Table 1: Characteristics two halves spectrum.Complete-Random-end, [0, 0.5]Models extreme endgenerated completely randomfashion.Candidate models possibletrees larger sizes.Deterministic-end, (0.5, 1]Models variants deterministic model.Candidate models modelssmaller sizes (because modelreaches pure leaves early).Maintaining high predictive accuracy providing degree diversity.Subspacing Bagging preferred region.Models higher diversity.Aggregating preferredregion.5.2 Strength-Correlation Analysissection, examine strength correlation profiles produced Aggregating, BaggingSubspacing. Firstly, illustrate relationship generalisation error, strengthcorrelation using map. Secondly, plot strength-correlation curves actual data setscharacterise behaviours. Thirdly, continue analysis Section 5.1 exploretwo halves spectrum light strength-correlation analysis.Breimans (2001) inequality, P E (1 s2 )/s2 useful discerning relationship generalisation error P E, strength correlation . Briefly, strength ensemblemeasures expected margin function ensemble. margin function probabilitycorrect classification minus maximum probability incorrect classification instance. Correlation ensemble measure similar predictions among individual trees.high reading correlation indicates individual trees making similar predictions. usestrength correlation estimation procedures Buttrey Kobayashi (2003)corrected version Breiman (2001). make paper self-contained, estimation proceduresgiven Appendix C.Figure 3 shows map strength-correlation profiles grey scale indicating generalisation error. see error rate lower high-strength low-correlation corner(at bottom-right), error rate lower darker grey level. ensembleclassifiers general, goal get region estimated error ratelow possible. Aggregating different values typically spans fashion eithercurve (a) curve (b) shown Figure 3, curve starts = 0 (at bottom-left)= 1 (at top-right). (a), lowest error rate found = 0. (b), lower error363fiL IU , ING , U , & Z HOUrates found larger value . However, error rates also increase approaches1. case, search would necessary determine optimal .Figure 3: Ensemble generalisation error distribution using Breimans inequality strengthcorrelation. Curves (a) (b) represent two typical spans Agg.VR-Trees 0 1.=1(a)=0(b)Figure 4 shows three different ensemble methods provide different ranges strengthcorrelation profile. effective way use VR-Trees Aggregating.consistently produces longest span strength-correlation curve data setsused. Bagging usually second longest span, followed Subspacing. Note Aggregatingusually spans strength correlation dimensions; whereas Bagging Subspacingsignificantly smaller span, especially correlation dimension. Table 2 shows rangeminimum maximum values strength well correlation, averaged forty-fivedata sets. result shows Aggregating produces largest range trees comparisonBagging Subspacing. interestingly, best Aggregating usually located lowersimilar error region Bagging Subspacing.Table 2: Average ranges strength correlation forty-five data sets.Strength CorrelationAgg.VR-Trees( [0, 1])0.1350.6960.1360.160Bag.VR-Trees( [0, 1])0.0690.088Subspace.VR-Trees( [0, 1])also interesting note =0.5 always close changing cornerstrength-varying leg correlation-varying leg examples shown Figure 4. means= 0.5 either close lowest generalisation error region, ensemble exhibits364fiS PECTRUMVARIABLE -R ANDOM REESFigure 4: Strength-Correlation distribution Aggregating, Bagging Subspacing differentvalues . solid-filled marks represent =0.5. Aggregating, first half range( [0, 0.5]) characterised lower correlation lower strength, located bottomleft corner diagrams. second half ( [0.5, 1]) characterised higher correlationhigher strength, located top-right corner diagrams.Dataset: codingDataset: credit-a11Agg.VR-TreesBag.VR-TreesSubspace.VR-Trees0.8correlationcorrelation0.8Agg.VR-TreesBag.VR-TreesSubspace.VR-Trees0.60.40.20.60.40.200.20.250.30.350.40.4500.450.50.50.55strengthDataset: DNAAgg.VR-TreesBag.VR-TreesSubspace.VR-Trees0.8correlationcorrelation0.71Agg.VR-TreesBag.VR-TreesSubspace.VR-Trees0.80.60.40.20.60.40.2000.20.30.40.50.60.70.80.90.50.550.60.65strength0.70.750.80.85strengthDataset: segmentDataset: sick11Agg.VR-TreesBag.VR-TreesSubspace.VR-TreesAgg.VR-TreesBag.VR-TreesSubspace.VR-Trees0.8correlation0.8correlation0.65Dataset: ionosphere10.60.40.20.60.40.200.650.70.750.80.850.900.780.950.80.820.84strength0.860.880.90.920.940.960.70.75strengthDataset: threeOf9Dataset: tic-tac-toe11Agg.VR-TreesBag.VR-TreesSubspace.VR-TreesAgg.VR-TreesBag.VR-TreesSubspace.VR-Trees0.8correlation0.8correlation0.6strength0.60.40.20.60.40.200.40.50.60.7strength0.80.900.3513650.40.450.50.550.6strength0.65fiL IU , ING , U , & Z HOUcurve (b) like behaviour Figure 3. Thus, = 0.5 often serves upper limit rangevalues performs well.Combining result analysis two halves spectrum Section5.1, find Aggregating exhibits following characteristics data sets Figure4:first-half spectrum, strength increases rapidly = 0 slows= 0.5; however, correlation varies small degree vary all.second-half spectrum, correlation increases rapidly = 0.5 peaks= 1. range, strength correlation high, error rates optimal.summary, better performing Aggregating models found range 00.5. single operating points [0, 0.5] shown work well Section 5.1.next section, show alternative approach achieves even better result usingrange [0, 0.5] identified thus far.5.3 Coalescence Different Operating Pointssection, show combining single models multiple points spectrumachieve similar better performance compared using single point spectrum.coalesce VR-Trees sampled fixed interval. example, form 100-model ensemble,construct trees = {0, 0.005, 0.01, ..., 0.495}. call approach Coalescence.Coalescence approach appealing need know value producesaccurate ensemble given data. introducing members different talentsrandom-half spectrum; Coalescence forms committee different members.committee, far know, members good approximating non-axis-parallel boundarymembers good avoiding over-fitting. make-up committee helps handleunforeseeable conditions arise training sample.result, Coalescence provides comparable predictive performance search-for-the-best approach (Liu & Ting, 2006) without cost searching optimal value. Figure 5,Coalescence shown better single operating point first-half spectrumusing Friedman test, forty-five data sets.additional comparison also conducted well known tree ensemble classifiers. complete result Coalescence, Aggregating VR-Trees (=0) (i.e. Max-Diverse Ensemble), Bagging, Subspacing (=1) (i.e. Random Subspace), C5 Boosting Random Forestspresented Table 3. average error rates forty-five data sets provided last rowtable. Figure 6 shows result Friedman test following observations:Coalescence ranks highest among five benchmarking ensembles Friedman test.second highest ranking ensembles: Random Forests C5 Boosting almost identicalranking Friedman test, similar average error rates.third highest ranking ensemble Random Subspace; lowest ranking ensemblesBagging Aggregating VR-Trees ( = 0). ensemble methods similar averageerror rates.366fiS PECTRUMVARIABLE -R ANDOM REESFigure 5: Friedman test result comparing Coalescence five operating points Agg.VRTrees first-half spectrum. Horizontal axis indicates rank values. Coalescence rankstop compared different points first-half spectrum.CoalescenceAgg( = 0)( = 0.1)( = 0.2)( = 0.3)( = 0.4)( = 0.5)2.533.544.555.56Figure 6: Friedman test result comparing Coalescence five benchmarking methods. Horizontal axis indicates rank values. Notice Coalescence method significantlybetter Agg. = 0 (Max-Diverse Ensemble) Bag. = 1 (Bagging).CoalescenceAgg( = 0)Bag( = 1)Subspace( = 1)C5 BoostRandomForests22.533.536744.55fiL IU , ING , U , & Z HOUTable 3: Experimental results: Average error rate ten-fold cross-validation.data setsabaloneannealaudiologyautosbalancebreastwbreastychessclevelandcodingcredit-acredit-gdnaechoflareglasshayeshepatitishorsehypoionosphereirislaborled24led7liverlymphnurserypimapostprimarysatimagesegmentsicksolarsonarsoybeanthreeOf9tic-tac-toevehiclevotewaveform21waveform40winezoomeanCoalescence30.01.517.218.114.43.028.70.742.916.412.323.15.333.518.621.018.118.013.30.85.74.77.027.926.727.014.90.923.440.055.28.82.12.129.415.95.40.03.024.54.114.515.73.41.015.6Agg.VR-Trees(=0)30.21.417.722.512.32.425.91.641.616.813.025.726.534.219.222.921.915.517.91.78.54.73.330.326.927.914.32.224.636.757.210.43.15.730.315.96.00.69.727.15.314.717.01.12.016.8Bag.VR-Trees(=1)30.73.516.219.518.13.427.32.344.224.212.822.76.029.017.721.517.520.015.20.85.44.010.728.126.327.320.43.724.037.856.69.02.42.227.224.06.61.414.324.94.616.116.53.43.016.7368Subspace.VR-Trees(=1)30.03.220.314.711.43.025.11.841.916.713.024.83.732.017.121.517.518.616.01.35.46.012.029.127.531.915.65.923.232.251.98.32.34.327.818.75.310.022.325.44.614.715.32.34.016.4C5Boost31.15.015.015.618.93.126.90.341.615.414.322.44.837.417.521.416.914.122.50.85.44.015.728.127.829.619.10.925.030.056.98.11.82.225.715.96.20.01.223.34.815.615.15.63.015.9RandomForests30.99.720.815.216.33.429.71.141.917.513.023.23.432.018.521.016.917.314.11.06.53.35.028.526.326.116.92.323.232.256.98.12.12.127.213.95.70.81.326.14.114.714.92.35.015.6fiS PECTRUMVARIABLE -R ANDOM REESensembles, Coalescence shown significantly better AggregatingVR-Trees ( = 0) Bagging Friedman test.interesting note employing oracle find best , optimal error rate14.6% 2 (Liu & Ting, 2006). Thus, Coalescence approach comes close optimal resultwithout using oracle.Figure 7 shows predictive performance Coalescence relation Aggregating eightdata sets. Coalescence sometimes outperforms parents ( [0, 0.5]), often comes closebest performing Aggregating.5.4 Strengths Weaknessessection examine strengths weaknesses VR-Tree. Although Coalescencesignificantly better Random Forests Friedman test, Table 3 clearly shows Coalescence better Random Forests data sets worse others. section alsoexamine conditions Coalescence performs better Random Forestsvice versa.find ensembles complete-random-trees following strengths: 1) capableapproximating non-axis-parallel boundary, 2) highly stable learners termsPointwise Hypothesis Stability (Bousquet & Elisseeff, 2002). analysis based PointwiseHypothesis Stability found Appendix B. verify complete-random-trees abilityapproximate non-axis-parallel boundary, Figure 8, provide visualization example usingGaussian mixture data set (Hastie, Tibshirani, & Friedman, 2001). Figure 8 shows ensemble complete-random-trees (using 100 trees) able better approximate non-axis-parallelboundary compared single deterministic tree.Moreover, described analysis Appendix B, one strengths complete-randomtrees also one weaknesses: complete-random trees trend over-fit; problemstems mainly ability approximate non-axis-parallel boundary. also findoverfitting problem aggravated irrelevant attributes, class noise small training sizeshown following empirical examination.denote X input space output space. learning algorithm outputsfunction approximates underlying true function f inspecting training set. trainingset = {zi }mi=1 , zi = (xi , yi ) yi = f (xi ), contains i.i.d. examples Z = Xdrawn uniform distribution instance space. define input space twovariables, xi = hi,a , i,b output space two possible classes = {+1, 1}.case, instance space square point (a = 1, b = 1) point (a = 1, b = 1).define two concepts follows:(+1 (i,a > i,b ), xi = hi,a , i,bConcept A: f (xi ) = yi =1 else(+1 (i,a > 0)Concept B: f B (xi ) = yi =, xi = hi,a , i,b1 else2. Averaged forty-five data sets used paper.369fiL IU , ING , U , & Z HOUFigure 7: Detail results comparing Coalescence Aggregating eight data sets.Dataset: abalone32.5Agg.VR-TreesCoalescenceDataset: balance2432Agg.VR-TreesCoalescenceError %2031181630.5143000.20.40.60.812100.20.40.60.81Dataset: DNADataset: ionosphere309Agg.VR-TreesCoalescenceAgg.VR-TreesCoalescence8.52587.5Error %Error %201576.510655.50500.20.40.60.8100.20.4Dataset: nursery0.810.810.81Dataset: segment10Agg.VR-TreesCoalescenceAgg.VR-TreesCoalescence8866Error %Error %0.61042420000.20.40.60.8100.20.40.6Dataset: solarDataset: waveform 4028Agg.CR-TreeCoalescence34Agg.VR-TreesCoalescence2624Error %32Error %Error %2231.5302822201816261400.20.40.60.8100.20.40.6370fiS PECTRUMVARIABLE -R ANDOM REESFigure 8: Visualisation non-axis-parallel decision boundary Gaussian mixture data usingensemble complete-random-trees compared single deterministic tree (C4.5).Error 11.3%Error 14.2%(a) Complete-random trees(b) Single deterministic treepositive classnegative class(c) Training sample(d) Actual decision boundaryConcept useful illustrating ability approximate non-axis-parallel boundary. ConceptB axis-parallel concept, used control condition experiment. conductfour experiments using (a) training sample k = 1024 instances, (b) training sampleirrelevant attributes, (c) training sample class noise (d) size-reduced training samplek = 64. train ensemble models using Coalescence, VR-Trees ( = 0, = 0.5) RandomForests. use 100 trees ensemble. Finally, evaluate models 10000 latticesamples instance space, average error rate 10 runs reported.Table 4 shows that:VR-Trees ( = 0) best performer approximating non-axis-parallel boundary conceptA, even small training size k = 64. However, worst performer axis-parallelconcept B k = 1024 k = 64.VR-Trees ( = 0.5) performs best class noise irrelevant attribute conditionsconcepts B. irrelevant attributes added, error rate VR-Trees ( =0) increases faster rate compare VR-Trees ( = 0.5). Similarly, class noise affectscomplete-random-end ( = 0) spectrum severely middle pointspectrum ( = 0.5).371fiL IU , ING , U , & Z HOUTable 4: Averaged error rates VR-Trees ( = 0, = 0.5), Coalescence Random ForestsConcepts B. default number training samples k = 1024, unless otherwise specified.best error rate row bold faced.VR-TreesVR-Trees( = 0) Coalescence ( = 0.5) Random Forests1.4%1.6%2.0%1.9%A, 8 irr. att.7.6%3.4%3.0%3.2%A, 40% class noise30%21.9%14.7%33.4%A, k = 646.7%8.9%10.4%7.3%B0.3%0%0%0%B, 8 irr. att.5.2%0%0%0%B, 40% class noise30.7%4.2%2.6%32.5%B, k = 642.4%1.1%1.1%0.8%Coalescence Random Forests, Coalescence performs better class noise condition; Random Forests better performer small training size conceptsB.empirical results show that, complete-random trees, i.e. VR-Trees ( = 0)good approximating non-axis-parallel boundary, easily over-fitted irrelevant attributes,class noise small training sample. Although case, find Coalescence waymanage propensities without search specific value. Table 4, findCoalescence tends performance closer better performing learner either VR-Trees( = 0) VR-Trees ( = 0.5); exception learning non-axis-parallel conceptsmall training sample.understanding irrelevant attributes affect Coalescence, performsimple check eighteen data sets Random Forests performs better (ignoring twoartificial data sets: led waveform already known results withoutirrelevant attributes).remove less-important half attributes according Random Forests variableimportance (Breiman, 2001). evaluate eighteen data sets using CoalescenceRandom Forests 10-fold cross validation. result Table 5 shows Coalescence performs better ten eighteen data sets; Random Forests performs betterfour data sets. result indicates influence irrelevant attributes Coalescencegreater Random Forests. Among eighteen data sets, Coalescences error ratesreduced half labor, hayes tic-tac-toe data sets. result indicatesmanagement irrelevant attributes indeed improve performance Coalescence.6. Related Worksection, first highlight differences VR-Trees Random Forestscapability vary degree randomness, distinguish VR-Trees others. Then, orderbetter position VR-Trees, discuss various different decision tree ensembles relatedVR-Trees.372fiS PECTRUMVARIABLE -R ANDOM REESTable 5: Evaluation Coalescence Random Forests respect condition irrelevant attribute: eighteen data sets listed less-important half attributes removed.Boldfaced indicates improvement error rate using reduced number attributesCoalescenceRandom Forestsdata sets Full att. Half att. Full att. Half att.autos18.113.615.214.6cleveland42.945.241.943.9dna5.33.93.43.2echo33.539.032.042.8flare18.619.018.518.9hayes40.618.141.316.9hepatitis18.016.717.317.4iris4.74.03.33.3labor7.03.35.05.0liver27.033.626.133.1pima23.424.523.224.5post40.037.832.236.7satimage8.89.48.19.3solar29.430.327.228.7sonar15.913.013.916.4tic-tac-toe18.83.018.31.3vote4.14.44.14.4wine3.41.72.32.86.1 Relationship Random Forestsinteresting note Breiman (2001) found Random Forests accuracy overlysensitive value F , parameter intended vary degree randomness.F parameter corresponds parameter VR-Tree. Yet, experiments section 5.1 clearlyshows varying degree randomness (using ) significant impact predictiveperformance resulting ensembles. thus important identify differencestwo ensembles cause different behaviours. following paragraphs.Algorithm 6: random feature selection framework Random ForestsInput: Dt - Training set, F - number featuresOutput: node: tree noderandomly select F features available features0 = subset Dt according F featuresnode = DeterministicTestSelection(D 0 )return nodesurface, Random Forests similar Agg.VR-Trees usedeterministic random test-selections tree induction process. However, differway test-selection applied decision node. randomisation frameworkRandom Forests described Algorithm 6.373fiL IU , ING , U , & Z HOUapplying test-selection, Random Forests applies random feature selection deterministic test-selection node; however VR-Tree applies either random deterministictest-selection node. controls probability whether deterministic random testselection applied node; whereas mixed application two selection processesnode constrains amount randomness introduced Random Forests.case Random Forests, F controls number features randomly selected. selected, deterministic test-selection chooses best feature. Thus, best feature readilyselected first place, matter F is, best feature always chosendeterministic test-selection. agrees Breimans observation error rateoverly sensitive different values F Random Forests.accessibility different degrees randomness directly affects diversity modelsproduced, Figure 9 shows strength-correlation curves Random Forests (usingavailable F values, 19 segment data 21 waveform21 data), comparison Agg.VRTrees using eleven values sampled equal interval. find Random Forests producesensembles highly correlated many similar strength.result also reported Breiman (2001). Note fitted curves Random Forestsvisual aids mean represent accessibility points. contrast, Agg.VR-Treesproduces ensembles accessible along curve spread along wider range.nutshell, randomisation framework used Random Forests significantly limits ability scale different levels randomness total number features small.hand, VR-Trees able scale different levels randomness regardless numberfeatures.6.2 Related Workapproach search best value proposed (Liu & Ting, 2006). approach searchesoptimal value based average progressive training errors. estimated optimalb taskgenerated follows:N1 Xerr(, i, Dt )](2)b = arg min[00.5 Ni=1N total number trees ensemble, err() returns training error rateensemble size i, Agg.VR-Trees set training set Dt . obtainingb, i.e.,best performing , ensemble employs modelb actual predictive tasks. Noteunpruned tree ensemble stops growing number training examples nodefour less (the default setting C4.5.) avoids generating zero training error trees. Thoughmethod comparable prediction performance Coalescence approach, requiressubstantial computational overhead ensembles values search mustproduced.contrary common belief, Fan et al (2003) first propose use complete-random treesproduce accurate predictive models. Fan (2004) explains reason combinationcomplete-random trees probability averaging produces accurate models. Using completerandom trees, Liu et al. (2005) show ensembles perform comparably BaggingRandom Forests.Cutler Zhao (2001) propose PERT (Perfect Random Tree Ensembles) randomisestest-selection continuous-valued features achieve higher randomisation. potential374fiS PECTRUMVARIABLE -R ANDOM REESFigure 9: Strength-Correlation plots Random Forests Aggregating VR-Trees differentF values. Aggregating VR-Trees wider range correlation compared RandomForests.Dataset: segment1Agg.VR-TreesRandom Forests0.90.8correlation0.70.60.50.40.30.20.10.550.60.650.70.750.8strength0.850.90.95Dataset: waveform 211Agg.VR-TreesRandom Forests0.90.8correlation0.70.60.50.40.30.20.10.340.360.380.40.42 0.44strength3750.460.480.50.52fiL IU , ING , U , & Z HOUsplit, PERT first randomly selects two examples different classes local training set.feature selected randomly. cut point feature randomly selected valuestwo samples. leaf formed two examples different classes cannot found tentrials. PERT also shown competitive Bagging Random Forests. believemethod likely close complete-random-end spectrum. However, uncleardifferent degrees randomisation introduced PERT framework.Extra-Trees (Geurts, Ernst, & Wehenkel, 2006) relies framework Algorithm6. However, random split-selection used instead deterministic split-selection. DifferentPERT, Extra-Trees cut points randomly selected maximum minimumvalues given samples. compared PERT, Extra-Trees requires additional data-scanevery node tree find maximum minimum values, disadvantageterms computational complexity. categorical features, random subset split usedExtra-Trees. method also shown competitive Random Forests.Robnik-Sikonja (2004) reports improved version Random Forests using five differentdeterministic test-selection criteria instead one. achieves effect increased diversityproducing different variants deterministic models starting point Random Forests,Bagging Random Subspace.MultiBoosting (Webb, 2000; Webb & Zheng, 2004) another approach combinesone type ensembles. MultiBoosting tightly-coupled method incorporates bagging(random sample weight assignment) main Boosting procedure (the incremental sampleweight modifier) order increase model diversity. Like Bagging Random Forests,focusing increasing diversity deterministic-end spectrum.many algorithms reported literature, listed paper, suggesting different ways combine models generated one algorithm different algorithms,(e.g., see Breiman, 1996b; Ting & Witten, 1999; Perrone & Cooper, 1993) . requirekind learning estimation order either selectively choose available modelsbuild meta-model combine them. Coalescence approach simpleapproaches require learn meta-model or/and kind estimation.7. Conclusionspaper, make following contributions:Propose new algorithm, generates spectrum VR-Trees span completerandom trees deterministic trees. show different points spectrumsignificantly different terms predictive accuracy. opens new opportunitiesdecision tree ensembles.Show existing ensemble methods Bagging, Random Subspace, Max-DiverseEnsemble either end spectrum. performance ensemblesimproved moving towards middle spectrum improvementssignificant.Discover two halves spectrum distinctive characteristics, separatedcritical point =0.5. point two interesting characteristics. First, producesequal percentage random deterministic decision nodes VR-Trees. Second, often376fiS PECTRUMVARIABLE -R ANDOM REESlies lowest generalisation error region (or close it) typical strength-correlationcurve. Ensembles generated [0.0, 0.5] often out-perform generated[0.5, 1.0].Propose new approach building better performing ensembles. Coalescence approachcoalesces number points first-half spectrum. show ranks bettersingle operating point whole spectrum.Identify key differences ensembles constructed frameworks RandomForests VR-Tree explain predictive accuracy sensitive parameterVR-Tree framework, Random Forests framework.empirical evaluation, Coalescence compared five benchmarking ensemble methods: Max-Diverse Ensemble, Bagging, Random Forests, Random Subspace C5 Boosting.study reveals Coalescence ranks top ensemble method significantlybetter Bagging Max-Diverse Ensemble using Friedman test.Although Coalescence significantly better Random Forests, identified that:Random Forests performs better Coalescence conditions irrelevant attributesmall training size, Coalescence performs better learning non-axis-parallel conceptsclass noise.AcknowledgmentsY. Yu Z.-H. Zhou supported National Science Foundation China GrantNos. 60635030 60721002. Part research conducted K. M. Ting visitingLAMDA group, Nanjing University.377fiL IU , ING , U , & Z HOUAppendix A. Data characteristics data setsTable 6: Data characteristics forty-five data sets used experiments. Data takenUCI repository (Asuncion & Newman, 2007).data setssize#att. #class descriptionabalone41771n, 7c2 Abalone growthanneal898 13n, 6c, 19b6 Steel annealingaudiology2268n, 61b23 Standardised Audiology Databaseauto2056n, 15c, 4b7 1985 Auto Imports Databasebalance6254c3 Balance Scale weight Distance Databasebreast-w69910c2 Winconsin breast cancer databasebreast-y2866n, 3b2 Ljubljana Breast cancer databasechess319635n, 1b2 Chess end gamescleveland3034n, 6c, 3b5 Cleveland heart disease databasecoding 2000015n2 Coding databasecredit-a6904n, 6c, 4b2 Australian Credit databasecredit-g100012c, 12b2 German credit databasedna318660n3 Primate splice-junction gene sequencesecho1336c, 1b2 Echocardiogram dataflare10663n, 2c, 5b2 Predicting solar flareglass2149c7 Glass identification databasehayes1604c3 Hayes-Roth & Hayes-Roth databasehepatitis1556c, 13b2 Hepatitis Domainhorse36813n, 7c, 2b2 Horse colic databasehypo31637c, 18b2 Thyroid disease databaseionosphere35134c2 Radar returns ionosphereiris1504c3 Iris plants databaselabor575n, 8c, 3b2 Final settlements labour negotiationsled24320024b10 LED display + 17 irrelevant attributesled732007b10 LED display irrelevant attributeliver3456c2 BUPA liver disorderlymph1486n, 3c, 9b4 Lymphography domainnursery 129608n5 Nursery databasepima7688c2 Diabetes female Pima Indianspost907n, 1c3 Postoperative patient dataprimary3393n, 14b22 Primary tumor domainsatimage643536c7 Satellite image data set NASAsegment231019c7 Image segmentation datasick31637c, 18b2 Sick-euthyroid datasolar3233n, 3c, 6b6 Solar data setsonar20860c2 Classification sonar signalssoybean68319n, 16b19 Soy bean disease diagnosisthreeOf95129b2 concept three ninetic-tac-toe9589n2 Tic-Tac-Toe board configurationsvehicle84618c4 Vehicle silhouette data setvote43516n2 Votes U.S. Congressmenwaveform21500021c3 Waveform datawaveform40500040c3 Waveform data 19 noise attributeswine17813c3 Wine recognition datazoo1011n, 15b7 Zoo databaseAttribute type indicated n: nominal, c: continuous, b: binary.378fiS PECTRUMVARIABLE -R ANDOM REESAppendix B. Theoretical Analysis VR-Trees ( = 0)notations similar Bousquet Elisseeff (2002). Denote X inputspace output space. learning algorithm function : Z k F, F Xfunction space. denote f function F. learning algorithm outputs functionapproximates underlying true function f inspecting training set. training set= {zi }ki=1 , zi = (xi , yi ) yi = f (xi ), contains k i.i.d. examples Z = Xdrawn unknown distribution D. consider case X bounded Real space= {1, +1}. Denote training set removing i-th example\i = {z1 , . . . , zi1 , zi+1 , . . . , zk }Given learning algorithm trained S, denote generalisation errorR(T, S) = Ez=(x,y)[`(fT,S (x), y)] ,empirical error1 Xk`(fT,S (xi ), yi ) ,i=1k= (S) ` : R loss function.(T, S) =fT,Sstudy generalisation ability viewpoint Pointwise Hypothesis Stability(Bousquet & Elisseeff, 2002).Definition 1 (Pointwise Hypothesis Stability)algorithm pointwise hypothesisstability respect loss function ` holds {1, . . . , k}ESD [|`(fT,S (xi ), yi ) `(fT,S \i (xi ), yi )|]Theorem 11 Bousquet Elisseeff (2002) reveals relationship Pointwise Hypothesis Stability generalisation error. self-contained, write theoremLemma 1.Lemma 1 learning algorithm pointwise hypothesis stability respect lossfunction ` , 0 `(, ) , probability 1 ,r2 + 12M kR(T, S) (T, S) +2kAssume (i) one attribute X attribute Real values, (ii) everytraining example unique attribute values, (iii) internal node non-empty subsets, (iv)tree building process stops nodes contain one example, (v) outputVR-Tree +1 1 input instance case binary classification. buildingensemble VR-Trees, run VR-Tree algorithm N times produce set trees {fi }Ni=1 .Given test instance z = (x, y), output ensemble is:f N (x) =1 XNfi (x)i=1N379fiL IU , ING , U , & Z HOUwhose range [1, +1]. following analysis, let N approach infinity.order study Pointwise Hypothesis Stability VR-Trees ( = 0) ensembles, needboundESD [|`(fT,S (xi ), yi ) `(fT,S \i (xi ), yi )|].specify loss function1,`(y1 , y2 ) = 0.5,0,|y1 y2 | > 1|y1 y2 | = 1 .|y1 y2 | < 1Let denote number class transitions, example pairs (zj , zk )yj 6= yk example xj xk . used bound later.Now, training instance z 0 = (x0 , 0 ) held test exampleleaving \i , want know VR-Tree ensemble makes predictions z 0 . findone following four events happen classifying z 0 , illustrated Figure 10.Figure 10: Illustration four possible places test instance. Circles +1, 1 ? denotepositive, negative test instances, respectively.?(a )+1(b )+1(c)(d )?+1-1+1??+1a) z 0 duplicate training example zi = (xi , yi ), illustrated Figure 10(a).f n (x0 ) = yi , every tree leaf pure, means empirical error always zero. Sinceassume every instance unique location, ignore case .b) z 0 located two training examples zi = (xi , yi ) zj = (xj , yj ) yi = yj ,illustrated Figure 10(b). case, z 0 fall leaf node containing zi zj . Therefore,f n (x0 ) = yi = yj . z 0 wrongly classified, is, z 0 different label yiyj , two counts added .c) z 0 located outside training set zi = (xi , yi ) nearest training example,illustrated Figure 10(c). case z 0 fall leaf node must contain zi , thusf n (x0 ) = yi . z 0 wrongly classified, one count added .d) z 0 located two training examples zi = (xi , yi ) zj = (xj , yj ) yi 6= yj .z 0 wrongly classified, is, z 0 different label either yi yj , one count added .380fiS PECTRUMVARIABLE -R ANDOM REESTherefore, given example zi = (xi , yi ) S, `(fT,S (xi ), yi ) = 0, yiincluded S, case Figure 10(a). analysis, also numbererrors upper-bounded .= ESD [|`(fT,S (xi ), yi ) `(fT,S \i (xi ), yi )|]= ESD [`(fT,S \i (xi ), yi )]ESD [S ]/k .Note varies training sets, upper bounded class transitionsunderlying true function f , is, letfifi= fi{x X | > 0 x0 : f (x)f (x0 ) < 0 kx x0 k < }fi ,constant.: ,= large enough. Therefore,/k .Since constant, ensemble VR-Trees ( = 0) stable learner, whose generalisationerror bounded, probability 1 ,r1 + 12.R(T, S)2kobserved features large weaken predictive performance.Moreover, find irrelevant attributes, class noise insufficient training samplecause large . irrelevant attributes, perform almost random projection X Y.f random projection, i.e. P (f (x) = +1) = 0.5, probability classtransitions training setP (S ) (1/2)|S|/S 1considering transitions divide equal segments. implies irrelevantattributes weaken predictive performance VR-Trees ( = 0). class noise insufficiencytraining samples, hard see also increase hence also weakenperformance VR-Trees ( = 0).381fiL IU , ING , U , & Z HOUAppendix C. Estimation Strength Correlationmake paper self contained, provide estimation strength correlationdefined Breiman (2001) corrected Kobayashi (2002). Given ensemble N trees{f1 , ..., fN }, hold-out set Dh = {(x1 , y1 ), ..., (xk , yk )} k number test cases,estimate followings:Strength - estimated average margin Dh .Correlation - estimated taking variance margin standard deviationrandom vectors, represent trees.C.1 Estimation Strengthestimation Strength given by:k1Xsik(3)NN1 X1 XI(fj (xi ) = yi )I(fj (xi ) = c(i))NN(4)=i=1margin si given by:si =j=1j=1test case i, c(i) class label receives maximum votes among N trees c(i)class label true label yi . I(.) indicator function returns 1true, 0 otherwise.C.2 Estimation Correlationestimation Correlation given by:=var(s)N1 Psd(j)N(5)j=1variance margin var(s) given by:var(s) =k1X 2si s2k(6)i=1standard deviation random vector sd(j) given by:sd(j) = [p1 + p2 (p1 p2 )2 ]1/2(7)k1Xp1 =I(fj (xi ) = yi )k(8)i=1k1XI(fj (xi ) = c(i))p2 =ki=1382(9)fiS PECTRUMVARIABLE -R ANDOM REESReferencesAsuncion, A., & Newman, D. J. (2007).UCIhttp://www.ics.uci.edu/mlearn/MLRepository.html.machinelearningrepository..Bauer, E., & Kohavi, R. (1999). empirical comparison voting classification algorithms: Bagging, boosting, variants. Machine Learning, 36(1-2), 105139.Bousquet, O., & Elisseeff, A. (2002). Stability generalization. Journal Machine LearningResearch, 2, 499526.Breiman, L. (1996a). Bagging predictors. Machine Learning, 24(2), 123140.Breiman, L. (1996b). Stacked regressions. Machine Learning, 24(1), 4964.Breiman, L. (2000). Randomizing outputs increase prediction accuracy. Machine Learning,40(3), 229242.Breiman, L. (2001). Random forests. Machine Learning, 45(1), 532.Buttrey, S. E., & Kobayashi, I. (2003). strength correlation random forests. Proceedings2003 Joint Statistical Meetings, Section Statistical Computing, San Francisco, CA.American Statistical Association.Cutler, A., & Zhao, G. (2001). PERT - perfect random tree ensembles. Computing ScienceStatistics, Vol. 33, pp. 490497, Costa Mesa, Orange Country, California.Demsar, J. (2006). Statistical comparisons classifiers multiple data sets. Journal MachineLearning Research, 7, 130.Dietterich, T. G. (2000). experimental comparison three methods constructing ensemblesdecision trees: Bagging, boosting, randomization. Machine Learning, 40(2), 139157.Fan, W. (2004). optimality probability estimation random decision trees. Proceedings Nineteenth National Conference Artificial Intelligence, Sixteenth ConferenceInnovative Applications Artificial Intelligence (AAAI), pp. 336341, California, USA.AAAI Press / MIT Press.Fan, W., Wang, H., Yu, P. S., & Ma, S. (2003). random model better? accuracy efficiency. ICDM 03: Proceedings Third IEEE International Conferenceon Data Mining,5158.Geurts, P., Ernst, D., & Wehenkel, L. (2006). Extremely randomized trees. Machine Learning,63(1), 342.Hastie, T., Tibshirani, R., & Friedman, J. (2001). elements statistical learning : Data mining,Inference, Prediction. Springer-Verlag.Ho, T. K. (1998). random subspace method constructing decision forests. IEEE TransactionsPattern Analysis Machine Intelligence, 20(8), 832844.Kobayashi, I. (2002). Randomized Ensemble Methods Classification Trees. Ph.D. thesis, NavalPostgraduate School, Monterey, CA.Liu, F. T., & Ting, K. M. (2006). Variable randomness decision tree ensembles. AdvancesKnowledge Discovery Data Mining, 10th Pacific-Asia Conference (PAKDD 2006), pp.8190, Singapore.383fiL IU , ING , U , & Z HOULiu, F. T., Ting, K. M., & Fan, W. (2005). Maximizing tree diversity building complete-randomdecision trees. Advances Knowledge Discovery Data Mining, 9th Pacific-Asia Conference (PAKDD 2005), pp. 605610, Hanoi, Vietnam.Melville, P., & Mooney, R. (2003). Constructing diverse classifier ensembles using artificial training examples. Proceedings Eighteenth International Joint Conference ArtificialIntelligence, pp. 505510, Mexico.Perrone, M. P., & Cooper, L. N. (1993). networks disagree: ensemble methods hybridneural networks. Artificial Neural Networks Speech Vision, 126142.Quinlan, R. J. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann, San Mateo,Calif.Robnik-Sikonja, M. (2004). Improving random forests.. Boulicaut, J.-F., Esposito, F., Giannotti,F., & Pedreschi, D. (Eds.), Proceedings 15th European Conference Machine Learning (ECML 2004), Vol. 3201 Lecture Notes Computer Science, pp. 359370, Pisa, Italy.Springer.Ting, K. M., & Witten, I. H. (1999). Issues stacked generalization. Journal Artifical IntelligenceResearch (JAIR), 10, 271289.Webb, G. I. (2000). Multiboosting: technique combining boosting wagging. MachinceLearning, 40(2), 159196.Webb, G. I., & Zheng, Z. (2004). Multistrategy ensemble learning: Reducing error combiningensemble learning techniques. IEEE Transactions Knowledge Data Engineering,16(8), 980991.Zadrozny, B., & Elkan, C. (2001). Obtaining calibrated probability estimates decision treesnaive bayesian classifiers. ICML 01: Proceedings Eighteenth International ConferenceMachine Learning, 609616.384fiJournal Artificial Intelligence Research 32 (2008) 123167Submitted 09/07; published 05/08Constraint Programming Approach SolvingQueueing Control ProblemDaria TerekhovJ. Christopher Beckdterekho@mie.utoronto.cajcb@mie.utoronto.caDepartment Mechanical & Industrial EngineeringUniversity Toronto, CanadaAbstractfacility front room back room operations, useful switch workersrooms order cope changing customer demand. Assuming stochasticcustomer arrival service times, seek policy switching workersexpected customer waiting time minimized expected back room staffingsufficient perform work. Three novel constraint programming models severalshaving procedures models presented. Experimental results show modelbased closed-form expressions together combination shaving proceduresefficient. model able find prove optimal solutions many probleminstances within reasonable run-time. Previously, available approachheuristic algorithm. Furthermore, hybrid method combining heuristic bestconstraint programming method shown perform well heuristic termssolution quality time, achieving performance terms provingoptimality pure constraint programming model. first workaware solves queueing-based problems constraint programming.1. Introductionoriginal motivation study scheduling resource allocation problems withinartificial intelligence (AI) constraint programming (CP) that, contrast Operations Research (OR), full richness problem domain could representedreasoned using techniques knowledge representation (Fox, 1983). muchsuccess constraint-based scheduling due algorithmic advances (Baptiste,Le Pape, & Nuijten, 2001), recently, interest complex problemsinvolving uncertainty (Policella, Smith, Cesta, & Oddi, 2004; Sutton, Howe,& Whitley, 2007; Beck & Wilson, 2007). broader constraint programming community significant work past five years reasoning uncertainty(Brown & Miguel, 2006). Nonetheless, recognized constraint solving changeuncertainty infancy (Brown & Miguel, 2006, p. 754).Queueing theory intensively studied design control systems resourceallocation uncertainty (Gross & Harris, 1998). Although much studydescriptive sense developing mathematical models queues, prescriptivework attempts develop queue designs control policies optimize quantitiesinterest (e.g., customers expected waiting time) (Tadj & Choudhury, 2005). Onechallenges queueing theory, however, analytical models yet extendc2008AI Access Foundation. rights reserved.fiTerekhov & Beckricher characteristics encountered real-world problems rostering call centres(Cezik & LEcuyer, 2008).long-term goal integrate constraint programming queueing theory twoends mind: extension constraint programming reason better uncertaintyexpansion richness problems queueing theory broughtbear. achieve goal here. Rather, paper represents first stepsolve queueing control problem constraint programming techniques. Specifically,develop constraint programming approach queueing control problem arisesretail facilities, stores banks, back room front room operations.front room, workers serve arriving customers, customers form queuewait served workers busy. back room, work directlydepend customer arrivals may include tasks sorting processing paperwork.workers facility cross-trained assumed able perform backroom tasks equally well serve customers service rate. Therefore, makessense managers facility switch workers front roomback room depending number customers front room amountwork performed back room. managers thus interestedfinding switching policy minimizes expected customer waiting time frontroom, subject constraint expected number workers back roomsufficient complete required work. queueing control problem studieddetail Berman, Wang Sapna (2005), propose heuristic solving it.contributions twofold. Firstly, constraint programming is, first time,used solve stochastic queueing control problem. Secondly, complete approachproblem heuristic algorithm existed previously presented.paper organized follows. Section 2 presents description problemwork done Berman et al. (2005). next section, three CP modelsproblem proposed. Sections 4 5 present methods improving efficiencymodels, focusing dominance rules shaving procedures, respectively. Section 6 showsexperimental results comparing proposed CP models combinations inferencemethods. performance CP techniques contrasted heuristicmethod Berman et al. Based results, hybrid method proposed evaluatedSection 7. Section 8, discussion results presented. Section 9 describes relatedproblems states directions future work. Section 10 concludes paper.appendix containing derivations expressions used paper also included.2. Problem DescriptionLet N denote number workers facility, let maximum numbercustomers allowed front room one time.1 customers present,arriving customers allowed join front room queue leave withoutservice. Customers arrive according Poisson process rate . Service timesfront room follow exponential distribution rate . minimum expected numberworkers required present back room order completenecessary work assumed known, denoted Bl , l stands lower1. notation used Berman et al. (2005) adopted throughout paper.124fiA CP Approach Queueing Control Problembound. one worker allowed switched time, switching timeswitching cost assumed negligible. goal problem find optimalapproach switching workers front room back room minimizeexpected customer waiting time, denoted Wq , time ensuringexpected number workers back room least Bl . Thus, policy needsconstructed specifies many workers front room back roomparticular time switches occur.2.1 Policy Definitionterm policy used queueing control literature describe rule prescribes,given particular queue state, actions taken order control queue.research optimal control queues focused determiningparticular type policy optimal, rather finding actual optimal valuesparameters policy (Gross & Harris, 1998). term optimal policy usedliterature mean optimal type policy optimal parameter valuesgiven policy type. distinction two important since showingparticular policy type optimal theoretical question, whereas finding optimalvalues specific policy type computational one.policy type adopted one proposed Berman et al. (2005). policydefined terms quantities ki , = 0, . . . , N states workersfront room whenever ki1 + 1 ki customers (inclusive)front room, = 1, 2, . . . , N . consequence interpretation, followingconstraints hold: ki ki1 1, k0 0 kN = S. example, consider facility= 6 N = 3, suppose policy (k0 , k1 , k2 , k3 ) = (0, 2, 3, 6) employed.policy states k0 + 1 = 1 k1 = 2 customers front room,one worker front room; 3 customers, 2 workers;4, 5, 6 customers, 3 workers employed front. Alternatively,ki interpreted upper bound number customers servedworkers given policy. Yet another interpretation type switchingpolicy comes noticing soon number customers front roomincreased 1 particular switching point ki , number workers frontroom changes + 1. definition policy forms basis model proposedBerman et al., switching points ki , = 0, . . . , N 1, decision variablesproblem, kN fixed S, capacity system.policy formulation allow worker permanently assigned backroom: definition ki every worker will, system state,serving customers. Due definition, exist problem instances infeasible policy type yet feasible reality. Consequently, proposed policyformulation sub-optimal (Terekhov, 2007). However, goal current workdemonstrate applicability constraint programming computing optimal valuesgiven policy type address theoretical optimality questions. Therefore,term optimal policy used throughout paper refer optimal numericalparameters policy type proposed Berman et al. (2005).125fiTerekhov & Beck2.2 Berman et al. Modelorder determine expected waiting time expected number workersback room given policy defined particular values ki , Berman et al. first define setprobabilities, P (j), j = k0 , k0 + 1, . . . , S. P (j) denotes steady-state (longrun) probability queue state j, is, exactly j customersfacility. Based Markovian properties queueing system (exponentialinter-arrival service times), Berman et al. define set detailed balance equationsdetermination probabilities:P (j) = P (j + 1)1j = k0 , k0 + 1, . . . , k1 1P (j) = P (j + 1)2......j = k1 , k1 + 1, . . . , k2 1...P (j) = P (j + 1)i......j = ki1 , ki1 + 1, . . . , ki 1...P (j) = P (j + 1)Nj = kN 1 , kN 1 + 1, . . . , kN 1.(1)PSprobabilities P (j) also satisfy equationj=k0 P (j) = 1. Intuitively,steady-state, average flow state j state j + 1 equal averageflow state j + 1 state j (Gross & Harris, 1998). Since P (j) viewedlong-run proportion time system state j, mean flow state jj + 1 P (j) mean flow state j + 1 j P (j + 1)i 1N depending values switching points.equations, Berman et al. derive following expressions P (j):P (j) = j P (k0 ),(2)j=Xi =1 j = k0jk0,1 jki1Xiki1 + 1 j ki = 1, . . . , Ni1 kg kg11g=1(3)(4)g(X1 1), = 1, . . . , N.P (k0 ) calculated using following equation, derived summingsides Equation (2) values j:P (k0 )Xj = 1.j=0126(5)fiA CP Approach Queueing Control Problemquantities interest expressed terms probabilities P (j). Expectednumber workers front roomF=NXkiXiP (j),(6)i=1 j=ki1 +1expected number workers back roomB = N F.(7)expected number customers front roomL =XjP (j).(8)j=k0Expected waiting time queue expressedWq =L1.(1 P (kN ))(9)expression derived using Littles Laws system capacity kN = S.Given family switching policies K = {K; K = {k0 , k1 , ..., kN 1 , S}, ki integers,ki ki1 1, k0 0, kN 1 < S}, problem formally stated as:minimizeKK Wq(10)s.t. B BlPSj=k0 P (j) = 1equations (1), (6), (7), (8), (9).Berman et al. (2005) refer problem problem P1 . important note B,F L expected values real-valued. Consequently, constraint B Blstates expected number workers back room resulting realizationpolicy greater equal minimum expected number backroom workers needed complete work back room. particular time point,may, fact, fewer Bl workers back room.far aware, computational complexity problem P1determined. Berman et al. (p. 354) state solving problem P1 exactly extremelydifficult since constraints set (the detailed balance equations) changes policychanges.2.2.1 Berman et al.s HeuristicBerman et al. (2005) propose heuristic method solution problem.method based two corollaries theorem, stated provedauthors. results key understanding problem, are, therefore,repeated below.127fiTerekhov & BeckTheorem 2.1 (Berman et al.s Theorem 1) Consider two policies K K 0equal one ki . particular, suppose value kJ0 equals kJ 1,J set {0, ..., N 1} kJ kJ1 2, ki0 = ki 6= J. (a)Wq (K) Wq (K 0 ), (b) F (K) F (K 0 ), (c) B(K) B(K 0 ).result Theorem 2.1, seen two policies exist specialproperties. Firstly, consider policyK = {k0 = 0, k1 = 1, k2 = 2, ..., kN 1 = N 1, kN = S} .policy results largest possible F , smallest possible B Wq .policy yields smallest possible expected waiting time, optimal feasible.hand, smallest possible F largest possible Wq B obtainedapplying policyK = {k0 = N, k1 = N + 1, ..., kN 1 = 1, kN = S} .Therefore, policy infeasible, problem (10) infeasible also.Berman et al. propose notions eligible type 1 type 2 components. eligibletype 1 component switching point ki satisfying condition ki ki1 > 10 < < N ki > 0 = 0. switching point ki eligible type 2 componentki+1 ki > 1 0 < N . simply, eligible type 1 component ki variablewhich, decreased 1, still greater ki1 , eligible type 2 componentki variable which, increased 1, remain smaller ki+1 . Eligible type 1components eligible type 2 components referred simply type 1type 2 components, respectively.Based definitions policies K K, notions type 1 2 components,Theorem 2.1, Berman et al. propose heuristic, nameproblem used for, P1 :1. Start K = K.2. B(K) < Bl , problem infeasible. Otherwise, let imb Wq = Wq (K)imb K = K. Set J = N .3. Find smallest j s.t. 0 j < J kj type 1 component. jexists, go 5. Otherwise, set kj = kj 1. B(K) < Bl , set J = j go 5.B(K) Bl , go 4.4. Wq (K) < imb Wq , let imb Wq = Wq (K) imb K = K. Go 3.5. Find smallest j s.t. 0 j < J kj type 2 component. jexists, go 6. Otherwise, set kj = kj + 1. B(K) < Bl , repeat 5. B(K) Bl ,go 4.6. Stop return imb K best solution.128fiA CP Approach Queueing Control ProblemParameterNBlMeaningfront room capacitynumber workers facilityarrival rateservice rateexpected number workers required back roomTable 1: Summary problem parameters.NotationFBLWqMeaningexpected number workers front roomexpected number workers back roomexpected number customers front roomexpected customer waiting timeDefinitionEquation (6)Equation (7)Equation (8)Equation (9)Table 2: Summary quantities interest.Limiting choice j 0 J, resetting J every timeinfeasible policy found, prevents heuristic entering infinite cycle. heuristicguarantees optimality policy returns K K.Empirical results regarding performance heuristic P1 presentedpaper Berman et al. (2005). particular, clear close policies providedP1 optimal policies.2.3 Summary Parameters Quantities InterestBerman et al.s model problem P1 requires five input parameters (Table 1)expressions calculating four main quantities interest (Table 2),non-linear.3. Constraint Programming Modelswork done extending CP stochastic problems (Tarim, Manandhar,& Walsh, 2006; Tarim & Miguel, 2005; Walsh, 2002). problem differentproblems addressed papers stochastic information explicitlyencoded constraints expected values, need either stochasticvariables scenarios.known creating effective constraint programming model usually requiresone experiment various problem representations (Smith, 2006). Consequently,present, experiment with, three alternative models problem P1 (Equation (10)).Although first model based directly formulation Berman et al.,129fiTerekhov & Beckmodels motivated standard CP modelling techniques, use dualvariables (Smith, 2006). investigate following three CP models:If-Then model CP version formal definition Berman et al.PSums model uses slightly different set variables, constraintsbased closed-form expressions derived constraints usedIf-Then model.Dual model includes set dual decision variables addition variablesused If-Then PSums models. constraints modelexpressed terms dual variables.3.1 Common Model Componentsnumber modelling components common constraintmodels. presenting models, therefore present common aspects.3.1.1 Decision Variablesthree proposed models set decision variables ki , = 0, 1, . . . , N , representing switching policy. ki set domain [i, + 1, . . . , N + i]satisfy constraint ki < ki+1 (since number workers front room, i,increases number customers, ki , increases). Due Berman et al.s policydefinition, kN must equal S.3.1.2 Additional Expressionsmodels include variables constraints representation balance equations(Equation (1)), expressions F , expected number workers front room,L, expected number customers front room. However, representationsdiffer slightly depending model, noted inPSections 3.2, 3.3 3.4.kiset auxiliary variables, Sum(ki ), definedj=ki1 +1 j , 1N 1, included models (see Equations (2)(4) definition j ).necessary representing Equation (11), relates variables P (k0 ),floating point variable domain [0..1] representing probability k0 customersfacility. auxiliary variables constraint ensure assignmentdecision variables leads unique solution balance equations. discuss formaldefinition auxiliary variables Section 3.2.4.P (k0 )NXSum(ki ) = 1(11)i=0back room constraint, B Bl , stated models N F Bl . equationWq stated models Equation (9).3.2 If-Then Modelinitial model includes variables P (j) j = k0 , k0 + 1, . . . , k1 , k1 + 1, . . . , kN1, kN , representing steady-state probability j customers front130fiA CP Approach Queueing Control Problemminimize Wqsubjectki<ki+1 {0, 1, . . . , N 1};kN=S;(ki j ki+1 1)P (j) = P (j + 1)(i + 1),{0, 1, . . . , N 1}, j {0, 1, . . . , 1};(j < k0 )(P (j) = 0), j {0, 1, . . . , N 1};P (j)=1;(k0 = j)P (j)Xj=0NXSum(ki ) = 1,i=0j {0, 1, . . . , S};L=(ki1 + 1 j ki )XjP (j);j=0r(i, j) = iP (j),{1, 2, . . . , N }, j {0, 1, . . . , S};(ki1 + 1 > j j > ki )r(i, j) = 0,{1, 2, . . . , N }, j {0, 1, . . . , S};F=N XXr(i, j);i=1 j=0Wq=N FauxiliaryL1;(1 P (kN ))Bl ;constraints.Figure 1: Complete If-Then Modelroom. floating point variables domain [0..1] satisfy system balanceequations (Equation (1)) used express L F .complete If-Then model presented Figure 1.3.2.1 Balance Equation Constraintsbalance equations represented set if-then constraints. example, firstbalance equation, P (j) = P (j + 1) j = k0 , k0 + 1, ..., k1 1, representedconstraint (k0 j k1 1) P (j) = P (j + 1). Thus, somewhat inelegantly, if-then131fiTerekhov & Beckconstraint kind added j 0 1 (inclusive) orderrepresent one balance equation. order represent rest equations,technique applied pair switching points ki , ki+1 0 N 1.results total N if-then constraints.Pprobabilities P (j) also satisfy constraint Sj=k0 P (j) = 1. difficultyconstraint fact sum starts j = k0 , k0 decision variable.order deal complication, add meta-constraint ((j < k0 ) (P (j) = 0))j set {0, 1, . . . , N 1}.2 implies values P (j)jPless k0 0 allows us express sum-of-probabilities constraintj=0 P (j) = 1.3.2.2 Expected Number Workers Constraintsset if-then constraints also included order represent Equation (6)constraint model. due dependence constraint sumsvariables two switching points, decision variables. specifically,add set variables r(i, j) representing product P (j) jki1 + 1 ki , constraints (ki1 + 1 j ki ) r(i, j) = iP (j)(ki1 + 1 > j j > ki ) r(i, j) = 0 1 N j 0 S.total number if-then constraints 2N (S + 1). F simply statedsum indices j variables r(i, j).3.2.3 Expected Number Customers ConstraintL defined according Equation (8). Since meta-constraint ((j < k0 ) (P (j) = 0))added model order ensure P (j) = 0 j < k0 , constraintL simply stated sum products j P (j) j 0 S:L =XjP (j).(12)j=03.2.4 Auxiliary Variables Constraintsmodel includes set N 2 auxiliary expressions Xi 3 N (X1X2 always equal 1). Instead including j variables (refer Equation (2)definition j ), use N + 1 continuous auxiliary variables Sum(ki ) domain[0 . . . 1 + ], represent sums j variables j = ki1 + 1 j = ki(inclusive). Sum(k0 ) constrained equal 1, rest variables defined2. need add constraint j 0 upper bound domaink0 N .132fiA CP Approach Queueing Control Problemaccording Equation (13). validity equation proved appendix.! ki ki1ki1 k0 +1 11!6= 1XkX1Sum(ki ) =j =j=ki1 +1ki1 k0 +11Xi(ki ki1 ) otherwise.(13){1, . . . , N };PP Nexpressed Nsum kj=ki=0 Sum(ki ). requirement P (k0 )0 jPkNequal 1 stated set if-then constraints (k0 = j) P (j)0 jPj=kNi=0 Sum(ki ) = 1 j {0, 1, . . . , S}.summary, auxiliary constraints present model are: Sum(k0 ) =Qi1 1 kg kg11, Equation (13) Xi = g=1{3, . . . , N }.gIf-Then model includes total 3N + 2N + + 1 if-then constraints,often ineffective search propagation occurs either left-hand sidesatisfied right-hand side becomes false. Consequently, next model attemptsavoid, much possible, use constraints.3.3 PSums Modelsecond CP model based closed-form expressions derived balance equations(the details derivations provided appendix). set P (j) variablesformulation Berman et al. replaced set P Sums(ki ) variables= 0, . . . , N 1, together set probabilities P (j) j = k0 , k1 , k2 , . . . , kN . NoteP (j) defined switching point only, values {0, 1, . . . , S}.P Sums(ki ) variable represents sum probabilities ki ki+1 1.complete PSums model presented Figure 2. remainder sectionprovides details model.3.3.1 Probability ConstraintsBalance equations explicitly stated model. However, expressions P (ki )P Sums(ki ) derived way balance equations satisfied.P Sums(ki ) variables defined Equation (14). Equation (15) recursive formulacomputing P (ki+1 ).ki+1 1P Sums(ki ) =XP (j)j=ki133fiTerekhov & Beckki+1 ki1(i + 1)P (ki )(i+1)6= 1=1(i + 1)P (ki )(ki+1 ki )otherwise.P (ki+1 ) =(i + 1)ki+1 ki(14)P (ki ), {0, 1, . . . , N 1}.(15)PN 1P Sums(ki ) +Additionally, probability variables satisfy constraintP (kN ) = 1.i=03.3.2 Expected Number Workers ConstraintF , expected number workers front room, expressed terms P (ki )P Sums(ki ) shown Equation (16):F=NX[P Sums(ki1 ) P (ki1 ) + P (ki )].(16)i=13.3.3 Expected Number Customers Constraintsequation LL =N1XL(ki ) + kN P (kN )(17)i=0L(ki ) = ki P Sums(ki ) + P (ki )(i + 1)ki+1 ki 1ki+1 ki(ki ki+1 ) + (i+1)(ki+1 ki 1) + 1(i+1).21 (i+1)3.3.4 Auxiliary Constraintsauxiliary constraintsPare exactly If-Then model.However,PNNequal1statedP(k)Sum(krequirement P (k0 ) kj=k0) = 1,i=00 jrather set if-then constraints, model explicit closed-formexpression variable P (k0 ).3.4 Dual Modelproblem alternatively formulated using variables wj , represent numberworkers front room j customers present. wj variables134fiA CP Approach Queueing Control Problemminimize WqsubjectN1Xki<ki+1 {0, 1, . . . , N 1};kN=P (ki+1 )=S;P Sums(ki )=P Sums(ki )+P (k0 )ki+1 kiP (ki ),(i + 1){0, 1, . . . , N 1};ki+1 ki1(i + 1)P (ki )6= 1(i+1),1(i+1)P (ki )(ki+1 ki )otherwise{1, 2, . . . , N 1};P (kN ) = 1;i=0FL==NXi=0NXSum(ki ) = 1;[P Sums(ki1 ) P (ki1 ) + P (ki )] ;i=1N1XL(ki ) + kN P (kN ),i=0L(ki )=ki P Sums(ki ) + P (ki )(i+1)ki+1 ki 1(i + 1)(ki ki+1 ) +1(i+1)(i+1){0, 1, . . . , N 1};ki+1 ki2L1;(1 P (kN ))Bl ;Wq =N Fauxiliaryconstraints.Figure 2: Complete PSums Model135(ki+1 ki 1) + 1,fiTerekhov & Beckreferred dual variables because, compared ki s, roles variablesvalues reversed (Hnich, Smith, & Walsh, 2004; Smith, 2006). stated Smith,use dual variables constraint programming model beneficial constraintsproblem easier express using new variables. case problemfact, use dual variables allows us significantly reduce number if-thenconstraints necessary stating relations probabilities.Dual model, + 1 wj variables, domain [0, 1, . . . , N ].variables satisfy following equations: w0 = 0, wS = N wj wj+1j 0 1. Additionally, complete set ki variables included model,since constraints easier express using ki rather wj s.complete Dual model presented Figure 3, details discussed below.3.4.1 Probability ConstraintsGiven dual variables, balance equations restatedP (j) = P (j + 1)wj+1 , j {0, 1, . . . , 1}.(18)formulation balance equations avoids inefficient if-then constraints. restrestrictions probability variablesPSstated terms ki variables,If-Then model. particular, constraints j=0 P (j) = 1 ((k0 > j) (P (j) = 0))j {0, . . . , N 1} present model.3.4.2 Channelling Constraintsorder use redundant variables, set channelling constraints addedmodel ensure assignment values one set variables lead uniqueassignment variables set. following channelling constraints included:wj < wj+1 kwj = jj {0, 1, . . . , 1},(19)wj = wj+1 kwj 6= jj {0, 1, . . . , 1},(20)j {0, 1, . . . , S}, {1, . . . , N }.(21)wj = ki1 + 1 j kiConstraints (19) (20) redundant given constraint wj wj+1 . However,redundancy often lead increased propagation (Hnich et al., 2004). One directionfuture work examining effect removing one constraints mayperformance program.3.4.3 Expected Number Workers Constraintexpression expected number workers front room F =3.4.4 Expected Number Customers ConstraintPSj=0 wj P (j).constraint used express L identical one used If-Then model: L =Pj=0 jP (j). equation valid P (j) j < k0 constrained 0.136fiA CP Approach Queueing Control Problemminimize Wqsubjectw0 = 0,wS = Nwjwj+1 j {0, 1, . . . , 1};ki<ki+1 {0, 1, . . . , N 1};kN=S;wj < wj+1kwj = jj {0, 1, . . . , 1};wj = wj+1kwj 6= jwj =ki1 + 1 j kij {0, 1, . . . , 1};{1, 2, . . . , N }, j {0, 1, . . . , S};P (j)=P (j + 1)wj+1 j {0, 1, . . . , 1};(k0 = j)P (j)NXSum(ki ) = 1,j {0, 1, . . . , S};i=0(j < k0 )X(P (j) = 0),P (j)=1;F=j {0, 1, . . . , N 1};j=0Xwj P (j);j=0L=XjP (j);j=0Wq=N FauxiliaryL1;(1 P (kN ))Bl ;constraints.Figure 3: Complete Dual Model137fiTerekhov & BeckStatistic/Model# decision variables# probability variables# probability constraints# constraints F# constraints L# if-then constraintstotal # constraintsIf-ThenN +1S+1N (S 1) + 2S + 22N (S + 1) + 113N + 2N + + 13N + 4N + 2S + 6PSumsN +12N + 12N + 11N +106N + 5DualN +S +2S+13S N + 211S+1N + 3N + 6S + 8Table 3: Summary main characteristics three proposed CP models.3.4.5 Auxiliary Constraintsauxiliary constraints presentexactly If-ThenPkmodelNmodel. requirement P (k0 ) j=k0 j = 1 also stated set if-thenPconstraints (k0 = j) P (j) Ni=0 Sum(ki ) = 1 j {0, 1, . . . , S}.3.5 SummaryTable 3 presents summary number variables constraints threeproposed models. seen PSums model smaller number probabilityvariables constraints slightly larger number constraints representing Ltwo models, if-then constraints. Dual model larger numberdecision variables If-Then PSums models. imply searchspace bigger model two sets variables linked channellingconstraints assigned values via propagation. Dual allows simplestrepresentations F L, requiring one constraint. number probabilityconstraints Dual smaller equal number constraintsIf-Then model greater PSums model. However, actual representationconstraints straightforward Dual since neither requires if-thenconstraints closed-form expressions.hard determine, simply looking Table 3, modelsefficient since, CP, larger number constraints and/or variables may actually leadpropagation effective model (Smith, 2006). However, knownif-then constraints propagate well, and, since difference numberconstraints PSums model If-Then model Dual modelquite significant, one may expect PSums model advantagetwo models.fact, preliminary experiments three models showed poor performance (seeTable 4 Section 6). Due complexity constraints relating decision variablesvariables representing probabilities, little constraint propagation, and, essentially,search required explore entire branch-and-bound tree. consequence,following two sections examine dominance rules (Beck & Prestwich, 2004; Smith, 2005)shaving (Caseau & Laburthe, 1996; Martin & Shmoys, 1996), two stronger inference138fiA CP Approach Queueing Control Problemforms used CP. Section 8, investigate models without dominance rulesshaving need search whole tree order prove optimality, also discussdifferences performance models based experimental results.4. Dominance Rulesdominance rule constraint forbids assignments values variablesknown sub-optimal (Beck & Prestwich, 2004; Smith, 2005). problem P1 ,dominance rule states that, given feasible solution, K, solutionsleast one switching point assigned lower value value assigned K.words, given two solutions K K 0 , Wq value resulting policy K 0smaller equal Wq value resulting K, exist switchingpoints ki0 ki K 0 K, respectively, satisfying condition ki0 < ki . followingtheorem states dominance rule formally.0 )Theorem 4.1 (Dominance Rule) Let K = (k0 , k1 , . . . , kN ) K 0 = (k00 , k10 , . . . , kN0two policies k0 = k00 = 0, k1 = k10 = 1, . . . , kJ1 = kJ1= J 1 kJ 6= kJ00(i.e. least one kJ , kJ strictly greater J) J {0, 1, . . . , N 1}. LetWq (K) Wq (K 0 ) denote expected waiting times resulting two policies KK 0 , respectively. Wq (K 0 ) < Wq (K), exists {J, J + 1, J + 2, . . . , N 1}ki0 < ki .Proof: [By Contraposition] prove contrapositive statement Theorem4.1: assume exist {J, J + 1, . . . , N 1} ki0 < kishow that, given assumption, Wq (K 0 ) greater equalWq (K).Assume {J, J + 1, . . . , N 1} exists ki0 < ki . onefollowing true:(a) kn = kn0 n {J, J + 1, . . . , N 1},(b) exists least one j {J, J + 1, . . . , N 1} kj0 > kj ,values rest switching points two policies.Case (a) implies K 0 K policy, Wq (K) = Wq (K 0 ).prove (b), suppose exists exactly one j {J, J + 1, J + 2, . . . , N 1}kj0 > kj , kn = kn0 n {J, . . . , N 1} \ {j}. K K 0different value exactly one switching point. Consequently, Theorem 2.1,Wq (K 0 ) Wq (K). Similarly, applying Theorem 2.1 several times, resultgeneralizes cases exists one j kj0 > kj .Therefore, {J, J + 1, . . . , N 1} exists ki0 < ki , followsWq (K 0 ) Wq (K). words, Wq (K 0 ) < Wq (K), exists{J, J + 1, . . . , N 1} ki0 < ki .2, . . . , kTheorem 4.1 implies that, given feasible policy (0, 1, . . . , ki , ki+1N 1 , S)switching points index greater assigned values strictly greaterlower bounds, know solution smaller Wq satisfy constraint) . . . (k((ki < ki ) (ki+1 < ki+1N 1 < kN 1 )). Therefore, order implement139fiTerekhov & Beckdominance rule, add constraint search every time feasible policyfound, lead reduction size search space. Section 6 presentsexperimental results regarding usefulness technique.5. ShavingShaving inference method temporarily adds constraints model, performspropagation, soundly prunes variable domains based resulting stateproblem (Demassey, Artigues, & Michelon, 2005; van Dongen, 2006). example,simple shaving procedure may based assignment value variable x.propagation following assignment results domain wipe-out variable,assignment inconsistent, value removed domain x (Demasseyet al., 2005; van Dongen, 2006). general case, temporary constraintinferences made based complex. Shaving particularly usefuljob-shop scheduling domain, used reduce domains start endtimes operations (Caseau & Laburthe, 1996; Martin & Shmoys, 1996). problems,shaving used either domain reduction technique search, incorporatedbranch-and-bound search variable domains shaved decision (Caseau& Laburthe, 1996).shaving procedure problem, temporarily assign particular valueswitching point variable, rest variables assigned either maximumminimum possible values. Depending whether resulting policies feasibleinfeasible, new bounds switching point variables may derived.instance N = 3, = 6, = 15, = 3, Bl = 0.32 used illustrationpurposes. Policy K, always yields smallest possible Wq , instance(k0 , k1 , k2 , k3 ) = (0, 1, 2, 6) policy K, always yields greatest possible Wq ,(3, 4, 5, 6). Thus, initial domains switching points [0..3], [1..4], [2..5] [6]k0 , k1 , k2 k3 , respectively. step, shaving may able reduce domainsone variables.5.1 Bl -based Shaving Procedureinitial shaving procedure consists two cases either upper lowerbounds variables may modified. first case, constraint ki = min(ki ),min(ki ) smallest value domain ki , temporarily added problemparticular value 0 N . switching points assignedmaximum possible values using function gMax. Given array variables,function gMax assigns maximum possible values variables yetvalue, returning true resulting assignment feasible, false otherwise.maximum possible values necessarily upper bound values domainscorresponding variables, rather highest values domains respectcondition kn < kn+1 , n {0, ..., N 1}. example, k1 assignedvalue 1, rest variables unbound, gMax would result policy (0, 1, 5, 6),feasible B value 0.508992, thus true would returned.140fiA CP Approach Queueing Control ProblemRecall assignment infeasible yields B value smallerBl . policy resulting addition ki = min(ki ) use gMaxinfeasible, min(ki ) + 1 max(ki ), constraint ki > min(ki ) addedproblem: variables except ki set maximum values, probleminfeasible, feasible policy ki must greater min(ki ). reasoningvalid since Theorem 2.1 states increasing value switching point increaseB. Note solution feasible, recorded best-so-far solutionWq value smaller Wq value previous best policy. easier reference,part shaving procedure referred gMax case.second case, constraint ki = max(ki ) added problem0 N , max(ki ) maximum value domain ki . restvariables assigned minimum values domains using functiongMin. assignments made way respects constraints kn < kn+1 ,n {0, ..., N 1}. resulting policy feasible, constraint ki < max(ki )permanently added problem, assuming max(ki ) 1 min(ki ). Since variablesexcept ki minimum values already, ki maximum, must true,Theorem 2.1, better solution value ki smallermax(ki ). case referred gMin case.cases, inferred constraint violates current upper lower boundki , best policy found point optimal. Whenever domainswitching point modified result inferences made gMax gMin case,switching points need re-considered. domain one variable reducedparticular shaving iteration, temporary constraints added nextround shaving different ones used previously, and, consequently, newinferences may possible. Thus, shaving procedure terminates optimalityproved inferences made.Consider example mentioned above. Suppose constraint k0 = 0 addedproblem, rest variables assigned maximum possible values (usinggMax ). resulting policy (k0 , k1 , k2 , k3 ) = (0, 4, 5, 6). policy yields B value0.63171, implies policy feasible 0.63171 > 0.32 = Bl ,domain reductions inferred. constraint k0 = 0 removed. Sincedomain k0 modified, procedure considers next variable. Thus,constraint k1 = 1 added, variables set maximum values.resulting policy (0, 1, 5, 6), also feasible since B value 0.508992.constraint k1 = 1 removed, k2 = 2 added. variablesset maximum values, resulting policy (0, 1, 2, 6). policy yields B value0.1116577, smaller Bl . Thus, policy infeasible, constraintk2 > 2 added problem. changes domain k2 [3..5]. Wheneverdomain variable reduced, next shaving step considers switching point,next constraint added k2 = 3.Now, consider gMin case assume variables full initial domains.Suppose constraint k0 = 3 added problem. rest variablesassigned smallest possible values consistent k0 = 3. Thus, policy (3, 4, 5, 6)considered. policy B value 0.648305 feasible (it fact K,infeasible, problem would infeasible). value k0 better solution141fiTerekhov & Becksmaller 3, domains variables become [0..2], [1..4], [2..5],[6]. constraint k0 = 3 removed, and, since domain k0 modified,constraint k0 = 2 added next. policy considered (2, 3, 4, 6). policyalso feasible, domains become [0..1], [1..4], [2..5], [6]. temporary constraintk0 = 2 removed, next one added k0 = 1. corresponding policy assignedgMin infeasible, domain reductions made. Since addition k0 = 1result domain reductions, need reconsider variable k0switching points looked at. Consequently, next temporary constraintadded k1 = 4.complete Bl -based shaving procedure, start either gMingMax case. Since policies considered gMin case generally smaller waitingtime ones considered gMax case, may beneficial start gMincase. approach take.complete Bl -based shaving algorithm presented Figure 4. assumedalgorithms presented functions add(constraint) remove(constraint)add remove constraint model, respectively.Upon completion shaving procedure, constraint Wq bestWq ,bestWq value best solution found point, added (Wq bestWqrather Wq < bestWq added numerical issues testing equalityfloating point numbers). However, although constraint rules policies higherWq infeasible, results almost propagation domains decision variableslittle reduce size search tree. order remedy problem, anothershaving procedure, time based constraint Wq bestWq proposed nextsub-section. issue lack propagation domains ki additionconstraint discussed detail Section 8.1.5.2 Wq -based Shaving ProcedureWq -based shaving procedure makes inferences based strictly constraint WqbestWq : constraint B Bl removed prior running procedure ordereliminate possibility incorrect inferences. Bl -based shaving, constraintform ki = max(ki ), max(ki ) maximum value domain ki , addedtemporarily, function gMin used assign values rest variables.Bl constraint removed, reason infeasibilitypolicy Wq value greater best Wq encountered far.Since switching points except ki assigned smallest possible values, infeasibilityimplies solution smaller expected waiting time, value kistrictly smaller max(ki ). shaving procedure stated Figure 5.5.3 Combination Shaving ProceduresWq -based Bl -based shaving result different domain reductions sincebased two different constraints. Moreover, using two together may causedomain modifications either used itself. Therefore, makes sense runBl -based Wq -based shaving procedures alternately (with Wq Bl constraints added142fiA CP Approach Queueing Control ProblemAlgorithm 1: Bl -based ShavingInput: S, N , , , Bl (problem instance parameters); bestSolution (best solution foundfar)Output: bestSolution (possibly) modified domains variables ki , bestSolutionproof optimality(there domain changes)0 N 1(shaving successful Domain(ki ))add( ki = max(Domain(ki )) )(gMin)(new best solution found)bestSolution = currentSolution;( max(Domain(ki )) 1 min(Domain(ki )) )add( ki < max(Domain(ki )) )elsereturn bestSolution; stop, optimality provedremove( ki = max(Domain(ki )) )(shaving successful Domain(ki ))add( ki = min(Domain(ki )) )(gMax )(new best solution found)bestSolution = currentSolution;else( min(Domain(ki )) + 1 max(Domain(ki )) )add( ki > min(Domain(ki )) )elsereturn bestSolution; stop, optimality provedremove( ki = min(Domain(ki )) )Figure 4: Bl -based shaving algorithm143fiTerekhov & BeckAlgorithm 2: Wq -based ShavingInput: S, N , , , Bl (problem instance parameters); bestSolution (best solution foundfar)Output: bestSolution (possibly) modified domains variables ki , bestSolutionproof optimality(there domain changes)0 N 1(shaving successful Domain(ki ))add( ki = max(Domain(ki )) )(!gMin)( max(Domain(ki )) 1 min(Domain(ki )) )add( ki < max(Domain(ki )) )elsereturn bestSolution; stop, optimality provedremove( ki = max(Domain(ki )) )Figure 5: Wq -based shaving algorithmremoved appropriately) domain pruning possible. combinationtwo shaving procedures referred AlternatingShaving.AlternatingShaving procedure effectively combined search following manner. AlternatingShaving run initially, domain modificationspossible. Search performed better solution found, pointAlternatingShaving applied again. Subsequently, search shaving alternateone proves optimality best solution found. approach maysuccessful search finds new best solution, new constraint Wqadded, Wq -based shaving may able reduce upper bounds switchingpoint variables. way combining search shaving referredAlternatingSearchAndShaving.variations shaving also possible. particular, Bl -based Wq -basedshaving procedures extended make inferences values two switching points.example, one assign maximum values pair switching point variables,assigning minimum values rest. resulting policy feasible, constraintstating least one variable pair assigned smaller valueadded problem. Preliminary experiments indicated shaving procedures basedtwo switching points not, general, result effective models. proceduresexplicitly reduce domains switching point variables rather add setconstraints model appear, practice, significantly reduce sizesearch space. One possible direction future work may investigatevariations shaving.144fiA CP Approach Queueing Control Problem6. Experimental ResultsSeveral sets experiments3 performed order evaluate efficiency proposed models effectiveness dominance rules shaving procedures, wellcompare performance best CP model performance heuristic P1 .constraint programming models implemented ILOG Solver 6.2, heuristicBerman et al. implemented using C++.note numerical results obtained experiments sensitive levelprecision set. constraint programming models, set default ILOG Solverprecision 0.000001. implies floating point variables modelconsidered bound maximum (max) minimum (min) values intervals((max min)/(max{1, |min|}) 0.000001 (Solver, 2006). order propagate constraints involving floating point variables, Equation (15), ILOG Solver usesstandard interval arithmetic outward rounding.46.1 Problem Instancesinformation gained policies K K explicitly used implementationthree models. K infeasible, program stops feasible solutioninstance. Otherwise, K feasible, optimal. two cases Koptimal K infeasible therefore trivial solved easily CP modelsBerman et al.s heuristic. Although instances K optimal hardsolve without shaving, using elementary Bl -based shaving procedure always result(usually fast) proof optimality policy. case also trivial Bermanet al.s heuristic. Consequently, experimental results presented basedinstances optimal solution K K.Preliminary experiments indicated value significant impactefficiency programs since higher values result larger domains ki variablesmodels higher number wj variables Dual model. indicatedTable 3 Section 3.5, also big impact number constraints IfThen Dual models. Therefore, consider instances valueset {10, 20, . . . , 100} order gain accurate understanding performancemodel heuristic. note instances greater 100, neithermethod Berman et al.s heuristic P1 may used due numerical instability.Thirty instances generated way ensure instancefeasible optimal policy neither K K. created random combinationsparameter values chose instances policy K foundfeasible, optimal, K determined infeasible. order checkK optimal, sufficient find feasible solution one switchingpoint assigned value lower upper bound. generating combinationsparameters, values N chosen uniform probability set {2, . . . , 38},3. Numerical values results slightly different ones presented previouswork (Terekhov & Beck, 2007) due minor errors discovered publication paper.main conclusions analysis previous work remain valid, however.4. Jean-Francois Puget - personal communication.145fiTerekhov & Beckvalues {5,. . . , 99}, values {1, . . . , 49} values Bl{1, . . . , 4}. appears easy way determining whether given instanceK K optimal solution based parameter values. Moreover,preliminary experiments indicated problem difficulty depends combinationproblem parameters (especially S, N Bl ) rather one parameter only.10-minute time limit overall run-time program enforcedexperiments. experiments performed Dual Core AMD 270 CPU 1 MBcache, 4 GB main memory, running Red Hat Enterprise Linux 4.6.2 Performance Measuresorder perform comparisons among CP models, CP modelsBerman et al.s heuristic, look mean run-times, number instancesoptimal solution found, number instances optimality proved,number instances best-known solution found mean relativeerror (MRE). MRE measure solution quality allows one observe quicklyparticular algorithm able find good solution. MRE definedRE(a, ) =1 X c(a, m) c (m)|M |c (m)(22)mMalgorithm used solve problem, set problem instancesalgorithm tested, c(a, m) cost solution found instancealgorithm a, c (m) best-known solution instance m. generatedinstances, c (m) best solution found experiments.6.3 Comparison Constraint Programming Models TechniquesCP model tested without shaving dominance rules. total 30CP-based methods therefore evaluated. model Bl -based shaving modelruns Bl -based shaving procedure domain changes possible, addsconstraint value Wq based best solution found shaving procedure runs search rest time. Similarly, models Wq -based shavingAlternatingShaving models run Wq -based shaving procedure AlternatingShaving procedure, respectively, longer possible reduce domainsswitching point variables, add constraint requiring Wq less expectedwaiting time best solution found shaving procedure use searchrest time. described previously, AlternatingSearchAndShaving alternatessearch AlternatingShaving procedure. models, search assigns switching pointsincreasing index order. smallest value domain variable tried first.6.3.1 Constraint Programming ModelsTable 4 presents number instances, 300, optimal solutionfound proved 30 proposed CP-based methods. table indicatesPSums model effective three, proving optimality largestnumber instances regardless use dominance rules shaving. Alter146fiA CP Approach Queueing Control ProblemShavingIf-ThenPSumsDual105126105ND105126105Bl -basedShavingND192 191202 201191 191Wq -basedShavingND105 105126 126105 105AlternatingShavingND219218225225218218AlternatingSearchAndShavingND234234238238232232Table 4: Number instances optimal solution found optimalityproved within 10 CPU-minutes total 300 problem instances (D -dominance rules, ND - without dominance rules).natingSearchAndShaving, PSums proves optimality largest number instances:79.3% instances, 238 239 instances optimality provedmodel.Figure 6 shows MRE changes first 50 seconds run-time If-Then,PSums Dual models AlternatingSearchAndShaving, Bermans heuristic(we comment performance heuristic Section 6.4). PSums is, average,able find better solutions two models given amount run-time.Table 5, additional statistics regarding performance three modelsAlternatingSearchAndShaving without dominance rules presented (we commentstatistics P1 Section 6.4). particular, model, numberinstances finds best solution (out 300), number instancesfinds optimal solution (out 239 cases optimality proved)number times proves optimality (out 300) presented. seenmodels find optimal solution 239 instances known. However,PSums model proves optimality 4 instances If-Then model 6instances Dual. PSums also finds best-known solution algorithm97.6% instances considered. detailed discussion differencesperformance CP models presented Section 8.2.6.3.2 Shaving ProceduresTable 4, observed CP models without shaving Wq based shaving procedure prove optimality fewest number cases. similarityperformance models without shaving Wq -based shaving surprisingWq -based procedure able start pruning domains valuebest policy found prior procedure quite good. Wq -based procedureused alone, one solution base inferences on, namely K. Since policiesresult smaller expected waiting time K, procedure useless.Employing Bl -based shaving procedure substantially improves performancemodels: without dominance rules, If-Then, PSums Dual models proveoptimality 86, 75 86 instances, respectively, corresponding modelswithout shaving Wq -based shaving; dominance rules, situation147fi0.10Terekhov & Beck0.060.040.000.02Mean Relative Error0.08IfThen ModelPSums ModelDual ModelBerman et al.s Heuristic P101020304050Run Time (seconds)Figure 6: Comparison MRE three CP models AlternatingSearchAndShavingBermans heuristic P1 .equivalent. results imply inferences made based Bl constraint effectivereducing domains decision variables.Models AlternatingShaving AlternatingSearchAndShaving perform even better models employing Bl -based shaving procedure. real power Wq based shaving becomes apparent combined Bl -based shavingBl -based shaving often finds good solution good value bestWq , allowingWq -based procedure infer domain reductions. observation explains AlternatingSearchAndShaving performs better AlternatingShaving. particular, AlternatingSearchAndShaving, Wq -based procedure used new best solution148fiA CP Approach Queueing Control ProblemPSumsIf-ThenDualP1# best found (/300)293282279282# opt. found (/239)239239239238# opt. proved (/300)2382342320Table 5: Comparison three CP models (with AlternatingSearchAndShaving withoutdominance rules) Bermans heuristic P1 .found shaving one found search. Therefore, higher qualitysolution found search, used Wq -based procedure prunedomains switching point variables.Figure 7, average run-times value 10 100 presentedfour shaving procedures PSums model. Since run-time limit 600 secondsused throughout experiments, assumed run-time 600 seconds instancesoptimality proved within limit. Therefore, mean run-timesreported throughout paper underestimates true means. Figure 7 shows that,value S, AlternatingSearchAndShaving procedure gives best performance.also seen that, increases, becomes increasingly difficult prove optimalityaverage run-times increase. stated previously, due larger domainsswitching point variables. AlternatingSearchAndShaving procedure, however, ablesignificantly reduce domains ki variables therefore provides effectivemethod instances higher values well.6.3.3 Dominance RulesTable 4 indicates rarely difference number instances solvedoptimality models without dominance rules. difference visiblemodel without shaving, Wq -based shaving AlternatingSearchAndShaving.Recall dominance rules implemented addition constraintvalues switching point variables solution found. constrainteffective switching point variables assigned minimum valuescurrent solution. Usually, policies also ones result smallerexpected waiting time. Similarly, Wq -based shaving useful solutionsmall expected waiting time found. leads conjecture dominance rulesmay effective instances Wq -based shaving procedureeffective. conjecture supported results Table 4. particular,Wq -based shaving procedure used itself, makes inferences based policyK, solution generally poorest quality instance. method singlerun Wq -based shaving therefore heavily relies search. Since search takes long timefind feasible solution good quality, effectiveness dominance rule-based constraintsalso visible within given time limit.149fi200300400500Wqbased ShavingBlbased ShavingAlternatingShavingAlternatingSearchAndShaving0100Average RunTimes (seconds)600Terekhov & Beck20406080100ValuesFigure 7: PSums model various shaving techniques: average run-times valueS. Average run-times PSums without shaving shown graphsince resulting curve would indistinguishable one Wq -basedshaving.hand, AlternatingSearchAndShaving, Wq -based procedure playskey role makes domain reductions based high quality solutions producedBl -based shaving, later, search. Dominance rules play role proceduresince shaving used every new solution found. However, even dominance ruleconstraints explicitly incorporated procedure (i.e. addednew run search), would redundant since serve essentiallypurpose Wq -based shaving procedure.shaving used, results equivalent achieved Wq -basedshaving employed. explanation absence difference models150fi1 e031 e071 e05PSums Model 10 secondsPSums Model 150 secondsPSums Model 500 secondsBerman et al.s Heuristic P11 e09Mean Relative Error1 e01CP Approach Queueing Control Problem20406080100ValuesFigure 8: MRE value P1 P Sums model.without dominance rules therefore also same. particular, takes long timesolution found whose quality allows dominance rule constrainteffectively reduce size search tree.Bl -based shaving AlternatingShaving used, dominance rules sometimes helpful. cases, two shaving procedures, subsequentsearch usually finds good solution quickly, and, since Wq -based shaving usedpoint, dominance rule constraint added effective reducingsize search tree.Overall, observed using AlternatingSearchAndShaving without dominancerules effective using Bl -based shaving AlternatingShaving dominance151fiTerekhov & Beckrules. Therefore, comparisons, focus modelsAlternatingSearchAndShaving without dominance rules.6.4 Heuristic P1 vs. Best Constraint Programming ApproachEmpirical results regarding performance heuristic P1 presented Bermanet al. (2005), ability P1 find good switching policies explicitlyevaluated previous work. wanted find well heuristic actually performscomparing CP methods.Table 5, present several measures performance three proposed modelsAlternatingSearchAndShaving heuristic P1 . heuristic performs well,finding best-known solution eleven fewer instances PSums model,three instances Dual model number instancesIf-Then model. Moreover, heuristic finds, but, course, cannot prove, optimalsolution 238 239 instances optimal known. three CP modelsfind optimal solution 239 these. run-time heuristic negligible,whereas mean run-time PSums model approximately 130 seconds (the meanrun-times two models slightly higher: 141 seconds If-Then model149 seconds Dual model).Table 5 also shows PSums model able find best-known solution 11instances heuristic. Closer examination reveals 275 instancesPSums model P1 find best-known solution, 18 instancesPSums able 7 heuristic finds best-known.Figure 6, observed heuristic achieves small MREnegligible amount time. 50 seconds run-time, MRE 300 instancesresulting PSums AlternatingSearchAndShaving becomes comparableheuristic MRE. Figure 8, MRE 30 instances value presentedheuristic PSums AlternatingSearchAndShaving 10, 150 500seconds run-time. 10 seconds, performance PSums comparableheuristic values smaller equal 40, heuristic appearsquite bit better higher values S. 150 seconds, performance PSumscomparable heuristic except values 50 80. 500 seconds,PSums smaller MRE 300 instances also lower (or equal) MREvalue except 50 100.Overall, results indicate heuristic performs wellits run-time negligible,finds optimal solution one cases known, findsbest solution 94% instances. Moreover, results low MRE. AlthoughPSums AlternatingSearchAndShaving able achieve slightly higher numbersperformance measures, clear improvements small givenPSums run-time much higher run-time heuristic.7. PSums-P1 HybridNaturally, desirable create method would able find solution highquality short amount time, Bermans heuristic, would alsohigh rate able prove optimality within reasonable run-time152fiA CP Approach Queueing Control ProblemPSumsP1PSums-P1Hybrid# best found (/300)293282300# opt. found (/239)239238239# opt. proved (/300)2380238Table 6: Comparison PSums model AlternatingSearchAndShaving Berman etal.s heuristic P1 Hybrid model.PSums AlternatingSearchAndShaving. therefore worthwhile experimentPSums-P1 Hybrid, starts running P1 then, assuming instancefeasible, uses PSums model AlternatingSearchAndShaving find better solutionprove optimality solution found P1 (infeasibility instance provedheuristic determines policy K infeasible).Since shown heuristic P1 fast, running first incurs almostoverhead. Throughout analysis experimental results, also notedperformance Wq -based shaving procedure depends quality best solutionfound used. shown heuristic provides solutions highquality. Therefore, first iteration Wq -based procedure may able significantlyprune domains switching point variables good-quality solution foundheuristic. Continuing alternating two shaving techniques search,also shown effective approach, lead good results.proposed Hybrid algorithm tested set 300 instancesused above. Results illustrating performance Hybrid well performanceP1 PSums AlternatingSearchAndShaving presented Table 6. Hybridable find best solution 300 cases: 275 instances heuristicPSums find best-known solution, 18 PSums finds best-known7 heuristic so. Hybrid finds optimal solution (forinstances known) proves optimality many instances PSumsmodel. mean run-time Hybrid essentially identical mean run-timePSums AlternatingSearchAndShaving, equalling approximately 130 seconds.Thus, Hybrid best choice solving problem: finds good solutionheuristic little time (close 0 seconds), able prove optimalitymany instances best constraint programming method, finds best-knownsolution instances considered. Moreover, improvements achieved withoutincrease average run-time PSums model.8. Discussionsection, examine reasons poor performance CP modelswithout shaving, suggest reasons observed differences among CP models, discussperformance Hybrid present perspectives work.153fiTerekhov & Beck8.1 Lack Back-Propagationexperiments, instances even PSums-P1 HybridAlternatingSearchAndShaving unable find prove optimal solution within10-minute time limit. fact, many instances, amount time spentsearch higher time spent shaving, run-time limit usually reachedsearch, rather shaving, phase. analysis algorithmsbehaviour suggests poor performance search explained lack backpropagation. Back-propagation refers pruning domains decision variablesdue addition constraint objective function: objective constraintpropagates back decision variables, removing domain values reducing search.CP models presented above, little back-propagation.Consider model without shaving. Throughout search, new best solution found,constraint Wq bestWq , bestWq new objective value, addedmodel. However, domains switching point variables usually reducedway addition constraint. illustrated observingamount propagation occurs PSums model Wq constrained.example, consider instance problem = 6, N = 3, = 15, = 3,Bl = 0.32 (this instance used Section 5 illustrate shaving procedures).initial domains switching point variables [0..3], [1..4], [2..5] [6]. initialdomains probability variables P (ki ) i, addition Wq boundsprovided K K, listed Table 7. initial domain Wq , also determinedobjective function values K K, [0.22225..0.425225]. initial domains L7F , [2.8175e ..6] [0..2.68], respectively. Upon addition constraintWq 0.306323, 0.306323 known optimal value instance, domainWq reduced [0.22225..0.306323], domain L becomes [1.68024..6]domain F remains [0..2.68]. domains P (ki ) addition listedTable 7. domains types probability variables reduced additionnew Wq constraint. However, domains switching point variables remainunchanged. Therefore, even though policies value Wq less 0.306323infeasible, constraining Wq less equal value resultreduction search space. still necessary search policies ordershow better feasible solution exists.One reasons lack pruning domains ki variables dueWq constraint likely complexity expression Wq = (1PL(kN )) 1 . exampleabove, Wq constrained less equal 0.306323, get constraint1L0.306323 15(1P(kN )) 3 , implies 9.594845(1P (kN )) L. explainsdomains L P (kN ) change upon addition model. domainsrest P (ki ) variables change relationships P (ki )s (Equation(15)) constraint sum probability variables 1.Similarly, domains P Sums(ki )s change variables expressedterms P (ki ) (Equation (14)). However, actual ki variables mostly occurexponents expressions P Sums(ki ), P (ki ), L(ki ), minor changes domainsP Sums(ki ), P (ki ), L(ki ) happen due constraint Wq effectdomains ki . analysis suggests may interesting investigate CP154fiA CP Approach Queueing Control Problemjk0k1k2k3addition Wq 0.306323P (j)P Sums(j)[4.40235e6 ..0.979592][0..1]7[1.76094e ..1][0..1][2.8175e8 ..0.6][2.8175e8 ..1][4.6958e8 ..1]N/Aaddition Wq 0.306323P (j)P Sums(j)[4.40235e6 ..0.979592][0..0.683666][0.000929106..1][0..0.683666][0.0362932..0.578224] [0.0362932..0.71996][0.28004..0.963707]N/ATable 7: Domains P (j) P Sums(j) variables j = k0 , k1 , k2 , k3 ,addition constraint Wq 0.306323.model based log-probabilities rather probabilities themselves. modelmay lead stronger propagation.Likewise, If-Then Dual models, domains decision variablesreduced bound objective function value added, although domainsprobabilities, L F modified. models, constraints relating F , Lprobability variables variables ki balance equations, quitecomplex. domains probability variables seem reduced significantlyenough due new Wq bound result pruning ki domainsconstraints.observations served motivation proposed shaving techniques.particular, Wq -based shaving procedure reduces domains switching point variablesshown values necessarily result higher Wq valuebest one found point. makes lack back-propagation.However, even procedure used new best solution found,AlternatingSearchAndShaving, always able prune enough values domainski able prove optimality within 10 minutes run-time.therefore seen inferences based value Wq limited powerand, therefore, domains switching point variables large shaving,possible prove optimality short period time.8.2 Differences Constraint Programming ModelsExperimental results demonstrate PSums model best three modelswithout shaving. section, examine models detailattempt understand reasons differences.8.2.1 Comparison PSums two modelsorder analyze performance models without shaving, look meannumber choice points statistics, give indication size search spaceexplored. compare three models, look mean number choicepoints considered first feasible solution found mean total numberchoice points explored within 600 seconds run-time.155fiTerekhov & BeckIf-ThenPSumsDual21 Instances Solved PSumsFirst SolutionTotal823413759264154649287528102408105 Instances Solved ModelsFirst SolutionTotal120137596106436590113236842Table 8: Mean number choice points explored first solution found meantotal number choice points explored within 600 seconds three modelswithout shaving without dominance rules. PSums, latter statisticcorresponds total number choice points needed prove optimality.Table 4, shown that, without shaving, PSums model proves optimality21 instances two models 105 instancesthree models prove optimality. Table 8, present mean number choice pointsstatistics three models sets instances. seenmean number choice points need explored PSums model orderfind initial solution smaller two models, 105 instanceseventually solved optimality models 21 instancessolved optimality PSums. variable value ordering heuristicsused models, observation implies propagation occurs searchPSums model two models. claim supportedfact mean total number choice points 105 instances solvedmodels smaller PSums two models.Table 8 also shows that, 21 instances solved optimalityPSums, mean total number choice points highest PSums model. SincePSums one three models solve instances, impliespropagation happening faster model. observation confirmed results105 instances solved three models: instances, Dualexplores average 713 choice points per second, If-Then model explores average895 choice points per second PSums model explores average 1989 choicepoints per second. words, appears propagation PSums modeltwice fast two models.detailed examination results showed 82 105 instancessolved models, number choice points explored, given instance,models. Moreover, instances value N ,number choice points explored equal. Figure 9, run-times three modelsnumber choice points increases illustrated. order create graph,averaged run-times instances number choice points examinedsame. points figure labeled (S, N ) order show relationshipnumber choice points, values N , run-times. noteone instance, = 10 N = 6, number choice pointsinstances = 10 N = 4. However, instances156fiA CP Approach Queueing Control Problem(70,3)300(40,4)(30,5)200(60,3)(20,9)(20,8)100RunTime (seconds)400IfThen ModelPSums ModelDual Model(50,3)(20,6)0(70,2)(20,7)050000100000150000Choice PointsFigure 9: Run-times averaged instances equal number choice points explored, 82 instances number choice pointsmodels. labels points indicate (S, N ) values correspondinginstances.(out 82), one-to-one correspondence (S, N ) number choicepoints.Several observations made Figure 9. Firstly, graph demonstratespropagation PSums model faster models. Secondly, behaviourPSums model appears quite different If-ThenDual models. run-times PSums model seem significantly influencedvalue N . example, = 20, run-times model increase N increases157fiTerekhov & Beck6 9. Moreover, given two instances one high low N ,low high N , PSums model generally needs longer timesolve instance low high N (e.g., compare points (20, 7) (40, 4),(20, 7) (70, 3)). If-Then Dual models, several casesrun-times instances high low N higher instanceslow high N (e.g., compare run-time (70, 3) (20, 7)), althoughopposite happens well (e.g., compare (50, 3) (40, 4)). Thus, appears Nparameter influencing run-times PSums most, two models,N influential, greater effect. Although characteristicsrequire additional investigation, one possible reason differences model behaviourcould relationship number constraints models problemparameters. Table 3, known number constraints mostly determinedvalue If-Then Dual models (since typically larger N ),value N PSums model. Combining observations Table 3 Figure9, appears effect N run-times due influencenumber constraints models.Overall, examination indicates superiority PSums model withoutshaving caused stronger propagation (Table 8) fact propagationfaster (Figure 9).shaving employed, PSums model also performs better DualIf-Then models, proving optimality greater number instances (Table 4) findinggood-quality solutions faster (Figure 6). models, shaving procedures makenumber domain reductions shaving based Wq Bl constraints,present models. However, time shaving iteration takesdifferent different models. empirical results show iteration shavingtakes smaller amount time PSums model If-Then Dualmodels. Thus, shaving, PSums model performs better two,shaving faster subsequent search, necessary, faster.8.2.2 Comparison If-Then Dual modelscomparison If-Then model AlternatingSearchAndShaving DualAlternatingSearchAndShaving using Figure 6 shows If-Then model usually ablefind good solutions smaller amount time. Moreover, shown Table 5,If-Then model AlternatingSearchAndShaving finds best solution threeinstances, proves optimality two instances, Dual modelshaving procedure. shaving procedures without shaving, statisticsshow almost difference performance two models. expectedDual would outperform If-Then model uses simpler representationbalance equations expressions F L, smaller number if-thenconstraints. (there Table 8 shows Dual explore smaller numberchoice points find initial solution. 105 instances modelssolve, total number choice points explored Dual also smaller. However,If-Then model faster, exploring, average, 895 choice points per second comparedaverage 713 choice points per second explored Dual. One possible explanation158fiA CP Approach Queueing Control ProblemDual slower fact assign variables (via propagation)models. particular, order represent switching policy, Dualassign + 1 wj variables addition N + 1 ki variables, usually muchlarger N .8.3 Performance PSums-P1 HybridExperimental results demonstrate PSums-P1 Hybrid finds good solutions quicklyable prove optimality large number instances. noted however,synergy results combination: number instances optimalityproved increase run-times decrease. Moreover, hybrid modelfinds best-known solution test instances simply casesPSums model heuristic able so. new best solutionsobtained using PSums-P1 Hybrid solve problem. appears startingPSums model solution found heuristic lead significantincrease amount propagation. Also, fact heuristic finds good-qualitysolution improve overall performance since, search used, placingconstraint Wq requires solutions better quality little effectdomains decision variables. observations imply order createeffective model problem, one would need improve back-propagationadding new constraints reformulating existing ones. back-propagation improved,good-quality heuristic solution may result better performance hybrid approach.8.4 PerspectivesCP methods developed are, ways, non-standard. commonapproaches faced poor results three basic CP models (without shaving)would create better models, develop global constraint could representefficiently reason relevant sub-structure problem, and/or inventsophisticated variable- value-ordering heuristics. Shaving proceduraltechnique must customized exploit particular problem structure. contrast,better model creation global constraint in-line declarative goalsCP. decision investigate shaving arose recognition needtightly link optimization function decision variables clear structureproblem appeared proved ideal shaving.believe scope better models novel global constraints. Modellingproblem P1 using CP straightforward formulation proposed Bermanet al. contains expressions Equation (6), upper lower limits sumauxiliary variables decision variables. constraints typical problemsusually modelled solved CP appear existing global constraintscould used facilitate approach. spite issues, modelsdemonstrate CP flexible enough support queueing constraints. However,believe likely generalized application CP solve larger class queueingcontrol problems require global constraints specific expressions commonly occurringqueueing theory. Given back-propagation analysis fact problem159fiTerekhov & Beckfind prove optimality, doubtful that, P1 , sophisticated search heuristicsperform significantly better simple heuristics.first time CP used solve queueing controlproblem first time instances P1 provably solved optimality,work paper viewed somewhat narrow: demonstrationparticular queueing control problem solved constraint programming techniques.work immediately deliver solutions general problems, however,believe open number directions inquiry problems.1. appears standard method within queueing theory address queueing control optimization problems. first application opens issue whetherCP become approach choice problems.2. noted Section 1, increasing interest incorporating reasoninguncertainty CP-based problem solving. Queueing theory provide formulations allow direct calculation stochastic quantities based expectation.challenge CP identify common sub-structures formulationsdevelop modelling, inference, search techniques exploit them.3. Challenging scheduling problems, staff rostering call centres (Cezik &LEcuyer, 2008), consist queues well rich resource temporal constraints(e.g., multiple resource requirements, alternative resources different speeds, taskdeadlines, precedence relations tasks). believe integrationCP queueing theory could prove promising approach problems.4. ability reason resource allocation uncertainty importantcomponent definitions intelligent behaviour bounded rationality (Simon,1997). cannot claim made significant contribution direction,perhaps ideas queueing theory serve inspiration contributionsfuture.9. Related Work Possible ExtensionsSeveral papers exist deal similar types problems one considered here.example, Berman Larson (2004) study problem switching workerstwo rooms retail facility customers front room divided twocategories, shopping store checkout. Palmer Mitrani(2004) consider problem switching computational servers different typesjobs randomness user demand may lead unequal utilization resources.Batta, Berman Wang (2007) study problem assigning cross-trained customerservice representatives different types calls call centre, depending estimateddemand patterns type call. three papers provide examples problemsCP could prove useful approach. Investigating CP solutions problemstherefore one possible direction future work.work may also include looking extensions problem discussedpaper. example, may consider realistic problem resourceconstraints one rooms, workers varying productivity.160fiA CP Approach Queueing Control ProblemAnother direction work improvement proposed models. particular, models, especially PSums, constraints variable exponents.One idea improving performance constraints explicitly representdifferences switching points (i.e., ki+1 ki ) variables.5 Another idea investigate model based logarithms probabilities rather probabilitiesthemselves. Additionally, ways increasing amount back-propagation needexamined.goal paper demonstrate applicability constraint programmingsolving particular queueing control problem. main direction future work is,therefore, explore possibility integrating CP queueing theoryattempt address stochastic scheduling resource allocation problems. problemslikely involve complex constraints, encoding necessary stochastic information stating typical scheduling requirements task precedences resourcecapacities. Combining queueing theory CP may help solving problems.10. Conclusionspaper, constraint programming approach proposed problem findingoptimal states switch workers front room back room retail facility stochastic customer arrival service times. first workaware examines solving stochastic queueing control problems usingconstraint programming. best pure CP method proposed able prove optimalitylarge proportion instances within 10-minute time limit. Previously, existednon-heuristic solution problem aside naive enumeration. resultexperiments, hybridized best pure CP model heuristic proposedproblem literature. hybrid technique able achieve performanceequivalent to, better than, individual approaches alone: ablefind good solutions negligible amount time due use heuristic,able prove optimality large proportion problem instances within 10 CPU-minutesdue CP model.paper demonstrates constraint programming good approach solving queueing control optimization problem. queueing problems optimalityimportant heuristics perform well, CP may prove effective methodology.Appendix A. Constraint Derivationssection, derivations constraints PSums model expressionsauxiliary variables constraints presented.A.1 Closed-form Expressions PSums modelPSums model two sets probability variables, P (ki ), = 0, 1, . . . , N , probability ki customers front room, P Sums(ki ), = 0, 1, . . . , N 1,sum probabilities two switching point variables. Balance equations5. Thanks anonymous reviewer suggestion.161fiTerekhov & Beckexplicitly stated model. However, expressions P (ki ) P Sums(ki ) derived way balance equations satisfied. technique usedderivations similar used Berman et al. (2005) simplify calculationprobabilities.Consider balance equation P (j) = P (j + 1)i, true j = ki1 , ki1 +1, . . . , ki 1 {1, 2 . . . , N }. particular, subset balance equationsP (ki1 ) = P (ki1 + 1)iP (ki1 + 1) = P (ki1 + 2)i...P (ki 1) = P (ki )i.equations imply following expressions:P (ki1 )P (ki1 + 1)= P (ki1 + 1)(23)= P (ki1 + 2)...P (ki 1)= P (ki ).Combining together, get P (ki ) =P (ki+1 ) =(i + 1)ki+1 kiki ki1P (ki1 ) 1 N ,P (ki ), {0, 1, . . . , N 1}.(24)equation included PSums model previously stated Equation(15).Similarly, Equation (23), see P (ki1 + 1) =P (ki1 ) 1N ,P (ki + 1) =P (ki ), {0, 1, . . . , N }.(i + 1)(25)Using Equation (25), expression P Sums(ki ), sum probabilities P (j) jki ki+1 1, derived follows:ki+1 1P Sums(ki ) =XP (j)j=ki= P (ki ) + P (ki + 1) + P (ki + 2) + . . . + P (ki+1 1)2= P (ki ) + P (ki )+ P (ki )(i + 1)(i + 1)162fiA CP Approach Queueing Control Problemki+1 ki 1+ . . . + P (ki )(i + 1)"2ki+1 ki 1 #++ ... += P (ki ) 1 +(i + 1)(i + 1)(i + 1)ki+1 ki1(i + 1)P (ki )6= 1(i+1)=1(i + 1)P (ki )(ki+1 ki )otherwise.(26)+last step derivation based observation expression [1 + (i+1)2ki+1 ki 1+ . . . + (i+1)] geometric series common ratio (i+1).(i+1)1, expression simply sum ki+1 ki ones. expression(i+1)P Sums(ki ) previously stated Equation (14).A.1.1 Expected Number Workers ConstraintF expressed terms P (ki ) P Sums(ki ) using following sequence steps:F=kiXNXiP (j)i=1 j=ki1 +1==NXi=1NX[P (ki1 + 1) + P (ki1 + 2) + . . . + P (ki 1) + P (ki )][P Sums(ki1 ) P (ki1 ) + P (ki )].(27)i=1A.1.2 Expected Number Customers Constraintsequation L derived similar manner:L =kNXjP (j)j=k0=kX1 1j=k0jP (j) +kX2 1jP (j) + . . . +j=k1kXN 1jP (j) + kN P (kN )j=kN1= L(k0 ) + L(k1 ) + . . . + L(kN 1 ) + kN P (kN )=N1XL(ki ) + kN P (kN )(28)i=0163fiTerekhov & BeckL(ki ) = ki P (ki ) + (ki + 1)P (ki + 1) + (ki + 2)P (ki + 2) + . . . + (ki+1 1)P (ki+1 1)= ki P (ki ) + ki P (ki + 1) + ki P (ki + 2) + . . . + ki P (ki+1 1) + P (ki + 1)+ 2P (ki + 2) + . . . + (ki+1 ki 1)P (ki+1 1)= ki [P (ki ) + P (ki + 1) + P (ki + 2) + . . . + P (ki+1 1)] + P (ki + 1)+ 2P (ki + 2) + . . . + (ki+1 ki 1)P (ki+1 1)2+ 2P (ki )= ki P Sums(ki ) + P (ki )(i + 1)(i + 1)ki+1 ki 1+ . . . + (ki+1 ki + 1)P (ki )(i + 1)= ki P Sums(ki ) + P (ki )(i + 1)"21+2+ ...+3(i + 1)(i + 1)ki+1 ki 2 #+ (ki+1 ki 1)P (ki )(i + 1)= ki P Sums(ki ) + P (ki )(i + 1)ki+1 ki 1Xn=0n(i + 1)n1(29)= ki P Sums(ki ) + P (ki )(i + 1)ki+1 ki 1ki+1 ki(ki ki+1 ) + (i+1)(ki+1 ki 1) + 1(i+1).21 (i+1)A.2 Auxiliary Variablesconstraint programming models contain Equation (13) (restated Equation(30)) defining Sum(ki ) variables, necessary expressing auxiliaryconstraint ensures balance equations unique solution. validityequation proved following derivation, uses formula sumfinite geometric series last step:Sum(ki ) =kiXjj=ki1 +1ki1 +1k0 ki1 +1ki11Xiki1 +2k0 ki1 +2ki1ki k0 ki ki111+Xi + . . . +Xi=164fiA CP Approach Queueing Control Problemki1 k0 +11= Xi"2 2k1 k0 (ki1 k0 +1) ki ki1 1 #111++ ... +1+ki1 k0 +1 ki ki1 1X1n= Xin=0!ki ki11ki1 k0 +1 1!6= 1X1=ki1 k0 +11Xi(ki ki1 ) otherwise.(30)Acknowledgments research supported part Natural SciencesEngineering Research Council ILOG, S.A. Thanks Nic Wilson Ken Browndiscussions comments work, Tom Goguen careful proofreadingfinal copy. preliminary version parts work previously published(Terekhov & Beck, 2007).ReferencesBaptiste, P., Le Pape, C., & Nuijten, W. (2001). Constraint-based Scheduling. KluwerAcademic Publishers.Batta, R., Berman, O., & Wang, Q. (2007). Balancing staffing switching costsservice center flexible servers. European Journal Operational Research, 177,924938.Beck, J. C., & Prestwich, S. (2004). Exploiting dominance three symmetric problems.Fourth International Workshop Symmetry Constraint Satisfaction Problems.Beck, J. C., & Wilson, N. (2007). Proactive algorithms job shop schedulng probabilistic durations. Journal Artificial Intelligence Research, 28, 183232.Berman, O., & Larson, R. (2004). queueing control model retail services backroom operations cross-trained workers. Computers Operations Research,31 (2), 201222.Berman, O., Wang, J., & Sapna, K. P. (2005). Optimal management cross-trained workersservices negligible switching costs. European Journal Operational Research,167 (2), 349369.165fiTerekhov & BeckBrown, K. N., & Miguel, I. (2006). Uncertainty change. Rossi, F., van Beek, P.,& Walsh, T. (Eds.), Handbook Constraint Programming, chap. 21, pp. 731760.Elsevier.Caseau, Y., & Laburthe, F. (1996). Cumulative scheduling task intervals. Proceedings Joint International Conference Symposium Logic Programming,pp. 363377. MIT Press.Cezik, M. T., & LEcuyer, P. (2008). Staffing multiskill call centers via linear programmingsimulation. Management Science, 54 (2), 310323.Demassey, S., Artigues, C., & Michelon, P. (2005). Constraint-propagation-based cuttingplanes: application resource-constrained project scheduling problem. INFORMS Journal Computing, 17 (1), 5265.Fox, M. S. (1983). Constraint-Directed Search: Case Study Job-Shop Scheduling. Ph.D.thesis, Carnegie Mellon University, Intelligent Systems Laboratory, Robotics Institute, Pittsburgh, PA. CMU-RI-TR-85-7.Gross, D., & Harris, C. (1998). Fundamentals Queueing Theory. John Wiley & Sons,Inc.Hnich, B., Smith, B. M., & Walsh, T. (2004). Dual modelling permutation injectionproblems. Journal Artificial Intelligence Research, 21, 357391.Martin, P., & Shmoys, D. B. (1996). new approach computing optimal schedulesjob shop scheduling problem. Proceedings Fifth Conference IntegerProgramming Combinatorial Optimization, pp. 389403.Palmer, J., & Mitrani, I. (2004). Optimal server allocation reconfigurable clustersmultiple job types. Proceedings International Conference ComputationalScience Applications (ICCSA04), pp. 7686.Policella, N., Smith, S. F., Cesta, A., & Oddi, A. (2004). Generating robust schedulestemporal flexibility. Proceedings Fourteenth International ConferenceAutomated Planning Scheduling (ICAPS04), pp. 209218.Simon, H. A. (1997). Models Bounded Rationality, Vol. 3. MIT Press.Smith, B. M. (2005). Modelling constraint programming. Lecture NotesFirst International Summer School Constraint Programming. Available at:http://www.math.unipd.it/frossi/cp-school/.Smith, B. M. (2006). Modelling. Rossi, F., van Beek, P., & Walsh, T. (Eds.), HandbookConstraint Programming, chap. 11, pp. 377406. Elsevier.Solver (2006). ILOG Scheduler 6.2 Users Manual Reference Manual. ILOG, S.A.Sutton, A. M., Howe, A. E., & Whitley, L. D. (2007). Using adaptive priority weightingdirect search probabilistic scheduling. Proceedings Seventeenth International Conference Automated Planning Scheduling, pp. 320327.Tadj, L., & Choudhury, G. (2005). Optimal design control queues. TOP, 13 (2),359412.166fiA CP Approach Queueing Control ProblemTarim, S. A., Manandhar, S., & Walsh, T. (2006). Stochastic constraint programming:scenario-based approach. Constraints, 11 (1), 5380.Tarim, S. A., & Miguel, I. (2005). hybrid Benders decomposition method solvingstochastic constraint programs linear recourse.. Joint ERCIM/CoLogNETInternational Workshop Constraint Solving Constraint Logic Programming,pp. 133148.Terekhov, D. (2007). Solving queueing design control problems constraint programming. Masters thesis, Department Mechanical Industrial Engineering,University Toronto.Terekhov, D., & Beck, J. C. (2007). Solving stochastic queueing control problemconstraint programming. Proceedings Fourth International ConferenceIntegration AI Techniques Constraint Programming CombinatorialOptimization Problems (CPAIOR07), pp. 303317. Springer-Verlag.van Dongen, M. R. C. (2006). Beyond singleton arc consistency. ProceedingsSeventeenth European Conference Artificial Intelligence (ECAI06), pp. 163167.Walsh, T. (2002). Stochastic constraint programming. Proceedings FifteenthEuropean Conference Artificial Intelligence, pp. 111115.167fiJournal Artificial Intelligence Research 32 (2008) 37-94Submitted 08/07; published 05/08Extended RDF Semantic FoundationRule Markup LanguagesAnastasia Analytianalyti@ics.forth.grInstitute Computer Science, FORTH-ICS, Crete, GreeceGrigoris Antoniouantoniou@ics.forth.grInstitute Computer Science, FORTH-ICS, Crete, GreeceDepartment Computer Science, University Crete, GreeceCarlos Viegas Damasiocd@di.fct.unl.ptCentro de Inteligencia Artificial, Universidade Nova de Lisboa,Caparica, PortugalGerd WagnerG.Wagner@tu-cottbus.deInstitute Informatics, Brandenburg UniversityTechnology Cottbus, GermanyAbstractOntologies automated reasoning building blocks Semantic Web initiative. Derivation rules included ontology define derived concepts, basedbase concepts. example, rules allow define extension class property, basedcomplex relation extensions classes properties.hand, inclusion negative information form negation-asfailure explicit negative information also needed enable various forms reasoning.paper, extend RDF graphs weak strong negation, well derivationrules. ERDF stable model semantics extended framework (Extended RDF)defined, extending RDF(S) semantics. distinctive feature theory, basedPartial Logic, truth falsity extensions properties classesconsidered, allowing truth value gaps. framework supports closed-worldopen-world reasoning explicit representation particular closed-world assumptions ERDF ontological categories total properties total classes.1. Introductionidea Semantic Web describe meaning web data way suitableautomated reasoning. means descriptive data (meta-data) machine readable form stored web used reasoning. Due distributedworld-wide nature, Web creates new problems knowledge representation research.Berners-Lee (1998) identifies following fundamental theoretical problems: negationcontradictions, open-world versus closed-world assumptions, rule systems Semantic Web. time being, first two issues circumvented discardingfacilities introduce them, namely negation closed-world assumptions. Thoughweb ontology language OWL (McGuinness & van Harmelen, 2004), based Description Logics (DLs) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003),includes form classical negation class complements, form limited.c2008AI Access Foundation. rights reserved.fiAnalyti, Antoniou, Damasio, & Wagnerbecause, achieve decidability, classes formed based specific class constructorsnegation properties fully considered. Rules constitute next layerontology languages Semantic Web and, contrast DL, allow arbitrary interactionvariables body rules. widely recognized need rules Semantic Web, demonstrated Rule Markup Initiative1 , restarted discussionfundamentals closed-world reasoning appropriate mechanisms implementrule systems.RDF(S)2 recommendation (Klyne & Carroll, 2004; Hayes, 2004) provides basic constructs defining web ontologies solid ground discuss issues.RDF(S) special predicate logical language restricted existentially quantified conjunctions atomic formulas, involving binary predicates only. Due purpose,RDF(S) number special features distinguish traditional logic languages:1. uses special jargon, things universe discourse called resources, types called classes, binary predicates called properties. Likebinary relations set theory, properties domain range. Resourcesclassified help property rdf :type (for stating resource typec, c class).2. distinguishes special sort resources, called literal values, denotationslexical representations strings, numbers, dates, basic datatypes.3. Properties resources, is, properties also elements universe discourse. Consequently, possible state properties properties, i.e., make statements predicates.4. resources, except anonymous ones literal values, named helpglobally unique reference schema, called Uniform Resource Identifier (URI)3 ,developed Web.5. RDF(S) comes non-standard model-theoretic semantics developed Pat Hayesbasis idea Christopher Menzel, allows self-application withoutviolating axiom foundation. example provable sentence statingrdfs:Class, class classes, instance itself.However, RDF(S) support negation rules. Wagner (1991) arguesdatabase, knowledge representation system, needs two kinds negation, namely weaknegation (expressing negation-as-failure non-truth) strong negation (expressingexplicit negative information falsity) able deal partial information.subsequent paper, Wagner (2003) makes also point Semantic Web,framework knowledge representation general. present paper, makeargument Semantic Web language RDF show extendedaccommodate two negations Partial Logic (Herre, Jaspars, & Wagner, 1999),well derivation rules. call new language Extended RDF denote ERDF.1. http://www.ruleml.org/2. RDF(S) stands Resource Description Framework (Schema).3. http://gbiv.com/protocols/uri/rfc/rfc3986.html38fiExtended RDF Semantic Foundation Rule Markup Languagesmodel-theoretic semantics ERDF, called ERDF stable model semantics, developedbased Partial Logic (Herre et al., 1999).Partial Logic, relating strong weak negation interpretation level allowsdistinguish four categories properties classes. Partial properties properties pmay truth-value gaps truth-value clashes, p(x, y) possibly neither truefalse, true false. Total properties properties p satisfy totalness,p(x, y) true false (but possibly both). Coherent properties properties psatisfy coherence, p(x, y) cannot true false. Classical properties totalcoherent properties. classical properties p, classical logic law applies: p(x, y)either true false. Partial, total, coherent, classical classes c defined similarly,replacing p(x, y) rdf :type(x, c).Partial logic also allows distinguish properties (and classes) completely represented knowledge base not. classificationproperty completely represented owner knowledge base:owner must know properties complete informationnot. Clearly, case completely represented (closed ) property p, entailmentp(x, y) allows derive p(x, y), underlying completeness assumption alsocalled Closed-World Assumption (CWA) AI literature.completeness assumption closing partial property p default mayexpressed ERDF means rule p(?x, ?y) p(?x, ?y) partial classc, means rule rdf :type(?x, c) rdf :type(?x, c). derivation rulescalled default closure rules. case total property p, default closure rulesapplicable. because, considered interpretations satisfy p(x, y)rest p(x, y)4 , preventing preferential entailment p(x, y). Thus, totalproperties, Open-World Assumption (OWA) applies. Similarly first-order-logic,order infer negated statements total properties, explicit negative informationsupplied, along ordinary (positive) information.example, consider ERDF knowledge base KB contains facts:interestedIn(Anastasia, SemanticWeb)interestedIn(Grigoris, Robotics)indicating Anastasia interested SemanticWeb area Grigoris interestedRobotics area. Then, statement interestedIn(Anastasia, Robotics) satisfiedsingle intended model KB . Thus, KB entails interestedIn(Anastasia, Robotics).Assume previous list areas interest complete AnastasiaGrigoris. Then, add knowledge base KB statement:rdf :type(interestedIn, erdf :TotalProperty)indicating interestedIn total property. case, open-world assumption made interestedIn KB entail interestedIn(Anastasia, Robotics),longer. particular, intended model revised KB satisfiesinterestedIn(Anastasia, Robotics). course, known Anastasia interestedRobotics interestedIn(Anastasia, Robotics) added KB .Assume add KB following facts:4. total properties p, Law Excluded Middle p(x, y)p(x, y) applies.39fiAnalyti, Antoniou, Damasio, & WagnerhasCar (Anastasia, Suzuki )hasCar (Grigoris, Volvo)assume KB complete knowledge property hasCar , far concernselements Herbrand Universe KB . Then, default closure rule hasCar (?x , ?y)hasCar (?x , ?y) safely added KB . result, hasCar (Anastasia, Volvo)satisfied intended models KB . Thus, KB entails hasCar (Anastasia, Volvo).previous example shows need supporting closed-world open-worldreasoning framework. Damasio et al. (2006) Analyti et al. (2004) provideexamples arguments need. Unfortunately, classical logic thus alsoOWL support open-world reasoning.Specifically, paper:1. extend RDF graphs ERDF graphs inclusion strong negation,ERDF ontologies (or ERDF knowledge bases) inclusion generalderivation rules. ERDF graphs allow express existential positive negativeinformation, whereas general derivation rules allow inferences based formulas builtusing connectives , , , , quantifiers , .2. extend vocabulary RDF(S) terms erdf :TotalPropertyerdf :TotalClass, representing metaclasses total properties total classes,open-world assumption applies.3. extend RDFS interpretations ERDF interpretations including truthfalsity extensions properties classes. Particularly, consider coherentERDF interpretations (imposing coherence properties). Thus, paper,total properties classes become synonymous classical properties classes.4. extend RDF graphs ERDF formulas built positive triples, usingconnectives , , , , quantifiers , . Then, define ERDFentailment two ERDF formulas, extending RDFS entailment RDFgraphs.5. define ERDF models, Herbrand interpretations, minimal Herbrandmodels ERDF ontology. Since minimal Herbrand models ERDFontology intended, define stable models ERDF ontology. definitionstable model based intuition that:(a) assertions stating property p class c total accepted,ontology contains direct support form acceptablerule sequence,(b) assertions []p(s, o) []rdf :type(o, c) accepted, (i) ontology contains direct support form acceptable rulesequence, (ii) property p class c total, respectively.6. show stable model entailment ERDF ontologies extends ERDF entailmentERDF graphs, thus also extends RDFS entailment RDF graphs. Moreover, show properties total, (boolean) Herbrand model reasoningstable model reasoning coincide. case, make open-world assumptionproperties classes.40fiExtended RDF Semantic Foundation Rule Markup Languagesdistinctive feature developed framework respect Partial Logic (Herreet al., 1999) properties classes declared total selective basis,extending RDF(S) new built-in classes providing support respective ontological categories. contrast, Partial Logic (Herre et al., 1999), choice partialtotal taken complete set predicates. Thus, approach presentedis, respect, flexible general.work extends conference paper (Analyti, Antoniou, Damasio, & Wagner, 2005)(i) considering full RDFS model, (ii) providing detailed characterizationproperties ERDF interpretations/models, Herbrand interpretations/models, finallyERDF stable models, (iii) discussing decidability issues, (iv) providing formal proofslemmas propositions.rest paper organized follows: Section 2, extend RDF graphsERDF graphs ERDF formulas. Section 3 defines ERDF interpretations ERDFentailment. show ERDF entailment extends RDFS entailment. Section 4,define ERDF ontologies Herbrand models ERDF ontology. Section 5,define stable models ERDF ontology. Section 6 defines stable model entailment,showing extends ERDF entailment. Section 7, provide brief sketchERDF/XML syntax. Decidability issues ERDF stable model semantics discussedSection 8. Section 9 shows developed ERDF model theory seenTarski-style model theory. Section 10 reviews related work Section 11 concludespaper, including future work. main definitions RDF(S) semantics reviewedAppendix A. Appendix B includes proofs lemmas propositions, presentedpaper.2. Extending RDF Graphs Negative Informationsection, extend RDF graphs ERDF graphs, adding strong negation. Moreover, extend RDF graphs ERDF formulas, built positive ERDF triples,connectives , , , , , quantifiers , .According RDF concepts (Klyne & Carroll, 2004; Hayes, 2004), URI referencesused globally unique names web resources. RDF URI reference Unicode string represents absolute URI (with optional fragment identifier).may represented qualified name, colon-separated two-part string consisting namespace prefix (an abbreviated name namespace URI) localname. example, given namespace prefix ex defined stand namespace URI http://www.example.org/, qualified name ex:Riesling (which standshttp://www.example.org/Riesling) URI reference.plain literal string s, sequence Unicode characters, pairstring language tag t, denoted s@t. typed literal pair stringdatatype URI reference d, denoted sd. example, 27xsd:integertyped literal.(Web) vocabulary V set URI references and/or literals (plain typed).denote set URI references URI, set plain literals PL, settyped literals L, set literals LIT . holds: URI LIT = .41fiAnalyti, Antoniou, Damasio, & Wagnerformalization, consider set Var variable symbols, sets Var ,URI, LIT pairwise disjoint. main text, variable symbols explicitly indicated,examples, variable symbols prefixed question mark symbol ?.RDF triple (Klyne & Carroll, 2004; Hayes, 2004) triple p o.,URI Var , p URI, URI LIT Var , expressing subject relatedobject property p. RDF graph set RDF triples.variable symbols appearing RDF graph called blank nodes, are, intuitively,existentially quantified variables. paper, denote RDF triple p o.p(s, o). extend notion RDF triple allow positive negativeinformation.Definition 2.1 (ERDF triple) Let V vocabulary. positive ERDF triple V(also called ERDF sentence atom) expression form p(s, o), s, V Varcalled subject 5 object, respectively, p V URI called predicate property.negative ERDF triple V strong negation p(s, o) positive ERDF triplep(s, o) V . ERDF triple V (also called ERDF sentence literal ) positivenegative ERDF triple V .example, ex:likes(ex:Gerd , ex:Riesling) positive ERDF triple, expressingGerd likes Riesling, ex:likes(ex:Carlos, ex:Riesling) negative ERDF triple, expressing Carlos dislikes Riesling. Note RDF triple positive ERDFtriple constraint subject triple literal. example,ex:denotationOf (Grigoris, ex:Grigoris) valid ERDF triple valid RDF triple.choice allowing literals appearing subject position based intuitioncase naturally appear knowledge representation (as previous example). Prudhommeaux & Seaborne (2008) de Bruijn et al. (2005) also consider literalssubject position RDF triples.Based notion ERDF triple, define ERDF graphs ERDF formulas,follows:Definition 2.2 (ERDF graph) ERDF graph G set ERDF triplesvocabulary V . denote variables appearing G Var (G), set URIreferences literals appearing G VG .Note RDF graph set RDF triples (Klyne & Carroll, 2004; Hayes, 2004),RDF graph also ERDF graph.Definition 2.3 (ERDF formula) Let V vocabulary. consider logical factors{, , , , , , }, , , called strong negation, weak negation,material implication, respectively. denote L(V ) smallest set containspositive ERDF triples V closed respect following conditions:F, G L(V ) {F, F, F G, F G, F G, xF, xF } L(V ), x Var .ERDF formula V element L(V ). denote set variables appearing5. Opposed pure RDF (Klyne & Carroll, 2004), allow literals subject position ERDFtriple.42fiExtended RDF Semantic Foundation Rule Markup LanguagesF Var (F ), set free variables6 appearing F FVar (F ). Moreover,denote set URI references literals appearing F VF .example, let:F = ?x ?y (rdf :type(?x, ex:Person) ex:hasChild (?y, ?x)) rdf :type(?z, ex:Person)Then, F ERDF formula vocabulary V = {rdf :type, ex:Person, ex:hasChild }Var (F ) = {?x, ?y, ?z} FVar (F ) = {?z}.denote sublanguages L(V ) formed means subset logicalfactors, L(V |S). example, L(V |{}) denotes set (positive negative) ERDFtriples V .3. ERDF Interpretationssection, extend RDF(S) semantics allowing partial properties classes.particular, define ERDF interpretations satisfaction ERDF formula, basednotion partial interpretation.3.1 Partial Interpretationsdefine partial interpretation extension simple interpretation (Hayes, 2004),property associated truth extension also falsityextension allowing partial properties. notation P(S), set, denotespowerset S.Definition 3.1 (Partial interpretation) partial interpretation vocabulary Vconsists of:non-empty set resources ResI , called domain universe I.set properties P ropI .vocabulary interpretation mapping IV 7 : V URI ResI P ropI .property-truth extension mapping PT : P ropI P(ResI ResI ).property-falsity extension mapping PF : P ropI P(ResI ResI ).mapping ILI : V L ResI .set literal values LV ResI , contains V PL.define mapping: : V ResI P ropI , called denotation, that:I(x) = IV (x), x V URI.I(x) = x, x V PL.I(x) = ILI (x), x V L.6. Without loss generality, assume variable cannot free bound occurrencesF , one bound occurrence.7. symbol IV , V stands Vocabulary.43fiAnalyti, Antoniou, Damasio, & WagnerNote truth falsity extensions property p according partial interpretation I, P TI (p) P FI (p), sets pairs hsubject, objecti resources.example, let:V = {ex:Carlos, ex:Grigoris, ex:Riesling, ex:likes, ex:denotationOf , Grigorisxsd:string}consider structure consists of:set resources ResI = {C, G, R, l, d, Grigoris}.set properties P ropI = {l, d}.vocabulary interpretation mapping IV : V URI ResI P ropI that:IV (ex:Carlos) = C, IV (ex:Grigoris) = G, IV (ex:Riesling) = R, IV (ex:likes) = l,IV (ex:denotationOf ) = d.property-truth extension mapping PT : P ropI P(ResI ResI ) that:P TI (d) = {hGrigoris, Gi}.property-falsity extension mapping PF : P ropI P(ResI ResI ) that:P FI (l) = {hC, Ri}.mapping ILI : V L ResI that: ILI (Grigorisxsd :string) = Grigoris.set literal values LV = {Grigoris}.easy see partial interpretation V , expressing that: (i) Grigorisdenotation Grigoris (ii) Carlos dislikes Riesling.Definition 3.2 (Coherent partial interpretation) partial interpretation vocabulary V coherent iff x P ropI , PT (x) PF (x) = .Coherent partial interpretations enforce constraint pair resources cannotbelong truth falsity extensions property (i.e., properties coherent). Intuitively, means ERDF triple cannot true false.Continuing previous example, note coherent partial interpretation.Consider partial interpretation J exactly I, except also holds:P TJ (l) = {hC, Ri} (expressing Carlos likes Riesling). Then, hC, Ri belongstruth falsity extension l (i.e., hC, Ri P TJ (l) P FJ (l)). Thus, J coherent.define satisfaction ERDF formula w.r.t. partial interpretation, need firstfollowing auxiliary definition.Definition 3.3 (Composition partial interpretation valuation) Letpartial interpretation vocabulary V let v partial function v : Var ResI(called valuation). define: (i) [I + v](x) = v(x), x Var , (ii) [I + v](x) = I(x),x V .Definition 3.4 (Satisfaction ERDF formula w.r.t. partial interpretationvaluation) Let F, G ERDF formulas let partial interpretationvocabulary V . Additionally, let v mapping v : Var (F ) ResI .F = p(s, o) I, v |= F iff p V URI, s, V Var , I(p) P ropI ,h[I + v](s), [I + v](o)i PT (I(p)).44fiExtended RDF Semantic Foundation Rule Markup LanguagesF = p(s, o) I, v |= F iff p V URI, s, V Var , I(p) P ropI ,h[I + v](s), [I + v](o)i PF (I(p)).F = G I, v |= F iff VG V I, v 6|= G.F = F1 F2 I, v |= F iff I, v |= F1 I, v |= F2 .F = F1 F2 I, v |= F iff I, v |= F1 I, v |= F2 .F = F1 F2 then8 I, v |= F iff I, v |= F1 F2 .F = x G I, v |= F iff exists mapping u : Var (G) ResI u(y) = v(y),Var (G) {x}, I, u |= G.F = x G I, v |= F iff mappings u : Var (G) ResI u(y) = v(y),Var (G) {x}, holds I, u |= G.cases ERDF formulas treated following DeMorgan-style rewrite rulesexpressing falsification compound ERDF formulas:(F G) F G, (F G) F G, (F ) F, ( F ) F 9 ,(x F ) x F, (x F ) x F, (F G) F G.Continuing previous example, let v : {?x, ?y, ?z} ResI v(?x) = C,v(?y) = R, v(?z) = G. holds:I, v |= ex:likes(?x, ?y) ex:denotationOf (Grigorisxsd:string, ?z).Definition 3.5 (Satisfaction ERDF formula w.r.t. partial interpretation)Let F ERDF formula let partial interpretation vocabulary V . saysatisfies F , denoted |= F , iff every mapping v : Var (F ) ResI , holdsI, v |= F.Continuing previous example, |= ?x ex:likes(ex:Carlos, ?x).define ERDF graph satisfaction, extending satisfaction RDF graph(Hayes, 2004) (see also Appendix A).Definition 3.6 (Satisfaction ERDF graph w.r.t. partial interpretation) LetG ERDF graph let partial interpretation vocabulary V . Let vmapping v : Var (G) ResI . define:I, v |=GRAPH G iff G, I, v |= t.satisfies ERDF graph G, denoted |=GRAPH G, iff exists mappingv : Var (G) ResI I, v |=GRAPH G.Intuitively, ERDF graph G represents existentially quantified conjunctionERDF triples. Specifically, let G = {t1 , ..., tn } ERDF graph, let Var (G) ={x1 , ..., xk }. Then, G represents ERDF formula formula(G) = ?x1 , ..., ?xk t1 ... tn .shown following lemma.8. Material implication logical relationship two ERDF formulas either firstnon-true second true.9. transformation expresses false F hold F holds.45fiAnalyti, Antoniou, Damasio, & WagnerLemma 3.1 Let G ERDF graph let partial interpretation vocabularyV . holds: |=GRAPH G iff |= formula(G).Following RDF terminology (Klyne & Carroll, 2004), variables ERDFgraph also called blank nodes intuitively denote anonymous web resources.example, consider ERDF graph:G = {rdf :type(?x, ex:EuropeanCountry), rdf :type(?x, ex:EU member)}.Then, G represents ERDF formula formula(G) =?x (rdf :type(?x, ex:EuropeanCountry) rdf :type(?x, ex:EU member)),expressing European country European Union member.Notational Convention: Let G ERDF graph, let partial interpretationvocabulary V , let v mapping v : Var (G) ResI . Due Lemma 3.1,write (by abuse notation) I, v |= G |= G instead I, v |=GRAPH G|=GRAPH G, respectively.3.2 ERDF Interpretations Entailmentsubsection, define ERDF interpretations entailment extension RDFSinterpretations entailment (Hayes, 2004). First, define vocabularies RDF,RDFS, ERDF.vocabulary RDF, VRDF , set URI references rdf : namespace (Hayes,2004), shown Table 1. vocabulary RDFS, VRDF , set URI referencesrdfs: namespace (Hayes, 2004), shown Table 1. vocabulary ERDF , VERDF ,set URI references erdf : namespace. Specifically, set ERDF predefinedclasses CERDF = {erdf :TotalClass, erdf :TotalProperty}. define VERDF = CERDF .Intuitively, instances metaclass erdf :TotalClass classes c satisfy totalness,meaning resource belongs truth falsity extension c. Similarly, instancesmetaclass erdf :TotalProperty properties p satisfy totalness, meaningpair resources belongs truth falsity extension p.ready define ERDF interpretation vocabulary V extensionRDFS interpretation (Hayes, 2004) (see also Appendix A), propertyclass associated truth extension also falsity extension, allowingpartial properties partial classes. Additionally, ERDF interpretation givesspecial semantics terms ERDF vocabulary.Definition 3.7 (ERDF interpretation) ERDF interpretation vocabulary Vpartial interpretation V VRDF VRDF VERDF , extended new ontologicalcategories ClsI ResI classes, TCls ClsI total classes, TProp P ropItotal properties, well class-truth extension mapping CT : ClsI P(ResI ),class-falsity extension mapping CF : ClsI P(ResI ), that:1. x CT (y) iff hx, yi PT (I(rdf :type)),x CF (y) iff hx, yi PF (I(rdf :type)).46fiExtended RDF Semantic Foundation Rule Markup LanguagesVRDFrdf :typerdf :Propertyrdf :XMLLiteralrdf :nilrdf :Listrdf :Statementrdf :subjectrdf :predicaterdf :objectrdf :firstrdf :restrdf :Seqrdf :Bagrdf :Altrdf : i, {1, 2, ...}rdf :valueVRDFrdfs:domainrdfs:rangerdfs:Resourcerdfs:Literalrdfs:Datatyperdfs:Classrdfs:subClassOfrdfs:subPropertyOfrdfs:memberrdfs:Containerrdfs:ContainerMembershipPropertyrdfs:commentrdfs:seeAlsordfs:isDefinedByrdfs:labelTable 1: vocabulary RDF RDFS2. ontological categories definedP ropI = CT (I(rdf :Property))ResI = CT (I(rdfs:Resource))TCls = CT (I(erdf :TotalClass))follows:ClsI = CT (I(rdfs:Class))LV = CT (I(rdfs:Literal))TProp = CT (I(erdf :TotalProperty)).3. hx, yi PT (I(rdfs:domain)) hz, wi PT (x) z CT (y).4. hx, yi PT (I(rdfs:range)) hz, wi PT (x) w CT (y).5. x ClsI hx, I(rdfs:Resource)i PT (I(rdfs:subClassOf )).6. hx, yi PT (I(rdfs:subClassOf )) x, ClsI , CT (x) CT (y),CF (y) CF (x).7. PT (I(rdfs:subClassOf )) reflexive transitive relation ClsI .8. hx, yi PT (I(rdfs:subPropertyOf )) x, P ropI , PT (x) PT (y),PF (y) PF (x).9. PT (I(rdfs:subPropertyOf )) reflexive transitive relation P ropI .10. x CT (I(rdfs:Datatype)) hx, I(rdfs:Literal)i PT (I(rdfs:subClassOf )).11. x CT (I(rdfs:ContainerMembershipProperty))hx, I(rdfs:member)i PT (I(rdfs:subPropertyOf )).12. x TCls CT (x) CF (x) = ResI .13. x TProp PT (x) PF (x) = ResI ResI .47fiAnalyti, Antoniou, Damasio, & Wagner14. srdf :XMLLiteral V well-typed XML literal string,ILI (srdf :XMLLiteral ) XML value s,ILI (srdf :XMLLiteral ) CT (I(rdf :XMLLiteral )).15. srdf :XMLLiteral V ill-typed XML literal stringILI (srdf :XMLLiteral ) ResI LV ,ILI (srdf :XMLLiteral ) CF (I(rdfs:Literal)).16. satisfies RDF RDFS axiomatic triples (Hayes, 2004), shown Table 2 Table 3Appendix A, respectively.17. satisfies following triples, called ERDF axiomatic triples:rdfs:subClassOf (erdf :TotalClass, rdfs:Class).rdfs:subClassOf (erdf :TotalProperty, rdfs:Class).Note RDFS intepretations (Hayes, 2004) imply two-valued interpretationinstances rdf :Property, longer case ERDF interpretations.Specifically, let ERDF interpretation, let p CTI (I (rdf :Property)), let hx, yiResI ResI . may case neither hx, yi P TI (p) hx, yi P FI (p).p(x, y) neither true false.Semantic conditions ERDF interpretations may impose constraints truthfalsity extensions properties classes. Specifically, consider semantic condition 6Definition 3.7 assume hx, yi PT (I(rdfs:subClassOf )). Then,satisfy CT (x) CT (y) (as RDFS interpretation does), also CF (y) CF (x).latter true certain resource z belong truthextension class certain z belong truth extension classx. Thus, falsity extension contained falsity extension x. Similarcase semantic condition 8. Semantic conditions 12 13 represent definitiontotal classes total properties, respectively. Semantic condition 15 expressesdenotation ill-typed XML literal literal value. Therefore (see semantic condition 2), certain contained truth extension class rdfs:Literal.Thus, contained falsity extension class rdfs:Literal.Let coherent ERDF interpretation vocabulary V . Since I(rdf :type) P ropI ,holds: x ClsI , CT (x) CF (x) = . Thus, properties classes coherentERDF interpretations coherent.Convention: rest document, consider coherent ERDF interpretations.means referring ERDF interpretation, implicitly mean coherentone. Moreover, improve readability examples, ignore examplenamespace ex:.According RDFS semantics (Hayes, 2004), source RDFS-inconsistencyappearance ill-typed XML literal l RDF graph, combinationderivation RDF triple x rdf :type rdfs:Literal. RDF RDFS entailmentrules, x blank node allocated l10 . triple called XML clash.10. RDF(S), literals allowed subject position RDF triples, whereas blank nodes are.reason, RDF RDFS entailment rules applied RDF graph, literalreplaced unique blank node. way inferences drawn literal value denotedliteral, without concern restriction (Hayes, 2004).48fiExtended RDF Semantic Foundation Rule Markup Languagesunderstand this, note semantic condition 3 Definition A.3 (RDF interpretation,Appendix A), follows denotation ill-typed XML literal cannot literalvalue. Now, semantic conditions 1 2 Definition A.5 (RDFS interpretation,Appendix A), follows denotation ill-typed XML literal cannot typerdfs:Literal. Therefore, derivation XML clash RDF graph Gapplication RDF RDFS entailment rules, indicates RDFSinterpretation satisfies G.ERDF graph ERDF-inconsistent11 , due appearanceill-typed XML literal ERDF graph (in combination semantic condition 15Definition 3.7), also due additional semantic conditions coherent ERDFinterpretations.example, let p, q, s, URI let G = {p(s, o), rdfs:subPropertyOf (p, q), q(s, o)}.Then, G ERDF-inconsistent, since (coherent) ERDF interpretation satisfies G.following proposition shows total properties total classes (coherent)ERDF interpretations, weak negation strong negation coincide (boolean truth values).Proposition 3.1 Let ERDF interpretation vocabulary V let V = VVRDF VRDF VERDF . Then,1. p, s, V I(p) TProp , holds:|= p(s, o) iff |= p(s, o) (equivalently, |= p(s, o) p(s, o)).2. x, c V I(c) TCls , holds:|= rdf :type(x, c) iff |= rdf :type(x, c)(equivalently, |= rdf :type(x, c) rdf :type(x, c)).define ERDF entailment two ERDF formulas ERDF graphs.Definition 3.8 (ERDF entailment) Let F, F ERDF formulas ERDF graphs.say F ERDF-entails F (F |=ERDF F ) iff every ERDF interpretation I, |= F|= F .example, let:F = ?x ?y (rdf :type(?x, Person) hasFather (?x, ?y)) rdf :type(John, Person).Additionally, let F = ?y hasFather (John, ?y) rdf :type(hasFather , rdf :Property).F |=ERDF F .following proposition shows ERDF entailment extends RDFS entailment (Hayes,2004) (see also Appendix A) RDF graphs ERDF formulas. words, ERDFentailment upward compatible RDFS entailment.Proposition 3.2 Let G, G RDF graphs VG VERDF = VG VERDF = .Then, G |=RDF G iff G |=ERDF G .easily follows Proposition 3.2 RDF graph RDFS satisfiable iffERDF satisfiable. Thus, RDF graph ERDF-inconsistent due XMLclash.11. Meaning (coherent) ERDF interpretation satisfies ERDF graph.49fiAnalyti, Antoniou, Damasio, & Wagner4. ERDF Ontologies & Herbrand Interpretationssection, define ERDF ontology pair ERDF graph G setP ERDF rules. ERDF rules considered derivation rules allow usinfer ontological information based declarations G. Moreover, defineHerbrand interpretations minimal Herbrand models ERDF ontology.Definition 4.1 (ERDF rule, ERDF program) ERDF rule r vocabulary Vexpression form: G F , F L(V ) {true} called conditionG L(V |{}) {false} called conclusion. assume bound variable Fappears free G. denote set variables set free variables r Var (r)FVar (r)12 , respectively. Additionally, write Cond(r) = F Concl(r) = G.ERDF program P set ERDF rules vocabulary V . denote setURI references literals appearing P VP .Recall L(V |{}) denotes set ERDF triples V . Therefore, conclusionERDF rule, unless false, either positive ERDF triple p(s, o) negativeERDF triple p(s, o).example, consider derivation rule r:allRelated (?P, ?Q) ?p rdf :type(?p, ?P ) ?q (rdf :type(?q, ?Q) related (?p, ?q)),Then, r ERDF rule, indicating two classes P Q, holds allRelated (P,Q) instances p class P , instance q class Qholds related (p, q). Note Var (r) = {?P, ?Q, ?p, ?q} FVar (r) = {?P, ?Q}.Cond(r) = true Var (r) = {}, rule r called ERDF fact. Concl(r) =false, rule r called ERDF constraint. assume every partial interpretationevery function v : Var ResI , holds I, v |= true, |= true, I, v 6|= false,6|= false.Intuitively, ERDF ontology combination (i) ERDF graph G containing(implicitly existentially quantified) positive negative information, (ii) ERDFprogram P containing derivation rules (whose free variables implicitly universally quantified).Definition 4.2 (ERDF ontology) ERDF ontology (or ERDF knowledge base)pair = hG, P i, G ERDF graph P ERDF program.following definition defines models ERDF ontology.Definition 4.3 (Satisfaction ERDF rule ERDF ontology) LetERDF interpretation vocabulary V .say satisfies ERDF rule r, denoted |= r, iff mappingsv : Var (r) ResI I, v |= Cond(r), holds I, v |= Concl(r).say satisfies ERDF ontology = hG, P (also, model O),denoted |= O, iff |= G |= r, r P .12. FVar (r) = FVar (F ) FVar (G).50fiExtended RDF Semantic Foundation Rule Markup Languagespaper, existentially quantified variables ERDF graphs handled skolemization, syntactic transformation commonly used automatic inference systems removing existentially quantified variables.Definition 4.4 (Skolemization ERDF graph) Let G ERDF graph.skolemization function G 1:1 mapping skG : Var (G) URI,x Var (G), skG (x) artificial URI, denoted G:x. set skG (Var (G)) calledSkolem vocabulary G.skolemization G, denoted sk(G), ground ERDF graph derived Greplacing variable x Var (G) skG (x).Intuitively, Skolem vocabulary G (that is, skG (Var (G))) contains artificial URIsgiving arbitrary names anonymous entities whose existence asserteduse blank nodes G.example, let: G = {rdf :type(?x, EuropeanCountry), rdf :type(?x, EU member)}.Then,sk(G) = {rdf :type(skG (?x), EuropeanCountry), rdf :type(skG (?x), EU member)}.following proposition expresses skolemization ERDF graphentailments original graph, provided contain URIsskolemization vocabulary.Proposition 4.1 Let G ERDF graph let F ERDF formulaVF skG (Var (G)) = . holds: G |=ERDF F iff sk(G) |=ERDF F .define vocabulary ERDF ontology O.Definition 4.5 (Vocabulary ERDF ontology) Let = hG, P ERDF ontology. vocabulary defined VO = Vsk(G) VP VRDF VRDF VERDF .Note vocabulary ontology = hG, P contains skolemization vocabulary G.Let = hG, P ERDF ontology. denote ResHunion VO setXML values well-typed XML literals VO minus well-typed XML literals.following definition defines Herbrand interpretations Herbrand modelsERDF ontology.Definition 4.6 (Herbrand interpretation, Herbrand model ERDF ontology)Let = hG, P ERDF ontology let ERDF interpretation VO . sayHerbrand interpretation iff:ResI = ResHO.IV (x) = x, x VO URI.ILI (x) = x, x typed literal VO well-typed XML literal,ILI (x) XML value x, x well-typed XML literal VO .51fiAnalyti, Antoniou, Damasio, & Wagnerdenote set Herbrand interpretations H (O).Herbrand interpretation Herbrand model iff |= hsk(G), P i. denoteset Herbrand models MH (O).Note Herbrand interpretation ERDF ontology I(x) = x,x VO well-typed XML literal.easy see every Herbrand model ERDF ontology modelO. Moreover, note every Herbrand interpretation ERDF ontology uniquelyidentified (i) set properties (ii) property-truth property-falsity extensionmappings.However, Herbrand models ERDF ontology desirable. example,let p, s, URI, let G = {p(s, o)}, let = hG, i. Then, Herbrand model|= p(o, s), whereas want p(o, s) satisfied intendedmodels O. p total property p(o, s) cannot derived(negation-as-failure)13 .define minimal Herbrand interpretations ERDF ontology O, needdefine partial ordering Herbrand interpretations O.Definition 4.7 (Herbrand interpretation ordering) Let = hG, P ERDF ontology. Let I, J H (O). say J extends I, denoted J (or J I), iffP ropI P ropJ , p P ropI , holds PT (p) PT J (p) PF (p) PF J (p).easy verify relation reflexive, transitive, antisymmetric. Thus,partial ordering H (O).intuition behind Definition 4.7 extending Herbrand interpretation,extend truth falsity extension properties, thus (since rdf :typeproperty), classes.following proposition expresses two Herbrand interpretations I, J ERDFontology incomparable, property-truth property-falsity extension totalproperty p w.r.t. J different.Proposition 4.2 Let = hG, P ERDF ontology let I, J H (O). Let pTProp TProp J . PT (p) 6= PT J (p) PF (p) 6= PF J (p) 6 J J 6 I.Definition 4.8 (Minimal Herbrand interpretations) Let ERDF ontologylet H (O). define minimal(I) = {I | 6 J : J =6 J I}.define minimal Herbrand models O, as:Mmin (O) = minimal(MH (O)).However minimal Herbrand models give intended semantics ERDF rules.ERDF rules derivation implication rules. Derivation rules13. hand, p total property p(o, s)p(o, s) satisfied intended models.Therefore, case, intended model satisfies p(o, s).52fiExtended RDF Semantic Foundation Rule Markup Languagesoften identified implications. But, general, two different concepts.implication expression logical formula language, derivation rule rathermeta-logical expression. logics, implication connective,derivation rule concept. standard logics (such classical intuitionisticlogic), close relationship derivation rule (also called sequent)corresponding implicational formula: models. non-monotonicrules (e.g. negation-as-failure), longer case: intended modelsrule are, general, intended models corresponding implication.easy see help example. Consider rule p q whose model set,according stable model semantics (Gelfond & Lifschitz, 1988, 1990; Herre & Wagner,1997; Herre et al., 1999), {{p}}, is, entails p. hand, model setcorresponding implication q p, equivalent disjunction p q,{{p}, {q}, {p, q}}; consequently, entail p.Similarly, let = h, P i, P = {p(s, o) q(s, o)} p, q, s, URI.minimal Herbrand models intended. particular, Mmin (O)|= q(s, o) p(s, o), whereas want q(s, o) p(s, o) satisfiedintended models O, q total property q(s, o) cannot derived rule(negation-as-failure).define intended (stable) models ERDF ontology, need first definegrounding ERDF rules.Definition 4.9 (Grounding ERDF program) Let V vocabulary let rERDF rule. denote [r]V set rules result r replacevariable x FVar (r) v(x), mappings v : FVar (r) V .Let P ERDF program. define [P ]V =rP [r]V .Note rule variable naturally appear subject position ERDF triple.Since variables instantiated literal, literal naturally appear subjectposition ERDF triple grounded version ERDF program. casesupports choice allowing literals subject position ERDF triple.5. ERDF Stable Modelssection, define intended models ERDF ontology O, called stable modelsO, based minimal Herbrand interpretations. particular, defining stable modelsO, minimal interpretations set Herbrand interpretations satisfycertain criteria considered.Below, define stable models ERDF ontology, based coherent stablemodels14 Partial Logic (Herre et al., 1999).Definition 5.1 (ERDF stable model) Let = hG, P ERDF ontology letH (O). say (ERDF) stable model iff chainHerbrand interpretations O, I0 ... Ik+1 Ik = Ik+1 = and:14. Note models extended logic programs equivalent (Herre et al., 1999) Answer Setsanswer set semantics (Gelfond & Lifschitz, 1990).53fiAnalyti, Antoniou, Damasio, & Wagner1. I0 minimal({I H (O) | |= sk(G)}).2. successor ordinals 0 < k + 1:minimal({I H (O) | I1 |= Concl(r), r P[I1 ,M ] }),P[I1 ,M ] = {r [P ]VO | |= Cond(r), H (O) s.t. I1 }.set stable models denoted Mst (O).Note I0 minimal Herbrand interpretation = hG, P satisfies sk(G),Herbrand interpretations I1 , ..., Ik+1 correspond stratified sequence rule applications, applied rules remain applicable throughout generation stablemodel . words, stable model generated bottom-up iterative applicationrules ERDF program P , starting information ERDF graph G.Thus, ERDF stable model semantics, refinement minimal model semantics, capturesintuition that:Assertions rdf :type(p, erdf :TotalProperty) rdf :type(c, erdf :TotalClass)accepted ontology contains direct support formacceptable rule sequence (that corresponds proof).Assertions p(s, o) p(s, o) accepted ontology containsdirect support form acceptable rule sequence,rdf :type(p, erdf :TotalProperty) accepted.Assertions rdf :type(o, c) rdf :type(o, c) accepted ontologycontains direct support form acceptable rule sequence,rdf :type(c, erdf :TotalClass) accepted.Wine Selection Example: Consider class Wine whose instances wines,property likes(X, ) indicating person X likes object . Assume wantselect wines dinner that, guest, table exactly one winehe/she likes. Let class Guest indicate persons invited dinnerlet class SelectedWine indicate wines chosen served. ERDF programP describes wine selection problem following (commas , bodyrules indicate conjunction ):id(?x, ?x) rdf :type(?x, rdfs:Resource).rdf :type(?y, SelectedWine) rdf :type(?x, Guest), rdf :type(?y, Wine), likes(?x, ?y),?z (rdf :type(?z, SelectedWine), id(?z, ?y) likes(?x, ?z)).Consider ERDF graph G, containing factual information:G = { rdf :type(Carlos, Guest), rdf :type(Gerd , Guest), rdf :type(Riesling, Wine),rdf :type(Retsina, Wine), rdf :type(Chardonnay, Wine), likes(Gerd , Riesling),likes(Gerd , Retsina), likes(Carlos, Chardonnay), likes(Carlos, Retsina) }.Then, according Definition 5.1, ERDF ontology = hG, P two stable models,M1 M2 , that:54fiExtended RDF Semantic Foundation Rule Markup LanguagesM1 |= rdf :type(Riesling, SelectedWine) rdf :type(Chardonnay, SelectedWine)rdf :type(Retsina, SelectedWine).M2 |= rdf :type(Retsina, SelectedWine) rdf :type(Riesling, SelectedWine)rdf :type(Chardonnay, SelectedWine).Note that, according stable model M1 , wines selected dinner RieslingChardonnay. because, (i) Gerd likes Riesling like Chardonnay,(ii) Carlos likes Chardonnay like Riesling.According stable model M2 , Retsina selected dinner. because,Gerd Carlos like Retsina.Stable model M1 reached chain I0 M1 M1 , I0 singleHerbrand interpretation minimal({I H (O) | |= sk(G)}). verify this, note that:P[I0 ,M1 ] = P[M1 ,M1 ] =[id(?x, ?x) rdf :type(?x, rdfs:Resource)]VO{rdf :type(Riesling, SelectedWine) rdf :type(Gerd , Guest),rdf :type(Riesling, Wine), likes(Gerd , Riesling),?z (rdf :type(?z, SelectedWine), id(?z , Riesling) likes(Gerd , ?z))}{rdf :type(Chardonnay, SelectedWine) rdf :type(Carlos, Guest),rdf :type(Chardonnay, Wine), likes(Carlos, Chardonnay),?z (rdf :type(?z, SelectedWine), id(?z , Chardonnay) likes(Carlos, ?z))}.Similarly, stable model M2 reached chain I0 M2 M2 . verify this,note that:P[I0 ,M2 ] = P[M2 ,M2 ] =[id(?x, ?x) rdf :type(?x, rdfs:Resource)]VO{rdf :type(Retsina, SelectedWine) rdf :type(Gerd , Guest),rdf :type(Retsina, Wine), likes(Gerd , Retsina),?z (rdf :type(?z, SelectedWine), id(?z, Retsina) likes(Gerd , ?z))}{rdf :type(Retsina, SelectedWine) rdf :type(Carlos, Guest),rdf :type(Retsina, Wine), likes(Carlos, Retsina),?z (rdf :type(?z, SelectedWine), id(?z, Retsina) likes(Carlos, ?z))}.Assume Retsina one selected wines,match food. indicate this, add P ERDF constraint:false rdf :type(Retsina, SelectedWine).Then, M1 single model modified ontology.easy verify ERDF ontology exactly O, withoutERDF constraints appearing O, Mst (O) Mst (O ). words, ERDFconstraints appearing ERDF ontology eliminate undesirable stable models.Paper Assignment Example: Consider class Paper whose instances papers submitted conference, class Reviewer whose instances potential reviewers55fiAnalyti, Antoniou, Damasio, & Wagnersubmitted papers, property conflict(R, P ) indicating conflict interest reviewer R paper P . Assume want assign papersreviewers based following criteria: (i) paper assigned one reviewer,(ii) reviewer assigned one paper, (iii) paper assigned reviewer,conflict interest. assignment paper P reviewer R indicatedproperty assign(P, R). ERDF triple allAssigned (Paper , Reviewer ) indicates paper assigned one reviewer. ERDF program P describingassignment papers following:id(?x, ?x) true.assign(?p, ?r) rdf :type(?p, Paper ), rdf :type(?p , Paper ), assign(?p , ?r), id(?p, ?p ).assign(?p, ?r) rdf :type(?r, Reviewer ), rdf :type(?r , Reviewer ), assign(?p, ?r ), id(?r, ?r ).assign(?p, ?r) conflict(?r, ?p).assign(?p, ?r)rdf :type(?r, Reviewer ), rdf :type(?p, Paper ), assign(?p, ?r).allAssigned (Paper , Reviewer ) ?p (rdf :type(?p, Paper )?r (rdf :type(?r, Reviewer ) assign(?p, ?r))).Consider ERDF graph G, containing factual information:G = { rdf :type(P 1, Paper ), rdf :type(P 2, Paper ), rdf :type(P 3, Paper ), rdf :type(R1, Reviewer ),rdf :type(R2, Reviewer ), rdf :type(R3, Reviewer ), conflict(P 1, R3), conflict(P 2, R2),conflict(P 3, R2) }.Then, according Definition 5.1, ERDF ontology = hG, P four stablemodels, denoted M1 , ..., M4 , that:M1M2M3M4|=|=|=|=assign(P 1, R1) assign(P 2, R3) allAssigned (Paper , Reviewer ),assign(P 1, R1) assign(P 3, R3) allAssigned (Paper , Reviewer ),assign(P 1, R2) assign(P 2, R1) assign(P 3, R3) allAssigned (Paper , Reviewer ),assign(P 1, R2) assign(P 2, R3) assign(P 3, R1) allAssigned (Paper , Reviewer ).would like note that, contrast previous examples, given ERDFontology = hG, P i, possible |minimal ({I H (O) | |= sk(G)})| > 1, duedeclaration total properties total classes. Specifically, numberinterpretations I0 item 1 Definition 5.1 one iff G contains ERDF triplesform rdf :type(p, erdf :TotalProperty) rdf :type(c, erdf :TotalClass). example, let= hG, i, where:G = {authorOf (John, book1 ), authorOf (Peter , book2 ), rdf :type(authorOf , erdf :TotalProperty)}.Then, I0 , I0 minimal ({I H (O) | |= sk(G)}) that:I0 |= authorOf (John, book2 ) I0 |= authorOf (John, book2 ).Note I0 I0 stable models O. However, I0 satisfies authorOf (John, book2 ),even though evidence John author book2 .following proposition shows stable model ERDF ontology Herbrand model O.56fiExtended RDF Semantic Foundation Rule Markup LanguagesProposition 5.1 Let = hG, P ERDF ontology let Mst (O). holdsMH (O).hand, properties total, Herbrand model ERDF ontology= hG, P stable model O15 . Obviously, desirable result since,case, open-world assumption made properties. Thus, preferentialentailment weak negation, properties. course, term stable modeldescriptive, degenerative case.Proposition 5.2 Let = hG, P ERDF ontologyrdfs:subClassOf (rdf :Property, erdf :TotalProperty) G. Then, Mst (O) = MH (O).final note that, similarly stable models defined Gelfond & Lifschitz (1988, 1990)Herre et al. (1999), ERDF stable models preserve Herbrand model satisfiability.example, let = h, P i, P = {p(s, o) p(s, o)} p, s, URI. Then,Mst (O) = , whereas Herbrand model satisfies p(s, o).6. ERDF Stable Model Entailment & Stable Answerssection, define stable model entailment ERDF ontologies, showing extends ERDF entailment ERDF graphs. Moreover, define skeptical credulousanswers ERDF formula (query) F w.r.t. ERDF ontology O.Definition 6.1 (Stable model entailment) Let = hG, P ERDF ontologylet F ERDF formula ERDF graph. say entails F (ERDF)stable model semantics, denoted |=st F iff Mst (O), |= F .example, let = h, P i, P = {p(s, o) q(s, o)} p, q, s, URI. Then,|=st q(s, o) p(s, o).Now, let G = {rdfs:subClassOf (rdf :Property, erdf :TotalProperty)} let Pprevious example. Then, hG, P |=st q(s, o) p(s, o), hG, P 6|=st q(s, o)hG, P 6|=st p(s, o). Note desirable result, since q total property(and thus, open-world assumption made q).another example, let p, s, URI, let G = {p(s, o)}, let P = {p(?x, ?y)p(?x, ?y)}. Then, hG, P |=st p(o, s) p(o, s) (note P contains CWA p).Now, let G = {rdf :type(p, erdf :TotalProperty), p(s, o)} let P previousexample. Then, hG, P |=st ?x ?y (p(?x, ?y) p(?x, ?y)) (see Proposition 3.1),hG, P 6|=st p(o, s) hG, P 6|=st p(o, s). Indeed, CWA P affectsemantics p, since p total property.EU Membership Example: Consider following ERDF program P , specifyingrules concluding country member state European Union (EU).(r1 )(r2 )rdf :type(?x, EUMember)rdf :type(?x, EUMember)rdf :type(?x, AmericanCountry).rdf :type(?x, EuropeanCountry),rdf :type(?x, EUMember).15. Note that, case, minimal({I H (O) | |= sk(G)}) minimal({I H (O) ||= Concl(r), r P[M,M ] }).57fiAnalyti, Antoniou, Damasio, & Wagnerrather incomplete ERDF ontology = hG, P obtained including followinginformation ERDF graph G:rdf :type(Russia, EUMember).rdf :type(Austria, EUMember).rdf :type(?x, EuropeanCountry).rdf :type(Canada, AmericanCountry).rdf :type(Italy, EuropeanCountry).rdf :type(?x, EUMember).Using stable model entailment O, concluded Austria member EU,Russia Canada members EU, exists European Countrymember EU. However, also concluded Italy member EU, wrong statement. G contain completeinformation European countries EU members (e.g., containrdf :type(Italy, EUMember)). Thus, incorrect information obtained closed-worldassumption expressed rule r2 . case rdf :type(EUMember, erdf :TotalClass)added G (that is, open-world assumption made class EUMember)rdf :type(Italy, EUMember) thus, rdf :type(Italy, EUMember) longer entailed.because, stable model extended ERDF ontology satisfiesrdf :type(Italy, EUMember). Moreover, complete information European countriesmembers EU included G stable model conclusions alsocorrect (the closed-world assumption correctly applied). Note that, case,G include ERDF triple rdf :type(Italy, EUMember).following proposition follows directly fact stable modelERDF ontology ERDF interpretation.Proposition 6.1 Let = hG, P ERDF ontology let F, F ERDF formulas.|=st F F |=ERDF F |=st F .ERDF graphs G, G , proved hG, |=st G iff G |=ERDF G (seebelow). question arises whether result generalized replacingERDF graph G ERDF formula F . following example showscase. Let G = {p(s, o)} let F = p(o, s), p, s, URI. hG, |=st F ,whereas G 6|=ERDF F . However, G replaced ERDF d-formula F , definedfollows:Definition 6.2 (ERDF d-formula) Let F ERDF formula. say FERDF d-formula iff (i) F disjunction existentially quantified conjunctions ERDFtriples, (ii) FVar (F ) = .example, let:F = (?x rdf :type(?x , Vertex ) rdf :type(?x , Red ))(?x rdf :type(?x , Vertex ) rdf :type(?x , Blue)).Then, F ERDF d-formula. easy see G ERDF graph formula(G)ERDF d-formula.58fiExtended RDF Semantic Foundation Rule Markup LanguagesProposition 6.2 Let G ERDF graph let F ERDF formulaVF skG (Var (G)) = . holds:1. F ERDF d-formula hG, |=st F G |=ERDF F .2. G |=ERDF F hG, |=st F .Let G ERDF graph let F ERDF d-formula ERDF graphVF skG (Var (G)) = . direct consequence Proposition 6.2 that:hG, |=st F iff G |=ERDF F .following proposition direct consequence Proposition 3.2 Proposition6.2, shows stable model entailment extends RDFS entailment RDF graphsERDF ontologies.Proposition 6.3 Let G, G RDF graphs VG VERDF = , VG VERDF = ,VG skG (Var (G)) = . holds: G |=RDF G iff hG, |=st G .Recall Skolem vocabulary G (that is, skG (Var (G))) contains artificial URIsgiving arbitrary names anonymous entities whose existence asserteduse blank nodes G. Thus, condition VG skG (Var (G)) = Proposition 6.3actually trivial.Definition 6.3 (ERDF query, ERDF stable answers) Let = hG, P ERDFontology. (ERDF) query F ERDF formula. (ERDF) stable answers Fw.r.t. defined follows:FVar (F ) = Mst (O) : |= FyesstFVar (F ) = Mst (O) : 6|= FAns (F ) ={v : FVar (F ) VO | Mst (O), |= v(F )}FVar (F ) 6= ,v(F ) formula F replacing free variables x F v(x).example, let p, q, c, s, URI, let G = {p(s, o), rdf :type(s, c), rdf :type(o, c)},let P = {q(?x, ?y) rdf :type(?x, c) rdf :type(?y, c) p(?x, ?y)}. Then, stableanswers F = q(?x, ?y) w.r.t. = hG, P Ans st(F ) = {{?x = o, ?y = o}, {?x =s, ?y = s}, {?x = o, ?y = s}}.Let = hG, P i, q, s, URI, G = {rdf :type(p, erdf :TotalProperty), q(s, o)},stP = {p(?x, ?y) p(?x, ?y)}. Then, holds Ans st(p(?x, ?y))= Ans (p(?x, ?y)) =stAns (p(?x, ?y)) = . because, contrast example, p totalproperty. Thus, mappings v : {?x, ?y} VO , stable model|= v(p(?x, ?y) p(?x, ?y)), another stable model|= v(p(?x, ?y) p(?x, ?y)).Consider ERDF ontology paper assignment example, Definitionst5.1. Then, Ans st(assign(P 1, R2)) =yes Ans (assign(P 2, R1)) =no. ThoughstAns (assign(P 2, R1)) =no, assign(P 2, R1) satisfied stable models O, stable model (M3 ) satisfies assign(P 2, R1). Indeed answersquery assign(?x, ?y) w.r.t. stable models M3 M4 particular interest since59fiAnalyti, Antoniou, Damasio, & WagnerM3 M4 satisfy allAssigned (Paper , Reviewer ), indicating desirable paperassignment achieved.following definition defines credulous stable answers query F w.r.t.ERDF ontology O, answers F w.r.t. particular stable models O.Definition 6.4 (Credulous ERDF stable answers) Let = hG, P ERDF ontology. credulous (ERDF) stable answers query F w.r.t. defined follows:yes FVar (F ) = Mst (O) : |= FstFVar (F ) = Mst (O) : 6|= Fc-Ans (F ) ={ans (F ) 6= | Mst (O)}FVar (F ) 6= ,ans (F ) = {v : FVar (F ) VO | |= v(F )}.Continuing paper assignment example, consider query:F = allAssigned (Paper , Reviewer ).stThen, although Ans st(F ) =no, holds c-Ans (F ) =yes, indicatingleast one desirable assignment papers P 1, P 2, P 3 reviewers R1, R2, R3.Consider query F = allAssigned (Paper , Reviewer ) assign(?x , ?y). Then,c-Ans st(F ) = {{{?x = P 1, ?y = R2}, {?x = P 2, ?y = R1}, {?x = P 3, ?y = R3}},{{?x = P 1, ?y = R2}, {?x = P 2, ?y = R3}, {?x = P 3, ?y = R1}}},indicating possible desirable assignments papers. Obviously, credulous stableanswers query F provide alternative solutions, useful rangeapplications, alternative scenarios naturally appear.Closing section, would like indicate several differences ERDF stablemodel semantics w.r.t. first-order logic (FOL). First, semantics domain closure assumption made. due fact domain every Herbrand interpretationERDF ontology ResH, union vocabulary (VO )set XML values well-typed XML literals VO minus well-typed XML literals.implies quantified variables always range closed domain. understandimplications assumption, consider ERDF graph:G = {rdf :type(x, c1) | x {c1, c2} V },V = (VRDF {rdf : | }) VRDF VERDF . Additionally, consider ERDFprogram:P = { rdf :type(?x, c1) rdf :type(?x, rdfs:ContainerMembershipProperty).rdf :type(?x, c2) true.}.Let F = ?x rdf :type(?x, c2) rdf :type(?x, c1). holds hG, P |=st F . However,G P 6|=F OL F . because, FOL model G P domainvariable assignment v:{?x} M, v |= rdf :type(?x, c2) M, v 6|=rdf :type(?x, c1).Another difference due fact definition ERDF stable modelsemantics, minimal Herbrand interpretations considered. Let60fiExtended RDF Semantic Foundation Rule Markup LanguagesG = {teaches(Anne, CS301 ), teaches(Peter , CS505 ), rdf :type(CS505 , GradCourse)}.Let F = ?x teaches(Peter , ?x) rdf :type(?x, GradCourse). Then, hG, |=st F .However, G 6|=F OL F . because, FOL model G domainvariable assignment v:{?x} M, v |= teaches(Peter , ?x) M, v 6|=rdf :type(?x, GradCourse). words, FOL makes open-world assumptionteaches.Consider G = G {rdf :type(teaches, erdf :TotalProperty)}. Then, similarlyFOL, holds = hG , 6|=st F . teaches total property. Thus,stable model variable assignment v: {?x} ResHM, v |= teaches(Peter , ?x) M, v 6|= rdf :type(?x, GradCourse). worlds,open-world assumption made teaches, FOL. Thus, might exist coursetaught Peter , even explicitly indicated G .example also shows that, contrast FOL, stable model entailment nonmonotonic.Note previous ERDF graph G also seen Description Logic A-Box(Baader et al., 2003),= {teaches(Anne, CS301), teaches(Peter , CS505), GradCourse(CS505)}Consider T-Box = . Since Description Logics (DLs) fragments first-orderlogic, holds L = hA, 6|=DL teaches.GradCourse(Peter ), meaning Lsatisfy courses taught Peter graduate courses. interesting approachsupporting non-monotonic conclusions DLs taken Donini et al. (2002), DLsminimal knowledge negation failure (MKNF-DLs) defined, extending DLstwo modal operators K, A. Intuitively, K expresses minimal knowledge expressesweak negation. holds L |=MKNF-DL Kteaches.KGradCourse(Peter ), expressingcourses known taught Peter known graduate courses. Noteconclusion non-monotonic, thus cannot derived classical DLs. However,compared theory, MKNF-DLs support rules closed-world assumptionsproperties (i.e., p(?x, ?y) p(?x, ?y)).7. XML-based Syntax ERDFnatural approach define XML syntax ERDF is: (i) follow RDF/XMLsyntax (Beckett, 2004), much possible, (ii) extend suitable way,necessary. Following approach, briefly present XML syntax ERDF.Details going given subsequent paper.Classes properties defined help rdfs:Class rdf:Propertyelements RDF/XML syntax. Similarly, total classes total properties defined help erdf:TotalClass erdf:TotalProperty elementsERDF/XML syntax.Example 7.1 following ERDF/XML statements:<rdf:Property rdf:about="#likes"><rdfs:domain rdf:resource="#Person"/>61fiAnalyti, Antoniou, Damasio, & Wagner</rdf:Property><erdf:TotalProperty rdf:about="#authorOf"><rdfs:domain rdf:resource="#Person"/><rdfs:range rdf:resource="#Book"/></erdf:TotalProperty>correspond ERDF graph:G = { rdf :type(likes, rdf :Property), rdfs:domain(likes, Person),rdf :type(authorOf , erdf :TotalProperty), rdfs:domain(authorOf , Person),rdfs:range(authorOf , Book )}.ERDF triples (and sets ERDF triples sharing subject term) encodedmeans erdf:Description element. description contains non-empty list(possibly negated) property-value slots subject term.URI references, blank node identifiers, variables appear subject positionERDF triple expressed values erdf:about attribute, usingSPARQL syntax (Prudhommeaux & Seaborne, 2008) blank node identifiersvariables. hand, literals appear subject position ERDFtriple expressed text content erdf:about subelement.URI references, blank node identifiers, variables appear object position ERDF triple expressed values attributes rdf:resource,rdf:nodeID, erdf:variable, respectively. hand, literals appear object position ERDF triple expressed text contentcorresponding property subelement.Example 7.2 following erdf:Description statements:<erdf:Description erdf:about="#Gerd"><ex:authorOf rdf:nodeID="x"/><ex:likes rdf:resource="#Chicken"/><ex:likes erdf:negationMode="Sneg" rdf:resource="#Pork"/></erdf:Description><erdf:Description><erdf:About rdf:datatype="&xsd;string">Grigoris</erdf:About><ex:denotationOf rdf:resource="#Grigoris"/></erdf:Description>correspond ERDF graph:G = { authorOf (Gerd , ?x ), likes(Gerd , Chicken), likes(Gerd , Pork ),denotationOf (Grigorisxsd :string, Grigoris) }.Now, order express ERDF rules XML, use rule markup language R2ML(REWERSE Rule Markup Language) (Wagner, Giurca, & Lukichev, 2006, 2005),general XML-based markup language representing derivation rules integrityconstraints. demonstrated following example:62fiExtended RDF Semantic Foundation Rule Markup LanguagesExample 7.3 following erdf:DerivationRule statement:<r2ml:DerivationRule r2ml:ruleID="R1"><r2ml:conditions><erdf:Description erdf:about="?x"><rdf:type rdf:resource="#MainDish"/></erdf:Description><erdf:Description erdf:about="?y"><rdf:type rdf:resource="#Guest"/><ex:likes erdf:variable="x"/></erdf:Description><r2ml:NegationAsFailure><r2ml:ExistentiallyQuantifiedFormula><r2ml:GenericVariable r2ml:name="z" r2ml:class="#Guest"/><erdf:Description erdf:about="?z"><ex:likes erdf:negationMode="Sneg" erdf:variable="x"/></erdf:Description></r2ml:ExistentiallyQuantifiedFormula></r2ml:NegationAsFailure></r2ml:conditions><r2ml:conclusion><erdf:Description erdf:about="?x"><rdf:type rdf:resource="#SelectedMainDish"/></erdf:Description></r2ml:conclusion></r2ml:DerivationRule>expresses main dish selected dinner, guest likes guestdislikes it. Specifically, corresponds ERDF rule:rdf :type(?x, SelectedMainDish) rdf :type(?x, MainDish), rdf :type(?y, Guest), likes(?y, ?x),(?z rdf :type(?z, Guest), likes(?z, ?x)).8. Undecidability ERDF Stable Model Semanticsmain difficulty computation ERDF stable model semantics factVRDF infinite, thus vocabulary ERDF ontology also infinite (note{rdf : | } VRDF VO ). Due fact, satisfiability entailmentERDF stable model semantics general undecidable.proof undecidability exploits reduction unbounded tiling problem.unbounded tiling problem consists placing tiles infinite grid, satisfying given setconstraints adjacent tiles. Specifically, unbounded tiling problem structure= hT , H, V i, = {T1 , ..., Tn } finite set tile types H, V specifytiles adjacent horizontally vertically, respectively. solutiontiling, is, total function : that: ( (i, j), (i + 1, j)) H( (i, j), (i, j + 1)) V , i, j . existence solution given unboundedtiling problem known undecidable (Berger, 1966).Let = hT , H, V instance unbounded tiling problem, = {T1 , ..., Tn }.construct ERDF ontology OD = hG, P ERDF formula FDsolution iff OD entail FD ERDF stable model semantics.63fiAnalyti, Antoniou, Damasio, & WagnerConsider (i) class Tile whose instances tiles placed infinite grid, (ii)property right(x , y) indicating tile right next tile x, (iii) property above(x , y)indicating tile exactly tile x, (iv) class HasRight whose instancestiles exists tile right next them, (v) class HasAbove whose instancestiles exists tile exactly them, (vi) property Type(x, ),indicating type tile x , (vii) property HConstraint(T, ), indicating(T, ) H, (viii) property VConstraint(T, ), indicating (T, ) V .Let G ERDF graph:G={rdfs:subClassOf (rdfs:ContainerMembershipProperty, Tile),rdfs:subClassOf (Tile, rdfs:ContainerMembershipProperty)}{HConstraint(T, ) | (T, ) H} {VConstraint(T, ) | (T, ) V } .Let P ERDF program, containing following rules (and constraints):(1)Type(?x, T1 ) rdf :type(?x, Tile), Type(?x, T2 ), ..., Type(?x, Tn ).Type(?x, Ti ) rdf :type(?x, Tile), Type(?x, T1 ), ..., Type(?x, Ti1 ),Type(?x, Ti+1 ), ..., Type(?x, Tn ), = 2, ..., n 1.Type(?x, Tn ) rdf :type(?x, Tile), Type(?x, T1 ), ..., Type(?x, Tn1 ).(2)right(?x, ?y)right(?x, ?y)rdf :type(?x, Tile), rdf :type(?y, Tile), right(?x, ?y).rdf :type(?x, Tile), rdf :type(?y, Tile), right(?x, ?y).(3)above(?x, ?y)above(?x, ?y)rdf :type(?x, Tile), rdf :type(?y, Tile), above(?x, ?y).rdf :type(?x, Tile), rdf :type(?y, Tile), above(?x, ?y).(4)rdf :type(?x, HasRight) right(?x, ?y).rdf :type(?x, HasAbove) above(?x, ?y).false rdf :type(?x, Tile), rdf :type(?x, HasRight).false rdf :type(?x, Tile), rdf :type(?x, HasAbove).id (?x, ?x) rdf :type(?x, rdfs:Resource).false right(?x, ?y), right(?x, ?y ), id (?y, ?y ).false above(?x, ?y), above(?x, ?y ), id (?y, ?y ).(5)false right(?x, ?y), Type(?x, ?T ), Type(?y, ?T ), HConstraint(?T, ?T ).false above(?x, ?y), Type(?x, ?T ), Type(?y, ?T ), VConstraint(?T, ?T ).Note stable models OD = hG, P i, class Tile contains exactly(infinite mumber) rdf : terms, . because, computing stable modelsO, minimal models sk(G) considered (see Definition 5.1, Step 1). Thus,tile infinite grid represented rdf : term, .Intuitively, rule set (1) expresses tile exactly one associated type. Rule set (2) expresses two tiles either horizontally adjacent grid64fiExtended RDF Semantic Foundation Rule Markup Languageshorizontally adjacent. Rule set (3) expresses two tiles either vertically adjacentgrid vertically adjacent. Rule set (4) expresses tile exactlyone tile right next exactly one tile right it. Rule set (5) expressestypes horizontally vertically adjacent tiles respect H V relationsD, respectively.finalize reduction, define:FD = ?x, ?y, ?x , ?y , ?x right(?x, ?y) above(?y, ?y ) right(?x , ?y ) above(?x , ?x )id (?x, ?x ).Formula FD expresses tile x that, starting x, move:one step right one step one step left one stepmeet tile x different x.Proposition 8.1 Let instance unbounded tiling problem. holds:1. solution iff OD {false FD } stable model.2. solution iff OD 6|=st FD .Since unbounded tiling problem undecidable (Berger, 1966), follows directlyProposition 8.1 satisfiability entailment ERDF stable model semanticsgeneral undecidable.previous reduction shows problems remain undecidable ERDF ontology = hG, P i, even (i) body rule P form t1 , ..., tk , tk+1 , ..., tn ,ti ERDF triple (ii) terms erdf :TotalClass erdf :TotalPropertyappear O, is, (VG VP ) VERDF = . Note since constraint false Fappears ERDF ontology replaced rule F ,RDF, RDFS, ERDF axiomatic triple, presence constraints affectdecidability.Future work concerns identification syntactic restrictions ERDF ontologyERDF stable model entailment decidable.9. ERDF Model Theory Tarski-style Model TheoryTarski-style model theory limited classical first-order models, employedsemantics OWL. allows various extensions, relaxing bivalence assumption(e.g., allowing partial models) allowing higher-order models. also compatibleidea non-monotonic inference, simply considering models ruleintended, models satisfy certain criteria. Thus, stable modelsemantics normal (generalized) extended logic programs (Gelfond & Lifschitz, 1988,1990; Herre & Wagner, 1997; Herre et al., 1999) viewed Tarski-style modeltheoretic semantics non-monotonic derivation rules.Tarski-style model theory triple hL, I, |=i that:L set formulas, called language,65fiAnalyti, Antoniou, Damasio, & Wagnerset interpretations,|= relation interpretations formulas, called model relation.Tarski-style model theory hL, I, |=i, define:notion derivation rule G F , F L called condition G Lcalled conclusion,set derivation rules DRL = {G F | F, G L},extension model relation |= include also pairs interpretationsderivation rules,standard model operator M(KB ) = {I | |= X, X KB }, KBL DRL set formulas and/or derivation rules, called knowledge base.Notice way define rules also logics containimplication connective. shows concept rule independent conceptimplication.Typically, knowledge representation theories, models knowledge baseintended models. Except standard model operator M, also non-standardmodel operators, provide models knowledge base, specialsubset supposed capture intended models according semantics.particularly important type intended model semantics obtainedbasis information ordering , allows compare information contenttwo interpretations I1 , I2 I. Whenever I1 I2 , say I1 less informativeI2 . information model theory hL, I, |=, Tarski-style model theory, extendedinformation ordering .information model theory, define number natural non-standardmodel operators, minimal model operator:Mmin (KB ) = minimal (M(KB ))various refinements it, like stable generated models (Gelfond & Lifschitz, 1988,1990; Herre & Wagner, 1997; Herre et al., 1999).given model operator Mx : P(L DRL ) P(I), knowledge base KBL DRL , F L, define entailment relation:KB |=x Fiff Mx (KB ), |= Fnon-standard model operators, like minimal stable models, entailmentrelation typically non-monotonic, sense extension KB KB maycase KB entails F , KB entail F .(ERDF) stable model theory seen Tarski-style model theory,L = L(URI LIT ), set ERDF interpretations vocabulary VURI LIT , model relation |= defined Definitions 3.5 4.3.theory, intended model operator (Mst ) assigns ERDF ontology (possiblyempty) set stable models (Definition 5.1).66fiExtended RDF Semantic Foundation Rule Markup Languages10. Related Worksection, briefly review extensions web ontology languages rules.Ter Horst (2005b, 2004) generalizes RDF graphs generalized RDF graphs, allowing variables property position RDF triples. Additionally, author extendsRDFS semantics datatypes part OWL vocabulary, defining pD semantics, extends if-semantics RDFS weaker iff-semanticsD-entailment (Hayes, 2004) OWL Full (Patel-Schneider, Hayes, & Horrocks, 2004).sound complete set entailment rules pD entailment also presented.subsequent work, ter Horst (2005a) considers extension previous framework inclusion rules form G G , G RDF graph withoutblank nodes possibly variables G generalized RDF graph, possiblyblank nodes variables. Intuitively, rule variables universally quantifiedfront rule (like free variables rules) blank nodes head rulecorrespond existentially quantified variables (this feature supported model).Based set rules R datatype map D, R-entailment16 defined twogeneralized RDF graphs G G (G |=R G ), set sound complete rulesR-entailment presented. relate work ter Horst (2005a), statefollowing proposition:Let datatype map, containing rdf :XMLLiteral , let R set rules formG G constraints: (i) terms appearing property position URIs, (ii)G 6= {} blank node appears G , (iii) VR (VpOWL VERDF ) = , VpOWLdenotes part OWL vocabulary, included pD semantics. Let G, G RDF graphs(VG VG ) (VpOWL VERDF ) = . based G R, define, simpletransformation, ERDF ontology G |=R G iff |=st G .However, work, weak strong negation considered. Thus, closed-worldreasoning supported. Additionally, theory, condition rule ERDFformula vocabulary V , (thus, involving logical factors , , , , , ,), conjunction positive triples.TRIPLE (Sintek & Decker, 2002) rule language Semantic Webespecially designed querying transforming RDF models (or contexts), supportingRDF subset OWL Lite. syntax based F-Logic (Kifer, Lausen, & Wu,1995) supports important fragment first-order logic. triple representedstatement form s[p o] sets statements, sharing subject s,aggregated using molecules form s[p1 o1 ; p2 o2 ; ....]. variables mustexplicitly quantified, either existentially universally. Arbitrary formulas usedbody, head rules restricted atoms conjunctions molecules.interesting relevant feature TRIPLE use models collect sets relatedsentences. particular, part semantics RDF(S) vocabulary representedpre-defined rules (and semantic conditions interpretations), groupedtogether module. TRIPLE provides features like path expressions, skolem modelterms, well model intersection difference. Finally, mentionedqueries models compiled XSB Prolog. TRIPLE uses Lloyd-Toportransformations (Lloyd & Topor, 1984) take care first-order connectives16. symbol appear explicitly notation R-entailement, reasons simplification.67fiAnalyti, Antoniou, Damasio, & Wagnersentences supports weak negation well-founded semantics (Gelder, Ross, &Schlipf, 1991). Strong negation used.Flora-2 (Yang, Kifer, & Zhao, 2003) rule-based object-oriented knowledge base system reasoning semantic information Web. based F-logic (Kiferet al., 1995) supports metaprogramming, non-monotonic multiple inheritance, logical database updates, encapsulation, dynamic modules, two kinds weak negation.Specifically, supports Prolog negation well-founded negation (Gelder et al., 1991),invocation corresponding operators \+ tnot XSB system (Rao,Sagonas, Swift, Warren, & Freire, 1997). formal semantics non-monotonic multiple inheritance defined Yang & Kifer (2003a). addition, Flora-2 supports reification anonymous resources (Yang & Kifer, 2003b). particular, Flora-2, reifiedstatements ${s(p o)}$ objects. contrast, RDF(S), referred URI blank node x, associated following RDF triples:rdf :type(x, rdf :Statement), rdf :subject(x, s), rdf :predicate(x, p), rdf :object(x, o).RDF(S) model theory (and thus, theory), special semantics given reifiedstatements. Flora-2, anonymous resources handled skolemization (similarlytheory).Notation 3 (N3) (Berners-Lee, Connolly, Kagal, Scharf, & Hendler, 2008) provideshuman readable syntax RDF also extends RDF adding numerous predefined constructs (built-ins) able express rules conveniently. Remarkably,N3 contains built-in (log:definitiveDocument) making restricted completeness assumptions another built-in (log:notIncludes) expressing simple negation-as-failuretests. addition constructs motivated use cases. However, N3provide strong negation closed-world reasoning fully supported. N3 supportedCWM system17 , forward engine especially designed Semantic Web,Euler system18 , backward engine relying loop checking techniques guaranteetermination.Alferes et al. (2003) propose paraconsistent well-founded semantics explicitnegation (WFSXP )19 , appropriate semantics reasoning (possibly, contradictory) information Semantic Web. Supporting arguments include: (i) possiblereasoning, even presence contradiction, (ii) program transformation WFS,(iii) polynomial time inference procedures. formal model theory explicitlyprovided integrated logic.DR-Prolog (Antoniou, Bikakis, & Wagner, 2004) DR-DEVICE (Bassiliades, Antoniou, & Vlahavas, 2004) two systems integrate RDFS ontologies rules (strictdefeasible), partially ordered superiority relation, based semanticsdefeasible logic (Antoniou, Billington, Governatori, & Maher, 2001; Maher, 2002). Defeasible logic contains one kind negation (strong negation) object language20allows reason presence contradiction incomplete information. supports17. http://www.w3.org/2000/10/swap/doc/cwm.html.18. http://www.agfa.com/w3c/euler/.19. WFSXP (Alferes, Damasio, & Pereira, 1995) extension well-founded semantics explicitnegation (WFSX) extended logic programs (Pereira & Alferes, 1992) and, thus, also wellfounded semantics (WFS) normal logic programs (Gelder et al., 1991).20. However, defeasible logic, negation-as-failure easily simulated language ingredients.68fiExtended RDF Semantic Foundation Rule Markup Languagesmonotonic non-monotonic rules, exceptions, default inheritance, preferences.formal model theory explicitly provided integrated logic.OWL-DL (McGuinness & van Harmelen, 2004) ontology representation languageSemantic Web, syntactic variant SHOIN (D) description logicdecidable fragment first-order logic (Horrocks & Patel-Schneider, 2003). However,need extending expressive power OWL-DL rules initiated several studies,including SWRL (Semantic Web Rule Language) proposal (Horrocks, Patel-Schneider,Boley, Tabet, Grosof, & Dean, 2004). Horrocks & Patel-Schneider (2004) showextension general undecidable. AL-log (Donini, Lenzerini, Nardi, & Schaerf, 1998)one first efforts integrate Description Logics (safe) datalog rules,achieving decidability. considers basic description logic ALC imposes constraint concept DL-atoms allowed appear body rules, whereasheads rules always non DL-atoms. Additionally, variable appearingconcept DL atom body rule also appear non DL-atom bodyhead rule. CARIN (Levy & Rousset, 1998) provides framework studyingeffects combining description logic ALCN R (safe) datalog rules. CARIN,concept role DL-atoms allowed body rules. shownintegration decidable rules non-recursive, certain combinations constructorsallowed DL component, rules role-safe (imposing constraintvariables role DL atoms body rules)21 . Motik et al. (2004) showintegration SHIQ(D) knowledge base L disjunctive datalog program P decidable, P DL-safe, is, variables rule occur least one non DL-atombody rule. work, contrast AL-log CARIN, tableaux algorithmemployed query answering L translated disjunctive logic program DD(L)combined P answering ground queries.category works, entailment DL, extended rules,based first-order logic. means DL component logic programviewed set first-order logic statements. Thus, negation-as-failure, closed-worldassumptions, non-monotonic reasoning cannot supported. contrast, worksupports weak strong negation, allows closed-world open-world reasoningselective basis.different kind integration achieved Eiter et al. (2004a). work,SHOIN (D) knowledge base L communicates extended logic program P (possiblyweak strong negation), DL-query atoms body rules.particular, description logic component L used answering augmented,input logic program, queries appearing (possibly weakly negated) DL-queryatoms, thus allowing flow knowledge P L vice-versa. answer set semantics hL, P defined, generalization answer set semantics (Gelfond& Lifschitz, 1990) ordinary extended logic programs. similar kind integrationachieved Eiter et al. (2004b). work, SHOIN (D) knowledge base L communicates normal logic program P (possibly weak negation), DL-queryatoms body rules. well-founded semantics hL, P defined,21. rule role-safe least one variables x, role DL atom R(x, y) bodyrule, appears body atom base predicate, base predicate ordinary predicateappears facts rule bodies.69fiAnalyti, Antoniou, Damasio, & Wagnergeneralization well-founded semantics (Gelder et al., 1991) ordinary normal logicprograms. Obviously, works, derived information concerns non DLatoms (that possibly used input DL-query atoms). Thus, rule-based reasoningsupported non DL-atoms. contrast, work, properties classes appearing ERDF graphs freely appear heads bodies rules, allowingeven derivation metalevel statements subclass subproperty relationships,property transitivity, property class totalness.Rosati (1999) defines semantics disjunctive AL-log knowledge base, basedstable model semantics disjunctive databases (Gelfond & Lifschitz, 1991), extendingAL-log (Donini et al., 1998). disjunctive AL-log knowledge base integrationALC knowledge base (safe) disjunctive logic program P allows conceptrole DL-atoms body rules (along weak negation non DL-atoms).safety condition enforces variable head rule also appearbody rule. Additionally, constants P DL-individuals. Similarlycase, defining disjunctive AL-log semantics, grounded versionsrules considered (by instantiating variables DL individuals). However rule-basedreasoning supported non DL-atoms, DL-atoms body rulesmainly express constraints.subsequent work, Rosati (2005) defines r-hybrid knowledge bases. r-hybridknowledge bases, DL-atoms allowed head rules DL componentSHOIN (D) knowledge base. Additionally, constants P necessarily DLindividuals. However, stronger safety condition imposed, rule variableappear (positive) non DL-atom body rule. Additionally, weak negationallowed non DL-atoms rule-based meta-reasoning supported. general,say non DL-atoms, closed-world assumption made, DL-atomsconform open-world assumption, SHOIN (D) fragment first-order logic.11. Conclusionspaper, extended RDF graphs ERDF graphs allowing negative triplesrepresenting explicit negative information. Then, proceeded defining ERDFontology ERDF graph complemented set derivation rules connectives(weak negation), (strong negation), (material implication), , , , bodyrule, strong negation head rule. Moreover, extendedRDF(S) vocabulary adding predefined vocabulary elements erdf :TotalPropertyerdf :TotalClass, representing metaclasses total properties total classes,open-world assumption applies.defined ERDF formulas, ERDF interpretations, ERDF entailmentERDF formulas, showing conservatively extends RDFS entailment RDF graphs.developed model-theoretic semantics ERDF ontologies, called ERDF stablemodel semantics, showing stable model entailment extends ERDF entailment ERDFgraphs, thus also extends RDFS entailment RDF graphs. ERDF stable modelsemantics based Partial Logic and, particular, generalized definition stablemodels (Herre & Wagner, 1997; Herre et al., 1999) (which extends answer set semanticsextended logic programs). shown classical (boolean) Herbrand model70fiExtended RDF Semantic Foundation Rule Markup Languagesreasoning special case semantics, properties total. case,similarly classical logic, open-world assumption made properties classestwo negations (weak strong negation) collapse. Allowing (a) totalityproperties classes declared selective basis (b) explicit representationclosed-world assumptions (as derivation rules) enables combination open-worldclosed-world reasoning framework.particular, total property p, open-world assumption applies, sinceconsidered Herbrand interpretation I, computation ERDF stable models, satisfiesp(x, y)p(x, y), pair (x, y) ontology vocabulary terms. closed property p,default closure rule form p(?x, ?y) p(?x, ?y) added, allows inferfalsity p(x, y), evidence p(x, y) holds. However, methodworks partial properties. total property p, may happen stablemodel, p(x, y) holds, even though evidence (see exampleSection 5, Proposition 5.1). fact, p total property, existencecorresponding default closure rule affect ontology semantics.main advantages ERDF summarized follows:Tarski-style model theory, desirable feature logic languagesSemantic Web (Bry & Marchiori, 2005).based Partial Logic (Herre et al., 1999), simplest conservativeextension classical logic supports weak strong negation. Partiallogic also extends Answer Set Programming (ASP)22 (Gelfond & Lifschitz, 1990),allowing logical factors , , , , , , body rule.enables combination open-world (monotonic) closed-world (non-monotonic)reasoning, framework.extends RDFS ontologies derivation rules integrity constraints.Satisfiability entailment ERDF stable model semantics general undecidable. subsequent paper, plan identify syntactic restrictions ERDFontologies guarantee decidability reasoning elaborate ERDF computability complexity issues.work, consider coherent ERDF interpretations. However, dueSemantic Webs decentralized distributed nature, contradictory information frequent(Schaffert, Bry, Besnard, Decker, Decker, Enguix, & Herzig, 2005). Though Partial Logicallows truth-value clashes, handling inconsistency Semantic Web topicdeserves extended treatment, outside scope paper. futureplans consider general ERDF interpretations extend vocabulary ERDFterms erdf :CoherentProperty erdf :CoherentClass, whose instances propertiesclasses satisfy coherence. Thus, coherence decided per property22. ASP well-known accepted knowledge representation formalism allows (through credulousreasoning) definition concepts ranging space choices. feature enables compactrepresentation search optimization problems (Eiter, Ianni, Polleres, & Schindlauer, 2006).71fiAnalyti, Antoniou, Damasio, & Wagnerper class basis. Admitting incoherent models interesting combinationsecond preference criterion minimal incoherence (Herre et al., 1999).future work also concerns support datatypes, including XSD datatypes,extension predefined ERDF vocabulary adding useful constructs, possiblyaccordance extensions ter Horst (2005b). also plan formally defineERDF/XML syntax, briefly presented Section 7. Moreover, plan implementERDF inference engine.Finally, would like mention success Semantic Web impossible without support modularity, encapsulation, information hiding, access control.Modularity mechanisms syntactic restrictions merging knowledge bases Semantic Web explored Damasio et al. (2006). However, work, knowledge basesexpressed extended logic programs. future plans include extension ERDFmechanisms allowing sharing knowledge different ERDF ontologies, alonglines proposed Damasio et al. (2006).Acknowledgmentsauthors would like thank reviewers valuable comments. researchpartially funded European Commission Swiss Federal Office Education Science within 6th Framework Programme project REWERSEnum. 506779 (www.rewerse.net).Appendix A: RDF(S) Semanticsself-containment, Appendix, review definitions simple, RDF,RDFS interpretations, well definitions satisfaction RDF graph RDFSentailment. details, see W3C Recommendation RDF semantics (Hayes, 2004).Let URI denote set URI references, PL denote set plain literals, Ldenote set typed literals, respectively. vocabulary V subset URI PL L.vocabulary RDF, VRDF , vocabulary RDFS, VRDF , shown Table1 (Section 3).Definition A.1 (Simple interpretation) simple interpretation vocabulary Vconsists of:non-empty set resources ResI , called domain universe I.set properties P ropI .vocabulary interpretation mapping IV : V URI ResI P ropI .property extension mapping PT : P ropI P(ResI ResI ).mapping ILI : V L ResI .set literal values LV ResI , contains V PL.define mapping: : V ResI P ropI that:I(x) = IV (x), x V URI.72fiExtended RDF Semantic Foundation Rule Markup LanguagesI(x) = x, x V PL.I(x) = ILI (x), x V L.Definition A.2 (Satisfaction RDF graph w.r.t. simple interpretation) LetG RDF graph let simple interpretation vocabulary V . Let vmapping v : Var (G) ResI . x Var (G), define [I + v](x) = v(x). x V ,define [I + v](x) = I(x). define:I, v |= G iff p(s, o) G, holds that: p V, s, V Var , I(p) P ropI ,h[I + v](s), [I + v](o)i PT (I(p)).satisfies RDF graph G, denoted |= G, iff exists mapping v :Var (G) ResI I, v |= G.rdf :type(rdf :type, rdf :Property)rdf :type(rdf :subject, rdf :Property)rdf :type(rdf :predicate, rdf :Property)rdf :type(rdf :object, rdf :Property)rdf :type(rdf :f irst, rdf :Property)rdf :type(rdf :rest, rdf :Property)rdf :type(rdf :value, rdf :Property)rdf :type(rdf : i, rdf :Property), {1, 2, ...}rdf :type(rdf :nil, rdf :List)Table 2: RDF axiomatic triplesDefinition A.3 (RDF interpretation) RDF interpretation vocabulary Vsimple interpretation V VRDF , satisfies following semantic conditions:1. x P ropI iff hx, I(rdf :Property)i PT (I(rdf :type)).2. srdf :XMLLiteral V well-typed XML literal string,ILI (srdf :XMLLiteral ) XML value s,ILI (srdf :XMLLiteral ) LV ,hILI (srdf :XMLLiteral ), I(rdf :XMLLiteral )i PT (I(rdf :type)).3. srdf :XMLLiteral V ill-typed XML literal stringILI (srdf :XMLLiteral ) ResI LV ,hILI (srdf :XMLLiteral ), I(rdf :XMLLiteral )i 6 PT (I(rdf :type)).4. satisfies RDF axiomatic triples, shown Table 2.Definition A.4 (RDF entailment) Let G, G RDF graphs. say G RDFentails G (G |=RDF G ) iff every RDF interpretation I, |= G |= G .Definition A.5 (RDFS interpretation) RDFS interpretation vocabulary VRDF interpretation V VRDF VRDF , extended new ontological categoryClsI ResI classes, well class extension mapping CT : ClsI P(ResI ),that:73fiAnalyti, Antoniou, Damasio, & Wagnerrdfs:domain(rdf :type, rdfs:Resource)rdfs:domain(rdfs:domain, rdf :Property)rdfs:domain(rdfs:range, rdf :Property)rdfs:domain(rdfs:subPropertyOf , rdf :Property)rdfs:domain(rdfs:subClassOf , rdfs:Class)rdfs:domain(rdf :subject, rdf :Statement)rdfs:domain(rdf :predicate, rdf :Statement)rdfs:domain(rdf :object, rdf :Statement)rdfs:domain(rdfs:member, rdfs:Resource)rdfs:domain(rdf :f irst, rdf :List)rdfs:domain(rdf :rest, rdf :List)rdfs:domain(rdfs:seeAlso, rdfs:Resource)rdfs:domain(rdfs:isDef inedBy, rdfs:Resource)rdfs:domain(rdfs:comment, rdfs:Resource)rdfs:domain(rdfs:label, rdfs:Resource)rdfs:domain(rdfs:value, rdfs:Resource)rdfs:range(rdf :type, rdfs:Class)rdfs:range(rdfs:domain, rdfs:Class)rdfs:range(rdfs:range, rdfs:Class)rdfs:range(rdfs:subPropertyOf , rdf :Property)rdfs:range(rdfs:subClassOf , rdfs:Class)rdfs:range(rdf :subject, rdfs:Resource)rdfs:range(rdf :predicate, rdfs:Resource)rdfs:range(rdf :object, rdfs:Resource)rdfs:range(rdfs:member, rdfs:Resource)rdfs:range(rdf :f irst, rdfs:Resource)rdfs:range(rdf :rest, rdf :List)rdfs:range(rdfs:seeAlso, rdfs:Resource)rdfs:range(rdfs:isDef inedBy, rdfs:Resource)rdfs:range(rdfs:comment, rdfs:Literal)rdfs:range(rdfs:label, rdfs:Literal)rdfs:range(rdf :value, rdfs:Resource)rdfs:subClassOf (rdf :Alt, rdfs:Container)rdfs:subClassOf (rdf :Bag, rdfs:Container)rdfs:subClassOf (rdf :Seq, rdfs:Container)rdfs:subClassOf (rdfs:ContainerMembershipProperty, rdf :Property)rdfs:subPropertyOf (rdfs:isDef inedBy, rdfs:seeAlso)rdf :type(rdf :XMLLiteral , rdfs:Datatype)rdfs:subClassOf (rdf :XMLLiteral , rdfs:Literal)rdfs:subClassOf (rdfs:Datatype, rdfs:Class)rdf :type(rdf : i, rdfs:ContainerMembershipProperty), {1, 2, ...}rdfs:domain(rdf : i, rdfs:Resource), {1, 2, ...}rdfs:range(rdf : i, rdfs:Resource), {1, 2, ...}Table 3: RDFS axiomatic triples74fiExtended RDF Semantic Foundation Rule Markup Languages1. x CT (y) iff hx, yi PT (I(rdf :type)).2. ontological categories defined follows:ClsI = CT (I(rdfs:Class)),ResI = CT (I(rdfs:Resource)),LV = CT (I(rdfs:Literal)).3. hx, yi PT (I(rdfs:domain)) hz, wi PT (x) z CT (y).4. hx, yi PT (I(rdfs:range)) hz, wi PT (x) w CT (y).5. x ClsI hx, I(rdfs:Resource)i PT (I(rdfs:subClassOf )).6. hx, yi PT (I(rdfs:subClassOf )) x, ClsI , CT (x) CT (y).7. PT (I(rdfs:subClassOf )) reflexive transitive relation ClsI .8. hx, yi PT (I(rdfs:subPropertyOf )) x, P ropI , PT (x) PT (y).9. PT (I(rdfs:subPropertyOf )) reflexive transitive relation P ropI .10. x CT (I(rdfs:Datatype)) hx, I(rdfs:Literal)i PT (I(rdfs:subClassOf )).11. x CT (I(rdfs:ContainerM embershipP roperty))hx, I(rdfs:member)i PT (I(rdfs:subPropertyOf )).12. satisfies RDFS axiomatic triples, shown Table 3.Definition A.6 (RDFS entailment) Let G, G RDF graphs. say G RDFSentails G (G |=RDF G ) iff every RDFS interpretation I, |= G |= G .Appendix B: ProofsAppendix, prove lemmas propositions presented main paper.addition, provide Lemma B.1, used proofs. reduce sizeproofs, eliminated namespace URIs VRDF VRDF VERDF .Lemma B.1 Let F ERDF formula let partial interpretationvocabulary V . Let u, u mappings u, u : Var (F ) ResI u(x) = u (x),x FVar (F ). holds: I, u |= F iff I, u |= F .Proof: prove proposition induction. Without loss generality, assumeappears front positive ERDF triples. Otherwise apply transformationrules Definition 3.4, get equivalent formula satisfies assumption.Let F = p(s, o). holds: I, u |= F iff p V , s, V Var , I(p) P ropI ,h[I + u](s), [I + u](o)i PT (I(p)) iff p V , s, V Var , I(p) P ropI , h[I +u ](s), [I + u ](o)i PT (I(p)) iff I, u |= p(s, o).Let F = p(s, o). holds: I, u |= F iff p V , s, V Var , I(p) P ropI ,h[I + u](s), [I + u](o)i PF (I(p)) iff p V , s, V Var , I(p) P ropI ,h[I + u ](s), [I + u ](o)i PF (I(p)) iff I, u |= p(s, o).Assumption: Assume lemma holds subformulas F .show lemma holds also F .Let F = G. holds: I, u |= F iff I, u |= G iff VG V I, u 6|= G iff VG VI, u 6|= G iff I, u |= G iff I, u |= F .75fiAnalyti, Antoniou, Damasio, & WagnerLet F = F1 F2 . holds: I, u |= F iff I, u |= F1 F2 iff I, u |= F1 I, u |= F2 iffI, u |= F1 I, u |= F2 iff I, u |= F1 F2 iff I, u |= F .Let F = x G. show (i) I, u |= F I, u |= F (ii) I, u |= FI, u |= F .(i) Let I, u |= F . Then, I, u |= xG. Thus, exists mapping u1 : Var (G) ResIs.t. u1 (y) = u(y), Var (G) {x}, I, u1 |= G. Let u2 mapping u2 :Var (G) ResI s.t. u2 (y) = u (y), Var (G) {x}, u2 (x) = u1 (x). Since u(z) =u (z), z FVar (F ) x FVar (G), follows u1 (z) = u2 (z), z FVar (G).Thus, I, u2 |= G. Therefore, exists mapping u2 : Var (G) ResI s.t. u2 (y) = u (y),Var (G) {x}, I, u2 |= G. Thus, I, u |= x G, implies I, u |= F .(ii) prove statement similarly (i) exchanging u u .Let F = F1 F2 F = F1 F2 F = xG. prove, similarlycases, I, u |= F iff I, u |= F .Lemma 3.1. Let G ERDF graph let partial interpretation vocabularyV . holds: |=GRAPH G iff |= formula(G).Proof: Let G = {t1 , ..., tn } F = formula(G).) Assume |=GRAPH G, show |= F . Since |=GRAPH G, followsv : Var (G) ResI I, v |= ti , = 1, ..., n. Thus, v : Var (G) ResII, v |= t1 ...tn . implies u : Var (G) ResI I, u |= F . SinceFVar (F ) = , follows Lemma B.1 u : Var (G) ResI , holds I, u |= F .Thus, |= F .) Assume |= F , show |=GRAPH G. Since |= F , followsv : Var (G) ResI holds I, v |= F . Thus, v : Var (G) ResII, v |= F . implies u : Var (G) ResI I, u |= t1 ...tn . Thus,u : Var (G) ResI I, u |= ti , = 1, ..., n. Therefore, |=GRAPH G.Proposition 3.1. Let ERDF interpretation vocabulary V let V =V VRDF VRDF VERDF . Then,1. p, s, V I(p) TProp , holds:|= p(s, o) iff |= p(s, o) (equivalently, |= p(s, o) p(s, o)).2. x, c V I(c) TCls , holds:|= rdf :type(x, c) iff |= rdf :type(x, c)(equivalently, |= rdf :type(x, c) rdf :type(x, c)).Proof:1) holds: |= p(s, o) iff 6|= p(s, o) iff hI(s), I(o)i 6 PT (p) iff (since p TProp )hI(s), I(o)i PF (p) iff |= p(s, o). Therefore, |= p(s, o) iff |= p(s, o).also show |= p(s, o) p(s, o). holds |= p(s, o) |= p(s, o).implies |= p(s, o) |= p(s, o), thus, |= p(s, o) p(s, o).2) proof similar proof 1) replacing p(s, o) type(x, c) TPropTCls .Proposition 3.2. Let G, G RDF graphs VG VERDF = VG VERDF =. Then, G |=RDF G iff G |=ERDF G .76fiExtended RDF Semantic Foundation Rule Markup LanguagesProof:) Let G |=ERDF G . show G |=RDF G . particular, let RDFSinterpretation vocabulary V s.t. |= G, show |= G .Since |= G, holds v : Var (G) ResI s.t. I, v |= G. goal constructERDF interpretation J V s.t. J |= G. consider 1-1 mapping res : VERDF R,R set disjoint ResI . Additionally, let V = V VRDF VRDF VERDF .Based mapping res, construct partial interpretation J V follows:ResJ = ResI res(VERDF ).JV (x) = IV (x), x (V VERDF ) URI JV (x) = res(x), x VERDF .define mapping: ILJ : V L ResJ that: ILJ (x) = ILI (x).define mapping: J : V ResJ that:J(x) = JV (x), x V URI.J(x) = x, x V PL.J(x) = ILJ (x), x V L.define mapping PT J : ResJ P(ResJ ResJ ) follows:(PT1) x, y, z ResI hx, yi PT (z) hx, yi PT J (z).(PT2) hres(TotalClass), J(Class)i PT J (J(subClassOf )).(PT3) hres(TotalProperty), J(Property)i PT J (J(subClassOf )).Starting derivations (PT1), (PT2), (PT3), following rulesapplied recursively, fixpoint reached:(PT4) hx, yi PT J (J(domain)) hz, wi PT J (x)hz, yi PT J (J(type)).(PT5) hx, yi PT J (J(range)) hz, wi PT J (x)hw, yi PT J (J(type)).(PT6) hx, J(Class)i PT J (J(type))hx, J(Resource)i PT J (J(subClassOf )).(PT7) hx, yi PT J (J(subClassOf )) hx, J(Class)i PT J (J(type)).(PT8) hx, yi PT J (J(subClassOf )) hy, J(Class)i PT J (J(type)).(PT9) hx, yi PT J (J(subClassOf )) hz, xi PT J (J(type))hz, yi PT J (J(type)).(PT10) hx, J(Class)i PT J (J(type)) hx, xi PT J (J(subClassOf )).(PT11) hx, yi PT J (J(subClassOf )) hy, zi PT J (J(subClassOf ))hx, zi PT J (J(subClassOf )).(PT12) hx, yi PT J (J(subPropertyOf )) hx, J(Property)i PT J (J(type)).(PT13) hx, yi PT J (J(subPropertyOf )) hy, J(Property)i PT J (J(type)).(PT14) hx, yi PT J (J(subPropertyOf )) hz, wi PT J (x)hz, wi PT J (y).77fiAnalyti, Antoniou, Damasio, & Wagner(PT15) hx, J(Property)i PT J (J(type)) hx, xi PT J (J(subPropertyOf )).(PT16) hx, yi PT J (J(subPropertyOf )) hy, zi PT J (J(subPropertyOf ))hx, zi PT J (J(subPropertyOf )).(PT17) hx, J(Datatype)i PT J (J(type))hx, J(Literal)i PT J (J(subClassOf )).(PT18) hx, J(ContainerM embershipP roperty)i PT J (J(type))hx, J(member)i PT J (J(subPropertyOf )).reaching fixpoint, nothing else contained PT J (x), x ResJ .P ropJ = {x ResJ | hx, J(Property)} PT J (J(type))}.mapping PT J : P ropJ P(ResJ ResJ ) defined follows:PT J (x) = PT J (x), x P ropJ .LV J = {x ResJ | hx, J(Literal)i PT J (J(type))}.mapping PF J : P ropJ P(ResJ ResJ ) defined follows:(PF1) srdf :XMLLiteral V ill-typed XML-LiteralhILJ (srdf :XMLLiteral ), J(Literal)i PF J (J(type)).(PF2) hJ(TotalClass), J(TotalClass)i PT J (J(type))x ResJ {J(TotalClass)}, hx, J(TotalClass)i PF J (J(type)).(PF3) hJ(TotalProperty), J(TotalProperty)i PT J (J(type))x, ResJ , hx, yi PF J (J(TotalProperty)).Starting derivations (PF1), (PF2), (PF3), following rulesapplied recursively, fixpoint reached:(PF4) hx, yi PT J (J(subClassOf )) hz, yi PF J (J(type))hz, xi PF J (type).(PF5) hx, yi PT J (J(subPropertyOf )) hz, wi PF J (y)hz, wi PF J (x).reaching fixpoint, nothing else contained PF J (x), x P ropJ .continue, prove following lemma:Lemma: x, y, x ResJ , hx, yi PT J (z) iff hx, yi PT J (z).Proof :) hx, yi PT J (z), definition PT J , follows immediatelyhx, yi PT J (z).) Let hx, yi PT J (z). Then, definition PT J , follows holds (i)z P ropI (ii) w ResJ , s.t. hw, zi PT J (J(subPropertyOf )).(i) Assume z P ropI . Then, hz, I(Property)i PT (I(type)). implieshz, J(Property)i PT (J(type)). (PT1), follows hz, J(v)i PT J (J(type)).Therefore, z P ropJ . definition PT J , follows hx, yi PT J (z).78fiExtended RDF Semantic Foundation Rule Markup Languages(ii) Assume w ResJ s.t. hw, zi PT J (J(subPropertyOf )). Then, (PT13),follows hz, J(Property)i PT J (J(type)). Therefore, z P ropJ . definitionPT J , follows hx, yi PT J (z).End LemmaThough mentioned explicitly, Lemma used throughout restproof.show J partial interpretation V , enough show V PL LV J .Let x V PL. Then, x LV . Thus, hx, I(Literal)i PT (I(type)). Due (PT1),implies hx, J(Literal)i PT J (J(type)). Thus, x LV J .Now, extend J ontological categories:ClsJ = {x ResJ | hx, J(Class)i PT J (J(type))},TCls J = {x ResJ | hx, J(TotalClass)i PT J (J(type))},TProp J = {x ResJ | hx, J(TotalProperty)i PT J (J(type))}.define CT J , CF J : ClsJ P(ResJ ) follows:x CT J (y) iff hx, yi PT J (J(type)),x CF J (y) iff hx, yi PF J (J(type)).show J ERDF interpretation V . Specifically, showJ satisfies semantic conditions Definition 3.7 (ERDF interpretation) Definition3.2 (Coherent ERDF interpretation).First, show J satisfies semantic condition 2 Definition 3.7. startproving ResJ = CT J (J(Resource)). Obviously,CT J (J(Resource)) ResJ . Thus, enough prove ResJ CT J (J(Resource)).Let x ResJ . Then, distinguish following cases:Case 1) x ResI . Since RDFS interpretation, holds hx, I(Resource)iPT (I(type)). Thus, holds hx, J(Resource)i PT J (J(type)), implies xCT J (J(Resource)).Case 2) x res(VERDF ). definition PT J , followshx, J(Resource)i PT J (J(type)). Thus, hx, J(Resource)i PT J (J(type)), impliesx CT J (J(Resource)).Thus, ResJ = CT J (J(Resource)).Additionally, easy see holds P ropJ = CT J (J(Property)), ClsJ =CT J (J(Class)), LV J = CT J (J(Literal)), TCls J = CT J (J(TotalClass)),TProp J = CT J (J(TotalProperty)).show J satisfies semantic condition 3 Definition 3.7. Let hx, yiPT J (J(domain)) hz, wi PT J (x). Then, (PT4) definition CT J ,follows z CT J (y).show J satisfies semantic condition 4 Definition 3.7. Let hx, yiPT J (J(range)) hz, wi PT J (x). Then, (PT5) definition CT J ,follows w CT J (y).show J satisfies semantic condition 5 Definition 3.7. Let xClsJ . Thus, holds: hx, J(Class)i PT J (J(type)). (PT6), followshx, J(Resource)i PT J (J(subClassOf )).79fiAnalyti, Antoniou, Damasio, & Wagnershow J satisfies semantic condition 6 Definition 3.7. Let hx, yiPT J (J(subClassOf )). Then, (PT7), (PT8), definition CT J , followsx, ClsJ .Let hx, yi PT J (J(subClassOf )). show CT J (x) CT J (y). particular,let z CT J (x). Then, (PT9) definition CT J , follows z CT J (y).Let hx, yi PT J (J(subClassOf )). show CF J (y) CF J (x). particular,let z CF J (y). Then, (PF4) definition CF J , follows z CF J (x).similar manner, prove J also satisfies semantic conditions 7, 8, 9,10, 11 Definition 3.7.continue rest proof, need make observations.Consider mapping h : ResJ ResI , defined follows:x ResIxI(Class)x = res(TotalClass)h(x) =I(Property) x = res(TotalProperty)Observation 1: hx, yi PT J (z) res(VERDF ) x = y.Observation 2: x res(VERDF ) x P ropJ PT J (x) = .Observation 3: hx, yi PT J (z) hh(x), h(y)i PT (h(z)).Observation 4: x, y, z ResI hx, yi PT J (z) hx, yi PT (z)23 .proof observations made induction. easy see observationshold derivations (PT1), (PT2), (PT3). Assume observationshold derivations obtained step k application fixpoint operatorPT J . Then, observations also hold derivations obtained step k + 1.show J satisfies semantic condition 12 Definition 3.7. Let xTCls J . Thus, hx, J(TotalClass)i PT J (J(type)). Observation 1, follows x =J(TotalClass). (PF2), follows CT J (J(TotalClass))CF J (J(TotalClass)) =ResJ . Thus, CT J (x) CF J (x) = ResJ .show J satisfies semantic condition 13 Definition 3.7. Let xTProp J . Thus, hx, J(TotalProperty)i PT J (J(type)). Observation 1, followsx = J(TotalProperty). (PF3), follows PT J (J(TotalProperty))PF J (J(TotalProperty)) = ResJ ResJ . Thus, PT J (x) PF J (x) = ResJ ResJ .show J satisfies semantic condition 14 Definition 3.7.Let srdf :XMLLiteral well-typed XML-Literal V ILJ (srdf :XMLLiteral )= ILI (srdf :XMLLiteral ) XML value s. Additionally, since RDFSinterpretation V , holds: hILI (srdf :XMLLiteral ), I(XMLLiteral )i PT (I(type)).Therefore, (PT1), follows hILJ (srdf :XMLLiteral ), J(XMLLiteral )iPT J (J(type)).show J satisfies semantic condition 15 Definition 3.7. Letsrdf :XMLLiteral V s.t. well-typed XML literal string. AssumeILJ (srdf :XMLLiteral ) LV J . Then, hILJ (srdf :XMLLiteral ), J(Literal)iPT J (J(type)). Observation 4, follows hILJ (srdf :XMLLiteral ), J(Literal)iPT (J(type)). Therefore, follows hILI (srdf :XMLLiteral ), I(Literal)i23. Note Observation 3 implies Observation 4.80fiExtended RDF Semantic Foundation Rule Markup LanguagesPT (I(type)). Thus, ILI (srdf :XMLLiteral ) LV , impossible sinceRDFS interpretation V . Therefore, ILJ (srdf :XMLLiteral ) ResJ LV J .Additionally, (PF1), follows hILJ (srdf :XMLLiteral ), J(Literal)iPF J (J(type)).J also satisfies semantic condition 16 Definition 3.7, due (PT1). Finally, J satisfiessemantic condition 17, due (PT2) (PT3).Thus, J ERDF interpretation V .Now, show J coherent ERDF interpretation (Definition 3.2). Assumecase. Thus, z P ropJ s.t. PT J (z) PF J (z) 6= . Assumehx, yi PT J (z) PF J (z), z. distinguish following cases:Case 1) z res(VERDF ). Then, Observation 2, follows PT J (z) = ,contradiction.Case 2) res(VERDF ) z ResI . Then, holds:(i) hz, res(TotalProperty)i PT J (J(subPropertyOf )),(ii) hz, J(type)i PT J (J(subPropertyOf )) hx, yi PF J (J(type)).Now, Observation 1 since z ResI , (i) impossible. Thus, hz, J(type)iPT J (J(subPropertyOf )) hx, yi PF J (J(type)). implies= res(TotalClass). Observation 1, follows x = res(TotalClass),impossible since, due (PF2), hres(TotalClass), res(TotalClass)i 6 PF J (J(type)).Case 3) x res(VERDF ) y, z ResI . Then, holds:(i) hz, res(TotalProperty)i PT J (J(subPropertyOf )),(ii) hz, J(type)i PT J (J(subPropertyOf )) hx, yi PF J (J(type)).Now, Observation 1 since z ResI , (i) impossible. Thus, hz, J(type)iPT J (J(subPropertyOf )) hx, yi PF J (J(type)). implies= res(TotalClass), impossible, since ResI .Case 4) x, y, z ResI . Then, x = ILJ (s), ill-typed XML-LiteralV , hz, J(type)i PT J (J(subPropertyOf )) hy, J(Literal)i PT J (J(subClassOf )).Since hx, yi PT J (z), follows hx, yi PT J (J(type)). Since hy, J(Literal)iPT J (J(subClassOf )), follows hx, J(Literal)i PT J (J(type)). Observation 4,follows hILJ (s), J(Literal)i PT (J(type)). Therefore,hILI (s), I(Literal)i PT (I(type)). implies ILI (s) LV , impossible since RDFS interpretation V .Since cases lead contradiction, follows that:z P ropJ , PT J (z) PF J (z) = .show J, v |= G. Let p(s, o) G. Since I, v |= G, holdsp V , s, V Var . Note that, due (PT1), holds P ropI P ropJ . Sincep 6 VERDF , holds J(p) = I(p) P ropI P ropJ . Since s, 6 VERDF , holds[I + v](s) = [J + v](s) [I + v](o) = [J + v](o). Since I, v |= G, holds h[I + v](s), [I +v](o)i PT (I(p)). Thus, h[J + v](s), [J + v](o)i PT (J(p)). (PT1), followsh[J + v](s), [J + v](o)i PT J (J(p)). Thus, J, v |= G, implies J |= G.Since J ERDF interpretation G |=ERDF G , follows J |= G . Thus,u : Var (G ) ResJ = ResI res(VERDF ) s.t. J, u |= G . define mappingu : Var (G ) ResI follows:81fiAnalyti, Antoniou, Damasio, & Wagneru(x) ResIu(x)I(Class)u(x) = res(TotalClass)u (x) =I(Property) u(x) = res(TotalProperty)show I, u |= G . Let p(s, o) G . Since J |= G VG VERDF = ,follows p V VRDF VRDF , s, V VRDF VRDF Var , J(p)P ropJ . Thus, hJ(p), J(type)i PT J (J(Property)), implies (since p 6 VERDF )hI(p), I(type)i PT J (I(Property). Due Observation 4, follows hI(p), I(type)iPT (I(Property). Thus, I(p) P ropI . Additionally, holds: h[J + u](s), [J + u](o)iPT J (J(p)). want show h[I + u ](s), [I + u ](o)i PT (I(p)).Case 1) holds: (i) Var (G ) u(s) 6 res(VERDF ) (ii) Var (G )u(o) 6 res(VERDF ).Then, [J + u](s) = [J + u ](s) = [I + u ](s) ResI , [J + u](o) = [J + u ](o) = [I +u ](o) ResI , J(p) = I(p) ResI . Thus, h[J + u](s), [J + u](o)i PT J (J(p)) impliesh[I + u ](s), [I + u ](o)i PT J (I(p)). Observation 4, latter impliesh[I + u ](s), [I + u ](o)i PT (I(p)).Case 2) holds: (i) Var (G ) u(s) res(VERDF ) (ii) Var (G )u(o) 6 res(VERDF ).Assume u(s) = res(TotalClass), [J + u](o) = y, J(p) = z. y, z ResI .Additionally, I(p) = J(p) = z [I + u ](o) = [J + u](o) = y. Thus, h[I + u ](s), [I +u ](o)i = hI(Class), yi. holds hres(TotalClass), yi PT J (z). Due Observation 3,holds hI(Class), yi PT (z). Thus, h[I + u ](s), [I + u ](o)i = hI(Class), yi PT (z) =PT (I(p)).Similarly, u(s) = res(TotalProperty), prove h[I+u ](s), [I+u ](o)i PT (I(p)).Case 3) holds: Var (G ) u(o) res(VERDF ). Then, Observation 1,follows Var (G ) u(s) = u(o). Assume u(o) = res(TotalClass),J(p) = z. Then, z ResI I(p) = J(p) = z. Additionally, h[I + u ](s), [I + u ](o)i =hI(Class), I(Class)i. holds hres(TotalClass), res(TotalClass)i PT J (z). Due Observation 3, follows hI(Class), I(v)i PT (z). Thus, h[I + u ](s), [I + u ](o)i =hI(Class), I(Class)i PT (z) = PT (I(p)).Similarly, u(o) = res(TotalProperty), prove h[I+u ](s), [I+u ](o)i PT (I(p)).cases, holds h[I + u ](s), [I + u ](o)i = PT (I(p)), follows I, u |= G ,implies |= G .) Let G |=RDF G . show G |=ERDF G . Let ERDF interpretationvocabulary V , |= G. Thus, u : Var (G) ResI s.t. I, u |= G.show |= G .define V = V VRDF VRDF VERDF . Based I, construct RDFS interpretation J V that: ResJ = ResI , P ropJ = P ropI , LV J = LV , ClsJ =ClsI , JV (x) = IV (x), x V URI, PT J (x) = PT (x), x P ropJ , ILJ (x) =ILI (x), x V L, CT J (x) = CT (x), x ClsJ .show J indeed RDFS interpretation V .First, show J satisfies semantic condition 1 Definition A.3 (AppendixA, RDF interpretation). holds: x P ropJ iff x P ropI iff x CT (I(Property)) iffhx, I(Property)i PT (I(type)) iff hx, J(Property)i PT J (J(type)).82fiExtended RDF Semantic Foundation Rule Markup Languagesshow J satisfies semantic condition 2 Definition A.3.Let srdf :XMLLiteral V well-typed XML literal string. Then,follows definition J fact ERDF interpretation VILJ (srdf :XMLLiteral ) XML value s, ILJ (srdf :XMLLiteral )CT J (J(XMLLiteral )). show ILJ (srdf :XMLLiteral ) LV J . SinceERDF interpretation, ILI (srdf :XMLLiteral ) CT (I(XMLLiteral )). Additionally,hI(XMLLiteral ), I(Literal)i PT (I(subClassOf )). Therefore, ILI (srdf :XMLLiteral )CT (I(Literal)), thus, ILI (srdf :XMLLiteral ) LV . last statement impliesILJ (srdf :XMLLiteral ) LV J .show J satisfies semantic condition 3 Definition A.3.Let srdf :XMLLiteral V ill-typed XML literal string. Then,follows definition J fact ERDF interpretation VILJ (srdf :XMLLiteral ) ResJ LV J . showhILJ (srdf :XMLLiteral ), J(XMLLiteral )i 6 PT J (J(type)). AssumehILJ (srdf :XMLLiteral ), J(XMLLiteral )i PT J (J(type)). Then,hILI (srdf :XMLLiteral ), I(XMLLiteral )i PT (I(type)). Thus,ILI (srdf :XMLLiteral ) CT (I(XMLLiteral )). Since holdshI(XMLLiteral ), I(Literal)i PT (I(subClassOf )), followsILI (srdf :XMLLiteral ) CT (I(Literal)). Thus, ILI (srdf :XMLLiteral ) LV ,impossible since ERDF interpretation V . Therefore,hILJ (srdf :XMLLiteral ), J(XMLLiteral )i 6 PT J (J(type)).easy see J satisfies semantic condition 4 Definition A.3semantic conditions Definition A.5 (Appendix A, RDFS Interpretation). Therefore, JRDFS interpretation V .show J, u |= G. Let p(s, o) G. Since |= G, holds p V ,s, V Var , J(p) = I(p) P ropI = P ropJ . holds: h[J + u](s), [J + u](o)iPT J (J(p)) iff h[I + u](s)), [I + u](o)i P ropI (I(p)), true, since I, u |= G. Thus,J, u |= G, implies J |= G. Since G |=RDF G , follows J |= G . Thus,v : Var (G ) ResJ s.t. J, v |= G .show |= G . Let p(s, o) G . Since J, v |= G , holds p V ,s, V Var , I(p) = J(p) P ropJ = P ropI . holds: h[I + v](s), [I + v](o)iPT (I(p)) iff h[J + v](s), [J + v](o)i PT J (J(p)), true, since J, v |= G . Thus,I, v |= G , implies |= G .Proposition 4.1. Let G ERDF graph let F ERDF formulaVF skG (Var (G)) = . holds: G |=ERDF F iff sk(G) |=ERDF F .Proof:) Let G |=ERDF F . show sk(G) |=ERDF F . Let ERDF interpretationvocabulary V s.t. |= sk(G). show |= G. define V = V VRDFVRDF VERDF . Additionally, define total function u : Var (G) ResI s.t. u(x) =IV (skG (x)), x Var (G). Moreover, define total function u : V Var (G) V s.t.u (x) = skG (x), x Var (G) u (x) = x, otherwise.Let p(s, o) G. Then, p V , s, V Var , I(p) P ropI . holds: h[I+u](s), [I+u](o)i PT (I(p)) iff hI(u (s)), I(u (o))i PT (I(p)), true, since p(u (s), u (o))sk(G) |= sk(G). Thus, I, u |= p(s, o).83fiAnalyti, Antoniou, Damasio, & WagnerLet p(s, o) G. Then, p V , s, V Var , I(p) P ropI . holds:h[I + u](s), [I + u](o)i PF (I(p)) iff hI(u (s)), I(u (o))i PF (I(p)), true, sincep(u (s), u (o)) sk(G) |= sk(G). Thus, I, u |= p(s, o).Therefore, |= G. Since G |=ERDF F , follows |= F .) Let sk(G) |=ERDF F . show G |=ERDF F . Let ERDF interpretationvocabulary V |= G. show |= F . Since |= G, totalfunction u : Var (G) ResI s.t. I, u |= G. define V = V VRDF VRDF VRDF .construct ERDF interpretation J V skG (Var (G)) follows: ResJ = ResI , P ropJ =P ropI , LV J = LV , ClsJ = ClsI . define JV : (V skG (Var (G))) URI ResJ ,1follows: JV (x) = IV (x), x V URI JV (x) = u(skG(x)), x skG (Var (G)).Moreover, PT J (x) = PT (x), x P ropJ , PF J (x) = PF (x), x P ropJ , ILJ (x) =ILI (x), x V L, CT J (x) = CT (x), x ClsJ , CF J (x) = CF (x), x ClsJ .Since ERDF interpretation V , easy see J indeed ERDF interpretation V skG (Var (G)). show J |= sk(G). First, define total func1tion g : V skG (Var (G)) V Var (G) follows: g(x) = skG(x), x skG (Var (G))g(x) = x, otherwise. Let p(s, o) sk(G). Since |= G, follows p V ,s, V Var , J(p) = I(p) P ropI = P ropJ . holds J(s) = [I + u](g(s)),J(o) = [I + u](g(o)), J(p) = I(p). Therefore, holds: hJ(s), J(o)i PT J (J(p)) iffh[I + u](g(s)), [I + u](g(o))i PT (I(p)), holds since p(g(s), g(o)) G I, u |= G.Let v : {} ResJ . follows J, v |= p(s, o). Let p(s, o) sk(G). showJ, v |= p(s, o), similar manner. Therefore, J |= sk(G).Since sk(G) |=ERDF F , follows J |= F . show |= F . defineV = V VRDF VRDF VERDF . Note ResJ = ResI .Lemma: every mapping u : Var (F ) ResJ , holds J, u |= F iff I, u |= F .Proof: prove Lemma induction. Without loss generality, assumeappears front positive ERDF triples. Otherwise apply transformationrules Definition 3.4, get equivalent formula satisfies assumption.Let F = p(s, o). Assume J, u |= F . Since VF skG (Var (G)) = , followsp V , s, V Var , J(p) = I(p) P ropI = P ropJ . Since h[J + u](s), [J + u](o)iPT J (J(p)), follows h[I + u](s), [I + u](o)i PT (I(p)). Therefore, I, u |= F .Assume I, u |= F . follows p V , s, V Var , J(p) = I(p) P ropI =P ropJ . Since h[I + u](s), [I + u](o)i PT (I(p)), follows h[J + u](s), [J + u](o)iPT J (J(p)). Therefore, J, u |= F .Let F = p(s, o). Similarly, prove J, u |= F iff I, u |= F .Assumption: Assume lemma holds subformulas F .show lemma holds also F .Let F = G. holds: I, u |= F iff VG V I, u 6|= G iff VG V J, u 6|= G iffJ, u |= F .Let F = F1 F2 . holds: I, u |= F iff I, u |= F1 I, u |= F2 iff J, u |= F1J, u |= F2 iff J, u |= F .84fiExtended RDF Semantic Foundation Rule Markup LanguagesLet F = x G. holds: I, u |= F iff I, u |= x G iff v : Var (G) ResIs.t. v(y) = u(y), Var (G) {x} I, v |= G iff v : Var (G) ResJ s.t.v(y) = u(y), Var (G) {x} J, v |= G iff J, u |= x G iff J, u |= F .Let F = F1 F2 F = F1 F2 F = xG. prove, similarlycases, I, u |= F iff J, u |= F .End lemmaSince J |= F , follows every mapping u : Var (F ) ResJ ,J, u |= F .Therefore, follows Lemma fact ResJ = ResI every mappingu : Var (F ) ResI , I, u |= F . Thus, |= F .Proposition 4.2. Let = hG, P ERDF ontology let I, J H (O). Letp TProp TProp J . PT (p) 6= PT J (p) PF (p) 6= PF J (p) 6 J J 6 I.Proof: Assume PT (p) 6= PT J (p). Now, assume J. Then, PT (p) PT J (p)PF (p) PF J (p). Since I, J H (O) p TProp TProp J , holdsHPF (p) = ResHPT (p) PF J (p) = ResO PT J (p). Thus, PF (p) PF J (p),contradiction. Thus, 6 J. Similarly, prove J 6 I.Assume PF (p) 6= PF J (p). Then, prove 6 J J 6 I,similar manner.Proposition 5.1. Let = hG, P ERDF ontology let Mst (O). holdsMH (O).Proof: Let Mst (O). Obviously, H (O) |= sk(G). show|= r, r P . Let r P . Let v mapping v : Var (r) ResHs.t. M, v |= Cond(r).enough show M, v |= Concl(r).mapping u : X ResH (O), X Var , define mapping u : XVO follows:u(x) u(x) xml value well-typed XML literal VOu (x) =u(x) xml value well-typed XML literal VOLet x VO , define xu = x. Let x X, define xu = u (x). Let F L(VO ){true, f alse} FVar (F ) X, define F u formula results Freplacing free variable F u (x). easy see holds: Concl(r)vConcl(r)v [r]VO [P ]VO .Lemma: Let F ERDF formula VO let u mapping u : Var (F ) ResHO.holds: M, u |= F iff M, u |= F u .Proof: prove lemma induction. Without loss generality, assumeappears front positive ERDF triples. Otherwise apply transformationrules Definition 3.4, get equivalent formula satisfies assumption.Let F = p(s, o). holds: M, u |= F iff M, u |= p(s, o) iff h[M + u](s), [M + u](o)iPT (M (p)) iff h[M + u](su ), [M + u](ou )i PT (M (p)) iff M, u |= p(s, o)u .Let F = p(s, o). holds: M, u |= F iff M, u |= p(s, o) iff h[M + u](s), [M + u](o)iPF (M (p)) iff h[M + u](su ), [M + u](ou )i PF (M (p)) iff M, u |= (p(s, o))u .Assumption: Assume lemma holds subformulas F .show lemma holds also F .85fiAnalyti, Antoniou, Damasio, & WagnerLet F = G. holds: M, u |= F iff M, u |= G iff M, u 6|= G iff M, u 6|= Gu iffM, u |= Gu iff M, u |= F u .Let F = F1 F2 . holds: M, u |= F iff M, u |= F1 F2 iff M, u |= F1 M, u |= F2 iffM, u |= F1u M, u |= F2u iff M, u |= (F1 F2 )u iff M, u |= F u .Let F = xG. holds: M, u |= F iff exists mapping u1 : Var (G) ResHs.t.u1 (y) = u(y), Var (G) {x} s.t. M, u1 |= G iff exists mapping u1 : Var (G)u1 iff exists mappingResHs.t. u1 (y) = u(y), Var (G) {x} s.t. M, u1 |= Gu1 : Var (G) ResHu1 |= (xG)u1 iff (sinces.t. u1 (y) = u(y), Var(G) {x} s.t. M,u1 (y) = u (y), FVar (xG)) M, u |= (xG)u iff M, u |= F u .Let F = F1 F2 F = F1 F2 F = xG. prove, similarlycases, M, u |= F iff M, u |= F u .End LemmaFirst assume Cond(r) 6= true. Then, Cond(r) L(VO ) thus, Cond(r) ERDFformula VO . Since M, v |= Cond(r), follows Lemma M, v |= Cond(r)v .since FVar (Cond(r)v ) = , follows Lemma B.1 |= Cond(r)v . SinceMst (O), follows |= Concl(r)v . Thus, Concl(r) 6= f alse Concl(r)L(VO |{}). since FVar (Concl(r)v ) = , follows lemma B.1 M, v |=Concl(r)v . Since Concl(r) ERDF formula VO , follows LemmaM, v |= Concl(r).Assume Cond(r) = true. Then, |= Cond(r)v . Since Mst (O),follows |= Concl(r)v . Therefore, Concl(r) 6= f alse, proveM, v |= Concl(r).Therefore, |= r, r P .Proposition 5.2. Let = hG, P ERDF ontology,rdfs:subClassOf (rdf :Property, erdf :TotalProperty) G. Then, Mst (O) = MH (O).Proof: Proposition 5.1, follows Mst (O) MH (O). showMH (O) Mst (O). Let MH (O). follows |= sk(G). showminimal({I H (O) | |= sk(G)}).Let J H (O) s.t. J |= sk(G) J . show J = . Since J ,follows P ropJ P ropM p P ropJ , holds PT J (p) PT (p)PF J (p) PF (p). Let p P ropJ . Since J |= sk(G), follows P ropJ TProp J .Thus, p TProp J . Assume PT J (p) 6= PT (p). Then, hx, yi PT (p)s.t. hx, yi 6 PT J (p). Then, hx, yi PF J (p). Thus, hx, yi PF (p), impossible,since hx, yi PT (p). Thus, PT J (p) = PT (p). Similarly, prove PF J (p) =PF (p). Therefore, p P ropJ , holds PT J (p) = PT (p) PF J (p) = PF (p).show P ropJ = P ropM . holds P ropJ ={x ResH| hx, PropertyiPT J (type)} = {x ResH|hx,PropertyiPT(type)}=Prop.Basedresults,fact J, H (O), follows J = . Therefore, minimal({IH (O) | |= sk(G)}).show minimal({I H (O) | |= Concl(r),r P[M,M ] }). Since MH (O) follows {I H (O) ||= Concl(r), r P[M,M ] }. Let J {I H (O) | |= Concl(r),r P[M,M ] } J . Since J , follows P ropM P ropJ ,p P ropM , holds PT (p) PT J (p) PF (p) PF J (p). Since J ,86fiExtended RDF Semantic Foundation Rule Markup Languagesfollows P ropJ P ropM , p P ropJ , holds PT J (p) PT (p)PF J (p) PF (p). Therefore, follows P ropM = P ropJ , p P ropM ,holds PT (p) = PT J (p) PF (p) = PF J (p). Based result, factJ, H (O), follows J = .Thus, minimal({I H (O) | |= Concl(r), r P[M,M ] }).Since satisfies conditions Definition 5.1 (Stable Model), followsst(O). Thus, holds MH (O) Mst (O).Therefore, MH (O) = Mst (O).Proposition 6.2. Let G ERDF graph let F ERDF formulaVF skG (Var (G)) = . holds:1. F ERDF d-formula hG, |=st F G |=ERDF F .2. G |=ERDF F hG, |=st F .Proof:1) Let hG, |=st F . show sk(G) |=ERDF F . Let ERDF interpretationvocabulary V s.t. |= sk(G). show |= F . define V = V VRDFVRDF VERDF .Let = hG, i. Based I, construct partial interpretation J VO follows:ResJ = ResHO.JV (x) = x, x VO URI.define mapping: ILJ : VO L ResJ that:ILJ (x) = x, x typed literal VO well-typed XML literal,ILI (x) XML value x, x well-typed XML literal VO .define mapping: J : VO ResJ that:J(x) = JV (x), x VO URI.J(x) = x, x VO PL.J(x) = ILJ (x), x VO L.P ropJ = {x ResJ | x VO , J(x ) = x I(x ) P ropI }.mapping PT J : P ropJ P(ResJ ResJ ) defined follows:x, y, z VO , holds:hJ(x), J(y)i PT J (J(z)) iff hI(x), I(y)i PT (I(z)).define mapping PF J : P ropJ P(ResJ ResJ ) follows:x, y, z VO , holds:hJ(x), J(y)i PF J (J(z)) iff hI(x), I(y)i PF (I(z)).LV J = {x ResJ | hx, J(Literal)i PT J (J(type))}.87fiAnalyti, Antoniou, Damasio, & Wagnershow J partial interpretation, enough show VO PL LV J .Let x VO PL. Then, x LV . Thus, hx, I(Literal)i PT (I(type)). implieshx, J(Literal)i PT J (J(type)). Thus, x LV J .Now, extend J ontological categories:ClsJ = {x ResJ | hx, J(Class)i PT J (J(type))},TCls J = {x ResJ | hx, J(TotalClass)i PT J (J(type))},TProp J = {x ResJ | hx, J(TotalProperty)i PT J (J(type))}.define mappings CT J , CF J : ClsJ P(ResJ ) follows:x CT J (y) iff hx, yi PT J (J(type)),x CF J (y) iff hx, yi PF J (J(type)).show J ERDF interpretation VO . First, showJ satisfies semantic condition 2 Definition 3.7 (ERDF Interpretation), numbersteps:Step 1: Here, prove ResJ = CT J (J(Resource)). Obviously, CT J (J(Resource))ResJ . show ResJ CT J (J(Resource)). Let x ResJ . Then,x VO J(x ) = x. want show hJ(x ), J(Resource)i PT J (J(type)).holds: hJ(x ), J(Resource)i PT J (J(type)) iff hI(x ), I(Resource)i PT (I(type)),true, since ERDF interpretation satisfies sk(G) I(x ) ResI .Thus, x = J(x ) CT J (J(ResourceResource)).Therefore, ResJ = CT J (J(Resource)).Step 2: Here, prove P ropJ = CT J (J(Property)). show P ropJCT J (J(Property)). Let x P ropJ . Then, x VO J(x ) = xI(x ) P ropI . want show hJ(x ), J(Property)i PT J (J(type)). holds:hJ(x ), J(Property)i PT J (J(type)) iff hI(x ), I(Property)i PT (I(type)),true, since I(x ) P ropI . Thus, x = J(x ) CT J (J(Property)).Therefore, P ropJ CT J (J(Property)).show CT J (J(Property)) P ropJ . Let x CT J (J(Property)). Then,x VO J(x ) = x. holds hJ(x ), J(Property)i PT J (J(type)), implieshI(x ), I(Property)i PT (I(type)). Thus, I(x ) P ropI x P ropJ .Therefore, CT J (J(Property)) P ropJ .Step 3: definition, holds ClsJ = CT J (J(Class)), LV J = CT J (J(Literal)), TCls J =CT J (J(TotalClass)) TProp J = CT J (J(TotalProperty)).show J satisfies semantic condition 3 Definition 3.7 (ERDF Interpretation). Let hx, yi PT J (J(domain)) hz, wi PT J (x). showz CT J (y). x , VO J(x ) = x, J(y ) = y. Thus,hJ(x ), J(y )i PT J (J(domain)). Additionally, z , w VO J(z ) =z, J(w ) = w. Thus, hJ(z ), J(w )i PT J (J(x )). Then, hI(x ), I(y )i PT (I(domain))hI(z ), I(w )i PT (I(x )). Since ERDF interpretation, hI(z ), I(y )iPT (I(type)). Thus, hJ(z ), J(y )i PT J (J(type)) z CT J (y).similar manner, prove J also satisfies rest semantic conditionsDefinition 3.7. Thus, J ERDF interpretation VO .Moreover, show J coherent ERDF interpretation (Definition 3.2).Assume case. Thus, z P ropJ s.t. PT J (z) PF J (z) 6= .Thus, x, ResJ s.t. hx, yi PT J (z) PF J (z), z. Then,88fiExtended RDF Semantic Foundation Rule Markup Languagesx , , z VO s.t. J(x ) = x, J(y ) = y, J(z ) = z. holds: hJ(x ), J(y )i PT J (J(z ))hJ(x ), J(y )i PF J (J(z )). Thus, hI(x ), I(y )i PT (I(z )) hI(x ), I(y )iPF (I(z )). impossible, since (coherent) ERDF interpretation. Therefore,J also coherent ERDF interpretation.Thus, J H (O).show J |= sk(G). Let p(s, o) sk(G). holds p, s, VO . Since|= sk(G), holds I(p) P ropI . Thus, hI(p), I(Property)i PT (I(type)), implieshJ(p), J(Property)i PT J (J(type)). this, follows J(p) P ropJ .holds: hJ(s), J(o)i PT J (J(p)) iff hI(s), I(o)i PT (I(p)). last statement truesince |= sk(G). Let u : {} ResH. Then, J, u |= p(s, o). Let p(s, o) sk(G).show J, u |= p(s, o), similar manner. Thus, J |= sk(G).Now, Definition 5.1 (Stable Model) fact J |= sk(G), followsK Mst (O) s.t. K J. fact |=st F , follows K |= F .Since F ERDF d-formula, holdsF = (?x1 , ..., ?xk1 F1 ) ... (?x1 , ..., ?xkn Fn ),Fi = t1 ... tmi tj , j = 1, ..., mi , ERDF triple. Thus,{1, ..., n} u : Var (Fi ) ResHs.t. K, u |= Fi .show J, u |= Fi .Let p(s, o) {t1 , ..., tmi }. Since K ERDF interpretation VO , K, u |= Fi ,P ropK P ropJ , follows p VO , s, VO Var , J(p) = K(p) P ropKP ropJ . Additionally, h[K +u](s), [K +u](o)i PT K (p). Since h[J +u](s), [J +u](o)i = h[K +u](s), [K + u](o)i PT K (p) PT J (p), follows h[J + u](s), [J + u](o)i PT J (p).Thus, J, u |= p(s, o).Let p(s, o) {t1 , ..., tmi }. Since K ERDF interpretation VO , K, u |= Fi ,P ropK P ropJ , follows p VO , s, VO Var , J(p) = K(p) P ropKP ropJ . Additionally, h[K +u](s), [K +u](o)i PF K (p). Since h[J +u](s), [J +u](o)i = h[K +u](s), [K + u](o)i PF K (p) PF J (p), follows h[J + u](s), [J + u](o)i PF J (p).Thus, J, u |= p(s, o).define total function u : VFi Var (Fi ) VO , follows:u(x) x Var (Fi )u(x) xml value well-typed XML literal VOx Var (Fi )u (x) =u(x) xml value well-typed XML literal VOxotherwiseMoreover, define total function u : Var (Fi ) ResI s.t. u (x) = I(u (x)).show I, u |= Fi .Let p(s, o) {t1 , ..., tmi }. Then, p VFi s, VFi Var . Since J, u |= Fi , followsVFi VO . Therefore, VFi Vsk(G) VRDF VRDF VERDF V . Thus, p Vs, V Var .show I(p) P ropI . holds:hI(p), I(Property)i PT (I(type)) iffhJ(p), J(Property)i PT J (J(type)), holds since J, u |= Fi .89fiAnalyti, Antoniou, Damasio, & Wagnerwant show h[I +v ](s), [I +v ](o)i PT (I(p)). Note x VFi , holds:[I + u ](x) = I(u (x)) = I(x) J(u (x)) = [J + u](x) = J(x). Moreover, x Var (Fi ),holds: [I + u ](x) = I(u (x)) J(u (x)) = [J + u](x) (recall definition J(.)).Therefore, holds:h[I + u ](s), [I + u ](o)i PT (I(p)) iffhI(u (s)), I(u (o))i PT (I(p)) iffhJ(u (s)), J(u (o))i PT J (J(p)) iffh[J + u](s), [J + u](o)i PT J (J(p)), true since J, u |= Fi . Thus, I, u |= p(s, o).Let p(s, o) {t1 , ..., tmi }. show I, u |= p(s, o), similar manner.Thus, I, u |= Fi , implies I, u |= ?x1 , ..., ?xki Fi . Thus, I, u |= F . Now,follows Lemma B.1 |= F .Thus, sk(G) |=ERDF F . Now, follows Proposition 4.1 G |=ERDF F .2) Let G |=ERDF F . follows Proposition 4.1 sk(G) |=ERDF F . showhG, |=st F . particular, let = hG, let Mst (O). NoteERDF interpretation VO , |= sk(G). Since sk(G) |=ERDF F , follows|= F .Proposition 8.1 Let instance unbounded tiling problem. holds:1. solution iff OD {false FD } stable model.2. solution iff OD 6|=st FD .Proof:1) statement follows easily statement 2).2) ) Let solution D. Since denumerable, exists bijectivefunction : . Consider Herbrand interpretation OD that:1. CTI (Tile) = CTI (HasRight) = CTI (HasAbove) = {rdf : | }CFI (T ile) = CFI (HasRight) = CFI (HasAbove) = .2. P TI (id ) = {hx, xi | x VO } P FI (id ) = .3. P TI (HConstraint) = H P FI (HConstraint) = .4. P TI (VConstraint) = V P FI (VConstraint) = .5. P TI (Type) = {hrdf : (i, j), (i, j)i | i, j } P FI (Type) = .6. P TI (right) = {hrdf : (i, j), rdf : (i + 1, j)i | i, j }P FI (right) = {hrdf : i, rdf : ji | i, j hrdf : i, rdf : ji 6 P TI (right)}.7. P TI (above) = {hrdf : (i, j), rdf : (i, j + 1)i | i, j }P FI (above) = {hrdf : i, rdf : ji | i, j hrdf : i, rdf : ji 6 P TI (above)}.easy see stable model OD 6|= FD . Thus, OD 6|=st FD .) Let = hT , H, V i, = {T1 , ..., Tn }. Assume OD 6|=st FD letstable model OD = hG, P 6|= FD . Obviously, CTI (Tile) = {rdf : | }.Due rule sets (2)-(4) P since OD 6|=st FD , holds starting tile rdf : 090fiExtended RDF Semantic Foundation Rule Markup Languagesplacing tiles according P TI (right) P TI (above) relations, grid formed.define (i, j) = k, i, j, k , iff tile rdf : k placed hi, ji positionprevious grid. Note total function. Due rule set (1) P , tileassigned unique type = {T1 , ..., Tn }. Due rule set (5) P , type assignmentsatisfies horizontal vertical adjacency constraints D. Thus, solution: , (i, j) = iff hrdf : (i, j), P TI (T ype). Since totalfunction and, k , tile rdf : k assigned unique type , followstotal function.ReferencesAlferes, J. J., Damasio, C. V., & Pereira, L. M. (1995). Logic Programming SystemNon-monotonic Reasoning. Special Issue Journal Automated Reasoning,14 (1), 93147.Alferes, J. J., Damasio, C. V., & Pereira, L. M. (2003). Semantic Web Logic Programming Tools. International Workshop Principles Practice Semantic WebReasoning (PPSWR03), pp. 1632.Analyti, A., Antoniou, G., Damasio, C. V., & Wagner, G. (2004). Negation NegativeInformation W3C Resource Description Framework. Annals Mathematics,Computing & Teleinformatics (AMCT), 1 (2), 2534.Analyti, A., Antoniou, G., Damasio, C. V., & Wagner, G. (2005). Stable Model TheoryExtended RDF Ontologies. 4th International Semantic Web Conference (ISWC2005), pp. 2136.Antoniou, G., Bikakis, A., & Wagner, G. (2004). System Nonmonotonic RulesWeb. 3rd International Workshop Rules Rule Markup LanguagesSemantic Web (RULEML03), pp. 2336.Antoniou, G., Billington, D., Governatori, G., & Maher, M. J. (2001). RepresentationResults Defeasible Logic. ACM Transactions Computational Logic (TOCL),2 (2), 255287.Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).(2003). Description Logic Handbook: Theory, Implementation, Applications.Cambridge University Press.Bassiliades, N., Antoniou, G., & Vlahavas, I. P. (2004). DR-DEVICE: Defeasible LogicSystem Semantic Web. 2nd International Workshop PrinciplesPractice Semantic Web Reasoning (PPSWR04), pp. 134148.Beckett, D. (2004). RDF/XML Syntax Specification (Revised). W3C Recommendation.Available http://www.w3.org/TR/2004/REC-rdf-syntax-grammar-20040210/.Berger, R. (1966). Undecidability Dominoe Problem. Memoirs AmericanMathematical Society, 66, 172.Berners-Lee, T. (1998). Design Issues - Architectual Philosophical Points. Personalnotes. Available http://www.w3.org/DesignIssues.91fiAnalyti, Antoniou, Damasio, & WagnerBerners-Lee, T., Connolly, D., Kagal, L., Scharf, Y., & Hendler, J. (2008). N3Logic:Logical Framework World Wide Web. published Theory PracticeLogic Programming (TPLP), Special Issue Logic Programming Web.Bry, F., & Marchiori, M. (2005). Ten Theses Logic Languages Semantic Web.3rd International Workshop Principles Practice Semantic Web Reasoning(PPSWR-2005), pp. 4249.Damasio, C. V., Analyti, A., Antoniou, G., & Wagner, G. (2006). Supporting OpenClosed World Reasoning Web. 4th Workshop Principles PracticeSemantic Web Reasoning (PPSWR-2006), pp. 149163.de Bruijn, J., Franconi, E., & Tessaris, S. (2005). Logical Reconstruction Normative RDF.OWL: Experiences Directions Workshop (OWLED-2005), Galway, Ireland.Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1998). AL-log: Integrating DatalogDescription Logics. Journal Intelligent Information Systems, 10 (3), 227252.Donini, F. M., Nardi, D., & Rosati, R. (2002). Description Logics Minimal KnowledgeNegation Failure. ACM Transactions Computational Logic, 3 (2), 177225.Eiter, T., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2004a). Combining Answer SetProgramming Description Logics Semantic Web. 9th InternationalConference Principles Knowledge Representation Reasoning (KR04), pp.141151.Eiter, T., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2004b). Well-Founded SemanticsDescription Logic Programs Semantic Web. 3rd International WorkshopRules Rule Markup Languages Semantic Web (RuleML04), pp. 8197.Eiter, T., Ianni, G., Polleres, A., & Schindlauer, R. (2006). Answer Set ProgrammingSemantic Web. Tutorial co-located 3d European Semantic Web Conference(ESWC-2006).Gelder, A. V., Ross, K. A., & Schlipf, J. S. (1991). Well-Founded Semantics GeneralLogic Programs. Journal ACM, 38 (3), 620650.Gelfond, M., & Lifschitz, V. (1988). Stable Model Semantics Logic Programming.Kowalski, R., & Bowen, K. A. (Eds.), 5th International Conference Logic Programming, pp. 10701080. MIT Press.Gelfond, M., & Lifschitz, V. (1990). Logic programs Classical Negation. Warren,& Szeredi (Eds.), 7th International Conference Logic Programming, pp. 579597.MIT Press.Gelfond, M., & Lifschitz, V. (1991). Classical Negation Logic programs DisjunctiveDatabases. New Generation Computing, 9, 365385.Hayes, P. (2004). RDF Semantics. W3C Recommendation. Available http://www.w3.org/TR/2004/REC-rdf-mt-20040210/.Herre, H., Jaspars, J., & Wagner, G. (1999). Partial Logics Two Kinds NegationFoundation Knowledge-Based Reasoning. Gabbay, D. M., & Wansing, H.(Eds.), Negation? Kluwer Academic Publishers.92fiExtended RDF Semantic Foundation Rule Markup LanguagesHerre, H., & Wagner, G. (1997). Stable Models Generated Stable Chain. JournalLogic Programming, 30 (2), 165177.Horrocks, I., & Patel-Schneider, P. F. (2003). Reducing OWL Entailment DescriptionLogic Satisfiability. 2nd International Semantic Web Conference (ISWC-2003),pp. 1729.Horrocks, I., & Patel-Schneider, P. F. (2004). Proposal OWL Rules Language.13th International Conference World Wide Web (WWW04), pp. 723731. ACMPress.Horrocks, I., Patel-Schneider, P. F., Boley, H., Tabet, S., Grosof, B., & Dean, M.(2004). SWRL: semantic web rule language combining OWL RuleML.W3C Member Submission. Available http://www.w3.org/Submission/2004/SUBM-SWRL-20040521/.Kifer, M., Lausen, G., & Wu, J. (1995). Logical Foundations Object-Oriented FrameBased Languages. Journal ACM, 42 (4), 741843.Klyne, G., & Carroll, J. J. (2004). Resource Description Framework (RDF): ConceptsAbstract Syntax. W3C Recommendation. Available http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/.Levy, A. Y., & Rousset, M. (1998). Combining Horn Rules Description LogicsCARIN. Artificial Intelligence, 104 (1-2), 165209.Lloyd, J. W., & Topor, R. W. (1984). Making Prolog Expressive. Journal LogicProgramming, 1 (3), 225240.Maher, M. J. (2002). Model-Theoretic Semantics Defeasible Logic. ICLP 2002Workshop Paraconsistent Computational Logic (PCL-2002), pp. 255287.McGuinness, D. L., & van Harmelen, F. (2004).OWL Web Ontology LanguageOverview. W3C Recommendation. Available http://www.w3.org/TR/2004/REC-owl-features-20040210/.Motik, B., Sattler, U., & Studer, R. (2004). Query Answering OWL-DL Rules.3rd International Semantic Web Conference (ISWC-2004), pp. 549563.Patel-Schneider, P. F., Hayes, P., & Horrocks, I. (2004). OWL Web Ontology LanguageSemantics Abstract Syntax. W3C Recommendation. Available http://www.w3.org/TR/2004/REC-owl-semantics-20040210/.Pereira, L. M., & Alferes, J. J. (1992). Well-Founded Semantics Logic ProgramsExplicit Negation. Neumann, B. (Ed.), European Conference Artificial Intelligence, pp. 102106. John Wiley & Sons.Prudhommeaux, E., & Seaborne, A. (2008). SPARQL Query Language RDF. W3CRecommendation. Available http://www.w3.org/TR/rdf-sparql-query/.Rao, P., Sagonas, K. F., Swift, T., Warren, D. S., & Freire, J. (1997). XSB: SystemEfficiently Computing WFS. Proceedings 4th International Conference LogicProgramming Nonmonotonic Reasoning (LPNMR97), pp. 10701080.93fiAnalyti, Antoniou, Damasio, & WagnerRosati, R. (1999). Towards Expressive KR Systems Integrating Datalog DescriptionLogics: Preliminary Report. Proc. 1999 Description Logic Workshop (DL99),pp. 160164.Rosati, R. (2005). Decidability Complexity Integrating Ontologies Rules.Journal Web Semantics, 3, 6173.Schaffert, S., Bry, F., Besnard, P., Decker, H., Decker, S., Enguix, C. F., & Herzig, A.(2005). Paraconsistent Reasoning Semantic Web. Workshop UncertaintyReasoning Semantic Web, co-located ISWC-2005, pp. 104105.Sintek, M., & Decker, S. (2002). TRIPLE - Query, Inference, Transformation Language Semantic Web. 1st International Semantic Web Conference (ISWC2002), pp. 364378. Springer-Verlag.ter Horst, H. J. (2004). Extending RDFS Entailment Lemma. 3rd InternationalSemantic Web Conference (ISWC-2004), pp. 7791.ter Horst, H. J. (2005a). Combining RDF Part OWL Rules: Semantics, Decidability, Complexity. 4th International Semantic Web Conference (ISWC-2005),pp. 668684.ter Horst, H. J. (2005b). Completeness, Decidability Complexity EntailmentRDF Schema Semantic Extension Involving OWL Vocabulary. JournalWeb Semantics, 3 (2-3), 79115.Wagner, G. (1991). Database Needs Two Kinds Negation. 3rd SymposiumMathematical Fundamentals Database Knowledge Base Systems (MFDBS91),pp. 357371. Springer-Verlag.Wagner, G. (2003). Web Rules Need Two Kinds Negation. 1st International Workshop Principles Practice Semantic Web Reasoning (PPSWR03), pp. 3350.Springer-Verlag.Wagner, G., Giurca, A., & Lukichev, S. (2005). General Markup Framework IntegrityDerivation Rules. Dagstuhl Seminar Proceedings: Principles PracticesSemantic Web Reasoning.Wagner, G., Giurca, A., & Lukichev, S. (2006). Usable Interchange Format RichSyntax Rules Integrating OCL, RuleML SWRL. Workshop ReasoningWeb (RoW-2006), co-located WWW-2006).Yang, G., & Kifer, M. (2003a). Inheritance Rules Object-Oriented Semantic WebLanguages. 2nd International Workshop Rules Rule Markup LanguagesSemantic Web (RULEML03), pp. 95110.Yang, G., & Kifer, M. (2003b). Reasoning Anonymous Resources Meta Statements Semantic Web. Journal Data Semantics, 1, 6997.Yang, G., Kifer, M., & Zhao, C. (2003). Flora-2: Rule-Based Knowledge RepresentationInference Infrastructure Semantic Web. 2nd International ConferenceOntologies, DataBases, Applications Semantics Large Scale InformationSystems (ODBASE03), pp. 671688.94fi