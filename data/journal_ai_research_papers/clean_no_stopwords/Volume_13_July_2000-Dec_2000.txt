ff fi"!$#%'&)(***,+-#.0/1((23,456782,9//:;4!7<#*9*0*=?>A@B@DCFEHGJILK$MONBPRQSTK$UVIWGYX[Z\XGYPHPHQP^]`_aYUcbedHP^f6ghUiaePHQj<K$Mlkm]YKJPonVIQPlpqarPsC-@mK<nVK$UutvQPHQInuQf?@maetwGYQPhIxzy8{W|~}VW|{V|{F-- o-<8}AW{VyW|}us|-L'<Wo-<8VF $o, 'V)'0)[L),)),, - ,"[' W60s u 0 [ euh 6^)00 eH ) FF0 ) " 0s e ) F )0s < u[<~<)s )h8< " ' h e 0s $"V sssz - 00[ ~~ ^ "W ) < e huoo )^H ^6 e^ " e 0i ^"R 'u-$~ -- "F^ ) <su$i " eVF0hH 0 R ) H h )F" 0s -e 6 ^ ,6 J 0o eL) '0' ' s8V< $s\ fihFffsT "F[[0- F6 -) e0mV ) s,"' $ 0 H 0 H ^)" VuH Hs e )FV)V ^"'u e 08 ) u ) iYsu ^6 0sHH J 0^eo LT-s$ ^ < 6 V6o $ sV0V6H s0 s0''6eFYH ^fi ffs006'" !$#%#'&)(*!+-,."!//.&"/.01&2/4356378#9;:.7<!=.9%5?>@!$9%&6(A047*/.7*B%!$9%&65?/C5$DE!$(89%&65?/.#9%5F9;B%!37*Bfi#%7G!H#%9;!$9'7A#;,!+(87<9%5!$(8:.&67837F#;,78(8&JI(C05?!$#'9;!$9%78#*KMLE!BN&)5.=.#<!$605?Bfi&69;:>F#<:!37FO787*/P783478)5.,78PQ9%51!$PPBfi78#'#G9;:.7F#%9;!$9%7SR!$(89%&65?/TBfi7*,Bfi78#%7*/49;!$9'&)5./U!/.PT9;:.7<#%7*!4Bfi(V:WDX5.B !$(89%&65?/.#*K YZB%!$P&69%&65?/!+)6[T9;:.78#%7!$605?Bfi&69;:>\#A:!37]O^787*/(8"!$#%#%&JI78P!$(8(85?BfiP&"/.0F9%5<9S:.78&2B_#%7*!Bfi(8:#S,!$(87`BN7*,Bfi78#%7*/49;!$9%&65?/C!$#E78&69;:.7*B_#%9;!$9%7SR#S,!$(87`,."!//.7*BN#]ab7Kc0KedfghZijbkl L 7865#%5m7891!$nK"dGo*pp4qrF5?BC,."!/4R#;,!$(87Q,."!//.7*Bfi#mab7Kc0K"d st fhZf dAuZ7*/.O7*Bfi9;:4[wvyxz78)Pdo*pp4{r8K|}/.78~Bfi78#%7*!BN(V:@9;Bfi7*/.P1:!$#AO^787*/C9%5FP7837865?,1/.78~7*/.(85.P&2/.04#A5$D ,.2!4//.&2/.01,Bfi5?O.67*>F#&2/C5.BfiP7*B9%5!$P5?,.97S(8&67*/9A!+)045?Bfi&69;:>F# DBfi5?>59S:.7*B Bfi78#%7*!BN(V:C!Bfi7*!$#d?67*!$P&"/.0C9%5#%&60?/.&JI(*!/49 P7837865?,>\7*/9%#&"/,.2!4//.&2/.0w!+)045?Bfi&69;:>F#*d !+##;=BN378[78PO4[mxQ786Pafio*pp4pr8KwY :.&6#1(8"!$#%#C5$D<,."!//.&"/.0!$605?BN&)9S:>F#&"/.(8"=.P78# k-g-ff$- an"=>v=BN#%9*dEoppr9;:!$9F=.#%78#]!W5~_R0?B%!,:7*/.(85.P&"/.09%5U(85?/.#'9;B%!$&"/9;:.7H#%7*!Bfi(8:!/.P f+- aG!=.9%]v.78">!/d oppr 9S:!$9A7*/.(85.P78#G9;:.7F,.2!4//.&2/.0,Bfi5?O.67*>y!$#G!#;!$9'&)#NI!O.&6)&69fi[,BN5?O.67*>!/.P=.#'78# D!+#%9A>F5.P78#;!$9'&)#ND!$(89'&)5./1!$605?Bfi&69;:>F#9%5FI/.P!H#%542=.9'&)5./Ke (**0*8fi-fi7 V 7$0 W ,ff56 4!fi!,Fb7$fi0 J78(87*/9%6[d!/.59S:.7*B /.78~w,.2!4//.7*BM f &">!$9%9'&?789E!$nK"doppr~ !$# &"/9;BN5?P=.(878P9S:!$9 7*/.(85?P78# !,."!//.&"/.0P5?>!+&2/@!$#!H/.5?/4RP789%7*B%>F&"/.&6#%9%&6(MI/.&69%7G!=.9%5?>@!$9%5?/1an|`r BN7*,Bfi78#%7*/49%78PWO4[!/GBfiP7*BN78P&"/!Bfi[QG78(8&6#%&65?/G&"!$0?B'!>a h ii dE Bfi[?!/49*d o*p4r8K1/z(85?/49;B%!$#%9H9%5U9S:.7W,Bfi783.&65?=.#!$605?Bfi&69;:>\#*df 7S78(89%&63786[U78.9%7*/.P#H9%5W/.5./4RP789%7*B%>F&"/.&6#%9%&6(CP5?>!$&"/.#],BN5?P=.(8&"/.0=/.&637*BN#;!$E,."!/.#]!+#`Bfi5.O=.#%9#%5"=.9%&65?/.#K <=.7 9%5A9S:.7 #%(*!$"!O.&66&69[F5$D9;:.7 =/.P7*Bfi6[?&"/.0>\5?P78(8:.78(V4&"/.0]Bfi7*,BN78#%7*/9S!$9%&65?/F!/.PF#%7*!Bfi(8:9%78(8:/.&6=.78#*d^&)9(*!/O^7G#;:.5$~A/@9%5O7<!H37*Bfi[7SC(8&)7*/49A/.5?/4RP789'7*B%>F&"/.&6#%9%&6(],."!//.7*B<a &">!$9%9%&789 !$nK"do*pp4!doppOr8KG/.75$D5?=B>!$&"/Bfi78#'7*!Bfi(8:F5?O.fi78(89%&6378# &6# 9%5GP7837865?,,."!//.&"/.0#%[.#%9%7*>\# #;=.&69;!O.67 Dn5?B ,."!//.&"/.0&"/=/.(87*Bfi9;!+&2/d.#%&"/.067d.5?B >F=.69%&JR!$07*/49E7*/43?&"Bfi5./>F7*/9'#AanA!+&)0.:vL 7865#%5do*pp^$L 78)54#%5<789E!$nK"dopp.9%5?/.7vL 7865#'5dEo*p4pr8K<Y:.7\=/.&637*BN#;!$,."!//.&"/.0Q!,,Bfi5.!$(V:d!+#G5?Bfi&60&"/!$66[QP7837865?,78Pan.(8:.5?,4R,7*BN#*d o*p4r8d&6#<!,,^7*!$6&"/.0UDn5?B9;:.&6#<9fi[,7F5$DE7*/43?&"Bfi5?/>\7*/9%#K<|=/.&)347*Bfi#;!$E,."!/T&6#]!@#%789G5$D #%9;!$9%7SR!$(89%&65?/B%=.678#H9;:!$9G!$&">y!$9G(85$37*Bfi&"/.019;:.7,5#'#%&"O.)71>F=.69%&",.)7@#%&69;=!$9%&65?/.#H&2/9;:.7/.5?/4RP789%7*B%>F&"/.&6#%9%&6(7*/43?&"Bfi5?/>\7*/9*K|=/.&637*Bfi#;!+-,."!/1&6#M78?78(*=.9%78PQO4[&"/49%7*Bfi67*!3?&"/.019;:.7H#%78678(89%&65?/15+D!/T!$(89%&65?/&2/9;:.7,."!/w!4/.Pw5?O.#%7*BN3?&"/.09;:.7Bfi78#S=.)9'&2/.07S78(89%#&"/9;:.71~ 5?Bfi6PKA/.&637*Bfi#S!$A,."!//.&"/.0Bfi78#'7*>]O.678#C9;:.75?=.9%(85.>F7<5$DBN78&2/4Dn5?Bfi(87*>\7*/9G67*!B%/.&"/.0an=.9'9%5?/Uv !Bfi9'5do*pp4r8d&"/T9;:!$9M9;:.7<#'9;!$9%7SR!$(89'&)5./U>F5.P78(*!,.9;=BN78#-9;:.7 =/.(87*Bfi9;!$&"/49fi[A5$D9S:.7E~ 5?Bfi6PK-M/.&637*Bfi#;!+,."!//.&"/.0G&6#!M,Bfi78(*=Bfi#'5?B!,,Bfi5.!$(V:d # ~A:.7*Bfi7 !$6,."!//.&"/.0&6# P5?/.7G,Bfi&65?B9%5<78.78(*=.9%&65?/dO=.&66P&"/.0W=,^5?/@9;:.7`!+#%#;=>,.9%&65?/@9;:!$9 !F/.5?/4RP789%7*B%>F&"/.&6#%9%&6(>F5.P785$D-9S:.7G78?78(*=.9%&65?/7*/3.&"Bfi5?/>F7*/49 (*!/1O^7]!$(8=.&"Bfi78Pd!/.P167*!$P&"/.0C9;:.7*Bfi7SDn5?Bfi7G9%5!F#'5?=/.PU!/.P(85?>,.6789%7F,."!//.&"/.0U!,,Bfi5?!$(8:K5$~ 7837*B8d=/.&637*Bfi#;!$ ,."!//.&"/.0:!$#MO787*/T(*Bfi&69%&6(8&6878Pab7Kc0KedG&"/.#;O^7*Bfi0dZo*pprd?P=.7F9%5C!,59'7*/4R9%&"!$78,5./.7*/9%&"!$Z0?Bfi5$~ 9S:5$DZ9;:.7]=/.&637*Bfi#S!$-,."!/1#'&)87H~ &69;:19S:.7</?=>FO7*B5$D,BN5?,5#'&)9'&)5./.#AP7SI/.&"/.0!CP5?>!+&2/#%9;!$9%7K|`/T&">,5.Bfi9;!/49G(85?/49;Bfi&"O=.9%&65?/5$D f &6#G9;:.=.#<9;:.7=.#%7F5$D h ii #<9'5UBfi7*,Bfi78#%7*/49=/.&637*Bfi#;!+,."!/.#*KE/F9;:.7 ~ 5?BN#%9 (*!$#%7d49;:.&6#Bfi7*,Bfi78#'7*/9;!+9%&65?/>![]!+)#'5<0?Bfi5$~m78,5?/.7*/49%&"!$66[~&)9S:F9;:.7/.=>]O^7*BF5$D P5?>!+&2/m,BN5?,5#'&)9'&)5./.#*dO=.9O^78(*!=.#%7 h ii #C!Bfi7C37*Bfi[Q(85?>@,!$(89Bfi7*,Bfi78#%7*/49;!$9%&65?/.#F5$DO5.567*!/D=/.(89%&65?/.#*d9;:.&6#A&6#M5$DX9'7*/1/.59A9S:.7G(*!$#%7GDn5?B P5.>!$&"/.#A~&)9S:U!Bfi780.=.2!4B #%9SB%=.(89;=Bfi7an_&2>@!$9%9%&789<!$nK"dZo*pp4!r8KAY:.7*Bfi7SDX5.Bfi7d h ii RO!$#'78P,."!//.&"/.0U#%787*>F#G9'5WO^7\!1,BN5?>F&6#%&"/.0U!,,Bfi5?!+(V:T9%51=/.&JR37*Bfi#S!$,.2!4//.&2/.0Kf #;,^78(8&I78#C!U,."!//.&"/.0QP5?>!+&2/&2/z9;:.7!+(89%&65?/P78#%(*Bfi&",.9%&65?/"!/.0.=!$07HaG&"=/.(V:.&606&"!789!$nK"dMo*pp4rF!/.Pm9;B%!/.#'2!+9%78#&699%5!(85?B%Bfi78#;,^5?/.P&"/.0|Fd:.7*/.(87T6&2>\&)9'78P9'5,."!//.&"/.0w,Bfi5?O4R67*>F#F~ &69;:I/.&)9'71#%9;!$9%7@#;,!$(878#*KUY:.79;B'!/.#%&69%&65?/Bfi78"!$9%&65?/z5$DA9;:.7!4=.9%5?>!$9'5?/&6#F7*/.(85?P78Pw!$#F!/h ii 9;:!$9E!+)65$~ #DX5?BZ9S:.7A=.#%7 5$D7S(8&67*/9 OBfi7*!+P9;:4RnIBfi#%9#%7*!Bfi(8:H9%78(V:/.&6=.78# P7837865?,78PCDn5?B >F5.P78(8:.78(V4&"/.0ab(8&66"!/doppr8K f &2/.(8"=.P78# 9fi~ 5A!$605?Bfi&69;:>\#Dn5?BZ=/.&)347*Bfi#;!$,."!//.&"/.0KEY :.7M8n46bG!$605?Bfi&69;:>9SBfi&678# 9%507*/.7*B'!$9%7G!,."!/19;:!+9&6# 0.=!B%!/49%7878P19%5!$(8:.&67837<9;:.7G045?!$Dn5?B !+)5$D9;:.7,^5#%#%&"O.6715?=.9%(85.>F78#<5$D 9;:.7C/.5?/4RP789%7*B%>F&"/.&6#%9%&6(!$(89%&65?/.#*K1bDA/.5T#;=.(V:z#%9;BN5?/.01#%5"=.9%&65?/z78?&6#%9'#*d9;:.7!$605?BN&)9S:>D!$&66#*K<Y:.78nfiT*$ b 6bF!$605?Bfi&69;:>yBfi789S=B%/.#`!C#'9;Bfi5?/.0C#%5"=.9%&65?/d&D 5?/.778.&)#'9%#*dZ5?BG549;:.7*Bfi~ &6#%79SBfi&678#]9'5U07*/.7*B%!+9%7!U,."!/z9;:!$9F>![U(85?/49;!$&"/z65?5?,.#FO=.9F&)#F0?=!4B%!/49%7878PQ9%5!$(8:.&678379S:.705.!$nd-04&)347*/Q9;:!+9]!$6E(8[.(86&6(178?78(*=.9%&65?/.#@7837*/9S=!$6)[9%7*B%>\&2/!+9%7KUbDA/.5#;=.(8:Q#'9;Bfi5?/.0(8[.(8)&6(F#%5"=.9%&65?/78?&6#%9'#*d9;:.7G#%9SBfi5?/.0(8[.(86&)(,."!//.&"/.0U!$605?BN&)9S:>D!+&)6#*K/m9;:.&6#1!Bfi9%&6(867U~ 7U,Bfi78#'7*/9C5?=B h ii RO!$#%78P,."!//.&"/.0m#%[?#'9%7*>1d hZf aA/.&637*Bfi#S!$ Q=.69%&JR!$07*/49U< ii RO!$#'78Pu"!//.7*B%r8d 9;:!+9\=.#'78#W!Q/.78~ h ii RO!$#%78P7*/.(85.P&"/.0d 07*/.7*B'!$9%78#=/.&637*Bfi#S!$,."!/.#G&"/>]=.69%&JR!$047*/9G/.5?/4RP789%7*B%>\&2/.&6#%9'&)(@P5?>!$&"/.#*d!/.PU&"/.(8"=.P78#!/.78~y%5?,.9'&2>\&)#'9%&6(8W,."!//.&"/.0!$605?BN&)9S:>1Knm?b'*nfi8'n'cSncF-fi'\fiZ%"nS8;cF'*bSn n]b%b%$;Sn$bSZ*'\nfi;)S8fi b'"'cff *fifififi 0)-- -)G=B5$37*B%!$6E!,,Bfi5.!$(V:DX5?BMP78#'&)0./.&2/.0!/ h ii RO!+#%78Pw,."!//.7*BH&)#H#%&">F&6"!BF9%519;:.7C!,,Bfi5?!$(8:P7837865?,^78PFDX5.BA f K G=B >!$&"/G(85?/49;Bfi&"O=.9%&65?/H&6#E!/7S(8&67*/9Z7*/.(85?P&"/.0H5$D! /.78~DBfi5?/497*/.PFP5?>!$&"/P78#%(*Bfi&",.9%&65?/C"!/.0.=!$07d"!#an5?/4RP789%7*B'>F&"/.&)#'9%&6(G|A07*/49EG5.>!$&"/%$!/.0?=!$07*rK&"!'#m:!$#>\5?Bfi7Bfi78#%7*>FO."!/.(87T~ &69;:w,Bfi783.&65?=.#1,.2!4//.&2/.0"!/.0?=!$0478#9;:!4/9;:.7U!$(89%&65?/P78#%(*BN&2,.9'&)5./w"!/.0.=!$071<(*=B%Bfi7*/49%6[m=.#%78PwO[A f K9:!$#,5$~ 7*BD=.!$(89'&)5./P78#%(*Bfi&",.9%&65?/.#C9;:!$9F(*!/,7*BDn5?B%> !Bfi&69;:>\789%&6(5?,^7*B%!$9%&65?/.#M5?/1/.=>F7*Bfi&6(*!$ZP5?>!$&"/13$!Bfi&"!O.678#*K G5?>@!$&"/.#A(85?>@,Bfi&6#%78PU5$D #%[/.(V:BN5?/.&6878P!+07*/9'# (*!/O7G>F5.P786)78PUO4[F&"/9;BN5?P=.(8&"/.0F(85?/.(*=B%Bfi7*/49!+(89%&65?/.#O!$#'78P5?/@!<>F=.69%&JR!$07*/49EP78(85?>,^5#%&69%&65?/@5$D9;:.7P5?>!+&2/K/Q!$PP&69%&65?/"!#Q&"/9;BN5?P=.(878#!1#'7*,!B%!$9%7!/.Pz78,.6&6(8&697*/3.&"Bfi5?/>F7*/49>F5.P78 P7SI/.78Pw!$#<!#%789H5$)(.Snfi b6+ *8- , !$047*/9%#d&nKc7K"dE!+07*/9'#G~A:.5#%71!$(89%&65?/.#F(*!//.59FO7U!T,!Bfi9<5+D_9S:.707*/.7*B'!$9%78P,."!/.K /"!#:!+#AO787*/T(*!Bfi7SD=.66[1P78#%&60?/.78P9%5!$665$~DX5.B 7S(8&67*/9 h ii R7*/.(85.P&"/.0KAY:?=.#*dhZf(85?/49;Bfi&"O=.9%78#G!F,!Bfi9%&69%&65?/.78P9;B%!4/.#%&69%&65?/1Bfi78"!$9%&65?/TBfi7*,Bfi78#%7*/49;!$9%&65?/C5+D9;:.7<|}9;:!$9 &6#G?/.5$~A/@DBfi5?>>F5.P78E(8:.78(V4&"/.09%5U#%(*!+)71=,z~786Gan =Bfi(V:789]!$nK"d o*pp^o !4/N!/z789]!+XKedoppqr8KG=BH7*>,.&"Bfi&6(*!$78,7*Bfi&">F7*/49%#M#;=.0078#%99;:!$9 9;:.&6#A&6#G!$6#%59;:.7G(*!+#%7MDX5.B h-f K0 h-f &2/.(8"=.P78#M9;:.7A,Bfi783.&65?=.#%6[CP7837865?,78PU!$605.Bfi&69;:>F# Dn5?B h ii RO!$#%78PU=/.&637*Bfi#;!+,."!//.&"/.0K/m!+PP&)9'&)5./dE~ 7C&2/49;Bfi5.P=.(87U!/.78~ %5?,.9%&">F&6#%9%&6(8,."!//.&"/.0m!$605?Bfi&69;:> 9;:!+9\BN782!+?78#F5?,.9%&">!+)&69fi[0?=!B'!/9'7878#A!/.P07*/.7*B%!$9'78#`,."!=.#%&"O.671=/.&)347*Bfi#;!$ ,."!/.#G&"/UP5.>!$&"/.#G~A:.7*Bfi7F/.51#%9;Bfi5?/.0C/.5?B #'9;Bfi5?/.0(8[.(8)&6(F#%5"=.9%&65?/78?&6#%9'#*KY:.7<!Bfi9%&6(867G&6# 5?Bfi0?!4/.&)878PU!+# DX54)65$~ #*K .78(89%&65?/T{HP&)#'(*=.#%#%78#G,Bfi783?&65?=.#G!,,Bfi5.!$(V:.78# 9%5,."!//.&"/.0&"/Q/.5?/4RP789%7*B'>F&"/.&)#'9%&6(P5?>@!$&"/.#*K].78(89'&)5./Q04&)3478#<!1OBfi&67SD_5$37*BN3?&678~ 5$D h ii #]!/.PQ|7*/.(85.P.R&"/.0#*K n9A>@!*[1O7H#;4&2,,^78PQO[UBfi7*!+P7*Bfi#A!$"Bfi7*!$P[CD!>F&66&"!B~ &69;:19;:.7G#S=O.78(89K.78(89%&65?2/ 1&"/9SBfi5?P=.(878#"!'# #;:.5$~ #:.5$~9'57*/.(85.P7U!,."!//.&"/.0,Bfi5?O.67*>1d !/.PzDX5?B'>!$66[QP78#%(*Bfi&"O78#C9;:.7C#%[/9;!+!/.P#%7*>!4/9%&6(8#5$D9;:.&6# P78#%(*Bfi&",.9%&65?/@"!/.0?=!$047 &"/9%7*B'>F# 5$D-!/|]K$xz7A!$6#%5<P&6#%(*=.#'#9S:.7A,Bfi5?,^7*Bfi9%&678#E5$D9;:.7G"!/.0?=!+07<O!$#%78P15?/C!/C78!>,.67<!/.PU!BN0?=.7MDX5.BE5.=BEP78#'&)0./1(8:.5&6(878#*KE.78(89%&65?/Tq],BN78#%7*/9'# 9;:.7"!#QP5?>!$&"/TP78#%(*Bfi&",.9%&65?/.#K].78(89%&65?/zP78#%(*BN&2O^78#<9;:.7FP&J7*Bfi7*/49<!$605$Rh ii Bfi7*,Bfi78#%7*/49;!$9%&65?/5$Bfi&69;:>F# 9S:!$9:!37 O787*/=.#'78P<Dn5?B h ii RO!$#'78P,."!//.&"/.0\!4/.P<&"/9SBfi5?P=.(878#5?=B5?,.9%&">F&6#%9%&6(A,."!//.&"/.0!$605?BN&)9S:>1K<.78(89%&65?/QC,Bfi78#%7*/49%#G7*>,.&"Bfi&6(*!$ Bfi78#;=.69%#G&"/#%78347*B%!$ ,.2!4//.&2/.0TP5?>!$&"/.#*dZB%!/.0&"/.0CDBfi5?>#%&"/.067SR!$07*/49<!/.P1P789%7*B'>F&"/.&)#'9%&6(5?/.78#M9%5>F=.69%&JR!$07*/49`!/.P/.5?/4RP789%7*B%>\&2/.&6#%9'&)(@5?/.78#*K xQ7H78,^7*Bfi&JR>F7*/49E~ &69;:C,Bfi783.&65?=.#%6[W=.#'78PP5?>@!$&"/.#_!4/.P&"/49;Bfi5.P=.(87G9~ 5</.78~5?/.78#*d./!>F786[1!<,5$~ 7*BE,."!/49 !/.P!1#'5?(8(87*BP5?>!$&"/d !$#</.5?/4RP789%7*B'>F&"/.&)#'9%&6(d>F=.69%&JR!$07*/49],."!//.&"/.0,Bfi5.O.)7*>\#*K &"/!$66[dE.78(89%&65?/PB%!~ # (85?/.(8"=.#%&65?/.#<!/.PTP&6#%(*=.#%#%78#MP&"Bfi78(89%&65?/.#MDn5?BD=.9;=BN7G~5.B%K93-54768 6 :V<;87 (*=B%BN7*/9!4,,Bfi5?!$(8:.78#],^7*BDX5.B%>F&"/.0,."!//.&"/.0Q&"/9'7*Bfi67*!*3478Pm5?BH&2/,!B%!+)678 ~&)9S:Q78.78(*=.9%&65?/m:!37O787*/@~ &6P786[W=.#'78P&"/C/.5?/4RP789%7*B%>F&"/.&6#%9%&6(FBfi5?O549%&6( P5?>!$&"/.#<ab7Kc0K"dG785?BN07Sv=$!/.#;4[do*pH!$9*do*pp4{*x&6"&"/.# 789E!$nK"do*p4>p 1$A!$&60?:@vwL 7865#'5do*pp4r8K|0?Bfi5?=,5$D,."!//.7*Bfi##;=.&69;!O.67 Dn5?BZBfi78(*=B%Bfi7*/49,."!//.&"/.0G&6#E!$(89%&65?/G#'78)78(89'5?Bfi# O!$#%78P<5./G:.7*=Bfi&6#%9%&6( #%7*!BN(V:anM5.7*/.&60<vw.&">>F5./.#*d?oppq$5?/.789-789Z!$nK"do*pp4r8K-Y:.7`>F&"/4R>!+ g4?&? !$605?Bfi&69;:>yanM5.7*/.&60v.&">>F5./.#*do*ppq>\&2B'/.53@789!$nK"do*pp4rZ(*!/07*/.7*B%!+9%7#S=O5?,.9%&">!+A,."!/.#&"/m/.5?/4RP789'7*B%>F&"/.&6#%9%&6(P5?>!$&"/.#9;:BN5?=.0?:!#%7*!Bfi(8:!/.Pm78?78(*=.9'&)5./&69%7*B%!$9'&)5./KAY :.7H#%7*!Bfi(8:1&6#GO!$#%78PU5./U!C:.7*=Bfi&6#%9%&6(F05?!$P&6#%9S!/.(87HD=/.(89%&65?/T9;:!+9A>F=.#%9AO7,Bfi5$3.&6P78PDn5?B!#;,^78(8&JI(,BN5?O.67*>1KwY :.7 f !$605.Bfi&69;:> an5?/.7897891!$nK"dMo*pp4r<=.#%78#1!#%&">F&6"!BC!,,Bfi5?!$(8:!/.PD=Bfi9;:.7*BFP7SI/.78#1!Q:.7*=Bfi&6#%9'&)(D=/.(89'&)5./Dn5?AB @CB)DFEHGI@R)&"$7mab &"$78#vy&6)#'#%5?/dGo*p^o*rF!$(89%&65?/Bfi7*,Bfi78#'7*/9;!+9%&65?/.#*K</(85?/49;B%!$#%9M9%51>F&"/4R>!$ g4?J? f P5.78#</.59<!$#%#S=>F7!1/.5?/4RP789%7*B%>F&"/.&6#%9%&6(fi Kfifi0 J7*/43?&"Bfi5?/>\7*/9*d O=.9H&)#FBfi5?O=.#%9H9%5/.5./4RP789%7*B%>F&"/.&6#;> (*!=.#%78PmO[Q!$(89'&)5./Q,7*Bfi9S=B%O!$9%&65?/.#Uab&nKc7K"d9;:!$9!/.59S:.7*B !$(89%&65?/C9;:!4/C9;:.7],."!//.78P!$(89%&65?/1&6# (8:.5#%7*/~ &69;:C#%5?>\7<,Bfi5?O!O.&66&69[r8K/T07*/.7*B%!$nd-BN78(*=B%Bfi7*/49<!,,Bfi5?!$(8:.78#<!Bfi7H&"/.(85?>,.6789%71O78(*!=.#%7C!$(89%&"/.015?/!/T&"/.(85?>,.6789%71,.2!4/(*!/C>!$7 9;:.7M05?!+=/!$(V:.&6783$!O.67K uEBfi78(*=Bfi#'5?BE!,,BN5?!$(8:.78#,7*BDX5?B'>!$6P78(8&6#%&65?/T>!4&"/.0],Bfi&65?B9%578.78(*=.9%&65?/U!/.P19S:?=.# >![O^7<!O.67<9%5F07*/.7*B%!+9%7A(85.>,.6789%7<,."!/.#`O4[9;!4&"/.0!$6-,^5#%#%&"O.67<7S78(89%# 5$D!$(89%&65?/.#&2/49%5!+(8(85?=/9KE 5$~ 7837*B8d.9;:.78[1Bfi786[15?/1!F(85.>,.6789%7]>F5.P785$D 9;:.7G~ 5?Bfi6MP Lc#M=/.(87*Bfi9;!$&"/49[KY:.7A,Bfi78(*=Bfi#%5.B!4,,Bfi5?!$(8:.78#E&"/.(8"=.P7<(85?/.P&69%&65?/!+Oa N 9'8&)5./.&789!+XKedopp{uZ7859 v>\&)9S:dopp{6[?9;:.7v L 7865#%5dFo*pp4r8d ,Bfi5?O!4O.&)6&6#%9%&6(ab<B'=>>F5?/.Pv Bfi78#%&"/!dFo*pKp P^GG7*!/789U!$nK"d<oppq6[?9;:.7do*pp4rA!/.P=/.&)347*Bfi#;!$E,."!//.&"/.0wan.(8:.5?,,7*Bfi#dEo*p4 &">!$9%9'&-789<!$nK"d opp!dZo*pp4OG!RO!/.*!F789M!$nK"do*pp4r8K5?B 78!>,.67d9;:.7Ft f ,!Bfi9'&2!+5?BfiP7*B8d.(85?/.P&69%&65?/!+,."!//.7*BM:!/.P678#</.5?/4RP789%7*B%>\&2/.&6#;> O[Q(85./.#%9;B%=.(89%&"/.0Q!T(85?/.P&69%&65?/!$ ,."!/9;:!$9F!$(8(85?=/49%#HDX5.BG7*!$(8:,^5#%#%&"O.67U#'&)9S=!$9%&65?/5?B(85?/49%&"/.07*/.(8[U9;:!+9A(85?=.6PQ!Bfi&6#%7UanuZ78549`v>F&69;:d o*pp{4r8K | 9M78.78(*=.9%&65?/T9%&">F7F&)9G&6#GP789%7*B'>F&"/.78P~A:.&6(8:,!BN9G5$D 9;:.7,."!/z9%5178?78(*=.9'7WO4[Q,7*BDn5?B%>F&"/.0T#%7*/.#%&"/.0Q!$(89%&65?/.#<9S:!$9]!BN7]&"/.(8"=.P78Pw&"/z9;:.7,."!/19'59%78#%9Dn5?B 9;:.7<!,,BN5?,Bfi&"!$9%7G(85?/.P&69%&65?/.#KuBfi5?O!O.&66&6#%9%&6(],."!//.7*BN#A9;BN[9%5>!+?&">F&687<9S:.7<,Bfi5?O!O.&66&69[15+D05.!$#;!$9%&6#fiD!$(89%&65?/d.0&637*/(85?/.P&JR9%&65?/!$Z!$(89%&65?/.#M~ &69;:U,Bfi5.O!O.&6)&6#%9'&)(@7S78(89'#*KE<B'=>>F5?/.PU!/.P Bfi78#%&"/!aNo*pKp Pr Bfi7*,BN78#%7*/9G,."!/.#G!$#!@#%789 5$D.&69;=!$9'78P_5?/9SBfi5 =.678#an #Sr]ab<B'=>>F5?/.Pdoppr >@!,,.&"/.01#%&69;=!$9%&65?/.#M9%5!$(89'&)5./.#*K:.7,."!//.&"/.0!+)045?Bfi&69;:>yO7804&2/.#O4[Q!$PP&"/.0 #<(85?B'Bfi78#;,5./.P&2/.09%519S:.7>F5#%9<,BN5?O!O.6778.7SR(*=.9%&65?/,!$9;:9;:!$9A!+(V:.&678378#G9;:.7H05?!$nK 9A9;:.7*/(85?/49%&"/?=.78#<!$PP&"/.0U # Dn5?B)78#'#`,Bfi5.O!O.67\,!+9;:.#!/.PU>![C7*/.PU~ &69;:1!\(85?>,.6789%7],."!/9;!4&"/.0!$6-,54#%#%&"O.67\,!+9;:.# &"/9'5!$(8(85?=/49*KM/.&637*Bfi#;!+,.2!4/.#P&J7*BZDBfi5?> (85?/.P&69%&65?/!$!/.P,Bfi5?O!4O.&)6&6#%9%&6(F,.2!4/.#O4[]#S,78(8&JDX[.&"/.0!,,Bfi5.,Bfi&"!$9%7!$(89%&65?/.#\DX5?BH7837*BN[,^5#%#%&"O.67U#%9S!$9%7C&2/ 9;:.71P5?>!+&2/QK $&"$71(85./.P&)9'&)5./!$`!4/.Pw,Bfi5?O!O.&66&6#%9%&6(Q,.2!4/.#*d=/.&637*Bfi#;!+-,."!/.#`BN78=.&"Bfi7H9;:.7<~ 5?Bfi6P19%5O^7]!$(8(878#%#'&2O.67F&"/15?BNP7*B 9%578.78(*=.9%7<9S:.7<,.2!4/KM/.&637*Bfi#;!+,."!//.&"/.0C~ !$# &2/49;Bfi5.P=.(878PO4[.(8:.5?,,7*BN#\afiopr~A:.5=.#%78P1P78(8&6#%&65?/19;BN7878# 9%5Bfi7*,4RBfi78#%7*/49<,."!/.#*K 78(87*/9F!,,Bfi5?!$(8:.78#M&2/.(8"=.P71G!O!/.*!C789<!$nKEafioppr !4/.P_&2>@!$9%9%&Z789`!+XKEaNo*pp!^do*pp4Or8KAG!4O!/.*!789<!+XK afio*p4pr Bfi7*,BN78#%7*/9'#`=/.&637*Bfi#S!$E,."!/.#<!$6#%5U!$#G!#'789A5$D.&69;=!+9%78P 5?/49;Bfi54=.678#*K :.78&"BA!+)045?Bfi&69;:>&"/.(*Bfi7*>F7*/49;!$66[!+PP#A #M9%5!HI/!$-,."!/&"/U!H~ !*[@#%&">F&62!4BM9%5F<B%=>HR>F5?/.PQ!/.P Bfi78#%&"/!afiopKp Pr8K Y:.7<05?!+-&6#<!\DX5?B'>]=."!@&"/9%7*>,^5?B%!$6504&)(@9;:!$9A>F=.#%9G:.56PQ5?/T!/[3!+)&6P#%78=.7*/.(87<5$DE!$(89%&65?/.#K9 $7*!B%/.&"/.0a $r`a=.9%9%5?/Cv !BN9%5doppr(*!/!+)#'5\O^7<Bfi780?!BNP78PW!$# =/.&637*Bfi#S!$78&2/4Dn5?Bfi(87*>\7*/,."!//.&"/.0K / $9S:.7A05.!$&6# Bfi7*,Bfi78#%7*/49%78PUO4[!<BN78~_!4BfiPD=/.(89%&65?/C&"/1!GQ!B'$5$3]G78(8&6#%&65?/1uBfi5.(878#%#ab<uErM>F5.P78 5$D9S:.7]P5.>!$&"/K]/9;:.7,Bfi78(*=Bfi#'5?BM37*Bfi#%&65?/5$D $ d9;:.7F<u&)#<!+#%#;=>F78PQ9'5WO^7./.5~M/Q!/.Pm!C(85?/9SBfi5E,54)&6(8[m>!$.&">F&68&2/.0z9;:.778,78(89'78PBfi78~ !BNPQ&6#<Dn5?=/.P,BN&)5.B<9'5178?78(*=.9%&65?/K:.7,54)&6(8[(*!/78&)9S:.7*B<O7Bfi7*,BN78#%7*/9'78P78,.6&6(8&)9')[z&2/!@9;!O.67F5?B&2>@,.)&6(8&69%6[mO[U!\D=/.(89'&)5./wab7Kc0Ked!/.7*=B%!$ /.789fi~ 5?B%.r8K78(*!=.#'7 $&6#!,Bfi5?O!O.&66&6#%9%&6(!4,,Bfi5?!$(8:d &69%#FP5?>!$&"/mBfi7*,BN78#%7*/9S!$9%&65?/z&)#>F5?BN7G(85?>,.67819;:!/9;:.7GP5?>!$&"/TBfi7*,Bfi78#%7*/49;!$9%&65?/T=.#%78PO4[1!/.5?/4RP789'7*B%>F&"/.&6#%9%&6(,.2!4//.7*B8K :.=.#*d~ 7`>@!*[F78,78(89A/.5./4RP789%7*B%>F&"/.&6#%9%&6(,."!//.7*Bfi# 9%5FO7<!O.67G9%5:!/.P67GP5?>@!$&"/.#~&)9S:1!<"!BN07*B#%9;!$9%7#;,!$(87F9;:!4/ $ K- =.9 $>![U,Bfi5?P=.(87C,56&6(8&678#~ &69;:!1:.&60?:.7*BG=!$6&69[T9;:!/!1=/.&637*Bfi#;!+E,.2!4/07*/.7*B%!+9%78PO4[!z/.5?/4R,Bfi5?O!O.&66&6#%9%&6(d</.5?/4RP789'7*B%>F&"/.&6#%9%&6(,."!//.7*B8K5?Bfi785$37*Bd &"/m9;:.7Bfi78(*=B%Bfi7*/4937*Bfi#'&)5./d $z)7*!4B%/.#A9S:.7G~5.Bfi6PU>F5?P78P=Bfi&"/.078.78(*=.9%&65?/U!4/.P19;:?=.#P5?78#G/.59 Bfi78=.&2BN7]!F(85?>@,.)789'7~ 5?Bfi6P>\5?P78 ,Bfi&65?B9%578.78(*=.9%&65?/KAY :.5.=.0?:d&"/T9;:.785?BN[&69G/.7878P#G&2/4I/.&69%7@78?78(*=.9%&65?/T78!>,.678#G9%5(85?/437*Bfi07G9'5]9S:.7<5?,.9%&">!+=/.&637*BN#;!$ ,.2!4/K|M6,Bfi783.&65?=.#A!,,BN5?!$(8:.78#E9%5F=/.&)347*Bfi#;!$,.2!4//.&2/.0d78.(87*,.9 &">!$9%9%&789 !$nKafio*pp4!d?oppOr8d.=.#%7!/78,.6&6(8&69]Bfi7*,Bfi78#'7*/9;!+9%&65?/C5$DE9;:.7F=/.&)347*Bfi#;!$ ,."!/ab7K 0K"d #;rK :?=.#d&"/19S:.7<07*/.7*B%!+(*!$#%7d!/fi Rfi 0)-- -)78,5?/.7*/49%&"!$#%&687H5$D-9;:.7<,."!/C&"/19S:.7`/.=>FO7*B 5$D ,Bfi5?,54#%&69%&65?/.#AP7SI/.&"/.01!HP5?>!$&"/C#%9;!+9%7`>F=.#%9 O^778,78(89%78Pd!$#A!BN0?=.78PUO[1G&"/.#;O^7*Bfi0Qafio*pp4r8KY:.7<(85?>,!+(89<!/.P&2>@,.)&6(8&69Bfi7*,Bfi78#%7*/49;!$9'&)5./15$D =/.&)347*Bfi#;!$ ,."!/.#<5?O.9S!$&"/.78P~&)9S: h ii #<P5.78#/.59F/.78(878#%#;!Bfi&66[Q0?BN5~ 78,5./.7*/9%&"!$66[zDX5?BGBN780?=."!Bfi6[Q#%9;B'=.(89;=Bfi78PQP5?>@!$&"/.#\!+#G#;:.5$~A/QO4[_&2>@!$9%9%&789<!$nK afio*pp4!r8K =BN9;:.7*B8d9;:.7 h ii RO!+#%78PBfi7*,Bfi78#'7*/9;!+9%&65?/T5$DE9;:.7 | 5+D!1/.5?/4RP789%7*B%>F&"/.&6#%9%&6(P5?>!+&2/7*/!O.678#G9;:.7!,,.6&6(*!$9%&65?/T5$DE7S(8&67*/49A#%7*!4Bfi(V:T!$605?Bfi&69;:>\# DBfi5?>>\5?P78Z(V:.78(84&2/.01(*!4,!O.675$DE:!/.P6&"/.0137*Bfi[C2!4Bfi07G#%9;!+9%7A#S,!$(878#*K'UTVWXW'|G/wGBfiP7*Bfi78P&"/!Bfi[mG78(8&6#%&65?/G&"!$0.B%!> Bfi[.!/9d o*p4rF&6#W!z(*!/.5?/.&6(*!$GBfi7*,Bfi78#%7*/49;!$9'&)5./m5$D<!O5.567*!/WD=/.(89%&65?/~ &69;Z: YT6&"/.7*!BM5?BfiP7*BN78PU!Bfi0?=>F7*/49%#\[ #] [ (^]_`_a_`] [/b?K|G/ h ii &)#E!MBfi5.59%78PdP&"Bfi78(89%78P!$(8[.(8)&6( 0.B%!,:G~&)9S:<5./.7E5?B9fi~ 5 9%7*B'>F&"/!$/.5.P78# 5$D5?=.9NRP780?Bfi78787*Bfi5H2!4O78678PQo 5?"B Pd?!4/.P!G#%7895$D3!BN&2!4O.)7</.5.P78d# c@5$D5?=.9fiRP780.Bfi787A9fi~ 5KY :.7M9fi~ 5G5?=.9%05&"/.0H78P078#!Bfi7C0&637*/O[Q9S:.7CD=/.(89%&65?/.f# e?JK eOa crF!/.P K g<Oa cr8hK N!$(8:3!Bfi&"!O.67/.5?P71&6#!$#'#%5?(8&"!$9'78Pm~ &69;:m!,Bfi5?,^5#%&69%&65?/!$ 3$!Bfi&"!O.67&"/9;:.7O^5?567*!/D=/.(89%&65?/9;:.7 h ii BN7*,Bfi78#%7*/49%#*K<Y :.7F0?B'!,:1&6#<5.BfiP7*Bfi78P&"/19;:.7G#'7*/.#%7<9;:!+9_!+) ,!$9;:.#&2/9;:.7G0?B%!4,:1Bfi78#;,78(899;:.7<5?BNP7*Bfi&"/.05$D 9;:.7G3!BN&2!4O.)78#K|G/ h ii Bfi7*,BN78#%7*/9'&2/.0C9;:.7D=/.(89%&65?f/ Oa [ #] [ ( Xr jU[ #lk [ ( &6#G#;:.5$~A/C&"/ &)0.=Bfi7oK G&637*/T!/!$#%#'&)0./>F7*/95$D9S:.7<!Bfi0?=>F7*/49%# [ # !/.ZP [ ( d^9;:.7G3!$"=.7F5$XiT&)#MP789%7*B%>\&2/.78PQO4[W!@,!$9;:C#%9;!4Bfi9%&"/.0!$99;:.7GBfi5.59 /.5?P7<!/.P1&69%7*B'!$9%&63786[Dn5665$~ &"/.09;:.7G:.&)0.:178P07d.&JD9S:.7`!$#'#%5?(8&"!$9'78P13!Bfi&"!O.67H&)#9;B%=.7d!/.P9;:.7G65$~78P07d&JDE9;:.7<!+#%#%5.(8&2!+9%78P13!BN&2!4O.)7F&6#D!$6#%7KY :.7G3!$"=.7H5$JiT&6# ( , 5.oB n , &JD 9;:.7G"!O785$D 9;:.7<Bfi7*!$(8:.78PU9'7*B%>F&"/!$-/.5.P7G&6#]oG5.\B PdBfi78#;,^78(89%&63786[Kx1x201&60?=Bfi7oKp|`/ h ii Bfi7*,Bfi78#'7*/9%&"/.0C9;:.7GD=/.(89%&65?/7i aO[ # ] [ ( rjq[PB%!~A/C!$# #%56&6P!4/.PUP59%9'78P1)&"/.78#*dBfi78#;,^78(89%&63786[K# k[ (K &)0.:U!/.PT)5$~}78P078#G!Bfi7| / h 0?B%!,:\&)# Bfi78P=.(878PC#%5H9;:!$9/.5H9fi~5GP&6#%9'&2/.(89M/.5?P78#\cC!/.PsrF:!37 9S:.7 #;!>F7M3!4Bfi&"!O.67G/!>F7C!/.P65$~ !4/.P:.&60?:z#;=.(8(878#%#%5.Bfi#Uab &60?=Bfi7{!4r8d-!/.Pm/.5U3$!Bfi&"!O.67U/.5.P7sc:!$#F&6P7*/9%&6(*!$ 65$~!/.PU:.&60?:#;=.(8(878#%#%5?BN#\ab &60?=BN7<{Or8KY:.7 h ii Bfi7*,Bfi78#%7*/49;!$9'&)5./Q:!$#G9fi~ 5U>!$fi5?BM!$P3!/49;!$0478# pG &"Bfi#%9*d&69<&6#!/7S(8&67*/49]Bfi7*,Bfi78#'7*/4R9;!$9'&)5./5$D`O^5?567*!/D=/.(89%&65?/.#O78(*!4=.#%719;:.71/?=>FO7*BH5$DA/.5.P78#F5$DX9'7*/Q&6#>F=.(V:z#;>!$667*BH9;:!/z9;:.7/.=>]O^7*BM5$D 9;B%=.9;:T!$#%#%&60?/>F7*/49%#M5$DE9;:.7H3!Bfi&"!O.678#*KHY :.7]/.=>FO7*BM5$DE/.5?P78#G(*!4/10?Bfi5$~78,5./.7*/9%&"!$~ &69;:@9;:.7</.=>]O^7*B5$D-3!4Bfi&"!O.678#*dO=.9 >F5#%9(85?>>F5./.)[C7*/.(85?=/49%7*Bfi78P@D=/.(89'&)5./.#A:!37`!FBfi7*!+#%5?/!O.67Bfi7*,Bfi78#'7*/9;!+9%&65?/K .78(85?/.Pd!/[F5?,^7*B%!$9%&65?/\5?/F9fi~ 5 h ii #*d.(85?B%BN78#;,5?/.P&"/.0F9%5F!<O5.567*!/@5?,7*B'!$9%&65?/5?/F9S:.7 D=/.(89'&)5./.#9S:.78[\BN7*,Bfi78#%7*/49*d?:!$#E!65~(85?>,.678.&)9fi[O^5?=/.P78P1O[F9;:.7A,BN5?P=.(895$D9;:.78&"BE/.5.P7(85?=/49%#]an Bfi[.!/9dopr8Kfifi0 Juvuxxx(b)(a)&60?=Bfi7<{p 87 P=.(89%&65?/.#H5$D h ii #*K1an!r/.5?P78#F!$#%#%5.(8&"!$9%78P9%5C9;:.7F#;!>F7H3!Bfi&"!O.67~&)9S:78=!+-65$~!/.PU:.&60?:1#S=.(8(878#%#%5?Bfi#~ &6) O7H(85?/437*Bfi9%78P19%5C!<#'&2/.04)7/.5.P7K`aOr/.5.P78#M(*!=.#%&"/.0WBN78P=/4RP!/499'78#%9%# 5?/1!\3!Bfi&"!O.67F!Bfi7G786&">F&"/!$9%78PK| P &)#S!$P3!/49;!$047A5$D h ii # &6# 9;:!$9 9;:.7M#%&687G5$D-!/ h ii Bfi7*,Bfi78#'7*/9%&"/.0#'5?>F7D=/.(89%&65?/C&6# 37*Bfi[P7*,7*/.P7*/49F5?/9;:.7@5?BfiP7*Bfi&"/.0U5+D9;:.73$!Bfi&"!O.678#*K1Y51I/.P!/5?,.9%&">!$ 3$!Bfi&"!O.675.BfiP7*Bfi&"/.0U&6#]!(85$RMuZR(85?>,.6789%7,Bfi5.O.)7*> &"/&)9'#%78JDNdO=.9G!$#M&)6"=.#%9;B'!$9%78P&"/U &60?=BN7]C!05.5.P:.7*=Bfi&6#%9%&6(HDn5?B (8:.5.5#%&"/.0!/C5?BNP7*Bfi&"/.0&6# 9%5@)5.(*!$9%7FBfi78"!$9%78P13$!Bfi&"!O.678#</.7*!B 7*!$(8:C59;:.7*BHan "!B%$7G789M!$nK"do*ppprKx1x1y1x2x2x3x2x3x3y1y2y2x3y3y1y1y1y2y310x310(a)(b)&60?=Bfi7<pMY :.&6#< &60?=Bfi7C#;:.5$~ #G9;:.7C7S78(89<5$D 3$!Bfi&"!O.675.BfiP7*Bfi&"/.0TDX5?BM9S:.778,Bfi78#%#%&65?/aO[ #lkvu# r&wOa [ ( k7u ( &r wmOa [ % k7u % r8KY:.7 h ii &"/an!4rM5?/.6[0.Bfi5$~ #G6&2/.7*!4Bfi6[~ &69;:9;:.7/.=>]O^7*BG5$D3!BN&2!4O.)78#C&"/9;:.7178,Bfi78#%#%&65?/d~A:.&6679S:.7 h ii &"/anOrF:!$#C!/Q78,5./.7*/9%&"!$M0?Bfi5$~ 9S:K:.7F78!4>,.67&66"=.#%9;B%!$9'78#<9;:!$9<,."!$(8&"/.0QBfi78"!$9%78Pz3!Bfi&"!O.678#/.7*!BM9%517*!+(V:T59;:.7*B&"/9;:.75?BfiP7*Bfi&"/.05+DX9%7*/C&6#G!F05.5?PU:.7*=BN&)#'9%&6(Kh ii #E:!37EO787*/F#S=.(8(878#%#fiD=.)6[!,,.6&678P9%5G>F5.P78?(8:.78(V4&"/.0K /F>F5.P78?(8:.78(8&"/.0G9;:.7 O7*:!3?&65?B5$D-!H#%[.#%9%7*> &6# >F5.P786)78PO[C!MI/.&69%7G#%9;!+9%7A!=.9%5.>!$9%5?/H~ &69;:@9;:.7M9;B%!/.#'&)9'&)5./Bfi78"!$9'&)5./Bfi7*,BN78#%7*/9'78P!$# !/ h ii KEG78#%&"B%!4O.)7<,BN5?,7*Bfi9'&)78#G!Bfi7M(8:.78(V$78PUO4[1=.#%&"/.0 h ii >!/.&",=."!$9%&65?/.#M9%5!/!$6[?87H9;:.7#%9;!+9%7A#S,!$(87<5$D 9;:.7G#%[.#%9%7*> "!B%$7M789A!$nK"do*p.(8&6)"!/dZo*pp4r8Kfi yxfi 0)-- -)/9'7*Bfi78#%9%&"/.06[dG!#%&">F&6"!BU!,,BN5?!$(8:m(*!/O7=.#%78PDX5?B@#%563.&"/.0/.5?/4RP789%7*B'>F&"/.&)#'9%&6(,."!//.&"/.0, Bfi5?O.67*>F#K |A# !/@78!>@,.)7d.(85?/.#'&)P7*B_9;:.7A|Bfi7*,Bfi78#%7*/49;!$9'&)5./F5$D-!</.5?/4RP789'7*B%>F&"/.&6#%9%&6(],."!//.&"/.0P5?>!+&2/C#S:.5~M/&"/1 &60?=BN71?!K /C9;:.&6# P5.>!$&"/C9;:.7*Bfi7<!BN7 Dn5?=B #%9S!$9%78#E04&)347*/UO4[9;:.7MDn5?=B ,5#%#'&2O.673!+2=.7!$#'#%&60?/>F7*/49%# 5$D 9;:.7G9fi~ 5\O^5?54)7*!4/1#%9;!+9%7G3!BN&2!4O.)78# [ # !4/.Pz[ ( K /,=.9%#M9%5F9;:.7]|}P7*/.59%7!$(89%&65?/.#G&"/9S:.7P5?>!$&"/!/.PQ!Bfi7FP7SI/.78PmO[T9;:.7O5.567*!/3$!Bfi&"!O.6s7 {KFY :.7 h ii Bfi7*,BN78#%7*/9'&2/.09;:.7G9;B'!/.#%&69%&65?/1Bfi78"!$9%&65?2/ |]Oa { ] [ #] [ (] [} # ] [}( r 5$DZ9;:.7< |}&6# #;:.5$~A/C&"/1 &60?=BN7 1?OKEY :.7GP7SI/.&69%&65?/5$&|&6#M#%9;B%!+&)0.:9fiDn5?Bfi~ !BNMP pDn5?B #%5?>\7<!$#%#%&60?/>F7*/49M5$D&69%#G!Bfi0.=>F7*/9'#*d |&6#A9;B'=.7<&J!$(89%&65?Z/ {C(*!4=.#%78#!F9;B'!/.#%&69%&65?/CDBfi5?> 9;:.7<(*=B%BN7*/9M#%9;!$9'7G0&637*/UO4[9S:.7<3!+2=.7F5$X[ # !/.zP [ ( 9%59;:.7F/.78?9M#%9;!+9%7A04&)347*/O4[19;:.7<3$!$"=.7]5+J[ } # !/.7P [ }( K ( 59'7<9;:!$99;:.7 h ii BN7*,Bfi78#%7*/49%&"/.%0 | DX5.B 9;:.7<78!>,.67F9;=B%/.#5?=.9/.59 9'5]P7*,^7*/.P5?/ [ }( K0001100011111x1x1x1x1x2x210(a)(b)P1001G0x101x1x211001(d)(c)&60?=Bfi71p|,.2!4//.&2/.0zP5?>!$&"/Bfi7*,Bfi78#%7*/49%78Pm!$#!/Q| &6##S:.5~M/&"/an!4r8K1.9;!$9%78#F!Bfi7CP7SI/.78PO[O5.567*!/#%9S!$9%7<3$!Bfi&"!O.678)# [ # !/.vP [ ( d!/.P9;:.7!$(89%&65?/T&"/,=.9F9%5C9;:.7 |&)#G04&)347*/O[@9;:.7`O^5?54)7*!4/13!4Bfi&"!O.6)7 {KY:.7G#%[>FO^56&)(FBfi7*,Bfi78#%7*/49;!$9'&)5./5$DZ9;:.7G9;B%!/.#'&)9'&)5./Bfi78"!$9'&)5./5$D 9;:.7 | &6#G#;:.5$~A/&2/manOrKA/mab(*rd ~ # &6#G9;:.7H#%789A5+DE#%9;!$9%7F!$(89%&65?/T,!$&"Bfi#MDX5.B ~A:.&6(V:d78?78(*=.9'&)5./U5+D9;:.7F!$(89%&65?/T(*!/T67*!$P9%59S:.7]045?!$nKGY :.7F#%[>FO^56&)(CBfi7*,Bfi78#%7*/49;!$9%&65?/T5$Dd~ #&6##S:.5~M/F&"/QabPr8K n9&)#5?O.9;!$&"/.78P@DBfi5.> 9;:.7M9;B%!/.#'&)9'&)5./Bfi78"!$9'&)5./O4[Bfi78#%9SBfi&6(89%&"/.0]9S:.7`/.78.9#%9;!$9'7A9%55 Po4K|M#%#;=>\79S:!$9-9S:.7#'9;!$9%7 PoE&6#E!M05.!$?#'9;!$9%"7 K|$78[<5.,7*B%!$9'&)5./d$~A:.7*/H07*/.7*B%!$9%&"/.0<!G=/.&637*Bfi#S!$., "!/DX5.B !$(8:.&)783.&"/.0d&6#M9%5FI/.PQ!$69;:.7H#%9;!$9'7<!$(89%&65?/U,!+&2BN#\aO ] {.r #;=.(8:19;:!+9\(*!/TO7Bfi7*!$(8:.78PDBfi5?><O4[78.78(*=.9%&"/.0{KEY:.&)##%789 &6# "!O78678Pv~ # &2/ &60?=Bfi71(K Y5FI/.Pz~ # DBfi5?>|~ 7G(85?/.#'9;B%!$&"/[ } 9%5 n , !/.P [ }( 9%5 ( , &"7/ |HKY :.&6#Bfi78P=.(878# |9%519;:.7 h ii #S:.5~M/U&"/z &60?=Bfi751PKCY :.7#Bfi78#;=.69%&"/.0 h ii Bfi7*,Bfi78#%7*/49%5# ~ # ~&)9S:9;:.71#%9;!$9'78#]P78#'(*Bfi&"O78P&"/m9;:.7C(*=B%Bfi7*/49#%9;!+9%73$!Bfi&"!O.678# [ #$;b$ Sn;b'<cff8\K'K--fi *HHS&fi$nF"% *n*-'8 nnSnX%'nHfifiX<HSn$;n<b&$ ;nSn;b' fi nZb fiS 'MSn;b'<c< y* n$8 cfifi0 J!/.PQ[ ( KZ$504&)(*!+)6[~ 7,7*BDn5?B%>\78P9;:.7@5?,7*B%!+9%&65?/h[Bfi7*,Bfi78#'7*/9%&"/.0~ # K} ] [ } _ [ } k#(#[ } k(|9%5T5?O.9;!$&"/9S:.7h ii'65AX6'Uv6 -6 qJ'/9;:.&6#<#%78(89%&65?/d~ 7<IBN#%9AP&6#%(*=.#'#]9S:.7\,BN5?,7*Bfi9'&)78#H5$"!#O!$#'78P5./!4/&"/4DX5?B'>!$ZP7SI/.&)9'&)5./Q5$D9;:.7C"!/.0?=!$07U!4/.P!TP5?>!+&2/7*/.(85.P&"/.0Q78!>,.67KQxQ719S:.7*/QP78#%(*BN&2O^7U9;:.7@Dn5?B%>!$#%[/9;!+!/.P#%7*>!4/9%&6(8# 5+d"!#K|G2/ /"!#QP5?>!+&2/@P78#%(*Bfi&",.9%&65?/(85?/.#%&6#%9%# 5$HD p-!HP7SI/.&69%&65?/C5$DV ,* + *8- , 8d?!P78#%(*Bfi&",.9%&65?/C5$D8*8 , !/.P , fi ), ,bVd!/.PU!F#S,78(8&JI(*!$9%&65?/T5$DE!/bbn !/.P1. ; bnKY:.7G#%789 5$D-#'9;!$9%7M3!BN&2!4O.)7!$#'#%&60?/>F7*/49%# P7SI/.78#A9;:.7H#%9;!$9'7A#;,!+(87G5$D9S:.7GP5?>!$&"/K |G/1!$07*/49 Lc#P78#%(*Bfi&",.9%&65?/H&6#-! #'7895$D *nb8K-Y:.7E!$07*/49%#(8:!/.07 9;:.7 #%9S!$9%7 5$D9;:.7 ~ 5?Bfi6P<O4[`,^7*BDX5.B%>F&"/.0`!+(89%&65?/.#9;:!$9 !Bfi7G!$#%#;=>\78P19%5]O^7G78?78(*=.9%78P1#'[/.(8:Bfi5?/.5?=.#')[1!/.PC9%5:!37`!I.78PW!4/.P178=!$P=B%!+9%&65?/KZ| 97*!$(8:Q#'9%7*,d !$65+DA9;:.71!$07*/49%#],^7*BDX5.B%> 78!$(89')[z5?/.7W!+(89%&65?/d !/.Pm9;:.71Bfi78#;=.69%&"/.0!$(89%&65?/z9;=,.67C&)#! V4nKY :.71#'[?#%9'7*> !+07*/9'#\>\5?P78 9S:.7UO7*:!3?&65?B\5$DA9;:.7U!$047*/9%#F(85?/49;Bfi54)"!O.67O[9;:.7,."!//.7*B8d ~A:.&6679;:.717*/43?&"Bfi5?/>\7*/9!+07*/9'#>F5.P78 9;:.7U=/.(85?/49;Bfi56"!O.67~ 5?Bfi6PKm| 3!$6&6PP5?>!$&"/P78#%(*Bfi&",.9%&65?/ Bfi78=.&"Bfi78#19S:!$99S:.7U#%[.#%9%7*> !/.P7*/3.&"Bfi5?/>F7*/491!$07*/49%#(85./.#%9;B%!$&"/!QP&6#nfi5&"/91#'7895$D3!4Bfi&"!O.678#*K|G/!$(89%&65?/:!$#19;:BN787,!Bfi9%^# p!z#%78915$D8 ,* + *8- , 8dA! , ; bn DX5?B'>]=."!dM!/.P!/,Oo, Dn5?B%>F=."!K]/9S=.&)9'&)3478)[z9;:.7!$(89%&65?/T9;!$78#GBfi78#;,^5?/.#%&"O.&66&"9fi[Q5$D(85./.#%9;B%!$&"/.&"/.0U9S:.7]3$!$"=.78#<5$D 9;:.7#%9;!+9%7F3!Bfi&"!O.678#F&"/9;:.7/.78.9<#'9;!$9%7KFn9D=Bfi9S:.7*BG:!$#<78.(8"=.#%&637W!+(8(878#%#<9%519S:.78#%73!4Bfi&"!O.678#]P=BN&2/.078.78(*=.9%&65?/K/5.BfiP7*B@DX5.B9;:.7!$(89%&65?/9%5wO7!,,.6&)(*!4O.)7dH9;:.7Q,Bfi78(85?/.P&69%&65?/Dn5?B%>F=."!m>]=.#'9WO^7#;!$9'&)#NI78PU&"/T9;:.7<(*=B%BN7*/9M#%9;!$9'7KY :.7F7S78(89 5$DE9S:.7]!$(89%&65?/&6#AP7SI/.78PO4[C9;:.7<7S78(89DX5?B'>]=."!@9;:!$9>F=.#%9<O7@#;!$9%&6#fiI78P&"/9;:.7/.78.9G#%9;!+9%7K<Y5U!$6)5$~(85?/.P&69%&65?/!$ 7S78(89%#*d9;:.7@7S78(89G78,Bfi78#%#'&)5./(*!/Bfi7SDn7*B 9'5O59S:1(*=B%Bfi7*/49G!/.P/.78.9A#'9;!$9%7H3!Bfi&"!O.678#*d~M:.7*Bfi7<9;:.7F/.78?9M#%9;!+9%7G3!BN&2!4O.)78#</.7878P9%51O7F!,!Bfi9Z5$D9;:.7 #%789Z5$D(85?/.#'9;B%!$&"/.78PF3!BN&2!4O.)78#5$D9S:.7!$(89%&65?/KZ|M6/.78?9 #%9S!$9%7 3!4Bfi&"!O.678#/.59Z(85?/.#%9;B'!$&"/.78PO4[W!/4[1!$(89%&65?/C&"/U!Mfi5&"/49A!$(89%&65?/T>!$&"/9S!$&"/C9;:.78&"B 3$!$"=.7KY:.7 &"/.&)9'&2!+!4/.P05?!$.(85?/.P&69%&65?/1!BN7EDn5?B%>F=."!$#9;:!$9E>F=.#%9EO7M#;!$9'&)#NI78P&"/9;:.7&2/.&69%&"!$#%9S!$9%7 !/.P9;:.7MI/!$#%9;!$9%7dBfi78#S,78(89%&63786[KY:.7*Bfi7!Bfi7F9fi~ 51(*!=.#%78#DX5?BG/.5./4RP789%7*B%>F&"/.&6#;> &"Q/ "!#QP5?>!$&"/.# p1afio*rG!$(89'&)5./.#]/.59<Bfi78#'9;Bfi&6(89fiR&"/.0!$6 9;:.78&"BF(85?/.#%9SB%!$&"/.78Pm3!BN&2!4O.)78#@9%5Q!U#S,78(8&JI(U3$!$"=.7U&"/9;:.71/.78?9F#%9;!+9%7d !/.Pan{r9;:.7U/.5?/4RP789%7*B%>\&2/.&6#%9'&)(F#%78678(89%&65?/T5$DZ7*/3.&"Bfi5?/>F7*/49`!+(89%&65?/.#*K|#%&">,.67C78!4>,.675$DA!/ /"!#P5.>!$&"/P78#%(*Bfi&",.9%&65?/&6#F#;:.5$~A/T&"/Q &60?=BN7WqKCY :.7CP5?>!$&"/P78#%(*Bfi&"O^78#\!T,."!//.&"/.0,BN5?O.67*> Dn5?BG.(V:.5.,,7*Bfi# L afio*p4rABN5?O59NRO!O[TP5?>!$&"/KCY :.7P5?>!+&2/z:!$#9fi~ 5G#%9;!$9%7 3$!Bfi&"!O.678# pE!H/?=>F7*BN&)(*!+5?/.7d,54#%&69%&65?/ $ ~ &69;:CB%!/.0F7 P ] ] { ] !/.P!<,BN5?,5#'&)9'&)5./!$# # 8 * A!/.P5?/.7dfi * g *.KEY :.7GBfi5?O549&6# 9S:.7A5?/.6[C#%[.#%9%7*>!$07*/49!/.P1&69 :!$# 9fi~ 5]!$(89%&65?/.o#Kg , * KUY:.77 # V Uan!/.QP #K g , H 8r<!$(89%&65?/:!$#!T(85?/.P&69%&65?/!$ 7S78(89FP78#%(*Bfi&"O78PO4[!/T&JD R9S:.7*/4R78)#'75?,^7*B%!$9%5.BOa myr p &JD<fi * g *&6#G9;B'=.7]9S:.7*/ # 8 *y G&"/.(*Bfi7*!$#'78#<9;:.7O.65?(8,54#%&69%&65?/mO[z5?/.7786#%7C9;:.71O.65?(8,^5#%&69%&65?/&6#=/.(V:!4/.078PKQY :.71O!O[z&)#F9;:.7C5?/.6[7*/3.&"Bfi5?/>F7*/49!$07*/49!4/.P&69_:!+#E5?/.7G!$(89%&65?A/ Gb * nK 78(*!=.#%7M7*!$(8:!+07*/9 >F=.#%9E,7*BDn5?B%>78!$(89%6[F5?/.7G!$(89%&65?/!$9Z7*!$(8:<#'9%7*,d$9;:.7*Bfi7 !Bfi79fi~5fi5&"/9 !$(89%&65?/.#Aa # V `b * 4"r!/.PUa #K g , * -d Gb * "rK/.&69%&"!$66[ >y^ >> &6#!+#%#;=>F78P9'5<O7 9;B'=.7d9;:.7 Bfi5?O^59 &6#!$#'#;=>F78PF9%5<:.56P1!GO.)5.(8]!+9EuZ5#%&69%&65?/Pd!4/.P1&)9'# 9;!$#;@&6# 9%56&JDn9A&69A=,9%5uZ5#%&69%&65?/1^KY:.73$!Bfi&"!O.67 >y^ o>_(*!/O7M>!$P7 D!$6#%7AO4[<9;:.7MO!O[K :.7MO!OC[ Lc#E!$(89'&)5.A/ `b * .&)#/.5?/4RP789%7*B'>F&"/.&)#'9%&6(d!+#&69-5?/.6[H(85?/.#%9;B%!+&2/.# fi * g * O4[A9;:.77S78(89-78,Bfi78#%#%&65?/ fi * g 4Hfifi 0)-- -)>KyKCK< xKM+H>K+^""8XCy+&?+^X V'fi^ H[&fi^ H[8uCy+ X&?+^X V>y+C>K+^ 6<VCy+&^XVKaJ & V+KJV&60?=Bfi7<qp |G/f"!'#P5?>!$&"/CP78#%(*BN&2,.9'&)5./Kfi * g * } KCY :.=.#*d~A:.7*/ * g *&6#H9;B%=.7&"/z9;:.7F(*=B%Bfi7*/49<#%9;!+9%7d9;:.7F7S78(89<78,Bfi78#%#'&)5./5$D\G * P5?78#</.59G!,,.6[d!/.P * g *F(*!/T78&)9S:.7*BGO7F9;B%=.7H5?B D!$6#%7H&2/T9;:.7/.78.9A#'9;!$9%7KG/9;:.7F59S:.7*BG:!/.Pd&JD<fi * 4 g *&6#GD!$6#%7&"/9S:.7(*=B%Bfi7*/49G#%9;!$9'7d`b * 4+787*,.#<&69MD!+)#'7&"/9;:.7/.78.9<#%9;!+9%7K<Y :.A7 G * E!$(89'&)5./>\5?P786#]!4/!+#;,78(89H5$DE9;:.77*/43?&"Bfi5./>F7*/9</.549G(85?/9SBfi56678PO4[m9;:.7QBfi5?O549!$07*/49*d &"/9;:.&6#1(*!$#%7!O!O4[dAO4[m&69%#17S78(89%#5./fi * g *.K/9;:.7T78!4>,.67!O^5347dEfi * g *C#%9;![.#MD!$6#%7@~A:.7*/Q&69F:!$#]O^78(85?>F7@D!$6#%7d Bfi7S78(89'&2/.09;:!+9G9;:.7WBN5?O59G(*!4//.59#;,^5?/9S!/.785?=.#%6[UO7I?78PUO4[U!F:.&69A5$D 9;:.7<O!O4[d5?B!/[C59;:.7*B !$(89'&)5./1&"/19;:.7G7*/43.&2BN5?/>F7*/49*K|G/m| Bfi7*,Bfi78#%7*/49%&"/.09;:.71P5?>!$&"/&6##;:.5$~A/&"/ &60?=Bfi7KY:.71(*!$6(*=.2!+9%&65?/5$DA9;:.7U/.78.9#%9;!+9%7<3$!$"=.75$D $&"/9;:.A7 # 8 * !+(89%&65?/T#;:.5$~ #G9;:!$9G/?=>\7*Bfi&6(*!$E3$!Bfi&"!O.678#<(*!/O^7=,P!$9'78PO4[!/Q!4Bfi&69;:>F789%&6(78,Bfi78#%#%&65?/5?/9;:.7F(*=B%Bfi7*/49<#'9;!$9%7H3!Bfi&"!O.678#*KCY :.7=,^P!$9%778,Bfi78#%#%&65?/5$D $!/.PC9;:.7<=.#%7M5$D-9;:.7G&JDbR9;:.7*/4R786#%7<5.,7*B%!$9'5?BZD=Bfi9;:.7*BP7*>F5?/.#'9;B%!$9%7M9;:.7G!$P3!4/9;!+07A5+D=.#%&"/.078,.)&6(8&69Bfi7SDn7*Bfi7*/.(878#F9%5U(*=B'Bfi7*/9F#%9S!$9%7!/.P/.78?9F#%9;!+9%73!4Bfi&"!O.678#&"/Q7S78(89F78,BN78#%#%&65?/.#*K "!'#QP5.78#/.59Bfi78#%9SBfi&6(89 9;:.7<Bfi7*,Bfi78#'7*/9;!+9%&65?/1O[C7*/4Dn5?Bfi(8&"/.0W!\#%9;B%=.(89;=BN7G#%7*,!B%!$9'&2/.0@(*=B%Bfi7*/49 #%9;!$9'7`!/.P/.78?9#%9;!$9%778,Bfi78#%#%&65?/.#*K :.7&JD R9;:.7*/4R786#%75?,7*B%!+9%5?B:!$# O787*/F!$PP78PF9%5#;=,,5?BN9-(85?>@,.)78H(85?/.P&69%&65?/!$.7S78(89%#9;:!$9 5$Dn9%7*/C!Bfi7M7S(8&67*/9')[U!4/.PU/!$9;=B%!$66[1Bfi7*,Bfi78#%7*/49%78PU!$#A!H#%789 5$D /.78#%9%78PU&JDbR9;:.7*/4R786#%7<5.,7*B%!$9'5?Bfi#*KY:.7A78,.6&)(8&69<Bfi7*,BN78#%7*/9S!$9%&65?/@5$D-(85?/.#%9SB%!$&"/.78P1#%9;!$9'7 3!BN&2!4O.)78#M7*/!O.678#G!/[C/.5?/4RP789%7*B%>F&"/.&6#%9%&6(5?BMP789%7*B'>F&"/.&)#'9%&6(7S78(89<5+D_!/!$(89%&65?/9%5UO7BN7*,Bfi78#%7*/49%78Pd-!$#G9S:.7(85?/.#%9;B'!$&"/.78PQ3!BN&2!4O.)78#F(*!/O^7!$#%#'&)0./.78P19%5]!/4[F3!$"=.7H&2/@9;:.7G/.78?9 #%9S!$9%7M9;:!$9E#S!$9%&6#fiI78# 9;:.7M7S78(89Dn5?B%>F=."!K n9D=Bfi9;:.7*B9;=B%/.#5?=.99%5:!37<!\(8)7*!4B &"/9;=.&69%&637>F7*!4/.&2/.0d!$#9;:.7<!$(89%&65?/9;!$78# 9;:.71;BN78#;,5?/.#'&2O.&66&69fi[?T5$D#S,78(8&JDX[.&"/.019;:.73!+2=.78#M5$D 9;:.7G(85?/.#%9SB%!$&"/.78PU3$!Bfi&"!O.678#G&"/19;:.7</.78.9 #%9;!+9%7Kfifi0 Jrobot_worksfalsetrueG0G123pos&60?=Bfi7<pMY :.7|5$D 9;:.7WBN5?O59NRO!O[TP5?>!$&"/ab#%787C &60?=Bfi7qrKWY:.7*Bfi7&6#F5?/.7W,BN5?,5#'&)9'&)5./!$!/.P5?/.7Q/?=>\7*Bfi&6(*!$<#%9S!$9%7U3$!Bfi&"!O.6K7 p} * g *!/.P +?K Y:.7a # 8 * -d G* "rZ!/.PWa #<g , *y $-d G * "r.fi5&"/9 !$(89%&65?/.#E!4Bfi7EPB%!~A/~ &69;:G#'56&)PC!/.P<P!$#S:.78P!B%Bfi5$~ #d?Bfi78#;,^78(89%&63786[K.9S!$9%78#A>!4B%$78PC~ &69;:;nC!/.Pm;G!BN7G&2/.&69%&"!$ !/.P105.!$#%9S!$9%78#*K_5?>,!Bfi78P9%5C9;:.7!+(89%&65?/TP78#%(*Bfi&",.9%&65?/z2!4/.0?=!$07 an78Dn5?/.PQv$&JDn9%#%(8:.&69%d o*pprM!/.P<9;:!$9F!Bfi7C9;:.7C5?/.6[,Bfi&65?BH"!/.0?=!+078#=.#%78PDX5.B h ii RO!+#%78P,."!//.&"/.0abG&Q!/.85789\!+XKedopp&">!$9'9%& 789U!$nK"dGo*p4p!d oppOd oppr8."!'#&"/9;BN5?P=.(878#U!/ 78,.6&6(8&697*/3.&"Bfi5?/>F7*/49U>F5.P78XdM!>F=.69%&JR!$07*/49P78(85?>,^5#%&69%&65?/d !/.Pw/.=>F7*Bfi&6(*!$ #'9;!$9%7C3!BN&2!4O.)78#K9(*!/zD=Bfi9;:.7*BO^71#;:.5$~A/9;:!$9"!'#(*!/1O7F=.#%78PU9'5\>\5?P78-!4/[CP5?>!$&"/9;:!$9 (*!/UO^7<>F5.P78)678PO4[C<ab#%787G|G,,7*/.P&6U|Gr8KY:.7 (85?/.(*=B%Bfi7*/49 !$(89%&65?/.#&22/ "!'#m!Bfi7A!+#%#;=>F78PC9%5]O^7A#'[/.(8:Bfi5?/.5?=.#')[@78?78(*=.9%78P1!/.PC9%5<:!37I.78P!/.Pz78=!$ P=B'!$9%&65?/K|07*/.7*B'!$Bfi7*,BN78#%7*/9S!$9%&65?/Q!$665$~ &"/.0,!BN9%&"!$6)[z5347*Bfi"!,,.&"/.0!+(89%&65?/.#!/.P!+(89%&65?/.# ~ &69;:FP&J7*Bfi7*/49 P=B%!$9%&65?/.# :!$#EO787*/!354&)P78Pd.!$# &69Bfi78=.&"Bfi78#>\5?Bfi7 (85?>,.678F9%7*>,^5?B%!$,."!//.&"/.0wab#%787F7Kc0K"dh Nf+- 5?B f-g f$Z dw =B%BN&)7vYZ!$9%7d o*pp^o $7837*B<v &6(8:!BfiP#*do*p>p 1?rKG=Bfi5&"/49!$(89%&65?/@Bfi7*,Bfi78#%7*/49;!$9%&65?/F:!$# >F5?Bfi7 Bfi78#'7*>]O."!/.(87M~ &69;:FF!B'!$v G78JDX5./.Pdo*pp4r !/.Pan&2=/.(8:.&606&"!qv $&JDX#'(V:.&69%doppr8d4~A:.7*Bfi7M#%789%#5$D-!$(89%&65?/.# !Bfi7G,7*BDn5?B%>\78PW!$97*!$(8:9%&">F7M#%9%7*,K /(85?/49;B%!$#'99'5]9S:.78#%7]!,,BN5?!$(8:.78#*d.9;:.5?=.0?:d~ 7<>F5.P78->F=.69%&JR!$07*/49AP5.>!$&"/.#*K|G/@&2>@,5?Bfi9S!/9 &6#%#;=.7G9'5\!+PPBfi78#%# ~A:.7*/C&"/9SBfi5?P=.(8&"/.0(85./.(*=B%Bfi7*/49_!+(89%&65?/.# &6# #%[/.7*BN0789%&6(G7S78(89%#O789fi~ 787*/@#%&">]=.69;!4/.785?=.#%6[78.78(*=.9%&"/.0!$(89%&65?/.#GOa $&"/.0?!BfiP1v &6(V:!BNP#*do*pprK|(85?>>F5./F78!4>,.675$DP78#%9;B'=.(89%&637 #%[/.7*Bfi0789%&6(M7S78(89%# &6#~M:.7*/F9fi~5G5.B >F5?Bfi7A!+(89%&65?/.#Bfi78=.&"Bfi7M78?(8"=.#%&637G=.#%7 5$D-!#%&"/.067Bfi78#%5.=Bfi(87G5?B ~A:.7*/9fi~5!$(89'&)5./.#A:!37G&"/.(85?/.#%&6#%9%7*/49A7S78(89%#)&"$7 $ } j@!/.P $ } j{^K7/ "!#!$(89%&65?/.#M(*!//.59GO7,^7*BDX5.B%>F78PU(85./.(*=B%Bfi7*/49%6[U&"/T9;:.7GDn5665~&2/.019fi~ 5(85?/.P&69%&65?/.^# p<o*r9;:.78[F:!*347E&"/.(85?/.#%&6#%9%7*/49E7S78(89%#d$5?B {r9;:.78[<(85?/.#'9;B%!$&"/F!/H5$37*Bfi"!,,.&"/.0G#%789 5$D#%9;!$9%73!BN&2!4O.)78#KEY :.7IBfi#%9(85?/.P&69%&65?/C&6#EP=.7M9%5<9S:.7 D!+(89E9;:!$9#%9;!$9'7_./.5$~ 678P07M&)# 78,Bfi78#%#%78PC&"/!H>F5?/.59'5?/.&6( 650&6(G9;:!$9(*!//.59FBfi7*,Bfi78#%7*/49<&"/.(85?/.#'&)#'9%7*/9C?/.5$~ 678P07K1Y :.7#'78(85?/.P(85./.P&)9'&)5./m!$PPBfi78#%#%78#H9;:.7,Bfi5.O.)7*> 5$D#;:!BN&2/.0Bfi78#'5?=Bfi(878#*K 5?/.#'&)P7*BDn5?B 78!>,.67M9fi~5F!$07*/49%# 9;Bfi[.&"/.0F9%5<PBfi&"/F9S:.7A#;!4>F7 0"!$#%#5$D~ !$9%7*B8KbDF5?/.6[9;:.7TIBfi#%91(85?/.P&69%&65?/P7SI/.78P}&"/9'7*BDX7*BN&2/.0w!+(89%&65?/.#*dGO59S:!+07*/9'#(85?=.6P}#%&">F=.)9S!/.785?=.#%6[7*>,.9fi[9;:.7M0"!$#%#d?!$# 9;:.7M7S78(89E 6 , nF5$D-9S:.7A9fi~ 5<!$(89%&65?/.# ~ 5?=.6PUO7M(85?/.#%&6#%9%7*/49*K x&69;:@9;:.7#%78(85?/.P@(85?/.P&69%&65?/!$PP78Pd49;:.78#%7A!$(89'&)5./.#E!Bfi7 &"/9%7*BDX7*Bfi&"/.0F!/.PF(*!//.59EO7M,7*BDn5?B%>F78PF(85?/.(*=B'Bfi7*/9')[KY:.7U(*=B%Bfi7*/49137*Bfi#'&)5./w5+5/"!#Q5?/.6[!35&6P#P78#'9;B%=.(89%&637#%[/.7*Bfi0789%&6(7S78(89%#*K n9CP5.78#U/.59&"/.(8"=.P7Q~ !*[.#5$D<Bfi7*,Bfi78#'7*/9%&"/.0(85?/.#%9;B'=.(89%&637U#%[/.7*Bfi0789'&)(T7S78(89%#O789fi~ 787*/m#'&2>F=.69;!/.785.=.#U!$(89%&"/.0!$07*/49%#*KH|(85?/.#%9;B'=.(89%&637#%[/.7*Bfi0789%&6(7S78(89G&6#F&6)"=.#%9SB%!$9%78P&"/Q !B%!$E!/.PQ78Dn5?/.Pafio*pp4r8d~A:.7*Bfi7!/!$07*/49<#;,.&666##'5?=,DBfi5?> !UO^5~~M:.7*/Q9;BN[?&"/.09%5U6&JDn9&69\=,z~ &69;:z5?/.71:!/.PdEO=.9/.59<~M:.7*/6&JDX9%&"/.0&69 =,~ &69;:CO549;::!/.P#*K/ !/.PF9;:.&6# &"/.PC5$D#%[/.7*BN0789%&6(A7S78(89%#(*!/O^7`Bfi7*,BN78#%7*/9'78Pfifi0)-- -)O4[78,.6&)(8&69%6[Q#'9;!$9%&"/.09;:.7H7S78(89 5$DE!F(85?>,^5?=/.PQ!$(89%&65?/K|#%&">F&6"!BG!,,BN5?!$(8:C(85?=.6PO^7]=.#%78PT&"/"!'#mO=.9 &6#A(*=B'Bfi7*/9')[1/.59 #S=,,5?Bfi9'78PKC6{5.B%>!$66[d!/f"!#P78#%(*Bfi&",.9%&65?/T&6#A!R9;=,.675!ja]J]A]* ]]] ]r8d~A:.7*Bfi7Kp&6#!zI/.&)9'7Q#%78915$D<#%9S!$9%73!BN&2!4O.)78#T(85?>,Bfi&6#%78P 5$D\!I/.&69%7#'7895$D,Bfi5?,54#%&69%&65?/!$3$!Bfi&"!O.678#*d) ?d!/.P!HI/.&69%7<#%7895$D/.=>F7*Bfi&6(*!$Z3!Bfi&"!O.678#*d K&)#G!HI/.&69%7d/.5?/.7*>,.9fi[C#%789 5$D #%[.#%9%7*>!$07*/49%#*K&6#`!]I/.&)9'7<#%789 5+D-7*/3.&"Bfi5?/>F7*/49G!$07*/49%#*Kdj)AM&)#!T#%789H5$DA!$(89%&65?/zP78#%(*Bfi&",.9%&65?/.#a ]M] r ~M:.7*Bfi7\&)#F9;:.7C#%789F5$D #%9;!+9%73!4Bfi&"!O.678#(85?/4R#%9;B%!$&"/.78PUO4[9;:.7<!$(89'&)5./d &6#A!F,Bfi78(85?/.P&69%&65?/T#%9S!$9%7 Dn5?B%>F=."!H&2/9;:.7M#%789on !/.P &6#A!/7S78(89DX5.B%>F=.2!H&"/C9;:.7M#%789\n K :.=.#<a ] ] rffZM* fi{n n K :.7A#'789%#n !/.7P n !4Bfi7GP7SI/.78PQO7865$~<Kp+.'{ &)#G!HD=/.(89%&65?/U>@!,,.&"/.0W!$047*/9%#FaO?'j r 9%5F9;:.78&"BM!$(89%&65?/.#*KE78(*!=.#'7!/1!$(89%&65?/1O^78)5./.0#A9'578!+(89%6[5?/.7F!$07*/49*d >F=.#%9 #;!+9%&6#fiDX[C9;:.7MDn5665~&2/.0C(85?/.P&69%&65?/.#pZr&jUM*47n'7n#] (Z? _ #"j#!(r $# %r j#&( &&6# 9;:.7H&"/.&)9'&2!+-(85?/.P&69%&65?/K&6#A9S:.7G05?!$(85?/.P&69%&65?/K5.BE!G3$!$6&6PUP5?>!+&2/@P78#%(*Bfi&",.9%&65?/d^~7MBfi78=.&"Bfi7M9;:!$9 !$(89%&65?/.# 5$D#%[.#%9%7*>!$07*/49%#!BN7 &"/.P7*,7*/.P7*/49A5$D!$(89%&65?/.#5$D7*/43.&2BN5?/>F7*/49`!$047*/9%#^pa{?r)${(raO{.r&j#&*{+]aO*r~A:.7*Bfi7*a{?r &6# 9S:.7<#%789 5+D-(85?/.#%9;B'!$&"/.78PU3!4Bfi&"!O.678#G5$D!$(89%&65?/f{KY:.7G#%789 5$DZDX5.B%>F=.2!+#n &6#M(85?/.#%9;B'=.(89%78PCDBN5?>9;:.7GDn5665~&2/.01!$",:!O^789A5$DZ#%[>FO^56#p|I/.&69%7H#%789 5$D (*=B%Bfi7*/49 #%9;!+9%7IrC!/.PU/.78.9A#'9;!$9%7rK}3$!Bfi&"!O.678#*d~A:.7*Bfi7r:.7</!$9;=B%!+/.=>FO7*Bfi#.-QK:.7<!Bfi&69;:>F789'&)(H5?,7*B'!$9%5?Bfi#0/Fd21Gd3d54]!/.P:.7<Bfi78"!$9%&65?/5?,7*B%!+9%5?Bfi#.6Gd27Gd28Gd:9Gdj !/.P Fj! K:.7<O5.567*!/15.,7*B%!$9'5?Bfi#wk dd<;!/.PZKfi^K] r<},7dFKfi0 J:.7G#;,78(8&"!$Z#%[>FO56#Fn( , ^d ,,!Bfi7*/49;:.78#%78#A!/.P1(85.>>!K:.7G#%7895$D!4Bfi&69;:>F789%&6(<78,Bfi78#%#'&)5./.#A&6#M(85?/.#%9;B%=.(89'78PCDBfi5.>9S:.7GDn56)5$~ &"/.01B%=.678#po4K\N 37*Bfi[C/?=>F7*BN&)(*!+-#%9S!$9%7M3!Bfi&"!O.675r(<&6#A!4/1!Bfi&69;:>F789%&6(H78,Bfi78#'#%&65?/K^{ K |/!$9S=B%!$/?=>FO^7*B &6#A!/1!4Bfi&69;:>F789%&6(<78,Bfi78#%#'&)5./K^K bD # !4/.P ( !Bfi7F!Bfi&69;:>F789'&)(H78,BN78#%#%&65?/.#G!/.P>= &6#G!/1!Bfi&69;:>F789'&)(H5?,7*B'!$9%5?B8d^9;:.7*/&)#M!/1!Bfi&69;:>F789'&)(H78,BN78#%#%&65?/K&"/!$66[d9;:.7G#%789 5$DZDn5?B%>F=."!$#on4o4KGn(=(&)#07*/.7*B%!$9%78PO[C9;:.7<B%=.678#p, !Bfi7MDn5?B%>F=."!$#*K{^K uEBfi5?,^5#%&69%&65?/!$#%9;!$9%7M3!4Bfi&"!O.678#r(7,!/.P#^K bD # !/.P (!HDX5.B%>F=.2!^K<!4Bfi7MDX5.B%>F=.2!+#*K!Bfi7]!4Bfi&69;:>F789%&6(F78,BN78#%#%&65?/.#<!/.P &)#<!CBfi78"!$9%&65?/T5?,^7*B%!$9%5.B8d9S:.7*/1K bDdi # di ( !/.Pvi %!/.PaOi # (>] %!Bfi7DX5?B'>]=."!$#d#%51!Bfi7ar8K#rdaOi# wZi (r8d aOi# k(r8d aOi#(#r8d-ai# ;&)#((ru !Bfi7*/49;:.78#%78#M:!*347<9;:.78&"BM=.#;=!$ >F7*!/.&"/.0U!/.PU5.,7*B%!$9'5?Bfi#A:!37G9;:.78&"BG=.#;=!+-,Bfi&65?Bfi&69fi[!/.P!$#%#%5.(8&"!R9%&63.&)9fi[C~ &69;:19S:.7G&DbR9;:.7*/4R786#%75.,7*B%!$9'5?BH0&637*/)5$~ 78#%9 ,Bfi&65?Bfi&69fi[Knfin&6#W!#S=O.#%7895+DA9;:.71Dn5?B%>F=."!$#F5?/.6[wBfi7SDn7*B%Bfi&"/.09%5(*=B%Bfi7*/49#%9S!$9%7C3!Bfi&"!O.678#*K:.78#%7DX5.B%>F=.2!+#_!4Bfi7G(*!$6678P8 , 8 ( 6 VK@?W|,A[W{CBED|M)5+D9;:.7 #%[>FO56#E&"/F9S:.7_!+2,:!4O789E5+DDn5?B%>F=."!$#:!37 9;:.78&"B=.#;=!$>F7*!/.&"/.0<~ &69;:\9;:.7 &DbR9;:.7*/4R786#%75?,^7*B%!$9%5?B # (>] % O78&"/.01!/1!OOBfi783.&"!$9%&65?/WDX5?BGaOi #Mk ( rwQa #'k % r8KXN!$(V:1/.=>F7*Bfi&6(*!$#%9;!$9%73!4Bfi&"!O.67 r+7 <:!+#A!HI/.&69%7]B'!/.07S-aOr.r&jqP ] ]CFGFHF] I>.d?~M:.7*Bfi7I*6PKY:.7Dn5?B%>!+E#%7*>!4/9%&6(8#F5$DA!TP5?>!$&"/zP78#%(*Bfi&",.9%&65?/ ! j J]J]5] * ]]] <r&6#0&637*/z&"/9%7*B%>\# 5$D!/T L| KpM|ON{2B`CBV{=QP -R ST7ge , , Xy?(*nb b" , FV ,`/ , ,W"8n nEbb , (.n Fne VU O(-,GW K jaEX ]Y][Z r W" , b (. *4 ( , W Z p\X] _{ ^ e ,a`V ,/9;:.7TDX5665$~ &"/.0(85?/.#%9;B%=.(89'&)5./w5+DcKdA~ 778,Bfi78#%#U9S:.7/.78.9U#'9;!$9%7D=/.(89%&65?/}!$#U!9;B'!/.#%&69%&65?/Bfi78"!$9%&65?/&K $78:9 dP7*/.59%79;:.7E#%7895$D^O^5?54)7*!4/<3$!$"=.78#m( , ] n , .K=BN9;:.7*B8d)789Z9;:.7Mye , S"8nby(?*nb>ep"fgd!$#%#%5.(8&"!$9%78P9%51!C#%78*9 e]hifO7P7SI/.78PmO>[ eFOa [r j Oa [jkeAr8K % G&637*/!/l| K ~ 7 P7SI/.7 &69%#An ^Vbnb ,) n F|ihmXn .X!+#!M#%7895$D9;BN&2,.678# ~ &69;:F(8:!B%!+(89%7*Bfi&6#%9%&6(D=/.(89%&65?/ |]a ]o] } &r jOa }Z Oa ]o r'r8KY:.7E#%789Z5$D#%9;!+9%782# X5$D K 78=!$6# 9;:.7 #%789-5+D^!$6,^5#%#%&"O.67 3!4Bfi&"!O.67_!+#%#%&60?/>F7*/49%:# X ja )d%r Wap-Tr8K-Y:.7A&"/,=.9 5$qK &)#9;:.7 #%7895$D?fi5&"/49!$(89%&65?/.#5$D#%[.#%9%7*> !$07*/49%#EBfi7*,BN78#%7*/9'78Pr,s-Sn[ tb ';finnnb"- *fib'<$;b$ n; Z$; ESb$ nfiRfi)-- -)0!$#G#%789'#*KY :!$9H&)#d&{ # ] { ( ]CFHFCFH] {,u vwuff>&JDA!/.PQ5?/.6[&JD]aO{ # ] { ( ]CFHFCF] {u v\ucrxQyz z P7*/.549%78# 9;:.7F/?=>FO7*B_5$D7867*>F7*/49%#M&"/Kxz7GP7SI/.7<9;:.7G9;B'!/.#%&69%&65?/UBN782!+9%&65?/ |Up_X) VX {dw5+DffK O[Cp|]a ]o] } r&jU_|}~v-rd~A:.7*Bfi7VaO ] | ] } r ]_[o hj| k~A:.7*Bfi7 opX]~m>Xd&6#G9;:.7<9SB%!/.#%&69%&65?/Bfi78"!$9%&65?/Dn5?B5&"/49`!+(89%&65?/.#*~m5$DO^59;:T#%[.#%9%7*> !/.P7*/43?&"Bfi5?/>\7*/9 !$07*/49%#*K-Y:.7 78?&6#%9'7*/9%&"!$=!/49%&JI(*!$9%&65?/>@!$78# 9;:.7A!+(89%&65?/.# 5$D7*/43.&2BN5?/>F7*/49!$07*/49%#=/.(85?/49;Bfi56"!O.67d#'&2/.(87.|]aO ][oH] } r&)# 9SB%=.7d$&JD9;:.7*Bfi778?&6#%9'# #%5?>F75&"/49!+(89%&65?/H5$D7*/3.&"Bfi5?/>F7*/49E!$07*/49%##;=.(V:9;:!$9 9;:.7H(85?>FO.&2/.78P@54&2/49A!$(89'&)5./ |)j >!$78# 8Oa ] | ] y})r_9;B%=.7KY:.719;B%!/.#%&69%&65?/Bfi78"!$9%&65?/ F&6#W!(85?/N=/.(89'&)5./m5$DG9;:Bfi787UBfi78"!$9'&)5./.# eH!/.P &8Oa ] | ] }r jeFOa | y}rOa | y}r|r8K &)347*/U!/1!$(89'&)5.Z/{j8r?F!*(='BfiB*7/9%#;9$!%97F!./UP./87.9%#;9!$9%7] ]k] ]kf]M]} d.678\9 ~q.Oa *r !/.P Oa ] } rEP7*/.549%7A9S:.7A3$!$"=.7<5$D-9S:.7`,Bfi78(85./.P&)9'&)5./@DX5?B'>]=."! !/.PC7S78(89Dn5?B%>F=."!+5J{dBfi78#;,^78(89%&63786[Ke=pX)>~kVX{d&6#A9;:.7*/CP7SI/.78PQO4[ pFaOr j] | ] }&e~:?aOr k aO ] } r _HP7SI/.78#9S:.7(85?/.#'9;B%!$&"/49%#<5?/z9;:.7C(*=B%Bfi7*/49<#%9S!$9%7!/.Pm/.78.9]#'9;!$9%75+DE54&2/49]!+(89%&65?/.#*KeD=BN9;:.7*B7*/.#;=Bfi78#Z9;:!$9Z!$(89%&65?/.#Z~ &69;:M&"/.(85?/.#%&6#%9%7*/49 7S78(89%#(*!//.59ZO7 ,7*BDn5?B%>\78PG(85?/.(*=B%Bfi7*/49%6[d!$#eBfi78P=.(878#9%5FD!$6#%7H&D !/4[W,!$&"BM5$DE!$(89%&65?/.#M&"/!M54&2/49G!$(89%&65?/U:!+#A&"/.(85?/.#%&6#%9'7*/9G7S78(89%#*K :.=.#*5d e !$6#%5#'9;!$9%78#9;:.7MIBfi#'9 (85?/.P&69%&65?/mab#%787<.78(89%&65?/ 1.rDX5.B !35&6P&"/.0&"/9%7*BDX7*Bfi7*/.(87O^789~ 787*/C(85?/.(*=B'Bfi7*/9M!$(89%&65?/.#*KpX)~kX d&)#<!]DB%!4>F7<Bfi78"!$9%&65?/T7*/.#;=Bfi&"/.019;:!+9A=/.(85?/.#%9;B'!$&"/.78PU3!4Bfi&"!O.678#<>!$&"/9S!$&"/9;:.78&"B3!$"=.7K $78"9 Oa {.rP7*/.59%7<9S:.7G#%789 5$D (85?/.#%9;B'!$&"/.78PU3!4Bfi&"!O.678#A5$DE!$(89%&65?/ {K xQ7H9;:.7*/1:!3K7 peaOr j] | ] }&~A:.7*Bfi7.j#HaOrj=r } r]2aO{.r8Kp~d7*/.#;=Bfi78#G9;:!+9 (85?/.(*=B%Bfi7*/49<!$(89%&65?/.#M(85?/.#%9SB%!$&"/U!C/.5?/T5$37*Bfi"!,,.&"/.01#%789M5$DE3!BN&2!4O.)78#!/.P19;:.=.# #%9S!$9%78# 9;:.7G#'78(85?/.PU(85?/.P&69%&65?/WDX5.B !35&6P&"/.0&"/9%7*BDX7*Bfi7*/.(87O^789~ 787*/C(85?/.(*=B'Bfi7*/9M!$(89%&65?/.#p|rj~A:.7*Bfi70|-(TVWXWP7*/.59'78# 9;:.7G#%789aO{#y] { (476 < 6 6 +&r z {+ HaO{ # r%$VaO{ ( rXj&_r(|(|#] { (k'Uv6 -6]{ #.jU{ ( ?K!Y5O=.&6)PQ!/ h|Bfi7*,Bfi78#%7*/49%&"/.0C9;:.7G9;B%!/.#'&)9'&)5./UBfi78"!$9%&65?/2|]aO ]oH] } r 5$D 9;:.7]|}5$D!\P5?>!$&"/P78#%(*Bfi&",.9%&65?/!ja J]&]5] M8 ]]] <r8d$~ 7<>F=.#%9 P7SI/.7!F#'789 5$DEO5.567*!/T3!Bfi&"!O.678#G9%5CBfi7*,Bfi7SR#%7*/49G9;:.7F(*=B%Bfi7*/49G#%9;!$9')7 d9;:.7M54&2/49<!$(89%&65?/T&"/,=.9 d!/.PQ9;:.7/.78.9G#%9;!$9'7) } KH|A#G&"/Q.78(89'&)5./71K{~ 7HIBfi#%9GO=.&)6Pm!C9;B%!/.#'&)9'&)5./BN782!+9%&65?/T~ &69;:9;:.7Mfi5&"/9G!$(89%&65?/.#H5$DO59S:U#'[?#%9'7*>y!/.P7*/3.&"Bfi5?/>F7*/49!$07*/49%# !$# &2/,=.9M!/.P9S:.7*/1Bfi78P=.(87G&699'5\!9;B%!/.#'&)9'&)5./1Bfi78"!$9%&65?/@~ &69;:C5?/.6[<fi5&"/49_!+(89%&65?/.# 5$D#%[?#'9%7*>!$07*/49%# !$# &"/,=.9*K[ pA!+#%#;=>F7!$(89%&65?7/ {&)#H&6P7*/9%&JI78P5&"/9<!$(89'&)5./U&"/,=.9%#F!Bfi7Bfi7*,Bfi78#'7*/9%78Pz&2/T9;:.7\DX5665$~ &"/.01~ !*CO4[!/?=>FO7*B !/.P(*!/O7Q,7*BDX5?B'>F78PO4[w!$07*/4>9 EK { &6#19;:.7*/P7SI/.78P9%5mO7z9;:.7Q!$(89%&65?/RKfifi 0 J5$D`!+07*/9Ed&JDA9;:.7U/.=>FO7*BH78,BN78#%#%78Pm&"/mO.&"/!Bfi[mO[!T#%789F5$DGO5.567*!/3!BN&2!4O.)78#Ve dE=.#%78Pm9%5Bfi7*,Bfi78#'7*/9 9S:.7]!$(89%&65?/.#5$ffd&6#M78=!$9%5 KEuBfi5?,^5#%&69%&65?/!$#'9;!$9%7G3$!Bfi&"!O.678#`!4Bfi7<Bfi7*,Bfi78#%7*/49%78PO[1!#%&"/.067O^5?567*!/3!Bfi&"!O.67dZ~A:.&667/.=>F7*Bfi&6(*!$E#'9;!$9%7H3!Bfi&"!O.678#F!Bfi7Bfi7*,Bfi78#%7*/49%78P&"/QO.&"/!Bfi[QO4[!@#%7895$DEO5.567*!/13$!Bfi&"!O.678#*K$789 e ]^___] e !/.jP e" ]_^__] e C P7*/.59%7F#%789%#M5$DO^5?567*!/T3$!Bfi&"!O.678#<=.#%78P9%51Bfi7*,Bfi78#%7*/499;:.754&2/49A!$(89'&)5./5$DZ7*/3.&"Bfi5?/>F7*/49A!/.PC#%[.#%9%7*>!$07*/49%#*KZ=BN9;:.7*B8d.6789 [,I !/.P [ } P7*/.59'7A9;:.7 $ eO5.567*!/3$!Bfi&"!O.67U=.#%78P9%5Bfi7*,BN78#%7*/9H#%9;!$9'73!BN&2!4O.)Z7 rU&2/9S:.7(*=B%Bfi7*/49!/.P/.78.9<#'9;!$9%7K:.7GO5.567*!/@3!Bfi&"!O.678#A!4Bfi7 5?BfiP7*BN78P~ &69;:\9;:.7A&"/,=.9 3!4Bfi&"!O.678# IBfi#%9*d+DX54)65$~ 78PWO4[!/@&2/49%7*Bfi67*!3.&2/.05$D 9;:.7<O5.567*!/3!Bfi&"!O.678#G5$D (*=B%Bfi7*/49 #%9S!$9%7G!/.PU/.78?9M#%9;!+9%7A3$!Bfi&"!O.678# peeFCFCF[ I#[ I#[ } I#0#FHFCFe[wI[ }FCFCF[ } I#eFCFCFO[CFHFCF[ }]~A:.7*Bfi7V &6#]9S:.7/?=>FO^7*B<5+DO^5?54)7*!4/Q3$!Bfi&"!O.678#=.#%78P9%5UBfi7*,Bfi78#'7*/9F#%9;!+9%7F3!Bfi&"!O.67ZrH !/.PQY&)#78=!$9%5 z J z K :.7 (85./.#%9;B%=.(89%&65?/\5$D |m&6# =.&69%7#%&">F&62!4BE9'5G9;:.7 (85?/.#%9SB%=.(89%&65?/H5$DM|m&"/.78(89%&65?/1K{K|G/ h ii Bfi7*,BN78#%7*/9'&2/.0m!650&6(*!$M78,Bfi78#%#%&65?/ &)#1O=.&6691&2/9;:.7T#%9;!/.P!4BfiPm~ !*[an Bfi[?!/49*dMo*prK|GBfi&69;:>F789%&6(<78,Bfi78#%#'&)5./.#`!BN7`Bfi7*,BN78#%7*/9'78P!$# 6&6#%9%#5$D h ii #AP7SI/.&"/.0C9;:.7G(85?B%BN78#;,5?/.P&"/.01O.&"/!Bfi[/.=>]O^7*B8KEY :.78[1(854)"!,.#'7<9%5#'&2/.04)7 h ii #M~A:.7*/UBN782!+9%78PO4[W!4Bfi&69;:>F789%&6(]BN782!+9%&65?/.#*KY5UO=.&66P!/ h ii eP7SI/.&"/.0Q9;:.7@(85?/.#%9;B%!+&2/49%#H5$D9;:.7Hfi5&"/9F!$(89%&65?/.#*d~7C/.7878P9%5Bfi7SDn7*BG9%59;:.7H3!$"=.78#G5$DE9;:.7FO5.567*!/T3!BN&2!4O.)78#<BN7*,Bfi78#%7*/49%&"/.019;:.7]!$(89'&)5./.#*K $789 -r O^7F9;:.7GD=/.(89%&65?/T9;:!$9>!,.#G!/!$07*/4.9 m9%5C9;:.7<3$!$"=.75$DE9;:.7O^5?567*!/3!Bfi&"!O.678#<Bfi7*,BN78#%7*/9'&2/.0T&69%#`!+(89%&65?/!/.P)789 {?rO7G9S:.7G&)P7*/49%&JI7*B3!$"=.7<5+D!$(89'&)5.%/ {K =BN9;:.7*B 6789 ~ Oa {.r !/.P Oa {.rP7*/.59%7 h ii Bfi7*,Bfi78#'7*/9;!+9%&65?/.#5$D 9;:.7<,Bfi78(85?/.P&69%&65?/!/.P17S78(89 Dn5?B%>F=."!F5$DE!/1!$(89'&)5./ {K e &6#A9S:.7*/10&637*/1O4[ pe:j?-ra-r&j=aO{?r&~aO{?r kOa {.r _jZ{+59%7G9;:!$9 6 50&6(*!$Z5?,^7*B%!$9%5?BN#_/.5$~P7*/.549%7G9;:.7G(85?B%BN78#;,5?/.P&"/.0 h ii 5?,^7*B%!$9%5?BN#*K|G/ h ii Bfi7*,Bfi78#%7*/49%&"/.0C9;:.7MDB'!>F7<Bfi78"!$9%&65?/ }(V:!4/.078#A&"/T!F#%&">F&6"!B ~ ![CpUjv?-raZr'j=aO{?r&r>>3 aO{.r%r%r }I jUaIC]kZ{+~A:.7*Bfi7>a{?r&)#@9;:.71#%789F5$DA(85?/.#'9;B%!$&"/.78Pm3!4Bfi&"!O.678#5$DG!$(89%&65?/Q{Q!/.PaIZj y}I 78,Bfi78#%#%78#@9;:!$9!$6(*=B%Bfi7*/49A!/.PU/.78.9 #%9S!$9%7<O5.567*!/3!Bfi&"!O.678#GBfi7*,Bfi78#%7*/49%&"/.0 r!Bfi7<,!$&"Bfi~&)#'7<78=!$nKY :.7G78,Bfi78#%#'&)5./r>3 Oa {?r783$!$"=!$9%78# 9%v5 ( , 5?.B n , !/.P1&6#ABN7*,Bfi78#%7*/49%78PO4[9;:.7 h ii DX5?B ( , 5?.B n , KEY :.7!$(89%&65?/C&"/49%7*BDn7*Bfi7*/.(87F(85?/.#%9;B'!$&"/9 &)#M0&637*/1O4[pja{# ] ( r{# ] ( r>a # ] (# &r jUaO{ # &r(rRR( rjUaO{ ( r k!fi0a{r# ] ()-- -)# r&jUaO{ # r&( r(r>a # ] (#] { (jUaO{ ( r ]!r~A:.7*Bfi7*a #] ( r&jqaO{ #y] { ( r z aO{ #] { ( r# r ( r k aO{ # r2$VaO{ ( r j#&?K!&"/!$66[U9S:.7 h ii Bfi7*,Bfi78#%7*/49%&"/.0C9;:.7G9;B%!/.#'&)9'&)5./UBfi78"!$9%&65?/ |6&#9.:<78(?5/%=/.(89%&65?/15+D <e!/.P~6&;91:$!8(%96&?5/3!fiB"&!.687#$5;9.:G7*7/.3"&fiB?5/F>*74/G9$!0*74/%9#87?6&%#%9*74/%9"&$!66U[=!4/%9J&87Pp| jU\e ]HFCFCF] e >_ e k'Y,^CBaCB)i{B{ Q_W|jW{VCBaCBV{k_xH|-CB)i{:.7<!$605.Bfi&69;:>F# ~ 7<=.#%7MDn5?B07*/.7*B%!$9'&2/.01=/.&637*Bfi#;!+-,."!/.#A!$6(85?/.#'&)#'9A5$D !O!$(8~ !BfiPC#%7*!BN(V:@DBfi5?>9;:.7M#%9;!$9'78#E#;!$9'&)#NDX[.&"/.0]9S:.7A05.!$(85?/.P&69%&65?/C9'5<9;:.7M#%9;!$9'78#E#;!$9'&)#NDX[.&"/.09;:.7M&2/.&69%&"!$(85?/.P&69%&65?/KXNE>@,.&2BR&6(*!$E#'9;=.P&678#<&"/>\5?P78(V:.78(8&"/.0:!*347#;:.5$~A/T9;:!$9G9;:.7C>F5#%9G(85?>@,.)78z5?,7*B%!+9%&65?/TDn5?BM9;:.&6#]4&"/.P5$D !$605?Bfi&69;:>F#F/.5?B%>!+)6[&6#<9%5CI/.Pz9;:.7 , F , 5$D !C#%789<5$D 3?&6#%&69%78P#%9;!+9%78# J !4/N!/789<!$nK"do*pp4qr8KM|ON\| , n+]K, EG8 , > z .VX k{2B`CBV{?Px|\BA, F,Je<e,jyaEX]Y][Z r}Z aO [Y]]r, @V ,_ }JbJhXWe,|w,BN78&2>@!$07E&6#Z#;!$&6PF9%5 78.&6#%9*d$&JD&69Z&)# /.5?/.7*>,.9fi[K 59%7E9S:!$9#%9;!+9%78#-!$"Bfi7*!$P[<O^7865?/.0&"/.0<9%5 J (*!/!$6#%5FO7A!G,!BN9 5$D9;:.7 ,Bfi78&">!$07M5$D J KZ|A#%#S=>F7 9;:!$9 9S:.7 #%789E5+D3.&)#'&)9'78P#%9S!$9%78# !Bfi7ABfi7*,BN78#%7*/9'78PO[!/ h ii 78,Bfi78#%#%&65?/ J 5./1/.78?9M#%9;!$9'7A3$!Bfi&"!O.678#<!/.P19;:!$9dDn5?B &69%7*B'!$9%&65?/1,=B%,^5#%78#*d~ 7A~ !/49 9%507*/.7*B%!+9%7A9S:.7<,Bfi78&">!$07 ~ !$6#%578,Bfi78#%#%78P1&"/1/.78.9 #%9;!$9%7M3!4Bfi&"!O.678#*KE5?B !F>F5?/.56&69;:.&6(<9;B'!/.#%&69%&65?/Bfi78"!$9%&65?/ |7 p~ 7G9;:.7*/C(*!$6(*=."!$9%KjaO[~j} _} _ | krG[\3:[ <}J~A:.7*Bfi7 d"[!4/.P[/}-P7*/.549%7F&2/,=.9d(*=B%Bfi7*/49M#%9;!$9%7F!/.PQ/.78?9G#'9;!$9%7H3!Bfi&"!O.678#*dZ!/.Pm[,3:[} P7*/.549%78#9;:.7F#;=O.#%9'&)9S=.9%&65?/T5$DE(*=B%Bfi7*/49A#'9;!$9%7H3!Bfi&"!O.678#H~ &69;:/.78?9M#%9;!$9'7<3!4Bfi&"!O.678#*K<Y :.7F#%789M78,BN78#%#%78PQO[7 AO^7865?/.0# 9%5H9;:.7G,Bfi78&">!$07 5+D J !/.P(85?/.#%&6#%9'#E5$D#%9;!+9%7 &"/,=.9A,!$&"Bfi#<Oa ]o rdDX5?B~A:.&6(V:@9;:.7M#%9;!$9'9;:.7 &"/,=.9 >![H(*!=.#%7A!M9;B'!/.#%&69%&65?/HDBfi5?> 9%5<!M#%9;!+9%7 &2/ J K :.7&2/,=.9 5$D!/|Bfi7*,BN78#%7*/9'&2/.0!Q,."!//.&"/.0wP5.>!$&"/m&6#1!#%7895$D<!$(89%&65?/.#KY :?=.#d Dn5?B!Q,."!//.&"/.0P5?>!$&"/m9S:.7178)7*>\7*/9%#1&"/!Bfi7H#%9;!$9'7SR!$(89%&65?/U,!+&2BN#*KGY :.7F07*/.7*B%!$9%78P=/.&637*Bfi#;!$E,."!/.#G5+DE9;:.7]=/.&637*Bfi#S!$E,."!//.&"/.0!$605?BN&)9S:>F#,Bfi78#%7*/49%78PF&"/F9;:.7 /.78.9 #%78(89%&65?/F!Bfi7 #%789%#Z5$D9;:.78#'7#'9;!$9%7SR!$(89'&)5./],!+&2BN#*KZxQ7 Bfi7SDX7*B9%5G9;:.7#%9;!$9'7SR!$(89%&65?/,!$&"Bfi#<!$#H8 , *nbQ ( - , VdO78(*!=.#'79;:.78[U!$#%#'5?(8&"!$9%7F#%9S!$9%78# 9%51!$(89%&65?/.#M9;:!$9M(*!/O7,^7*BDX5.B%>F78P&"/19;:.78#'7G#%9;!$9%78#KY:.7 h ii Bfi7*,Bfi78#'7*/9%&"/.0T9;:.79;B%!4/.#%&69%&65?/Bfi78"!$9%&65?/ |!/.Pz9;:.7F#%789G5$D3.&6#%&69%78PQ#%9S!$9%78# J 9%7*/.P#9%5FO7M2!4Bfi07d!/.P!F>F5?BN7 7S(8&67*/9 (85?>@,=.9;!$9%&65?/@(*!/O^7A5?O.9S!$&"/.78PUO[,7*BDX5?B'>F&"/.0]9S:.7A78.&6#%9%7*/49%&"!$=!/9'&I(*!+9%&65?/Q5+D/.78.9<#'9;!$9%7F3!4Bfi&"!O.678#7*!Bfi6[&"/Q9;:.7C(*!$6(*=."!$9%&65?/an =Bfi(8:789F!$nK"dEo*p4po !/%!/789-!$nK"d.o*pp4qr8KY5AP5 9S:.&)#d$9;:.7 9;B%!/.#'&)9'&)5./<Bfi78"!$9%&65?/H:!$#Z9%5 O7 #;,.6&69-&"/49%5`!_(85?/N=/.(89'&)5./<5+D^,!BN9%&69%&65?/.#| j | #'kFCFHFk | bC!$6)5$~ &"/.0C9;:.7<>F5.P&JI78P(*!+)(*=."!$9'&)5.C/ pjaO[}b _ C| b kFHFCF Oa [ } _ | (k(~j} _RtaO[} _ | #Mk#Jr%rFHFCFrG[\ 3q[}fi0 J:!$9 &6#*d | # (*!/1BN7SDX7*B9%5!+)3!Bfi&"!O.678#*d | ( (*!4/1Bfi7SDX7*B_9%5!$63!BN&2!4O.)78#M78.(87*,.9>[ } # | % (*!/1Bfi7SDn7*B 9%5!$63$!Bfi&"!O.678#G78?(87*,.9[ } # !/.P#[ }( !/.PU#%5@5?/K|M#Z#;:.5$~A/<O4[ !4/N!/H789 !$nKafio*pp4qr9;:.7(85?>,=.9;!$9'&)5./<9'&2>\7_=.#'78P]9'5A(*!$6(*=."!$9%79;:.7,Bfi78&">!+07 &)#!G(85./378\D=/.(89%&65?/C5+D9S:.7`/.=>FO7*B5$D-,!Bfi9%&69%&65?/.#KEY :.7GBfi7*!$#%5?/]DX5?B9;:.&6# &6# 9;:!$9*d+DX5?B#%5?>F7G/.=>]O^7*B5$DA,!Bfi9'&)9'&)5./.#*d !@D=Bfi9S:.7*BG#;=O^P&)3.&6#%&65?/m5+D_9S:.7,!Bfi9%&69%&65?/.#F~ &66 /.59<Bfi78P=.(87C9;:.7C9%59;!$Z(85?>,.678.&69[dO78(*!4=.#%7G9;:.7M(85?>,.678?&69fi[C&2/49;Bfi5.P=.(878PUO4[9;:.7M2!4Bfi07*BE/.=>FO7*B 5+D h ii 5?,7*B'!$9%&65?/.# &6#_:.&60?:.7*B_9;:!/9;:.7<Bfi78P=.(89'&)5./15$D 9;:.7G(85?>,.678.&69[T5$D-7*!$(8: h ii 5?,^7*B%!$9%&65?/KY:.7`Bfi7*,BN78#%7*/9S!$9%&65?/@5$D-9;:.7M650&6(*!$78,Bfi78#%#%&65?/@Dn5?B7*!$(V:1BN782!+9%&65?V/ e<\d }!/.P :!$#AO^787*/C(*!Bfi7SRD=.)6[F(8:.5#%7*/H#;=.(8:<9S:!$9-&69Z(85?/.#%&6#%9%# 5+D!M(85?/N=/.(89'&)5./<5+D#;=O78,Bfi78#%#%&65?/.# 9;:!+95?/.6[Bfi7SDX7*B9%5G! #S>!$6#;=O.#%7895$D/.78.9 #%9S!$9%7M3!Bfi&"!O.678#*K :.&6#ABfi7*,BN78#%7*/9S!$9%&65?/1!$665$~ #A=.#M9%5F#%5?Bfi9 5?=.9 9;:.7G#;=O^78,BN78#%#%&65?/.#&"/1(85?/%=/.(89%&637F,!Bfi9%&69%&65?/.#M~ &69;:1/.7*!B 5.,.9%&">!$#%&6878#M9;:!$9 #S!$9%&6#fiDn[9;:.7]!4O5$37GBfi78=.&"Bfi7*>\7*/9%#KTVWXW'W 66 8058&4"8O VxQ7 IBN#%9 P78#%(*Bfi&"O7G9fi~ 5G,Bfi&65?BE!$605.Bfi&69;:>F# Dn5?B h ii RO!$#%78PU=/.&637*Bfi#;!+,."!//.&"/.0!/.PP&6#%(*=.#%# ~A:.&6(8:4&2/.Pz5$DP5?>@!$&"/.#G9;:.78[!BN7]#S=.&)9S!O.67FDX5.B8KGxQ7F9;:.7*/,Bfi78#%7*/49<!1/.78~!$605?BN&)9S:> (*!$6)78P n "8nb6bG9;:!$9 &6#M#;=.&69;!O.67GDn5?B #%5.>F7GP5?>!$&"/.#A/.549 (85$37*Bfi78PUO4[9;:.7F,Bfi&65?B !$605.Bfi&69;:>F#*KY:.79S:Bfi787 =/.&)347*Bfi#;!$,.2!4//.&2/.0F!$605?Bfi&69;:>\# P&)#'(*=.#%#%78P1!Bfi7 !$6O!$#%78P5./]!4/<&69%7*B'!$9%&65?/H5$D,BN78&2>]R!$07(*!$6(*=."!$9%&65?/.#*KEY:.7E&69%7*B%!$9'&)5./G(85?B%BN78#;,5?/.P# 9'5A!A,!4B%!$6678O!$(V4~ !BfiP<OBfi7*!+P9;:4RnIBfi#%9Z#%7*!Bfi(8:G#'9;!Bfi9fiR&"/.0!$9H9;:.705.!$-#'9;!$9%78#<!/.Pz7*/.P&2/.0z~A:.7*/Q!$6E&"/.&69%&"!$ #%9;!+9%78#<!Bfi7F&"/.(8"=.P78Pw&"/Q9S:.7#%789G5$D 3.&)#'&)9'78P#%9;!+9%78#<ab#%787G &60?=Bfi7Fr8K :.7<>!$&"/P&7*Bfi7*/.(87FO789fi~ 787*/19;:.7<!+)045?Bfi&69;:>F# &6#M9;:.7G~ ![9;:.7],BN78&2>@!$07&6#AP7SI/.78PKPre3Pre2Pre1InitGoal&60?=Bfi7<p|`/@&662=.#'9;B%!$9%&65?/5$D9;:.7<,!B%!+)678O!$(8~ !BNPWOBfi7*!+P9;:4RnIBfi#%9 #%7*!BN(V:C=.#%78P1O[ h ii RO!$#%78P=/.&637*Bfi#;!$Z,.2!4//.&2/.0U!+)045?Bfi&69;:>F#*d.(85?>@,=.9%&"/.0W,Bfi78&">!+078#`uBfi7oduEBN7*{!/.PUuEBN7*KCCV{5W{2m|,x|\BA\|"& >!$9'9%& 789]!$nKEafiopp!r&"/9SBfi5?P=.(878#F9fi~ 51P&J7*BN7*/9F&"/.P#F5$D_,BN78&2>@!$078#<(*!+)678P8nfi!4/.Pg ;,, F , VK|#%9SBfi5?/.0,Bfi78&">!$07H&6# P7SI/.78PQO[CpM|ON\| Tm ,fon+K jaEX, EG8 , z .VX k][Y]Zr , GV ,aO ][or h J \bY_ZJhVnfi4 , F , J e<e ,:.=.#*d$Dn5?BZ!G#%9S!$9%7dMO7865?/.04&2/.0F9%5G9S:.7#'9;Bfi5?/.0<,Bfi78&">!+07 5$D!M#%789 5$D#'9;!$9%78# J d9;:.7*Bfi7 78.&6#%9%#!+9 )7*!+#%95?/.7!+(89%&65?/ ~A:.7*BN7!$6E9;:.7@9;B%!/.#%&69%&65?/.#DBN5?> !+#%#%5.(8&2!+9%78PQ~ &69;: )7*!+P&"/49%5 J K 5?/.#'&)P7*B9;:.7XWe{2B`CBV{PCCV{5Qx|\BA,Ryxfi 0)-- -)78!>,.67 #;:.5$~A/F&"/F &60?=BN7AK :.7 P59'#E!/.P!B%BN5~#-&"/F9;:.&6#I0?=Bfi7 P7*/.59'7 #%9;!$9'78#E!/.P9;B%!4/.#%&69%&65?/.#Dn5?BA!4/|~&)9S:Q!#'&2/.04)71/.5?/4RP789%7*B'>F&"/.&)#'9%&6(W!$(89'&)5./K<5?B 9S:.7#%789M5$D#%9S!$9%78#G<T#;:.5$~A/T&"/9;:.7I0?=Bfi7d49;:.7 9;:BN787#'9;!$9%78#E:!3.&2/.0F!M9;B%!/.#'&)9'&)5./F&"/9%5F<F!Bfi7 9;:.7#%9;Bfi5?/.0<,BN78&2>@!$07 5$D<UaX&2/.P&6(*!$9'78PO4[W!F#'56&)PT786)&",.#%7C!/.P1"!O786678P,Bfi7o*rd!$#A!$69SB%!/.#%&69%&65?/.# DBfi5?>9;:.78#'7<#%9;!+9%78# )7*!+PU&"/9'5<K| |,:|BA \| &6#G78=!$Z9%51!/15.BfiP&"/!Bfi[,BN78&2>@!$07!$#MP7SI/.78P&"/G7SI/.&69%&65?/{KMY :.=.#*d&"/1 &60?=Bfi7F!+)9;:.7<#%9SBfi5?/.0,Bfi78&">!+078#`!4Bfi7<!$6#%5C~7*!4,Bfi78&">!+078#*dO=.9M9;:.7],BN78&2>@!$078#M#;:.5$~A/1O[P!$#;:.78PC7866&2,.#'78#`!BN7A5?/.6[~ 7*!F,Bfi78&">!+078#*d!$# 9;:.7 P!$#S:.78P19;B%!/.#%&69%&65?/.# P5F/.59 #;!$9%&6#fiDn[F9;:.7A#'9;Bfi5?/.0,Bfi78&">!$047<P7SI/.&69%&65?/KPre4Pre3Pre2Pre1GS&60?=Bfi7<pM.9;Bfi5?/.0!/.PT~ 7*!,BN78&2>@!$07<(*!+)(*=."!$9'&)5./.#*K .56&6P7866&",.#%78#<P7*/.549%7],Bfi78&">!+078# 9;:!$9M!Bfi7O59S:<#'9;Bfi5?/.0<!/.PF~ 7*!d$~A:.&667AP!$#S:.78P7866&",.#%78# P7*/.59%7A,Bfi78&">!+078#E9;:!+9E!Bfi7 5?/.6[F~7*!4KG/.6[5./.7\!+(89%&65?/&6#]!$#'#;=>F78P9%5178.&)#'9<&"/9;:.7FP5?>!+&2/KYZB%!/.#'&)9'&)5./.#<(*!=.#%&"/.0!C#%9;!$9%79%51O7865?/.019%51!~ 7*!T,Bfi78&">!$07B%!+9;:.7*B 9S:!/U!@#%9;Bfi5./.0W,Bfi78&">!+07!Bfi7FPB%!~A/1P!+#;:.78PK:.7G#%789 5+D05.!$#%9;!$9%78# &6#G>!B%$78P;H.K@?CCV{5W{2:CCV{5:D BED>)W{u{2B"{5|w#'9;Bfi5?/.0 5.B#%9SBfi5?/.0M(8[?(86&6(A,."!/F&6# 9;:.7 =/.&)5./F5$D9;:.7 #%9;!+9%7SR!$(89%&65?/HB%=.678# Dn5?=/.PF~A:.7*/H(*!$6(*=."!$9%&"/.09;:.7<,Bfi78&">!+078#`/.78(878#'#;!Bfi[@DX5.B (85$37*Bfi&"/.09;:.7G#%7895$D&"/.&69%&"!$Z#%9;!$9'78#]a &6#AP7SI/.78P&"/U.78(89'&)5./Uqr8K.9SBfi5?/.0,."!//.&"/.0m5?/.6[m(85?/.#%&6P7*Bfi#1#'9;Bfi5?/.0,BN78&2>@!$078#*KbD]!z#%78=.7*/.(875+D`#%9SBfi5?/.0,Bfi78&">!+078##%9;!4Bfi9%&"/.0Q!$99;:.71#%789@5$DA05?!+#'9;!$9%78#F(*!/mO^7U(*!$6(*=."!$9%78Pd #;=.(8:9;:!$9@9;:.71#%7895$DM&2/.&69%&"!$M#%9;!+9%78#F&)#(85$37*Bfi78PdZ#%9;BN5?/.0Q,.2!4//.&2/.0z#;=.(8(87878P#W!4/.PBfi789S=B%/.#F9;:.7W=/.&637*BN#;!$ ,."!/(85?/.#%&6#%9%&"/.0Q5+DA9;:.71=/.&)5./5$DE!$6-9S:.7<#%9;!+9%7SR!$(89%&65?/1B%=.678#G5$D 9;:.7H(*!$6(*=.2!+9%78P#%9;Bfi5?/.0C,Bfi78&">!$078#*K M9S:.7*Bfi~ &6#%7F&69 D!$&66#\an_&2>@!$9%9%&789A!$nK"do*ppOrK_5?/.#%&6P7*BF9;:.7C78!4>,.67U&"/ &60?=Bfi7UKz|A#P7*,.&6(89%78P&"/m9;:.7@I0?=Bfi7d !T#%9;Bfi5?/.0Q,Bfi78&">!+071(*!/O7\DX5?=/.P&"/9;:.7HIBfi#'9<,Bfi78&">!$07F(*!$6(*=."!$9%&65?/d O=.9G5?/.6[Q!C~ 7*!1,Bfi78&">!$07@(*!/O7FDn5?=/.P&"/9;:.7#%78(85?/.PC(*!$6(*=."!$9%&65?/K :.=.#*d#%9SBfi5?/.0],."!//.&"/.05./.)[@#;=.(8(87878P# &"/9S:.&)# 78!>,.67d&JD-9;:.7#%789 5$D&2/.&69%&"!$#%9;!+9%78# &)#M(85$37*Bfi78PUO4[9S:.7MIBfi#%9A,Bfi78&">!+07]!/.P19S:.7G#%789 5$D 05?!$#%9S!$9%78# K.9SBfi5?/.0,."!//.&"/.0&6# (85?>@,.)789'7<~ &69;:1Bfi78#S,78(89 9%5F#%9SBfi5?/.0F#%5"=.9%&65?/.#*KbDE!<#'9;Bfi5?/.0,."!/178.&6#%9%# Dn5?B#%5?>\7\,."!//.&"/.0Q,Bfi5?O.67*> 9;:.7F#%9;Bfi5?/.01,."!//.&"/.0Q!$605?Bfi&69;:> ~ &66BN789;=B%/T&69*d59S:.7*Bfi~ &6#%7d&69<Bfi789S=B%/.#Rfi0 J9;:!$9/.5Q#'5"=.9%&65?/78?&6#%9'#*Km.9;Bfi5?/.0Q,."!//.&"/.0&6#1!$6#%55?,.9%&">!$P=.719%59;:.7UOBfi7*!$P9;:4RnIBN#%9#%7*!BN(V:K:.=.#*d!F#'9;Bfi5?/.0,."!/1~&)9S:C9;:.7GDn78~ 78#%9 /?=>FO7*B5$D #%9%7*,.# &"/9;:.7G~ 5?Bfi#%9 (*!$#%7G&6#GBfi789;=B%/.78PK.9SBfi5?/.0F(8[?(86&6(,."!//.&"/.01&)#G!Bfi78"!$.78P137*Bfi#'&)5./15$D #%9;Bfi5./.0\,."!//.&"/.0dO78(*!4=.#%7<&69G!$6#%5(85?/.#%&6P7*Bfi#~ 7*!U,BN78&2>@!$078#*KF.9;Bfi5?/.01(8[.(86&6(W,."!//.&"/.0TI/.P#]!C#%9SBfi5?/.01,.2!4/d&JD &)9H78?&6#%9'#*K]M9S:.7*Bfi~ &6#%7d~M:.7*/=/!O.67M9%5 I/.PC!M#%9;Bfi5?/.0<,BN78&2>@!$07 9;:.7 !$605.Bfi&69;:> !$PP#!M~ 7*!H,Bfi78&">!$07K 9 9;:.7*/F9;Bfi&678#9%5`,B'=/.79;:.&6#G,Bfi78&">!$07<O4[WBN7*>F5$3?&"/.01!$6#%9;!$9'78#9S:!$9 :!*347G9;B%!/.#%&69%&65?/.#M67*!$P&"/.05.=.9 5$D 9;:.7<,Bfi78&">!$07F!/.P9;:.7F#%789M5$D3.&6#%&69%78PQ#%9S!$9%78# J K<XD&69G#;=.(8(87878P#*d9;:.7Bfi7*>!$&"/.&"/.01#%9;!$9'78#A&"/9;:.7,Bfi78&">!+07!Bfi7]!+PP78P9%5 J !4/.P&69<!$0.!$&"/U9SBfi&678#G9%5W!+PPQ#%9;Bfi5./.0W,Bfi78&">!+078#*K<XD&69MD!+&)6#*d&69<!$PP#<!1/.78~<d~ 7*!U,BN78&2>@!$07!/.PUBfi7*,^7*!$9%#M9;:.7<,B%=/.&"/.01,Bfi5?(878#'#\an_&2>@!$9%9%&789A!$nK"do*p4p!r8K_5?/.#%&6P7*BA!+0?!$&"/19;:.7H78!4>,.67]&"/T &60?=Bfi7FK :.7F#;:.5$~A/C#%78=.7*/.(87F5$D,Bfi78&">!$07H(*!$6(*=."!$9%&65?/.#(85?=.6PQ:!37<O787*/T(85?>,=.9'78PO4[9;:.7H#%9;Bfi5./.0(8[?(86&6(,."!//.&"/.0U!$605?Bfi&69;:>1K :.7F!$605?Bfi&69;:> ,Bfi7SDn7*Bfi##%9;BN5?/.0Q,Bfi78&">!$078#*d&JD`9S:.78[Q78?&6#%9dE#%5z9;:.7IBN#%9!$PP78Pw,Bfi78&">!+07anuEBN7o*rH&)#@#%9;Bfi5?/.0KQ 5Q#'9;Bfi5?/.0#%78(85?/.Pw,BN78&2>@!$07C78?&6#%9%#!/.Pm9S:.7~ 7*!,Bfi78&">!$047anuEBfi7*{4rG(*!//.59O^7U,B%=/.78Pm9%5U5./.)[(85?/49;!$&"/#%9;!+9%78#E/.59E:!3?&"/.0H9;B%!/.#%&69%&65?/.# 67*!$P&"/.0F5?=.9 5$D9;:.7A,Bfi78&">!+07A!/.P9S:.7#'789 5$D3?&6#%&69%78P#'9;!$9%78#*KZY :.=.#*d9;:.7G#%9SBfi5?/.0(8[.(86&)(!$605.Bfi&69;:>65?5.#DX5.B !/.59;:.7*B~ 7*!C,Bfi78&">!$07K :.&6#G,Bfi78&">!$071anuEBfi7*4r:!+#A/.55?=.9%045&"/.09;B%!/.#'&)9'&)5./.#*d~A:.&6(8:U>F7*!/.#9;:!$9 9;:.7G#'78=.7*/.(87H5$D~ 7*!C,Bfi78&">!$0478# (*!/UO^7<9%7*B'>F&"/!$9%78P!/.PC9;:.7G!$605?Bfi&69;:> (*!/BN789;=B%/@9%5<65.5?HDn5?B #%9SBfi5?/.0<,Bfi78&">!$0478#auEBfiy7 1?rKXD-9;:.7 #'7895+D&"/.&69%&"!$#'9;!$9%78#!Dn9%7*BE!+PP&2/.0,Bfi78&">!+07`uBfiy7 1<(85$37*BN# 9;:.7A#'789E5$D&"/.&69%&"!$#%9;!+9%78# 9;:.7G!$605?Bfi&69;:> #;=.(8(87878P#*d59S:.7*Bfi~ &6#%7&69<(85?/49%&"/.=.78#\=/49%&6E78&69;:.7*BF/.5T#%9;Bfi5?/.015.B<,B%=/.78P~ 7*!U,Bfi78&">!+07(*!4/QO7@DX5.=/.Pab&"/Q~M:.&)(8:z(*!$#%79;:.7<!$605.Bfi&69;:> D!$&66#;r5?B9;:.7G#%789 5$D-3.&6#%&69%78PU#'9;!$9%78# (85$37*Bfi# 9;:.7G#%789 5$D &"/.&69%&"!$#%9;!+9%78#<ab&"/C~A:.&6(V:(*!$#%79;:.7<!$605.Bfi&69;:>#;=.(8(87878P#;rK| #'9;Bfi5?/.0C(8[?(86&6(,."!/T5?/.6[U0.=!B%!/49%7878#G,Bfi50?Bfi78#'#A9%5$~ !BfiP#9;:.7<05.!$-&"/T9;:.7F#%9;BN5?/.0,!Bfi9'#*KA/9;:.7G~ 7*!C,!Bfi9%#d(8[?(8678#M(*!/15.(8(*=B8K Y5@7*/.#;=Bfi7<9S:!$9 9;:.7],."!/)7*/.049;:1&6#I/.&)9'7d~ 7<>F=.#%9A!$#'#;=>F79;:!$9G9SB%!/.#%&69%&65?/.#H)7*!+P&2/.05?=.9G5+D_9S:.7~ 7*!U,!BN9%#G7837*/49;=!$66[Q~ &66O7C9;!+7*/KY :.7!$605?BN&)9S:> &)#(85?>,.6789%7G~&)9S:1Bfi78#;,78(899%5F#%9;Bfi5?/.0F#%542=.9'&)5./.#*d!$# !F#%9SBfi5?/.0<#'5"=.9%&65?/1~ &66-O^7<Bfi789;=B%/.78Pd&JD &69 78?&6#%9'#*K@CC|{5_V^W{2lBAjBaCB)i{ z,CV{5~W{2=CV{5nqD EBDsW{8{2B{|G/1&">,5.Bfi9;!/49 Bfi7*!$#%5?/@Dn5?B #'9;=.P[?&"/.01=/.&637*Bfi#;!$Z,.2!4//.&2/.01&6#M9;:!$9A=/.&637*BN#;!$-,."!//.&"/.0U!$605?BN&)9S:>F#(*!/,BN53.&6P7 #%9;!+9%7SR!$(89%&65?/FB%=.678# 9%5G(85?>,.6789%786[:!4/.P)7G!G/.5?/4RP789%7*B'>F&"/.&)#'9%&6(G7*/3.&"Bfi5?/>F7*/49*K :.=.#*d&JD!w,."!/78.&6#%9%#1Dn5?B1,!$&"/49%&"/.09;:.7T5?5?Bd`!4/!$07*/49178?78(*=.9'&2/.0!m=/.&637*Bfi#S!$],."!/~&)6!$6~ ![?#!35&6Pw,!$&"/49%&"/.0&69%#%78JD<&"/49%59S:.71(85?B%/.7*BF5?B@Bfi7*!$(8:m!/[59;:.7*B@=/Bfi78(85$37*B%!O.67UP7*!+P.R7*/.PK.9;Bfi5?/.0,."!//.&"/.0!4/.Pm#%9;Bfi5?/.0(8[.(86&)(Q,."!//.&"/.0m!$605?Bfi&69;:>F#@(85?/9SBfi&"O=.9%7UO[m,Bfi5$3.&)P&"/.0Q(85.>,.6789%7 h ii RO!$#%78P!$605?Bfi&69;:>\# DX5.B=/.&637*BN#;!$ ,.2!4//.&2/.0KM/4Dn5?Bfi9;=/!$9'78)[d Bfi7*!+R~ 5?BN)PP5?>!+&2/.#F(*!/z:!*347P7*!$P.R7*/.P#9;:!+9]!Bfi7C/.59]!$6~ ![?#F!35&6P!O.67K5?/.#'&)P7*BdDn5?B78!>@,.)7d.(8:.5?,,^7*Bfi# LBfi5?O549fiRO!O4[1P5?>!$&"/1P78#'(*Bfi&"O78PQ&"/.78(89%&65?f/ 1K |M#AP7*,.&6(89%78P&"/T &)0.=Bfi7d/.5U=/.&637*BN#;!$ ,.2!4/BN7*,Bfi78#%7*/49%78PQO[!#%789M5$DE#%9S!$9%7SR!$(89%&65?/TB%=.678#<(*!/0?=!B%!4/9%787H9;:.705?!+9%5FO7<Bfi7*!$(8:.78PC&2/1!I/.&69%7G5?B&"/4I/.&69%7]/.=>FO7*B5$D-#'9%7*,.#*d!$#!+)Bfi786783$!/9M!$(89%&65?/.# >![67*!$P19%5!/1=/Bfi78(85$37*B'!O.67<P7*!$P.R7*/.PK|>F5?Bfi7F&"/49%7*Bfi78#%9%&"/.0T78!>@,.)7@&)#<:.5$~ 9%51047*/.7*B%!$9%7!1=/.&637*Bfi#;!+E,.2!4/TDX5.BG!#%[.#%9%7*> 9S:!$9G(*!/O71&"/!O!+Pm#%9;!$9%7d05?5.Pm#%9;!+9%75?B!4/=/Bfi78(85$37*B'!O.671D!$&6)78P#%9;!$9'7abP7*!+P.R7*/.Pr8K|A#%#S=>F719;:!$9!$(89%&65?/.#(*!/1O7G78.78(*=.9%78PU9S:!$9 (*!/1OBfi&"/.09;:.7G#%[.#%9%7*> DBfi5.> !4/[O!+P1#%9;!$9%7M9%5!H05.5?P1#%9S!$9%7dO=.97*/43?&"Bfi5?/>\7*/9!$(89%&65?/.#F=/4DX5.Bfi9;=/!$9%786[Q(*!4/!$6#%5>!$7@9;:.7#'[?#%9'7*>y#%9;![T&2/!UO!+P#%9S!$9%75?B7837*/(8:!/.07M9%5!/1=/Bfi78(85$37*B%!O.67MD!$&6678P1#%9;!$9%7ab#%787M &60?=Bfi7<prK5F#%9;Bfi5./.0]/.5?B #'9;Bfi5?/.0H(8[?(86&6(<#%542=.9'&)5./Rfi0)-- -)(*!/O7CDn5?=/.PdEO^78(*!=.#%7U!/Q=/BN78(85347*B%!O.671#%9;!$9'7(*!/O7UBfi7*!$(8:.78PzDBfi5.> !4/[&"/.&69%&"!$ #%9;!+9%7KU|G/78!>,.67<5$D #;=.(8:1!FP5?>!+&2/an!F,5$~ 7*B ,."!/49;r &6#A#'9;=.P&678PU&"/U.78(89%&65?/TK"oKJ{KBad StatesGood StatesUnrecoverableFailedStates(Dead-Ends)&60?=Bfi7]p/p-|GO.#%9;B'!$(89 P78#%(*Bfi&",.9%&65?/5$D 9;:.7< |}5$DE!F#%[.#%9%7*>~&)9S:1=/Bfi78(85$37*B%!O.67<#'9;!$9%78#*K|G/.59;:.7*BH6&">F&69;!$9%&65?/5$DM#%9;Bfi5?/.0Q!/.Pm#'9;Bfi5?/.0(8[.(8)&6(,.2!4//.&2/.0m&6#9S:.71&2/:.7*BN7*/91,78#%#'&2>\&)#S> 5$D9;:.78#%71!$605?BN&)9S:>F#*KU 5./.#%&6P7*BGDn5?B78!>@,.)7C9;:.7CP5?>!$&"/abG5?>!+&2/ o*rG&66"=.#%9;B%!$9'78Pm&2/z &60?=Bfi7oPK:.7GP5?>@!$&"/1(85?/.#%&6#%9'# 5$DXY(/oG#%9;!+9%78# !/.PU9fi~ 5<P&J7*Bfi7*/49`!$(89'&)5./.#]abP!$#;:.78P!/.P1#%56&6Pr8K...0GSn1&60?=Bfi7oP/p G5?>!$&"/Qo4KY :.7|5+D!FP5?>!$&"/~ &69;:19fi~ 5!$(89%&65?/.#abPB%!~A/1!+# #%56&6PQ!/.PUP!$#S:.78P!B%Bfi5$~ #;r&662=.#'9;B%!$9%&"/.0H9;:.7 ,5#%#'&2O.67M65#%#E5+D#;:.5?Bfi9E,."!/F67*/.09S:.#E~A:.7*/F,Bfi7SDn7*B%Bfi&"/.0<#'9;Bfi5?/.0#%5"=.9%&65?/.#*K T!/.PU<1!BN7G9;:.7G&"/.&)9'&2!+E!/.P105?!$#%9S!$9%7dBfi78#;,^78(89%&63786[KY:.7 #%9;Bfi5?/.0 (8[.(86&)(M!$605?Bfi&69;:>BN789;=B%/.#-! #'9;Bfi5?/.0 ,."!/ aOP ] ^_ r ] ao ] ^ r ]GFGFHF] aOY:1Go ] ^_ r?K:.&6# ,.2!4/G~ 5?=.6P<:!37E! O78#'9-!/.P<~ 5?BN#%9fiR(*!$#%7Z67*/.09;:M5$DYZK =.9Z! #%9;Bfi5./.0(8[.(86&6(_,."!/aOP ] {C r ]Oa YV1o ] ^_ r<!$6#%5178?&6#%9'#<!/.PU(85?=.6PO^7,Bfi7SDX7*B'!O.67O78(*!=.#'7]9S:.7\O^78#%9fiR(*!$#'7]67*/.09S:U5+DAoF5$DE9;:.7(8[.(8)&6(Q#'5"=.9%&65?/>![m:!*347!m>F=.(V::.&60?:.7*B1,Bfi5?O!O.&66&69fi[9S:!/m9;:.7&"/4I/.&69%7Q~ 5?Bfi#'9fiR(*!$#%7T)7*/.049;:K.9;Bfi5./.0](8[.(86&6(],."!//.&"/.0~&)6-!+)~ ![.#_,BN7SDX7*B 9'5\BN789;=B%/C!<#'9;Bfi5?/.0,."!/d&JD &)978?&6#%9%#d?78347*/C9;:.5?=.0?:C!#%9;BN5?/.0F(8[?(86&6(,.2!4/U>![C78?&6#%9 ~&)9S:U!F#;:.5.Bfi9%7*B8dO78#'9fiR(*!$#%7F,.2!4/167*/.09;:K[Q!$PP&"/.0Q!/Q=/Bfi78(85$37*B'!O.67P7*!$P.R7*/.PDn5?BM9;:.7P!$#S:.78P!$(89'&)5./Q!/.P>!4&"/.0#%56&6P!+(89%&65?/.#/.5?/4RP789%7*B'>F&"/.&)#'9%&6(Wab#%787MG5?>!+&2/C{d. &60?=Bfi7Foo*r8d4#%9;Bfi5./.0<(8[.(8)&6(F,.2!4//.&2/.0C/.5~Bfi789S=B%/.# 9;:.7A#'9;Bfi5?/.0(8[.(8)&6(,."!7/ Oa P ] ^_ r ] ao ] ^ r ]GFHFGF] Oa Y(1o ] ^_ r ?K- =.9 ~ 7F>F&60?:9#%9%&66O^7<&"/9'7*Bfi78#%9%78PT&2/9;:.7,."!7/ Oa P ] {C ^ r ] Oa Yx1wo ] _ r 7837*/9;:.5?=.0?:C9S:.7G05?!$&6#`/.549 0?=!B%!4/9%7878PC9%5O^7]!$(8:.&678378PKMCBAjBCBDsW{u{B{:.7 !/!$6[.#%&6# &"/<9S:.7,Bfi783.&65?=.# #%78(89%&65?/H#;:.5$~ #Z9;:!$9Z9;:.7*Bfi7 !Bfi7EP5.>!$&"/.#E!/.P,."!//.&"/.0],Bfi5?O.67*>F#ZDn5?B~A:.&6(8:~ 7G>![]~ !/49 9%5F=.#%7`!D=.6)[1Bfi78"!$.78PW!+)045?Bfi&69;:>9;:!$9E!$6~ ![?#&"/.(82=.P78#M9;:.7GO78#%9NR(*!$#%7G,.2!4/!/.PBfi789;=B'/.#!#%5"=.9%&65?/H7837*/H&JD&69 &"/.(8"=.P78#EP7*!$P.R7*/.P#9;:!$9Z(*!//.59 O7 0?=!B%!4/9%7878PH9%5GO7 !35&6P78PKxQ7G&"/49;Bfi5.P=.(87]!/1!$605.Bfi&69;:>#%&">F&6"!B9%5F9;:.7G#%9;Bfi5./.0\,."!//.&"/.0U!$605.Bfi&69;:>9;:!$9 !$PP#M!/C5?BfiP&"/!Bfi[,Bfi78&">!$047G&2/C7*!+(V:@&69%7*B%!$9%&65?/1:!+#9S:.78#%7<,Bfi5?,^7*Bfi9%&678#*KE78(*!=.#%7G#'9;!$9%7SR!$(89'&)5./B%=.678# 67*!$P&"/.0C9%5]=/BN7SR(85$37*B%!O.67HP7*!$P.R7*/.P#]>![1O7!$PP78P9%5C9;:.7]=/.&637*BN#;!$E,."!/d~ 7G(*!$6 9;:.&6#<!$605?Bfi&69;:> n "8nbRfi0 J0...0GSn1&60?=Bfi7oo<p G5?>!$&"/U{^KY :.7|5+D!FP5?>!$&"/~ &69;:19fi~ 5!$(89%&65?/.#abPB%!~A/1!+# #%56&6PQ!/.PUP!$#S:.78P!B%Bfi5$~ #;r_&)6"=.#%9;B'!$9%&"/.09;:.7,5#%#'&2O.67C65#%#G#;:.5?BN9`,."!/67*/.09S:.#<~A:.7*/,Bfi7SDn7*B%Bfi&"/.0U#'9;Bfi5?/.0(8[?(86&6(F#%5"=.9%&65?/.#*K T!/.PU<1!BN7G9;:.7G&"/.&)9'&2!+E!/.P105?!$#%9S!$9%7dBfi78#;,^78(89%&63786[K6b?KY:.7!+)045?Bfi&69;:> &)#1#S:.5~M/m&"/ &60?=Bfi7o*{KY:.7UD=/.(89%&65?/uEBN78&2>@!$07?a>Z"8b , > , 8rBfi789;=B'/.#<9;:.7#'789<5$D #%9;!$9%7SR!+(89%&65?/QB%=.678# !$#%#'5?(8&"!$9%78P~ &69;:9S:.7,Bfi78&">!$07@5$D 9;:.73.&)#'&)9'78P#%9S!$9%78#*KuEB'=/.7?a , M*nbHd Z"8b , > , 8rGBfi7*>F5$378#M9;:.7#%9S!$9%7SR!$(89%&65?/B%=.678#*d~A:.7*Bfi79S:.7]#'9;!$9%7!$"Bfi7*!$P[&6# &2/.(8"=.P78P&"/C9;:.7M#%789 5$D3?&6#%&69%78P1#%9S!$9%78#*d.!/.PU.9;!$9%78#SD'a (. , > , M*nb^;rBN789;=B%/.# 9;:.7M#%789E5$D#%9;!+9%78# 5$DE9;:.7,B%=/.78P#%9S!$9%7SR!$(89%&65?/TB%=.678#*K 0 hZf &2/.(8"=.P78#H9;:.7F5?,.9%&">F&6#%9%&6(,."!//.&"/.0!$605?BN&)9S:>1K:.7 5.,.9%&">F&6#%9%&6(`,."!//.&"/.0!$605?BN&)9S:> &6#E&"/.(85?>,.6789%7M~ &69;:BN78#;,78(899%5G#%9;Bfi5./.0G#%5"=.9%&65?/.#*d.O78(*!=.#%7M&69CyyCy < s[W) ) E ,)F0ffW ), (ffjC))FF )ff W^C 0F ,"ff W F ) )F[C>@ F ,)*jL , (ffAL , 0 F ,)F0ff ) F,0F ))y>[Cyy>Cyk e 6 )W ),&60?=Bfi7Co*{p :.7<5.,.9%&">F&6#%9%&6(],."!//.&"/.0U!$605?BN&)9S:>1KP5.78#`/.59M/.78(878#%#;!Bfi&66[BN789;=B%/U!\#%9;Bfi5?/.0@#%5"=.9%&65?/d&D 5?/.7G78.&)#'9%#*K /49;=.&69%&63786[d5?,.9%&">F&6#%9'&)(,."!//.&"/.05?/.6[0?=!B%!/49%7878#H9;:!$9<9S:.7*Bfi778.&6#%9%#F#%5?>\77S78(89F5$DA!,.2!4/m!$(89%&65?/z)7*!+P&2/.0z9%5U9;:.7C05?!+Xd~A:.7*Bfi7#%9;BN5?/.0,.2!4//.&2/.010.=!B%!/49%7878# 9;:!+9_!+)7S78(89'# 5$DE,.2!4/U!$(89%&65?/.#)7*!+PU9%59S:.7G05?!$nKY:.7A,=B%,54#%7A5+D5.,.9%&">F&6#%9%&6(<,."!//.&"/.0&6#/.599%5<#;=O.#'9%&69;=.9%7M#%9;Bfi5?/.0G5.B #%9;BN5?/.0<(8[.(86&)(<,."!//.&"/.0K:.78#%7U!+)045?Bfi&69;:>F#F#;:.5?=.6PO7U=.#%78Pm&"/mP5.>!$&"/.#~A:.7*Bfi71#'9;Bfi5?/.0T5?BH#%9;Bfi5?/.0(8[.(86&)(,."!/.#(*!4/O^7Dn5?=/.Pw!/.Pw045?!$ !$(8:.&)78347*>F7*/9C:!$#9;:.7U:.&60?:.78#%91,Bfi&65?Bfi&69fi[KwG,.9%&">F&6#%9'&)(Q,."!//.&"/.0w>F&60?:49O7T9;:.7O789'9%7*B (8:.5&6(87G&"/P5?>@!$&"/.#E~A:.7*Bfi7M05?!+!$(V:.&67837*>\7*/9 (*!//.59 O7M0?=!B%!4/9%7878P@5?BZ9;:.7M#;:.5?Bfi9%78#'9E,.2!4/#;:.5?=.6PO7<&"/.(8"=.P78PQ&"/19S:.7<=/.&)347*Bfi#;!$-,."!/K_5?/.#%&6P7*B !$0?!+&2/d!$#A!/C78!>,.67d9;:.7<BN5?O59NRO!O[CP5?>!+&2/P78#%(*Bfi&"O78P&"/U.78(89'&)5./ 1KZ5.B 9;:.&6#,Bfi5?O.67*> 9;:.7H5?,.9%&">F&6#%9%&6(#'5"=.9%&65?/>!$78# 9S:.7\BN5?O599;Bfi[19%5C6&Dn9G9;:.7FO.)5.(81~A:.7*/T9;:.7F,5#%&69%&65?/T5$D9;:.71O.65?(8&6#678#%#F9;:!/U!/.P9;:.71Bfi5?O^59<&6#F~ 5?B%4&2/.0KY :.&6#F#%787*>F#F9%5O7C9;:.7C5?/.6[Bfi7*!+#%5?/!O.67#%9;B'!$9%780[dZ7837*/9;:.5.=.0?:m/.5Q0?=!B'!/9'787Dn5?BH05?!$ !$(8:.&)78347*>F7*/9@(*!/mO7T0&637*/Kmn9F&6#~ 5?Bfi9S:~A:.&667Rfi0Z; 'c8b''nn8b'"8c'nn)-- -)$bnb+fin'nYZ!O.67oKp :.7 O^78#%9!4/.P]~ 5?BN#%9fiR(*!$#%7E,."!/H67*/.09;:H5$D,^5#%#%&"O.67 #%9SBfi5?/.0d#%9;Bfi5./.0 (8[?(86&6(A!/.PF5?,.9'&2>\&)#'9%&6(,."!/.#&2/\G5?>!$&"/.# !/.P{Fab#%787 &60?=Bfi78# o^PA!/.P1oor8K aeRrZ>F7*!/.# 9;:!+9/.5G#%542=.9'&)5./F78?&6#%9'#*Kk r&"/.P&)(*!+9%78#G9;:!$9 9S:.7],."!/167*/.09S:U&6#M&"/4I/.&)9'7d!/.P!/T=/Bfi78(85$37*B%!O.67HP7*!$P.R7*/.P&)#Bfi7*!+(V:.78PK(85?/.#%9SB%=.(89%&"/.0W!4/C5?,.9%&">F&6#%9%&6(,."!/CDn5?B 9;:.&6#G&"/.PT5$DP5.>!$&"/.#A#'&2/.(87F9;:.7<!+)9'7*B%/!$9%&637H&)#G/.5,."!/!$9!$6nK|#%&">F&6"!BH5?,.9%&">F&6#%9'&)(1,."!/z&)#H07*/.7*B%!$9'78PDX5.BM9;:.7P5.>!$&"/#;:.5$~A/&"/ &60?=BN7WpK@5?BG!+) O!$P#%9;!+9%78#*d9;:.75.,.9%&">F&6#%9%&6(W,."!/!$#%#'5?(8&"!$9%78#<!4/Q!$(89%&65?/9S:!$9<OBfi&"/.0#F9;:.7F#%[.#%9%7*>y9'5U!C05?5.PQ#%9S!$9%7F&"/5?/.7#%9%7*,K :.&6#T(85?/9'&2/.=.78#U!$#C65?/.0w!$#C9;:.77*/3.&"Bfi5?/>F7*/49U$787*,.#19;:.7#%[.#%9%7*> &"/!O!+P#'9;!$9%7K78(*!=.#%7/.5C#%9;B'!$9%780[1(*!/O^7\=.#'78PQ9%51,Bfi7837*/49G9;:.7F7*/3.&"Bfi5?/>F7*/49MDBN5?> OBN&2/.04&2/.0T9;:.7F#%[.#%9%7*> 9%5!/1=/Bfi78(85$37*B'!O.67<P7*!$P.R7*/.Pd9;:.7G5?,.9%&">F&6#%9'&)(F#%542=.9'&)5./1&6# =.&)9'7<#%7*/.#%&"O.67K5?BG5?>!$&"/.# !/.P{ #;:.5$~A/&"/F &60?=Bfi78# PM!/.P1ood5?,.9%&">F&6#%9%&6(A,."!//.&"/.0Bfi789;=B'/.#!G=/.&637*Bfi#S!$,."!2/ Oa P ] n , r ] Oa Y.1To ] 8 r ?KZ5?B O^59;:@P5?>!$&"/.#9;:.&6#&6# !<=/.&637*BN#;!$,."!/@~ &69;:@9;:.7 #;:.5.Bfi9%78#%9O78#'9fiR(*!$#%7M67*/.09;:KE 5.>,!Bfi78P9'5G9;:.7 #%9;BN5?/.0<(8[.(86&)(G#'5"=.9%&65?/9S:.7 (85#%9&2/@9;:.7 IBN#%9EP5?>@!$&"/&6# 9;:!$99;:.7A,."!/@>![]:!37 !/F&"/4I/.&69%7M)7*/.049;:d.~A:.&6)7M9;:.7 (854#%9 &"/F9;:.7 #%78(85./.PP5?>!$&"/F&6#9;:!$9E!GP7*!+P.R7*/.P>![O7<Bfi7*!+(V:.78PKEY:.7`Bfi78#S=.)9'# 5$D#%9;Bfi5./.0d#%9SBfi5?/.0<(8[.(86&6(d!/.PC5?,.9%&">F&6#%9'&)(F,."!//.&"/.0&"/CG5?>!$&"/.#Go!/.PU{!4Bfi7G#;=>>!4Bfi&6878PU&"/UYZ!O.67oK(- 8476 $ 8:.7 &"/,=.9E9%5<s hZf &)# !/"!'#QP78#%(*BN&2,.9'&)5./`!/.P!M#;,^78(8&JI(*!$9%&65?/@5$D~A:.&6(V:@,.2!4//.&2/.0!$605.Bfi&69;:>9%5U=.#%7KCY :.&6#<P78#%(*BN&2,.9'&)5./Q&6#F9;:.7*/(85?/437*Bfi9'78P9'5U!1#%789G5$D h ii #Bfi7*,Bfi78#'7*/9%&"/.0T9;:.7,!Bfi9'&)9'&)5./.78P9;B%!4/.#%&69%&65?/mBfi78"!$9%&65?/!$#FP78#%(*Bfi&"O78P&"/w.78(89%&65?/qKmY :.7 h ii Bfi7*,Bfi78#%7*/49;!$9'&)5./&)#1=.#%78PO[m!T#%7895$D],."!//.&"/.0w!$605?BN&)9S:>F#9'507*/.7*B'!$9%7U!,.2!4/KY :.75?=.9;,=.9C5$D<s h-f &)#U!z=/.&)347*Bfi#;!$<,."!/ 5?B#%78=.7*/9'&2!+,."!/@P7*,7*/.P&"/.05?/@9;:.7A,."!//.&"/.0!$605?Bfi&69;:>K |=/.&)347*Bfi#;!$,.2!4/&6#BN7*,Bfi78#%7*/49%78PWO4[!/h ii KHn9GP7SI/.78#MDn5?B 7*!+(V:TP5?>!+&2/T#%9S!$9%7]!@#%789G5$Dfi5&"/49`!$(89'&)5./.#G9;:!$9M9;:.7F#%[?#'9%7*> !+07*/9'#`>F=.#%978.78(*=.9%7<#%[/.(8:Bfi5?/.5?=.#%6[T&2/5?BfiP7*B 9'5!$(8:.&)78347]9S:.7G05?!$nK :.7<&">,.67*>F7*/49%78P,."!//.&"/.0U!$605?BN&)9S:>F#!Bfi7Kpo4K .9;Bfi5?/.0,."!//.&"/.0K{^K .9;Bfi5?/.0F(8[.(8)&6(,."!//.&"/.0K^K G,.9%&">F&6#%9%&6(,.2!4//.&2/.0K1K"!$#%#%&6(*!$P789'7*B%>F&"/.&6#%9%&6(,.2!4//.&2/.0Kffannff$b%G;fifinCb *n*Xfi*nSb%Gc$ff *fi'c$;'fi%.H;n' $'* b%nb$ b Sn' ' M"*fi *nSb'\ bZfi 'c*cM +Sn%Znb%'* "% ' G%>M'- H *n*2'ORfi0 JG789%7*B%>\&2/.&6#%9'&)(1,."!//.&"/.0Q(*!/QO^73.&678~ 78P!$#<!#;,78(8&"!$ (*!$#'75$D_/.5./4RP789%7*B%>F&"/.&6#%9%&6(U,."!//.&"/.0KU/h-f d~ 7=.#%78PQ9S:.7]5.,.9%&">F&6#%9%&6(,."!//.&"/.0Q!$605?Bfi&69;:>Dn5?B9;:.7O!$(V4~ !BfiPT#%7*!Bfi(8:T5$D(8"!$#%#'&)(*!+ P7SR9%7*B%>\&2/.&6#%9'&)(,."!//.&"/.0KFanY :.7<#'9;Bfi5?/.05.B #%9;Bfi5./.0(8[?(86&6(!$605.Bfi&69;:>(85?=.6PQ!$6#%51:!*347<O787*/=.#%78Pd!$#!$6 9;:.7HP78#%(*Bfi&"O78Pm/.5?/4RP789%7*B%>\&2/.&6#%9'&)(1!$605?BN&)9S:>F#`O^7*:!37F#%&">F&62!4Bfi6[&"/P789'7*B%>F&"/.&6#%9%&6(P5?>!+&2/.#Kr:.7F5?/.6[/.78~ Dn7*!$9;=Bfi7H5$DE9;:.7FP789%7*B%>F&"/.&6#%9%&6(!+)045?Bfi&69;:> &6#A9S:!$9`!@#%78=.7*/9'&2!+E,.2!4/U&6#G07*/.7*B'!$9%78PDBfi5?>9;:.7=/.&)347*Bfi#;!$E,."!/O4[U(8:.5?5#'&2/.0U!4/U&"/.&69%&"!$#%9;!$9%7F!/.P&69%7*B%!$9%&63786[!$PP&"/.0!/!$(89%&65?/DBfi5?>9;:.7=/.&637*Bfi#;!+,."!/Q=/49%&6 !C05?!$#'9;!$9%7H&)#<BN7*!$(V:.78PKGY :.7FP789%7*B'>F&"/.&)#'9%&6(,."!//.&"/.0!+)045?Bfi&69;:> :!$#O787*/C&">,.67*>F7*/49%78PT9%5<37*BN&Dn[9;:.7G,7*BDX5?B'>!/.(87M5$D-s hZf (85?>,!Bfi78PC9%5H59;:.7*B(8"!$#%#%&6(*!$,."!//.7*Bfi#*Kn9G:!$#</.59GO787*/T5?=B&"/9'7*/9%&65?/T&"/9;:.&6#M~ 5?B%d9;:.5.=.0?:d9'5P7837865?,!\D!+#%9 h ii RO!$#%78P(8"!$#%#%&6(*!$,."!//.&"/.0!+)045?Bfi&69;:> 6&"$7UG& Q!/.85789!$nKAafio*p4pr8KG=B>!$&"/&"/49%7*Bfi78#%9&6#/.5./4RP789%7*B%>F&"/.&6#%9%&6(d>F=.69%&JR!$07*/49A=/.&637*Bfi#;!$ ,."!//.&"/.0KY:.7Us hZf ,.2!4//.&2/.0m#'[?#%9'7*>&6#&">,.67*>F7*/49%78P&"/w40/./ !4/.Pw=.#%78#9;:.7Uiil ,!+(V!$07Oa $&"/.P.R&678)#'7*/d<o*pp4prDX5.B h ii >@!/.&",=.2!+9%&65?/.#*KQ<=BN&2/.0Q,."!//.&"/.09;:.7P[/!>F&6(13$!Bfi&"!O.67UBfi7SR5?BfiP7*BN&2/.0D!$(8&6)&69fi[<5+D9;:.7Eiil ,!$(8*!$047 &)# =.#%78PF9%5 I/.PF! O789'9%7*B5.BfiP7*Bfi&"/.0A5+D9;:.7 h ii 3!Bfi&"!O.678#*K/9;:.7HDn56)5$~ &"/.0TDn5?=BM#;=O.#%78(89%&65?/.#H~ 7,Bfi78#%7*/49<Bfi78#;=.69%#H5?O.9;!$&"/.78PQ~ &69;:9;:.7s h-f ,."!//.&"/.0#%[.#%9%7*>&"/U/.&"/.7<P&J7*Bfi7*/49 P5?>!+&2/.#MB%!/.0&"/.0HDBN5?>P789%7*B%>F&"/.&6#%9%&6(!/.P1#%&"/.067SR!$07*/49 ~ &69;:1/.5@7*/3.&JRBfi5?/>\7*/9<!$(89'&)5./.#<9%5U/.5?/4RP789'7*B%>F&"/.&6#%9%&6(U!/.P>F=.69%&JR!$07*/49<~ &69;:(85?>,.6787*/3.&"Bfi5?/>F7*/49]!$(89'&)5./.#*K|M)Z78,7*Bfi&">F7*/49%#G~ 7*Bfi7<(*!4B%Bfi&678P5?=.9 5?/T5! 1?<q PFQ FuZ7*/9%&"=> uE ~ &69;:QoF<[.9%7 |AB'=//.&2/.09 $&"/?=.1K{KZ|>\5?Bfi7MP789;!$&6678PUP78#%(*Bfi&",.9%&65?/5$D-9;:.7G78,7*BN&2>\7*/9%#&2/.(8"=.P&"/.0U9S:.7A(85?>@,.)789'778PUM!$\P78#%(*Bfi&",.9%&65?/T5$DZ9;:.57 "!#QP5?>!$&"/.#M(*!/1O7DX5?=/.PT&"/ 7*/.#%7*/afio*pp4pr8K-V{ M||\AjB{ByCBEDM2A[,B"{VxQ7 IBN#%9 9%78#%9Es hZf Lc#E,7*BDn5?B%>@!/.(87DX5?BZ#'5?>F7 5$D9S:.7A/.5?/4RP789%7*B%>\&2/.&6#%9'&)(GP5.>!$&"/.# #%56378PO4[A f K78?9*d~ 7,Bfi78#'7*/9-!M,5$~ 7*B,."!/49-P5.>!$&"/<!/.PI/!$66[d~ 7 #;:.5$~Bfi78#;=.69%#DBfi5?>!M>F=.)9'&R!+07*/9Z#%5.(8(87*BP5?>!+&2/Kfffiffh Z j B l fG/.75$D<9;:.7P5?>!+&2/.#1#'56378PO[wM f &6#U!/.5?/4RP789%7*B%>\&2/.&6#%9'&)(9;B%!4/.#;,5?BN9;!$9%&65?/P5?>@!$&"/KY :.7P 5?>!+&2/(85?/.#%&6#%9%#H5$D_!#%789G5$D65.(*!$9%&65?/.#!/.PQ!1#'789G5$D_!$(89'&)5./.#<6&"$7PBN&)347SR9;B%=.(8dPBfi&637SR9;B%!$&"/!/.P[9%5F>F5$37AO^789~ 787*/C9;:.7M65?(*!+9%&65?/.#*KE5?/4RP789%7*B%>\&2/.&6#;> &)# (*!=.#'78PUO[/.5./4RP789%7*B%>F&"/.&6#%9%&6(]!+(89%&65?/.#ab7Kc0K"d!Dn9%7*BE!PBfi&637`!+(89%&65?/!9;B%=.(8F>![]5.BE>![/.59:!37_D=.7867SDX9SrE!/.PC7*/3.&"Bfi5?/>F7*/49;!$(V:!4/.078#ab7Kc0K"d8Dn50<!$9 !$&"B%,^5?Bfi9%#*dH &">!$9%9%&789E!+XKedo*pp!4r8KxQ7 P7SI/.78P9S:.79fi~ 5MP5?>!$&"/F78!>,.678#E9'78#%9%78PO[f Dn5?B #%9;BN5?/.0!/.P1#%9;Bfi5./.0<(8[.(8)&6(,."!//.&"/.01&"f/ /"!#!4/.PUB%!/1s h-f =.#%&"/.0C#%9;Bfi5?/.0!/.P1#'9;Bfi5?/.0(8[.(8)&6(,."!//.&"/.0KA59S:178!>,.678#A~ 7*Bfi7H#%56378PT&2/678#%#A9S:!Z/ PK PqH#%78(85?/.P#*K .&">F&6"!BGBfi78#;=.69%#M~ 7*Bfi75?O.9;!+&2/.78PT~ &69;:1M f K |}07*/.7*B%!$37*Bfi#%&65?/C5$D 9;:.7<:.=/9'7*B !/.PU,Bfi78[C5?BH;uE=BN#;=.&69%P5?>!+&2/an7*/.P!789]!+XKed o*prG!4/.P!1O7*!> ~ !$"UP5.>!$&"/Q:!37!$6#%5O^787*/9%78#%9'78PO[QM f KCY :.707*/.7*B%!+)&6*!$9'&)5./5$DE9;:.7F:?=/49%7*BG!/.P,Bfi78[UP5.>!$&"/1&6#</.59MP78#%(*Bfi&"O78P&"/TP789;!$&6-&"/man &">!+9%9%&789<!$nK"dZo*pp4!r8K :.=.#*d~ 7:!*347]/.59GO787*/Q!4O.)7@9%5W>@!$7]!4v/ "!#&">,.67*>F7*/49;!$9%&65?/5$DE9;:.&6#<P5?>@!$&"/1Dn5?BM!1>F7*!/.&"/.0$D=.(85?>,!4Bfi&6#%5?/KY:.7<,Bfi5?O.67*>&"/9;:.7]O^7*!>~ !$"P5.>!$&"/1&6#DX5?B !/T!$07*/49 9%5~ !$"FDBfi5?>5?/.7H7*/.PU5$DE!O^7*!>9%5C9;:.7<549;:.7*B ~&)9S:.5?=.9 D!$66&"/.01P5~M/KAbDE9S:.7]!$07*/49D!$66#*d&69G:!$#A9'5~ !$"1O!$(8C9%59;:.7H7*/.P5$DE9;:.7O7*!4>!/.PQ9SBfi[!$0.!$&"/KY :.7FI/.&69%7#%9S!$9%7>!$(8:.&"/.75$D 9;:.7FP5?>!$&"/&6#F#;:.5$~A/T&"/ &60?=Bfi7UoKY :.778P078#P7*/.59%7 9;:.75?=.9%(85?>F7 5$D-!M~ !$"F!$(89%&65?/KZx:.7*/F9;:.7A!+07*/9 &6#5?/F9;:.7AO^7*!>1d49;:.7 ~ !$"F!$(89%&65?/RKfifi 0)-- -)(*!/z78&69;:.7*B>F5$37C&69]5./.71#%9%7*,D=Bfi9S:.7*B<5./Q9;:.7UO^7*!>O7*!4>1K5?BF>!$7C&69HD!$6 9%5Q!T65?(*!$9'&)5./m=/.P7*BF9;:.7true...false0...12Gn-2n-1pos&60?=Bfi7o*/p :.7<O7*!>~ !$"FP5?>@!$&"/KEY :.7 "!'#7*/.(85?P&"/.05+D-9;:.7GO7*!>~ !$"FP5.>!$&"/1:!$# 5?/.7,Bfi5?,54#%&69%&65?/!$#%9S!$9%7 3$!Bfi&"!O.67%( 9;:!$9 &6# 9;B%=.7M&JD9S:.7`!$047*/9&)#5?/9S:.7<O7*!>!/.PCD!$6#%759;:.7*Bfi~ &6#%7d !/.P!1/?=>F7*BN&)(*!+E#%9;!+9%73!4Bfi&"!O.67 $9;:!$9GP7*/.59'78#<9;:.7,^5#%&69%&65?/z5$D 9;:.7!$07*/49<78&69;:.7*BG5./9;:.7O^7*!> 5?BM5?/9;:.7@0?Bfi5?=/.PK;U!/.PSGU!Bfi7@9;:.7&"/.&69%&"!$ #%9;!$9%7!/.PU05.!$#%9;!$9%7GBfi78#;,^78(89%&63786[Kxz7 &">,.67*>F7*/9'78PW!07*/.7*B%!$9%5.B ,Bfi50?B%!4> DX5?B""!#P78#%(*BN&2,.9'&)5./.#5+DO7*!4>~ !$"P5?>!$&"/.# !/.P, Bfi5.P=.(878PP5.>!$&"/.#<~ &69;: 119%51+PpC,54#%&69%&65?/.#*K178(*!=.#%79;:.7@P5?>!$&"/5?/.6[(85?/49;!$&"/.#<9fi~ 5C#%9;!$9%73!4Bfi&"!O.678#*ds h-f (*!//.59 78,.65&69A!<,!BN9%&69%&65?/.78P19;B%!/.#'&)9'&)5./Bfi78"!$9'&)5./@DX5?B9;:.&6# P5?>!+&2/dO=.9 :!$# 9%5=.#%7F!]>\5?/.56&69;:.&6(\BN7*,Bfi78#%7*/49;!$9%&65?/KY:.7],7*BDX5?B'>!/.(87F5$Ds h-f !/.PQM f &6#G#;:.5$~A/1&"/ &60?=BN7Wo 1K G&6#%(85?=/49%&"/.0U9S:!$9`M f ~ !$#B%=/H5?/F!#%65~ 7*B >!+(V:.&"/.7d 9;:.7 ,7*BDX5?B'>!/.(87 5$Ds hZf !/.PA f &6# =.&69%7 #%&">F&6"!B &"/F9S:.&)# P5.>!$&"/K5.BMP5?>!$&"/.#<~M:.7*Bfi7s hZf (*!/78,.65&69]!1,!4Bfi9%&69%&65?/.78PmBfi7*,Bfi78#%7*/49;!$9%&65?/d~ 7~ 5?=.6PQ78,78(89H&69<9%5O7!O.67F9%5C#%5637H2!4Bfi07*BM,Bfi5?O.67*>F#G9;:!/TA f d#%&"/.(87A f (*=B'Bfi7*/9')[T(*!/15./.)[=.#%7]!C>F5?/.54)&69;:.&6(Bfi7*,Bfi78#'7*/9;!+9%&65?/K =BN9;:.7*B (85?>,!4Bfi&6#%5?/.#AO^789~ 787*/Ts hZf !/.PUM f !Bfi7G5?/C5?=BBfi78#%7*!Bfi(8:1!$07*/.P!^KvG h g G $- h -jb:.7 ,=B%,^5#%7 5$D9;:.7 Bfi7*>!$&"/.&"/.0G78,^7*Bfi&">F7*/49%#E&"/F/.5?/4RP789%7*B%>\&2/.&6#%9'&)(MP5?>!+&2/.#Z&6# 9%5M#;:.5$~=/.&637*Bfi#S!$,."!//.&"/.0UBfi78#;=.69%#DX5.BEP5.>!$&"/.#A~M:.7*Bfi7<9;:.7<>F=.69%&JR!$07*/49G!/.P17*/3.&"Bfi5?/>F7*/49G>F5?P7866&"/.0CDX7*!+9;=Bfi78# 5$D"!'#m:!37`O^787*/=.#%78PKY:.7,5$~ 7*BZ,.2!4/9ZP5?>!$&"/HP7*>F5?/.#'9;B%!$9%78#Z!A>F=.69%&JR!$07*/49-P5.>!$&"/H~ &69;:<!/H7*/43?&"Bfi5./>F7*/9 >F5.P78!/.PMD=Bfi9;:.7*B78?7*>@,.)&JI78# 5?,.9'&2>\&)#'9%&6(,."!//.&"/.0KE9-(85?/.#'&)#'9%#5$DBfi7*!$(89%5.Bfi#*d*:.7*!$978?(8:!/.07*Bfi#d*9;=B'O.&2/.78#!/.P13$!$6378#*K|P5?>@!$&"/178!>,.67<&6# #;:.5$~A/C&"/ &)0.=Bfi7o*q^K/C9;:.7],^5$~7*B,.2!4/9 P5.>!$&"/17*!$(8:C(85?/49;Bfi56"!O.67]=/.&69M&6#`!$#'#%5?(8&"!$9'78PU~ &69;:1!/T!$07*/49 #;=.(V:9;:!$9!$6 (85?/49;Bfi5Z!$(89%&65?/.#G(*!/O7,^7*BDX5.B%>F78P#%&">F=.)9S!/.785?=.#%6[KFY :.7F7*/3.&"Bfi5?/>F7*/49G(85?/.#%&6#%9%#G5+D!C#%&"/.067!$07*/49 9;:!+9`!$9M!/[19'&2>\7](*!4/CD!$&6!C/?=>FO^7*B 5$D:.7*!+9A78.(V:!4/.078#`!4/.P9S=B%O.&"/.78#<!/.P!$6#%5C7*/.#;=Bfi78#9;:!$9 !$"Bfi7*!$P[D!+&)678P=/.&69%#EBN7*>!$&"/D!$&6678PK |D!$&6678P:.7*!$9 78.(V:!/.047*B67*!4# ~ !$9%7*BDBfi5.>9;:.7 &"/9%7*B'/!$9%59S:.7G78?9%7*B'/!$~ !$9%7*B 65.5?,U!4/.PU>]=.#'9AO7H(8)54#%78PO4[U!O.65?(81!$(89%&65?f/ $KEY :.7G7*/.7*Bfi04[U,Bfi5?P=.(89'&)5./DBfi5?> 9;:.71Bfi7*!$(89%5.BM(*!/QO7C(85?/49;Bfi56678PmO[ 9'51I9<9S:.7P7*>!/.QP iZdO=.9H9;:.71Bfi7*!$(89%5?BM~&)6 !$6~ ![?#,Bfi5.P=.(87<5?/.7G7*/.7*BN0[W=/.&69*KY59SB%!/.#;,^5?Bfi9 9;:.7G7*/.7*Bfi0[@DBfi5?>9;:.7<Bfi7*!$(89'5?B !*~ ![HDBN5?>9;:.7G,."!/9G!$967*!$#%9H5?/.7:.7*!$9H78?(8:!/.07*BG!/.Pz5?/.79;=B%O.&"/.71>F=.#%9<O7C~ 5?B%4&2/.0KM9;:.7*Bfi~ &6#%7C9;:.7,."!/49<&6#<&"/!/=/Bfi78(85$37*B%!4O.)7D!+&)678PT#%9;!$9%7d.~A:.7*BN7<9;:.7<Bfi7*!+(89%5?B~ &66-5$37*B%:.7*!+9*KfffiBv*bff !ff,b`"58nfi$#M#;C $nGnSSn b fi nM"'&%('*)4RKfififi0 J100001000Time / Sec1001010.10.01UMOPMBP0.001050010001500200025003000Number Beam Locations350040004500&60?=Bfi7oy1p u "!//.&"/.019%&">F7<5+Ds hZf !/.PM f &2/T9;:.7<O^7*!>~_!+2@P5?>!+&2/K :.7FA f P!$9;!@:!$#O787*/T78?9SB%!$(89%78PC~ &69;:U,^5#%#%&"O.67F65#%# 5$DE!$(8(*=B%!+(8[DBfi5?>a &">!$9%9%&789A!+XKedopp!rKY:.7#%9;!$9'7]#S,!$(875$D 9;:.7,5$~ 7*B<,."!/49<(*!4/QO7P&63.&6P78Pm&2/49%5T9;:Bfi787P&6#nfi5&"/9F#%789'# pG05.5?PdZO!$P! /.PzD!+&)678P#%9;!$9%78#K/9;:.7045?5.P#'9;!$9%78#*dZ9;:.7*BN7SDX5?BN79;:.7C05?!+E#%9;!+9%78#*dZ9;:.7U,5$~ 7*B<,."!/49#;!+9%&6#fiI78#&69%#F#;!Dn789fi[!/.P!+(89%&63?&69fi[Bfi78=.&"Bfi7*>F7*/49%#*KU/Q5?=B78!4>,.679;:.7@#;!Dn789[QBfi78=.&"Bfi7*>F7*/49%#F7*/.#;=Bfi7C9;:!$97*/.7*Bfi0[C(*!/TO7<9SB%!/.#;,^5?Bfi9%78P1!~_![FDBfi5?>9;:.7F,."!/9d!/.P19;:!$9_D!$&6678P=/.&69%#G!Bfi7G#S:?=.9 P5$~AC/ p+-,/.0,/132*46587/.:90,<;3107/.>=@?0A/13;0,CB:7/D07/4:E31FA8G-;3H0,<?0I37/.*;JA8K0HMLON3PQARK0H0S:N3P:A8K0H0T:N3P:A8KFH3U VWP3NJA8K0;MLON3PQARK0;0S:N3P:A8K0;0T:N3P:A8KF;3U VWP3N+:H0,37/;-,/X>5@H07/.320,C1>=W90I3A05KY,/B[ZEQE07>Z@I3,/BJR\RA8KFH L^]0_:9 L3VWP3NJR\RA8KFH0S`]0_:90SfiVWP3NJR\RA8KFH0T`]0_:90TfiVWP3NJR\RA8KFH3U<]0_:93U VWP3N+:;3a3139fiZ.0,b=<=R;FA/?3?0,/BcZE:E07YZRI3,/BJR\RA8KF; L^]0_[=YL3VWP3NJR\RA8KF;0S`]0_[=CSfiVWP3NJR\RA8KF;0T`]0_[=CTfiVWP3NJR\RA8KF;3U<]0_[=RU V:.7A!$(89%&63.&)9fi[Bfi78=.&"Bfi7*>F7*/49%# #%9;!$9%7 9;:!+9E9;:.7 7*/.7*BN0[,Bfi5?P=.(89'&)5./C78=!$6#9;:.7AP7*>@!/.P!/.PC9;:!$9!$63$!$6378#A9'5~ 5?B%4&2/.0F9;=B'O.&2/.78#<!4Bfi7A5.,7*/CpRKfiRfi 0)-- -)+:?0A/D0,/1:?31FA/B3a>5R;dZRA/.:,/e*a073Ib=WBF,8GY7/.3B?:]QE-P3N+:;3a3139fiZ.0,<f073I/f0,-Z/=:A/?F,/.cZE:;3a3139fiZ.0,QZ/=gA8KJA8K0;ML^]0_hfML3VWP3NJA8K0;FS`]0_hfFSfiVWP3NJA8K0;FT`]0_hfFTfiVWP3NJA8K0;*U<]0_hf*U V/!HO!$PC#%9;!$9%7d$9S:.7`,."!/49 P5?78# /.59#;!$9%&6#fiDn[F9;:.7A#S!DX789fi[F!/.PW!+(89%&63?&69fi[Bfi78=.&"Bfi7*>F7*/49%# O=.9E&6# /.59=/Bfi78(85$37*B%!4O.)[CD!$&6678PK/U!HD!$&6678PU#%9S!$9%7G!$6-:.7*!$9 78.(V:!4/.07*Bfi#M5?B9;=B%O.&"/.78#`!4Bfi7MD!+&)678PKY:.7\=/.&637*BN#;!$,."!//.&"/.09;!$#S1&6#<9%51047*/.7*B%!$9%7!1=/.&637*BN#;!$E,."!/z9%510789MDBfi5?>y!/4[O!$Pz#%9;!$9%79%5T#%5?>F7045?5.P#'9;!$9%7~&)9S:.5?=.9F7*/.P&2/.0z&2/!D!+&)678Pm#%9S!$9%7KC|A#'#;=>F&"/.0Q9;:!$9F/.5=/.&69%#FD!$&6EP=BN&2/.078.78(*=.9%&65?/d&69E&6# 5?O43.&)5.=.# 9;:!$9 5?/.6[<5./.7-fi5&"/49E!$(89%&65?/H&6#/.7878P78PKEM/4DX5.Bfi9;=/!$9%786[d49;:.7 7*/3.&"Bfi5?/>F7*/49(*!/HD!$&6!4/[/.=>]O^7*B 5$D-=/.&69%#P=Bfi&"/.0]78.78(*=.9%&65?/d.9;:.=.#*d?!+#EP78#%(*Bfi&"O^78P1&2/C.78(89%&65?/CK{dDn5?B !/4[O!$P#%9;!+9%7A9S:.7<Bfi78#;=.69%&"/.0<fi5&"/49A!$(89%&65?/1>![65.5?,1O!$(89'5\!FO!$P1#'9;!$9%7M5?B (*!=.#'7G9;:.7<,."!/99%5F7*/.PU&"/1!D!$&6)78PT#%9;!$9'7ab#%787G &60?=Bfi7<prK 5?B9;:.&6#GBfi7*!$#%5./1/.5#%9;Bfi5./.0]5.B #%9;Bfi5./.0](8[.(86&6(]#'5"=.9%&65?/178.&6#%9*Kokh1 b1okh2b2v1 okt1T1H2H1s1v2 okt2s2T2Rpokpv3 okt3T3v4 okt4H3okh3s3fT4H4b3 okh4s4b4ijlkYmfinporqCsMtvuxwfiy3zvoCnWw>{}|0~bWfiyYr|3j}~oRd|brw>{lo0>o:npoC|3RyYnOjlmfinnpyYmfi~>fioRb[ffyYmfinO>oC|3oRYRfi|0~>k0oCnpOq0vfi<|0~> [>o>oC|3^oRYRfi|0~>k0oCnpOwfinpyY m>Ro<>jlkYcwfinpoRmfinpooC|0yffyYmfinoR{oRRnpjlRjp`k0oC~>oCn|3j}~>kOmfin>j}~>oRq0fi|b~>v vu|3jl{loR:>oC|3oRYRfi|0~>k0oCnjm>fiorR{ly0oR[b|>{ly>@<|3Rjy>~:RiyYn|^|Fj{loRQmfin>j}~>oWvj>oyYw:|FRjlyY~<Cm>fioOC|0nnpjloR:yYm>/r>oroC~>oCnpk0:wfinpy> m>RjlyY~hy3>onpoC|3Ry>nj|0~>C|0~`fioRyY~0npy0{l{loRy >ofioCr|0~>:|3@mfin>j}~>orvj$C|0~`fioR{yboR:0W||3{ oY>o3 |0npj}|0>{loRC|0w>mfinpo>ozvyYnbj}~>kr|3m>yF>omfi~>jl/u~^yYw>j}jljly0{}m>jlyY~Oj}rw>{lWjlkY~>yYnpoRfi|3$py0j}~0|3RjlyY~>vC|0~O{lyYy>wWfi|3@yr|fi|FW|3oyYn{loC|3Qy||3jl{loRQ|3oO|0~><~>fir|Wyb{m>jy>~:y`>owfinpy>>{oC|/ffoCnyY~>owfinpoRj}r|3kboC|3{lCm>{}|3jlyY~~bm>jlj oR{l0>oyYw>jjjlw>{}|0~|FmfioRfi|3~>ymfi~>jlzj{l{(|Fj{( mfinpj}~>koR>oRCm>jlyY~|b~>|3{lz|/YR>yYy0oRpy0j}~0|3RjlyY~>fi|3{loC|3-fij}npoRR{gnpy>|Wfi|3Q|3oy|Wk0y>yYh|3o0O>oryYw>jjjlw>{}|0~<jl|0~yYw>j}r|3{RyY~bnpyb{$n|FoRk00fioRC|0m>o^j|F{z|/>R>yYyboR>o>yYnpoRw>{}|0~<yW|^k0yYy>0fi00 0Q03|Fo|0~>Q~>yWy0>oCnn|3oRk0WoR>jlfi|3C|b~:| y0jl<{y>yYw>j}~>kfi|F@`yW|Wfi|3:|3oy>noC~>:j}~:||3jl{oR`|3o0>ojlRo:y3>o|Fowfi|FRoy3>o:|0My owMy*zvoCnOw>{}|0~0Wfiy>r|3j}~[jl0p3u~yYw>jjjly0{}m>jlyY~`z|3kboC~>oCn|3oR<0$Wj}~<fi0oRRyY~>fi|0~>Ry>~0|Fj~>oR:0bfiqCr $[~>y>fioRCu|0~oRfi|0rw>{lo0 |py0j}~0|3Rjy>~Oz|FoR>n|FRoROnyY>ow>{}|0~^y>n|fi|3W|3oz>oCnor|b~>zvoCnpo|3jl{oRc|b~>:oC~>oCnpk0:fioC^|0~>Qz|3WoC~>oCnpkb-mfi~>jlCz>jl{loW>ooC~>oCnk0:wfinpyY m>Rjy>~<:z|3yY~>{lqmfi~>jlC<>oOoRYn|3RjlyY~<joz|3fifiqCOoRRyY~>fir|0~>$|3oRdwMoRRoR$>oOoRyFpy0j}~b|FRjlyY~>j}~>R{}m>fioR-|j}~>k0{lopy0j}~b|3RjlyY~@fi|0~>kbj~>kORr|0~>yrnm>o|0~>WoRj}~>kWyrfifffi(- fi[>owfimfinwMy0o`y3>oWy>RRoCnfiyY^|3j}~QjOy<fioCyY~>n|3oW|<m>{j|Fk0oC~0^fiyYr|3j}~[zj[|yYnpooR{}|0fiy>n|3oWoC~ j}npy>~fioC~0OyYfioR{fi|0~h>owfiy3zvoCnrw>{}|0~brfiyYr|3j}~[ffRyY~>jry3pzy`oC|0y3w>{}|/0oCnpfi|3C|0~:y oj}~:|OkYnpjl<zvyYnp{l:|0~>:wfi|3|Wfi|3{l{yWoC|F@`y0>oCn8uoC|3R`j}ooCw<|w>{}|/0oCnoRjl>oCny oRj}~`yY~>oy3>oyYmfinr|FyYnfijnoRRjlyY~>yYnwfi|3oR>ofi|3{l{yW|0~>y0>oCnoC|0w>{}|/0oCnR>o|3jlykboC~>oCn|3o|mfi~>j oCnp|3{$w>{}|0~OffyYnyY~>oy3$>ooC|0fi|3vC|0~Ofio|bwfiw>{jloR<yRyYno|k0yY|3{z>oC~>o oCnv>ooC|0wfiy0oRoR>ofi|3{l{ffuj}rw>{lo "!'# fioRCnpj}w>jlyY~hy3>ory>RRoCnfiyY^|3j}~-y>fioR{l>oOoC|0wMy0oRj}~>k>ofi|3{l{|3O>oC|3kboC~0Ofi|3^C|0~cy o|b~>wfi|Fr>o:fi|3{l{j~>fioCwMoC~>fioC~0`y3oC|3R[y0>oCnR>m>C|w>{}|/0oCnwfiy0oRj}~>k<>orfi|3{l{C|0~<|3{lz|/Ywfi|Fy|0~by0>oCnoC|0w>{}|/0oCnR>oryYwfiwMyY~>oC~0oC|0jlyYfioR{l{loR-|3|^oRyFoC~ j}npyY~fioC~b|Fk0oC~0fi|3C|b~y oj}~W>offyYmfin^|3pyYnfijnoRRjlyY~>fim>fi| o~>y<|3RjlyY~>y>nfi|0~>fi{lj~>kQ>ofi|F{{ff<>ork0y>|3{y3>oWmfi~>j oCnp|3{vw>{|b~:jlyfi| oO|w>{}|/0oCnzjlO>ofi|3{l{$j}~OnpyY~bvy3>oyYwfiwfiyY~>oC~bk0y>|3{zjl>yYm>fi| j~>kO|0~0OyYwfiwMyY~>oC~0j}~W>ok0y>|3{$|0npoC|firjlj}rwMy0j}>{lo:yhk0oC~>oCn|3o|gnpyY~>kQw>{|b~[fi|3^Ry oCnpr|3{l{wMy0j}>{lo<j~>jlj}|3{|3oRC6i yYnj}~>|0~>Ror|0~j}~>jj|F{|3ozjl|b~WyYwfiwMyY~>oC~0{ly>C|3oR`j~`>ok0y>|3{|0npoC|^fi|3~>yOnyY~>ky0{}m>jlyY~vm>|npyY~>kw>{}|0~^Ry oCnpj}~>k|3r|b~0j~>jlj}|3{|3oR|3wMy0j}>{lojljl{l{m>om>{fffifioRC|0m>ojlfio~>oR|3{l{>o[RyYnpj}~>k0<|FoRy3>oWkY|0oW|0~>Qmfinp>oCnrwfinpy jlfioR|<w>{|b~QffyYnRyYnpj}~>k:>oWkbyY|3{~>yr|3oCn>o|3RjlyY~>CM>oyYwfiwMyY~>oC~bw>{}|CboCnpR>yYy0o0hoj}rw>{loCoC~boRc|0~ "!'# k0oC~>oCn|3y>nffyYny>RRoCnfiyY^|3j}~>zj:fij$oCnpoC~b oR{l[jlRoR|0~>~>mfiMoCnpy3|3k0oC~bC>om>{lj|3k0oC~bkYn|0wfi^j}~WijlkYmfinpoOqC>y3z$ w>{}|0~fi~>j}~>kWj}om>j~>k>onyY~>kw>{}|0~fi~>j}~>kg|F{kbyYnpjlfij}~<y>RRoCnfiyY^|3j}~>zjl:F{ly>C|3jlyY~>|b~>:yY~>oyWjl:w>{}|CboCnpyY~OoC|3ROoC|0>ow>{|b~fi~>j~>kOj}ooRoCykYnpy3z!oRdwMyY~>oC~bj}|3{l{`zjlO>o~>mfifioCnvy3w>{|/0oCnC>jlWjl~>y0rmfinwfinpjlj}~>k|3W~>y0ryY~>{l>o<|3oWwfi|3Ro:fim>|3{lyQ>o<~YmfifioCnOyFy0j}~b|FRjlyY~>kYnpy3z[oRfiwfiyY~>oC~bj}|3{l{lzj>o~>mfifioCnyF(|Fk0oC~0C(yj}~ oRjlkY|3o>oRyY^w>{oR>jlprj}~0nyY m>RoRW0py0j}~0|3RjlyY~>zvoRy>~>nm>RoR<| oCnpjlyY~WyF>oyYRRoCnfiyYr|Fj~`zjl`yY~>{l|^j}~>k0{lor>oC|0~>oC~ j}npyY~fioC~0r|3k0oC~b|b~>n|b~-$-|3k>|3j}~>oWj}~>k0{lo|3k0oC~bkYn|0wfihj}~:ijlkYmfinoqC`>y3z>on|0^|3jlfioRCnpoC|3oj}~<RyYrwfim>|3jlyY~`j}o0j~>ybyY jlyYm>>yYm>k>fi|3m>j}~>kyYnpor|3k0oC~bj}~>CnpoC|3oRO>oWRyY^wfim>|3jlyY~fi|3{v{lyY|3$v|3>jl~>yYnr|3{l{lc|3{ly:npoR m>RoR>o~YmfifioCny3wfinoRj^|3k0oC|3{lCm>{}|3jlyY~>CfioRC|0m>o|{}|0npk0oCn~YmfifioCny3|3oRjlrnpoC|3R>oR-j}~hoC|3@hjloCn|3jy>~~>fioRoR$j}~Q|oCnpjy>~yF>owfiy3zvoCnw>{}|0~bfiyYr|3j}~zjlfioRoCnj}~>jjl|3Rjy>~>C3zvoffyYmfi~>>ow>{}|0~fi~>j}~>kj}oyfioRCnpoC|3o oRo>owMy3zoCnw>{}|0~bkYn|0wfiWj}~WijlkYmfinoqC 8fiz>oC~<yYnpo|Fk0oC~0zvoCnpo|3fifioR oC~>oC~qC0b R<ukY|3j}~[zvoWoC|3mfinpoR[>oWj}oOy>noR>n|FRj}~>k-|3RjlyY~>npyY >oWkboC~>oCn|3oRcmfi~>j oCnp|3{w>{}|0~>CWiyYn>om>{lj|3k0oC~b oCnjlyY~:yF>o oWw>{|/0oCny>RRoCnfiy>r|3j}~:>orpzy^ybj~b|FRjlyY~>|3R>jlo j}~>k>ok0yY|F{$>y3z~`j}~ijlkYmfinoq/zvoCnpooRYn|3RoRWnpyY >ormfi~>j oCnp|3{w>{}|0~<j}~`{oRfi|0~fibfiqoRRyY~>fiC0fififf fffi vfifi}10000Multi-AgentSingle-AgentPower Plant1000Time / Sec1001010.10.010246Number Players81012ijlkYmfinporqCMt {}|0~fi~>j}~>kWj}oyF$yYnk0oC~>oCn|3j}~>knpyY~>krmfi~>j oCnp|F{w>{}|0~>j}~Wy>RRoCnfiy>r|3j}~>zjlyY~>oyjlw>{}|/0oCnpyY~oC|3RoC|bW$iyYn$>om>{lj|3k0oC~boRdwMoCnpj}oC~0oC|F@w>{}|/0oCnz|F|3yYRj}|3oROzjlr|b~|3kboC~0C3z>j{loyY~>{l|j}~>k0{lo>oC |b~>oC~ j}npy>~fioC~0|3kboC~0z|Fm>oRj}~>oj}~>k0{lo|3k0oC~bvoRdwMoCnpj}oC~0/>owMy*zvoCnw>{}|0~bvkYn|0wfi>y3zw>{}|0~fi~>j}~>kj}oy>n|fioRoCnj}~>jljl oCnpjy>~ry3>owMy3zoCnvw>{}|0~bvfiyYr|3j}~Wm>j}~>kqyWqCYoC|3k0oC~bC221253242153543454511244511333(a)(b)(c)ijlkYmfinporqCMt {}|0~OoR>oRCm>jlyY~Wo"!fim>oC~>Ro0v>ofinpoRo|3oRv>y3z|r0fiwfiyb>oRjlC|3{$|3|F@rfi|FoRWyY~|mfi~>j oCnp|3{ w>{}|0~>o|3o |(jl|ORyYnj~>kb|3o03fioRC|bm>o>o|F|3R3oCnp >{|F@C|0~`oRYn|3R|W~>y>~>oCrw>poRy3py0j}~b|3RjlyY~>npyY>ormfi~>j oCnp|3{w>{}|0~$#>y>y0j}~>kyYoybj~b|3RjlyY~>npyY>ow>{|b~fi>o|3|3R3oCnpC|0~^oC~0oCn>ok0yY|F{|0npoC| fi|3fioRzjlW>ofi|F{{zjl>j}~Wpzvyj}ooCw> |3o |0~> % v~>yr|3oCnzfi|3|3Rjy>~>C>oyYwfiwMyY~>oC~bw>{}|CboCnpR>yYy0o0&(')+*-,.",0/214365738".%39:*-;<1:=3>578u~YmfifioCny3oRdwMoCnpj}oC~0fi| ofioRoC~C|0nnpjloRWyYm>vj}~OfioRoCnj}~>jljlfiyYr|Fj~>vj}~Wy>npfioCny oCnpj$ wMoCn y>nr|0~>Ro<|0~>!j{l{}m>n|3o<>o<k0oC~>oCn|3{ljlp[y3mfi~>j oCnp|F{w>{|b~> oCnpm>OR{}|3jlC|3{ffo0@?fi00 0Q03!fim>oC~0j}|3{w>{}|0~>C:oRyYrwfi|0nponmfi~jonpoRm>{ly>>|3j}~>oRzjl<$Wj}~`yYoy3>ouBACRyYrwMoRjljlyY~QfiyYr|Fj~>y`>oWnpoRm>{ly3>oORyYrwfioRjjy>~[w>{|b~fi~>oCnp "E oCny0/qC0bR O:o>oC~>y3zfi|3|Omfi~>j oCnp|F{w>{}|0~j~<|fioRoCnj}~>jljlrfiyYr|Fj~jlyYnpok0oC~>oCn|3{$fi|0~`|R{}|3jlC|3{"o !fim>oC~0j|F{w>{}|0~fioRC|0m>o|O{}|0npkbo~YmfiMoCnyFR{}|3jlC|3{o"!fim>oC~0j}|3{vw>{}|0~>r|0nporRyY~b|3j}~>oR:j}~h>omfi~>j oCnp|F{w>{}|0~GFffIHKJLNMGOP:Q R R <Sy>mfinw>{|b~fi~>j~>k>oCUT3$(V XW 6Y |0m>[Z\A>oR{}r|0~qC002R*fi 6Y y>oC>{oCnoR|3{ff} q/00R]S R6^ y>~>_k Zi y3 qCb C|b~> S/ |3{mfi+Za`o~>oCnR>000cbRyYrwMoRoRj}~O>ofinpoRofiyYr|Fj~>vzvofi| orm>fijoR$` T3$d Vd W!jlfi|FoR:yYe~ SR R T3$z>jl{lo fiQ|0~>fR c|0npo^kYn|0wfifiw>{}|0~bfi|3oRw>{}|0~fi~>oCnpC SCQm>oRr|<>oCmfinpjljloC|0npR-|0wfiwfinyY|3R:fi|3oR[yY~Q|<wfinpoCwfinpy>RoRj}~>k:y3>ofiy>r|3j}~>ou gA Cw>{}|0~fi~>oCnpzvoCnponmfi~OyY~O00 yYnYb % hfioC~bj}mfiiB#zj<qC C >oRvu "o !dm>j}wfiwfioRhzjl ^ j}~Ym>>oWkYnpj}wfiwfioCnOfiyY^|3j}~[RyY~>jlry3zvy:npy>yYru|b~>c|-npy>fiy0zjl<|^{off|0~>:npjlkYbk>npj}wfiwfioCn|0~>:|O~YmfifioCnyFfi|3{l{fi|3C|0~Mory oRb>onpyYMy0C>o|3jlvyy o|3{l{>ofi|3{l{lnpy> npy>yYu!ynyYyY6zjl>onpyYfiybj}~>jlj}|3{l{Wj}~rnyYyY u>>o|Fo |0npj}|0>{loRy3v>o "!# oC~>Ry>fij}~>ky3v>ofiyYr|3j}~<|0npo>owfiy0jljlyY~gy3>ornpy>fiy0|0~><>owfiybjljlyY~ryF>ofi|3{l{lC>owfiy0jljlyY~^y3>onpyYfiybjoRjl>oCn nyYyYu yYnvq nyYyYfi R3z>j{lo>owfiybjljlyY~Wy3|fi|F{{(C|0~Wfio npy>yYu Rq npyYy> fi Rfi j}~W{loffkYnpj}wfiwMoCn vyYnv j}~WnpjlkYbkYnpj}wfiwfioCqn Ry>n>our BA2 COkYnpj}wfiwfioCnwfinpy>>{oC>or~YmfifioCny3w>{}|0~<oCw>j}~:|b~WyYw>j}r|F{w>{}|0~`kYnpy3z{j}~boC|0np{lOzjl^>owfinpyY>{loC~>mfifioCnRs nyY>{loCqRyY~0|3j}~>rfi|3{l{lC|0~>O>o~>mfifioCnvyFfi|3{l{lkYnpy3zbpzvyy>noC|3@<wfinpy>>{oC>onpoRm>{yF>ooRdwMoCnpj}oC~0jl>y3z~j~h|0>{lo^y0kboR>oCnzjl>ornpoRm>{lyF>orw>{}|0~fi~>oCnpj~g>our BA C^RyYrwMoRjljlyY~u k>n|0wfi>jlC|3{npoCwfinpoRoC~b|3jlyY~`y3v>ow>{}|0~fi~>j}~>kj}oj}~`>o|0>{lojl>y3z~Wj}~<ijlkYmfinpoO%q Cfi$tWkboC~>oCn|3oRj}~>j}mfi{loC~>k0:w>{}|0~>m>ory`jlwfi|0n|3{l{oR{vfinpoC|3fibffnoC|bnp@<|3{lk0y>npjlfiWrufioCw>jlRoRQj}~:ijlkYmfino%q Cfijl| ybjfi>ooRfiwfiyY~>oC~bj}|3{kYnpy3z^y3>ow>{}|0~fi~>j}~>kWj}ofi|3Rfi|0n|3RoCnpjlRoR|3{l{yF>oRy>rwfioRjljlyY~<w>{}|0~fi~>oCnpoR>RoCw> SC>oC~Om>j}~>k|wfi|0njljlyY~>oRWn|0~>jjy>~npoR{}|3jy>~$rjl>oyY~>{lw>{}|0~fi~>oCnvC|bwfi|0>{loy3k0oC~>oCn|Fj}~>kyYw>j}r|F{w>{}|0~>y>n|3{l{>owfinpy>>{oCCWi y>n>jlfiy>r|3j}~:>on|0~>jljlyY~:npoR{}|3jy>~y3|0~ "!'# fioRCnpj}w>jlyY~<C|0~<Mofij jlfioR-j}~bvuKwqfi|3jlrwfi|0npjjy>~>Cz>oCnp$u:jl>or~>mfiMoCny3fi|3{l{lCufijlCm>oR<jx~ A>oRRjlyY~<sfiM>oyYw>j^|3{~>mfiMoCnyFwfi|0npjljlyY~>jl~>y0~>oRRoR|bnpjl{`>o{}|0npk0oR~>mfifioCny3wfi|0njljlyY~>Ci yYn(>ovnpoRm>{lj}~|0>{looC|3@wfi|0njljlyY~RyY~>jloRy3|RyY~/mfi~>jlyY~`y3qCWfi|Fjlwfi|bnpjljlyY~>Cy#yYrwfi|0npoR`yO>oryY~>y0{ljl>jln|0~>jljlyY~<npoR{}|3jlyY~<npoCwfinoRoC~0|3jlyY~>onpoRm>{lvyY>|Fj~>oROzjl^>owfi|0njljlyY~>oRWn|0~>jjy>~rnpoR{}|3jlyY~^zvoCnpojk>~>j C|b~0{lWfioRoCnyY~^>o{}|0npk0oCnwfinpyY>{loCCW>oOoCyYnp:m>|Fk0ory3wfinpyY>{loC 0Wzjlh|wfi|0npjjy>~>oR-n|0~>jljlyY~:npoR{}|3jy>~z|3[ Cb YoR/0z>jl{lojloR>RoRoRfioRW>o{ljjvy3vqC C YoR|3vwfinpyY>{loCqCffyYn>oyY~>yb{jl>jln|b~>jljlyY~WnpoR{}|3jlyY~j$kd,ml/n3opop,0/*-;<1:=3>5~>oy jlofiyY^|3j}~[>o`|3hjlyhk0oRR>j}w>Cvfij}wwMyYwv@>oRoRo|0~>:Cn|3@FoCnpC$npoRzj}~>-|`y jlor|0~>:oR>orRyYmfi~boCnyWRoCnpyr>oryY~>{l<j}~0oCnoCnpoC~>RoWfioRpzvoRoC~j$kd,{za;|N3G,\*-;71x=365}~7~ sq6sn6%gg 6@fid](@fiqn"ndnfi rr%p nd@g~~7n6Un"q$]@r$r][qn fi6nfi]nrfig6@fi 6@qfir% q]nrNr"6]]@~UNnfi6B<pr[6]dns>nq]7@gnfidsq@gNr%nfi"%]prfiqnfir"]r@7]@Unsdd6@sp %]fi%@r"r6q[d6sqyfi@fiN r%nnq y]s@g~0@fififf fffi vfiq%}}qN2 ~qq@q@}@"q}@q@@q}qq}q@}"@@q@}""q@}}@""}cq@%N2qn~@@qq@q}@"@q@@q} qq"}"" @"}@%cq"}}@}q"}}@r"}q@}@@@fi}rqq}@"q}}}q@}@}cq@cq"}@}@}}""@""%}@@%@@@"}@}}@}@}q"]q|0>{lofit `npj}wfiwfioCnfiyYr|3j}~WnpoRm>{l/B#vy0{}mfir~OyY~>o|b~>WzvyyYnoC|3RWw>{}|0~fi~>oCnv>y3z>ow>{}|0~fi~>j}~>kj}oj}~j{l{ljloRRyY~>fi|0~>^>ow>{}|0~^{loC~>k0 t-|0np/|0~>W$ yY~>y>y*z>ow>{}|0~fi~>j}~>kjoffyYn(m>j}~>k|Owfi|0npjljlyY~>oR:|0~>:|ryY~>y0{ljl>jlrn|0~>jjy>~noR{|FjlyY~npoRwfioRRj oR{l0i yYn$(zjlwfi|0njljlyY~>oRn|0~>jljlyY~npoR{}|3jlyY~>ov>j}npRy0{}mfir~>y3z>ov~YmfiMoCn$y3 wfi|0npjjy>~>C (oC|0~>fi|3$>ow>{}|0~fi~>oCnm>oRyYnofi|0~rq/ C >oR$y3oCyYnpy>n$z|3oCnj}~fi|3oRrfioffyYnponoRmfin~>j}~>k|yb{m>jy>~ ~>{lnoRm>{ly>noRYoRCm>jlyY~>m>j}~>kO{loRfi|0~<qC C YoR|0npo>y3z~^ffyYnv$>omfi>k0yY|F{jlfi|3>oy jom>MonoRzy>mfi~>$YfioffyYnpo>oRyYmfi~boCnC|0~OfiooRvyRoCnpy>owfinpyY>{loCj}~:>oOy jlorfiyY^|3j}~<yY~>{l<fij$oCnb>o~>mfifioCny3yY>poRRy3oC|F@<pfiwfioryFffyYy>$>o~>mfifioCny3y>>oRRj}~>CnpoC|FoR{lj}~>oC|0np{lnpy>sffyYfin npyY>{loC qyr3ffyYfin nyY>{loC 0Mmfin "!# fioRCnpj}w>jlyY~y3>oy jofiy>r|3j}~noCwfinpoRoC~boC|3Rrpfiwfioy3$ffyYy>|3|~YmfioCnpjlC|3{|Fo |0npj}|0>{loWzjlh|n|0~>kbo"o !fimfi|3{y>oO~YmfifioCny3yY>poRRy3fi|3pfiwfioryFffy>yY$O|0>{loW>y3z>ow>{}|0~fi~>j}~>kjoffyYn$|0~>>ovRyYrwMoRjljlyY~rw>{}|0~fi~>oCnpffyYn>oy jlofiy>r|3j}~wfinpyYb{loCC ~r>joRdwMoCnpj}oC~b|0~>r>onpoCr|3j}~>j}~>kroRfiwfioCnpj}oC~b$rm>oRjlfio|0m>{lwfi|0npjjy>~>j~>ky3>oWn|0~>jljlyY~npoR{}|3jlyY~Qi y>no oCnpwfinyY>{loC |3{l{>ow>{|b~fi~>oCnpr~>[>oWyYw>j}r|3{y0{}m>jlyY~^ j}3oyby3>oRyYrwfioRjjy>~Ww>{|b~fi~>oCnp$^fi|3|{ly3zcRyYrwfim>|3jlyY~^j}o0fifim>jlvjlv>oyY~>{lw>{}|0~fi~>oCn~>y0>y*zj~>k<|0~bj}~>CnpoC|Foj}~hRyYrwfim>|FjlyY~<j}oro oC~:>y>m>kY$>o^jlRoy3>o^|3owfi|3RoyFjloC~>RyYfij}~>kOj}~>CnpoC|3oRnyY0py *>oW{ly0k0jljlRrfiy>r|3j}~ oR{ly0y qC0FRyY~>jry3RjljloRCvnm>@bC[|3j}nw>{}|0~>oRv|0~>rwfi|3@/|3k0oRCnm>R0C|0~yY~>{lry ovfioRpzvoRoC~{lyYC|3jy>~>j~>ov|0ovRjlp0uj}nw>{}|0~>oRC|0~^yY~>{ly oMoRpzoRoC~^|3j}nwfiyYn{ly>C|3jlyY~>vj}~rfij$oCnpoC~bvRjjoR/>o|3jlvyy owfi|3@/|3k0oRvywMoRRj ^{y>C|3jlyY~>/ nyY>{loCfij&oCnbW>o~>mfifioCny3wfi|3RC|3k0oR/ RjljloRC$|3j}nw>{}|0~>oR|0~>nm>@bC>oW{ly0k0jljROfiyYr|Fj~Qjlfi|bnp$|b~>[yY~>{lnpyY>{loC q0MsfiW|0~>qbqWy3>obwfinyY>{loCrzvoCnpoy0{ oRcb-|0~b-w>{}|0~fi~>oCnj}~Q>oOu BA C`RyYrwfioRjjy>~ oRoW|0>{loW 8>o "!# fioRCnpj}w>jlyY~j$kd,\s;U(38".%398*-;71x=3650fi00 0Q03100001000Time (Sec.)10010UMOP Part.UMOP Mono.STANHSPIPPBLACKBOX10.10.01024681012Problem Number14161820ijlkYmfinporq%CMt {}|0~fi~>j}~>kj}oy>n$`|0~>>ourBA2CrRyY^wfioRjljlyY~<w>{}|0~fi~>oCnpffyYnv>okYnpj}wfiwfioCnfiyYr|3j}~:wfinpy>>{oCCtx|0np/|0~>Q$ yY~>y >y3z>orw>{}|0~fi~>j}~>k:j}offyYn$Wm>j}~>kW|rwfi|0npjljlyY~>oR<|0~>|^yY~>y0{ljl>jln|0~>jljlyY~noR{|FjlyY~ npoRwMoRRj oR{l0y3>oW{ybk0jljlRrfiyYr|Fj~[m>oR~YmfioCnpjlC|3{|Fo |0npj}|0>{loROynpoCwfinoRoC~0^{y>C|3jlyY~>^y3wfi|3RC|3kboRCz>oCnponm>@b|0~>W|3j}nw>{}|0~>oR|bnponpoC|3oRW|3vwfioRRj}|3{({y>C|3jlyY~>/ oC~r>yYm>kY^>o|3owfi|FRoy3>o^|3{l{wfinpyY>{loCjly>fioCn|3o0d(O|3jl{lyyb{ o|0~0Oy3>owfinpyY>{loCj}~W>ofiyYr|3j}~m>RRoRoRfiyrk0oC~>oCn|3o>on|0~>jljlyY~noR{|FjlyY~Wfim>|3jl{ly~>jl`>owfinoRj^|3k0oC|F{Cm>{}|3jy>~>Chofi| om>fijloR<>o{ybk0jljlRfiyYr|3j}~oRYoC~>j oR{0$noRRoC~0{ly>Cm>j}~>ky>~ $fi|FoR<fiooCnj~>jlj:w>{}|0~fi~>j}~>k >o`{ybk0jljlRWfiyYr|3j}~6oRoCOy-fio<fi|0np!m>j}~>kc|:w>{}|3j}~ (fi|3oR|0wfiwfinpy>|3@|3>ojlRoRy3>owfinpoRj}r|3k0oRkYnpy3z!y>y|3C(y|3fi npoR>jlRyY^w>{oR>jlp0 zvofi|fio oR{lyYwMoR|0~`|0>n|3RjlyY~`oRRfi~>j !fim>offyYn$fi|3oRQfioRoCnj~>jljOw>{|b~fi~>j~>kr~<|W~Ym>>oR{l{|WwfinpyY>{loCjlny0{ oR:m>j}~>k|b~|b>n|3Rn|b~>jljlyY~`YoCWz>oCnpooC|3R`n|0~>jjy>~Ry>nnpowMyY~>fiy|WoRy3oCnpj}|3{ljlC|0>{lo|3RjlyY~>CW>oC~h>oroCw>j}~h>o|0>n|3Rw>{}|0~Q|0nporoCnj|F{jlRoRm>j}~>k|0~yYnpfij}~fi|0npWn|0~>jljlyY~WYoCWv!jW>jl~>oRz |3{lk0yYnpjlfiWMzofi| oMoRoC~|b>{oyOy0{oCn|3{y3>oRyYrw>{loRWur BA2 CRyYrwMoRjljlyY~`{ybk0jljlRwfinyY>{loC oC~>oC~WoR|3{ff}fi000 8GFGF(f R $d Q>oy>>|3R{lofiyY^|3j}~fi|3MoRoC~rRy>~>nm>RoR^yfioCy>~>n|3o>ok0oC~>oCn|3{ljlpy3mfi~>j oCnp|3{w>{|b~>CffRyY~>jly3|WkYnjQzvyYnp{l:zjlh RoR{l{lCB u[y>>|3R{loR|b~>-|npy>fiy0|3k0oC~bC>oOwfiy0jljlyY~>y3>ory>>|3R{loR|bnpor~>y0fio~>oR$>o^k0yY|3{wMy0jljlyY~<y3v>onpy>fiy0jl>omfiwfiwfioCnnpjlkY0RyYn~>oCny3>ork>npjl$|b~>:>oO|3yYn>onpy>fiy0jlyy onpyY|b~0:wfiybjljlyY~:j}~h>orkYnjQyW>o^k0yY|3{wfiybjljlyY~WoRC|bm>o>orj}~>jj|F{{ly>C|3jlyY~>y3yY>|3R{loR|0npormfi~fiY~>y3z~>oOmfi~>j oCnp|3{vw>{|b~-m>|0Fo|0~bwfiy0j}>{owfiy0jjy>~Wy3$yY>|3R{loRj}~0yr|3RRy>mfi~0C>z>jlROk0j n< j}~>jlj}|3{$|3oRCy>n|wMoRRj j}~>jlj}|3{|Fo|"o !dm>oC~bj}|3{w>{|b~rC|0~^fiok0oC~>oCn|3oRnpyY >omfi~>j oCnp|3{w>{|b~>m>Cn< W"o !dm>oC~bj}|3{w>{}|0~>|0nporRy>rwfinpjloR:j}~<yY~>oOmfi~>j oCnp|3{vw>{|b~y0ofi|3|Wmfi~>j oCnp|3{0@fififf fffi vfi}}qqqq}qq|0>{lofitN2}}}}}}}}}}}"2qqq}qq""""qq}"}}r""""@q}}"@""q}@@@}}"}}}}@@q@@%%qqqqqqqqqqfi}"%]@c}@}@@}@@%@@"@}@}@}@}@}jlo<fiyYr|Fj~!npoRm>{C >o<|0>{lo<>y3zO>o:nmfi~j}o<j}~jl{l{jloRRy>~>fi<y>nroC|3R>w {}|0~fi~>oCnR oC|b~>fi|3>ow>{}|0~fi~>oCnm>oR[yYnpo^fi|0~[qC2C >oRyFoCyYnp`yYnz|3voCnj~fi|FoRMoyYnonpoRmfin~>j}~>kW|y0{}m>jlyY~u{l{w>{}|0~fi~>oCnpk0oC~>oCn|3oRWyYw>j}r|F{&w>{}|0~>y3{oC~>kbM[ tOm>oRO|0n{loRfi|0~`qC C >oRffyYnv|0~bwfinyY>{loCj}~O>jlfiy>r|3j}~w>{}|0~OzjluyY>|3R{loRvj}~>R{}m>fioR|0~0Omfi~>j oCnp|3{$w>{}|0~Ozjl<qyuWyY>|3R{loRC |3vyY>|3R{loRC|b~Mow>{}|3RoR|F>o|0o{ly>C|3jlyY~B y0oy>npoRy oCnR0fi|3v>omfi~>j oCnp|3{w>{|b~>~>o oCnRy oCn|F{{(j~>jlj}|3{|FoRCMoRC|0m>oWyY>|3R{loRC|0~QfioWw>{}|3RoRc|3>ork0yY|F{wfiybjljlyY~|0~>:y>>|3R{loRC|0~Q>{y>R<>onpyYMy0Cumfi~>j oCnp|F{w>{}|0~yYn^|0~[yY>|3R{lofiyY^|3j}~[zjls<yY>|FR{oROz|3k0oC~>oCn|3oRczjl$j}~QY0oRRyY~>fir|0~>QRyY~0|3j}~>oR[ CC00bW$~>y>fioR qCfi >oR@ RA>"o !dm>oC~bj}|3{w>{}|0~>zvoCnpooR>n|3RoRnyY>omfi~>j oCn|3{w>{|b~`y>n|^wfioRRj wMy0jljlyY~<yF>oyY>|3R{loRCijlkYmfinpoWqC^>y3z>ooR>n|3Rjy>~rj}oyF"o !fim>oC~bj}|3{w>{}|0~>vffyYnv|0~Oj}~>CnpoC|3j}~>kW~YmfiMoCnvy3oCw>j}~W>ow>{}|0~ oC~>yYm>k><>o $!npoCwfinoRoC~0j~>k<>oOmfi~>j oCnp|3{w>{}|0~hj{}|0npk0o0>oroR>n|FRjlyY~<jl oCnpW|3|0~>yY~>{lWkYnpy3zv{lj}~>oC|0np{lzjO>ow>{}|0~W{loC~>k0>ooRy3|FRjlyY~>|3y>Rj}|3oR<zjl<|^|3oj}~:|Omfi~>j oCnp|3{w>{}|0~O<jloRYn|3RoR<0`RyYwfim>j}~>k>oRy>~/mfi~>RjlyY~gy3>or$cnoCwfinpoRoC~b|3jlyY~`y3r|b~>$ufioRCnpj}fioRQj~ A>oRRjlyY~:fi>jly>wfioCn|3jy>~:fi|3|b~:mfiwfiwfioCnfiy>mfi~>:RyYrw>{loR>jp:yF 8i yYn>omfi~>j oCnp|3{w>{}|0~<j}~h>o0@fi00 0Q03N2r"}%"}@@"qcq@}r}@@@}@@}c@@}q@}@@q@}"}@}"}""|0>{lo ^ k0jljlRvfiyYr|3j}~OnpoRm>{lCiyYnoC|3Rw>{}|0~fi~>oCnRy0{}mfir~Oy>~>o|0~>Ozvy>y3z>onmfi~^j}oj}~:jl{l{ljloRRyY~>fi|0~>h>orw>{|b~:{loC~>k0 oC|b~>fi|3>ow>{}|0~fi~>oCnm>oR-y>npofi|0~qC2 C >oRy3oCyYnyYnz|3oCnj}~fi|3oRMoyYnonpoRmfin~>j~>kO|y0{}m>jlyY~0.0080.007Time (Sec.)0.0060.0050.0040.0030.0020.0010246810Number Plan Steps121416ijlkYmfinporqCMtvj}oyYnoR>n|3Rj}~>kWo"!fim>oC~bj}|3{w>{}|0~>nyY|Wmfi~>j oCnp|F{w>{}|0~`ffyYnv>oy>>|3R{lofiy3r|3j}~WzjlWsyY>|3R{loRCyY>|3R{lofiy>r|3j}~`zjlO oyY>|3R{loRC>jlRyYrwfim>|3jy>~z|3|3 {loRfi|0~WyY~>ojl{l{jloRRy>~>|0~>WzvyYm>{l|F{{ly3z|0~oRYoRCm>j~>kWnpyYMy0vyoRoR{y3znoC|3RjlyY~WjoRyY~>n|Fj~bC7s-7ggnr7mdBmsg~`>jl|0njlR{o^zofi| orwfinpoRoC~boR:|~>oRz $fi|3oR-w>{}|0~fi~>j}~>kYoCW$$My>nw>{}|0~fi~>j}~>kj}~!~>yY~bfioRoCnj}~>jljl0m>{lj|3k0oC~bWfiyYr|Fj~>/u~[oRfiwfinpoRj ofiy>r|3j}~fioRCnpj}w>jlyY~!{}|0~>kYmfi|Fk0o0! fi|3MoRoC~Wfio oR{lyYwMoR:|0~>|0~Ofio ffORjoC~b$[npoCwfinpoRoC~b|3jy>~Oy3jl ifiuoC^|0~0jRfi|3fioRoC~gfioRCnpj}fioR$hofi| o|0~fi|3{l>RoRgwfino jlyYm>w>{}|0~fi~>j}~>k|F{kbyYnpjlfiffyYn $fi|3oR-w>{}|0~fi~>j}~>k|0~>fioRoCwfioC~>oR>omfi~>fioCn|0~>fij}~>ky3dz>oC~>oRovw>{}|0~fi~>j}~>k|3{lk0yYnpjlfi|0npo|0wfiwfinpy>wfinpj}|3o0ij}~fi|3{l{0zvofi| owfinpy>wfiy0oRQ|0~<yYw>j}jljWw>{}|0~fi~>j}~>k-|3{lk0yYnjfiffyYn~>fij}~>k<oC~>j}>{loWy0{}m>jlyY~>j}~:yYofiyYr|Fj~>z>oCnpo~>ynpyY~>kyYnvnpyY~>krR>R{ljlry0{}m>jlyY~oRYjl/>onpoRm>{yY>|3j}~>oRzjl`$|0npooC~>RyYmfin|3k0j}~>k fi|3(rfi|3v|k0y>yYOwfioCn ffyYnr|b~>RoRyY^wfi|0npoRryyYoy3$>o|3oRR{}|3jlC|3{w>{}|0~fi~>oCnpY~>y3z~OyY |/0Cfififf fffi vfifi}mfin^npoRoC|0npRQfi|3r n|/z~hyYmfin|3oC~bjlyY~hy:|:~>mfiMoCny3y>wfioCe~ !fim>oRjlyY~>rfi|FzvoWzvyYm>{l{lj}3oy|3fi npoRj}~h>om>mfinpo0r~hwfi|0npjlCm>{}|0nzvorzvyY~>fioCn>y3zzvoR{l{yYmfinoC~>Ry>fij}~>k<y3w>{}|0~fi~>j}~>kwfinpyY>{loCC|3{loRRy>rwfi|0npoRy>ovoC~>Ry>fij}~>km>oR0s #mfinnpoC~b{lr oC~>RyYfij}~>kfiy>oR~>y0mfiwfiwMyYnp|rwfi|0npjljlyY~>oR<npoCwfinpoRoC~b|3jy>~Wy3>on|0~>jljlyY~WnpoR{}|3jy>~fim>>ooC~>Ry>fij}~>kr|/fi|y0>oCnwfinpyYwMoCnpjloRfi|3CfioRw>jlor>oryY~>yb{jl>jlnoCwfinpoRoC~b|3jlyY~$r|/<r|03oj|WMoRoCn@>ybjRo0>oOpzyWYoCr|/:|3{ly<fi| o|0~<"o !fimfi|3{wfioCn ffyYn^|0~>Rorz>oC~Qfiy0-|0nom>j}~>k:|yY~>yb{jl>jlnpoCwfinpoRoC~0|FjlyY~ |3j}~h>oMoC|0 z|3{}<oRd|brw>{l%o Rz>jl@Q>y>m>{[kbj o$:|0~Q|3 |0~b|3kborj}~fiyYr|Fj~>z>oCnpo|^wfi|0npjljlyY~>j}~>kOy3>on|0~>jljlyY~WnoR{|FjlyY~WC|0~`fiofio~>oR$u~>y0>oCnj}~boCnpoRj~>k !dm>oRjlyY~rjlyj}~ oRjlkY|3ovz>jlRbj}~>ry3&w>{}|0~fi~>j}~>krfiy>r|3j}~>jlm>j|0>{loffyYn$fi|3oRw>{}|0~fi~>j}~>k z|3mfinwfinpjlj}~>kyYnm>fi|3>o{ly0kbjjlRfiy>r|3j}~mfin~>oRryYm>yMoyrfi|0npOffyYnv$voRRoC~b{lWzvofi| om>fijloR`>jlfiyYr|3j}~O>y>npyYm>kY>{l0 j}~>k|0~O|0>n|3RjlyY~oRRfi~>j !dm>orzvofi| o~>y3zfioRoC~h|0>{loyry0{ oo oCn|3{$y3>o{ly0k0jljRwfinyY>{loCj}~>ou BA CRyYrwMoRjljlyY~ oC~>oC~WoR|3{ff} 00b R>oCmfinnpoC~bfio~>jljlyY~y3:jlwfiy3zvoCn m>{>fim>>y>m>{fiooRYoC~>fioRyoC~fi|0>{loy>fioR{l{j}~>ky3RyY~>nm>Rj ovfi~>oCnpk0oRjlvo$oRR|FfioRCnjMoRj}$~ A>oRRjlyY~ u{y 3zvooC~ jljlyY~ryYnpooRfiwfioCnjoC~0RyYrwfi|bnpj}~>km>{lj|3k0oC~b|b~>:j}~>k0{lo|3k0oC~bfiyYr|3j}~>yj}~ oRjlkY|3o^>orRyYrw>{loR>jlhy3npoCwfinpoRoC~0|FjlyY~Oy3RyY~>CmfinnpoC~b|3RjlyY~>/A>o oCn|3{w>{}|0~fi~>oCnp/ j}~<wfi|0npjlCm>{}|0n $ oR{ly0yoR|3{ff}$qCb0s Rfifi| o>y*z~Ofi|3fiyYr|3j}~>~>y*z{oRfikbo>y>m>{Wfiom>oRbr|w>{}|0~fi~>j}~>kr>oCj}~^yYnpfioCnyC|3{lomfiwrynoC|3{zy>np{lwfinpy>>{oCCu{y v|3R@>m>yZ |0fi|b~>C|fiqC002 >y3z>y3z>ooC|0npR`npoRoyF|^ffyYnpz|0npWRfi|3j}~>j}~>kgw>{}|0~fi~>oCnC|0~Ofiofio ffRjloC~b{lwfinmfi~>oR0r|3j}~>k>ok0y>|3{|3v|yYnm>{}|j~OoCrwfiyYn|3{ {ly0k0jlyY~^>o"o !fim>oC~>Roy3$|3RjlyY~>{loC|3fij}~>kry>ok0yY|F{~^>jlz|/>ok0yY|3{>C|0~j}~>R{}m>fio>~>y3z{loRfik0o|0MyYm>>ofiyYr|3j}~o0 k}Cfi|3y*zvoCnpj}~>o>{y>R0zy>np{lm>fiofim>j{lnpy>My0yYyyYw R(ucj}jl{}|0n|0wfiwfinpyY|3RffyYnnpoR m>Rj}~>k:>orRyYrw>{loR>jpQy3$fi|3oRcw>{|b~fi~>j~>khoRoCrwfinpyYjlj}~>k oRwfioRRj}|3{l{lcfioRC|0m>ooRRfi~>j !dm>oRy>nvoRj}~>kroCrwfiyYn|3{ffyYnm>{}|3|3{}npoC|3fifi| ofioRoC~fio oR{lyYwfioR<j}~yYfioR{$R>oRR0j}~>k>oCnm>mfinpoW@fi|F{{loC~>k0oRWj}~>R{}m>fio:j}~bnpy> m>Rj~>k[|0>n|3RjlyY~Qj}~c (fi|3oRw>{}|0~fi~>j}~>kc|0~>fio~>j}~>kwfioRRj}|3{ljlRoRw>{}|0~fi~>j}~>k|3{lk0y>npjlfiffyYnm>{lj|3k0oC~bfiyY^|3j}~> o0 k }3|3{lk0yYnpjlfim>j}~>k>o{loC|3~>mfifioCny3|3k0oC~byYny0{ j}~>kW|wfinpyY>{loCRsBU\U7AfiwfioRRj}|3{fi|0~fibvy|3y0{lyn|oCnpy |0npRyrvy oCnj&|b~>>oyb>oCnvoCfioCnpvy3>o Afi!k>npyYmfiwffyYnj}~0npyY m>Rj}~>k[m>yg |0~>[ffyYnr|0~b[npoRz|0npfij}~>k:fijlCm>jlyY~>Wy>~c $fi|FoRw>{}|0~fi~>j}~>k|0~>[y>fioR{R>oRR0j}~>k h:o|F{y:zjlhy<fi|0~fi:|b~> |3{vnpY|0~bC mfi~>#v{}|0n3o0oC~finpj}(u~>fioCnpoC~ n~ ^ j}~>> jloR{}oC~|0~> ^ |0npj}nFoR |3{ffyYn|F jlRoOyY~: $!jlm>oR|b~><y>nr|3{npoCwfinpoRoC~0|FjlyY~ij~fi|F{{l0zvofi|0~fi>o|0~>yY~bdyYm>npo jloRzvoCnpyYn>oRjnRyY^oC~0fi|3kYnpoC|3{j}rwfinpy oRW>owfinpoRoC~b|3jlyY~OyF>jl|bnpjlR{lo0>jWzvyYn:z|3OC|0nnpjloR!yYm>Oz>jl{lo:>o`npW|0m>>yYn^z|3 jljlj}~>k #|0n~>oRk0jlo oR{l{lyY!~ ~>j{ ~>j oCnpjp:yF EoC~fi^|0n>oWnpoRoC|0npRhjwMyY~>yYnoR-j}~[wfi|0np0oCnpjp`npyY>o(oR@fi~>jlC|3"j}~>oRfZ+#vyYrwfi|b~007 A>oR{}oC$n Zn|0~>o0 y>~>$$>EoffoC~>ou |0~>RoR[voRoC|0n@4npy0poRR^^uk0oC~>R Euu |b~>>ouj}ni y>npRovoRoC|0n@ |0MyYn|3yYnp ui mfi~>fioCn|3kYnoRoCoC~0~>mfiMoCni00b0/0/*00s0fi>o jloRz|b~>rRyY~>R{}m>jlyY~>vRyY~b|3j}~>oR>oCnoRj~O|0npov>yboy3>o|0m>>y>np|0~>>yYm>{l-~>ybfiorj}~boCnwfinpoRoRQ|3~>oRRoR|0npjl{l-npoCwfinpoRoC~0j}~>k`>o#y ffRj}|3{wfiyb{jlRjloRryYnoC~>fiyYnpoCoC~0CC0fi00 0Q03oRjl>oCnvoRfiwfinpoRoRWy>nj}rw>{ljloR$ y3>o EoffoC~>ou |0~>RoRoRoC|0npRnpy0poRRuk0oC~>R Eu uR>ouj}niyYnpRovoRoC|bnp@ ^ |0MyYn|3y>np ui ^ vy>nv>o$A`y oCn~fioC~0/&%%(' *)+-,-.{gBgK/B1032 s\54j$kd,;(/,17698;: =<?>A@CB0DC@<E3BFG>GHIfi<KJL(< E3<EM>RDCFNPORQS@ET UV@GTVFGWLX FYEZLXVFI[F\F^]_<;>`L>-@CB0DC@<EBF^>`HYIa<bJcL(< Ede<fEdhg<LXiLX FC>G@DCFP>`FYDC@EL(<HG>@#>\PjklpI / : ^ oRmn\o qpZrastrau fioC~>y0o>o ifiu oRo[Eo~>jljlyY~`q%(RyYnnpoRwfiyY~>fij}~>ky>ovoCr|0~bjlRy3v[|3fio~>oRb `j}mfi~>@>jlk0{lj}|oR|3{ff qC002Ru~w- fiyYr|Fj~fioRCnjw>jy>~xzjloCr|0~bjlRo"!fimfi|3{fiy$m n C|0~MoRyY~>nm>RoRj}~r>offy0{l{ly3zj}~>kz|/t ^ oRAfio|j}~>k0{lo|3kboC~0vfiyYr|Fj~z>oCnpo|3{l{y m>oC~b|0npooC~>Ry>fioR|3~>mfioCnpjlC|3{|Fo |0npj}|0>{loR|0~>W>oCnojl|b~W|3RjlyY~^ffyYnvoC|3ROoR{loCoC~0j}~<>or|3{}wfifi|0MoR y3zm n y#yY~>jlfioCn>or|3RjlyY~&{ |Fy>Rj|FoR<yWj}~fiwfim>A| ^ oR>ooRy3RyY~>n|3j}~>oR<|3o |0nj|b>{oRy3l{ o"!dmfi|F{>ooRyF|Fo |0npj}|0>{loRj}~} >orwfinpoRRyY~>fijljlyY~<y3{ jl|0~`oRdwfinpoRjlyY~<fi|3fio~>oR>ooRy3v|3oRfi| j}~>k<|0~`yYm>k0y0j}~>kOn|0~>jjy>~`y>nj~fiwfim>W>oro$oRRRyY~>fijljlyY~:yF*`{ jlr|ORyY~/mfi~>RjlyY~:yFRy>~>fijjy>~fi|3{vo&oRR (~ C RO>oCnporjlyY~>oRyY~>fijljlyY~fi|3{o$oRR^y>nroC|3R|3oWfi|3Wfi|3|0~[y>m>k0y0j}~>kQn|0~>jljlyY~6ffyYn^j~fiwfim> ~j}~>oRyY~>fijljlyY~fi|3{o$oRR|3yYRj}|3oRzj|3ojl>oRfi|0n|3RoCnpjljloRdwfinpoRjlyY~`ffyYn^|0~> jl|Rfi|0n|3RoCnjjloRfiwfinpoRjlyY~yYn>ooRy3~>oRY|3oR u r rRV@ 2BUv|3RRYm>Ci}pZ |bfi|0~>C|fii q/00R3 j~>k<oC^wfiyYn|F{{ly0k0jlOyRyY~bnpyb{oC|0npR:j}~Q|OffyYnpz|0np@fi|3j}~>j}~>krw>{}|0~fi~>oCnR$~K`fi|3{l{}|0 }Z jl{|b~>j>u fiCRVFgB<I[FaHYL(< E>A<fEC 8 JcQ@EE<E>Twfiw$qR qfi qCs0fi>A npoRCv|0n|3{ff#%ZI`oR{y>~>$ qCb0R0voC|FyY~>j}~>k|0MyYm>o&oRRy3 Ry>~>CmfinnpoC~b|3RjlyY~>CX F*d UVIfiE@Q: $ <Hk-pI I[@DCDC<EYT C0s_ q0qCfiUBgoC~> |fi |3kY|0~fi~fi|3fi|0~ }UZmEy> >j|/z|3{}|fi qCCbRN~yYw>j}r|3{(RyYyYwMoCn|3jlyY~y3Y~>y3z{oRfik0oy>mfinpRoR$$|0~oC^w>jnjC|F{fij~ oRjk>|3jlyY~Y(oRR0npoCwCfi# A0`0fiqCYfiCfiCy>oRj~>ku |b~>RoR$oRRfi~>y0{ly0k0#voC~boCnR y>oRj~>k #vyY^wfim>j}~>kA>oCn jRoR/{}mfiWfiu} ZimfinpC ^ qC0b Ri|3vw>{}|0~fi~>j}~>kfinyYm>kYOw>{|b~fi~>j~>kOkYn|bwfir|0~fi|3{l>jlCIaL(< lHY<f@Q8 EL(FYQfQ<T FEHaF/ #0 2 Cfifiq d0bfi{lY>o0 qCb CRV k-QS@EE<ETPU EBFI-UVEHfiFIfiL(@<fEL(<fEwB#E@DC<HB0 DC@<E>R0 Er/>oRjlC #vyY^wfim>oCnA>RjoC~>R$EoCwfi|0noC~bC #|0n~>oRk0jlo oR{l{lyYi~ ~>j oCnpjlp0[ # # Ab C*qRYfi{lY>o0 } Z oR{ly0y qCb0 R(u~fi|3{ly0kbjC|F{npoCw>{}|/rffyYnfio ffRjloC~0RyY~>fijljlyY~fi|3{w>{|b~fi~>j~>k ~klIp HfiFaFfiB<ET> : LX F^LXZ@L(< E@Qv E : FYI[FEHfiF EZIfiL(< lH<@Q 8 EL(FYQfQ<T FEHaFPf"\ 8 00wfiw0 CYdb0fi0uuug npoR/yY~>oRC0} ^ yYoCnj~>R/0 `^}] Z\`o~>oCnR0 qC0b R0u!npyYfim>|b~>|F|FRjlyY~oR{loRRjlyY~roR@fi|0~>jlyYnw>{}|0~fi~>j}~>k 3~ klIp HfiFfiFaB<ET> : LX FCGLXM@L(< E@Q E : FYIFYEHfiF EMIaL(< lHY<f@Q 8 EL(FQQS<T FYEHfiFf"\ 8 0Ywfiw MqR# dfiq/fi0uuufi npoRCvnpY|0~bC$ qC C02 R`n|bwfibfi|3oR-|3{lk0y>npjlfiffyYnMyYy0{loC|0~gmfi~>RjlyY~-^|0~>j}wfim>{|FjlyY~ 8I@E>G@HL(< E> E D*JcUVL(FI>@v b 00Y fi0fiq0CCfififf fffi vfivmfinpR }#v{}|0n3o0>}ZjlyY~npoR{}|3jy>~>C ~yYnbyb{{}|0~>$^fi}y>~>k Er Cq bfiq%RA>fifiy0{ljly>fioR{@>oRR0j}~>kzjlOwfi|0npjljlyY~>oRWn|0~>j8 EL(FIfiE@L(<E@QE : FI[FEHaFvE}FIfi" v@IT F-Ha@QSF 8 E L(FT I[@L(<E /wfiw3YYdsCM#vj}r|3jff$u}N`jmfi~>R>jlk0{lj}|fi}N`j}mfi~>@>jlk0{lj}|fii}7Zn| oCnpy 7 qCb0R${}|0~fi~>j}~>k j}|y>fioR{@>oRR0j}~>k t(u[fioRRjljlyY~rwfinpy>RoR mfinpoy>nN$O<Y ~Mk-IpHfiFaFfiB<ET> : LX FlLX UVIpfiJFfi@ExE : FI[FEHaFEMk-Q@EE<fETZ k 0 ^ oRRmfinp y0oRj}~unpj Rj}|3{~boR{l{jlk0oC~>Ro0wfiwfiqC0Yq8Yfi%Afiwfinpj}~>k0oCnoCnp{}|3k#vj}r|3jffuyB EEfi|FoR[k0oC~>oCn|3jy>~:y3oCnpjff }B Zn| oCny qC0 C02| ROum>y>r|3jlmfi~>j oCnp|3{w>{}|0~>Wj}~~>yY~bfioRoCnj}~>jljl:fiy>r|3j}~>C~klIpHfiFfiFaB<ET> : LX F#LX1@L(< E@QE : FI[FYEHfiFExIaL(< lHY<f@Q 8 EL(FQQ<TVFEHaFM\" 8 fiF>wfiwC00s_0CCfiq0buuu[npoRC#vj}r|3jffvu}yA>npyY~>k[w>{}|0~fi~>j}~>kcj}~!~>yY~bfioRoCnj}~>jljloCnpjff }fi Zxn| oCnpy [ qC02 C0 RIfiyYr|3j}~> }j |Wy>fioR{R>oR@bj}~>k ~klIpHfiFfiFaB<ET> : LX FMLX 8 EL(FIfiE@L(< E@QE : FI[FYEHfiFEIaL(< lHY<f@Q 8 EL(FQQ<TVFEHaF"k-QS@EE<ETCY>`L(FYDef 8 k #fi0Ywfiw 0_ YYfi>uuuB npoRC#v{}|0nFo0 }`nmfifioCnpk }UZoR{loR$Erq/00RYBFQX FaH8_<ET>\npoRC#v{}|0nFo0}oCnpyY~uB Z A>jl{}|fivu qC2 C0 R<um>yYr|Fjl oCnj C|FjlyY~[y3~>jo|3oRyY~>CmfinnpoC~bYoCm>j}~>k<oCrwfiy>n|3{{ybk0jlwMoRRj C|3jlyY~>CxCI@E>G@HL(<E>EklIp TVI@DDC<ETw @ET UV@GTVFG>@EB3>GL(FDP>R R30# db0fi}7Z|3o0u /q 0fiq%Rw>{|b~t>oyYwMoC~:w>{}|0~fi~>j}~>k-|0npR>jloRRmfinpo0MtIaL(< lHY<f@QFEHaF/c #b Y_ 0C0fi#mfinnjo08 EL(FQQS<;EoC|0~}|3oR{}>{lj}~>k ^ j}nr|0~ } Z jl@>yb{yY~u q/00sRa{}|0~fi~>j}~>kmfi~>fioCnj}oRyY~>n|Fj~bvj}~WyYRfi|3jlfiyY^|3j}~>CltIaL(< lHY<f@Q 8 EL(FQQ<TVFEHfiFCvY3 0sYfi3Ej|0~>Ry }`j}mfi~>@>jlk0{lj}|fi}ZmffW~>y qC0C8({|b~fi~>j~>k j}|y>fioR{R>oRR0j}~>kj}~OfioRoCnj}~>jjlfiyY^|3j}~>CtsnpoR{lj}j}~fi|0npWnpoCwfiy>npC~3k-IYHaFfiFfiB<fET>v : LX FPLX 8 EL(FIfiE@L(<E@QE : FIFEHfiF EiIfiL(< lH<@Q 8 EL(FQQ<TVFEHaF#-&FLXYY B0 Ql TVW->GL(FDP>$@EBM-JJcQ<fHfi@L(<E>Z 8 #fi0wfiw 0fifiq fi00fi Afiwfinpj}~>k0oCn oCnp{}|3kCq C08A>jlmfi|FoRcRyY~bnpyb{nm>{loRC!~=klIpHfiFfiFaB<ET>r : LX F >GL 8 EL(FIfiE@L(<E@QE : FI[FYEHfiF Ek-Ia<EHY<KJcQF^> :C E glQFfiBGT FwlFJcI[FG>GFEL(@L(< E@EBlFfi@#>RE<ET 0$wfiwqC0YqbqCfi yYnkY|0~ |0mb^|0~fi~EnmfiryY~>$EnmfiryY~>$ZvnpoRj}~fi|fi qC008 u~0>j}o:d~b>oRjl-wfinpyboRRjy>~t |3Yj}jlRj}~>k!>owfinpyYfi|0>jl{ljl[yFk0yY|3{v|3j|3Rjy>~c~=klIpYHaFfiFaB<fET>r : LXVFLX!E : FI[FYEHfiFWE1IaL(< lHY<f@Q8 EL(FYQfQ<T FEHaF/0wfiw$qC C_qR0 buuu[ npoRCRjy>~>j }|0~fi0/ A}hoR{$Er}fiEn|0wMoCnRgEr} ^ } Z!jl{l{lj|byY~ qCb0R[u~|0wfiwfinpyY|3RyYnw>{|b~fi~>j~>kWzjWj}~>RyYrw>{loRoj~bffyYn^|3jlyY~ ~&klIpYHaFfiFaB<fET> : LXVFM IB 8 EL(FIE@L(< E@QE : FI[FEHaF EdklIfi<fEH<bJcQFG> : E glQFfiBGT FP-F[JcI[FG>GFEL(@L(< Ei@EBM-Fa@>@E<fETYij}3oRC}BZ j{lyY~qC0fiqR AfigAtvux~>oRz|0wfiwfinpyY|3R[y->o<|0wfiw>{ljlC|3jlyY~!y3>oRyYnpoCwfiny j}~>kryrwfinpyY>{loCy0{ j~>klIfiL(< lH<@Q 8 EL(FQQS<T FYEHfiFC0(q%C0YdbCfiCCfi00 0Q03`|FCqCb0 R ~0oRk>n|3j}~>kWw>{|b~fi~>j~>k<|0~>:npoC|3Rj}~>kOj}~:|>oRoCnpy0k0oC~>oRyYm>|3fi~>RfinpyY~>yYm>|0n@>joRRmfinpoyYnRyY~bnpy0{l{lj}~>knpoC|F{zvyYn{:y>>j{lonyYfiy0C~}klIpHfiFfiFaB<ET> : LXVF#LXi@L(< E@QE : FI[FYEHfiF ExIaL(< lHY<f@Q 8 EL(FQQ<TVFEHaFM\" 8 fiF>wfiw C00_ 0CfiqCsfibuuu[npoRC`oR{ffyY~>$ }(Z ^ j@>jl0 qCb0 RoCwfinpoRoC~bj}~>kg|FRjlyY~:|0~>:Rfi|0~>k0orb<{ybk0jlwfinpybkYn|0/X FPd U IfiE@Q( : $ <Hk-Ip I[@DCDC<ETY R 0Mfiq d00M`oRy>npk0o} Z ^ |0~>b0 u ^ qC C0 8voC|3Rj onpoC|3y>~>j~>kW|0~><w>{|b~fi~>j~>k &~ k-IY HaFfiFfiB<fET>: LX F1#LX@L(< E@Q$ E : FI[FEHaF< EIaL(< lHY<f@Q 8 EL(FQQ<TVFEHfiF"\ 8 0wfiw0bY C0My>npkY|0~ |0mbr|0~fi~`j}~>MoCnpk ^ qC C02Rc ~>j oCnp|F{&w>{}|0~fi~>j}~>k tu~ |F{y0@ mfi~>j oCnp|3{$fi|3OjlfioC|fi 8 @^T @#G<fEFC# R>YY Yb`j}mfi~>R>jlk0{lj|M} |0npfi|fi`r } Z ^ j@>jl0 qC0b R$voCwfinpoRoC~0j}~>k|FRjlyY~t~>fioRoCnj}~fi|3R|0~>n|0j C|3jy>~>C IfiL(< -H<@Q 8 EL(FQQ<TVFEHfiFC 3fiY0_ YY Cfi`j}mfi~>R>jlk0{lj|M}7Z ^ jR>jl0 qC02 CRu~:|FRjlyY~<{}|0~>kYmfi|3kbofi|3oR:y>~<C|0m>|3{oRfiw>{}|0~fi|3jlyY~tnpoR{lj}j}~fi|0np<npoCwfiyYnC~ k-Ip HfiFaFfiB<ET> : LX F#LXi@L(< E@Q E : FYI[FEHfiF EiIfiL(< -H<@Q 8 EL(FQQS<T FYEHfiFMf"" 8 #fi0fiwfiw 00_ d00fibuuu[ npoRC|3jlkY } Z oR{ly0y q/0 CRd {}|0~fi~>j}~>k MoRYoRCm>jlyY~`|0~>O{oC|bn~>j}~>krj}~W|nyYfiy0j|3k0oC~bC~ klIpY HaFfiFaB<fET> : LX FLX 8 EL(FIfiE@L(< E@Qv E : FI[FEHaF EZIfiL(< -H<@Q 8 EL(FYQfQ<T FYEHfiFAk-QS@EE<ETcY>`L(FYDP> 8 k #fi0Ywfiw(qC0Y q/0fi0uuufi noRC|3{}mfiWd ( Z `o~>oCnR 000 8u jlj}>{lo>oCmfinpjljlRyYnyYw>j}r|3{w>{}|0~fi~>j}~>k &~ k-Ip HfiFaFfiB#<fET> : LXVF#LX 8 EL(FYIaE@L(< E@Q* E : FYIFYEHfiFW EtIaL(< lHY<f@Q 8 EL(FYQfQ<T FYEHfiFMk-QS@EE<ET&Y>`L(FYDf 8 k fiF>wfiw$qRY_ qRYfibuuu[ npoRCoC~>oC~> qCb0 R] gEEfi|3oRrmfi~>j oCnp|3{fiw>{}|0~fi~>j}~>kj}~m>{j|Fk0oC~0/0~>yY~bfioRoCnj}~>jljlfiy3r|3j}~>C |3oCnR >oRjlCfi$oRRfi~>jlC|3c{ ~>j oCnpjlpy3( EoC~fi^|0n] EoCwfi|0npoC~by3um>y>r|3jlyY~ff\u 03i0MoC~>oC~fi } oR{ly0y } Z!nY|0~bC00 0b0 R>u>n|3Rjy>~oR@fi~>j !fim>oRffyYNn g EEfi|3oRw>{|b~fi~>j~>kiyYnp>RyYj~>k|0fi|0~>C|M3i}*v|0nfioC|0m }] ZIA>p EoC~>jlC0 qCb0 R {|b~fi~>j~>kRyY~bnpy0{Ynm>{oRffyYnnpoC|3Rj ov|3k0oC~bCIaL(< lHY<f@Q 8 EL(FQQ<TVFEHaF/ 3 0_ q0qCfi|0m>0} Z A>oR{}r|0~ qC0b R m>>j}~>kc>o:oC~ oR{lyYwMo0t {|b~fi~>j~>kwfinpyYwMy0jljlyY~fi|3{{ybk0jl|0~>:y>Rfi|3jlOoC|0npRW~ klIp HfiFaFfiB<ET> : LXVF#LX}@L(< E@QA E : FI[FEHaFr EIaL(< lHY<f@Q8 EL(FYQfQ<T FEHaFC\" 8 aF y0{ff fiwfiw$q0qC3 qC0fiqb3uuufi npoRC|0m>00}] Z A>oR{}r|0~> q/00 R ~>j>j}~>k A>ufi|FoRr|0~>kYn|0wfibfi|3oRrw>{}|0~fi~>j}~>k~ k-Ip HfiFaFfiB#<fET> : LXVF#LX 8 EL(FYIaE@L(< E@Qd <fELA E : FYI[FEHfiF E3IaL(< lHY<f@Q 8 EL(FQQ<TVFEHaFC 8 c8 0y0{ff$q0 wfiw M%q CYd0bsfi yYnpk>|0~ |bmbr|b~fi~y>oC>{oCn8 }oCMoR{Yy3r|0~fi~ } ZIEj}yYwfiy>m>{ybC qCb0 R YoC~>fij~>krw>{}|0~fi~>j}~>krkYn|0wfi>y|0~u E ^ mfi>oR/x~ k-IY HaFfiFfiB<fET>< : LX FLX UVI` JFfi@E E : FYIFYEHfiF: Ek-QS@EE<ETk 0Ywfiw b0Y Cbsfi Afiwfinpj}~>k0oCn oCnp{}|3kC"fififf fffi vfiy>oC~>jk A[Z A>j}ryY~>/[`^ q/00sRvoC|3{j}o:oC|0npR[j}~~>yY~bfioRoCnj}~>jjl-fiyY^|3j}~>C~klIpYHaFfiFaB<fET> : LX FGLX 8 EL(FIfiE@L(< E@Qfi<ELwE : FI[FYEHfiFE1IfiL(< lH<@Q 8 EL(FQQS<T FYEHfiF88 F>wfiw$qC0bYqC0bfi yYnpkY|0~ |0mbr|0~fi~Co n8 } 7Zvjl@fi|0nfiC qC038Ckl@I[Hw>{|b~t|w>{}|0~fi~>j}~>k:|0npR>jloRRmfinporzj:wfi|0n|3{l{loR{|FRjlyY~>|0~>-Ry>~>n|3j}~bC ~1 vFfiHL(UVI[FML(FG>x<EIaL(< lHY<f@Q 8 EL(FYQfQ<T FEHaF/$wfiwfiqCYdb0fiA AFAfiwfinpj}~>k0oCn oCnp{}|3k^j}~>>joR{loC~( qCb0RQmEEhuj}~fi|0nEoRRjjy>~ Ej}|3k>n|0 |3RC|3k0o0[(oR@vnpoCwq/00/02Cfi0~>jm>oyF~bffyYnr|FjlyY~r(oR@fi~>y0{ly0kb0 $oRRfi~>jlC|3{v~>j oCnjlOy37EoC~fir|0nVA V fi-z[Y^^j}~>kY|0n$fiu}Zvjl@fi|bnpfiCfi Cq 02CR7{}|0~fi~>j}~>kWwfi|0n|3{l{loR{|3RjlyY~>CIfiL(< -H<@Q3 0Mfiq d038 EL(FYQfQ<T FYEHfiFCqC008>oqC0C^uw>{}|0~fi~>j}~>k<>oCRyYrwMoRjljlyY~MIfiL(< -H<@QmfifijloR 88 EL(FQQS<T FYEHfiFyY~>kEr}Z y3 qCbCRdEyY^|3j}~Oj~>fioCwMoC~>fioC~0w>{}|0~fi~>oCnRyYrw>jl{}|3jlyY~~i 8 k #WI>aXY` J Eg-QSFfiBGTVF ETV<fEFaFYIa<ET@EB"HfiUV<?>G<fL(<E : Idk-QS@EE<ET&AIfi<fB^T <ET!X FIfi@EBklI[@HL(<HfiFC>uuuoR@fi~>jlC|3{oCwfiyYnp\AbC/0M^" EoCny0CEr@GTV@#`<EF/fi}j {l{}|0~ ^ Cq 008#DQ<HYBFQX FfiHR_<ET> }{ mbzoCnuC|3fioCjl mfi>{qC0bR # BtRu[yYmfi~>$/RyYrw>{loRo00wfi|bnpj}|3{3yYnpfioCn$w>{}|0~fi~>oCn(ffyYnklI YHaFfiFaB<fET> : LX F I[B 8 EL(FIfiE@L(<E@QE : FYIFYEHfiFE3klIfi<fEH<bJcQFG> :A Eg-QSFfiBGTVFlF[JIF^>`FYEL(@L(<Ed@EBwlFfi@#>RE<ET>YwfiwqC0YqbqR yYnpkY|b~ |0mb^|0~fi~oRy0C }dZ Afij7Er qC002R#vyY~>fijljlyY~fi|3{~>yY~>{lj}~>oC|0nw>{|b~fi~>j~>kr~k-I YHaFfiFfiB<fET> : LX Fx >GL8 EL(FYIaE@L(< E@Q E : FI[FEHaF EdIaL(< lHY<f@Q 8 EL(FQQ<TVFEHfiFk-Q@EE<fETZY>`L(FYD$>xf 8 k #fi0 wfiwq%C0Y q/0fi yYnkY|0~ |0mb^|0~fi~|0~/|0~$ uRjl0$u}vn|/>yY~ }7{loRjloCnR}(Z+j>{loR0N# qCb0sRff RjloC~0gEE|3{lk0yYnpjlfi^ffyYnOiBA fi~0>oRjl|0~> oCnpj C|3jy>~ ~ 8 C7klpI HfiFfiFaB<ET> : LXVF8 EL(FYIaE@L(< E@Q-W >fiYX fi-J Ed$ TV<fHtc#ELXVFG>G<?R>A>R>yYwfiwfioCnp/ q/C0R~>j oCnp|3{w>{}|0~>ffyYnnpoC|FRj onpyYMy0j}~Wmfi~fiwfinpoRfijlR|0>{looC~ j}npy>~fioC~0C~klpI YHaFfiFaB<fET> : LX F#LX 8 EL(FIfiE@L(< E@Qfi <ELw E : FI[FYEHfiF E1IfiL(< lH<@Q 8 EL(FQQS<T FYEHfiF88 F >wfiw$qC0bY qC3>fi yYnpkY|0~ |0mbr|0~fi~Afij}n~>y} yYoC~>jlk 7A} oR{ly0y }UZ A>j}ryY~>C$ qC0bRvff RjloC~bk0yY|3{fij}npoRRoR<oRfiw>{yFn|3jlyY~ ~&k-I YHaFfiFfiB<fET>AWLXVFiX <IaL(FfiFELXM@L(< E@Q E : FYIFYEHfiF ExIfiL(< -H<@Q 8 EL(FQQS<T FYEHfiFf"\ 8 0 Ywfiw b0Yd 0bfi0uuufinpoRCoC~>fioCnpb0 %A}%Z6:oR{l$]Er"Au E ^ ~ pA>yY~>o0U0ZoR{ly0y qCbCR$y3z|0npfivRy0{l{}|0fiy>n|3j o|0~>|3 oCnp|0npj}|3{${loC|0n~>j}~>k tuC|3om>firj}~WnpyYMy0jlyYRRoCn8 8 EL(FIfiE@L(<E@QfiU IfiE@Q : UVDC@EfiD*JcUVL(FI"cL(UVB<FG>M 8 0Afim>y>~dA}Z|bnpy bu`rqC0C8v-F<E : I[HaFYDFYELQFa@IfiE<ET-@Ed<fEL(IYBUVHL(<EC%?InpoRCfi00 0Q03oR{ly0y }#|0nMyY~>oR{l{ }CoC npoR0buYyYnn|Fy]Er}0ij}~fid}Z{l>>o0 qC00s2R ~boRkYn|3j}~>kw>{|b~fi~>j~>k[|0~>[{loC|0n~>j}~>k t`>ofi E`t |bnp@>jloRRmfinpo01fiUVIaE@Q :3 ]JFIfi<DFYEL(@Q-@EBX F I[FL(<Ha@QIfiL(< lH<@Q 8 EL(FQQS<T FYEHfiFCv q%RUCfiqfiqCbfioR{ly0y q/03R klQ@EE<ETZ@EB3QSFfi@IfiE<ETxGM@E@Ql TV<fHfi@QI[Fa@#>R E<fETYN Afiwfinj~>kboCn oCnp{}|3koR{ly0y }yb{{}|3R }Z #y* qC0 C8 |3jlyY~fi|F{ofi|FoRyY~>jlyYnpj}~>k[ffyYnw>{|b~fi~>j~>kj}~fid~fi|bjloC~ jnyY~fioC~bC~ k-Ip HfiFaFfiB<ET> : LXVFALX 8 EL(FIfiE@L(< E@Q E : FI[FEHaFExIaL(< lHY<f@Q 8 EL(FQQ<TVFEHaF"k-QS@EE<ETCY>`L(FYD$>M 8 k #fi0fiwfiw$qCfifiq qC0fibuuug npoRC:oR{l$ Er qC002 RoRRoC~0|F |0~>RoRj}~Ouw>{}|0~fi~>j}~>k IfiL(< -H<@Q 8 EL(FQQ<TVFEHaF*@GTV@#G<fEFCYbY qC0M!jl{bj}~>C Er3 0oCnp/ ^ ^ y3zn|0~>Ro0 Er}n Z:oR{oR0 ^ ] qCb3 R {|b~fi~>j~>k|b~>noC|3Rj}~>kj~mfi~>RoCnp|3j}~!|0~>fid~fi|0j`oC~ j}npyY~fioC~bCfi UVIaE@Q :d ]JFIfi<DFYEL(@Q*@EBX F I[FL(<Ha@QIaL(< lHY<f@Q 8 EL(FQQ<TVFEHaF/ 3$qC0_ d00fiC%fiJournal Artificial Intelligence Research 13 (2000) 305-338Submitted 6/00; published 12/00Conformant Planning via Symbolic Model Checkingcimatti@irst.itc.itAlessandro CimattiITC-irst, Via Sommarive 18, 38055 Povo, Trento, Italyroveri@irst.itc.itMarco RoveriITC-irst, Via Sommarive 18, 38055 Povo, Trento, ItalyDSI, University Milano, Via Comelico 39, 20135 Milano, ItalyAbstracttackle problem planning nondeterministic domains, presenting newapproach conformant planning. Conformant planning problem finding sequence actions guaranteed achieve goal despite nondeterminismdomain. approach based representation planning domain finitestate automaton. use Symbolic Model Checking techniques, particular Binary Decision Diagrams, compactly represent eciently search automaton. papermake following contributions. First, present general planning algorithmconformant planning, applies fully nondeterministic domains, uncertaintyinitial condition action effects. algorithm based breadth-first, backward search, returns conformant plans minimal length, solution planningproblem exists, otherwise terminates concluding problem admits conformantsolution. Second, provide symbolic representation search space based BinaryDecision Diagrams (Bdds), basis search techniques derived symbolicmodel checking. symbolic representation makes possible analyze potentially largesets states transitions single computation step, thus providing ecientimplementation. Third, present Cmbp (Conformant Model Based Planner), ecientimplementation data structures algorithm described above, directly basedBdd manipulations, allows compact representation search layersecient implementation search steps. Finally, present experimental comparison approach state-of-the-art conformant planners Cgp, QbfplanGpt. analysis includes planning problems distribution packagessystems, plus problems defined stress number specific factors. approach appears effective: Cmbp strictly expressive QbfplanCgp and, problems comparison possible, Cmbp outperformscompetitors, sometimes orders magnitude.1. Introductionrecent years, growing interest planning nondeterministic domains.Rejecting fundamental (and often unrealistic) assumptions classical planning, domains considered actions uncertain effects, exogenous events possible,initial state partly specified. challenge find strong plan,guaranteed achieve goal despite nondeterminism domain, regardlessuncertainty initial condition effect actions. Conditional planning (Cassandra, Kaelbling, & Littman, 1994; Weld, Anderson, & Smith, 1998; Cimatti,Roveri, & Traverso, 1998b) tackles problem searching conditional coursec 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiCimatti & Roveriactions, depends information gathered run-time. certain domains,however, run-time information gathering may expensive simply impossible. Conformant planning (Goldman & Boddy, 1996) problem finding unconditionedcourse actions, i.e. classical plan, depend run-time information gathering guarantee achievement goal. Conformant planning recognizedsignificant problem Artificial Intelligence since work Michie (1974): BlindRobot problem requires program activity sensorless agent, positioned location given room, guaranteed achieve givengoal. Conformant planning also seen problem control systemunobservable unknown state, microprocessor power-up, softwaresystem black-box testing.uncertainty, plan associated potentially many different executions,must taken account order guarantee goal achievement. makesconformant planning significantly harder classical planning (Rintanen, 1999a; De Giacomo & Vardi, 1999). Despite increased complexity, several approaches conformantplanning recently proposed, based (extensions of) main planning techniques classical planning. interesting Cgp (Smith & Weld, 1998) basedGraphplan, Qbfplan (Rintanen, 1999a) extends SAT-plan approachQBF, Gpt (Bonet & Geffner, 2000) encodes conformant planning heuristicsearch. paper, propose new approach conformant planning, based Symbolic Model Checking (McMillan, 1993). Symbolic Model Checking formal verificationtechnique, allows one analyze finite state automata high complexity, relyingsymbolic techniques, Binary Decision Diagrams (Bdds) (Bryant, 1986) particular,compact representation ecient search automaton. approach buildsplanning via model checking paradigm presented Cimatti colleagues (1997,1998b, 1998a), finite state automata used represent complex, nondeterministicplanning domains, planning based (extensions of) basic model checking steps.make following contributions.First, present general algorithm conformant planning, appliesnondeterministic domain uncertain action effects initial condition, expressednondeterministic finite-state automaton. algorithm performs breadth-firstsearch, exploring plans increasing length, plan found candidateplans available. algorithm complete, i.e. returns failureproblem admits conformant solution. problem admits solution,algorithm returns conformant plan minimal length.Second, provide symbolic representation search space based BinaryDecision Diagrams, allows application search techniques derivedsymbolic model checking. symbolic representation makes possible analyzesets transitions single computation step. sets compactly represented eciently manipulated despite potentially large cardinality.way possible overcome enumerative nature approachesconformant planning, degree nondeterminism tends limitingfactor.306fiConformant Planning via Symbolic Model CheckingThird, developed Cmbp (Conformant Model Based Planner), ecientimplementation data structures algorithm described above. Cmbp developed top Mbp, planner based symbolic model checking techniquesdeveloped Cimatti, Roveri Traveso (1998b, 1998a). Cmbp implements severalnew techniques, directly based Bdd manipulations, compact search layersoptimize termination checking.Finally, provide experimental evaluation state-of-the-art conformant planners, comparing Cmbp Cgp, Qbfplan Gpt. differenceexpressivity, problems tackled Cmbp also represented planners. However, problems direct comparisonpossible, Cmbp outperforms competitors. particular, features betterqualitative behavior, directly related number initial states uncertainaction effects, stable respect use heuristics.paper structured follows. Section 2 review representation (nondeterministic) planning domains finite state automata. Section 3 provideintuitions formal definition conformant planning setting. Section 4present planning algorithm, Section 5 discuss symbolic representationsearch space, allows ecient implementation. Section 6 presentCmbp planner, Section 7 present experimental results. Section 8discuss related work. Section 9 draw conclusions discuss futureresearch directions.2. Planning Domains Finite State Automatainterested complex, nondeterministic planning domains, actionspreconditions, conditional effects, uncertain effects, initial statepartly specified. rest paper, use simple though paradigmatic domainexplanatory purposes, variation Moore's bomb toilet domain (McDermott,1987) (from called BTUC | BT Uncertain Clogging). two packages,one contains armed bomb. possible dunk either packagetoilet (actions Dunk1 Dunk2 ), provided toilet clogged. Dunking eitherpackage uncertain effect clogging toilet. Furthermore, dunking packagecontaining bomb effect disarming bomb. action F lush effectunclogging toilet.represent domains finite state automata. Figure 1 depicts automatonBTUC domain. state given number, contains propositions holdingstate. instance, state 1 represents state bomb package 1,defused, toilet clogged. Given one bomb, write In2abbreviation negation In1 . Arrows states depict transitionsautomaton, representing possible behavior actions. transition state 2state 1 labeled F lush represents fact action F lush, executed state2, effect removing clogging. execution Dunk1 state 1,uncertain effect clogging toilet, represented multiple transitionsstates 5 6. Since transition outgoing state 2 labelled Dunk1 ,307fiCimatti & RoveriFlushFlushDunk_1In_1 5Defused!CloggedIn_1 1!Defused!CloggedDunk_1,Dunk_2Dunk_2FlushFlushIn_1 2!DefusedCloggedIn_1 6DefusedCloggedFlushFlushDunk_2In_2 7Defused!Clogged3In_2!Defused!CloggedDunk_1,Dunk_2Dunk_1FlushFlushIn_2 8DefusedCloggedIn_2 4!DefusedCloggedFigure 1: automaton BTUC domainstate 2 satisfy preconditions action Dunk1 , i.e. Dunk1 applicablestate 2.formally define nondeterministic planning domains follows.Definition 1 (Planning Domain) Planning Domain 4-tuple = (P ; ; A; R),P (finite) set atomic propositions, 2P set states,(finite) set actions, R transition relation.Intuitively, proposition state holds state. followingassume planning domain given. use s, s0 s00 denote states D,ff denote actions. R(s; ff; s0 ) holds iff executing action ff statestate s0 possible outcome. say action ff applicable iff leastone state s0 R(s; ff; s0 ) holds. say action ff deterministic iffunique state s0 R(s; ff; s0 ) holds. action ff uncertain outcomeleast two distinct states s0 s00 R(s; ff; s0 ) R(s; ff; s00 )hold. described Cimatti colleagues (1997), automaton given domaineciently built starting compact description given expressive high levelaction language, instance AR (Giunchiglia, Kartha, & Lifschitz, 1997).3. Conformant PlanningConformant planning (Goldman & Boddy, 1996) described problem findingsequence actions guaranteed achieve goal regardless nondeterminismdomain. is, possible initial states, uncertain action effects,execution plan results goal state.Consider following problem BTUC domain. Initially, bomb armedposition status toilet uncertain, i.e. initial statestates f1; 2; 3; 4g . goal reach state bomb defused, toilet308fiConformant Planning via Symbolic Model CheckingIn_1 1!Defused!CloggedIn_1 2!DefusedCloggedIn_2 3!Defused!CloggedIn_2 4!DefusedCloggedIn_1 5Defused!CloggedFlushFlushFlushFlushIn_1 5Defused!CloggedFlushFlushIn_1 1!Defused!CloggedDunk_1In_1 6DefusedCloggedIn_1 5Defused!CloggedDunk_2In_1 6DefusedCloggedIn_2 3!Defused!CloggedDunk_2In_2 7Defused!CloggedFlushIn_2 3!Defused!CloggedDunk_1In_2 3!Defused!CloggedIn_2 4!DefusedCloggedFlushIn_2 8DefusedCloggedFlushFlushFlushIn_1 5Defused!CloggedIn_2 7Defused!CloggedFlushFigure 2: conformant solution BTUC problemclogged, i.e. set goal states f5; 7g. conformant plan solving problemF lush; Dunk1 ; F lush ; Dunk2 ; F lush(1)Figure 2 outlines possible executions plan, possible initial statesuncertain action effects. initial uncertainty lies fact domain mightstates f1; 2; 3; 4g . possible initial states planning domaincollected set dashed line. call set belief state. Intuitively, beliefstate expresses condition uncertainty domain, collecting togetherstates indistinguishable point view agent reasoningdomain. first action, F lush, used remove possible clogging. reducesuncertainty belief state f1; 3g. Despite remaining uncertainty (i.e. stillknown package bomb is), action Dunk1 guaranteed applicableprecondition met states. Dunk1 effect defusing bombcontained package 1, uncertain effect clogging toilet. resultingbelief state f3; 4; 5; 6g . following action, F lush, removes clogging, reducinguncertainty belief state f3; 5g, guarantees applicability Dunk2 .Dunk2 , bomb guaranteed defused, toilet might clogged (states6 8 belief state f5; 6; 7; 8g ). final F lush reduces uncertainty beliefstate f5; 7g, guarantees achievement goal.general, order plan conformant solution, action must executedstates satisfy preconditions, state resultexecution plan (for initial states uncertain action effects)goal state. main diculty achieving conditions information(assumed be) available run-time. Therefore, planning time face problemreasoning action execution belief state, i.e. condition uncertainty.Definition 2 (Action Applicability) Let Bs Belief State. action ffapplicable Bs iff Bs 6= ; ff applicable every state 2 Bs.309fiCimatti & Roveriorder action applicable belief state, require preconditionsmust guaranteed notwithstanding uncertainty. words, reject \reckless"plans, take chance applying action without guarantee applicability.choice strongly motivated practical domains, possibly fatal consequencesfollow attempt apply action preconditions might satisfied(e.g. starting fix electrical device without sure powered). effectaction execution uncertain condition defined follows.Definition 3 (Action Image) Let Bs belief state, let ff action applicable Bs. image (also called execution) ff Bs, written Image [ff](Bs), definedfollows.Image [ff](Bs) =_fs0 j exists 2 Bs R(s; ff; s0 )gNotice image action combines uncertainty belief state uncertainty action effects. (Consider instance Image [Dunk1 ](f1; 3g)=f3; 4; 5; 6g .)following, write Image [ff](s) instead Image [ff](fsg).Plans elements , i.e. finite sequences actions. use 0-lengthplan, denote generic plans, ; plan concatenation. notionsapplicability image generalize plans follows.Definition 4 (Plan Applicability Image) Let 2 , let Bs . applicable Bs iff one following holds:1. = Bs 6= ;;2. = ff; , ff applicable Bs, applicable Image [ff](Bs).image (also called execution) Bs, written Image [](Bs), defined as:1. Image [](Bs) =_ Bs;2. Image [ff; ](Bs) =_ Image [](Image [ff](Bs));planning problem formally characterized set initial goal states.following definition captures intuitive meaning conformant plan given above.Definition 5 (Conformant Planning) Let = (P ; ; A; R) planning domain.Planning Problem triple (D; ; G ), ; 6= ; 6= G .plan conformant plan (that is, conformant solution to) planningproblem (D; ; G ) iff following conditions hold:(i) applicable ;(ii) Image [](I ) G .following, clear context, omit domain planningproblem, simply write (I ; G ).310fiConformant Planning via Symbolic Model Checking4. Conformant Planning Algorithmconformant planning algorithm based exploration space plans, limitingexploration plans conformant construction. algorithm builds Beliefstate-Plan (BsP) pairs form hBs : i, Bs non-empty belief stateplan. idea use BsP pair associate explored plan maximalbelief state applicable, guaranteed result goal states.exploration based basic function SPreImage [ff](Bs), that, given belief stateBs action ff, returns belief state containing states ff applicable,whose image ff contained Bs.Definition 6 (Strong Pre-Image) Let ; 6= Bs belief state let ffaction. strong pre-image Bs ff, written SPreImage [ff](Bs), definedfollows.SPreImage [ff](Bs) =_ fs j ff applicable s; Image [ff](s)BsgSPreImage [ff](Bs) empty, ff applicable it, conformant solution problem (SPreImage [ff](Bs); Bs). Therefore, plan conformantsolution problem (Bs; G ), plan ff; conformant solution problem(SPreImage [ff](Bs); G ).Figure 3 depicts space BsP pairs built algorithm solving BTUCproblem. levels built goal, right, towards initial states,left. level 0, BsP pair hf5; 7g : i, composed set goal states indexed0-length plan . (Notice conformant solution every problem goal setf5; 7g initial states contained f5; 7g.) dashed arrows represent applicationSPreImage . level 1, BsP pair hf5; 6; 7; 8g : F lushi built, since strongpre-image belief state 0 actions Dunk1 Dunk2 empty. level 2,three BsP pairs, (overlapping) belief states Bs2 , Bs3 Bs4 , indexed, respectively,length 2 plans Dunk1 ; F lush, F lush; F lush Dunk2 ; F lush. (A plan associatedbelief state Bsi sequence actions labeling path Bsi Bs0 .) NoticeBs3 equal Bs1 , therefore deserves expansion. expansionbelief states 2 4 gives belief states 5 6, obtained strong pre-imageF lush, strong pre-image actions Dunk1 Dunk2 returns emptybelief states. expansion Bs5 results three belief states. one resultingstrong pre-image F lush reported, since equal Bs5 . Belief state7 also equal Bs2 , deserves expansion. Belief state 8 obtainedexpanding Bs5 Bs6 . level 5, expansion produces Bs10 , containsinitial states. Therefore, corresponding plans conformant solutionsproblem.conformant planning algorithm ConformantPlan presented Figure 4.takes input planning problem form set states G (the domainassumed globally available). algorithm performs backwards breadth-firstsearch, exploring BsP pairs corresponding plans increasing length step.status search (each level Figure 3) represented BsP table, i.e. set BsPpairsBsPT = fhBs1 : 1 i; : : : ; hBsn : n ig311fiCimatti & RoveriLevel543210In_1 1!Defused!CloggedIn_1 1!Defused!CloggedDunk_1In_1 1!Defused!CloggedIn_1 5Defused!CloggedIn_1 2!DefusedCloggedIn_2 7Defused!CloggedIn_1 5Defused!CloggedIn_1 1!Defused!CloggedFlushIn_1 5Defused!CloggedDunk_1Bs 7In_1 6DefusedCloggedIn_1 2!DefusedCloggedIn_2 7Defused!CloggedBs 2In_2 3!Defused!CloggedIn_1 1!Defused!CloggedIn_2 4!DefusedCloggedIn_2 3!Defused!CloggedFlushIn_1 5Defused!CloggedIn_1 5Defused!CloggedIn_1 6DefusedCloggedIn_2 7Defused!CloggedIn_2 7Defused!CloggedIn_1 5Defused!CloggedDunk_2In_2 8DefusedCloggedIn_1 6DefusedCloggedBs 5In_2 3!Defused!CloggedFlushIn_1 6DefusedCloggedIn_2 7Defused!CloggedIn_2 7Defused!CloggedIn_2 8DefusedCloggedIn_2 8DefusedCloggedFlushIn_1 5Defused!CloggedIn_2 7Defused!CloggedBs 0Dunk_1Bs 8In_1 5Defused!CloggedIn_2 4!DefusedCloggedBs 3Bs 17In_2Defused!CloggedIn_1 5Defused!CloggedIn_2 3!Defused!Clogged8In_2DefusedCloggedBs 10In_2 3!Defused!CloggedIn_1 5Defused!CloggedFlushIn_1 6DefusedCloggedDunk_2In_1 5Defused!CloggedDunk_2In_2 7Defused!CloggedIn_2 7Defused!CloggedBs 4In_2 7Defused!CloggedIn_2 8DefusedCloggedBs 9Bs 6Figure 3: BsP tables BTUC problemplans length, 6= j 1 j 6=i n.call Bsi belief set indexed . ambiguity arises, write BsPT(i )Bsi. array BsPTables used store BsP tables representing levelssearch. algorithm first checks (line 4) plans length 0, i.e.solution. conformant plan length exists ((P lans = ;) line 4),loop entered. iteration, conformant plans increasing length explored(lines 5 8). step line 6 expands BsP table BsPTables[i 1] storesresulting BsP table BsPTables[i]. BsP pairs redundant respectcurrent search eliminated BsPTables[i] (line 7). possible solutions containedBsPTables[i] extracted stored P lans (line 8). loop terminates eitherplan found (P lans 6= ;), space conformant plans completely explored(BsPTables[i] = ;).definitions basic functions used algorithm reported Figure 5.function ExpandBsPTable expands BsP table provided argument, containingconformant plans length 1, returns BsP table conformant plans lengthi. BsP input BsP table expanded ExpandBsPPair. possible312fiConformant Planning via Symbolic Model Checking012345678910111213function ConformantPlan(I ,G )begin= 0;BsPTables[0] := f hG : g;Plans := ExtractSolution(I ; BsPTables[0]);((BsPTables[i] 6= ;) ^ (P lans = ;)):= + 1;BsPTables[i] := ExpandBsPTable(BsPTables[i-1]);BsPTables[i] := PruneBsPTable(BsPTables[i]; BsPTables; i);Plans := ExtractSolution(I ; BsPTables[i]);done(BsPTables[i] = ;)return Fail;else return Plans;endFigure 4: conformant planning algorithm.action ff, strong pre-image Bs computed, resulting belief state Bs0empty, i.e. belief state ff guarantees achievement Bs,plan extended ff hBs0 : ff; returned. expansion BsP tableunion expansions BsP pair. function ExtractSolution takesinput BsP table returns (possibly empty) set plans index belief statescontaining . PruneBsPTable takes input BsP table pruned, arraypreviously constructed BsP tables BsPTables, index current step. removesBsP table input plans worth exploredcorresponding belief states already visited.algorithm following properties. First, always terminates. followsfact set explored belief sets (stored BsPTables) monotonicallyincreasing | step proceed least one new belief state generated.finiteness (the set accumulated belief states contained 2Sfinite), fix point eventually reached. Second, correct, i.e. plan returnedconformant solution given problem. correctness algorithm followsproperties SPreImage : plan associated belief stateconformant, i.e. guaranteed applicable resultsbelief state contained goal. Third, algorithm optimal, i.e. returns plansminimal length. property follows breadth-first style search. Finally,algorithm able decide whether problem admits solution, returning Failcases. Indeed, conformant solution always associated belief state containinginitial states. SPreImage generates maximal belief state associated conformantplan, new belief state generated exploration compared initial statescheck solution, plan pruned equivalent plan alreadyexplored.313fiCimatti & Roveri(BsPT) =_ExpandBsPTable[hBs : i2BsPT(hBs : i)ExpandBsPPair(hBs : i) =_ fhBs0 : ff; ij Bs0 = SPreImage [ff](Bs) 6= ;gExpandBsPPair(BsPT; BsPTables; i) =_fhBs : 2 BsPT j j < i; hBs : 0 2 BsPTables[j ] (Bs0 = Bs)gPruneBsPTable(I ; BsPT) =_ f j exists hBs : 2 BsPT BsgExtractSolutionFigure 5: primitives used conformant planning algorithm.5. Conformant Planning via Symbolic Model CheckingModel checking formal verification technique based exploration finite stateautomata (Clarke, Emerson, & Sistla, 1986). Symbolic model checking (McMillan, 1993)particular form model checking using Binary Decision Diagrams compactly representeciently analyze finite state automata. introduction symbolic techniquesmodel checking led breakthrough size model could analyzed (Burchet al., 1992), made possible model checking routinely applied industry,especially logic circuits design (for survey see Clarke & Wing, 1996).rest section, provide overview Binary Decision Diagrams,describe representation planning domains, based Bdd-basedrepresentation finite state automata used model checking. Then, discussextension allows symbolically represent BsP tables transformations, thusallowing ecient implementation algorithm described previous section.5.1 Binary Decision DiagramsReduced Ordered Binary Decision Diagram (Bryant, 1992, 1986) (improperly called Bdd)directed acyclic graph (DAG). terminal nodes either rue F alse. nonterminal node associated boolean variable, two Bdds, called left rightbranches. Figure 6 (a) depicts Bdd (a1 $ b1 ) ^ (a2 $ b2 ) ^ (a3 $ b3 ).non-terminal node, right [left, respectively] branch depicted solid [dashed, resp.]line, represents assignment value rue [F alse, resp.] correspondingvariable. Bdd represents boolean function. given truth assignment variablesBdd, value function determined traversing graph rootleaves, following branch indicated value assigned variables1.1. path root leaf visit nodes associated subset variables Bdd.See instance path associated a1 ; :b1 Figure 6(a).314fiConformant Planning via Symbolic Model Checkinga1a1b1a2b1a3a2b2b2b1b1a3b1b2a3a2a3b1b1b2a3b1b2b1b2b3b3b3b3TrueFalseTrueFalse(a)b1(b)Figure 6: Two Bdds formula (a1 $ b1 ) ^ (a2 $ b2 ) ^ (a3 $ b3 ).reached leaf node labeled resulting truth value. v Bdd, size jvjnumber nodes. n node, var(n) indicates variable indexing node n.Bdds canonical representation Boolean functions. canonicity followsimposing total order < set variables used label nodes,node n respective non-terminal child m, variables must ordered, i.e. var(n) <var(m), requiring Bdd contains isomorphic subgraphs.Bdds combined usual boolean transformations (e.g. negation, conjunction, disjunction). Given two Bdds, instance, conjunction operator buildsreturns Bdd corresponding conjunction arguments. Substitution alsorepresented Bdd transformations. following, v variable,Bdds, indicate [v= ] Bdd resulting substitution v .v1 v2 vectors (the number of) distinct variables, indicate [v1 =v2 ]parallel substitution variables vector v1 (corresponding) variablesv2 .Bdds also allow transformations described quantifications, style Quantified Boolean Formulae (QBF). QBF definitional extension propositional logic,propositional variables universally existentially quantified. terms Bddcomputations, quantification corresponds tranformation mapping Bddvariable vi quantified Bdd resulting (propositional) formula.formula, vi one variables, existential quantification vi , written9vi:(v1 ; : : : ; vn ), equivalent (v1; : : : ; vn )[vi=F alse] _ (v1; : : : ; vn )[vi=T rue]. Analogously, universal quantification 8vi :(v1 ; : : : ; vn ) equivalent (v1 ; : : : ; vn )[vi =F alse]^315fiCimatti & Roveri(v1 ; : : : ; vn )[vi =T rue]. QBF, quantifiers arbitrarily applied nested. general, QBF formula equivalent propositional formula, conversion subjectexponential blow-up.time complexity algorithm computing truth-functional boolean transformation f1 <op> f2 O(jf1 j jf2 j). far quantifications concerned, timecomplexity quadratic size Bdd quantified, linear numbervariables quantified, i.e. O(jvj jf j2 ) (Bryant, 1992, 1986).Bdd packages ecient implementations data structures algorithms (Braceet al., 1990; Somenzi, 1997; Yang et al., 1998; Coudert et al., 1993). Basically, Bdd package deals single multi-rooted DAG, node represents boolean function.Memory eciency obtained using \unique table", sharing common subgraphsBdds. unique table used guarantee time isomorphic subgraphs redundant nodes multi-rooted DAG. creatingnew node, unique table checked see node already present,case new node created stored unique table. unique tableallows perform equivalence check two Bdds constant time (since twoequivalent functions always share subgraph) (Brace et al., 1990; Somenzi, 1997).Time eciency obtained maintaining \computed table", keeps trackresults recently computed transformations, thus avoiding recomputation.critical computational factor Bdds order variables used. (Figure 6shows example impact change variable ordering size Bdd.)certain class boolean functions, size corresponding Bdd exponentialnumber variables possible variable ordering (Bryant, 1991). many practicalcases, however, finding good variable ordering rather easy. Beside affecting memoryused represent Boolean function, finding good variable ordering big impactcomputation times, since complexity transformation algorithms dependssize operands. Bdd packages provide heuristic algorithms finding goodvariable orderings, called try reduce overall size stored Bdds.reordering algorithms also activated dynamically package, Bddcomputation, total number nodes package reaches predefined threshold(dynamic reoredering).5.2 Symbolic Representation Planning Domainsplanning domain (P ; ; A; R) represented symbolically using Bdds, follows.set (distinct) Bdd variables, called state variables, devoted representationstates domain. variables direct association propositiondomain P used description domain. instance, BTUCdomain, In1 , Defused Clogged associated unique Bdd variable.following write x vector state variables. particular orderirrelevant performance issues, rest section distinguishproposition corresponding Bdd variable.state set propositions P (specifically, propositions intendedhold it). state s, corresponding assignment state variablesx , i.e. assignment variable corresponding proposition p 2 assigned316fiConformant Planning via Symbolic Model Checkingrue, variable assigned F alse. represent Bdd (s),assignment unique satisfying assignment. instance, (6) =_ (In1 ^Defused ^ Clogged) Bdd representing state 6, (4) =_ :In1 ^ :Defused ^Clogged represents state 4, on. (Without loss generality, followingdistinguish propositional formula corresponding Bdd.) representationnaturally extends set states Q follows:(Q) =__ (s)s2Qwords, associate set states generalized disjunction Bddsrepresenting states. Notice satisfying assignments (Q)exactly assignment representations states Q. representation mechanismnatural. instance, Bdd (I ) representing set initial statesBTUC =_ f1; 2; 3; 4g :Defused, set goal states G =_ f5; 7gcorresponding Bdd Defused ^ :Clogged. Bdd also used represent setstates domain automaton. BTUC, (S ) = rue = 2P .different formulation, two independent propositions In1 In2 used representposition bomb, (S ) would Bdd In1 $ :In2 .general, Bdd represents set (states correspond to) models.consequence, set theoretic transformations naturally represented propositionaloperations, follows.(SnQ)=_ (S ) ^ : (Q)(Q1 [ Q2 ) =_ (Q1 ) _ (Q2 )(Q1 \ Q2 ) =_ (Q1 ) ^ (Q2 )main eciency symbolic representation lies fact cardinalityrepresented set directly related size Bdd. instance, (G ) usestwo (non-terminal) nodes represent two states, (I ) uses one node represent fourstates. limit cases, (S ) (fg) (the leaf Bdds) rue F alse, respectively.advantage, symbolic representation extremely ecient dealing irrelevantinformation. Notice, instance, variable Defused occurs (f5; 6; 7; 8g ).reason, symbolic representation dramatic improvement explicit,enumerative representation. allows symbolic, Bdd-based model checkershandle finite state automata large number states (see instance Burchet al., 1992). following, collapse set states Bdd representing it.Another set Bdd variables, called action variables, written ff , used representactions. use one action variable possible action A. Intuitively, Bdd actionvariable true corresponding action executed. assumesequential encoding used, i.e. concurrent actions allowed, also use Bdd,Seq(ff ), express exactly one action variables must true time2 .2. specific case sequential encoding, alternative approach using dlog jAje possible:assignment action variables denotes specific action executed. Two assignmentsmutually exclusive, constraint Seq(ff ) needs represented. cardinalityset actions power two, standard solution associate one assignmentcertain values. optimized solution, actually used implementation, describedsake simplicity.317fiCimatti & RoveriBTUC problem, contains three actions, use three Bdd variables Dunk1 ,Dunk2 F lush, express serial encoding constraint following Bdd:Seq(ff) =_ (Dunk1 _ Dunk2 _ F lush) ^:(Dunk1 ^ Dunk2 ) ^:(Dunk1 ^ F lush) ^:(Dunk2 ^ F lush)state variables, referring Bdd action variables symbolic namessake simplicity. practice, internally represented integers,position ordering Bdd package totally irrelevant logical terms.Bdd variables x ff represents set state-action pairs, i.e. relationstates actions. instance, applicability relation BTUC (i.e.,actions possible states, except dunking actions require toiletclogged) represented Bdd :(Clogged ^ (Dunk1 _ Dunk2 )). Noticerepresents set 16 state-action pairs, associating state applicable action.transition 3-tuple composed state (the initial state transition),action (the action executed), state (the resulting state transition).represent transitions, another vector x 0 Bdd variables, called next state variables,allocated Bdd package. write 0 (s) representation statenext state variables. 0 (Q) denote construction Bdd correspondingset states Q, using variable next state vector x 0 instead currentstate variables x . require jx j = jx 0 j, assume i-th variable xi-th variable x 0 correspond. define representation set states nextvariables follows.0 (s) =_ (s)[x =xx0 ]call operation [x =xx0 ] \forward shifting", transforms representationset \current" states representation set \next" states. dual operation[x 0 =xx] called backward shifting. following, call x current state variablesdistinguish next state variables. transition represented assignmentx , ff x 0 . BTUC, transition corresponding application actionDunk1 state 1 resulting state 5 represented following Bdd(h1; Dunk1 ; 5i) =_ (1) ^ Dunk1 ^ 0 (5)transition relation R automaton corresponding planning domainsimply set transitions, thus represented Bdd Bdd variables x , ffx0 , satisfying assignment represents possible transition._(R) =_ Seq(ff ) ^ (t)t2Rrest paper, assume Bdd representation planning domaingiven. particular, assume given vectors variables x ;xx0 ;ffff, encodingfunctions 0 , simply call , R, G Bdd representing statesdomain, transition relation, initial states goal states, respectively. write(v) stress Bdd depends variables v. representation,possible reason plans, simulating symbolically execution sets actionssets states, means QBF transformations. Bdd representing applicabilityrelation directly obtained following computation.ff) =_ 9x 0 :R(x ;ffff;xx0 )Applicable(x ;ff318fiConformant Planning via Symbolic Model Checkingresulting Bdd, Applicable(x ;ffff), represents set state-action pairsaction applicable state. Bdd representing states reachable Qone step obtained following computation.9x:9ff:(R(x ;ffff;xx0 )^Q(x))[x 0=xx]Notice that, single operation, symbolically simulate effect applicationapplicable action states Q. Similarly, following transformation allows symbolically compute SPreImage set states Q possibleactions one single computation:8x0:(R(x ;ffff;xx0 ) ! Q(x )[x=xx0 ]) ^(x ;ffff)Applicableresulting Bdd represents state-action pairs hx : ffi ff applicablex execution ff x results states Q.5.3 Symbolic Search Space Belief Statesmain strength symbolic approach allows perform symbolic breadthfirst search, provides way compactly representing eciently expandingfrontier. instance, plans constructed symbolic breadth-first searchspace states, repeatedly applying strong pre-image goal states (Cimatti et al.,1998b). However, machinery presented previous section cannot directly appliedtackle conformant planning. basic difference conformant planningsearching space belief states3 , therefore frontier search basicallyset sets states. introduce way symbolically represent BsP tables. Basically,seen construction demand, based algorithm steps, increasinglylarge portions space belief states. key intuition BsP tablefhfs11 ; : : : ; s1n1 g : 1i; : : : ; hfsk1 ; : : : ; skn g : k igkrepresented relation plans (of length) states, associatingplan directly state belief state indexed plan, follows:fhs11 : 1i; : : : ; hs1n1 : 1i; : : : ; hsk1 : k i; : : : ; hskn : k igk(2)use additional variables represent plans BsP tables. order representplans increasing length, step algorithm, vector new Bdd variables,called plan variables, introduced. vector plan variables introduced i-th stepalgorithm written [i], j [i]j = jff j, used encode i-th last actionplan4. step one algorithm, introduce vector plan variables [1]represent action corresponding 1-length possible conformant plan. BsP3. principle, machinery symbolic search could used conformant planning applieddeterminization domain automaton, i.e. automaton 2S state space. However,would require introduction exponential number state variables, impracticaleven small domains.4. search performed backwards, plans need reversed found.319fiCimatti & Roveritable BsPT1 level 1 built ExpandBsPTable performing following Bddcomputation starting BsP table level 0, i.e. G (x ):(8x 0 :(R(x ;ffff;xx0 ) ! G (x )[x =xx0 ]) ^ Applicable(x ;ffff))[ff = [1]]computation collects state-action pairs hx : ff (the action representedby) ff applicable (the state represented by) x , resulting (statesrepresented by) x 0 goal states. replace vector action variables fffirst vector plan variables [1]. resulting Bdd, BsPT(x ; [1]), represents BsPtable containing plans length one form relation states plans(2). general case, step 1, BsP table BsPTi 1 , associating belief statesplans length 1, represented Bdd state variables x planvariables [i 1] ; : : : ; [1]. computation performed ExpandBsPTable stepimplemented following Bdd transformation BsPTi 1(8x 0 :(R(x ;ffff;xx0 ) ! BsPTi 1 (x ; [i 1]; : : : ; [1] )[x =xx0 ]) ^ Applicable(x ;ffff))[ff = [i]](3)next state variables R BsPTi 1 (resulting forward shifting) disappearuniversal quantification. action variables ff renamed newlyintroduced plan variables [i], next step algorithm constructionrepeated.ExtractSolution extracts assignments plan variables corresponding set contains initial states. terms Bdd transformations, ExtractSolutionimplemented follows:8x:(I (x) ! BsPTi(x; [i]; : : : ; [1]))(4)result Bdd plan variables [i]; : : : ; [1]. Bdd F alse,solutions length i. Otherwise, satisfying assignments resulting Bddrepresents conformant solution problem.guarantee termination algorithm, step BsP table returnedExpandBsPTable simplified PruneBsPTable removing belief statesdeserve expansion. requires comparison belief statescontained BsP table belief states contained BsP tables builtprevious levels. one crucial steps terms eciency. earlier implementation step logical Bdd transformations, following directly set-theoreticaldefinition PruneBsPTable, extremely inecient (Cimatti & Roveri, 1999). Furthermore, noticed serial encoding could yield BsP tables containing largenumber equivalent plans, indexing exactly belief state. Often equivalent plans differ order independent actions, potential sourcecombinatorial explosion. occurs even simple version BTUC (in Figure 3,two equivalent conformant plans associated Bs8 ). Therefore, developed newimplementation could tackle two problems operating directly BsPtable. idea depicted Figure 7. Initially, cache contains Bs1 , Bs2 Bs3 .simplification performs traversal Bdd, accumulating subtrees representingbelief states, comparing ones built previous levels, inserting newones cache (in Figure 7, Bs4 , Bs5 Bs6 ). time path identified320fiConformant Planning via Symbolic Model CheckingBsP TableBs4Bs2Bs5Pruned BsP TableBs6Bs4Bs5Cached Belief StatesBs1Bs2Bs6Cached Belief StatesBs3Bs1Bs2Bs3Bs4Bs5Bs6Figure 7: example pruning BsP tablerepresents plan indexing already cached belief state, plan redundantcorresponding path pruned5. cost simplification linear size BsPsimplified highly effective pruning.6. CMBP: BDD-based Conformant PlannerCmbp (Conformant Model Based Planner) conformant planner implementing datastructures algorithms conformant planning described previous sections. Cmbpinherits features Mbp (Cimatti et al., 1997, 1998b, 1998a), planner basedsymbolic model checking techniques. Mbp built top NuSMV, symbolic modelchecker jointly developed ITC-IRST CMU (Cimatti et al., 2000), usesCUDD (Somenzi, 1997) state-of-the-art Bdd package. Mbp two-stage system.first stage, internal Bdd-based representation domain built,second stage planning problems solved. Currently, planning domains describedmeans high-level action language AR (Giunchiglia et al., 1997). AR allowsspecify (conditional uncertain) effects actions means high level assertions.instance, Figure 8 shows AR description BTUC problem6. semanticsAR yields serial encoding, i.e. exactly one action assumed executed5. pruning mechanism actually weaker earlier one (Cimatti & Roveri, 1999).require belief state must expanded twice search, earlierversion prune belief states contained previously explored ones. may increase numberexplored belief states. However, allows much ecient implementation, without impactingproperties algorithm.6. ! & stand negation conjunction, respectively. description slightly edited sakereadability. particular, Mbp currently accept parameterized AR descriptions. practiceuse script language generate ground instances different complexity parameterized problemdescription.321fiCimatti & RoveriDOMAIN BTUCACTIONS Dunk_1, Dunk_2, Flush;FLUENTS In_1, In_2, Defused, Clogged : boolean;INERTIAL Clogged, Defused, In_1, In_2;ALWAYS In_1 <-> !In_2;Flush CAUSES !Clogged;[1, 2] {Dunk_<i> PRECONDITIONS !Clogged;Dunk_<i> CAUSES Defused In_<i>;Dunk_<i> POSSIBLY CHANGES Clogged;}INITIALLY !Defused;CONFORMANT Defused & !Clogged;Figure 8: AR description BTUC problemtime. automaton corresponding AR description obtained meansminimization procedure Giunchiglia (1996). procedure solves frame problemramification problem, eciently implemented Mbp (Cimatti et al., 1997).separation domain construction planning phases, Mbpbound AR. Standard deterministic domains specified Pddl (Ghallab et al.,1998) also given Mbp means (prototype) compiler. also startinginvestigate potential use C action language (Giunchiglia & Lifschitz, 1998),allows represent domains parallel actions.Different planning algorithms applied specified planning problems.operate solely automaton representation, completely independentparticular language used specify domain. Mbp allows automatic constructionconditional plans total observability, implementing algorithms strong planning (Cimatti et al., 1998b), strong cyclic plannig (Cimatti et al., 1998a; Daniele,Traverso, & Vardi, 1999). Cmbp, implemented ideas described previoussections. primitives construct prune BsP tables required lot tuning,particular ordering Bdd variables. found general ordering strategyworks reasonably well: action variables positioned top ordering, followedplan variables, followed state variables, current state next state variables interleaved. specific ordering within action variables, plan variables, state variablesdetermined standard mechanism implemented NuSMV. Cmbp implements severalalgorithms conformant planning. addition backward algorithm presented322fiConformant Planning via Symbolic Model CheckingSection 4, Cmbp implements algorithm based forward search, allows exploitinitial knowledge problem, sometimes resulting significant speed ups (Cimatti& Roveri, 2000). Backward forward search also combined, tackle exponential growth search time depth search. algorithms,different options enable disable different versions termination check.7. Experimental Evaluationsection present experimental evaluation approach, carriedcomparing Cmbp state-of-the-art conformant planners. first describeconformant planners considered analysis, present experimentalcomparison carried out.7.1 Conformant PlannersCgp (Smith & Weld, 1998) extends ideas Graphplan (Blum & Furst, 1995, 1997)deal uncertainty. Basically, planning graph built every possible sequence possible worlds, constraints among planning graphs propagated ensure conformance.Cgp system takes input domains described extension Pddl (Ghallab et al.,1998), possible specify uncertainty initial state. Cgp inheritsGraphplan ability deal parallel actions. Cgp first ecient conformant planner: shown outperform several planners Buridan (Peot,1998) UDTPOP (Kushmerick, Hanks, & Weld, 1995). detailed comparison reported Smith Weld (1998) leaves doubt superiority Cgp respectsystems. Therefore, compared Cmbp Cgp considersystems analyzed Smith Weld (1998). Cmbp expressive Cgp tworespects. First, Cgp handle uncertainty initial state. instance, Cgpcannot analyze BTUC domain presented Section 3. Smith Weld (1998) describeapproach extended actions uncertain effects. Second, Cgp cannotconclude planning problem conformant solutions.Qbfplan (our name for) planning system Rintanen (1999a). Qbfplan generalizes idea SAT-based planning (Kautz, McAllester, & Selman, 1996; Kautz & Selman,1996, 1998) nondeterministic domains, encoding problems QBF. Qbfplanapproach limited conformant planning, used conditional planninguncertainty, also partial observability: different encodings, correspondingdifferent structures resulting plan, synthesized. paper,considering encodings enforce resulting plan sequence. Given boundlength plan, first QBF encoding problem generated, QBFsolver (Rintanen, 1999b) called. solution found, new encoding longer planmust generated solved. Qbfplan able handle actions uncertain effects.done introducing auxiliary (choice) variables, assignments different possible outcomes actions correspond. variables universally quantifiedensure conformance solution. Differently e.g. Blackbox (Kautz & Selman,1998), Qbfplan heuristic guess \right" length plan. Givenlimit length plan, generates encodings specified length,repeatedly calls QBF solver encodings increasing length plan found.323fiCimatti & RoveriCgp, Qbfplan cannot conclude planning problem conformant solutions.Similarly Cmbp, Qbfplan relies symbolic representation problem, althoughQBF transformations performed theorem prover rather Bdds.Gpt (Bonet & Geffner, 2000) general planning framework, conformantplanning problem seen deterministic search problem space belief states. Gptuses explicit representation search space, belief state representedseparate data structure. search based algorithm (Nilsson, 1980),driven domain dependent heuristics automatically generated problemdescription. Gpt accepts problem descriptions syntax based Pddl, extended dealprobabilities uncertainty. possible represent domains uncertain actioneffects (although representation actions resulting large number different statesrather awkward). planning algorithm, Gpt able conclude givenplanning problem conformant solution exhaustively exploring space beliefstates.7.2 Experiments Resultsevaluation performed running systems number parameterized problem domains. considered problems Cgp Gpt distributions, plusproblems defined test specific features planners. considereddomains uncertainty limited initial state, domains uncertain actioneffects. Besides problems admitting solution, also considered problems admittingsolution, case measured effectiveness plannner returningfailure.Given different expressivity, possible run systemsexamples. Cmbp run classes examples, Gpt run one.Cgp run problems admit solution, uncertainty limitedinitial condition. Qbfplan run examples encodingalready available Qbfplan distribution. subset problemsexpressible Cgp. main limiting factor low level input formatQbfplan: problem descriptions must specified ML code generates QBFencodings. Writing new encodings turned dicult task, especially duelack documentation.ran Cgp, Qbfplan Cmbp Intel 300MHz Pentium-II, 512MB RAM,running Linux. comparison Cmbp Gpt run Sun Ultra Sparc270MHz, 128Mb RAM running Solaris (Gpt available binary). However,performance two machines comparable | run times Cmbp almostidentical. CPU time limited 7200 sec (two hours) test. avoid swapping,memory limit fixed physical memory machine. following,write \|" \===" test complete within time memorylimits, respectively. performance systems reported tables listingsearch time. excludes time needed Qbfplan generate encodings,time spent Cmbp construct automaton representation Bdd, timeneeded Gpt generate source code internal representation, compileit. Overall, significant time ignored automaton construction Cmbp.324fiConformant Planning via Symbolic Model CheckingCurrently, automaton construction fully optimized. Even complexexamples, however, construction never required couple minutes7.7.2.1 Bomb ToiletBomb Toilet. first domain tackled classical bomb toilet,notion clogging. call problem BT(p), parameter pnumber packages. uncertainty initial condition,known package contains bomb. goal defuse bomb. resultsBT problem shown Table 1. columns relative Cmbp lengthplan (jPj), number cached belief states number hits cache (#BS#NBS respectively), time (expressed seconds) needed searching automatonPentium/Linux (Time(L)) Sparc/Solaris (Time(S)). following,clear context, execution platform omitted. columns relative Cgpnumber levels planning graphs (jLj) search time. column relativeGpt search time.BT(2)BT(3)BT(4)BT(5)BT(6)BT(7)BT(8)BT(9)BT(10)jPj2345678910Cmbp#BS/#BSH2/26 / 1114 / 3630 / 10362 / 266126 / 641254 / 1496510 / 34631022 / 7862Time(L)0.0000.0000.0000.0000.0100.0100.0300.0700.150Time(S)0.0000.0000.0000.0000.0100.0300.0300.0700.140jLj111111111CgpTime0.0000.0000.0000.0000.0100.0100.0200.0200.020GptTime0.0740.0770.0800.0870.1020.1390.2300.4811.018Table 1: Results BT problems.BT problem intrinsically parallel, i.e. depth planning graph alwaysone, packages dunked time. Cgp inherits Graphplan ability deal parallel actions eciently, therefore almost insensitiveproblem size. problem Cgp outperforms Cmbp Gpt. Noticenumber levels explored Cgp always 1, length plan producedCmbp Cgp grows linearly. Cmbp performs slightly better Gpt.Bomb Toilet Clogging. call BTC(p) extension BT(p)dunking package (always) clogs toilet, ushing remove clogging, clogging precondition dunking package. Again, p number packages. toiletinitially clogged. modification, problem longer allows parallelsolution. results problem listed Table 2. impact depthplan length becomes significant systems. Cmbp Gpt outperform Cgp.case Cmbp performs better Gpt, especially large instances (see BTC(16)).7. precisely, maximum time building automaton required BMTC(10,6) examples(88 secs.), RING(10) example (77 secs.), BMTC(9,6) examples (40 secs.), BMTC(10,5)examples (41 secs.). examples, time required automaton constructionless 10 seconds.325fiCimatti & RoveriQbfplanBTC(2)BTC(3)BTC(4)BTC(5)BTC(6)BTC(7)BTC(8)BTC(9)BTC(10)jPj35791113151719CmbpCgp#BS/#BSH Time(L) Time(S) jLj Time6/80.0000.010 30.00014 / 230.0000.000 50.01030 / 610.0100.010 70.03062 / 1500.0200.020 90.130126 / 3470.0200.020 110.860254 / 7960.0700.080 132.980510 / 18440.1500.160 15 13.6901022 / 41490.3200.330 17 41.0102046 / 91900.7100.700 19 157.590BTC(16) 31 131070 / 92135599.20099.800GptTime0.0740.0770.0820.0940.1130.1660.2880.6071.309351.457BTC(6)jPj Time10.0020.0130.2640.6351.5362.8276.80814.06935.591093.3411 (+) 2.48BTC(10)Time10.0220.0330.7842.3054.8768.90722.61852.729156.1210410.8611 1280.8813 3924.9614|jPj::::::18|19 (+) 16.84Table 2: Results BTC problems.comparison Qbfplan limited 6 10 package instances (the ones available distribution package). performance Qbfplan reported lefttable Table 2. line reports time needed decide whether planlength i. performance Qbfplan rather good tackling encoding admitting solution (in Table 2 entries labeled (+)). instance, BTC(10)Qbfplan finds solution solving encodings depth 19 reasonably fast. However,solution cannot found, i.e. QBF formula admits model, performanceQbfplan degrades significantly (for depth 18 encoding, let solver run 10CPU hours complete search). difference performance,diculty writing new domains, rest comparison considerQbfplan.Bomb Multiple Toilets. next domain, called BMTC(p,t), generalizationBTC problem case multiple toilets (p number packages,number toilets). problem becomes parallelizable numbertoilets increases. Furthermore, considered three versions problem increasinguncertainty initial states. first class tests (\Low Uncertainty" columns),uncertainty position bomb unknown, toilets knownclogged. \Mid Uncertainty" \High Uncertainty" columns show resultspresence uncertainty initial state. second [third, respectively] classtests, status every odd [every, resp.] toilet either clogged clogged.increases number possible initial states.results reported Table 3 (for comparison Cgp) Table 4(for comparison Gpt). column represents number initial statescorresponding problem. Cgp able fully exploit parallelism problem.However, Cgp never able explore 9 levels planning graph, depthdecreasing number initial states. results also show Cmbp Gptmuch less sensitive number initial states Cgp. increasing initial326fi(p,t)(2,2)(3,2)(4,2)(5,2)(6,2)(7,2)(8,2)(9,2)(10,2)(2,3)(3,3)(4,3)(5,3)(6,3)(7,3)(8,3)(9,3)(10,3)(2,4)(3,4)(4,4)(5,4)(6,4)(7,4)(8,4)(9,4)(10,4)(2,5)(3,5)(4,5)(5,5)(6,5)(7,5)(8,5)(9,5)(10,5)(2,6)(3,6)(4,6)(5,6)(6,6)(7,6)(8,6)(9,6)(10,6)bmtc234567891023456789102345678910234567891023456789102468101214161823579111315172346810121416234579111315234568101214jPjLow UncertaintyCmbp#BS/#BSH Time10 / 18 0.00026 / 84 0.00058 / 250 0.020122 / 652 0.030250 / 1552 0.070506 / 3586 0.1801018 / 8262 0.4002042 / 18484 0.9404090 / 40676 1.82018 / 42 0.00047 / 202 0.010110 / 736 0.030237 / 2034 0.080492 / 5106 0.2301003 / 12128 0.5602026 / 27836 1.3004073 / 62470 3.3308168 / 138046 7.28029 / 75 0.01092 / 492 0.020206 / 1686 0.060457 / 4987 0.190964 / 12456 0.4101983 / 29453 1.0404026 / 68466 2.7408117 / 153895 6.69016304 / 339160 14.42043 / 117 0.010164 / 1031 0.040416 / 4304 0.150872 / 11763 0.4901875 / 31695 1.3003901 / 78009 3.9907974 / 183036 9.67016142 / 416333 24.25032501 / 927329 54.91060 / 168 0.010270 / 1848 0.070786 / 9294 0.3001777 / 29075 1.1603613 / 71123 3.2907625 / 180127 9.06015726 / 429198 20.71032012 / 986188 50.61064675 / 2.21106e+06 111.830Time0.0000.0200.0301.3903.490508.510918.960|0.0100.0100.1100.1700.3406248.010|Cgp327Mid UncertaintyCmbp#BS/#BSH Time412 / 34 0.000628 / 106 0.000860 / 286 0.02010124 / 702 0.03012252 / 1614 0.08014508 / 3662 0.190161020 / 8362 0.430182044 / 18602 0.960204092 / 40810 1.990824 / 99 0.0001256 / 349 0.02016120 / 942 0.04020248 / 2335 0.11024504 / 5520 0.25028101 / 12673 0.59032204 / 28530 1.35036408 / 63331 3.37040818 / 139092 7.460829 / 75 0.00012108 / 808 0.03016236 / 2356 0.08020492 / 5888 0.230241004 / 13648 0.470282028 / 31004 1.120324076 / 70584 2.870368172 / 15654 6.9004016364 / 34234 14.6301643 / 117 0.01024212 / 2008 0.08032475 / 6375 0.26040987 / 15928 0.700482011 / 37759 1.890564059 / 86716 4.480648155 / 195055 10.5907216347 / 432408 25.6008032731 / 948279 56.4201660 / 168 0.01024270 / 1848 0.07032920 / 13810 0.500401958 / 37636 1.940484005 / 90111 4.080568100 / 208050 10.1306416291 / 469277 22.62072 32674 / 1.04173e+06 53.51080 65441 / 2.28585e+06 116.4400.0200.2900.730|1 0.2001 0.8302 30.6302 30.1402 57.3002|1 0.1302 3.5402 6.3202 37,9592|12220.0900.2000.990|223323455Time0.0100.0400.46013,180|CgpjLjTable 3: Results BMTC problems.10.00010.01010.01030.50031.16032.41038.5404|10.01010.02010.02010.05035.9203 18.4103 62.0403 194.6403 289,68010.01010.01010.04010.06010.1003 211.7203 1015.1603 3051.9902|133557771133354jLjHigh UncertaintyCmbp#BS/#BSH Time812 / 40 0.0001228 / 112 0.0101660 / 294 0.01020124 / 710 0.04024252 / 1622 0.08028508 / 3670 0.190321020 / 8372 0.450362044 / 18612 0.950404092 / 40820 2.0301624 / 126 0.0102456 / 373 0.02032120 / 972 0.04040248 / 2371 0.12048504 / 5562 0.240561016 / 12721 0.640642040 / 28584 1.330724088 / 63391 3.390808184 / 139158 7.4303248 / 332 0.02048112 / 960 0.04064240 / 2532 0.09080496 / 6092 0.240961008 / 13876 0.4701122032 / 31260 1.1601284080 / 70912 2.9101448176 / 156904 6.97016016368 / 342736 14.7706493 / 751 0.03096224 / 2591 0.120128480 / 6740 0.260160992 / 16393 0.7301922016 / 38334 1.9802244064 / 87411 4.5402568160 / 195880 10.64028816352 / 433373 25.37032032736 / 949394 56.290128171 / 1533 0.040192448 / 6248 0.310256960 / 16344 0.6903201984 / 39710 2.1203844032 / 92772 4.6004488128 / 211370 10.40051216320 / 473328 23.000576 32704 / 1.04658e+06 54.010640 65472 / 2.29158e+06 116.2401.6108.69032.190|0.1700.690|Time0.03013.560145.830|Cgp2 337.6042 1459.1102 5643.4502|2 21.1202 138.4302 551.2102 1523.8402|22232232444jLjConformant Planning via Symbolic Model CheckingfiCimatti & Roveribmtc(p,t)(2,2)(3,2)(4,2)(5,2)(6,2)(7,2)(8,2)(9,2)(10,2)(2,4)(3,4)(4,4)(5,4)(6,4)(7,4)(8,4)(9,4)(10,4)(2,6)(3,6)(4,6)(5,6)(6,6)(7,6)(8,6)(9,6)(10,6)Low Unc.CmbpTime0.0000.0100.0000.0400.0800.1900.3900.9101.8500.0000.0100.0500.1800.3701.0802.7008.97014.2100.0100.0500.3101.1103.4008.91021.24049.880113.680GptTime0.0790.0870.1050.1460.2270.4410.9222.2115.1690.1090.1560.2700.6161.4353.4848.76723.85859.9660.3030.5621.3543.2578.69125.67768.427289.000486.969High Unc.CmbpTime0.0100.0100.0200.0400.0700.2000.4000.9501.9000.0100.0400.1000.2400.4601.1902.8306.920114.6900.0600.2600.6202.0604.66010.43023.86054.190118.590GptTime0.0790.0910.1210.1980.3760.8501.9664.74310.6200.1210.2841.0163.2829.37427.34872.344180.039440.3080.4822.47117.40674.623243.113701.431===Table 4: Results BMTC problems.uncertainty, Cgp almost unable solve trivial problems. Gpt performs betterCgp, suffers explicit representation search space.Bomb Toilet Uncertain Clogging. BTUC(p) domain domaindescribed Section 2, clogging uncertain outcome dunking package.kind problem cannot expressed Cgp. results Cmbp Gpt reportedTable 5. Although Cmbp performs better Gpt (by factor two three),significant difference behavior. interesting compare results CmbpBTC BTUC problems. Gpt slight difference noticeable, resultingincreased branching factor search space due uncertainties effectsaction executions. performance Cmbp, number uncertainties directfactor | example, BTC(16) BTUC(16), performance almost same.7.2.2 Ring RoomsSimple Ring Room. considered another domain, robot movering rooms. room window, either open, closed locked.robot move (either clockwise counterclockwise), close window roomis, lock closed. goal windows locked.328fiConformant Planning via Symbolic Model CheckingCmbpjPjBTUC(2)BTUC(3)BTUC(4)BTUC(5)BTUC(6)BTUC(7)BTUC(8)BTUC(9)BTUC(10)BTUC(16)#BS/#BSH6/814 / 2330 / 6162 / 150126 / 347254 / 796510 / 18441022 / 41492046 / 9190131070 / 9213553579111315171931Time0.0000.0000.0100.0100.0300.0500.1700.3100.72098.270GptTime0.0760.0780.0850.0980.1280.2050.3800.8121.828486.252Table 5: Results BTUC problems.N-1N12problem RING(r), r number rooms, uncertaintyinitial condition: position robot status windowsuncertain. problems parallel solution, large number initialstates (r 3r ), corresponding full uncertainty position robotstatus window. results8 reported left Table 6. Cmbp outperformsRING(2)RING(3)RING(4)RING(5)RING(6)RING(7)RING(8)RING(9)RING(10)jPj5811141720232629Cmbp#BS/#BSH8 / 2426 / 7880 / 240242 / 726728 / 21842186 / 65586560 / 1968019682 / 5904659048 / 177144Time0.0000.0200.0400.1200.3701.4204.95027.330106.870jLj34CgpTime0.070|GptTime0.0850.0870.3921.1506.62023.636105.158===124816Cgp RING(5)jLj55555Time0.0100.0600.4206.150|jLj99999Time0.0200.1401.950359.680|Table 6: results RING problems.Cgp Gpt, although Gpt performs much better Cgp. Cgp Gptsuffer increasing complexity problem. right Table 6, plot (forRING(5) problem) dependency Cgp number initial states combinednumber levels explored (different goals provided requireexploration different levels). clear number initial states depthsearch critical factors Cgp.8. times reported Cgp refer scaled-down version problem, locking takenaccount, thus maximum number initial states r 2r .329fiCimatti & RoveriRing Rooms Uncertain Action Effects. considered variationRING domain, called URING, first introduced Cimatti Roveri (1999),expressible Cgp. window locked robot performing actiondetermine status (e.g. closing it), window open close nondeterministically. instance, robot moving room 1 room 2, windowsroom 3 4 could open closed wind. domain clearly designed stressability planner deal actions large number resulting states.worst case (e.g. move action performed window locked), 2r possibleresulting states. Although seemingly artificial, captures fact environmentspractice highly nondeterministic. tried compare Cmbp Gpt URINGproblem. principle Gpt able deal uncertainty action effects. However,failed codify URING Gpt language, requires conditional description uncertain effects. Therefore, experimented variation RINGdomain featuring higher degree nondeterminism, called NDRING following.NDRING domain contains increasing number additional propositions, calledfollowing noninertial propositions, initially unknown nondeterministicallyaltered action. number noninertial propositions, action 2iNDRING(2)NDRING(3)NDRING(4)NDRING(5)NDRING(6)NDRING(7)NDRING(8)NDRING(9)NDRING(10)jPj5811141720232629Cmbp#BS/#BSH8 / 2426 / 7880 / 240242 / 726728 / 21842186 / 65586560 / 1968019682 / 5904659048 / 177144Time (5)0.0000.0200.0400.1100.3501.3504.99027.060103.760Time (2)0.1400.2561.0464.55018.758108.854===GptTime (3)0.3840.6793.02512.96057.300===Time (4)0.9482.57412.54848.426===Time (5)4.54413.96067.714===Table 7: results NDRING problems.possible outcomes. results listed Table 7, columns labeled Time(i).growing branching factor search major impact performanceGpt, Cmbp insensitive kind uncertainty. (The performance Cmbplower number noninertial propositions reported basicallysame.)URING problem run Cmbp. results listed Table 8.noticed performances Cmbp improve significantly respect RINGproblem. explained considering that, despite larger number transitions,number explored belief states significantly smaller (see Bs cache statisticsTables 6 8).7.2.3 Square Cubefollowing domains SQUARE(n) CUBE(n) Gpt distribution (Bonet& Geffner, 2000). problems consist robot navigating square cube siden. domains actions moving robot possible directions.Moving robot boundary leaves robot position. original330fiConformant Planning via Symbolic Model CheckingURING(2)URING(3)URING(4)URING(5)URING(6)URING(7)URING(8)URING(9)URING(10)jPj5811141720232629Cmbp#BS/#BSH5 / 1611 / 3423 / 7047 / 14295 / 286191 / 574383 / 1150767 / 23021535 / 4606Time0.0000.0100.0200.0400.0800.1900.4100.9802.2300Table 8: Results URING problems.problems, called CORNER following, require robot reach corner, startingcompletely unspecified position. introduced two variations. first, calledFACE, initial position position given side [face] square [cube],goal reach central position opposite side [face]. second, calledCENTER, initial position completely unspecified, goal centersquare [cube]. corner problem, simple heuristic perform steps towardscorner, thus pruning half actions. variations designed allowsimple heuristic | instance, CENTER problem, action eliminated.SQUARE(i)SQUARE(2)SQUARE(4)SQUARE(6)SQUARE(8)SQUARE(10)SQUARE(12)SQUARE(14)SQUARE(16)SQUARE(18)SQUARE(20)CUBE(i)CUBE(2)CUBE(3)CUBE(4)CUBE(5)CUBE(6)CUBE(7)CUBE(8)CUBE(9)CUBE(10)CUBE(15)jPj36912151821242742jPj261014182226303438CORNERCmbp#BS/#BSH Time2 / 4 0.00015 / 37 0.00035 / 93 0.00063 / 173 0.02099 / 277 0.030143 / 405 0.050195 / 557 0.070255 / 733 0.080323 / 933 0.120399 / 1157 0.160CORNERCmbp#BS/#BSH Time6 / 19 0.00026 / 99 0.01063 / 261 0.020124 / 537 0.040215 / 957 0.050342 / 1551 0.100511 / 2349 0.160728 / 3381 0.330999 / 4677 0.4403374 / 16167 1.940GptTime0.3320.1680.4300.2760.5000.5671.0821.7652.0689.207GptTime0.0740.0800.0920.1150.1490.1960.2610.3570.5030.638jPj361114192227303554jPj271217222732374247FACECmbp#BS/#BSH Time2 / 4 0.00033 / 83 0.00086 / 232 0.020163 / 453 0.040264 / 746 0.090389 / 1111 0.150538 / 1548 0.230711 / 2057 0.320908 / 2638 0.5401129 / 3291 0.650GptTime0.0580.0650.0890.1390.2280.3710.5820.9081.3431.883jPj281420263238445056CENTERCmbp#BS/#BSH Time2 / 4 0.00076 / 190 0.010218 / 592 0.040432 / 1210 0.090718 / 2044 0.1901076 / 3094 0.3601506 / 4360 0.5602008 / 5842 0.8202582 / 7540 1.3303228 / 9454 1.790GptTime0.0600.0830.2160.6952.1355.34012.28426.24152.09194.204FACECENTERCmbpGptCmbpGpt#BS/#BSH Time Time jPj #BS/#BSH Time Time6 / 19 0.000 0.061 36 / 19 0.010 0.06126 / 99 0.000 0.069 626 / 99 0.010 0.144319 / 1360 0.050 0.193 12722 / 3091 0.130 0.569709 / 3095 0.220 0.412 151696 / 7402 0.430 2.0101343 / 6116 0.430 1.479 21 3365 / 15432 0.910 10.7172255 / 10377 0.840 3.323 24 5797 / 26814 1.860 34.0743519 / 16464 1.400 8.161 30 9248 / 43541 3.520 109.8525169 / 24331 2.810 16.272 33 13786 / 65237 7.260 701.9107279 / 34564 4.550 32.226 39 19667 / 93898 9.990===26439 / 127825 28.560=== 60 74041 / 359354 58.930Table 9: Results SQUARE CUBE problems.results problems reported Table 9. tests runCmbp Gpt. experiments highlight eciency Gpt strongly dependsquality heuristic function. If, first set experiments, heuristics331fiCimatti & Roverieffective, Gpt almost good Cmbp. Otherwise, Gpt degrades significantly.general, finding heuristics effective belief space appears nontrivialproblem. Cmbp appears stable9 , performs blind, breadth-first search,relies cleverness symbolic representation achieve eciency.7.2.4 OmeletteFinally, considered OMELETTE(i) problem (Levesque, 1996). goalgood eggs bad ones one two bowls capacity i. unlimited numbereggs, unpredictably good bad. eggs grabbed brokenbowl. content bowl discarded, poured bowl. Breakingrotten egg bowl effect spoiling bowl. bowl always cleaneddiscarding content. problem originally presented partial observabilityproblem, sensing action allowing test bowl spoiled not. consideredvariation problem without sensing action: case conformant solutionexists. used OMELETTE problems test ability Cmbp Gpt discoverproblem admits conformant solution. results reported Table 10.table shows Cmbp effective checking absence conformant solution,outperforms Gpt several orders magnitude.OMELETTE(3)OMELETTE(4)OMELETTE(5)OMELETTE(6)OMELETTE(7)OMELETTE(8)OMELETTE(9)OMELETTE(10)OMELETTE(15)OMELETTE(20)OMELETTE(30)# steps911131517192123334363CMBP#BS/#BSH15 / 3419 / 4223 / 5027 / 5831 / 6635 / 7439 / 8243 / 9063 / 13083 / 170123 / 250Time0.0200.0300.0400.0500.0600.0900.1100.1200.2100.4400.890GPTTime0.2370.5821.4182.9045.18910.30718.74432.623225.530===Table 10: Results OMELETTE problems.7.3 Summarizing RemarksOverall, Cmbp appears implement effective approach conformant planning,terms expressivity performance. Cgp able deal uncertaintiesinitial states, cannot conclude problem admit conformantsolution. main problem Cgp seems enumerative approach uncertainties,increased number initial states severely affects performance (see Table 3Table 6).Qbfplan principle able deal uncertain action effects, cannot concludeproblem admit conformant solution. small number ex9. Consider also problems increasingly dicult (see instance plan length).332fiConformant Planning via Symbolic Model Checkingperiments could perform, approach implemented Qbfplan limitedSatplan style search: intermediate results obtained solving encodingdepth k reused solving encodings increasing depth. Furthermore,solver appears specialized finding model, rather proving unsatisfiability.However, latter ability needed encodings final one.Gpt expressive system, allows eciently dealing wide classplanning problems. far conformant planning concerned, expressiveCmbp. allows dealing uncertain action effects, conclude problemconformant solution. However, Cmbp appears outperform Gptseveral respects. First, behaviour Gpt appears directly related numberpossible outcomes action. Furthermore, eciency Gpt dependseffectiveness heuristic functions, sometimes dicult devise,cannot help problem admit solution.main strength Cmbp independence number uncertainties,achieved use symbolic techniques. fully symbolic, Cmbpexhibit enumerative behaviour competitors. Compared original approachdescribed Cimatti Roveri (1999), substantial improvement performanceobtained new implementation pruning step. disclaimerorder. well known Bdd based computations subject blow-up memoryrequirements computing certain classes boolean functions, e.g. multipliers (Bryant,1986). would trivial make example performance Cmbp degradesexponentially. However, none examples considered, includedexamples distribution Cgp Gpt, phenomenon occurred.8. Related Workterm conformant planning first introduced Goldman (1996), presentingformalism constructing conformant plans based extension dynamic logic. Recently, Ferraris Giunchiglia (2000) presented another conformant planner based SATtechniques. system available direct comparison Cmbp. effectiveness approach dicult evaluate, limited testing described (Ferraris &Giunchiglia, 2000). performance claimed comparable Cgp. However,results reported enconding corresponding solution, behaviourQbfplan reported Table 2 suggests kind analysis might limited.Several works share idea planning based automata theory. closelyrelated works lines planning via model checking (Cimatti et al., 1997), uponwork based. approach allows, instance, automatically constructuniversal plans guaranteed achieve goal finite number steps (Cimattiet al., 1998b), implement trial-and-error strategies (Cimatti et al., 1998a; Danieleet al., 1999). results obtained hypothesis total observability,run-time observation available. main difference substantial extension required lift symbolic techniques search space belief states. DeGiacomo Vardi (1999) analyze several forms planning automata theoreticframework. Goldman, Musliner Pelican (2000) present method model checkingtimed automata interleaved plan formation activity, make sure333fiCimatti & Roveritiming constraints met. Finally, Hoey colleagues (1999) use algebraic decisiondiagrams tackle problem stochastic planning.9. Conclusions Future Workpaper presented new approach conformant planning, based useSymbolic Model Checking techniques. algorithm general, appliescomplex planning domains, uncertainty initial condition action effects,described finite state automata. algorithm based breadthfirst, backward search, returns conformant plans minimal length, solutionplanning problem exists. Otherwise, terminates failure. algorithm designedtake full advantage symbolic representation based Bdds. implementationapproach Cmbp system highly optimized, particular crucialstep termination checking. performed experimental comparison approachstate art conformant planners Cgp, Qbfplan Gpt. Cmbp strictlyexpressive Qbfplan Cgp. problems comparisonpossible, Cmbp outperformed competitors terms run times, sometimesorders magnitude. Thanks use symbolic data structures, Cmbp able dealeciently problems large numbers initial states action outcomes.hand, qualitative behavior Cgp Gpt seems depend heavilyenumerative nature algorithms. Differently Gpt, Cmbp independenteffectiveness heuristic used drive search.research presented paper extended following directions. First,investigating alternative approach conformant planning, breadthfirst style search given up. techniques appear extremely promising |preliminary experiments led speed ups two orders magnituderesults presented paper problems admit solution. Second, tackleproblem conditional planning partial observability, hypothesislimited amount information acquired run time. conformant planning,problem seen search belief space. However, appears significantlycomplicated need dealing run-time observation conditional plans. Finally, considering extension domain construction plannerexpressive input language, C , invariant detection techniques.AcknowledgementsFausto Giunchiglia provided continuous encouragement feedback work.thank Piergiorgio Bertoli, Blai Bonet, Marco Daniele, Hector Geffner, Enrico Giunchiglia,Jussi Rintanen, David Smith, Paolo Traverso, Dan Weld valuable discussions conformant planning various comments paper. David Smith provided codeCgp, large number examples, time-out mechanism used experimentalevaluation. Jussi Rintanen made Qbfplan available Linux.334fiConformant Planning via Symbolic Model CheckingReferencesBlum, A. L., & Furst, M. L. (1995). Fast planning planning graph analysis.Proc. Ijcai.Blum, A. L., & Furst, M. L. (1997). Fast planning planning graph analysis. Artificial Intelligence 1{2, 90, 279{298.Bonet, B., & Geffner, H. (2000). Planning Incomplete Information Heuristic Search Belief Space. Chien, S., Kambhampati, S., & Knoblock, C. (Eds.), 5thInternational Conference Artificial Intelligence Planning Scheduling, pp. 52{61. AAAI-Press.Brace, K., Rudell, R., & Bryant, R. (1990). Ecient Implementation BDD Package. 27th ACM/IEEE Design Automation Conference, pp. 40{45 Orlando, Florida.ACM/IEEE, IEEE Computer Society Press.Bryant, R. E. (1986). Graph-Based Algorithms Boolean Function Manipulation. IEEETransactions Computers, C-35 (8), 677{691.Bryant, R. E. (1991). complexity VLSI implementations graph representationsBoolean functions application integer multiplication. IEEE TransactionsComputers, 40 (2), 205{213.Bryant, R. E. (1992). Symbolic Boolean manipulation ordered binary-decision diagrams. ACM Computing Surveys, 24 (3), 293{318.Burch, J. R., Clarke, E. M., McMillan, K. L., Dill, D. L., & Hwang, L. J. (1992). SymbolicModel Checking: 1020 States Beyond. Information Computation, 98 (2),142{170.Cassandra, A., Kaelbling, L., & Littman, M. (1994). Acting optimally partially observablestochastic domains. Proc. AAAI-94. AAAI-Press.Cimatti, A., Clarke, E., Giunchiglia, F., & Roveri, M. (2000). NuSMV : new symbolicmodel checker. International Journal Software Tools Technology Transfer(STTT), 2 (4).Cimatti, A., Giunchiglia, E., Giunchiglia, F., & Traverso, P. (1997). Planning via ModelChecking: Decision Procedure AR. Steel, S., & Alami, R. (Eds.), ProceedingFourth European Conference Planning, No. 1348 Lecture Notes ArtificialIntelligence, pp. 130{142 Toulouse, France. Springer-Verlag. Also ITC-IRST TechnicalReport 9705-02, ITC-IRST Trento, Italy.Cimatti, A., & Roveri, M. (1999). Conformant Planning via Model Checking. Biundo,S. (Ed.), Proceeding Fifth European Conference Planning, Lecture NotesArtificial Intelligence Durham, United Kingdom. Springer-Verlag. Also ITC-IRSTTechnical Report 9908-01, ITC-IRST Trento, Italy.335fiCimatti & RoveriCimatti, A., & Roveri, M. (2000). Forward Conformant Planning via Symbolic ModelChecking. Proceeding AIPS2k Workshop Model-Theoretic ApproachesPlanning Breckenridge, Colorado.Cimatti, A., Roveri, M., & Traverso, P. (1998a). Automatic OBDD-based GenerationUniversal Plans Non-Deterministic Domains. Proceeding Fifteenth NationalConference Artificial Intelligence (AAAI-98) Madison, Wisconsin. AAAI-Press.Also IRST-Technical Report 9801-10, Trento, Italy.Cimatti, A., Roveri, M., & Traverso, P. (1998b). Strong Planning Non-DeterministicDomains via Model Checking. Proceeding Fourth International ConferenceArtificial Intelligence Planning Systems (AIPS-98) Carnegie Mellon University,Pittsburgh, USA. AAAI-Press.Clarke, E. M., & Wing, J. M. (1996). Formal methods: State art future directions.ACM Computing Surveys, 28 (4), 626{643.Clarke, E., Emerson, E., & Sistla, A. (1986). Automatic verification finite-state concurrent systems using temporal logic specifications. ACM Transactions ProgrammingLanguages Systems, 8 (2), 244{263.Coudert, O., Madre, J. C., & Touati, H. (1993). TiGeR Version 1.0 User Guide. DigitalParis Research Lab.Daniele, M., Traverso, P., & Vardi, M. Y. (1999). Strong Cyclic Planning Revisited.Biundo, S. (Ed.), Proceeding Fifth European Conference Planning, LectureNotes Artificial Intelligence Durham, United Kingdom. Springer-Verlag.De Giacomo, G., & Vardi, M. (1999). Automata-Theoretic Approach Planning Temporally Extended Goals. Biundo, S. (Ed.), Proceeding Fifth European Conference Planning, Lecture Notes Artificial Intelligence Durham, United Kingdom.Springer-Verlag.Ferraris, P., & Giunchiglia, E. (2000). Planning satisfiability nondeterministic domains. Proceedings Seventeenth National Conference Artificial Intelligence(AAAI'00) Austin, Texas. AAAI Press.Ghallab, M., Howe, A., Knoblock, C., McDermott, D., Ram, A., Weld, D., & Wilkins,D. (1998). PDDL | Planning Domain Definition Language. Tech. rep. CVCTR-98-003/DCS TR-1165, Yale Center Computational Vision Control.Giunchiglia, E. (1996). Determining Ramifications Situation Calculus. FifthInternational Conference Principles Knowledge Representation Reasoning(KR'96) Cambridge, Massachusetts. Morgan Kaufmann Publishers.Giunchiglia, E., Kartha, G. N., & Lifschitz, V. (1997). Representing action: Indeterminacyramifications. Artificial Intelligence, 95 (2), 409{438.336fiConformant Planning via Symbolic Model CheckingGiunchiglia, E., & Lifschitz, V. (1998). action language based causal explanation:Preliminary report. Proceedings 15th National Conference Artificial Intelligence (AAAI-98) 10th Conference Innovative Applications ArtificialIntelligence (IAAI-98), pp. 623{630 Menlo Park. AAAI Press.Goldman, R. P., Musliner, D. J., & Pelican, M. J. (2000). Using Model CheckingPlan Hard Real-Time Controllers. Proceeding AIPS2k Workshop ModelTheoretic Approaches Planning Breckenridge, Colorado.Goldman, R., & Boddy, M. (1996). Expressive Planning Explicit Knowledge.Proceedings 3rd International Conference Artificial Intelligence PlanningSystems (AIPS-96), pp. 110{117. AAAI Press.Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). Spudd: Stochastic planning using decision diagrams. Proceedings Fifteenth Conference UncertaintyArticial Intelligence (1999), pp. 279{288. AAAI Press.Kautz, H., & Selman, B. (1998). BLACKBOX: New Approach ApplicationTheorem Proving Problem Solving. Working notes Workshop PlanningCombinatorial Search Pittsburgh, PA, USA.Kautz, H. A., McAllester, D., & Selman, B. (1996). Encoding Plans Propositional Logic.Proc. KR-96.Kautz, H. A., & Selman, B. (1996). Pushing Envelope: Planning, Propositional Logic,Stochastic Search. Proc. AAAI-96.Kushmerick, N., Hanks, S., & Weld, D. S. (1995). algorithm probabilistic planning.Artificial Intelligence, 76 (1-2), 239{286.Levesque, H. J. (1996). planning presence sensing?. ProceedingsThirteenth National Conference Artificial Intelligence Eighth InnovativeApplications Artificial Intelligence Conference, pp. 1139{1146 Menlo Park. AAAIPress / MIT Press.McDermott, D. (1987). critique pure reason. Computational Intelligence, 3 (3), 151{237.McMillan, K. (1993). Symbolic Model Checking. Kluwer Academic Publ.Michie, D. (1974). Machine Intelligence Edinburgh. Machine Intelligence, pp.143{155. Edinburgh University Press.Nilsson, N. (1980). Principles Artificial Intelligence. Morgan Kaufmann Publishers, Inc.,Los Altos, CA.Peot, M. (1998). Decision-Theoretic Planning. Ph.D. thesis, Dept. Engineering-EconomicSystems | Stanford University.Rintanen, J. (1999a). Constructing conditional plans theorem-prover. JournalArtificial Intellegence Research, 10, 323{352.337fiCimatti & RoveriRintanen, J. (1999b). Improvements Evaluation Quantified Boolean Formulae.Dean, T. (Ed.), 16th Iinternational Joint Conference Artificial Intelligence, pp.1192{1197. Morgan Kaufmann Publishers.Smith, D. E., & Weld, D. S. (1998). Conformant graphplan. Proceedings 15thNational Conference Artificial Intelligence (AAAI-98) 10th ConferenceInnovative Applications Artificial Intelligence (IAAI-98), pp. 889{896 MenloPark. AAAI Press.Somenzi, F. (1997). CUDD: CU Decision Diagram package | release 2.1.2. DepartmentElectrical Computer Engineering | University Colorado Boulder.Weld, D. S., Anderson, C. R., & Smith, D. E. (1998). Extending graphplan handleuncertainty sensing actions. Proceedings 15th National ConferenceArtificial Intelligence (AAAI-98) 10th Conference Innovative Applications Artificial Intelligence (IAAI-98), pp. 897{904 Menlo Park. AAAI Press.Yang, B., Bryant, R. E., O'Hallaron, D. R., Biere, A., Coudert, O., Janssen, G., Ranjan,R. K., & Somenzi, F. (1998). performance study BDD-based model checking.Proceedings Formal Methods Computer-Aided Design, pp. 255{289.338fiJournal Artificial Intelligence Research 13 (2000) 1-31Submitted 8/99; published 8/00Space Efficiency Propositional Knowledge RepresentationFormalismsMarco Cadolicadoli@dis.uniroma1.itDipartimento di Informatica e SistemisticaUniversita di Roma La SapienzaVia Salaria 113, I-00198, Roma, ItalyFrancesco M. Doninidonini@dis.uniroma1.itPolitecnico di BariDipartimento di di Elettrotecnica ed ElettronicaVia Orabona 4, I-70125, Bari, ItalyPaolo LiberatoreMarco Schaerfliberato@dis.uniroma1.itschaerf@dis.uniroma1.itDipartimento di Informatica e SistemisticaUniversita di Roma La SapienzaVia Salaria 113, I-00198, Roma, ItalyAbstractinvestigate space efficiency Propositional Knowledge Representation (PKR)formalism. Intuitively, space efficiency formalism F representing certain pieceknowledge , size shortest formula F represents . paperassume knowledge either set propositional interpretations (models) setpropositional formulae (theorems). provide formal way talking relativeability PKR formalisms compactly represent set models set theorems.introduce two new compactness measures, corresponding classes, showrelative space efficiency PKR formalism representing models/theorems directlyrelated classes. particular, consider formalisms nonmonotonic reasoning,circumscription default logic, well belief revision operators stablemodel semantics logic programs negation. One interesting result formalismstime complexity necessarily belong space efficiency class.1. Introductionlast years large number formalisms knowledge representation (KR)proposed literature. formalisms studied several perspectives, including semantical properties, computational complexity. investigatespace efficiency, property minimal size needed represent certain piece knowledge given formalism. study motivated factpiece knowledge represented two formalisms using different amountspace. Therefore, else remaining same, formalism could preferred anotherone needs less space store information.definition space efficiency, however, simple. Indeed, formalism may allowseveral different ways represent piece knowledge. example, let us assumewant represent piece knowledge today Monday. Propositionalc2000AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiCadoli, Donini, Liberatore, & SchaerfLogic may decide use single propositional variable monday. fact todayMonday represented formula monday, also formula monday,well monday (rain rain), formulae Propositional Logiclogically equivalent monday represent exactly information.Propositional Logic, consider shortest equivalent formulae usedrepresent information have. principle applied genericformalism: allows several formulae represent information, takeaccount shortest one. Therefore, say space efficiency formalism Frepresenting certain piece knowledge size shortest formula Frepresents . Space efficiency also called succinctness compactness formalismmeasure ability representing knowledge small amount space.paper focus propositional KR (PKR) formalisms. giveformal definition formalisms propositional one not: intuitively,propositional formalism, quantifications allowed, thus formulaesyntactically bounded formed using propositional connectives, pluskind nonclassical connectives (for instance, negation logic programs, etc.).far, discussed knowledge represents. possible way thinkpiece knowledge represents facts inferred it. words,knowing something knowing everything logically implied.second way cases natural think piece knowledgeset states world consider possible.formal way, say knowledge represented either set propositional interpretations (those describing states world consider plausible) setformulae (those implied know). Consequently, focus reasoningproblems model checking theorem proving. following example showsreally think knowledge ways.Example 1 want eat fast food, want either sandwich salad(but both), either water coke (but both).Propositional Logic, choice represented model, followingmodels represent possible choices (models represented writing lettersmapped true).= {{sandwich, water}, {sandwich, coke}, {salad, water}, {salad, coke}}representing set choices use formulae instead models. case,write set formulae whose models represent exactly allowed choices,follows.C = (sandwich salad) (sandwich salad) (sandwich salad)(water coke) (water coke) (coke water)Actually, get rid redundancies, end following formula.F = (sandwich salad) (sandwich salad) (water coke) (water coke)2fiSpace Efficiency Propositional Knowledge Representation Formalismsformally, F represents set models A, interpretation I,holds |= F . formula F also represents set formulae C,Cn(F ) = Cn(C), Cn(.) function gives set conclusionsdrawn propositional formula.1.1 State Artquestion deeply investigated, related space efficiency,possibility translating formula expressed one formalism formula expressedanother formalism (under assumption, course, formulae representknowledge).cases, analysis possibility translating formulae differentformalisms Propositional Logic (PL). example, Ben-Eliyahu Dechter (1991, 1994)proposed translation default logic PL, translation disjunctive logicprograms PL, Winslett (1989) introduced translation revised knowledgebases PL, Gelfond, Przymusinska, Przymusinskyi (1989) defined translationcircumscription PL.translations, well many ones literature, leadexponential increase size formula, worst case. best knowntranslation yields formula target formalism exponential size w.r.t.formula source formalism, natural question arising whether exponentialblow due specific translation, intrinsic problem. example,although proposed translations default logic PL lead exponential blowup, cannot conclude possible translations suffer problem: couldpolynomial translation exists, discovered far.works focussed question whether kind exponential increasesize intrinsic not. Cadoli, Donini, Schaerf (1996) shown many interesting fragments default logic circumscription cannot expressed polynomialtime fragments PL without super-polynomially increasing size formulae.proved super-polynomial increase size necessary translatingunrestricted propositional circumscription (Cadoli, Donini, Schaerf, & Silvestri, 1997)operators belief revision PL (Cadoli, Donini, Liberatore, & Schaerf, 1999;Liberatore, 1995).Gogic collegues (1995) analyzed relative succinctness several PKR formalismsrepresenting sets models. Among results, showed skeptical default logicrepresent sets models succinctly circumscription.Kautz, Kearns, Selman (1995) Khardon Roth (1996, 1997) consideredrepresentations knowledge bases based notion characteristic model, comparingrepresentations, e.g., based clauses. showed representationknowledge bases characteristic models sometimes exponentially compactones, converse true cases.However, results based specific proofs, tailored specific reduction, help us define equivalence classes space efficiency KRformalisms. recent paper (Cadoli, Donini, Liberatore, & Schaerf, 1996b), new complexity measure decision problems, called compilability, introduced.3fiCadoli, Donini, Liberatore, & Schaerfpresent paper show new measure directly used characterize spaceefficiency PKR formalisms. emphasize methodological aspects, expressinggeneral context many results presented before.1.2 Goalnotion polynomial time complexity great importance KR (as well manyfields computer science), problems solved polynomial timeconsidered easy, computational point view.notion polynomial many-one reducibility also intuitive meaningapplied KR: exists polynomial many-one reduction one formalismanother one, time complexity reasoning two formalisms comparable.allows say, e.g., inference PL coNP-complete, i.e. one hardestproblems among complexity class coNP.result, formal tool comparing difficulty reasoning twoformalisms. missing way saying one formalism able representinformation less space.Example 2 consider lunch scenario previous example. showreduce size representation using circumscription instead PropositionalLogic. PL, knowledge previous example represented formula F :F = (sandwich salad) (sandwich salad) (water coke) (water coke)set models formula A, models exactly minimalmodels formula Fc defined follows.Fc = (sandwich salad) (water coke)definition circumscription (McCarthy, 1980) holds F equivalentCIRC(Fc ; {sandwich, salad, water, coke}, , ). Note Fc shorter F . resultproved hold arbitrary sets models, may conclude circumscriptionspace efficient Propositional Logic representing knowledge expressed setsmodels.goal provide formal way talking relative ability PKR formalisms compactly represent information, information either set modelsset theorems. particular, would like able say specific PKRformalism provides one compact ways represent models/theorems amongPKR formalisms specific class.1.3 Resultsintroduce two new compactness measures (model theorem compactness)corresponding classes (model-C thm-C, C complexity class like P, NP, coNP,etc.). classes form two hierarchies isomorphic polynomial-time hierarchy(Stockmeyer, 1976). show relative space efficiency PKR formalism4fiSpace Efficiency Propositional Knowledge Representation Formalismsdirectly related classes. particular, ability PKR formalism compactlyrepresent sets models/theorems directly related class model/theoremhierarchy belongs to. Problems higher model/theorem hierarchy representsets models/theorems compactly formalisms lower classes.classification obtained general framework making directcomparisons specific translations various PKR formalisms. Furthermore,approach also allows simple intuitive notion completeness modeltheorem hierarchies. notion precisely characterizes relationformalisms different levels, relations formalisms level.interesting result two PKR formalisms model checking inference belongtime complexity class may belong different compactness classes. maysuggest criterion choosing two PKR formalisms reasoningtime complexitynamely, choose compact one. Also, two PKR formalismsmay belong theorem compactness class, yet different model compactnessclasses. stresses importance clarifying whether one wants represent modelstheorems choosing PKR formalism.1.4 Outlinenext section introduce notation assumptions adoptwork. Section 3 (Compilability) briefly recall notions non-uniform computation important follows recall basic definitions compilabilityclasses (Cadoli et al., 1996b). Section 4 (Reductions) describe constraintsimpose reductions, Section 5 (Space Efficiency) introduce compactnessclasses. Section 6 (Applications) actually compare many known PKR formalismsusing framework. Finally, Section 7 (Related Work Conclusions) comparework proposals presented literature draw conclusions.2. Notations Assumptionssection define knowledge bases formalisms are. Since wantconsider formalisms different syntax semantics, needgeneral definitions. Let us consider, base case, formalism propositional calculus.Formally, assume composed three parts:1. syntax, used define well-formed formulae;2. proof theory, allows saying formula follows another one;3. model-theoretic semantics, establishes model satisfies formula.syntax defined finite alphabet propositional symbols L = {a, b, c, . . .},possibly subscripts, usual set propositional connectives , , .terms knowledge representation, proof theory seen way extracting knowledge knowledge base. example, knowledge base c,fact b holds. thus say formula b part knowledge representedc.5fiCadoli, Donini, Liberatore, & Schaerfcases, want knowledge bases represent models rather sets formulas.interpretation alphabet propositional variables L mapping L{true, false}. model-theoretic semantics propositional calculus usual wayextending interpretation L well-formed formulas.Let us extend definition generic formalisms: formalism composedsyntax, proof theory, model-theoretic semantics.remark formalism syntax: instance, default logic includesternary connective : denoting default rules, logic programming specialunary connective not(), on. knowledge base formalism F simply wellformed formula, according syntax formalism.formalism proof theory well. proof theory formalism Fbinary relation `F set knowledge bases formulae. Intuitively, F B `F meansconsequence knowledge base KB, according rules formalismF . result, set formulae implied knowledge base KB exactlyknowledge represented KB.base comparison two different formalisms concept equivalence,allowing saying two knowledge bases (of two different formalisms) representpiece knowledge. Since knowledge represented knowledge base setformulas implies, assume syntax formulaeformalisms. Namely, always assume formulae implied knowledgebase well-formed formulae propositional calculus. words, formalismsyntax knowledge bases: however, assume proof theory relatesknowledge bases (formulae syntax formalism) propositional formulae. So,writing KB `F , assume KB knowledge base syntax F ,propositional formula.allows saying two knowledge bases KB1 KB2 , expressed two different formalisms F1 F2 , represent piece knowledge: true when,propositional formula holds KB1 `F1 KB2 `F2 .model-theoreric semantics formalism relation |=F propositionalmodels knowledge bases. case, assume fixed alphabet L, thus setinterpretations common formalisms. model knowledge baseKB relation, write |=F KB. Intuitively, means modelsupports piece knowledge represented KB.remark formalisms, e.g. credolous default logic (Reiter, 1980),proof theory, model-theoretic semantics. also possible conceiveformalisms model-theoretic semantics proof theory.defined, assume related following formula:KB `Fiff. |= KB implies |=Regarding proof theory formalisms, consider formulae shorterknowledge base, is, assume knowledge represented knowlegdebase KB set formulae KB `F , size sizeKB. done two reasons: first, formulas larger KB likely6fiSpace Efficiency Propositional Knowledge Representation Formalismscontain large parts actually independent KB; second, give technicalsresult simple way using compilability classes introduced next section.Assumption 1 consider formulae whose size less equalknowledge base.formalisms consider satisfy right-hand side distruibutivity conjunction,is, KB `F KB `F KB `F . assumption sizerestrictive case, CNF formula.3. Compilability Classesassume reader familiar basic complexity classes, P, NP (uniform)classes polynomial hierarchy (Stockmeyer, 1976; Garey & Johnson, 1979).briefly introduce non-uniform classes (Johnson, 1990). sequel, C, C0 , etc. denotearbitrary classes polynomial hierarchy.assume input instances problems strings built alphabet .denote empty string assume alphabet contains specialsymbol # denote blanks. length string x denoted |x|.Definition 1 advice function takes integer returns string.Advices important complexity theory definitions results oftenbased special Turing machines determine result oracle free,is, constant time.Definition 2 advice-taking Turing machine Turing machine enhancedpossibility determine A(|x|) constant time, x input string.course, fact A(|x|) determined constant time (whileintractable even undecidable function) makes definitions based advice-takingTuring machine different ones based regular Turing machine. example,advice-taking Turing machine calculate polynomial time many functionsregular Turing machine cannot (including untractable ones).Note advice function size input, input itself.Hence, advice-taking Turing machines closely related non-uniform families circuits(Boppana & Sipser, 1990). Clearly, advice allowed access whole instance,would able determine solution problem constant time.Definition 3 advice-taking Turing machine uses polynomial advice existspolynomial p advice oracle satisfies |A(n)| p(n) nonnegativeintegers n.non-uniform complexity classes based advice-taking Turing machines.paper consider simplified definition, based classes polynomial hierarchy.7fiCadoli, Donini, Liberatore, & SchaerfDefinition 4 C class polynomial hierarchy, C/poly class languages defined Turing machines time bounds C, augmented polynomial advice.class C/poly also known non-uniform C, non-uniformity duepresence advice. Non-uniform uniform complexity classes related: KarpLipton (1980) proved NP P/poly p2 = p2 = PH, i.e., polynomial hierarchy collapses second level, Yap (1983) generalized results, particularshowing NP coNP/poly p3 = p3 = PH, i.e., polynomial hierarchycollapses third level. inprovement results given KoblerWatanabe (1998): proved kp pk /poly implies polynomial hierarchy collapses ZPP(pk+1 ). collapse polynomial hierarchy consideredunlikely researchers structural complexity.summarize definitions results proposed formalize compilabilityproblems (Cadoli et al., 1996b), adapting context terminology PKRformalisms. remark aim paper give formalizationcompilability problems, analyze problems point view. Rather, showuse compilability classes technical tool proving results relativeefficiency formalisms representing knowledge little space.Several papers literature focus problem reducing complexityproblems via preprocessing phase (Kautz & Selman, 1992; Kautz et al., 1995; Khardon& Roth, 1997). motivates introduction measure complexity problemsassuming preprocessing allowed. Following intuition knowledge baseknown well questions posed it, divide reasoning problem two parts:one part fixed accessible off-line (the knowledge base), second one varying,accessible on-line (the interpretation/formula). Compilability aims capturing on-linecomplexity solving problem composed inputs, i.e., complexity respectsecond input first one preprocessed arbitrary way. nextsection show close connection compilability space efficiency PKRformalisms.function f called poly-size exists polynomial p stringsx holds |f (x)| p(|x|). exception definition x represents number:case, impose |f (x)| p(x). result, say function usedadvice-taking turing machine polysize function.function g called poly-time exists polynomial q x, g(x)computed time less equal q(|x|). definitions easily extendbinary functions usual.define language pairs subset . necessary representtwo inputs PKR reasoning problem, i.e., knowledge base (KB), formulainterpretation. example, problem Inference Propositional Logic (pli)defined follows.pli = {hx, yi | x set propositional formulae (the KB), formula, x ` y}well known pli coNP-complete, i.e., one hardest problemsamong belonging coNP. goal prove pli hardest theorem8fiSpace Efficiency Propositional Knowledge Representation Formalismsproving problem among coNP solved preprocessing first inputarbitrary way, i.e., KB. end, introduce new hierarchy classes,non-uniform compilability classes, denoted k;C, C generic uniform complexityclass, P, NP, coNP, p2 .Definition 5 (k;C classes) language pairs belongs k;C iffexists binary poly-size function f language pairs 0 C hx, yiholds:hf (x, |y|), yi 0 iff hx, yiNotice poly-size function f takes input x (the KB) size(either formula interpretation). done technical reason, is,assumption allows obtaining results impossible prove function ftakes x input (Cadoli et al., 1996b). assuption useful proving negative results,is, theorems impossibility compilation: indeed, impossible reducecomplexity problem using function takes x |y| input,reduction also impossible using function taking x argument.Theorem 1 (Cadoli, Donini, Liberatore, & Schaerf, 1997, Theorem 6) Let Cclass polynomial hierarchy . problem belongs k;Cexists poly-size function f language pairs 0 ,hx, yi holds that:1. |y| k, hf (x, k), yi 0 hx, yi S;2. 0 C.Clearly, problem whose time complexity C also k;C: take f (x, |y|) = x0 = S. interesting problem C may belong k;C0C0 C, e.g.,, problems NP k;P. true example problemsbelief revision (Cadoli et al., 1999). rest paper, however, mainly focuscomplete problems, defined below. pictorial representation class k;CFigure 1, assume 0 C.problem pli method proving belongs k;P known. ordershow (probably) belong k;P, define notion reductioncompleteness.Definition 6 (Non-uniform comp-reducibility) Given two problems B,non-uniformly comp-reducible B (denoted nucomp B) iff exist two poly-sizebinary functions f1 f2 , polynomial-time binary function g everypair hx, yi holds hx, yi hf1 (x, |y|), g(f2 (x, |y|), y)i B.nucomp reductions represented depicted Figure 2. reductionssatisfy important properties reduction.Theorem 2 (Cadoli et al., 1996b, Theorem 5) reductions nucomp satisfy transitivity compatible (Johnson, 1990) class k;C every complexity class C.9fiCadoli, Donini, Liberatore, & Schaerff!!1x| | ! |y|6hx, yi-S0-Figure 1: representation k;C.x|y|||- f1- f26-x?g--y00Figure 2: nu-comp-C reductions.Therefore, possible define notions hardness completeness k;Cevery complexity class C.Definition 7 (k;C-completeness) Let language pairs C complexity class.k;C-hard iff problems k;C nucomp S. Moreover,k;C-complete k;C k;C-hard.right complexity class completely characterize problem pli.fact pli k;coNP-complete (Cadoli et al., 1996b, Theorem 7). Furthermore, hierarchyformed compilability classes proper polynomial hierarchyproper (Cadoli et al., 1996b; Karp & Lipton, 1980; Yap, 1983) fact widely conjecturedtrue.Informally, may say k;NP-hard problems compilable P,considerations know exists preprocessing fixed partmakes on-line solvable polynomial time, polynomial hierarchy collapses.holds k;coNP-hard problems. general, problem k;C-completeclass C regarded toughest problem C, even arbitrary preprocessing fixed part. hand, problem k;C problem that,preprocessing fixed part, becomes problem C (i.e., compilable C).close section giving another example use compilability classeswell-known formalism Circumscription (McCarthy, 1980). Let x propositionalformula. minimal models x truth assignments satisfying xpositive values possible (w.r.t. set containment). problem consider is: checkwhether given model minimal model propositional formula. problem, calledMinimal Model checking (mmc), reformulated problem model checkingCircumscription, known co-NP-complete (Cadoli, 1992).10fiSpace Efficiency Propositional Knowledge Representation Formalismsconsider knowledge base x given off-line, truth assignmentgiven on-line, obtain following definition:mmc = {hx, yi | minimal model x }problem shown k;coNP-complete (Cadoli et al., 1996b, Theorem 13).Hence, unlikely k;P; is, unlikely existsoff-line processing knowledge base, yielding (say) data structure x0 ,given y, checked polynomial time whether minimal model x.This, course, unless x0 exponential size. observation applies also x0knowledge base Propositional Logic, led interpretation Circumscriptioncompact, succint, PL (Cadoli, Donini, & Schaerf, 1995; Gogic et al., 1995).framework allows generalize results PKR formalisms, shownsequel.4. Reductions among KR Formalismsdefine forms reduction PKR formalisms analyzefollowing sections. formula always represented string alphabet ,hence consider translations functions transforming strings.Let F1 F2 two PKR formalisms. exists poly-size reduction F1F2 , denoted f : F1 7 F2 , f poly-size function given knowledgebase KB F1 , f (KB) knowledge base F2 . Clearly, reductions restrictedproduce meaningful output. particular, discuss reductions preservemodels original theory.semantic approach Gogic collegues (1995) models twoknowledge bases must exactly same. words, knowledge base KBformalism F1 translated knowledge base KB 0 formalism F2 , |=F1 KB|=F2 KB 0 . approach summarized by: reductionformalisms F1 F2 way translate knowledge bases F1 knowledge bases F2 ,preserving sets models. semantics intuitively grounded, easyshow examples two formalisms consider equally space-efficient cannottranslated other. Let us consider instance variant propositional calculussyntax formulas must form x1 F , F regularformula variables x2 , . . .. Clearly, formalism able represent knowledgespace propositional calculus (apart polynomial factor). However,according definition, formalism cannot translated propositional calculus:knowledge base equivalent KB = x1 . Indeed, modelKB , model consistent knowledge base modified propositionalcalculus contains x1 .propose general approach deal also functions f changelanguage KB. end, allow translation gKB models KBmodels f (KB). stress that, general possible, translation may dependKB i.e., different knowledge bases may different translations models.want translation easy compute, since otherwise computation gKB couldhide complexity reasoning formalism. However, observe end,11fiCadoli, Donini, Liberatore, & Schaerfsufficient impose gKB computable polynomial time. fact, KBfixed, models could trivially translated models f (KB) constant time, usinglookup table. table would exponentially large, though; wantforbid. Hence, impose gKB circuit polynomial-size wrt KB. still usefunctional notation gKB (M ) denote result applying model circuitgKB . formal definition follows.Definition 8 (Model Preservation) poly-size reduction f : F1 7 F2 satisfies modelpreservation exists polynomial p that, knowledge base KB F1exists circuit gKB whose size bounded p(|KB|), every interpretationvariables KB holds |=F1 KB iff gKB (M ) |=F2 f (KB).rationale model-preserving reduction knowledge base KB firstformalism F1 converted knowledge base f (KB) second one F2 ,reduction model F1 easily translated modelgKB (M ) F2 .require g depend KB, transformation f , general, could takeactual form KB account. happens following example modelpreserving translation.Example 3 reduce fragment skeptical default logic (Kautz & Selman, 1991)circumscription varying letters, using transformation introduced Etherington(1987). Let hD, W prerequisite-free normal (PFN) default theory, i.e., defaultsform : , generic formula. Let Z set letters occurringhD, W i. Define PD set letters {a | : D}. function f definedfollowing way: f (hD, W i) = CIRC(T ; PD ; Z), = W {a |a PD },PD letters minimized, Z (the set letters occurring hD, W i)varying letters. show f model-preserving poly-size reduction. fact, givenset PFN defaults let gD function interpretation Z,gD (M ) = {a PD |M |= }. Clearly, f poly-size, gD realized circuitwhose size polynomial |D|, model least one extension hD, W iffgD (M ) |= CIRC(T ; PD ; Z). dependence g stresses fact that,case, circuit g depend whole knowledge base hD, W i, D.Clearly, models preserved, theorems preserved well. weaker formreduction following one, theorems preserved. Also caseallow theorems KB translated simple circuit gKB theorems KB.Definition 9 (Theorem Preservation) poly-size reduction f : F1 7 F2 satisfies theorempreservation exists polynomial p that, knowledge base KB F1 ,exists circuit gKB whose size bounded p(|KB|), every formulavariables KB, holds KB `F1 iff f (KB) `F2 gKB ().theorem-preserving reduction property similar model-preservingreduction, knowledge bases used represent theorems rather models.Namely, knowledge base KB translated another knowledge base f (KB)12fiSpace Efficiency Propositional Knowledge Representation Formalismsused represent set theorems. precisely, theoremKB represented theorem gKB () f (KB).Winslett (1989) shown example reduction updated knowledge basesPL theorem-preserving model-preserving. Using Winsletts reduction, onecould use machinery propositional reasoning KB,update (plus reduction). Also reduction shown previous Example 3theorem-preserving, time g identity circuit.remark definitions reduction general proposedGogic collegues (1995). fact, authors consider notion analogousDefinition 8. case g identity i.e., models two formalismsidentical. allowing simple translation g models Definition 8 coversgeneral forms reductions preserving models, like one Example 3.5. Comparing Space Efficiency PKR Formalismssection show use compilability classes defined Section 3 comparesuccinctness PKR formalisms.Let F1 F2 two formalisms representing sets models. proveknowledge base F1 reduced, via poly-size reduction, knowledge baseF2 satisfying model-preservation compilability class problemmodel checking (first input: KB, second input: interpretation) F2 higher equalcompilability class problem model checking F1 .Similarly, prove theorem-preserving poly-size reductions existcompilability class problem inference (first input: KB, second input: formula, cf.definition problem pli) F1 higher equal compilability classproblem inference F2 .order simplify presentation proof theorems introducedefinitions.Definition 10 (Model hardness/completeness) Let F PKR formalism Ccomplexity class. problem model checking F belongs compilability classk;C, model varying part instances, say F model-C.Similarly, model checking k;C-complete (hard), say F model-C-complete(hard).Definition 11 (Theorem hardness/completeness) Let F PKR formalismC complexity class. problem inference formalism F belongscompilability class k;C, whenever formula varying part instance, sayF thm-C. Similarly, inference k;C-complete (hard), say F thmC-complete (hard).definitions implicitly define two hierarchies, parallel polynomial hierarchy (Stockmeyer, 1976): model hierarchy (model-P,model-NP,model-p2 ,etc.)theorem hierarchy (thm-P,thm-NP,thm-2p ,etc.). higher formalism modelhierarchy, efficiency representing models analogously theorems.example (Cadoli et al., 1996, Thm. 6), characterize model theorem classesPropositional Logic.13fiCadoli, Donini, Liberatore, & SchaerfTheorem 3 PL model-P thm-coNP-complete.formally establish connection succinctness representationscompilability classes. following theorems, complexity classes C, C1 , C2 belongpolynomial hierarchy (Stockmeyer, 1976). Theorems 5 7 assumepolynomial hierarchy collapse.start showing existence model-preserving reductions formalismanother one easily obtained levels model hierarchy satisfy simplecondition.Theorem 4 Let F1 F2 two PKR formalisms. F1 model-C F2 modelC-hard, exists poly-size reduction f : F1 7 F2 satisfying model preservation.Proof. Recall since F1 model-C, model checking F1 k;C, since F2model-C-hard, model checking F1 non-uniformly comp-reducible model checkingF2 . is, (adapting Def. 6) exist two poly-size binary functions f1 f2 ,polynomial-time binary function g every pair hKB, holds|=F1 KB g(f2 (KB, |M |), ) |=F2 f1 (KB, |M |)(note g poly-time function appearing Def. 6, different gKBpoly-size circuit appearing Def. 8).observe |M | computed KB simply counting letters appearing KB; let f3 counting function, i.e., |M | = f3 (KB). Clearly, f3 poly-size.Define reduction f f (KB) = f1 (KB, f3 (KB)). Since poly-size functions closedcomposition, f poly-size. show f model-preserving reduction.Definition 8, need prove exists polynomial p knowledgebase KB F1 , exists poly-size circuit gKB every interpretationvariables KB holds |=F1 KB iff gKB (M ) |=F2 f (KB).proceed follows: Given KB F1 , compute z = f2 (KB, |M |) = f2 (KB, f3 (KB)).Since f2 f3 poly-size, z size polynomial respect |KB|. Define circuitgKB (M ) one computing g(z, ) = g(f2 (KB, f3 (KB)), ). Since g poly-timefunction inputs, z poly-size KB, exists representation g(z, )circuit gKB whose size polynomial wrt KB. construction, |=F1 KB iffgKB (M ) |=F2 f (KB). Hence, thesis follows.following theorem, instead, gives simple method prove modelpreserving reduction one formalism another one.Theorem 5 Let F1 F2 two PKR formalisms. polynomial hierarchycollapse, F1 model-C1 -hard, F2 model-C2 , C2 C1 , poly-sizereduction f : F1 7 F2 satisfying model preservation.Proof. show reduction exists, C1 /poly C2 /poly impliespolynomial hierarchy collapses level (Yap, 1983). Let complete problemclass C1 e.g., C1 p3 may validity -quantified boolean formulae(Stockmeyer, 1976). Define problem follows.= {hx, yi | x = (the empty string) A}14fiSpace Efficiency Propositional Knowledge Representation Formalismsalready proved (Cadoli et al., 1996b, Thm. 6) k;C1 -complete. Since modelchecking F1 model-C1 -hard, non-uniformly comp-reducible model checkingF1 . is, (adapting Def. 6) exist two poly-size binary functions f1 f2 ,polynomial-time binary function g every pair h, yi, holds h, yig(f2 (, |y|), y) |=F1 f1 (, |y|). Let |y| = n. Clearly, knowledge base f1 (, |y|)depends n, i.e., exactly one knowledge base integer. Call KBn .Moreover, f2 (, |y|) = f2 (, n) also depends n only: call (for Oracle). ObserveKBn polynomial size respect n.exists poly-size reduction f : F1 7 F2 satisfying model preservation, givenknowledge base KBn exists poly-size circuit hn g(On , y) |=F1 KBnhn (g(On , y)) |=F2 f (KBn ).Therefore, k;C1 -complete problem non-uniformly reduced problemk;C2 follows: Given y, size |y| = n one obtains (with preprocessing)f (KBn ) . one checks whether interpretation hn (g(On , y)) (computablepolynomial time given n, ) model F2 f (KBn ). fact modelchecking F2 k;C2 , k;C1 k;C2 . proved previous paperresult implies C1 /poly C2 /poly (Cadoli et al., 1996b, Thm. 9),turns implies polynomial hierarchy collapses (Yap, 1983).theorems show hierarchy classes model-C exactly characterizesspace efficiency formalism representing sets models. fact, two formalismslevel model hierarchy reduced via poly-sizereduction (Theorem 4), poly-size reduction formalism (F1 ) higherhierarchy one (F2 ) lower class (Theorem 5). latter case sayF1 space-efficient F2 .Analogous results (with similar proofs) hold poly-size reductions preserving theorems.Namely, next theorem shows infer existence theorem-preserving reductions,one gives way prove theorem-preserving reductionone formalism another one.Theorem 6 Let F1 F2 two PKR formalisms. F1 thm-C F2 thm-Chard, exists poly-size reduction f : F1 7 F2 satisfying theorem preservation.Proof. Recall since F1 thm-C, inference F1 k;C, since F2 thm-Chard, inference F1 non-uniformly comp-reducible inference F2 . is, (adaptingDef. 6) exist two poly-size binary functions f1 f2 , polynomial-time binaryfunction g1 every pair hKB, holdsKB `F1 f1 (KB, ||) `F2 g(f2 (KB, ||), )(here distinguish poly-time function g appearing Def. 6 poly-size circuitgKB appearing Def. 9).Using Theorem 1 replace || upper bound formula.Assumption 1, know size less equal size KB; thereforereplace || |KB|. formula becomesKB `F1 f1 (KB, |KB|) `F2 g(f2 (KB, |KB|), )15fiCadoli, Donini, Liberatore, & SchaerfDefine reduction f f (KB) = f1 (KB, f3 (KB)), f3 poly-size functioncomputes size input. Since poly-size functions closed composition,f poly-size.Now, show f theorem-preserving reduction, i.e.,f satisfies Def. 9.amounts proving knowledge base KB F1 exists circuit gKB ,whose size poynomial wrt KB, every formula variables KBholds KB `F1 iff f (KB) `F2 gKB ().proceed proof Theorem 4: Given KB F1 , let z = f2 (KB, f3 (KB)).Since f2 f3 poly-size, z polynomial size respect |KB|. Define gKB () =g(z, ) = g(f2 (KB, f3 (KB)), ). Clearly, gKB represented circuit polynomialsize wrt KB. construction, KB `F1 iff f (KB) `F2 gKB (). Hence, claimfollows.Theorem 7 Let F1 F2 two PKR formalisms. polynomial hierarchycollapse, F1 thm-C1 -hard, F2 thm-C2 , C2 C1 , poly-sizereduction f : F1 7 F2 satisfying theorem preservation.Proof. show reduction exists, C1 /poly C2 /poly polynomialhierarchy collapses level (Yap, 1983). Let complete problem class C1 .Define problem proof Theorem 5: problem k;C1 -complete (Cadoliet al., 1996b, Thm. 6). Since inference F1 thm-C1 -hard, non-uniformly compreducible inference F1 . is, (adapting Def. 6) exist two poly-size binaryfunctions f1 f2 , polynomial-time binary function g every pair h, yi,h, yi f1 (, |y|) `F1 g(f2 (, |y|), y). Let |y| = n. Clearly, knowledgebase f1 (, |y|) depends n, i.e., one knowledge base integer. CallKBn . Moreover, also f2 (, |y|) = f2 (, n) depends n: call (for Oracle).Observe KBn polynomial size respect n.exists poly-size reduction f : F1 7 F2 satisfying theorem preservation,given knowledge base KBn exists poly-time function hn KBn `F1g(On , y) f (KBn ) `F2 hn (g(On , y)).Therefore, k;C1 -complete problem non-uniformly reduced problemk;C2 follows: Given y, size |y| = n one obtains (with arbitrary preprocessing)f (KBn ) . one checks whether formula hn (g(On , y)) (computable polytime given ) theorem F2 f (KBn ). fact inference F2k;C2 , k;C1 k;C2 . follows C1 /poly C2 /poly (Cadoli et al.,1996b, Thm. 9), implies polynomial hierarchy collapses (Yap, 1983).Theorems 4-7 show compilability classes characterize precisely relativecapability PKR formalisms represent sets models sets theorems. example,consequence Theorems 3 7 poly-size reduction PLsyntactic restriction PL allowing Horn clauses preserves theorems, unlesspolynomial hierarchy collapses. Kautz Selman (1992) proved non-existencereduction problem strictly related pli using specific proof.16fiSpace Efficiency Propositional Knowledge Representation Formalisms6. Applicationssection devoted application theorems presented previous section.Using Theorems 4-7 results previously known literature, able assesmodel- theorem-compactness PKR formalisms.assume definitions Propositional Logic, default logic (Reiter, 1980),circumscription (McCarthy, 1980) known. Definitions WIDTIO, SBR, GCWA,stable model semantics appropriate subsections.following proofs refer problem 3QBF, is, problem verifyingwhether quantified Boolean formula XY.F valid, X disjoint setsvariables, F set clauses alphabet X , composed three literals.example, simple formula belonging class is: x1 , x2 y1 , y2 ((x1 y2 )(x1 x2 y1 ) (y1 x2 y2 ) (x1 x2 )).problem deciding validity 3QBF complete class p2 . consequence, corresponding problem 3QBF, deciding whether input composedstring () fixed part quantified Boolean formula XY.F varyingone, complete class k;2p (Liberatore, 1998). Notice hardnessproofs show sequel use problems without meaningful fixed part.6.1 Stable Model SemanticsStable model semantics (SM) introduced Gelfond Lifschitz (1988) toolprovide semantics logic programs negation. original proposal onestandard semantics logic programs. recall definition propositionalstable model.Let P propositional, general logic program. Let subset (i.e., interpretation) atoms P . Let P program obtained P following way:clause C P contains body negated atom C deleted;body clause contains negated atom 6 deletedbody clause. least Herbrand model P stable modelP.formalism sm, consider program P knowledge base. writeP |=sm Q denote query Q implied logic program P Stable Modelsemantics.order prove result, need define kernel graph.Definition 12 (Kernel) Let G = (V, E) graph. kernel G set K Vthat, denoting H = V K, holds:1. H vertex cover G2. j H, exists K (i, j) E.state theorem compilability class inference stable modelsemantics, corresponding theorem compactness class.Theorem 8 problem inference Stable Model semantics k;coNP-complete,thus Stable Model Semantics thm-coNPcomplete.17fiCadoli, Donini, Liberatore, & SchaerfProof. Membership class follows fact problem coNP-complete(Marek & Truszczynski, 1991). hardness, adapt proof MarekTruszczynski (1991) showing deciding whether query true stable modelscoNP-hard.Let kernel language {, G} G graph least one kernel.Let |G| = n, observe G cannot vertices size n.show n, exists logic program Pn every graph Gn vertices, exists query QG G kernel iff Pn 6|=sm QG .Let alphabet Pn composed following 2n2 + n propositional letters:{ai |i {1..n} } {rij , sij |i, j {1..n} }.program Pn defined as:ajsijrij:::ai , rijriji, j {1..n}sijGiven graph G = (V, E), query QG definedQG = (_(i,j)Erij ) (_rij )(i,j)6Ereduction kernel sm defined as: f1 (x, n) = Pn , i.e., f1 dependssecond argument, f2 (x, n) = , i.e., f2 constant function, g = Qy , i.e., givengraph G, circuit g computes query QG .result, k; reduction. show reduction correct, i.e.,h, Gi kernel (G kernel) iff Pn 6|=SM QG .If-part. Suppose Pn 6|=SM QG . Then, exists stable model Pn|= QG . Observe QG equivalent conjunction rij (i, j) E,rij (i, j) 6 E. Simplifying Pn QG obtain clauses:aj : ai , (i, j) E(1)Observe contains sij (i, j) 6 E, order stable, i.e.,support atoms rij (i, j) E contains atom sij (i, j) E.Let H = {j|aj }, K = {i|ai 6 }. H vertex cover G, sinceedge (i, j) E, satisfy corresponding clause (1) aj : ai , hence eitherai , aj . Moreover, j H, atom aj , sincestable model, exists clause aj : ai ai 6 , is, K. Therefore,K kernel G.Only-if part. Suppose G = (V, E) kernel K, let H = V K. Letinterpretation= {rij |(i, j) E} {sij |(i, j) 6 E} {aj |j H}Obviously, 6|= QG . show stable model Pn , i.e., leastHerbrand model PnM . fact, PnM contains following clauses:sijrijaj: rij(i, j) 6 E(2)K(4)(i, j) E18(3)fiSpace Efficiency Propositional Knowledge Representation FormalismsClauses last line obtained clauses Pn form aj : ai , rij ,clauses H (hence ai ) deleted, clauses negatedatom ai deleted, since K, hence ai 6 . aj , vertex j H,hence edge (i, j) E, K. Hence clauses (4) (3) PnM , henceleast Herbrand model PnM exactly aj j H.6.2 Minimal Model ReasoningOne successful form non-monotonic reasoning based selectionminimal models. Among various formalisms based minimal model semantics consider Circumscription (McCarthy, 1980) Generalized Closed World Assumption(GCWA) (Minker, 1982), formalism represent knowledge closed world.assume reader familiar Circumscription, briefly presentdefinition GCWA. model semantics GCWA defined (a letter):|=GCW KB iff |= KB{a | positive clause , KB 6` KB 6` a}present results two formalisms.Theorem 9 problem model checking Circumscription k;coNP-complete, thusCircumscription model-coNP-complete.result trivial corollary theorem already proved (Cadoli et al., 1997, Theorem 6). fact, proof implicitly shows model checking circumscriptionk;coNP-complete.Theorem 10 problem model checking GCWA k;P, thus GCWAmodel-P.Proof. already pointed (Cadoli et al., 1997), possible rewrite GCW A(T )propositional formula F that, given model , |= GCW A(T )|= F . Moreover, size F polynomially bounded size .consequence, model compactness GCWA class PL. Theorem 3thesis follows.Theorem 11 problem inference Circumscription k;p2 -complete, thus Circumscription thm-p2 -complete.result trivial corollary theorem published previous paper (Cadoliet al., 1997, Theorem 7) implicitly shows inference circumscription k;2p complete.Theorem 12 problem inference GCWA k;coNP-complete, thus GCWAthm-coNP-complete.Proof. already pointed proof Theorem 10, possible rewriteGCW A(T ) formula F equivalent it. consequence, formulatheorem GCW A(T ) theorem F . Thus, GCWAtheorem compexity PL. Since GCWA generalization PL, follows GCWAtheorem compactness class PL. Hence, GCWA thm-coNP-complete.19fiCadoli, Donini, Liberatore, & Schaerf6.3 Default Logicsubsection present results default logic, two variants (credulousskeptical). details two main variants default logic, referreader paper Kautz Selman (1991). Notice model-compactnessapplicable skeptical default logic.Theorem 13 problem model checking skeptical default logic k;p2 complete,thus skeptical default logic model-p2 complete.Proof. proof membership straightforward: since model checking skepticaldefault logic p2 (Liberatore & Schaerf, 1998), follows also k;p2 .proof k;p2 -hardness similar proof p2 -hardness (Liberatore & Schaerf,1998). reduction problem 3QBF. Let h, instance 3QBF,= XY.F represents valid 3QBF formula, string.Let n size formula F . implies variables formulan. Let = {1 , . . . , k } set clauses three literals alphabet.number clauses three literals alphabet n variables less O(n3 ),thus bounded polynomial n.prove XY.F valid model extension hW, Di,=W=[ : cici: ci,ci= {ci | F }[ : w (w xi ) : w (w xi ),w xixi Xw xi(:wVciwset {ci | 1 k} set new variables, one-to-one elements .Note W depends size n F , depends F . result,nucomp reduction.prove formula valid model extensiondefault theory hW, Di. similar already published proof (Liberatore &Schaerf, 1998). Consider evaluation C1 variables {ci } evaluation X1variables X. Let D0 following set defaults.0=[ : ci [ : cici C1cici 6C1ci[ : w (w xi )w xixi X1[xi X1: w (w xi )w xiset defaults chosen set R consequences correspondssets C1 X1 . Namely, have:ci C1 iff R |= cici 6 C1 iff R |= cixi X1 iff R |= w xixi 6 X1 iff R |= w xi20)fiSpace Efficiency Propositional Knowledge Representation FormalismsNow, prove consequences set defaults extensiondefault theory QBF formula valid. Since defaults semi-normal,prove that:1. set consequences D0 consistent;2. default applicable, is, default whose preconditionconsistent R.Consistency R follows construction: assigning ci true ci C1 , etc.,obtain model R.:ciprove default applicable. ci C1 , default c:ciapplicable, vice versa, ci C1 , ci applicable. Moreover, nonei), applicable xi 6 X1 , case w xi R, thusdefaults :w(wxwxiw would follow (while w justification default). similar statement holds:w(wxi )xi X1 .wxiV:wciresult, applicable default may last one,(recallwF negated). default applicable if, given evaluation cixi s, set clauses satisfiable. amount say: extensionlast default applicable QBF formula valid. Now,last default applicable, model extension wconsequence last default w 6|= . converse also holds: last defaultapplicable model default theory.result, QBF valid model given default theory.Theorem 14 inference problem skeptical default logic k;p2 complete, thus skeptical default logic thm-2p complete.Proof. Since inference skeptical default logic p2 , also k;p2 . k;p2 -hardnesscomes simple reduction circumscription. Indeed, circumscriptionformula equivalent conjunction extensions default theory hT, Di,(Etherington, 1987):D=[ : xixiresult, CIRC(T ) |= Q Q implied hT, Di skeptical semantics. Since hT, Di depends (and Q) nucomp reduction. Sinceinference circumscription k;2p -complete (see Theorem 11), follows skepticaldefault logic k;2p -hard.Theorem 15 inference problem credulous default logic k;p2 complete, thuscredulous default logic thm-p2 complete.21fiCadoli, Donini, Liberatore, & SchaerfProof. proof similar proof model checking skeptical default logic.Indeed, problems k;p2 complete. Since problem p2 , proved Gottlob(1992), also k;p2 . Thus, prove hard class.prove 3QBF problem reduced problem verifying whetherformula implied extensions default theory (that is, inference credulousdefault logic).Namely, formula XY.F valid Q derived extensiondefault theory hD, W i, W defined follows ( setclauses three literals alphabet F , C set new variables, one-to-one).W==[ : cicici CQ =^Fci: ci,ci^6F[ : xixi Xxi: xi,xi(V(ci C ciw) :)ci wInformally, proof goes follows: truth evaluation variables CX set defaults justified consistent. simple necessarysufficient condition consequences set defaults extensionfollowing. If, evaluation, formula^ci =truevalid, last default applicable, thus extension also contains w. conversealso holds: formula valid evaluation, variable wextension.result, exists extension Q holds existsextension ci true F , w also holds.variables ci given value, formula equivalent F . result,extension exists exists truth evaluation variables XF valid.6.4 Belief RevisionMany formalisms belief revision proposed literature, focustwo them: WIDTIO (When Doubt Throw Out) SBR (Skeptical Belief Revision).Let K set propositional formulae, representing agents knowledge world.new formula added K, problem possible inconsistency Karises. first step define set sets formulae W (K, A) followingway:W (K, A) = {K 0 K 0 maximal consistent subset K {A} containing }22fiSpace Efficiency Propositional Knowledge Representation Formalismsset formulae K 0 W (K, A) maximal choice formulae Kconsistent and, therefore, may retain incorporating A. definitionset leads two different revision operators: SBR WIDTIO.SBR Skeptical Belief Revision (Fagin, Ullman, & Vardi, 1983; Ginsberg, 1986). revised.theory defined set theories: K = {K 0 | K 0 W (K, A)}. Inferencerevised theory defined inference theories:K `SBR Q iffK 0 W (K, A) , K 0 ` Qmodel semantics defined as:|=SBR K iffexists K 0 W (K, A) |= K 0WIDTIO Doubt Throw (Winslett, 1990). simpler (but somewhatdrastical) approach so-called WIDTIO, retain formulae Kbelong sets W (K, A). Thus, inference defined as:K `W IDT IO Q iffW (K, A) ` Qmodel semantics formalism defined as:|=W IDT IO Kiff|=\W (K, A)results model compactness shown Liberatore Schaerf (2000).recall them.Theorem 16 (Liberatore & Schaerf, 2000, Theorem 11) problem model checking WIDTIO k;P, thus WIDTIO model-P.Theorem 17 (Liberatore & Schaerf, 2000, Theorem 5) problem model checking Skeptical Belief Revision k;coNP-complete, thus Skeptical Belief Revisionmodel-coNP-complete.results theorem compactness quite simple provide proofs.Theorem 18 problem inference WIDTIO k;coNP-complete, thus WIDTIOthm-coNP-complete.Proof. Membership class thm-coNP immediately follows definition. fact,rewrite K propositional formula computing set W (K, A)constructing intersection. construction intersection size less equalsize K A. consequence, preprocessing, deciding whether formula Qfollows K problem coNP. Hardness follows obvious fact PLreduced WIDTIO PL thm-coNP-complete (see Theorem 3).Theorem 19 problem inference Skeptical Belief Revision k;p2 -complete,thus Skeptical Belief Revision thm-p2 -complete.23fiCadoli, Donini, Liberatore, & SchaerfPropositionalLogicWIDTIOSkepticalBelief RevisionCircumscriptionGCWASkepticalDefault ReasoningCredulousDefault ReasoningStable ModelSemanticsTime ComplexityPp2 -complete(Liberatore & Schaerf, 1996)coNP-complete(Liberatore & Schaerf, 1996)coNP-complete(Cadoli, 1992)coNP-hard,p2 [log n](Eiter & Gottlob, 1993)p2 -complete(Liberatore & Schaerf, 1998)N/ASpace Efficiencymodel-Pmodel-PTh. 16model-coNP-completeTh. 17model-coNP-completeTh. 9model-PTh. 10Pmodel-Pmodel-p2 -completeTh. 13N/ATable 1: Complexity model checking Space Efficiency Model RepresentationsProof. Membership follows complexity results Eiter Gottlob (1992),show deciding whether K `SBR Q p2 -complete problem. Hardnessfollows easily Theorem 17. fact, |=SBR K iff K 6`SBR f orm(M ),f orm(M ) formula represents model . consequence, model checkingreduced complement inference. Thus inference k;p2 -complete.6.5 DiscussionTables 1 2 summarize results space efficiency PKR formalismsproved (a dash denotes folklore result).First all, notice space efficiency always related time complexity.example, compare detail WIDTIO circumscription. table followsmodel checking harder WIDTIO circumscription, inferencecomplexity cases. Nevertheless, since circumscription thm-p2 -completeWIDTIO thm-coNP-complete (and thus thm-p2 ), exists poly-size reductionWIDTIO circumscription satisfying theorem preservation. conversehold: since circumscription thm-2p -complete WIDTIO thm-coNP, unless Polynomial Hierarchy collapse theorem-preserving poly-size reductionformer formalism latter. Hence, circumscription compact formalismWIDTIO represent theorems. Analogous considerations done models.Intuitively, due fact WIDTIO model checking inferencerequire lot work revised knowledge base alonecomputing intersection24fiSpace Efficiency Propositional Knowledge Representation FormalismsPropositionalLogicWIDTIOSkepticalBelief RevisionCircumscriptionGCWASkepticalDefault ReasoningCredulousDefault ReasoningStable ModelSemanticsTime ComplexitycoNP-complete(Cook, 1971)p2 -complete(Eiter & Gottlob, 1992) & (Nebel, 1998)p2 -complete(Eiter & Gottlob, 1992)p2 -complete(Eiter & Gottlob, 1993)p2 -complete(Eiter & Gottlob, 1993) & (Nebel, 1998)p2 -complete(Gottlob, 1992)p2 -complete(Gottlob, 1992)coNP-complete(Marek & Truszczynski, 1991)Space Efficiencythm-coNP-complete(Cadoli et al., 1996)thm-coNP-completeTh. 18thm-p2 -completeTh. 19thm-p2 -completeTh. 11thm-coNP-completeTh. 12thm-p2 -completeTh. 14thm-p2 -completeTh. 15thm-coNP-completeTh. 8Table 2: Complexity inference Space Efficiency Theorem Representationselements W (K, A). done, one left model checking inferencePL. Hence, WIDTIO space efficiency PL, circumscription.Figures 3 4 contain information Tables 1 2, highlight existing reductions. figure contains two diagrams, left one showing existencepolynomial-time reductions among formalisms, right one showing existence polysize reductions. arrow formalism another denotes formerreduced latter one. use bidirectional arrow denote arrows directionsdashed box enclose formalisms reduced one another. Noteformalisms appropriate representing sets models, others performbetter sets formulae. interesting relation exists skeptical default reasoningcircumscription. model-preserving poly-size reduction circumscription skeptical default reasoning (Gogic et al., 1995), theorem-preserving poly-sizereduction exists, shown Theorem 14.7. Related Work Conclusionsidea comparing compactness KR formalisms representing informationnovel AI. well known first-order circumscription represented secondorder logic (Schlipf, 1987). Kolaitis Papadimitriou (1990) discuss several computationalaspects circumscription. Among many interesting results show reductionrestricted form first-order circumscription first-order logic. proposed reductionincrease size original formula exponential factor. left openproblem show whether increase intrinsic, different compactnessproperties two formalisms, exists space-efficient reduction.25fiCadoli, Donini, Liberatore, & Schaerf- SkepticalWIDTIODefaultSkeptical Default66GCWA6SBR - CircumscriptionSBR - Circumscription.66PL - Stable ModelPL - WIDTIOa. Time Complexity- GCWA - StableModelb. Space EfficiencyFigure 3: Complexity Model Checking vs. Space Efficiency Model RepresentationWIDTIO - GCWA6?SkepticalSBR - Circum - DefaultPL -CredulousDefaultSBR - Circum-SkepticalDefaultAKStableModelCredulousDefaultStablePL- WIDTIO - GCWA - Modela. Time complexityb. Space efficiencyFigure 4: Complexity Inference vs. Space Efficiency Theorem Representation26fiSpace Efficiency Propositional Knowledge Representation Formalismsfirst-order language used, results compactness existence reductionsreported Schlipf (1995).Khardon Roth (1996, 1997), Kautz, Kearns Selman (1995) propose modelbased representations KB Propositional Logic, compare formula-basedrepresentations. Although results significant comparing representations withinPL, refer formalism, hence applicable comparison different PKR formalisms. comment applies also idea representingKB efficient basis Moses Tennenholz (1996), since refers onePKR formalism, namely, PL.active area research studies connections various non-monotonic logics.particular, several papers discussing existence translations polynomial time satisfy intuitive requirements modularity faithfulness.Janhunen (1998), improving results Imielinski (1987) Gottlob (1995), showsdefault logic expressive, among non-monotonic logics examined, sincecircumscription autoepistemic logic modularly faithfully embedded default logic, way around. results interest helpfully understand relation among many knowledge representation formalisms,directly related ours. fact, allow translations generalpolynomial time, papers consider translations usepolynomial time also satisfy additional requirements.first result compactness representations propositional language presented, best knowledge, Kautz Selman (1992). show that, unlesscollapse polynomial hierarchy, size smallest representationleast Horn upper bound propositional theory superpolynomial sizeoriginal theory. results also presented different form comprehensive paper (Selman & Kautz, 1996). technique used proofused us researchers prove several results relative complexitypropositional knowledge representation formalisms (Cadoli et al., 1996, 1997, 1999; Gogicet al., 1995).recent paper (Cadoli et al., 1996b) introduced new complexity measure, i.e.,compilability. paper shown measure inherently relatedsuccinctness PKR formalisms. analyzed PKR formalisms respect two succinctness measures: succinctness representing sets models succinctness representingsets theorems.main advantage framework machinery necessary formal waytalking relative ability PKR formalisms compactly represent information.particular, able formalize intuition specific PKR formalism providesone compact ways represent models/theorems among PKR formalismsspecific class.opinion, proposed framework improves state art two differentaspects:1. proofs presented previous papers compare pairs PKR formalisms, example propositional circumscription Propositional Logic (Cadoliet al., 1997). results allow precise classification level27fiCadoli, Donini, Liberatore, & Schaerfcompactness considered formalisms. Rephrasing adapting resultsframework allows us infer circumscription model-coNP-completethm-p2 -complete. consequence, also space-efficientWIDTIO belief revision formalism representing sets models sets theorems.2. Using proposed framework possible find criteria adapting existentpolynomial reductions showing C-hardness reductions show model-C thmC-hardness, C class polynomial hierarchy (Liberatore, 1998).Acknowledgmentspaper extended revised version paper authors appearedproceedings fifth international conference principles knowledge representation reasoning (KR96) (Cadoli, Donini, Liberatore, & Schaerf, 1996a). Partialsupported given ASI (Italian Space Agency) CNR (National ResearchCouncil Italy).ReferencesBen-Eliyahu, R., & Dechter, R. (1991). Default logic, propositional logic constraints.Proceedings Ninth National Conference Artificial Intelligence (AAAI91),pp. 379385.Ben-Eliyahu, R., & Dechter, R. (1994). Propositional semantics disjunctive logic programs. Annals Mathematics Artificial Intelligence, 12, 5387.Boppana, R., & Sipser, M. (1990). complexity finite functions. van Leeuwen, J.(Ed.), Handbook Theoretical Computer Science, Vol. A, chap. 14. Elsevier SciencePublishers (North-Holland), Amsterdam.Cadoli, M. (1992). complexity model checking circumscriptive formulae. Information Processing Letters, 44, 113118.Cadoli, M., Donini, F., Liberatore, P., & Schaerf, M. (1996a). Comparing space efficiencypropositional knowledge representation formalisms. Proceedings Fifth International Conference Principles Knowledge Representation Reasoning(KR96), pp. 364373.Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1996b). Feasibility unfeasibility off-line processing. Proceedings Fourth Israeli Symposium TheoryComputing Systems (ISTCS96), pp. 100109. IEEE Computer Society Press.url = ftp://ftp.dis.uniroma1.it/PUB/AI/papers/cado-etal-96.ps.gz.Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1997).Preprocessing intractable problems.Tech. rep. DIS 24-97, Dipartimento diurl =Informatica e Sistemistica, Universita di Roma La Sapienza.http://ftp.dis.uniroma1.it/PUB/AI/papers/cado-etal-97-d-REVISED.ps.gz.28fiSpace Efficiency Propositional Knowledge Representation FormalismsCadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1999). size revisedknowledge base. Artificial Intelligence, 115 (1), 2564.Cadoli, M., Donini, F. M., & Schaerf, M. (1995). compact representations propositional circumscription. Proceedings Twelfth Symposium Theoretical Aspects Computer Science (STACS95), pp. 205216. Extended version RAP.14.95DIS, Univ. Roma La Sapienza, July 1995.Cadoli, M., Donini, F. M., & Schaerf, M. (1996). intractability non-monotonic reasoningreal drawback?. Artificial Intelligence, 88 (12), 215251.Cadoli, M., Donini, F. M., Schaerf, M., & Silvestri, R. (1997). compact representationspropositional circumscription. Theoretical Computer Science, 182, 183202.Cook, S. A. (1971). complexity theorem-proving procedures. ProceedingsThird ACM Symposium Theory Computing (STOC71), pp. 151158.Eiter, T., & Gottlob, G. (1992). complexity propositional knowledge base revision,updates counterfactuals. Artificial Intelligence, 57, 227270.Eiter, T., & Gottlob, G. (1993). Propositional circumscription extended closed worldreasoning 2p -complete. Theoretical Computer Science, 114, 231245.Etherington, D. V. (1987). Reasoning incomplete information. Morgan Kaufmann,Los Altos, Los Altos, CA.Fagin, R., Ullman, J. D., & Vardi, M. Y. (1983). semantics updates databases.Proceedings Second ACM SIGACT SIGMOD Symposium PrinciplesDatabase Systems (PODS83), pp. 352365.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: GuideTheory NP-Completeness. W.H. Freeman Company, San Francisco, Ca.Gelfond, M., & Lifschitz, V. (1988). stable model semantics logic programming.Proceedings Fifth Logic Programming Symposium, pp. 10701080. MITPress.Gelfond, M., Przymusinska, H., & Przymusinsky, T. (1989). relationshipcircumscription negation failure. Artificial Intelligence, 38, 4973.Ginsberg, M. L. (1986). Conterfactuals. Artificial Intelligence, 30, 3579.Gogic, G., Kautz, H. A., Papadimitriou, C., & Selman, B. (1995). comparative linguistics knowledge representation. Proceedings Fourteenth InternationalJoint Conference Artificial Intelligence (IJCAI95), pp. 862869.Gottlob, G. (1992). Complexity results nonmonotonic logics. Journal LogicComputation, 2, 397425.Gottlob, G. (1995). Translating default logic standard autoepistemic logic. JournalACM, 42, 711740.29fiCadoli, Donini, Liberatore, & SchaerfImielinski, T. (1987). Results translating defaults circumscription. Artificial Intelligence, 32, 131146.Janhunen, T. (1998). intertranslatability autoepistemic, default prioritylogics, parallel circumscription. Proceedings Sixth European WorkshopLogics Artificial Intelligence (JELIA98), No. 1489 Lecture Notes ArtificialIntelligence, pp. 216232. Springer-Verlag.Johnson, D. S. (1990). catalog complexity classes. van Leeuwen, J. (Ed.), HandbookTheoretical Computer Science, Vol. A, chap. 2. Elsevier Science Publishers (NorthHolland), Amsterdam.Karp, R. M., & Lipton, R. J. (1980). connections non-uniform uniformcomplexity classes. Proceedings Twelfth ACM Symposium TheoryComputing (STOC80), pp. 302309.Kautz, H. A., Kearns, M. J., & Selman, B. (1995). Horn approximations empirical data.Artificial Intelligence, 74, 129145.Kautz, H. A., & Selman, B. (1991). Hard problems simple default logics. ArtificialIntelligence, 49, 243279.Kautz, H. A., & Selman, B. (1992). Forming concepts fast inference. ProceedingsTenth National Conference Artificial Intelligence (AAAI92), pp. 786793.Khardon, R., & Roth, D. (1996). Reasoning models. Artificial Intelligence, 87, 187213.Khardon, R., & Roth, D. (1997). Defaults relevance model-based reasoning. ArtificialIntelligence, 97, 169193.Kobler, J., & Watanabe, O. (1998). New collapse consequences NP small circuits.SIAM Journal Computing, 28 (1), 311324.Kolaitis, P. G., & Papadimitriou, C. H. (1990). computational aspects circumscription. Journal ACM, 37 (1), 114.Liberatore, P. (1995). Compact representation revision Horn clauses. Yao, X. (Ed.),Proceedings Eighth Australian Joint Artificial Intelligence Conference (AI95),pp. 347354. World Scientific.Liberatore, P. (1998). Compilation intractable problems application artificialintelligence.Ph.D.thesis,Dipartimento di Informatica e Sistemistica, Universita di Roma La Sapienza. URL =ftp://ftp.dis.uniroma1.it/pub/AI/papers/libe-98-c.ps.gz.Liberatore, P., & Schaerf, M. (1996). complexity model checking belief revision update. Proceedings Thirteenth National Conference ArtificialIntelligence (AAAI96), pp. 556561.30fiSpace Efficiency Propositional Knowledge Representation FormalismsLiberatore, P., & Schaerf, M. (1998). complexity model checking propositionaldefault logics. Proceedings Thirteenth European Conference ArtificialIntelligence (ECAI98), pp. 1822.Liberatore, P., & Schaerf, M. (2000). compactness belief revision update operators. Tech. rep., Dipartimento di Informatica e Sistemistica, Universita di Roma LaSapienza.Marek, W., & Truszczynski, M. (1991). Autoepistemic logic. Journal ACM, 38 (3),588619.McCarthy, J. (1980). Circumscription - form non-monotonic reasoning. ArtificialIntelligence, 13, 2739.Minker, J. (1982). indefinite databases closed world assumption. ProceedingsSixth International Conference Automated Deduction (CADE82), pp. 292308.Moses, Y., & Tennenholtz, M. (1996). Off-line reasoning on-line efficiency: knowledgebases. Artificial Intelligence, 83, 229239.Nebel, B. (1998). hard revise belief base?. Dubois, D., & Prade, H. (Eds.),Belief Change - Handbook Defeasible Reasoning Uncertainty Management Systems, Vol. 3. Kluwer Academic.Reiter, R. (1980). logic default reasoning. Artificial Intelligence, 13, 81132.Schlipf, J. S. (1987). Decidability definability circumscription. Annals PureApplied Logic, 35, 173191.Schlipf, J. S. (1995). survey complexity undecidability results logic programming. Annals Mathematics Artificial Intelligence, 15, 257288.Selman, B., & Kautz, H. A. (1996). Knowledge compilation theory approximation.Journal ACM, 43, 193224.Stockmeyer, L. J. (1976). polynomial-time hierarchy. Theoretical Computer Science,3, 122.Winslett, M. (1989). Sometimes updates circumscription. Proceedings EleventhInternational Joint Conference Artificial Intelligence (IJCAI89), pp. 859863.Winslett, M. (1990). Updating Logical Databases. Cambridge University Press.Yap, C. K. (1983). consequences non-uniform conditions uniform classes. Theoretical Computer Science, 26, 287300.31fi fffiff ff! #"$ % ff'&)(+*-,/.0001324+5(+4*6789: ;)(ff<ff00!=>8%&;?2@<ff00ACBD7EGF-HID@JLKMACNOJLPIQRD@HTSUACVWSXKYQBZ\[]$^_]a`cb-dfe$gihjek^lkmknpokmkqrWs$tvuwxq3npyzwxqpsi{3|_wx}Xty~L7Oz'@@_ffY@+ff-@ _z'ff~LXffffkff'X!YOfifi'RWRfi! $cj/L+77+ff)7i+++Yf!/+O+xYj)ffTxz 3k7Y+c+7i7Yi+)7iff/++LYxkff+xi+xYz7xffx)?!+7++ficfici+7fiz7fix:x!+@/ff++v+c7xzO+ X+xz7fix+$+@/ff/fi:Ic?'Yk#ffzx7++ vfixX7!ix+Y7ffp/pvx:z$i+jx@ +++c+/7:/ffc+:!Tx:fi/?)7xcz7+zff+ffOk+czx/#fiff ffi+Lx)/z7piff/W+Wff vcx7x!pcT7+z7+)++3'++z7+xx7:+?@++xiff+Lfix!p+W/zX+)+x+7pLc7@W/--+)T@@z@O+++xO7-7xz+iff@i+/! z O+xYf! 7_ x+x7fx+@x+?+T+x7+ff+ff++i!+O+7+ v_7?fiffR+ +@@z++xi+Y:fffx7+L+7xz7?77!v!z7+ffff@@iX77i+k77xzxcv:Lx!+cff+ff+-)+x+7ffOi :-XT@ fffivpv@ffvpRff!#"$fi%fi&'fi()'%fi*,+-fi./vpv 012345fi6v7fi*8@ff4v,Rfi09fi(W@ff6pfffi(04%fi':;<ff430=v>?pff@ fi 099+ff40Av?pff90ff:B'fi:;ffC4ff!6DffpE9()34?fiA+F?04<'fi7R()fiff4@ffG'.1ff!IHT?pffvff*JKpffpL3MfiN%fi0 ()fi@ff4OE3P'ffQ?0L+F44pfi3R3 @ ffR1S?3Pfi !CT$p? @ ()fi@ *Jv 0fi G'fiU30/v pC Rfi 7+F?G04 0*V4cff 26'fiS4fi 0 P?3fi 7'fiU ff W Rfi/()SL04 N'fiSL'fiG33 M()fi@ ff30P'ff!XT$p? 'fi/34fi Zfi([ ff pM*,?Mfi\+ ff*Afi()#P. ff?pKfi ]^Rff?ff/fi0`_a[?0pV'fi[@ ff2! ( T$p? fifi(M?4J@ ff ffE?72V'fiF E(bQ?p9Rff?ff/fiJfi(0!,ffc a434d*fi3MAfi @ ffL4-'fipfifffiev $O?fi1-()fi5pPv 0v p>+fp? ?p?pARff?ff/fiOfi(av ff v p[fi <@ ffPv g+F4?v L?3-%fi33 0Jfi(a@ ff'Rff`k_ ff7fi 'v Ahi2 ffvffkl6()$ ff pM!gTF?4,43pff< ()1v 3>?-fiRff9@ [@ ff ffC()fi-v pj fiR4 mfi YC+ ff4Q E(b/v pZ?fi0[fiRvff@ ff ffn(bfiK834fi'/'ffP$ +F?4E?Kfi pNfifPfi@ Lfi [Pd 0!o/P0 Gfi(7XfiRpq4[4fi\a]^KrOCstff+u'h [4fi\a*IvffwxylE!zT$?4C4ff+>*F+F?2??74'fiYRW3 ffm1{ ff4W 0R|<'}fffi 0fh~vffwwlE*<'ff8?8Wfi 8PdMfi7?E~IEd\LQ^5^M~^~E9~B^\>5pE^\p9;ff\d^\[^d9'EI\i;ffB7B'\\~idiB~XdB'\^;^^^b82ff tdB)~t5~-pB~ffBE~59^'B^;.000j %%a!; +L ff;A?@ ff9 ff! x9 ff,8!%&%;&@%$%2@ ;\fimknpokmkq?13PYfiQ4fi\+?30PY'fifiO8'fiC?EK!5T$?p7vUfi'03fiKfi(<{ff4SG|<'}fffit4j84<'fiUP]>()fi@+ P@ ff ff@ P3M'fiMfifi37fi 8v 'fiK@ ff`:B+-fi414fi*V+pff'fiO@ ffa Zfi03Mfi 04'0v COff 0>fi(F.1v 3Y?pfffiRf2fid]^rO'7sJff+>!!!^?Mfid+MfiY+ ''fiZfi3M7()>(bfi3 pU3L?0v m?3fi/ff>fi(-fiR/v pfi3MQfipEEffk[(9fi3M *?2f4d+4['fifi pg()fiL@ ff[40v ffO4fi G 0 pff0f'fiKRfiRfi 2} ff v 'fi'Rff`_Ffiff-ff'0 Nfi KI'/'ffK*30?K jH Fpff L30 ff]^_a !^kGT$?2V0Rt0M@ ff fft{ ff4I 8|<'}fffi 0]^t2'fiQPOv >?p-fi o/gfi(a Afi ff!Ttfi7@ fffi 0'fi7?3f40'fi8*+ f f+-fi./v pL'fi\+- j [4fi\12 k>0 [fi */+F?2?+ I3k_ p>'fi Rfi Q?N()*% SC@ ff'fi 00 eNP 3pd*+fp? ?p.3 :;pk_ pffmfiffL@ P0@ ff ffR()Ifi !3E?Rfi 7+F42_ff4?p7fi 'v?pffO4fi 8?Mfi10*fig 0g?pffP ff ffOv 73E?8f+Aff7g'fiQ@ ff@ 9?3ff 90fiRff!T$3? `_afi =O?Mfi/3Off=p? @ *IId 7/)*Nfi 4'Kfi(80342v pW&k_fi/pffVfi(9'/'ff Gp? ff.1v 3+F3? ?pf?37pff4@ ffYfi;Y?Mfi40Fv U?[Pfi1pffB!5+c S?pfi o1Qfi(6?4F0Rff*0fi/pfftp? ff.1v 3CpEPv pffF+Fp? ?pNn =(bfi[04 Sq SfiRp*M! !*/+Fp? ?3-a4 j Pfi1pff2kChi4~$_ ffEl<fiRp!<T$3? ffi3M'a3M-2 ff4?p j ffkIfi j Mfik*a( j fiM*^kPfi p7fiffi@ 7fi33 o/a ffF@ 7fi\143ff!,Sfi1pfftp? ff.1 p?$fid S'fiR\Rff()fiN( p:;E4O042fi k* ! M!*VCPfi1pfftp? ff. >33fi\ @ fffi4444''fi3cfi7v WU'/'ffpff pffR'fiSP. C0342v p[fi@ ff?13. ff4'!KT$?4fiA+-fi34 ?dQ$3 ffp? ffCI''33(bfiQ'fiI+-fi ff?13. Q/fi 0*E?pA?R?pffhi|,4 ff2Mf!*VvffwwlE!|9 44*fi/pff1p? ff.1 pL4g3MA()fi5 ffE??Mfip3 ?8?p5 6fi(41@ ff?00 $'ff6fi(?p502 7'fiLEp? ff.(0?p5fiRp7?Mfi40!Jc(0?p5a4 I?6[k_ ?13>ROfi(%'ff*?04gfi/ffP ff!6Ufi/pffMEp? ff./v p4fi0MfiRvffOfi(N834fi <02 ?64O5fia o/4;8?4Yo/%fi p2<v m?p137Nfi(5fi ! * {?RY2@ 30>RNfi(Afi *O?2N4Nfi34RU@ fi3N0fi0 ffK!+c W()*j ZPfi1pff,p? ff.1v 3SUv p Pfi 7a4 m+F?RS?33 30>Rfi($'ffIZfi03Mfi 04mfi?40! @ ffIpff<fi(5@ ff@ ff?Xv ?3 `_fifiP833 p82<3M@ 7(bfi/3 fffi @ ff34fi Iff3? 2p3 ff<(bfi,?0 4v 3f 4 F'f0ffhBA4. G{v pM*NvffwwlE!YpYfi(N?pS4@ ff'C'1ffPPfi1pffFp? ff. ff'fiZG3v p?3ff@ ff30fi Gff3? 2p3 ffQ?Xvy (+.0 'ffhiA3ME? N!*<vffwwlE!7TH ?pffv ff*?p0240444;fi(IP fi(>?pff@ @ ff3fi ffE3? 41p3 ff 4 @ ff'4ff ( +@fi0 ff4n3M'fiff!r3M?pfi@ p* Mfi 3[fi(J?pffz@ Q4fi@ ffC()fi?eAff `_afi ()F ff 0v p7?0-4@ ff?p1'ffY! fiO?Mfi10Lv m?p23M@ @ pff pffR(bfi>fi(bp+-@ ?>E? pff!KYp?Iff0?4} ffe0*<>fi3ME>Mfi ff*O4 fi.fi2'.R 0 fi4./]^Ch~vffwwlE!Sffid+Mfi pfi(O?pffK*v 043v 3 fi.fi4.K fi./]^5O?Mfi/*0@ >00440v f'fi832fi F1'ff$v+F?2?>v p 5fi 9fi34*?3@ Ev pN?pL4fi0MRff?ff/fi<fi(%?pFfid 4a'/'ffK!ffcfi 'E'*0fi3MffiE?YM@ ff ff5?pL2Off4v pff9fi(O4 >7304fi $'1ffP!-fi 04pC?Mfid+@ _afi q_0 v 'fifi3MCfid 2[0 mfi K()O+-fi.!ffc ?4()O+-fi.Shi NrO 3M@ 8vdlE*/?p@ N@ [fi pffi5fi [fi A+F? j 14Offk802 fhB[ ( '"F *PvffwwlE*L! !*Q04 C?Y Rfi v1344ff3Mff=v n@ fffi 0 m'fi#v 7 No/?1fi-fi 0fi ff!m|9?&fi ]^P02 Z4I3Off'fimv ?pC(bfiEfi(f_k4 :;'3M'fiP'fihir NlE!$r fK?0ff R?MfidT+ 'fiXRff4 @ @ ff fi fi(ffa%p[B<E^5'^'>V<'p`<~'OBp[B<\E\B^ EdB'\^;^<EFB^5~da%<i;B~O^L5-BEE~g\ELB<`B5'Q[;d^\$B\9%B~E>\BffdtE0i;B~g^NB<^dff\EEE~VE\<~9d~B^EQ;fi[pIB"!%&QrOrgsJcH[|$*04t}-m{t/s$qos0pt{al qMff ERb0bigd0a B;( fffi ffffi 1`)ff1Va$#Y1` ffffJa`p/ A` ; 'E 0 (QHfsJcH[|$fi]) ^Fa4/),+.- )Kff ( 1 d0V`)p+04*10E '2fffi3 ~ff4 1 iff1Ja5~& Ei/ # 'ffE $#U 1` ffffJa`6 87 /ff P0)/ffL4)YE~B9'p/B; ~EMffE 0 (rO3@Pv;::`_0N7fi!@ffGfiC040=<''ffGhiA3M./?*Qvffww>@?BAL0M}ff/*Nvffwwx@?>AOfffG.fid/?V*vffww C@?arfi ff*tvffww ClE!F35R#v +F4?K'ZvL Krgv 3M@ Pv!,T$3? @ L@ >[ff'$Pfi3Ma Nfi(O+-d1F?$?p>r04 0ffi32UR8(bfiffG 444!FrMfiLfi p* ?13P G04 Gpff 3[fi32p pQ?pv 404 0!-TF?4$PdK@ ff13@ >fi 04pc fifU. Mfi\+F ffpL! U0ff2v pP 0 74F'fifihi! !*O ff R3v p fi43M4fi fi?0PElf?pCv 4g02 Lv RS4834ff/fi /:!,rMfi ff,h~vffww ClAfi3M4v pffA8fi/ff3M F()fi)fi1 pr f-?$4RffN()fi$137R-fi(fia ffP*Mv23v p8 YffU fi Yfi(t?pED<4fi pff]^5DQ4 ffP/![3P n04 p pCfifi43Mfi fi4?PCpfffi04 0?Y4'()q]^Ofifi4I'fiRm? ?p@ !Wffid+ ff*-'fimfid/4p''27Rff?d1fiEX3E ff*-()fiP`_fi q44'fiR@ ff13 ff!T$3? @ (bfi Y+ G3Y?0fi'fiRk_ ff4v p?3Ufi ff*$?phi83blfi ta4 L?tRL `k_ ffL3fi Fv 3,'fif3Pv p,+Fp? ?pOJ2~$_ ffg4fiRffhi'0L 0G>lE!8c( Mfi*?pa4 S2Q@ 0@ ffXhi'RlE! D,2 0N4YMfiNM@ ff ffmv G?40Rff*4?Mfip3 ?P94< P4Pfi6'fi025(bfi9()33M@ 5@ ff ffE?! a,Q?Mfi33 ?PL@ ff13 $'fiO4_afi !c($?p@ K4Imv 3 fi *9?348?Ifi 3 r 04 X ?4I4-?4`k_ ffW W@ 0@ ffV*t?( pffpff!K{ C4<?048(1fiH=I,J !RhiT$?2Mfifi V*g7+ ff2,7fi?pMfifi C3 ffv ?pf0Rff*49v430pffv ?pTfi8fi( fR7! lOc(?3@ f@ [73009?0-fi1fiRfi*/+ [fi 4p9;+-fi7%fi0444ff!ffc( Kfi z* Pfi 53 ff,?pQO834fi F02 *+F?2?Y4f j fi130kfi(6?p7 /43fi f02 !5T$?45832fi f044Q()fiOffR 0m `$_ ffm'fiU@ (94Q4~$_ ffc4fi067304fi Lfifi0v fi mfiff!IT$p?834fi 7a4 Z2>@ a@ ff(5 `_a4fi Wfi130ff7 fi*6! !*<(i443M Pfi($?pC04 Z'fi4(bKP0fiR;!-+c'2fffi $* ffE?mfi QvpR33 ffF4$fidT+ Uv 0123a04 !Ttfi8 E(bfi0fiRffff*fi pQfi(J?3Qfi 5. ff5?3ffi1309fi(V?3ff Nv /430/04 9'fi()fi C834fi F02 !FT$?2$7304fi f02 U4F+F?Q4$ $_ ff!FrMfi>Kfi *fi p8fifi@ 7v /430M04 5 L@ 0 ffK(J?p>fiR; 4)Mfif4~$_ ffV!f(bN?3I 4a4JhiElF?ff R`$_ ffm G@ a@ ff*?pfi >@ 8$_ ff4pff![{?4_$ff43ffh)fi 4v p\lE*9?pUfi 00Zv ff v p#h ! M!)* fi43fi XfiR'fiElI'fiR?pffPa4 JhiEl3ffpffhi'xlE!ff 0v pRPd&K ff34@ ffX'fiZ0?pYa4 &'fiZ?0 Y33 3 o1RffffMLfimknpokmkq30fiQfi>'fiU_kp :B333?pP04V!cp($ (1fiNHOI fiI ( fffi *J?3vpvUhi734blfi>044K0ff!cp(P'2fffi *N fi UY4 fid+Wr 7*F()Y+F?2??pR834fih)fi/3Elfa4 4N :B()fiOff!Prfi74643fi *Jfi pfi >?pE04Yff`_$ffN?3p+hi83blfi 04 Z'fi3M@ '42,4'$_ ffI?pC@ ff30@ ffZfiRffKhi'0PCG #lE!WW" :()fiPfi fi(8?pR834fi K04 0@ `_a4fi @ R@ ff13 ff'fiqK2O: eQ%fia >Rff3 8?pG@ R(bfiEOffUfi 04v p*R?0[v ? ?K2O:;4O43fi !{p? p $h)@ \lp `_fi >(i44ff*Jfi/3ffJ[fi33 @ oM0 9?g2J3 ff7'fi34p<?39?fi4fi(O U 0 L ff pIfiE'fiFfi$fi?pf04 @ a$LpffpffZhi'8QlE!,TF?4Afi/ff5fi(ff3MvpM*00vpM*M K ()1v 3702 51v ff$v p_ ffPL3ffpff!,T$3? Qv (bfi/35fi(?4Aaf45'0RCP S!"f04N@ E`_afi 7(bO ff p$4JF. N'fifE? /v p$4ffNfi g@ff'%fi ffff!OQ3Mtfi p:Ufit2$'fi oMPv 374Vv ff v pP?Mfi10F G4%fi$fiRpK4 ffF'fiCpEPv p?p134. ff'L@ E`_afi R?Mfi1m()fiffE?Zfi70v 4fi fi(-v ff v pKO?fi1R 0fiRp4ff!qffc #?4Ia+ S@ ff p+ ff3?0v q3 (i3$ ff 0v pmfiR'fi4fi3 ff&'fiRRj ( ffkW+F?@ ff'Rff'fiW2%fi4 fffi([fiRff!ffc Xfi?p+-fi*,4($?p fiRpZ?Mfi40>()fi?pK04 Z4fi8'fiRv ff v pM*O?p&4I2I30 ff#'fiR'44?Mfi4&()v ff v pMT! c([ #fi 30 ffI?pff Uv ff v pmfiR'fiff*-+f44930 ffq'fi@ ff@ I?pfiRfff+F?GUWVff `_a4fi m@ ff30@ ff*%! !*J'a9C?fip3 X? QCv mrO 3M@ Kvpfffi<R)ff3MffV!<T$?464<?p$Rff'6fipFfi304P?MfiR5()fi,v Pfi 04v p530fi +Fp? 504@ff'%fi @ C2OU444!#rMfifi?p ff 0v pmfiR'fi 0&fiRpZ4@ ffPfi3Mm0fi@ff30f 3 !LFfi\+ d*V(bfiQ?pffN U+F?04?S+ I?d p I@ ff34*a+ I@ ffMfi\ffZUW[K \Yff U']_^a`5 `_afi fi4?P!YT$3? ff?fi18fi/4} ?p@ E`_afi vfip,'fi7d F4OFfid -'fi0@ E`_afi ()fi?V! 4{ Q4'fiN0@ ff ,Mfi\ ff0 fiE?()fiTev K :B(bfiv p7344fi [04 V*(bfi[?p83fi hB'2fffi l$v Y+F?04?U?p@@ >8340 Qfi k* ff?S ff v 3I pRp!T$3? TMfi\ ff;Pfi(Jfi3M90fi? 4 Mfi-PE?v pFv ff v p>fi9 _afi- */03M<E?p?pW?pff4 fi(8?pRp+9fiM! T$p? @ R 30Ofi3K4PfiK%fi4N042fi fi(8fi3Mfi?!<rMfi?oM0 */(V /3 ffXfi Nfi@ ffQRff?d1fiE,'fiIfi>a-/30 ff*+pffC'fiO3@ F?5?3MOfi Mfi?fi L33 3ff0v 51E3~:;4. fRff?ff/fiff:! Mfi?p)oM0 Q48Pv pNfi A?-Pbk o/a80,?pff4,04 0<'fiIM0P45fi03vp14fi03M8+F?Mfi@ CRff?d14fiI2pff30ffZfi 'v pff()fifiEfi +F?v ff3M Cfi0fiMfiP ! ? o/a 4704 3fid EI?807'fi33 (bfi@ ff@ Zfi 0fi >+F?4@ffP v pP+f?vE4J4fi Y0OffR! cQMfi?pYoM0 84f3'fiPffG()'fiv ff?CP'fi ff340OP()243M@ ffI03fi p3 UfiEfi #+F4?v ff 4F'fiffCfi?p$'Rff`_fi :! [4'fiM*/?p@ L@ Qfi pfi p43fi 95?pBdT4 ffQW" ff ffE?['fi/4fi #%fi3M233?0v pmfi0p33 P 33ffX ff?4ff'fiZ33 &4%fi'fi o/R4O!T$p? f o/E4O>+-fi34ZRC ff43M'fiPff*9 +9fi32W?138@ ff13@ fi?X0fi &Rff?ff/fi3Mffff!T$3? Y4'4PfiPa44fi X?+-Y+f44AOfi #4P X?pUMfiP &fi(f%fi\+4ff fffi733 04fi )pp+9fi.1!9T$p? [(bfi44fid+Fv 384A $?5fi/3M@ ffffh eJ/Bf8 gih0eg) * ff>R F/v*gvffww/v*aA3v 3ff fffi alE!,+c vffww/v7v KWH j+ c<fi.*0fi/Rff ff'4L344ff?S3ffP Zfid fiV!Gffc 'ffvpG'fim4'8v'fi/v pY?pC@ fi 09?fi'()2j* 6TfNT03ML4QfidW+ p'fi>fi R?pPfi/6%fid+2!PT$?4[+A>Y 360fi *g03MN3E?k a%^V\'9EMdBbKl2dB~Bff^-^'B^A;i EB$B^EnmEWo\Bit^Bff\\~'Fffqpg;\8iNrOr=s;fft v)u \B~5~;5BffdVB6E B~Q\'[^[~E5dBV~^~O`Et^5\Bff^<BOB^512l 15w ~^~\$EKxfi[t}-m{t/s$qos0pt{al qM4fi>@ o1RffffR'fiUfffiv@ffvpG3M'fiPffvm?pP(i3M3M@!fLY@ff3Nfi(6TFNTL]^84fi *6?p@ C+A8Gfi/-fi\+ Ifi\ fiX 6TFNT fi'I7fidW+ %fid+ ff*6+F?2?@ff30ff C7@ ff./Mfi\W+ Cfi(J?pc6TFNTn@ 4fi %fiP7$3 4fi ?pp+9fi.a!,TFp? f fip~:+-fi.@ ff./MfidT+ Wfi0fi ffW'fim@ ffK0fi <0@ ff.1fidW+ v RfiP833 2fi 8'1ffP!T$?2$@ ff./Mfi\W+ S2'fi'v @ ffG()423M@ ff$fi(< Yfi?3Qfi 'fipp+9fi./Qfi[?pIfi33 '*3E? A?pNA'eNfi 'fip;+-fi.X! [9'Eff ff4fi +F2pf+A5?13MAMfidW+ V!6+c C?pQ(i3M3M@ *<4<@ ff'fi 0v 5'fi o1Rff<?09'fiOTp;+-fi.fi 'fi4 6+f441R$40 ffff3v pN7300 *4'03MffSfifiRpK'fi(bp+- fi ff!ITF?4To/a MP42U4423''ff[?pI%fi/:4,13 p042;Yfi(5fi3Ifi 06@ ff'fi3MEff733 0 ffN?pff@ fi 84'()G^@`y`Afi(5?pP()fi4fi\+Fv pE4/6fi v13Mfi3ff3Mfi z<Pfi 'fiv 3Ma* bk o/a F4fi 'fi8(i443@ ff*/( p{<\ ff44042;*K2Off ff'%fi ff!9Q3$fiE? 3M ff5?Ffif4'()C4%fi(g?pff !T$?04V0Rg4Vfi@ 0} ffIJ()fi4fi\+F! fffi I5fi\12pfft 84423'' : oM0v ,?O4g3 ff?Mfi33 ?Mfi3Mg?p9ad! fffi |>f?g?p?pffff70. fi33 07p_ fi Jfi(0r [*fiRp;1Rff*F()fiPf `_fi *F PE?v pm ff 0v pWfiR'fiE! fiE$@ ff30()fiK'Rff`_aPE?v p- ff v 35fiE'fiO@ 9v fffi 7M!gTFp? ff 9v ff v pFfiE'fit4g3M'fi'fiIff3ff?pm} ))nE0)) 'fi/4ffX+F? ffpffa! ' 4fi Zfi 0fi 'Rff`$_ ff7?pfi 4fi 33 pO+F?04?N' :B'fi:;N' 4fi 8ffIRAp!O{ F@ ff@ 6%fi4 AQ0fi@ff30V()fi6'fiO-fi(?pff -fiR'fi*+F3? @ 9 j %fi -FEfi ff3kfOff t?O?p-v ff v pfiR'fi[@ ff fff'Rff`k_ ffK4$fi(60fiRff!5YK?p7fi?pf? 0*0fi33 @ oM0 ff[@@ ff@ ff&'fiR?Mfid+ ?0P'fiOKfi([?pY ff pSfiR'fiEPMafi Mfipffff4Z@ ff C?3fffiRvff! fffi Ux o/$?p>Efi@ ff34A(bfi$?3>7304fi $3fi U'2fffi3 !rMfi74OffN+Fp? 8+ Ifiv p4 P0fit@ ff34* fffi XCCfi\12pffQv@ ffOfi4?PL()fi> :B(bfiv pY?pC7304fi >04 Z R ()1v 3Y*6fi pY+F4?WY+-fi'~:;@fi0v o/p 412C 0ffP04f4Ofia o/4;q ff3!T$p? ff0E4F ff3 ?Mfi\+83?& ( :B0244fi /:B()fi4Rff03MZ()fifi p fi(F?3@ ffO5 fi?0P>fi\ '0.`_fi !RT$p? 0Rfi43pff8+F4?Zm43fi Rfi(F@ ff4ff+-fi.Z 02pff8(bfiI(i3M3M@@ff ffE?!~OM -fik.W{,RvL+F?Lf7344fi# o/a<()fiO ( fffi fiO'2fffi3 ?t2V3ff>?Mfi3p?Mfi3V?p0RA'fiP44430''F?3Q3k_ fi 09 43ff!<T$?pQfffiYACM@ffvp'2fffi3 *+Fp? I8340 7fi L?d P?pffQfi\W+ v pR3f04 !7sJ>v S?pP ff4fi m+ P@ff( fffi *+Fp? @ cffE?Ufi f3@ ff$ @ fiv $832fi $a4 !c~Pfi v p$>fi>+Fp? @ $N ff?04$?<2 pfffi PL04 p6()fi<?p$a3M%fi AfiR( o104fifiL0 <fi4v fffi *(bfioM0v <Jv N?32DO?/k_ pV4fi Q'fifSff!gst. <?p2DO?/_ pff*?p@ P2[K2 phi4 fffi j stkl[(bfi+F?4E?GKPfi04 8fidffO@ ff!ffid+ ff*Ov G?4N?p L@ N;+-fifid E<?pN(i>h j rOkl-fid F(bfi$2'? o/0fi4fi *M ?pNvff4h j c~klAfi\ F()fi$' '( pP Y0 ffA()fi rX'fis,!{ U3O#fi 3ff pP?3fffiRff&?pYv 44,a4 I()fir-*6cE*< qs<*<?Mfi\W+v rO 3MffL X>1!PT$p? ff P P402`$_ ff*?pN? R@ ff44'2*04U()fiN?pP03%fi 7fi(4423''fi V!JA44*fid ,rG4jff?p6fi4 ffpN0 ffO <hiv I'f$QstsJ|-5T5cHNLlVfi,2,pff44 v pN?pff'fi7fid -cFh)+Fp? rW4<v ,'QDQ|,sJ,c :Q|,"FcHNLlE!"Ffid Ac6ff4?p9R@ffff/v pP0 ff= <0P()fizfi\ Qrh)+fp? YcA4F Y$"f|-A|,,c :[cHN\l5fi[FU3ff4?pff 'fi4 pfsnh)+F3? Y$4$ Y$DQ|,sJ,c :Q|,"FcHN\lE!-c(OsZ4Fv Y$"f|-A|,,c :[c~HQ*fiK bl ^^~' {bl \^ p1RRmknpokmkqrJ:;fi2ffqcp:;pff4$QsJst|-AT$cHN/ff4ff4RR ff2{ "F|AA|<c1:fc~HN ul4B~~~^/ rt:;pff2cp:;pff44cp:B@ffffs:B'PsV:B')DQ|,sJc,Q: |,"FcHN ubl \~^^ 1RR ff2)K lb\^ p1 DQ|,sJc,:Q|,"FcHNRR rJ:;3ff4qcp:B@ffffrO3M>12,40A(bfiffidfrhi(bEl5KcNh)?ElE!RR ff2K 4l ;E5{' T$"W[H Uc~T$T5c~HNc;:B@ffff4.l2EsV:B03ff4@/{K l2E\c;:Bffffff2Rc~HQs:B'0P)K l4B~~^ { "f|-A|,c,:[c~HQ c;s:B:B0ffff30 .c;:;3ff4 RRs:B@ffff4rO3M>12D,4Y(bfi$?3L4pFs,!?pK5KffffL?pLP0ff=<8()fic!a[?p+F2*MsRfi34 RQ03''P4'vpI'fi|9?WhivY>T$"W[H Uc~T$T5c~HNLl9fiF030vp8R;+Ufi8hivK'.Dd c~HQ>lE![6Ofi pffI%fid *a4 g@ 5@ @ ff@ ffI3 p[r [! Tr ?Ofk_ 6fi(affhi! !*?p> 4ffElF U44fid+A0 7 :B'fi:;'P' fi >hiB! !*a?pI ffff\ffpfffR;+4fflE!>T$p? 803M%fi 7fi(,?d1v 3C'ff>4F'fiK4143>?pIfi ]^Lfid 4O.Sv 'fiC3M'./!'N+f?v 0fiPv p>fid+ Mfi-()fi Pfi?3$'N4A G))) B;! D,4ff3fiRv 5v K Uv 4%'!D94 ff3Mfi fi/3M<,?pFfi ,. ff9fi *13?,fi 9rm./v pLfi rJ:;fi2 fffiPrt:;pff4 d!|9?#fi ?m@ R'fi4@ fi([%fiafi 0*930 Ifi(F+F?04?&PdZR. ()fi ff?Rfi(9Q'ff! a4 mpff pN'Rff()K?4Q30 Q(bfiYffE?W'!8T$p??fi4Pfi(A 0432Lfi ()fi?4N30 L4Nfi/pff ffmv m?3Prfi pP 4'4!c[4[30OffG?Q()3M?pf44/* MfiQ'Rff`$_ ffU3? @ *@ 3 ffS'fi . 8?37_ V$3 /:B4O?fi4Lfi(6v pv Lfi K()fi P'![3['R#ff() ?p7 [fi(<fi [()fiTffE?mfi(6?pIfi hir-*%cE*sgl5 Yfi3Mo/P0 !frq?;+-fiL%fi40fi Ort:;fi4 ff,rt:;pff2 ff!OT$p? A_0O4fi POff 6?0,rmfi4 ff<P0 ffz<\fi/*< 0W?p fffifi XOff 8?I8pff4 7?pff ffP7'fiGcE!O"$fi\ cL4'fi?Ip+9fiW4fi Kcp:B@ ffff Xcp:;pff4 d!T$3? _a'fi #Off 0Ic>@ ffff ff0v ff= <()fir-*6 Z?p fffi XOff 8?8pff4 7?pff ffP8'fims<!Os?8?M@fi s:' P*tsV:B03 *O Zs:B@ ffff !YTFp? _0>fiff >s' PLS'fiS|9?*g?pfffi ?0$9a3 ffAR;+ fiRfi *M C?pQ?4?0$A@ffff ff5a ff= <8()fic!/rfiLff?Sr >*/?pN 5fi(g4fid+A0 Nfi 05(bfi ffE?U'L2-ff$_ ff v rgv 3M@ ff5I>=Kfi[t}-m{t/s$qos0pt{al qMvmP2t()fip o1Q'fiK?p'!rMfi o/a*fid8rRfiY.4fiRrt:;pff4Q()fi$DQ|9sV1c :[|9"$c~HQ!T$3? [} ~ biKabi hiB! !*?p-4fi 4o1 fffi g4Rff4v 3$?pffpffEltv 8 8r #04pffER[?3> $fi(6fi F?La LP' :B'fi:;''E fi 'fifi13ff!,T$p? NfiR'fiBj fH[D8*^k XOff j N"L*^kPff j HNQTN!^kPT$3? Qfi 0fi j ff4 ffkP+F44aRQ3k_ pff?Mfi!6T$p? $' 0fi Pfi 0fi ,fi(fi pffi@ ( -'fi>?p[fi 9fi(fi pFfiAfi@ ffi?pfi!9T$?04A45Rff3 cffE?Ufi f453Off 'fiPRN@ ff>'fiP+F?$$?0Afi0 ffUfi?pfifMfi pM!6cp( Mfi5/4a *1fi [fiP7$3 4L?pff4$fi UE?Mfi4!0P Rfi ]^8fi W@ R'fi@ >4fid+A0fi 0L()fi ff?Z'?d Rp k_ 3ff* j ff4 ffkSRRPpk_ 3ff!T$p? 'E fi Wfi fi j ff2 ffkU4Rff4 pC Wfi3Mfiv pffp()fi8L4A C0@ /4fi CpMfivp8?pN Afi(t20@ ffv v p8fi A?5PdR[.()fi?p'?8@ Mfi8@ ffMmfid ffZ1mfi?p>'E fi Wfi fi 0!rMfio/P0 *v rO 3M >1*ts<]^L?M@ 'fifi 4fi Q()fi'TFW" [H Sc~T$T5c~HQ @ Gh)c;:B@ ffff4s:B'E PElE*Vh)cp:B@ ffff Zs:B030 \lE*M j ff4 !^kRsfi 4. Ls:B' P-fiAsV:B03 [()fi?4A'!-Ffi\+ d*0fi\ $c9fi32. >c;:;pff2 $v 'ffCfi(gc;:B ffff !,T$p? (bfi@ * ?04Aff13 A'fiSh'h)cp:;pff44 qXs:B'E PElh)c;:;pff2 qXsV:B03 \l'lE!j ff4 ffk4)Kr 02@ ff 5fi(t44fid+A0 Qfi ff1p3ff!6ffc 0234ff*/804 4-?p5fi(O4%fi ff1p3ff$?5Rv v K Kv 2a'7 fiR?pN' 0fifi 4fi !o/P0fi ff33Y4fi\+ ffqXr-]^P04 &2Yh'hirJ:;fi2 ffcp:;pff44 lE*Qhirt:;fi4v ffcp:B@ ffff \lE*<hirt:;pff4 B&cp:B@ ffff \lE*O!!! lC+F3? @ >r&. ffQ[fi [ Yfia fffcE]^[fi [ff?G'U ?p> ff1p3!A933 /:B4*?pff fr 04 0<@ Fv ffPv I?p$()fi4fi\+Fv pL 3pff! AX 2@4OPV#* fi hir-*Vc*Jsgl[4N>fipPfi(-?pP'ff8v Q04 V*V RLffv ffN?pp o/fi 'fiL. X! T-?Mfi1fi $?3ff,fi ,v pRp!gT$p? IMfi MfiXpffP'fi7'?fi }fifi?Mfi2!,T$p? Q?Mfi2[fi(tfi KP ?,R[0 ff*1(bfi?oM0 */fi fiPv M03<()fi?p/fi![?Mfip3 ?YfiP0 P02 m+9fi32v4337?pa4Q()fi>fi WE?Mfi4*Ofi pffZfi\ *6p? @ P+ ffd C733'Rff`$_ ffmv R?pr 04 0!CQ3M>fi (bfi8Mfiv p?474L?0>?7?p()fi130>fi(5?2N0R74>fi W?3 `_fi Rfi(5fiffL%fi3M7fi@ fffi ff33ff!,T$3? N04A()fiFfi?Mfi4745@ ffv 5'fiP?pff >fiRff!0ffE?&fi ?I?fi X4fi *<4-fi @ K3OffZ'fiSfi0@ ?pC4fifi(<?p8fi?pNfi Q?N@ 8fi 3ffSv G[r ' fi Ufi 0fi ff!frMfiYoM0 *r-]^' fifi 4fifi c]^Lfi ff*t'fiYr pff0N'fiYfi0 P+F?NcQ4!A fffi ZfidT+fi C P?fi Ffi(%?pffi?p,@ ffv 9fidhiElE*M fi 9.Mfi\+F<?pp o/,'['fi>+F?2?Q+F42V' fi !LT$3? @ 4[fi Yfi p%fi0 3 o1L'Rff3@ 8?3Ir [N@ P3OffG'fiRpEPv 44!SrMfioM0 *<4($r=48v IFQsJst|-AT5c~HN '*9 Z48E?Mfifi@ fffirt:;fi4 ff*g RQfi0 ffLcF./v pY4fi Rcp:;pff4 ff*?3RQ+F44g'dm >$QstsJ|-5T5cHN'!TFp? fi/ffNfi(-Rffv pKv mY'*O?Mfi1fiv 3Y Rfi *tfi0 /v p ?pPfi 0Nfi(Afi?p*0?pUfid/v p'fifip o1f'*a4AffffSv p_ ff!fim(iff*<+ K?ff KRX3v pm'2fffi3 +fp? @ ff?#fi P?8fi\W+ &v 4140304 V![c(<+ 3OP( fffi *?pff?Rfi L3 ff[?pO834fi [04 S'fiYpff4pfi ff! 832fi I04 48()fiOffX1R.1 p j fi/3kXhipk_ pffXv3Ma fffi >1!4vdlfi(<?p804 0$()fi[r-*cE*% Gs<!aTF?4$0fi103Ffi/pff4f?p7E?Mfi Mfi30FRff?d1fi[fi(6?pIfi ff*+Fp? j '0?Mfi fi3kKOff >?>ffE?W2OP'aRfi >. ff8 Rfi V*Jfi0 fffi [fi(<fi?3[fi *V S?pS' 4fi $'fiKfp o1['!NT$p? 7fi/3$02 S4F(bfiEOff*ff 444*11C.1v 38?37Aff40fi1039fi(g?pNv 01233M'fi'fiU'ff$ 0 ?pLv /:=fimknpokmkqff4fiUfi(O?pL'EfiKfi0fiff!,S304fiffiW0L :B'fi:;'P'fivW?p0fi103L04V!UrMfi o/P0*<(A?p fi @ fivm. ?pC4fiIrJ:;3ff4IWcp:@ffff Q s:B' P*?34afi -+F24M' fi()fi?p @ fiv9'hB$QstsJ|-5T5cHN8*"F|AA|<1c :[cHN8*%T$W" [H Uc~T$T5c~HQ>lA'fi ?p @ fiv Q'UhiDQ|,sV1c :Q|,"$c~HN8*DQ|9sVc1:[|9"$c~HQ8*"f|6:A|,,c :[cHNLl5@ @ ff ffY1K'a ff5fi(6'ffQv Y?3>r ff()fifr-*0c*a Ss<! 832fi f04fi 2'[fi(,?p Qfi(92tfi @ ffp3 0ffQ?NRv @ fiv Nv04V'fi(9?pIfi/304 K 0KfiR?p>' 4fi Cfi 0fi ff!{p? ?p6?p543fi I4<2fffi3 fi9( fffi *[832fi O04 pff0O'fi[R-(bfiEOff'fiE(bfiaV7304fiFfifiEvfiYfiRff>hi@7'mfi(6rO3M@vdlE!::_afiYfi(fi0Q0fiRfffi4Kfi(8.1v3+f?p?pW)9fi(8?pRfi=ff33ffY4fi\+-ffn?pfi/3A02 K4'()?3LfiRp!3-4tfi(ghfi0blJfiRvffVfi(0a434g4%fi*+f?4?84gM@ffff7?3@*4t?fi(g(bfia4pC7304fiA4fi5?5+N+-5fi3M$fi$'fi4+-d1$dfi4*4vff \i0EfiRvff! o/P0 Y40fiR;DFv h)c;:;3ff4sV:B' ElE*9+f?4?q'ff?0?Mfi304Y+Aff/[>?p7I?fc$Mfi ffTMfi[pff4 [f?37O84O>?[sX4F' 'vpM!T$?2f0fiR;Y0@ Lfi0 fff?0LPffm4 I()fi?p4 3L483pfi3K@ ffff/v pp+0 (bficf+F?04 8'P4'vp fi4pLY'fiY|,?!PTFp? P fffi R2%fiN4L/:M@ ff ffUp? 74 & / [fiRvff!5T$p? ff 7fiRfff'I?f(<04304f834fifi &h)?p j ' klf?ffi/3M@ ff*?p 304U Mfi?3[834fi Qfi &h)?p3ffff~:j ff'%fi ffklQ+f446fi/3Mff\! oM0v 4L0fiR;XD-18c($rt:;pff4 8?7fi13M@ ff*g?p34 sR+F24ff3M8s:B@ ffff4 !c(%?pF02 ,v rgv 3M@ ff9>>L@ [fi>0 pffv 'fi777304fi ,04 V*+F44/?04,834fi04U4'() fiRffDfv>D9'+vp?04$13pff'fiS45fia0 `e34-fi[4%fi~:>()fiNfi'[@ffpN(6?38pEPv fi G4[0 ffGfi G/43gv 'Rfffi Ufi(,?pIr [! c?p@ @ fi 4mUfi3M0 Pfi(A RP4*g40v Ir [> ?4coM0v CT$?04L4423''ffN?Mfi\+GP( +20 Lfi *+Fp? UpM*ao/?00$fia \fi0Jff?0ff/fi*?3@P./v pfi06fi LRff?d1fiL0`e3f'fiK@ ff2!Av ffY?p I2N pffm(bfi> fifi3[R:?d1fiE3 ff* ff'Rff444G>?p1377 Rfi0v o/pmfi($fi 8v@ ff@ ff!KUfi/pff3? ff./v pY(i34S3M'fiPff7?4Qfi/ff! [fi p 'fiYfi3M>fi/pff6Ep? ff. ff*O?p0fi103Q04()fiFr-*cE* 0YsW2~$_ ff5fiffDFvL 0D91!"ffifi3f3ffY@m4'fi pff3ffn()Yff0vpM! 3M%fiU2p s<]^C''Pfiff!ZT$3? Zfi 3 v ff v pSfiR'fi?Pfi34R04 ff48'fiRpff Ks,]^fis:B'E P*5+f?4?q?p@ ff() @ ?04fi n()fi Rffv 3R. (bfi 'mT$T" fH :ScT$T$cHN8! f(bC0/v pm ff pmfiE'fiff*9 `_afi qPffXRK@ ff13@ ff!XrMfi?402345fiR'fi7hipffv vpPfi lEk* Mfi@ E`_afi Y2)pffpffWhi fffi KlE!ffc U7344fi[34fi*+F?TfPfi1_$ffK1 vffvpm{n?Mfi(bfiEPFU`_$ffF?pfi/37r QZ+F?MfiS(bfiL@04>(A_afi()24*OW+f?747@04@ffaT$?p'+ Q'fiY?3ff p3 fffi NpRGfi m+Fp? ?3LN4>( fffi fi7'2fffi3 !7c(A( Kfi *?p>fi F+f?K?p ffff'ffia3Mfi Jfi\+ d3* ! M!*2 p$sv fi3MLoM0 *aPv v?pfi130f02 mG0/v p ff p'fiY4*V ()1 pC4*V@ 0Ev p4Lcpffpff*J 0m?p0v p7Ifi1fi(g9'fiP20fi(t?pNfi5'fi3@ !6cp(g'2fffi3 *M Cfi $02 ff9v ff v p7'fi6fi\W+ Pv 0/4304 !OT$p? $v 0/4304 O@ $?3P <'fiL?pFfiP03Mfi 2>%fid+ (i3*+F?Mfif(bfiEPg?pAfi/3t07 E`$fi_ fft?OfiRffg@ 54~k_ ff!Jc(@ 04t ?3ffpff*fipLfiffi@ >fi $@ a5?pff$fi\W+ Yv 0/43004 !=Jfi[t}-m{t/s$qos0pt{al qMc$4$3OffKp? @ N?0FP?0v pLff0vpIfiR'fi$@704ffCfip :;~:;:B2O>R$fi?pO?Iv70E?*(aKfi *?3Afi6fi:fi$a4tL./vp[3M0gvffvpyh Dgfi'ff*VvffwwlE!, fi C?pff [3Pfi*?46a-Mfiff-Mfi9(bfi/3,fi?p[ffpLfiR:'fiOR< Lh)fi?p<?0 I'fiLpk_ pA?pfflE!OcO(bfi/3 ff6v 0'ff8fi ?pAfi3MfiO$ ff3v3F()fi?p024fi Gfi(-Kv ff v pCfiR'fiff!Iffc m0432ff*+ P0M@ ffQ?p `_afi 2p3 !8T$p?p o/Ffffi \ ffF3@ ()30z. fi33 U3k_ fi 0 pff3ffK(bfi[33 pE'v 3>@ _afi !| ,{RT$?2 fffi fid/4pffpk_ 4fi Ifi(Nr f*Afiff*- `_afi V*A 0qPE?v pUv ff v pfiR'fi!Yrfi8Sffff*O33703Mfi3>33pvp fi(5?p@ff34Lv?2L0Rff*gPfi(?pff 7pk_ 4fi A@ N(bfiEP!}b ez {] ]G ekgz^KE]k^r [L?ffLff'N()fi3MLMfiff7fid844g02PhiHf24'fi*6vffwQy?6DffZ {ff42P*vffww/vdlE!YrMfi8fip*O334.P246a4*t?pP;1RPfi(A_k4 :;'3M'fi'fiW02L3ffW?p@4fi\+F[%fi42Yv /k_ 0Chiv 0pPv 0\l$ 3?G4fi ff33ff! &T$?2Ffid/4pffQffifi/fi/pffAfiW( ff7Rffpff&fi P?Pfi v1344W@ ff'%fi 'fiW?pffO1fi8+f?Mfi3MU`_a2VP fi Y'fiC?pff[Rff?ff/fiff!f|6ff3Mfi G Sv ff v pPffUR> v ffff ffmv3MP 3pd! TMfi?3A fiN4,?0Ar 04 -?ff N'ffff* ?3f02 pff p3 [?pff N'ffA'fiI@ ff A3M0'.19fi(V?pQfi\ 4'.!,T$?493M/4pffO?3f02 v 'fiP2 -$3 *?p@ 1%fi24Pvffv p7?p[fiP@ ffp? 0444;8fi(J?p[04 ff! ff54'fi0v I0`Lfi W?Mfi4ff>>`R@ [2Off* R4(,?pP 0'fiS M03MQ@ ?pPO!?fifi(5r 04 >4Q?>?3m P04304Y+ ff4`:;3ffm'fiSPfi1pff2v p ?pfi3M@ [Rff?ff/fi[fi(,830v Lfi! G0'137Rffi(,v p :;fi fa4 FmRfffiRpffP pRp> ?pPfi%fi ffv 'fi>L'?fi Mfi3<832fi 904 Yh)()fi,+F?2?fi0O0fiRff[PffGR8ffffalQv mK''v ?'()fi+AP 3pd!7rOv 44*%r 04 QR`k_ ffR3v 3C?3P m%fi034NRff/BB 4)/ 'iYPfi1pff6Ep? ff./v pSO?Mfi/*! M!*a@ AL3M?h~vffwwlE!2Mfi8fi(<r 04 Fffi0fi@ ffY'fi24t04 F4$?[?p@ 84F@ ffQpfffi(J ff ff?Y?$?ARMfi pQfi K3M'fiP42()fiPv 3844%04 0z* ! M!*a Lff{ff44 =h~vffww/vdlE!cp4P33v ff?Mfid+ 730?#fi(Q?2PP ?PRY0420 'fiWr [!c&?pfi?pC? * fi43Mfi fiE?PPqRY3 ffX'fi fi4 Sr 04 YhirMfi ffB*Lvffww ClE!4M fi[fi(Jr 04 0,-fi%fi ff'fi702 ,fi%fi ffCfi(VE3 F A49?-?pQ4'5Pffo/@ ff6[04Ifi A3v!OTH ?pff ffO()fi604 0J?6ff34@ 9()fiP1 _afi *r [@ P0@ ( E0 Rff3 P?3fi0 oRvfi 0N?7Wfi13>R;+WE3 ffNP. ?pffY?EY'fi E(b!5rMfiEPJ E`_afi U(bfiQr ff4F137'fi0?44ffU U+F4pff43 ffSv( ;:;4tv 30''4042fi !T$?04<3Ma fffi *+F?4E?46a fffi AL3ME? Uh~vffwwlE*ME $b0I3PPE} ff<?pf04,fi(?pr [830 ff'fiRfi13ff9fi 04 ff!mrO 3@ ffS 0>G4243''?pKpk_ 4fi !mT$?470R()fi13@ ffLfi r f>?7fi/pff9fi 8+F?WY%fi24v /k_ 04( 4O*J @ ff ffW8v /_ :; p? j ''Ev pkYhi! !*0 ff33Lfi(Ofi ElE!dvJ~B%`O~BWmVBqo\BO^^ B^5~J$'$O \f^9pg;\EIiNr=rOsdNr=rOr;=Kfimknpokmkq()fi@ 3vpUfi3M8043fiRfi(F3'fiP/*,+0$b0ff>'fip _pC-fi1fiffX`:/!X|6oM0vffI?Mfi3p?Mfi38?0480RI?0ffU3M'fi'fi#'fiXfi4fi o/@ffffv -fifivffY/*MRff3@L9fi1fiffYv330PP}ff-?pff@N'4fiKfi`:fiff!<9fi1fiffS452'fiP3(i3(bfif3v4 o/@ffpI?pLfiRvff!6r3M?pEfi@*C4ffvC()fi 3'fiXpffERY;+-fiXfi(N?pG@ffOF`_afinfi4?P4([+m3-fifiv ff U EOMfifi !,TFp? @ ()fi@ *+ N $b0C3P} Q?pL04Afi(69fi1fi ff /!-fifiv ffZ 04>Y >fi?( ff ff>+F?R2'vp32p? ff ff ffO>yU 0#v*g4fi ff3 33<?pF-fi1fi ff* -*1 fiEfi * 04(b/v pN?p['EfiRfffh .fi'./*vffw CwlE!9rfiffv ffOK7fi( Ia* q.I494 ff?p7PEfi( 0aa* q.I494 ff?p)fi'(0*1 C464 ff?37Fa4Pfffi'( J!6rMfi9?Mfi $ ffp<+f?MfiL@ F33 ()P244g+F?-fifiv ff Cv +F?Mfi>+A -'fi[v 34fi P()fi9?pff@ ffiRfi 0*149Pdp? ffC'fi82Pfi v p?ff? ff ffO>fi( 47ff(5U@ *:! M!*<U 8fi($fi !S* @ fiv *Ofi0 ffO+-fi34Z?pXRC v ff4fi *<33 fi V*< Xfi0 ffO*<@ ffff4 ff!R|, ffIym v*v W?4>@ *6+9fi32RR?pfffpR Kh}lL Z?pC 7fi(f4ff ff8v R?3C33 0 S,h LlE*@ff'Rffff4!T$3? U-fifi ff @ U3Off&'fiZ_ !qT$3? @ U4PR045fiEpfi pZ?pff ffO'* L*1+F?4E?49pk_ pffCZ4(% 0fi 4P'( q.PJ!6c9Pdp? ffC'fi8?v M.Ifi;(fi fi30f'fi()fiQ !fTFp? ff ff[y 0Wv8@ 83k_ pffUzI*%t*zI*8v!AT$p? ^a]1V3\Chi fi fi3F'fi p : ff ffOFl-fi;( Iv* 9yh >lE*%@ L?3Mfi M}fi ff ffOfi( Pv 04P+F?K ff'Rff$'fiL!9+c K?3Lfid EWoM0 *afi [r-*cE*a Ys ff?G?ff 7?pfffidT+ q-fifiv ff q ER+F?&4'fiP!T$p? S'fiPfi(Nr-]^-fi1fi ff n R U4firt:;fi4 ffACrJ:;3ff4 ff ?1?pf'fi9fi(c]^- 8@ Fcp:B@ ffff Q c;:;3ff4 ff ??p['fi9fi(Vs,]^E@ 8s:B' P*asV:B03 * UsV:B@ ffff !FTFp? ff ffIhirJ:;fi2 ffBqrt:;pff4 lAfi(<r-]^-fifiv ff U EpffRffA?pL@ $fi(6fi rt:;fi4 ff*%rt:;pff4 ff !-fifiv ff09qV4<.K=^@`1K 3 ^8fiW( ='( qV4<Mfi 3ff;30 <fiW( ?094<4fi ff3 33f?pIfiRfi 0B* A*V * m4'fiK?[?p4'p343? ff ff ffONy Xv.!4>?p~ff Md>K fffi($3M0 09q~\! TZ'fifi(5?3fi/3Lv U2L?pCO7fi(?pQ'fiPAfi(V?pQ3Ma !6rMfi?oM0 *Mn( ^ (K ^ @ N'fiP-fi(t3Ma(J *@ff'Rffff4*M?p^ ( ^ 45 S'fi fi( ( q~!T$3? L9fi1fi ff P()fiFfi [r-]^Ffi f4A?p>P4 ff'5fi3Lfi v 0v p?p>'fifi(,r-]^Q /!fcpQfi v Q4J-fi1fi ff ff ffO[(bfiffU()fi r9]^Q'fiPN30v pP?38-fi1fi fffiR'fiG-* -*N *Nv23v py# v! T$p? ff ROWpk_ 0fi ?Mfi4()fiUc s,]^EK! n4?pY0fi103R30 ffX(bfiC4A' fi &fi 4fiv ?pR832fiK02 hiB! !*N?pfi/3Cfi(7?3Wr-*5cE*$ sr flE! cpR'fi fi(8?pfi/3[qR28hirt:;fi4 ffEqcp:B@ ffff s:B030 \lE!NT$?4[4f?pI(bfi fi(94fi. Y2734pfi3I1?p[?M@ Nfi ! [ EZ{* *@ N3M0 0,fi(t?pfi/35 0PffJ!3@ 3 Mfi\+ 'fiZ3M'fi/!nrMfiEP4*A qr fi(Q?pY;1R#Ufi 2p@ ff&p? @ S4P?M@ :B30 U h ChB<l hB<l ahB<l'lI+Fp? @ ChB<l84I?pY@ fi(f 2ffUhi'ffElfi(N-Z*47?p-fi1fi ff X Gfi@ fffi 0v pK'fiW-* hB<lP;hB<Bl hB,l 2L?p P'`oRfi(' fi 8fi fi 0t+f?4?8@ ?ff ffO6fi( I*ahB<l hB,lJ 5?p5v 4'ff ! f[4'fiM*hB,Rl hB<l hl@y C2N?p@ >fi(5@ ffff ffp ff>fi $pffvpS0Nfi(hB,lL4ffFfi(6A! h\lE*a+F?4E?K45 U0@ /4fi C(bfi hB<l hlE*a45?3N' 4fi Kfi fi fi(ffa%pBt\$E^9gE\ M\J;= _EE%^6\BELiNr=r k ;%\^m%E5~^F\^iB^E^[FEffNVNEpBE BE&B6MEB\tEE~B~tN%m ~ pV^J^J5B'$BO EJ;B^E1^5\^~b=fi[t}-m{t/s$qos0pt{al qM{ff$NsJsJ|AAT5c~HQI*/"f|-A|,c,:[c~HQ8*TF"W[H Sc~T$T5c~HQfirt:;pff4qXc;:B@ffff4.Xs:B'EPff)DQ|9sVc1:[|9"$c~HQ8*aD[|9sVc1:[|9"$c~HQI*0"F|AA|<c1:fc~HNfirO3M@NMDg$fi(O?pNfi/3502 (bfiFfi[r9*Mc*0Us<!hB<l[fi@ ff'%fi v 3P'fi ffpP!>Hffi?Q+8fiYffpffN4Rffff j y/!^k=-Ufi3MNp _fi*ffp>+F?MfiN'0fifi4fiK45yPMfiffLMfi) oM4'!6{8Y0ff pMfi hlhM l6()fi-?pf'EfiCfi4fififf'%fivp>'fi7?pffpfivp8(bfi oMV'fio!rfi oM0*Jrgv3M@>1* h'hiT$"W[H Uc~T$T5c~HN8*nDRd c~HNLl'lN4h)c;:B@ffff4s:Ba3 \lE!rO 3M ffFI>2443''Ef?pff@ >r 3k_ fi 0!gTFp? @ L@ >r 04 0A(bfi$?@ Lfi ff*r-*ac*% SsX+f?U 4ffR* ffpff* G' fi Ufi 0fi ffT! Sv 0fiPv pfid+'fiK*Mfi$()fiCfi?pf'* $_ ff-?$?452$ Yv 44%'!834fi$04 U45()fiOffK()fi v p Nfi f02 ./v pP?3; A'ff /d5hi4'fi4 ffG?p j 'E?Mfi Mfi3[fi130kfiN20 j fi130kl$fi(9?pr fQfi ff'%fi v p'fiK?pv 0123a4!6rMfi4*?pN0'fiFfi130A25pk_ pffK( RhB;l ahBBl'l+F?p2$?pAff4Sfi/3*aU?3>'fi[fi/3hBB l hB lAfi(;UW'/:4fi&P'4ffP2Pp _kpff& hB ( l hB l h'h (J ( l h ' l'lP hB ( l h (J ( lhB l h z lQ()fiCh (J ( l. hB ( lE* h lE hB lE! c+R+9fi0*t?3Pfi/3h$hB;lr 4f()fiOffGK./vpC?pAff4U0fi103Ffi(<?372ffNG?p8@fffiGfi(<?p'fiGfifi0!Qc+44J'ffLfi(<?pIfi/3[r @I3M0vffF()fiOffG()fi ?pv4'ffffi(t?3>v41403r [!T$3? ffi130-r Pfi1pff2-I Afi(g'0?Mfi fi3Ar f!<TF?p[-fi1fiffKE8fi@ff'%fi/:v pI'fiP?3Nfi/35r2A?pNfi/35 E/!<rMfifrO 3M ffFI>1*'fiP()fi834N?pLrfi/pff4v 3?pO@ 834fi N04 *%+ . P?pfiLfi/3r c s&fi(-?p?M@ Yr [!qrfi?4I0'fiPfi/3*Zah l hB$QsJst|-AT$cHN8*<"F|AA|<1c :[cHN8*<T$T" fH :ScT$T$cHNLlE*5hB$QsJst|-AT$cHN8*J"f|-A|,,c :[cHN8* Dd c~HQ>lE*5hBFQsJst|-AT5c~HN8*J"F|AA|<1c :[cHN8*"F|AA|<1c :[cHNLl 2! DO$fi(O?pN0'fiFfi130Ar 4$?MfidW+ K YrO 3M@ QM!H o1g+9pk_ 3,?pZ` ^@Uaz^1-fi( 8r 7*d+F?2?>2V?p- tfi(2fi 8 ff1p3fftEP'ff1>?p5r #a4 !tTtfiLMfiQ?4ff*+ -_0'6p_ p-BK]3 YUa*+F?4E?82gQ ff1p35fi(fi $hi'fiPlE!rMfi4*VK'' p#2[ Wv /k_ 4 :;4Ofi % ff'fiff*A h 0 9l "-yh >l V*J! !*J ''Ev pK4Qv /_ Rh -l 3?# ff1p3Ufi(Nfi 0Gh)+F3? @ 2?pS-fi1fi ff qv W30 ff&1#,lE!O1 C } b!4Im ff1p3Wh 0 l7fi(F 4ff3E?X?EY'* h #" (l y/*! !* vR h J M#" ( l[Rff3 ?pPz- 'fiP!+c fi?3L+9fiE*t E33 mfi(5Y''v 3 2N?pff1p3Cfi($ 4ff8/4ffv Z r +Fp? R?3C''Ev pS4~$_ ff7?pC' 4fi Wfi 4fifi p?3ffpff!=%$fiT$?3L4p3fiLfi(Or&hB<lqmknpokmkqX45p _pffK-hy>lW?0$33'Gh0l$vU+F4?( 0 ahB<l3E?K332N4ffqEEbG=/M*gW4L4m'fi^@[O[)v]['vp<!G@ff13@fffiYpP330Afi(OUr @>+F?$454vffK?p>r pB0EKE);iM!,cff ?4F*?pIEfi Gfi 4Ffi(<fi 3Ifi 0fi $0vp 33 f730'fRv Gv G mv 4'! T$p? G `_afi 443M@44C?pff r [*F+f?4?Y/k_ :;v p?q''Ev p*6: ^@ ]_VM\^a]_^Gyh AL3?*vffwwlE!z( +fi@ pk_ 4fi L@ pffpff! Tr z4.[$VM\*)`iff ]~ 4(p*g(bfiffE?&'' hB<lE*q2tfia 5(O'Rh6lv!c+8fi?p+fi*rff`$_ fft+F?6' :B'fi:;'+,.-0/43,211' fi C?pLfi F?Mfi32C. N(bfiF2a%fi0 [fi 5. U?pNfi?pFfi !9TF?4A4C G@ ff'fi 0v 83fi G'fiKP. Rff3 Ifi?p+F4 I?pfi N+9fi32fi[.Mfi\++F?'fiYMfiYv m'fiP3PffO! Tr 465ff ]~d3 \YUWY K]1Y[I>7U( 3 38:9 h 43 lh 43 l<y/!tffc >fi?3g+-fiff*?p-E?Mfi4Afi(0fi I33 0433 ff[3 Pv pfft+F?2I? ffp 9+f44R. ()fiQ'! r q4Opv 4'2,(04g463Pv 2'4<:ffE?fi(4gff;! dT0 fffi?p+f4 K'ff*-4I3Off&3? @ ?45r ffiP0 Xpv 4'2!RT$p?@ ff''E4fi I'fi>pEPv 449r [<4:fi<LP @ fi<fi0v ffff30 5(bfiXMfi pv 4'2r ?p@ >45pPv 04'4ffi p>pP?p>OL4 p3fi Cyh AL3M?0 *vffwwlE!{ 4'fi pffS?ppk_ 4fi Kfi(,[$a[$`va?![Sfi/pfftEp? ff./v p p024Kfi 4ffi(fi1fi.1 p()fiL/ff*JL3ffRffGv fffi X>1 ! >1O! ;)^ ]<v mr 4QK ff33fi(, 2ff"(hl ChB<l*t(bfiEU>=zv3E??hM J#" ( 9l hB<l[()fiY$y U@?v*g! !*03 Bh MA"l/Z!pc(BC*9?pD=4Im1v !Z|,E?&1v Yv Z&r 04 4fi\+FI?p(0%fia44pC?0N?pPfi 7Rv /k_ 0ffKfi()*VfiLLfi 3 >pff@ ffV*@ 12[?p 4ffNfi(?p>/!<c$4'fiP204 ff-?fP3M0'v pIYRQ@ RffffUv 0pk_ ff4!{ 3 o154423''Q'fiONfi(t?pff Lpk_ 0fi! o/a N''v pv C?pL2 p3fi Qfi(gr*M?p77344fi $r ?$25?pNfi/35fi(Or-*c* Ys<*a4h'hirt:;fi4 ffBcp:B@ ffff &s:B' PElE*hirt:;pff44 qcp:B@ ffff &s:B@ ffff \lE*hirt:;pff44 qcp:B@ ffff &s:B' PElE*hirt:;pff44 qcp:;pff4 qsV:B@ ffff \lE*t!!! lE!T$?2545P ff1p3Lfi(O'fiPFfi( 3! =$3 fi(g?045''v 3I2h'hB$NsJsJ|AAT5c~HQI*"F|-5|<1c :fc~HQI*T$W" [H Uc~T$T5c~HQ>lE*hiDQ|,sJ,c :Q|,"FcHN8*0"f|-A|,,c :[c~HQ8*a"F|-5|<1c :fc~HQ>lE*hiDQ|,sJ,c :Q|,"FcHN8*0"f|-A|,,c :[c~HQ8*aT$W" [H Uc~T$T5c~HQ>lE*hiDQ|,sJ,c :Q|,"FcHN8*0DQ|9sV1c :[|9"$c~HQ8*a"F|-5|<1c :fc~HQ>lE*hB$NsJsJ|AAT5c~HQI*"F|-5|<1c :fc~HQI*"F|-5|<1c :fc~HQ>lE*!!! lE![4Vr [$v YrO 3@ ffF>@ >fia > SpPv 04'4!6rMfio/P0 *0v KrO 3@ >1*fi\ 5c9Cfi P. L4fi c;:;pff2 9()fiADQ|,sJ,c :Q|,"FcHN'!9ffid+ Lfi0fi?fi47fi(9s3Pv pff[C33 41p3 p o/[8()fi[c$(bfi DQ|,sJ,c :Q|,"FcHN8!%rMfiYo/P0 *(6s. ffNs:B' 0Pf?pUc$83'['ffSv U'DQ|,sJ,c :Q|,"FcHN8*% G(6sZ. ffNsV:B@ ffff 8fis:Ba3 N?pKcA83'LfiP'fi'7"f|-A|,,c :[cHN8!=dfi[bFEt}-m{t/s$qos0pt{al qMgieHG2zgJ[Hffid+?N+?ffffffm?3Ir (bfi44 3ffG()fiLfiN02*%+-@ff[?p1p3 ff'fi fi(O?Mfid+'fi(bfi4} QfiRffff!Orfi$`_a4fi*MfiRffA@Npa44 o/@ffffff?pffr[7h)()fif3M'fi:B?pfi@ 47`_a4fi al-fif Kff%fiVfi 2!-T@ *0+ >3O`yYUVff ^1[ff%fifi 2!gffc fi?p,+9fi0*+ F30O5?<45fi/ff<4v 3ff> P+ Ffi Mfifi 2p>4833fi3Q%fi0v 7(i3M3M@ ffff! dfv pK?pv fi?fi( :<0< {fiRh~vffw QClE*fipfi 7R4v pffNffPfiE<fi 4()fi834Y 'fiU Z3M'fiP'fih)Rff3 3M'fi@ Cfi@ f o1 ff P?0 Z4 pff>ff%fi9fi 4dlE!U-fi?Z@ @ ff@ fi 07@ 3 ffZp? @ !YTtfi4P04()Lfi3M9fi1fi()9v fffi M*1fiRff< W o/@ ff ffv ff%fiafi 4!6rMfi5'fiOffi(?pvffO`_afi WO?Mfi/Nv fffi XC1*t+ 3 3M'fiP:B?pfi 4?Mfi10Q+F?Yr @ ff 4fi K(bfiF?pLfiRp!93,#p_ v pQff%fiafi 45fiff!OS fi(?pFp_ fi 6 F0 fffiG 3CD-33 ff49h~vffww/vdlE!>TtfiC4p7?pGRp+-m3M'fiPRh)()fiQ04 lA Gff%fifi 4Ph)()fi$fiffElE*M+ pffK'fip_ pLU$'1B)a B;zyh [E,: K]1^a]~lEX!fi03M4fi Y4v /_ 7@ ffp3 0fi(,ff%fi44:Bfi3@ ffm'fiP*J! !*tK''v pMI!:;'C4Q 'fivfi03fi !Pffc fi?p7+9fi0*JL4>Whiv pv 8fi87304fi ElQfi R?0Lfi13L>Yv p4O5'P 8NfiP03Mfi !g{ Ffi v1p3 A'fiN@ ( 6'fiL 3'fiP'fiP'F640L j '!^k4LUfiR;m?84N'33 Gh)(i4 \lQ()fi7 Zr -!U zhB 8lE*O4(- Wfi (A4F'p3 L()fiTU''v 3Pv Y?p84 p3fi & hB,lLh)(i4 >()fiffiO8'v pPv & hB,l'lE!$T$p? Ififi=Ih 7lQff >''Ev p(q2~$_ ffhiMfi fffi>4'()lffiR;m*g! !*t?3PfiRp?Mfi40Ihizfi ffcMfiN?Mfi2al5()fi66!7()fi@ Ppk_ 0v pC+F?NQOff Q()fiNfiRff['fiKR7'Ep3 Yhi! !*?Mfi4l9(bfi['' pM*M+ >_0'$3 k_ pL+F?0F$Off F(bfi[P()fi834I?[4$-fi1fi ff \ o/@ fffi'fi R>'p3:;'! K 4 B; .1J)R4f'p3 Kh)()4@ \lFQ:;'Pzp*! !'* z5 K)q h z9)%lf(-Gfi S4;( K) h )%lE*J! !n* L) yRhB ylfff30 J)W4N 9fi1fi ff o/@ fffi+F?Mfi20 ffffi U?pIO-fi1fi ff C3 ffSYr -*z,4[ m'fi fi(,?E/!8rMfio/a *Vrt:;fi4 ffhirt:;fi4 ff.nrt:;pff2 lF(bfiL:;rt:;fi4 ffL:;()fi7304Whirt:;fi4 ff=rJ:;3ff4 lE!SY34'fiU4.W%fi3M>G:;' (bfi834Yff pY'p3 fi(i4 N()fiF Y'fiK*vLP:;745 S'fiK!:;'S(bfi834')4I'p3 UfiP()2 Yv &0434P:;'fffi(NR'' pM! D<fiRp4kp_ 3ffv P9fiM( )J*/ C4<'33 ffi-()2 Ffi(J @ Q''v pM!Offc P0434ff* Z fiN()fi$?p>''Ev pO6!{ L()fi/35fi Kp+9fiP0fiR;C4 ff5?0F@ Lfi p?Mfi Lfi'F(b@ ff1p30fi33 ffYv?p `_afi 4E3M@ 8+c E40ZW" ff'%fi Pfiff! +c 4C ZW" ff'%fifiRvff$@ 72. ff 'fiR>3@ ()3V(bfiQfi ff!FrMfiQ?p7@ >fi(<v p >fi hB(1fiNH=I,M lE*0ffc /:4AfiRvffgo1 ffO?pA@ ff13@ ffOO?<Q0234Ofi p <Rff3ffQ! P" fffi 0 fiRLffN 4'fiY3@ ()3O()fi>Yv p fi !T$p?RRP3@ ffm'fiU ()U?070fi(Q?pSfi ]^Cfi +F44-fi/3Mv X?3Ufi@ ffPfiEpGhi! !*A j @ fffi 0 ffkR+Aff/P()fi`:fi\+Ff j ' EklF Y?p804 !5ffc S?p8fi o/[fi(97340 Lfi PhB( fffi fiL'2fffi3 lffc 4IfiffWo/@ fff?33ffS()fiN02 fft832fi Nfi1fiv 4fi !Q+c G0234ff*?po/@ ff,?983a 5fi A?Mfi304OMfi94730pfi34>R()fi 'fiO[fi ba4pL ,fi(sda%^a~\ $^BpBB^ ^AJ^5\5~B'AEOBJl4B^5;~;daB\~ABBWm5J,Ed;EJBB,VBE\6B\-B^8^<$ff8E E<R\E5\^LBB5tE%\NmG%m ~^B<B~ff^'7\ 7BB^yo ~B9EE~ OE^5Bt'EdB\B'N^>7So Bg`\B^%Tff;VUJBBB~B9EB^4BEfV~EN5B'F B#B<tEaB^:WaAX,;ZYdg^NE~;/\<$'fV^NB5B^<dOB^~^N',B\BBENE\[EJ\BEB^~V5EVpEE=MLfimknpokmkqfiff!f"Lfffi0>fiffL o/@fff?ppffS(bfiQff13p2t834fiQfifi0vfi!$rfio/@ ff$?pN@ ff13@ ffOA?$fipLfi ]^[fi Y83'5()fi4fid+ @ ff'%fi'fia434 j 'E v pk4fi Yfi(O Mfi?pffi !@ *M+ ffi 4@ ff 5vM()fiP03k_ fi 0<fi(V?pff@ [0fiRff ??pf()fiP%p_ fi 9@v FR0`om[! TZffc 4fiRpR^] )h j ffc 4 Mfi8)klL4>'p3 Cfi($S'v p_( )4 j p EkR'p3 *A! !*5`( )4Mfi'p3 Yv # X:;'Sfi(Q?pS''v pM!#a][bh ) cedl48SW" ff'%fi CfiRp*6+Fp? @ (cff j 304!^k{ K4.)X?3 j 'v km fdU?p" ff'%fi S(bfi834Z'ff ? ' 4344hiv qk_ S4O\lj @ ff'%fi @ !^k W()fi4fid+ ffK1CP@ ff'%fi !TtfiW4423'' ?pff UfiR;;1Rff*A+fi v1p3 Y?3Yfid E q4 poM0 !T$p?fiRp"Dfv (bfi fffi n1*9+F?04?&'ff?4P?Mfi34X+Aff/PRK?pSY?PcIMfi ffMfiF3ff4fF?p7>4ON?0fs4$' 0P'v3M*45()fiP4fo/@ ff ffSF U+c E4fiRpDFv8pk_ pffYffDFv8g] h h)cp:;pff4 B&sV:B' P4El'lER! D<fiRpD9P(bfi fffi m1*+F?2?m'ffL?0L(9rt:;pff44 L?Qfi13@ ffm?p 344Ss#+f44ff3MsV:B@ ffff *g4NoM0 Lfi(<L" ff'%fi @ NfiR;!5T$?04$4)o/@ ff ffUv Yff%fiJfi 2LD-h]hirt:;pff2ic s:B@ ffff \lE!H o1-fi 43<?pFr @ @ ff@ fi (bfi9fiRffff_! [<+F42RL o/04 pffv fffi >1 ! >fiX `_fi *,+f?I+-K@ ff2 pffX'fi o/@ ffI(bfi3'fiP:B?pfi@ 4U `_a4fi &4I?ppfi Sfi(6?p7fiR;*! !* ! 'v 3Fv K?374 p30fi 7fi(<r 14fi4>fi;K!cff>?2J0Rff*+ -3O-? 4_o/@ ff ff83 p$?p-%fi034gk30j?l6:;3M'fiP'fiYhi*3jE?*vffw ClE!6{ Apff4pff>'fi[3@ ,?p-k30j?1r XRff3 ,fi 3,fi(M?pA4a ff'V7fi'jff Ofi/pff3? ff./v pI fi?,v ?p[23M@ f3Off9?49;1RFfi(Jr ()fi-?pf0fiR;*/ P+ Q3?N fi4? hi 3M0 ff4fi q>1 ! > 0GC1!4vdlEI! k3j?0g3'fiP'fiR4[pk_ pffS'fiKR7C()fi3M':3M0v Ch ChB<l hB<l ahB,l :m hB<l'lE*O+Fp? hB<.l hB<l>47U 8fi( j 0kS'ff!UTtfikp_ 3L?p>4 33fi >fi(6*3jE?J3'fiP'fi*+ 7@ ff30@ N?pL()fi4fi\+Fv p0@ ff44P p_ fi !rMfiIY$3 #fi($r -:* n,Ih glL hB<loMFpG()fi7v /_ ffGP qM>v R33 !Kffcfi?p5+-fir* n,Ih tl ff134-?pN 5fi(g40 4ff5fi(OZ?0-fi/3M$v Mk_ ffIfi()Kv C?pQ33 '6!T$p? C()fi$Ik30j?3M'fi'fiS-* & hB<lX 9h sRl lm?057E33 'Sh 03 lAv KZ+F?0 GahB<l$n,Ih t.l u hB<l a!5ffc Yfi?p[+9fiE*?p8k3j?t3M'fiP'fim?f0Efi ?7@ff30@ ffN/4pK'fiOaW' v /k_ 0ffYfi()*6L+ ff4,>Rv 3v 3 Rv 44%'!\ oM0 7pPv 04'4Qk3j?0Vr (bfi DFv*a+Fp? @ Nffc 4 0LfiRpDfv7p] h)cp:ff44pXs:B' 0PElE*4Fv KrO 3M@ >xKh)fi U?p> ()El9+F? h Dfvdl<!9HffiL?F14vp'v h DFvdl5 /k_ ff4fi()S204 ffFk30j?g*V SRff3 7?p8r o/@ ff ff?p)pfi Ifi(?pAfiRp*12vp[ j 0kQ'Fv h DFvdltv Mk_ ffQfi()P4g33 pff0 !rMfirgv 3M@ PxC+ PW ?LG''Ev pC?Lv430pffh)c;:;pff2ns:B' PElF+F44t/4'SRv /k_ 4fffi(bV*- h DFvdlP!qT$?13 ''Ev pm?'v &'ZvYv23pffNh)c;:;3ff4 qXsV:B' El94$v & h DFvdl5 ?p@ ()fi@ L/fi4ffFfiR;DFv!H o1fi 4p7W" ff'%fi fiRff>fi($?pC(bfiv]Qbh ) cedlE!GrMfiI?4Lad*g?3fi 4;1RFfi(Jr ?0,+pff()fi9 4(b/v pLW" ff'%fi F0fiRff<4,?p[ P4a $pP 4'43j?06r ()fiN?ppfi Rfi(- j rO~:;Lk" ff'%fi ffk 0fiR;! 2 hiPv 24cpffpffm()fifi3MeC-v /@ ff 4fi ! 30 fffi C1!4v!l rO~:;L" ff'%fi 50fiR;Ip? ff.1oM0*M?prdURi;E^`}m;I\\B^ AB5ffONmVOB<^<^6\E<^\ABf~\iB,f\BpB5\^iBNYVw ;EdB$BVBBo B$ff-p9gi;B~9 EAB\$~B^EVB$`\^'t F\BE)xJzy{}|Z~FQE\~ty%EB\BE~59^'B^;=Kxfi[RR{ff4v)RRt}-m{t/s$qos0pt{al qMc;:;3ff4XsV:B'P4{Rv>RRv*sV:B@ffffff4@v)Rrt:;pff44ff4@rO3M@>x1Fc+E4Pfi; DFvYhi()ElN0?prO~:;"Lff'%fiEfifi(AfiRph) ?ElA$k3j?0Vr fff*+F?p@ hB<l< a7()fi$%fi?K3M'fiP/!D9+F?p?3t?pN'YBff]%'vH)v>''v3F4V()fi4fi\+-ff71LF@fffi0`d1!grO3@-x7h)fi7?3,?El?Mfi\+F5Ik3j?r =(bfi5?3QrOE'~:;L" ff'%fi @ ffi;fi@ ff'%fi v 3>'fi D-1*1+fp? @ QfiRpD9&]hirt:;pff2csV:B@ ffff \lE!rfiY?4Cr >* h D-lC! T''Ev pW+f?Mfiv3L33 I/4,'[N /k_ ff4Nfi(b+F44/v23pA?p5_0E'O' 9 0Mfi<?pF@ ff'%fi?>?Mfi34m()fi4fi\+![>23 ff 3M0 ff4fi C1!x1*t E(b/v pYrOE'~:;L" ff'%fi @ fiRffYv 'fiQ43P'0ffLhiv430v p>4%fi(tfi3M)o/E4OEl6RYff13 A'fi 4(b/v p8?p(i34JW" ff'%fi 7fiR;]Qbh )8cdlE![W0(bfi?V*+Fp?+ 3 7?p7E j W" ff'%fi ffk?4[430OffK'fiv 043p[%fi?K?pN(i34%L" ff'%fi @ L K?pLrO~:;L" ff'%fi N Efi !bp@8*['^" ekg79gfi[F]{i[e^ehHffid+?8+-C?ff fi3M7@@fffi7()fi702>ZfiRvff*t84L%fi0'fimpff4fi/pff1p? ff.1v 3M*! !*(bfig04 U >0fiR;Lpv v pF+Fp? ?p<# !OrO'*?Mfi\+ d*+pff'fiKRv G+F?mp+9\fi ff 26pk_ 4fi [fi(9ff4044paNff0444;Yfi(-fi p()fi Mfi?pd*0ff0444;fi(O Y'fi (bfi o%!^j[[ek^ :: 47 ~ 0 (OKfi4 4(t?p o/2'Fa?Y()fi0 'fi7 !Z^j[ [ek^E A'fi ^ l( 9hy>lQ2IE o@ 0 (,0mfiU(,?p@O oM4'N0?K()fih 0 'fi7 ^ l( h (K lE!Z[ffa44pL(bfiv41'ff<26''fiLfi13ff1E?pff./vpM!<TF?p9ff'fiP4O?pA()fi4fi\+FvpM!"Lff2()fi fffi>1!N?9fiRp846'3p8h)()4@\lg(bfi-r -*%hi!!*/# hB7l'lE*(L fi #4(L2'33S()fi''EvpZ#?pm4p3fi & hB<lSh)()4@S()fiK'fiOm''vpvk_ fi V* R''v 3Yv ?pC4 p3fi ?0>vpS33 !TFp? @ ()fi@ *g4>4& hB<l'lE!K9R3fifpffff'fi 4(bC?pNfiRp()fi$''v 3A?$?ff 7vpE33 !6-Cp_ fi *vpI$3 Rv 0,+F4?C v 04'!,T$p? (bfi@* 'Lv KpI334Lff0 (bfi Wv 04O'*6R'fi hi:;'\l>v RS''v 3Kfi(-?34 p3fi C4ff40 [(bfi v 2'!-Av ff*1?pffi 0P'ff5 C'fiPA?)pff'fiI[v fi ffv_afi Y@ >?Mfi >ff0 N()fi v 04'ff!ffc 40>fiRvff$G>@ : o10@ ff ffGv KFfi(<ff0444;!5ffc 47fiRp] )Rfi34GR7@ ffffRNff/v p ?N?3@ Mzfi ffcfi oM4'NS'fi ^a*+Fp? .^G )J*%?4fff0v L(bfi U 4'!Fcp[4$83?Gfi@ 8`e34-'fi o10@ ff[L" ff'%fi >fiRff=dfimknpokmkq3Z30v pff4044p!qTH ?pffffff*Aff042;Z04ff/P.XfivUvX()/vp4fiRvff*M$+F420RLS?Mfi!T$3? @ U@ 137fi(N+-d1C'fiWR()fi Pfi1pfff?pff.1pM*$03M?p@Y+U()fi130fiq;+-fiM!T$p? f_09O?Mfi/C49ff_a4I4fi ff()fiAfi p[2-fi(VfiRffff ??pQ fffi 4-3/eC4p()fi53 QvE(b/v p8P4 ff-fi(tfiRff!OT$p? [fi 0 f()fi5?Mfi1fiv 388'Rff`_ac3 fi4?4O?<?04O4fid+f6(bfi9Nfi04fi I'fi>pP pA?p$fi03M4fie0v pffC1fiRp:;ff_a$44fiv phi@ 3M0 ff4fi C1!xlE!<ffc C?4- fffi *M+? ?M:; fff.p? ffUfi(L?pff m;+-fi#fi/pffQp? ff.1 p& fi?!T$p? G()32f fiE?PC@ vfffi C1!T$3? O_0' fi4?45 [20 6ev J?Mfi1L4fi@ ffQ()fiVffc 4,fiRff] )J!WrMfi49' *9?4IO?Mfi/Rv X/4I X'fi ^ff40 K()fiiMp!&c([?4'fiu?0Mfi ffMZR&Ep? ff. ff*$4Pp? ff.1'fiW Y+fp? ?p^\)J!GcR( ^)J*6?pZ?4848fi 43 @ ffZS `_fi ()423M@ !Uc(5?p@ @ MfiG()243M@ ff*`_fi Y3ff0!T$3? fffi XO?fi1*<3'fiP:B?pfi@ 4R'h 6T[l>Pfi1pff-3? ff./v pM*,4> W%fi0304>v Z?p`_fi 443M@ >h ! M*/ R:<E {fiRff*0vffw QCl9 Pf3 ff'fi7 ().^@UW8fiM:p o/@ ff40 >Nk_ :;I3'fiP'fi!Lc[2f3@ ffG3? @ 7()fiNrO'~:;W" ff'%fi >0fiRff!cff6Tfi/pff0Ep? ff./v pM*/'./v p>+fp? ?pA 4ff13 <'fi8.1v 3>+Fp? ?3 & hB<Zl & hi7l()fiLfiRpS!T$?2[2Yff34 Q'fi & hB<Zl u & hi7lN h)+F3? @ & hi7lNpMfiffN?pPfiP0 :fi( & hi7l'lE*-+F?4E?X4 fi?0P44ffff#1R_08./v p?3K'fifi/3Ifi([?p04 Sr # S?p8rfi@ ff'%fi pI'fi hi! !*V 7lE!$T$p? 7rfi@ ff'%fi p'fiL4 p3fi Cv fffi !YTFp? fiE?& hi8lE!YT$p? 0'fi70fi103>4a ffOl mhB 8lE!LT$?4?pmpv pff[+Fp? ?p & hB 7Pl 1*+F?4E?S204 ff & hB,.l u & hi7v fi 4,4P0 ffOpffA>Ep? ff.(bfiA/ffAv P?pffi/3,r ?A@ Q:ff40 5()fi'fifv 44M'Q P?-4'() fi?39fi 4fi <v ?pFr0Efi !6L" ff4a?A7/N497 ff1p3Qfi(V 4ffNh 03 l<3?C?N0!/45ff0 L()fi Uv 4%'7(tfi 3Nfi(O45 4fff45ffa N(bfiz?p>v 04%'!/N?545ff40 [(bfiz K 4'L ?$4~$_ ff-?pLr>4fi4a4 ffAOMfi pff0;K4 p3fi !9T$?454ARff3@ >P'' p45v K?pL2 p3fi Lfi(6 Yr (O$4/k_ :;v p?P ff1p3ffi(%4fi 94'()1v 3N?pfr0QEfi *+F?4E?P+Aff/v23pff$?37@ ff13 ffOf?NQvpC33 G83'[ Sv G mv 2V'! [4t /k_Rff?ff/fi)34053MUv KP/NRff3 >?pLr ?$8k_ 437R5fi(gff!T$3? @ (bfi *,'fiRRKv &?0?pY4 p3fi Y2OMfi pffp*-4I2Opffff'fiWpEPv p+Fp? ?3fff0v >1v 74'$_ ff$?p8r4fi !-TFp? >Efi Kfi(<v ':ff'I4L?3C*3jE?,4fi *O()fi7?p(bfi2fid+F pY@ ff'fi V!Uc7473Offp? @ ?8?pf3fifi(A?pPfi;&h 7lQ4o1 ff ffW7Yk3j?0<3M'fiP'fi!KT$?04N402 ff[?L?pr R:v pK ffEp? ff*O! !*g P*J4L4fiYKk30j?<3'fiP'fi*gff30 .1 pK?pPfiLfi/3@ ff@ ffF?04F4fi !$T$3? Nk_p? ff.Yfi(<?4f fi?0 4F+Fp? ?p[ff0 818v2~$_ ffN?pk30j?<0CEfi *Jff30 v m?7?p4 p3fi 2MfiffpI! 0fi103[4Qv hB 7l$+Fp? p L4[?QKfi%fi pQ'v h 7lE*! M!*[hBFQsJst|-AT5c~HN8*t"f|-A|,,c :[cHN8*O"F|AA|<1c :fc~HN8*Ol748v hB D-lN()fiIfiR;D9Rff3 Y8()fi3M?&fi%fi pP4'UGfi( h D9lE! [fiv p'fi?3Yk30j?50Efi *</4pG hB 7l8v /k_ 0ffmfi(bhi3P pqW4ff0 ()fiq 4A'\l402 ff & hB 78l 1!T$?2I+F445?0&4J( 40Pfi(N qff40/!$ffc U?Q*Jj`_fi Y(i44!fQ?p+F4@ *a:( MfiCff0v >fi/3f'K=fi[t}-m{t/s$qos0pt{al qMhB 7lQ4>12ffRvM_kffUfi(bhi!!*6>4Mfi7vWU/\lE*O?p & hB 7lL?p@()fi@ & hB<l & hi7lE*,B!!*9z 0W_afiX3ffff!a @ff44ff efi4?(bfi6T`_fiK()fiz?p>23M@N4A0@ffffUv ff4fiC1!bAZefhj]G]$^]\_['^ ]3gi^j['^802IS8fiRp>*fi/pff/?pff.1pNpPv3ffJ+F?3?p,# !OHW o/O+-5fi4pQ?pIIfi(,ff0vpM*0+f?4?S4[E?pI'fiU-!fT$?04F30fffiGM@ffffF?3843p>fi(9?Mfid+ff0v pIfiR'fiFURfffP04 U'fi373+04 U !{ KRv ZW@ ff vpmGo1fi Mfi7fi(Fr ff7 v pYfiR'fiE!cp4I44. ff?ff0v p7?Mfi1 ()fi5fi0 L3Pv 2'4$r [A+F440RQfiPfi@ fffi(tfi pQfi$fi@ Qfi(J?3fffiR'fi!PHffi?v 3Yfi3[fi3>fiE?@ ff13@ ffYfi23Mfi ff 0v pCL@ ?g?Mfi\+ 7'fiP. S?pU43fi Xfi*5?44?3Y(bfiE fi(N ff v 3G?04P3Off#p? !qffc &?pfi o1Qfi( fi43fi Uv fi?Pff*0?pIr ff pPfiR'fiQ@ 8R3M04fi *030?m83Mfi *a04 ffC'fiP?3Lr f!0l%2e\l % Bl% \% %!M %6 4 J4%I 44 4lO042rO3M@C19T$?pNfi3M4pQfi(gfi43MfiCfiE?K!{ 3OI?Lff7vpfi13MENvG;+-fiY0?fffff?pfi3FCvpImfi4v370?0ffhirO:3M@7vdlE!<D[3vpN?pFfi3Fvp50?0*ffE?Cfi9,+f?PN0MfiPIv24}ff8%fi034fiIfi(2fr a4 !OT$?4,%fi034fi 4fi ffK30v pL?3Wfi43M4fi P fi4?fi34v pffvZrgv 3M@ C1!GT$p? P Wfi1fiZfi(5?04> fiE?fi 04'7fi($ ff ffv3U0@ >a4 L()fi?p%fi0344fi *0/v p R3Mafi GfiE'fiN'fiY?p0@ N'fiYfi/3Ifi' pM* 23~:v p ?p8_0@3ffQfi(9?38fi'Ev pM*m?pm@ 3 v pC?pfi' p'fiK?38%fi0304fi G(,?p@ 3/eY_a!F()>?04cfi43M4fi *V `_fi R 0>@ Mfi p'fiY?pff@ v 24pffU04ff!A5?pL5fi(J?pNfi 04v pf0? p* ffE?Yfi fff ff5fi p j Rff'kYhifiEv pI'fiP j _0@3ff(i33fi klL04 Z()fi47%fi0304fi R()fiff3fi !RT$p? fi @ ?pZ$_ ff4pff Z04ff3Mfi 84Jv ffd ff>+f?L ff 0v p>hi4fi alE*@ `_fi * L02 L@ 04J_3ffpff!T$p? 03M%fi fi($v ff v pS3Mv 3K?pfi 2v p0?@ 4>'fiUk_ 3 :B33p?304 Z W084L'fi.X0+f?W344G?0(bp /fi 0O*Ov 0Mfi4G@ ff`:B+-fi414fi@ )Mfi6'4!6T$3? Xfi43M4fi > fiE?fi(arO 3M ZC[4O4'fiQ3 ffI3MEv p$?2J0?0 *03Mg?p30fi C4-7%fi0304fi } Qfi(Vfi pN v@ ff% ff 0v pChiB! !*1fi 3Qv ff v p7fiR'fiKKfimknpokmkq02ffC$r R)pfilE!,T$?452A4()fiF43fiAv +F?04? ?pc/fiO?0 pffL034*M?3$?K044!rMfiEP4*,mPE?v pK ff7 v pSfiE'fiPVW9?3ffP&h)fi/38fiPv/430blr 'fi7%fi'~:; ff pNr = :! P00v pQR;+p+9fiI3M'fiP 0C 4,pk_ pffP-Pav pR;+G?pffWffv ffOIhiAff ff*6vffw Q>lEY! Af?37? 3? ff'F ff*a+ 8m3M/4p[?pff 0v pIfiR'fi$fi0v pP'fiP?3cff ffO$fi(O?pLr ?$?pKffp47fi(FfiE'fi70*6pffff*6fifi\ffffpC'4fiWfi4fi!YcffZfi?pc+-fiff* VI hB<l; hB lE!Mfi?p[45fi(gfiR'fif*pffvff*Mfiffi\ffLffpffff*a!!*VIhB<l;hB lE!TF?pm?n4 fi(7fiE'fiYCfiYpffvff 4ff*NfipX+F4??3ffffpff*N!!*VIa hB<;l hB lA 0V8 hB<2l hB lE!TF?p(bfi3?4Kfi(8fiR'fiYE?pffU?pR-fififf=&3@ffvn?p'fifi 0fi ff*/B! !v* V8 !@ *+ PMffi fiQ3 k_ p8fiE'fiQ?NGfiLpff 'ff!7+c Gfi?pN+-fiff*+Mfi MfiMff[?pI?G4Qfi(9fiE'fi!>T$?37@fffi4[?N+F?S?pI;1R8fi(-r fN30ffm?p@*pfi7pff vpKK'Mzfi ffMfi*Jv @ff(p*VRffLfiff!8cpL4Q+f?Q+-fiC+f?m?pffpff'fia<\(bfi R'G &?pff'E fi qfi fi 07?0#+Fp? ?pfiRp4'p3 Ufi (i4 S(bfi R02 !T$?44PRff3@ SfiffP@ G'p3 Sfi(i4 S(bfifiM:;'ffhi'fiPElfE?pf?0 S()fiNr 'ff!Lr03M?pPfi@ *%?4f0R[Mfi ffYMfiQM@ ff[?0 pffQ'fi?p89fi1fi ff E/*a+F?04?S4$?p7(bfi3?S2Ffi(6fiR'fiE!FT$?2F4ffi(6fiR'fi*+F?2?v23pffA0''4fi *0450M@ ff ffK U[fiMfi h~vffww QlE!T$3? @ (bfi <+ -@ ,()fi/3v p5fi >?p<_a'V7 fffi 84 ffJfi(MfiE'fi!J{ Apk_ 3<fiR'fiEp? ffP*1?p-? PfiR'fiE! P? pF ff 0v pNfiR'fiAp? ff7a4 ffO'fiI33 'Rff`$_ ffh)40 \l 4ffW* ffpff*f 0q' 0fifi fi 0!{n3? v 0'2ffq+F4?q04344ffff* ff3ff*M 'E fifi 4fi *4,RfffiOffA>PE?v pQ ff 0v pLfiR'fiff!6ffc PfiEp'fiRfffi2&ff43Y*<?p fiR'fiEp? ffPpk_ 0fi 8fi 438fi 0W?p @ ffv 0Ifi([?pr 7z* ! M!*/?Mfi [09?5@ ff!6T$p? f4- 20444O34fi P?A433 'Rff`$_ ff0Ffi(O?p>r @ ffPv Y?p>O8(bffiR'fi[0044fi V!,T$p? @ L2F4'fi Y3Pfi?A?pQ ff p3@ ff,?0A20fiR'fi-. K?pQ3M'fiP'fiUfi0 Q pP 4'4!T$3? 7fiR'fiQmR7@ v U?37o/fi Mfi7Xh)0fi al5fi(9rO 3M@ P!Q{ Ppk_ pIff?fi(?pQfi@ ff'%fi 0v pLfiR'fi5Ep? ffPA-(bfi44fid+F*1Rv 3 pN+F??p[Pfi'-3fi p*M4 ffV\ fiKH=I *+F?4E?K? 3ffL ffp 7' 4fi Cfi 0fi ffEQg6b)*[ffUJ~B a8ffzV\ fiKHOIG;g]{e$g(o\ ]fe0 - ff%X eJ/g,F ( aV\ fiKHOI hB<l hB l Q/Eiff1 dM _P h ( . ln5h (J . l hB<l$0 h (J * l Ah (K * lZ hB<l eJM9V\ fiKHOI h h (K . l'lB ; 0%V\ fiJH=I h h ( * l'l h ( * lZ! B ;p )/h ( . lZg< ~ V\ fiKHOI # E $ }g< ;p )M ( AB'P0bi ~ff1h (J . l0m)M E0mBm )/ abiBff1 h (J * l Q ;ffE0b # V\ fiJH=I #5 # aI )/ 8} g< ;pKOfi[Zh~vdl^@Uo5'hlff~:;fiV fi( *V\ I1I, " HOIV _( *os0pt{at}-m{t/s$ql qMV\ fiJH=Iff':;fiV\ I," fiV ffI " fi'h~vdlVI'XhlAfip'Zh~vdlV I,ffIV\ I1I,V KI'XhlV fi 4 3HOIV fi 4V H=I,V KI " HOIrO3M@7,Tgo/fiMfi>Rh)0fial9fi(6ff0vp8fiE'fi![4Vfi(O?p7@ffPv0vpfiR'fi[@fff'fi pffERLvKPffi(<@Ffi(6()fi3MFE4PfiR'fi!RTF?p@()fi@*6+- p o/p _kp?pff ()fi3MI4P4ff*g+F?4E?Z@ fip :;&fiE'fi?-@ f'Rff40ff,fin( V\ fiJH=I 0Rff99?pF%fi''fi?,9vffffffAv?p[?E?8fi(rO 3M@ >!,TFp? [_0'A;+-fiPE4PffiR'fi$pffvhyV I,l-KRhyV fi 4l-ffpff!<{7p _kpV\ I1I, 'fip ffv ffp h (J . l-+F? ?3Nfi E'fifEp? ffP/; {]DE>e\4 - ff, Vr g6b)(fdZ 1K~P 0 4V\ I1I,( ag6b)GV I,hB<lP hB<lV h (J . l K 4;ZffUh (J . lK& ))1,aa4 ;,ffQ1 } ))m0))@ * M~BV\ I1I, E1`)/' 'EEi' Md) N V fiJH=I )16 # 5 ;p p 0Kbi00)))1A 7P aP # )1\V I1I, h h ( . l'l h ( . lq{8p _kpBV fi 4>'fi ff3h (J * l9+f? ?pLfiR'fi[?pff/G;g]{ e$g(o\ ] eH 0H - Jj NVr6g ))O[ff4GJ~9 0PffWV fi 4N/Xf ( a6g b)BV fi 4> hB<l hB,l h (3 * l $ 8fffh (J * l5 k M~ B;V fi 4E LE iff~ Md)J L \V fiJH=I )1,E # f ; Bff 0)/L))a0)))/ h (K * l N i>BKEa # )|V fiT$?3Pfi?p8;+-fiSE4PIfiR'fi8@C'Rff44}fffi#hyV ffI lLa34}fffinhyV H=I lE!G g] e$g(o\Rff444}fffi70p4}ff4fi@5fiR'fi6fiPfi0>()fi33Iv8?p$PE?vp5ff7vpQ4~:Q>lE!-+c ?pQfi o/5fi(J r 7*/ ff24}fffi fid+ -?pQ fffi(A 0432Q :B'fi:;' ' 4fi Gfi 0fi mv m?pa4gfip|L*J+Fp? @ ffc3`:}fffi C2 ff94*1Av CS4p? ff4B]^2::fi 0ffNhiS4p? ff4B*vffw3 QlE!9+c 04304ff*7' fifi 4fim'Rff444} ffq+F4?nXOY0np2} ff+F?n @ fi *$+F?2?4fi fi30>'fiGv pYSfi @ 33>'fiG'Rff44} P 0WU04 @ 33N'fi34} C7v WS4E?4'./h~vffw Q>lE!rMfiEP4*+ Lpk_ 3L'Rff44}fffi K 0 p44}fffi *M@ ff'Rffff4*$()fi4fid+f3M*!M!*@QG4E?4'./th~vffw; {]!Ke - a$*Vr 6g b)[ff4 J~R 0L4{V ffI /XeJ/ffX,gPEZ ( a9V ffI hB<l hB l gg /ff~V ffI h h (J . l'l5 h (J . lG g] e$g(o\K=fimknpokmkqMffBRV ffI E1`8E iff~K ffitE 8 V\ fiJH=I )/B# [ ;p 0C)/8)i0g}g<CE0)i V KI h h (J . l'l h (J . llqCV ffIV\ I,h (J . l| b08)/Og6 qV KI / 0 [ffG;g]{ e$g(o\ ]D>e - ELVrg6))L[ff 1Kff 04WV H=I, /XJe /8,g 8 ( aV H=I, hB,l; hB l gg/ff~qV H=I, h h (ff * l'l< h (J * lWP ; ~ B|V H=I, 1 E i' Md)5E U V fiJH=I )/;p B 0S)/}<g S))a-E0)) )1 h (J * l#V H=I,V fi 4 h (J * l E E I)/ff=6g 6m ' / 00 E# E i7 BEV H=I,HT o1*QvyRff7vpGfiR'fiP@Y3 _kpff(bfi?pffY()fi3M2Pff!Z-ff4fid+ \V fiKHOI v?pLfiR'fi[?E?Cfi(6rO3M7I@Lp+9fiC3M'@ffff!9T$?pNE?f3M'@Lfi04'5fi(Ofip :;fiR'fi*G?p()Q3M'@8fi4ffi(9;+-fi:;'RfiR'fiff![{P3 _kp>?38p+9fi fip :;fiR'fi @ 3'ORfffid+ V\ fiJH=I _0'$hiA+ @ 3'6p k_ pff8?pAE4P -fiR'fiORfffi\+?3fflE]f e\4 0 eJ E~B>E Ea # b ffb)MQ )M9b4V I1I, V ffI\G;g]{e$g(o\ ]eH 0 eJ E~BQ E # )P))/$ )/6E)Ib)EMffB V fi 4KRV HOI; {G g] e$g(o\)b7M~Bc94,@ ff -9?046%fiv<'fi8'fi/35p+9fiIfi@ffiR'fi9?-TMfi-v?p[?E?fi(7rO3@! TF?pq@Mfi v?p?E?qff30S?3@O@ff4qPvMfiC4fi(V\ I1I, IaffI N XV fi 4 3HOI ?3RMfiMfiLRfffi3Y''4GRfffid+fi3M>fip,fiR'fiV\ fiKH=I !,T$p? ff [fiR 'fi5@ Nv 'fi/3ffCp? @ [R ff3 Q?p [ 3 (i3a 4'fiIR ff3?p@ 3 ff'fiq@ ff RfiP0 pff fi(Ir [!+c fi?pU+9fi0*[4(>?pRr 4fi0v 4fi>'fiS00/v pK?pff fiR'fi8?pW4>+F42Ofi0 (bI041v 3K?pffK!" ff2a()fi fffi K7?ff?Kr 'L29fi14ffC+F4?I -fi(J4fi\+-a [4fi -?LPdZRC. #()fi?'!&T$p? ff fiR'fipff fiP& &fi &()fi?pK fi(4fi\+-a Lfi F(bfiz'G g] e$g(o\; {]f; {]5'QEa`ii(G g] e$g(o\'Ea`ii(e\4 %EM B \4; #bg< Uffi B;L ( #PE iiY # fB78 ['0t/4bff V 1II, aKIeH 0 %r4EMffBVfi 3HOIg<a 4Rd)&Ea i)=IF&)`~ B; ( ## E UBX ['I/4bTtfiq33 3'n+F?V\ I1I, I4vfifi 4' fi(8fi pRfiUfi R0444fi Cfi.( V\ I1I, IaKI *fi2p$?pL()fi4fi\+Fvp oM0!9c+Urgv3M@>1*a3ffvpPrt:;fi4ff[FS4fid+A0L4fiY()fir-]^<$QsJst|-AT$cHN#- ff3gv 8rJ:;3ff4ffv p$?pAfi 4L4fid+A0fi 8()fi?g!r3M?pfi@ *?4F@ ff34Fv U?pffpKhB$QstsJ|AAT5c~HQ8*$QstsJ|-5T5cHNLl-Rffv 3pff ffG?pOffpUhBFQsJst|-AT5c~HN8*D[|9sV1c :[|9"$c~HQ>lfff pCff24} ff!8T$p? I@ ff'fi pK4Q4P42f()fi+F?V fi 4 vfi 4Afi p>fiFfi@ >0044fi 09fi( V fi 3H=I, !T$3? [ ffPv v 3>fiR'fi*M+f?4?C@ L4fi(t?pQfiE'fiAfi ?pN ()A3M'@Qfi( V\ fiJH=I vrO 3M@ *Jfi 04'Nfi(9;+-fiU'0Q?pI_a'Q'fiK@ fffi\fi 4fi q (bfiGffpUh (J . lE*t m?pKfi[t}-m{t/s$qos0pt{aTj6T$|Fv)Tj6T$|9l qMTj6T$|>)Tj6T$|<rO3M@Q1,Sfid/vp'4fiCfi0fiARp+-ffpff!fffi R'fiU#h)?pO\lQfi 4fi qK'fi\ffpUh (J * lE!PT$p? I_a'N'Wfi 4'Nfi(A0/v pfip$04P4,fiR'fiff*1?p$fffi0'fi04'6fi(%041v3[Mfi?3<4-fiE'fiff!|< fi pNfi(g?pN()fi4fi\+Fv p8fiR'fiA@ ff@ ff$pv 4 Kfia pff5fi(g?pLr [!cfffi?p5+-fiff*1(?3Qr =493Pv 2'4$ Cfi0v [0fi,'fiIfiR'fi$a44fi ?p5+F24NpEPv 44Q Kfi0 7(b+Aff!e EMff B /Y i0)EgBY)1F V\ fiJH=Iff g6))]@efiff JaG4E) & pa0%g6))0K)q)/U ( 0bi n)/g<~ g, 1V _ h h (J . l'l h (K . l G^@oU 5V _ h h (J * l'l h (K * l.h (J . l h (J * lZ hB<l Je /ff~ ~ 9V ~ aff18B 0)/ff[4fi(0?p9ffPvv3$fiR'fiO@A'Rff4ffOfi( V !g{ARvL+F4?L?p-?t3'@fi(V _G;g]{ e$g(o\ ]vf@eH\4 " 0 Aa # \V I,BUdPh (J . lI0U)/Ea # V fi 4UBdIh (J * loM0fi(V I," fi 4*g3vpYrO3M@1Q *O2N'fiG3ffffph Tj6T$|Fv* Tj6T$|> lhi!!*P. h Tj6T$|fv* T_6T$|Z> lJ#ylL0>W3+ ffpQh T_6TF|Fv* Tj6T$|Fvdl+f?N'fifi4fi h Tj6T$|fv* Tj6T$|Fvdl$!G;g]{ e$g(o\ ] E>@e " 0 - # V ffI BIff1$h (J . l50>)/ E # V fi 4>B8ff1h ( * lrMfif oM0*53pRrO3M@1Q *5+Sfid j 0kW(bfi ffph T_6T$|fv* Tj6T$|9l8'fiXp+F4U@ffffffpYh Tj6T$|fv* Tj6T$|FvdlF'fiYP. h Tj6T$|fv* Tj6T$|9lQh Tj6T$|Fv* Tj6T$|fvdl7 V!UT$?04L48'Rff44}ff4fiRfi(5?pfi4fiRfi ffpmh T_6T$|fv*Tj6T$|9lA(bfi44fid+ffK 4fi fi(_ff3h Tj6T$|Fv* T_6T$|fvdlE!! A?4I%fiv*9P4P@ff'fi o/PpK?pHT o1fi4p?pY()3M'Kfi(qV _![4tfi(<?p8fiE'fifU?pI()f3'@@ff'fiG()fiQ?p7'a4Fv'fiC?p7p+9fiK3'@ffffi(2V _; {G g] e$g(o\4(bYfi 0fi ?84L4v ff?p jff042;mfi4fi!^kT$?04Nfi4fiR'ff7?4fi$'fiCvffvpGhiS2'fi()[vffvplE*(O ( 2Fff40>()fi 'fiO8v4J'?pW f* 43 ffW'fim4'fiYRff0 (bfi?8v46!YT$?pP@fffiW()fi?4Aafi +F24fffi>ffFv 3M0 ff4fi M!1*+Fp? @ N+ >?fid+=?0FI?pfi@ ff ?Mfi4A()fi?p[;+-fi:;'YfiR'fiA(V 0fi 4P(?pQff024;Pfi 0fi 4,'Ep3 !<T$p? f@ ff'fi C?A?pK$fimknpokmkq;+-fi fiR'fiQvS?p7E?[3M'7fi(2V ()2J'fi 4'()Y?pIff4044pKfifiG4[3p'fiP?pff4$?ff/vpPV fi >$?3ff$fffiYV!,T$?pL3 _kfiCfi(V fi 4>'ffF?0 h (J * l$y4fi<'fi7fiR'fiA024fi * 0P?p@ ()fi@ f+ Q?ff YMfi 30Qfi( * ]^9ff042;*z? ( 2-ff0 [()fi Kv 4a'!,T$3? f()fi4fi\+Fv p8@ Q?pNpk_ 4fi <fi(t?pQfiE'fi()fi$+F?4E? ?p>ff0444;fi 0fi K4A'Ep3; {]G g] e$g(o\dIhJ(*l@eH\4 " H-a# V ,IBSdhJ . l0Y)/ffZEa # V HOI(BPQ1*V+-PWfidP?3Pfifi j 0k ()fi ffpUh T_6T$|fv*Tj6T$|9l9'fiffph Tj6T$|Fv* T_6TF|> l9'fiP.h Tj6T$|Fv* T_6T$|-l$ yP 'fiP.h T_6T$|fv* Tj6T$|>l,RmESV!6T$?04O46pfffiPfi(kffp8h T_6TF|Fv* T_6T$|-lt(bfi2fid+ff1pE4}fffiKfi(O?pN'EfiKfi0fi fiffph Tj6T$|Fv* T_6TF|> lE!G;g]{e$g(o\ ]lK@e " Aa # V KI B8ff1fh ( . l$a>)/ffCa # V H=I BIff1h (J * l[tI oM0*v7rgv3M@1Q *+-8fidA?p-4 @ 33j akF()fi ff3Qh Tj6T$|fv* T_6T$|-l'ficffp>h Tj6T$|fv* Tj6T$|> lt'fiNP. h T_6T$|fv* Tj6T$|9l6n9 h T_6T$|fv*Tj6T$|> lNV!7T$?4Q4QK'Rff444}fffiUfi(9?p'fiGfifiGfiffpUh T_6T$|fv*Tj6T$|9l%(bfi44fid+ffNQ?p2}fffiNfi(/?p6'0fiNfi0fiQficffp[h T_6TF|Fv* T_6TF|> lE!G;g]{ e$g(o\ ] >@eH Je / ( ))& )/ P )1N V 6g b)Za 44Ei & aEI ;4* * 6g ))Y ;4* ( ffff # gg /ff~ G)/E<g )MIEM~ BE\bKCE0)i ~ ff1Ih ( . lLBYff18h ( ( lHffi$?:ffE?fiR'fiF) B0)iCfi(?p5?pffQ(bfi;V 4fi +f44R5N'Rff4/5fi(fipfi(6?pL()fi4fi\+FvpMZ\V I," fi *zV ffI " fi 4*z\V I1I, " H=I *0fiqV ffI " H=I, !Acp[4$fi2p@ffV 4fi (6fiP4(fi?pQfffiC' fi(?p[fiE'fi-?3F'fifi0fi49PfidffC'fiffpIh (K ( lE!rMfi o/P0*O3vpYrO3@1Q *O+F?p+004ffmfiR'fiV ffI " fi 4UhivW?p oM0%fi\\l[L oM0*JGrO3@'fiSPfid?34 @ 330j akK(bfiGffpSh T_6T$|fv* T_6TF|9l['fi ffpGh Tj6T$|fv* Tj6T$|fvdl['fiP. h T_6TF|Fv* T_6T$|-l # h T_6T$|fv* T_6T$|fvdl V*<?2fi34RSfi 4p@ ffn '4fifi(V fi *FC+ff4fV ffI " fi 4!sJ.+F2U+F?3q+G004ffV\ I1I, " fi 4L'fi p ff ff3 Ch jT 6T$|Fv* Tj6T$|>l$YffpCh T_6T$|fv* T_6T$|fvdlA+F4? jk$?3N' 4fi Kfi fi V*1?04$fi34K2'fiRLfi 4pffK Uv 44fi fi(V fi4 ![R'fiV fi4 46 ff'Rff44>30 ()30/fiE'fiff!Ocp,. ff<?p$ ff'fi 0 $34fi I?+Fp? & qfi fiRfi p+A 'fiR' 4fi 'fiW Mfi?pZh ! M!*5 ffpU43ff ffalE*?pIfi @ 3'fff/Nv Uf3M@ Q'!fffc Yfi?3[+-fiff*0?pIfi 4fi Y()fif' 0fi p'fiMfi?pPY48' '( @ ffZ'fi?pffpK ffv 3U0.Z'fim?pK3M '!rMfiOo/P0 *3M0fi@ fi\ cLRfffiOff'3.ZI?pK4 p3MfiI@3}fi3I+F?r=()fi Xv p:PEfi1Zfi(F2O!Wcfi34 pYmff%fiW04 hi YrO 3M Kl7?08.acf G4[DQ|9sV1c :[|9"$c~HQ 'U3ff vpffpYhiDQ|,sJ,c :Q|,"FcHN8*V"F|AA|<1c :[cHNLlf P./v phQ,|Jc,Q:,|F"cNH8*[9|Vc1[:9|$"~cQH>lWvhi #DQ|9sV1c :[|9"$c~HQ+9fi32&?ff Y'fiWfffiUv 44%'\lE!" ff4<?7ff024;G4LU. m2pW3 ()fiL `_fi !Hffid+?>+?d Y@ >fi(AfiM:'fi>p? ffPff*t N3Nfi 4pN?Mfid+?pff fiR'fiNRff7ff0444;Y()fiv 44J'ffff!KEfi[t}-m{t/s$qos0pt{al qMA4E(b/v p?4[+F42JRI@ ff Q()fi>33 3' pP%fi?m?pP 4fiJ0fifi(iQ%fi3MNfiRp@ff@4fi*tR?pfifi()fi>?3v@ff6`_afiWfi?!T$?p@;+-fi()33 0O$+Aff/?Cfi3MC ff v 3fiE'fiff#Rffff042;`yV3[$^a`y`yhiM:@ /4ff j sJklE*9B! !*5& ff4v pR?3Uffa44pfi(N'fiPfi 'ff*5fia`yV3O^@`y`hi 14ff j NklE*,! !*<1ZEv pffa44pfi(['ffPfiP'fiP?Pfi32ZRC12ff; ?370Ffi(<?p8r fi/`$_ ffYK?pI ff 0v pPfiR'fiff!f+c S0234ff* UE? p8'fi?p8ff024;Cfi( ( H* .* * fi['fifv h (J . l5fi h (J * lE*0 ( @ 0ffSv Y?pLfiR':'fif3k_ fi V*/2$fi 43@ ffKfi/!-A? 3ff5'fiff4044pfi(O Cfi?3f'ffffiF'fiP[4fi 2p@ fffi0B![Ao/a ffi(t Ks#hifi1)l<E? pN'fiffa44p*13v p7rO 3@ qQ1*3%fi F?pQfi4fid fp+4fi j k?0f$U. !-cF0 j k'fiC$fi U@ R'fi@ *0F+ ff2V'fiY?p Lfi(54fi\+-0v Pfi N()fifi pfi(-?3P'ff7 Lr 7!V+c ma434d*V?pPfiff43pff<'fiI4fid+ j k7(bfi j6T$|Fvf 3ff4pff<'fiI04.V H=I, 'fi7?pf' 4fifi 0fi ()fih j6T$|Fv* j6T$| >lg'fi9fi 4fi jV!^kGTFp? 'fi j kL+- Mfi<@ /fi3>ff40()fi Cv 4a'*M03MA(V+ N3j6T$|fv[29ff0 [(bfi K 4a'N?pC?p024fi Cfi( V H=I, P3N?pN'fi j kff0 ;! d[v prO 3M BQ8'fiP2443''E[zh 4fi0bl?0 p'fiGff024;*g3M%fi+ 3ff ffpmh _6TF|Fv* j6T$| >lQ ?L_k3M@ ! T$p?j6T$|<M*0+F?4E?K+-F@ /fi34ff40 h)Rff3 L+ 73Oj6T$|fvL4$ff0 \lA2LMfifi p9ff0v !<c?p5fi?p-? *?pF()9? j6T$| >Q2MfiLfi p9ff0v $4<Lfi/?0 p!Hffid+ + Y@ K ffMZ'fiR3PP4} C+f?I?pY ff pmfiE'fi#Mfi'fiWff042;!rO'*6+ v 'fi130fi p fi@ Mfififi!RTFp? C17%fi4RRpMfi jvff ffk> jpff@ ff *^k8@ ffff4 ff*1 0XXp Mfi j3MfiAv@ ff ffk7 j/:Mfi>3ff@ ff *^kG@ ff'Rffff!C{ 30 ?pff C'17%fi4Q+F?z Zs<* ! M!*ff L?07ff0v pQfiR'fi9mh)03M<Mfi ffXfiXpffffE4lgv@ ff Wfi0ff0444;* X7sSff?FYfiE'fif3Mfifpff@ ff >4fi1Vff042;!T$3? N@ ff3A()fi$?p>4P4 ffiR'fiEF@ Lv 34 fffi114fi3V\ I1I, sjV ffI C sjV fi 4B sCV H=I, C sj?3K4P4fiR'fiEPfi\12pK'+%fi3MPE?pff&ff024;()fi4Afi(Q?p$fip :;'KfiR'fiE!,rMfi5?pQp+9fi:;'SfiR'fiLhi!!*{V _ K4afiE'fiARfffid+AvC?p?E?fi(OrO 3M@ 7lE*0+ 3ffK'fiPfi 43$?p9UV ] Rff!9rMfif?pN@ ff345v ?4A0Rff*/+fi pffG'fiK(bfi/3[fi mfi p4'fi Y?p@ 08v G?pOpY RffN(bfiN?fi IfiE'fi?64(b7?p-ff042;>fi 0fi hi! !*?p5 ()O3M'@Afiv( V lJ 3g?p)p ff6()fi?Mfi NfiR'fiEA?$MOfi Mfi$4(b?4Afi 4fi mhiB! !*M?pQ ?A3M'@\lE!,TFp? 3) ff$fi(?Mfi 5fiR'fiE6?,2'(b7?p$ff4044p>fi 4fi 4O?<ff0444;Ch 4fi0M fi/bl+F42pp 9RF@ ff ff*1! !*= Xs<!T$3? $@ ff'fi C4,9()fi4fid+f!69fi1fi./v p>9?pF@ ff3()fi9?pf4 AfiR'fi*/92,0 9?,?pF_0'9'C P?pff f;+-fi:;'KfiR'fi-pIv@ ff@ff4044p*tRff3@ ?p_0'7'2>+Aff/V I,fiV ffI !YT$p? @ ()fi@ *O'fi3 33'K?37v 3fiff?0v K?2$@ ff30$+IpffS'fi oMPv 3L?pI fffi G'V![-fi 04pK$LfimknpokmkqV\ I1I, " H=I,XV KI " HOI !KHffi?.V HOIfi ffMfi8v@ ff fi0,ff042;XOhLlE*g03MNvfffi16ff042;Wh!UsOlE!c"Us#pc RffL3pI'fiK?pp2}fffi'ff3 'fiPQ@ 7ff pP' ( ffU()fi fi 37fi3Mfi pff3>fi(,'fi7 ( 'fiC fi?pfi3Mfiv 3fff3fi_( ( +f?U?3ff ;+-fiKfiR'fi*1Upk_ 0fi S?pfi1ff0444;Kfi(,?fi'fiPF(bfi Uv 44a>+F42fi$RLv@ ff ffU$pL Rff!-+c fi?pf+9fiE*M?p>'fi@ Kff40 (bfi Xv 04,U($ fi Z8( ( 2*< ?pff Kp+9fiR ff7 v pSfiE'fifi MfiQv@ ff 8?pIffa44pKfi`( ( ![r3?pfi *1Up_ fi h (J * PlyCfif'fiff0v pM*M'fiP?pLff042;fi( * 4)fiFv@ ff@ ff!6{ >fi430pN?PsZ45p) Rff!X444V4v 3gfi(Mff'fi v p) o/04 %+F?[fiR'fiV fi +f44fi MfiJv0@ ff ,fi/ff042;![R'fiV fi3fi-v0@ ff W4fi00ff0444;* (9, ffp*/Rff3 f?pFfi 4ffp7?$?4$fiE'fiffi34UY4>h ( ( lE!9ffcfi23fi *M4V?M@ LfiR'fiF?F2'(b?p[ff024;Ifi 4fi P?d f3X Rff,fin( UWVM]6v@ ff pLffa44p Oh PX8sglE!MY?p8fi?pN? *aRff3 >fiR'fiEqV\ I1I, " fi 4I 8V ffI " fi 4>?d |V fi 4IF?3ff[@ fffi m'V*?pKYv 0@ ff Lff4044p!" ff35()fi 4fidW+ fv ?p7?E?Cfi(6rO 3M@ 7I >v p? EffK3MK?pL'!-rMfiTo/P0 *Rff3 V\ I1I, " fi 4CWv 0@ ff fia,ff4044p* V _Z>+ ff4! T$p? ()fi4fi\+Fv pY2>3P fi(6?p8@ ff f@ff3F+ 8?ff 'fi()Q%fi3Mf?fid+?p>p+9fi:;'Rv ff v pfiE'fi?0 pPffa44p!>TJfiKdfi4mfid +fp? ff4Pv 3?pI@ ff3ff*+ 0@ ff [fi0Y?Mfi I@ ff3pffff (bfiF$3 p'0v p8?4A0Rff!V fi4 *vV\ I," H=I, *vV ffI " H=I CV\ I1I, " fi 4*zV ffI " fi * V *zV fiJH=I #K()fi@fi430vpC?4QfffiV*V+-$0bfi4pNK@Q04fimfi(9?pPvffvpfiR'fi-??-@$$b ffffKv?pfo/fiMfi7fi(JrO3@Q!<T$?04,@,afi4-3ffff~:I()fi9330p'0vp[?pFL0fi1fi1fi(i,%fi3M6?pf@fffifi("Wff'%fi5fiRvffFhivfffi lE!9rMfiF?29afi */+ N+F4?'fi2'vp32??Mfi NfiR'fiE5?$Yv 'fi/3Qff'Afipcp+'' p7+f? v Mk_ ffI@ Rffv3830''v 3h ! M!*thi/*V*^*^* *^V* *^* *!!! l+fp? @?pOff4204F@ @ ff@ Nv Mk_ 7@ Rfi Gfi(-m()fi4fi\+ ffm1\l[ 'fi ?pPr 4 p3fi 3?Mfi f?,3fi!TIfiE'fi9?9'fiP,'fi7?p$' 0fi Pfi 0fi I()fi9 ffpv G/*mffpI'fi oM4'v3/*%fiQ0S ffpI'fi@ ffP3+/8Rfffi p'fi?pQ_0E'54>h)?pL25?$Y0K3?U3M0''Ev pElE!OT$?13A?4-_0E'545v23pff9fi3MfiR'fi5?0$@ ffp+/ff>h ! M!* V fi Rff3 N$Y0KOp+ ffph (J ( l'lE*0+ff4<fi3,fiR'fi,?9p4} F?p$' fi Pfi 0fi Pfi 3>'fiOLff3Ffi(>/h ! M!n* V\ I1I, " H=I,ff30 7[p4} h ( ( l'lE!fT$p? 7fiR'fi[@ 8012pff R;+?pff >;+-fi4@ ff$$()fi4fi\+Ffi 4*MV HOI J* V fi 4 3H=I, *3V fi 4 vfi *3V fi *MV I," H=I, *JV ffI " H=I, *MV\ I1I, " fi 4*MV ffI " fi *MV _ *V\ fiJH=I1!R\V I1I, *zV ffI * V I,ffI E* V\ I1I, Ivfic$4$4%fiA'fiMfi>?R^@` `gfi(g?pLp+9fi:;'GfiR'fif@Lv ?pN_a'$4ff!v!RVA-?4,fi<+f?0ffNp _kpff79fi(J3(i3MfiR'fiE[h)/4L?3ff,fiR'fiA?3ffPEl<?fipYfi3200W'fiR&r 04()fi4fi!Z{?X?pff@KfiE'fi*94I2Ifi0'fi40fid 5?pW ff3ff,fi(%N02*'fi7,6'fi7?A1fi30>33(bfi@ff@O o17K=xfi[t}-m{t/s$qos0pt{al qMR0<fi4fi!PTtfi\3M?p3(i3v3ffQfi(A?pffff7vp fiR'fi*g?pvffppffP'fiWp? ff.&?P?0MfipEffqR3@ ff ffI04 hiB! !* & hB<l lE! [?Mfi33 ? Mfiff ffPv I?0460Rff*+ F@ [3M@ 8pfffi0v 3c e9O?Mfi/6()fi,P./v p>?463? ff.3 pI?pN. fid+F ff3Lfi(t?3> ff pI?$+A$Mfi p!T$3? 90434g?Mfi4Afi( ff v 3$fiR'fiO@ ff@ ffIp? @ -+A6fiff71L()fi3MO()'fiE!rO'*?pff 8fiR'fi[' 48 'fifff':B'fi:;40 ffQR3Mafi 5fi( ' ffQv G0 *+F?2?4N?3@ @ ff fi Wfi(-r [L3 ffWv mfi3M>4P0 ffOfi #hi fffi ClE! fffi *?pff CfiR'fi8+ @ C '0 ffm?p 4E3M@ !UrMfiOoM0v j* 34}fffi 'Rff4`:}fffi fiR'fiA@ Nfi 2p@ ffP(i33 0Oa()fiA 3fv M( @8hiG4E?4'./*%vffw Q>lE*0ff pa<v 3Qr ffppff9@ L RffF()fiXfi/v pLr [FhirMfi ffB*%vffww ClE!9T$?V*?3ff $fiR':'fi$pN4@ Nv C?pLfi o1$fi(O024fi 9?$+@ Nfi 43@ ff!<rMfi3M?*M?p02348o1fi Mfi ffI@ ff ffXp? @ (i44C%fid+ ()30,?pfi 45 ff0E4<@ ff3()fi5@ ff3v 3>?pQ4ONfia o/4;fi(t@ `_fi *M5?Mfi\W+ Kv ?pQ@ ffPv 3-fi(t?4-0Rff!$G$fi R)#: ('2*),+13ff&%-+/.R0'- 21 3R 3433 afffi>1!p _pffE30()300ff0vpLfiR'fi5E?pffP-'fifi10(b3M'fi'fiff3ffyh VI hB<l; hB l'lAK?3L'4fiKfi4fi5fipOffpffIhyVI hB,l; hB l'lE!9T$?p@ff30<v ?04< fffi ff'042?I+F?4E?fi(?pff FfiE'fi9Ep? ffPZVN@ fN0fi3E ff'fi@ ff@ 7p+9fi fi;U4 ff[fi(,v @ ff'h)+c E4SW" ff'%fi \lE!QT$?4f fffi m30Off?P4A ff7 v pSfiE'fiP@ K02 ffW'fiRv p r 02 *<! !*A(1fiNH=I,J fi( Kfi !fffi x7M@ ff@ ff,?pf' 2fi Pfi(V?3FfiR'fiA02 ff'fiI7 p $02 v 'fi>?pff4XRfffi>fi/3602 Uh)(bfi5'2fffi lE*/ ?Mfi\+q?2<ff-?pF@ ff3ff!t{ [Rv II()fiP44kp_ 0v p8+F?$+>Off K1 j ( 7P? p> ff 0v p8fiE'fiff!^k}b 65M]87^ '['^Z]\_['^ z]$gi^j['^3M6fi @ fff46'fi>fi\+-9?p54O$fiP0 oM;8fi(%@`_a4fi!6T$?p54pfffi43Mfi4O'fi74p/:Q() > )a>4)LO?fi1Nh GsJlE*1+f?4? @NPE?vpQff0vp>fiR'fiEA?$@CfiE3ffm'fi @ff80fiRffIhi4'fiK4ff jfi@ff@pffN@ff1v3PavpklG@ ff13@ MfiC33 /:B4>@ _afi !LrMfiLC02 mGfiRpU*3M0fi@>`_fi?N30ffpff4fif'fiY ff7 v pM*! !'* g* & hB<l[4a4 ffBn hiB! !*g 7lE!8T$p?PE?v p5 ff 0v pFfiR'fi;VMhB<lg2t GsY(a 8fi >(0 `_afi I4j3E ff'fiL3ff()f ff 0v pM!O+c Yfi?p$+-fi*(<VMhB<lE*?pm# 204 ff$ !30 fffi M!Pfid/4pff$@ ff35%fi3M5?3>4fi%( pKfi(g?v 3> ff pIfiE'fi!fiO7fi(6?p> ff3fv 30 fffi KM!@ p ![WH ?3ff ff*%?Mfip3 ?U+ IMfi fi[?d&GfiE3 K()fi?pff K ff pUfiR'fiff* fffi Cm?Mfi\+F8?I+-KXR()fi@ _afi Yfi c e? K'fiV `_afi Y(bfi?V!Ma[I]v;@KKbFE:9* e$gs$309RvC1Cfi04pp>?pN04P4FfiR'fi!<TF?pQ@ff3-()fi$4%2PffiE'fi@fifi44E ffFfi(9;+-fi ()33 0OJ?pfi ffP*T$p? fi@ ff8v8 1*%+F?4E?Sff fifRI4PO:4ff4Wv 3 !Wrfio/P0 *,ff@ ff'fi 'fiR3'Rff8?P(F ffpY283ff ff'fiO+fp? @ fi pK?pI0?R(bfiC'v L'fiY @ ff'%fi *?3m?4Qfi34m3 (i443@ 7fi(A"Lfffi 0 NfiRp'fi?fi4 Rff3 N?pL ff'%fi L4)Mfifi pFff0 !6ffc K(i*0?452LMfiKEfimknpokmkq'3p!O{?,32?RO46?9pfffi<@ff35?pT37R<fi(''v36vI?pF433fi!c(a?p5fiEv/4p30fi$4~_$ff<?p5fiRp>?pPfiLMfizff,4O?p$P2<4p3fi!6T$?3fi@ffvN()fiP44} ffA?4!M@ -, Vrbi0Fff VmOeJ/ff & hB lZ & hB<l9* e$gg6))([ff4XJ~P - -P)ffi6B /g6))( aRV hB lE hB<l gg/' hB l hB<lgieevbT$?3-433fi-ff>R4@ff71L?p54fiLfi($p+ ffpffO?0O?0ff)p+FLvffpff'fiIfi 4fi !Oc7?3-fi?3,? *Rff3 ?v3N337@ffPvOp33R@ ffNfi)( pC+ ffpffffZ* & hB lQ4a4 ffJ& hB,lE*g W+ @ p 7@ ff3pK?p}Nfi(O?pL2 p3fi !,T$3? @ (bfi * & hB ql & hB<lEB! ]T$?pff3Q%fi3MN?p?v3Ivffvp fiR'fi>?pffPEV\4v ffI,Vfi 4(bfi2fid+7fifi`:]3g=< V I,?> - g6))~ MffAB # 'E #gieev b [3 P!8T$?3O* & hB<l[204ff8 !8DT _pPV\ I,hB<l[ !I9T$?pfiffuv* & hB lE & hB<lE!CTF?p@()fi@*W6* & hB lQ402ffJ !{Cfi433?*B!!*v\V I,hB<l$ P!8]IekgeTtfi8R[fi4'A+F?T$?pfi@ffv*v -fifi44>fi0Uh)03M?MfiAv?3f@ff-fi(J?pfa lE*+3 7 ()fi$?pN0@ : V fi 47r G(bfiF?pL%fi'~: V fi 47r >*B! !v* V fi 4hB l6=-!]3g=<KE V fi 4W& / f' Eigieev b [3OIekge0LaE Eb # @>- # #)0ff M)\i0EGa!CT$?386* & hB lQ204ff= P!C-T$?pfi@ff v* & hB lEP& hB<lE!T$?pq+-m3MfiSv?K P*$!!*5?V 4hB l !rMfiKv0'*fifi33@ oM0Y()fiffc 4K0fiR; ] )fi13ME(F+ U&#ff0 ff3K+F?'fiKfi4fi*)J! ]H fid+z+Kfi4pISfi,@ff34>()fi|Vf?pfi@ff (bfiFfiE'fifE?pffPPV1!ffI"VH=I,!W*6+- RvW+F?SffM@ E - 5 qVr 6g b)[ff4 1Kff 0G4ZVUt (VG hB lB hB<lgg/'BAr zh (J * lE hB l W)/V/h h (J * l'lhlnJeBhlBh<l&&(J *gieev b 4P42-'fi?pNfi1fi(tfi(6T$?3fi@ffv!8]# ~ EM #Iekge ]3g=< V ffI ?> - >gieev b 4P42-'fi?pNfi1fi(tfi(,-fifi44GvQfi(OTF?pfi@ffv!8]b # >?> - K# ~ EM # )0ff M) \i0EGaIekge ]3g=<> V HOI 0LE E& / f' E9* e$gfi[gieevbt}-m{t/s$qos0pt{al qM4P42-'fi?pNfi1fi(tfi(,-fifi44 8fi(OTF?pfi@ffv!8]{[Md+#?p$()fi4fi\+FvpLfi043fi0O(bfi?p$?pfi@ff,Pfifi44ff @ 36@ffff(f?pKfi3 :;'qff0vpGfiE'fi*-?MfiK? 3ff#'fiR GsJI()fiQ0fiR;U.V\ I1I, *WV KI *GV I,ffI >h)+F?4E?S204ff$?BV I,Ivfi 4[4'fiSsW(bfiF fiRpMlE!{)pffp6R-fipff8+F?7?p9_0'g'v8Fp+9fi:;'fiE'fiff!OcpO4_3ff'fiL Ssnh)Rff3 V\ I1I, fiqV ffI 45+Aff/$?pN_a'5'lE!H o19fi4p6?3fi@ffP<?0,@LpffpffP'fi>M@ff6?p5;+-fi:;'CfiR'fi!:[?Mfi3p?I+()fi33C@ff34A(bfi5?pNfi3 :;'YfiR'fi5?05+-@c3Mfi3p? 'fiP0M@ff^@UWPfi;*+7+ @ 7$3 0 L'fi MfiC4. +F4@ L(bfi[?p8;+-fi:;'fiR'fi!QQ3MF ff3F(bfi[?p8;+-fi:;'fiM:'fiApEPv pf+Fp? ?p-?pff [fiR'fiA@ Ypffff2 Sst,()fiA+c 4ffiAW" ff'%fifiRvffLv 0434ff!r3M3M@ +9fi.+f44Ofi 43>fi?p7fiR;4 ff! T$p? P?pfi ffP@ F13$ 34 !gTFp? A_0'6?3fi@ ff2'vp32p? fft?Mfi F ff7 v pQfiR'fi6?0,+F422'(bcff4N0fiRffA()fiz?Mfi >?$+F42kMfiM@ zI )aI4E)GEMffB M;UBE@@> - g6))8' ff5BU #Ei07~EMffE # 0R #Cz0 - ~)}= ggi C@C}g<4 ;EMffB bFa`i )1A)/IE~B ( )/IEE }b`b # abi9* e$g3M%fiPP sm@[%fi?'33!<s-c+E4F0fiR;P ] )J!:[3O[4,'3pFfi(Vr =R4fi,'fiIffv3M!gT$?3P()fi-''vp < & hB<lE*M973,RF?p[[?)W4Q'Ep3 v:;'Pfi( < !Icp(-ffa44pYfi(-'fi>4Y MfiLv0@ ff ff#hi! !* GsglE*0?pY583'$RQ?p>>?L K:;'7fij( 'v pL & hB lE*+F3? @ 7 VMhB<lE*4Q4'fi K:;'fi(-'fiO''v 3Cv & hB<lE!7T$3? @ (bfi *()ficG''v p & hB lE*VQ73[R?pLN?0 )Y4A'p3 Nv C:;'7fiN( 6!<+c fi?3$+9fiE*Mfid/v pI' 0fifi 4fifi33 0Yv Sr +f?Mfi3M$v0@ ffv pPff042;+F44MfifF?p7'3M?Kfi(6 U+c E4fiRp*M+F?4E?K?Mfi45:;'>fi( K''Ev pv ?p>2 p3fi Nfi(O?p>r 7!3%fi QXfi fs<!ffc@ ff p$ffa44pFfi(M'fiPg404v ff0?p,fi042;5fi(Mv 'fi/3v pK:;'Cv 'fiOP''Ev p( & hB lE*V+fp? @ VMhB<lE*J?N+AfiLvm '' pfi( & hB<lE!T$?2fR30 14fi4fi Gfi(9 mffc 4 080fiR;*VL U?pPfi$3 @ oM0v S?3fi1fi(fi(g-fifi441;! Afid+Fv 3>? )K4,'Ep3 [:;'Nfi( C''v p7fi( & hB<l60fid/4pffMfi3>?0 )S4A'Ep3 Lv K:;'8fi_( 'v pfi( & hB lEB! ]gieevbv0P+-C@ffMR?0ffC@ff3>'fiSfi\?pfip :;fiR'fiE*O+-fpffWfi0fi4p7?p;+-fi:;'GfiR'fi!]3g=<K Je /C )a4)RE~B / V\ I1I, " H=I V KI " HOI 0V fi ~M~;EBXE7?> -O g6b)' MdNB # \Ei0>~EM # C)- )/EMff B 2Ca -e /X 1ba& b=EMff B / \V I," fi V KI " fi 4 V _JIekge ]3g=<V\ fiKH=I ~ R0aE E b # @> g- 6g b)~ MffPB# #X \ i0E ~EMffE # EE)t )M IM~B 2CIekgefi{Tj6T$|Fv/mknpokmkq{Tj6T$|fv) RTj6T$|9 R/)Tj6T$|>/) RTj6T$|- R. X/Tj6T$|Z>rO3M@>w1,TF?pL3M'fiP vPhi()ElA vh)?ElE!T$?3Ip o/F?pfiffE?}ffQ?fi8vffvpfiR'fiEF?Q3Mfi[R3ffG'fiR GsJ5+F4? @ff'Rff5'fi"Lfffi0NfiRffff!M@ N # )a4)mEM~B /S)1Q)}dME Sag }E)8g6))m) (); # 'pMb }bC)BC)/Vr Mff18a[M~;EKBI?> & / f'Ei9* e$ggieevb[3r 4'$_ ff7GL" fffi 0 fi;4fi7'fim ff pM!ST$3? @ (bfi' 'pSff#&=4~_$ff8?p fiRp!RrMfiOff?qff&''v3M*:Zv0'Rh)fi?p _0E'v'Y4(f4rO~:;"Lff'%fi fi;l7fi(f?pK'E434(bfi2fid+ff1 ff'%fi ! 3M%fi C?pYPE?v pYv ff v pmfiR'fi 'fi/3ff3+ ''v 3G+f?&v /_ ffC@ Rffvp 3M0'v p 'fiC?pI4 p3fi !LT$3? SQ4f%fi0v L?Q?p8@ _oUfi(,?4'' pPR(bfi L?p7 /k_ ff4P@ Rffvp3M0'' pPv430pff5P'v [ MfiP@ fffi 0 *a U?pv /_ ff@ Rffv3P3M0''Ev pIMzfi ffWMfiFv23pQ@ ff'%fi! ]v0P+-C@ffMR?0ffC@ff3>'fiSfi\?pfip :;fiR'fiE*O+-fpffWfi0fi4p7?p;+-fi:;'GfiR'fi!]3g=< [bO )/}<g 4 ;4)UM~B 00ELM~;ESBO?> -O g6))' fffB & / L~EMffE EE )/ # 'bZ)/ ( [ )R)/Q/bi~`;SB) )/~ff )/ # # b}~ffM }) g6))Yb ( b; # ~Mb }E)Iekgefi04pONfi3M0-fi(%4443'X oM0vfftfi(%T$?pfiffL0846fifi44*30vpQrO3M@5w1!D<EfiI'fiRffv3Xh)?pYr fiX?pY()Ifi([rO3@YwlE*6*9+F?p@ & h vdlE*` DZ>1*,()fi"Lfffi 0 fiR;D>R ] hicalE! [3OKfiR'fiV fi4v v pff ffffph j6T$|91* j6T$| >l$ \34} ffF?p>' 0fifi 0fi Kfi ffph _6TF|91* _6T$|-l'fi j BZk hi NrO 3M@ NwLfi C?p[ ?ElE!<T$p? ?pQ''v 37fi 2'vpLfi(JC()fi4fi\+ ff1v /k_ 0ffP Y/]^8h)J*^/*^/*^/*!!! Gl & h v l-a3MPD>1!5T$?4Fp? ff0$30$'fi >+F?V fi 2LMfiT3ffff44GsX(bfiLW" ff'%fi IfiRvff!NT$p?oM0 4423''fff+F?V\ I1I, " H=I,3MfiNR3ffK'fiIR[ Gs()fi5L" ff'%fi [fiRvff!grfiRV ffI " H=I, *M3M%fi f?pNfi 4fi ()fih j6T$|91* _6T$Z| >l$2 j( #k v v* j Z( k v v 03MT 1?v pfff4 I4f?pOF Krgv 3M@ >w1X! Tv *M+ >U@ L?pNfia ff(bfifL" ff'%fi N0fiRff!{8fi43pQ1K3PPE}ffvp8?pN%fiLPfia@ff30V\ I1I, *V KI *V\ I1I, ffI N0V\ I1I, I4vfiffPfiEVfi4dlE!OK@ SstN()fi7G0fiR;#h o/@ff40vfi[V\ I1I, " H=I * V ffI " H=I,Vt}-m{t/s$qfi4os0pt{al qM@ GsJA()fiFcff2NfiRffff!K?3p7Pfi@ff3V fi 4*{V H=I, *{V fi 4 H=I, *{V fi 4 vfi *V ffI " fi 4* \V 1II, " fi 4*{V _V\ I1I, " H=I * V ffI " H=I,4Sst5()fi$c+4NfiF"Wff'%fiNfiff!Vfi4V\fiJH=IYfi?3ffff~:@MfiL3ffff44 GsJA()fiF"Wff'%fiNfiRvff!?3Y()?4$?M@Uff7vpmfiR'fi?2'(b?pUff042;Xfi4fi#@$3ffI'fi[ GsJt(bfiOcff49fiRfft4g`_*Rff3-c+E4-fiRff@O o/'@ffOffm3 (i3OfiPPfi (bfi> ()1v 3C'/'ffP> P G4PfiN024fipffKfi 0ff$fiRffAfi(g?4527hiTffOF!*Jvffww QlE!rOv 2*,()fiuTFp? fi@ ffPSvY qR+ Uv ff pff&??pUp? ffPfi(Q?pUfi0 ff()fi45fi(?pOp @ ff3Q4ff?3L V HOI fiL V fi 4P'J!7sJ7v G?4[0RQ+ P@ ff?pff Q'fi30 ff'fiOQ'0A1k_ 0v p8fi@ e5O?Mfi/-(bfi5pff4 p7+f??3ff? C'fi@ _afi (bfiE?!,ffid+ ff*/_0'*1v P?3p o/9@fffi *1+ Qfi 439?Mfi\+fi3MAL0fi@ff305@ L' 04ffK()fi Pv p Q'fi0fi1035r =()fi[fffi !3\fi :X-: ficffW?28@fffi+- M@ff'2fffi3 +F?3@ffE?&fiPv70Z3@ff847fi\+WXv/:DBEk 3 R1 3R34/430tr 7*V03MN()fiL_afi?pfi/3Nr 3ffN'fiYR()fiOffRR`_$ff!Prfifffi *[ffv3$fiR'fig2ta4ffN'fiQ7/43fi6r &>?3>?p9fi/34Q()fiOff!T$?3@(bfi*JN4cpffffm'fiYfi4pN?3'44fimfi(-ffE?Wff0vp fiR'fi()fiv 41403'fiYfi130[r 7*V ?Mfi\+ ?>RffL?pP 0fi Gs&@ ff34Q@ ff ff%fid !rMfi,fiE'fiV ffI " H=I, * V\ I1I, " H=I, * V ffI " fi 4* 0.V\ I1I, " H=I *+ 5fi 4pOfi L?3-'E ~:4fi 0Qfi(9?p4P4 7fiR'fiE!PT$?4Q4QRff3@ ?pP' 4fi [fi(A?pff fiR'fi7@4P0P' 04fi Afi(O?pff454P4 [fiPfi 3!9T$p? N@ ffP v p8' 2fi $@V ffI 'E4ffF'fiV ffI z<\fiRV\ I1I, !V\ I1I, '4ff$'fiV ffI z<\fiRV\ I1I, !V H=I, 'E4ffF'fiV H=I z<\fiRV fi 4!V fi 4L'04ff5'fiV H=I, 0z<\fiV fi !V 4fi '2ff5'fiV fi 0z\< fiRV _ !V 'E4ff5'fiV !V\ fiJH=I '04ff5'fiV fiJH=I !cNPffMfiNR8v 34 >'fiK?pI@ ffpL?fid+ V H=Im' 4I'fiV fi !>Ttfi 2443''E*a+3IrO3M@Cvy/*+F?p@8?p8'4fiUfi0fi*%3E?SPhidlE*VpMfiIQfi(2\z`]_Y^ 1ffU']fi ff! 3M%fi .V H=I 4f004 ffS'fiffpUh~v*Jl[v S?p (bPfi'Nr 'fi ?Q?pI' fifi 4fi 72_Mfi\+hi|UlE!OT$p? 8p+ ffpLh~vv */v lt4gpff7'fiQ?pAfi/3Jr h) ?fi'fiR{/FGvR{mknpokmkqvJ;8{ZLKM9/vvv)HIFGRHIrgv3M@Pvy/AYp4}ff4fiYKRfffiO>0fiKv fi130!v rgv3M@7vyl<+F??p['4fifi4fiPV!6"Wff40?09'fi8(bfiE?pffi/3,r +-f.?pZAff4fi130Cfi(8?p2ffU?3RvEfffifi(8?pR'0fifi4fi!sJ4. +F4* V ffI '04ff5'fiff4?pRV KI fiRV\ I1I, ?pNfi/35r 7!TtfiW4423''C+F?V fiqfffiV X?pKfi/3*<+ U3@ rO 3@ >1! 3M%fi+pff ?pff3GhiT$T" fH ScT$T$cHN8*t"f|-A|,,c :[c~HQLlN Wfi\ ?3' fi Rfi 0fi'fi ff3hiT$T" fH ScT$T$cHN8*,T$T" fH ScTFT5cHNLlE!,TFp? &?p\4fi0F'XhiDQ|,sV1c :Q|,"$c~HN8*DQ|,sJ,c :Q|,"FcHN8*0T$T" fH ScTFT5cHNLl9RfffiOfffff0v N(bfi v 4%' hB$QstsJ|-5T5cHN8*"F|AA|<1c :[cHN8*%T$W" [H Uc~T$T5c~HQ>l51Y./v p 7344fi Qfi #hirt:;pff44 Bqcp:B@ ffff s:' PElE! D<@ /fi3*?A834fi -4fi ()fiff?p[fi/3,r 'fifi8'fi hiDQ|,sJ,c :Q|,"$:c~HQ8*aD[|9sV1c :[|9"$c~HQI*0"F|AA|<1c :fc~HNLlE!{?4024fi LfiS?pff@ ' 04fi 8?d C()fiI?p ( pWfi($?3C ff7 v pSfiE'fi()fi>?pfi/3Lr NTFp? P%fi PUfiEO@ ff3L(bfi.V I," H=I,* V ffI " H=I, *gV fi ()fi@ ff@ 1 pQ+c E4$fiffORfffiOWp f(bfi9?p$fi130!gT$?04646Rff3 V HOI PffRfffiOEV fi 47 0V fi ffYRfffiO.V !FYU?p>fi?3[? 0*?pL%fi4 >P0fi%@ ff3()fi.V\ I1I,* V ffI* V I,ffI NV\ I1I, I4vfi 0@ ff /v pU26fiRff>@ ffPv Rfi P()fi?pIfi/3!7rMfiV\ I1I, n* V KI n* V\ I1I, IaffI *VV\ I1I, I4vfi *V?4Q4P04 ffF?N?pIfi/3r C3pffQ'fiKRI(bfiEOff*@ _afi mMfi ffcMfiL?ff 'fiKIMfi p*t G?3Q?3@ P40mE33 /:B4OCfi'*X X()fi7300 fi Pv ff v pG3M'fiMfifi30a! [8fi 3ff%fi\ *?pY'fi3M0 ff'fiY0fi(L45fiR'fi@ S33 K'fiW?pffV H=I fiV fi 4SfiPfi 3!n+c q?pp o/fffi &+ Kpfffi#O?Mfi/I()fi@ ff3pU?pYfiP0 oM;Zfi(F@ _afi Xfid 'fi@ _afi K()fiE?Y+fp? K?pff NfiR'fiEF?d >RK04v ff!v%G{3}?"Lff2<?7fiE'fiV ffI 0V\ I1I,3fi73fi0vffPN+F?R?p(;Rfi(5ff7vpM*+F?pff;V H=I PV fi F@52'. hi!!*WMfi,N0fi3ffP'fi> SstElE!6r3M?pEfi@*V H=I, 0V fi 43Cfi0vffPN+F?pW?pW@?p fffi'WYp+9fi:;&fiE'fiff!NOir fi3$ff*A+-Y?0ffS3fffiRff&v@ffO5@E`_afi&fiE?PI()fiP?pffUfiE'fi?FS_a4pff@ff7?pN4OLfiP0 oM;Cfid['fiV@E`_afiK()fi?!" ff4,?08?3@ C p+9fim+Aff/7?08fiRW'fiE7Xff4044pafia4#hBLl>fifi/4&hisglE!gr3M?pEfi@ *O@ ff46?0.V fiv@ ff Cffa44p ff?3>+Affq!h W:msglE*+Fp? ffV H=I,Wfi vffff4044pG4fi144XOh W:GsglE!J{ d?EV H=I, ?Lfi 4fi[t}-m{t/s$qos0pt{al qMj fi12} ff\k RffIfiZff042;*O+F?pff>?p ffIfi(V fi 4CPffRE0?fi3p?P0Qfi(9?pr >!T$?384P044fiS4[?Q+-P?0ff eQ@ffO6O?Mfi/()fi,@ _afiP2fi@ffP()fiV H=I, *+F?p@ff,+-[3Mfi-Mfi744.+F4A()fiZV fi !Oc+fi?p9+-fiff*fi@ ffi12} ffORff,fiff042;I402 ffg?,,2:ff <'fi8fi/4} 5@_afiP'fiv'Rff!6T$?264,4fiL'p3 f(bfi9fi?;+-fi:;' fiR'fi9?9?d qV H=I, ,?pff4, fffi C'V*/! !*V\ I1I, " H=I,V KI " HOI @ UOY'fiZ@ ffO7hifi/4} ffl8@ `_fi ! ff30MfiMfiG4v pffX1Zfi 04pp8V fi 4KR*9+ Ypfffi#@ ffOA@ `_fifi4?PN()fiL?pPfi'p,fiR'fiEV\ fiKHOI !TFp? ff P fi4?PLaU'fi8V fi 4 0R4fi?p['Rff4ff$fi( V\ fiKH=I !{ Q?ff QpfffiRff;+-fi7pRff9fi(Vv@ ff0@ `_a4fi fiE?Pg?Mfi f?-(bfi44fid+?pC0444fi fiR( V H=I, *6 W?fi ?8()fi4fid+?p0044fi Rfi( V fiJH=I !YrfiffE?fi(5fi3Mff0v pGfiE'fi*,fi 3CfiPPfi@ Kfi(F?pff@ fi?82044!R(bfi@ K0@ ff vpm?pvffOfi4?P* 3M0 ff4fi C1!4v[@ ff ,p+9fiI fi4?P9(bfi-'fi@ `_a4fi P()fiE?p*ff*fi pF()fi-+c E4F0fiRff< ?pFfi?39()fi-2fiRvff:o10@ ffa $r [*M9+ ff45 fi?()fi-./v p8?p['fi5fi130,fi(V?pQr [!<T$p? ff Q fi4?P04'fiY(1fiNHOI *%( Kfi *0fiL'2fffi ! 3M0@ fffi C1!fff@ ffOV fi $fi(4-?pK fi4?P8v 3M0 ff4fi "C1!4v!T$p? ff fi?0PI@ K044+F3? Z?3 v ff v pfiR'fi$4V\ fiKH=I fi$ fi(t5'Rff4ff!,r3?pfi *1?304'fiP fi(O(1fiH=I *( fffi *$fiK '2fffi3 ! 3M0@ fffi C1 ! >?Cv@ fff fi?P()fiK(1fiH=I,J( fffi */v ff v p7fiR 'fi V H=I */ 0ffc 2[ 0(i34aL" ff'%fi @ ffi ff,v 0234ff!T$p? $@ fffi Pfi430pffO+F?I?pfi 40ffP041@ ff36fi0Ev pN?p54$fi0 oM;fi(<?p8v@ ffg fi?0P$+F4?Y?p84O8fi0 oM;Kfi(6?38fi ff'%fi v p'fig fihi$+ ff4F+F?ff?Sfi?plE!T$3? Ifi< Spff4fi0v p 4tfi(,?pvffOg@ E`_afi fiE?P[4QPoM4PjeP:!9T$p? ff >v fi?P5. >?pL3fi ?0[q fiA'fiC ff v 3M*1+f?4?Kff?-fiE<()fi33 fi @ /fi3O `_fi JhiEl<?ff [@ ffMIRI_ff!6T$p? Pv ff v pQfi/:3M7yh V/hB,l, lE*()fi4fi\+ ffY ?p>@ ffO@ _afi fiE?hi 7rgv 3M@ vdlE!-WH o/-3,fi 4p-?p /0aSEFaff;afi(V?p[ fi?*+F3? @ F+ f3OTfiPPfi :! [4fi(V?3Q@ ffO0 `_afi fiE?P,@ ff ff p? @ Q@ [fi33 mhi! !*+Fp? 3 ?p&fi430pK?P@ E`_afi #3ff*5P4v &(iP'p3 Y?0C 7l8()fi+ ''@ffk0fiRff[ j 0@ fffi v ffkP0fiRfff(bfiQ+F?2?S?p3fi 4To1 ff~:j Mfi\W0v L[k3j?gr 7!aD[fidW+ 0''@ff fiRff7h)+F?4E?Uv23p" fffi 0 \l5Ep? ff.U@ ffp3 0ffffi(fv Yff%fifipdp* ! M!*a+F3? ?pL*)G2A(bfi44fid+ ffY1Cd!,+cfi ''* j 3M0'@ffPkfiRvffF3? ff.U()fiNv U@ Iff%fiJfipff* ! M!*+Fp? ?3TdP4f@ ffffpffS)J! (+0 DQ fffi ffffiRvff*a30?m[+c E4*%4%fi Ififi3f()fiNp? ff.1v 3M! fiO7fi(?p8v@ ffOg fi?F@ I4'fiCfi0 *%! !*%+Fp? p [?pYfi23p>?0f@ E`_a:fi (i44*6I4Iv Z()I'p3 ? !qhiT$p? C@ ffp?fi34fffi4&fi M(i3 p jfiafi4?PkI+F? jfiP0 >r 7!^kl{p? X@ _afi ()24*<IMfi ff'fimff30 fi(Ffi p fiPfi@ fi*6+Fp? j fik4a4 ff5?p 84[fiR;K/fi44fi ZhB 7lE!QT$p? @ I@ 8;+-fiC+Aff/['fi @ ff'fi I3E? fi!|,4?p5@ 3M 'fi?3L@ ff ff7 v pIr 8hil- K?Mfi1fi > fi?pf ff7 v p8fiR'fiF K ()v *tfi>. ?pP@ ff3Qfi(5 ff p 03MN@04L?pr IhiElQv fiOfiPfi?p7+-d'fiY_om?pfid!{?nfi p o/4fi *$?pfi0v G fiE?PCv #?04@ fffi k_^a`y`L'Ep3 fiPOdRQK^^^ qd'B<BE~59^'B^ d~BTo'$BO\iB'8[E\NmV\iB'E&d^iB\B^NV~^-B~Tl~EE%B-B\6E\^^'E\^bQE1B6^B5~;EE^BB5J\~pB~[BEO$fi""""mknpokmkq""0M"Tg0PvFT$?3'fiR(i334fiR()fi8fis<]^8r z04!YTF?pfi\+F8fi@ff'%fiZ'fiGffY?pLfi430$fi@ ff'%fi 'fi7344fi $4fi!v'fi/3ffm1mvffvpM!T$?pPfi4?PN?8@Mfi7fi0CPff4'fiY_k(i4Ofi!fi?0?0O_kO4/Ifi 4>'p3 ?fi6@ ff'fi4 5?pff )fiE6v ff?p6fi(?p5;+-fi+Aff/X! Cv fi??5Mfi fffi,_k04kfiE9fi-k_ 0,(i4 [fi pff-@ ff13@ ff-fi@ [@ ff''E4fffi6ff'fi43Mfi V!tffc 80432ff*6fi >53@ ffI+F?8?pA_0O?Mfi1I()fi6@ ff'fi41v 3Tfi*+F?2?>fi 04'tfi(?Mfi1fiv 3[ Mfi?3g ff7 v p$fiR'fid!gT$3? 9 fi4?PV?0g@ A'fi33 0L03MjMfifi0v ChiSk_ U()4@ fil5@ 8fid 4Y3Mfi3!F+c Ufi?pf+-fiff*0?pUffK@ fffiPfffi2v pPPv ff v pIfiR'fi$+fp? Yv (i$?pLfiR'fifPff RL( L'fi04!()fi@$@ff@v3[?3$v@ffOMfi4?P*+A_0'Off<vfi?PO()fi<'fiM':`_a4fi7()fiE?!6T$?pff5fiE?PgMficMfi639?<vffvp[?gfi13@ff*8?p04P'fi4%30fi !6T$p? @ Lfi 3,'h Mfi544fi@ ff (bfif ff v 3lE*03Avff?e*? Kfi3Fv@ ffVv fi?Pff!} gieh.f]$^_hS9e ]vBT;H gfi[F]{i[e^ $ekgi[%P ekgy8[J_]{[ek^brMfi-40fffi eC*2/fi(afi3M9fi?63O5?0,r f,@5@@ff@ffP30vp70 Ffi(J?pF' 0fi ()33 0fi VU1h (M ^/l,. *+F?04?Off 9?9()fi5' ( *1./vpIfi^X ff'fi p o1' . *5P?Mfi\W+ #v #Tg0 Rv!n"$fi\+Ffi ff'%fi X'fiW'ffC#fi430fi@ fffi 0W'fi834fifi !T$?2>@ 0@ ff fi X4ff34 >'fim?pKfi /432v3 Q@ 0@ ff fi Yfi(OrO 3M@ ffFP>1!<ffc K0234ff*MTg0 PvL4?ff13 5'fiP?p>rvGrO 3M@ P>(bfiQ?p4 0pNfi Ls,!ffc mTta Cv*t'ffL@ 14ffG1U?pff4F_0E'[v'ff*C?pL834fi 5fi 5@ L0@ /4ffC?pff-_0'A'ff!,rMfiLoM0 * jkfffi7rn. ff74fi hirt: fi4 ffElE*gc[. ffCh)cp: g ffff4 \lE*O 0Rs#. ff his: E PElE!T$3? 0fi 2'<fi( ' ff9(bfi9?pT3 o1-'*M! !*14,fi@ ff'%fi 06'fi7?3F' fi (i33fi Vj! j yk8v?pf0 QOff 9?9?3@ f4 Mfi8fi0 A' 0fi (bfi-?49' :;fi K04ff!6cpf43fiv m+F?04?m?2Nfi13L4Q+F3? R Rfi Z4cMfi>44fid+ ff(bfiY'!K-fi 43L o/P03 fi(9?p0 P()fiPL()fiL_ :;'3'fiP/! ffi0v pY'fiY?3_0'hi3RL (bPfi'El'v Tta Yv*g4(9sq2Lv 'CT$W" [H Uc~T$T5c~HQh j T$klQ Rr. ff8fi ZrJ:;fi2 ff*tc. ffIcp:B@ ffff *< Zs. ffsV:B' h)+F?2?R'fi?3P4>834fi 74fi jklE*6?ps+f449' 4fi X'fiRP"F|AA|<1c :[cHNh j "FklP'*5! !* U1hiTN*,El "L!<{?X?4Ia34@ ff 4fi v* V\ fiKH=I 4$4a ffOffK$R3Mafi hi73M4fi al-fiR'fiF?0F? 3ffZ0 '#'fiX Mfi?p 0MfiPX?Mfi@ 4p3 S(bfi ?pp o/C'! [R'fiV H=I 4CR3Mafi fiR'fi-?-E? pff-7''fi8Ip o1-'Q@ ffMPRff pLv P?09fi\+L!rMfiIo/a _* 34}ffv 3C?3P' 4fifi 4fi Rfi p ffpmhiTN*^"[lNRRPfi042p? ff1? 3v p8fi pffi(J?p[yA'fiI "v ?pF_a',fid+fi(JTg0 Iv!<T$?494,Rff3@ f?p[' fifi 4fi I'fi/4ff+F?ffp>hiTL*^"[lg4g?p$ 6fi(4/7304fi <fi 6?6'E fi 7()fifi[t}-m{t/s$qos0pt{al qMW % P[ Z\L]_^"\!PX %Ha`PX b W c`#\X ZYedgfh X NPXPX _s\ PX `#\I`z 0l ei i_jkalnmomplqi_jr042 0l`#\H X 20 ti` z j ^ l t\vd/\ Yw[``#\ X Z` !| j ^ l x{z|\ i?[ x{za`#\`#\* 204lk lnmomplqi j rj u[ PX20 Vyx{zX y|l x{z`#\H` [ Z\}d~\ Yw[ Pjk_jr[[[[!|\x{z fpfof \|x{z 4 `rO3MPvv;VM]_^a` u Qfi/3$fi4?K!T'fi "L*!!**%$7vSTt0vv!FT$?2$f2L o/@ffffSvU-fi1fiffG8h)cp:B@ffffPs:B'E PElQhi@>rO3M@>lE!rMfi,( Kfi fi6'2fffi3 *fiV'fi[`_fiL?p-7304fiJfi/3tr &pffJ'fiR$()fiOff(bfi?p[v /430fi 5r fQhi Qrgv 3M@ 7vdlE!6{ Q40v ffO,?pf fiE?VM]_^a` u L()fiT 3vpC?p8fi/3Fr 30v pP?37 ''303M@ >fi(<Tg0 v8f?fidW+ GvrO 3M@ Svv!K+c W?pfi130>r >*t oM0 fi/3>' W' 4fi R4~U1hBA"OTN*gMElDQD["Rff3@ U1hB[*MEl[ D8w* U1hi"L*MEl[ D8* }UhiTL*%Elf"()fiQfi Nr-*%cE*% Gs<*@ ff'Rffff4!mTFp? C 4<fffi($?p fi/38r @ C(bfiEOffRffvpm+Fp? ?pv 0123V'fi(-?pIfi/3Q4Nv 04J!Irfio/P0 *V(,D8*VD8*V " Pv 4'ff[()fi[r-*0c*a 0Ss,*0@ ff'Rffff4*0?pGDQD["+f44%R> Gv 48()fifr c s<! F()()fiPv pW?pUfi130'ffK 'Rff(b/v p+f?4?@ mv 4B*,?pm fi?0 fi(LrO 3M@ vv'Rff`$_ ffA?3~U7' 4fi (bfiLC0fi1035'7 U7344fi $4fi !HffiP?L?3P fi? rgv 3M@ Kvv()fiPN?p0fi103[r zn()fiLff'v3K+c E4fiRvff!CTtfiUff8rO~:;L" ff'%fi 0fiRff>30v p\6T `_a4fi *t+ 3ffR'fiG(bfi?pfi/3-r !<TtfiMfi8?4-4048 ff34@ ff9fi 04pp 'fiIRf?3Iyh U&vdl'5fi!T$p? G fi?0 v rO 3M@ XvvS2fi/`k_ ff#1#? p p"U='fi U}v +fp? @ !cp 4C4'fi4%fi$'fiMfi>?0[v^a`y`<43fi 8hiv430v p(1fiNHOI lEv* qVJ]_^@` u >83'fRff3ff'fiU()fi?pPfi/37 ?( 6T _afi W2L'fiYRMfi 3!ffc(1fiH=I,J *<4 @ 3L?pv 3 Pfi 8r XU#4v!rMfiP( fffi* U# vP4'fiM!Cffc fi?p>+-fiff*J()fiI( fffi ?p834fi 504 *1fiN()fiOff*23 F3M/4pff ?p@ ()fi@ L5fi34 RQfi 43@ ffK4.Cv p 8fi Q02 !$ffc U%fi?Sfi(<?pffff*( 6T _afi G4ffi p7?3>fi/3[4f.fi(O?pLv 3 Q04 Kr K?pL0fiR; r >!Q4 ?<?pFfi/36r ?06RI(bfiffPk( 3ffpff*?3I?p5k_ Vhi83)lfi ,rRR$_ ff!{ P_a'Lfi 2p>K 20 Ifi/pff<3? ff./v pU fiE?K*J2 ffGVM]_^a`y*4fi ff&'Rff`_a4()fi ()/v pG+c E4fiRff8fi(f?pK(bfiE] )t!ZT$p? K fiE?K*?Mfi\W+ PrO 3M@ >vff1*Mfi 4'<fi(V>p?/:_0E',ffE?fi(tRv 3v 3Q? v 44M'X!ff40 F'fi ^ %?-4606fi(>' 4fifi fi V*+Fp? @ ^aq)J*/fi4ff,?p$fi;!h){ 7'fi@ L?pL Ffi(O2'fiPR^aq)U()fi$04Kff! lOJLfimknpokmkqH204l``#\H X ti~ !# %2`#\`#\H XJ tiJ !# %` Z2% PX `#`#\* 0l%2H204l `#`#\H X8 x{z W [ x{zV [0` |l x z 8PX l`#\`#\H X8 x z W e | i8l x z` !! Z*% PX`#\* 0l%2`I\ `%6`# 4 `rO3M@Pvff1qVJ]_^@` QE`_afiYvfi?K!H o15+Lfi2p5Kfi? ()fi5()1p.^@UWP0fiR;P+f?MficpfiY2- o/@ff0N k3j?Or 7*v430vpPrO'~:;"Wff'%fi8fiRffff!QTF?pI@ffpNPffS+F4?S'fiK@/+?p? ?M:; ffpff4fi[fi(/?46TWPfi1pffE?pff./vpffi?0@ffff>v 30fffi9>1! >AR()fi@fi v30v pM!Orgv 3M@ Qffv > ff6[04- 4fi 7fi(a?4O fi4?(bfi -fi3fi3MR4j6!Ah~vffwwlPffi}ffP $O,!$h~vffww ClE! (({ [4M?46v fi? qVJ]_^@`!RYRff3 F<26'fi03M'fi:?pfi@ 2L `_fi !9W" ff4V?0Fv\6Tfi13ffVp? ff.1 pM*0?p>fiR;K4$@ @ ff ffUfr 7*1 './v p7+Fp? ?p$q 2Xff13 ,'fi'./v p>+fp? ?p & hB,l & hi8l<()fiAfiRp!YT$?04L4ff34 L'fi & hB<l u & hi7l8 1*O+F?2?W2L fi?0P44Gff'ffZ./v pS?p'fi50fi103,fi(t?pQ04r ?pQr =fi@ ff'%fi v 3>'fi !6c( & hB 7lA 7?pl & hi7lE*/! !*Mq = `_fi 30ff ?1fi?p+F4@ *M`_a4fi P(i44!& hB<ZT$p? Q fi4?fi(grO 3M@ Iffv >83Off-?A?pYpfi fi(t?p[fiR;Gh 7l,4o1 ff ffP*3jE?V3M'fiP'fiS K?p7r =Rffv p ffEp? ffS4F P![ fi?0 qVJ]_^@`!R<*Vv Grgv 3M@ ffv >1*J34Sp? ff.1N+fp? ?p> ()fiL SfiRpU!TtfiPp? ff.C(O*M+ NYpP pf+fp? ?p & hB 7l1!,TF?4-4-'p3 N(V?p Q25'fiOj 0km'Kv hB 7lN ff?0v C()fi Xv 2,'Y Z@ ffE?0 ()fiff(*<! !*09fi(J Cff0v f/[ 0P?p@ ()fi@ [12ff /k_ ff4>fi()!,T$p? f fi?fi(VrO 3M@ 8ffv >R()fiP6?2<p? ff.3v pL3ff'ffp0?/:_0'9ff?fi P?3Ffi/3<r = !6T$p? $_0'?M:_0'F@ff?GRpv ffv44V'ffQ S12F2Jff40 7ff!F{p? p NC'hB 7l-254fi\ @ ff*0$254 ffY j ffV*^k YO3ff'ffU ffE?YRv 09'fiCfi1fi.()fif/7?0f@3 $'fiC?pI ff!fcp(6?p 84[C1*?4[4a4 ffA?p hB 7lLhi@ ffalf'mR>/4ffGv /_ ff fi()*V S?p@ ()fi@ I?p4 p3fi 4Mfi pff0;hiB! !*?3@ 4Q'fiOfi ff33Lv K?pNa4 ?FMfi ffLMfif4'()?3NfiR;l- K E`_afi K(i44ff!%^6^EBB\n^O~L^LB;m%~^Tl2ffNmV7d^LidiB~=Ag~$MNrOrff;`U5ff\To'B^Bm%EO$E\<BBt\^^'FE^BB&`JB~^^)\EWm%E% EJ1 w5ff`VBO'mVBg^V^J\^;^OB-EBVBo\BiJ BT o'B^QB'a%^V5ff\T o'B^Q$ ~%Bg~iB'F'E^ oBiJ^[\B'\dB<d`~Offxfi[t}-m{t/s$qos0pt{al qMH204l``#\H X ti~ !c,W2`#\`#\H XJ tiJ !,W` Z2% PX `#``#\* 0l%2H204l `#`,iBM !B,W PX`#\ X tiJB !B,W %*a`#\`#``#\H X 4\ f [ a`i` Z*% PX `# 4 ``#\* 0l%2H204l a`# PX 2 XJ%`#\H X 4\ f [ a`il I2\` ! Z PX P eP ffi` = Z*% PX e a`# ```#\* 0l%2rO3M@Pvff>1;VM]1^@`gR_afiYfiE?K!fimknpokmkqW %PX ZY [ w. _`0[ t\-\ f\Ru_ ruP `0 |\ z [ x nPT * z #` \H ti z *Z yx TPT f\L]^"\PX ` PX b W `#\ #d@fh X NPX \ PX _s042 0l`#\H X 20 ti juk lnmompmoli z lnmompmoli jr `#\I``lD` | j ^ l x nPT`#\H` [ Z\}d~\ Yw[ PX "||\ i8[ x TPP !|\ j k [ x TPT [ fofpf [ #z! [ fofpf [ |\ j r [ x`#\* 204lrO3M@Pv MZZi zl x TPTTPT `UW[ u Qfi/35fi4?K!VM]_^a`y< 0B VM]_^a`R@9fi33>0>fi0v[h)()fi^@UW[fiR;Q+F?fiXpfi84# o1ff~:0vQ[P*3jE?Jr NlE*0U?pC_kK4E`_afifi!-(bfi@Iff4%fivpfiU?4*_0'MfiN?5?3Nj `_a4fi fikI?50`5fi3Mfi4fiK(bfiqVJ]_^@` QqVJ]_^@`R<!rMfiVM]1^@`y fi>2L Rff40 0R'fi hi! !*t Z'fi ^)Z+Fp? @ ?p0fiR;m4] )%lE!$rMfiVM]1^@` g F4f Uff0 70U'8?Q4$a$fi(<C1!$TFp? L@ ff'fi VM]_^a`4A'fi33 0C29?0$Zbafi 5)fiE9fi 0P?Mfi L'fi^)J!6cp$2-fiP0 N Ck_ 094fiRff3 [9Mfi ffo/?03' [ ff? ff'v37fi(V4ff40 f'fi! VM]1^@`gm4-4'fi7fi33Kfia *()fiF fi fi3$@ ff'fi !9ff30 BVM]_^a`yNVM]_^a`Rmk_ K4Rfi*/?pKR-30 ff>+f?ff4?p6O?Mfi/7fi$( fiO@ ff'fi43fi ChiB! !*?Mfi1fi $ Mfi?pOfiR'fi6fig_o>?3-r NlE!M@^]v $ekgi[%| e$g \V fi KH=IbFE^Zg[J_]{[ek^]$^_h[4fi(6?p8fi?FvK?3>@/fi3F3M0ff4fiUS7''@ffP4pff S?f4f45.Mfi\+W?N ffpfiR'fihivS?4[*V\ fiJH=I lf?[RG004ff!QrMfiL20444;*a2tfi(,fi3Mfi4?P83OV\ fiKH=I 4802ffR'fiGvpC'fi hi7304fiI4fialE!rMfiO o/P0*+I3O8?N( U1h J ^ fi fi, l$ 3 p*%?pGV\ fiKH=I h Uh ^ fi fi, l'l$ 3 +fp? @ 3 , 3 @'ffCh)fi7y/*t20/v pfM\fi p o/L'\lE*g 0X^ fi fi ff 4NY834fi Lfi !I+ 30 ?p0325@ @ ff fi *0?4A' 4ff$'fiE? pv 3fi pL0 c'!rO 3M >v 7?fid+F9 vffOfi fiz( qVJ]_^@` *14 ffPaUW[ *1+F?4E?P462fi@ ff()fi@ :B()fiPv 3?p7fi/3Fr ()BV\ fiJH=I ?fRSa4 ff!$T$3? > fi?0 fi(,rO 3@ v 4()fi,ffc 4FfiRff ?(bfi6T# _afiE? pUK'fiEURvFv ?p[ fi? 3O? UW[ u N450240 Qv K443fi A+F3? K.1 p?p2A?pyh Uvdl'Ffi!-[?Mfi33fi/35+f? ?pLfi; r >*?3L4fi4fi U(bfifpfffi0v 3?4$ fiE?z+A?pN834fi FRKfi !<W" ff4?5vC?4-34fi z* 4[ ff7 v p84A04 ff'fimv 41403fi 7r >*?pIfi/3Q730'fRI :B()fiOffG'fiK ()\fi0gfiff!QT$p?+A'ffUfi'Ffi(Ofiv pI?4$PfiffY?p>pfffiaOFfi(t?2$ fi?0K![ fi?0UW[ u U3Off??3Sfi130P+-()fiOfffi v 2nh)R()fi@ ff7 v pl3 pVM]_^a` u! UW[ u C04} ff>fi ?p. Mfid+f ffpCfi(5+F?Khiv /430tfiI834fi El'qh MBl 0n832fi Kfi yh ^ fi fi, l' fi 'fiX p+ p o/K'K'Rff`$_ fffiR'fi2V\ fiKHOI !OT$?46 fiE?30OffO?<?p50@ ff ff pFfi130n46''fi ff!6T$p??pfi 4Sfi130Qr 'ff>+F?Mfi p o/L'fpffQ'fiYRPfi/`$_ ffm@ ?fi 'ff>?=Kfi[t}-m{t/s$qos0pt{al qM042 0l!##` \H X 20 ti i_jukulnmompmoli z lnmompmoli_jr `#\I``lD Zi z` PX ! # #e`` | j ^ l x nPT`#\H` [ Z\}d~\ Yw[ PX "|l x TPTPP|\ i8[ x! |\ j ku[ x TPT [ fofpf [ #z [ fpfof [ |\ j r@[ x TPP 4 ``#\* 204lrO3M@Pvffx1UW[ u fifi130tfi4?KJ[4fi8fi(zaUW[u 9?p+&v04'ffff!v23p8MJ'fiPfifi^ fi fi, !<T$?3fO?Mfi/(bfiA@_afiC?A493Off'fi()fi4fid+ UW[ u N4A'fiV@E`_afi*a!!*VM]1^@`yQfiVM]_^a`R<!H o1*9fi 4p8 Mfi?pI08fi(5fi/38Z@ `_a4fi ZfiE?P7?I4 o/Rffff'fiR*Mfi\ 4*afi444Sfi@ ev !AT$p? fit45'fiC''@ffP4 pN@ `_fi Y()V\ fiKH=I !ST$?2N@ ff13@ ff7U( + 40 PE? pffI'fiG?pv fi?Pff!UT$p? Pfi4fi W()fi7?3ff?0 pff92<?9+Fp? Pfi/pffEp? ff./v p8MfidT+ ''@fffiRffa* V fiJH=I ?<fi 4 j fidW+ '@ffRff*^kI! !*6fi48Rff,?3$ff024;8fi( 4ff, 0P'fiP,4@ ffP1.V\ fiJH=I fi<?fi?$+-fi34 RN/4ff 1C `_afi Z ;ff 8?Mfi > ffKV\ fiKH=I !-fi 04p[?pP?0 pff!8{ P'N1U0324v pP Nfi(9?pAff4 Gfi/3Q'ffehl7?@K)(fiffX)(fi?p'J[?+APRffffq1Z ff7vp!T$p?(_09+Aff?A+-N?MfiC@ E`_afi 4-13v 3>?pff N'ff5A?3p+v 040ff()fi>@ _afi !Pffc R(i*g+ 3ffRfi ff ff>?Mfi ?0L+- /4ff3v p ?pPfiE v`_fi hi! !*9@ Kff0 C(bfi?3CfiE v -v 2,'ffElE!Z+c fi?p+-fiff*<3M%fi()fiFfi qY'v* V\ fiKHOI fi/`$_ ffLU1h J ^ fi fi lE!9T$p?+ N@ ffv 24} [?pL $fi(6v 44affF'fiPR0WZ4<fi130>'ff8(bfiEOffR()fi^M5?8+@ P. ff j /4ff0kY3M p @ /fi3`_fi !6T$?46R$Mfi p$1fi/()1v 3[?3$fi/3< fiE?fi(VrO 3M Lv 7-?MfidT+ PvrO 3M@ Pvffx1!9T$p? L fi4?zfi(Orgv 3M@ Pvffx4$'fiP(bfiEz?pNfi/3$r ()fi$ ()1 pI+c E4fiRvff!TJfim()fi?p fi/3>()fiO6T `_a4fi *,3Ma'3MahB 7l>(bfi|hB<l8yh UvdlF(bfi UWv GrO 3M@ Cvffx1!f{4t?4[vffOtfi/3[ fi?0 UW[ u fi *a+fp? @Mfiff$?p>()$?0$+-7@ c'vpfp+v 2ff!j HfckP3T$3? P fffi R+Aff'fiG''@ffP2v pI@ `_fi W4Lmfi Gfi 43v pKY'E fi mfi Z:fi ^ fi fi *Q?pRfi =+F?Mfi UZ4p3 m+AUfi/`$_ ffn1nv ff v pM*f(bfi ?pff ap+v 4'ff!TFp? @ ff()ff*$v@ ffF@ `_a4fi #fi/ffo/#2. Y'fiIh)@ \lp `_a4fi !{?W?3ff CE? pff* qVJ]_^@` RfffiOffUW[ fi *<?MfidW+ Zv WrO 3@ Sffv C1!msJ. +f4 *g+F?R?3ff?0 pffZVM]1^@`gmRfffiOffUW[R fi *MA?Mfi\W+ v Crgv 3M@ Ivd!,rO 3M@ 8vd8?Mfid+f9fi 0P? 3ff'fi8fi/ff3M@ ff<()9 P 4(b?0M()946?pQO[9 PrO 3M@ 8ffv >1!9Yp$_ ''@ff4v v 3Lpff'fiB UW[$ fi *03MjMfi UW[g fi *4t?OfiL?3p+&v 04'ff,?ff j /4ffkf@ ffv 044} ff'fiKy/!NT$?4f8Mfi p8(bfiQffc 4 07fiff$Rff3 I?pS@ fiQfi 0pffG+F?S?p+(.fipFfi(g'fiP[v K''Ev p!ff%d\EynPn$E\-N1mZ'\E7aP;ub-_m-ff^^B~PEP\B~ff^\9 pBo'B^EO|u)ffEBE\~\BE)P` Bfi;BAB6BEMa;B~7E^aB5OB6~E8-d^B'7 BpZm%E8B'ff8~~LB~iB'L`,B\-dBbZgLBAB<E mV~>B~iB^ Bi} lJ~\A\BEB^~6B-;\<=fimknpokmkq042N ``#\ X > ti~M !# %2a`#\`#\ X > ti~M !# %` Z2 PX a`# 4 `a`#\* 204l042a`#%[ X 2 |l x nPT [ PX`iJ !#` !x TPT PX l I\ `` Z2 PX `# ! 4 ``#\H X8 x z W[ x z V [0` |l x z 8PX l I\ ``#\`#\H X8 x{z We | i8l x{z 6%` !! Z*% PX `# 4 ``#\`* 204lrgv3M@PvffC1aUW[$fiQ`_afiUfi?Y!aUW[O fi $2,'fi33 0P()fi,ffc 4ffiR ffff*aUW[g fi $49'fi33 P(bfiAPfid+W'@fffi4@fffiffIfiR;Z+F?Mfi\3fiq4 o/@ff0 *3jE?$r 7*<v430vpGrg4'~:" fffi 0 ffc 4L! [3v pY z0fiQ'fiU041v 3V\ fiKH=I 'fiY()fi *t(<?3ffvffO% `_afi Kv fi?P5fi 043pQ?f *M?3K'fi@ _afi +-fi344'fiKfi23pI?7 !IL" ff4O?N'fi<@ `_a4fi 4N'fi33 V!>T$p? @ ()fi@ *V?3PO4['p3 8()fiN?pff v@ fffi4?P!Lr3?pfi *?3ff Pv@ ffg@ _afi m`:fi?0PP+F24-k_ &4Ffi(N?pp+ 14fi4fi fi(N?3UfiRp&v 'fi130ff& V\ fiKHOI !T$p?@ff'fi Z?pC fi?>@ C'fi33 Z k_ 0R4X3C+ fiEh)()fi8Mfi\W+ ''fffiI@ ff4fi fffiRvffEl<29?05?p@ N@ Nfi ;+-fi+-d15?0$ff024;Nfi/`k_ ff1 fi(tfi3Mff0v pLfiR'fiff*1v 043v 3V\ fiJH=I 6fi/4Ifi 4fi04!6L" ff20?Afi/0E? pN9?pff4044pYfi(9'fi ^ fi fi fiN?pPUh ^ fi fi ff lE*Jfi0OE? pPQ?3Iff':a44pUfi($'ffIfiI'fiP8?>+-fi34WRP12ff ;ff Uh ^ fi fi, lE!Yffc pff4?p7hifi/fiYfi0bl$+F42?pI ff v 3fiR'fiNfi/()ff042;Kfi(,'fiQfiQffQ14ffUR()fi@ *03MfiL(bff* ^ fi fi, !CQ3M7 fi?Q@ () oM?3 ff&hi! !*J?pm@ E(bG>83?Z'fi-@ `_fi W+-fi34l[(bfiI49'fiPI Zff7/44ffW8fi8()|^ fi fi ! vP?3ffvffOg fi4?P$R()fi @ `_fi oMY?387+AffGf?3ffF'fiO fiBE5O^OB~^~EE'90 7, m%E6\Bd^E>d^B'^\~A<5^O\E6-~~>ff^^B~> B\\\B^FEa P_M~\ F\f<,mKiB^;mV^Bf<1mYBZE;\p%B5^ff^BgB Bi}lJ~E\BE)Z%\B`B#>\~'dfBP>Bd^B' ` Bi} lt~E7\BpB~Od[\Ef`Equ)ffEBE\~\BEB^~~=Jfi[t}-m{t/s$qos0pt{al qMH204l``#\H X ti~ !c,W2`#\`#\H X tiJ c,W` Z2% PX `#`#\* 0l%2H204l `#`,iBM !B,W PX`#\ X tiJB !B,W %*a`#\`#``,iJ ,WX 2 |l x TPT [ PX `# !`#\ X 44\ f [ 4` Z2 PX a`#a`#\`* 0l%2rO3@PvdD,fi1ff03M@ffA() YM(iAfi(O?p9UW[ R%`! Z2 [`i%`fi`_afiUfi?Y!MfiU()>?3P0Lfi(A?pr ?7+-7fi/`_$ffm1GvffvpM*J?pm+F44t_k2p+ fiv'fi/3ff 1C ff7 v pM!aUW[O fi Y2Pfi0 S(bfic+4KfiffRff3U4|bafifi30vpm?pGOO?Mfi/#PVM]1^@` *- #ff30 Yffc 4K0fiRff@ G@ fffi 0 ffI #@ U?p@ ()fi@4R/fi3P'fi?3Gfi/fi nfi(>'fiPK X'' pM!Yn?pGfi?pK? VR* UW[R fi G4fMfifi0v S()fiC2FMfi\W+ ''@ff fiRvff!rMfi oM0 *$C4fiCfia U()fi fiRff?53? ff.C(bfiA?p6'YB ff]6fi/3MQfi(J80' v C8'v pM* ! M!*rO'~:;W" ff'%fi F0fiRff!ff3 | UW[R fi >Mfi ffTfi[43() +Fp? ?p[?pp+v 4J'ffN@ 8()fi@ 7fiQ()Q?p_0fi13M@*?p@ <4fi5+-d['fif. fid+W(?p6_0E'fi13@<4Vff p5p? ff. ff7()t ff7 v pM!H ?pff ffff*?04F4.Yfi(,fia pfff(bfiQrO'~:;W" ff'%fi 7fiRff$304K3M 0Ffi3Mf'fiLI K3 (i3%'*FR+ >+F44%4fid Fv 3M0 fffi C1!x1!bM@^]v $ekgi[%| e$g V H=I,^Zg]$^_h1fiH=I M@( fffi({3 o1L0@ffLfi3MN_k06;+-fiSv0@ffO6@`_fiWfiE?P*J+F?4E?R@a440fiv q(1fiH=I q( fffi *-+Fp? X?p Y4fi pYr 'fiR@ 4(b!qTFp? ff Y@ K%fid+ (i3fi4?PAv CPAfi(t?pff50042;P'fi ff3Q?pLfi0v o/pPfi(g@ `_a4fi !<Ffi\+ d*?pff$fi33 3pff9 ff4 ffAfi ?pL3fi C?F?pL ff p7fiR'fid]^)ffffiKffa44p=KfimknpokmkqH204l X 4 4 2`i k .l 8[ PX % l 4% f` !/ PX J l 4 fN % PX 4Na` Pr f ``* 0l%2rO3@PvffQ12UW[ H=I, Q@`_a4fiYfi?0K!4fi12}ff*5!!*5?C4V HOI +F?n (1fiNH=I,M fiY ( fffi 03MMfiK'2fffi h)+f?p@V H=I, P ?NRfffiOPV fi 4\lEO! R4%fiQMfifi(-?pffPvfi?PN2f?0L?pp@ff13@ (bfiEPv pGS0fi1037r 7:* MfiI n *,@ ff>fi(f+Fp? ?pI?pCfiRpW4;1R" ff'%fi @ !OTFp? N fi??v eRffv p|=VM]<K44fi@ ffC'fiI'Rff`_a[fiRp;1R^@Uo5P'fi'Rff`_aN ff 0v pIfiR'fiff!9TFp? Nfi @ ff7v K3fffi0v p?3ff > fi4?P5+APoM4Pevm?3@ (bfi P?pS`_aPfia pffL 0z<\fiN?pP042;Y'fiYk_ m4fiE!T$3? ff ;+-fiqvffONv fi?PK 4fi@ ff()fiK@ `_fi ()YfiR'fiV H=I, ![3O>?f0fiR;K ?Mfi45()fiQ#4fi$'fiCv ff v pM*a! !*% =!$HFfi\++ Ip4} 7?p' fi Ufi 0fi h (K * l5 K'fi (bfi 14V H=I h h (ff * l'lQ 9tM*%+F3? @ Ey/!<{ 7+- $'fiP E(bC?[ P!3Yfi 0-3k_ fi X4pffpff&R()fi@ K@ ff vpmfi3M fi?!&{ Y@ /fi3kp_ 3ff+F?KYOff ()fiU&:;'Z(bfiE732)'fiqm'33 mS&:;'*L03M 'fi#204()?pL fiE?P-+ L4'fipk_ pQ+F?0$5Off A()fi$:;'>()fi8347'fiPR['Ep3 Qfi(g8' fifi 4fi!:;'f(bfiE732 )46pk_ 3ffI'fiLR-'Ep3 5fi(Q' 4fifi fi |a*! !* j)J*^k(< Gfi 4Y;( >)&h)+F?04?mGR84P0 ffOffY1Yff'v3C+fp? ?p[(bfiYG'fi ^ja*^q)J! l30R#v +F?n?pm fi4? UW[ H=I,l Xh)+F?4E?fi 4'Cfi(Lp+9fi& 4P0 UffEl4fi ff()fiV H=I C+c E4[fiRff*/?Mfi\W+ v Crgv 3M@ Iffv Q1!,L" ff4? h (J * l6V HOI h h (J * l'l-jtM! UW[ H=I,l *0+f?4?Yff' j )J*^kfi12} ff5@ _afi Y'fi@ff''E4ff%fifi fi(A?pr >!9hirfie0:* Z )Z4N4a ffOffR>KffL(bfi*G)?pf? )SRff3 )G4F;1044fo/ffffS'fiRLfi 73$? )J!l f3O?p ffc 4fi;W4I ] )q #z !WTFp? 'v p v & hB<l74'$_ ffcff47fiRpY*fi()fiTffE@? 6* )m4f'p3 7fi( U'fiv 6!fTF?4$4P04 ff)J!T$?26'ffO-4,0 fffi fi3M934fi ?` ( 4,ff0v $(bfi 4/'!<cR( Mfi*@ _afi P4XMfiXpffpffV!OTFp? )p44}fffi +F42pMfi614fi4F!6T$p? (bfi@ *?pF fiE?Rv 1Xff'vpZ+Fp? ?p( +-12ff&fi q@ /fi3 _afi !cY( fi*-?3Ufi3M'034fi ffWMfiF$?3>ffa44pfi( ( !lj 3ff!^khiHffi>?V HOI MzaUW[ HOI 4A'fi33 K KfiP0 Q(bfiF+c E4QfiR ffff!<Y p4}ff4fi fi( h ( * l4t0444fi Nfiv( V H=I, h h (J * l'lg ' $'fi[(bfi !gTF?4fiR'fiV H=I ff fft+c E4fiRpY4(O 0Yfi Y4(< *0+f?4?S4f'p3 74(O 0Yfi Y4( )J!fT$p? > ff'fi U()fi[?44A?F+-L. Mfi\+X4'$_ ff$()fizfi3MFfi v _afi *0 0 ?p@ ()fi@ )S2A'p3 N()fiF4'fiPIv R2,'' pLv & hB<lE!YTFp? Pfi m%fi40 Op+'fiP7 & hB lN03Mfi7v & hB<l>@v M!Nc`( )t*?3 )2F'33 7()fiL4g'fiPNv & hB lE*+F?2?G404v ff$?Y G''v pCv=fi[t}-m{t/s$qH204l X 4 4PX` !/ JN %` !/ PXN %`* 0l%2os0pt{al qMl`rgv3M@Pvffw12aUW[PX % l4% fPX 4Na` Pr `l 4 fJNa` Pr `PX 4HOI fi`_afiUfi?Y!Bh lL4~_kff>! cffRfi?p8+9fiE*O z!YT$?p@()fi@* UW[ H=I,l 2>'fi330!{C4'fiS.Mfi\+?I4Ifi0 Rff3 ("A^a*2^ h*2^)J*<?34883'8R?pK? !cffGfi 043fi V'* aUW[ H=I *+F?2?mfi 4ffi(-?pIff' j )J*^kK4Q'fi33mfiP0!7rfiPoM4Pev*Jfi3ML20 ffOfi mfi( UW[ H=I,l I?4L(b>?p_0fiff*J?fi3p?4$40 Q'fiCfi10(b 5'fik_ Y4fi>hi 0Y?4FMzfi ffTMfif `_aCff[?pffa44Ofi0v o/p>@ ff3tfi( 3Ma fffi C1!x1z* Mfi6Mfi ffO6ff6?pA+-fi'~:;52O5fi0 oM;lE!UW[ H=I, N4$vffOVff30 >F2$fi/4} ff 'fi @ 3'$Ep? ff./v p+Fp? ?pF?pLfiRpC?fi4fi(J?3Y3+F3ffC'fi$v M*/?p5? K2'fi$v & hB lE!<rOv 2*?04A fi? fi 4pfff'fiCff3ffm(bfiV H=I *aa3MTMfi[()fiqV ffI " H=I fiV I," H=I, *ff30 EV H=I, 4$?37fi 44fi P?-UV 3 'fiP914p4}ff4fi !6L" ff40?V ffI " H=IV\ I1I, " HOI @GsJ5()fi$ffc 4 0NfiRff![PoM0v Kfiq( aUW[ HOI *-3%fi K/*-J*9*5*9 0 U@ Y'fiPff*A 0&?pK' fifi 4fi XR;+ _6TF|FvG j6T$|-ff134C/!=hi/*$V*5V*5*$V*!!! lE*$+Fp? @ S?pff444048v 2ffv /k_ 4@ R4fi Zfi(Q*-RW''v pv & hB<l8?v433ff j6T$|fvYj6T$|9P$?pN_a'5;+-fi 4ffFv U$v3P33 !9T$p? NfiRpC2$ ]! f3O?pI(i[?0[?04f'v p 4~k_ ff 8+-Q0fid ffv S?pIfi V _afi ! 3%fi V H=I,p2} ff h j6T$|Fv* j6T$|9lF()fi 'fiRhiqdl7hi! !*Q[3+44fid+A0fi7(bfi j6T$|FvdlE*%+F?04?Gf?pI''v pmhi*%V*aJ**%*!!! lY'fi & hB lE!QT$p? S?pQ? Sff'+Fp? ?3F4fi(6?pff ffO$fi( /*aV*0*a8@ P *a+ 7@ ff4CfipffY'fiff'F+fp? ?p*R|ff3 LL4A?3Lfi fp+FCpffY'fiK!T$3? Tp o/9 fiE?K@* UW[ H=I,lfi *4,(bfip44}fffi 0P(i34W" ff'%fi $fiffFhi C4Mfi? p7fi[?0 CfiO[40 fff'ElE!,st. UW[ H=I,l {* UW[ H=I,lfi 4fi144} ff9 `_afi C'fi@ff''E4ffY%fifi Yfi(g?37r 7k! [3ON?37L" fffi 0 LfiRpC2F h][bh ) cdlE*+Fp? )45?3L' F 0td45?pN ff'%fi N()fif:;'7(bfi834fi )Sd1! [30ONfiRpC ?fi4()fi[4fi-'fi ff v 3 hB 7lE!,Hffid++ cpE4} h (K * l<'fi(bfiE 0/v pV H=I, h h (J * l'l$ B'M*+fp? @ 9BCy/!,{ pffK'fiP E(bC?[ !aUW[ HOI fi ()fiR V H=I, (i34%W" ff'%fi NfiR vff545v KrO 3M@ Pvffw1![yh UW[ H=I,lfi 454'fi04`:0 Q()fiV I," H=I,V ffI " H=I, ! l8T$p? N fiE?_0'Ap? ff.1F+Fp? ?p58@ ff'%fi @ [fi32R@ff13@ ffSfi(,?38'E fifi 0fi h ( * lE! @ fffi 0 82f ff34@ ffS4(p*%()fiLQvff'Nfip'' pSv & hB,lN+F?fi $3 Wv23pffh (J * lE*6?pC@ _ofi($?27''Ev pYR()fi@ /4pYffA?pL'v V) MfiF(bfi44fid+ ffY1C ff'%fi 6d1*0 Y?3>''v 3P3/ePo (b* Mzfi ffTMfi( v430pv23pOdff4?pff! 30?Y''Ev pY4'$_ ffQ?pPfi;m(9 Rfi( Z d1!PT$?13L4(&=%$fimknpokmkqK?3NfiR; 4A'33 Nfi5'fiv ffvpKhi!!*()fi[<lE*0?pY$4A%fi0[?Fff'%fiN4@ff13@ffV!5c+U?4[3fihi!!*'G dlE*a?p8fi +AffS'fiR73M@L+88(ChB 7lF4F4(?pp+FKpffSfi 4fi C4'fi?0$?p7@ff'%fi*!!*Y d1!$cp(:Mfi*?pU?p@Ifi34YRp+''v pFv & hB l-+F?Mfi 7vp33 $43pPh (J * lA03FMfi Mfif4'() ?p>fi;!rMfio/a *-3M%fi K/*9J*-*A ## Y'fiP*5 &?pK'E fi &fi 0fi ZR;+j6T$|<C j6T$|-Ox ff134FV!-#8X hi/*aJ*V*aV*V!!! lR>''Ev pPv & hB<l5?fv043pffj6T$|< j6T$|-x7-?pQ()fi3M? _0()?C 4ff5vv38E33 !6T$p? [fiRp4]hiiclE*1 ?p@ ()fi@ BKdL! 3M%fi RV H=I, p4} ff h _6T$|,M*j6T$|9xlN(bfiR'fiXhidlE*O+Fp? U47*t+F?04?R0N?p'v p( hi/*OV*tV*t*A!!! l#'fi? d!>cp(<?3I'v p 3/ePoG(bhi/*VJ*J*VdlFzfi ffcMfiNv043p>V*?3m?p@ 4& hB lE!8T$pMfi\+ K''v pC?Lv043pfff?p' Na3MNMzfi ffcfiLv433>?pI@ ff'%fi @ !>ffc mfi?pN+9fiE*!ArOv 44*/4(0td* o/'Ep? ff.Y4FPpL'fiL3M@ )URff3@ >'fifi304KRQfi?U@ ff'%fi N U' ff!9WH +'E $?fi34 RLfffi43ff!T$3? $ fffi 06fi(a?p$v fi?'ff,?,(dffi p+&' E<@ Fv 'fi/3ff1p2}fffi *?pG?pIfiR'fi>2 j ( ffkK'fiKMfiM!7cpN43 ff'fi R( YhB 7lv W?48ff30 C4( hd1*6?pU ff'%fi [$^@UWUWVM]N@ ff13@ ffZp? @ !G+c Zfi?pI+9fiE*Rff3 C *g(bfiIR'' pYv & hB<lQ+f?MfipU$3 Rv430pffPh (J * lEj* ff?pMfi' 8fi/3M@ ffWfiL'fiS12vpt ( *gfiI' 8+-7(bfi2fid+ ffW1Y@ fffi 0 P4fiL'fi/4v3* ( *MfifI ff'%fi Nfi/3M@ ffY()$/4pL * !aUW[ HOI fi 4,fi33 Pa3M-Mfi-fi0 [()fi9(i34W" ff'%fi ffiR ff!gc9'fi$3 3 pff94<a fffiS?p7(i[?qV H=I,fi ffTMfi[v@ ff@ 7ff4044p fi(< 4ff[fiQ'fiP[14ffU()N'h!!*fi46l?p@()fi@ [@`_a4fiCR[4fi144} ffP'fiIfi h (J * lE;! UW[ H=I,fi*4MfiCfiP0 URff3@ GPff#fi3'03M j ffi4?4v 'Sfi( V H=I, kW+fp? qv q()V H=I,4( Y'fiRMfiM!#rfioM0 *-( }dG03O }d1*-?pY fi4?+F449fi3M'03 j Ffi4#?4vffi( V H=I !^8k c$APffCRf?pQN?F _( MfiI' :)Y@ ffffpff-@ fffi 0 Jd7vff'%fi72F)(!f{np?8WU[fi3'03f`_fi(443M@ *fzfi ffMfi& hB lE*%fiQ(<P*H=I,fi3M003/e$vM()fiPfi Y(bfi[r 0ff!-|<fiE$730'$RL@ ff'fi4 ffY1 @ ff ffvp fi?pff0v pIfiR'fiff!AHFfi8? j fik?fP`R@ Ffi 3Mfi4fi Y()fiUW[ H=I,fi ? U(bfif?p6TE`_afi Yv fi?Pff! j ffi4!!!^kRfi3M'03M$4Ffi 4pffK fiff!Mfi?pC4MfiYfiq( aUW[ H=I fi 4I?Pzfi ffMfiP4fi\+ p44}fffi ?C' E!tc(MJ4tpffE0 O'fi[0p+Z' t3Mv 3)p4}ff4fi *\?3Lfi ppffV'fiQfi/()UW[ H=I,fi 'fiQ4 aUW[ R +fp? N@ _afi L+f?9 UW[ H=I,fi (i44ff*\v ffNfi(Mfi3M'03M'p j ffi4?4N 'fi( V H=I !^kT$?4NPfi1_afi m4'fi _ffN?p(i4 OfiLfia ffKn* ^@Uo5Y@ ff@ ff?pfifi3A4Lff/v p8hi fffi C1!xlA+Fp? `_afi U3ff!Ma[I]vbA9* e$gMuI] K9[ qezfG [_<^#]v!<n[e$g" ff2%?$fipNfi(gfi3M52PPfi @ ffff$2A4OffCfi$ff'%fiff!<T$?04AfffiYfia@ffL?p+-fi'~:;4Ofi0 oMpSfi(-?pfi?0P!IsL3QRvG+F?G?p2Ofi0 oM;fi(ZVM]1^@` u !T$?4N4/h'h ( ThB l^l7 9hy>lffUJlf'fiU()fi?pP0fi103Qfi(-?3v41403fr [5(bfi5ffc 2NfiRfip _afi * ch'h ( hBBl^l [ [ -yh >lff 2UJl9'fi()fi ?pffi/36()fi 6T `_a4fi !6W@ qUY46?3T13>R<fi(fi *JT ChB l4<?3W30>Rfi(A'ffLv p Ifi >r z*9 4Q?p137R[fi(A'ffL m?pIfiR;Gr P*J9yh >l4Q?p'fi137Nfi(A'fiPhi832fi >4fi ElE!T$p? I@ ff'fi R(bfiL?04Qfi0 oM;@ff30,49?-?p@ f ( hBBlfi/3,'ffA()fi-+c E4F0fiR; _afi */=dfi[t}-m{t/s$qos0pt{al qMh ( TChBBl^lf fi/3A'ff5()fiL6T`_fi!<T$?pQfi3M$4fifiKfi(VM]1^@` u QEff?Mfi33?G2Jfi130f'ff!LTF?p83pf4fifimfi(VM]_^a` u 8ff[?Mfi3p?m45 9hy>ld0'fi!Hffi5?Fh ( ChBBl^(l V -yh >lff\ Kh ( hBBl^{l J V -hy>lff\@5?p5}ffOfi(a?pAfi/3r ' 4fi S(i334fi m0 ffQ032f()fiBVM]_^a` 8VM]_^a`R<*V@ ff'Rffff4E! qVJ]_^@` IMfi ffFfiRUfi1fi.13MaA(bfiLffE?Y0v '!-Kfi02'fi *fi3Mfv@ ffOV fiE? aUW[ Q()fiWpEvpP?p>fi/35r ?l((4O[fi0 oMpVPh'h ( ( hBBl^gl nUJl<fi&Ph'h ( hB'l^gl 9 nUJl-'fiIfi10(b8?p[fi/3r &()fi6+c 4-fiRpL@ `_fi 8fi6TX `_afi V*@ ff'Rffff!OT$?4t4gRff3l((?p'fi137R>fi(5@ /4 ffWfi/3>'ffI4Ch ( hB;llNfiKh ( hB;ll 8 *<fifi pL'fi 45fi 04p@ ffh)Rff3 N+ L30O9V\ fiJH=IE? pff5?3p o1$L(bfi$v p'fi ^ fi fi lE!9T$p? N4OLfiP0 oM;Cfi( aUW[ u fi 4$?pLO>F?$fi( UW[ u !H o1ffi 4p5?3L+9fiE'~:;>2OLfi0 oMpfi(g'fi9h)@ \lp E`_afi S(bf?pNfi/3?KR()fiOffV! cpY2Ph'h ( hB l^yl K 9yh >ldl (bfiqVJ]_^@` ! T$?4 4KRff3@ *Lv ?p+-fi'f@ $* Kfi130F'72$ffa > U?p@ ()fi@ 'Kv K?3>fi/3$r' fi N(i33fi Qa ,4/44ff_! [3v p> 4?p-37RVfi( j 0k8hiv N?p,*3jE?@ \l'ff5v ?p[fi/3-r >*1?pC?p[+9fi~:;N4OQfi0 oMpPfiW( VM]_^a`Rm4ZPh'h &vdlh ( ChB Bl^fil 5 5 -yh >lfflE!<T$?2949R ff3 *Mv ?p[+9fiA@ p*'v ?p[fi/3r z' 0fi W()33 0fi Wa C2>/4ffWfifi ?pC3?/:_07ffE?X *6()fiIffE?&0'*/fi v fi ?3Lpff'ffPp0?/:_0'6ffE?! dM(bfi33ff*?3$+9fiE'~:;$4$fi0 oM;fi;( UW[$ fi L 8UW[R fi L@ >?37O8F?0ffi( VM]_^a` > 0VM]_^a`R<*0@ ff'Rffff4!$T$?44Rff3 *$v q?pS+-fi'* #fi/3'444$ff0 !T$p? U ff''4fi q'fi' fi fi fi ^ fi fi, $_a'5Mfi ffLMfi$@ff03N?p j 0v Qkfi0 oMp!rOv 2*9+fi 04pP?3Y+9fiE'~:;fi0 oM;Xfi9( aUW[ HOIaUW[ H=I fi !rO*9+kp_ 3F()fi- -fifiv ff o/@ fffi t* 62<?pY30>R,fi( ffv ffOAv ^S ^-yh >l< 0^n!6rMfi,+c E4-fiR ffOfi(0?3-()fip] )J*0 ff134$ )O\v 0-+ Aff'6()fi:ff?P'fi^+Fp? ?3Z^K f) ?p-? ^K )t*Rff3 f+ o/ffN )Og)Ov pE!6T$p? UW[ H=I,l@ff13@ ffF4OPh a( N )Ol$'fi pPv 3>+Fp? ?3JU )t!7hBAp? ff.1v 3+F3? ?pJ( +A[12ff@ff13@ ff[fi 'Q4O!l [3v pm )6fi( hB;lgh)+F?4E?S?fi34YR8'p3 o/0Q3$p0}ff@ P3'ffElE*g v9yh >lE* UW[ H=I,l ff ff74OfidVM]1^@`! UW[ H=I,fi@ff13@ ff74OPh'hdMl h a88h )6 dl'l'l>'fim3Pv p+Fp? ?p|d1*< Z?p'fi+(*v p,+fpp? ?pZd$ *)J! 5 ffPh Kl 9yh >ldRff3 9>?p9p_ fifi( V H=I, {* R7y/!,T$p? @ ()fi@ *M30Pv pKh )On n d"l h'h _ qvd,l FT ( hB;l_ F lQh)+F?2?*v *t?Mfi34GRfiI'p3 oM>vG0}ff@3PffElE*V?p+-fi'~:;@ P4Ofi0v o/pSfi(UW[ H=I,fi 4$4fid+ $? Y?0$fi( qVJ]_^@`R<!BfGj[gfi[]v9[yIezfG [<bFIe !Gj]3gfi[ ek^{fi'~:;@P4OIfi0 oMpS4MfiN+Aff/NC30()30tOff3M@!LT$?p(bfi@I+3M0vffOf?p+-fi'~:;/ ff+F4? ff02A@ ff3Pfi #03&4!Q3M04PZfi @ ffSv X?3ffo/E4ON4['fiUfi0@ ?3Pv@ ffO6 fi4?PQ+F?G'fi6 `_afi V*JN+ ff46+F?ff?Gfi?pff*%()fif?37fi o1Qfi:( fi/v pff?0ff/fi4Kfi ff[r [!FT$3? >4O7@ ff13@ ff'dfiOBpB5\^9mVB\" \y 'ECL\\Q\BB5^\\EmV\B"LDy UJ^\`"0 J\\B^EB5eW0A X, 0c { ^J\~'d'fBA\ffB "mV[^\-BOB~dB~~;B^EQE0/^A%^J\J\Etn J\V5^ Rd)F~E5BEVEJ~E~^\E~=MLfimknpokmkq()fi$@`_fiY45v`_aA'fiM@ff$(J+L+AF4OffCfif@ff'%fiff*/Rff3N':`_a4fi fi13MEF()8 # ff7 v p8fiR'fifa44fi !()fi@ pffav pP?p o/E4OJ@ ff3* [30[fi 04p[?p o1R4?Mfi/Mfi`:fiX! [40fi/pf+--+$4'v # 33 fi 33 Pd['vy8+9fi./'fi V!6+c fi3M o/R4O*r [9+ @ N 0MfiPPv 044} ffV*3M @ ff-'fiIv @ ff''2fi !6T$p? [@ ff'fi ()fiA Mfipff4I??44mp025+-dX'fiWv 24} /432>v X%fi032fi (bfiP fi23Mfifi4?K!9TFp? @ >@ 7;+-fi ff''4fi 05fi Y?p7r f!5rO*0?Mfip3 ?SpP 4fiI:0 3fffi(Lr [C@ff3Mfi *F?pC? q E`_afi *$2p3 ff #?p (bfi@ pff MfiR(bfiff()fiI?pff o1R2Off*gfi3IE?Mfi4fi($0304I@ @ ff fi fi($?pKr [KhiTg0 vdl$ ff''4f?pr f['fi Rffv p3Pv 2'4! fffi *%Rff3 8?pv@ ffOg fi:?0Pf3O= fiQ'fiK ff pM*a+ @ ff''E4f?3Ir [Q'fiYfi04Y+F?S?4!LTFp? @@ $p+9fiI 0 FO?Mfi/,(bfi M(bfiEv pN?4,v ?pL o/R4O5h~vdl63@ F'0@ $r f[hi! !*+F?K KyElA 0K.pEvpp+r ff33 2a'fit `_fi Y3ff07h)+F?2?KMfi ffMfi[. fi p+f?S0 8r fElE*%fihlf3 73Ir fTpv p ffU'fif3 fiRp4()fi V!>ffc Sa434d*%3Ir fN@ ()fiffm'fiY2'(bU+c E4IfiRffO] )v @ vpIy$v fKfi43fi(g?pN' 0fi (i33fi C0v hi3?KFTt0v Pvdl-4Rff ffC+F?G'fi ^)J!f@ 8r [f 7()fiffS'fiC2'(bYrOE'~:;L" ff'%fi @ >fiRvff$+F?U' _)@ ff'%fi dQ1v pNy9v Pfi434Rff ffI+F?P C'fi ^q)t!OTF?4:ff42Pv ff' E<v 24!JHffiF?Xff?p<fi(%?pff fO?Mfi/<46N/40v 5+-d8'fi7v 42} -Lfia34fifi(r f<(bfi fi43Mfi Rff3@ F:3@ ff:ffI3ff,v P4'()1v 3[?3$fiRp!OTF?4O0R@ ff@ Qfi 4Y?pI@ ff34f+f?mpr [! [fifi #h~vffwwwlQ()fiN?pI@ ff3[+F4?S'ar [! (Mfi?p o/R4Of3ff pff44fi q+A 'fi#?Mfi\+ff3Mv n?p} Gfi(>?pr [!T$?Mfip3 ?Mfi3M9?pW o/R4O,?3@ f+ @ Q3ff'fi8RF?M@ Qfi p* ff? +F4?P?pQOIvff834fi Nfi !L|9?v 0123fi Lr ?xfiNxff! (+4 {?GxffN?p' fi 0 Lfi v Fx * fvff' ff!30Nfi(O_0Lffc 4 07 _a 7L" fffi 0 LfiRvffA+-[3 ff*0+F?4E?Y4F \F00`o[!+c 47fiRfff+ o/@ ff@ ffU1U''fi p?38 [fi(,4g'fiPB^)m()fiQfiRp] )J!T$?23/eCffP()fiC4Ffi(Lfi3MC fi?P4fi ffq()fiffc 2YfiRvff!rMfi 6T`_fi *%L" ff'%fi 7fiRffF+ o/@ ff@ ffU+F4?UCrg4'~:;L" fffi 0 7*3jE?tr ()fiQ?ppfi fi(5?pfiRp! o/04 fi Wfi($+F?m?484>pff13C(bfi8fi3Mo1R2O74Rfffid+>!grMfiUW[ H=I,lfi *')J* 07 ff'%fi Nd*4/'fiP;^aq) 0.^\BKdF+ @ $''fi@ ffV!v 30pW o1R4F+ @ 8R()fiOffS'fi 4(b ffE?mfi(<?p8fiRff!F+c Sfi?pQ+9fiE*C@ `_fi K fi?0+-5ffffK+F?>y$3`o$3 -(bfi)ffE?Yfi(V_a N+c E4fi$_a >L" ff'%fi @ NfiRff!9rMfiWKfi 3Lfi(O?pff L$3 *P@ $ 0Mfi ffU+-f3 ff()fippY?pP?@ r [!Cffid+ ff*6>2L4%fiN'fiU%fiv Qfi3M>?74< fi4?PRffv pfi0 ff +F?ff?Ufi?pFd+=?pJ^@\YNr fff!,rMfiWoM0v *v Tt0v >8+ Lfia@UW[ u h)fi\+vdlE* UW[ u fi h)fid+=lE*VM]_^a` u Ph)fi\+lE!AT$p? 4Vv Ma3MA?p>O>?M@r [!r3M?pfi@ *t?p ff7 v p fiR'fihi'Rff`_v '4fi fi(-?pPfiR'fi83? ffPl+A$?pLL(bfif4 fi?0P5Rffv pfi0 ff!k %dB %Ut' OQ\\B^E\EE;\E\5^9B\5B~$$B~B^ ^FB< BV~ E^\B^%\d;E,^gB^g~\Bg`<1w5~^~OR\~\B^EO^L-\^B^~B)[5ff\~^941~\~\[~N r=sOr ;tff%6B UtENO k tff$t9i;B~~%At~~\;B6B5^\9B~\^BWmVB[B\<\~\ % UJ~\B\E$t,i;B~BP B'[-~F B~60EF^ffBp`B^-\B~~J EgE[\\B'Ef^AB^5=Kxfi[t}-m{t/s$qos0pt{al qMQ30[fi 04p#Q?pI@ ff30f GTg0 ffNC>1!Lffc Gfi?Gfi(9?pff I0 ff* ffE?mfid+fi@ :'%fiQ'fiGWfiE?K!"Ffid+F7@137ff(bfi84>(@0!T$?p'vffcR()fi':P@ ff3ff*V'fiYRPpff4ff?Mfi!Tg0fi0@ ff>?pPR()fiPfi(A'fi< ':`_a4fi #+F?#?pS fiE?Pfi( 3M0 fffi C1!1*5+F?4E?#+ @ Spff pff&()fiV\ fiKHOI^a`y`30fi !XT$p? K30fi &3ffX(bfi?pff@ o1R4I+Afffi !&T$?M 3fihi3M @ ff['fi ?p%fid :;fi 3ff@ ff''2fi ElFr f[+ @ pff*t G?pm?pfi/3+A(bfiff!T$p? Y@ ff3P+AWfi/3Pr 2'(b/v p?3UfiRp!=[R'fiV\ fiKH=I +AF?3Sa4 ff*+f?4?Sfi 4ffUfi(< Mfih)03Mffi F'fiCC'Iv 'ffSfi(<yl?0 p'fiKE MfiPYE?Mfi m0v 'Gv U?3Ir ' 4fi U0v 8()fiNC Mfi?fi4fi(Vfi p[fi(V?p[?M@ Qfi !,rOv 44*?p[fi/3-r +--@ :B()fiOffC @ `_fi Mfi p!T$3??Mfi/Mfifi ()fipvpmTg0 >Y+-I2P44L'fiG?8(bfiITg0 1:* oM8?V H=I, +A?pY ff 0v pGfi E'fi X?pY43fi X+AP3ffX'fiRR( fffi !Xffc Xfi?p+-fi*?p$fi/3<r +A<()fiOffV* ?pV H=I, 004 ffI'fi7?p)a3 V\5 z[=]<r nfi(%?pf?M@*<?pCfi/37+-8. X+F?W?pCfiRfi;Rr )( pffpff(bfi6T `_a4fi *<?pm@ E`_afi mR()fiOff!7QE'fi9V HOIfi 4'ffmfi(9E?Mfi1fiv pK fi ff-G Mfifi ^aF()fi8+f?4? U}h K ^ ;l8 *<?Mfi1fiv pmG Mfifi ^\(bfiI+F?2?U1}h ff ^\\l$y/*a?pY 'pVU1}h ff ^\\l$ =!fi23Yv STta fffPfi>4Rff ff j ffk ff[Off *afid 9>y33 *Mfi(6?3703U4Ofi(6?p8 fi?Y!f-fi43054Rff ff j 'k >?p8'Rff3MYfi\ N'fi*%! !*a?3703U4O>fi(?pvffOfiE? v G?Qfid+ 4143ffKS?p03G4O8fi(,?pfi ff'%fi v p'fifi4?K!UrMfioM0 *O?p j 'k 'R()fiUW[ u v Wfid+v ffI7a3R4O/4pff1?pCa3W4fi( VM]_^a` u v Zfid+ !m-fi2374Rff ff j kG?Mfi\+?pd fi \30>Rfi(6 `_fi \fi$fi\ B>y33 0!-TF?4$4f4%fi$'fiPfi 'fiFRff3@ *0()fiToM0 *a?p03W4O47fi'8''fi pmfi@ ff2ffW+F4?R?p137RLfi($'ff j 12ffkY03Mv pKM(i*O? 6T `_afi C493 ff!6|< fiR;Ofi93@ ff0M(),'fi7Rj /4ff0k73M pM(i<+F34 ffG+F4?Sfpff'ff ffE?*+F?04?GPffSR71384O:;fi 30Pv pMY! f2'fiM*[2[2%fif'fiMfiN?F+-L2ffi5(bfiL `_a4fi fiA'fiPfi/3Mff!6c5+A5fi3M5fi @ ffL'fiPfi 'fi03&2OU33 33M$43P'0ffP()fifi/v pWr [!q{p? fifi Y?pX+ @?p\3-@ ff3Ifi([0/v pG ff v 3SfiR'fiff!XT$p? j Ekfi43I@ Y4v 3U()fiTg0 >8Rff3 L?3N433 ff$@ L4y/*0B! !$* MOfi fiE5fi13@ ffK3MEv p8?pc o1R2OFp3 Q'fi041v 3PV fiJH=I *%?Mfip3 ?S+ 7?d 7fi0@ fffiF'fifi/3M[+F?Y?045fiR'ficMfif03Mv p?pT o1R4!OT$p? f4.PfiR( fi,v ?pW o/R4O6@ ff3ffP()fi?pF0434<E Mfir [7?0P?RpffR'fiRpff#3v pY?3f o/R4O!|<fiI@ K30Cfifi+F?U?pI'Rff`_a9V H=I, fi Sfi;( V\ fiJH=I *[G8 Gv UTg0 >1!fHffi8?0 j BH <ifk 4fv?p j kYfi43m(bfi8 1?v pKfi?3>? RY `_afi Z fi?0ff30 j kY@ ( EN'fi`_fi fiE!T$3? fi?0Ph)fi\+FElN?Mfi34mRfi 4p@ ffR '0v ff j V*^k j a*^km j J*^kSfiIff4 8v 3 $ff j g[V!^k j 0Ik p o/,'fi7 fi?0Cfv Tg0 fLfiZ>>pMfiff-,4<>fi/3fi4?K* j /k?F$25I `_afi U fi?Y* K j 0kI?f$4A?p>3fi(t?3 j 0kj 1k 'v ff*! !*1?p$2O$()fiZOVM]<@ :B()fiPv 3N?pFfi130<P@ E(b/v pM!OrMfio/P0 *UW[$h)%lA4$fi 4p @ ffK'fiR > fi4? 04$fi 4'pfi;( UW[ u h)l-()fi4fi\-+ ffYVM]_^a`yh)lfhi@ [fid+FQv,: >7fi(tTg0 NlE!6cp_( Mfi70fi103-pff09'fiIR[(bfiff*?3?3 j 0k8 fi fi(t?pfi4? 494p4M'fi>?3 j /kL 4fi *v P+F?4E?P[?p@ f26fi Ifi p$fid+4Rff ff j g[V!^kTg0 ffFM*x1*a 0C :B0@ ff fP30 7hi03U4OLfi 0MlAfi(g?37(bfi Tt0v fffP 0>v mC(bfiEPN?N(i44ffQ'fiOPfiP04'fi 0!Lffc Sfi?3L+9fiE*Tta ffQM*Jx1*JCfi v=dfimknpokmkq>1!<ffc KTg0 ff5M*ax1* 0C1*@ ff305@j V*^k j a*^kfi j V!^ks#t3ff2fiE<fiLfip9fi@,v@ff'pF43pO()fi@,24'vpAfi3M# o1R4?1fi?3ffff!"Lff2M?<+$@ F3v pNNrO'~:;W" ff'%fi 5fiRp7r n ?,?46r np? ff.19fi 7??pL'YBff]5'E Nv \ G''v 34f(bfi2fid+ ffGYC@ ff'%fi @ !frMfiNfi3Mfi43M4fi Y0Eh)+F? p0 Nr v 044}ff4fi alO+F3? C30v p UW[ R fi */ 4(b/v pIrO'~:;W" ff'%fi QfiRp4Tff30v ['fi ()1v 3?p8()304JW" ff'%fi 8fiR;!NT$3? 7(i4 fif()fi33aUW[g fip3 8'fiKQvfia pff[@ v S()Q/fi4fi ffi(,?37(i34tL" ff'%fi 8fiRp! ( T$p? @ ()fi@()fiUW[R fi *rO'~:;W" ff'%fi $r [,@ T4@ ffIpff30F()fi,@ _afi Pfi((i34MW" ff'%fifiRvff!,ff3 7+ 83 ffK?pfi43Mfi Ca K?pff o/R4O*0 0KRff3UW[ R fi ()fi33 ?pmO30>RCfi( fiVM]_^a` g hi! !* aUW[ g fi (bfi33 0Mfi(i4fiEElE*()fiA?pfr [Av P?3ff o/R4O9ff'vpIrO'~:;W" ff'%fi $fiff<+Aff34'fiPff'pP(i34%L" ff'%fi @ [0fiRff!rMfiffi3MLo/R4O*/_0 L?%fi?pff@ ff$+ @ Nff'ffMfip+/*fi@()fiP'ffU0(bfizTt0vffFIfi3MRff 17vW[fi4?PC4fi@ffff_a4&()fiKc+4fiff()'U??Mfi()fi6T`_afi V*VRff3 I?pP4'> Op`:B03%fi hi 0?pfi/3Q fi4?P43pQ U0fi %r NlE![1FTF?pfv0@ffOafiE?P9@F(i'A??pf'fi%fiE?P,(bfi-%fi?Pfi/3-`_afi V!,T$?4A4Lo1RffffK'fiPRN'33 Nff30 L?pC+ L4fi@ ffY(bfiFv ff v pM!q>1FTF?p j HfckKEfiNfi(A?pv@ffO,vfi?PN(i'>?R?pff>fi330*+f?4?nMfi MfiC_kp+v4f'ff! T$?044f o/RffffRff3@Sfi(7?pG@ffmv'@ffP4v 0vpM!FMRaUW[ H=I [aUW[ H=I fi @Q?p[()ff'Afi(J4a?p[vfi?Pff*Rff3[?p[2fi@ff()fiQPffWp47ffv3fiE'fi8hi!!*'V H=I ?p[?V fiJH=I lE*)z`yz>?pY@84'fi2fi@ ffK()fi$fi p>ff_a[fiR;C;1R* K?pKE`_aQ_k0vpI4Rfiff![x1RaUW[ H=I $ UW[ H=I,lfi +F44/?dF?pfRff'<ff3MfiRffff!tT$?pI+F24Mfi<.Qfi@2OPNr }v@ffffff!8T$?04Q4' o/ff4fiRfiOffN()fi?p+-fi'~:;4OfiP0 oM; /4!3Ma44 23pff[+O o/Pp@?pRfiPfi(,+$fip @ff44fih)(bfi9aUW[ R fiUW[ H=I,fi *+F?04?K@ IMfi$fi0 7 fi?ElE* K?pLo/483fi0@ ffU'R ff3V!T$3? N@ ff35 L?pN()fi4fid+fv pYhi33 ff5'ffUfi?3+F4 *4fifi. $?p j ffkfi430ElE7vFTtfiXm?pSff3*5vnTg0GZfi1fi.# fid+FGvS?Mfi3p?wfiP0@ff?fi\+R?47 8+F?Rfid+(Lw1!Y+c Zfi?p8+9fiE*6fi0@ Cfi\+ v+F?Wfid+vy/*6fid+ Y+F?fid+ vv*O RfiYfi !"Ffid+Fv?Mfip3 ?ZwY@ v fi?PQ()fi>ffc 2fiRffff*JNd%B'$^0B\`^^NmV^\fiO~\ %UC^BE^'BE[B~B~ %Ut{mVB$OB^EB~HU ^'B\^<;BE^%B~[E\^^'UJ BV^'B\^\{ R ~^B~BTo'BENPaP5i;BO BEmV^p[-,mUB^E~E5' J~~F\\'5A^'B^d%ff\,PaP,BEV^5B %UmV^^g;~; '-B,~t^%^ ^^m'AgBEHUa~ p-E~;B^$Ed ^\B^EA~B^\,;E\^Qff5EiEV^'B\^<;BE^6E\^^'L U,EQB^O^O^55'\B~[`E^N m'8[B~BT o'BEINBEgB~^\B^EC\~'d';%pB`Bg~ p51 mB^p'mV^J'E\%t0 R E1\Ea`^^N m%'F$OB~\ffBJ\B\^~mV^^1O^55'd B~$B~^ 'Jfi[t}-m{t/s$qos0pt{al qM] K `6WKGhE8] K `6WKGh8UW[ u G!^yyyMvffx!^yyw!^yyyw!^yyxxVM]_^a`!^y>wQ!wxCC>!yCyC!w\ >yEWU[^!>wxx^!>!CQwQ!x/vvff>>UW[ u fi G !^yyyyC !^yyCx !^yyyC/vd !^yy>yUW[$ fi *!^yyyMvffCw!^yyCQy!^yyyxQ!^yy>yUW[$ fi!^yyy>x!^yyMvvy!^yyMvd3C!^yy >xVM]_^a` u G!^y>/vffxwv!^y!4vffw\v!^yVM]_^a`!^y Qv!^y!/vvffQx/vv!^yVM]_^a`! >yQ/vdv!^y!y Cxv!^y!^yyyw>!^yyxy!^yyMvffx/v!^yyxwUW[ u G!^y/vvy>!wQwy>!4vd3CCx!wCQCwv VM]_^a` g>!^ywQ!yy!4vffQyy!>vE UW[ RUW[ u fi G !^yyyx\ !^yyxwy !^yyMvd3QC !^yy>yl UW[ R fi k !^yywyMvv ! >\xy !^ywyQ !wxyUW[ R fi !^yywxQx !^ywyy !^ywQ !4vffyMvff>!^ywCv!^y!xQ\wCv!^yVM]_^a` u G!^yyCv!^y!4vffQ>ywv!^yVM]_^a` g>!4vff/vff>v!^y! ywyxv!^yVM]_^a` gTg0>1Wffi$R()fiP9fid>y[3$-hxffiff*Cf33_ff?alt+f?>fiR'fi;V\ fiJH=ISp7r f!$"Ffid+F8vL?fi3p?Sw@>()fiF`_afiUfi(6c+47fiRffYfid+F>vy?Mfi3p?WvffQ@L()fiW6T`_afiYfi(g"Wff'%fiNfiRvff!ff]{K `WGhzgig]{K`nKffGhgg!^yyyyyMvM!xi :px!y!^yyyyyw1! xi :,C!^yaUW[ HOIaUW[O fi!^yyyyyQ1!x/v :px!y!^yyyyy>v! iC :px!^yE!^y>xyyv!^y!y!yxyQv!^y!^yVM]_^a`yOaUW[ HOI fi !^yyyyy !>i :,Q! 3>!^yyyyyC1!^ywi :pw! 3>aUW[ fi G!^yyyyyCx1!i :pxHB<i!^yyyyyCQ !x/v :,C1HBi<aUW[_R fiwM! CCyyy!wQyww>xCw1! >> 1> !xxyyyy ! Q vffxx1> !yaUW[ R fiwM! CCyyC!wwQHB<i1> !xxyyyC ! Q /vHBi<VM]_^a` u G!4vv Qxv!^yHB<i! yw>v!^yHBi<VM]_^a`R>wC1!wxyyv!^y>xCw1! >> Q y/!^yQ yyyyv!^yvffxx1> !ywC1! C/vyxv!^yHB<iQ y/! 3Q w>v!^yHBi<8 VM]_^a`RTg0>1WffiR()fiP7fid9>yC338hxPfiff*C 330Lff?l$+f?YfiR'fiBV H=I,Sp7r f!$"Ffid+F8vL?fi3p?> @>()fiF`_afiUfi(6c+47fiRffEH8?Yfid+F$P?Mfi3p?WvyP@L()fi$@_afiKfi(O"Lff'%fi@NfiRff!fiE]{JEH?WU [ u GUW[ u fiVM]1^@` u GVM]1^@`yOUW[ fiVM]1^@`yOUW[$UW[ fiVM]1^@`ymknpokmkqGn`]{J`n?!^yyyMvffx!^yyyy C!^y >/vffxw!^y >w Q!^yyyMffv Cw!^y Q!^y >wxx!^yyy >x! >y Q/vd!^yyyw!^yyy C/vd!4vffw\!y Cy C!^yyyx Q!/vffv Qx/v!y CQw Q!^yyMvd3 C!y CxTg0NMWffiI03K4OhiKfffiElAfidB>yP330A+F? fiR'fiV\ fiJH=I K_0Lcff`:>fiRff!6T$?04A0 L4F3M042fi Cfi(OfiONfi(O?p>P2vKTta>1!WU [ u GUW[ u fi GVM]1^@` u GVM]1^@` gUW[ R fiVM]1^@` g>UW[RUW[ R fiVM]1^@` gE]{JEH?n!^yyyw>`]{J`n?!^yyMvffx/v!^yyMvd3 QC!x Q\w C!4vd3 CCx!^ywy Q!4ffv Q>yw!4ffv Qyy!^yw Q! ywyx!^yyyx\!^yw C!^y/vvy >!^yywyMvv!^yy C!^yw Q!^yywx Qx!4vff/ffv >Tg0>x1Wffi50374O[hi>fffi0Elfi\;>yF3$V+F4?LfiR'fiV\ fiJH=I >_0-"Wff'%fififf!6T$?4Aa L4$3044fi Cfi(6'fiONfi(O?pL4VvKTg0>1!EaUW[ HOI fi GaUW[ GVM]_^a` u GaUW[ HOI fiaUW[_R>VM]_^a` RaUW[ HOI fiaUW_[ RVM]_^a` R*]{KEH?n`!^yyyyyC!4vv Qx!^yyyyywM! CCyyywC1!wxyy!^yyyyywM ! CCyy CwC1 ! C/vyxff]{K8W`!^yyyyyC! yw >!^yyyyy C>1!xxyyyyQy/!^y Qyyyy!^yyyyy C>1!xxyyy CQy/! 3 Qw >Tg0C1WffiI03K4OhiYfffil-fidB>y330A+F? fiR'fiV H=I, _0>"Wff'%fififf!6T$?4Aa L4$3044fi Cfi(6'fiONfi(O?pL4VvKTg0>1!fi[t}-m{t/s$qos0pt{al qMfid+FCvyU?fi3p?nvffQU@Cfi?0PL()fi6T`_fi!YcffWTg0>1*gfid+FCv?fi3p?>@ fi?0P$()fifffc 270fiR ff*0 0Ufi\+FQxP?Mfip3 ?ZvyC@ fi?0P$()fi6T_afi !R-fi0 fi\+zS+F4?*< >U+F?qvy/!#hi"Ffid+FKvC ZG3fi8RfiP0@ ffXRff3 fi\+zm?0 v fi?4fi ffZ()fiPL" fffi 0 0fiRff! lHffi?0[?3ff 8fiP04'fi 0f@ IR;+m j @[ak G j V!^kvj@[akOff 0 j /kCfij J*^kS?04L4LSfi@ ff8fi04fi ! 9*aff gMaJK8_e %_]{ [ G ezJ!<fi M_0OffC()fi-4a@ ff309 Tg0 B>1!9Y?pQfi?p^_e Le!G J@!<.ek^.jg3h-b cA49?0 *M?pN@ ff3$@ L`ff ()fifTg0 >1![1FTF?pffff'+Aff'fiRfi0@K4I'fi oMPv3 Tg0ff8M*x1*-C1!c+?pffY@ff?pfiP04'fi 4-R;+K?p[_0E'9p+9fifid+F52ffv ff j 0kKh)fi j /k8fi j 0kl9 30A?pQ?fid+zfi(F?0IOK4Rff!T$p? @ ff'fi ()fiPP.1 pS?3ff Kfi04fi I48?I?pC_0'p+9fifid+F9fi(tI4Rff0fi ff'%fi 'fiI v@ fffi?0 h oM5()fi9fi\+8fi(Tg0 ff[ mxlF 0U?p8?EYfi\+fi(9m4Rfftfi@ ff'%fi F'fiK'fi6 fiE?K![0 ff*/fi pNfi34foMPv pNTg0 ff58>1!6+c Tg0 L1*/fid+FNv[?Mfip3? CCh)fi?3?0 lO KvyN?fip3 ?KvffxPh)fi?p,? YvvdlO@ fv@ ffOfi4?P* Ifid+F,1*vv*?Mfip3 ?w1*% 0ffv C?Mfip3 ?Xffv Q 8'fiO `_afi fiE?P!QT$p? Ifi4fiP04'fi 0- [Rp+- fi\+FQvQ Y*1 Y*x8Q1* >7 0 w1v* C7 Kw1*VvyI mffv C1*ffv >I 0Gffv C1*Vv mvd*Vvff8 mffv Q1* mvffxI mffv Q1!<ffc Tg0 9>1*/fi\+FNv*1* C8?fip3 ?L@ Fv 0@ ffO0 fi4?P* 0Pfi\+F>>QN?fip3 ?Yvy7@ F'fi!<TFp? Ffi4fiP04'fi 0L@ PR;+Rfid+FPv>1*gY>1*JS #vy/*6xQ1* CK Rw1*O Z0Wvy/! ?gM@KKek^.gM Eb T$3? L'4'4_aNfi(6?p>fiP04'fi 0$vTg0 fffP>+ @ >ff'ff! d[v 3P \ oMF{44fiffo/fi YE M.:;3ff'*a2Vfi02'fiff P'fiR?%fi?pff4[m &Tt0v Um@ Y'4'24Z `_aYhb) yMvY *-vPfi'Fff* )cny yyyMvdlE!5+c YTg0 >1*?Mfid+ ff*?p>@ 0ff5R;+ UW[R fi LqVJ]_^@`RWh)%fi?I?pLh)Mlg Yh)%lV Efi Elt@ LfiO4'4447 0`_ag6?p:)cy yMvvffX! [4%fi?pFfia4'fi Fv KTta >@ Lv `_a$$?pN)cy yMv> ff!q>1FTF?4?%fi?pff2PMfiffMfiaZ'fiW?pUfi4?P()fi@ :B()fiPpm?pYfi/3Prff3@ *ffi114fi3*F4C+f44$ ff34@ Sfi G2ORm'fi K?pp+v 4f'ffK()fiK?p{ +F4?S'fiKff'N?p|V\dM ^@`y`<4Iff/v pQfi(,?3 j Hfc~k Efi *'fi+j Hfc~k fi ff!Nfi 0'Kfi Z?pCfid+f74Rff ff j V!^k TFp? @ ffv 8fiP04'fi 0>@ Cfid+ 3QRTtaRfid+zC 3.QKv Tg0 x1!m'h f4 ff*gfi pfi304Rfi0 fi\+ >E3C1* Kfi\+vffP 3Lvffxv YTg0 >1! l|,E?Ufi(O?pff 7fi04fi $45Rp+-G3 0-Efi fi(J?3Qv fi? ?$4-?pNLA?p j HFc~kj Hfc~kI fi Kfi3Efi o/A?A-Mzfi ff?Mfi9_kp+v 40'ff!,Tg0 ff>> 0C7@ YMfi9ffff3@ 8?3Ufi S?ff P?3 j Hfc~k Efi Q03MfiQ?pff4Qfi$R3 a!Ch){ fi 0Sd+?3pffm'fiYP. fi pfi0E4'fi mR;+R4 j Hfck fi Q m?pffNfi33 0*+f?4?G4f@ $b$ffffmv GTta 81! l yLgMaKKek^.gM < Gev%a[ b f()[ffvp?3>'4'2V _a*545()fi33 ?0$?pN@ ff30$@ Lv `_a>hb)cqy yMvdlE!FMFTtfiNpPp-fQ@ff13@fftfi4pEvpfTg0>f03MjMfi6Tt0vA1!6T$?2J4gRff3-+-Afi43ff'fi>fi0F fi?0PO(bfi,+F?4E|? V HOI ?6RP02 ff!g-fiP0@ 5fi\+vA 30,1*v9 30>1*f 36* L[ 309vyf'fi[@ 9?p-@ ff30_! [4@ ff3t?Mfi\"+ aUW[ HOI Fh)fi\+vdl6 PUW[ H=I,fi h)fi\+#lg'fiLR5,ff'96()<<?p5fi?p9 fi?! 9*zgMae$gMUW[ H=I,l Lh)fi\+vdl6 3UW[$ fi Lh)fid+lE*1?p@[ eek^.jg3h-b ffc P40ff,fi?3,?Jfimknpokmkq2$Mfi4ffaL'Rff3MV!6cffKfi'fff*0?3>'Rff3MK4$13LP4!X[4Mfi2ff0ff03M05@ L'2'44 0`_aNhb)qy yyyMvdlE![x1FTtfi&ff'YQx1*[fiP0@m?pm_0 j 'kfi43h)()fiUx\:;'r fEl+F4??3m@fffifi23G+F?S?4Q4Rff9h)()fiNx\:;'r [ElE!fi@pff40>ff3M?Mfi\+FN fid+4p3 7(bfi j 'kP[?p8} 7fi(6?p8r@ffff!Fcp[404vffA?f?374fifi(6?pI032Ofi(F?pv 0@ ffO9 fiE?'fiG?3C03Z4Ofi($?pC'fiA fi?pff@ ff@ ffPfi@ h)fiFv 0@ ff ff$ ffEl-5?pLr } Lv 0@ ff ff!-cpN?Mfi34 P. >?4A;+-fi:;fi43fiP04'fi 7()figfid+FAv9?Mfip3? CIh)03M_fiOlt Cvy[?Mfip3 ?CvffxIh)03M_fiAvvdlJfi(0Tg0 A1*0mfid+FvI R1*t GK?Mfip3 ?Rfi(9Tg0 P>Rff3 I?pff P I2t?3Pv@ ffOv fi?Pff!Ah){ [Mfi ]<@ F%fi3M6?3$'fi0 fiE?PORff3 j %0k[2*18p_ fi *4+-d1Pv!^y ()fiL?pffY! l ( &c(,fi pPfi 4p[?pI@ ff30[fi(- fiE?PQRffv 3Cv G%fi?fffh ! M!{* UW[ fi ?fid+F<0` ,ff3MPfiff<v ?3$;+-fi>0v ff*03<+-Tpff'fiNfi 4pO%fi?7 Ofi(a@ ff34ElE*?pIff9aUW[ H=I fh)fid+vdltUW[ H=I,lfi h)fid+&ltvTg0 >Q?fid+#?pARff'<v ff3MIfi(41?p$@ ffOfi?0P! [ ek^.jg3zh-b c250@ A(bfi?3 j @ ffkPfi4309?05?pQ4ONfi0 oMpfi(g?pff Q;+-fi fi4?Pzfi ff?Mfi5vff Ih)fi?35? Pv Mfi2ba33fi 0El<5r =} Qv@ ff ffNhi NTg0 B>lE!fi3M0fi(f3Ma44G43pff8@ffid+ Mffff!mrMfifip*<ff4-?| UW[ g fiUW[ H=I,fi @ cMfi5fi0 !<TFp? @ ()fi@ *M54,ffA'fiPfi439?pQRfiNfi(tv0fi@ff@ff04fi Qhi! !*M(i4 Yfil6?p Pp;! aUW[g fi [PpcMfi p!<rMfiF?pQ@ ff3Av Tg0 B>1*>>fiff fi( UW[ H=I,lfi ]^A@ ff2fi --+ N+$fi pYhi! !*M(i4 YfiEEl,(bfi$?3Q} >xIr fff*/ 0Yx ff+@ L+$fi 3(bfi$?3>} Nxr [!4rOv 2*-fi 04p?pSPoM483 fi0@ U'Rff3MJ! aUW[ HOI fi ?Mfid+FC ( O: ff2)`)v.fffi\VM]1^@`gfi 7} <xfr fi0v ffP,hiff Efi ffIfid>yF$3 E=l JT$?04V4 /4fft73E?fi(%?pFfi7 %fi3M;aUW[ HOI fi ]^,()2 )fi9!<rMfio/a* ?p$a4I@ `_fi4Ofi( UW[ H=I,lfi *g fifi34W3 >'fiS@ E(bmU4fi pU ff1p3fi($ ff v 3KfiE'fi34v vpfv >fi 39?62~$_ ffg?p-fiR;7v >fi 4p04L fft4O-? It. ffqVJ]_^@`R'fiP@ E(bCfi p>v ff v p8fiR'fiff!{fi 043p?2[@ fffi 1G30PP}ff pM*v Tta *J?p(i'ff'>v fi? h)0@ fffifi3MP ff3El>()fiXfiR'fiff*A30fi *9 0&fiRpZpR!#ffc #Tg0 S*AP4P3Off?Grg4'~:;L" fffi 0 r 273@ ffW()fiO6T E`_afi fi(FW" ff'%fi 0fiRff![R'fiV fi 4 vfi 4ffiP'ffG()fi ?4f0 8R ff3 I[2T MfiNffNQ?4f4O8+Fp? ?3[[+-fi34R()>'fiUaU'fi9@ `_fi fiLR()fi830v I0044fi 0ffi(A?pv@ ffOfi4?h)fi p7(bfiTff?S4P4 QfiE'fif024fi alE! fffi Qfi 4pE$ U7'fi43fi R7()3M3@ P+-fi.a! ffc WTg0 C* j Hffi pffkGOfffiU@ _afi W2L@ ff13@ ffV*J! !*O?pff0v p>fiR'fi54-8fiE33 ff 'fiIR[ Gsm(bfi-?4A43fi C 0fiR;2!G k$U#fi%T$?pG?CRn ff 3ffffi(>@ffKffff?fifi/pff[?pff.1pM*f fifi/pff3? ff./v pCfi(,4''E03MffK'/'ffPhiffi}ffP3V*gvffww/vdlE!NHT?pffffff*a?p84fY44'7vU?p4E3M@ %fi3M>fi/pff63? ff./v pUa4 ffG'fiU'/'ffPN?0>? 3!T5+-fi Mfi0v o/4fi@ ?pP@ ff ffE?Zfi( fi.fi4'.1m 0 fi.Wh~vffwwlLfi Wv 0@ ffO<@ _afi W 0?>fi(ffzub:S)TO-`g,BE;EE^EEBBQ\B^VJdgB-B\Oi;B^iB^'/BB^Q^[B\fB^5fi[t}-m{t/s$qfiNH=I,J Kfi0Kc+4V fiJH=IaUW[$ fiV I,V3UVV ffIV3UVV fi 4aUW[$ fiV HOIUW[ =H QfiRaUW[O fiV I,ffIV3UVV I,vfiV3UVV fi 4 H=IUW[$ fiVUW[$ fiV I," fi 4UW[$ fiV ffI " fiUW[$ fiV I," H=I,3V UVV ffI " H=I,3V UVV 4fi3V UVos0pt{al qMfiH=I fffiK"Wff'%fiaUW[_R fiVMUVVMUVaUW_[ R fiUW[ H=I,fiVMVUVMVUaUW_[ R fiaUW_[ R fiaUW_[ R fiaUW_[ R fiUW[ H=I,fiUW[ H=I, fiaUW_[ R fi'2fffi3Kcff4UW[$ fiV3UVV3UVUW[$ fiUW[$ fiV3UVV3UVUW[$ fiUW[$ fiUW[$ fiUW[$ fiUW[ fiUW[$ fiUW[$ fiKfi0K"Lff'%fiUW[g fiV3UVV3UVUW[ g fiUW[ g fiV3UVV3UVUW[ g fiUW[ g fiUW[ g fiUW[ g fiUW[ g fiUW[ g fiUW[ g fiTg079sffpfiR'fi5+F4? ?pL(i'ff'$@E`_afiY?Mfi1V!.IB!qh~vffwwlE!Z-fi?Xfi(F?pff@CaE7@K%fi3M8@`_fifi(f'fi();+A@Y()P3fff?pQ?m08fi!>HT?pffvff[?p7+-fi.G4Fff4ff! fi.fi4'.1U fi4.3 ?3Pfi10n%:;43430Q'fi\ o10@ ffQffc 4sJ 3ffNfiRvff!T$p? G@ ff LvffOfi >fi(Ffi/pffp? ff. g?0gMfi ffVafi1.:B1:B0fi/. fi0fi03Mfi 0Jfi(/_ff%fiv *\E?pO? 6TZfiOfi;:;'Rff`_a<fi/pff1p? ff.1 pNt+ AMfiM!gTFp? -v ff v pFfiE'fi30Off>1L?pff4g fiE?@ -ff3-pffv fi = <0fi Jfi 7f@ @ ff@ fi I2P44t'fi[r [4 ffXs/T h)0383304. Cfi3M7344fi I+-fi.*,?p3OKmv 3 Cs/T lE!OTFp? +-fi'~:;@4Ofi0v o/p#fi(L?pff4 v fi? 4?pO? fi(L'fiQ@ `_fi *f4?Mfip3 ??pff ffP04[@ ff3 @ afi1fi1V!HffiR?K+W?d WX4fi[@ ff3C()fi\ffpW3ff fi !ffid+ 9+ $Mcfi Mfi<?d F PvffOMv fi?'Rff`_4N4fi ffI(bfi:ffpffi Kh)()fi830v Ifi 6Tfi70fiR;:;'Rff`_afi/pff<Ep? ff./v p=l ?,?3>?4LffRY()34'()3fffi #()fi(i3M3M@ U@ ff ffE?! .C!]^CfiE?fi 4'fi(Lfi pW30 U 'fir [*6?p pvpm Zff'vpG(i33fi 7?IPX(bfi?pC%fi'~:Q'fiS?pC@ ffv ff v pr # 7fiRp!tc(?pApff4@ ffL(i33fi 88R,()fi33 *?p>aLF?3fi@ ff(bfi AL3?h~vffwwlE*6+F?2? 3E ff7?L?pC ff pK4 j ( !^k [?Mfi33? MfiUfia o/4;m@ ff34L@fi\12pff*?pp :; 0/:Bff'5fiE??09?pP3ffR$Rff,'fi7RFfia3Mfi 44o/0 !ffc qfi '' 'fi .f!*5+ m?d Sfifi(i 0 ff044L/4p0U?fi3M?Mfi/@ ev PV*9v X'fiOKffff*9??3@ K30'44WPfi@ e?'fiV `_afi Y(bfi?V!T$3? @ L4$4fiP@ ff4ffU@ ff ffE?Uv K?3N$_ ff4Kfi(64402 3v pM!Offc K0234ff*M{ ff4S|<'}ff4fiFh~vffwwl7?ffCYO?Mfi/R'fiGv@ff44Uff'8Wfi]^7a4'fiUpff2pP+F?p?3L'fip+fi 0>'fiS?p02 ! ffi 0>@ CpffRfi 0m+Fp? R?3ffRff8fi Mfi71fi2v U;1RLfi(O+c E4NfiRp!9T$3? ff$O?Mfi/U?F'fiO>2P444ff-+F? fi3MqUW[ H=I,lfi4?K!Icp`R@4[?Nfi3MLO?fi14Q()fiN@ ff?pN? mfi @ ffa4 !$fimknpokmkqMfi?pI4L?8fi3M8`_a4fiWO?Mfi/4 o/@ff@ffZ3vpK?pC(bfiEP<()fi334fi>vW?pfi/pff?3ff./vp443M@!fQ?dIm?fid+WS'fi ff4@ff4fiAfi(V@ffLfi5040=<''ffLhiA3M.1?E*vffww>@?zAL0M}ff/*Vvffwwx@?5OffS.fid/?*tvffwwC@?0rMfiff*VvffwwClE!-r 0409?dQ 3@ ff%fi?(bfi5834fi 5fiR`:fifi1fiv4fi!6rMfiL oM0*MrMfiff]^7h~vffwwClAfi: fi/v pPr fi 5()fi$fiROS04ff/vp+@Ufi3ffq%fid! 4P42;Z+F?Xfi3M+9fi.X2?rMfi ff[30Offfi]0025@c o/@ffffU 6:;3M'fiP/!-WH ?pff ff*arMfi ff_3 f43 ff- `_fifi(<?pff@702!QQfi4PGG"$fip? ffv Xh~vffwwl[ ff [?Mfi/U()fiN7344fi [fifiE`:fiS?N30Offfr 04![S300 Lfi Lfi1fiR71Y./v pfi ['fi ()dfi0A?pff?14fi!6T$p? QfifiRfi ''EC4-4a ffOff1804 CpfffiR-+F?MfiP 134 ff4?pmr [!T$3? U@ ff44fi ?q'fiZ?pG+-fi.#p? @ m4P?0C?3&@ ff Cr' (bfifi 8?O3@ C832fifi1fiv 4fi !RsJ. +f4 *<v fi3MI@ ff ffE?*-Gv ff /:v pCfiR'fiN?N4QCfiE#3E ff j ( ffkK()fiN'fiO7304fi Qfi1fi fi GfiRp' (bfi7?3 r +F?4 3MEv pSfi1fi fi! [?Mfip3 ?%fi??pffIO?Mfi/& fi3M3>?045fifiEv fi V*1?3ffF'fi43fi K45P 13+F3? @ ff5fi3MF4?4@ ff 3M'fiPff![QO4fipffmvU?38v'fi/3fiUfi(<?4f0Rff*%rfi-fi(0?p5fi@ -@ ff6ff ff?Ifi Ifi <fifiEv fi 802 ffV()fiP1 `_fi IO?/:fi/!LrMfi oM0v*%s#PmDQ3M(Kh~vffwwlNfi13fft?3ffNfi]V@ffP2Q+F?mC(bfi44444-'fiDj'p>h)E?pF? Ur fElE!<TFp? E(b '?fi }fffi Rh)ffc 2\l9fiR':ff*+F?2?Y0@ Npff0fi1.a*%3 pfi/pffJEp? ff./v pM!Qr3M?pfi@ *%SDQ3M( 833 ff'@fffi\ ()fi()4v ffP `_fi 30v pLp+9fiO?fi1ffgfi0Avff v pM* 8O?Mfi/C fi:fi3-'fiI?A3 ff"fPpN {fi ?h~vffw QwlE!--3.1?0Sh~vffww >lA 0ALa M}ffCh~vffwwxl30OFfi -a4 <@ f@ @ ff@ ffCV6:;3'fiP/*M P?pP0M@ ff94p3 ff,fi(fi13ff03? ff.:v pXff%fiQfi 2S0fiRffCfi(7?3 @ fiv Rhi7344fi El 04 !T$?30?3@ R4 fid+fv p@ ffff37(bfi8M@ ffv pY834fi 7fi1fiv 0fi 1o/@ ffv pK02 L6:;3M'fiPm()/v pP?pff +F4?Sfi/pfftp? ff.1v 3M!LN3MF+-fi.Y030445fi S?4f@ ffff3* G4'ffi o10*VRff3@ Mfi pfi(-?4Q@ /fi3[@ ff@ ff?ZM@ ff@ ffYe> `_fi m(bfi7fi >?ffV!r v2*6?p@Y@Y4YO?fi18()fiPfi''vpG?pKRff?ff/fiIfi(Qfi*-+F?2?@[fi0ffOI'fi>`_afi@ff(2:B0ff!grMfi? o/a* ?fi? T#33?Mfi'}h~vffwwxlgpffv 7fi O?OfiRL'fi124d+Ffi* ! M!*( ;8fi fi ff*N ff''4v3F?pAfi ff]fi ff!<WH ?3ff ff*1?pFa4 pff p9ffMfi9RF0 f'fi7 40Q3v p9204d+Fv'fiK?pfi 7R(bfi ff? R* ff'Rff444U(,?3Pfi 7?ff 'fiU0!Yp'fi43M4fi G2Q'fiY34d+F-?54fid+PoM483ib$ o/4044pYhirg4'fi3%#33?Mfi'}*vffww QlE!9ffid+ $?29'fi23Mfifi ffTfi[44fid+(bfiNv G?0 pffNv Y?3702 *a30?Gf?380fi KfiNpff 4fi Ufi(<fi 0!KRff4v pI 4 L+9fi304CQ'fifi30 Nv 2$p pv pIfi(O'fi/44ff+fA+F?f e@ _afi Y()f ff7 v pM!?Mfi/(bfi 3Mv pZ0?124&%fi33 pffRff?ff/fi fi(Ifi Y2 j `_a4[0?/4kh RffK [fifi*7vffwwwlE!{?`_a4[0?/4*F832fi Rff?ff/fiK4C@ff''E4ff`_4V()fiff[R;+S?38fi!NHW?pffff*%+F?3fi3$pI3$20ff30P'ff*`_40?12pffJ'fiFR<fi0 ffff>+F?N@ E`_afi 7 j ' pk()fiF ff(b:B@ 0>hBQfiMfi FB!*JvffwwwlE!\fi[2):t}-m{t/s$qos0pt{al qM!#"$&%')( *+*,')-/.(10fi230%465879%:;0=<>58(% :;?A@B58C284D58(E@F0@HG28?A:;53*I4JKL0D.?(CB23'M:;')?2ONP<Q0:/28C6C10fiRS?58(B7T?(C658(UN*,'V:;?-W*X530fi(B7><>58*X.Y@F'V0@H23'ZB?A%')(*X7&RG7,*\[]'^?CB?A@*X?A[B23'Z@B:X')C58-W*X?A[B28'Z_`o5a:;?A@B58CB234:X')7X@F0fi(B7X53b'J $c(?A@@:;0fi?-;.D*,0a:X')7,0fi23bU58(1%d*X.1')7X'^@]0*,')( *X5e?28234a-W0fi(1fH58-W*X58(1%g:;')hG58:X')Rg')(*X7\587i@:;')7,')( *,')Cj.1'V:X'JQkl(j7XGRdNRS?A:X4Z<Q'&.?)b'c7X.10=<&(d*X.?A* -W'V:X*X?58(aRS?-;.B58(1'\23')?A:;(B58(1%90@]'V:;?A*,0:;7 ?A:;'\?T@:;530:/5]mn<&53*X.S(10T:;GB(UNP*X58Rg':X'Vb'V:;53oH-V?A*X530fi(Hp 7X?Aqn'>*,0d@]'V:Xqr0:/R JLks(a0*X.1'V:t<Q0:;CB7VZ<&.1')(-W'V:X*X?5e(uC')7X53:;?A[B28'i@:X0@]'V:X*X53')7Q.10fi28Ca@:;530:*,0+23')?A:;(B58(1%1ZA*X.1'V4d?A:X'i%fiG?A:;?(*,'V')Cg*,0+.10fi28Cd@]0fi7,*sNv28')?A:;(58(1%1JwK\.1't@:X0@]'V:X*I4^-V28?7X7X')7-W0fi(7X58C'V:X')Cd.1'V:X'?A:X'ckl(bA?A:;58?(-W'c?(Cax>')7,@]0fi(7,'Jy')?A:;(58(1%^0@F'V:/?A*,0:;7tz ;{I|}{v~8{=ZUzr { Z1\z X{l|{I~8{ e { /ZU?(BCa\z X{l|{I~8{ B ~3A l<'V:X'aqn0fiG(C*,0@:X')7,'V:Xb'a@:X0@]'V:X*X53')7M58(')53*X.1'V:0qi*X.1')7,'Y-V28?7X7,')7VJ10:fi X {v~ ?(BCfi | FZ<>.1'V:;'>*X.1'V:X'9587Q?M7X5e(1%fi23'MmRG23*X5rpl?A%')(*iU$@B28?(Z\z X{I|}{v~8{s {IBZUz e { v {I ?(Cz ~ 4 <'V:X'&qn0fiG(C*,0@:X')7X'V:Xb'Tkl(bA?A:;58?(-W'T@:;0@F'V:;*X53')7VJ$&2e2H0q*X.1'+?d@B:;530:;5F:X')7XGB23*X7t?A:X'^58(C'V@F')(BC1')( *0qw*X.1'+7X53V'T0qw*X.1'U$?(CD?A:X'+*X.'V:X'Vqr0:;'^?A@@B285e-V?A[B23'c*,0?( 4 U$*X.?A*>.?7\[F'V')(Rg0C')2-;.1')-;')C0:;58%fi58(?28234J't*X.1')(dC587;-VG7X7,')C^*,:;?(7,qn0:;RS?A*X530fi(B70qB28')?A:;(58(1%>0@F'V:/?A*,0:;7?(C^*X.')53:w-W0:;:X')7,@]0fi(C58(%>?>@B:;530:;5:X')7XGB23*X7*,09?\@:X0UCG-W*@H28?(JLK>.587?CC:X')7X7,')7] =|~ | ZA<>.1'V:X' RG23*X58@B23'?A%')( *X7w')?-/..B?)b'*X.1')53:0=<&(@B28?(j[BG1*c*X.1'gRMG28*X58?A%')( *9@B2e?(jRG7,*c[]'M:X'WNPqn0:;Rg')C6?(BCj:;'Vb'V:;5OoB')Cj*,0DC1'V*,'V:;RS5e(1'M<&.1'V*X.1'V:RG23*X58?A%')(*T@:X0@]'V:X*X53')7T?A:;'g@:X')7,'V:Xb')CJgkI*T<t?7+C587;-W0=b'V:X')C*X.?A*T0fi(234zAX{I|}{v~8{=Zze {I ZzAX{l|{I~8{l e {IZ?(CDzAX{l|{I~8{; ~33la@B:X')7,'V:Xb'T*X.1')58:\?g@:;530:/5H:;')7XG23*X7tqn0:>*X.5e7i7X53*XG?A*X530fi(J58(?2e234Z)<'@:X')7,')(*,')C+(10b')2 58(B-W:X')Rg')(*X?2A:X'Vb'V:;5OoH-V?A*X580fi(+?23%0:;53*X.BRS7]qr0:L?282fi-V?7,')7L58(9<>.58-/.9*X.1'?i@:/530:;5:X')7XG28*X7?A:X' (1'V%fi?A*X53b'Jkl*<?7L7X.10<>(+58(9[]0*X.T*X.1'V0:;'V*X58-V?2 ?(C+')Rg@B53:;5e-V?2-W0fiRS@B?A:;587,0fi(B7]*X.?A**X.1')7,'?23%0:;53*X.BRS7\-V?(E7XG1[B7,*X?(*X58?28284a58Rg@B:X0=b'+*X.'^*X58Rg'M-W0fiRg@B23'W153*v4Y0q:X'V b'V:;53oH-V?A*X530fi(D0b'V:9*,0*X?2:X'Vb'V:;53oH-V?A*X530fi(Eqr:;0fiR7X-W:;?A*X-;.J^ Rg@H53:;58-V?2:X')7XG23*X797X.0=<')C6?7TRMG-/.6?79? NP[B582828580fi(UNPqr0fi2eCD7,@]'V')CG1@JK\.1')7X'>?A:X'&5e(53*X58?2U:X')7;G23*X7VZfi[HG1* -W0fi(*X58(G')CS:X')7,')?A:/-;.Y?230fi(1%+*X.')7,'&285e(1')7<>582e212853')234[]'>?A@@B285e-V?A[B23't*,0?a<>58C1'+:/?(1%'M0qQ58Rg@]0:X*X?(*>@:X0[H23')RS7VZB5e(-V28GC5e(1%g?bA?A:;53'V*I4D0q ?A%')(*9C0fiRS?58(7c?7><')282w?79Rg0:X'%')(1'V:;?27,0qr*I<?A:;'M?A@@B2e58-V?A*X530fi(7VJ.1')(23')?A:;(5e(1%587a:X')hG53:X')CZt<'67XG1%%')7,*Y*X.?A*u*X.1'?@:/530:;5\:;')7XG23*X7u7X.10fiG2eC[]'E-W0fi(7;G23*,')CoB:;7X*VJMklqQ(10 @]0fi7X58*X53b'd:X')7XG23*X7gm5J'J3Z*X.1'S23')?A:;(5e(1%a0@]'V:;?A*,0:M587T?(UEywp>'W1587,*VZ*X.1')(58(-W:X')Rg')(*X?2:X'Vb'V:;53oH-V?A*X530fi( @:;0-W'V')C7)JKL0 *,')7,*+0fiG1:T0b'V:;?282qn:;?Rg'V<0:XFZ<'S.?)b'5eRg@B23')Rg')(*,')CE*X.1'g:X0b'V:;7T'W1?Rg@B23'g0qQ*X.5879@B?A@]'V:?7^-W0ANP'Vb0fi28b58(%?A%')( *X7?7X7XGBRS58(1%fiFQ=|~ | Zw5J'J3ZLRG23*X58@B23'd?A%')( *X7M')?-;.<>53*X.653*X7+0=<>(@B28?(J4uGB7X58(1%*X.1'+?d@:;580:;5B:;')7XG23*X7t?(C 58(B-W:X')Rg')(*X?2?23%0:;53*X.R7VZU<Q'+?-/.53'Vb')CD7X58%fi(5OoH-V?(*i7,@]'V')CG1@B7VJ'j.B?)b'j?2e7,0C1'Vb')230@]')C?Rg0:X'7X0@B.587,*X5e-V?A*,')C?A@@B285e-V?A*X530fi(*X.?A*aGB7,')7S:X'Vb'V:/5OoH-V?A*X530fi(CBG1:;58(1%'Vb0fi28G1*X580fi(JLKi<0&?A%')(*X7-W0fiRg@]'V*,'5e(+?\[]0fi?A:;CT%fi?RS'Zfi?(C+0fi(1'Q0q1*X.1' ?A%')(*X7w'Vb0fi23b')753*X77,*,:/?A*,'V%4+*,058Rg@B:X0=b'>58*VJK>.1'i'V4S23')7X7,0fi(S*X.?A*Q.?7 []'V')(S23')?A:;(1')CSqr:X0fiR*X.5875eRg@B23')Rg')(*X?A*X530fi(S5e7*X.?A*Q?28*X.10fiG1%fi.*X.1'9*v4@]')7i0qLU$c7i?(C 23')?A:/(58(1%0@]'V:;?A*,0:;7i?A:X'+7X2e53%fi. *X284SC5O]'V:X')( *tqn:X0fiR*X.10fi7,'T@B:X')7,')(*,')C 58(u*X.587@B?A@]'V:)Z1?(Cu*X.1'9@:X0@]'V:X*I4a587thG53*,'9C5O]'V:X')(*Tm58*t5e7?g-/.1')-Xuqn0:\?g-W'V:X*X?58(Y*v4@]'90qL-W4U-V2858-c[]').?)bU530:0fi(E*X.1'^[]0fi?A:;CFp/ZF58(B53*X58?2'WU@]'V:;53')(-W')7c7X.10<*X.B?A*&*X.1'dRg'V*X.10UC10fi230%4j?(C[H?7X58-+:;')7XG23*X7&.1'V:X'd-W0fiG28C@]0*,')( *X5e?28234[]'T')?7X5e234'WU*,')(C1')C*,0?gbA?A:;53'V*v4u0qRMGB23*X58?A%')(*\?A@@B2858-V?A*X580fi(7VJG*XG1:X'E<Q0:X<>58282iqn0U-VG7@B:;58RS?A:;5e2340fi('WU*,')(C58(1%*X.1'?@:/530:;5\:;')7XG23*X7*,0!0*X.'V:Y28')?A:;(58(1%0@]'V:;?A*,0:;7/RS'V*X.10CB7+?(C@:X0@]'V:X*I4j-V28?7;7,')7VZC1'Vb')230@H58(1%Y0*X.1'V:^58(B-W:X')Rg')(*X?2:X'Vb'V:;5OoH-V?A*X580fi(6?23%0AN:;53*X.BRS7VZ1?(C 'WU@B230:;5e(1%M@H28?( :X'V@B?58:i*,0S:X')-W0=b'V:cqn:X0fiR:X'Vb'V:;53oH-V?A*X530fi( q?5828G1:;')7VJ 9(1'T<t?)4 58(Y<>.5e-;.*X.1'^?S@:;530:;5]:;')7XG23*X7\R53%fi. *i[]'T'WU*,')(C1')Cj587i[4YCB587X-W0=b'V:/58(1%g<>.1')(23')?A:;(58(%g0@F'V:/?A*,0:;7\<>5e282]RS?A'?g@:;0@F'V:;*v4*,:/G1'Z'Vb')(58qL53*i<t?7>(10*>*,:;G1'T[]'Vqr0:;'+23')?A:;(58(%1J$hG1')7,*X530fi(d*X.?A* <?7Q(10* ?CC1:;')7X7,')CS.1'V:;'\587<>.1'V*X.1'V: *X.1'>58(-W:;')Rg')( *X?2BRg'V*X.10UC7?A:X'>G7X'VqnG2158qRG23*X53@B28'RS?-/.58(1'\28')?A:;(58(1%c0@]'V:;?A*,0:;7Q?A:X'\?A@@B2858')C5e(d[B?A*X-;.Emn'J%1J3Z?70fi('>RS53%fi.*<>587X.*,0^C0T<>53*X.;Afi9H1HH0@]'V:;?A*,0:^z ;W ~33lBp/Jdkl(*X.1'gqG1*XG1:;'<'S<0fiG28C2853'd*,0D'WU@B230:;'d.0=<*,0.B?(C23'*X.587T7X58*XG?A*X530fi(587a58*aRg0:X'E'Wa-V53')(*u*,0!*,:;')?A*u*X.'0@]'V:;?A*,0:;7Y?7Y.?)bU58(1%![]'V')(C0fi(1'0fi(1'WNv?A*sNv?NP*X5eRg'?(CG7,'58(-W:;')Rg')( *X?2L:X'Vb'V:;53oH-V?A*X530fi(Eqr0:9')?-/.H9:c587&*,0*X?2:X'Vb'V:;53oH-V?A*X530fi(jqr:X0fiR7X-W:/?A*X-;.@:X'Vqn'V:;?A[B28'c:)Z[]'V*,*,'V:\4'V*VZH-V?(<Q'MC1'Vb')230@D'Wa-V58')( *>5e(-W:X')Rg')(*X?2?23%0:;58*X.RS7tqn0:&)s^0q23')?A:;(58(%0@]'V:;?A*,0:/7/28?(:;'V@B?53:c<?7T(10*TC5e7X-VG7X7,')Cj58(j*X.587&@B?A@]'V:c?(C587c?(658Rg@]0:X*X?(*>qG1*XG1:;'MC53:;')-W*X530fi(J9K\.1':X')7,')?A:/-;.0q9c'x>?A')C*?(C :;G14U(100%fi.1'ms)p/Zi<>.5e-;.G7,')7-W0fiG( *,'V:;'WU?Rg@H23')7g*,0%fiG5eC1' *X.1':X'VbU587X530fi(0qT*X.'V0:;53')77;G1[U,')-W*g*,0nvP /n/Usnn/ZcRS?)4@B:X0=bU58C1'D7,0fiRS'j58C1')?7)JK\.1'V:;'?A:X'?287,0u@B28?(:X'V@B?53:9Rg'V*X.10UC7c58(D*X.'-V28?7;7X58-V?2L@B28?((5e(1%S2853*,'V:;?A*XG:X'^*X.?A*9RS53%fi.*c[F'M:X')23'VbA?( *c*,0u0fiG1:?A@@:;0fi?-;. m0fi7X2e58(^ 0fi2e28?-XFZ)1 ')28Cd*,)530fi(5PZ)p/Jkl*<0fiG28C^[F'58(*,'V:X')7,*X58(1%>*,0c-W0fiRg@H?A:X'*X.1'+*X5eRg'T*,0S:X'V@B?58:i@B28?(7tb'V:;7;G7i*,:X4U58(1%g?(10*X.1'V:c23')?A:;(5e(1%d0@F'V:/?A*,0:>?(C :X'Vb'V:/53qr4U58(1%1J$2858R53*X?A*X530fi(0qt0fiG1:M?A@B@:X0fi?-;.587T*X.?A*M53*^C10')7^(10*M.?(C23'g7,*,0U-;.B?7,*X58-S@H28?(7T0:M@:X0@]'V:X*X53')7<>53*X.*X5eRg'D2858RS58*X7VZ'J%1J3Z\?x>')7,@]0fi(7,' @:X0@]'V:X*I4qn0:S<>.B58-;.*X.1':X')7,@]0fi(7,'DRG7,*g0U-V-VG1:<&53*X.58(?7,@]')-V5OoB')Cj*X58Rg'g?Aqn*,'V:T*X.1'd*,:;53%%'V:)J 'g<Q0fiGB28C2853'*,0u'WU*,')(C*X.5e7&:;')7,')?A:;-;.*,0Y7X*,0-/.?7,*X58-gU$c7mKiV')(1%1Z)fipu?(C*X58Rg')CU$&7@:X0@]'V:X*X53')7m$&2eG1:Y958282Z+)1M+?A[B?(1)?UZ)fip/Z9?7u<')282?7M0*X.1'V:-W0fiRRg0fi(?A%')(*d:X'V@:X')7X')( *X?A*X530fi(B7+[]')7X58C1')7^1$&7VJD$c(10*X.1'V:dC53:;')-W*X530fi(6qn0:MqG1*XG1:X'<Q0:X<0fiG28C[]'D*,0'W*,')(BC0fiG1:u:X')7XGB23*X7g*,07,4RM[]0fi2858- Rg0UC1')2&-;.1')-;58(%1Zi<>.58-/.G7,')7[B58(?A:;4!C1')-V5e7X530fi(C58?A%:/?RS7+m c97/p\7,0g*X.?A*\*X.'TqnG2e2F7X*X?A*,'^7,@B?-W'^('V')C (10*i[]'T'W@B2e58-V53*X234g'WU@B230:X')C CBG1:;58(1%dRg0UC1')2-;.')-XU58(1%Sm G1:;-;.'V*?2J3Z)p/Jks(d7,0fiRg't-V?7,')7VZ 7,4URM[F0fi2e58- Rg0UC1')2-;.1')-;5e(1%T-V?(M@B:X0CBG-W'QC1:/?RS?A*X587,@]'V')CG1@J>0<Q'Vb'V:=Z9(0fi(1'j0q^*X.1'E-VG:X:X')(*:X')7,')?A:/-;.0fi(7,4UR^[]0fi2858-Rg0C')2&-/.1')-XU58(1%?CC1:;')7X7,')7?C?A@*X58b'^7,4U7,*,')RS7VJ$cCC53*X580fi(?28234ZQ*X.1'58C1')?7.1'V:X'?A:X'E?A@@B285e-V?A[B23'Y*,07,0fiRS'D0qT*X.1'U$>NP[B?7,')C-W0fi( *,:X0fi2&*X.1'V0:X4<0:XHJ10:'W1?Rg@B28'ZAx&?RS?C1%'i?(C 0fi(.?Rms)fip?7X7XGRg'tU$:X'V@B:X')7,')(*X?A*X530fi(7wqr0:[]0*X.*X.1'@B28?(*Mmn<>.B58-;.j587\?7X7XGBRg')CD*,0u[]'^?C5e7X-W:X'V*,'WNP'Vb')(*c7,4U7,*,')Rap\?(CD*X.'M7XG1@]'V:XbU587,0:^mn<&.58-;.j-W0fi( *,:;0fi287*X.1'?-W*X530fi(7a0q+*X.1'E@B28?(*/p/J '6?A:X'-VG1:X:X')(*X234?A@@B234U58(1%7,0fiRg'E0qT*X.1'j@B:;58(-V53@H23')7d0qT'Wa-V53')(*:X'Vb'V:;53oH-V?A*X530fi(a*,0-/.?(1%'c*X.1'c7;G1@]'V:Xb5e7,0: 58(:X')7,@]0fi(7,'\*,0d-;.B?(1%')7i58(S*X.'>@B28?(*Q5e(?RS?((1'V:Q*X.?A*@:X')7X'V:Xb')7i@:X0@]'V:X*X58')7+mPc0:;C10fi(E+53:;5e?A58CB587VZA p/J58(?2e234Z1qnG*XG1:X'T<0:X 7X.10fiG28CYqn0U-VG7\0fi(j7,*XGC14U58(1%g.10<*,00@]'V:;?A*X580fi(?2853V'+$c7X58Rg0bH7&y?=<>7\qn0:58(*,')282853%')(*g?A%')( *X7)J .?A*a7X0:X*X7g0q9@:X0@]'V:X*X53')7d[]')7,*g'WU@:X')7;7S*X.1')7,'j28?)<>7 ')28C?(C *,)530fi(5ms)p\@:X0=bU58C1'T7,0fiRS'^58(53*X5e?2F7;G1%%')7,*X530fi(7)Z1[BG1*\RG-;.DRS0:X'+:X')RS?5e(7t*,0S[]'+C10fi(1'J6fi "BjvBLHfiK\.5e7+:X')7,')?A:/-;.5877XG1@@]0:X*,')C[4*X.1' ca-W'a0q&&?=b?2ix>')7,')?A:;-;.mc1W x&A1V pu58(-W0fi(UNXG(-W*X530fi(Y<>58*X.a*X.'D;')RS?(*X58-M0fi(7;587,*,')(-W4Udjcx\kJ1k?R%:;?A*,'VqnGB2*,0 5e282858?R@]')?A:;7VZfi0fi7,'V@B.c0:;C0fi(ZU*X?(!U?C5e(Zwt.58*,00:U:;58(53bA?7X?(ZLx>?Rg')7X. .?A:;?C1<t?WZ9?(&0'V4Z?(BC*X.1'?(0fi( 4NRg0fiG7>:X'VbU53'V<Q'V:/7>qn0:&G7X'VqnG27XG1%%')7,*X530fi(B7&?(Cj?C1bU58-W'J\K\.1'+@B:X')7,')(*X?A*X530fi(0q*X.'MRS?A*,'V:;5e?2L58(D*X.587@B?A@]'V:i<t?7\')(10:;RS0fiG7X234u58Rg@:X0b')C *X.?(1U7i*,0 58282e58?R@]')?A:;7V17XG1%%')7,*X580fi(7VJ;fic/3 1 fiiHff fi6v"AF""FffP"w!1X!"Ar$Kfi # X {v~fi # |fiFQ=|~ |%$ &' mPp( mP psUn;finn)*2 3 Pm p42 3 6m 5 87 59p4HmP p_l;z :Dn< mPp= Nv?G1*,0fiRS?A*,0fi(? >fi//A @]r DB >Ufi//A @]PB/S)rv)/C; @D3)vFEHGJI)vV neFEHGJI@UK) 3L NI)s_sfi///N M!8POsRQUT/B@FsB@1)/@ lB@ VW U@ 1VZ]EQr;N_ W U@ 1VZ]@ lB@V` mPpbacbce2W > ; rK; @D3)vg; rKj0C')287^m7X?A*X587,oB')7/p$b'V:;5OoF-V?A*X530fi(DRg'V*X.10UC ')(*X?582858(%[B:;G1*,'WNPqn0:;-W'+7,')?A:;-/.$cG1*,0fiRS?A*X?NP*X.1'V0:;'V*X58-MRS0C1')2-/.1')-XU58(1%U5e(1%fi23'T?A%')( *c7X53*XG?A*X580fi(EG23*X58?A%')(*>7X58*XG?A*X530fi(Y<>.1'V:;'T')?-;.E?A%')( *cG7,')7\?gRG23*X58?A%')(*\@B2e?(EG23*X58?A%')(*>7X58*XG?A*X530fi(Y<>.1'V:;'T')?-;.E?A%')( *cG7,')7\?(D5e(C53bU58CG?2B@B28?(58(58*,'WNv7,*X?A*,'M?G1*,0fiR?A*,0fi(K\.'+7,'V*\0q7,*X?A*,')7Mmnb'V:;*X58-W')7/pt0qU$K\.'+7,'V*\0q7,*X?A*,'WNP*,0ANv7,*X?A*,'d*,:;?(7;53*X530fi(7+mn')C1%')7pt0q1$y0%fi58-V?2C1')7;-W:;53@*X530fi(u0q*X.1'+7,'V*>0q?-W*X580fi(7i')(?A[B2e58(1%g?g*,:;?(7;53*X530fi($ 00fi28')?(?23%'V[:/?+10 - +00fi23')?(?23%'V[B:;?g@B?A:X*X5e?2]0:;C1'V:), + *.- 53/K\.'+RS?A*,:;5OY0qL*,:/?(7X53*X530fi( -W0fi(BC53*X530fi(7t0qU$KL:;?(7X58*X530fi( -W0fi(C53*X580fi( ?7X7,0U-V58?A*,')CD<>58*X.Y')C1%'a6m 5 8 7 5!9pK\.'+7,'V*\0q58(53*X5e?2H7,*X?A*,')7&0q1$:;58RS53*X58b'c')28')Rg')( *X7>0qw? 00fi23')?(j?23%'V[:;?U?A*,0fiRS7&?A:X'^?-W*X530fi(B7')hG1')(-W'^0qw?-W*X530fi(B7Mm?A*,0fiRS7/pK\.'+28?(1%fiG?A%'+0q\m7,'V*>0qw7,*,:/58(1%fi7\?-V-W'V@*,')C[4ptU$$c(DU$*X.?A*>?-V-W'V@*X7&58(UoH(B53*,'WNv23')(1%*X.Y7,*,:;5e(1%fi7K\.'+7,')hG')(-W'+0qU$b'V:;*X58-W')7\bU587X53*,')CY[4Y?S7,*,:;58(%K\.'T:;G(Y0q?g7,*,:;58(%S58(Y*X.1'+U$ 28?(1%fiG?A%'$:X')hG53:X')RS')( *t0q?-V-W'V@*X58(1%:;G(7t0q?(U$K\.'T*,')(7,0:m7X4(-/.1:X0fi(10fiGB7/pQ@:X0UCG-W*t0qU$&7@]')-V5OoH')7i?g*,:;?(7X58*X530fi(Yqr0:>'Vb'V:X4Y@F0fi7;7X53[B23'9?-W*X530fi(K\.'+-;.10fi58-W'^0q?-W*X580fi(G(5ehG1')284SC1'V*,'V:;RS58(')7i*X.1'+(1'WU*>7,*X?A*,'')hG1')(-W'^0qLb'V:X*X5e-W')7>-W0fi((1')-W*,')Cj[4a')C1%')7$@B?A*X. <&53*X. 7,*X?A:X*>?(BC ')(CYb'V:X*X58-W')7&58C1')(*X58-V?20fiRS@BG1*X?A*X530fi(?27,*X?A*,'H?(D?-W*X580fi(D0U-V-VG1:X:;58(%d5e( ?g-W0fiRg@BG1*X?A*X530fi(K\.'V:X'T'WU5e7,*X7\?g@B?A*X.Dqr:X0fiRKL')Rg@]0:;?2230%fi5e- ,58( bA?A:;58?(*XKL')Rg@]0:;?2230%fi5e- s'Vb')( *XG?2e234QWVX Z5J'J3ZQskl(b?A:/58?( *&(10* XQ X\[ R^] p/Z5J'J3Z,b'V:;4 X 587i'Vb')(*XG?28234uqr0fi2e230=<')C [4 ]K\.'9oB:;7,* X mn*,:/53%%'V:pt587tqn0fi28230<Q')CD[4 ? ] mn:X')7,@]0fi(7X'pK\.'+7,'V*\0qTs[B?CDmn*,0a[F'+?=b0fi58C1')CHpi7,*X?A*,')7\0qU$t?(58(-W:X')?7,'+?-V-W')7;7X53[B582e53*v4t?(B(10*>58(-W:;')?7,'+?-V-W')7X7X53[H582853*I4t?(C1')-W:X')?7,'?-V-W')7X7X53[B5e2853*v4t?(B(10*>C1')-W:X')?7X'^?-V-W')7X7X53[H582853*I4U?Aqn'^RS?-/.58(1'+28')?A:;(58(1%d0@]'V:;?A*,0:)ZH5J'J3Z1@:X')7,'V:;b')7\@:X0@]'V:X*X58')79('T*X.?A*>587\-W0:;:X')-W*\<>.1')(53*i7,*X?A*,')7>*X.B?A*c gf9('T*X.?A*>587\-W0:;:X')-W*\<>.1')(53*i7,*X?A*,')7>*X.B?A*chbgfK\.'+1$*,:/?(7X53*X530fi(uqG(-W*X530fi(;kjfi9H1HHiHff fiel nmcBhi"&v"offIp fi"oiffPBK\.5e7w?A@B@F')(BC5O]Z<&.58-;.d587[H?7,')Cd0fi(SE?((?T?(C (G1')2e5]ms)U=p/ZUqn0:;RS?28284^C1'WoH(1')7ks(b?A:;5e?(-W'\?(Cx\')7X@F0fi(B7,'\@:X0@]'V:X*X58')7 58(g*,')RS@F0:/?2H230%fi5e-AJ 'c[]'V%fi58(S[4SC1'WoH(B58(1%T*X.1'&[B?7X58->*,')Rg@]0:;?20@]'V:;?A*,0:&qmc( *X582np/J '9?7X7XGBRg'&?M7,*,:;58(1%uKm +r 7ststs p0q-Nv7,*X?A*,')7t0qU$Z<&.1'V:X'>1uwv 7yx7{z J K>.1')(Sqn0:-Nv7X*X?A*,'qn0:;RMGB28?A' X ?(C ] Z1<'^C1'WoH('+&(*X582?|7 +J9 X q ]~} qn0:>7,0fiRg' z\4x UZ +D ] Z?(BC qr0:>'Vb'V:X4 v7XG-/. *X.?A* x u.v z UZ + X Jks( bA?A:;58?(B-W'^@:X0@]'V:X*X58')7\?A:X'dC1'WoH(1')Cj58( *,'V:;RS7&0q b')( *XG?2e234Y@:X0@]'V:X*X53')7VZH7,0a<'MC'WoH(1'^ b')(UN*XG?28284doB:;7,*VJ10:\-Nv7,*X?A*,'Tqn0:;RG28? X ?(CaU$ Z<Q'TC1'WoH('&@:;0@F'V:;*v4 f R X mW,b')(*XG?28234 X p?7T?a@:X0@]'V:X*v4*X.?A*T587>*,:/G1'Ymnq?287,'p>qn0:T?u7X*,:;58(1%u53q53*9587c*,:;G1' mnqn?287X'p>?A*9*X.1'd58(58*X58?2-Nv7,*X?A*,1' + r 0q*X.1'd7,*,:;58(%1J&10:;R?28234Z]53q Km +r 7ststs p&587&?a7X*,:;58(1%a0q U$ ZF*X.1')/( R X } +Dr 8 ;Hq X Z5J'J3Z9s'Vb')( *XGB?28234 X J$@:;0@F'V:;*v4 f QnVX mWskl(bA?A:;58?(*M(10* X pT587MC1'WoH(1')C?^7 QnVX }V R X ZH5J'J3Zi,(1'Vb'V: X J58(?28284ZH?ux\')7,@]0fi(7,'Mqr0:/RMG2e?587&0q*X.1'qr0:/R Q X/[ RF] p/ZF<>.1'V:X' X587&-V?2823')C *X.1'Es*,:;53%%'V:;u?(C ] *X.1'js:X')7X@F0fi(B7,'J$x\')7,@]0fi(7,'^qr0:;RG28?7,*X?A*,')7>*X.B?A*&'Vb'V:X4 *,:/53%%'V:587t'Vb')(*XG?28234Yqn0fi28230<Q')C [4Y?g:X')7,@]0fi(7,'JiHff fi"iffPHl"/fiiffIBLK\.1'Tqn0fi28230<>58(1%oBb'Tks(b?A:;5e?(-W'T@:X0@]'V:X*X53')7t<'V:X'^GB7,')CD58(Y*X.1'T*,')7X*&7XG58*,': 0yNP*,:;?(7XR53*/p,pQ V mnkvNvC')2853b'V: 0yNP@B?G7,'p,pQ V mnkvNvC')2853b'V* 0kvNvC1')2e53b'V:p,pQ V mLNv-W0fi2823')-W* 0kvNvC1')2e53b'VZ: 0yNP:;')-W')53b'p,pQ V mLNv-W0fi2823')-W: 0kvNP:X')-W')58b' 0yNP@B?G7,'p,pQ V mLNvC1')2853b'VK\.1'Tqn0fi28230<>58(1%oBb'+x>')7,@]0fi(7,'T@:;0@F'V:;*X53')7t<Q'V:;'^G7,')C 58(Y*X.'+*,')7,*>7XGB53*,'Q mNvC')2853b'V: [ R yNP:;')-W')53b'pQ mNvC')2853b'V: [ R kINP:X')-W')53b'pQ mNv-W0fi2e23')-W* [ R yNP*,:/?(7XRS53*/p* 0!kINvC1')2858b'V:p [ R yNP:X')-W')53b'pQ m,mNv-W0fi2e23')-WvNC)'8235bV':' 0yNP:X')-W')53b'p,pQ[ R mnkINP:X')-W')53bUsfiBQUB$c28G1:)Zx+J3Z9582e2ZJ ms)p/JS$*X.1'V0:;4j0q*X58Rg')C?G1*,0fiRS?A*X?UJ4o1X,V;; @D>UvVG])VBWZZL) J$c7X58Rg0bHZkJms)A p/J S{Y MXfiJic:;'V')( <>5e-;.ZFiK~1?)<t-W'V*,* G1[B2e58-V?A*X530fi(7VZks(-AJ?)b')2ZwJms)fip/J l>1)PuKU\oU;/ 8OI>UP PAJ x>')7,*,0fi(ZU$^ :X')(*X58-W'WNvc?282JG -;.B5ZAJ ms)fip/J1T(+?>C1')-V5e7X530fi(TRg'V*X.10UC+58(+:X')7,*,:;58-W*,')CM7,')-W0fi(CUNP0:/C1'V:L?A:;58*X.Rg'V*X58-AJAks()Kfi;3Xfi ^3W? @S8 OGF)VB is)// n A+8 OcK UZGHP OW8 BvVBB; H sW;Z@B@JAJ*X?(1qn0:;CZBt$ F*X?(1qn0:;C c(53b'V:;7;53*v4 :X')7;7VJ?fic/3 1 fiG1:;-/.ZB1J3ZHt2e?A:X'Z1\J3Zy0fi(%1ZBJ3Zj-VE582828?(ZUSJ3ZB c5e282ZUMJms)p/Ji4UR^[]0fi2858-cRg0UC1')2]-;.1')-;5e(1%qn0:+7,')hG1')(*X58?2-V53:;-VG58*&b'V:;53oH-V?A*X530fi(JP Bl1Wfi=n1ug @D>UvV?_6I96 {dW 8OBvPfisv{ rs >UnnM GHWv /Z U ZHfi1 1J( \l=/;{ nG1:XU.?A:;CZ^J1ms) fip/J1y53b')(')7X7L?(BC+q?53:;(1')7X7@:X0@]'V:X*X53')758(^RG23*X5ONv?A%')(*7,47X*,')RS7VJ ksOuK U4on/v/VK vVBB; n1nO)V,VBDI9/ Q=n; v VB Z@B@J AUJt.?RM[F'V:;4ZU:;?(-W'Jt?A:;RS')2ZJ3ZBE?A:X0=bU53*X-;.ZFFJms) fip/Jy')?A:;(B58(1%dRg0C')287 0qw58(*,')282853%')(*?A%')(*X7VJks\( \l=/;{ nK Upon/v/VK /MB; O)V,VBYI9 Q=n v! KO VB IZII Z@@J J0:X*X2e?(CZH9x^Jt28?A:;'Z\J3Z 58(%1Z 1Jims) p/J10:;R?2iRg'V*X.10UC7 u*X?A*,'Y0q&*X.1'u?A:;*d?(BCqG1*XG1:;'aC53:;')-W*X530fi(7VJ@>nG>U? T)=Z UK Z J0fiG1:/-W0fiG1[]'V*X587VZHcJ3UZ ?A:/C5ZBJ3Z 0fi23@]'V:)Z J3Z]?((B?A?AU587VZFJLms)fip/J&')Rg0:;4 NP'Wa-V58')( *c?23%0AN:/53*X.RS7Qqr0:t*X.1'9b'V:;53oH-V?A*X530fi(a0q*,')Rg@]0:;?2F@:X0@]'V:X*X58')7VJ E ; DVK ATn\GF=Wv dW 1ZZH Jc'x>?A')C*VZy J3Z :;G14U(100%fi.'ZQJ>ms)p/Jks( *,'V:;?-W*X58b'D*X.1'V0:X4:X'VbU587X530fi(Jks(j58-/.?287,U5Zx+J3ZK')-VG-V5PZtJ&mQC7VJ}p/WZ fi rFLXn SJ Z@B@J 1JiU?(j?A*,'V01Z>t$ 0:;%fi?(^?G1qnR?((Jc')?(ZLK+J3Z ')2e28RS?(ZJ ms)U=p/J n3Br 4Bl; JaU?(6E?A*,'V01Zi^$ j0:X%fi?(+?G1qrNR?((J2e7,')?58C14Z J3ZBt28')?)b')28?(CZx^J3Z1 ?G1%fi.ZJ]ms)p/&J 'V:;53qn4U58(1%^?(Y58(*,')282853%')(*Q7X*,:;G-W*XG1:X'c-W0fi( *,:;0fi27X47,*,')R $-V?7,'7,*XGC4Jkl( \l=/;{ n A8 O6K U X; _wGHv /GH @UW > ZT@@JAJU?(jfiG?(Z G1'V:X*,0Sx&58-W01J53*,0fiG7X7;5ZfiJ3ZK')(B(1')(.10fi23*,ZJBms)fip/JE58(58RS?2U7,0U-V58?2U28?=<>7VJBks( is); fir c8 O>K 1WE OWv;VBKMB; n O)V,VBgI9 ); v! KO VBWZ@@J1 UAJ]E?C587,0fi(Z kJ10%')2ZJms) fip/JT9(*X.1'^:;')28?A*X530fi(7X.B53@D[]'V*v<'V')(jCBG1:;?A*X530fi(0q?(')(-W0fiGB( *,'V:T?(C*X.1'^'Vb0fi28G*X530fi(0qi-W00@]'V:;?A*X530fi(58(*X.1'S53*,'V:;?A*,')C@:;587,0fi('V:)7TC5823')RRS?UJ >UB@>PfinZ y! ZJc0fi28CBRS?(ZwFJ3Zwx\0fi7,')(B7X-;.1')5e(ZwJ ms)p/JYQRg'V:X%')(*^-W00:;C58(?A*X580fi(6*X.1:X0fiG1%fi.*X.'G7,'S0qt-W00@1N'V:/?A*X53b'D7,*X?A*,'WNv-;.B?(1%fi58(1%6:;GB23')7VJEkl( is); fir 8 OuK 1 ! OWK pMnfiB; ^ O)V,VH/DfiIT/ Q); Bv! K O)B/VZ1@@J1fifi J')?A*,*X23'Z $MJc0:;C0fi(ZJ ms)fip/J ')282ONP[]').?)b')C[F0:;%fi7VZ[]0fi230fi7VZ?(C[]'V:;7,'V:X'V:/7VJ+ks( is)// n Aa OSK UEQ O?_v/VK v)/BfinH; OV)s)B/c1fi nHPLXn Z@B@J Jj?C5e7,0fi(ZkJc0:;C0fi(ZMJ ms)fip/Jx\'WNPb'V:;53oH-V?A*X530fi(0q ?C?A@B*X53b'g?A%')( *X7V@H28?(7VJTKL')-;.J:X'V@J3Z]c?)b4t')( *,'V:Tqn0:$&@@B2e53')CYx\')7,')?A:;-/.j5e( $>:X*X53oH-V58?2Fkl(*,')282858%')(-W'Jc0:;C0fi(ZMJ3Z+53:;58?AU58C5e7VZ=gJUmA p/J$&C?A@B*X53b'Q7XG@F'V:;b587X0:X4c-W0fi(*,:X0fi20q58(*,'V:;-W0fi((1')-W*,')CdC5e7X-W:X'V*,''Vb')(* 7,4U7,*,')RS7)Jks( is); fir &8 O\K U v)/BfinH; OV)s)B/cs; JIn@@O;UWI Z@@JA J]$&(B-;.10:;?A%'ZF$&SJ?fi9H1HHc0:;C0fi(ZBJ3ZU@F')?A:/7VZ J3Z00fi287X4ZJ3ZHy'V'ZBkJLms)fip/J>9587,*,:/53[BG1*,')C 7,@H?A*X58?2-W0fi(*,:X0fi2ZH%fi230[B?2RS0fi(53*,0:;58(%?(C7,*,'V'V:;58(1%d0qRg0[B5828'&@B.4U7X58-V?2]?A%')( *X7)Jkl(\l=/;{n AO^KUPwS v)B_HnfiB; fi OV)s)B/dfi OV nfi v OOVH/ fi GHv SS G Z@@J,U J?7;.58(1%*,0fi(ZHMJ}9Jc:X'Vqn')(7,*,'V*,*,'ZT1J3Zt >x ?RS7,'V4Z9cJ9ms)fip/J$c(?A@@:X0fi?-/.*,0?(4*X58RS'E23')?A:/(58(1%1J ks(g\l=/;{_n M8 O^rBK B vVBB;~D{"=BB@fi!rH~LXn ZH@@J) )J$&[]'V:;C1'V')(Z1-W0*X28?(CJ&')53*XRg'V4'V:)ZcJ3Z+58:X[4Z1J3ZfiyL?A[B?)<^Z J3Zfi$&:;-;.'V:)ZfiJ3Z .?A:;?C1<t?VZAx^JUms)fip/Jc7X58(%&?A[B7,*,:/?-W*X530fi(?(BC6Rg0C')2w-/.1')-XU58(1% *,0DC'V*,')-W*^7X?Aqn'V*v4Eb530fi2e?A*X530fi(7958(E:X')hG53:X')Rg')(*X797,@]')-V5OoH-V?A*X530fi(B7VJPBsUW )Udfi/GF8 rH/VrZ k? Z] J&0fi23)RS?((ZTJ3Z ')28')CZ&J3ZT?(B(?A?AU587VZ9JTms) fip/J9((1')7,*,')C C'V@*X.UNoB:;7X*a7,')?A:;-/.Jks(\l=/;{ n Ag8 O^K U^G];;G@Fng"{ ?@HZB@@JBU JFx&G1*,%'V:;7VZB91J&0fi23)RS?((ZHdJH1Jms)U=p/|J dOfi ; 6 fi8 @D>UvV\lfiP);; }/Jt9, :X')( *X5e-W'WNv&?282PJ0fi7;2858(ZJ3Z 0fi2e28?-XFZQJ\ms)p/J!y')?7,*sNv-W0fi7,*gfH?=<:X'V@B?53:a$@B28?(!:X'WoF(1')Rg')(*7,*,:/?A*,'V%4qn0:@H?A:X*X58?2ONP0:;C'V:a@B2e?((58(1%1Jks(g\l=/;{n A8OjK1 ! OK vVBB;OV)s)B/6fiIT/ Q); Bv! K O)B/VZ1@@JVA VfiJL')?A*,*X23'Z $MJ+?A[B?()?UZUJms)fip/J 4U(-;.1:;0fi(53)58(1%MRMG28*X58?A%')( *t@B2e?(7 G7;58(1%^*,')RS@F0:/?2H230%fi5e-&7,@]')-V5OoF-V?A*X530fi(7VJks(\l=/;{ n 8K UEQnXW BvVBB; ~ O)V,VB>O))WGHWv IG Z@B@JBU 1JU?( 1:;?(-V5e7X-W01Z]t$MJ+G1:/7X.?(Zx+Jims)p/J @D>UvVITK { V Q;8 = firBfir p\l)/VJ :;5e(-W'V*,0fi(Z9, :;58(B-W'V*,0fi( &(58b'V:;7X53*I4 :X')7X7VJy'V'ZJ3Z 9G1:Xqn'V'Z\Jms)p/J9(j'W@B2e58-V53*>@H28?(28?(%fiG?A%')79qr0:9-W00:;C58(?A*X5e(1%aRG23*X58?A%')(*c@B28?('WU')-VG1*X530fi(J^ks(p\l=/;{n Aa8OgKUE>U/K vVBBD{"=BB@ Vo1XWITl rv;)N >U, 1fi >1) IFI Z1@@J )J :X0b5eC1')(-W'Zx\kJE?((?UZwJ3Z (G1')285Z]$^Jms)U=p/Jg0fiRg@B23'V*X5e(1%*X.1'*,')Rg@]0:;?2w@B58-W*XG:X'JoUX,V;Z @D>UvVG])VBWZ A? ZF AUJE58-;.?2e7,5PZx+JQms)fip/JY$*X.'V0:X46?(C6Rg'V*X.10UC10fi230%40qt58(CG-W*X58b'd28')?A:;(58(1%1Jdks(6j5e-;.?287X5ZLx+J3Zi?A:X[]0fi(1')282ZU1J3ZBj58*X-;.1')282PZUK+JmQC7VJ}p/Z fi! nH;/Br Z1@@JU 1J ?280d$c23*,01ZHt$K>530%fi?UJE53*X-;.1')2e2ZcK+JTms)Afip/J V;G@Ufi/wI9hI@@]l) !PH/A@];rJ .JMJi*X.')7X587VZU*X?(1qr0:/CD&(B53b'V:;7X53*I4Jc58287X7,0fi(ZUMJLms)A p/J|irH)@3WM8OI9 Q); vO VBWJ ?230S$c23*,01ZFi$^BK>530%fi?UJ0*,*,'V:)Z9J+ms)p/JoUdOfi 4I9B eE8O6 @D>UPBZ +8O)?@1VsKT>U1J .JJ*X.1')7X5e7VZHc'V0:X%'j?7,0fi(j&(B53b'V:;7X53*I4Jx>?R?C1%'Z J3Z1 fi0 (.?R Z J]ms)fip/JK\.1'c-W0fi( *,:X0fi2F0qCB587X-W:X'V*,'&'Vb')( *i7,4U7,*,')RS7VJ&is)//n A^8OK 1Z ZHU J?fic/3 1 fi'VA?A:)Zx+J3Zy58(ZoJ NIJ3Z x>?RS?A:;587;.(?(Zw9JQms)p/Jj0C1')2e58(1% *,')-;.B(58hG1')7+qn0:^'Vb0fi23bU58(1%jC587sN*,:/53[BG1*,')C?A@@H2858-V?A*X530fi(7)Jks(.is)//n AjOE ;dV)@FH;!BK>1W EW Z@B@JB J 'V:;('ZF<>53*,V'V:/28?(CJU.10fi.B?R Z dJ3ZBK')(B(1')(.10fi23ZUJms)fip/JQ9(Y7,0-V5e?2F2e?)<>7qn0:t?A:;*X5OoH-V58?2]?A%')(*\7,0-V58'V*X53')7 Q&FNv2858(1'C')7X53%fi(PJ I9 Q); v OOVH/WHZ UA _ ZF U JU530:/7,5PZUx^Jms) fip/WJ c) 3;Ik MVs/J &'V< 0:;HZBF@:;5e(1%'V:, N 'V:;28?A%1J00fi287X4ZJ3ZFURg0fi28?UZFJLms)p/Jckl(-W:;')Rg')( *X?2wRg0UC1')2-/.1')-XU58(1%u58(D*X.'MRg0UC?2RGUNv-V?28-VGB28G7VJks( \l);{ rAd @D>UvV? _6I96 { V ; Z1@@,J U J*X?(qr0:;CZHt$^J@]')?A:;7VZ J3ZBc0:;C10fi(ZJms)fip/J c7X58(1%g?A:X*X53oH-V58?2H@B. 4U7X58-V7*,0S-W0fi(*,:X0fi2?A%')(*X7VJkl( \l=/;{ nO+K UwS vVBB O)V,VH/ OWfi HS v! K O)B/ GHv Z@@JU J ?7X.B58(1%*,0fi(ZBJ}cJKL')((1')(.10fi28*,ZJ3Z 0fi7,')7)&Z J\ms)fip/J9(!-W00@]'V:;?A*X580fi(!5e(?ERMGB23*X5ONP')(*X53*v4RS0C1')2PJDkl( \l _;{ rAM8 O+K U 3 TVBK v)/BfinH; o1nZOV)s)B/I9 ); v! K O)B/VZ1@@JU) JKiV')(1%1Z Jms)fip/J]y')?A:;(5e(1%>@:;0[B?A[B58285e7,*X58-?G1*,0fiR?A*X?T?(CdRS?A:X0=bd-;.B?58(7bU58?&hG1'V:;58')7VJ fi rFLXn Z Z)U J?A:;C5PZwJ3Z 0fi23@]'V:)Z Jtms) fip/J$&(?G1*,0fiRS?A*X?NP*X.1'V0:;'V*X58- ?A@@:X0fi?-/.*,0?G1*,0fiRS?A*X5e-@:;0%:;?Rb'V:/5OoH-V?A*X530fi(Jks.( is)// n j8 ODK UEQr;I9U >1; GH @UW > fiXfiEr@D>UvVG])VB G Z@@,J Jt?RM[:;5eC1%'Zj$MJ( is)// n AD8 OuK U ! OWK')28CZJ3Z *,)530fi(5ZJms)p/J6K\.'goB:;7,*28?)< 0qi:X0[]0*X58-V7VJ ksMB; n O)V,VBgI9 ); v! KO VBWZ@@J]VA VA JLU')?A*,*X23'Z $^J?fiJournal Artificial Intelligence Research 13 (2000) 227-303Submitted 11/99; published 11/00Hierarchical Reinforcement Learning MAXQ ValueFunction DecompositionThomas G. DietterichDepartment Computer Science, Oregon State UniversityCorvallis, 97331Abstracttgd@cs.orst.edupaper presents new approach hierarchical reinforcement learning based decomposing target Markov decision process (MDP) hierarchy smaller MDPsdecomposing value function target MDP additive combinationvalue functions smaller MDPs. decomposition, known MAXQ decomposition, procedural semantics|as subroutine hierarchy|and declarativesemantics|as representation value function hierarchical policy. MAXQ unifiesextends previous work hierarchical reinforcement learning Singh, Kaelbling,Dayan Hinton. based assumption programmer identify usefulsubgoals define subtasks achieve subgoals. defining subgoals,programmer constrains set policies need considered reinforcementlearning. MAXQ value function decomposition represent value functionpolicy consistent given hierarchy. decomposition also creates opportunities exploit state abstractions, individual MDPs within hierarchyignore large parts state space. important practical applicationmethod. paper defines MAXQ hierarchy, proves formal results representational power, establishes five conditions safe use state abstractions. paperpresents online model-free learning algorithm, MAXQ-Q, proves convergesprobability 1 kind locally-optimal policy known recursively optimal policy,even presence five kinds state abstraction. paper evaluates MAXQrepresentation MAXQ-Q series experiments three domains showsexperimentally MAXQ-Q (with state abstractions) converges recursively optimalpolicy much faster Q learning. fact MAXQ learns representationvalue function important benefit: makes possible compute executeimproved, non-hierarchical policy via procedure similar policy improvementstep policy iteration. paper demonstrates effectiveness non-hierarchicalexecution experimentally. Finally, paper concludes comparison related workdiscussion design tradeoffs hierarchical reinforcement learning.c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiDietterich1. Introductionarea Reinforcement Learning (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998)studies methods agent learn optimal near-optimal plans interactingdirectly external environment. basic methods reinforcement learningbased classical dynamic programming algorithms developed late1950s (Bellman, 1957; Howard, 1960). However, reinforcement learning methods offer twoimportant advantages classical dynamic programming. First, methods online.permits focus attention parts state space importantignore rest space. Second, methods employ function approximation algorithms (e.g., neural networks) represent knowledge. allowsgeneralize across state space learning time scales much better.Despite recent advances reinforcement learning, still many shortcomings.biggest lack fully satisfactory method incorporating hierarchiesreinforcement learning algorithms. Research classical planning shown hierarchical methods hierarchical task networks (Currie & Tate, 1991), macro actions(Fikes, Hart, & Nilsson, 1972; Korf, 1985), state abstraction methods (Sacerdoti, 1974;Knoblock, 1990) provide exponential reductions computational cost findinggood plans. However, basic algorithms probabilistic planning reinforcement learning \ at" methods|they treat state space one huge search space.means paths start state goal state long,length paths determines cost learning planning, informationfuture rewards must propagated backward along paths.Many researchers (Singh, 1992; Lin, 1993; Kaelbling, 1993; Dayan & Hinton, 1993;Hauskrecht, et al., 1998; Parr & Russell, 1998; Sutton, Precup, & Singh, 1998) experimented different methods hierarchical reinforcement learning hierarchicalprobabilistic planning. research explored many different points design spacehierarchical methods, several systems designed specific situations.lack crisp definitions main approaches clear understanding relativemerits different methods.paper formalizes clarifies one approach attempts understandcompares techniques. approach, called MAXQ method, provideshierarchical decomposition given reinforcement learning problem set subproblems. simultaneously provides decomposition value function givenproblem set value functions subproblems. Hence, declarativesemantics (as value function decomposition) procedural semantics (as subroutinehierarchy).decomposition subproblems many advantages. First, policies learnedsubproblems shared (reused) multiple parent tasks. Second, value functionslearned subproblems shared, subproblem reused new task,learning overall value function new task accelerated. Third, state abstractions applied, overall value function represented compactlysum separate terms depends subset state variables.compact representation value function require less data learn, hence,learning faster.228fiMAXQ Hierarchical Reinforcement LearningPrevious research shows several important design decisions mustmade constructing hierarchical reinforcement learning system. provideoverview results paper, let us review issues see MAXQmethod approaches them.first issue specify subtasks. Hierarchical reinforcement learning involvesbreaking target Markov decision problem hierarchy subproblems subtasks.three general approaches defining subtasks. One approach definesubtask terms fixed policy provided programmer (orlearned separate process). \option" method Sutton, Precup, Singh(1998) takes approach. second approach define subtask terms nondeterministic finite-state controller. Hierarchy Abstract Machines (HAM) methodParr Russell (1998) takes approach. method permits programmerprovide \partial policy" constrains set permitted actions point,specify complete policy subtask. third approach definesubtask terms termination predicate local reward function. definemeans subtask completed final reward completingsubtask. MAXQ method described paper follows approach, buildingupon previous work Singh (1992), Kaelbling (1993), Dayan Hinton (1993), DeanLin (1995).advantage \option" partial policy approaches subtaskdefined terms amount effort course action rather termsachieving particular goal condition. However, \option" approach (at leastsimple form described paper), requires programmer provide complete policiessubtasks, dicult programming task real-world problems.hand, termination predicate method requires programmer guess relativedesirability different states subtask might terminate. alsodicult, although Dean Lin show guesses revised automaticallylearning algorithm.potential drawback hierarchical methods learned policy maysuboptimal. hierarchy constrains set possible policies considered.constraints poorly chosen, resulting policy suboptimal. Nonetheless,learning algorithms developed \option" partial policy approachesguarantee learned policy best possible policy consistentconstraints.termination predicate method suffers additional source suboptimality.learning algorithm described paper converges form local optimalitycall recursive optimality. means policy subtask locally optimalgiven policies children. might exist better hierarchical policiespolicy subtask must locally suboptimal overall policy optimal.example, subtask buying milk might performed suboptimally (at distantstore) larger problem also involves buying film (at store). problemavoided careful definition termination predicates local reward functions,added burden programmer. (It interesting note problemrecursive optimality noticed previously. previous work229fiDietterichfocused subtasks single terminal state, cases, problemarise.)second design issue whether employ state abstractions within subtasks.subtask employs state abstraction ignores aspects state environment.example, many robot navigation problems, choices route takereach goal location independent robot currently carrying.exceptions, state abstraction explored previously. see MAXQmethod creates many opportunities exploit state abstraction, abstractionshuge impact accelerating learning. also see importantdesign tradeoff: successful use state abstraction requires subtasks definedterms termination predicates rather using option partial policy methods.MAXQ method must employ termination predicates, despite problemscreate.third design issue concerns non-hierarchical \execution" learned hierarchical policy. Kaelbling (1993) first point value function learnedhierarchical policy could evaluated incrementally yield potentially muchbetter non-hierarchical policy. Dietterich (1998) Sutton, et al. (1999) generalizedshow arbitrary subroutines could executed non-hierarchically yield improvedpolicies. However, order support non-hierarchical execution, extra learningrequired. Ordinarily, hierarchical reinforcement learning, states learningrequired higher levels hierarchy states one subroutines could terminate (plus possible initial states). support non-hierarchicalexecution, learning required states (and levels hierarchy). general,requires additional exploration well additional computation memory.consequence hierarchical decomposition value function, MAXQ methodable support either form execution, see many problemsimprovement non-hierarchical execution worth added cost.fourth final issue form learning algorithm employ. important advantage reinforcement learning algorithms typically operate online.However, finding online algorithms work general hierarchical reinforcement learningdicult, particularly within termination predicate family methods. Singh'smethod relied subtask unique terminal state; Kaelbling employed mixonline batch algorithms train hierarchy; work within \options" framework usually assumes policies subproblems given needlearned all. best previous online algorithms HAMQ Q learning algorithmParr Russell (for partial policy method) Feudal Q algorithm DayanHinton. Unfortunately, HAMQ method requires \ attening" hierarchy,several undesirable consequences. Feudal Q algorithm tailored specific kindproblem, converge well-defined optimal policy.paper, present general algorithm, called MAXQ-Q, fully-online learninghierarchical value function. algorithm enables subtasks within hierarchylearned simultaneously online. show experimentally theoreticallyalgorithm converges recursively optimal policy. also show substantiallyfaster \ at" (i.e., non-hierarchical) Q learning state abstractions employed.230fiMAXQ Hierarchical Reinforcement Learningremainder paper organized follows. introducing notationSection 2, define MAXQ value function decomposition Section 3 illustratesimple example Markov decision problem. Section 4 presents analyticallytractable version MAXQ-Q learning algorithm called MAXQ-0 algorithmproves convergence recursively optimal policy. shows extend MAXQ0 produce MAXQ-Q algorithm, shows extend theorem similarly.Section 5 takes issue state abstraction formalizes series five conditionsstate abstractions safely incorporated MAXQ representation.State abstraction give rise hierarchical credit assignment problem, paperbrie discusses one solution problem. Finally, Section 7 presents experimentsthree example domains. experiments give idea generality MAXQrepresentation. also provide results relative importance temporal stateabstractions importance non-hierarchical execution. paper concludesdiscussion design issues brie described above, particular,addresses tradeoff method defining subtasks (via termination predicates)ability exploit state abstractions.readers may disappointed MAXQ provides way learning structure hierarchy. philosophy developing MAXQ (which sharereinforcement learning researchers, notably Parr Russell) draw inspirationdevelopment Belief Networks (Pearl, 1988). Belief networks first introducedformalism knowledge engineer would describe structure networks domain experts would provide necessary probability estimates. Subsequently,methods developed learning probability values directly observational data.recently, several methods developed learning structure beliefnetworks data, dependence knowledge engineer reduced.paper, likewise require programmer provide structurehierarchy. programmer also need make several important design decisions.see MAXQ representation much like computer program,rely programmer design modules indicate permissibleways modules invoke other. learning algorithms fill\implementations" module way overall program work well.believe approach provide practical tool solving large real-worldMDPs. also believe help us understand structure hierarchical learningalgorithms. hope subsequent research able automatework currently requiring programmer do.2. Formal Definitionsbegin introducing definitions Markov Decision Problems Semi-Markov Decision Problems.2.1 Markov Decision Problemsemploy standard definition Markov Decision Problems (also known Markovdecision processes). paper, restrict attention situations agent231fiDietterichinteracting fully-observable stochastic environment. situation modeledMarkov Decision Problem (MDP) hS; A; P; R; P0 defined follows:: finite set states environment. point time, agentobserve complete state environment.A: finite set actions. Technically, set available actions dependscurrent state s, suppress dependence notation.P : action 2 performed, environment makes probabilistic transition current state resulting state s0 according probabilitydistribution P (s0 js; a).R: Similarly, action performed environment makes transitions0 , agent receives real-valued (possibly stochastic) reward r whoseexpected value R(s0 js; a). simplify notation, customary treatreward given time action initiated, even though maygeneral depend s0 well a.P0 : starting state distribution. MDP initialized, stateprobability P0 (s).policy, , mapping states actions tells action = (s) performenvironment state s.consider two settings: episodic infinite-horizon.episodic setting, rewards finite least one zero-cost absorbingterminal state. absorbing terminal state state actions lead backstate probability 1 zero reward. technical reasons, considerproblems deterministic policies \proper"|that is, deterministic policiesnon-zero probability reaching terminal state started arbitrary state.(We believe condition relaxed, verified formally.)episodic setting, goal agent find policy maximizes expectedcumulative reward. special case rewards non-positive, problemsreferred stochastic shortest path problems, rewards viewedcosts (i.e., lengths), policy attempts move agent along path minimumexpected cost.infinite horizon setting, rewards also finite. addition, discountfactor , agent's goal find policy minimizes infinite discounted sumfuture rewards.value function V policy function tells, state s,expected cumulative reward executing policy starting state s. Let rtrandom variable tells reward agent receives time step followingpolicy . define value function episodic settingV (s) = E frt + rt+1 + rt+2 + jst = s; g :discounted setting, value functionfinV (s) = E rt + rt+1 + 2 rt+2 + fifi st = s; :232fiMAXQ Hierarchical Reinforcement Learningsee equation reduces previous one = 1. However, infinitehorizon MDPs sum may converge = 1.value function satisfies Bellman equation fixed policy:V (s) =Xs0P (s0 js; (s)) R(s0 js; (s)) + V (s0 ) :quantity right-hand side called backed-up value performing actionstate s. possible successor state s0 , computes reward would receivedvalue resulting state weights according probabilityending s0 .optimal value function V value function simultaneously maximizesexpected cumulative reward states 2 . Bellman (1957) proved uniquesolution known Bellman equation:V (s) = maxXs0P (s0 js; a) R(s0 js; a) + V (s0 ) :(1)may many optimal policies achieve value. policy choosesachieve maximum right-hand side equation optimal policy.denote optimal policy . Note optimal policies \greedy"respect backed-up value available actions.Closely related value function so-called action-value function, Q function(Watkins, 1989). function, Q (s; a), gives expected cumulative reward performing action state following policy thereafter. Q function also satisfiesBellman equation:Q (s; a) =Xs0P (s0 js; a) R(s0 js; a) + Q (s0 ; (s0 )) :optimal action-value function written Q (s; a), satisfies equationXQ (s; a) = P (s0 js; a)s0R(s0 js; a) + max Q(s0 ; a0 )a0:(2)Note policy greedy respect Q optimal policy. maymany optimal policies|they differ break ties actionsidentical Q values.action order, denoted !, total order actions within MDP. is, !anti-symmetric, transitive relation !(a1 ; a2 ) true iff a1 strictly preferreda2 . ordered greedy policy, ! greedy policy breaks ties using !. example,suppose two best actions state a1 a2 , Q(s; a1 ) = Q(s; a2 ),!(a1 ; a2 ). ordered greedy policy ! choose a1 : ! (s) = a1 . Notealthough may many optimal policies given MDP, ordered greedy policy,! , unique.233fiDietterich2.2 Semi-Markov Decision Processesorder introduce prove properties MAXQ decomposition,need consider simple generalization MDPs|the semi-Markov decision process.discrete-time semi-Markov Decision Process (SMDP) generalization MarkovDecision Process actions take variable amount time complete.particular, let random variable N denote number time steps action takesexecuted state s. extend state transition probability functionjoint distribution result states s0 number time steps N actionperformed state s: P (s0 ; N js; a). Similarly, expected reward changedR(s0 ; N js; a).1straightforward modify Bellman equation define value functionfixed policyhXV (s) = P (s0; N js; (s)) R(s0 ; N js; (s)) + N V (s0 ) :s0 ;Nchange expected value right-hand side taken respects0 N , raised power N ect variable amount timemay elapse executing action a.Note expectation linear operator, write Bellmanequations sum expected reward performing action expected valueresulting state s0 . example, rewrite equationX(3)V (s) = R(s; (s)) + P (s0 ; N js; (s)) N V (s0 ):s0 ;NR(s; (s)) expected reward performing action (s) state s, expectation taken respect s0 N .results given paper generalized apply discrete-time semiMarkov Decision Processes. consequence whenever paper talksexecuting primitive action, could easily talk executing hand-coded openloop \subroutine". subroutines would learned, could executioninterrupted discussed Section 6. many applications (e.g., robotcontrol limited sensors), open-loop controllers useful (e.g., hide partialobservability). example, see Kalmar, Szepesvari, A. Lorincz (1998).Note episodic case, difference MDP Semi-MarkovDecision Process, discount factor 1, therefore neither optimal policyoptimal value function depend amount time action takes.2.3 Reinforcement Learning Algorithmsreinforcement learning algorithm algorithm tries construct optimal policyunknown MDP. algorithm given access unknown MDP via following1. formalization slightly different standard formulation SMDPs, separatesP (s0js; a) F (tjs; a), F cumulative distribution function probabilityterminate time units, real-valued rather integer-valued. case, importantconsider joint distribution s0 N , need consider actions arbitraryreal-valued durations.234fiMAXQ Hierarchical Reinforcement Learningreinforcement learning protocol. time step t, algorithm told current stateMDP set actions A(s) executable state.algorithm chooses action 2 A(s), MDP executes action (which causesmove state s') returns real-valued reward r. absorbing terminal state,set actions A(s) contains special action reset, causes MDP moveone initial states, drawn according P0 .paper, make use two well-known learning algorithms: Q learning(Watkins, 1989; Watkins & Dayan, 1992) SARSA(0) (Rummery & Niranjan, 1994).apply algorithms case action value function Q(s; a) representedtable one entry pair state action. Every entry tableinitialized arbitrarily.Q learning, algorithm observed s, chosen a, received r, observed s0 ,performs following update:Qt (s; a) := (1 , fft )Qt,1 (s; a) + fft [r + maxQ (s0 ; a0 )];a0 t,1fft learning rate parameter.Jaakkola, Jordan Singh (1994) Bertsekas Tsitsiklis (1996) proveagent follows \exploration policy" tries every action every state infinitely oftenXXlimff=1limff2t < 1(4)!1T!1t=1t=1Qt converges optimal action-value function Q probability 1. proofholds settings discussed paper (episodic infinite-horizon).SARSA(0) algorithm similar. observing s, choosing a, observing r,observing s0 , choosing a0 , algorithm performs following update:Qt (s; a) := (1 , fft )Qt,1 (s; a) + fft [r + Qt,1 (s0 ; a0 )];fft learning rate parameter. key difference Q value chosenaction a0 , Q(s0 ; a0 ), appears right-hand side place Q learning usesQ value best action. Singh, et al. (1998) provide two important convergence results:First, fixed policy employed choose actions, SARSA(0) convergevalue function policy provided fft decreases according Equations (4). Second,so-called GLIE policy employed choose actions, SARSA(0) converge valuefunction optimal policy, provided fft decreases according Equations (4).GLIE policy defined follows:Definition 1 GLIE (Greedy Limit Infinite Exploration) policy policysatisfying1. action executed infinitely often every state visited infinitely often.2. limit, policy greedy respect Q-value function probability1.235fiDietterich4RG32100B1234Figure 1: Taxi Domain.3. MAXQ Value Function Decompositioncenter MAXQ method hierarchical reinforcement learning MAXQvalue function decomposition. MAXQ describes decompose overall value functionpolicy collection value functions individual subtasks (and subsubtasks,recursively).3.1 Motivating Examplemake discussion concrete, let us consider following simple example. Figure 1shows 5-by-5 grid world inhabited taxi agent. four specially-designatedlocations world, marked R(ed), B(lue), G(reen), Y(ellow). taxi problemepisodic. episode, taxi starts randomly-chosen square.passenger one four locations (chosen randomly), passenger wishestransported one four locations (also chosen randomly). taxi must gopassenger's location (the \source"), pick passenger, go destination location(the \destination"), put passenger there. (To keep things uniform, taximust pick drop passenger even he/she already located destination!)episode ends passenger deposited destination location.six primitive actions domain: (a) four navigation actions movetaxi one square North, South, East, West, (b) Pickup action, (c) Putdown action.reward ,1 action additional reward +20 successfullydelivering passenger. reward ,10 taxi attempts executePutdown Pickup actions illegally. navigation action would cause taxi hitwall, action no-op, usual reward ,1.simplify examples throughout section, make six primitive actions deterministic. Later, make actions stochastic order create greaterchallenge learning algorithms.seek policy maximizes total reward per episode. 500 possiblestates: 25 squares, 5 locations passenger (counting four starting locationstaxi), 4 destinations.task simple hierarchical structure two main sub-tasks:Get passenger Deliver passenger. subtasks turn involves236fiMAXQ Hierarchical Reinforcement Learningsubtask navigating one four locations performing Pickup Putdownaction.task illustrates need support temporal abstraction, state abstraction,subtask sharing. temporal abstraction obvious|for example, process navigating passenger's location picking passenger temporally extendedaction take different numbers steps complete depending distancetarget. top level policy (get passenger; deliver passenger) expressedsimply temporal abstractions employed.need state abstraction perhaps less obvious. Consider subtask gettingpassenger. subtask solved, destination passengercompletely irrelevant|it cannot affect nagivation pickup decisions. Perhapsimportantly, navigating target location (either source destinationlocation passenger), target location important. factcases taxi carrying passenger cases irrelevant.Finally, support subtask sharing critical. system could learn solvenavigation subtask once, solution could shared \Get passenger"\Deliver passenger" subtasks. show MAXQ method providesvalue function representation learning algorithm supports temporal abstraction,state abstraction, subtask sharing.construct MAXQ decomposition taxi problem, must identify setindividual subtasks believe important solving overall task.case, let us define following four tasks:Navigate(t). subtask, goal move taxi current locationone four target locations, indicated formal parameter t.Get. subtask, goal move taxi current locationpassenger's current location pick passenger.Put. goal subtask move taxi current locationpassenger's destination location drop passenger.Root. whole taxi task.subtasks defined subgoal, subtask terminatessubgoal achieved.defining subtasks, must indicate subtask subtasksprimitive actions employ reach goal. example, Navigate(t) subtaskuse four primitive actions North, South, East, West. Get subtaskuse Navigate subtask Pickup primitive action, on.information summarized directed acyclic graph called taskgraph, shown Figure 2. graph, node corresponds subtaskprimitive action, edge corresponds potential way one subtask\call" one child tasks. notation formal=actual (e.g., t=source) tells formalparameter bound actual parameter.suppose subtasks, write policy (e.g., computerprogram) achieve subtask. refer policy subtask \subroutine", view parent subroutine invoking child subroutine via ordinary237fiDietterichRootGetPutt/sourcePickupt/destinationNavigate(t)NorthSouthEastPutdownWestFigure 2: task graph Taxi problem.subroutine-call-and-return semantics. policy subtask, givesus overall policy Taxi MDP. Root subtask executes policy callingsubroutines policies Get Put subtasks. Get policy calls subroutinesNavigate(t) subtask Pickup primitive action. on. callcollection policies hierarchical policy. hierarchical policy, subroutine executesenters terminal state subtask.3.2 DefinitionsLet us formalize discussion far.MAXQ decomposition takes given MDP decomposes finite setsubtasks fM0 ; M1 ; : : : ; Mn g convention M0 root subtask (i.e., solvingM0 solves entire original MDP ).Definition 2 unparameterized subtask three-tuple, hTi; Ai ; R~i i, defined follows:1. Ti termination predicate partitions set active states, Si , setterminal states, Ti : policy subtask Mi executed currentstate Si . If, time subtask Mi executed, MDP entersstate Ti , Mi terminates immediately (even still executing subtask, seebelow).2. Ai set actions performed achieve subtask Mi . actionseither primitive actions A, set primitive actions MDP,subtasks, denote indexes i. referactions \children" subtask i. sets Ai define directed graphsubtasks M0 ; : : : ; Mn , graph may contain cycles. Stated another way,subtask invoke recursively either directly indirectly.child subtask Mj formal parameters, interpreted subtaskoccurred multiple times Ai , one occurrence possible tuple actual238fiMAXQ Hierarchical Reinforcement Learningvalues could bound formal parameters. set actions Ai may differone state another one set actual parameter values another,technically, Ai function actual parameters. However, suppressdependence notation.3. R~ (s0 ) pseudo-reward function, specifies (deterministic) pseudo-rewardtransition terminal state s0 2 Ti . pseudo-reward tells desirableterminal states subtask. typically employed give goalterminal states pseudo-reward 0 non-goal terminal states negativereward. definition, pseudo-reward R~ (s) also zero non-terminal statess. pseudo-reward used learning, mentionedSection 4.primitive action primitive subtask MAXQ decompositionalways executable, always terminates immediately execution,pseudo-reward function uniformly zero.subtask formal parameters, possible binding actual valuesformal parameters specifies distinct subtask. think values formalparameters part \name" subtask. practice, course, implementparameterized subtask parameterizing various components task. b specifiesactual parameter values task Mi , define parameterized terminationpredicate Ti (s; b) parameterized pseudo-reward function R~ (s0 ; b). simplify notationrest paper, usually omit parameter bindings. However,noted parameter subtask takes large number possible values,equivalent creating large number different subtasks, needlearned. also create large number candidate actions parent task,make learning problem dicult parent task well.Definition 3 hierarchical policy, , set containing policy subtasksproblem: = f0 ; : : : ; n g:subtask policy takes state returns name primitive actionexecute name subroutine (and bindings formal parameters) invoke.terminology Sutton, Precup, Singh (1998), subtask policy deterministic\option", probability terminating state (which denote fi (s)) 02 Si , 1 2 Ti .parameterized task, policy must parameterized well takesstate bindings formal parameters returns chosen action bindings(if any) formal parameters.Table 1 gives pseudo-code description procedure executing hierarchicalpolicy. hierarchical policy executed using stack discipline, similar ordinaryprogramming languages. Let Kt denote contents pushdown stack time t.subroutine invoked, name actual parameters pushed onto stack.subroutine terminates, name actual parameters popped stack.Notice (line 16) subroutine stack terminates, subroutines239fiDietterichTable 1: Pseudo-Code Execution Hierarchical Policy.123456789101112131415161718192021Procedure ExecuteHierarchicalPolicy()st state world timeKt state execution stack timeLet = 0; Kt = empty stack; observe stpush (0; nil) onto stack Kt (invoke root task parameters)repeattop(Kt ) primitive actionLet (i; fi ) := top(Kt),name \current" subroutine,fi gives parameter bindingsLet (a; fa ) := (s; fi ),action fa gives parameter bindings chosen policypush (a; fa ) onto stack Ktend //Let (a; nil) := pop(Kt) primitive action top stack.Execute primitive action a, observe st+1, receive reward R(st+1jst ; a)subtask Kt terminated st+1Let 0 terminated subtask highest (closest root) stack.top(Kt) 6= 0 pop(Kt)pop(Kt)Kt+1 := Kt resulting execution stack.Kt+1 emptyend ExecuteHierarchicalPolicyimmediately aborted, control returns subroutine invokedterminated subroutine.sometimes useful think contents stack additional partstate space problem. Hence, hierarchical policy implicitly defines mappingcurrent state st current stack contents Kt primitive action a. actionexecuted, yields resulting state st+1 resulting stack contents Kt+1 .added state information stack, hierarchical policy non-Markovianrespect original MDP.hierarchical policy maps states stack contents K actions,value function hierarchical policy must assign values combinations statesstack contents K .Definition 4 hierarchical value function, denoted V (hs; K i), gives expected cumu-lative reward following hierarchical policy starting state stack contentsK.hierarchical value function exactly learned Ron Parr's (1998b) HAMQalgorithm, discuss below. However, paper, focus learningprojected value functions subtasks M0 ; : : : ; Mn hierarchy.240fiMAXQ Hierarchical Reinforcement LearningDefinition 5 projected value function hierarchical policy subtask Mi, denotedV (i; s), expected cumulative reward executing (and policies descendentsMi ) starting state Mi terminates.purpose MAXQ value function decomposition decompose V (0; s) (theprojected value function root task) terms projected value functions V (i; s)subtasks MAXQ decomposition.3.3 Decomposition Projected Value Functiondefined hierarchical policy projected value function, showvalue function decomposed hierarchically. decomposition basedfollowing theorem:Theorem 1 Given task graph tasks M0 ; : : : ; Mn hierarchical policy ,subtask Mi defines semi-Markov decision process states Si , actions Ai , probabilitytransition function Pi (s0 ; N js; a), expected reward function R(s; a) = V (a; s),V (a; s) projected value function child task state s. primitiveaction,V (a; s) defined expected immediate reward executing s: V (a; s) =P00s0 P (s js; a)R(s js; a).Proof: Consider subroutines descendents task Mi task graph.subroutines executing fixed policies (specified hierarchical policy), probability transition function Pi (s0 ; N js; a) well defined, stationary distributionchild subroutine a. set states Si set actions Ai obvious.interesting part theorem fact expected reward function R(s; a)SMDP projected value function child task .see this, let us write value V (i; s):V (i; s) = E frt + rt+1 + 2 rt+2 + jst = s; g(5)sum continues subroutine task Mi enters state Ti .let us suppose first action chosen subroutine a. subroutineinvoked, executes number steps N terminates state s0 accordingPi (s0 ; N js; a). rewrite Equation (5)V (i; s) = E(NX,1u=0u rt+u+1Xu=Nfififit+u fifiurst = s;)(6)first summation right-hand side Equation (6) discounted sum rewardsexecuting subroutine starting state terminates, words, V (a; s),projected value function child task . second term right-hand sideequation value s0 current task i, V (i; s0 ), discounted N ,s0 current state subroutine terminates. write formBellman equation:X(7)V (i; s) = V (i (s); s) + Pi (s0 ; N js; (s)) N V (i; s0 )s0 ;N241fiDietterichform Equation (3), Bellman equation SMDP,first term expected reward R(s; (s)). Q.E.D.obtain hierarchical decomposition projected value function, let us switchaction-value (or Q) representation. First, need extend Q notationhandle task hierarchy. Let Q (i; s; a) expected cumulative reward subtaskMi performing action state following hierarchical policy subtaskMi terminates. Action may either primitive action child subtask.notation, re-state Equation (7) follows:Q (i; s; a) = V (a; s) +Xs0 ;NPi (s0 ; N js; a) N Q (i; s0 ; (s0 ));(8)right-most term equation expected discounted reward completing taskMi executing action state s. term depends i, s, a,summation marginalizes away dependence s0 N . Let us define C (i; s; a)equal term:Definition 6 completion function, C (i; s; a), expected discounted cumulativereward completing subtask Mi invoking subroutine subtask state s.reward discounted back point time begins execution.C (i; s; a) =Xs0 ;NPi (s0; N js; a) N Q (i; s0 ; (s0 ))(9)definition, express Q function recursivelyQ (i; s; a) = V (a; s) + C (i; s; a):(10)Finally, re-express definition V (i; s)V (i; s) =((i; s; (s))QcompositeP00s0 P (s js; i)R(s js; i) primitive(11)refer equations (9), (10), (11) decomposition equationsMAXQ hierarchy fixed hierarchical policy . equations recursively decomposeprojected value function root, V (0; s) projected value functionsindividual subtasks, M1 ; : : : ; Mn individual completion functions C (j; s; a)j = 1; : : : ; n. fundamental quantities must stored represent valuefunction decomposition C values non-primitive subtasks V valuesprimitive actions.make easier programmers design debug MAXQ decompositions,developed graphical representation call MAXQ graph. MAXQ graphTaxi domain shown Figure 3. graph contains two kinds nodes, Max nodesQ nodes. Max nodes correspond subtasks task decomposition|thereone Max node primitive action one Max node subtask (includingRoot) task. primitive Max node stores value V (i; s). Q nodes correspondactions available subtask. Q node parent task i, state242fiMAXQ Hierarchical Reinforcement LearningMaxRootQPickupQGetQPutMaxGetMaxPutQNavigateForPutQNavigateForGett/sourceQPutdownt/destinationPickupPutdownMaxNavigate(t)QNorth(t)QEast(t)QSouth(t)QWest(t)NorthEastSouthWestFigure 3: MAXQ graph Taxi Domain.subtask stores value C (i; s; a). children node unordered|thatis, order drawn Figure 3 imply anything orderexecuted. Indeed, child action may executed multiple timesparent subtask completed.addition storing information, Max nodes Q nodes viewed performing parts computation described decomposition equations. Specifically,Max node viewed computing projected value function V (i; s)subtask. primitive Max nodes, information stored node. compositeMax nodes, information obtained \asking" Q node corresponding (s).Q node parent task child task viewed computing valueQ (i; s; a). \asking" child task projected value function V (a; s)adding completion function C (i; s; a).243fiDietterichexample, consider situation shown Figure 1, denote s1 .Suppose passenger R wishes go B. Let hierarchical policyevaluating optimal policy denoted (we omit superscript * reduceclutter notation). value state 10, cost 1unit move taxi R, 1 unit pickup passenger, 7 units move taxi B,1 unit putdown passenger, total 10 units (a reward ,10).passenger delivered, agent gets reward +20, net value +10.Figure 4 shows MAXQ hierarchy computes value. compute valueV (Root; s1 ), MaxRoot consults policy finds Root (s1) Get. Hence, \asks"Q node, QGet compute Q (Root; s1 ; Get). completion cost Root taskperforming Get, C (Root; s1 ; Get), 12, cost 8 units delivercustomer (for net reward 20 , 8 = 12) completing Get subtask. However,reward completing Get, must ask MaxGet estimate expectedreward performing Get itself.policy MaxGet dictates s1 , Navigate subroutine invokedbound R, MaxGet consults Q node, QNavigateForGet compute expectedreward. QNavigateForGet knows completing Navigate(R) task, one action(the Pickup) required complete Get, C (MaxGet; s1 ; Navigate(R)) = ,1.asks MaxNavigate(R) compute expected reward performing Navigatelocation R.policy MaxNavigate chooses North action, MaxNavigate asks QNorthcompute value. QNorth looks completion cost, finds C (Navigate; s1 ; North)0 (i.e., Navigate task completed performing North action). consultsMaxNorth determine expected cost performing North action itself.MaxNorth primitive action, looks expected reward, ,1.series recursive computations conclude follows:Q (Navigate(R); s1 ; North) = ,1 + 0V (Navigate(R); s1 ) = ,1Q (Get; s1 ; Navigate(R)) = ,1 + ,1(,1 perform Navigate plus ,1 complete Get.V (Get; s1) = ,2Q (Root; s1; Get) = ,2 + 12(,2 perform Get plus 12 complete Root task collect final reward).end result value V (Root; s1 ) decomposed sumC terms plus expected reward chosen primitive action:V (Root; s1 ) = V (North; s1 ) + C (Navigate(R); s1 ; North) +C (Get; s1 ; Navigate(R)) + C (Root; s1; Get)= ,1 + 0 + ,1 + 12= 10244fiMAXQ Hierarchical Reinforcement Learning10MaxRoot1012QGetQPut-2MaxGetMaxPut-2QPickupQNavigateForPutQNavigateForGetQPutdown-1-1PickupPutdownMaxNavigate(t)-10QNorth(t)QEast(t)QSouth(t)QWest(t)EastSouthWest-1NorthFigure 4: Computing value state using MAXQ hierarchy. C valueQ node shown left node. numbers show valuesreturned graph.general, MAXQ value function decomposition formV (0; s) = V (am ; s) + C (am,1 ; s; ) + : : : + C (a1 ; s; a2 ) + C (0; s; a1 ); (12)a0 ; a1 ; : : : ; \path" Max nodes chosen hierarchical policy goingRoot primitive leaf node. summarized graphically Figure 5.summarize presentation section following theorem:Theorem 2 Let = fi; = 0; : : : ; ng hierarchical policy defined given MAXQgraph subtasks M0 ; : : : ; Mn ; let = 0 root node graph.exist values C (i; s; a) (for internal Max nodes) V (i; s) (for primitive, leaf Max245fiDietterichV (0X; s)XXXXXXV (a ; s)PPPPP1.V (am,1 ; s). .ZZZZV (am ; s) C (am,1 ; s; )r1r2r3r4r5C (a1 ; s; a2 ). . .r8r9C (0; s; a1 )r10 r11 r12 r13 r14Figure 5: MAXQ decomposition; r1 ; : : : ; r14 denote sequence rewards receivedprimitive actions times 1; : : : ; 14.nodes) V (0; s) (as computed decomposition equations (9), (10), (11))expected discounted cumulative reward following policy starting state s.Proof: proof induction number levels task graph.level i, compute values C (i; s; (s)) (or V (i; s); primitive) accordingdecomposition equations. apply decomposition equations computeQ (i; s; (s)) apply Equation (8) Theorem 1 conclude Q (i; s; (s)) givesvalue function level i. = 0, obtain value function entirehierarchical policy. Q. E. D.important note representation theorem mention pseudoreward function, pseudo-reward used learning. theoremcaptures representational power MAXQ decomposition, addressquestion whether learning algorithm find given policy.subject next section.4. Learning Algorithm MAXQ Decompositionsection presents central contributions paper. First, discuss optimality criteria employed hierarchical reinforcement learning. introduceMAXQ-0 learning algorithm, learn value functions (and policies) MAXQhierarchies pseudo-rewards (i.e., pseudo-rewards zero).central theoretical result paper MAXQ-0 converges recursively optimalpolicy given MAXQ hierarchy. followed brief discussion waysaccelerating MAXQ-0 learning. section concludes description MAXQ-Qlearning algorithm, handles non-zero pseudo-reward functions.246fiMAXQ Hierarchical Reinforcement Learning4.1 Two Kinds Optimalityorder develop learning algorithm MAXQ decomposition, must considerexactly hoping achieve. course, MDP , would like findoptimal policy . However, MAXQ method (and hierarchical reinforcementlearning general), programmer imposes hierarchy problem. hierarchyconstrains space possible policies may possible representoptimal policy value function.MAXQ method, constraints take two forms. First, within subtask,possible primitive actions may permitted. example, taxi task,Navigate(t), North, South, East, West actions available|the PickupPutdown actions allowed. Second, consider Max node Mj child nodesfMj ; : : : ; Mjk g. policy learned Mj must involve executing learned policieschild nodes. policy child node Mji executed, run entersstate Tji . Hence, policy learned Mj must pass subsetterminal state sets fTj ; : : : ; Tjk g.HAM method shares two constraints addition, imposespartial policy node, policy subtask Mi must deterministicrefinement given non-deterministic initial policy node i.\option" approach, policy even constrained. approach,two non-primitive levels hierarchy, subtasks lower level (i.e.,whose children primitive actions) given complete policies programmer.Hence, learned policy upper level must constructed \concatenating"given lower level policies order.purpose imposing constraints policy incorporate prior knowledgethereby reduce size space must searched find good policy.However, constraints may make impossible learn optimal policy.can't learn optimal policy, next best target would learn bestpolicy consistent (i.e., represented by) given hierarchy.11Definition 7 hierarchically optimal policy MDP policy achieveshighest cumulative reward among policies consistent given hierarchy.Parr (1998b) proves HAMQ learning algorithm converges probability 1hierarchically optimal policy. Similarly, given fixed set options, Sutton, Precup,Singh (1998) prove SMDP learning algorithm converges hierarchicallyoptimal value function. Incidentally, also show primitive actions alsomade available \trivial" options, SMDP method converges optimalpolicy. However, case, hard say anything formal options speedlearning process. may fact hinder (Hauskrecht et al., 1998).MAXQ decomposition represent value function hierarchicalpolicy, could easily construct modified version HAMQ algorithm applylearn hierarchically optimal policies MAXQ hierarchy. However, decidedpursue even weaker form optimality, reasons become clear proceed.form optimality called recursive optimality.247fiDietterichMaxRootGQExitQGotoGoalMaxExitMaxGotoGoal**QExitNorthQExitSouthQExitEastNorthQNorthGSouthQSouthGQEastGEastFigure 6: simple MDP (left) associated MAXQ graph (right). policy shownleft diagram recursively optimal hierarchically optimal. shadedcells indicate points locally-optimal policy globally optimal.Definition 8 recursively optimal policy Markov decision process MAXQdecomposition fM0 ; : : : ; Mk g hierarchical policy = f0 ; : : : ; k gsubtask Mi , corresponding policy optimal SMDP defined set statesSi , set actions Ai , state transition probability function P (s0 ; N js; a),reward function given sum original reward function R(s0 js; a) pseudoreward function R~ (s0 ).Note state transition probability distribution, P (s0 ; N js; a) subtask Midefined locally optimal policies fj g subtasks descendents MiMAXQ graph. Hence, recursive optimality kind local optimalitypolicy node optimal given policies children.reason seek recursive optimality rather hierarchical optimality recursive optimality makes possible solve subtask without reference contextexecuted. context-free property makes easier share re-usesubtasks. also turn essential successful use state abstraction.proceed describe learning algorithm recursive optimality, let us seerecursive optimality differs hierarchical optimality.easy construct examples policies recursively optimal hierarchically optimal (and vice versa). Consider simple maze problem associatedMAXQ graph shown Figures 6. Suppose robot starts somewhere left room,must reach goal G right room. robot three actions, North, South,East, actions deterministic. robot receives reward ,1 move.Let us define two subtasks:248fiMAXQ Hierarchical Reinforcement LearningExit. task terminates robot exits left room. set pseudo-reward function R~ 0 two terminal states (i.e., two states indicated*'s).GotoGoal. task terminates robot reaches goal G.arrows Figure 6 show locally optimal policy within room. arrowsleft seek exit left room shortest path, specifiedset pseudo-reward function 0. arrows right follow shortestpath goal, fine. However, resulting policy neither hierarchically optimaloptimal.exists hierarchical policy would always exit left room upperdoor. MAXQ value function decomposition represent value functionpolicy, policy would locally optimal (because, example, states\shaded" region would follow shortest path doorway). Hence,example illustrates recursively optimal policy hierarchically optimalhierarchically optimal policy recursively optimal.consider moment, see way fix problem. valueupper starred state optimal hierarchical policy ,2 value lowerstarred state ,6. Hence, changed R~ values (instead zero),recursively-optimal policy would hierarchically optimal (and globally optimal).words, programmer guess right values terminal statessubtask, recursively optimal policy hierarchically optimal.basic idea first pointed Dean Lin (1995). describe algorithmmakes initial guesses values starred states updatesguesses based computed values starred states resulting recursivelyoptimal policy. proved converge hierarchically optimal policy.drawback method requires repeated solution resulting hierarchicallearning problem, always yield speedup solving original,problem.Parr (1998a) proposed interesting approach constructs set different R~ functions computes recursively optimal policy subtask.method chooses R~ functions way hierarchically optimal policyapproximated desired degree. Unfortunately, method quite expensive,relies solving series linear programming problems requires timepolynomial several parameters, including number states jSi j within subtask.discussion suggests while, principle, possible learn good valuespseudo-reward function, practice, must rely programmer specify singlepseudo-reward function, R~ , subtask. programmer wishes consider smallnumber alternative pseudo-reward functions, handled defining smallnumber subtasks identical except R~ functions, permittinglearning algorithm choose one gives best recursively-optimal policy.experiments, employed following simplified approach definingR~ . subtask Mi, define two predicates: termination predicate, Ti ,goal predicate, Gi . goal predicate defines subset terminal states \goalstates", pseudo-reward 0. terminal states fixed constant249fiDietterichpseudo-reward (e.g., ,100) set always better terminate goal statenon-goal state. problems tested MAXQ method,worked well.experiments MAXQ, found easy make mistakesdefining Ti Gi . goal defined carefully, easy create set subtaskslead infinite looping. example, consider problem Figure 6. Supposepermit fourth action, West, MDP let us define termination goalpredicates right hand room satisfied iff either robot reaches goalexits room. natural definition, since quite similar definitionleft-hand room. However, resulting locally-optimal policy roomattempt move nearest three locations: goal, upper door,lower door. easily see states near goal, policiesconstructed MaxRoot loop forever, first trying leave left roomentering right room, trying leave right room entering left room.problem easily fixed defining goal predicate Gi right room truerobot reaches goal G. avoiding \undesired termination" bugshard complex domains.worst case, possible programmer specify pseudo-rewardsrecursively optimal policy made arbitrarily worse hierarchically optimalpolicy. example, suppose change original MDP Figure 6 stateimmediately left upper doorway gives large negative reward ,L wheneverrobot visits square. rewards everywhere else ,1, hierarchicallyoptimal policy exits room lower door. suppose programmer choseninstead force robot exit upper door (e.g., assigning pseudo-reward,10L leaving via lower door). case, recursively-optimal policy leaveupper door suffer large ,L penalty. making L arbitrarily large,make difference hierarchically-optimal policy recursively-optimalpolicy arbitrarily large.4.2 MAXQ-0 Learning Algorithmunderstanding recursively optimal policies, present two learningalgorithms. first one, called MAXQ-0, applies case pseudo-rewardfunction R~ always zero. first prove convergence properties showextended give second algorithm, MAXQ-Q, works generalpseudo-reward functions.Table 2 gives pseudo-code MAXQ-0. MAXQ-0 recursive function executescurrent exploration policy starting Max node state s. performs actionsreaches terminal state, point returns count total number primitiveactions executed. execute action, MAXQ-0 calls recursively(line 9). recursive call returns, updates value completion functionnode i. uses count number primitive actions appropriately discountvalue resulting state s0 . leaf nodes, MAXQ-0 updates estimated one-stepexpected reward, V (i; s). value fft (i) \learning rate" parametergradually decreased zero limit.250fiMAXQ Hierarchical Reinforcement LearningTable 2: MAXQ-0 learning algorithm.123456789101112131415161718function MAXQ-0(MaxNode i, State s)primitive MaxNodeexecute i, receive r, observe result state s0Vt (i; s) := (1 , fft (i)) Vt (i; s) + fft (i) rtreturn 1elselet count = 0Ti (s) falsechoose action according current exploration policy x(i; s)let N = MAXQ-0(a; 0s) (recursive call)observe result stateCt (i; s; a) := (1 , fft (i)) Ct (i; s; a) + fft (i) N Vt (i; s0 )+1+1count := count + N:= s0endreturn countend MAXQ-0// Main programinitialize V (i; s) C (i; s; j ) arbitrarilyMAXQ-0(root node 0, starting state s0 )three things must specified order make algorithm descriptioncomplete.First, keep pseudo-code readable, Table 2 show \ancestor termination" handled. Recall action, termination predicatessubroutines calling stack checked. termination predicate onesatisfied, calling stack unwound highest terminated subroutine. cases, C values updated subroutines interruptedexcept follows. subroutine invoked subroutine j , j 's termination conditionsatisfied, subroutine update value C (i; s; j ).Second, must specify compute Vt (i; s0 ) line 11, since storedMax node. computed following modified versions decompositionequations:(maxa Qt (i; s; a) composite(13)Vt (i; s)primitiveQt(i; s; a) = Vt(a; s) + Ct (i; s; a):(14)equations ect two important changes compared Equations (10) (11).First, Equation (13), Vt (i; s) defined terms Q value best action a, ratheraction chosen fixed hierarchical policy. Second, superscripts,current value function, Vt (i; s), based fixed hierarchical policy .compute Vt (i; s) using equations, must perform complete searchpaths MAXQ graph starting node ending leaf nodes. Table 3Vt (i; s) =251fiDietterichTable 3: Pseudo-code Greedy Execution MAXQ Graph.function EvaluateMaxNode(i; s)1234567primitive Max nodereturn hVt (i; s); iielsej 2 Ai ,let hVt (j; s); aj = EvaluateMaxNode(j; s)let j hg = argmaxj Vt(j; s) + Ct (i; s; j )return hVt (j hg ; s); ajhgend // EvaluateMaxNodegives pseudo-code recursive function, EvaluateMaxNode, implements depthfirst search. addition returning Vt (i; s), EvaluateMaxNode also returns actionleaf node achieves value. information needed MAXQ-0,useful later consider non-hierarchical execution learned recursivelyoptimal policy.search computationally expensive, problem future researchdevelop ecient methods computing best path graph. Oneapproach perform best-first search use bounds values within subtreesprune useless paths MAXQ graph. better approach would makecomputation incremental, state environment changes,nodes whose values changed result state change re-considered.possible develop ecient bottom-up method similar RETE algorithm (andsuccessors) used SOAR architecture (Forgy, 1982; Tambe & Rosenbloom,1994).third thing must specified complete definition MAXQ-0exploration policy, x . require x ordered GLIE policy.Definition 9 ordered GLIE policy GLIE policy (Greedy Limit InfiniteExploration) converges limit ordered greedy policy, greedy policyimposes arbitrary fixed order ! available actions breaks ties favoraction appears earliest order.need property order ensure MAXQ-0 converges uniquely-definedrecursively optimal policy. fundamental problem recursive optimalitygeneral, Max node choice many different locally optimal policies givenpolicies adopted descendent nodes. different locally optimal policiesachieve locally optimal value function, give rise different probability transition functions P (s0 ; N js; i). result Semi-Markov DecisionProblems defined next level node MAXQ graph differ dependingvarious locally optimal policies chosen node i. differences maylead better worse policies higher levels MAXQ graph, even though makedifference inside subtask i. practice, designer MAXQ graph needdesign pseudo-reward function subtask ensure locally optimal policies252fiMAXQ Hierarchical Reinforcement Learningequally valuable parent subroutine. carry formal analysis,rely arbitrary tie-breaking mechanism.2 establish fixed orderingMax nodes MAXQ graph (e.g., left-to-right depth-first numbering), break tiesfavor lowest-numbered action, defines unique policy Max node.consequently, induction, defines unique policy entire MAXQ graph. Letus call policy r . use r subscript denote recursively optimal quantitiesordered greedy policy. Hence, corresponding value function Vr , CrQr denote corresponding completion function action-value function. proveMAXQ-0 algorithm converges r .Theorem 3 Let = hS; A; P; R; P0 either episodic MDP deterministicpolicies proper discounted infinite horizon MDP discount factor . Let HMAXQ graph defined subtasks fM0 ; : : : ; Mk g pseudo-reward functionR~ (s0 ) zero s0. Let fft (i) > 0 sequence constants Max nodeXXlimff()=1limff2t (i) < 1(15)!1!1t=1t=1Let x (i; s) ordered GLIE policy node state assumeimmediate rewards bounded. probability 1, algorithm MAXQ-0 convergesr , unique recursively optimal policy consistent H x.Proof: proof follows argument similar introduced prove convergenceQ learning SARSA(0) (Bertsekas & Tsitsiklis, 1996; Jaakkola et al., 1994).employ following result stochastic approximation theory, state withoutproof:Lemma 1 (Proposition 4.5 Bertsekas Tsitsiklis, 1996) Consider iterationrt+1 (i) := (1 , fft (i))rt (i) + fft (i)((Urt )(i) + wt (i) + ut(i)):Let Ft = fr0 (i); : : : ; rt (i); w0 (i); : : : ; wt,1 (i); ff0 (i); : : : ; fft (i); 8ig entire historyiteration.(a) fft (i) 0 satisfy conditions (15)(b) every t, noise terms wt (i) satisfy E [wt (i)jFt ] = 0(c) Given norm jj jj Rn , exist constants B E [wt2 (i)jFt ]+ B jjrtjj2 .(d) exists vector r , positive vector , scalar fi 2 [0; 1),t,jjUrt , rjj fi jjrt , rjj2. Alternatively, could break ties using stochastic policy chose randomly among tiedactions.253fiDietterich(e) exists nonnegative random sequence converges zero probability1jut (i)j t(jjrt jj + 1)rt converges r probability 1. notation jj jj denotes weighted maximumnormjA(i)j :jjAjj = max(i)structure proof Theorem 3 inductive, starting leavesMAXQ graph working toward root. employ different time clocknode count number update steps performed MAXQ-0 node.variable always refer time clock current node i.prove base case primitive Max node, note line 3 MAXQ-0standard stochastic approximation algorithm computing expected rewardperforming action state s, therefore converges conditions givenabove.prove recursive case, consider composite Max node child node j . LetPt (s0; N js; j ) transition probability distribution performing child action j statetime (i.e., following exploration policy descendent nodes node j ).inductive assumption, MAXQ-0 applied j converge (unique) recursively optimal value function Vr (j; s) probability 1. Furthermore, MAXQ-0following ordered GLIE policy j descendents, converge executing greedy policy respect value functions, Pt (s0 ; N js; j ) convergePr (s0 ; N js; j ), unique transition probability function executing child jlocally optimal policy r . remains shown update assignment C(line 11 MAXQ-0 algorithm) converges optimal Cr function probability1.prove this, apply Lemma 1. identify x lemmastate-action pair (s; a). vector rt completion-cost table Ct (i; s; a)s; fixed update steps. vector r optimal completion-costCr (i; s; a) (again, fixed i). Define mapping U(UC )(i; s; a) =Xs00 00 0Pr (s0 ; N js; a) N max0 [C (i; ; ) + Vr (a ; )]C update MDP Mi assuming descendent value functions,Vr (a; s), transition probabilities, Pr(s0 ; N js; a), converged.apply lemma, must first express C update formula formupdate rule lemma. Let state results performing state s. Line11 writtenCt+1 (i; s; a) :=(1 , fft (i)) Ct (i; s; a) + fft (i) Nmax[Ct (i; s; a0 ) + Vt (a0 ; s)]a0:= (1 , fft (i)) Ct (i; s; a) + fft (i) [(UCt )(i; s; a) + wt (i; s; a) + ut (i; s; a)]254fiMAXQ Hierarchical Reinforcement Learningwt (i; s; a) = N max[C (i; s; a0 ) + Vt (a0 ; s)] ,a0Xs0 ;Nut (i; s; a) =Xs0 ;NXs0 ;NPt (s0 ; N js; a) N max[C (i; s0 ; a0 ) + Vt (a0 ; s0 )]a00 00 0Pt (s0 ; N js; a) N max0 [Ct (i; ; ) + Vt (a ; )],Pr (s0; N js; a) N max[C (i; s0 ; a0 ) + Vr (a0 ; s0 )]a0wt (i; s; a) difference update node using single samplepoint drawn according Pt (s0 ; N js; a) update using full distributionPt (s0; N js; a). value ut (i; s; a) captures difference update usingcurrent probability transitions Pt (s0 ; N js; a) current value functions childrenVt (a0; s0 ) update using optimal probability transitions Pr (s0 ; N js; a)optimal values children Vr (a0 ; s0 ).verify conditions Lemma 1.Condition (a) assumed conditions theorem fft (s; a) = fft (i).Condition (b) satisfied sampled Pt (s0 ; N js; a), expected valuedifference zero.Condition (c) follows directly fact jCt (i; s; a)j jVt (i; s)j bounded.show bounded episodic case discounted casefollows. episodic case, assumed policies proper. Hence, trajectoriesterminate finite time finite total reward. discounted case, infinite sumfuture rewards bounded one-step rewards bounded. values C Vcomputed temporal averages cumulative rewards received finite number(bounded) updates, hence, means, variances, maximum valuesbounded.Condition (d) condition U weighted max norm pseudo-contraction.derive starting weighted max norm Q learning. well knownQ weighted max norm pseudo-contraction (Bertsekas & Tsitsiklis, 1996)episodic case deterministic policies proper (and discount factor = 1)infinite horizon discounted case (with < 1). is, exists positivevector scalar fi 2 [0; 1), t,jjTQt , Qjj fi jjQt , Qjj ;(16)operator(TQ)(s; a) =Xs0 ;NP (s0; N js; a) N [R(s0 js; a) + maxQ(s0 ; a0 )]:a0show derive pseudo-contraction C update operator U .plan show first express U operator learning C terms operatorupdating Q values. replace TQ pseudo-contraction equation Q255fiDietterichlearning UC , show U weighted max-norm pseudo-contractionweights fi .Recall Eqn. (10) Q(i; s; a) = C (i; s; a) + V (a; s). Furthermore, U operatorperforms updates using optimal value functions child nodes, writeQt (i; s; a) = Ct (i; s; a) + V (a; s). children node converged,Q-function version Bellman equation MDP Mi writtenQ(i; s; a) =Xs0 ;NPr(s0 ; N js; a) N [Vr (a; s) + maxQ(i; s0 ; a0 )]:a0noted before, Vr (a; s) plays role immediate reward function Mi .Therefore, node i, operator rewritten(TQ)(i; s; a) =Xs0 ;NPr (s0 js; a) N [Vr(a; s) + maxQ(i; s0 ; a0 )]:a0replace Q(i; s; a) C (i; s; a) + Vr (a; s), obtain(TQ)(i; s; a) =Xs0 ;NPr (s0; N js; a) N (Vr (a; s) + max[C (i; s0 ; a0 ) + Vr (a0 ; s0 )]):a0Note Vr (a; s) depend s0 N , move outside expectationobtain(TQ)(i; s; a) = Vr (a; s) +Xs0 ;NPr (s0 ; N js; a) N (max[C (i; s0 ; a0 ) + Vr (a0 ; s0 )])a0= Vr (a; s) + (UC )(i; s; a)Abusing notation slightly, express vector form TQ(i) = Vr + UC (i).Similarly, write Qt (i; s; a) = Ct (i; s; a)+ Vr (a; s) vector form Qt (i) = Ct (i)+ Vr .substitute two formulas max norm pseudo-contraction formula, Eqn. (16) obtainjjVr + UCt (i) , (Cr(i) + Vr)jj fi jjVr + Ct (i) , (Cr(i) + Vr)jj :Thus, U weighted max-norm pseudo-contraction,jjUCt (i) , Cr(i)jj fi jjCt (i) , Cr(i)jj ;condition (d) satisfied.Finally, easy verify (e), important condition. assumption,ordered GLIE policies child nodes converge probability 1 locally optimalpolicies children. Therefore Pt (s0 ; N js; a) converges Pr (s0 ; N js; a) s0; N; s;probability 1 Vt (a; s) converges probability 1 Vr (a; s) childactions a. Therefore, jut j converges zero probability 1. trivially constructsequence = jut j bounds convergence,jut (s; a)j (jjCt (s; a)jj + 1):256fiMAXQ Hierarchical Reinforcement Learningverified conditions Lemma 1, conclude Ct (i) convergesCr(i) probability 1. induction, conclude holds nodesMAXQ including root node, value function represented MAXQ graphconverges unique value function recursively optimal policy r . Q.E.D.important aspect theorem proves Q learning takeplace levels MAXQ hierarchy simultaneously|the higher levels needwait lower levels converged begin learning. necessarylower levels eventually converge (locally) optimal policies.4.3 Techniques Speeding MAXQ-0Algorithm MAXQ-0 extended accelerate learning higher nodes graphtechnique call \all-states updating". action chosen Max nodestate s, execution move environment sequence states= s1; : : : ; sN ; sN +1 = s0. subroutines Markovian, resultingstate s0 would reached started executing action state s2 , s3 ,state including sN . Hence, execute version line 11 MAXQ-0intermediate states shown replacement pseudo-code:11aj 1 N11bCt (i; sj ; a) := (1 , fft (i)) Ct(i; sj ; a) + fft (i) N ,j maxa Qt (i; s0 ; a0 )11cend //implementation, composite action executed MAXQ-0, constructslinked list sequence primitive states visited. list returnedcomposite action terminates. parent Max node process statelist shown above. parent Max node concatenates state lists receiveschildren passes parent terminates. experiments paperemploy all-states updating.Kaelbling (1993) introduced related, powerful, method accelerating hierarchical reinforcement learning calls \all-goals updating." understandmethod, suppose primitive action, several composite tasks couldinvoked primitive action. all-goals updating, whenever primitive actionexecuted, equivalent line 11 MAXQ-0 applied every composite task couldinvoked primitive action. Sutton, Precup, Singh (1998) provecomposite tasks converge optimal Q values all-goals updating. Furthermore, point exploration policy employed choosing primitiveactions different policies subtasks learned.straightforward implement simple form all-goals updating within MAXQhierarchy case composite tasks invoke primitive actions. Whenever oneprimitive actions executed state s, update C (i; s; a) value parenttasks invoke a.However, additional care required implement all-goals updating non-primitiveactions. Suppose executing exploration policy, following sequence worldstates actions obtained: s0 ; a0 ; s1 ; : : : ; ak,1 ; sk,1 ; ak ; sk+1 . Let j composite task terminated state sk+1 , let sk,n; ak,n ; : : : ; ak,1 ; ak sequenceactions could executed subtask j children. words, suppose(+1257+1)0fiDietterichpossible \parse" state-action sequence terms series subroutine callsreturns one invocation subtask j . possible parent task invokes j ,update value C (i; sk,n ; j ). course, order updates useful,exploration policy must ordered GLIE policy converge recursivelyoptimal policy subtask j descendents. cannot follow arbitrary explorationpolicy, would produce accurate samples result states drawn accordingP (s0 ; N js; j ). Hence, unlike simple case described Sutton, Precup, Singh,exploration policy cannot different policies subtasks learned.Although considerably reduces usefulness all-goals updating,completely eliminate it. simple way implementing non-primitive all-goals updatingwould perform MAXQ-Q learning usual, whenever subtask j invokedstate returned, could update value C (i; s; j ) potential calling subtasksi. implemented this, however, complexity involved identifyingpossible actual parameters potential calling subroutines.4.4 MAXQ-Q Learning Algorithmshown convergence MAXQ-0, let us design learning algorithmwork arbitrary pseudo-reward functions, R~ (s0 ). could add pseudoreward MAXQ-0, would effect changing MDPdifferent reward function. pseudo-rewards \contaminate" valuescompletion functions computed hierarchy. resulting learned policyrecursively optimal original MDP.problem solved learning one completion function use \inside"Max node separate completion function use \outside" Max node. quantities used \inside" node written tilde: R~ , C~ , Q~ . quantities used\outside" node written without tilde.\outside" completion function, C (i; s; a) completion functiondiscussing far paper. computes expected reward completing taskMi performing action state following learned policy Mi .computed without reference R~ . completion function used parenttasks compute V (i; s), expected reward performing action starting state s.second completion function C~ (i; s; a) completion function use\inside" node order discover locally optimal policy task Mi . functionincorporate rewards \real" reward function, R(s0 js; a),pseudo-reward function, R~ (s0 ). also used EvaluateMaxNode line 6choose best action j hg execute. Note, however, EvaluateMaxNode stillreturn \external" value Vt (j hg ; s) chosen action.employ two different update rules learn two completion functions.C~ function learned using update rule similar Q learning rule line 11MAXQ-0. C function learned using update rule similar SARSA(0)|purpose learn value function policy discovered optimizing C~ .Pseudo-code resulting algorithm, MAXQ-Q shown Table 4.key step lines 15 16. line 15, MAXQ-Q first updates C~ using valuegreedy action, , resulting state. update includes pseudo-reward R~ .258fiMAXQ Hierarchical Reinforcement LearningTable 4: MAXQ-Q learning algorithm.1234567891011121314151617181920212223function MAXQ-Q(MaxNode i, State s)let seq = () sequence states visited executingprimitive MaxNodeexecute i, receive r, observe result state s0Vt (i; s) := (1 , fft (i)) Vt (i; s) + fft (i) rtpush onto beginning seqelselet count = 0Ti (s) falsechoose action according current exploration policy x(i; s)let childSeq = MAXQ-Q(a;s), childSeq sequence states visited+1executing action a. (in reverse order)observe result state s0let = argmaxa [C~t (i; s0 ; a0 ) + Vt (a0 ; s0 )]let N = 1childSeqC~t+1 (i; s; a) := (1 , fft (i)) C~t (i; s; a) + fft (i) N [R~ (s0 ) + C~t (i; s0 ; ) + Vt(a ; s)]Ct+1 (i; s; a) := (1 , fft (i)) Ct (i; s; a) + fft (i) N [Ct (i; s0 ; ) + Vt(a ; s0 )]N := N + 1end //appendchildSeq onto front seq:= s0end //end // elsereturn seqend MAXQ-Q0line 16, MAXQ-Q updates C using greedy action , even wouldgreedy action according \uncontaminated" value function. update,course, include pseudo-reward function.important note whereever Vt (a; s) appears pseudo-code, refers\uncontaminated" value function state executing Max node a.computed recursively exactly way MAXQ-0.Finally, note pseudo-code also incorporates all-states updating, callMAXQ-Q returns list states visited execution,updates lines 15 16 performed states. list statesordered most-recent-first, states updated starting last state visitedworking backward starting state, helps speed algorithm.MAXQ-Q converged, resulting recursively optimal policy computednode choosing action maximizes Q~ (i; s; a) = C~ (i; s; a)+ V (a; s) (breakingties according fixed ordering established ordered GLIE policy).reason gave name \Max nodes" nodes represent subtasks (andlearned policies) within MAXQ graph. Q node j parent node storesC~ (i; s; j ) C (i; s; j ), computes Q~ (i; s; j ) Q(i; s; j ) invoking childMax node j . Max node takes maximum Q values computes eitherV (i; s) computes best action, using Q~ .259fiDietterichCorollary 1 conditions Theorem 3, MAXQ-Q converges uniquerecursively optimal policy MDP defined MAXQ graph H , pseudo-reward functionsR~ , ordered GLIE exploration policy x.Proof: argument identical to, tedious than, proof Theorem 3.proof convergence C~ values identical original proof C values,relies proving convergence \new" C values well, followsweighted max norm pseudo-contraction argument. Q.E.D.5. State Abstractionmany reasons introduce hierarchical reinforcement learning, perhapsimportant reason create opportunities state abstraction. introducedsimple taxi problem Figure 1, pointed within subtask, ignorecertain aspects state space. example, performing MaxNavigate(t),taxi make navigation decisions regardless whether passengertaxi. purpose section formalize conditions safeintroduce state abstractions show convergence proofs MAXQ-Qextended prove convergence presence state abstraction. Specifically,identify five conditions permit \safe" introduction state abstractions.Throughout section, use taxi problem running example,see five conditions permit us reduce number distinct valuesmust stored order represent MAXQ value function decomposition.establish starting point, let us compute number values must storedtaxi problem without state abstraction.MAXQ representation must tables C functions internalnodes V functions leaves. First, six leaf nodes, store V (i; s),must store 500 values node, 500 states; 25 locations, 4 possibledestinations passenger, 5 possible current locations passenger (the fourspecial locations inside taxi itself). Second, root node, two children,requires 2 500 = 1000 values. Third, MaxGet MaxPut nodes, 2actions each, one requires 1000 values, total 2000. Finally, MaxNavigate(t),four actions, must also consider target parameter t, takefour possible values. Hence, effectively 2000 combinations states valuesaction, 8000 total values must represented. total, therefore, MAXQrepresentation requires 14,000 separate quantities represent value function.place number perspective, consider Q learning representation muststore separate value six primitive actions 500 possible states,total 3,000 values. Hence, see without state abstraction, MAXQrepresentation requires four times memory Q table!5.1 Five Conditions Permit State Abstractionintroduce five conditions permit introduction state abstractions.condition, give definition prove lemma states condition satisfied, value function corresponding class policies260fiMAXQ Hierarchical Reinforcement Learningrepresented abstractly (i.e., abstract versions V C functions). condition, provide rules identifying condition satisfiedgive examples taxi domain.begin introducing definitions notation.Definition 10 Let MDP H MAXQ graph defined . Supposestate written vector values set state variables. Maxnode i, suppose state variables partitioned two sets Xi Yi , letfunction projects state onto values variables Xi . H combinedcalled state-abstracted MAXQ graph.cases state variables partitioned, often write = (x; y)mean state represented vector values state variables Xvector values state variables . Similarly, sometimes writeP (x0 ; y0; N jx; y; a), V (a; x; y), R~ (x0 ; y0 ) place P (s0; N js; a), V (a; s), R~ (s0 ),respectively.Definition 11 (Abstract Policy) abstract hierarchical policy MDP state-abstracted MAXQ graph H associated abstraction functions , hierarchical policypolicy (corresponding subtask Mi ) satisfies condition twostates s1 s2 (s1 ) = (s2 ), (s1 ) = (s2 ). (When stochastic policy,exploration policy, interpreted mean probability distributionschoosing actions states.)order MAXQ-Q converge presence state abstractions, requiretimes (instantaneous) exploration policy abstract hierarchical policy.One way achieve construct exploration policy uses information relevant state variables deciding action perform. Boltzmann exploration based (state-abstracted) Q values, -greedy exploration, counter-basedexploration based abstracted states abstract exploration policies. Counter-basedexploration based full state space abstract exploration policy.introduced notation, let us describe analyze five abstraction conditions. identified three different kinds conditionsabstractions introduced. first kind involves eliminating irrelevant variableswithin subtask MAXQ graph. form abstraction, nodes towardleaves MAXQ graph tend relevant variables, nodes highergraph relevant variables. Hence, kind abstraction usefullower levels MAXQ graph.second kind abstraction arises \funnel" actions. macro actionsmove environment large number initial states small numberresulting states. completion cost subtasks represented using numbervalues proportional number resulting states. Funnel actions tend appear higherMAXQ graph, form abstraction useful near root graph.third kind abstraction arises structure MAXQ graph itself.exploits fact large parts state space subtask may reachabletermination conditions ancestors MAXQ graph.261fiDietterichbegin describing two abstraction conditions first type. presenttwo conditions second type. finally, describe one condition third type.5.1.1 Condition 1: Max Node Irrelevancefirst condition arises set state variables irrelevant Max node.Definition 12 (Max Node Irrelevance) Let Mi Max node MAXQ graph HMDP . set state variables irrelevant node state variablespartitioned two sets X stationary abstract hierarchicalpolicy executed descendents i, following two properties hold:state transition probability distribution P (s0; N js; a) node factoredproduct two distributions:P (x0 ; y0 ; N jx; y; a) = P (y0jx; y; a) P (x0 ; N jx; a);(17)y0 give values variables , x x0 give valuesvariables X .pair states s1 = (x; y1 ) s2 = (x; y2 ) (s1) = (s2) = x,child action a, V (a; s1 ) = V (a; s2 ) R~ (s1 ) = R~ (s2 ).Note two conditions must hold stationary abstract policies executeddescendents subtask i. discuss rather strongrequirements satisfied practice. First, however, prove conditionssucient permit C V tables represented using state abstractions.Lemma 2 Let MDP full-state MAXQ graph H , suppose state vari-ables Yi irrelevant Max node i. Let (s) = x associated abstraction functionprojects onto remaining relevant variables Xi . Let abstract hierarchicalpolicy. action-value function Q node represented compactly,one value completion function C (i; s; j ) equivalence class statesshare values relevant variables.Specifically Q (i; s; j ) computed follows:Q (i; s; j ) = V (j; (s)) + C (i; (s); j )C (i; x; j ) =Xx0 ;NP (x0 ; N jx; j ) N [V ((x0 ); x0 ) + R~ (x0 ) + C (i; x0 ; (x0 ))];V (j 0 ; x0 ) = V (j 0 ; x0 ; y0 ), R~ (x0 ) = R~ (x0 ; y0 ), (x) = (x; y0 ) arbitraryvalue y0 irrelevant state variables Yi .262fiMAXQ Hierarchical Reinforcement LearningProof: Define new MDP i(Mi ) node follows:States: X = fx j i(s) = x; 2 g.Actions: A.Transition probabilities: P (x0 ; N jx; a)Reward function: V (a; x) + R~ i(x0)abstract policy, decisions states (s) = xx. Therefore, also well-defined policy (Mi ). action-value function(Mi ) unique solution following Bellman equation:XQ (i; x; j ) = V (j; x) + P (x0 ; N jx; j ) N [R~ i(x0 ) + Q (i; x0 ; (x0 ))](18)x0 ;NCompare Bellman equation Mi :XQ (i; s; j ) = V (j; s) + P (s0; N js; j ) N [R~i (s0) + Q (i; s0 ; (s0 ))]s0 ;N(19)note V (j; s) = V (j; (s)) = V (j; x) R~ (s0 ) = R~ ((s0 )) = R~ (x0 ). Furthermore, know distribution P factored separate distributions YiXi . Hence, rewrite (19)XXQ (i; s; j ) = V (j; x) + P (y0 jx; y; j ) P (x0 ; N jx; j ) N [R~ (x0 ) + Q (i; s0 ; (s0 ))]y0x0 ;Nright-most sum depend y0 , sum y0 evaluates 1,eliminated giveXQ (i; s; j ) = V (j; x) + P (x0 ; N jx; j ) N [R~ i(x0 ) + Q (i; s0 ; (s0 ))]:(20)x0 ;NFinally, note equations (18) (20) identical except expressionsQ values. Since solution Bellman equation unique, must concludeQ (i; s; j ) = Q (i; (s); j ):rewrite right-hand side obtainQ (i; s; j ) = V (j; (s)) + C (i; (s); j );XC (i; x; j ) = P (x0; N jx; j ) N [V ((x0 ); x0 ) + R~ (x0 ) + C (i; x0 ; (x0 ))]:Q.E.D.x0 ;Ncourse primarily interested able discover represent optimalpolicy node i. following corollary shows optimal policy abstractpolicy, hence, represented abstractly.263fiDietterichCorollary 2 Consider conditions Lemma 2, change ab-stract hierarchical policy executed descendents node i, nodei. Let ! ordering actions. optimal ordered policy ! nodeabstract policy, action-value function represented abstractly.Proof: Define policy ! optimal ordered policy abstract MDP(M ), let Q (i; x; j ) corresponding optimal action-value function.argument given above, Q also solution optimal Bellman equationoriginal MDP. means policy ! defined ! (s) = ((s)) optimalordered policy, construction, abstract policy. Q.E.D.stated, Max node irrelevance condition appears quite dicult satisfy, sincerequires state transition probability distribution factor X componentspossible abstract hierarchical policies. However, practice, condition oftensatisfied.example, let us consider Navigate(t) subtask. source destinationpassenger irrelevant achievement subtask. policy successfully completes subtask value function regardless sourcedestination locations passenger. abstracting away passenger source destination, obtain huge savings space. Instead requiring 8000 values representC functions task, require 400 values (4 actions, 25 locations, 4 possiblevalues t).advantages form abstraction similar obtained Boutilier,Dearden Goldszmidt (1995) belief network models actions exploitedsimplify value iteration stochastic planning. Indeed, one way understandingconditions Definition 12 express form decision diagram, shownFigure 7. diagram shows irrelevant variables affect rewardseither directly indirectly, therefore, affect either value functionoptimal policy.One rule noticing cases abstraction condition holds examinesubgraph rooted given Max node i. set state variables irrelevant leafstate transition probabilities reward functions also pseudo-reward functionstermination conditions subgraph, variables satisfy Max NodeIrrelevance condition:Lemma 3 Let MDP associated MAXQ graph H , let Max nodeH . Let Xi Yi partition state variables . set state variables Yiirrelevant nodeprimitive leaf node descendent i,P (x0 ; y0 jx; y; a) = P (y0 jx; y; a)P (x0 jx; a)R(x0; y0 jx; y; a) = R(x0 jx; a),internal node j equal node descendent , R~ j (x0 ; y0) =R~j (x0) termination predicate Tj (x0 ; y0 ) true iff Tj (x0).264fiMAXQ Hierarchical Reinforcement LearningjVXXFigure 7: dynamic decision diagram represents conditions Definition 12.probabilistic nodes X represent state variables time t, nodesX 0 0 represent state variables later time + N . square actionnode j chosen child subroutine, utility node V represents valuefunction V (j; x) child action. Note X may uence 0 ,cannot affect X 0 , therefore, cannot affect V .Proof: must show abstract hierarchical policy give rise SMDPnode whose transition probability distribution factors whose reward function dependsXi . definition, abstract hierarchical policy choose actions basedupon information Xi . primitive probability transition functions factorindependent component Xi since termination conditions nodesbased variables Xi , probability transition function Pi (x0 ; y0 ; N jx; y; a)must also factor Pi (y0 jx; y; a) Pi (x0 ; N jx; a). Similarly, reward functionsV (j; x; y) must equal V (j; x), rewards received within subtree (eitherleaves pseudo-rewards) depend variables Xi . Therefore,variables Yi irrelevant Max node i. Q.E.D.Taxi task, primitive navigation actions, North, South, East, Westdepend location taxi location passenger. pseudoreward function termination condition MaxNavigate(t) node dependlocation taxi (and parameter t). Hence, lemma applies, passengersource destination irrelevant MaxNavigate node.5.1.2 Condition 2: Leaf Irrelevancesecond abstraction condition describes situations apply state abstractions leaf nodes MAXQ graph. leaf nodes, obtain stronger resultLemma 2 using slightly weaker definition irrelevance.265fiDietterichDefinition 13 (Leaf Irrelevance) set state variables irrelevant primitiveaction MAXQ graph states expected value reward function,XV (a; s) = P (s0 js; a)R(s0 js; a)s0depend values state variables . words,pair states s1 s2 differ values variables ,Xs01P (s01 js1 ; a)R(s01 js1 ; a) =Xs02P (s02 js2 ; a)R(s02 js2 ; a):condition satisfied leaf a, following lemma showsrepresent value function V (a; s) compactly.Lemma 4 Let MDP full-state MAXQ graph H , suppose state vari-ables irrelevant leaf node a. Let (s) = x associated abstraction functionprojects onto remaining relevant variables X . represent V (a; s)state abstracted value function V (a; (s)) = V (a; x).Proof: According definition Leaf Irrelevance, two states differirrelevant state variables value V (a; s). Hence, representunique value V (a; x). Q.E.D.two rules finding cases Leaf Irrelevance applies. first rule showsprobability distribution factors, Leaf Irrelevance.Lemma 5 Suppose probability transition function primitive action a, P (s0js; a), factors P (x0 ; y0 jx; y; a) = P (y0 jx; y; a)P (x0 jx; a) reward function satisfies R(s0 js; a) =R(x0jx; a). variables irrelevant leaf node a.Proof: Plug definition V (a; s) simplify.XV (a; s) =P (s0 js; a)R(s0 js; a)===s0Xx0 ;y0Xy0Xx0P (y0 jx; y; a)P (x0 jx; a)R(x0 jx; a)P (y0 jx; y; a)Xx0P (x0 jx; a)R(x0 jx; a)P (x0 jx; a)R(x0 jx; a)Hence, expected reward action depends variables Xvariables . Q.E.D.second rule shows reward function primitive action constant,apply state abstractions even P (s0 js; a) factor.Lemma 6 Suppose R(s0js; a) (the reward function action MDP ) always equalconstant ra . entire state irrelevant primitive action a.266fiMAXQ Hierarchical Reinforcement LearningProof:V (a; s) =Xs0P (s0 js; a)R(s0 js; a)X=P (s0 js; a)ra0= ra :depend s, entire state irrelevant primitive action a. Q.E.D.lemma satisfied four leaf nodes North, South, East, West taxitask, one-step reward constant (,1). Hence, instead requiring 2000values store V functions, need 4 values|one action. Similarly,expected rewards Pickup Putdown actions require 2 values, dependingwhether corresponding actions legal illegal. Hence, together, require 4values, instead 1000 values.5.1.3 Condition 3: Result Distribution Irrelevanceconsider condition results \funnel" actions.Definition 14 (Result Distribution Irrelevance). set state variables Yj irrelevant result distribution action j if, abstract policies executed node jdescendents MAXQ hierarchy, following holds: pairs states s1s2 differ values state variables Yj ,P (s0 ; N js1 ; j ) = P (s0 ; N js2 ; j )s0 N .condition satisfied subtask j , C value parent taskrepresented compactly:Lemma 7 Let MDP full-state MAXQ graph H , suppose setstate variables Yj irrelevant result distribution action j , child Maxnode i. Let ij associated abstraction function: ij (s) = x. defineabstract completion cost function C (i; ij (s); j ) states s,C (i; s; j ) = C (i; ij (s); j ):Proof: completion function fixed policy defined follows:XC (i; s; j ) = P (s0 ; N js; j ) N Q (i; s0 ):s0 ;N(21)Consider two states s1 s2 , ij (s1 ) = ij (s2 ) = x. Result Distribution Irrelevance, transition probability distributions same. Hence,right-hand sides (21) value, concludeC (i; s1 ; j ) = C (i; s2 ; j ):267fiDietterichTherefore, define abstract completion function, C (i; x; j ) represent quantity. Q.E.D.undiscounted cumulative reward problems, definition result distribution irrelevance weakened eliminate N , number steps. neededpairs states s1 s2 differ irrelevant state variables,P (s0 js1 ; j ) = P (s0 js2; j ) (for s0 ). undiscounted case, Lemma 7 still holdsrevised definition.might appear result distribution irrelevance condition would rarely satisfied, often find cases condition true. Consider, example, Getsubroutine taxi task. matter location taxi state s, taxipassenger's starting location Get finishes executing (i.e.,taxi completed picking passenger). Hence, starting locationirrelevant resulting location taxi, P (s0 js1 ; Get) = P (s0 js2 ; Get)states s1 s2 differ taxi's location.Note, however, maximizing discounted reward, taxi's location wouldirrelevant, probability Get terminate exactly N steps woulddepend location taxi, could differ states s1 s2 . Different valuesN produce different amounts discounting (21), hence, cannot ignoretaxi location representing completion function Get.undiscounted case, applying Lemma 7, represent C (Root; s; Get)using 16 distinct values, 16 equivalence classes states (4 source locationstimes 4 destination locations). much less 500 quantities unabstractedrepresentation.Note although state variables may irrelevant result distributionsubtask j , may important within subtask j . Taxi task, locationtaxi critical representing value V (Get; s), irrelevant result statedistribution Get, therefore irrelevant representing C (Root; s; Get). Hence,MAXQ decomposition essential obtaining benefits result distribution irrelevance.\Funnel" actions arise many hierarchical reinforcement learning problems. example, abstract actions move robot doorway move car onto entranceramp freeway property. Result Distribution Irrelevance conditionapplicable situations long undiscounted setting.5.1.4 Condition 4: Terminationfourth condition closely related \funnel" property. applies subtaskguaranteed cause parent task terminate goal state. sense, subtaskfunneling environment set states described goal predicateparent task.Lemma 8 (Termination). Let Mi task MAXQ graph statesgoal predicate Gi (s) true, pseudo-reward function R~ (s) = 0. Supposechild task state hierarchical policies ,8 s0 Pi (s0; N js; a) > 0 ) Gi(s0 ):268fiMAXQ Hierarchical Reinforcement Learning(i.e., every possible state s0 results applying make goal predicate,Gi , true.)policy executed node i, completion cost C (i; s; a) zeroneed explicitly represented.Proof: action executed state s, guaranteed result state s0Gi (s) true. definition, goal states also satisfy termination predicate Ti (s),task terminate. Gi(s) true, terminal pseudo-reward zero,hence, completion function always zero. Q.E.D.example, Taxi task, states taxi holding passenger,Put subroutine succeed result goal terminal state Root.termination predicate Put (i.e., passenger destination location)implies goal condition Root (which same). means C (Root; s; Put)uniformly zero, states Put terminated.easy detect cases Termination condition satisfied. needcompare termination predicate Ta subtask goal predicate Gi parenttask. first implies second, termination lemma satisfied.5.1.5 Condition 5: Shieldingshielding condition arises structure MAXQ graph.Lemma 9 (Shielding). Let Mi task MAXQ graph statepaths root graph node Mi subtask j (possibly equal i)whose termination predicate Tj (s) true, Q nodes Mi need representC values state s.Proof: order task executed state s, must exist path ancestorstask leading root graph ancestor tasksterminated. condition lemma guarantees false, hence taskcannot executed state s. Therefore, C values need represented. Q.E.D.Termination condition, Shielding condition verified analyzingstructure MAXQ graph identifying nodes whose ancestor tasks terminated.Taxi domain, simple example arises Put task, terminatedstates passenger taxi. means needrepresent C (Root; s; Put) states. result that, combinedTermination condition above, need explicitly represent completion functionPut all!5.1.6 Dicussionapplying five abstraction conditions, obtain following \safe" state abstractions Taxi task:North, South, East, West. terminal nodes require one quantity each,total four values. (Leaf Irrelevance).269fiDietterichPickup Putdown require 2 values (legal illegal states), total four.(Leaf Irrelevance.)QNorth(t), QSouth(t), QEast(t), QWest(t) require 100 values (four values25 locations). (Max Node Irrelevance.)QNavigateForGet requires 4 values (for four possible source locations). (The passenger destination Max Node Irrelevant MaxGet, taxi starting locationResult Distribution Irrelevant Navigate action.)QPickup requires 100 possible values, 4 possible source locations 25 possible taxilocations. (Passenger destination Max Node Irrelevant MaxGet.)QGet requires 16 possible values (4 source locations, 4 destination locations). (ResultDistribution Irrelevance.)QNavigateForPut requires 4 values (for four possible destination locations).(The passenger source destination Max Node Irrelevant MaxPut; taxilocation Result Distribution Irrelevant Navigate action.)QPutdown requires 100 possible values (25 taxi locations, 4 possible destination locations). (Passenger source Max Node Irrelevant MaxPut.)QPut requires 0 values. (Termination Shielding.)gives total 632 distinct values, much less 3000 values requiredQ learning. Hence, see applying state abstractions, MAXQrepresentation give much compact representation value function.key thing note state abstractions, value function decomposed sum terms single term depends entire state MDP,even though value function whole depend entire state MDP.example, consider state described Figures 1 4. There, showedvalue state s1 passenger R, destination B, taxi (0,3)decomposedV (Root; s1 ) = V (North; s1 ) + C (Navigate(R); s1 ; North) +C (Get; s1 ; Navigate(R)) + C (Root; s1 ; Get)state abstractions, see term right-hand side dependssubset features:V (North; s1) constantC (Navigate(R); s1 ; North) depends taxi location passenger's sourcelocation.C (Get; s1; Navigate(R)) depends source location.C (Root; s1 ; Get) depends passenger's source destination.270fiMAXQ Hierarchical Reinforcement LearningWithout MAXQ decomposition, features irrelevant, value function depends entire state.prior knowledge required part programmer order identifystate abstractions? suces know qualitative constraints one-stepreward functions, one-step transition probabilities, termination predicates, goalpredicates, pseudo-reward functions within MAXQ graph. Specifically, MaxNode Irrelevance Leaf Irrelevance conditions require simple analysis one-steptransition function reward pseudo-reward functions. Opportunities applyResult Distribution Irrelevance condition found identifying \funnel" effectsresult definitions termination conditions operators. Similarly,Shielding Termination conditions require analysis termination predicatesvarious subtasks. Hence, applying five conditions introduce state abstractionsstraightforward process, model one-step transition reward functionslearned, abstraction conditions checked see satisfied.5.2 Convergence MAXQ-Q State Abstractionshown state abstractions safely introduced MAXQ valuefunction decomposition five conditions described above. However, conditions guarantee value function fixed abstract hierarchical policyrepresented|they show recursively optimal policies represented,show MAXQ-Q learning algorithm find recursively optimal policyforced use state abstractions. goal section prove tworesults: (a) ordered recursively-optimal policy abstract policy (and, hence,represented using state abstractions) (b) MAXQ-Q convergepolicy applied MAXQ graph safe state abstractions.Lemma 10 Let MDP full-state MAXQ graph H abstract-state MAXQgraph (H ) abstractions satisfy five conditions given above. Let !ordering actions MAXQ graph. following statements true:unique ordered recursively-optimal policy r defined , H , ! abstract policy (i.e., depends relevant state variables node; seeDefinition 11),C V functions (H ) represent projected value function r.Proof: five abstraction lemmas tell us ordered recursively-optimal policyabstract, C V functions (H ) represent value function. Hence,heart lemma first claim. last two forms abstraction (ShieldingTermination) place restrictions abstract policies, ignoreproof.proof induction levels MAXQ graph, starting leaves.base case, let us consider Max node whose children primitive actions.case, policies executed within children Max node. Hence variablesYi irrelevant node i, apply abstraction lemmas representvalue function policy node i|not abstract policies. Consequently, value271fiDietterichfunction optimal policy node represented, propertyQ(i; s1 ; a) = Q (i; s2 ; a)(22)states s1 s2 (s1 ) = (s2 ).let us impose action ordering ! compute optimal ordered policy. Considertwo actions a1 a2 !(a1 ; a2 ) (i.e., ! prefers a1 ), suppose\tie" Q function state s1 valuesQ (i; s1 ; a1 ) = Q (i; s1 ; a2 )two actions maximize Q state. optimal orderedpolicy must choose a1 . states s2 (s1 ) = (s2 ),established (22) Q values same. Hence, tie exista1 a2 , hence, optimal ordered policy must make choicestates. Hence, optimal ordered policy node abstract policy.let us turn recursive case Max node i. Make inductive assumptionordered recursively-optimal policy abstract within descendent nodes considerlocally optimal policy node i. set state variables irrelevantnode i, Corollary 2 tells us Q (i; s1 ; j ) = Q (i; s2 ; j ) states s1 s2i(s1) = (s2 ). Similarly, set variables irrelevant result distributionparticular action j , Lemma 7 tells us thing. Hence, orderingargument given above, ordered optimal policy node must abstract. induction,proves lemma. Q.E.D.lemma, established combination MDP , abstractMAXQ graph H , action ordering defines unique recursively-optimal ordered abstract policy. ready prove MAXQ-Q converge policy.Theorem 4 Let = hS; A; P; R; P0 either episodic MDP deterministicpolicies proper discounted infinite horizon MDP discount factor < 1. Let Hunabstracted MAXQ graph defined subtasks fM0 ; : : : ; Mk g pseudo-rewardfunctions R~ (s0 ). Let (H ) state-abstracted MAXQ graph defined applying stateabstractions node H five conditions given above. Let x (i; (s))abstract ordered GLIE exploration policy node state whose decisionsdepend \relevant" state variables node i. Let r unique recursivelyoptimal hierarchical policy defined x , , R~ . probability 1, algorithmMAXQ-Q applied (H ) converges r provided learning rates fft (i) satisfyEquation (15) one-step rewards bounded.Proof: Rather repeating entire proof MAXQ-Q, describemust change state abstraction. last two forms state abstraction refer stateswhose values inferred structure MAXQ graph, thereforeneed represented all. Since values updated MAXQ-Q,ignore them. consider first three forms state abstraction turn.begin considering primitive leaf nodes. Let leaf node let setstate variables Leaf Irrelevant a. Let s1 = (x; y1 ) s2 = (x; y2 ) two states272fiMAXQ Hierarchical Reinforcement Learningdiffer values . Leaf Irrelevance, probability transitionsP (s01 js1 ; a) P (s02 js2 ; a) need same, expected reward performingstates must same. MAXQ-Q visits abstract state x,\know" value y, part state abstracted away. Nonetheless,draws sample according P (s0 jx; y; a), receives reward R(s0 jx; y; a), updatesestimate V (a; x) (line 4 MAXQ-Q). Let Pt (y) probability MAXQ-Qvisiting (x; y) given unabstracted part state x. Line 4 MAXQ-Qcomputing stochastic approximationXs0 ;N;ywriteXPt (y)Pt (s0 ; N jx; y; a)R(s0 jx; y; a):Pt (y)Xs0 ;NPt (s0 ; N jx; y; a)R(s0 jx; y; a):According Leaf Irrelevance, inner sum value states(s) = x. Call value r0 (x). givesXPt (y)r0 (x);equal r0 (x) distribution Pt (y). Hence, MAXQ-Q converges LeafIrrelevance abstractions.let us turn two forms abstraction apply internal nodes: Max NodeIrrelevance Result Distribution Irrelevance. Consider SMDP defined nodeabstracted MAXQ graph time MAXQ-Q. would ordinary SMDPtransition probability function Pt (x0 ; N jx; a) reward function Vt (a; x) + R~ (x0 )except MAXQ-Q draws samples state transitions, drawn accordingdistribution Pt (s0 ; N js; a) original state space. prove theorem, mustshow drawing (s0 ; N ) according second distribution equivalent drawing(x0 ; N ) according first distribution.Max Node Irrelevance, know abstract policies applied nodedescendents, transition probability distribution factorsP (s0 ; N js; a) = P (y0 jx; y; a)P (x0 ; N jx; a):exploration policy abstract policy, Pt (s0 ; N js; a) factors way.means Yi components state cannot affect Xi components, hence,sampling Pt (s0 ; N js; a) discarding Yi values gives samples Pt (x0 ; N jx; a).Therefore, MAXQ-Q converge Max Node Irrelevance abstractions.Finally, consider Result Distribution Irrelevance. Let j child node i, supposeYj set state variables irrelevant result distribution j .SMDP node wishes draw sample Pt (x0 ; N jx; j ), \know"current value y, irrelevant part current state. However,matter, Result Distribution Irrelevance means possible values y,Pt (x0 ; y0; N jx; y; j ) same. Hence, MAXQ-Q converge Result DistributionIrrelevance abstractions.273fiDietterichthree cases, MAXQ-Q converge locally-optimal ordered policynode MAXQ graph. Lemma 10, produces locally-optimal orderedpolicy unabstracted SMDP node i. Hence, induction, MAXQ-Q convergeunique ordered recursively optimal policy r defined MAXQ-Q H , MDP ,ordered exploration policy x . Q.E.D.5.3 Hierarchical Credit Assignment Problemstill situations would like introduce state abstractionsfive properties described permit them. Consider followingmodification taxi problem. Suppose taxi fuel tank timetaxi moves one square, costs one unit fuel. taxi runs fueldelivering passenger destination, receives reward ,20, trialends. Fortunately, filling station taxi execute Fillup action fillfuel tank.solve modified problem using MAXQ hierarchy, introduce anothersubtask, Refuel, goal moving taxi filling station fillingtank. MaxRefuel child MaxRoot, invokes Navigate(t) (with boundlocation filling station) move taxi filling station.introduction fuel possibility might run fuel meansmust include current amount fuel feature representing every C value(for internal nodes) V value (for leaf nodes) throughout MAXQ graph.unfortunate, intuition tells us amount fuel uencedecisions inside Navigate(t) subtask. is, either taxi enoughfuel reach target (in case, chosen navigation actions dependfuel), else taxi enough fuel, hence, fail reach regardlessnavigation actions taken. words, Navigate(t) subtaskneed worry amount fuel, even enough fuel,action Navigate(t) take get fuel. Instead, top-level subtasksmonitoring amount fuel deciding whether go refuel, go pickpassenger, go deliver passenger.Given intuition, natural try abstracting away \amount remainingfuel" within Navigate(t) subtask. However, doesn't work, taxiruns fuel ,20 reward given, QNorth, QSouth, QEast, QWest nodescannot \explain" reward received|that is, consistent waysetting C tables predict negative reward occur, Cvalues ignore amount fuel tank. Stated formally, dicultyMax Node Irrelevance condition satisfied one-step reward functionR(s0js; a) actions depends amount fuel.call hierarchical credit assignment problem. fundamental issueMAXQ decomposition information rewards stored leaf nodeshierarchy. would like separate basic rewards received navigation(i.e., ,1 action) reward received exhausting fuel (,20). makereward leaves depend location taxi, Max Node Irrelevancecondition satisfied.274fiMAXQ Hierarchical Reinforcement LearningOne way programmer manually decompose reward functionindicate nodes hierarchy \receive" reward. Let R(s0 js; a) =P00R(i; js; a) decomposition reward function, R(i; js; a) specifiespart reward must handled Max node i. modified taxi problem,example, decompose reward leaf nodes receive originalpenalties, out-of-fuel rewards must handled MaxRoot. Lines 15 16MAXQ-Q algorithm easily modified include R(i; s0 js; a).domains, believe easy designer hierarchy decomposereward function. straightforward problems studied.However, interesting problem future research develop algorithmsolve hierarchical credit assignment problem autonomously.6. Non-Hierarchical Execution MAXQ Hierarchypoint paper, focused exclusively representing learninghierarchical policies. However, often optimal policy MDP strictly hierarchical. Kaelbling (1993) first introduced idea deriving non-hierarchical policyvalue function hierarchical policy. section, exploit MAXQ decompositiongeneralize ideas apply recursively levels hierarchy.describe two methods non-hierarchical execution.first method based dynamic programming algorithm known policyiteration. policy iteration algorithm starts initial policy 0 . repeatsfollowing two steps policy converges. policy evaluation step, computesvalue function V k current policy k . Then, policy improvement step,computes new policy, k+1 according rulek+1(s) := argmaxXs0P (s0 js; a)[R(s0 js; a) + V k (s0 )]:(23)Howard (1960) proved k optimal policy, k+1 guaranteedimprovement. Note order apply method, need know transitionprobability distribution P (s0 js; a) reward function R(s0 js; a).know P (s0 js; a) R(s0 js; a), use MAXQ representation valuefunction perform one step policy iteration. start hierarchical policyrepresent value function using MAXQ hierarchy (e.g., could learned viaMAXQ-Q). Then, perform one step policy improvement applying Equation (23)using V (0; s0 ) (computed MAXQ hierarchy) compute V (s0 ).Corollary 3 Let g (s) = argmaxa Ps0 P (s0js; a)[R(s0 js; a) + V (0; s)], V (0; s)value function computed MAXQ hierarchy primitive action. Then,optimal policy, g strictly better least one state .Proof: direct consequence Howard's policy improvement theorem. Q.E.D.Unfortunately, can't iterate policy improvement process, new policy,g unlikely hierarchical policy (i.e., unlikely representable275fiDietterichTable 5: procedure executing one-step greedy policy.procedure ExecuteHGPolicy(s)1repeat2Let hV (0; s); ai := EvaluateMaxNode(0; s)34execute primitive actionLet resulting stateend // ExecuteHGPolicyterms local policies node MAXQ graph). Nonetheless, one step policyimprovement give significant improvements.approach non-hierarchical execution ignores internal structure MAXQgraph. effect, MAXQ hierarchy viewed way represent V |anyrepresentation would give one-step improved policy g .second approach non-hierarchical execution borrows idea Q learning.One great beauties Q representation value functions computeone step policy improvement without knowing P (s0 js; a), simply taking new policyg (s) := argmaxa Q(s; a). gives us one-step greedy policycomputed using one-step lookahead. MAXQ decomposition, performpolicy improvement steps levels hierarchy.already defined function need. Table 3 presented functionEvaluateMaxNode, which, given current state s, conducts search along pathsgiven Max node leaves MAXQ graph finds pathbest value (i.e., maximum sum C values along path, plus V valueleaf). equivalent computing best action greedily levelMAXQ graph. addition, EvaluateMaxNode returns primitive action endbest path. action would first primitive action executedlearned hierarchical policy executed starting current state s. second methodnon-hierarchical execution MAXQ graph call EvaluateMaxNodestate, execute primitive action returned. pseudo-code shownTable 5.call policy computed ExecuteHGPolicy hierarchical greedy policy,denote hg , superscript * indicates computing greedyaction time step. following theorem shows give better policyoriginal, hierarchical policy.Theorem 5 Let G MAXQ graph representing value function hierarchical policy(i.e., terms C (i; s; j ), computed i; s, j ). Let V hg (0; s) valuecomputed ExecuteHGPolicy (line 2), let hg resulting policy. DefineV hg value function hg. states s, caseV (s) V hg (0; s) V hg (s):(24)Proof: (sketch) left inequality Equation (24) satisfied construction line 6EvaluateMaxNode. see this, consider original hierarchical policy, ,276fiMAXQ Hierarchical Reinforcement Learningviewed choosing \path" MAXQ graph running root oneleaf nodes, V (0; s) sum C values along chosen path (plusV value leaf node). contrast, EvaluateMaxNode performs traversalpaths MAXQ graph finds best path, is, path largestsum C (and leaf V ) values. Hence, V hg (0; s) must least large V (0; s).establish right inequality, note construction V hg (0; s) value functionpolicy, call hg , chooses one action greedily level MAXQ graph(recursively), follows thereafter. consequence fact line6 EvaluateMaxNode C right-hand side, C represents cost\completing" subroutine following , following other, greedier, policy.(In Table 3, C written Ct .) However, execute ExecuteHGPolicy (andhence, execute hg ), opportunity improve upon hg time step.Hence, V hg (0; s) underestimate actual value hg . Q.E.D.Note theorem works one direction. says find stateV hg (0; s) > V (s), greedy policy, hg , strictly better .However, could optimal policy yet structure MAXQgraph prevents us considering action (either primitive composite) wouldimprove . Hence, unlike policy improvement theorem Howard (where primitiveactions always eligible chosen), guarantee suboptimal,hierarchically greedy policy strict improvement.contrast, perform one-step policy improvement discussed startsection, Corollary 3 guarantees improve policy. seegeneral, neither two methods non-hierarchical execution always betterother. Nonetheless, first method operates level individual primitiveactions, able produce large improvements policy. contrast,hierarchical greedy method obtain large improvements policy changingactions (i.e., subroutines) chosen near root hierarchy. Hence, general,hierarchical greedy execution probably better method. (Of course, value functionsmethods could computed, one better estimated value couldexecuted.)Sutton, et al. (1999) simultaneously developed closely-related method nonhierarchical execution macros. method equivalent ExecuteHGPolicyspecial case MAXQ hierarchy one level subtasks. interestingaspect ExecuteHGPolicy permits greedy improvements levelstree uence action chosen.care must taken applying Theorem 5 MAXQ hierarchy whose C valueslearned via MAXQ-Q. online algorithm, MAXQ-Q correctly learned values states nodes MAXQ graph. example,taxi problem, value C (Put; s; QPutdown) learned well exceptfour special locations R, G, B, Y. Put subtask cannotexecuted passenger taxi, usually means Getcompleted, taxi passenger's source location. exploration, children Put tried states. PutDown usually fail (and receive negativereward), whereas Navigate eventually succeed (perhaps lengthy exploration)277fiDietterichtake taxi destination location. all-states updating, valuesC (Put; s; Navigate(t)) learned states along pathpassenger's destination, C values Putdown action learnedpassenger's source destination locations. Hence, train MAXQ representation using hierarchical execution (as MAXQ-Q), switch hierarchically-greedyexecution, results quite bad. particular, need introduce hierarchicallygreedy execution early enough exploration policy still actively exploring. (Intheory, GLIE exploration policy never ceases explore, practice, want findgood policy quickly, asymptotically).course alternative would use hierarchically-greedy executionbeginning learning. However, remember higher nodes MAXQ hierarchyneed obtain samples P (s0 ; N js; a) child action a. hierarchical greedyexecution interrupts child reached terminal state (i.e., statealong way, another subtask appears better EvaluateMaxNode), samplescannot obtained. Hence, important begin purely hierarchical executiontraining, make transition greedy execution point.approach taken implement MAXQ-Q wayspecify number primitive actions L taken hierarchically hierarchical execution \interrupted" control returns top level (where new actionchosen greedily). start L set large, execution completelyhierarchical|when child action invoked, committed execute actionterminates. However, gradually, reduce L becomes 1, pointhierarchical greedy execution. time reaches 1 timeBoltzmann exploration cools temperature 0.1 (which exploration effectivelyhalted). experimental results show, generally gives excellent resultslittle added exploration cost.7. Experimental Evaluation MAXQ Methodperformed series experiments MAXQ method three goalsmind: (a) understand expressive power value function decomposition, (b)characterize behavior MAXQ-Q learning algorithm, (c) assess relativeimportance temporal abstraction, state abstraction, non-hierarchical execution.section, describe experiments present results.7.1 Fickle Taxi Taskfirst experiments performed modified version taxi task. versionincorporates two changes task described Section 3.1. First, fournavigation actions noisy, probability 0.8 moves intended direction,probability 0.1 instead moves right (of intended direction)probability 0.1 moves left. purpose change create realisticdicult challenge learning algorithms. second changetaxi picked passenger moved one square away passenger's sourcelocation, passenger changes destination location probability 0.3.278fiMAXQ Hierarchical Reinforcement Learningpurpose change create situation optimal policy hierarchicalpolicy effectiveness non-hierarchical execution measured.compared four different configurations learning algorithm: (a) Q learning,(b) MAXQ-Q learning without form state abstraction, (c) MAXQ-Q learningstate abstraction, (d) MAXQ-Q learning state abstraction greedy execution.configurations controlled many parameters. include following: (a)initial values Q C functions, (b) learning rate (we employed fixedlearning rate), (c) cooling schedule Boltzmann exploration (the GLIE policyemployed), (d) non-hierarchical execution, schedule decreasing L, numbersteps consecutive hierarchical execution. optimized settings separatelyconfiguration goal matching exceeding (with primitive trainingactions possible) best policy could code hand. Boltzmann exploration,established initial temperature cooling rate. separate temperaturemaintained Max node MAXQ graph, temperature reducedmultiplying cooling rate time subtask terminates goal state.process optimizing parameter settings algorithm time-consuming,Q learning MAXQ-Q. critical parameter schedulecooling temperature Boltzmann exploration: cooled rapidly,algorithms converge suboptimal policy. case, tested nine differentcooling rates. choose different cooling rates various subtasks, startedusing fixed policies (e.g., either random hand-coded) subtasks except subtasksclosest leaves. Then, chosen schedules subtasks, allowedparent tasks learn policies tuned cooling rates, on. Onenice effect method cooling temperature subtask terminatesnaturally causes subtasks higher MAXQ graph cool slowly. meantgood results could often obtained using cooling rate Maxnodes.choice learning rate easier, since determined primarily degreestochasticity environment. tested three four different ratesconfiguration. initial values Q C functions set based knowledgeproblems|no experiments required.took care tuning parameters experiments one wouldnormally take real application, wanted ensure methodcompared best possible conditions. general form results (particularlyspeed learning) wide ranges cooling rate learning rateparameter settings.following parameters selected based tuning experiments. Qlearning: initial Q values 0.123 states, learning rate 0.25, Boltzmann explorationinitial temperature 50 cooling rate 0.9879. (We use initial valuesend .123 \signature" debugging detect weight modified.)MAXQ-Q learning without state abstraction, used initial values 0.123, learning rate 0.50, Boltzmann exploration initial temperature 50 coolingrates 0.9996 MaxRoot MaxPut, 0.9939 MaxGet, 0.9879 MaxNavigate.279fiDietterich200MAXQ AbstractMean Cumulative Reward0MAXQAbstract+Greedy-200MAXQAbstract-400Flat Q-600-800-1000020000400006000080000100000Primitive Actions120000140000Figure 8: Comparison performance hierarchical MAXQ-Q learning (without state abstractions, state abstractions, state abstractions combinedhierarchical greedy evaluation) Q learning.MAXQ-Q learning state abstraction, used initial values 0.123, learningrate 0.25, Boltzmann exploration initial temperature 50 cooling rates0.9074 MaxRoot, 0.9526 MaxPut, 0.9526 MaxGet, 0.9879 MaxNavigate.MAXQ-Q learning non-hierarchical execution, used settingsstate abstraction. addition, initialized L 500 decreased 10trial reached 1. 50 trials, execution completely greedy.Figure 8 shows averaged results 100 training runs. training run involvesperforming repeated trials convergence. different trials execute differentnumbers primitive actions, plotted number primitive actionshorizontal axis rather number trials.first thing note forms MAXQ learning better initial performanceQ learning. constraints introduced MAXQ hierarchy.example, agent executing Navigate subtask, never attempt pickupputdown passenger, actions available Navigate. Similarly,agent never attempt putdown passenger first picked passenger(and vice versa) termination conditions Get Put subtasks.second thing notice without state abstractions, MAXQ-Q learning actually takes longer converge, Flat Q curve crosses MAXQ/no abstraction280fiMAXQ Hierarchical Reinforcement Learningcurve. shows without state abstraction, cost learning huge numberparameters MAXQ representation really worth benefits. suspectconsequence model-free nature MAXQ-Q algorithm. MAXQ decomposition represents information redundantly. example, cost performingPut subtask computed C (Root; s; Get) also V (Put; s). model-basedalgorithm could compute learned model, MAXQ-Q must learnseparately experience.third thing notice state abstractions, MAXQ-Q convergesquickly hierarchically optimal policy. seen clearly Figure 9,focuses range reward values neighborhood optimal policy.see MAXQ abstractions attains hierarchically optimal policyapproximately 40,000 steps, whereas Q learning requires roughly twice long reachlevel. However, Q learning, course, continue onward reach optimalperformance, whereas MAXQ hierarchy, best hierarchical policy slowrespond \fickle" behavior passenger he/she changes destination.last thing notice greedy execution, MAXQ policy also ableattain optimal performance. execution becomes \more greedy",temporary drop performance, MAXQ-Q must learn C values new regionsstate space visited recursively optimal policy. Despitedrop performance, greedy MAXQ-Q recovers rapidly reaches hierarchically optimalperformance faster purely-hierarchical MAXQ-Q learning. Hence, addedcost|in terms exploration|for introducing greedy execution.experiment presents evidence favor three claims: first, hierarchical reinforcement learning much faster Q learning; second, state abstractionrequired MAXQ-Q learning good performance; third, non-hierarchicalexecution produce significant improvements performance little addedexploration cost.7.2 Kaelbling's HDG Methodsecond task consider simple maze task introduced Leslie Kaelbling(1993) shown Figure 11. trial task, agent starts randomlychosen state must move randomly-chosen goal state using usual North, South,East, West operators (we employed deterministic operators). small costmove, agent must minimize undiscounted sum costs.goal state 100 different locations, actually 100different MDPs. Kaelbling's HDG method starts choosing arbitrary set landmarkstates defining Voronoi partition state space based Manhattan distanceslandmarks (i.e., two states belong Voronoi cell iffnearest landmark). method defines one subtask landmark l. subtaskmove state current Voronoi cell neighboring Voronoi celllandmark l. Optimal policies subtasks computed.HDG policies subtasks, solve abstract Markov DecisionProblem moving landmark state landmark state using subtasksolutions macro actions (subroutines). computes value function MDP.281fiDietterich10MAXQ Abstract+GreedyMean Cumulative Reward5Optimal PolicyFlat QHier-Optimal Policy0MAXQ AbstractMAXQ Abstract-5-10-15050000100000150000200000Primitive Actions250000300000Figure 9: Close-up view previous figure. figure also shows two horizontal linesindicating optimal performance hierarchically optimal performancedomain. make figure readable, applied 100-step movingaverage data points (which average 100 runs).Finally, possible destination location g within Voronoi cell landmark l,HDG method computes optimal policy getting l g.combining subtasks, HDG method construct good approximationoptimal policy follows. addition value functions discussed above,agent maintains two functions: NL(s), name landmark nearest state s,N (l), list landmarks cells immediate neighbors cell l.combining these, agent build list state current landmarklandmarks neighboring cells. landmark, agent computes sumthree terms:(t1) expected cost reaching landmark,(t2) expected cost moving landmark landmark goal cell,(t3) expected cost moving goal-cell landmark goal state.Note terms (t1) (t3) exact estimates, term (t2) computed usinglandmark subtasks subroutines. means corresponding path must passintermediate landmark states rather going directly goal landmark.282fiMAXQ Hierarchical Reinforcement Learning1098765432112345678910Figure 10: Kaelbling's 10-by-10 navigation task. circled state landmark state,heavy lines show boundaries Voronoi cells. episode,start state goal state chosen random. figure, start stateshown black square, goal state shown black hexagon.Hence, term (t2) typically overestimate required distance. (Also note (t3)choices intermediate landmarks, need explicitlyincluded computation best action agent enters cell containinggoal.)Given information, agent chooses move toward best landmarks(unless agent already goal Voronoi cell, case agent moves towardgoal state). example, Figure 10, term (t1) cost reaching landmarkrow 6, column 6, 4. Term (t2) cost getting row 6, column 6landmark row 1 column 4 (by going one landmark another). case,best landmark-to-landmark path go directly row 6 column 6 row 1 column4. Hence, term (t2) 6. Term (t3) cost getting row 1 column 4 goal,1. sum 4 + 6 + 1 = 11. comparison, optimal pathlength 9.Kaelbling's experiments, employed variation Q learning learn terms (t1)(t3), computed (t2) regular intervals via Floyd-Warshall all-sourcesshortest paths algorithm.Figure 11 shows MAXQ approach solving problem. overall task Root,takes one argument g, specifies goal cell. three subtasks:283fiDietterichMaxRoot(g)gl/NL(g)QGotoGoalLmk(gl)QGotoGoal(g)MaxGotoGoalLmk(gl)QGotoLmk(l,gl)MaxGotoLmk(l)QNorthLmk(l)QSouthLmk(l)MaxGotoGoal(g)QEastLmk(l)NorthQWestLmk(l)SouthQNorthG(g)EastQSouthG(g)QEastG(g)QWestG(g)WestFigure 11: MAXQ graph HDG navigation task.GotoGoalLmk, go landmark nearest goal location. terminationpredicate subtask true agent reaches landmark nearestgoal. goal predicate termination predicate.GotoLmk(l), go landmark l. termination predicate true either (a)agent reaches landmark l (b) agent outside region definedVoronoi cell l neighboring Voronoi cells, N (l). goal predicatesubtask true condition (a).GotoGoal(g), go goal location g. termination predicate subtasktrue either agent goal location agent outside Voronoicell NL(g) contains g. goal predicate subtask true agentgoal location.284fiMAXQ Hierarchical Reinforcement LearningMAXQ decomposition essentially Kaelbling's method, somewhatredundant. Consider state agent inside Voronoi cell goalg. states, HDG decomposes value function three terms (t1), (t2), (t3).Similarly, MAXQ also decomposes three terms:V (GotoLmk(l); s; a) cost getting landmark l. represented sumV (a; s) C (GotoLmk(l); s; a).C (GotoGoalLmk(gl); s; MaxGotoLmk(l)) cost getting landmark llandmark gl nearest goal.C (Root; s; GotoGoalLmk(gl)) cost getting goal location reaching gl(i.e., cost completing Root task reaching gl).agent inside goal Voronoi cell, HDG MAXQ storeessentially information. HDG stores Q(GotoGoal(g); s; a), MAXQ breakstwo terms: C (GotoGoal(g); s; a) V (a; s) sums two quantitiescompute Q value.Note MAXQ decomposition stores information twice|specifically,cost getting goal landmark gl goal stored C (Root; s; GotoGoalLmk(gl))C (GotoGoal(g); s; a) + V (a; s).Let us compare amount memory required Q learning, HDG, MAXQ.100 locations, 4 possible actions, 100 possible goal states, Q learningmust store 40,000 values.compute quantity (t1), HDG must store 4 Q values (for four actions)state respect landmark landmarks N (NL(s)). givestotal 2,028 values must stored.compute quantity (t2), HDG must store, landmark, informationshortest path every landmark. 12 landmarks. Consider landmarkrow 6, column 1. 5 neighboring landmarks constitute five macro actionsagent perform move another landmark. nearest landmarkgoal cell could 11 landmarks, gives total 55 Q valuesmust stored. Similar computations 12 landmarks give total 506 valuesmust stored.Finally, compute quantity (t3), HDG must store information, square insideVoronoi cell, get squares inside Voronoicell. requires 3,536 values.Hence, grand total HDG 6,070, huge savings Q learning.let's consider MAXQ hierarchy without state abstractions.V (a; s): expected reward primitive action state.100 states 4 primitive actions, requires 400 values. However,reward constant (,1), apply Leaf Irrelevance store single value.C (GotoLmk(l); s; a), one four primitive actions. requiresamount space (t1) Kaelbling's representation|indeed, combinedV (a; s), represents exactly information (t1). requires 2,028 values.state abstractions applied.285fiDietterichC (GotoGoalLmk(gl); s; GotoLmk(l)): cost completing GotoGoalLmktask going landmark l. primitive actions deterministic,GotoLmk(l) always terminate location l, hence, need storepair l gl. exactly Kaelbling's quantity (t2),requires 506 values. However, primitive actions stochastic|asKaelbling's original paper|then must store value possibleterminal state GotoLmk action. actions could terminatetarget landmark l one states bordering set Voronoi cellsneighbors cell l. requires 6,600 values. Kaelbling storesvalues (t2), effectively making assumption GotoLmk(l)never fail reach landmark l. approximation introduceMAXQ representation choice state abstraction node.C (GotoGoal; s; a): cost completing GotoGoal task executing oneprimitive actions a. quantity (t3) HDG representation,requires amount space: 3,536 values.C (Root; s; GotoGoalLmk): cost reaching goal reachedlandmark nearest goal. MAXQ must represent combinationsgoal landmarks goals. requires 100 values. Note valuesvalues C (GotoGoal(g); s; a) + V (a; s) primitive actions.means MAXQ representation stores information twice, whereasHDG representation stores (as term (t3)).C (Root; s; GotoGoal). cost completing Root task exe-cuted GotoGoal task. primitive action deterministic, always zero,GotoGoal reached goal. Hence, apply Terminationcondition store values all. However, primitive actions stochastic, must store value possible state borders Voronoi cellcontains goal. requires 96 different values. Again, Kaelbling's HDGrepresentation value function, ignoring probability GotoGoalterminate non-goal state. MAXQ exact representation valuefunction, ignore possibility. (incorrectly) apply Terminationcondition case, MAXQ representation becomes function approximation.stochastic case, without state abstractions, MAXQ representation requires12,760 values. safe state abstractions, requires 12,361 values. approximations employed Kaelbling (or equivalently, primitive actions deterministic),MAXQ representation state abstractions requires 6,171 values. numberssummarized Table 6. see that, unsafe state abstractions, MAXQrepresentation requires slightly space HDG representation (becauseredundancy storing C (Root; s; GotoGoalLmk).example shows HDG task, start fully-general formulation provided MAXQ impose assumptions obtain method similarHDG. MAXQ formulation guarantees value function hierarchicalpolicy represented exactly. assumptions introduce approximations286fiMAXQ Hierarchical Reinforcement LearningTable 6: Comparison number values must stored represent valuefunction using HDG MAXQ methods.HDG MAXQHDG MAXQ MAXQ MAXQitem itemvalues abs safe abs unsafe absV (a; s)040011(t1) C (GotoLmk(l); s; a)2,028 2,0282,0282,028(t2) C (GotoGoalLmk; s; GotoLmk(l))506 6,6006,600506(t3) C (GotoGoal(g); s; a)3,536 3,5363,5363,536C (Root; s; GotoGoalLmk)0100100100C (Root; s; GotoGoal)096960Total Number Values Required6,070 12,760 12,3616,171value function representation. might useful general design methodologybuilding application-specific hierarchical representations. long-term goal developmethods new application require inventing new set techniques. Instead, off-the-shelf tools (e.g., based MAXQ) could specialized imposingassumptions state abstractions produce ecient special-purpose systems.One important contributions HDG method introducedform non-hierarchical execution. soon agent crosses one Voronoi cellanother, current subtask reaching landmark cell \interrupted",agent recomputes \current target landmark". effect (untilreaches goal Voronoi cell), agent always aiming landmark outsidecurrent Voronoi cell. Hence, although agent \aims for" sequence landmark states,typically visit many states way goal. states provideconvenient set intermediate targets. taking \shortcuts", HDG compensatesfact that, general, overestimated cost getting goal,computed value function based policy agent goes one landmarkanother.effect obtained hierarchical greedy execution MAXQ graph (whichdirectly inspired HDG method). Note storing NL (nearest landmark)function, Kaelbing's HDG method detect eciently current subtaskinterrupted. technique works navigation problems spacedistance metric. contrast, ExecuteHGPolicy performs kind \polling",checks primitive action whether interrupt current subroutineinvoke new one. important goal future research MAXQ find generalpurpose mechanism avoiding unnecessary \polling"|that is, mechanismdiscover eciently-evaluable interrupt conditions.Figure 12 shows results experiments HDG using MAXQ-Q learning algorithm. employed following parameters: Flat Q learning, initial values0.123, learning rate 1.0, initial temperature 50, cooling rate 0.9074;MAXQ-Q without state abstractions: initial values ,25:123, learning rate 1.0, initial287fiDietterich0Flat QMAXQ +Abstract-20Mean Cumulative Reward-40MAXQ Abstract-60-80-100-120-1400200000400000600000800000Primitive Actions1e+061.2e+061.4e+06Figure 12: Comparison Flat Q learning MAXQ-Q learning without stateabstraction. (Average 100 runs.)temperature 50, cooling rates 0.9074 MaxRoot, 0.9999 MaxGotoGoalLmk,0.9074 MaxGotoGoal, 0.9526 MaxGotoLmk; MAXQ-Q state abstractions:initial values ,20:123, learning rate 1.0, initial temperature 50, cooling rates0.9760 MaxRoot, 0.9969 MaxGotoGoal, 0.9984 MaxGotoGoalLmk, 0.9969MaxGotoLmk. Hierarchical greedy execution introduced starting 3000 primitive actions per trial, reducing every trial 2 actions, 1500 trials,execution completely greedy.figure confirms observations made experiments Fickle Taxi task.Without state abstractions, MAXQ-Q converges much slowly Q learning.state abstractions, converges roughly three times fast. Figure 13 shows close-upview Figure 12 allows us compare differences final levels performancemethods. Here, see MAXQ-Q state abstractions ablereach quality hand-coded hierarchical policy|presumably even explorationwould required achieve this, whereas state abstractions, MAXQ-Q ableslightly better hand-coded policy. hierarchical greedy execution, MAXQ-Qable reach goal using one fewer action, average|so approachesperformance best hierarchical greedy policy (as computed value iteration). Noticehowever, best performance obtained hierarchical greedy executionbest recursively-optimal policy cannot match optimal performance. Hence, Flat Q288fiMAXQ Hierarchical Reinforcement Learning-6Optimal PolicyMean Cumulative RewardHierarchical Greedy Optimal PolicyMAXQ Abstract + GreedyMAXQ + Abstract-8-10Flat QHierarchical Hand-coded PolicyMAXQ Abstract-12-140200000400000600000800000Primitive Actions1e+061.2e+061.4e+06Figure 13: Expanded view comparing Flat Q learning MAXQ-Q learningwithout state abstraction without hierarchical greedy execution.(Average 100 runs.)learning achieves policy reaches goal state, average, one fewerprimitive action. Finally notice taxi domain, added explorationcost shifting greedy execution.Kaelbling's HDG work recently extended generalized Moore, BairdKaelbling (1999) sparse MDP overall task get givenstart state desired goal state. key success approachlandmark subtask guaranteed terminate single resulting state. makespossible identify sequence good intermediate landmark states assemblepolicy visits sequence. Moore, Baird Kaelbling show constructhierarchy landmarks (the \airport" hierarchy) makes planning process ecient.Note subtask terminate single state (as general MDPs),airport method would work, would combinatorial explosionpotential intermediate states would need considered.7.3 Parr Russell: Hierarchies Abstract Machines(1998b) dissertation work, Ron Parr considered approach hierarchical reinforcement learning programmer encodes prior knowledge form hierarchyfinite-state controllers called HAM (Hierarchy Abstract Machines). hierarchy289fiDietterichIntersectionVertical HallwayHorizontal HallwayGoalFigure 14: Parr's maze problem (on left). start state upper left corner,states lower right-hand room terminal states. smaller diagramright shows hallway intersection structure maze.executed using procedure-call-and-return discipline, provides partial policytask. policy partial machine include non-deterministic \choice"machine states, machine lists several options action specifyone chosen. programmer puts \choice" states pointhe/she know action performed. Given partial policy, Parr'sgoal find best policy making choices choice states. words,goal learn hierarchical value function V (hs; mi), state (of externalenvironment) contains internal state hierarchy (i.e., contentsprocedure call stack values current machine states machinesappearing stack). key observation necessary learn valuefunction choice states hs; mi. Parr's algorithm learn decomposition valuefunction. Instead, \ attens" hierarchy create new Markov decision problemchoice states hs; mi. Hence, hierarchical primarily sense programmerstructures prior knowledge hierarchically. advantage Parr's methodfind optimal hierarchical policy subject constraints provided programmer.disadvantage method cannot executed \non-hierarchically" producebetter policy.Parr illustrated work using maze shown Figure 14. maze large-scalestructure (as series hallways intersections), small-scale structure (a seriesobstacles must avoided order move hallways intersections).290fiMAXQ Hierarchical Reinforcement Learningtrial, agent starts top left corner, must move statebottom right corner room. agent usual four primitive actions, North, South,East, West. actions stochastic: probability 0.8, succeed,probability 0.1 action move \left" probability 0.1 actionmove \right" instead (e.g., North action move east probability 0.1west probability 0.1). action would collide wall obstacle,effect.maze structured series \rooms", containing 12-by-12 block states(and various obstacles). rooms parts \hallways", connectedtwo rooms opposite sides. rooms \intersections", twohallways meet.test representational power MAXQ hierarchy, want see wellrepresent prior knowledge Parr able represent using HAM. begindescribing Parr's HAM maze task, present MAXQ hierarchycaptures much prior knowledge.3Parr's top level machine, MRoot, consists loop single choice statechooses among four possible child machines: MGo(East), MGo(South), MGo(West),MGo(North). loop terminates agent reaches goal state. MRootinvoke particular machine hallway specified direction. Hence,start state, consider MGo(South) MGo(East).MGo(d) machine begins executing agent intersection. firstthing tries exit intersection hallway specified direction d.attempts traverse hallway reaches another intersection. firstinvoking MExitIntersection(d) machine. machine returns, invokesMExitHallway(d) machine. machine returns, MGo also returns.MExitIntersection MExitHallway machines identical except termination conditions. machines consist loop one choice state chooses amongfour possible subroutines. simplify description, suppose MGo(East) chosen MExitIntersection(East). four possible subroutines MSniff (East; North),MSniff (East; South), MBack(East; North), MBack(East; South).MSniff (d; p) machine always moves direction encounters wall (eitherpart obstacle part walls maze). moves perpendiculardirection p reaches end wall. wall \end" two ways: eitheragent trapped corner walls directions p elselonger wall direction d. first case, MSniff machine terminates; secondcase, resumes moving direction d.MBack(d; p) machine moves one step backwards (in direction opposite d)moves five steps direction p. moves may may succeed,actions stochastic may walls blocking way. actions carriedcase, MBack machine returns.MSniff MBack machines also terminate reach end hallend intersection.3. author thanks Ron Parr providing details HAM task.291fiDietterichfinite-state controllers define highly constrained partial policy. MBack,MSniff, MGo machines contain choice states all. choice pointsMRoot, must choose direction move, MExitIntersectionMExitHall, must decide call MSniff, call MBack,\perpendicular" direction tell machines try cannot move forward.MaxRootGo(d)r/ROOMMaxGo(d,r)QExitInter(d,r)QExitHall(d,r)MaxExitInter(d,r)MaxExitHall(d,r)QSniffEI(d,p)QBackEI(d,p)QSniffEH(d,p)x/XQBackEH(d,p)x/Xy/Yy/YMaxSniff(d,p)MaxBack(d,p,x,y)QFollowWall(d,p)QToWall(d)QBackOne(d)QPerpThree(p)MaxFollowWall(d,p)MaxToWall(d)MaxBackOne(d)MaxPerpThree(p)d/pd/dQMoveFW(d)d/Inv(d)QMoveTW(d)QMoveBO(d)d/pQMoveP3(d)MaxMove(d)Figure 15: MAXQ graph Parr's maze task.Figure 15 shows MAXQ graph encodes similar set constraints policy.subtasks defined follows:292fiMAXQ Hierarchical Reinforcement LearningRoot. exactly MRoot machine. must choose directioninvoke Go. terminates agent enters terminal state. alsogoal condition (of course).Go(d; r). (Go direction leaving room r.) parameter r bound identi-fication number corresponding current 12-by-12 \room" agentlocated. Go terminates agent enters room end hallwaydirection leaves desired hallway (e.g., wrong direction).goal condition Go satisfied agent reaches desired intersection.ExitInter(d; r). terminates agent exited room r. goal conditionagent exit room r direction d.ExitHall(d; r). terminates agent exited current hall (intointersection). goal condition agent entered desired intersectiondirection d.Sniff (d; r). encodes subtask equivalent MSniff machine. However,Sniff must two child subtasks, ToWall FollowWall, simply internalstates MSniff. necessary, subtask MAXQ framework cannotcontain internal state, whereas finite-state controller HAM representationcontain many internal states necessary. particular, one statemoving forward another state following wall sideways.ToWall(d). equivalent one part MSniff. terminateswall \front" agent direction d. goal conditiontermination condition.FollowWall(d; p). equivalent part MSniff. moves directionp wall direction ends (or stuck corner wallsdirections p). goal condition termination condition.Back(d; p; x; y). attempts encode information MBack machine,case MAXQ hierarchy cannot capture information.MBack simply executes sequence 6 primitive actions (one step back, five stepsdirection p). this, MBack must 6 internal states, MAXQallow. Instead, Back subtask subgoal moving agent leastone square backwards least 3 squares direction p. order determinewhether achieved subgoal, must remember x positionstarted execute, bound parameters Back. Back terminatesachieves desired change position runs walls preventachieving subgoal. goal condition termination condition.BackOne(d; x; y). moves agent one step backwards (in direction opposited. needs starting x position order tell succeeded.terminates moved least one unit direction walldirection. goal condition termination condition.293fiDietterichPerpThree(p; x; y). moves agent three steps direction p. needsstarting x positions order tell succeeded. terminatesmoved least three units direction p wall direction.goal condition termination condition.Move(d). \parameterized primitive" action. executes one primitive movedirection terminates immediately.this, see three major differences MAXQ representation HAM representation. First, HAM finite-state controller containinternal states. convert MAXQ subtask graph, must make separatesubtask internal state HAM. Second, HAM terminate based\amount effort" (e.g., performing 5 actions), whereas MAXQ subtask must terminatebased change state world. impossible define MAXQ subtask performs k steps terminate regardless effects steps (i.e.,without adding kind \counter" state MDP). Third, dicultformulate termination conditions MAXQ subtasks HAM machines.example, HAM, necessary specify MExitHallway machine terminates entered different intersection one MGo executed.However, important MAXQ method, MAXQ, subtask learnsvalue function policy|independent parent tasks. example, withoutrequirement enter different intersection, learning algorithms MAXQalways prefer MaxExitHall take one step backward return roomGo action started (because much easier terminal state reach).problem arise HAM approach, policy learned subtaskdepends whole \ attened" hierarchy machines, returning stateGo action started help solve overall problem reaching goal statelower right corner.construct MAXQ graph problem, introduced three programmingtricks: (a) binding parameters aspects current state (in order serve kind\local memory" subtask began executing), (b) parameterizedprimitive action (in order able pass parameter value specifies primitiveaction perform), (c) employing \inheritance termination conditions"|that is,subtask MAXQ graph (but others paper) inherits terminationconditions ancestor tasks. Hence, agent middle executing ToWallaction leaves intersection, ToWall subroutine terminates ExitIntertermination condition satisfied. behavior similar standard behaviorMAXQ. Ordinarily, ancestor task terminates, descendent tasks forcedreturn without updating C values. inheritance termination conditions,hand, descendent tasks forced terminate, updating Cvalues. words, termination condition child task logical disjuntiontermination conditions ancestors (plus termination condition).inheritance made easier write MAXQ graph, parents needpass children information necessary children definecomplete termination goal predicates.294fiMAXQ Hierarchical Reinforcement Learningessentially opportunities state abstraction task,irrelevant features state. opportunities apply ShieldingTermination properties, however. particular, ExitHall(d) guaranteed causeparent task, MaxGo(d), terminate, require stored C values.many states subtasks terminated (e.g., Go(East) statewall east side room), C values need stored.Nonetheless, even applying state elimination conditions, MAXQ representation task requires much space representation. exactcomputation dicult, applying MAXQ-Q learning, MAXQ representationrequired 52,043 values, whereas Q learning requires fewer 16,704 values. Parrstates method requires 4,300 values.test relative effectiveness MAXQ representation, compare MAXQ-Qlearning Q learning. large negative values statesacquire (particularly early phases learning), unable get Boltzmannexploration work well|one bad experience would cause action receivelow Q value, would never tried again. Hence, experimented-greedy exploration counter-based exploration. -greedy exploration policyordered, abstract GLIE policy random action chosen probability ,gradually decreased time. counter-based exploration policy keeps trackmany times action executed state s. choose action states, selects action executed fewest times actionsexecuted times. switches greedy execution. Hence, genuine GLIEpolicy. Parr employed counter-based exploration policies experiments task.domains, conducted several experimental runs (e.g., testing Boltzmann, -greedy, counter-based exploration) determine best parametersalgorithm. Flat Q learning, chose following parameters: learning rate 0.50, greedy exploration initial value 1.0, decreased 0.001 successfulexecution Max node, initial Q values ,200:123. MAXQ-Q learning, chosefollowing parameters: counter-based exploration = 10, learning rate equalreciprocal number times action performed, initial values Cvalues selected carefully provide underestimates true C values. example,initial values QExitInter ,40:123, worst case, completingExitInter task, takes 40 steps complete subsequent ExitHall task hence,complete Go parent task. Performance quite sensitive initial C values,potential drawback MAXQ approach.Figure 16 plots results. see MAXQ-Q learning converges 10times faster Flat Q learning. know whether MAXQ-Q convergedrecursively optimal policy. comparison, also show performance hierarchicalpolicy coded hand, hand-coded policy, used knowledge contextualinformation choose operators, policy surely better best recursivelyoptimal policy. HAMQ learning converge policy equal slightly betterhand-coded policy.experiment demonstrates MAXQ representation capture most|butall|of prior knowledge represented HAMQ hierarchy. also295fiDietterich-100-150Mean Reward Per Trial-200Hand-coded hierarchical policy-250-300-350MAXQ Q LearningFlat Q Learning-400-450-50001e+062e+063e+06Primitive Steps4e+065e+066e+06Figure 16: Comparison Flat Q learning MAXQ-Q learning Parr maze task.shows MAXQ representation requires much care design goalconditions subtasks.7.4 Domainsaddition three domains discussed above, developed MAXQ graphsSingh's (1992) \ ag task", treasure hunter task described Tadepalli Dietterich(1997), Dayan Hinton's (1993) Feudal-Q learning task. taskseasily naturally placed MAXQ framework|indeed, fit easilyParr Russell maze task.MAXQ able exactly duplicate Singh's work decomposition valuefunction|while using exactly amount space represent value function.MAXQ also duplicate results Tadepalli Dietterich|however,MAXQ explanation-based method, considerably slower requires substantially space represent value function.Feudal-Q task, MAXQ able give better performance Feudal-Q learning.reason Feudal-Q learning, subroutine makes decisions using Qfunction learned level hierarchy|that is, without informationestimated costs actions descendents. contrast, MAXQ value functiondecomposition permits Max node make decisions based sum completionfunction, C (i; s; j ), costs estimated descendents, V (j; s). course, MAXQ296fiMAXQ Hierarchical Reinforcement Learningalso supports non-hierarchical execution, possible Feudal-Q,learn value function decomposition.8. Discussionconcluding paper, wish discuss two issues: (a) design tradeoffs hierarchical reinforcement learning (b) methods automatically learning (or leastimproving) MAXQ hierarchies.8.1 Design Tradeoffs Hierarchical Reinforcement Learningintroduction paper, discussed four issues concerning design hierarchical reinforcement learning architectures: (a) method defining subtasks, (b)use state abstraction, (c) non-hierarchical execution, (d) design learning algorithms. subsection, want highlight tradeoff first twoissues.MAXQ defines subtasks using termination predicate Ti pseudo-reward functionR~ . least two drawbacks method. First, hard programmer define Ti R~ correctly, since essentially requires guessing value functionoptimal policy MDP states subtask terminates. Second,leads us seek recursively optimal policy rather hierarchically optimal policy.Recursively optimal policies may much worse hierarchically optimal ones,may giving substantial performance.However, return two drawbacks, MAXQ obtains important benefit:policies value functions subtasks become context-free. words,depend parent tasks larger context invoked.understand point, consider MDP shown Figure 6. clearoptimal policy exiting left-hand room (the Exit subtask) depends locationgoal. top right-hand room, agent preferexit via upper door, whereas bottom right-hand room, agentprefer exit lower door. However, define subtask exitingleft-hand room using pseudo-reward zero doors, obtain policyoptimal either case, policy re-use cases. Furthermore,policy depend location goal. Hence, apply Max nodeirrelevance solve Exit subtask using location robot ignorelocation goal.example shows obtain benefits subtask reuse state abstraction define subtask using termination predicate pseudo-rewardfunction. termination predicate pseudo-reward function provide barrierprevents \communication" value information Exit subtask context.Compare Parr's HAM method. HAMQ algorithm finds best policyconsistent hierarchy. achieve this, must permit information propagate\into" Exit subtask (i.e., Exit finite-state controller) environment.means state reached leaving Exit subtask differentvalues depending location goal, different values propagateback Exit subtask. represent different values, Exit subtask must know297fiDietterichlocation goal. short, achieve hierarchically optimal policy within Exitsubtask, must (in general) represent value function using entire state space. Stateabstractions cannot employed without losing hierarchical optimality.see, therefore, direct tradeoff achieving hierarchicaloptimality employing state abstractions. Methods hierarchical optimalityfreedom defining subtasks (e.g., using partial policies, HAM approach).cannot (safely) employ state abstractions within subtasks, general, cannotreuse solution one subtask multiple contexts. Methods recursive optimality,hand, must define subtasks using method (such pseudo-reward functionsMAXQ fixed policies options framework) isolates subtaskcontext. return, apply state abstraction learned policyreused many contexts (where less optimal).interesting iterative method described Dean Lin (1995)viewed method moving along tradeoff. Dean Lin method,programmer makes initial guess values terminal states subtask(i.e., doorways Figure 6). Based initial guess, locally optimal policiessubtasks computed. locally optimal policy parent taskcomputed|while holding subtask policies fixed (i.e., treating options).point, algorithm computed recursively optimal solution originalproblem, given initial guesses. Instead solving various subproblems sequentiallyvia oine algorithm Dean Lin suggested, could use MAXQ-Q learningalgorithm.method Dean Lin stop here. Instead, computes new valuesterminal states subtask based learned value function entireproblem. allows update \guesses" values terminal states.entire solution process repeated obtain new recursively optimal solution,based new guesses. prove process iterated indefinitely,converge hierarchically optimal policy (provided, course, state abstractionsused within subtasks).suggests extension MAXQ-Q learning adapts R~ values online.time subtask terminates, could update R~ function based computed valueterminated state. precise, j subtask i, j terminatesstate s0 , update R~ j (s0 ) equal V~ (i; s0 ) = maxa0 Q~ (i; s0 ; a0 ). However,work R~ j (s0 ) represented using full state s0. subtask j employing stateabstractions, x = (s), R~ j (x0 ) need average value V~ (i; s0 ),average taken states s0 x0 = (s0 ) (weighted probabilityvisiting states). easily accomplished performing stochastic approximationupdate formR~j (x0 ) = (1 , fft )R~ j (x0 ) + fftV~ (i; s0 )time subtask j terminates. algorithm could expected convergebest hierarchical policy consistent given state abstractions.also suggests problems, may worthwhile first learn recursivelyoptimal policy using aggressive state abstractions use learned valuefunction initialize MAXQ representation detailed representationstates. progressive refinements state space could guided monitoring298fiMAXQ Hierarchical Reinforcement Learningdegree values V~ (i; x0 ) vary abstract state x0 . largevariance, means state abstractions failing make important distinctionsvalues states, refined.kinds adaptive algorithms take longer converge basicMAXQ method described paper. tasks agent must solve many timeslifetime, worthwhile learning algorithms provide initial usefulsolution gradually improve solution optimal. important goalfuture research find methods diagnosing repairing errors (or sub-optimalities)initial hierarchy ultimately optimal policy discovered.8.2 Automated Discovery Abstractionsapproach taken paper rely upon programmer designMAXQ hierarchy including termination conditions, pseudo-reward functions, stateabstractions. results paper, particularly concerning state abstraction, suggestways might able automate construction hierarchy.main purpose hierarchy create opportunities subtask sharingstate abstraction. actually closely related. order subtask sharedtwo different regions state space, must case value functiontwo different regions identical except additive offset. MAXQ framework,additive offset would difference C values parent task. one wayfind reusable subtasks would look regions state space value functionexhibits additive offsets.second way would search structure one-step probability transitionfunction P (s0 js; a). subtask useful enables state abstractions MaxNode Irrelevance. formulate problem identifying regionstate space that, conditioned region, P (s0 js; a) factors accordingEquation 17. top-down divide-and-conquer algorithm similar decision-tree algorithmsmight able this.third way would search funnel actions looking bottlenecks statespace policies must travel. would useful discovering casesResult Distribution Irrelevance.ways, dicult kinds state abstractions discoverarbitrary subgoals introduced constrain policy (and sacrifice optimality).example, could algorithm automatically decide impose landmarks ontoHDG task? Perhaps detecting large region state space without bottlenecksvariations reward function?problem discovering hierarchies important challenge future,least paper provided guidelines constitute good state abstractions,serve objective functions guiding automated search abstractions.9. Concluding Remarkspaper introduced new representation value function hierarchical reinforcement learning|the MAXQ value function decomposition. provedMAXQ decomposition represent value function hierarchical policy299fiDietterichfinite-horizon undiscounted, cumulative reward criterion infinite-horizondiscounted reward criterion. representation supports subtask sharing re-use, overall value function decomposed value functions individual subtasks.paper introduced learning algorithm, MAXQ-Q learning, provedconverges probability 1 recursively optimal policy. paper argued althoughrecursive optimality weaker either hierarchical optimality global optimality,important form optimality permits subtask learn locally optimalpolicy ignoring behavior ancestors MAXQ graph. increasesopportunities subtask sharing state abstraction.shown MAXQ decomposition creates opportunities state abstraction, identified set five properties (Max Node Irrelevance, Leaf Irrelevance,Result Distribution Irrelevance, Shielding, Termination) allow us ignore largeparts state space within subtasks. proved MAXQ-Q still convergespresence forms state abstraction, showed experimentally state abstraction important practice successful application MAXQ-Q learning|atleast Taxi HDG tasks.paper presented two different methods deriving improved non-hierarchical policies MAXQ value function representation, formalized conditionsmethods improve hierarchical policy. paper verifiedexperimentally non-hierarchical execution gives improved performance FickleTaxi Task (where achieves optimal performance) HDG task (where givessubstantial improvement).Finally, paper argued tradeoff governing design hierarchicalreinforcement learning methods. one end design spectrum \context free"methods MAXQ-Q learning. provide good support state abstractionsubtask sharing learn recursively optimal policies. endspectrum \context-sensitive" methods HAMQ, options framework,early work Dean Lin. methods discover hierarchically optimalpolicies (or, cases, globally optimal policies), drawback cannoteasily exploit state abstractions share subtasks. great speedupsenabled state abstraction, paper argued context-free approachpreferred|and relaxed needed obtain improved policies.Acknowledgementsauthor gratefully acknowledges support National Science Foundationgrant number IRI-9626584, Oce Naval Research grant number N00014-95-10557, Air Force Oce Scientific Research grant number F49620-98-1-0375,Spanish government program Estancias de Investigadores Extranjeros enRegimen de A~no Sabatico en Espa~na. addition, author indebted many colleagueshelping develop clarify ideas paper including Valentina Zubek, LeslieKaelbling, Bill Langford, Wes Pinchot, Rich Sutton, Prasad Tadepalli, Sebastian Thrun.particularly want thank Eric Chown encouraging study Feudal reinforcementlearning, Ron Parr providing details HAM machines, Sebastian Thrunencouraging write single comprehensive paper. also thank Andrew Moore300fiMAXQ Hierarchical Reinforcement Learning(the action editor), Valentina Zubek, two sets anonymous reviewers previousdrafts paper suggestions careful reading, improved paperimmeasurably.ReferencesBellman, R. E. (1957). Dynamic Programming. Princeton University Press.Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,Belmont, MA.Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure policy construction. Proceedings Fourteenth International Joint Conference ArtificialIntelligence, pp. 1104{1111.Currie, K., & Tate, A. (1991). O-plan: open planning architecture. Artificial Intelligence, 52 (1), 49{86.Dayan, P., & Hinton, G. (1993). Feudal reinforcement learning. Advances NeuralInformation Processing Systems, 5, pp. 271{278. Morgan Kaufmann, San Francisco,CA.Dean, T., & Lin, S.-H. (1995). Decomposition techniques planning stochastic domains.Tech. rep. CS-95-10, Department Computer Science, Brown University, Providence,Rhode Island.Dietterich, T. G. (1998). MAXQ method hierarchical reinforcement learning.Fifteenth International Conference Machine Learning, pp. 118{126. Morgan Kaufmann.Fikes, R. E., Hart, P. E., & Nilsson, N. J. (1972). Learning executing generalized robotplans. Artificial Intelligence, 3, 251{288.Forgy, C. L. (1982). Rete: fast algorithm many pattern/many object patternmatch problem. Artificial Intelligence, 19 (1), 17{37.Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., & Boutilier, C. (1998). Hierarchicalsolution Markov decision processes using macro-actions. ProceedingsFourteenth Annual Conference Uncertainty Artificial Intelligence (UAI{98), pp.220{229 San Francisco, CA. Morgan Kaufmann Publishers.Howard, R. A. (1960). Dynamic Programming Markov Processes. MIT Press, Cambridge, MA.Jaakkola, T., Jordan, M. I., & Singh, S. P. (1994). convergence stochastic iterativedynamic programming algorithms. Neural Computation, 6 (6), 1185{1201.Kaelbling, L. P. (1993). Hierarchical reinforcement learning: Preliminary results. Proceedings Tenth International Conference Machine Learning, pp. 167{173 SanFrancisco, CA. Morgan Kaufmann.301fiDietterichKalmar, Z., Szepesvari, C., & Lorincz, A. (1998). Module based reinforcement learningreal robot. Machine Learning, 31, 55{85.Knoblock, C. A. (1990). Learning abstraction hierarchies problem solving. ProceedingsEighth National Conference Artificial Intelligence, pp. 923{928 Boston, MA.AAAI Press.Korf, R. E. (1985). Macro-operators: weak method learning. Artificial Intelligence,26 (1), 35{77.Lin, L.-J. (1993). Reinforcement learning robots using neural networks. Ph.D. thesis,Carnegie Mellon University, Department Computer Science, Pittsburgh, PA.Moore, A. W., Baird, L., & Kaelbling, L. P. (1999). Multi-value-functions: Ecient automatic action hierarchies multiple goal MDPs. Proceedings International Joint Conference Artificial Intelligence, pp. 1316{1323 San Francisco. Morgan Kaufmann.Parr, R. (1998a). Flexible decomposition algorithms weakly coupled Markov decisionproblems. Proceedings Fourteenth Annual Conference UncertaintyArtificial Intelligence (UAI{98), pp. 422{430 San Francisco, CA. Morgan KaufmannPublishers.Parr, R. (1998b). Hierarchical control learning Markov decision processes. Ph.D.thesis, University California, Berkeley, California.Parr, R., & Russell, S. (1998). Reinforcement learning hierarchies machines. Advances Neural Information Processing Systems, Vol. 10, pp. 1043{1049 Cambridge,MA. MIT Press.Pearl, J. (1988). Probabilistic Inference Intelligent Systems. Networks Plausible Inference. Morgan Kaufmann, San Mateo, CA.Rummery, G. A., & Niranjan, M. (1994). Online Q-learning using connectionist systems.Tech. rep. CUED/FINFENG/TR 166, Cambridge University Engineering Department, Cambridge, England.Sacerdoti, E. D. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence,5 (2), 115{135.Singh, S., Jaakkola, T., Littman, M. L., & Szepesvari, C. (1998). Convergence resultssingle-step on-policy reinforcement-learning algorithms. Tech. rep., UniversityColorado, Department Computer Science, Boulder, CO. appear MachineLearning.Singh, S. P. (1992). Transfer learning composing solutions elemental sequentialtasks. Machine Learning, 8, 323{339.Sutton, R. S., Singh, S., Precup, D., & Ravindran, B. (1999). Improved switching amongtemporally abstract actions. Advances Neural Information Processing Systems,Vol. 11, pp. 1066{1072. MIT Press.302fiMAXQ Hierarchical Reinforcement LearningSutton, R., & Barto, A. G. (1998). Introduction Reinforcement Learning. MIT Press,Cambridge, MA.Sutton, R. S., Precup, D., & Singh, S. (1998). MDPs Semi-MDPs: Learning,planning, representing knowledge multiple temporal scales. Tech. rep., University Massachusetts, Department Computer Information Sciences, Amherst,MA. appear Artificial Intelligence.Tadepalli, P., & Dietterich, T. G. (1997). Hierarchical explanation-based reinforcementlearning. Proceedings Fourteenth International Conference MachineLearning, pp. 358{366 San Francisco, CA. Morgan Kaufmann.Tambe, M., & Rosenbloom, P. S. (1994). Investigating production system representationsnon-combinatorial match. Artificial Intelligence, 68 (1), 155{199.Watkins, C. J. C. H. (1989). Learning Delayed Rewards. Ph.D. thesis, King's College,Oxford. (To reprinted MIT Press.).Watkins, C. J., & Dayan, P. (1992). Technical note Q-Learning. Machine Learning, 8, 279.303fiJournal Artificial Intelligence Research 13 (2000) 155-188Submitted 6/00; published 10/00AIS-BN: Adaptive Importance Sampling AlgorithmEvidential Reasoning Large Bayesian NetworksJian ChengMarek J. Druzdzeljcheng@sis.pitt.edumarek@sis.pitt.eduDecision Systems LaboratorySchool Information Sciences Intelligent Systems ProgramUniversity Pittsburgh, Pittsburgh, PA 15260 USAAbstractStochastic sampling algorithms, attractive alternative exact algorithmslarge Bayesian network models, observed perform poorly evidentialreasoning extremely unlikely evidence. address problem, propose adaptive importance sampling algorithm, AIS-BN, shows promising convergence rateseven extreme conditions seems outperform existing sampling algorithmsconsistently. Three sources performance improvement (1) two heuristicsinitialization importance function based theoretical properties importance sampling finite-dimensional integrals structural advantages Bayesiannetworks, (2) smooth learning method importance function, (3) dynamicweighting function combining samples different stages algorithm.tested performance AIS-BN algorithm along two state artgeneral purpose sampling algorithms, likelihood weighting (Fung & Chang, 1989; Shachter& Peot, 1989) self-importance sampling (Shachter & Peot, 1989). usedtests three large real Bayesian network models available scientific community:CPCS network (Pradhan et al., 1994), PathFinder network (Heckerman, Horvitz,& Nathwani, 1990), ANDES network (Conati, Gertner, VanLehn, & Druzdzel,1997), evidence unlikely 1041 . AIS-BN algorithm always performedbetter two algorithms, majority test cases achieved ordersmagnitude improvement precision results. Improvement speed given desiredprecision even dramatic, although unable report numerical results here,algorithms almost never achieved precision reached even firstiterations AIS-BN algorithm.1. IntroductionBayesian networks (Pearl, 1988) increasingly popular tools modeling uncertaintyintelligent systems. practical models reaching size several hundreds variables(e.g., Pradhan et al., 1994; Conati et al., 1997), becomes increasingly important address problem feasibility probabilistic inference. Even though several ingeniousexact algorithms proposed, large models stumble theoretically demonstrated NP-hardness inference (Cooper, 1990). significanceresult observed practice exact algorithms applied large, densely connectedpractical networks require either prohibitive amount memory prohibitive amountcomputation unable complete. approximating inference desiredprecision shown NP-hard well (Dagum & Luby, 1993), comc2000AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiCheng & Druzdzelplex networks alternative produce result all. Furthermore,obtaining result crucial applications, precision guarantees may criticaltypes problems traded speed computation.prominent subclass approximate algorithms family stochastic samplingalgorithms (also called stochastic simulation Monte Carlo algorithms). precisionobtained stochastic sampling generally increases number samples generatedfairly unaffected network size. Execution time fairly independenttopology network linear number samples. Computationinterrupted time, yielding anytime property algorithms, important timecritical applications.stochastic sampling performs well predictive inference, diagnostic reasoning, i.e., reasoning observed evidence nodes ancestors network oftenexhibits poor convergence. number observations increases, especiallyobservations unlikely a-priori, stochastic sampling often fails converge reasonable estimates posterior probabilities. Although problem known sincefirst sampling algorithm proposed Henrion (1988), little doneaddress effectively. Furthermore, various sampling algorithms proposed testedsimple small networks, networks special topology, without presenceextremely unlikely evidence practical significance problem underestimated. Given typical number samples used real-time feasibletodays hardware, say 106 samples, behavior stochastic sampling algorithmdrastically different different size networks. network consisting 10 nodesobservations, may possible converge exact probabilities, largenetworks negligibly small fraction total sample space probed. Onepractical Bayesian network models used tests, subset CPCSnetwork (Pradhan et al., 1994), consists 179 nodes. total sample space larger1061 . 106 samples, sample 1055 fraction sample space.believe crucial (1) study feasibility convergence propertiessampling algorithms large practical networks, (2) develop sampling algorithms show good convergence extreme, yet practical conditions,evidential reasoning given extremely unlikely evidence. all, small networksupdated using existing exact algorithms precisely large networksstochastic sampling useful. likelihood evidence, knowstochastic sampling generally perform well high (Henrion, 1988). So,important look cases evidence unlikely. paper, testtwo existing state art stochastic sampling algorithms Bayesian networks, likelihood weighting (Fung & Chang, 1989; Shachter & Peot, 1989) self-importance sampling(Shachter & Peot, 1989), subset CPCS network extremely unlikely evidence. show exhibit similarly poor convergence rates. propose newsampling algorithm, call adaptive importance sampling Bayesian networks(AIS-BN), suitable evidential reasoning large multiply-connected Bayesiannetworks. AIS-BN algorithm based importance sampling, widelyapplied method variance reduction simulation also applied Bayesian networks (e.g., Shachter & Peot, 1989). demonstrate empirically three largepractical Bayesian network models AIS-BN algorithm consistently outperforms156fiAdaptive Importance Sampling Bayesian Networkstwo algorithms. majority test cases, achieved two ordersmagnitude improvement convergence. Improvement speed given desired precisioneven dramatic, although unable report numerical results here,algorithms never achieved precision reached even first iterationsAIS-BN algorithm. main sources improvement are: (1) two heuristicsinitialization importance function based theoretical properties importance sampling finite-dimensional integrals structural advantages Bayesiannetworks, (2) smooth learning method updating importance function, (3)dynamic weighting function combining samples different stages algorithm.study value two heuristics used AIS-BN algorithm: (1) initializationprobability distributions parents evidence nodes uniform distribution(2) adjusting small probabilities conditional probability tables, showplay important role AIS-BN algorithm moderate roleexisting algorithms.remainder paper structured follows. Section 2 first gives generalintroduction importance sampling domain finite-dimensional integrals,originally proposed. show importance sampling used compute probabilities Bayesian networks draw additional benefits graphicalstructure network. develop generalized sampling scheme aid usreviewing previously proposed sampling algorithms describing AIS-BNalgorithm. Section 3 describes AIS-BN algorithm. propose two heuristics initialization importance function discuss theoretical foundations. describesmooth learning method importance function dynamic weighting functioncombining samples different stages algorithm. Section 4 describes empirical evaluation AIS-BN algorithm. Finally, Section 5 suggests several possibleimprovements AIS-BN algorithm, possible applications learning scheme,directions future work.2. Importance Sampling Algorithms Bayesian Networksfeel useful go back theoretical roots importance sampling orderable understand source speedup AIS-BN algorithm relativeexisting state art importance sampling algorithms Bayesian networks. firstreview general idea importance sampling finite-dimensional integralsreduce sampling variance. discuss application importance samplingBayesian networks. Readers interested details directed literatureMonte Carlo methods computation finite integrals, excellent expositionRubinstein (1981) essentially following first section.2.1 Mathematical FoundationsLet g(X) function variables X = (X1 , ..., Xm ) domain Rm ,computing g(X) X feasible. Consider problem approximate computationintegralZI=g(X) dX .157(1)fiCheng & DruzdzelImportance sampling approaches problem writing integral (1)ZI=g(X)f (X) dX ,f (X)f (X), often referred importance function, probability density function. f (X) used importance sampling exists algorithm generatingsamples f (X) importance function zero original functionzero, i.e., g(X) 6= 0 = f (X) 6= 0.independently sampled n points s1 , s2 , . . . , sn , si , accordingprobability density function f (X), estimate integraln1Xg(si )=n i=1 f (si )(2)estimate varianceb (In ) =2nX1g(si )n (n 1) i=1 f (si )2.(3)straightforward show estimator following properties:1. E(In ) =2. limn =n3. n (In I) Normal(0, f2(X) ),f2(X)Z=g(X)f (X)2f (X) dX(4)b 2 (In ) = 2 (In ) = f2 (X) /n4. Evariance proportional f2(X) inversely proportional numbersamples. minimize variance , either increase number samplestry decrease f2(X) . respect latter, Rubinstein (1981) reports followinguseful theorem corollary.Theorem 1 minimum f2(X) equalf2(X)2Z|g(X)| dX=I2occurs X distributed according following probability density functionf (X) = R|g(X)|.|g(X)| dX158fiAdaptive Importance Sampling Bayesian NetworksCorollary 1 g(X) >0, optimal probability density functionf (X) =g(X)f2(X) = 0.Although practice sampling precisely f (X) = g(X)/I occur rarely, expectfunctions close enough still reduce variance effectively. Usually,closer shape function f (X) shape function g(X), smallerf2(X) . high-dimensional integrals, selection importance function, f (X), farcritical increasing number samples, since former dramaticallyaffect f2(X) . seems prudent put energy choosing importance functionwhose shape close possible g(X) apply brute force methodincreasing number samples.worth noting f (X) uniform, importance sampling becomes generalMonte Carlo sampling. Another noteworthy property importance samplingderived Equation 4 avoid f (X) |g(X) f (X)| partdomain sampling, even f (X) matches well g(X)/I important regions.f (X) |g(X) f (X)|, variance become large even infinite.avoid adjusting f (X) larger unimportant regions domain X.section discussed importance sampling continuous variables,results stated valid discrete variables well, case integrationsubstituted summation.2.2 Generic Importance Sampling Algorithm Bayesian Networksfollowing discussion, random variables used multiple-valued, discrete variables.Capital letters, A, B, C, denote random variables. Bold capital letters,A, B, C, denote sets variables. Bold capital letter E usually used denoteset evidence variables. Lower case letters a, b, c denote particular instantiationsvariables A, B, C respectively. Bold lower case letters, a, b, c, denoteparticular instantiations sets A, B, C respectively. Bold lower case letter e,particular, used denote observations, i.e., instantiations set evidencevariables E. Anc(A) denotes set ancestors node A. Pa(A) denotes setparents (direct ancestors) node A. pa(A) denotes particular instantiation Pa(A). \denotes set difference. Pa(A)|E=e denotes use extended vertical bar indicatesubstitution e E A.know joint probability distribution variables Bayesian network model, Pr(X), product probability distributions nodesconditional parents, i.e.,Pr(X) =nPr(Xi |Pa(Xi )) .(5)i=1order calculate Pr(E = e), need sum Pr(X\E, E = e).Pr(E = e) =XPr(X\E, E = e)X\E159(6)fiCheng & Druzdzelsee Equation 6 almost identical Equation 1 except integrationreplaced summation domain replaced X\E. theoretical resultsderived importance sampling reviewed previous section thusdirectly applied computing probabilities Bayesian networks.previous work importance sampling-based algorithms Bayesian networks, postpone discussion work next section.present generic stochastic sampling algorithm help us reviewingprior work presenting algorithm.posterior probability Pr(a|e) obtained first computing Pr(a, e) Pr(e)combining based definition conditional probabilityPr(a|e) =Pr(a, e).Pr(e)(7)order increase accuracy results importance sampling computing posterior probabilities different network variables given evidence, general usedifferent importance functions Pr(a, e) Pr(e). increases computation time linearly gain accuracy may significant given obtainingdesired accuracy exponential nature. often, common practice useimportance function (usually Pr(e)) sample probabilities. difference1. Order nodes according topological order.2. Initialize importance function Pr0 (X\E), desired number samplesm, updating interval l, score arrays every node.3. k 0,4. 15.(i mod l == 0)6.k k+17.Update importance function Prk (X\E) based .end8.si generate sample according Prk (X\E)9.{si }10.Calculate Score(si , Pr(X\E, e), Prk (X\E)) add corresponding entry every score array according instantiated states.end11. Normalize score arrays every node.Figure 1: generic importance sampling algorithm.160fiAdaptive Importance Sampling Bayesian Networksoptimal importance functions two quantities large, perforccmance may deteriorate significantly. Although Pr(a,e) Pr(e)unbiased estimatorscaccording Property 1 (Section 2.1), Pr(a|e)obtained means Equation 7unbiased estimator. However, number samples increases, bias decreasesignored altogether sample size large enough (Fishman, 1995).Figure 1 presents generic stochastic sampling algorithm capturesexisting sampling algorithms. Without loss generality, restrictdescription so-called forward sampling, i.e., generation samples topologicalorder nodes network. forward sampling order accomplishedinitialization performed Step 1, parents node placed nodeitself. forward sampling, Step 8 algorithm, actual generation samples, worksfollows. (i) evidence node instantiated observed state omittedsample generation; (ii) root node randomly instantiated one possiblestates, according importance prior probability node, derivedPrk (X\E); (iii) node whose parents instantiated randomly instantiatedone possible states, according importance conditional probability distributionnode given values parents, also derived Prk (X\E); (iv)procedure followed nodes instantiated. complete instantiation sinetwork based method one sample joint importance probability distributionPrk (X\E) variables network. scoring Step 10 amounts calculatingPr(si , e)/Prk (si ), required Equation 2. ratio total score sumnumber samples unbiased estimator Pr(e). Step 10, also count scoresum condition = a, i.e., unobserved variables values a,ratio score sum number samples unbiased estimatorPr(a, e).existing algorithms focus posterior probability distributions individualnodes. mentioned above, sake efficiency count score sum corresponding Pr(A = a, e), X\E, record score array node A.entry array corresponds specified state A. method introduces additionalvariance, opposed using importance function derived Prk (X\E) samplePr(A = a, e), X\E, directly.2.3 Existing Importance Sampling Algorithms Bayesian Networksmain difference various stochastic sampling algorithms processSteps 2, 7, 8 generic importance sampling algorithm Figure 1.Probabilistic logic sampling (Henrion, 1988) simplest first proposed sampling algorithm Bayesian networks. importance function initialized Step 2Pr(X) never updated (Step 7 null). Without evidence, Pr(X) optimal importance function evidence set, empty anyway. escapes authorsPr(X) may optimal importance function Pr(A = a), X,root node. mismatch optimal actually used importancefunction may result large variance. sampling process evidencewithout evidence except Step 10 count scores samplesinconsistent observed evidence, amounts discarding them.161fiCheng & Druzdzelevidence unlikely, large difference Pr(X) optimalimportance function. Effectively, samples discarded performance logicsampling deteriorates badly.Likelihood weighting (LW) (Fung & Chang, 1989; Shachter & Peot, 1989) enhanceslogic sampling never discards samples. likelihood weighting, importancefunction Step 2fififiPr(X\E) =Pr(xi |Pa(Xi ))fififixi e/.E=eLikelihood weighting update importance function Step 7. Although likelihood weighting improvement logic sampling, convergence rate stillslow large difference optimal importance function Pr(X\E),especially situations evidence unlikely. simplicity,likelihood weighting algorithm commonly used simulation methodBayesian network inference. often matches performance other, sophisticatedschemes simple able increase precision generating samplesalgorithms amount time.Backward sampling (Fung & del Favero, 1994) changes Step 1 generic algorithmallows generating samples evidence nodes direction oppositetopological order nodes network. Step 2, backward sampling uses likelihood observed evidence instantiated nodes calculate Pr0 (X\E).Although Fung del Favero mentioned possibility dynamic node ordering,propose scheme updating importance function Step 7. Backwardsampling suffers problems similar likelihood weighting, i.e., possible mismatch importance function optimal importance functionlead poor convergence.Importance sampling (Shachter & Peot, 1989) generic sampling algorithm. Shachter Peot introduced two variants importance sampling: self-importance(SIS) heuristic importance. importance function used first stepself-importance algorithmfififi0Pr (X\E) =Pr(xi |Pa(Xi ))fififixi e/.E=efunction updated Step 7. algorithm tries revise conditional probabilitytables (CPTs) periodically order make sampling distribution gradually approachposterior distribution. Since data used update importance functioncompute estimator, process introduces bias estimator. Heuristicimportance first removes edges network becomes polytree,uses modified version polytree algorithm (Pearl, 1986) compute likelihoodfunctions unobserved nodes. Pr0 (X\E) combination likelihoodfunctions Pr(X\E, e). Step 7 heuristic importance update Prk (X\E).Shachter Peot (1989) point out, heuristic importance function still leadbad approximation optimal importance function. exist also algorithmscombination self-importance heuristic importance (Shachter & Peot, 1989;162fiAdaptive Importance Sampling Bayesian NetworksShwe & Cooper, 1991). Although researchers suggested may promisingdirection work sampling algorithms, seen results wouldfollow this.separate group stochastic sampling methods formed so-called Markov ChainMonte Carlo (MCMC) methods divided Gibbs sampling, Metropolis sampling,Hybrid Monte Carlo sampling (Geman & Geman, 1984; Gilks, Richardson, & Spiegelhalter, 1996; MacKay, 1998). Roughly speaking, methods draw random samplesunknown target distribution f (X) biasing search distribution towardshigher probability regions. applied Bayesian networks (Pearl, 1987; Chavez &Cooper, 1990) approach determines sampling distribution variableprevious sample given Markov blanket (Pearl, 1988). corresponds updatingPrk (X\E) sampling every node. Prk (X\E) converge optimal importancefunction Pr(e) Pr0 (X\E) satisfies ergodic properties (York, 1992). Sinceconvergence limiting distribution slow calculating updates sampling distribution costly, algorithms used practice often simplelikelihood weighting scheme.also simulation algorithms, bounded variance algorithm(Dagum & Luby, 1997) AA algorithm (Dagum et al., 1995), essentiallybased LW algorithm Stopping-Rule Theorem (Dagum et al., 1995). Canoet al. (1996) proposed another importance sampling algorithm performed somewhatbetter LW cases extreme probability distributions, but, authors state,general cases produced similar results likelihood weighting algorithm. Hernandezet al. (1998) also applied importance sampling reported moderate improvementlikelihood weighting.2.4 Practical Performance Existing Sampling Algorithmslargest network tested using sampling algorithms QMR-DT (QuickMedical Reference Decision Theoretic) (Shwe et al., 1991; Shwe & Cooper, 1991),contains 534 adult diseases 4,040 findings, 40,740 arcs depicting disease-to-findingdependencies. QMR-DT network belongs class special bipartite networksstructure often referred BN2O (Henrion, 1991), two-layercomposition: disease nodes top layer finding nodes bottom layer. Shwecolleagues used algorithm combining self-importance heuristic importancetested convergence properties QMR-DT network. since heuristic methoditerative tabular Bayes (ITB) makes use version Bayes rule designedBN2O networks, cannot generalized arbitrary networks. Although Shwecolleagues concluded Markov blanket scoring self-importance sampling significantlyimprove convergence rate model, cannot extend conclusion generalnetworks. computation Markov blanket scoring complex general multiconnected network BN2O network. Also, experiments conducted lackedgold-standard posterior probability distribution could serve judge convergencerate.Pradhan Dagum (1996) tested efficient version LW algorithm boundedvariance algorithm (Dagum & Luby, 1997) AA algorithm (Dagum et al., 1995)163fiCheng & Druzdzel146 node, multiply connected medical diagnostic Bayesian network. One limitationtests probability evidence cases selected testing ratherhigh. Although 10% cases probability evidence order108 smaller, simple calculation based reported mean = 34.5 numberevidence nodes, shows average probability observed state evidence nodeconditional direct predecessors order (108 )1/34.5 0.59. Givenalgorithm essentially based LW algorithm, based tests suspectperformance deteriorate cases evidence unlikely.algorithms focus marginal probability one hypothesis node. manyqueried nodes, efficiency may deteriorate.tested algorithms discussed Section 2.3 several large networks.experimental results show cases unlikely evidence, none algorithmsconverges reasonable estimates posterior probabilities within reasonable amounttime. convergence becomes worse number evidence nodes increases. Thus,using algorithms large networks, simply cannot trust results.present results tests LW SIS algorithms detail Section 4.3. AIS-BN: Adaptive Importance Sampling Bayesian Networksmain reason existing stochastic sampling algorithms converge slowlyfail learn good importance function sampling process and, effectively,fail reduce sampling variance. importance function optimal,probabilistic logic sampling without evidence, algorithms capableconverging fairly good estimates posterior probabilities within relativelysamples. example, assuming posterior probabilities extreme (i.e., largersay 0.01), 1,000 samples may sufficient obtain good estimates.section, present adaptive importance sampling algorithm Bayesian networks(AIS-BN) that, demonstrate next section, performs welltests. first describe details algorithm prove two theoremsuseful learning optimal importance sampling function.3.1 Basic Algorithm AIS-BNCompared importance sampling used normal finite-dimensional integrals, importance sampling used Bayesian networks several significant advantages. First,network joint probability distribution Pr(X) decomposable factoredcomponent parts. Second, network clear structure, represents many conditional independence relationships. properties helpful estimatingoptimal importance function.basic AIS-BN algorithm presented Figure 2. main differencesAIS-BN algorithm basic importance sampling algorithm Figure 1introduce monotonically increasing weight function wk two effective heuristicinitialization methods Step 2. also introduce special learning component Step 7let updating process run smoothly, avoiding oscillation parameters.164fiAdaptive Importance Sampling Bayesian Networks1. Order nodes according topological order.2. Initialize importance function Pr0 (X\E) using heuristic methods, initialize weight w0 , set desired number samples updatinginterval l, initialize score arrays every node.3. k 0, , wT Score 0, wsum 04. 15.(i mod l == 0)6.k k+17.Update importance function Prk (X\E) wk based .end8.si generate sample according Prk (X\E)9.{si }10.wiScore Score (si , Pr(X\E, e), Prk (X\E), wk )11.wT Score wT Score + wiScore(Optional: add wiScore corresponding entry every score array)12.wsum wsum + wkend13. Output estimate Pr(E) wT Score /wsum(Optional: Normalize score arrays every node)Figure 2: adaptive importance sampling Bayesian Networks (AIS-BN) algorithm.score processing Step 10Pr(si , e)wiScore = wk k.Pr (si )Note respect algorithm Figure 1 becomes special case AIS-BNwk = 1. reason use wk want give different weightssampling results obtained different stages algorithm. stage updatesimportance function, different distance optimal importanceb k ,b k standard deviation estimatedfunction. recommend wk 1/1kstage k using Equation 3. order keep w monotonically increasing, wk smallerwk1 , adjust value wk1 . weighting scheme may introduce bias1. similar weighting scheme based variance apparently developed independently OrtizKaelbling (2000), recommend weight wk 1/(bk )2 .165fiCheng & Druzdzelfinal result. Since initial importance sampling functions often inefficientintroduce big variance results, also recommend wk = 0 firststages algorithm. designed weighting scheme reflect factpractice estimates small estimated variance usually good estimates.3.2 Modifying Sampling Distribution AIS-BNBased theoretical considerations Section 2.1, know crucial elementalgorithm converging good approximation optimal importance function.follows, first give optimal importance function calculating Pr(E = e)discuss use structural advantages Bayesian networks approximatefunction. sequel, use symbol denote importance samplingfunction denote optimal importance sampling function.Since Pr(X\E, E = e) > 0, Corollary 1(X\E) =Pr(X\E, E = e)= Pr(X|E = e) .Pr(E = e)following corollary captures result.Corollary 2 optimal importance sampling function (X\E) calculating Pr(E = e)Equation 6 Pr(X|E = e).Although know mathematical expression optimal importance samplingfunction, difficult obtain function exactly. algorithm, use followingimportance sampling function(X\E) =nPr(Xi |Pa(Xi ), E) .(8)i=1function partially considers effect evidence every nodesampling process. network structure networkabsorbed evidence, function optimal importance sampling function.easy learn and, experimental results show, good approximationoptimal importance sampling function. Theoretically, posterior structuremodel changes drastically result observed evidence, importance samplingfunction may perform poorly. tried find practical networks wouldhappen, day encountered drastic example effect.Section 2.2, know score sums corresponding {xi , pa(Xi ), e}yield unbiased estimator Pr(xi , pa(Xi ), e). According definition conditionalprobability, get estimator Pr0 (xi |pa(Xi ), e). achieved maintaining updating table every node, structure mimicks structureCPT. tables allow us decompose importance function components learned individually. call tables importance conditionalprobability tables (ICPT).Definition 1 importance conditional probability table (ICPT) node X tableposterior probabilities Pr(X|Pa(X), E = e) conditional evidence indexedimmediate predecessors, Pa(X).166fiAdaptive Importance Sampling Bayesian NetworksICPT tables modified process learning importance function.prove useful theorem lead considerable savings learningprocess.Theorem 2Xi X, Xi/ Anc(E) Pr(Xi |Pa(Xi ), E) = Pr(Xi |Pa(Xi )) .(9)Proof: Suppose set values parents node Xi pa(Xi ). Node Xidependent evidence E given pa(Xi ) Xi d-connecting E given pa(Xi )(Pearl, 1988). According definition d-connectivity, happensexists member Xi descendants belongs set evidence nodes E.words Xi/ Anc(E).2Theorem 2 important AIS-BN algorithm. states essentiallyICPT tables nodes ancestors evidence nodes equalCPT tables throughout learning process. need learn ICPT tablesancestors evidence nodes. often lead significant savingscomputation. If, example, evidence nodes root nodes, ICPT tablesevery node already AIS-BN algorithm becomes identical likelihood weightingalgorithm. Without evidence, AIS-BN algorithm becomes identical probabilisticlogic sampling algorithm.worth pointing Xi , Pr(Xi |Pa (Xi ), E) (i.e., ICPT tableXi ), easily calculated using exact methods. example, Xi parentevidence node Ej Ej child Xi , posterior probability distributionXi straightforward compute exactly. Since focus current paperInput: Initialized importance function Pr0 (X\E), learning rate (k).Output: estimated importance function PrS (X\E).stage k 01. Sample l points sk1 , sk2 , . . . , skl independently according current importance function Prk (X\E).2. every node Xi Xi X\E Xi/ Anc(E) count score sumscorresponding {xi , pa(Xi ), e} estimate Pr0 (xi |pa(Xi ), e) based sk1 ,sk2 , . . . , skl .3. Update Prk (X\E) according following formula:Prk+1 (xi |pa(Xi ), e) =Prk (xi |pa(Xi ), e) + (k) Pr0 (xi |pa(Xi ), e) Prk (xi |pa(Xi ), e)endFigure 3: AIS-BN algorithm learning optimal importance function.167fiCheng & Druzdzelsampling, test results reported paper include improvementAIS-BN algorithm.Figure 3 lists algorithm implements Step 7 basic AIS-BN algorithm listedFigure 2. estimate Pr0 (xi |pa(Xi ), e), use samples obtainedcurrent stage. One reason information obtained previous stagesabsorbed Prk (X\E). reason principle, successive iterationaccurate previous one importance function closer optimalimportance function. Thus, samples generated Prk+1 (X\E) bettergenerated Prk (X\E). Pr0 (Xi |pa(Xi ), e) Prk (Xi |pa(Xi ), e) corresponds vectorfirst partial derivatives direction maximum decrease error. (k)positive function determines learning rate. (k) = 0 (lower bound),update importance function. (k) = 1 (upper bound), stagediscard old function. convergence speed directly related (k). small,convergence slow due large number updating steps neededreach local minimum. hand, large, convergence rate initiallyfast, algorithm eventually start oscillate thus may reachminimum. many papers field neural network learning discusschoose learning rate let estimated importance function converge quicklydestination function. method improve learning rate applicablealgorithm. Currently, use following function proposed Ritter et al. (1991)k/kmax(k) =b,(10)initial learning rate b learning rate last step. functionreported perform well neural network learning (Ritter et al., 1991).3.3 Heuristic Initialization AIS-BNdimensionality problem Bayesian network inference equal numbervariables network, networks considered paper high.result, learning space optimal importance function large. Choiceinitial importance function Pr0 (X\E) important factor affecting learninginitial value importance function close optimal importance functiongreatly affect speed convergence. section, present two heuristicshelp achieve goal.Due explicit encoding structure decomposable joint probability distribution, Bayesian networks offer computational advantages compared finite-dimensionalintegrals. possible first approximation optimal importance function priorprobability distribution network variables, Pr(X). propose improvementinitialization. know effect evidence nodes node attenuatedpath length node evidence nodes increased (Henrion, 1989)affected nodes direct ancestors evidence nodes. Initializing ICPTtables parents evidence nodes uniform distributions experience improves convergence rate. Furthermore, CPT tables parents evidencenode E may favorable observed state e probability E = e without168fiAdaptive Importance Sampling Bayesian Networkscondition less small value, Pr(E = e) < 1/(2 nE ), nEnumber outcomes node E. Based observation, change CPT tablesparents evidence node E uniform distributions experimentPr(E = e) < 1/(2 nE ), otherwise leave unchanged. kind initializationinvolves knowledge Pr(E = e), marginal probability without evidence. Probabilistic logic sampling (Henrion, 1988) enhanced Latin hypercube sampling (Cheng &Druzdzel, 2000b) quasi-Monte Carlo methods (Cheng & Druzdzel, 2000a) producegood estimate Pr(E = e). one-time effort made modelbuilding stage worth pursuing desired precision.Another serious problem related sampling extremely small probabilities. Supposeexists root node state prior probability Pr(s) = 0.0001. Letposterior probability state given evidence Pr(s|E) = 0.8. simple calculationshows update importance function every 1, 000 samples, expecthit every 10 updates. Thus ss convergence rate slow.overcome problem setting threshold replacing every probability p <network .2 time, subtract ( p) largest probabilityconditional probability distribution. example, value = 10/l, lupdating interval, allow us sample 10 times often first stagealgorithm. state turns likely (having large weight), increaseprobability even order converge correct answer faster. Consideringavoid f (X) |g(X) f (X)| unimportant region discussedSection 2.1, need make threshold larger. found convergencerate quite sensitive threshold. Based empirical tests, suggest use= 0.04 networks whose maximum number outcomes per node exceed five.smaller threshold might lead fast convergence cases slow convergenceothers. one threshold work, changing specific network usually improveconvergence rate.3.4 Selection Parametersseveral tunable parameters AIS-BN algorithm. base choiceparameters Central Limit Theorem (CLT). According CLT, Z1 , Z2 , . . . ,Zn independent identically distributed random variables E(Zi ) = ZVar(Zi ) = Z2 , = 1, ..., n, Z = (Z1 +...+Zn )/n approximately normally distributedn sufficiently large. Thus,lim P (nfififififiZ z fizZ2Z / n2ex /2 dx .t) =z2(11)Although approximation holds n approaches infinity, CLT knownrobust lead excellent approximations even small n. formula Equation 11(r , ) Relative Approximation, estimate satisfiesP(| |r ) .2. initialization heuristic apparently developed independently Ortiz Kaelbling (2000).169fiCheng & Druzdzelfixed,Z / nr =1Z ( ),z2Z (z) = 12 z ex /2 dx. Since sampling problem, z (correspondingPr(E) Figure 2) fixed, setting r smaller value amounts letting Z / nsmaller. So, adjust parameters based Z / n, estimatedbkusing Equation 3. also theoretical intuition behind recommendation wk 1/Section 3.1. expect work well networks, guaranteesgiven exist always extreme cases sampling algorithmsgood estimate variance obtained.R23.5 Generalization AIS-BN: Problem Estimating Pr(a|e)typical focus systems based Bayesian networks posterior probability variousoutcomes individual variables given evidence, Pr(a|e). generalizedcomputation posterior probability particular instantiation set variablesgiven evidence, i.e., Pr(A = a|e). two methods capable performingcomputation. first method efficient expense precision. secondmethod less efficient, offers general better convergence rates. methodsbased Equation 7.first method reuses samples generated estimate Pr(e) estimating Pr(a, e).Estimation Pr(a, e) amounts counting scored sum condition = a.main advantage method efficiency use set samplesestimate posterior probability state subset network given evidence.main disadvantage variance estimated Pr(a, e) large, especiallynumerical value Pr(a|e) extreme. method widely usedapproach existing stochastic sampling algorithms.second method, used much rarely (e.g., Cano et al., 1996; Pradhan & Dagum,1996; Dagum & Luby, 1997), calls estimating Pr(e) Pr(a, e) separately.estimating Pr(e), additional call algorithm made instantiationset variables interest A. Pr(a, e) estimated sampling networkset observations e extended = a. main advantage methodmuch better reducing variance first method. main disadvantagecomputational cost associated sampling possibly many combinations statesnodes interest.Cano et al. (1996) suggested modified version second method. Supposeinterested posterior distribution Pr(ai |e) possible values ai A, = 1,2, . . . , k. estimate Pr(ai , e) = 1, . . . , k separately, use valuePki=1 Pr(ai , e) estimate Pr(e). assumption behind approachestimate Pr(e) accurate large sample drawn.However, even guarantee small variance every Pr(ai , e), cannot guaranteesum also small variance. So, AIS-BN algorithm usepure form methods. algorithm listed Figure 2 based firstmethod optional computations Steps 12 13 performed. algorithm170fiAdaptive Importance Sampling Bayesian Networkscorresponding second method skips optional steps calls basic AIS-BNalgorithm twice estimate Pr(e) Pr(a, e) separately.first method attractive simplicity possible computationalefficiency. However, shown Section 2.2, performance sampling algorithm uses one set samples (as first method above) estimate Pr(a|e)deteriorate difference optimal importance functions Pr(a,e)Pr(e) large. main focus computation high accuracy posterior probability distribution small number nodes, strongly recommend use algorithmbased second method. Also, algorithm easily used estimate confidenceintervals solution.4. Experimental Resultssection, first describe experimental method used tests. tests focusCPCS network, one largest realistic networks availableknow precisely nodes observable. were, therefore, ablegenerate realistic test cases. Since AIS-BN algorithm uses two initializationheuristics, designed experiment studies contribution twoheuristics performance algorithm. probe extent AIS-BN algorithmsexcellent performance, test several real large networks.4.1 Experimental Methodperformed empirical tests comparing AIS-BN algorithm likelihood weighting(LW) self-importance sampling (SIS) algorithms. two algorithms basicallystate art general purpose belief updating algorithms. AA (Dagum et al.,1995) bounded variance (Dagum & Luby, 1997) algorithms, suggestedreviewer, essentially enhanced special purpose versions basic LW algorithm.implementation three algorithms relied essentially code separatefunctions algorithms differed. fair assume, therefore, observeddifferences purely due theoretical differences among algorithms dueefficiency implementation. order make comparison AIS-BN algorithmLW SIS fair, used first method computation (Section 3.5), i.e., onerelies single sampling rather calling basic AIS-BN algorithm twice.measured accuracy approximation achieved simulation termsMean Square Error (MSE), i.e., square root sum square differences Pr0 (xij )Pr(xij ), sampled exact marginal probabilities state j (j = 1, 2, . . . , ni )node i, Xi/ E. precisely,vuuMSE = P1Xi N\E niXniX(Pr0 (xij ) Pr(xij ))2 ,Xi N\E j=1N set nodes, E set evidence nodes, ni numberoutcomes node i. diagrams, reported MSE averaged 10 runs. usedclustering algorithm (Lauritzen & Spiegelhalter, 1988) compute gold standard171fiCheng & Druzdzelresults comparisons mean square error. performed experimentsPentium II, 333 MHz Windows computer.MSE perfect, simplest way capturing error lendstheoretical analysis. example, possible derive analytically idealizedconvergence rate terms MSE, which, turn, used judge qualityalgorithm. MSE used virtually previous tests sampling algorithms,allows interested readers tie current results past studies. reviewer offeredinteresting suggestion using cross-entropy technique weights smallchanges near zero much strongly equivalent size change middle[0, 1] interval. measure would penalize algorithm imprecisions possiblyseveral orders magnitude small probabilities. idea interesting,aware theoretical reasons measure would make differencecomparisons AIS-BN, LW SIS algorithms. MSE, mentioned above,allow us compare empirically determined convergence rate theoreticallyderived ideal convergence rate. Theoretically, MSE inversely proportionalsquare root sample size.Since several tunable parameters used AIS-BN algorithm, listvalues parameters used test: l = 2, 500; wk = 0 k 9 wk = 1otherwise. stopped updating process Step 7 Figure 2 k 10.words, used samples collected last step algorithm. learningparameters used algorithm kmax = 10, = 0.4, b = 0.14 (see Equation 10).used empirically determined value threshold = 0.04 (Section 3.3).change CPT tables parents special evidence node uniform distributionsPr(A = a) < 1/(2 nA ). parameters matter design decision(e.g., number samples tests), others chosen empirically. Althoughfound parameters may different optimal values different Bayesiannetworks, used values tests AIS-BN algorithm describedpaper. Since set parameters led spectacular improvement accuracytested networks, fair say superiority AIS-BN algorithmalgorithms sensitive values parameters.SIS algorithm, wk = 1 design algorithm. used l = 2, 500.updating function Step 7 Figure 1 (Shwe et al., 1991; Cousins, Chen, &Frisse, 1993):Prknew (xi |pa(Xi ), e) =cPr(xi |pa(Xi )) + k Prcurrent (xi |pa(Xi ), e) ,1+kcPr(xi |pa(Xi )) original sampling distribution, Prcurrent (xi |pa(Xi ), e)equivalent ICPT tables estimator based currently available information,k updating step.4.2 Results CPCS Networkmain network used tests subset CPCS (Computer-based Patient CaseStudy) model (Pradhan et al., 1994), large multiply-connected multi-layer network consisting 422 multi-valued nodes covering subset domain internal medicine.172fiAdaptive Importance Sampling Bayesian NetworksAmong 422 nodes, 14 nodes describe diseases, 33 nodes describe history risk factors, remaining 375 nodes describe various findings related diseases.CPCS network among largest real networks available research communitypresent time. CPCS network contains many extreme probabilities, typicallyorder 104 . analysis based subset 179 nodes CPCS network,created Max Henrion Malcolm Pradhan. used smaller version orderable compute exact solution purpose measuring approximation errorsampling algorithms.AIS-BN algorithm learning overhead. following comparison execution time vs. number samples may give reader idea overhead. UpdatingCPCS network 20 evidence nodes system takes AIS-BN algorithmtotal 8.4 seconds learn. generates subsequently 3,640 samples per second,SIS algorithm generates 2,631 samples per second, LW algorithm generates 4,167samples per second. order remain conservative towards AIS-BN algorithm,experiments fixed execution time algorithms (our limit 60 seconds)rather number samples. CPCS network 20 evidence nodes, 60seconds, AIS-BN generates 188,000 samples, SIS generates 158,000 samplesLW generates 250,000 samples.12100%90%1080%Frequency870%60%650%40%430%20%210%00%1E-401E-341E-281E-221E-161E-10Probability evidenceFigure 4: probability distribution evidence Pr(E = e) experiments.generated total 75 test cases consisting five sequences 15 test cases each.ran test case 10 times, time different setting random number seed.sequence progressively higher number evidence nodes: 15, 20, 25, 30,35 evidence nodes respectively. evidence nodes chosen randomly (equiprobablesampling without replacement) nodes described various plausible medical173fiCheng & Druzdzelfindings. Almost nodes leaf nodes network. believeconstituted realistic test cases algorithms. distribution prior probability evidence, Pr(E = e), across test runs experiments shown Figure 4.least likely evidence 5.54 1042 , likely evidence 1.37 109 ,median 7 1024 .0.30AIS-BNSISLWMean Square Error0.250.200.150.100.050.00153045607590105120135150Sample time (seconds)Figure 5: typical plot convergence tested sampling algorithms experimentsMean Square Error function execution time subsetCPCS network 20 evidence nodes chosen randomly among plausible medicalobservations (Pr(E = e) = 3.33 1026 particular case) AIS-BN,SIS, LW algorithms. curve AIS-BN algorithmclose horizontal axis.Figures 5 6 show typical plot convergence tested sampling algorithmsexperiments. case illustrated involves updating CPCS network 20 evidencenodes. plot MSE initial 15 seconds algorithms startconverging. particular, learning step AIS-BN algorithm usually completedwithin first 9 seconds. ran three algorithms case 150 seconds rather60 seconds actual experiment order able observe wider rangeconvergence. plot MSE AIS-BN algorithm almost touches X axisFigure 5. Figure 6 shows plot finer scale order show detailAIS-BN convergence curve. clear AIS-BN algorithm dramatically improvesconvergence rate. also see results AIS-BN converge exact resultsfast sampling time increases. case captured Figures 5 6, tenfoldincrease sampling time (after subtracting overhead AIS-BN algorithm,174fiAdaptive Importance Sampling Bayesian Networks0.0025AIS-BNMean Square Error0.00200.00150.00100.00050.0000153045607590105120135150Sample time (seconds)Figure 6: lower part plot Figure 5 showing convergence AIS-BNalgorithm correct posterior probabilities.corresponds 21.5-fold increase number samples) results 4.55-fold decreaseMSE (to MSE 0.00048). observed convergence SIS LW algorithmspoor. tenfold increase sampling time practically effect accuracy. Pleasenote typical case observed experiments.AbsentMildModerateSevereOriginal CPT0.996310.001830.000930.00093Exact ICPT0.00370.15600.11900.7213Learned ICPT0.0150.1640.1310.690Table 1: fragment conditional probability table node CPCS network(node gasAcute, parents hepAcute=Mild wbcTotTho=False) Figure 6.Figure 7 illustrates ICPT learning process AIS-BN algorithm samplecase shown Figure 6. displayed conditional probabilities belong node gasAcuteparent two evidence nodes, difInfGasMuc abdPaiExaMea. nodegasAcute four states: absent, mild, moderate, severe, two parents.randomly chose combination parents states displayed configuration.original CPT configuration without evidence, exact ICPT evidencelearned ICPT evidence summarized numerically Table 1. Figure 7 illustrates175fiCheng & Druzdzel0.8AbsentMildModerateSevere0.7Probability0.60.50.40.30.20.10012345678910Updating stepFigure 7: Convergence conditional probabilities example run AISBN algorithm captured Figure 6. displayed fragment conditionalprobability table belongs node gasAcute parent one evidencenodes.learned importance conditional probabilities begin converge exact resultsstably three updating steps. learned probabilities Step 10 closeexact results. example, difference Pr(xi |pa(Xi ), e) Pr(xi |pa(Xi ))large. Sampling Pr(xi |pa(Xi )) instead Pr(xi |pa(Xi ), e) would introduce largevariance results.minmedianmaxAIS-BN0.000820.000220.000490.000780.00184SIS0.1100.0760.00160.1050.316LW0.1480.0930.00310.1540.343Table 2: Summary simulation results 75 simulation cases CPCSnetwork. Figure 8 shows 75 cases graphically.Figure 8 shows MSE 75 test cases experiments summarystatistics Table 2. paired one-tailed t-test resulted statistically highly significantdifferences AIS-BN SIS algorithms (p < 3.1 1020 ), also176fiAdaptive Importance Sampling Bayesian Networks1AIS-BNSISLWMean Square Error0.10.010.0010.00011.4E-09 2.1E-14 3.1E-18 5.7E-22 6.7E-24 1.4E-27 1.3E-32 3.8E-39Probability evidenceFigure 8: Performance AIS-BN, SIS, LW algorithms: Mean Square Error75 individual test cases plotted probability evidence.sampling time 60 seconds.SIS LW algorithms (p < 1.7 108 ). far magnitude differenceconcerned, AIS-BN significantly better SIS. SIS better LW,difference small. mean MSEs SIS LW algorithms greater0.1, suggests neither algorithms suitable large Bayesian networks.graph Figure 9 shows MSE ratio AIS-BN SIS algorithms.see percentage cases whose ratio greater 100 (two ordersmagnitude improvement!) 60%. words, obtained two orders magnitudeimprovement MSE half cases. 80% cases, ratio greater50. smallest ratio experiments 2.67, happened posteriorprobabilities dominated prior probabilities. case, even though LWSIS algorithms converged fast, MSE still far larger AIS-BN.next experiment aimed showing close AIS-BN algorithm approachbest possible sampling results. know optimal importance sampling function,convergence AIS-BN algorithm forward samplingwithout evidence. words, results probabilistic logic sampling algorithmwithout evidence approach limit well stochastic sampling perform. ranlogic sampling algorithm CPCS network without evidence mimicking testruns AIS-BN algorithm, i.e., 5 blocks 15 runs, repeated 10 timesdifferent random number seed. number samples generated equal averagenumber samples generated AIS-BN algorithm series 15 test runs.177fiCheng & Druzdzel18100%1690%80%1470%Frequency1260%1050%840%630%420%2010%fffiff ff ff0%ratio MSE SIS AIS-BNFigure 9: ratio MSE SIS AIS-BN versus percentage.obtained average MSE = 0.00057, = 0.000025, min = 0.00052,max = 0.00065. best results around range. Table 2,see minimum MSE AIS-BN algorithm 0.00049, within rangeoptimal result. mean MSE AIS-BN 0.00082, far optimalresults. standard deviation, , significantly larger AIS-BN algorithm,understandable given process learning optimal importance functionheuristic nature. difficult understand exist differenceAIS-BN results optimal results. First, AIS-BN algorithm tests updatedsampling distribution 10 times, may times let convergeoptimal importance distribution. Second, even algorithm convergedoptimal importance distribution, sampling algorithm still let parameteroscillate around distribution always small differences twodistributions.Figure 10 shows convergence rate tested cases four-fold increasesampling time (between 15 60 seconds). adjusted convergence ratioAIS-BN algorithm dividing constant. According Equation 3, theoreticallyexpected convergence ratio four-fold increase number samplesaround two. 96% cases among AIS-BN runs whose ratio laysinterval (1.75, 2.25], sharp contrast 11% 13% cases SIS LWalgorithms. ratios remaining 4% cases AIS-BN lay interval [2.25, 2.5].SIS LW algorithms, percentage cases whose ratio smaller 1.571% 77% respectively. Less 1.5 means number samplessmall estimate variance results cannot trusted. ratio greater 2.25178fiAdaptive Importance Sampling Bayesian Networks70%AIS-BNSISLW59%60%Frequency50%40%37%36%35%30%20%20%20%19%11%9%9%10%5%8%5%5%5%3%0%0%4%3%3%4%0%0%0.5 - 0.75 0.75 - 1.0 1.0 - 1.25 1.25 - 1.5 1.5 - 1.75 1.75 - 2.0 2.0 - 2.25 2.25 - 2.5 2.5 - 2.75 2.75 - 3.0Convergence rateFigure 10: distribution convergence ratio AIS-BN, SIS, LW algorithms number samples increases four times.means possibly 60 seconds long enough estimate variance, 15 secondsshort.4.3 Role AIS-BN Heuristics Performance Improvementexperimental results see AIS-BN algorithm improvesampling performance significantly. next series tests focused studying roletwo AIS-BN initialization heuristics. first initializing ICPT tablesparents evidence uniform distributions, denoted U. second adjusting smallprobabilities, denoted S. denote AIS-BN without heuristic initialization methodAIS algorithm. AIS+U+S equals AIS-BN. compared following versionsalgorithms: SIS, AIS, SIS+U, AIS+U, SIS+S, AIS+S, SIS+U+S, AIS+U+S.algorithms SIS used number samples SIS. algorithms AIS usednumber samples AIS-BN. tested algorithms 75 testcases used previous experiment. Figure 11 shows MSE samplingalgorithms summary statistics Table 3. Even though AIS algorithm betterSIS algorithm, difference large case AIS+U, AIS+S,AIS-BN algorithms. seems heuristic initialization methods help much. resultsSIS+S, SIS+U, SIS+U+S algorithms suggest although heuristic initializationmethods improve performance, alone cannot improve much. fair saysignificant performance improvement AIS-BN algorithm comingcombination AIS heuristic methods, method alone. difficult179fiCheng & Druzdzelunderstand that, good heuristic initialization methods possible letlearning process quickly exit oscillation areas. Although U methods aloneimprove performance, improvement moderate compared combinationtwo.0.120.110Mean Square Error0.100.0750.080.0600.060.0500.0500.040.020.0080.001510.00fi fffi ffff fiff0.00082ff ffDifferent AlgorithmsFigure 11: comparison different algorithms CPCS network. bar based75 test cases. dotted bar shows MSE SIS algorithmgray bar shows MSE AIS algorithm.minmedianmaxSIS0.1100.0760.00160.1050.316AIS0.0600.0490.000740.0450.207SIS+U0.0500.0520.00110.0310.212AIS+U0.00840.0250.000580.00140.208SIS+S0.0750.0740.000720.0520.279AIS+S0.00150.00160.000560.000870.0085SIS+U+S0.0500.0590.000860.0280.265AIS-BN0.000820.000220.000490.000780.0018Table 3: Summary simulation results different algorithms CPCS network.4.4 Results Networksorder make sure AIS-BN algorithm performs well general, testedtwo large networks.first network used tests PathFinder network (Heckermanet al., 1990), core element expert system assists surgical pathologists180fiAdaptive Importance Sampling Bayesian Networksdiagnosis lymph-node diseases. two versions network. usedlarger version, consisting 135 nodes. contrast CPCS network, PathFindercontains many conditional probabilities equal 1, reflects deterministicrelationships certain settings. make sampling challenging, randomly selected20 evidence nodes among leaf nodes. observable node (DavidHeckerman, personal communication). verified case probabilityselected evidence equal zero.fixed execution time algorithms 60 seconds. learning overheadAIS-BN algorithm PathFinder network 3.5 seconds. 60seconds, AIS-BN generated 366,000 samples, SIS generated 250,000 samplesLW generated 2,700,000 samples. reason LW could generate10 times many samples SIS within amount time LW algorithmterminates sample generation early stage many samples, weightsample becomes zero. result determinism probability tables, mentionedabove. see LW benefits greatly generating samples.parameters used AIS-BN used CPCS network.tested 20 cases, randomly selected 20 evidence nodes. reported MSEcase averaged 10 runs. runs SIS LW algorithmsmanage generate effective samples (the weight score sum equal zero). SIS75% effective runs LW 89% effective runs, meansruns SIS LW unable yield information posterior distributions.cases, discarded run averaged effective runs.runs AIS-BN algorithm effective. report experimental resultssummary statistics Table 4. data, see AIS-BN algorithmstill significantly better SIS LW algorithms. Since LW algorithmgenerate ten times number samples SIS algorithm, performancebetter SIS algorithm.minmedianmaxeffective runsAIS-BN0.000500.000370.000250.000370.0017200SIS0.1660.1070.001160.1840.467150LW0.0890.07070.000800.08660.294178Table 4: Summary simulation results 20 simulation casesPathFinder network.second network tested one ANDES networks (Conati et al.,1997). ANDES intelligent tutoring system classical Newtonian physicsdeveloped team researchers Learning Research Development CenterUniversity Pittsburgh researchers United States Naval Academy.student model ANDES uses Bayesian network longterm knowledge assessment,181fiCheng & Druzdzelplan recognition, prediction students actions problem solving. selectedlargest ANDES network available us, consisting 223 nodes.contrast previous two networks, depth ANDES network significantly larger connectivity. 22 leaf nodes. quitepredictable kind networks pose difficulties learning. selected 20evidence nodes randomly potential evidence nodes tested 20 cases. parameters used CPCS network. fixed execution timealgorithms 60 seconds. learning overhead AIS-BN algorithmANDES network 13.4 seconds. 60 seconds, AIS-BN generated 114,000samples, SIS generated 98,000 samples LW generated 180,000 samples.network, LW still generate almost two times number samples generatedSIS algorithm.report experimental results summary statistics Table 5. resultsshow also ANDES network AIS-BN algorithm significantly betterSIS LW algorithms. Since LW generated almost two times number samplesgenerated SIS algorithm, performance better SISalgorithm.minmedianmaxAIS-BN0.00590.00490.00230.00450.0237SIS0.06280.1020.00280.01900.321LW0.04040.05390.00280.01980.221Table 5: Summary simulation results 20 simulation cases ANDESnetwork.AIS-BN algorithm average order magnitude precisetwo algorithms, performance improvement smallertwo networks. reason performance improvement AIS-BN algorithmSIS LW algorithms ANDES network smaller comparedCPCS PathFinder networks that: (1) ANDES network used testsapparently challenging enough sampling algorithms general. ANDESnetwork, SIS LW also perform well cases. minimum MSE SISLW tested cases almost AIS-BN. (2) number samplesgenerated AIS-BN network significantly smaller previoustwo networks AIS-BN needs time learn. Although increasing numbersamples improve performance three algorithms, improves performanceAIS-BN since convergence ratio AIS-BN algorithm usually largerSIS LW (see Figure 10). (3) parameters used networktuned CPCS network. (4) large depth fewer leaf nodes ANDESnetwork pose difficulties learning.182fiAdaptive Importance Sampling Bayesian Networks5. Discussionfundamental trade-off AIS-BN algorithm time spentlearning importance function time spent sampling. current approach,believe reasonable, stop learning point importancefunction good enough. experiments stopped learning 10 iterations.several ways improving initialization conditional probabilitytables outset AIS-BN algorithm. current version algorithm,initialize ICPT table every parent N evidence node E (N Pa(E), E E)uniform distribution Pr(E = e) < 1/(2 nE ). improved further.extend initialization nodes severely affected evidence.identified examining network structure local CPTs.view learning process AIS-BN algorithm network rebuildingprocess. algorithm constructs new network whose structure originalnetwork (except delete evidence nodes corresponding arcs). constructednetwork models joint probability distribution (X\E) Equation 8, approachesoptimal importance function. use learned 0 approximate distribution.0 approximates Pr(X|E) accurately enough, use new network solveapproximate tasks, problem computing Maximum A-Posterior assignment(MAP) (Pearl, 1988), finding k likely scenarios (Seroussi & Golmard, 1994), etc.large advantage approach solve problems networkevidence nodes.know Markov blanket scoring improve convergence rates samplingalgorithms (Shwe & Cooper, 1991). may also applied AIS-BN algorithmimprove convergence rate. According Property 4 (Section 2.1), technique2creduce variance Prreduce variance Pr(e)correspondingly improve(e)sampling performance. Since variance stratified sampling (Rubinstein, 1981)never much worse random sampling, much better, improveconvergence rate. expect variance reduction methods statistics, as:(i) expected value random variable; (ii) antithetic variants correlations (stratifiedsampling, Latin hypercube sampling, etc.); (iii) systematic sampling, also improvesampling performance.Current learning algorithm used simple approach. heuristic learning methods,adjusting learning rates according changes error (Jacobs, 1988),also applicable algorithm. several tunable parameters AIS-BNalgorithm. Finding optimal values parameters given network anotherinteresting research topic.worth observing plots presented Figure 8 fairly flat. words,tests convergence sampling algorithms depend stronglyprobability evidence. seems contradict common belief forward samplingschemes suffer unlikely evidence. AIS-BN one shows fairly flat plot.convergence SIS LW algorithms seems decrease slightly unlikely evidence.possible three algorithms perform much worse probabilityevidence drops threshold value, tests failed approach.183fiCheng & Druzdzelrelationship studied carefully, conjecture probability evidencegood measure difficulty approximate inference.Given problem approximating probabilistic inference NP-hard, existnetworks challenging algorithm doubt evenAIS-BN algorithm perform poorly them. day, foundnetworks. one characteristic networks may challenging AIS-BNalgorithm. general, number parameters need learned AISBN algorithm increases, performance deteriorate. Nodes many parents,example, challenging AIS-BN learning algorithm, update ICPTtables combinations parent nodes. possible conditional probabilitydistributions causal independence properties, Noisy-OR distributions (Pearl,1988; Henrion, 1989; Diez, 1993; Srinivas, 1993; Heckerman & Breese, 1994), commonlarge practical networks, treated differently lead considerable savingslearning time.One direction testing approximate algorithms, suggested us reviewer, uselarge networks exact solution cannot computed all. case, onetry infer difference variance various stages algorithm whetherconverging not. interesting idea worth exploring, especiallycombined theoretical work stopping criteria line work DagumLuby (1997).6. ConclusionComputational complexity remains major problem application probability theorydecision theory knowledge-based systems. important develop schemesimprove performance updating algorithms even though theoretically demonstrated worst case remain NPhard, many practical cases may become tractable.paper, studied importance sampling Bayesian networks. reviewingimportant theoretical results related importance sampling finite-dimensionalintegrals, proposed new algorithm importance sampling Bayesian networkscall adaptive importance sampling (AIS-BN). process learning optimalimportance function AIS-BN algorithm computationally intractable, basedtheory importance sampling finite-dimensional integrals proposed several heuristicsseem work well practice. proposed heuristic methods initializingimportance function shown accelerate learning process, smooth learning method updating importance function using structural advantages Bayesiannetworks, dynamic weighting function combining samples different stagesalgorithm. methods help AIS-BN algorithm get fairly accurateestimates posterior probabilities limited time. two applied heuristics,adjustment small probabilities, seems lead largest improvement performance,although largest decrease MSE achieved combination two heuristicsAIS-BN algorithm.AIS-BN algorithm lead dramatic improvement convergence rateslarge Bayesian networks evidence compared existing state art algorithms.compared performance AIS-BN algorithm performance likelihood184fiAdaptive Importance Sampling Bayesian Networksweighting self-importance sampling large practical model, CPCS network,evidence unlikely 5.54 1042 typically 7 1.024 . experiments,observed AIS-BN algorithm always better likelihood weighting selfimportance sampling 60% cases reached two orders magnitudeimprovement accuracy. Tests performed two networks, PathFinderANDES, yielded similar results.Although may exist approximate algorithms prove superior AISBN networks special structure distribution, AIS-BN algorithm simplerobust general evidential reasoning problems large multiply-connected Bayesiannetworks.Acknowledgmentsthank anonymous referees several insightful comments led substantialimprovement paper. research supported National Science FoundationFaculty Early Career Development (CAREER) Program, grant IRI9624629,Air Force Office Scientific Research grants F496209710225 F496200010112. earlier version paper received 2000 School Information SciencesRobert R. Korfhage Award, University Pittsburgh. Malcolm Pradhan Max HenrionInstitute Decision Systems Research shared us CPCS network kindpermission developers Internist system University Pittsburgh.thank David Heckerman PathFinder network Abigail Gerner ANDESnetwork used tests. experimental data obtained using SMILE,Bayesian inference engine developed Decision Systems Laboratory availablehttp://www2.sis.pitt.edu/genie.ReferencesCano, J. E., Hernandez, L. D., & Moral, S. (1996). Importance sampling algorithmspropagation probabilities belief networks. International Journal ApproximateReasoning, 15, 7792.Chavez, M. R., & Cooper, G. F. (1990). randomized approximation algorithm probabilistic inference Bayesian belief networks. Networks, 20 (5), 661685.Cheng, J., & Druzdzel, M. J. (2000a). Computational investigations low-discrepancysequences simulation algorithms Bayesian networks. Proceedings Sixteenth Annual Conference Uncertainty Artificial Intelligence (UAI2000), pp.7281 San Francisco, CA. Morgan Kaufmann Publishers.Cheng, J., & Druzdzel, M. J. (2000b). Latin hypercube sampling Bayesian networks.Proceedings 13th International Florida Artificial Intelligence Research Symposium Conference (FLAIRS-2000), pp. 287292 Orlando, Florida.Conati, C., Gertner, A. S., VanLehn, K., & Druzdzel, M. J. (1997). On-line student modelingcoached problem solving using Bayesian networks. Proceedings Sixth185fiCheng & DruzdzelInternational Conference User Modeling (UM96), pp. 231242 Vienna, New York.Springer Verlag.Cooper, G. F. (1990). computational complexity probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42 (23), 393405.Cousins, S. B., Chen, W., & Frisse, M. E. (1993). tutorial introduction stochasticsimulation algorithm belief networks. Artificial Intelligence Medicine, chap. 5,pp. 315340. Elsevier Science Publishers B.V.Dagum, P., Karp, R., Luby, M., & Ross, S. (1995). optimal algorithm MonteCarlo estimation (extended abstract). Proceedings 36th IEEE SymposiumFoundations Computer Science, pp. 142149 Portland, Oregon.Dagum, P., & Luby, M. (1993). Approximating probabilistic inference Bayesian beliefnetworks NP-hard. Artificial Intelligence, 60 (1), 141153.Dagum, P., & Luby, M. (1997). optimal approximation algorithm Bayesian inference.Artificial Intelligence, 93, 127.Diez, F. J. (1993). Parameter adjustment Bayes networks. generalized noisy ORgate. Proceedings Ninth Annual Conference Uncertainty ArtificialIntelligence (UAI93), pp. 99105 San Francisco, CA. Morgan Kaufmann Publishers.Fishman, G. S. (1995). Monte Carlo: concepts, algorithms, applications. SpringerVerlag.Fung, R., & Chang, K.-C. (1989). Weighing integrating evidence stochastic simulation Bayesian networks. Uncertainty Artificial Intelligence 5, pp. 209219New York, N. Y. Elsevier Science Publishing Company, Inc.Fung, R., & del Favero, B. (1994). Backward simulation Bayesian networks. ProceedingsTenth Annual Conference Uncertainty Artificial Intelligence (UAI94),pp. 227234 San Francisco, CA. Morgan Kaufmann Publishers.Geman, S., & Geman, D. (1984). Stochastic relaxations, Gibbs distributions Bayesian restoration images. IEEE Transactions Pattern Analysis MachineIntelligence, 6 (6), 721742.Gilks, W., Richardson, S., & Spiegelhalter, D. (1996). Markov chain Monte Carlo practice. Chapman Hall.Heckerman, D., & Breese, J. S. (1994). new look causal independence. ProceedingsTenth Annual Conference Uncertainty Artificial Intelligence (UAI94),pp. 286292 San Mateo, CA. Morgan Kaufmann Publishers, Inc.Heckerman, D. E., Horvitz, E. J., & Nathwani, B. N. (1990). Toward normative expertsystems: Pathfinder project. Tech. rep. KSL9008, Medical Computer ScienceGroup, Section Medical Informatics, Stanford University, Stanford, CA.186fiAdaptive Importance Sampling Bayesian NetworksHenrion, M. (1988). Propagating uncertainty Bayesian networks probabilistic logicsampling. Uncertainty Artificial Intellgience 2, pp. 149163 New York, N. Y.Elsevier Science Publishing Company, Inc.Henrion, M. (1989). practical issues constructing belief networks. Kanal, L.,Levitt, T., & Lemmer, J. (Eds.), Uncertainty Artificial Intelligence 3, pp. 161173.Elsevier Science Publishers B.V., North Holland.Henrion, M. (1991). Search-based methods bound diagnostic probabilities largebelief nets. Proceedings Seventh Annual Conference Uncertainty Artificial Intelligence (UAI91), pp. 142150 San Mateo, California. Morgan KaufmannPublishers.Hernandez, L. D., Moral, S., & Antonio, S. (1998). Monte Carlo algorithm probabilisticpropagation belief networks based importance sampling stratified simulationtechniques. International Journal Approximate Reasoning, 18, 5391.Jacobs, R. A. (1988). Increased rates convergence learning rate adaptation.Neural Networks, 1, 295307.Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computations probabilitiesgraphical structures application expert systems. Journal RoyalStatistical Society, Series B (Methodological), 50 (2), 157224.MacKay, D. (1998). Intro Monte Carlo methods. Jordan, M. I. (Ed.), LearningGraphical Models. MIT Press, Cambridge, Massachusetts.Ortiz, L. E., & Kaelbling, L. P. (2000). Adaptive importance sampling estimationstructured domains. Proceedings Sixteenth Annual Conference Uncertainty Artificial Intelligence (UAI2000), pp. 446454 San Francisco, CA. MorganKaufmann Publishers.Pearl, J. (1986). Fusion, propagation, structuring belief networks. Artificial Intelligence, 29 (3), 241288.Pearl, J. (1987). Evidential reasoning using stochastic simulation causal models. ArtificalIntelligence, 32, 245257.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks PlausibleInference. Morgan Kaufmann Publishers, Inc., San Mateo, CA.Pradhan, M., & Dagum, P. (1996). Optimal Monte Carlo inference. ProceedingsTwelfth Annual Conference Uncertainty Artificial Intelligence (UAI96), pp.446453 San Francisco, CA. Morgan Kaufmann Publishers.Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineeringlarge belief networks. Proceedings Tenth Annual Conference Uncertainty Artificial Intelligence (UAI94), pp. 484490 San Francisco, CA. MorganKaufmann Publishers.187fiCheng & DruzdzelRitter, H., Martinetz, T., & Schulten, K. (1991). Neuronale Netze. Addison-Wesley,Munchen.Rubinstein, R. Y. (1981). Simulation Monte Carlo Method. John Wiley & Sons.Seroussi, B., & Golmard, J. L. (1994). algorithm directly finding K probableconfigurations Bayesian networks. International Journal Approximate Reasoning,11, 205233.Shachter, R. D., & Peot, M. A. (1989). Simulation approaches general probabilisticinference belief networks. Uncertainty Artificial Intelligence 5, pp. 221231New York, N. Y. Elsevier Science Publishing Company, Inc.Shwe, M. A., & Cooper, G. F. (1991). empirical analysis likelihood-weighting simulation large, multiply-connected medical belief network. Computers BiomedicalResearch, 24 (5), 453475.Shwe, M., Middleton, B., Heckerman, D., Henrion, M., Horvitz, E., & Lehmann, H. (1991).Probabilistic diagnosis using reformulation INTERNIST1/QMR knowledgebase: I. probabilistic model inference algorithms. Methods InformationMedicine, 30 (4), 241255.Srinivas, S. (1993). generalization noisy-OR model. Proceedings NinthAnnual Conference Uncertainty Artificial Intelligence (UAI93), pp. 208215San Francisco, CA. Morgan Kaufmann Publishers.York, J. (1992). Use Gibbs sampler expert systems. Artificial Intelligence, 56,115130.188fiJournal Artificial Intelligence Research 13 (2000) 33{94Submitted 9/99; published 8/00Value-Function Approximations Partially ObservableMarkov Decision ProcessesMilos Hauskrechtmilos@cs.brown.eduComputer Science Department, Brown UniversityBox 1910, Brown University, Providence, RI 02912, USAAbstractPartially observable Markov decision processes (POMDPs) provide elegant mathematical framework modeling complex decision planning problems stochasticdomains states system observable indirectly, via set imperfectnoisy observations. modeling advantage POMDPs, however, comes price |exact methods solving computationally expensive thus applicablepractice simple problems. focus ecient approximation (heuristic)methods attempt alleviate computational problem trade accuracyspeed. two objectives here. First, survey various approximation methods,analyze properties relations provide new insights differences.Second, present number new approximation methods novel refinements existing techniques. theoretical results supported experiments problemagent navigation domain.1. IntroductionMaking decisions dynamic environments requires careful evaluation cost benefits immediate action also choices may future.evaluation becomes harder effects actions stochastic, must pursue evaluate many possible outcomes parallel. Typically, problem becomescomplex look future. situation becomes even worseoutcomes observe imperfect unreliable indicators underlying processspecial actions needed obtain reliable information. Unfortunately, manyreal-world decision problems fall category.Consider, example, problem patient management. patient comeshospital initial set complaints. rarely allow physician (decisionmaker) diagnose underlying disease certainty, number disease optionsgenerally remain open initial evaluation. physician multiple choicesmanaging patient. He/she choose nothing (wait see), order additional testslearn patient state disease, proceed radical treatment(e.g. surgery). Making right decision easy task. disease patient suffersprogress time may become worse window opportunity particulareffective treatment missed. hand, selection wrong treatment maymake patient's condition worse, may prevent applying correct treatment later.result treatment typically non-deterministic outcomes possible.addition, treatment investigative choices come different costs. Thus,c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiHauskrechtcourse patient management, decision-maker must carefully evaluate costsbenefits current future choices, well interaction ordering.decision problems similar characteristics | complex temporal cost-benefit tradeoffs,stochasticity, partial observability underlying controlled process | include robotnavigation, target tracking, machine mantainance replacement, like.Sequential decision problems modeled Markov decision processes (MDPs)(Bellman, 1957; Howard, 1960; Puterman, 1994; Boutilier, Dean, & Hanks, 1999)extensions. model choice problems similar patient management partiallyobservable Markov decision process (POMDP) (Drake, 1962; Astrom, 1965; Sondik, 1971;Lovejoy, 1991b). POMDP represents two sources uncertainty: stochasticityunderlying controlled process (e.g. disease dynamics patient management problem),imperfect observability states via set noisy observations (e.g. symptoms,findings, results tests). addition, lets us model uniform way controlinformation-gathering (investigative) actions, well effects cost-benefit tradeoffs. Partial observability ability model reason information-gatheringactions main features distinguish POMDP widely known fullyobservable Markov decision process (Bellman, 1957; Howard, 1960).Although useful modeling perspective, POMDPs disadvantage hard solve (Papadimitriou & Tsitsiklis, 1987; Littman, 1996; Mundhenk, Goldsmith,Lusena, & Allender, 1997; Madani, Hanks, & Condon, 1999), optimal -optimal solutions obtained practice problems low complexity. challenging goalresearch area exploit additional structural properties domain and/or suitableapproximations (heuristics) used obtain good solutions eciently.focus heuristic approximation methods, particular approximations basedvalue functions. Important research issues area design new ecientalgorithms, well better understanding existing techniques relations,advantages disadvantages. paper address issues. First,survey various value-function approximations, analyze properties relationsprovide insights differences. Second, present number new methodsnovel refinements existing techniques. theoretical results findings alsosupported empirically problem agent navigation domain.2. Partially Observable Markov Decision Processespartially observable Markov decision process (POMDP) describes stochastic controlprocess partially observable (hidden) states. Formally, corresponds tuple(S; A; ; T; O; R) set states, set actions, set observations,: ! [0; 1] set transition probabilities describe dynamic behaviormodeled environment, : ! [0; 1] set observation probabilitiesdescribe relationships among observations, states actions, R : ! IRdenotes reward model assigns rewards state transitions models payoffs associated transitions. instances definition POMDP also includespriori probability distribution set initial states .34fiValue-Function Approximations POMDPso0t2ott1t+1sta0t2t+1t1rFigure 1: Part uence diagram describing POMDP model. Rectangles corresponddecision nodes (actions), circles random variables (states) diamondsreward nodes. Links represent dependencies among components. st ; ; otrt denote state, action, observation reward time t. Note actiontime depends past observations actions, states.2.1 Objective FunctionGiven POMDP, goal construct control policy maximizes objective (value)function. objective function combines partial (stepwise) rewards multiple stepsusing various kinds decision models. Typically, models cumulative basedexpectations. Two models frequently used practice:finite-horizon model maximize E (PTt=0 rt ), rt reward obtainedtime t.infinite-horizon discounted model maximize E (P1t=0 rt ), 0 << 1 discount factor.Note POMDPs cumulative decision models provide rich language modelingvarious control objectives. example, one easily model goal-achievement tasks (aspecific goal must reached) giving large reward transition statezero smaller rewards transitions.paper focus primarily discounted infinite-horizon model. However,results easily applied also finite-horizon case.2.2 Information StatePOMDP process states hidden cannot observe makingdecision next action. Thus, action choices based information available us quantities derived information. illustrateduence diagram Figure 1, action time depends previousobservations actions, states. Quantities summarizing information calledinformation states. Complete information states represent trivial case.35fiHauskrechtt+1stt+1t+1t+1rtrtFigure 2: uence diagram POMDP information states correspondinginformation-state MDP. Information states (It It+1 ) representeddouble-circled nodes. action choice (rectangle) depends currentinformation state.Definition 1 (Complete information state). complete information state time (denoted ItC ) consists of:prior belief b0 states time 0;complete history actions observations fo0 ; a0 ; o1 ; a1 ; ; ot 1 ; 1 ; ot g starting time = 0.sequence information states defines controlled Markov process callinformation-state Markov decision process information-state MDP. policyinformation-state MDP defined terms control function : ! mappinginformation state space actions. new information state (It ) deterministic functionprevious state (It 1 ), last action (at 1 ) new observation (ot ):= (It 1 ; ot ; 1 ):: ! update function mapping information state space, observationsactions back information space.1 easy see one always convertoriginal POMDP information-state MDP using complete information states.relation components two models sketch reductionPOMDP information-state MDP, shown Figure 2.2.3 Bellman Equations POMDPsinformation-state MDP infinite-horizon discounted case like fully-observableMDP satisfies standard fixed-point (Bellman) equation:()XV (I ) = max (I; a) + P (I 0 jI; a)V (I 0 ) :a2AI0(1)1. paper, denotes generic update function. Thus use symbol even informationstate space different.36fiValue-Function Approximations POMDPsPHere, V (I ) denotes optimal value function maximizing E ( 1t=0 rt ) state . (I; a)expected one-step reward equalsXXX(I; a) = (s; a)P (sjI ) =R(s; a; s0 )P (s0 js; a)P (sjI ):s2Ss2 s0 2(s; a) denotes expected one-step reward state action a.Since next information state 0 = (I; o; a) deterministic function previousinformation state , action a, observation o, Equation 1 rewrittencompactly summing possible observations :V (I ) = maxa2A(Xs2S(s; a)P (sjI ) +Xo2)P (ojI; a)V ( (I; o; a)) :(2)optimal policy (control function) : ! selects value-maximizing action()XX(I ) = arg max(s; a)P (sjI ) + P (ojI; a)V ( (I; o; a)) :a2A s2So2(3)value control functions also expressed terms action-value functions(Q-functions)V (I ) = max Q (I; a)(I ) = arg max Q (I; a);a2Aa2AXXQ (I; a) = (s; a)P (sjI ) + P (ojI; a)V ( (I; o; a)):(4)s2So2Q-function corresponds expected reward chosing fixed action (a) firststep acting optimally afterwards.2.3.1 Sufficient Statisticsderive Equations 1|3 implicitly used complete information states. However,remarked earlier, information available decision-maker also summarizedquantities. call sucient information states. states must preservenecessary information content also Markov property information-statedecision process.Definition 2 (Sucient information state process). Let information state space: ! update function defining information process =(It 1 ; 1 ; ot ). process sucient regard optimal control when,time step t, satisfiesP (st jIt ) = P (st jItC )P (ot jIt 1 ; 1 ) = P (ot jItC 1 ; 1 );ItC ItC 1 complete information states.easy see Equations 1 | 3 complete information states must hold alsosucient information states. key benefit sucient statistics often37fiHauskrechteasier manipulate store, since unlike complete histories, may expandtime. example, standard POMDP model sucient work belief statesassign probabilities every possible process state (Astrom, 1965).2 caseBellman equation reduces to:(V (b) = maxa2AXs2S(s; a)b(s) +XXo2 s2S)P (ojs; a)b(s)V ( (b; o; a)) ;(5)next-step belief state b0Xb0 (s) = (b; o; a)(s) = fiP (ojs; a)P (sja; s0 )b(s0 ):02Sfi = 1=P (ojb; a) normalizing constant. defines belief-state MDPspecial case continuous-state MDP. Belief-state MDPs also primary focusinvestigation paper.2.3.2 Value-Function Mappings PropertiesBellman equation 2 belief-state MDP also rewritten value-functionmapping form. Let V space real-valued bounded functions V : ! IR definedbelief information space , let h : B ! IR definedh(b; a; V ) =Xs2S(s; a)b(s) +XXo2 s2SP (ojs; a)b(s)V ( (b; o; a)):defining value function mapping H : V ! V (HV )(b) = maxa2A h(b; a; V ),Bellman equation 2 information states written V = HV : wellknown H (for MDPs) isotone mapping contractionsupremum norm (see (Heyman & Sobel, 1984; Puterman, 1994)).Definition 3 mapping H isotone, V; U2 V V U implies HV HU .Definition 4 Let k:k supremum norm. mapping H contractionsupremum norm, V; U 2 V , kHV HU k fi kV U k holds 0 fi < 1.2.4 Value Iterationoptimal value function (Equation 2) approximation computed using dynamic programming techniques. simplest approach value iteration (Bellman,1957) shown Figure 3. case, optimal value function V determinedlimit performing sequence value-iteration steps Vi = HVi 1 , Viith approximation value function (ith value function).3 sequence estimates2. Models belief states sucient include POMDPs observation action channellags (see Hauskrecht (1997)).3. note update V = HV 1 applied solve finite-horizon problemstandard way. difference V stands i-steps-to-go value function V0 representsvalue function (rewards) end states.38fiValue-Function Approximations POMDPsValue iteration (P OMDP , )initialize V b 2 ;repeatV0 V;update V HV 0 b 2 ;supb j V (b) V 0 (b) jreturn V;Figure 3: Value iteration procedure.converges unique fixed-point solution direct consequence Banach'stheorem contraction mappings (see, example, Puterman (1994)).practice, stop iteration well reaches limit solution. stoppingcriterion use algorithm (Figure 3) examines maximum difference valuefunctions obtained two consecutive steps | so-called Bellman error (Puterman, 1994;Littman, 1996). algorithm stops quantity falls threshold .accuracy approximate solution (ith value function) regard V expressedterms Bellman error .Theorem 1 Let = supb jVi (b) Vi 1 (b)j = kVi Vi 1 k magnitude Bellmanerror. kVi V k 1 kVi 1 V k 1 hold.Then, obtain approximation V precision Bellman error fall(1 ) .2.4.1 Piecewise Linear Convex Approximations Value Functionmajor diculty applying value iteration (or dynamic programming) beliefstate MDPs belief space infinite need compute update Vi = HVi 1it. poses following threats: value function ith step mayrepresentable finite means and/or computable finite number steps.address problem Sondik (Sondik, 1971; Smallwood & Sondik, 1973) showedone guarantee computability ith value function well finite descriptionbelief-state MDP considering piecewise linear convex representationsvalue function estimates (see Figure 4). particular, Sondik showed piecewiselinear convex representation Vi 1 , Vi = HVi 1 computable remains piecewiselinear convex.Theorem 2 (Piecewise linear convex functions). Let V0 initial value functionpiecewise linear convex. ith value function obtained finitenumber update steps belief-state MDP also finite, piecewise linear convex,equal to:XVi (b) = max b(s)ffi (s);ffi 2 s2Sb ffi vectors size jS j finite set vectors (linear functions) ffi .39fiHauskrechtVi (b)01b(s1 )Figure 4: piecewise linear convex function POMDP two process statesfs1 ; s2g. Note b(s1) = 1 b(s2 ) holds belief state.key part proof express update ith value functionterms linear functions 1 defining Vi 1 :8<XVi (b) = max :a2As2S(s; a)b(s) +Xmaxo2 ffi 1 2"X X1 s0 2S s2S#9=P (s0 ; ojs; a)b(s) ffi 1 (s0 ); :(6)leads piecewise linear convex value function Vi representedfinite set linear functions ffi , one linear function every combination actionsjpermutations ffi 1 vectors size jj. Let W = (a; fo1 ; ffji 1 1 g; fo2 ; ffji 2 1 g; fojj ; ffi j1j g)combination. linear function corresponding definedXXffWP (s0 ; ojs; a)ffji 1 (s0 ):(7)(s) = (s; a) +o2 s0 2STheorem 2 basis dynamic programming algorithm finding optimalsolution finite-horizon models value-iteration algorithm finding nearoptimal approximations V discounted, infinite-horizon model. Note, however,result imply piecewise linearity optimal (fixed-point) solution V .2.4.2 Algorithms Computing Value-Function Updateskey part value-iteration algorithm computation value-function updatesVi = HVi 1 . Assume ith value function Vi represented finite number linearsegments (ff vectors). total number possible linear functions jAjj 1 jjj (oneevery combination actions permutations ffi 1 vectors size jj)enumerated O(jAjjS j2 j 1 jjj ) time. However, complete set linear functionsrarely needed: linear functions dominated others omissionchange resulting piecewise linear convex function. illustratedFigure 5.40fiValue-Function Approximations POMDPsVi (b)redundant linearfunction01b(s1 )Figure 5: Redundant linear function. function dominate regionsbelief space excluded.linear function eliminated without changing resulting value functionsolution called redundant. Conversely, linear function singlehandedly achievesoptimal value least one point belief space called useful.4sake computational eciency important make size linearfunction set small possible (keep useful linear functions) value-iteration steps.two main approaches computing useful linear functions. first approachbased generate-and-test paradigm due Sondik (1971) Monahan (1982).idea enumerate possible linear functions first, test usefulnesslinear functions set prune redundant vectors. Recent extensionsmethod interleave generate test stages early pruning set partiallyconstructed linear functions (Zhang & Liu, 1997a; Cassandra, Littman, & Zhang, 1997;Zhang & Lee, 1998).second approach builds Sondik's idea computing useful linear functionsingle belief state (Sondik, 1971; Smallwood & Sondik, 1973), done eciently.key problem locate belief points seed useful linear functionsdifferent methods address problem differently. Methods implement ideaSondik's one- two-pass algorithms (Sondik, 1971), Cheng's methods (Cheng, 1988),Witness algorithm (Kaelbling, Littman, & Cassandra, 1999; Littman, 1996; Cassandra,1998).2.4.3 Limitations Complexitymajor diculty solving belief-state MDP complexity piecewiselinear convex function grow extremely fast number update steps.specifically, size linear function set defining function grow exponentially (innumber observations) single update step. Then, assuming initialvalue functionlinear, number linear functions defining ith value functionO(jAjjj 1 ).4. defining redundant useful linear functions assume linear function duplicates,i.e. one copy linear function kept set .41fiHauskrechtpotential growth size linear function set bad news.remarked earlier, piecewise linear convex value function usually less complexworst case many linear functions pruned away updates. However,turned task identifying useful linear functions computationallyintractable well (Littman, 1996). means one faces potentialsuper-exponential growth number useful linear functions, also inecienciesrelated identification vectors. significant drawback makesexact methods applicable relatively simple problems.analysis suggests solving POMDP problem intrinsically hardtask. Indeed, finding optimal solution finite-horizon problem PSPACE-hard(Papadimitriou & Tsitsiklis, 1987). Finding optimal solution discounted infinitehorizon criterion even harder. corresponding decision problem shownundecidable (Madani et al., 1999), thus optimal solution may computable.2.4.4 Structural Refinements Basic Algorithmstandard POMDP model uses state space full transition reward matrices.However, practice, problems often exhibit structure representedcompactly, example, using graphical models (Pearl, 1988; Lauritzen, 1996), oftendynamic belief networks (Dean & Kanazawa, 1989; Kjaerulff, 1992) dynamic uencediagrams (Howard & Matheson, 1984; Tatman & Schachter, 1990).5 many waystake advantage problem structure modify improve exact algorithms.example, refinement basic Monahan algorithm compact transition rewardmodels studied Boutilier Poole (1996). hybrid framework combinesMDP-POMDP problem-solving techniques take advantage perfectly partially observable components model subsequent value function decompositionproposed Hauskrecht (1997, 1998, 2000). similar approach perfect informationregion (subset states) containing actual underlying state discussedZhang Liu (1997b, 1997a). Finally, Casta~non (1997) Yost (1998) explore techniquessolving large POMDPs consist set smaller, resource-coupled otherwiseindependent POMDPs.2.5 Extracting Control StrategyValue iteration allow us compute ith approximation value function Vi . However,ulimate goal find optimal control strategy : ! close approximation.Thus focus problem extraction control strategies resultsvalue iteration.2.5.1 Lookahead Designsimplest way define control function : ! value function Vi viagreedy one-step lookahead:((b) = arg maxa2AXs2S(s; a)b(s) +Xo2)P (ojb; a)Vi ( (b; o; a)) :5. See survey Boutilier, Dean Hanks (1999) different ways represent structured MDPs.42fiValue-Function Approximations POMDPsVi (b)a1a3a2a10b1b(s1 )Figure 6: Direct control design. Every linear function defining Vi associatedaction. action selected linear function (or Q-function) maximal.Vi represents ith approximation optimal value function, questionarises good resulting controller really is.6 following theorem (Puterman, 1994;Williams & Baird, 1994; Littman, 1996) relates accuracy (lookahead) controllerBellman error.Theorem 3 Let = kVi Vi 1 k magnitude Bellman error. Let ViLAexpected reward lookahead controller designed Vi . kViLA V k 12 .bound used construct value-iteration routine yields lookaheadstrategy minimum required precision. result also extended kstep lookahead design straightforward way; k steps, error bound becomeskViLA(k) V k (12 ) .k2.5.2 Direct Designextract control action via lookahead essentially requires computing one full update.Obviously, lead unwanted delays reaction times. general, speedresponse remembering using additional information. particular, every linearfunction defining Vi associated choice action (see Equation 7). actionbyproduct methods computing linear functions extra computation requiredfind it. action corresponding best linear function selected directlybelief state. idea illustrated Figure 6.bound accuracy direct controller infinite-horizon casederived terms magnitude Bellman error.Theorem 4 Let = kVi Vi 1 k magnitude Bellman error. Let ViDRexpected reward direct controller designed Vi . kViDR V k 12 .direct action choice closely related notion action-value function (orQ-function). Analogously Equation 4, ith Q-function satisfiesVi (b) = max Qi (b; a);a2A6. Note control action extracted via lookahead V optimal (i + 1) steps-to-gofinite-horizon model. main difference V optimal value function steps go.43fiHauskrechta1o2o1a2o1 ,2o2o1 ,a2o1 , o22o2a2a1o1o1a2o1 , o2a1Figure 7: policy graph (finite-state machine) obtained two value iteration steps.Nodes correspond linear functions (or states finite-state machine)links dependencies linear functions (transitions states). Everylinear function (node) associated action. ensure policyalso applied infinite-horizon problem, add cycle last state(dashed line).Qi (b; a) = R(b; a) +Xo2P (ojb; a)Vi 1 ( (b; a; o)):perspective, direct strategy selects action best (maximum) Qfunction given belief state.72.5.3 Finite-State Machine Designcomplex refinement technique remember, every linear functionVi , action choice also choice linear function previousstep observations (see Equation 7). idea appliedrecursively linear functions previous steps, obtain relatively complexdependency structure relating linear functions Vi ; Vi 1 ; V0 , observations actionsrepresents control strategy (Kaelbling et al., 1999).see this, model structure graphical terms (Figure 7). different nodesrepresent linear functions, actions associated nodes correspond optimizing actions,links emanating nodes correspond different observations, successor nodes correspond linear functions paired observations. graphs also called policy graphs(Kaelbling et al., 1999; Littman, 1996; Cassandra, 1998). One interpretation dependency structure represents collection finite-state machines (FSMs) manypossible initial states implement POMDP controller: nodes correspond statescontroller, actions controls (outputs), links transitions conditioned inputs7. Williams Baird (1994) also give results relating accuracy direct Q-function controllerBellman error Q-functions.44fiValue-Function Approximations POMDPs(observations). start state FSM controller chosen greedily selectinglinear function (controller state) optimizing value initial belief state.advantage finite-state machine representation strategyfirst steps works observations directly; belief-state updates needed.contrasts two policy models (lookahead direct models), must keeptrack current belief state update time order extract appropriatecontrol. drawback approach FSM controller limited stepscorrespond number value iteration steps performed. However, infinitehorizon model controller expected run infinite number steps. One wayremedy deficiency extend FSM structure create cycles let usvisit controller states repeatedly. example, adding cycle transition end stateFSM controller Figure 7 (dashed line) ensures controller also applicableinfinite-horizon problem.2.6 Policy Iterationalternative method finding solution discounted infinite-horizon problempolicy iteration (Howard, 1960; Sondik, 1978). Policy iteration searches policy spacegradually improves current control policy one belief states. methodconsists two steps performed iteratively:policy evaluation: computes expected value current policy;policy improvement: improves current policy.saw Section 2.5, many ways represent control policyPOMDP. restrict attention finite-state machine model observationscorrespond inputs actions outputs (Platzman, 1980; Hansen, 1998b; Kaelblinget al., 1999).82.6.1 Finite-State Machine Controllerfinite-state machine (FSM) controller C = (M; ; A; ; ; ) POMDP describedset memory states controller, set observations (inputs) , setactions (outputs) A, transition function : ! mapping states FSMnext memory states given observation, output function : ! mappingmemory states actions. function : I0 ! selects initial memory state giveninitial information state. initial information state corresponds either priorposterior belief state time t0 depending availability initial observation.2.6.2 Policy Evaluationfirst step policy iteration policy evaluation. important propertyFSM model value function specific FSM strategy computedeciently number controller states . key ecient computability8. policy-iteration algorithm policies defined regions belief space describedfirst Sondik (1978).45fiHauskrechtx2o1o2a2a1x1o2o1a1o2o1x32a2o1x4Figure 8: example four-state FSM policy. Nodes represent states, links transitions states (conditioned observations). Every memory stateassociated control action (output).fact value function executing FSM strategy memory state xlinear (Platzman, 1980).9Theorem 5 Let C finite-state machine controller set memory states .value function applying C memory state x 2 , V C (x; b), linear. Valuefunctions x 2 found solving system linear equations jS jjM jvariables.illustrate main idea example. Assume FSM controller four memorystates fx1 ; x2 ; x3 ; x4 g, Figure 8, stochastic process two hidden states =fs1 ; s2g. value policy augmented state space satisfies systemlinear equationsV (x1 ; s1 ) = (s1 ; (x1 )) +V (x1 ; s2 ) = (s2 ; (x1 )) +V (x2 ; s1 ) = (s1 ; (x2 )) +V (x4 ; s2 ) = (s2 ; (x4 )) +XXo2 s2SXXo2 s2SXXo2 s2SXXo2 s2SP (o; sjs1 ; (x1 ))V ((x1 ; o); s)P (o; sjs2 ; (x1 ))V ((x1 ; o); s)P (o; sjs1 ; (x2 ))V ((x2 ; o); s)P (o; sjs2 ; (x4 ))V ((x4 ; o); s);(x) action executed x (x; o) state one transitsseeing input (observation) o. Assuming start policy memory state x1 ,value policy is:XV C (x1 ; b) = V (x1 ; s)b(s):s2S9. idea linearity ecient computability value functions fixed FSM-based strategyaddressed recently different contexts number researchers (Littman, 1996; Cassandra,1998; Hauskrecht, 1997; Hansen, 1998b; Kaelbling et al., 1999). However, origins ideatraced earlier work Platzman (1980).46fiValue-Function Approximations POMDPsThus value function linear computed eciently solving systemlinear equations.Since general FSM controller start memory state, alwayschoose initial memory state greedily, maximizing expected value result.case optimal choice function defined as:(b) = arg max V C (x; b);x2Mvalue FSM policy C belief state b is:V C (b) = max V C (x; b) = V C ( (b); b):x2MNote resulting value function strategy C piecewise linear convexrepresents expected rewards following C . Since strategy perform betteroptimal strategy, V C V must hold.2.6.3 Policy Improvementpolicy-iteration method, searching space controllers, starts arbitrary initial policy improves gradually refining finite-state machine (FSM) description.particular, one keeps modifying structure controller adding removing controller states (memory) transitions. Let C C 0 old new FSM controller.improvement step must satisfy0V C (b) V C (b) b 2 ;9b 2 V C 0 (b) > V C (b):guarantee improvement, Hansen (1998a, 1998b) proposed policy-iteration algorithm relies exact value function updates obtain new improved policy structure.10 basic idea improvement based observation one switchback forth FSM policy description piecewise-linear convexrepresentation value function. particular:value function FSM policy piecewise-linear convex every linearfunction describing corresponds memory state controller;individual linear functions comprising new value function updateviewed new memory states FSM policy, described Section 2.5.3.allows us improve policy adding new memory states corresponding linearfunctions new value function obtained exact update. techniquerefined removing linear functions (memory states) whenever fullydominated one linear functions.10. policy-iteration algorithm exploits exact value function updates works policies definedbelief space used earlier Sondik (1978).47fiHauskrechtb 1,1o1o2a1ba2b 1,2Figure 9: two-step decision tree. Rectangles correspond decision nodes (movesdecision-maker) circles chance nodes (moves environment).Black rectangles represent leaves tree. reward specific pathassociated every leaf tree. Decision nodes associatedinformation states obtained following action observation choices alongpath root tree. example, b1;1 belief state obtainedperforming action a1 initial belief state b observing observation o1 .2.7 Forward (Decision Tree) Methodsmethods discussed far assume prior knowledge initial belief state treatbelief states equally likely. However, initial state known fixed, methodsoften modified take advantage fact. example, finite-horizonproblem, finite number belief states reached given initial state.case often easier enumerate possible histories (sequences actionsobservations) represent problem using stochastic decision trees (Raiffa, 1970).example two-step decision tree shown Figure 9.algorithm solving stochastic decision tree basically mimics value-functionupdates, restricted situations reached initial belief state.key diculty number possible trajectories grows exponentiallyhorizon interest.2.7.1 Combining Dynamic-Programming Decision-Tree Techniquessolve POMDP fixed initial belief state, apply two strategies: one constructs decision tree first solves it, solves problem backwardfashion via dynamic programming. Unfortunately, techniques inecient, onesuffering exponential growth decision tree size, super-exponentialgrowth value function complexity. However, two techniques combined48fiValue-Function Approximations POMDPsway least partially eliminates disadvantages. idea based facttwo techniques work solution two different sides (one forwardbackward) complexity worsens gradually. solutioncompute complete kth value function using dynamic programming (value iteration)cover remaining steps forward decision-tree expansion.Various modifications idea possible. example, one often replaceexact dynamic programming two ecient approximations providing upperlower bounds value function. decision tree must expandedbounds sucient determine optimal action choice. number searchtechniques developed AI literature (Korf, 1985) combined branch-and-boundpruning (Satia & Lave, 1973) applied type problem. Several researchersexperimented solve POMDPs (Washington, 1996; Hauskrecht, 1997;Hansen, 1998b). methods applicable problem based Monte-Carlosampling (Kearns, Mansour, & Ng, 1999; McAllester & Singh, 1999) real-time dynamicprogramming (Barto, Bradtke, & Singh, 1995; Dearden & Boutilier, 1997; Bonet & Geffner,1998).2.7.2 Classical Planning FrameworkPOMDP problems fixed initial belief states solutions closely relatedwork classical planning extensions handle stochastic partially observabledomains, particularly work BURIDAN C-BURIDAN planners (Kushmerick,Hanks, & Weld, 1995; Draper, Hanks, & Weld, 1994). objective plannersmaximize probability reaching goal state. However, task similardiscounted reward task terms complexity, since discounted reward modelconverted goal-achievement model introducing absorbing state (Condon,1992).3. Heuristic Approximationskey obstacle wider application POMDP framework computationalcomplexity POMDP problems. particular, finding optimal solution finitehorizon case PSPACE-hard (Papadimitriou & Tsitsiklis, 1987) discounted infinitehorizon case may even computable (Madani et al., 1999). One approachproblems approximate solution -precision. Unfortunately, evenremains intractable general POMDPs cannot approximated eciently (Burago,Rougemont, & Slissenko, 1996; Lusena, Goldsmith, & Mundhenk, 1998; Madani et al.,1999). also reason simple problems solved optimallynear-optimally practice.alleviate complexity problem, research POMDP area focused variousheuristic methods (or approximations without error parameter) ecient.11Heuristic methods also focus here. Thus, referring approximations, meanheuristics, unless specifically stated otherwise.11. quality heuristic approximation tested using Bellman error, requires one exactupdate step. However, heuristic methods per se contain precision parameter.49fiHauskrechtmany approximation methods combinations divided two oftenclosely related classes: value-function approximations policy approximations.3.1 Value-Function Approximationsmain idea value-function approximation approach approximate optimalvalue function V : ! IR function Vb : ! IR defined informationspace. Typically, new function lower complexity (recall optimal nearoptimal value function may consist large set linear functions) easier computeexact solution. Approximations often formulated dynamic programmingproblems expressed terms approximate value-function updates Hb . Thus,understand differences advantages various approximations exact methods,often sucient analyze compare update rules.3.1.1 Value-Function BoundsAlthough heuristic approximations guaranteed precision, many casesable say whether overestimate underestimate optimal value function.information bounds used multiple ways. example, upper- lowerbounds help narrowing range optimal value function, eliminationsuboptimal actions subsequent speed-ups exact methods. Alternatively, oneuse knowledge value-function bounds determine accuracy controllergenerated based one bounds (see Section 3.1.3). Also, instances, lowerbound alone sucient guarantee control choice always achieves expectedreward least high one given bound (Section 4.7.2).bound property different methods determined examining updatesbound relations.Definition 5 (Upper bound). Let H exact value-function mapping Hb apb )(b) (HV )(b) holdsproximation. say Hb upper-bounds H V (HVevery b 2 .analogous definition constructed lower bound.3.1.2 Convergence Approximate Value IterationLet Hb value-function mapping representing approximate update. approximate value iteration computes ith value function Vbi = Hb Vbi 1 . fixed-pointsolution Vc = Hb Vb close approximation would represent intended outputapproximation routine. main problem iteration method generalconverge unique multiple solutions, diverge, oscillate, depending Hbinitial function Vb0 . Therefore, unique convergence cannot guaranteed arbitrarymapping Hb convergence specific approximation method must proved.Definition 6 (Convergence Hb ). value iteration Hb converges value function V0 limn!1(Hb n V0 ) exists.50fiValue-Function Approximations POMDPsDefinition 7 (Unique convergence Hb ). value iteration converges uniquely Vevery V 2 V , limn!1(Hb n V ) exists pairs V; U 2 V , limn!1(Hb n V ) =limn!1(Hb n U ).sucient condition unique convergence show Hb contraction.contraction bound properties Hb combined, additional conditions,show convergence iterative approximation method bound. addressissue present theorem comparing fixed-point solutions two value-function mappings.Theorem 6 Let H1 H2 two value-function mappings defined V1 V21. H1 , H2 contractions fixed points V1 , V2 ;2. V1 2 V2 H2 V1 H1 V1 = V1 ;3. H2 isotone mapping.V2 V1 holds.Note theorem require V1 V2 cover space valuefunctions. example, V2 cover possible value functions belief-state MDP,V1 restricted space piecewise linear convex value functions.gives us exibility design iterative approximation algorithms computingvalue-function bounds. analogous theorem also holds lower bound.3.1.3 Controlapproximation value-function available, used generatecontrol strategy. general, control solutions correspond options presented Section2.5 include lookahead, direct (Q-function) finite-state machine designs.drawback control strategies based heuristic approximationsprecision guarantee. One way find accuracy strategies one exactupdate value function approximation adopt result Theorems 1 3Bellman error. alternative solution problem bound accuracycontrollers using upper- lower-bound approximations optimal valuefunction. illustrate approach, present prove (in Appendix) followingtheorem relates quality bounds quality lookahead controller.Theorem 7 Let VbU VbL upper lower bounds optimal value functiondiscounted infinite-horizon problem. Let = supb jVbU (b) VbL (b)j = kVbU VbL kmaximum bound difference. expected reward lookahead controller Vb LA ,constructed either VbU VbL , satisfies kVb LA V k (1(2 )) .3.2 Policy Approximationalternative value-function approximation policy approximation. shown earlier,strategy (controller) POMDP represented using finite-state machine (FSM)model. policy iteration searches space possible policies (FSMs) optimal near-optimal solution. space usually enormous, bottleneck51fiHauskrechtmethod. Thus, instead searching complete policy space, restrict attentionsubspace believe contain optimal solution good approximation. Memoryless policies (Platzman, 1977; White & Scherer, 1994; Littman, 1994; Singh,Jaakkola, & Jordan, 1994), policies based truncated histories (Platzman, 1977; White &Scherer, 1994; McCallum, 1995), finite-state controllers fixed number memorystates (Platzman, 1980; Hauskrecht, 1997; Hansen, 1998a, 1998b) examplespolicy-space restriction. following consider finite-state machine model(see Section 2.6.1), quite general; models viewed special cases.States FSM policy model represent memory controller and, general,summarize information past activities observations. Thus, best viewedapproximations information states, feature states. transition modelcontroller () approximates update function information-state MDP( ) output function FSM () approximates control function () mappinginformation states actions. important property model, shown Section2.6.2, value function fixed controller fixed initial memory stateobtained eciently solving system linear equations (Platzman, 1980).apply policy approximation approach first need decide (1) restrictspace policies (2) judge policy quality.restriction frequently used consider controllers fixed numberstates, say k. structural restrictions narrowing space policiesrestrict either output function (choice actions different controller states),transitions current next states. general, heuristic domain-relatedinsight may help selecting right biases.Two different policies yield value functions better different regionsbelief space. Thus, order decide policy best, need defineimportance different regions combinations. multiple solutions this.example, Platzman (1980) considers worst-case measure optimizes worst(minimal) value initial belief states. Let C space FSM controllers satisfyinggiven restrictions. quality policy worst case measure is:max min max V C (x; b):C 2C b2I x2MCAnother option consider distribution initial belief states maximizeexpectation value function values. However, common objective choosepolicy leads best value single initial belief state b0 :max max V C (x; b0 ):C 2C x2MCFinding optimal policy case reduces combinatorial optimization problem.Unfortunately, trivial cases, even problem computationally intractable.example, problem finding optimal policy memoryless case (only current observations considered) NP-hard (Littman, 1994). Thus, various heuristicstypically applied alleviate diculty (Littman, 1994).52fiValue-Function Approximations POMDPsValuefunctionapproximationsGridbased linearfunction methodsSection 4.7Fully observable MDPapproximationsSection 4.1Fast informed boundapproximationsSection 4.2Fixed strategyapproximationsSection 4.4CurvefittingapproximationsSection 4.6Gridbased value interpolationextrapolation methodsSection 4.5Unobservable MDPapproximationsSection 4.3Figure 10: Value-function approximation methods.3.2.1 Randomized Policiesrestricting space policies simplify policy optimization problem.hand, simultaneously give opportunity find best optimal policy, replacing best restricted policy. point, considered deterministicpolicies fixed number internal controller states, is, policies deterministicoutput transition functions. However, finding best deterministic policy always best option: randomized policies, randomized output transition functions,usually lead far better performance. application randomized (or stochastic)policies POMDPs introduced Platzman (1980). Essentially, deterministicpolicy represented randomized policy single action transition,best randomized policy worse best deterministic policy. difference control performance two policies shows often cases numberstates controller relatively small compared optimal strategy.advantage stochastic policies space larger parameterspolicy continuous. Therefore problem finding optimal stochastic policybecomes non-linear optimization problem variety optimization methodsapplied solve it. example gradient-based approach (see Meuleau et al., 1999).4. Value-Function Approximation Methodssection discuss depth value-function approximation methods. focus approximations belief information space.12 survey known techniques,also include number new methods modifications existing methods. Figure 10summarizes methods covered. describe methods means update rules12. Alternative value-function approximations may work complete histories past actions observations. Approximation methods used White Scherer (1994) example.53fiHauskrecht151617181910111213145678901234MovesSensorsFigure 11: Test example. maze navigation problem: Maze20.implement, simplifies analysis theoretical comparison. focus following properties: complexity dynamic-programming (value-iteration) updates;complexity value functions method uses; ability methods boundexact update; convergence value iteration approximate update rules;control performance related controllers. results theoretical analysis illustrated empirically problem agent-navigation domain. addition, useagent navigation problem illustrate give intuitions characteristicsmethods theoretical underpinning. Thus, results generalizedproblems used rank different methods.Agent-Navigation ProblemMaze20 maze-navigation problem 20 states, six actions eight observations.maze (Figure 11) consists 20 partially connected rooms (states) robotoperates collects rewards. robot move four directions (north, south, eastwest) check presence walls using sensors. But, neither \move"actions sensor inputs perfect, robot end moving unintendeddirections. robot moves unintended direction probability 0.3 (0.15neighboring directions). move wall keeps robotposition. Investigative actions help robot navigate activating sensor inputs. Twoinvestigative actions allow robot check inputs (presence wall) northsouth east-west directions. Sensor accuracy detecting walls 0.75 two-wallcase (e.g. north south wall), 0.8 one-wall case (north south) 0.89no-wall case, smaller probabilities wrong perceptions.control objective maximize expected discounted rewards discountfactor 0.9. small reward given every action leading bumping wall(4 points move 2 points investigative action), one large reward (150points) given achieving special target room (indicated circle figure)recognizing performing one move actions. collectingreward, robot placed random new start position.Although Maze20 problem moderate complexity regard sizestate, action observation spaces, exact solution beyond reach currentexact methods. exact methods tried problem include Witness algorithm(Kaelbling et al., 1999), incremental pruning algorithm (Cassandra et al., 1997)1313. Many thanks Anthony Cassandra running algorithms.54fiValue-Function Approximations POMDPsVMDP*VMDP(s 1 )*VMDP(s 2 )VQMDPQ *MDP(s 2 ,a 1 )Q *MDP(s 1 ,a 1)Q *MDP(s 2 ,a 2)Q *MDP(s 1 ,a 2)*VPOMDP0*VPOMDP1b(s1 )0(a)1b(s1 )(b)Figure 12: Approximations based fully observable version two state POMDP(with states s1 ; s2 ): (a) MDP approximation; (b) QMDP approximation.Values extreme points belief space solutions fully observableMDP.policy iteration FSM model (Hansen, 1998b). main obstacle preventingalgorithms obtaining optimal close-to-optimal solution complexityvalue function (the number linear functions needed describe it) subsequentrunning times memory problems.4.1 Approximations Fully Observable MDPPerhaps simplest way approximate value function POMDP assumestates process fully observable (Astrom, 1965; Lovejoy, 1993). caseoptimal value function V POMDP approximated as:Vb (b) =Xs2S(s);b(s)VMDP(8)(s) optimal value function state fully observable versionVMDPprocess. refer approximation MDP approximation. ideaapproximation illustrated Figure 12a. resulting value function linearfully defined values extreme points belief simplex. correspondoptimal values fully observable case. main advantage approximationfully observable MDP (FOMDP) solved eciently finitehorizon problem discounted infinite-horizon problems.14 update step (fullyobservable) MDP is:8<ViMDP(s; a) ++1 (s) = max:Xs0 2S9=P (s0 js; a)ViMDP (s0 ); :14. solution finite-state fully observable MDP discounted infinite-horizon criterionfound eciently formulating equivalent linear programming task (Bertsekas, 1995)55fiHauskrecht4.1.1 MDP ApproximationMDP-approximation approach (Equation 8) also described terms valuefunction updates belief-space MDP. Although step strictly speaking redundanthere, simplifies analysis comparison approach approximations.Let Vbi linear value function described vector ffMDPcorresponding valuesViMDP (s0 ) states s0 2 . (i + 1)th value function Vbi+1Vbi+1 (b) =Xs2S2b(s) max 4(s; a) +a2A= (HMDP Vbi )(b):Xs0 2S3P (s0 js; a)ffMDP(s0 )5Vbi+1 described linear function componentsMDPffMDPi+1 (s) = Vi+1 (s) = max((s; a) +Xs2S)P (s0 js; a)ffMDP(s0 ) :MDP-based rule HMDP also rewritten general form startsarbitrary piecewise linear convex value function Vi , represented set linearfunctions :Vbi+1 (b) =Xs2S8<b(s) max :(s; a) +a2AXs0 2S9=P (s0 js; a) max ffi (s0 ); :ffi 2application HMDP mapping always leads linear value function.update easy compute takes O(jAjjS j2 + j jjS j) time. reduces O(jAjjS j2 )time MDP-based updates strung together. remarked earlier, optimalsolution infinite-horizon, discounted problem solved eciently via linearprogramming.update MDP approximation upper-bounds exact update, is, H VbiHMDP Vbi . show property later Theorem 9, covers cases. intuitioncannot get better solution less information, thus fully observableMDP must upper-bound partially observable case.4.1.2 Approximation Q-Functions (QMDP)variant approximation based fully observable MDP uses Q-functions (Littman,Cassandra, & Kaelbling, 1995):XVb (b) = max b(s)QMDP (s; a);a2A s2SX(s0 )QMDP (s; a) = (s; a) +P (s0 js; a)VMDPs0 2Soptimal action-value function (Q-function) fully observable MDP. QMDPapproximation Vb piecewise linear convex jAj linear functions, corresponding56fiValue-Function Approximations POMDPsone action (Figure 12b). QMDP update rule (for belief state MDP) Vbilinear functions ffki 2 is:Vbi+1 (b) = maxXa2A s2S2b(s) 4(s; a) += (HQMDP Vbi )(b):Xs0 2S3P (s0 js; a) max ffi (s0 )5ffi 2HQMDP generates value function jAj linear functions. time complexityupdate MDP-approximation case { O(jAjjS j2 + j jjS j), reducesO(jAjjS j2 ) time QMDP updates used. HQMDP contraction mappingfixed-point solution found solving corresponding fully observable MDP.QMDP update upper-bounds exact update. bound tighterMDP update; is, H Vbi HQMDP Vbi HMDP Vbi , prove later Theorem 9.inequalities hold fixed-point solutions (through Theorem 6).illustrate difference quality bounds MDP approximationQMDP method, use Maze20 navigation problem. measure qualitybound use mean value-function values. Since belief states equally importantassume uniformly distributed. approximate measure usingaverage values fixed set N = 2000 belief points. points setselected uniformly random beginning. set chosen, fixedremained tests (here later). Figure 13 shows resultsexperiment; include also results fast informed bound method presentednext section.15 Figure 13 also shows running times methods. methodsimplemented Common Lisp run Sun Ultra 1 workstation.4.1.3 ControlMDP QMDP value-function approximations used construct controllers based one-step lookahead. addition, QMDP approximation also suitabledirect control strategy, selects action corresponding best (highestvalue) Q-function. Thus, method special case Q-function approach discussedSection 3.1.3.16 advantage direct QMDP method fasterlookahead designs. hand, lookahead tends improve control performance.shown Figure 14, compares control performance different controllersMaze20 problem.quality policy b , preference towards particular initial belief state,measured mean value-function values b uniformly distributed initialbelief states. approximate measure using average discounted rewards15. confidence interval limits probability level 0.95 range (0:45; 0:62) respectiveaverage scores holds bound experiments paper. relatively smallinclude graphs.16. pointed Littman et al. (1995), instances, direct QMDP controller never selectsinvestigative actions, is, actions try gain information underlying processstate. Note, however, observation true general QMDP-based controllerdirect action selection may select investigative actions, even though fully observable versionproblem investigative actions never chosen.57fiHauskrechtbound qualityMDPapproximationQMDPapproximationrunning timesfast informedbound6014050time [sec]score1201004030208010600MDPapproximation40QMDPapproximationfast informedboundFigure 13: Comparison MDP, QMDP fast informed bound approximations:bound quality (left); running times (right). bound-quality scoreaverage value approximation set 2000 belief points (chosen uniformly random). methods upper-bound optimal value function,ip bound-quality graph longer bars indicate better approximations.2000 control trajectories obtained fixed set N = 2000 initial belief states (selecteduniformly random beginning). trajectories obtained simulation60 steps long.17validate comparison along averaged performance scores, must showscores result randomness methods indeed statisticallysignificantly different. rely pairwise significance tests.18 summarizeobtained results, score differences 1.54, 2.09 2.86 two methods (herealso later paper) sucient reject method lower scorebetter performer significance levels 0.05, 0.01 0.001 respectively.19 Error-barsFigure 14 ect critical score difference significance level 0.05.Figure 14 also shows average reaction times different controllersexperiments. results show clear dominance direct QMDP controller,need lookahead order extract action, compared two MDPbased controllers.4.2 Fast Informed Bound MethodMDP QMDP approaches ignore partial observability use fullyobservable MDP surrogate. improve approximations account (at least17. length trajectories (60 steps) Maze20 problem chosen ensure estimates(discounted) cumulative rewards far actual rewards infinite number steps.18. alternative way compare two methods compute confidence limits scores inspectoverlaps. However, case, ability distinguish two methods reduced dueuctuations scores different initializations. Maze20, confidence interval limits probabilitylevel 0.95 range (1:8; 2:3) respective average scores. covers control experimentslater. Pairwise tests eliminate dependency examining differences individual valuesthus improve discriminative power.19. critical score differences listed cover worst case combination. Thus, may pairssmaller difference would suce.58fiValue-Function Approximations POMDPsreaction timescontrol performance0.02570lookahead0.02lookaheadlookaheadtime [sec]score6050direct40lookaheaddirect0.0150.01300.005200MDPapproximationdirectfast informedboundQMDPapproximationlookaheadlookaheadMDPapproximationdirectQMDPapproximationfast informedboundFigure 14: Comparison control performance MDP, QMDP fast informed boundmethods: quality control (left); reaction times (right). quality-of-controlscore average discounted rewards 2000 control trajectories obtainedfixed set 2000 initial belief states (selected uniformly random).Error-bars show critical score difference value (1.54) two methods become statistically different significance level 0.05.degree) partial observability propose new method { fast informed boundmethod. Let Vbi piecewise linear convex value function represented set linearfunctions . new update defined8<XXX9=XVbi+1 (b) = max : (s; a)b(s) +maxP (s0 ; ojs; a)b(s)ffi (s0 );a2A s2Sff20o2 s2S2S892<XXX= max : b(s) 4(s; a) +maxa2A s2So2 ff 2 s0 2S= (HF IB Vbi )(b):3=P (s0 ; ojs; a)ffi (s0 )5;fast informed bound update obtained exact update followingderivation:8<XXXX9=X9=(H Vbi )(b) = max : (s; a)b(s) +maxP (s0 ; ojs; a)b(s)ffi (s0 );a2A s2Sff2o2s0 2S s2S8<Xmax(s; a)b(s) +a2A :s2SX2XXmaxo2 s2S ffi 2 s0 2SXXP (s0 ; ojs; a)b(s)ffi (s0 );3= max b(s) 4(s; a) +maxP (s0 ; ojs; a)ffi (s0 )5a2A s2Sff2o2s0 2SX= max b(s)ffai+1 (s)a2A s2S= (HF IB Vbi )(b):value function Vbi+1 = HF IB Vbi one obtains update piecewise linearconvex consists jAj different linear functions, corresponding one59fiHauskrechtactionffai+1 (s) = (s; a) +XXmaxP (s0 ; ojs; a)ffi (s0 ):ff2o2s0 2SHF IB update ecient computed O(jAjjS j2 jjj j) time. methodalways outputs jAj linear functions, computation done O(jAj2 jS j2 jj) time,many HF IB updates strung together. significant complexity reductioncompared exact approach: latter lead function consisting jAjj jjjlinear functions, exponential number observations worst casetakes O(jAjjS j2 j jjj) time.HF IB updates polynomial complexity one find approximationfinite-horizon case eciently. open issue remains problem finding solutioninfinite-horizon discounted case complexity. address establishfollowing theorem.Theorem 8 solution fast informed bound approximation found solvingMDP jS jjAjjj states, jAj actions discount factor .full proof theorem deferred Appendix. key part proofconstruction equivalent MDP jS jjAjjj states representing HF IB updates.Since finite-state MDP solved linear program conversion, fixed-pointsolution fast informed bound update computable eciently.4.2.1 Fast Informed Bound versus Fully-Observable MDP Approximationsfast informed update upper-bounds exact update tighter MDPQMDP approximation updates.Theorem 9 Let Vbi corresponds piecewise linear convex value function definedlinear functions. H Vbi HF IB Vbi HQMDP Vbi HMDP Vbi :key trick deriving result swap max sum operators (theproof Appendix) thus obtain upper-bound inequalitiessubsequent reduction complexity update rules compared exact update.also shown Figure 15. UMDP approximation, also included Figure 15,discussed later Section 4.3. Thus, difference among methods boilssimple mathematical manipulations. Note inequality relations derivedupdates hold also fixed-point solutions (through Theorem 6).Figure 13a illustrates improvement bound MDP-based approximationsMaze20 problem. Note, however, improvement paid increasedrunning-time complexity (Figure 13b).4.2.2 Controlfast informed bound always outputs piecewise linear convex function, onelinear function per action. allows us build POMDP controller selects actionassociated best (highest value) linear function directly. Figure 14 comparescontrol performance direct lookahead controllers MDP QMDPcontrollers. see fast informed bound leads tighter bounds also60fiValue-Function Approximations POMDPsUMDP update:V + 1 ( b ) = ax b ( ) ( , ) + axaAexact update:V + 1 ( b ) = ax b ( ) ( , ) +aAP ( ' | , )b ( )'ax'fast informed bound update:V + 1 ( b ) = ax b ( ) ( , ) +aAaxP ( ' | , ) ax ( ' )MDP approx. update:V + 1 ( b ) = b ( ) ax ( , ) +aAP ( ' , | , ) ( ' )''( s' )P ( ' , | , )b ( ) ( ' )QMDP approx. update:V + 1 ( b ) = ax b ( ) ( , ) +'SP ( ' | , ) ax ( ' )Figure 15: Relations exact update UMDP, fast informed bound,QMDP MDP updates.improved control average. However, stress currently theoreticalunderpinning observation thus may true belief statesproblem.4.2.3 Extensions Fast Informed Bound Methodmain idea fast informed bound method select best linear functionevery observation every current state separately. differs exact updateseek linear function gives best result every observationcombination states. However, observe great deal middle groundtwo extremes. Indeed, one design update rule chooses optimal(maximal) linear functions disjoint sets states separately. illustrate idea,assume partitioning = fS1 ; S2 ; ; Sm g state space . new update is:Vbi+1 (b) = maxa2A(Xs2S(s; a)b(s) +X24 maxX Xo2 ffi 2 s2S1 s0 2SP (s0 ; ojs; a)b(s)ffi (s0 )+X XmaxP (s0 ; ojs; a)b(s)ffi (s0 ) + +ff 2 s2S s0 2S2maxX Xffi 2 s2S s0 2S39=P (s0 ; ojs; a)b(s)ffi (s0 )5;easy see update upper-bounds exact update. Explorationapproach various partitioning heuristics remains interesting open research issue.61fiHauskrecht4.3 Approximation Unobservable MDPMDP-approximation assumes full observability POMDP states obtain simplerecient updates. extreme discard observations availabledecision maker. MDP observations called unobservable MDP (UMDP)one may choose value-function solution alternative approximation.find solution unobservable MDP, derive corresponding updaterule, HUMDP , similarly update partially observable case. HUMDP preservespiecewise linearity convexity value function contraction. updateequals:8<X9=XXVbi+1 (b) = max : (s; a)b(s) + maxP (s0 js; a)b(s)ffi (s0 );a2A s2Sff 2 s2S s0 2S= (HUMDP Vbi )(b);set linear functions describing Vbi . Vbi+1 remains piecewise linear convexconsists j jjAj linear functions. contrast exact update,number possible vectors next step grow exponentially numberobservations leads jAjj jjj possible vectors. time complexity updateO(jAjjS j2 j j). Thus, starting Vb0 one linear function, running-time complexityk updates bounded O(jAjk jS j2 ). problem finding optimal solutionunobservable MDP remains intractable: finite-horizon case NP-hard(Burago et al.,1996), discounted infinite-horizon case undecidable (Madani et al., 1999). Thus,usually useful approximation.update HUMDP lower-bounds exact update, intuitive result ectingfact one cannot better less information. provide insighttwo updates related, following derivation, also proves boundproperty elegant way:8<XX9=XX(H Vbi )(b) = max : (s; a)b(s) +maxP (s0 ; ojs; a)b(s)ffi (s0 );a2A s2Sff2o2s0 2S s2S89<X(s; a)b(s) + maxmaxa2A :ff2s2S8<XXXXo2 s2S s0 2SXX=P (s0 ; ojs; a)b(s)ffi (s0 );9== max : (s; a)b(s) + maxP (s0 js; a)b(s)ffi (s0 );a2A s2Sff 2 s2S s0 2S= (HUMDP Vbi )(b):see difference exact UMDP updates maxsum next-step observations exchanged. causes choice ff vectorsHUMDP become independent observations. sum max operationsexchanged, observations marginalized out. Recall idea swaps leadsnumber approximation updates; see Figure 15 summary.62fiValue-Function Approximations POMDPs4.4 Fixed-Strategy Approximationsfinite-state machine (FSM) model used primarily define control strategy.strategy require belief state updates since directly maps sequences observationssequences actions. value function FSM strategy piecewise linear convexfound eciently number memory states (Section 2.6.1).policy iteration policy approximation contexts value function specific strategyused quantify goodness policy first place, value function alonealso used substitute optimal value function. case, value function(defined belief space) equalsV C (b) = max V C (x; b);x2MPV C (x; b) = s2S V C (x; s)b(s) obtained solving set jS jjM j linear equations(Section 2.6.2). remarked earlier, value fixed strategy lower-boundsoptimal value function, V C V .simplify comparison fixed-strategy approximation approximations,rewrite solution also terms fixed-strategy updates8<X9=XXXVbi+1 (b) = max : (s; (x))b(s) +P (o; s0 js; (x))b(s)ffi ((x; o); s0 ); ;x2M s2So2 s2S s0 2S89<X2XX3== max : b(s) 4(s; (x)) +P (o; s0 js; (x))ffi ((x; o); s0 )5;x2M s2So2 s0 2S= (HF SM Vbi )(b):value function Vbi piecewise linear convex consists jM j linear functionsffi (x; :). infinite-horizon discounted case ffi (x; s) represents ith approximationV C (x; s). Note update applied finite-horizon case straightforwardway.4.4.1 Quality ControlAssume FSM strategy would like use substitute optimalcontrol policy. three different ways use extract control.first simply execute strategy represented FSM. needupdate belief states case. second possibility choose linear functionscorresponding different memory states associated actions repeatedly everystep. refer controller direct (DR) controller. approach requiresupdating belief states every step. hand control performanceworse FSM control. final strategy discards informationactions extracts policy using value function Vb (b) one-step lookahead.method (LA) requires belief state updates lookaheads leads worstreactive time. Like DR, however, strategy guaranteed worse FSMcontroller. following theorem relates performances three controllers.63fiHauskrechtcontrol performancereaction times700.0256050time [sec]score0.02400.0150.01300.0052000.0001DR controllerFSM controllerFSM controllerLA controllerDR controllerLA controllerFigure 16: Comparison three different controllers (FSM, DR LA) Maze20problem collection one-action policies: control quality (left) response time (right). Error-bars control performance graph indicatecritical score difference two methods become statistically differentsignificance level 0.05.Theorem 10 Let CF SM FSM controller. Let CDR CLA directone-step-lookahead controllers constructed based CF SM . V C (b) V C (b)V C (b) V C (b) hold belief states b 2 .Though prove direct controller lookahead controlleralways better underlying FSM controller (see Appendix full prooftheorem), cannot show similar property first two controllers initialbelief states. However, lookahead approach typically tends dominate, ectingusual trade-off control quality response time. illustrate trade-offrunning Maze20 example collection jAj one-action policies, generatingsequence action. Control quality response time results shown Figure16. see controller based FSM fastest three, alsoworst terms control quality. hand, direct controller slower (it needsupdate belief states every step) delivers better control. Finally, lookaheadcontroller slowest best control performance.F SMF SMDRLA4.4.2 Selecting FSM Modelquality fixed-strategy approximation depends strongly FSM model used.model provided priori constructed automatically. Techniques automaticconstruction FSM policies correspond search problem either completerestricted space policies examined find optimal near-optimal policyspace. search process equivalent policy approximations policy-iterationtechniques discussed earlier Sections 2.6 3.2.64fiValue-Function Approximations POMDPs4.5 Grid-Based Approximations Value Interpolation-Extrapolationvalue function continuous belief space approximated finite set gridpoints G interpolation-extrapolation rule estimates value arbitrarypoint belief space relying points grid associated values.Definition 8 (Interpolation-extrapolation rule) Let f : ! IR real-valued functiondefined information space , G = fbG1 ; bG2 ; bGk g set grid points G =f(bG1 ; f (bG1 )); (bG2 ; f (bG2 )); ; (bGk ; f (bGk))g set point-value pairs. function RG :(I IR)jGj ! IR estimates f point information space usingvalues associated grid points called interpolation-extrapolation rule.main advantage interpolation-extrapolation model estimating true valuefunction requires us compute value updates finite set grid pointsG. Let Vbi approximation ith value function. approximation(i + 1)th value function Vbi+1 obtainedVbi+1 (b) = RG (b; Gi+1 );values associated every grid point bGj 2 G (and included Gi+1 ) are:'i+1 (bGj ) = (H Vbi )(bGj ) = maxa2A((b; a) +X2P (ojb; a)Vbi ( (bGj ; o; a))):(9)grid-based update also described terms value-function mapping HG :Vbi+1 = HG Vbi . complexity update O(jGjjAjjS j2 jjCEval (RG ; jGj))CEval (RG ; jGj) computational cost evaluating interpolation-extrapolation ruleRG jGj grid points. show later (Section 4.5.3), instances, needevaluate interpolation-extrapolation rule every step eliminated.4.5.1 Family Convex Rulesnumber possible interpolation-extrapolation rules enormous. focusset convex rules relatively small important subset interpolationextrapolation rules.20Definition 9 (Convex rule) Let f function defined space , G = fbG1 ; bG2 ; bGk gset grid points, G = f(bG1 ; f (bG1 )); (bG2 ; f (bG2 )); ; (bGk ; f (bGk ))g set pointvalue pairs. rule RG estimating f using G called convex every b 2 ,value fb(b) is:fb(b) = RG (b; G ) =0 bj 1 every j = 1; ; jGj,jGjXbj f (bj );j =1PjGjbj =1 j= 1.20. note convex rules used work special case averagers introduced Gordon (1995).difference minor; definition averager includes constant (independent grid pointsvalues) added convex combination.65fiHauskrechtkey property convex rules corresponding grid-based update HGcontraction max norm (Gordon, 1995). Thus, approximate value iteration basedHG converges unique fixed-point solution. addition, HG based convex rulesisotone.4.5.2 Examples Convex Rulesfamily convex rules includes approaches commonly used practice,like nearest neighbor, kernel regression, linear point interpolations many others.Take, example, nearest-neighbor approach. function belief point bestimated using value grid point closest terms distance metricdefined belief space. Then, point b, exactly one nonzero parameterbj = 1 k b bGj kM k b bGi kM holds = 1; 2; ; k.zero. Assuming Euclidean distance metric, nearest-neighbor approach leadspiecewise constant approximation, regions equal values correspond regionscommon nearest grid point.nearest neighbor estimates function value taking account onegrid point value. Kernel regression expands upon using grid points.adds weights contributions (values) according distance targetpoint. example, assuming Gaussian kernels, weight grid point bGjbj = fi exp kbbGk2M =22 ;jPfi normalizing constant ensuring jjG=1j bj = 1 parameterattens narrows weight functions. Euclidean metric, kernel-regressionrule leads smooth approximation function.Linear point interpolations subclass convex rules addition constraintsDefinition 9 satisfyjGjXb = bj bGj :j =1is, belief point b convex combination grid points corresponding coecients. optimal value function POMDP convex,new constraint sucient prove upper-bound property approximation.general, many different linear point-interpolations given grid. challenging problem find rule best approximation. discuss issuesSection 4.5.7.4.5.3 Conversion Grid-Based MDPAssume would like find approximation value function using gridbased convex rule grid-based update (Equation 9). view process alsoprocess finding sequence values '1 (bGj ); '2 (bGj ); ; 'i (bGj ); grid-pointsbGj 2 G. show instances sequence values computed withoutapplying interpolation-extrapolation rule every step. cases, problem66fiValue-Function Approximations POMDPsconverted fully observable MDP states corresponding grid-points G.21call MDP grid-based MDP.Theorem 11 Let G finite set grid points RG convex rule parameters bj fixed. values '(bGj ) bGj 2 G found solving fullyobservable MDP jGj states discount factor .Proof grid point bGj write:('i+1 (bGj ) = max (bGj ; a) +a2A8<Xo2XP (ojbGj ; a)VbiG ( (bGj ; a; o)))39jGj=XG )5P (ojbGj ; a) 4 o;a'(bj;k k ;2= max :(bGj ; a) +a2Ao2k=18"#9jGj<h=XX= max : (bGj ; a) + 'Gi (bGk )P (ojbGj ; a)o;aj;k ;a2Ak=1o2PG Gdenoting [ o2 P (ojbj ; a)G o;aj;k ] P (bk jbj ; a), construct fully observableMDP problem states corresponding grid points G discount factor .update step equals:8<'i+1 (bGj ) = max :(bGj ; a) +a2AjGjXk=19=P (bGk jbGj ; a)'Gi (bGk ); :Pprerequisite 0 bj 1 every j = 1; ; jGj jjG=1j bj = 1 guaranteesP (bGk jbGj ; a) interpreted true probabilities. Thus, one compute values '(bGj )solving equivalent fully-observable MDP. 24.5.4 Solving Grid-Based Approximationsidea converting grid-based approximation grid-based MDP basissimple powerful approximation algorithm. Brie y, key findparameters (transition probabilities rewards) new MDP model solveit. process relatively easy parameters used interpolate-extrapolatevalue non-grid point fixed (the assumption Theorem 11). case,determine parameters new MDP eciently one step, grid set G.nearest neighbor kernel regression examples rules property. Noteleads polynomial-time algorithms finding values grid points (recallMDP solved eciently finite discounted, infinite-horizon criteria).problem solving grid-based approximation arises parametersused interpolation-extrapolation fixed subject optimizationitself. happens, example, multiple ways interpolating value21. note similar result also proved independently Gordon (1995).67fiHauskrechtpoint belief space would like find best interpolation (leadingbest values) grid points G. case, corresponding \optimal"grid-based MDP cannot found single step iterative approximation, solvingsequence grid-based MDPs, usually needed. worst-case complexity problemremains open question.4.5.5 Constructing Gridsissue touched far selection grids. multiple waysselect grids. divide two classes { regular non-regular grids.Regular grids (Lovejoy, 1991a) partition belief space evenly equal-size regions.22main advantage regular grids simplicity locate grid pointsneighborhood belief point. disadvantage regular gridsrestricted specific number points, increase grid resolution paidexponential increase grid size. example, sequence regular grids20-dimensional belief space (corresponds POMDP 20 states) consists 20, 210,1540, 8855, 42504, grid points.23 prevents one using method highergrid resolutions problems larger state spaces.Non-regular grids unrestricted thus provide exibility grid resolution must increased adaptively. hand, due irregularities, methodslocating grid points adjacent arbitrary belief point usually complexcompared regular grids.4.5.6 Linear Point Interpolationfact optimal value function V convex belief-state MDPs usedshow approximation based linear point interpolation always upper-boundsexact solution (Lovejoy, 1991a, 1993). Neither kernel regression nearest neighborguarantee us bound.Theorem 12 (Upper bound property grid-based point interpolation update). Let Vbiconvex value function. H Vbi HG Vbi .upper-bound property HG update convex value functions follows directlyJensen's inequality. convergence upper-bound follows Theorem 6.Note point-interpolation update imposes additional constraint choicegrid points. particular, easy see valid grid must also include extreme points belief simplex (extreme points correspond (1; 0; 0; ); (0; 1; 0; ),22. Regular grids used Lovejoy (1991a) based Freudenthal triangulation (Eaves, 1984). Essentially, idea used partition evenly n-dimensional subspace IR . fact,ane transform allows us map isomorphically grid points belief space grid pointsn-dimensional space (Lovejoy, 1991a).23. number points regular grid sequence given (Lovejoy, 1991a):n+ jS j 1)!;jGj = (M!(jS j 1)!= 1; 2; grid refinement parameter.68fiValue-Function Approximations POMDPsetc.). Without extreme points one would unable cover whole belief space viainterpolation. Nearest neighbor kernel regression impose restrictions grid.4.5.7 Finding best interpolationgeneral, multiple ways interpolate point belief space. objectivefind best interpolation, is, one leads tightest upper boundoptimal value function.Let b belief point f(bj ; f (bj ))jbj 2 Gg set grid-value pairs. bestinterpolation point b is:jGjXfb(b) = min j f (bj )j =1P= 1; ; jGj, jjG=1j jPsubject 0 j 1 j= 1, b = jjG=1j j bGj .linear optimization problem. Although solved polynomial time(using linear programming techniques), computational cost still relativelylarge, especially considering fact optimization must repeated many times.alleviate problem seek ecient ways finding interpolation, sacrificingoptimality.One way find (suboptimal) interpolation quickly apply regular grids proposedLovejoy (1991a). case value belief point approximated usingconvex combination grid points closest it. approximation leads piecewise linearconvex value functions. interpolations fixed here, problem findingapproximation converted equivalent grid-based MDP solvedfinite-state MDP. However, pointed previous section, regular grids must usespecific number grid points increase resolution grid paidexponential increase grid size. feature makes method less attractiveproblem large state space need achieve high grid resolution.24present work focus non-regular (or arbitrary) grids. propose interpolation approach searches limited space interpolations guaranteed runtime linear size grid. idea approach interpolate pointb belief space dimension jS j set grid points consists arbitrarygrid point b0 2 G jS j 1 extreme points belief simplex. coecientsinterpolation found eciently search best interpolation. Letb0 2 G grid point defining one interpolation. value point b satisfies0bbVbi (b) = min0 Vi (b);b 2GVbib0 value interpolation grid point b0 . Figure 17 illustratesresulting approximation. function characterized \sawtooth" shape,uenced choice interpolating set.find best value-function solution close approximation apply valueiteration procedure search best interpolation every update step.24. One solution problem may use adaptive regular grids grid resolution increasedparts belief space. leave idea future work.69fiHauskrechtV(b)V(b)*V(b)0 b0b1b2b3b 4 1 b(s )1Figure 17: Value-function approximation based linear-time interpolation approach(a two-dimensional case). Interpolating sets restricted single internalpoint belief space.drawback approach interpolations may remain unchanged manyupdate steps, thus slowing solution process. alternative approach solvesequence grid-based MDPs instead. particular, every stage find best(minimum value) interpolations belief points reachable grid points one step, fixcoecients interpolations (s), construct grid-based MDP solve (exactlyapproximately). process repeated improvement (or improvementlarger threshold) seen values different grid points.4.5.8 Improving Grids Adaptivelyquality approximation (bound) depends strongly points used grid.objective provide good approximation smallest possible set gridpoints. However, task impossible achieve, since cannot known advance(before solving) belief points pick. way address problem build gridsincrementally, starting small set grid points adding others adaptively,places greater chance improvement. key part approachheuristic choosing grid points added next.One heuristic method developed attempts maximize improvements boundvalues via stochastic simulations. method builds fact every interpolationgrid must also include extreme points (otherwise cannot cover entire belief space).extreme points values affect grid points, try improvevalues first place. general, value grid point b improvesprecise values used successor belief points, is, belief states correspond(b; ; o) choice observation o. current optimal action choice b.Incorporating points grid makes larger improvement valueinitial grid point b likely. Assuming initial point extreme point,heuristic tends improve value point. Naturally, one proceedselection incorporating successor points first-level successorsgrid well, forth.70fiValue-Function Approximations POMDPsgenerate new grid points (G; Vb G )set Gnew = fgextreme points brepeat b 2= G [ GnewnPset = arg maxa (b; a) + o2 P (ojb; a)Vb G ( (b; a; o))select observation according P (ojb; )update b = (b; ; o)add b Gnewreturn GnewFigure 18: Procedure generating additional grid points based bound improvement heuristic.bound qualityMDPfast interpolationQMDPinformed regular gridinterpolationadaptive gridinterpolationrandom grid14040 80 120 160 200 240 280 320 360 400120score401008080120 160 200240 280 320 360 4006040Figure 19: Improvement upper bound quality grid-based point-interpolationsbased adaptive-grid method. method compared randomlyrefined grid regular grid 210 points. upper-bound approximations (the MDP, QMDP fast informed bound methods) includedcomparison.capture idea, generate new grid points via simulation, starting oneextremes belief simplex continuing belief point currentlygrid reached. algorithm implements bound improvement heuristicexpands current grid G set jS j new grid points relying currentvalue-function approximation Vb G shown Figure 18.Figure 19 illustrates performance (bound quality) adaptive grid methodMaze20 problem. use combination adaptive grids linear-timeinterpolation approach. method gradually expands grid 40 point increments400 grid points. Figure 19 also shows performance random-grid method71fiHauskrechtrunning times500045004000400time [sec]35003000360250032040020003602801500280 32024010002402005001.261.2650.0240 8012020016012040 801600MDP QMDP fastinformedinterpolationregular gridinterpolationadaptive gridinterpolationrandom gridFigure 20: Running times grid-based point-interpolation methods. Methods tested include adaptive grid, random grid, regular grid 210 gridpoints. Running times adaptive-grid cumulative, ecting dependencies higher grid resolutions lower-level resolutions. runningtime results MDP, QMDP, fast informed bound approximationsshown comparison.new points grid selected iniformly random (results 40 grid point incrementsshown). addition, figure gives results regular grid interpolation (basedLovejoy (1991a)) 210 belief points upper-bound methods: MDP,QMDP fast informed bound approximations.see dramatic improvement quality bound adaptive method.contrast this, uniformly sampled grid (random-grid approach) hardly changesbound. two reasons this: (1) uniformly sampled grid points likelyconcentrated center belief simplex; (2) transition matrix Maze20problem relatively sparse, belief points one obtains extreme points onestep boundary simplex. Since grid points center simplexnever used interpolate belief states reachable extremes one step cannotimprove values extremes bound change.One drawback adaptive method running time (for every grid size needsolve sequence grid-based MDPs). Figure 20 compares running times differentmethods Maze20 problem. grid-expansion adaptive method dependsvalue function obtained previous steps, plot cumulative running times.see relatively large increase running time, especially larger grid sizes, ectingtrade-off bound quality running time. However, noteadaptive-grid method performs quite well initial steps, 80 gridpoints outperforms regular grid (with 210 points) bound quality.Finally, note heuristic approaches constructing adaptive grids pointinterpolation possible. example, different approach refines grid ex-72fiValue-Function Approximations POMDPscontrol performance70score604005020040 200 400404020040400200400403020fastinterpolation interpolationMDP QMDP informed (regular grid) (adaptive grid)interpolation(random grid)nearest-neighbor nearest-neighbor(adaptive grid)(random grid)Figure 21: Control performance lookahead controllers based grid-based point interpolation nearest neighbor methods varying grid sizes. resultscompared MDP, QMDP fast informed bound controllers.amining differences values current grid points recently proposed Brafman(1997).4.5.9 ControlValue functions obtained different grid-based methods define variety controllers. Figure 21 compares performances lookahead controllers based point-interpolationnearest-neighbor methods. run two versions approaches, one adaptive grid, random grid, show results obtained 40, 200 400grid points. addition, compare performances interpolation regulargrids (with 210 grid points), MDP, QMDP fast informed bound approaches.Overall, performance interpolation-extrapolation techniques testedMaze20 problem bit disappointing. particular, better scores achievedsimpler QMDP fast informed bound methods. see that, although heuristicsimproved bound quality approximations, lead similar improvementQMDP fast informed bound methods terms control. resultshows bad bound (in terms absolute values) always imply bad controlperformance. main reason control performance uenced mostlyrelative rather absolute value-function values (or, words, shapefunction). interpolation-extrapolation techniques use (except regular gridinterpolation) approximate value function functions piecewise linearconvex; interpolations based linear-time interpolation techniquesawtooth-shaped function, nearest-neighbor leads piecewise-constant function.allow match shape optimal function correctly.factor affects performance large sensitivity methods selection gridpoints, documented, example, comparison heuristic random grids.73fiHauskrechttests focused lookahead controllers only. However, alternative waydefine controller grid-based interpolation-extrapolation methods use Q-functionapproximations instead value functions, either direct lookahead designs.25 Qfunction approximations found solving grid-based MDP, keepingvalues (functions) different actions separate end.4.6 Approximations Value Functions Using Curve Fitting (Least-SquaresFit)alternative way approximate function continuous space use curve-fittingtechniques. approach relies predefined parametric model value functionset values associated finite set (grid) belief points G. approachsimilar interpolation-extrapolation techniques relies set belief-valuepairs. difference curve fitting, instead remembering belief-value pairs,tries summarize terms given parametric function model. strategy seeksbest possible match model parameters observed point values. bestmatch defined using various criteria, often least-squares fit criterion,objective minimizeError(f ) =1X[y2 j jf (bj )]2 :bj yj correspond belief point associated value. index j rangespoints sample set G.4.6.1 Combining Dynamic Programming Least-Squares Fitleast-squares approximation function used construct dynamic-programmingalgorithm update step: Vbi+1 = HLSF Vbi . approach two steps. First,obtain new values set sample points G:'i+1 (b) = (H Vbi )(b) = maxa2A(Xs2S(s; a)b(s) +XXo2 s2S)bP (ojs; a)b(s)Vi ( (b; a; o)) :Second, fit parameters value-function model Vbi+1 using new sample-value pairssquare-error cost function. complexity update O(jGjjAjjS j2 jjCEval (Vbi )+CFit (Vbi+1 ; jGj)) time, CEval(Vbi ) computational cost evaluating VbiCFit (Vbi+1 ; jGj) cost fitting parameters Vbi+1 jGj belief-value pairs.advantage approximation based least-squares fit requires uscompute updates finite set belief states. drawback approachthat, combined value-iteration method, lead instability and/ordivergence. shown MDPs several researchers (Bertsekas, 1994; Boyan& Moore, 1995; Baird, 1995; Tsitsiklis & Roy, 1996).25. similar QMDP method, allows lookahead greedy designs. fact, QMDPviewed special case grid-based method Q-function approximations, gridpoints correspond extremes belief simplex.74fiValue-Function Approximations POMDPs4.6.2 On-line Version Least-Squares Fitproblem finding set parameters best fit solved availableoptimization procedure. includes on-line (or instance-based) version gradientdescent method, corresponds well-known delta rule (Rumelhart, Hinton, &Williams, 1986).Let f denote parametric value function belief space adjustable weightsw = fw1 ; w2 ; ; wk g. on-line update weight wi computed as:wiwi ffi (f (bj ) yj )@fj ;@wi bjffi learning constant, bj yj last-seen point value. Notegradient descent method requires function differentiable regardadjustable weights.solve discounted infinite-horizon problem, stochastic (on-line) versionleast-squares fit combined either parallel (synchronous) incremental (GaussSeidel) point updates. first case, value function previous step fixednew value function computed scratch using set belief point samplesvalues computed one-step expansion. parameters stabilized (byattenuating learning rates), newly acquired function fixed, process proceedsanother iteration. incremental version, single value-function modeltime updated used compute new values sampled points. Littman et al. (1995)Parr Russell (1995) implement approach using asynchronous reinforcementlearning backups sample points updated next obtained via stochasticsimulation. stress versions subject threat instability divergence,remarked above.4.6.3 Parametric Function Modelsapply least-squares approach must first select appropriate value functionmodel. Examples simple convex functions linear quadratic functions,complex models possible well.One interesting relatively simple approach based least-squares approximation linear action-value functions (Q-functions) (Littman et al., 1995).value function Vbi+1 approximated piecewise linear convex combination Qb i+1functions:Vbi+1 (b) = max Qb i+1 (b; a);a2AQb i+1 (b; a) least-squares fit linear function set sample points G.Values points G obtained'ai+1 (b) = (b; a) +Xo2P (ojb; a)Vbi ( (b; o; a)):method leads approximation jAj linear functions coecientsfunctions found eciently solving set linear equations. Recall twoapproximations (the QMDP fast informed bound approximations) also work75fiHauskrechtjAj linear functions. main differences methods QMDPfast informed bound methods update linear functions directly, guarantee upperbounds unique convergence.sophisticated parametric model convex function softmax model (Parr& Russell, 1995):2Vb (b) = 4"X Xff2s2S#k 3 1ff(s)b(s) 5k;set linear functions ff adaptive parameters fit k \temperature" parameter provides better fit underlying piecewise linear convex functionlarger values. function represents soft approximation piecewise linear convexfunction, parameter k smoothing approximation.4.6.4 Controltested control performance least-squares approach linear Q-functionmodel (Littman et al., 1995) softmax model (Parr & Russell, 1995). softmaxmodel varied number linear functions, trying cases 10 15 linear functionsrespectively. first set experiments used parallel (synchronous) updatessamples fixed set 100 belief points. applied stochastic gradient descent techniquesfind best fit cases. tested control performance value-functionapproximations obtained 10, 20 30 updates, starting QMDP solution.second set experiments, applied incremental stochastic update schemeGauss-Seidel-style updates. results method acquired every grid pointupdated 150 times, learning rates decreasing linearly range 0:20:001. started QMDP solution. results lookahead controllerssummarized Figure 22, also shows control performance direct Q-functioncontroller and, comparison, results QMDP method.linear-Q function model performed well results lookahead designbetter results QMDP method. difference quite apparentdirect approaches. general, good performance method attributedchoice function model let us match shape optimal value functionreasonably well. contrast, softmax models (with 10 15 linear functions)perform expected. probably softmax model linear functionsupdated every sample point. leads situations multiple linear functionstry track belief point update. circumstances hard capturestructure optimal value function accurately. negative featureeffects on-line changes linear functions added softmax approximation,thus could bias incremental update schemes. ideal case, would like identifyone vector ff responsible specific belief point update (modify) vector.linear Q-function approach avoids problem always updating single linearfunction (corresponding action).76fiValue-Function Approximations POMDPscontrol performance7060lookahead10 20iter iter 30 stochiter10 20iter iter10 20 30iter iter iterscore5040stoch30iter stoch2010 iteriter30iterstochdirect3020QMDPapproximationlinear Q-functionlookaheadlinear Q-functiondirectsoftmax(10 linear functions)softmax(15 linear functions)Figure 22: Control performance least-squares fit methods. Models tested include: linearQ-function model (with direct lookahead control) softmax models 10 15 linear functions (lookahead control only). Value functionsobtained 10, 20 30 synchronous updates value functions obtainedincremental stochastic update scheme used define differentcontrollers. comparison, also include results two QMDP controllers.4.7 Grid-Based Approximations Linear Function Updatesalternative grid-based approximation method constructed applying Sondik'sapproach computing derivatives (linear functions) points grid (Lovejoy, 1991a,1993). Let Vbi piecewise linear convex function described set linear functions .new linear function belief point b action computed eciently(Smallwood & Sondik, 1973; Littman, 1996)ffb;ai+1 (s) = (s; a) +XXo2 s0 2SP (s0 ; ojs; a)ffi(b;a;o) (s0 );(b; a; o) indexes linear function ffi set linear functionsmaximizes expression"X Xs0 2S s2S(10)(defining Vbi )#P (s0 ; ojs; a)b(s) ffi (s0 )fixed combination b; a; o. optimizing function b acquired choosingvector best overall value action vectors. is, assuming bi+1set candidate linear functions, resulting functions satisfies= arg max X ffb (s)b(s):ffb;i+1i+1ff +1 2 +1 s2Scollection linear functions obtained set belief points combinedpiecewise linear convex value function. idea behind number exactbb77fiHauskrechtV(b)*V(b)V(b)new linear function0b1b(s1 )Figure 23: incremental version grid-based linear function method. piecewiselinear lower bound improved new linear function computed beliefpoint b using Sondik's method.algorithms (see Section 2.4.2). However, exact case, set points coverlinear functions defining new value function must located first, hard taskitself. contrast, approximation method uses incomplete set belief pointsfixed least easy locate, example via random heuristic selection. useHGL denote value-function mapping grid approach.advantage grid-based method leads ecient updates.time complexity update polynomial equals O(jGjjAjjS j2 jj). yields setjGj linear functions, compared jAjj jjj possible functions exact update.Since set grid-points incomplete, resulting approximation lower-boundsvalue function one would obtain performing exact update (Lovejoy, 1991a).Theorem 13 (Lower-bound property grid-based linear function update). Let Vbipiecewise linear value function G set grid points used compute linear functionupdates. HGL Vbi H Vbi .4.7.1 Incremental Linear-Function Approachdrawback grid-based linear function method HGL contractiondiscounted infinite-horizon case, therefore value iteration method basedmapping may converge (Lovejoy, 1991a). remedy problem, proposeincremental version grid-based linear function method. idea refinementprevent instability gradually improving piecewise linear convex lower boundvalue function.Assume Vbi V convex piecewise linear lower bound optimal valuefunction defined linear function set , let ffb linear function point bcomputed Vbi using Sondik's method. one construct new improvedvalue function Vbi+1 Vbi simply adding new linear function ffb . is:i+1 = [ ffb . idea incremental update, illustrated Figure 23, similarincremental methods used Cheng (1988) Lovejoy (1993). method78fiValue-Function Approximations POMDPsrunning timesbound quality2500659200060108score55504540234567891023456789time [sec]710161500105981000476354500123351.2650.02121030standard approachQMDPincremental approachfastinformedstandard approachincremental approachFigure 24: Bound quality running times standard incremental versiongrid-based linear-function method fixed 40-point grid. Cumulativerunning times (including previous update cycles) shown methods.Running times QMDP fast informed bound methods includedcomparison.extended handle set grid points G straightforward way. Note alsoadding one new linear functions , previous linear functions maybecome redundant removed value function. Techniques redundancychecking applied exact approaches (Monahan, 1982; Eagle, 1984).incremental refinement stable converges fixed set grid points.price paid feature linear function set grow size iterationsteps. Although growth linear number iterations, comparedpotentially exponential growth exact methods, linear function set describingpiecewise linear approximation become huge. Thus, practice usually stopincremental updates well method converges. question remains opencomplexity (hardness) problem finding fixed-point solution fixed setgrid points G.Figure 24 illustrates trade-offs involved applying incremental updatescompared standard fixed-grid approach Maze20 problem. usegrid 40 points techniques initial value function. Results 1-10update cycles shown. see incremental method longer running timesstandard method, since number linear functions grow every update.hand, bound quality incremental method improves rapidlynever become worse update steps.4.7.2 Minimum Expected Rewardincremental method improves lower bound value function. value function, say Vbi , used create controller (with either lookahead direct-actionchoice). general case, cannot say anything performance qualitycontrollers regard Vbi . However, certain conditions performancecontrollers guaranteed never fall Vbi . following theorem (provedAppendix) establishes conditions.Theorem 14 Let Vbi value function obtained via incremental linear function method,starting Vb0 , corresponds fixed strategy C0 . Let CLA;i CDR;i two79fiHauskrechtcontrollers based Vbi : lookahead controller direct action controller, V C ,VCrespective value functions. Vbi V CVbi V Chold.note property holds incremental version exact value iteration.is, lookahead direct controllers perform worse Vi obtainedincremental updates V0 corresponding FSM controller C0 .LA;iDR;iLA;iDR;i4.7.3 Selecting Grid Pointsincremental version grid-based linear-function approximation exibleworks arbitrary grid.26 Moreover, grid need fixed changedline. Thus, problem finding grids reduces problem selecting belief pointsupdated next. One apply various strategies this. example, one usefixed set grid points update repeatedly, one select belief points lineusing various heuristics.incremental linear function method guarantees value function alwaysimproved (all linear functions previous steps kept unless found redundant).quality new linear function (to added next) depends strongly qualitylinear functions obtained previous steps. Therefore, objective select orderpoints better chances larger improvement. designed two heuristicstrategies selecting ordering belief points.first strategy attempts optimize updates extreme points belief simplexordering heuristically. idea heuristic based fact stateshigher expected rewards (e.g. designated goal states) backpropagate effects(rewards) locally. Therefore, desirable states neighborhood highestreward state updated first, distant ones later. apply idea orderextreme points belief simplex, relying current estimate value functionidentify highest expected reward states POMDP model determineneighbor states.second strategy based idea stochastic simulation. strategy generatessequence belief points likely reached (fixed) initial belief point.points sequence used reverse order generate updates. intentheuristic \maximize" improvement value function initial fixedpoint. run heuristic, need find initial belief point set initial beliefpoints. address problem, use first heuristic allows us orderextreme points belief simplex. points used initial beliefssimulation part. Thus, two-tier strategy: top-level strategy orders extremesbelief simplex, lower-level strategy applies stochastic simulation generatesequence belief states likely reachable specific extreme point.tested order heuristics two-tier heuristics Maze20 problem,compared also two simple point selection strategies: fixed-grid strategy,set 40 grid points updated repeatedly, random-grid strategy,points always chosen uniformly random. Figure 25 shows bound quality26. restriction grid points must included grid, requiredexample linear point-interpolation scheme, use extreme points beliefsimplex.80fiValue-Function Approximations POMDPsbound quality6560score552505 6 7 8 9 103 421374 5 68 9 1024327 8 9 105 63111457 8 9 104 5 6403530fixed gridrandom gridorder heuristic2-tier heuristicFigure 25: Improvements bound quality incremental linear-function methodfour different grid-selection heuristics. cycle includes 40 grid-pointupdates.methods 10 update cycles (each cycle consists 40 grid point updates)Maze20 problem. see differences quality value-function approximationsdifferent strategies (even simple ones) relatively small. noteobserved similar results also problems, Maze20.relatively small improvement heuristics explained factevery new linear function uences larger portion belief space thus methodless sensitive choice specific point.27 However, another plausible explanation heuristics good accurate heuristics combinationsheuristics could constructed. Ecient strategies locating grid points usedexact methods, e.g. Witness algorithm (Kaelbling et al., 1999) Cheng's methods (Cheng, 1988) potentially applied problem. remains open arearesearch.4.7.4 Controlgrid-based linear-function approach leads piecewise linear convex approximation. Every linear function comes natural action choice lets us chooseaction greedily. Thus run lookahead direct controllers. Figure 26compares performance four different controllers fixed grid 40 points, combining standard incremental updates lookahead direct greedy control 1,5 10 update cycles. results (see also Figure 24) illustrate trade-offscomputational time obtaining solution quality. see incrementalapproach lookahead controller design tend improve control performance.prices paid worse running reaction times, respectively.27. small sensitivity incremental method selection grid points would suggest onecould, many instances, replace exact updates simpler point selection strategies. couldincrease speed exact value-iteration methods (at least initial stages), sufferineciencies associated locating complete set grid points updated every step. However,issue needs investigated.81fiHauskrechtcontrol performance7056010101lookaheadlookahead5105101511score50direct40direct3020QMDPfastinformeddirectstandardlookaheadstandarddirectincrementallookaheadincrementalFigure 26: Control performance four different controllers based grid-based linear function updates 1, 5 10 update cycles 40-point grid. Controllers represent combinations two update strategies (standard incremental) two action-extraction techniques (direct lookahead). Runningtimes two update strategies presented Figure 24. comparison include also performances QMDP fast informed boundmethods (with direct lookahead designs).control performance705score5101011601510151050403020QMDPfastinformedfixed gridrandom gridorder heuristic2-tier heuristicFigure 27: Control performances lookahead controllers based incremental linearfunction approach different point-selection heuristics 1, 5 10 improvement cycles. comparison, scores QMDP fast informedbound approximations shown well.Figure 27 illustrates effect point selection heuristics control. compareresults lookahead control only, using approximations obtained 1, 5 10 improvement cycles (each cycle consists 40 grid point updates). test results show that,82fiValue-Function Approximations POMDPsbound quality, big differences among various heuristics, suggestingsmall sensitivity control selection grid points.4.8 Summary Value-Function ApproximationsHeuristic value-function approximations methods allow us replace hard-to-compute exactmethods trade solution quality speed. numerous methods employ, different properties different trade-offs quality versus speed. Tables 12 summarize main theoretical properties approximation methods coveredpaper. majority methods polynomial complexity least ecient (polynomial) Bellman updates. makes good candidates complexPOMDP problems reach exact methods.methods heuristic approximations give solutionsguaranteed precision. Despite fact proved solutions methodsworse others terms value function quality (see Figure 15). onemain contributions paper. However, currently minimal theoretical resultsrelating methods terms control performance; exception resultsFSM-controllers FSM-based approximations. key observationquality control (lookahead control) important approximate shape(derivatives) value function correctly. also illustrated empirically gridbased interpolation-extrapolation methods Section 4.5.9 based non-convexvalue functions. main challenges find ways analyzing comparingcontrol performance different approximations also theoretically identify classesPOMDPs certain methods dominate others.Finally, note list methods complete value-function approximation methods refinements existing methods possible. example, WhiteScherer (1994) investigate methods based truncated histories lead upperlower bound estimates value function complete information states (completehistories). Also, additional restrictions methods change propertiesgeneric method. example, possible additional assumptionsable ensure convergence least-squares fit approximation.5. ConclusionsPOMDPs offers elegant mathematical framework representing decision processesstochastic partially observable domains. Despite modeling advantages, however,POMDP problems hard solve exactly. Thus, complexity problem solvingprocedures becomes key aspect sucessful application model real-worldproblems, even expense optimality. recent complexity resultsapproximability POMDP problems encouraging (Lusena et al., 1998; Madaniet al., 1999), focus heuristic approximations, particular approximations valuefunctions.83fiHauskrechtMethodMDP approximationQMDP approximationFast informed boundUMDP approximationFixed-strategy methodGrid-based interpolation-extrapolationNearest neighborKernel regressionLinear point interpolationCurve-fitting (least-squares fit)linear Q-functionGrid-based linear function methodIncremental version (start lower bound)BoundupperupperupperlowerlowerupperlowerlowerIsotonicityppppp-pppContraction-p-*ppppp-pppTable 1: Properties different value-function approximation methods: bound property,isotonicity contraction property underlying mappings 0 < 1.(*) Although incremental version grid-based linear-function methodcontraction always converges.MethodMDP approximationQMDP approximationFast informed boundUMDP approximationFixed-strategy methodGrid-based interpolation-extrapolationNearest neighborKernel regressionLinear point interpolationFixed interpolationBest interpolationCurve-fitting (least-squares fit)linear Q-functionGrid-based linear function methodIncremental versionFinite-horizonPPPNP-hardPvariesPPPPPvariesPPNADiscounted infinite-horizonPPPundecidablePNAPPvariesP?NANANA?Table 2: Complexity value-function approximation methods finite-horizon problemdiscounted infinite-horizon problem. objective discounted infinitehorizon case find corresponding fixed-point solution. complexityresults take account, addition components POMDPs, alsoapproximation specific parameters, e.g., size grid G grid-based methods. ? indicates open instances NA methods applicable oneproblems (e.g. possible divergence).84fiValue-Function Approximations POMDPs5.1 Contributionspaper surveys new known value-function approximation methods solving POMDPs.focus primarily theoretical analysis comparison methods, findings results supported experimentally problem moderate size agentnavigation domain. analyze methods different perspectives: computational complexity, capability bound optimal value function, convergence propertiesiterative implementations, quality derived controllers. analysis includes newtheoretical results, deriving properties individual approximations, relationsexact methods. general, relations trade-offs among different methodswell understood. provide new insights issues analyzingcorresponding updates. example, showed differences among exact,MDP, QMDP, fast-informed bound, UMDP methods boil simplemathematical manipulations subsequent effect value-function approximation. allowed us determine relations among different methods terms qualityrespective value functions one main results paper.also presented number new methods heuristic refinements existingtechniques. primary contributions area include fast-informed bound, gridbased point interpolation methods (including adaptive grid approaches based stochastic sampling), incremental linear-function method. also showedinstances solutions obtained eciently converting original approximation equivalent finite-state MDP. example, grid-based approximationsconvex rules often solved via conversion grid-based MDP (in grid pointscorrespond new states), leading polynomial-complexity algorithm finite discounted infinite-horizon cases (Section 4.5.3). result dramaticallyimprove run-time performance grid-based approaches. similar conversionequivalent finite-state MDP, allowing polynomial-time solution discountedinfinite-horizon problem, shown fast informed bound method (Section 4.2).5.2 Challenges Future DirectionsWork POMDPs approximations far complete. complexity resultsremain open, particular, complexity grid-based approach seeking best interpolation, complexity finding fixed-point solution incremental versiongrid-based linear-function method. Another interesting issue needs investigation convergence value iteration least-squares approximation. Althoughmethod unstable general case, possible certain restrictionsconverge.paper use single POMDP problem (Maze20) support theoreticalfindings illustrate intuitions. Therefore, results supported theoretically (related mostly control) cannot generalized used rank different methods,since performance may vary problems. general, area POMDPsPOMDP approximations suffers shortage larger-scale experimental workmultiple problems different complexities broad range methods. Experimentalwork especially needed study compare different methods regard controlquality. main reason theoretical results relating85fiHauskrechtcontrol performance. studies help focus theoretical exploration discoveringinteresting cases possibly identifying classes problems certain approximations less suitable. preliminary experimental results showsignificant differences control performance among different methodsmay suitable approximate control policies. example, grid-basednearest-neighbor approach piecewise-constant approximation typically inferioroutperformed simpler (and ecient) value-function methods.present work focused heuristic approximation methods. investigated general ( at) POMDPs take advantage additional structural refinements.However, real-world problems usually offer structure exploited devisenew algorithms perhaps lead speed-ups. also possiblerestricted versions POMDPs (with additional structural assumptions) solvedapproximated eciently, even though general complexity results POMDPs approximations encouraging (Papadimitriou & Tsitsiklis, 1987; Littman, 1996;Mundhenk et al., 1997; Lusena et al., 1998; Madani et al., 1999). challengeidentify models allow ecient solutions time interesting enoughpoint application.Finally, number interesting issues arise move problems large state,action, observation spaces. Here, complexity value-function updatesalso belief state updates becomes issue. general, partial observability hiddenprocess states allow us factor decompose belief states (and updates),even transitions great deal structure represented compactly.Promising directions deal issues include various Monte-Carlo approaches (Isard& Blake, 1996; Kanazawa, Koller, & Russell, 1995; Doucet, 1998; Kearns et al., 1999)),methods approximating belief states via decomposition (Boyen & Koller, 1998, 1999),combination two approaches (McAllester & Singh, 1999).AcknowledgementsAnthony Cassandra, Thomas Dean, Leslie Kaelbling, William Long, Peter Szolovitsanonymous reviewers provided valuable feedback comments work. researchsupported grant RO1 LM 04493 grant 1T15LM07092 National LibraryMedicine, DOD Advanced Research Project Agency (ARPA) contract numberN66001-95-M-1089 DARPA/Rome Labs Planning Initiative grant F30602-95-1-0020.Appendix A. Theorems proofsA.1 Convergence BoundTheorem 6 Let H1 H2 two value-function mappings defined V1 V2 s.t.1. H1 , H2 contractions fixed points V1 , V2 ;2. V1 2 V2 H2 V1 H1 V1 = V1 ;3. H2 isotone mapping.V2 V1 holds.86fiValue-Function Approximations POMDPsProof applying H2 condition 2 expanding result condition 2get: H22 V1 H2 V1 H1 V1 = V1 . Repeating get limit V2 H2n V1H22 V1 H2V1 H1V1 = V1 , proves result. 2A.2 Accuracy Lookahead Controller Based BoundsTheorem 7 Let VbU VbL upper lower bounds optimal value functiondiscounted infinite-horizon problem. Let = supb jVbU (b) VbL (b)j = kVbU VbL kmaximum bound difference. expected reward lookahead controller Vb LA ,constructed either VbU VbL , satisfies kVb LA V k (1(2 )) .Proof Let Vb denotes either upper lower bound approximation V H LAvalue function mapping corresponding lookahead policy Vb . Note, sincelookahead policy always optimizes actions regard Vb , H Vb = H LA Vb must hold.error Vb LA bounded using triangle inequalitykVb LA V k kVb LA Vb k + kVb V k:first component satisfies:kVb LA Vb k = kH LA Vb LA Vb kkH LA Vb LA H Vb k + kH Vb Vb k= kH LA Vb LA H LA Vb k + kH Vb Vb kkVb LA Vb k +inequality: kH Vb Vb k follows isotonicity H fact Vb eitherupper lower bound. Rearranging inequalities, obtain: kVb LA Vb k = (1 ) .bound second term kVb V k trivial.(2 )Therefore, kVb LA V k [ (1 1 ) + 1] = (1) . 2A.3 MDP, QMDP Fast Informed BoundsTheorem 8 solution fast informed bound approximation found solvingMDP jS jjAjjj states, jAj actions discount factor .Proof Let ffai linear function action defining Vbi . Let ffi (s; a) denote parametersfunction. parameters Vbi+1 satisfy:ffi+1 (s; a) = (s; a) +LetXX00 0max0 2A 0 P (s ; ojs; a)ffi (s ; ):o22Sffi+1 (s; a; o) = max0X2 s0 287P (s0 ; ojs; a)ffi (s0 ; a0 ):fiHauskrechtNow, rewrite ffi+1 (s; a; o) every s; a; as:8<X2Xffi+1 (s; a; o) = maxP (s0 ; ojs; a) 4(s0 ; a0 ) +a0 2A :s0 2So0 282< X4= maxa0 2A : 03239=ffi (s0 ; a0 ; o0 )5;P (s0 ; ojs; a)(s0 ; a0 )5 + 4X Xo0 2 s0 2S2S39=P (s0 ; ojs; a)ffi (s0 ; a0 ; o0 )5;equations define MDP state space , action space discountfactor . Thus, solution fast informed bound update found solvingequivalent finite-state MDP. 2Theorem 9 Let Vbi corresponds piecewise linear convex value function definedlinear functions. H Vbi HF IB Vbi HQMDP Vbi HMDP Vbi :Proof8<XX9=XXmax : (s; a)b(s) +maxP (s0 ; ojs; a)b(s)ffi (s0 );a2A s2Sff20o22S s2S= (HVi )(b)maxa2AXs2S2b(s) 4(s; a) += (HF IB Vi )(b)maxa2AXs2Sb(s) 4(s; a) +s2Sa2A2= (HMDP Vbi )(b)3P (s0 ; ojs; a)ffi (s0 )53Xs0 2S2b(s) max 4(s; a) +maxXo2 ffi 2 s0 2S2= (HQMDP Vbi )(b)XXP (s0 js; a) max ffi (s0 )5ffi 23Xs0 2SP (s0 js; a) max ffi (s0 )5ffi 2A.4 Fixed-Strategy ApproximationsTheorem 10 Let CF SM FSM controller. Let CDR CLA directone-step-lookahead controllers constructed based CF SM . V C (b) V C (b)V C (b) V C (b) hold belief states b 2 .Proof value function FSM controller CF SM satisfies:F SMF SMLAVCF SM(b) = max V (x; b) = V ( (b); b)x2MV (x; b) = (b; (x)) +Xo2P (ojb; (x))V ((x; o); (b; (x); o)):88DRfiValue-Function Approximations POMDPsdirect controller CDR selects action greedily every step, is, alwayschooses according (b) = arg maxx2M V (x; b). lookahead controller CLA selectsaction based V (x; b) one step away:LA (b) = arg maxa2A"#X0(b; a) + P (ojb; a) max0 2M V (x ; (b; a; o)) :xo2expanding value function CF SM one step get:VCF SM(b) = max V (x; b)x2M"#X= max (b; (x)) + P (ojb; (x))V ((x; o); (b; (x); o))x2Mo2= (b; ( (b))) +(b; ( (b))) +"Xo2Xo2(1)P (ojb; ( (b)))V ((x; o); (b; ( (b)); o))P (ojb; ( (b))) maxV (x0 ; (b; ( (b)); o))0x 2MX(2)#0max(b; a) + P (ojb; a) max0 2M V (x ; (b; a; o))a2Axo2XLA0LA= (b; (b)) + P (ojb; LA (b)) max0 2M V (x ; (b; (b); o))xo2(3)Iteratively expanding maxx0 2M V (x; :) 2 3 expression 1 substituing improved(higher value) expressions 2 3 back obtain value functions directlookahead controllers. (Expansions 2 lead value direct controllerexpansions 3 value lookahead controller.) Thus V CVCCCVV must hold. Note, however, action choices (b) LA(b)expressions 2 3 different leading different next step belief statessubsequently different expansion sequences. Therefore, result implyV DR (b) V LA (b) b 2 . 2F SMF SMDRLAA.5 Grid-Based Linear-Function MethodTheorem 14 Let Vbi value function obtained via incremental linear function method,starting Vb0 , corresponds fixed strategy C0 . Let CLA;i CDR;i twocontrollers based Vbi : lookahead controller direct action controller, V C ,VCrespective value functions. Vbi V CVbi V Chold.Proof initializing method value function FSM controller C0 ,incremental updates interpreted additions new states FSM controller (anew linear function corresponds new state FSM). Let Ci controllerstep i. V C= Vbi holds inequalities follow Theorem 10. 2LA;iDR;iLA;iF SM;i89DR;ifiHauskrechtReferencesAstrom, K. J. (1965). Optimal control Markov decision processes incomplete stateestimation. Journal Mathematical Analysis Applications, 10, 174{205.Baird, L. C. (1995). Residual algorithms: Reinforcement learning function approximation. Proceedings Twelfth International Conference Machine Learning,pp. 30{37.Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamicprogramming. Artificial Intelligence, 72, 81{138.Bellman, R. E. (1957). Dynamic programming. Princeton University Press, Princeton, NJ.Bertsekas, D. P. (1994). counter-example temporal differences learning. Neural Computation, 7, 270{279.Bertsekas, D. P. (1995). Dynamic programming optimal control. Athena Scientific.Bonet, B., & Geffner, H. (1998). Learning sorting classification POMDPs.Proceedings Fifteenth International Conference Machine Learning.Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Artificial Intelligence, 11, 1{94.Boutilier, C., & Poole, D. (1996). Exploiting structure policy construction. ProceedingsThirteenth National Conference Artificial Intelligence, pp. 1168{1175.Boyan, J. A., & Moore, A. A. (1995). Generalization reinforcement learning: safelyapproximating value function. Advances Neural Information ProcessingSystems 7. MIT Press.Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.Proceedings Fourteenth Conference Uncertainty Artificial Intelligence, pp.33{42.Boyen, X., & Koller, D. (1999). Exploiting architecture dynamic systems. Proceedings Sixteenth National Conference Artificial Intelligence, pp. 313{320.Brafman, R. I. (1997). heuristic variable grid solution method POMDPs. Proceedings Fourteenth National Conference Artificial Intelligence, pp. 727{233.Burago, D., Rougemont, M. D., & Slissenko, A. (1996). complexity partiallyobserved Markov decision processes. Theoretical Computer Science, 157, 161{183.Cassandra, A. R. (1998). Exact approximate algorithms partially observable Markovdecision processes. Ph.D. thesis, Brown University.Cassandra, A. R., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple,fast, exact algorithm partially observable Markov decision processes. ProceedingsThirteenth Conference Uncertainty Artificial Intelligence, pp. 54{61.90fiValue-Function Approximations POMDPsCasta~non, D. (1997). Approximate dynamic programming sensor management.Proceedings Conference Decision Control.Cheng, H.-T. (1988). Algorithms partially observable Markov decision processes. Ph.D.thesis, University British Columbia.Condon, A. (1992). complexity stochastic games. Information Computation,96, 203{224.Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.Computational Intelligence, 5, 142{150.Dearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning. Artificial Intelligence, 89, 219{283.Doucet, A. (1998). sequential simulation-based methods Bayesian filtering. Tech.rep. CUED/F-INFENG/TR 310, Department Engineering, Cambridge University.Drake, A. (1962). Observation Markov process noisy channel. Ph.D. thesis,Massachusetts Institute Technology.Draper, D., Hanks, S., & Weld, D. (1994). Probabilistic planning information gatheringcontingent execution. Proceedings Second International ConferenceAI Planning Systems, pp. 31{36.Eagle, J. N. (1984). optimal search moving target search path constrained.Operations Research, 32, 1107{1115.Eaves, B. (1984). course triangulations soving differential equations deformations. Springer-Verlag, Berlin.Gordon, G. J. (1995). Stable function approximation dynamic programming. Proceedings Twelfth International Conference Machine Learning.Hansen, E. (1998a). improved policy iteration algorithm partially observable MDPs.Advances Neural Information Processing Systems 10. MIT Press.Hansen, E. (1998b). Solving POMDPs searching policy space. ProceedingsFourteenth Conference Uncertainty Artificial Intelligence, pp. 211{219.Hauskrecht, M. (1997). Planning control stochastic domains imperfect information. Ph.D. thesis, Massachusetts Institute Technology.Hauskrecht, M., & Fraser, H. (1998). Planning medical therapy using partially observableMarkov decision processes. Proceedings Ninth International WorkshopPrinciples Diagnosis (DX-98), pp. 182{189.Hauskrecht, M., & Fraser, H. (2000). Planning treatment ischemic heart diseasepartially observable Markov decision processes. Artificial Intelligence Medicine, 18,221{244.91fiHauskrechtHeyman, D., & Sobel, M. (1984). Stochastic methods operations research: stochasticoptimization. McGraw-Hill.Howard, R. A. (1960). Dynamic Programming Markov Processes. MIT Press, Cambridge.Howard, R. A., & Matheson, J. (1984). uence diagrams. Principles ApplicationsDecision Analysis, 2.Isard, M., & Blake, A. (1996). Contour tracking stochastic propagation conditionaldensity. Proccedings Europian Conference Computer Vision, pp. 343{356.Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1999). Planning actingpartially observable stochastic domains. Artificial Intelligence, 101, 99{134.Kanazawa, K., Koller, D., & Russell, S. J. (1995). Stochastic simulation algorithmsdynamic probabilistic networks. Proceedings Eleventh Conference Uncertainty Artificial Intelligence, pp. 346{351.Kearns, M., Mansour, Y., & Ng, A. Y. (1999). sparse sampling algorithm nearoptimal planning large Markov decision processes. Proceedings SixteenthInternational Joint Conference Artificial Intelligence, pp. 1324{1331.Kjaerulff, U. (1992). computational scheme reasoning dynamic probabilistic networks. Proceedings Eighth Conference Uncertainty Artificial Intelligence, pp. 121{129.Korf, R. (1985). Depth-first iterative deepening: optimal admissible tree search. ArtificialIntelligence, 27, 97{109.Kushmerick, N., Hanks, S., & Weld, D. (1995). algorithm probabilistic planning.Artificial Intelligence, 76, 239{286.Lauritzen, S. L. (1996). Graphical models. Clarendon Press.Littman, M. L. (1994). Memoryless policies: Theoretical limitations practical results.Cliff, D., Husbands, P., Meyer, J., & Wilson, S. (Eds.), Animals Animats 3: Proceedings Third International Conference Simulation AdaptiveBehavior. MIT Press, Cambridge.Littman, M. L. (1996). Algorithms sequential decision making. Ph.D. thesis, BrownUniversity.Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995). Learning policies partiallyobservable environments: scaling up. Proceedings Twelfth InternationalConference Machine Learning, pp. 362{370.Lovejoy, W. S. (1991a). Computationally feasible bounds partially observed Markovdecision processes. Operations Research, 39, 192{175.92fiValue-Function Approximations POMDPsLovejoy, W. S. (1991b). survey algorithmic methods partially observed Markovdecision processes. Annals Operations Research, 28, 47{66.Lovejoy, W. S. (1993). Suboptimal policies bounds parameter adaptive decisionprocesses. Operations Research, 41, 583{599.Lusena, C., Goldsmith, J., & Mundhenk, M. (1998). Nonapproximability results Markovdecision processes. Tech. rep., University Kentucky.Madani, O., Hanks, S., & Condon, A. (1999). undecidability probabilistic planninginfinite-horizon partially observable Markov decision processes. ProceedingsSixteenth National Conference Artificial Intelligence.McAllester, D., & Singh, S. P. (1999). Approximate planning factored POMDPs usingbelief state simplification. Proceedings Fifteenth Conference UncertaintyArtificial Intelligence, pp. 409{416.McCallum, R. (1995). Instance-based utile distinctions reinforcement learninghidden state. Proceedings Twelfth International Conference MachineLearning.Monahan, G. E. (1982). survey partially observable Markov decision processes: theory,models, algorithms. Management Science, 28, 1{16.Mundhenk, M., Goldsmith, J., Lusena, C., & Allender, E. (1997). Encyclopaedia complexity results finite-horizon Markov decision process problems. Tech. rep., CSDept TR 273-97, University Kentucky.Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). complexity Markov decision processes. Mathematics Operations Research, 12, 441{450.Parr, R., & Russell, S. (1995). Approximating optimal policies partially observablestochastic domains. Proceedings Fourteenth International Joint ConferenceArtificial Intelligence, pp. 1088{1094.Pearl, J. (1988). Probabilistic reasoning intelligent systems. Morgan Kaufman.Platzman, L. K. (1977). Finite memory estimation control finite probabilistic systems.Ph.D. thesis, Massachusetts Institute Technology.Platzman, L. K. (1980). feasible computational approach infinite-horizon partiallyobserved Markov decision problems. Tech. rep., Georgia Institute Technology.Puterman, M. L. (1994). Markov decision processes: discrete stochastic dynamic programming. John Wiley, New York.Raiffa, H. (1970). Decision analysis. Introductory lectures choices uncertainty.Addison-Wesley.Rumelhart, D., Hinton, G. E., & Williams, R. J. (1986). Learning internal representationserror propagation. Parallel Distributed Processing, pp. 318{362.93fiHauskrechtSatia, J., & Lave, R. (1973). Markovian decision processes probabilistic observationstates. Management Science, 20, 1{13.Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Learning without state-estimationpartially observable Markovian decision processes. Proceedings EleventhInternational Conference Machine Learning, pp. 284{292.Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observableprocesses finite horizon. Operations Research, 21, 1071{1088.Sondik, E. J. (1971). optimal control partially observable Markov decision processes.Ph.D. thesis, Stanford University.Sondik, E. J. (1978). optimal control partially observable processes infinitehorizon: Discounted costs. Operations Research, 26, 282{304.Tatman, J., & Schachter, R. D. (1990). Dynamic programming uence diagrams.IEEE Transactions Systems, Man Cybernetics, 20, 365{379.Tsitsiklis, J. N., & Roy, B. V. (1996). Feature-based methods large-scale dynamicprogramming. Machine Learning, 22, 59{94.Washington, R. (1996). Incremental Markov model planning. Proceedings EightIEEE International Conference Tools Artificial Intelligence, pp. 41{47.White, C. C., & Scherer, W. T. (1994). Finite memory suboptimal design partiallyobserved Markov decision processes. Operations Research, 42, 439{455.Williams, R. J., & Baird, L. C. (1994). Tight performance bounds greedy policies basedimperfect value functions. Proceedings Tenth Yale Workshop AdaptiveLearning Systems Yale University.Yost, K. A. (1998). Solution large-scale allocation problems partially observableoutcomes. Ph.D. thesis, Naval Postgraduate School, Monterey, CA.Zhang, N. L., & Lee, S. S. (1998). Planning partially observable Markov decisionprocesses: Advances exact solution method. Proceedings Fourteenth Conference Uncertainty Artificial Intelligence, pp. 523{530.Zhang, N. L., & Liu, W. (1997a). model approximation scheme planning partiallyobservable stochastic domains. Journal Artificial Intelligence Research, 7, 199{230.Zhang, N. L., & Liu, W. (1997b). Region-based approximations planning stochasticdomains. Proceedings Thirteenth Conference Uncertainty ArtificialIntelligence, pp. 472{480.94fi