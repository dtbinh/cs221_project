
ff fi


"!$#%'&)(***,+-#.0/1((2

3,456782,9//:;4
!7<#*9*0*

=?>A@B@DCFEHGJILK$MONBPRQSTK$UVIWGYX[Z\XGYPHPHQP^]`_aYUcbedHP^f6ghUiaePHQj<K$Mlkm]YKJPonVI
QPlpqarPsC-@mK<nVK$UutvQPHQInuQf?@maetwGYQPhI

xzy8{W|~}VW|{V|{

F-- o-<8

}AW{VyW|}us|-L

'<Wo-<8

VF $o, 'V)'0)[L),)
),, - ,"

[' W
60s u 0 [ euh 6^)
0
0 eH ) FF0 ) " 0s e ) F )
0s < u[<~<)s )
h8< " ' h e 0s $"V ss
sz - 00[ ~~ ^ "
W ) < e huoo )^H ^
6 e^ " e 0i ^
"R 'u
-
$~ -- "F^ ) <su$i " e
VF0
hH 0 R ) H h )F

" 0s -e 6 ^ ,
6 J 0o e
L) '0' ' s8V< $s
\ fi
h
F

ffsT "F[[
0- F

6 -
) e0m


V ) s,
"' $ 0 H 0 H ^
)
" VuH Hs e )
FV)V ^"'u e 08 ) u ) iYsu ^
6 0sHH J 0^eo LT
-
s$ ^ < 6 V6o $ sV0
V6H s0 s0'

'6
eFYH ^fi ffs006

'
" !$#%#'&)(*!+-,."!//.&"/.01&2/4356378#9;:.7<!=.9%5?>@!$9%&6(A047*/.7*B%!$9%&65?/C5$DE!$(89%&65?/.#9%5F9;B%!37*Bfi#%7G!H#%9;!$9'7A#;,!+(87<9%5
!$(8:.&67837F#;,78(8&JI(C05?!$#'9;!$9%78#*KMLE!BN&)5.=.#<!$605?Bfi&69;:>F#<:!37FO787*/P783478)5.,78PQ9%51!$PPBfi78#'#G9;:.7F#%9;!$9%7SR
!$(89%&65?/TBfi7*,Bfi78#%7*/49;!$9'&)5./U!/.PT9;:.7<#%7*!4Bfi(V:WDX5.B !$(89%&65?/.#*K YZB%!$P&69%&65?/!+)6[T9;:.78#%7!$605?Bfi&69;:>\#A:!37]O^787*/
(8"!$#%#%&JI78P!$(8(85?BfiP&"/.0F9%5<9S:.78&2B_#%7*!Bfi(8:#S,!$(87`BN7*,Bfi78#%7*/49;!$9%&65?/C!$#E78&69;:.7*B_#%9;!$9%7SR#S,!$(87`,."!//.7*BN#]ab7Kc0Ked
fghZijbkl L 7865#%5m7891!$nK"dGo*pp4qrF5?BC,."!/4R#;,!$(87Q,."!//.7*Bfi#mab7Kc0K"d st fhZf dAuZ7*/.O7*Bfi9;:4[wvyxz78)Pd
o*pp4{r8K
|}/.78~Bfi78#%7*!BN(V:@9;Bfi7*/.P1:!$#AO^787*/C9%5FP7837865?,1/.78~7*/.(85.P&2/.04#A5$D ,.2!4//.&2/.01,Bfi5?O.67*>F#&2/C5.BfiP7*B
9%5!$P5?,.97S(8&67*/9A!+)045?Bfi&69;:>F# DBfi5?>59S:.7*B Bfi78#%7*!BN(V:C!Bfi7*!$#d?67*!$P&"/.0C9%5#%&60?/.&JI(*!/49 P7837865?,>\7*/9%#
&"/,.2!4//.&2/.0w!+)045?Bfi&69;:>F#*d !+##;=BN378[78PO4[mxQ786Pafio*pp4pr8KwY :.&6#1(8"!$#%#C5$D<,."!//.&"/.0!$605?BN&)9S:>F#
&"/.(8"=.P78# k-g
-ff$- an"=>v=BN#%9*dEoppr9;:!$9F=.#%78#]!W5~_R0?B%!,:7*/.(85.P&"/.09%5U(85?/.#'9;B%!$&"/
9;:.7H#%7*!Bfi(8:!/.P f+- aG!=.9%]v.78">!/d oppr 9S:!$9A7*/.(85.P78#G9;:.7F,.2!4//.&2/.0,Bfi5?O.67*>y!$#G!
#;!$9'&)#NI!O.&6)&69fi[,BN5?O.67*>!/.P=.#'78# D!+#%9A>F5.P78#;!$9'&)#ND!$(89'&)5./1!$605?Bfi&69;:>F#9%5FI/.P!H#%542=.9'&)5./K
e (**0*8fi-fi7 V 7$0 W ,ff56 4
!fi

!,Fb7$

fi

0 J

78(87*/9%6[d!/.59S:.7*B /.78~w,.2!4//.7*BM f &">!$9%9'&?789E!$nK"doppr~ !$# &"/9;BN5?P=.(878P9S:!$9 7*/.(85?P78# !
,."!//.&"/.0P5?>!+&2/@!$#!H/.5?/4RP789%7*B%>F&"/.&6#%9%&6(MI/.&69%7G!=.9%5?>@!$9%5?/1an|`r BN7*,Bfi78#%7*/49%78PWO4[!/GBfiP7*BN78P
&"/!Bfi[QG78(8&6#%&65?/G&"!$0?B'!>a h ii dE Bfi[?!/49*d o*p4r8K1/z(85?/49;B%!$#%9H9%5U9S:.7W,Bfi783.&65?=.#!$605?Bfi&69;:>\#*d
f 7S78(89%&63786[U78.9%7*/.P#H9%5W/.5./4RP789%7*B%>F&"/.&6#%9%&6(CP5?>!$&"/.#],BN5?P=.(8&"/.0=/.&637*BN#;!$E,."!/.#]!+#`Bfi5.O=.#%9
#%5"=.9%&65?/.#K <=.7 9%5A9S:.7 #%(*!$"!O.&66&69[F5$D9;:.7 =/.P7*Bfi6[?&"/.0>\5?P78(8:.78(V4&"/.0]Bfi7*,BN78#%7*/9S!$9%&65?/F!/.PF#%7*!Bfi(8:
9%78(8:/.&6=.78#*d^&)9(*!/O^7G#;:.5$~A/@9%5O7<!H37*Bfi[7SC(8&)7*/49A/.5?/4RP789'7*B%>F&"/.&6#%9%&6(],."!//.7*B<a &">!$9%9%&
789 !$nK"d
o*pp4!doppOr8K
G/.75$D5?=B>!$&"/Bfi78#'7*!Bfi(8:F5?O.fi78(89%&6378# &6# 9%5GP7837865?,,."!//.&"/.0#%[.#%9%7*>\# #;=.&69;!O.67 Dn5?B ,."!//.&"/.0
&"/=/.(87*Bfi9;!+&2/d.#%&"/.067d.5?B >F=.69%&JR!$07*/49E7*/43?&"Bfi5./>F7*/9'#AanA!+&)0.:vL 7865#%5d
o*pp^$L 78)54#%5<789E!$nK"dopp
.9%5?/.7vL 7865#'5dEo*p4pr8K<Y:.7\=/.&637*BN#;!$,."!//.&"/.0Q!,,Bfi5.!$(V:d!+#G5?Bfi&60&"/!$66[QP7837865?,78Pan.(8:.5?,4R
,7*BN#*d o*p4r8d&6#<!,,^7*!$6&"/.0UDn5?B9;:.&6#<9fi[,7F5$DE7*/43?&"Bfi5?/>\7*/9%#K<|=/.&)347*Bfi#;!$E,."!/T&6#]!@#%789G5$D #%9;!$9%7SR
!$(89%&65?/B%=.678#H9;:!$9G!$&">y!$9G(85$37*Bfi&"/.019;:.7,5#'#%&"O.)71>F=.69%&",.)7@#%&69;=!$9%&65?/.#H&2/9;:.7/.5?/4RP789%7*B%>F&"/.&6#%9%&6(
7*/43?&"Bfi5?/>\7*/9*K|=/.&637*Bfi#;!+-,."!/1&6#M78?78(*=.9%78PQO4[&"/49%7*Bfi67*!3?&"/.019;:.7H#%78678(89%&65?/15+D!/T!$(89%&65?/&2/9;:.7
,."!/w!4/.Pw5?O.#%7*BN3?&"/.09;:.7Bfi78#S=.)9'&2/.07S78(89%#&"/9;:.71~ 5?Bfi6PKA/.&637*Bfi#S!$A,."!//.&"/.0Bfi78#'7*>]O.678#C9;:.7
5?=.9%(85.>F7<5$DBN78&2/4Dn5?Bfi(87*>\7*/9G67*!B%/.&"/.0an=.9'9%5?/Uv !Bfi9'5do*pp4r8d&"/T9;:!$9M9;:.7<#'9;!$9%7SR!$(89'&)5./U>F5.P78
(*!,.9;=BN78#-9;:.7 =/.(87*Bfi9;!$&"/49fi[A5$D9S:.7E~ 5?Bfi6PK-M/.&637*Bfi#;!+,."!//.&"/.0G&6#!M,Bfi78(*=Bfi#'5?B!,,Bfi5.!$(V:d # ~A:.7*Bfi7 !$6
,."!//.&"/.0&6# P5?/.7G,Bfi&65?B9%5<78.78(*=.9%&65?/dO=.&66P&"/.0W=,^5?/@9;:.7`!+#%#;=>,.9%&65?/@9;:!$9 !F/.5?/4RP789%7*B%>F&"/.&6#%9%&6(
>F5.P785$D-9S:.7G78?78(*=.9%&65?/7*/3.&"Bfi5?/>F7*/49 (*!/1O^7]!$(8=.&"Bfi78Pd!/.P167*!$P&"/.0C9;:.7*Bfi7SDn5?Bfi7G9%5!F#'5?=/.PU!/.P
(85?>,.6789%7F,."!//.&"/.0U!,,Bfi5?!$(8:K
5$~ 7837*B8d=/.&637*Bfi#;!$ ,."!//.&"/.0:!$#MO787*/T(*Bfi&69%&6(8&6878Pab7Kc0KedG&"/.#;O^7*Bfi0dZo*pprd?P=.7F9%5C!,59'7*/4R
9%&"!$78,5./.7*/9%&"!$Z0?Bfi5$~ 9S:5$DZ9;:.7]=/.&637*Bfi#S!$-,."!/1#'&)87H~ &69;:19S:.7</?=>FO7*B5$D,BN5?,5#'&)9'&)5./.#AP7SI/.&"/.0
!CP5?>!+&2/#%9;!$9%7K|`/T&">,5.Bfi9;!/49G(85?/49;Bfi&"O=.9%&65?/5$D f &6#G9;:.=.#<9;:.7=.#%7F5$D h ii #<9'5UBfi7*,Bfi78#%7*/49
=/.&637*Bfi#;!+,."!/.#*KE/F9;:.7 ~ 5?BN#%9 (*!$#%7d49;:.&6#Bfi7*,Bfi78#'7*/9;!+9%&65?/>![]!+)#'5<0?Bfi5$~m78,5?/.7*/49%&"!$66[~&)9S:F9;:.7
/.=>]O^7*BF5$D P5?>!+&2/m,BN5?,5#'&)9'&)5./.#*dO=.9O^78(*!=.#%7 h ii #C!Bfi7C37*Bfi[Q(85?>@,!$(89Bfi7*,Bfi78#%7*/49;!$9%&65?/.#F5$D
O5.567*!/D=/.(89%&65?/.#*d
9;:.&6#A&6#M5$DX9'7*/1/.59A9S:.7G(*!$#%7GDn5?B P5.>!$&"/.#A~&)9S:U!Bfi780.=.2!4B #%9SB%=.(89;=Bfi7an_&2>@!$9%9%&
789<!$nK"dZo*pp4!r8KAY:.7*Bfi7SDX5.Bfi7d h ii RO!$#'78P,."!//.&"/.0U#%787*>F#G9'5WO^7\!1,BN5?>F&6#%&"/.0U!,,Bfi5?!+(V:T9%51=/.&JR
37*Bfi#S!$,.2!4//.&2/.0
K
f #;,^78(8&I78#C!U,."!//.&"/.0QP5?>!+&2/&2/z9;:.7!+(89%&65?/P78#%(*Bfi&",.9%&65?/"!/.0.=!$07HaG&"=/.(V:.&606&"!
789!$nK"dMo*pp4rF!/.Pm9;B%!/.#'2!+9%78#&699%5!(85?B%Bfi78#;,^5?/.P&"/.0|Fd:.7*/.(87T6&2>\&)9'78P9'5,."!//.&"/.0w,Bfi5?O4R
67*>F#F~ &69;:I/.&)9'71#%9;!$9%7@#;,!$(878#*KUY:.79;B'!/.#%&69%&65?/Bfi78"!$9%&65?/z5$DA9;:.7!4=.9%5?>!$9'5?/&6#F7*/.(85?P78Pw!$#F!/
h ii 9;:!$9E!+)65$~ #DX5?BZ9S:.7A=.#%7 5$D7S(8&67*/9 OBfi7*!+P9;:4RnIBfi#%9#%7*!Bfi(8:H9%78(V:/.&6=.78# P7837865?,78PCDn5?B >F5.P78
(8:.78(V4&"/.0ab(8&66"!/doppr8K f &2/.(8"=.P78# 9fi~ 5A!$605?Bfi&69;:>\#Dn5?BZ=/.&)347*Bfi#;!$,."!//.&"/.0KEY :.7M8n4
6
bG!$605?Bfi&69;:>9SBfi&678# 9%507*/.7*B'!$9%7G!,."!/19;:!+9&6# 0.=!B%!/49%7878P19%5!$(8:.&67837<9;:.7G045?!$Dn5?B !+)5$D
9;:.7,^5#%#%&"O.6715?=.9%(85.>F78#<5$D 9;:.7C/.5?/4RP789%7*B%>F&"/.&6#%9%&6(!$(89%&65?/.#*K1bDA/.5T#;=.(V:z#%9;BN5?/.01#%5"=.9%&65?/z78?&6#%9'#*d
9;:.7!$605?BN&)9S:>D!$&66#*K<Y:.78nfiT*$ b 6
bF!$605?Bfi&69;:>yBfi789S=B%/.#`!C#'9;Bfi5?/.0C#%5"=.9%&65?/d&D 5?/.7
78.&)#'9%#*dZ5?BG549;:.7*Bfi~ &6#%79SBfi&678#]9'5U07*/.7*B%!+9%7!U,."!/z9;:!$9F>![U(85?/49;!$&"/z65?5?,.#FO=.9F&)#F0?=!4B%!/49%7878PQ9%5
!$(8:.&678379S:.705.!$nd-04&)347*/Q9;:!+9]!$6E(8[.(86&6(178?78(*=.9%&65?/.#@7837*/9S=!$6)[9%7*B%>\&2/!+9%7KUbDA/.5#;=.(8:Q#'9;Bfi5?/.0
(8[.(8)&6(F#%5"=.9%&65?/78?&6#%9'#*d9;:.7G#%9SBfi5?/.0(8[.(86&)(,."!//.&"/.0U!$605?BN&)9S:>D!+&)6#*K
/m9;:.&6#1!Bfi9%&6(867U~ 7U,Bfi78#'7*/9C5?=B h ii RO!$#%78P,."!//.&"/.0m#%[?#'9%7*>1d hZf aA/.&637*Bfi#S!$ Q=.69%&JR
!$07*/49U< ii RO!$#'78Pu"!//.7*B%r8d 9;:!+9\=.#'78#W!Q/.78~ h ii RO!$#%78P7*/.(85.P&"/.0d 07*/.7*B'!$9%78#=/.&637*Bfi#S!$
,."!/.#G&"/>]=.69%&JR!$047*/9G/.5?/4RP789%7*B%>\&2/.&6#%9'&)(@P5?>!$&"/.#*d!/.PU&"/.(8"=.P78#!/.78~y%5?,.9'&2>\&)#'9%&6(8W,."!//.&"/.0
!$605?BN&)9S:>1K
nm?b'*nfi8'n'cSncF-fi'\fiZ%"
nS8;cF'*bSn n]b%b%$;Sn$bSZ*'\n
fi;)S8fi b'
"'cff *fifi
fi

fi 0



)-
- -)

G=B5$37*B%!$6E!,,Bfi5.!$(V:DX5?BMP78#'&)0./.&2/.0!/ h ii RO!+#%78Pw,."!//.7*BH&)#H#%&">F&6"!BF9%519;:.7C!,,Bfi5?!$(8:
P7837865?,^78PFDX5.BA f K G=B >!$&"/G(85?/49;Bfi&"O=.9%&65?/H&6#E!/7S(8&67*/9Z7*/.(85?P&"/.0H5$D! /.78~DBfi5?/497*/.PFP5?>!$&"/
P78#%(*Bfi&",.9%&65?/C"!/.0.=!$07d"!#an5?/4RP789%7*B'>F&"/.&)#'9%&6(G|A07*/49EG5.>!$&"/%$!/.0?=!$07*rK&"!'#m:!$#>\5?Bfi7
Bfi78#%7*>FO."!/.(87T~ &69;:w,Bfi783.&65?=.#1,.2!4//.&2/.0"!/.0?=!$0478#9;:!4/9;:.7U!$(89%&65?/P78#%(*BN&2,.9'&)5./w"!/.0.=!$071<
(*=B%Bfi7*/49%6[m=.#%78PwO[A f K9:!$#,5$~ 7*BD=.!$(89'&)5./P78#%(*Bfi&",.9%&65?/.#C9;:!$9F(*!/,7*BDn5?B%> !Bfi&69;:>\789%&6(
5?,^7*B%!$9%&65?/.#M5?/1/.=>F7*Bfi&6(*!$ZP5?>!$&"/13$!Bfi&"!O.678#*K G5?>@!$&"/.#A(85?>@,Bfi&6#%78PU5$D #%[/.(V:BN5?/.&6878P!+07*/9'# (*!/
O7G>F5.P786)78PUO4[F&"/9;BN5?P=.(8&"/.0F(85?/.(*=B%Bfi7*/49!+(89%&65?/.#O!$#'78P5?/@!<>F=.69%&JR!$07*/49EP78(85?>,^5#%&69%&65?/@5$D9;:.7
P5?>!+&2/K
/Q!$PP&69%&65?/
"!#Q&"/9;BN5?P=.(878#!1#'7*,!B%!$9%7!/.Pz78,.6&6(8&697*/3.&"Bfi5?/>F7*/49>F5.P78 P7SI/.78Pw!$#<!
#%789H5$)
(.Snfi b6+ *8- , !$047*/9%#d&nKc7K"dE!+07*/9'#G~A:.5#%71!$(89%&65?/.#F(*!//.59FO7U!T,!Bfi9<5+D_9S:.707*/.7*B'!$9%78P
,."!/.
K /"!#:!+#AO787*/T(*!Bfi7SD=.66[1P78#%&60?/.78P9%5!$665$~DX5.B 7S(8&67*/9 h ii R7*/.(85.P&"/.0KAY:?=.#*d
hZf
(85?/49;Bfi&"O=.9%78#G!F,!Bfi9%&69%&65?/.78P9;B%!4/.#%&69%&65?/1Bfi78"!$9%&65?/TBfi7*,Bfi78#%7*/49;!$9%&65?/C5+D9;:.7<|}9;:!$9 &6#G?/.5$~A/@DBfi5?>
>F5.P78E(8:.78(V4&"/.09%5U#%(*!+)71=,z~786Gan =Bfi(V:789]!$nK"d o*pp^o !4/N!/z789]!+XKedoppqr8KG=BH7*>,.&"Bfi&6(*!$
78,7*Bfi&">F7*/49%#M#;=.0078#%99;:!$9 9;:.&6#A&6#G!$6#%59;:.7G(*!+#%7MDX5.B h-f K
0 h-f &2/.(8"=.P78#M9;:.7A,Bfi783.&65?=.#%6[CP7837865?,78PU!$605.Bfi&69;:>F# Dn5?B h ii RO!$#%78PU=/.&637*Bfi#;!+,."!//.&"/.0K
/m!+PP&)9'&)5./dE~ 7C&2/49;Bfi5.P=.(87U!/.78~ %5?,.9%&">F&6#%9%&6(8,."!//.&"/.0m!$605?Bfi&69;:> 9;:!+9\BN782!+?78#F5?,.9%&">!+)&69fi[
0?=!B'!/9'7878#A!/.P07*/.7*B%!$9'78#`,."!=.#%&"O.671=/.&)347*Bfi#;!$ ,."!/.#G&"/UP5.>!$&"/.#G~A:.7*Bfi7F/.51#%9;Bfi5?/.0C/.5?B #'9;Bfi5?/.0
(8[.(8)&6(F#%5"=.9%&65?/78?&6#%9'#*K
Y:.7<!Bfi9%&6(867G&6# 5?Bfi0?!4/.&)878PU!+# DX54)65$~ #*K .78(89%&65?/T{HP&)#'(*=.#%#%78#G,Bfi783?&65?=.#G!,,Bfi5.!$(V:.78# 9%5,."!//.&"/.0
&"/Q/.5?/4RP789%7*B'>F&"/.&)#'9%&6(P5?>@!$&"/.#*K].78(89'&)5./Q04&)3478#<!1OBfi&67SD_5$37*BN3?&678~ 5$D h ii #]!/.PQ|7*/.(85.P.R
&"/.0#*K n9A>@!*[1O7H#;4&2,,^78PQO[UBfi7*!+P7*Bfi#A!$"Bfi7*!$P[CD!>F&66&"!B~ &69;:19;:.7G#S=O.78(89K.78(89%&65?2
/ 1&"/9SBfi5?P=.(878#
"!'# #;:.5$~ #:.5$~9'57*/.(85.P7U!,."!//.&"/.0,Bfi5?O.67*>1d !/.PzDX5?B'>!$66[QP78#%(*Bfi&"O78#C9;:.7C#%[/9;!+!/.P
#%7*>!4/9%&6(8#5$D9;:.&6# P78#%(*Bfi&",.9%&65?/@"!/.0?=!$047 &"/9%7*B'>F# 5$D-!/|]K$xz7A!$6#%5<P&6#%(*=.#'#9S:.7A,Bfi5?,^7*Bfi9%&678#E5$D
9;:.7G"!/.0?=!+07<O!$#%78P15?/C!/C78!>,.67<!/.PU!BN0?=.7MDX5.BE5.=BEP78#'&)0./1(8:.5&6(878#*KE.78(89%&65?/Tq],BN78#%7*/9'# 9;:.7
"!#QP5?>!$&"/TP78#%(*Bfi&",.9%&65?/.#K].78(89%&65?/zP78#%(*BN&2O^78#<9;:.7FP&J7*Bfi7*/49<!$605$R
h ii Bfi7*,Bfi78#%7*/49;!$9%&65?/5$
Bfi&69;:>F# 9S:!$9:!37 O787*/=.#'78P<Dn5?B h ii RO!$#'78P,."!//.&"/.0\!4/.P<&"/9SBfi5?P=.(878#5?=B5?,.9%&">F&6#%9%&6(A,."!//.&"/.0
!$605?BN&)9S:>1K<.78(89%&65?/QC,Bfi78#%7*/49%#G7*>,.&"Bfi&6(*!$ Bfi78#;=.69%#G&"/#%78347*B%!$ ,.2!4//.&2/.0TP5?>!$&"/.#*dZB%!/.0&"/.0CDBfi5?>
#%&"/.067SR!$07*/49<!/.P1P789%7*B'>F&"/.&)#'9%&6(5?/.78#M9%5>F=.69%&JR!$07*/49`!/.P/.5?/4RP789%7*B%>\&2/.&6#%9'&)(@5?/.78#*K xQ7H78,^7*Bfi&JR
>F7*/49E~ &69;:C,Bfi783.&65?=.#%6[W=.#'78PP5?>@!$&"/.#_!4/.P&"/49;Bfi5.P=.(87G9~ 5</.78~5?/.78#*d./!>F786[1!<,5$~ 7*BE,."!/49 !/.P
!1#'5?(8(87*BP5?>!$&"/d !$#</.5?/4RP789%7*B'>F&"/.&)#'9%&6(d>F=.69%&JR!$07*/49],."!//.&"/.0,Bfi5.O.)7*>\#*K &"/!$66[dE.78(89%&65?/
PB%!~ # (85?/.(8"=.#%&65?/.#<!/.PTP&6#%(*=.#%#%78#MP&"Bfi78(89%&65?/.#MDn5?BD=.9;=BN7G~5.B%K
9
3-54768 6 :

V<;

87 (*=B%BN7*/9!4,,Bfi5?!$(8:.78#],^7*BDX5.B%>F&"/.0,."!//.&"/.0Q&"/9'7*Bfi67*!*3478Pm5?BH&2/,!B%!+)678 ~&)9S:Q78.78(*=.9%&65?/m:!37
O787*/@~ &6P786[W=.#'78P&"/C/.5?/4RP789%7*B%>F&"/.&6#%9%&6(FBfi5?O549%&6( P5?>!$&"/.#<ab7Kc0
K"dG785?BN07Sv=$!/.#;4[do*pH!$9*d
o*pp4{*x&6"&"/.# 789E!$nK"do*p4>p 1$A!$&60?:@vwL 7865#'5do*pp4r8K|0?Bfi5?=,5$D,."!//.7*Bfi##;=.&69;!O.67 Dn5?BZBfi78(*=B%Bfi7*/49
,."!//.&"/.0G&6#E!$(89%&65?/G#'78)78(89'5?Bfi# O!$#%78P<5./G:.7*=Bfi&6#%9%&6( #%7*!BN(V:anM5.7*/.&60<vw.&">>F5./.#*d?oppq$5?/.789-789Z!$nK"d
o*pp4r8K-Y:.7`>F&"/4R>!+ g4?&? !$605?Bfi&69;:>yanM5.7*/.&60v.&">>F5./.#*do*ppq>\&2B'/.53@789!$nK"do*pp4rZ(*!/
07*/.7*B%!+9%7#S=O5?,.9%&">!+A,."!/.#&"/m/.5?/4RP789'7*B%>F&"/.&6#%9%&6(P5?>!$&"/.#9;:BN5?=.0?:!#%7*!Bfi(8:!/.Pm78?78(*=.9'&)5./
&69%7*B%!$9'&)5./KAY :.7H#%7*!Bfi(8:1&6#GO!$#%78PU5./U!C:.7*=Bfi&6#%9%&6(F05?!$P&6#%9S!/.(87HD=/.(89%&65?/T9;:!+9A>F=.#%9AO7,Bfi5$3.&6P78P
Dn5?B!#;,^78(8&JI(,BN5?O.67*>1KwY :.7 f !$605.Bfi&69;:> an5?/.7897891!$nK"dMo*pp4r<=.#%78#1!#%&">F&6"!BC!,,Bfi5?!$(8:
!/.PD=Bfi9;:.7*BFP7SI/.78#1!Q:.7*=Bfi&6#%9'&)(D=/.(89'&)5./Dn5?A
B @CB)DFEHGI@R)&"$7mab &"$78#vy&6)#'#%5?/dGo*p^o*rF!$(89%&65?/
Bfi7*,Bfi78#'7*/9;!+9%&65?/.#*K</(85?/49;B%!$#%9M9%51>F&"/4R>!$ g4?J
? f P5.78#</.59<!$#%#S=>F7!1/.5?/4RP789%7*B%>F&"/.&6#%9%&6(
fi Kfi

fi

0 J

7*/43?&"Bfi5?/>\7*/9*d O=.9H&)#FBfi5?O=.#%9H9%5/.5./4RP789%7*B%>F&"/.&6#;> (*!=.#%78PmO[Q!$(89'&)5./Q,7*Bfi9S=B%O!$9%&65?/.#Uab&nKc7K"d9;:!$9
!/.59S:.7*B !$(89%&65?/C9;:!4/C9;:.7],."!//.78P!$(89%&65?/1&6# (8:.5#%7*/~ &69;:C#%5?>\7<,Bfi5?O!O.&66&69[r8K
/T07*/.7*B%!$nd-BN78(*=B%Bfi7*/49<!,,Bfi5?!$(8:.78#<!Bfi7H&"/.(85?>,.6789%71O78(*!=.#%7C!$(89%&"/.015?/!/T&"/.(85?>,.6789%71,.2!4/
(*!/C>!$7 9;:.7M05?!+=/!$(V:.&6783$!O.67K uEBfi78(*=Bfi#'5?BE!,,BN5?!$(8:.78#,7*BDX5?B'>!$6P78(8&6#%&65?/T>!4&"/.0],Bfi&65?B9%5
78.78(*=.9%&65?/U!/.P19S:?=.# >![O^7<!O.67<9%5F07*/.7*B%!+9%7A(85.>,.6789%7<,."!/.#`O4[9;!4&"/.0!$6-,^5#%#%&"O.67<7S78(89%# 5$D
!$(89%&65?/.#&2/49%5!+(8(85?=/9KE 5$~ 7837*B8d.9;:.78[1Bfi786[15?/1!F(85.>,.6789%7]>F5.P785$D 9;:.7G~ 5?Bfi6M
P Lc#M=/.(87*Bfi9;!$&"/49[K
Y:.7A,Bfi78(*=Bfi#%5.B!4,,Bfi5?!$(8:.78#E&"/.(8"=.P7<(85?/.P&69%&65?/!+Oa N 9'8&)5./.&789!+XKedopp{uZ7859 v>\&)9S:dopp{
6[?9;:.7v L 7865#%5dFo*pp4r8d ,Bfi5?O!4O.&)6&6#%9%&6(ab<B'=>>F5?/.Pv Bfi78#%&"/!dFo*pKp P^GG7*!/789U!$nK"d<oppq
6[?9;:.7do*pp4rA!/.P=/.&)347*Bfi#;!$E,."!//.&"/.0wan.(8:.5?,,7*Bfi#dEo*p4 &">!$9%9'&-789<!$nK"d opp!dZo*pp4OG!R
O!/.*!F789M!$nK"do*pp4r8K
5?B 78!>,.67d9;:.7Ft f ,!Bfi9'&2!+5?BfiP7*B8d.(85?/.P&69%&65?/!+,."!//.7*BM:!/.P678#</.5?/4R
P789%7*B%>\&2/.&6#;> O[Q(85./.#%9;B%=.(89%&"/.0Q!T(85?/.P&69%&65?/!$ ,."!/9;:!$9F!$(8(85?=/49%#HDX5.BG7*!$(8:,^5#%#%&"O.67U#'&)9S=!$9%&65?/
5?B(85?/49%&"/.07*/.(8[U9;:!+9A(85?=.6PQ!Bfi&6#%7UanuZ78549`v>F&69;:d o*pp{4r8K | 9M78.78(*=.9%&65?/T9%&">F7F&)9G&6#GP789%7*B'>F&"/.78P
~A:.&6(8:,!BN9G5$D 9;:.7,."!/z9%5178?78(*=.9'7WO4[Q,7*BDn5?B%>F&"/.0T#%7*/.#%&"/.0Q!$(89%&65?/.#<9S:!$9]!BN7]&"/.(8"=.P78Pw&"/z9;:.7
,."!/19'59%78#%9Dn5?B 9;:.7<!,,BN5?,Bfi&"!$9%7G(85?/.P&69%&65?/.#K
uBfi5?O!O.&66&6#%9%&6(],."!//.7*BN#A9;BN[9%5>!+?&">F&687<9S:.7<,Bfi5?O!O.&66&69[15+D05.!$#;!$9%&6#fiD!$(89%&65?/d.0&637*/(85?/.P&JR
9%&65?/!$Z!$(89%&65?/.#M~ &69;:U,Bfi5.O!O.&6)&6#%9'&)(@7S78(89'#*KE<B'=>>F5?/.PU!/.P Bfi78#%&"/!aNo*pKp Pr Bfi7*,BN78#%7*/9G,."!/.#G!$#
!@#%789 5$D.&69;=!$9'78P_5?/9SBfi5 =.678#an #Sr]ab<B'=>>F5?/.Pdoppr >@!,,.&"/.01#%&69;=!$9%&65?/.#M9%5!$(89'&)5./.#*K
:.7,."!//.&"/.0!+)045?Bfi&69;:>yO7804&2/.#O4[Q!$PP&"/.0 #<(85?B'Bfi78#;,5./.P&2/.09%519S:.7>F5#%9<,BN5?O!O.6778.7SR
(*=.9%&65?/,!$9;:9;:!$9A!+(V:.&678378#G9;:.7H05?!$nK 9A9;:.7*/(85?/49%&"/?=.78#<!$PP&"/.0U # Dn5?B)78#'#`,Bfi5.O!O.67\,!+9;:.#
!/.PU>![C7*/.PU~ &69;:1!\(85?>,.6789%7],."!/9;!4&"/.0!$6-,54#%#%&"O.67\,!+9;:.# &"/9'5!$(8(85?=/49*K
M/.&637*Bfi#;!+,.2!4/.#P&J7*BZDBfi5?> (85?/.P&69%&65?/!$!/.P,Bfi5?O!4O.&)6&6#%9%&6(F,.2!4/.#O4[]#S,78(8&JDX[.&"/.0!,,Bfi5.,Bfi&"!$9%7
!$(89%&65?/.#\DX5?BH7837*BN[,^5#%#%&"O.67U#%9S!$9%7C&2/ 9;:.71P5?>!+&2/Q
K $&"$71(85./.P&)9'&)5./!$`!4/.Pw,Bfi5?O!O.&66&6#%9%&6(Q,.2!4/.#*d
=/.&637*Bfi#;!+-,."!/.#`BN78=.&"Bfi7H9;:.7<~ 5?Bfi6P19%5O^7]!$(8(878#%#'&2O.67F&"/15?BNP7*B 9%578.78(*=.9%7<9S:.7<,.2!4/K
M/.&637*Bfi#;!+,."!//.&"/.0C~ !$# &2/49;Bfi5.P=.(878PO4[.(8:.5?,,7*BN#\afiopr~A:.5=.#%78P1P78(8&6#%&65?/19;BN7878# 9%5Bfi7*,4R
Bfi78#%7*/49<,."!/.#*K 78(87*/9F!,,Bfi5?!$(8:.78#M&2/.(8"=.P71G!O!/.*!C789<!$nKEafioppr !4/.P_&2>@!$9%9%&Z789`!+XKEaNo*pp!^d
o*pp4Or8KAG!4O!/.*!789<!+XK afio*p4pr Bfi7*,BN78#%7*/9'#`=/.&637*Bfi#S!$E,."!/.#<!$6#%5U!$#G!#'789A5$D.&69;=!+9%78P 5?/49;Bfi54
=.678#*K :.78&"BA!+)045?Bfi&69;:>&"/.(*Bfi7*>F7*/49;!$66[!+PP#A #M9%5!HI/!$-,."!/&"/U!H~ !*[@#%&">F&62!4BM9%5F<B%=>HR
>F5?/.PQ!/.P Bfi78#%&"/!afiopKp Pr8K Y:.7<05?!+-&6#<!\DX5?B'>]=."!@&"/9%7*>,^5?B%!$6504&)(@9;:!$9A>F=.#%9G:.56PQ5?/T!/[
3!+)&6P#%78=.7*/.(87<5$DE!$(89%&65?/.#K
9 $7*!B%/.&"/.0a $r`a=.9%9%5?/Cv !BN9%5doppr(*!/!+)#'5\O^7<Bfi780?!BNP78PW!$# =/.&637*Bfi#S!$
78&2/4Dn5?Bfi(87*>\7*/
,."!//.&"/.0K / $9S:.7A05.!$&6# Bfi7*,Bfi78#%7*/49%78PUO4[!<BN78~_!4BfiPD=/.(89%&65?/C&"/1!GQ!B'$5$3]G78(8&6#%&65?/1uBfi5.(878#%#
ab<uErM>F5.P78 5$D9S:.7]P5.>!$&"/K]/9;:.7,Bfi78(*=Bfi#'5?BM37*Bfi#%&65?/5$D $ d9;:.7F<u&)#<!+#%#;=>F78PQ9'5WO^7
./.5~M/Q!/.Pm!C(85?/9SBfi5E,54)&6(8[m>!$.&">F&68&2/.0z9;:.778,78(89'78PBfi78~ !BNPQ&6#<Dn5?=/.P,BN&)5.B<9'5178?78(*=.9%&65?/K
:.7,54)&6(8[(*!/78&)9S:.7*B<O7Bfi7*,BN78#%7*/9'78P78,.6&6(8&)9')[z&2/!@9;!O.67F5?B&2>@,.)&6(8&69%6[mO[U!\D=/.(89'&)5./wab7Kc0Ked
!/.7*=B%!$ /.789fi~ 5?B%.r8K78(*!=.#'7 $&6#!,Bfi5?O!O.&66&6#%9%&6(!4,,Bfi5?!$(8:d &69%#FP5?>!$&"/mBfi7*,BN78#%7*/9S!$9%&65?/z&)#
>F5?BN7G(85?>,.67819;:!/9;:.7GP5?>!$&"/TBfi7*,Bfi78#%7*/49;!$9%&65?/T=.#%78PO4[1!/.5?/4RP789'7*B%>F&"/.&6#%9%&6(,.2!4//.7*B8K :.=.#*d
~ 7`>@!*[F78,78(89A/.5./4RP789%7*B%>F&"/.&6#%9%&6(,."!//.7*Bfi# 9%5FO7<!O.67G9%5:!/.P67GP5?>@!$&"/.#~&)9S:1!<"!BN07*B#%9;!$9%7
#;,!$(87F9;:!4/ $ K- =.9 $>![U,Bfi5?P=.(87C,56&6(8&678#~ &69;:!1:.&60?:.7*BG=!$6&69[T9;:!/!1=/.&637*Bfi#;!+E,.2!4/
07*/.7*B%!+9%78PO4[!z/.5?/4R,Bfi5?O!O.&66&6#%9%&6(d</.5?/4RP789'7*B%>F&"/.&6#%9%&6(,."!//.7*B8K5?Bfi785$37*Bd &"/m9;:.7Bfi78(*=B%Bfi7*/49
37*Bfi#'&)5./d $z)7*!4B%/.#A9S:.7G~5.Bfi6PU>F5?P78P=Bfi&"/.078.78(*=.9%&65?/U!4/.P19;:?=.#P5?78#G/.59 Bfi78=.&2BN7]!F(85?>@,.)789'7
~ 5?Bfi6P>\5?P78 ,Bfi&65?B9%578.78(*=.9%&65?/KAY :.5.=.0?:d&"/T9;:.785?BN[&69G/.7878P#G&2/4I/.&69%7@78?78(*=.9%&65?/T78!>,.678#G9%5
(85?/437*Bfi07G9'5]9S:.7<5?,.9%&">!+=/.&637*BN#;!$ ,.2!4/K
|M6,Bfi783.&65?=.#A!,,BN5?!$(8:.78#E9%5F=/.&)347*Bfi#;!$,.2!4//.&2/.0
d78.(87*,.9 &">!$9%9%&789 !$nKafio*pp4!d?oppOr8d.=.#%7
!/78,.6&6(8&69]Bfi7*,Bfi78#'7*/9;!+9%&65?/C5$DE9;:.7F=/.&)347*Bfi#;!$ ,."!/ab7K 0K"d #;rK :?=.#d&"/19S:.7<07*/.7*B%!+(*!$#%7d!/
fi R

fi 0



)-
- -)

78,5?/.7*/49%&"!$#%&687H5$D-9;:.7<,."!/C&"/19S:.7`/.=>FO7*B 5$D ,Bfi5?,54#%&69%&65?/.#AP7SI/.&"/.01!HP5?>!$&"/C#%9;!+9%7`>F=.#%9 O^7
78,78(89%78Pd!$#A!BN0?=.78PUO[1G&"/.#;O^7*Bfi0Qafio*pp4r8K
Y:.7<(85?>,!+(89<!/.P&2>@,.)&6(8&69Bfi7*,Bfi78#%7*/49;!$9'&)5./15$D =/.&)347*Bfi#;!$ ,."!/.#<5?O.9S!$&"/.78P~&)9S: h ii #<P5.78#
/.59F/.78(878#%#;!Bfi&66[Q0?BN5~ 78,5./.7*/9%&"!$66[zDX5?BGBN780?=."!Bfi6[Q#%9;B'=.(89;=Bfi78PQP5?>@!$&"/.#\!+#G#;:.5$~A/QO4[_&2>@!$9%9%&
789<!$nK afio*pp4!r8K =BN9;:.7*B8d9;:.7 h ii RO!+#%78PBfi7*,Bfi78#'7*/9;!+9%&65?/T5$DE9;:.7 | 5+D!1/.5?/4RP789%7*B%>F&"/.&6#%9%&6(
P5?>!+&2/7*/!O.678#G9;:.7!,,.6&6(*!$9%&65?/T5$DE7S(8&67*/49A#%7*!4Bfi(V:T!$605?Bfi&69;:>\# DBfi5?>>\5?P78Z(V:.78(84&2/.01(*!4,!O.67
5$DE:!/.P6&"/.0137*Bfi[C2!4Bfi07G#%9;!+9%7A#S,!$(878#*K
'

UTVWXW'

|G/wGBfiP7*Bfi78P&"/!Bfi[mG78(8&6#%&65?/G&"!$0.B%!> Bfi[.!/9d o*p4rF&6#W!z(*!/.5?/.&6(*!$GBfi7*,Bfi78#%7*/49;!$9'&)5./m5$D<!
O5.567*!/WD=/.(89%&65?/~ &69;Z
: YT6&"/.7*!BM5?BfiP7*BN78PU!Bfi0?=>F7*/49%#\[ #] [ (^]_`_a_`] [/b?K
|G/ h ii &)#E!MBfi5.59%78PdP&"Bfi78(89%78P!$(8[.(8)&6( 0.B%!,:G~&)9S:<5./.7E5?B9fi~ 5 9%7*B'>F&"/!$/.5.P78# 5$D5?=.9NRP780?Bfi787
87*Bfi5H2!4O78678PQo 5?"
B Pd?!4/.P!G#%7895$D3!BN&2!4O.)7</.5.P78d
# c@5$D5?=.9fiRP780.Bfi787A9fi~ 5KY :.7M9fi~ 5G5?=.9%05&"/.0H78P078#
!Bfi7C0&637*/O[Q9S:.7CD=/.(89%&65?/.f
# e?JK eOa crF!/.P K g<Oa c
r8h
K N!$(8:3!Bfi&"!O.67/.5?P71&6#!$#'#%5?(8&"!$9'78Pm~ &69;:m!
,Bfi5?,^5#%&69%&65?/!$ 3$!Bfi&"!O.67&"/9;:.7O^5?567*!/D=/.(89%&65?/9;:.7 h ii BN7*,Bfi78#%7*/49%#*K<Y :.7F0?B'!,:1&6#<5.BfiP7*Bfi78P
&"/19;:.7G#'7*/.#%7<9;:!+9_!+) ,!$9;:.#&2/9;:.7G0?B%!4,:1Bfi78#;,78(899;:.7<5?BNP7*Bfi&"/.05$D 9;:.7G3!BN&2!4O.)78#K
|G/ h ii Bfi7*,BN78#%7*/9'&2/.0C9;:.7D=/.(89%&65?f
/ Oa [ #] [ ( X
r jU[ #lk [ ( &6#G#;:.5$~A/C&"/ &)0.=Bfi7oK G&637*/T!/
!$#%#'&)0./>F7*/95$D9S:.7<!Bfi0?=>F7*/49%
# [ # !/.Z
P [ ( d^9;:.7G3!$"=.7F5$X
iT&)#MP789%7*B%>\&2/.78PQO4[W!@,!$9;:C#%9;!4Bfi9%&"/.0!$9
9;:.7GBfi5.59 /.5?P7<!/.P1&69%7*B'!$9%&63786[Dn5665$~ &"/.09;:.7G:.&)0.:178P07d.&JD9S:.7`!$#'#%5?(8&"!$9'78P13!Bfi&"!O.67H&)#9;B%=.7d!/.P
9;:.7G65$~78P07d&JDE9;:.7<!+#%#%5.(8&2!+9%78P13!BN&2!4O.)7F&6#D!$6#%7KY :.7G3!$"=.7H5$J
iT&6
# ( , 5.o
B n , &JD 9;:.7G"!O78
5$D 9;:.7<Bfi7*!$(8:.78PU9'7*B%>F&"/!$-/.5.P7G&6#]oG5.\
B PdBfi78#;,^78(89%&63786[K
x1
x2
0

1

&60?=Bfi7oKp|`/ h ii Bfi7*,Bfi78#'7*/9%&"/.0C9;:.7GD=/.(89%&65?/7i aO[ # ] [ ( rjq[
PB%!~A/C!$# #%56&6P!4/.PUP59%9'78P1)&"/.78#*dBfi78#;,^78(89%&63786[K

# k

[ (

K &)0.:U!/.PT)5$~}78P078#G!Bfi7

| / h 0?B%!,:\&)# Bfi78P=.(878PC#%5H9;:!$9/.5H9fi~5GP&6#%9'&2/.(89M/.5?P78#\cC!/.PsrF:!37 9S:.7 #;!>F7M3!4Bfi&"!O.67
G
/!>F7C!/.P65$~ !4/.P:.&60?:z#;=.(8(878#%#%5.Bfi#Uab &60?=Bfi7{!4r8d-!/.Pm/.5U3$!Bfi&"!O.67U/.5.P7sc:!$#F&6P7*/9%&6(*!$ 65$~
!/.PU:.&60?:#;=.(8(878#%#%5?BN#\ab &60?=BN7<{Or8K
Y:.7 h ii Bfi7*,Bfi78#%7*/49;!$9'&)5./Q:!$#G9fi~ 5U>!$fi5?BM!$P3!/49;!$0478# pG &"Bfi#%9*d&69<&6#!/7S(8&67*/49]Bfi7*,Bfi78#'7*/4R
9;!$9'&)5./5$D`O^5?567*!/D=/.(89%&65?/.#O78(*!4=.#%719;:.71/?=>FO7*BH5$DA/.5.P78#F5$DX9'7*/Q&6#>F=.(V:z#;>!$667*BH9;:!/z9;:.7
/.=>]O^7*BM5$D 9;B%=.9;:T!$#%#%&60?/>F7*/49%#M5$DE9;:.7H3!Bfi&"!O.678#*KHY :.7]/.=>FO7*BM5$DE/.5?P78#G(*!4/10?Bfi5$~78,5./.7*/9%&"!$
~ &69;:@9;:.7</.=>]O^7*B5$D-3!4Bfi&"!O.678#*dO=.9 >F5#%9(85?>>F5./.)[C7*/.(85?=/49%7*Bfi78P@D=/.(89'&)5./.#A:!37`!FBfi7*!+#%5?/!O.67
Bfi7*,Bfi78#'7*/9;!+9%&65?/K .78(85?/.Pd!/[F5?,^7*B%!$9%&65?/\5?/F9fi~ 5 h ii #*d.(85?B%BN78#;,5?/.P&"/.0F9%5F!<O5.567*!/@5?,7*B'!$9%&65?/
5?/F9S:.7 D=/.(89'&)5./.#9S:.78[\BN7*,Bfi78#%7*/49*d?:!$#E!65~(85?>,.678.&)9fi[O^5?=/.P78P1O[F9;:.7A,BN5?P=.(895$D9;:.78&"BE/.5.P7
(85?=/49%#]an Bfi[.!/9dopr8K
fi

fi

0 J

u

v

u

x

x

x

(b)

(a)

&60?=Bfi7<{p 87 P=.(89%&65?/.#H5$D h ii #*K1an!r/.5?P78#F!$#%#%5.(8&"!$9%78P9%5C9;:.7F#;!>F7H3!Bfi&"!O.67~&)9S:78=!+-65$~
!/.PU:.&60?:1#S=.(8(878#%#%5?Bfi#~ &6) O7H(85?/437*Bfi9%78P19%5C!<#'&2/.04)7/.5.P7K`aOr/.5.P78#M(*!=.#%&"/.0WBN78P=/4R
P!/499'78#%9%# 5?/1!\3!Bfi&"!O.67F!Bfi7G786&">F&"/!$9%78PK
| P &)#S!$P3!/49;!$047A5$D h ii # &6# 9;:!$9 9;:.7M#%&687G5$D-!/ h ii Bfi7*,Bfi78#'7*/9%&"/.0#'5?>F7D=/.(89%&65?/C&6# 37*Bfi[
P7*,7*/.P7*/49F5?/9;:.7@5?BfiP7*Bfi&"/.0U5+D9;:.73$!Bfi&"!O.678#*K1Y51I/.P!/5?,.9%&">!$ 3$!Bfi&"!O.675.BfiP7*Bfi&"/.0U&6#]!(85$R
MuZR(85?>,.6789%7,Bfi5.O.)7*> &"/&)9'#%78JDNdO=.9G!$#M&)6"=.#%9;B'!$9%78P&"/U &60?=BN7]C!05.5.P:.7*=Bfi&6#%9%&6(HDn5?B (8:.5.5#%&"/.0
!/C5?BNP7*Bfi&"/.0&6# 9%5@)5.(*!$9%7FBfi78"!$9%78P13$!Bfi&"!O.678#</.7*!B 7*!$(8:C59;:.7*BHan "!B%$7G789M!$nK"do*ppprK
x1

x1

y1

x2

x2
x3

x2

x3

x3

y1

y2
y2

x3

y3

y1

y1

y1

y2

y3
1

0

x3

1

0

(a)

(b)

&60?=Bfi7<pMY :.&6#< &60?=Bfi7C#;:.5$~ #G9;:.7C7S78(89<5$D 3$!Bfi&"!O.675.BfiP7*Bfi&"/.0TDX5?BM9S:.778,Bfi78#%#%&65?/aO[ #lkvu# r&w
Oa [ ( k7u ( &
r wmOa [ % k7u % r8KY:.7 h ii &"/an!4rM5?/.6[0.Bfi5$~ #G6&2/.7*!4Bfi6[~ &69;:9;:.7/.=>]O^7*BG5$D
3!BN&2!4O.)78#C&"/9;:.7178,Bfi78#%#%&65?/d~A:.&6679S:.7 h ii &"/anOrF:!$#C!/Q78,5./.7*/9%&"!$M0?Bfi5$~ 9S:K
:.7F78!4>,.67&66"=.#%9;B%!$9'78#<9;:!$9<,."!$(8&"/.0QBfi78"!$9%78Pz3!Bfi&"!O.678#/.7*!BM9%517*!+(V:T59;:.7*B&"/9;:.7
5?BfiP7*Bfi&"/.05+DX9%7*/C&6#G!F05.5?PU:.7*=BN&)#'9%&6(K
h ii #E:!37EO787*/F#S=.(8(878#%#fiD=.)6[!,,.6&678P9%5G>F5.P78?(8:.78(V4&"/.0K /F>F5.P78?(8:.78(8&"/.0G9;:.7 O7*:!3?&65?B
5$D-!H#%[.#%9%7*> &6# >F5.P786)78PO[C!MI/.&69%7G#%9;!+9%7A!=.9%5.>!$9%5?/H~ &69;:@9;:.7M9;B%!/.#'&)9'&)5./Bfi78"!$9'&)5./Bfi7*,BN78#%7*/9'78P
!$# !/ h ii KEG78#%&"B%!4O.)7<,BN5?,7*Bfi9'&)78#G!Bfi7M(8:.78(V$78PUO4[1=.#%&"/.0 h ii >!/.&",=."!$9%&65?/.#M9%5!/!$6[?87H9;:.7
#%9;!+9%7A#S,!$(87<5$D 9;:.7G#%[.#%9%7*> "!B%$7M789A!$nK"do*p.(8&6)"!/dZo*pp4r8K
fi yx

fi 0



)-
- -)

/9'7*Bfi78#%9%&"/.06[dG!#%&">F&6"!BU!,,BN5?!$(8:m(*!/O7=.#%78PDX5?B@#%563.&"/.0/.5?/4RP789%7*B'>F&"/.&)#'9%&6(,."!//.&"/.0
, Bfi5?O.67*>F#K |A# !/@78!>@,.)7d.(85?/.#'&)P7*B_9;:.7A|Bfi7*,Bfi78#%7*/49;!$9'&)5./F5$D-!</.5?/4RP789'7*B%>F&"/.&6#%9%&6(],."!//.&"/.0
P5?>!+&2/C#S:.5~M/&"/1 &60?=BN71?!K /C9;:.&6# P5.>!$&"/C9;:.7*Bfi7<!BN7 Dn5?=B #%9S!$9%78#E04&)347*/UO4[9;:.7MDn5?=B ,5#%#'&2O.67
3!+2=.7!$#'#%&60?/>F7*/49%# 5$D 9;:.7G9fi~ 5\O^5?54)7*!4/1#%9;!+9%7G3!BN&2!4O.)78
# [ # !4/.Pz[ ( K /,=.9%#M9%5F9;:.7]|}P7*/.59%7
!$(89%&65?/.#G&"/9S:.7P5?>!$&"/!/.PQ!Bfi7FP7SI/.78PmO[T9;:.7O5.567*!/3$!Bfi&"!O.6s
7 {
KFY :.7 h ii Bfi7*,BN78#%7*/9'&2/.0
9;:.7G9;B'!/.#%&69%&65?/1Bfi78"!$9%&65?2
/ |]Oa { ] [ #] [ (] [} # ] [}( r 5$DZ9;:.7< |}&6# #;:.5$~A/C&"/1 &60?=BN
7 1?OKEY :.7GP7SI/.&69%&65?/
5$&
|&6#M#%9;B%!+&)0.:9fiDn5?Bfi~ !BNM
P pDn5?B #%5?>\7<!$#%#%&60?/>F7*/49M5$D&69%#G!Bfi0.=>F7*/9'#*d |&6#A9;B'=.7<&J!$(89%&65?Z
/ {C(*!4=.#%78#
!F9;B'!/.#%&69%&65?/CDBfi5?> 9;:.7<(*=B%BN7*/9M#%9;!$9'7G0&637*/UO4[9S:.7<3!+2=.7F5$X
[ # !/.z
P [ ( 9%59;:.7F/.78?9M#%9;!+9%7A04&)347*/
O4[19;:.7<3$!$"=.7]5+J
[ } # !/.7
P [ }( K ( 59'7<9;:!$99;:.7 h ii BN7*,Bfi78#%7*/49%&"/.%
0 | DX5.B 9;:.7<78!>,.67F9;=B%/.#5?=.9
/.59 9'5]P7*,^7*/.P5?
/ [ }( K


0

00
1

10

0

01

1

11

1

x1

x1

x1

x1

x2

x2

1

0

(a)

(b)



P1
00

1

G
0

x1

01

x1
x2

1

10

0

1

(d)

(c)

&60?=Bfi71p|,.2!4//.&2/.0zP5?>!$&"/Bfi7*,Bfi78#%7*/49%78Pm!$#!/Q| &6##S:.5~M/&"/an!4r8K1.9;!$9%78#F!Bfi7CP7SI/.78P
O[O5.567*!/#%9S!$9%7<3$!Bfi&"!O.678)
# [ # !/.v
P [ ( d!/.P9;:.7!$(89%&65?/T&"/,=.9F9%5C9;:.7 |&)#G04&)347*/
O[@9;:.7`O^5?54)7*!4/13!4Bfi&"!O.6)
7 {
KY:.7G#%[>FO^56&)(FBfi7*,Bfi78#%7*/49;!$9'&)5./5$DZ9;:.7G9;B%!/.#'&)9'&)5./Bfi78"!$9'&)5./
5$D 9;:.7 | &6#G#;:.5$~A/&2/manOrKA/mab(*rd ~ # &6#G9;:.7H#%789A5+DE#%9;!$9%7F!$(89%&65?/T,!$&"Bfi#MDX5.B ~A:.&6(V:d
78?78(*=.9'&)5./U5+D9;:.7F!$(89%&65?/T(*!/T67*!$P9%59S:.7]045?!$nKGY :.7F#%[>FO^56&)(CBfi7*,Bfi78#%7*/49;!$9%&65?/T5$Dd~ #
&6##S:.5~M/F&"/QabPr8K n9&)#5?O.9;!$&"/.78P@DBfi5.> 9;:.7M9;B%!/.#'&)9'&)5./Bfi78"!$9'&)5./O4[Bfi78#%9SBfi&6(89%&"/.0]9S:.7`/.78.9
#%9;!$9'7A9%5
5 Po4K
|M#%#;=>\79S:!$9-9S:.7#'9;!$9%
7 PoE&6#E!M05.!$?#'9;!$9%"
7 K|$78[<5.,7*B%!$9'&)5./d$~A:.7*/H07*/.7*B%!$9%&"/.0<!G=/.&637*Bfi#S!$
., "!/DX5.B !$(8:.&)783.&"/.0d&6#M9%5FI/.PQ!$69;:.7H#%9;!$9'7<!$(89%&65?/U,!+&2BN#\aO ] {.r #;=.(8:19;:!+9\(*!/TO7Bfi7*!$(8:.78P
DBfi5?><O4[78.78(*=.9%&"/.0{KEY:.&)##%789 &6# "!O78678Pv~ # &2/ &60?=Bfi71(K Y5FI/.Pz~ # DBfi5?>|~ 7G(85?/.#'9;B%!$&"/
[ } 9%
5 n , !/.
P [ }( 9%
5 ( , &"7
/ |HKY :.&6#Bfi78P=.(878

# |9%519;:.7 h ii #S:.5~M/U&"/z &60?=Bfi751PKCY :.7
#
Bfi78#;=.69%&"/.0 h ii Bfi7*,Bfi78#%7*/49%5
# ~ # ~&)9S:9;:.71#%9;!$9'78#]P78#'(*Bfi&"O78P&"/m9;:.7C(*=B%Bfi7*/49#%9;!+9%73$!Bfi&"!O.678
# [ #
$;b$ Sn;b'<cff8\K'K--fi *HHS&fi
$nF"% *n*-'8 nnSnX%'nHfifiX<HS
n$;n<b&$ ;nSn;b' fi nZb fiS 'MSn;b'<c< y* n$8 c
fi

fi

0 J

!/.PQ[ ( KZ$504&)(*!+)6[~ 7,7*BDn5?B%>\78P9;:.7@5?,7*B%!+9%&65?/h[
Bfi7*,Bfi78#'7*/9%&"/.0~ # K

} ] [ } _ [ } k
#
(
#

[ } k
(

|9%5T5?O.9;!$&"/9S:.7

h ii

'6
5AX6'Uv6 -6 q
J
'
/9;:.&6#<#%78(89%&65?/d~ 7<IBN#%9AP&6#%(*=.#'#]9S:.7\,BN5?,7*Bfi9'&)78#H5$
"!#O!$#'78P5./!4/&"/4DX5?B'>!$ZP7SI/.&)9'&)5./Q5$D
9;:.7C"!/.0?=!$07U!4/.P!TP5?>!+&2/7*/.(85.P&"/.0Q78!>,.67KQxQ719S:.7*/QP78#%(*BN&2O^7U9;:.7@Dn5?B%>!$#%[/9;!+!/.P
#%7*>!4/9%&6(8# 5+d
"!#K
|G2
/ /"!#QP5?>!+&2/@P78#%(*Bfi&",.9%&65?/(85?/.#%&6#%9%# 5$HD p-!HP7SI/.&69%&65?/C5$DV ,* + *8- , 8d?!P78#%(*Bfi&",.9%&65?/C5$D
8*8 , !/.P , fi ), ,
bVd!/.PU!F#S,78(8&JI(*!$9%&65?/T5$DE!/bbn !/.P1. ; bn
K
Y:.7G#%789 5$D-#'9;!$9%7M3!BN&2!4O.)7!$#'#%&60?/>F7*/49%# P7SI/.78#A9;:.7H#%9;!$9'7A#;,!+(87G5$D9S:.7GP5?>!$&"/K |G/1!$07*/49 Lc#
P78#%(*Bfi&",.9%&65?/H&6#-! #'7895$D *nb8K-Y:.7E!$07*/49%#(8:!/.07 9;:.7 #%9S!$9%7 5$D9;:.7 ~ 5?Bfi6P<O4[`,^7*BDX5.B%>F&"/.0`!+(89%&65?/.#
9;:!$9 !Bfi7G!$#%#;=>\78P19%5]O^7G78?78(*=.9%78P1#'[/.(8:Bfi5?/.5?=.#')[1!/.PC9%5:!37`!I.78PW!4/.P178=!$P=B%!+9%&65?/KZ| 9
7*!$(8:Q#'9%7*,d !$65+DA9;:.71!$07*/49%#],^7*BDX5.B%> 78!$(89')[z5?/.7W!+(89%&65?/d !/.Pm9;:.71Bfi78#;=.69%&"/.0!$(89%&65?/z9;=,.67C&)#
! V4
n
KY :.71#'[?#%9'7*> !+07*/9'#\>\5?P78 9S:.7UO7*:!3?&65?B\5$DA9;:.7U!$047*/9%#F(85?/49;Bfi54)"!O.67O[9;:.7

,."!//.7*B8d ~A:.&6679;:.717*/43?&"Bfi5?/>\7*/9!+07*/9'#>F5.P78 9;:.7U=/.(85?/49;Bfi56"!O.67~ 5?Bfi6PKm| 3!$6&6PP5?>!$&"/
P78#%(*Bfi&",.9%&65?/ Bfi78=.&"Bfi78#19S:!$99S:.7U#%[.#%9%7*> !/.P7*/3.&"Bfi5?/>F7*/491!$07*/49%#(85./.#%9;B%!$&"/!QP&6#nfi5&"/91#'7895$D
3!4Bfi&"!O.678#*K
|G/!$(89%&65?/:!$#19;:BN787,!Bfi9%^# p!z#%78915$D8 ,* + *8- , 8dA! , ; bn DX5?B'>]=."!dM!/.P!/
,Oo, Dn5?B%>F=."!K]/9S=.&)9'&)3478)[z9;:.7!$(89%&65?/T9;!$78#GBfi78#;,^5?/.#%&"O.&66&"9fi[Q5$D(85./.#%9;B%!$&"/.&"/.0U9S:.7]3$!$"=.78#<5$D 9;:.7
#%9;!+9%7F3!Bfi&"!O.678#F&"/9;:.7/.78.9<#'9;!$9%7KFn9D=Bfi9S:.7*BG:!$#<78.(8"=.#%&637W!+(8(878#%#<9%519S:.78#%73!4Bfi&"!O.678#]P=BN&2/.0
78.78(*=.9%&65?/K/5.BfiP7*B@DX5.B9;:.7!$(89%&65?/9%5wO7!,,.6&)(*!4O.)7dH9;:.7Q,Bfi78(85?/.P&69%&65?/Dn5?B%>F=."!m>]=.#'9WO^7
#;!$9'&)#NI78PU&"/T9;:.7<(*=B%BN7*/9M#%9;!$9'7KY :.7F7S78(89 5$DE9S:.7]!$(89%&65?/&6#AP7SI/.78PO4[C9;:.7<7S78(89DX5?B'>]=."!@9;:!$9
>F=.#%9<O7@#;!$9%&6#fiI78P&"/9;:.7/.78.9G#%9;!+9%7K<Y5U!$6)5$~(85?/.P&69%&65?/!$ 7S78(89%#*d9;:.7@7S78(89G78,Bfi78#%#'&)5./(*!/
Bfi7SDn7*B 9'5O59S:1(*=B%Bfi7*/49G!/.P/.78.9A#'9;!$9%7H3!Bfi&"!O.678#*d~M:.7*Bfi7<9;:.7F/.78?9M#%9;!+9%7G3!BN&2!4O.)78#</.7878P9%51O7F!
,!Bfi9Z5$D9;:.7 #%789Z5$D(85?/.#'9;B%!$&"/.78PF3!BN&2!4O.)78#5$D9S:.7!$(89%&65?/KZ|M6/.78?9 #%9S!$9%7 3!4Bfi&"!O.678#/.59Z(85?/.#%9;B'!$&"/.78P
O4[W!/4[1!$(89%&65?/C&"/U!Mfi5&"/49A!$(89%&65?/T>!$&"/9S!$&"/C9;:.78&"B 3$!$"=.7K
Y:.7 &"/.&)9'&2!+!4/.P05?!$.(85?/.P&69%&65?/1!BN7EDn5?B%>F=."!$#9;:!$9E>F=.#%9EO7M#;!$9'&)#NI78P&"/9;:.7&2/.&69%&"!$#%9S!$9%7 !/.P
9;:.7MI/!$#%9;!$9%7dBfi78#S,78(89%&63786[K
Y:.7*Bfi7!Bfi7F9fi~ 51(*!=.#%78#DX5?BG/.5./4RP789%7*B%>F&"/.&6#;> &"Q
/ "!#QP5?>!$&"/.# p1afio*rG!$(89'&)5./.#]/.59<Bfi78#'9;Bfi&6(89fiR
&"/.0!$6 9;:.78&"BF(85?/.#%9SB%!$&"/.78Pm3!BN&2!4O.)78#@9%5Q!U#S,78(8&JI(U3$!$"=.7U&"/9;:.71/.78?9F#%9;!+9%7d !/.Pan{r9;:.7U/.5?/4R
P789%7*B%>\&2/.&6#%9'&)(F#%78678(89%&65?/T5$DZ7*/3.&"Bfi5?/>F7*/49`!+(89%&65?/.#*K
|#%&">,.67C78!4>,.675$DA!
/ /"!#P5.>!$&"/P78#%(*Bfi&",.9%&65?/&6#F#;:.5$~A/T&"/Q &60?=BN7WqKCY :.7CP5?>!$&"/
P78#%(*Bfi&"O^78#\!T,."!//.&"/.0,BN5?O.67*> Dn5?BG.(V:.5.,,7*Bfi# L afio*p4rABN5?O59NRO!O[TP5?>!$&"/KCY :.7P5?>!+&2/z:!$#
9fi~ 5G#%9;!$9%7 3$!Bfi&"!O.678# pE!H/?=>F7*BN&)(*!+5?/.7d,54#%&69%&65?/ $ ~ &69;:CB%!/.0F
7 P ] ] { ] !/.P!<,BN5?,5#'&)9'&)5./!$
# # 8 * A!/.P
5?/.7dfi * g *.KEY :.7GBfi5?O549&6# 9S:.7A5?/.6[C#%[.#%9%7*>!$07*/49!/.P1&69 :!$# 9fi~ 5]!$(89%&65?/.o
#K
g , * KUY:.7
7 # V Uan!/.Q
P #K g , H 8r<!$(89%&65?/:!$#!T(85?/.P&69%&65?/!$ 7S78(89FP78#%(*Bfi&"O78P
O4[!/T&JD R9S:.7*/4R78)#'75?,^7*B%!$9%5.BOa myr p &JD<fi * g *&6#G9;B'=.7]9S:.7*
/ # 8 *y G&"/.(*Bfi7*!$#'78#<9;:.7O.65?(8
,54#%&69%&65?/mO[z5?/.7786#%7C9;:.71O.65?(8,^5#%&69%&65?/&6#=/.(V:!4/.078PKQY :.71O!O[z&)#F9;:.7C5?/.6[7*/3.&"Bfi5?/>F7*/49
!$07*/49!4/.P&69_:!+#E5?/.7G!$(89%&65?A
/ Gb * nK 78(*!=.#%7M7*!$(8:!+07*/9 >F=.#%9E,7*BDn5?B%>78!$(89%6[F5?/.7G!$(89%&65?/
!$9Z7*!$(8:<#'9%7*,d$9;:.7*Bfi7 !Bfi79fi~5fi5&"/9 !$(89%&65?/.#Aa # V `b * 4"r!/.PUa #K g , * -d Gb * "rK
/.&69%&"!$66
[ >y^ >> &6#!+#%#;=>F78P9'5<O7 9;B'=.7d9;:.7 Bfi5?O^59 &6#!$#'#;=>F78PF9%5<:.56P1!GO.)5.(8]!+9EuZ5#%&69%&65?/
Pd!4/.P1&)9'# 9;!$#;@&6# 9%56&JDn9A&69A=,9%5uZ5#%&69%&65?/1^K
Y:.73$!Bfi&"!O.6
7 >y^ o>_(*!/O7M>!$P7 D!$6#%7AO4[<9;:.7MO!O[K :.7MO!OC[ Lc#E!$(89'&)5.A
/ `b * .&)#
/.5?/4RP789%7*B'>F&"/.&)#'9%&6(d!+#&69-5?/.6[H(85?/.#%9;B%!+&2/.# fi * g * O4[A9;:.77S78(89-78,Bfi78#%#%&65?/ fi * g 4H

fi

fi 0



)-
- -)

>KyKCK
< xK

M+
H>
K+^
""8



X
Cy+
&?+
^X V

'

fi^ H[

&

fi^ H[

8u



Cy+ X
&?+
^X V
>y+C>
K+^ 6

<


V

Cy+
&
^X
V
Ka
J & V

+K

J

V



&60?=Bfi7<qp |G/f"!'#P5?>!$&"/CP78#%(*BN&2,.9'&)5./K
fi * g * } KCY :.=.#*d~A:.7*/ * g *&6#H9;B%=.7&"/z9;:.7F(*=B%Bfi7*/49<#%9;!+9%7d9;:.7F7S78(89<78,Bfi78#%#'&)5./
5$D\G * P5?78#</.59G!,,.6[d!/.P * g *F(*!/T78&)9S:.7*BGO7F9;B%=.7H5?B D!$6#%7H&2/T9;:.7/.78.9A#'9;!$9%7K
G/9;:.7F59S:.7*BG:!/.Pd&JD<fi * 4 g *&6#GD!$6#%7&"/9S:.7(*=B%Bfi7*/49G#%9;!$9'7d`b * 4+787*,.#<&69MD!+)#'7&"/
9;:.7/.78.9<#%9;!+9%7K<Y :.A
7 G * E!$(89'&)5./>\5?P786#]!4/!+#;,78(89H5$DE9;:.77*/43?&"Bfi5./>F7*/9</.549G(85?/9SBfi56678P
O4[m9;:.7QBfi5?O549!$07*/49*d &"/9;:.&6#1(*!$#%7!O!O4[dAO4[m&69%#17S78(89%#5./fi * g *.K/9;:.7T78!4>,.67
!O^5347dEfi * g *C#%9;![.#MD!$6#%7@~A:.7*/Q&69F:!$#]O^78(85?>F7@D!$6#%7d Bfi7S78(89'&2/.09;:!+9G9;:.7WBN5?O59G(*!4//.59
#;,^5?/9S!/.785?=.#%6[UO7I?78PUO4[U!F:.&69A5$D 9;:.7<O!O4[d5?B!/[C59;:.7*B !$(89'&)5./1&"/19;:.7G7*/43.&2BN5?/>F7*/49*K
|G/m| Bfi7*,Bfi78#%7*/49%&"/.09;:.71P5?>!$&"/&6##;:.5$~A/&"/ &60?=Bfi7KY:.71(*!$6(*=.2!+9%&65?/5$DA9;:.7U/.78.9
#%9;!+9%7<3$!$"=.75$D $&"/9;:.A
7 # 8 * !+(89%&65?/T#;:.5$~ #G9;:!$9G/?=>\7*Bfi&6(*!$E3$!Bfi&"!O.678#<(*!/O^7=,P!$9'78P
O4[!/Q!4Bfi&69;:>F789%&6(78,Bfi78#%#%&65?/5?/9;:.7F(*=B%Bfi7*/49<#'9;!$9%7H3!Bfi&"!O.678#*KCY :.7=,^P!$9%778,Bfi78#%#%&65?/5$D $
!/.PC9;:.7<=.#%7M5$D-9;:.7G&JDbR9;:.7*/4R786#%7<5.,7*B%!$9'5?BZD=Bfi9;:.7*BP7*>F5?/.#'9;B%!$9%7M9;:.7G!$P3!4/9;!+07A5+D=.#%&"/.078,.)&6(8&69
Bfi7SDn7*Bfi7*/.(878#F9%5U(*=B'Bfi7*/9F#%9S!$9%7!/.P/.78?9F#%9;!+9%73!4Bfi&"!O.678#&"/Q7S78(89F78,BN78#%#%&65?/.#*
K "!'#QP5.78#/.59
Bfi78#%9SBfi&6(89 9;:.7<Bfi7*,Bfi78#'7*/9;!+9%&65?/1O[C7*/4Dn5?Bfi(8&"/.0W!\#%9;B%=.(89;=BN7G#%7*,!B%!$9'&2/.0@(*=B%Bfi7*/49 #%9;!$9'7`!/.P/.78?9#%9;!$9%7
78,Bfi78#%#%&65?/.#*K :.7&JD R9;:.7*/4R786#%75?,7*B%!+9%5?B:!$# O787*/F!$PP78PF9%5#;=,,5?BN9-(85?>@,.)78H(85?/.P&69%&65?/!$.7S78(89%#
9;:!$9 5$Dn9%7*/C!Bfi7M7S(8&67*/9')[U!4/.PU/!$9;=B%!$66[1Bfi7*,Bfi78#%7*/49%78PU!$#A!H#%789 5$D /.78#%9%78PU&JDbR9;:.7*/4R786#%7<5.,7*B%!$9'5?Bfi#*K
Y:.7A78,.6&)(8&69<Bfi7*,BN78#%7*/9S!$9%&65?/@5$D-(85?/.#%9SB%!$&"/.78P1#%9;!$9'7 3!BN&2!4O.)78#M7*/!O.678#G!/[C/.5?/4RP789%7*B%>F&"/.&6#%9%&6(
5?BMP789%7*B'>F&"/.&)#'9%&6(7S78(89<5+D_!/!$(89%&65?/9%5UO7BN7*,Bfi78#%7*/49%78Pd-!$#G9S:.7(85?/.#%9;B'!$&"/.78PQ3!BN&2!4O.)78#F(*!/O^7
!$#%#'&)0./.78P19%5]!/4[F3!$"=.7H&2/@9;:.7G/.78?9 #%9S!$9%7M9;:!$9E#S!$9%&6#fiI78# 9;:.7M7S78(89Dn5?B%>F=."!K n9D=Bfi9;:.7*B9;=B%/.#5?=.9
9%5:!37<!\(8)7*!4B &"/9;=.&69%&637>F7*!4/.&2/.0
d!$#9;:.7<!$(89%&65?/9;!$78# 9;:.71;BN78#;,5?/.#'&2O.&66&69fi[?T5$D#S,78(8&JDX[.&"/.019;:.7
3!+2=.78#M5$D 9;:.7G(85?/.#%9SB%!$&"/.78PU3$!Bfi&"!O.678#G&"/19;:.7</.78.9 #%9;!+9%7K
fi

fi

0 J

robot_works
false

true

G



0

G

1

2

3

pos

&60?=Bfi7<pMY :.7|5$D 9;:.7WBN5?O59NRO!O[TP5?>!$&"/ab#%787C &60?=Bfi7qrKWY:.7*Bfi7&6#F5?/.7W,BN5?,5#'&)9'&)5./!$
!/.P5?/.7Q/?=>\7*Bfi&6(*!$<#%9S!$9%7U3$!Bfi&"!O.6K7 p} * g *!/.P +?K Y:.7a # 8 * -d G
* "rZ!/.PWa #<
g , *y $-d G * "r.fi5&"/9 !$(89%&65?/.#E!4Bfi7EPB%!~A/~ &69;:G#'56&)PC!/.P<P!$#S:.78P
!B%Bfi5$~ #d?Bfi78#;,^78(89%&63786[K.9S!$9%78#A>!4B%$78PC~ &69;:;nC!/.Pm;G!BN7G&2/.&69%&"!$ !/.P105.!$#%9S!$9%78#*K

_5?>,!Bfi78P9%5C9;:.7!+(89%&65?/TP78#%(*Bfi&",.9%&65?/z2!4/.0?=!$07 an78Dn5?/.PQv$&JDn9%#%(8:.&69%d o*pprM!/.P<
9;:!$9F!Bfi7C9;:.7C5?/.6[,Bfi&65?BH"!/.0?=!+078#=.#%78PDX5.B h ii RO!+#%78P,."!//.&"/.0abG&Q!/.85789\!+XKedopp
&">!$9'9%& 789U!$nK"dGo*p4p!d oppOd oppr8.
"!'#&"/9;BN5?P=.(878#U!/ 78,.6&6(8&697*/3.&"Bfi5?/>F7*/49U>F5.P78XdM!
>F=.69%&JR!$07*/49P78(85?>,^5#%&69%&65?/d !/.Pw/.=>F7*Bfi&6(*!$ #'9;!$9%7C3!BN&2!4O.)78#K9(*!/zD=Bfi9;:.7*BO^71#;:.5$~A/9;:!$9
"!'#(*!/1O7F=.#%78PU9'5\>\5?P78-!4/[CP5?>!$&"/9;:!$9 (*!/UO^7<>F5.P78)678PO4[C<
ab#%787G|G,,7*/.P&6U|Gr8K
Y:.7 (85?/.(*=B%Bfi7*/49 !$(89%&65?/.#&22
/ "!'#m!Bfi7A!+#%#;=>F78PC9%5]O^7A#'[/.(8:Bfi5?/.5?=.#')[@78?78(*=.9%78P1!/.PC9%5<:!37
I.78P!/.Pz78=!$ P=B'!$9%&65?/K|07*/.7*B'!$Bfi7*,BN78#%7*/9S!$9%&65?/Q!$665$~ &"/.0,!BN9%&"!$6)[z5347*Bfi"!,,.&"/.0!+(89%&65?/.#
!/.P!+(89%&65?/.# ~ &69;:FP&J7*Bfi7*/49 P=B%!$9%&65?/.# :!$#EO787*/!354&)P78Pd.!$# &69Bfi78=.&"Bfi78#>\5?Bfi7 (85?>,.678F9%7*>,^5?B%!$
,."!//.&"/.0wab#%787F7Kc0K"d
h Nf+- 5?B f-g f$Z dw =B%BN&)7vYZ!$9%7d o*pp^o $7837*B<v &6(8:!BfiP#*do*p>p 1?rK
G=Bfi5&"/49!$(89%&65?/@Bfi7*,Bfi78#%7*/49;!$9%&65?/F:!$# >F5?Bfi7 Bfi78#'7*>]O."!/.(87M~ &69;:FF
!B'!$v G78JDX5./.Pdo*pp4r !/.P
an&2=/.(8:.&606&"!q
v $&JDX#'(V:.&69%doppr8d4~A:.7*Bfi7M#%789%#5$D-!$(89%&65?/.# !Bfi7G,7*BDn5?B%>\78PW!$97*!$(8:9%&">F7M#%9%7*,K /
(85?/49;B%!$#'99'5]9S:.78#%7]!,,BN5?!$(8:.78#*d.9;:.5?=.0?:d~ 7<>F5.P78->F=.69%&JR!$07*/49AP5.>!$&"/.#*K
|G/@&2>@,5?Bfi9S!/9 &6#%#;=.7G9'5\!+PPBfi78#%# ~A:.7*/C&"/9SBfi5?P=.(8&"/.0(85./.(*=B%Bfi7*/49_!+(89%&65?/.# &6# #%[/.7*BN0789%&6(G7S78(89%#
O789fi~ 787*/@#%&">]=.69;!4/.785?=.#%6[78.78(*=.9%&"/.0!$(89%&65?/.#GOa $&"/.0?!BfiP1v &6(V:!BNP#*do*pprK|(85?>>F5./F78!4>,.67
5$DP78#%9;B'=.(89%&637 #%[/.7*Bfi0789%&6(M7S78(89%# &6#~M:.7*/F9fi~5G5.B >F5?Bfi7A!+(89%&65?/.#Bfi78=.&"Bfi7M78?(8"=.#%&637G=.#%7 5$D-!#%&"/.067
Bfi78#%5.=Bfi(87G5?B ~A:.7*/9fi~5!$(89'&)5./.#A:!37G&"/.(85?/.#%&6#%9%7*/49A7S78(89%#)&"$7 $ } j@!/.P $ } j{^K
7
/ "!#!$(89%&65?/.#M(*!//.59GO7,^7*BDX5.B%>F78PU(85./.(*=B%Bfi7*/49%6[U&"/T9;:.7GDn5665~&2/.019fi~ 5(85?/.P&69%&65?/.^# p<o*r
9;:.78[F:!*347E&"/.(85?/.#%&6#%9%7*/49E7S78(89%#d$5?B {r9;:.78[<(85?/.#'9;B%!$&"/F!/H5$37*Bfi"!,,.&"/.0G#%789 5$D#%9;!$9%73!BN&2!4O.)78#KEY :.7
IBfi#%9(85?/.P&69%&65?/C&6#EP=.7M9%5<9S:.7 D!+(89E9;:!$9#%9;!$9'7_./.5$~ 678P07M&)# 78,Bfi78#%#%78PC&"/!H>F5?/.59'5?/.&6( 650&6(G9;:!$9
(*!//.59FBfi7*,Bfi78#%7*/49<&"/.(85?/.#'&)#'9%7*/9C?/.5$~ 678P07K1Y :.7#'78(85?/.P(85./.P&)9'&)5./m!$PPBfi78#%#%78#H9;:.7,Bfi5.O.)7*> 5$D
#;:!BN&2/.0Bfi78#'5?=Bfi(878#*K 5?/.#'&)P7*BDn5?B 78!>,.67M9fi~5F!$07*/49%# 9;Bfi[.&"/.0F9%5<PBfi&"/F9S:.7A#;!4>F7 0"!$#%#5$D~ !$9%7*B8K
bDF5?/.6[9;:.7TIBfi#%91(85?/.P&69%&65?/P7SI/.78P}&"/9'7*BDX7*BN&2/.0w!+(89%&65?/.#*dGO59S:!+07*/9'#(85?=.6P}#%&">F=.)9S!/.785?=.#%6[
7*>,.9fi[9;:.7M0"!$#%#d?!$# 9;:.7M7S78(89E 6 , nF5$D-9S:.7A9fi~ 5<!$(89%&65?/.# ~ 5?=.6PUO7M(85?/.#%&6#%9%7*/49*K x&69;:@9;:.7
#%78(85?/.P@(85?/.P&69%&65?/!$PP78Pd49;:.78#%7A!$(89'&)5./.#E!Bfi7 &"/9%7*BDX7*Bfi&"/.0F!/.PF(*!//.59EO7M,7*BDn5?B%>F78PF(85?/.(*=B'Bfi7*/9')[K
Y:.7U(*=B%Bfi7*/49137*Bfi#'&)5./w5+5
/"!#Q5?/.6[!35&6P#P78#'9;B%=.(89%&637#%[/.7*Bfi0789%&6(7S78(89%#*K n9CP5.78#U/.59
&"/.(8"=.P7Q~ !*[.#5$D<Bfi7*,Bfi78#'7*/9%&"/.0(85?/.#%9;B'=.(89%&637U#%[/.7*Bfi0789'&)(T7S78(89%#O789fi~ 787*/m#'&2>F=.69;!/.785.=.#U!$(89%&"/.0
!$07*/49%#*KH|(85?/.#%9;B'=.(89%&637#%[/.7*Bfi0789%&6(7S78(89G&6#F&6)"=.#%9SB%!$9%78P&"/Q !B%!$E!/.PQ78Dn5?/.Pafio*pp4r8d
~A:.7*Bfi7
!/!$07*/49<#;,.&666##'5?=,DBfi5?> !UO^5~~M:.7*/Q9;BN[?&"/.09%5U6&JDn9&69\=,z~ &69;:z5?/.71:!/.PdEO=.9/.59<~M:.7*/
6&JDX9%&"/.0&69 =,~ &69;:CO549;::!/.P#*K
/ !/.P
F9;:.&6# &"/.PC5$D#%[/.7*BN0789%&6(A7S78(89%#(*!/O^7`Bfi7*,BN78#%7*/9'78P
fi

fi

0

)-
- -)

O4[78,.6&)(8&69%6[Q#'9;!$9%&"/.09;:.7H7S78(89 5$DE!F(85?>,^5?=/.PQ!$(89%&65?/K|#%&">F&6"!BG!,,BN5?!$(8:C(85?=.6PO^7]=.#%78PT&"/
"!'#mO=.9 &6#A(*=B'Bfi7*/9')[1/.59 #S=,,5?Bfi9'78PK
C6{

5.B%>!$66[d!/f"!#P78#%(*Bfi&",.9%&65?/T&6#A!R9;=,.675!ja

]J]A]

* ]]] ]
r8d~A:.7*Bfi7Kp



&6#!zI/.&)9'7Q#%78915$D<#%9S!$9%73!BN&2!4O.)78#T(85?>,Bfi&6#%78P 5$D\!I/.&69%7#'7895$D
,Bfi5?,54#%&69%&65?/!$3$!Bfi&"!O.678#*d) ?d!/.P!HI/.&69%7<#%7895$D/.=>F7*Bfi&6(*!$Z3!Bfi&"!O.678#*d K



&)#G!HI/.&69%7d/.5?/.7*>,.9fi[C#%789 5$D #%[.#%9%7*>!$07*/49%#*K




&6#`!]I/.&)9'7<#%789 5+D-7*/3.&"Bfi5?/>F7*/49G!$07*/49%#*K

dj

)

AM&)#!T#%789H5$DA!$(89%&65?/zP78#%(*Bfi&",.9%&65?/.#a ]M] r ~M:.7*Bfi7\&)#F9;:.7C#%789F5$D #%9;!+9%73!4Bfi&"!O.678#(85?/4R
#%9;B%!$&"/.78PUO4[9;:.7<!$(89'&)5./d &6#A!F,Bfi78(85?/.P&69%&65?/T#%9S!$9%7 Dn5?B%>F=."!H&2/9;:.7M#%789on !/.P &6#A!/
7S78(89DX5.B%>F=.2!H&"/C9;:.7M#%789\n K :.=.#<a ] ] rff
ZM* fi{n n K :.7A#'789%#
n !/.7
P n !4Bfi7GP7SI/.78PQO7865$~<K


p+.'
{ &)#G!HD=/.(89%&65?/U>@!,,.&"/.0W!$047*/9%#FaO?'j r 9%5F9;:.78&"BM!$(89%&65?/.#*KE78(*!=.#'7
!/1!$(89%&65?/1O^78)5./.0#A9'578!+(89%6[5?/.7F!$07*/49*d >F=.#%9 #;!+9%&6#fiDX[C9;:.7MDn5665~&2/.0C(85?/.P&69%&65?/.#p



Zr&jUM*





4


7n








'
7n


#] (
Z? _ #"j#
!
(

r $
# %

r j#&
( &

&6# 9;:.7H&"/.&)9'&2!+-(85?/.P&69%&65?/K
&6#A9S:.7G05?!$(85?/.P&69%&65?/K

5.BE!G3$!$6&6PUP5?>!+&2/@P78#%(*Bfi&",.9%&65?/d^~7MBfi78=.&"Bfi7M9;:!$9 !$(89%&65?/.# 5$D#%[.#%9%7*>!$07*/49%#!BN7 &"/.P7*,7*/.P7*/49A5$D
!$(89%&65?/.#5$D7*/43.&2BN5?/>F7*/49`!$047*/9%#^p
a{?r)$



{(








r

aO{.r&j#&




*

{+




]


aO*r

~A:.7*Bfi7*a{?r &6# 9S:.7<#%789 5+D-(85?/.#%9;B'!$&"/.78PU3!4Bfi&"!O.678#G5$D!$(89%&65?/f{K
Y:.7G#%789 5$DZDX5.B%>F=.2!+#n &6#M(85?/.#%9;B'=.(89%78PCDBN5?>9;:.7GDn5665~&2/.01!$",:!O^789A5$DZ#%[>FO^56#p


|I/.&69%7H#%789 5$D (*=B%Bfi7*/49 #%9;!+9%7IrC!/.PU/.78.9A#'9;!$9%7rK}3$!Bfi&"!O.678#*d
~A:.7*Bfi7
r


:.7</!$9;=B%!+/.=>FO7*Bfi#.-QK


:.7<!Bfi&69;:>F789'&)(H5?,7*B'!$9%5?Bfi#0/Fd21Gd3d54]!/.P


:.7<Bfi78"!$9%&65?/5?,7*B%!+9%5?Bfi#.6Gd27Gd28Gd:9Gdj !/.P F
j! K


:.7<O5.567*!/15.,7*B%!$9'5?Bfi#

w

k dd<;



!/.PZK
fi

^K

] r<},
7dFK

fi



0 J

:.7G#;,78(8&"!$Z#%[>FO56#Fn( , ^d ,
,!Bfi7*/49;:.78#%78#A!/.P1(85.>>!K

:.7G#%7895$D!4Bfi&69;:>F789%&6(<78,Bfi78#%#'&)5./.#A&6#M(85?/.#%9;B%=.(89'78PCDBfi5.>9S:.7GDn56)5$~ &"/.01B%=.678#p
o4K\N 37*Bfi[C/?=>F7*BN&)(*!+-#%9S!$9%7M3!Bfi&"!O.675r(


<&6#A!4/1!Bfi&69;:>F789%&6(H78,Bfi78#'#%&65?/K
^{ K |/!$9S=B%!$/?=>FO^7*B &6#A!/1!4Bfi&69;:>F789%&6(<78,Bfi78#%#'&)5./K


^K bD # !4/.P ( !Bfi7F!Bfi&69;:>F789'&)(H78,BN78#%#%&65?/.#G!/.P>= &6#G!/1!Bfi&69;:>F789'&)(H5?,7*B'!$9%5?B8d^9;:.7*/
&)#M!/1!Bfi&69;:>F789'&)(H78,BN78#%#%&65?/K
&"/!$66[d9;:.7G#%789 5$DZDn5?B%>F=."!$#on4
o4KGn(

=

(

&)#07*/.7*B%!$9%78PO[C9;:.7<B%=.678#p

, !Bfi7MDn5?B%>F=."!$#*K
{^K uEBfi5?,^5#%&69%&65?/!$#%9;!$9%7M3!4Bfi&"!O.678#
r(
7
,

!/.P



#



^K bD # !/.P (
!HDX5.B%>F=.2!^K

<!4Bfi7MDX5.B%>F=.2!+#*K
!Bfi7]!4Bfi&69;:>F789%&6(F78,BN78#%#%&65?/.#<!/.P &)#<!CBfi78"!$9%&65?/T5?,^7*B%!$9%5.B8d9S:.7*/

1
K bDdi # di ( !/.Pvi %
!/.PaOi # (>] %



!Bfi7DX5?B'>]=."!$#d#%51!Bfi7a
r8K

#

rdaOi

# wZi (

r8d aOi

# k

(

r8d aOi

#

(

#

r8d-ai


# ;

&)#

(

(

r

u !Bfi7*/49;:.78#%78#M:!*347<9;:.78&"BM=.#;=!$ >F7*!/.&"/.0U!/.PU5.,7*B%!$9'5?Bfi#A:!37G9;:.78&"BG=.#;=!+-,Bfi&65?Bfi&69fi[!/.P!$#%#%5.(8&"!R
9%&63.&)9fi[C~ &69;:19S:.7G&DbR9;:.7*/4R786#%75.,7*B%!$9'5?BH0&637*/)5$~ 78#%9 ,Bfi&65?Bfi&69fi[K
n
fi
n
&6#W!#S=O.#%7895+DA9;:.71Dn5?B%>F=."!$#F5?/.6[wBfi7SDn7*B%Bfi&"/.09%5(*=B%Bfi7*/49#%9S!$9%7C3!Bfi&"!O.678#*K
:.78#%7DX5.B%>F=.2!+#_!4Bfi7G(*!$6678P8 , 8 ( 6 VK
@?

W|,A[W{CBED

|M)5+D9;:.7 #%[>FO56#E&"/F9S:.7_!+2,:!4O789E5+DDn5?B%>F=."!$#:!37 9;:.78&"B=.#;=!$>F7*!/.&"/.0<~ &69;:\9;:.7 &DbR9;:.7*/4R786#%7
5?,^7*B%!$9%5?
B # (>] % O78&"/.01!/1!OOBfi783.&"!$9%&65?/WDX5?BGaOi #Mk ( rwQa #'k % r8KXN!$(V:1/.=>F7*Bfi&6(*!$#%9;!$9%7
3!4Bfi&"!O.6
7 r+
7 <:!+#A!HI/.&69%7]B'!/.07S-aOr.r&jqP ] ]CFGFHF] I>.d?~M:.7*Bfi7
I*6PK
Y:.7Dn5?B%>!+E#%7*>!4/9%&6(8#F5$DA!TP5?>!$&"/zP78#%(*Bfi&",.9%&65?/ ! j J]J]5] * ]]] <r&6#0&637*/z&"/
9%7*B%>\# 5$D!/T L
| Kp
M|ON

{2B`CBV{=QP -R ST7

ge , , X
y?
(
*nb b

" , FV ,

`/ , ,
W



"8n nEbb , (.n F
ne VU O(
-,GW K jaEX ]Y][Z r W
" , b (. *4 ( , W Z p\X] _{ ^ e ,a`
V ,

/9;:.7TDX5665$~ &"/.0(85?/.#%9;B%=.(89'&)5./w5+DcKdA~ 778,Bfi78#%#U9S:.7/.78.9U#'9;!$9%7D=/.(89%&65?/}!$#U!9;B'!/.#%&69%&65?/
Bfi78"!$9%&65?/&
K $78:
9 dP7*/.59%79;:.7E#%7895$D^O^5?54)7*!4/<3$!$"=.78#m( , ] n , .K=BN9;:.7*B8d)789Z9;:.7Mye , S"8nb
y(?
*nb>
ep"fgd!$#%#%5.(8&"!$9%78P9%51!C#%78*
9 e]hifO7P7SI/.78PmO>
[ eFOa [

r j Oa [j
keAr8K % G&637*/!/
l
| K ~ 7 P7SI/.7 &69%#An ^Vbnb ,) n F
|ihmXn .X!+#!M#%7895$D9;BN&2,.678# ~ &69;:F(8:!B%!+(89%7*Bfi&6#%9%&6(
D=/.(89%&65?
/ |]a ]o] } &
r jOa }
Z Oa ]o r'r8K
Y:.7E#%789Z5$D#%9;!+9%782
# X5$D K 78=!$6# 9;:.7 #%789-5+D^!$6,^5#%#%&"O.67 3!4Bfi&"!O.67_!+#%#%&60?/>F7*/49%:
# X ja )

d%
r Wa
p-Tr8K-Y:.7A&"/,=.9 5$q
K &)#9;:.7 #%7895$D?fi5&"/49!$(89%&65?/.#5$D#%[.#%9%7*> !$07*/49%#EBfi7*,BN78#%7*/9'78P
r,
s-Sn[ tb ';finnnb"
- *fib'<$;b$ n; Z$; ESb$ nfi
R

fi)-
- -)



0

!$#G#%789'#*KY :!$9H&)#d&{ # ] { ( ]CFHFCFH] {,u vwuff>
&JDA!/.PQ5?/.6[&JD]aO{ # ] { ( ]CFHFCF] {u v\ucrx
Qy
z z P7*/.549%78# 9;:.7F/?=>FO7*B_5$D7867*>F7*/49%#M&"/

K
xz7GP7SI/.7<9;:.7G9;B'!/.#%&69%&65?/UBN782!+9%&65?
/ |Up_X) VX {dw5+DffK O[Cp
|]a ]o] } r&jU_|}
~



v

-rd~A:.7*Bfi7

VaO ] | ] } r ]

_[o hj| k

~A:.7*Bfi7 opX]~m>Xd&6#G9;:.7<9SB%!/.#%&69%&65?/Bfi78"!$9%&65?/Dn5?B5&"/49`!+(89%&65?/.#*~m5$DO^59;:T#%[.#%9%7*> !/.P
7*/43?&"Bfi5?/>\7*/9 !$07*/49%#*K-Y:.7 78?&6#%9'7*/9%&"!$=!/49%&JI(*!$9%&65?/>@!$78# 9;:.7A!+(89%&65?/.# 5$D7*/43.&2BN5?/>F7*/49!$07*/49%#
=/.(85?/49;Bfi56"!O.67d#'&2/.(87.|]aO ][oH] } r&)# 9SB%=.7d$&JD9;:.7*Bfi778?&6#%9'# #%5?>F75&"/49!+(89%&65?/H5$D7*/3.&"Bfi5?/>F7*/49E!$07*/49%#
#;=.(V:9;:!$9 9;:.7H(85?>FO.&2/.78P@54&2/49A!$(89'&)5.
/ |)j >!$78
# 8Oa ] | ] y})r_9;B%=.7K

Y:.719;B%!/.#%&69%&65?/Bfi78"!$9%&65?
/ F&6#W!(85?/N=/.(89'&)5./m5$DG9;:Bfi787UBfi78"!$9'&)5./.
# eH
!/.P &
8Oa ] | ] }
r j
eFOa | y}r
Oa | y}r
|r8K &)347*/U!/1!$(89'&)5.Z
/
{

j



8
r
?

F
!
*
(

=
'
B
fi
B
*
7

/

9
%
#
;
9
$
!
%
9
7


F


!
.
/
U
P
.
/
8
7
.


9
%
#
;
9
!$9%7
] ]
k
] ]
kf
]M]
} d.678\
9 ~q.Oa *r !/.P Oa ] } rEP7*/.549%7A9S:.7A3$!$"=.7<5$D-9S:.7`,Bfi78(85./.P&)9'&)5./@DX5?B'>]=."! !/.PC7S78(89Dn5?B%>F=."!
+
5

J


{
dBfi78#;,^78(89%&63786[K

e=pX)>~kVX
{d&6#A9;:.7*/CP7SI/.78PQO4
[ p
FaO

r j
] | ] }&

e

~:?aOr k aO ] } r _




H

P7SI/.78#9S:.7(85?/.#'9;B%!$&"/49%#<5?/z9;:.7C(*=B%Bfi7*/49<#%9S!$9%7!/.Pm/.78.9]#'9;!$9%75+DE54&2/49]!+(89%&65?/.#*KeD=BN9;:.7*B
7*/.#;=Bfi78#Z9;:!$9Z!$(89%&65?/.#Z~ &69;:M&"/.(85?/.#%&6#%9%7*/49 7S78(89%#(*!//.59ZO7 ,7*BDn5?B%>\78PG(85?/.(*=B%Bfi7*/49%6[d!$#eBfi78P=.(878#
9%5FD!$6#%7H&D !/4[W,!$&"BM5$DE!$(89%&65?/.#M&"/!M54&2/49G!$(89%&65?/U:!+#A&"/.(85?/.#%&6#%9'7*/9G7S78(89%#*K :.=.#*5d e !$6#%5#'9;!$9%78#
9;:.7MIBfi#'9 (85?/.P&69%&65?/mab#%787<.78(89%&65?/ 1.rDX5.B !35&6P&"/.0&"/9%7*BDX7*Bfi7*/.(87O^789~ 787*/C(85?/.(*=B'Bfi7*/9M!$(89%&65?/.#*K

pX)~kX d&)#<!]DB%!4>F7<Bfi78"!$9%&65?/T7*/.#;=Bfi&"/.019;:!+9A=/.(85?/.#%9;B'!$&"/.78PU3!4Bfi&"!O.678#<>!$&"/9S!$&"/
9;:.78&"B3!$"=.7
K $78"
9 Oa {.rP7*/.59%7<9S:.7G#%789 5$D (85?/.#%9;B'!$&"/.78PU3!4Bfi&"!O.678#A5$DE!$(89%&65?
/ {
K xQ7H9;:.7*/1:!3K7 p
e

aO

r j
] | ] }&



~A:.7*Bfi7.


j#

H

aOr
j=r } r




]

2

aO{.r8K


p~d7*/.#;=Bfi78#G9;:!+9 (85?/.(*=B%Bfi7*/49<!$(89%&65?/.#M(85?/.#%9SB%!$&"/U!C/.5?/T5$37*Bfi"!,,.&"/.01#%789M5$DE3!BN&2!4O.)78#

!/.P19;:.=.# #%9S!$9%78# 9;:.7G#'78(85?/.PU(85?/.P&69%&65?/WDX5.B !35&6P&"/.0&"/9%7*BDX7*Bfi7*/.(87O^789~ 787*/C(85?/.(*=B'Bfi7*/9M!$(89%&65?/.#p
|rj

~A:.7*Bfi70|
-


(

TVWXW

P7*/.59'78# 9;:.7G#%789aO{

#y] { (

476 < 6 6 +








&

r z {



+ H



aO{ # r%$VaO{ ( rXj&_


r
(|(|
#] { (
k

'Uv6 -6

]

{ #.jU
{ ( ?K
!




Y5O=.&6)PQ!/ h
|
Bfi7*,Bfi78#%7*/49%&"/.0C9;:.7G9;B%!/.#'&)9'&)5./UBfi78"!$9%&65?/2|]aO ]oH] } r 5$D 9;:.7]|}5$D!\P5?>!$&"/
P78#%(*Bfi&",.9%&65?/!ja J]&]5] M8 ]]] <r8d$~ 7<>F=.#%9 P7SI/.7!F#'789 5$DEO5.567*!/T3!Bfi&"!O.678#G9%5CBfi7*,Bfi7SR
#%7*/49G9;:.7F(*=B%Bfi7*/49G#%9;!$9')
7 d9;:.7M54&2/49<!$(89%&65?/T&"/,=.9 d!/.PQ9;:.7/.78.9G#%9;!$9'7) } KH|A#G&"/Q.78(89'&)5./71K{
~ 7HIBfi#%9GO=.&)6Pm!C9;B%!/.#'&)9'&)5./BN782!+9%&65?/T~ &69;:9;:.7Mfi5&"/9G!$(89%&65?/.#H5$DO59S:U#'[?#%9'7*>y!/.P7*/3.&"Bfi5?/>F7*/49
!$07*/49%# !$# &2/,=.9M!/.P9S:.7*/1Bfi78P=.(87G&699'5\!9;B%!/.#'&)9'&)5./1Bfi78"!$9%&65?/@~ &69;:C5?/.6[<fi5&"/49_!+(89%&65?/.# 5$D#%[?#'9%7*>
!$07*/49%# !$# &"/,=.9*K
[ pA!+#%#;=>F7!$(89%&65?7
/ {&)#H&6P7*/9%&JI78P
5&"/9<!$(89'&)5./U&"/,=.9%#F!Bfi7Bfi7*,Bfi78#'7*/9%78Pz&2/T9;:.7\DX5665$~ &"/.01~ !*C
O4[!/?=>FO7*B !/.P(*!/O7Q,7*BDX5?B'>F78PO4[w!$07*/4>
9 E
K { &6#19;:.7*/P7SI/.78P9%5mO7z9;:.7Q!$(89%&65?/
RKfi

fi 0 J



5$D`!+07*/9Ed&JDA9;:.7U/.=>FO7*BH78,BN78#%#%78Pm&"/mO.&"/!Bfi[mO[!T#%789F5$DGO5.567*!/3!BN&2!4O.)78#Ve dE=.#%78Pm9%5
Bfi7*,Bfi78#'7*/9 9S:.7]!$(89%&65?/.#5$ff
d&6#M78=!$9%5 KEuBfi5?,^5#%&69%&65?/!$#'9;!$9%7G3$!Bfi&"!O.678#`!4Bfi7<Bfi7*,Bfi78#%7*/49%78PO[1!
#%&"/.067O^5?567*!/3!Bfi&"!O.67dZ~A:.&667/.=>F7*Bfi&6(*!$E#'9;!$9%7H3!Bfi&"!O.678#F!Bfi7Bfi7*,Bfi78#%7*/49%78P&"/QO.&"/!Bfi[QO4[!@#%789
5$DEO5.567*!/13$!Bfi&"!O.678#*K
$78
9 e ]^___] e !/.j
P e" ]_^__] e C P7*/.59%7F#%789%#M5$DO^5?567*!/T3$!Bfi&"!O.678#<=.#%78P9%51Bfi7*,Bfi78#%7*/49
9;:.754&2/49A!$(89'&)5./5$DZ7*/3.&"Bfi5?/>F7*/49A!/.PC#%[.#%9%7*>!$07*/49%#*KZ=BN9;:.7*B8d.678
9 [,I !/.
P [ } P7*/.59'7A9;:.7 $ e
O5.567*!/3$!Bfi&"!O.67U=.#%78P9%5Bfi7*,BN78#%7*/9H#%9;!$9'73!BN&2!4O.)Z
7 r
U&2/9S:.7(*=B%Bfi7*/49!/.P/.78.9<#'9;!$9%7K
:.7GO5.567*!/@3!Bfi&"!O.678#A!4Bfi7 5?BfiP7*BN78P~ &69;:\9;:.7A&"/,=.9 3!4Bfi&"!O.678# IBfi#%9*d+DX54)65$~ 78PWO4[!/@&2/49%7*Bfi67*!3.&2/.0
5$D 9;:.7<O5.567*!/3!Bfi&"!O.678#G5$D (*=B%Bfi7*/49 #%9S!$9%7G!/.PU/.78?9M#%9;!+9%7A3$!Bfi&"!O.678# p
e

e


FCFCF

[ I#


[ I#




[ } I#



0#FHFCF

e

[wI


[ }

FCFCF

[ } I#


e

FCFCFO

[

C






FHFCF





[ }

]

~A:.7*Bfi7V &6#]9S:.7/?=>FO^7*B<5+DO^5?54)7*!4/Q3$!Bfi&"!O.678#=.#%78P9%5UBfi7*,Bfi78#'7*/9F#%9;!+9%7F3!Bfi&"!O.67ZrH !/.PQY&)#
78=!$9%5 z J z K :.7 (85./.#%9;B%=.(89%&65?/\5$D |m
&6# =.&69%7#%&">F&62!4BE9'5G9;:.7 (85?/.#%9SB%=.(89%&65?/H5$DM|m&"/.78(89%&65?/1K{K
|G/ h ii Bfi7*,BN78#%7*/9'&2/.0m!650&6(*!$M78,Bfi78#%#%&65?/ &)#1O=.&6691&2/9;:.7T#%9;!/.P!4BfiPm~ !*[an Bfi[?!/49*dMo*prK
|GBfi&69;:>F789%&6(<78,Bfi78#%#'&)5./.#`!BN7`Bfi7*,BN78#%7*/9'78P!$# 6&6#%9%#5$D h ii #AP7SI/.&"/.0C9;:.7G(85?B%BN78#;,5?/.P&"/.01O.&"/!Bfi[
/.=>]O^7*B8KEY :.78[1(854)"!,.#'7<9%5#'&2/.04)7 h ii #M~A:.7*/UBN782!+9%78PO4[W!4Bfi&69;:>F789%&6(]BN782!+9%&65?/.#*K
Y5UO=.&66P!/ h ii e
P7SI/.&"/.0Q9;:.7@(85?/.#%9;B%!+&2/49%#H5$D9;:.7Hfi5&"/9F!$(89%&65?/.#*d~7C/.7878P9%5Bfi7SDn7*BG9%5
9;:.7H3!$"=.78#G5$DE9;:.7FO5.567*!/T3!BN&2!4O.)78#<BN7*,Bfi78#%7*/49%&"/.019;:.7]!$(89'&)5./.#*
K $789 -r O^7F9;:.7GD=/.(89%&65?/T9;:!$9
>!,.#G!/!$07*/4.
9 m9%5C9;:.7<3$!$"=.75$DE9;:.7O^5?567*!/3!Bfi&"!O.678#<Bfi7*,BN78#%7*/9'&2/.0T&69%#`!+(89%&65?/!/.P)78

9 {?r
O7G9S:.7G&)P7*/49%&JI7*B3!$"=.7<5+D!$(89'&)5.%
/ {
K =BN9;:.7*B 6789 ~ Oa {.r !/.P Oa {.rP7*/.59%7 h ii Bfi7*,Bfi78#'7*/9;!+9%&65?/.#
5$D 9;:.7<,Bfi78(85?/.P&69%&65?/!/.P17S78(89 Dn5?B%>F=."!F5$DE!/1!$(89'&)5.
/ {K e &6#A9S:.7*/10&637*/1O4[ p


e:
j


?
-r



a-r&j=aO{?r&

~
aO{?r k

Oa {.r _

j
Z

{+


59%7G9;:!$9 6 50&6(*!$Z5?,^7*B%!$9%5?BN#_/.5$~P7*/.549%7G9;:.7G(85?B%BN78#;,5?/.P&"/.0 h ii 5?,^7*B%!$9%5?BN#*K
|G/ h ii Bfi7*,Bfi78#%7*/49%&"/.0C9;:.7MDB'!>F7<Bfi78"!$9%&65?/ }
(V:!4/.078#A&"/T!F#%&">F&6"!B ~ ![Cp
U
j







v




?
-r

aZr'j=aO{?r&

r>
>
3 aO{.r%r%r }I jUaIC
]

k
Z

{+


~A:.7*Bfi7>a{?r&)#@9;:.71#%789F5$DA(85?/.#'9;B%!$&"/.78Pm3!4Bfi&"!O.678#5$DG!$(89%&65?/Q{Q!/.PaIZj y}I 78,Bfi78#%#%78#@9;:!$9!$6
(*=B%Bfi7*/49A!/.PU/.78.9 #%9S!$9%7<O5.567*!/3!Bfi&"!O.678#GBfi7*,Bfi78#%7*/49%&"/.
0 r!Bfi7<,!$&"Bfi~&)#'7<78=!$nKY :.7G78,Bfi78#%#'&)5./
r>

3 Oa {?r783$!$"=!$9%78# 9%v
5 ( , 5?.
B n , !/.P1&6#ABN7*,Bfi78#%7*/49%78PO4[9;:.7 h ii DX5?B ( , 5?.
B n , KEY :.7
!$(89%&65?/C&"/49%7*BDn7*Bfi7*/.(87F(85?/.#%9;B'!$&"/9 &)#M0&637*/1O4[p


j




a{



# ] ( r


{
# ] ( r
>a # ] (



# &
r jUaO{ # &
r

(

r
RR



( r

jU
aO{ ( r k
!

fi

0




a{



r

# ] (



)-
- -)

# r&jUaO{ # r&

( r


(



r
>a # ] (
#] { (

jU
aO{ ( r ]
!

r

~A:.7*Bfi7*a #] ( r&jqaO{ #y] { ( r z aO{ #] { ( r
# r ( r k aO{ # r2$VaO{ ( r j#
&?K
!
&"/!$66[U9S:.7 h ii Bfi7*,Bfi78#%7*/49%&"/.0C9;:.7G9;B%!/.#'&)9'&)5./UBfi78"!$9%&65?/ |
6
&

#

9
.
:
<
7
8
(
?
5

/
%

=/.(89%&65?/15+D <
e
!/.P


~
6
&
;
9
1
:
$
!
8
(
%
9
6
&
?
5

/

3

!
fi
B
"
&

!
.

6

8
7

#
$
5


;
9
.
:
G
7
*
7

/
.
3
"
&
fi
B
?
5

/
F
>
*
7
4
/
G
9
$
!

0
*
7
4
/
%
9
#
8
7
?

6
&
%
#
%
9
*
7
4
/
%
9
"
&
$
!
6

6

U
[



=

!
4
/
%
9
J
&


8
7

P
p

| jU\e ]HFCFCF] e >_ e k
'Y,^CBaCB)i{B{ Q_W|jW{VCBaCBV{



k

_

xH|-CB)i{

:.7<!$605.Bfi&69;:>F# ~ 7<=.#%7MDn5?B07*/.7*B%!$9'&2/.01=/.&637*Bfi#;!+-,."!/.#A!$6(85?/.#'&)#'9A5$D !O!$(8~ !BfiPC#%7*!BN(V:@DBfi5?>
9;:.7M#%9;!$9'78#E#;!$9'&)#NDX[.&"/.0]9S:.7A05.!$(85?/.P&69%&65?/C9'5<9;:.7M#%9;!$9'78#E#;!$9'&)#NDX[.&"/.09;:.7M&2/.&69%&"!$(85?/.P&69%&65?/KXNE>@,.&2BR
&6(*!$E#'9;=.P&678#<&"/>\5?P78(V:.78(8&"/.0:!*347#;:.5$~A/T9;:!$9G9;:.7C>F5#%9G(85?>@,.)78z5?,7*B%!+9%&65?/TDn5?BM9;:.&6#]4&"/.P
5$D !$605?Bfi&69;:>F#F/.5?B%>!+)6[&6#<9%5CI/.Pz9;:.7 , F , 5$D !C#%789<5$D 3?&6#%&69%78P#%9;!+9%78# J !4/N!/789<!$nK"d
o*pp4qr8K
M|ON

\| , n+]K
, EG8 , > z .
VX k

{2B`CBV{?Px|\BA

, F

,



J

e<e

,

jyaEX



]Y][Z r
}
Z aO [
Y]
]

r

, @V ,
_ }


J

b

J

hX
W

e
,

|w,BN78&2>@!$07E&6#Z#;!$&6PF9%5 78.&6#%9*d$&JD&69Z&)# /.5?/.7*>,.9fi[K 59%7E9S:!$9#%9;!+9%78#-!$"Bfi7*!$P[<O^7865?/.0&"/.0<9%5 J (*!/
!$6#%5FO7A!G,!BN9 5$D9;:.7 ,Bfi78&">!$07M5$D J KZ|A#%#S=>F7 9;:!$9 9S:.7 #%789E5+D3.&)#'&)9'78P#%9S!$9%78# !Bfi7ABfi7*,BN78#%7*/9'78PO[
!/ h ii 78,Bfi78#%#%&65?/ J 5./1/.78?9M#%9;!$9'7A3$!Bfi&"!O.678#<!/.P19;:!$9dDn5?B &69%7*B'!$9%&65?/1,=B%,^5#%78#*d
~ 7A~ !/49 9%5
07*/.7*B%!+9%7A9S:.7<,Bfi78&">!$07 ~ !$6#%578,Bfi78#%#%78P1&"/1/.78.9 #%9;!$9%7M3!4Bfi&"!O.678#*KE
5?B !F>F5?/.56&69;:.&6(<9;B'!/.#%&69%&65?/
Bfi78"!$9%&65?/ |
7 p
~ 7G9;:.7*/C(*!$6(*=."!$9%K


j

aO[

~

j

} _

} _ | k

r

G[\
3:[ <}
J

~A:.7*Bfi7 d"[
!4/.P[/
}-P7*/.549%7F&2/,=.9d(*=B%Bfi7*/49M#%9;!$9%7F!/.PQ/.78?9G#'9;!$9%7H3!Bfi&"!O.678#*dZ!/.Pm[,
3:[
} P7*/.549%78#
9;:.7F#;=O.#%9'&)9S=.9%&65?/T5$DE(*=B%Bfi7*/49A#'9;!$9%7H3!Bfi&"!O.678#H~ &69;:/.78?9M#%9;!$9'7<3!4Bfi&"!O.678#*K<Y :.7F#%789M78,BN78#%#%78PQO[
7 AO^7865?/.0# 9%5H9;:.7G,Bfi78&">!$07 5+D J !/.P
(85?/.#%&6#%9'#E5$D#%9;!+9%7 &"/,=.9A,!$&"Bfi#<Oa ]o rdDX5?B~A:.&6(V:@9;:.7M#%9;!$9'
9;:.7 &"/,=.9 >![H(*!=.#%7A!M9;B'!/.#%&69%&65?/HDBfi5?> 9%5<!M#%9;!+9%7 &2/ J K :.7&2/,=.9 5$D!/|Bfi7*,BN78#%7*/9'&2/.0
!Q,."!//.&"/.0wP5.>!$&"/m&6#1!#%7895$D<!$(89%&65?/.#KY :?=.#d Dn5?B!Q,."!//.&"/.0P5?>!$&"/m9S:.7178)7*>\7*/9%#1&"/
!Bfi7H#%9;!$9'7SR!$(89%&65?/U,!+&2BN#*KGY :.7F07*/.7*B%!$9%78P=/.&637*Bfi#;!$E,."!/.#G5+DE9;:.7]=/.&637*Bfi#S!$E,."!//.&"/.0!$605?BN&)9S:>F#
,Bfi78#%7*/49%78PF&"/F9;:.7 /.78.9 #%78(89%&65?/F!Bfi7 #%789%#Z5$D9;:.78#'7#'9;!$9%7SR!$(89'&)5./],!+&2BN#*KZxQ7 Bfi7SDX7*B9%5G9;:.7#%9;!$9'7SR!$(89%&65?/
,!$&"Bfi#<!$#H8 , *nbQ ( - , VdO78(*!=.#'79;:.78[U!$#%#'5?(8&"!$9%7F#%9S!$9%78# 9%51!$(89%&65?/.#M9;:!$9M(*!/O7,^7*BDX5.B%>F78P
&"/19;:.78#'7G#%9;!$9%78#K
Y:.7 h ii Bfi7*,Bfi78#'7*/9%&"/.0T9;:.79;B%!4/.#%&69%&65?/Bfi78"!$9%&65?/ |
!/.Pz9;:.7F#%789G5$D3.&6#%&69%78PQ#%9S!$9%78# J 9%7*/.P#
9%5FO7M2!4Bfi07d!/.P!F>F5?BN7 7S(8&67*/9 (85?>@,=.9;!$9%&65?/@(*!/O^7A5?O.9S!$&"/.78PUO[,7*BDX5?B'>F&"/.0]9S:.7A78.&6#%9%7*/49%&"!$
=!/9'&I(*!+9%&65?/Q5+D/.78.9<#'9;!$9%7F3!4Bfi&"!O.678#7*!Bfi6[&"/Q9;:.7C(*!$6(*=."!$9%&65?/an =Bfi(8:789F!$nK"dEo*p4po !/%!/
789-!$nK"d.o*pp4qr8KY5AP5 9S:.&)#d$9;:.7 9;B%!/.#'&)9'&)5./<Bfi78"!$9%&65?/H:!$#Z9%5 O7 #;,.6&69-&"/49%5`!_(85?/N=/.(89'&)5./<5+D^,!BN9%&69%&65?/.#
| j | #'kFCFHFk | bC!$6)5$~ &"/.0C9;:.7<>F5.P&JI78P(*!+)(*=."!$9'&)5.C
/ p


j

aO[

}b _ C
| b kFHFCF Oa [ } _ | (k
(

~

j

} _

Rt

aO[

} _ | #Mk
#

J

r%r

FHFCF

rG[\ 3q[

}

fi

0 J

:!$9 &6#*d | # (*!/1BN7SDX7*B9%5!+)3!Bfi&"!O.678#*d | ( (*!4/1Bfi7SDX7*B_9%5!$63!BN&2!4O.)78#M78.(87*,.9>[ } # | % (*!/1Bfi7SDn7*B 9%5
!$63$!Bfi&"!O.678#G78?(87*,.9[ } # !/.P#[ }( !/.PU#%5@5?/K
|M#Z#;:.5$~A/<O4[ !4/N!/H789 !$nKafio*pp4qr9;:.7(85?>,=.9;!$9'&)5./<9'&2>\7_=.#'78P]9'5A(*!$6(*=."!$9%79;:.7,Bfi78&">!+07 &)#
!G(85./378\D=/.(89%&65?/C5+D9S:.7`/.=>FO7*B5$D-,!Bfi9%&69%&65?/.#KEY :.7GBfi7*!$#%5?/]DX5?B9;:.&6# &6# 9;:!$9*d+DX5?B#%5?>F7G/.=>]O^7*B
5$DA,!Bfi9'&)9'&)5./.#*d !@D=Bfi9S:.7*BG#;=O^P&)3.&6#%&65?/m5+D_9S:.7,!Bfi9%&69%&65?/.#F~ &66 /.59<Bfi78P=.(87C9;:.7C9%59;!$Z(85?>,.678.&69[d
O78(*!4=.#%7G9;:.7M(85?>,.678?&69fi[C&2/49;Bfi5.P=.(878PUO4[9;:.7M2!4Bfi07*BE/.=>FO7*B 5+D h ii 5?,7*B'!$9%&65?/.# &6#_:.&60?:.7*B_9;:!/
9;:.7<Bfi78P=.(89'&)5./15$D 9;:.7G(85?>,.678.&69[T5$D-7*!$(8: h ii 5?,^7*B%!$9%&65?/K
Y:.7`Bfi7*,BN78#%7*/9S!$9%&65?/@5$D-9;:.7M650&6(*!$78,Bfi78#%#%&65?/@Dn5?B7*!$(V:1BN782!+9%&65?V
/ e<\d }!/.P :!$#AO^787*/C(*!Bfi7SR
D=.)6[F(8:.5#%7*/H#;=.(8:<9S:!$9-&69Z(85?/.#%&6#%9%# 5+D!M(85?/N=/.(89'&)5./<5+D#;=O78,Bfi78#%#%&65?/.# 9;:!+95?/.6[Bfi7SDX7*B9%5G! #S>!$6
#;=O.#%7895$D/.78.9 #%9S!$9%7M3!Bfi&"!O.678#*K :.&6#ABfi7*,BN78#%7*/9S!$9%&65?/1!$665$~ #A=.#M9%5F#%5?Bfi9 5?=.9 9;:.7G#;=O^78,BN78#%#%&65?/.#
&"/1(85?/%=/.(89%&637F,!Bfi9%&69%&65?/.#M~ &69;:1/.7*!B 5.,.9%&">!$#%&6878#M9;:!$9 #S!$9%&6#fiDn[9;:.7]!4O5$37GBfi78=.&"Bfi7*>\7*/9%#K
TVWXW'W 6







6 8058

&


4"
8O V


xQ7 IBN#%9 P78#%(*Bfi&"O7G9fi~ 5G,Bfi&65?BE!$605.Bfi&69;:>F# Dn5?B h ii RO!$#%78PU=/.&637*Bfi#;!+,."!//.&"/.0!/.PP&6#%(*=.#%# ~A:.&6(8:
4&2/.Pz5$DP5?>@!$&"/.#G9;:.78[!BN7]#S=.&)9S!O.67FDX5.B8KGxQ7F9;:.7*/,Bfi78#%7*/49<!1/.78~!$605?BN&)9S:> (*!$6)78P n "8nb
6
bG9;:!$9 &6#M#;=.&69;!O.67GDn5?B #%5.>F7GP5?>!$&"/.#A/.549 (85$37*Bfi78PUO4[9;:.7F,Bfi&65?B !$605.Bfi&69;:>F#*K
Y:.79S:Bfi787 =/.&)347*Bfi#;!$,.2!4//.&2/.0F!$605?Bfi&69;:>\# P&)#'(*=.#%#%78P1!Bfi7 !$6O!$#%78P5./]!4/<&69%7*B'!$9%&65?/H5$D,BN78&2>]R
!$07(*!$6(*=."!$9%&65?/.#*KEY:.7E&69%7*B%!$9'&)5./G(85?B%BN78#;,5?/.P# 9'5A!A,!4B%!$6678O!$(V4~ !BfiP<OBfi7*!+P9;:4RnIBfi#%9Z#%7*!Bfi(8:G#'9;!Bfi9fiR
&"/.0!$9H9;:.705.!$-#'9;!$9%78#<!/.Pz7*/.P&2/.0z~A:.7*/Q!$6E&"/.&69%&"!$ #%9;!+9%78#<!Bfi7F&"/.(8"=.P78Pw&"/Q9S:.7#%789G5$D 3.&)#'&)9'78P
#%9;!+9%78#<ab#%787G &60?=Bfi7Fr8K :.7<>!$&"/P&7*Bfi7*/.(87FO789fi~ 787*/19;:.7<!+)045?Bfi&69;:>F# &6#M9;:.7G~ ![9;:.7],BN78&2>@!$07
&6#AP7SI/.78PK

Pre3
Pre2
Pre1

Init

Goal

&60?=Bfi7<p|`/@&662=.#'9;B%!$9%&65?/5$D9;:.7<,!B%!+)678O!$(8~ !BNPWOBfi7*!+P9;:4RnIBfi#%9 #%7*!BN(V:C=.#%78P1O[ h ii RO!$#%78P
=/.&637*Bfi#;!$Z,.2!4//.&2/.0U!+)045?Bfi&69;:>F#*d.(85?>@,=.9%&"/.0W,Bfi78&">!+078#`uBfi7oduEBN7*{!/.PUuEBN7*K

CCV{5W{2m

|,x|\BA

\|

"& >!$9'9%& 789]!$nKEafiopp!r&"/9SBfi5?P=.(878#F9fi~ 51P&J7*BN7*/9F&"/.P#F5$D_,BN78&2>@!$078#<(*!+)678P8nfi!4/.Pg ;,
, F , VK|#%9SBfi5?/.0,Bfi78&">!$07H&6# P7SI/.78PQO[Cp
M|ON

\| Tm ,

fon+K jaEX
, EG8 , z .
VX k

][Y]Z

r , GV ,
aO ][o
r h J \b
Y_Z

J

h

Vnfi4 , F , J e<e ,


:.=.#*d$Dn5?BZ!G#%9S!$9%7dMO7865?/.04&2/.0F9%5G9S:.7#'9;Bfi5?/.0<,Bfi78&">!+07 5$D!M#%789 5$D#'9;!$9%78# J d9;:.7*Bfi7 78.&6#%9%#!+9 )7*!+#%9
5?/.7!+(89%&65?/ ~A:.7*BN7!$6E9;:.7@9;B%!/.#%&69%&65?/.#DBN5?> !+#%#%5.(8&2!+9%78PQ~ &69;: )7*!+P&"/49%5 J K 5?/.#'&)P7*B9;:.7
X

W

e

{2B`CBV{PCCV{5Qx|\BA



,

Ryx

fi 0



)-
- -)

78!>,.67 #;:.5$~A/F&"/F &60?=BN7AK :.7 P59'#E!/.P!B%BN5~#-&"/F9;:.&6#I0?=Bfi7 P7*/.59'7 #%9;!$9'78#E!/.P9;B%!4/.#%&69%&65?/.#
Dn5?BA!4/|~&)9S:Q!#'&2/.04)71/.5?/4RP789%7*B'>F&"/.&)#'9%&6(W!$(89'&)5./K<
5?B 9S:.7#%789M5$D#%9S!$9%78#G<T#;:.5$~A/T&"/9;:.7
I0?=Bfi7d49;:.7 9;:BN787#'9;!$9%78#E:!3.&2/.0F!M9;B%!/.#'&)9'&)5./F&"/9%5F<F!Bfi7 9;:.7#%9;Bfi5?/.0<,BN78&2>@!$07 5$D<UaX&2/.P&6(*!$9'78P
O4[W!F#'56&)PT786)&",.#%7C!/.P1"!O786678P,Bfi7o*rd!$#A!$69SB%!/.#%&69%&65?/.# DBfi5?>9;:.78#'7<#%9;!+9%78# )7*!+PU&"/9'5<K
| |,:|BA \| &6#G78=!$Z9%51!/15.BfiP&"/!Bfi[,BN78&2>@!$07!$#MP7SI/.78P&"/G7SI/.&69%&65?/{KMY :.=.#*d

&"/1 &60?=Bfi7F!+)9;:.7<#%9SBfi5?/.0,Bfi78&">!+078#`!4Bfi7<!$6#%5C~7*!4,Bfi78&">!+078#*dO=.9M9;:.7],BN78&2>@!$078#M#;:.5$~A/1O[
P!$#;:.78PC7866&2,.#'78#`!BN7A5?/.6[~ 7*!F,Bfi78&">!+078#*d!$# 9;:.7 P!$#S:.78P19;B%!/.#%&69%&65?/.# P5F/.59 #;!$9%&6#fiDn[F9;:.7A#'9;Bfi5?/.0
,Bfi78&">!$047<P7SI/.&69%&65?/K

Pre4
Pre3
Pre2
Pre1
GS

&60?=Bfi7<pM.9;Bfi5?/.0!/.PT~ 7*!,BN78&2>@!$07<(*!+)(*=."!$9'&)5./.#*K .56&6P7866&",.#%78#<P7*/.549%7],Bfi78&">!+078# 9;:!$9M!Bfi7
O59S:<#'9;Bfi5?/.0<!/.PF~ 7*!
d$~A:.&667AP!$#S:.78P7866&",.#%78# P7*/.59%7A,Bfi78&">!+078#E9;:!+9E!Bfi7 5?/.6[F~7*!4K
G/.6[5./.7\!+(89%&65?/&6#]!$#'#;=>F78P9%5178.&)#'9<&"/9;:.7FP5?>!+&2/KYZB%!/.#'&)9'&)5./.#<(*!=.#%&"/.0!C#%9;!$9%7
9%51O7865?/.019%51!~ 7*!T,Bfi78&">!$07B%!+9;:.7*B 9S:!/U!@#%9;Bfi5./.0W,Bfi78&">!+07!Bfi7FPB%!~A/1P!+#;:.78PK
:.7G#%789 5+D05.!$#%9;!$9%78# &6#G>!B%$78P;H.K

@?

CCV{5W{2:CCV{5
:D BED>)W{u{2B"{5

|w#'9;Bfi5?/.0 5.B#%9SBfi5?/.0M(8[?(86&6(A,."!/F&6# 9;:.7 =/.&)5./F5$D9;:.7 #%9;!+9%7SR!$(89%&65?/HB%=.678# Dn5?=/.PF~A:.7*/H(*!$6(*=."!$9%&"/.0
9;:.7<,Bfi78&">!+078#`/.78(878#'#;!Bfi[@DX5.B (85$37*Bfi&"/.09;:.7G#%7895$D&"/.&69%&"!$Z#%9;!$9'78#]a &6#AP7SI/.78P&"/U.78(89'&)5./Uqr8K
.9SBfi5?/.0,."!//.&"/.0m5?/.6[m(85?/.#%&6P7*Bfi#1#'9;Bfi5?/.0,BN78&2>@!$078#*KbD]!z#%78=.7*/.(875+D`#%9SBfi5?/.0,Bfi78&">!+078#
#%9;!4Bfi9%&"/.0Q!$99;:.71#%789@5$DA05?!+#'9;!$9%78#F(*!/mO^7U(*!$6(*=."!$9%78Pd #;=.(8:9;:!$9@9;:.71#%7895$DM&2/.&69%&"!$M#%9;!+9%78#F&)#
(85$37*Bfi78PdZ#%9;BN5?/.0Q,.2!4//.&2/.0z#;=.(8(87878P#W!4/.PBfi789S=B%/.#F9;:.7W=/.&637*BN#;!$ ,."!/(85?/.#%&6#%9%&"/.0Q5+DA9;:.71=/.&)5./
5$DE!$6-9S:.7<#%9;!+9%7SR!$(89%&65?/1B%=.678#G5$D 9;:.7H(*!$6(*=.2!+9%78P#%9;Bfi5?/.0C,Bfi78&">!$078#*K M9S:.7*Bfi~ &6#%7F&69 D!$&66#\an_&2>@!$9%9%&
789A!$nK"do*ppOrK
_5?/.#%&6P7*BF9;:.7C78!4>,.67U&"/ &60?=Bfi7UKz|A#P7*,.&6(89%78P&"/m9;:.7@I0?=Bfi7d !T#%9;Bfi5?/.0Q,Bfi78&">!+071(*!/
O7\DX5?=/.P&"/9;:.7HIBfi#'9<,Bfi78&">!$07F(*!$6(*=."!$9%&65?/d O=.9G5?/.6[Q!C~ 7*!1,Bfi78&">!$07@(*!/O7FDn5?=/.P&"/9;:.7
#%78(85?/.PC(*!$6(*=."!$9%&65?/K :.=.#*d#%9SBfi5?/.0],."!//.&"/.05./.)[@#;=.(8(87878P# &"/9S:.&)# 78!>,.67d&JD-9;:.7#%789 5$D&2/.&69%&"!$
#%9;!+9%78# &)#M(85$37*Bfi78PUO4[9S:.7MIBfi#%9A,Bfi78&">!+07]!/.P19S:.7G#%789 5$D 05?!$#%9S!$9%78
# K
.9SBfi5?/.0,."!//.&"/.0&6# (85?>@,.)789'7<~ &69;:1Bfi78#S,78(89 9%5F#%9SBfi5?/.0F#%5"=.9%&65?/.#*KbDE!<#'9;Bfi5?/.0,."!/178.&6#%9%# Dn5?B
#%5?>\7\,."!//.&"/.0Q,Bfi5?O.67*> 9;:.7F#%9;Bfi5?/.01,."!//.&"/.0Q!$605?Bfi&69;:> ~ &66BN789;=B%/T&69*d59S:.7*Bfi~ &6#%7d&69<Bfi789S=B%/.#
R

fi

0 J

9;:!$9/.5Q#'5"=.9%&65?/78?&6#%9'#*Km.9;Bfi5?/.0Q,."!//.&"/.0&6#1!$6#%55?,.9%&">!$P=.719%59;:.7UOBfi7*!$P9;:4RnIBN#%9#%7*!BN(V:K
:.=.#*d!F#'9;Bfi5?/.0,."!/1~&)9S:C9;:.7GDn78~ 78#%9 /?=>FO7*B5$D #%9%7*,.# &"/9;:.7G~ 5?Bfi#%9 (*!$#%7G&6#GBfi789;=B%/.78PK
.9SBfi5?/.0F(8[?(86&6(,."!//.&"/.01&)#G!Bfi78"!$.78P137*Bfi#'&)5./15$D #%9;Bfi5./.0\,."!//.&"/.0dO78(*!4=.#%7<&69G!$6#%5(85?/.#%&6P7*Bfi#
~ 7*!U,BN78&2>@!$078#*KF.9;Bfi5?/.01(8[.(86&6(W,."!//.&"/.0TI/.P#]!C#%9SBfi5?/.01,.2!4/d&JD &)9H78?&6#%9'#*K]M9S:.7*Bfi~ &6#%7d~M:.7*/
=/!O.67M9%5 I/.PC!M#%9;Bfi5?/.0<,BN78&2>@!$07 9;:.7 !$605.Bfi&69;:> !$PP#!M~ 7*!H,Bfi78&">!$07K 9 9;:.7*/F9;Bfi&678#9%5`,B'=/.7
9;:.&6#G,Bfi78&">!$07<O4[WBN7*>F5$3?&"/.01!$6#%9;!$9'78#9S:!$9 :!*347G9;B%!/.#%&69%&65?/.#M67*!$P&"/.05.=.9 5$D 9;:.7<,Bfi78&">!$07F!/.P
9;:.7F#%789M5$D3.&6#%&69%78PQ#%9S!$9%78# J K<XD&69G#;=.(8(87878P#*d9;:.7Bfi7*>!$&"/.&"/.01#%9;!$9'78#A&"/9;:.7,Bfi78&">!+07!Bfi7]!+PP78P
9%5 J !4/.P&69<!$0.!$&"/U9SBfi&678#G9%5W!+PPQ#%9;Bfi5./.0W,Bfi78&">!+078#*K<XD&69MD!+&)6#*d&69<!$PP#<!1/.78~<d~ 7*!U,BN78&2>@!$07
!/.PUBfi7*,^7*!$9%#M9;:.7<,B%=/.&"/.01,Bfi5?(878#'#\an_&2>@!$9%9%&789A!$nK"do*p4p!r8K
_5?/.#%&6P7*BA!+0?!$&"/19;:.7H78!4>,.67]&"/T &60?=Bfi7FK :.7F#;:.5$~A/C#%78=.7*/.(87F5$D,Bfi78&">!$07H(*!$6(*=."!$9%&65?/.#
(85?=.6PQ:!37<O787*/T(85?>,=.9'78PO4[9;:.7H#%9;Bfi5./.0(8[?(86&6(,."!//.&"/.0U!$605?Bfi&69;:>1K :.7F!$605?Bfi&69;:> ,Bfi7SDn7*Bfi#
#%9;BN5?/.0Q,Bfi78&">!$078#*d&JD`9S:.78[Q78?&6#%9dE#%5z9;:.7IBN#%9!$PP78Pw,Bfi78&">!+07anuEBN7o*rH&)#@#%9;Bfi5?/.0
KQ 5Q#'9;Bfi5?/.0
#%78(85?/.Pw,BN78&2>@!$07C78?&6#%9%#!/.Pm9S:.7~ 7*!,Bfi78&">!$047anuEBfi7*{4rG(*!//.59O^7U,B%=/.78Pm9%5U5./.)[(85?/49;!$&"/
#%9;!+9%78#E/.59E:!3?&"/.0H9;B%!/.#%&69%&65?/.# 67*!$P&"/.0F5?=.9 5$D9;:.7A,Bfi78&">!+07A!/.P9S:.7#'789 5$D3?&6#%&69%78P#'9;!$9%78#*KZY :.=.#*d
9;:.7G#%9SBfi5?/.0(8[.(86&)(!$605.Bfi&69;:>65?5.#DX5.B !/.59;:.7*B~ 7*!C,Bfi78&">!$07K :.&6#G,Bfi78&">!$071anuEBfi7*4r:!+#A/.5
5?=.9%045&"/.09;B%!/.#'&)9'&)5./.#*d~A:.&6(8:U>F7*!/.#9;:!$9 9;:.7G#'78=.7*/.(87H5$D~ 7*!C,Bfi78&">!$0478# (*!/UO^7<9%7*B'>F&"/!$9%78P
!/.PC9;:.7G!$605?Bfi&69;:> (*!/BN789;=B%/@9%5<65.5?HDn5?B #%9SBfi5?/.0<,Bfi78&">!$0478#auEBfiy7 1?rKXD-9;:.7 #'7895+D&"/.&69%&"!$#'9;!$9%78#
!Dn9%7*BE!+PP&2/.0,Bfi78&">!+07`uBfiy7 1<(85$37*BN# 9;:.7A#'789E5$D&"/.&69%&"!$#%9;!+9%78# 9;:.7G!$605?Bfi&69;:> #;=.(8(87878P#*d59S:.7*Bfi~ &6#%7
&69<(85?/49%&"/.=.78#\=/49%&6E78&69;:.7*BF/.5T#%9;Bfi5?/.015.B<,B%=/.78P~ 7*!U,Bfi78&">!+07(*!4/QO7@DX5.=/.Pab&"/Q~M:.&)(8:z(*!$#%7
9;:.7<!$605.Bfi&69;:> D!$&66#;r5?B9;:.7G#%789 5$D-3.&6#%&69%78PU#'9;!$9%78# (85$37*Bfi# 9;:.7G#%789 5$D &"/.&69%&"!$#%9;!+9%78#<ab&"/C~A:.&6(V:(*!$#%7
9;:.7<!$605.Bfi&69;:>#;=.(8(87878P#;rK
| #'9;Bfi5?/.0C(8[?(86&6(,."!/T5?/.6[U0.=!B%!/49%7878#G,Bfi50?Bfi78#'#A9%5$~ !BfiP#9;:.7<05.!$-&"/T9;:.7F#%9;BN5?/.0,!Bfi9'#*KA/
9;:.7G~ 7*!C,!Bfi9%#d(8[?(8678#M(*!/15.(8(*=B8K Y5@7*/.#;=Bfi7<9S:!$9 9;:.7],."!/)7*/.049;:1&6#I/.&)9'7d~ 7<>F=.#%9A!$#'#;=>F7
9;:!$9G9SB%!/.#%&69%&65?/.#H)7*!+P&2/.05?=.9G5+D_9S:.7~ 7*!U,!BN9%#G7837*/49;=!$66[Q~ &66O7C9;!+7*/KY :.7!$605?BN&)9S:> &)#
(85?>,.6789%7G~&)9S:1Bfi78#;,78(899%5F#%9;Bfi5?/.0F#%542=.9'&)5./.#*d!$# !F#%9SBfi5?/.0<#'5"=.9%&65?/1~ &66-O^7<Bfi789;=B%/.78Pd&JD &69 78?&6#%9'#*K
@

CC|{5_V^W{2lBAjBaCB)i{ z,CV{5~W{2=CV{5n
qD EBDsW{8{2B{

|G/1&">,5.Bfi9;!/49 Bfi7*!$#%5?/@Dn5?B #'9;=.P[?&"/.01=/.&637*Bfi#;!$Z,.2!4//.&2/.01&6#M9;:!$9A=/.&637*BN#;!$-,."!//.&"/.0U!$605?BN&)9S:>F#
(*!/,BN53.&6P7 #%9;!+9%7SR!$(89%&65?/FB%=.678# 9%5G(85?>,.6789%786[:!4/.P)7G!G/.5?/4RP789%7*B'>F&"/.&)#'9%&6(G7*/3.&"Bfi5?/>F7*/49*K :.=.#*d
&JD!w,."!/78.&6#%9%#1Dn5?B1,!$&"/49%&"/.09;:.7T5?5?Bd`!4/!$07*/49178?78(*=.9'&2/.0!m=/.&637*Bfi#S!$],."!/~&)6!$6~ ![?#
!35&6Pw,!$&"/49%&"/.0&69%#%78JD<&"/49%59S:.71(85?B%/.7*BF5?B@Bfi7*!$(8:m!/[59;:.7*B@=/Bfi78(85$37*B%!O.67UP7*!+P.R7*/.PK.9;Bfi5?/.0
,."!//.&"/.0!4/.Pm#%9;Bfi5?/.0(8[.(86&)(Q,."!//.&"/.0m!$605?Bfi&69;:>F#@(85?/9SBfi&"O=.9%7UO[m,Bfi5$3.&)P&"/.0Q(85.>,.6789%7 h ii R
O!$#%78P!$605?Bfi&69;:>\# DX5.B=/.&637*BN#;!$ ,.2!4//.&2/.0
K
M/4Dn5?Bfi9;=/!$9'78)[d Bfi7*!+R~ 5?BN)PP5?>!+&2/.#F(*!/z:!*347P7*!$P.R7*/.P#9;:!+9]!Bfi7C/.59]!$6~ ![?#F!35&6P!O.67K
5?/.#'&)P7*BdDn5?B78!>@,.)7d.(8:.5?,,^7*Bfi# LBfi5?O549fiRO!O4[1P5?>!$&"/1P78#'(*Bfi&"O78PQ&"/.78(89%&65?f
/ 1K |M#AP7*,.&6(89%78P
&"/T &)0.=Bfi7d/.5U=/.&637*BN#;!$ ,.2!4/BN7*,Bfi78#%7*/49%78PQO[!#%789M5$DE#%9S!$9%7SR!$(89%&65?/TB%=.678#<(*!/0?=!B%!4/9%787H9;:.7
05?!+9%5FO7<Bfi7*!$(8:.78PC&2/1!I/.&69%7G5?B&"/4I/.&69%7]/.=>FO7*B5$D-#'9%7*,.#*d!$#!+)Bfi786783$!/9M!$(89%&65?/.# >![67*!$P19%5
!/1=/Bfi78(85$37*B'!O.67<P7*!$P.R7*/.PK
|>F5?Bfi7F&"/49%7*Bfi78#%9%&"/.0T78!>@,.)7@&)#<:.5$~ 9%51047*/.7*B%!$9%7!1=/.&637*Bfi#;!+E,.2!4/TDX5.BG!#%[.#%9%7*> 9S:!$9G(*!/
O71&"/!O!+Pm#%9;!$9%7d05?5.Pm#%9;!+9%75?B!4/=/Bfi78(85$37*B'!O.671D!$&6)78P#%9;!$9'7abP7*!+P.R7*/.Pr8K|A#%#S=>F719;:!$9
!$(89%&65?/.#(*!/1O7G78.78(*=.9%78PU9S:!$9 (*!/1OBfi&"/.09;:.7G#%[.#%9%7*> DBfi5.> !4/[O!+P1#%9;!$9%7M9%5!H05.5?P1#%9S!$9%7dO=.9
7*/43?&"Bfi5?/>\7*/9!$(89%&65?/.#F=/4DX5.Bfi9;=/!$9%786[Q(*!4/!$6#%5>!$7@9;:.7#'[?#%9'7*>y#%9;![T&2/!UO!+P#%9S!$9%75?B7837*/
(8:!/.07M9%5!/1=/Bfi78(85$37*B%!O.67MD!$&6678P1#%9;!$9%7ab#%787M &60?=Bfi7<prK5F#%9;Bfi5./.0]/.5?B #'9;Bfi5?/.0H(8[?(86&6(<#%542=.9'&)5./
R

fi

0

)-
- -)

(*!/O7CDn5?=/.PdEO^78(*!=.#%7U!/Q=/BN78(85347*B%!O.671#%9;!$9'7(*!/O7UBfi7*!$(8:.78PzDBfi5.> !4/[&"/.&69%&"!$ #%9;!+9%7KU|G/
78!>,.67<5$D #;=.(8:1!FP5?>!+&2/an!F,5$~ 7*B ,."!/49;r &6#A#'9;=.P&678PU&"/U.78(89%&65?/TK"oKJ{K
Bad States

Good States

Unrecoverable
Failed
States
(Dead-Ends)

&60?=Bfi7]p/p-|GO.#%9;B'!$(89 P78#%(*Bfi&",.9%&65?/5$D 9;:.7< |}5$DE!F#%[.#%9%7*>~&)9S:1=/Bfi78(85$37*B%!O.67<#'9;!$9%78#*K
|G/.59;:.7*BH6&">F&69;!$9%&65?/5$DM#%9;Bfi5?/.0Q!/.Pm#'9;Bfi5?/.0(8[.(8)&6(,.2!4//.&2/.0m&6#9S:.71&2/:.7*BN7*/91,78#%#'&2>\&)#S> 5$D
9;:.78#%71!$605?BN&)9S:>F#*KU 5./.#%&6P7*BGDn5?B78!>@,.)7C9;:.7CP5?>!$&"/abG5?>!+&2/ o*rG&66"=.#%9;B%!$9'78Pm&2/z &60?=Bfi7oPK
:.7GP5?>@!$&"/1(85?/.#%&6#%9'# 5$DXY(/oG#%9;!+9%78# !/.PU9fi~ 5<P&J7*Bfi7*/49`!$(89'&)5./.#]abP!$#;:.78P!/.P1#%56&6Pr8K
...


0

GS
n

1

&60?=Bfi7oP/p G5?>!$&"/Qo4KY :.7|5+D!FP5?>!$&"/~ &69;:19fi~ 5!$(89%&65?/.#abPB%!~A/1!+# #%56&6PQ!/.PUP!$#S:.78P
!B%Bfi5$~ #;r&662=.#'9;B%!$9%&"/.0H9;:.7 ,5#%#'&2O.67M65#%#E5+D#;:.5?Bfi9E,."!/F67*/.09S:.#E~A:.7*/F,Bfi7SDn7*B%Bfi&"/.0<#'9;Bfi5?/.0
#%5"=.9%&65?/.#*K T!/.PU<1!BN7G9;:.7G&"/.&)9'&2!+E!/.P105?!$#%9S!$9%7dBfi78#;,^78(89%&63786[K
Y:.7 #%9;Bfi5?/.0 (8[.(86&)(M!$605?Bfi&69;:>BN789;=B%/.#-! #'9;Bfi5?/.0 ,."!/ aOP ] ^_ r ] ao ] ^ r ]GFGFHF] aOY:1Go ] ^_ r?K
:.&6# ,.2!4/G~ 5?=.6P<:!37E! O78#'9-!/.P<~ 5?BN#%9fiR(*!$#%7Z67*/.09;:M5$DYZK =.9Z! #%9;Bfi5./.0(8[.(86&6(_,."!/aOP ] {C r ]
Oa YV1o ] ^_ r<!$6#%5178?&6#%9'#<!/.PU(85?=.6PO^7,Bfi7SDX7*B'!O.67O78(*!=.#'7]9S:.7\O^78#%9fiR(*!$#'7]67*/.09S:U5+DAoF5$DE9;:.7
(8[.(8)&6(Q#'5"=.9%&65?/>![m:!*347!m>F=.(V::.&60?:.7*B1,Bfi5?O!O.&66&69fi[9S:!/m9;:.7&"/4I/.&69%7Q~ 5?Bfi#'9fiR(*!$#%7T)7*/.049;:K
.9;Bfi5./.0](8[.(86&6(],."!//.&"/.0~&)6-!+)~ ![.#_,BN7SDX7*B 9'5\BN789;=B%/C!<#'9;Bfi5?/.0,."!/d&JD &)978?&6#%9%#d?78347*/C9;:.5?=.0?:C!
#%9;BN5?/.0F(8[?(86&6(,.2!4/U>![C78?&6#%9 ~&)9S:U!F#;:.5.Bfi9%7*B8dO78#'9fiR(*!$#%7F,.2!4/167*/.09;:K
[Q!$PP&"/.0Q!/Q=/Bfi78(85$37*B'!O.67P7*!$P.R7*/.PDn5?BM9;:.7P!$#S:.78P!$(89'&)5./Q!/.P>!4&"/.0#%56&6P!+(89%&65?/.#
/.5?/4RP789%7*B'>F&"/.&)#'9%&6(Wab#%787MG5?>!+&2/C{d. &60?=Bfi7Foo*r8d4#%9;Bfi5./.0<(8[.(8)&6(F,.2!4//.&2/.0C/.5~Bfi789S=B%/.# 9;:.7A#'9;Bfi5?/.0
(8[.(8)&6(,."!7
/ Oa P ] ^_ r ] ao ] ^ r ]GFHFGF] Oa Y(1o ] ^_ r ?K- =.9 ~ 7F>F&60?:9#%9%&66O^7<&"/9'7*Bfi78#%9%78PT&2/9;:.7
,."!7
/ Oa P ] {C ^ r ] Oa Yx1wo ] _ r 7837*/9;:.5?=.0?:C9S:.7G05?!$&6#`/.549 0?=!B%!4/9%7878PC9%5O^7]!$(8:.&678378PK



MCBAjBCBDsW{u{B{

:.7 !/!$6[.#%&6# &"/<9S:.7,Bfi783.&65?=.# #%78(89%&65?/H#;:.5$~ #Z9;:!$9Z9;:.7*Bfi7 !Bfi7EP5.>!$&"/.#E!/.P,."!//.&"/.0],Bfi5?O.67*>F#ZDn5?B

~A:.&6(8:~ 7G>![]~ !/49 9%5F=.#%7`!D=.6)[1Bfi78"!$.78PW!+)045?Bfi&69;:>9;:!$9E!$6~ ![?#&"/.(82=.P78#M9;:.7GO78#%9NR(*!$#%7G,.2!4/
!/.PBfi789;=B'/.#!#%5"=.9%&65?/H7837*/H&JD&69 &"/.(8"=.P78#EP7*!$P.R7*/.P#9;:!$9Z(*!//.59 O7 0?=!B%!4/9%7878PH9%5GO7 !35&6P78PK
xQ7G&"/49;Bfi5.P=.(87]!/1!$605.Bfi&69;:>#%&">F&6"!B9%5F9;:.7G#%9;Bfi5./.0\,."!//.&"/.0U!$605.Bfi&69;:>9;:!$9 !$PP#M!/C5?BfiP&"/!Bfi[
,Bfi78&">!$047G&2/C7*!+(V:@&69%7*B%!$9%&65?/1:!+#9S:.78#%7<,Bfi5?,^7*Bfi9%&678#*KE78(*!=.#%7G#'9;!$9%7SR!$(89'&)5./B%=.678# 67*!$P&"/.0C9%5]=/BN7SR
(85$37*B%!O.67HP7*!$P.R7*/.P#]>![1O7!$PP78P9%5C9;:.7]=/.&637*BN#;!$E,."!/d~ 7G(*!$6 9;:.&6#<!$605?Bfi&69;:> n "8nb
R

fi

0 J

0

...


0

GS
n

1

&60?=Bfi7oo<p G5?>!$&"/U{^KY :.7|5+D!FP5?>!$&"/~ &69;:19fi~ 5!$(89%&65?/.#abPB%!~A/1!+# #%56&6PQ!/.PUP!$#S:.78P
!B%Bfi5$~ #;r_&)6"=.#%9;B'!$9%&"/.09;:.7,5#%#'&2O.67C65#%#G#;:.5?BN9`,."!/67*/.09S:.#<~A:.7*/,Bfi7SDn7*B%Bfi&"/.0U#'9;Bfi5?/.0
(8[?(86&6(F#%5"=.9%&65?/.#*K T!/.PU<1!BN7G9;:.7G&"/.&)9'&2!+E!/.P105?!$#%9S!$9%7dBfi78#;,^78(89%&63786[K
6
b?KY:.7!+)045?Bfi&69;:> &)#1#S:.5~M/m&"/ &60?=Bfi7o*{KY:.7UD=/.(89%&65?/uEBN78&2>@!$07?a>Z"8b , > , 8r
Bfi789;=B'/.#<9;:.7#'789<5$D #%9;!$9%7SR!+(89%&65?/QB%=.678# !$#%#'5?(8&"!$9%78P~ &69;:9S:.7,Bfi78&">!$07@5$D 9;:.73.&)#'&)9'78P#%9S!$9%78#*K
uEB'=/.7?a , M*nbHd Z"8b , > , 8rGBfi7*>F5$378#M9;:.7#%9S!$9%7SR!$(89%&65?/B%=.678#*d~A:.7*Bfi79S:.7]#'9;!$9%7!$"Bfi7*!$P[
&6# &2/.(8"=.P78P&"/C9;:.7M#%789 5$D3?&6#%&69%78P1#%9S!$9%78#*d.!/.PU.9;!$9%78#SD'a (. , > , M*nb^;rBN789;=B%/.# 9;:.7M#%789E5$D
#%9;!+9%78# 5$DE9;:.7,B%=/.78P#%9S!$9%7SR!$(89%&65?/TB%=.678#*K 0 hZf &2/.(8"=.P78#H9;:.7F5?,.9%&">F&6#%9%&6(,."!//.&"/.0!$605?BN&)9S:>1K
:.7 5.,.9%&">F&6#%9%&6(`,."!//.&"/.0!$605?BN&)9S:> &6#E&"/.(85?>,.6789%7M~ &69;:BN78#;,78(899%5G#%9;Bfi5./.0G#%5"=.9%&65?/.#*d.O78(*!=.#%7M&69
CyyCy < s[W) ) E ,
)F0ff

W ), (ffj

C

)
)F
F )ff W^C 0
F ,"ff W F ) )F
[C>
@ F ,)*j
L , (
ffAL , 0 F ,)
F0
ff ) F,
0F )
)


y>[Cy

y>Cyk e 6 )

W ),

&60?=Bfi7Co*{p :.7<5.,.9%&">F&6#%9%&6(],."!//.&"/.0U!$605?BN&)9S:>1K
P5.78#`/.59M/.78(878#%#;!Bfi&66[BN789;=B%/U!\#%9;Bfi5?/.0@#%5"=.9%&65?/d
&D 5?/.7G78.&)#'9%#*K /49;=.&69%&63786[d5?,.9%&">F&6#%9'&)(,."!//.&"/.0
5?/.6[0?=!B%!/49%7878#H9;:!$9<9S:.7*Bfi778.&6#%9%#F#%5?>\77S78(89F5$DA!,.2!4/m!$(89%&65?/z)7*!+P&2/.0z9%5U9;:.7C05?!+Xd~A:.7*Bfi7
#%9;BN5?/.0,.2!4//.&2/.010.=!B%!/49%7878# 9;:!+9_!+)7S78(89'# 5$DE,.2!4/U!$(89%&65?/.#)7*!+PU9%59S:.7G05?!$nK
Y:.7A,=B%,54#%7A5+D5.,.9%&">F&6#%9%&6(<,."!//.&"/.0&6#/.599%5<#;=O.#'9%&69;=.9%7M#%9;Bfi5?/.0G5.B #%9;BN5?/.0<(8[.(86&)(<,."!//.&"/.0K
:.78#%7U!+)045?Bfi&69;:>F#F#;:.5?=.6PO7U=.#%78Pm&"/mP5.>!$&"/.#~A:.7*Bfi71#'9;Bfi5?/.0T5?BH#%9;Bfi5?/.0(8[.(86&)(,."!/.#(*!4/O^7
Dn5?=/.Pw!/.Pw045?!$ !$(8:.&)78347*>F7*/9C:!$#9;:.7U:.&60?:.78#%91,Bfi&65?Bfi&69fi[KwG,.9%&">F&6#%9'&)(Q,."!//.&"/.0w>F&60?:49O7T9;:.7
O789'9%7*B (8:.5&6(87G&"/P5?>@!$&"/.#E~A:.7*Bfi7M05?!+!$(V:.&67837*>\7*/9 (*!//.59 O7M0?=!B%!4/9%7878P@5?BZ9;:.7M#;:.5?Bfi9%78#'9E,.2!4/
#;:.5?=.6PO7<&"/.(8"=.P78PQ&"/19S:.7<=/.&)347*Bfi#;!$-,."!/K
_5?/.#%&6P7*B !$0?!+&2/d
!$#A!/C78!>,.67d9;:.7<BN5?O59NRO!O[CP5?>!+&2/P78#%(*Bfi&"O78P&"/U.78(89'&)5.
/ 1KZ5.B 9;:.&6#
,Bfi5?O.67*> 9;:.7H5?,.9%&">F&6#%9%&6(#'5"=.9%&65?/>!$78# 9S:.7\BN5?O599;Bfi[19%5C6&Dn9G9;:.7FO.)5.(81~A:.7*/T9;:.7F,5#%&69%&65?/T5$D
9;:.71O.65?(8&6#678#%#F9;:!/U!/.P9;:.71Bfi5?O^59<&6#F~ 5?B%4&2/.0
KY :.&6#F#%787*>F#F9%5O7C9;:.7C5?/.6[Bfi7*!+#%5?/!O.67
#%9;B'!$9%780[dZ7837*/9;:.5.=.0?:m/.5Q0?=!B'!/9'787Dn5?BH05?!$ !$(8:.&)78347*>F7*/9@(*!/mO7T0&637*/Kmn9F&6#~ 5?Bfi9S:~A:.&667
R

fi

0

Z; 'c

8b'
'n







n


8b'"8c
'n




n





)-
- -)







$bnb

+fin
'n









YZ!O.67oKp :.7 O^78#%9!4/.P]~ 5?BN#%9fiR(*!$#%7E,."!/H67*/.09;:H5$D,^5#%#%&"O.67 #%9SBfi5?/.0d#%9;Bfi5./.0 (8[?(86&6(A!/.PF5?,.9'&2>\&)#'9%&6(
,."!/.#&2/\G5?>!$&"/.# !/.P{Fab#%787 &60?=Bfi78# o^PA!/.P1oor8K aeRrZ>F7*!/.# 9;:!+9/.5G#%542=.9'&)5./F78?&6#%9'#*K
k r&"/.P&)(*!+9%78#G9;:!$9 9S:.7],."!/167*/.09S:U&6#M&"/4I/.&)9'7d!/.P!/T=/Bfi78(85$37*B%!O.67HP7*!$P.R7*/.P&)#
Bfi7*!+(V:.78PK
(85?/.#%9SB%=.(89%&"/.0W!4/C5?,.9%&">F&6#%9%&6(,."!/CDn5?B 9;:.&6#G&"/.PT5$DP5.>!$&"/.#A#'&2/.(87F9;:.7<!+)9'7*B%/!$9%&637H&)#G/.5,."!/!$9
!$6nK
|#%&">F&6"!BH5?,.9%&">F&6#%9'&)(1,."!/z&)#H07*/.7*B%!$9'78PDX5.BM9;:.7P5.>!$&"/#;:.5$~A/&"/ &60?=BN7WpK@5?BG!+) O!$P
#%9;!+9%78#*d9;:.75.,.9%&">F&6#%9%&6(W,."!/!$#%#'5?(8&"!$9%78#<!4/Q!$(89%&65?/9S:!$9<OBfi&"/.0#F9;:.7F#%[.#%9%7*>y9'5U!C05?5.PQ#%9S!$9%7F&"/
5?/.7#%9%7*,K :.&6#T(85?/9'&2/.=.78#U!$#C65?/.0w!$#C9;:.77*/3.&"Bfi5?/>F7*/49U$787*,.#19;:.7#%[.#%9%7*> &"/!O!+P#'9;!$9%7K
78(*!=.#%7/.5C#%9;B'!$9%780[1(*!/O^7\=.#'78PQ9%51,Bfi7837*/49G9;:.7F7*/3.&"Bfi5?/>F7*/49MDBN5?> OBN&2/.04&2/.0T9;:.7F#%[.#%9%7*> 9%5
!/1=/Bfi78(85$37*B'!O.67<P7*!$P.R7*/.Pd
9;:.7G5?,.9%&">F&6#%9'&)(F#%542=.9'&)5./1&6# =.&)9'7<#%7*/.#%&"O.67K

5?BG5?>!$&"/.# !/.P{ #;:.5$~A/&"/F &60?=Bfi78# PM!/.P1ood5?,.9%&">F&6#%9%&6(A,."!//.&"/.0Bfi789;=B'/.#!G=/.&637*Bfi#S!$
,."!2
/ Oa P ] n , r ] Oa Y.1To ] 8 r ?KZ5?B O^59;:@P5?>!$&"/.#9;:.&6#&6# !<=/.&637*BN#;!$,."!/@~ &69;:@9;:.7 #;:.5.Bfi9%78#%9
O78#'9fiR(*!$#%7M67*/.09;:KE 5.>,!Bfi78P9'5G9;:.7 #%9;BN5?/.0<(8[.(86&)(G#'5"=.9%&65?/9S:.7 (85#%9&2/@9;:.7 IBN#%9EP5?>@!$&"/&6# 9;:!$9
9;:.7A,."!/@>![]:!37 !/F&"/4I/.&69%7M)7*/.049;:d.~A:.&6)7M9;:.7 (854#%9 &"/F9;:.7 #%78(85./.PP5?>!$&"/F&6#9;:!$9E!GP7*!+P.R7*/.P
>![O7<Bfi7*!+(V:.78PKEY:.7`Bfi78#S=.)9'# 5$D#%9;Bfi5./.0d#%9SBfi5?/.0<(8[.(86&6(d!/.PC5?,.9%&">F&6#%9'&)(F,."!//.&"/.0&"/CG5?>!$&"/.#Go
!/.PU{!4Bfi7G#;=>>!4Bfi&6878PU&"/UYZ!O.67oK


(

- 8476 $ 8

:.7 &"/,=.9E9%5<s hZf &)# !/"!'#QP78#%(*BN&2,.9'&)5./`!/.P!M#;,^78(8&JI(*!$9%&65?/@5$D~A:.&6(V:@,.2!4//.&2/.0!$605.Bfi&69;:>

9%5U=.#%7KCY :.&6#<P78#%(*BN&2,.9'&)5./Q&6#F9;:.7*/(85?/437*Bfi9'78P9'5U!1#%789G5$D h ii #Bfi7*,Bfi78#'7*/9%&"/.0T9;:.7,!Bfi9'&)9'&)5./.78P
9;B%!4/.#%&69%&65?/mBfi78"!$9%&65?/!$#FP78#%(*Bfi&"O78P&"/w.78(89%&65?/qKmY :.7 h ii Bfi7*,Bfi78#%7*/49;!$9'&)5./&)#1=.#%78PO[m!T#%789
5$D],."!//.&"/.0w!$605?BN&)9S:>F#9'507*/.7*B'!$9%7U!,.2!4/KY :.75?=.9;,=.9C5$D<s h-f &)#U!z=/.&)347*Bfi#;!$<,."!/ 5?B
#%78=.7*/9'&2!+,."!/@P7*,7*/.P&"/.05?/@9;:.7A,."!//.&"/.0!$605?Bfi&69;:>K |=/.&)347*Bfi#;!$,.2!4/&6#BN7*,Bfi78#%7*/49%78PWO4[!/
h ii KHn9GP7SI/.78#MDn5?B 7*!+(V:TP5?>!+&2/T#%9S!$9%7]!@#%789G5$Dfi5&"/49`!$(89'&)5./.#G9;:!$9M9;:.7F#%[?#'9%7*> !+07*/9'#`>F=.#%9
78.78(*=.9%7<#%[/.(8:Bfi5?/.5?=.#%6[T&2/5?BfiP7*B 9'5!$(8:.&)78347]9S:.7G05?!$nK :.7<&">,.67*>F7*/49%78P,."!//.&"/.0U!$605?BN&)9S:>F#
!Bfi7Kp
o4K .9;Bfi5?/.0,."!//.&"/.0K
{^K .9;Bfi5?/.0F(8[.(8)&6(,."!//.&"/.0K
^K G,.9%&">F&6#%9%&6(,.2!4//.&2/.0
K
1
K

"!$#%#%&6(*!$P789'7*B%>F&"/.&6#%9%&6(,.2!4//.&2/.0
K

ffa
nnff$b%G;fifinCb *n*
Xfi*nSb%Gc$ff *fi'c$;'fi%.H;n' $'* b
%nb$ b Sn' ' M"
*fi *nSb'\ bZfi 'c*cM +Sn%Znb%'* "% ' G%>M
'- H *n*2
'O


R

fi

0 J

G789%7*B%>\&2/.&6#%9'&)(1,."!//.&"/.0Q(*!/QO^73.&678~ 78P!$#<!#;,78(8&"!$ (*!$#'75$D_/.5./4RP789%7*B%>F&"/.&6#%9%&6(U,."!//.&"/.0KU/
h-f d~ 7=.#%78PQ9S:.7]5.,.9%&">F&6#%9%&6(,."!//.&"/.0Q!$605?Bfi&69;:>Dn5?B9;:.7O!$(V4~ !BfiPT#%7*!Bfi(8:T5$D(8"!$#%#'&)(*!+ P7SR
9%7*B%>\&2/.&6#%9'&)(,."!//.&"/.0KFanY :.7<#'9;Bfi5?/.05.B #%9;Bfi5./.0(8[?(86&6(!$605.Bfi&69;:>(85?=.6PQ!$6#%51:!*347<O787*/=.#%78Pd!$#
!$6 9;:.7HP78#%(*Bfi&"O78Pm/.5?/4RP789%7*B%>\&2/.&6#%9'&)(1!$605?BN&)9S:>F#`O^7*:!37F#%&">F&62!4Bfi6[&"/P789'7*B%>F&"/.&6#%9%&6(P5?>!+&2/.#Kr
:.7F5?/.6[/.78~ Dn7*!$9;=Bfi7H5$DE9;:.7FP789%7*B%>F&"/.&6#%9%&6(!+)045?Bfi&69;:> &6#A9S:!$9`!@#%78=.7*/9'&2!+E,.2!4/U&6#G07*/.7*B'!$9%78P
DBfi5?>9;:.7=/.&)347*Bfi#;!$E,."!/O4[U(8:.5?5#'&2/.0U!4/U&"/.&69%&"!$#%9;!$9%7F!/.P&69%7*B%!$9%&63786[!$PP&"/.0!/!$(89%&65?/DBfi5?>
9;:.7=/.&637*Bfi#;!+,."!/Q=/49%&6 !C05?!$#'9;!$9%7H&)#<BN7*!$(V:.78PKGY :.7FP789%7*B'>F&"/.&)#'9%&6(,."!//.&"/.0!+)045?Bfi&69;:> :!$#
O787*/C&">,.67*>F7*/49%78PT9%5<37*BN&Dn[9;:.7G,7*BDX5?B'>!/.(87M5$D-s hZf (85?>,!Bfi78PC9%5H59;:.7*B(8"!$#%#%&6(*!$,."!//.7*Bfi#*K
n9G:!$#</.59GO787*/T5?=B&"/9'7*/9%&65?/T&"/9;:.&6#M~ 5?B%
d9;:.5.=.0?:d9'5P7837865?,!\D!+#%9 h ii RO!$#%78P(8"!$#%#%&6(*!$
,."!//.&"/.0!+)045?Bfi&69;:> 6&"$7UG& Q!/.85789!$nKAafio*p4pr8KG=B>!$&"/&"/49%7*Bfi78#%9&6#/.5./4RP789%7*B%>F&"/.&6#%9%&6(d
>F=.69%&JR!$07*/49A=/.&637*Bfi#;!$ ,."!//.&"/.0K
Y:.7Us hZf ,.2!4//.&2/.0m#'[?#%9'7*>&6#&">,.67*>F7*/49%78P&"/w
40
/./ !4/.Pw=.#%78#9;:.7U
ii
l ,!+(V!$07
Oa $&"/.P.R&678)#'7*/d<o*pp4prDX5.B h ii >@!/.&",=.2!+9%&65?/.#*KQ<=BN&2/.0Q,."!//.&"/.09;:.7P[/!>F&6(13$!Bfi&"!O.67UBfi7SR
5?BfiP7*BN&2/.0D!$(8&6)&69fi[<5+D9;:.7E
ii
l ,!$(8*!$047 &)# =.#%78PF9%5 I/.PF! O789'9%7*B5.BfiP7*Bfi&"/.0A5+D9;:.7 h ii 3!Bfi&"!O.678#*K
/9;:.7HDn56)5$~ &"/.0TDn5?=BM#;=O.#%78(89%&65?/.#H~ 7,Bfi78#%7*/49<Bfi78#;=.69%#H5?O.9;!$&"/.78PQ~ &69;:9;:.7s h-f ,."!//.&"/.0
#%[.#%9%7*>&"/U/.&"/.7<P&J7*Bfi7*/49 P5?>!+&2/.#MB%!/.0&"/.0HDBN5?>P789%7*B%>F&"/.&6#%9%&6(!/.P1#%&"/.067SR!$07*/49 ~ &69;:1/.5@7*/3.&JR
Bfi5?/>\7*/9<!$(89'&)5./.#<9%5U/.5?/4RP789'7*B%>F&"/.&6#%9%&6(U!/.P>F=.69%&JR!$07*/49<~ &69;:(85?>,.6787*/3.&"Bfi5?/>F7*/49]!$(89'&)5./.#*K
|M)Z78,7*Bfi&">F7*/49%#G~ 7*Bfi7<(*!4B%Bfi&678P5?=.9 5?/T5
! 1?<q PFQ FuZ7*/9%&"=> uE ~ &69;:QoF<[.9%7 |AB'=//.&2/.0
9 $&"/?=.
1
K{KZ|>\5?Bfi7MP789;!$&6678PUP78#%(*Bfi&",.9%&65?/5$D-9;:.7G78,7*BN&2>\7*/9%#&2/.(8"=.P&"/.0U9S:.7A(85?>@,.)789'7
78PUM!$\
P78#%(*Bfi&",.9%&65?/T5$DZ9;:.5
7 "!#QP5?>!$&"/.#M(*!/1O7DX5?=/.PT&"/ 7*/.#%7*/afio*pp4pr8K




-



V{ M||\AjB{ByCBEDM2A[,B"{V

xQ7 IBN#%9 9%78#%9Es hZf Lc#E,7*BDn5?B%>@!/.(87DX5?BZ#'5?>F7 5$D9S:.7A/.5?/4RP789%7*B%>\&2/.&6#%9'&)(GP5.>!$&"/.# #%56378PO4[A f K
78?9*d~ 7,Bfi78#'7*/9-!M,5$~ 7*B,."!/49-P5.>!$&"/<!/.PI/!$66[d~ 7 #;:.5$~Bfi78#;=.69%#DBfi5?>!M>F=.)9'&R!+07*/9Z#%5.(8(87*B
P5?>!+&2/K

ff
fiff
h Z j B l f
G/.75$D<9;:.7P5?>!+&2/.#1#'56378PO[wM f &6#U!/.5?/4RP789%7*B%>\&2/.&6#%9'&)(9;B%!4/.#;,5?BN9;!$9%&65?/P5?>@!$&"/KY :.7
P 5?>!+&2/(85?/.#%&6#%9%#H5$D_!#%789G5$D65.(*!$9%&65?/.#!/.PQ!1#'789G5$D_!$(89'&)5./.#<6&"$7PBN&)347SR9;B%=.(8dPBfi&637SR9;B%!$&"/!/.P
[9%5F>F5$37AO^789~ 787*/C9;:.7M65?(*!+9%&65?/.#*KE5?/4RP789%7*B%>\&2/.&6#;> &)# (*!=.#'78PUO[/.5./4RP789%7*B%>F&"/.&6#%9%&6(]!+(89%&65?/.#
ab7Kc0
K"d!Dn9%7*BE!PBfi&637`!+(89%&65?/!9;B%=.(8F>![]5.BE>![/.59:!37_D=.7867SDX9SrE!/.PC7*/3.&"Bfi5?/>F7*/49;!$
(V:!4/.078#
ab7Kc0
K"d8Dn50<!$9 !$&"B%,^5?Bfi9%#*dH &">!$9%9%&789E!+XKedo*pp!4r8KxQ7 P7SI/.78P9S:.79fi~ 5MP5?>!$&"/F78!>,.678#E9'78#%9%78PO[
f Dn5?B #%9;BN5?/.0!/.P1#%9;Bfi5./.0<(8[.(8)&6(,."!//.&"/.01&"f
/ /"!#!4/.PUB%!/1s h-f =.#%&"/.0C#%9;Bfi5?/.0!/.P1#'9;Bfi5?/.0
(8[.(8)&6(,."!//.&"/.0KA59S:178!>,.678#A~ 7*Bfi7H#%56378PT&2/678#%#A9S:!Z
/ PK PqH#%78(85?/.P#*K .&">F&6"!BGBfi78#;=.69%#M~ 7*Bfi7
5?O.9;!+&2/.78PT~ &69;:1M f K |}07*/.7*B%!$37*Bfi#%&65?/C5$D 9;:.7<:.=/9'7*B !/.PU,Bfi78[C5?BH;uE=BN#;=.&69%P5?>!+&2/an7*/.P!
789]!+XKed o*prG!4/.P!1O7*!> ~ !$"UP5.>!$&"/Q:!37!$6#%5O^787*/9%78#%9'78PO[QM f KCY :.707*/.7*B%!+)&6*!$9'&)5./
5$DE9;:.7F:?=/49%7*BG!/.P,Bfi78[UP5.>!$&"/1&6#</.59MP78#%(*Bfi&"O78P&"/TP789;!$&6-&"/man &">!+9%9%&789<!$nK"dZo*pp4!r8K :.=.#*d
~ 7:!*347]/.59GO787*/Q!4O.)7@9%5W>@!$7]!4v
/ "!#&">,.67*>F7*/49;!$9%&65?/5$DE9;:.&6#<P5?>@!$&"/1Dn5?BM!1>F7*!/.&"/.0$D=.
(85?>,!4Bfi&6#%5?/K
Y:.7<,Bfi5?O.67*>&"/9;:.7]O^7*!>~ !$"P5.>!$&"/1&6#DX5?B !/T!$07*/49 9%5~ !$"FDBfi5?>5?/.7H7*/.PU5$DE!O^7*!>
9%5C9;:.7<549;:.7*B ~&)9S:.5?=.9 D!$66&"/.01P5~M/KAbDE9S:.7]!$07*/49D!$66#*d&69G:!$#A9'5~ !$"1O!$(8C9%59;:.7H7*/.P5$DE9;:.7
O7*!4>!/.PQ9SBfi[!$0.!$&"/KY :.7FI/.&69%7#%9S!$9%7>!$(8:.&"/.75$D 9;:.7FP5?>!$&"/&6#F#;:.5$~A/T&"/ &60?=Bfi7UoKY :.7
78P078#P7*/.59%7 9;:.75?=.9%(85?>F7 5$D-!M~ !$"F!$(89%&65?/KZx:.7*/F9;:.7A!+07*/9 &6#5?/F9;:.7AO^7*!>1d49;:.7 ~ !$"F!$(89%&65?/
RKfi

fi 0



)-
- -)

(*!/z78&69;:.7*B>F5$37C&69]5./.71#%9%7*,D=Bfi9S:.7*B<5./Q9;:.7UO^7*!>
O7*!4>1K

5?BF>!$7C&69HD!$6 9%5Q!T65?(*!$9'&)5./m=/.P7*BF9;:.7


true

...

false
0

...
1

2

G

n-2

n-1

pos

&60?=Bfi7o*/p :.7<O7*!>~ !$"FP5?>@!$&"/KEY :.7 "!'#7*/.(85?P&"/.05+D-9;:.7GO7*!>~ !$"FP5.>!$&"/1:!$# 5?/.7
,Bfi5?,54#%&69%&65?/!$#%9S!$9%7 3$!Bfi&"!O.67%( 9;:!$9 &6# 9;B%=.7M&JD9S:.7`!$047*/9&)#5?/9S:.7<O7*!>!/.PCD!$6#%7
59;:.7*Bfi~ &6#%7d !/.P!1/?=>F7*BN&)(*!+E#%9;!+9%73!4Bfi&"!O.67 $9;:!$9GP7*/.59'78#<9;:.7,^5#%&69%&65?/z5$D 9;:.7
!$07*/49<78&69;:.7*BG5./9;:.7O^7*!> 5?BM5?/9;:.7@0?Bfi5?=/.PK;U!/.PSGU!Bfi7@9;:.7&"/.&69%&"!$ #%9;!$9%7
!/.PU05.!$#%9;!$9%7GBfi78#;,^78(89%&63786[K
xz7 &">,.67*>F7*/9'78PW!07*/.7*B%!$9%5.B ,Bfi50?B%!4> DX5?B""!#P78#%(*BN&2,.9'&)5./.#5+DO7*!4>~ !$"P5?>!$&"/.# !/.P
, Bfi5.P=.(878PP5.>!$&"/.#<~ &69;: 119%51+PpC,54#%&69%&65?/.#*K178(*!=.#%79;:.7@P5?>!$&"/5?/.6[(85?/49;!$&"/.#<9fi~ 5C#%9;!$9%7
3!4Bfi&"!O.678#*ds h-f (*!//.59 78,.65&69A!<,!BN9%&69%&65?/.78P19;B%!/.#'&)9'&)5./Bfi78"!$9'&)5./@DX5?B9;:.&6# P5?>!+&2/dO=.9 :!$# 9%5
=.#%7F!]>\5?/.56&69;:.&6(\BN7*,Bfi78#%7*/49;!$9%&65?/K
Y:.7],7*BDX5?B'>!/.(87F5$Ds h-f !/.PQM f &6#G#;:.5$~A/1&"/ &60?=BN7Wo 1K G&6#%(85?=/49%&"/.0U9S:!$9`M f ~ !$#
B%=/H5?/F!#%65~ 7*B >!+(V:.&"/.7d 9;:.7 ,7*BDX5?B'>!/.(87 5$Ds hZf !/.PA f &6# =.&69%7 #%&">F&6"!B &"/F9S:.&)# P5.>!$&"/K
5.BMP5?>!$&"/.#<~M:.7*Bfi7s hZf (*!/78,.65&69]!1,!4Bfi9%&69%&65?/.78PmBfi7*,Bfi78#%7*/49;!$9%&65?/d~ 7~ 5?=.6PQ78,78(89H&69<9%5
O7!O.67F9%5C#%5637H2!4Bfi07*BM,Bfi5?O.67*>F#G9;:!/TA f d#%&"/.(87A f (*=B'Bfi7*/9')[T(*!/15./.)[=.#%7]!C>F5?/.54)&69;:.&6(
Bfi7*,Bfi78#'7*/9;!+9%&65?/K =BN9;:.7*B (85?>,!4Bfi&6#%5?/.#AO^789~ 787*/Ts hZf !/.PUM f !Bfi7G5?/C5?=BBfi78#%7*!Bfi(8:1!$07*/.P!^K
vG h g G $- h -jb
:.7 ,=B%,^5#%7 5$D9;:.7 Bfi7*>!$&"/.&"/.0G78,^7*Bfi&">F7*/49%#E&"/F/.5?/4RP789%7*B%>\&2/.&6#%9'&)(MP5?>!+&2/.#Z&6# 9%5M#;:.5$~=/.&637*Bfi#S!$
,."!//.&"/.0UBfi78#;=.69%#DX5.BEP5.>!$&"/.#A~M:.7*Bfi7<9;:.7<>F=.69%&JR!$07*/49G!/.P17*/3.&"Bfi5?/>F7*/49G>F5?P7866&"/.0CDX7*!+9;=Bfi78# 5$D
"!'#m:!37`O^787*/=.#%78PK
Y:.7,5$~ 7*BZ,.2!4/9ZP5?>!$&"/HP7*>F5?/.#'9;B%!$9%78#Z!A>F=.69%&JR!$07*/49-P5.>!$&"/H~ &69;:<!/H7*/43?&"Bfi5./>F7*/9 >F5.P78
!/.PMD=Bfi9;:.7*B78?7*>@,.)&JI78# 5?,.9'&2>\&)#'9%&6(,."!//.&"/.0KE9-(85?/.#'&)#'9%#5$DBfi7*!$(89%5.Bfi#*d*:.7*!$978?(8:!/.07*Bfi#d*9;=B'O.&2/.78#
!/.P13$!$6378#*K|P5?>@!$&"/178!>,.67<&6# #;:.5$~A/C&"/ &)0.=Bfi7o*q^K
/C9;:.7],^5$~7*B,.2!4/9 P5.>!$&"/17*!$(8:C(85?/49;Bfi56"!O.67]=/.&69M&6#`!$#'#%5?(8&"!$9'78PU~ &69;:1!/T!$07*/49 #;=.(V:9;:!$9
!$6 (85?/49;Bfi5Z!$(89%&65?/.#G(*!/O7,^7*BDX5.B%>F78P#%&">F=.)9S!/.785?=.#%6[KFY :.7F7*/3.&"Bfi5?/>F7*/49G(85?/.#%&6#%9%#G5+D!C#%&"/.067
!$07*/49 9;:!+9`!$9M!/[19'&2>\7](*!4/CD!$&6!C/?=>FO^7*B 5$D:.7*!+9A78.(V:!4/.078#`!4/.P9S=B%O.&"/.78#<!/.P!$6#%5C7*/.#;=Bfi78#
9;:!$9 !$"Bfi7*!$P[D!+&)678P=/.&69%#EBN7*>!$&"/D!$&6678PK |D!$&6678P:.7*!$9 78.(V:!/.047*B67*!4# ~ !$9%7*BDBfi5.>9;:.7 &"/9%7*B'/!$
9%59S:.7G78?9%7*B'/!$~ !$9%7*B 65.5?,U!4/.PU>]=.#'9AO7H(8)54#%78PO4[U!O.65?(81!$(89%&65?f
/ $KEY :.7G7*/.7*Bfi04[U,Bfi5?P=.(89'&)5./
DBfi5?> 9;:.71Bfi7*!$(89%5.BM(*!/QO7C(85?/49;Bfi56678PmO[ 9'51I9<9S:.7P7*>!/.Q
P iZdO=.9H9;:.71Bfi7*!$(89%5?BM~&)6 !$6~ ![?#
,Bfi5.P=.(87<5?/.7G7*/.7*BN0[W=/.&69*KY59SB%!/.#;,^5?Bfi9 9;:.7G7*/.7*Bfi0[@DBfi5?>9;:.7<Bfi7*!$(89'5?B !*~ ![HDBN5?>9;:.7G,."!/9G!$9
67*!$#%9H5?/.7:.7*!$9H78?(8:!/.07*BG!/.Pz5?/.79;=B%O.&"/.71>F=.#%9<O7C~ 5?B%4&2/.0
KM9;:.7*Bfi~ &6#%7C9;:.7,."!/49<&6#<&"/!/
=/Bfi78(85$37*B%!4O.)7D!+&)678PT#%9;!$9%7d.~A:.7*BN7<9;:.7<Bfi7*!+(89%5?B~ &66-5$37*B%:.7*!+9*K

ff
fi

B

v*bff !ff,b`"58nfi$#M#;C $nGnSSn b fi nM"'&%('*)4
RKfifi

fi

0 J

10000

1000

Time / Sec

100

10

1

0.1

0.01

UMOP
MBP

0.001
0

500

1000

1500
2000
2500
3000
Number Beam Locations

3500

4000

4500

&60?=Bfi7oy1p u "!//.&"/.019%&">F7<5+Ds hZf !/.PM f &2/T9;:.7<O^7*!>~_!+2@P5?>!+&2/K :.7FA f P!$9;!@:!$#
O787*/T78?9SB%!$(89%78PC~ &69;:U,^5#%#%&"O.67F65#%# 5$DE!$(8(*=B%!+(8[DBfi5?>a &">!$9%9%&789A!+XKedopp!rK
Y:.7#%9;!$9'7]#S,!$(875$D 9;:.7,5$~ 7*B<,."!/49<(*!4/QO7P&63.&6P78Pm&2/49%5T9;:Bfi787P&6#nfi5&"/9F#%789'# pG05.5?PdZO!$P
! /.PzD!+&)678P#%9;!$9%78#K/9;:.7045?5.P#'9;!$9%78#*dZ9;:.7*BN7SDX5?BN79;:.7C05?!+E#%9;!+9%78#*dZ9;:.7U,5$~ 7*B<,."!/49#;!+9%&6#fiI78#
&69%#F#;!Dn789fi[!/.P!+(89%&63?&69fi[Bfi78=.&"Bfi7*>F7*/49%#*KU/Q5?=B78!4>,.679;:.7@#;!Dn789[QBfi78=.&"Bfi7*>F7*/49%#F7*/.#;=Bfi7C9;:!$9
7*/.7*Bfi0[C(*!/TO7<9SB%!/.#;,^5?Bfi9%78P1!~_![FDBfi5?>9;:.7F,."!/9d!/.P19;:!$9_D!$&6678P=/.&69%#G!Bfi7G#S:?=.9 P5$~AC/ p

+-,/.0,/132*46587/.:90,<;3107/.>=@?0A/13;0,CB:7/D07/4:E31FA8G-;3H0,<?0I37/.*;
JA8K0HMLON3PQARK0H0S:N3P:A8K0H0T:N3P:A8KFH3U VWP3N
JA8K0;MLON3PQARK0;0S:N3P:A8K0;0T:N3P:A8KF;3U VWP3N
+:H0,37/;-,/X>5@H07/.320,C1>=W90I3A05KY,/B[ZEQE07>Z@I3,/B
JR\RA8KFH L^]0_:9 L3VWP3N
JR\RA8KFH0S`]0_:90SfiVWP3N
JR\RA8KFH0T`]0_:90TfiVWP3N
JR\RA8KFH3U<]0_:93U VWP3N
+:;3a3139fiZ.0,b=<=R;FA/?3?0,/BcZE:E07YZRI3,/B
JR\RA8KF; L^]0_[=YL3VWP3N
JR\RA8KF;0S`]0_[=CSfiVWP3N
JR\RA8KF;0T`]0_[=CTfiVWP3N
JR\RA8KF;3U<]0_[=RU V
:.7A!$(89%&63.&)9fi[Bfi78=.&"Bfi7*>F7*/49%# #%9;!$9%7 9;:!+9E9;:.7 7*/.7*BN0[,Bfi5?P=.(89'&)5./C78=!$6#9;:.7AP7*>@!/.P!/.PC9;:!$9

!$63$!$6378#A9'5~ 5?B%4&2/.0F9;=B'O.&2/.78#<!4Bfi7A5.,7*/Cp

RKfiR

fi 0



)-
- -)

+:?0A/D0,/1:?31FA/B3a>5R;dZRA/.:,/e*a073Ib=WBF,8GY7/.3B
?:]QE-P3N
+:;3a3139fiZ.0,<f073I/f0,-Z/=:A/?F,/.cZE:;3a3139fiZ.0,QZ/=gA8K
JA8K0;ML^]0_hfML3VWP3N
JA8K0;FS`]0_hfFSfiVWP3N
JA8K0;FT`]0_hfFTfiVWP3N
JA8K0;*U<]0_hf*U V
/!HO!$PC#%9;!$9%7d$9S:.7`,."!/49 P5?78# /.59#;!$9%&6#fiDn[F9;:.7A#S!DX789fi[F!/.PW!+(89%&63?&69fi[Bfi78=.&"Bfi7*>F7*/49%# O=.9E&6# /.59
=/Bfi78(85$37*B%!4O.)[CD!$&6678PK/U!HD!$&6678PU#%9S!$9%7G!$6-:.7*!$9 78.(V:!4/.07*Bfi#M5?B9;=B%O.&"/.78#`!4Bfi7MD!+&)678PK
Y:.7\=/.&637*BN#;!$,."!//.&"/.09;!$#S1&6#<9%51047*/.7*B%!$9%7!1=/.&637*BN#;!$E,."!/z9%510789MDBfi5?>y!/4[O!$Pz#%9;!$9%7
9%5T#%5?>F7045?5.P#'9;!$9%7~&)9S:.5?=.9F7*/.P&2/.0z&2/!D!+&)678Pm#%9S!$9%7KC|A#'#;=>F&"/.0Q9;:!$9F/.5=/.&69%#FD!$&6EP=BN&2/.0
78.78(*=.9%&65?/d&69E&6# 5?O43.&)5.=.# 9;:!$9 5?/.6[<5./.7-fi5&"/49E!$(89%&65?/H&6#/.7878P78PKEM/4DX5.Bfi9;=/!$9%786[d49;:.7 7*/3.&"Bfi5?/>F7*/49
(*!/HD!$&6!4/[/.=>]O^7*B 5$D-=/.&69%#P=Bfi&"/.0]78.78(*=.9%&65?/d.9;:.=.#*d?!+#EP78#%(*Bfi&"O^78P1&2/C.78(89%&65?/CK{dDn5?B !/4[O!$P
#%9;!+9%7A9S:.7<Bfi78#;=.69%&"/.0<fi5&"/49A!$(89%&65?/1>![65.5?,1O!$(89'5\!FO!$P1#'9;!$9%7M5?B (*!=.#'7G9;:.7<,."!/99%5F7*/.PU&"/1!
D!$&6)78PT#%9;!$9'7ab#%787G &60?=Bfi7<prK 5?B9;:.&6#GBfi7*!$#%5./1/.5#%9;Bfi5./.0]5.B #%9;Bfi5./.0](8[.(86&6(]#'5"=.9%&65?/178.&6#%9*K
okh1 b1

okh2

b2
v1 okt1

T1

H2

H1

s1

v2 okt2

s2

T2

R

p
okp

v3 okt3

T3
v4 okt4

H3
okh3

s3

f

T4

H4
b3 okh4

s4

b4

ijlkYmfinporqCsMtvuxwfiy3zvoCnWw>{}|0~bWfiyYr|3j}~oRd|brw>{lo0>o:npoC|3RyYnOjlmfinnpyYmfi~>fioRb[ffyYmfinO>oC|3
oRYRfi|0~>k0oCnpOq0vfi<|0~> [>o>oC|3^oRYRfi|0~>k0oCnpOwfinpyY m>Ro<>jlkYcwfinpoRmfinpo
oC|0yffyYmfinoR{oRRnpjlRjp`k0oC~>oCn|3j}~>kOmfin>j}~>oRq0fi|b~>v vu|3jl{loR:>oC|3
oRYRfi|0~>k0oCnjm>fiorR{ly0oR[b|>{ly>@<|3Rjy>~:RiyYn|^|Fj{loRQmfin>j}~>oWvj>o
yYw:|FRjlyY~<Cm>fioOC|0nnpjloR:yYm>/r>oroC~>oCnpk0:wfinpy> m>RjlyY~hy3>onpoC|3Ry>nj
|0~>C|0~`fioRyY~0npy0{l{loRy >ofioCr|0~>:|3@mfin>j}~>orvj$C|0~`fioR{yboR:0W|
|3{ oY>o3 |0npj}|0>{loRC|0w>mfinpo>ozvyYnbj}~>kr|3m>yF>omfi~>jl/
u~^yYw>j}jljly0{}m>jlyY~Oj}rw>{lWjlkY~>yYnpoRfi|3$py0j}~0|3RjlyY~>vC|0~O{lyYy>wWfi|3@yr|fi|FW|3oyYn
{loC|3Qy||3jl{loRQ|3oO|0~><~>fir|Wyb{m>jy>~:y`>owfinpy>>{oC|/ffoCnyY~>owfinpoRj}r|3kboC|3{lCm>{}|3jlyY~
~bm>jlj oR{l0>oyYw>jjjlw>{}|0~|FmfioRfi|3~>ymfi~>jlzj{l{(|Fj{( mfinpj}~>koR>oRCm>jlyY~|b~>|3{lz|/Y
R>yYy0oRpy0j}~0|3RjlyY~>fi|3{loC|3-fij}npoRR{gnpy>|Wfi|3Q|3oy|Wk0y>yYh|3o0O>oryYw>jjjl
w>{}|0~<jl|0~yYw>j}r|3{RyY~bnpyb{$n|FoRk00fioRC|0m>o^j|F{z|/>R>yYyboR>o>yYnpoRw>{}|0~<yW|^k0yYy>
0

fi00 0Q03
|Fo|0~>Q~>yWy0>oCnn|3oRk0WoR>jlfi|3C|b~:| y0jl<{y>yYw>j}~>kfi|F@`yW|Wfi|3:|3oy>noC~>:j}~:|
|3jl{oR`|3o0
>ojlRo:y3>o|Fowfi|FRoy3>o:|0My owMy*zvoCnOw>{}|0~0Wfiy>r|3j}~[jl0p3u~yYw>jjjl
y0{}m>jlyY~`z|3kboC~>oCn|3oR<0$Wj}~<fi0oRRyY~>fi|0~>Ry>~0|Fj~>oR:0bfiqCr $[~>y>fioRCu|0~
oRfi|0rw>{lo0 |py0j}~0|3Rjy>~Oz|FoR>n|FRoROnyY>ow>{}|0~^y>n|fi|3W|3oz>oCnor|b~>zvoCnpo
|3jl{oRc|b~>:oC~>oCnpk0:fioC^|0~>Qz|3WoC~>oCnpkb-mfi~>jlCz>jl{loW>ooC~>oCnk0:wfinpyY m>Rjy>~<:z|3yY~>{l
qmfi~>jlC<>oOoRYn|3RjlyY~<joz|3fifiqCOoRRyY~>fir|0~>$|3oRdwMoRRoR$>oOoRyFpy0j}~b|FRjlyY~>
j}~>R{}m>fioR-|j}~>k0{lopy0j}~b|3RjlyY~@fi|0~>kbj~>kORr|0~>yrnm>o|0~>WoRj}~>kWyrfi
fffi(- fi[
>owfimfinwMy0o`y3>oWy>RRoCnfiyY^|3j}~QjOy<fioCyY~>n|3oW|<m>{j|Fk0oC~0^fiyYr|3j}~[zj[|yYnpo
oR{}|0fiy>n|3oWoC~ j}npy>~fioC~0OyYfioR{fi|0~h>owfiy3zvoCnrw>{}|0~brfiyYr|3j}~[ffRyY~>jry3pzy`oC|0y3
w>{}|/0oCnpfi|3C|0~:y oj}~:|OkYnpjl<zvyYnp{l:|0~>:wfi|3|Wfi|3{l{yWoC|F@`y0>oCn8uoC|3R`j}ooCw<|
w>{}|/0oCnoRjl>oCny oRj}~`yY~>oy3>oyYmfinr|FyYnfijnoRRjlyY~>yYnwfi|3oR>ofi|3{l{yW|0~>y0>oCnoC|0
w>{}|/0oCnR>o|3jlykboC~>oCn|3o|mfi~>j oCnp|3{$w>{}|0~OffyYnyY~>oy3$>ooC|0fi|3vC|0~Ofio|bwfiw>{jloR<y
RyYno|k0yY|3{z>oC~>o oCnv>ooC|0wfiy0oRoR>ofi|3{l{ff
uj}rw>{lo "!'# fioRCnpj}w>jlyY~hy3>ory>RRoCnfiyY^|3j}~-y>fioR{l>oOoC|0wMy0oRj}~>k>ofi|3{l{
|3O>oC|3kboC~0Ofi|3^C|0~cy o|b~>wfi|Fr>o:fi|3{l{j~>fioCwMoC~>fioC~0`y3oC|3R[y0>oCnR>m>C|
w>{}|/0oCnwfiy0oRj}~>k<>orfi|3{l{C|0~<|3{lz|/Ywfi|Fy|0~by0>oCnoC|0w>{}|/0oCnR>oryYwfiwMyY~>oC~0oC|0
jlyYfioR{l{loR-|3|^oRyFoC~ j}npyY~fioC~b|Fk0oC~0fi|3C|b~y oj}~W>offyYmfin^|3pyYnfijnoRRjlyY~>fim>
fi| o~>y<|3RjlyY~>y>nfi|0~>fi{lj~>kQ>ofi|F{{ff<>ork0y>|3{y3>oWmfi~>j oCnp|3{vw>{|b~:jlyfi| oO|w>{}|/0oCn
zjlO>ofi|3{l{$j}~OnpyY~bvy3>oyYwfiwfiyY~>oC~bk0y>|3{zjl>yYm>fi| j~>kO|0~0OyYwfiwMyY~>oC~0j}~W>ok0y>|3{$|0npoC|fi
rjlj}rwMy0j}>{lo:yhk0oC~>oCn|3o|gnpyY~>kQw>{|b~[fi|3^Ry oCnpr|3{l{wMy0j}>{lo<j~>jlj}|3{|3oRC6i yYn
j}~>|0~>Ror|0~j}~>jj|F{|3ozjl|b~WyYwfiwMyY~>oC~0{ly>C|3oR`j~`>ok0y>|3{|0npoC|^fi|3~>yOnyY~>ky0{}m>jlyY~
vm>|npyY~>kw>{}|0~^Ry oCnpj}~>k|3r|b~0j~>jlj}|3{|3oR|3wMy0j}>{lojljl{l{m>om>{fffifioRC|0m>ojlfio~>oR
|3{l{>o[RyYnpj}~>k0<|FoRy3>oWkY|0oW|0~>Qmfinp>oCnrwfinpy jlfioR|<w>{|b~QffyYnRyYnpj}~>k:>oWkbyY|3{~>y
r|3oCn>o|3RjlyY~>CM>oyYwfiwMyY~>oC~bw>{}|CboCnpR>yYy0o0
hoj}rw>{loCoC~boRc|0~ "!'# k0oC~>oCn|3y>nffyYny>RRoCnfiyY^|3j}~>zj:fij$oCnpoC~b oR{l[jlRoR|0~>
~>mfiMoCnpy3|3k0oC~bC>om>{lj|3k0oC~bkYn|0wfi^j}~WijlkYmfinpoOqC>y3z$ w>{}|0~fi~>j}~>kWj}om>j~>k
>onyY~>kw>{}|0~fi~>j}~>kg|F{kbyYnpjlfij}~<y>RRoCnfiyY^|3j}~>zjl:F{ly>C|3jlyY~>|b~>:yY~>oyWjl:w>{}|CboCnp
yY~OoC|3ROoC|0>ow>{|b~fi~>j~>kOj}ooRoCykYnpy3z!oRdwMyY~>oC~bj}|3{l{`zjlO>o~>mfifioCnvy3w>{|/0oCnC
>jlWjl~>y0rmfinwfinpjlj}~>k|3W~>y0ryY~>{l>o<|3oWwfi|3Ro:fim>|3{lyQ>o<~YmfifioCnOyFy0j}~b|FRjlyY~>
kYnpy3z[oRfiwfiyY~>oC~bj}|3{l{lzj>o~>mfifioCnyF(|Fk0oC~0C(yj}~ oRjlkY|3o>oRyY^w>{oR>jlprj}~0nyY m>RoRW0
py0j}~0|3RjlyY~>zvoRy>~>nm>RoR<| oCnpjlyY~WyF>oyYRRoCnfiyYr|Fj~`zjl`yY~>{l|^j}~>k0{lor>oC|0~>
oC~ j}npyY~fioC~0r|3k0oC~b|b~>n|b~-$-|3k>|3j}~>oWj}~>k0{lo|3k0oC~bkYn|0wfihj}~:ijlkYmfinoqC`>y3z>o
n|0^|3jlfioRCnpoC|3oj}~<RyYrwfim>|3jlyY~`j}o0j~>ybyY jlyYm>>yYm>k>fi|3m>j}~>kyYnpor|3k0oC~b
j}~>CnpoC|3oRO>oWRyY^wfim>|3jlyY~fi|3{v{lyY|3$v|3>jl~>yYnr|3{l{lc|3{ly:npoR m>RoR>o~YmfifioCny3wfinoRj^|3k0o
C|3{lCm>{}|3jlyY~>CfioRC|0m>o|{}|0npk0oCn~YmfifioCny3|3oRjlrnpoC|3R>oR-j}~hoC|3@hjloCn|3jy>~~>fioRoR$j}~Q|
oCnpjy>~yF>owfiy3zvoCnw>{}|0~bfiyYr|3j}~zjlfioRoCnj}~>jjl|3Rjy>~>C3zvoffyYmfi~>>ow>{}|0~fi~>j}~>kj}oy
fioRCnpoC|3o oRo>owMy3zoCnw>{}|0~bkYn|0wfiWj}~WijlkYmfinoqC 8fiz>oC~<yYnpo|Fk0oC~0zvoCnpo|3fifioR oC~>oC~
qC0b R<ukY|3j}~[zvoWoC|3mfinpoR[>oWj}oOy>noR>n|FRj}~>k-|3RjlyY~>npyY >oWkboC~>oCn|3oRcmfi~>j oCnp|3{
w>{}|0~>CWiyYn>om>{lj|3k0oC~b oCnjlyY~:yF>o oWw>{|/0oCny>RRoCnfiy>r|3j}~:>orpzy^ybj~b|FRjlyY~>
|3R>jlo j}~>k>ok0yY|F{$>y3z~`j}~ijlkYmfinoq/zvoCnpooRYn|3RoRWnpyY >ormfi~>j oCnp|3{w>{}|0~<j}~`{oRfi|0~
fibfiqoRRyY~>fiC
0

fi
fiff fffi vfi


fi}

10000
Multi-Agent
Single-Agent
Power Plant
1000

Time / Sec

100

10

1

0.1

0.01
0

2

4

6
Number Players

8

10

12

ijlkYmfinporqCMt {}|0~fi~>j}~>kWj}oyF$yYnk0oC~>oCn|3j}~>knpyY~>krmfi~>j oCnp|F{w>{}|0~>j}~Wy>RRoCnfiy>r|3j}~>
zjlyY~>oyjlw>{}|/0oCnpyY~oC|3RoC|bW$iyYn$>om>{lj|3k0oC~boRdwMoCnpj}oC~0oC|F@w>{}|/0oCn
z|F|3yYRj}|3oROzjlr|b~|3kboC~0C3z>j{loyY~>{l|j}~>k0{lo>oC |b~>oC~ j}npy>~fioC~0|3kboC~0
z|Fm>oRj}~>oj}~>k0{lo|3k0oC~bvoRdwMoCnpj}oC~0/>owMy*zvoCnw>{}|0~bvkYn|0wfi>y3zw>{}|0~fi~>j}~>k
j}oy>n|fioRoCnj}~>jljl oCnpjy>~ry3>owMy3zoCnvw>{}|0~bvfiyYr|3j}~Wm>j}~>kqyWqCYoC
|3k0oC~bC

2

2

1

2

5

3
2

4

2

1

5

3

5

4

3
4
5

4

5
1

1
2

4

4

5
1

1

3

3

3

(a)

(b)

(c)

ijlkYmfinporqCMt {}|0~OoR>oRCm>jlyY~Wo"!fim>oC~>Ro0v>ofinpoRo|3oRv>y3z|r0fiwfiyb>oRjlC|3{$|3|F@rfi|FoRWyY~
|mfi~>j oCnp|3{ w>{}|0~>o|3o |(jl|ORyYnj~>kb|3o03fioRC|bm>o>o|F|3R3oCnp >{|F@
C|0~`oRYn|3R|W~>y>~>oCrw>poRy3py0j}~b|3RjlyY~>npyY>ormfi~>j oCnp|3{w>{}|0~$
#>y>y0j}~>k
yYoybj~b|3RjlyY~>npyY>ow>{|b~fi>o|3|3R3oCnpC|0~^oC~0oCn>ok0yY|F{|0npoC| fi|3fioR
zjlW>ofi|F{{zjl>j}~Wpzvyj}ooCw> |3o |0~> % v~>yr|3oCnzfi|3|3Rjy>~>C
>oyYwfiwMyY~>oC~bw>{}|CboCnpR>yYy0o0
&(')+*-,.",0/214365738".%39:*-;<1:=3>578

u~YmfifioCny3oRdwMoCnpj}oC~0fi| ofioRoC~C|0nnpjloRWyYm>vj}~OfioRoCnj}~>jljlfiyYr|Fj~>vj}~Wy>npfioCny oCnpj
$ wMoCn y>nr|0~>Ro<|0~>!j{l{}m>n|3o<>o<k0oC~>oCn|3{ljlp[y3mfi~>j oCnp|F{w>{|b~> oCnpm>OR{}|3jlC|3{ffo
0@?

fi00 0Q03
!fim>oC~0j}|3{w>{}|0~>C:oRyYrwfi|0nponmfi~jonpoRm>{ly>>|3j}~>oRzjl<$Wj}~`yYoy3>ouBAC
RyYrwMoRjljlyY~QfiyYr|Fj~>y`>oWnpoRm>{ly3>oORyYrwfioRjjy>~[w>{|b~fi~>oCnp "
E oCny0/qC0bR O:o

>oC~>y3zfi|3|Omfi~>j oCnp|F{w>{}|0~j~<|fioRoCnj}~>jljlrfiyYr|Fj~jlyYnpok0oC~>oCn|3{$fi|0~`|R{}|3jlC|3{
"o !fim>oC~0j|F{w>{}|0~fioRC|0m>o|O{}|0npkbo~YmfiMoCnyFR{}|3jlC|3{o"!fim>oC~0j}|3{vw>{}|0~>r|0nporRyY~b|3j}~>oR:j}~h>o
mfi~>j oCnp|F{w>{}|0~
GFffIHKJLNMGOP:Q R R <S
y>mfinw>{|b~fi~>j~>k>oCUT3$(V XW 6Y |0m>[Z\A>oR{}r|0~qC002R*fi 6Y y>oC>{oCnoR|3{ff} q/00R]S R
6^ y>~>_
k Zi y3 qCb C|b~> S/ |3{mfi+Za`o~>oCnR>000cbRyYrwMoRoRj}~O>ofinpoRofiyYr|Fj~>vzvo
fi| orm>fijoR$` T3$d Vd W!jlfi|FoR:yYe
~ SR R T3$z>jl{lo fiQ|0~>f
R c|0npo^kYn|0wfifiw>{}|0~bfi|3oR
w>{}|0~fi~>oCnpC SCQm>oRr|<>oCmfinpjljloC|0npR-|0wfiwfinyY|3R:fi|3oR[yY~Q|<wfinpoCwfinpy>RoRj}~>k:y3>ofiy>r|3j}~
>ou gA Cw>{}|0~fi~>oCnpzvoCnponmfi~OyY~O00 yYnYb % hfioC~bj}mfii
B#zj<qC C >oR
vu "o !dm>j}wfiwfioRhzjl ^ j}~Ym>
>oWkYnpj}wfiwfioCnOfiyY^|3j}~[RyY~>jlry3zvy:npy>yYru|b~>c|-npy>fiy0

zjl<|^{off|0~>:npjlkYbk>npj}wfiwfioCn|0~>:|O~YmfifioCnyFfi|3{l{fi|3C|0~Mory oRb>onpyYMy0C>o
|3jlvyy o|3{l{>ofi|3{l{lnpy> npy>yYu!ynyYyY6zjl>onpyYfiybj}~>jlj}|3{l{Wj}~rnyYyY u>>o
|Fo |0npj}|0>{loRy3v>o "!# oC~>Ry>fij}~>ky3v>ofiyYr|3j}~<|0npo>owfiy0jljlyY~gy3>ornpy>fiy0|0~><>o
wfiybjljlyY~ryF>ofi|3{l{lC>owfiy0jljlyY~^y3>onpyYfiybjoRjl>oCn nyYyY

u yYnvq nyYyYfi R3z>j{lo>o
wfiybjljlyY~Wy3|fi|F{{(C|0~Wfio npy>yY

u Rq npyYy> fi Rfi j}~W{loffkYnpj}wfiwMoCn vyYnv j}~WnpjlkYbkYnpj}wfiwfioCqn R
y>n>our BA2 COkYnpj}wfiwfioCnwfinpy>>{oC>or~YmfifioCny3w>{}|0~<oCw>j}~:|b~WyYw>j}r|F{w>{}|0~`kYnpy3z{j}~b
oC|0np{lOzjl^>owfinpyY>{loC~>mfifioCnRs nyY>{loCqRyY~0|3j}~>rfi|3{l{lC|0~>O>o~>mfifioCnvyFfi|3{l{lkYnpy3z
bpzvyy>noC|3@<wfinpy>>{oC>onpoRm>{yF>ooRdwMoCnpj}oC~0jl>y3z~j~h|0>{lo^y0kboR>oCnzjl
>ornpoRm>{lyF>orw>{}|0~fi~>oCnpj~g>our BA C^RyYrwMoRjljlyY~u k>n|0wfi>jlC|3{npoCwfinpoRoC~b|3jlyY~`y3v>o
w>{}|0~fi~>j}~>kj}oj}~`>o|0>{lojl>y3z~Wj}~<ijlkYmfinpoO%q Cfi$
tWkboC~>oCn|3oRj}~>j}mfi{loC~>k0:w>{}|0~>
m>ory`jlwfi|0n|3{l{oR{vfinpoC|3fibffnoC|bnp@<|3{lk0y>npjlfiWrufioCw>jlRoRQj}~:ijlkYmfino%q Cfijl| ybjfi>o
oRfiwfiyY~>oC~bj}|3{kYnpy3z^y3>ow>{}|0~fi~>j}~>kWj}ofi|3Rfi|0n|3RoCnpjlRoR|3{l{yF>oRy>rwfioRjljlyY~<w>{}|0~fi~>oCnp
oR>RoCw> SC>oC~Om>j}~>k|wfi|0njljlyY~>oRWn|0~>jjy>~npoR{}|3jy>~$rjl>oyY~>{lw>{}|0~fi~>oCnvC|bwfi|0>{lo
y3k0oC~>oCn|Fj}~>kyYw>j}r|F{w>{}|0~>y>n|3{l{>owfinpy>>{oCCWi y>n>jlfiy>r|3j}~:>on|0~>jljlyY~:npoR{}|3jy>~
y3|0~ "!'# fioRCnpj}w>jlyY~<C|0~<Mofij jlfioR-j}~bv
uKwqfi|3jlrwfi|0npjjy>~>Cz>oCnp$
u:jl>or~>mfiMoCn
y3fi|3{l{lCufijlCm>oR<jx
~ A>oRRjlyY~<sfiM>oyYw>j^|3{~>mfiMoCnyFwfi|0npjljlyY~>jl~>y0~>oRRoR|bnpjl{`>o
{}|0npk0oR~>mfifioCny3wfi|0njljlyY~>Ci yYn(>ovnpoRm>{lj}~|0>{looC|3@wfi|0njljlyY~RyY~>jloRy3|RyY~/mfi~>
jlyY~`y3qCWfi|Fjlwfi|bnpjljlyY~>Cy
#yYrwfi|0npoR`yO>oryY~>y0{ljl>jln|0~>jljlyY~<npoR{}|3jlyY~<npoCwfinoRoC~0|3jlyY~
>onpoRm>{lvyY>|Fj~>oROzjl^>owfi|0njljlyY~>oRWn|0~>jjy>~rnpoR{}|3jlyY~^zvoCnpojk>~>j C|b~0{lWfioRoCnyY~^>o
{}|0npk0oCnwfinpyY>{loCCW>oOoCyYnp:m>|Fk0ory3wfinpyY>{loC 0Wzjlh|wfi|0npjjy>~>oR-n|0~>jljlyY~:npoR{}|3jy>~
z|3[ Cb YoR/0z>jl{lojloR>RoRoRfioRW>o{ljjvy3vqC C YoR|3vwfinpyY>{loCqCffyYn>oyY~>yb{jl>jl
n|b~>jljlyY~WnpoR{}|3jlyY~
j$kd,ml/n3opop,0/*-;<1:=3>5

~>oy jlofiyY^|3j}~[>o`|3hjlyhk0oRR>j}w>Cvfij}wwMyYwv@>oRoRo

|0~>:Cn|3@FoCnpC$npoRzj}~>-|`y jlor|0~>:oR>orRyYmfi~boCnyWRoCnpyr>oryY~>{l<j}~0oCnoCnpoC~>RoWfioRpzvoRoC~
j$kd,{za;|N3G,\*-;71x=365

}~7~ sq6sn6%gg 6@fid](@fiqn"ndnfi rr%p nd@g~
~7n6Un
"q$]@r$r][qn fi6nfi]nrfig6@fi 6@qfir% q]
nrNr"6]]@~UNnfi6B<
pr[6]dns>nq
]7@gnfidsq@g
N
r%nfi"%]prfiqnfir"]r@7]@Unsdd6@s
p %]fi%@r"r6q[
d6sqyfi@fi
N r%nnq y]s@g~

0@

fi
fiff fffi vfi


q%





}











}



q

N2 ~
q




q
@
q
@}@
"
q}@



q@@
q
}
q
q}q@
}"

@@




q@}"


"
q@}

}@"

"

}cq@

%


























N2

qn~


@

@
q
q@
q
}@
"
@

q@

@
q
} q
q
"
}"

" @

"}@
%cq

"}

}@}q
"
}}@










r"
}

q@}
@@

















@




































fi}
r
q

q
}@
"


q}
}}
q@
}@
}
cq@
cq"
}@}
@
}
}
""
@"


"
%



}@



@
%
@

@

@
"

}



@

}}@
}@}

















q


































"]


q






































|0>{lofit `npj}wfiwfioCnfiyYr|3j}~WnpoRm>{l/B#vy0{}mfir~OyY~>o|b~>WzvyyYnoC|3RWw>{}|0~fi~>oCnv>y3z>ow>{}|0~fi~>j}~>k
j}oj}~j{l{ljloRRyY~>fi|0~>^>ow>{}|0~^{loC~>k0 t-|0np/|0~>W$ yY~>y>y*z>o
w>{}|0~fi~>j}~>kjoffyYn(m>j}~>k|Owfi|0npjljlyY~>oR:|0~>:|ryY~>y0{ljl>jlrn|0~>jjy>~noR{|FjlyY~
npoRwfioRRj oR{l0i yYn$(zjlwfi|0njljlyY~>oRn|0~>jljlyY~npoR{}|3jlyY~>ov>j}npRy0{}mfir~>y3z
>ov~YmfiMoCn$y3 wfi|0npjjy>~>C (oC|0~>fi|3$>ow>{}|0~fi~>oCnm>oRyYnofi|0~rq/ C >oR$y3
oCyYnpy>n$z|3oCnj}~fi|3oRrfioffyYnponoRmfin~>j}~>k|yb{m>jy>~ ~>{lnoRm>{ly>noRYoRCm>jlyY~>
m>j}~>kO{loRfi|0~<qC C YoR|0npo>y3z~^ffyYnv$
>omfi>k0yY|F{jlfi|3>oy jom>MonoRzy>mfi~>$YfioffyYnpo>oRyYmfi~boCnC|0~OfiooRvyRoCnpy>o
wfinpyY>{loCj}~:>oOy jlorfiyY^|3j}~<yY~>{l<fij$oCnb>o~>mfifioCny3yY>poRRy3oC|F@<pfiwfioryFffyYy>$
>o~>mfifioCny3y>>oRRj}~>CnpoC|FoR{lj}~>oC|0np{lnpy>sffyYfin npyY>{loC qyr3ffyYfin nyY>{loC 0M
mfin "!# fioRCnpj}w>jlyY~y3>oy jofiy>r|3j}~noCwfinpoRoC~boC|3Rrpfiwfioy3$ffyYy>|3|~YmfioCnpjlC|3{
|Fo |0npj}|0>{loWzjlh|n|0~>kbo"o !fimfi|3{y>oO~YmfifioCny3yY>poRRy3fi|3pfiwfioryFffy>yY$O|0>{loW
>y3z>ow>{}|0~fi~>j}~>kjoffyYn$|0~>>ovRyYrwMoRjljlyY~rw>{}|0~fi~>oCnpffyYn>oy jlofiy>r|3j}~wfinpyYb
{loCC ~r>joRdwMoCnpj}oC~b|0~>r>onpoCr|3j}~>j}~>kroRfiwfioCnpj}oC~b$rm>oRjlfio|0m>{lwfi|0npjjy>~>j~>k
y3>oWn|0~>jljlyY~npoR{}|3jlyY~Qi y>no oCnpwfinyY>{loC |3{l{>ow>{|b~fi~>oCnpr~>[>oWyYw>j}r|3{y0{}m>jlyY~
^ j}3oyby3>oRyYrwfioRjjy>~Ww>{|b~fi~>oCnp$^fi|3|{ly3zcRyYrwfim>|3jlyY~^j}o0fifim>jlvjlv>oyY~>{l
w>{}|0~fi~>oCn~>y0>y*zj~>k<|0~bj}~>CnpoC|Foj}~hRyYrwfim>|FjlyY~<j}oro oC~:>y>m>kY$>o^jlRoy3>o^|3o
wfi|3RoyFjloC~>RyYfij}~>kOj}~>CnpoC|3oRnyY0py *
>oW{ly0k0jljlRrfiy>r|3j}~ oR{ly0y qC0FRyY~>jry3RjljloRCvnm>@bC
[
|3j}nw>{}|0~>oRv|0~>rwfi|3@/|3k0oRCnm>R0C|0~yY~>{lry ovfioRpzvoRoC~{lyYC|3jy>~>j~>ov|0ovRjlp0uj}nw>{}|0~>oR
C|0~^yY~>{ly oMoRpzoRoC~^|3j}nwfiyYn{ly>C|3jlyY~>vj}~rfij$oCnpoC~bvRjjoR/>o|3jlvyy owfi|3@/|3k0oRvy
wMoRRj ^{y>C|3jlyY~>/ nyY>{loCfij&oCnbW>o~>mfifioCny3wfi|3RC|3k0oR/ RjljloRC$|3j}nw>{}|0~>oR|0~>nm>@bC
>oW{ly0k0jljROfiyYr|Fj~Qjlfi|bnp$|b~>[yY~>{l
npyY>{loC q0MsfiW|0~>qbqWy3>obwfinyY>{loCrzvoCnpo
y0{ oRcb-|0~b-w>{}|0~fi~>oCnj}~Q>oOu BA C`RyYrwfioRjjy>~ oRoW|0>{loW 8>o "!# fioRCnpj}w>jlyY~
j$kd,\s;U(38".%398*-;71x=365

0

fi00 0Q03

10000

1000



Time (Sec.)

100

10
UMOP Part.
UMOP Mono.
STAN
HSP
IPP
BLACKBOX

1

0.1

0.01
0

2

4

6

8
10
12
Problem Number

14

16

18

20

ijlkYmfinporq%CMt {}|0~fi~>j}~>kj}oy>n$`|0~>>ourBA2CrRyY^wfioRjljlyY~<w>{}|0~fi~>oCnpffyYnv>okYnpj}wfiwfioCn
fiyYr|3j}~:wfinpy>>{oCCtx|0np/|0~>Q$ yY~>y >y3z>orw>{}|0~fi~>j}~>k:j}offyYn
$Wm>j}~>kW|rwfi|0npjljlyY~>oR<|0~>|^yY~>y0{ljl>jln|0~>jljlyY~noR{|FjlyY~ npoRwMoRRj oR{l0
y3>oW{ybk0jljlRrfiyYr|Fj~[m>oR~YmfioCnpjlC|3{|Fo |0npj}|0>{loROynpoCwfinoRoC~0^{y>C|3jlyY~>^y3wfi|3RC|3kboRC
z>oCnponm>@b|0~>W|3j}nw>{}|0~>oR|bnponpoC|3oRW|3vwfioRRj}|3{({y>C|3jlyY~>/ oC~r>yYm>kY^>o|3owfi|FRoy3
>o^|3{l{wfinpyY>{loCjly>fioCn|3o0d(O|3jl{lyyb{ o|0~0Oy3>owfinpyY>{loCj}~W>ofiyYr|3j}~
m>RRoRoRfiyrk0oC~>oCn|3o>on|0~>jljlyY~noR{|FjlyY~Wfim>|3jl{ly~>jl`>owfinoRj^|3k0oC|F{Cm>{}|3jy>~>C
hofi| om>fijloR<>o{ybk0jljlRfiyYr|3j}~oRYoC~>j oR{0$noRRoC~0{ly>Cm>j}~>ky>~ $fi|FoR<fio
oCnj~>jlj:w>{}|0~fi~>j}~>k >o`{ybk0jljlRWfiyYr|3j}~6oRoCOy-fio<fi|0np!m>j}~>kc|:w>{}|3j}~ (fi|3oR
|0wfiwfinpy>|3@|3>ojlRoRy3>owfinpoRj}r|3k0oRkYnpy3z!y>y|3C(y|3fi npoR>jlRyY^w>{oR>jlp0 zvofi|
fio oR{lyYwMoR|0~`|0>n|3RjlyY~`oRRfi~>j !fim>offyYn$fi|3oRQfioRoCnj~>jljOw>{|b~fi~>j~>kr~<|W~Ym>>oR{l{
|WwfinpyY>{loCjlny0{ oR:m>j}~>k|b~|b>n|3Rn|b~>jljlyY~`YoCWz>oCnpooC|3R`n|0~>jjy>~Ry>nnpo
wMyY~>fiy|WoRy3oCnpj}|3{ljlC|0>{lo|3RjlyY~>CW>oC~h>oroCw>j}~h>o|0>n|3Rw>{}|0~Q|0nporoCnj|F{jlRoR
m>j}~>k|0~yYnpfij}~fi|0npWn|0~>jljlyY~WYoCWv!jW>jl~>oRz |3{lk0yYnpjlfiWMzofi| oMoRoC~|b>{oyOy0{
oCn|3{y3>oRyYrw>{loRWur BA2 CRyYrwMoRjljlyY~`{ybk0jljlRwfinyY>{loC oC~>oC~WoR|3{ff}fi000 8
GFGF(f R $d Q
>oy>>|3R{lofiyY^|3j}~fi|3MoRoC~rRy>~>nm>RoR^yfioCy>~>n|3o>ok0oC~>oCn|3{ljlpy3mfi~>j oCnp|3{w>{|b~>C
ffRyY~>jly3|WkYnjQzvyYnp{l:zjlh RoR{l{lCB u[y>>|3R{loR|b~>-|npy>fiy0|3k0oC~bC>oOwfiy0jljlyY~>y3
>ory>>|3R{loR|bnpor~>y0fio~>oR$>o^k0yY|3{wMy0jljlyY~<y3v>onpy>fiy0jl>omfiwfiwfioCnnpjlkY0RyYn~>oCny3
>ork>npjl$|b~>:>oO|3yYn>onpy>fiy0jlyy onpyY|b~0:wfiybjljlyY~:j}~h>orkYnjQyW>o^k0yY|3{
wfiybjljlyY~WoRC|bm>o>orj}~>jj|F{{ly>C|3jlyY~>y3yY>|3R{loR|0npormfi~fiY~>y3z~>oOmfi~>j oCnp|3{vw>{|b~-m>
|0Fo|0~bwfiy0j}>{owfiy0jjy>~Wy3$yY>|3R{loRj}~0yr|3RRy>mfi~0C>z>jlROk0j n< j}~>jlj}|3{$|3oRC
y>n|wMoRRj j}~>jlj}|3{|Fo|"o !dm>oC~bj}|3{w>{|b~rC|0~^fiok0oC~>oCn|3oRnpyY >omfi~>j oCnp|3{w>{|b~>m>C
n< W"o !dm>oC~bj}|3{w>{}|0~>|0nporRy>rwfinpjloR:j}~<yY~>oOmfi~>j oCnp|3{vw>{|b~
y0ofi|3|Wmfi~>j oCnp|3{
0@

fi
fiff fffi vfi








}










}



q


q
q

q}

q
q


|0>{lofit

N2





}






}

}

}

}

}

}

}
}
}


}

"2



q



q

q}

q
q

"

"



"
"

q

q

}"
}
}

r
"
"
"
"
@q
}
}

"
@
"
"q}
@
@
@
}
}"
}}




}}
@

@
q
@
@
%

%






q
q



q






q
q
q
q
q


q

q




fi}
"%]
@
c

}


@

}

@

@}

@
@
%
@


@
"

@


}@
}@
}@
}@}

jlo<fiyYr|Fj~!npoRm>{C >o<|0>{lo<>y3zO>o:nmfi~j}o<j}~jl{l{jloRRy>~>fi<y>nroC|3R
>w {}|0~fi~>oCnR oC|b~>fi|3>ow>{}|0~fi~>oCnm>oR[yYnpo^fi|0~[qC2C >oRyFoCyYnp`yYn
z|3voCnj~fi|FoRMoyYnonpoRmfin~>j}~>kW|y0{}m>jlyY~u{l{w>{}|0~fi~>oCnpk0oC~>oCn|3oRWyYw>j}r|F{&w>{}|0~>
y3{oC~>kbM[ tOm>oRO|0n{loRfi|0~`qC C >oRffyYnv|0~bwfinyY>{loCj}~O>jlfiy>r|3j}~


w>{}|0~OzjluyY>|3R{loRvj}~>R{}m>fioR|0~0Omfi~>j oCnp|3{$w>{}|0~Ozjl<qyuWyY>|3R{loRC |3vyY>|3R{loRC|b~Mo
w>{}|3RoR|F>o|0o{ly>C|3jlyY~B y0oy>npoRy oCnR0fi|3v>omfi~>j oCnp|3{w>{|b~>~>o oCnRy oCn|F{{(j~>jlj}|3{
|FoRCMoRC|0m>oWyY>|3R{loRC|0~QfioWw>{}|3RoRc|3>ork0yY|F{wfiybjljlyY~|0~>:y>>|3R{loRC|0~Q>{y>R<>o
npyYMy0C
umfi~>j oCnp|F{w>{}|0~yYn^|0~[yY>|3R{lofiyY^|3j}~[zjls<yY>|FR{oROz|3k0oC~>oCn|3oRczjl$
j}~QY0oRRyY~>fir|0~>QRyY~0|3j}~>oR[ CC00bW$~>y>fioR qCfi >oR@ R
A>"o !dm>oC~bj}|3{w>{}|0~>zvoCnpo
oR>n|3RoRnyY>omfi~>j oCn|3{w>{|b~`y>n|^wfioRRj wMy0jljlyY~<yF>oyY>|3R{loRCijlkYmfinpoWqC^>y3z
>ooR>n|3Rjy>~rj}oyF"o !fim>oC~bj}|3{w>{}|0~>vffyYnv|0~Oj}~>CnpoC|3j}~>kW~YmfiMoCnvy3oCw>j}~W>ow>{}|0~ oC~
>yYm>k><>o $!npoCwfinoRoC~0j~>k<>oOmfi~>j oCnp|3{w>{}|0~hj{}|0npk0o0>oroR>n|FRjlyY~<jl oCnpW|3|0~>
yY~>{lWkYnpy3zv{lj}~>oC|0np{lzjO>ow>{}|0~W{loC~>k0
>ooRy3|FRjlyY~>|3y>Rj}|3oR<zjl<|^|3oj}~:|Omfi~>j oCnp|3{w>{}|0~O<jloRYn|3RoR<0`RyY
wfim>j}~>k>oRy>~/mfi~>RjlyY~gy3>or$cnoCwfinpoRoC~b|3jlyY~`y3r|b~>$ufioRCnpj}fioRQj
~ A>oRRjlyY~:fi
>jly>wfioCn|3jy>~:fi|3|b~:mfiwfiwfioCnfiy>mfi~>:RyYrw>{loR>jp:yF 8i yYn>omfi~>j oCnp|3{w>{}|0~<j}~h>o
0@

fi00 0Q03







N2


r"
}

%
"
}@@"
q


cq@}
















r
}

@
@
@}

@
@
}
c
@


@
}


q@



}@
@


q@}"

}@}
"




}"
"

|0>{lo ^ k0jljlRvfiyYr|3j}~OnpoRm>{lCiyYnoC|3Rw>{}|0~fi~>oCnRy0{}mfir~Oy>~>o|0~>Ozvy>y3z>onmfi~^j}o
j}~:jl{l{ljloRRyY~>fi|0~>h>orw>{|b~:{loC~>k0 oC|b~>fi|3>ow>{}|0~fi~>oCnm>oR-y>npofi|0~
qC2 C >oRy3oCyYnyYnz|3oCnj}~fi|3oRMoyYnonpoRmfin~>j~>kO|y0{}m>jlyY~
0.008

0.007



Time (Sec.)

0.006

0.005

0.004

0.003

0.002

0.001
0

2

4

6
8
10
Number Plan Steps

12

14

16

ijlkYmfinporqCMtvj}oyYnoR>n|3Rj}~>kWo"!fim>oC~bj}|3{w>{}|0~>nyY|Wmfi~>j oCnp|F{w>{}|0~`ffyYnv>oy>>|3R{lofiy3
r|3j}~WzjlWsyY>|3R{loRC
yY>|3R{lofiy>r|3j}~`zjlO oyY>|3R{loRC>jlRyYrwfim>|3jy>~z|3|3 {loRfi|0~WyY~>ojl{l{jloRRy>~>
|0~>WzvyYm>{l|F{{ly3z|0~oRYoRCm>j~>kWnpyYMy0vyoRoR{y3znoC|3RjlyY~WjoRyY~>n|Fj~bC

7

s-7ggnr7mdBmsg

~`>jl|0njlR{o^zofi| orwfinpoRoC~boR:|~>oRz $fi|3oR-w>{}|0~fi~>j}~>kYoCW$$My>nw>{}|0~fi~>j}~>k
j}~!~>yY~bfioRoCnj}~>jljl0m>{lj|3k0oC~bWfiyYr|Fj~>/u~[oRfiwfinpoRj ofiy>r|3j}~fioRCnpj}w>jlyY~!{}|0~>kYmfi|Fk0o0

! fi|3MoRoC~Wfio oR{lyYwMoR:|0~>|0~Ofio ffORjoC~b$[npoCwfinpoRoC~b|3jy>~Oy3jl ifiuoC^|0~0jRfi|3
fioRoC~gfioRCnpj}fioR$hofi| o|0~fi|3{l>RoRgwfino jlyYm>w>{}|0~fi~>j}~>k|F{kbyYnpjlfiffyYn $fi|3oR-w>{}|0~fi~>j}~>k
|0~>fioRoCwfioC~>oR>omfi~>fioCn|0~>fij}~>ky3dz>oC~>oRovw>{}|0~fi~>j}~>k|3{lk0yYnpjlfi|0npo|0wfiwfinpy>wfinpj}|3o0ij}~fi|3{l{0
zvofi| owfinpy>wfiy0oRQ|0~<yYw>j}jljWw>{}|0~fi~>j}~>k-|3{lk0yYnjfiffyYn~>fij}~>k<oC~>j}>{loWy0{}m>jlyY~>j}~:yYo
fiyYr|Fj~>z>oCnpo~>ynpyY~>kyYnvnpyY~>krR>R{ljlry0{}m>jlyY~oRYjl/>onpoRm>{yY>|3j}~>oRzjl`$
|0npooC~>RyYmfin|3k0j}~>k fi|3(rfi|3v|k0y>yYOwfioCn ffyYnr|b~>RoRyY^wfi|0npoRryyYoy3$>o|3oRR{}|3jlC|3{
w>{}|0~fi~>oCnpY~>y3z~OyY |/0
C

fi
fiff fffi vfi


fi}

mfin^npoRoC|0npRQfi|3r n|/z~hyYmfin|3oC~bjlyY~hy:|:~>mfiMoCny3y>wfioCe
~ !fim>oRjlyY~>rfi|FzvoWzvyYm>{l
{lj}3oy|3fi npoRj}~h>om>mfinpo0r~hwfi|0npjlCm>{}|0nzvorzvyY~>fioCn>y3zzvoR{l{yYmfinoC~>Ry>fij}~>k<y3w>{}|0~fi~>j}~>k
wfinpyY>{loCC|3{loRRy>rwfi|0npoRy>ovoC~>Ry>fij}~>km>oR0s #mfinnpoC~b{lr oC~>RyYfij}~>kfiy>oR~>y0
mfiwfiwMyYnp|rwfi|0npjljlyY~>oR<npoCwfinpoRoC~b|3jy>~Wy3>on|0~>jljlyY~WnpoR{}|3jy>~fim>>ooC~>Ry>fij}~>kr|/fi|
y0>oCnwfinpyYwMoCnpjloRfi|3CfioRw>jlor>oryY~>yb{jl>jlnoCwfinpoRoC~b|3jlyY~$r|/<r|03oj|WMoRoCn@>ybjRo0
>oOpzyWYoCr|/:|3{ly<fi| o|0~<"o !fimfi|3{wfioCn ffyYn^|0~>Rorz>oC~Qfiy0-|0nom>j}~>k:|yY~>yb{jl>jl
npoCwfinpoRoC~0|FjlyY~ |3j}~h>oMoC|0 z|3{}<oRd|brw>{l%o Rz>jl@Q>y>m>{[kbj o$:|0~Q|3 |0~b|3kborj}~
fiyYr|Fj~>z>oCnpo|^wfi|0npjljlyY~>j}~>kOy3>on|0~>jljlyY~WnoR{|FjlyY~WC|0~`fiofio~>oR$
u~>y0>oCnj}~boCnpoRj~>k !dm>oRjlyY~rjlyj}~ oRjlkY|3ovz>jlRbj}~>ry3&w>{}|0~fi~>j}~>krfiy>r|3j}~>jlm>j|0>{lo
ffyYn$fi|3oRw>{}|0~fi~>j}~>k z|3mfinwfinpjlj}~>kyYnm>fi|3>o{ly0kbjjlRfiy>r|3j}~mfin~>oRryYm>yMo
yrfi|0npOffyYnv$voRRoC~b{lWzvofi| om>fijloR`>jlfiyYr|3j}~O>y>npyYm>kY>{l0 j}~>k|0~O|0>n|3RjlyY~
oRRfi~>j !dm>orzvofi| o~>y3zfioRoC~h|0>{loyry0{ oo oCn|3{$y3>o{ly0k0jljRwfinyY>{loCj}~>ou BA C
RyYrwMoRjljlyY~ oC~>oC~WoR|3{ff} 00b R
>oCmfinnpoC~bfio~>jljlyY~y3
:jlwfiy3zvoCn m>{>fim>>y>m>{fiooRYoC~>fioRyoC~fi|0>{loy>fioR{l{j}~>ky3
RyY~>nm>Rj ovfi~>oCnpk0oRjlvo$oRR|FfioRCnjMoRj}$
~ A>oRRjlyY~ u{y 3zvooC~ jljlyY~ryYnpooRfiwfioCnjoC~0
RyYrwfi|bnpj}~>km>{lj|3k0oC~b|b~>:j}~>k0{lo|3k0oC~bfiyYr|3j}~>yj}~ oRjlkY|3o^>orRyYrw>{loR>jlhy3

npoCwfinpoRoC~0|FjlyY~Oy3RyY~>CmfinnpoC~b|3RjlyY~>/
A>o oCn|3{w>{}|0~fi~>oCnp/ j}~<wfi|0npjlCm>{}|0n $ oR{ly0yoR|3{ff}$qCb0s Rfifi| o>y*z~Ofi|3fiyYr|3j}~
>~>y*z{oRfikbo>y>m>{Wfiom>oRbr|w>{}|0~fi~>j}~>kr>oCj}~^yYnpfioCnyC|3{lomfiwrynoC|3{zy>np{lwfinpy>>{oCC
u{y v|3R@>m>y
Z |0fi|b~>C|fiqC002 >y3z>y3z>ooC|0npR`npoRoyF|^ffyYnpz|0npWRfi|3j}~>j}~>kgw>{}|0~fi~>oCn
C|0~Ofiofio ffRjloC~b{lwfinmfi~>oR0r|3j}~>k>ok0y>|3{|3v|yYnm>{}|j~OoCrwfiyYn|3{ {ly0k0jlyY~^>o"o !fim>oC~>Ro
y3$|3RjlyY~>{loC|3fij}~>kry>ok0yY|F{~^>jlz|/>ok0yY|3{>C|0~j}~>R{}m>fio>~>y3z{loRfik0o|0MyYm>>ofiyYr|3j}~
o0 k}Cfi|3y*zvoCnpj}~>o>{y>R0zy>np{lm>fiofim>j{lnpy>My0yYyyY
w R(ucj}jl{}|0n|0wfiwfinpyY|3R
ffyYnnpoR m>Rj}~>k:>orRyYrw>{loR>jpQy3$fi|3oRcw>{|b~fi~>j~>khoRoCrwfinpyYjlj}~>k oRwfioRRj}|3{l{lcfioRC|0m>o
oRRfi~>j !dm>oRy>nvoRj}~>kroCrwfiyYn|3{ffyYnm>{}|3|3{}npoC|3fifi| ofioRoC~fio oR{lyYwfioR<j}~yYfioR{$R>oRR0j}~>k
>oCnm>mfinpoW@fi|F{{loC~>k0oRWj}~>R{}m>fio:j}~bnpy> m>Rj~>k[|0>n|3RjlyY~Qj}~c (fi|3oRw>{}|0~fi~>j}~>kc|0~>
fio~>j}~>kwfioRRj}|3{ljlRoRw>{}|0~fi~>j}~>k|3{lk0y>npjlfiffyYnm>{lj|3k0oC~bfiyY^|3j}~> o0 k }3|3{lk0yYnpjlfim>j}~>k>o
{loC|3~>mfifioCny3|3k0oC~byYny0{ j}~>kW|wfinpyY>{loC
R

sBU
\U7
AfiwfioRRj}|3{fi|0~fibvy|3y0{lyn|

oCnpy |0npRyrvy oCnj&|b~>>oyb>oCnvoCfioCnpvy3>o Afi!k>npyYmfiw
ffyYnj}~0npyY m>Rj}~>k[m>yg |0~>[ffyYnr|0~b[npoRz|0npfij}~>k:fijlCm>jlyY~>Wy>~c $fi|FoRw>{}|0~fi~>j}~>k
|0~>[y>fioR{R>oRR0j}~>k h:o|F{y:zjlhy<fi|0~fi:|b~> |3{vnpY|0~bC mfi~>
#v{}|0n3o0oC~finpj}
(u~>fioCnpoC~ n~ ^ j}~>> jloR{}oC~|0~> ^ |0npj}nFoR |3{ffyYn|F jlRoOyY~: $!jlm>oR|b~><y>nr|3{
npoCwfinpoRoC~0|FjlyY~ij~fi|F{{l0zvofi|0~fi>o|0~>yY~bdyYm>npo jloRzvoCnpyYn>oRjnRyY^oC~0fi|3kYnpoC|3{
j}rwfinpy oRW>owfinpoRoC~b|3jlyY~OyF>jl|bnpjlR{lo0
>jWzvyYn:z|3OC|0nnpjloR!yYm>Oz>jl{lo:>o`npW|0m>>yYn^z|3 jljlj}~>k #|0n~>oRk0jlo oR{l{lyY!
~ ~>j
{ ~>j oCnpjp:yF EoC~fi^|0n>oWnpoRoC|0npRhjwMyY~>yYnoR-j}~[wfi|0np0
oCnpjp`npyY>o(oR@fi~>jlC|3"
j}~>oRf
Z+#vyYrwfi|b~007 A>oR{}oC$
n Zn|0~>o0 y>~>$$>
EoffoC~>ou |0~>RoR[voRoC|0n@4
npy0poRR

^

^
uk0oC~>R Eu

u |b~>>ouj}ni y>npRovoRoC|0n@ |0MyYn|3yYnp ui mfi~>fioCn|3kYnoRoCoC~0~>mfiMoCn
i00b0/0/*00s0fi>o jloRz|b~>rRyY~>R{}m>jlyY~>vRyY~b|3j}~>oR>oCnoRj~O|0npov>yboy3>o|0m>>y>np|0~>
>yYm>{l-~>ybfiorj}~boCnwfinpoRoRQ|3~>oRRoR|0npjl{l-npoCwfinpoRoC~0j}~>k`>o#y ffRj}|3{wfiyb{jlRjloRryYnoC~>fiyYnpoCoC~0C
C0

fi00 0Q03
oRjl>oCnvoRfiwfinpoRoRWy>nj}rw>{ljloR$ y3>o EoffoC~>ou |0~>RoRoRoC|0npRnpy0poRRuk0oC~>R Eu u
R
>ouj}niyYnpRovoRoC|bnp@ ^ |0MyYn|3y>np ui ^ vy>nv>o$A`y oCn~fioC~0/

&%%

(' *)
+-,-.{gBgK/B1032 s\54
j$kd,;(/,17698;: =<?>A@CB0DC@<E3BFG>GHIfi<KJL(< E3<EM>RDCFNPORQS@ET UV@GTVFGW
LX FYEZLXVFI[F\F^]_<;>`L>-@CB0DC@<E
BF^>`HYIa<bJcL(< Ede<fEd
hg<LXiLX FC>G@DCFP>`FYDC@EL(<HG>@#>\Pj
klpI / : ^ oRmn\o qpZrastrau fioC~>y0o>o ifiu oRo[Eo~>jljlyY~`q%(RyYnnpoRwfiyY~>fij}~>ky>ovoCr|0~bjlR
y3v[
|3fio~>oRb `j}mfi~>@>jlk0{lj}|oR|3{ff qC002Ru~w- fiyYr|Fj~fioRCnjw>jy>~x
zjloCr|0~bjlR
o"!fimfi|3{fiy$m n C|0~MoRyY~>nm>RoRj}~r>offy0{l{ly3zj}~>kz|/t ^ oRA
fio|j}~>k0{lo|3kboC~0vfiyYr|Fj~z>oCnpo
|3{l{y m>oC~b|0npooC~>Ry>fioR|3~>mfioCnpjlC|3{|Fo |0npj}|0>{loR|0~>W>oCnojl|b~W|3RjlyY~^ffyYnvoC|3ROoR{loCoC~0
j}~<>or|3{}wfifi|0MoR y3zm n y#yY~>jlfioCn>or|3RjlyY~&
{ |Fy>Rj|FoR<yWj}~fiwfim>A| ^ oR>ooRy3
RyY~>n|3j}~>oR<|3o |0nj|b>{oRy3l
{ o"!dmfi|F{>ooRyF|Fo |0npj}|0>{loRj}~} >orwfinpoRRyY~>fijljlyY~<y3
{ jl|0~`oRdwfinpoRjlyY~<fi|3fio~>oR>ooRy3v|3oRfi| j}~>k<|0~`yYm>k0y0j}~>kOn|0~>jjy>~`y>nj~fiwfim>
W
>oro$oRRRyY~>fijljlyY~:yF*`
{ jlr|ORyY~/mfi~>RjlyY~:yFRy>~>fijjy>~fi|3{vo&oRR (~ C RO>oCnporjlyY~>o
RyY~>fijljlyY~fi|3{o$oRR^y>nroC|3R|3oWfi|3Wfi|3|0~[y>m>k0y0j}~>kQn|0~>jljlyY~6ffyYn^j~fiwfim> ~
j}~>o
RyY~>fijljlyY~fi|3{o$oRR|3yYRj}|3oRzj|3ojl>oRfi|0n|3RoCnpjljloRdwfinpoRjlyY~`ffyYn^|0~> jl|
Rfi|0n|3RoCnjjloRfiwfinpoRjlyY~yYn>ooRy3~>oRY|3oR u r rR
V@ 2BU
v|3RRYm>Ci}pZ |bfi|0~>C|fii q/00R3 j~>k<oC^wfiyYn|F{{ly0k0jlOyRyY~bnpyb{oC|0npR:j}~Q|OffyYnpz|0np
@fi|3j}~>j}~>krw>{}|0~fi~>oCnR$~K`fi|3{l{}|0 }Z jl{|b~>j>u fiCRVFgB<I[FaHYL(< E>A<fEC 8 JcQ@EE<E>T
wfiw$qR qfi qCs0fi>A npoRC
v|0n|3{ff#%ZI`oR{y>~>$ qCb0R0voC|FyY~>j}~>k|0MyYm>o&oRRy3 Ry>~>CmfinnpoC~b|3RjlyY~>CX F*d UVIfiE@Q
: $ <Hk-pI I[@DCDC<EYT C0s_ q0qCfi
UBg

oC~> |fi |3kY|0~fi~fi|3fi|0~ }UZmEy> >j|/z|3{}|fi qCCbRN~yYw>j}r|3{(RyYyYwMoCn|3jlyY~y3Y~>y3z{
oRfik0oy>mfinpRoR$$|0~oC^w>jnjC|F{fij~ oRjk>|3jlyY~Y(oRR0npoCwCfi# A0`0fiqCYfiCfiCy>oRj~>ku |b~>RoR
$oRRfi~>y0{ly0k0
#voC~boCnR y>oRj~>
k #vyY^wfim>j}~>kA>oCn jRoR/
{}mfiWfiu} ZimfinpC ^ qC0b Ri|3vw>{}|0~fi~>j}~>kfinyYm>kYOw>{|b~fi~>j~>kOkYn|bwfir|0~fi|3{l>jlCIaL(< lHY<f@Q
8 EL(FYQfQ<T FEHaF/ #0 2 Cfifiq d0bfi
{lY>o0 qCb CRV k-QS@EE<ETPU EBFI-UVEHfiFIfiL(@<fEL(<fEwB#E@DC<HB0 DC@<E>R0 Er/>oRjlC #vyY^wfim>oCn
A>RjoC~>R$
EoCwfi|0noC~bC #|0n~>oRk0jlo oR{l{lyYi
~ ~>j oCnpjlp0[ # # Ab C*qRYfi
{lY>o0 } Z oR{ly0y qCb0 R(u~fi|3{ly0kbjC|F{npoCw>{}|/rffyYnfio ffRjloC~0RyY~>fijljlyY~fi|3{w>{|b~fi~>j~>k ~
klIp HfiFaFfiB<ET> : LX F^LXZ@L(< E@Qv E : FYI[FEHfiF EZIfiL(< lH<@Q 8 EL(FYQfQ<T FEHaFPf"\ 8 00wfiw
0 CYdb0fi0uuug npoR/
yY~>oRC0} ^ yYoCnj~>R/0 `^}] Z\`o~>oCnR0 qC0b R0u!npyYfim>|b~>|F|FRjlyY~oR{loRRjlyY~roR@fi|0~>jl
yYnw>{}|0~fi~>j}~>k 3
~ klIp HfiFfiFaB<ET> : LX FCGLXM@L(< E@Q E : FYIFYEHfiF EMIaL(< lHY<f@Q 8 EL(FQQS<T FYEHfiF
f"\ 8 0Ywfiw MqR# dfiq/fi0uuufi npoRC
vnpY|0~bC$ qC C02 R
`n|bwfibfi|3oR-|3{lk0y>npjlfiffyYnMyYy0{loC|0~gmfi~>RjlyY~-^|0~>j}wfim>{|FjlyY~ 8
I@E>G@HL(< E> E D*JcUVL(FI>@v b 00Y fi0fiq0
CC

fi
fiff fffi vfi


vmfinpR }#v{}|0n3o0>}Z
jlyY~npoR{}|3jy>~>C ~
yYnbyb{{}|0~>$

^

fi}

y>~>k Er Cq bfiq%RA>fifiy0{ljly>fioR{@>oRR0j}~>kzjlOwfi|0npjljlyY~>oRWn|0~>j

8 EL(FIfiE@L(<E@QE : FI[FEHaFvE}FIfi" v@IT F-Ha@QSF 8 E L(FT I[@L(<E /wfiw3YYdsCM

#vj}r|3jff$u}N`jmfi~>R>jlk0{lj}|fi}N`j}mfi~>@>jlk0{lj}|fii}7Z

n| oCnpy 7 qCb0R${}|0~fi~>j}~>k j}|y>fioR{
@>oRR0j}~>k t(u[fioRRjljlyY~rwfinpy>RoR mfinpoy>nN$O<Y ~Mk-IpHfiFaFfiB<ET> : LX FlLX UVIpfiJFfi@ExE : FI[FEHaF
EMk-Q@EE<fETZ k 0 ^ oRRmfinp y0oRj}~unpj Rj}|3{~boR{l{jlk0oC~>Ro0wfiwfiqC0Yq8Yfi%Afiwfinpj}~>k0oCn
oCnp{}|3k

#vj}r|3jffuy

B E
Efi|FoR[k0oC~>oCn|3jy>~:y3
oCnpjff }B Zn| oCny qC0 C02| ROum>y>r|3jl
mfi~>j oCnp|3{w>{}|0~>Wj}~~>yY~bfioRoCnj}~>jljl:fiy>r|3j}~>C~klIpHfiFfiFaB<ET> : LX F#LX1@L(< E@Q
E : FI[FYEHfiFExIaL(< lHY<f@Q 8 EL(FQQ<TVFEHaFM\" 8 fiF>wfiwC00s_0CCfiq0buuu[npoRC

#vj}r|3jffvu}y

A>npyY~>k[w>{}|0~fi~>j}~>kcj}~!~>yY~bfioRoCnj}~>jljl
oCnpjff }fi Zxn| oCnpy [ qC02 C0 RI
fiyYr|3j}~> }j |Wy>fioR{R>oR@bj}~>k ~klIpHfiFfiFaB<ET> : LX FMLX 8 EL(FIfiE@L(< E@QE : FI[FYEHfiFE
IaL(< lHY<f@Q 8 EL(FQQ<TVFEHaF"k-QS@EE<ETCY>`L(FYDef 8 k #fi0Ywfiw 0_ YYfi>uuuB npoRC

#v{}|0nFo0 }`nmfifioCnpk }UZ

oR{loR$Er

q/00RYBFQX FaH8_<ET>



\npoRC

#v{}|0nFo0

}oCnpyY~uB Z A>jl{}|fivu qC2 C0 R<um>yYr|Fjl oCnj C|FjlyY~[y3~>jo
|3oRyY~>CmfinnpoC~bYoCm>j}~>k<oCrwfiy>n|3{{ybk0jlwMoRRj C|3jlyY~>CxCI@E>G@HL(<E>E
klIp TVI@DDC<ETw @ET UV@GTVFG>@EB3>GL(FDP>R R30# db0fi
}7Z|3o0u /q 0fiq%R
w>{|b~t>oyYwMoC~:w>{}|0~fi~>j}~>k-|0npR>jloRRmfinpo0MtIaL(< lHY<f@Q
FEHaF/c #b Y_ 0C0fi

#mfinnjo0



8 EL(FQQS<;

EoC|0~}

|3oR{}>{lj}~>k ^ j}nr|0~ } Z jl@>yb{yY~u q/00sRa{}|0~fi~>j}~>kmfi~>fioCnj}o
RyY~>n|Fj~bvj}~WyYRfi|3jlfiyY^|3j}~>CltIaL(< lHY<f@Q 8 EL(FQQ<TVFEHfiFCvY3 0sYfi3

Ej

|0~>Ry }`j}mfi~>@>jlk0{lj}|fi}ZmffW~>y qC0C8({|b~fi~>j~>k j}|y>fioR{R>oRR0j}~>kj}~OfioRoCn
j}~>jjlfiyY^|3j}~>CtsnpoR{lj}j}~fi|0npWnpoCwfiy>npC~3k-IYHaFfiFfiB<fET>v : LX FPLX 8 EL(FIfiE@L(<E@QE : FI
FEHfiF EiIfiL(< lH<@Q 8 EL(FQQ<TVFEHaF#-&FLXYY B0 Ql TVW->GL(FDP>$@EBM-JJcQ<fHfi@L(<E>Z 8 #fi0
wfiw 0fifiq fi00fi Afiwfinpj}~>k0oCn oCnp{}|3k
Cq C08A>jlmfi|FoRcRyY~bnpyb{nm>{loRC!~=klIpHfiFfiFaB<ET>r : LX F >GL 8 EL(FIfiE@L(<E@Q
E : FI[FYEHfiF Ek-Ia<EHY<KJcQF^> :C E glQFfiBGT FwlFJcI[FG>GFEL(@L(< E@EBlFfi@#>RE<ET 0$wfiw
qC0YqbqCfi yYnkY|0~ |0mb^|0~fi~

EnmfiryY~>$

EnmfiryY~>$

ZvnpoRj}~fi|fi qC008 u~0>j}o:d~b>oRjl-wfinpyboRRjy>~t |3Yj}jlRj}~>k!>o
wfinpyYfi|0>jl{ljl[yFk0yY|3{v|3j|3Rjy>~c~=klIpYHaFfiFaB<fET>r : LXVFLX!E : FI[FYEHfiFWE1IaL(< lHY<f@Q
8 EL(FYQfQ<T FEHaF/0wfiw$qC C_qR0 buuu[ npoRC

Rjy>~>j }|0~fi0/ A}hoR{$Er}fiEn|0wMoCnRgEr} ^ } Z!jl{l{lj|byY~ qCb0R[u~
|0wfiwfinpyY|3RyYnw>{|b~fi~>j~>kWzjWj}~>RyYrw>{loRoj~bffyYn^|3jlyY~ ~&klIpYHaFfiFaB<fET> : LXVFM IB 8 EL(FI
E@L(< E@Q
E : FI[FEHaF EdklIfi<fEH<bJcQFG> : E glQFfiBGT FP-F[JcI[FG>GFEL(@L(< Ei@EBM-Fa@>@E<fETY
ij}3oRC}BZ j{lyY~
qC0fiqR AfigAtvux~>oRz|0wfiwfinpyY|3R[y->o<|0wfiw>{ljlC|3jlyY~!y3
>oRyYnpoCwfiny j}~>kryrwfinpyY>{loCy0{ j~>klIfiL(< lH<@Q 8 EL(FQQS<T FYEHfiFC0(q%C0YdbCfi
CC

fi00 0Q03
`|FC

qCb0 R ~0oRk>n|3j}~>kWw>{|b~fi~>j~>k<|0~>:npoC|3Rj}~>kOj}~:|>oRoCnpy0k0oC~>oRyYm>|3fi~>RfinpyY~>yYm>|0n@>j
oRRmfinpoyYnRyY~bnpy0{l{lj}~>knpoC|F{zvyYn{:y>>j{lonyYfiy0C~}klIpHfiFfiFaB<ET> : LXVF#LXi@L(< E@Q
E : FI[FYEHfiF ExIaL(< lHY<f@Q 8 EL(FQQ<TVFEHaFM\" 8 fiF>wfiw C00_ 0CfiqCsfibuuu[npoRC
`oR{ffyY~>$ }(
Z ^ j@>jl0 qCb0 RoCwfinpoRoC~bj}~>kg|FRjlyY~:|0~>:Rfi|0~>k0orb<{ybk0jlwfinpybkYn|0/
X FPd U IfiE@Q( : $ <Hk-Ip I[@DCDC<ETY R 0Mfiq d00M
`oRy>npk0o
} Z ^ |0~>b0 u ^ qC C0 8voC|3Rj onpoC|3y>~>j~>kW|0~><w>{|b~fi~>j~>k &
~ k-IY HaFfiFfiB<fET>
: LX F1#LX@L(< E@Q$ E : FI[FEHaF< EIaL(< lHY<f@Q 8 EL(FQQ<TVFEHfiF"\ 8 0wfiw0bY C0M
y>npkY|0~ |0mbr|0~fi~
`j}~>MoCnpk ^ qC C02
Rc ~>j oCnp|F{&w>{}|0~fi~>j}~>k tu~ |F{y0@ mfi~>j oCnp|3{$fi|3OjlfioC|fi 8 @^T @#G<fEFC
# R>YY Yb
`j}mfi~>R>jlk0{lj|M} |0npfi|fi
`r } Z ^ j@>jl0 qC0b R$voCwfinpoRoC~0j}~>k|FRjlyY~t~>fioRoCnj}~fi|3R
|0~>n|0j C|3jy>~>C IfiL(< -H<@Q 8 EL(FQQ<TVFEHfiFC 3fiY0_ YY Cfi
`j}mfi~>R>jlk0{lj|M}7
Z ^ jR>jl0 qC02 CRu~:|FRjlyY~<{}|0~>kYmfi|3kbofi|3oR:y>~<C|0m>|3{oRfiw>{}|0~fi|3jlyY~t
npoR{lj}j}~fi|0np<npoCwfiyYnC
~ k-Ip HfiFaFfiB<ET> : LX F#LXi@L(< E@Q E : FYI[FEHfiF EiIfiL(< -H<@Q 8 E
L(FQQS<T FYEHfiFMf"" 8 #fi0fiwfiw 00_ d00fibuuu[ npoRC
|3jlkY } Z oR{ly0y q/0 CRd {}|0~fi~>j}~>k MoRYoRCm>jlyY~`|0~>O{oC|bn~>j}~>krj}~W|nyYfiy0j|3k0oC~bC

~ klIpY HaFfiFaB<fET> : LX FLX 8 EL(FIfiE@L(< E@Qv E : FI[FEHaF EZIfiL(< -H<@Q 8 EL(FYQfQ<T FYEHfiFAk-QS@EE<ET
cY>`L(FYDP> 8 k #fi0Ywfiw(qC0Y q/0fi0uuufi noRC
|3{}mfiWd ( Z `o~>oCnR 000 8u jlj}>{lo>oCmfinpjljlRyYnyYw>j}r|3{w>{}|0~fi~>j}~>k &
~ k-Ip HfiFaFfiB#
<fET> : LXVF#LX 8 EL(FYIaE@L(< E@Q* E : FYIFYEHfiFW EtIaL(< lHY<f@Q 8 EL(FYQfQ<T FYEHfiFMk-QS@EE<ET&Y>`L(FYD
f 8 k fiF>wfiw$qRY_ qRYfibuuu[ npoRC
oC~>oC~> qCb0 R] g
E
Efi|3oRrmfi~>j oCnp|3{fiw>{}|0~fi~>j}~>kj}~m>{j|Fk0oC~0/0~>yY~bfioRoCnj}~>jljlfiy3
r|3j}~>C |3oCnR >oRjlCfi$oRRfi~>jlC|3c{ ~>j oCnpjlpy3( EoC~fi^|0n] EoCwfi|0npoC~by3um>y>r|3jlyY~
ff\
u 03i0M
oC~>oC~fi } oR{ly0y } Z!nY|0~bC00 0b0 R>u>n|3Rjy>~oR@fi~>j !fim>oRffyYN
n g E
Efi|3oR
w>{|b~fi~>j~>kiyYnp>RyYj~>k
|0fi|0~>C|M3i}*v|0nfioC|0m }] ZIA>p EoC~>jlC0 qCb0 R {|b~fi~>j~>kRyY~bnpy0{Ynm>{oRffyYnnpoC|3Rj ov|3k0oC~bC
IaL(< lHY<f@Q 8 EL(FQQ<TVFEHaF/ 3 0_ q0qCfi
|0m>0} Z A>oR{}r|0~ qC0b R m>>j}~>kc>o:oC~ oR{lyYwMo0t {|b~fi~>j~>kwfinpyYwMy0jljlyY~fi|3{{ybk0jl
|0~>:y>Rfi|3jlOoC|0npRW
~ klIp HfiFaFfiB<ET> : LXVF#LX}@L(< E@QA E : FI[FEHaFr EIaL(< lHY<f@Q

8 EL(FYQfQ<T FEHaFC\" 8 aF y0{ff fiwfiw$q0qC3 qC0fiqb3uuufi npoRC
|0m>00}] Z A>oR{}r|0~> q/00 R ~>j>j}~>k A>ufi|FoRr|0~>kYn|0wfibfi|3oRrw>{}|0~fi~>j}~>k
~ k-Ip HfiFaFfiB#
<fET> : LXVF#LX 8 EL(FYIaE@L(< E@Qd <fELA E : FYI[FEHfiF E3IaL(< lHY<f@Q 8 EL(FQQ<TVFEHaFC 8 c
8 0
y0{ff$q0 wfiw M%q CYd0bsfi yYnpk>|0~ |bmbr|b~fi~
y>oC>{oCn8 }
oCMoR{Yy3r|0~fi~ } ZIEj}yYwfiy>m>{ybC qCb0 R YoC~>fij~>krw>{}|0~fi~>j}~>krkYn|0wfi>
y|0~
u E ^ mfi>oR/x
~ k-IY HaFfiFfiB<fET>< : LX FLX UVI` JFfi@E E : FYIFYEHfiF: Ek-QS@EE<ET
k 0Ywfiw b0Y Cbsfi Afiwfinpj}~>k0oCn oCnp{}|3k
C"

fi
fiff fffi vfi


y>oC~>jk A[Z A>j}ryY~>/[`^ q/00sRvoC|3{j}o:oC|0npR[j}~~>yY~bfioRoCnj}~>jjl-fiyY^|3j}~>C
~klIpYHaFfiFaB<fET> : LX FGLX 8 EL(FIfiE@L(< E@Qfi<ELwE : FI[FYEHfiFE1IfiL(< lH<@Q 8 EL(FQQS<T FYEHfiF
8
8 F>wfiw$qC0bYqC0bfi yYnpkY|0~ |0mbr|0~fi~



Co n8 } 7Zvjl@fi|0nfiC qC038Ckl@I[Hw>{|b~t|w>{}|0~fi~>j}~>k:|0npR>jloRRmfinporzj:wfi|0n|3{l{loR{|FRjlyY~>
|0~>-Ry>~>n|3j}~bC ~1 vFfiHL(UVI[FML(FG>x<EIaL(< lHY<f@Q 8 EL(FYQfQ<T FEHaF/$wfiwfiqCYdb0fiA AF
Afiwfinpj}~>k0oCn oCnp{}|3k
^

j}~>>joR{loC~( qCb0RQmE
Ehuj}~fi|0nEoRRjjy>~ Ej}|3k>n|0 |3RC|3k0o0[(oR@vnpoCw
q/00/02Cfi0~>jm>oyF~bffyYnr|FjlyY~r(oR@fi~>y0{ly0kb0 $oRRfi~>jlC|3{v~>j oCnjlOy37EoC~fir|0n
VA V fi-z[Y
^

^

j}~>kY|0n$fiu}Zvjl@fi|bnpfiCfi Cq 02CR7{}|0~fi~>j}~>kWwfi|0n|3{l{loR{|3RjlyY~>CIfiL(< -H<@Q
3 0Mfiq d03

8 EL(FYQfQ<T FYEHfiFC

qC008>oqC0C^uw>{}|0~fi~>j}~>k<>oCRyYrwMoRjljlyY~MIfiL(< -H<@Q
mfifijloR 8

8 EL(FQQS<T FYEHfiF

yY~>kEr}Z y3 qCbCRdEyY^|3j}~Oj~>fioCwMoC~>fioC~0w>{}|0~fi~>oCnRyYrw>jl{}|3jlyY~~i 8 k #WI
>aXY` J Eg-QSFfiBGTVF ETV<fEFaFYIa<ET@EB"HfiUV<?>G<fL(<E : Idk-QS@EE<ET&AIfi<fB^T <ET!X FIfi@EB
klI[@HL(<HfiFC>uuuoR@fi~>jlC|3{oCwfiyYnp\AbC/0M
^

" EoCny0CEr

@GTV@#`<EF/



fi}



j {l{}|0~ ^ Cq 008#DQ<HYBFQX FfiHR_<ET> }{ mbzoCnuC|3fioCjl mfi>{

qC0bR # B
tRu[yYmfi~>$/RyYrw>{loRo00wfi|bnpj}|3{3yYnpfioCn$w>{}|0~fi~>oCn(ffyYn
klI YHaFfiFaB<fET> : LX F I[B 8 EL(FIfiE@L(<E@QE : FYIFYEHfiFE3klIfi<fEH<bJcQFG> :A Eg-QSFfiBGTVF
lF[JIF^>`FYEL(@L(<Ed@EBwlFfi@#>RE<ET>YwfiwqC0YqbqR yYnpkY|b~ |0mb^|0~fi~
oRy0C }dZ Afij7Er qC002R
#vyY~>fijljlyY~fi|3{~>yY~>{lj}~>oC|0nw>{|b~fi~>j~>kr~k-I YHaFfiFfiB<fET
> : LX Fx >GL
8 EL(FYIaE@L(< E@Q E : FI[FEHaF EdIaL(< lHY<f@Q 8 EL(FQQ<TVFEHfiFk-Q@EE<fETZY>`L(FYD$>xf 8 k #fi0 wfiw
q%C0Y q/0fi yYnkY|0~ |0mb^|0~fi~
|0~/|0~$ uRjl0$u}vn|/>yY~ }7{loRjloCnR}(Z+j>{loR0N# qCb0sR
ff RjloC~0gE
E
|3{lk0yYnpjlfi^ffyYnOiBA fi~0>oRjl|0~> oCnpj C|3jy>~ ~ 8 C7klpI HfiFfiFaB<ET
> : LXVF
8 EL(FYIaE@L(< E@Q-W >fiYX fi-J Ed$ TV<fHtc#ELXVFG>G<?R>
A>R>yYwfiwfioCnp/ q/C0R
~>j oCnp|3{w>{}|0~>ffyYnnpoC|FRj onpyYMy0j}~Wmfi~fiwfinpoRfijlR|0>{looC~ j}npy>~fioC~0C
~klpI YHaFfiFaB<fET
> : LX F#LX 8 EL(FIfiE@L(< E@Qfi <ELw E : FI[FYEHfiF E1IfiL(< lH<@Q 8 EL(FQQS<T FYEHfiF
8
8 F >wfiw$qC0bY qC3>fi yYnpkY|0~ |0mbr|0~fi~
Afij}n~>y
} yYoC~>jlk 7A} oR{ly0y }UZ A>j}ryY~>C$ qC0bRvff RjloC~bk0yY|3{fij}npoRRoR<oRfiw>{yF
n|3jlyY~ ~&k-I YHaFfiFfiB<fET>AWLXVFiX <IaL(FfiFELXM@L(< E@Q E : FYIFYEHfi
F ExIfiL(< -H<@Q 8 EL(FQQS<T FYEHfiF
f"\ 8 0 Ywfiw b0Yd 0bfi0uuufinpoRC
oC~>fioCnpb0 %A}%Z6:oR{l$]Er"A
u E ^ ~ p


A>yY~>o0U0Z

oR{ly0y qCbCR$y3z|0npfivRy0{l{}|0fiy>n|3j o|0~>|3 oCnp|0npj}|3{${loC|0n~>j}~>k tuC|3o
m>firj}~WnpyYMy0jlyYRRoCn8 8 EL(FIfiE@L(<E@QfiU IfiE@Q : UVDC@EfiD*JcUVL(FI"cL(UVB<FG>M 8 0

Afim>y>~dA}Z|bnpy bu`r

qC0C8v-F<E : I[HaFYDFYELQFa@IfiE<ET-@Ed<fEL(IYBUVHL(<E
C%?



InpoRC

fi00 0Q03
oR{ly0y }#|0nMyY~>oR{l{ }CoC npoR0buYyYnn|Fy]Er}0ij}~fid}Z{l>>o0 qC00s2R ~boRkYn|3j}~>k
w>{|b~fi~>j~>k[|0~>[{loC|0n~>j}~>k t`>ofi E`t |bnp@>jloRRmfinpo01fiUVIaE@Q :3 ]JFIfi<DFYEL(@Q-@EB
X F I[FL(<Ha@QIfiL(< lH<@Q 8 EL(FQQS<T FYEHfiFCv q%RUCfiqfiqCbfi
oR{ly0y q/03
R klQ@EE<ETZ@EB3QSFfi@IfiE<ETxGM@E@Ql TV<fHfi@QI[Fa@#>R E<fETYN Afiwfinj~>kboCn oCnp{}|3k
oR{ly0y }
yb{{}|3R }

Z #y* qC0 C8 |3jlyY~fi|F{ofi|FoRyY~>jlyYnpj}~>k[ffyYn
w>{|b~fi~>j~>kj}~fid~fi|bjloC~ jnyY~fioC~bC
~ k-Ip HfiFaFfiB<ET> : LXVFALX 8 EL(FIfiE@L(< E@Q E : FI[FEHaF
ExIaL(< lHY<f@Q 8 EL(FQQ<TVFEHaF"k-QS@EE<ETCY>`L(FYD$>M 8 k #fi0fiwfiw$qCfifiq qC0fibuuug npoRC
:oR{l$ Er qC002 RoRRoC~0|F |0~>RoRj}~Ouw>{}|0~fi~>j}~>k IfiL(< -H<@Q 8 EL(FQQ<TVFEHaF*@GTV@#G<fEFCYbY qC0M
!jl{bj}~>C Er3 0oCnp/ ^ ^ y3zn|0~>Ro0 Er}n Z:oR{oR0 ^ ] qCb3 R {|b~fi~>j~>k|b~>noC|3Rj}~>k
j~mfi~>RoCnp|3j}~!|0~>fid~fi|0j`oC~ j}npyY~fioC~bC
fi UVIaE@Q :d ]JFIfi<DFYEL(@Q*@EBX F I[FL(<Ha@Q
IaL(< lHY<f@Q 8 EL(FQQ<TVFEHaF/ 3$qC0_ d00fi


C%

fiJournal Artificial Intelligence Research 13 (2000) 305-338

Submitted 6/00; published 12/00

Conformant Planning via Symbolic Model Checking
cimatti@irst.itc.it

Alessandro Cimatti

ITC-irst

, Via Sommarive 18, 38055 Povo, Trento, Italy

roveri@irst.itc.it

Marco Roveri

ITC-irst

, Via Sommarive 18, 38055 Povo, Trento, Italy

DSI, University Milano, Via Comelico 39, 20135 Milano, Italy

Abstract

tackle problem planning nondeterministic domains, presenting new
approach conformant planning. Conformant planning problem finding sequence actions guaranteed achieve goal despite nondeterminism
domain. approach based representation planning domain finite
state automaton. use Symbolic Model Checking techniques, particular Binary Decision Diagrams, compactly represent eciently search automaton. paper
make following contributions. First, present general planning algorithm
conformant planning, applies fully nondeterministic domains, uncertainty
initial condition action effects. algorithm based breadth-first, backward search, returns conformant plans minimal length, solution planning
problem exists, otherwise terminates concluding problem admits conformant
solution. Second, provide symbolic representation search space based Binary
Decision Diagrams (Bdds), basis search techniques derived symbolic
model checking. symbolic representation makes possible analyze potentially large
sets states transitions single computation step, thus providing ecient
implementation. Third, present Cmbp (Conformant Model Based Planner), ecient
implementation data structures algorithm described above, directly based
Bdd manipulations, allows compact representation search layers
ecient implementation search steps. Finally, present experimental comparison approach state-of-the-art conformant planners Cgp, Qbfplan
Gpt. analysis includes planning problems distribution packages
systems, plus problems defined stress number specific factors. approach appears effective: Cmbp strictly expressive Qbfplan
Cgp and, problems comparison possible, Cmbp outperforms
competitors, sometimes orders magnitude.
1. Introduction

recent years, growing interest planning nondeterministic domains.
Rejecting fundamental (and often unrealistic) assumptions classical planning, domains considered actions uncertain effects, exogenous events possible,
initial state partly specified. challenge find strong plan,
guaranteed achieve goal despite nondeterminism domain, regardless
uncertainty initial condition effect actions. Conditional planning (Cassandra, Kaelbling, & Littman, 1994; Weld, Anderson, & Smith, 1998; Cimatti,
Roveri, & Traverso, 1998b) tackles problem searching conditional course
c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiCimatti & Roveri
actions, depends information gathered run-time. certain domains,
however, run-time information gathering may expensive simply impossible. Conformant planning (Goldman & Boddy, 1996) problem finding unconditioned
course actions, i.e. classical plan, depend run-time information gathering guarantee achievement goal. Conformant planning recognized
significant problem Artificial Intelligence since work Michie (1974): Blind
Robot problem requires program activity sensorless agent, positioned location given room, guaranteed achieve given
goal. Conformant planning also seen problem control system
unobservable unknown state, microprocessor power-up, software
system black-box testing.
uncertainty, plan associated potentially many different executions,
must taken account order guarantee goal achievement. makes
conformant planning significantly harder classical planning (Rintanen, 1999a; De Giacomo & Vardi, 1999). Despite increased complexity, several approaches conformant
planning recently proposed, based (extensions of) main planning techniques classical planning. interesting Cgp (Smith & Weld, 1998) based
Graphplan, Qbfplan (Rintanen, 1999a) extends SAT-plan approach
QBF, Gpt (Bonet & Geffner, 2000) encodes conformant planning heuristic
search. paper, propose new approach conformant planning, based Symbolic Model Checking (McMillan, 1993). Symbolic Model Checking formal verification
technique, allows one analyze finite state automata high complexity, relying
symbolic techniques, Binary Decision Diagrams (Bdds) (Bryant, 1986) particular,
compact representation ecient search automaton. approach builds
planning via model checking paradigm presented Cimatti colleagues (1997,
1998b, 1998a), finite state automata used represent complex, nondeterministic
planning domains, planning based (extensions of) basic model checking steps.
make following contributions.

First, present general algorithm conformant planning, applies

nondeterministic domain uncertain action effects initial condition, expressed
nondeterministic finite-state automaton. algorithm performs breadth-first
search, exploring plans increasing length, plan found candidate
plans available. algorithm complete, i.e. returns failure
problem admits conformant solution. problem admits solution,
algorithm returns conformant plan minimal length.

Second, provide symbolic representation search space based Binary

Decision Diagrams, allows application search techniques derived
symbolic model checking. symbolic representation makes possible analyze
sets transitions single computation step. sets compactly represented eciently manipulated despite potentially large cardinality.
way possible overcome enumerative nature approaches
conformant planning, degree nondeterminism tends limiting
factor.
306

fiConformant Planning via Symbolic Model Checking

Third, developed Cmbp (Conformant Model Based Planner), ecient

implementation data structures algorithm described above. Cmbp developed top Mbp, planner based symbolic model checking techniques
developed Cimatti, Roveri Traveso (1998b, 1998a). Cmbp implements several
new techniques, directly based Bdd manipulations, compact search layers
optimize termination checking.
Finally, provide experimental evaluation state-of-the-art conformant planners, comparing Cmbp Cgp, Qbfplan Gpt. difference
expressivity, problems tackled Cmbp also represented planners. However, problems direct comparison
possible, Cmbp outperforms competitors. particular, features better
qualitative behavior, directly related number initial states uncertain
action effects, stable respect use heuristics.
paper structured follows. Section 2 review representation (nondeterministic) planning domains finite state automata. Section 3 provide
intuitions formal definition conformant planning setting. Section 4
present planning algorithm, Section 5 discuss symbolic representation
search space, allows ecient implementation. Section 6 present
Cmbp planner, Section 7 present experimental results. Section 8
discuss related work. Section 9 draw conclusions discuss future
research directions.
2. Planning Domains Finite State Automata

interested complex, nondeterministic planning domains, actions
preconditions, conditional effects, uncertain effects, initial state
partly specified. rest paper, use simple though paradigmatic domain
explanatory purposes, variation Moore's bomb toilet domain (McDermott,
1987) (from called BTUC | BT Uncertain Clogging). two packages,
one contains armed bomb. possible dunk either package
toilet (actions Dunk1 Dunk2 ), provided toilet clogged. Dunking either
package uncertain effect clogging toilet. Furthermore, dunking package
containing bomb effect disarming bomb. action F lush effect
unclogging toilet.
represent domains finite state automata. Figure 1 depicts automaton
BTUC domain. state given number, contains propositions holding
state. instance, state 1 represents state bomb package 1,
defused, toilet clogged. Given one bomb, write In2
abbreviation negation In1 . Arrows states depict transitions
automaton, representing possible behavior actions. transition state 2
state 1 labeled F lush represents fact action F lush, executed state
2, effect removing clogging. execution Dunk1 state 1,
uncertain effect clogging toilet, represented multiple transitions
states 5 6. Since transition outgoing state 2 labelled Dunk1 ,
307

fiCimatti & Roveri
Flush
Flush

Dunk_1
In_1 5
Defused
!Clogged

In_1 1
!Defused
!Clogged

Dunk_1,
Dunk_2

Dunk_2
Flush

Flush
In_1 2
!Defused
Clogged

In_1 6
Defused
Clogged

Flush
Flush

Dunk_2
In_2 7
Defused
!Clogged

3

In_2
!Defused
!Clogged

Dunk_1,
Dunk_2

Dunk_1
Flush

Flush
In_2 8
Defused
Clogged

In_2 4
!Defused
Clogged

Figure 1: automaton BTUC domain
state 2 satisfy preconditions action Dunk1 , i.e. Dunk1 applicable
state 2.
formally define nondeterministic planning domains follows.
Definition 1 (Planning Domain) Planning Domain 4-tuple = (P ; ; A; R),

P (finite) set atomic propositions, 2P set states,
(finite) set actions, R transition relation.



Intuitively, proposition state holds state. following
assume planning domain given. use s, s0 s00 denote states D,
ff denote actions. R(s; ff; s0 ) holds iff executing action ff state
state s0 possible outcome. say action ff applicable iff least
one state s0 R(s; ff; s0 ) holds. say action ff deterministic iff
unique state s0 R(s; ff; s0 ) holds. action ff uncertain outcome
least two distinct states s0 s00 R(s; ff; s0 ) R(s; ff; s00 )
hold. described Cimatti colleagues (1997), automaton given domain
eciently built starting compact description given expressive high level
action language, instance AR (Giunchiglia, Kartha, & Lifschitz, 1997).
3. Conformant Planning

Conformant planning (Goldman & Boddy, 1996) described problem finding
sequence actions guaranteed achieve goal regardless nondeterminism
domain. is, possible initial states, uncertain action effects,
execution plan results goal state.
Consider following problem BTUC domain. Initially, bomb armed
position status toilet uncertain, i.e. initial state
states f1; 2; 3; 4g . goal reach state bomb defused, toilet
308

fiConformant Planning via Symbolic Model Checking

In_1 1
!Defused
!Clogged

In_1 2
!Defused
Clogged

In_2 3
!Defused
!Clogged

In_2 4
!Defused
Clogged

In_1 5
Defused
!Clogged

Flush

Flush

Flush

Flush

In_1 5
Defused
!Clogged

Flush

Flush
In_1 1
!Defused
!Clogged

Dunk_1

In_1 6
Defused
Clogged

In_1 5
Defused
!Clogged

Dunk_2

In_1 6
Defused
Clogged

In_2 3
!Defused
!Clogged

Dunk_2

In_2 7
Defused
!Clogged

Flush
In_2 3
!Defused
!Clogged

Dunk_1

In_2 3
!Defused
!Clogged

In_2 4
!Defused
Clogged

Flush

In_2 8
Defused
Clogged

Flush

Flush

Flush

In_1 5
Defused
!Clogged

In_2 7
Defused
!Clogged

Flush

Figure 2: conformant solution BTUC problem
clogged, i.e. set goal states f5; 7g. conformant plan solving problem

F lush; Dunk1 ; F lush ; Dunk2 ; F lush
(1)
Figure 2 outlines possible executions plan, possible initial states
uncertain action effects. initial uncertainty lies fact domain might
states f1; 2; 3; 4g . possible initial states planning domain
collected set dashed line. call set belief state. Intuitively, belief
state expresses condition uncertainty domain, collecting together
states indistinguishable point view agent reasoning
domain. first action, F lush, used remove possible clogging. reduces
uncertainty belief state f1; 3g. Despite remaining uncertainty (i.e. still
known package bomb is), action Dunk1 guaranteed applicable
precondition met states. Dunk1 effect defusing bomb
contained package 1, uncertain effect clogging toilet. resulting
belief state f3; 4; 5; 6g . following action, F lush, removes clogging, reducing
uncertainty belief state f3; 5g, guarantees applicability Dunk2 .
Dunk2 , bomb guaranteed defused, toilet might clogged (states
6 8 belief state f5; 6; 7; 8g ). final F lush reduces uncertainty belief
state f5; 7g, guarantees achievement goal.
general, order plan conformant solution, action must executed
states satisfy preconditions, state result
execution plan (for initial states uncertain action effects)
goal state. main diculty achieving conditions information
(assumed be) available run-time. Therefore, planning time face problem
reasoning action execution belief state, i.e. condition uncertainty.
Definition 2 (Action Applicability) Let Bs Belief State. action ff
applicable Bs iff Bs 6= ; ff applicable every state 2 Bs.
309

fiCimatti & Roveri
order action applicable belief state, require preconditions
must guaranteed notwithstanding uncertainty. words, reject \reckless"
plans, take chance applying action without guarantee applicability.
choice strongly motivated practical domains, possibly fatal consequences
follow attempt apply action preconditions might satisfied
(e.g. starting fix electrical device without sure powered). effect
action execution uncertain condition defined follows.
Definition 3 (Action Image) Let Bs belief state, let ff action applicable Bs. image (also called execution) ff Bs, written Image [ff](Bs), defined

follows.

Image [ff](Bs) =_

fs0 j exists 2 Bs R(s; ff; s0 )g

Notice image action combines uncertainty belief state uncertainty action effects. (Consider instance Image [Dunk1 ](f1; 3g)=f3; 4; 5; 6g .)
following, write Image [ff](s) instead Image [ff](fsg).
Plans elements , i.e. finite sequences actions. use 0-length
plan, denote generic plans, ; plan concatenation. notions
applicability image generalize plans follows.
Definition 4 (Plan Applicability Image) Let 2 , let Bs . applicable Bs iff one following holds:
1. = Bs 6= ;;
2. = ff; , ff applicable Bs, applicable Image [ff](Bs).
image (also called execution) Bs, written Image [](Bs), defined as:
1. Image [](Bs) =_ Bs;
2. Image [ff; ](Bs) =_ Image [](Image [ff](Bs));

planning problem formally characterized set initial goal states.
following definition captures intuitive meaning conformant plan given above.
Definition 5 (Conformant Planning) Let = (P ; ; A; R) planning domain.
Planning Problem triple (D; ; G ), ; 6= ; 6= G .
plan conformant plan (that is, conformant solution to) planning
problem (D; ; G ) iff following conditions hold:

(i) applicable ;
(ii) Image [](I ) G .
following, clear context, omit domain planning
problem, simply write (I ; G ).
310

fiConformant Planning via Symbolic Model Checking
4. Conformant Planning Algorithm

conformant planning algorithm based exploration space plans, limiting
exploration plans conformant construction. algorithm builds Belief
state-Plan (BsP) pairs form hBs : i, Bs non-empty belief state
plan. idea use BsP pair associate explored plan maximal
belief state applicable, guaranteed result goal states.
exploration based basic function SPreImage [ff](Bs), that, given belief state
Bs action ff, returns belief state containing states ff applicable,
whose image ff contained Bs.
Definition 6 (Strong Pre-Image) Let ; 6= Bs belief state let ff
action. strong pre-image Bs ff, written SPreImage [ff](Bs), defined
follows.

SPreImage [ff](Bs) =_ fs j ff applicable s; Image [ff](s)

Bsg

SPreImage [ff](Bs) empty, ff applicable it, conformant solution problem (SPreImage [ff](Bs); Bs). Therefore, plan conformant
solution problem (Bs; G ), plan ff; conformant solution problem
(SPreImage [ff](Bs); G ).
Figure 3 depicts space BsP pairs built algorithm solving BTUC
problem. levels built goal, right, towards initial states,
left. level 0, BsP pair hf5; 7g : i, composed set goal states indexed
0-length plan . (Notice conformant solution every problem goal set
f5; 7g initial states contained f5; 7g.) dashed arrows represent application
SPreImage . level 1, BsP pair hf5; 6; 7; 8g : F lushi built, since strong
pre-image belief state 0 actions Dunk1 Dunk2 empty. level 2,
three BsP pairs, (overlapping) belief states Bs2 , Bs3 Bs4 , indexed, respectively,
length 2 plans Dunk1 ; F lush, F lush; F lush Dunk2 ; F lush. (A plan associated
belief state Bsi sequence actions labeling path Bsi Bs0 .) Notice
Bs3 equal Bs1 , therefore deserves expansion. expansion
belief states 2 4 gives belief states 5 6, obtained strong pre-image
F lush, strong pre-image actions Dunk1 Dunk2 returns empty
belief states. expansion Bs5 results three belief states. one resulting
strong pre-image F lush reported, since equal Bs5 . Belief state
7 also equal Bs2 , deserves expansion. Belief state 8 obtained
expanding Bs5 Bs6 . level 5, expansion produces Bs10 , contains
initial states. Therefore, corresponding plans conformant solutions
problem.
conformant planning algorithm ConformantPlan presented Figure 4.
takes input planning problem form set states G (the domain
assumed globally available). algorithm performs backwards breadth-first
search, exploring BsP pairs corresponding plans increasing length step.
status search (each level Figure 3) represented BsP table, i.e. set BsP
pairs
BsPT = fhBs1 : 1 i; : : : ; hBsn : n ig
311

fiCimatti & Roveri
Level

5

4

3

2

1

0

In_1 1
!Defused
!Clogged

In_1 1
!Defused
!Clogged
Dunk_1

In_1 1
!Defused
!Clogged

In_1 5
Defused
!Clogged

In_1 2
!Defused
Clogged

In_2 7
Defused
!Clogged

In_1 5
Defused
!Clogged

In_1 1
!Defused
!Clogged
Flush

In_1 5
Defused
!Clogged

Dunk_1

Bs 7

In_1 6
Defused
Clogged

In_1 2
!Defused
Clogged

In_2 7
Defused
!Clogged
Bs 2

In_2 3
!Defused
!Clogged

In_1 1
!Defused
!Clogged

In_2 4
!Defused
Clogged

In_2 3
!Defused
!Clogged

Flush

In_1 5
Defused
!Clogged

In_1 5
Defused
!Clogged

In_1 6
Defused
Clogged

In_2 7
Defused
!Clogged

In_2 7
Defused
!Clogged

In_1 5
Defused
!Clogged

Dunk_2
In_2 8
Defused
Clogged

In_1 6
Defused
Clogged

Bs 5

In_2 3
!Defused
!Clogged

Flush

In_1 6
Defused
Clogged

In_2 7
Defused
!Clogged

In_2 7
Defused
!Clogged

In_2 8
Defused
Clogged

In_2 8
Defused
Clogged

Flush

In_1 5
Defused
!Clogged

In_2 7
Defused
!Clogged
Bs 0

Dunk_1

Bs 8

In_1 5
Defused
!Clogged

In_2 4
!Defused
Clogged

Bs 3

Bs 1

7

In_2
Defused
!Clogged

In_1 5
Defused
!Clogged

In_2 3
!Defused
!Clogged

8

In_2
Defused
Clogged

Bs 10

In_2 3
!Defused
!Clogged

In_1 5
Defused
!Clogged

Flush

In_1 6
Defused
Clogged
Dunk_2

In_1 5
Defused
!Clogged

Dunk_2

In_2 7
Defused
!Clogged

In_2 7
Defused
!Clogged

Bs 4

In_2 7
Defused
!Clogged

In_2 8
Defused
Clogged
Bs 9

Bs 6

Figure 3: BsP tables BTUC problem
plans length, 6= j 1 j 6=i n.
call Bsi belief set indexed . ambiguity arises, write BsPT(i )
Bsi. array BsPTables used store BsP tables representing levels
search. algorithm first checks (line 4) plans length 0, i.e.
solution. conformant plan length exists ((P lans = ;) line 4),
loop entered. iteration, conformant plans increasing length explored
(lines 5 8). step line 6 expands BsP table BsPTables[i 1] stores
resulting BsP table BsPTables[i]. BsP pairs redundant respect
current search eliminated BsPTables[i] (line 7). possible solutions contained
BsPTables[i] extracted stored P lans (line 8). loop terminates either
plan found (P lans 6= ;), space conformant plans completely explored
(BsPTables[i] = ;).
definitions basic functions used algorithm reported Figure 5.
function ExpandBsPTable expands BsP table provided argument, containing
conformant plans length 1, returns BsP table conformant plans length
i. BsP input BsP table expanded ExpandBsPPair. possible
312

fiConformant Planning via Symbolic Model Checking

0
1
2
3
4
5
6
7
8
9
10
11
12
13

function ConformantPlan(I ,G )
begin

= 0;
BsPTables[0] := f hG : g;
Plans := ExtractSolution(I ; BsPTables[0]);
((BsPTables[i] 6= ;) ^ (P lans = ;))
:= + 1;
BsPTables[i] := ExpandBsPTable(BsPTables[i-1]);
BsPTables[i] := PruneBsPTable(BsPTables[i]; BsPTables; i);
Plans := ExtractSolution(I ; BsPTables[i]);

done
(BsPTables[i] = ;)
return Fail;
else return Plans;
end

Figure 4: conformant planning algorithm.
action ff, strong pre-image Bs computed, resulting belief state Bs0
empty, i.e. belief state ff guarantees achievement Bs,
plan extended ff hBs0 : ff; returned. expansion BsP table
union expansions BsP pair. function ExtractSolution takes
input BsP table returns (possibly empty) set plans index belief states
containing . PruneBsPTable takes input BsP table pruned, array
previously constructed BsP tables BsPTables, index current step. removes
BsP table input plans worth explored
corresponding belief states already visited.
algorithm following properties. First, always terminates. follows
fact set explored belief sets (stored BsPTables) monotonically
increasing | step proceed least one new belief state generated.
finiteness (the set accumulated belief states contained 2S
finite), fix point eventually reached. Second, correct, i.e. plan returned
conformant solution given problem. correctness algorithm follows
properties SPreImage : plan associated belief state
conformant, i.e. guaranteed applicable results
belief state contained goal. Third, algorithm optimal, i.e. returns plans
minimal length. property follows breadth-first style search. Finally,
algorithm able decide whether problem admits solution, returning Fail
cases. Indeed, conformant solution always associated belief state containing
initial states. SPreImage generates maximal belief state associated conformant
plan, new belief state generated exploration compared initial states
check solution, plan pruned equivalent plan already
explored.
313

fiCimatti & Roveri

(BsPT) =_

ExpandBsPTable

[
hBs : i2BsPT

(hBs : i)

ExpandBsPPair

(hBs : i) =_ fhBs0 : ff; ij Bs0 = SPreImage [ff](Bs) 6= ;g

ExpandBsPPair

(BsPT; BsPTables; i) =_
fhBs : 2 BsPT j j < i; hBs : 0 2 BsPTables[j ] (Bs0 = Bs)g
PruneBsPTable

(I ; BsPT) =_ f j exists hBs : 2 BsPT Bsg

ExtractSolution

Figure 5: primitives used conformant planning algorithm.
5. Conformant Planning via Symbolic Model Checking

Model checking formal verification technique based exploration finite state
automata (Clarke, Emerson, & Sistla, 1986). Symbolic model checking (McMillan, 1993)
particular form model checking using Binary Decision Diagrams compactly represent
eciently analyze finite state automata. introduction symbolic techniques
model checking led breakthrough size model could analyzed (Burch
et al., 1992), made possible model checking routinely applied industry,
especially logic circuits design (for survey see Clarke & Wing, 1996).
rest section, provide overview Binary Decision Diagrams,
describe representation planning domains, based Bdd-based
representation finite state automata used model checking. Then, discuss
extension allows symbolically represent BsP tables transformations, thus
allowing ecient implementation algorithm described previous section.
5.1 Binary Decision Diagrams

Reduced Ordered Binary Decision Diagram (Bryant, 1992, 1986) (improperly called Bdd)
directed acyclic graph (DAG). terminal nodes either rue F alse. nonterminal node associated boolean variable, two Bdds, called left right
branches. Figure 6 (a) depicts Bdd (a1 $ b1 ) ^ (a2 $ b2 ) ^ (a3 $ b3 ).
non-terminal node, right [left, respectively] branch depicted solid [dashed, resp.]
line, represents assignment value rue [F alse, resp.] corresponding
variable. Bdd represents boolean function. given truth assignment variables
Bdd, value function determined traversing graph root
leaves, following branch indicated value assigned variables1.
1. path root leaf visit nodes associated subset variables Bdd.
See instance path associated a1 ; :b1 Figure 6(a).

314

fiConformant Planning via Symbolic Model Checking

a1

a1

b1

a2

b1

a3

a2

b2

b2

b1

b1

a3

b1

b2

a3

a2

a3

b1

b1

b2

a3

b1

b2

b1

b2

b3

b3

b3

b3

True

False

True

False

(a)

b1

(b)

Figure 6: Two Bdds formula (a1 $ b1 ) ^ (a2 $ b2 ) ^ (a3 $ b3 ).
reached leaf node labeled resulting truth value. v Bdd, size jvj
number nodes. n node, var(n) indicates variable indexing node n.
Bdds canonical representation Boolean functions. canonicity follows
imposing total order < set variables used label nodes,
node n respective non-terminal child m, variables must ordered, i.e. var(n) <
var(m), requiring Bdd contains isomorphic subgraphs.
Bdds combined usual boolean transformations (e.g. negation, conjunction, disjunction). Given two Bdds, instance, conjunction operator builds
returns Bdd corresponding conjunction arguments. Substitution also
represented Bdd transformations. following, v variable,
Bdds, indicate [v= ] Bdd resulting substitution v .
v1 v2 vectors (the number of) distinct variables, indicate [v1 =v2 ]
parallel substitution variables vector v1 (corresponding) variables
v2 .
Bdds also allow transformations described quantifications, style Quantified Boolean Formulae (QBF). QBF definitional extension propositional logic,
propositional variables universally existentially quantified. terms Bdd
computations, quantification corresponds tranformation mapping Bdd
variable vi quantified Bdd resulting (propositional) formula.
formula, vi one variables, existential quantification vi , written
9vi:(v1 ; : : : ; vn ), equivalent (v1; : : : ; vn )[vi=F alse] _ (v1; : : : ; vn )[vi=T rue]. Analogously, universal quantification 8vi :(v1 ; : : : ; vn ) equivalent (v1 ; : : : ; vn )[vi =F alse]^
315

fiCimatti & Roveri
(v1 ; : : : ; vn )[vi =T rue]. QBF, quantifiers arbitrarily applied nested. general, QBF formula equivalent propositional formula, conversion subject
exponential blow-up.
time complexity algorithm computing truth-functional boolean transformation f1 <op> f2 O(jf1 j jf2 j). far quantifications concerned, time
complexity quadratic size Bdd quantified, linear number
variables quantified, i.e. O(jvj jf j2 ) (Bryant, 1992, 1986).
Bdd packages ecient implementations data structures algorithms (Brace
et al., 1990; Somenzi, 1997; Yang et al., 1998; Coudert et al., 1993). Basically, Bdd package deals single multi-rooted DAG, node represents boolean function.
Memory eciency obtained using \unique table", sharing common subgraphs
Bdds. unique table used guarantee time isomorphic subgraphs redundant nodes multi-rooted DAG. creating
new node, unique table checked see node already present,
case new node created stored unique table. unique table
allows perform equivalence check two Bdds constant time (since two
equivalent functions always share subgraph) (Brace et al., 1990; Somenzi, 1997).
Time eciency obtained maintaining \computed table", keeps track
results recently computed transformations, thus avoiding recomputation.
critical computational factor Bdds order variables used. (Figure 6
shows example impact change variable ordering size Bdd.)
certain class boolean functions, size corresponding Bdd exponential
number variables possible variable ordering (Bryant, 1991). many practical
cases, however, finding good variable ordering rather easy. Beside affecting memory
used represent Boolean function, finding good variable ordering big impact
computation times, since complexity transformation algorithms depends
size operands. Bdd packages provide heuristic algorithms finding good
variable orderings, called try reduce overall size stored Bdds.
reordering algorithms also activated dynamically package, Bdd
computation, total number nodes package reaches predefined threshold
(dynamic reoredering).
5.2 Symbolic Representation Planning Domains
planning domain (P ; ; A; R) represented symbolically using Bdds, follows.

set (distinct) Bdd variables, called state variables, devoted representation
states domain. variables direct association proposition
domain P used description domain. instance, BTUC
domain, In1 , Defused Clogged associated unique Bdd variable.
following write x vector state variables. particular order
irrelevant performance issues, rest section distinguish
proposition corresponding Bdd variable.
state set propositions P (specifically, propositions intended
hold it). state s, corresponding assignment state variables
x , i.e. assignment variable corresponding proposition p 2 assigned
316

fiConformant Planning via Symbolic Model Checking
rue, variable assigned F alse. represent Bdd (s),
assignment unique satisfying assignment. instance, (6) =_ (In1 ^
Defused ^ Clogged) Bdd representing state 6, (4) =_ :In1 ^ :Defused ^
Clogged represents state 4, on. (Without loss generality, following
distinguish propositional formula corresponding Bdd.) representation
naturally extends set states Q follows:
(Q) =_

_ (s)

s2Q

words, associate set states generalized disjunction Bdds
representing states. Notice satisfying assignments (Q)
exactly assignment representations states Q. representation mechanism
natural. instance, Bdd (I ) representing set initial states
BTUC =_ f1; 2; 3; 4g :Defused, set goal states G =_ f5; 7g
corresponding Bdd Defused ^ :Clogged. Bdd also used represent set
states domain automaton. BTUC, (S ) = rue = 2P .
different formulation, two independent propositions In1 In2 used represent
position bomb, (S ) would Bdd In1 $ :In2 .
general, Bdd represents set (states correspond to) models.
consequence, set theoretic transformations naturally represented propositional
operations, follows.
(SnQ)
=_ (S ) ^ : (Q)
(Q1 [ Q2 ) =_ (Q1 ) _ (Q2 )
(Q1 \ Q2 ) =_ (Q1 ) ^ (Q2 )

main eciency symbolic representation lies fact cardinality
represented set directly related size Bdd. instance, (G ) uses
two (non-terminal) nodes represent two states, (I ) uses one node represent four
states. limit cases, (S ) (fg) (the leaf Bdds) rue F alse, respectively.
advantage, symbolic representation extremely ecient dealing irrelevant
information. Notice, instance, variable Defused occurs (f5; 6; 7; 8g ).
reason, symbolic representation dramatic improvement explicit,
enumerative representation. allows symbolic, Bdd-based model checkers
handle finite state automata large number states (see instance Burch
et al., 1992). following, collapse set states Bdd representing it.
Another set Bdd variables, called action variables, written ff , used represent
actions. use one action variable possible action A. Intuitively, Bdd action
variable true corresponding action executed. assume
sequential encoding used, i.e. concurrent actions allowed, also use Bdd,
Seq(ff ), express exactly one action variables must true time2 .
2. specific case sequential encoding, alternative approach using dlog jAje possible:
assignment action variables denotes specific action executed. Two assignments
mutually exclusive, constraint Seq(ff ) needs represented. cardinality
set actions power two, standard solution associate one assignment
certain values. optimized solution, actually used implementation, described
sake simplicity.

317

fiCimatti & Roveri
BTUC problem, contains three actions, use three Bdd variables Dunk1 ,
Dunk2 F lush, express serial encoding constraint following Bdd:

Seq(ff) =_ (Dunk1 _ Dunk2 _ F lush) ^:(Dunk1 ^ Dunk2 ) ^:(Dunk1 ^ F lush) ^:(Dunk2 ^ F lush)

state variables, referring Bdd action variables symbolic names
sake simplicity. practice, internally represented integers,
position ordering Bdd package totally irrelevant logical terms.
Bdd variables x ff represents set state-action pairs, i.e. relation
states actions. instance, applicability relation BTUC (i.e.,
actions possible states, except dunking actions require toilet
clogged) represented Bdd :(Clogged ^ (Dunk1 _ Dunk2 )). Notice
represents set 16 state-action pairs, associating state applicable action.
transition 3-tuple composed state (the initial state transition),
action (the action executed), state (the resulting state transition).
represent transitions, another vector x 0 Bdd variables, called next state variables,
allocated Bdd package. write 0 (s) representation state
next state variables. 0 (Q) denote construction Bdd corresponding
set states Q, using variable next state vector x 0 instead current
state variables x . require jx j = jx 0 j, assume i-th variable x
i-th variable x 0 correspond. define representation set states next
variables follows.
0 (s) =_ (s)[x =xx0 ]
call operation [x =xx0 ] \forward shifting", transforms representation
set \current" states representation set \next" states. dual operation
[x 0 =xx] called backward shifting. following, call x current state variables
distinguish next state variables. transition represented assignment
x , ff x 0 . BTUC, transition corresponding application action
Dunk1 state 1 resulting state 5 represented following Bdd
(h1; Dunk1 ; 5i) =_ (1) ^ Dunk1 ^ 0 (5)
transition relation R automaton corresponding planning domain
simply set transitions, thus represented Bdd Bdd variables x , ff
x0 , satisfying assignment represents possible transition.
_
(R) =_ Seq(ff ) ^ (t)
t2R

rest paper, assume Bdd representation planning domain
given. particular, assume given vectors variables x ;xx0 ;ffff, encoding
functions 0 , simply call , R, G Bdd representing states
domain, transition relation, initial states goal states, respectively. write
(v) stress Bdd depends variables v. representation,
possible reason plans, simulating symbolically execution sets actions
sets states, means QBF transformations. Bdd representing applicability
relation directly obtained following computation.
ff) =_ 9x 0 :R(x ;ffff;xx0 )
Applicable(x ;ff
318

fiConformant Planning via Symbolic Model Checking
resulting Bdd, Applicable(x ;ffff), represents set state-action pairs
action applicable state. Bdd representing states reachable Q
one step obtained following computation.

9x:9ff:(R(x ;ffff;xx0 )^Q(x))[x 0=xx]
Notice that, single operation, symbolically simulate effect application
applicable action states Q. Similarly, following transformation allows symbolically compute SPreImage set states Q possible
actions one single computation:

8x0:(R(x ;ffff;xx0 ) ! Q(x )[x=xx0 ]) ^

(x ;ffff)

Applicable

resulting Bdd represents state-action pairs hx : ffi ff applicable
x execution ff x results states Q.
5.3 Symbolic Search Space Belief States

main strength symbolic approach allows perform symbolic breadthfirst search, provides way compactly representing eciently expanding
frontier. instance, plans constructed symbolic breadth-first search
space states, repeatedly applying strong pre-image goal states (Cimatti et al.,
1998b). However, machinery presented previous section cannot directly applied
tackle conformant planning. basic difference conformant planning
searching space belief states3 , therefore frontier search basically
set sets states. introduce way symbolically represent BsP tables. Basically,
seen construction demand, based algorithm steps, increasingly
large portions space belief states. key intuition BsP table

fhfs11 ; : : : ; s1n1 g : 1i; : : : ; hfsk1 ; : : : ; skn g : k ig
k

represented relation plans (of length) states, associating
plan directly state belief state indexed plan, follows:

fhs11 : 1i; : : : ; hs1n1 : 1i; : : : ; hsk1 : k i; : : : ; hskn : k ig
k

(2)

use additional variables represent plans BsP tables. order represent
plans increasing length, step algorithm, vector new Bdd variables,
called plan variables, introduced. vector plan variables introduced i-th step
algorithm written [i], j [i]j = jff j, used encode i-th last action
plan4. step one algorithm, introduce vector plan variables [1]
represent action corresponding 1-length possible conformant plan. BsP
3. principle, machinery symbolic search could used conformant planning applied
determinization domain automaton, i.e. automaton 2S state space. However,
would require introduction exponential number state variables, impractical
even small domains.
4. search performed backwards, plans need reversed found.

319

fiCimatti & Roveri
table BsPT1 level 1 built ExpandBsPTable performing following Bdd
computation starting BsP table level 0, i.e. G (x ):
(8x 0 :(R(x ;ffff;xx0 ) ! G (x )[x =xx0 ]) ^ Applicable(x ;ffff))[ff = [1]]
computation collects state-action pairs hx : ff (the action represented
by) ff applicable (the state represented by) x , resulting (states
represented by) x 0 goal states. replace vector action variables ff
first vector plan variables [1]. resulting Bdd, BsPT(x ; [1]), represents BsP
table containing plans length one form relation states plans
(2). general case, step 1, BsP table BsPTi 1 , associating belief states
plans length 1, represented Bdd state variables x plan
variables [i 1] ; : : : ; [1]. computation performed ExpandBsPTable step
implemented following Bdd transformation BsPTi 1
(8x 0 :(R(x ;ffff;xx0 ) ! BsPTi 1 (x ; [i 1]; : : : ; [1] )[x =xx0 ]) ^ Applicable(x ;ffff))[ff = [i]](3)
next state variables R BsPTi 1 (resulting forward shifting) disappear
universal quantification. action variables ff renamed newly
introduced plan variables [i], next step algorithm construction
repeated.
ExtractSolution extracts assignments plan variables corresponding set contains initial states. terms Bdd transformations, ExtractSolution
implemented follows:
8x:(I (x) ! BsPTi(x; [i]; : : : ; [1]))
(4)
result Bdd plan variables [i]; : : : ; [1]. Bdd F alse,
solutions length i. Otherwise, satisfying assignments resulting Bdd
represents conformant solution problem.
guarantee termination algorithm, step BsP table returned
ExpandBsPTable simplified PruneBsPTable removing belief states
deserve expansion. requires comparison belief states
contained BsP table belief states contained BsP tables built
previous levels. one crucial steps terms eciency. earlier implementation step logical Bdd transformations, following directly set-theoretical
definition PruneBsPTable, extremely inecient (Cimatti & Roveri, 1999). Furthermore, noticed serial encoding could yield BsP tables containing large
number equivalent plans, indexing exactly belief state. Often equivalent plans differ order independent actions, potential source
combinatorial explosion. occurs even simple version BTUC (in Figure 3,
two equivalent conformant plans associated Bs8 ). Therefore, developed new
implementation could tackle two problems operating directly BsP
table. idea depicted Figure 7. Initially, cache contains Bs1 , Bs2 Bs3 .
simplification performs traversal Bdd, accumulating subtrees representing
belief states, comparing ones built previous levels, inserting new
ones cache (in Figure 7, Bs4 , Bs5 Bs6 ). time path identified
320

fiConformant Planning via Symbolic Model Checking
BsP Table

Bs4

Bs2

Bs5

Pruned BsP Table

Bs6

Bs4

Bs5

Cached Belief States
Bs1

Bs2

Bs6

Cached Belief States

Bs3

Bs1

Bs2

Bs3

Bs4

Bs5

Bs6

Figure 7: example pruning BsP table
represents plan indexing already cached belief state, plan redundant
corresponding path pruned5. cost simplification linear size BsP
simplified highly effective pruning.
6. CMBP: BDD-based Conformant Planner

Cmbp (Conformant Model Based Planner) conformant planner implementing data
structures algorithms conformant planning described previous sections. Cmbp
inherits features Mbp (Cimatti et al., 1997, 1998b, 1998a), planner based
symbolic model checking techniques. Mbp built top NuSMV, symbolic model
checker jointly developed ITC-IRST CMU (Cimatti et al., 2000), uses
CUDD (Somenzi, 1997) state-of-the-art Bdd package. Mbp two-stage system.
first stage, internal Bdd-based representation domain built,
second stage planning problems solved. Currently, planning domains described
means high-level action language AR (Giunchiglia et al., 1997). AR allows
specify (conditional uncertain) effects actions means high level assertions.
instance, Figure 8 shows AR description BTUC problem6. semantics
AR yields serial encoding, i.e. exactly one action assumed executed

5. pruning mechanism actually weaker earlier one (Cimatti & Roveri, 1999).
require belief state must expanded twice search, earlier
version prune belief states contained previously explored ones. may increase number
explored belief states. However, allows much ecient implementation, without impacting
properties algorithm.
6. ! & stand negation conjunction, respectively. description slightly edited sake
readability. particular, Mbp currently accept parameterized AR descriptions. practice
use script language generate ground instances different complexity parameterized problem
description.

321

fiCimatti & Roveri

DOMAIN BTUC
ACTIONS Dunk_1, Dunk_2, Flush;
FLUENTS In_1, In_2, Defused, Clogged : boolean;
INERTIAL Clogged, Defused, In_1, In_2;
ALWAYS In_1 <-> !In_2;
Flush CAUSES !Clogged;
[1, 2] {
Dunk_<i> PRECONDITIONS !Clogged;
Dunk_<i> CAUSES Defused In_<i>;
Dunk_<i> POSSIBLY CHANGES Clogged;
}
INITIALLY !Defused;
CONFORMANT Defused & !Clogged;

Figure 8: AR description BTUC problem
time. automaton corresponding AR description obtained means
minimization procedure Giunchiglia (1996). procedure solves frame problem
ramification problem, eciently implemented Mbp (Cimatti et al., 1997).
separation domain construction planning phases, Mbp
bound AR. Standard deterministic domains specified Pddl (Ghallab et al.,
1998) also given Mbp means (prototype) compiler. also starting
investigate potential use C action language (Giunchiglia & Lifschitz, 1998),
allows represent domains parallel actions.
Different planning algorithms applied specified planning problems.
operate solely automaton representation, completely independent
particular language used specify domain. Mbp allows automatic construction
conditional plans total observability, implementing algorithms strong planning (Cimatti et al., 1998b), strong cyclic plannig (Cimatti et al., 1998a; Daniele,
Traverso, & Vardi, 1999). Cmbp, implemented ideas described previous
sections. primitives construct prune BsP tables required lot tuning,
particular ordering Bdd variables. found general ordering strategy
works reasonably well: action variables positioned top ordering, followed
plan variables, followed state variables, current state next state variables interleaved. specific ordering within action variables, plan variables, state variables
determined standard mechanism implemented NuSMV. Cmbp implements several
algorithms conformant planning. addition backward algorithm presented
322

fiConformant Planning via Symbolic Model Checking
Section 4, Cmbp implements algorithm based forward search, allows exploit
initial knowledge problem, sometimes resulting significant speed ups (Cimatti
& Roveri, 2000). Backward forward search also combined, tackle exponential growth search time depth search. algorithms,
different options enable disable different versions termination check.
7. Experimental Evaluation

section present experimental evaluation approach, carried
comparing Cmbp state-of-the-art conformant planners. first describe
conformant planners considered analysis, present experimental
comparison carried out.
7.1 Conformant Planners

Cgp (Smith & Weld, 1998) extends ideas Graphplan (Blum & Furst, 1995, 1997)
deal uncertainty. Basically, planning graph built every possible sequence possible worlds, constraints among planning graphs propagated ensure conformance.
Cgp system takes input domains described extension Pddl (Ghallab et al.,
1998), possible specify uncertainty initial state. Cgp inherits
Graphplan ability deal parallel actions. Cgp first ecient conformant planner: shown outperform several planners Buridan (Peot,
1998) UDTPOP (Kushmerick, Hanks, & Weld, 1995). detailed comparison reported Smith Weld (1998) leaves doubt superiority Cgp respect
systems. Therefore, compared Cmbp Cgp consider
systems analyzed Smith Weld (1998). Cmbp expressive Cgp two
respects. First, Cgp handle uncertainty initial state. instance, Cgp
cannot analyze BTUC domain presented Section 3. Smith Weld (1998) describe
approach extended actions uncertain effects. Second, Cgp cannot
conclude planning problem conformant solutions.
Qbfplan (our name for) planning system Rintanen (1999a). Qbfplan generalizes idea SAT-based planning (Kautz, McAllester, & Selman, 1996; Kautz & Selman,
1996, 1998) nondeterministic domains, encoding problems QBF. Qbfplan
approach limited conformant planning, used conditional planning
uncertainty, also partial observability: different encodings, corresponding
different structures resulting plan, synthesized. paper,
considering encodings enforce resulting plan sequence. Given bound
length plan, first QBF encoding problem generated, QBF
solver (Rintanen, 1999b) called. solution found, new encoding longer plan
must generated solved. Qbfplan able handle actions uncertain effects.
done introducing auxiliary (choice) variables, assignments different possible outcomes actions correspond. variables universally quantified
ensure conformance solution. Differently e.g. Blackbox (Kautz & Selman,
1998), Qbfplan heuristic guess \right" length plan. Given
limit length plan, generates encodings specified length,
repeatedly calls QBF solver encodings increasing length plan found.

323

fiCimatti & Roveri
Cgp, Qbfplan cannot conclude planning problem conformant solutions.
Similarly Cmbp, Qbfplan relies symbolic representation problem, although
QBF transformations performed theorem prover rather Bdds.
Gpt (Bonet & Geffner, 2000) general planning framework, conformant
planning problem seen deterministic search problem space belief states. Gpt
uses explicit representation search space, belief state represented
separate data structure. search based algorithm (Nilsson, 1980),
driven domain dependent heuristics automatically generated problem
description. Gpt accepts problem descriptions syntax based Pddl, extended deal
probabilities uncertainty. possible represent domains uncertain action
effects (although representation actions resulting large number different states
rather awkward). planning algorithm, Gpt able conclude given
planning problem conformant solution exhaustively exploring space belief
states.
7.2 Experiments Results

evaluation performed running systems number parameterized problem domains. considered problems Cgp Gpt distributions, plus
problems defined test specific features planners. considered
domains uncertainty limited initial state, domains uncertain action
effects. Besides problems admitting solution, also considered problems admitting
solution, case measured effectiveness plannner returning
failure.
Given different expressivity, possible run systems
examples. Cmbp run classes examples, Gpt run one.
Cgp run problems admit solution, uncertainty limited
initial condition. Qbfplan run examples encoding
already available Qbfplan distribution. subset problems
expressible Cgp. main limiting factor low level input format
Qbfplan: problem descriptions must specified ML code generates QBF
encodings. Writing new encodings turned dicult task, especially due
lack documentation.
ran Cgp, Qbfplan Cmbp Intel 300MHz Pentium-II, 512MB RAM,
running Linux. comparison Cmbp Gpt run Sun Ultra Sparc
270MHz, 128Mb RAM running Solaris (Gpt available binary). However,
performance two machines comparable | run times Cmbp almost
identical. CPU time limited 7200 sec (two hours) test. avoid swapping,
memory limit fixed physical memory machine. following,
write \|" \===" test complete within time memory
limits, respectively. performance systems reported tables listing
search time. excludes time needed Qbfplan generate encodings,
time spent Cmbp construct automaton representation Bdd, time
needed Gpt generate source code internal representation, compile
it. Overall, significant time ignored automaton construction Cmbp.
324

fiConformant Planning via Symbolic Model Checking
Currently, automaton construction fully optimized. Even complex
examples, however, construction never required couple minutes7.
7.2.1 Bomb Toilet

Bomb Toilet. first domain tackled classical bomb toilet,
notion clogging. call problem BT(p), parameter p

number packages. uncertainty initial condition,
known package contains bomb. goal defuse bomb. results
BT problem shown Table 1. columns relative Cmbp length
plan (jPj), number cached belief states number hits cache (#BS
#NBS respectively), time (expressed seconds) needed searching automaton
Pentium/Linux (Time(L)) Sparc/Solaris (Time(S)). following,
clear context, execution platform omitted. columns relative Cgp
number levels planning graphs (jLj) search time. column relative
Gpt search time.
BT(2)
BT(3)
BT(4)
BT(5)
BT(6)
BT(7)
BT(8)
BT(9)
BT(10)

jPj

2
3
4
5
6
7
8
9
10

Cmbp

#BS/#BSH
2/2
6 / 11
14 / 36
30 / 103
62 / 266
126 / 641
254 / 1496
510 / 3463
1022 / 7862

Time(L)
0.000
0.000
0.000
0.000
0.010
0.010
0.030
0.070
0.150

Time(S)
0.000
0.000
0.000
0.000
0.010
0.030
0.030
0.070
0.140

jLj
1
1
1
1
1
1
1
1
1

Cgp

Time
0.000
0.000
0.000
0.000
0.010
0.010
0.020
0.020
0.020

Gpt

Time
0.074
0.077
0.080
0.087
0.102
0.139
0.230
0.481
1.018

Table 1: Results BT problems.
BT problem intrinsically parallel, i.e. depth planning graph always
one, packages dunked time. Cgp inherits Graphplan ability deal parallel actions eciently, therefore almost insensitive
problem size. problem Cgp outperforms Cmbp Gpt. Notice
number levels explored Cgp always 1, length plan produced
Cmbp Cgp grows linearly. Cmbp performs slightly better Gpt.
Bomb Toilet Clogging. call BTC(p) extension BT(p)
dunking package (always) clogs toilet, ushing remove clogging, clogging precondition dunking package. Again, p number packages. toilet
initially clogged. modification, problem longer allows parallel
solution. results problem listed Table 2. impact depth
plan length becomes significant systems. Cmbp Gpt outperform Cgp.
case Cmbp performs better Gpt, especially large instances (see BTC(16)).
7. precisely, maximum time building automaton required BMTC(10,6) examples
(88 secs.), RING(10) example (77 secs.), BMTC(9,6) examples (40 secs.), BMTC(10,5)
examples (41 secs.). examples, time required automaton construction
less 10 seconds.

325

fiCimatti & Roveri
Qbfplan

BTC(2)
BTC(3)
BTC(4)
BTC(5)
BTC(6)
BTC(7)
BTC(8)
BTC(9)
BTC(10)

jPj

3
5
7
9
11
13
15
17
19

Cmbp

Cgp

#BS/#BSH Time(L) Time(S) jLj Time
6/8
0.000
0.010 3
0.000
14 / 23
0.000
0.000 5
0.010
30 / 61
0.010
0.010 7
0.030
62 / 150
0.020
0.020 9
0.130
126 / 347
0.020
0.020 11
0.860
254 / 796
0.070
0.080 13
2.980
510 / 1844
0.150
0.160 15 13.690
1022 / 4149
0.320
0.330 17 41.010
2046 / 9190
0.710
0.700 19 157.590

BTC(16) 31 131070 / 921355

99.200

99.800

Gpt

Time
0.074
0.077
0.082
0.094
0.113
0.166
0.288
0.607
1.309
351.457

BTC(6)
jPj Time
1
0.00
2
0.01
3
0.26
4
0.63
5
1.53
6
2.82
7
6.80
8
14.06
9
35.59
10
93.34
11 (+) 2.48

BTC(10)
Time
1
0.02
2
0.03
3
0.78
4
2.30
5
4.87
6
8.90
7
22.61
8
52.72
9
156.12
10
410.86
11 1280.88
13 3924.96
14
|

jPj

:::
:::
18
|
19 (+) 16.84

Table 2: Results BTC problems.
comparison Qbfplan limited 6 10 package instances (the ones available distribution package). performance Qbfplan reported left
table Table 2. line reports time needed decide whether plan
length i. performance Qbfplan rather good tackling encoding admitting solution (in Table 2 entries labeled (+)). instance, BTC(10)
Qbfplan finds solution solving encodings depth 19 reasonably fast. However,
solution cannot found, i.e. QBF formula admits model, performance
Qbfplan degrades significantly (for depth 18 encoding, let solver run 10
CPU hours complete search). difference performance,
diculty writing new domains, rest comparison consider
Qbfplan.
Bomb Multiple Toilets. next domain, called BMTC(p,t), generalization
BTC problem case multiple toilets (p number packages,
number toilets). problem becomes parallelizable number
toilets increases. Furthermore, considered three versions problem increasing
uncertainty initial states. first class tests (\Low Uncertainty" columns),
uncertainty position bomb unknown, toilets known
clogged. \Mid Uncertainty" \High Uncertainty" columns show results
presence uncertainty initial state. second [third, respectively] class
tests, status every odd [every, resp.] toilet either clogged clogged.
increases number possible initial states.
results reported Table 3 (for comparison Cgp) Table 4
(for comparison Gpt). column represents number initial states
corresponding problem. Cgp able fully exploit parallelism problem.
However, Cgp never able explore 9 levels planning graph, depth
decreasing number initial states. results also show Cmbp Gpt
much less sensitive number initial states Cgp. increasing initial
326

fi(p,t)
(2,2)
(3,2)
(4,2)
(5,2)
(6,2)
(7,2)
(8,2)
(9,2)
(10,2)
(2,3)
(3,3)
(4,3)
(5,3)
(6,3)
(7,3)
(8,3)
(9,3)
(10,3)
(2,4)
(3,4)
(4,4)
(5,4)
(6,4)
(7,4)
(8,4)
(9,4)
(10,4)
(2,5)
(3,5)
(4,5)
(5,5)
(6,5)
(7,5)
(8,5)
(9,5)
(10,5)
(2,6)
(3,6)
(4,6)
(5,6)
(6,6)
(7,6)
(8,6)
(9,6)
(10,6)

bmtc


2
3
4
5
6
7
8
9
10
2
3
4
5
6
7
8
9
10
2
3
4
5
6
7
8
9
10
2
3
4
5
6
7
8
9
10
2
3
4
5
6
7
8
9
10

2
4
6
8
10
12
14
16
18
2
3
5
7
9
11
13
15
17
2
3
4
6
8
10
12
14
16
2
3
4
5
7
9
11
13
15
2
3
4
5
6
8
10
12
14

jPj

Low Uncertainty
Cmbp
#BS/#BSH Time
10 / 18 0.000
26 / 84 0.000
58 / 250 0.020
122 / 652 0.030
250 / 1552 0.070
506 / 3586 0.180
1018 / 8262 0.400
2042 / 18484 0.940
4090 / 40676 1.820
18 / 42 0.000
47 / 202 0.010
110 / 736 0.030
237 / 2034 0.080
492 / 5106 0.230
1003 / 12128 0.560
2026 / 27836 1.300
4073 / 62470 3.330
8168 / 138046 7.280
29 / 75 0.010
92 / 492 0.020
206 / 1686 0.060
457 / 4987 0.190
964 / 12456 0.410
1983 / 29453 1.040
4026 / 68466 2.740
8117 / 153895 6.690
16304 / 339160 14.420
43 / 117 0.010
164 / 1031 0.040
416 / 4304 0.150
872 / 11763 0.490
1875 / 31695 1.300
3901 / 78009 3.990
7974 / 183036 9.670
16142 / 416333 24.250
32501 / 927329 54.910
60 / 168 0.010
270 / 1848 0.070
786 / 9294 0.300
1777 / 29075 1.160
3613 / 71123 3.290
7625 / 180127 9.060
15726 / 429198 20.710
32012 / 986188 50.610
64675 / 2.21106e+06 111.830
Time
0.000
0.020
0.030
1.390
3.490
508.510
918.960
|
0.010
0.010
0.110
0.170
0.340
6248.010
|

Cgp

327

Mid Uncertainty
Cmbp

#BS/#BSH Time
4
12 / 34 0.000
6
28 / 106 0.000
8
60 / 286 0.020
10
124 / 702 0.030
12
252 / 1614 0.080
14
508 / 3662 0.190
16
1020 / 8362 0.430
18
2044 / 18602 0.960
20
4092 / 40810 1.990
8
24 / 99 0.000
12
56 / 349 0.020
16
120 / 942 0.040
20
248 / 2335 0.110
24
504 / 5520 0.250
28
101 / 12673 0.590
32
204 / 28530 1.350
36
408 / 63331 3.370
40
818 / 139092 7.460
8
29 / 75 0.000
12
108 / 808 0.030
16
236 / 2356 0.080
20
492 / 5888 0.230
24
1004 / 13648 0.470
28
2028 / 31004 1.120
32
4076 / 70584 2.870
36
8172 / 15654 6.900
40
16364 / 34234 14.630
16
43 / 117 0.010
24
212 / 2008 0.080
32
475 / 6375 0.260
40
987 / 15928 0.700
48
2011 / 37759 1.890
56
4059 / 86716 4.480
64
8155 / 195055 10.590
72
16347 / 432408 25.600
80
32731 / 948279 56.420
16
60 / 168 0.010
24
270 / 1848 0.070
32
920 / 13810 0.500
40
1958 / 37636 1.940
48
4005 / 90111 4.080
56
8100 / 208050 10.130
64
16291 / 469277 22.620
72 32674 / 1.04173e+06 53.510
80 65441 / 2.28585e+06 116.440
0.020
0.290
0.730
|

1 0.200
1 0.830
2 30.630
2 30.140
2 57.300
2
|

1 0.130
2 3.540
2 6.320
2 37,959
2
|

1
2
2
2

0.090
0.200
0.990
|

2
2
3
3

2
3
4
5
5

Time
0.010
0.040
0.460
13,180
|

Cgp

jLj

Table 3: Results BMTC problems.

1
0.000
1
0.010
1
0.010
3
0.500
3
1.160
3
2.410
3
8.540
4
|
1
0.010
1
0.020
1
0.020
1
0.050
3
5.920
3 18.410
3 62.040
3 194.640
3 289,680
1
0.010
1
0.010
1
0.040
1
0.060
1
0.100
3 211.720
3 1015.160
3 3051.990
2
|

1
3
3
5
5
7
7
7
1
1
3
3
3
5
4

jLj

High Uncertainty
Cmbp

#BS/#BSH Time
8
12 / 40 0.000
12
28 / 112 0.010
16
60 / 294 0.010
20
124 / 710 0.040
24
252 / 1622 0.080
28
508 / 3670 0.190
32
1020 / 8372 0.450
36
2044 / 18612 0.950
40
4092 / 40820 2.030
16
24 / 126 0.010
24
56 / 373 0.020
32
120 / 972 0.040
40
248 / 2371 0.120
48
504 / 5562 0.240
56
1016 / 12721 0.640
64
2040 / 28584 1.330
72
4088 / 63391 3.390
80
8184 / 139158 7.430
32
48 / 332 0.020
48
112 / 960 0.040
64
240 / 2532 0.090
80
496 / 6092 0.240
96
1008 / 13876 0.470
112
2032 / 31260 1.160
128
4080 / 70912 2.910
144
8176 / 156904 6.970
160
16368 / 342736 14.770
64
93 / 751 0.030
96
224 / 2591 0.120
128
480 / 6740 0.260
160
992 / 16393 0.730
192
2016 / 38334 1.980
224
4064 / 87411 4.540
256
8160 / 195880 10.640
288
16352 / 433373 25.370
320
32736 / 949394 56.290
128
171 / 1533 0.040
192
448 / 6248 0.310
256
960 / 16344 0.690
320
1984 / 39710 2.120
384
4032 / 92772 4.600
448
8128 / 211370 10.400
512
16320 / 473328 23.000
576 32704 / 1.04658e+06 54.010
640 65472 / 2.29158e+06 116.240
1.610
8.690
32.190
|

0.170
0.690
|

Time
0.030
13.560
145.830
|

Cgp

2 337.604
2 1459.110
2 5643.450
2
|

2 21.120
2 138.430
2 551.210
2 1523.840
2
|

2
2
2
3

2
2
3

2
4
4
4

jLj

Conformant Planning via Symbolic Model Checking

fiCimatti & Roveri
bmtc

(p,t)
(2,2)
(3,2)
(4,2)
(5,2)
(6,2)
(7,2)
(8,2)
(9,2)
(10,2)
(2,4)
(3,4)
(4,4)
(5,4)
(6,4)
(7,4)
(8,4)
(9,4)
(10,4)
(2,6)
(3,6)
(4,6)
(5,6)
(6,6)
(7,6)
(8,6)
(9,6)
(10,6)

Low Unc.

Cmbp

Time
0.000
0.010
0.000
0.040
0.080
0.190
0.390
0.910
1.850
0.000
0.010
0.050
0.180
0.370
1.080
2.700
8.970
14.210
0.010
0.050
0.310
1.110
3.400
8.910
21.240
49.880
113.680

Gpt

Time
0.079
0.087
0.105
0.146
0.227
0.441
0.922
2.211
5.169
0.109
0.156
0.270
0.616
1.435
3.484
8.767
23.858
59.966
0.303
0.562
1.354
3.257
8.691
25.677
68.427
289.000
486.969

High Unc.

Cmbp

Time
0.010
0.010
0.020
0.040
0.070
0.200
0.400
0.950
1.900
0.010
0.040
0.100
0.240
0.460
1.190
2.830
6.920
114.690
0.060
0.260
0.620
2.060
4.660
10.430
23.860
54.190
118.590

Gpt

Time
0.079
0.091
0.121
0.198
0.376
0.850
1.966
4.743
10.620
0.121
0.284
1.016
3.282
9.374
27.348
72.344
180.039
440.308
0.482
2.471
17.406
74.623
243.113
701.431
===

Table 4: Results BMTC problems.
uncertainty, Cgp almost unable solve trivial problems. Gpt performs better
Cgp, suffers explicit representation search space.
Bomb Toilet Uncertain Clogging. BTUC(p) domain domain
described Section 2, clogging uncertain outcome dunking package.
kind problem cannot expressed Cgp. results Cmbp Gpt reported
Table 5. Although Cmbp performs better Gpt (by factor two three),
significant difference behavior. interesting compare results Cmbp
BTC BTUC problems. Gpt slight difference noticeable, resulting
increased branching factor search space due uncertainties effects
action executions. performance Cmbp, number uncertainties direct
factor | example, BTC(16) BTUC(16), performance almost same.
7.2.2 Ring Rooms

Simple Ring Room. considered another domain, robot move
ring rooms. room window, either open, closed locked.
robot move (either clockwise counterclockwise), close window room
is, lock closed. goal windows locked.
328

fiConformant Planning via Symbolic Model Checking
Cmbp

jPj

BTUC(2)
BTUC(3)
BTUC(4)
BTUC(5)
BTUC(6)
BTUC(7)
BTUC(8)
BTUC(9)
BTUC(10)
BTUC(16)

#BS/#BSH
6/8
14 / 23
30 / 61
62 / 150
126 / 347
254 / 796
510 / 1844
1022 / 4149
2046 / 9190
131070 / 921355

3
5
7
9
11
13
15
17
19
31

Time
0.000
0.000
0.010
0.010
0.030
0.050
0.170
0.310
0.720
98.270

Gpt

Time
0.076
0.078
0.085
0.098
0.128
0.205
0.380
0.812
1.828
486.252

Table 5: Results BTUC problems.
N-1

N

1

2

problem RING(r), r number rooms, uncertainty
initial condition: position robot status windows
uncertain. problems parallel solution, large number initial
states (r 3r ), corresponding full uncertainty position robot
status window. results8 reported left Table 6. Cmbp outperforms
RING(2)
RING(3)
RING(4)
RING(5)
RING(6)
RING(7)
RING(8)
RING(9)
RING(10)

jPj

5
8
11
14
17
20
23
26
29

Cmbp

#BS/#BSH
8 / 24
26 / 78
80 / 240
242 / 726
728 / 2184
2186 / 6558
6560 / 19680
19682 / 59046
59048 / 177144

Time
0.000
0.020
0.040
0.120
0.370
1.420
4.950
27.330
106.870

jLj
3
4

Cgp

Time
0.070
|

Gpt

Time
0.085
0.087
0.392
1.150
6.620
23.636
105.158
===


1
2
4
8
16

Cgp RING(5)

jLj
5
5
5
5
5

Time
0.010
0.060
0.420
6.150
|

jLj
9
9
9
9
9

Time
0.020
0.140
1.950
359.680
|

Table 6: results RING problems.
Cgp Gpt, although Gpt performs much better Cgp. Cgp Gpt
suffer increasing complexity problem. right Table 6, plot (for
RING(5) problem) dependency Cgp number initial states combined
number levels explored (different goals provided require
exploration different levels). clear number initial states depth
search critical factors Cgp.
8. times reported Cgp refer scaled-down version problem, locking taken
account, thus maximum number initial states r 2r .

329

fiCimatti & Roveri
Ring Rooms Uncertain Action Effects. considered variation

RING domain, called URING, first introduced Cimatti Roveri (1999),
expressible Cgp. window locked robot performing action
determine status (e.g. closing it), window open close nondeterministically. instance, robot moving room 1 room 2, windows
room 3 4 could open closed wind. domain clearly designed stress
ability planner deal actions large number resulting states.
worst case (e.g. move action performed window locked), 2r possible
resulting states. Although seemingly artificial, captures fact environments
practice highly nondeterministic. tried compare Cmbp Gpt URING
problem. principle Gpt able deal uncertainty action effects. However,
failed codify URING Gpt language, requires conditional description uncertain effects. Therefore, experimented variation RING
domain featuring higher degree nondeterminism, called NDRING following.
NDRING domain contains increasing number additional propositions, called
following noninertial propositions, initially unknown nondeterministically
altered action. number noninertial propositions, action 2i
NDRING(2)
NDRING(3)
NDRING(4)
NDRING(5)
NDRING(6)
NDRING(7)
NDRING(8)
NDRING(9)
NDRING(10)

jPj

5
8
11
14
17
20
23
26
29

Cmbp

#BS/#BSH
8 / 24
26 / 78
80 / 240
242 / 726
728 / 2184
2186 / 6558
6560 / 19680
19682 / 59046
59048 / 177144

Time (5)
0.000
0.020
0.040
0.110
0.350
1.350
4.990
27.060
103.760

Time (2)
0.140
0.256
1.046
4.550
18.758
108.854
===

Gpt

Time (3)
0.384
0.679
3.025
12.960
57.300
===

Time (4)
0.948
2.574
12.548
48.426
===

Time (5)
4.544
13.960
67.714
===

Table 7: results NDRING problems.
possible outcomes. results listed Table 7, columns labeled Time(i).
growing branching factor search major impact performance
Gpt, Cmbp insensitive kind uncertainty. (The performance Cmbp
lower number noninertial propositions reported basically
same.)
URING problem run Cmbp. results listed Table 8.
noticed performances Cmbp improve significantly respect RING
problem. explained considering that, despite larger number transitions,
number explored belief states significantly smaller (see Bs cache statistics
Tables 6 8).
7.2.3 Square Cube

following domains SQUARE(n) CUBE(n) Gpt distribution (Bonet
& Geffner, 2000). problems consist robot navigating square cube side
n. domains actions moving robot possible directions.
Moving robot boundary leaves robot position. original
330

fiConformant Planning via Symbolic Model Checking

URING(2)
URING(3)
URING(4)
URING(5)
URING(6)
URING(7)
URING(8)
URING(9)
URING(10)

jPj
5
8
11
14
17
20
23
26
29

Cmbp

#BS/#BSH
5 / 16
11 / 34
23 / 70
47 / 142
95 / 286
191 / 574
383 / 1150
767 / 2302
1535 / 4606

Time
0.000
0.010
0.020
0.040
0.080
0.190
0.410
0.980
2.2300

Table 8: Results URING problems.
problems, called CORNER following, require robot reach corner, starting
completely unspecified position. introduced two variations. first, called
FACE, initial position position given side [face] square [cube],
goal reach central position opposite side [face]. second, called
CENTER, initial position completely unspecified, goal center
square [cube]. corner problem, simple heuristic perform steps towards
corner, thus pruning half actions. variations designed allow
simple heuristic | instance, CENTER problem, action eliminated.
SQUARE(i)
SQUARE(2)
SQUARE(4)
SQUARE(6)
SQUARE(8)
SQUARE(10)
SQUARE(12)
SQUARE(14)
SQUARE(16)
SQUARE(18)
SQUARE(20)
CUBE(i)
CUBE(2)
CUBE(3)
CUBE(4)
CUBE(5)
CUBE(6)
CUBE(7)
CUBE(8)
CUBE(9)
CUBE(10)
CUBE(15)

jPj

3
6
9
12
15
18
21
24
27
42

jPj

2
6
10
14
18
22
26
30
34
38

CORNER
Cmbp
#BS/#BSH Time
2 / 4 0.000
15 / 37 0.000
35 / 93 0.000
63 / 173 0.020
99 / 277 0.030
143 / 405 0.050
195 / 557 0.070
255 / 733 0.080
323 / 933 0.120
399 / 1157 0.160

CORNER
Cmbp
#BS/#BSH Time
6 / 19 0.000
26 / 99 0.010
63 / 261 0.020
124 / 537 0.040
215 / 957 0.050
342 / 1551 0.100
511 / 2349 0.160
728 / 3381 0.330
999 / 4677 0.440
3374 / 16167 1.940

Gpt

Time
0.332
0.168
0.430
0.276
0.500
0.567
1.082
1.765
2.068
9.207

Gpt

Time
0.074
0.080
0.092
0.115
0.149
0.196
0.261
0.357
0.503
0.638

jPj

3
6
11
14
19
22
27
30
35
54

jPj

2
7
12
17
22
27
32
37
42
47

FACE
Cmbp
#BS/#BSH Time
2 / 4 0.000
33 / 83 0.000
86 / 232 0.020
163 / 453 0.040
264 / 746 0.090
389 / 1111 0.150
538 / 1548 0.230
711 / 2057 0.320
908 / 2638 0.540
1129 / 3291 0.650

Gpt

Time
0.058
0.065
0.089
0.139
0.228
0.371
0.582
0.908
1.343
1.883

jPj

2
8
14
20
26
32
38
44
50
56

CENTER
Cmbp
#BS/#BSH Time
2 / 4 0.000
76 / 190 0.010
218 / 592 0.040
432 / 1210 0.090
718 / 2044 0.190
1076 / 3094 0.360
1506 / 4360 0.560
2008 / 5842 0.820
2582 / 7540 1.330
3228 / 9454 1.790

Gpt

Time
0.060
0.083
0.216
0.695
2.135
5.340
12.284
26.241
52.091
94.204

FACE
CENTER
Cmbp
Gpt
Cmbp
Gpt
#BS/#BSH Time Time jPj #BS/#BSH Time Time
6 / 19 0.000 0.061 3
6 / 19 0.010 0.061
26 / 99 0.000 0.069 6
26 / 99 0.010 0.144
319 / 1360 0.050 0.193 12
722 / 3091 0.130 0.569
709 / 3095 0.220 0.412 15
1696 / 7402 0.430 2.010
1343 / 6116 0.430 1.479 21 3365 / 15432 0.910 10.717
2255 / 10377 0.840 3.323 24 5797 / 26814 1.860 34.074
3519 / 16464 1.400 8.161 30 9248 / 43541 3.520 109.852
5169 / 24331 2.810 16.272 33 13786 / 65237 7.260 701.910
7279 / 34564 4.550 32.226 39 19667 / 93898 9.990
===
26439 / 127825 28.560
=== 60 74041 / 359354 58.930

Table 9: Results SQUARE CUBE problems.
results problems reported Table 9. tests run
Cmbp Gpt. experiments highlight eciency Gpt strongly depends
quality heuristic function. If, first set experiments, heuristics
331

fiCimatti & Roveri
effective, Gpt almost good Cmbp. Otherwise, Gpt degrades significantly.
general, finding heuristics effective belief space appears nontrivial
problem. Cmbp appears stable9 , performs blind, breadth-first search,
relies cleverness symbolic representation achieve eciency.
7.2.4 Omelette

Finally, considered OMELETTE(i) problem (Levesque, 1996). goal
good eggs bad ones one two bowls capacity i. unlimited number
eggs, unpredictably good bad. eggs grabbed broken
bowl. content bowl discarded, poured bowl. Breaking
rotten egg bowl effect spoiling bowl. bowl always cleaned
discarding content. problem originally presented partial observability
problem, sensing action allowing test bowl spoiled not. considered
variation problem without sensing action: case conformant solution
exists. used OMELETTE problems test ability Cmbp Gpt discover
problem admits conformant solution. results reported Table 10.
table shows Cmbp effective checking absence conformant solution,
outperforms Gpt several orders magnitude.
OMELETTE(3)
OMELETTE(4)
OMELETTE(5)
OMELETTE(6)
OMELETTE(7)
OMELETTE(8)
OMELETTE(9)
OMELETTE(10)
OMELETTE(15)
OMELETTE(20)
OMELETTE(30)

# steps
9
11
13
15
17
19
21
23
33
43
63

CMBP
#BS/#BSH
15 / 34
19 / 42
23 / 50
27 / 58
31 / 66
35 / 74
39 / 82
43 / 90
63 / 130
83 / 170
123 / 250

Time
0.020
0.030
0.040
0.050
0.060
0.090
0.110
0.120
0.210
0.440
0.890

GPT
Time
0.237
0.582
1.418
2.904
5.189
10.307
18.744
32.623
225.530
===

Table 10: Results OMELETTE problems.
7.3 Summarizing Remarks

Overall, Cmbp appears implement effective approach conformant planning,
terms expressivity performance. Cgp able deal uncertainties
initial states, cannot conclude problem admit conformant
solution. main problem Cgp seems enumerative approach uncertainties,
increased number initial states severely affects performance (see Table 3
Table 6).
Qbfplan principle able deal uncertain action effects, cannot conclude
problem admit conformant solution. small number ex9. Consider also problems increasingly dicult (see instance plan length).

332

fiConformant Planning via Symbolic Model Checking
periments could perform, approach implemented Qbfplan limited
Satplan style search: intermediate results obtained solving encoding
depth k reused solving encodings increasing depth. Furthermore,
solver appears specialized finding model, rather proving unsatisfiability.
However, latter ability needed encodings final one.
Gpt expressive system, allows eciently dealing wide class
planning problems. far conformant planning concerned, expressive
Cmbp. allows dealing uncertain action effects, conclude problem
conformant solution. However, Cmbp appears outperform Gpt
several respects. First, behaviour Gpt appears directly related number
possible outcomes action. Furthermore, eciency Gpt depends
effectiveness heuristic functions, sometimes dicult devise,
cannot help problem admit solution.
main strength Cmbp independence number uncertainties,
achieved use symbolic techniques. fully symbolic, Cmbp
exhibit enumerative behaviour competitors. Compared original approach
described Cimatti Roveri (1999), substantial improvement performance
obtained new implementation pruning step. disclaimer
order. well known Bdd based computations subject blow-up memory
requirements computing certain classes boolean functions, e.g. multipliers (Bryant,
1986). would trivial make example performance Cmbp degrades
exponentially. However, none examples considered, included
examples distribution Cgp Gpt, phenomenon occurred.
8. Related Work

term conformant planning first introduced Goldman (1996), presenting
formalism constructing conformant plans based extension dynamic logic. Recently, Ferraris Giunchiglia (2000) presented another conformant planner based SAT
techniques. system available direct comparison Cmbp. effectiveness approach dicult evaluate, limited testing described (Ferraris &
Giunchiglia, 2000). performance claimed comparable Cgp. However,
results reported enconding corresponding solution, behaviour
Qbfplan reported Table 2 suggests kind analysis might limited.
Several works share idea planning based automata theory. closely
related works lines planning via model checking (Cimatti et al., 1997), upon
work based. approach allows, instance, automatically construct
universal plans guaranteed achieve goal finite number steps (Cimatti
et al., 1998b), implement trial-and-error strategies (Cimatti et al., 1998a; Daniele
et al., 1999). results obtained hypothesis total observability,
run-time observation available. main difference substantial extension required lift symbolic techniques search space belief states. De
Giacomo Vardi (1999) analyze several forms planning automata theoretic
framework. Goldman, Musliner Pelican (2000) present method model checking
timed automata interleaved plan formation activity, make sure
333

fiCimatti & Roveri
timing constraints met. Finally, Hoey colleagues (1999) use algebraic decision
diagrams tackle problem stochastic planning.
9. Conclusions Future Work

paper presented new approach conformant planning, based use
Symbolic Model Checking techniques. algorithm general, applies
complex planning domains, uncertainty initial condition action effects,
described finite state automata. algorithm based breadthfirst, backward search, returns conformant plans minimal length, solution
planning problem exists. Otherwise, terminates failure. algorithm designed
take full advantage symbolic representation based Bdds. implementation
approach Cmbp system highly optimized, particular crucial
step termination checking. performed experimental comparison approach
state art conformant planners Cgp, Qbfplan Gpt. Cmbp strictly
expressive Qbfplan Cgp. problems comparison
possible, Cmbp outperformed competitors terms run times, sometimes
orders magnitude. Thanks use symbolic data structures, Cmbp able deal
eciently problems large numbers initial states action outcomes.
hand, qualitative behavior Cgp Gpt seems depend heavily
enumerative nature algorithms. Differently Gpt, Cmbp independent
effectiveness heuristic used drive search.
research presented paper extended following directions. First,
investigating alternative approach conformant planning, breadthfirst style search given up. techniques appear extremely promising |
preliminary experiments led speed ups two orders magnitude
results presented paper problems admit solution. Second, tackle
problem conditional planning partial observability, hypothesis
limited amount information acquired run time. conformant planning,
problem seen search belief space. However, appears significantly
complicated need dealing run-time observation conditional plans. Finally, considering extension domain construction planner
expressive input language, C , invariant detection techniques.
Acknowledgements

Fausto Giunchiglia provided continuous encouragement feedback work.
thank Piergiorgio Bertoli, Blai Bonet, Marco Daniele, Hector Geffner, Enrico Giunchiglia,
Jussi Rintanen, David Smith, Paolo Traverso, Dan Weld valuable discussions conformant planning various comments paper. David Smith provided code
Cgp, large number examples, time-out mechanism used experimental
evaluation. Jussi Rintanen made Qbfplan available Linux.
334

fiConformant Planning via Symbolic Model Checking
References

Blum, A. L., & Furst, M. L. (1995). Fast planning planning graph analysis.
Proc. Ijcai.
Blum, A. L., & Furst, M. L. (1997). Fast planning planning graph analysis. Artificial Intelligence 1{2, 90, 279{298.
Bonet, B., & Geffner, H. (2000). Planning Incomplete Information Heuristic Se
arch Belief Space. Chien, S., Kambhampati, S., & Knoblock, C. (Eds.), 5th
International Conference Artificial Intelligence Planning Scheduling, pp. 52{
61. AAAI-Press.
Brace, K., Rudell, R., & Bryant, R. (1990). Ecient Implementation BDD Package. 27th ACM/IEEE Design Automation Conference, pp. 40{45 Orlando, Florida.
ACM/IEEE, IEEE Computer Society Press.
Bryant, R. E. (1986). Graph-Based Algorithms Boolean Function Manipulation. IEEE
Transactions Computers, C-35 (8), 677{691.
Bryant, R. E. (1991). complexity VLSI implementations graph representations
Boolean functions application integer multiplication. IEEE Transactions
Computers, 40 (2), 205{213.
Bryant, R. E. (1992). Symbolic Boolean manipulation ordered binary-decision diagrams. ACM Computing Surveys, 24 (3), 293{318.
Burch, J. R., Clarke, E. M., McMillan, K. L., Dill, D. L., & Hwang, L. J. (1992). Symbolic
Model Checking: 1020 States Beyond. Information Computation, 98 (2),
142{170.
Cassandra, A., Kaelbling, L., & Littman, M. (1994). Acting optimally partially observable
stochastic domains. Proc. AAAI-94. AAAI-Press.
Cimatti, A., Clarke, E., Giunchiglia, F., & Roveri, M. (2000). NuSMV : new symbolic
model checker. International Journal Software Tools Technology Transfer
(STTT), 2 (4).
Cimatti, A., Giunchiglia, E., Giunchiglia, F., & Traverso, P. (1997). Planning via Model
Checking: Decision Procedure AR. Steel, S., & Alami, R. (Eds.), Proceeding
Fourth European Conference Planning, No. 1348 Lecture Notes Artificial
Intelligence, pp. 130{142 Toulouse, France. Springer-Verlag. Also ITC-IRST Technical
Report 9705-02, ITC-IRST Trento, Italy.
Cimatti, A., & Roveri, M. (1999). Conformant Planning via Model Checking. Biundo,
S. (Ed.), Proceeding Fifth European Conference Planning, Lecture Notes
Artificial Intelligence Durham, United Kingdom. Springer-Verlag. Also ITC-IRST
Technical Report 9908-01, ITC-IRST Trento, Italy.
335

fiCimatti & Roveri
Cimatti, A., & Roveri, M. (2000). Forward Conformant Planning via Symbolic Model
Checking. Proceeding AIPS2k Workshop Model-Theoretic Approaches
Planning Breckenridge, Colorado.
Cimatti, A., Roveri, M., & Traverso, P. (1998a). Automatic OBDD-based Generation
Universal Plans Non-Deterministic Domains. Proceeding Fifteenth National
Conference Artificial Intelligence (AAAI-98) Madison, Wisconsin. AAAI-Press.
Also IRST-Technical Report 9801-10, Trento, Italy.
Cimatti, A., Roveri, M., & Traverso, P. (1998b). Strong Planning Non-Deterministic
Domains via Model Checking. Proceeding Fourth International Conference
Artificial Intelligence Planning Systems (AIPS-98) Carnegie Mellon University,
Pittsburgh, USA. AAAI-Press.
Clarke, E. M., & Wing, J. M. (1996). Formal methods: State art future directions.
ACM Computing Surveys, 28 (4), 626{643.
Clarke, E., Emerson, E., & Sistla, A. (1986). Automatic verification finite-state concurrent systems using temporal logic specifications. ACM Transactions Programming
Languages Systems, 8 (2), 244{263.
Coudert, O., Madre, J. C., & Touati, H. (1993). TiGeR Version 1.0 User Guide. Digital
Paris Research Lab.
Daniele, M., Traverso, P., & Vardi, M. Y. (1999). Strong Cyclic Planning Revisited.
Biundo, S. (Ed.), Proceeding Fifth European Conference Planning, Lecture
Notes Artificial Intelligence Durham, United Kingdom. Springer-Verlag.
De Giacomo, G., & Vardi, M. (1999). Automata-Theoretic Approach Planning Temporally Extended Goals. Biundo, S. (Ed.), Proceeding Fifth European Conference Planning, Lecture Notes Artificial Intelligence Durham, United Kingdom.
Springer-Verlag.
Ferraris, P., & Giunchiglia, E. (2000). Planning satisfiability nondeterministic domains. Proceedings Seventeenth National Conference Artificial Intelligence
(AAAI'00) Austin, Texas. AAAI Press.
Ghallab, M., Howe, A., Knoblock, C., McDermott, D., Ram, A., Weld, D., & Wilkins,
D. (1998). PDDL | Planning Domain Definition Language. Tech. rep. CVC
TR-98-003/DCS TR-1165, Yale Center Computational Vision Control.
Giunchiglia, E. (1996). Determining Ramifications Situation Calculus. Fifth
International Conference Principles Knowledge Representation Reasoning
(KR'96) Cambridge, Massachusetts. Morgan Kaufmann Publishers.

Giunchiglia, E., Kartha, G. N., & Lifschitz, V. (1997). Representing action: Indeterminacy
ramifications. Artificial Intelligence, 95 (2), 409{438.
336

fiConformant Planning via Symbolic Model Checking
Giunchiglia, E., & Lifschitz, V. (1998). action language based causal explanation:
Preliminary report. Proceedings 15th National Conference Artificial Intelligence (AAAI-98) 10th Conference Innovative Applications Artificial
Intelligence (IAAI-98), pp. 623{630 Menlo Park. AAAI Press.

Goldman, R. P., Musliner, D. J., & Pelican, M. J. (2000). Using Model Checking
Plan Hard Real-Time Controllers. Proceeding AIPS2k Workshop ModelTheoretic Approaches Planning Breckenridge, Colorado.
Goldman, R., & Boddy, M. (1996). Expressive Planning Explicit Knowledge.
Proceedings 3rd International Conference Artificial Intelligence Planning
Systems (AIPS-96), pp. 110{117. AAAI Press.

Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). Spudd: Stochastic planning using decision diagrams. Proceedings Fifteenth Conference Uncertainty
Articial Intelligence (1999), pp. 279{288. AAAI Press.
Kautz, H., & Selman, B. (1998). BLACKBOX: New Approach Application
Theorem Proving Problem Solving. Working notes Workshop Planning
Combinatorial Search Pittsburgh, PA, USA.
Kautz, H. A., McAllester, D., & Selman, B. (1996). Encoding Plans Propositional Logic.
Proc. KR-96.
Kautz, H. A., & Selman, B. (1996). Pushing Envelope: Planning, Propositional Logic,
Stochastic Search. Proc. AAAI-96.
Kushmerick, N., Hanks, S., & Weld, D. S. (1995). algorithm probabilistic planning.
Artificial Intelligence, 76 (1-2), 239{286.
Levesque, H. J. (1996). planning presence sensing?. Proceedings
Thirteenth National Conference Artificial Intelligence Eighth Innovative
Applications Artificial Intelligence Conference, pp. 1139{1146 Menlo Park. AAAI

Press / MIT Press.
McDermott, D. (1987). critique pure reason. Computational Intelligence, 3 (3), 151{
237.
McMillan, K. (1993). Symbolic Model Checking. Kluwer Academic Publ.
Michie, D. (1974). Machine Intelligence Edinburgh. Machine Intelligence, pp.
143{155. Edinburgh University Press.
Nilsson, N. (1980). Principles Artificial Intelligence. Morgan Kaufmann Publishers, Inc.,
Los Altos, CA.
Peot, M. (1998). Decision-Theoretic Planning. Ph.D. thesis, Dept. Engineering-Economic
Systems | Stanford University.
Rintanen, J. (1999a). Constructing conditional plans theorem-prover. Journal
Artificial Intellegence Research, 10, 323{352.
337

fiCimatti & Roveri
Rintanen, J. (1999b). Improvements Evaluation Quantified Boolean Formulae.
Dean, T. (Ed.), 16th Iinternational Joint Conference Artificial Intelligence, pp.
1192{1197. Morgan Kaufmann Publishers.
Smith, D. E., & Weld, D. S. (1998). Conformant graphplan. Proceedings 15th
National Conference Artificial Intelligence (AAAI-98) 10th Conference
Innovative Applications Artificial Intelligence (IAAI-98), pp. 889{896 Menlo

Park. AAAI Press.
Somenzi, F. (1997). CUDD: CU Decision Diagram package | release 2.1.2. Department
Electrical Computer Engineering | University Colorado Boulder.
Weld, D. S., Anderson, C. R., & Smith, D. E. (1998). Extending graphplan handle
uncertainty sensing actions. Proceedings 15th National Conference
Artificial Intelligence (AAAI-98) 10th Conference Innovative Applications Artificial Intelligence (IAAI-98), pp. 897{904 Menlo Park. AAAI Press.

Yang, B., Bryant, R. E., O'Hallaron, D. R., Biere, A., Coudert, O., Janssen, G., Ranjan,
R. K., & Somenzi, F. (1998). performance study BDD-based model checking.
Proceedings Formal Methods Computer-Aided Design, pp. 255{289.

338

fiJournal Artificial Intelligence Research 13 (2000) 1-31

Submitted 8/99; published 8/00

Space Efficiency Propositional Knowledge Representation
Formalisms
Marco Cadoli

cadoli@dis.uniroma1.it

Dipartimento di Informatica e Sistemistica
Universita di Roma La Sapienza
Via Salaria 113, I-00198, Roma, Italy

Francesco M. Donini

donini@dis.uniroma1.it

Politecnico di Bari
Dipartimento di di Elettrotecnica ed Elettronica
Via Orabona 4, I-70125, Bari, Italy

Paolo Liberatore
Marco Schaerf

liberato@dis.uniroma1.it
schaerf@dis.uniroma1.it

Dipartimento di Informatica e Sistemistica
Universita di Roma La Sapienza
Via Salaria 113, I-00198, Roma, Italy

Abstract
investigate space efficiency Propositional Knowledge Representation (PKR)
formalism. Intuitively, space efficiency formalism F representing certain piece
knowledge , size shortest formula F represents . paper
assume knowledge either set propositional interpretations (models) set
propositional formulae (theorems). provide formal way talking relative
ability PKR formalisms compactly represent set models set theorems.
introduce two new compactness measures, corresponding classes, show
relative space efficiency PKR formalism representing models/theorems directly
related classes. particular, consider formalisms nonmonotonic reasoning,
circumscription default logic, well belief revision operators stable
model semantics logic programs negation. One interesting result formalisms
time complexity necessarily belong space efficiency class.

1. Introduction
last years large number formalisms knowledge representation (KR)
proposed literature. formalisms studied several perspectives, including semantical properties, computational complexity. investigate
space efficiency, property minimal size needed represent certain piece knowledge given formalism. study motivated fact
piece knowledge represented two formalisms using different amount
space. Therefore, else remaining same, formalism could preferred another
one needs less space store information.
definition space efficiency, however, simple. Indeed, formalism may allow
several different ways represent piece knowledge. example, let us assume
want represent piece knowledge today Monday. Propositional

c
2000
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiCadoli, Donini, Liberatore, & Schaerf

Logic may decide use single propositional variable monday. fact today
Monday represented formula monday, also formula monday,
well monday (rain rain), formulae Propositional Logic
logically equivalent monday represent exactly information.
Propositional Logic, consider shortest equivalent formulae used
represent information have. principle applied generic
formalism: allows several formulae represent information, take
account shortest one. Therefore, say space efficiency formalism F
representing certain piece knowledge size shortest formula F
represents . Space efficiency also called succinctness compactness formalism
measure ability representing knowledge small amount space.
paper focus propositional KR (PKR) formalisms. give
formal definition formalisms propositional one not: intuitively,
propositional formalism, quantifications allowed, thus formulae
syntactically bounded formed using propositional connectives, plus
kind nonclassical connectives (for instance, negation logic programs, etc.).
far, discussed knowledge represents. possible way think
piece knowledge represents facts inferred it. words,
knowing something knowing everything logically implied.
second way cases natural think piece knowledge
set states world consider possible.
formal way, say knowledge represented either set propositional interpretations (those describing states world consider plausible) set
formulae (those implied know). Consequently, focus reasoning
problems model checking theorem proving. following example shows
really think knowledge ways.
Example 1 want eat fast food, want either sandwich salad
(but both), either water coke (but both).
Propositional Logic, choice represented model, following
models represent possible choices (models represented writing letters
mapped true).
= {{sandwich, water}, {sandwich, coke}, {salad, water}, {salad, coke}}
representing set choices use formulae instead models. case,
write set formulae whose models represent exactly allowed choices,
follows.
C = (sandwich salad) (sandwich salad) (sandwich salad)
(water coke) (water coke) (coke water)

Actually, get rid redundancies, end following formula.
F = (sandwich salad) (sandwich salad) (water coke) (water coke)
2

fiSpace Efficiency Propositional Knowledge Representation Formalisms

formally, F represents set models A, interpretation I,
holds |= F . formula F also represents set formulae C,
Cn(F ) = Cn(C), Cn(.) function gives set conclusions
drawn propositional formula.
1.1 State Art
question deeply investigated, related space efficiency,
possibility translating formula expressed one formalism formula expressed
another formalism (under assumption, course, formulae represent
knowledge).
cases, analysis possibility translating formulae different
formalisms Propositional Logic (PL). example, Ben-Eliyahu Dechter (1991, 1994)
proposed translation default logic PL, translation disjunctive logic
programs PL, Winslett (1989) introduced translation revised knowledge
bases PL, Gelfond, Przymusinska, Przymusinskyi (1989) defined translation
circumscription PL.
translations, well many ones literature, lead
exponential increase size formula, worst case. best known
translation yields formula target formalism exponential size w.r.t.
formula source formalism, natural question arising whether exponential
blow due specific translation, intrinsic problem. example,
although proposed translations default logic PL lead exponential blow
up, cannot conclude possible translations suffer problem: could
polynomial translation exists, discovered far.
works focussed question whether kind exponential increase
size intrinsic not. Cadoli, Donini, Schaerf (1996) shown many interesting fragments default logic circumscription cannot expressed polynomialtime fragments PL without super-polynomially increasing size formulae.
proved super-polynomial increase size necessary translating
unrestricted propositional circumscription (Cadoli, Donini, Schaerf, & Silvestri, 1997)
operators belief revision PL (Cadoli, Donini, Liberatore, & Schaerf, 1999;
Liberatore, 1995).
Gogic collegues (1995) analyzed relative succinctness several PKR formalisms
representing sets models. Among results, showed skeptical default logic
represent sets models succinctly circumscription.
Kautz, Kearns, Selman (1995) Khardon Roth (1996, 1997) considered
representations knowledge bases based notion characteristic model, comparing
representations, e.g., based clauses. showed representation
knowledge bases characteristic models sometimes exponentially compact
ones, converse true cases.
However, results based specific proofs, tailored specific reduction, help us define equivalence classes space efficiency KR
formalisms. recent paper (Cadoli, Donini, Liberatore, & Schaerf, 1996b), new complexity measure decision problems, called compilability, introduced.

3

fiCadoli, Donini, Liberatore, & Schaerf

present paper show new measure directly used characterize space
efficiency PKR formalisms. emphasize methodological aspects, expressing
general context many results presented before.
1.2 Goal
notion polynomial time complexity great importance KR (as well many
fields computer science), problems solved polynomial time
considered easy, computational point view.
notion polynomial many-one reducibility also intuitive meaning
applied KR: exists polynomial many-one reduction one formalism
another one, time complexity reasoning two formalisms comparable.
allows say, e.g., inference PL coNP-complete, i.e. one hardest
problems among complexity class coNP.
result, formal tool comparing difficulty reasoning two
formalisms. missing way saying one formalism able represent
information less space.
Example 2 consider lunch scenario previous example. show
reduce size representation using circumscription instead Propositional
Logic. PL, knowledge previous example represented formula F :
F = (sandwich salad) (sandwich salad) (water coke) (water coke)
set models formula A, models exactly minimal
models formula Fc defined follows.
Fc = (sandwich salad) (water coke)
definition circumscription (McCarthy, 1980) holds F equivalent
CIRC(Fc ; {sandwich, salad, water, coke}, , ). Note Fc shorter F . result
proved hold arbitrary sets models, may conclude circumscription
space efficient Propositional Logic representing knowledge expressed sets
models.
goal provide formal way talking relative ability PKR formalisms compactly represent information, information either set models
set theorems. particular, would like able say specific PKR
formalism provides one compact ways represent models/theorems among
PKR formalisms specific class.
1.3 Results
introduce two new compactness measures (model theorem compactness)
corresponding classes (model-C thm-C, C complexity class like P, NP, coNP,
etc.). classes form two hierarchies isomorphic polynomial-time hierarchy
(Stockmeyer, 1976). show relative space efficiency PKR formalism
4

fiSpace Efficiency Propositional Knowledge Representation Formalisms

directly related classes. particular, ability PKR formalism compactly
represent sets models/theorems directly related class model/theorem
hierarchy belongs to. Problems higher model/theorem hierarchy represent
sets models/theorems compactly formalisms lower classes.
classification obtained general framework making direct
comparisons specific translations various PKR formalisms. Furthermore,
approach also allows simple intuitive notion completeness model
theorem hierarchies. notion precisely characterizes relation
formalisms different levels, relations formalisms level.
interesting result two PKR formalisms model checking inference belong
time complexity class may belong different compactness classes. may
suggest criterion choosing two PKR formalisms reasoning
time complexitynamely, choose compact one. Also, two PKR formalisms
may belong theorem compactness class, yet different model compactness
classes. stresses importance clarifying whether one wants represent models
theorems choosing PKR formalism.
1.4 Outline
next section introduce notation assumptions adopt
work. Section 3 (Compilability) briefly recall notions non-uniform computation important follows recall basic definitions compilability
classes (Cadoli et al., 1996b). Section 4 (Reductions) describe constraints
impose reductions, Section 5 (Space Efficiency) introduce compactness
classes. Section 6 (Applications) actually compare many known PKR formalisms
using framework. Finally, Section 7 (Related Work Conclusions) compare
work proposals presented literature draw conclusions.

2. Notations Assumptions
section define knowledge bases formalisms are. Since want
consider formalisms different syntax semantics, need
general definitions. Let us consider, base case, formalism propositional calculus.
Formally, assume composed three parts:
1. syntax, used define well-formed formulae;
2. proof theory, allows saying formula follows another one;
3. model-theoretic semantics, establishes model satisfies formula.
syntax defined finite alphabet propositional symbols L = {a, b, c, . . .},
possibly subscripts, usual set propositional connectives , , .
terms knowledge representation, proof theory seen way extracting knowledge knowledge base. example, knowledge base c,
fact b holds. thus say formula b part knowledge represented
c.
5

fiCadoli, Donini, Liberatore, & Schaerf

cases, want knowledge bases represent models rather sets formulas.
interpretation alphabet propositional variables L mapping L
{true, false}. model-theoretic semantics propositional calculus usual way
extending interpretation L well-formed formulas.
Let us extend definition generic formalisms: formalism composed
syntax, proof theory, model-theoretic semantics.
remark formalism syntax: instance, default logic includes
ternary connective : denoting default rules, logic programming special
unary connective not(), on. knowledge base formalism F simply wellformed formula, according syntax formalism.
formalism proof theory well. proof theory formalism F
binary relation `F set knowledge bases formulae. Intuitively, F B `F means
consequence knowledge base KB, according rules formalism
F . result, set formulae implied knowledge base KB exactly
knowledge represented KB.
base comparison two different formalisms concept equivalence,
allowing saying two knowledge bases (of two different formalisms) represent
piece knowledge. Since knowledge represented knowledge base set
formulas implies, assume syntax formulae
formalisms. Namely, always assume formulae implied knowledge
base well-formed formulae propositional calculus. words, formalism
syntax knowledge bases: however, assume proof theory relates
knowledge bases (formulae syntax formalism) propositional formulae. So,
writing KB `F , assume KB knowledge base syntax F ,
propositional formula.
allows saying two knowledge bases KB1 KB2 , expressed two different formalisms F1 F2 , represent piece knowledge: true when,
propositional formula holds KB1 `F1 KB2 `F2 .
model-theoreric semantics formalism relation |=F propositional
models knowledge bases. case, assume fixed alphabet L, thus set
interpretations common formalisms. model knowledge base
KB relation, write |=F KB. Intuitively, means model
supports piece knowledge represented KB.
remark formalisms, e.g. credolous default logic (Reiter, 1980),
proof theory, model-theoretic semantics. also possible conceive
formalisms model-theoretic semantics proof theory.
defined, assume related following formula:
KB `F

iff

. |= KB implies |=

Regarding proof theory formalisms, consider formulae shorter
knowledge base, is, assume knowledge represented knowlegde
base KB set formulae KB `F , size size
KB. done two reasons: first, formulas larger KB likely

6

fiSpace Efficiency Propositional Knowledge Representation Formalisms

contain large parts actually independent KB; second, give technicals
result simple way using compilability classes introduced next section.
Assumption 1 consider formulae whose size less equal
knowledge base.
formalisms consider satisfy right-hand side distruibutivity conjunction,
is, KB `F KB `F KB `F . assumption size
restrictive case, CNF formula.

3. Compilability Classes
assume reader familiar basic complexity classes, P, NP (uniform)
classes polynomial hierarchy (Stockmeyer, 1976; Garey & Johnson, 1979).
briefly introduce non-uniform classes (Johnson, 1990). sequel, C, C0 , etc. denote
arbitrary classes polynomial hierarchy.
assume input instances problems strings built alphabet .
denote empty string assume alphabet contains special
symbol # denote blanks. length string x denoted |x|.
Definition 1 advice function takes integer returns string.
Advices important complexity theory definitions results often
based special Turing machines determine result oracle free,
is, constant time.
Definition 2 advice-taking Turing machine Turing machine enhanced
possibility determine A(|x|) constant time, x input string.
course, fact A(|x|) determined constant time (while
intractable even undecidable function) makes definitions based advice-taking
Turing machine different ones based regular Turing machine. example,
advice-taking Turing machine calculate polynomial time many functions
regular Turing machine cannot (including untractable ones).
Note advice function size input, input itself.
Hence, advice-taking Turing machines closely related non-uniform families circuits
(Boppana & Sipser, 1990). Clearly, advice allowed access whole instance,
would able determine solution problem constant time.
Definition 3 advice-taking Turing machine uses polynomial advice exists
polynomial p advice oracle satisfies |A(n)| p(n) nonnegative
integers n.
non-uniform complexity classes based advice-taking Turing machines.
paper consider simplified definition, based classes polynomial hierarchy.

7

fiCadoli, Donini, Liberatore, & Schaerf

Definition 4 C class polynomial hierarchy, C/poly class languages defined Turing machines time bounds C, augmented polynomial advice.
class C/poly also known non-uniform C, non-uniformity due
presence advice. Non-uniform uniform complexity classes related: Karp
Lipton (1980) proved NP P/poly p2 = p2 = PH, i.e., polynomial hierarchy collapses second level, Yap (1983) generalized results, particular
showing NP coNP/poly p3 = p3 = PH, i.e., polynomial hierarchy
collapses third level. inprovement results given Kobler
Watanabe (1998): proved kp pk /poly implies polynomial hierarchy collapses ZPP(pk+1 ). collapse polynomial hierarchy considered
unlikely researchers structural complexity.
summarize definitions results proposed formalize compilability
problems (Cadoli et al., 1996b), adapting context terminology PKR
formalisms. remark aim paper give formalization
compilability problems, analyze problems point view. Rather, show
use compilability classes technical tool proving results relative
efficiency formalisms representing knowledge little space.
Several papers literature focus problem reducing complexity
problems via preprocessing phase (Kautz & Selman, 1992; Kautz et al., 1995; Khardon
& Roth, 1997). motivates introduction measure complexity problems
assuming preprocessing allowed. Following intuition knowledge base
known well questions posed it, divide reasoning problem two parts:
one part fixed accessible off-line (the knowledge base), second one varying,
accessible on-line (the interpretation/formula). Compilability aims capturing on-line
complexity solving problem composed inputs, i.e., complexity respect
second input first one preprocessed arbitrary way. next
section show close connection compilability space efficiency PKR
formalisms.
function f called poly-size exists polynomial p strings
x holds |f (x)| p(|x|). exception definition x represents number:
case, impose |f (x)| p(x). result, say function used
advice-taking turing machine polysize function.
function g called poly-time exists polynomial q x, g(x)
computed time less equal q(|x|). definitions easily extend
binary functions usual.
define language pairs subset . necessary represent
two inputs PKR reasoning problem, i.e., knowledge base (KB), formula
interpretation. example, problem Inference Propositional Logic (pli)
defined follows.
pli = {hx, yi | x set propositional formulae (the KB), formula, x ` y}
well known pli coNP-complete, i.e., one hardest problems
among belonging coNP. goal prove pli hardest theorem8

fiSpace Efficiency Propositional Knowledge Representation Formalisms

proving problem among coNP solved preprocessing first input
arbitrary way, i.e., KB. end, introduce new hierarchy classes,
non-uniform compilability classes, denoted k;C, C generic uniform complexity
class, P, NP, coNP, p2 .
Definition 5 (k;C classes) language pairs belongs k;C iff
exists binary poly-size function f language pairs 0 C hx, yi
holds:
hf (x, |y|), yi 0 iff hx, yi
Notice poly-size function f takes input x (the KB) size
(either formula interpretation). done technical reason, is,
assumption allows obtaining results impossible prove function f
takes x input (Cadoli et al., 1996b). assuption useful proving negative results,
is, theorems impossibility compilation: indeed, impossible reduce
complexity problem using function takes x |y| input,
reduction also impossible using function taking x argument.
Theorem 1 (Cadoli, Donini, Liberatore, & Schaerf, 1997, Theorem 6) Let C
class polynomial hierarchy . problem belongs k;C
exists poly-size function f language pairs 0 ,
hx, yi holds that:
1. |y| k, hf (x, k), yi 0 hx, yi S;
2. 0 C.
Clearly, problem whose time complexity C also k;C: take f (x, |y|) = x
0 = S. interesting problem C may belong k;C0
C0 C, e.g.,, problems NP k;P. true example problems
belief revision (Cadoli et al., 1999). rest paper, however, mainly focus
complete problems, defined below. pictorial representation class k;C
Figure 1, assume 0 C.
problem pli method proving belongs k;P known. order
show (probably) belong k;P, define notion reduction
completeness.
Definition 6 (Non-uniform comp-reducibility) Given two problems B,
non-uniformly comp-reducible B (denoted nucomp B) iff exist two poly-size
binary functions f1 f2 , polynomial-time binary function g every
pair hx, yi holds hx, yi hf1 (x, |y|), g(f2 (x, |y|), y)i B.
nucomp reductions represented depicted Figure 2. reductions
satisfy important properties reduction.
Theorem 2 (Cadoli et al., 1996b, Theorem 5) reductions nucomp satisfy transitivity compatible (Johnson, 1990) class k;C every complexity class C.
9

fiCadoli, Donini, Liberatore, & Schaerf

f
!
!1

x

| | ! |y|


6

hx, yi
-

S0

-

Figure 1: representation k;C.
x
|y|
||


- f1
- f2

6

-x

?

g
-

-y

0

0

Figure 2: nu-comp-C reductions.

Therefore, possible define notions hardness completeness k;C
every complexity class C.
Definition 7 (k;C-completeness) Let language pairs C complexity class.
k;C-hard iff problems k;C nucomp S. Moreover,
k;C-complete k;C k;C-hard.
right complexity class completely characterize problem pli.
fact pli k;coNP-complete (Cadoli et al., 1996b, Theorem 7). Furthermore, hierarchy
formed compilability classes proper polynomial hierarchy
proper (Cadoli et al., 1996b; Karp & Lipton, 1980; Yap, 1983) fact widely conjectured
true.
Informally, may say k;NP-hard problems compilable P,
considerations know exists preprocessing fixed part
makes on-line solvable polynomial time, polynomial hierarchy collapses.
holds k;coNP-hard problems. general, problem k;C-complete
class C regarded toughest problem C, even arbitrary preprocessing fixed part. hand, problem k;C problem that,
preprocessing fixed part, becomes problem C (i.e., compilable C).
close section giving another example use compilability classes
well-known formalism Circumscription (McCarthy, 1980). Let x propositional
formula. minimal models x truth assignments satisfying x
positive values possible (w.r.t. set containment). problem consider is: check
whether given model minimal model propositional formula. problem, called
Minimal Model checking (mmc), reformulated problem model checking
Circumscription, known co-NP-complete (Cadoli, 1992).
10

fiSpace Efficiency Propositional Knowledge Representation Formalisms

consider knowledge base x given off-line, truth assignment
given on-line, obtain following definition:
mmc = {hx, yi | minimal model x }
problem shown k;coNP-complete (Cadoli et al., 1996b, Theorem 13).
Hence, unlikely k;P; is, unlikely exists
off-line processing knowledge base, yielding (say) data structure x0 ,
given y, checked polynomial time whether minimal model x.
This, course, unless x0 exponential size. observation applies also x0
knowledge base Propositional Logic, led interpretation Circumscription
compact, succint, PL (Cadoli, Donini, & Schaerf, 1995; Gogic et al., 1995).
framework allows generalize results PKR formalisms, shown
sequel.

4. Reductions among KR Formalisms
define forms reduction PKR formalisms analyze
following sections. formula always represented string alphabet ,
hence consider translations functions transforming strings.
Let F1 F2 two PKR formalisms. exists poly-size reduction F1
F2 , denoted f : F1 7 F2 , f poly-size function given knowledge
base KB F1 , f (KB) knowledge base F2 . Clearly, reductions restricted
produce meaningful output. particular, discuss reductions preserve
models original theory.
semantic approach Gogic collegues (1995) models two
knowledge bases must exactly same. words, knowledge base KB
formalism F1 translated knowledge base KB 0 formalism F2 , |=F1 KB
|=F2 KB 0 . approach summarized by: reduction
formalisms F1 F2 way translate knowledge bases F1 knowledge bases F2 ,
preserving sets models. semantics intuitively grounded, easy
show examples two formalisms consider equally space-efficient cannot
translated other. Let us consider instance variant propositional calculus
syntax formulas must form x1 F , F regular
formula variables x2 , . . .. Clearly, formalism able represent knowledge
space propositional calculus (apart polynomial factor). However,
according definition, formalism cannot translated propositional calculus:
knowledge base equivalent KB = x1 . Indeed, model
KB , model consistent knowledge base modified propositional
calculus contains x1 .
propose general approach deal also functions f change
language KB. end, allow translation gKB models KB
models f (KB). stress that, general possible, translation may depend
KB i.e., different knowledge bases may different translations models.
want translation easy compute, since otherwise computation gKB could
hide complexity reasoning formalism. However, observe end,
11

fiCadoli, Donini, Liberatore, & Schaerf

sufficient impose gKB computable polynomial time. fact, KB
fixed, models could trivially translated models f (KB) constant time, using
lookup table. table would exponentially large, though; want
forbid. Hence, impose gKB circuit polynomial-size wrt KB. still use
functional notation gKB (M ) denote result applying model circuit
gKB . formal definition follows.
Definition 8 (Model Preservation) poly-size reduction f : F1 7 F2 satisfies modelpreservation exists polynomial p that, knowledge base KB F1
exists circuit gKB whose size bounded p(|KB|), every interpretation
variables KB holds |=F1 KB iff gKB (M ) |=F2 f (KB).
rationale model-preserving reduction knowledge base KB first
formalism F1 converted knowledge base f (KB) second one F2 ,
reduction model F1 easily translated model
gKB (M ) F2 .
require g depend KB, transformation f , general, could take
actual form KB account. happens following example modelpreserving translation.
Example 3 reduce fragment skeptical default logic (Kautz & Selman, 1991)
circumscription varying letters, using transformation introduced Etherington
(1987). Let hD, W prerequisite-free normal (PFN) default theory, i.e., defaults
form : , generic formula. Let Z set letters occurring
hD, W i. Define PD set letters {a | : D}. function f defined
following way: f (hD, W i) = CIRC(T ; PD ; Z), = W {a |a PD },
PD letters minimized, Z (the set letters occurring hD, W i)
varying letters. show f model-preserving poly-size reduction. fact, given
set PFN defaults let gD function interpretation Z,
gD (M ) = {a PD |M |= }. Clearly, f poly-size, gD realized circuit
whose size polynomial |D|, model least one extension hD, W iff
gD (M ) |= CIRC(T ; PD ; Z). dependence g stresses fact that,
case, circuit g depend whole knowledge base hD, W i, D.
Clearly, models preserved, theorems preserved well. weaker form
reduction following one, theorems preserved. Also case
allow theorems KB translated simple circuit gKB theorems KB.
Definition 9 (Theorem Preservation) poly-size reduction f : F1 7 F2 satisfies theorempreservation exists polynomial p that, knowledge base KB F1 ,
exists circuit gKB whose size bounded p(|KB|), every formula
variables KB, holds KB `F1 iff f (KB) `F2 gKB ().
theorem-preserving reduction property similar model-preserving
reduction, knowledge bases used represent theorems rather models.
Namely, knowledge base KB translated another knowledge base f (KB)
12

fiSpace Efficiency Propositional Knowledge Representation Formalisms

used represent set theorems. precisely, theorem
KB represented theorem gKB () f (KB).
Winslett (1989) shown example reduction updated knowledge bases
PL theorem-preserving model-preserving. Using Winsletts reduction, one
could use machinery propositional reasoning KB,
update (plus reduction). Also reduction shown previous Example 3
theorem-preserving, time g identity circuit.
remark definitions reduction general proposed
Gogic collegues (1995). fact, authors consider notion analogous
Definition 8. case g identity i.e., models two formalisms
identical. allowing simple translation g models Definition 8 covers
general forms reductions preserving models, like one Example 3.

5. Comparing Space Efficiency PKR Formalisms
section show use compilability classes defined Section 3 compare
succinctness PKR formalisms.
Let F1 F2 two formalisms representing sets models. prove
knowledge base F1 reduced, via poly-size reduction, knowledge base
F2 satisfying model-preservation compilability class problem
model checking (first input: KB, second input: interpretation) F2 higher equal
compilability class problem model checking F1 .
Similarly, prove theorem-preserving poly-size reductions exist
compilability class problem inference (first input: KB, second input: formula, cf.
definition problem pli) F1 higher equal compilability class
problem inference F2 .
order simplify presentation proof theorems introduce
definitions.
Definition 10 (Model hardness/completeness) Let F PKR formalism C
complexity class. problem model checking F belongs compilability class
k;C, model varying part instances, say F model-C.
Similarly, model checking k;C-complete (hard), say F model-C-complete
(hard).
Definition 11 (Theorem hardness/completeness) Let F PKR formalism
C complexity class. problem inference formalism F belongs
compilability class k;C, whenever formula varying part instance, say
F thm-C. Similarly, inference k;C-complete (hard), say F thmC-complete (hard).
definitions implicitly define two hierarchies, parallel polynomial hierarchy (Stockmeyer, 1976): model hierarchy (model-P,model-NP,model-p2 ,etc.)
theorem hierarchy (thm-P,thm-NP,thm-2p ,etc.). higher formalism model
hierarchy, efficiency representing models analogously theorems.
example (Cadoli et al., 1996, Thm. 6), characterize model theorem classes
Propositional Logic.
13

fiCadoli, Donini, Liberatore, & Schaerf

Theorem 3 PL model-P thm-coNP-complete.
formally establish connection succinctness representations
compilability classes. following theorems, complexity classes C, C1 , C2 belong
polynomial hierarchy (Stockmeyer, 1976). Theorems 5 7 assume
polynomial hierarchy collapse.
start showing existence model-preserving reductions formalism
another one easily obtained levels model hierarchy satisfy simple
condition.
Theorem 4 Let F1 F2 two PKR formalisms. F1 model-C F2 modelC-hard, exists poly-size reduction f : F1 7 F2 satisfying model preservation.
Proof. Recall since F1 model-C, model checking F1 k;C, since F2
model-C-hard, model checking F1 non-uniformly comp-reducible model checking
F2 . is, (adapting Def. 6) exist two poly-size binary functions f1 f2 ,
polynomial-time binary function g every pair hKB, holds
|=F1 KB g(f2 (KB, |M |), ) |=F2 f1 (KB, |M |)
(note g poly-time function appearing Def. 6, different gKB
poly-size circuit appearing Def. 8).
observe |M | computed KB simply counting letters appearing KB; let f3 counting function, i.e., |M | = f3 (KB). Clearly, f3 poly-size.
Define reduction f f (KB) = f1 (KB, f3 (KB)). Since poly-size functions closed
composition, f poly-size. show f model-preserving reduction.
Definition 8, need prove exists polynomial p knowledge
base KB F1 , exists poly-size circuit gKB every interpretation
variables KB holds |=F1 KB iff gKB (M ) |=F2 f (KB).
proceed follows: Given KB F1 , compute z = f2 (KB, |M |) = f2 (KB, f3 (KB)).
Since f2 f3 poly-size, z size polynomial respect |KB|. Define circuit
gKB (M ) one computing g(z, ) = g(f2 (KB, f3 (KB)), ). Since g poly-time
function inputs, z poly-size KB, exists representation g(z, )
circuit gKB whose size polynomial wrt KB. construction, |=F1 KB iff
gKB (M ) |=F2 f (KB). Hence, thesis follows.
following theorem, instead, gives simple method prove modelpreserving reduction one formalism another one.

Theorem 5 Let F1 F2 two PKR formalisms. polynomial hierarchy
collapse, F1 model-C1 -hard, F2 model-C2 , C2 C1 , poly-size
reduction f : F1 7 F2 satisfying model preservation.
Proof. show reduction exists, C1 /poly C2 /poly implies
polynomial hierarchy collapses level (Yap, 1983). Let complete problem
class C1 e.g., C1 p3 may validity -quantified boolean formulae
(Stockmeyer, 1976). Define problem follows.
= {hx, yi | x = (the empty string) A}
14

fiSpace Efficiency Propositional Knowledge Representation Formalisms

already proved (Cadoli et al., 1996b, Thm. 6) k;C1 -complete. Since model
checking F1 model-C1 -hard, non-uniformly comp-reducible model checking
F1 . is, (adapting Def. 6) exist two poly-size binary functions f1 f2 ,
polynomial-time binary function g every pair h, yi, holds h, yi
g(f2 (, |y|), y) |=F1 f1 (, |y|). Let |y| = n. Clearly, knowledge base f1 (, |y|)
depends n, i.e., exactly one knowledge base integer. Call KBn .
Moreover, f2 (, |y|) = f2 (, n) also depends n only: call (for Oracle). Observe
KBn polynomial size respect n.
exists poly-size reduction f : F1 7 F2 satisfying model preservation, given
knowledge base KBn exists poly-size circuit hn g(On , y) |=F1 KBn
hn (g(On , y)) |=F2 f (KBn ).
Therefore, k;C1 -complete problem non-uniformly reduced problem
k;C2 follows: Given y, size |y| = n one obtains (with preprocessing)
f (KBn ) . one checks whether interpretation hn (g(On , y)) (computable
polynomial time given n, ) model F2 f (KBn ). fact model
checking F2 k;C2 , k;C1 k;C2 . proved previous paper
result implies C1 /poly C2 /poly (Cadoli et al., 1996b, Thm. 9),
turns implies polynomial hierarchy collapses (Yap, 1983).
theorems show hierarchy classes model-C exactly characterizes
space efficiency formalism representing sets models. fact, two formalisms
level model hierarchy reduced via poly-size
reduction (Theorem 4), poly-size reduction formalism (F1 ) higher
hierarchy one (F2 ) lower class (Theorem 5). latter case say
F1 space-efficient F2 .
Analogous results (with similar proofs) hold poly-size reductions preserving theorems.
Namely, next theorem shows infer existence theorem-preserving reductions,
one gives way prove theorem-preserving reduction
one formalism another one.
Theorem 6 Let F1 F2 two PKR formalisms. F1 thm-C F2 thm-Chard, exists poly-size reduction f : F1 7 F2 satisfying theorem preservation.
Proof. Recall since F1 thm-C, inference F1 k;C, since F2 thm-Chard, inference F1 non-uniformly comp-reducible inference F2 . is, (adapting
Def. 6) exist two poly-size binary functions f1 f2 , polynomial-time binary
function g1 every pair hKB, holds
KB `F1 f1 (KB, ||) `F2 g(f2 (KB, ||), )
(here distinguish poly-time function g appearing Def. 6 poly-size circuit
gKB appearing Def. 9).
Using Theorem 1 replace || upper bound formula.
Assumption 1, know size less equal size KB; therefore
replace || |KB|. formula becomes
KB `F1 f1 (KB, |KB|) `F2 g(f2 (KB, |KB|), )
15

fiCadoli, Donini, Liberatore, & Schaerf

Define reduction f f (KB) = f1 (KB, f3 (KB)), f3 poly-size function
computes size input. Since poly-size functions closed composition,
f poly-size.
Now, show f theorem-preserving reduction, i.e.,f satisfies Def. 9.
amounts proving knowledge base KB F1 exists circuit gKB ,
whose size poynomial wrt KB, every formula variables KB
holds KB `F1 iff f (KB) `F2 gKB ().
proceed proof Theorem 4: Given KB F1 , let z = f2 (KB, f3 (KB)).
Since f2 f3 poly-size, z polynomial size respect |KB|. Define gKB () =
g(z, ) = g(f2 (KB, f3 (KB)), ). Clearly, gKB represented circuit polynomial
size wrt KB. construction, KB `F1 iff f (KB) `F2 gKB (). Hence, claim
follows.
Theorem 7 Let F1 F2 two PKR formalisms. polynomial hierarchy
collapse, F1 thm-C1 -hard, F2 thm-C2 , C2 C1 , poly-size
reduction f : F1 7 F2 satisfying theorem preservation.
Proof. show reduction exists, C1 /poly C2 /poly polynomial
hierarchy collapses level (Yap, 1983). Let complete problem class C1 .
Define problem proof Theorem 5: problem k;C1 -complete (Cadoli
et al., 1996b, Thm. 6). Since inference F1 thm-C1 -hard, non-uniformly compreducible inference F1 . is, (adapting Def. 6) exist two poly-size binary
functions f1 f2 , polynomial-time binary function g every pair h, yi,
h, yi f1 (, |y|) `F1 g(f2 (, |y|), y). Let |y| = n. Clearly, knowledge
base f1 (, |y|) depends n, i.e., one knowledge base integer. Call
KBn . Moreover, also f2 (, |y|) = f2 (, n) depends n: call (for Oracle).
Observe KBn polynomial size respect n.
exists poly-size reduction f : F1 7 F2 satisfying theorem preservation,
given knowledge base KBn exists poly-time function hn KBn `F1
g(On , y) f (KBn ) `F2 hn (g(On , y)).
Therefore, k;C1 -complete problem non-uniformly reduced problem
k;C2 follows: Given y, size |y| = n one obtains (with arbitrary preprocessing)
f (KBn ) . one checks whether formula hn (g(On , y)) (computable polytime given ) theorem F2 f (KBn ). fact inference F2
k;C2 , k;C1 k;C2 . follows C1 /poly C2 /poly (Cadoli et al.,
1996b, Thm. 9), implies polynomial hierarchy collapses (Yap, 1983).
Theorems 4-7 show compilability classes characterize precisely relative
capability PKR formalisms represent sets models sets theorems. example,
consequence Theorems 3 7 poly-size reduction PL
syntactic restriction PL allowing Horn clauses preserves theorems, unless
polynomial hierarchy collapses. Kautz Selman (1992) proved non-existence
reduction problem strictly related pli using specific proof.

16

fiSpace Efficiency Propositional Knowledge Representation Formalisms

6. Applications
section devoted application theorems presented previous section.
Using Theorems 4-7 results previously known literature, able asses
model- theorem-compactness PKR formalisms.
assume definitions Propositional Logic, default logic (Reiter, 1980),
circumscription (McCarthy, 1980) known. Definitions WIDTIO, SBR, GCWA,
stable model semantics appropriate subsections.
following proofs refer problem 3QBF, is, problem verifying
whether quantified Boolean formula XY.F valid, X disjoint sets
variables, F set clauses alphabet X , composed three literals.
example, simple formula belonging class is: x1 , x2 y1 , y2 ((x1 y2 )
(x1 x2 y1 ) (y1 x2 y2 ) (x1 x2 )).
problem deciding validity 3QBF complete class p2 . consequence, corresponding problem 3QBF, deciding whether input composed
string () fixed part quantified Boolean formula XY.F varying
one, complete class k;2p (Liberatore, 1998). Notice hardness
proofs show sequel use problems without meaningful fixed part.
6.1 Stable Model Semantics
Stable model semantics (SM) introduced Gelfond Lifschitz (1988) tool
provide semantics logic programs negation. original proposal one
standard semantics logic programs. recall definition propositional
stable model.
Let P propositional, general logic program. Let subset (i.e., interpretation) atoms P . Let P program obtained P following way:
clause C P contains body negated atom C deleted;
body clause contains negated atom 6 deleted
body clause. least Herbrand model P stable model
P.
formalism sm, consider program P knowledge base. write
P |=sm Q denote query Q implied logic program P Stable Model
semantics.
order prove result, need define kernel graph.
Definition 12 (Kernel) Let G = (V, E) graph. kernel G set K V
that, denoting H = V K, holds:
1. H vertex cover G
2. j H, exists K (i, j) E.
state theorem compilability class inference stable model
semantics, corresponding theorem compactness class.
Theorem 8 problem inference Stable Model semantics k;coNP-complete,
thus Stable Model Semantics thm-coNPcomplete.
17

fiCadoli, Donini, Liberatore, & Schaerf

Proof. Membership class follows fact problem coNP-complete
(Marek & Truszczynski, 1991). hardness, adapt proof Marek
Truszczynski (1991) showing deciding whether query true stable models
coNP-hard.
Let kernel language {, G} G graph least one kernel.
Let |G| = n, observe G cannot vertices size n.
show n, exists logic program Pn every graph G
n vertices, exists query QG G kernel iff Pn 6|=sm QG .
Let alphabet Pn composed following 2n2 + n propositional letters:
{ai |i {1..n} } {rij , sij |i, j {1..n} }.
program Pn defined as:
aj
sij
rij

:
:
:



ai , rij

rij
i, j {1..n}


sij

Given graph G = (V, E), query QG defined
QG = (

_

(i,j)E

rij ) (

_

rij )

(i,j)6E

reduction kernel sm defined as: f1 (x, n) = Pn , i.e., f1 depends
second argument, f2 (x, n) = , i.e., f2 constant function, g = Qy , i.e., given
graph G, circuit g computes query QG .
result, k; reduction. show reduction correct, i.e.,
h, Gi kernel (G kernel) iff Pn 6|=SM QG .
If-part. Suppose Pn 6|=SM QG . Then, exists stable model Pn
|= QG . Observe QG equivalent conjunction rij (i, j) E,
rij (i, j) 6 E. Simplifying Pn QG obtain clauses:
aj : ai , (i, j) E

(1)

Observe contains sij (i, j) 6 E, order stable, i.e.,
support atoms rij (i, j) E contains atom sij (i, j) E.
Let H = {j|aj }, K = {i|ai 6 }. H vertex cover G, since
edge (i, j) E, satisfy corresponding clause (1) aj : ai , hence either
ai , aj . Moreover, j H, atom aj , since
stable model, exists clause aj : ai ai 6 , is, K. Therefore,
K kernel G.
Only-if part. Suppose G = (V, E) kernel K, let H = V K. Let
interpretation
= {rij |(i, j) E} {sij |(i, j) 6 E} {aj |j H}

Obviously, 6|= QG . show stable model Pn , i.e., least
Herbrand model PnM . fact, PnM contains following clauses:
sij
rij
aj

: rij

(i, j) 6 E

(2)

K

(4)

(i, j) E

18

(3)

fiSpace Efficiency Propositional Knowledge Representation Formalisms

Clauses last line obtained clauses Pn form aj : ai , rij ,
clauses H (hence ai ) deleted, clauses negated
atom ai deleted, since K, hence ai 6 . aj , vertex j H,
hence edge (i, j) E, K. Hence clauses (4) (3) PnM , hence
least Herbrand model PnM exactly aj j H.
6.2 Minimal Model Reasoning
One successful form non-monotonic reasoning based selection
minimal models. Among various formalisms based minimal model semantics consider Circumscription (McCarthy, 1980) Generalized Closed World Assumption
(GCWA) (Minker, 1982), formalism represent knowledge closed world.
assume reader familiar Circumscription, briefly present
definition GCWA. model semantics GCWA defined (a letter):
|=GCW KB iff |= KB{a | positive clause , KB 6` KB 6` a}
present results two formalisms.
Theorem 9 problem model checking Circumscription k;coNP-complete, thus
Circumscription model-coNP-complete.
result trivial corollary theorem already proved (Cadoli et al., 1997, Theorem 6). fact, proof implicitly shows model checking circumscription
k;coNP-complete.
Theorem 10 problem model checking GCWA k;P, thus GCWA
model-P.
Proof. already pointed (Cadoli et al., 1997), possible rewrite GCW A(T )
propositional formula F that, given model , |= GCW A(T )
|= F . Moreover, size F polynomially bounded size .
consequence, model compactness GCWA class PL. Theorem 3
thesis follows.
Theorem 11 problem inference Circumscription k;p2 -complete, thus Circumscription thm-p2 -complete.
result trivial corollary theorem published previous paper (Cadoli
et al., 1997, Theorem 7) implicitly shows inference circumscription k;2p complete.
Theorem 12 problem inference GCWA k;coNP-complete, thus GCWA
thm-coNP-complete.
Proof. already pointed proof Theorem 10, possible rewrite
GCW A(T ) formula F equivalent it. consequence, formula
theorem GCW A(T ) theorem F . Thus, GCWA
theorem compexity PL. Since GCWA generalization PL, follows GCWA
theorem compactness class PL. Hence, GCWA thm-coNP-complete.
19

fiCadoli, Donini, Liberatore, & Schaerf

6.3 Default Logic
subsection present results default logic, two variants (credulous
skeptical). details two main variants default logic, refer
reader paper Kautz Selman (1991). Notice model-compactness
applicable skeptical default logic.
Theorem 13 problem model checking skeptical default logic k;p2 complete,
thus skeptical default logic model-p2 complete.
Proof. proof membership straightforward: since model checking skeptical
default logic p2 (Liberatore & Schaerf, 1998), follows also k;p2 .
proof k;p2 -hardness similar proof p2 -hardness (Liberatore & Schaerf,
1998). reduction problem 3QBF. Let h, instance 3QBF,
= XY.F represents valid 3QBF formula, string.
Let n size formula F . implies variables formula
n. Let = {1 , . . . , k } set clauses three literals alphabet.
number clauses three literals alphabet n variables less O(n3 ),
thus bounded polynomial n.
prove XY.F valid model extension hW, Di,

=

W

=

[ : ci



ci

: ci
,
ci

= {ci | F }







[ : w (w xi ) : w (w xi )

,

w xi

xi X

w xi



(

:w

V

ci

w



set {ci | 1 k} set new variables, one-to-one elements .
Note W depends size n F , depends F . result,
nucomp reduction.
prove formula valid model extension
default theory hW, Di. similar already published proof (Liberatore &
Schaerf, 1998). Consider evaluation C1 variables {ci } evaluation X1
variables X. Let D0 following set defaults.
0

=

[ : ci [ : ci

ci C1

ci

ci 6C1

ci



[ : w (w xi )

w xi

xi X1

[

xi X1



: w (w xi )
w xi



set defaults chosen set R consequences corresponds
sets C1 X1 . Namely, have:
ci C1 iff R |= ci

ci 6 C1 iff R |= ci

xi X1 iff R |= w xi

xi 6 X1 iff R |= w xi
20

)

fiSpace Efficiency Propositional Knowledge Representation Formalisms

Now, prove consequences set defaults extension
default theory QBF formula valid. Since defaults semi-normal,
prove that:
1. set consequences D0 consistent;
2. default applicable, is, default whose precondition
consistent R.
Consistency R follows construction: assigning ci true ci C1 , etc.,
obtain model R.
:ci
prove default applicable. ci C1 , default c

:ci
applicable, vice versa, ci C1 , ci applicable. Moreover, none

i)
, applicable xi 6 X1 , case w xi R, thus
defaults :w(wx
wxi
w would follow (while w justification default). similar statement holds
:w(wxi )
xi X1 .
wxi

V

:w

ci


result, applicable default may last one,
(recall
w
F negated). default applicable if, given evaluation ci
xi s, set clauses satisfiable. amount say: extension
last default applicable QBF formula valid. Now,
last default applicable, model extension w
consequence last default w 6|= . converse also holds: last default
applicable model default theory.
result, QBF valid model given default theory.

Theorem 14 inference problem skeptical default logic k;p2 complete, thus skeptical default logic thm-2p complete.
Proof. Since inference skeptical default logic p2 , also k;p2 . k;p2 -hardness
comes simple reduction circumscription. Indeed, circumscription
formula equivalent conjunction extensions default theory hT, Di,
(Etherington, 1987):
D=

[ : xi

xi

result, CIRC(T ) |= Q Q implied hT, Di skeptical semantics. Since hT, Di depends (and Q) nucomp reduction. Since
inference circumscription k;2p -complete (see Theorem 11), follows skeptical
default logic k;2p -hard.
Theorem 15 inference problem credulous default logic k;p2 complete, thus
credulous default logic thm-p2 complete.

21

fiCadoli, Donini, Liberatore, & Schaerf

Proof. proof similar proof model checking skeptical default logic.
Indeed, problems k;p2 complete. Since problem p2 , proved Gottlob
(1992), also k;p2 . Thus, prove hard class.
prove 3QBF problem reduced problem verifying whether
formula implied extensions default theory (that is, inference credulous
default logic).
Namely, formula XY.F valid Q derived extension
default theory hD, W i, W defined follows ( set
clauses three literals alphabet F , C set new variables, one-to-one
).
W

=

=

[ : ci

ci

ci C

Q =

^

F

ci

: ci
,
ci

^

6F





[ : xi

xi X

xi

: xi
,
xi





(

V

(

ci C ci

w

) :

)

ci w

Informally, proof goes follows: truth evaluation variables C
X set defaults justified consistent. simple necessary
sufficient condition consequences set defaults extension
following. If, evaluation, formula


^



ci =true

valid, last default applicable, thus extension also contains w. converse
also holds: formula valid evaluation, variable w
extension.
result, exists extension Q holds exists
extension ci true F , w also holds.
variables ci given value, formula equivalent F . result,
extension exists exists truth evaluation variables X
F valid.
6.4 Belief Revision
Many formalisms belief revision proposed literature, focus
two them: WIDTIO (When Doubt Throw Out) SBR (Skeptical Belief Revision).
Let K set propositional formulae, representing agents knowledge world.
new formula added K, problem possible inconsistency K
arises. first step define set sets formulae W (K, A) following
way:
W (K, A) = {K 0 K 0 maximal consistent subset K {A} containing }
22

fiSpace Efficiency Propositional Knowledge Representation Formalisms

set formulae K 0 W (K, A) maximal choice formulae K
consistent and, therefore, may retain incorporating A. definition
set leads two different revision operators: SBR WIDTIO.
SBR Skeptical Belief Revision (Fagin, Ullman, & Vardi, 1983; Ginsberg, 1986). revised
.
theory defined set theories: K = {K 0 | K 0 W (K, A)}. Inference
revised theory defined inference theories:
K `SBR Q iff

K 0 W (K, A) , K 0 ` Q

model semantics defined as:
|=SBR K iff

exists K 0 W (K, A) |= K 0

WIDTIO Doubt Throw (Winslett, 1990). simpler (but somewhat
drastical) approach so-called WIDTIO, retain formulae K
belong sets W (K, A). Thus, inference defined as:
K `W IDT IO Q iff



W (K, A) ` Q

model semantics formalism defined as:
|=W IDT IO K

iff

|=

\

W (K, A)

results model compactness shown Liberatore Schaerf (2000).
recall them.
Theorem 16 (Liberatore & Schaerf, 2000, Theorem 11) problem model checking WIDTIO k;P, thus WIDTIO model-P.
Theorem 17 (Liberatore & Schaerf, 2000, Theorem 5) problem model checking Skeptical Belief Revision k;coNP-complete, thus Skeptical Belief Revision
model-coNP-complete.
results theorem compactness quite simple provide proofs.
Theorem 18 problem inference WIDTIO k;coNP-complete, thus WIDTIO
thm-coNP-complete.
Proof. Membership class thm-coNP immediately follows definition. fact,
rewrite K propositional formula computing set W (K, A)
constructing intersection. construction intersection size less equal
size K A. consequence, preprocessing, deciding whether formula Q
follows K problem coNP. Hardness follows obvious fact PL
reduced WIDTIO PL thm-coNP-complete (see Theorem 3).
Theorem 19 problem inference Skeptical Belief Revision k;p2 -complete,
thus Skeptical Belief Revision thm-p2 -complete.
23

fiCadoli, Donini, Liberatore, & Schaerf

Propositional
Logic
WIDTIO
Skeptical
Belief Revision
Circumscription
GCWA

Skeptical
Default Reasoning
Credulous
Default Reasoning
Stable Model
Semantics

Time Complexity
P

p2 -complete
(Liberatore & Schaerf, 1996)
coNP-complete
(Liberatore & Schaerf, 1996)
coNP-complete
(Cadoli, 1992)
coNP-hard,
p2 [log n]
(Eiter & Gottlob, 1993)
p2 -complete
(Liberatore & Schaerf, 1998)
N/A

Space Efficiency
model-P

model-P
Th. 16
model-coNP-complete
Th. 17
model-coNP-complete
Th. 9
model-P
Th. 10

P


model-P


model-p2 -complete
Th. 13
N/A

Table 1: Complexity model checking Space Efficiency Model Representations
Proof. Membership follows complexity results Eiter Gottlob (1992),
show deciding whether K `SBR Q p2 -complete problem. Hardness
follows easily Theorem 17. fact, |=SBR K iff K 6`SBR f orm(M ),
f orm(M ) formula represents model . consequence, model checking
reduced complement inference. Thus inference k;p2 -complete.
6.5 Discussion
Tables 1 2 summarize results space efficiency PKR formalisms
proved (a dash denotes folklore result).
First all, notice space efficiency always related time complexity.
example, compare detail WIDTIO circumscription. table follows
model checking harder WIDTIO circumscription, inference
complexity cases. Nevertheless, since circumscription thm-p2 -complete
WIDTIO thm-coNP-complete (and thus thm-p2 ), exists poly-size reduction
WIDTIO circumscription satisfying theorem preservation. converse
hold: since circumscription thm-2p -complete WIDTIO thm-coNP, unless Polynomial Hierarchy collapse theorem-preserving poly-size reduction
former formalism latter. Hence, circumscription compact formalism
WIDTIO represent theorems. Analogous considerations done models.
Intuitively, due fact WIDTIO model checking inference
require lot work revised knowledge base alonecomputing intersection

24

fiSpace Efficiency Propositional Knowledge Representation Formalisms

Propositional
Logic
WIDTIO
Skeptical
Belief Revision
Circumscription
GCWA
Skeptical
Default Reasoning
Credulous
Default Reasoning
Stable Model
Semantics

Time Complexity
coNP-complete
(Cook, 1971)
p2 -complete
(Eiter & Gottlob, 1992) & (Nebel, 1998)
p2 -complete
(Eiter & Gottlob, 1992)
p2 -complete
(Eiter & Gottlob, 1993)
p2 -complete
(Eiter & Gottlob, 1993) & (Nebel, 1998)
p2 -complete
(Gottlob, 1992)
p2 -complete
(Gottlob, 1992)
coNP-complete
(Marek & Truszczynski, 1991)

Space Efficiency
thm-coNP-complete
(Cadoli et al., 1996)
thm-coNP-complete
Th. 18
thm-p2 -complete
Th. 19
thm-p2 -complete
Th. 11
thm-coNP-complete
Th. 12
thm-p2 -complete
Th. 14
thm-p2 -complete
Th. 15
thm-coNP-complete
Th. 8

Table 2: Complexity inference Space Efficiency Theorem Representations
elements W (K, A). done, one left model checking inference
PL. Hence, WIDTIO space efficiency PL, circumscription.
Figures 3 4 contain information Tables 1 2, highlight existing reductions. figure contains two diagrams, left one showing existence
polynomial-time reductions among formalisms, right one showing existence polysize reductions. arrow formalism another denotes former
reduced latter one. use bidirectional arrow denote arrows directions
dashed box enclose formalisms reduced one another. Note
formalisms appropriate representing sets models, others perform
better sets formulae. interesting relation exists skeptical default reasoning
circumscription. model-preserving poly-size reduction circumscription skeptical default reasoning (Gogic et al., 1995), theorem-preserving poly-size
reduction exists, shown Theorem 14.

7. Related Work Conclusions
idea comparing compactness KR formalisms representing information
novel AI. well known first-order circumscription represented secondorder logic (Schlipf, 1987). Kolaitis Papadimitriou (1990) discuss several computational
aspects circumscription. Among many interesting results show reduction
restricted form first-order circumscription first-order logic. proposed reduction
increase size original formula exponential factor. left open
problem show whether increase intrinsic, different compactness
properties two formalisms, exists space-efficient reduction.

25

fiCadoli, Donini, Liberatore, & Schaerf

- Skeptical

WIDTIO

Default

Skeptical Default

6

6

GCWA
6

SBR - Circumscription

SBR - Circumscription

.

6

6

PL - Stable Model

PL - WIDTIO

a. Time Complexity

- GCWA - Stable
Model

b. Space Efficiency

Figure 3: Complexity Model Checking vs. Space Efficiency Model Representation

WIDTIO - GCWA

6

?
Skeptical
SBR - Circum - Default



PL -

Credulous
Default



SBR - Circum-



Skeptical
Default

AK


Stable
Model

Credulous
Default






Stable
PL- WIDTIO - GCWA - Model

a. Time complexity

b. Space efficiency

Figure 4: Complexity Inference vs. Space Efficiency Theorem Representation

26

fiSpace Efficiency Propositional Knowledge Representation Formalisms

first-order language used, results compactness existence reductions
reported Schlipf (1995).
Khardon Roth (1996, 1997), Kautz, Kearns Selman (1995) propose modelbased representations KB Propositional Logic, compare formula-based
representations. Although results significant comparing representations within
PL, refer formalism, hence applicable comparison different PKR formalisms. comment applies also idea representing
KB efficient basis Moses Tennenholz (1996), since refers one
PKR formalism, namely, PL.
active area research studies connections various non-monotonic logics.
particular, several papers discussing existence translations polynomial time satisfy intuitive requirements modularity faithfulness.
Janhunen (1998), improving results Imielinski (1987) Gottlob (1995), shows
default logic expressive, among non-monotonic logics examined, since
circumscription autoepistemic logic modularly faithfully embedded default logic, way around. results interest help
fully understand relation among many knowledge representation formalisms,
directly related ours. fact, allow translations general
polynomial time, papers consider translations use
polynomial time also satisfy additional requirements.
first result compactness representations propositional language presented, best knowledge, Kautz Selman (1992). show that, unless
collapse polynomial hierarchy, size smallest representation
least Horn upper bound propositional theory superpolynomial size
original theory. results also presented different form comprehensive paper (Selman & Kautz, 1996). technique used proof
used us researchers prove several results relative complexity
propositional knowledge representation formalisms (Cadoli et al., 1996, 1997, 1999; Gogic
et al., 1995).
recent paper (Cadoli et al., 1996b) introduced new complexity measure, i.e.,
compilability. paper shown measure inherently related
succinctness PKR formalisms. analyzed PKR formalisms respect two succinctness measures: succinctness representing sets models succinctness representing
sets theorems.
main advantage framework machinery necessary formal way
talking relative ability PKR formalisms compactly represent information.
particular, able formalize intuition specific PKR formalism provides
one compact ways represent models/theorems among PKR formalisms
specific class.
opinion, proposed framework improves state art two different
aspects:
1. proofs presented previous papers compare pairs PKR formalisms, example propositional circumscription Propositional Logic (Cadoli
et al., 1997). results allow precise classification level
27

fiCadoli, Donini, Liberatore, & Schaerf

compactness considered formalisms. Rephrasing adapting results
framework allows us infer circumscription model-coNP-complete
thm-p2 -complete. consequence, also space-efficient
WIDTIO belief revision formalism representing sets models sets theorems.
2. Using proposed framework possible find criteria adapting existent
polynomial reductions showing C-hardness reductions show model-C thmC-hardness, C class polynomial hierarchy (Liberatore, 1998).

Acknowledgments
paper extended revised version paper authors appeared
proceedings fifth international conference principles knowledge representation reasoning (KR96) (Cadoli, Donini, Liberatore, & Schaerf, 1996a). Partial
supported given ASI (Italian Space Agency) CNR (National Research
Council Italy).

References
Ben-Eliyahu, R., & Dechter, R. (1991). Default logic, propositional logic constraints.
Proceedings Ninth National Conference Artificial Intelligence (AAAI91),
pp. 379385.
Ben-Eliyahu, R., & Dechter, R. (1994). Propositional semantics disjunctive logic programs. Annals Mathematics Artificial Intelligence, 12, 5387.
Boppana, R., & Sipser, M. (1990). complexity finite functions. van Leeuwen, J.
(Ed.), Handbook Theoretical Computer Science, Vol. A, chap. 14. Elsevier Science
Publishers (North-Holland), Amsterdam.
Cadoli, M. (1992). complexity model checking circumscriptive formulae. Information Processing Letters, 44, 113118.
Cadoli, M., Donini, F., Liberatore, P., & Schaerf, M. (1996a). Comparing space efficiency
propositional knowledge representation formalisms. Proceedings Fifth International Conference Principles Knowledge Representation Reasoning
(KR96), pp. 364373.
Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1996b). Feasibility unfeasibility off-line processing. Proceedings Fourth Israeli Symposium Theory
Computing Systems (ISTCS96), pp. 100109. IEEE Computer Society Press.
url = ftp://ftp.dis.uniroma1.it/PUB/AI/papers/cado-etal-96.ps.gz.
Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1997).
Preprocessing intractable problems.
Tech. rep. DIS 24-97, Dipartimento di
url =
Informatica e Sistemistica, Universita di Roma La Sapienza.
http://ftp.dis.uniroma1.it/PUB/AI/papers/cado-etal-97-d-REVISED.ps.gz.
28

fiSpace Efficiency Propositional Knowledge Representation Formalisms

Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1999). size revised
knowledge base. Artificial Intelligence, 115 (1), 2564.
Cadoli, M., Donini, F. M., & Schaerf, M. (1995). compact representations propositional circumscription. Proceedings Twelfth Symposium Theoretical Aspects Computer Science (STACS95), pp. 205216. Extended version RAP.14.95
DIS, Univ. Roma La Sapienza, July 1995.
Cadoli, M., Donini, F. M., & Schaerf, M. (1996). intractability non-monotonic reasoning
real drawback?. Artificial Intelligence, 88 (12), 215251.
Cadoli, M., Donini, F. M., Schaerf, M., & Silvestri, R. (1997). compact representations
propositional circumscription. Theoretical Computer Science, 182, 183202.
Cook, S. A. (1971). complexity theorem-proving procedures. Proceedings
Third ACM Symposium Theory Computing (STOC71), pp. 151158.
Eiter, T., & Gottlob, G. (1992). complexity propositional knowledge base revision,
updates counterfactuals. Artificial Intelligence, 57, 227270.
Eiter, T., & Gottlob, G. (1993). Propositional circumscription extended closed world
reasoning 2p -complete. Theoretical Computer Science, 114, 231245.
Etherington, D. V. (1987). Reasoning incomplete information. Morgan Kaufmann,
Los Altos, Los Altos, CA.
Fagin, R., Ullman, J. D., & Vardi, M. Y. (1983). semantics updates databases.
Proceedings Second ACM SIGACT SIGMOD Symposium Principles
Database Systems (PODS83), pp. 352365.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide
Theory NP-Completeness. W.H. Freeman Company, San Francisco, Ca.
Gelfond, M., & Lifschitz, V. (1988). stable model semantics logic programming.
Proceedings Fifth Logic Programming Symposium, pp. 10701080. MIT
Press.
Gelfond, M., Przymusinska, H., & Przymusinsky, T. (1989). relationship
circumscription negation failure. Artificial Intelligence, 38, 4973.
Ginsberg, M. L. (1986). Conterfactuals. Artificial Intelligence, 30, 3579.
Gogic, G., Kautz, H. A., Papadimitriou, C., & Selman, B. (1995). comparative linguistics knowledge representation. Proceedings Fourteenth International
Joint Conference Artificial Intelligence (IJCAI95), pp. 862869.
Gottlob, G. (1992). Complexity results nonmonotonic logics. Journal Logic
Computation, 2, 397425.
Gottlob, G. (1995). Translating default logic standard autoepistemic logic. Journal
ACM, 42, 711740.
29

fiCadoli, Donini, Liberatore, & Schaerf

Imielinski, T. (1987). Results translating defaults circumscription. Artificial Intelligence, 32, 131146.
Janhunen, T. (1998). intertranslatability autoepistemic, default priority
logics, parallel circumscription. Proceedings Sixth European Workshop
Logics Artificial Intelligence (JELIA98), No. 1489 Lecture Notes Artificial
Intelligence, pp. 216232. Springer-Verlag.
Johnson, D. S. (1990). catalog complexity classes. van Leeuwen, J. (Ed.), Handbook
Theoretical Computer Science, Vol. A, chap. 2. Elsevier Science Publishers (NorthHolland), Amsterdam.
Karp, R. M., & Lipton, R. J. (1980). connections non-uniform uniform
complexity classes. Proceedings Twelfth ACM Symposium Theory
Computing (STOC80), pp. 302309.
Kautz, H. A., Kearns, M. J., & Selman, B. (1995). Horn approximations empirical data.
Artificial Intelligence, 74, 129145.
Kautz, H. A., & Selman, B. (1991). Hard problems simple default logics. Artificial
Intelligence, 49, 243279.
Kautz, H. A., & Selman, B. (1992). Forming concepts fast inference. Proceedings
Tenth National Conference Artificial Intelligence (AAAI92), pp. 786793.
Khardon, R., & Roth, D. (1996). Reasoning models. Artificial Intelligence, 87, 187
213.
Khardon, R., & Roth, D. (1997). Defaults relevance model-based reasoning. Artificial
Intelligence, 97, 169193.
Kobler, J., & Watanabe, O. (1998). New collapse consequences NP small circuits.
SIAM Journal Computing, 28 (1), 311324.
Kolaitis, P. G., & Papadimitriou, C. H. (1990). computational aspects circumscription. Journal ACM, 37 (1), 114.
Liberatore, P. (1995). Compact representation revision Horn clauses. Yao, X. (Ed.),
Proceedings Eighth Australian Joint Artificial Intelligence Conference (AI95),
pp. 347354. World Scientific.
Liberatore, P. (1998). Compilation intractable problems application artificial
intelligence.
Ph.D.
thesis,
Dipartimento di Informatica e Sistemistica, Universita di Roma La Sapienza. URL =
ftp://ftp.dis.uniroma1.it/pub/AI/papers/libe-98-c.ps.gz.
Liberatore, P., & Schaerf, M. (1996). complexity model checking belief revision update. Proceedings Thirteenth National Conference Artificial
Intelligence (AAAI96), pp. 556561.

30

fiSpace Efficiency Propositional Knowledge Representation Formalisms

Liberatore, P., & Schaerf, M. (1998). complexity model checking propositional
default logics. Proceedings Thirteenth European Conference Artificial
Intelligence (ECAI98), pp. 1822.
Liberatore, P., & Schaerf, M. (2000). compactness belief revision update operators. Tech. rep., Dipartimento di Informatica e Sistemistica, Universita di Roma La
Sapienza.
Marek, W., & Truszczynski, M. (1991). Autoepistemic logic. Journal ACM, 38 (3),
588619.
McCarthy, J. (1980). Circumscription - form non-monotonic reasoning. Artificial
Intelligence, 13, 2739.
Minker, J. (1982). indefinite databases closed world assumption. Proceedings
Sixth International Conference Automated Deduction (CADE82), pp. 292
308.
Moses, Y., & Tennenholtz, M. (1996). Off-line reasoning on-line efficiency: knowledge
bases. Artificial Intelligence, 83, 229239.
Nebel, B. (1998). hard revise belief base?. Dubois, D., & Prade, H. (Eds.),
Belief Change - Handbook Defeasible Reasoning Uncertainty Management Systems, Vol. 3. Kluwer Academic.
Reiter, R. (1980). logic default reasoning. Artificial Intelligence, 13, 81132.
Schlipf, J. S. (1987). Decidability definability circumscription. Annals Pure
Applied Logic, 35, 173191.
Schlipf, J. S. (1995). survey complexity undecidability results logic programming. Annals Mathematics Artificial Intelligence, 15, 257288.
Selman, B., & Kautz, H. A. (1996). Knowledge compilation theory approximation.
Journal ACM, 43, 193224.
Stockmeyer, L. J. (1976). polynomial-time hierarchy. Theoretical Computer Science,
3, 122.
Winslett, M. (1989). Sometimes updates circumscription. Proceedings Eleventh
International Joint Conference Artificial Intelligence (IJCAI89), pp. 859863.
Winslett, M. (1990). Updating Logical Databases. Cambridge University Press.
Yap, C. K. (1983). consequences non-uniform conditions uniform classes. Theoretical Computer Science, 26, 287300.

31

fi ff
fiff ff


! #"$ % ff'&)(+*-,/.0001324+5(+4*

6789: ;)(ff<ff00!=>8
%&;?2@<ff00

ACBD7EGF-HID@JLKMACNOJLPIQRD@HTSUACVWSXKYQB

Z\[]$^_]a`cb-dfe$gihjek^

lkmknpokmkqrWs$tvuwxq3npyzwxqpsi{3|_wx}Xty

~L7Oz'@@_ffY@+ff-@ _z'ff
~LXffffkff'X!Y
Ofifi'RWRfi! $


cj/L+77+ff)7i+++Yf!/+O
+xYj)ffTxz 3k7Y+c+7i7Yi+)7iff/++LYxk
ff+xi+xYz7xffx)?!+7++ficfici+7fiz7fix:x!+@
/ff++v+c7xzO+ X+xz7fix+$+@
/ff/fi:Ic?'Y
k#ffzx7++ vfixX7!ix+Y7ffp/pvx:z$i+
jx@ +++c+/7:/ffc+:!Tx:fi/?)7xcz7
+zff+ffOk+czx/#fiff ffi+Lx)/z7p
iff/W+Wff vcx7x!pcT7+z7+)++3
'++z7+xx7:+?@++xiff+Lfix!p+W/zX+)+x+7pLc
7@W/--+)T@@z@O+++xO7-7xz+
iff@i+/! z O+xYf! 7_ x+x7fx+@
x+?+T+x7+ff+ff++i!+O+7+ v_7?
fiffR+ +@@z++xi+Y:fffx7+L+7xz7?77!v!
z7+ffff@@iX77i+k77xzxcv:Lx!+cff+ff
+-)+x+7ff
Oi :-X
T@ ff
fivpv
@ffvp

Rff
!#"$fi%fi&'fi()'%fi*,+-fi./vp

v 012345fi6v7
fi
*
8@ff4v,Rfi09fi(W@ff6pfffi(04%fi':;<ff430=v>?pff
@ fi 099+ff40Av?pff90ff:B'fi:;ffC4ff!6DffpE9
()34?fiA+F?04<'fi7R()fi
ff4@ffG'.1ff!IHT?pffvff*JKpffpL
3MfiN%fi0 ()fi@ff4O
E
3P'
ffQ?0L+F44
p

fi3
R
3 @ ffR1S?3Pfi !CT$p
? @ ()fi@ *Jv 0fi G'fiU30/v pC Rfi 7+F?G04 0*V
4c
ff 26'fiS4fi 0 P?3fi 7'fiU ff W Rfi/()SL04 N'fiSL'fiG3
3 M()fi@ ff


30P'
ff!XT$p
? 'fi/3
4fi Zfi([ ff pM*,?Mfi\+ ff*Afi()#P. ff?pKfi ]^R
ff?ff/fi
0`_a
[?0p
V'fi[@ ff2
! ( T$p
? fifi(M?4J@ ff ff
E?72V'fiF E(bQ?p9R
ff?ff/fiJfi(0
!,ffc a4
34d*fi3MAfi @ ff
L4-'fip
fi
fffi
e
v $O?fi1-()fi5p
Pv 0v p>+fp
? ?p
?pAR
ff?ff/fiOfi(av ff v p[fi <@ ffPv g+F4?v L?3-%fi3
3 0Jfi(a@ ff'R
ff
`k
_ ff7
fi 'v Ahi
2 ff
vffkl6()$ ff pM!gTF?4,
43p
ff< ()1v 3>?-fiR
ff9@ [@ ff ffC()fi-v p
j fiR
4 mfi YC
+ ff4Q E(b/v pZ?fi0[fiR
vff@ ff ffn(bfiK834fi
'/'ffP$ +F?4
E?Kfi pNfifPfi@ Lfi [Pd 0!
o/P0 Gfi(7XfiR
pq4
[4fi\a]^KrOCstff+u'h [4fi\a*IvffwxylE!zT$?4C4ff+>*F+F?2
?
?74'fiYR
W3 ffm1
{ ff4W 0R|<'}fffi 0fh~vffwwlE*<'ff8?8Wfi 8Pd
Mfi7?E

~IEd\LQ^5^M~^~E9~B^\>5pE^\p9;ff\d^\[^d9'EI\i;ffB7B'\\~
idiB~XdB'\^;^^^b82ff tdB)~t5~-pB~ffBE~59^'B^;


.000j %%a!; +L ff;A?@ ff9 ff! x9 ff,8!
%&%;

&@%$%2@ ;\

fi

mknpokmkq

?13PYfiQ4fi\+?30PY'fi
fiO8'fiC?EK!5T$?p7vU
fi'03fiKfi(<{ff4SG|<'}fffit4
j8
4<'fiUP]>
()fi@
+ P@ ff ff@ P3M'fiMfifi37fi 8v 'fiK@ ff`:B+-fi4
14fi
*V
+
pff'fiO

@ ffa Z
fi03Mfi 04'
0v CO
ff 0>fi(F.1v 3Y?pfffiR

f2fid]^
rO'7sJff+>!!!^?Mfid+MfiY
+ ''fiZfi3M7()
>(bfi
3 pU3L?0v m?3fi/
ff>fi(-fiR
/v p
fi3MQfip
EEffk[(9
fi3M *?2f4d+4['fi
fi pg()fiL@ ff
[40v ffO
4fi G 0 pff0f'fiKR

fiR
fi 2} ff v 'fi'R
ff
`_
Ffi
ff-ff'0 Nfi KI'/'ffK*30
?K j
H Fp
ff L30 ff]^
_a !^kGT$?2V0R
t0M@ ff fft
{ ff4I 8|<'}fffi 0]^t
2'fiQPOv >?p-
fi o/gfi(a Afi ff!
Ttfi7@ fffi 0'fi7?3f
40'fi8*
+ f f+-fi./v pL'fi\+- j [4fi\12 k>0 [fi */+F?2
?
+ I3

k
_ p>'fi R
fi Q?N
()*% SC@ ff'fi 00 e
NP 3pd*+fp
? ?p

.

3 :;p
k
_ pffmfi
ffL@ P0@ ff ffR()Ifi !
3
E?Rfi 7+F42_
ff4?p7
fi 'v
?pffO4fi 8
?Mfi10*fig 0g?pffP ff ffOv 73
E?8f+Aff7g'fiQ@ ff@ 9?3ff 90fiR
ff!
T$3
? `_a
fi =O
?Mfi/3O
ff=p
? @ *IId 7/)*N
fi 4'Kfi(80342v pW&k
_
fi/p
ffVfi(9'/'ff G
p
? ff
.1v 3+F3
? ?pf?37p
ff4@ ffYfi
;Y?Mfi40Fv U?[Pfi1p
ffB!5+c S?p

fi o1Qfi(6?4F0R
ff*0fi/p
fft
p
? ff
.1v 3Cp
EPv pffF+Fp
? ?pNn =(bfi[04 Sq SfiR
p
*M! !*/+Fp
? ?3-a4 j Pfi1p
ff2kChi4~$
_ ffEl<fiR
p!<T$3
? ffi3M'a3M-2 ff4?p j ffkIfi j Mfik
*a( j fiM*^kPfi p7fiffi@ 7
fi3

3 o/a ffF@ 7fi\143
ff!,Sfi1p
fft
p
? ff
.1 p?$fid S'fiR

\

Rff
()fiN( p:;
E4
O042
fi k
* ! M!*VCPfi1p
fft
p
? ff
. >3
3
fi\ @ fffi444
4''fi3c
fi7v WU'/'ffp
ff pffR'fiSP. C0342v p[fi@ ff?13. ff4'!KT$?4
fiA+-fi34 ?d
Q$
3 ffp
? ffCI''3
3(bfi
Q'fiI+-fi ff?13. Q/fi 0*E?pA?
R
?pffhi|,4 ff2M
f!*VvffwwlE!
|9 44*fi/p
ff1
p
? ff
.1 pL4g3MA()fi
5 ff
E??Mfip
3 ?8?p5 6fi(41@ ff
?00 $'ff6fi(
?p502 7'fiL
Ep
? ff
.(0?p5fiR
p7?Mfi40!Jc(0?p5a4 I?6[k
_ ?13>R
Ofi(%'ff*?04gfi/
ff
P ff!6Ufi/p
ffM
Ep
? ff
./v p4fi0MfiR
vffOfi(N834fi <02 ?64O
5
fia o/4;8?
4Y
o/%fi p2<v m?p137
Nfi(5fi ! * {?RY2@ 30>R
Nfi(Afi *O?2N4N
fi34R

U@ fi3N0fi0 ffK!+c W()
*j ZPfi1p
ff,
p
? ff
.1v 3SUv p Pfi 7a4 m+F?RS?3
3 30>R

fi($'ffI
Z

fi03Mfi 04mfi?40
! @ ffIpff<fi(5@ ff@ ff
?Xv ?3 `_
fi

fiP83
3 p82<
3M@ 7(bfi/
3 fffi @ ff3
4fi Iff
3
? 2p
3 ff<(bfi,?0 4v 3f 4 F'f0
ff
hBA4. G{v pM*NvffwwlE!Y
pYfi(N?pS4@ ff'C'1ffPPfi1p
ffF
p
? ff
. ff'fiZG3v p?3ff
@ ff30

fi Gff
3
? 2p
3 ffQ?Xvy (+.0 'ffhiA3ME

? N!*<vffwwlE!7T
H ?pffv ff*?p024
0444;
fi(IP fi(>?pff@ @ ff3
fi ff
E3
? 41p
3 ff 4 @ ff'4
ff ( +@
fi0 ff4n3M'fiff!
r3M?pfi@ p
* Mfi 3[fi(J?pffz@ Q4fi@ ffC()fi?
e
Aff `_a
fi ()F ff 0v p7?0-4@ ff
?p1'ffY! fiO

?Mfi10Lv m?p23M@ @ p
ff pffR(bfi>fi(bp+-@ ?>
E? pff!KY
p


?Iff0?4} ff
e
0
*<>fi3ME>M
fi ff*O4 fi.fi2'.R 0 fi4./]^Ch~vffwwlE!Sffid
+
Mfi p
fi(O?pffK*v 0
43v 3 fi.fi4.K fi./]^5O
?Mfi/*0@ >0044
0v f'fi832fi F1'ff$v
+F?2
?>v p 5fi 9
fi34*?3@ Ev pN?pL4fi0MR
ff?ff/fi<fi(%?pFfid 4a'/'ffK!
ffc

fi 'E'*0fi3Mffi
E?YM@ ff ff5?pL2O
ff4v pff9fi(O4 >7304fi $'1ffP!
-fi 04p
C?Mfid+@ _a
fi q_0 v 'fifi3MCfid 2[0 mfi K()O
+-fi.!ffc ?4
()O
+-fi.Shi NrO 3M@ 8vdlE*/?p@ N@ [fi pffi5fi [fi A+F? j 14O
ffk802 fhB[ ( '
"F *PvffwwlE*L! !*Q04 C?Y R
fi v1344
ff
3Mff=v n@ fffi 0 m'fi#v 7 N
o/
?
1fi
-
fi 0fi ff!m|9
?&fi ]^P02 Z4I3O
ff'fim
v ?pC(bfiEfi(f
_k4 :;'3M'fiP'fihir NlE!$r fK?0ff R
?MfidT
+ 'fiXR
ff
4 @ @ ff fi fi(
ffa%p[B<E^5'^'>V<'p`<~'OBp[B<\E\B^ EdB'\^;^<EFB^5~
da%<i;B~O^L5-BEE~g\ELB<`B5'Q[;d^\$B\9%B~E>\BffdtE0i;B~g^NB<^dff\E
EE~VE\<~9d~B^EQ;


fi[

pI
B

"
!
%
&

QrOrgsJcH[|$


*


04

t}-m{t/s$q



os0pt{a

l qM


ff ERb0bigd0a B;
( ff
fi f
ff
fi 1`)ff1Va
$#Y
1` ffffJa`
p/ A` ; '
E 0 (

QHfsJcH[|$
fi]) ^Fa4

/

)

,+.- )Kff ( 1 d0V`


)

p+04



*

10E '2
ff
fi3 ~ff4 1 iff1Ja
5~
& Ei/ # 'ffE $#U 1` ffffJa`
6 8
7 /ff P0)/ffL4)YE~B9'p/
B; ~EMffE 0 (

rO3@Pv;::`_0N7fi!

@ff
GfiC040=<''ffGhiA3M./?*Qvffww>@?BAL0M}ff/*Nvffwwx@?>AOfffG.fid/
?V*
vffww C@?arfi ff*tvffww ClE!
F35R
#
v +F4?K'ZvL Krgv 3M@ Pv!,T$3
? @ L@ >[ff'$P
fi3Ma Nfi(O+-d1F?$?p>r
04 0f
fi32UR
8(bfi
ffG 444!FrMfiLfi p* ?13P G04 Gp
ff 3[
fi32
p pQ?pv 4
04 0!-TF?4$PdK@ ff13@ >
fi 04p
c fifU
. Mfi\+F ffp
L
! U0
ff2v pP 0 74F'fi
fi
hi! !*O ff R3v p fi43M4fi fi?0PElf?pCv 4g02 Lv RS4834ff
/fi /:

!,rMfi ff,h~vffww ClAfi3M4v pffA8fi/
ff3M F()fi)

fi1 pr f-?$4
Rff
N()fi$
137R
-fi(
fia ffP*Mv
23v p8 YffU fi Yfi(t?pED<4fi pff]^5DQ4 ffP/!
[3P n04 p pCfi
fi43Mfi fi4?PC
p
fffi04 0?Y4'()q
]^O
fi
fi4I'fiRm? ?p
@ !Wffid
+ ff*-'fimfid/4p
''2
7Rff?d1fiEX
3E ff*-()fiP
`_
fi q44'fiR@ ff13 ff!T$3
? @ (bfi Y+ G3
Y?0fi'fiRk
_ ff4v p?3Ufi ff*$?p
hi83blfi ta4 L?tR
L `k
_ ffL3fi Fv 3,'fif3
Pv p,+Fp
? ?pOJ2~$
_ ffg
4
fiR
ff
hi'0L 0G
>lE!8c( Mfi*?pa4 S2Q@ 0@ ffXhi'RlE
! D,2 0N4Y
MfiNM@ ff ffmv G?4
0R
ff*4?Mfip
3 ?P94< P4Pfi6'fi02
5(bfi9()33M@ 5@ ff ffE
?! a,Q?Mfi3
3 ?PL@ ff13 $'fiO


4_a
fi !c($?p@ K4Imv 3 fi *9?348?Ifi 3 r 04 X ?4I4-?4
`k
_ ffW W@ 0@ ffV*t?
( pffp
ff!K
{ C
4<?048
(1fiH=I,J !RhiT$?2
Mfifi V*g7+ ff2,7fi?p
Mfi
fi C3 ffv ?pf0R
ff*49v
430p
ffv ?pTfi8fi
( fR

7! lOc(?3@ f@ [7300
9?0-
fi1fiR
fi
*/+ [
fi 4p
9;+-fi7%fi0444ff!ffc
( K
fi z
* Pfi 53 ff,?pQO

834fi F02 *+F?2
?Y4f j fi130
kfi(6?p7 /43fi f02 !5T$?45832fi f04
4Q()fiO
ffR 0m `$
_ ffm'fiU@ (94Q4~$
_ ffc
4fi067304fi L
fifi0v fi mfi
ff!IT$p
?
834fi 7a4 Z2>@ a@ ff(5 `_a
4fi Wfi130
ff7 fi*6! !*<(i443M Pfi($?pC04 Z'fi
4(bKP0fiR
;!-+c
'2
ff
fi $
* ff
E?mfi Qvp
R
3
3 ffF4$fidT
+ Uv 0123a04 !
Ttfi8 E(b
fi0fiR
ffff*fi pQfi(J?3Qfi 5. ff5?3ffi130
9fi(V?3ff Nv /430/04 9'fi
()fi C834fi F02 !FT$?2$7304fi f02 U4F+F?Q4$ $
_ ff!FrMfi>

K
fi *fi p8fi
fi@ 7v /430M04 5 L@ 0 ffK(J?p>fiR
; 4)
Mfif4~$
_ ffV!
f(bN?3I 4a4
JhiElF?ff R
`$
_ ffm G@ a@ ff*?pfi >@ 8$
_ ff4p
ff![{?4
_$ff43
ffh)fi 4v p\lE*9?pUfi 00Zv ff v p#h ! M!)
* fi43fi XfiR
'fiElI'fiR?pffPa4 JhiEl

3ffp
ffhi'xlE!
ff 0v pRPd&
K ff34@ ffX'fiZ0?pYa4 &'fiZ?0 Y3
3 3 o1R
ff
ff
ML

fi

mknpokmkq

30fiQfi>'fiU_kp :B333?pP04V!cp($ (1fiNHOI fiI ( ff
fi *J?3vpvUhi734blfi>04
4K0ff!cp(P
'2
ff
fi *N fi UY4 fid+Wr 7*F()Y+F?2
??pR834fi
h)fi/3
Elfa4 4N :B()fiO
ff!Prfi74643fi *Jfi pfi >?pE04Yff`_$ffN?3p+
hi83blfi 04 Z'fi3M@ '42,4'$
_ ffI?pC@ ff30@ ffZfiR
ffKhi'0P
CG #lE!WW
" :
()fiPfi fi(8?pR834fi K04 0@ `_a
4fi @ R@ ff13 ff'fiq
K2O
: e

Q%fia >R
ff
3 8?pG@ R
(bfiEO
ffUfi 04v p*R
?0[v ? ?K2O
:;
4
O43fi !
{p
? p $h)@ \lp `_
fi >(i44ff*Jfi/3
ffJ[
fi3
3 @ oM0 9?g2J3 ff7'fi34p
<?39
?fi4

fi(O U 0 L ff pIfi
E'fiFfi$fi?pf04 @ a$L
pffp
ffZhi'8
QlE!,TF?4Afi/
ff5fi(

ff
3MvpM*00vpM*M K ()1v 3702 5
1
v ff$v p

_ ffPL
3ffp
ff!,T$3
? Qv (bfi/
35fi(
?4Aa
f45'0R
CP S!
"f04N@ E`_a
fi 7(bO ff p$4JF. N'fif
E? /v p$4
ffNfi g@ff'%fi ffff!OQ3Mtfi p:
U
fit2$'fi oMPv 374Vv ff v pP
?Mfi10F G4%fi$fiR
pK
4 ffF'fiCp
EPv p
?p134
. ff'L@ E`_a
fi R
?Mfi1m()fi
ff
E?Z
fi70v 4fi fi(-v ff v pKO
?fi1R 0fiR
p

4ff!qffc #?4Ia
+ S@ ff p+ ff3?0
v q3 (i3$ ff 0v pmfiR
'fi
4fi
3 ff&'fiRR
j ( ffkW+F?@ ff'R
ff
'fiW2%fi
4 fffi([fiR
ff!ffc Xfi?p
+-fi*,4($?p fiR
pZ?Mfi40>()fi?pK04 Z4fi8'fiRv ff v pM*O?p&4I2I
30 ff#'fiR'44
?Mfi4&()v ff v pMT! c([ #fi 30 ffI?pff Uv ff v pmfiR
'fiff*-+f449
30 ffq'fi
@ ff@ I?pfiR
fff+F?GUWVff `_a
4fi m@ ff30@ ff*%! !*J'a9
C?fip
3 X
? QCv mrO 3M@ Kv
pff
fi<R)
ff
3MffV!<T$?464<?p$R
ff'6fipF
fi304P?MfiR
5()fi,v Pfi 04v p530fi +Fp
? 504
@ff'%fi @ C2O
U4
44
!#rMfifi?p ff 0v pmfiR
'fi 0&fiR
pZ
4@ ffPfi3Mm0fi
@ff30f 3 !LFfi\+ d*V(bfiQ?p
ffN U+F?04
?S+ I?d p I@ ff34*a
+ I@ ff
Mfi\
ffZ
UW[K \Yff U']_^a`5 `_a
fi fi4?P!YT$3
? ff
?fi18fi/
4} ?p@ E`_a
fi v
fip
,'fi7d F4O
Ffid -'fi0@ E`_a
fi ()fi

?V! 4
{ Q4'fiN0@ ff ,Mfi\ ff0 fiE?
()fiT
e
v K :B(bfiv p7344fi [04 V*(bfi[?p83fi hB
'2
ff
fi l$v Y+F?04
?U?p@
@ >8340 Qfi k
* ff
?S ff v 3I p
R
p
!
T$3
? TMfi\ ff;Pfi(Jfi3M90fi
? 4 Mfi-P
E?v pFv ff v p>fi9 _a
fi
- */03M<E?p
?pW
?pff4 fi(8?pRp+9fiM! T$p
? @ R 30O
fi3K4PfiK%fi4N042
fi fi(8fi3M
fi
?!<rMfi?
oM0 */(V /3 ffX
fi Nfi@ ff
QR
ff?d1fiE,'fiI
fi>a-/30 ff*
+
pffC'fiO3@ F?5?3MO
fi Mfi?fi L3
3 3
ff0v 51E3~:;4. fR
ff?ff/fiff:
! Mfi?p)
oM0 Q4
8Pv pNfi A?-
Pbk o/a80,?pff4,04 0<'fiIM
0P4
5
fi03vp14fi

03M8+F?Mfi@ CR
ff?d14fiI2p
ff30ffZ
fi 'v pff()fifi
Efi +F?v ff
3M Cfi0fi
MfiP ! ? o/a 4704 3fid EI?807'fi3
3 (bfi@ ff@ Z
fi 0fi >+F?4
@ffP v pP+f?v
E4
J4fi Y0O
ffR
! cQMfi?pY
oM0 84f3'fiPffG()
'fiv ff
?CP'fi ff340O
P()243M@ ffI03
fi p
3 Ufi
Efi #+F4?v ff 4F'fi
ffC
fi?p$'R
ff
`_
fi :
! [4'fiM*/?p@ L@ Qfi pfi p4
3fi 95?pBdT4 ff
QW
" ff ff
E?
['fi/
4
fi #%fi3M23
3
?0v pmfi0p3
3 P 33ffX ff?4
ff'fiZ3
3 &4%fi'fi o/R
4O
!
T$p
? f o/
E4O
>+-fi34ZR
C ff43M'fiPff*9 +9fi32W?138@ ff13@ fi?X0fi &
Rff?ff/fi3M
ffff!
T$3
? Y4'4PfiPa44
fi X?+-Y+f44AO
fi #4P X?pUMfiP &fi(f%fi\+
4
ff ff
fi73

3 04
fi )
pp+9fi.1!9T$p
? [(bfi44fid+Fv 384A $?5fi/

3M@ ffffh eJ/B
f8 gih0
eg) * ff>R F/v*gvffww/v*aA3v 3ff ff
fi alE!,+c vffww/v7v KWH j
+ c<fi.*0fi/
R
ff ff
'4
L344ff
?S3
ffP Zfid fiV!Gffc 'ffvpG'fim4'8v'fi/v pY?pC@ fi 09?fi'()2j
* 6TfNT
03ML4QfidW
+ p'fi>fi R?pPfi/
6%fid
+
2!PT$?4[+A>Y 360fi *g03MN3
E?

k a%^V\'9EMdBbKl2dB~Bff^-^'B^A;i EB$B^EnmEWo\Bit^Bff\\~'Fffqpg;\8iNrOr=s;
fft v)u \B~5~;5BffdVB6E B~Q\'[^[~E5dBV~^~O`Et^5\Bff^<BOB^512l 15
w ~^~\$E
Kx

fi[

t}-m{t/s$q



os0pt{a

l qM


4fi>@ o1Rff
ffR'fiUff
fiv
@ffvpG3M'fiPffvm?pP(i3M3M@!fLY@ff3Nfi(
6TFNTL]^8
4
fi *6?p@ C+A8Gfi/
-fi\+ Ifi\ fiX 6TFNT fi'I7fidW
+ %fid
+ ff*6+F?2
?
@ff30ff C7@ ff./Mfi\W
+ Cfi(J?pc6TFNTn@ 4fi %
fiP7$
3 4
fi ?
pp+9fi.a!,TFp
? f fi
p~:
+-fi.@ ff./MfidT
+ Wfi0fi ffW'fim
@ ffK
0fi <0@ ff.1fidW
+ v R
fiP83
3 2
fi 8'1ffP!
T$?2$@ ff./Mfi\W
+ S2'fi'v @ ffG()423M@ ff$fi(< Yfi?3Q
fi 'fi
pp+9fi./Q
fi[?pI
fi3
3 '*
3
E? A?pNA'e
N
fi 'fi
p;+-fi.X
! [9'Eff ff
4fi +F2p
f+A5?13MAMfidW
+ V!6+c C?pQ(i3M3M@ *
<4<@ ff'fi 0v 5'fi o1R
ff
<?09'fiO
Tp;+-fi.
fi 'fi4 6+f441R
$40 ff
ff3v pN7300 *
4'03MffS
fifiR
pK'fi(bp+- fi ff!ITF?4T
o/a MP4
2U4423''ff[?pI%fi/:
4,13 p042;Yfi(5fi3I
fi 06@ ff'fi3ME
ff73
3 0 ffN?pff@ fi 84'()G
^@`y`Afi(5?pP()fi4fi\+Fv p

E4/6
fi v13Mfi3

ff
3Mfi z<Pfi 'fiv 3Ma
* bk o/a F4fi 'fi8(i443@ ff*/( p{<\ ff44042;*
K2O

ff ff'%fi ff!9Q3$fi
E? 3M ff5?Ffif4'()C4%fi(g?pff !
T$?04V0R
g4Vfi@ 0} ffIJ()fi4fi\+F! ff
fi I5fi\12p
fft 84423'' : oM0v ,?O4g3 ff
?Mfi3
3 ?Mfi3Mg?p9a
d! ff
fi |>f?g?p?pff
ff70

. fi3
3 07p

_ fi Jfi(0r [*fiR
p
;1R
ff*F()fiPf `_
fi *F P
E?v pm ff 0v pWfiR
'fiE! fiE$@ ff30()fiK'R
ff
`_a

P
E?v p- ff v 35fi
E'fiO@ 9v ff
fi 7M!gTFp
? ff 9v ff v pFfi
E'fit4g3M'fi'fiIff3
ff
?pm} ))nE0)) 'fi/
4ffX+F? ffp

ffa
! ' 4fi Z
fi 0fi 'R
ff
`$
_ ff7?p

fi 4fi 3
3 p
O+F?04
?N' :B'fi:;N' 4fi 8ffIR
Ap
!O
{ F@ ff@ 6%fi4 AQ0fi
@ff30V()fi6'fiO
-fi(?pff -fiR
'fi*+F3
? @ 9 j %fi -FEfi ff3kfO
ff t?O?p-v ff v p
fiR
'fi[@ ff fff'R
ff
`k
_ ffK
4$fi(60fiR
ff!5Y
K?p7fi?pf? 0*0
fi3
3 @ oM0 ff[@
@ ff@ ff&'fiR?Mfid+ ?0P'fiO
Kfi([?pY ff pSfiR
'fiEPMa
fi Mfipff
ff4Z@ ff C?3ff
fiR
vff! ff
fi U
x o/$?p>Efi@ ff34A(bfi$?3>7304fi $3fi U
'2
ff
fi3 !

rMfi74O
ffN+Fp
? 8
+ Ifiv p4 P0fit@ ff34* ff
fi XCCfi\12p
ffQv
@ ffO

fi4?PL()fi> :B(bfiv pY?pC7304fi >04 Z R ()1v 3Y*6fi pY+F4?WY+-fi'~:;
@

fi0v o/p 412C 0
ffP04
f4O

fia o/4;q ff3!T$p
? ff0E4
F ff3 ?Mfi\+
83
?& ( :B0244fi /:B()fi4R
ff03MZ()fifi p fi(F?3
@ ffO
5 fi?0P>fi\ '0
.
`_
fi !RT$p
? 0R

fi
43p
ff8+F4?Zm4
3fi Rfi(F@ ff4ff+-fi.Z 02p
ff8(bfiI(i3M3M@
@ff ffE
?!

~OM -fik.W

{,RvL+F?Lf7344fi# o/a<()fiO ( ff
fi fiO'2
ff
fi3 ?t2V3ff>?Mfi3p?Mfi3V?p
0R
A'fiP44430''F?3Q3
k
_ fi 09 43
ff!<T$?pQff
fiYACM@ffvp'2
ff
fi3 *
+Fp
? I8340 7fi L?d P?pffQfi\W
+ v p
R
3
f04 !7sJ>v S?pP ff
4fi m+ P@ff
( ff
fi *+Fp? @ cff
E?Ufi f3@ ff$ @ fiv $832fi $a4 !
c~Pfi v p$>
fi>+Fp
? @ $N ff?04
$?<2 p
fffi PL04 p6()fi<?p$a3M%fi AfiR
( o104fifi
L0 <
fi4v ff
fi *(bfi

oM0v <Jv N?32DO?/k
_ p
V4fi Q'fifSff!gst. <?p2DO?/
_ p
ff*
?p@ P2[K2 p
hi
4 fffi j stkl[(bfi+F?4
E?GKPfi04 8fid
ffO
@ ff!ffid
+ ff*Ov G?4

N?p L@ N;+-fifid E<?pN(i>h j rOkl-fid F(bfi$2'? o/0fi4fi *M ?pNv
ff4
h j c~klAfi\ F()fi$' '( pP Y0 ffA()fi rX'fis,!
{ U3O

#fi 3ff pP?3
fffiR
ff&?pYv 44,a4 I()fir-*6cE*< qs<*<?Mfi\W
+
v rO 3M
ffL X
>1!PT$p
? ff P P402`$
_ ff*?pN? R@ ff44'2
*04
U()fiN?pP03%fi 7fi(
4423''fi V!JA4
4*fid ,rG4j
ff?p6
fi4 ff
pN0 ffO <hiv I'f$QstsJ|-5T5cHNLlVfi
,2,p
ff44 v pN?pff'fi7fid -cFh)+Fp
? rW4<v ,'QDQ|,sJ,c :Q|,"FcHNLlE!"Ffid Ac6
ff4?p9R

@ff
ff/v pP0 ff= <0P()fizfi\ Qrh)+fp
? YcA4F Y$"f|-A|,,c :[cHN\l5fi[F
U3
ff4
?pff 'fi4 p
fsnh)+F3
? Y$4$ Y$DQ|,sJ,c :Q|,"FcHN\lE!-c(OsZ4Fv Y$"f|-A|,,c :[c~HQ*


fi

K bl ^^~' {
bl \^ p1

RR

mknpokmkq

rJ:;
fi2ff
qcp:;pff4

$QsJst|-AT$cHN



/

ff4

ff4

RR ff2
{ "F|AA|<c1:fc~HN ul4B~~~^
/ rt:;pff2
cp:;pff44
cp:B@ff
ff
s:B'P
sV:B'
)
DQ|,sJc,Q: |,"FcHN ubl \~^^ 1
RR ff2

)
K lb\^ p1 DQ|,sJc,:Q|,"FcHN
RR rJ:;3ff4qcp:B@ff
ff
rO3M>12,
40A(bfiffidfrhi(bEl5KcNh)?ElE!
RR ff2

K 4l ;E5{' T$"W[H Uc~T$T5c~HN
c;:B@ff
ff4.
l2E

sV:B03

ff

4

@



/

{
K l2E\
c;:Bff
ff
ff2

R
c~HQ
s:B'0P

)
K l4B~~^ { "f|-A|,c,:[c~HQ c;s:B:B0ff
ff30 .
c;:;3ff4 R
R
s:B@ff
ff4

rO3M>12D,4Y(bfi$?3L4pFs,!

?pK5
Kff
ffL?pLP0ff=<8()fic!a[?p+F2*MsR
fi34 RQ03''P4'vpI
'fi|9?WhivY>T$"W[H Uc~T$T5c~HNLl9fiF030vp8R;+U
fi8hivK'.Dd c~HQ>lE!
[6O
fi pffI%fid *a4 g@ 5@ @ ff@ ffI3 p[r [
! Tr ?Ofk
_ 6fi(aff
hi! !*?p> 4
ffElF U44fid+A0 7 :B'fi:;'P' fi >hiB! !*a?pI ff
ff\
ffp
fffR
;+
4
fflE!>T$p
? 803M%fi 7fi(,?d1v 3C'ff>4F'fiK4143
>?pIfi ]^Lfid 4O.Sv 'fiC3M'./!
'N+f?
v 0
fiPv p>fid+ Mfi-()fi Pfi?3$'N4A G))) B;
! D,4
ff
3fi
Rv 5v K Uv 4%'!
D94 ff
3Mfi fi/

3M<,?pFfi ,. ff9
fi *13
?,fi 9rm./v pL
fi rJ:;
fi2 ff

fiPrt:;p
ff4 d!|9
?#fi ?m@ R
'fi4@ fi([%fia
fi 0*930 Ifi(F+F?04
?&PdZR

. ()fi ff
?Rfi(9Q'ff
! a4 mp
ff pN
'R
ff
()K?4Q30 Q(bfiY
ff
E?W'!8T$p
?

?fi4
Pfi(A 04
32L
fi ()fi?4N30 L4Nfi/p
ff ffmv m?3Pr
fi p
P 4'4
!
c[4[30O
ffG?Q()3M?pf
44/
* MfiQ'R
ff
`$
_ ffU3
? @ *@ 3 ffS'fi . 8?37
_ V$
3 /:B4O


?fi4
Lfi(6v pv L
fi K()fi P'!
[3['R
#
ff
() ?p7 [fi(<
fi [()fiT
ff
E?mfi(6?pIfi hir-*%cE*sgl5 Yfi3M
o/P0 !frq?
;+-fiL%fi40
fi Ort:;
fi4 ff
,rt:;p
ff2 ff!OT$p
? A_0O
4fi PO
ff 6?0,rm
fi4 ff
<P0 ff
z

<\fi/*< 0W?p ff
fi
fi XO
ff 8?I8pff4 7?pff ffP7'fiGcE!O"$fi\ cL4'fi
?Ip+9fiW
4fi Kcp:B@ ff
ff Xcp:;p
ff4 d!T$3
? _a'
fi #O
ff 0Ic>@ ff
ff ff0v ff= <
()fir-*6 Z?p ff
fi XO
ff 8?8pff4 7?pff ffP8'fims<!Os?8?M@
fi s:
' P*tsV:B03 *O Zs:B@ ff
ff !YTFp
? _0>
fi
ff >s' PLS'fiS|9?*g?p
ff

fi ?0$9a3 ffAR
;+ fiR
fi *M C?pQ?4?0$A@ff
ff ff5a ff= <8()fi
c!/rfiL
ff
?Sr >*/?pN 5fi(g4fid+A0 N
fi 05(bfi ff
E?U'L2-
ff
$
_ ff v rgv 3M@ ff5I
>

=K

fi[

t}-m{t/s$q



os0pt{a

l qM


vmP2t()fip o1Q'fiK?p'!rMfi o/a*fid8r
RfiY.
4fiRrt:;pff4Q()fi
$DQ|9sV1c :[|9"$c~HQ!
T$3
? [} ~ biKabi hiB! !*?p-4fi 4

o1 fffi g4R
ff4v 3$?pffp
ffEltv 8 8r #04
pff
ER
[?3> $fi(6
fi F?La LP' :B'fi:;''E fi 'fifi1

3ff!,T$p
? NfiR
'fiB

j fH[D8*^
k XO
ff j N"L*^kP
ff j HNQTN!^kPT$3
? Q
fi 0fi j ff4 ffkP+F44aR
Q3
k
_ pff
?Mfi!6T$p
? $' 0fi P
fi 0fi ,fi(fi pffi
@ ( -'fi>?p[
fi 9fi(fi pFfiAfi@ ffi?p
fi!9T$?04A45R
ff
3 cff
E?Ufi f453O
ff 'fiPR
N@ ff
>'fiP+F?$$?0Afi0 ffUfi?p
fifMfi pM!6cp
( Mfi5/4a *1fi [
fiP7$
3 4
L?pff4$
fi U
E?Mfi4
!
0
P Rfi ]^8
fi W@ R

'fi@ >4fid+A0
fi 0L()fi ff
?Z'?d R

p k
_ 3ff* j ff4 ffkS
RR
Pp
k
_ 3ff!T$p
? 'E fi W
fi fi j ff2 ffkU4R
ff4 pC Wfi3Mfiv pffp

()fi8L4A C0@ /4fi Cp
Mfivp8?pN Afi(t20@ ffv v p8
fi A?5PdR
[.
()fi?p'?8@ Mfi8@ ffMm
fid ffZ1mfi?p>'E fi W
fi fi 0!rMfi
o/P0 *
v rO 3M >1*ts<]^L?M@ '
fi
fi 4fi Q()fi'TFW
" [H Sc~T$T5c~HQ @ Gh)c;:B@ ff
ff4
s:B'E PElE*Vh)cp:B@ ff
ff Zs:B030 \lE*M j ff4 !^kRs
fi 4. Ls:B' P-fiAsV:B03 [()fi
?4A'!-Ffi\+ d*0fi\ $c9
fi32. >c;:;p
ff2 $v 'ffCfi(gc;:B ff
ff !,T$p
? (bfi@ * ?04A

ff13 A'fiSh'h)cp:;p
ff44 q
Xs:B'E PElh)c;:;p
ff2 q
XsV:B03 \l'lE!
j ff4 ffk4)
Kr 02
@ ff 5fi(t44fid+A0 Q
fi ff1p
3
ff!6ffc 02
34ff*/804 4-?p
5fi(O4%

fi ff1p
3
ff$?5Rv v K Kv 2a'7 fiR
?pN' 0fi
fi 4fi !
o/P0

fi ff3
3
Y4fi\+ ffqXr-]^P04 &2Yh'hirJ:;
fi2 ff
cp:;p
ff44 lE*Qhirt:;
fi4v ff

cp:B@ ff
ff \lE*<hirt:;p
ff4 B
&cp:B@ ff
ff \lE*O!!! lC+F3
? @ >r&. ffQ[
fi [ Yfia fffcE]^[
fi [
ff
?G'U ?p> ff1p
3
!
A93
3 /:B4
*?pff fr 04 0<@ Fv ffPv I?p$()fi4fi\+Fv pL 3pff
! AX 2
@
4O
PV#
* fi hir-*Vc*Jsgl[4N>fipPfi(-?pP'ff8v Q04 V*V RLffv ff
N?pp o/

fi 'fiL. X
! T-
?Mfi1fi $?3ff,
fi ,v p
R
p
!gT$p
? IM
fi MfiXpffP'fi7'

?fi }
fi
fi
?Mfi2
!,T$p
? Q
?Mfi2
[fi(t
fi KP ?,R[0 ff*1(bfi?
oM0 */fi fiPv M03<()fi
?p/fi
![?Mfip
3 ?Y
fiP0 P02 m+9fi32v
433
7?pa4Q()fi>
fi W
E?Mfi4
*O
fi pffZfi\ *6p

? @ P
+ ffd C733'R
ff
`$
_ ffmv R?pr 04 0!CQ3M>fi (bfi8Mfiv p
?474L?0>?7?p()fi1
30>fi(5?2N0R
74>fi W?3 `_
fi Rfi(5fi
ffL%fi3M7
fi@ ff


fi ff3
3
ff!,T$3
? N04A()fiF
fi
?Mfi4
745@ ffv 5'fiP?pff >fiR
ff!
0
ff
E?&fi ?I
?fi X
4fi *<4-fi @ K3O

ffZ'fiSfi0@ ?pC
4fi
fi(<?p8fi?pNfi Q?N@ 8
fi 3ffSv G[r ' fi U
fi 0fi ff!frMfiY
oM0 *r-]^
' fi
fi 4fi
fi c]^L
fi ff*t'fiYr pff0N'fiYfi0 P+F?NcQ4!A fffi Z
fidT
+
fi C P?fi Ffi(%?pffi?p,@ ffv 9fidhiElE*M fi 9.Mfi\+F<?pp o/,'['fi>+F?2
?
Q+F42V' fi !LT$3
? @ 4[fi Yfi p%fi0 3 o1L'R
ff
3@ 8?3Ir [N@ P3O
ffG'fi
Rp
EPv 44
!SrMfi
oM0 *<4($r=48v IFQsJst|-AT5c~HN '*9 Z48
E?Mfifi@ ff
fi
rt:;
fi4 ff
*g RQfi0 ffLcF./v pY
4fi Rcp:;p
ff4 ff*?3RQ+F44g'dm >$QstsJ|-5T5cHN
'!TFp
? fi/
ffNfi(-R
ffv pKv mY'*O
?Mfi1fiv 3Y R
fi *tfi0 /v p ?pP
fi 0Nfi(Afi?p
*0?pUfid/v p'fi
fi
p o1f'*a4A
ffffSv p

_ ff!
fim(iff*<
+ K?ff KR
X3v pm
'2
ff
fi3 +fp
? @ ff
?#fi P?8fi\W
+ &v 41403
04 V![c(<
+ 3O
P
( ff
fi *?pff
?Rfi L3 ff[?pO
834fi [04 S'fiYp
ff
4p



fi ff! 832fi I04 48()fiO
ffX1R.1 p j fi/3
kXhip
k
_ pffXv
3Ma ff
fi >1!4vdl
fi(<?p804 0$()fi[r-*cE*% Gs<!aTF?4$0fi103
Ffi/p
ff4f?p7

E?Mfi Mfi30FR
ff?d1fi[fi(6?pIfi ff*
+Fp
? j '
0
?Mfi fi3kKO
ff >?>ff
E?W2O
P'a
Rfi >. ff8 R
fi V*Jfi0 ff

fi [fi(<fi?3[fi *V S?pS' 4fi $'fiKf
p o1['!NT$p
? 7fi/3
$02 S4F(bfiEO
ff*
ff 444*11C.1v 38?37Aff4
0fi103
9fi(g?pNv 01233M'fi'fiU'ff$ 0 ?pLv /:

=

fi

mknpokmkq

ff
4fiUfi(O?pL'EfiK
fi0fiff!,S304fif
fiW0L :B'fi:;'P'fi

vW?p0fi103
L04V!UrMfi o/P0*<(A?p fi @ fivm. ?pC
4fiIrJ:;3ff4IWcp:
@ff
ff Q s:B' P*?34afi -+F24M' fi()fi?p @ fiv9'hB$QstsJ|-5T5cHN8*
"F|AA|<1c :[cHN8*%T$W
" [H Uc~T$T5c~HQ>lA'fi ?p @ fiv Q'UhiDQ|,sV1c :Q|,"$c~HN8*DQ|9sVc1:[|9"$c~HQ8*"f|6:
A|,,c :[cHNLl5@ @ ff ffY1K'a ff5fi(6'ffQv Y?3>r ff()fifr-*0c*a Ss<
! 832fi f04

fi 2'[fi(,?p Qfi(92t
fi @ ffp
3 0
ffQ?NRv @ fiv Nv04V'fi(9?pIfi/3

04 K 0KfiR
?p>' 4fi C
fi 0fi ff!
{p
? ?p6?p543fi I4<
2
ff
fi3 fi9
( ff
fi *[832fi O04 pff0O'fi[R
-(bfiEO
ff
'fiE(bfiaV7304fiF
fifiEvfiYfiRff>hi@7'mfi(6rO3M@vdlE!::_a
fiYfi(
fi0Q0fiRff
fi4Kfi(8.1v3+f?p?pW)9fi(8?pR
fi=ff33
ffY4fi\+-ffn?p
fi/3
A02 K4'()?3LfiR
p!

3-
4tfi(ghfi0blJfiRvffVfi(0a4
34g4%fi
*+f?4
?84gM@ffff7?3@*4t?

fi(g(bfia4pC7304fiA
4fi5?5+N+-5fi3M$fi$'fi4+-d1$dfi4*
4vff \i0E
fiR
vff! o/P0 Y40fiR
;DFv h)c;:;3
ff4
sV:B' ElE*9+f?4
?q'ff?0
?Mfi304Y+Aff/[
>?p7
I?fc$M
fi ffT
Mfi[pff4 [f?37O
84O
>?[sX4F' 'vpM!
T$?2f0fiR
;Y0@ Lfi0 fff?0LPffm4 I()fi?p4 3
L483pfi3K@ ff
ff/v p
p+0 (bficf+F?04 8'
P4'vp fi4p
LY'fiY|,?!PTFp
? P ff
fi R2%fiN
4L/:
M@ ff ffUp
? 74 & / [fiR
vff!5T$p
? ff 7fiR
fff'I?f(<04
304f834fi

fi &h)?p j ' klf?ffi/

3M@ ff*?p 304U Mfi?3[834fi Q
fi &h)?p3ff
ff~:
j ff'%fi ffklQ+f446fi/

3Mff\
! oM0v 4L0fiR
;XD-18c($rt:;p
ff4 8?7fi1

3M@ ff*g?p

34 sR+F24

ff
3M8s:B@ ff
ff4 !
c(%?pF02 ,v rgv 3M@ ff9>
>L@ [
fi>0 pffv 'fi777304fi ,04 V*+F44/?04,834fi
04U4'() fiRffDfv>D9'+vp?04$13pff'fiS45fia0 `e
34-fi[4%fi~:
>()fiNfi'[@ffp
N(6?38p
EPv fi G4[0 ffGfi G/43gv 'R
ff
fi Ufi(,?pIr [
! c
?p@ @ fi 4mU
fi3M0 Pfi(A RP4*g40v Ir [> ?4c
oM0v CT$?04L4423''ffN?Mfi\+

GP( +20 Lfi *+Fp
? U
pM*a
o/?00$
fia \
fi0J
ff?0ff/fi*?3@
P./v pfi06fi LRff?d1fiL0`e
3f'fiK@ ff2
!Av ffY?p I2N pffm(bfi> fifi3[R
:
?d1fiE
3 ff
* ff'R
ff
444G>?p137
7 R
fi0v o/pmfi($fi 8v
@ ff@ ff!KUfi/p
ff

3
? ff
./v pY(i34S3M'fiPff7?4Qfi/
ff
! [

fi p 'fiYfi3M>fi/p
ff6
Ep
? ff
. ff*O?p0fi103
Q04
()fiFr-*cE* 0YsW2~$
_ ff5fi
ff
DFvL 0
D91!
"ffifi3f3ffY@m4'fi pff3ffn()Yff0vpM! 3M%fiU2p s<]^C''
Pfi
ff!ZT$3
? Zfi 3 v ff v pSfiR
'fi?P
fi34R
04 ff48'fiRp
ff Ks,]^
fi
s:B'E P*5+f?4
?q?p@ ff() @ ?04
fi n()fi R
ffv 3R. (bfi 'mT$T
" fH :
ScT$T$cHN8
! f(bC0/v pm ff pmfi
E'fiff*9 `_a
fi qPffXR
K@ ff13@ ff!XrMfi?4
02
345fiR
'fi7hip
ffv vpP
fi lEk
* Mfi@ E`_a
fi Y2)
pffp
ffWhi ff
fi KlE!

ffc U7344fi[34fi*+F?TfPfi1_$ffK1 vffvpm{n?Mfi(bfiEPFU`_$ffF?p
fi/3
7r QZ+F?MfiS(bfiL@04>(A_a
fi()24*OW+f?747@04@ffaT$?p
'
+ Q'fiY?3ff p
3 fffi Np
R
Gfi m+Fp
? ?3LN4>
( ff
fi fi7
'2
ff
fi3 !7c(A
( K
fi *
?p>fi F+f?K?p ffff'f
fia3Mfi Jfi\+ d3
* ! M!*2 p
$sv fi3ML
oM0 *aPv v
?pfi130
f02 mG0/v p ff p'fiY4*V ()1 pC4*V@ 0Ev p4Lc
pffp
ff*J 0m?p
0v p7I
fi1fi(g9'fiP20fi(t?pNfi
5'fi3@ !6cp(g
'2
ff
fi3 *M Cfi $02 ff9v ff v p7'fi
6fi\W
+ Pv 0/4304 !OT$p
? $v 0/4304 O@ $?3P <'fiL?pF
fiP03Mfi 2>%fid
+ (i3
*+F?Mfif(bfiEPg?pAfi/3
t07 E`$
fi
_ fft?OfiR
ffg@ 54~k
_ ff!Jc(@ 04t ?3ffp
ff*
fipLfiffi@ >fi $@ a5?pff$fi\W
+ Yv 0/43004 !

=J

fi[

t}-m{t/s$q



os0pt{a

l qM


c$4$3O
ffKp
? @ N?0FP
?0v pLff0vpIfiR'fi$@704ffCfip :;~:;:B2O>R$fi
?pO?Iv70
E?*(a
K
fi *?3Afi6
fi:fi$a4tL./vp[3M0gvffvp
yh Dgfi'ff*VvffwwlE!, fi C?pff [3Pfi*?46a-Mfiff-Mfi9(bfi/
3,fi?p[ffpLfiR:
'fiOR
< Lh)fi?p<?0 I'fiLp
k
_ pA?pfflE!OcO(bfi/
3 ff6v 0'ff8fi ?pAfi3M
fiO
$ ff3v3F()fi?p
024
fi Gfi(-Kv ff v pCfiR
'fiff!Iffc m04
32ff*+ P0M@ ffQ?p `_a
fi 2p
3 !8T$p
?
p o/Fff

fi \ ffF3@ ()30
z
. fi3
3 U3
k
_ fi 0 pff3
ffK(bfi[3
3 p
E'v 3>@ _a
fi !

| ,

{

R



T$?2 ff
fi fid/4p
ffp
k
_ 4fi Ifi(Nr f*Afi
ff*- `_a
fi V*A 0qP
E?v pUv ff v p
fiR'fi!Yrfi8S
ffff*O33703Mfi3>33pvp fi(5?p@ff34Lv?2L0Rff*gPfi(
?pff 7p
k
_ 4fi A@ N(bfiEP!

}b ez {] ]G ekgz^KE]k^

r [L?ffLff'N()fi3MLMfiff7fid8
44
g02PhiHf24'fi*6vffwQy?6DffZ {ff42P*
vffww/vdlE!YrMfi8fip*O334.P
24
6a4*t?pP;1RPfi(A_k4 :;'3M'fi'fiW02L3ffW?p@
4fi\+F[%fi42Yv /k
_ 0Chiv 0p
Pv 0\l$ 3?G
4fi ff3
3
ff! &T$?2Ffid/4p
ffQf
fifi/
fi/p
ffAfiW
( ff7R
ffp
ff&fi P?P
fi v1344W@ ff'%fi 'fiW?pffO
1fi
8+f?Mfi3M
U`_a
2VP fi Y'fiC?pff[R
ff?ff/fiff!f|6
ff
3Mfi G Sv ff v pPffUR
> v ffff ffmv
3MP 3pd
! TMfi?3A fiN4,?0Ar 04 -?ff N'ffff* ?3f02 p
ff p

3 [?pff N'ffA'fiI@ ff A3M0'.19fi(V?pQfi\ 4'.!,T$?493M/4p
ffO?3f02 v 'fi
P2 -$
3 *?p@ 1%fi24Pv
ffv p7?p[
fiP@ ffp
? 0444;8fi(J?p[04 ff! ff54'fi
0v I0`
L
fi W
?Mfi4
ff>>`R@ [2O
ff
* R4(,?pP 0'fiS M03MQ@ ?pPO
!
?
fifi(5r 04 >4Q?>?3m P04
304Y
+ ff4`:;3ffm'fiSPfi1p
ff2v p ?p

fi
3M@ [Rff?ff/fi[fi(,830v Lfi
! G0'
137R
ffi(,v p :;fi fa4 F
mR

fffiR
p
ffP p
R
p
> ?pP
fi%fi ffv 'fi>L'

?fi Mfi3<832fi 904 Yh)()fi,+F?2
?
fi0O0fiR
ff[PffGR
8ffffalQv mK''v ?'()fi+AP 3pd!7rOv 44*%r 04 Q
R

`k
_ ffR3v 3C?3P m%fi034N
Rff
/BB 4)/ 'iYPfi1p
ff6
Ep
? ff
./v pSO
?Mfi/*
! M!*a@ AL3M?
h~vffwwlE!
2M
fi8fi(<r 04 Fffi0fi@ ffY'fi
24
t04 F4$?[?p@ 84F
@ ffQpff
fi(J ff ff
?Y?$?AR
Mfi pQfi K3M'fiP4
2()fiPv 38
44
%04 0z
* ! M!*a L
ff

{ff44 =h~vffww/vdlE!cp4P3
3
v ff?Mfid+ 730
?#fi(Q?2PP ?PRY042
0 'fiWr [!c
&?p
fi?pC? * fi43Mfi fiE?PP
qR
Y3 ffX'fi fi4 Sr 04 YhirMfi ffB*Lvffww ClE!
4M fi[fi(Jr 04 0,-fi%fi ff'fi702 ,
fi%fi ffCfi(VE3 F A49?-?pQ4'5Pff
o/@ ff6[04
Ifi A3

v
!OT
H ?pff ffO()fi604 0J?6ff34@ 9()fiP1 _a
fi *r [
@ P0@ ( E0 R
ff
3 P?3
fi0 oRv
fi 0N?7
Wfi1

3>R
;+WE3 ffNP. ?pff

Y?EY'fi E(b!5rMfiEPJ E`_a
fi U(bfiQr ff4F137'fi0?44
ffU U+F4p
ff43 ffSv
( ;:;
4
tv 30''4042
fi !
T$?04<3Ma ff
fi *+F?4
E?46a fffi AL3ME? Uh~vffwwlE*ME $b0I3PPE} ff<?pf04
,fi(?p
r [830 ff'fiRfi13
ff9fi 04 ff!mrO 3@ ffS 0
>G4243''?pKp
k
_ 4fi !mT$?470R


()fi1
3@ ffLfi r f>?7fi/p
ff9fi 8+F?WY%fi24v /k
_ 04( 4O
*J @ ff ffW8
v /
_ :; p? j ''Ev pkYhi! !*0 ff3
3
Lfi(O
fi ElE!

dvJ~B%`O~BWmVBqo\BO^^ B^5~J$'$O \f^9pg;\EIiNr=rOsdNr=rOr;
=K

fi

mknpokmkq

()fi@ 3vpUfi3M804
3fiRfi(F3'fiP/*,+0$b0ff>'fip _pC-fi1fiffX`:

/!X|6oM0vffI?Mfi3p?Mfi38?0480RI?0ffU3M'fi'fi#'fiX
fi4fi o/@ffff

v -fifivffY/*MRff
3@L9fi1fiffYv3


30PP}ff-?pff@N'4fiK
fi`:
fiff!<9fi1fiffS452'fiP3(i3(bfif3

v
4 o/@ffpI?pLfiRvff!6r3M?pEfi@*
C4ffvC()fi 3'fiXpff
ERY;+-fiXfi(N?pG
@ffOF`_a
finfi4?P4([+m3
-fifiv ff U EO
Mfifi !,TFp
? @ ()fi@ *
+ N $b0C3P} Q?pL04
Afi(69fi1fi ff /!
-fifiv ff
Z 0
4>Y >fi?
( ff ff
>+F?R2'vp32p
? ff ff ffO
>yU 0#v*g
4fi ff
3 3
3
<?pF-fi1fi ff
* -*1 fi
Efi * 04(b/v pN?p['EfiR
fffh .fi'./*
vffw CwlE!9rfi
ffv ffO
K
7fi
( Ia
* q.I49
4 ff?p7PEfi
( 0
aa
* q.I49
4 ff?p)
fi'
(
0*1 C46
4 ff?37Fa4Pfffi'
( J!6rMfi9?Mfi $ ffp
<+f?MfiL@ F3
3 ()P244g+F?
-fifiv ff Cv +F?Mfi>+A -'fi
[v 34fi P()fi9?pff@ ffiR
fi 0*149Pdp
? ffC'fi82Pfi v p
?ff
? ff ffO
>fi
( 47ff(5U@ *:! M!*<U 8fi($
fi !S
* @ fiv *O
fi0 ffO

+-fi34Z?pXR
C v ff
4fi *<3
3 fi V*< X
fi0 ffO
*<@ ff
ff
4 ff!R|, ff
Iym v*
v W?4>
@ *6+9fi32RR
?pfffpR Kh}lL Z?pC 7fi(f4
ff ff
8v R?3C3
3 0 S,h LlE*
@ff'R
ff
ff4!
T$3
? U-fifi ff @ U3O
ff&'fiZ

_ !qT$3
? @ U4PR045fiEp
fi pZ?p
ff ffO
'
* L*1+F?4
E?49p
k
_ pffCZ
4(% 0fi 4P'
( q.P
J!6c9Pdp
? ffC'fi8?v M.Ifi;
(
fi fi30f'fi()fiQ !fTFp

? ff ff
[y 0Wv8@ 83
k
_ pffU
zI*%
t*
zI*
8v!AT$p? ^a]1V3\Chi fi fi3F'fi p : ff ffO
Fl-fi;
( Iv
* 9yh >lE*%@ L?3Mfi M}
fi ff ffO

fi
( Pv 04P+F?K ff'R
ff
$'fiL!9+c K?3Lfid EW
oM0 *afi [r-*cE*a Ys ff
?G?ff 7?pff
fidT
+ q-fifiv ff q ER+F?&4'fiP!T$p
? S'fiPfi(Nr-]^-fi1fi ff n R U
4fi
rt:;
fi4 ff
ACrJ:;3
ff4 ff ?1?pf'fi9fi(c]^- 8@ Fcp:B@ ff
ff Q c;:;3
ff4 ff ??p['fi9fi(Vs,]^
E@ 8s:B' P*asV:B03 * UsV:B@ ff
ff !FTFp
? ff ff
IhirJ:;
fi2 ff
Bqrt:;p
ff4 lAfi(<r-]^
-fifiv ff U Ep
ff
R
ffA?pL@ $fi(6
fi rt:;
fi4 ff
*%rt:;p
ff4 ff !
-fifiv ff
09
qV4<.
K=^@`1K 3 ^8fiW
( ='
( qV4<
Mfi 3ff;30 <fiW
( ?094<
4fi ff
3 3
3
f?pIfiR
fi 0B

* A*V * m4'fiK?[?p4'p343
? ff ff ffO
Ny Xv.
!
4>?p
~ff Md>K fffi($3M0 09
q~\
! TZ'fifi(5?3fi/3
Lv U2L?pCO
7fi(
?pQ'fiPAfi(V?pQ3Ma !6rMfi?
oM0 *Mn
( ^ (K ^ @ N'fiP-fi(t3Ma
(J *
@ff'R
ff
ff4*M?p^ ( ^ 45 S'fi fi( ( q~!
T$3
? L9fi1fi ff P
()fiFfi [r-]^F
fi f4A?p>P4 ff'5fi3L
fi v 0v p?p>'fi
fi(,r-]^Q /!fcpQ
fi v Q4J-fi1fi ff ff ffO
[(bfi
ffU()fi r9]^Q'fiPN30v pP?38-fi1fi ff
fiR
'fiG
-
* -*N *Nv
23v py# v! T$p
? ff RO
Wp
k
_ 0fi ?Mfi4()fiUc s,]^
E
K

! n4?pY0fi103
R30 ffX(bfiC4A' fi &
fi 4fi
v ?pR832fi
K02 hiB! !*N?pfi/3
Cfi(7?3Wr-*5cE*$ sr flE! c
pR'fi fi(8?p
fi/3
[
qR28hirt:;
fi4 ff
Eqcp:B@ ff
ff s:B030 \lE!NT$?4[4f?pI(bfi fi(9
4fi
. Y2734pfi3I1?p[?M@ Nfi ! [ EZ
{
* *
@ N3M0 0,fi(t?p
fi/3
5 0P
ffJ!
3@ 3 Mfi\+ 'fiZ3M'fi/!nrMfiEP4*A qr fi(Q?pY;1R
#
U
fi 2p
@ ff&p
? @ S4P
?M@ :B30 U h ChB<l hB<l ahB<l'lI+Fp
? @ ChB<l84I?pY@ fi(f 2
ffUhi'ffElfi(N-Z
*
47?p-fi1fi ff X G
fi@ fffi 0v pK'fiW-* hB<lP;
hB<B
l hB,
l 2L?p P'`oRfi(
' fi 8
fi fi 0t+f?4
?8@ ?ff ffO
6fi
( I*
ahB<
l hB,lJ 5?p5v 4'ff ! f[4'fiM*
hB,R
l hB<l h
l
@y C2N?p@ >fi(5@ ff
ff ffp ff>
fi $pff
vpS0Nfi(
hB,lL
4
ffFfi(6A! h\lE*a+F?4
E?K45 U0@ /4fi C(bfi hB<l hlE*a45?3N' 4fi K
fi fi fi(

ffa%pBt\$E^9gE\ M\J;= _EE%^6\BELiNr=r k ;%\^m%E5~^F\^iB^E^[FEff
NV
NEpBE BE&B6MEB\tEE~B~tN%m ~ pV^J^J5B'$BO EJ;B^E1^5\^~b
=

fi[

t}-m{t/s$q



os0pt{a

l qM


{


ff

$NsJsJ|AAT5c~HQI*/"f|-A|,c,:[c~HQ8*TF"W[H Sc~T$T5c~HQ

fi

rt:;pff4qXc;:B@ff
ff4.Xs:B'EP

ff

)

DQ|9sVc1:[|9"$c~HQ8*aD[|9sVc1:[|9"$c~HQI*0"F|AA|<c1:fc~HN

fi

rO3M@NMDg$fi(O?pNfi/3
502 (bfiFfi[r9*Mc*0Us<!



hB<l[
fi@ ff'%fi v 3P'fi ffp
P!>Hffi?Q+8fiYffpffN4Rffff j y/!^k=-Ufi3MNp _fi*
ffp>+F?MfiN'0fi
fi4fiK45yPMfiffLMfi) oM4'!6{8
Y0ff pMfi hl
hM l6()fi-?pf'EfiC
fi4fi
fiff'%fivp>'fi7?pffpfivp8(bfi oMV'fi
o!rfi oM0*Jrgv3M@>1* h'hiT$"W[H Uc~T$T5c~HN8*nDRd c~HNLl'lN4h)c;:B@ff
ff4
s:Ba3 \lE!
rO 3M ffFI
>2443''Ef?pff@ >r 3
k
_ fi 0!gTFp
? @ L@ >r 04 0A(bfi$?@ Lfi ff*
r-*ac*% SsX+f?U 4
ffR
* ffp
ff* G' fi U
fi 0fi ffT
! Sv 0
fiPv pfid+'fiK*
Mfi$()fi
Cfi?pf'* $
_ ff-?$?452$ Yv 44%'!
834fi
$04 U45()fiO
ffK()fi v p Nfi f02 ./v pP?3; A'
ff /d5hi4'fi


4 ffG?p j '

E?Mfi Mfi3[fi130
kfiN20 j fi130
kl$fi(9?pr fQ
fi ff'%fi v p'fiK?p
v 0123a4
!6rMfi4*?pN0'fiFfi130
A25p
k
_ pffK

( R

hB;l ahBBl'l
+F?p2$?pAff4Sfi/3
*aU?3>'fi[fi/3
hBB l hB lAfi(;UW'/:
4fi&P'4
ffP2Pp _kpff& hB ( l hB l h'h (J ( l h ' l'lP hB ( l h (J ( l
hB l h z lQ()fiCh (J ( l. hB ( lE* h lE hB lE! c+R+9fi0*t?3Pfi/3



h$hB;l

r 4f()fiOffGK./vpC?pAff4U0fi103
Ffi(<?372
ffNG?p8@ff
fiGfi(<?p
'fiG
fifi0!Qc+44J'ffLfi(<?pIfi/3
[r @I3M0vffF()fiOffG()fi ?pv4
'ffffi(t?3>v41403r [!
T$3
? ffi130
-r Pfi1p
ff2-I Afi(g'
0
?Mfi fi3Ar f!<TF?p[-fi1fiffKE8
fi@ff'%fi/:

v pI'fiP?3Nfi/3
5r
2A?pNfi/3
5 E/!<rMfifrO 3M ffFI

>1*'fiP()fi834N?pLr
fi/p
ff4v 3?pO@ 834fi N04 *%
+ . P?pfiLfi/3
r c s&fi(-?p
?M@ Yr [!qrfi?4I0'fiPfi/3
*Zah l hB$QsJst|-AT$cHN8*<"F|AA|<1c :[cHN8*<T$T
" fH :

ScT$T$cHNLlE*5hB$QsJst|-AT$cHN8*J"f|-A|,,c :[cHN8
* Dd c~HQ>lE*5hBFQsJst|-AT5c~HN8*J"F|AA|<1c :[cHN8*
"F|AA|<1c :[cHNLl 2
! DO$fi(O?pN0'fiFfi130
Ar 4$?MfidW
+ K YrO 3M@ QM!

H o1g+9p

k
_ 3,?pZ` ^@Uaz^1-fi( 8r 7*d+F?2
?>2V?p- tfi(2
fi 8 ff1p
3
fft
EP'ff
1>?p5r #a4 !tTtfiLMfiQ?4ff*
+ -_0'6p
_ p-B
K]3 YUa*+F?4
E?82gQ ff1p
3
5fi(
fi $hi'fiPlE!
rMfi4*VK'' p#2[ Wv /k
_ 4 :;4O
fi % ff
'fiff*A h 0 9
l "-yh >l V*J! !*J ''Ev pK4Q
v /
_ Rh -l 3?# ff1p
3
Ufi(N
fi 0Gh)+F3
? @ 2?pS-fi1fi ff qv W30 ff&1#,lE!
O1 C } b!
4Im ff1p
3
Wh 0 l7fi(F 4
ff3
E?X?EY'* h #" (
l y/*
! !
* vR h J M#" ( l[R
ff
3 ?pPz- 'fiP!+c fi?3L+9fiE*t E3
3 mfi(5Y''v 3 2N?p
ff1p
3
Cfi($ 4
ff8/4ffv Z r +Fp
? R?3C''Ev pS4~$
_ ff7?pC' 4fi W
fi 4fi
fi p?3ffp
ff!

=%$

fi

T$?3L4p3fiLfi(Or
&

hB<lq



mknpokmkq

X45p _pffK

-hy>l

W?0$33'G

h
0

l$vU+F4?( 0 ahB<l


3
E?K332N
4ffqEEbG=/M*gW4L4m'fi^@[O[)v]['vp<!G@ff13@ff
fiY

pP330Afi(OUr @>+F?$45
4vffK?p>r pB0EKE);iM!,cff ?4F
*
?pI



Efi G
fi 4Ffi(<fi 3I
fi 0fi $

0vp 3
3 f730'fRv Gv G mv 4
'! T$p
? G `_a
fi 443M@
44C?pff r [*F+f?4
?

Y/k
_ :;v p?q''Ev p*
6: ^@ ]_VM\^a]_^Gyh AL3?
*vffwwlE!
z( +fi@ p
k
_ 4fi L@ pffp
ff
! Tr z4.
[$VM\*)`iff ]~ 4(p*g(bfi
ff
E?&'' hB<lE*
q2t
fia 5(O'R
h
6
l



v

!
c
+
8


fi
?
p




+

fi







*




r

ff




`$
_ fft+F?6' :B'fi:;'
+,.-0/
43
,211
' fi C?pLfi F?Mfi32C. N(bfiF2a%fi0 [
fi 5. U?pNfi?pFfi !9TF?4A4
C G@ ff'fi 0v 83fi G'fiKP. R
ff
3 Ifi?p+F4 I?pfi N+9fi32
fi[.Mfi\++F?
'fiYMfiYv m'fi
P

3P
ffO
! Tr 46
5ff ]~d3 \YUWY K]1Y[I>7U( 3 38:9 h 43 l
h 43 l<y/!tffc >fi?3g+-fiff*?p-
E?Mfi4
Afi(0
fi I33 0433 ff[3 Pv pfft+F?2
I? ffp 9+f44R
. ()fiQ'
! r q4Op
v 4'2
,(04g463
Pv 2'4
<:ff
E?fi(4gff;
! dT0 ff
fi?p+f4 K'ff*-4I3O
ff&3
? @ ?45r f
fiP0 Xp
v 4'2
!RT$p
?

@ ff''E4

fi I'fi>p
EPv 44
9r [<4:
fi<LP @ fi<fi0v ff
ff
30 5(bfiX

Mfi p
v 4'2

r ?p@ >45p
Pv 04'4
ffi p>

pP?p>O
L4 p3fi Cyh AL3M?0 *vffwwlE!
{ 4'

fi pffS?pp
k
_ 4fi Kfi(,
[$a[$`v
a?![Sfi/p
fft
Ep
? ff
./v p p02
4K
fi 4ffi(
fi1fi.1 p()fiL
/
ff*JL3
ff
R
ffGv ff
fi X>1 ! >1O
! ;)^ ]<v mr 4QK ff3
3
fi(, 2
ff
"

(
h
l ChB<l
*t(bfiE
U>=zv3
E??hM J#" ( 9
l hB<l[()fi
Y$y U@?v*g! !*
03 B
h MA"
l


/

Z
!
p
c
(
B


C
*9?pD=4Im
1
v !Z|,
E?&
1
v Yv Z

&

r 04 4fi\+FI?p
(

0
%fia44pC?0N?pPfi 7
Rv /k
_ 0ffKfi()*VfiLLfi 3 >p
ff@ ffV*@ 12[?p 4
ffNfi(
?p>
/
!<c$4'fiP204 ff-?fP3M0'v pI
YR
Q@ R
ffffUv 0p
k
_ ff4!
{ 3 o154423''Q'fiO

Nfi(t?pff Lp
k
_ 0fi
! o/a N''v pv C?pL2 p3fi Qfi(gr
*M?p77344fi $r ?$25?pNfi/3
5fi(Or-*c* Ys<*a4
h'hirt:;
fi4 ff
Bcp:B@ ff
ff &s:B' PElE*
hirt:;p
ff44 q
cp:B@ ff
ff &s:B@ ff
ff \lE*
hirt:;p
ff44 q
cp:B@ ff
ff &s:B' PElE*
hirt:;p
ff44 q
cp:;p
ff4 q
sV:B@ ff
ff \lE*t!!! lE!
T$?2545P ff1p
3
Lfi(O'fiPFfi( 3
! =$
3 fi(g?045''v 3I2
h'hB$NsJsJ|AAT5c~HQI*"F|-5|<1c :fc~HQI*T$W
" [H Uc~T$T5c~HQ>lE*
hiDQ|,sJ,c :Q|,"FcHN8*0"f|-A|,,c :[c~HQ8*a"F|-5|<1c :fc~HQ>lE*
hiDQ|,sJ,c :Q|,"FcHN8*0"f|-A|,,c :[c~HQ8*aT$W
" [H Uc~T$T5c~HQ>lE*
hiDQ|,sJ,c :Q|,"FcHN8*0DQ|9sV1c :[|9"$c~HQ8*a"F|-5|<1c :fc~HQ>lE*
hB$NsJsJ|AAT5c~HQI*"F|-5|<1c :fc~HQI*"F|-5|<1c :fc~HQ>lE*!!! lE!
[4Vr [$v YrO 3@ ffF

>@ >
fia > Sp
Pv 04'4
!6rMfi
o/P0 *0v KrO 3@ >1*
fi\ 5c9
Cfi P. L
4fi c;:;p
ff2 9()fiADQ|,sJ,c :Q|,"FcHN'!9ffid
+ L
fi0

fi
?fi4
7fi(9s3
Pv pff[C3
3 41p
3 p o/[8()fi[c$(bfi DQ|,sJ,c :Q|,"FcHN8!%rMfiY
o/P0 *
(6s. ffNs:B' 0Pf?pUc$83'['ffSv U'DQ|,sJ,c :Q|,"FcHN8*% G(6sZ. ffNsV:B@ ff
ff 8fi
s:Ba3 N?pKcA83'LfiP'fi'7"f|-A|,,c :[cHN8!

=d

fi[



bFE

t}-m{t/s$q



os0pt{a

l qM


gieHG2zgJ[

Hffid+?N+?ffffffm?3Ir (bfi44 3ffG()fiLfiN02*%+-
@ff[?p
1p
3 ff'fi fi(O?Mfid+'fi(bfi4} QfiR
ffff!Orfi$`_a
4fi*MfiRffA@Npa4
4 o/@ffff

ff?pffr
[7h)()fif3M'fi:B?pfi@ 4
7
`_a
4fi al-fif Kff%fiVfi 2
!-T
@ *0
+ >3O

`yYUVff ^1[ff%fifi 2
!gffc fi?p,+9fi0*+ F30O
5?<4
5fi/
ff<4v 3ff> P+ F
fi Mfi

fi 2p
>4833fi3Q%fi0v 7(i3M3M@ ffff
! dfv pK?pv fi?fi
( :<0< {fiR
h~vffw QClE*
fip

fi 7R4v pffNffPfiE<fi 4
()fi834Y 'fiU Z3M'fiP'fih)R
ff
3 3M'fi
@ Cfi@ f o1 ff P?0 Z4 pff>ff%fi9fi 4
dlE!U-fi?Z@ @ ff@ fi 07@ 3 ffZp
? @ !YTtfi

4P04()Lfi3M9fi1fi()9v ff
fi M*1fiR
ff< W o/@ ff ffv ff%fiafi 4
!6rMfi5'fiO
ffi(?p
v

ffO
`_a
fi WO
?Mfi/Nv ff
fi XC1*t+ 3 3M'fiP:B?pfi 4

?Mfi10Q+F?
Yr @ ff 4fi K(bfiF?pLfiR

p!
93,
#
p

_ v pQff%fiafi 4
5fi
ff!OS fi(?pFp

_ fi 6 F0 fffi
G 3C
D-3
3 ff49h~vffww/vdlE!>TtfiC4p
7?pGR
p+-m3M'fiPRh)()fiQ04 lA Gff%fi
fi 4
Ph)()fi$fi
ffElE*M
+ pffK'fip

_ pLU$'
1B)a B;zyh [E,: K]1^a]~lEX
!
fi03M4fi Y4
v /

_ 7@ ffp
3 0
fi(,ff%fi44:Bfi3
@ ffm'fiP*J! !*tK''v pMI
!
:;'C4Q 'fiv

fi03fi !Pffc fi?p7+9fi0*JL4>Whiv pv 8fi87304fi ElQ
fi R?0Lfi1

3L>Yv p
4O
5'P 8N
fiP03Mfi !g
{ F
fi v1p
3 A'fiN@ ( 6'fiL 3'fiP'fiP'F640L j '!^k
4LUfiR
;m?84N'3
3 Gh)(i4 \lQ()fi7 Zr -!U zhB 8lE*O4(- Wfi (A
4F'p
3 L()fiT
U''v 3Pv Y?p84 p3fi & hB,lLh)(i4 >()fiffiO
8'v pPv & hB,l'lE!$T$p
? Ififi
=
Ih 7lQ
ff >''Ev p(q2~$
_ ffhiM
fi ff
fi>4'()lffiR
;m*g! !*t?3PfiR
p
?Mfi40Ihiz
fi ffc
MfiN?Mfi2al5()fi6
6!7
()fi@ Pp
k
_ 0v pC+F?NQOff Q()fiNfiR
ff['fiKR
7'Ep
3 Yhi! !*
?Mfi4l9(bfi['' pM*M+ >_0'$3 k
_ pL+F?0F$Off F(bfi[P()fi834I?[4$-fi1fi ff \ o/@ fffi
'fi R
>'p
3
:;'
! K 4 B; .
1J
)R4f'p
3 Kh)()4@ \lFQ
:;'Pzp*! !'
* z5 K
)q h z9
)%lf(-
Gfi S4;
( K) h )%lE*J! !n
* L) yRhB ylf
ff
30 J)W4N 9fi1fi ff o/@ fffi
+F?Mfi20 ffffi U?pIO
-fi1fi ff C3 ffSYr -*
z,4[ m'fi fi(,?
E/!8rMfi
o/a *Vrt:;
fi4 ff
hirt:;
fi4 ff
.nrt:;p
ff2 lF(bfiL
:;rt:;
fi4 ff
L
:;
()fi7304Whirt:;
fi4 ff
=rJ:;3
ff4 lE!SY
3
4'fiU4.W%fi3M>G
:;' (bfi834Y
ff pY'p
3 fi
(i4 N()fiF Y'fiK*v
LP
:;745 S'fiK!

:;'S(bfi834'
)4I'p
3 UfiP()2 Yv &04
34P
:;'fffi(NR'' pM
! D<fiR
p4
k
p
_ 3ffv P9fiM
( )J*/ C4<'3
3 ffi-()2 Ffi(J @ Q''v pM!Offc P04
34ff
* Z fiN

()fi$?p>''Ev pO6!
{ L()fi/
35fi Kp+9fiP0fiR

;C
4 ff5?0F@ Lfi p?Mfi Lfi'F(b@ ff1p
3
0
fi3
3 ffYv
?p `_a
fi 4E3M@ 8+c E4
0ZW
" ff'%fi Pfi
ff! +c 4
C ZW
" ff'%fi
fiR
vff$@ 72. ff 'fiR
>3@ ()3V(bfiQfi ff!FrMfiQ?p7
@ >fi(<v p >fi hB
(1fiNH=I,M lE*0ffc /:
4
AfiR
vffg
o1 ffO?pA@ ff13@ ffO
O?<Q02
34O
fi p <R

ff
3ffQ! P
" fffi 0 fiR
L
ffN 4'fiY3@ ()3O()fi>Yv p fi !T$p
?
RR
P3@ ffm'fiU ()U?07
0fi(Q?pSfi ]^C
fi +F44-fi/

3Mv X?3U
fi@ ff
PfiEp
Ghi! !*A j @ fffi 0 ffkR+Aff/P()fi`:
fi\+Ff j ' EklF Y?p804 !5ffc S?p8
fi o/[fi(97340 Lfi PhB
( ff
fi fiL
'2
ff
fi3 l
ffc 4

Ifi
ffW
o/@ fff?33ffS()fiN02 fft832fi N
fi1fiv 4fi !Q+c G02
34ff*
?p
o/@ ff,?983a 5fi A?Mfi304O
Mfi94730pfi34>R
()fi 'fiO
[
fi ba4
pL ,fi(

sda%^a~\ $^BpBB^ ^AJ^5\5~B'AEOBJl4B^5;~;daB\~ABBWm5J,Ed;EJB
B,V
BE\6B\-B^8^<$ff8E E<R\E5\^LBB5tE%\NmG%m ~^B<B~ff^'7\ 7BB^yo ~
B9EE~ OE^5Bt'EdB\B'N^>7So Bg`\B^%Tff;VUJBBB~B9EB^4BEfV~EN
5B'F B#B<tEaB^:WaAX,;ZYdg^NE~;/\<$'fV
^NB5B^<dOB^~^N',B\
BBENE\[EJ\BEB^~V5EVpEE

=ML

fi

mknpokmkq


fiff!f"Lfffi0>fiffL o/@fff?ppffS(bfiQff13p2t834fiQ
fifi0vfi!$rfi
o/@ ff$?pN@ ff13@ ffO
A?$fipLfi ]^[
fi Y83'5()fi4fid+ @ ff'%fi
'fia4
34 j 'E v pk
4fi Yfi(O Mfi?pffi !
@ *M

+ ffi 4@ ff 5vM()fiP03
k
_ fi 0<fi(V?pff@ [0fiR
ff ??pf()fiP%p

_ fi 9@
v FR
0`om[
! TZffc 4
fiR
pR^
] )h j ffc 4 Mfi8)klL4>'p
3 Cfi($S'v p
_
( )4 j p EkR'p
3 *A! !*5`
( )4
Mfi'p
3 Yv # X
:;'Sfi(Q?pS''v pM!#a
][bh ) cedl
48SW
" ff'%fi CfiR
p*6+Fp
? @ (c
ff j 304!^k
{ K
4.
)X?3 j 'v km f
dU?p
" ff'%fi S(bfi834Z'ff ? ' 4
344hiv qk
_ S4O
\l
j @ ff'%fi @ !^k W
()fi4fid
+ ffK1CP@ ff'%fi !
TtfiW4423'' ?pff UfiR
;;1R
ff*A+
fi v1p
3 Y?3Yfid E q4 p

oM0 !T$p
?
fiR
p"Dfv (bfi ff
fi n1*9+F?04
?&'ff?4P?Mfi34X+Aff/PR
K?pS
Y?PcIM
fi ff
MfiF3ff4
fF?p7
>4O
N?0fs4$' 0P'v3M*45()fiP4f
o/@ ff ffSF U+c E4

fiR
pDFv8p
k
_ pffYff
DFv8g
] h h)cp:;p
ff4 B
&sV:B' P4El'lER
! D<fiR
pD9P(bfi ff
fi m1*
+F?2
?m'ffL?0L(9rt:;p
ff44 L?Qfi1

3@ ffm?p 344Ss#+f44

ff
3MsV:B@ ff
ff *g4N
oM0 Lfi(<L
" ff'%fi @ NfiR
;!5T$?04$4)
o/@ ff ffUv Yff%fiJfi 2
L
D-h
]hirt:;p
ff2
ic s:B@ ff
ff \lE!
H o1-
fi 43

<?pFr @ @ ff@ fi (bfi9fiR
ffff_
! [<+F42R
L o/04 pffv ff
fi >1 ! >
fiX `_
fi *,+f?I+-K@ ff2 pffX'fi o/@ ffI(bfi3'fiP:B?pfi@ 4
U `_a
4fi &4I?p
p
fi Sfi(6?p7fiR
;*! !* ! 'v 3Fv K?374 p30fi 7fi(<r 14fi4>fi
;K!
cff>?2J0R
ff*
+ -3O
-? 4_
o/@ ff ff83 p$?p-%fi034gk
30j
?l
6:;3M'fiP'fiYhi*
3j
E?*
vffw ClE!6
{ Ap
ff
4p
ff>'fi[3@ ,?p-k
30j
?1r XR
ff
3 ,fi 3,fi(M?pA4a ff'V7fi'jff Ofi/p
ff

3
? ff
./v pI fi?,v ?p[23M@ f3O
ff9?49;1R
Ffi(Jr ()fi-?pf0fiR
;*/ P
+ Q3
?N fi4? hi 3M0 ff
4fi q
>1 ! > 0G
C1!4vdlEI
! k
3j
?0g3'fiP'fiR4[p
k
_ pffS'fiKR
7C()fi3M':
3M0v Ch ChB<l hB<l ahB,l :m hB<l'lE*O+Fp
? hB<.
l hB<l>47U 8fi( j 0kS'ff!UTtfi
k
p
_ 3L?p>4 33fi >fi(6*
3j
E?J3'fiP'fi*+ 7@ ff30@ N?pL()fi4fi\+Fv p0@ ff44P p

_ fi !
rMfiIY$
3 #fi($r -:
* n,Ih glL hB<lo
MFp
G()fi7v /
_ ffGP q
M>v R3
3 !Kffc
fi?p5+-fir
* n,Ih tl ff134-?pN 5fi(g40 4
ff5fi(OZ?0-fi/

3M$v Mk
_ ffIfi()Kv C?pQ3
3 '6!
T$p
? C()fi$Ik
30j
?3M'fi'fiS-* & hB<lX 9h sRl l
m?057E3
3 'Sh 03 lAv KZ+F?

0 GahB<l$
n,Ih t.
l u hB<
l a!5ffc Yfi?p[+9fiE*?p8k
3j
?t3M'fiP'fim?f

0


Efi ?7@ff30@ ffN/4pK'fiO
aW' v /k
_ 0ffYfi()*6L
+ ff4,>R
v 3v 3 R
v 44%'!
\ oM0 7p
Pv 04'4
Qk
3j
?0Vr (bfi DFv*a+Fp
? @ Nffc 4 0
LfiR
pDfv7p
] h)cp:
ff44
p
Xs:B' 0PElE*4Fv KrO 3M@ >xKh)fi U?p> ()El9+F? h Dfvdl<
!9HffiL?F14vp
'v h DFvdl5 /k
_ ff4fi()S204 ffFk
30j
?g


*V SR
ff
3 7?p8r o/@ ff ff
?p)pfi Ifi(?pAfiR
p*12vp[ j 0kQ'Fv h DFvdltv Mk
_ ffQfi()P4g3
3 p
ff0 !
rMfirgv 3M@ PxC+ P
W ?LG''Ev pC?Lv
430p
ffh)c;:;p
ff2
ns:B' PElF+F44t/4
'SRv /k
_ 4fffi(bV*- h DFvdlP
!qT$?13 ''Ev pm?'v &'ZvY
v
23p
ffNh)c;:;3
ff4 q
XsV:B' El94$v & h DFvdl5 ?p@ ()fi@ L/fi4ffFfiR
;DFv!
H o1
fi 4p

7W
" ff'%fi fiR
ff>fi($?pC(bfiv
]Qbh ) cedlE!GrMfiI?4La
d*g?3fi 4

;1R
Ffi(Jr ?0,+pff()fi9 4(b/v pLW
" ff'%fi F0fiR
ff<4,?p[ P4a $p
P 4'4

3j
?06r ()fiN?ppfi Rfi(- j rO~:;L
k
" ff'%fi ffk 0fiR
;! 2 hi
Pv 24c
pffp
ffm()fi
fi3M
eC
-v /@ ff 4fi ! 30 ff
fi C1!4v!
l rO~:;L
" ff'%fi 50fiR
;I
p
? ff
.1
oM0*M?p


rdURi;E^`}m;I\\B^ AB5ffONmVOB<^<^6\E<^\ABf~\iB,f\BpB5\^iBNYVw ;
EdB$BV
BBo B$ff-p9gi;B~9 EAB\$~B^EVB$`\^'t F\BE)xJzy{}|Z~
FQE\~ty%
EB\BE~59^'B^;
=Kx

fi[

RR

{

ff4

v




)

RR

t}-m{t/s$q

os0pt{a

l qM




c;:;3ff4XsV:B'P4





{

R



v

>
RR
v


*

sV:B@ff
ff

ff4@



v



)

R



rt:;pff44


ff4@

rO3M@>x1Fc+E4
Pfi; DFvYhi()ElN0?prO~:;"Lff'%fiEfifi(AfiRp
h) ?ElA$k
3j
?0Vr fff*+F?p@ hB<l< a7()fi$%fi?K3M'fiP/!

D9

+F?p?3t?pN'YBff]%'vH)v>''v3F4V()fi4fi\+-ff71LF@fffi0`d1!grO3@-x7h)fi7?3,?El
?Mfi\+F5Ik
3j
?r =(bfi5?3QrOE'~:;L
" ff'%fi @ ffi
;
fi@ ff'%fi v 3>'fi D-1*1+fp
? @ QfiR
p

D9&
]hirt:;p
ff2
csV:B@ ff
ff \lE!rfiY?4Cr >* h D-lC

! T''Ev pW+f?Mfi


v3L3
3 I/4,'[N /k
_ ff4Nfi(b+F44/v
23p
A?p5_0E'O' 9 0
Mfi<?pF@ ff'%fi

?>?Mfi34m()fi4fi\+![>2
3 ff 3M0 ff
4fi C1!x1*t E(b/v pYrOE'~:;L
" ff'%fi @ fiR
ff

Yv 'fi
Q
4
3P'0
ffLhiv
430v p>4%fi(tfi3M)
o/
E4O
El6R
Yff13 A'fi 4(b/v p8?p
(i34JW
" ff'%fi 7fiR
;]Qbh )8cdlE![W
0
(bfi?V*+Fp
?
+ 3 7?p7E j W
" ff'%fi ffk?4[4
30O
ffK'fiv 0
43p
[%fi?K?pN(i34%L
" ff'%fi @ L K?pLrO~:;L
" ff'%fi N Efi !

bp


@8*['^" ekg79gfi[F]{i[e^

eh

Hffid+?8+-C?ff fi3M7@@fffi7()fi702>ZfiRvff*t84L%fi0'fimpff
4
fi/p
ff1
p
? ff
.1v 3M*! !*(bfig04 U >0fiR
;Lp
v v pF+Fp
? ?p<# !OrO'*?Mfi\+ d*
+pff'fiKR
v G+F?mp+9\
fi ff 26p
k
_ 4fi [fi(9

ff4044paN

ff0444;Yfi(-fi p
()fi Mfi?pd*0

ff0444;fi(O Y'fi (bfi o%!

^j[[ek^ :: 47 ~ 0 (OKfi4 4(t?p o/2'Fa?Y()fi

0 'fi7 !
Z
^j[ [ek^E A'fi ^ l( 9hy>lQ2IE o@ 0 (,0mfiU(,?p@O oM4'N
0?K()fih 0 'fi7 ^ l( h (K lE!
Z

[


ffa44pL(bfiv41'ff<26
''fiLfi13ff1
E?pff
./vpM!<TF?p9ff'fiP4O?pA()fi4fi\+FvpM!
"Lff
2()fi ff
fi>1!N?9fiRp846'3p8h)()4@\lg(bfi-r -*%hi!!*/# hB7l'lE*
(L fi #4(L2'33S()fi''EvpZ#?pm4p3fi & hB<lSh)()4@S()fiK'fiOm''vpv
k
_ fi V
* R''v 3Yv ?pC4 p3fi ?0>

vpS3
3 !TFp
? @ ()fi@ *g4>4
& hB<l'lE!K9R3
fif
pff
ff'fi 4(bC?pNfiR
p()fi$''v 3A?$?ff 7

vpE3
3 !6-Cp

_ fi *



vpI$
3 R
v 0,+F4?C v 04'!,T$p
? (bfi@
* 'Lv K

pI3
3
4L

ff0 (bfi Wv 04O'*6
R'fi hi
:;'\l>v RS''v 3Kfi(-?34 p3fi C4


ff40 [(bfi v 2'!-Av ff*1?pffi 0P'ff5 C'fiPA?)pff'fiI
[v fi ff
v
_a
fi Y@ >?Mfi >

ff0 N()fi v 04'ff!
ffc 4
0
>fiR
vff$
G
>@ : o10@ ff ffGv KFfi(<

ff0444;!5ffc 4
7fiR
p
] )R
fi34GR
7@ ffffRNff/v p ?N?3@ Mz
fi ffc
fi oM4'NS'fi ^a*+Fp
? .^G )J*%?
4f

ff0v L(bfi U 4'!Fcp[4$83
?Gfi@ 8`e
34-'fi o10@ ff[L
" ff'%fi >fiR
ff

=d

fi

mknpokmkq

3


Z30v p

ff4044p!qT
H ?pffffff*A

ff042;Z04ff/P.XfivUvX()/vp4
fiRvff*M$+F420RLS?Mfi!
T$3
? @ U@ 137
fi(N+-d1C'fiWR()fi Pfi1pfff
?pff
.1pM*$03M?p@Y+U()fi1
30fiq;+-fiM!
T$p
? f_09O?Mfi/C49
ff
_a
4I4fi ff()fiAfi p[
2-fi(VfiR
ffff ??pQ ff
fi 4-3/eC
4
p()fi53 Qv
E(b/v p8P
4 ff-fi(tfiR
ff!OT$p
? [fi 0 f()fi5
?Mfi1fiv 388'R
ff
`_a

c

3 fi4?4O?<?04O4fid+f6(bfi9N
fi04fi I'fi>p
P pA?p$
fi03M4fi
e
0

v pffC1fiR
p:;
ff
_a
$44fiv phi@ 3M0 ff
4fi C1!xlE!<ffc C?4- ff
fi *M
+
? ?M:; fff.
p
? ffUfi(L?pff m;+-fi#fi/p
ffQ
p
? ff
.1 p& fi?!T$p
? G()32f fiE?PC@ v
ff
fi C1!
T$3
? O_0' fi4?45 [20 6
e
v J?Mfi1L4fi@ ffQ()fiVffc 4
,fiR
ff



] )J!WrMfi
49' *9?4IO
?Mfi/R
v X/4I X'fi ^


ff40 K()fii
Mp!&c([?4'fiu?0
Mfi ffMZR
&
Ep
? ff
. ff*$4P
p
? ff
.1'fiW Y+fp
? ?p
^\)J!GcR( ^)J*6?pZ?4848
fi 43 @ ffZS `_
fi ()423M@ !Uc(5?p@ @ MfiG()243M@ ff*
`_
fi Y3

ff0!
T$3
? ff
fi XO
?fi1*<3'fiP:B?pfi@ 4
R'h 6T[l>Pfi1p
ff-
3
? ff
./v pM*,4> W%fi0304>v Z?p
`_
fi 443M@ >h ! M*/ R:<E {fiR
ff*0vffw QCl9 P

f3 ff'fi7 ().
^@UW8fiM:
p o/@ ff40 >Nk
_ :;I3'fiP'fi!Lc[2f3@ ffG3
? @ 7()fiNrO'~:;W
" ff'%fi >0fiR
ff!
cff6Tfi/p
ff0
Ep
? ff
./v pM*/'./v p>+fp
? ?pA 4
ff13 <'fi8.1v 3>+Fp
? ?3 & hB<Z
l & hi7l
()fiLfiR
pS!T$?2[2Y
ff34 Q'fi & hB<Z
l u & hi7lN h)+F3
? @ & hi7lNp
MfiffN?pP
fiP0 :
fi( & hi7l'lE*-+F?4
E?X4 fi?0P4
4ffff#1R_08./v p?3K'fifi/3
Ifi([?p

04 Sr # S?p8r
fi@ ff'%fi pI'fi hi! !*V 7lE!$T$p
? 7r
fi@ ff'%fi p'fi
L4 p3fi Cv ff
fi !YTFp
? fiE?


& hi8lE!YT$p? 0'fi70fi103
>4a ffO
l mhB 8lE!LT$?4
?pmp
v pff[+Fp
? ?p & hB 7P
l 1*+F?4
E?S204 ff & hB,.
l u & hi7
v fi 4,4P0 ffO
p
ffA>
Ep
? ff
.(bfiA
/
ffAv P?pffi/3
,r ?A@ Q
:

ff40 5()fi'fi

fv 44M'Q P?-4'() fi?39
fi 4fi <v ?pFr

0


Efi !6L
" ff
4a?A7
/
N497 ff1p
3
Qfi(V 4
ffNh 03 l<3
?C?N
0
!
/

45

ff0 L()fi Uv 4%'7(tfi 3Nfi(O45 4
fff45

ffa N(bfiz?p>v 04%'!

/
N?545

ff40 [(bfiz K 4'L ?$4~$
_ ff-?pLr


>
4fi
4a4 ffAO
Mfi pff0;K4 p3fi !9T$?454AR
ff
3@ >P'' p45v K?pL2 p3fi Lfi(6 Yr (O$4
/k

_ :;v p?P ff1p
3
ffi(%
4fi 94'()1v 3N?pfr

0
Q
Efi *+F?4
E?P+Aff/
v
23p
ff$?37@ ff13 ffO
f?NQ

vpC3
3 G83'[ Sv G mv 2V'
! [4t /k
_
Rff?ff/fi)
34
053MUv KP
/
NR
ff
3 >?pLr ?$8k
_ 437R
5fi(gff!
T$3
? @ (bfi *,'fiRR
K
v &?0?pY4 p3fi Y2O
Mfi pffp*-4I2O
pff
ff'fiWp
EPv p
+Fp
? ?3f

ff0v >
1
v 74'$
_ ff$?p8r



4fi !-TFp
? >
Efi Kfi(<v ':
ff'I4L?3C*
3j
E?,
4fi *O()fi7?p(bfi2fid+F pY@ ff'fi V!Uc7473O
ffp
? @ ?8?pf3fi
fi(A?pPfi
;&h 7lQ4
o1 ff ffW7Yk
3j
?0<3M'fiP'fi!KT$?04N402 ff[?L?pr R
:
v pK ffE
p
? ff*O! !*g P*J4L4fiYKk
30j
?<3'fiP'fi*g
ff
30 .1 pK?pPfiLfi/3

@ ff@ ffF?04F
4fi !$T$3
? Nk
_
p
? ff
.Yfi(<?4f fi?0 4F+Fp
? ?p[

ff0 8
1
8v
2~$
_ ffN?pk
30j
?<

0
C
Efi *J
ff
30 v m?7
?p4 p3fi 2
Mfi
ffpI
! 0fi103
[4Qv hB 7l$+Fp
? p L4[?QK
fi%fi pQ'v h 7lE*
! M!*[hBFQsJst|-AT5c~HN8*t"f|-A|,,c :[cHN8*O"F|AA|<1c :fc~HN8*Ol748v hB D-lN()fiIfiR
;D9
Rff
3 Y8()fi3M?&
fi%fi pP4'UGfi( h D9lE! [

fiv p'fi?3Yk
30j
?5

0


Efi *</4pG hB 7l8v /k
_ 0ffmfi(bhi3P pqW4

ff0 ()fi
q 4A'\l402 ff & hB 78

l 1!T$?2I+F445?0
&4J
( 40Pfi(N q

ff40

/
!$ffc U?Q
*Jj
`_
fi Y(i44!fQ?p+F4@ *a:
( MfiC

ff0v >fi/3
f'

K=

fi[

t}-m{t/s$q



os0pt{a

l qM




hB 7lQ4>12ffRvM_kffUfi(bhi!!*6>4Mfi7vWU
/
\lE*O?p & hB 7lL

?p@()fi@ & hB<l & hi7lE*,B!!*9z 0W_a
fiX3

ffff!a @ff44ff e

fi4?(bfi
6T`_
fiK()fiz?p>23M@N4A0@ffffUv ff
4fiC1!




bA


Z

efhj]G]$^

]\_['^ ]3gi^j['^

802IS8fiRp>*fi/pff/
?pff
.1pNpPv3ffJ+F?3?p,# !OHW o/O+-5
fi4p
Q
?pI
Ifi(,ff0vpM*0+f?4
?S4[
E?pI'fiU-!fT$?04F30ff
fiGM@ffffF?3843p>fi(9?Mfid+
ff
0v pIfiR
'fiF
URff
fP04 U'fi37
3+04 U !
{ KR

v ZW@ ff vpmGo1fi Mfi7fi(Fr ff7 v pYfiR
'fiE!cp4I44. ff?
ff
0v p7
?Mfi1 ()fi5
fi0 L3
Pv 2'4
$r [A+F440R
Q
fiPfi@ fffi(tfi pQfi$fi@ Qfi(J?3ff
fiR
'fi!PHffi?v 3Yfi3[fi3>fi
E?@ ff13@ ffY
fi23Mfi ff 0v pC
L@ ?g?Mfi\+ 7'fi
P. S?pU4
3fi X
fi
*5?44?3Y(bfiE fi(N ff v 3G?04P3O
ff#p
? !qffc &?p

fi o1Qfi
( fi43fi Uv fi?Pff*0?pIr ff pPfiR
'fiQ@ 8R
3M04fi *030
?m
83Mfi *a04 ffC'fiP?3Lr f!
0l%2e\
l % B
l% \

% %
!M %6 4 J
4
%I 4
4 4
l
O042





rO3M@C19T$?pNfi3M4pQfi(gfi43MfiCfiE?K!
{ 3OI?Lff7vpfi1

3MENvG;+-fiY0?fffff?pfi3FCvpImfi4v370?0ffhirO:

3M@7vdlE!<D[3vpN?pFfi3Fvp50?0*ff
E?Cfi9,+f?PN0MfiPIv24}ff8%fi034fiIfi(

2fr a4 !OT$?4,%fi034fi 4
fi ffK30v pL?3Wfi43M4fi P fi4?fi34v pff
vZrgv 3M@ C1!GT$p
? P Wfi1fiZfi(5?04> fiE?
fi 04'7fi($ ff ff
v3U0@ >a4 L()fi?p
%fi0344fi *0/v p R
3Mafi Gfi
E'fiN'fiY?p0@ N'fiYfi/3
Ifi' pM
* 23~:
v p ?p8_0@3ffQfi(9?38fi'Ev pM*
m?pm@ 3 v pC?pfi' p'fiK?38%fi0304fi G(,?p
@ 3/e
Y_a!F()>?04c
fi43M4fi *V `_
fi R 0>@ Mfi p'fiY?pff@ v 24
pffU04
ff!
A5?pL5fi(J?pN
fi 04v pf0? p
* ff
E?Yfi fff ff
5fi p j R
ff'kYhi

fiEv pI'fiP j _0@3ff
(i3
3
fi klL04 Z()fi47%fi0304fi R()fi

ff
3fi !RT$p
? fi @ ?pZ$
_ ff4p
ff Z04

ff
3Mfi 84Jv ffd ff>+f?L ff 0v p>hi4fi alE*@ `_
fi * L02 L@ 04J_
3ffp
ff!
T$p
? 03M%fi fi($v ff v pS3Mv 3K?pfi 2v p0?@ 4>'fiUk
_ 3 :B33p?304 Z W084L'fi
.X0
+f?W
344G?0(bp /fi 0O
*Ov 0
Mfi4G@ ff`:B+-fi4
14fi

@ )Mfi6'4
!6T$3
? Xfi43M4fi > fiE?fi(arO 3M ZC[4O4'fiQ3 ffI3MEv p$?2J0?0 *03Mg?p
30fi C4-7%fi0304fi } Qfi(Vfi pN v
@ ff
% ff 0v pChiB! !*1fi 3Qv ff v p7fiR
'fi

KK

fi

mknpokmkq

02ffC$r R)pfilE!,T$?452A
4
()fiF43fiAv +F?04
? ?pc/fiO

?0 pffL
034*M?3$?K04
4!
rMfiEP4*,mP
E?v pK ff7 v pSfi
E'fiPVW9
?3ffP&h)fi/3
8fiPv/430bl
r 'fi7%fi'~:; ff pNr = :
! P00v pQR
;+p+9fiI3M'fiP 0C 4,p
k
_ pffP-
Pav pR
;+G?pffW
ffv ffO
IhiAff ff*6vffw Q>lEY
! Af?37? 3
? ff'F ff*a+ 8
m3M/4p
[?p
ff 0v pIfiR
'fi$

fi0v pP'fiP?3cff ffO
$fi(O?pLr ?$?pKff
p
47fi(FfiE'fi70*6pffff*6fifi\ffffpC'4fiW
fi4fi!YcffZfi?p
c
+-fiff* VI hB<l; hB lE!


Mfi?p[
45fi(gfiR'fif*pffvff*Mfiffi\ffLffpffff*a!!*


VI

hB<l;



hB lE!

TF?pm?n
4 fi(7fiE'fiYCfiYpffvff 4
ff*NfipX+F4??3ffffpff*N!!*
VIa hB<;l hB lA 0
V8 hB<2
l hB lE!




TF?p(bfi3?
4Kfi(8fiR'fiY
E?pffU?pR-fififf=&3@ffvn?p'fi

fi 0fi ff*/B! !v
* V8 !

@ *

+ PMf
fi fiQ3 k
_ p8fi
E'fiQ?NGfiLp
ff 'ff!7+c Gfi?pN+-fiff*+Mfi Mfi
Mff[?pI?G
4Qfi(9fiE'fi!>T$?37@fffi4[?N+F?S?pI;1R8fi(-r fN30ffm?p@*
pfi7p
ff vpKK'Mz
fi ff
Mfi*Jv @ff(p*VRff
Lfi
ff!8cpL4Q+f?Q+-fiC+f?m?p
ffp
ff'fia<\(bfi R'G &?pff'E fi q
fi fi 07?0
#+Fp
? ?pfiR
p
4'p
3 Ufi (i4 S(bfi R02 !T$?44PR
ff
3@ Sfi
ffP@ G'p
3 Sfi(i4 S(bfi
fiM:;'ff
hi'fiPElfE?pf?0 S()fiNr 'ff!Lr03M?pPfi@ *%?4f0R
[M
fi ffY
MfiQM@ ff[
?0 pffQ'fi
?p89fi1fi ff E/*a+F?04
?S4$?p7(bfi3?S
2Ffi(6fiR
'fiE!FT$?2F
4ffi(6fiR
'fi*+F?2
?
v
23p
ffA0''
4fi *0450M@ ff ffK U[fiMfi h~vffww QlE!
T$3
? @ (bfi <+ -@ ,()fi/
3v p5fi >?p<_a'V7 ff
fi 8
4 ffJfi(Mfi
E'fi!J
{ Ap
k
_ 3<fiR
'fi

Ep
? ffP*1?p-? PfiR
'fiE
! P
? pF ff 0v pNfiR
'fiA
p
? ff7a4 ffO'fiI3
3 'R
ff
`$
_ ff
h)40 \l 4
ffW
* ffp
ff*f 0q' 0fi
fi fi 0!{n3
? v 0'2ffq+F4?q04
34
4
ffff
* ff3
ff*M 'E fi
fi 4fi *4,Rff
fiO
ffA>P
E?v pQ ff 0v pLfiR
'fiff!6ffc PfiEp

'fiRfffi2&ff43Y*<?p fiR
'fi
Ep
? ffPp
k
_ 0fi 8
fi 43
8fi 0W?p @ ffv 0Ifi([?p
r 7z
* ! M!*/?Mfi [09?5@ ff!6T$p
? f4- 2044
4O34fi P?A43
3 'R
ff
`$
_ ff

0Ffi(O?p>r @ ffPv Y?p>O
8(bffiR
'fi[0044
fi V!,T$p
? @ L2F4'fi Y3Pfi
?A?pQ ff p
3@ ff,?0A20fiR
'fi-. K?pQ3M'fiP'fiU
fi0 Q p
P 4'4
!
T$3
? 7fiR
'fiQ
mR
7@ v U?37o/fi Mfi7Xh)0fi al5fi(9rO 3M@ P!Q
{ Pp
k
_ pIff
?fi(
?pQ
fi@ ff'%fi 0v pLfiR
'fi5
Ep
? ffPA-(bfi44fid+F*1R
v 3 pN+F??p[Pfi'-3fi p*M
4 ff
V\ fiKH=I *+F?4
E?K
? 3ffL ffp 7' 4fi C
fi 0fi ff
EQ
g6b)*
[ffUJ~B a8ffzV\ fiKHOI
G;g]{e$g(o\ ]fe0 - ff%
X eJ/g,F ( aV\ fiKHOI hB<l hB l Q/Eiff1 dM _P h ( . l
n
5h (J . l hB<l$0 h (J * l Ah (K * lZ hB<l eJM9V\ fiKHOI h h (K . l'l
B ; 0%V\ fiJH=I h h ( * l'l h ( * lZ! B ;p )/
h ( . lZ
g< ~ V\ fiKHOI # E $ }g< ;p )M ( AB'P0bi ~ff1h (J . l
0m)M E0mBm )/ abiBff1 h (J * l Q ;ffE0b # V\ fiJH=I #
5 # aI )/ 8} g< ;p

KO

fi[

Zh~vdl^@Uo5'hl


ff~:;
fi




V fi



( *



V\ I1
I, " HOI












V _







( *














os0pt{a



t}-m{t/s$q







l qM


V\ fiJH=I





ff':;
fi







V\ I,
" fi

V ffI " fi






'h~vdlVI'XhlAfip










'Zh~vdl

V I,
ffI

V\ I1
I,



V KI






'Xhl

V fi 4 3HOI



V fi 4



V H=I,

V KI " HOI
rO3M@7,Tgo/fiMfi>Rh)0fial9fi(6ff0vp8fiE'fi!

[4Vfi(O?p7@ffPv0vpfiR'fi[@fff'fi pff
ERLvKPffi(<@Ffi(6()fi3MFE4P

fiR'fi!RTF?p@()fi@*6+- p o/p _kp?pff ()fi3MI4P4ff*g+F?4
E?Z@ fip :;&fiE'fi
?-@ f'R
ff
40
ff,fin
( V\ fiJH=I 0Rff99?pF%fi''fi?,9vffffffAv?p[?E
?8fi(
rO 3M@ >!,TFp
? [_0'A;+-fiPE4PffiR'fi$pffvhyV I,
l-KRhyV fi 4l-ffpff!<{7p _kp
V\ I1
I, 'fip ffv ffp h (J . l-+F? ?3Nfi E'fif
Ep? ffP/

; {
]DE>e\4 - ff, Vr g6b)(fdZ 1K~P 0 4V\ I1
I,
( ag6b)GV I,
hB<lP hB<lV h (J . l K 4;ZffUh (J . lK
& ))1,aa4 ;,ffQ1 } ))m0))@ * M~BV\ I1
I, E1`)/' 'EE
i' Md) N V fiJH=I )16 # 5 ;p p 0Kbi00))
)1A 7P aP # )1\V I1
I, h h ( . l'l h ( . lq
{8p _kpBV fi 4>'fi ff3h (J * l9+f? ?pLfiR'fi[
?pff/
G;
g]{ e$g(o\ ] eH 0H - Jj NVr6g ))O[ff4GJ~9 0PffWV fi 4N/X
f ( a6
g b)BV fi 4> hB<l hB,l h (3 * l $ 8fffh (J * l5 k M~ B;V fi 4

E LE iff~ Md)J L \V fiJH=I )1,E # f ; Bff 0)/L))a
0)))/ h (K * l N i>
BKEa # )|V fi
T$?3Pfi?p8;+-fiSE4PIfiR'fi8@C'Rff
44}fffi#hyV ffI lLa34}fffinhyV H=I lE!
G g] e$g(o\



Rff
444}fffi70p4}ff4fi@5fiR'fi6
fiPfi0>()fi33Iv8?p$P
E?vp5ff7vpQ4~:
Q>lE!-+c ?pQ
fi o/5fi(J r 7*/ ff
24}fffi fid+ -?pQ ff
fi(A 04
32Q :B'fi:;' ' 4fi G
fi 0fi mv m?pa4gfip
|
L*J+Fp
? @ ffc
3`:
}fffi C2 ff94*1Av CS4
p
? ff4B]^2
::fi 0
ffNhiS4
p
? ff4B*vffw3 QlE!9+c 04
304ff*7' fi

fi 4fi

m'R
ff
444} ffq+F4?nXO
Y0n

p2} ff+F?n @ fi *$+F?2
?4
fi fi30>'fiGv pYS
fi @ 3

3
>'fiG'R
ff
44} P 0WU04 @ 3
3
N'fi34} C7v WS4
E?4'./
h~vffw Q>lE!
rMfiEP4*
+ Lp
k
_ 3L'R
ff
44}fffi K 0 p44}fffi *M@ ff'R
ff
ff4*$()fi4fid+f
3M*!M!*@QG4
E?4'./th~vffw





; {
]!Ke - a$*Vr 6g b)[ff4 J~R 0L4{V ffI /X
e
J
/

ff

X

,
g
P

E

Z ( a9V ffI hB<l hB l g
g /ff~V ffI h h (J . l'l5 h (J . l

G g] e$g(o\

K=

fi

mknpokmkq

MffBRV ffI E1`8E iff~K ffitE 8 V\ fiJH=I )/
B
# [ ;p 0C)/8)i0g}g<CE0)i V KI h h (J . l'l h (J . l
lqC
V ffI
V\ I,
h (J . l| b08)/Og6 qV KI / 0 [ff
G;
g]{ e$g(o\ ]D>e - ELVrg6))L[ff 1Kff 04WV H=I, /X
J
e /8,g 8 ( aV H=I, hB,l; hB l gg/ff~qV H=I, h h (ff * l'l< h (J * lW
P ; ~ B|V H=I, 1 E i' Md)5E U V fiJH=I )/
;p B 0S)/}<g S))a-E0)) )1 h (J * l
#
V H=I,

V fi 4 h (J * l E E I)/ff=6g 6m ' / 00 E# E i7 BEV H=I,
HT o1*QvyRff7vpGfiR'fiP@Y3 _kpff(bfi?pffY()fi3M2Pff!Z-ff4fid+ \V fiKHOI v

?pLfiR'fi[?E
?Cfi(6rO3M7I@Lp+9fiC3M'@ffff!9T$?pNE?f3M'@L
fi04'5fi(Ofip :;
fiR'fi*G?p()Q3M'@8
fi4ffi(9;+-fi:;'RfiR'fiff![{P3 _kp>?38p+9fi fip :;
fiR
'fi @ 3'ORfffid
+ V\ fiJH=I _0'$hi
A+ @ 3'6p k
_ pff8?pAE4P -fiR
'fiOR
fffi\+?3fflE

]f e\4 0 eJ E~B>E Ea # b ffb)MQ )M9b4
V I1
I, V ffI
\
G;g]{e$g(o\ ]eH 0 eJ E~BQ E # )P))/$ )/6E)Ib)
EMffB V fi 4KRV HOI

; {

G g] e$g(o\



)b7M~B

c94,@ ff -9?046%fiv<'fi8'fi/3
5p+9fiIfi@ffiR'fi9?-TMfi-v?p[?
E?
fi(7rO3@! TF?pq@Mfi v?p?
E?qff
30S?3@O@ff4qPvMfiC4fi(
V\ I1
I, IaffI N XV fi 4 3HOI ?3RMfiMfiLRfffi3Y''4
GRfffid+fi3M>fip,fiR'fi
V\ fiKH=I !,T$p? ff [fiR 'fi5@ Nv 'fi/3
ffCp? @ [R ff
3 Q?p [ 3 (i3a 4'fiIR ff
3
?p@ 3 ff'fiq@ ff R
fiP0 pff fi(Ir [!+c fi?pU+9fi0*[4(>?pRr 4

fi0v 4fi>'fiS00/v pK?pff fiR
'fi8?pW4>+F42O

fi0 (bI041v 3K?pffK!
" ff
2a()fi ff
fi K7?ff
?Kr 'L29fi1
4ffC+F4?I -fi(J4fi\+-a [
4fi -?
L
PdZR
C. #()fi?'!&T$p
? ff fiR
'fip
ff fiP& &
fi &()fi?pK fi(
4fi\+-a L
fi F(bfiz'


G g] e$g(o\

; {



]f



; {



]

5'QEa`ii
(
G g] e$g(o\

'Ea`ii
(





e\4 %
EM B \


4; #bg< Uffi B;L ( #
PE iiY # fB78 ['0t/4b

ff V 1I
I, aKI

eH 0 %r
4


EMffBV

fi 3HOI

g<a 4Rd)
&Ea i)=I

F&)`


~ B; ( #
# E UBX ['I/4b

Ttfiq3
3 3
'n+F?
V\ I1
I, I4vfi
fi 4' fi(8fi pRfiUfi R044
4fi Cfi.
( V\ I1
I, IaKI *

fi2p$?pL()fi4fi\+Fvp oM0!9c+Urgv3M@>1*a3ffvpPrt:;
fi4ff
[FS4fid+A0L
4fiY()fi
r-]^<$QsJst|-AT$cHN#- ff3gv 8rJ:;3
ff4
ffv p$?pAfi 4L4fid+A0
fi 8()fi?g!
r3M?pfi@ *?4F@ ff34Fv U?pffp
KhB$QstsJ|AAT5c~HQ8*$QstsJ|-5T5cHNLl-R
ffv 3p
ff ffG
?pOffp
UhBFQsJst|-AT5c~HN8*D[|9sV1c :[|9"$c~HQ>lf
ff pC
ff
24} ff!8T$p
? I@ ff'fi pK4Q4P42f()fi
+F?
V fi 4 vfi 4Afi p>fiFfi@ >0044
fi 09fi
( V fi 3H=I, !
T$3
? [ ffPv v 3>fiR
'fi*M+f?4
?C@ L4fi(t?pQfi
E'fiAfi ?pN ()A3M'@Qfi
( V\ fiJH=I v
rO 3M@ *J
fi 04'Nfi(9;+-fiU'0Q?pI_a'Q'fiK@ fffi\
fi 4fi q (bfiG
ffp
Uh (J . lE*t m?p

K

fi[



t}-m{t/s$q



os0pt{a



Tj6T$|Fv




)
Tj6T$|9

l qM















Tj6T$|>


)

Tj6T$|<



rO3M@Q1,Sfid/vp'4fiC
fi0fiARp+-ffpff!
ff

fi R'fiU#h)?pO
\lQ
fi 4fi qK'fi\ffp
Uh (J * lE!PT$p
? I_a'N'W
fi 4'Nfi(A0/v p
fip$04P4,fiR'fiff*1?p$ff
fi0'
fi04'6fi(%041v3[Mfi?3<4-fiE'fiff!
|< fi pNfi(g?pN()fi4fi\+Fv p8fiR
'fiA@ ff@ ff$p
v 4 K
fia pff5fi(g?pLr [!
cfffi?p5+-fiff*1(?3Qr =493
Pv 2'4
$ C
fi0v [0fi,'fiIfiR
'fi$a44
fi ?p
5+F24
Np
EPv 44
Q K
fi0 7(b+Aff!

e EMff B /Y i0)EgBY)1F V\ fiJH=Iff g6))

]
@efiff J
aG4E) & pa0%g6))0K)q)/U ( 0bi n)/g<~ g, 1
V _ h h (J . l'l h (K . l G^@oU 5V _ h h (J * l'l h (K * l.
h (J . l h (J * lZ hB<l J
e /ff~ ~ 9V ~ aff18B 0)/ff
[4fi(0?p9ffPvv3$fiR'fiO@A'Rff
4
ffOfi( V !g{ARvL+F4?L?p-?t3'@

fi(V _
G;
g]{ e$g(o\ ]vf
@eH\4 " 0 Aa # \V I,
BUdPh (J . lI0U)/Ea # V fi 4UB
dIh (J * l
oM0fi(
V I,
" fi 4*g3vpYrO3M@1Q *O2N'fiG3ffffph Tj6T$|Fv* Tj6T$|> lhi!!*
P. h Tj6T$|fv* T_6T$|Z> lJ#ylL0>W3+ ffpQh T_6TF|Fv* Tj6T$|Fvdl+f?N'fi

fi4fi h Tj6T$|fv* Tj6T$|Fvdl$
!
G;
g]{ e$g(o\ ] E>@e " 0 - # V ffI BIff1$h (J . l50>)/ E # V fi 4>B8ff1
h ( * l
rMfif oM0*53pRrO3M@1
Q *5+S
fid j 0kW(bfi ffph T_6T$|fv* Tj6T$|9l8'fiX


p+F4U
@ffffffpYh Tj6T$|fv* Tj6T$|FvdlF'fiYP. h Tj6T$|fv* Tj6T$|9lQ

h Tj6T$|Fv* Tj6T$|fvdl7 V!UT$?04L48'Rff
44}ff4fiRfi(5?p
fi4fiRfi ffpmh T_6T$|fv*
Tj6T$|9lA(bfi44fid+ffK 4fi fi(_ff3h Tj6T$|Fv* T_6T$|fvdlE!
! A?4I%fiv*9P4P@ff'fi o/PpK?p
HT o1
fi4p?pY()3M'Kfi(qV _
![4tfi(<?p8fiE'fifU?pI()f3'@
@ff'fiG()fiQ?p7'a4Fv'fiC?p7p+9fiK3'@ffffi(2V _


; {

G g] e$g(o\

4(bY
fi 0fi ?84L
4v ff?p j

ff042;m
fi4fi!^kT$?04N
fi4fiR'ff7?
4fi$'fiCvffvpGhiS2'fi()[vffvplE*(O ( 2F

ff40>()fi 'fiO8v4J'
?pW f
* 4
3 ffW'fim4'fiYR


ff0 (bfi?8v46!YT$?pP@fffiW()fi

?4Aafi +F24
ff
fi
>
ffFv 3M0 ff
4fi M!1*+Fp
? @ N
+ >?fid+=?0FI?pfi@ ff ?Mfi4A()fi
?p[;+-fi:;'YfiR
'fiA(V 0fi 4P(?pQ

ff024;P
fi 0fi 4,'Ep
3 !<T$p
? f@ ff'fi C?A?p

K$

fi

mknpokmkq

;+-fi fiR'fiQvS?p7E?[3M'7fi(2V ()2J'fi 4'()Y?pI

ff4044pK
fifiG4[3p
'fiP?pff4$?ff/vpPV fi >$?3ff$ff
fiYV!,T$?pL3 _kfiCfi(V fi 4>'ffF?0 h (J * l$y
4fi<'fi7fiR
'fiA024
fi * 0P?p@ ()fi@ f+ Q?ff YM
fi 30Qfi( * ]^9

ff042;*z
? ( 2-

ff0 [()fi Kv 4a'!,T$3
? f()fi4fi\+Fv p8@ Q?pNp
k
_ 4fi <fi(t?pQfi
E'fi
()fi$+F?4
E? ?p>

ff0444;
fi 0fi K4A'Ep
3


; {


]

G g] e$g(o\

dIh

J
(



*

l

@eH\4 " H



-a

# V ,I


BSdh

J . l0Y)/ffZEa # V HOI
(

B

PQ1*V+-P
WfidP?3P
fifi j 0k ()fi ffpUh T_6T$|fv*
Tj6T$|9l9'fiffph Tj6T$|Fv* T_6TF|> l9'fiP.
h Tj6T$|Fv* T_6T$|-l$ yP 'fiP.
h T_6T$|fv* Tj6T$|>l,
Rm
ES
V!6T$?04O46pfffiPfi(kffp8h T_6TF|Fv* T_6T$|-lt(bfi2fid+ff

1pE4}fffiKfi(O?pN'EfiK
fi0fi fiffph Tj6T$|Fv* T_6TF|> lE!
G;g]{e$g(o\ ]l
K@e " Aa # V KI B8ff1fh ( . l$a>)/ffCa # V H=I BIff1
h (J * l
[tI oM0*v7rgv3M@1
Q *+-
8fidA?p-4 @ 33
j akF()fi ff3Qh Tj6T$|fv* T_6T$|-l


'ficffp>h Tj6T$|fv* Tj6T$|> lt'fiNP. h T_6T$|fv* Tj6T$|9l6n9 h T_6T$|fv*
Tj6T$|> lN

V!7T$?4Q4QK'Rff
444}fffiUfi(9?p'fiG
fifiGfiffpUh T_6T$|fv*
Tj6T$|9l%(bfi44fid+ffNQ?p2}fffiNfi(/?p6'0fiN
fi0fiQficffp[h T_6TF|Fv* T_6TF|> lE!
G;
g]{ e$g(o\ ] >@eH Je / ( ))& )/ P )1N V 6g b)Za 44
Ei & aEI ;4* * 6
g ))Y ;4* ( ffff # gg /ff~ G)/E<g )MIEM~ B
E
\bKCE0)i ~ ff1Ih ( . lLBYff18h ( ( l
Hffi$?:ff
E?fiR'fiF) B0)iCfi(?p5
?pffQ(bfi;V 4fi +f44R5N'Rff
4/
5fi(fip
fi(6?pL()fi4fi\+FvpMZ\V I,
" fi *zV ffI " fi 4*z\V I1
I, " H=I *0fiqV ffI " H=I, !Acp[4$
fi2p@ffV 4fi (6
fiP4(fi?pQff
fiC' fi(?p[fiE'fi-?3F'fi
fi0fi49PfidffC'fiffpIh (K ( lE!
rMfi o/P0*O3vpYrO3@1
Q *O+F?p+004ffmfiR'fiV ffI " fi 4UhivW?p oM0%fi\\l
[L oM0*JGrO3@

'fiSPfid?34 @ 330
j akK(bfiGffpSh T_6T$|fv* T_6TF|9l['fi ffpGh Tj6T$|fv* Tj6T$|fvdl['fi
P. h T_6TF|Fv* T_6T$|-l # h T_6T$|fv* T_6T$|fvdl V*<?2
fi34
RS
fi 4p
@ ffn '4fifi(V fi *FC+ff4fV ffI " fi 4!sJ.+F2U+F?3q+G004ff
V\ I1
I, " fi 4L'fi p ff ff3 Ch jT 6T$|Fv* Tj6T$|>l$YffpCh T_6T$|fv* T_6T$|fvdlA+F4? j
k
$?3N' 4fi K
fi fi V*1?04$
fi34K2'fiR
L
fi 4p
ffK Uv 44fi fi(V fi4 !
[R
'fi
V fi4 46 ff'R
ff
44>30 ()30/fi
E'fiff!Ocp,. ff<?p$ ff'fi 0 $34fi I?
+Fp
? & qfi fiRfi p+A 'fiR' 4fi 'fiW Mfi?pZh ! M!*5 ffp
U43
ff ffalE*
?pIfi @ 3'fff/Nv Uf
3M@ Q'!fffc Yfi?3[+-fiff*0?pI
fi 4fi Y()fif' 0fi p'fi
Mfi?pPY48' '( @ ffZ'fi?pffp

K ffv 3U0
.Z'fim?pK
3M '!rMfiO
o/P0 *
3M0fi@ fi\ cLR
ff
fiO
ff'3
.ZI?pK4 p

3MfiI@3
}fi3I+F?r=()fi Xv p
:
P
Efi1Zfi(F2O
!Wc
fi34 pYmff%fiW04 hi YrO 3M Kl7?08.a
cf G4[DQ|9sV1c :[|9"$c~HQ 'U3
ff vpffp
YhiDQ|,sJ,c :Q|,"FcHN8*V"F|AA|<1c :[cHNLlf P./v p

h
Q

,
|
J

c
,
Q
:
,
|
F
"

c
N
H
8

*
[

9
|
V

c
1
[
:
9
|
$
"
~
c
Q
H
>


l


W
v
hi #DQ|9sV1c :[|9"$c~HQ+9fi32&?ff Y'fiW
ff
fi
U

v 44%'\lE!
" ff
4<?7

ff024;G4LU. m2p
W
3 ()fiL `_
fi !Hffid+?>+?d Y@ >fi(AfiM:
'fi>
p
? ffPff*t N3N
fi 4p
N?Mfid+?pff fiR
'fiNRff
7

ff0444;Y()fiv 44J'ffff!

KE

fi[

t}-m{t/s$q



os0pt{a

l qM


A4E(b/v p?4[+F42JR
I@ ff Q()fi>3
3 3
' pP%fi?m?pP 4fiJ0fifi(iQ%fi3MNfiR
p
@ff@4fi*tR?pfifi()fi>?3v
@ff6`_a
fiWfi?!T$?p@
;+-fi()3
3 0O
$+Aff/?Cfi3MC ff v 3fi
E'fiff#Rff


ff042;
`yV3[$^a`y`yhiM:
@ /4ff j sJklE*9B! !*5& ff
4v pR?3U

ffa44pfi(N'fiPfi 'ff*5fi
a`yV3O^@`y`
hi 14ff j NklE*,! !*<1ZEv p

ffa44pfi(['ffPfiP'fiP?P
fi32ZR
C12ff
; ?370Ffi(<?p8r fi/`$
_ ffYK?pI ff 0v pPfiR
'fiff!f+c S02
34ff* U
E? p8'fi
?p8

ff024;Cfi
( ( H
* .
* * fi['fifv h (J . l5fi h (J * lE*0 ( @ 0
ffSv Y?pLfiR
':
'fif3
k
_ fi V*/2$
fi 43
@ ffKfi/
!-A? 3ff5'fi

ff4044pfi(O Cfi?3f'ffffiF'fiP[4

fi 2p
@ ff
fi0B!
[A
o/a ffi(t Ks#hifi1
)l<
E? pN'fi

ffa44p*13v p7rO 3@ qQ1*3%fi F?pQfi
4
fid f
p+
4fi j k?0f$
U. !-cF0 j k'fiC$
fi U@ R
'fi@ *0F+ ff2V
'fiY?p Lfi(54fi\+-0v P
fi N()fifi pfi(-?3P'ff7 Lr 7!V+c ma4
34d*V?pPfi
ff
43
p
ff<'fiI4fid+ j k7(bfi j
6T$|Fvf 3
ff
4p
ff<'fiI04.
V H=I, 'fi7?pf' 4fi
fi 0fi ()fi
h j
6T$|Fv* j
6T$
| >lg'fi9
fi 4fi j

V!^kGTFp
? 'fi j kL+- Mfi<@ /fi3>

ff40
()fi Cv 4a'*M03MA(V
+ N3
j
6T$|fv[29

ff0 [(bfi K 4a'N?pC?p
024
fi Cfi
( V H=I, P3
N?pN'fi j k

ff0 ;
! d[v prO 3M BQ8'fiP2443''E[zh 4fi0bl

?0 p'fiG

ff024;*g3M%fi
+ 3
ff ffp
mh _
6TF|Fv* j
6T$
| >lQ ?L_k3M@ ! T$p
?
j

6T$|<M*0+F?4
E?K+-F@ /fi34

ff40 h)R
ff
3 L
+ 73O
j
6T$|fvL4$

ff0 \lA2L
Mfi
fi p9

ff0v !<c
?p5fi?p-? *?pF()
9? j
6T$
| >Q2
MfiLfi p9

ff0v $4<Lfi/


?0 p!
Hffid+ + Y@ K ffMZ'fiR3PP4} C+f?I?pY ff pmfi
E'fi
#Mfi'fiW

ff042;!
rO'*6
+ v 'fi130
fi p fi@ Mfifi
fi
!RTFp
? C17%fi4
R
Rp
Mfi j

v

ff ffk> j
p
ff
@ ff *^k8@ ff
ff
4 ff*1 0X

X

p Mfi j
3MfiAv
@ ff ffk7 j
/:
Mfi>3ff
@ ff *^kG@ ff'R
ff
ff!C
{ 30 ?pff C'17%fi4Q+F?z Zs<
* ! M!
*
ff L?07
ff
0v pQfiR
'fi9
mh)03M<M
fi ffX
fiXpff
ffE4lgv
@ ff Wfi0

ff0444;* X
7
sS
ff
?FYfi
E'fif
3Mfifpff
@ ff >4fi1
V

ff042;!
T$3
? N@ ff3A()fi$?p>4P4 ffiR
'fiEF@ Lv 34 fffi114fi3





V\ I1
I, sj
V ffI C sj
V fi 4B sC
V H=I, C sj

?3K4P4fiR'fiEPfi\12pK'+%fi3MP
E?pff&

ff024;()fi4Afi(Q?p
$
fip :;'KfiR'fiE!,rMfi5?pQp+9fi:;'SfiR'fiLhi!!*{V _ K4afiE'fiARfffid+AvC?p
?E
?fi(OrO 3M@ 7lE*0+ 3ffK'fiP
fi 43
$?p9UV ] Rff
!9rMfif?pN@ ff345v ?4A0R
ff*/
+
fi pffG'fiK(bfi/
3[fi mfi p4'
fi Y?p@ 0
8v G?pOpY Rff
N(bfiN?fi Ifi
E'fi
?64(b7?p-

ff042;>
fi 0fi hi! !*?p5 ()O3M'@Afiv
( V lJ 3g?p)p ff
6()fi
?Mfi NfiR
'fiEA?$MO
fi Mfi$4(b?4A
fi 4fi mhiB! !*M?pQ ?A3M'@\lE!,TFp
? 3) ff
$fi(
?Mfi 5fiR
'fiE6?,2'(b7?p$

ff4044p>
fi 4fi 4O?<

ff0444;Ch 4fi0M fi/
bl
+F42p
p 9R
F
@ ff ff*1! !
*
= X

s<!T$3? $@ ff'fi C4,9()fi4fid+f!69fi1fi./v p>9?pF@ ff3
()fi9?pf4 AfiR
'fi*/92,0 9?,?pF_0'9'C P?pff f;+-fi:;'KfiR
'fi-

p
Iv
@ ff@

ff4044p*tR
ff
3@ ?p_0'7'2>+Aff/
V I,
fi
V ffI !YT$p
? @ ()fi@ *O'fi
3 3
3
'K?37v 3fi
ff?0v K?2$@ ff30$+IpffS'fi oMPv 3L?pI ff
fi G'V![-fi 04p


K$L

fi

mknpokmkq

V\ I1
I, " H=I,

X

V KI " HOI !KHffi?.V HOI
fi ff
Mfi8v
@ ff fi0,

ff042;XOh
LlE*g03M
N
v
fffi1
6

ff042;Wh!UsOlE!c"Us#pc Rff
L3pI'fiK?pp2}fffi'
ff
3 'fiPQ@ 7
ff pP' ( ffU()fi fi 37fi3Mfi pff3
>fi(,'fi
7 ( 'fiC fi?p
fi3Mfiv 3fff3
fi_
( ( +f?U?3ff ;+-fiKfiR
'fi*1Up
k
_ 0fi S?pfi1


ff0444;Kfi(,?fi
'fiPF(bfi Uv 44a>+F42
fi$RLv
@ ff ffU$
pL Rff
!-+c fi?pf+9fiE*M?p>'fi
@ K

ff40 (bfi Xv 04,U($ fi Z8
( ( 2*< ?pff Kp+9fiR ff7 v pSfi
E'fi

fi MfiQv
@ ff 8?pI

ffa44pKfi`
( ( ![r3?pfi *1Up

_ fi h (J * P
l
yCfif'fi
ff
0v pM*M'fiP?pL

ff042;fi
( * 4)
fiFv
@ ff@ ff!6
{ >
fi
430p
N?PsZ45
p) Rff
!
X444V4v 3gfi(M
ff'fi v p) o/04 %+F?[fiR
'fi
V fi +f44fi MfiJv0
@ ff ,fi/


ff042;!
[R
'fi
V fi
3fi-v0
@ ff W4fi00

ff0444;
* (9, ffp
*/R
ff
3 f?pFfi 4
ffp
7?$?4$fi
E'fif
fi34UY4>h ( ( lE!9ffc
fi
23fi *M4V?M@ LfiR
'fiF?F2'(b
?p[

ff024;I
fi 4fi P?d f
3X Rff
,fin
( UWVM]6v
@ ff pL

ffa44p Oh P
X
8
sglE!MY
?p8fi?pN? *aR
ff
3 >fiR
'fiEq
V\ I1
I, " fi 4I 8
V ffI " fi 4>?d |V fi 4IF?3ff[@ ff
fi m'V*
?pK
Yv 0
@ ff L

ff4044p!
" ff35()fi 4fid
W
+ fv ?p7?
E?Cfi(6rO 3M@ 7I >v p
? EffK3MK?pL'!-rMfiT
o/P0 *
Rff
3 V\ I1
I, " fi 4C
Wv 0
@ ff fia,

ff4044p
* V _
Z>
+ ff4! T$p
? ()fi4fi\+Fv pY2>
3P fi(6?p8@ ff f@ff3F+ 8?ff 'fi()Q%fi3Mf?fid+?p>p+9fi:;'Rv ff v pfi
E'fi


?0 pP

ffa44p!>TJfiKdfi4mfid +fp
? ff4Pv 3?pI@ ff3
ff*+ 0@ ff [fi0Y?Mfi I@ ff3
pff

ff (bfiF$
3 p
'0v p8?4A0R
ff!

V fi4 *vV\ I,
" H=I, *vV ffI " H=I C
V\ I1
I, " fi 4*zV ffI " fi * V *zV fiJH=I #K
()fi@
fi
430vpC?4Qff
fiV*V+-$0
b
fi4pNK@Q04fimfi(9?pPvffvp
fiR'fi-??-@$$
b ff
ffKv?pfo/fiMfi7fi(JrO3@Q!<T$?04,@,afi4-3ff
ff~:


I()fi9330p'0vp[?pFL0fi1fi1fi(i,%fi3M6?pf@fffifi("Wff'%fi5fiRvffFhiv
ff
fi lE!9rMfiF?29afi */
+ N+F4?'fi2'vp32??Mfi NfiR
'fiE5?$
Yv 'fi/3
Q
ff'Afipcp+'' p7+f? v Mk
_ ffI@ R
ffv3830''v 3h ! M!*thi/*V*^
*^* *^V* *^* *!!! l+fp
? @
?pOff4204F@ @ ff@ Nv Mk
_ 7@ R
fi Gfi(-m()fi4fi\+ ffm1
\l[ 'fi ?pPr 4 p3fi 3
?Mfi f?,
3fi!TIfi
E'fi9?9
'fiP,'fi7?p$' 0fi P
fi 0fi I()fi9 ffp

v G
/
*m
ffp
I'fi oM4'v3
/
*%fiQ0S ffp
I'fi
@ ffP
3+
/
8R
fffi p
'fi?pQ_0E'5
4>h)?pL
25?$
Y0K3
?U3M0''Ev pElE!OT$?13A?4-_0E'5
45v
23p
ff9fi3M
fiR
'fi5?0$

@ ffp+
/
ff>h ! M!
* V fi R
ff
3 N$
Y0KO
p+ ffp
h (J ( l'lE*0
+ff4<fi3,fiR
'fi,?9
p4} F?p$' fi P
fi 0fi Pfi 3>'fiO
Lff3
Ffi(>
/

h ! M!n
* V\ I1
I, " H=I,
ff
30 7[
p4} h ( ( l'lE!fT$p
? 7fiR
'fi[@ 8012p
ff R
;+
?pff >;+-fi
4@ ff$$()fi4fi\+F

fi 4*MV HOI J* V fi 4 3H=I, *3V fi 4 vfi *3V fi *MV I,
" H=I, *JV ffI " H=I, *MV\ I1
I, " fi 4*MV ffI " fi *MV _ *
V\ fiJH=I
1!R\V I1
I, *zV ffI * V I,
ffI E* V\ I1
I, Ivfi
c$4$4%fiA'fiMfi>?R^@` `gfi(g?pLp+9fi:;'GfiR'fif@Lv ?pN_a'$
4ff!
v!RV

A-?4,fi<+f?0ffNp _kpff79fi(J3(i3MfiR'fiE[h)/4L?3ff,fiR'fiA
?3ffPEl<?

fipY
fi3200W'fiR&r 04()fi4fi!Z{?X?pff@KfiE'fi*94I2Ifi0'fi
40fid 5?pW ff
3ff,fi(%N02*'fi7,6'fi7?A1fi30>33(bfi@ff@O o17

K=x

fi[

t}-m{t/s$q



os0pt{a

l qM


R0<
fi4fi!PTtfi\3M?p3(i3v3ffQfi(A?pffff7vp fiR'fi*g?pvffp
pffP'fiW
p
? ff
.&?P?0
MfipEffqR3@ ff ffI04 hiB! !* & hB<
l lE! [?Mfi3
3 ? Mfi
ff ffPv I?0460R
ff*
+ F@ [
3M@ 8p
fffi0v 3c e
9O?Mfi/6()fi,P./v p>?46
3
? ff
.
3 pI?pN
. fid+F ff3
Lfi(t?3> ff pI?$+A$Mfi p!
T$3
? 904
34g
?Mfi4
Afi( ff v 3$fiR
'fiO@ ff@ ffIp
? @ -+A6fiff71L()fi3MO()
'fiE!
rO'*?pff 8fiR
'fi[' 48 'fifff':B'fi:;40 ff
QR3Mafi 5fi
( ' ffQv G0 *


+F?2
?4N?3@ @ ff fi Wfi(-r [L3 ffWv mfi3M>4P0 ffO
fi #hi ff
fi ClE! ff
fi *
?pff CfiR
'fi8
+ @ C '0 ffm?p 4E3M@ !UrMfiO
oM0v j
* 34}fffi 'R
ff
4`:
}fffi fiR
'fiA@ N
fi 2p
@ ffP(i3
3 0O
a()fiA 3
fv M( @
8hiG4
E?4'./*%vffw Q>lE*0
ff pa<v 3Qr ffp
p
ff9@ L Rff
F()fiX
fi/v pLr [FhirMfi ffB*%vffww ClE!9T$?V*?3ff $fiR
':
'fi$p
N
4
@ Nv C?pL
fi o1$fi(O024
fi 9?$+@ N
fi 43
@ ff!<rMfi3M?*M?p
02
348o1fi Mfi ffI@ ff ffXp
? @ (i
44C%fid
+ ()30,?pfi 4
5 ff0E4
<@ ff3
()fi5@ ff3
v 3>?pQ4O
N
fia o/4;fi(t@ `_
fi *M5?Mfi\W
+ Kv ?pQ@ ffPv 3
-fi(t?4-0R
ff!
$



G$fi R)#: ('2*),+13ff



&%

-+/.

R

0'- 21 3R 34

3


3 aff
fi>1!p _pffE30()300ff0vpLfiR'fi5
E?pffP-'fifi10(b3M'fi'fiff3ff
yh VI hB<l; hB l'lAK?3L'4fiK
fi4fi5fipOffpffIhyVI hB,l; hB l'lE!9T$?p
@ff30<v ?04< ff
fi ff'042?I+F?4
E?fi(?pff Ffi
E'fi9
Ep
? ffPZ
VN@ fN0fi
3E ff'fi
@ ff@ 7p+9fi fi
;U
4 ff[fi(,v @ ff'h)+c E4
SW
" ff'%fi \lE!QT$?4f ff
fi m30O
ff
?P4A ff7 v pSfi
E'fiP@ K02 ffW'fiRv p r 02 *<! !*A
(1fiNH=I,J fi
( K
fi !
ff
fi x7M@ ff@ ff,?pf' 2fi Pfi(V?3FfiR
'fiA02 ff'fiI7 p $02 v 'fi>?pff4X
Rff

fi>fi/3
602 Uh)(bfi5
'2
ff
fi lE*/ ?Mfi\+q?2<ff
-?pF@ ff3ff!t
{ [R
v II()fiP44
k
p
_ 0v p8+F?$+>O
ff K1 j ( 7P
? p> ff 0v p8fi
E'fiff!^k

}

b 65M]

87



^ '['^



Z



]\_['^ z]$gi^j['^

3M6fi @ ff
f46'fi>fi\+-9?p54O$
fiP0 oM;8fi(%@`_a
4fi!6T$?p54pfffi43Mfi4O'fi74p/:
Q
() > )a>4)LO?fi1Nh GsJlE*1+f?4
? @NP
E?vpQff0vp>fiR'fiEA?$@
CfiE3ffm'fi @ff80fiRffIhi4'fiK
4ff j
fi@ff
@pffN@ff1v3Pavpkl
G@ ff13@ MfiC3

3 /:B4
>@ _a
fi !LrMfiLC02 mGfiRpU*3M0fi@>`_
fi
?N30

ffp
ff4fif'fiY ff7 v pM*! !'
* g
* & hB<l[4a4 ffB
n hiB! !*g 7lE!8T$p
?
P
E?v p5 ff 0v pFfiR
'fi;
VMhB<lg2t GsY(a 8fi >(0 `_a
fi I4j
3E ff'fiL3

ff
()f ff 0v pM!O+c Yfi?p$+-fi*(<
VMhB<lE*?pm# 204 ff$ !
30 ff
fi M!Pfid/4p
ff$@ ff35%fi3M5?3>4fi%( pKfi(g
?v 3> ff pIfi
E'fi!
fiO

7fi(6?p> ff3fv 30 ff
fi KM!@ p ![W
H ?3ff ff*%?Mfip
3 ?U+ IM
fi fi[?d
&GfiE

3 K()fi?pff K ff pUfiR
'fiff* ff
fi Cm?Mfi\+F8?I+-K
XR
()fi
@ _a
fi Yfi c e
? K'fiV `_a
fi Y(bfi

?V!

Ma[I]v;@KK

bFE:9* e$g

s$309RvC1C
fi04pp>?pN04P4FfiR'fi!<TF?pQ@ff3-()fi$4%2PffiE'fi
@
fifi44E ffFfi(9;+-fi ()3
3 0O
J?pfi ffP*T$p
? fi@ ff8v8 1*%+F?4
E?Sff fifRI4PO
:
4ff4Wv 3 !Wrfi
o/P0 *,ff@ ff'fi 'fiR3'R
ff
8?P(F ffp
Y283
ff ff
'fiO
+fp
? @ fi pK?pI0?R(bfiC'v L'fiY @ ff'%fi *?3m?4Q
fi34m
3 (i443@ 7fi(A
"Lfffi 0 NfiR
p'fi?fi4 R
ff
3 N?pL ff'%fi L4)
Mfifi pF

ff0 !6ffc K(i
*0?452L
Mfi

KE

fi

mknpokmkq

'3p!O{?,
32?RO46?9pfffi<@ff3
5?pT37R<fi(''v36vI?pF433fi!
c(a?p5fiEv/4p30fi$4~_$ff<?p5fiRp>?pPfiLMfizff,4O?p$P2<4p3fi!6T$?3fi@ff
vN()fiP44} ffA?4!

M@ -

, Vr
bi0Fff VmO
eJ/ff & hB lZ & hB<l

9* e$g

g6))([ff4XJ~P - -P)ffi6B /g6))
( aR
V hB lE hB<l gg/' hB l hB<l

gieevb

T$?3-433fi-ff>R4@ff71L?p54fiLfi($p+ ffpffO?0O?0ff)p+FLvffpff
'fiI
fi 4fi !Oc
7?3-fi?3,? *R
ff
3 ?

v3N337@ffPvO

p
33R@ ffNfi)
( pC
+ ffp
ffffZ
* & hB lQ4a4 ffJ
& hB,lE*g W+ @ p 7@ ff3
pK?p
}Nfi(O?pL2 p3fi !,T$3
? @ (bfi * & hB q
l & hB<lEB
! ]
T$?pff3Q%fi3MN?p
?v3Ivffvp fiR'fi>
?pffPEV\
4v ff

I,


V

fi 4(bfi2fid+

7
fifi`:

]3g=< V I,
?> - g6))~ MffAB # 'E #
gieev b [3 P!8T$?3O* & hB<l[204ff8 !8DT _pPV\ I,
hB<l[ !I9
T$?pfiffuv* & hB lE & hB<lE!CTF?p@()fi@*W6* & hB lQ402ffJ !{C
fi
433?
*B!!*v\V I,
hB<l$ P!8]
Iekge

Ttfi8R[
fi4'A+F?T$?pfi@ffv*v -fifi44>fi0Uh)03M?MfiAv?3f@ff-fi(J?pfa lE*+
3 7 ()fi$?pN0@ : V fi 47r G(bfiF?pL%fi'~: V fi 47r >*B! !v
* V fi 4hB l6=-!

]3g=<KE V fi 4W
& / f' Ei
gieev b [3O
Iekge

0LaE Eb # @>

- # #

)0ff M)

\i0EGa

!CT$?386* & hB lQ204ff= P!C-T$?pfi@ff v* & hB lE
P
& hB<lE!T$?pq+-m
3MfiS
v?K P*$!!*5?V 4hB l !rMfiKv0'
*
fi

fi33@ oM0Y()fiffc 4
K0fiR
; ] )fi1

3ME(F
+ U&#

ff0 ff3K+F?
'fiK
fi4fi*)J
! ]
H fid+z+K
fi4pISfi,@ff34>()fi|V
f
?pfi@ff (bfiFfiE'fif
E?pffPPV1!

ffI

"V

H=I,

!W*6+- RvW+F?Sff

M@ E - 5 qVr 6g b)[ff4 1Kff 0G4ZVUt (
VG hB lB hB<lgg/'BAr z
h (J * lE hB l W)/V/h h (J * l'l

h
ln
J
e





B
h


l

B
h
<

l
&
&
(J *



gieev b 4P42-'fi?pNfi1fi(tfi(6T$?3fi@ffv!8]
# ~ EM #
Iekge ]3g=< V ffI ?> - >
gieev b 4P42-'fi?pNfi1fi(tfi(,-fifi44GvQfi(OTF?pfi@ffv!8]
b # >?> - K
# ~ EM # )0ff M) \i0EGa
Iekge ]3g=<> V HOI 0LE E
& / f' E
9* e$g



fi[

gieevb

t}-m{t/s$q



os0pt{a

l qM


4P42-'fi?pNfi1fi(tfi(,-fifi44 8fi(OTF?pfi@ffv!8]

{[
Md+#?p$()fi4fi\+FvpL
fi0
43fi0O(bfi

?p$?pfi@ff,P
fifi44ff @ 36@ffff

(f?pKfi3 :;'qff0vpGfiE'fi*-?MfiK? 3ff#'fiR GsJI()fi
Q
0fiR;U.V\ I1
I, *WV KI *GV I,
ffI >h)+F?4
E?S204ff$?BV I,
Ivfi 4[4'fi
SsW(bfiF fiR
pMlE!




{)pffp6R-
fi
pff8+F?7?p9_0'g'v8Fp+9fi:;'fiE'fiff!OcpO4_3ff
'fi
L Ssnh)R
ff
3 V\ I1
I, fiq
V ffI 45+Aff/$?pN_a'5'lE!

H o19
fi4p6?3fi@ffP<?0,@LpffpffP'fi>M@ff6?p5;+-fi:;'CfiR'fi!:[?Mfi3p?I+

()fi33C@ff34A(bfi5?pNfi3 :;'YfiR'fi5?05+-@c3Mfi3p? 'fiP0M@ff^@UWPfi;*
+7
+ @ 7$
3 0 L'fi MfiC4. +F4@ L(bfi[?p8;+-fi:;'fiR
'fi!QQ3MF ff3F(bfi[?p8;+-fi:;'fiM:
'fiAp
EPv pf+Fp
? ?p-?pff [fiR
'fiA@ Ypff
ff2 Sst,()fiA+c 4
ffiAW
" ff'%fi
fiR
vffLv 04
34ff!r3M3M@ +9fi.+f44O
fi 43
>fi?p7fiR
;
4 ff! T$p
? P?pfi ffP
@ F13$ 34 !gTFp
? A_0'6?3fi@ ff2'vp32p
? fft?Mfi F ff7 v pQfiR
'fi6?0,+F422'(b
cff4
N0fiR
ffA()fiz?Mfi >?$+F42k
Mfi

M@ zI )aI4E)GEMffB M;UBE@@> - g6))8' ff5BU #
Ei07~EMffE # 0R #

Cz0 - ~)}= ggi C@C}g<4 ;
EMffB bFa`i )1A)/IE~B ( )/IEE }b`b # abi

9* e$g

3M%fiP
P sm@[%fi?'33!<s-c+E4
F0fiR;P ] )J!:[3O[

4,'3pFfi(Vr =R4fi,'fiIffv3M!gT$?3P()fi-''vp < & hB<lE*M973,RF?p[
[?

)W4Q'Ep3 v
:;'Pfi( < !Icp(-

ffa44pYfi(-'fi>4Y MfiLv0
@ ff ff#hi! !* G
sglE*0?pY583'$RQ?p>
>?L K
:;'7fij
( 'v pL & hB lE*+F3
? @ 7 VMhB<lE*
4Q4'fi K
:;'fi(-'fiO
''v 3Cv & hB<lE!7T$3
? @ (bfi *()fic
G''v p & hB lE*VQ73[R
?pL
N?0 )Y4A'p
3 Nv C
:;'7fiN
( 6!<+c fi?3$+9fiE*Mfid/v pI' 0fi
fi 4fi

fi3
3 0Yv Sr +f?Mfi3M$v0
@ ffv pP

ff042;+F44
MfifF?p7'3M?Kfi(6 U+c E4

fiR
p*M+F?4
E?K?Mfi45
:;'>fi
( K''Ev pv ?p>2 p3fi Nfi(O?p>r 7!
3%fi QXfi fs<!ffc
@ ff p$

ffa44pFfi(M'fiPg404v ff0?p,fi042;5fi(Mv 'fi/3
v p
K
:;'Cv 'fiO
P''Ev p( & hB lE*V+fp
? @ VMhB<lE*J?N+A
fiLvm '' pfi( & hB<lE!
T$?2f
R
30 14fi4fi Gfi(9 mffc 4 0
80fiR
;*VL U?pP
fi$
3 @ oM0v S?3fi1fi(
fi(g-fifi441;
! Afid+Fv 3>? )K4,'Ep
3 [
:;'Nfi
( C''v p7fi( & hB<l60fid/4p
ff
Mfi
3
>?0 )S4A'Ep
3 Lv K
:;'8fi_
( 'v pfi( & hB lEB
! ]

gieevb

v0
P+-C@ffMR?0ffC@ff3>'fiS
fi\?pfip :;fiR'fiE*O+-fpffWfi0
fi4p7?p
;+-fi:;'GfiR'fi!

]3g=<K Je /C )a4)RE~B / V\ I1
I, " H=I V KI " HOI 0V fi ~
M~;EBXE7?> -O g6b)' MdNB # \Ei0>~EM # C)- )/
EMff B 2
Ca -
e /X 1ba& b=EMff B / \V I,
" fi V KI " fi 4 V _
J
Iekge ]3g=<
V\ fiKH=I ~ R0aE E b # @> g- 6g b)~ MffPB# #X \ i0E ~EMffE # EE
)t )M IM~B 2C
Iekge



fi

{

Tj6T$|Fv

/













mknpokmkq

{



Tj6T$|fv

) R
Tj6T$|9 R

/
)
Tj6T$|>

/

















) R
Tj6T$|- R
. X

/

Tj6T$|Z>

rO3M@>w1,TF?pL3M'fiP vPhi()ElA v


h)?ElE!

T$?3Ip o/F?pfiff
E?
}ffQ?fi8vffvpfiR'fiEF?Q
3Mfi[R3ffG'fi
R G
sJ5+F4? @ff'Rff
5'fi"Lfffi0NfiRffff!

M@ N # )a4)mEM~B /S)1Q)}dME Sag }E)8g6))
m) (
); # 'pMb }bC)BC)/Vr Mff18a[M~;EKBI?> & / f'Ei
9* e$g

gieevb

[3
r 4'$
_ ff7GL
" fffi 0 fi
;4fi7'fim ff pM!ST$3
? @ (bfi
' 'pS

ff#&=4~_$ff8?p fiRp!RrMfiOff
?q

ff&''v3M*:Zv0'
Rh)fi
?p _0E'v'
Y4(f4rO~:;"Lff'%fi fi;l7fi(f?pK'E434(bfi2fid+ff
1 ff'%fi ! 3M%fi C?pYP
E?v pYv ff v pmfiR
'fi 'fi/3
ff
3+ ''v 3G+f?&
v /
_ ffC@ R
ffvp 3M0'v p 'fiC?pI4 p3fi !LT$3
? SQ4f%fi0v L?Q?p8@ _oUfi(,?4
'' pPR
(bfi L?p7 /k
_ ff4P@ R
ffvp3M0'' pPv
430p
ff5P'v [ MfiP@ fffi 0 *a U?p
v /
_ ff@ R
ffv3P3M0''Ev pIMz
fi ffW
MfiFv
23p
Q@ ff'%fi
! ]

v0
P+-C@ffMR?0ffC@ff3>'fiS
fi\?pfip :;fiR'fiE*O+-fpffWfi0
fi4p7?p
;+-fi:;'GfiR'fi!

]3g=< [bO )/}<g 4 ;4)UM~B 00ELM~;ESBO?> -O g6))
' fffB & / L~EMffE EE )/ # 'bZ)/ ( [ )R)/Q/bi~`;SB
) )/~ff )/ # # b}~ffM }) g6))Yb ( b; # ~Mb }E)

Iekge

fi04pON
fi3M0-fi(%4443'X oM0vfftfi(%T$?pfiffL0846
fifi44*30vpQrO3M@5w1!
D<EfiI'fiRffv3Xh)?pYr fiX?pY()Ifi([rO3@YwlE*6*9+F?p@ & h vdlE*` DZ>1*,()fi
"Lfffi 0 fiR
;D>R ] hi
calE! [3O
KfiR
'fi
V fi4
v v p
ff ff
ffp

h j
6T$|91* j
6T$
| >l$ \
34} ffF?p>' 0fi
fi 0fi Kfi ffp
h _
6TF|91* _
6T$|-l
'fi j BZk hi NrO 3M@ NwLfi C?p[ ?ElE!<T$p
? ?pQ''v 37
fi 2'vpLfi(JC()fi4fi\+ ff1v /k
_ 0ff
P Y/]^8h)J*^/*^/*^/*!!! G
l & h v l-a3MP
D>1!5T$?4Fp
? ff0$30$'fi >+F?
V fi 2L
MfiT3ff
ff44
GsX(bfiLW

" ff'%fi IfiR
vff!NT$p
?
oM0 4423''fff+F?
V\ I1
I, " H=I,
3MfiNR
3
ffK'fiIR
[ Gs()fi5L
" ff'%fi [fiR
vff!grfiR
V ffI " H=I, *M3M%fi f?pN
fi 4fi ()fi
h j
6T$|91* _
6T$Z
| >l$2 j
( #k v v* j Z
( k v v 03MT 1?v pfff4 I4f?pO

F Krgv 3M@ >w1X
! Tv *M
+ >
U@ L?pNfia ff(bfifL
" ff'%fi N0fiR
ff!



{8
fi
43pQ1K3PPE}ffvp8?pN%fiLPfia@ff30

V\ I1
I, *V KI *V\ I1
I, ffI N0V\ I1
I, I4vfi

ffPfiEVfi4
dlE!

OK

@ SstN()fi7G0fiR;#h o/@ff40v

fi[

V\ I1
I, " H=I * V ffI " H=I,


V

t}-m{t/s$q

fi4



os0pt{a

l qM


@ GsJA()fiFcff2
NfiRffff!

K?3p7Pfi@ff3


V fi 4*{V H=I, *{V fi 4 H=I, *{V fi 4 vfi *V ffI " fi 4* \V 1I
I, " fi 4*{V _



V\ I1
I, " H=I * V ffI " H=I,

4

Sst5()fi$c+4
NfiF"Wff'%fiNfiff!
V

fi4

V\

fiJH=I

Yfi?3ff
ff~:

@MfiL3ff
ff44 GsJA()fiF"Wff'%fiNfiRvff!

?3Y()
?4$?M@Uff7vpmfiR'fi?2'(b?pU

ff042;X
fi4fi#@
$
3ffI'fi[ GsJt(bfiOcff4
9fiRfft4g`_
*Rff
3-c+E4
-fiRff
@O o/'@ffO
ffm3 (i3O
fiPPfi (bfi> ()1v 3C'/'ffP> P G4PfiN024
fi
pffK
fi 0ff$fiR
ffAfi(g?45
27hiT
ffO
F!*Jvffww QlE!
rOv 2*,()fiuTFp
? fi@ ffPSvY qR+ Uv ff pff&??pUp
? ffPfi(Q?pUfi0 ff()fi45fi(
?pOp @ ff3Q4
ff?3L V HOI fiL V fi 4P'J!7sJ7v G?4[0R
Q+ P@ ff
?pff Q'fi30 ff'fiO
Q'0A1k
_ 0v p8fi@ e
5O?Mfi/-(bfi5p
ff4 p7+f??3ff? C'fi
@ _a
fi (bfi

E?!,ffid
+ ff*/_0'*1v P?3p o/9@ff
fi *1+ Q
fi 43
9?Mfi\+fi3MAL0fi
@ff305@ L' 04ffK()fi Pv p Q'fi0fi103
5r =()fi[

ff
fi !

3\fi :X-: fi
cffW?28@ff
fi+- M@ff'2
ff
fi3 +F?3@ff
E?&fiPv70Z3@ff847fi\+WXv/:


DBE

k 3 R1 3R

34

/430tr 7*V03MN()fiL_a
fi?pfi/3
Nr 3ffN'fiYR()fiOffRR`_$ff!Prfi

ff
fi *[ffv3$fiR'fig2ta4ffN'fiQ7/43fi6r &>?3>?p9fi/3

4Q()fiO
ff!T$?3@(bfi*JN4cpff
ffm'fiY
fi4pN?3'44fimfi(-ff
E?Wff0vp fiR'fi
()fiv 41403'fiYfi130
[r 7*V ?Mfi\+ ?>Rff
L?pP 0fi Gs&@ ff34Q@ ff ff
%fid !
rMfi,fi
E'fi
V ffI " H=I, * V\ I1
I, " H=I, * V ffI " fi 4* 0.
V\ I1
I, " H=I *+ 5
fi 4p
Ofi L?3-'E ~:
4fi 0Qfi(9?p4P4 7fiR
'fiE!PT$?4Q4QR
ff
3@ ?pP' 4fi [fi(A?pff fiR
'fi7@
4P0P' 04fi Afi(O?pff454P4 [
fiPfi 3!9T$p
? N@ ffP v p8' 2fi $@








V ffI 'E4ffF'fiV ffI z<\fiRV\ I1
I, !
V\ I1
I, '4ff$'fiV ffI z<\fiRV\ I1
I, !
V H=I, 'E4ffF'fiV H=I z<\fiRV fi 4!
V fi 4L'04ff5'fiV H=I, 0z<\fiV fi !
V 4fi '2ff5'fiV fi 0z\< fiRV _ !
V 'E4ff5'fiV !
V\ fiJH=I '04ff5'fiV fiJH=I !

cNPff
MfiNR8v 34 >'fiK?pI@ ffp
L?fid+ V H=I
m' 4I'fiV fi !>Ttfi 2443''E*a+
3IrO3M@Cvy/*+F?p@8?p8'4fiU
fi0fi*%3
E?SPhi
dlE*VpMfiIQfi(2\z`]_Y^ 1ffU']

fi ff! 3M%fi .V H=I 4f004 ffS'fiffp
Uh~v*Jl[v S?p (bPfi'Nr 'fi ?Q?pI' fi

fi 4fi 72_
Mfi\+hi|
UlE!OT$p
? 8
p+ ffp
Lh~vv */v lt4gp
ff7'fiQ?pAfi/3
Jr h) ?fi'



fi




R

{

/


FG



v



R

{

mknpokmkq


v

J;

8





{



ZLK

M9







/v




vv



)

HI

FG

R



HI




rgv3M@Pvy/AYp4}ff4fiY
KRff
fiO>0fiKv fi130
!
v rgv3M@7vyl<+F??p['4fi
fi4fiPV!6"Wff
40?09'fi8(bfiE?pffi/3
,r +-f.
?pZAff4fi130
Cfi(8?p2
ffU?3RvEff
fifi(8?pR'0fi
fi4fi!
sJ4. +F4* V ffI '04ff5'fiff4?pRV KI fiRV\ I1
I, ?pNfi/3
5r 7!
TtfiW4423''C+F?
V fi
q
ff
fi
V X?pKfi/3
*<
+ U3@ rO 3@ >1! 3M%fi
+p
ff ?pff3
GhiT$T
" fH ScT$T$cHN8*t"f|-A|,,c :[c~HQLlN Wfi\ ?3' fi R
fi 0fi
'fi ff3
hiT$T
" fH ScT$T$cHN8*,T$T
" fH ScTFT5cHNLlE!,TFp
? &?p\4fi0F'XhiDQ|,sV1c :Q|,"$c~HN8*

DQ|,sJ,c :Q|,"FcHN8*0T$T
" fH ScTFT5cHNLl9R
ff
fiO
fff

ff0v N(bfi v 4%' hB$QstsJ|-5T5cHN8*
"F|AA|<1c :[cHN8*%T$W
" [H Uc~T$T5c~HQ>l51Y./v p 7344fi Q
fi #hirt:;p
ff44 B
qcp:B@ ff
ff s:
' PElE
! D<@ /fi3*?A834fi -
4fi ()fi
ff?p[fi/3
,r 'fifi8'fi hiDQ|,sJ,c :Q|,"$:
c~HQ8*aD[|9sV1c :[|9"$c~HQI*0"F|AA|<1c :fc~HNLlE!
{?4024
fi LfiS?pff@ ' 04fi 8?d C()fiI?p ( pWfi($?3C ff7 v pSfi
E'fi
()fi>?pfi/3
Lr NTFp
? P%fi PUfiEO@ ff3L(bfi.
V I,
" H=I,
* V ffI " H=I, *g
V fi ()fi
@ ff@ 1 pQ+c E4
$fi
ffOR
ff
fiO
Wp f(bfi9?p$fi130
!gT$?04646R
ff
3 V HOI Pff
Rff
fiO
EV fi 47 0
V fi ffYR
ff
fiO
.V !FY
U?p>fi?3[? 0*?pL%fi4 >P0fi%@ ff3
()fi.
V\ I1
I,
* V ffI
* V I,
ffI N
V\ I1
I, I4vfi 0@ ff /v pU26fiR
ff>@ ffPv Rfi P()fi
?pIfi/3
!7rMfi
V\ I1
I, n
* V KI n
* V\ I1
I, IaffI *V
V\ I1
I, I4vfi *V?4Q4P04 ffF?N?pIfi/3

r C3
pffQ'fiKR
I(bfiEO
ff*@ _a
fi mM
fi ffc
MfiL?ff 'fiK
IMfi p*t G?3Q?3@ P4
0mE3
3 /:B4O
C
fi'*X X()fi7300 fi Pv ff v pG3M'fiMfifi30a
! [8
fi 3ff%fi\ *
?pY'fi3M0 ff'fi
Y0fi(L45fiR
'fi@ S3
3 K'fiW?pff
V H=I fi
V fi 4S
fiPfi 3!n+c q?p
p o/ff

fi &
+ Kp
fffi#O
?Mfi/I()fi@ ff3
pU?pY
fiP0 oM;Zfi(F@ _a
fi Xfid 'fi
@ _a
fi K()fi
E
?Y+fp
? K?pff NfiR
'fiEF?d >R
K04v ff!

v

%G{3}?
"Lff
2<?7fiE'fiV ffI 0V\ I1
I,
3fi7
3fi0vffPN+F?R?p(;Rfi(5ff7vpM*
+F?pff;V H=I PV fi F@52'. hi!!*WMfi,N0fi3ffP'fi> SstElE!6r3M?pEfi@*
V H=I, 0V fi 4

3Cfi0vffPN+F?pW?pW@?p ff
fi'WYp+9fi:;&fiE'fiff!
NOi



r fi3$ff*A+-Y?0ffS3fffiRff&v
@ffO5@E`_a
fi&fiE?PI()fiP?pffUfiE'fi

?F
S_a
4pff
@ff7?pN4OL
fiP0 oM;Cfid['fiV@E`_a
fiK()fi

?!
" ff
4,?08?3@ C p+9fim+Aff/7?08fiR
W
'fiE7
X

ff4044pa
fia4#hBLl>fi
fi/
4&hisglE!gr3M?pEfi@ *O@ ff
46?0.V fi
v
@ ff C

ffa44p ff?3>+Affq!h W:
msglE*
+Fp
? ff
V H=I,
Wfi v
ff

ff4044pG4fi1
44XOh W
:
GsglE!J
{ d?EV H=I, ?Lfi 4



fi[

t}-m{t/s$q



os0pt{a

l qM


j fi1
2} ff\
k Rff
IfiZ

ff042;*O+F?pff>?p ff
Ifi(V fi 4CPffRE0?fi3p?P
0Qfi(9?pr >!T$?384P044
fiS4[?Q+-P
?0ff e
Q
@ffO6O?Mfi/
()fi,@ _a
fiP2fi@ffP()fiV H=I, *+F?p@ff,+-[
3Mfi-Mfi744.+F4A()fiZV fi !Oc+fi?p9+-fiff*
fi@ ffi1
2} ffO
Rff
,fi

ff042;I402 ffg?,,2:
ff <'fi8fi/
4} 5@_a
fiP'fiv
'R
ff!6T$?264,4fiL'p
3 f(bfi9fi?;+-fi:;' fiR
'fi9?9?d qV H=I, ,?pff4, ff
fi C'V*/! !*
V\ I1
I, " H=I,
V KI " HOI @ UO
Y'fiZ
@ ffO
7hifi/
4} ffl8@ `_
fi ! ff
30
MfiM
fiG4
v pffX1Z
fi 04p
p8V fi 4KR
*9
+ Yp
fffi#
@ ffO
A@ `_
fi
fi4?PN()fiL?pPfi'p,fiR
'fiE
V\ fiKHOI !TFp
? ff P fi4?PLaU'fi8V fi 4 0R4
fi?p['R
ff
4
ff$fi
( V\ fiKH=I !
{ Q?ff Qp

fffiR
ff;+-fi7pR
ff9fi(Vv
@ ff
0@ `_a
4fi fiE?Pg?Mfi f?-(bfi44fid+
?pC044
4fi fiR
( V H=I, *6 W?fi ?8()fi4fid+?p0044
fi Rfi
( V fiJH=I !Yrfi
ff
E?fi(5fi3M
ff
0v pGfi
E'fi*,fi 3CfiPPfi@ Kfi(F?pff@ fi?82044
!R
(bfi@ K0@ ff vpm?p
v

ffO
fi4?P* 3M0 ff
4fi C1!4v[@ ff ,p+9fiI fi4?P9(bfi-'fi@ `_a
4fi P()fi


E?p
*
ff*fi pF()fi-+c E4
F0fiR
ff< ?pFfi?39()fi-2fiR
vff:
o10@ ffa $
r [*M9
+ ff45 fi?()fi-./v p8?p['fi5fi130
,fi(V?pQr [!<T$p
? ff Q fi4?P
04'fiY
(1fiNHOI *%
( K
fi *0fiL
'2
ff
fi ! 3M0@ ff
fi C1!
fff
@ ffO
V fi $fi(
4-?pK fi4?P8v 3M0 ff
4fi "C1!4v!T$p
? ff fi?0PI@ K044
+F3
? Z?3 v ff v p
fiR
'fi$4
V\ fiKH=I fi$ fi(t5'R
ff
4
ff!,r3?pfi *1?304'fiP fi(O
(1fiH=I *
( ff
fi *$fiK '2
ff
fi3 ! 3M0@ ff
fi C1 ! >?Cv
@ ff
f fi?P()fiK
(1fiH=I,J
( ff
fi */v ff v p7fiR 'fi V H=I */ 0ffc 2
[ 0(i34aL" ff'%fi @ ffi ff,v 02
34ff!
T$p
? $@ ff
fi P
fi
430p
ffO+F?I?pfi 4
0
ffP04
1@ ff36
fi0Ev pN?p54
$
fi0 oM;
fi(<?p8v
@ ff
g fi?0P$+F4?Y?p84O
8
fi0 oM;Kfi(6?38
fi ff'%fi v p'fig fi
hi$
+ ff4F+F?ff
?Sfi?plE!
T$3
? Ifi< Sp
ff4fi0v p 4tfi(,?pv
ffO
g@ E`_a
fi fiE?P[4QPoM4Pj
eP:


!9T$p
? ff >v fi?P5. >?pL3fi ?0[q fiA'fiC ff v 3M*1+f?4
?K
ff
?-
fiE<()fi3
3 fi @ /fi3O `_
fi JhiEl<?ff [@ ffMIR
I_
ff!6T$p
? Pv ff v pQfi/
:

3M7yh V/hB,l, lE*()fi4fi\+ ffY ?p>
@ ffO
@ _a
fi fiE?hi 7rgv 3M@ vdlE!-W
H o/







-3,

fi 4p
-?p /0a
SEFaff;a

fi(V?p[ fi?*+F3
? @ F
+ f3O
TfiP
P
fi :
! [4fi(V?3Q
@ ffO
0 `_a
fi fiE?P,@ ff ff p
? @ Q@ [fi3
3 mhi! !*
+Fp
? 3 ?p&
fi
430p
K?P@ E`_a
fi #3

ff*5P4v &(i
P'p
3 Y?0C 7l8()fi
+ ''@ffk0fiR
ff[ j 0@ ff
fi v ffkP0fiR
fff(bfiQ+F?2
?S?p3fi 4T
o1 ff~:
j Mfi\W
0v L[k
3j
?gr 7!aD[fidW
+ 0''@ff fiR
ff7h)+F?4
E?Uv
23p

" fffi 0 \l5
Ep
? ff
.U@ ffp
3 0
ffffi(

fv Yff%fifip
dp
* ! M!*a+F3
? ?pL
*
)G2A(bfi44fid
+ ffY1C
d!,+c
fi ''* j 3M0'@ffPk
fiR
vffF
3
? ff
.U()fi
Nv U@ Iff%fiJfip
ff
* ! M!*+Fp
? ?3T

dP4f@ ff
ffp
ffS
)J! (+0 DQ ff
fi ffffiR

vff*a30
?m[+c E4
*%4%fi Ififi3
f()fiN
p
? ff
.1v 3M! fiO
7fi(
?p8v
@ ffO
g fi?F@ I4'fiC
fi0 *%! !*%+Fp
? p [?pY
fi
23p
>?0f@ E`_a
:
fi (i44*6I4Iv Z()
I'p
3 ? !qhiT$p
? C@ ffp
?fi34fffi4&
fi M(i3 p j
fia
fi4?PkI+F? j
fiP0 >r 7!^kl
{p
? X@ _a
fi ()24*<IM
fi ff'fim
ff
30 fi(Ffi p fiPfi@ fi*6+Fp
? j fik
4a4 ff5?p 84[fiR
;K/fi44fi ZhB 7lE!QT$p
? @ I@ 8;+-fiC+Aff/['fi @ ff'fi I3
E
? fi!

|,4?p5@ 3M 'fi?3L@ ff ff7 v pIr 8hil- K
?Mfi1fi > fi?pf ff7 v p8fiR
'fiF K ()
v *tfi>. ?pP@ ff3Qfi(5 ff p 03MN@04L?pr IhiElQv fiO
fi
Pfi?p7+-d'fiY_om?p
fid!{?n
fi p o/
4fi *$?p
fi0v G fiE?PCv #?04@ ff
fi k
_
^a`y`L'Ep
3 fi

POdRQK^^^ qd'B<BE~59^'B^ d~BTo'$BO\iB'8[E\NmV\iB'E&d^iB\B^NV~^-B~Tl
~EE%B-B\6E\^^'E\^bQE1B6^B5~;EE^BB5J\~pB~[BE

O$

fi


"





"











"














"


mknpokmkq








"









"










0M

"







Tg0PvFT$?3'fiR(i33
4fiR()fi8fis<]^8r z04!YTF?pfi\+F8
fi@ff'%fiZ'fiGff
Y?pL
fi43

0$
fi@ ff'%fi 'fi7344fi $
4fi!

v'fi/3
ffm1mvffvpM!T$?pPfi4?PN?8@Mfi7
fi0CPff4'fiY_k(i4Ofi!
fi?0?0O_kO4/
Ifi 4>'p
3 ?fi6
@ ff'fi4 5?pff )fiE6v ff?p6fi(?p5;+-fi
+Aff/X
! Cv fi??5M
fi ff
fi,_k04k
fiE9fi-k
_ 0,(i4 [fi pff-@ ff13@ ff-fi@ [@ ff''E4
ff
fi6
ff'fi43Mfi V!tffc 804
32ff*6
fi >
53@ ffI+F?8?pA_0O?Mfi1I()fi6@ ff'fi41v 3Tfi*
+F?2
?>
fi 04'tfi(
?Mfi1fiv 3[ Mfi?3g ff7 v p$fiR
'fid!gT$3
? 9 fi4?PV?0g@ A'fi3
3 0L03MjMfi

fi0v Chi
Sk
_ U()4@ fil5@ 8fid 4Y
3Mfi3!F+c Ufi?pf+-fiff*0?pUffK@ ff
fiP

fffi2v pPPv ff v pIfiR
'fi$+fp
? Yv (i
$?pLfiR
'fifPff R
L( L'fi04!
()fi@$@ff@v3[?3$v
@ffOMfi4?P*+A_0'Off<vfi?PO()fi<'fiM':

`_a
4fi7()fi

E?!6T$?pff5fiE?PgMficMfi639?<vffvp[?gfi1

3@ff*8?p
04P'fi4%30fi !6T$p
? @ Lfi 3,'h Mfi544fi@ ff (bfif ff v 3lE*03Avff?
e
*
? Kfi3Fv
@ ff
Vv fi?Pff!

} gieh.f]$^_hS9e ]vBT;H gfi[F]{i[e^ $ekgi[%P ekgy8[J_]{[ek^

b

rMfi-40fffi eC

*2/fi(afi3M9fi?63O5?0,r f,@5@@ff@ffP30vp
70 Ffi(J?pF' 0fi ()3
3 0
fi VU1h (M ^/l,
. *+F?04
?O
ff 9?9()fi5' ( *1./vpI
fi
^X ff'fi p o1' . *5P?Mfi\W+ #v #Tg0 Rv!n"$fi\+F
fi ff'%fi X'fiW'ffC#
fi430

fi@ fffi 0W'fi834fi
fi !T$?2>@ 0@ ff fi X4
ff34 >'fim?pKfi /432
v3 Q@ 0@ ff fi Yfi(OrO 3M@ ffFP
>1!<ffc K02
34ff*MTg0 PvL4?
ff13 5'fiP?p>r
vGrO 3M@ P>(bfiQ?p4 0p
Nfi Ls,!ffc mTta Cv*t'ffL@ 14ffG1U?pff4F_0E'[v'ff*
C?pL834fi 5
fi 5@ L0@ /4ffC?pff-_0'A'ff!,rMfiL
oM0 * j
k
ff
fi7rn. ff7
4fi hirt: fi4 ff
ElE*gc[. ffCh)cp: g ff
ff4 \lE*O 0Rs#. ff his: E PElE!T$3
? 0

fi 2'<fi
( ' ff9(bfi9?pT3 o1-'*M! !*14,
fi@ ff'%fi 06'fi7?3F' fi (i3
3
fi Vj
! j yk8v
?pf0 QO
ff 9?9?3@ f4 Mfi8fi0 A' 0fi (bfi-?49' :;
fi K04ff!6c
pf43fi
v m+F?04
?m?2Nfi1

3L4Q+F3
? R R
fi Z4c
Mfi>44fid
+ ff(bfiY'!K-fi 43
L o/P0
3 fi(9?p0 P()fiPL()fiL
_ :;'3'fiP/! f

fi0v pY'fiY?3_0'hi3R
L (bPfi'El
'v Tta Yv*g4(9sq2Lv 'CT$W
" [H Uc~T$T5c~HQh j T$klQ Rr. ff8
fi ZrJ:;
fi2 ff
*tc
. ffIcp:B@ ff
ff *< Zs. ffsV:B' h)+F?2
?R'fi?3P4>834fi 7
4fi j
klE*6?p
s+f449' 4fi X'fiRP"F|AA|<1c :[cHNh j "FklP'*5! !
* U1hiTN*,
El "L!<{?X?4Ia34
@ ff 4fi v
* V\ fiKH=I 4$4a ffO
ffK$R
3Mafi hi73M4fi al-fiR
'fiF?0F
? 3ff
Z0 '#'fiX Mfi?p 0MfiPX
?Mfi@ 4p
3 S(bfi ?pp o/C'! [R
'fi
V H=I 4C
R3Mafi fiR
'fi-?-
E? pff-7
''fi8I
p o1-'Q@ ffMPR
ff pLv P?09fi\+L!
rMfiI
o/a _
* 34}ffv 3C?3P' 4fi
fi 4fi Rfi p ffp
mhiTN*^"[lN
RR
P

fi042p
? ff
1
? 3v p8fi pffi(J?p[yA'fiI "v ?pF_a',fid+fi(JTg0 Iv!<T$?494,R
ff
3@ f?p[' fi

fi 4fi I'fi/
4ff+F?ffp
>hiTL*^"[lg4g?p$ 6fi(4/7304fi <
fi 6?6'E fi 7()fi



fi[

t}-m{t/s$q



os0pt{a

l qM




W % P
[ Z\L]_^"\!PX %
Ha`
PX b W c`#\
X ZY
edgf
h X NPX
PX _s
\ P
X `#\I`
z 0l ei i_jkalnmomplqi_jr
042 0l
`#\H X 20 ti
` z j ^ l t\vd/\ Yw[
`
`#\ X Z
` !| j ^ l x{z
|\ i?[ x{z
a`#\
`#\
* 204l



k lnmomplqi j r
j u
[ PX





20 V

yx{z
X y|
l x{z

`#\H` [ Z\}d~\ Yw[ P

j
k
_
j
r

[
[
[

[

!|\
x{z fpfof \
|
x{z 4 `







rO3MPvv;VM]_^a` u Qfi/3
$fi4?K!
T'fi "L*!!*
*%$7vSTt0vv!FT$?2$f2L o/@ffffSvU-fi1fiffG8h)cp:B@ff
ffP
s:B'E PElQhi@>rO3M@>lE!
rMfi,
( K
fi fi6
'2
ff
fi3 *fiV'fi[`_
fiL?p-7304fiJfi/3
tr &pffJ'fi
R$()fiO
ff(bfi?p[v /430fi 5r fQhi Qrgv 3M@ 7vdlE!6
{ Q
40v ffO
,?pf fiE?
VM]_^a` u L()fiT 3vpC?p8fi/3
Fr 30v pP?37 ''30
3M@ >fi(<Tg0 v8f?fidW+ Gv
rO 3M@ Svv!K+c W?pfi130
>r >*t oM0 fi/3
>' W' 4fi R4~
U1hBA"OTN*gMEl
DQD["R
ff
3@ U1hB[*MEl[ D8w
* U1hi"L*MEl[ D8* }
UhiTL*%Elf"()fiQfi Nr-*%cE*% Gs<*
@ ff'R
ff
ff4!mTFp
? C 4<fffi($?p fi/3
8r @ C(bfiEO
ffRffvpm+Fp
? ?p

v 0123V'fi(-?pIfi/3
Q4N
v 04J!Irfi
o/P0 *V(,D8*VD8*V " Pv 4
'ff[()fi[r-*0c*a 0Ss,*0@ ff'R
ff
ff4*0?pGDQD["+f44%R
> Gv 48()fifr c s<
! F()
()fiPv pW?pUfi130
'ffK 'R
ff
(b/v p+f?4
?@ mv 4B*,?pm fi?0 fi(LrO 3M@ vv
'R
ff
`$
_ ffA?3~U7' 4fi (bfiL
C0fi103
5'7 U7344fi $
4fi !
HffiP?L?3P fi? rgv 3M@ Kvv()fiPN?p0fi103
[r zn()fiLff'v3K+c E4

fiR
vff!CTtfiUff8rO~:;L
" ff'%fi 0fiR
ff>30v p\6T `_a
4fi *t
+ 3ffR'fiG(bfi?p

fi/3
-r !<TtfiMfi8?4-4048 ff34@ ff9
fi 04p
p 'fiIR
f?3Iyh U&vdl'5fi!
T$p
? G fi?0 v rO 3M@ XvvS2fi/`k
_ ff#1#
? p p"U='fi U}v +fp
? @ !cp 4C4'fi
4%fi$'fiMfi>?0[v^a`y`<43fi 8hiv
430v p
(1fiNHOI lEv
* qVJ]_^@` u >83'fR
ff
3ff
'fiU()fi?pPfi/3
7 ?
( 6T _a
fi W2L'fiYR
Mfi 3!ffc
(1fiH=I,J *<4 @ 3L?p
v 3 Pfi 8r X
U#4v!rMfiP
( ff
fi
* U# vP4'fiM!Cffc fi?p>+-fiff*J()fiI
( ff
fi ?p
834fi 504 *1fi
N()fiO
ff*2
3 F3M/4p
ff ?p@ ()fi@ L5
fi34 R
Q
fi 43
@ ffK4.
Cv p 8fi Q02 !$ffc U%fi?Sfi(<?pff
ff*
( 6T _a
fi G4ffi p7?3>fi/3
[4f.
fi(O?pLv 3 Q04 Kr K?pL0fiR
; r >!
Q4 ?<?pFfi/3
6r ?06R
I(bfi
ffPk
( 3ffp
ff*?3I?p5k
_ Vhi83)lfi ,r

RR
$
_ ff!
{ P_a'L
fi 2p
>K 20 Ifi/p
ff<
3
? ff
./v pU fiE?K*J
2 ffG
VM]_^a`y*
4fi ff&'R
ff
`_a
4()fi ()/v pG+c E4
fiR
ff8fi(f?pK(bfiE
] )t!ZT$p
? K fiE?K*
?Mfi\W
+ PrO 3M@ >vff1*M
fi 4'<fi(V>p
?/:_0E',ff
E?fi(tR
v 3v 3Q
? v 44M'X
!


ff40 F'fi ^ %?-4606fi(>' 4fi
fi fi V*+Fp
? @ ^aq)J*/fi4ff,?p$fi
;!
h)
{ 7'fi@ L?pL Ffi(O2'fiPR
^aq)U()fi$04K

ff! l

OJL

fi

mknpokmkq

H204l
`
`#\H X ti~ !# %
2
`#\
`#\H XJ tiJ !# %
` Z2% PX `#
`#\
* 0l%2
H204l `#

`#\H X8 x{z W [ x{zV [0
` |
l x z 8PX l
`#\
`#\H X8 x z W e | i8l x z
` !
! Z*% PX
`#\
* 0l%2





`

I\ `
%
6

`# 4 `

rO3M@Pvff1qVJ]_^@` QE`_a
fiYvfi?K!
H o15+L
fi2p5Kfi? ()fi5()1p.^@UWP0fiR;P+f?MficpfiY2- o/@ff0

N k3j
?Or 7*v
430vpPrO'~:;"Wff'%fi8fiRffff!QTF?pI@ffpNPffS+F4?S'fiK@/+?p
? ?M:; ffp
ff
4fi[fi(/?46TWPfi1pff
E?pff
./vpffi?0@ffff>v 30ff
fi9>1! >AR()fi@

fi v30v pM!Orgv 3M@ Qffv > ff6[04
- 4fi 7fi(a?4O fi4?(bfi -fi3
fi3MR
4j
6!Ah~vffwwl
Pffi}ffP $O,!$h~vffww ClE! ((
{ [
4M?46v fi? qVJ]_^@`!RYR
ff
3 F<26'fi03M'fi:
?pfi@ 2
L `_
fi !9W
" ff
4V?0Fv\6Tfi13
ffV
p
? ff
.1 pM*0?p>fiR
;K4$@ @ ff ffUf
r 7*1 './v p7+Fp
? ?p$q 2X
ff13 ,'fi'./v p>+fp
? ?p & hB,
l & hi8l<()fiAfiR
p
!YT$?04L4
ff34 L'fi & hB<
l u & hi7l8 1*O+F?2
?W2L fi?0P4
4Gff'ffZ./v pS?p
'fi50fi103
,fi(t?pQ04
r ?pQr =
fi@ ff'%fi v 3>'fi !6c( & hB 7lA 7?p
l & hi7lE*/! !*Mq = `_
fi 30

ff ?1fi?p+F4@ *M
`_a
4fi P(i44!
& hB<Z
T$p
? Q fi4?fi(grO 3M@ Iffv >83O
ff-?A?pYpfi fi(t?p[fiR
;Gh 7l,4
o1 ff ff
P*
3j
E?V3M'fiP'fiS K?p7r =R
ffv p ffE
p
? ffS4F P!
[ fi?0 qVJ]_^@`!R<*Vv Grgv 3M@ ff
v >1*J
34S
p
? ff
.1N+fp
? ?p> ()fiL SfiR
pU!
TtfiP
p
? ff
.C(O
*M
+ N
Yp
P pf+fp
? ?p & hB 7
l
1!,TF?4-4-'p
3 N(V?p Q25'fiO

j 0km'Kv hB 7lN ff
?0v C()fi Xv 2,'Y Z@ ff
E?0 ()fiff(*<! !*
09fi(J C

ff0v f
/
[ 0P?p@ ()fi@ [12ff /k
_ ff4>fi()!,T$p
? f fi?fi(VrO 3M@ 8ffv >
R()fiP6?2<
p
? ff
.3v pL
3ff'ffp
0?/:_0'9ff
?fi P?3Ffi/3
<r = !6T$p
? $_0'
?M:_0'F@ff
?GR
p
v ffv44V'ffQ S12F2J

ff40 7ff!F{p
? p NC'
hB 7l-254
fi\ @ ff*0$25
4 ffY j ffV*^k YO
3ff'ffU ffE
?YR
v 09'fiCfi1fi.()fif

/
7?0f@3 $'fiC?pI ff!fcp(6?p 84[C
1
*?4[4a4 ffA?p hB 7lLhi@ ffalf'

mR
>/4ffGv /
_ ff fi()*V S?p@ ()fi@ I?p4 p3fi 4
Mfi pff0;hiB! !*?3@ 4Q'fiO


fi ff3
3
Lv K?pNa4 ?FM
fi ffL
Mfif4'()?3NfiR
;l- K E`_a
fi K(i44ff!
%^6^EBB\n^O~L^LB;m%~^Tl2ffNmV7d^LidiB~=Ag~$MNrOrff;`U5ff\To'B^Bm%EO$E\<B
Bt\^^'FE^BB&`JB~^^)\EW
m%E% EJ1 w5ff`VBO'
mVBg^V^J\^;^OB-E
BVB
o\BiJ BT o'B^QB'a%^V5ff\T o'B^Q$ ~%Bg~iB'F'E^ oBiJ^[\B'\dB<d`~

Offx

fi[

t}-m{t/s$q



os0pt{a

l qM


H204l
`
`#\H X ti~ !c,W
2
`#\
`#\H XJ tiJ !,W

` Z2% PX `#
`
`#\
* 0l%2
H204l `#

`,iBM !B,W PX

`#\ X tiJB !B,W %
*
a`#\
`#
`
`#\H X 4\ f [ a`i
` Z*% PX `# 4 `
`#\
* 0l%2
H204l a`# PX 2 XJ
%
`#\H X 4\ f [ a`i
l I2\
` ! Z PX P eP ffi

` = Z*% PX e a`# `
`
`#\
* 0l%2

rO3M@Pvff>1;VM]1^@`gR_a
fiYfiE?K!



fi

mknpokmkq



W %
PX ZY [ w. _`0
[ t\-\ f
\Ru_ ruP `0 |\ z [ x nPT * z #` \H ti z *
Z yx TPT f
\L]^"\PX ` PX b W `#\ #
d@f
h X NPX \ PX _s



042 0l
`#\H X 20 ti juk lnmompmoli z lnmompmoli jr `#\I``lD
` | j ^ l x nPT
`#\H` [ Z\}d~\ Yw[ PX "|
|\ i8[ x TPP !|\ j k [ x TPT [ fofpf [ #z! [ fofpf [ |\ j r [ x
`#\
* 204l



rO3M@Pv MZ



Zi z

l x TPT
TPT `



UW[ u Qfi/3
5fi4?K!

VM]_^a`y< 0B VM]_^a`R@9fi33>0>
fi0v[h)()fi^@UW[fiR;Q+F?fiXpfi84# o1ff~:
0vQ[P*3j
E?Jr NlE*0U?pC_kK4E`_a
fifi!-(bfi@Iff4%fivpfiU?4*_0'
MfiN?5?3N
j `_a
4fi fikI?50`5
fi3Mfi4fiK(bfiqVJ]_^@` QqVJ]_^@`R<!
rMfi
VM]1^@`y fi>2L R

ff40 0R'fi hi! !*t Z'fi ^)Z+Fp
? @ ?p0fiR
;m4
] )%lE!$rMfi
VM]1^@` g F4f U

ff0 70U'8?Q4$a$fi(<C
1
!$TFp
? L@ ff'fi VM]_^a`
4A'fi3
3 0C29?0$Zbafi 5)
fiE9fi 0P?Mfi L'fi
^)J!6cp$2-
fiP0 N Ck
_ 094
fi
Rff
3 [9M
fi ff
o/?03' [ ff
? ff'v37fi(V4

ff40 f'fi
! VM]1^@`gm4-4'fi7fi3
3
K
fia *()fiF fi fi3$@ ff'fi !9
ff
30 BVM]_^a`yN
VM]_^a`Rmk
_ K4R
fi*/?pK

R-30 ff>+f?ff4?p6O
?Mfi/7fi$
( fiO@ ff'fi43fi ChiB! !*
?Mfi1fi $ Mfi?pOfiR
'fi6fig_o>?3-r NlE!
M@^]v $ekgi[%| e$g \V fi KH=I

bFE^Zg

[J_]{[ek^

]$^_h

[4fi(6?p8fi?FvK?3>@/fi3F3M0ff
4fiU
S7''@ffP4pff S?f4f45.Mfi\+W
?N ffpfiR'fihivS?4[
*V\ fiJH=I lf?[RG004ff!QrMfiL2044
4;*a2tfi(,fi3M
fi4?P83OV\ fiKH=I 4802ffR'fiGvpC'fi hi7304fiI
4fialE!rMfiO o/P0*
+I3O
8?N
( U1h J ^ fi fi, l$ 3 p*%?pGV\ fiKH=I h Uh ^ fi fi, l'l$ 3 +fp
? @ 3 , 3 @
'ffCh)fi7y/*t20/v pfM\
fi p o/L'\lE*g 0X
^ fi fi ff 4NY834fi L
fi !
I+ 30 ?p
0325@ @ ff fi *0?4A' 4ff$'fi
E? pv 3fi pL0 c'!
rO 3M >v 7?fid+F9 v
ffO
fi fiz
( qVJ]_^@` *1
4 ffP
aUW[ *1+F?4
E?P462fi@ ff()fi
@ :B()fiPv 3?p7fi/3
Fr ()B
V\ fiJH=I ?fR
Sa4 ff!$T$3
? > fi?0 fi(,rO 3@ v 4
()fi,ffc 4
FfiR
ff ?(bfi
6T# _a
fi
E? pUK'fiEURvFv ?p[ fi? 3O

? UW[ u N45024
0 Qv K443fi A+F3
? K.1 p?p
2A?pyh Uvdl'Ffi!-[?Mfi33
fi/3
5+f? ?pLfi
; r >*?3L4fi4fi U(bfifp
fffi0v 3?4$ fiE?z+A
?pN834fi FR
K
fi !<W
" ff
4?5vC?4-34fi z
* 4
[ ff7 v p84A04 ff'fi

mv 41403fi 7r >*?pIfi/3
Q730'fRI :B()fiO

ffG'fiK ()\
fi0gfi
ff!QT$p
?
+A'ffU
fi'Ffi(Ofiv pI?4$PfiffY?p>p
fffiaO
Ffi(t?2$ fi?0K!
[ fi?0
UW[ u U3O
ff??3Sfi130
P+-()fiO
fffi v 2nh)R
()fi@ ff7 v pl
3 pVM]_^a` u
! UW[ u C
04} ff>fi ?p
. Mfid+f ffp
Cfi(5+F?Khiv /430tfiI834fi El
'qh MBl 0n832fi K
fi yh ^ fi fi, l' fi 'fiX p+ p o/K'K'R
ff
`$
_ ff

fiR
'fi2
V\ fiKHOI !OT$?46 fiE?30O
ffO?<?p50@ ff ff pFfi130
n46''fi ff!6T$p
?
?pfi 4Sfi130
Qr 'ff>+F?Mfi p o/L'fpffQ'fiYR
Pfi/`$
_ ffm@ ?fi 'ff>?

=K

fi[

t}-m{t/s$q



os0pt{a

l qM


042 0l
!#
#` \H X 20 ti i_jukulnmompmoli z lnmompmoli_jr `#\I``lD Zi z
` PX ! # #e
`
` | j ^ l x nPT
`#\H` [ Z\}d~\ Yw[ PX "|
l x TPT


P



P


|\ i8[ x
! |\ j ku[ x TPT [ fofpf [ #z [ fpfof [ |\ j r@[ x TPP 4 `
`#\
* 204l



rO3M@Pvffx1

UW[ u fi



fi130
tfi4?KJ[4fi8fi(zaUW[



u 9?p+&v04'ffff!

v
23p8MJ'fiPfi
fi^ fi fi, !<T$?3fO?Mfi/(bfiA@_a
fiC?A493Off'fi
()fi4fid+ UW[ u N4A'fiV@E`_a
fi*a!!*VM]1^@`yQfiVM]_^a`R<!
H o1*9
fi 4p

8 Mfi?pI08fi(5fi/3
8Z@ `_a
4fi ZfiE?P7?I4 o/Rff
ff
'fiR
*Mfi\ 4*afi444
Sfi@ e
v !AT$p
? fit45'fiC''@ffP4 pN@ `_
fi Y()
V\ fiKH=I !ST$?2N@ ff13@ ff7U( + 40 P
E? pffI'fiG?pv fi?Pff!UT$p? Pfi4fi W()fi7?3ff

?0 pff92<?9+Fp
? Pfi/p
ff
Ep
? ff
./v p8MfidT
+ ''@fffiR
ffa
* V fiJH=I ?<fi 4 j fidW
+ '@ff
Rff
*^kI! !*6fi48Rff
,?3$


ff024;8fi( 4
ff, 0P'fiP,4@ ffP1.
V\ fiJH=I fi<?fi
?$+-fi34 R
N/4ff 1C `_a
fi Z ;ff 8?Mfi > ffK
V\ fiKH=I !
-fi 04p
[?pP
?0 pff!8
{ P'N1U0324v pP Nfi(9?pAff4 Gfi/3
Q'ffe

h

l7?@

K

)
(

fi




ff

X

)
(



fi


?
p


'




J[?+APRff
ffq1Z ff7




v
p




!
T$p
?

(
_09+Aff?A+-N
?MfiC@ E`_a
fi 4-13v 3>?pff N'ff5A?3p+v 040ff
()fi>@ _a
fi !Pffc R(i
*g+ 3ffRfi ff ff
>?Mfi ?0L+- /4ff3v p ?pPfiE v
`_
fi hi! !*9@ K

ff0 C(bfi?3CfiE v -v 2,'ffElE!Z+c fi?p+-fiff*<3M%fi

()fiFfi qY'v
* V\ fiKHOI fi/`$
_ ffL
U1h J ^ fi fi lE!9T$p
?
+ N@ ffv 24} [?pL $fi(6v 44affF'fiPR

0WZ4<fi130
>'ff8(bfiEO
ffR()fi^
M5?8+@ P. ff j /4ff0kY3M p @ /fi3
`_
fi !6T$?46
R

$Mfi p$1fi/()1v 3[?3$fi/3
< fiE?fi(VrO 3M Lv 7-?MfidT
+ Pv

rO 3M@ Pvffx1!9T$p
? L fi4?zfi(Orgv 3M@ Pvffx4$'fiP(bfiEz?pNfi/3
$r ()fi$ ()1 pI+c E4

fiR
vff!TJfim()fi?p fi/3
>()fiO
6T `_a
4fi *,3Ma'3MahB 7l>(bfi|
hB<l8
yh UvdlF(bfi UWv GrO 3M@ Cvffx1!f
{
4t?4[v
ffO
tfi/3
[ fi?0 UW[ u fi *a+fp
? @
Mfiff$?p>()
$?0$+-7@ c'vpfp+v 2ff!
j HfckP3
T$3
? P ff
fi R+Aff'fiG''@ffP2v pI@ `_
fi W4Lmfi G
fi 43
v pKY'E fi mfi Z
:
fi ^ fi fi *Q?pR
fi =+F?Mfi UZ4p
3 m+AUfi/`$
_ ffn1nv ff v pM*f(bfi ?pff ap+v 4
'ff!TFp
? @ ff()ff*$v
@ ff
F@ `_a
4fi #fi/
ff
o/
#2. Y'fiIh)@ \lp `_a
4fi !
{?W?3ff C
E? pff
* qVJ]_^@` R
ff
fiO
ff
UW[ fi *<?MfidW
+ Zv WrO 3@ Sffv C1!msJ. +f4 *g+F?R?3ff

?0 pffZ
VM]1^@`gmR
ff
fiO
ff
UW[R fi *MA?Mfi\W
+ v Crgv 3M@ Ivd!,rO 3M@ 8vd8?Mfid+f9fi 0P
? 3ff
'fi8fi/
ff3M@ ff<()9 P 4(b
?0M()946?pQO
[9 PrO 3M@ 8ffv >1!9Y
p$
_ ''@ff4v v 3Lp
ff
'fiB UW[$ fi *03MjMfi UW[g fi *4t?OfiL?3p+&v 04'ff,?ff j /4ffkf@ ffv 044} ff
'fiKy/!NT$?4f

8Mfi p8(bfiQffc 4 0
7fi
ff$R
ff
3 I?pS@ fiQ
fi 0
pffG+F?S?p
+
(
.
fip
Ffi(g'fiP[v K''Ev p!

ff%d\EynPn$E\-N1mZ'\E7aP;ub-_m-ff^^B~PEP\B~ff^\9 pBo'B^EO|u)ffEBE\~
\BE)P` Bfi;BAB6BEM
a;B~7E^aB5OB6~E8-d^B'7 Bp
Zm%E8B'ff8
~~LB~iB'L`,B\-dBbZ
gLBAB<E mV~>B~iB^ Bi} lJ~\A\BEB^~6B-;\<

=

fi

mknpokmkq

042N `
`#\ X > ti~M !# %
2
a`#\
`#\ X > ti~M !# %
` Z2 PX a`# 4 `
a`#\
* 204l
042a`#
%
[ X 2 |
l x nPT [ PX
`iJ !#
` !x TPT PX l I\ `
` Z2 PX `# ! 4 `

`#\H X8 x z W
[ x z V [0
` |
l x z 8PX l I\ `
`#\
`#\H X8 x{z W
e | i8l x{z 6%


` !




! Z*% PX `# 4 `

`#\
`
* 204l







rgv3M@PvffC1aUW[$

fi

Q`_a
fiUfi?Y!

aUW[O fi $2,'fi33 0P()fi,ffc 4
ffiR ffff*
aUW[g fi $49'fi3
3 P(bfiAPfid+W'@ff
fi4@ff
fiffIfiR;Z+F?Mfi\3fiq4 o/@ff0 *3j
E?$r 7*<v
430vpGrg4'~:
" fffi 0 ffc 4

L
! [3v pY z0fiQ'fiU041v 3V\ fiKH=I 'fiY()fi *t(<?3ff
v

ffO
% `_a
fi Kv fi?P5
fi 0
43p
Q?f *M?3K'fi@ _a
fi +-fi34
4'fiK
fi
23p
I?7 !IL
" ff
4O?N'fi<@ `_a
4fi 4N'fi3
3 V!>T$p
? @ ()fi@ *V?3PO

4['p
3 8()fiN?pff v
@ ff
fi4?P!Lr3?pfi *?3ff Pv
@ ff
g@ _a
fi m`:
fi?0PP+F24-k
_ &4Ffi(N?pp+ 14fi4fi fi(N?3UfiR
p&v 'fi130
ff& V\ fiKHOI !T$p
?
@ff'fi Z?pC fi?>@ C'fi3
3 Z k
_ 0R4X
3C
+ fiEh)()fi8Mfi\W
+ ''fffiI@ ff
4fi ff
fiR
vffEl<29?05?p@ N@ Nfi ;+-fi+-d15?0$

ff024;

Nfi/`k
_ ff1 fi(tfi3M
ff
0v pLfiR
'fiff*1v 0
43v 3V\ fiJH=I 6fi/
4Ifi 4fi04!6L
" ff
20?Afi/
0
E? pN9?p


ff4044pYfi(9'fi ^ fi fi fiN?pPUh ^ fi fi ff lE*J
fi0O
E? pPQ?3I

ff':
a44pUfi($'ffIfiI'fiP8?>+-fi34WR
P12ff ;ff Uh ^ fi fi, lE!Yffc pff4?p7
hifi/

fiY
fi0bl$+F42?pI ff v 3fiR
'fiNfi/()

ff042;Kfi(,'fiQfiQffQ14ffUR
()fi@ *
03MfiL(bff
* ^ fi fi, !CQ3M7 fi?Q@ () oM?3 ff&hi! !*J?pm@ E(bG>83
?Z
'fi-@ `_
fi W+-fi34l[(bfiI49'fiPI Zff7/44ffW8fi8()|
^ fi fi ! v
P?3ff
v

ffO
g fi4?P$R
()fi @ `_
fi oM
Y?38
7+AffGf?3ffF'fiO fi
BE5O^OB~^~EE'90 7, m%E6\Bd^E>d^B'^\~A<5^O\E6-~~>ff^^B~> B\
\\B^FEa P_M~\ F\f<,mKiB^;mV^Bf<1mYBZE;\p%B5^ff^BgB Bi}lJ~E
\BE)Z%\B`B#
>\~'dfBP>Bd^B' ` Bi} lt~E7\BpB~Od[\Ef`Equ)ffEBE\~
\BEB^~~

=J

fi[

t}-m{t/s$q



os0pt{a

l qM


H204l
`
`#\H X ti~ !c,W
2
`#\
`#\H X tiJ c,W
` Z2% PX `#
`#\
* 0l%2
H204l `#

`,iBM !B,W PX

`#\ X tiJB !B,W %
*
a`#\
`#
`

`,iJ ,W
X 2 |
l x TPT [ PX `# !

`#\ X 44\ f [ 4
` Z2 PX a`#
a`#\
`
* 0l%2



rO3@PvdD,fi1
ff03M@ffA() YM(iAfi(O?p9

UW[ R

%
`

! Z2 [

`i%

`

fi



`_a
fiUfi?Y!

MfiU()>?3P0Lfi(A?pr ?7+-7fi/`_$ffm1GvffvpM*J?pm+F44t_k2p+ fi
v'fi/3
ff 1C ff7 v pM!
aUW[O fi Y2P
fi0 S(bfic+4
KfiffRff
3U4|bafifi30vpm?pGO
O?Mfi/#P
VM]1^@` *- #
ff
30 Yffc 4
K0fiR
ff@ G@ ff
fi 0 ffI #@ U?p@ ()fi@
4R
/fi3P'fi?3Gfi/
fi nfi(>'fiPK X'' pM!Y
n?pGfi?pK? VR
* UW[R fi G4f
Mfi

fi0v S()fiC2FMfi\W
+ ''@ff fiR
vff!rMfi oM0 *$C4
fiC
fia U()fi fiR
ff
?5
3
? ff
.C(bfiA?p6'YB ff]6fi/

3M
Qfi(J80' v C8'v pM
* ! M!*rO'~:;W
" ff'%fi F0fiR
ff!
ff
3 | UW[R fi >M

fi ffT
fi[43
() +Fp
? ?p[?pp+v 4J'ffN@ 8
()fi@ 7fiQ()Q?p
_0fi1

3M@
*?p@ <4
fi5+-d['fif
. fid+W(?p6_0E'fi1

3@
<4V
ff p5
p
? ff
. ff7()t ff7 v pM!
H ?pff ffff*?04F4
.Yfi(,
fia pfff(bfiQrO'~:;W

" ff'%fi 7fiR
ff$
304K3M 0Ffi3Mf'fi
LI K3 (i3%'*FR
+ >+F44%4
fid Fv 3M0 ff
fi C1!x1!
b



M@^]v $ekgi[%| e$g V H=I,

^Zg

]$^_h



1fiH=I M@( ff
fi
(

{3 o1L0@ffLfi3MN_k06;+-fiSv0
@ffO6@`_
fiWfiE?P*J+F?4
E?R@a44
0
fiv q
(1fiH=I q
( ff
fi *-+Fp
? X?p Y4fi pYr 'fiR@ 4(b!qTFp
? ff Y@ K%fid
+ (i3
fi4?PAv CPAfi(t?pff5
0042;P'fi ff3
Q?pL
fi0v o/pPfi(g@ `_a
4fi !<Ffi\+ d*
?pff$fi3
3 3
pff9 ff4 ffAfi ?pL3fi C?F?pL ff p7fiR
'fid]^)
ff
ffiK

ffa44p

=K

fi

mknpokmkq

H204l X 4 4 2
`i k .
l 8[ PX % l 4% f

` !/ PX J l 4 f
N % PX 4Na` Pr f `
`
* 0l%2

rO3@PvffQ12

UW[ H=I, Q@`_a
4fiYfi?0K!

4fi1
2}ff*5!!*5?C4V HOI +F?n (1fiNH=I,M fiY ( ff
fi 03MMfiK'2
ff
fi h)+f?p@
V H=I, P ?NRff
fiO
PV fi 4\lEO
! R4%fiQMfifi(-?pffPvfi?PN2f?0L?pp
@ff13@ (bfiEPv pGS0fi103
7r 7:
* MfiI n *,@ ff>fi(f+Fp
? ?pI?pCfiR
pW4
;1R

" ff'%fi @ !OTFp
? N fi??
v e

R
ffv p|=VM]<K44fi@ ffC'fiI'R
ff
`_a
[fiR
p
;1R
^@Uo5P'fi'R
ff
`_a
N ff 0v pIfiR
'fiff!9TFp
? Nfi @ ff
7v K3
fffi0v p?3ff > fi4?P5+A
PoM4P
e
v
m?3@ (bfi P?pS
`_a
P
fia pffL 0z
<\fiN?pP042;Y'fiYk
_ m4
fiE!
T$3
? ff ;+-fiqv
ffO
Nv fi?PK 4fi@ ff()fiK@ `_
fi ()YfiR
'fi
V H=I, !
[3O
>?f0fiR
;K ?Mfi45()fiQ#4fi$'fiCv ff v pM*a! !*% =!$HFfi\++ Ip4} 7?p
' fi U
fi 0fi h (K * l5 K'fi (bfi 14
V H=I h h (ff * l'lQ 9tM*%+F3
? @ E
y/!<
{ 7+- $'fiP E(bC?[ P!
3Yfi 0-3

k
_ fi X4
pffp
ff&R
()fi@ K@ ff vpmfi3M fi?!&
{ Y@ /fi3
k
p
_ 3ff+F?KYOff ()fiU&
:;'Z(bfiE732
)'fiq
m'3
3 mS&
:;'*L03M 'fi#204()
?pL fiE?P-
+ L4'fip
k
_ pQ+F?0$5Off A()fi$
:;'>()fi8347'fiPR
['Ep
3 Qfi(g8' fi

fi 4fi
!
:;'f(bfiE732 )46p
k
_ 3ffI'fiLR
-'Ep
3 5fi(Q' 4fi
fi fi |a*! !* j
)J*^k
(< Gfi 4Y;
( >)&h)+F?04
?m
GR
84P0 ffO
ffY1Yff'v3C+fp
? ?p[(bfiY
G'fi ^ja*
^q)J! l
30R
#
v +F?n?pm fi4? UW[ H=I,l Xh)+F?4
E?
fi 4'Cfi(Lp+9fi& 4P0 UffEl
4fi ff()fi
V H=I C+c E4
[fiR
ff*/?Mfi\W
+ v Crgv 3M@ Iffv Q1!,L
" ff
4? h (J * l6



V HOI h h (J * l'l-j
tM
! UW[ H=I,l *0+f?4
?Yff' j )J*^kfi1
2} ff5@ _a
fi Y'fi
@ff''E4
ff%fifi fi(A?pr >!9hirfi
e
0
:
* Z )Z4N4a ffO
ffR>KffL(bfi*
G)
?pf? )SR
ff
3 )G4F;104
4f
o/
ff
ffS'fiR
Lfi 73


$? )J!
l f3O

?p ffc 4
fi
;W4I ] )q #z !WTFp
? 'v p v & hB<l74'$
_ ff
cff4
7fiR
pY*fi()fiT
ff
E@
? 6* )m4f'p
3 7fi
( U'fiv 6!fTF?4$4P04 ff
)J!
T$?26'ffO
-4,0 fffi fi3M934fi ?` ( 4,

ff0v $(bfi 4/'!<cR
( Mfi*
@ _a
fi P4X
MfiXpffp
ffV!OTFp
? )p44}fffi +F42p
Mfi614fi4F!6T$p
? (bfi@ *?pF fiE?
Rv 1Xff'vpZ+Fp
? ?p
( +-12ff&fi q@ /fi3 _a
fi !cY
( fi*-?3Ufi3M'034
fi ffW
MfiF$?3>

ffa44pfi
( ( !l
j 3

ff!^khiHffi>?V HOI Mz
aUW[ HOI 4A'fi33 K K
fiP0 Q(bfiF+c E4
QfiR ffff!<Y p4}ff4fi fi( h ( * l
4t044
4fi Nfiv
( V H=I, h h (J * l'lg ' $'fi[(bfi !gTF?4fiR
'fi
V H=I ff fft+c E4

fiR
pY4(O 0Yfi Y4(< *0+f?4
?S4f'p
3 74(O 0Yfi Y4
( )J!fT$p
? > ff'fi U()fi[?4
4A?F+-L
. Mfi\+X4'$
_ ff$()fizfi3MFfi v _a
fi *0 0 ?p@ ()fi@ )S2A'p
3 N()fiF4
'fiPIv R2,'' pLv & hB<lE!YTFp
? Pfi m%fi40 Op+'fiP7 & hB lN03Mfi7v & hB<l>@
v M!Nc`
( )t*?3 )2F'3
3 7()fiL4g'fiPNv & hB lE*+F?2
?G404v ff$?Y G''v pCv

=

fi[

t}-m{t/s$q

H204l X 4 4
PX
` !/ J
N %

` !/ PX
N %
`
* 0l%2



os0pt{a

l qM


l

`

rgv3M@Pvffw12aUW[

P
X % l
4% f
PX 4Na` Pr `
l 4 f
J

Na` Pr `
PX 4

HOI fi

`_a
fiUfi?Y!

Bh lL4~_kff>! cffRfi?p8+9fiE*O z!YT$?p@()fi@* UW[ H=I,l 2>'fi330!{C4'fiS.Mfi\+
?I4I
fi0 Rff
3 ("A^a*2^ h*2^
)J*<?34883'8R?pK
? !
cffG
fi 0
43fi V'
* aUW[ H=I *+F?2
?m
fi 4ffi(-?pIff' j )J*^kK4Q'fi33m
fiP0!7rfi
PoM4P
e
v
*Jfi3ML20 ffO
fi mfi
( UW[ H=I,l I?4L(b>?p_0fiff*J?fi3p?
4$40 Q'fiCfi10(b 5'fik
_ Y4
fi>hi 0Y?4FMz
fi ffT
Mfif `_a
Cff
[?pffa4

4O

fi0v o/p>@ ff3tfi( 3Ma ff
fi C1!x1z
* Mfi6M
fi ffO6ff
6?pA+-fi'~:;
52O
5
fi0 oM;lE!
UW[ H=I, N4$v
ffO
V
ff
30 >F2$fi/
4} ff 'fi @ 3'$
Ep
? ff
./v p+Fp
? ?pF?pLfiR
pC?fi4
fi(J?3Y3+F3
ffC'fi$v M*/?p5? K2'fi$v & hB lE!<rOv 2*?04A fi? fi 4
pfff'fiC

ff
3ffm(bfi
V H=I *aa3MTMfi[()fiq
V ffI " H=I fi
V I,
" H=I, *
ff
30 EV H=I, 4$?37fi 4
4fi P?-

UV 3 'fiP914
p4}ff4fi !6L
" ff
40?V ffI " H=I
V\ I1
I, " HOI @
GsJ5()fi$ffc 4 0
NfiR
ff!
[P
oM0v Kfiq
( aUW[ HOI *-3%fi K/*-J*9
*5*9 0 U@ Y'fiPff*A 0&?pK' fi

fi 4fi XR
;+ _
6TF|FvG j
6T$|-
ff134C/!=
hi/*$V*5V*5*$V*!!! lE*$+Fp
? @ S?p

ff444048v 2
ffv /k
_ 4@ R
4fi Zfi(Q*-R
W''v pv & hB<l8?v
433
ff j
6T$|fvY
j
6T$|9P$?pN_a'5;+-fi 4
ffFv U$

v3P3
3 !9T$p
? NfiR
pC2$ ]
! f3O

?pI(i
[?0[?04f'v p 4~k
_ ff 8+-Q0fid ffv S?pIfi V _a
fi ! 3%fi V H=I,
p2
} ff h j
6T$|Fv* j
6T$|9lF()fi 'fiRhi
q
dl7hi! !*Q[
3+44fid+A0
fi

7(bfi j
6T$|FvdlE*%+F?04
?Gf?pI''v pmhi
*%V*aJ**%*!!! lY'fi & hB lE!QT$p
? S?pQ? Sff'
+Fp
? ?3F4fi(6?pff ffO
$fi( /*aV*0
*a
8@ P *a+ 7@ ff4Cfi
pffY'fiff'F+fp
? ?p

*R
|
ff
3 L
L4A?3Lfi f
p+FCp
ffY'fiK!
T$3
? Tp o/9 fiE?K@
* UW[ H=I,lfi *4,(bfi
p44}fffi 0P(i34W
" ff'%fi $fi
ffFhi C4
Mfi? p7fi
[?0 CfiO
[40 fff'ElE!,st. UW[ H=I,l {
* UW[ H=I,lfi 4fi1
44} ff9 `_a
fi C'fi
@ff''E4
ffY%fifi Yfi(g?37r 7k
! [3O
N?37L
" fffi 0 LfiR
pC2F h
][bh ) cdlE*+Fp
? )
45?3L' F 0t
d45?pN ff'%fi N()fif
:;'7(bfi834fi )S
d1! [30O
NfiR
pC ?fi4
()fi[4fi-'fi ff v 3 hB 7lE!,Hffid++ cpE4} h (K * l<
'fi(bfiE 0/v p
V H=I, h h (J * l'l$ B'M*+fp? @ 9BCy/!,
{ pffK'fiP E(bC?[ !
aUW[ HOI fi ()fiR V H=I, (i34%W" ff'%fi NfiR vff545v KrO 3M@ Pvffw1![yh UW[ H=I,lfi 454'fi04`:

0 Q()fi
V I,
" H=I,
V ffI " H=I, ! l8T$p
? N fiE?_0'A
p
? ff
.1F+Fp
? ?p58@ ff'%fi @ [
fi32R

@ff13@ ffSfi(,?38'E fi
fi 0fi h ( * lE
! @ fffi 0 82f ff34@ ffS4(p*%()fiLQvff'Nfip
'' pSv & hB,lN+F?fi $
3 Wv
23p
ffh (J * lE*6?pC@ _ofi($?27''Ev pYR
()fi@ /4pY

ffA?pL'v V
) MfiF(bfi44fid
+ ffY1C ff'%fi 6d1*0 Y?3>''v 3P3/ePo (b
* Mz
fi ffT
Mfi
( v
430p
v
23p
Odff4?pff! 30
?Y''Ev pY4'$
_ ffQ?pPfi
;m(9 Rfi
( Z d1!PT$?13L4
(
&

=%$

fi

mknpokmkq

K?3NfiR

; 4A'3
3 Nfi5'fiv ffvpKhi!!*()fi[<lE*0?pY$4A%fi0[?Fff'%fiN4
@ff13@ffV!5c+U?4[3fihi!!*'G dlE*a?p8fi +AffS'fiR73M@L+88(ChB 7lF4F4(
?pp+FKp
ffS
fi 4fi C4'fi?0$?p7@ff'%fi*!!*Y d1!$cp(:Mfi*?pU?p@I
fi34YR
p+''v pFv & hB l-+F?Mfi 7


vp3
3 $
43p
Ph (J * lA03FM
fi Mfif4'() ?p>fi
;!
rMfi
o/a *-3M%fi K/*9J*-
*A ## Y'fiP*5 &?pK'E fi &
fi 0fi ZR
;+
j
6T$|<C j
6T$|-O
x ff134FV!-#
8X hi/*aJ*V*aV*V!!! lR
>''Ev pPv & hB<l5?fv0
43p
ff
j
6T$|< j
6T$|-x7-?pQ()fi3M? _0()?C 4
ff5v

v38E3
3 !6T$p
? [fiR
p4
]hi
iclE*1 ?p@ ()fi@ BK
dL
! 3M%fi RV H=I, p4} ff h _
6T$|,M*
j
6T$|9xlN(bfiR'fiXhi

dlE*O+Fp
? U47
*t+F?04
?R0N?p'v p( hi/*OV*tV*t
*A!!! l#'fi
? d!>cp(<?3I'v p 3/ePoG(bhi/*VJ*J*V
dlFz
fi ffc
MfiNv0
43p
>V*?3m?p@ 4
& hB lE!8T$p
Mfi\+ K''v pC?Lv0
43p
fff?p' Na3MNMz
fi ffc
fiLv
433
>?pI@ ff'%fi @ !>ffc mfi?pN+9fiE*

!ArOv 44*/4
(
0t

d* o/'
Ep
? ff
.Y4FPp
L'fi
L3M@ )UR
ff
3@ >

'fi
fi304KR
Qfi?U@ ff'%fi N U' ff!9W
H +'E $?fi34 R
Lfffi43
ff!
T$3
? $ ff
fi 06fi(a?p$v fi?'ff,?,
(
df

fi p+&' E<@ Fv 'fi/3
ff
1
p2}fffi *?pG?pIfiR
'fi>2 j ( ffkK'fiKMfiM!7cpN4
3 ff'fi R
( YhB 7l
v W?48

ff
30 C4
( h
d1*6?pU ff'%fi [$^@UWUWVM]N
@ ff13@ ffZp
? @ !G+c Zfi?pI+9fiE*
Rff
3 C *g(bfiI
R'' pYv & hB<lQ+f?Mfi

pU$
3 Rv
430p
ffPh (J * lEj
* ff?p
Mfi
' 8fi/

3M@ ffWfiL'fiS12vpt ( *gfiI
' 8+-7(bfi2fid
+ ffW1Y@ fffi 0 P4fiL'fi
/4v3* ( *MfifI ff'%fi Nfi/

3M@ ffY()$/4pL * !
aUW[ HOI fi 4,fi33 Pa3M-Mfi-
fi0 [()fi9(i34W" ff'%fi ffiR ff!gc9'fi$3 3 pff94<a ff
fiS?p7(i
[?qV H=I,
fi ffT
Mfi[v
@ ff@ 7

ff4044p fi(< 4
ff[fiQ'fiP[14ffU()N'


h


!




!
*






fi




4





6
l






?
p



@
()fi@ [@
`_a
4fi
CR
[4fi1
44} ffP'fiIfi h (J * lE;
! UW[ H=I,fi
*
4
MfiC
fiP0 UR
ff
3@ GPff#fi3'03M j ffi4?4v '
Sfi
( V H=I, kW+fp
? qv q()
V H=I,
4( Y'fiRMfiM!#rfi
oM0 *-
( }
dG03O }
d1*-?pY fi4?+F449fi3M'03 j Ffi4#?4
v

ffi
( V H=I !^8
k c$APffCR
f?pQ
N?F _
( MfiI' :
)Y@ ff
ffp
ff-@ fffi 0 Jd7v

ff

'

%

fi





7

2

F



)
(




!f{np
?


8


W
U
[

fi

3
'
0

3

f






`


_







fi




(


443M@ *fz
fi ff
Mfi
& hB lE*%fiQ(<P
*
H=I,fi

3M003/e
$vM()fiPfi Y(bfi[r 0ff!-|<fiE$730'$RL@ ff'fi4 ffY1 @ ff ff
vp fi?p
ff
0v pIfiR
'fiff!AHFfi8? j fik?fP`R@ F
fi 3Mfi4fi Y()fi
UW[ H=I,fi ? U(bfif?p
6T
E`_a
fi Yv fi?Pff
! j ffi4!!!^kRfi3M'03M$4F
fi 4p
ffK fiff!
Mfi?pC4M
fiYfiq
( aUW[ H=I fi 4I?Pz
fi ff
MfiP4fi\+ p44}fffi ?C
' E!tc(MJ4tp
ffE0 O'fi[0
p+Z' t3Mv 3)p4}ff4fi *\?3Lfi ppffV'fiQfi/()
UW[ H=I,fi 'fiQ
4 aUW[ R +fp? N@ _a
fi L+f?9 UW[ H=I,fi (i44ff*\v ffNfi(Mfi3M'03M'p j ffi4
?4N '
fi
( V H=I !^kT$?4NPfi1_a
fi m4'fi _
ffN?p(i4 OfiLfia ffKn
* ^@Uo5Y@ ff@ ff

?pfifi3A4
Lff/v p8hi ff
fi C1!xlA+Fp
? `_a
fi U3

ff!

Ma[I]v

bA9* e$g

MuI] K9[ qezfG [_<^#]v!<n[

e$g

" ff
2%?$fipNfi(gfi3M52PPfi @ ff
ff$2A4OffCfi$ff'%fiff!<T$?04Aff
fiY
fia@ff
L
?p+-fi'~:;
4O
fi0 oMpSfi(-?pfi?0P!IsL3QRvG+F?G?p2O
fi0 oM;
fi(ZVM]1^@` u !T$?4N4/h'h ( ThB l^l7 9hy>lffUJlf'fiU()fi?pP0fi103
Qfi(-?3v41403
fr [5(bfi5ffc 2
NfiR
fi
p _a
fi * c
h'h ( hB
Bl^l [ [ -yh >lff 2UJl9'fi
()fi ?pffi/3
6()fi 6T `_a
4fi !6W
@ qUY46?3T13>R
<fi(fi *JT ChB l4<?3W30>R

fi(A'ffLv p Ifi >r z
*9 4Q?p137R
[fi(A'ffL m?pIfiR
;Gr P*J
9yh >l4Q?p'fi
137
Nfi(A'fiPhi832fi >
4fi ElE!T$p
? I@ ff'fi R(bfiL?04Q
fi0 oM;
@ff30,49?-?p@ f ( hB
Blfi/3
,'ffA()fi-+c E4
F0fiR
; _a
fi */

=d

fi[



t}-m{t/s$q

os0pt{a

l qM


h ( TChBBl^lf fi/3
A'ff5()fiL6T`_
fi!<T$?pQfi3M$4fifiKfi(VM]1^@` u QEff
?Mfi33?G2Jfi130
f'ff!LTF?p83pf4fifimfi(VM]_^a` u 8ff[?Mfi3p?m45 9hy>ld0'fi!
Hffi5?Fh ( ChB
Bl^(l V -yh >lff\ Kh ( hB
Bl^{l J V -hy>lff\@5?p5}ffOfi(a?pAfi/3

r ' 4fi S(i3
3
4fi m0 ffQ032f()fiB
VM]_^a` 8
VM]_^a`R<*V@ ff'R
ff
ff4E
! qVJ]_^@` IM
fi ff
FfiRUfi1fi.13MaA(bfiL
ff
E?Y0v '!
-K
fi02'fi *fi3Mfv
@ ffO
V fiE? aUW[ Q()fiW
pEvpP?p>fi/3
5r ?
l



(




(
4O
[
fi0 oMpVPh'h ( ( hB
Bl^g
l nUJl<fi&
Ph'h ( hB'
l^g
l 9 nUJl-'fiIfi10(b8?p[fi/3

r &()fi6+c 4
-fiR
pL@ `_
fi 8fi
6TX `_a
fi V*@ ff'R
ff
ff!OT$?4t4gR
ff
3
l



(




(
?p'fi
137R
>fi(5@ /4 ffWfi/3
>'ffI4Ch ( hB
;llNfiKh ( hB
;l
l 8 *<
fifi pL'fi 45
fi 04p
@ ffh)R
ff
3 N
+ L30O
9V\ fiJH=I
E? pff5?3p o1$L(bfi$v p
'fi ^ fi fi lE!9T$p
? N4O
L
fiP0 oM;Cfi
( aUW[ u fi 4$?pLO
>F?$fi
( UW[ u !
H o1f
fi 4p

5?3L+9fiE'~:;
>2O
L
fi0 oMpfi(g'fi9h)@ \lp E`_a
fi S(bf?pNfi/3

?KR
()fiO
ffV! cpY2
Ph'h ( hB l^y
l K 9yh >ldl (bfi
qVJ]_^@` ! T$?4 4KR
ff
3@ *Lv ?p
+-fi'f
@ $
* Kfi130
F'72$

ffa > U?p@ ()fi@ 'Kv K?3>fi/3
$r
' fi N(i3
3
fi Qa ,4/44ff_
! [3v p> 4?p-37R
Vfi( j 0k8hiv N?p,*
3j
E?@ \l
'ff5v ?p[fi/3
-r >*1?pC?p[+9fi~:;
N4O
Q
fi0 oMpPfiW
( VM]_^a`Rm4Z
Ph'h &vd
l

h ( ChB Bl^fil 5 5 -yh >lfflE!<T$?2949R ff
3 *Mv ?p[+9fiA
@ p*
'v ?p[fi/3

r z' 0fi W()3
3 0
fi Wa C2>/4ffWfi
fi ?pC3
?/:_07ff
E?X *6()fiI
ff
E?&0
'*/fi v fi ?3Lpff'ffPp
0?/:_0'6ffE
?
! dM(bfi33ff*?3$+9fiE'~:;
$4
$
fi0 oM;
fi;
( UW[$ fi L 8
UW[R fi L@ >?37O
8F?0ffi
( VM]_^a` > 0
VM]_^a`R<*0@ ff'R
ff
ff4!$T$?4
4R
ff
3 *$v q?pS+-fi'

* #fi/3
'444$

ff0 !T$p
? U ff''4
fi q'fi
' fi fi fi ^ fi fi, $_a'5M
fi ffL
Mfi$@ff03
N?p j 0v Qk
fi0 oMp!
rOv 2*9
+
fi 04p
P?3Y+9fiE'~:;

fi0 oM;Xfi9
( aUW[ HOI
aUW[ H=I fi !rO*9
+
k
p
_ 3F()fi- -fifiv ff o/@ fffi t* 62<?pY30>R
,fi
( ffv ffO
Av ^S ^-yh >l< 0
^
n!6rMfi,+c E4
-fiR ffOfi(0?3-()fip
] )J*0 ff134$ )O\v 0
-
+ Aff'6()fi:
ff
?P'fi
^
+Fp
? ?3Z
^K f
) ?p-? ^K )t*R
ff
3 f+ o/
ff
N )Og
)Ov pE!6T$p
? UW[ H=I,l
@ff13@ ffF4O
Ph a( N )Ol$'fi p
Pv 3>+Fp
? ?3J
U )t!7hBAp
? ff
.1v 3+F3
? ?pJ
( +A[12ff
@ff13@ ff[
fi 'Q4O
!
l [3v pm )6fi
( hB
;lgh)+F?4
E?S?fi34YR
8'p
3 o/
0Q3$p

0}ff@ P

3'
ffElE*g v

9yh >lE
* UW[ H=I,l ff ff74O
fid
VM]1^@`
! UW[ H=I,fi
@ff13@ ff74O
Ph'h

dM
l h a8
8h )6 dl'l'l>'fim3
Pv p+Fp
? ?p|
d1*< Z?p'fi
+
(
*
v p,+fp
p
? ?pZ

d$ *
)J! 5 ffPh K
l 9yh >ldR
ff
3 9>?p9p

_ fi
fi
( V H=I, {
* R7y/!,T$p
? @ ()fi@ *M30Pv pKh )On n d"
l h'h _ qvd,
l FT ( hB
;l_ F lQh)+F?2
?*
v *t?Mfi34GR
fi
I'p
3 oM
>vG0}ff@

3P
ffElE*V?p+-fi'~:;
@ P4O

fi0v o/pSfi(
UW[ H=I,fi 4$4fid+ $? Y?0$fi( qVJ]_^@`R<!

BfGj[gfi[]v9[yIezfG [<

bF





Ie !Gj]3gfi[ ek^

{fi'~:;
@P4OI
fi0 oMpS4MfiN+Aff/NC30()30tOff3M@!LT$?p(bfi@I+3M0vffOf?p
+-fi'~:;
/ ff+F4? ff02
A@ ff3Pfi #
03&4
!Q3M04PZfi @ ff
Sv X?3ff
o/
E4O
N4['fiU
fi0@ ?3Pv
@ ffO
6 fi4?PQ+F?G'fi6 `_a
fi V*JN
+ ff46
+F?ff
?Gfi?pff*%()fif?37
fi o1Qfi:
( fi/v p
ff?0ff/fi4K
fi ff
[r [!FT$3
? >4O
7@ ff13@ ff
'dfiOBpB5\^9mVB\" \y 'ECL\\Q\BB5^\\EmV\B"LDy UJ^\`"0 J
\\B^EB5e
W0A X, 0c { ^J\~'d'fBA\ffB "mV[^\-BOB~dB~~;B^EQE0/^A
%^J\J\Etn J\V5^ Rd)F~E5BEVEJ~E~^\E~

=ML

fi

mknpokmkq

()fi$@`_
fiY45v`_a
A'fiM@ff$(J+L+AF4OffCfif@ff'%fiff*/Rff
3N':
`_a
4fi fi1

3MEF()8 # ff7 v p8fiR
'fifa44
fi !
()fi@ p

ff
av pP?p o/
E4O
J@ ff3* [30[
fi 04p
[?p o1R
4

?Mfi/Mfi`:

fiX
! [40
fi/p
f+--+$4'v # 3
3 fi 3
3 Pd['vy8+9fi./'fi V!6+c fi3M o/R
4O
*
r [9
+ @ N 0MfiPPv 044} ffV*3M @ ff
-'fiI
v @ ff''2
fi !6T$p
? [@ ff'fi ()fiA Mfi
pff
4I??44mp02
5+-dX'fiWv 24} /432>v X%fi032fi (bfiP fi23Mfi
fi4?K!9TFp
? @ >@ 7;+-fi ff''4
fi 05fi Y?p7r f!5rO*0?Mfip
3 ?Sp
P 4
fiI:
0 3fffi(Lr [C@
ff
3Mfi *F?pC? q E`_a
fi *$2p
3 ff #?p (bfi@ pff Mfi
R(bfi
ff()fiI?pff o1R
2O
ff*gfi3I
E?Mfi4
fi($0304I@ @ ff fi fi($?pKr [Khi

Tg0 vdl$ ff''4
f?pr f['fi R
ffv p3
Pv 2'4
! ff
fi *%R
ff
3 8?pv
@ ffO
g fi:
?0Pf3O
= fiQ'fiK ff pM*a
+ @ ff''E4
f?3Ir [Q'fiY
fi04Y+F?S?4!LTFp
? @

@ $p+9fiI 0 FO
?Mfi/,(bfi M(bfiE
v pN?4,v ?pL o/R
4O
5h~vdl63@ F'0@ $r f[hi! !*
+F?K KyElA 0K.
pEvpp+r ff3
3 2a'fit `_
fi Y3

ff07h)+F?2
?KM
fi ff
Mfi[
. fi p+f?S0 8r fElE*%fihlf3 73
Ir fT
pv p ffU'fif3 fiR
p
4()
fi V!>ffc Sa4
34d*%3
Ir fN@ ()fi
ffm'fiY2'(bU+c E4
IfiR
ffO
] )
v @ vpIy$v f
K
fi43
fi(g?pN' 0fi (i3
3
fi C0v hi3
?KFTt0v Pvdl-4R
ff ffC+F?
G'fi ^)J!f

@ 8r [f 7()fi
ffS'fiC2'(bYrOE'~:;L
" ff'%fi @ >fiR
vff$+F?U' _
)
@ ff'%fi dQ1v pNy9v P
fi43

4R
ff ffI+F?P C'fi ^q)t!OTF?4:
ff42Pv ff
' E<v 24!JHffiF?Xff?p<fi(%?pff fO
?Mfi/<46N/40v 5+-d8'fi7v 42} -Lfia34fi

fi(r f<(bfi fi43Mfi R
ff
3@ F:3@ ff:
ffI3

ff,v P4'()1v 3[?3$fiR
p!OTF?4O0R

@ ff@ Qfi 4Y?pI@ ff34f+f?mp
r [! [fifi #h~vffwwwlQ()fiN?pI@ ff3[+F4?S'a
r [! (
Mfi?p o/R
4O
f3
ff p
ff
44fi q+A 'fi#?Mfi\+
ff3Mv n?p} Gfi(>?pr [!
T$?Mfip
3 ?Mfi3M9?pW o/R
4O
,?3@ f
+ @ Q3
ff'fi8R
F?M@ Qfi p
* ff
? +F4?P?pQO
Ivff
834fi N
fi !L|9
?v 0123fi Lr ?xfiNxff! (+4 {?GxffN?p
' fi 0 L
fi v Fx * fvff
' ff!
30Nfi(O_0
Lffc 4 0
7 _a 7L
" fffi 0 LfiR
vffA+-[3 ff*0+F?4
E?Y4F \F0
0`o
[!+c 4
7fiR
fff+ o/@ ff@ ffU1U''fi p?38 [fi(,4g'fiPB
^)m()fiQfiR
p
] )J!T$?23/eC

ffP()fiC4Ffi(Lfi3MC fi?P4fi ffq()fiffc 2
YfiR
vff!rMfi 6T
`_
fi *%L
" ff'%fi 7fiR
ffF+ o/@ ff@ ffU+F4?UCrg4'~:;L
" fffi 0 7*
3j
E?tr ()fiQ?p
p
fi fi(5?pfiR
p
! o/04 fi Wfi($+F?m?484>p
ff13C(bfi8fi3M
o1R
2O
74
Rfffid+>!grMfi
UW[ H=I,lfi *'
)J* 07 ff'%fi Nd*4/'fiP;
^aq) 0.
^\BKdF
+ @ $''fi@ ffV!
v 3

0p
W o1R
4
F+ @ 8R
()fiO
ffS'fi 4(b ff
E?mfi(<?p8fiR
ff!F+c Sfi?pQ+9fiE*

C@ `_
fi K fi?0+-5ffffK+F?>y$
3
`o$
3 -(bfi)
ff
E?Yfi(V_a N+c E4

fi$_a >L
" ff'%fi @ NfiR
ff!9rMfiW
Kfi 3Lfi(O?pff L$
3 *P@ $ 0Mfi ffU+-f3 ff
()fi
ppY?pP?@ r [!Cffid
+ ff*6>2L4%fiN'fiU%fiv Qfi3M>?74< fi4?P
Rffv p
fi0 ff +F?ff
?Ufi?pFd+=?pJ^@\YNr fff!,rMfiW
oM0v *v Tt0v >8+ L
fia@
UW[ u h)fi\+vdlE* UW[ u fi h)fid+=lE*
VM]_^a` u Ph)fi\+lE!AT$p
? 4Vv Ma3MA?p>O
>?M@

r [!r3M?pfi@ *t?p ff7 v p fiR
'fihi'R
ff
`_
v '4fi fi(-?pPfiR
'fi8
3
? ffPl
+A$?pL
L(bfif4 fi?0P5R
ffv p
fi0 ff!
k %
dB %Ut' OQ\\B^E\EE;\E\5^9B\5B~$$B~B^ ^FB< BV~ E^\B^%\
d;E,^gB^g~\Bg`<1w5~^~OR\~\B^EO^L-\^B^~B)[5ff\~^941~\~\[~
N r=sOr ;
tff%6B UtENO k tff
$t9i;B~~%At~~\;B6B5^\9B~\^BW
mVB[B\<\~\ % UJ~\B\E
$t,i;B~BP B'[-~F B~60EF^ffBp`B^-\B~~J EgE[\\B'Ef^AB^5

=Kx

fi[

t}-m{t/s$q



os0pt{a

l qM


Q30[
fi 04p
#
Q?pI@ ff30f GTg0 ffNC
>1!Lffc Gfi?Gfi(9?pff I0 ff
* ff
E?mfid+
fi@ :
'%fiQ'fiGWfiE?K!"Ffid+F7@137ff(bfi84>(@0
!T$?p'vffcR()fi':
P
@ ff3ff*V'fiYR
Pp
ff
4
ff?Mfi!Tg0
fi0@ ff>?pPR
()fiP
fi(A'fi< ':
`_a
4fi #+F?#?pS fiE?Pfi( 3M0 ff
fi C1!1*5+F?4
E?#
+ @ Sp
ff pff&()fi
V\ fiKHOI
^a`y`
30fi !XT$p
? K30fi &3
ffX(bfi?pff@ o1R
4
I+A

ff
fi !&T$?M 3

fihi3M @ ff
['fi ?p%fid :;
fi 3ff@ ff''2
fi ElFr f[
+ @ pff*t G?pm?p
fi/3
+A(bfi
ff!T$p
? Y@ ff3P+AWfi/3
Pr 2'(b/v p?3UfiR
p!=[R
'fi
V\ fiKH=I +AF?3Sa4 ff*+f?4
?S
fi 4ffUfi(< Mfih)03Mffi F'fiCC'Iv 'ffSfi(<yl

?0 p'fiKE MfiPY
E?Mfi m0v 'Gv U?3Ir ' 4fi U0v 8()fiNC Mfi
?fi4

fi(Vfi p[fi(V?p[?M@ Qfi !,rOv 44*?p[fi/3
-r +--@ :B()fiO
ffC @ `_
fi Mfi p!
T$3
?
?Mfi/Mfifi ()fi
pvpmTg0 >Y+-I2P44L'fiG?8(bfiITg0 1:
* oM
8?
V H=I, +A?pY ff 0v pGfi E'fi X?pY43fi X+AP3
ffX'fiRR

( ff
fi !Xffc Xfi?p
+-fi*?p$fi/3
<r +A<()fiO
ffV* ?pV H=I, 004 ffI'fi7?p)a3 V\5 z[=]<r nfi(%?pf?M@
*<?pCfi/3
7+-8. X+F?W?pCfiR
fi
;Rr )
( pffp
ff(bfi
6T `_a
4fi *<
?pm@ E`_a
fi mR
()fiO
ff!7Q
E'fi9
V HOI
fi 4'ffmfi(9
E?Mfi1fiv pK fi ff-
G Mfi
fi ^aF()fi8+f?4

? U}h K ^ ;l8 *<
?Mfi1fiv pmG Mfi
fi ^\(bfiI+F?2
?
U1}h ff ^\\l$y/*a
?pY 'pVU1}h ff ^\\l$ =!

fi23
Yv STta fffPfi
>4R
ff ff j ff

k ff[O
ff *afid 9
>y3
3 *Mfi(6?37
03U4O

fi(6?p8 fi?Y!f-fi43
054R
ff ff j '
k >?p8'R
ff3MYfi\ N'fi*%! !*a?37
03U4O
>fi(
?pv
ffO
fiE? v G?Qfid+ 4143
ffKS?p
03G4O
8fi(,?p
fi ff'%fi v p'fi
fi4?K!UrMfi
oM0 *O?p j 'k 'R()fi
UW[ u v Wfid+
v ffI7
a3R4O
/4p
ff
1?pC
a3W4
fi
( VM]_^a` u v Zfid+ !m-fi23
74R
ff ff j kG?Mfi\+?pd fi \30>R

fi(6 `_
fi \fi$fi\ B
>y3
3 0!-TF?4$4f4%fi$'fiPfi 'fiFR
ff
3@ *0()fiT
oM0 *a?p

03W4O
47fi'8''fi pm
fi@ ff2ffW+F4?R?p137R
Lfi($'ff j 12ffkY03Mv pKM(i*O
? 6T `_a
fi C493 ff!6|< fiR
;Ofi9
3@ ff
0M(),'fi7R

j /4ff0k73M pM(i<+F3

4 ffG+F4?Sf
pff'ff ffE
?*+F?04
?GPffSR
71384O
:;
fi 30Pv pMY
! f2'fiM*[2[2%fif'fi
MfiN?F+-L2f
fi5(bfi
L `_a
4fi fiA'fiPfi/

3Mff!6c5+A5fi3M5fi @ ff
L'fiPfi 'fi

03&2O
U3
3 3

3M$
4
3P'0
ffP()fi
fi/v pWr [!q{p
? fifi Y?pX
+ @
?p\3-@ ff3Ifi([0/v pG ff v 3SfiR
'fiff!XT$p
? j Ek
fi43
I@ Y4v 3U()fi
Tg0 >8R
ff
3 L?3N43
3 ff$@ L4y/*0B! !$
* MO
fi fiE5fi1

3@ ffK3MEv p8?pc o1R
2O
Fp
3 Q'fi
041v 3PV fiJH=I *%?Mfip
3 ?S
+ 7?d 7fi0@ ff
fiF'fifi/

3M[+F?Y?045fiR
'fic
Mfif03Mv p
?pT o1R
4
!OT$p
? f4
.PfiR
( fi,v ?pW o/R
4O
6@ ff3ffP()fi?pF04
34<E Mfi
r [7?0P?R
pffR'fiR
pff#3v pY?3f o/R
4O
!|<fiI@ K30C
fifi
+F?U?pI'R
ff
`_a9

V H=I, fi Sfi;
( V\ fiJH=I *[
G
8 Gv UTg0 >1!fHffi8?0 j B
H <ifk 4fv
?p j kY
fi43
m(bfi8 1?v pKfi?3>? RY `_a
fi Z fi?0
ff
30 j kY@ ( EN'fi
`_
fi fiE!
T$3
? fi?0Ph)fi\+FElN?Mfi34mR

fi 4p
@ ffR '0v ff j V*^k j a*^km j J*^kSfiI
ff4 8
v 3 $ff j g
[V!^
k j 0I
k p o/,'fi7 fi?0C

fv Tg0 fLfiZ
>>p
Mfiff-,4<>fi/3

fi4?K* j /k?F$25I `_a
fi U fi?Y* K j 0kI?f$4A?p>3fi(t?3 j 0k
j 1

k 'v ff*! !*1?p$2O
$()fiZ
OVM]<@ :B()fiPv 3N?pFfi130
<P@ E(b/v pM!OrMfi
o/P0 *
UW[$h)%lA4$
fi 4p @ ffK'fiR > fi4? 04$
fi 4'pfi;( UW[ u h)l-()fi4fi\-+ ffY
VM]_^a`y
h)lfhi@ [fid+FQv,: >7fi(tTg0 NlE!6cp_
( Mfi70fi103
-pff09'fiIR
[(bfi
ff*?3?3 j 0k8 fi fi(t?p
fi4? 494p
4
M'fi>?3 j /kL 4fi *v P+F?4
E?P
[?p@ f26fi Ifi p$fid+4R
ff ff j g
[V!^k
Tg0 ffFM*x1*a 0
C :B0@ ff fP30 7hi
03U4O
Lfi 0MlAfi(g?37(bfi Tt0v fffP 0
>
v mC(bfiEPN?N(i
44ffQ'fiO
P
fiP04'fi 0!Lffc Sfi?3L+9fiE*Tta ffQM*Jx1*J
C
fi v

=d

fi

mknpokmkq

>1!<ffc KTg0 ff5M*ax1* 0
C1*@ ff305@
j V*^k j a*^kfi j V!^k
s#t3ff2fiE<fiLfip9fi@,v@ff'pF43pO()fi@,24'vpAfi3M# o1R4?1fi?3ffff!
"Lff
2M?<+$@ F3v pNNrO'~:;W
" ff'%fi 5fiR
p7r n ?,?46r n
p
? ff
.19fi 7?
?pL'YBff]5'E Nv \ G''v 34f(bfi2fid
+ ffGYC@ ff'%fi @ !frMfiNfi3M
fi43M4fi Y0E
h)+F? p
0 Nr v 044}ff4fi alO+F3
? C30v p UW[ R fi */ 4(b/v pIrO'~:;W
" ff'%fi QfiR
p
4T
ff30v ['fi ()1v 3?p8()304JW
" ff'%fi 8fiR
;!NT$3
? 7(i4 fif()fi3
3
aUW[g fi
p
3 8'fiKQv
fia pff[@ v S()
Q/fi4fi ffi(,?37(i34tL
" ff'%fi 8fiR
p! ( T$p
? @ ()fi@
()fi
UW[R fi *rO'~:;W
" ff'%fi $r [,@ T4@ ffIp
ff30F()fi,@ _a
fi Pfi((i34MW
" ff'%fi
fiR
vff!,
ff
3 7+ 83 ffK?pfi43Mfi Ca K?pff o/R
4O
*0 0KR
ff
3
UW[ R fi ()fi33 ?pmO
30>R
Cfi
( fi
VM]_^a` g hi! !
* aUW[ g fi (bfi3
3 0
Mfi(i4
fiEElE*()fiA?pfr [Av P?3ff o/R
4O
9ff'vpIrO'~:;W
" ff'%fi $fi
ff<+A
ff34
'fiPff'pP(i34%L
" ff'%fi @ [0fiR
ff!
rMfiffi3ML
o/R
4O
*/_0 L?%fi?pff@ ff$
+ @ Nff'ff
Mfip+/*fi@()fiP'ffU0(bfizTt0vffFI

fi3MRff 1

7vW[fi4?PC4fi@ffff
_a
4&()fiKc+4
fiff()'U??Mfi()fi
6T
`_a
fi V*VR
ff
3 I?pP4'> Op`:B03%fi hi 0?pfi/3
Q fi4?P

43p
Q U0fi %r NlE!
[1FTF?pfv0
@ffOafiE?P9@F(i'A??pf'fi%fiE?P,(bfi-%fi?Pfi/3
-
`_a
fi V!,T$?4A4L

o1R
ff
ffK'fiPR
N'3
3 N
ff
30 L?pC+ L4fi@ ffY(bfiFv ff v pM!
q>1FTF?p j HfckKEfiNfi(A?pv
@ffO,vfi?PN(i'>?R?pff>
fi330*
+f?4
?nMfi MfiC_kp+v4f'ff! T$?044f o/Rff
ffRff
3@Sfi(7?pG
@ffmv
'@ffP4v 0vpM!
FMRaUW[ H=I [aUW[ H=I fi @Q?p[()ff'Afi(J4a?p[vfi?Pff*Rff
3[?p[2fi@ff
()fiQPffWp4
7ffv3fiE'fi8hi!!*'V H=I ?p[?V fiJH=I lE*)z`yz>?pY@84'fi
2fi@ ffK()fi$fi p>
ff
_a
[fiR
;C;1R
* K?pK
E`_a
Q_k0vpI4Rfiff!
[x1RaUW[ H=I $ UW[ H=I,lfi +F44/?dF?pfRff'<
ff3MfiRffff!tT$?pI+F24Mfi<.Qfi@
2OPNr }v
@ffffff!8T$?04Q4' o/ff
4fiR
fiOffN()fi?p+-fi'~:;
4O

fiP0 oM; /4!
3Ma44 23pff[+O o/Pp@?pR
fiPfi(,+$fip @ff4
4fih)(bfi9aUW[ R fi
UW[ H=I,fi *+F?04
?K@ IMfi$
fi0 7 fi?ElE* K?pLo/483fi0@ ffU'R ff3V!
T$3
? N@ ff35 L?pN()fi4fid+fv pYhi3
3 ff5'ffUfi?3+F4 *4fifi. $?p j ff
k
fi43
0ElE
7vFTtfiXm?pSff3*5vnTg0GZfi1fi.# fid+FGvS?Mfi3p?w
fiP0@ff
?fi\+
R?47 8+F?Rfid+(
Lw1!Y+c Zfi?p8+9fiE*6
fi0@ Cfi\+ v+F?Wfid+vy/*6fid+ Y+F?
fid+ vv*O RfiYfi !"Ffid+Fv?Mfip
3 ?ZwY@ v fi?PQ()fi>ffc 2
fiR
ffff*J

Nd%B'$^0B\`^^NmV^\fiO~\ %UC^BE^'BE[B~B~ %Ut{mVB$OB^EB~HU ^'B\^<;BE
^%B~[E\^^'
UJ BV^'B\^\{ R ~^B~BTo'BENPaP5i;BO BEmV^p[-,mUB^E
~E5' J~~F\\'5A^'B^d%ff\,PaP,BEV^5B %UmV^^g;~; '-B,~t^%^ ^^m'
AgBEH
Ua~ p-E~;B^$Ed ^\B^EA~B^\,;E\^Qff5EiEV^'B\^<;BE
^6E\^^'L U,EQB^O^O^55'\B~[`E^N m'8[B~BT o'BEINBEgB~^\B^EC\~'d';
%pB`Bg~ p51 mB^p'
mV^J'E\%t
0 R E1\Ea`^^N m%'F$OB~\ffBJ\B\^~
mV^^1O^55'd B~$B~^ '

J

fi[



t}-m{t/s$q

os0pt{a

l qM


] K `6W
K
Gh


E8

] K `6W
K
Gh


8

UW[ u G
!^yyyMvffx
!^yyw
!^yyyw
!^yyxx
VM]_^a`
!^y>wQ
!wxCC>
!yCyC
!w\ >y
E




W
U
[
^
!





>

w

x
x
^
!








>

!





C

Q

w
Q
!x/vvff>>

UW[ u fi G !^yyyyC !^yyCx !^yyyC/vd !^yy>y

UW[$ fi *
!^yyyMvffCw
!^yyCQy
!^yyyxQ
!^yy>y

UW[$ fi
!^yyy>x
!^yyMvvy
!^yyMvd3C
!^yy >x

VM]_^a` u G
!^y>/vffxw
v!^y
!4vffw\
v!^y

VM]_^a`

!^y Q
v!^y
!/vvffQx/v
v!^y

VM]_^a`
! >yQ/vd
v!^y
!y Cx
v!^y

!^yyyw>
!^yyxy
!^yyMvffx/v
!^yyxw

UW[ u G
!^y/vvy>
!wQwy>
!4vd3CCx
!wCQCw
v VM]_^a` g>
!^ywQ
!yy
!4vffQyy
!>v
E UW[ R
UW[ u fi G !^yyyx\ !^yyxwy !^yyMvd3QC !^yy>y
l UW[ R fi k !^yywyMvv ! >\xy !^ywyQ !wxy
UW[ R fi !^yywxQx !^ywyy !^ywQ !4vffyMvff>
!^ywC
v!^y
!xQ\wC
v!^y
VM]_^a` u G
!^yyC
v!^y
!4vffQ>yw
v!^y
VM]_^a` g>
!4vff/vff>
v!^y
! ywyx
v!^y
VM]_^a` g
Tg0>1Wffi$R()fiP
9fid>y[3$-hxffiff*Cf33_ff
?alt+f?>fiR'fi;V\ fiJH=I


Sp7r f!$"Ffid+F8vL?fi3p?Sw@>()fiF`_a
fiUfi(6c+4
7fiRff
Yfid+F>vy?Mfi3p?WvffQ@L()fiW6T`_a
fiYfi(g"Wff'%fiNfiRvff!

ff]{K `W
Gh
zgig

]{K`n
K
ff
Gh
gg

!^yyyyyMv
M!xi :px
!y
!^yyyyy
w1! xi :,C
!^y
aUW[ HOI
aUW[O fi
!^yyyyy
Q1!x/v :px
!y
!^yyyyy>
v! iC :px
!^y
E
!^y>xyy
v!^y
!y
!yxyQ
v!^y
!^y
VM]_^a`yO
aUW[ HOI fi !^yyyyy !>i :,Q
! 3>
!^yyyyyC
1!^ywi :pw
! 3>

aUW[ fi G
!^yyyyyC
x1!i :px
HB<i
!^yyyyyC
Q !x/v :,C
1
HBi<

aUW[_R fi
wM! CCyyy
!wQyww
>xCw1! >> 1> !xxyyyy ! Q vffxx1> !y

aUW[ R fi
wM! CCyyC
!wwQ
HB<i
1
> !xxyyyC ! Q /v
HBi<

VM]_^a` u G
!4vv Qx
v!^y
HB<i
! yw>
v!^y
HBi<

VM]_^a`R>
wC1!wxyy
v!^y
>xCw1! >> Q y/!^yQ yyyy
v!^y
vffxx1
> !y

wC1! C/vyx
v!^y
HB<i
Q y/! 3Q w>
v!^y
HBi<
8 VM]_^a`R
Tg0>1WffiR()fiP
7fid9>yC338hxPfiff*
C 330Lff
?l$+f?YfiR'fiBV H=I,

Sp7r f!$"Ffid+F8vL?fi3p?
> @>()fiF`_a
fiUfi(6c+4
7fiRff
EH8

?

Yfid+F$P?Mfi3p?WvyP@L()fi$@_a
fiKfi(O"Lff'%fi@NfiRff!



fi

E



]{J

EH?

WU [ u G
UW[ u fi
VM]1^@` u G
VM]1^@`yO

UW[ fi
VM]1^@`yO

UW[$
UW[ fi
VM]1^@`y










mknpokmkq

G

n

`

]{J`n

?

!^yyyMvffx
!^yyyy C
!^y >/vffxw
!^y >w Q
!^yyyMffv Cw
!^y Q
!^y >wxx
!^yyy >x
! >y Q/vd

!^yyyw
!^yyy C/vd
!4vffw\
!y Cy C
!^yyyx Q
!/vffv Qx/v
!y CQw Q
!^yyMvd3 C
!y Cx

Tg0NMWffiI
03K4OhiKff
fiElAfidB>yP330A+F? fiR'fiV\ fiJH=I K_0Lcff`:

>fiR

ff!6T$?04A0 L4F3M042
fi Cfi(OfiO
Nfi(O?p>P2vKTta>1!

WU [ u G
UW[ u fi G
VM]1^@` u G
VM]1^@` g
UW[ R fi
VM]1^@` g>
UW[R
UW[ R fi
VM]1^@` g


E











]{J

EH?

n
!^yyyw>
`

]{J`n

?

!^yyMvffx/v
!^yyMvd3 QC
!x Q\w C
!4vd3 CCx
!^ywy Q
!4ffv Q>yw
!4ffv Qyy
!^yw Q
! ywyx

!^yyyx\
!^yw C
!^y/vvy >
!^yywyMvv
!^yy C
!^yw Q
!^yywx Qx
!4vff/ffv >

Tg0>x1Wffi5
0374O[hi>ff
fi0Elfi\;>yF3$V+F4?LfiR'fiV\ fiJH=I >_0-"Wff'%fi
fi
ff!6T$?4Aa L4$3044
fi Cfi(6'fiO
Nfi(O?pL4VvKTg0>1!


E











aUW[ HOI fi G
aUW[ G
VM]_^a` u G
aUW[ HOI fi
aUW[_R>
VM]_^a` R
aUW[ HOI fi
aUW_[ R
VM]_^a` R*

]{K

EH?

n

`

!^yyyyyC
!4vv Qx
!^yyyyy
wM! CCyyy
wC1!wxyy
!^yyyyy
wM ! CCyy C
wC1 ! C/vyx



ff]{K

8

W

`

!^yyyyyC
! yw >
!^yyyyy C
>1!xxyyyy
Qy/!^y Qyyyy
!^yyyyy C
>1!xxyyy C
Qy/! 3 Qw >



Tg0C1WffiI
03K4OhiYff
fil-fidB>y330A+F? fiR'fiV H=I, _0>"Wff'%fi
fi
ff!6T$?4Aa L4$3044
fi Cfi(6'fiO
Nfi(O?pL4VvKTg0>1!



fi[

t}-m{t/s$q



os0pt{a

l qM


fid+FCvyU?fi3p?nvffQU@Cfi?0PL()fi6T`_
fi!YcffWTg0>1*gfid+FCv?fi3p?
>@ fi?0P$()fifffc 2
70fiR ff*0 0Ufi\+FQxP?Mfip3 ?ZvyC@ fi?0P$()fi
6T
_a
fi !R-fi0 fi\+zS+F4?*< >U+F?qvy/!#hi"Ffid+FKvC ZG
3fi8R

fiP0@ ffXR
ff
3 fi\+zm?0 v fi?4fi ffZ()fiPL
" fffi 0 0fiR
ff! lHffi

?0[?3ff 8
fiP04'fi 0f@ IR
;+m j @
[ak G j V!^k
v

j@
[akO
ff 0 j /kCfi
j J*^kS?04L4LS
fi@ ff
8
fi04fi ! 9*aff gMaJK8_e %_]{ [ G ezJ!<
fi M_0O
ffC()fi-4a@ ff309 Tg0 B>1!9Y
?pQfi?p
^_e Le!G J@!<.ek^.jg3h-b cA49

?0 *M?pN@ ff3$@ L`
ff ()fifTg0 >1!
[1FTF?pffff'+Aff'fiR
fi0@K4I'fi oMPv3 Tg0ff8M*x1*-C1!c+?pffY
@ff?p

fiP04'fi 4-R
;+K?p[_0E'9p+9fifid+F52
ffv ff j 0kKh)fi j /k8fi j 0kl9 30A?pQ?
fid+zfi(F?0IO
K4R
ff!T$p
? @ ff'fi ()fiPP.1 pS?3ff K
fi04fi I48?I?pC_0'
p+9fifid+F9fi(tI
4R
ff0
fi ff'%fi 'fiI v
@ ff
fi?0 h oM
5()fi9fi\+8fi(
Tg0 ff[ mxlF 0U?p8?EYfi\+fi(9
m4R
fft
fi@ ff'%fi F'fiK'fi6 fiE?K!
[
0 ff*/fi pN
fi34f
oMPv pNTg0 ff58
>1!6+c Tg0 L1*/fid+FNv[?Mfip
3
? CCh)fi?3
?0 lO KvyN?fip
3 ?KvffxPh)fi?p,? YvvdlO@ fv
@ ffO
fi4?P* Ifid+F,1*vv*
?Mfip
3 ?w1*% 0ffv C?Mfip
3 ?Xffv Q 8'fiO `_a
fi fiE?P!QT$p
? Ifi4

fiP04'fi 0- [R
p+- fi\+FQvQ Y*1 Y*x8
Q1* >7 0 w1v
* C7 Kw1*VvyI mffv C1*
ffv >I 0Gffv C1*Vv mvd*Vvff8 mffv Q1* mvffxI mffv Q1!<ffc Tg0 9>1*/fi\+FNv*1* C8?fip3 ?
L@ Fv 0
@ ffO
0 fi4?P* 0Pfi\+F
>>
QN?fip
3 ?Yvy7@ F'fi!<TFp
? Ffi4

fiP04'fi 0L@ PR
;+Rfid+FPv
>1*gY
>1*JS #vy/*6x
Q1
* CK Rw1*O Z
0Wvy/! ?gM@KKek^.gM Eb T$3

? L'4'4
_a

Nfi(6?p>
fiP04'fi 0$v
Tg0 fffP
>+ @ >ff'ff
! d[v 3P \ oM
F{44
fiffo/fi YE M.:;3ff'*a2V
fi02'fi
ff P'fiR?%fi?pff4[m &Tt0v Um@ Y'4'2
4Z `_a
Yhb) yMvY *-v

Pfi'F
ff
* )cny yyyMvdlE!5+c YTg0 >1*?Mfid
+ ff*?p>@ 0
ff5R
;+ UW[R fi L
qVJ]_^@`RWh)%fi?I?pLh)Mlg Yh)%lV Efi Elt@ LfiO4'4
447 0`_a
g6?p:)cy yMv
v
ffX
! [4%fi?pF
fia4'fi Fv KTta >@ Lv `_a
$$?pN)cy yMv> ff!
q>1FTF?4?%fi?pff2PMfiffMfiaZ'fiW?pUfi4?P()fi@ :B()fiPpm?pYfi/3
Pr
ff
3@ *ffi114fi3*F4C+f44$ ff34@ Sfi G2O
R
m'fi K?pp+v 4f'ffK()fiK?p
{ +F4?S'fiKff'N?p|V\dM ^@`y`<4
Iff/v pQfi(,?3 j Hfc~k Efi *'fi
+
j Hfc~k fi ff!N

fi 0
'Kfi Z?pCfid+f74R
ff ff j V!^k TFp
? @ ffv 8
fiP04'fi 0>@ Cfid+ 3
Q
RTta
Rfid+zC 3.
QKv Tg0 x1!m'h f4 ff*gfi p
fi304R
fi0 fi\+ >
E3

C1* Kfi\+vffP 3Lvffxv YTg0 >1! l|,
E?Ufi(O?pff 7
fi04fi $45R
p+-G
3 0-Efi fi(J?3Qv fi? ?$4-?pN
LA?p j HFc~k
j Hfc~kI fi K
fi3
Efi o/
A?A-Mz

fi ff?
Mfi9_k
p+v 40'ff!,Tg0 ff
>> 0
C7@ YMfi9ff
ff
3@ 8?3Ufi S?ff P?3 j Hfc~k Efi Q03MfiQ?pff4Q
fi$
R
3 a!Ch)
{ fi 0Sd+
?3pffm'fiYP. fi p
fi0E4'fi mR
;+R4 j Hfck fi Q m?pffN
fi3
3 0*
+f?4
?G4f@ $b$ff
ffmv GTta 81! l yLgMaKKek^.gM < Gev%a[ b f()[ffvp
?3>'4'2
V _a

*545()fi3
3 ?0$?pN@ ff30$@ Lv `_a
>hb)cqy yMvdlE!

FMFTtfiNpPp-fQ@ff13@fft
fi4pEvpfTg0>f03MjMfi6Tt0vA1!6T$?2J4gRff
3-+-Afi4
3ff'fi>
fi0
F fi?0PO(bfi,+F?4
E|
? V HOI ?6R
P02 ff!g-fiP0@ 5fi\+vA 30,1*
v9 30
>1*f 36* L[ 309vyf'fi[@ 9?p-@ ff30_
! [4@ ff3t?Mfi\"
+ aUW[ HOI Fh)fi\+
vdl6 P
UW[ H=I,fi h)fi\+#lg'fiLR
5,ff'96()<<?p5fi?p9 fi?! 9*zgMae$gM

UW[ H=I,l Lh)fi\+vdl6 3
UW[$ fi Lh)fid+lE*1?p@
[ eek^.jg3h-b ffc P40
ff,fi?3,?

J

fi

mknpokmkq

2$Mfi4
ffaL'Rff3MV!6cffKfi'f
ff*0?3>'Rff3MK4$13LP4
!X[4Mfi2
ff0

ff03M05@ L'2'4
4 0`_a
Nhb)qy yyyMvdlE!
[x1FTtfi&ff'YQx1*[
fiP0@m?pm_0 j 'k
fi43h)()fiUx\:;'r fEl+F4??3m@ff
fi

fi23
G+F?S?4Q4R
ff9h)()fiNx\:;'r [ElE!fi@pff40>
ff3M?Mfi\+FN fid+
4p
3 7(bfi j 'kP[?p8} 7fi(6?p8r
@ffff!Fcp[404vffA?f?374fifi(6?pI
03
2O
fi(F?pv 0
@ ffO
9 fiE?'fiG?3C
03Z4O
fi($?pC'fiA fi?p
ff
@ ff@ ff
Pfi@ h)fiFv 0
@ ff ff$ ffEl-5?pLr } Lv 0
@ ff ff!-c
pN?Mfi34 P. >?4A;+-fi:;
fi43


fiP04'fi 7()figfid+FAv9?Mfip
3
? CIh)03M_fiOlt Cvy[?Mfip
3 ?CvffxIh)03M_fiAvvdlJfi(0Tg0 A1*
0mfid+FvI R1*t GK?Mfip

3 ?Rfi(9Tg0 P>R
ff
3 I?pff P I2t?3Pv
@ ffO

v fi?Pff!Ah)
{ [Mfi ]<
@ F%fi3M6?3$'fi0 fiE?POR
ff
3 j %0k[2*18p

_ fi *
4+-d1Pv!^y ()fiL?pffY! l ( &c(,fi pP
fi 4p
[?pI@ ff30[fi(- fiE?PQR
ffv 3Cv G%fi?
fffh ! M!{
* UW[ fi ?fid+F<0` ,
ff3MPfi
ff<v ?3$;+-fi>0v ff*03<+-Tpff
'fiN
fi 4p
O%fi?7 Ofi(a@ ff34ElE*?pI
ff9
aUW[ H=I fh)fid+vdlt
UW[ H=I,lfi h)fid+&ltv
Tg0 >Q?fid+#?pAR
ff'<
v ff3MIfi(41?p$
@ ffO
fi?0P! [ ek^.jg3zh-b c
250@ A(bfi?3 j @ ff
kP
fi43
09?05?pQ4O
N
fi0 oMpfi(g?pff Q;+-fi fi4?P
z
fi ff?
Mfi5v
ff Ih)fi?35? Pv Mfi2
ba3
3fi 0El<5r =} Qv
@ ff ffNhi NTg0 B>lE!

fi3M0fi(f3Ma44G43pff8@ffid+ Mffff!mrMfifip*<ff
4-?| UW[ g fi
UW[ H=I,fi @ cMfi5
fi0 !<TFp? @ ()fi@ *M54,ffA'fiP
fi439?pQR
fiNfi(tv0
fi@ff

@ff04
fi Qhi! !*M(i4 Yfil6?p Pp
;
! aUW[g fi [Pp
cMfi p!<rMfiF?pQ@ ff3Av Tg0 B>1*
>>fiff fi( UW[ H=I,lfi ]^A@ ff2
fi --+ N+$fi pYhi! !*M(i4 YfiEEl,(bfi$?3Q} >xIr fff*/ 0Yx ff
+@ L+$fi 3(bfi$?3>} Nxr [!
4
rOv 2*-
fi 04p
?pSPoM483 fi0@ U'R
ff3MJ
! aUW[ HOI fi ?Mfid+FC ( O: ff2)`)v
.

fffi\
VM]1^@`gfi 7} <xfr fi0v ffP,hiff Efi ffIfid
>yF$
3 E=l JT$?04V4 /4fft73
E?
fi(%?pF
fi
7 %fi3M;aUW[ HOI fi ]^,()2 )fi9!<rMfi
o/a
* ?p$a4I@ `_
fi
4O
fi
( UW[ H=I,lfi *g fi
fi34W3 >'fiS@ E(bmU4fi pU ff1p
3
fi($ ff v 3Kfi
E'fi

34v vpfv >fi 39?62~$
_ ffg?p-fiR
;7v >
fi 4p
04L fft4O
-? It. ff
qVJ]_^@`R
'fiP@ E(bCfi p>v ff v p8fiR
'fiff!
{
fi 0
43p

?2[@ ff
fi 1G30PP}ff pM*v Tta *J?p(i'ff'>v fi? h)0@ fffi
fi3MP ff3El>()fi
XfiR
'fiff*A30fi *9 0&fiR
pZpR
!#ffc #Tg0 S*AP4P3O
ff

?Grg4'~:;L
" fffi 0 r 273@ ffW()fiO
6T E`_a
fi fi(FW
" ff'%fi 0fiR
ff![R
'fi
V fi 4 vfi 4ffiP'ffG()fi ?4f0 8R ff
3 I[2T MfiN
ffNQ?4f4O
8+Fp
? ?3[[+-fi34
R()>'fiUaU'fi9@ `_
fi fiLR
()fi830v I0044
fi 0ffi(A?pv
@ ffO


fi4?h)fi p7(bfiT
ff
?S4P4 Qfi
E'fif024
fi alE! ff
fi Q
fi 4p
E$ U7
'fi43fi R7()3M3@ P+-fi.a! ffc WTg0 C* j Hffi pffkGO
ff
fiU@ _a
fi W2L@ ff13@ ffV*J! !*O?p
ff
0v p>fiR
'fi54-8fiE3
3 ff 'fiIR
[ Gsm(bfi-?4A43fi C 0fiR
;
2!





G k$U#fi
%

T$?pG?CRn ff 3ffffi(>@ff
Kffff
?fifi/pff[
?pff
.1pM*f fifi/pff

3
? ff
./v pCfi(,4''E03MffK'/'ffPhiffi}ffP3V*gvffww/vdlE!NHT?pffffff*a?p84fY44'7vU?p
4E3M@ %fi3M>fi/p
ff6
3
? ff
./v pUa4 ffG'fiU'/'ffPN?0>
? 3!T5+-
fi Mfi0v o/
4fi


@ ?pP@ ff ffE
?Zfi( fi.fi4'.1m 0 fi.Wh~vffwwlLfi Wv 0
@ ffO
<@ _a
fi W 0?>fi(
ffzub:S)T
O-`g,BE;EE^EEBBQ\B^VJdgB-B\Oi;B^iB^'/BB^Q^[B\fB^5



fi[

t}-m{t/s$q

fiNH=I,J K
fi
0Kc+4


V fiJH=I
aUW[$ fi
V I,

V3UV
V ffI
V3UV
V fi 4
aUW[$ fi
V HOI
UW[ =H QfiRaUW[O fi
V I,
ffI
V3UV
V I,
vfi
V3UV
V fi 4 H=I
UW[$ fi
V
UW[$ fi
V I,
" fi 4
UW[$ fi
V ffI " fi
UW[$ fi
V I,
" H=I,
3V UV

V ffI " H=I,
3V UV
V 4fi
3V UV



os0pt{a

l qM


fiH=I ff
fi
K"Wff'%fi


aUW[_R fi
VMUV
VMUV
aUW_[ R fi
UW[ H=I,fi
VMVU
VMVU
aUW_[ R fi
aUW_[ R fi
aUW_[ R fi
aUW_[ R fi
UW[ H=I,fi
UW[ H=I, fi
aUW_[ R fi

'2
ff
fi3
Kcff4


UW[$ fi
V3UV
V3UV
UW[$ fi
UW[$ fi
V3UV
V3UV
UW[$ fi
UW[$ fi
UW[$ fi
UW[$ fi
UW[ fi
UW[$ fi
UW[$ fi






















K
fi
0K"Lff'%fi

UW[g fi
V3UV
V3UV
UW[ g fi
UW[ g fi
V3UV
V3UV
UW[ g fi
UW[ g fi
UW[ g fi
UW[ g fi
UW[ g fi
UW[ g fi
UW[ g fi












Tg079sffpfiR'fi5+F4? ?pL(i'ff'$@E`_a
fiY?Mfi1V!

.IB!qh~vffwwlE!Z-fi?Xfi(F?pff@CaE7@K%fi3M8@`_
fifi(f'fi();+A@Y()P3
fff?pQ?m08fi!>HT?pffvff[?p7+-fi.G4Fff4ff! fi.fi4'.1U fi4.
3 ?3Pfi10
n%:;
4
3430Q'fi\ o10@ ffQffc 4
sJ 3ffNfiR
vff!T$p
? G@ ff L
v

ffO
fi >fi(Ffi/p
ff
p
? ff
. g?0gM
fi ffVafi1
.:B1:B0fi/

. fi0
fi03Mfi 0Jfi(/_
ff
%fiv *\E?pO? 6TZfiOfi
;:;'R
ff
`_a
<fi/p
ff1
p
? ff
.1 pNt
+ AMfiM!gTFp
? -v ff v pFfi
E'fi
30O
ff>1L?pff4g fiE?@ -ff3
-p
ffv fi = <0fi Jfi 7f@ @ ff@ fi I2P44t'fi[r [

4 ffXs/T h)0383304. Cfi3M7344fi I+-fi.*,?p3O
Kmv 3 Cs/T lE!OTFp
? +-fi'~:;
@
4O

fi0v o/p#fi(L?pff4 v fi? 4?pO
? fi(L'fiQ@ `_
fi *f4?Mfip
3 ?
?pff ffP04
[@ ff3 @ afi1fi1V!HffiR?K+W?d WX4fi[@ ff3C()fi\
ffp
W3
ff fi !
ffid
+ 9
+ $Mc
fi Mfi<?d F Pv
ffO
Mv fi?'R
ff
`_
4N4fi ffI(bfi:
ffp
ffi Kh)()fi
830v Ifi 6Tfi70fiR
;:;'R
ff
`_a
fi/p
ff<
Ep
? ff
./v p=l ?,?3>?4LffR
Y()34'()3
ff
fi #()fi(i3M3M@ U@ ff ff
E?! .
C!]^Cfi
E?
fi 4'fi(L
fi pW30 U 'fi
r [*6?p pvpm Zff'vpG(i3
3
fi 7?IPX(bfi?pC%fi'~:Q'fiS?pC@ ffv ff v p
r # 7fiR
p!tc(?pAp
ff4@ ffL(i3
3
fi 8
8R
,()fi3
3 *?p>aLF?3fi@ ff(bfi AL3?
h~vffwwlE*6+F?2

? 3E ff7?L?pC ff pK4 j ( !^k [?Mfi3
3
? MfiU
fia o/4;m@ ff34L@
fi\12p
ff*?pp :; 0/:Bff'5fi
E??09?pP3
ff
R
$R
ff,'fi7R
F
fia3Mfi 44
o/
0 !ffc q
fi '' 'fi .f
!*5+ m?d Sfifi(i 0 ff044
L
/4p
0
U?fi3M
?Mfi/@ e
v PV*9v X'fiO

K
ffff*9??3@ K30'44WPfi@ e
?
'fiV `_a
fi Y(bfi

?V!
T$3
? @ L4$4fiP@ ff4ffU@ ff ff
E?Uv K?3N$
_ ff4Kfi(6
44
02 3v pM!Offc K02
34ff*M
{ ff4S
|<'}ff4fiFh~vffwwl7?ffCYO?Mfi/R'fiGv
@ff44Uff'8Wfi]^7a4'fiUpff
2pP+F?p?3L'fi
p+
fi 0>'fiS?p02 ! f
fi 0>@ Cp
ffRfi 0m+Fp
? R?3ff
Rff
8fi Mfi71fi2

v U;1R
Lfi(O+c E4
NfiR
p!9T$3
? ff$O
?Mfi/U?F'fiO
>2P444ff-+F? fi3Mq
UW[ H=I,l
fi4?K!Ic
p`R@
4[?Nfi3MLO
?fi14Q()fiN@ ff
?pN? mfi @ ff
a4 !

$

fi

mknpokmkq

Mfi?pI4L?8fi3M8`_a
4fiWO?Mfi/4 o/@ff@ffZ3vpK?pC(bfiEP<()fi334fi>vW?p

fi/pff
?3ff
./vp443M@!

fQ?dIm?fid+WS'fi ff
4
@ff4fiAfi(V@ff
Lfi5040=<''ffLhiA3M.1?E*vffww>@?zAL0M}ff/*Vvffwwx@?5Off
S.fid/
?*tvffwwC@?0rMfiff*VvffwwClE!-r 0409?dQ 3@ ff%fi?(bfi5834fi 5
fiR
`:

fi
fi1fiv4fi!6rMfiL oM0*MrMfiff]^7h~vffwwClA
fi: fi/v pPr fi 5()fi$
fiR

OS04ff/vp+@Ufi3ffq%fid! 4P42;Z+F?Xfi3M+9fi.X2?rM
fi ff[30O
ff
fi]0025@c o/@ffffU 6:;3M'fiP/!-W
H ?pff ff*arMfi ff_
3 f4
3 ff- `_
fi
fi(<?pff@702!QQfi4PGG"$fi
p
? ffv Xh~vffwwl[ ff [
?Mfi/U()fiN7344fi [
fifiE`:
fiS?N30Offfr 04
![S300 Lfi L
fi1fiR
71Y./v p
fi ['fi ()dfi0
A?pff?14fi
!6T$p
? Q
fifiR
fi ''EC4-4a ffO
ff1804 Cp
fffiR
-+F?Mfi
P 134 ff4?pmr [!T$3
? U@ ff44fi ?q'fiZ?pG+-fi.#p
? @ m4P?0C?3&@ ff Cr
' (bfifi 8?O3@ C832fi
fi1fiv 4fi !RsJ. +f4 *<v fi3MI@ ff ffE
?*-Gv ff /:
v pCfiR
'fiN?N4QCfiE#
3E ff j ( ffkK()fiN'fiO
7304fi Q
fi1fi fi GfiR
p
' (bfi7?3 r +F?4 3MEv pS
fi1fi fi
! [?Mfip
3 ?%fi??pffIO
?Mfi/& fi3M
3
>?045
fifiEv fi V*1?3ffF'fi43fi K45P 13+F3
? @ ff5fi3MF4?
4@ ff 3M'fiPff!
[QO4fipffmvU?38v'fi/3
fiUfi(<?4f0Rff*%r

fi
-fi(0?p5fi@ -@ ff
6ff ff
?Ifi Ifi <
fifiEv fi 802 ffV()fiP1 `_
fi IO
?/:
fi/!LrMfi oM0v*%s#PmDQ3M(Kh~vffwwlNfi13fft?3ffNfi]V@ffP2
Q+F?mC(bfi44
444-'fiDj'
p>h)E?pF? Ur fElE!<TFp
? E(b '

?fi }fffi Rh)ffc 2
\l9fiR
':
ff*+F?2
?Y0@ Npff0fi1
.a*%3 pfi/p
ffJ
Ep
? ff
./v pM!Qr3M?pfi@ *%
SDQ3M( 83
3 ff'
@ff
fi\ ()fi()4v ffP `_
fi 30v pLp+9fiO
?fi1ffg
fi
0Avff v pM* 8O
?Mfi/C fi:
fi3-'fiI?A3 ff"fPp
N {fi ?h~vffw QwlE!--3.1?0Sh~vffww >lA 0
ALa M}ffCh~vffwwxl
30O
Ffi -a4 <@ f@ @ ff@ ffCV
6:;3'fiP/*M P?pP0M@ ff94p
3 ff,fi(fi13
ff0
3
? ff
.:
v pXff%fiQ
fi 2
S0fiR
ffCfi(7?3 @ fiv Rhi7344fi El 04 !T$?30?3@ R4 fid+fv p
@ ff
ff3
7(bfi8M@ ffv pY834fi 7
fi1fiv 0fi 1
o/@ ffv pK02 L
6:;3M'fiPm
()/v pP?pff +F4?Sfi/p
fft
p
? ff
.1v 3M!LN3MF+-fi.Y030445fi S?4f@ ff
ff3
* G4'f
fi o10
*VR
ff
3@ Mfi pfi(-?4Q@ /fi3[@ ff@ ff
?ZM@ ff@ ffY
e
> `_
fi m(bfi7fi >?
ff
V!
r v2*6?p@Y@Y4YO?fi18()fiP
fi''vpG?pKRff?ff/fiIfi(Qfi*-+F?2
?

@[
fi0ffOI'fi>`_a
fi@ff(2:B0ff!grMfi? o/a* ?fi? T#33?Mfi'}
h~vffwwxlgp
ffv 7fi O?OfiR
L'fi1
24d+Ffi* ! M!*( ;8
fi fi ff*N ff''4
v3F?pAfi ff]

fi ff!<W
H ?3ff ff*1?pFa4 p
ff p9ff
Mfi9RF0 f'fi7 4
0Q
3v p9204d+F
v'fiK?pfi 7R
(bfi ff? R
* ff'R
ff
444U(,?3Pfi 7?ff 'fiU0!Y
p'fi43M4fi G2Q'fiY3
4d+F-?54fid+PoM483i
b$ o/4044pYhirg4'fi3%#
33?Mfi'}*vffww QlE!9ffid
+ $?29'fi23Mfi

fi ffT
fi[44fid+(bfiN
v G
?0 pffNv Y?3702 *a30
?Gf?380fi KfiNp
ff 4fi Ufi(<
fi 0!
KR
ff4v pI 4 L+9fi304C
Q'fi
fi30 Nv 2$
p pv pIfi(O'fi/
44ff+fA+F?f e

@ _a
fi Y()f ff7 v pM!

?Mfi/(bfi 3Mv pZ0?12
4&%fi3
3 p
ffR
ff?ff/fi fi(Ifi Y2 j `_a
4[0?/4
k
h RffK [fifi*7vffwwwlE!{?`_a
4[0?/4
*F832fi Rff?ff/fiK4C@ff''E4
ff
`_
4V()fi
ff[R;+S?38fi!NHW?pffff*%+F?3
fi3$pI3$2
0ff


30P'
ff*`_
40?12

pffJ'fiFR
<
fi0 ff
ff>+F?N@ E`_a
fi 7 j ' pk
()fiF ff(b:B@ 0>hBQfiMfi FB!*JvffwwwlE!

\

fi[



2

):

t}-m{t/s$q



os0pt{a

l qM


!#"



$&%')( *+*,')-/.(10fi230%465879%:;0=<>58(% :;?A@B58C284D58(E@F0@HG28?A:;53*I4JKL0D.?(CB23'M:;')?2ONP<Q0:/28C6C10fiRS?58(B7T?(C658(UN
*,'V:;?-W*X530fi(B7><>58*X.Y@F'V0@H23'ZB?A%')(*X7&RG7,*\[]'^?CB?A@*X?A[B23'Z@B:X')C58-W*X?A[B28'Z_`o5a:;?A@B58CB234
:X')7X@F0fi(B7X53b'J $c(
?A@@:;0fi?-;.D*,0a:X')7,0fi23bU58(1%d*X.1')7X'^@]0*,')( *X5e?28234a-W0fi(1fH58-W*X58(1%g:;')hG58:X')Rg')(*X7\587i@:;')7,')( *,')Cj.1'V:X'JQkl(j7XGRdN
RS?A:X4Z<Q'&.?)b'c7X.10=<&(d*X.?A* -W'V:X*X?58(aRS?-;.B58(1'\23')?A:;(B58(1%90@]'V:;?A*,0:;7 ?A:;'\?T@:;530:/5]mn<&53*X.S(10T:;GB(UNP*X58Rg'
:X'Vb'V:;53oH-V?A*X530fi(Hp 7X?Aqn'>*,0d@]'V:Xqr0:/R JLks(a0*X.1'V:t<Q0:;CB7VZ<&.1')(
-W'V:X*X?5e(uC')7X53:;?A[B28'i@:X0@]'V:X*X53')7Q.10fi28Ca@:;530:
*,0+23')?A:;(B58(1%1ZA*X.1'V4d?A:X'i%fiG?A:;?(*,'V')Cg*,0+.10fi28Cd@]0fi7,*sNv28')?A:;(58(1%1JwK\.1't@:X0@]'V:X*I4^-V28?7X7X')7-W0fi(7X58C'V:X')Cd.1'V:X'
?A:X'ckl(bA?A:;58?(-W'c?(Cax>')7,@]0fi(7,'Jy')?A:;(58(1%^0@F'V:/?A*,0:;7tz ;{I|}{v~8{=ZUzr { Z1\z X{l|{I~8{ e { /ZU?(BCa\z X{l|{I~8{ B ~3A l
<'V:X'aqn0fiG(C*,0@:X')7,'V:Xb'a@:X0@]'V:X*X53')7M58(')53*X.1'V:0qi*X.1')7,'Y-V28?7X7,')7VJ10:
fi X {v~ ?(BCfi | FZ
<>.1'V:;'>*X.1'V:X'9587Q?M7X5e(1%fi23'MmRG23*X5rpl?A%')(*iU$@B28?(Z\z X{I|}{v~8{s {IBZUz e { v {I ?(C
z ~ 4 <'V:X'&qn0fiG(C
*,0
@:X')7X'V:Xb'Tkl(bA?A:;58?(-W'T@:;0@F'V:;*X53')7VJ$&2e2H0q*X.1'+?d@B:;530:;5F:X')7XGB23*X7t?A:X'^58(C'V@F')(BC1')( *0qw*X.1'+7X53V'T0qw*X.1'
U$?(CD?A:X'+*X.'V:X'Vqr0:;'^?A@@B285e-V?A[B23'c*,0
?( 4 U$*X.?A*>.?7\[F'V')(Rg0C')2-;.1')-;')C0:;58%fi58(?28234J
't*X.1')(dC587;-VG7X7,')C^*,:;?(7,qn0:;RS?A*X530fi(B70qB28')?A:;(58(1%>0@F'V:/?A*,0:;7?(C^*X.')53:w-W0:;:X')7,@]0fi(C58(%>?>@B:;530:;5
:X')7XGB23*X7*,09?\@:X0UCG-W*@H28?(JLK>.587?CC:X')7X7,')7] =|~ | ZA<>.1'V:X' RG23*X58@B23'?A%')( *X7w')?-/..B?)b'*X.1')53:
0=<&(@B28?(j[BG1*c*X.1'gRMG28*X58?A%')( *9@B2e?(jRG7,*c[]'M:X'WNPqn0:;Rg')C6?(BCj:;'Vb'V:;5OoB')Cj*,0DC1'V*,'V:;RS5e(1'M<&.1'V*X.1'V:
RG23*X58?A%')(*T@:X0@]'V:X*X53')7T?A:;'g@:X')7,'V:Xb')CJgkI*T<t?7+C587;-W0=b'V:X')C*X.?A*T0fi(234zAX{I|}{v~8{=Zze {I ZzAX{l|{I~8{l e {IZ
?(CDzAX{l|{I~8{; ~33la@B:X')7,'V:Xb'T*X.1')58:\?g@:;530:/5H:;')7XG23*X7tqn0:>*X.5e7i7X53*XG?A*X530fi(J
58(?2e234Z)<'@:X')7,')(*,')C+(10b')2 58(B-W:X')Rg')(*X?2A:X'Vb'V:;5OoH-V?A*X580fi(+?23%0:;53*X.BRS7]qr0:L?282fi-V?7,')7L58(9<>.58-/.9*X.1'
?i@:/530:;5:X')7XG28*X7?A:X' (1'V%fi?A*X53b'Jkl*<?7L7X.10<>(+58(9[]0*X.T*X.1'V0:;'V*X58-V?2 ?(C+')Rg@B53:;5e-V?2-W0fiRS@B?A:;587,0fi(B7]*X.?A*
*X.1')7,'?23%0:;53*X.BRS7\-V?(E7XG1[B7,*X?(*X58?28284a58Rg@B:X0=b'+*X.'^*X58Rg'M-W0fiRg@B23'W153*v4Y0q:X'V b'V:;53oH-V?A*X530fi(D0b'V:9*,0*X?2
:X'Vb'V:;53oH-V?A*X530fi(Eqr:;0fiR7X-W:;?A*X-;.J^ Rg@H53:;58-V?2:X')7XG23*X797X.0=<')C6?7TRMG-/.6?79? NP[B582828580fi(UNPqr0fi2eCD7,@]'V')CG1@J
K\.1')7X'>?A:X'&5e(53*X58?2U:X')7;G23*X7VZfi[HG1* -W0fi(*X58(G')CS:X')7,')?A:/-;.Y?230fi(1%+*X.')7,'&285e(1')7<>582e212853')234[]'>?A@@B285e-V?A[B23't*,0
?a<>58C1'+:/?(1%'M0qQ58Rg@]0:X*X?(*>@:X0[H23')RS7VZB5e(-V28GC5e(1%g?
bA?A:;53'V*I4D0q ?A%')(*9C0fiRS?58(7c?7><')282w?79Rg0:X'
%')(1'V:;?27,0qr*I<?A:;'M?A@@B2e58-V?A*X530fi(7VJ
.1')(23')?A:;(5e(1%587a:X')hG53:X')CZt<'67XG1%%')7,*Y*X.?A*u*X.1'?@:/530:;5\:;')7XG23*X7u7X.10fiG2eC[]'E-W0fi(7;G23*,')C
oB:;7X*VJMklqQ(10 @]0fi7X58*X53b'd:X')7XG23*X7gm5J'J3Z*X.1'S23')?A:;(5e(1%a0@]'V:;?A*,0:M587T?(UEywp>'W1587,*VZ*X.1')(58(-W:X')Rg')(*X?2
:X'Vb'V:;53oH-V?A*X530fi( @:;0-W'V')C7)J
KL0 *,')7,*+0fiG1:T0b'V:;?282qn:;?Rg'V<0:XFZ<'S.?)b'
5eRg@B23')Rg')(*,')CE*X.1'g:X0b'V:;7T'W1?Rg@B23'g0qQ*X.5879@B?A@]'V:
?7^-W0ANP'Vb0fi28b58(%?A%')( *X7?7X7XGBRS58(1%fiFQ=|~ | Zw5J'J3ZLRG23*X58@B23'd?A%')( *X7M')?-;.<>53*X.653*X7+0=<>(@B28?(J
4uGB7X58(1%*X.1'+?d@:;580:;5B:;')7XG23*X7t?(C 58(B-W:X')Rg')(*X?2?23%0:;53*X.R
7VZU<Q'+?-/.53'Vb')CD7X58%fi(5OoH-V?(*i7,@]'V')CG1@B7VJ
'j.B?)b'j?2e7,0C1'Vb')230@]')C?Rg0:X'7X0@B.587,*X5e-V?A*,')C?A@@B285e-V?A*X530fi(*X.?A*aGB7,')7S:X'Vb'V:/5OoH-V?A*X530fi(CBG1:;58(1%
'Vb0fi28G1*X580fi(JLKi<0&?A%')(*X7-W0fiRg@]'V*,'5e(+?\[]0fi?A:;CT%fi?RS'Zfi?(C+0fi(1'Q0q1*X.1' ?A%')(*X7w'Vb0fi23b')753*X77,*,:/?A*,'V%4+*,0
58Rg@B:X0=b'>58*VJK>.1'i'V4S23')7X7,0fi(S*X.?A*Q.?7 []'V')(S23')?A:;(1')CSqr:X0fiR*X.5875eRg@B23')Rg')(*X?A*X530fi(S5e7*X.?A*Q?28*X.10fiG1%fi.
*X.1'9*v4@]')7i0qLU$c7i?(C 23')?A:/(58(1%0@]'V:;?A*,0:;7i?A:X'+7X2e53%fi. *X284SC5O]'V:X')( *tqn:X0fiR*X.10fi7,'T@B:X')7,')(*,')C 58(u*X.587
@B?A@]'V:)Z1?(Cu*X.1'9@:X0@]'V:X*I4a587thG53*,'9C5O]'V:X')(*Tm58*t5e7?g-/.1')-Xuqn0:\?g-W'V:X*X?58(Y*v4@]'90qL-W4U-V2858-c[]').?)bU530:
0fi(E*X.1'^[]0fi?A:;CFp/ZF58(B53*X58?2'WU@]'V:;53')(-W')7c7X.10<*X.B?A*&*X.1'dRg'V*X.10UC10fi230%4j?(C[H?7X58-+:;')7XG23*X7&.1'V:X'd-W0fiG28C
@]0*,')( *X5e?28234
[]'T')?7X5e234
'WU*,')(C1')C*,0
?gbA?A:;53'V*v4u0qRMGB23*X58?A%')(*\?A@@B2858-V?A*X580fi(7VJ
G*XG1:X'E<Q0:X<>58282iqn0U-VG7
@B:;58RS?A:;5e2340fi('WU*,')(C58(1%*X.1'?@:/530:;5\:;')7XG23*X7
*,0!0*X.'V:Y28')?A:;(58(1%
0@]'V:;?A*,0:;7/RS'V*X.10CB7+?(C@:X0@]'V:X*I4j-V28?7;7,')7VZC1'Vb')230@H58(1%Y0*X.1'V:^58(B-W:X')Rg')(*X?2:X'Vb'V:;5OoH-V?A*X580fi(6?23%0AN
:;53*X.BRS7VZ1?(C 'WU@B230:;5e(1%M@H28?( :X'V@B?58:i*,0S:X')-W0=b'V:cqn:X0fiR:X'Vb'V:;53oH-V?A*X530fi( q?5828G1:;')7VJ 9(1'T<t?)4 58(Y<>.5e-;.
*X.1'^?S@:;530:;5]:;')7XG23*X7\R
53%fi. *i[]'T'WU*,')(C1')Cj587i[4YCB587X-W0=b'V:/58(1%g<>.1')(23')?A:;(58(%g0@F'V:/?A*,0:;7\<>5e282]RS?A'
?g@:;0@F'V:;*v4
*,:/G1'Z'Vb')(58qL53*i<t?7>(10*>*,:;G1'T[]'Vqr0:;'+23')?A:;(58(%1J
$hG1')7,*X530fi(d*X.?A* <?7Q(10* ?CC1:;')7X7,')CS.1'V:;'\587<>.1'V*X.1'V: *X.1'>58(-W:;')Rg')( *X?2BRg'V*X.10UC7?A:X'>G7X'VqnG2158q
RG23*X53@B28'RS?-/.58(1'\28')?A:;(58(1%c0@]'V:;?A*,0:;7Q?A:X'\?A@@B2858')C5e(d[B?A*X-;.Emn'J%1J3Z?70fi('>RS53%fi.*<>587X.*,0^C0T<>53*X.
;A

fi9H1HH
0@]'V:;?A*,0:^z ;W ~33lBp/Jdkl(*X.1'gqG1*XG1:;'<'S<0fiG28C2853'd*,0D'WU@B230:;'d.0=<*,0.B?(C23'*X.587T7X58*XG?A*X530fi(
587a58*aRg0:X'E'Wa-V53')(*u*,0!*,:;')?A*u*X.'0@]'V:;?A*,0:;7Y?7Y.?)bU58(1%![]'V')(C0fi(1'0fi(1'WNv?A*sNv?NP*X5eRg'?(CG7,'
58(-W:;')Rg')( *X?2L:X'Vb'V:;53oH-V?A*X530fi(Eqr0:9')?-/.H9:c587&*,0*X?2:X'Vb'V:;53oH-V?A*X530fi(jqr:X0fiR7X-W:/?A*X-;.@:X'Vqn'V:;?A[B28'c:)Z
[]'V*,*,'V:\4'V*VZH-V?(<Q'MC1'Vb')230@D'Wa-V58')( *>5e(-W:X')Rg')(*X?2?23%0:;58*X.RS7tqn0:&)s^0q23')?A:;(58(%0@]'V:;?A*,0:/7/
28?(:;'V@B?53:c<?7T(10*TC5e7X-VG7X7,')Cj58(j*X.587&@B?A@]'V:c?(C587c?(658Rg@]0:X*X?(*>qG1*XG1:;'MC53:;')-W*X530fi(J9K\.1'
:X')7,')?A:/-;.0q9c'x>?A')C*
?(C :;G14U(100%fi.1'ms)p/Zi<>.5e-;.G7,')7
-W0fiG( *,'V:;'WU?Rg@H23')7g*,0%fiG5eC1' *X.1'
:X'VbU587X530fi(0qT*X.'V0:;53')7
7;G1[U,')-W*g*,0nvP /n/Usnn/ZcRS?)4@B:X0=bU58C1'D7,0fiRS'j58C1')?7)JK\.1'V:;'?A:X'
?287,0u@B28?(:X'V@B?53:9Rg'V*X.10UC7c58(D*X.'-V28?7;7X58-V?2L@B28?((5e(1%S2853*,'V:;?A*XG:X'^*X.?A*9RS53%fi.*c[F'M:X')23'VbA?( *c*,0u0fiG1:
?A@@:;0fi?-;. m0fi7X2e58(^ 0fi2e28?-XFZ)1 ')28Cd*,)530fi(5PZ)p/Jkl*<0fiG28C^[F'58(*,'V:X')7,*X58(1%>*,0c-W0fiRg@H?A:X'
*X.1'+*X5eRg'T*,0S:X'V@B?58:i@B28?(7tb'V:;7;G7i*,:X4U58(1%g?(10*X.1'V:c23')?A:;(5e(1%d0@F'V:/?A*,0:>?(C :X'Vb'V:/53qr4U58(1%1J
$2858R
53*X?A*X530fi(0qt0fiG1:M?A@B@:X0fi?-;.587T*X.?A*M53*^C10')7^(10*M.?(C23'g7,*,0U-;.B?7,*X58-S@H28?(7T0:M@:X0@]'V:X*X53')7
<>53*X.*X5eRg'D2858RS58*X7VZ'J%1J3Z\?x>')7,@]0fi(7,' @:X0@]'V:X*I4qn0:S<>.B58-;.*X.1':X')7,@]0fi(7,'DRG7,*g0U-V-VG1:
<&53*X.58(?
7,@]')-V5OoB')Cj*X58Rg'g?Aqn*,'V:T*X.1'd*,:;53%%'V:)J 'g<Q0fiGB28C2853'*,0u'WU*,')(C*X.5e7&:;')7,')?A:;-;.*,0Y7X*,0-/.?7,*X58-gU$c7
mKiV')(1%1Z)fipu?(C*X58Rg')CU$&7@:X0@]'V:X*X53')7m$&2eG1:Y958282Z+)1M+?A[B?(1)?UZ)fip/Z9?7u<')282
?7M0*X.1'V:-W0fiR
Rg0fi(?A%')(*d:X'V@:X')7X')( *X?A*X530fi(B7+[]')7X58C1')7^1$&7VJD$c(10*X.1'V:dC53:;')-W*X530fi(6qn0:MqG1*XG1:X'
<Q0:X
<0fiG28C[]'D*,0'W*,')(BC0fiG1:u:X')7XGB23*X7g*,07,4RM[]0fi2858- Rg0UC1')2&-;.1')-;58(%1Zi<>.58-/.G7,')7
[B58(?A:;4!C1')-V5e7X530fi(
C58?A%:/?RS7+m c97/p\7,0g*X.?A*\*X.'TqnG2e2F7X*X?A*,'^7,@B?-W'^('V')C (10*i[]'T'W@B2e58-V53*X234g'WU@B230:X')C CBG1:;58(1%dRg0UC1')2
-;.')-XU58(1%Sm G1:;-;.'V*?2J3Z)p/Jks(d7,0fiRg't-V?7,')7VZ 7,4URM[F0fi2e58- Rg0UC1')2-;.1')-;5e(1%T-V?(M@B:X0CBG-W'QC1:/?RS?A*X587,@]'V')CG1@J>0<Q'Vb'V:=Z9(0fi(1'j0q^*X.1'E-VG:X:X')(*
:X')7,')?A:/-;.0fi(7,4UR^[]0fi2858-Rg0C')2&-/.1')-XU58(1%?CC1:;')7X7,')7
?C?A@*X58b'^7,4U7,*,')RS7VJ
$cCC53*X580fi(?28234ZQ*X.1'58C1')?7
.1'V:X'?A:X'E?A@@B285e-V?A[B23'Y*,07,0fiRS'D0qT*X.1'U$>NP[B?7,')C-W0fi( *,:X0fi2&*X.1'V0:X4
<0:XHJ10:'W1?Rg@B28'ZAx&?RS?C1%'i?(C 0fi(.?Rms)fip?7X7XGRg'tU$:X'V@B:X')7,')(*X?A*X530fi(7wqr0:[]0*X.*X.1'
@B28?(*Mmn<>.B58-;.j587\?7X7XGBRg')CD*,0u[]'^?
C5e7X-W:X'V*,'WNP'Vb')(*c7,4U7,*,')Rap\?(CD*X.'M7XG1@]'V:XbU587,0:^mn<&.58-;.j-W0fi( *,:;0fi287
*X.1'?-W*X530fi(7a0q+*X.1'E@B28?(*/p/J '6?A:X'-VG1:X:X')(*X234?A@@B234U58(1%7,0fiRg'E0qT*X.1'j@B:;58(-V53@H23')7d0qT'Wa-V53')(*
:X'Vb'V:;53oH-V?A*X530fi(a*,0-/.?(1%'c*X.1'c7;G1@]'V:Xb5e7,0: 58(
:X')7,@]0fi(7,'\*,0d-;.B?(1%')7i58(S*X.'>@B28?(*Q5e(
?RS?((1'V:Q*X.?A*
@:X')7X'V:Xb')7i@:X0@]'V:X*X58')7+mPc0:;C10fi(E+53:;5e?A58CB587VZA p/J
58(?2e234Z1qnG*XG1:X'T<0:X 7X.10fiG28CYqn0U-VG7\0fi(j7,*XGC14U58(1%g.10<*,0
0@]'V:;?A*X580fi(?2853V'+$c7X58Rg0bH7&y?=<>7\qn0:
58(*,')282853%')(*g?A%')( *X7)J .?A*a7X0:X*X7g0q9@:X0@]'V:X*X53')7d[]')7,*g'WU@:X')7;7S*X.1')7,'j28?)<>7 ')28C?(C *,)530fi(5
ms)p\@:X0=bU58C1'T7,0fiRS'^58(53*X5e?2F7;G1%%')7,*X530fi(7)Z1[BG1*\RG-;.DRS0:X'+:X')RS?5e(7t*,0S[]'+C10fi(1'J
6fi "BjvBLHfi
K\.5e7+:X')7,')?A:/-;.5877XG1@@]0:X*,')C[4*X.1' ca-W'a0q&&?=b?2ix>')7,')?A:;-;.mc1W x&A1V pu58(-W0fi(UN
XG(-W*X530fi(Y<>58*X.a*X.'D;')RS?(*X58-M0fi(7;587,*,')(-W4Udjcx\kJ1k?R%:;?A*,'VqnGB2*,0 5e282858?R@]')?A:;7VZfi0fi7,'V@B.
c0:;C0fi(ZU*X?(!U?C5e(Zwt.58*,00:U:;58(53bA?7X?(ZLx>?Rg')7X. .?A:;?C1<t?WZ9?(&0'V4Z?(BC*X.1'
?(0fi( 4N
Rg0fiG7>:X'VbU53'V<Q'V:/7>qn0:&G7X'VqnG27XG1%%')7,*X530fi(B7&?(Cj?C1bU58-W'J\K\.1'+@B:X')7,')(*X?A*X530fi(0q*X.'MRS?A*,'V:;5e?2L58(D*X.587
@B?A@]'V:i<t?7\')(10:;RS0fiG7X234u58Rg@:X0b')C *X.?(1U7i*,0 58282e58?R@]')?A:;7V17XG1%%')7,*X580fi(7VJ

;

fic/3 1 fi

iH
ff fi6v"AF""FffP"


w!1X!"Ar
$K
fi # X {v~
fi # |
fiFQ=|~ |
%
$ &
' mPp
( mP p
sUn;finn
)

*
2 3 Pm p
4
2 3 6m 5 87 59p
4

HmP p
_l;z :D
n
< mPp
= Nv?G1*,0fiRS?A*,0fi(
? >
fi//A @]r DB >U
fi//A @]PB/S)rv)/
C

; @D3)vFEHGJI
)vV neFEHGJI
@UK
) 3
L NI)s_s
fi///N M!8P
Os
R

Q

UT/B@FsB@1)/
@ lB@ V
W U@ 1VZ]
EQr;N_ W U@ 1VZ]
@ lB@V



` mPp
ba
c



bc

e
2
W > ; rK
; @D3)vg; rK

j0C')287^m7X?A*X587,oB')7/p
$b'V:;5OoF-V?A*X530fi(DRg'V*X.10UC ')(*X?582858(%[B:;G1*,'WNPqn0:;-W'+7,')?A:;-/.
$cG1*,0fiRS?A*X?NP*X.1'V0:;'V*X58-MRS0C1')2-/.1')-XU58(1%
U5e(1%fi23'T?A%')( *c7X53*XG?A*X580fi(
EG23*X58?A%')(*>7X58*XG?A*X530fi(Y<>.1'V:;'T')?-;.E?A%')( *cG7,')7\?gRG23*X58?A%')(*\@B2e?(
EG23*X58?A%')(*>7X58*XG?A*X530fi(Y<>.1'V:;'T')?-;.E?A%')( *cG7,')7\?(D5e(C53bU58CG?2B@B28?(
58(58*,'WNv7,*X?A*,'M?G1*,0fiR
?A*,0fi(
K\.'+7,'V*\0q7,*X?A*,')7Mmnb'V:;*X58-W')7/pt0qU$
K\.'+7,'V*\0q7,*X?A*,'WNP*,0ANv7,*X?A*,'d*,:;?(7;53*X530fi(7+mn')C1%')7pt0q1$
y0%fi58-V?2C1')7;-W:;53@*X530fi(u0q*X.1'+7,'V*>0q?-W*X580fi(7i')(?A[B2e58(1%g?g*,:;?(7;53*X530fi(
$ 00fi28')?(?23%'V[:/?
+10 - +
00fi23')?(?23%'V[B:;?g@B?A:X*X5e?2]0:;C1'V:), + *.- 53/
K\.'+RS?A*,:;5OY0qL*,:/?(7X53*X530fi( -W0fi(BC53*X530fi(7t0qU$
KL:;?(7X58*X530fi( -W0fi(C53*X580fi( ?7X7,0U-V58?A*,')CD<>58*X.Y')C1%'a6m 5 8 7 5!9p
K\.'+7,'V*\0q58(53*X5e?2H7,*X?A*,')7&0q1$
:;58RS53*X58b'c')28')Rg')( *X7>0qw? 00fi23')?(j?23%'V[:;?U?A*,0fiRS7&?A:X'^?-W*X530fi(B7
')hG1')(-W'^0qw?-W*X530fi(B7Mm?A*,0fiRS7/p
K\.'+28?(1%fiG?A%'+0q\m7,'V*>0qw7,*,:/58(1%fi7\?-V-W'V@*,')C[4ptU$
$c(DU$*X.?A*>?-V-W'V@*X7&58(UoH(B53*,'WNv23')(1%*X.Y7,*,:;5e(1%fi7
K\.'+7,')hG')(-W'+0qU$b'V:;*X58-W')7\bU587X53*,')CY[4Y?S7,*,:;58(%
K\.'T:;G(Y0q?g7,*,:;58(%S58(Y*X.1'+U$ 28?(1%fiG?A%'
$:X')hG53:X')RS')( *t0q?-V-W'V@*X58(1%
:;G(7t0q?(U$
K\.'T*,')(7,0:m7X4(-/.1:X0fi(10fiGB7/pQ@:X0UCG-W*t0qU$&7
@]')-V5OoH')7i?g*,:;?(7X58*X530fi(Yqr0:>'Vb'V:X4Y@F0fi7;7X53[B23'9?-W*X530fi(
K\.'+-;.10fi58-W'^0q?-W*X580fi(G(5ehG1')284SC1'V*,'V:;RS58(')7i*X.1'+(1'WU*>7,*X?A*,'
')hG1')(-W'^0qLb'V:X*X5e-W')7>-W0fi((1')-W*,')Cj[4a')C1%')7
$@B?A*X. <&53*X. 7,*X?A:X*>?(BC ')(CYb'V:X*X58-W')7&58C1')(*X58-V?2
0fiRS@BG1*X?A*X530fi(?27,*X?A*,'H?(D?-W*X580fi(D0U-V-VG1:X:;58(%d5e( ?g-W0fiRg@BG1*X?A*X530fi(
K\.'V:X'T'WU5e7,*X7\?g@B?A*X.Dqr:X0fiR
KL')Rg@]0:;?2230%fi5e- ,58( bA?A:;58?(*X
KL')Rg@]0:;?2230%fi5e- s'Vb')( *XG?2e234
QWVX Z5J'J3ZQskl(b?A:/58?( *&(10* X
Q X\[ R^] p/Z5J'J3Z,b'V:;4 X 587i'Vb')(*XG?28234uqr0fi2e230=<')C [4 ]
K\.'9oB:;7,* X mn*,:/53%%'V:pt587tqn0fi28230<Q')CD[4 ? ] mn:X')7,@]0fi(7X'p
K\.'+7,'V*\0qTs[B?CDmn*,0a[F'+?=b0fi58C1')CHpi7,*X?A*,')7\0qU$
t?(58(-W:X')?7,'+?-V-W')7;7X53[B582e53*v4
t?(B(10*>58(-W:;')?7,'+?-V-W')7X7X53[H582853*I4
t?(C1')-W:X')?7,'?-V-W')7X7X53[B5e2853*v4
t?(B(10*>C1')-W:X')?7X'^?-V-W')7X7X53[H582853*I4
U?Aqn'^RS?-/.58(1'+28')?A:;(58(1%d0@]'V:;?A*,0:)ZH5J'J3Z1@:X')7,'V:;b')7\@:X0@]'V:X*X58')7
9('T*X.?A*>587\-W0:;:X')-W*\<>.1')(53*i7,*X?A*,')7>*X.B?A*c gf
9('T*X.?A*>587\-W0:;:X')-W*\<>.1')(53*i7,*X?A*,')7>*X.B?A*chbgf
K\.'+1$*,:/?(7X53*X530fi(uqG(-W*X530fi(
;kj

fi9H1HH


iH
ff fiel nmcBhi"&v"offIp fi"oiffPB
K\.5e7w?A@B@F')(BC5O]Z<&.58-;.d587[H?7,')Cd0fi(SE?((?T?(C (G1')2e5]ms)U=p/ZUqn0:;RS?28284^C1'WoH(1')7ks(b?A:;5e?(-W'\?(C
x\')7X@F0fi(B7,'\@:X0@]'V:X*X58')7 58(g*,')RS@F0:/?2H230%fi5e-AJ 'c[]'V%fi58(S[4SC1'WoH(B58(1%T*X.1'&[B?7X58->*,')Rg@]0:;?20@]'V:;?A*,0:&q
mc( *X582np/J '9?7X7XGBRg'&?M7,*,:;58(1%uKm +r 7ststs p0q-Nv7,*X?A*,')7t0qU$Z<&.1'V:X'>1
uwv 7yx7{z J K>.1')(Sqn0:-Nv7X*X?A*,'


qn0:;RMGB28?A' X ?(C ] Z1<'^C1'WoH('+&(*X582?|
7 +J9 X q ]~} qn0:>7,0fiRg' z\4x UZ +D ] Z?(BC qr0:>'Vb'V:X
4 v
7XG-/. *X.?A* x u.v z UZ + X J
ks( bA?A:;58?(B-W'^@:X0@]'V:X*X58')7\?A:X'dC1'WoH(1')Cj58( *,'V:;RS7&0q b')( *XG?2e234Y@:X0@]'V:X*X53')7VZH7,0a<'MC'WoH(1'^ b')(UN
*XG?28284doB:;7,*VJ10:\-Nv7,*X?A*,'Tqn0:;RG28? X ?(CaU$ Z<Q'TC1'WoH('&@:;0@F'V:;*v4 f R X mW,b')(*XG?28234 X p
?7T?a@:X0@]'V:X*v4*X.?A*T587>*,:/G1'Ymnq?287,'p>qn0:T?u7X*,:;58(1%u53q53*9587c*,:;G1' mnqn?287X'p>?A*9*X.1'd58(58*X58?2-Nv7,*X?A*,1
' + r 0q
*X.1'd7,*,:;58(%1J&10:;R
?28234Z]53
q Km +r 7ststs p&587&?a7X*,:;58(1%a0q U$ ZF*X.1')/
( R X } +Dr 8 ;H
q X Z
5J'J3Z9s'Vb')( *XGB?28234 X J$@:;0@F'V:;*v4 f QnVX mWskl(bA?A:;58?(*M(10* X pT587MC1'WoH(1')C?^
7 QnVX }
V R X ZH5J'J3Zi,(1'Vb'V: X J58(?28284ZH?ux\')7,@]0fi(7,'Mqr0:/RMG2e?
587&0q*X.1'qr0:/R Q X/[ RF] p/ZF<>.1'V:X' X
587&-V?2823')C *X.1'Es*,:;53%%'V:;u?(C ] *X.1'js:X')7X@F0fi(B7,'J$x\')7,@]0fi(7,'^qr0:;RG28?
7,*X?A*,')7>*X.B?A*&'Vb'V:X4 *,:/53%%'V:
587t'Vb')(*XG?28234Yqn0fi28230<Q')C [4Y?g:X')7,@]0fi(7,'J

iH
ff fi
"iffPHl"/fiiffIBL
K\.1'Tqn0fi28230<>58(1%oBb'Tks(b?A:;5e?(-W'T@:X0@]'V:X*X53')7t<'V:X'^GB7,')CD58(Y*X.1'T*,')7X*&7XG58*,'
: 0yNP*,:;?(7XR
53*/p,p
Q V mnkvNvC')2853b'V
: 0yNP@B?G7,'p,p
Q V mnkvNvC')2853b'V
* 0kvNvC1')2e53b'V:p,p
Q V mLNv-W0fi2823')-W
* 0kvNvC1')2e53b'VZ
: 0yNP:;')-W')53b'p,p
Q V mLNv-W0fi2823')-W
: 0kvNP:X')-W')58b
' 0yNP@B?G7,'p,p
Q V mLNvC1')2853b'V
K\.1'Tqn0fi28230<>58(1%oBb'+x>')7,@]0fi(7,'T@:;0@F'V:;*X53')7t<Q'V:;'^G7,')C 58(Y*X.'+*,')7,*>7XGB53*,'
Q mNvC')2853b'V: [ R yNP:;')-W')53b'p
Q mNvC')2853b'V: [ R kINP:X')-W')53b'p
Q mNv-W0fi2e23')-W* [ R yNP*,:/?(7XRS53*/p
* 0!kINvC1')2858b'V:p [ R yNP:X')-W')53b'p
Q m,mNv-W0fi2e23')-W




v
N

C
)
'
8
2
3
5

b
V
'
:
' 0yNP:X')-W')53b'p,p
Q
[ R mnkINP:X')-W')53b
UsfiBQUB

$c28G1:)Zx+J3Z9582e2ZJ ms)p/JS$*X.1'V0:;4j0q*X58Rg')C?G1*,0fiRS?A*X?UJ4o1X,V;; @D>UvVG])VBWZ
ZL) J
$c7X58Rg0bHZkJms)A p/J S{Y MXfiJic:;'V')( <>5e-;.ZFiK~1?)<t-W'V*,* G1[B2e58-V?A*X530fi(7VZks(-AJ
?)b')2ZwJms)fip/J l>1)PuKU\oU;/ 8OI>UP PAJ x>')7,*,0fi(ZU$^ :X')(*X58-W'WNvc?282J
G -;.B5ZAJ ms)fip/J1T(+?>C1')-V5e7X530fi(TRg'V*X.10UC+58(+:X')7,*,:;58-W*,')CM7,')-W0fi(CUNP0:/C1'V:L?A:;58*X.Rg'V*X58-AJAks()Kfi;3X
fi ^3W? @S8 OGF)VB is)// n A+8 OcK UZGHP OW8 BvVBB; H sW;Z@B@J
AJ*X?(1qn0:;CZBt
$ F*X?(1qn0:;C c(53b'V:;7;53*v4 :X')7;7VJ
?

fic/3 1 fi
G1:;-/.ZB1J3ZHt2e?A:X'Z1\J3Zy0fi(%1ZBJ3Zj-VE582828?(ZUSJ3ZB c5e282ZUMJms)p/Ji4UR^[]0fi2858-cRg0UC1')2]-;.1')-;5e(1%
qn0:+7,')hG1')(*X58?2-V53:;-VG58*&b'V:;53oH-V?A*X530fi(J
P Bl1Wfi=n1ug @D>UvV?_6I96 {dW 8O
BvPfisv{ rs >UnnM GHWv /Z U ZHfi1 1J
( \l=/;{ n
G1:XU.?A:;CZ^J1ms) fip/J1y53b')(')7X7L?(BC+q?53:;(1')7X7@:X0@]'V:X*X53')758(^RG23*X5ONv?A%')(*7,47X*,')RS7VJ ks
OuK U4on/v/VK vVBB; n1n
O)V,VBD
I9/ Q=n; v VB Z
@B@J AUJt.?RM[F'V:;4ZU:;?(-W'J
t?A:;RS')2ZJ3ZBE?A:X0=bU53*X-;.ZFFJms) fip/Jy')?A:;(B58(1%dRg0C')287 0qw58(*,')282853%')(*?A%')(*X7VJks\
( \l=/;{ n

K Upon/v/VK /MB; O)V,VBY
I9 Q=n v! KO VB IZII Z@@J J
0:X*X2e?(CZH9x^J
t28?A:;'Z\J3Z 58(%1Z 1Jims) p/J10:;R
?2iRg'V*X.10UC7 u*X?A*,'Y0q&*X.1'u?A:;*d?(BCqG1*XG1:;'aC53:;')-W*X530fi(7VJ
@>n
G>U? T)=Z UK Z J
0fiG1:/-W0fiG1[]'V*X587VZHcJ3UZ ?A:/C5ZBJ3Z 0fi23@]'V:)Z J3Z]
?((B?A?AU587VZFJLms)fip/J&')Rg0:;4 NP'Wa-V58')( *c?23%0AN
:/53*X.RS7Qqr0:t*X.1'9b'V:;53oH-V?A*X530fi(a0q*,')Rg@]0:;?2F@:X0@]'V:X*X58')7VJ E ; DVK ATn\
GF=Wv dW 1Z
ZH J
c'x>?A')C*VZy J3Z :;G14U(100%fi.'ZQJ>ms)p/Jks( *,'V:;?-W*X58b'D*X.1'V0:X4:X'VbU587X530fi(Jks(j58-/.?287,U5Zx+J3Z
K')-VG-V5PZtJ&mQC7VJ}p/W
Z fi rF
LXn SJ Z@B@J 1JiU?(j?A*,'V01Z>t
$ 0:;%fi?(
^?G1qnR
?((J
c')?(ZLK+J3Z ')2e28RS?(ZJ ms)U=p/
J n3Br 4Bl; JaU?(6E?A*,'V01Zi^
$ j0:X%fi?(+?G1qrN
R
?((J
2e7,')?58C14Z J3ZBt28')?)b')28?(CZx^J3Z1 ?G1%fi.ZJ]ms)p/&
J 'V:;53qn4U58(1%^?(Y58(*,')282853%')(*Q7X*,:;G-W*XG1:X'c-W0fi( *,:;0fi2
7X47,*,')
R $-V?7,'7,*XGC4Jkl
( \l=/;{ n A8 O6K U X; _w
GHv /
GH @UW > ZT@@J
AJU?(jfiG?(Z G1'V:X*,0Sx&58-W01J
53*,0fiG7X7;5ZfiJ3ZK')(B(1')(.10fi23*,ZJBms)fip/JE58(58RS?2U7,0U-V58?2U28?=<>7VJBks
( is); fir c8 O>K 1WE OWv;VBK
MB; n O)V,VBg
I9 ); v! KO VBWZ@@J1 UAJ]E?C587,0fi(Z kJ
10%')2ZJms) fip/JT9(*X.1'^:;')28?A*X530fi(7X.B53@D[]'V*v<'V')(jCBG1:;?A*X530fi(0q?(')(-W0fiGB( *,'V:T?(C*X.1'^'Vb0fi28G*X530fi(
0qi-W00@]'V:;?A*X530fi(58(*X.1'S53*,'V:;?A*,')C@:;587,0fi('V:)7TC5823')R
RS?UJ >UB
@>PfinZ y! Z
J
c0fi28CBRS?(ZwFJ3Zwx\0fi7,')(B7X-;.1')5e(ZwJ ms)p/JYQRg'V:X%')(*^-W00:;C58(?A*X580fi(6*X.1:X0fiG1%fi.*X.'
G7,'S0qt-W00@1N
'V:/?A*X53b'D7,*X?A*,'WNv-;.B?(1%fi58(1%6:;GB23')7VJEkl
( is); fir 8 OuK 1 ! OWK pMnfiB; ^ O)V,VH/Dfi
IT/ Q); Bv! K O)B/VZ1@@J1fifi J')?A*,*X23'Z $MJ
c0:;C0fi(ZJ ms)fip/J ')282ONP[]').?)b')C[F0:;%fi7VZ[]0fi230fi7VZ?(C[]'V:;7,'V:X'V:/7VJ+ks
( is)// n Aa OSK UEQ O?_
v/VK v)/BfinH; OV)s)B/c1
fi nHP
LXn Z@B@J Jj?C5e7,0fi(Z
kJ
c0:;C0fi(ZMJ ms)fip/Jx\'WNPb'V:;53oH-V?A*X530fi(0q ?C?A@B*X53b'g?A%')( *X7V@H28?(7VJTKL')-;.J:X'V@J3Z]c?)b4t')( *,'V:Tqn0:
$&@@B2e53')CYx\')7,')?A:;-/.j5e( $>:X*X53oH-V58?2Fkl(*,')282858%')(-W'J
c0:;C0fi(ZMJ3Z+53:;58?AU58C5e7VZ=gJUmA p/J$&C?A@B*X53b'Q7XG@F'V:;b587X0:X4c-W0fi(*,:X0fi20q58(*,'V:;-W0fi((1')-W*,')CdC5e7X-W:X'V*,'
'Vb')(* 7,4U7,*,')RS7)Jks
( is); fir &8 O\K U v)/BfinH; OV)s)B/c
s; JIn@@O;U
WI Z@@JA J]$&(B-;.10:;?A%'ZF$&SJ
?

fi9H1HH
c0:;C0fi(ZBJ3ZU@F')?A:/7VZ J3Z00fi287X4ZJ3ZHy'V'ZBkJLms)fip/J>9587,*,:/53[BG1*,')C 7,@H?A*X58?2-W0fi(*,:X0fi2ZH%fi230[B?2
RS0fi(53*,0:;58(%?(C7,*,'V'V:;58(1%d0qRg0[B5828'&@B.4U7X58-V?2]?A%')( *X7)Jkl(\l=/;{n AO^KU
PwS v)B_
HnfiB; fi OV)s)B/dfi OV nfi v OOVH/ fi GHv SS G Z@@J,U J
?7;.58(1%*,0fi(ZHMJ}9J
c:X'Vqn')(7,*,'V*,*,'ZT1J3Zt >
x ?RS7,'V4Z9cJ9ms)fip/J$c(?A@@:X0fi?-/.*,0?(4*X58RS'E23')?A:/(58(1%1J ks(g\l=/;{_
n M8 O^rBK B vVBB;~D{"=BB@fi!rH~LXn ZH@@J) )J$&[]'V:;C1'V')(Z
1-W0*X28?(CJ
&')53*XRg'V4'V:)ZcJ3Z+58:X[4Z1J3ZfiyL?A[B?)<^Z J3Zfi$&:;-;.'V:)ZfiJ3Z .?A:;?C1<t?VZAx^JUms)fip/Jc7X58(%&?A[B7,*,:/?-W*X530fi(
?(BC6Rg0C')2w-/.1')-XU58(1% *,0DC'V*,')-W*^7X?Aqn'V*v4Eb530fi2e?A*X530fi(7958(E:X')hG53:X')Rg')(*X797,@]')-V5OoH-V?A*X530fi(B7VJ
P
BsUW )Udfi/
GF8 rH/VrZ k? Z] J
&0fi23)RS?((ZTJ3Z ')28')CZ&J3ZT
?(B(?A?AU587VZ9JTms) fip/J9((1')7,*,')C C'V@*X.UNoB:;7X*a7,')?A:;-/.Jks(
\l=/;{ n Ag8 O^K U^G];;
G@Fng
"{ ?@HZB@@JBU JFx&G1*,%'V:;7VZB91J
&0fi23)RS?((ZHdJH1Jms)U=p/|
J dOfi ; 6 fi8 @D>UvV
\lfiP);; }/Jt9, :X')( *X5e-W'WNv&?282PJ
0fi7;2858(ZJ3Z 0fi2e28?-XFZQJ\ms)p/J!y')?7,*sNv-W0fi7,*gfH?=<:X'V@B?53:a$@B28?(!:X'WoF(1')Rg')(*7,*,:/?A*,'V%4qn0:
@H?A:X*X58?2ONP0:;C'V:a@B2e?((58(1%1Jks(g\l=/;{n A8OjK1 ! OK vVBB;OV)s)B/6fi
IT/ Q); Bv! K O)B/VZ1@@JVA VfiJL')?A*,*X23'Z $MJ
+?A[B?()?UZUJms)fip/J 4U(-;.1:;0fi(53)58(1%MRMG28*X58?A%')( *t@B2e?(7 G7;58(1%^*,')RS@F0:/?2H230%fi5e-&7,@]')-V5OoF-V?A*X530fi(7VJks(
\l=/;{ n 8
K UEQnXW BvVBB; ~ O)V,VB
>O))W
GHWv IG Z
@B@JBU 1JU?( 1:;?(-V5e7X-W01Z]t$MJ
+G1:/7X.?(Zx+Jims)p/
J @D>UvV
ITK { V Q;8 = firBfir p
\l)/VJ :;5e(-W'V*,0fi(Z
9, :;58(B-W'V*,0fi( &(58b'V:;7X53*I4 :X')7X7VJ
y'V'ZJ3Z 9G1:Xqn'V'Z\Jms)p/J9(j'W@B2e58-V53*>@H28?(28?(%fiG?A%')79qr0:9-W00:;C58(?A*X5e(1%aRG23*X58?A%')(*c@B28?(
'WU')-VG1*X530fi(J^ks(p\l=/;{n Aa8OgKUE>U/K vVBBD{"=BB@ Vo1XW
ITl rv;)N >U, 1fi >1) IFI Z1@@J )J :X0b5eC1')(-W'Zx\kJ
E?((?UZwJ3Z (G1')285Z]$^Jms)U=p/Jg0fiRg@B23'V*X5e(1%
*X.1'*,')Rg@]0:;?2w@B58-W*XG:X'JoUX,V;Z @D>UvV
G])VBWZ A? ZF AUJ

E58-;.?2e7,5PZx+JQms)fip/JY$*X.'V0:X46?(C6Rg'V*X.10UC10fi230%40qt58(CG-W*X58b'd28')?A:;(58(1%1Jdks(6j5e-;.?287X5ZLx+J3Z
i?A:X[]0fi(1')282ZU1J3ZBj58*X-;.1')282PZUK+JmQC7VJ}p/Z fi! nH
;/Br Z1@@JU 1J ?280d$c23*,01ZHt
$
K>530%fi?UJ
E53*X-;.1')2e2ZcK+JTms)Afip/J V;G@Ufi/wI9hI@@]l) !PH/A@];rJ .JMJi*X.')7X587VZ
U*X?(1qr0:/CD&(B53b'V:;7X53*I4J
c58287X7,0fi(ZUMJLms)A p/J|irH)@3WM8OI9 Q); vO VBWJ ?230S$c23*,01ZFi$^BK>530%fi?UJ
0*,*,'V:)Z9J+ms)p/JoUdOfi 4I9B eE8O6 @D>UPBZ +8O)?@1VsKT
>U1J .JJ*X.1')7X5e7VZHc'V0:X%'j?7,0fi(j&(B53b'V:;7X53*I4J
x>?R
?C1%'Z J3Z1 fi0 (.?R Z J]ms)fip/JK\.1'c-W0fi( *,:X0fi2F0qCB587X-W:X'V*,'&'Vb')( *i7,4U7,*,')RS7VJ&is)//n A^8O
K 1

Z ZHU J
?

fic/3 1 fi
'VA?A:)Zx+J3Zy58(ZoJ NIJ3Z x>?RS?A:;587;.(?(Zw9JQms)p/Jj0C1')2e58(1% *,')-;.B(58hG1')7+qn0:^'Vb0fi23bU58(1%jC587sN
*,:/53[BG1*,')C?A@@H2858-V?A*X530fi(7)Jks(.is)//n AjOE ;dV)@FH;!BK>1W EW Z
@B@JB J 'V:;('ZF<>53*,V'V:/28?(CJ
U.10fi.B?R Z dJ3ZBK')(B(1')(.10fi23ZUJms)fip/JQ9(Y7,0-V5e?2F2e?)<>7qn0:t?A:;*X5OoH-V58?2]?A%')(*\7,0-V58'V*X53')7 Q&FNv2858(1'
C')7X53%fi(P
J I9 Q); v OOVH/WH
Z UA _ ZF U J
U530:/7,5PZUx^Jms) fip/W
J c) 3;
Ik MVs/J &'V
< 0:;HZB
F@:;5e(1%'V:, N 'V:;28?A%1J
00fi287X4ZJ3ZFURg0fi28?UZFJLms)p/Jckl(-W:;')Rg')( *X?2wRg0UC1')2-/.1')-XU58(1%u58(D*X.'MRg0UC?2RGUNv-V?28-VGB28G7VJ
ks
( \l);{ rAd @D>UvV? _6I96 { V ; Z1@@,J U J*X?(qr0:;CZHt$^J
@]')?A:;7VZ J3ZBc0:;C10fi(ZJms)fip/J c7X58(1%g?A:X*X53oH-V58?2H@B. 4U7X58-V7*,0S-W0fi(*,:X0fi2?A%')(*X7VJkl
( \l=/;{ n
O+K U

wS vVBB O)V,VH/ OWfi HS v! K O)B/ GHv Z@@J
U J ?7X.B58(1%*,0fi(ZBJ}cJ
KL')((1')(.10fi28*,ZJ3Z 0fi7,')7)&Z J\ms)fip/J9(!-W00@]'V:;?A*X580fi(!5e(?ERMGB23*X5ONP')(*X53*v4RS0C1')2PJDkl
( \l _
;{ rAM8 O+K U 3 TVBK v)/BfinH; o1nZ
OV)s)B/
I9 ); v! K O)B/VZ1@@J
U) J
KiV')(1%1Z Jms)fip/J]y')?A:;(5e(1%>@:;0[B?A[B58285e7,*X58-?G1*,0fiR
?A*X?T?(CdRS?A:X0=bd-;.B?58(7bU58?&hG1'V:;58')7VJ fi rF
LXn Z Z)U J
?A:;C5PZwJ3Z 0fi23@]'V:)Z Jtms) fip/J$&(?G1*,0fiRS?A*X?NP*X.1'V0:;'V*X58- ?A@@:X0fi?-/.*,0?G1*,0fiRS?A*X5e-
@:;0%:;?R
b'V:/5OoH-V?A*X530fi(Jks.
( is)// n j8 ODK UEQr;
I9U >1; GH @UW > fi
XfiEr
@D>UvV
G])VB G Z@@,J Jt?RM[:;5eC1%'Zj$MJ
( is)// n AD8 OuK U ! OWK
')28CZJ3Z *,)530fi(5ZJms)p/J6K\.'goB:;7,*28?)< 0qi:X0[]0*X58-V7VJ ks
MB; n O)V,VBg
I9 ); v! KO VBWZ@@J]VA VA JLU')?A*,*X23'Z $^J

?

fiJournal Artificial Intelligence Research 13 (2000) 227-303

Submitted 11/99; published 11/00

Hierarchical Reinforcement Learning MAXQ Value
Function Decomposition
Thomas G. Dietterich

Department Computer Science, Oregon State University
Corvallis, 97331

Abstract

tgd@cs.orst.edu

paper presents new approach hierarchical reinforcement learning based decomposing target Markov decision process (MDP) hierarchy smaller MDPs
decomposing value function target MDP additive combination
value functions smaller MDPs. decomposition, known MAXQ decomposition, procedural semantics|as subroutine hierarchy|and declarative
semantics|as representation value function hierarchical policy. MAXQ unifies
extends previous work hierarchical reinforcement learning Singh, Kaelbling,
Dayan Hinton. based assumption programmer identify useful
subgoals define subtasks achieve subgoals. defining subgoals,
programmer constrains set policies need considered reinforcement
learning. MAXQ value function decomposition represent value function
policy consistent given hierarchy. decomposition also creates opportunities exploit state abstractions, individual MDPs within hierarchy
ignore large parts state space. important practical application
method. paper defines MAXQ hierarchy, proves formal results representational power, establishes five conditions safe use state abstractions. paper
presents online model-free learning algorithm, MAXQ-Q, proves converges
probability 1 kind locally-optimal policy known recursively optimal policy,
even presence five kinds state abstraction. paper evaluates MAXQ
representation MAXQ-Q series experiments three domains shows
experimentally MAXQ-Q (with state abstractions) converges recursively optimal
policy much faster Q learning. fact MAXQ learns representation
value function important benefit: makes possible compute execute
improved, non-hierarchical policy via procedure similar policy improvement
step policy iteration. paper demonstrates effectiveness non-hierarchical
execution experimentally. Finally, paper concludes comparison related work
discussion design tradeoffs hierarchical reinforcement learning.

c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiDietterich

1. Introduction
area Reinforcement Learning (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998)
studies methods agent learn optimal near-optimal plans interacting
directly external environment. basic methods reinforcement learning
based classical dynamic programming algorithms developed late
1950s (Bellman, 1957; Howard, 1960). However, reinforcement learning methods offer two
important advantages classical dynamic programming. First, methods online.
permits focus attention parts state space important
ignore rest space. Second, methods employ function approximation algorithms (e.g., neural networks) represent knowledge. allows
generalize across state space learning time scales much better.
Despite recent advances reinforcement learning, still many shortcomings.
biggest lack fully satisfactory method incorporating hierarchies
reinforcement learning algorithms. Research classical planning shown hierarchical methods hierarchical task networks (Currie & Tate, 1991), macro actions
(Fikes, Hart, & Nilsson, 1972; Korf, 1985), state abstraction methods (Sacerdoti, 1974;
Knoblock, 1990) provide exponential reductions computational cost finding
good plans. However, basic algorithms probabilistic planning reinforcement learning \ at" methods|they treat state space one huge search space.
means paths start state goal state long,
length paths determines cost learning planning, information
future rewards must propagated backward along paths.
Many researchers (Singh, 1992; Lin, 1993; Kaelbling, 1993; Dayan & Hinton, 1993;
Hauskrecht, et al., 1998; Parr & Russell, 1998; Sutton, Precup, & Singh, 1998) experimented different methods hierarchical reinforcement learning hierarchical
probabilistic planning. research explored many different points design space
hierarchical methods, several systems designed specific situations.
lack crisp definitions main approaches clear understanding relative
merits different methods.
paper formalizes clarifies one approach attempts understand
compares techniques. approach, called MAXQ method, provides
hierarchical decomposition given reinforcement learning problem set subproblems. simultaneously provides decomposition value function given
problem set value functions subproblems. Hence, declarative
semantics (as value function decomposition) procedural semantics (as subroutine
hierarchy).
decomposition subproblems many advantages. First, policies learned
subproblems shared (reused) multiple parent tasks. Second, value functions
learned subproblems shared, subproblem reused new task,
learning overall value function new task accelerated. Third, state abstractions applied, overall value function represented compactly
sum separate terms depends subset state variables.
compact representation value function require less data learn, hence,
learning faster.
228

fiMAXQ Hierarchical Reinforcement Learning

Previous research shows several important design decisions must
made constructing hierarchical reinforcement learning system. provide
overview results paper, let us review issues see MAXQ
method approaches them.
first issue specify subtasks. Hierarchical reinforcement learning involves
breaking target Markov decision problem hierarchy subproblems subtasks.
three general approaches defining subtasks. One approach define
subtask terms fixed policy provided programmer (or
learned separate process). \option" method Sutton, Precup, Singh
(1998) takes approach. second approach define subtask terms nondeterministic finite-state controller. Hierarchy Abstract Machines (HAM) method
Parr Russell (1998) takes approach. method permits programmer
provide \partial policy" constrains set permitted actions point,
specify complete policy subtask. third approach define
subtask terms termination predicate local reward function. define
means subtask completed final reward completing
subtask. MAXQ method described paper follows approach, building
upon previous work Singh (1992), Kaelbling (1993), Dayan Hinton (1993), Dean
Lin (1995).
advantage \option" partial policy approaches subtask
defined terms amount effort course action rather terms
achieving particular goal condition. However, \option" approach (at least
simple form described paper), requires programmer provide complete policies
subtasks, dicult programming task real-world problems.
hand, termination predicate method requires programmer guess relative
desirability different states subtask might terminate. also
dicult, although Dean Lin show guesses revised automatically
learning algorithm.
potential drawback hierarchical methods learned policy may
suboptimal. hierarchy constrains set possible policies considered.
constraints poorly chosen, resulting policy suboptimal. Nonetheless,
learning algorithms developed \option" partial policy approaches
guarantee learned policy best possible policy consistent
constraints.
termination predicate method suffers additional source suboptimality.
learning algorithm described paper converges form local optimality
call recursive optimality. means policy subtask locally optimal
given policies children. might exist better hierarchical policies
policy subtask must locally suboptimal overall policy optimal.
example, subtask buying milk might performed suboptimally (at distant
store) larger problem also involves buying film (at store). problem
avoided careful definition termination predicates local reward functions,
added burden programmer. (It interesting note problem
recursive optimality noticed previously. previous work
229

fiDietterich

focused subtasks single terminal state, cases, problem
arise.)
second design issue whether employ state abstractions within subtasks.
subtask employs state abstraction ignores aspects state environment.
example, many robot navigation problems, choices route take
reach goal location independent robot currently carrying.
exceptions, state abstraction explored previously. see MAXQ
method creates many opportunities exploit state abstraction, abstractions
huge impact accelerating learning. also see important
design tradeoff: successful use state abstraction requires subtasks defined
terms termination predicates rather using option partial policy methods.
MAXQ method must employ termination predicates, despite problems
create.
third design issue concerns non-hierarchical \execution" learned hierarchical policy. Kaelbling (1993) first point value function learned
hierarchical policy could evaluated incrementally yield potentially much
better non-hierarchical policy. Dietterich (1998) Sutton, et al. (1999) generalized
show arbitrary subroutines could executed non-hierarchically yield improved
policies. However, order support non-hierarchical execution, extra learning
required. Ordinarily, hierarchical reinforcement learning, states learning
required higher levels hierarchy states one subroutines could terminate (plus possible initial states). support non-hierarchical
execution, learning required states (and levels hierarchy). general,
requires additional exploration well additional computation memory.
consequence hierarchical decomposition value function, MAXQ method
able support either form execution, see many problems
improvement non-hierarchical execution worth added cost.
fourth final issue form learning algorithm employ. important advantage reinforcement learning algorithms typically operate online.
However, finding online algorithms work general hierarchical reinforcement learning
dicult, particularly within termination predicate family methods. Singh's
method relied subtask unique terminal state; Kaelbling employed mix
online batch algorithms train hierarchy; work within \options" framework usually assumes policies subproblems given need
learned all. best previous online algorithms HAMQ Q learning algorithm
Parr Russell (for partial policy method) Feudal Q algorithm Dayan
Hinton. Unfortunately, HAMQ method requires \ attening" hierarchy,
several undesirable consequences. Feudal Q algorithm tailored specific kind
problem, converge well-defined optimal policy.
paper, present general algorithm, called MAXQ-Q, fully-online learning
hierarchical value function. algorithm enables subtasks within hierarchy
learned simultaneously online. show experimentally theoretically
algorithm converges recursively optimal policy. also show substantially
faster \ at" (i.e., non-hierarchical) Q learning state abstractions employed.
230

fiMAXQ Hierarchical Reinforcement Learning

remainder paper organized follows. introducing notation
Section 2, define MAXQ value function decomposition Section 3 illustrate
simple example Markov decision problem. Section 4 presents analytically
tractable version MAXQ-Q learning algorithm called MAXQ-0 algorithm
proves convergence recursively optimal policy. shows extend MAXQ0 produce MAXQ-Q algorithm, shows extend theorem similarly.
Section 5 takes issue state abstraction formalizes series five conditions
state abstractions safely incorporated MAXQ representation.
State abstraction give rise hierarchical credit assignment problem, paper
brie discusses one solution problem. Finally, Section 7 presents experiments
three example domains. experiments give idea generality MAXQ
representation. also provide results relative importance temporal state
abstractions importance non-hierarchical execution. paper concludes
discussion design issues brie described above, particular,
addresses tradeoff method defining subtasks (via termination predicates)
ability exploit state abstractions.
readers may disappointed MAXQ provides way learning structure hierarchy. philosophy developing MAXQ (which share
reinforcement learning researchers, notably Parr Russell) draw inspiration
development Belief Networks (Pearl, 1988). Belief networks first introduced
formalism knowledge engineer would describe structure networks domain experts would provide necessary probability estimates. Subsequently,
methods developed learning probability values directly observational data.
recently, several methods developed learning structure belief
networks data, dependence knowledge engineer reduced.
paper, likewise require programmer provide structure
hierarchy. programmer also need make several important design decisions.
see MAXQ representation much like computer program,
rely programmer design modules indicate permissible
ways modules invoke other. learning algorithms fill
\implementations" module way overall program work well.
believe approach provide practical tool solving large real-world
MDPs. also believe help us understand structure hierarchical learning
algorithms. hope subsequent research able automate
work currently requiring programmer do.

2. Formal Definitions
begin introducing definitions Markov Decision Problems Semi-Markov Decision Problems.

2.1 Markov Decision Problems
employ standard definition Markov Decision Problems (also known Markov
decision processes). paper, restrict attention situations agent
231

fiDietterich

interacting fully-observable stochastic environment. situation modeled
Markov Decision Problem (MDP) hS; A; P; R; P0 defined follows:
: finite set states environment. point time, agent
observe complete state environment.
A: finite set actions. Technically, set available actions depends
current state s, suppress dependence notation.
P : action 2 performed, environment makes probabilistic transition current state resulting state s0 according probability
distribution P (s0 js; a).
R: Similarly, action performed environment makes transition
s0 , agent receives real-valued (possibly stochastic) reward r whose
expected value R(s0 js; a). simplify notation, customary treat
reward given time action initiated, even though may
general depend s0 well a.
P0 : starting state distribution. MDP initialized, state
probability P0 (s).
policy, , mapping states actions tells action = (s) perform
environment state s.
consider two settings: episodic infinite-horizon.
episodic setting, rewards finite least one zero-cost absorbing
terminal state. absorbing terminal state state actions lead back
state probability 1 zero reward. technical reasons, consider
problems deterministic policies \proper"|that is, deterministic policies
non-zero probability reaching terminal state started arbitrary state.
(We believe condition relaxed, verified formally.)
episodic setting, goal agent find policy maximizes expected
cumulative reward. special case rewards non-positive, problems
referred stochastic shortest path problems, rewards viewed
costs (i.e., lengths), policy attempts move agent along path minimum
expected cost.
infinite horizon setting, rewards also finite. addition, discount
factor , agent's goal find policy minimizes infinite discounted sum
future rewards.
value function V policy function tells, state s,
expected cumulative reward executing policy starting state s. Let rt
random variable tells reward agent receives time step following
policy . define value function episodic setting
V (s) = E frt + rt+1 + rt+2 + jst = s; g :
discounted setting, value function
fi

n



V (s) = E rt + rt+1 + 2 rt+2 + fifi st = s; :
232

fiMAXQ Hierarchical Reinforcement Learning

see equation reduces previous one = 1. However, infinitehorizon MDPs sum may converge = 1.
value function satisfies Bellman equation fixed policy:

V (s) =

X

s0

P (s0 js; (s)) R(s0 js; (s)) + V (s0 ) :




quantity right-hand side called backed-up value performing action
state s. possible successor state s0 , computes reward would received
value resulting state weights according probability
ending s0 .
optimal value function V value function simultaneously maximizes
expected cumulative reward states 2 . Bellman (1957) proved unique
solution known Bellman equation:

V (s) = max


X

s0

P (s0 js; a) R(s0 js; a) + V (s0 ) :




(1)

may many optimal policies achieve value. policy chooses
achieve maximum right-hand side equation optimal policy.
denote optimal policy . Note optimal policies \greedy"
respect backed-up value available actions.
Closely related value function so-called action-value function, Q function
(Watkins, 1989). function, Q (s; a), gives expected cumulative reward performing action state following policy thereafter. Q function also satisfies
Bellman equation:

Q (s; a) =

X

s0

P (s0 js; a) R(s0 js; a) + Q (s0 ; (s0 )) :




optimal action-value function written Q (s; a), satisfies equation
X
Q (s; a) = P (s0 js; a)

s0





R(s0 js; a) + max Q(s0 ; a0 )
a0

:

(2)

Note policy greedy respect Q optimal policy. may
many optimal policies|they differ break ties actions
identical Q values.
action order, denoted !, total order actions within MDP. is, !
anti-symmetric, transitive relation !(a1 ; a2 ) true iff a1 strictly preferred
a2 . ordered greedy policy, ! greedy policy breaks ties using !. example,
suppose two best actions state a1 a2 , Q(s; a1 ) = Q(s; a2 ),
!(a1 ; a2 ). ordered greedy policy ! choose a1 : ! (s) = a1 . Note
although may many optimal policies given MDP, ordered greedy policy,
! , unique.
233

fiDietterich

2.2 Semi-Markov Decision Processes

order introduce prove properties MAXQ decomposition,
need consider simple generalization MDPs|the semi-Markov decision process.
discrete-time semi-Markov Decision Process (SMDP) generalization Markov
Decision Process actions take variable amount time complete.
particular, let random variable N denote number time steps action takes
executed state s. extend state transition probability function
joint distribution result states s0 number time steps N action
performed state s: P (s0 ; N js; a). Similarly, expected reward changed
R(s0 ; N js; a).1
straightforward modify Bellman equation define value function
fixed policy
h

X
V (s) = P (s0; N js; (s)) R(s0 ; N js; (s)) + N V (s0 ) :
s0 ;N

change expected value right-hand side taken respect
s0 N , raised power N ect variable amount time
may elapse executing action a.
Note expectation linear operator, write Bellman
equations sum expected reward performing action expected value
resulting state s0 . example, rewrite equation
X
(3)
V (s) = R(s; (s)) + P (s0 ; N js; (s)) N V (s0 ):
s0 ;N

R(s; (s)) expected reward performing action (s) state s, expectation taken respect s0 N .
results given paper generalized apply discrete-time semiMarkov Decision Processes. consequence whenever paper talks
executing primitive action, could easily talk executing hand-coded openloop \subroutine". subroutines would learned, could execution
interrupted discussed Section 6. many applications (e.g., robot
control limited sensors), open-loop controllers useful (e.g., hide partialobservability). example, see Kalmar, Szepesvari, A. Lorincz (1998).
Note episodic case, difference MDP Semi-Markov
Decision Process, discount factor 1, therefore neither optimal policy
optimal value function depend amount time action takes.

2.3 Reinforcement Learning Algorithms

reinforcement learning algorithm algorithm tries construct optimal policy
unknown MDP. algorithm given access unknown MDP via following
1. formalization slightly different standard formulation SMDPs, separates
P (s0js; a) F (tjs; a), F cumulative distribution function probability
terminate time units, real-valued rather integer-valued. case, important
consider joint distribution s0 N , need consider actions arbitrary
real-valued durations.

234

fiMAXQ Hierarchical Reinforcement Learning

reinforcement learning protocol. time step t, algorithm told current state
MDP set actions A(s) executable state.
algorithm chooses action 2 A(s), MDP executes action (which causes
move state s') returns real-valued reward r. absorbing terminal state,
set actions A(s) contains special action reset, causes MDP move
one initial states, drawn according P0 .
paper, make use two well-known learning algorithms: Q learning
(Watkins, 1989; Watkins & Dayan, 1992) SARSA(0) (Rummery & Niranjan, 1994).
apply algorithms case action value function Q(s; a) represented
table one entry pair state action. Every entry table
initialized arbitrarily.
Q learning, algorithm observed s, chosen a, received r, observed s0 ,
performs following update:

Qt (s; a) := (1 , fft )Qt,1 (s; a) + fft [r + max
Q (s0 ; a0 )];
a0 t,1
fft learning rate parameter.
Jaakkola, Jordan Singh (1994) Bertsekas Tsitsiklis (1996) prove
agent follows \exploration policy" tries every action every state infinitely often



X
X
lim
ff
=
1

lim
ff2t < 1
(4)

!1
T!1
t=1

t=1

Qt converges optimal action-value function Q probability 1. proof
holds settings discussed paper (episodic infinite-horizon).
SARSA(0) algorithm similar. observing s, choosing a, observing r,
observing s0 , choosing a0 , algorithm performs following update:

Qt (s; a) := (1 , fft )Qt,1 (s; a) + fft [r + Qt,1 (s0 ; a0 )];
fft learning rate parameter. key difference Q value chosen
action a0 , Q(s0 ; a0 ), appears right-hand side place Q learning uses
Q value best action. Singh, et al. (1998) provide two important convergence results:
First, fixed policy employed choose actions, SARSA(0) converge
value function policy provided fft decreases according Equations (4). Second,
so-called GLIE policy employed choose actions, SARSA(0) converge value
function optimal policy, provided fft decreases according Equations (4).
GLIE policy defined follows:

Definition 1 GLIE (Greedy Limit Infinite Exploration) policy policy

satisfying

1. action executed infinitely often every state visited infinitely often.
2. limit, policy greedy respect Q-value function probability
1.
235

fiDietterich

4

R

G

3
2
1
0
0

B
1

2

3

4

Figure 1: Taxi Domain.

3. MAXQ Value Function Decomposition
center MAXQ method hierarchical reinforcement learning MAXQ
value function decomposition. MAXQ describes decompose overall value function
policy collection value functions individual subtasks (and subsubtasks,
recursively).

3.1 Motivating Example

make discussion concrete, let us consider following simple example. Figure 1
shows 5-by-5 grid world inhabited taxi agent. four specially-designated
locations world, marked R(ed), B(lue), G(reen), Y(ellow). taxi problem
episodic. episode, taxi starts randomly-chosen square.
passenger one four locations (chosen randomly), passenger wishes
transported one four locations (also chosen randomly). taxi must go
passenger's location (the \source"), pick passenger, go destination location
(the \destination"), put passenger there. (To keep things uniform, taxi
must pick drop passenger even he/she already located destination!)
episode ends passenger deposited destination location.
six primitive actions domain: (a) four navigation actions move
taxi one square North, South, East, West, (b) Pickup action, (c) Putdown action.
reward ,1 action additional reward +20 successfully
delivering passenger. reward ,10 taxi attempts execute
Putdown Pickup actions illegally. navigation action would cause taxi hit
wall, action no-op, usual reward ,1.
simplify examples throughout section, make six primitive actions deterministic. Later, make actions stochastic order create greater
challenge learning algorithms.
seek policy maximizes total reward per episode. 500 possible
states: 25 squares, 5 locations passenger (counting four starting locations
taxi), 4 destinations.
task simple hierarchical structure two main sub-tasks:
Get passenger Deliver passenger. subtasks turn involves
236

fiMAXQ Hierarchical Reinforcement Learning

subtask navigating one four locations performing Pickup Putdown
action.
task illustrates need support temporal abstraction, state abstraction,
subtask sharing. temporal abstraction obvious|for example, process navigating passenger's location picking passenger temporally extended
action take different numbers steps complete depending distance
target. top level policy (get passenger; deliver passenger) expressed
simply temporal abstractions employed.
need state abstraction perhaps less obvious. Consider subtask getting
passenger. subtask solved, destination passenger
completely irrelevant|it cannot affect nagivation pickup decisions. Perhaps
importantly, navigating target location (either source destination
location passenger), target location important. fact
cases taxi carrying passenger cases irrelevant.
Finally, support subtask sharing critical. system could learn solve
navigation subtask once, solution could shared \Get passenger"
\Deliver passenger" subtasks. show MAXQ method provides
value function representation learning algorithm supports temporal abstraction,
state abstraction, subtask sharing.
construct MAXQ decomposition taxi problem, must identify set
individual subtasks believe important solving overall task.
case, let us define following four tasks:
Navigate(t). subtask, goal move taxi current location
one four target locations, indicated formal parameter t.
Get. subtask, goal move taxi current location
passenger's current location pick passenger.
Put. goal subtask move taxi current location
passenger's destination location drop passenger.
Root. whole taxi task.
subtasks defined subgoal, subtask terminates
subgoal achieved.
defining subtasks, must indicate subtask subtasks
primitive actions employ reach goal. example, Navigate(t) subtask
use four primitive actions North, South, East, West. Get subtask
use Navigate subtask Pickup primitive action, on.
information summarized directed acyclic graph called task
graph, shown Figure 2. graph, node corresponds subtask
primitive action, edge corresponds potential way one subtask
\call" one child tasks. notation formal=actual (e.g., t=source) tells formal
parameter bound actual parameter.
suppose subtasks, write policy (e.g., computer
program) achieve subtask. refer policy subtask \subroutine", view parent subroutine invoking child subroutine via ordinary
237

fiDietterich

Root

Get

Put
t/source

Pickup

t/destination

Navigate(t)

North

South

East

Putdown

West

Figure 2: task graph Taxi problem.
subroutine-call-and-return semantics. policy subtask, gives
us overall policy Taxi MDP. Root subtask executes policy calling
subroutines policies Get Put subtasks. Get policy calls subroutines
Navigate(t) subtask Pickup primitive action. on. call
collection policies hierarchical policy. hierarchical policy, subroutine executes
enters terminal state subtask.

3.2 Definitions

Let us formalize discussion far.
MAXQ decomposition takes given MDP decomposes finite set
subtasks fM0 ; M1 ; : : : ; Mn g convention M0 root subtask (i.e., solving
M0 solves entire original MDP ).
Definition 2 unparameterized subtask three-tuple, hTi; Ai ; R~i i, defined follows:
1. Ti termination predicate partitions set active states, Si , set
terminal states, Ti : policy subtask Mi executed current
state Si . If, time subtask Mi executed, MDP enters
state Ti , Mi terminates immediately (even still executing subtask, see
below).
2. Ai set actions performed achieve subtask Mi . actions
either primitive actions A, set primitive actions MDP,
subtasks, denote indexes i. refer
actions \children" subtask i. sets Ai define directed graph
subtasks M0 ; : : : ; Mn , graph may contain cycles. Stated another way,
subtask invoke recursively either directly indirectly.
child subtask Mj formal parameters, interpreted subtask
occurred multiple times Ai , one occurrence possible tuple actual
238

fiMAXQ Hierarchical Reinforcement Learning

values could bound formal parameters. set actions Ai may differ
one state another one set actual parameter values another,
technically, Ai function actual parameters. However, suppress
dependence notation.
3. R~ (s0 ) pseudo-reward function, specifies (deterministic) pseudo-reward
transition terminal state s0 2 Ti . pseudo-reward tells desirable
terminal states subtask. typically employed give goal
terminal states pseudo-reward 0 non-goal terminal states negative
reward. definition, pseudo-reward R~ (s) also zero non-terminal states
s. pseudo-reward used learning, mentioned
Section 4.
primitive action primitive subtask MAXQ decomposition
always executable, always terminates immediately execution,
pseudo-reward function uniformly zero.

subtask formal parameters, possible binding actual values
formal parameters specifies distinct subtask. think values formal
parameters part \name" subtask. practice, course, implement
parameterized subtask parameterizing various components task. b specifies
actual parameter values task Mi , define parameterized termination
predicate Ti (s; b) parameterized pseudo-reward function R~ (s0 ; b). simplify notation
rest paper, usually omit parameter bindings. However,
noted parameter subtask takes large number possible values,
equivalent creating large number different subtasks, need
learned. also create large number candidate actions parent task,
make learning problem dicult parent task well.

Definition 3 hierarchical policy, , set containing policy subtasks
problem: = f0 ; : : : ; n g:
subtask policy takes state returns name primitive action
execute name subroutine (and bindings formal parameters) invoke.
terminology Sutton, Precup, Singh (1998), subtask policy deterministic
\option", probability terminating state (which denote fi (s)) 0
2 Si , 1 2 Ti .
parameterized task, policy must parameterized well takes
state bindings formal parameters returns chosen action bindings
(if any) formal parameters.
Table 1 gives pseudo-code description procedure executing hierarchical
policy. hierarchical policy executed using stack discipline, similar ordinary
programming languages. Let Kt denote contents pushdown stack time t.
subroutine invoked, name actual parameters pushed onto stack.
subroutine terminates, name actual parameters popped stack.
Notice (line 16) subroutine stack terminates, subroutines
239

fiDietterich

Table 1: Pseudo-Code Execution Hierarchical Policy.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

Procedure ExecuteHierarchicalPolicy()

st state world time
Kt state execution stack time
Let = 0; Kt = empty stack; observe st
push (0; nil) onto stack Kt (invoke root task parameters)

repeat
top(Kt ) primitive action

Let (i; fi ) := top(Kt),
name \current" subroutine,
fi gives parameter bindings
Let (a; fa ) := (s; fi ),
action fa gives parameter bindings chosen policy
push (a; fa ) onto stack Kt
end //
Let (a; nil) := pop(Kt) primitive action top stack.
Execute primitive action a, observe st+1, receive reward R(st+1jst ; a)
subtask Kt terminated st+1
Let 0 terminated subtask highest (closest root) stack.
top(Kt) 6= 0 pop(Kt)
pop(Kt)
Kt+1 := Kt resulting execution stack.
Kt+1 empty
end ExecuteHierarchicalPolicy

immediately aborted, control returns subroutine invoked
terminated subroutine.
sometimes useful think contents stack additional part
state space problem. Hence, hierarchical policy implicitly defines mapping
current state st current stack contents Kt primitive action a. action
executed, yields resulting state st+1 resulting stack contents Kt+1 .
added state information stack, hierarchical policy non-Markovian
respect original MDP.
hierarchical policy maps states stack contents K actions,
value function hierarchical policy must assign values combinations states
stack contents K .

Definition 4 hierarchical value function, denoted V (hs; K i), gives expected cumu-

lative reward following hierarchical policy starting state stack contents
K.

hierarchical value function exactly learned Ron Parr's (1998b) HAMQ
algorithm, discuss below. However, paper, focus learning
projected value functions subtasks M0 ; : : : ; Mn hierarchy.
240

fiMAXQ Hierarchical Reinforcement Learning

Definition 5 projected value function hierarchical policy subtask Mi, denoted
V (i; s), expected cumulative reward executing (and policies descendents
Mi ) starting state Mi terminates.
purpose MAXQ value function decomposition decompose V (0; s) (the
projected value function root task) terms projected value functions V (i; s)
subtasks MAXQ decomposition.

3.3 Decomposition Projected Value Function

defined hierarchical policy projected value function, show
value function decomposed hierarchically. decomposition based
following theorem:

Theorem 1 Given task graph tasks M0 ; : : : ; Mn hierarchical policy ,
subtask Mi defines semi-Markov decision process states Si , actions Ai , probability
transition function Pi (s0 ; N js; a), expected reward function R(s; a) = V (a; s),
V (a; s) projected value function child task state s. primitive
action,
V (a; s) defined expected immediate reward executing s: V (a; s) =
P
0
0
s0 P (s js; a)R(s js; a).
Proof: Consider subroutines descendents task Mi task graph.

subroutines executing fixed policies (specified hierarchical policy
), probability transition function Pi (s0 ; N js; a) well defined, stationary distribution
child subroutine a. set states Si set actions Ai obvious.
interesting part theorem fact expected reward function R(s; a)
SMDP projected value function child task .
see this, let us write value V (i; s):

V (i; s) = E frt + rt+1 + 2 rt+2 + jst = s; g
(5)
sum continues subroutine task Mi enters state Ti .
let us suppose first action chosen subroutine a. subroutine
invoked, executes number steps N terminates state s0 according
Pi (s0 ; N js; a). rewrite Equation (5)
V (i; s) = E

(

NX
,1
u=0

u rt+u

+

1
X

u=N

fi
fi
fi
t+u fifi

ur

st = s;

)

(6)

first summation right-hand side Equation (6) discounted sum rewards
executing subroutine starting state terminates, words, V (a; s),
projected value function child task . second term right-hand side
equation value s0 current task i, V (i; s0 ), discounted N ,
s0 current state subroutine terminates. write form
Bellman equation:
X
(7)
V (i; s) = V (i (s); s) + Pi (s0 ; N js; (s)) N V (i; s0 )
s0 ;N

241

fiDietterich

form Equation (3), Bellman equation SMDP,
first term expected reward R(s; (s)). Q.E.D.
obtain hierarchical decomposition projected value function, let us switch
action-value (or Q) representation. First, need extend Q notation
handle task hierarchy. Let Q (i; s; a) expected cumulative reward subtask
Mi performing action state following hierarchical policy subtask
Mi terminates. Action may either primitive action child subtask.
notation, re-state Equation (7) follows:

Q (i; s; a) = V (a; s) +

X

s0 ;N

Pi (s0 ; N js; a) N Q (i; s0 ; (s0 ));

(8)

right-most term equation expected discounted reward completing task

Mi executing action state s. term depends i, s, a,
summation marginalizes away dependence s0 N . Let us define C (i; s; a)
equal term:

Definition 6 completion function, C (i; s; a), expected discounted cumulative

reward completing subtask Mi invoking subroutine subtask state s.
reward discounted back point time begins execution.

C (i; s; a) =

X

s0 ;N

Pi (s0; N js; a) N Q (i; s0 ; (s0 ))

(9)

definition, express Q function recursively

Q (i; s; a) = V (a; s) + C (i; s; a):

(10)

Finally, re-express definition V (i; s)

V (i; s) =

(

(i; s; (s))
Q
composite
P
0
0
s0 P (s js; i)R(s js; i) primitive

(11)

refer equations (9), (10), (11) decomposition equations
MAXQ hierarchy fixed hierarchical policy . equations recursively decompose
projected value function root, V (0; s) projected value functions
individual subtasks, M1 ; : : : ; Mn individual completion functions C (j; s; a)
j = 1; : : : ; n. fundamental quantities must stored represent value
function decomposition C values non-primitive subtasks V values
primitive actions.
make easier programmers design debug MAXQ decompositions,
developed graphical representation call MAXQ graph. MAXQ graph
Taxi domain shown Figure 3. graph contains two kinds nodes, Max nodes
Q nodes. Max nodes correspond subtasks task decomposition|there
one Max node primitive action one Max node subtask (including
Root) task. primitive Max node stores value V (i; s). Q nodes correspond
actions available subtask. Q node parent task i, state
242

fiMAXQ Hierarchical Reinforcement Learning

MaxRoot

QPickup

QGet

QPut

MaxGet

MaxPut

QNavigateForPut

QNavigateForGet

t/source

QPutdown

t/destination

Pickup

Putdown

MaxNavigate(t)

QNorth(t)

QEast(t)

QSouth(t)

QWest(t)

North

East

South

West

Figure 3: MAXQ graph Taxi Domain.
subtask stores value C (i; s; a). children node unordered|that
is, order drawn Figure 3 imply anything order
executed. Indeed, child action may executed multiple times
parent subtask completed.
addition storing information, Max nodes Q nodes viewed performing parts computation described decomposition equations. Specifically,
Max node viewed computing projected value function V (i; s)
subtask. primitive Max nodes, information stored node. composite
Max nodes, information obtained \asking" Q node corresponding (s).
Q node parent task child task viewed computing value
Q (i; s; a). \asking" child task projected value function V (a; s)
adding completion function C (i; s; a).
243

fiDietterich

example, consider situation shown Figure 1, denote s1 .
Suppose passenger R wishes go B. Let hierarchical policy
evaluating optimal policy denoted (we omit superscript * reduce
clutter notation). value state 10, cost 1
unit move taxi R, 1 unit pickup passenger, 7 units move taxi B,
1 unit putdown passenger, total 10 units (a reward ,10).
passenger delivered, agent gets reward +20, net value +10.
Figure 4 shows MAXQ hierarchy computes value. compute value
V (Root; s1 ), MaxRoot consults policy finds Root (s1) Get. Hence, \asks"
Q node, QGet compute Q (Root; s1 ; Get). completion cost Root task
performing Get, C (Root; s1 ; Get), 12, cost 8 units deliver
customer (for net reward 20 , 8 = 12) completing Get subtask. However,
reward completing Get, must ask MaxGet estimate expected
reward performing Get itself.
policy MaxGet dictates s1 , Navigate subroutine invoked
bound R, MaxGet consults Q node, QNavigateForGet compute expected
reward. QNavigateForGet knows completing Navigate(R) task, one action
(the Pickup) required complete Get, C (MaxGet; s1 ; Navigate(R)) = ,1.
asks MaxNavigate(R) compute expected reward performing Navigate
location R.
policy MaxNavigate chooses North action, MaxNavigate asks QNorth
compute value. QNorth looks completion cost, finds C (Navigate; s1 ; North)
0 (i.e., Navigate task completed performing North action). consults
MaxNorth determine expected cost performing North action itself.
MaxNorth primitive action, looks expected reward, ,1.
series recursive computations conclude follows:

Q (Navigate(R); s1 ; North) = ,1 + 0
V (Navigate(R); s1 ) = ,1
Q (Get; s1 ; Navigate(R)) = ,1 + ,1
(,1 perform Navigate plus ,1 complete Get.
V (Get; s1) = ,2
Q (Root; s1; Get) = ,2 + 12
(,2 perform Get plus 12 complete Root task collect final reward).
end result value V (Root; s1 ) decomposed sum
C terms plus expected reward chosen primitive action:

V (Root; s1 ) = V (North; s1 ) + C (Navigate(R); s1 ; North) +
C (Get; s1 ; Navigate(R)) + C (Root; s1; Get)
= ,1 + 0 + ,1 + 12
= 10
244

fiMAXQ Hierarchical Reinforcement Learning

10
MaxRoot
10
12

QGet

QPut

-2
MaxGet

MaxPut
-2

QPickup

QNavigateForPut

QNavigateForGet

QPutdown

-1
-1
Pickup

Putdown

MaxNavigate(t)
-1
0

QNorth(t)

QEast(t)

QSouth(t)

QWest(t)

East

South

West

-1
North

Figure 4: Computing value state using MAXQ hierarchy. C value
Q node shown left node. numbers show values
returned graph.
general, MAXQ value function decomposition form

V (0; s) = V (am ; s) + C (am,1 ; s; ) + : : : + C (a1 ; s; a2 ) + C (0; s; a1 ); (12)
a0 ; a1 ; : : : ; \path" Max nodes chosen hierarchical policy going
Root primitive leaf node. summarized graphically Figure 5.
summarize presentation section following theorem:

Theorem 2 Let = fi; = 0; : : : ; ng hierarchical policy defined given MAXQ
graph subtasks M0 ; : : : ; Mn ; let = 0 root node graph.
exist values C (i; s; a) (for internal Max nodes) V (i; s) (for primitive, leaf Max
245

fiDietterich



V (0X; s)

XXXXX




X
V (a ; s)
P
PPPP
1

.
V (am,1 ; s)

. .

ZZ

ZZ


V (am ; s) C (am,1 ; s; )
r1

r2

r3

r4

r5

C (a1 ; s; a2 )
. . .

r8

r9

C (0; s; a1 )

r10 r11 r12 r13 r14

Figure 5: MAXQ decomposition; r1 ; : : : ; r14 denote sequence rewards received
primitive actions times 1; : : : ; 14.
nodes) V (0; s) (as computed decomposition equations (9), (10), (11))
expected discounted cumulative reward following policy starting state s.

Proof: proof induction number levels task graph.

level i, compute values C (i; s; (s)) (or V (i; s); primitive) according
decomposition equations. apply decomposition equations compute
Q (i; s; (s)) apply Equation (8) Theorem 1 conclude Q (i; s; (s)) gives
value function level i. = 0, obtain value function entire
hierarchical policy. Q. E. D.
important note representation theorem mention pseudoreward function, pseudo-reward used learning. theorem
captures representational power MAXQ decomposition, address
question whether learning algorithm find given policy.
subject next section.

4. Learning Algorithm MAXQ Decomposition
section presents central contributions paper. First, discuss optimality criteria employed hierarchical reinforcement learning. introduce
MAXQ-0 learning algorithm, learn value functions (and policies) MAXQ
hierarchies pseudo-rewards (i.e., pseudo-rewards zero).
central theoretical result paper MAXQ-0 converges recursively optimal
policy given MAXQ hierarchy. followed brief discussion ways
accelerating MAXQ-0 learning. section concludes description MAXQ-Q
learning algorithm, handles non-zero pseudo-reward functions.
246

fiMAXQ Hierarchical Reinforcement Learning

4.1 Two Kinds Optimality

order develop learning algorithm MAXQ decomposition, must consider
exactly hoping achieve. course, MDP , would like find
optimal policy . However, MAXQ method (and hierarchical reinforcement
learning general), programmer imposes hierarchy problem. hierarchy
constrains space possible policies may possible represent
optimal policy value function.
MAXQ method, constraints take two forms. First, within subtask,
possible primitive actions may permitted. example, taxi task,
Navigate(t), North, South, East, West actions available|the Pickup
Putdown actions allowed. Second, consider Max node Mj child nodes
fMj ; : : : ; Mjk g. policy learned Mj must involve executing learned policies
child nodes. policy child node Mji executed, run enters
state Tji . Hence, policy learned Mj must pass subset
terminal state sets fTj ; : : : ; Tjk g.
HAM method shares two constraints addition, imposes
partial policy node, policy subtask Mi must deterministic
refinement given non-deterministic initial policy node i.
\option" approach, policy even constrained. approach,
two non-primitive levels hierarchy, subtasks lower level (i.e.,
whose children primitive actions) given complete policies programmer.
Hence, learned policy upper level must constructed \concatenating"
given lower level policies order.
purpose imposing constraints policy incorporate prior knowledge
thereby reduce size space must searched find good policy.
However, constraints may make impossible learn optimal policy.
can't learn optimal policy, next best target would learn best
policy consistent (i.e., represented by) given hierarchy.
1

1

Definition 7 hierarchically optimal policy MDP policy achieves
highest cumulative reward among policies consistent given hierarchy.

Parr (1998b) proves HAMQ learning algorithm converges probability 1
hierarchically optimal policy. Similarly, given fixed set options, Sutton, Precup,
Singh (1998) prove SMDP learning algorithm converges hierarchically
optimal value function. Incidentally, also show primitive actions also
made available \trivial" options, SMDP method converges optimal
policy. However, case, hard say anything formal options speed
learning process. may fact hinder (Hauskrecht et al., 1998).
MAXQ decomposition represent value function hierarchical
policy, could easily construct modified version HAMQ algorithm apply
learn hierarchically optimal policies MAXQ hierarchy. However, decided
pursue even weaker form optimality, reasons become clear proceed.
form optimality called recursive optimality.
247

fiDietterich

MaxRoot

G

QExit

QGotoGoal

MaxExit

MaxGotoGoal

*

*

QExitNorth

QExitSouth

QExitEast

North

QNorthG

South

QSouthG

QEastG

East

Figure 6: simple MDP (left) associated MAXQ graph (right). policy shown
left diagram recursively optimal hierarchically optimal. shaded
cells indicate points locally-optimal policy globally optimal.

Definition 8 recursively optimal policy Markov decision process MAXQ
decomposition fM0 ; : : : ; Mk g hierarchical policy = f0 ; : : : ; k g

subtask Mi , corresponding policy optimal SMDP defined set states
Si , set actions Ai , state transition probability function P (s0 ; N js; a),
reward function given sum original reward function R(s0 js; a) pseudoreward function R~ (s0 ).

Note state transition probability distribution, P (s0 ; N js; a) subtask Mi
defined locally optimal policies fj g subtasks descendents Mi
MAXQ graph. Hence, recursive optimality kind local optimality
policy node optimal given policies children.
reason seek recursive optimality rather hierarchical optimality recursive optimality makes possible solve subtask without reference context
executed. context-free property makes easier share re-use
subtasks. also turn essential successful use state abstraction.
proceed describe learning algorithm recursive optimality, let us see
recursive optimality differs hierarchical optimality.
easy construct examples policies recursively optimal hierarchically optimal (and vice versa). Consider simple maze problem associated
MAXQ graph shown Figures 6. Suppose robot starts somewhere left room,
must reach goal G right room. robot three actions, North, South,
East, actions deterministic. robot receives reward ,1 move.
Let us define two subtasks:
248

fiMAXQ Hierarchical Reinforcement Learning

Exit. task terminates robot exits left room. set pseudo-

reward function R~ 0 two terminal states (i.e., two states indicated
*'s).
GotoGoal. task terminates robot reaches goal G.
arrows Figure 6 show locally optimal policy within room. arrows
left seek exit left room shortest path, specified
set pseudo-reward function 0. arrows right follow shortest
path goal, fine. However, resulting policy neither hierarchically optimal
optimal.
exists hierarchical policy would always exit left room upper
door. MAXQ value function decomposition represent value function
policy, policy would locally optimal (because, example, states
\shaded" region would follow shortest path doorway). Hence,
example illustrates recursively optimal policy hierarchically optimal
hierarchically optimal policy recursively optimal.
consider moment, see way fix problem. value
upper starred state optimal hierarchical policy ,2 value lower
starred state ,6. Hence, changed R~ values (instead zero),
recursively-optimal policy would hierarchically optimal (and globally optimal).
words, programmer guess right values terminal states
subtask, recursively optimal policy hierarchically optimal.
basic idea first pointed Dean Lin (1995). describe algorithm
makes initial guesses values starred states updates
guesses based computed values starred states resulting recursivelyoptimal policy. proved converge hierarchically optimal policy.
drawback method requires repeated solution resulting hierarchical
learning problem, always yield speedup solving original,
problem.
Parr (1998a) proposed interesting approach constructs set different R~ functions computes recursively optimal policy subtask.
method chooses R~ functions way hierarchically optimal policy
approximated desired degree. Unfortunately, method quite expensive,
relies solving series linear programming problems requires time
polynomial several parameters, including number states jSi j within subtask.
discussion suggests while, principle, possible learn good values
pseudo-reward function, practice, must rely programmer specify single
pseudo-reward function, R~ , subtask. programmer wishes consider small
number alternative pseudo-reward functions, handled defining small
number subtasks identical except R~ functions, permitting
learning algorithm choose one gives best recursively-optimal policy.
experiments, employed following simplified approach defining
R~ . subtask Mi, define two predicates: termination predicate, Ti ,
goal predicate, Gi . goal predicate defines subset terminal states \goal
states", pseudo-reward 0. terminal states fixed constant
249

fiDietterich

pseudo-reward (e.g., ,100) set always better terminate goal state
non-goal state. problems tested MAXQ method,
worked well.
experiments MAXQ, found easy make mistakes
defining Ti Gi . goal defined carefully, easy create set subtasks
lead infinite looping. example, consider problem Figure 6. Suppose
permit fourth action, West, MDP let us define termination goal
predicates right hand room satisfied iff either robot reaches goal
exits room. natural definition, since quite similar definition
left-hand room. However, resulting locally-optimal policy room
attempt move nearest three locations: goal, upper door,
lower door. easily see states near goal, policies
constructed MaxRoot loop forever, first trying leave left room
entering right room, trying leave right room entering left room.
problem easily fixed defining goal predicate Gi right room true
robot reaches goal G. avoiding \undesired termination" bugs
hard complex domains.
worst case, possible programmer specify pseudo-rewards
recursively optimal policy made arbitrarily worse hierarchically optimal
policy. example, suppose change original MDP Figure 6 state
immediately left upper doorway gives large negative reward ,L whenever
robot visits square. rewards everywhere else ,1, hierarchicallyoptimal policy exits room lower door. suppose programmer chosen
instead force robot exit upper door (e.g., assigning pseudo-reward
,10L leaving via lower door). case, recursively-optimal policy leave
upper door suffer large ,L penalty. making L arbitrarily large,
make difference hierarchically-optimal policy recursively-optimal
policy arbitrarily large.

4.2 MAXQ-0 Learning Algorithm
understanding recursively optimal policies, present two learning
algorithms. first one, called MAXQ-0, applies case pseudo-reward
function R~ always zero. first prove convergence properties show
extended give second algorithm, MAXQ-Q, works general
pseudo-reward functions.
Table 2 gives pseudo-code MAXQ-0. MAXQ-0 recursive function executes
current exploration policy starting Max node state s. performs actions
reaches terminal state, point returns count total number primitive
actions executed. execute action, MAXQ-0 calls recursively
(line 9). recursive call returns, updates value completion function
node i. uses count number primitive actions appropriately discount
value resulting state s0 . leaf nodes, MAXQ-0 updates estimated one-step
expected reward, V (i; s). value fft (i) \learning rate" parameter
gradually decreased zero limit.
250

fiMAXQ Hierarchical Reinforcement Learning

Table 2: MAXQ-0 learning algorithm.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

function MAXQ-0(MaxNode i, State s)
primitive MaxNode
execute i, receive r, observe result state s0
Vt (i; s) := (1 , fft (i)) Vt (i; s) + fft (i) rt
return 1
else
let count = 0
Ti (s) false
choose action according current exploration policy x(i; s)
let N = MAXQ-0(a; 0s) (recursive call)
observe result state
Ct (i; s; a) := (1 , fft (i)) Ct (i; s; a) + fft (i) N Vt (i; s0 )
+1

+1

count := count + N
:= s0

end
return count

end MAXQ-0

// Main program
initialize V (i; s) C (i; s; j ) arbitrarily
MAXQ-0(root node 0, starting state s0 )

three things must specified order make algorithm description
complete.
First, keep pseudo-code readable, Table 2 show \ancestor termination" handled. Recall action, termination predicates
subroutines calling stack checked. termination predicate one
satisfied, calling stack unwound highest terminated subroutine. cases, C values updated subroutines interrupted
except follows. subroutine invoked subroutine j , j 's termination condition
satisfied, subroutine update value C (i; s; j ).
Second, must specify compute Vt (i; s0 ) line 11, since stored
Max node. computed following modified versions decomposition
equations:
(

maxa Qt (i; s; a) composite
(13)
Vt (i; s)
primitive
Qt(i; s; a) = Vt(a; s) + Ct (i; s; a):
(14)
equations ect two important changes compared Equations (10) (11).
First, Equation (13), Vt (i; s) defined terms Q value best action a, rather
action chosen fixed hierarchical policy. Second, superscripts,
current value function, Vt (i; s), based fixed hierarchical policy .
compute Vt (i; s) using equations, must perform complete search
paths MAXQ graph starting node ending leaf nodes. Table 3

Vt (i; s) =

251

fiDietterich

Table 3: Pseudo-code Greedy Execution MAXQ Graph.
function EvaluateMaxNode(i; s)
1
2
3
4
5
6
7

primitive Max node
return hVt (i; s); ii
else
j 2 Ai ,
let hVt (j; s); aj = EvaluateMaxNode(j; s)
let j hg = argmaxj Vt(j; s) + Ct (i; s; j )
return hVt (j hg ; s); ajhg
end // EvaluateMaxNode

gives pseudo-code recursive function, EvaluateMaxNode, implements depthfirst search. addition returning Vt (i; s), EvaluateMaxNode also returns action
leaf node achieves value. information needed MAXQ-0,
useful later consider non-hierarchical execution learned recursivelyoptimal policy.
search computationally expensive, problem future research
develop ecient methods computing best path graph. One
approach perform best-first search use bounds values within subtrees
prune useless paths MAXQ graph. better approach would make
computation incremental, state environment changes,
nodes whose values changed result state change re-considered.
possible develop ecient bottom-up method similar RETE algorithm (and
successors) used SOAR architecture (Forgy, 1982; Tambe & Rosenbloom,
1994).
third thing must specified complete definition MAXQ-0
exploration policy, x . require x ordered GLIE policy.

Definition 9 ordered GLIE policy GLIE policy (Greedy Limit Infinite

Exploration) converges limit ordered greedy policy, greedy policy
imposes arbitrary fixed order ! available actions breaks ties favor
action appears earliest order.

need property order ensure MAXQ-0 converges uniquely-defined
recursively optimal policy. fundamental problem recursive optimality
general, Max node choice many different locally optimal policies given
policies adopted descendent nodes. different locally optimal policies
achieve locally optimal value function, give rise different probability transition functions P (s0 ; N js; i). result Semi-Markov Decision
Problems defined next level node MAXQ graph differ depending
various locally optimal policies chosen node i. differences may
lead better worse policies higher levels MAXQ graph, even though make
difference inside subtask i. practice, designer MAXQ graph need
design pseudo-reward function subtask ensure locally optimal policies
252

fiMAXQ Hierarchical Reinforcement Learning

equally valuable parent subroutine. carry formal analysis,
rely arbitrary tie-breaking mechanism.2 establish fixed ordering
Max nodes MAXQ graph (e.g., left-to-right depth-first numbering), break ties
favor lowest-numbered action, defines unique policy Max node.
consequently, induction, defines unique policy entire MAXQ graph. Let
us call policy r . use r subscript denote recursively optimal quantities
ordered greedy policy. Hence, corresponding value function Vr , Cr
Qr denote corresponding completion function action-value function. prove
MAXQ-0 algorithm converges r .

Theorem 3 Let = hS; A; P; R; P0 either episodic MDP deterministic

policies proper discounted infinite horizon MDP discount factor . Let H
MAXQ graph defined subtasks fM0 ; : : : ; Mk g pseudo-reward function
R~ (s0 ) zero s0. Let fft (i) > 0 sequence constants Max node



X
X
lim
ff
(

)
=
1

lim
ff2t (i) < 1
(15)

!1
!1
t=1

t=1

Let x (i; s) ordered GLIE policy node state assume
immediate rewards bounded. probability 1, algorithm MAXQ-0 converges
r , unique recursively optimal policy consistent H x.

Proof: proof follows argument similar introduced prove convergence

Q learning SARSA(0) (Bertsekas & Tsitsiklis, 1996; Jaakkola et al., 1994).
employ following result stochastic approximation theory, state without
proof:

Lemma 1 (Proposition 4.5 Bertsekas Tsitsiklis, 1996) Consider iteration
rt+1 (i) := (1 , fft (i))rt (i) + fft (i)((Urt )(i) + wt (i) + ut(i)):
Let Ft = fr0 (i); : : : ; rt (i); w0 (i); : : : ; wt,1 (i); ff0 (i); : : : ; fft (i); 8ig entire history
iteration.


(a) fft (i) 0 satisfy conditions (15)
(b) every t, noise terms wt (i) satisfy E [wt (i)jFt ] = 0
(c) Given norm jj jj Rn , exist constants B E [wt2 (i)jFt ]
+ B jjrtjj2 .
(d) exists vector r , positive vector , scalar fi 2 [0; 1),
t,

jjUrt , rjj fi jjrt , rjj

2. Alternatively, could break ties using stochastic policy chose randomly among tied
actions.

253

fiDietterich

(e) exists nonnegative random sequence converges zero probability
1
jut (i)j t(jjrt jj + 1)
rt converges r probability 1. notation jj jj denotes weighted maximum
norm
jA(i)j :
jjAjj = max
(i)

structure proof Theorem 3 inductive, starting leaves
MAXQ graph working toward root. employ different time clock
node count number update steps performed MAXQ-0 node.
variable always refer time clock current node i.
prove base case primitive Max node, note line 3 MAXQ-0
standard stochastic approximation algorithm computing expected reward
performing action state s, therefore converges conditions given
above.
prove recursive case, consider composite Max node child node j . Let
Pt (s0; N js; j ) transition probability distribution performing child action j state
time (i.e., following exploration policy descendent nodes node j ).
inductive assumption, MAXQ-0 applied j converge (unique) recursively optimal value function Vr (j; s) probability 1. Furthermore, MAXQ-0
following ordered GLIE policy j descendents, converge executing greedy policy respect value functions, Pt (s0 ; N js; j ) converge
Pr (s0 ; N js; j ), unique transition probability function executing child j
locally optimal policy r . remains shown update assignment C
(line 11 MAXQ-0 algorithm) converges optimal Cr function probability
1.
prove this, apply Lemma 1. identify x lemma
state-action pair (s; a). vector rt completion-cost table Ct (i; s; a)
s; fixed update steps. vector r optimal completion-cost
Cr (i; s; a) (again, fixed i). Define mapping U
(UC )(i; s; a) =

X

s0





0 0
0 0
Pr (s0 ; N js; a) N max
0 [C (i; ; ) + Vr (a ; )]


C update MDP Mi assuming descendent value functions,
Vr (a; s), transition probabilities, Pr(s0 ; N js; a), converged.
apply lemma, must first express C update formula form
update rule lemma. Let state results performing state s. Line
11 written

Ct+1 (i; s; a) :=

(1 , fft (i)) Ct (i; s; a) + fft (i) N





max[Ct (i; s; a0 ) + Vt (a0 ; s)]
a0

:= (1 , fft (i)) Ct (i; s; a) + fft (i) [(UCt )(i; s; a) + wt (i; s; a) + ut (i; s; a)]
254

fiMAXQ Hierarchical Reinforcement Learning







wt (i; s; a) = N max
[C (i; s; a0 ) + Vt (a0 ; s)] ,
a0
X

s0 ;N

ut (i; s; a) =

X

s0 ;N
X

s0 ;N









Pt (s0 ; N js; a) N max
[C (i; s0 ; a0 ) + Vt (a0 ; s0 )]
a0

0 0
0 0
Pt (s0 ; N js; a) N max
0 [Ct (i; ; ) + Vt (a ; )]




,



Pr (s0; N js; a) N max
[C (i; s0 ; a0 ) + Vr (a0 ; s0 )]
a0

wt (i; s; a) difference update node using single sample
point drawn according Pt (s0 ; N js; a) update using full distribution
Pt (s0; N js; a). value ut (i; s; a) captures difference update using
current probability transitions Pt (s0 ; N js; a) current value functions children
Vt (a0; s0 ) update using optimal probability transitions Pr (s0 ; N js; a)
optimal values children Vr (a0 ; s0 ).
verify conditions Lemma 1.
Condition (a) assumed conditions theorem fft (s; a) = fft (i).
Condition (b) satisfied sampled Pt (s0 ; N js; a), expected value
difference zero.
Condition (c) follows directly fact jCt (i; s; a)j jVt (i; s)j bounded.
show bounded episodic case discounted case
follows. episodic case, assumed policies proper. Hence, trajectories
terminate finite time finite total reward. discounted case, infinite sum
future rewards bounded one-step rewards bounded. values C V
computed temporal averages cumulative rewards received finite number
(bounded) updates, hence, means, variances, maximum values
bounded.
Condition (d) condition U weighted max norm pseudo-contraction.
derive starting weighted max norm Q learning. well known
Q weighted max norm pseudo-contraction (Bertsekas & Tsitsiklis, 1996)
episodic case deterministic policies proper (and discount factor = 1)
infinite horizon discounted case (with < 1). is, exists positive
vector scalar fi 2 [0; 1), t,

jjTQt , Qjj fi jjQt , Qjj ;

(16)

operator
(TQ)(s; a) =

X

s0 ;N

P (s0; N js; a) N [R(s0 js; a) + max
Q(s0 ; a0 )]:
a0

show derive pseudo-contraction C update operator U .
plan show first express U operator learning C terms operator
updating Q values. replace TQ pseudo-contraction equation Q
255

fiDietterich

learning UC , show U weighted max-norm pseudo-contraction
weights fi .
Recall Eqn. (10) Q(i; s; a) = C (i; s; a) + V (a; s). Furthermore, U operator
performs updates using optimal value functions child nodes, write
Qt (i; s; a) = Ct (i; s; a) + V (a; s). children node converged,
Q-function version Bellman equation MDP Mi written

Q(i; s; a) =

X

s0 ;N

Pr(s0 ; N js; a) N [Vr (a; s) + max
Q(i; s0 ; a0 )]:
a0

noted before, Vr (a; s) plays role immediate reward function Mi .
Therefore, node i, operator rewritten
(TQ)(i; s; a) =

X

s0 ;N

Pr (s0 js; a) N [Vr(a; s) + max
Q(i; s0 ; a0 )]:
a0

replace Q(i; s; a) C (i; s; a) + Vr (a; s), obtain
(TQ)(i; s; a) =

X

s0 ;N

Pr (s0; N js; a) N (Vr (a; s) + max
[C (i; s0 ; a0 ) + Vr (a0 ; s0 )]):
a0

Note Vr (a; s) depend s0 N , move outside expectation
obtain
(TQ)(i; s; a) = Vr (a; s) +

X

s0 ;N

Pr (s0 ; N js; a) N (max
[C (i; s0 ; a0 ) + Vr (a0 ; s0 )])
a0

= Vr (a; s) + (UC )(i; s; a)

Abusing notation slightly, express vector form TQ(i) = Vr + UC (i).
Similarly, write Qt (i; s; a) = Ct (i; s; a)+ Vr (a; s) vector form Qt (i) = Ct (i)+ Vr .
substitute two formulas max norm pseudo-contraction formula
, Eqn. (16) obtain

jjVr + UCt (i) , (Cr(i) + Vr)jj fi jjVr + Ct (i) , (Cr(i) + Vr)jj :
Thus, U weighted max-norm pseudo-contraction,

jjUCt (i) , Cr(i)jj fi jjCt (i) , Cr(i)jj ;
condition (d) satisfied.
Finally, easy verify (e), important condition. assumption,
ordered GLIE policies child nodes converge probability 1 locally optimal
policies children. Therefore Pt (s0 ; N js; a) converges Pr (s0 ; N js; a) s0; N; s;
probability 1 Vt (a; s) converges probability 1 Vr (a; s) child
actions a. Therefore, jut j converges zero probability 1. trivially construct
sequence = jut j bounds convergence,

jut (s; a)j (jjCt (s; a)jj + 1):
256

fiMAXQ Hierarchical Reinforcement Learning

verified conditions Lemma 1, conclude Ct (i) converges
Cr(i) probability 1. induction, conclude holds nodes
MAXQ including root node, value function represented MAXQ graph
converges unique value function recursively optimal policy r . Q.E.D.
important aspect theorem proves Q learning take
place levels MAXQ hierarchy simultaneously|the higher levels need
wait lower levels converged begin learning. necessary
lower levels eventually converge (locally) optimal policies.

4.3 Techniques Speeding MAXQ-0

Algorithm MAXQ-0 extended accelerate learning higher nodes graph
technique call \all-states updating". action chosen Max node
state s, execution move environment sequence states
= s1; : : : ; sN ; sN +1 = s0. subroutines Markovian, resulting
state s0 would reached started executing action state s2 , s3 ,
state including sN . Hence, execute version line 11 MAXQ-0
intermediate states shown replacement pseudo-code:
11a
j 1 N
11b
Ct (i; sj ; a) := (1 , fft (i)) Ct(i; sj ; a) + fft (i) N ,j maxa Qt (i; s0 ; a0 )
11c
end //
implementation, composite action executed MAXQ-0, constructs
linked list sequence primitive states visited. list returned
composite action terminates. parent Max node process state
list shown above. parent Max node concatenates state lists receives
children passes parent terminates. experiments paper
employ all-states updating.
Kaelbling (1993) introduced related, powerful, method accelerating hierarchical reinforcement learning calls \all-goals updating." understand
method, suppose primitive action, several composite tasks could
invoked primitive action. all-goals updating, whenever primitive action
executed, equivalent line 11 MAXQ-0 applied every composite task could
invoked primitive action. Sutton, Precup, Singh (1998) prove
composite tasks converge optimal Q values all-goals updating. Furthermore, point exploration policy employed choosing primitive
actions different policies subtasks learned.
straightforward implement simple form all-goals updating within MAXQ
hierarchy case composite tasks invoke primitive actions. Whenever one
primitive actions executed state s, update C (i; s; a) value parent
tasks invoke a.
However, additional care required implement all-goals updating non-primitive
actions. Suppose executing exploration policy, following sequence world
states actions obtained: s0 ; a0 ; s1 ; : : : ; ak,1 ; sk,1 ; ak ; sk+1 . Let j composite task terminated state sk+1 , let sk,n; ak,n ; : : : ; ak,1 ; ak sequence
actions could executed subtask j children. words, suppose
(

+1

257

+1

)

0

fiDietterich

possible \parse" state-action sequence terms series subroutine calls
returns one invocation subtask j . possible parent task invokes j ,
update value C (i; sk,n ; j ). course, order updates useful,
exploration policy must ordered GLIE policy converge recursively
optimal policy subtask j descendents. cannot follow arbitrary exploration
policy, would produce accurate samples result states drawn according
P (s0 ; N js; j ). Hence, unlike simple case described Sutton, Precup, Singh,
exploration policy cannot different policies subtasks learned.
Although considerably reduces usefulness all-goals updating,
completely eliminate it. simple way implementing non-primitive all-goals updating
would perform MAXQ-Q learning usual, whenever subtask j invoked
state returned, could update value C (i; s; j ) potential calling subtasks
i. implemented this, however, complexity involved identifying
possible actual parameters potential calling subroutines.

4.4 MAXQ-Q Learning Algorithm
shown convergence MAXQ-0, let us design learning algorithm
work arbitrary pseudo-reward functions, R~ (s0 ). could add pseudoreward MAXQ-0, would effect changing MDP
different reward function. pseudo-rewards \contaminate" values
completion functions computed hierarchy. resulting learned policy
recursively optimal original MDP.
problem solved learning one completion function use \inside"
Max node separate completion function use \outside" Max node. quantities used \inside" node written tilde: R~ , C~ , Q~ . quantities used
\outside" node written without tilde.
\outside" completion function, C (i; s; a) completion function
discussing far paper. computes expected reward completing task
Mi performing action state following learned policy Mi .
computed without reference R~ . completion function used parent
tasks compute V (i; s), expected reward performing action starting state s.
second completion function C~ (i; s; a) completion function use
\inside" node order discover locally optimal policy task Mi . function
incorporate rewards \real" reward function, R(s0 js; a),
pseudo-reward function, R~ (s0 ). also used EvaluateMaxNode line 6
choose best action j hg execute. Note, however, EvaluateMaxNode still
return \external" value Vt (j hg ; s) chosen action.
employ two different update rules learn two completion functions.
C~ function learned using update rule similar Q learning rule line 11
MAXQ-0. C function learned using update rule similar SARSA(0)|
purpose learn value function policy discovered optimizing C~ .
Pseudo-code resulting algorithm, MAXQ-Q shown Table 4.
key step lines 15 16. line 15, MAXQ-Q first updates C~ using value
greedy action, , resulting state. update includes pseudo-reward R~ .
258

fiMAXQ Hierarchical Reinforcement Learning

Table 4: MAXQ-Q learning algorithm.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

function MAXQ-Q(MaxNode i, State s)
let seq = () sequence states visited executing
primitive MaxNode
execute i, receive r, observe result state s0
Vt (i; s) := (1 , fft (i)) Vt (i; s) + fft (i) rt
push onto beginning seq
else
let count = 0
Ti (s) false
choose action according current exploration policy x(i; s)
let childSeq = MAXQ-Q(a;s), childSeq sequence states visited
+1

executing action a. (in reverse order)
observe result state s0
let = argmaxa [C~t (i; s0 ; a0 ) + Vt (a0 ; s0 )]
let N = 1
childSeq
C~t+1 (i; s; a) := (1 , fft (i)) C~t (i; s; a) + fft (i) N [R~ (s0 ) + C~t (i; s0 ; ) + Vt(a ; s)]
Ct+1 (i; s; a) := (1 , fft (i)) Ct (i; s; a) + fft (i) N [Ct (i; s0 ; ) + Vt(a ; s0 )]
N := N + 1
end //
append
childSeq onto front seq
:= s0
end //
end // else
return seq
end MAXQ-Q
0

line 16, MAXQ-Q updates C using greedy action , even would
greedy action according \uncontaminated" value function. update,
course, include pseudo-reward function.
important note whereever Vt (a; s) appears pseudo-code, refers
\uncontaminated" value function state executing Max node a.
computed recursively exactly way MAXQ-0.
Finally, note pseudo-code also incorporates all-states updating, call
MAXQ-Q returns list states visited execution,
updates lines 15 16 performed states. list states
ordered most-recent-first, states updated starting last state visited
working backward starting state, helps speed algorithm.
MAXQ-Q converged, resulting recursively optimal policy computed
node choosing action maximizes Q~ (i; s; a) = C~ (i; s; a)+ V (a; s) (breaking
ties according fixed ordering established ordered GLIE policy).
reason gave name \Max nodes" nodes represent subtasks (and
learned policies) within MAXQ graph. Q node j parent node stores
C~ (i; s; j ) C (i; s; j ), computes Q~ (i; s; j ) Q(i; s; j ) invoking child
Max node j . Max node takes maximum Q values computes either
V (i; s) computes best action, using Q~ .
259

fiDietterich

Corollary 1 conditions Theorem 3, MAXQ-Q converges unique

recursively optimal policy MDP defined MAXQ graph H , pseudo-reward functions
R~ , ordered GLIE exploration policy x.
Proof: argument identical to, tedious than, proof Theorem 3.
proof convergence C~ values identical original proof C values,
relies proving convergence \new" C values well, follows
weighted max norm pseudo-contraction argument. Q.E.D.

5. State Abstraction

many reasons introduce hierarchical reinforcement learning, perhaps
important reason create opportunities state abstraction. introduced
simple taxi problem Figure 1, pointed within subtask, ignore
certain aspects state space. example, performing MaxNavigate(t),
taxi make navigation decisions regardless whether passenger
taxi. purpose section formalize conditions safe
introduce state abstractions show convergence proofs MAXQ-Q
extended prove convergence presence state abstraction. Specifically,
identify five conditions permit \safe" introduction state abstractions.
Throughout section, use taxi problem running example,
see five conditions permit us reduce number distinct values
must stored order represent MAXQ value function decomposition.
establish starting point, let us compute number values must stored
taxi problem without state abstraction.
MAXQ representation must tables C functions internal
nodes V functions leaves. First, six leaf nodes, store V (i; s),
must store 500 values node, 500 states; 25 locations, 4 possible
destinations passenger, 5 possible current locations passenger (the four
special locations inside taxi itself). Second, root node, two children,
requires 2 500 = 1000 values. Third, MaxGet MaxPut nodes, 2
actions each, one requires 1000 values, total 2000. Finally, MaxNavigate(t),
four actions, must also consider target parameter t, take
four possible values. Hence, effectively 2000 combinations states values
action, 8000 total values must represented. total, therefore, MAXQ
representation requires 14,000 separate quantities represent value function.
place number perspective, consider Q learning representation must
store separate value six primitive actions 500 possible states,
total 3,000 values. Hence, see without state abstraction, MAXQ
representation requires four times memory Q table!

5.1 Five Conditions Permit State Abstraction

introduce five conditions permit introduction state abstractions.
condition, give definition prove lemma states condition satisfied, value function corresponding class policies
260

fiMAXQ Hierarchical Reinforcement Learning

represented abstractly (i.e., abstract versions V C functions). condition, provide rules identifying condition satisfied
give examples taxi domain.
begin introducing definitions notation.

Definition 10 Let MDP H MAXQ graph defined . Suppose

state written vector values set state variables. Max
node i, suppose state variables partitioned two sets Xi Yi , let
function projects state onto values variables Xi . H combined
called state-abstracted MAXQ graph.

cases state variables partitioned, often write = (x; y)
mean state represented vector values state variables X
vector values state variables . Similarly, sometimes write
P (x0 ; y0; N jx; y; a), V (a; x; y), R~ (x0 ; y0 ) place P (s0; N js; a), V (a; s), R~ (s0 ),
respectively.

Definition 11 (Abstract Policy) abstract hierarchical policy MDP state-

abstracted MAXQ graph H associated abstraction functions , hierarchical policy
policy (corresponding subtask Mi ) satisfies condition two
states s1 s2 (s1 ) = (s2 ), (s1 ) = (s2 ). (When stochastic policy,
exploration policy, interpreted mean probability distributions
choosing actions states.)

order MAXQ-Q converge presence state abstractions, require
times (instantaneous) exploration policy abstract hierarchical policy.
One way achieve construct exploration policy uses information relevant state variables deciding action perform. Boltzmann exploration based (state-abstracted) Q values, -greedy exploration, counter-based
exploration based abstracted states abstract exploration policies. Counter-based
exploration based full state space abstract exploration policy.
introduced notation, let us describe analyze five abstraction conditions. identified three different kinds conditions
abstractions introduced. first kind involves eliminating irrelevant variables
within subtask MAXQ graph. form abstraction, nodes toward
leaves MAXQ graph tend relevant variables, nodes higher
graph relevant variables. Hence, kind abstraction useful
lower levels MAXQ graph.
second kind abstraction arises \funnel" actions. macro actions
move environment large number initial states small number
resulting states. completion cost subtasks represented using number
values proportional number resulting states. Funnel actions tend appear higher
MAXQ graph, form abstraction useful near root graph.
third kind abstraction arises structure MAXQ graph itself.
exploits fact large parts state space subtask may reachable
termination conditions ancestors MAXQ graph.
261

fiDietterich

begin describing two abstraction conditions first type. present
two conditions second type. finally, describe one condition third type.
5.1.1 Condition 1: Max Node Irrelevance

first condition arises set state variables irrelevant Max node.

Definition 12 (Max Node Irrelevance) Let Mi Max node MAXQ graph H

MDP . set state variables irrelevant node state variables
partitioned two sets X stationary abstract hierarchical
policy executed descendents i, following two properties hold:

state transition probability distribution P (s0; N js; a) node factored
product two distributions:

P (x0 ; y0 ; N jx; y; a) = P (y0jx; y; a) P (x0 ; N jx; a);

(17)

y0 give values variables , x x0 give values
variables X .

pair states s1 = (x; y1 ) s2 = (x; y2 ) (s1) = (s2) = x,
child action a, V (a; s1 ) = V (a; s2 ) R~ (s1 ) = R~ (s2 ).

Note two conditions must hold stationary abstract policies executed
descendents subtask i. discuss rather strong
requirements satisfied practice. First, however, prove conditions
sucient permit C V tables represented using state abstractions.

Lemma 2 Let MDP full-state MAXQ graph H , suppose state vari-

ables Yi irrelevant Max node i. Let (s) = x associated abstraction function
projects onto remaining relevant variables Xi . Let abstract hierarchical
policy. action-value function Q node represented compactly,
one value completion function C (i; s; j ) equivalence class states
share values relevant variables.
Specifically Q (i; s; j ) computed follows:

Q (i; s; j ) = V (j; (s)) + C (i; (s); j )


C (i; x; j ) =

X

x0 ;N

P (x0 ; N jx; j ) N [V ((x0 ); x0 ) + R~ (x0 ) + C (i; x0 ; (x0 ))];

V (j 0 ; x0 ) = V (j 0 ; x0 ; y0 ), R~ (x0 ) = R~ (x0 ; y0 ), (x) = (x; y0 ) arbitrary
value y0 irrelevant state variables Yi .
262

fiMAXQ Hierarchical Reinforcement Learning

Proof: Define new MDP i(Mi ) node follows:
States: X = fx j i(s) = x; 2 g.
Actions: A.
Transition probabilities: P (x0 ; N jx; a)
Reward function: V (a; x) + R~ i(x0)

abstract policy, decisions states (s) = x
x. Therefore, also well-defined policy (Mi ). action-value function
(Mi ) unique solution following Bellman equation:
X
Q (i; x; j ) = V (j; x) + P (x0 ; N jx; j ) N [R~ i(x0 ) + Q (i; x0 ; (x0 ))]
(18)
x0 ;N

Compare Bellman equation Mi :
X
Q (i; s; j ) = V (j; s) + P (s0; N js; j ) N [R~i (s0) + Q (i; s0 ; (s0 ))]
s0 ;N

(19)

note V (j; s) = V (j; (s)) = V (j; x) R~ (s0 ) = R~ ((s0 )) = R~ (x0 ). Furthermore, know distribution P factored separate distributions Yi
Xi . Hence, rewrite (19)
X
X
Q (i; s; j ) = V (j; x) + P (y0 jx; y; j ) P (x0 ; N jx; j ) N [R~ (x0 ) + Q (i; s0 ; (s0 ))]
y0

x0 ;N

right-most sum depend y0 , sum y0 evaluates 1,
eliminated give
X
Q (i; s; j ) = V (j; x) + P (x0 ; N jx; j ) N [R~ i(x0 ) + Q (i; s0 ; (s0 ))]:
(20)
x0 ;N

Finally, note equations (18) (20) identical except expressions
Q values. Since solution Bellman equation unique, must conclude
Q (i; s; j ) = Q (i; (s); j ):
rewrite right-hand side obtain
Q (i; s; j ) = V (j; (s)) + C (i; (s); j );

X
C (i; x; j ) = P (x0; N jx; j ) N [V ((x0 ); x0 ) + R~ (x0 ) + C (i; x0 ; (x0 ))]:

Q.E.D.

x0 ;N

course primarily interested able discover represent optimal
policy node i. following corollary shows optimal policy abstract
policy, hence, represented abstractly.
263

fiDietterich

Corollary 2 Consider conditions Lemma 2, change ab-

stract hierarchical policy executed descendents node i, node
i. Let ! ordering actions. optimal ordered policy ! node
abstract policy, action-value function represented abstractly.

Proof: Define policy ! optimal ordered policy abstract MDP

(M ), let Q (i; x; j ) corresponding optimal action-value function.
argument given above, Q also solution optimal Bellman equation
original MDP. means policy ! defined ! (s) = ((s)) optimal

ordered policy, construction, abstract policy. Q.E.D.
stated, Max node irrelevance condition appears quite dicult satisfy, since
requires state transition probability distribution factor X components
possible abstract hierarchical policies. However, practice, condition often
satisfied.
example, let us consider Navigate(t) subtask. source destination
passenger irrelevant achievement subtask. policy successfully completes subtask value function regardless source
destination locations passenger. abstracting away passenger source destination, obtain huge savings space. Instead requiring 8000 values represent
C functions task, require 400 values (4 actions, 25 locations, 4 possible
values t).
advantages form abstraction similar obtained Boutilier,
Dearden Goldszmidt (1995) belief network models actions exploited
simplify value iteration stochastic planning. Indeed, one way understanding
conditions Definition 12 express form decision diagram, shown
Figure 7. diagram shows irrelevant variables affect rewards
either directly indirectly, therefore, affect either value function
optimal policy.
One rule noticing cases abstraction condition holds examine
subgraph rooted given Max node i. set state variables irrelevant leaf
state transition probabilities reward functions also pseudo-reward functions
termination conditions subgraph, variables satisfy Max Node
Irrelevance condition:

Lemma 3 Let MDP associated MAXQ graph H , let Max node

H . Let Xi Yi partition state variables . set state variables Yi
irrelevant node
primitive leaf node descendent i,
P (x0 ; y0 jx; y; a) = P (y0 jx; y; a)P (x0 jx; a)
R(x0; y0 jx; y; a) = R(x0 jx; a),

internal node j equal node descendent , R~ j (x0 ; y0) =
R~j (x0) termination predicate Tj (x0 ; y0 ) true iff Tj (x0).
264

fiMAXQ Hierarchical Reinforcement Learning

j

V

X

X





Figure 7: dynamic decision diagram represents conditions Definition 12.
probabilistic nodes X represent state variables time t, nodes
X 0 0 represent state variables later time + N . square action
node j chosen child subroutine, utility node V represents value
function V (j; x) child action. Note X may uence 0 ,
cannot affect X 0 , therefore, cannot affect V .

Proof: must show abstract hierarchical policy give rise SMDP

node whose transition probability distribution factors whose reward function depends
Xi . definition, abstract hierarchical policy choose actions based
upon information Xi . primitive probability transition functions factor
independent component Xi since termination conditions nodes
based variables Xi , probability transition function Pi (x0 ; y0 ; N jx; y; a)
must also factor Pi (y0 jx; y; a) Pi (x0 ; N jx; a). Similarly, reward functions
V (j; x; y) must equal V (j; x), rewards received within subtree (either
leaves pseudo-rewards) depend variables Xi . Therefore,
variables Yi irrelevant Max node i. Q.E.D.
Taxi task, primitive navigation actions, North, South, East, West
depend location taxi location passenger. pseudoreward function termination condition MaxNavigate(t) node depend
location taxi (and parameter t). Hence, lemma applies, passenger
source destination irrelevant MaxNavigate node.
5.1.2 Condition 2: Leaf Irrelevance

second abstraction condition describes situations apply state abstractions leaf nodes MAXQ graph. leaf nodes, obtain stronger result
Lemma 2 using slightly weaker definition irrelevance.
265

fiDietterich

Definition 13 (Leaf Irrelevance) set state variables irrelevant primitive
action MAXQ graph states expected value reward function,
X
V (a; s) = P (s0 js; a)R(s0 js; a)
s0

depend values state variables . words,
pair states s1 s2 differ values variables ,
X

s01

P (s01 js1 ; a)R(s01 js1 ; a) =

X

s02

P (s02 js2 ; a)R(s02 js2 ; a):

condition satisfied leaf a, following lemma shows
represent value function V (a; s) compactly.

Lemma 4 Let MDP full-state MAXQ graph H , suppose state vari-

ables irrelevant leaf node a. Let (s) = x associated abstraction function
projects onto remaining relevant variables X . represent V (a; s)
state abstracted value function V (a; (s)) = V (a; x).

Proof: According definition Leaf Irrelevance, two states differ

irrelevant state variables value V (a; s). Hence, represent
unique value V (a; x). Q.E.D.
two rules finding cases Leaf Irrelevance applies. first rule shows
probability distribution factors, Leaf Irrelevance.

Lemma 5 Suppose probability transition function primitive action a, P (s0js; a), factors P (x0 ; y0 jx; y; a) = P (y0 jx; y; a)P (x0 jx; a) reward function satisfies R(s0 js; a) =
R(x0jx; a). variables irrelevant leaf node a.
Proof: Plug definition V (a; s) simplify.
X
V (a; s) =
P (s0 js; a)R(s0 js; a)
=
=
=

s0

X

x0 ;y0
X

y0

X

x0

P (y0 jx; y; a)P (x0 jx; a)R(x0 jx; a)

P (y0 jx; y; a)

X

x0

P (x0 jx; a)R(x0 jx; a)

P (x0 jx; a)R(x0 jx; a)

Hence, expected reward action depends variables X
variables . Q.E.D.
second rule shows reward function primitive action constant,
apply state abstractions even P (s0 js; a) factor.

Lemma 6 Suppose R(s0js; a) (the reward function action MDP ) always equal

constant ra . entire state irrelevant primitive action a.
266

fiMAXQ Hierarchical Reinforcement Learning

Proof:
V (a; s) =

X

s0

P (s0 js; a)R(s0 js; a)

X

=
P (s0 js; a)ra
0

= ra :
depend s, entire state irrelevant primitive action a. Q.E.D.
lemma satisfied four leaf nodes North, South, East, West taxi
task, one-step reward constant (,1). Hence, instead requiring 2000
values store V functions, need 4 values|one action. Similarly,
expected rewards Pickup Putdown actions require 2 values, depending
whether corresponding actions legal illegal. Hence, together, require 4
values, instead 1000 values.
5.1.3 Condition 3: Result Distribution Irrelevance

consider condition results \funnel" actions.

Definition 14 (Result Distribution Irrelevance). set state variables Yj irrelevant result distribution action j if, abstract policies executed node j
descendents MAXQ hierarchy, following holds: pairs states s1
s2 differ values state variables Yj ,

P (s0 ; N js1 ; j ) = P (s0 ; N js2 ; j )
s0 N .

condition satisfied subtask j , C value parent task
represented compactly:

Lemma 7 Let MDP full-state MAXQ graph H , suppose set

state variables Yj irrelevant result distribution action j , child Max
node i. Let ij associated abstraction function: ij (s) = x. define
abstract completion cost function C (i; ij (s); j ) states s,

C (i; s; j ) = C (i; ij (s); j ):

Proof: completion function fixed policy defined follows:
X
C (i; s; j ) = P (s0 ; N js; j ) N Q (i; s0 ):
s0 ;N

(21)

Consider two states s1 s2 , ij (s1 ) = ij (s2 ) = x. Result Distribution Irrelevance, transition probability distributions same. Hence,
right-hand sides (21) value, conclude

C (i; s1 ; j ) = C (i; s2 ; j ):
267

fiDietterich

Therefore, define abstract completion function, C (i; x; j ) represent quantity. Q.E.D.
undiscounted cumulative reward problems, definition result distribution irrelevance weakened eliminate N , number steps. needed
pairs states s1 s2 differ irrelevant state variables,
P (s0 js1 ; j ) = P (s0 js2; j ) (for s0 ). undiscounted case, Lemma 7 still holds
revised definition.
might appear result distribution irrelevance condition would rarely satisfied, often find cases condition true. Consider, example, Get
subroutine taxi task. matter location taxi state s, taxi
passenger's starting location Get finishes executing (i.e.,
taxi completed picking passenger). Hence, starting location
irrelevant resulting location taxi, P (s0 js1 ; Get) = P (s0 js2 ; Get)
states s1 s2 differ taxi's location.
Note, however, maximizing discounted reward, taxi's location would
irrelevant, probability Get terminate exactly N steps would
depend location taxi, could differ states s1 s2 . Different values
N produce different amounts discounting (21), hence, cannot ignore
taxi location representing completion function Get.
undiscounted case, applying Lemma 7, represent C (Root; s; Get)
using 16 distinct values, 16 equivalence classes states (4 source locations
times 4 destination locations). much less 500 quantities unabstracted
representation.
Note although state variables may irrelevant result distribution
subtask j , may important within subtask j . Taxi task, location
taxi critical representing value V (Get; s), irrelevant result state
distribution Get, therefore irrelevant representing C (Root; s; Get). Hence,
MAXQ decomposition essential obtaining benefits result distribution irrelevance.
\Funnel" actions arise many hierarchical reinforcement learning problems. example, abstract actions move robot doorway move car onto entrance
ramp freeway property. Result Distribution Irrelevance condition
applicable situations long undiscounted setting.
5.1.4 Condition 4: Termination

fourth condition closely related \funnel" property. applies subtask
guaranteed cause parent task terminate goal state. sense, subtask
funneling environment set states described goal predicate
parent task.

Lemma 8 (Termination). Let Mi task MAXQ graph states
goal predicate Gi (s) true, pseudo-reward function R~ (s) = 0. Suppose
child task state hierarchical policies ,

8 s0 Pi (s0; N js; a) > 0 ) Gi(s0 ):
268

fiMAXQ Hierarchical Reinforcement Learning

(i.e., every possible state s0 results applying make goal predicate,
Gi , true.)
policy executed node i, completion cost C (i; s; a) zero
need explicitly represented.

Proof: action executed state s, guaranteed result state s0

Gi (s) true. definition, goal states also satisfy termination predicate Ti (s),
task terminate. Gi(s) true, terminal pseudo-reward zero,
hence, completion function always zero. Q.E.D.
example, Taxi task, states taxi holding passenger,
Put subroutine succeed result goal terminal state Root.
termination predicate Put (i.e., passenger destination location)
implies goal condition Root (which same). means C (Root; s; Put)
uniformly zero, states Put terminated.
easy detect cases Termination condition satisfied. need
compare termination predicate Ta subtask goal predicate Gi parent
task. first implies second, termination lemma satisfied.
5.1.5 Condition 5: Shielding

shielding condition arises structure MAXQ graph.
Lemma 9 (Shielding). Let Mi task MAXQ graph state
paths root graph node Mi subtask j (possibly equal i)
whose termination predicate Tj (s) true, Q nodes Mi need represent
C values state s.
Proof: order task executed state s, must exist path ancestors
task leading root graph ancestor tasks
terminated. condition lemma guarantees false, hence task
cannot executed state s. Therefore, C values need represented. Q.E.D.
Termination condition, Shielding condition verified analyzing
structure MAXQ graph identifying nodes whose ancestor tasks terminated.
Taxi domain, simple example arises Put task, terminated
states passenger taxi. means need
represent C (Root; s; Put) states. result that, combined
Termination condition above, need explicitly represent completion function
Put all!
5.1.6 Dicussion

applying five abstraction conditions, obtain following \safe" state abstractions Taxi task:
North, South, East, West. terminal nodes require one quantity each,
total four values. (Leaf Irrelevance).
269

fiDietterich

Pickup Putdown require 2 values (legal illegal states), total four.
(Leaf Irrelevance.)

QNorth(t), QSouth(t), QEast(t), QWest(t) require 100 values (four values
25 locations). (Max Node Irrelevance.)

QNavigateForGet requires 4 values (for four possible source locations). (The passenger destination Max Node Irrelevant MaxGet, taxi starting location
Result Distribution Irrelevant Navigate action.)

QPickup requires 100 possible values, 4 possible source locations 25 possible taxi
locations. (Passenger destination Max Node Irrelevant MaxGet.)

QGet requires 16 possible values (4 source locations, 4 destination locations). (Result
Distribution Irrelevance.)

QNavigateForPut requires 4 values (for four possible destination locations).

(The passenger source destination Max Node Irrelevant MaxPut; taxi
location Result Distribution Irrelevant Navigate action.)

QPutdown requires 100 possible values (25 taxi locations, 4 possible destination locations). (Passenger source Max Node Irrelevant MaxPut.)

QPut requires 0 values. (Termination Shielding.)
gives total 632 distinct values, much less 3000 values required
Q learning. Hence, see applying state abstractions, MAXQ
representation give much compact representation value function.
key thing note state abstractions, value function decomposed sum terms single term depends entire state MDP,
even though value function whole depend entire state MDP.
example, consider state described Figures 1 4. There, showed
value state s1 passenger R, destination B, taxi (0,3)
decomposed

V (Root; s1 ) = V (North; s1 ) + C (Navigate(R); s1 ; North) +
C (Get; s1 ; Navigate(R)) + C (Root; s1 ; Get)
state abstractions, see term right-hand side depends
subset features:

V (North; s1) constant
C (Navigate(R); s1 ; North) depends taxi location passenger's source
location.

C (Get; s1; Navigate(R)) depends source location.
C (Root; s1 ; Get) depends passenger's source destination.
270

fiMAXQ Hierarchical Reinforcement Learning

Without MAXQ decomposition, features irrelevant, value function depends entire state.
prior knowledge required part programmer order identify
state abstractions? suces know qualitative constraints one-step
reward functions, one-step transition probabilities, termination predicates, goal
predicates, pseudo-reward functions within MAXQ graph. Specifically, Max
Node Irrelevance Leaf Irrelevance conditions require simple analysis one-step
transition function reward pseudo-reward functions. Opportunities apply
Result Distribution Irrelevance condition found identifying \funnel" effects
result definitions termination conditions operators. Similarly,
Shielding Termination conditions require analysis termination predicates
various subtasks. Hence, applying five conditions introduce state abstractions
straightforward process, model one-step transition reward functions
learned, abstraction conditions checked see satisfied.

5.2 Convergence MAXQ-Q State Abstraction

shown state abstractions safely introduced MAXQ value
function decomposition five conditions described above. However, conditions guarantee value function fixed abstract hierarchical policy
represented|they show recursively optimal policies represented,
show MAXQ-Q learning algorithm find recursively optimal policy
forced use state abstractions. goal section prove two
results: (a) ordered recursively-optimal policy abstract policy (and, hence,
represented using state abstractions) (b) MAXQ-Q converge
policy applied MAXQ graph safe state abstractions.

Lemma 10 Let MDP full-state MAXQ graph H abstract-state MAXQ
graph (H ) abstractions satisfy five conditions given above. Let !
ordering actions MAXQ graph. following statements true:
unique ordered recursively-optimal policy r defined , H , ! abstract policy (i.e., depends relevant state variables node; see
Definition 11),
C V functions (H ) represent projected value function r.

Proof: five abstraction lemmas tell us ordered recursively-optimal policy
abstract, C V functions (H ) represent value function. Hence,
heart lemma first claim. last two forms abstraction (Shielding
Termination) place restrictions abstract policies, ignore
proof.
proof induction levels MAXQ graph, starting leaves.
base case, let us consider Max node whose children primitive actions.
case, policies executed within children Max node. Hence variables
Yi irrelevant node i, apply abstraction lemmas represent
value function policy node i|not abstract policies. Consequently, value
271

fiDietterich

function optimal policy node represented, property

Q(i; s1 ; a) = Q (i; s2 ; a)
(22)
states s1 s2 (s1 ) = (s2 ).
let us impose action ordering ! compute optimal ordered policy. Consider
two actions a1 a2 !(a1 ; a2 ) (i.e., ! prefers a1 ), suppose
\tie" Q function state s1 values

Q (i; s1 ; a1 ) = Q (i; s1 ; a2 )
two actions maximize Q state. optimal ordered
policy must choose a1 . states s2 (s1 ) = (s2 ),
established (22) Q values same. Hence, tie exist
a1 a2 , hence, optimal ordered policy must make choice
states. Hence, optimal ordered policy node abstract policy.
let us turn recursive case Max node i. Make inductive assumption
ordered recursively-optimal policy abstract within descendent nodes consider
locally optimal policy node i. set state variables irrelevant
node i, Corollary 2 tells us Q (i; s1 ; j ) = Q (i; s2 ; j ) states s1 s2
i(s1) = (s2 ). Similarly, set variables irrelevant result distribution
particular action j , Lemma 7 tells us thing. Hence, ordering
argument given above, ordered optimal policy node must abstract. induction,
proves lemma. Q.E.D.
lemma, established combination MDP , abstract
MAXQ graph H , action ordering defines unique recursively-optimal ordered abstract policy. ready prove MAXQ-Q converge policy.

Theorem 4 Let = hS; A; P; R; P0 either episodic MDP deterministic

policies proper discounted infinite horizon MDP discount factor < 1. Let H
unabstracted MAXQ graph defined subtasks fM0 ; : : : ; Mk g pseudo-reward
functions R~ (s0 ). Let (H ) state-abstracted MAXQ graph defined applying state
abstractions node H five conditions given above. Let x (i; (s))
abstract ordered GLIE exploration policy node state whose decisions
depend \relevant" state variables node i. Let r unique recursivelyoptimal hierarchical policy defined x , , R~ . probability 1, algorithm
MAXQ-Q applied (H ) converges r provided learning rates fft (i) satisfy
Equation (15) one-step rewards bounded.

Proof: Rather repeating entire proof MAXQ-Q, describe

must change state abstraction. last two forms state abstraction refer states
whose values inferred structure MAXQ graph, therefore
need represented all. Since values updated MAXQ-Q,
ignore them. consider first three forms state abstraction turn.
begin considering primitive leaf nodes. Let leaf node let set
state variables Leaf Irrelevant a. Let s1 = (x; y1 ) s2 = (x; y2 ) two states
272

fiMAXQ Hierarchical Reinforcement Learning

differ values . Leaf Irrelevance, probability transitions
P (s01 js1 ; a) P (s02 js2 ; a) need same, expected reward performing
states must same. MAXQ-Q visits abstract state x,
\know" value y, part state abstracted away. Nonetheless,
draws sample according P (s0 jx; y; a), receives reward R(s0 jx; y; a), updates
estimate V (a; x) (line 4 MAXQ-Q). Let Pt (y) probability MAXQ-Q
visiting (x; y) given unabstracted part state x. Line 4 MAXQ-Q
computing stochastic approximation
X

s0 ;N;y

write

X



Pt (y)Pt (s0 ; N jx; y; a)R(s0 jx; y; a):

Pt (y)

X

s0 ;N

Pt (s0 ; N jx; y; a)R(s0 jx; y; a):

According Leaf Irrelevance, inner sum value states
(s) = x. Call value r0 (x). gives
X



Pt (y)r0 (x);

equal r0 (x) distribution Pt (y). Hence, MAXQ-Q converges Leaf
Irrelevance abstractions.
let us turn two forms abstraction apply internal nodes: Max Node
Irrelevance Result Distribution Irrelevance. Consider SMDP defined node
abstracted MAXQ graph time MAXQ-Q. would ordinary SMDP
transition probability function Pt (x0 ; N jx; a) reward function Vt (a; x) + R~ (x0 )
except MAXQ-Q draws samples state transitions, drawn according
distribution Pt (s0 ; N js; a) original state space. prove theorem, must
show drawing (s0 ; N ) according second distribution equivalent drawing
(x0 ; N ) according first distribution.
Max Node Irrelevance, know abstract policies applied node
descendents, transition probability distribution factors

P (s0 ; N js; a) = P (y0 jx; y; a)P (x0 ; N jx; a):
exploration policy abstract policy, Pt (s0 ; N js; a) factors way.
means Yi components state cannot affect Xi components, hence,
sampling Pt (s0 ; N js; a) discarding Yi values gives samples Pt (x0 ; N jx; a).
Therefore, MAXQ-Q converge Max Node Irrelevance abstractions.
Finally, consider Result Distribution Irrelevance. Let j child node i, suppose
Yj set state variables irrelevant result distribution j .
SMDP node wishes draw sample Pt (x0 ; N jx; j ), \know"
current value y, irrelevant part current state. However,
matter, Result Distribution Irrelevance means possible values y,
Pt (x0 ; y0; N jx; y; j ) same. Hence, MAXQ-Q converge Result Distribution
Irrelevance abstractions.
273

fiDietterich

three cases, MAXQ-Q converge locally-optimal ordered policy
node MAXQ graph. Lemma 10, produces locally-optimal ordered
policy unabstracted SMDP node i. Hence, induction, MAXQ-Q converge
unique ordered recursively optimal policy r defined MAXQ-Q H , MDP ,
ordered exploration policy x . Q.E.D.

5.3 Hierarchical Credit Assignment Problem

still situations would like introduce state abstractions
five properties described permit them. Consider following
modification taxi problem. Suppose taxi fuel tank time
taxi moves one square, costs one unit fuel. taxi runs fuel
delivering passenger destination, receives reward ,20, trial
ends. Fortunately, filling station taxi execute Fillup action fill
fuel tank.
solve modified problem using MAXQ hierarchy, introduce another
subtask, Refuel, goal moving taxi filling station filling
tank. MaxRefuel child MaxRoot, invokes Navigate(t) (with bound
location filling station) move taxi filling station.
introduction fuel possibility might run fuel means
must include current amount fuel feature representing every C value
(for internal nodes) V value (for leaf nodes) throughout MAXQ graph.
unfortunate, intuition tells us amount fuel uence
decisions inside Navigate(t) subtask. is, either taxi enough
fuel reach target (in case, chosen navigation actions depend
fuel), else taxi enough fuel, hence, fail reach regardless
navigation actions taken. words, Navigate(t) subtask
need worry amount fuel, even enough fuel,
action Navigate(t) take get fuel. Instead, top-level subtasks
monitoring amount fuel deciding whether go refuel, go pick
passenger, go deliver passenger.
Given intuition, natural try abstracting away \amount remaining
fuel" within Navigate(t) subtask. However, doesn't work, taxi
runs fuel ,20 reward given, QNorth, QSouth, QEast, QWest nodes
cannot \explain" reward received|that is, consistent way
setting C tables predict negative reward occur, C
values ignore amount fuel tank. Stated formally, diculty
Max Node Irrelevance condition satisfied one-step reward function
R(s0js; a) actions depends amount fuel.
call hierarchical credit assignment problem. fundamental issue
MAXQ decomposition information rewards stored leaf nodes
hierarchy. would like separate basic rewards received navigation
(i.e., ,1 action) reward received exhausting fuel (,20). make
reward leaves depend location taxi, Max Node Irrelevance
condition satisfied.
274

fiMAXQ Hierarchical Reinforcement Learning

One way programmer manually decompose reward function

indicate nodes hierarchy \receive" reward. Let R(s0 js; a) =
P
0
0
R(i; js; a) decomposition reward function, R(i; js; a) specifies
part reward must handled Max node i. modified taxi problem,
example, decompose reward leaf nodes receive original
penalties, out-of-fuel rewards must handled MaxRoot. Lines 15 16
MAXQ-Q algorithm easily modified include R(i; s0 js; a).
domains, believe easy designer hierarchy decompose
reward function. straightforward problems studied.
However, interesting problem future research develop algorithm
solve hierarchical credit assignment problem autonomously.

6. Non-Hierarchical Execution MAXQ Hierarchy

point paper, focused exclusively representing learning
hierarchical policies. However, often optimal policy MDP strictly hierarchical. Kaelbling (1993) first introduced idea deriving non-hierarchical policy
value function hierarchical policy. section, exploit MAXQ decomposition
generalize ideas apply recursively levels hierarchy.
describe two methods non-hierarchical execution.
first method based dynamic programming algorithm known policy
iteration. policy iteration algorithm starts initial policy 0 . repeats
following two steps policy converges. policy evaluation step, computes
value function V k current policy k . Then, policy improvement step,
computes new policy, k+1 according rule

k+1(s) := argmax


X

s0

P (s0 js; a)[R(s0 js; a) + V k (s0 )]:

(23)

Howard (1960) proved k optimal policy, k+1 guaranteed
improvement. Note order apply method, need know transition
probability distribution P (s0 js; a) reward function R(s0 js; a).
know P (s0 js; a) R(s0 js; a), use MAXQ representation value
function perform one step policy iteration. start hierarchical policy
represent value function using MAXQ hierarchy (e.g., could learned via
MAXQ-Q). Then, perform one step policy improvement applying Equation (23)
using V (0; s0 ) (computed MAXQ hierarchy) compute V (s0 ).

Corollary 3 Let g (s) = argmaxa Ps0 P (s0js; a)[R(s0 js; a) + V (0; s)], V (0; s)
value function computed MAXQ hierarchy primitive action. Then,
optimal policy, g strictly better least one state .

Proof: direct consequence Howard's policy improvement theorem. Q.E.D.
Unfortunately, can't iterate policy improvement process, new policy,

g unlikely hierarchical policy (i.e., unlikely representable
275

fiDietterich

Table 5: procedure executing one-step greedy policy.
procedure ExecuteHGPolicy(s)
1
repeat
2
Let hV (0; s); ai := EvaluateMaxNode(0; s)
3
4

execute primitive action
Let resulting state
end // ExecuteHGPolicy

terms local policies node MAXQ graph). Nonetheless, one step policy
improvement give significant improvements.
approach non-hierarchical execution ignores internal structure MAXQ
graph. effect, MAXQ hierarchy viewed way represent V |any
representation would give one-step improved policy g .
second approach non-hierarchical execution borrows idea Q learning.
One great beauties Q representation value functions compute
one step policy improvement without knowing P (s0 js; a), simply taking new policy
g (s) := argmaxa Q(s; a). gives us one-step greedy policy
computed using one-step lookahead. MAXQ decomposition, perform
policy improvement steps levels hierarchy.
already defined function need. Table 3 presented function
EvaluateMaxNode, which, given current state s, conducts search along paths
given Max node leaves MAXQ graph finds path
best value (i.e., maximum sum C values along path, plus V value
leaf). equivalent computing best action greedily level
MAXQ graph. addition, EvaluateMaxNode returns primitive action end
best path. action would first primitive action executed
learned hierarchical policy executed starting current state s. second method
non-hierarchical execution MAXQ graph call EvaluateMaxNode
state, execute primitive action returned. pseudo-code shown
Table 5.
call policy computed ExecuteHGPolicy hierarchical greedy policy,
denote hg , superscript * indicates computing greedy
action time step. following theorem shows give better policy
original, hierarchical policy.

Theorem 5 Let G MAXQ graph representing value function hierarchical policy

(i.e., terms C (i; s; j ), computed i; s, j ). Let V hg (0; s) value
computed ExecuteHGPolicy (line 2), let hg resulting policy. Define
V hg value function hg. states s, case
V (s) V hg (0; s) V hg (s):
(24)

Proof: (sketch) left inequality Equation (24) satisfied construction line 6
EvaluateMaxNode. see this, consider original hierarchical policy, ,
276

fiMAXQ Hierarchical Reinforcement Learning

viewed choosing \path" MAXQ graph running root one
leaf nodes, V (0; s) sum C values along chosen path (plus
V value leaf node). contrast, EvaluateMaxNode performs traversal
paths MAXQ graph finds best path, is, path largest
sum C (and leaf V ) values. Hence, V hg (0; s) must least large V (0; s).
establish right inequality, note construction V hg (0; s) value function
policy, call hg , chooses one action greedily level MAXQ graph
(recursively), follows thereafter. consequence fact line
6 EvaluateMaxNode C right-hand side, C represents cost
\completing" subroutine following , following other, greedier, policy.
(In Table 3, C written Ct .) However, execute ExecuteHGPolicy (and
hence, execute hg ), opportunity improve upon hg time step.
Hence, V hg (0; s) underestimate actual value hg . Q.E.D.
Note theorem works one direction. says find state
V hg (0; s) > V (s), greedy policy, hg , strictly better .
However, could optimal policy yet structure MAXQ
graph prevents us considering action (either primitive composite) would
improve . Hence, unlike policy improvement theorem Howard (where primitive
actions always eligible chosen), guarantee suboptimal,
hierarchically greedy policy strict improvement.
contrast, perform one-step policy improvement discussed start
section, Corollary 3 guarantees improve policy. see
general, neither two methods non-hierarchical execution always better
other. Nonetheless, first method operates level individual primitive
actions, able produce large improvements policy. contrast,
hierarchical greedy method obtain large improvements policy changing
actions (i.e., subroutines) chosen near root hierarchy. Hence, general,
hierarchical greedy execution probably better method. (Of course, value functions
methods could computed, one better estimated value could
executed.)
Sutton, et al. (1999) simultaneously developed closely-related method nonhierarchical execution macros. method equivalent ExecuteHGPolicy
special case MAXQ hierarchy one level subtasks. interesting
aspect ExecuteHGPolicy permits greedy improvements levels
tree uence action chosen.
care must taken applying Theorem 5 MAXQ hierarchy whose C values
learned via MAXQ-Q. online algorithm, MAXQ-Q correctly learned values states nodes MAXQ graph. example,
taxi problem, value C (Put; s; QPutdown) learned well except
four special locations R, G, B, Y. Put subtask cannot
executed passenger taxi, usually means Get
completed, taxi passenger's source location. exploration, children Put tried states. PutDown usually fail (and receive negative
reward), whereas Navigate eventually succeed (perhaps lengthy exploration)
277

fiDietterich

take taxi destination location. all-states updating, values
C (Put; s; Navigate(t)) learned states along path
passenger's destination, C values Putdown action learned
passenger's source destination locations. Hence, train MAXQ representation using hierarchical execution (as MAXQ-Q), switch hierarchically-greedy
execution, results quite bad. particular, need introduce hierarchicallygreedy execution early enough exploration policy still actively exploring. (In
theory, GLIE exploration policy never ceases explore, practice, want find
good policy quickly, asymptotically).
course alternative would use hierarchically-greedy execution
beginning learning. However, remember higher nodes MAXQ hierarchy
need obtain samples P (s0 ; N js; a) child action a. hierarchical greedy
execution interrupts child reached terminal state (i.e., state
along way, another subtask appears better EvaluateMaxNode), samples
cannot obtained. Hence, important begin purely hierarchical execution
training, make transition greedy execution point.
approach taken implement MAXQ-Q way
specify number primitive actions L taken hierarchically hierarchical execution \interrupted" control returns top level (where new action
chosen greedily). start L set large, execution completely
hierarchical|when child action invoked, committed execute action
terminates. However, gradually, reduce L becomes 1, point
hierarchical greedy execution. time reaches 1 time
Boltzmann exploration cools temperature 0.1 (which exploration effectively
halted). experimental results show, generally gives excellent results
little added exploration cost.

7. Experimental Evaluation MAXQ Method
performed series experiments MAXQ method three goals
mind: (a) understand expressive power value function decomposition, (b)
characterize behavior MAXQ-Q learning algorithm, (c) assess relative
importance temporal abstraction, state abstraction, non-hierarchical execution.
section, describe experiments present results.

7.1 Fickle Taxi Task
first experiments performed modified version taxi task. version
incorporates two changes task described Section 3.1. First, four
navigation actions noisy, probability 0.8 moves intended direction,
probability 0.1 instead moves right (of intended direction)
probability 0.1 moves left. purpose change create realistic
dicult challenge learning algorithms. second change
taxi picked passenger moved one square away passenger's source
location, passenger changes destination location probability 0.3.
278

fiMAXQ Hierarchical Reinforcement Learning

purpose change create situation optimal policy hierarchical
policy effectiveness non-hierarchical execution measured.
compared four different configurations learning algorithm: (a) Q learning,
(b) MAXQ-Q learning without form state abstraction, (c) MAXQ-Q learning
state abstraction, (d) MAXQ-Q learning state abstraction greedy execution.
configurations controlled many parameters. include following: (a)
initial values Q C functions, (b) learning rate (we employed fixed
learning rate), (c) cooling schedule Boltzmann exploration (the GLIE policy
employed), (d) non-hierarchical execution, schedule decreasing L, number
steps consecutive hierarchical execution. optimized settings separately
configuration goal matching exceeding (with primitive training
actions possible) best policy could code hand. Boltzmann exploration,
established initial temperature cooling rate. separate temperature
maintained Max node MAXQ graph, temperature reduced
multiplying cooling rate time subtask terminates goal state.
process optimizing parameter settings algorithm time-consuming,
Q learning MAXQ-Q. critical parameter schedule
cooling temperature Boltzmann exploration: cooled rapidly,
algorithms converge suboptimal policy. case, tested nine different
cooling rates. choose different cooling rates various subtasks, started
using fixed policies (e.g., either random hand-coded) subtasks except subtasks
closest leaves. Then, chosen schedules subtasks, allowed
parent tasks learn policies tuned cooling rates, on. One
nice effect method cooling temperature subtask terminates
naturally causes subtasks higher MAXQ graph cool slowly. meant
good results could often obtained using cooling rate Max
nodes.
choice learning rate easier, since determined primarily degree
stochasticity environment. tested three four different rates
configuration. initial values Q C functions set based knowledge
problems|no experiments required.
took care tuning parameters experiments one would
normally take real application, wanted ensure method
compared best possible conditions. general form results (particularly
speed learning) wide ranges cooling rate learning rate
parameter settings.
following parameters selected based tuning experiments. Q
learning: initial Q values 0.123 states, learning rate 0.25, Boltzmann exploration
initial temperature 50 cooling rate 0.9879. (We use initial values
end .123 \signature" debugging detect weight modified.)
MAXQ-Q learning without state abstraction, used initial values 0.123, learning rate 0.50, Boltzmann exploration initial temperature 50 cooling
rates 0.9996 MaxRoot MaxPut, 0.9939 MaxGet, 0.9879 MaxNavigate.
279

fiDietterich

200
MAXQ Abstract

Mean Cumulative Reward

0
MAXQ
Abstract+
Greedy

-200

MAXQ
Abstract

-400

Flat Q

-600

-800

-1000
0

20000

40000

60000
80000
100000
Primitive Actions

120000

140000

Figure 8: Comparison performance hierarchical MAXQ-Q learning (without state abstractions, state abstractions, state abstractions combined
hierarchical greedy evaluation) Q learning.
MAXQ-Q learning state abstraction, used initial values 0.123, learning
rate 0.25, Boltzmann exploration initial temperature 50 cooling rates
0.9074 MaxRoot, 0.9526 MaxPut, 0.9526 MaxGet, 0.9879 MaxNavigate.
MAXQ-Q learning non-hierarchical execution, used settings
state abstraction. addition, initialized L 500 decreased 10
trial reached 1. 50 trials, execution completely greedy.
Figure 8 shows averaged results 100 training runs. training run involves
performing repeated trials convergence. different trials execute different
numbers primitive actions, plotted number primitive actions
horizontal axis rather number trials.
first thing note forms MAXQ learning better initial performance
Q learning. constraints introduced MAXQ hierarchy.
example, agent executing Navigate subtask, never attempt pickup
putdown passenger, actions available Navigate. Similarly,
agent never attempt putdown passenger first picked passenger
(and vice versa) termination conditions Get Put subtasks.
second thing notice without state abstractions, MAXQ-Q learning actually takes longer converge, Flat Q curve crosses MAXQ/no abstraction
280

fiMAXQ Hierarchical Reinforcement Learning

curve. shows without state abstraction, cost learning huge number
parameters MAXQ representation really worth benefits. suspect
consequence model-free nature MAXQ-Q algorithm. MAXQ decomposition represents information redundantly. example, cost performing
Put subtask computed C (Root; s; Get) also V (Put; s). model-based
algorithm could compute learned model, MAXQ-Q must learn
separately experience.
third thing notice state abstractions, MAXQ-Q converges
quickly hierarchically optimal policy. seen clearly Figure 9,
focuses range reward values neighborhood optimal policy.
see MAXQ abstractions attains hierarchically optimal policy
approximately 40,000 steps, whereas Q learning requires roughly twice long reach
level. However, Q learning, course, continue onward reach optimal
performance, whereas MAXQ hierarchy, best hierarchical policy slow
respond \fickle" behavior passenger he/she changes destination.
last thing notice greedy execution, MAXQ policy also able
attain optimal performance. execution becomes \more greedy",
temporary drop performance, MAXQ-Q must learn C values new regions
state space visited recursively optimal policy. Despite
drop performance, greedy MAXQ-Q recovers rapidly reaches hierarchically optimal
performance faster purely-hierarchical MAXQ-Q learning. Hence, added
cost|in terms exploration|for introducing greedy execution.
experiment presents evidence favor three claims: first, hierarchical reinforcement learning much faster Q learning; second, state abstraction
required MAXQ-Q learning good performance; third, non-hierarchical
execution produce significant improvements performance little added
exploration cost.

7.2 Kaelbling's HDG Method
second task consider simple maze task introduced Leslie Kaelbling
(1993) shown Figure 11. trial task, agent starts randomlychosen state must move randomly-chosen goal state using usual North, South,
East, West operators (we employed deterministic operators). small cost
move, agent must minimize undiscounted sum costs.
goal state 100 different locations, actually 100
different MDPs. Kaelbling's HDG method starts choosing arbitrary set landmark
states defining Voronoi partition state space based Manhattan distances
landmarks (i.e., two states belong Voronoi cell iff
nearest landmark). method defines one subtask landmark l. subtask
move state current Voronoi cell neighboring Voronoi cell
landmark l. Optimal policies subtasks computed.
HDG policies subtasks, solve abstract Markov Decision
Problem moving landmark state landmark state using subtask
solutions macro actions (subroutines). computes value function MDP.
281

fiDietterich

10

MAXQ Abstract+Greedy

Mean Cumulative Reward

5

Optimal Policy
Flat Q
Hier-Optimal Policy

0
MAXQ Abstract

MAXQ Abstract

-5

-10

-15
0

50000

100000

150000
200000
Primitive Actions

250000

300000

Figure 9: Close-up view previous figure. figure also shows two horizontal lines
indicating optimal performance hierarchically optimal performance
domain. make figure readable, applied 100-step moving
average data points (which average 100 runs).
Finally, possible destination location g within Voronoi cell landmark l,
HDG method computes optimal policy getting l g.
combining subtasks, HDG method construct good approximation
optimal policy follows. addition value functions discussed above,
agent maintains two functions: NL(s), name landmark nearest state s,
N (l), list landmarks cells immediate neighbors cell l.
combining these, agent build list state current landmark
landmarks neighboring cells. landmark, agent computes sum
three terms:
(t1) expected cost reaching landmark,
(t2) expected cost moving landmark landmark goal cell,
(t3) expected cost moving goal-cell landmark goal state.
Note terms (t1) (t3) exact estimates, term (t2) computed using
landmark subtasks subroutines. means corresponding path must pass
intermediate landmark states rather going directly goal landmark.
282

fiMAXQ Hierarchical Reinforcement Learning

10
9
8
7
6
5
4
3
2
1
1

2

3

4

5

6

7

8

9

10

Figure 10: Kaelbling's 10-by-10 navigation task. circled state landmark state,
heavy lines show boundaries Voronoi cells. episode,
start state goal state chosen random. figure, start state
shown black square, goal state shown black hexagon.

Hence, term (t2) typically overestimate required distance. (Also note (t3)
choices intermediate landmarks, need explicitly
included computation best action agent enters cell containing
goal.)
Given information, agent chooses move toward best landmarks
(unless agent already goal Voronoi cell, case agent moves toward
goal state). example, Figure 10, term (t1) cost reaching landmark
row 6, column 6, 4. Term (t2) cost getting row 6, column 6
landmark row 1 column 4 (by going one landmark another). case,
best landmark-to-landmark path go directly row 6 column 6 row 1 column
4. Hence, term (t2) 6. Term (t3) cost getting row 1 column 4 goal,
1. sum 4 + 6 + 1 = 11. comparison, optimal path
length 9.
Kaelbling's experiments, employed variation Q learning learn terms (t1)
(t3), computed (t2) regular intervals via Floyd-Warshall all-sources
shortest paths algorithm.
Figure 11 shows MAXQ approach solving problem. overall task Root,
takes one argument g, specifies goal cell. three subtasks:
283

fiDietterich

MaxRoot(g)
gl/NL(g)

QGotoGoalLmk(gl)

QGotoGoal(g)

MaxGotoGoalLmk(gl)

QGotoLmk(l,gl)

MaxGotoLmk(l)

QNorthLmk(l)

QSouthLmk(l)

MaxGotoGoal(g)

QEastLmk(l)

North

QWestLmk(l)

South

QNorthG(g)

East

QSouthG(g)

QEastG(g)

QWestG(g)

West

Figure 11: MAXQ graph HDG navigation task.

GotoGoalLmk, go landmark nearest goal location. termination
predicate subtask true agent reaches landmark nearest
goal. goal predicate termination predicate.

GotoLmk(l), go landmark l. termination predicate true either (a)

agent reaches landmark l (b) agent outside region defined
Voronoi cell l neighboring Voronoi cells, N (l). goal predicate
subtask true condition (a).

GotoGoal(g), go goal location g. termination predicate subtask

true either agent goal location agent outside Voronoi
cell NL(g) contains g. goal predicate subtask true agent
goal location.
284

fiMAXQ Hierarchical Reinforcement Learning

MAXQ decomposition essentially Kaelbling's method, somewhat
redundant. Consider state agent inside Voronoi cell goal
g. states, HDG decomposes value function three terms (t1), (t2), (t3).
Similarly, MAXQ also decomposes three terms:
V (GotoLmk(l); s; a) cost getting landmark l. represented sum
V (a; s) C (GotoLmk(l); s; a).
C (GotoGoalLmk(gl); s; MaxGotoLmk(l)) cost getting landmark l
landmark gl nearest goal.
C (Root; s; GotoGoalLmk(gl)) cost getting goal location reaching gl
(i.e., cost completing Root task reaching gl).
agent inside goal Voronoi cell, HDG MAXQ store
essentially information. HDG stores Q(GotoGoal(g); s; a), MAXQ breaks
two terms: C (GotoGoal(g); s; a) V (a; s) sums two quantities
compute Q value.
Note MAXQ decomposition stores information twice|specifically,
cost getting goal landmark gl goal stored C (Root; s; GotoGoalLmk(gl))
C (GotoGoal(g); s; a) + V (a; s).
Let us compare amount memory required Q learning, HDG, MAXQ.
100 locations, 4 possible actions, 100 possible goal states, Q learning
must store 40,000 values.
compute quantity (t1), HDG must store 4 Q values (for four actions)
state respect landmark landmarks N (NL(s)). gives
total 2,028 values must stored.
compute quantity (t2), HDG must store, landmark, information
shortest path every landmark. 12 landmarks. Consider landmark
row 6, column 1. 5 neighboring landmarks constitute five macro actions
agent perform move another landmark. nearest landmark
goal cell could 11 landmarks, gives total 55 Q values
must stored. Similar computations 12 landmarks give total 506 values
must stored.
Finally, compute quantity (t3), HDG must store information, square inside
Voronoi cell, get squares inside Voronoi
cell. requires 3,536 values.
Hence, grand total HDG 6,070, huge savings Q learning.
let's consider MAXQ hierarchy without state abstractions.
V (a; s): expected reward primitive action state.
100 states 4 primitive actions, requires 400 values. However,
reward constant (,1), apply Leaf Irrelevance store single value.
C (GotoLmk(l); s; a), one four primitive actions. requires
amount space (t1) Kaelbling's representation|indeed, combined
V (a; s), represents exactly information (t1). requires 2,028 values.
state abstractions applied.
285

fiDietterich

C (GotoGoalLmk(gl); s; GotoLmk(l)): cost completing GotoGoalLmk

task going landmark l. primitive actions deterministic,
GotoLmk(l) always terminate location l, hence, need store
pair l gl. exactly Kaelbling's quantity (t2),
requires 506 values. However, primitive actions stochastic|as
Kaelbling's original paper|then must store value possible
terminal state GotoLmk action. actions could terminate
target landmark l one states bordering set Voronoi cells
neighbors cell l. requires 6,600 values. Kaelbling stores
values (t2), effectively making assumption GotoLmk(l)
never fail reach landmark l. approximation introduce
MAXQ representation choice state abstraction node.

C (GotoGoal; s; a): cost completing GotoGoal task executing one
primitive actions a. quantity (t3) HDG representation,
requires amount space: 3,536 values.

C (Root; s; GotoGoalLmk): cost reaching goal reached

landmark nearest goal. MAXQ must represent combinations
goal landmarks goals. requires 100 values. Note values
values C (GotoGoal(g); s; a) + V (a; s) primitive actions.
means MAXQ representation stores information twice, whereas
HDG representation stores (as term (t3)).

C (Root; s; GotoGoal). cost completing Root task exe-

cuted GotoGoal task. primitive action deterministic, always zero,
GotoGoal reached goal. Hence, apply Termination
condition store values all. However, primitive actions stochastic, must store value possible state borders Voronoi cell
contains goal. requires 96 different values. Again, Kaelbling's HDG
representation value function, ignoring probability GotoGoal
terminate non-goal state. MAXQ exact representation value
function, ignore possibility. (incorrectly) apply Termination
condition case, MAXQ representation becomes function approximation.

stochastic case, without state abstractions, MAXQ representation requires
12,760 values. safe state abstractions, requires 12,361 values. approximations employed Kaelbling (or equivalently, primitive actions deterministic),
MAXQ representation state abstractions requires 6,171 values. numbers
summarized Table 6. see that, unsafe state abstractions, MAXQ
representation requires slightly space HDG representation (because
redundancy storing C (Root; s; GotoGoalLmk).
example shows HDG task, start fully-general formulation provided MAXQ impose assumptions obtain method similar
HDG. MAXQ formulation guarantees value function hierarchical
policy represented exactly. assumptions introduce approximations
286

fiMAXQ Hierarchical Reinforcement Learning

Table 6: Comparison number values must stored represent value
function using HDG MAXQ methods.
HDG MAXQ
HDG MAXQ MAXQ MAXQ
item item
values abs safe abs unsafe abs
V (a; s)
0
400
1
1
(t1) C (GotoLmk(l); s; a)
2,028 2,028
2,028
2,028
(t2) C (GotoGoalLmk; s; GotoLmk(l))
506 6,600
6,600
506
(t3) C (GotoGoal(g); s; a)
3,536 3,536
3,536
3,536
C (Root; s; GotoGoalLmk)
0
100
100
100
C (Root; s; GotoGoal)
0
96
96
0
Total Number Values Required
6,070 12,760 12,361
6,171
value function representation. might useful general design methodology
building application-specific hierarchical representations. long-term goal develop
methods new application require inventing new set techniques. Instead, off-the-shelf tools (e.g., based MAXQ) could specialized imposing
assumptions state abstractions produce ecient special-purpose systems.
One important contributions HDG method introduced
form non-hierarchical execution. soon agent crosses one Voronoi cell
another, current subtask reaching landmark cell \interrupted",
agent recomputes \current target landmark". effect (until
reaches goal Voronoi cell), agent always aiming landmark outside
current Voronoi cell. Hence, although agent \aims for" sequence landmark states,
typically visit many states way goal. states provide
convenient set intermediate targets. taking \shortcuts", HDG compensates
fact that, general, overestimated cost getting goal,
computed value function based policy agent goes one landmark
another.
effect obtained hierarchical greedy execution MAXQ graph (which
directly inspired HDG method). Note storing NL (nearest landmark)
function, Kaelbing's HDG method detect eciently current subtask
interrupted. technique works navigation problems space
distance metric. contrast, ExecuteHGPolicy performs kind \polling",
checks primitive action whether interrupt current subroutine
invoke new one. important goal future research MAXQ find general
purpose mechanism avoiding unnecessary \polling"|that is, mechanism
discover eciently-evaluable interrupt conditions.
Figure 12 shows results experiments HDG using MAXQ-Q learning algorithm. employed following parameters: Flat Q learning, initial values
0.123, learning rate 1.0, initial temperature 50, cooling rate 0.9074;
MAXQ-Q without state abstractions: initial values ,25:123, learning rate 1.0, initial
287

fiDietterich

0
Flat Q

MAXQ +
Abstract

-20

Mean Cumulative Reward

-40

MAXQ Abstract

-60
-80
-100
-120
-140
0

200000

400000

600000
800000
Primitive Actions

1e+06

1.2e+06

1.4e+06

Figure 12: Comparison Flat Q learning MAXQ-Q learning without state
abstraction. (Average 100 runs.)
temperature 50, cooling rates 0.9074 MaxRoot, 0.9999 MaxGotoGoalLmk,
0.9074 MaxGotoGoal, 0.9526 MaxGotoLmk; MAXQ-Q state abstractions:
initial values ,20:123, learning rate 1.0, initial temperature 50, cooling rates
0.9760 MaxRoot, 0.9969 MaxGotoGoal, 0.9984 MaxGotoGoalLmk, 0.9969
MaxGotoLmk. Hierarchical greedy execution introduced starting 3000 primitive actions per trial, reducing every trial 2 actions, 1500 trials,
execution completely greedy.
figure confirms observations made experiments Fickle Taxi task.
Without state abstractions, MAXQ-Q converges much slowly Q learning.
state abstractions, converges roughly three times fast. Figure 13 shows close-up
view Figure 12 allows us compare differences final levels performance
methods. Here, see MAXQ-Q state abstractions able
reach quality hand-coded hierarchical policy|presumably even exploration
would required achieve this, whereas state abstractions, MAXQ-Q able
slightly better hand-coded policy. hierarchical greedy execution, MAXQ-Q
able reach goal using one fewer action, average|so approaches
performance best hierarchical greedy policy (as computed value iteration). Notice
however, best performance obtained hierarchical greedy execution
best recursively-optimal policy cannot match optimal performance. Hence, Flat Q
288

fiMAXQ Hierarchical Reinforcement Learning

-6
Optimal Policy

Mean Cumulative Reward

Hierarchical Greedy Optimal Policy
MAXQ Abstract + Greedy
MAXQ + Abstract

-8

-10

Flat Q

Hierarchical Hand-coded Policy

MAXQ Abstract

-12

-14

0

200000

400000

600000
800000
Primitive Actions

1e+06

1.2e+06

1.4e+06

Figure 13: Expanded view comparing Flat Q learning MAXQ-Q learning
without state abstraction without hierarchical greedy execution.
(Average 100 runs.)
learning achieves policy reaches goal state, average, one fewer
primitive action. Finally notice taxi domain, added exploration
cost shifting greedy execution.
Kaelbling's HDG work recently extended generalized Moore, Baird
Kaelbling (1999) sparse MDP overall task get given
start state desired goal state. key success approach
landmark subtask guaranteed terminate single resulting state. makes
possible identify sequence good intermediate landmark states assemble
policy visits sequence. Moore, Baird Kaelbling show construct
hierarchy landmarks (the \airport" hierarchy) makes planning process ecient.
Note subtask terminate single state (as general MDPs),
airport method would work, would combinatorial explosion
potential intermediate states would need considered.

7.3 Parr Russell: Hierarchies Abstract Machines

(1998b) dissertation work, Ron Parr considered approach hierarchical reinforcement learning programmer encodes prior knowledge form hierarchy
finite-state controllers called HAM (Hierarchy Abstract Machines). hierarchy
289

fiDietterich

Intersection
Vertical Hallway
Horizontal Hallway
Goal

Figure 14: Parr's maze problem (on left). start state upper left corner,
states lower right-hand room terminal states. smaller diagram
right shows hallway intersection structure maze.
executed using procedure-call-and-return discipline, provides partial policy
task. policy partial machine include non-deterministic \choice"
machine states, machine lists several options action specify
one chosen. programmer puts \choice" states point
he/she know action performed. Given partial policy, Parr's
goal find best policy making choices choice states. words,
goal learn hierarchical value function V (hs; mi), state (of external
environment) contains internal state hierarchy (i.e., contents
procedure call stack values current machine states machines
appearing stack). key observation necessary learn value
function choice states hs; mi. Parr's algorithm learn decomposition value
function. Instead, \ attens" hierarchy create new Markov decision problem
choice states hs; mi. Hence, hierarchical primarily sense programmer
structures prior knowledge hierarchically. advantage Parr's method
find optimal hierarchical policy subject constraints provided programmer.
disadvantage method cannot executed \non-hierarchically" produce
better policy.
Parr illustrated work using maze shown Figure 14. maze large-scale
structure (as series hallways intersections), small-scale structure (a series
obstacles must avoided order move hallways intersections).
290

fiMAXQ Hierarchical Reinforcement Learning

trial, agent starts top left corner, must move state
bottom right corner room. agent usual four primitive actions, North, South,
East, West. actions stochastic: probability 0.8, succeed,
probability 0.1 action move \left" probability 0.1 action
move \right" instead (e.g., North action move east probability 0.1
west probability 0.1). action would collide wall obstacle,
effect.
maze structured series \rooms", containing 12-by-12 block states
(and various obstacles). rooms parts \hallways", connected
two rooms opposite sides. rooms \intersections", two
hallways meet.
test representational power MAXQ hierarchy, want see well
represent prior knowledge Parr able represent using HAM. begin
describing Parr's HAM maze task, present MAXQ hierarchy
captures much prior knowledge.3
Parr's top level machine, MRoot, consists loop single choice state
chooses among four possible child machines: MGo(East), MGo(South), MGo(West),
MGo(North). loop terminates agent reaches goal state. MRoot
invoke particular machine hallway specified direction. Hence,
start state, consider MGo(South) MGo(East).
MGo(d) machine begins executing agent intersection. first
thing tries exit intersection hallway specified direction d.
attempts traverse hallway reaches another intersection. first
invoking MExitIntersection(d) machine. machine returns, invokes
MExitHallway(d) machine. machine returns, MGo also returns.
MExitIntersection MExitHallway machines identical except termination conditions. machines consist loop one choice state chooses among
four possible subroutines. simplify description, suppose MGo(East) chosen MExitIntersection(East). four possible subroutines MSniff (East; North),
MSniff (East; South), MBack(East; North), MBack(East; South).
MSniff (d; p) machine always moves direction encounters wall (either
part obstacle part walls maze). moves perpendicular
direction p reaches end wall. wall \end" two ways: either
agent trapped corner walls directions p else
longer wall direction d. first case, MSniff machine terminates; second
case, resumes moving direction d.
MBack(d; p) machine moves one step backwards (in direction opposite d)
moves five steps direction p. moves may may succeed,
actions stochastic may walls blocking way. actions carried
case, MBack machine returns.
MSniff MBack machines also terminate reach end hall
end intersection.
3. author thanks Ron Parr providing details HAM task.

291

fiDietterich

finite-state controllers define highly constrained partial policy. MBack,
MSniff, MGo machines contain choice states all. choice points
MRoot, must choose direction move, MExitIntersection
MExitHall, must decide call MSniff, call MBack,
\perpendicular" direction tell machines try cannot move forward.

MaxRoot

Go(d)
r/ROOM
MaxGo(d,r)

QExitInter(d,r)

QExitHall(d,r)

MaxExitInter(d,r)

MaxExitHall(d,r)

QSniffEI(d,p)

QBackEI(d,p)

QSniffEH(d,p)
x/X

QBackEH(d,p)
x/X
y/Y

y/Y
MaxSniff(d,p)

MaxBack(d,p,x,y)

QFollowWall(d,p)

QToWall(d)

QBackOne(d)

QPerpThree(p)

MaxFollowWall(d,p)

MaxToWall(d)

MaxBackOne(d)

MaxPerpThree(p)

d/p

d/d

QMoveFW(d)

d/Inv(d)

QMoveTW(d)

QMoveBO(d)

d/p
QMoveP3(d)

MaxMove(d)

Figure 15: MAXQ graph Parr's maze task.
Figure 15 shows MAXQ graph encodes similar set constraints policy.
subtasks defined follows:
292

fiMAXQ Hierarchical Reinforcement Learning

Root. exactly MRoot machine. must choose direction

invoke Go. terminates agent enters terminal state. also
goal condition (of course).

Go(d; r). (Go direction leaving room r.) parameter r bound identi-

fication number corresponding current 12-by-12 \room" agent
located. Go terminates agent enters room end hallway
direction leaves desired hallway (e.g., wrong direction).
goal condition Go satisfied agent reaches desired intersection.

ExitInter(d; r). terminates agent exited room r. goal condition
agent exit room r direction d.

ExitHall(d; r). terminates agent exited current hall (into

intersection). goal condition agent entered desired intersection
direction d.

Sniff (d; r). encodes subtask equivalent MSniff machine. However,

Sniff must two child subtasks, ToWall FollowWall, simply internal
states MSniff. necessary, subtask MAXQ framework cannot

contain internal state, whereas finite-state controller HAM representation
contain many internal states necessary. particular, one state
moving forward another state following wall sideways.

ToWall(d). equivalent one part MSniff. terminates

wall \front" agent direction d. goal condition
termination condition.

FollowWall(d; p). equivalent part MSniff. moves direction

p wall direction ends (or stuck corner walls
directions p). goal condition termination condition.

Back(d; p; x; y). attempts encode information MBack machine,

case MAXQ hierarchy cannot capture information.

MBack simply executes sequence 6 primitive actions (one step back, five steps
direction p). this, MBack must 6 internal states, MAXQ
allow. Instead, Back subtask subgoal moving agent least

one square backwards least 3 squares direction p. order determine
whether achieved subgoal, must remember x position
started execute, bound parameters Back. Back terminates
achieves desired change position runs walls prevent
achieving subgoal. goal condition termination condition.

BackOne(d; x; y). moves agent one step backwards (in direction opposite

d. needs starting x position order tell succeeded.
terminates moved least one unit direction wall
direction. goal condition termination condition.
293

fiDietterich

PerpThree(p; x; y). moves agent three steps direction p. needs

starting x positions order tell succeeded. terminates
moved least three units direction p wall direction.
goal condition termination condition.

Move(d). \parameterized primitive" action. executes one primitive move
direction terminates immediately.

this, see three major differences MAXQ representation HAM representation. First, HAM finite-state controller contain
internal states. convert MAXQ subtask graph, must make separate
subtask internal state HAM. Second, HAM terminate based
\amount effort" (e.g., performing 5 actions), whereas MAXQ subtask must terminate
based change state world. impossible define MAXQ subtask performs k steps terminate regardless effects steps (i.e.,
without adding kind \counter" state MDP). Third, dicult
formulate termination conditions MAXQ subtasks HAM machines.
example, HAM, necessary specify MExitHallway machine terminates entered different intersection one MGo executed.
However, important MAXQ method, MAXQ, subtask learns
value function policy|independent parent tasks. example, without
requirement enter different intersection, learning algorithms MAXQ
always prefer MaxExitHall take one step backward return room
Go action started (because much easier terminal state reach).
problem arise HAM approach, policy learned subtask
depends whole \ attened" hierarchy machines, returning state
Go action started help solve overall problem reaching goal state
lower right corner.
construct MAXQ graph problem, introduced three programming
tricks: (a) binding parameters aspects current state (in order serve kind
\local memory" subtask began executing), (b) parameterized
primitive action (in order able pass parameter value specifies primitive
action perform), (c) employing \inheritance termination conditions"|that is,
subtask MAXQ graph (but others paper) inherits termination
conditions ancestor tasks. Hence, agent middle executing ToWall
action leaves intersection, ToWall subroutine terminates ExitInter
termination condition satisfied. behavior similar standard behavior
MAXQ. Ordinarily, ancestor task terminates, descendent tasks forced
return without updating C values. inheritance termination conditions,
hand, descendent tasks forced terminate, updating C
values. words, termination condition child task logical disjuntion
termination conditions ancestors (plus termination condition).
inheritance made easier write MAXQ graph, parents need
pass children information necessary children define
complete termination goal predicates.
294

fiMAXQ Hierarchical Reinforcement Learning

essentially opportunities state abstraction task,
irrelevant features state. opportunities apply Shielding
Termination properties, however. particular, ExitHall(d) guaranteed cause
parent task, MaxGo(d), terminate, require stored C values.
many states subtasks terminated (e.g., Go(East) state
wall east side room), C values need stored.
Nonetheless, even applying state elimination conditions, MAXQ representation task requires much space representation. exact
computation dicult, applying MAXQ-Q learning, MAXQ representation
required 52,043 values, whereas Q learning requires fewer 16,704 values. Parr
states method requires 4,300 values.
test relative effectiveness MAXQ representation, compare MAXQ-Q
learning Q learning. large negative values states
acquire (particularly early phases learning), unable get Boltzmann
exploration work well|one bad experience would cause action receive
low Q value, would never tried again. Hence, experimented
-greedy exploration counter-based exploration. -greedy exploration policy
ordered, abstract GLIE policy random action chosen probability ,
gradually decreased time. counter-based exploration policy keeps track
many times action executed state s. choose action state
s, selects action executed fewest times actions
executed times. switches greedy execution. Hence, genuine GLIE
policy. Parr employed counter-based exploration policies experiments task.
domains, conducted several experimental runs (e.g., testing Boltzmann, -greedy, counter-based exploration) determine best parameters
algorithm. Flat Q learning, chose following parameters: learning rate 0.50, greedy exploration initial value 1.0, decreased 0.001 successful
execution Max node, initial Q values ,200:123. MAXQ-Q learning, chose
following parameters: counter-based exploration = 10, learning rate equal
reciprocal number times action performed, initial values C
values selected carefully provide underestimates true C values. example,
initial values QExitInter ,40:123, worst case, completing
ExitInter task, takes 40 steps complete subsequent ExitHall task hence,
complete Go parent task. Performance quite sensitive initial C values,
potential drawback MAXQ approach.
Figure 16 plots results. see MAXQ-Q learning converges 10
times faster Flat Q learning. know whether MAXQ-Q converged
recursively optimal policy. comparison, also show performance hierarchical
policy coded hand, hand-coded policy, used knowledge contextual
information choose operators, policy surely better best recursively
optimal policy. HAMQ learning converge policy equal slightly better
hand-coded policy.
experiment demonstrates MAXQ representation capture most|but
all|of prior knowledge represented HAMQ hierarchy. also
295

fiDietterich

-100
-150

Mean Reward Per Trial

-200

Hand-coded hierarchical policy

-250
-300
-350

MAXQ Q Learning

Flat Q Learning

-400
-450
-500
0

1e+06

2e+06

3e+06
Primitive Steps

4e+06

5e+06

6e+06

Figure 16: Comparison Flat Q learning MAXQ-Q learning Parr maze task.
shows MAXQ representation requires much care design goal
conditions subtasks.

7.4 Domains
addition three domains discussed above, developed MAXQ graphs
Singh's (1992) \ ag task", treasure hunter task described Tadepalli Dietterich
(1997), Dayan Hinton's (1993) Feudal-Q learning task. tasks
easily naturally placed MAXQ framework|indeed, fit easily
Parr Russell maze task.
MAXQ able exactly duplicate Singh's work decomposition value
function|while using exactly amount space represent value function.
MAXQ also duplicate results Tadepalli Dietterich|however,
MAXQ explanation-based method, considerably slower requires substantially space represent value function.
Feudal-Q task, MAXQ able give better performance Feudal-Q learning.
reason Feudal-Q learning, subroutine makes decisions using Q
function learned level hierarchy|that is, without information
estimated costs actions descendents. contrast, MAXQ value function
decomposition permits Max node make decisions based sum completion
function, C (i; s; j ), costs estimated descendents, V (j; s). course, MAXQ
296

fiMAXQ Hierarchical Reinforcement Learning

also supports non-hierarchical execution, possible Feudal-Q,
learn value function decomposition.

8. Discussion

concluding paper, wish discuss two issues: (a) design tradeoffs hierarchical reinforcement learning (b) methods automatically learning (or least
improving) MAXQ hierarchies.

8.1 Design Tradeoffs Hierarchical Reinforcement Learning

introduction paper, discussed four issues concerning design hierarchical reinforcement learning architectures: (a) method defining subtasks, (b)
use state abstraction, (c) non-hierarchical execution, (d) design learning algorithms. subsection, want highlight tradeoff first two
issues.
MAXQ defines subtasks using termination predicate Ti pseudo-reward function
R~ . least two drawbacks method. First, hard programmer define Ti R~ correctly, since essentially requires guessing value function
optimal policy MDP states subtask terminates. Second,
leads us seek recursively optimal policy rather hierarchically optimal policy.
Recursively optimal policies may much worse hierarchically optimal ones,
may giving substantial performance.
However, return two drawbacks, MAXQ obtains important benefit:
policies value functions subtasks become context-free. words,
depend parent tasks larger context invoked.
understand point, consider MDP shown Figure 6. clear
optimal policy exiting left-hand room (the Exit subtask) depends location
goal. top right-hand room, agent prefer
exit via upper door, whereas bottom right-hand room, agent
prefer exit lower door. However, define subtask exiting
left-hand room using pseudo-reward zero doors, obtain policy
optimal either case, policy re-use cases. Furthermore,
policy depend location goal. Hence, apply Max node
irrelevance solve Exit subtask using location robot ignore
location goal.
example shows obtain benefits subtask reuse state abstraction define subtask using termination predicate pseudo-reward
function. termination predicate pseudo-reward function provide barrier
prevents \communication" value information Exit subtask context.
Compare Parr's HAM method. HAMQ algorithm finds best policy
consistent hierarchy. achieve this, must permit information propagate
\into" Exit subtask (i.e., Exit finite-state controller) environment.
means state reached leaving Exit subtask different
values depending location goal, different values propagate
back Exit subtask. represent different values, Exit subtask must know
297

fiDietterich

location goal. short, achieve hierarchically optimal policy within Exit
subtask, must (in general) represent value function using entire state space. State
abstractions cannot employed without losing hierarchical optimality.
see, therefore, direct tradeoff achieving hierarchical
optimality employing state abstractions. Methods hierarchical optimality
freedom defining subtasks (e.g., using partial policies, HAM approach).
cannot (safely) employ state abstractions within subtasks, general, cannot
reuse solution one subtask multiple contexts. Methods recursive optimality,
hand, must define subtasks using method (such pseudo-reward functions
MAXQ fixed policies options framework) isolates subtask
context. return, apply state abstraction learned policy
reused many contexts (where less optimal).
interesting iterative method described Dean Lin (1995)
viewed method moving along tradeoff. Dean Lin method,
programmer makes initial guess values terminal states subtask
(i.e., doorways Figure 6). Based initial guess, locally optimal policies
subtasks computed. locally optimal policy parent task
computed|while holding subtask policies fixed (i.e., treating options).
point, algorithm computed recursively optimal solution original
problem, given initial guesses. Instead solving various subproblems sequentially
via oine algorithm Dean Lin suggested, could use MAXQ-Q learning
algorithm.
method Dean Lin stop here. Instead, computes new values
terminal states subtask based learned value function entire
problem. allows update \guesses" values terminal states.
entire solution process repeated obtain new recursively optimal solution,
based new guesses. prove process iterated indefinitely,
converge hierarchically optimal policy (provided, course, state abstractions
used within subtasks).
suggests extension MAXQ-Q learning adapts R~ values online.
time subtask terminates, could update R~ function based computed value
terminated state. precise, j subtask i, j terminates
state s0 , update R~ j (s0 ) equal V~ (i; s0 ) = maxa0 Q~ (i; s0 ; a0 ). However,
work R~ j (s0 ) represented using full state s0. subtask j employing state
abstractions, x = (s), R~ j (x0 ) need average value V~ (i; s0 ),
average taken states s0 x0 = (s0 ) (weighted probability
visiting states). easily accomplished performing stochastic approximation
update form
R~j (x0 ) = (1 , fft )R~ j (x0 ) + fftV~ (i; s0 )
time subtask j terminates. algorithm could expected converge
best hierarchical policy consistent given state abstractions.
also suggests problems, may worthwhile first learn recursively
optimal policy using aggressive state abstractions use learned value
function initialize MAXQ representation detailed representation
states. progressive refinements state space could guided monitoring
298

fiMAXQ Hierarchical Reinforcement Learning

degree values V~ (i; x0 ) vary abstract state x0 . large
variance, means state abstractions failing make important distinctions
values states, refined.
kinds adaptive algorithms take longer converge basic
MAXQ method described paper. tasks agent must solve many times
lifetime, worthwhile learning algorithms provide initial useful
solution gradually improve solution optimal. important goal
future research find methods diagnosing repairing errors (or sub-optimalities)
initial hierarchy ultimately optimal policy discovered.

8.2 Automated Discovery Abstractions

approach taken paper rely upon programmer design
MAXQ hierarchy including termination conditions, pseudo-reward functions, state
abstractions. results paper, particularly concerning state abstraction, suggest
ways might able automate construction hierarchy.
main purpose hierarchy create opportunities subtask sharing
state abstraction. actually closely related. order subtask shared
two different regions state space, must case value function
two different regions identical except additive offset. MAXQ framework,
additive offset would difference C values parent task. one way
find reusable subtasks would look regions state space value function
exhibits additive offsets.
second way would search structure one-step probability transition
function P (s0 js; a). subtask useful enables state abstractions Max
Node Irrelevance. formulate problem identifying region
state space that, conditioned region, P (s0 js; a) factors according
Equation 17. top-down divide-and-conquer algorithm similar decision-tree algorithms
might able this.
third way would search funnel actions looking bottlenecks state
space policies must travel. would useful discovering cases
Result Distribution Irrelevance.
ways, dicult kinds state abstractions discover
arbitrary subgoals introduced constrain policy (and sacrifice optimality).
example, could algorithm automatically decide impose landmarks onto
HDG task? Perhaps detecting large region state space without bottlenecks
variations reward function?
problem discovering hierarchies important challenge future,
least paper provided guidelines constitute good state abstractions,
serve objective functions guiding automated search abstractions.

9. Concluding Remarks

paper introduced new representation value function hierarchical reinforcement learning|the MAXQ value function decomposition. proved
MAXQ decomposition represent value function hierarchical policy
299

fiDietterich

finite-horizon undiscounted, cumulative reward criterion infinite-horizon
discounted reward criterion. representation supports subtask sharing re-use, overall value function decomposed value functions individual subtasks.
paper introduced learning algorithm, MAXQ-Q learning, proved
converges probability 1 recursively optimal policy. paper argued although
recursive optimality weaker either hierarchical optimality global optimality,
important form optimality permits subtask learn locally optimal
policy ignoring behavior ancestors MAXQ graph. increases
opportunities subtask sharing state abstraction.
shown MAXQ decomposition creates opportunities state abstraction, identified set five properties (Max Node Irrelevance, Leaf Irrelevance,
Result Distribution Irrelevance, Shielding, Termination) allow us ignore large
parts state space within subtasks. proved MAXQ-Q still converges
presence forms state abstraction, showed experimentally state abstraction important practice successful application MAXQ-Q learning|at
least Taxi HDG tasks.
paper presented two different methods deriving improved non-hierarchical policies MAXQ value function representation, formalized conditions
methods improve hierarchical policy. paper verified
experimentally non-hierarchical execution gives improved performance Fickle
Taxi Task (where achieves optimal performance) HDG task (where gives
substantial improvement).
Finally, paper argued tradeoff governing design hierarchical
reinforcement learning methods. one end design spectrum \context free"
methods MAXQ-Q learning. provide good support state abstraction
subtask sharing learn recursively optimal policies. end
spectrum \context-sensitive" methods HAMQ, options framework,
early work Dean Lin. methods discover hierarchically optimal
policies (or, cases, globally optimal policies), drawback cannot
easily exploit state abstractions share subtasks. great speedups
enabled state abstraction, paper argued context-free approach
preferred|and relaxed needed obtain improved policies.

Acknowledgements
author gratefully acknowledges support National Science Foundation
grant number IRI-9626584, Oce Naval Research grant number N00014-95-10557, Air Force Oce Scientific Research grant number F49620-98-1-0375,
Spanish government program Estancias de Investigadores Extranjeros en
Regimen de A~no Sabatico en Espa~na. addition, author indebted many colleagues
helping develop clarify ideas paper including Valentina Zubek, Leslie
Kaelbling, Bill Langford, Wes Pinchot, Rich Sutton, Prasad Tadepalli, Sebastian Thrun.
particularly want thank Eric Chown encouraging study Feudal reinforcement
learning, Ron Parr providing details HAM machines, Sebastian Thrun
encouraging write single comprehensive paper. also thank Andrew Moore
300

fiMAXQ Hierarchical Reinforcement Learning

(the action editor), Valentina Zubek, two sets anonymous reviewers previous
drafts paper suggestions careful reading, improved paper
immeasurably.

References

Bellman, R. E. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure policy construction. Proceedings Fourteenth International Joint Conference Artificial
Intelligence, pp. 1104{1111.
Currie, K., & Tate, A. (1991). O-plan: open planning architecture. Artificial Intelligence, 52 (1), 49{86.
Dayan, P., & Hinton, G. (1993). Feudal reinforcement learning. Advances Neural
Information Processing Systems, 5, pp. 271{278. Morgan Kaufmann, San Francisco,
CA.
Dean, T., & Lin, S.-H. (1995). Decomposition techniques planning stochastic domains.
Tech. rep. CS-95-10, Department Computer Science, Brown University, Providence,
Rhode Island.
Dietterich, T. G. (1998). MAXQ method hierarchical reinforcement learning.
Fifteenth International Conference Machine Learning, pp. 118{126. Morgan Kaufmann.
Fikes, R. E., Hart, P. E., & Nilsson, N. J. (1972). Learning executing generalized robot
plans. Artificial Intelligence, 3, 251{288.
Forgy, C. L. (1982). Rete: fast algorithm many pattern/many object pattern
match problem. Artificial Intelligence, 19 (1), 17{37.
Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., & Boutilier, C. (1998). Hierarchical
solution Markov decision processes using macro-actions. Proceedings
Fourteenth Annual Conference Uncertainty Artificial Intelligence (UAI{98), pp.
220{229 San Francisco, CA. Morgan Kaufmann Publishers.
Howard, R. A. (1960). Dynamic Programming Markov Processes. MIT Press, Cambridge, MA.
Jaakkola, T., Jordan, M. I., & Singh, S. P. (1994). convergence stochastic iterative
dynamic programming algorithms. Neural Computation, 6 (6), 1185{1201.
Kaelbling, L. P. (1993). Hierarchical reinforcement learning: Preliminary results. Proceedings Tenth International Conference Machine Learning, pp. 167{173 San
Francisco, CA. Morgan Kaufmann.
301

fiDietterich

Kalmar, Z., Szepesvari, C., & Lorincz, A. (1998). Module based reinforcement learning
real robot. Machine Learning, 31, 55{85.
Knoblock, C. A. (1990). Learning abstraction hierarchies problem solving. Proceedings
Eighth National Conference Artificial Intelligence, pp. 923{928 Boston, MA.
AAAI Press.
Korf, R. E. (1985). Macro-operators: weak method learning. Artificial Intelligence,
26 (1), 35{77.
Lin, L.-J. (1993). Reinforcement learning robots using neural networks. Ph.D. thesis,
Carnegie Mellon University, Department Computer Science, Pittsburgh, PA.
Moore, A. W., Baird, L., & Kaelbling, L. P. (1999). Multi-value-functions: Ecient automatic action hierarchies multiple goal MDPs. Proceedings International Joint Conference Artificial Intelligence, pp. 1316{1323 San Francisco. Morgan Kaufmann.
Parr, R. (1998a). Flexible decomposition algorithms weakly coupled Markov decision
problems. Proceedings Fourteenth Annual Conference Uncertainty
Artificial Intelligence (UAI{98), pp. 422{430 San Francisco, CA. Morgan Kaufmann
Publishers.
Parr, R. (1998b). Hierarchical control learning Markov decision processes. Ph.D.
thesis, University California, Berkeley, California.
Parr, R., & Russell, S. (1998). Reinforcement learning hierarchies machines. Advances Neural Information Processing Systems, Vol. 10, pp. 1043{1049 Cambridge,
MA. MIT Press.
Pearl, J. (1988). Probabilistic Inference Intelligent Systems. Networks Plausible Inference. Morgan Kaufmann, San Mateo, CA.
Rummery, G. A., & Niranjan, M. (1994). Online Q-learning using connectionist systems.
Tech. rep. CUED/FINFENG/TR 166, Cambridge University Engineering Department, Cambridge, England.
Sacerdoti, E. D. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence,
5 (2), 115{135.
Singh, S., Jaakkola, T., Littman, M. L., & Szepesvari, C. (1998). Convergence results
single-step on-policy reinforcement-learning algorithms. Tech. rep., University
Colorado, Department Computer Science, Boulder, CO. appear Machine
Learning.
Singh, S. P. (1992). Transfer learning composing solutions elemental sequential
tasks. Machine Learning, 8, 323{339.
Sutton, R. S., Singh, S., Precup, D., & Ravindran, B. (1999). Improved switching among
temporally abstract actions. Advances Neural Information Processing Systems,
Vol. 11, pp. 1066{1072. MIT Press.
302

fiMAXQ Hierarchical Reinforcement Learning

Sutton, R., & Barto, A. G. (1998). Introduction Reinforcement Learning. MIT Press,
Cambridge, MA.
Sutton, R. S., Precup, D., & Singh, S. (1998). MDPs Semi-MDPs: Learning,
planning, representing knowledge multiple temporal scales. Tech. rep., University Massachusetts, Department Computer Information Sciences, Amherst,
MA. appear Artificial Intelligence.
Tadepalli, P., & Dietterich, T. G. (1997). Hierarchical explanation-based reinforcement
learning. Proceedings Fourteenth International Conference Machine
Learning, pp. 358{366 San Francisco, CA. Morgan Kaufmann.
Tambe, M., & Rosenbloom, P. S. (1994). Investigating production system representations
non-combinatorial match. Artificial Intelligence, 68 (1), 155{199.
Watkins, C. J. C. H. (1989). Learning Delayed Rewards. Ph.D. thesis, King's College,
Oxford. (To reprinted MIT Press.).
Watkins, C. J., & Dayan, P. (1992). Technical note Q-Learning. Machine Learning, 8, 279.

303

fiJournal Artificial Intelligence Research 13 (2000) 155-188

Submitted 6/00; published 10/00

AIS-BN: Adaptive Importance Sampling Algorithm
Evidential Reasoning Large Bayesian Networks
Jian Cheng
Marek J. Druzdzel

jcheng@sis.pitt.edu
marek@sis.pitt.edu

Decision Systems Laboratory
School Information Sciences Intelligent Systems Program
University Pittsburgh, Pittsburgh, PA 15260 USA

Abstract
Stochastic sampling algorithms, attractive alternative exact algorithms
large Bayesian network models, observed perform poorly evidential
reasoning extremely unlikely evidence. address problem, propose adaptive importance sampling algorithm, AIS-BN, shows promising convergence rates
even extreme conditions seems outperform existing sampling algorithms
consistently. Three sources performance improvement (1) two heuristics
initialization importance function based theoretical properties importance sampling finite-dimensional integrals structural advantages Bayesian
networks, (2) smooth learning method importance function, (3) dynamic
weighting function combining samples different stages algorithm.
tested performance AIS-BN algorithm along two state art
general purpose sampling algorithms, likelihood weighting (Fung & Chang, 1989; Shachter
& Peot, 1989) self-importance sampling (Shachter & Peot, 1989). used
tests three large real Bayesian network models available scientific community:
CPCS network (Pradhan et al., 1994), PathFinder network (Heckerman, Horvitz,
& Nathwani, 1990), ANDES network (Conati, Gertner, VanLehn, & Druzdzel,
1997), evidence unlikely 1041 . AIS-BN algorithm always performed
better two algorithms, majority test cases achieved orders
magnitude improvement precision results. Improvement speed given desired
precision even dramatic, although unable report numerical results here,
algorithms almost never achieved precision reached even first
iterations AIS-BN algorithm.

1. Introduction
Bayesian networks (Pearl, 1988) increasingly popular tools modeling uncertainty
intelligent systems. practical models reaching size several hundreds variables
(e.g., Pradhan et al., 1994; Conati et al., 1997), becomes increasingly important address problem feasibility probabilistic inference. Even though several ingenious
exact algorithms proposed, large models stumble theoretically demonstrated NP-hardness inference (Cooper, 1990). significance
result observed practice exact algorithms applied large, densely connected
practical networks require either prohibitive amount memory prohibitive amount
computation unable complete. approximating inference desired
precision shown NP-hard well (Dagum & Luby, 1993), comc
2000
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiCheng & Druzdzel

plex networks alternative produce result all. Furthermore,
obtaining result crucial applications, precision guarantees may critical
types problems traded speed computation.
prominent subclass approximate algorithms family stochastic sampling
algorithms (also called stochastic simulation Monte Carlo algorithms). precision
obtained stochastic sampling generally increases number samples generated
fairly unaffected network size. Execution time fairly independent
topology network linear number samples. Computation
interrupted time, yielding anytime property algorithms, important timecritical applications.
stochastic sampling performs well predictive inference, diagnostic reasoning, i.e., reasoning observed evidence nodes ancestors network often
exhibits poor convergence. number observations increases, especially
observations unlikely a-priori, stochastic sampling often fails converge reasonable estimates posterior probabilities. Although problem known since
first sampling algorithm proposed Henrion (1988), little done
address effectively. Furthermore, various sampling algorithms proposed tested
simple small networks, networks special topology, without presence
extremely unlikely evidence practical significance problem underestimated. Given typical number samples used real-time feasible
todays hardware, say 106 samples, behavior stochastic sampling algorithm
drastically different different size networks. network consisting 10 nodes
observations, may possible converge exact probabilities, large
networks negligibly small fraction total sample space probed. One
practical Bayesian network models used tests, subset CPCS
network (Pradhan et al., 1994), consists 179 nodes. total sample space larger
1061 . 106 samples, sample 1055 fraction sample space.
believe crucial (1) study feasibility convergence properties
sampling algorithms large practical networks, (2) develop sampling algorithms show good convergence extreme, yet practical conditions,
evidential reasoning given extremely unlikely evidence. all, small networks
updated using existing exact algorithms precisely large networks
stochastic sampling useful. likelihood evidence, know
stochastic sampling generally perform well high (Henrion, 1988). So,
important look cases evidence unlikely. paper, test
two existing state art stochastic sampling algorithms Bayesian networks, likelihood weighting (Fung & Chang, 1989; Shachter & Peot, 1989) self-importance sampling
(Shachter & Peot, 1989), subset CPCS network extremely unlikely evidence. show exhibit similarly poor convergence rates. propose new
sampling algorithm, call adaptive importance sampling Bayesian networks
(AIS-BN), suitable evidential reasoning large multiply-connected Bayesian
networks. AIS-BN algorithm based importance sampling, widely
applied method variance reduction simulation also applied Bayesian networks (e.g., Shachter & Peot, 1989). demonstrate empirically three large
practical Bayesian network models AIS-BN algorithm consistently outperforms
156

fiAdaptive Importance Sampling Bayesian Networks

two algorithms. majority test cases, achieved two orders
magnitude improvement convergence. Improvement speed given desired precision
even dramatic, although unable report numerical results here,
algorithms never achieved precision reached even first iterations
AIS-BN algorithm. main sources improvement are: (1) two heuristics
initialization importance function based theoretical properties importance sampling finite-dimensional integrals structural advantages Bayesian
networks, (2) smooth learning method updating importance function, (3)
dynamic weighting function combining samples different stages algorithm.
study value two heuristics used AIS-BN algorithm: (1) initialization
probability distributions parents evidence nodes uniform distribution
(2) adjusting small probabilities conditional probability tables, show
play important role AIS-BN algorithm moderate role
existing algorithms.
remainder paper structured follows. Section 2 first gives general
introduction importance sampling domain finite-dimensional integrals,
originally proposed. show importance sampling used compute probabilities Bayesian networks draw additional benefits graphical
structure network. develop generalized sampling scheme aid us
reviewing previously proposed sampling algorithms describing AIS-BN
algorithm. Section 3 describes AIS-BN algorithm. propose two heuristics initialization importance function discuss theoretical foundations. describe
smooth learning method importance function dynamic weighting function
combining samples different stages algorithm. Section 4 describes empirical evaluation AIS-BN algorithm. Finally, Section 5 suggests several possible
improvements AIS-BN algorithm, possible applications learning scheme,
directions future work.

2. Importance Sampling Algorithms Bayesian Networks
feel useful go back theoretical roots importance sampling order
able understand source speedup AIS-BN algorithm relative
existing state art importance sampling algorithms Bayesian networks. first
review general idea importance sampling finite-dimensional integrals
reduce sampling variance. discuss application importance sampling
Bayesian networks. Readers interested details directed literature
Monte Carlo methods computation finite integrals, excellent exposition
Rubinstein (1981) essentially following first section.
2.1 Mathematical Foundations
Let g(X) function variables X = (X1 , ..., Xm ) domain Rm ,
computing g(X) X feasible. Consider problem approximate computation
integral
Z
I=

g(X) dX .


157

(1)

fiCheng & Druzdzel

Importance sampling approaches problem writing integral (1)
Z

I=


g(X)
f (X) dX ,
f (X)

f (X), often referred importance function, probability density function
. f (X) used importance sampling exists algorithm generating
samples f (X) importance function zero original function
zero, i.e., g(X) 6= 0 = f (X) 6= 0.
independently sampled n points s1 , s2 , . . . , sn , si , according
probability density function f (X), estimate integral
n
1X
g(si )
=
n i=1 f (si )

(2)

estimate variance
b (In ) =

2

n
X
1
g(si )

n (n 1) i=1 f (si )



2

.

(3)

straightforward show estimator following properties:
1. E(In ) =
2. limn =

n
3. n (In I) Normal(0, f2(X) ),
f2(X)


Z

=


g(X)

f (X)

2

f (X) dX

(4)



b 2 (In ) = 2 (In ) = f2 (X) /n
4. E

variance proportional f2(X) inversely proportional number
samples. minimize variance , either increase number samples
try decrease f2(X) . respect latter, Rubinstein (1981) reports following
useful theorem corollary.
Theorem 1 minimum f2(X) equal
f2(X)

2

Z

|g(X)| dX

=

I2



occurs X distributed according following probability density function
f (X) = R

|g(X)|
.
|g(X)| dX

158

fiAdaptive Importance Sampling Bayesian Networks

Corollary 1 g(X) >0, optimal probability density function
f (X) =

g(X)


f2(X) = 0.
Although practice sampling precisely f (X) = g(X)/I occur rarely, expect
functions close enough still reduce variance effectively. Usually,
closer shape function f (X) shape function g(X), smaller
f2(X) . high-dimensional integrals, selection importance function, f (X), far
critical increasing number samples, since former dramatically
affect f2(X) . seems prudent put energy choosing importance function
whose shape close possible g(X) apply brute force method
increasing number samples.
worth noting f (X) uniform, importance sampling becomes general
Monte Carlo sampling. Another noteworthy property importance sampling
derived Equation 4 avoid f (X) |g(X) f (X)| part
domain sampling, even f (X) matches well g(X)/I important regions.
f (X) |g(X) f (X)|, variance become large even infinite.
avoid adjusting f (X) larger unimportant regions domain X.
section discussed importance sampling continuous variables,
results stated valid discrete variables well, case integration
substituted summation.
2.2 Generic Importance Sampling Algorithm Bayesian Networks
following discussion, random variables used multiple-valued, discrete variables.
Capital letters, A, B, C, denote random variables. Bold capital letters,
A, B, C, denote sets variables. Bold capital letter E usually used denote
set evidence variables. Lower case letters a, b, c denote particular instantiations
variables A, B, C respectively. Bold lower case letters, a, b, c, denote
particular instantiations sets A, B, C respectively. Bold lower case letter e,
particular, used denote observations, i.e., instantiations set evidence
variables E. Anc(A) denotes set ancestors node A. Pa(A) denotes set
parents (direct ancestors) node A. pa(A) denotes particular instantiation Pa(A). \
denotes set difference. Pa(A)|E=e denotes use extended vertical bar indicate
substitution e E A.
know joint probability distribution variables Bayesian network model, Pr(X), product probability distributions nodes
conditional parents, i.e.,
Pr(X) =

n


Pr(Xi |Pa(Xi )) .

(5)

i=1

order calculate Pr(E = e), need sum Pr(X\E, E = e).
Pr(E = e) =

X

Pr(X\E, E = e)

X\E

159

(6)

fiCheng & Druzdzel

see Equation 6 almost identical Equation 1 except integration
replaced summation domain replaced X\E. theoretical results
derived importance sampling reviewed previous section thus
directly applied computing probabilities Bayesian networks.
previous work importance sampling-based algorithms Bayesian networks, postpone discussion work next section.
present generic stochastic sampling algorithm help us reviewing
prior work presenting algorithm.
posterior probability Pr(a|e) obtained first computing Pr(a, e) Pr(e)
combining based definition conditional probability
Pr(a|e) =

Pr(a, e)
.
Pr(e)

(7)

order increase accuracy results importance sampling computing posterior probabilities different network variables given evidence, general use
different importance functions Pr(a, e) Pr(e). increases computation time linearly gain accuracy may significant given obtaining
desired accuracy exponential nature. often, common practice use
importance function (usually Pr(e)) sample probabilities. difference
1. Order nodes according topological order.
2. Initialize importance function Pr0 (X\E), desired number samples
m, updating interval l, score arrays every node.
3. k 0,
4. 1
5.

(i mod l == 0)

6.

k k+1

7.

Update importance function Prk (X\E) based .
end

8.

si generate sample according Prk (X\E)

9.

{si }

10.

Calculate Score(si , Pr(X\E, e), Prk (X\E)) add corresponding entry every score array according instantiated states.
end

11. Normalize score arrays every node.
Figure 1: generic importance sampling algorithm.
160

fiAdaptive Importance Sampling Bayesian Networks

optimal importance functions two quantities large, perforc
c
mance may deteriorate significantly. Although Pr(a,
e) Pr(e)
unbiased estimators
c
according Property 1 (Section 2.1), Pr(a|e)
obtained means Equation 7
unbiased estimator. However, number samples increases, bias decreases
ignored altogether sample size large enough (Fishman, 1995).
Figure 1 presents generic stochastic sampling algorithm captures
existing sampling algorithms. Without loss generality, restrict
description so-called forward sampling, i.e., generation samples topological
order nodes network. forward sampling order accomplished
initialization performed Step 1, parents node placed node
itself. forward sampling, Step 8 algorithm, actual generation samples, works
follows. (i) evidence node instantiated observed state omitted
sample generation; (ii) root node randomly instantiated one possible
states, according importance prior probability node, derived
Prk (X\E); (iii) node whose parents instantiated randomly instantiated
one possible states, according importance conditional probability distribution
node given values parents, also derived Prk (X\E); (iv)
procedure followed nodes instantiated. complete instantiation si
network based method one sample joint importance probability distribution
Prk (X\E) variables network. scoring Step 10 amounts calculating
Pr(si , e)/Prk (si ), required Equation 2. ratio total score sum
number samples unbiased estimator Pr(e). Step 10, also count score
sum condition = a, i.e., unobserved variables values a,
ratio score sum number samples unbiased estimator
Pr(a, e).
existing algorithms focus posterior probability distributions individual
nodes. mentioned above, sake efficiency count score sum corresponding Pr(A = a, e), X\E, record score array node A.
entry array corresponds specified state A. method introduces additional
variance, opposed using importance function derived Prk (X\E) sample
Pr(A = a, e), X\E, directly.
2.3 Existing Importance Sampling Algorithms Bayesian Networks
main difference various stochastic sampling algorithms process
Steps 2, 7, 8 generic importance sampling algorithm Figure 1.
Probabilistic logic sampling (Henrion, 1988) simplest first proposed sampling algorithm Bayesian networks. importance function initialized Step 2
Pr(X) never updated (Step 7 null). Without evidence, Pr(X) optimal importance function evidence set, empty anyway. escapes authors
Pr(X) may optimal importance function Pr(A = a), X,
root node. mismatch optimal actually used importance
function may result large variance. sampling process evidence
without evidence except Step 10 count scores samples
inconsistent observed evidence, amounts discarding them.
161

fiCheng & Druzdzel

evidence unlikely, large difference Pr(X) optimal
importance function. Effectively, samples discarded performance logic
sampling deteriorates badly.
Likelihood weighting (LW) (Fung & Chang, 1989; Shachter & Peot, 1989) enhances
logic sampling never discards samples. likelihood weighting, importance
function Step 2
fi
fi
fi
Pr(X\E) =
Pr(xi |Pa(Xi ))fifi
fi
xi e
/


.
E=e

Likelihood weighting update importance function Step 7. Although likelihood weighting improvement logic sampling, convergence rate still
slow large difference optimal importance function Pr(X\E),
especially situations evidence unlikely. simplicity,
likelihood weighting algorithm commonly used simulation method
Bayesian network inference. often matches performance other, sophisticated
schemes simple able increase precision generating samples
algorithms amount time.
Backward sampling (Fung & del Favero, 1994) changes Step 1 generic algorithm
allows generating samples evidence nodes direction opposite
topological order nodes network. Step 2, backward sampling uses likelihood observed evidence instantiated nodes calculate Pr0 (X\E).
Although Fung del Favero mentioned possibility dynamic node ordering,
propose scheme updating importance function Step 7. Backward
sampling suffers problems similar likelihood weighting, i.e., possible mismatch importance function optimal importance function
lead poor convergence.
Importance sampling (Shachter & Peot, 1989) generic sampling algorithm. Shachter Peot introduced two variants importance sampling: self-importance
(SIS) heuristic importance. importance function used first step
self-importance algorithm
fi
fi
fi
0
Pr (X\E) =
Pr(xi |Pa(Xi ))fifi
fi
xi e
/


.
E=e

function updated Step 7. algorithm tries revise conditional probability
tables (CPTs) periodically order make sampling distribution gradually approach
posterior distribution. Since data used update importance function
compute estimator, process introduces bias estimator. Heuristic
importance first removes edges network becomes polytree,
uses modified version polytree algorithm (Pearl, 1986) compute likelihood
functions unobserved nodes. Pr0 (X\E) combination likelihood
functions Pr(X\E, e). Step 7 heuristic importance update Prk (X\E).
Shachter Peot (1989) point out, heuristic importance function still lead
bad approximation optimal importance function. exist also algorithms
combination self-importance heuristic importance (Shachter & Peot, 1989;
162

fiAdaptive Importance Sampling Bayesian Networks

Shwe & Cooper, 1991). Although researchers suggested may promising
direction work sampling algorithms, seen results would
follow this.
separate group stochastic sampling methods formed so-called Markov Chain
Monte Carlo (MCMC) methods divided Gibbs sampling, Metropolis sampling,
Hybrid Monte Carlo sampling (Geman & Geman, 1984; Gilks, Richardson, & Spiegelhalter, 1996; MacKay, 1998). Roughly speaking, methods draw random samples
unknown target distribution f (X) biasing search distribution towards
higher probability regions. applied Bayesian networks (Pearl, 1987; Chavez &
Cooper, 1990) approach determines sampling distribution variable
previous sample given Markov blanket (Pearl, 1988). corresponds updating
Prk (X\E) sampling every node. Prk (X\E) converge optimal importance
function Pr(e) Pr0 (X\E) satisfies ergodic properties (York, 1992). Since
convergence limiting distribution slow calculating updates sampling distribution costly, algorithms used practice often simple
likelihood weighting scheme.
also simulation algorithms, bounded variance algorithm
(Dagum & Luby, 1997) AA algorithm (Dagum et al., 1995), essentially
based LW algorithm Stopping-Rule Theorem (Dagum et al., 1995). Cano
et al. (1996) proposed another importance sampling algorithm performed somewhat
better LW cases extreme probability distributions, but, authors state,
general cases produced similar results likelihood weighting algorithm. Hernandez
et al. (1998) also applied importance sampling reported moderate improvement
likelihood weighting.
2.4 Practical Performance Existing Sampling Algorithms
largest network tested using sampling algorithms QMR-DT (Quick
Medical Reference Decision Theoretic) (Shwe et al., 1991; Shwe & Cooper, 1991),
contains 534 adult diseases 4,040 findings, 40,740 arcs depicting disease-to-finding
dependencies. QMR-DT network belongs class special bipartite networks
structure often referred BN2O (Henrion, 1991), two-layer
composition: disease nodes top layer finding nodes bottom layer. Shwe
colleagues used algorithm combining self-importance heuristic importance
tested convergence properties QMR-DT network. since heuristic method
iterative tabular Bayes (ITB) makes use version Bayes rule designed
BN2O networks, cannot generalized arbitrary networks. Although Shwe
colleagues concluded Markov blanket scoring self-importance sampling significantly
improve convergence rate model, cannot extend conclusion general
networks. computation Markov blanket scoring complex general multiconnected network BN2O network. Also, experiments conducted lacked
gold-standard posterior probability distribution could serve judge convergence
rate.
Pradhan Dagum (1996) tested efficient version LW algorithm bounded
variance algorithm (Dagum & Luby, 1997) AA algorithm (Dagum et al., 1995)
163

fiCheng & Druzdzel

146 node, multiply connected medical diagnostic Bayesian network. One limitation
tests probability evidence cases selected testing rather
high. Although 10% cases probability evidence order
108 smaller, simple calculation based reported mean = 34.5 number
evidence nodes, shows average probability observed state evidence node
conditional direct predecessors order (108 )1/34.5 0.59. Given
algorithm essentially based LW algorithm, based tests suspect
performance deteriorate cases evidence unlikely.
algorithms focus marginal probability one hypothesis node. many
queried nodes, efficiency may deteriorate.
tested algorithms discussed Section 2.3 several large networks.
experimental results show cases unlikely evidence, none algorithms
converges reasonable estimates posterior probabilities within reasonable amount
time. convergence becomes worse number evidence nodes increases. Thus,
using algorithms large networks, simply cannot trust results.
present results tests LW SIS algorithms detail Section 4.

3. AIS-BN: Adaptive Importance Sampling Bayesian Networks
main reason existing stochastic sampling algorithms converge slowly
fail learn good importance function sampling process and, effectively,
fail reduce sampling variance. importance function optimal,
probabilistic logic sampling without evidence, algorithms capable
converging fairly good estimates posterior probabilities within relatively
samples. example, assuming posterior probabilities extreme (i.e., larger
say 0.01), 1,000 samples may sufficient obtain good estimates.
section, present adaptive importance sampling algorithm Bayesian networks
(AIS-BN) that, demonstrate next section, performs well
tests. first describe details algorithm prove two theorems
useful learning optimal importance sampling function.
3.1 Basic Algorithm AIS-BN
Compared importance sampling used normal finite-dimensional integrals, importance sampling used Bayesian networks several significant advantages. First,
network joint probability distribution Pr(X) decomposable factored
component parts. Second, network clear structure, represents many conditional independence relationships. properties helpful estimating
optimal importance function.
basic AIS-BN algorithm presented Figure 2. main differences
AIS-BN algorithm basic importance sampling algorithm Figure 1
introduce monotonically increasing weight function wk two effective heuristic
initialization methods Step 2. also introduce special learning component Step 7
let updating process run smoothly, avoiding oscillation parameters.
164

fiAdaptive Importance Sampling Bayesian Networks

1. Order nodes according topological order.
2. Initialize importance function Pr0 (X\E) using heuristic methods, initialize weight w0 , set desired number samples updating
interval l, initialize score arrays every node.
3. k 0, , wT Score 0, wsum 0
4. 1
5.

(i mod l == 0)

6.

k k+1

7.

Update importance function Prk (X\E) wk based .
end

8.

si generate sample according Prk (X\E)

9.

{si }

10.

wiScore Score (si , Pr(X\E, e), Prk (X\E), wk )

11.

wT Score wT Score + wiScore
(Optional: add wiScore corresponding entry every score array)

12.

wsum wsum + wk
end

13. Output estimate Pr(E) wT Score /wsum
(Optional: Normalize score arrays every node)

Figure 2: adaptive importance sampling Bayesian Networks (AIS-BN) algorithm.
score processing Step 10
Pr(si , e)
wiScore = wk k
.
Pr (si )
Note respect algorithm Figure 1 becomes special case AIS-BN
wk = 1. reason use wk want give different weights
sampling results obtained different stages algorithm. stage updates
importance function, different distance optimal importance
b k ,
b k standard deviation estimated
function. recommend wk 1/
1
k
stage k using Equation 3. order keep w monotonically increasing, wk smaller
wk1 , adjust value wk1 . weighting scheme may introduce bias
1. similar weighting scheme based variance apparently developed independently Ortiz
Kaelbling (2000), recommend weight wk 1/(
bk )2 .

165

fiCheng & Druzdzel

final result. Since initial importance sampling functions often inefficient
introduce big variance results, also recommend wk = 0 first
stages algorithm. designed weighting scheme reflect fact
practice estimates small estimated variance usually good estimates.
3.2 Modifying Sampling Distribution AIS-BN
Based theoretical considerations Section 2.1, know crucial element
algorithm converging good approximation optimal importance function.
follows, first give optimal importance function calculating Pr(E = e)
discuss use structural advantages Bayesian networks approximate
function. sequel, use symbol denote importance sampling
function denote optimal importance sampling function.
Since Pr(X\E, E = e) > 0, Corollary 1
(X\E) =

Pr(X\E, E = e)
= Pr(X|E = e) .
Pr(E = e)

following corollary captures result.
Corollary 2 optimal importance sampling function (X\E) calculating Pr(E = e)
Equation 6 Pr(X|E = e).
Although know mathematical expression optimal importance sampling
function, difficult obtain function exactly. algorithm, use following
importance sampling function
(X\E) =

n


Pr(Xi |Pa(Xi ), E) .

(8)

i=1

function partially considers effect evidence every node
sampling process. network structure network
absorbed evidence, function optimal importance sampling function.
easy learn and, experimental results show, good approximation
optimal importance sampling function. Theoretically, posterior structure
model changes drastically result observed evidence, importance sampling
function may perform poorly. tried find practical networks would
happen, day encountered drastic example effect.
Section 2.2, know score sums corresponding {xi , pa(Xi ), e}
yield unbiased estimator Pr(xi , pa(Xi ), e). According definition conditional
probability, get estimator Pr0 (xi |pa(Xi ), e). achieved maintaining updating table every node, structure mimicks structure
CPT. tables allow us decompose importance function components learned individually. call tables importance conditional
probability tables (ICPT).
Definition 1 importance conditional probability table (ICPT) node X table
posterior probabilities Pr(X|Pa(X), E = e) conditional evidence indexed
immediate predecessors, Pa(X).
166

fiAdaptive Importance Sampling Bayesian Networks

ICPT tables modified process learning importance function.
prove useful theorem lead considerable savings learning
process.
Theorem 2
Xi X, Xi
/ Anc(E) Pr(Xi |Pa(Xi ), E) = Pr(Xi |Pa(Xi )) .

(9)

Proof: Suppose set values parents node Xi pa(Xi ). Node Xi
dependent evidence E given pa(Xi ) Xi d-connecting E given pa(Xi )
(Pearl, 1988). According definition d-connectivity, happens
exists member Xi descendants belongs set evidence nodes E.
words Xi
/ Anc(E).
2
Theorem 2 important AIS-BN algorithm. states essentially
ICPT tables nodes ancestors evidence nodes equal
CPT tables throughout learning process. need learn ICPT tables
ancestors evidence nodes. often lead significant savings
computation. If, example, evidence nodes root nodes, ICPT tables
every node already AIS-BN algorithm becomes identical likelihood weighting
algorithm. Without evidence, AIS-BN algorithm becomes identical probabilistic
logic sampling algorithm.
worth pointing Xi , Pr(Xi |Pa (Xi ), E) (i.e., ICPT table
Xi ), easily calculated using exact methods. example, Xi parent
evidence node Ej Ej child Xi , posterior probability distribution
Xi straightforward compute exactly. Since focus current paper
Input: Initialized importance function Pr0 (X\E), learning rate (k).
Output: estimated importance function PrS (X\E).
stage k 0
1. Sample l points sk1 , sk2 , . . . , skl independently according current importance function Prk (X\E).
2. every node Xi Xi X\E Xi
/ Anc(E) count score sums
corresponding {xi , pa(Xi ), e} estimate Pr0 (xi |pa(Xi ), e) based sk1 ,
sk2 , . . . , skl .
3. Update Prk (X\E) according following formula:
Prk+1 (xi |pa(Xi ), e) =




Prk (xi |pa(Xi ), e) + (k) Pr0 (xi |pa(Xi ), e) Prk (xi |pa(Xi ), e)
end

Figure 3: AIS-BN algorithm learning optimal importance function.
167

fiCheng & Druzdzel

sampling, test results reported paper include improvement
AIS-BN algorithm.
Figure 3 lists algorithm implements Step 7 basic AIS-BN algorithm listed
Figure 2. estimate Pr0 (xi |pa(Xi ), e), use samples obtained
current stage. One reason information obtained previous stages
absorbed Prk (X\E). reason principle, successive iteration
accurate previous one importance function closer optimal
importance function. Thus, samples generated Prk+1 (X\E) better
generated Prk (X\E). Pr0 (Xi |pa(Xi ), e) Prk (Xi |pa(Xi ), e) corresponds vector
first partial derivatives direction maximum decrease error. (k)
positive function determines learning rate. (k) = 0 (lower bound),
update importance function. (k) = 1 (upper bound), stage
discard old function. convergence speed directly related (k). small,
convergence slow due large number updating steps needed
reach local minimum. hand, large, convergence rate initially
fast, algorithm eventually start oscillate thus may reach
minimum. many papers field neural network learning discuss
choose learning rate let estimated importance function converge quickly
destination function. method improve learning rate applicable
algorithm. Currently, use following function proposed Ritter et al. (1991)
k/kmax

(k) =

b


,

(10)

initial learning rate b learning rate last step. function
reported perform well neural network learning (Ritter et al., 1991).
3.3 Heuristic Initialization AIS-BN
dimensionality problem Bayesian network inference equal number
variables network, networks considered paper high.
result, learning space optimal importance function large. Choice
initial importance function Pr0 (X\E) important factor affecting learning
initial value importance function close optimal importance function
greatly affect speed convergence. section, present two heuristics
help achieve goal.
Due explicit encoding structure decomposable joint probability distribution, Bayesian networks offer computational advantages compared finite-dimensional
integrals. possible first approximation optimal importance function prior
probability distribution network variables, Pr(X). propose improvement
initialization. know effect evidence nodes node attenuated
path length node evidence nodes increased (Henrion, 1989)
affected nodes direct ancestors evidence nodes. Initializing ICPT
tables parents evidence nodes uniform distributions experience improves convergence rate. Furthermore, CPT tables parents evidence
node E may favorable observed state e probability E = e without
168

fiAdaptive Importance Sampling Bayesian Networks

condition less small value, Pr(E = e) < 1/(2 nE ), nE
number outcomes node E. Based observation, change CPT tables
parents evidence node E uniform distributions experiment
Pr(E = e) < 1/(2 nE ), otherwise leave unchanged. kind initialization
involves knowledge Pr(E = e), marginal probability without evidence. Probabilistic logic sampling (Henrion, 1988) enhanced Latin hypercube sampling (Cheng &
Druzdzel, 2000b) quasi-Monte Carlo methods (Cheng & Druzdzel, 2000a) produce
good estimate Pr(E = e). one-time effort made model
building stage worth pursuing desired precision.
Another serious problem related sampling extremely small probabilities. Suppose
exists root node state prior probability Pr(s) = 0.0001. Let
posterior probability state given evidence Pr(s|E) = 0.8. simple calculation
shows update importance function every 1, 000 samples, expect
hit every 10 updates. Thus ss convergence rate slow.
overcome problem setting threshold replacing every probability p <
network .2 time, subtract ( p) largest probability
conditional probability distribution. example, value = 10/l, l
updating interval, allow us sample 10 times often first stage
algorithm. state turns likely (having large weight), increase
probability even order converge correct answer faster. Considering
avoid f (X) |g(X) f (X)| unimportant region discussed
Section 2.1, need make threshold larger. found convergence
rate quite sensitive threshold. Based empirical tests, suggest use
= 0.04 networks whose maximum number outcomes per node exceed five.
smaller threshold might lead fast convergence cases slow convergence
others. one threshold work, changing specific network usually improve
convergence rate.
3.4 Selection Parameters
several tunable parameters AIS-BN algorithm. base choice
parameters Central Limit Theorem (CLT). According CLT, Z1 , Z2 , . . . ,
Zn independent identically distributed random variables E(Zi ) = Z
Var(Zi ) = Z2 , = 1, ..., n, Z = (Z1 +...+Zn )/n approximately normally distributed
n sufficiently large. Thus,
lim P (

n

fi
fi
fi
fi
fiZ z fi

z


Z
2
Z / n
2
ex /2 dx .
t) =

z
2

(11)

Although approximation holds n approaches infinity, CLT known
robust lead excellent approximations even small n. formula Equation 11
(r , ) Relative Approximation, estimate satisfies
P(

| |
r ) .


2. initialization heuristic apparently developed independently Ortiz Kaelbling (2000).

169

fiCheng & Druzdzel

fixed,


Z / n

r =
1
Z ( ),
z
2

Z (z) = 12 z ex /2 dx. Since sampling problem, z (corresponding

Pr(E) Figure 2) fixed, setting r smaller value amounts letting Z / n

smaller. So, adjust parameters based Z / n, estimated
bk
using Equation 3. also theoretical intuition behind recommendation wk 1/
Section 3.1. expect work well networks, guarantees
given exist always extreme cases sampling algorithms
good estimate variance obtained.
R

2

3.5 Generalization AIS-BN: Problem Estimating Pr(a|e)
typical focus systems based Bayesian networks posterior probability various
outcomes individual variables given evidence, Pr(a|e). generalized
computation posterior probability particular instantiation set variables
given evidence, i.e., Pr(A = a|e). two methods capable performing
computation. first method efficient expense precision. second
method less efficient, offers general better convergence rates. methods
based Equation 7.
first method reuses samples generated estimate Pr(e) estimating Pr(a, e).
Estimation Pr(a, e) amounts counting scored sum condition = a.
main advantage method efficiency use set samples
estimate posterior probability state subset network given evidence.
main disadvantage variance estimated Pr(a, e) large, especially
numerical value Pr(a|e) extreme. method widely used
approach existing stochastic sampling algorithms.
second method, used much rarely (e.g., Cano et al., 1996; Pradhan & Dagum,
1996; Dagum & Luby, 1997), calls estimating Pr(e) Pr(a, e) separately.
estimating Pr(e), additional call algorithm made instantiation
set variables interest A. Pr(a, e) estimated sampling network
set observations e extended = a. main advantage method
much better reducing variance first method. main disadvantage
computational cost associated sampling possibly many combinations states
nodes interest.
Cano et al. (1996) suggested modified version second method. Suppose
interested posterior distribution Pr(ai |e) possible values ai A, = 1,
2, . . . , k. estimate Pr(ai , e) = 1, . . . , k separately, use value
Pk
i=1 Pr(ai , e) estimate Pr(e). assumption behind approach
estimate Pr(e) accurate large sample drawn.
However, even guarantee small variance every Pr(ai , e), cannot guarantee
sum also small variance. So, AIS-BN algorithm use
pure form methods. algorithm listed Figure 2 based first
method optional computations Steps 12 13 performed. algorithm
170

fiAdaptive Importance Sampling Bayesian Networks

corresponding second method skips optional steps calls basic AIS-BN
algorithm twice estimate Pr(e) Pr(a, e) separately.
first method attractive simplicity possible computational
efficiency. However, shown Section 2.2, performance sampling algorithm uses one set samples (as first method above) estimate Pr(a|e)
deteriorate difference optimal importance functions Pr(a,e)
Pr(e) large. main focus computation high accuracy posterior probability distribution small number nodes, strongly recommend use algorithm
based second method. Also, algorithm easily used estimate confidence
intervals solution.

4. Experimental Results
section, first describe experimental method used tests. tests focus
CPCS network, one largest realistic networks available
know precisely nodes observable. were, therefore, able
generate realistic test cases. Since AIS-BN algorithm uses two initialization
heuristics, designed experiment studies contribution two
heuristics performance algorithm. probe extent AIS-BN algorithms
excellent performance, test several real large networks.
4.1 Experimental Method
performed empirical tests comparing AIS-BN algorithm likelihood weighting
(LW) self-importance sampling (SIS) algorithms. two algorithms basically
state art general purpose belief updating algorithms. AA (Dagum et al.,
1995) bounded variance (Dagum & Luby, 1997) algorithms, suggested
reviewer, essentially enhanced special purpose versions basic LW algorithm.
implementation three algorithms relied essentially code separate
functions algorithms differed. fair assume, therefore, observed
differences purely due theoretical differences among algorithms due
efficiency implementation. order make comparison AIS-BN algorithm
LW SIS fair, used first method computation (Section 3.5), i.e., one
relies single sampling rather calling basic AIS-BN algorithm twice.
measured accuracy approximation achieved simulation terms
Mean Square Error (MSE), i.e., square root sum square differences Pr0 (xij )
Pr(xij ), sampled exact marginal probabilities state j (j = 1, 2, . . . , ni )
node i, Xi
/ E. precisely,
v
u
u
MSE = P

1

Xi N\E ni

X

ni
X

(Pr0 (xij ) Pr(xij ))2 ,

Xi N\E j=1

N set nodes, E set evidence nodes, ni number
outcomes node i. diagrams, reported MSE averaged 10 runs. used
clustering algorithm (Lauritzen & Spiegelhalter, 1988) compute gold standard
171

fiCheng & Druzdzel

results comparisons mean square error. performed experiments
Pentium II, 333 MHz Windows computer.
MSE perfect, simplest way capturing error lends
theoretical analysis. example, possible derive analytically idealized
convergence rate terms MSE, which, turn, used judge quality
algorithm. MSE used virtually previous tests sampling algorithms,
allows interested readers tie current results past studies. reviewer offered
interesting suggestion using cross-entropy technique weights small
changes near zero much strongly equivalent size change middle
[0, 1] interval. measure would penalize algorithm imprecisions possibly
several orders magnitude small probabilities. idea interesting,
aware theoretical reasons measure would make difference
comparisons AIS-BN, LW SIS algorithms. MSE, mentioned above,
allow us compare empirically determined convergence rate theoretically
derived ideal convergence rate. Theoretically, MSE inversely proportional
square root sample size.
Since several tunable parameters used AIS-BN algorithm, list
values parameters used test: l = 2, 500; wk = 0 k 9 wk = 1
otherwise. stopped updating process Step 7 Figure 2 k 10.
words, used samples collected last step algorithm. learning
parameters used algorithm kmax = 10, = 0.4, b = 0.14 (see Equation 10).
used empirically determined value threshold = 0.04 (Section 3.3).
change CPT tables parents special evidence node uniform distributions
Pr(A = a) < 1/(2 nA ). parameters matter design decision
(e.g., number samples tests), others chosen empirically. Although
found parameters may different optimal values different Bayesian
networks, used values tests AIS-BN algorithm described
paper. Since set parameters led spectacular improvement accuracy
tested networks, fair say superiority AIS-BN algorithm
algorithms sensitive values parameters.
SIS algorithm, wk = 1 design algorithm. used l = 2, 500.
updating function Step 7 Figure 1 (Shwe et al., 1991; Cousins, Chen, &
Frisse, 1993):
Prknew (xi |pa(Xi ), e) =

c
Pr(xi |pa(Xi )) + k Pr
current (xi |pa(Xi ), e) ,
1+k

c
Pr(xi |pa(Xi )) original sampling distribution, Pr
current (xi |pa(Xi ), e)
equivalent ICPT tables estimator based currently available information,
k updating step.

4.2 Results CPCS Network
main network used tests subset CPCS (Computer-based Patient Case
Study) model (Pradhan et al., 1994), large multiply-connected multi-layer network consisting 422 multi-valued nodes covering subset domain internal medicine.
172

fiAdaptive Importance Sampling Bayesian Networks

Among 422 nodes, 14 nodes describe diseases, 33 nodes describe history risk factors, remaining 375 nodes describe various findings related diseases.
CPCS network among largest real networks available research community
present time. CPCS network contains many extreme probabilities, typically
order 104 . analysis based subset 179 nodes CPCS network,
created Max Henrion Malcolm Pradhan. used smaller version order
able compute exact solution purpose measuring approximation error
sampling algorithms.
AIS-BN algorithm learning overhead. following comparison execution time vs. number samples may give reader idea overhead. Updating
CPCS network 20 evidence nodes system takes AIS-BN algorithm
total 8.4 seconds learn. generates subsequently 3,640 samples per second,
SIS algorithm generates 2,631 samples per second, LW algorithm generates 4,167
samples per second. order remain conservative towards AIS-BN algorithm,
experiments fixed execution time algorithms (our limit 60 seconds)
rather number samples. CPCS network 20 evidence nodes, 60
seconds, AIS-BN generates 188,000 samples, SIS generates 158,000 samples
LW generates 250,000 samples.

12
100%
90%

10

80%

Frequency

8

70%
60%

6

50%
40%

4
30%
20%

2

10%
0

0%
1E-40

1E-34

1E-28

1E-22

1E-16

1E-10

Probability evidence

Figure 4: probability distribution evidence Pr(E = e) experiments.
generated total 75 test cases consisting five sequences 15 test cases each.
ran test case 10 times, time different setting random number seed.
sequence progressively higher number evidence nodes: 15, 20, 25, 30,
35 evidence nodes respectively. evidence nodes chosen randomly (equiprobable
sampling without replacement) nodes described various plausible medical
173

fiCheng & Druzdzel

findings. Almost nodes leaf nodes network. believe
constituted realistic test cases algorithms. distribution prior probability evidence, Pr(E = e), across test runs experiments shown Figure 4.
least likely evidence 5.54 1042 , likely evidence 1.37 109 ,
median 7 1024 .

0.30

AIS-BN

SIS

LW

Mean Square Error

0.25

0.20

0.15

0.10

0.05

0.00
15

30

45

60

75

90

105

120

135

150

Sample time (seconds)

Figure 5: typical plot convergence tested sampling algorithms experiments
Mean Square Error function execution time subset
CPCS network 20 evidence nodes chosen randomly among plausible medical
observations (Pr(E = e) = 3.33 1026 particular case) AIS-BN,
SIS, LW algorithms. curve AIS-BN algorithm
close horizontal axis.

Figures 5 6 show typical plot convergence tested sampling algorithms
experiments. case illustrated involves updating CPCS network 20 evidence
nodes. plot MSE initial 15 seconds algorithms start
converging. particular, learning step AIS-BN algorithm usually completed
within first 9 seconds. ran three algorithms case 150 seconds rather
60 seconds actual experiment order able observe wider range
convergence. plot MSE AIS-BN algorithm almost touches X axis
Figure 5. Figure 6 shows plot finer scale order show detail
AIS-BN convergence curve. clear AIS-BN algorithm dramatically improves
convergence rate. also see results AIS-BN converge exact results
fast sampling time increases. case captured Figures 5 6, tenfold
increase sampling time (after subtracting overhead AIS-BN algorithm,
174

fiAdaptive Importance Sampling Bayesian Networks

0.0025

AIS-BN

Mean Square Error

0.0020

0.0015

0.0010

0.0005

0.0000
15

30

45

60

75

90

105

120

135

150

Sample time (seconds)

Figure 6: lower part plot Figure 5 showing convergence AIS-BN
algorithm correct posterior probabilities.

corresponds 21.5-fold increase number samples) results 4.55-fold decrease
MSE (to MSE 0.00048). observed convergence SIS LW algorithms
poor. tenfold increase sampling time practically effect accuracy. Please
note typical case observed experiments.

Absent
Mild
Moderate
Severe

Original CPT
0.99631
0.00183
0.00093
0.00093

Exact ICPT
0.0037
0.1560
0.1190
0.7213

Learned ICPT
0.015
0.164
0.131
0.690

Table 1: fragment conditional probability table node CPCS network
(node gasAcute, parents hepAcute=Mild wbcTotTho=False) Figure 6.

Figure 7 illustrates ICPT learning process AIS-BN algorithm sample
case shown Figure 6. displayed conditional probabilities belong node gasAcute
parent two evidence nodes, difInfGasMuc abdPaiExaMea. node
gasAcute four states: absent, mild, moderate, severe, two parents.
randomly chose combination parents states displayed configuration.
original CPT configuration without evidence, exact ICPT evidence
learned ICPT evidence summarized numerically Table 1. Figure 7 illustrates
175

fiCheng & Druzdzel

0.8

Absent

Mild

Moderate

Severe

0.7

Probability

0.6
0.5
0.4
0.3
0.2
0.1
0
0

1

2

3

4

5

6

7

8

9

10

Updating step

Figure 7: Convergence conditional probabilities example run AISBN algorithm captured Figure 6. displayed fragment conditional
probability table belongs node gasAcute parent one evidence
nodes.

learned importance conditional probabilities begin converge exact results
stably three updating steps. learned probabilities Step 10 close
exact results. example, difference Pr(xi |pa(Xi ), e) Pr(xi |pa(Xi ))
large. Sampling Pr(xi |pa(Xi )) instead Pr(xi |pa(Xi ), e) would introduce large
variance results.



min
median
max

AIS-BN
0.00082
0.00022
0.00049
0.00078
0.00184

SIS
0.110
0.076
0.0016
0.105
0.316

LW
0.148
0.093
0.0031
0.154
0.343

Table 2: Summary simulation results 75 simulation cases CPCS
network. Figure 8 shows 75 cases graphically.

Figure 8 shows MSE 75 test cases experiments summary
statistics Table 2. paired one-tailed t-test resulted statistically highly significant
differences AIS-BN SIS algorithms (p < 3.1 1020 ), also
176

fiAdaptive Importance Sampling Bayesian Networks

1

AIS-BN

SIS

LW

Mean Square Error

0.1

0.01

0.001

0.0001
1.4E-09 2.1E-14 3.1E-18 5.7E-22 6.7E-24 1.4E-27 1.3E-32 3.8E-39

Probability evidence

Figure 8: Performance AIS-BN, SIS, LW algorithms: Mean Square Error
75 individual test cases plotted probability evidence.
sampling time 60 seconds.

SIS LW algorithms (p < 1.7 108 ). far magnitude difference
concerned, AIS-BN significantly better SIS. SIS better LW,
difference small. mean MSEs SIS LW algorithms greater
0.1, suggests neither algorithms suitable large Bayesian networks.
graph Figure 9 shows MSE ratio AIS-BN SIS algorithms.
see percentage cases whose ratio greater 100 (two orders
magnitude improvement!) 60%. words, obtained two orders magnitude
improvement MSE half cases. 80% cases, ratio greater
50. smallest ratio experiments 2.67, happened posterior
probabilities dominated prior probabilities. case, even though LW
SIS algorithms converged fast, MSE still far larger AIS-BN.
next experiment aimed showing close AIS-BN algorithm approach
best possible sampling results. know optimal importance sampling function,
convergence AIS-BN algorithm forward sampling
without evidence. words, results probabilistic logic sampling algorithm
without evidence approach limit well stochastic sampling perform. ran
logic sampling algorithm CPCS network without evidence mimicking test
runs AIS-BN algorithm, i.e., 5 blocks 15 runs, repeated 10 times
different random number seed. number samples generated equal average
number samples generated AIS-BN algorithm series 15 test runs.
177

fiCheng & Druzdzel

18

100%

16

90%
80%

14

70%

Frequency

12

60%
10
50%
8
40%
6

30%

4

20%

2
0

10%



fffiff ff ff

0%

ratio MSE SIS AIS-BN

Figure 9: ratio MSE SIS AIS-BN versus percentage.

obtained average MSE = 0.00057, = 0.000025, min = 0.00052,
max = 0.00065. best results around range. Table 2,
see minimum MSE AIS-BN algorithm 0.00049, within range
optimal result. mean MSE AIS-BN 0.00082, far optimal
results. standard deviation, , significantly larger AIS-BN algorithm,
understandable given process learning optimal importance function
heuristic nature. difficult understand exist difference
AIS-BN results optimal results. First, AIS-BN algorithm tests updated
sampling distribution 10 times, may times let converge
optimal importance distribution. Second, even algorithm converged
optimal importance distribution, sampling algorithm still let parameter
oscillate around distribution always small differences two
distributions.
Figure 10 shows convergence rate tested cases four-fold increase
sampling time (between 15 60 seconds). adjusted convergence ratio
AIS-BN algorithm dividing constant. According Equation 3, theoretically
expected convergence ratio four-fold increase number samples
around two. 96% cases among AIS-BN runs whose ratio lays
interval (1.75, 2.25], sharp contrast 11% 13% cases SIS LW
algorithms. ratios remaining 4% cases AIS-BN lay interval [2.25, 2.5].
SIS LW algorithms, percentage cases whose ratio smaller 1.5
71% 77% respectively. Less 1.5 means number samples
small estimate variance results cannot trusted. ratio greater 2.25
178

fiAdaptive Importance Sampling Bayesian Networks

70%

AIS-BN

SIS

LW
59%

60%

Frequency

50%

40%

37%

36%
35%

30%
20%

20%

20%
19%

11%

9%9%

10%
5%

8%
5%

5%5%

3%
0%

0%

4%

3%

3%

4%

0%

0%
0.5 - 0.75 0.75 - 1.0 1.0 - 1.25 1.25 - 1.5 1.5 - 1.75 1.75 - 2.0 2.0 - 2.25 2.25 - 2.5 2.5 - 2.75 2.75 - 3.0



Convergence rate

Figure 10: distribution convergence ratio AIS-BN, SIS, LW algorithms number samples increases four times.

means possibly 60 seconds long enough estimate variance, 15 seconds
short.
4.3 Role AIS-BN Heuristics Performance Improvement
experimental results see AIS-BN algorithm improve
sampling performance significantly. next series tests focused studying role
two AIS-BN initialization heuristics. first initializing ICPT tables
parents evidence uniform distributions, denoted U. second adjusting small
probabilities, denoted S. denote AIS-BN without heuristic initialization method
AIS algorithm. AIS+U+S equals AIS-BN. compared following versions
algorithms: SIS, AIS, SIS+U, AIS+U, SIS+S, AIS+S, SIS+U+S, AIS+U+S.
algorithms SIS used number samples SIS. algorithms AIS used
number samples AIS-BN. tested algorithms 75 test
cases used previous experiment. Figure 11 shows MSE sampling
algorithms summary statistics Table 3. Even though AIS algorithm better
SIS algorithm, difference large case AIS+U, AIS+S,
AIS-BN algorithms. seems heuristic initialization methods help much. results
SIS+S, SIS+U, SIS+U+S algorithms suggest although heuristic initialization
methods improve performance, alone cannot improve much. fair say
significant performance improvement AIS-BN algorithm coming
combination AIS heuristic methods, method alone. difficult
179

fiCheng & Druzdzel

understand that, good heuristic initialization methods possible let
learning process quickly exit oscillation areas. Although U methods alone
improve performance, improvement moderate compared combination
two.

0.12

0.110

Mean Square Error

0.10

0.075

0.08

0.060
0.06

0.050

0.050

0.04

0.02

0.008
0.00151
0.00




fi ff
fi ff

ff fiff

0.00082
ff ff

Different Algorithms

Figure 11: comparison different algorithms CPCS network. bar based
75 test cases. dotted bar shows MSE SIS algorithm
gray bar shows MSE AIS algorithm.



min
median
max

SIS
0.110
0.076
0.0016
0.105
0.316

AIS
0.060
0.049
0.00074
0.045
0.207

SIS+U
0.050
0.052
0.0011
0.031
0.212

AIS+U
0.0084
0.025
0.00058
0.0014
0.208

SIS+S
0.075
0.074
0.00072
0.052
0.279

AIS+S
0.0015
0.0016
0.00056
0.00087
0.0085

SIS+U+S
0.050
0.059
0.00086
0.028
0.265

AIS-BN
0.00082
0.00022
0.00049
0.00078
0.0018

Table 3: Summary simulation results different algorithms CPCS network.

4.4 Results Networks
order make sure AIS-BN algorithm performs well general, tested
two large networks.
first network used tests PathFinder network (Heckerman
et al., 1990), core element expert system assists surgical pathologists
180

fiAdaptive Importance Sampling Bayesian Networks

diagnosis lymph-node diseases. two versions network. used
larger version, consisting 135 nodes. contrast CPCS network, PathFinder
contains many conditional probabilities equal 1, reflects deterministic
relationships certain settings. make sampling challenging, randomly selected
20 evidence nodes among leaf nodes. observable node (David
Heckerman, personal communication). verified case probability
selected evidence equal zero.
fixed execution time algorithms 60 seconds. learning overhead
AIS-BN algorithm PathFinder network 3.5 seconds. 60
seconds, AIS-BN generated 366,000 samples, SIS generated 250,000 samples
LW generated 2,700,000 samples. reason LW could generate
10 times many samples SIS within amount time LW algorithm
terminates sample generation early stage many samples, weight
sample becomes zero. result determinism probability tables, mentioned
above. see LW benefits greatly generating samples.
parameters used AIS-BN used CPCS network.
tested 20 cases, randomly selected 20 evidence nodes. reported MSE
case averaged 10 runs. runs SIS LW algorithms
manage generate effective samples (the weight score sum equal zero). SIS
75% effective runs LW 89% effective runs, means
runs SIS LW unable yield information posterior distributions.
cases, discarded run averaged effective runs.
runs AIS-BN algorithm effective. report experimental results
summary statistics Table 4. data, see AIS-BN algorithm
still significantly better SIS LW algorithms. Since LW algorithm
generate ten times number samples SIS algorithm, performance
better SIS algorithm.



min
median
max
effective runs

AIS-BN
0.00050
0.00037
0.00025
0.00037
0.0017
200

SIS
0.166
0.107
0.00116
0.184
0.467
150

LW
0.089
0.0707
0.00080
0.0866
0.294
178

Table 4: Summary simulation results 20 simulation cases
PathFinder network.

second network tested one ANDES networks (Conati et al.,
1997). ANDES intelligent tutoring system classical Newtonian physics
developed team researchers Learning Research Development Center
University Pittsburgh researchers United States Naval Academy.
student model ANDES uses Bayesian network longterm knowledge assessment,
181

fiCheng & Druzdzel

plan recognition, prediction students actions problem solving. selected
largest ANDES network available us, consisting 223 nodes.
contrast previous two networks, depth ANDES network significantly larger connectivity. 22 leaf nodes. quite
predictable kind networks pose difficulties learning. selected 20
evidence nodes randomly potential evidence nodes tested 20 cases. parameters used CPCS network. fixed execution time
algorithms 60 seconds. learning overhead AIS-BN algorithm
ANDES network 13.4 seconds. 60 seconds, AIS-BN generated 114,000
samples, SIS generated 98,000 samples LW generated 180,000 samples.
network, LW still generate almost two times number samples generated
SIS algorithm.
report experimental results summary statistics Table 5. results
show also ANDES network AIS-BN algorithm significantly better
SIS LW algorithms. Since LW generated almost two times number samples
generated SIS algorithm, performance better SIS
algorithm.



min
median
max

AIS-BN
0.0059
0.0049
0.0023
0.0045
0.0237

SIS
0.0628
0.102
0.0028
0.0190
0.321

LW
0.0404
0.0539
0.0028
0.0198
0.221

Table 5: Summary simulation results 20 simulation cases ANDES
network.

AIS-BN algorithm average order magnitude precise
two algorithms, performance improvement smaller
two networks. reason performance improvement AIS-BN algorithm
SIS LW algorithms ANDES network smaller compared
CPCS PathFinder networks that: (1) ANDES network used tests
apparently challenging enough sampling algorithms general. ANDES
network, SIS LW also perform well cases. minimum MSE SIS
LW tested cases almost AIS-BN. (2) number samples
generated AIS-BN network significantly smaller previous
two networks AIS-BN needs time learn. Although increasing number
samples improve performance three algorithms, improves performance
AIS-BN since convergence ratio AIS-BN algorithm usually larger
SIS LW (see Figure 10). (3) parameters used network
tuned CPCS network. (4) large depth fewer leaf nodes ANDES
network pose difficulties learning.
182

fiAdaptive Importance Sampling Bayesian Networks

5. Discussion
fundamental trade-off AIS-BN algorithm time spent
learning importance function time spent sampling. current approach,
believe reasonable, stop learning point importance
function good enough. experiments stopped learning 10 iterations.
several ways improving initialization conditional probability
tables outset AIS-BN algorithm. current version algorithm,
initialize ICPT table every parent N evidence node E (N Pa(E), E E)
uniform distribution Pr(E = e) < 1/(2 nE ). improved further.
extend initialization nodes severely affected evidence.
identified examining network structure local CPTs.
view learning process AIS-BN algorithm network rebuilding
process. algorithm constructs new network whose structure original
network (except delete evidence nodes corresponding arcs). constructed
network models joint probability distribution (X\E) Equation 8, approaches
optimal importance function. use learned 0 approximate distribution.
0 approximates Pr(X|E) accurately enough, use new network solve
approximate tasks, problem computing Maximum A-Posterior assignment
(MAP) (Pearl, 1988), finding k likely scenarios (Seroussi & Golmard, 1994), etc.
large advantage approach solve problems network
evidence nodes.
know Markov blanket scoring improve convergence rates sampling
algorithms (Shwe & Cooper, 1991). may also applied AIS-BN algorithm
improve convergence rate. According Property 4 (Section 2.1), technique
2
c
reduce variance Pr
reduce variance Pr(e)
correspondingly improve
(e)
sampling performance. Since variance stratified sampling (Rubinstein, 1981)
never much worse random sampling, much better, improve
convergence rate. expect variance reduction methods statistics, as:
(i) expected value random variable; (ii) antithetic variants correlations (stratified
sampling, Latin hypercube sampling, etc.); (iii) systematic sampling, also improve
sampling performance.
Current learning algorithm used simple approach. heuristic learning methods,
adjusting learning rates according changes error (Jacobs, 1988),
also applicable algorithm. several tunable parameters AIS-BN
algorithm. Finding optimal values parameters given network another
interesting research topic.
worth observing plots presented Figure 8 fairly flat. words,
tests convergence sampling algorithms depend strongly
probability evidence. seems contradict common belief forward sampling
schemes suffer unlikely evidence. AIS-BN one shows fairly flat plot.
convergence SIS LW algorithms seems decrease slightly unlikely evidence.
possible three algorithms perform much worse probability
evidence drops threshold value, tests failed approach.
183

fiCheng & Druzdzel

relationship studied carefully, conjecture probability evidence
good measure difficulty approximate inference.
Given problem approximating probabilistic inference NP-hard, exist
networks challenging algorithm doubt even
AIS-BN algorithm perform poorly them. day, found
networks. one characteristic networks may challenging AIS-BN
algorithm. general, number parameters need learned AISBN algorithm increases, performance deteriorate. Nodes many parents,
example, challenging AIS-BN learning algorithm, update ICPT
tables combinations parent nodes. possible conditional probability
distributions causal independence properties, Noisy-OR distributions (Pearl,
1988; Henrion, 1989; Diez, 1993; Srinivas, 1993; Heckerman & Breese, 1994), common
large practical networks, treated differently lead considerable savings
learning time.
One direction testing approximate algorithms, suggested us reviewer, use
large networks exact solution cannot computed all. case, one
try infer difference variance various stages algorithm whether
converging not. interesting idea worth exploring, especially
combined theoretical work stopping criteria line work Dagum
Luby (1997).

6. Conclusion
Computational complexity remains major problem application probability theory
decision theory knowledge-based systems. important develop schemes
improve performance updating algorithms even though theoretically demonstrated worst case remain NPhard, many practical cases may become tractable.
paper, studied importance sampling Bayesian networks. reviewing
important theoretical results related importance sampling finite-dimensional
integrals, proposed new algorithm importance sampling Bayesian networks
call adaptive importance sampling (AIS-BN). process learning optimal
importance function AIS-BN algorithm computationally intractable, based
theory importance sampling finite-dimensional integrals proposed several heuristics
seem work well practice. proposed heuristic methods initializing
importance function shown accelerate learning process, smooth learning method updating importance function using structural advantages Bayesian
networks, dynamic weighting function combining samples different stages
algorithm. methods help AIS-BN algorithm get fairly accurate
estimates posterior probabilities limited time. two applied heuristics,
adjustment small probabilities, seems lead largest improvement performance,
although largest decrease MSE achieved combination two heuristics
AIS-BN algorithm.
AIS-BN algorithm lead dramatic improvement convergence rates
large Bayesian networks evidence compared existing state art algorithms.
compared performance AIS-BN algorithm performance likelihood
184

fiAdaptive Importance Sampling Bayesian Networks

weighting self-importance sampling large practical model, CPCS network,
evidence unlikely 5.54 1042 typically 7 1.024 . experiments,
observed AIS-BN algorithm always better likelihood weighting selfimportance sampling 60% cases reached two orders magnitude
improvement accuracy. Tests performed two networks, PathFinder
ANDES, yielded similar results.
Although may exist approximate algorithms prove superior AISBN networks special structure distribution, AIS-BN algorithm simple
robust general evidential reasoning problems large multiply-connected Bayesian
networks.

Acknowledgments
thank anonymous referees several insightful comments led substantial
improvement paper. research supported National Science Foundation
Faculty Early Career Development (CAREER) Program, grant IRI9624629,
Air Force Office Scientific Research grants F496209710225 F49620001
0112. earlier version paper received 2000 School Information Sciences
Robert R. Korfhage Award, University Pittsburgh. Malcolm Pradhan Max Henrion
Institute Decision Systems Research shared us CPCS network kind
permission developers Internist system University Pittsburgh.
thank David Heckerman PathFinder network Abigail Gerner ANDES
network used tests. experimental data obtained using SMILE,
Bayesian inference engine developed Decision Systems Laboratory available
http://www2.sis.pitt.edu/genie.

References
Cano, J. E., Hernandez, L. D., & Moral, S. (1996). Importance sampling algorithms
propagation probabilities belief networks. International Journal Approximate
Reasoning, 15, 7792.
Chavez, M. R., & Cooper, G. F. (1990). randomized approximation algorithm probabilistic inference Bayesian belief networks. Networks, 20 (5), 661685.
Cheng, J., & Druzdzel, M. J. (2000a). Computational investigations low-discrepancy
sequences simulation algorithms Bayesian networks. Proceedings Sixteenth Annual Conference Uncertainty Artificial Intelligence (UAI2000), pp.
7281 San Francisco, CA. Morgan Kaufmann Publishers.
Cheng, J., & Druzdzel, M. J. (2000b). Latin hypercube sampling Bayesian networks.
Proceedings 13th International Florida Artificial Intelligence Research Symposium Conference (FLAIRS-2000), pp. 287292 Orlando, Florida.
Conati, C., Gertner, A. S., VanLehn, K., & Druzdzel, M. J. (1997). On-line student modeling
coached problem solving using Bayesian networks. Proceedings Sixth
185

fiCheng & Druzdzel

International Conference User Modeling (UM96), pp. 231242 Vienna, New York.
Springer Verlag.
Cooper, G. F. (1990). computational complexity probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42 (23), 393405.
Cousins, S. B., Chen, W., & Frisse, M. E. (1993). tutorial introduction stochastic
simulation algorithm belief networks. Artificial Intelligence Medicine, chap. 5,
pp. 315340. Elsevier Science Publishers B.V.
Dagum, P., Karp, R., Luby, M., & Ross, S. (1995). optimal algorithm Monte
Carlo estimation (extended abstract). Proceedings 36th IEEE Symposium
Foundations Computer Science, pp. 142149 Portland, Oregon.
Dagum, P., & Luby, M. (1993). Approximating probabilistic inference Bayesian belief
networks NP-hard. Artificial Intelligence, 60 (1), 141153.
Dagum, P., & Luby, M. (1997). optimal approximation algorithm Bayesian inference.
Artificial Intelligence, 93, 127.
Diez, F. J. (1993). Parameter adjustment Bayes networks. generalized noisy ORgate. Proceedings Ninth Annual Conference Uncertainty Artificial
Intelligence (UAI93), pp. 99105 San Francisco, CA. Morgan Kaufmann Publishers.
Fishman, G. S. (1995). Monte Carlo: concepts, algorithms, applications. SpringerVerlag.
Fung, R., & Chang, K.-C. (1989). Weighing integrating evidence stochastic simulation Bayesian networks. Uncertainty Artificial Intelligence 5, pp. 209219
New York, N. Y. Elsevier Science Publishing Company, Inc.
Fung, R., & del Favero, B. (1994). Backward simulation Bayesian networks. Proceedings
Tenth Annual Conference Uncertainty Artificial Intelligence (UAI94),
pp. 227234 San Francisco, CA. Morgan Kaufmann Publishers.
Geman, S., & Geman, D. (1984). Stochastic relaxations, Gibbs distributions Bayesian restoration images. IEEE Transactions Pattern Analysis Machine
Intelligence, 6 (6), 721742.
Gilks, W., Richardson, S., & Spiegelhalter, D. (1996). Markov chain Monte Carlo practice. Chapman Hall.
Heckerman, D., & Breese, J. S. (1994). new look causal independence. Proceedings
Tenth Annual Conference Uncertainty Artificial Intelligence (UAI94),
pp. 286292 San Mateo, CA. Morgan Kaufmann Publishers, Inc.
Heckerman, D. E., Horvitz, E. J., & Nathwani, B. N. (1990). Toward normative expert
systems: Pathfinder project. Tech. rep. KSL9008, Medical Computer Science
Group, Section Medical Informatics, Stanford University, Stanford, CA.
186

fiAdaptive Importance Sampling Bayesian Networks

Henrion, M. (1988). Propagating uncertainty Bayesian networks probabilistic logic
sampling. Uncertainty Artificial Intellgience 2, pp. 149163 New York, N. Y.
Elsevier Science Publishing Company, Inc.
Henrion, M. (1989). practical issues constructing belief networks. Kanal, L.,
Levitt, T., & Lemmer, J. (Eds.), Uncertainty Artificial Intelligence 3, pp. 161173.
Elsevier Science Publishers B.V., North Holland.
Henrion, M. (1991). Search-based methods bound diagnostic probabilities large
belief nets. Proceedings Seventh Annual Conference Uncertainty Artificial Intelligence (UAI91), pp. 142150 San Mateo, California. Morgan Kaufmann
Publishers.
Hernandez, L. D., Moral, S., & Antonio, S. (1998). Monte Carlo algorithm probabilistic
propagation belief networks based importance sampling stratified simulation
techniques. International Journal Approximate Reasoning, 18, 5391.
Jacobs, R. A. (1988). Increased rates convergence learning rate adaptation.
Neural Networks, 1, 295307.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computations probabilities
graphical structures application expert systems. Journal Royal
Statistical Society, Series B (Methodological), 50 (2), 157224.
MacKay, D. (1998). Intro Monte Carlo methods. Jordan, M. I. (Ed.), Learning
Graphical Models. MIT Press, Cambridge, Massachusetts.
Ortiz, L. E., & Kaelbling, L. P. (2000). Adaptive importance sampling estimation
structured domains. Proceedings Sixteenth Annual Conference Uncertainty Artificial Intelligence (UAI2000), pp. 446454 San Francisco, CA. Morgan
Kaufmann Publishers.
Pearl, J. (1986). Fusion, propagation, structuring belief networks. Artificial Intelligence, 29 (3), 241288.
Pearl, J. (1987). Evidential reasoning using stochastic simulation causal models. Artifical
Intelligence, 32, 245257.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible
Inference. Morgan Kaufmann Publishers, Inc., San Mateo, CA.
Pradhan, M., & Dagum, P. (1996). Optimal Monte Carlo inference. Proceedings
Twelfth Annual Conference Uncertainty Artificial Intelligence (UAI96), pp.
446453 San Francisco, CA. Morgan Kaufmann Publishers.
Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering
large belief networks. Proceedings Tenth Annual Conference Uncertainty Artificial Intelligence (UAI94), pp. 484490 San Francisco, CA. Morgan
Kaufmann Publishers.
187

fiCheng & Druzdzel

Ritter, H., Martinetz, T., & Schulten, K. (1991). Neuronale Netze. Addison-Wesley,
Munchen.
Rubinstein, R. Y. (1981). Simulation Monte Carlo Method. John Wiley & Sons.
Seroussi, B., & Golmard, J. L. (1994). algorithm directly finding K probable
configurations Bayesian networks. International Journal Approximate Reasoning,
11, 205233.
Shachter, R. D., & Peot, M. A. (1989). Simulation approaches general probabilistic
inference belief networks. Uncertainty Artificial Intelligence 5, pp. 221231
New York, N. Y. Elsevier Science Publishing Company, Inc.
Shwe, M. A., & Cooper, G. F. (1991). empirical analysis likelihood-weighting simulation large, multiply-connected medical belief network. Computers Biomedical
Research, 24 (5), 453475.
Shwe, M., Middleton, B., Heckerman, D., Henrion, M., Horvitz, E., & Lehmann, H. (1991).
Probabilistic diagnosis using reformulation INTERNIST1/QMR knowledge
base: I. probabilistic model inference algorithms. Methods Information
Medicine, 30 (4), 241255.
Srinivas, S. (1993). generalization noisy-OR model. Proceedings Ninth
Annual Conference Uncertainty Artificial Intelligence (UAI93), pp. 208215
San Francisco, CA. Morgan Kaufmann Publishers.
York, J. (1992). Use Gibbs sampler expert systems. Artificial Intelligence, 56,
115130.

188

fiJournal Artificial Intelligence Research 13 (2000) 33{94

Submitted 9/99; published 8/00

Value-Function Approximations Partially Observable
Markov Decision Processes

Milos Hauskrecht

milos@cs.brown.edu

Computer Science Department, Brown University
Box 1910, Brown University, Providence, RI 02912, USA

Abstract

Partially observable Markov decision processes (POMDPs) provide elegant mathematical framework modeling complex decision planning problems stochastic
domains states system observable indirectly, via set imperfect
noisy observations. modeling advantage POMDPs, however, comes price |
exact methods solving computationally expensive thus applicable
practice simple problems. focus ecient approximation (heuristic)
methods attempt alleviate computational problem trade accuracy
speed. two objectives here. First, survey various approximation methods,
analyze properties relations provide new insights differences.
Second, present number new approximation methods novel refinements existing techniques. theoretical results supported experiments problem
agent navigation domain.
1. Introduction

Making decisions dynamic environments requires careful evaluation cost benefits immediate action also choices may future.
evaluation becomes harder effects actions stochastic, must pursue evaluate many possible outcomes parallel. Typically, problem becomes
complex look future. situation becomes even worse
outcomes observe imperfect unreliable indicators underlying process
special actions needed obtain reliable information. Unfortunately, many
real-world decision problems fall category.
Consider, example, problem patient management. patient comes
hospital initial set complaints. rarely allow physician (decisionmaker) diagnose underlying disease certainty, number disease options
generally remain open initial evaluation. physician multiple choices
managing patient. He/she choose nothing (wait see), order additional tests
learn patient state disease, proceed radical treatment
(e.g. surgery). Making right decision easy task. disease patient suffers
progress time may become worse window opportunity particular
effective treatment missed. hand, selection wrong treatment may
make patient's condition worse, may prevent applying correct treatment later.
result treatment typically non-deterministic outcomes possible.
addition, treatment investigative choices come different costs. Thus,
c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiHauskrecht

course patient management, decision-maker must carefully evaluate costs
benefits current future choices, well interaction ordering.
decision problems similar characteristics | complex temporal cost-benefit tradeoffs,
stochasticity, partial observability underlying controlled process | include robot
navigation, target tracking, machine mantainance replacement, like.
Sequential decision problems modeled Markov decision processes (MDPs)
(Bellman, 1957; Howard, 1960; Puterman, 1994; Boutilier, Dean, & Hanks, 1999)
extensions. model choice problems similar patient management partially
observable Markov decision process (POMDP) (Drake, 1962; Astrom, 1965; Sondik, 1971;
Lovejoy, 1991b). POMDP represents two sources uncertainty: stochasticity
underlying controlled process (e.g. disease dynamics patient management problem),
imperfect observability states via set noisy observations (e.g. symptoms,
findings, results tests). addition, lets us model uniform way control
information-gathering (investigative) actions, well effects cost-benefit tradeoffs. Partial observability ability model reason information-gathering
actions main features distinguish POMDP widely known fully
observable Markov decision process (Bellman, 1957; Howard, 1960).
Although useful modeling perspective, POMDPs disadvantage hard solve (Papadimitriou & Tsitsiklis, 1987; Littman, 1996; Mundhenk, Goldsmith,
Lusena, & Allender, 1997; Madani, Hanks, & Condon, 1999), optimal -optimal solutions obtained practice problems low complexity. challenging goal
research area exploit additional structural properties domain and/or suitable
approximations (heuristics) used obtain good solutions eciently.
focus heuristic approximation methods, particular approximations based
value functions. Important research issues area design new ecient
algorithms, well better understanding existing techniques relations,
advantages disadvantages. paper address issues. First,
survey various value-function approximations, analyze properties relations
provide insights differences. Second, present number new methods
novel refinements existing techniques. theoretical results findings also
supported empirically problem agent navigation domain.
2. Partially Observable Markov Decision Processes

partially observable Markov decision process (POMDP) describes stochastic control
process partially observable (hidden) states. Formally, corresponds tuple
(S; A; ; T; O; R) set states, set actions, set observations,
: ! [0; 1] set transition probabilities describe dynamic behavior
modeled environment, : ! [0; 1] set observation probabilities
describe relationships among observations, states actions, R : ! IR
denotes reward model assigns rewards state transitions models payoffs associated transitions. instances definition POMDP also includes
priori probability distribution set initial states .

34

fiValue-Function Approximations POMDPs

o0

t2



ot

t1

t+1

st

a0

t2

t+1



t1

r



Figure 1: Part uence diagram describing POMDP model. Rectangles correspond
decision nodes (actions), circles random variables (states) diamonds
reward nodes. Links represent dependencies among components. st ; ; ot
rt denote state, action, observation reward time t. Note action
time depends past observations actions, states.

2.1 Objective Function
Given POMDP, goal construct control policy maximizes objective (value)
function. objective function combines partial (stepwise) rewards multiple steps
using various kinds decision models. Typically, models cumulative based
expectations. Two models frequently used practice:

finite-horizon model maximize E (PTt=0 rt ), rt reward obtained
time t.

infinite-horizon discounted model maximize E (P1t=0 rt ), 0 <
< 1 discount factor.

Note POMDPs cumulative decision models provide rich language modeling
various control objectives. example, one easily model goal-achievement tasks (a
specific goal must reached) giving large reward transition state
zero smaller rewards transitions.
paper focus primarily discounted infinite-horizon model. However,
results easily applied also finite-horizon case.

2.2 Information State
POMDP process states hidden cannot observe making
decision next action. Thus, action choices based information available us quantities derived information. illustrated
uence diagram Figure 1, action time depends previous
observations actions, states. Quantities summarizing information called
information states. Complete information states represent trivial case.
35

fiHauskrecht

t+1



st

t+1



t+1

t+1





rt

rt

Figure 2: uence diagram POMDP information states corresponding
information-state MDP. Information states (It It+1 ) represented
double-circled nodes. action choice (rectangle) depends current
information state.

Definition 1 (Complete information state). complete information state time (denoted ItC ) consists of:




prior belief b0 states time 0;
complete history actions observations fo0 ; a0 ; o1 ; a1 ; ; ot 1 ; 1 ; ot g starting time = 0.

sequence information states defines controlled Markov process call
information-state Markov decision process information-state MDP. policy
information-state MDP defined terms control function : ! mapping
information state space actions. new information state (It ) deterministic function
previous state (It 1 ), last action (at 1 ) new observation (ot ):

= (It 1 ; ot ; 1 ):
: ! update function mapping information state space, observations
actions back information space.1 easy see one always convert
original POMDP information-state MDP using complete information states.
relation components two models sketch reduction
POMDP information-state MDP, shown Figure 2.
2.3 Bellman Equations POMDPs
information-state MDP infinite-horizon discounted case like fully-observable
MDP satisfies standard fixed-point (Bellman) equation:
(

)

X
V (I ) = max (I; a) + P (I 0 jI; a)V (I 0 ) :
a2A
I0

(1)

1. paper, denotes generic update function. Thus use symbol even information
state space different.
36

fiValue-Function Approximations POMDPs

P

Here, V (I ) denotes optimal value function maximizing E ( 1
t=0 rt ) state . (I; a)
expected one-step reward equals
X
XX
(I; a) = (s; a)P (sjI ) =
R(s; a; s0 )P (s0 js; a)P (sjI ):
s2S
s2 s0 2

(s; a) denotes expected one-step reward state action a.
Since next information state 0 = (I; o; a) deterministic function previous
information state , action a, observation o, Equation 1 rewritten
compactly summing possible observations :
V (I ) = max
a2A

(

X
s2S

(s; a)P (sjI ) +

X
o2

)

P (ojI; a)V ( (I; o; a)) :

(2)

optimal policy (control function) : ! selects value-maximizing action
(

)

X
X
(I ) = arg max
(s; a)P (sjI ) + P (ojI; a)V ( (I; o; a)) :
a2A s2S
o2

(3)

value control functions also expressed terms action-value functions
(Q-functions)
V (I ) = max Q (I; a)
(I ) = arg max Q (I; a);
a2A
a2A
X
X

Q (I; a) = (s; a)P (sjI ) + P (ojI; a)V ( (I; o; a)):
(4)
s2S
o2
Q-function corresponds expected reward chosing fixed action (a) first
step acting optimally afterwards.
2.3.1 Sufficient Statistics

derive Equations 1|3 implicitly used complete information states. However,
remarked earlier, information available decision-maker also summarized
quantities. call sucient information states. states must preserve
necessary information content also Markov property information-state
decision process.

Definition 2 (Sucient information state process). Let information state space
: ! update function defining information process =
(It 1 ; 1 ; ot ). process sucient regard optimal control when,
time step t, satisfies
P (st jIt ) = P (st jItC )
P (ot jIt 1 ; 1 ) = P (ot jItC 1 ; 1 );
ItC ItC 1 complete information states.
easy see Equations 1 | 3 complete information states must hold also
sucient information states. key benefit sucient statistics often
37

fiHauskrecht

easier manipulate store, since unlike complete histories, may expand
time. example, standard POMDP model sucient work belief states
assign probabilities every possible process state (Astrom, 1965).2 case
Bellman equation reduces to:
(

V (b) = max
a2A

X
s2S

(s; a)b(s) +

XX
o2 s2S

)

P (ojs; a)b(s)V ( (b; o; a)) ;

(5)

next-step belief state b0
X
b0 (s) = (b; o; a)(s) = fiP (ojs; a)
P (sja; s0 )b(s0 ):
0
2S

fi = 1=P (ojb; a) normalizing constant. defines belief-state MDP
special case continuous-state MDP. Belief-state MDPs also primary focus
investigation paper.
2.3.2 Value-Function Mappings Properties

Bellman equation 2 belief-state MDP also rewritten value-function
mapping form. Let V space real-valued bounded functions V : ! IR defined
belief information space , let h : B ! IR defined

h(b; a; V ) =

X

s2S

(s; a)b(s) +

XX

o2 s2S

P (ojs; a)b(s)V ( (b; o; a)):

defining value function mapping H : V ! V (HV )(b) = maxa2A h(b; a; V ),
Bellman equation 2 information states written V = HV : well
known H (for MDPs) isotone mapping contraction
supremum norm (see (Heyman & Sobel, 1984; Puterman, 1994)).

Definition 3 mapping H isotone, V; U

2 V V U implies HV HU .

Definition 4 Let k:k supremum norm. mapping H contraction
supremum norm, V; U 2 V , kHV HU k fi kV U k holds 0 fi < 1.
2.4 Value Iteration
optimal value function (Equation 2) approximation computed using dynamic programming techniques. simplest approach value iteration (Bellman,
1957) shown Figure 3. case, optimal value function V determined
limit performing sequence value-iteration steps Vi = HVi 1 , Vi
ith approximation value function (ith value function).3 sequence estimates
2. Models belief states sucient include POMDPs observation action channel
lags (see Hauskrecht (1997)).
3. note update V = HV 1 applied solve finite-horizon problem
standard way. difference V stands i-steps-to-go value function V0 represents
value function (rewards) end states.






38

fiValue-Function Approximations POMDPs

Value iteration (P OMDP , )
initialize V b 2 ;
repeat
V0 V;
update V HV 0 b 2 ;
supb j V (b) V 0 (b) j
return V;
Figure 3: Value iteration procedure.
converges unique fixed-point solution direct consequence Banach's
theorem contraction mappings (see, example, Puterman (1994)).
practice, stop iteration well reaches limit solution. stopping
criterion use algorithm (Figure 3) examines maximum difference value
functions obtained two consecutive steps | so-called Bellman error (Puterman, 1994;
Littman, 1996). algorithm stops quantity falls threshold .
accuracy approximate solution (ith value function) regard V expressed
terms Bellman error .

Theorem 1 Let = supb jVi (b) Vi 1 (b)j = kVi Vi 1 k magnitude Bellman
error. kVi V k 1 kVi 1 V k 1 hold.
Then, obtain approximation V precision Bellman error fall
(1 ) .
2.4.1 Piecewise Linear Convex Approximations Value Function

major diculty applying value iteration (or dynamic programming) beliefstate MDPs belief space infinite need compute update Vi = HVi 1
it. poses following threats: value function ith step may
representable finite means and/or computable finite number steps.
address problem Sondik (Sondik, 1971; Smallwood & Sondik, 1973) showed
one guarantee computability ith value function well finite description
belief-state MDP considering piecewise linear convex representations
value function estimates (see Figure 4). particular, Sondik showed piecewise
linear convex representation Vi 1 , Vi = HVi 1 computable remains piecewise
linear convex.

Theorem 2 (Piecewise linear convex functions). Let V0 initial value function
piecewise linear convex. ith value function obtained finite
number update steps belief-state MDP also finite, piecewise linear convex,
equal to:
X
Vi (b) = max b(s)ffi (s);
ffi 2 s2S

b ffi vectors size jS j finite set vectors (linear functions) ffi .
39

fiHauskrecht

Vi (b)

0

1

b(s1 )

Figure 4: piecewise linear convex function POMDP two process states
fs1 ; s2g. Note b(s1) = 1 b(s2 ) holds belief state.

key part proof express update ith value function
terms linear functions 1 defining Vi 1 :
8
<X

Vi (b) = max :
a2A

s2S

(s; a)b(s) +

X

max

o2 ffi 1 2



"
X X
1 s0 2S s2S

#

9
=

P (s0 ; ojs; a)b(s) ffi 1 (s0 ); :

(6)

leads piecewise linear convex value function Vi represented
finite set linear functions ffi , one linear function every combination actions
j
permutations ffi 1 vectors size jj. Let W = (a; fo1 ; ffji 1 1 g; fo2 ; ffji 2 1 g; fojj ; ffi j1j g)
combination. linear function corresponding defined
XX
ffW
P (s0 ; ojs; a)ffji 1 (s0 ):
(7)
(s) = (s; a) +
o2 s0 2S


Theorem 2 basis dynamic programming algorithm finding optimal
solution finite-horizon models value-iteration algorithm finding nearoptimal approximations V discounted, infinite-horizon model. Note, however,
result imply piecewise linearity optimal (fixed-point) solution V .
2.4.2 Algorithms Computing Value-Function Updates

key part value-iteration algorithm computation value-function updates
Vi = HVi 1 . Assume ith value function Vi represented finite number linear
segments (ff vectors). total number possible linear functions jAjj 1 jjj (one
every combination actions permutations ffi 1 vectors size jj)
enumerated O(jAjjS j2 j 1 jjj ) time. However, complete set linear functions
rarely needed: linear functions dominated others omission
change resulting piecewise linear convex function. illustrated
Figure 5.

40

fiValue-Function Approximations POMDPs

Vi (b)

redundant linear
function
0

1

b(s1 )

Figure 5: Redundant linear function. function dominate regions
belief space excluded.

linear function eliminated without changing resulting value function
solution called redundant. Conversely, linear function singlehandedly achieves
optimal value least one point belief space called useful.4
sake computational eciency important make size linear
function set small possible (keep useful linear functions) value-iteration steps.
two main approaches computing useful linear functions. first approach
based generate-and-test paradigm due Sondik (1971) Monahan (1982).
idea enumerate possible linear functions first, test usefulness
linear functions set prune redundant vectors. Recent extensions
method interleave generate test stages early pruning set partially
constructed linear functions (Zhang & Liu, 1997a; Cassandra, Littman, & Zhang, 1997;
Zhang & Lee, 1998).
second approach builds Sondik's idea computing useful linear function
single belief state (Sondik, 1971; Smallwood & Sondik, 1973), done eciently.
key problem locate belief points seed useful linear functions
different methods address problem differently. Methods implement idea
Sondik's one- two-pass algorithms (Sondik, 1971), Cheng's methods (Cheng, 1988),
Witness algorithm (Kaelbling, Littman, & Cassandra, 1999; Littman, 1996; Cassandra,
1998).
2.4.3 Limitations Complexity

major diculty solving belief-state MDP complexity piecewise
linear convex function grow extremely fast number update steps.
specifically, size linear function set defining function grow exponentially (in
number observations) single update step. Then, assuming initial
value function
linear, number linear functions defining ith value function
O(jAjjj 1 ).


4. defining redundant useful linear functions assume linear function duplicates,
i.e. one copy linear function kept set .


41

fiHauskrecht

potential growth size linear function set bad news.
remarked earlier, piecewise linear convex value function usually less complex
worst case many linear functions pruned away updates. However,
turned task identifying useful linear functions computationally
intractable well (Littman, 1996). means one faces potential
super-exponential growth number useful linear functions, also ineciencies
related identification vectors. significant drawback makes
exact methods applicable relatively simple problems.
analysis suggests solving POMDP problem intrinsically hard
task. Indeed, finding optimal solution finite-horizon problem PSPACE-hard
(Papadimitriou & Tsitsiklis, 1987). Finding optimal solution discounted infinitehorizon criterion even harder. corresponding decision problem shown
undecidable (Madani et al., 1999), thus optimal solution may computable.
2.4.4 Structural Refinements Basic Algorithm

standard POMDP model uses state space full transition reward matrices.
However, practice, problems often exhibit structure represented
compactly, example, using graphical models (Pearl, 1988; Lauritzen, 1996), often
dynamic belief networks (Dean & Kanazawa, 1989; Kjaerulff, 1992) dynamic uence
diagrams (Howard & Matheson, 1984; Tatman & Schachter, 1990).5 many ways
take advantage problem structure modify improve exact algorithms.
example, refinement basic Monahan algorithm compact transition reward
models studied Boutilier Poole (1996). hybrid framework combines
MDP-POMDP problem-solving techniques take advantage perfectly partially observable components model subsequent value function decomposition
proposed Hauskrecht (1997, 1998, 2000). similar approach perfect information
region (subset states) containing actual underlying state discussed
Zhang Liu (1997b, 1997a). Finally, Casta~non (1997) Yost (1998) explore techniques
solving large POMDPs consist set smaller, resource-coupled otherwise
independent POMDPs.

2.5 Extracting Control Strategy
Value iteration allow us compute ith approximation value function Vi . However,
ulimate goal find optimal control strategy : ! close approximation.
Thus focus problem extraction control strategies results
value iteration.
2.5.1 Lookahead Design

simplest way define control function : ! value function Vi via
greedy one-step lookahead:
(

(b) = arg max
a2A

X
s2S

(s; a)b(s) +

X
o2

)

P (ojb; a)Vi ( (b; o; a)) :

5. See survey Boutilier, Dean Hanks (1999) different ways represent structured MDPs.
42

fiValue-Function Approximations POMDPs

Vi (b)
a1
a3

a2
a1

0

b

1

b(s1 )

Figure 6: Direct control design. Every linear function defining Vi associated
action. action selected linear function (or Q-function) maximal.
Vi represents ith approximation optimal value function, question
arises good resulting controller really is.6 following theorem (Puterman, 1994;
Williams & Baird, 1994; Littman, 1996) relates accuracy (lookahead) controller
Bellman error.

Theorem 3 Let = kVi Vi 1 k magnitude Bellman error. Let ViLA
expected reward lookahead controller designed Vi . kViLA V k 12 .
bound used construct value-iteration routine yields lookahead
strategy minimum required precision. result also extended kstep lookahead design straightforward way; k steps, error bound becomes
kViLA(k) V k (12 ) .
k

2.5.2 Direct Design

extract control action via lookahead essentially requires computing one full update.
Obviously, lead unwanted delays reaction times. general, speed
response remembering using additional information. particular, every linear
function defining Vi associated choice action (see Equation 7). action
byproduct methods computing linear functions extra computation required
find it. action corresponding best linear function selected directly
belief state. idea illustrated Figure 6.
bound accuracy direct controller infinite-horizon case
derived terms magnitude Bellman error.

Theorem 4 Let = kVi Vi 1 k magnitude Bellman error. Let ViDR
expected reward direct controller designed Vi . kViDR V k 12 .
direct action choice closely related notion action-value function (or
Q-function). Analogously Equation 4, ith Q-function satisfies
Vi (b) = max Qi (b; a);
a2A

6. Note control action extracted via lookahead V optimal (i + 1) steps-to-go
finite-horizon model. main difference V optimal value function steps go.




43

fiHauskrecht

a1

o2
o1

a2

o1 ,

2

o2

o1 ,

a2

o1 , o2

2

o2
a2

a1

o1

o1

a2

o1 , o2
a1

Figure 7: policy graph (finite-state machine) obtained two value iteration steps.
Nodes correspond linear functions (or states finite-state machine)
links dependencies linear functions (transitions states). Every
linear function (node) associated action. ensure policy
also applied infinite-horizon problem, add cycle last state
(dashed line).

Qi (b; a) = R(b; a) +

X
o2

P (ojb; a)Vi 1 ( (b; a; o)):

perspective, direct strategy selects action best (maximum) Qfunction given belief state.7
2.5.3 Finite-State Machine Design

complex refinement technique remember, every linear function
Vi , action choice also choice linear function previous
step observations (see Equation 7). idea applied
recursively linear functions previous steps, obtain relatively complex
dependency structure relating linear functions Vi ; Vi 1 ; V0 , observations actions
represents control strategy (Kaelbling et al., 1999).
see this, model structure graphical terms (Figure 7). different nodes
represent linear functions, actions associated nodes correspond optimizing actions,
links emanating nodes correspond different observations, successor nodes correspond linear functions paired observations. graphs also called policy graphs
(Kaelbling et al., 1999; Littman, 1996; Cassandra, 1998). One interpretation dependency structure represents collection finite-state machines (FSMs) many
possible initial states implement POMDP controller: nodes correspond states
controller, actions controls (outputs), links transitions conditioned inputs
7. Williams Baird (1994) also give results relating accuracy direct Q-function controller
Bellman error Q-functions.

44

fiValue-Function Approximations POMDPs

(observations). start state FSM controller chosen greedily selecting
linear function (controller state) optimizing value initial belief state.
advantage finite-state machine representation strategy
first steps works observations directly; belief-state updates needed.
contrasts two policy models (lookahead direct models), must keep
track current belief state update time order extract appropriate
control. drawback approach FSM controller limited steps
correspond number value iteration steps performed. However, infinitehorizon model controller expected run infinite number steps. One way
remedy deficiency extend FSM structure create cycles let us
visit controller states repeatedly. example, adding cycle transition end state
FSM controller Figure 7 (dashed line) ensures controller also applicable
infinite-horizon problem.

2.6 Policy Iteration
alternative method finding solution discounted infinite-horizon problem
policy iteration (Howard, 1960; Sondik, 1978). Policy iteration searches policy space
gradually improves current control policy one belief states. method
consists two steps performed iteratively:




policy evaluation: computes expected value current policy;
policy improvement: improves current policy.

saw Section 2.5, many ways represent control policy
POMDP. restrict attention finite-state machine model observations
correspond inputs actions outputs (Platzman, 1980; Hansen, 1998b; Kaelbling
et al., 1999).8
2.6.1 Finite-State Machine Controller

finite-state machine (FSM) controller C = (M; ; A; ; ; ) POMDP described
set memory states controller, set observations (inputs) , set
actions (outputs) A, transition function : ! mapping states FSM
next memory states given observation, output function : ! mapping
memory states actions. function : I0 ! selects initial memory state given
initial information state. initial information state corresponds either prior
posterior belief state time t0 depending availability initial observation.
2.6.2 Policy Evaluation

first step policy iteration policy evaluation. important property
FSM model value function specific FSM strategy computed
eciently number controller states . key ecient computability
8. policy-iteration algorithm policies defined regions belief space described
first Sondik (1978).
45

fiHauskrecht

x2
o1
o2

a2

a1

x1

o2


o1
a1

o2

o1

x3

2

a2

o1

x4

Figure 8: example four-state FSM policy. Nodes represent states, links transitions states (conditioned observations). Every memory state
associated control action (output).

fact value function executing FSM strategy memory state x
linear (Platzman, 1980).9

Theorem 5 Let C finite-state machine controller set memory states .
value function applying C memory state x 2 , V C (x; b), linear. Value
functions x 2 found solving system linear equations jS jjM j
variables.
illustrate main idea example. Assume FSM controller four memory
states fx1 ; x2 ; x3 ; x4 g, Figure 8, stochastic process two hidden states =
fs1 ; s2g. value policy augmented state space satisfies system
linear equations
V (x1 ; s1 ) = (s1 ; (x1 )) +
V (x1 ; s2 ) = (s2 ; (x1 )) +
V (x2 ; s1 ) = (s1 ; (x2 )) +



V (x4 ; s2 ) = (s2 ; (x4 )) +

XX

o2 s2S

XX

o2 s2S

XX

o2 s2S

XX
o2 s2S

P (o; sjs1 ; (x1 ))V ((x1 ; o); s)
P (o; sjs2 ; (x1 ))V ((x1 ; o); s)
P (o; sjs1 ; (x2 ))V ((x2 ; o); s)
P (o; sjs2 ; (x4 ))V ((x4 ; o); s);

(x) action executed x (x; o) state one transits
seeing input (observation) o. Assuming start policy memory state x1 ,
value policy is:
X
V C (x1 ; b) = V (x1 ; s)b(s):
s2S
9. idea linearity ecient computability value functions fixed FSM-based strategy
addressed recently different contexts number researchers (Littman, 1996; Cassandra,
1998; Hauskrecht, 1997; Hansen, 1998b; Kaelbling et al., 1999). However, origins idea
traced earlier work Platzman (1980).
46

fiValue-Function Approximations POMDPs

Thus value function linear computed eciently solving system
linear equations.
Since general FSM controller start memory state, always
choose initial memory state greedily, maximizing expected value result.
case optimal choice function defined as:
(b) = arg max V C (x; b);
x2M
value FSM policy C belief state b is:

V C (b) = max V C (x; b) = V C ( (b); b):
x2M

Note resulting value function strategy C piecewise linear convex
represents expected rewards following C . Since strategy perform better
optimal strategy, V C V must hold.
2.6.3 Policy Improvement

policy-iteration method, searching space controllers, starts arbitrary initial policy improves gradually refining finite-state machine (FSM) description.
particular, one keeps modifying structure controller adding removing controller states (memory) transitions. Let C C 0 old new FSM controller.
improvement step must satisfy
0

V C (b) V C (b) b 2 ;

9b 2 V C 0 (b) > V C (b):
guarantee improvement, Hansen (1998a, 1998b) proposed policy-iteration algorithm relies exact value function updates obtain new improved policy structure.10 basic idea improvement based observation one switch
back forth FSM policy description piecewise-linear convex
representation value function. particular:

value function FSM policy piecewise-linear convex every linear
function describing corresponds memory state controller;

individual linear functions comprising new value function update
viewed new memory states FSM policy, described Section 2.5.3.

allows us improve policy adding new memory states corresponding linear
functions new value function obtained exact update. technique
refined removing linear functions (memory states) whenever fully
dominated one linear functions.
10. policy-iteration algorithm exploits exact value function updates works policies defined
belief space used earlier Sondik (1978).

47

fiHauskrecht

b 1,1

o1

o2

a1

b

a2

b 1,2

Figure 9: two-step decision tree. Rectangles correspond decision nodes (moves
decision-maker) circles chance nodes (moves environment).
Black rectangles represent leaves tree. reward specific path
associated every leaf tree. Decision nodes associated
information states obtained following action observation choices along
path root tree. example, b1;1 belief state obtained
performing action a1 initial belief state b observing observation o1 .

2.7 Forward (Decision Tree) Methods
methods discussed far assume prior knowledge initial belief state treat
belief states equally likely. However, initial state known fixed, methods
often modified take advantage fact. example, finite-horizon
problem, finite number belief states reached given initial state.
case often easier enumerate possible histories (sequences actions
observations) represent problem using stochastic decision trees (Raiffa, 1970).
example two-step decision tree shown Figure 9.
algorithm solving stochastic decision tree basically mimics value-function
updates, restricted situations reached initial belief state.
key diculty number possible trajectories grows exponentially
horizon interest.
2.7.1 Combining Dynamic-Programming Decision-Tree Techniques

solve POMDP fixed initial belief state, apply two strategies: one constructs decision tree first solves it, solves problem backward
fashion via dynamic programming. Unfortunately, techniques inecient, one
suffering exponential growth decision tree size, super-exponential
growth value function complexity. However, two techniques combined
48

fiValue-Function Approximations POMDPs

way least partially eliminates disadvantages. idea based fact
two techniques work solution two different sides (one forward
backward) complexity worsens gradually. solution
compute complete kth value function using dynamic programming (value iteration)
cover remaining steps forward decision-tree expansion.
Various modifications idea possible. example, one often replace
exact dynamic programming two ecient approximations providing upper
lower bounds value function. decision tree must expanded
bounds sucient determine optimal action choice. number search
techniques developed AI literature (Korf, 1985) combined branch-and-bound
pruning (Satia & Lave, 1973) applied type problem. Several researchers
experimented solve POMDPs (Washington, 1996; Hauskrecht, 1997;
Hansen, 1998b). methods applicable problem based Monte-Carlo
sampling (Kearns, Mansour, & Ng, 1999; McAllester & Singh, 1999) real-time dynamic
programming (Barto, Bradtke, & Singh, 1995; Dearden & Boutilier, 1997; Bonet & Geffner,
1998).
2.7.2 Classical Planning Framework

POMDP problems fixed initial belief states solutions closely related
work classical planning extensions handle stochastic partially observable
domains, particularly work BURIDAN C-BURIDAN planners (Kushmerick,
Hanks, & Weld, 1995; Draper, Hanks, & Weld, 1994). objective planners
maximize probability reaching goal state. However, task similar
discounted reward task terms complexity, since discounted reward model
converted goal-achievement model introducing absorbing state (Condon,
1992).
3. Heuristic Approximations

key obstacle wider application POMDP framework computational
complexity POMDP problems. particular, finding optimal solution finitehorizon case PSPACE-hard (Papadimitriou & Tsitsiklis, 1987) discounted infinitehorizon case may even computable (Madani et al., 1999). One approach
problems approximate solution -precision. Unfortunately, even
remains intractable general POMDPs cannot approximated eciently (Burago,
Rougemont, & Slissenko, 1996; Lusena, Goldsmith, & Mundhenk, 1998; Madani et al.,
1999). also reason simple problems solved optimally
near-optimally practice.
alleviate complexity problem, research POMDP area focused various
heuristic methods (or approximations without error parameter) ecient.11
Heuristic methods also focus here. Thus, referring approximations, mean
heuristics, unless specifically stated otherwise.
11. quality heuristic approximation tested using Bellman error, requires one exact
update step. However, heuristic methods per se contain precision parameter.

49

fiHauskrecht

many approximation methods combinations divided two often
closely related classes: value-function approximations policy approximations.

3.1 Value-Function Approximations
main idea value-function approximation approach approximate optimal
value function V : ! IR function Vb : ! IR defined information
space. Typically, new function lower complexity (recall optimal nearoptimal value function may consist large set linear functions) easier compute
exact solution. Approximations often formulated dynamic programming
problems expressed terms approximate value-function updates Hb . Thus,
understand differences advantages various approximations exact methods,
often sucient analyze compare update rules.
3.1.1 Value-Function Bounds

Although heuristic approximations guaranteed precision, many cases
able say whether overestimate underestimate optimal value function.
information bounds used multiple ways. example, upper- lowerbounds help narrowing range optimal value function, elimination
suboptimal actions subsequent speed-ups exact methods. Alternatively, one
use knowledge value-function bounds determine accuracy controller
generated based one bounds (see Section 3.1.3). Also, instances, lower
bound alone sucient guarantee control choice always achieves expected
reward least high one given bound (Section 4.7.2).
bound property different methods determined examining updates
bound relations.

Definition 5 (Upper bound). Let H exact value-function mapping Hb apb )(b) (HV )(b) holds
proximation. say Hb upper-bounds H V (HV
every b 2 .
analogous definition constructed lower bound.
3.1.2 Convergence Approximate Value Iteration

Let Hb value-function mapping representing approximate update. approximate value iteration computes ith value function Vbi = Hb Vbi 1 . fixed-point
solution Vc = Hb Vb close approximation would represent intended output
approximation routine. main problem iteration method general
converge unique multiple solutions, diverge, oscillate, depending Hb
initial function Vb0 . Therefore, unique convergence cannot guaranteed arbitrary
mapping Hb convergence specific approximation method must proved.

Definition 6 (Convergence Hb ). value iteration Hb converges value function V0 limn!1(Hb n V0 ) exists.

50

fiValue-Function Approximations POMDPs

Definition 7 (Unique convergence Hb ). value iteration converges uniquely V
every V 2 V , limn!1(Hb n V ) exists pairs V; U 2 V , limn!1(Hb n V ) =
limn!1(Hb n U ).
sucient condition unique convergence show Hb contraction.
contraction bound properties Hb combined, additional conditions,
show convergence iterative approximation method bound. address
issue present theorem comparing fixed-point solutions two value-function mappings.
Theorem 6 Let H1 H2 two value-function mappings defined V1 V2
1. H1 , H2 contractions fixed points V1 , V2 ;
2. V1 2 V2 H2 V1 H1 V1 = V1 ;

3. H2 isotone mapping.
V2 V1 holds.

Note theorem require V1 V2 cover space value
functions. example, V2 cover possible value functions belief-state MDP,
V1 restricted space piecewise linear convex value functions.
gives us exibility design iterative approximation algorithms computing
value-function bounds. analogous theorem also holds lower bound.
3.1.3 Control

approximation value-function available, used generate
control strategy. general, control solutions correspond options presented Section
2.5 include lookahead, direct (Q-function) finite-state machine designs.
drawback control strategies based heuristic approximations
precision guarantee. One way find accuracy strategies one exact
update value function approximation adopt result Theorems 1 3
Bellman error. alternative solution problem bound accuracy
controllers using upper- lower-bound approximations optimal value
function. illustrate approach, present prove (in Appendix) following
theorem relates quality bounds quality lookahead controller.

Theorem 7 Let VbU VbL upper lower bounds optimal value function
discounted infinite-horizon problem. Let = supb jVbU (b) VbL (b)j = kVbU VbL k
maximum bound difference. expected reward lookahead controller Vb LA ,
constructed either VbU VbL , satisfies kVb LA V k (1(2 )) .
3.2 Policy Approximation
alternative value-function approximation policy approximation. shown earlier,
strategy (controller) POMDP represented using finite-state machine (FSM)
model. policy iteration searches space possible policies (FSMs) optimal near-optimal solution. space usually enormous, bottleneck
51

fiHauskrecht

method. Thus, instead searching complete policy space, restrict attention
subspace believe contain optimal solution good approximation. Memoryless policies (Platzman, 1977; White & Scherer, 1994; Littman, 1994; Singh,
Jaakkola, & Jordan, 1994), policies based truncated histories (Platzman, 1977; White &
Scherer, 1994; McCallum, 1995), finite-state controllers fixed number memory
states (Platzman, 1980; Hauskrecht, 1997; Hansen, 1998a, 1998b) examples
policy-space restriction. following consider finite-state machine model
(see Section 2.6.1), quite general; models viewed special cases.
States FSM policy model represent memory controller and, general,
summarize information past activities observations. Thus, best viewed
approximations information states, feature states. transition model
controller () approximates update function information-state MDP
( ) output function FSM () approximates control function () mapping
information states actions. important property model, shown Section
2.6.2, value function fixed controller fixed initial memory state
obtained eciently solving system linear equations (Platzman, 1980).
apply policy approximation approach first need decide (1) restrict
space policies (2) judge policy quality.
restriction frequently used consider controllers fixed number
states, say k. structural restrictions narrowing space policies
restrict either output function (choice actions different controller states),
transitions current next states. general, heuristic domain-related
insight may help selecting right biases.
Two different policies yield value functions better different regions
belief space. Thus, order decide policy best, need define
importance different regions combinations. multiple solutions this.
example, Platzman (1980) considers worst-case measure optimizes worst
(minimal) value initial belief states. Let C space FSM controllers satisfying
given restrictions. quality policy worst case measure is:
max min max V C (x; b):
C 2C b2I x2M
C

Another option consider distribution initial belief states maximize
expectation value function values. However, common objective choose
policy leads best value single initial belief state b0 :
max max V C (x; b0 ):
C 2C x2M
C

Finding optimal policy case reduces combinatorial optimization problem.
Unfortunately, trivial cases, even problem computationally intractable.
example, problem finding optimal policy memoryless case (only current observations considered) NP-hard (Littman, 1994). Thus, various heuristics
typically applied alleviate diculty (Littman, 1994).

52

fiValue-Function Approximations POMDPs

Valuefunction
approximations

Gridbased linear
function methods
Section 4.7

Fully observable MDP
approximations
Section 4.1

Fast informed bound
approximations
Section 4.2

Fixed strategy
approximations
Section 4.4

Curvefitting
approximations
Section 4.6

Gridbased value interpolation
extrapolation methods
Section 4.5

Unobservable MDP
approximations
Section 4.3

Figure 10: Value-function approximation methods.
3.2.1 Randomized Policies

restricting space policies simplify policy optimization problem.
hand, simultaneously give opportunity find best optimal policy, replacing best restricted policy. point, considered deterministic
policies fixed number internal controller states, is, policies deterministic
output transition functions. However, finding best deterministic policy always best option: randomized policies, randomized output transition functions,
usually lead far better performance. application randomized (or stochastic)
policies POMDPs introduced Platzman (1980). Essentially, deterministic
policy represented randomized policy single action transition,
best randomized policy worse best deterministic policy. difference control performance two policies shows often cases number
states controller relatively small compared optimal strategy.
advantage stochastic policies space larger parameters
policy continuous. Therefore problem finding optimal stochastic policy
becomes non-linear optimization problem variety optimization methods
applied solve it. example gradient-based approach (see Meuleau et al., 1999).
4. Value-Function Approximation Methods

section discuss depth value-function approximation methods. focus approximations belief information space.12 survey known techniques,
also include number new methods modifications existing methods. Figure 10
summarizes methods covered. describe methods means update rules
12. Alternative value-function approximations may work complete histories past actions observations. Approximation methods used White Scherer (1994) example.

53

fiHauskrecht

15

16

17

18

19

10

11

12

13

14

5

6

7

8

9

0

1

2

3

4

Moves

Sensors

Figure 11: Test example. maze navigation problem: Maze20.
implement, simplifies analysis theoretical comparison. focus following properties: complexity dynamic-programming (value-iteration) updates;
complexity value functions method uses; ability methods bound
exact update; convergence value iteration approximate update rules;
control performance related controllers. results theoretical analysis illustrated empirically problem agent-navigation domain. addition, use
agent navigation problem illustrate give intuitions characteristics
methods theoretical underpinning. Thus, results generalized
problems used rank different methods.
Agent-Navigation Problem

Maze20 maze-navigation problem 20 states, six actions eight observations.
maze (Figure 11) consists 20 partially connected rooms (states) robot
operates collects rewards. robot move four directions (north, south, east
west) check presence walls using sensors. But, neither \move"
actions sensor inputs perfect, robot end moving unintended
directions. robot moves unintended direction probability 0.3 (0.15
neighboring directions). move wall keeps robot
position. Investigative actions help robot navigate activating sensor inputs. Two
investigative actions allow robot check inputs (presence wall) northsouth east-west directions. Sensor accuracy detecting walls 0.75 two-wall
case (e.g. north south wall), 0.8 one-wall case (north south) 0.89
no-wall case, smaller probabilities wrong perceptions.
control objective maximize expected discounted rewards discount
factor 0.9. small reward given every action leading bumping wall
(4 points move 2 points investigative action), one large reward (150
points) given achieving special target room (indicated circle figure)
recognizing performing one move actions. collecting
reward, robot placed random new start position.
Although Maze20 problem moderate complexity regard size
state, action observation spaces, exact solution beyond reach current
exact methods. exact methods tried problem include Witness algorithm
(Kaelbling et al., 1999), incremental pruning algorithm (Cassandra et al., 1997)13
13. Many thanks Anthony Cassandra running algorithms.
54

fiValue-Function Approximations POMDPs

VMDP

*
VMDP
(s 1 )

*
VMDP
(s 2 )

VQMDP

Q *MDP(s 2 ,a 1 )

Q *MDP(s 1 ,a 1)

Q *MDP(s 2 ,a 2)

Q *MDP(s 1 ,a 2)

*
VPOMDP
0

*
VPOMDP
1

b(s1 )

0

(a)

1

b(s1 )

(b)

Figure 12: Approximations based fully observable version two state POMDP
(with states s1 ; s2 ): (a) MDP approximation; (b) QMDP approximation.
Values extreme points belief space solutions fully observable
MDP.
policy iteration FSM model (Hansen, 1998b). main obstacle preventing
algorithms obtaining optimal close-to-optimal solution complexity
value function (the number linear functions needed describe it) subsequent
running times memory problems.

4.1 Approximations Fully Observable MDP
Perhaps simplest way approximate value function POMDP assume
states process fully observable (Astrom, 1965; Lovejoy, 1993). case
optimal value function V POMDP approximated as:
Vb (b) =

X

s2S

(s);
b(s)VMDP

(8)

(s) optimal value function state fully observable version
VMDP
process. refer approximation MDP approximation. idea
approximation illustrated Figure 12a. resulting value function linear
fully defined values extreme points belief simplex. correspond
optimal values fully observable case. main advantage approximation
fully observable MDP (FOMDP) solved eciently finitehorizon problem discounted infinite-horizon problems.14 update step (fully
observable) MDP is:
8
<

ViMDP
(s; a) +
+1 (s) = max
:

X
s0 2S

9
=

P (s0 js; a)ViMDP (s0 ); :

14. solution finite-state fully observable MDP discounted infinite-horizon criterion
found eciently formulating equivalent linear programming task (Bertsekas, 1995)

55

fiHauskrecht

4.1.1 MDP Approximation

MDP-approximation approach (Equation 8) also described terms valuefunction updates belief-space MDP. Although step strictly speaking redundant
here, simplifies analysis comparison approach approximations.
Let Vbi linear value function described vector ffMDP
corresponding values

ViMDP (s0 ) states s0 2 . (i + 1)th value function Vbi+1

Vbi+1 (b) =

X
s2S

2

b(s) max 4(s; a) +
a2A

= (HMDP Vbi )(b):

X
s0 2S

3

P (s0 js; a)ffMDP
(s0 )5


Vbi+1 described linear function components
MDP
ffMDP
i+1 (s) = Vi+1 (s) = max


(

(s; a) +

X
s2S

)

P (s0 js; a)ffMDP
(s0 ) :


MDP-based rule HMDP also rewritten general form starts
arbitrary piecewise linear convex value function Vi , represented set linear
functions :

Vbi+1 (b) =

X
s2S

8
<

b(s) max :(s; a) +
a2A

X
s0 2S

9
=

P (s0 js; a) max ffi (s0 ); :
ffi 2



application HMDP mapping always leads linear value function.
update easy compute takes O(jAjjS j2 + j jjS j) time. reduces O(jAjjS j2 )
time MDP-based updates strung together. remarked earlier, optimal
solution infinite-horizon, discounted problem solved eciently via linear
programming.
update MDP approximation upper-bounds exact update, is, H Vbi
HMDP Vbi . show property later Theorem 9, covers cases. intuition
cannot get better solution less information, thus fully observable
MDP must upper-bound partially observable case.
4.1.2 Approximation Q-Functions (QMDP)

variant approximation based fully observable MDP uses Q-functions (Littman,
Cassandra, & Kaelbling, 1995):
X
Vb (b) = max b(s)QMDP (s; a);
a2A s2S

X
(s0 )
QMDP (s; a) = (s; a) +
P (s0 js; a)VMDP
s0 2S
optimal action-value function (Q-function) fully observable MDP. QMDP
approximation Vb piecewise linear convex jAj linear functions, corresponding
56

fiValue-Function Approximations POMDPs

one action (Figure 12b). QMDP update rule (for belief state MDP) Vbi
linear functions ffki 2 is:

Vbi+1 (b) = max

X

a2A s2S

2

b(s) 4(s; a) +

= (HQMDP Vbi )(b):

X
s0 2S

3

P (s0 js; a) max ffi (s0 )5
ffi 2



HQMDP generates value function jAj linear functions. time complexity
update MDP-approximation case { O(jAjjS j2 + j jjS j), reduces
O(jAjjS j2 ) time QMDP updates used. HQMDP contraction mapping
fixed-point solution found solving corresponding fully observable MDP.
QMDP update upper-bounds exact update. bound tighter
MDP update; is, H Vbi HQMDP Vbi HMDP Vbi , prove later Theorem 9.
inequalities hold fixed-point solutions (through Theorem 6).
illustrate difference quality bounds MDP approximation
QMDP method, use Maze20 navigation problem. measure quality
bound use mean value-function values. Since belief states equally important
assume uniformly distributed. approximate measure using
average values fixed set N = 2000 belief points. points set
selected uniformly random beginning. set chosen, fixed
remained tests (here later). Figure 13 shows results
experiment; include also results fast informed bound method presented
next section.15 Figure 13 also shows running times methods. methods
implemented Common Lisp run Sun Ultra 1 workstation.
4.1.3 Control

MDP QMDP value-function approximations used construct controllers based one-step lookahead. addition, QMDP approximation also suitable
direct control strategy, selects action corresponding best (highest
value) Q-function. Thus, method special case Q-function approach discussed
Section 3.1.3.16 advantage direct QMDP method faster
lookahead designs. hand, lookahead tends improve control performance.
shown Figure 14, compares control performance different controllers
Maze20 problem.
quality policy b , preference towards particular initial belief state,
measured mean value-function values b uniformly distributed initial
belief states. approximate measure using average discounted rewards

15. confidence interval limits probability level 0.95 range (0:45; 0:62) respective
average scores holds bound experiments paper. relatively small
include graphs.
16. pointed Littman et al. (1995), instances, direct QMDP controller never selects
investigative actions, is, actions try gain information underlying process
state. Note, however, observation true general QMDP-based controller
direct action selection may select investigative actions, even though fully observable version
problem investigative actions never chosen.
57

fiHauskrecht

bound quality
MDP
approximation

QMDP
approximation

running times

fast informed
bound

60

140

50
time [sec]

score

120
100

40
30
20

80
10

60

0
MDP
approximation

40

QMDP
approximation

fast informed
bound

Figure 13: Comparison MDP, QMDP fast informed bound approximations:
bound quality (left); running times (right). bound-quality score
average value approximation set 2000 belief points (chosen uniformly random). methods upper-bound optimal value function,
ip bound-quality graph longer bars indicate better approximations.
2000 control trajectories obtained fixed set N = 2000 initial belief states (selected
uniformly random beginning). trajectories obtained simulation
60 steps long.17
validate comparison along averaged performance scores, must show
scores result randomness methods indeed statistically
significantly different. rely pairwise significance tests.18 summarize
obtained results, score differences 1.54, 2.09 2.86 two methods (here
also later paper) sucient reject method lower score
better performer significance levels 0.05, 0.01 0.001 respectively.19 Error-bars
Figure 14 ect critical score difference significance level 0.05.
Figure 14 also shows average reaction times different controllers
experiments. results show clear dominance direct QMDP controller,
need lookahead order extract action, compared two MDPbased controllers.

4.2 Fast Informed Bound Method
MDP QMDP approaches ignore partial observability use fully
observable MDP surrogate. improve approximations account (at least
17. length trajectories (60 steps) Maze20 problem chosen ensure estimates
(discounted) cumulative rewards far actual rewards infinite number steps.
18. alternative way compare two methods compute confidence limits scores inspect
overlaps. However, case, ability distinguish two methods reduced due
uctuations scores different initializations. Maze20, confidence interval limits probability
level 0.95 range (1:8; 2:3) respective average scores. covers control experiments
later. Pairwise tests eliminate dependency examining differences individual values
thus improve discriminative power.
19. critical score differences listed cover worst case combination. Thus, may pairs
smaller difference would suce.
58

fiValue-Function Approximations POMDPs

reaction times

control performance
0.025

70

lookahead
0.02

lookahead

lookahead

time [sec]

score

60
50

direct
40

lookahead

direct

0.015
0.01

30

0.005

20

0
MDP
approximation

direct

fast informed
bound

QMDP
approximation

lookahead

lookahead

MDP
approximation

direct

QMDP
approximation

fast informed
bound

Figure 14: Comparison control performance MDP, QMDP fast informed bound
methods: quality control (left); reaction times (right). quality-of-control
score average discounted rewards 2000 control trajectories obtained
fixed set 2000 initial belief states (selected uniformly random).
Error-bars show critical score difference value (1.54) two methods become statistically different significance level 0.05.
degree) partial observability propose new method { fast informed bound
method. Let Vbi piecewise linear convex value function represented set linear
functions . new update defined
8
<X

XX

9
=

X

Vbi+1 (b) = max : (s; a)b(s) +
max
P (s0 ; ojs; a)b(s)ffi (s0 );
a2A s2S
ff
2
0
o2 s2S
2S
8
9


2

<X



X

X

= max : b(s) 4(s; a) +
max
a2A s2S
o2 ff 2 s0 2S
= (HF IB Vbi )(b):




3
=
P (s0 ; ojs; a)ffi (s0 )5;

fast informed bound update obtained exact update following
derivation:
8
<X

X

XX

9
=

X

9
=

(H Vbi )(b) = max : (s; a)b(s) +
max
P (s0 ; ojs; a)b(s)ffi (s0 );
a2A s2S
ff
2
o2
s0 2S s2S


8
<X

max
(s; a)b(s) +
a2A :
s2S

X

2

XX



max

o2 s2S ffi 2 s0 2S

X

X

P (s0 ; ojs; a)b(s)ffi (s0 );
3

= max b(s) 4(s; a) +
max
P (s0 ; ojs; a)ffi (s0 )5
a2A s2S
ff
2
o2
s0 2S


X



= max b(s)ffai+1 (s)
a2A s2S
= (HF IB Vbi )(b):

value function Vbi+1 = HF IB Vbi one obtains update piecewise linear
convex consists jAj different linear functions, corresponding one
59

fiHauskrecht

action

ffai+1 (s) = (s; a) +

X

X
max
P (s0 ; ojs; a)ffi (s0 ):
ff
2
o2
s0 2S




HF IB update ecient computed O(jAjjS j2 jjj j) time. method
always outputs jAj linear functions, computation done O(jAj2 jS j2 jj) time,
many HF IB updates strung together. significant complexity reduction
compared exact approach: latter lead function consisting jAjj jjj
linear functions, exponential number observations worst case
takes O(jAjjS j2 j jjj) time.
HF IB updates polynomial complexity one find approximation
finite-horizon case eciently. open issue remains problem finding solution
infinite-horizon discounted case complexity. address establish
following theorem.

Theorem 8 solution fast informed bound approximation found solving
MDP jS jjAjjj states, jAj actions discount factor .
full proof theorem deferred Appendix. key part proof
construction equivalent MDP jS jjAjjj states representing HF IB updates.
Since finite-state MDP solved linear program conversion, fixed-point
solution fast informed bound update computable eciently.
4.2.1 Fast Informed Bound versus Fully-Observable MDP Approximations

fast informed update upper-bounds exact update tighter MDP
QMDP approximation updates.

Theorem 9 Let Vbi corresponds piecewise linear convex value function defined
linear functions. H Vbi HF IB Vbi HQMDP Vbi HMDP Vbi :
key trick deriving result swap max sum operators (the
proof Appendix) thus obtain upper-bound inequalities
subsequent reduction complexity update rules compared exact update.
also shown Figure 15. UMDP approximation, also included Figure 15,
discussed later Section 4.3. Thus, difference among methods boils
simple mathematical manipulations. Note inequality relations derived
updates hold also fixed-point solutions (through Theorem 6).
Figure 13a illustrates improvement bound MDP-based approximations
Maze20 problem. Note, however, improvement paid increased
running-time complexity (Figure 13b).
4.2.2 Control

fast informed bound always outputs piecewise linear convex function, one
linear function per action. allows us build POMDP controller selects action
associated best (highest value) linear function directly. Figure 14 compares
control performance direct lookahead controllers MDP QMDP
controllers. see fast informed bound leads tighter bounds also
60

fiValue-Function Approximations POMDPs

UMDP update:


V + 1 ( b ) = ax b ( ) ( , ) + ax
aA






exact update:


V + 1 ( b ) = ax b ( ) ( , ) +
aA


P ( ' | , )b ( )

'

ax

'



fast informed bound update:



V + 1 ( b ) = ax b ( ) ( , ) +
aA



ax




P ( ' | , ) ax ( ' )







MDP approx. update:


V + 1 ( b ) = b ( ) ax ( , ) +
aA




P ( ' , | , ) ( ' )



'

'


( s' )



P ( ' , | , )b ( ) ( ' )




QMDP approx. update:


V + 1 ( b ) = ax b ( ) ( , ) +











'S


P ( ' | , ) ax ( ' )



Figure 15: Relations exact update UMDP, fast informed bound,
QMDP MDP updates.
improved control average. However, stress currently theoretical
underpinning observation thus may true belief states
problem.
4.2.3 Extensions Fast Informed Bound Method

main idea fast informed bound method select best linear function
every observation every current state separately. differs exact update
seek linear function gives best result every observation
combination states. However, observe great deal middle ground
two extremes. Indeed, one design update rule chooses optimal
(maximal) linear functions disjoint sets states separately. illustrate idea,
assume partitioning = fS1 ; S2 ; ; Sm g state space . new update is:

Vbi+1 (b) = max
a2A

(

X
s2S

(s; a)b(s) +

X

2
4 max

X X

o2 ffi 2 s2S1 s0 2S

P (s0 ; ojs; a)b(s)ffi (s0 )+

X X
max
P (s0 ; ojs; a)b(s)ffi (s0 ) + +
ff 2 s2S s0 2S
2




max

X X

ffi 2 s2S s0 2S


39
=
P (s0 ; ojs; a)b(s)ffi (s0 )5;

easy see update upper-bounds exact update. Exploration
approach various partitioning heuristics remains interesting open research issue.
61

fiHauskrecht

4.3 Approximation Unobservable MDP
MDP-approximation assumes full observability POMDP states obtain simpler
ecient updates. extreme discard observations available
decision maker. MDP observations called unobservable MDP (UMDP)
one may choose value-function solution alternative approximation.
find solution unobservable MDP, derive corresponding update
rule, HUMDP , similarly update partially observable case. HUMDP preserves
piecewise linearity convexity value function contraction. update
equals:
8
<X

9
=

XX

Vbi+1 (b) = max : (s; a)b(s) + max
P (s0 js; a)b(s)ffi (s0 );
a2A s2S
ff 2 s2S s0 2S
= (HUMDP Vbi )(b);




set linear functions describing Vbi . Vbi+1 remains piecewise linear convex
consists j jjAj linear functions. contrast exact update,
number possible vectors next step grow exponentially number
observations leads jAjj jjj possible vectors. time complexity update
O(jAjjS j2 j j). Thus, starting Vb0 one linear function, running-time complexity
k updates bounded O(jAjk jS j2 ). problem finding optimal solution
unobservable MDP remains intractable: finite-horizon case NP-hard(Burago et al.,
1996), discounted infinite-horizon case undecidable (Madani et al., 1999). Thus,
usually useful approximation.
update HUMDP lower-bounds exact update, intuitive result ecting
fact one cannot better less information. provide insight
two updates related, following derivation, also proves bound
property elegant way:
8
<X

X

9
=

XX

(H Vbi )(b) = max : (s; a)b(s) +
max
P (s0 ; ojs; a)b(s)ffi (s0 );
a2A s2S
ff
2
o2
s0 2S s2S
8
9


<X

(s; a)b(s) + max
max
a2A :
ff2
s2S

8
<X







XXX

o2 s2S s0 2S

XX

=

P (s0 ; ojs; a)b(s)ffi (s0 );
9
=

= max : (s; a)b(s) + max
P (s0 js; a)b(s)ffi (s0 );
a2A s2S
ff 2 s2S s0 2S
= (HUMDP Vbi )(b):




see difference exact UMDP updates max
sum next-step observations exchanged. causes choice ff vectors
HUMDP become independent observations. sum max operations
exchanged, observations marginalized out. Recall idea swaps leads
number approximation updates; see Figure 15 summary.
62

fiValue-Function Approximations POMDPs

4.4 Fixed-Strategy Approximations
finite-state machine (FSM) model used primarily define control strategy.
strategy require belief state updates since directly maps sequences observations
sequences actions. value function FSM strategy piecewise linear convex
found eciently number memory states (Section 2.6.1).
policy iteration policy approximation contexts value function specific strategy
used quantify goodness policy first place, value function alone
also used substitute optimal value function. case, value function
(defined belief space) equals
V C (b) = max V C (x; b);
x2M

P

V C (x; b) = s2S V C (x; s)b(s) obtained solving set jS jjM j linear equations
(Section 2.6.2). remarked earlier, value fixed strategy lower-bounds
optimal value function, V C V .
simplify comparison fixed-strategy approximation approximations,
rewrite solution also terms fixed-strategy updates
8
<X

9
=

XXX

Vbi+1 (b) = max : (s; (x))b(s) +
P (o; s0 js; (x))b(s)ffi ((x; o); s0 ); ;
x2M s2S
o2 s2S s0 2S
8
9
<X

2

XX

3

=

= max : b(s) 4(s; (x)) +
P (o; s0 js; (x))ffi ((x; o); s0 )5;
x2M s2S
o2 s0 2S
= (HF SM Vbi )(b):

value function Vbi piecewise linear convex consists jM j linear functions
ffi (x; :). infinite-horizon discounted case ffi (x; s) represents ith approximation
V C (x; s). Note update applied finite-horizon case straightforward
way.
4.4.1 Quality Control

Assume FSM strategy would like use substitute optimal
control policy. three different ways use extract control.
first simply execute strategy represented FSM. need
update belief states case. second possibility choose linear functions
corresponding different memory states associated actions repeatedly every
step. refer controller direct (DR) controller. approach requires
updating belief states every step. hand control performance
worse FSM control. final strategy discards information
actions extracts policy using value function Vb (b) one-step lookahead.
method (LA) requires belief state updates lookaheads leads worst
reactive time. Like DR, however, strategy guaranteed worse FSM
controller. following theorem relates performances three controllers.
63

fiHauskrecht

control performance

reaction times

70

0.025
60

50

time [sec]

score

0.02

40

0.015
0.01

30

0.005

20

0

0.0001
DR controller

FSM controller

FSM controller

LA controller

DR controller

LA controller

Figure 16: Comparison three different controllers (FSM, DR LA) Maze20
problem collection one-action policies: control quality (left) response time (right). Error-bars control performance graph indicate
critical score difference two methods become statistically different
significance level 0.05.

Theorem 10 Let CF SM FSM controller. Let CDR CLA direct
one-step-lookahead controllers constructed based CF SM . V C (b) V C (b)
V C (b) V C (b) hold belief states b 2 .
Though prove direct controller lookahead controller
always better underlying FSM controller (see Appendix full proof
theorem), cannot show similar property first two controllers initial
belief states. However, lookahead approach typically tends dominate, ecting
usual trade-off control quality response time. illustrate trade-off
running Maze20 example collection jAj one-action policies, generating
sequence action. Control quality response time results shown Figure
16. see controller based FSM fastest three, also
worst terms control quality. hand, direct controller slower (it needs
update belief states every step) delivers better control. Finally, lookahead
controller slowest best control performance.
F SM

F SM

DR

LA

4.4.2 Selecting FSM Model

quality fixed-strategy approximation depends strongly FSM model used.
model provided priori constructed automatically. Techniques automatic
construction FSM policies correspond search problem either complete
restricted space policies examined find optimal near-optimal policy
space. search process equivalent policy approximations policy-iteration
techniques discussed earlier Sections 2.6 3.2.

64

fiValue-Function Approximations POMDPs

4.5 Grid-Based Approximations Value Interpolation-Extrapolation
value function continuous belief space approximated finite set grid
points G interpolation-extrapolation rule estimates value arbitrary
point belief space relying points grid associated values.
Definition 8 (Interpolation-extrapolation rule) Let f : ! IR real-valued function
defined information space , G = fbG1 ; bG2 ; bGk g set grid points G =
f(bG1 ; f (bG1 )); (bG2 ; f (bG2 )); ; (bGk ; f (bGk))g set point-value pairs. function RG :
(I IR)jGj ! IR estimates f point information space using
values associated grid points called interpolation-extrapolation rule.
main advantage interpolation-extrapolation model estimating true value
function requires us compute value updates finite set grid points
G. Let Vbi approximation ith value function. approximation
(i + 1)th value function Vbi+1 obtained
Vbi+1 (b) = RG (b; Gi+1 );
values associated every grid point bGj 2 G (and included Gi+1 ) are:

'i+1 (bGj ) = (H Vbi )(bGj ) = max
a2A

(

(b; a) +

X
2

P (ojb; a)Vbi ( (bGj ; o; a))

)

:

(9)

grid-based update also described terms value-function mapping HG :
Vbi+1 = HG Vbi . complexity update O(jGjjAjjS j2 jjCEval (RG ; jGj))
CEval (RG ; jGj) computational cost evaluating interpolation-extrapolation rule
RG jGj grid points. show later (Section 4.5.3), instances, need
evaluate interpolation-extrapolation rule every step eliminated.
4.5.1 Family Convex Rules

number possible interpolation-extrapolation rules enormous. focus
set convex rules relatively small important subset interpolationextrapolation rules.20

Definition 9 (Convex rule) Let f function defined space , G = fbG1 ; bG2 ; bGk g
set grid points, G = f(bG1 ; f (bG1 )); (bG2 ; f (bG2 )); ; (bGk ; f (bGk ))g set pointvalue pairs. rule RG estimating f using G called convex every b 2 ,
value fb(b) is:
fb(b) = RG (b; G ) =
0 bj 1 every j = 1; ; jGj,

jGj
X
bj f (bj );

j =1

PjGj

b
j =1 j

= 1.

20. note convex rules used work special case averagers introduced Gordon (1995).
difference minor; definition averager includes constant (independent grid points
values) added convex combination.
65

fiHauskrecht

key property convex rules corresponding grid-based update HG
contraction max norm (Gordon, 1995). Thus, approximate value iteration based
HG converges unique fixed-point solution. addition, HG based convex rules
isotone.
4.5.2 Examples Convex Rules

family convex rules includes approaches commonly used practice,
like nearest neighbor, kernel regression, linear point interpolations many others.
Take, example, nearest-neighbor approach. function belief point b
estimated using value grid point closest terms distance metric
defined belief space. Then, point b, exactly one nonzero parameter
bj = 1 k b bGj kM k b bGi kM holds = 1; 2; ; k.
zero. Assuming Euclidean distance metric, nearest-neighbor approach leads
piecewise constant approximation, regions equal values correspond regions
common nearest grid point.
nearest neighbor estimates function value taking account one
grid point value. Kernel regression expands upon using grid points.
adds weights contributions (values) according distance target
point. example, assuming Gaussian kernels, weight grid point bGj

bj = fi exp kb

bG
k2M =22 ;
j

P
fi normalizing constant ensuring jjG=1j bj = 1 parameter
attens narrows weight functions. Euclidean metric, kernel-regression
rule leads smooth approximation function.
Linear point interpolations subclass convex rules addition constraints
Definition 9 satisfy
jGj
X
b = bj bGj :
j =1

is, belief point b convex combination grid points corresponding coecients. optimal value function POMDP convex,
new constraint sucient prove upper-bound property approximation.
general, many different linear point-interpolations given grid. challenging problem find rule best approximation. discuss issues
Section 4.5.7.
4.5.3 Conversion Grid-Based MDP

Assume would like find approximation value function using gridbased convex rule grid-based update (Equation 9). view process also
process finding sequence values '1 (bGj ); '2 (bGj ); ; 'i (bGj ); grid-points
bGj 2 G. show instances sequence values computed without
applying interpolation-extrapolation rule every step. cases, problem
66

fiValue-Function Approximations POMDPs

converted fully observable MDP states corresponding grid-points G.21
call MDP grid-based MDP.

Theorem 11 Let G finite set grid points RG convex rule parameters bj fixed. values '(bGj ) bGj 2 G found solving fully
observable MDP jGj states discount factor .
Proof grid point bGj write:
(

'i+1 (bGj ) = max (bGj ; a) +
a2A

8
<

X
o2

X

P (ojbGj ; a)VbiG ( (bGj ; a; o))

)

39
jGj
=
X
G )5
P (ojbGj ; a) 4 o;a
'
(
b
j;k k ;
2

= max :(bGj ; a) +
a2A
o2
k=1
8
"
#9
jGj
<h
=

X
X
= max : (bGj ; a) + 'Gi (bGk )
P (ojbGj ; a)o;a
j;k ;
a2A
k=1
o2
P

G G
denoting [ o2 P (ojbj ; a)G o;a
j;k ] P (bk jbj ; a), construct fully observable
MDP problem states corresponding grid points G discount factor .
update step equals:

8
<

'i+1 (bGj ) = max :(bGj ; a) +
a2A

jGj
X
k=1

9
=

P (bGk jbGj ; a)'Gi (bGk ); :

P
prerequisite 0 bj 1 every j = 1; ; jGj jjG=1j bj = 1 guarantees
P (bGk jbGj ; a) interpreted true probabilities. Thus, one compute values '(bGj )
solving equivalent fully-observable MDP. 2
4.5.4 Solving Grid-Based Approximations

idea converting grid-based approximation grid-based MDP basis
simple powerful approximation algorithm. Brie y, key find
parameters (transition probabilities rewards) new MDP model solve
it. process relatively easy parameters used interpolate-extrapolate
value non-grid point fixed (the assumption Theorem 11). case,
determine parameters new MDP eciently one step, grid set G.
nearest neighbor kernel regression examples rules property. Note
leads polynomial-time algorithms finding values grid points (recall
MDP solved eciently finite discounted, infinite-horizon criteria).
problem solving grid-based approximation arises parameters
used interpolation-extrapolation fixed subject optimization
itself. happens, example, multiple ways interpolating value
21. note similar result also proved independently Gordon (1995).
67

fiHauskrecht

point belief space would like find best interpolation (leading
best values) grid points G. case, corresponding \optimal"
grid-based MDP cannot found single step iterative approximation, solving
sequence grid-based MDPs, usually needed. worst-case complexity problem
remains open question.
4.5.5 Constructing Grids

issue touched far selection grids. multiple ways
select grids. divide two classes { regular non-regular grids.
Regular grids (Lovejoy, 1991a) partition belief space evenly equal-size regions.22
main advantage regular grids simplicity locate grid points
neighborhood belief point. disadvantage regular grids
restricted specific number points, increase grid resolution paid
exponential increase grid size. example, sequence regular grids
20-dimensional belief space (corresponds POMDP 20 states) consists 20, 210,
1540, 8855, 42504, grid points.23 prevents one using method higher
grid resolutions problems larger state spaces.
Non-regular grids unrestricted thus provide exibility grid resolution must increased adaptively. hand, due irregularities, methods
locating grid points adjacent arbitrary belief point usually complex
compared regular grids.
4.5.6 Linear Point Interpolation

fact optimal value function V convex belief-state MDPs used
show approximation based linear point interpolation always upper-bounds
exact solution (Lovejoy, 1991a, 1993). Neither kernel regression nearest neighbor
guarantee us bound.

Theorem 12 (Upper bound property grid-based point interpolation update). Let Vbi
convex value function. H Vbi HG Vbi .
upper-bound property HG update convex value functions follows directly
Jensen's inequality. convergence upper-bound follows Theorem 6.
Note point-interpolation update imposes additional constraint choice
grid points. particular, easy see valid grid must also include extreme points belief simplex (extreme points correspond (1; 0; 0; ); (0; 1; 0; ),
22. Regular grids used Lovejoy (1991a) based Freudenthal triangulation (Eaves, 1984). Essentially, idea used partition evenly n-dimensional subspace IR . fact,
ane transform allows us map isomorphically grid points belief space grid points
n-dimensional space (Lovejoy, 1991a).
23. number points regular grid sequence given (Lovejoy, 1991a):
n

+ jS j 1)!
;
jGj = (M
!(jS j 1)!

= 1; 2; grid refinement parameter.

68

fiValue-Function Approximations POMDPs

etc.). Without extreme points one would unable cover whole belief space via
interpolation. Nearest neighbor kernel regression impose restrictions grid.
4.5.7 Finding best interpolation

general, multiple ways interpolate point belief space. objective
find best interpolation, is, one leads tightest upper bound
optimal value function.
Let b belief point f(bj ; f (bj ))jbj 2 Gg set grid-value pairs. best
interpolation point b is:
jGj
X
fb(b) = min j f (bj )
j =1
P
= 1; ; jGj, jjG=1j j

P
subject 0 j 1 j
= 1, b = jjG=1j j bGj .
linear optimization problem. Although solved polynomial time
(using linear programming techniques), computational cost still relatively
large, especially considering fact optimization must repeated many times.
alleviate problem seek ecient ways finding interpolation, sacrificing
optimality.
One way find (suboptimal) interpolation quickly apply regular grids proposed
Lovejoy (1991a). case value belief point approximated using
convex combination grid points closest it. approximation leads piecewise linear
convex value functions. interpolations fixed here, problem finding
approximation converted equivalent grid-based MDP solved
finite-state MDP. However, pointed previous section, regular grids must use
specific number grid points increase resolution grid paid
exponential increase grid size. feature makes method less attractive
problem large state space need achieve high grid resolution.24
present work focus non-regular (or arbitrary) grids. propose interpolation approach searches limited space interpolations guaranteed run
time linear size grid. idea approach interpolate point
b belief space dimension jS j set grid points consists arbitrary
grid point b0 2 G jS j 1 extreme points belief simplex. coecients
interpolation found eciently search best interpolation. Let
b0 2 G grid point defining one interpolation. value point b satisfies
0

bb
Vbi (b) = min
0 Vi (b);
b 2G

Vbib0 value interpolation grid point b0 . Figure 17 illustrates
resulting approximation. function characterized \sawtooth" shape,
uenced choice interpolating set.
find best value-function solution close approximation apply value
iteration procedure search best interpolation every update step.
24. One solution problem may use adaptive regular grids grid resolution increased
parts belief space. leave idea future work.
69

fiHauskrecht

V(b)

V(b)
*

V(b)

0 b
0

b1

b2

b3

b 4 1 b(s )
1

Figure 17: Value-function approximation based linear-time interpolation approach
(a two-dimensional case). Interpolating sets restricted single internal
point belief space.
drawback approach interpolations may remain unchanged many
update steps, thus slowing solution process. alternative approach solve
sequence grid-based MDPs instead. particular, every stage find best
(minimum value) interpolations belief points reachable grid points one step, fix
coecients interpolations (s), construct grid-based MDP solve (exactly
approximately). process repeated improvement (or improvement
larger threshold) seen values different grid points.
4.5.8 Improving Grids Adaptively

quality approximation (bound) depends strongly points used grid.
objective provide good approximation smallest possible set grid
points. However, task impossible achieve, since cannot known advance
(before solving) belief points pick. way address problem build grids
incrementally, starting small set grid points adding others adaptively,
places greater chance improvement. key part approach
heuristic choosing grid points added next.
One heuristic method developed attempts maximize improvements bound
values via stochastic simulations. method builds fact every interpolation
grid must also include extreme points (otherwise cannot cover entire belief space).
extreme points values affect grid points, try improve
values first place. general, value grid point b improves
precise values used successor belief points, is, belief states correspond
(b; ; o) choice observation o. current optimal action choice b.
Incorporating points grid makes larger improvement value
initial grid point b likely. Assuming initial point extreme point,
heuristic tends improve value point. Naturally, one proceed
selection incorporating successor points first-level successors
grid well, forth.
70

fiValue-Function Approximations POMDPs

generate new grid points (G; Vb G )
set Gnew = fg
extreme points b
repeat b 2= G [ Gnew
n

P
set = arg maxa (b; a) + o2 P (ojb; a)Vb G ( (b; a; o))
select observation according P (ojb; )
update b = (b; ; o)
add b Gnew
return Gnew
Figure 18: Procedure generating additional grid points based bound improvement heuristic.

bound quality
MDP

fast interpolation
QMDP
informed regular grid

interpolation
adaptive grid

interpolation
random grid

140
40 80 120 160 200 240 280 320 360 400

120

score

40
100

80

80

120 160 200

240 280 320 360 400

60

40

Figure 19: Improvement upper bound quality grid-based point-interpolations
based adaptive-grid method. method compared randomly
refined grid regular grid 210 points. upper-bound approximations (the MDP, QMDP fast informed bound methods) included
comparison.
capture idea, generate new grid points via simulation, starting one
extremes belief simplex continuing belief point currently
grid reached. algorithm implements bound improvement heuristic
expands current grid G set jS j new grid points relying current
value-function approximation Vb G shown Figure 18.
Figure 19 illustrates performance (bound quality) adaptive grid method
Maze20 problem. use combination adaptive grids linear-time
interpolation approach. method gradually expands grid 40 point increments
400 grid points. Figure 19 also shows performance random-grid method
71

fiHauskrecht

running times
5000
4500
4000
400

time [sec]

3500
3000

360

2500
320

400

2000
360

280

1500

280 320

240

1000

240

200

500
1.26

1.26

50.02

40 80

120

200

160
120
40 80

160

0
MDP QMDP fast
informed

interpolation
regular grid

interpolation
adaptive grid

interpolation
random grid

Figure 20: Running times grid-based point-interpolation methods. Methods tested include adaptive grid, random grid, regular grid 210 grid
points. Running times adaptive-grid cumulative, ecting dependencies higher grid resolutions lower-level resolutions. running
time results MDP, QMDP, fast informed bound approximations
shown comparison.
new points grid selected iniformly random (results 40 grid point increments
shown). addition, figure gives results regular grid interpolation (based
Lovejoy (1991a)) 210 belief points upper-bound methods: MDP,
QMDP fast informed bound approximations.
see dramatic improvement quality bound adaptive method.
contrast this, uniformly sampled grid (random-grid approach) hardly changes
bound. two reasons this: (1) uniformly sampled grid points likely
concentrated center belief simplex; (2) transition matrix Maze20
problem relatively sparse, belief points one obtains extreme points one
step boundary simplex. Since grid points center simplex
never used interpolate belief states reachable extremes one step cannot
improve values extremes bound change.
One drawback adaptive method running time (for every grid size need
solve sequence grid-based MDPs). Figure 20 compares running times different
methods Maze20 problem. grid-expansion adaptive method depends
value function obtained previous steps, plot cumulative running times.
see relatively large increase running time, especially larger grid sizes, ecting
trade-off bound quality running time. However, note
adaptive-grid method performs quite well initial steps, 80 grid
points outperforms regular grid (with 210 points) bound quality.
Finally, note heuristic approaches constructing adaptive grids point
interpolation possible. example, different approach refines grid ex-

72

fiValue-Function Approximations POMDPs

control performance
70

score

60

400

50

200
40 200 400

40
40

200
40

400
200

400

40

30

20

fast
interpolation interpolation
MDP QMDP informed (regular grid) (adaptive grid)

interpolation
(random grid)

nearest-neighbor nearest-neighbor
(adaptive grid)
(random grid)

Figure 21: Control performance lookahead controllers based grid-based point interpolation nearest neighbor methods varying grid sizes. results
compared MDP, QMDP fast informed bound controllers.
amining differences values current grid points recently proposed Brafman
(1997).
4.5.9 Control

Value functions obtained different grid-based methods define variety controllers. Figure 21 compares performances lookahead controllers based point-interpolation
nearest-neighbor methods. run two versions approaches, one adaptive grid, random grid, show results obtained 40, 200 400
grid points. addition, compare performances interpolation regular
grids (with 210 grid points), MDP, QMDP fast informed bound approaches.
Overall, performance interpolation-extrapolation techniques tested
Maze20 problem bit disappointing. particular, better scores achieved
simpler QMDP fast informed bound methods. see that, although heuristics
improved bound quality approximations, lead similar improvement
QMDP fast informed bound methods terms control. result
shows bad bound (in terms absolute values) always imply bad control
performance. main reason control performance uenced mostly
relative rather absolute value-function values (or, words, shape
function). interpolation-extrapolation techniques use (except regular grid
interpolation) approximate value function functions piecewise linear
convex; interpolations based linear-time interpolation technique
sawtooth-shaped function, nearest-neighbor leads piecewise-constant function.
allow match shape optimal function correctly.
factor affects performance large sensitivity methods selection grid
points, documented, example, comparison heuristic random grids.

73

fiHauskrecht

tests focused lookahead controllers only. However, alternative way
define controller grid-based interpolation-extrapolation methods use Q-function
approximations instead value functions, either direct lookahead designs.25 Qfunction approximations found solving grid-based MDP, keeping
values (functions) different actions separate end.

4.6 Approximations Value Functions Using Curve Fitting (Least-Squares
Fit)
alternative way approximate function continuous space use curve-fitting
techniques. approach relies predefined parametric model value function
set values associated finite set (grid) belief points G. approach
similar interpolation-extrapolation techniques relies set belief-value
pairs. difference curve fitting, instead remembering belief-value pairs,
tries summarize terms given parametric function model. strategy seeks
best possible match model parameters observed point values. best
match defined using various criteria, often least-squares fit criterion,
objective minimize
Error(f ) =

1X
[y
2 j j

f (bj )]2 :

bj yj correspond belief point associated value. index j ranges
points sample set G.
4.6.1 Combining Dynamic Programming Least-Squares Fit

least-squares approximation function used construct dynamic-programming
algorithm update step: Vbi+1 = HLSF Vbi . approach two steps. First,
obtain new values set sample points G:

'i+1 (b) = (H Vbi )(b) = max
a2A

(

X
s2S

(s; a)b(s) +

XX
o2 s2S

)
b
P (ojs; a)b(s)Vi ( (b; a; o)) :

Second, fit parameters value-function model Vbi+1 using new sample-value pairs
square-error cost function. complexity update O(jGjjAjjS j2 jjCEval (Vbi )+
CFit (Vbi+1 ; jGj)) time, CEval(Vbi ) computational cost evaluating Vbi
CFit (Vbi+1 ; jGj) cost fitting parameters Vbi+1 jGj belief-value pairs.
advantage approximation based least-squares fit requires us
compute updates finite set belief states. drawback approach
that, combined value-iteration method, lead instability and/or
divergence. shown MDPs several researchers (Bertsekas, 1994; Boyan
& Moore, 1995; Baird, 1995; Tsitsiklis & Roy, 1996).
25. similar QMDP method, allows lookahead greedy designs. fact, QMDP
viewed special case grid-based method Q-function approximations, grid
points correspond extremes belief simplex.
74

fiValue-Function Approximations POMDPs

4.6.2 On-line Version Least-Squares Fit

problem finding set parameters best fit solved available
optimization procedure. includes on-line (or instance-based) version gradient
descent method, corresponds well-known delta rule (Rumelhart, Hinton, &
Williams, 1986).
Let f denote parametric value function belief space adjustable weights
w = fw1 ; w2 ; ; wk g. on-line update weight wi computed as:

wi

wi ffi (f (bj ) yj )

@f
j ;
@wi b
j

ffi learning constant, bj yj last-seen point value. Note
gradient descent method requires function differentiable regard
adjustable weights.
solve discounted infinite-horizon problem, stochastic (on-line) version
least-squares fit combined either parallel (synchronous) incremental (GaussSeidel) point updates. first case, value function previous step fixed
new value function computed scratch using set belief point samples
values computed one-step expansion. parameters stabilized (by
attenuating learning rates), newly acquired function fixed, process proceeds
another iteration. incremental version, single value-function model
time updated used compute new values sampled points. Littman et al. (1995)
Parr Russell (1995) implement approach using asynchronous reinforcement
learning backups sample points updated next obtained via stochastic
simulation. stress versions subject threat instability divergence,
remarked above.
4.6.3 Parametric Function Models

apply least-squares approach must first select appropriate value function
model. Examples simple convex functions linear quadratic functions,
complex models possible well.
One interesting relatively simple approach based least-squares approximation linear action-value functions (Q-functions) (Littman et al., 1995).
value function Vbi+1 approximated piecewise linear convex combination Qb i+1
functions:
Vbi+1 (b) = max Qb i+1 (b; a);
a2A
Qb i+1 (b; a) least-squares fit linear function set sample points G.
Values points G obtained

'ai+1 (b) = (b; a) +

X

o2

P (ojb; a)Vbi ( (b; o; a)):

method leads approximation jAj linear functions coecients
functions found eciently solving set linear equations. Recall two
approximations (the QMDP fast informed bound approximations) also work
75

fiHauskrecht

jAj linear functions. main differences methods QMDP

fast informed bound methods update linear functions directly, guarantee upper
bounds unique convergence.
sophisticated parametric model convex function softmax model (Parr
& Russell, 1995):
2

Vb (b) = 4

"
X X

ff2

s2S

#k 3 1
ff(s)b(s) 5

k

;

set linear functions ff adaptive parameters fit k \temperature" parameter provides better fit underlying piecewise linear convex function
larger values. function represents soft approximation piecewise linear convex
function, parameter k smoothing approximation.
4.6.4 Control

tested control performance least-squares approach linear Q-function
model (Littman et al., 1995) softmax model (Parr & Russell, 1995). softmax
model varied number linear functions, trying cases 10 15 linear functions
respectively. first set experiments used parallel (synchronous) updates
samples fixed set 100 belief points. applied stochastic gradient descent techniques
find best fit cases. tested control performance value-function
approximations obtained 10, 20 30 updates, starting QMDP solution.
second set experiments, applied incremental stochastic update scheme
Gauss-Seidel-style updates. results method acquired every grid point
updated 150 times, learning rates decreasing linearly range 0:2
0:001. started QMDP solution. results lookahead controllers
summarized Figure 22, also shows control performance direct Q-function
controller and, comparison, results QMDP method.
linear-Q function model performed well results lookahead design
better results QMDP method. difference quite apparent
direct approaches. general, good performance method attributed
choice function model let us match shape optimal value function
reasonably well. contrast, softmax models (with 10 15 linear functions)
perform expected. probably softmax model linear functions
updated every sample point. leads situations multiple linear functions
try track belief point update. circumstances hard capture
structure optimal value function accurately. negative feature
effects on-line changes linear functions added softmax approximation,
thus could bias incremental update schemes. ideal case, would like identify
one vector ff responsible specific belief point update (modify) vector.
linear Q-function approach avoids problem always updating single linear
function (corresponding action).

76

fiValue-Function Approximations POMDPs

control performance
70

60
lookahead

10 20
iter iter 30 stoch
iter

10 20
iter iter

10 20 30
iter iter iter

score

50

40

stoch

30
iter stoch

20
10 iter
iter

30
iter

stoch

direct

30

20
QMDP
approximation

linear Q-function
lookahead

linear Q-function
direct

softmax
(10 linear functions)

softmax
(15 linear functions)

Figure 22: Control performance least-squares fit methods. Models tested include: linear
Q-function model (with direct lookahead control) softmax models 10 15 linear functions (lookahead control only). Value functions
obtained 10, 20 30 synchronous updates value functions obtained
incremental stochastic update scheme used define different
controllers. comparison, also include results two QMDP controllers.

4.7 Grid-Based Approximations Linear Function Updates
alternative grid-based approximation method constructed applying Sondik's
approach computing derivatives (linear functions) points grid (Lovejoy, 1991a,
1993). Let Vbi piecewise linear convex function described set linear functions .
new linear function belief point b action computed eciently
(Smallwood & Sondik, 1973; Littman, 1996)
ffb;a
i+1 (s) = (s; a) +

XX
o2 s0 2S

P (s0 ; ojs; a)ffi(b;a;o) (s0 );

(b; a; o) indexes linear function ffi set linear functions
maximizes expression
"
X X

s0 2S s2S

(10)


(defining Vbi )

#

P (s0 ; ojs; a)b(s) ffi (s0 )

fixed combination b; a; o. optimizing function b acquired choosing
vector best overall value action vectors. is, assuming bi+1
set candidate linear functions, resulting functions satisfies
= arg max X ffb (s)b(s):
ffb;i+1
i+1
ff +1 2 +1 s2S
collection linear functions obtained set belief points combined
piecewise linear convex value function. idea behind number exact
b


b


77

fiHauskrecht

V(b)
*

V(b)
V(b)
new linear function

0

b

1

b(s1 )

Figure 23: incremental version grid-based linear function method. piecewise
linear lower bound improved new linear function computed belief
point b using Sondik's method.
algorithms (see Section 2.4.2). However, exact case, set points cover
linear functions defining new value function must located first, hard task
itself. contrast, approximation method uses incomplete set belief points
fixed least easy locate, example via random heuristic selection. use
HGL denote value-function mapping grid approach.
advantage grid-based method leads ecient updates.
time complexity update polynomial equals O(jGjjAjjS j2 jj). yields set
jGj linear functions, compared jAjj jjj possible functions exact update.
Since set grid-points incomplete, resulting approximation lower-bounds
value function one would obtain performing exact update (Lovejoy, 1991a).

Theorem 13 (Lower-bound property grid-based linear function update). Let Vbi
piecewise linear value function G set grid points used compute linear function
updates. HGL Vbi H Vbi .
4.7.1 Incremental Linear-Function Approach

drawback grid-based linear function method HGL contraction
discounted infinite-horizon case, therefore value iteration method based
mapping may converge (Lovejoy, 1991a). remedy problem, propose
incremental version grid-based linear function method. idea refinement
prevent instability gradually improving piecewise linear convex lower bound
value function.
Assume Vbi V convex piecewise linear lower bound optimal value
function defined linear function set , let ffb linear function point b
computed Vbi using Sondik's method. one construct new improved
value function Vbi+1 Vbi simply adding new linear function ffb . is:
i+1 = [ ffb . idea incremental update, illustrated Figure 23, similar
incremental methods used Cheng (1988) Lovejoy (1993). method
78

fiValue-Function Approximations POMDPs

running times

bound quality

2500

65
9

2000

60

10

8

score

55
50
45
40

2

3

4

5

6

7

8

9

10

2

3

4

5

6

7

8

9

time [sec]

7

10

1

6

1500

10

5

9
8
1000

4

7
6

3

5
4

500

1

2

3

35
1.26

50.02

1

2

1

0

30
standard approach

QMDP

incremental approach

fast
informed

standard approach

incremental approach

Figure 24: Bound quality running times standard incremental version
grid-based linear-function method fixed 40-point grid. Cumulative
running times (including previous update cycles) shown methods.
Running times QMDP fast informed bound methods included
comparison.
extended handle set grid points G straightforward way. Note also
adding one new linear functions , previous linear functions may
become redundant removed value function. Techniques redundancy
checking applied exact approaches (Monahan, 1982; Eagle, 1984).
incremental refinement stable converges fixed set grid points.
price paid feature linear function set grow size iteration
steps. Although growth linear number iterations, compared
potentially exponential growth exact methods, linear function set describing
piecewise linear approximation become huge. Thus, practice usually stop
incremental updates well method converges. question remains open
complexity (hardness) problem finding fixed-point solution fixed set
grid points G.
Figure 24 illustrates trade-offs involved applying incremental updates
compared standard fixed-grid approach Maze20 problem. use
grid 40 points techniques initial value function. Results 1-10
update cycles shown. see incremental method longer running times
standard method, since number linear functions grow every update.
hand, bound quality incremental method improves rapidly
never become worse update steps.
4.7.2 Minimum Expected Reward

incremental method improves lower bound value function. value function, say Vbi , used create controller (with either lookahead direct-action
choice). general case, cannot say anything performance quality
controllers regard Vbi . However, certain conditions performance
controllers guaranteed never fall Vbi . following theorem (proved
Appendix) establishes conditions.

Theorem 14 Let Vbi value function obtained via incremental linear function method,
starting Vb0 , corresponds fixed strategy C0 . Let CLA;i CDR;i two
79

fiHauskrecht

controllers based Vbi : lookahead controller direct action controller, V C ,
VC
respective value functions. Vbi V C
Vbi V C
hold.
note property holds incremental version exact value iteration.
is, lookahead direct controllers perform worse Vi obtained
incremental updates V0 corresponding FSM controller C0 .
LA;i

DR;i

LA;i

DR;i

4.7.3 Selecting Grid Points

incremental version grid-based linear-function approximation exible
works arbitrary grid.26 Moreover, grid need fixed changed
line. Thus, problem finding grids reduces problem selecting belief points
updated next. One apply various strategies this. example, one use
fixed set grid points update repeatedly, one select belief points line
using various heuristics.
incremental linear function method guarantees value function always
improved (all linear functions previous steps kept unless found redundant).
quality new linear function (to added next) depends strongly quality
linear functions obtained previous steps. Therefore, objective select order
points better chances larger improvement. designed two heuristic
strategies selecting ordering belief points.
first strategy attempts optimize updates extreme points belief simplex
ordering heuristically. idea heuristic based fact states
higher expected rewards (e.g. designated goal states) backpropagate effects
(rewards) locally. Therefore, desirable states neighborhood highest
reward state updated first, distant ones later. apply idea order
extreme points belief simplex, relying current estimate value function
identify highest expected reward states POMDP model determine
neighbor states.
second strategy based idea stochastic simulation. strategy generates
sequence belief points likely reached (fixed) initial belief point.
points sequence used reverse order generate updates. intent
heuristic \maximize" improvement value function initial fixed
point. run heuristic, need find initial belief point set initial belief
points. address problem, use first heuristic allows us order
extreme points belief simplex. points used initial beliefs
simulation part. Thus, two-tier strategy: top-level strategy orders extremes
belief simplex, lower-level strategy applies stochastic simulation generate
sequence belief states likely reachable specific extreme point.
tested order heuristics two-tier heuristics Maze20 problem,
compared also two simple point selection strategies: fixed-grid strategy,
set 40 grid points updated repeatedly, random-grid strategy,
points always chosen uniformly random. Figure 25 shows bound quality
26. restriction grid points must included grid, required
example linear point-interpolation scheme, use extreme points belief
simplex.
80

fiValue-Function Approximations POMDPs

bound quality
65
60

score

55
2

50

5 6 7 8 9 10
3 4
2

1

3

7
4 5 6

8 9 10
2

4

3

2

7 8 9 10
5 6

3

1

1

1

45

7 8 9 10
4 5 6

40
35
30
fixed grid

random grid

order heuristic

2-tier heuristic

Figure 25: Improvements bound quality incremental linear-function method
four different grid-selection heuristics. cycle includes 40 grid-point
updates.
methods 10 update cycles (each cycle consists 40 grid point updates)
Maze20 problem. see differences quality value-function approximations
different strategies (even simple ones) relatively small. note
observed similar results also problems, Maze20.
relatively small improvement heuristics explained fact
every new linear function uences larger portion belief space thus method
less sensitive choice specific point.27 However, another plausible explanation heuristics good accurate heuristics combinations
heuristics could constructed. Ecient strategies locating grid points used
exact methods, e.g. Witness algorithm (Kaelbling et al., 1999) Cheng's methods (Cheng, 1988) potentially applied problem. remains open area
research.
4.7.4 Control

grid-based linear-function approach leads piecewise linear convex approximation. Every linear function comes natural action choice lets us choose
action greedily. Thus run lookahead direct controllers. Figure 26
compares performance four different controllers fixed grid 40 points, combining standard incremental updates lookahead direct greedy control 1,
5 10 update cycles. results (see also Figure 24) illustrate trade-offs
computational time obtaining solution quality. see incremental
approach lookahead controller design tend improve control performance.
prices paid worse running reaction times, respectively.
27. small sensitivity incremental method selection grid points would suggest one
could, many instances, replace exact updates simpler point selection strategies. could
increase speed exact value-iteration methods (at least initial stages), suffer
ineciencies associated locating complete set grid points updated every step. However,
issue needs investigated.
81

fiHauskrecht

control performance
70

5

60

10

10
1

lookahead

lookahead

5

10

5

10

1

5
1

1

score

50
direct
40

direct

30

20
QMDP

fast
informed

direct
standard

lookahead
standard

direct
incremental

lookahead
incremental

Figure 26: Control performance four different controllers based grid-based linear function updates 1, 5 10 update cycles 40-point grid. Controllers represent combinations two update strategies (standard incremental) two action-extraction techniques (direct lookahead). Running
times two update strategies presented Figure 24. comparison include also performances QMDP fast informed bound
methods (with direct lookahead designs).
control performance
70
5

score

5

10

10

1

1

60

1

5

10
1

5

10

50

40

30

20
QMDP

fast
informed

fixed grid

random grid

order heuristic

2-tier heuristic

Figure 27: Control performances lookahead controllers based incremental linearfunction approach different point-selection heuristics 1, 5 10 improvement cycles. comparison, scores QMDP fast informed
bound approximations shown well.
Figure 27 illustrates effect point selection heuristics control. compare
results lookahead control only, using approximations obtained 1, 5 10 improvement cycles (each cycle consists 40 grid point updates). test results show that,

82

fiValue-Function Approximations POMDPs

bound quality, big differences among various heuristics, suggesting
small sensitivity control selection grid points.

4.8 Summary Value-Function Approximations
Heuristic value-function approximations methods allow us replace hard-to-compute exact
methods trade solution quality speed. numerous methods employ, different properties different trade-offs quality versus speed. Tables 1
2 summarize main theoretical properties approximation methods covered
paper. majority methods polynomial complexity least ecient (polynomial) Bellman updates. makes good candidates complex
POMDP problems reach exact methods.
methods heuristic approximations give solutions
guaranteed precision. Despite fact proved solutions methods
worse others terms value function quality (see Figure 15). one
main contributions paper. However, currently minimal theoretical results
relating methods terms control performance; exception results
FSM-controllers FSM-based approximations. key observation
quality control (lookahead control) important approximate shape
(derivatives) value function correctly. also illustrated empirically gridbased interpolation-extrapolation methods Section 4.5.9 based non-convex
value functions. main challenges find ways analyzing comparing
control performance different approximations also theoretically identify classes
POMDPs certain methods dominate others.
Finally, note list methods complete value-function approximation methods refinements existing methods possible. example, White
Scherer (1994) investigate methods based truncated histories lead upper
lower bound estimates value function complete information states (complete
histories). Also, additional restrictions methods change properties
generic method. example, possible additional assumptions
able ensure convergence least-squares fit approximation.
5. Conclusions

POMDPs offers elegant mathematical framework representing decision processes
stochastic partially observable domains. Despite modeling advantages, however,
POMDP problems hard solve exactly. Thus, complexity problem solvingprocedures becomes key aspect sucessful application model real-world
problems, even expense optimality. recent complexity results
approximability POMDP problems encouraging (Lusena et al., 1998; Madani
et al., 1999), focus heuristic approximations, particular approximations value
functions.

83

fiHauskrecht

Method
MDP approximation
QMDP approximation
Fast informed bound
UMDP approximation
Fixed-strategy method
Grid-based interpolation-extrapolation
Nearest neighbor
Kernel regression
Linear point interpolation
Curve-fitting (least-squares fit)
linear Q-function
Grid-based linear function method
Incremental version (start lower bound)

Bound
upper
upper
upper
lower
lower
upper
lower
lower

Isotonicity

p
p
p
p
p
-p
p
p

Contraction

-p

-*

p
p
p
p
p
-p
p
p

Table 1: Properties different value-function approximation methods: bound property,
isotonicity contraction property underlying mappings 0 < 1.
(*) Although incremental version grid-based linear-function method
contraction always converges.
Method
MDP approximation
QMDP approximation
Fast informed bound
UMDP approximation
Fixed-strategy method
Grid-based interpolation-extrapolation
Nearest neighbor
Kernel regression
Linear point interpolation
Fixed interpolation
Best interpolation
Curve-fitting (least-squares fit)
linear Q-function
Grid-based linear function method
Incremental version

Finite-horizon
P
P
P
NP-hard
P
varies
P
P
P
P
P
varies
P
P
NA

Discounted infinite-horizon
P
P
P
undecidable
P
NA
P
P
varies
P
?
NA
NA
NA
?

Table 2: Complexity value-function approximation methods finite-horizon problem
discounted infinite-horizon problem. objective discounted infinitehorizon case find corresponding fixed-point solution. complexity
results take account, addition components POMDPs, also
approximation specific parameters, e.g., size grid G grid-based methods. ? indicates open instances NA methods applicable one
problems (e.g. possible divergence).

84

fiValue-Function Approximations POMDPs

5.1 Contributions
paper surveys new known value-function approximation methods solving POMDPs.
focus primarily theoretical analysis comparison methods, findings results supported experimentally problem moderate size agent
navigation domain. analyze methods different perspectives: computational complexity, capability bound optimal value function, convergence properties
iterative implementations, quality derived controllers. analysis includes new
theoretical results, deriving properties individual approximations, relations
exact methods. general, relations trade-offs among different methods
well understood. provide new insights issues analyzing
corresponding updates. example, showed differences among exact,
MDP, QMDP, fast-informed bound, UMDP methods boil simple
mathematical manipulations subsequent effect value-function approximation. allowed us determine relations among different methods terms quality
respective value functions one main results paper.
also presented number new methods heuristic refinements existing
techniques. primary contributions area include fast-informed bound, gridbased point interpolation methods (including adaptive grid approaches based stochastic sampling), incremental linear-function method. also showed
instances solutions obtained eciently converting original approximation equivalent finite-state MDP. example, grid-based approximations
convex rules often solved via conversion grid-based MDP (in grid points
correspond new states), leading polynomial-complexity algorithm finite discounted infinite-horizon cases (Section 4.5.3). result dramatically
improve run-time performance grid-based approaches. similar conversion
equivalent finite-state MDP, allowing polynomial-time solution discounted
infinite-horizon problem, shown fast informed bound method (Section 4.2).
5.2 Challenges Future Directions
Work POMDPs approximations far complete. complexity results
remain open, particular, complexity grid-based approach seeking best interpolation, complexity finding fixed-point solution incremental version
grid-based linear-function method. Another interesting issue needs investigation convergence value iteration least-squares approximation. Although
method unstable general case, possible certain restrictions
converge.
paper use single POMDP problem (Maze20) support theoretical
findings illustrate intuitions. Therefore, results supported theoretically (related mostly control) cannot generalized used rank different methods,
since performance may vary problems. general, area POMDPs
POMDP approximations suffers shortage larger-scale experimental work
multiple problems different complexities broad range methods. Experimental
work especially needed study compare different methods regard control
quality. main reason theoretical results relating
85

fiHauskrecht

control performance. studies help focus theoretical exploration discovering
interesting cases possibly identifying classes problems certain approximations less suitable. preliminary experimental results show
significant differences control performance among different methods
may suitable approximate control policies. example, grid-based
nearest-neighbor approach piecewise-constant approximation typically inferior
outperformed simpler (and ecient) value-function methods.
present work focused heuristic approximation methods. investigated general ( at) POMDPs take advantage additional structural refinements.
However, real-world problems usually offer structure exploited devise
new algorithms perhaps lead speed-ups. also possible
restricted versions POMDPs (with additional structural assumptions) solved
approximated eciently, even though general complexity results POMDPs approximations encouraging (Papadimitriou & Tsitsiklis, 1987; Littman, 1996;
Mundhenk et al., 1997; Lusena et al., 1998; Madani et al., 1999). challenge
identify models allow ecient solutions time interesting enough
point application.
Finally, number interesting issues arise move problems large state,
action, observation spaces. Here, complexity value-function updates
also belief state updates becomes issue. general, partial observability hidden
process states allow us factor decompose belief states (and updates),
even transitions great deal structure represented compactly.
Promising directions deal issues include various Monte-Carlo approaches (Isard
& Blake, 1996; Kanazawa, Koller, & Russell, 1995; Doucet, 1998; Kearns et al., 1999)),
methods approximating belief states via decomposition (Boyen & Koller, 1998, 1999),
combination two approaches (McAllester & Singh, 1999).
Acknowledgements

Anthony Cassandra, Thomas Dean, Leslie Kaelbling, William Long, Peter Szolovits
anonymous reviewers provided valuable feedback comments work. research
supported grant RO1 LM 04493 grant 1T15LM07092 National Library
Medicine, DOD Advanced Research Project Agency (ARPA) contract number
N66001-95-M-1089 DARPA/Rome Labs Planning Initiative grant F30602-95-1-0020.
Appendix A. Theorems proofs

A.1 Convergence Bound
Theorem 6 Let H1 H2 two value-function mappings defined V1 V2 s.t.
1. H1 , H2 contractions fixed points V1 , V2 ;
2. V1 2 V2 H2 V1 H1 V1 = V1 ;

3. H2 isotone mapping.
V2 V1 holds.
86

fiValue-Function Approximations POMDPs

Proof applying H2 condition 2 expanding result condition 2
get: H22 V1 H2 V1 H1 V1 = V1 . Repeating get limit V2 H2n V1
H22 V1 H2V1 H1V1 = V1 , proves result. 2
A.2 Accuracy Lookahead Controller Based Bounds
Theorem 7 Let VbU VbL upper lower bounds optimal value function
discounted infinite-horizon problem. Let = supb jVbU (b) VbL (b)j = kVbU VbL k
maximum bound difference. expected reward lookahead controller Vb LA ,
constructed either VbU VbL , satisfies kVb LA V k (1(2 )) .

Proof Let Vb denotes either upper lower bound approximation V H LA
value function mapping corresponding lookahead policy Vb . Note, since
lookahead policy always optimizes actions regard Vb , H Vb = H LA Vb must hold.
error Vb LA bounded using triangle inequality

kVb LA V k kVb LA Vb k + kVb V k:
first component satisfies:

kVb LA Vb k = kH LA Vb LA Vb k
kH LA Vb LA H Vb k + kH Vb Vb k
= kH LA Vb LA H LA Vb k + kH Vb Vb k
kVb LA Vb k +
inequality: kH Vb Vb k follows isotonicity H fact Vb either
upper lower bound. Rearranging inequalities, obtain: kVb LA Vb k = (1 ) .
bound second term kVb V k trivial.
(2 )
Therefore, kVb LA V k [ (1 1 ) + 1] = (1
) . 2
A.3 MDP, QMDP Fast Informed Bounds
Theorem 8 solution fast informed bound approximation found solving
MDP jS jjAjjj states, jAj actions discount factor .
Proof Let ffai linear function action defining Vbi . Let ffi (s; a) denote parameters
function. parameters Vbi+1 satisfy:
ffi+1 (s; a) = (s; a) +
Let

X

X
0
0 0
max
0 2A 0 P (s ; ojs; a)ffi (s ; ):

o2
2S

ffi+1 (s; a; o) = max
0

X

2 s0 2

87

P (s0 ; ojs; a)ffi (s0 ; a0 ):

fiHauskrecht

Now, rewrite ffi+1 (s; a; o) every s; a; as:
8
<X

2

X

ffi+1 (s; a; o) = max
P (s0 ; ojs; a) 4(s0 ; a0 ) +
a0 2A :s0 2S
o0 2
8
2
< X
4
= max
a0 2A : 0

3

2

39
=
ffi (s0 ; a0 ; o0 )5;

P (s0 ; ojs; a)(s0 ; a0 )5 + 4

X X

o0 2 s0 2S

2S

39
=
P (s0 ; ojs; a)ffi (s0 ; a0 ; o0 )5;

equations define MDP state space , action space discount
factor . Thus, solution fast informed bound update found solving
equivalent finite-state MDP. 2

Theorem 9 Let Vbi corresponds piecewise linear convex value function defined
linear functions. H Vbi HF IB Vbi HQMDP Vbi HMDP Vbi :
Proof
8
<X

X



9
=

XX

max : (s; a)b(s) +
max
P (s0 ; ojs; a)b(s)ffi (s0 );
a2A s2S
ff
2
0
o2
2S s2S
= (HVi )(b)


max
a2A

X
s2S

2

b(s) 4(s; a) +

= (HF IB Vi )(b)

max
a2A

X
s2S

b(s) 4(s; a) +



s2S

a2A

2

= (HMDP Vbi )(b)

3

P (s0 ; ojs; a)ffi (s0 )5
3

X
s0 2S

2

b(s) max 4(s; a) +

max

X

o2 ffi 2 s0 2S

2

= (HQMDP Vbi )(b)
X

X



P (s0 js; a) max ffi (s0 )5
ffi 2



3

X
s0 2S

P (s0 js; a) max ffi (s0 )5
ffi 2



A.4 Fixed-Strategy Approximations
Theorem 10 Let CF SM FSM controller. Let CDR CLA direct
one-step-lookahead controllers constructed based CF SM . V C (b) V C (b)
V C (b) V C (b) hold belief states b 2 .
Proof value function FSM controller CF SM satisfies:
F SM

F SM

LA

VC

F SM



(b) = max V (x; b) = V ( (b); b)
x2M

V (x; b) = (b; (x)) +

X
o2

P (ojb; (x))V ((x; o); (b; (x); o)):
88

DR

fiValue-Function Approximations POMDPs

direct controller CDR selects action greedily every step, is, always
chooses according (b) = arg maxx2M V (x; b). lookahead controller CLA selects
action based V (x; b) one step away:

LA (b) = arg max
a2A

"

#

X

0
(b; a) + P (ojb; a) max
0 2M V (x ; (b; a; o)) :
x
o2

expanding value function CF SM one step get:

VC

F SM

(b) = max V (x; b)
x2M
"

#

X

= max (b; (x)) + P (ojb; (x))V ((x; o); (b; (x); o))
x2M
o2
= (b; ( (b))) +

(b; ( (b))) +
"

X

o2

X

o2

(1)

P (ojb; ( (b)))V ((x; o); (b; ( (b)); o))

P (ojb; ( (b))) max
V (x0 ; (b; ( (b)); o))
0
x 2M

X

(2)

#

0
max
(b; a) + P (ojb; a) max
0 2M V (x ; (b; a; o))
a2A
x
o2
X
LA
0
LA
= (b; (b)) + P (ojb; LA (b)) max
0 2M V (x ; (b; (b); o))
x
o2

(3)

Iteratively expanding maxx0 2M V (x; :) 2 3 expression 1 substituing improved
(higher value) expressions 2 3 back obtain value functions direct
lookahead controllers. (Expansions 2 lead value direct controller
expansions 3 value lookahead controller.) Thus V C
VC
C
C
V
V must hold. Note, however, action choices (b) LA(b)
expressions 2 3 different leading different next step belief states
subsequently different expansion sequences. Therefore, result imply
V DR (b) V LA (b) b 2 . 2
F SM

F SM

DR

LA

A.5 Grid-Based Linear-Function Method
Theorem 14 Let Vbi value function obtained via incremental linear function method,
starting Vb0 , corresponds fixed strategy C0 . Let CLA;i CDR;i two
controllers based Vbi : lookahead controller direct action controller, V C ,
VC
respective value functions. Vbi V C
Vbi V C
hold.
Proof initializing method value function FSM controller C0 ,
incremental updates interpreted additions new states FSM controller (a
new linear function corresponds new state FSM). Let Ci controller
step i. V C
= Vbi holds inequalities follow Theorem 10. 2
LA;i

DR;i

LA;i

F SM;i

89

DR;i

fiHauskrecht

References

Astrom, K. J. (1965). Optimal control Markov decision processes incomplete state
estimation. Journal Mathematical Analysis Applications, 10, 174{205.
Baird, L. C. (1995). Residual algorithms: Reinforcement learning function approximation. Proceedings Twelfth International Conference Machine Learning,
pp. 30{37.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic
programming. Artificial Intelligence, 72, 81{138.
Bellman, R. E. (1957). Dynamic programming. Princeton University Press, Princeton, NJ.
Bertsekas, D. P. (1994). counter-example temporal differences learning. Neural Computation, 7, 270{279.
Bertsekas, D. P. (1995). Dynamic programming optimal control. Athena Scientific.
Bonet, B., & Geffner, H. (1998). Learning sorting classification POMDPs.
Proceedings Fifteenth International Conference Machine Learning.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Artificial Intelligence, 11, 1{94.
Boutilier, C., & Poole, D. (1996). Exploiting structure policy construction. Proceedings
Thirteenth National Conference Artificial Intelligence, pp. 1168{1175.
Boyan, J. A., & Moore, A. A. (1995). Generalization reinforcement learning: safely
approximating value function. Advances Neural Information Processing
Systems 7. MIT Press.
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.
Proceedings Fourteenth Conference Uncertainty Artificial Intelligence, pp.
33{42.
Boyen, X., & Koller, D. (1999). Exploiting architecture dynamic systems. Proceedings Sixteenth National Conference Artificial Intelligence, pp. 313{320.
Brafman, R. I. (1997). heuristic variable grid solution method POMDPs. Proceedings Fourteenth National Conference Artificial Intelligence, pp. 727{233.
Burago, D., Rougemont, M. D., & Slissenko, A. (1996). complexity partially
observed Markov decision processes. Theoretical Computer Science, 157, 161{183.
Cassandra, A. R. (1998). Exact approximate algorithms partially observable Markov
decision processes. Ph.D. thesis, Brown University.
Cassandra, A. R., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple,
fast, exact algorithm partially observable Markov decision processes. Proceedings
Thirteenth Conference Uncertainty Artificial Intelligence, pp. 54{61.
90

fiValue-Function Approximations POMDPs

Casta~non, D. (1997). Approximate dynamic programming sensor management.
Proceedings Conference Decision Control.
Cheng, H.-T. (1988). Algorithms partially observable Markov decision processes. Ph.D.
thesis, University British Columbia.
Condon, A. (1992). complexity stochastic games. Information Computation,
96, 203{224.
Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.
Computational Intelligence, 5, 142{150.
Dearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning. Artificial Intelligence, 89, 219{283.
Doucet, A. (1998). sequential simulation-based methods Bayesian filtering. Tech.
rep. CUED/F-INFENG/TR 310, Department Engineering, Cambridge University.
Drake, A. (1962). Observation Markov process noisy channel. Ph.D. thesis,
Massachusetts Institute Technology.
Draper, D., Hanks, S., & Weld, D. (1994). Probabilistic planning information gathering
contingent execution. Proceedings Second International Conference
AI Planning Systems, pp. 31{36.
Eagle, J. N. (1984). optimal search moving target search path constrained.
Operations Research, 32, 1107{1115.
Eaves, B. (1984). course triangulations soving differential equations deformations. Springer-Verlag, Berlin.
Gordon, G. J. (1995). Stable function approximation dynamic programming. Proceedings Twelfth International Conference Machine Learning.
Hansen, E. (1998a). improved policy iteration algorithm partially observable MDPs.
Advances Neural Information Processing Systems 10. MIT Press.
Hansen, E. (1998b). Solving POMDPs searching policy space. Proceedings
Fourteenth Conference Uncertainty Artificial Intelligence, pp. 211{219.
Hauskrecht, M. (1997). Planning control stochastic domains imperfect information. Ph.D. thesis, Massachusetts Institute Technology.
Hauskrecht, M., & Fraser, H. (1998). Planning medical therapy using partially observable
Markov decision processes. Proceedings Ninth International Workshop
Principles Diagnosis (DX-98), pp. 182{189.
Hauskrecht, M., & Fraser, H. (2000). Planning treatment ischemic heart disease
partially observable Markov decision processes. Artificial Intelligence Medicine, 18,
221{244.
91

fiHauskrecht

Heyman, D., & Sobel, M. (1984). Stochastic methods operations research: stochastic
optimization. McGraw-Hill.
Howard, R. A. (1960). Dynamic Programming Markov Processes. MIT Press, Cambridge.
Howard, R. A., & Matheson, J. (1984). uence diagrams. Principles Applications
Decision Analysis, 2.
Isard, M., & Blake, A. (1996). Contour tracking stochastic propagation conditional
density. Proccedings Europian Conference Computer Vision, pp. 343{356.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1999). Planning acting
partially observable stochastic domains. Artificial Intelligence, 101, 99{134.
Kanazawa, K., Koller, D., & Russell, S. J. (1995). Stochastic simulation algorithms
dynamic probabilistic networks. Proceedings Eleventh Conference Uncertainty Artificial Intelligence, pp. 346{351.
Kearns, M., Mansour, Y., & Ng, A. Y. (1999). sparse sampling algorithm near
optimal planning large Markov decision processes. Proceedings Sixteenth
International Joint Conference Artificial Intelligence, pp. 1324{1331.
Kjaerulff, U. (1992). computational scheme reasoning dynamic probabilistic networks. Proceedings Eighth Conference Uncertainty Artificial Intelligence, pp. 121{129.
Korf, R. (1985). Depth-first iterative deepening: optimal admissible tree search. Artificial
Intelligence, 27, 97{109.
Kushmerick, N., Hanks, S., & Weld, D. (1995). algorithm probabilistic planning.
Artificial Intelligence, 76, 239{286.
Lauritzen, S. L. (1996). Graphical models. Clarendon Press.
Littman, M. L. (1994). Memoryless policies: Theoretical limitations practical results.
Cliff, D., Husbands, P., Meyer, J., & Wilson, S. (Eds.), Animals Animats 3: Proceedings Third International Conference Simulation Adaptive
Behavior. MIT Press, Cambridge.
Littman, M. L. (1996). Algorithms sequential decision making. Ph.D. thesis, Brown
University.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995). Learning policies partially
observable environments: scaling up. Proceedings Twelfth International
Conference Machine Learning, pp. 362{370.
Lovejoy, W. S. (1991a). Computationally feasible bounds partially observed Markov
decision processes. Operations Research, 39, 192{175.
92

fiValue-Function Approximations POMDPs

Lovejoy, W. S. (1991b). survey algorithmic methods partially observed Markov
decision processes. Annals Operations Research, 28, 47{66.
Lovejoy, W. S. (1993). Suboptimal policies bounds parameter adaptive decision
processes. Operations Research, 41, 583{599.
Lusena, C., Goldsmith, J., & Mundhenk, M. (1998). Nonapproximability results Markov
decision processes. Tech. rep., University Kentucky.
Madani, O., Hanks, S., & Condon, A. (1999). undecidability probabilistic planning
infinite-horizon partially observable Markov decision processes. Proceedings
Sixteenth National Conference Artificial Intelligence.
McAllester, D., & Singh, S. P. (1999). Approximate planning factored POMDPs using
belief state simplification. Proceedings Fifteenth Conference Uncertainty
Artificial Intelligence, pp. 409{416.
McCallum, R. (1995). Instance-based utile distinctions reinforcement learning
hidden state. Proceedings Twelfth International Conference Machine
Learning.
Monahan, G. E. (1982). survey partially observable Markov decision processes: theory,
models, algorithms. Management Science, 28, 1{16.
Mundhenk, M., Goldsmith, J., Lusena, C., & Allender, E. (1997). Encyclopaedia complexity results finite-horizon Markov decision process problems. Tech. rep., CS
Dept TR 273-97, University Kentucky.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). complexity Markov decision processes. Mathematics Operations Research, 12, 441{450.
Parr, R., & Russell, S. (1995). Approximating optimal policies partially observable
stochastic domains. Proceedings Fourteenth International Joint Conference
Artificial Intelligence, pp. 1088{1094.
Pearl, J. (1988). Probabilistic reasoning intelligent systems. Morgan Kaufman.
Platzman, L. K. (1977). Finite memory estimation control finite probabilistic systems.
Ph.D. thesis, Massachusetts Institute Technology.
Platzman, L. K. (1980). feasible computational approach infinite-horizon partiallyobserved Markov decision problems. Tech. rep., Georgia Institute Technology.
Puterman, M. L. (1994). Markov decision processes: discrete stochastic dynamic programming. John Wiley, New York.
Raiffa, H. (1970). Decision analysis. Introductory lectures choices uncertainty.
Addison-Wesley.
Rumelhart, D., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations
error propagation. Parallel Distributed Processing, pp. 318{362.
93

fiHauskrecht

Satia, J., & Lave, R. (1973). Markovian decision processes probabilistic observation
states. Management Science, 20, 1{13.
Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Learning without state-estimation
partially observable Markovian decision processes. Proceedings Eleventh
International Conference Machine Learning, pp. 284{292.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable
processes finite horizon. Operations Research, 21, 1071{1088.
Sondik, E. J. (1971). optimal control partially observable Markov decision processes.
Ph.D. thesis, Stanford University.
Sondik, E. J. (1978). optimal control partially observable processes infinite
horizon: Discounted costs. Operations Research, 26, 282{304.
Tatman, J., & Schachter, R. D. (1990). Dynamic programming uence diagrams.
IEEE Transactions Systems, Man Cybernetics, 20, 365{379.
Tsitsiklis, J. N., & Roy, B. V. (1996). Feature-based methods large-scale dynamic
programming. Machine Learning, 22, 59{94.
Washington, R. (1996). Incremental Markov model planning. Proceedings Eight
IEEE International Conference Tools Artificial Intelligence, pp. 41{47.
White, C. C., & Scherer, W. T. (1994). Finite memory suboptimal design partially
observed Markov decision processes. Operations Research, 42, 439{455.
Williams, R. J., & Baird, L. C. (1994). Tight performance bounds greedy policies based
imperfect value functions. Proceedings Tenth Yale Workshop Adaptive
Learning Systems Yale University.
Yost, K. A. (1998). Solution large-scale allocation problems partially observable
outcomes. Ph.D. thesis, Naval Postgraduate School, Monterey, CA.
Zhang, N. L., & Lee, S. S. (1998). Planning partially observable Markov decision
processes: Advances exact solution method. Proceedings Fourteenth Conference Uncertainty Artificial Intelligence, pp. 523{530.
Zhang, N. L., & Liu, W. (1997a). model approximation scheme planning partially
observable stochastic domains. Journal Artificial Intelligence Research, 7, 199{230.
Zhang, N. L., & Liu, W. (1997b). Region-based approximations planning stochastic
domains. Proceedings Thirteenth Conference Uncertainty Artificial
Intelligence, pp. 472{480.

94

fi
