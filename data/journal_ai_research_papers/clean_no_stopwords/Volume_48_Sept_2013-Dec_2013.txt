Journal Artificial Intelligence Research 48 (2013) 635-669Submitted 11/12; published 11/13Reasoning ExplanationsNegative Query Answers DL-LiteDiego Calvanesecalvanese@inf.unibz.itFree University Bozen-Bolzano, ItalyMagdalena Ortizortiz@kr.tuwien.ac.atVienna University Technology, AustriaMantas Simkussimkus@dbai.tuwien.ac.atVienna University Technology, AustriaGiorgio StefanoniGiorgio.Stefanoni@cs.ox.ac.ukUniversity Oxford, United KingdomAbstractorder meet usability requirements, logic-based applications provide explanation facilities reasoning services. holds also Description Logics, researchfocused explanation TBox reasoning and, recently, query answering. Besides explaining presence tuple query answer, important explainalso given tuple missing. address latter problem instance conjunctive query answering DL-Lite ontologies adopting abductive reasoning; is,look additions ABox force given tuple result. reasoningtasks consider existence recognition explanation, relevance necessitygiven assertion explanation. characterize computational complexityproblems arbitrary, subset minimal, cardinality minimal explanations.1. IntroductionOntology-based data access (OBDA) systems new form information systemsuse ontology, set logical constraints, mediate access data. roleontology OBDA system twofold. one hand, intermediate layerdomain user physical data providing unified view informationheld various data sources. many cases, ontology extends data vocabularyintroducing new intensional predicates used query informationsuccinct declarative way. hand, ontology provides constraints,taken account answering queries may contribute enrichobtained answers. Hence, potentially relevant implicit knowledge deriveddata, plus ontology, made explicit using specifically tailored reasoningalgorithms. existing OBDA systems based DL-Lite family lightweightDescription Logics (DLs), introduced Calvanese, De Giacomo, Lembo, Lenzerini,Rosati (2007), also basis QL profile OWL 2 ontology language(Motik, Fokoue, Horrocks, Wu, Lutz, & Grau, 2009).argued McGuinness Patel-Schneider (1998), order meet usability requirements set domain users, knowledge-based systems equipped explanation algorithms reasoning services. holds also Description Logics,c2013AI Access Foundation. rights reserved.fiCalvanese, Ortiz, Simkus & Stefanoniresearch focused explanation TBox reasoning (cf., McGuinness & Borgida,1995; Borgida, Franconi, & Horrocks, 2000; Penaloza & Sertkaya, 2010; Horridge, Parsia, &Sattler, 2008). Additionally, Borgida, Calvanese, Rodriguez-Muro (2008) studiedproblem explaining positive query answers conjunctive queries DL-Lite ontologies. particular, outlined procedure computing reasons tupleanswer query, minimizing corresponding explanation shownuser. addition, Borgida et al. (2008) suggested OBDA systems, besides explainingpositive query answers, also explain negative query answers; is, tuplesuser expects result actually occur there. OBDA systemsanswer queries ontological constraints, explaining negative query answers trivial: constraints need taken account understand required tuplemissing answers. procedure explaining negative query answers wouldimprove usability OBDA systems.reason, formalize explanation problem context query answeringDL ontologies. Following Eiter Gottlob (1995), adopt abductive reasoning;is, explanations set facts need asserted ABox force requiredtuple result. explanations help users debugging negative answergiving effective way repairing OBDA system terms updates data layer.Since ontologies used enrich data vocabulary, consider also restrictionsvocabulary additional assertions constructed. precisely,given DL TBox , ABox A, query q, set predicates, explanationgiven tuple ~c new ABox E, whose predicates occur , answerq ontology hT , Ei contains ~c. According Occams razor principle,important aspect explanations provide users solutions simpleunderstand free redundancy, hence small possible. address requirement,study various restrictions explanations, particular, focus subset minimalcardinality minimal ones. consider standard decision problems associated logic-basedabduction: (i) existence explanation, (ii) recognition given ABoxexplanation, (iii) relevance (iv) necessity ABox assertionthat is, whetheroccurs explanations. first, latter two problems may appear ratherartificial, however, provide valuable information user debugging negativeanswers. Relevance used test whether assertion user deems relatednegative answer indeed so; whereas, necessity used test whether assertionintrinsically related negative answer.idea restricting vocabulary explanations adaptation conceptintroduced Baader, Bienvenu, Lutz, Wolter (2010), study among othersquery emptiness problem. is, given query q TBox decide whetherABoxes given signature , evaluating q hT , Ai leadsempty result. Section 3, shall see framework deciding existenceexplanation relates query non-emptiness problem. fact, many DLs, decidingwhether query non-empty w.r.t. TBox reduces checking whether existsexplanation missing answer.purpose paper shed light computational complexity explainingmissing answers queries ontologies formulated DL-Lite expressive memberDL-Lite family DLs. end, consider two important classes queries636fiReasoning Explanations Negative Query Answers DL-Liteis, instance queries unions conjunctive queries (UCQs)and provide computational complexity results four decision problems defined above. Moreover,perform complexity analysis two different explanation settings. considercase explanation vocabulary strict subset vocabulary ontologydata, well case explanations constructed arbitrarypredicates. Section 4, show consider instance queries input,relevant decision problems NL-complete, irrespective chosen explanation settingparticular minimality criterion applied explanations. Section 5, analyze complexity problem admit UCQs input, showcomplexity varies respect chosen explanation setting minimalitycriterion. complexity results UCQs summarized Table 5.1.2. Preliminariessection, first introduce ontologies formulated DLs, particular focusDL DL-Lite . introduce languages querying ontologies consider,recall important properties DL-Lite used throughoutpaper. Finally, briefly present less known complexity classesmentioned later.2.1 Description Logic Ontologiesusual DLs, consider countably infinite sets NC , NR , NI atomic concepts,atomic roles, individuals, respectively. Whenever distinction atomic concepts roles immaterial, call element NC NR predicate.DL TBox finite set axioms, whose form depends specific DLconsidered; DL-Lite , DL adopted paper, definition given below.DL ABox finite set ABox assertions, expressions form A(c)P (c, d), atomic concept, P atomic role, c individuals.DL ontology pair = hT , Ai, DL TBox DL ABox.semantics DL ontologies based first-order interpretations = hI , i,non-empty set called domain interpretation function mappingindividual c NI object cI , atomic concept NC set AI ,atomic role P NR binary relation P .interpretation satisfies ABox assertion A(c) cI AI , satisfiesassertion P (c, d) hcI , dI P . Satisfaction TBox axioms also defined accordingform specific DL; define DL-Lite . interpretationmodel hT , Ai, satisfies axioms assertions A. call hT , Aiconsistent admits least one model, inconsistent otherwise. Also, ABoxconsistent TBox ontology hT , Ai consistent.2.1.1 DL-LiteDL-Lite member DL-Lite family DLs (Calvanese et al., 2007; Calvanese,De Giacomo, Lembo, Lenzerini, Poggi, Rodriguez-Muro, & Rosati, 2009),designed dealing efficiently large amounts extensional information. DL-Lite ,637fiCalvanese, Ortiz, Simkus & Stefanoniconcept expressions (or, concepts) C, denoting sets objects, role expressions (or,roles) R, denoting binary relations objects, formed according followingsyntax, denotes atomic concept P atomic role.1R P | PC | RDL-Lite TBox consists axioms following form.C1 v C2R1 v R2C1 v C2R1 v R2(funct R)Axioms first column called positive inclusions (among concepts roles, respectively), second column disjointness axioms, third columnfunctionality assertions roles. order retain tractability reasoning, DL-LiteTBoxes must satisfy additional restriction roles functional inverse functional cannot specialized. Formally, DL-Lite TBox contains (funct P ) (funct P ),role R contain R v P R v P (Calvanese et al., 2007).semantics concept expressions specified follows.(R)I= {o | o0 : ho, o0 RI }(P )I= {ho, o0 | ho0 , oi P }interpretation satisfies axiom 1 v 2 1I 2I , satisfies axiom 1 v 21I 2I = , satisfies axiom (funct R) RI partial functionthat is,set objects {o, o1 , o2 } , ho, o1 RI ho, o2 RI , o1 = o2 .Following common practice DLs DL-Lite family (Calvanese et al.,2007), usually adopt unique name assumption (UNA)that is, interpretation individual pair c 6= d, require cI 6= dI . Whenever dropassumption, explicitly say so. UNA, problem checking whetherDL-Lite ontology consistent NL-complete, whereas without UNA, problembecomes PTime-complete (Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009).2.2 Instance Queries Conjunctive QueriesLet NV countably infinite set variables. Together NI NV form set terms.Expressions form A(t) P (t, t0 ), atomic concept, P atomic role,t, t0 terms, called atoms.conjunctive query (CQ) q arity n 0 expression q(x1 , . . . , xn ) a1 , . . . , ,where, {1, . . . , m}, ai atom. tuple hx1 , . . . , xntuple answer variables q. Let NV (q) set variables occurring q, let NI (q)set individuals q, let at(q) = {a1 , . . . , }, let |q| number termsoccurring q. consider safe CQsthat is, answer variable xi q occursleast one atoms q. Boolean conjunctive query CQ arity 0, shallwrite simply set atoms. instance query q(x) conjunctive query whose bodyconsists single unary atom A(x). union conjunctive queries (UCQ) set CQs1. ignore distinction data values objects present DL-Lite OWL 2 QL,since immaterial results. is, consider value domains attributes.638fiReasoning Explanations Negative Query Answers DL-Litearity, assume w.l.o.g. CQs UCQ tupleanswer variables. following, denote IQ set instance queriesCQ set UCQs.match n-ary CQ q interpretation mapping : NV (q) NI (q)(i) (c) = cI , c NI (q),(ii) (t) AI , A(t) at(q),(iii) h(t), (t0 )i P , P (t, t0 ) at(q).n-tuple individuals hc1 , . . . , cn answer q I, exists matchq hcI1 , . . . , cIn = h(x1 ), . . . , (xn )i. let ans(q, I) denote setanswers q I. Boolean CQ returns answer either , representingvalue false,S empty tuple hi, representing value true. UCQ q, letans(q, I) = q0 q ans(q 0 , I). certain answer UCQ q arity n ontology hT , Aidefinedcert(q, , A) = {~c (NI )n | ~c ans(q, I), model hT , Ai}.2.3 Query Answering DL-Liteproblem query answering DLs problem computing certain answergiven query given DL ontology. Formulated way, query answeringcomputation problem decision problem. Since paper interestedestablishing computational complexity results, identify query answering decisionproblem, sometimes called recognition problem, input constitutedDL ontology hT , Ai, query q(~x), tuple ~c arity |~x|, task determinewhether ~c cert(q, , A). special case instance queries, problem also knowninstance checking. Notice that, since consider ontology querypart input, considering so-called combined complexity (Vardi, 1982).many DLs, instance checking reduced problem deciding ontologyconsistency. holds also DL-Lite and, thus, answering instance querydone nondeterministic logarithmic space. contrast, problem answering UCQ(and hence CQ) q DL-Lite ontology hT , Ai solved nondeterministicpolynomial time adopting pure query rewriting approach (Calvanese et al., 2007, 2009).technique works two steps. first step, compute perfect reformulationRq,T q w.r.t. is, rewrite input query q respect TBoxUCQ Rq,T . rewriting step, portion TBox relevant answering qcompiled Rq,T . second step, simply evaluate computed rewriting Rq,TABox Aseen first order interpretation. captured propositionbelow, makes use notion interpretation associated ABox, formalizedfollowing definition.Definition 2.1. Given ABox A, let DB interpretation whose domain DBset individuals occurring A,(i) cDB = c, individuals c occurring A;639fiCalvanese, Ortiz, Simkus & Stefanoni(ii) ADB = {c | A(c) A}, NC ;(iii) P DB = {hc, di | P (c, d) A}, P NR .following proposition summarizes results query answering based rewriting shown logics DL-Lite family (and DL-Lite particular) exploit following.Proposition 2.1. (Calvanese et al., 2007, 2009) Let hT , Ai DL-LiteA ontology, let qUCQ, let max(q) = maxqi q |at(qi )|. possible construct UCQ Rq,T , calledperfect reformulation q w.r.t. ,cert(q, , A) = ans(Rq,T , DB ).Moreover, Rq,T satisfies following properties.predicates occurring Rq,T occur q.qr Rq,T max(q) atoms 2 max(q) terms.q consists single instance query, qr Rq,T one atom.qr Rq,T obtained nondeterministic polynomial time combinedsize q.Deciding whether given tuple individuals ans(Rq,T , DB ) also achievednondeterministic polynomial time combined size q.2.4 Complexity Theorybriefly outline definition non-canonical complexity classes used paper;details, refer reader standard textbooks computational complexity(e.g., Papadimitriou, 1994). class P2 member Polynomial Hierarchy:class decision problems solvable nondeterministic polynomial time using NPoracle. class PNPk contains decision problems solved polynomial timeNP oracle, oracle calls must first prepared issued parallel.class DP contains problems that, considered languages, characterizedintersection language NP language coNP. Additionally, class NLcontains decision problems solved nondeterministic Turing machine usingPlogarithmic amount space. believed NL PTime NP DP PNPk 2strict hierarchy inclusions. make assumption.usual, use reductions problems infer complexity bounds throughoutpaper. Unless stated otherwise, many-one logarithmic space reductions.3. Explaining Negative Query Answerssection, formalize abductive task problem finding explanationsnegative answers queries DL ontologies.DL TBox , DL ABox A, query q IQ CQ, let (T , A, q) denoteset predicates occur , A, q. signature non-empty finitesubset NC NR . Furthermore, ABox -ABox assertions usepredicates ; is, (, A, ) .640fiReasoning Explanations Negative Query Answers DL-LiteDefinition 3.1. Let hT , Ai DL ontology, q(~x) query IQ CQ, ~c tupleindividuals arity |~x|, signature. call P = hT , A, q, ~c, Query AbductionProblem (QAP). explanation (or, solution to) P -ABox E(i) ontology hT , Ei consistent,(ii) ~c cert(q, , E).set explanations P denoted expl(P). predicates onesallowed explanations, hence call abducible predicates. (T , A, q) , sayP unrestricted explanation signature; otherwise, contain symbols(T , A, q), say P restricted explanation signature.QAP, call tuple ~c negative answer q hT , Ai, ~c/ cert(q, , A).Clearly, query q ontology hT , Ai admits negative answer hT , Ai consistent.Also, condition (i), ontology inconsistent, P admit explanations.Ontology languages, DL-Lite , allow specification existentialrestrictions negative constraints (e.g., disjointness axioms), sometimes require explanations introduce fresh individuals occur within QAP. next preciselycharacterize individuals.Definition 3.2. Let P = hT , A, q, ~c, QAP let E solution P. arbitraryindividual u occurring E anonymous occur , A, q, ~c.Now, use example highlight query abduction problems usefuldebugging negative query answers.Example 3.1. Let Au following set assertions particular university.DPhil(Anna)enroll(Anna, KR)enroll(Luca, IDB )DPhil(Beppe)teach(Marco, KR)teach(Carlo, IDB )is, Anna Beppe doctoral students, Anna enrolled KR course,taught Marco, Luca enrolled introductory DB course (IDB ),taught Carlo. Now, consider following DL-Lite TBox Tu formalizing universitydomain, Au (partial) instance.enroll v Studentenroll v CourseDPhil v Studentteach v Lecturerteach v CourseCourse v teachTu models objects domain enroll Students, objects domainteach Lecturers, whereas objects range enroll teach Courses. Amongstudents DPhil students. Finally, every Course must taught someone.Now, assume university administration interested findingteaching course least one enrolled students doctoral student,captured following query.qu (x) teach(x, y), enroll(z, y), DPhil(z)641fiCalvanese, Ortiz, Simkus & StefanoniAssume Carlo expected part result. case, Lucastudent Carlo known DPhil student. Hence Carlo/ cert(q, , A)Carlo negative answer. Suppose complete informationpredicates enroll teachthat is, latter predicates abducible. easyseeEu = {teach(Carlo, c), enroll(Beppe, c), enroll(Luca, c)}explanation QAP Pu = hTu , Au , qu , Carlo, {enroll, teach}i, suggestsexistence course, represented anonymous individual c, occurABox Au .example shows certain explanations may assumptiveinclude assertions required solve problem. Indeed, examplesexplanation reason assume Luca enrolled anonymous coursec. following, examine various restrictions expl(P) reduce redundancyexplanations, achieved introducing preference relation among explanations.relation reflexive transitivethat is, pre-order among explanations.pre-order expl(P), write E E 0 E E 0 E 0 E.Definition 3.3. preferred explanations expl (P) QAP P pre-order ,called -explanations (-solutions), defined follows.expl (P) = { E expl(P) | E 0 expl(P) E 0 E }consider two preference orders commonly adopted comparing abductive solutions: subset-minimality order, denoted , minimum explanationsize order, denoted . latter order defined E E 0 iff |E| |E 0 |. Consideringthat, definition, explanations finite, arbitrary QAP P,-solution P also -solution P; is, expl (P) expl (P).Example 3.2. already argued, ABox Eu redundant solution QAP Puintroduced Example 3.1. Next, introduce two minimal solutions. First, considersolution asserting Carlo teach anonymous course c Beppe enrolledcourse. ABox Eu0 = {teach(Carlo, c), enroll(Beppe, c)} -explanation. Second,consider solution asserting Beppe enrolled IDB course. ABoxEu00 = {enroll(Beppe, IDB )} -explanation (and hence also -explanation).context logic-based abduction, four main decision problems considered interest (Eiter & Gottlob, 1995), parametrized according chosenpreference order .~ abducible predicate ,Definition 3.4. Given QAP P, ABox assertion (d)ABox E, define following decision problems.-exist(ence): exist -explanation P?~ occur -explanations P?-nec(essity): assertion (d)~ occur -explanation P?-rel(evance): assertion (d)642fiReasoning Explanations Negative Query Answers DL-Lite-rec(ognition): ABox E -explanation P?Whenever preference applied (i.e., identity), omit writefront problems names.paper, study complexity reasoning problems query abduction. start highlighting, remaining part section, interesting propertiesquery abduction problems important connections reasoning tasks.3.1 Reductions Reasoning Problemsshow introduced problems reduced other. Unlessotherwise stated, reductions present work DLs, instance queriesUCQs, restricted unrestricted explanation signatures.start showing nec least hard non-exist (i.e., complementexist problem).Proposition 3.1. every DL, non-exist reducible nec.~ arbitrary ABox assertion,Proof. Assume QAP P = hT , A, q, ~c, let (d)~d~ occur P. following holds: P explanation iff (d)0necessary P = hT , A, q, ~c, {}i. construction, follows solutionP also solution P 0 ; furthermore, solution E 0 P 0 , 6 (, E 0 , ) impliesE 0 solution P. definition P 0 since d~ globally fresh,~ABox E, E explanation P 0 E \ {(d)}0explanation P . correctness reduction immediately follows.QAPs restricted explanation signatures, next show nec reducesnon-exist. reduction works every DL allows disjointness axioms.Proposition 3.2. every DL allows concept role disjointness axioms,restricted explanation signatures, nec reducible non-exist.Proof. Consider instance nec given QAP P = hT , A, q, ~c, might~ Next, show construct QAP P 0restricted, ABox assertion (d).0~(d) necessary P iff P admit solutions. end, let 0two globally fresh predicates arity ; furthermore, let TBox 0 , ABox A0 ,signature 0 follows.0 := {0 v } { v 0 }~A0 := {(d)}0 := { | 6= } {0 }Finally, let P 0 := hT 0 , A0 , q, ~c, 0 i. Now, show correctness reduction; is,~ necessary P iff P 0 admit solutions.(d)() prove contrapositive. Suppose P 0 solution E 0 . definition~ 6 E 0 predicate occur E 0 .hT 0 , A0 0 , 0 (d)Let ABox E defined follows.E := {(~t) E 0 | 6= 0 } {(~t) | 0 (~t) E 0 }643fiCalvanese, Ortiz, Simkus & Stefanoni~ remains showconstruction, E -ABox contain (d).E solution P. end, please observe model J hT 0 , A0 E 0model hT , Ei, since 0 v 0 . addition, model hT , Eiextended model J hT 0 , A0 E 0 setting 0J := {(~t)J | 0 (~t) E 0 }~ J }. follows hT 0 , A0 E 0 conservative extension hT , Ei. GivenJ := {(d)~c cert(q, 0 , A0 E 0 ) q hT , Ai, obtain ~c cert(q, , E).Furthermore, since hT 0 , A0 E 0 consistent, also hT , Ei consistent;~ required.E solution P contain assertion (d),() prove contrapositive. Suppose solution E P exists~(d) 6 E. Let ABox E 0 defined follows.E 0 := {(~t) E | 6= } {0 (~t) | (~t) E}~ remains showconstruction, E 0 0 -ABox contain 0 (d).00000E solution P . seen before, hT , E conservative extension hT , Ei. Given ~c cert(q, , E), obtain ~c cert(q, 0 , A0 E 0 ).~ 6 E 0 , also hT 0 , A0 E 0Furthermore, since hT , Ei consistent 0 (d)consistent; E 0 solution P 0 , required.simple modification Proposition 3.2 shows result applies also DLsallow negative ABox assertions form A(c) P (c, c0 ) instead disjointnessaxioms. next show rel exist mutually reducible.Proposition 3.3. every DL, rel exist mutually reducible.Proof. First, show reduce rel exist. Let P arbitrary QAP~ arbitrary ABox assertion .form hT , A, q, ~c, let (d)0~ relevant P P 0 admits solution.construct QAP P (d)~end, let A0 ABox defined A0 = {(d)}.Then, define QAP P 000P = hT , , q, ~c, i. Next, prove correctness reduction. only-if directionimmediate. direction, suppose P 0 admits solution E 0 . follows,~ consistent TBox . Moreover, latterdefinition P 0 , -ABox E 0 {(d)}ABox also solution P and, therefore, given assertion relevant.Second, prove exist reducible rel. Let P arbitrary QAPform hT , A, q, ~c, i, let arbitrary predicate , let d~ arbitrary tupleindividuals occurring P d~ arity predicate . prove~ relevant P. direction follows definitionP admits solution iff (d)~relevance. show only-if direction, suppose P admits solution E. (d)~occurs E, relevant P. Otherwise, since individuals occur P ,~ also solution P, hence (d)~ relevant P.ABox E {(d)}Moreover, -nec nec also mutually reducible.Proposition 3.4. every DL, -nec nec mutually reducible.~ (d)~Proof. arbitrary QAP P arbitrary ABox assertion (d),~occurs -minimal explanations P iff (d) occurs explanations P. Thus,nec -nec equivalent problems.644fiReasoning Explanations Negative Query Answers DL-LiteFinally, since preference orders prefer smaller explanations and, definition,explanations finite, orders well-founded. immediately follows existsexplanation arbitrary QAP P P admits minimal explanationpreference orders.Proposition 3.5. every DL, -exist, -exist, exist mutually reducible.3.2 QAPs Query Emptiness Problemmentioned introduction, deciding existence explanation relatedquery emptiness problem studied Baader et al. (2010). Since rely probleminfer complexity bounds throughout paper, briefly introduce here.Definition 3.5. Let DL TBox, Q {IQ, CQ} query language, signature.say Q-query q empty given every -ABox consistentcert(q, , A) = . Otherwise, say q non-empty given. Q non-emptiness problem consists deciding, input , q, , whether qnon-empty given .Next, first show that, every DL, instance queries Boolean UCQs,query non-emptiness reduces exist. Then, show DL-Lite case holdseven arbitrary UCQs.Proposition 3.6. every DL instance queries Boolean UCQs, Q nonemptiness reducible exist.Proof. Let arbitrary DL TBox, let q IQ CQ arbitrary queryq CQ implies q Boolean UCQ, let arbitrary signature. showconstruct QAP P q non-empty given iff P admits solution.end, let ~c arbitrary tuple q CQ implies ~c = hi, q IQimplies ~c = hai globally fresh individual. Clearly, qnon-empty given iff P = hT , , q, ~c, admits solution.relationship CQ non-emptiness exist tightened, restrictattention DL-Lite TBoxes.Proposition 3.7. DL-LiteA , CQ non-emptiness reducible exist.Proof. Consider DL-Lite TBox , signature , time n-ary query q CQ.W.l.o.g., assume q CQ. Then, cannot immediately extend proof givenBoolean CQs introducing n (distinct) individuals since might forced matchdistinct answer variables q individual ABox witnessing non-emptinessq. However, adapt proof case follows. let N fresh atomicconcept occurring (T , , q). define 0 = {N } let q 0 BooleanCQ at(q 0 ) = at(q) {N (x1 ), . . . , N (xn )}. Finally, let P = hT , , q 0 , hi, 0QAP. following, show q non-empty given iff P admits solution.() Suppose q non-empty given . is, exists -ABoxhT , Ai consistent exists n-ary tuple ~a = ha1 , . . . , individuals645fiCalvanese, Ortiz, Simkus & Stefanoni~a cert(q, , A). Now, consider 0 -ABox E = {N (ai ) | 1 n}.Since N fresh predicate, hT , Ei conservative extension hT , Ai.is, model hT , Ai extended model hT , Ei, every modelhT , Ei also model hT , Ai. assumption hT , Ai consistent~a cert(q, , A), conclude E solution P.() Suppose P admits solution E. follows hT , Ei consistentmodel hT , Ei, exists match q 0 |= q 0 . Since Nfresh predicate occurring answer variable xi q atom N (xi )contained q 0 , (xi ) = aIi ai NI N (ai ) E.follows ~a cert(q, , E). Consider -ABox obtained E removingassertions N ; immediately follows hT , Ei conservative extension hT , Ai.Therefore, also ~a cert(q, , A) and, thus, q non-empty given .Proposition 3.7 generalized Horn DLsthat is, DLsanswering instance conjunctive queries reduces evaluating input querysingle, canonical model ontology. follows DL-Lite and, general,Horn-DLs, deciding exist generalizes query non-emptiness problem. Hence,hardness results non-emptiness obtained Baader et al. (2010) holdinstance queries UCQs apply also exist problem restricted explanationsignatures. However, since also consider ABoxes require specific tuplequery answer, converse hold always transfer upperbounds setting.3.3 Canonical Explanationsstudying complexity reasoning query abduction problems, first showrestrict search explanations. order so, define notioninstantiation conjunctive query.Definition 3.6. Let q n-ary CQ answer variables hx1 , . . . , xn i; furthermore, let~c = hc1 , . . . , cn tuple individuals. Let mapping terms q NIidentity NI answer variable xj q (xj ) = cj .Then, call ABoxE = {A((t)) | A(t) at(q)} {R((s), (t)) | R(s, t) at(q)}~c-instantiation q. Given DL ontology O, additionally that,quantified variable y, (y) distinct anonymous individual uy occurring q O,say E direct O.Note following distinguish instantiations differassignment anonymous individuals variables. Hence, CQ finitenumber distinct instantiations, unique direct one.3.3.1 Unrestricted Explanation Signatureobtain explanation QAP P unrestricted explanation signature,iterate set possible instantiations input query, searching one646fiReasoning Explanations Negative Query Answers DL-Liteinstantiation consistent input ontology. absence UNA,even consider one single instantiation CQ: direct instantiation,existentially quantified variables mapped distinct anonymous individuals.presence UNA, underlying DL expressive enough enforce inequalitiesindividuals occurring P (e.g., means disjointness axioms), reduceproblem searching CQ whose direct instantiation consistent inputontology, UNA dropped.Proposition 3.8. Let = hT , Ai arbitrary DL ontology let P = hT , A, q, ~c,arbitrary QAP unrestricted explanation signature. Furthermore, qi q,let Eqi direct ~c-instantiation qi O. following hold:1. UNA, solution P exists iff ~c-instantiation E qi q existshT , E consistent.2. Without UNA, solution P exists iff query qi q exists hT , Eqiconsistent.3. Furthermore, suppose DL supports concept disjointness axioms.UNA, solution P exists iff query qi q exists hT 0 , A0 Eqi consistent without UNA, A0 0 extend quadratic numberassertions axioms, respectively.Proof. Consider arbitrary qi q let E arbitrary ~c-instantiation qi .first prove consistency hT , E (with without UNA) implies Esolution P (with without UNA, resp.). shows direction 1 2. Letmapping generating E suppose hT , E consistent. Letarbitrary model hT , E i. build match qi setting (t) = (t)Iterm qi . (xj ) = (xj )I = cIj answer variable xj , matchwitnesses ~c ans(q, I). Hence ~c cert(q, , E ) E solution P, desired.only-if direction 1, assume arbitrary solution E P, useshow exists ~c-instantiation E qi q hT , E consistent.Since E solution P, definition, exists model hT , EiUNA. Without loss generality, assume = NI c NIcI = c. Moreover, interpretation admits match qi q witnessing~c ans(qi , I). define mapping , let (t) = (t) term occurring qi .model E . Since also model hT , Ai, model hT , Eshows latter consistent, desired.only-if direction 2 shown similarly. Suppose P admits solution E.exists model hT , Ei (without UNA) admits matchqi q witnessing ~c ans(qi , I). obtain interpretation J modelhT , Eqi i, extend follows. every anonymous individual uy introducedEqi due existentially quantified variable y, let uy J = (y). resultinginterpretation model Eqi , since individuals uy occur ontology,modelhood hT , Ai preserved.3, use extended ABox A0 TBox 0 enforce UNA individuals occurring P. ABox A0 extends assertion Ac (c) individual647fiCalvanese, Ortiz, Simkus & Stefanonic occurring P, Ac fresh concept name. TBox 0 consists axiomsAc v Ac0 pairs c 6= c0 individuals occurring P. Since interpretationshT , Ai UNA hT 0 , A0 without UNA coincide, claim easily followsstatement 2 above.direct consequence proposition that, DLs, restrict searchexplanations result instantiating input query.Corollary 3.9. Let P = hT , A, q, ~c, QAP unrestricted explanation signature,let max(q) = maxqi q |at(qi )|, let max-terms(q) = maxqi q |qi |. P explanation,P explanation concepts roles q, max(q) atoms,max-terms(q) individuals.3.3.2 Restricted Explanation Signatureallow restricted explanation signatures, Proposition 3.8 hold anymore,search space possible explanations becomes significantly larger. seefollowing sections, notable effect complexity different decisionproblems. However, case DL-Lite , still show weaker versionproposition allows us restrict search instantiations queriesperfect reformulation input query q. Moreover, every -minimal explanationobtained way.Proposition 3.10. Let P = hT , A, q, ~c, QAP hT , Ai DL-LiteA ontology,let Rq,T perfect reformulation q w.r.t. . solution P exists~c-instantiation E qr Rq,T exists (i) hT , E consistent,(ii) E \ -ABox. Moreover, E 0 -minimal explanation implies queryqr Rq,T ABox E exist E ~c-instantiation qr E 0 = E \ A.Proof. first part claim shown analogously item 1 Proposition 3.8 (recallDL-Lite make UNA). direction, consider arbitrary qr Rq,Tlet E ~c-instantiation qr generated mapping . assume hT , Econsistent E \ -ABox. Then, show E \ solution P,suffices show existence match qr DB AE witnessing ~c cert(q, , AE ).easily obtained setting (t) = (t)I term qr . only-ifdirection, assume arbitrary solution E P use show exists ~cinstantiation E qr Rq,T satisfies conditions (i) (ii). Since E solutionP, definition, E -ABox, hT , Ei consistent, ~c cert(q, , E).Proposition 2.1, follows exists query qr Rq,T match qrDB AE witness ~c ans(qr , DB AE ). define mapping setting (t) = (t)term qr . Then, resulting ~c-instantiation E E E A,implies consistency hT , E E \ also -ABox desired.show second part claim, suppose E -minimal solution P.Proposition 2.1, exists qr Rq,T exists matchwitnessing ~c ans(qr , DB AE ). construct ~c-instantiation E qr follows:E = {A((t)) E | A(t) at(qr )} {R((t), (t0 )) E | R(t, t0 ) at(qr )}minimality E, E = E \ A.648fiReasoning Explanations Negative Query Answers DL-LiteSimilarly above, implies consider small explanations whose sizelinear size input query q, signature depends q,also signature input TBox .Corollary 3.11. Let P = hT , A, q, ~c, QAP hT , Ai DL-LiteA ontology.Furthermore, let max(q) = maxqi q |at(qi )|. P = hT , A, q, ~c, explanation,P explanation concepts roles q, max(q) atoms,2 max(q) individuals.4. Complexity Instance Queriesstudy complexity reasoning query abduction problems. considercomplexity unrestricted restricted explanation signatures, considerdifferent minimality criteria abductive solutions. measure complexityQAP P = hT , A, q, ~c, terms combined size , A, q, is,consider combined complexity. section, investigate complexity reasoningQAPs body input query consists single unary atomthat is,consider instance queries. following section, shall turn attention UCQs.4.1 Existence Explanationsgiving first complexity results, show that, instance queries, -minimal-minimal explanations coincide. see this, consider arbitrary QAP P = hT , A, q, c,q IQ let qr arbitrary CQ perfect reformulation Rq,T . Propositions 2.1 3.10, follows ~c-instantiation qr consistent hT , Aicontains explanation P; moreover, -minimal explanation P obtainedway. explanations contain one assertion (cf. Proposition 2.1), -minimal explanations size one, obtain following result.Proposition 4.1. Let P = hT , A, q, ~c, QAP hT , Ai DL-LiteA ontologyq IQ, let E arbitrary -ABox. Then, E solution P impliessolution E 0 E P exists |E 0 | 1. Hence, expl (P) = expl (P).consider complexity deciding existence explanation.Theorem 4.2. DL-LiteA , instance queries, unrestricted restrictedexplanation signatures, exist, -exist, -exist NL-complete.Proof. Proposition 3.5, suffices show result exist. first providealgorithm yields desired upper bound, even restricted explanation signatures.show problem NL-hard already case unrestricted signatures.(membership) Let P = hT , A, q, c, QAP q IQ. decide existnon-deterministic logarithmic space, exploit Proposition 4.1 test candidatesingleton explanations iterating , individuals occurring P,two anonymous individuals. results polynomially many candidate solutions E constant size. test whether hT , Ei consistentc cert(q, , E). Since DL-Lite ontology consistency instance checkingsolved non-deterministic logarithmic space, exist NL.649fiCalvanese, Ortiz, Simkus & StefanoniAlgorithm 1 isNEC~ hT , Ai DL-Lite ontology,Input: QAP P = hT , A, q, ~c, assertion (d)q IQ CQ, unrestricted, .~ necessary P.Output: yes iff (d)1: Let globally fresh predicate arity .~2: Let 0 := { v } let A0 := {(d)}.003: hT , , q, ~c, admits solution, return no.~4: Let set individuals occurring P d.Let u globally fresh anonymous individual.~ 6 E-ABoxes E individuals {u} s.t. |E | 1 (d)~ hT , E , q, ~c, admits solution, return no.7:hT , E |= (d)8: end9: Return yes.5:6:(hardness) reduce DL-Lite ontology consistency problem (under UNA)exist. Consider arbitrary DL-Lite ontology hT , Ai. Furthermore, considerarbitrary atomic concept occurring hT , Ai, let q = A(x), let c NI arbitraryindividual, let P = hT , A, q, c, QAP unrestricted . show hT , Aiconsistent P admits solution. direction trivial. onlyif direction, suppose hT , Ai consistent, consider E = {A(c)}. Since hT , Aiconsistent fresh, hT , Ei also consistent. model hT , Eisatisfies assertion A(c), E solution P.4.2 Deciding NecessitySection 3.1, seen QAPs restricted explanation signaturesDLs allow disjointness axioms, nec reduces non-exist. case QAPsunrestricted explanation signatures ontologies restricted DL-Lite , provideAlgorithm 1 Turing reduction (non-)exist; is, procedure solves necemploying subroutine solving exist. following proposition proves correctness.Proposition 4.3. DL-LiteA , instance queries UCQs, unrestricted explanation signatures, algorithm isNEC decides nec.Proof. Let P = hT , A, q, ~c, QAP hT , Ai DL-Lite ontology, query~ assertionq IQ CQ, signature unrestricted; furthermore, let (d)~abducible predicate . prove (d) necessary P iff isNEC returns yes.only-if direction, prove contrapositive. Suppose isNEC returns~ 6 E. Accordinggiven instance; show solution E P exists (d)construction isNEC, consider two alternative cases.QAP hT 0 , A0 , q, ~c, admits solution E. DL-Lite , Calvanese et al. (2009)showed negative inclusion axioms affect consistency given ontology,contribute towards computing certain answer; is, ~c cert(q, 0 , A0 )~iff hT 0 , A0 consistent ~c cert(q, , A0 ). Then, since assertion (d)00predicate occurring P hT , consistent, E also solution650fiReasoning Explanations Negative Query Answers DL-Lite~ sinceP = hT , A, q, ~c, i. definition, solution contain (d),000~ v .hT , |= (d)QAP hT 0 , A0 , q, ~c, solution. Since isNEC returns no, -ABox E ex~ 6 E , hT , E |= (d),~ QAP hT , E , q, ~c,ists |E | 1, (d)~ entailed hT , Esolution E. Given assertion (d)0~E := E \ {(d)} also solution hT , E , q, ~c, i. conclude E 0 E~ required.solution hT , A, q, ~c, contain (d),direction, prove contrapositive. Suppose -ABox E exists~ 6 E; show isNEC returns no. W.l.o.g.,E solution P (d)~ Eindividual u Algorithm 1 occur E. Now, hT , Ei 6|= (d),00solution QAP hT , , q, ~c, i, isNEC returns no, required. Otherwise, consider~ take conjunctive query q 0 (~x) (~x).case hT , Ei |= (d)assumption, d~ cert(q 0 , , E). Proposition 2.1, query r Rq0 ,Tmatch r exist r contains single atom d~ ans(r, DB AE ) witnessed. Let (~y ) unique atom occurring r ~x ~y let (~t)assertion obtained (~y ) replacing variable ~y (y). Clearly,(~t) E. Next, distinguish among two cases.variable ~y (y) I. Then, let E := , (~t) A,~ 6 E ,let E := {(~t)}, (~t) E. either case, (d)~ E E. Hence, E solution QAP hT , E , q, ~c, i;hT , E |= (d),isNEC returns no, required.Variable ~y exists (y) 6 I. Given d~ I, d~ ans(r, DB AE ),predicates arity 2, d~ form d~ := hdi, NC ,NR . follows CQ r form r(x) (x, y) r(x) (y, x). Next,consider former case only, case symmetrical. Then, assertion(~t) form (d, (y)). Since (y) 6 I, (d, (y)) E. Now, letE 0 ABox obtained E replacing occurrence individual (y)individual u Algorithm 1. Since E 0 obtained solution E uniformlyreplacing anonymous individual individual occur E P,E 0 also solution P. definition, (d) 6 E 0 (d, u) E 0 .Now, let E := {(d, u)}. Since ans(r, DB AE ) witnesseddefinition E , hT , E |= (d). last, since E E 0 E 0solution P, conclude ABox E 0 solution hT , E , q, ~c, i. Hence,isNEC returns no, required.Next, use Algorithm 1 Propositions 3.1 3.2 characterize complexitynec presence instance queries.Theorem 4.4. DL-LiteA , instance queries, unrestricted restrictedexplanation signatures, nec, -nec, -nec NL-complete.Proof. NL upper bound nec restricted signatures, observe that,Proposition 3.2, nec reduces non-exist. Theorem 4.2, proved exist NL.651fiCalvanese, Ortiz, Simkus & StefanoniGiven NL = coNL, nec NL well. NL upper boundcase unrestricted signature established using algorithm isNEC Proposition 4.3.Indeed, given NL = coNL, non-exist coNL, checking whetherassertion entailed DL-Lite ontology coNL well, immediately obtainisNEC runs nondeterministic logarithmic space. coNL-hardness thus also NLhardness nec follows Proposition 3.1 Theorem 4.2. addition, Proposition 3.4states nec -nec equivalent and, thus, also -nec NL-complete. Finally,Proposition 4.1, conclude -nec NL-complete.4.3 Deciding RelevanceProposition 3.3, deciding relevance assertion QAP equivalent assessingwhether QAP admits solution. already showed latter problem NL-complete(see Theorem 4.2). Therefore, following result easily follows.Theorem 4.5. DL-LiteA , instance queries, unrestricted restrictedexplanation signatures, rel NL-complete.next theorem, show complexity problem change evenapply minimality criterion solutions.Theorem 4.6. DL-LiteA , instance queries, restricted unrestrictedexplanation signatures, -rel -rel NL-complete.Proof. Proposition 4.1, suffices show -rel NL-complete.~(membership) Let P = hT , A, q, c, QAP q IQ let (d)~ -relevant PABox assertion abducible predicate . argue (d)~~(i) c 6 cert(q, , A), (ii) hT , {(d)}i consistent, (iii) c cert(q, , {(d)}).show only-if direction, since direction directly follows Proposition 4.1~ -relevant P. definitiondefinition solution. Suppose (d)minimal solution, follows c 6 cert(q, , A). Also, Proposition 4.1, follows~ -solution P. then, c cert(q, , {(d)})~{(d)}~ontology hT , {(d)}i consistent. Since conditions (i-iii) decidednon-deterministic logarithmic space DL-Lite ontologies, conclude that, instancequeries (un)restricted explanation signatures, -rel NL.(hardness) Hardness proved employing reduction Theorem 4.2taking A(c) assertion shown relevant. Proposition 4.1,hT , Ai consistent A(c) -relevant P.4.4 Deciding RecognitionFinally, consider problem deciding whether given ABox solution QAP.Theorem 4.7. DL-LiteA , instance queries, unrestricted restrictedexplanation signatures, rec NL-complete.Proof. (membership) Let P = hT , A, q, c, QAP (where may restricted)q IQ let E ABox. definition solution QAP, decide652fiReasoning Explanations Negative Query Answers DL-Lite-existunrestr.restr.nonePTimeNP-nec-rel-recunrestr.restr.unrestr.restr.PTimecoNPPTimeNPPNPkPTimePNPkcoNPP2unrestr.restr.NPDPP2DPTable 5.1: Complexity reasoning QAPs UCQs DL-Lite . entriestable denote completeness results, except -rel unrestricted explanation signatures.whether E expl(P) three steps: (i) check E -ABox, (ii) check hT , Eiconsistent, (iii) check c cert(q, , E). DL-Lite ontologies,perform three steps non-deterministic logarithmic space. Thus, instance queriesrestricted unrestricted signatures, rec NL.(hardness) provide reduction consistency problem DL-Lite ontologies. Consider arbitrary ontology hT , Ai. Then, let fresh concept nameoccurring ontology let c fresh individual. Furthermore, let q(x) A(x)instance query. Finally, let P = hT , A, q, c, query abduction problemunrestricted explanation signature let E = {A(c)} target ABox.difficult see hT , Ai consistent iff E solution P.Unsurprisingly, complexity change consider minimality criterionsolutions.Theorem 4.8. DL-LiteA , instance queries, unrestricted restrictedexplanation signatures, -rec -rec NL-completeProof. Proposition 4.1, focus -rec.(membership) order decide whether E expl (P) first check E indeedsolution P, non-deterministic logarithmic space (see Theorem 4.7).Then, Proposition 4.1, need check |E| 1 E empty ABoxwhenever c cert(q, , A). Since instance checking DL-Lite NL, conclude-rec NL well.(hardness) reuse reduction consistency DL-Lite provided Theorem 4.7 show that, instance queries unrestricted explanation signatures,-nec NL-hard. conclude that, restricted unrestricted explanationsignature, -nec -nec NL-complete.5. Complexity Unions Conjunctive Queriessection, consider general problem reasoning query abductionproblems admit UCQs input. establish complexity various rea653fiCalvanese, Ortiz, Simkus & StefanoniAlgorithm 2 someExplanationInput: QAP P = hT , A, q, ~c, i.Output: yes iff P explanation.1: Guess CQ qr perfect reformulation Rq,T q w.r.t. .2: Guess ~c-instantiation E qr .3: E \ -ABox hT , E consistent, return yes.4: Return no.soning tasks problems DL-Lite , unrestricted restricted explanation signatures, different minimality criteria. results sectionsummarized Table 5.1.5.1 Existence Explanationsfirst focus problem deciding whether query abduction problem unrestricted signature admits least one explanation.follows Proposition 3.8 complexity problem coincidescomplexity deciding consistency without UNA underlying DL. Proposition 3.5, extends -exist, -exist. Since reasoning without UNAPTime-complete DL-Lite (Artale et al., 2009), obtain following result.Theorem 5.1. every DL L, UCQs, unrestricted explanation signatures,exist, -exist, -exist complexity consistency checking withoutUNA L. Hence DL-LiteA , mentioned problems PTime-complete.allow restricted explanation signatures, deciding exist becomes harder.DL-Lite , complexity increases PTime NP.Theorem 5.2. DL-LiteA , UCQs, restricted explanation signatures, exist,-exist, -exist NP-complete. NP-hardness holds already following restricted settings:1. QAPs TBox contains concept inclusions forms A1 v A2A1 vA2 concept names A1 A2 , ABox empty, query BooleanCQ consisting conjunction unary atoms single quantified variable.2. QAPs empty TBox.Proof. Proposition 3.5, sufficient show exist NP-complete.(membership) upper bound follows guess-and-check Algorithm 2,immediate Proposition 3.10. guesses non-deterministically CQ qr perfectreformulation Rq,T q w.r.t. , ~c-instantiation E qr . algorithm checkspolynomial time E \ -ABox ontology hT , E consistent;shown Calvanese et al. (2009) latter check polynomial.(hardness) Next, provide two hardness results. first one follows directlyProposition 3.7 hardness proof CQ query emptiness sublogic654fiReasoning Explanations Negative Query Answers DL-LiteDL-LiteA known DL-Litecore given Theorem 17 Baader et al. (2010). showing hardness second setting, reduce following NP-complete problem: givenpair directed graphs G = (V, E) G0 = (V 0 , E 0 ), decide whether exists homomorphism G G0 . end, let = {e(ca , cb ) | (a, b) E 0 } ABox.Furthermore, B arbitrary atomic concept c globally fresh individual, letq = {e(xa , xb ) | (a, b) E} {B(c)} Boolean CQ = {B} signature. Finally, let PG,G0 = h, A, q, QAP; show exists homomorphismG G0 iff solution PG,G0 . Indeed, homomorphism G G0 ,{B(c)} solution P. direction, assume explanation EP. Since binary atoms prohibited occurring E selection ,must exist match q DB . mapping also witnesses existencehomomorphism G G0 .5.2 Deciding NecessityNow, consider problem checking whether assertion occurs solutionsQAP P; is, whether assertion necessary P. case restrictedexplanation signatures, use reductions Section 3.1 Theorem 5.2 derivenec -nec coNP-complete. case unrestricted explanation signatures, use procedure solving nec described Algorithm 1 show nec-nec PTime-complete.Theorem 5.3. DL-LiteA , UCQs, unrestricted explanation signatures, nec-nec PTime-complete. Furthermore, restricted explanation signatures,nec -nec coNP-complete.Proof. Theorem 5.1 Theorem 5.2, proved problems deciding existence solution QAP unrestricted restricted explanation signaturesPTime-complete NP-complete, respectively. applying reduction Proposition 3.1, nec PTime-hard unrestricted coNP-hardrestricted explanation signatures.upper bound, first consider case restricted explanation signatures.Proposition 3.2, nec reduces non-exist. Theorem 5.2, latter problemsolved nondeterministic polynomial time. readily obtain nec coNP.case unrestricted signatures, Proposition 4.3 states algorithm isNEC solves nec,even consider UCQs input. definition, isNEC requires checking whetherpolynomially many QAPs admit solution, whether polynomially many DLLite ontologies entail given assertion. Since DL-Lite , instance checking PTimeand, Theorem 5.1, non-exist PTime, conclude isNEC runs polynomialtime. Thus, nec unrestricted signatures PTime.conclude nec PTime-complete unrestricted coNP-completerestricted explanation signatures.Finally, Proposition 3.4 states nec -nec equivalent and, thus, also -necPTime-complete unrestricted coNP-complete restricted explanationsignatures.655fiCalvanese, Ortiz, Simkus & StefanoniNow, consider complexity -nec show that, common assumptions, problem harder nec. Intuitively, one first computeminimal size explanation, inspect explanations size.following, use [i..j] denote integer interval {i, . . . , j}.Theorem 5.4. DL-LiteA , UCQs, unrestricted restricted explanation signatures, -nec PNPk -complete. hardness holds already QAPsempty TBox CQ.Proof. structure proof follows. First, show -nec PNPk . Then,NPprove problem Pk -hard restricted signatures. Finally, arguereduction also used particular case unrestricted signatures.(membership) Consider arbitrary QAP P = hT , A, q, ~c, (where signaturemay restricted) let arbitrary ABox assertion. Corollary 3.9, knowP explanation, exists explanation whose size boundedmax(q) = maxqi q |at(qi )|. Observe hP, negative instance -nec iff[0..m] (a) P explanation E |E| = 6 E, (b) E-minimal. Thus, use auxiliary problem size-out, decide giventuple hP 0 , 0 , n0 i, P 0 QAP, 0 assertion, n0 integer, whetherexists explanation E 0 P 0 |E 0 | = n0 0 6 E 0 . Furthermore, problemno-smaller decide, given tuple hP 0 , n0 QAP integer, whetherexplanation E 0 P 0 |E 0 | < n0 . Observe size-out NP,no-smaller coNP. Take tuple = hA0 , B0 , . . . , , Bm i, Ai = hP, , iiBi = hP, ii, [0..m]. Due observation, occurs -minimalexplanations E P iff [0..m], one following holds: (i) Ai negativeinstance size-out, (ii) Bi negative instance no-smaller. builtpolynomial time size input, whether instances instances satisfy (i)(ii) decided making 2m parallel calls NP oracle. Thus obtainmembership PNPk .(hardness) give reduction OddMinVertexCover, PNPk -complete(Wagner, 1987). instance problem given graph G = (V, E),asked whether least cardinality vertex covers G odd. is,odd integer k [1..|V |] G vertex cover C |C| = k,vertex cover C 0 G |C 0 | < k?reduction exploit following property. Given integer k directedgraph G = (V, E) vertices, construct new graph G0 = ([1..m], E 0 )exist two symmetric edges [1..k] j [1..m]. following holds:injective homomorphism h G G0 , G vertex cover size k.Indeed, take C = {v V | h(v) k}. Due injectivity, |C| = k. Assume arbitraryedge {v1 , v2 } E. Since h homomorphism, due selection edges musth(v1 ) k h(v2 ) k. {v1 , v2 } C 6= selection C.Assume arbitrary graph G = (V, E) vertices V = {v1 , . . . , vm }. W.l.o.g.,G connected, directed, least 2 nodes. construct next QAP PG =h, A|V | , qG , hi, G assertion G G positive instance OddMinVertexCover iff G -necessary PG . reduction use individuals odd , even, cij ,i, j [0..m], concept names , L, roles P , 6=, Edge.656fiReasoning Explanations Negative Query Answers DL-LiteoddA0LLA1LLLA2LLLA3LA4LevenFigure 1: structure A|V | graph G = (V, E) 4 vertices. Solid arcsA` represent assertions Egde(a, b) A` introduced (b). dashed arcABox A` individual par (`) represents collection assertionsrelate individual A` par (`) via role P .Let qG Boolean query consisting atoms(i) Edge(xi1 , xi2 ), edge (vi1 , vi2 ) E,(ii) 6=(xi1 , xi2 ), i1 , i2 [1..m], i1 6= i2 ,(iii) L(x1 ), . . . , L(xm ) P (x1 , y), (y).Intuitively, (i) represent graph G query. use atoms (ii)ensure different variables mapped distinct elements. atoms L(xi )used measure size vertex covers, atoms P (x1 , y) (y) useddetermine parity. allow explanations concept names, thus setG = {M, L}.define A|V | , first construct collection A0 , . . . , ABoxes, Ajconsists assertions(a) L(cji ), [j..m],(b) Edge(cji1 , cji2 ), i1 , i2 [1..m] i1 j i2 j,(c) 6=(cji1 , cji2 ), i1 , i2 [1..m] i1 6= i2 .integer k, let par (k) = odd k odd, par (k) = even, otherwise. Let A0 ={P (cji , par (j)) | i, j [0..m]}. A|V | = A0 A0 . See Figure 1 example.Finally, let G = (odd ). prove correctness reduction, defineup(k) = {L(ck1 ), . . . , L(ckk ), (par (k))}, claim following:claim 1: C vertex cover G size k, up(k) explanation PG .Let = A|V | up(k). suffices show existence match qG DB . Takeenumeration z1 , . . . , zm variables x1 , . . . , xm {z1 , . . . , zk } = {xi | vi C}.Take mapping (zi ) = cki [1..m], (y) = par (k). Assumeatom Edge(xi1 , xi2 ) qG . Due (b) definition Aj , suffices show(xi1 ) = ck` (xi2 ) = ck` ` k. Indeed, since C vertex cover, vi1 Cvi2 C. due enumeration variables, xi1 = z` xi2 = z` ` k.Due definition , (xi1 ) = ck` (xi2 ) = ck` ` k. atoms 6=(xi1 , xi2 )qG properly mapped due (c) construction Aj fact injective657fiCalvanese, Ortiz, Simkus & Stefanoniconstruction. atom L(xi ) qG two options. (xi ) = ck` ` k,L(ck` ) up(k) definition up(k). Otherwise, ` > k, L(ck` ) Akdefinition Ak . atom P ((x1 ), (y)) belongs due definition A0 ,((y)) up(k) construction up(k).claim 2: Assume up(k) explanation PG . G vertex cover size k.Let = A|V | up(k) let match qG DB . Observe due irreflexivityrole 6= atoms (ii) qG , must injective. Observe also ` [1..m],` 6= k, |{c`i | L(c`i ) A` }| < m. Due connectedness G atomsL(x1 ), . . . , L(xm ) qG , must use atoms Ak A0 up(k). is, alsomatch qG DB Ak A0 up(k) . Let C = {vi V | (xi ) = ckn , n [1..k]}. |C| = kdue injectivity . see C vertex cover, assume edge (vi1 , vi2 ) E.construction, qG atom Edge(xi1 , xi2 ). Since match DB Ak A0 up(k) ,Edge((xi1 ), (xi2 )) Ak . Then, construction Ak , (xi1 ) = ckn (xi2 ) = cknn k. selection C, {(xi1 ), (xi2 )} C 6= .claim 3: Assume E -minimal explanation PG size k. E = up(k1). Since G connected E -minimal, exist index ` [1..m]E {L(c`1 ), . . . , L(c`m ), (par (`))} match qG A` A0 E. SinceL(c`i ) A` [`+1..m] definition A` , cardinality minimalityE {L(c`1 ), . . . , L(c`` ), (par (`))}. definition A` , |{c`i | L(c`i ) A` }| = `.Thus, due injectivity match qG , must |{c`i | L(c`i ) E}| `.Hence, E = {L(c`1 ), . . . , L(c`` ), (par (`))} = up(`). Since |E| = k, ` = k 1.finalize correctness proof:() Suppose exists odd integer k [1..|V |] G vertex cover C|C| = k, vertex cover C 0 G |C 0 | < k. claim 1, up(k)explanation PG . make sure up(k) -minimal. Suppose existsexplanation E 0 size |E 0 | < |up(k)|, i.e., |E 0 | = ` ` k. assumeE 0 -minimal. claim 3, E 0 = up(` 1). follows claim 2 Gvertex cover size ` 1. Since ` 1 < k, arrive contradiction assumptionG vertex cover size < k. Thus up(k) -minimal. Since k odd,(odd ) up(k). claim 3, apart up(k) -minimal explanationPG . is, (odd ) occurs -minimal explanations PG .() Assume (odd ) occurs -minimal explanations PG . claim 3,know up(k) unique -minimal explanation, integer k. Since (odd )up(k), get k odd. Then, claim 2, vertex cover C size k.remains ensure vertex cover C 0 size ` < k. Assume opposite.claim 1 up(`) explanation size |up(`)| < |up(k)|,contradicts assumption up(k) -minimal. Thus G positive instanceOddMinVertexCover.definition G prohibits binary atoms occurring -minimal explanations.effect achieved using G = (, A|V | , qG ) modifying A|V | qGmake prohibitively expensive binary atoms -minimal explanations. Simplyreplace binary assertion r(c, d) A|V | fresh assertions r1 (c, d), . . . , rm+2 (c, d),binary r(x, y) qG r1 (x, y), . . . , rm+2 (x, y). way lower-boundshown unrestricted explanation signatures.658fiReasoning Explanations Negative Query Answers DL-Lite5.3 Deciding RelevanceUsing Theorems 5.1 5.2, reductions Section 3, obtain following results.Theorem 5.5. DL-LiteA , UCQs, unrestricted explanation signatures, relPTime-complete. restricted explanation signatures, rel NP-complete.Unsurprisingly, UCQs, -rel complexity -nec. Indeed, twoproblems share source complexity, namely need inspect explanationscomputed size, allows us reduce OddMinVertexCover problem.fact, PNPk -hardness shown using reduction proof Theorem 5.4,matching upper bound obtained slightly modifying algorithm -nec.Theorem 5.6. DL-LiteA , UCQs, unrestricted restricted explaNPnation signatures, -rel PNPk -complete. Pk -hardness holds already QAPsempty TBox CQ.Proof. First, show that, restricted explanation signatures, problem -relNPPNPk . Second, argue that, unrestricted explanation signatures, -rel Pk -hard.(membership) -rel tackled way similar -nec. fact, algorithmdescribed Theorem 5.4 modified order solve problem. Let size-in solvefollowing problem: given tuple hP, , ni, P QAP, assertion, ninteger, decide whether exists explanation E, |E| = n E. Then,change positivity condition -nec algorithm follows: occurs-minimal explanation E P iff [0..m] holds that: (i) Ai positiveinstance size-in, (ii) Bi positive instance no-smaller. easy seesize-in solvable NP, hence whole problem PNPk .(hardness) Recall reduction OddMinVertexCover -nec proofTheorem 5.4. argue exactly reduction also shows PNPk -hardness-rel. Assume directed graph G let PG G QAP assertionresulting reduction. prove claim suffices show following equivalence:G -necessary PG iff G -relevant PG . equivalence follows directlyclaim 3, states PG unique -minimal explanation.turn attention -rel. problem obtain precise complexitycharacterization case restricted explanation signatures, leave openwhether unrestricted signatures P2 upper bound shown tight.2 notelatter case, coNP lower bound easily shown, instance,reduction non-existence homomorphism two graphs.Theorem 5.7. DL-LiteA , UCQs, unrestricted restricted explanation signatures, -rel P2 . restricted explanation signatures, -rel P2 -hard,hardness holds already QAPs empty TBox CQ.Proof. (membership) Let P = hT , A, q, ~c, QAP let ABox assertion.provide extended version algorithm solving existence, decides whether2. proof P2 lower bound unrestricted signatures Theorem 2 Calvanese, Ortiz, Simkus,Stefanoni (2011) incorrect.659fiCalvanese, Ortiz, Simkus & Stefanoni-relevant P. Let has-subexpl solve problem deciding whether given explanation E subset explanation. modified algorithm, similarlyAlgorithm 2, first non-deterministically guess CQ qr perfect reformulation Rq,Tq w.r.t. ~c-instantiation E qr E . Additionally consistencytest checking E -ABox, also check complement has-subexplE, order assure E -minimal. follows -relevant. Since checkingcomplement has-subexpl done coNP, problem solvable P2 .(hardness) reduce P2 -complete problem non-cert3col (Stewart, 1991, seealso Bonatti, Lutz, & Wolter, 2009). instance non-cert3col given graphG = (V, E) vertices V = {1, . . . , n} every edge labelled disjunctiontwo literals Boolean propositions {p(i,j) | 1 i, j n}. say edge e Eevaluates true truth assignment satisfies disjunction labelling e. Then,graph G positive instance non-cert3col iff truth assignment existsgraph (G)obtained G including edges evalute true3-colorable. Assume instance G non-cert3col. show buildpolynomial time QAP PG = hTG , AG , qG , ~cG , G ABox assertion G . firstpresent relevant definitions, discuss intuition behind reductionprove correctness.construction, use empty TBox Boolean CQ, thus TG = ~cG = hi.order define ABox AG , let L function assigns edge e Eset {l1 , l2 } literals occurring label. Moreover, let T(e) (resp., F(e)) setcontaining truth assignment literals L(e) edge e evaluates true(resp., false) . Finally, truth assignment literal l occurringG, define image l w.r.t. , written img (l), follows.(l (l) =img (l) :=l otherwiseready define ABox AG . definition, use individuals a1 , . . . , a4 ;moreover, literal l G, use individuals l l denote ls truth value. Also,1 k ` 4, edge e E, truth assignment T(e) F(e), lete,k,`fresh individual. ABox AG consists four distinct components , AtT , AfT ,AC introduce next.={d(l, l), d(l, l) | literal l occurs G}{B(ak ) | 1 k 3}e,e,AtT ={Re (ak , k,`), (k,`, a` ) | e E, T(e), 1 k < ` 3}e,{P (k,`, img (l)) | e E, T(e), l L(e), 1 k < ` 3}e,e,AfT ={Re (ak , k,`), (k,`, a` ) | e E, F(e), 1 k ` 3}e,{P (k,`, img (l)) | e E, F(e), l L(e), 1 k ` 3}e,e,AC ={Re (a4 , 4,4), (4,4, a4 ) | e E, T(e) F(e)}e,{P (4,4, img (l)) | e E, T(e) F(e), l L(e)}660fiReasoning Explanations Negative Query Answers DL-LiteNext, define Boolean query qG . end, vertex V , let xidistinct variable; edge hi, ji E, let yi,j distinct variable; and, literall occurring G, let zl zl two distinct variables. Then, edge hi, ji E, letqG contain following atoms.{B(xi ), (xi , yi,j ), (yi,j , xj ), B(xj )} {P (yi,j , zl ), Al (zl ), d(zl , zl ) | l L(e)}Finally, let G = B(a4 ) assertion want show relevant letG = {Al | literal l occurs G} {B} signature.Now, outline main idea behind construction. ABox AG encodes two structures: triangular structure AtT AfT cyclic structure AC . former structureindividuals a1 , a2 , a3 edges G evaluate true accordingarbitrary truth assignment mapped non-reflexive edges (cf. AtT ). contrast, edges G evaluate false according mapped arbitrary edge(cf. AfT ). latter, cyclic, structure AC individual a4 (which assertedmember B) G mapped AC possible truth assignments.Query qG obtained graph G requiring vertex graphmember concept B, reifying edges graph, incorporating disjunctionliterals. particular, literal l G, variables zl zl represent truthvalues l atom Al (zl ) used enforce particular truth assignment. Since ABox AGcontain assertions concept Al , minimal explanation E PG correspondstruth assignment G. is, E contains, literal l G, either Al (l)Al (l). Also, definition ABox, query qG mapped AtT AfTminimal explanation E implies (G) 3-colorable. contrast, every truthassignment , map query qG cyclic structure AC , provided explanationE asserts individual a4 member B. ready formally provecorrectness reduction.() Suppose truth assignment (G) 3-colorable; showassertion B(a4 ) -relevant PG . Consider -ABox E = {B(a4 )} E ,E = {Al (l) | (l) = t} {Al (l) | (l) = f }. Clearly, E explanation. Indeed,match query qG cyclic structure AC mapping variables xi qG(interpretation of) a4 . Suppose smaller explanation E 0 E. Observe E E 0 .because, literal l, concept Al occur AG occur qG .Then, E \ {B(a4 )} must explanation. qG matched triangularstructure encoded AG . Thus, (G) 3-colorable contradicts assumption.() Let E -minimal explanation PG containing B(a4 ); showexists truth assignment (G) 3-colorable. first argueliteral l either Al (l) E Al (l) E. follows three considerations.First, due signature restriction, predicate cannot occur E. Second, literall, query qG contains atoms Al (zl ) d(zl , zl ), whereas ABox AG contains assertions d(l, l)d(l, l). Third, literal l, concept Al occurs qG one variablezl . Therefore, since E minimal solution, know exactly one Al (l) EAl (l) E holds. Next, define truth assignment literals occurring G.literal l G, let (l) = Al (l) E, (l) = f Al (l) E. difficultargue t(G) 3-colorable thus G positive instance non-cert3col.661fiCalvanese, Ortiz, Simkus & StefanoniIndeed, (G) 3-colorable, qG could mapped triangle structure AGmaking E \ {B(a4 )} smaller explanation, contradiction.5.4 Recognizing ExplanationsUnsurprisingly, UCQs unrestricted restricted explanation signatures,rec NP. Indeed, order solve problem, need check consistencyexplanation ontology, check whether given tuple certain answerquery. former polynomial latter NP.Theorem 5.8. DL-LiteA , UCQs, restricted unrestricted explanation signatures, rec NP-complete. NP-hardness holds already QAPsempty TBox CQ.Proof. usual, first show that, (un)restricted explanation signatures, recNP. Then, argue that, unrestricted explanation signatures, problemNP-hard.(membership) Given QAP P = hT , A, q, ~c, ABox E, devise algorithmdeciding rec follows. Firstly, procedure checks E indeed -ABox; checklinear E. makes sure extending ontology ABox E leadinconsistent theory; checked polynomial time (Artale et al., 2009).last, decides whether ~c occurs cert(q, , E); Proposition 2.1 feasibleNP. Hence overall algorithm runs non-deterministic polynomial time.(hardness) use essentially reduction existence homomorphism directed graphs G G0 proof Theorem 5.2, differenceinstead reducing existence explanation signature= {B}, leave signature unrestricted (that is, = (T , A, q)), reduceproblem deciding whether E = {B(c)} explanation.case preference order place, recognize explanation one check minimality well. check coNP-hard - -minimality, leading completenessDP.Theorem 5.9. DL-LiteA , UCQs, restricted unrestricted explanation signatures, -rec -rec DP-complete. DP-hardness holdsalready QAPs empty TBox CQ.Proof. first argue that, (un)restricted explanation signatures, two problemsDP. Then, unrestricted explanation signatures, prove -rec-rec DP-hard.(membership) Membership problem DP shown providing twolanguages L1 NP L2 coNP, set yes-instances L1 L2 .-rec, simply letL1 = {(P, E) | E expl(P)}L2 = {(P, E) | P explanation E 0 s.t. |E 0 | < |E|}-rec, take L1 L2 = {(P, E) | P explanation E 0 s.t. E 0 ( E}.662fiReasoning Explanations Negative Query Answers DL-Lite(hardness) DP-hardness shown reduction problem HP-noHP.instance HP-noHP given two directed graphs G = (V, E) G0 = (V 0 , E 0 ),hG, G0 positive instance iff G Hamilton path G0 one.pair hG, G0 i, define QAP P = h, A, q, hi, -ABox E that:(a) hG, G0 positive instance HP-noHP iff E -minimal explanation P,(b) hG, G0 positive instance HP-noHP iff E -minimal explanation P.W.l.o.g., nodes G G0 disjoint ordinary individuals. Construct ABoxAG = {e(vi , vj ) | (vi , vj ) E} {d(vi , vj ) | vi , vj V, vi 6= vi }. Intuitively, assertione(vi , vj ) encodes edge (vi , vj ) graph G, whereas assertion d(vi , vj ) encodesnodes vi vj distinct. ABox AG0 encodes G0 similar way before, using rolese0 instead e, addition assertion A(vi0 ) vi0 V 0 . Take set freshindividuals = {o1 , . . . , o|V 0 | } ABox AC = {e0 (oi , oj ), d(oi , oj ) | 1 6= j |V 0 |}.ABox P defined = AG AG0 AC .Let q = q1 q10 q2 q20 q3 Boolean CQq1q10q2q20q3====={e(x1 , x2 ), e(x2 , x3 ), . . . , e(x|V |1 , x|V | ))},{d(xi , xj ) | vi , vj V, vi 6= vj },{e0 (y1 , y2 ), e0 (y2 , y3 ), . . . , e0 (y|V 0 |1 , y|V 0 | )},{d(yi , yj ) | vi0 , vj0 V 0 , vi0 6= vj0 },{A(y1 ), . . . , A(y|V 0 | )}.Intuitively, q1 q10 asks simple path |V | vertices related via role e. Analogously,q2 q20 asks simple path |V 0 | vertices related via role e0 . Additionally, q3 asksnode latter path satisfies A.Finally, let E = {A(oi ) | oi O} let = (T , A, q).() Assume hG, G0 positive instance HP-noHP, let a1 , . . . a|V |Hamilton path G. show E -minimal -minimal explanation P.end, first take mapping variables q (x1 ) = a1 , . . . , (x|V | ) = a|V |(y1 ) = o1 , . . . , (y|V 0 | ) = o|V 0 | . clearly match q DB AE , henceE explanation P. Indeed, subquery q1 q10 q fulfilled a1 , . . . a|V |Hamilton path G, q2 q20 fulfilled AC clique size |V 0 |, q3fulfilled E. assure minimality, assume towards contradictionexplanation E 0 |E 0 | < |E| |E 0 | |E|. case, |E 0 | < |V 0 |. Assume 0 matchq DB AE 0 . Note AG AG0 share individuals. Since q3 q20 asks|V 0 | elements satisfying |E 0 | < |V 0 |, 0 must map variables y1 , . . . , y|V 0 | |V 0 |distinct individuals AG0 . presence q2 q implies existence Hamiltonpath G0 . Contradiction.() Assume E expl (P) (resp., E expl (P)) match q DB AE .Note e0 occur AG e occur AG0 AC . constructionq1 q10 AG , maps variables x1 , . . . , x|V | |V | distinct constants AGG must Hamilton path. Towards contradiction suppose G0 also Hamiltonpath. construction AG0 , q2 q20 q3 match DB AG0 . meansbuild match 0 q DB AG0 , turn means explanation P.contradicts assumption E -minimal (resp., -minimal).663fiCalvanese, Ortiz, Simkus & Stefanoni6. Discussionsection, discuss issues remain investigation.6.1 Computing Explanationscomplexity analysis DL-Lite , considered problem computingsolutions query abduction problems. Nevertheless, infer upper boundscomplexity computing solutions QAP P presented results. inputquery P instance query, computing arbitrary solution computingminimal3 solutions easy, since Proposition 3.10, need consider singleton candidate explanations, number polynomially bounded. problemcomputing arbitrary solution E remains polynomial UCQs signature Punrestricted, since always obtain E creating suitable direct instantiationone CQs input (see Section 3.3). Instead, restricted signatures, totalnumber (minimal) solutions general exponential size signaturemaximal size query occurring input UCQ; computingrequires general exponential time. remains investigated whether solutionsenumerated polynomial delay (cf., Penaloza & Sertkaya, 2010). caserestricted signature, however, NP-harness result established Theorem 5.2 impliescompute solution E one essentially essentially cannot better guessingABox E deciding whether E expl (P).6.2 Data Complexitywork focused combined complexity. respect data complexity(i.e., complexity measured respect size ABox only,query TBox considered fixed) ontology complexity (i.e.,query considered fixed), observe inference tasks shownNP-complete essentially rely checking ontology consistency, hence AC0 datacomplexity (Calvanese et al., 2009). Moreover, Corollaries 3.9 3.11, one restrictattention explanations bounded size query, followsfixed query, polynomially many explanations considered. Hencereasoning tasks polynomial data complexity ontology complexity.6.3 Description Logics.lower bounds proved paper rely properties exclusiveDL-Lite , hence hold DLs well. fact, mentioned, manylower bounds hold even absence TBox. upper bounds,relied DL-Lite existence perfect reformulation given query (seeProposition 2.1) argue canonical explanations small restrictedsignature (more specifically, obtained instantiating CQs perfectreformulation input query) query answering done NP.reason, expect results carry DLs admit small explanations3. Since every ABox superset solution solution, dont impose minimalitycondition, always exponential number solutions, provided one exists.664fiReasoning Explanations Negative Query Answers DL-Litequery answering NP. instance, lower upper boundsestablished hold OWL 2 QL, obtained DL-Lite forbiddingfunctionality assertions dropping unique name assumption (as resultsrely functionality axioms, unique name assumption irrelevant).expressive DLs, bounds complexity reasoning tasks alsoinferred. Corollary 5.1, showed QAPs unrestricted explanation signatures, deciding existence explanation complexity ontology consistency without UNA. Hence, problem ExpTime-complete extensionsALC standard reasoning (with without UNA) also ExpTime-complete,like well known SHIQ. consider restricted explanation signatures, problembecomes significantly harder. witnessed lower bounds Baader et al. (2010)stemming CQ-emptiness (see Proposition 3.6): exist already 2ExpTime hardALCI (Theorem 28 Baader et al., 2010), undecidable ALCF (Theorem 29).ALC, authors recently improved lower bound CQ-emptiness ExpTimeNExpTime (personal communication). mentioned Section 3, upper boundsapply directly setting (although expect extend),precise characterization reasoning problems considered paper expressiveDLs remains open.7. Related Workproblem explaining missing query answers first considered databasecommunity (Jagadish, Chapman, Elkiss, Jayapandian, Li, Nandi, & Yu, 2007).literature, found three different models explanation missing answers, differnotion solution. First, Chapman Jagadish (2009) proposed modelexplanations relational operations (e.g., natural joins selections)responsible preventing given tuple returned answers. Second, TranChan (2010) defined solutions refinements input query giventuple answer relaxed query database. Third, Huang, Chen, Doan,Naughton (2008) defined solutions sequences database update operationsresult answering given conjunctive query updated relationalinstance includes missing answer. Herschel Hernandez (2010) generalizedlatter model considering UCQs aggregation grouping. Althoughexplanation model closely related ours, work Huang et al. HerschelHernandez tackle problem point view computing solutions, whereasinterested outlining computational complexity problem. Moreover,spirit abductive reasoning, solutions declarative rather operationalnaturethat is, solutions databases rather sequence database operations.classical logic, abductive reasoning form non sequitur argument,conclusion B logical consequence premises ( 6|= B), even though Bassumed follow theory (Eiter & Gottlob, 1995). aim find setformulas |= B. Abductive reasoning important also contextDescription Logics (Elsenbroich, Kutz, & Sattler, 2006), three orthogonal abductiveproblems studied. First, abduction studied explain conceptsthatis, given two concepts C TBox , concept abduction amounts finding665fiCalvanese, Ortiz, Simkus & Stefanoniconcept H |= C u H v C u H satisfiable w.r.t. (Noia, Sciascio, &Donini, 2009; Bienvenu, 2008). Second, Hubauer, Grimm, Lamparter, Roshchin (2012)applied TBox abductive reasoning diagnosis complex systems. particular,given TBox , set abducible axioms Ax , set axioms O, TBox abductionamounts finding subset Ax |= O. Third, Klarman, Endriss,Schlobach (2011) studied problem ABox abduction ALC ontologies.problem consists finding additions need made ABox order forceset ABox assertions logically entailed ontology. Along line, Du,Qi, Shen, Pan (2011a) considered problem practical perspective.recently, Du, Wang, Qi, Pan, Hu (2011b) defined problem abductiveconjunctive query answering, use basis new approach semanticmatchmaking. Given satisfiable DL ontology CQ q, tuple ~c called abductiveanswer q w.r.t. exists set E ABox assertions E |= q(~c).Similarly approach, authors allow restrict signature abductivesolutions constructed. addition, one limit impact E specifyingset closed predicates; assertion closed predicate requireE |= |= . main contribution paper procedurecomputing abductive answers CQs ontologies formulated DLP fragmentOWL 2, fragment orthogonal DL-Lite terms expressiveness. Consideringclosed predicates context DL-Lite QAPs interesting research direction.8. Conclusionspaper studied problem explaining negative answers user queriesDL-Lite ontologies. formalized problem abductive task: given(U)CQ q, consistent ontology tuple constants ~c ~ccertain answers q O, explanation defined set ABox assertions that,added O, preserve consistency result ~c certain answers.considered special cases allowing restricted signature assertionsexplanation, instance query rather full (U)CQinput. also considered preference orders explanations, studied twoorders: subset minimal cardinality minimal explanations. cases,obtained complexity bounds four decision problems inspired knowledge baseabduction: deciding existence explanation (exist), deciding whether given assertionoccurs (nec) (rel) explanations, recognizing explanations (rec).complexity bounds tight, exception rel subset minimal explanationsunrestricted signatures, leave open gap coNP-hardnessmembership P2 .Specifically, shown case instance queries decision problems tractable, fact NL-complete, even restricted explanations signaturespreference orders simultaneously considered. picture significantly different(U)CQs, results Table 5.1 show. Indeed, tractability always lost soonone considers restricted explanations signatures. signatures restricted, considering preference order also results intractability cases, exceptionsexist, always tractable, nec, polynomial subset minimal666fiReasoning Explanations Negative Query Answers DL-Liteexplanations PNPk cardinality minimal ones. contrast nec, rel harder,common assumptions, subset minimal cardinality minimal explanations. rechard even explanations signature restricted preference orderconsidered.would interesting apply framework lightweight description logics,starting EL-family. Also, would like investigate minimalitycriteria. instance, semantic criteria allow one reward explanations less/moreconstraining terms models ontology.Acknowledgmentsauthors would like thank anonymous referees careful readingsubmitted manuscript valuable comments. work partially supportedAustrian Science Fund (FWF) grants P20840 T515, EU FP7 projects ACSI(Artifact-Centric Service Interoperation), grant agreement n. FP7-257593, Optique(Scalable End-user Access Big Data), grant agreement n. FP7-318338, AlcatelLucent EPSRC.ReferencesArtale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). DL-Lite familyrelations. J. Artificial Intelligence Research, 36, 169.Baader, F., Bienvenu, M., Lutz, C., & Wolter, F. (2010). Query predicate emptinessdescription logics. Proc. 12th Int. Conf. Principles KnowledgeRepresentation Reasoning (KR 2010).Bienvenu, M. (2008). Complexity abduction EL family lightweight descriptionlogics. Proc. 11th Int. Conf. Principles Knowledge RepresentationReasoning (KR 2008), pp. 220230. AAAI Press.Bonatti, P. A., Lutz, C., & Wolter, F. (2009). complexity circumscription description logics. J. Artificial Intelligence Research, 35, 717773.Borgida, A., Franconi, E., & Horrocks, I. (2000). Explaining ALC subsumption. Proc.14th Eur. Conf. Artificial Intelligence (ECAI 2000).Borgida, A., Calvanese, D., & Rodriguez-Muro, M. (2008). Explanation DL-Lite family description logics. Proc. 7th Int. Conf. Ontologies, DataBases,Applications Semantics (ODBASE 2008), Vol. 5332 Lecture Notes ComputerScience, pp. 14401457. Springer.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodriguez-Muro, M.,& Rosati, R. (2009). Ontologies databases: DL-Lite approach. Tessaris,S., & Franconi, E. (Eds.), Semantic Technologies Informations Systems 5th Int.Reasoning Web Summer School (RW 2009), Vol. 5689 Lecture Notes ComputerScience, pp. 255356. Springer.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractablereasoning efficient query answering description logics: DL-Lite family. J.Automated Reasoning, 39 (3), 385429.667fiCalvanese, Ortiz, Simkus & StefanoniCalvanese, D., Ortiz, M., Simkus, M., & Stefanoni, G. (2011). complexity conjunctivequery abduction DL-Lite. Proc. 24th Int. Workshop Description Logic(DL 2011), Vol. 745 CEUR Electronic Workshop Proceedings, http://ceur-ws.org/.Chapman, A., & Jagadish, H. V. (2009). not?. Proc. ACM SIGMOD Int.Conf. Management Data, pp. 523534.Du, J., Qi, G., Shen, Y.-D., & Pan, J. Z. (2011a). Towards practical ABox abductionlarge OWL DL ontologies. Proc. 25th AAAI Conf. Artificial Intelligence(AAAI 2011). AAAI Press.Du, J., Wang, S., Qi, G., Pan, J. Z., & Hu, Y. (2011b). new matchmaking approachbased abductive conjunctive query answering. Proc. Joint Int. SemanticTech. Conf. (JIST 2011), pp. 144159.Eiter, T., & Gottlob, G. (1995). complexity logic-based abduction. J. ACM,42 (1), 342.Elsenbroich, C., Kutz, O., & Sattler, U. (2006). case abductive reasoningontologies. Proc. 2nd Int. Workshop OWL: Experiences Directions (OWLED 2006), Vol. 216. CEUR Electronic Workshop Proceedings, http://ceur-ws.org/.Herschel, M., & Hernandez, M. A. (2010). Explaining missing answers SPJUA queries.Proc. VLDB Endowment, 3 (1), 185196.Horridge, M., Parsia, B., & Sattler, U. (2008). Laconic precise justifications OWL.Proc. 7th Int. Semantic Web Conf. (ISWC 2008), Vol. 5318 Lecture NotesComputer Science, pp. 323338. Springer.Huang, J., Chen, T., Doan, A., & Naughton, J. (2008). provenance non-answersqueries extracted data. Proc. VLDB Endowment, 1 (1), 736747.Hubauer, T., Grimm, S., Lamparter, S., & Roshchin, M. (2012). diagnostics frameworkbased abductive description logic reasoning. Proc. IEEE Int. Conf.Industrial Technology, (ICIT 2012), pp. 1047 1054.Jagadish, H. V., Chapman, A., Elkiss, A., Jayapandian, M., Li, Y., Nandi, A., & Yu, C.(2007). Making database systems usable. Proc. ACM SIGMOD Int. Conf.Management Data, pp. 1324.Klarman, S., Endriss, U., & Schlobach, S. (2011). ABox abduction description logicALC. J. Automated Reasoning, 46 (1), 4380.McGuinness, D. L., & Borgida, A. (1995). Explaining subsumption description logics.Proc. 14th Int. Joint Conf. Artificial Intelligence (IJCAI 1995), pp. 816821.McGuinness, D. L., & Patel-Schneider, P. F. (1998). Usability issues knowledge representation systems. Proc. 15th Nat. Conf. Artificial Intelligence (AAAI 1998),pp. 608614. AAAI Press/The MIT Press.Motik, B., Fokoue, A., Horrocks, I., Wu, Z., Lutz, C., & Grau, B. C. (2009). OWL 2 WebOntology Language Profiles. W3C Recommendation, World Wide Web Consortium.668fiReasoning Explanations Negative Query Answers DL-LiteNoia, T. D., Sciascio, E. D., & Donini, F. M. (2009). tableaux-based calculus abductionexpressive description logics: Preliminary results. Proc. 22rd Int. WorkshopDescription Logic (DL 2009), Vol. 477. CEUR Electronic Workshop Proceedings,http://ceur-ws.org/.Papadimitriou, C. H. (1994). Computational Complexity. Addison Wesley Publ. Co.Penaloza, R., & Sertkaya, B. (2010). Complexity axiom pinpointing DL-Litefamily description logics. Proc. 19th Eur. Conf. Artificial Intelligence(ECAI 2010), pp. 2934. IOS Press.Stewart, I. A. (1991). Complete problems involving boolean labelled structures projection transactions. J. Logic Computation, 1 (6), 861882.Tran, Q. T., & Chan, C.-Y. (2010). ConQueR why-not questions. Proc.ACM SIGMOD Int. Conf. Management Data, pp. 1526.Vardi, M. Y. (1982). complexity relational query languages. Proc. 14thSymp. Theory computing (STOC 1982), pp. 137146.Wagner, K. W. (1987). complicated questions maxima minima,closures NP. Theoretical Computer Science, 51 (12), 5380.669fiJournal Artificial Intelligence Research 48 (2013) 733-782Submitted 04/13; published 11/13Unsupervised Sub-tree AlignmentTree-to-Tree TranslationTong XiaoJingbo Zhuxiaotong@mail.neu.edu.cnzhujingbo@mail.neu.edu.cnCollege Information Science EngineeringNortheastern University3-11, Wenhua Road, Heping DistrictShenyang, ChinaAbstractarticle presents probabilistic sub-tree alignment model applicationtree-to-tree machine translation. Unlike previous work, resort surface heuristics expensive annotated data, instead derive unsupervised model infersyntactic correspondence two languages. importantly, developed modelsyntactically-motivated rely word alignments. by-product,model outputs sub-tree alignment matrix encoding large number diverse alignmentssyntactic structures, machine translation systems efficiently extract translation rules often filtered due errors 1-best alignment.Experimental results show proposed approach outperforms three state-of-the-artbaseline approaches alignment accuracy grammar quality. appliedmachine translation, approach yields +1.0 BLEU improvement -0.9 TER reduction NIST machine translation evaluation corpora. tree binarizationfuzzy decoding, even outperforms state-of-the-art hierarchical phrase-based system.1. IntroductionRecent years witnessed increasing interest syntax-based methods many Artificial Intelligence (AI) Natural Language Processing (NLP) applications rangingtext summarization Machine Translation (MT). particular, syntax-based modelsintensively investigated Statistical Machine Translation (SMT). Approaches includestring-to-tree MT (Galley, Hopkins, Knight, & Marcu, 2004; Galley, Graehl, Knight, Marcu,DeNeefe, Wang, & Thayer, 2006), tree-to-string MT (Liu, Liu, & Lin, 2006; Huang, Kevin,& Joshi, 2006) tree-to-tree MT (Eisner, 2003; Zhang, Jiang, Aw, Li, Tan, & Li, 2008;Liu, Lu, & Liu, 2009a; Chiang, 2010), train tree-string/tree-tree pairsseek model translation equivalency relations learned parsed data. partfocus syntax-based MT, tree-to-tree models use synchronous context free grammars synchronous tree substitution grammars received growing interest, showingpromising results several well-established evaluation tasks (Zhang et al., 2008; Liuet al., 2009a; Chiang, 2010). example, recent studies (Chiang, 2010) demonstratedmodern tree-to-tree systems significantly outperform hierarchical phrase-basedcounterpart large scale Chinese-English Arabic-English translation.tree-to-tree MT, translation problem broadly regarded transformationsource-language syntax tree target-language syntax tree. model process,c2013AI Access Foundation. rights reserved.fiXiao & Zhutree-to-tree systems resort general framework synchronous grammars,pair trees generated derivations synchronous grammar rules (or translationrules). model, goal translation build underlying derivationspairs trees output target string encoded likely derivation. Figure1 shows intuitive example illustrate generation process tree pair usingsample grammar, source target-language sentences associatedphrase structure trees generated using automatic parsers.1Previous work shown acquisition good translation rules one essential factors contributing success syntax-based systems (DeNeefe, Knight, Wang,& Marcu, 2007). date, several research groups addressed issue rule acquisition designed effective algorithms extract high-coverage grammars bilingualparsed data (Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010). Despite differencesdetailed modeling, approaches rely syntactic alignments align tree nodessyntactic parse tree one language tree nodes other, alignmentscould employed standard tree-to-tree rule extraction algorithms (Liu et al., 2009a;Chiang, 2010).current tree-to-tree models heavily depend syntactic alignments twolanguages, alignments induced indirectly word alignments tree-to-treesystems sensitive word alignment behavior. Unfortunately, word alignmentsgeneral far perfect viewpoint syntactic alignment (Fossum, Knight,& Abney, 2008). cases, even one spurious word alignment prevent largenumber desirable rules extraction. example, Figure 2(a) shows tree-totree translation rules extracted using word alignment produced GIZA++.alignment incorrectly aligns source word (a past tense marker Chinese)target word the. spurious word alignment produces incorrect rule AS()DT(the) blocks extraction high-level syntactic transfer rules,IP(NN1 VP2 ) S(NP1 VP2 ).Obviously, desirable solution directly infer node correspondencessource target parse trees, namely sub-tree alignment. syntactic parse treesexplain underlying structure sentences well, performing alignment sub-tree levelmake benefits high-level structural information syntactic categorization.example, consider alignment Figure 2(b). links nodes two parsetrees (in Chinese English), rather aligning word level. example,confident align VP sub-tree (spanning ) source treeVP sub-tree (spanning drastically fallen) target tree.2 therefore1. phrase structure tree, leaf nodes words sentence. internal tree nodes followedleaf nodes labeled Part-Of-Speech (POS) tags, tree nodes labeled syntacticcategories defined treebanks (see appendix meanings POS tags syntactic categories usedwork). NLP, many well-developed parsers available automatic parsing. Also, severalgood-quality phrase structure treebanks across languages used train parsing models,Penn English Chinese Treebanks (Marcus, Santorini, & Marcinkiewicz, 1993; Xue, Xia, Chiou,& Palmer, 2005). Note addition phrase structure syntax, popular formalisms(e.g., dependency syntax) used syntax-based MT. discussion different formalismssyntactic parsing beyond scope article. instead focus tree-to-tree MT basedphrase structure trees throughout work.2. Chinese English follow subject-verb-object structure. verb phrases Chinesesentence frequently aligned verb phrases English translation.734fiUnsupervised Sub-tree Alignment Tree-to-Tree TranslationNPVPNPVPPRPVBDTarget-language Side(English)VPVBNPPsatisfiedPPNPr1r4DTNNSanswersr2(ta)(huida)PNN(biaoshi)(manyi)VVNNPPPNPPVPNPVPNPVPSource-language Side(Chinese)r3(dui)IPSynchronous Grammar UsedIDr1r2r3r4Source-language SideNP(PN())PP(P() NN())VP(PP1 VP(VV() NN()))IP(NP1 VP2 )Target-language SideNP(PRP(he))PP(IN(with) NP(DT(the) NNS(answers)))VP(VBD(was) VP(VBN(satisfied) PP1 ))S(NP1 VP2 )Figure 1: Example derivation tree-to-tree translation rules. rules representedaligned pairs tree-fragments (linked dotted lines). subscriptslanguage sides grammar rules indicate alignments frontier nonterminals. language side derivation, round-head lines linkfrontier non-terminals rewritten translation.know child nodes source-language VP likely aligned childnodes target-language VP. means two VPs aligned,children aligned outside VP sub-tree structure, i.e., preventalignment Chinese tree node English tree node DT dueinconsistency VP-VP alignment. case, correctly aligned VBP.735fiXiao & ZhuNPDTNPVPNNSVBPDTADVPimportsRBVPNNSVBPADVPimportsVBNRBdrastically fallendrastically fallenVVADNNVBNVPVVADNNVPVPVPIPIP(Minimal) Rules Extracted(Minimal) Rules Extractedr1AS() DT(the)r3AD() RB(drastically)r2NN() NNS(imports)r4VV() VBN(fallen)r3AD() RB(drastically)r6AS() VBP(have)r4VV() VBN(fallen)r7NN() NP(DT(the) NNS(imports))r5IP(NN1 VP(AD2 VP(VV3 AS4 )))r8VP(AD1 VP(VV2 AS3 ))VP(VBP3 ADVP(RB1 VBN2 ))S(NP(DT4 NNS1 ) VP(VBP(have) ADVP(RB2 VBN3 )))r9(a) word alignment extracted rulesIP(NN1 VP2 ) S(NP1 VP2 )(b) sub-tree alignment extracted rulesFigure 2: Tree-to-tree translation rules extracted via word alignment (a) sub-tree alignment (b). dashed lines represent word alignment links, dotted linesrepresent sub-tree alignment (or node alignment) links.result, bad rule AS() DT(the) ruled out, desirable rulesextracted using sub-tree alignment (including desirable rules blockedFigure 2(a)).Actually, researchers aware sub-tree alignment problem triedexplore solutions (Tinsley, Zhechev, Hearne, & Way, 2007; Sun, Zhang, & Tan, 2010b,2010a). example, proposed judge whether two nodes aligned not.work, alignment confidence first calculated using lexical translation probabilitiesclassifiers trained labeled data, final alignment determined accordingnode-level alignment score. However, inference sub-tree alignment approachesrelies heuristic algorithms, models essentially optimized within unifiedprobabilistic framework.Moreover, alignment result applied tree-to-tree translation, systemssuffer another problem translation rules extracted using 1-best alignment(Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010). problem significantly affects736fiUnsupervised Sub-tree Alignment Tree-to-Tree Translationrule-set coverage rate due alignment errors. simple solution issue usek-best alignments instead. However, k-best alignments often variations manyredundancies. differ alignment links. obviously inefficientextract rules similar alignments.article address sub-tree alignment issue principled way investigatemethods effectively apply sub-tree alignment result tree-to-tree MT. particular,develop unsupervised approach learning probabilistic sub-tree alignmentmodel bi-lingual parsed data.investigate different methods integrating sub-tree alignment tree-to-treemachine translation. Specifically, develop sub-tree alignment matrix encodingexponentially large number diverse sub-tree alignments, extract multiplealternative translation rules using alignment posteriors sub-tree alignmentmatrix.advantages approach three-fold. First, approach relyheuristic algorithms labeled data. Second, developed sub-tree alignment modelstructure model used MT, i.e., based synchronous treesubstitution grammars. means MT systems directly make benefits subtree alignment model, especially rule extraction MT parameter estimation. Third,accessing sub-tree alignment matrix encodes large number alignments,efficiently obtain rules often filtered due errors within 1best/k-best alignment result. experiment approach Chinese-English subtree alignment translation tasks. sub-tree alignment, significantly outperformsthree state-of-the-art baselines. machine translation, approach obtains significantimprovements tree-to-tree system rule quality translation quality.example, yields +1.0 BLEU improvement -0.9 TER reduction NIST MTevaluation corpora. Finally, system even outperforms state-of-the-art hierarchicalphrase-based system equipped tree binarization (Wang, Knight, & Marcu, 2007b)fuzzy decoding (Chiang, 2010) techniques.rest article structured follows. Section 2 briefly introduces subtree alignment task. Section 3 describes unsupervised approach sub-tree alignment.Then, Section 4 investigates effective methods applying alignment model tree-totree translation. Then, Section 5 presents experimental evaluation approach.reviewing related work Section 6, interesting issues discussed Section 7.Finally, article concluded summary Section 8.2. Problem Statementgeneral, sub-tree alignment defined task find alignmentnodes tree nodes another tree.3 restrict machine translation article, sub-tree alignment actually task must tightly coupledspecific applications. example, addition machine translation,3. work term tree refers data structure defined recursively collection nodesstarting root node. node list edges pointing nodes (or children),constraint edge duplicated points root (Knuth, 1997).737fiXiao & ZhuNLP tasks make benefits sub-tree alignment, including sentence simplification (Cohn & Lapata, 2009; Woodsend & Lapata, 2011), paraphrasing (Das & Smith,2009), question answering (Wang, Smith, & Mitamura, 2007a), parser adaptationprojection (Smith & Eisner, 2009).Ideally, would like sub-tree alignment system language independentapplication independent. Given parallel corpus training examples, ablelearn alignment model use infer syntactic correspondence treepairs. Broadly speaking, alignments paired linguistic tree structuresregarded instances sub-tree alignment. example, alignment performeddependency trees (Eisner, 2003; Nakazawa & Kurohashi, 2011) phrase structuretrees (Tinsley et al., 2007; Sun et al., 2010b).Although sub-tree alignment problem includes number tasks seek alignments syntactic tree structures, particularly interested aligning treenodes phrase structure trees work. focus phrase structure sub-tree alignment because: 1) phrase structure parsing one popular syntactic analysisformalisms. Several state-of-the-art full parsing models/tools developed manylanguages; 2) phrase structure trees basis many successful syntax-based MT systems. alternatives, dependency trees, also benefit MT systems,constituency-based models interest relatively larger portion MT community show state-of-the-art performance recent tree-to-tree systems (Zhang et al.,2008; Liu et al., 2009a; Chiang, 2010).natural language processing, phrase structure parse tree ordered rootedtree. represents syntactic structure sentence according phrase structure grammars (or constituency grammars) describe way words combine formphrases sentences (Chiswell & Hodges, 2007). Generally, phrase structure parse treesdistinguish terminal non-terminal nodes. leaf nodes labeled terminal categories (or words), internal nodes labeled non-terminal categoriesgrammar (or phrasal categories). example, English parse tree Figure 2(b),imports terminal, nodes NP NNS two non-terminals indicatingnoun phrase plural form nouns respectively.4 following descriptionexperiments, take Penn Treebank standard tree annotation.choose Penn Treebank one popular tree-annotated corporaused syntactic parsing good quality quantity several languages,Chinese English.Based definition, sub-tree alignment defined alignmentsnon-terminals source target-language (phrase structure) parse trees.5formally, given source-language parse tree target-language parse tree ,sub-tree alignment (denoted A(S, ) short) set node-to-node links. node pair (u, v) (S, ), good alignment follow three criteria(Tinsley et al., 2007):1. u (or v) aligned (indicating 1-to-1 alignment).4. Note non-terminals always followed leaf nodes also called pre-terminalslabeled part-of-speech tags. E.g., NNS node followed terminal node importsthus pre-terminal.5. contrast, word alignment regarded alignments terminals two languages.738fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation2. u aligned v, descendants u aligned descendants v.3. u aligned v, ancestors u aligned ancestors v.criteria prevent aligning constituents cross other. propertysimilar bi-parsing formalisms, synchronous context freegrammars synchronous tree substitution grammars. advantage enablesuse powerful synchronous grammars modeling sub-tree alignment problem.shown next section, based constraints take synchronoustree substitution grammars basis proposed model.According Tinsley et al.s (2007) work, alignments satisfying criteriacalled well-formed alignments. alignment ill-formed violatescriteria. work focus well-formed alignments. Hence sub-tree alignmenttask stated as: given pair parse trees (S, ), search likely wellformed alignment= arg max P(A | S, )(1)A(S,T )(S, ) set well-formed alignments, P(A | S, ) viewedalignment model predicts probability every alignment given .follows, describe approach sub-tree alignment tree-to-tree translation, including alignment model, training inference methods, effectiveuse model tree-to-tree MT systems.3. Unsupervised Sub-tree Alignmentsection present unsupervised sub-tree alignment model. first definebase model sub-tree alignment framework synchronous tree substitutiongrammars, describe model parameterization, training inference methods.3.1 Base Modelfundamental question sub-tree alignment define correspondencenodes source-language parse tree nodes target-language parse tree.address issue using Synchronous Tree Substitution Grammars (STSGs)widely adopted model transformation process source target-languageparse trees MT (Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010). generalframework STSGs (Chiang & Knight, 2006), assumed pair sourcetarget parse trees simultaneously generated using derivation STSG rules (ortree-to-tree transfer rules). example, grammar Figure 1 STSGrules used generate pair sentences. formally, STSG systemhNs , Nt , Ws , Wt , i, Ns Nt sets non-terminals source targetlanguages, Ws Wt sets terminals (or words) source target languages,finite set productions. production STSG rewrite rule (denoted r)pair source target-language non-terminals (snt , tnt ):hsnt , tnt hsr , tr , r739fiXiao & Zhusr source-language tree-fragment, whose frontier nodes either words Wsnon-terminals Ns (labeled x); tr corresponding target-language tree-fragment;r set 1-to-1 alignments connect frontier non-terminals srfrontier non-terminals tr . example, r5 Figure 2(a),snt = IPtnt =sr = IP(NN:x VP(AD:x VP(VV:x AS:x)))tr = S(NP(DT:x NNS:x) VP(VBP(have) ADVP(RB:x VBN:x)))r = {1-2, 2-3, 3-4, 4-1}Note non-terminals left-hand side rule actually rootscorresponding tree-fragments right hand side. means rule containsexactly information matter whether root nodes (snt , tnt ) explicitlyrepresented not. following parts article use hsr , tr , r simpler representation STSG rules. Beyond this, STSG rules writtencompact form alignment r encoded numbers assigned frontiernon-terminals sr tr . example, Figures 1 2, subscripts languagesides STSG rules indicate aligned pairs frontier non-terminals.STSG model, frontier non-terminals also called substitution nodes.applying STSGs, rewrite aligned pair substitution nodes tree-fragmentpair encoded STSG rule. constraint operation labelssubstituted non-terminals must match root labels rewrite rules. example,round-head lines Figure 1 show substitution operations used derivation.using STSG rules, parse tree pair generate corresponding derivations. generation process trivial: start pair root symbols repeatedly rewrite pairs non-terminal symbols using STSG rules. example, tree pairFigure 2(b), start root labels source target-language parse trees(the superscript indicates node index tree)h IP[1] , S[1]apply rule r9 .IP[1] S[1]h IP(NN[2] VP[3] ), S(NP[2] VP[3] )r9IP[1] S[1]represents operation rewrites aligned node pair IP[1]r9S[1] r9 (denoted IP[1] S[1] ). process proceeds repeatedly rewritingremaining frontier non-terminals get complete source target-language trees,like so:740fiUnsupervised Sub-tree Alignment Tree-to-Tree TranslationNN[2] NP[2]r7VP[3] VP[3]r8h IP(NN() VP[3] ), S(NP(DT(the) NNS(imports)) VP[3] )h IP(NN() VP(AD[4][5]VP(VVAS[6] ))),S(NP(DT(the) NNS(imports)) VP(VBPAD[4] RB[4]r3h IP(NN() VP(AD() VP(VV[5]S(NP(DT(the) NNS(imports)) VP(VBP[5]VV[6]ADVP(RB[4]VBN[5] )))AS[6] ))),[6]ADVP(RB(drastically) VBN[5] )))[5]VBNh IP(NN() VP(AD() VP(VV() AS[6] ))),r4S(NP(DT(the) NNS(imports)) VP(VBP[6]ADVP(RB(drastically) VBN(fallen))))AS[6] VBP[6]r6h IP(NN() VP(AD() VP(VV() AS()))),S(NP(DT(the) NNS(imports)) VP(VBP(have)ADVP(RB(drastically) VBN(fallen))))process, rewrite rule indicates node alignment. importantly,derivations model two nice properties: first, node usource-language (or target-language) parse tree, one node targetlanguage (or source-language) parse tree aligned u; second, hierarchicalstructure behind alignment avoids links constituents cross other.Consequently, well-formed sub-tree alignment A, always find derivationencodes alignment A. means sub-tree alignment problem essentiallyproblem finding likely STSG derivation. Thus sub-treealignment task (see Equation (1)) restated finding likely derivationgiven pair parse trees.model derivation probability, follow formulation adopted statisticalword alignment (Brown, Pietra, Pietra, & Mercer, 1993; Vogel, Ney, & Tillmann, 1996).transformation source-language tree target-language tree describedfollowing equation.XP(T | S) =P (T, d|S)(2)dD(S,T )D(S, ) set derivations transforming (say, aligning nodesnodes ), P (T, d|S) probability transforming usingderivation D(S, ), parameters model. use notationP () express dependence model parameters. general, optimalvalue learned parsed parallel data training criteria. example,context unsupervised learning, optimize model parameters maximizingprobability observed data (known maximum likelihood training).Given set optimal parameters , best sub-tree alignment (S, ) determinedP (T,d|S)choosing derivation P (d | S, ) greatest. Since P (d | S, ) = P (T |S)741fiXiao & ZhuP (T | S) constant given (S, ) , finding best derivationfinding derivation make P (T, | S) large possible. Hence reachfundamental equation sub-tree alignment.= arg max P (T, | S)(3)dD(S,T )formulation implies three fundamental issues sub-tree alignment, includingmodeling derivation probability (i.e., P (T, d|S)), learning model parameters (i.e., )finding best alignment given learned model (i.e., arg max operation).following parts section, describe solutions issues.3.2 Parameterizationsimplest case, alignment model one parameter instance derivation.However, model would unmanageable set parameters since numberderivations exponential length input sentences. choose simplesolution issue decomposes base model product trainable submodels. start assumption rules conditionally independent givensource-language parse tree S, probability P(T, | S) defined productrule probabilities (for conciseness, drop subscript on).P(T, | S)P(r | S)(4)rdNevertheless complex tree-to-tree mappings still result extremely large numberrules, causes computational problem degenerate analysisdata.6 control number parameters reasonable level, decomposerule probability simpler probability factors independence assumptions.First assume generation rule r independent input tree S,conditioned source-language side rule, is,P(r | S) P(r | sr )(5)Note strong assumption generation synchronous grammarrule depends source-language side. similar used statisticalmodeling machine translation (Brown et al., 1993; Koehn, Och, & Marcu, 2003; Galleyet al., 2004; Chiang, 2005) generation atomic alignment/translation unitsconditioned associated source-language words tree-fragments, ratherwhole input sentence tree. SMT, independence assumptions based phrasestranslation rules generally used decompose parallel corpus manageable unitsparameter estimation. successfully used modern SMTsystems, adopt similar assumption ease parameter estimation processmodel.decompose P(r | sr ) additional assumptions. Since r = hsr , tr , r i,P(r | sr ) written another form using chain rule:6. degenerate analysis refers case using models complex results overfittingpoor generalization ability unseen data.742fiUnsupervised Sub-tree Alignment Tree-to-Tree TranslationP(r | sr ) = P(sr , tr , r | sr )= P(r | sr , tr ) P(tr | sr )(6)Equation (6) indicates two sub-models, including reordering model frontier nonterminals P(r | sr , tr ), tree-fragment translation model P(tr | sr ).model P(r | sr , tr ), view frontier non-terminal reordering problem aligningelements two vectors non-terminals. Let vnt() function returnsvector leaf non-terminals given tree-fragment. r defines 1-to-1 alignmentnon-terminals vnt(sr ) vnt(tr ). example, r5 Figure 2(a),frontier non-terminal vectors sr5 tr5 are:vnt(sr5 ) = (NN, AD, VV, AS)vnt(tr5 ) = (DT, NNS, RB, VBN)r5 = {1-2, 2-3, 3-4, 4-1} indicates alignment vnt(sr5 ) vnt(tr5 ), say,NN aligned NNS, AD aligned RB on. opt simple modelselecting r . models non-terminal reordering probability conditionfrontier non-terminal vectors language sides, follows7P(r | tr , sr ) Preorder (r | vnt(sr ), vnt(tr ))(7)turn problem modeling tree-fragment translation P(tr | sr ) (i.e.,second sub-model defined Equation (6)). define tree-fragment consiststwo parts: words lex() (i.e., terminals ) tree structure tree() without lexiconsinvolved. example, r5 Figure 2(a), target-language tree-fragment containstwo elements lex(tr5 ) tree(tr5 ):lex(tr5 ) =tree(tr5 ) = S(NP(DT:x NNS:x) VP(VBP ADVP(RB:x VBN:x)))Let root() function returns root given tree-fragment. writeP(tr | sr ) as:P(tr | sr ) = P(lex(tr ), tree(tr ) | sr )= P(root(tr ) | sr )P(tree(tr ) | root(tr ), sr )P(lex(tr ) | tree(tr ), root(tr ), sr )(8)worth noting Equation (8) approximation. chooseone many ways P(tr | sr ) written product series7. reordering model defined ensures arbitrary 1-to-1 alignments handled. mightresult large model sparse parameter distributions big tree-fragments involved.considering issue, choose several pruning methods better control rule size sub-treealignment system. See Section 5.2.2 pruning settings work.743fiXiao & Zhuconditional probabilities. simply assert equation generating targetlanguage tree-fragment source-language tree-fragment, first choose rootsymbol target-language tree-fragment given source-language tree-fragment (inprobability P(root(tr ) | sr )). choose tree-structure target-languagetree-fragment given root symbol source-language tree-fragment (in probabilityP(tree(tr ) | root(tr ), sr )). choose target-language terminals associatedtree-fragment given target-language tree-structure, target-language root symbolsource-language tree-fragment (in probability P(lex(tr ) | tree(tr ), root(tr ), sr )).Another note Equation (8) actually reduce model complexity.example, P(lex(tr ) | tree(tr ), root(tr ), sr ) essentially indicates combinations sourcetarget-language tree-fragments. simpler model required feasible solutionparameter estimation. this, introduce additional assumptions relaxconditions probabilities reduce number parameters reasonable level.1. P(root(tr ) | sr ) depends root(sr ), i.e.,P(root(tr ) | sr ) Pnt (root(tr ) | root(sr ))(9)assumption implies node correspondence source targetlanguage parse trees.2. P(tree(tr ) | root(tr ), sr ) depends root(tr ), i.e.,P(tree(tr ) | root(tr ), sr ) Ptree (tree(tr ) | root(tr ))(10)second assumption results monolingual model generating target-languagetree-structures, generation tree-fragment conditionedroot. viewed analogy generative model used standard TSGs.3. P(lex(tr ) | tree(tr ), root(tr ), sr ) depends source words lex(sr ), i.e.,P(lex(tr ) | tree(tr ), root(tr ), sr ) Plex (lex(tr ) | lex(sr ))(11)allows us directly model terminal correspondence two languages.Then, substitute Equations (9)-(11) Equation (8), getP(tr | sr ) Pnt (root(tr ) | root(sr ))Ptree (tree(tr ) | root(tr ))Plex (lex(tr ) | lex(sr ))using Equations (6), (7) (12), Equation (4) finally written as:744(12)fiUnsupervised Sub-tree Alignment Tree-to-Tree Translationidruleprobabilityr3AD() RB(drastically)Pnt (RB | AD) Plex (drastically | )r4VV() VBN(fallen)Pnt (VBN | VV) Plex (fallen | )r6AS() VBP(have)Pnt (VBP | AS) Plex (have | )r7NN()Pnt (NP | NN) Plex (the imports | )NP(DT(the) NNS(imports))Ptree (NP(DT NNS) | NP)VP(AD1 VP(VV2 AS3 ))Pnt (VP | VP) Ptree (VP(VBP ADVP(RB VBN)) | VP)VP(VBP3 ADVP(RB1 VBN2 ))Preorder (1-2, 2-3, 3-1 | (AD, VV, AS), (VBP, RB, VBN))IP(NN1 VP2 ) S(NP1 VP2 )Pnt (S | IP) Ptree (S(NP VP) | S)r8r9Preorder (1-1, 2-2 | (NN, VP), (NP, VP))Table 1: Rule probabilities sample derivation = {r3 , r4 , r6 , r7 , r8 , r9 } Figure 2(b)P(T, | S)Pnt (root(tr ) | root(sr ))rdPtree (tree(tr ) | root(tr ))Plex (lex(tr ) | lex(sr ))Preorder (r | vnt(sr ), vnt(tr ))(13)simplified model generative story described section. takesrule generation probability product four probability factors: Pnt () nonterminal mapping probability, roughly captures syntactic correspondence subtrees two languages; Ptree () probability generating tree structure; Plex () probability terminal mappings two language sidesrule; Preorder () probability frontier non-terminal reordering encodedrule. See Table 1 rule probabilities sample derivation.model parameters assumed multinomial distributions. calculation Pnt (), Ptree () Preorder () straightforward: directly usedwithout decompositions assumptions. calculate Plex (), chooseform adopted popular models word alignment (Och & Ney, 2004; Thayer, Ettelaie, Knight, Marcu, Munteanu, Och, & Tipu, 2004), probability definedproduct word-based translation probabilities:l1 XPlex (t1 ...tl | s1 ...sm ) Plength (l | m)Pw (ti | sj )i=1(14)j=1ti target word, sj source word. Plength () used control numbertarget words produced given number source words. Pw () word translationprobability. sub-model principle something rather similar conventionalword-based translation tables, IBM Models (Brown et al., 1993).3.3 Node Deletion InsertionWord (or sub-tree) deletion/insertion common real-world alignment translationtasks. add flexibility modeling problem, allow production empty745fiXiao & Zhusub-trees either source target-language side rule model. formally,rule whose target-language side empty sub-tree, probability defined as:P(r | S) Pnt (root() | root(sr ))Ptree (tree() | root())Plex (lex() | lex(sr ))Preorder ( | vnt(sr ), vnt())(15)special symbol indicates nothing. Factors Pnt (root() | root(sr ))Plex (lex() | lex(sr )) model deletion probability different levels tree-fragment.Ptree (tree() | root()) probability generating empty tree-fragment. FactorPreorder ( | vnt(sr ), vnt()) regards special reordering pattern alignsfrontier non-terminals source side virtual node NULL. Obviously, valuesPtree (tree() | root()) Preorder ( | vnt(sr ), vnt()) simply 1.Similarly, rule whose source side empty sub-tree, probability defined as:P(r | S) Pnt (root(tr ) | root())Ptree (tree(tr ) | root(tr ))Plex (lex(tr ) | lex())Preorder ( | vnt(), vnt(tr ))(16)value Preorder ( | vnt(), vnt(tr )) also 1.worthwhile note word deletion insertion problems importantMT spite relatively less discussion recent studies tree-to-tree translation.actually analogy NULL-alignment used IBM Models(Brown et al., 1993). word/phrase-based models, removing words alignmentleave space correctly aligning words sentence.8 evennecessary (1-to-1) sub-tree alignment alignment respect syntacticconstraints language sides, e.g., sub-tree alignments allowed breakconstraints imposed neighbouring parts tree. cases, cannot obtaincorrect 1-to-1 alignment tree pair due one two bad nodesnecessarily aligned valid node counterpart tree. Instead, nodesskipped alignment thus impose bad constraints partstree node deletion/insertion allowed. especially true align sentencepairs flat tree structures free translations. work found nodedeletion insertion operations necessary achieve satisfactory sub-tree alignmentresult. therefore used implementation default.3.4 Trainingturn training problem. discussed Section 3.1, focus unsupervisedlearning model parameters, is, optimal values parameters estimated given8. Note current phrase-based approaches (Koehn et al., 2003; Och & Ney, 2004) allow NULL-alignedwords appear boundary phrase, viewed way implicit modelingword insertion/deletion problem746fiUnsupervised Sub-tree Alignment Tree-to-Tree Translationcollection tree pairs without annotation sub-tree level alignment. workchoose two approaches estimating parameters sub-tree alignment model, includingMaximum Likelihood Estimation (MLE) approach Bayesian approach.3.4.1 Maximum Likelihood TrainingMLE one popular methods parameter estimation statistical models.basic idea that, given model set parameters, MLE method selects valuesparameters generate distribution gives highest probability observeddata. MLE general approach parameter estimation widely adoptedmany AI NLP tasks, part-of-speech tagging. case sub-tree alignment,MLE simply described finding optimal values parameters leadmaximum probability aligning tree nodes source-language parse treetarget-language parse tree. formally, given set tree pairs {(S1 , T1 ), ..., (Sn , Tn )},objective MLE-based training defined be:= arg maxnXP (Ti , | Si )(17)i=1 dD(Si ,Ti )choose Expectation Maximization (EM, Dempster, Laird, & Rubin, 1977)algorithm solve optimization problem. Basically EM algorithmiterative training method finding maximum likelihood estimates model parameters,assumed observed data depends latent variables. algorithmperforms iteratively calling two sub-routines, namely Expectation (E)-stepMaximization (M)-step. E-step, calculates expected value likelihoodfunction associated parameters observed data, respect distributionlatent variables given observed data current estimates parameters.M-step, seeks parameters maximize expected likelihood foundE-step.applying EM algorithm case, view input pairs parsetrees {(S1 , T1 ), ..., (Sn , Tn )} observed data, underlying derivations rules latentvariables, distributions Pnt (), Ptree (), Preorder () Plex () (i.e., Plength ()Pw ()) unknown parameters. See Figure 3 pseudo-code training algorithmPnt () (denoted tnt |snt ). algorithm directly applicable estimationparameters model, skip description learning remaining parametershere. detailed description EM-based training model parametersrefer reader appendix.algorithm, snt tnt represent source-language non-terminal symboltarget-language non-terminal symbol, u v represent source-language tree nodetarget-language tree node, EC() represents expected count given variable,P(k) (T, | S) represents derivation probability based parameters obtainedk-th round. EM iteration, E-step algorithm accumulates expectedcount pairs parse trees. Then, M-step finds maximum likelihood estimateusing quantity. nontrivial part algorithm computationexpected count E-step. Roughly speaking, physical meaning right-handside line 9 relative probability derivation contains rule r (with root node747fiXiao & Zhu1: Function TrainModelWithEM ({(S1 , T1 ), ..., (Sn , Tn )})(0)2: Set {tnt |snt } = initial model3: k = 0 K 14:Foreach non-terminal symbol pair (snt , tnt )5:EC(tnt |snt ) = 06: E-step:6:Foreach tree pair (S, ) sequence {(S1 , T1 ), ..., (Sn , Tn )}7:Foreach node pair (u, v) symbol pair (tnt , snt ) (S, )8:Foreach rule r rootedP (u, v)Prooted (u,v)9:EC(tnt |snt ) + = d: rd r P0d0 P (k) (T,d |S)8: M-step:10:Foreach non-terminal symbol pair (snt , tnt )11:12:(k+1)tnt |snt =P(k) (T,d|S)EC(tnt |snt )EC(t0 |s )t0ntntnt(K)return {tnt |snt }Figure 3: EM-based training algorithm (for Pnt ())Ppair (u, v)). numerator d: rd r rooted (u,v) P(k) (T, | S) probability sumPderivations involve r , denominator d0 P(k) (T, d0 | S)overall probability alignment . However, brute-force computationexpected counts inefficient requires sum possible derivationswhose number exponential length input sentences.work use bilingual version inside outside probabilities (Manning& Schutze, 1999) avoid naive enumeration possible derivations computingvarious probabilities. inside probability (u, v) (denoted (u, v)) measureslikely generate sub-tree pair inside node pair (u, v). outside probability(denoted (u, v)) dual inside probability. measures likely generateremaining parts tree pair (S, ) start symbols. Like formulationused monolingual parsing (Manning & Schutze, 1999), (u, v) (u, v) definedusing following recursive forms:X(u, v) =P(r | S)(p, q)(18)r:root(r)=(u,v)(u, v) =X(p,q)yield(r)(root(r)) P(r | S)r:(u,v)yield(r)(p, q)(19)(p,q)yield(r)(p,q)6=(u,v)root(r) abbreviation node pair (root(sr ), root(tr )), yield(r)set aligned frontier non-terminal pairs yielded r. Based recursivedefinitions, (u, v) (u, v) efficiently computed dynamic programming.using inside outside probabilities, easy address computationproblem mentioned above. Let (u, v) denote probability tree node u alignedtree node v. probability expressed inside-outside fashion:748fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation(u, v) =XP(T, | S)d: (u,v)aligned= (u, v) (u, v)(20)way, overall alignment probability (i.e., denominatorright-hand side line 9) simply written as:XP(T, | S) = (root(S), root(T )) (root(S), root(T ))(21)numerator right-hand side line 9, let us view another angle.E-Step algorithm, expected count accumulated rules whose root (u, v).rules rooted (u, v) indicate node alignment u v, lines8-9 principle imply probability derivations aligning u v, preciselynode alignment probability (u, v). probability written simpleform using inside outside probabilities:XXP(T, | S) = (u, v)r: r rooted d: rd(u,v)= (u, v) (u, v)(22)Together result Equation (21), E-step efficiently implementedreplacing lines 8-9 following equationEC(tnt |snt ) + =(u, v) (u, v)(root(S), root(T )) (root(S), root(T ))(23)(snt , tnt ) symbol pair (u, v). Note (snt , tnt ), E-step step(u,v)(u,v)increases EC(tnt |snt ) sum (root(S),root(T))(root(S),root(T )) node pairs (u, v)whose symbols snt tnt . means (snt , tnt ) aligned different positionsinput tree pair, method considers alignment (snt , tnt ) multipletimes updates EC(tnt |snt ) accordingly.also worth noting several methods initializing model parameters EM-style training begins. example, model initializeduniform random distributions. work initialize parameters sub-treealignment model model obtained using word alignment result. standard way adopted many unsupervised models simpler model used goodstarting point training process. helpful optimization proceduresensitive initial setting model parameters (e.g., EM non-convex objective functions). experiments found using GIZA++ word alignment parameterinitialization resulted better performance fewer iterations convergenceuniform initial distributions. word alignment obtained unsupervisedmanner, change training condition approach. Thus chosemethod initializing model parameters implementation.749fiXiao & Zhu3.4.2 Bayesian ApproachMLE one standard approaches training unsupervised models, wellknown tendency overfit data. overfitting problem becomes severecomplex models since parameters fit training data better.case STSGs, likely result degenerate analysis data, i.e., rarebig rules dominate ML solution STSGs, considered noisygeneralize poorly unseen data (Cohn & Blunsom, 2009; Liu & Gildea, 2009).natural solution problem incorporate constraints proper priorstraining process. take Bayesian approach alternative solutiontraining problem.Unlike MLE, Bayesian approach plug single optimum point estimateparameter distribution data point, instead account uncertaintyvalue parameter. Bayesian models, parameters assumeddrawn probability distributions priors. parameters extra priordistributions called hyperparameters, denoted . parametersmodel viewed mathematically multinomials, choose Dirichlet distributions(Ferguson, 1973) prior model parameters. advantage using Dirichletdistributions conjugate multinomial distributions inferencepriors easier.Following previous description, use denote model parametersmultinomial outcomes {1, ..., K} (i.e., k probability outcome k {1, ..., K}).multinomial distribution sample set outcomes {x1 , ..., xn } probabilityP(xi = k) = k . Dirichlet prior distribution multinomials, sampleprior actually set parameter values . Therefore distributionmodeled as:xi | Multinomial()(24)| Dirichlet()(25)Equation (24) means xi distributed according multinomial parameters. Similarly, Equation (25) read distributed according Dirichlet distribution parameters . = {1 , ..., K } hyperparameter vector correspondingoutcomes. work use symmetric Dirichlet prior ( i.e., 1 , ..., K sharevalue), use represent single hyperparameter instead hyperparametervector.Using model, compute conditional distribution new observationxn+1 given previous observations {x1 , ..., xn } hyperparameter , follows:ZP(xn+1 | x1 , ..., xn , ) = P(xn+1 | x1 , ..., xn , ) P( | )(26)big advantage Bayesian approach introduce prior distributionunknown parameters model, meant capture knowledge beliefsmodel seeing data (Neal, 1998). especially important caseneed bias towards preferred situations. example, expectmodel favor high frequency rules dislike rare big rules. goal750fiUnsupervised Sub-tree Alignment Tree-to-Tree Translationeasily achieved using Bayesian approach appropriate choice priors, say,Dirichlet prior low concentration parameter . However, introduction priorsgenerally makes intractable estimate posterior analytically. practical systemsbased Bayesian approach, widely-used solution use approximate methodsseek compromise exact inference computational resources. workchoose Variational Bayes approximate inference. Variational Bayes good methodpreserves benefits introducing prior tractable inference procedure(Attias, 2000; Beal, 2003). successfully applied several NLP-related models,Hidden Markov Models (HMMs) IBM Models (Beal, 2003; Riley & Gildea,2010). One good thing Variational Bayes seen extensionEM algorithm resembles usual forms used EM. resulting procedure looks lotlike EM algorithm modified M-step, convenient implementation.follow approach presented previous work (Beal, 2003; Riley & Gildea, 2010)variational Bayesian algorithms applied similar tasks. needslight change M-step original EM algorithm presented Section 3.4.1.original EM algorithm (see Figure 3), M-step normalizes expected countscollected E-step standard MLE. variational Bayesian version M-stepslightly modifies formula performs inexact normalization passing countsfunction f (x) = exp((x)).tnt |snt =f (EC(tnt |snt ) + )Pf ( t0 (EC(t0nt |snt ) + ))(27)nt(x) digamma function (Johnson, 2007). approximate effectsubtracting 0.5 argument. choice controls behavior estimation.set low value, performs estimation way anti-smoothing.0.5 subtracted rule counts, small counts corresponding rare eventspenalized heavily, large counts corresponding frequent events affectedmuch. example, low values make Equation (27) favor non-terminal pairsaligned frequently distrust non-terminal pairs aligned rarely.way, variational Bayesian method could control overfitting caused abusingrare events. hand, larger used smoothing required.method applicable training parameters model.requires replacement M-step Figure 3 variational Bayesian M-step (asEquation (27)). implementation, variational Bayes-based training,perform additional round normalization without variational Bayes normalize ruleprobabilities sum one.99. additional normalization process makes posterior probabilities directly comparableobtained training methods, EM-based training. Note convert resultBayesian inference probability distributions good explanation various probabilityfactors model. hand, technical trick results pseudo-Bayesian procedureBayesian inference exactly though shows good results empirical study. Oneremove additional round normalization pure Bayesian approach. changesaffect overall pipeline approach (from practical standpoint).751fiXiao & Zhu1: Function Decode(S, )2:[], [] = GetInsideOutsideProbabilities (S, )3:Foreach node u bottom-up order4:Foreach node v bottom-up order5:[u, v] = [u, v] [u, v]6:Foreach tree-fragment sr rooted u7:Foreach tree-fragment tr rooted v8:Foreach frontier non-terminal alignment sr ts9:r = CreateRule(sQ r , tr , a)10:score = P(r | S) (p,q)yield(r) P(d[p, q])11:score > P(d[u, v])12:d[u, v] = CreateDerivation(r, {d[p, q] : (p, q) yield(r)})13: return (d[], [])14: Function GetInsideOutsideProbabilities (S, )15: Foreach node u bottom-up order16:Foreach node v bottom-up order17:Set [u, v] according Equation (18)18: Foreach node u top-down order19:Foreach node v top-down order20:Set [u, v] according Equation (19)21: return ([], [])Figure 4: Decoding algorithm proposed sub-tree alignment model 1-bestposterior-based outputs3.5 DecodingInference model straightforward. simplest case inferring 1-best subtree alignment. Given set learned parameters, first visit every node pair (u, v)bottom-up fashion, compute posterior probability aligning sub-tree pairrooting (u, v). procedure dynamic program used trainer.select derivation maximum sub-tree alignment probabilityinput tree pair. Also, generate list k-best derivations similar manner.addition 1-best/k-best output, model able output alignmentposterior probability every pair tree nodes. this, need recordprobability (u, v) node pair obtain inside outside probabilities.Note outputting alignment posterior probabilities also commonly used statisticalword phrasal aligners. provides flexible way making use alignment resultdownstream components, rule extraction system. presentednext sections, tree-to-tree MT systems make great benefits posteriorbased alignment output, results effective rule extraction method wellbetter translation results.Figure 4 depicts pseudo-code decoding algorithm 1-best posteriorbased outputs. algorithm, d[x, y] data structure records best derivation rooted (x, y). [x, y], [x, y] [x, y] data structures record insideprobabilities, output probabilities alignment posterior probabilities, respectively. Cre752fiUnsupervised Sub-tree Alignment Tree-to-Tree TranslationateRule() creates rule pair tree-fragments (sr , tr ) frontier non-terminalalignment a, calculates rule probability. CreateDerivation() builds derivationusing input rules. output, access 1-best alignment traversingd[root(S), root(T )], access alignment posterior [].Given pair trees (S, ), outer two loops algorithm iterates pairnodes two trees, resulting time complexity O(|S| |T |) | | represents2 ), Nsize input tree. Generating pairs tree-fragments requires O(Ntreetreemaximum number tree-fragments given tree node. Computing alignment(sr , tr ) requires O(L!) L maximum number leaf non-terminals2rule. Therefore time complexity algorithm O(|S| |T | NtreeL!), quadraticsize input trees. Note actual time complexity algorithm couldhigh potential alignments considered. example, Ntree generallyexponential function depth input tree-fragment, deep tree couldresults extremely large space alignments. make practical sub-tree alignmentsystems, pruning techniques taken account work. example,implementation, restrict depth tree-fragment reasonable number (see Section5.2.2). addition, commonly-used phrasal alignment related tasks, considerword alignments pruning discard sub-tree alignments violate certainnumber word alignments. example, throw away sub-tree alignmentstwo word alignment links outside spans covered aligned sub-trees.4. Applying Sub-tree Alignment Tree-to-Tree Translationsub-tree alignment obtained, current tree-to-tree systems directly learn translation rules node-aligned tree pairs. section investigate methods applyingsub-tree alignment tree-to-tree rule extraction.4.1 Rule Extraction Using 1-best/k-best Sub-tree Alignmentsdata, several methods developed tree-to-tree rule extraction (Zhang et al.,2008; Liu et al., 2009a; Chiang, 2010). popular GHKM-likemethod extends idea extracting syntactic translation rules string-tree pairs(Galley et al., 2004). GHKM-like extraction, first compute set minimallysized translation rules explain mappings source-language treetarget-language tree respecting alignment reorderingtwo languages. Larger rules learned composing two minimal rules.example, Figure 2(b), r7 r9 two minimal rules extracted according sub-treealignment. compose rules form larger rule, like this:IP(NN() VP1 ) S(NP(DT(the) NNS(imports)) VP1 )work use tree-to-tree version GHKM-like extraction describedLiu et al.s (2009a) work. See Figure 5(a) pseudo-code rule extraction1-best sub-tree alignment. choose method widely usedtree-to-tree systems. Note rule extraction tree-to-tree translation generallyrestricted performed 1-best sub-tree alignment result. GHKM-like753fiXiao & Zhu1: Function OneBestExtract (S, , A)2:Foreach node u3:Foreach node v4:Foreach tree-fragment pair (sr , tr )4:rooted (u, v)5:= OneToOneAlign(sr , tr , A)6:empty7:r = CreateRule(sr , tr , a)8:rules.Add(r)9:return rules10: Function OneToOneAlign(sr , tr , A)11: frontier non-terminals (sr , tr )11:1-to-1 alignments12:return frontier alignment (sr , tr )13: Else14:return1: Function MatrixExtract (S, , )2:Foreach node u3:Foreach node v4:IsExtractable({(u, v)}, )5:next loop6:Foreach tree-fragment pair (sr , tr )6:rooted (u, v)7:Foreach frontier alignment7:(sr , tr )8:IsExtractable(a, )9:r = CreateRule(sr , tr , a)10:rules.Add(r)11: return rules12: Function IsExtractable(a, )13: Foreach alignment (p, q)14:probability (p, q) < Pmin15:return false16: return true(a) 1-best Extraction(b) Matrix-based ExtractionFigure 5: 1-best matrix-based rule extraction algorithmsextraction method employed list k-best sub-tree alignments provided.k-best extraction need repeat procedure 1-best extractionsub-tree alignment k-best list.4.2 Rule Extraction Using Sub-tree Alignment MatricesPrevious work pointed current MT systems suffer error propagation duealignment errors made within 1-best alignment (Venugopal, Zollmann, Smith, &Stephan, 2008). sub-tree alignment early-stage step training pipeline,errors 1-best alignment likely propagated translation rule extractionparameter estimation translation model. Though problem alleviatedusing k-best alignments, limited scope k-best alignments still results inefficientlearning translation rules. example, preliminary experiment shows 95.8%extracted rules redundant 100-best alignments involved.instead present simple efficient method, namely matrix-based rule extraction. method, use posterior-based output aligner representsub-tree alignment compact structure - call sub-tree alignment matrix alignmentmatrix short (Liu, Xia, Xiao, & Liu, 2009b; de Gispert, Pino, & Byrne, 2010).See Figure 6(a) two example sub-tree alignment matrices made pair sentence segments. matrices, entry indexed pair source target nodes.score entry posterior probability alignment corresponding node pair, i.e., (u, v) probability defined Equation (20). probabilitystraightforwardly accessible output inference algorithm described Section3.5. principle (u, v) viewed measure sub-tree alignment confidence: highervalue indicates confident alignment two nodes. way754fihaveRB[4]VBNdrastically1fallenVV[4]AS[5]AD[2].1AS[5]4]5]VP[1].1AD[2].8.1.1.6.1.2VP[3].3.7VV[4].4AS[5]1 VV[4]1VBN[1]VBP [2]ADVP [3]RB [.9AD[2]VP[3]VP[3]VP[1]VP[1]1[5]VP [ADVP[3]VBVBP[2]VP [1]VP[1]P [2]ADVP [3]RB [4]VBN [5]Unsupervised Sub-tree Alignment Tree-to-Tree Translation.6= fixed alignment= possible alignmentMatrix 1: 1-best alignmentMatrix 2: posterior(a) Sub-tree alignment matrices sample sub-tree pairMinimal RulesExtracted Matrix 2 (posterior)r3AD() RB(drastically)r4VV() VBN(fallen)r6AS() VBP(have)r8VP(AD1 VP(VV2 AS3 ))VP(VBP3 ADVP(RB1 VBN2 ))r10 VP(VV() AS()) VBN(fallen)r11 VP(AD1 VP2 ) VP(VBP1 ADVP2 )Minimal RulesExtracted Matrix 1 (1-best)r3 AD() RB(drastically)r4 VV() VBN(fallen)r6 AS() VBP(have)r8 VP(AD1 VP(VV2 AS3 ))VP(VBP3 ADVP(RB1 VBN2 ))...(b) Rules extracted using 1-best alignment alignment posteriorFigure 6: Matrix-based representation sub-tree alignment sample rules extracted.Matrix 1 shows case 1-best sub-tree alignment, Matrix 2 showscase sub-tree alignment posterior.access possible sub-tree alignments (with different probabilities), rather limitednumber them.extract rules using sub-tree alignment matrix. method simple:collect rules associated entry matrix. core algorithmmethod essential used 1-best/k-best extraction. difference1-best/k-best extraction matrix-based method considers possible nodepairs extraction, rather visiting only. See Figure 5(b) pseudocode sub-tree alignment matrix-based rule extraction algorithm, representssub-tree alignment matrix pair trees (S, ). Compared extracting rulesk-best alignments, method efficiently obtain additional rules whose extractionblocked k-best extraction. example, right side Figure 6(b) two new rulesr10 r11 extracted, cannot obtained 1-best alignment result.prevent extraction great number noisy rules low alignment probabilities,755fiXiao & Zhuprune away rules whose alignment probabilities pre-specified threshold.formally, given pair nodes (u, v), rule extraction executed (u, v)satisfies:(u, v)< Pmin(28)(root(S), root(T ))expression measures relative probability alignment (u, v) respectsum probabilities possible derivations. Pmin empirical threshold controloften rules pruned (a larger Pmin means rules thrown away).work, set 107 default. Therefore, entries zero score Figure6(a) (denoted dot) excluded rule extraction.However, discarding rules relatively low probabilities turn results incompleteness problem, is, extracted rules might unable transform given sourceparse-tree, even training set. Nonetheless, problem severecase. experiments observed parse-tree pairs (over 90%) trainingcorpus could recovered extracted rules Pmin chose default value,contribution translation accuracy low confidence rules limited(generally less 0.1 BLEU points).Another note sub-tree alignment matrix-based extraction. advantagemethod follows general well-developed framework syntax-based MT,i.e., word/syntactic alignment + rule extraction/parameter estimation + MT decoding.need replace rule extraction component sub-tree alignment matrixbased system, preserve components pipeline. means stilluse heuristics obtain additional useful rules result sub-tree alignmentmatrix-based extraction, rule composing (Galley et al., 2006) SPMT extraction(Marcu, Wang, Echihabi, & Knight, 2006). Also, posterior probability encodedmatrix used better estimation various MT-oriented features.10Note basis approach STSG model, rules sub-treealignment model resemble general forms translation rules used tree-to-tree MTsystems. So, alternative simple way rule induction, directly infer translation rules sub-tree alignment model take corresponding rule probabilitiesfeatures translation model MT decoding. However, tree-to-tree MTmethod suffers several problems. First, sub-tree alignment model requires computation possible aligned tree-fragments, results high time complexitytraining decoding procedures. result, aggressive pruning usedreasonable size search space, e.g., consider relatively small tree-fragmentsimplementation acceptable running speed. side effect, many relatively largerules (e.g., composed rules SPMT rules) absent sub-tree alignment model,available use traditional alignment + extraction heuristics pipeline.engineering standpoint, efficient directly infer translation rulessub-tree alignment model, compared inferring rules using pruned fixed subtree alignment matrix plus heuristics. Second, rule probability optimizationobjective sub-tree alignment different used MT systems. example, use generative model maximum-likelihood/Bayesian approach sub-tree10. See Section 4.3 detailed discussion parameter estimation issue.756fiUnsupervised Sub-tree Alignment Tree-to-Tree Translationalignment, use discriminative model minimum error rate training MT. Manyfeatures employed MT decoder considered sub-tree alignment model.issues might lead unsatisfactory MT performance. shown experiments (see Section 5.3.5), directly inferring translation rules sub-tree alignmentmodel achieve promising results.4.3 Learning Features Machine Translationprevious work syntax-based MT, proved syntax-based systems make greatbenefits MT-oriented features, even necessarily well explainedsyntactic parsing viewpoint (e.g., phrase-based translation probabilities). However,features available word/sub-tree alignment model. Insteadneed learn features using additional step parameter estimation MT.this, follow commonly-used framework estimates values various MToriented features extracted rule set using MLE. procedure simple:translation rules extracted, obtain maximum-likelihood (or relative-frequency)estimate parameters according definition feature function.However, traditional tree-to-tree systems rule extracted tree paircount unit one, used calculate values various features.approach might enlarge influence noisy rules extracted sub-tree alignmentmatrices. E.g., rule high alignment probability equal weight rulelow alignment probability, thus unreasonably large impact MT systems.desired solution rule extracted derivation low probabilitypenalized accordingly feature learning. Motivated idea, use fractional countsestimate appearance rule (Mi & Huang, 2008). Given node pair (u, v)(S, ), alignment probability rule r rooted (u, v) defined (denoted(r; u, v)):X(r; u, v) =P(T, | S)(29)dD(S,T )rd(r; u, v) regarded probability sum derivations involving r (u, v).Also, rewrite Equation (29) inside-outside fashion:(r; u, v) = (u, v)(p, q) P(r | S)(30)(p,q)yield(r)define probability r involved derivations (S, ) as:X(r) =(r; u, v)(31)u,vEquation (31) sum probabilities r node pairs. meansrule probability considered multiple times particular derivations containr once. using (r), fractional count r defined be:c(r) =(r)(root(S), root(T ))757(32)fiXiao & ZhuEquation (32) reflects probability likely r involved derivation givenpair trees. set bilingual parse trees, c(r) accumulated tree pair.Obviously, c(r) used estimate parameters MT model, is,translation rules weighted, parameter estimation procedure proceed usual,weight counts. work c(r) employed learn five features usedMT decoder, including bi-directional phrase-based conditional translation probabilities(Marcu et al., 2006) three syntax-based conditional probabilities (Mi & Huang, 2008).Let () function returns sequence frontier nodes input tree-fragment.probabilities computed following equations:P00r00 :(sr00 )=(sr )(tr00 )=(tr ) c(r )PPphrase (tr | sr ) =(33)0r0 :(sr0 )=(sr ) c(r )P00r00 :(sr00 )=(sr )(tr00 )=(tr ) c(r )PPphrase (sr | tr ) =(34)0r0 :(t 0 )=(tr ) c(r )rc(r)P(r | root(r)) =PP(r | sr ) =Pr0 :root(r0 )=root(r) c(rc(r)r0 :sr0 =srP(r | tr ) =c(r0 )c(r)Pr0 :tr0 =trc(r0 )0)(35)(36)(37)5. Experimentsevaluation, first experimented approach Chinese-English sub-treealignment task, tested effectiveness state-of-the-art tree-to-tree MT system.5.1 BaselinesThree unsupervised sub-tree alignment methods chosen baselines experiments.WordAlign-1 : WordAlign-1 based GHKM-like method (Galley et al., 2004)uses word alignments infer syntactic correspondences. implementation,GIZA++ toolkit grow-diag-final-and method used obtainsymmetric word alignment sentence pairs. sub-tree alignmentsheuristically induced selecting node correspondences consistentword alignment result (i.e., sub-tree alignments violate wordalignments). chose method widely adopted moderntree-to-tree systems.WordAlign-2 : second baseline essentially WordAlign-1.difference WordAlign-1 improved word alignment system usinglink-deletion techniques (Fossum et al., 2008). basic idea delete harmfulalignment links initial word alignment result (e.g., deleting linkFigure 2(a)). experiments considered likelydeletion top-10 common Chinese words (including {, , , ,758fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation, , , , , }) top-10 common English words (including {the,of, and, to, in, a, is, that, for, on}).HeuristicAlgin: HeuristicAlgin re-implementation approach proposedTinsley et al.s (2007) work. method alignment confidence every nodepair first computed lexical translation probabilities, used obtainnode correspondences via heuristic algorithm. method requiretraining process successfully adopted several translation tasks,French-English translation, chosen another baseline comparison.5.2 Experimental Setupsettings experiments described follows.5.2.1 Data Preparationbilingual corpus consists 1.06 million sentence pairs.11 mentioned above,used GIZA++ grow-diag-final-and heuristics generate 1-best/k-best wordalignments, used baseline word alignment results. parse treesChinese English generated using Berkeley Parser.12 publicly availablecorpus used evaluate sub-tree alignment result.13 consists 736 node-alignedsentence pairs (with gold-standard parse trees language sides) LDC2003E07also included bilingual data. corpus divided two parts:held-out set used finding appropriate setting hyperparameters (99 sentencesarticles 301-309), test set used evaluating sub-tree alignment systems (637sentences articles 001-066). MT experiments, 5-gram language model trainedXinhua portion Gigaword corpus addition English part LDCbilingual training data.14 used NIST 2003 MT evaluation corpus developmentset (919 sentences) newswire portion NIST 2004-2006 MT evaluation corporatest set (3,486 sentences).5.2.2 Sub-tree Alignmentparameters sub-tree alignment model initialized add-one smoothingrule-set extracted using word alignments (i.e., WordAlign-1 baseline). Then,model trained parse trees bilingual corpus using EM algorithmVariational Bayes (VB) approach. implementation VB-based training,hyperparameters assumed share value.15 leads setting = 0.0111. LDC category: LDC2003E14, LDC2005T10, LDC2003E07, LDC2005T06, LDC2005E83, LDC2006E26,LDC2006E34, LDC2006E85, LDC2006E92 LDC2004T08. See http://www.ldc.upenn.edu/details.12. Note LDC2003E07 corpus reused gold-standard parse trees provided ChineseEnglish treebanks.13. Available http://www.nlplab.com/resources/nodealigned-bitreebank.html14. LDC category English Gigaword corpus: LDC2003T0515. Although could adopt different hyperparameters finer control priors model parameters, found setting hyperparameters value could also lead satisfactoryperformance.759fiXiao & Zhuoptimal value held-out set. default, trained model 5 EMVariational EM iterations. speed-up training process avoid degenerateanalysis caused large rules, restricted rules reasonable sizes rules five frontier non-terminals depth three. rulesfive frontier non-terminals, considered tree-fragments depth onerestrict number frontier non-terminals involved, is, flat tree structures,used associated height-one tree-fragments. Besides, discarded sub-treealignment every node pair whose terminals aligned outside correspondingspans two times WordAlign-1.5.2.3 Machine Translationused NiuTrans open-source toolkit (Xiao, Zhu, Zhang, & Li, 2012) buildtree-to-tree MT system. rule extraction, used extension GHKM methodextract minimal tree-to-tree transformation rules (Liu et al., 2009a) obtained largerrules composing two three minimal rules (Galley et al., 2006). used CKY-styledecoder cube pruning (Huang & Chiang, 2005) beam search decode Chinesesentences. default beam size set 50. addition features describedEquations (33)-(37), also used several features MT system, including5-gram language model, rule number bonus, target length bonus two binaryfeatures - lexicalized rule low frequency rule (Marcu et al., 2006). featurescombined log-linear fashion optimized using Minimum Error Rate Training (MERT,Och, 2003).5.3 Resultsfollowing part section, present experimental results, including evaluations sub-tree alignments, extracted rules, MT systems. Also, show resultsseveral improved methods effective use approach tree-to-tree MT.5.3.1 Evaluation AlignmentsFirst evaluated alignment quality various sub-tree alignment approaches termsprecision (P), recall (R) F-1 score.16 See Table 2 results three baselinesystems sub-tree alignment system. measures, VB-based systemsignificantly improves overall recall F-1 score, slightly degrading precisioncompared WordAlign-1/2. Also, VB-based training outperforms EM-based counterpart due priors introduced learning process. interesting observationthat, though EM training model suffers degenerate analysisdata, show extremely bad results experiment. phenomenon duerestriction size tree-fragment training. described Section 5.2.2,restricted translation rules reasonable-size tree-fragments several ways (e.g.,16. Let predicted number alignments system output, correct number correct alignmentssystem output, gold number alignments gold-standard. measure precision, recall2)precisionrecallcorrectF- score defined as: precision = predicted, recall = correctF- = (1+.gold2 precision+recallparameter controls preference recall (i.e., > 1) precision (i.e., 0 < 1).NLP tasks set 1, indicating equal weights recall precision.760fiUnsupervised Sub-tree Alignment Tree-to-Tree TranslationEntryOverallNP NPNN NNVP VPPU ,IPPU .NP NNNP PPNN NNSNR NNPNN NPPP PPNN JJPQP NPWordAlign-1PRF-175.4 63.6 69.086.9 59.0 70.383.9 75.6 79.575.3 61.9 68.084.7 76.5 80.492.5 87.5 89.998.5 98.7 98.677.6 71.5 74.446.8 63.8 54.084.2 76.0 79.969.4 41.2 51.865.1 59.8 62.484.6 68.4 75.790.7 76.5 83.081.5 69.8 75.272.2 64.9 68.3WordAlign-2PRF-176.3 64.5 69.987.0 62.1 72.583.6 77.0 80.274.9 61.9 67.884.7 76.5 80.492.3 87.7 89.998.5 98.7 98.683.0 71.5 76.849.7 63.5 55.783.8 76.5 80.069.9 41.7 52.365.9 61.0 63.385.1 69.2 76.390.4 76.3 82.782.3 68.3 74.772.6 65.2 68.7HeuristicAlginPRF-165.7 67.7 66.779.1 73.6 76.376.2 74.1 75.171.3 71.8 71.669.6 71.3 70.390.6 90.7 90.698.5 98.7 98.659.3 77.5 67.253.1 31.9 39.877.8 75.5 76.763.4 57.4 60.271.8 50.4 59.279.3 72.6 75.883.9 81.7 82.881.2 72.2 76.467.1 65.6 66.4(EM)PRF-179.8 46.2 58.584.2 48.2 61.381.8 63.7 71.680.5 49.0 60.982.7 67.7 74.590.4 76.8 83.094.7 86.8 90.675.4 62.3 68.243.2 42.1 42.683.7 58.3 68.757.7 41.5 48.370.5 48.1 57.285.6 56.9 68.484.2 69.1 75.979.9 57.9 67.178.3 42.2 54.8(VB)PRF-172.6 75.1 73.888.7 75.3 81.481.1 79.9 80.575.7 75.8 75.782.5 80.7 81.690.0 92.4 91.298.5 98.5 98.675.9 78.9 77.454.5 70.1 61.381.0 77.6 79.267.2 56.3 61.371.1 67.7 69.485.4 82.5 83.985.9 81.2 83.584.7 76.1 80.274.9 71.7 73.3Table 2: Evaluation results sub-tree alignment system baselines.measures reported percentage.set parameter maximum depth). constraints reduce number rulesinvolved training, prevents use rare large rules. result indicatesfact tree-fragment size constraint actually important efficiencyalso crucial learning. discussed previous work, without constraintsimposing proper prior, solution EM degenerate (Marcu & Wong, 2002; DeNero,Gillick, Zhang, & Klein, 2006).addition, Table 2 shows result 15 common types sub-tree alignment. expected, VB-based system achieves best F-1 score cases.interestingly, observed approach obtains significantly better performancehandling PP (Prepositional Phrase) alignment seems difficult problembaselines due unclear boundary indicators aligning PP structures. attributebetter use syntactic information language sides model,generally ignored traditional models based surface heuristics word alignments.5.3.2 Evaluation Extracted Rulesapplied sub-tree alignment result tree-to-tree system study impactsub-tree alignment MT. discussed Section 4, rule extraction downstreamcomponent sub-tree alignment current tree-to-tree MT pipeline. thereforechose evaluate quality rules obtained various sub-tree alignment results.determine goodness extracted grammars, computed rule precision, recall,F-1 scores approach baseline approaches test set used(1-best) alignment quality evaluation. make gold-standard grammar, chosemethod used Fossum et al.s (2008) work grammar automaticallygenerated manually-annotated alignment result, is, rules extracted usingannotated sub-tree alignments regarded gold-standard computing variousevaluation scores. Table 3 shows evaluation result grammars extracted761fimatrix1bestXiao & ZhuEntryWordAlign-1WordAlign-2HeuristicAlgin(EM)(VB)(VB + Pmin(VB + Pmin(VB + Pmin(VB + Pmin= 105 )= 106 )= 107 )= 108 )Rule P51.952.355.861.954.979.653.041.334.9Rule R60.861.855.349.265.234.570.075.679.5Rule F-155.956.655.554.859.648.260.353.448.5Table 3: Evaluation results rules obtained various sub-tree alignment approaches.measures reported percentage.different sub-tree alignment approaches. see improvements persistsub-tree alignments employed translation rule extraction. VB-based approachproduces grammars higher rule F-1 score three baselines.addition 1-best extraction, studied rule extraction behavessub-tree alignment matrix-based extraction method. Table 3 also shows resultsub-tree matrix-based extraction method different choices pruning parameterPmin . see smaller values Pmin result grammars higher rule recall. Also,better rule F-1 scores achieved adjusting Pmin seeking good balancerule precision rule recall , e.g., Pmin = 106 107 .scores informative measure grammar quality, also investigated differences rule sets obtained model comparedbaseline approaches, following Levenberg, Dyer, Blunsoms (2012) method. Figure 7shows probable rules (frequency 2) obtained bilingual corpus usingVB-based alignment approach appear model WordAlign-2alignment vice versa. asked two annotators sub-tree alignment estimaterule quality based syntactic correspondence adequacy frontier node sequencetwo languages sides. rule labeled good judges considered good quality. figure, see eight top-10 rules extractedusing approach absent WordAlign-2 grammar good rules. contrast,four top-10 rules baseline model good quality sense humanpreference. Furthermore, examined top-100 probable rules appeartwo grammars individually. Again, top-100 rules extracted using proposed modelbetter quality. results 61 good rules. contrast, 44% top-rankingrules induced using WordAlign-2 alignment good translation rules.5.3.3 Evaluation Translationsalso evaluated translations generated MT system different sub-tree alignment approaches. Since VB-based training shows best performance previousexperiments, chose default setting approach following experiments.Table 4 shows evaluation result translation quality estimated using caseinsensitive IBM-version BLEU4 (Papineni, Roukos, Ward, & Zhu, 2002) TER (Snover,762fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation1*2*3*45*6*7*8*9*10top-10 highest probability rules (for MT) approach absent WordAlign-2NP(DNP1 NN()) VP(ADVP1 VP(VBD(improved)))NP(PU( ) NP(CD() NN()) PU()) NP(DT(the) CD(two) NNS(sessions))NP(NP(QP1 NP(NN())) NP2 ) NP(X2 NP(CD1 NP(NNS(represents))))NP(NN() NP1 ) VP(VB(desire) NNP1 )NP(PU() NP(NR1 NN()) PU()) NP(() NP(NNP1 NN(independence)) ())NP(VP1 DEC() NP(NN() NN()))NP(ADJP(ADJP1 JJ(ideological)) NN(struggle))NP(NP(QP1 NP2 ) NP(ADJP(JJ()) NP(NN())))NP(NP(DT(the) JJ(important) NN(thinking)) IN(of) SBAR(WHNP1 S2 ))NP(IP1 DEG() NP(NN() NN())) NP(ADJP(ADJP1 JJ(practical))NN(significance))NP(NP(PU() NP(QP1 NN()) PU()) NP(ADJP2 NN()))NP(NP(DT(the) JJ2 NN(idea)) PP(IN(of) NP(DT(the) CD1 NNS(represents))))NP(PU() NN() PU1 ) VP(VBG(joining) NP(DT(the) NN1 ))top-10 highest probability rules (for MT) WordAlign-2 absent approach1LCP(QP1 LC()) ADJP(JJ1 )2*NP(DNP1 NN()) VP(ADVP1 VBP(changes))3*NP(NN() NN1 ) NP(CD(three) NNS(links) X1 )4NP(DNP(IP1 DEC()) NP(NN())) NP(ADJP1 NN(significance))5VP(ADVP1 VP(VV2 NP(NN() NN3 )))VP(ADVP1 VP(VP(VV2 ) NP(DT(the) JJ(mass) NN3 )))6IP(NP1 VP(VV() NP(NN() NN2 ))) NP(NP(NNS1 ) PP(IN(for) NP2 ))7NP(VP1 DEG() NN()) ADJP(JJ1 )8NP(NP(PU() NT1 PU()) NP(NN()) NR())NP(NP(PRP$(his)) QP(CD1 ) NN(speech))9*VP(VP(ADVP1 VP(VV() CC() VV())) NP2 )VP(ADVP1 VP(VP(VB(strengthen) CC(and) VB(improve)) NP2 ))10* NP(NP(PU() NN() PU()) NP1 )NP(NP(() NP(NN(taiwan) NN(independence)) ()) NNS1 )Figure 7: top-10 highest probability rules built proposed sub-tree alignmentapproach WordAlign-2 baseline grammar, top-10 rulesWordAlign-2 baseline grammar obtained using proposedsub-tree alignment approach. * = good translation rule.Dorr, Schwartz, Makhoul, Micciula, & Weischedel, 2005), significance test performed using bootstrap resampling method (Koehn, 2004). Moreover, efficiencyrule extraction reported terms rule-set-size/extraction-time. comparison,also report result rule extraction using word alignment matrices (Liu et al., 2009b)WordAlign-1 WordAlign-2.Table 4 indicates approach outperforms baselines BLEU TERmeasures 1-best 30-best extraction. addition, matrix-based methodmuch efficient k-best method. example, compared 30-best extraction, extracting rules sub-tree alignment matrices 9 times efficient. However,rules counted unit one parameter estimation translation model,using alignment matrices show significant BLEU improvements TER reductions comparison 30-best counterpart (see rows marked unitcount).many additionally extracted rules utilized real translation.example, observed 7.3% rules used generating final (1-best)763fiXiao & ZhuEntryWordAlign-1 (1-best)WordAlign-2 (1-best)HeuristicAlgin (1-best)WordAlign-1 (30-best)WordAlign-2 (30-best)HeuristicAlgin (30-best)WordAlign-1 (matrix)WordAlign-2 (matrix)(1-best + unitcount)(30-best + unitcount)(matrix + unitcount)(1-best + posterior)(30-best + posterior)(matrix + posterior)DevTestBLEU4[%] TER[%]BLEU4[%] TER[%]36.236.235.736.336.435.436.9*36.8*36.7*36.8*36.9*36.9*37.0*37.4**34.234.233.834.434.6*33.935.0*35.1*34.9*35.0*35.3**35.0*35.2**35.6**57.056.957.257.257.057.256.556.656.656.656.3*56.4*56.2**55.9**58.358.158.358.258.058.057.9*57.7*57.857.6*57.5*57.4*57.0**57.1**Rule-setsize24.8M25.3M22.7M32.7M33.0M32.4M50.2M53.8M27.0M37.4M54.9M27.0M37.4M54.9MEfficiency(rule/sec)75.475.972.03.83.93.835.837.978.84.137.778.84.137.7Table 4: Evaluation translations different alignment approaches. BLEU, higherbetter. TER, lower better. unitcount means take ruleoccurrence unit one parameter estimation, posterior means userule posterior probabilities fractional counts parameter estimation. * **= significantly better three 1-best baselines (p < 0.05 0.01).translations indeed extracted alignments seen 30-bestalignments. thus indicates fact naively increasing number rules mighteffective improving translation quality.last three rows Table 4 show result using alignment posterior probabilitiesparameter estimation (i.e., method described Section 4.3). see alignmentposterior probabilities helpful improving translation quality systemweight rules confidence (entries unitcount vs. entriesposterior ). using sub-tree alignment matrices rule extraction alignmentposterior probabilities parameter estimation, approach finally achieves +1.0 BLEUimprovement -0.9 TER reduction 30-best case baselines. evenoutperforms word alignment matrix-based counterpart +0.5 BLEU points -0.6TER points (both significant p < 0.05).Further, effectiveness proposed approach demonstrated terms BLEUTER scores rule-set size. Figure 8 compares approachbaseline approaches different numbers unique rules extracted.17 Clearly,number unique rules, proposed sub-tree alignment approach leads better translations baselines.5.3.4 Impact Alignment Grammar Quality MT Performanceexperiments demonstrate effectiveness proposed approach termsdifferent measures individually. next natural question sub-tree alignment17. this, adjusted Pmin obtain grammars different sizes approach.approaches, used different k-best lists rule extraction.764fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation431 - TER[%]BLEU4[%]363534HeuristicAlginWordAlign-2WordAlign-142HeuristicAlginWordAlign-2WordAlign-141331020304050607010203040506070Rule-set size (million)Rule-set size (million)Figure 8: BLEU 1-TER rule-set sizerule extraction affect translation quality. study issue importantoptimize upstream systems MT decoding select appropriate evaluationmetrics good prediction MT performance.therefore carried another set experiments compares translationquality different sub-tree alignment rule extraction settings. generate diversesub-tree alignment rule extraction results, varied values Pminsub-tree alignment rule extraction respectively. way, obtained ensemblessub-tree alignments grammars different precision recall scores.18 choseF- score evaluation metric sub-tree alignment system ruleextraction system. Instead fixing 1, varied value 0.5 3. Sinceparameter control bias towards precision recall, choosing different valueshelpful seeking good tradeoff precision recall. findappropriate evaluation measure sub-tree alignment rule extraction predictingMT performance well.Figures 9 10 plot F- scores measures MT performance sub-tree alignment rule extraction. Figure 10, see rule F-3 score correlates besttranslation quality measures, indicates MT system prefers rulerecall-biased metrics. agrees observation Figure 8 MT systemmake benefits rules. hand, curves Figure 9 showbetter correlation sub-tree alignment F-2/F-3 score translation quality measures, implying preference relatively higher sub-tree alignment recall. resultreasonable framework node alignment links result alignedtree-fragments (or rules) extracted. high-recall sub-tree alignment generally resultsbig grammar high rule recall, thus better BLEU TER results. also com18. example, larger value generally results higher alignment precision, small valueprefers higher alignment recall. rule extraction, larger value Pmin generally leads grammarhigher rule precision, choosing smaller Pmin generate grammar higher rule recall.765fiXiao & Zhu80sub-tree alignment F-[%]sub-tree alignment F-[%]807060F-0.50F-0.75F-1.00F-2.00F-3.00504033.53434.53535.57060F-0.50F-0.75F-1.00F-2.00F-3.0050403641BLEU4[%]42431 - TER[%]70706060rule F-[%]rule F-[%]Figure 9: BLEU 1-TER sub-tree alignment F- measure5040F-0.50F-0.75F-1.00F-2.00F-3.0030203434.5355040F-0.50F-0.75F-1.00F-2.00F-3.00302035.541.5BLEU4[%]4242.5431 - TER[%]Figure 10: BLEU 1-TER rule F- measureputed Pearsons correlation coefficients sub-tree alignment/rule F-3 scoreBLEU/TER. sub-tree alignment F-3, correlation coefficients BLEU TER0.971 -0.962 respectively. rule F-3, correlation coefficients BLEUTER 0.983 -0.963 respectively. show good correlations translation quality measures. Another interesting observation MT performancesensitive change rule F- score change sub-tree alignmentF- score. may lie rule extraction direct upstream step decodingimpacts output MT systems. contrast, sub-tree alignment front-endstep MT pipeline indirect effect actual translation process.766fiUnsupervised Sub-tree Alignment Tree-to-Tree TranslationSystemHierarchical phrase-basedTree 1-best word alignment (WordAlign-2)Word alignment matrixtree Sub-tree alignment matrixDevTestBLEU4[%] TER[%]BLEU4[%] TER[%]37.236.837.037.9*35.235.335.236.0**57.356.4*56.3*55.8**58.057.757.5*56.8**Table 5: MT Evaluation results rules obtained various alignment approaches.BLEU, higher better. TER, lower better. * ** = significantly betterhierarchical phrase-based baseline (p < 0.05 0.01).5.3.5 ImprovementsPrevious work pointed straightforward implementation tree-to-tree MTsuffers problems rules derivations either rule extractiondecoding process (Chiang, 2010). advance tree-to-tree system comparestate-of-the-art, employed tree binarization (Wang et al., 2007b) fuzzydecoding (Chiang, 2010) system. alignment approach equippedgeneral framework tree-to-tree translation, trivial conduct another setexperiments investigate effectiveness approach stronger system.19 Table5 shows BLEU TER scores system enhanced methods.20comparison, also report result state-of-the-art MT system implementshierarchical phrase-based model (Chiang, 2007) tree-to-tree system extractsrules using word alignment matrices (Liu et al., 2009b). Table 5 indicates superiorityapproach tree binarization fuzzy decoding involved. significantlyoutperforms hierarchical phrase-based system (+0.7 BLEU points -1.2 TER points)tree-to-tree system based word alignment matrices (+0.8 BLEU points-0.7 TER points).discussed Section 4, transfer rules sub-tree alignment model resemblesgeneral form STSGs directly used MT. Instead resortingexplicit step rule extraction, use rules sub-tree alignment modelMT decoding, i.e., sub-tree alignment cast grammar induction step. thereforebuilt another system directly acquires translation rules sub-tree alignmentstep. this, need output rules derivation forest generatedalignment model. rule probabilities obtained using inside outputprobabilities, pruning performed throwing away rules whose probabilityPmin . addition rule probability, reused n-gram language model, rule numberbonus, target length bonus, lexicalized rule low frequency rule indicatorsbase tree-to-tree system additional features fair comparison. obtain goodreasonable result, employed fuzzy decoding tree binarizaiton experiment.19. choose setting previous experiments gold-standard alignment annotation Penn Treebank-style trees only. difficult evaluate alignment grammarquality binarized trees due lack benchmark data. experiments first conductedexperiments individual tasks (see Sections 5.3.1-5.3.3), studied correlations simplereasonable setting consistent result sub-tree alignment MT (see Section 5.3.4).investigated effectiveness approach advanced tree-to-tree system (see Section 5.3.5).20. implementation parse trees binarized head-out fashion.767fiXiao & ZhuEntryBaseline (explicit rule extraction)Rules sub-tree alignment modelRules sub-tree alignment model + MERTBaseline + sub-tree alignment featuresDevTestBLEU4[%] TER[%]BLEU4[%] TER[%]37.936.236.738.236.034.334.936.155.857.957.355.856.858.858.056.9Table 6: MT Evaluation results obtaining rules sub-tree alignment modelobtaining rules traditional rule extraction pipelineTable 6 compares results sub-tree alignment matrix-based rule extraction inducing rules alignment model (Row 1 vs. Row 2). Unfortunately, straightforwardlyinferring rules probabilities sub-tree alignment model underperformsbaseline. might attributed several reasons. First, due large derivationspace, cannot enumerate relatively large tree-fragments sub-tree alignmentstep, instead access tree-fragments limited depths. contrast, baseline system extracts basic rules using sub-tree alignment matrices obtainslarge rules heuristics (e.g., rule composing). additional rules obtainedbaseline framework rule extraction general useful modern syntax-basedsystems (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007). Second, ruleprobability sub-tree alignment model defined product probability factorsgood generation story. However, MT systems usually use features required form generative model, features shown Equations (33)-(37).consequence, many well-developed features used baseline systemavailable sub-tree alignment model. Third, sub-tree alignment modeltrained maximizing likelihood criteria, consistentadopted MT system (i.e., minimizing evaluation-related error rate function).study issues, improved system two ways. First, treated fourprobability factors sub-tree alignment model (See Equation (13)) differentfeatures MT decoder, tuned weights using MERT. Row 3 Table 6 showsmethod achieves better results system employing unweighted probabilityfactors. However, performance still worse baseline, indicatesMT-oriented features rule extraction heuristics crucial successtree-to-tree system. Finally added probabilistic factors sub-tree alignmentmodel baseline system additional features. shown last row Table 6,enhanced system yields modest BLEU improvements baseline, TERimprovement achieved. results give us two interesting messages - 1) ruleextraction heuristics, MT-oriented features objectives learning key factors contributing good tree-to-tree system; 2) better use sub-tree alignmentmodel upstream module rule extraction decoding, rather usingsimple step grammar induction.last issue investigate whether sub-tree alignment model makebenefits labeled data. Although focus unsupervised learning work,proposed model require strictly unsupervised condition. Insteadenhanced use labeled data. idea simple: combine probabilityfactors sub-tree alignment model log-linear weighted fashion. means768fiUnsupervised Sub-tree Alignment Tree-to-Tree TranslationEntryUnweightedWeighted (weights learned labeled data)DevTestBLEU4[%] TER[%]BLEU4[%] TER[%]37.938.336.036.155.855.656.856.9Table 7: Comparison unweighted weighted sub-tree alignment modelsprobability factors sub-tree alignment model taken real valued featurefunctions, feature weights learned labeled data supervised methods.way, unweighted generative model (i.e., factor weight one)transformed weighted model (i.e., factor individual weight). Noteweighted model almost form used SMT systems.difference SMT model language model needed targetlanguage side fixed sub-tree alignment step. avoid bias towardsmany rules, also added rule number additional feature new model.training test, divided node-aligned gold-standard data two parts -first 310 sentences selected weight training, remaining 327 sentencesselected testing system. learn feature weights supervised manner, choseMERT one powerful tools training log-linear models. errorfunction used MERT defined one minus sub-tree alignment F-1 score.327-sentence test data (with tree annotation Penn Treebanks),weighted model achieves alignment F-1 score 75.4% rule F-1 score 60.0%,respectively. result better unweighed (and unsupervised) modelobtains alignment F-1 score 72.4% rule F-1 score 59.2%data set. Finally tested MT performance best setting (i.e., sub-tree alignmentmatrix-based rule extraction + tree binarization + fuzzy decoding).21 Table 7 showsweighted sub-tree alignment model leads better BLEU score tuning setshow promising improvements test data. size labeled corpussmall, expect better results labeled data available. Also worth studyingsophisticated supervised methods learn better weights, kernel-basedmethods (Sun et al., 2010b). supervised/semi-supervised learning focuswork, leave interesting issues future investigations.6. Related WorkSyntax-based approaches widely adopted machine translation lastten years. Many successful syntactic MT systems developed shown goodresults several translation tasks (Eisner, 2003; Galley et al., 2004, 2006; Liu et al.,2006; Huang et al., 2006; Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010). Despitedifferences modeling implementation details, models require alignmentstep acquire syntactic correspondence source target languages.standard SMT, syntax-based MT systems use word alignments infer syntacticalignments string-tree/tree-tree pairs. However, word alignments generallygood quality viewpoint syntactic alignment. makes sense directly21. sub-tree-aligned data binarized trees, reused weights learned PennTreebank-style trees four probability factors sub-tree alignment model.769fiXiao & Zhuinduce sub-tree level alignments pairs sentences syntactic informationeither language side both. especially true tree-to-tree MTactually need alignment sub-trees two languages, rather surfacealignment words. several lines work address syntactic alignmentproblem make better use various alignment results tree-to-tree translation.6.1 Word Sub-tree Alignment Machine Translationearliest efforts syntactic alignment focus enhancing word alignment modelssyntactic information. date, several research groups (Fraser & Marcu, 2007; DeNero& Klein, 2007; May & Knight, 2007; Fossum et al., 2008; Haghighi, Blitzer, DeNero, &Klein, 2009; Burkett, Blitzer, & Klein, 2010; Riesa, Irvine, & Marcu, 2011) proposedsyntax-augmented models advance word alignment systems. Although modelsachieved promising improvements, still address alignment problem word level.discussed Section 1, methods might desirable choices learningcorrespondence tree nodes two languages. alternative straightforward solution, researchers tried infer sub-tree level alignments pairs syntactictrees. example, Imamura (2001), Groves, Hearne, Way (2004), Tinsley et al.(2007) defined several scoring functions measure similarity sourcetarget sub-trees, aligned tree nodes greedy algorithms. approaches,though simple implement, derived principled way. example,models explicit optimization procedure, general framework statistical learning. Instead, model parameters obtained using additional alignmentmodels lexicons. another line research, Sun et al. (2010a, 2010b) attempted address sub-tree alignment problem supervised/semi-supervised models. usedtree kernels various syntactic features advance sub-tree alignment systemshowed promising results Chinese-English translation tasks. However, approach stillrelies heuristic algorithms inferring node correspondences two parse trees.Beyond this, train tree kernels, approach requires additional labeled datagenerally expensive build. Unlike studies, derive sub-tree modelprincipled way develop unsupervised sub-tree alignment framework tree-to-treeMT.6.2 Unsupervised Syntactic Alignmentalso previous studies resort labeled data sub-tree alignment.earliest Eisners (2003) work. designed unsupervised approachmodeling sub-tree alignment problem STSG formalism. However, sincedetailed derivation model decomposition provided, model computationallyexpensive, even difficult applied current tree-to-tree systems complex treestructures involved. Gildea (2003) also applied STSGs tree-to-tree/tree-to-stringalignment. developed loosely tree-based alignment method address issueparse-tree isomorphism bitext. work targets word alignment rathermodern syntactic MT systems. Recently Nakazawa Kurohashi (2011) proposedBayesian approach sub-tree alignment dependency trees, testedJapanese-English MT system. Actually model much common model770fiUnsupervised Sub-tree Alignment Tree-to-Tree Translationpresented work. example, apply unsupervised learning methodsBayesian models sub-tree alignment. hand, two studies differimportant aspects. First, Nakazawa Kurohashi (2011) restricted sub-treealignment dependency trees, different aligning tree nodesphrase structure trees. Since phrase structure trees involve complex structuressyntactic categories, alignment problem phrase structure trees relativelydifficult dependency-based counterpart. Second, model makes benefitsrecent advances STSGs directly applicable current state-of-the-art tree-totree systems.Another related work presented Pauls, Klein, Chiang, Knights(2010) work. factored node-to-string alignment model componentsgenerates target side synchronous rule source side. Moreover, probabilityrule fragment factored lexical structural component work. Actually,model proposed model two variants theme. appearobvious differences them. First, focus sub-tree alignment tree-to-treetranslation, Pauls et al. (2010) addressed alignment issue tree-to-string/stringto-tree translation. model, parse language sides independently, ratherparsing one side projecting syntactic categories. result, inference fasterwork since need consider possible parse trees unparsed sidealignment. Second, permutation model presented work generalorder handle non-ITG trees. Third, investigate methods effective usesub-tree alignment MT. particular, present rule extraction approach obtainingadditional translation rules using sub-tree alignment posteriors, rather learning rules1-best sub-tree alignment.6.3 Rule Extraction Using Various Alignment Resultsmachine translation, word syntactic alignments used extract translation rulesphrases. traditional pipeline rule phrase extraction, 1-best alignment result considered, suffers limited scope single alignment.efficiently obtain diverse alignment/parsing results, packed data structures adoptedimprove 1-best pipeline MT systems recent years (Mi & Huang, 2008; Liu et al., 2009a;Zhang, Zhang, Li, Aw, & Tan, 2009). example, Liu et al. (2009b) de Gispert et al.(2010) used alignment posterior probabilities phrase hierarchical phrase extraction.development sub-tree alignment matrices actually motivated similar ideaword alignment matrices. difference work use sub-treelanguage sides infer alignment posterior probabilities, probabilitiescalculated word/phrase-level previous work (Liu et al., 2009b; de Gispert et al.,2010). Moreover, knowledge, effectiveness sub-tree alignment matrixsystematically studied case tree-to-tree translation.Note approach presented work also something similar synchronous grammar induction. example, model results STSGformalism used MT. Recent studies Bayesian models (Blunsom, Cohn,Dyer, & Osborne, 2009; Cohn & Blunsom, 2009; Levenberg et al., 2012) shownpromising results directly learning synchronous grammars bilingual data hierar771fiXiao & Zhuchical phrase-based string-to-tree systems, rather extracting synchronous grammarrules based explicit word/syntactic alignment step. However rare see relatedwork tree-to-tree MT. principle article different previous work synchronous grammar induction. example, aim work learn sub-treealignment model, applied many potential applications except MT,sentence compression paraphrasing test summarization (Jing, 2000; Cohn & Lapata,2009). Unlike synchronous grammar induction alignment implicitly encodedlearning process, treat sub-tree alignment separate task. easesdevelopment tuning alignment system actually resort MTsystems slow difficult optimize. Another advantage approachmake benefits compact models, rather used MT greatnumber rules involved. Take implementation instance. alignment modellearned relatively small set grammar rules (rules limited depths),MT system accesses much larger grammar many additional rules involvedrule composing. method result efficient alignment system likelyalleviate degenerate analysis data, cost degrading MT performance.7. Discussionunderlying assumption proposed model 1-to-1 sub-tree alignmentsachieved based constraints imposed neighboring parts tree (see Section2). makes sense standpoint linguistically-motivated models, yet turnfaces problem constraints make difficult align sentences/trees correctly,particularly free translations. several reasons explainapproach works nice tree-to-tree MT suffer greatlystructure divergence languages. First, model flexible allowsnode deletion/insertion alignment. means levels treenecessary require every node aligned valid node language side,instead nodes dropped needed. advantage methodcannot confidently align node node counterpart tree, alignvirtual node enforce bad constraints aligning parts tree.useful flat tree structures partial translationssyntactically well-formed. Second, main purpose approach infersub-tree alignment probabilities used pruning sub-tree alignment matricesextracting rules MT systems. Though 1-to-1 alignment required trainingsub-tree alignment model, actually access large number alignmentalternatives rule extraction, even cannot appear derivationdue alignment constraints. Third, model work phrase structuretrees. Instead Penn Treebank-style trees difficult alignmentcases, sub-tree alignment system works well binarized trees shows promisingimprovements various baselines. Note tree binarization effective methodalleviate structure divergence problem, especially Chinese-English translation.Also, might interesting investigate methods dealing differencessyntactic structures languages, forest-based methods (Mi & Huang,2008; Liu et al., 2009a).772fiUnsupervised Sub-tree Alignment Tree-to-Tree TranslationAnother note approach. implemented naively, speed sub-tree alignment system slow since model needs calculation alignment probability pairs tree-fragments. Fortunately, thought computation, severaloptimizations make system much efficient practice. First, describedwork, pruning methods employed restrict number tree-fragmentsreasonable level. experiments, system pruning achieved speed 1.82.2sentence/second single core Intel Xeon 3.16-GHz CPU. Another way speedimprovement parallel processing. good property EM-style algorithmsE-step easily implemented parallel computation environment.need divide training data set number smaller parts, runinside-outside algorithm parts parallel (i.e., Map procedure). expectedcounts model parameters accumulated results parts (i.e.,Reduce procedure). M-step performed usual. implementationused 40 threads parallel training. running time one training iteration1-million-sentence corpus 14-17 hours. Note system speed-upexpected powerful distributed infrastructures available (e.g., clusters +Hadoop), difficult scale approach handle millions sentence pairsusing current training framework.8. Conclusionsproposed unsupervised probabilistic sub-tree alignment approach tree-totree translation. factoring alignment model several components, resultingmodel easily learned using EM algorithm variational Bayesian approach.Also, investigated different ways applying proposed model tree-to-treetranslation. particular, developed sub-tree alignment matrix encodesexponentially large number alignments. representation sub-tree alignment,desirable rules extracted efficiently using k-best sub-tree alignmentresult. experiments showed proposed model achieved significant improvementsalignment quality grammar quality several baselines. NIST ChineseEnglish evaluation corpora, achieved +1.0 BLEU improvement -0.9 TER reductiontop state-of-the-art tree-to-tree system. improved MT system even significantlyoutperformed state-of-the-art hierarchical phrase-based system equipped treebinarization fuzzy decoding.Acknowledgmentswork supported part National Science Foundation China (Grants61073140 61272376), Natural Science Foundation Youth China (Grant61300097), China Postdoctoral Science Foundation (Grant 2013M530131), Specialized Research Fund Doctoral Program Higher Education (Grant 20100042110031),Fundamental Research Funds Central Universities (Grant N100204002).authors would like thank anonymous reviewers pertinent insightful comments, Keh-Yih Su great help improving early version article, Ji773fiXiao & Zhuhelpful discussions, Chunliang Zhang Tongran Liu language refinement.corresponding author article Jingbo Zhu.Appendix A. Part-of-speech Tags Phrase Structure Labelswork annotation POS tagging phrase structure parsing follows standard defined Penn English Chinese Treebanks (Marcus et al., 1993; Xue et al.,2005). See Tables 8-11 lists POS tags constituent labels used exampletrees article.POS TagADNNNRPPNPUVVDescriptionAdverbAspect ParticleNoun (except proper nouns temporal nouns)Proper NounPrepositionPronounPunctuationVerb (except stative verbs, copulas, mainverbs , )Table 8: Chinese POS tags used examplesPOS TagDTJJNNPNNSPRPRBVBDVBNVBP,.DescriptionDeterminerPrepositionAdjectiveProper noun (singular)Noun (plural)Personal PronounAdverbVerb (past tense)Verb (past participle)Verb (non-3rd person singular present)CommaPeriodTable 9: English POS tags used examplesSyntactic LabelIPNPPPQPVPDescriptionSingle ClauseNoun PhrasePreposition PhraseQuantity PhraseVerb PhraseTable 10: Chinese constituent labels used examples774fiUnsupervised Sub-tree Alignment Tree-to-Tree TranslationSyntactic LabelADVPNPPPVPDescriptionAdverb PhraseNoun PhrasePreposition PhraseSentenceVerb PhraseTable 11: English constituent labels used examplesDistributionPnt ()Ptree ()Preorder ()Notationtnt |sntttree |tnttvnt |svntPlength ()Pw ()l|mtw |swDescriptionsnt tnt source target-language non-terminal symbolsttree target-language tree-fragmentsvnt tvnt vectors non-terminal symbols sourcetarget-languagesl numbers source target terminals (or words)sw tw source target terminals (or words)Table 12: Notations model parametersAppendix B. EM-based Training Sub-tree Alignment Modeldescribed Section 3, proposed sub-tree alignment model five types parameters, including non-terminal mapping probability Pnt (), target-language treefragment generation probability Ptree (), frontier non-terminal reordering probabilityPreorder (), word number probability Plength () word mapping probability Pw ().convenience use new set notations denote model parameters following description. See Table 12 symbol list.follow framework EM-based training described Figure3. See Figure 11 complete version EM algorithm parameters model.algorithm, EC() represents expected count input variable. (X = x)0-1 function returns 1 variable X takes value x, 0 otherwise. (k) (r; u, v)(k) (S, ) rule probability (see Equation (29)) probability subtree alignment (see Equation (20)), k indicatesprobabilities calculated based parameters k-th iteration. tree(), vnt()lex() functions return tree-structure, frontier non-terminal vector,terminal sequence input tree-fragment, respectively (see Section 3.2).basic idea E-step check rule r (given pair tree nodes u(r;u,v)v) update EC() relative probability (root(S),root(T)) . appliedupdate rules parameters tnt |snt , ttree |tnt , tvnt |svnt l|m (see lines 8-11).exception tw |sw . defined Equation (14), Pw (ti | sj ) directproduct factor,Pinstead use sum terminals source-language treefragment (i.e.,j=1 Pw (ti | sj )). follow result IBM Model 1 makeP|lex(s )|update magnitude proportional Pw (ti | sj )/ j 0 =1 r Pw (ti | sj 0 ). refer readerBrown et al.s (1993) work detailed derivation expected count IBM Model1. also worth noting algorithm performs parameter update baseddifferent choices (u, v) r E-step. means rule instance involvedparticular derivation one time (e.g., tree-fragment appears775fiXiao & Zhu1: Function TrainModelWithEM ({(S1 , T1 ), ..., (Sn , Tn )})(0)(0)(0)(0)(0)2: Initialize {tnt |snt , ttree |tnt , tvnt |svnt , l|m , tw |sw }3: k = 0 K 14:Set EC() = 0 model parameters6: E-step:5:Foreach tree pair (S, ) sequence {(S1 , T1 ), ..., (Sn , Tn )}6:Foreach node pair (u, v) (S, )7:Foreach rule r rooted (u, v)(k) (r;u,v)(snt =u)(tnt =v)(k) (root(S),root(T ))8:EC(tnt |snt )+=9:EC(ttree |tnt ) + =(k) (r;u,v)(tnt =v)(ttree =tree(tr ))(k) (root(S),root(T ))10:EC(tvnt |svnt ) + =(k) (r;u,v)(svnt =vnt(sr ))(tvnt =vnt(tr ))(k) (root(S),root(T ))11:EC(l|m )(k) (r;u,v)(m=|lex(sr )|)(l=|lex(tr )|)(k) (root(S),root(T ))12:Foreach word pair (sj , ti ) position (j, i) (lex(sr ), lex( tr ))+=(k)P(ti |sj )(sw =sj )(tw =ti )r P(k) (t |s )wj0j 0 =1(k) (r;u,v) P|lex(sw)|13:EC(tw |sw ) + =(k) (root(S),root(T ))10: M-step:14:15:16:17:18:19:20:21:22:23:24:Foreach non-terminal symbol pair (snt , tnt )(k+1)tnt |snt=EC(tnt |snt )Pt0ntEC t0nt |sntForeach target non-terminal symbol tnt tree-fragment structure ttreeEC(ttree |tnt )(k+1)ttree |tnt =Pt0treeEC t0tree |tntForeach pair non-terminal symbol vectors (svnt , tvnt )EC(tvnt |svnt )(k+1)tvnt |svnt =Pt0vntEC t0vnt |svntForeach pair word numbers (m, l)(k+1)l|m=EC(l|m )Pl0EC l0 |mForeach pair words (sw , tw )(k+1)tw |sw(K)=EC(tw |sw )Pt0w(K)EC t0w |sw(K)(K)(K)return {tnt |snt , ttree |tnt , tvnt |svnt , l|m , tw |sw }Figure 11: EM-based training algorithm model parametersdifferent positions), update corresponding parameters would carriedmultiple times.Another note EM algorithm. expected counts parametersefficiently calculated using inside outside probabilities according lines 8-11776fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation13. parameters efficient ways. example, discussedSection 3.4.1, expected count tnt |snt obtained without checking individualrule, is, omit loop r case. technique consideredspeed-up sub-tree alignment system.ReferencesAttias, H. (2000). variational bayesian framework graphical models. Solla, S. A.,Leen, T. K., & K., M. (Eds.), Advances Neural Information Processing Systems 12,pp. 209215. MIT Press.Beal, M. J. (2003). Variational algorithms approximate bayesian inference. Mastersthesis, University College London.Blunsom, P., Cohn, T., Dyer, C., & Osborne, M. (2009). gibbs sampler phrasalsynchronous grammar induction. Proceedings Joint Conference 47thAnnual Meeting ACL 4th International Joint Conference NaturalLanguage Processing AFNLP (ACL-IJCNLP), pp. 782790, Suntec, Singapore.Brown, P. E., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1993). mathematicsstatistical machine translation: Parameter estimation. Computational Linguistics,19, 263311.Burkett, D., Blitzer, J., & Klein, D. (2010). Joint parsing alignment weakly synchronized grammars. Human Language Technologies: 2010 Annual Conference North American Chapter Association Computational Linguistics(HLT:NAACL), pp. 127135, Los Angeles, California, USA.Chiang, D. (2005). hierarchical phrase-based model statistical machine translation.Proceedings 43rd Annual Meeting Association Computational Linguistics (ACL), pp. 263270, Ann Arbor, Michigan, USA.Chiang, D. (2007). Hierarchical phrase-based translation. Computational Linguistics, 33,4560.Chiang, D. (2010). Learning translate source target syntax. Proceedings48th Annual Meeting Association Computational Linguistics (ACL),pp. 14431452, Uppsala, Sweden.Chiang, D., & Knight, K. (2006). introduction synchronous grammars. Tutorials21st International Conference Computational Linguistics 44th AnnualMeeting Association Computational Linguistics (COLING-ACL).Chiswell, I., & Hodges, W. (2007). Mathematical Logic. Oxford University Press.Cohn, T., & Blunsom, P. (2009). Bayesian model syntax-directed tree string grammar induction. Proceedings 2009 Conference Empirical Methods NaturalLanguage Processing (EMNLP), pp. 352361, Singapore.Cohn, T., & Lapata, M. (2009). Sentence compression tree transduction. JournalArtificial Intelligence Research, 34, 637674.Das, D., & Smith, N. A. (2009). Paraphrase identification probabilistic quasi-synchronousrecognition. Proceedings Joint Conference 47th Annual Meeting777fiXiao & ZhuACL 4th International Joint Conference Natural Language ProcessingAFNLP (ACL-IJCNLP), pp. 468476, Suntec, Singapore.de Gispert, A., Pino, J., & Byrne, W. (2010). Hierarchical phrase-based translation grammars extracted alignment posterior probabilities. Proceedings 2010Conference Empirical Methods Natural Language Processing (EMNLP), pp.545554, Cambridge, MA, USA.Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data viaem algorithm. Journal Royal Statistical Society. Series B (Methodological),39, 138.DeNeefe, S., Knight, K., Wang, W., & Marcu, D. (2007). syntax-based MTlearn phrase-based MT?. Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural LanguageLearning (EMNLP-CoNLL), pp. 755763, Prague, Czech Republic.DeNero, J., Gillick, D., Zhang, J., & Klein, D. (2006). generative phrase models underperform surface heuristics. Proceedings Workshop Statistical MachineTranslation (WMT), pp. 3138, New York city, USA.DeNero, J., & Klein, D. (2007). Tailoring word alignments syntactic machine translation. Proceedings 45th Annual Meeting Association ComputationalLinguistics (ACL), pp. 1724, Prague, Czech Republic.Eisner, J. (2003). Learning non-isomorphic tree mappings machine translation.Companion Volume Proceedings 41st Annual Meeting AssociationComputational Linguistics (ACL), pp. 205208, Sapporo, Japan.Ferguson, T. S. (1973). bayesian analysis nonparametric problems. AnnalsStatistics, 1, 209230.Fossum, V., Knight, K., & Abney, S. (2008). Using syntax improve word alignmentprecision syntax-based machine translation. Proceedings Third WorkshopStatistical Machine Translation (WMT), pp. 4452, Columbus, Ohio, USA.Fraser, A., & Marcu, D. (2007). Getting structure right word alignment: LEAF.Proceedings 2007 Joint Conference Empirical Methods Natural LanguageProcessing Computational Natural Language Learning (EMNLP-CoNLL), pp. 5160, Prague, Czech Republic.Galley, M., Graehl, J., Knight, K., Marcu, D., DeNeefe, S., Wang, W., & Thayer, I. (2006).Scalable inference training context-rich syntactic translation models. Proceedings 21st International Conference Computational Linguistics44th Annual Meeting Association Computational Linguistics (COLINGACL), pp. 961968, Sydney, Australia.Galley, M., Hopkins, M., Knight, K., & Marcu, D. (2004). Whats translation rule?.Susan Dumais, D. M., & Roukos, S. (Eds.), Proceedings 2004 Human LanguageTechnology Conference North American Chapter Association Computational Linguistics (HLT:NAACL), pp. 273280, Boston, Massachusetts, USA.778fiUnsupervised Sub-tree Alignment Tree-to-Tree TranslationGildea, D. (2003). Loosely tree-based alignment machine translation. Proceedings41st Annual Meeting Association Computational Linguistics (ACL), pp.8087, Sapporo, Japan.Groves, D., Hearne, M., & Way, A. (2004). Robust sub-sentential alignment phrasestructure trees. Proceedings 20th International Conference ComputationalLinguistics (COLING), pp. 10721078, Geneva, Switzerland.Haghighi, A., Blitzer, J., DeNero, J., & Klein, D. (2009). Better word alignmentssupervised itg models. Proceedings Joint Conference 47th AnnualMeeting ACL 4th International Joint Conference Natural LanguageProcessing AFNLP (ACL-IJCNLP), pp. 923931, Suntec, Singapore.Huang, L., & Chiang, D. (2005). Better k-best parsing. Proceedings Ninth International Workshop Parsing Technology (IWPT), pp. 5364, Vancouver, BritishColumbia, Canada.Huang, L., Kevin, K., & Joshi, A. (2006). Statistical syntax-directed translationextended domain locality. Proceedings 7th Conference AssociationMachine Translation Americas (AMTA), pp. 6673, Cambridge, Massachusetts,USA.Imamura, K. (2001). Hierarchical phrase alignment harmonized parsing. Proceedings6th NLP Pacific Rim Symposium, pp. 377384.Jing, H. (2000). Sentence reduction automatic text summarization. Proceedings6th Applied Natural Language Processing Conference, pp. 310315.Johnson, M. (2007). doesnt EM find good HMM POS-taggers?. Proceedings2007 Joint Conference Empirical Methods Natural Language ProcessingComputational Natural Language Learning (EMNLP-CoNLL), pp. 296305, Prague,Czech Republic.Knuth, D. (1997). Art Computer Programming: Fundamental Algorithms. AddisonWesley.Koehn, P. (2004). Statistical significance tests machine translation evaluation. Lin,D., & Wu, D. (Eds.), Proceedings 2004 Conference Empirical MethodsNatural Language Processing (EMNLP), pp. 388395, Barcelona, Spain.Koehn, P., Och, F., & Marcu, D. (2003). Statistical phrase-based translation. Proceedings 2003 Human Language Technology Conference North AmericanChapter Association Computational Linguistics (HLT:NAACL), pp. 4854,Edmonton, Canada.Levenberg, A., Dyer, C., & Blunsom, P. (2012). bayesian model learning scfgsdiscontiguous rules. Proceedings 2012 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning(EMNLP-CoNLL), pp. 223232, Jeju Island, Korea.Liu, D., & Gildea, D. (2009). Bayesian learning phrasal tree-to-string templates. Proceedings 2009 Conference Empirical Methods Natural Language Processing(EMNLP), pp. 13081317, Singapore.779fiXiao & ZhuLiu, Y., Liu, Q., & Lin, S. (2006). Tree-to-string alignment template statistical machinetranslation. Proceedings 21st International Conference ComputationalLinguistics 44th Annual Meeting Association Computational Linguistics (COLING-ACL), pp. 609616, Sydney, Australia.Liu, Y., Lu, Y., & Liu, Q. (2009a). Improving tree-to-tree translation packed forests.Proceedings Joint Conference 47th Annual Meeting ACL4th International Joint Conference Natural Language Processing AFNLP(ACL-IJCNLP), pp. 558566, Suntec, Singapore.Liu, Y., Xia, T., Xiao, X., & Liu, Q. (2009b). Weighted alignment matrices statisticalmachine translation. Proceedings 2009 Conference Empirical MethodsNatural Language Processing (EMNLP), pp. 10171026, Singapore.Manning, C. D., & Schutze, H. (1999). Foundations Statistical Natural Language Processing. MIT Press.Marcu, D., Wang, W., Echihabi, A., & Knight, K. (2006). Spmt: Statistical machine translation syntactified target language phrases. Proceedings 2006 ConferenceEmpirical Methods Natural Language Processing (EMNLP), pp. 4452, Sydney,Australia.Marcu, D., & Wong, D. (2002). phrase-based,joint probability model statistical machine translation. Proceedings 2002 Conference Empirical MethodsNatural Language Processing (EMNLP), pp. 133139.Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building large annotatedcorpus english: penn treebank. Computational Linguistics, 19, 313330.May, J., & Knight, K. (2007). Syntactic re-alignment models machine translation.Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning (EMNLP-CoNLL),pp. 360368, Prague, Czech Republic.Mi, H., & Huang, L. (2008). Forest-based translation rule extraction. Proceedings2008 Conference Empirical Methods Natural Language Processing (EMNLP),pp. 206214, Honolulu, Hawaii, USA.Nakazawa, T., & Kurohashi, S. (2011). Bayesian subtree alignment model based dependency trees. Proceedings 5th International Joint Conference Natural LanguageProcessing (IJCNLP), pp. 794802, Chiang Mai, Thailand.Neal, R. (1998). Philosophy Bayesian Inference. http://www.cs.toronto.edu/radford/res-bayes-ex.html.Och, F. (2003). Minimum error rate training statistical machine translation. Proceedings 41st Annual Meeting Association Computational Linguistics(ACL), pp. 160167, Sapporo, Japan.Och, F., & Ney, H. (2004). alignment template approach statistical machine translation. Computational Linguistics, 30, 417449.Papineni, K., Roukos, S., Ward, T., & Zhu, W. (2002). Bleu: method automaticevaluation machine translation. Proceedings 40th Annual Meeting780fiUnsupervised Sub-tree Alignment Tree-to-Tree TranslationAssociation Computational Linguistics (ACL), pp. 311318, Philadelphia, Pennsylvania, USA.Pauls, A., Klein, D., Chiang, D., & Knight, K. (2010). Unsupervised syntactic alignmentinversion transduction grammars. Proceedings Human Language Technologies: 2010 Annual Conference North American Chapter AssociationComputational Linguistics (HLT:NAACL), pp. 118126, Los Angeles, California,USA.Riesa, J., Irvine, A., & Marcu, D. (2011). Feature-rich language-independent syntax-basedalignment statistical machine translation. Proceedings 2011 ConferenceEmpirical Methods Natural Language Processing (EMNLP), pp. 497507, Edinburgh, Scotland, UK.Riley, D., & Gildea, D. (2010). Improving performance giza++ using variationalbayes. Tech. rep., University Rochester.Smith, D. A., & Eisner, J. (2009). Parser adaptation projection quasi-synchronousgrammar features. Proceedings 2009 Conference Empirical MethodsNatural Language Processing (EMNLP), pp. 822831, Singapore.Snover, M., Dorr, B., Schwartz, R., Makhoul, J., Micciula, L., & Weischedel, R. (2005).Study Translation Error Rate Targeted Human Annotation. Tech. rep.LAMP-TR-126,CS-TR-4755,UMIACS-TR-2005-58, University Maryland, CollegePark BBN Technologies.Sun, J., Zhang, M., & Tan, C. L. (2010a). Discriminative induction sub-tree alignmentusing limited labeled data. Proceedings 23rd International ConferenceComputational Linguistics (COLING), pp. 10471055, Beijing, China.Sun, J., Zhang, M., & Tan, C. L. (2010b). Exploring syntactic structural features sub-treealignment using bilingual tree kernels. Proceedings 48th Annual MeetingAssociation Computational Linguistics (ACL), pp. 306315, Uppsala, Sweden.Thayer, I., Ettelaie, E., Knight, K., Marcu, D., Munteanu, D., Och, F., & Tipu, Q. (2004).isi/usc mt system. Proceedings International Workshop Spoken LanguageTranslation 2004, pp. 5960.Tinsley, J., Zhechev, V., Hearne, M., & Way, A. (2007). Robust language pair-independentsub-tree alignment. Proceedings Machine Translation Summit XI, pp. 467474,Copenhagen, Denmark.Venugopal, A., Zollmann, A., Smith, N. A., & Stephan, V. (2008). Wider pipelines: n-bestalignments parses mt training. Proceedings Eighth ConferenceAssociation Machine Translation Americas (AMTA), pp. 192201.Vogel, S., Ney, H., & Tillmann, C. (1996). Hmm-based word alignment statistical translation. Proceedings 16rd International Conference Computational Linguistics (COLING), pp. 836841.Wang, M., Smith, N. A., & Mitamura, T. (2007a). Jeopardy model? quasisynchronous grammar QA. Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural LanguageLearning (EMNLP-CoNLL), pp. 2232, Prague, Czech Republic.781fiXiao & ZhuWang, W., Knight, K., & Marcu, D. (2007b). Binarizing syntax trees improve syntaxbased machine translation accuracy. Proceedings 2007 Joint ConferenceEmpirical Methods Natural Language Processing Computational NaturalLanguage Learning (EMNLP-CoNLL), pp. 746754, Prague, Czech Republic.Woodsend, K., & Lapata, M. (2011). Learning simplify sentences quasi-synchronousgrammar integer programming. Proceedings 2011 Conference Empirical Methods Natural Language Processing (EMNLP), pp. 409420, Edinburgh,Scotland, UK.Xiao, T., Zhu, J., Zhang, H., & Li, Q. (2012). Niutrans: open source toolkit phrasebased syntax-based machine translation. Proceedings 50th Annual Meeting Association Computational Linguistics System Demonstrations (ACL),pp. 1924, Jeju Island, Korea.Xue, N., Xia, F., Chiou, F.-d., & Palmer, M. (2005). penn chinese treebank: Phrasestructure annotation large corpus. Natural Language Engineering, 11, 207238.Zhang, H., Zhang, M., Li, H., Aw, A., & Tan, C. L. (2009). Forest-based tree sequencestring translation model. Proceedings Joint Conference 47th AnnualMeeting ACL 4th International Joint Conference Natural LanguageProcessing AFNLP (ACL-IJCNLP), pp. 172180, Suntec, Singapore.Zhang, M., Jiang, H., Aw, A., Li, H., Tan, C. L., & Li, S. (2008). tree sequence alignmentbased tree-to-tree translation model. Proceedings 46th Annual MeetingAssociation Computational Linguistics: Human Language Techonologies(ACL:HLT), pp. 559567, Columbus, Ohio, USA.782fiJournal Artificial Intelligence Research 48 (2013) 175-230Submitted 05/13; published 10/13Online Mechanism Multi-Unit DemandApplication Plug-in Hybrid Electric Vehicle ChargingValentin RobuEnrico H. GerdingSebastian Steinvr2@ecs.soton.ac.ukeg@ecs.soton.ac.ukss2@ecs.soton.ac.ukUniversity Southampton, SO17 1BJ, Southampton, UKDavid C. Parkesparkes@eecs.harvard.eduHarvard University, Cambridge, 02138, USAAlex Rogersacr@ecs.soton.ac.ukUniversity Southampton, SO17 1BJ, Southampton, UKNicholas R. Jenningsnrj@ecs.soton.ac.ukUniversity Southampton, SO17 1BJ, Southampton, UKKing Abdulaziz University, Jeddah, Saudi ArabiaAbstractdevelop online mechanism allocation expiring resource dynamic agent population. agent non-increasing marginal valuation functionresource, upper limit number units allocatedperiod. propose two versions truthful allocation mechanism. modifiesdecisions greedy online assignment algorithm sometimes cancelling allocationresources. One version makes modification immediately upon allocation decisionsecond waits point agent departs market. Adoptingprior-free framework, show second approach better worst-case allocativeefficiency scalable. hand, first approach (with immediatecancellation) may easier practice need reclaim units previously allocated. consider application recharging plug-in hybrid electric vehicles(PHEVs). Using data real-world trial PHEVs UK, demonstrate highersystem performance fixed price system, performance comparable standard,non-truthful scheduling heuristic, ability support 50% vehiclesfuel cost simple randomized policy.1. IntroductionDesigning mechanisms allocating scarce resources self-interested agents centralresearch topic artificial intelligence (Sandholm, 2002; Engel & Wellman, 2010).aim work devise mechanisms satisfy certain desirable properties,truthfulness efficiency. Many settings mechanisms appliedcharacterised dynamic supply demand, i.e., agents arrive leave markettime, availability supply also changes time. led field onlinemechanism design, agents incentivised report truthfully valuec2013AI Access Foundation. rights reserved.fiRobu, Gerding, Stein, Parkes, Rogers & Jenningsgiven allocation, also period available market (Parkes, 2007).However, date, existing work field assumes valuationsagents certain allocation described single parameter, so-calledsingle-valued domains. Existing approaches consider multi-valued domains relyaccess probabilistic model supply demand ability compute optimalallocation policy, becomes computationally infeasible realistic settings.address shortcomings, extend state art developing novelmodel-free mechanism (i.e., assumes knowledge future demand supply)multi-valued demand. particular, consider domains multi-unit demandagents non-increasing marginal values. domains, first units allocatedagent higher (or equal) marginal value agent compared subsequentunits. online settings consider, resources continuously produced perishable,thus available supply must allocated period. Moreover, supply availableperiod known advance, start period.Examples settings non-increasing marginal values perishable resources occur many real-life settings. One example cloud computing, jobs arrivetime perishable computational resources must allocated jobs (Porter,2004; Stein, Gerding, Rogers, Larson, & Jennings, 2011). particular, non-increasingmarginal value model applies naturally large-scale data processing optimisationany-time computation. setting, first unit computation provides solutioncertain quality, subsequent units allow improving solution, levelcomputation longer useful. Hence, first units valuable,already provide good approximation desired solution, subsequent unitsincrease value, marginally non-increasing amount. Another example onlineadvertising, impressions need allocated soon users visit webpage (Constantin, Feldman, Muthukrishnan, & Pal, 2009). non-increasing marginal values alsoapplies setting, since additional exposure set users ad likelydecreasing impact.third example, studied extensively paper, allocation electricitycharging plug-in hybrid electric vehicles (PHEVs). Similar pure (non-hybrid) electricvehicles (pure EVs), vehicles charged directly electric charging point.difference PHEVs electric motor internal combustion engine,widely seen solution problem range anxiety, i.e., fear car runelectricity middle nowhere (Eberle & von Helmolt, 2010).1 However,associated increase demand electricity, significant concerns withinelectricity distribution industries regarding widespread use vehicles, sincehigh charging rates PHEVs require (up three times maximum current demandtypical home) could overload local electricity distribution networks peak times (Fairley,2010). One approach address concern (e.g., adopted Pacific Gas ElectricCompany California) introduce time-of-use pricing plans seek shift demand.sophisticated approach takes account valuations self-interestedowners, design online mechanism schedules access dynamically orderprevent network overload. assumption decreasing marginal values justified1. Practical examples PHEVs include versions cars Toyota Prius Honda Insight,drive petrol, whose batteries also charged directly electrical charging point.176fiAn Online Mechanism Multi-Unit Demandvehicle owner likely use first units electricity, alwaysuse combustion engine alternative case runs electricity (andcar still used even fully charged). background, maincontributions work are:develop new model-free online mechanism settings participating agentsnon-increasing marginal values units perishable good. adopt greedyalgorithm coupled method modify allocation ensure incentive compatibility. involves cancelling part proposed allocation, exploretwo ways performing cancellation: immediately, i.e., time step resource actually allocated, departure agent market(at point must take back allocated items). variants (weakly)dominant-strategy incentive compatible (DSIC), participants incentivemisreport valuations arrival-departure dynamics.analyse worst-case performance achieved mechanisms relativeoptimal offline allocation, considering number units need remainunallocated order achieve incentive compatibility total valueallocation (i.e., allocative efficiency). variation on-departure cancellationresults higher allocative efficiency, tractable, may involve additionalpractical challenges. example, PHEV charging domain, occasionallyrequires vehicles battery partially discharged prior departure.evaluate online mechanism numerical simulations abstract domain PHEV charging domain, compare results several benchmarksassume non-strategic agents, including optimal offline solution, schedulingheuristic greedy algorithm without cancellation. simulations PHEVdomain based real data large-scale trial (pure) EVs UK. Valuations derived real monetary savings, considering factors fuel prices,distance owner expects travel, energy efficiency vehicle. results establish mechanism outperforms fixed-price mechanismterms allocative efficiency domains, performing slightly worsenon-incentive compatible scheduling solutions. addition, mechanismon-departure cancellation scales easily hundreds agents.focus allocative efficiency rather revenue, appropriate manydomains interest. example, PHEV charging domain, reasonablegoal allocate capacity efficiently order maximise value user basepower company, given significant constraints charging capacity. Moreover, manycloud computing applications (for example, large-scale scientific computing), goalallocate capacity jobs urgent important. However, practiceseller wants guarantee minimum revenue unit sold, would easyinclude reserve price. minimum price set units timepoints, would affect properties mechanism.remainder paper organised follows. first discuss related work(Section 2), formally introducing model (Section 3). Section 4, define177fiRobu, Gerding, Stein, Parkes, Rogers & Jenningsonline mechanisms study strategic properties. Then, Section 5, developworst-case analytical results, followed Section 6 discussion computeallocations payments practice. Using real synthetic input data, presentresults experimental evaluation mechanisms Section 7, concludeSection 8.2. Related Worksection, first review existing work online mechanism design (Section 2.1),provide background PHEV charging application, along overviewprevious work considers problem (Section 2.2).2.1 Online Mechanism DesignOne line work online mechanism design aims develop online variants VickreyClarke-Groves (VCG) mechanism. context, Parkes Singh (2003) considerproblem maximising long-term allocative efficiency system self-interested agentsarrive depart dynamically. model online mechanism design problemMarkov decision process (MDP), whose solutions used implement optimal policiestruth-revealing Bayes-Nash equilibrium. related work, Gershkov Moldovanu(2010) examine allocation set goods dynamic population randomly arrivingbuyers. consider two settings: one common deadline allocatingobjects buyers, second one without firm deadline, buyersimpatient, assigning higher value items allocated sooner.Unlike Parkes Singh (2003), Gershkov Moldovanu (2010), mechanism proposed paper model-free (which advantage prior knowledge distribution required agents types future allocations),focus stronger concept dominant-strategy incentive compatibility (where reporting truthfully best response regardless agents doing, evenirrational). approach requires fewer assumptions, makes computing allocationstractable compared VCG-like approaches. VCG generally requiresallocations optimal expectation (perhaps constrained space policies),whereas, show, use greedy heuristics.Model-free settings considered Hajiaghayi et al. (2005), Parkes (2007) Porter(2004). work Porter examines scheduling jobs single machine proposesincentive compatible mechanism setting. However, work assumes settingresults job released agent completion agentsreported deadline. assumption reasonable scheduling computational jobsserver, suitable setting, since goods (i.e. electricity units) mustallocated instantly become available.work considers online setting similar one considerHajiaghayi et al. (2005). study problem online scheduling single, re-usableresource finite time period, agent arrival-departure windowactive market. Agents may misreport valuation, wellarrival departure, subject assumption limited misreports (i.e.,early arrival later departure misreports possible). setting, characterise178fiAn Online Mechanism Multi-Unit Demandtruthful allocation payment policies, prove worst-case approximation ratiosrespect optimal offline allocation. key limitation mechanism proposedHajiaghayi et al. concerns single-valued domains, whereas consider multi-unit settingdecreasing marginal values. show mechanism directly applymulti-unit case, requiring, cases, additional cancellation rules appliedensure truthfulness.Multi-unit demand considered work Lavi Nisan (2004), proposeonline auction model mechanism required make decisions bidreceived. provide characterisation incentive compatibility domainsterms supply curves, concept relates closely threshold mechanismcharacterisation. However, online auctions model, auctioneer must respondbid immediately, considering bids. response, mechanismdetermines quantity sold price paid. wouldapplicable setting presented paper, limit numberperishable units allocated time interval. Moreover, window-basedallocation allows prices determined dynamically, based bids observedagents departure. similar vein, Babaioff, Blumrosen, Roth (2010)consider online auction model future supply unknown, characterise severalsubclasses truthful mechanisms. domain different ours, biddersmodel specify multi-dimensional demands non-increasing marginal values.related work online mechanism design adapts consensus algorithmonline stochastic optimisation proposed Bent Van Hentenryck (2004) settingself-interested agents. context, Parkes Duong (2007), ConstantinParkes (2009) first propose idea modifying decision algorithm cancellingpart allocation order ensure incentive compatibility. Unlike present paper,setting assumes single-valued private information approaches applicableagents non-increasing marginal values. Also single-valued settingspure EV domain, Stein, Gerding, Robu, Jennings (2012) propose model-based onlinemechanism assumes knowledge future supply uses pre-commitment ensureonline allocations truthful.2.2 Electric Vehicle ChargingMulti-agent systems AI techniques increasingly used address challengesSmart Grid (e.g., Vytelingum, Voice, Ramchurn, Rogers, & Jennings, 2011; Robu, Kota,Chalkiadakis, Rogers, & Jennings, 2012), EV charging one importantapplication areas. Work automatic scheduling EV charging typically allows individual vehicle owners indicate times car available charging,enabling automatic scheduling satisfying constraints distribution network.vein, Clement, Haesen, Driesen (2009) propose centralised scheduler,makes optimal use network capacity vehicle owners report expected futurevehicle use system. Sundstrom Binding (2012) tackle problem chargingmultiple electric vehicles considering distribution grid constraints, formalise underlying optimisation problem propose novel method based load flow solve it.strategic behaviour remains possible approaches; e.g., owner may indicate179fiRobu, Gerding, Stein, Parkes, Rogers & Jenningsearlier departure time travel distances order receive preferential charging.result high cognitive load car owners, may lead inefficient schedulesbased actual user requirements, leading efficiency loss.potential speculation strategic agents identified crucial problemscheduling problems, scheduling computational jobs cluster (Porter,2004), scheduling computation-intensive services cloud (Stein et al., 2011)market-based scheduling loads transportation logistics (Robu, Noot, La Poutre, & vanSchijndel, 2011). increase number EVs requiring charging, potentialmanipulation become increasingly pressing problem PHEV scheduling well.approaches EV scheduling include lottery-based solution proposed Vasirani Ossowski (2011), decision whether charge vehicledetermined lottery system, designed ensure level fairness resultingallocation. Unlike work, however, authors use game-theoretic principlesprove participating vehicles incentive report preferences truthfully,thus scheme agents may incentive speculate. Moreover, experimentalanalysis reported Vasirani Ossowski adopt real data derive EV drivingpatterns, charging capacities network constraints.recent work investigates using grid-integrated electric vehicles (GIVs) sell powerstorage capacity back grid concept known vehicle-to-grid (V2G) (c.f.Kamboj, Kempton, & Decker, 2011). work different ours, studyproblem coordinated charging PHEVs local network capacity constraints.subsequent work model present paper (which first appeared Gerding,Robu, Stein, Parkes, Rogers, & Jennings, 2011, Robu, Stein, Gerding, Parkes, Rogers,& Jennings, 2011), study problem charging pure EVs, must receive setamount charge, otherwise derive value allocation (Stein et al., 2012).second paper studies problem two-sided markets, PHEVs chargingstations compete matched (Gerding, Stein, Robu, Zhao, & Jennings, 2013). Unlikepresent model, papers consider single-minded bidders, work Steinet al. assumes access probabilistic model environment.3. Modelconsider online mechanism design setting discrete time steps,period, multiple indivisible units perishable good sold, agent requiresmultiple units within certain period. show Section 3.1, model alsoused continuously available resources, electricity computational resources.case, allocation decision consists amount resource consumedagent next period following time point.convenience, overview notation provided Table 1. Formally, let S(t)denote supply available time t. Let I(t) = {1, 2, . . .} denote set agentsmarket time already left market. assume accessprobabilistic model future arrivals, departures future supply beyond currenttime period t. Agents numbered according arrival time. agent I(t)stype described tuple = hvi , ai , di , ri , vi marginal valuationvector, ai di , di ai , arrival departure times (the earliest latest180fiAn Online Mechanism Multi-Unit Demandtimes agent available market), ri maximum consumption rate (i.e.,maximum number units agent consume time t), setadmissible types. Upon arrival, agent needs report valuation functionmaximum consumption rate. two aspects agents reported type requiredremain unchanged agent present, although departure time modified(it becomes known mechanism actual departure).element vi,k valuation vi called marginal valuation, representsagents willingness pay k th unit good, given acquired k 1 units.require:Assumption 1. Marginal valuations non-increasing, i.e., i, k : vi,k vi,k+1 .Given this, agents utility functionPis U (k, x, ) = V (k, ) x, x agentspayment mechanism, V (k, ) = kj=1 vi,j total value derived given type,k number units allocated agent arrival departure.mechanism asks agents report types and, based information, decidesallocation supply payment units received. Since agents misreporttype, aim design mechanism incentivises agents make truthfulreports. denote reported type = hvi , ai , di , ri i. results use commonassumption online mechanism design literature (Hajiaghayi et al., 2005), agentscannot report earlier arrival later departure. addition, assume agentscannot misreport higher maximum consumption rate.Assumption 2. Limited Misreports: Agents cannot report earlier arrival, laterdeparture, higher maximum consumption rate, i.e., ai ai , di di , ri ri must hold.following, reports satisfy Assumption 1 Assumption 2 saidadmissible. Given assumption, aim develop mechanism dominantstrategy incentive compatible (DSIC), i.e., agents best reporting = , matteragents report.Formally, let = {i |i I(t)} denote types agents time t, ={j |j I(t), j 6= i} types agents except i, similarly, denotecorresponding reported types. Note that, brevity, remove dependencehtinotation. Furthermore, ki denotes endowment (or number units allocated far)hti htibeginning time t, including allocation time t, khti = hk1 , k2 , . . .i.hd+1iFurthermore, ki = kidenotes agent endowment upon reported departure.htimechanism defined allocation policy, (I |khti ), I(t), determinesnumber units allocated agent time given current endowment,payment policy, xi (i |ki ), I(t), calculates total payment allocatedunits. allocation made online, payment needs finalised uponht+1ihtihtireported departure agent. Note ki= ki + (I |khti ). also use (i ) =Pdihtihtit=ai (I |k ) denote total number units allocated agent i, given reportedtype.aim find mechanism satisfying following property:181fiRobu, Gerding, Stein, Parkes, Rogers & JenningsDefinition 1. (Dominant-Strategy Incentive Compatible (DSIC)) mechanism DSICreporting truthfully, i.e., = , weakly dominant strategy. Formally, agents i,admissible , :V (i (i ), ) xi (i |i (i )) V (i (i ), ) xi (i |i (i ))(1)3.1 Application Plug-In Hybrid Electric Vehicle Chargingapplying model PHEV charging domain, agents compete limited chargingcapacity behalf EV owners within neighbourhood. assume marketelectricity PHEV charging separate regular household consumption.Given this, available supply, S(t), charging PHEV residual supplyregular household consumption removed. supply also include electricityuncertain sources, shared renewable generator, e.g., shared neighbourhoodwind turbine solar panel installation.scenario, unit electricity defined amount kWh charginglowest rate interval (e.g., lowest rate 6.5 A, 230 Vhourly slots, unit 6.5 230 V 1 h = 1.495 kWh). Charging points typically allowcharging occur different rates, maximum consumption rate, ri ,refers maximum charging rate charging point given battery. Since unitsindivisible, means charging rate needs multiple lowest rate.example, agent allocated 2 units single time step, charging ratetwice lowest charging rate time interval. Given focus network capacity,supply assumed perishable capacity left unused time cannot allocated later.time arrival, ai , departure, di , refers interval vehicleavailable charging (i.e., home used). However, agentbelieves benefit delaying arrival, wait pluggingvehicle electricity network. Therefore, arrival report ai timeowner physically plugs vehicle electricity network, misreport consistsplugging arrival. Similarly, reported departure, di , simply represents timevehicle unplugged electricity network. Although arrival departuremodeled part reported type, practice need communicatedadvance mechanism, simply observed occur, i.e., ownerplugs unplugs vehicle.limited misreports assumption (Assumption 2) reasonable context, sinceagents cannot physically plug EV home. requirement ri rialso natural PHEV charging. electric batteries configured chargeslower rate, charging faster rate one allowed manufacturermight destroy them.time arrival, agent needs report marginal valuation vector vi .clear interpretation marginal valuation PHEVs always use petrolsubstitute electricity. marginal value additional unit chargeexpected money saved incur cost petrol.2 determining exactvalue, also need consider amount purchased electricity owner use2. Note implicitly assume petrol (i.e. gasoline) always available substitute,sufficient refueling points vehicle run petrol.182fiAn Online Mechanism Multi-Unit Demandv1 = h10, 4iv2 = h5iAgent 1Agent 2Agent 3v3 = h2iS(t1 ) = 1S(t2 ) = 1Figure 1: Example showing arrivals, departures, valuation vectors 3 agents.expectation. example, certain use first unit electricity, maximumwillingness pay would equal equivalent cost petrol. units become less likelyused, expected value decreases. value marginal unit expectedsavings compared using petrol alternative. Section 7 provide detailed analysis,confirming non-increasing marginal value (Assumption 1) hybrid EVs.4. Online Mechanismsection, first present simple greedy allocation policy, show cannotcoupled payment rule provide truthfulness. Continuing, combine twovariations idea modifying allocation generated greedy rule, providetheoretical analysis properties.4.1 Greedy Allocation PolicyhtiLet vector vi= hvi,khti +1 , . . . , vi,khti +r denote agent reported marginal valueshtinext ri units, given endowment ki time t. agents reported willingnesshtipay units available time and, follows, refer vector viactive (reported) marginal valuations time t. Furthermore, let V hti denote multisetvalues agents present market time t, i.e., I(t)ai di . Next, define set operator maxhki V return highest kelements multiset V (or, |V| < k, return V). Then, greedy allocation policy is:Definition 2 (Greedy Allocation Policy). time step t, allocate S(t) unitsevery agent receives one unit good active marginal valuationsincluded maxhS(t)i V hti .ignore issues tie breaking throughout paper simplify exposition.note, however, results presented hold implementing either random tiebreaking rule, first come, first served rule, breaks ties favour agentarrived market first.provide example greedy allocation, consider active marginal valuationsgiven Table 2. case, multiset active marginal valuations consists V hti =h7, 6i h10, 6, 6i h8i = h7, 6, 10, 6, 6, 8i (in particular order). Then, S(t) = 3,highest active marginal values allocate resources agents marginalvalues 10, 8, 7. Thus, example, agent receive 1 unit.order show greedy always DSIC, consider example illustratedFigure 1, involving 2 time steps 3 agents. Suppose supply S(t) = 1 time183fiRobu, Gerding, Stein, Parkes, Rogers & JenningsSupply, Agents Preferences (Section 3)Supply perishable units timeset agents arrived far timeType agentMarginal valuation vector, vi,k value k th unitArrival time, departure time, maximum consumption rateSet admissible types, i.e., subject Assumptions 1 2types agents I(t)S(t)I(t) = {1, 2, ...}= hvi , ai , di , rivi = hvi,1 , vi,2 , ...i, , ri= {i |i I(t)}= {j |j I(t),j 6= i}, ,PV (k, ) = kj=1 vi,jU (k, x, ) =V (k, ) xhtikihti htikhti = hk1 , k2 , ...ihd+1iki = kitypes agents j I(t) exceptReported typesTotal value given k units allocated ai diAgent utility, x payment mechanismGeneral Mechanisms (Section 3)Agent endowment beginning timeendowment agents I(t) beginning timeAgent endowment reported departureAllocation policy, i.e., number units allocated timehti(I |khti )given agents reports current endowments khtiPayment policy, i.e., agent payment given reported typesxi (i |ki )agents, agent endowment departure.Greedy Policy DSIC Mechanism (Sections 4 4.2)htivi =Agent reported active marginal valueshvi,khti +1 , ..., vi,khti +rVi0Multiset reported active marginal values (where union operator used multiset operator throughout paper)Active reported marginal values agent would neverpresent markethtiZeros added multiset ensure |Vi0 | S(t)max V, min VReturns highest respectively lowest k elements multiset VV hti =htiiI(t) vihtiVihtihkihkiExternality imposed agent others, i.e., marginal valuations missing due agent allocated= min maxhrihS(t)imin(ri , S(t)) units timehtiVector marginal payments time t, pi,k price[htihtpi = incrEi agent charged k th unit, operator incr orders=aielements multiset increasing orderhtiEihtiVi0hdipi = piMarginal payment vector reported departure agentTable 1: Main notation references sections first introduced.184fiAn Online Mechanism Multi-Unit Demandagent (i)123ri231vih8, 7, 6, 5ih10, 6, 6, 4, 4ih9, 8, 8, 7ihtiki102htivih7, 6ih10, 6, 6ih8ihtiTable 2: Example three agents active marginal valuations vi , given marginalhtivaluations vi , endowments ki , maximum consumption rates ri .step ri = 1 agents i. Assuming truthful agents, greedy would allocateunits agent 1, agent 1 highest active marginal value time stepsh1ih1ih2ih2i(v1 = h10i > v2 = h5i, v1 = h4i > v3 = h2i).Now, consider question finding payment policy makes greedy allocationpolicy DSIC. much agent 1 pay? answer this, note paymentunit allocated time = 1 least 5. Otherwise, agent 1 presentmarket time = 1 valuation v1,1 (5 , 5), would incentivemisreport v1,1 > 5 still win. Similarly, payment unit allocated time= 2 least 2. Thus, minimum payment agent 1 allocated two unitsx1 (|1 = 2) = 7.hand, much agent 1 pay allocated one unitinstead? argue 2. Suppose, contradiction, payment setlarger value x1 (|1 = 1) = 2 + (where > 0). agents first marginalvalue v1,1 instead 2 < v1,1 < 2 + (with remaining marginal values zero),would win period 2, would pay 2 + hence negative utility. However,x1 (|1 = 2) 7 x1 (|1 = 1) 2, agent 1 wants one unit, two,allocated greedy mechanism (its utility one unit greater two,10 2 > 10 + 4 7). Hence, greedy allocation policy cannot made DSIC settingpayments.example shows problem particular payment policy,intrinsic greedy allocation policy. particular, problem allocationpolicy satisfy necessary property W-MON (Bikhchandani, Chatterji, Lavi,Mualem, Nisan, & Sen, 2006):Definition 3 (Weak Monotonicity (W-MON)). allocation policy , W-MON if,every I(t), = hvi , ai , di , ri i, = hvi , ai , di , ri , N 1 , settypes subject non-increasing marginal valuations, following equation holds:V (i (i ), ) V (i (i ), ) V (i (i ), ) V (i (i ), )(2)words, changing agent type (while keeping types agents fixed)type another type changes allocation (i ) (i ),resulting difference utilities new original outcomes evaluated newtype agent (denoted function V (, )) must less difference utilities185fiRobu, Gerding, Stein, Parkes, Rogers & Jenningsevaluated original type agent (denoted function V (, )). Using notiondemonstrate greedy allocation policy DSIC setting.Theorem 1. greedy allocation policy DSIC multi-valued domains nonincreasing marginal valuations.Proof. Bikhchandani et al. (2006, Lemma 1) know necessary conditionDSIC allocation policy satisfy W-MON (see Definition 3). Equation 2true, shown following must hold:(i )(i )> (i )X(i )vi,kk=i (i )+1Xvi,k(3)k=i (i )+1words, units allocated one type compared another type,type also higher marginal values units. Consider exampleFigure 1, look W-MON condition agent 1 varying type. keeparrival departure fixed, i.e., ai = ai , di = di . Suppose v1 = h10, 4iexample, v1 = h4 + , 4 + i, 0 < < 1. Note 1 (1 ) = 2, (1 ) = 1(agent 1 allocated unit first time step type changed 11 ). Since 1 allocated additional unit (compared 1 ), W-MON requires 1< v , therebyvalues second unit higher equal 1 . However, see v1,21,2violating Equation 3. example demonstrates greedy allocation policyW-MON, therefore DSIC.4.2 Achieving Truthfulness CancellationAddressing problem W-MON, consider two types modifications allocation decision greedy policy, designed achieve monotonicity. firstimmediate cancellation, units simply left unallocated, i.e., none agentsreceive unit, even demand. second on-departure cancellation,units initially allocated using greedy approach, departure agent,overallocated units removed.3model on-departure cancellation efficient generally requiresfewer cancellations, also computationally efficient calculating paymentsallocations. However, depending domain, may always possibleremove units allocated.following, detail allocation policies explain payments computed. give example shows difference two mechanismsand, lastly, provide analysis economic properties mechanisms.defining allocation policies, show compute agents marginalpayment vector, determines, additional unit, price agent would needpay unit. marginal payments used determinecancel allocation, well agents total payment given allocation.3. PHEV setting, corresponds first charging battery later discharging overallocatedunits.186fiAn Online Mechanism Multi-Unit Demandnecessary condition truthfulness payments cannot depend agentsreport except effect allocation; e.g., see work Nisan, Roughgarden,htiTardos, Vazirani (2007, Proposition 9.27). end, let Vi denote multisetactive marginal valuations agents market time t, agent removedmarket rerun ai onwards.hticannot simply derive Vi V hti since removing agent could affect endowments agents earlier time steps. example, setting Table 2,agents 1 3 endowments time therefore removing either agentslikely increase endowments agents, thereby changing dynamicshtientire market. ensure Vi truly independent agent i, market needsre-run point agent first entered market, processneeds repeated agent I(t) ai di , whose paymentscomputing.hticase |Vi | < S(t), furthermore add number zero-valued bids referhtihtienlarged set Vi0 , ensure |Vi0 | S(t). Next, similar operator maxhki ,define set operator minhki V return lowest k elements multiset V (or,|V| < k, return V). define externality agent would impose agentsmin(ri , S(t)) S(t) units time as:htihtiEi = min max Vi0hrihS(t)ihtihtiNote cardinality Ei equal |Ei | = min(ri , S(t)). Intuitively, multisethtiEi contains marginal values agents would lose agenthtiwin ri units time t. example, let V1 = h1, 4, 5, 7, 9, 10i (sorted convenience),htiS(t) = 4, r1 = 2. Then, E1 = h5, 7i.agent active single time step, externality would specifypayment unit. is, using example, agent 1 allocated singleunit mechanism, payment would 5. hand, allocated 2units, payment would 5 + 7 = 12. intuition regularVickrey-Clarke-Groves (VCG) mechanism, total payment correspondssum externalities.compute overall payments online, need combine externalities acrosstime steps agents active period current time t. this, definehtiordered vector marginal payments, pi , follows:hthtiE,pi = incr=aiincr operator orders elements multiset increasing order,use union symbol denote union multisets (and element appearmultiple times).htiNow, pi,k price agent charged k th unit good. Intuitively,minimum valuation agent could report winning unit time t.prices adjusted time step. particular, since vector increasing order,187fiRobu, Gerding, Stein, Parkes, Rogers & Jenningshtielements added time increases, pi,k either stays decreasesgiven k, never increase. following, use pi,k denote agent marginalpayment k th unit time di .Given this, decision payment policies defined follows:Allocation Policy decision allocate consists two stages:Stage 1 time step t, pre-allocate using greedy allocation policy (seeDefinition 2).Stage 2 consider two variations decide cancel pre-allocation:Immediate Cancellation (IM). Leave unit unallocated whenever marginal payment time unit greater marginal value, i.e.,whenever:htihtihtihtivi,k < pi,k ki < k ki +On-Departure Cancellation (OD). departing agent, cancel allocation unit k ki vi,k < pi,k .Payment Policy Payment always occurs reported departure. Given ki unitsallocated agent i, payment collected is:xi (i |ki ) =Xkik=1pi,k(4)following, refer two mechanisms immediate on-departurecancellation IM OD respectively, corresponding allocation policies imod . distinction two mechanisms made, used.payment policy mirrors allocation policy. example, immediate cancellationhtiused, agent times t, values pi vector computedre-running market, absence agent using immediate cancellation, basedreports agents. Conversely, on-departure cancellation used,policy used computing pi prices.4.3 Examplesdemonstrate two mechanisms work, present two examples. aimfirst example show difference immediate on-departure cancellation.second example illustrates effect changing maximum consumption rates.example shows specific instances, increasing agents maximum consumption rateactually increase number cancellations.4.3.1 Example 1first example extends setting shown Figure 1 include third time step, = 3.agents 1 3 remain market = 3 (i.e., d1 = d3 = 3) new agentsarrive. Furthermore, S(t) = 1 {1, 2, 3}, three unitsallocated total. before, suppose ri = 1 agents. Table 3 shows188fiAn Online Mechanism Multi-Unit Demandt=1t=2t=3IMt=3ODagent 1:a1 = 1, d1 = 3,v1 = h10, 4i, r1 = 1h1ih1ik1 = 0, v1 = h10ih1ih1iV1 = E1 = h5ih1ip1 = h5iimh1iodh1i1= 1=1h2ih1ik1 = 1, v1 = h4ih2ih2iV1 = E1 = h2ih2ip1 = h2, 5iimh2i1=0odh2i1=1h3ih1ik1 = 1, v1 = h4ih3ih3iV10 = E1 = h0ih3ip1 = h0, 2, 5iimh3i1=1h3iagent 2:a2 = 1, d2 = 1,v2 = h5i, r2 = 1h1ih1ik2 = 0, v2 = h5ih1ih1iV2 = E2 = h10ih1ip2 = h10iimh1iodh1i2= 2=0agent 3:a3 = 2, d3 = 3,v3 = h2i, r3 = 1h2ih1ih3ih1ih3ih1ik3 = 0, v3 = h2ih2ih2iV3 = E3 = h4ih2ip3 = h4iimh2iodh2i3= 3=0k3 = 0, v3 = h2ih3ih3iV3 = E3 = h4ih3ip3 = h4, 4iimh3i3=0h1ik1 = 2, v3 = hih3ih3iV10 = E1 = h0ih3ip1 = h0, 2, 5iodh3i1=0k3 = 0, v3 = h2ih3ih3iV30 = E3 = h0ih3ip3 = h0, 4iodh3i3=1Table 3: Example run mechanism 3 agents 3 time steps IMOD mechanisms. Grey cells indicate different values IM OD policies.htihtihtiendowments ki , active marginal valuations Vi , externalities, Ei , marginalhtihtipayments pi , allocation decisions different time steps.start considering allocations payments using immediate cancellation.time = 1, Stage 1 mechanism pre-allocates unit agent 1, sinceh1iv1,1 = 10 p1,1 = 5, pre-allocation cancelled second stage. time= 2, unit gets pre-allocated agent 1 since active marginal value greaterh2ih2ih2iagent 3, i.e., v1 = h4i > v3 = h2i. However, V1 = h2i insertedh2ih2ibeginning p1 vector, result v1,2 = 4 < p1,2 = 5 (at prices, agent1 prefers allocated one unit instead two). Consequently, pre-allocation getscancelled unit goes neither agents.h3itime = 3, active marginal value agent 1 still v1 = h4i, since endowmentunchanged, since agent 1 still highest active marginal value, preallocated unit. calculate marginal payment agent 1, recall allocationpolicy needs recomputed agent 1 entirely removed market. caseagent 3 would allocated unit time = 2, thus time = 3 activeh3imarginal value agent 0. Thus, value 0 inserted p1 vector.h3i= 3, however, v1,2 = 4 p1,2 = 2, therefore pre-allocation cancelled.interesting exercise see happens marginal payment vector agent3 = 3. calculate this, remove agent 3 rerun market = 2.189fiRobu, Gerding, Stein, Parkes, Rogers & Jenningsagent1agent2agent3v1 = h10, 8, 3iv2 = h7iv3 = h1iS(t1 ) = 2S(t2 ) = 1Figure 2: Example showing arrivals, departures, valuation vectors 3 agents.h2icase, time = 2, marginal payment vector agent 1 becomes p1 = h0, 5i. Sinceh2imarginal payment second unit, p1,2 , remains unchanged, pre-allocationstill cancelled! Therefore, even agent 3 market, second unit remainsh3iunallocated, agent 1s active marginal value = 3 v1 = h4i,h3ih3iV3 = h4i. Given this, agent 3s marginal payments become p3 = h4, 4i. Notemarginal payment first unit (4) higher marginal value unit (2),consistent allocation. Otherwise, marginal paymentlower, agent 3 would incentive overreport win unit = 3.So, case immediate cancellation, two three units allocated agent 1,h3ih3iagent pays x1 = p1,1 + p1,2 = 0 + 2 = 2. third unit allocatedagent. Note unit cannot go agent 3, payment wouldh3ip3,1 = 4, resulting negative utility agent 3.consider setting on-departure cancellation. first two timesteps before, except cancellation = 2 (since donedeparture needed). changes endowment state agent 1 = 3, thereforeh3iactive marginal value agent 1 = 3 equal v1 = hi,agent 3 removed market. Therefore, unit pre-allocated agent 3,h3ih3ipayment unit p3,1 = p3,1 = 0. vector p1 remains unchanged comparedimmediate case. point, longer need cancel one pre-allocationsagent 1, since received k = 2 units, allocation immediatecancellation policy, note v1,2 > p1,2 .pre-allocations cancelled on-departure policy, policyefficient. show remainder paper, on-departure policy nevercancels more, typically cancels fewer pre-allocations compared immediate one.Still, possible construct examples where, worst case, half pre-allocationsneed cancelled, even on-departure policy.Furthermore, note units given away free (i.e., paymentunits zero). standard problem auctions insufficient competition,trivially resolved e.g. introducing minimum price reserve priceunit good. However, reduce efficiency since units remain unallocatedfall reserve price. consider reserve prices paper,economic properties mechanism continue hold reserve prices, longreserve prices time points (otherwise could incentivemisreport arrival time).190fiAn Online Mechanism Multi-Unit Demand4.3.2 Example 2next example, depicted Figure 2, shows setting two time steps threeagents, different preferences supply first step two units,change maximum consumption rate agent 1. consider two cases:maximum consumption rate agent 1 r1 = 1.4 case, one marginalvalue taken agent per time step. time t1 , marginal valuations v1,1 = 10agent 1, v2,1 = 7 agent 2 pre-allocated, time t2 , marginal value v1,2 = 8agent 1 pre-allocated. prices charged agent 1 are: p1 = h0, 1i, withoutagent 1 market, would free, spare unit time t1 available unitt2 would sell agent 3 1. pre-allocation gets cancelled case, actualallocation equivalent optimal offline allocation.maximum consumption rate agent 1 r1 = 2. Then, time t1 , greedypolicy described allocates two marginal values agent 1: v1,2 = 10 v1,2 = 8,higher v2,1 = 7, agent 2 drops market. time t2 ,unit pre-allocated agent 1 (due marginal value 3 higher 1).However, marginal payments vector required agent 1 p1 = h0, 1, 7i,marginal valuations v1 = h10, 8, 3i. Given prices, agent 1 prefers two unitsthree (because 10 + 8 1 > 10 + 8 + 3 1 7), third cancelled. overallefficiency lower, pre-allocation third available unit cancelled, whereasr1 = 1 allocated agent 2. Note, however, although efficiency muchlower, agent 1 incentive declare true maximum consumption rate r1 = 2 as,case, payment change.4.4 Establishing Truthfulnesssection prove mechanisms DSIC assumptionsnon-increasing marginal valuations (Assumption 1) limited misreports (Assumption 2).following, first establish DSIC respect valuations only, provetruthful reporting arrival departure times separately. detail, proceedfollowing 3 stages:(i) define concept threshold policy, show that, coupledappropriate payment policy, given admissible pair hai , di i, allocation policythreshold policy, mechanism DSIC respect valuations (Lemma 1).(ii) show allocation policy threshold policy (Lemma 2).(iii) show agents truthfully report valuations, reporting ai = ai , di = di ,ri = ri weakly dominant strategy (Lemma 3).results combined Theorem 2 show mechanism DSIC.Definition 4 (Threshold Policy). allocation policy threshold policy if, givenagent fixed hai , di , ri , exists marginally non-decreasing thresholdvector , independent report vi made agent i, following holds: k, vi :(i , ) k vi,k k .4. Note two agents desire one unit, maximum consumption rate irrelevantexample.191fiRobu, Gerding, Stein, Parkes, Rogers & Jenningswords, threshold policy potentially different threshold k k,agent receive least k units reported valuation k thitem least k .threshold policy satisfies W-MON, sufficient DSIC domain sincebounded agent valuations domain completely ordered, meaningpayoff types agree weak preference ordering allocations (i.e.,always weakly better less), indifference way goods allocatedagents (Bikhchandani et al., 2006). show allocation policy threshold property, thus satisfies W-MON, also handles misreports arrivals,departures maximum charging rates.Importantly, vector non-decreasing, i.e., k+1 k ,independent reported valuation vector vi . properties satisfiedpi vector, use show mechanism threshold policy.First, however, show threshold policy appropriate payments DSICrespect valuations:Lemma 1. Fixing admissible haj , dj , rj j fixing , thresholdpolicy coupled payment policy:xi (i , ) =Pi (i ,i )k=1k ,vi marginally non-increasing, reporting vi truthfully weakly dominant strategy.Proof. Agent utility rewritten as:Pi (i ,i )ui (i ; ) = k=1(vi,k k )Since independent vi , agent potentially benefit changing allocation,(i , ). Since values k+1 k (non-decreasing threshold vector) vi,k+1 vi,k(non-increasing marginal values), Definition 4 vi,k k 0 k (i )vi,k k 0 k > (i ). Suppose that, misreporting agent allocated(i ) > (i ), ui (i ; ) < ui (i ; ) since:Pi (i ,i )k=i (i ,i )+1(vi,k k ) < 0Similarly, misreporting (i , ) < (i , ) results ui (i ; ) < ui (i ; )since:Pi (i ,i )(vi,k k ) 0k=i (i ,i )+1misreporting effect allocation, utility remains same. Therefore,incentive agent misreport valuations.Note greedy allocation policy threshold policy. Indeed, shownalready satisfy W-MON. next lemma shows threshold conditionholds cancel allocations according policies, set thresholdvalues k = pi,k .192fiAn Online Mechanism Multi-Unit DemandLemma 2. Given non-increasing marginal valuations, allocation policy Section 4.2(for either cancellation policy) threshold policy k = pi,k .htihtiProof. First, definition vector pi pi Section 4.2, values piindependent reports vi made agent i. component valueshahtiVii , . . . , Vi computed based reports agents, first removinghtiagent market. Note pi pi also affected reported arrivaltime, departure time, maximum consumption rate agent i, lemmaconcerned truthful reporting agents valuations, take reportedarrival deadline given, require truthful point.Second, need show two inequalities, thus proof done two parts. Part 1:Whenever vi,k pi,k , allocates least k units agent i. Part 2: Whenever vi,k < pi,k ,allocates strictly fewer k units agent i.Part 1: Let vi,k pi,k . Suppose agent uniform marginal values, vi,k ,htifirst k units (i.e., vi,1 = vi,2 = . . . = vi,k ). Note externality Ei contains marginalvalues time agent displace winning ri units good timestep (that is, marginal values reappear next time step agentsremain market). Given this, long agent fewer k units then, Stage1 mechanism, time step agent market, win exactlyhtiunits marginal values Ei less vi,k , i.e. win unitshtihti1 j |Ei | vi,k Ei,j (ignoring tie breaking). Note externalities (andthus marginal payments) calculated removing agent marketfirst time entered, contain displaced marginal values. However,even when, winning unit, agent displaces losing marginal value future timestep, since value less equal vi,k , affect allocations first kunits future time steps agent since continue higher marginal value.Now, pi,j pi,k j k (by definition), must least k unitshtihtipi,k Ei,j , 1 j |Ei | period ai di , since vi,k pi,k , agent winsleast k units Stage 1.Furthermore, whenever j units particular time step, marginal paymentsunits appear within first k + j first elements pti vector, knumber units earlier time steps (since values lowestclearing payment, ordered ascendingly). agent wins unithtihtiexternality Ei,j Stage 1 vi,k Ei,j (given uniform valuations), followsvi,k = vi,j pi,j whenever wins unit Stage 1. Therefore, pre-allocationscancelled Stage 2.holds agent uniform marginal values vi,k first k units.fact, however, non-increasing valuations, vi,j vi,k , 1 j k,thus allocation policy allocate least k units agent i.Part 2: Let vi,k < pi,k . First consider on-departure cancellation case. perdefinition Stage 2 mechanism, allocation unit k cancelled. However, stillneed show pre-allocated units j > k cancelled well. Since pi,j pi,k (bydefinition) vi,j vi,k (since valuations marginally non-increasing assumption)j > k, follows vi,j < pi,j j > k. Therefore even Stage 1 pre-allocates193fiRobu, Gerding, Stein, Parkes, Rogers & Jenningsk units, cancelled Stage 2, thus strictly fewer k unitsremain.hticonsider immediate cancellation case. Note pi,k pi,k tk di ,tk time k th unit allocated. is, marginal payment valueshtdecrease time. Since vi,k < pi,k (by assumption) pi,k pi,kk , followshthtivi,k pi,kk . Thus follows vi,k < pi,k (ai + k 1) di . Consider casewhere, time tk , k th unit allocated Stage 1. result, pre-allocation k thunit always cancelled time tk Stage 2 allocation policy. Therefore,final result allocation strictly fewer k units.setting k = pi,k , payment policy Equation 4 corresponds paymentpolicy Lemma 1. Therefore proposed mechanism shown DSIC valuations.complete proof showing truthful reporting arrival departuretimes also DSIC given limited misreports, assuming truthful reporting vi .Lemma 3. Given limited misreports, assuming truthfully reporting vi = vidominant strategy given arrival,departure maximum consumption rate reportshai , di , ri i, (subject limited misreports) dominant strategy report ai = ai ,di = di , ri = ri .ha ,d ,rProof. Let pi denote vector increasingly ordered marginal clearing values (computed without i), given agent reports = hvi , ai , di , ri i. reporting type , agentPi (i ) hai ,di ,riallocated (i ) items, total payment is:. agent i, misj=1 pi,jreporting results one two cases:(i ) = (i ): Misreporting agent cannot change values pi ,ever decrease size pi vector. particular, due limited misreportsha ,d ,rai ai , di di ri ri , thus p contains subset elementshai ,di ,ripi. vectors definition increasingly ordered, followshai ,di ,rihai ,di ,ripi,jpi,j, j (di ai + 1). Since payment consists first ki = kielements, increase misreporting.(i ) 6= (i ): First, show (i ) > (i ) could never occur. Sincethreshold values remain same, agent win fewer units per time step (whenreporting lower maximum consumption rate), and/or number time stepsallocations occur decreases (when reporting later arrival and/or earlier deadline), Stage1 mechanism allocate fewer equal numbers units. Furthermore, sinceha ,d ,rha ,d ,rpi,ji pi,ji , possibility cancelling increase Stage 2. Thus,always holds (i ) (i ).Now, consider case (i ) < (i ). First, shown case (i ) = (i )Pi (i ) hai ,di ,ri Pi (i ) hai ,di ,riabove, know j=1pi,jj=1 pi,k(i.e., payment unitsincrease misreporting arrival and/or departure). Furthermore, knowallocation (i ) preferable allocation (i ) < (i ), otherwisereporting true valuation vector vi would dominant strategy. Since paymentitems potentially even higher misreporting, agent cannot benefitwinning fewer items.194fiAn Online Mechanism Multi-Unit DemandTheorem 2. Given non-increasing marginal valuations limited misreports, ondeparture cancellation immediate cancellation policies payment policy accordingEquation 4 DSIC.Proof. proof theorem follows directly lemmas. Lemmas 12 show that, triple arrival/departure/consumption rate (mis)-reports, hai di , ri i,allocation policy truthful terms valuation vector vi , given appropriatepayment policy. Furthermore, payments Equation 4 correspond Lemma 2,therefore truthfully implement mechanism. Finally, Lemma 3 completesreasoning, showing that, truthful report valuation vector vi , agents cannot benefitmisreporting arrivals/departures.4.5 Implications PHEV DomainSince assume units perishable, may always possible cancel unitsallocated. Whereas problem immediate cancellation, sinceunits never allocated begin with, on-departure cancellation policy requiresbattery partially discharged departure vehicle. Although mayundesirable, agents best interest avoid paying units given designmechanism, since marginal value units less marginal payment.agent could avoid discharging unplugging units discharged,agent end paying relatively expensive units. noted, Section 7,addition comparing IM OD mechanisms, also evaluate extentmechanisms would manipulable designed simple greedy allocationpolicy, without assuming cancellation.5. Theoretical Bounds Allocative Efficiencyimportant question given online nature allocation allocative efficiency compares optimal offline allocation, assuming full knowledgefuture. discussed Sections 3 4.2, online setting, possible achieveoptimal allocation, agents arrive leave market continuously. Moreover,multi-dimensional online setting, allocation units needs cancelled ordermaintain truthfulness. Nevertheless, optimal offline allocation represents usefulupper bound could achieved terms allocative efficiency, preferencesavailability constraints agents known advance.end, following, study theoretical worst-case performanceIM OD. precisely, policies, consider two types inefficiencies:Worst-case cancellation ratio. fraction units allocatedsingle agent need cancelled worst case (and, maintain incentiveproperties, cannot allocated agent). Formally, let setagents present market time point time intervalbound computed. Denote ipre (I ) total number units pre-allocatedagent Stage 1 policy entire active period agent,icanc (I ) = ipre (I ) (I ) number units cancelled Stage 2.195fiRobu, Gerding, Stein, Parkes, Rogers & JenningsGiven this, cancellation ratio specific agent RC,i (I ) =icanc (I ).ipre (I )define worst-case cancellation ratio agents types |I|as:canc (I )max.RC= max max ipre|I| iI (I )Competitive ratio allocative efficiency. Whereas cancellation ratio considersworst-case individual agent, competitive ratio compares social welfaremechanism social welfare achieved optimal offline mechanism,full information future arrivals. Here, social welfare defined sumvaluations obtained agents (i.e., sum utilities excluding payments).detail, following work Parkes (2007), competitive ratio settingdefined follows. Let ion (I ) denote number units allocated onlinemechanism departure agent given types agents, |I| , iof f (I )denote number units allocated optimal offline mechanism agent i.P Pionsocial welfare allocations defined as: V (I ) = iI k=1vi,kfPPonline case, respectively V f (I ) =iIk=1 vi,k offline case. Now,competitive analysis assumes existence adversary chooseset inputs, case adversary choose set agent types|I| . Given this, online mechanism said onc-competitive efficiency,exists constant c 1 that: |I| : VVof f((I )) 1c . also sayonline mechanism guaranteed achieve within fractionoptimal offline algorithm.1cvaluemotivation studying two metrics follows. First, outlined Section4.2, variants mechanism propose require part allocationagents sometimes cancelled, order ensure truthfulness. natural askworst-case fraction number units allocated agent needcancelled, types mechanisms (i.e., immediate on-departure cancellation), market set-up. second criteria (i.e., competitive ratio allocativeefficiency), follow metric proposed Hajiaghayi et al. (2005) Parkes (2007)online domains, caveat deriving bound multi-dimensional caseconsiderably involved single-dimensional one, due required cancellations. Section 5.1 study issues mechanism immediate cancellation,Section 5.2 mechanism on-departure cancellation.5.1 Worst-Case Bounds Mechanism Immediate Cancellationfollowing theorem shows that, using online mechanism immediate cancellation, worst-case cancellation ratio goes 1 number units requiredsingle agent goes infinity.max = 1, n maximumTheorem 3. Using IM allocation policy, limn RCdemand.Proof. proof example. Consider following setting consisting agentmarginal valuation vector vA = hv1 , v2 , ...vn i, values strictly decreasing,196fiAn Online Mechanism Multi-Unit Demandi.e., v1 > v2 > .... > vn . assume agent arrives time aA = 1, departs dA ,dA = n (n + 1)/2, maximum charging speed rA = 1. Agent facessequence cursory (i.e. local) agents, agents desires exactly one unit,present market one timestep departs immediately afterwards.one time exactly one cursory agents market. valuationsagents follows. first agent valuation v1 = hv1 i, next two agents,= 2 = 3, valuations vi = hv2 i, next three agents, {4, 5, 6},valuations vi = hv3 i, next four agents, {7, 8, 9, 10}, vi = hv4 i, etc. Thus,total, sequence n (n + 1)/2 agents. Here, sufficiently smallv1 > v2 . result, agent imposes following externality timestep:hv1 , v2 , v2 , v3 , v3 , v3 , v4 , v4 , v4 , v4 , ...i (noting that, sincecursory agents present market single time step, cancelling allocationaffect externality, marginal payment).allocation settings proceeds follows. first time step, unitpre-allocated agent (since v1 > v1 ) cancellation. secondtime step, unit pre-allocated agent (since v2 > v2 ), pointh2imarginal payment pi = hv2 , v1 i. Since marginal value second unitless marginal payment unit, i.e., v2 < v1 , unit gets cancelled.Therefore, time = 3, marginal value agent still v2 , third unit alsogets allocated agent, time cancelled. However, next twotime-steps, units pre-allocated cancelled times. see this, noteh2imarginal payment time = 5 pi = hv3 , v3 , v2 , v2 , v1 i. Sinceh2iv3 < pi,3 = v2 , unit gets cancelled.generally, every k th unit allocated cancelled, marginalvalue agent becomes vk+1 , next k units first pre-allocated (sincemarginal value cursory agents vk+1 ), subsequently cancelled (sincemarginal payment vk ). (k + 1)th unit allocatedcancelled, next k + 1 units cancelled, on.pre= 1 + 2 + 3 + 4 + . . . + n = n (n + 1)/2 units pre-allocated.result,canc = 0 + 1 + 2 + 3 + . . . + (n 1) = (n 1) n/2 cancelled,units= n remain allocated. Therefore, ratio number units cancelled nis:cancn1n2 n= lim=1pre = limnn n + 1n n2 + nRC,A = limtheorem shows worst-case result individual agent unbounded. use result (and example constructed proof) derivesimilarly negative result allocative efficiency (i.e., overall efficiency system):Theorem 4. mechanism immediate cancellation, competitive ratioallocation efficiency unbounded. is, exists finite c, that:|I| :1V (I )fcV(I )197fiRobu, Gerding, Stein, Parkes, Rogers & JenningsProof. Generally, two potential sources inefficiency w.r.t. offline allocation:either units pre-allocated subsequently cancelled, units allocatedagents less utility agents would allocated offlinecase. proof based former source inefficiency uses examplegiven Theorem 3.example Theorem 3 showed possible construct examplewhere, given n (n + 1)/2 units supply, (n 1) n cancelled (and thusallocated agent) n allocated. Now, suppose valuationsunits agents (including agent A) [v, v], v/v = r finite constant.Since using optimal offline allocation units allocated, total valueleast: V f v n (n + 1)/2. hand, online allocation using onlineallocation using immediate cancellation value most: V v n.Given this, following holds:2rvnVlim= lim=0n v n (n + 1)/2n n + 1n V flimTherefore, constant c, always possible find counter exampleworst-case efficiency lower 1/c.Thus, theoretical bound efficiency loss using immediatecancellation allocation policy. However, proof relies agentinfinitely patient, infinite demand, higher valuation biddersunit. practice, extreme situation would never occur. considerpractical scenarios, therefore, Section 7 use simulations investigate realistic settings. showing worst-case bounds immediate cancellation, remainderSection 5 derive theoretical bounds on-departure cancellation mechanism.Specifically, show mechanism provides much better bounds. fact,competitive bounds efficiency single-unit demand settings,cancellation occurs.5.2 Worst-Case Bounds Mechanism On-Departure Cancellationsection divided two parts: Section 5.2.1 discuss worst-case cancellationratio particular agent provide tight bound, Section 5.2.2 considerbound allocative efficiency entire market.5.2.1 Worst-Case Cancellation Ratiosection organised follows. First, show half unitscancelled particular agent. go show exist exampleshalf cancelled. Note that, convenience, following lemma formulatedterms units retained instead units cancelled.Lemma 4. Using on-departure cancellation, suppose agent pre-allocated n unitsdeparture time di , k units kept Stage 2 (and mechanismcancels n k units). Then, type profile |I| , agent I, k n/2(i.e., least half units allocated).198fiAn Online Mechanism Multi-Unit DemandProof. prove property, start deriving two inequalities hold valuek. First, since k defined number units kept, remaining ones (n k)cancelled, must hold vi,k+1 < pi,k+1 (otherwise (k + 1)th unit wouldcancelled, contradicting definition).second inequality given vi,k+1 pi,nk less obvious. seealways holds, need observation greedy allocation works. Recall nnumber units pre-allocated greedy allocation policy. Therefore,active marginal values, vi,1 , . . . , vi,n , point [ai , di ] among top S(t)highest marginal values. Consequently, lowest marginal value ones pre-allocated,vi,n , must greater marginal payment least one unit (otherwise couldunit). Since marginal payments sorted increasing order,must therefore hold vi,n pi,1 . Similarly, next-lowest value, must holdvi,n1 pi,2 , on. general write vi,nj+1 pi,j , j {1, n}. setj = n k, get vi,k+1 pi,nk .Therefore, order greedy policy allocate n units mechanismsubsequently cancel units positions k + 1 n (assuming k + 1 < n, otherwisecancelling take place departure agent i), following inequalities mustsatisfied:(vi,k+1 < pi,k+1(5)vi,k+1 pi,nkGiven this, show k n/2 contradiction. Suppose k = n/2 1,i.e., strictly n/2 cancelled. conditions become:(vi,n/2 < pi,n/2(6)vi,n/2 pi,nn/2+1show contradiction, need consider separately cases n evenn odd. n even, n = n/2 + n/2, systembecomes:(vi,n/2 < pi,n/2(7)vi,n/2 pi,n/2+1implies pi,n/2+1 < pi,n/2 , since marginal price vector pi weakly increasingdefinition, leads contradiction. case n odd,n = n/2 + n/2 1, conditions become:(vi,n/2 < pi,n/2(8)vi,n/2 pi,n/2Clearly, equations cannot satisfied simultaneously, leading contradiction.Note value k < n/2 would lead contradiction due piincreasing, hence necessarily k n/2, completing proof.complete analysis, show bound tight, i.e., exist settingshalf units allocated agent cancelled.199fiRobu, Gerding, Stein, Parkes, Rogers & JenningsLemma 5. exist settings mechanism on-departure cancellationcancels allocation n/2 units, n/2 units kept, departureagent i, n number pre-allocated units.Proof. proof done constructing worst-case example. Considersingle agent, A, market n time periods, demand n units,n even. first n/2 marginal valuations equal 4, remaining ones 2.example, n = 8, marginal valuation vector becomes vA = h4, 4, 4, 4, 2, 2, 2, 2i. Similarproof Theorem 3, agent faced, time step, different, single cursoryagent, participates time step. valuations first n/2 cursoryagents sequence given vi = h3i, second half agents vi = h1i.Thus, n = 8, marginal payment agent would pA = h1, 1, 1, 1, 3, 3, 3, 3i.setting, mechanism on-departure cancellation would pre-allocate unitsagent (since vA,k = 4 > vk,1 = 3 k n/2 vA,k = 2 > vk,1 = 1 k > n/2).However, departure agent A, exactly half units allocated cancelled (sincevA,k = 2 < pA,k = 3 k > n/2).Finally, unify results Lemmas 4 5 following theorem.Theorem 5. setting on-departure cancellation non-increasing marginal values, number units agents present, worst case cancellation-ratiomax = 1 .number units allocated agent RC2Proof. Lemma 4 shows that, regardless set-up, half units allocatedmax 1 , regardless settingagent cancelled departure, thus RC2(i.e., possible input types agents). Lemma 5 shows exist settingsmax = 1 , completing proof.cancellation ratio exactly RC2Note practice smaller settings, significantly fewer half unitscancelled. worst case cancellation ratio 1/2 allocations occurs specifically constructed example, and, shown experimental analysis, realisticdistributions application domain, actual performance much better.5.2.2 Competitive Bound Allocative Efficiencyprevious section discusses cancellation problem perspective singleagents, whole market. section, show that, case agentsweakly decreasing marginal values, allocation returned on-departure cancellationmechanism 2-competitive optimal offline allocation. result meansmulti-unit demand case on-departure cancellation worse terms worst-casecompetitive bound single-unit demand problem discussed Hajiaghayi et al.(2005), Parkes (2007), despite fact single unit demand needcancellation ensure incentive compatibility. Formally, statefollowing theorem:Theorem 6. mechanism on-departure cancellation 2-competitive optimal offline allocation, setting non-increasing marginal values.200fiAn Online Mechanism Multi-Unit DemandProof. order establish competitive bound optimal offline allocation, usecharging argument similar Hajiaghayi et al. (2005).5 basic ideacharge (or match) marginal value units agent allocatedoffline case another, higher-valued unit allocated offline online.either unit itself, higher value unit causes allocatedonline market. Formally, consider units vi,p (belonging agent position p)allocated offline online case. unit vi,p chargedtwice, itself, lower valued unit allocated offline online,follows worst-case social welfare ratio online vs. offlineallocation cannot drop 1:2.Now, agents single-unit demand (such case discussed Hajiaghayi et al.,2005), easy see property always holds, unit vi,p allocatedonline once, thus displace one unit vj,q . Crucially, unitscancelled. multi-unit demand setting, argument becomes involved,unit vi,p (allocated online offline) affect online market several ways:displace another unit vj,q would allocated offline, displacemean specifically unit vj,q never pre-allocated online (hence cancellationapply it).cause cancellation another unit vj,q . second case, unit vj,qpre-allocated, allocation cancelled due presence unit vi,pmarket (meaning pre-allocation would cancelled departureagent j, unit vi,p present).main issue remains shown unit vi,p displace causecancellation one unit would allocated offline. Thus, cannotdisplace two units allocated offline, allocated online, duepresence unit vi,p .show contradiction. Formally, suppose three units: vi,p , vj,qvk,r allocated offline case (with vj,q < vi,p vk,r < vi,p ). Unit vi,p allocatedonline case (i.e., pre-allocated cancelled). Units vj,q vk,r allocatedonline case unit vi,p present, allocated online unit vi,p present.Given set-up, three possible cases:1. Neither units vj,q vk,r pre-allocated online unit vi,p present (hence,cancellation either vj,q vk,r ).2. Unit vj,q never pre-allocated online, unit vk,r pre-allocated allocationcancelled later (i.e., departure agent k market), unit vi,p present.3. units vj,q vk,r pre-allocated, pre-allocations cancelleddeparture market agents j, respectively k, unit vi,p present.5. term charging refer electricity charging, represents name proof deviceused online mechanism design.201fiRobu, Gerding, Stein, Parkes, Rogers & Jenningscases, unit vi,p present, units vj,q vk,r pre-allocated cancelled online case. order complete proof need show, contradiction,three cases could occur.Case 1 similar case single unit demand discussed Hajiaghayi et al.(2005), cancellation occurs units. relatively straightforward seecannot occur, unit vi,p pre-allocated (at time t), thusdisplace one unit would allocated otherwise.either unit allocated online time time t, unit allocated online later on,unit is, turn, displaced it.Case 2: Suppose vj,q (belonging agent j active [aj , dj ])unit assumed pre-allocated vi,p present, unit vk,r unitallocated cancelled. two subcases consider here, requireseparate discussion.Case 2A: First, consider vj,q > vk,r . case, agent k lower marginalvalue agent j, value vk,r still pre-allocated essentially greedyallocation policy, vj,q not. means agent k must patientagent j, hence dj < dk , otherwise vj,q would pre-allocated instead.Now, denote pk payment vector agent k, defined Section 4.2.unit vk,r cancelled must hold vk,r < pk,r . Now, denote pk<i> vectormarginal payments agent k agent present market, recallassumption value vj,q allocated. Thus, have:pk<i> = incr(pk \{vi,p } {vj,q })incr operator orders elements increasing order. Since vj,q > vk,r ,<i>follows vk,r< pk,r , thus allocation unit vk,r would still cancelled, evenwithout unit vi,p .Case 2B: second subcase, consider vj,q vk,r , i.e., value unitdisplaced agent lower one pre-allocated cancelled. Firstnote that, case occur, unit vj,q allocated online within [ak , dk ], activewindow agent k. obvious condition: agent j allocated online outsidewindow (and displaced agent market, displacement occursoutside [ak , dk ]), units vj,q vi,p cannot influence cancellation unit vk,r (becauseunit pre-allocated once, case pre-allocation vi,p wouldhappen outside [ak , dk ]).previously, recall condition unit vk,r < pk,r , required unit vk,rcancelled. Note means least k units [ak , dk ] highervalue unit vk,r , thus, offline allocation (which benchmark) would needtake priority it. setting, one units vi,p . even removingvi,p it, vector pk<i> = incr(pk \{vi,p } {vj,q }) must contain least k 1 values highervk,r . offline allocation without unit vi,p , k 1 values must given priority,together least unit vk,r . However, means unit vj,q cannot allocatedoffline [ak , dk ], lower value vk,r . gives contradictioninitial assumption units vi,j vk,r allocated offline (as well online)without unit vi,p present.202fiAn Online Mechanism Multi-Unit Demandexplain intuitively, means unit vi,p cause displacement (non-allocation) unit vj,q cancellation another one vk,r ,possible units high enough value allocated offlinecase well. Thus, one offline-allocated unit allocated onlinepresence unit vi,p market.Case 3: final case, units would need pre-allocated cancelled,absence value vi,p . contradiction case shown similarly Case 2Aabove. Considering marginal price vectors agents j k without vi,pmarket:pj<i> = incr(pj \{vi,p } {vk,r })pk<i> = incr(pk \{vi,p } {vj,q })easy see that, regardless whether value vj,q vk,r lower, valuecancellation would still occur departure market without agent i, leadingcontradiction.summarise, exhaustively shown contradiction holdspossible cases. Thus, unit vi,p allocated online offline displace(or lead cancellation of) one unit allocated offline. Thus, two unitsallocated offline charged unit allocated online, completing proof.6. Computational Aspectssection, consider implications implementing mechanisms practice, including computational complexity algorithms. examine on-departureimmediate cancellation separately, differ fundamentally complexity.6.1 Implementing On-Departure CancellationAlgorithm 1 briefly outlines implementation mechanism on-departure cancellation (OD). Here, assume first time step denoted t0 , secondt1 = t0 + 1, on. simplicity, use throughout section denotefull set agents arriving time points, note algorithm explicitlyuses information future arrivals. Initially, algorithm sets endowmentsagents 0 (line 2), units allocated. Then, every time step t,algorithm first pre-allocates units using greedy allocation policy (line 4).done O(N rmax ) using well known linear-time selection algorithm described Blum,Floyd, Pratt, Rivest, Tarjan (1973), N = |I| total number agentsrmax = maxiI ri maximum consumption rate.Next, algorithm computes marginal payments time step rerunningmarket without active agent (line 6). rerunning market particularhtiagent i, important note pre-allocations effect picancellations irrelevant, affect future development market.Therefore, necessary compute greedy allocation agent removed,total run-time, active agents, O(N 2 rmax ) (assuming resultshtiprevious time steps re-used). Updating pi new marginal payments203fiRobu, Gerding, Stein, Parkes, Rogers & JenningsAlgorithm 1 Mechanism On-Departure Cancellation (OD).1: procedure OnDepartureMechanism(I , S)ht2:kht0 h0, 0, . . . , 0iInitial endowments, ki 0 = 0,3:{t0 , t1 , . . .}4:kht+1i GreedyAllocation(I , S(t), khti )Run greedy allocation5:{j I|aj dj t}Iterate active agentshti6:update pi usingRun market without7:di =agent departinght+1i hti8:k , pi k, piFinal pre-allocation marginal payments9:vi,ki < pi,ki10:ki ki 1Cancel units necessary11:end Ppi,kFinal payment agent12:xi (i |ki ) kk=113:end14:end15:end16: end proceduredone using simple insertion algorithm, and, noting lowest |vi | paymentsagent need kept, done, agents, O(N rmax vmax ),vmax maximum length agents valuation vector.Finally, departure, lines 811, units lower valuationcorresponding marginal payments cancelled, final payment calculatedline 12. done O(N vmax ) simply iterating values.summary, time complexity algorithm OD mechanism O(N 2 rmax +N rmax vmax + N vmax ) time step. rmax vmax assumed constant6 ,simplifies O(N 2 ). Generally, means algorithm executed quickly,even large numbers agents.6.2 Implementing Immediate CancellationNext, consider mechanism immediate cancellation (IM), shown Algorithm 2. key difference OD mechanism units potentially cancelledevery time step agent active (lines 79), rather departure.small modification significant impact computational tractability mechanism. Unlike previous mechanism, computing marginal payments line 6,cancellations agents affect payments. feature already highlightedexample Section 4.3.1, cancellation second unit pre-allocatedagent 1 causes change marginal payments agent 3. generally, cancellationsimmediate effect endowments agents directly affects activemarginal valuations V hti subsequent time steps.6. reasonable, limited technological constraints practice. particular, rmaxlimited battery infrastructure constraints, vmax related petrol savings achievableEV (as detailed Section 7.3.3).204fiAn Online Mechanism Multi-Unit DemandAlgorithm 2 Mechanism Immediate Cancellation (IM).1: procedure ImmediateMechanism(I , S)ht2:kht0 h0, 0, . . . , 0iInitial endowments, ki 0 = 0,3:{t0 , t1 , . . .}4:kht+1i GreedyAllocation(I , S(t), khti )Run greedy allocation5:{j I|aj dj t}Iterate active agentshti6:update pi usingRun market withouthti7:vi,kht+1i < p ht+1i8:9:10:11:12:13:14:15:16:ht+1ikii,kiht+1iki1enddi =ht+1i htik , pi k,pP iixi (i |ki ) kk=1pi,kendendendend procedureCancel units immediatelyagent departingFinal allocation marginal paymentsFinal payment agentNow, order determine cancellations rerunning market without activeagent i, necessary compute marginal payments agentsmarkets (effectively executing full algorithm ). Clearly, leadsrecursion potentially sees possible subsets agents evaluated. worst case,therefore, cancellation decisions need executed every agent every possiblesubset I, N 2(N 1) times.7 Simplifying assuming vmax rmaxconstant, leads runtime complexity O(N 2N ).runtime exponential number agents clearly problem applyingmechanism realistic settings handful agents. However, tackleproblems, possible use technique akin branch-and-bound entersrecursion line 6 necessary. present following section.6.3 Speeding Immediate Cancellation Using Boundsobtain faster algorithm IM mechanism, instead calculating marginalhtipayments pi every time step, find iteratively refine lower upper boundspayments. intuition behind approach choose initial boundseasily calculated without resorting recursion. agents reported valuationpre-allocated unit, vi,k , lies outside bounds, immediately determine whetherunit cancelled not. hand, vi,k lies bounds, refine7. practice, recursion occurs set agents active timeagent evaluated, previous decisions affected agent presence similarlyagents arriving di affect i. means settings large numbers agents maystill tractable little overlap active agents, sake analysissection, assume worst case, N agents active concurrently.205fiRobu, Gerding, Stein, Parkes, Rogers & Jenningsiteratively calculating actual marginal payments agents activetime steps cancellation decision unambiguous.ht,siht,sidetail, use pipidenote lower upper bounds, respechtitively, pi . Here, indicates level refinement, time step actualmarginal payments calculated, ai 1 t. Analogous actualhtiht,siht,sipi vector, pipivectors increasing order, represent boundshtiht,sihtiht,sipi , pi,k pi,k pi,k , k s. level refinement, s,htiincreased, bounds become tighter, eventually converging pi,k . following,describe detail calculate initial bounds (with = ai 1, indicatingactual payments calculated yet):ht,a 1iorder calculate initial lower bounds, pi , rerun market withoutai using greedy allocation policy without cancellations.marginal payments market (as described Section 4.2) used lowerhtibounds. payments necessarily actual payments pi ,may cancellations latter, cause active marginal valuationschange subsequent time steps. However, seen easily indeedrepresent lower bound. Specifically, cancellations either influencecause increase one elements externality vector E hti (since agentslower endowment cancellations therefore equal higher valueobtaining additional units).ht,a 1icalculate initial upper bounds, pi , consider actual market including agent time step ai t, calculate externality agentwould impose others winning available unit throughout time interval use derive upper bounds payments. formally,htidefine new multiset set valuations, Vi0 , derived simply removingelements corresponding agent V hti padding zeroes, necessary, size least S(t). Then, proceed similar mannerSection 4.2 defining externality agent would impose others time stephtihtiEi = minhri (maxhS(t)i Vi0 ). Given this, final vector upper boundsht,a 1ihtpi= incr( tt =ai Ei ).Unlike actual marginal payments, upper bounds include effect agentmarket, may allocated units would allocatedothers, or, presence, cause cancellation units, which, turn,affect active valuations agents subsequent time steps. However,effect reduce supply available agents, including agentht,a 1imarket increase active valuations agents, therefore pihtiupper bound pi .Given bounds, quickly test agents marginal valuation vi,kht,a 1ipre-allocated unit falls outside bounds. vi,k < pi,k , unit canht,ai 1icelled immediately, vi,k pi,k, definitely cancelled. However,206fiAn Online Mechanism Multi-Unit DemandAlgorithm 3 Mechanism Immediate Cancellation (IM) Bounds.1: procedure BoundedImmediateMechanism(I , S)ht2:kht0 h0, 0, . . . , 0iInitial endowments, ki 0 = 0,3:4:si ai 1Initial refinement bounds5:end6:{t0 , t1 , . . .}7:kht+1i GreedyAllocation(I , S(t), khti )Run greedy allocation8:{j I|aj dj t}Iterate active agentsht,sht,s9:Calculate pi piAdd initial bounds time10:repeatht,si11:vi,kht+1i < p ht+1iht+1iki12:else vi,kht+1i13:15:16:17:19:20:21:22:23:24:25:26:27:Unit definitely cancelledi,kiRefine boundsi,kidi =agent departinght+1iki kiFinal allocationht,siht,six {1, 2, . . . , ki }, pi,x6= pi,xsi si + 1Refine final paymentsend Pht,sFinal payment agentpi,kxi (i |ki ) kk=1endendendend procedureht,ai 1ipi,k1ht,si< p ht+1isi si + 1ht,sht,sUpdate pi piendht,sivi,kht+1i p ht+1i14:18:i,kiht+1ikiht,ai 1ivi,k < pi,k, bounds ambiguous need refined.ht,siht,siobtain refined bounds pipicomputing actual marginal payments specified time s, bounds calculated above.effectively replaces initial bounds actual marginal payments, resultinght,sihtiht,siaccurate overall bounds. Eventually, = t, pi= pi = pi.ht,sihtformally, refined lower bounds calculated pi= incr(( st =ai Ei )hthti( tt =ai +1 Ei )), Ei externality vector market without i, using immehtidiate cancellations (as used actual payments), Ei corresponding vectormarket without without cancellations. Similarly, refined upper boundsht,sihththticalculated pi= incr(( st =ai Ei ) ( tt =ai +1 Ei )), Eidefinedabove.207fiRobu, Gerding, Stein, Parkes, Rogers & JenningsFull details IM mechanism using bounds given Algorithm 3. keepstrack level refinement bounds agent (as si , initialisedai 1 line 4). Then, instead updating actual payments, lines 1017 repeatedlycompare marginal valuation last unit pre-allocated (vi,kht+1i )current upper lower bounds, refining necessary. Note here, previouscalculations bounds reused, algorithm checks iteratively, eitherincreasing (when next time step calculated) si 1 (when bounds refined).case, means active valuations one additional time stepadded existing bounds. loop repeats statement line 17 becomestrue, captures cases last pre-allocated unit definitely cancelled(in case pre-allocated units also cancelled, marginalvaluations least high, respective payments equal lower),pre-allocated units time step cancelled.Finally, agent departs, final allocation payments calculatedlines 1824. Here, important calculate actual marginal paymentski allocated units. achieved refining bounds first ki upperlower bounds equal. incur computational effort,htiequivalent computing complete pi vector Algorithm 2. paymentsneed calculated units actually allocated departure, upperlower bounds may equal without needing compute actual payments, importantly,required full set agents recursively subsets agents.practice, algorithm bounds significantly reduces computational runtime mechanism (typically 99% throughout experiments conductedSection 7), often avoids re-running market possible subsets agents.However, important note worst-case run-time still equivalent Algorithm 2, i.e., O(N 2N ), exponential number agents. best case,recursion necessary, run-time reduces O(N 2 ).7. Experimental Evaluationsection, quantify performance mechanisms, compared numberbenchmarks, applying range settings. investigated theoreticalperformance bounds mechanisms Section 5, purpose section evaluateperformance realistic settings.Specifically, Section 7.2, consider general setting easily reproducibleshow mechanisms perform vary supply demand good. Then,Section 7.3, turn PHEV domain. this, first show deriveagents preferences based vehicle owners driving behaviour. Then, use real datacollected first large-scale trial pure electric vehicles (EVs) UK showtrends continue hold realistic application setting. Furthermore, lookgradual introduction fast-charging PHEVs would affect neighbourhoodlimited electricity supply, terms social welfare (which translates overall fuelsavings within neighbourhood) financial savings individuals. Throughoutexperiments, also consider simpler greedy allocation mechanism without cancellationquantify potential benefits agent would able achieve misreporting208fiAn Online Mechanism Multi-Unit Demandmechanism. demonstrates whether actually scope strategic misreportingrealistic settings whether cancellation needed practice.consider two specific settings, briefly outline common parametersbenchmarks used experiments.7.1 Experimental Setupevaluate mechanisms, simulate different settings number agents compete limited supply good allocated hourly basis 24-hourperiod. order test scenarios varying supply demand, sample agentsrandomly fixed probability distributions use range supply functions (theseoutlined detail Sections 7.2 7.3). order ensure statistical significance results, re-sample agents 1,000 times setting, plot 95%confidence intervals throughout section.addition two mechanisms proposed paper, immediate cancellation(IM) on-departure cancellation (OD), evaluate number benchmark mechanisms:Fixed fixed-price mechanism allocates units agentsvaluation least given constant p, price pay unit p.demand exceeds supply, unit allocated agent chosen uniformlyrandom set agents sufficiently high valuation. Here, agentmay receive multiple units, maximum consumption rate, ri . mechanismDSIC constitutes direct comparison mechanisms. However,optimise performance fixed-price mechanism, p must carefully chosen.Thus, given setting, test possible values (in steps 0.01) selectp achieves highest average efficiency (over 1,000 trials). Thus, showingresults Fixed, constitutes upper bound could achievedmechanism.Random special case Fixed, p = 0. Thus, using baseline benchmark,units allocated randomly agents pay anything.Greedy simple greedy allocation policy, described Section 4. Paymentscalculated using pi prices (as IM OD), cancellations.Thus, mechanism truthful, constitutes interesting comparisonmechanisms, allows us quantify loss efficiency causedcancellations, well potential benefits agents misreportingabsence cancellations.Heuristic allocates units weighted combination agents valuationurgency (proximity departure time) maximised. Here, [0, 1]parameter denotes importance urgency, = 1 correspondswell-known earliest-deadline-first heuristic scheduling (Pinedo, 2008), = 0indicates units always allocated agent highest valuation.truthful mechanism impose payments here, primarypurpose benchmark approach. Again, always select besttesting values steps 0.01 setting.209fiRobu, Gerding, Stein, Parkes, Rogers & JenningsOptimal assumes complete knowledge future arrivals supply, allocatesunits agents maximise overall allocative efficiency. Clearly, mechanismpossible assumes knowledge future also truthful(again impose payments), serves upper bound efficiencycould achieved.7.2 General Allocation SettingFirst, consider general synthetic setting, generate agents supplyfunction simple distributions. main reason examining scenarioturning realistic setup generate results easily reproducibletied specific application domain. following, outline distributionssample supply agents (Section 7.2.1), discuss results(Section 7.2.2).7.2.1 Synthetic Setupsetting, generate supply function S(t) randomly drawing discreteuniform distribution {1, 2, 3, . . . , s}, vary experiments representdifferent amounts good produced. agent i, sample arrivaltime ai discrete uniform distribution {0, 1, 2, . . . , 23} departure time{ai , ai + 1, . . . , 23}. sample maximum consumption rate ri {1, 2, 3, 4, 5},finally, generate vi first selecting number required units uniformly random {1, 2, 3, . . . , 20}. Then, first valuation vi,1 sampled exponentialdistribution rate 1, remaining valuations drawn uniformly randomcontinuous interval [0, vi,1 ] (ordered appropriately ensure non-increasing marginalvaluations).7.2.2 Synthetic ResultsFigure 3, examine allocative efficiency mechanisms increase number agents competing limited supply electricity. figure shows allocativeefficiency setting low supply (left), = 1, i.e., one unit available pertime step, setting high supply (right), = 20, i.e., 20 unitsavailable per time step. choose two extreme settings show full spectrumpotential supply scenarios (and focus supply settings based real data Section 7.3).Note due run-time complexity, plot IM smaller setting oneunit supply. supply high (s = 20), agent typically allocated largehtinumber units, causing IM require frequent refinements bounds pivectors thus leading computational bottleneck. non-truthful Heuristic approach consistently achieves around 99% Optimal full information = 1,plot readability, also use approximation Optimal= 20 (where Optimal also becomes computationally infeasible).Several trends emerge results. First, = 1, simple truthfulGreedy approach performs well (around 99% Optimal). Next, notetruthful mechanisms, IM OD also perform well, achieving around 95% 96%210fiAn Online Mechanism Multi-Unit DemandAllocative Efficiency (% Optimal)Approximate Allocative Efficiency (% Heuristic)100%100%90%90%80%80%GreedyODIMFixedRandom70%60%50%70%60%50%40%40%30%30%20%20%02550 75 100 125 150 175 200Number Agents02550 75 100 125 150 175 200Number AgentsFigure 3: Allocative efficiency synthetic setting (low supply, = 1, left,high supply, = 20, right).Optimal, respectively. slightly lower Greedy, indicates 34% loss efficiency incurred due cancellations. difference performanceIM OD expected here, IM cancel units (see Section 5); however, despiteunfavourable worst-case performance IM, surprising differencesignificant practice. Overall, results promising, indicating mechanismswork well settings, specific conditions cause cancellations (i.e.,valuations allocated units effectively cross marginal payments)occur frequently practice.fixed price mechanism, Fixed, performs significantly worse proposed mechanisms, achieving 81% 83% Optimal. because, order remaintruthful, mechanism sets single fixed price respond dynamicallychanges supply demand time step time step. also explainsmechanism performs worst some, much, competition, e.g., around30 agents. Here, fixed price starts rise, ensure agents higher valuationsallocated first, still considerable variance valuations time step,sometimes leaving high-value agents unallocated, times units allocatedall. contrast this, IM OD always allocate available units agentshighest valuations. Furthermore, noted Fixed, unlike IM OD,assumes priori knowledge distributions agents drawn. mayalways available practice, decrease performance.Finally, Random mechanism performs worst all, surprising, usesinformation agents valuations all. However, poor performance demon211fiRobu, Gerding, Stein, Parkes, Rogers & Jennings10%Proportion PreAllocated Units Cancelled10%ODIMOD5%5%0%0%050100150200Number Agents050100150200Number AgentsFigure 4: Proportion initially allocated units cancelled (s = 1 left, = 20right).strates clearly potential perils using poorly designed non-truthful mechanisms,strategically misreported valuations may relation agents actual valuations.setting abundant supply, = 20, broad trendsobserved. OD mechanism still achieves around 97% near-optimal Heuristicslightly less Greedy, Fixed Random perform significantly worse. However,gap performance smaller time, less competition oftensufficient units satisfy agents. settings, benefit alwaysallocating agents highest valuations generally lower. Note IMinfeasible settings due high run-time complexity, OD took, average,less 100ms execute 24 time steps even complex settings = 20200 agents.Next, illustrate actual cancellations IM OD mechanisms farworst-case bounds established Section 5, Figure 4 shows average proportionpre-allocated units cancelled. generally low, ranging 0% 7%. expected,OD mechanism cancels fewer units IM mechanism. Furthermore,general trend cancel units competition low (as usually sufficienthtiunits satisfy agents, leading mostly 0 valuations pi vectors). Cancellationspeak medium levels competition, start drop slightly.htipeak explained large variations pi vectors settingsalso agents generally allocated units settingscompetition (leading similarly higher variation valuations agents allocatedunits).Figure 5, next consider potential benefits misreporting cancellationsused (as Greedy mechanism). measure computing utilityagent on-departure cancellations used compare actual utility gainedGreedy mechanism. constitutes best deviation single agent couldachieved perfect hindsight actual pi prices. plot proportioncases agent achieve gain misreporting (light blue), conditional212fiAn Online Mechanism Multi-Unit DemandPotential Gain Utility Misreporting25%25%20%20%15%15%10%10%5%5%0%Proportion GainsConditional Utility IncreaseOverall Utility Increase0%0255075 100 125 150 175 200Number Agents0255075 100 125 150 175 200Number AgentsFigure 5: Potential gains misreporting Greedy mechanism (s = 1 left,= 20 right).proportional increase utility gain misreporting exists (dark red),product two, i.e., overall average proportional increase utility, including casesgain (light red).results indicate without cancellations, often cases agentbenefit misreporting 67% cases = 1 20% cases= 20. provides clear motivation using incentive compatible mechanismssettings. However, although individual gains lead average increaseutility 1520%, considering overall average utility increase (includingcases agents benefit), 12% significantly less manyspecific settings. offers promise settings cancellations infeasible,example immediate cancellations computationally feasible largesettings, on-departure cancellations cannot practically implemented.low expected gains 12% less, agent may wish exert additional effortsstrategise. Furthermore, gains represents upper bound achievedperfect foresight prices, likely unavailable practice. alsonote expected gain misreporting fluctuates significantly, dependingspecific setting similar cancellations, fluctuation caused variationspi vectors also valuations allocated units.conclude section, Figure 6 explores performance gapproposed mechanisms benchmarks supply increased. Here, fix numberagents 50 vary maximum number available units good pertime step, s, 1 70. Due complexity setting, omit OptimalIM analysis. clear relative benefit OD mechanismtruthful benchmarks decreases supply increased. surprising,eventually agents completely satisfied, even random allocation policy.213fiRobu, Gerding, Stein, Parkes, Rogers & JenningsApproximate Allocative Efficiency (% Heuristic)100%90%80%GreedyODFixedRandom70%60%50%40%30%010203040Supply (Maximum Units)506070Figure 6: Approximate allocative efficiency maximum supply per time step (s)increased.However, still significant benefit using OD relatively high supply levelsaround = 45, never outperformed truthful benchmarks, consistentlyperforming close near-optimal Heuristic (over 99% cases) closeGreedy mechanism without cancellations. terms potential gains deviations, similartrends discussed previously observed omit detailed figure.far, concentrated describing general performance mechanismssynthetic, easily reproducible setting. following, apply data realEV setting.7.3 PHEV Settingsection, use data real (pure) EV trial simulate typical charging patterns.8 allows us verify trends discussed previous sectioncontinue hold realistic setting. Furthermore, basing experiments actualPHEV characteristics enables us quantify actual utility drivers real terms (i.e.,monetary gain fuel saved). also investigate whether introductionfaster charging speeds lead benefits settings consider.investigation interesting, fast chargers already available domestic settingsallow vehicles charge twice normal rate (or faster).9 However, impactsettings unclear, still constrained overall supply electricity.following, first present principled approach deriving agents marginalvaluation vector show approach satisfies non-increasing marginal valuationassumption (Section 7.3.1). describe real-world data use8. Note that, whereas simulation based PHEVs, uses real-world experimental data pureEVs. However, believe reasonable assume charging behaviour would similar.9. See, example, http://www.pod-point.com/ http://www.charging-solutions.com/.214fiAn Online Mechanism Multi-Unit Demandexperiments (Section 7.3.2), followed outline used sample PHEVs(Section 7.3.3). Finally, discuss results (Section 7.3.4).7.3.1 Deriving Agents Marginal Valuation Vectorimportant part overall model method computing marginal valuationvector, vi , based real data. so, combine data sampled cars actualjourney distances principled approach calculating expected economic benefitcharging PHEVs. detail, first derive probability density function, p(m),data, describes probability distance travelled miles (describedSections 7.3.2 7.3.3). Given distribution, price fuel (in /litre), pp ,internal combustion engine efficiency (in miles/litre), ep , efficiency electricengine (in miles/kWh), ee , calculate expected utility certain amountcharge (in kWh), ce , follows:ZZppppp(m)dm(m ce ee ) p(m)dm,(9)E(u(ce )) =epce ee ep0first term expected fuel cost without charge, second termexpected cost battery charge ce . Therefore, utility function representsexpected savings terms real money given battery charge (without taking costcharge account). Given this, unit size (in kWh), se , straight-forwardcalculate marginal valuation k th unit follows:vk = E(u(k se )) E(u((k 1) se ))(10)Recall that, model, assume valuations marginally non-increasing.show that, using approach, assumption automatically satisfied.so, need show Equation 9 non-decreasing (i.e., first derivative positive)concave (i.e., second derivative negative). first derivative given by:ZppdE(u(ce ))=eep(m)(11)dceepce eesecond derivative given by:ppd2 E(u(ce ))= ee 2 p(ce ee )dce 2ep(12)Clearly, conditions satisfied, means valuations always positivemarginally non-increasing. follows, describe derive experimentalsettings, supply function, arrival departure agents, traveldistance probability distributions real-world dataset. also provide examplesmarginal valuation vectors using data.7.3.2 CABLED Datasetbase experiments data gathered CABLED (Coventry BirminghamLow Emissions Demonstration) project,10 first large-scale endeavour10. See http://cabled.org.uk/.215fiRobu, Gerding, Stein, Parkes, Rogers & JenningsCarsFrequency15 %ArrivalsDepartures10 %5%0%:22:20:18:16:14:12:10:08:06:04:02:00000000000000000000000000TimeFigure 7: Distributions arrival departure times 56 EVs CABLED dataset(assigning equal weight EV).UK record study driving charging behaviours EV owners. partproject, 110 EVs loaned public equipped GPS data loggersrecord comprehensive usage information, trip durations distances, homecharging patterns energy consumption.data, focus period March June 2011,provided information 63 distinct vehicles, total 13,273 journeys.journey, includes times ignition turned again, totalmileage (as derived GPS readings taken every 60 seconds), well labelsstarting end location, available (such home work).vehicles CABLED trial charged various locations mostly home,also work. purpose experiments, assume chargingvehicles takes place single location. work focuses coordinating charging EVs within specific neighbourhood considering effectmultiple markets electricity beyond scope work. available,choose charging location one labeled home data.11 Given this,since interested arrival departure times charging location,well consumption patterns visits charging location, aggregateintermediate journeys departure charging location next returnlocation single journey.Aggregating data way discarding vehicles without clear charging location results 4,302 distinct journeys 56 different EVs, covering total distance travelledclose 72,500 miles. overall distribution recorded arrival departure times11. vehicles dataset lack this, used shared fleet vehicles organisationcases, use appropriate alternative location label, charging took place,discard vehicle suitable label identified.216fiAn Online Mechanism Multi-Unit DemandFrequencyEV 1EV 2ArrivalsDepartures30 %30 %20 %20 %10 %10 %0%0%FrequencyEV 430 %30 %20 %20 %10 %10 %0%0%0:0200:0160:0120:0080:0040:0000:0200:0160:0120:0080:0040:000EV 5Frequency0:0200:0160:0120:0080:0040:0000:0200:0160:0120:0080:0040:000EV 3EV 660 %40 %40 %20 %20 %0%0%TimeFigure 8: Example distributions arrival departure times six EVs.21700:0200:0160:0120:0080:004:00000:0200:0160:0120:0080:004:000TimefiRobu, Gerding, Stein, Parkes, Rogers & JenningsEmpirical Distribution Function (CDF)10.90.80.70.6CarsCar 1Car 2Car 3Car 4Car 5Car 60.50.40.30.20.100.010.11101001000Miles per JourneyFigure 9: Empirical distribution functions distances travelled successive visitshome charging locations. Six example EVs combined summary56 EVs, equal weights assigned cars (in red), shown.charging location shown Figure 7.12 Here, almost arrivals happen9am 11pm, clear peak late afternoon evening (4pm 9pm). Departures take place throughout day peak around 8am 10am. showindividual driving patterns vary recorded cars, Figure 8 details arrivaldeparture time distributions six individual EVs data set. reflectgeneral trends shown previous figure, arrivals generally occurringevening departures morning. However, EVs 2 3 deviatepattern. vehicles shared fleet vehicles, collectedreturned main charging location throughout day.Figure 9 shows distribution distances vehicles travelled visitshome charging location (red line), well corresponding functions sixEVs Figure 8 (interrupted lines).13 Overall, average distance travelled 41miles, median around 9 miles (assigning uniform probabilities car type).six sample EVs show significantly different typical travel distances, rangingaverage 4.19 miles (EV 1) 100 miles (EV 6).following, discuss use data CABLED project instantiateEV charging simulations.12. allocate electricity hourly units, arrival departure times roundednext full hour.13. Note distance journeys exceeds range typical electric cars.charged alternative locations CABLED trial, ignore experiments.Since focus PHEVs work, practice, shortfall would made combustionengine.218fiAn Online Mechanism Multi-Unit DemandUnits (3 kWh each)15High SupplyLow Supply10500:0000:0220:0200:0180:0160:0140:0120:0100:0080:0060:0040:0020:000TimeFigure 10: Number 3 kWh units available PHEV charging two scenariosvarying supply (high low ), based neighbourhood 30 households.7.3.3 Generating Experimentsexperimental run, simulate small neighbourhood 30 householdsvariable number PHEVs 24-hour period, starting 8:00 morning8:00 following day. assume electricity allocated hourly time steps,unit corresponds 3 kWh (which approximate energy obtainedstandard 13 BS 1363 household socket UK charging hour).obtain supply function S(t), first compute overall average electricityconsumption throughout neighbourhood (without PHEVs), based average consumption single UK household weekday high summer.14 Then, assumeoverall electricity supply limited capacity local transformer,electricity available PHEV charging, S(t), difference capacity(possibly including additional safety margin) current overall consumption.detail, consider two possible scenarios: (1) high supply scenario,capacity limit set covers 150% peak consumption (at 10:00 pm),resulting 615 kWh available PHEV charging; (2) low supply scenario,capacity limit 80% peak consumption, resulting 99 kWh available charging.15corresponding units available PHEV charging two scenarios shownFigure 10.Furthermore, use specific empirical distribution journey distances corresponding cars type cars travel distance distribution, p(m) (for example,sampled car based car 3, use dashed dark blue distribution function Figure 9). use Equations 9 10 derive marginal valuations. case,empirical distribution function discrete, integrals Equation 9 replacedsums data points. Furthermore, initially assume ri drawn uniformly14. used data available http://data.ukedc.rl.ac.uk/browse/edc/Electricity/LoadProfile/data.15. Local transformers often undersized way since prior PHEV use, could coolovernight periods low demand.219fiRobu, Gerding, Stein, Parkes, Rogers & Jenningsrandom discrete set ri {1, 2, 3, 4}, is, cars charge one fourunits electricity per hour (corresponding 3 12 kWh).generate agents variety marginal valuations, note ee ep dependspecific make type PHEV. simulate this, draw ee uniformlyrandom 2 4 miles/kWh ep drawn 9 18 miles/litre. Next, drawcapacity car battery 15 25 kWh. realistic values modelledChevrolet Volt, first mass-produced PHEV. However, include varianceaccount vehicle types. Throughout experiments, hold price petrolconstant pp = 1.30 per litre.Table 4 shows example valuations corresponding six cars considered previously (fixing ep = 13.5 miles/litre, ee = 3 miles/kWh battery capacity 20 kWh).highlight longer expected journeys generally translate higher marginal valuations, also variable valuations individual agent. example,car 4 values first 3 kWh electricity 0.67, seventh unit worth0.038, far less likely used.Car123456...vi,10.3400.3040.4810.6700.7270.839...vi,20.1360.1780.1570.4530.6200.797...vi,30.0010.1620.0730.3330.5820.767...vi,4vi,5vi,6vi,70.1140.0620.3120.5400.711...0.0330.0350.2630.4980.630...0.1340.4450.555...0.0380.4450.540...Table 4: Example marginal valuations (in ).set experimental run, randomly generate set N PHEV agents,vary N 1 60 simulate different levels demand.16 agent i,first choose one 56 available cars uniformly random CABLED dataset,base agents type on. Then, randomly select one cars recordedjourneys use time day cars arrival charging location ai (at8:00 time window consider). ensure correlation arrivaltimes subsequent departure times dataset preserved, use departuretime journey immediately following sampled journey di (or 10 hoursarrival, whichever sooner).7.3.4 PHEV ResultsFirst, interested general trends mechanisms whether similartrends discussed Section 7.2.2. end, Figure 11 shows allocative efficiencymechanisms setting low supply (where feasible run Optimal16. Note realistic number PHEVs within neighbourhood served single distributiontransformer (Huang & Infield, 2010).220fiAn Online Mechanism Multi-Unit DemandAllocative Efficiency (% Optimal)100%90%OptimalHeuristicGreedyODIMFixedRandom80%70%60%051015202530354045505560Number EVsFigure 11: Allocative efficiency small neighbourhood 30 households.IM mechanisms). demonstrates broad trends previoussynthetic setup OD IM mechanisms clearly dominate truthful mechanism(with IM achieving slightly worse results due higher cancellation rates). This, again,due ability mechanisms always allocate agents highestvaluations.However, although still consistently achieve around 90% Optimal, relativeperformance OD IM mechanisms slightly lower. drop performance,also witnessed Heuristic Greedy mechanism, due constrainedreal-world settings, electricity available abundance certain times (i.e.,night), agents significantly less patient others.settings, often pay delay patient agents, even higher valuations,favour less patient ones. Furthermore, valuations directly relatedfuel costs saved unit electricity, less variance real-world valuations,causing gap OD IM mechanisms truthful benchmarksnarrow slightly. Turning potential gains misreporting setting, Figure 12confirms patterns observed previously. magnitude gainsslightly higher due different setting (reaching 3% terms overall gains30% conditional case).key advantage applying mechanisms real-world data allows usdetermine actual fuel savings agents could achieve settings. Thus, Figure 13shows average fuel savings agent various mechanisms, or,words, average amount agent would spent fuel, allocatedelectricity. Initially, high (around 1.15), little competition, startsdropping PHEV owners compete amount electricity. key interesthorizontal separation different mechanisms. given fuel savingper agent, mechanism sustain significantly larger number agentstruthful mechanisms. example, save least 0.40 per agent, Random support221fiRobu, Gerding, Stein, Parkes, Rogers & JenningsPotential Gain Utility Misreporting35%Proportion GainsExpected Benefit GainExpected Benefit Overall30%25%20%15%10%5%0%51015202530354045505560Number EVsFigure 12: Potential gains misreporting Greedy mechanism small neighbourhood 30 households.Fuel Savings per Agent per Day (in )OptimalHeuristicGreedyODIMFixedRandom1.2510.750.50.25051015202530354045505560Number EVsFigure 13: Saving per agent per day small neighbourhood 30 households.40 PHEV owners, IM OD achieve threshold around 60 PHEVowners (an approximately 50% improvement).Finally, consider detail presence fast-charging vehicles affectsoverall neighbourhood, terms overall fuel savings, occurrence cancellationsutilities individual agents. end, fix number PHEVs 60222fiAn Online Mechanism Multi-Unit DemandSocial Welfare / Fuel Savings (per Day, )9030OptimalHeuristicGreedyODIMFixed PriceRandom8580257520706501020 30 40 50 60Number FastCharging EVs010 20 30 40 50 60Number FastCharging EVsFigure 14: Social welfare introducing fast-charging cars neighbourhoodlow supply (left) high supply (right).Units Cancelled per Day (% Total Allocated)30%30%IMOD20%20%10%10%0%0%010 20 30 40 50 60Number FastCharging EVs01020304050Number FastCharging EVs60Figure 15: Cancellations introducing fast-charging cars neighbourhoodlow supply (left) high supply (right).consider low high demand settings shown Figure 10. Duecomputational cost, test Optimal IM mechanisms settinglow supply. investigate impact fast-charging, assume two agent typesfirst, normal, charge single unit 3 kWh per time step, second,fast, equipped fast chargers charge four units per time step.223fiRobu, Gerding, Stein, Parkes, Rogers & JenningsUtility Per Agent (per Day, p = 0.01)1.2514p12p110pOD (Fast)OD (Normal)Greedy (Fast)Greedy (Normal)8p0.750.5010 20 30 40 50 60Number FastCharging EVs010 20 30 40 50 60Number FastCharging EVsFigure 16: Individual agent utility introducing fast-charging cars neighbourhood low supply (left) high supply (right).Throughout experiments, vary number fast-charging PHEVs (out total60). Figure 14 first shows resulting social welfare (i.e., overall fuel savings)supply scenarios low supply (left) high supply (right). First, notetrends two scenarios different supply low, introductionfast-charging vehicles little effect overall social welfare mechanisms,supply high, mechanisms display increased savings. happensfirst scenario highly constrained, low supply resulting occasionsagent could charge single unit per time step. contrast, supply high,agents often allocated multiple units, thus enabling impatient agents particularachieve higher overall fuel savings.addition this, interesting note proposed mechanisms OD IMbenefit settings (achieving additional fuel savings almost two litres per daylow supply setting, seven litres high supply setting). reasonbecomes evident considering proportion units cancelled fastcharging PHEVs introduced mechanisms settings, numberunits cancelled consistently reduced around 7080% cars replacedfast-charging PHEVs (shown Figure 15). occurs mainlyhtiactive marginal valuations time step populate pi vectors, thus reducingnumber cancellations. also causes gap mechanisms Greedymechanism shrink, fewer cancellations take place.respect utility individual agents (including payments mechanism),Figure 16 shows agents settings always incentive switch fast224fiAn Online Mechanism Multi-Unit DemandPotential Gain Utility Misreporting40%40%Proportion GainsConditional Utility IncreaseOverall Utility Increase35%35%30%30%25%25%20%20%15%15%10%10%5%5%0%0%01020 30 40 50 60Number FastCharging EVs010 20 30 40 50 60Number FastCharging EVsFigure 17: Potential gains misreporting Greedy mechanism neighbourhoodlow supply (left) high supply (right).charging PHEVs (e.g., purchasing domestic fast charger), appliesOD mechanism Greedy mechanism. low supply, expected daily savingswitching fast-charging PHEV approximately 0.020.03, high supply,around 0.200.25. cases, benefit result increasing availablesupply per time step, well increasing size price vector. Furthermore, evenentire population switch slow charging PHEVs fast-charging PHEVs,individuals would, average, achieve higher utility. Note differencesutility OD Greedy mechanisms significantly smaller fast-charging vehicles,indicating fast-charging agents expect lower gains misreportingcancellations.Figure 17 investigates individual gains misreportingcancellations. shows interesting trend initial gains high (reaching8% one setting), decrease significantly fast-charging PHEVs introduced(to around 0.2% setting). clearly due significant reductioncancellations witnessed settings. Furthermore, note, comparing ODGreedy Figure 16, agents gain misreporting tendslow-charging ones. Overall, promising result settings cancellationsfeasible increasing consumption rate PHEVs (within realistic parametersachievable current technological trends), scope potential benefitsstrategising simple greedy mechanism reduced significantly. particularexample, supply high, fast-charging PHEV expect gain less 1course entire year strategising optimally.225fiRobu, Gerding, Stein, Parkes, Rogers & Jennings8. Conclusionscontributions paper theoretical practical. theoreticalside, propose novel online, model-free mechanism perishable goods agentsmulti-unit demand non-increasing marginal valuations. show that, orderensure dominant strategy incentive compatibility setting, mechanism occasionally requires units remain unallocated (we say pre-allocation cancelled),even demand units. define two ways cancellationperformed: immediate, i.e., actual allocation, departure agentmarket. study properties two variants, terms incentivesallocative efficiency. Furthermore, present algorithms computing paymentsallocations mechanisms, analyse computational tractability.on-departure cancellation mechanism better computational tractability,worst-case competitive bound, terms allocation efficiency, single-unitdemand case. However, mechanism requires cancellations done departureagent market always feasible. nave approach computingpayments mechanism immediate cancellations requires time exponentialnumber agents. address this, proposed branch-and-bound algorithmallows payments computed immediate cancellation policy many realisticallysized settings. Another potential problem immediate cancellation policyworst-case bound terms efficiency allocation.practical side, show mechanism applied within smart gridsolve important problem integrating increasing number high-consumptionPHEVs electricity grid. addition synthetic setting, empirically evaluatemechanism using real-world data large-scale trial electric vehicles UK.show proposed mechanism highly robust, scalable (in particular, ondeparture variant) achieves better allocative efficiency fixed-price benchmark,slightly less efficient established cooperative scheduling heuristic.Specifically, demonstrate mechanism sustain 50% vehiclesfuel cost achieved using simple randomised mechanism. variantsalso consistently achieve efficiency around 90%, compared hypothetical optimaloffline solution. Given theoretical results regarding bounds, surprisingresult, suggesting specific conditions cause cancellations often occurpractice allocation policies perform well realistic settings. Furthermore,consider introduction fast chargers within neighbourhood, showleads significant increase overall fuel savings reducesoccurrence cancellations. Finally, since on-departure cancellation requires dischargingbattery PHEV, consider potential gains misreporting unitscancelled (and assuming full knowledge types agents).settings considered, average potential gain 1% 8% overall, couldgo 30% average considering cases misreporting beneficial,could even higher individual cases. gain becomes smaller numbercompeting agents increases and, interestingly, fast-charging PHEVs particularly lowincentives misreporting.226fiAn Online Mechanism Multi-Unit DemandTaken together, mechanisms represent versatile range tools, maysuitable specific scenarios others. example, medium-sized settingsallocations cannot cancelled departure, IM mechanism maysuitable (e.g., low-supply PHEV settings outlined Section 7.3). settingson-departure cancellation feasible, OD mechanism leads higher averageworst-case efficiency, also scalable. Here, also important emphasizeon-departure cancellation occurs users best interest thus,entirely possible achieve optional action. Finally, show results,even IM OD infeasible, mechanism without cancellations may stillviable settings, may even possible significantly reduce scopemanipulation adjusting system parameters; e.g., introducing fast chargersPHEV setting increasing supply.several directions extending work. related work, Stein et al. (2012)discuss alternative model, uses probabilistic information future arrivalsdesigned elicit truthful reporting pure EVs, rather PHEVs. model,however, requires knowledge future supply assumes single-minded bidders; i.e.,preferences single-dimensional possible specify different valuesdifferent amounts charge received. future work, intend explore mechanismscombine benefits approaches.addition, intend test mechanism using real-world trial. seen(see Section 7.3), design agent elicits information regarding intended use(a probability distribution driving distance), combines informationprice petrol efficiency vehicle, derive owners marginalvaluation vector. agent also participate mechanism owners behalf,avoiding need owner understand details mechanism.spirit work hidden market design (Seuken, Parkes, Horvitz, Jain, Czerwinski, &Tan, 2012), aim design user interface users cognitiveload reduced hiding details underlying market. trial, intendapproach participants owning regular (non EV) cars, install GPS trackers cars.Participants asked predict driving requirements, agentuse information, well learned historic driving patterns, derive users utilityfunction participate mechanism behalf. Although trialregular cars, users able see much would hypothetically savedproviding accurate (and truthful) information intended use.Acknowledgmentswork supported iDEaS (www.ideasproject.info) ORCHID (www.orchid.ac.uk )projects University Southampton. David Parkes supported partHarvard SEAS TomKat fund.ReferencesBabaioff, M., Blumrosen, L., & Roth, A. (2010). Auctions online supply. Proceedings11th ACM Conference Electronic Commerce (EC10), pp. 1322.227fiRobu, Gerding, Stein, Parkes, Rogers & JenningsBent, R., & Van Hentenryck, P. (2004). value consensus online stochastic scheduling. Proceedings 14th International Conference Automated PlanningScheduling (ICAPS04), pp. 219226.Bikhchandani, S., Chatterji, S., Lavi, R., Mualem, A., Nisan, N., & Sen, A. (2006). Weakmonotonicity characterizes deterministic dominant-strategy implementation. Econometrica, 74 (4), 11091132.Blum, M., Floyd, R. W., Pratt, V., Rivest, R. L., & Tarjan, R. E. (1973). Time boundsselection. Journal Computer System Sciences, 7 (4), 448 461.Clement, K., Haesen, E., & Driesen, J. (2009). Coordinated charging multiple plug-in hybrid electric vehicles residential distribution grids. Proceedings IEEE/PESPower Systems Conference Exposition (PSCE09), pp. 17.Constantin, F., Feldman, J., Muthukrishnan, S., & Pal, M. (2009). online mechanismad slot reservations cancellations. Proceedings ACM-SIAM SymposiumDiscrete Algorithms (SODA09), pp. 12651274.Constantin, F., & Parkes, D. C. (2009). Self-correcting sampling-based dynamic multi-unitauctions. Proceedings 10th ACM Conference Electronic Commerce (EC09),pp. 8998.Eberle, U., & von Helmolt, R. (2010). Sustainable transportation based electric vehicleconcepts: brief overview. Energy & Environmental Science, 3, 689699.Engel, Y., & Wellman, M. P. (2010). Multiattribute auctions based generalized additiveindependence. Journal Artificial Intelligence Research (JAIR), 37, 479525.Fairley, P. (2010). Speed bumps ahead electric-vehicle charging. IEEE Spectrum, 47 (1),1314.Gerding, E., Stein, S., Robu, V., Zhao, D., & Jennings, N. R. (2013). Two-sided onlinemarkets electric vehicle charging. Proceedings 12th International ConferneceAutonomous Agents Multiagent Systems (AAMAS13), pp. 989996.Gerding, E., Robu, V., Stein, S., Parkes, D., Rogers, A., & Jennings, N. (2011). Onlinemechanism design electric vehicle charging. Proceedings 10th InternationalConference Autonomous Agents Multi-Agent Systems (AAMAS11), pp. 811818.Gershkov, A., & Moldovanu, B. (2010). Efficient sequential assignment incompleteinformation. Games Economic Behavior, 68 (1), 144154.Hajiaghayi, M., Kleinberg, R., Mahdian, M., & Parkes, D. C. (2005). Online auctionsre-usable goods. Proceedings 6th ACM Conference Electronic Commerce(EC05), pp. 165174.Huang, S., & Infield, D. (2010). impact domestic plug-in hybrid electric vehiclespower distribution system loads. Proceedings International ConferencePower System Technology (POWERCON 2010), pp. 17.Kamboj, S., Kempton, W., & Decker, K. S. (2011). Deploying power grid-integrated electricvehicles multi-agent system. Proceedings 10th International ConferenceAutonomous Agents Multiagent Systems (AAMAS11), pp. 1320.228fiAn Online Mechanism Multi-Unit DemandLavi, R., & Nisan, N. (2004). Competitive analysis incentive compatible on-line auctions.Theoretical Computer Science, 310, 159180.Nisan, N., Roughgarden, T., Tardos, E., & Vazirani, V. (2007). Algorithmic Game Theory.Cambridge University Press.Parkes, D. C. (2007). Online mechanisms. Nisan, N., Roughgarden, T., Tardos, E., &Vazirani, V. (Eds.), Algorithmic Game Theory, pp. 411439.Parkes, D. C., & Duong, Q. (2007). ironing-based approach adaptive online mechanism design single-valued domains. Proceedings 22nd National ConferenceArtificial Intelligence (AAAI07), pp. 94101.Parkes, D. C., & Singh, S. (2003). MDP-based approach online mechanism design. Proceedings 17th Conference Neural Information Processing Systems(NIPS03), pp. 791798.Pinedo, M. (2008). Scheduling: Theory, Algorithms, Systems (3rd edition). Springer.Porter, R. (2004). Mechanism design online real-time scheduling. Proceedings5th ACM Conference Electronic Commerce (EC04), pp. 6170.Robu, V., Stein, S., Gerding, E., Parkes, D., Rogers, A., & Jennings, N. (2011). online mechanism multi-speed electric vehicle charging. Proceedings 2ndInternational Conference Auctions, Market Mechanisms Applications(AMMA11), pp. 100112.Robu, V., Kota, R., Chalkiadakis, G., Rogers, A., & Jennings, N. R. (2012). Cooperativevirtual power plant formation using scoring rules. Proceedings 22nd AAAIConference Artificial Intelligence (AAAI12).Robu, V., Noot, H., La Poutre, J. A., & van Schijndel, W. (2011). multi-agent platformauction-based allocation loads transportation logistics. Expert SystemsApplications, 38 (4), 34833491.Sandholm, T. (2002). Algorithm optimal winner determination combinatorial auctions. Artificial Intelligence, 135 (1-2), 154.Seuken, S., Parkes, D. C., Horvitz, E., Jain, K., Czerwinski, M., & Tan, D. (2012). Market user interface design. Proceedings 13th ACM Conference ElectronicCommerce, pp. 898915. ACM.Stein, S., Gerding, E., Robu, V., & Jennings, N. R. (2012). model-based online mechanismpre-commitment application electric vehicle charging. Proceedings11th International Conference Autonomous Agents Multiagent Systems(AAMAS12), pp. 669676.Stein, S., Gerding, E., Rogers, A., Larson, K., & Jennings, N. R. (2011). Algorithmsmechanisms procuring services uncertain durations using redundancy. Artificial Intelligence, 175, 20212060.Sundstrom, O., & Binding, C. (2012). Flexible charging optimization electric vehiclesconsidering distribution grid constraints. IEEE Transactions Smart Grid, 3 (1),2637.229fiRobu, Gerding, Stein, Parkes, Rogers & JenningsVasirani, M., & Ossowski, S. (2011). computational monetary market plug-in electricvehicle charging. Proceedings 2nd International Conference Auctions,Market Mechanisms Applications (AMMA11), pp. 8899.Vytelingum, P., Voice, T., Ramchurn, S. D., Rogers, A., & Jennings, N. R. (2011). Theoretical practical foundations large-scale agent-based micro-storage smartgrid. Journal Artificial Intelligence Research (JAIR), 42, 765813.230fiJournal Artificial Intelligence Research 48 (2013) 475-511Submitted 04/13; published 11/13Horn Clause Contraction FunctionsJames P. Delgrandejim@cs.sfu.caSchool Computing Science,Simon Fraser University,Burnaby, B.C., V5A 1S6CanadaRenata Wassermannrenata@ime.usp.brDept. Computer ScienceUniversity Sao Paulo05508-090 Sao Paulo,BrazilAbstractclassical, AGM-style belief change, assumed underlying logic containsclassical propositional logic. clearly limiting assumption, particularly ArtificialIntelligence. Consequently recent interest studying belief change approaches full expressivity classical propositional logic obtained.paper investigate belief contraction Horn knowledge bases. pointobvious extension Horn case, involving Horn remainder sets starting point,problematic. Horn remainder sets undesirable properties, alsodesirable Horn contraction functions captured approach. Horn belief setcontraction, develop account terms model-theoretic characterisation involvingweak remainder sets. Maxichoice partial meet Horn contraction specified,show problems arising earlier work resolved approaches.well, constructions specific operators sets postulates provided, representation results obtained. also examine Horn package contraction, contractionset formulas. Again, give construction postulate set, linking viarepresentation result. Last, investigate closely-related notion forgetting Hornclauses. work arguably interesting since Horn clauses found widespread useAI; well, results given may potentially extended areas makeuse Horn-like reasoning, logic programming, rule-based systems, descriptionlogics. Finally, since Horn reasoning weaker classical reasoning, work shedslight foundations belief change.1. Introductionarea belief change knowledge representation studies rational agent mayalter beliefs presence new information. best-known approach areaso-called AGM paradigm (Alchourron, Gardenfors, & Makinson, 1985; Gardenfors,1988), named original developers. work focused primarily two beliefchange operations, belief contraction, agent may reduce stock beliefs,belief revision, new information consistently incorporated agents beliefcorpus. fundamental assumption approach underlying logic governingc2013AI Access Foundation. rights reserved.fiDelgrande & Wassermannagents beliefs subsumes classical propositional logic. However, artificial intelligence(AI) major concern efficient, limited, ideally tractable reasoning. Hencesignificant effort studying limited reasoners, including Horn clause basedapproaches, limited epistemic reasoning involving explicit belief (Lakemeyer & Levesque,2000), description logics (Baader, Calvanese, McGuiness, Nardi, & Patel-Schneider,2007). Moreover, since knowledge base evolve, crucially important changeknowledge base managed principled fashion. However, AGM approachcannot used guide change approach, mentioned above,subsume classical propositional logic.paper address belief change expressively-weak language Horn clauses,Horn clause written rule form a1 a2 n 0,a, ai (1 n) atoms. (Thus, expressed conjunctive normal form,Horn clause one positive literal.) approach, agents beliefsrepresented Horn clause knowledge base, input conjunction Hornclauses. focus belief contraction (and, later, operators related contraction)agents stock beliefs decreases.topic Horn clause contraction (and general topic Horn belief changegeneral) interesting several reasons. First, Horn clause reasoners constitute important class AI systems, Horn clauses found extensive use artificial intelligencedatabase theory, areas logic programming, truth maintenance systems,deductive databases. Horn clause belief change also sheds light theoretical underpinnings belief change, weakens assumption underlying logic containspropositional logic. Hence results obtained may relevant belief changeareas limited reasoning. example, approaches explicit belief often derives muchinspiration relevance logic (Anderson & Belnap Jr., 1975); description logics,constituting fragments classical first-order logic, nonetheless many casessupport full propositional reasoning.1Creignou, Papini, Pichler, Woltran (2012) provide motivation studybelief change tractable fragments propositional logic:many applications, language restricted priori. instance, rulebased formalization expert knowledge much easier handle standardusers. case users want revise rules, indeed expectoutcome still easy-to-read format used to. Many fragmentspropositional logic allow efficient reasoning methods. Suppose agentfrequently answer queries beliefs. doneefficiently thus beliefs stored formula known tractableclass. case beliefs agent undergoing revision, desiredresult operation yields formula fragment. Hence,agent still use dedicated solving method equippedfragment. case changes performed rarely, bother whetherrevision performed efficiently, importantoutcome still evaluated efficiently.1. fact, Booth, Meyer, Varzinczak (2009) point out, results also relevant belief changedescription logics, topic also elicited recent interest.476fiHorn Clause Contraction FunctionsHorn clause contraction become topic interest belief change recent years(Delgrande, 2008; Delgrande & Wassermann, 2010, 2011; Booth et al., 2009; Booth, Meyer,Varzinczak, & Wassermann, 2011; Zhuang & Pagnucco, 2010a, 2011, 2012). discussnext section, work centers notion remainder set, maximalsubset knowledge base fails imply given formula. show remaindersets Horn case restricted cannot give feasible contraction operators.well yield contraction operators undesirable properties.propose notion weak remainder set serves basis generatingHorn maxichoice contraction operators. Contraction also considered termsunderlying model theory, viewpoint proves highly enlightening studying Hornbelief change. Given specification maxichoice contraction based weak remainders,go develop specification partial meet Horn contractions, consider packagecontraction forgetting. contraction operators developed, provide postulatesets along constructions, show representation results. Consequently presentcomprehensive exploration landscape Horn contraction.next section introduces belief change following section discusses reasoningHorn clause theories. main approach presented Section 4, Section 5discusses considerations pertaining supplementary contraction postulates. Section 6covers related operators package contraction forgetting Horn theories.paper concludes discussion concluding section. Proofs given appendix.material presented previously Delgrande (2008) DelgrandeWassermann (2010, 2011).2. Backgroundsection, introduce main concepts area Belief Changeneed throughout paper.2.1 Belief Changepreviously mentioned, AGM approach (Alchourron et al., 1985; Gardenfors, 1988)original best-known approach belief change.2 goal approachdescribe belief change knowledge level, is, abstract level independentbeliefs represented manipulated. Belief states modelled sets sentences,called belief sets, closed logical consequence operator logic includesclassical propositional logic language L. Thus belief set K satisfies constraint:K logically entails K.central operators3 addressed contraction, agent reduces set beliefs,revision, agent consistently incorporates new belief. revision, sincenew belief may inconsistent agents beliefs, beliefs may need droppedorder maintain consistent set beliefs. third operator, belief expansion also2. well, Peppas (2008) provides excellent survey.3. paper, use terms operator f unction interchangeably refering belief changeoperations.477fiDelgrande & Wassermannintroduced: belief set K formula , expansion K , denoted K + ,deductive closure K {}. Expansion captures simplest form belief change;reasonably applied new information consistent belief setoperators characterised two means. one hand, set rationalitypostulates belief change function may provided; postulates stipulate constraints govern rational belief change function. hand, specificconstructions belief change function given. Representation results provided, showing set rationality postulates exactly captures operator givenparticular construction.review notions belief contraction. Informally, contraction belief setformula belief set formula believed. Formally, contractionfunction function 2L L 2L satisfying following postulates.(K 1) K belief set.(K 2) K K.(K 3) 6 K, K = K.(K 4) 6` , 6 K .(K 5) K, K (K ) + .(K 6) , K = K .(K 7) K K K ( ).(K 8) 6 K ( ) K ( ) K .Thus, contraction yields belief set (K 1) sentence contractionbelieved (unless tautology) (K 4). new sentences believed (K 2),formula originally believed contraction effect (K 3). fifthpostulate, so-called recovery postulate, states nothing lost one contractsexpands sentence. postulate controversial, discussed, exampleHansson (1999). sixth postulate asserts contraction independentsentence syntactically expressed. last two postulates express relationscontracting conjunctions contracting constituent conjuncts. Hence (K 7)says formula result contracting two formulasresult contracting conjunction. (K 8) says conjunctresult contracting conjunction, then, presence (K 7), contractingconjunct contracting conjunction. first six postulates referredbasic postulates last two referred supplementary postulates.Revision represents situation new information may inconsistentreasoners beliefs K, needs incorporated consistent manner, oneexception formula revision inconsistent. revision functionfunction 2L L 2L satisfying set postulates analogouscontraction. Contraction usually taken fundamental operator belief478fiHorn Clause Contraction Functionschange. Moreover, revision contraction interdefinable. Revision definedterms contraction means Levi Identity:K = (K ) + .(1)Thus, revise , make K consistent expand . Contractionsimilarly defined terms revision Harper identity:K = K (K ).Since consider revision functions paper, refer reader workGardenfors (1988) Peppas (2008) details.Various constructions proposed characterise belief change. originalconstruction terms remainder sets, -remainder K maximal subsetK fails imply . Formally:Definition 1 Let K L let L.K set sets formulas s.t. K 0 K iff1. K 0 K2. K 0 6`3. K 00 s.t. K 0 K 00 K, holds K 00 ` .K 0 K -remainder K.Thus K class maximal -nonimplying subsets K.ambiguity, also refer K 0 K simply remainder K.Two classes contraction functions relevant concerns. maxichoice contraction, contraction defined correspond single selected remainder. partial meetcontraction, contraction corresponds intersection subset remainders.Consequently, maxichoice contraction partial meet contraction vice versa.logical point view, -remainders comprise equally-good candidatescontraction K. Selection functions introduced reflect extra-logicalfactors need taken account, obtain best plausible remainders.maxichoice contraction, selection function determines single selected remaindercontraction. partial meet contraction, selection function returns subsetremainders, intersection constitutes contraction. Thus selectionfunction denoted (), contraction K formula expressed\K =(K ).(2)belief set K function 2L L 2L , proves casepartial meet contraction function iff satisfies basic contraction postulates (K 1)(K 6). Last, let transitive relation 2K , let selection function definedby:(K ) = {K 0 K | K 00 K , K 00 K 0 }.transitively relational selection function, defined termstransitively relational partial meet contraction function. have:479fiDelgrande & WassermannTheorem 1 (Alchourron et al., 1985) Let K belief set let function2L L 2L .1. partial meet contraction function iff satisfies contraction postulates(K 1)(K 6).2. transitively relational partial meet contraction function iff satisfies contraction postulates (K 1)(K 8).second major construction contraction functions called epistemic entrenchment. general idea extra-logic factors related contraction givenordering formulas agents belief set, reflecting willing agent wouldgive formula. contraction function defined terms removing lessentrenched formulas belief set. Gardenfors Makinson (1988) showlogics including classical propositional logic, two types constructions, selection functions remainder sets epistemic entrenchment orderings, capture classcontraction functions.Two constructions also proposed literature shown equivalenttransitively relational partial meet contraction: safe contraction (Alchourron & Makinson, 1985; Rott, 1992) systems spheres (Grove, 1988). address eitherconstruction paper.2.2 Belief Change Horn Clause TheoriesEarlier work belief change Horn theories focussed specific aspects problem,rather general characterisation Horn clause belief change. example,complexity specific approaches revising knowledge bases addressed EiterGottlob (1992). includes case knowledge base formularevision conjunctions Horn clauses, although results revision may Horn.unexpectedly, results generally better Horn case. Liberatore (2000) considersproblem compact representation revision Horn case. Given knowledgebase K formula , Horn, main problem addressed whether knowledgebase, revised according given operator, expressed propositional formulawhose size polynomial respect sizes K .Langlois, Sloan, Szorenyi, Turan (2008) approach study revising Horn formulas characterising existence complement Horn consequence; complement corresponds result contraction operator. work may seen specificinstance general framework developed Flouris, Plexousakis Antoniou (2004).study belief change broad notion logic. particular, give criterionexistence contraction operator satisfying basic AGM postulates termsdecomposability.present paper builds extends (Delgrande, 2008; Delgrande & Wassermann,2010, 2011). Delgrande (2008) addresses maxichoice belief contraction Horn clause theories, contraction defined terms remainder sets, using Definition 1, expressed terms derivations among Horn clauses. Booth, Meyer, Varzinczak (2009)Booth, Meyer, Varzinczak, Wassermann (2011) develop area,480fiHorn Clause Contraction Functionsconsidering versions contraction, based remainder sets: partial meet contraction, generalisation partial meet, package contraction. Horn contraction basedremainders found inadequate Delgrande Wassermann (2010), insteaddeveloped notion weak remainder. work Zhuang Pagnucco (2010a,2012) follows another line, focusing epistemic entrenchment model-based constructions. approaches discussed compared detail introducedoverall approach.Recently, revision operations Horn theories also developed (Delgrande &Peppas, 2011), revision fragments propositional logic also explored(Creignou et al., 2012). However relation work contraction operationsdescribed paper still unclear.3. Horn Clause Theoriesdeal languages based finite sets atoms, propositional letters P ={a, b, c, . . . }, P includes distinguished atom . L language propositionallogic P usual connectives , , , .4 LHC restriction LHorn formulas, Horn formula finite conjunction Horn clauses. LHCleast set given by:1. a1 a2 a, n 0, a, ai (1 n) atoms, Horn clause.2. Every Horn clause Horn formula.3. Horn formulas .well, convenience, > taken denoting specific atom a.Horn clause r 1 above, n = 0 r fact, also written a.Horn clause r 1 above, head(r) a, body(r) set {a1 , a2 , . . . , }.fact, head(r) a, body(r) empty. Horn clause rhead(r) = , r integrity constraint. Allowing conjunctions clauses, given3, adds nothing interest expressibility language respect reasoning.However, adds expressibility contraction, able contractsingle Horn clause.Semantics: interpretation L function P {true, f alse}assigned f alse. Sentences L true false interpretation accordingstandard rules propositional logic. interpretation model sentence (or setsentences), written |= , true . od() set models formula(or set formulas) ; thus od(>) set interpretations L. interpretationusually identified atoms true interpretation. Thus, language givenP = {p, q, r, s}, interpretation expressed {p, q} p q truer false. convenience, also express interpretations juxtapositionatoms. Thus set interpretations {{p, q}, {p}, {}} usually written {pq, p, }.notions inherited corresponding Horn formula language LHC .key point concerning Horn theories theories characterised semantically4. avoid clutter, ambiguity results, dont parameterize L P.481fiDelgrande & Wassermannfact models closed intersections positive atoms interpretation. is, Horn theory H satisfies constraint:M1 , M2 od(H) M1 M2 od(H).leads notion characteristic models (Khardon, 1995) Horn formulaset formulas: characteristic model formula every M1 , M2 od(),M1 M2 = implies = M1 = M2 . Thus example, {p q , r}models {pr, qr, r} characteristic models {pr, qr}. Since pr qr = r, r isntcharacteristic model .Proof Theory: assume suitable inference relation ` classical propositional logic.following axioms rules give inference relation Horn formulas,simplicity, b, possibly subscripted, taken ranging atoms.Axioms:aaRules:1. a1 b1 bn a1infer b1 bn a22. a1 infer a1 b3. Horn clauses r1 , r2 , body(r1 ) = body(r2 ) head (r1 ) = head (r2 )r1 infer r2 .4. (a)(b)inferinferRule 1 extended version modus ponens, Rule 2 strengthening antecedent. Rule 3 states order atoms body Horn clause irrelevant,repeated atoms.formula derived set formulas A, written `HC ,obtained finite number applications rules axioms;simplicity drop subscript write ` . = {} singleton setwrite ` . set formulas LHC inconsistent ` . userepresent logical equivalence, ` ` .Notation: collect reference notation used paper. Lower-caseGreek characters , , . . ., possibly subscripted, denote arbitrary formulas either LLHC . Upper case Roman characters A, B, . . . , possibly subscripted, denote arbitrary setsformulas. H, H1 , H 0 , etc. denote Horn belief sets, H iff H `HC .Cn(A) (classical, propositional) deductive closure formulaset formulas propositional logic. Cnh (A) deductive closure Horn formulaset formulas Horn derivability. set formulas A, Horn(A) = { |Horn formula}.use (possibly subscripted) denote maximal consistent Horn theory;is, 6` every Horn formula , either {} ` . Hence482fiHorn Clause Contraction Functionsexactly one model. often use maximal consistent sets formulas placeinterpretations, makes statement proof various results easier. || setmaximal, consistent Horn theories contain .(M1 , 0 , etc.) denote (classical, propositional) interpretations understood language. od(A) set models A. Arbitrary sets interpretationsdenoted (M0 etc.). Cl (M) intersection closure set interpretations M;is, Cl (M) least set interpretations1. Cl (M)2. M1 , M2 Cl (M) implies M1 M2 Cl (M).Note denotes interpretation expressed set atoms, denotesmaximal consistent set Horn formulas. Thus logical content same,interpretation defines maximal consistent set Horn formulas, vice versa.retain two interdefinable notations, since useful subsequent development.Similar comments apply od() vs. ||; also make use fact 1-1correspondence elements || od().Last, since P finite, (Horn propositional logic) belief set may finitely represented, is, X belief set, formula Cn() = X.4. Horn Clause Belief Set Contractionsection, examine possible constructions operation contractionHorn belief sets. begin operations based remainde sets proceed introducingconcept weak remainder set.4.1 Horn Clause Contraction Remainder Setsstraightforward way define Horn contraction function adapting construction used classical logic contraction. end, Delgrande (2008) developedremainder-set approach Horn contraction, subsequently generalised Booth,Meyer Varzinczak (2009). proves case approaches sufficiently expressive general Horn contraction; well, contraction based remaindersets shown undesirable properties. review pertinent aspectsapproaches here, particular consider results (classical, AGM) contractionreadily extend Horn case.definition remainder sets Horn clause belief sets (called e-remainder setsDelgrande, 2008) remainder set (Definition 1) respectHorn clauses Horn derivability. H Horn belief set LHC , sete-remainders respect H denoted H e .Definition 2 Let H LHC let LHC .H e set sets formulas H 0 H e iff1. H 0 H2. H 0 6`483fiDelgrande & Wassermann3. H 00 H 0 H 00 H holds H 00 ` .H 0 H e -e-remainder respect H.Usually H 0 referred simply remainder, since Horn contextunderlying formula clear.Observation 1 H e 1 = H e 2 , H 0 H, 1 Cnh (H 0 ) iff 2Cnh (H 0 ).Observation 2 (Upper bound property) X H 6 Cnh (X),X 0 X X 0 H e .Horn remainders given Definition 2 regarded comprising set candidatecontractions H formula ; single remainder could selectedmaxichoice contraction H . Booth, Meyer, Varzinczak (2009) subsequently arguemaxichoice contraction sufficient account Horn contraction functions.classical AGM contraction, set partial meet contraction functions definedtaking intersection remainders. However, Booth, Meyer, Varzinczakalso argue set Horn partial meet contractions sufficient capture fullrange possible contraction functions. Instead define infra remainder sets, follows:Definition 3 belief sets H X, X H e 5 iff X 0 H e\H e X X 0 .elements H e infra e-remainder sets H respect .Thus infra e-remainder set belief set contains intersection Horn remainders, contained Horn remainder. e-remainder sets clearly infrae-remainder sets, intersection set e-remainder sets. is:Observation 3 Let H LHC , LHC , let X H e . ( X) H e .Example 1 P = {a, b, c}, let H = Cnh (a b).Consider candidates H (a b).verified three remainder sets:Cnh (a (c b)),Cnh (b (c a)),Cnh ((ab) (b a) (c a) (c b)).well, remainder set infra remainder set must contain closure(c a) (c b).5. Booth, Meyer, Varzinczak (2009) write X H e set Horn clauses.484fiHorn Clause Contraction Functionssee last part example, note (c a) (c b) remainders,intersection remainders. however leads significant blemish. Callp inessential H conjunction atoms body containing p, H ` p bodyimplies either ` p body H ` body a. contraction defined termsremainder sets, intersections remainder sets, infra remainder sets,result:6Theorem 2 Let Horn contraction function defined via selection function(2) based (infra) remainder sets.H p inessential H, obtain (H ) + p ` .following example (based example Hansson, 1999) illustrates problem:1. believe Cleopatra son daughter (s d).2. learn source information unreliable, remove belief; i.e.compute contraction H (s d).3. learn raining outside (r).4. conclude Cleopatra son daughter (s d)behaviour clearly undesirable. However, consider example impliesHorn contraction point: H (s d) + r entails d. Hence,regardless defined terms (infra) remainders, models H (s d)r true must also true. turn means sdrcannot model d-remainder. last point curious, sdr clearlycounter-model d, yet take part remainder, takepart contraction.AGM contraction, -remainder belief set K characterized set models K together single countermodel , vice versa(e.g., see Gardenfors, 1988, p. 86). example shows equivalenceproof-theoretic notion remainders semantic notion minimallyextended sets models breaks Horn case.So, consider going Horn case: Assume H |= wish findmaximal belief set H 0 H 0 H H 0 6|= . is, H 0 -remainder setH. described, classical AGM (maxichoice) contraction, semantic side oneadds countermodel models H; set models characterises candidatetheory maxichoice contraction.Consider analogous process Horn theories. Since remainder set mustHorn theory, models Horn theory closed intersection, wouldneed make sure constraint holds here. So, intuitively, carry maxichoiceHorn contraction, would add countermodel formula contraction, closeresult intersection. However, theories resulting approachcorrespond obtained via remainder sets. see this, consider Example 1,pertinent results summarised Figure 1.6. result would also obtained package contraction, discussed Section 6, package contractiondefined terms infra remainder sets.485fiDelgrande & Wassermanncountermodelacbcbcinducedmodelsbresulting KBremainderset?(c b)bb (c a)(a b) (b a)(a b) (b a) (c a) (c b)Figure 1: Example: Candidates Horn contractionac (viz. abc) countermodel = ab; given first entryfirst row table. Since H model ab, intersection models,ab ac = must also included; item second column. resultingbelief set characterised interpretations od(H) {ac, a} = {abc, ab, ac, a},set models formula a, given third column. result isnt remainderset, since Cnh (a (c b)) logically stronger belief set fails imply b. lastbelief set, Cnh (a (c b)) appears second row table. observedmodels belief set made models H together countermodela, is, induced model first row.previously noted, three remainder sets, indicated last column.discussed, result problematic approaches Delgrande (2008)Booth, Meyer, Varzinczak (2009). example, none approachespapers possible obtain H e (a b) a, possible obtain H e (a b)((a b) (b a)). possibilities would desirable potential contractions.diagnosis problem presumably clear. example,countermodel given abc, possible set interpretations satisfying:1. closed intersections2. = od(H) abcsolution also seems clear: semantic point view, one wants characteristicmodels maxichoice candidates H e consist characteristic models Htogether single interpretation od(>) \ od(). resulting theories, calledweak remainder sets, would correspond theories given third column Figure 1;explore notion next subsection.conclude note shown (maxichoice) contraction basedremainder sets alone suffers triviality result analogous AGM contraction.Theorem 3 (Makinson, 2009) Let P atom, let H Horn belief setH. Let maxichoice Horn contraction function based remainder sets.every atom b, least one b b H (a ) + a.Hence false according H, contracting expandingyields belief set every atom believed true believed false.clearly far unrealistic useful.486fiHorn Clause Contraction Functions4.2 Horn Clause Contraction Weak Remainder Setsprevious section showed basing Horn contractions solely remainder sets (or infraremainder sets) problematic. suggested adequate version contractionbased weak remainder sets belief set H formula H,1-1 correspondence countermodels weak remainder sets. sectiondevelop Horn contraction based weak remainder sets. first give two constructionsweak remainder sets, terms belief sets terms sets models, showconstructions equivalent. characterise maxichoice Horn contractionterms weak remainder sets, showing via representation result characterisationsequivalent. Following similarly characterise partial meet contraction.Definition 4 Let H Horn belief set, let Horn formula.set sets formulas1. H then: H 0 iff H 0 = H |>| \ ||.2. Otherwise = {H}.H 0 weak remainder set H .Observation 4 H 0 , H 0 belief set, i.e., H 0 = Cnh (H 0 ).definition, maximal consistent set formulas, correspondsset formulas true interpretation. case underlying interpretationwould belong od(>)\M od(), say underlying interpretation wouldcountermodel .Example 2 P = {a, b, c}, let H = Cnh (a b) = b.m1 = Cnh (a b c) |>| \ ||, H m1 = Cnh (a (c b)).m2 = Cnh (a b c) |>| \ ||, H m2 = Cnh (a).Note (H m2 ) (H m1 ), also full propositional closure gives Cn(Hm2 ) = Cn(a (b c)).previous definition specifies weak remainder sets terms maximal consistentsets formulas. next definition similar, expressed directly terms countermodels formula.Definition 5 Let H Horn belief set, let Horn formula. Define H ||e by:1. H then: H ||e set sets formulas H 0 H ||e iff6 od() od(H 0 ) = Cl (M od(H) {M }).2. Otherwise H ||e = {H}.running example, H ||e given closure formulas column 3Figure 1.Perhaps surprisingly, two characterisations prove equivalent:487fiDelgrande & WassermannTheorem 4 H Horn belief set Horn formula:= H ||e .position define Horn contraction operator. start definingselection function, basically done AGM approach. Given selection function,straightforward define maxichoice contraction operator, following this, partialmeet contraction operator.Definition 6 Let H Horn belief set. selection function H if, everyLHC ,1. 6= =6 (He ) .2. = (He ) = {H}.Definition 7 Let selection function H (He ) = {H 0 }H 0 .maxichoice Horn contraction based weak remainders given by:H w = (He )Hence result maxichoice contraction characterised single weak remainderset.obtain following representation result, relating construction postulateset characterising contraction:Theorem 5 Let H Horn belief set. w operator maxichoice Horncontraction based weak remainders iff w satisfies following postulates.(H w 1) H w Horn belief set.(closure)(H w 2) ` , 6 H w .(success)(H w 3) H w H.(inclusion)(H w 4) 6 H, H w = H.(vacuity)(H w 5) ` H w = H(failure)(H w 6) , H w = H w .(extensionality)(H w 7) H 6= H w LHC {, } ` , H w Cnh () H 0s.t H w H 0 H H 06 Cnh ().(maximality)first four postulates (H w 6) obvious counterparts AGM contractionpostulates. Notably, obtain recovery postulate. following providescounterexample.488fiHorn Clause Contraction FunctionsExample 3 Let H = Cn(p q) = p r q.H w 6` p q, since p q ` p r q.Thus H w Cn({p r q | P \ {p, r}}.)H w + Cn({p r q | P \ {p, r}) + 6` p qhence H w + 6` p q.Postulate (H w 5) derivable using AGM postulates, relies recovery postulate (K 5) proof. Since lack recovery postulate, requiredpostulate, covering special case, right.Postulate (H w 7) complicated others, expresses basic defining characteristic maxichoice revision: contraction nontrivial (viz. H 6= H w ),countermodel model H w . expressed ,mutually inconsistent, H w subset closure , H w maximalset formulas holds. turn means that, even though recoverypostulate hold, nonetheless trivial contraction, entire belief setdiscarded, excluded legal contraction operator. verified Example 1 (see also Figure 1) countermodels abc abc fulfill conditions(H w 7), say, postulate captures notion weak remainder set.turn next partial meet Horn contraction. definition partial meet Horncontraction analogous AGM contraction, based weak remainder sets:Definition 8 Let selection function H (He ) (He ).partial meet Horn contraction based weak remainders given by:H pm =\(He )representation result involves modification last postulate maxichoice contraction:Theorem 6 Let H Horn belief set. pm operator partial meet Horncontraction based weak remainders iff pm satisfies postulates (H w 1) (H w 6)and:(H pm 7) H \(H pm ), H 0 H pm H 0 , 6 Cnh (H 0 )Cnh (H 0 {})(weak relevance)Example 4 running example, partial meet given first last weakremainder sets Figure 1 givenCnh ((b a) (c a)).terms models, characterised models b, together two countermodels given atoms ac , closed intersections.489fiDelgrande & Wassermann5. Supplementary Postulatessection investigate different proposals Horn contraction operationsbehave respect supplementary postulates (K 7) (K 8). Throughoutsection, assume selection functions transitively relational.First consider operation Horn partial meet e-contraction (Delgrande, 2008).following example shows that, considering e defined Delgrande (see also Definition 2), Horn partial meet e-contraction satisfy (K 7):Example 5 Let H = Cnh ({a b, b c, d, c}).H e (a c) = {H1 , H2 , H3 , H4 }H e (b c) = {H5 }where:H1H2H3H4H5= Cnh ({a b, d}),= Cnh ({a b, c d, c}),= Cnh ({b c, c b, d}),= Cnh ({a c b, b c, c d, c, b, b d}),= Cnh ({a b, d, c})Note two first elements H e (a c) subsets single elementH e (b c) hence, cannot belong H e (a c b c).H e (a c b c) = {H3 , H4 , H5 }take selection function based transitive relation remainder setsgives priority order appear example, i.e., H5 H4 H3H2 H1 , have:H (a c) = H1H (b c) = H5H (a c b c) = H3seeH (a c) H (b c) = H1 6 H3 = H (a c b c)example shows operation satisfy (K 8):c 6 H (a c b c) H (a c b c) 6 H (a c).restrictions selection function, example alsoshows contraction based infra-remainders satisfy supplementary postulates. Note remainder set example also infra-remainderselection function always selects single element. suffices assign remaininginfra-remainders lower priority.show operation partial meet based weak remainders (PMWR)better behaviour respect supplementary postulates:490fiHorn Clause Contraction FunctionsTheorem 7 Partial meet based weak remainders transitive relational selectionfunction satisfies (K 7) (K 8).recently, Zhuang Pagnucco (2010a) addressed Horn contractionpoint view epistemic entrenchment. compare AGM contraction via epistemicentrenchment classical propositional logic contraction Horn logics. postulateset provided shown characterise entrenchment-based Horn contraction. factAGM contraction allows disjunctions formulas, general Horn,handled considering Horn strengthenings postulate set, say, logicallyweakest Horn formulas subsume disjunction. contrast earlier work,postulate set includes equivalents supplemental postulates, goes beyondset basic postulates. detail, Zhuang Pagnucco (2010a) following:Definition 9 given clause , set Horn strengthenings ()H set()H Horn clause Horn clause 00 .ten postulates given Zhuang Pagnucco (2010a) characterize epistemicentrenchment Horn contraction (EEHC), postulates (H 1), (H 2), (H 4), (H 6), (H 7)(H 8) correspond exactly AGM postulates numbers. (H 1),(H 2), (H 3), (H 4) (H 6) correspond postulates (H w 1)-(H w 6) characterizing partial meet contraction based weak remainders defined. three new postulates are:(H 5) H H(H 9) H \ H ( )H , 6 H(H 10) ( )H , 6 H 6 H \ HSubsequently, Zhuang Pagnucco (2010b) shown transitively relationalPMWR defined general EEHC. means operationsatisfying set 10 postulates (which include (K 7) (K 8)) PMWR.seen PMWR satisfies (K 7) (K 8), hence, order compare PMWREEHC, need know whether PMWR satisfies (H 5), (H 9) (H 10).Theorem 8 PMWR satisfies (H 5).Zhuang (2012) shown weak relevance implies (H 9), hence, PMWR satisfies(H 9). PMWR general satisfy (H 10), following example shows:Example 6 Let H = Cnh ({a, b}).= {H1 , H3 }(a b) = {H1 , H2 , H3 },491fiDelgrande & WassermannH1 = Cnh ({b a, b}),H2 = Cnh ({a})H3 = Cnh ({b}).Assuming selection function based transitive relation H1 H2H1 H3 (and H2 H3 H3 H2 ),H = H3 H (a b) = H2 H3Since (ab)H = {a, b}, (ab)H , 6 H (ab), b H a.6. Operatorssection consider two contraction-like operators. first, package contraction,like contraction, defined respect set formulas. second operator,forget, regarded removal atom set atoms languagediscourse.6.1 Package ContractionAGM-style belief change propositional logic, given belief set K set formulas, package contraction K p form contraction (non-tautological)member K p . propositional logic effect package contraction maynearly, quite, obtained contracting disjunction elements . seedifference, consider = {, }. Clearly, 6 K ( ) whereas seemssimultaneous contraction K p {, } allow possibilitytrue outcome.Booth, Meyer, Varzinczak (2009) note, package contraction interest Hornclause theories, given limited expressivity theories. is, , Hornformulas, H ( ) undefined whenever non-Horn (which, course,time). hand, expressing contractionH p {, } seems perfectly fine.development Horn package contraction analogous maxichoice Horncontraction based weak remainders. Essentially, package contraction H p ,ensure countermodel among models H p .Definition 10 Let H Horn belief set, let = {1 , . . . , n } finite7 set Hornformulas.Hp set sets formulas H 0 Hp iffevery 1 n, mi that:6` H mi |>| \ |i |; otherwise mi = LHC ;H 0 = H ni=1 mi .next definition, notion selection function H (Definition 6) extendedobvious fashion apply set Horn formulas.7. Since assume underlying language finite, set formulas equivalent finiteset formulas, logical equivalence formulas.492fiHorn Clause Contraction FunctionsDefinition 11 Let selection function H (Hp ) = {H 0 }H 0 Hp .(maxichoice) package Horn contraction based weak remainders given by:H p = (Hp )=6 H 6 Cnh (>); H otherwise.following result relates elements Hp weak remainders.Theorem 9 Let H Horn belief set let = {1 , . . . , n } set Horn formulas1 n 6` .H 0 Hp iff 1 n Hi H 0 = ni=1 Hi .follows immediately maxichoice Horn contraction defines packagecontraction, vice versa.Corollary 1 Let p operator maxichoice Horn package contraction.H = H p= {}operator maxichoice Horn contraction based weak remainders.Corollary 2 Let operator maxichoice Horn contraction based weak remainders.\H p =Hoperator maxichoice Horn package contraction.Example 7 Consider Horn belief set H = Cnh ({a, b}) P = {a, b, c}. wantdetermine elementsHp = Cnh ({a, b})p {a, b}.total 14 elements Hp 14 candidate package contractions.candidates described follows:1. 4 countermodels a, given by:= {bc, b, c, }.Thus four weak remainders corresponding countermodels,four candidates maxichoice Horn contraction a.2. Similarly 4 countermodels b:B = {ac, a, c, }.493fiDelgrande & Wassermann3. Members Hp givenCl (M od(H) {x} {y})x B.example, x = bc, = , Cl (M od(H) {x} {y}) = {abc, ab, bc, b, },set models (c b) (a b).x = bc, = ac, Cl (M od(H) {x} {y}) = Cnh (>); holdschoice x y.example indicates informally great deal choice respectcandidates package contraction. extent, combinatorial explosionpossibilities expected, given fact formula general largenumber countermodels, compounded fact formulapackage contraction associated countermodel. However, alsonoted candidate package contractions contain redundancies, selectedcountermodel may also countermodel b, case seemsreason allow possible incorporation separate countermodel b. Consequently,also consider versions package contraction sense yield maximal beliefset. However, first provide results regarding package contraction.following result:Theorem 10 Let H Horn belief set. p operator maxichoice Hornpackage contraction based weak remainders iff p satisfies following postulates:(H p 1) H p belief set.(closure)(H p 2) , ` , 6 H p(success)(H p 3) H p H(inclusion)(H p 4) H p = H p (H )(vacuity)(H p 5) H p = H p ( \ Cnh (>))(failure)(H p 5b) H p = H(triviality)(H p 6) ,H p ( {}) = H p ( {})(extensionality)(H p 7) H 6= H p0 = ( \ Cnh (>)) H = {1 , . . . , n }= {1 , . . . , n } 1 n,{i , } ` H p Cnh (i )H 0 s.t H p H 0 H, H 0 6 Cnh ()494(maximality)fiHorn Clause Contraction Functionsexception last postulate, postulates clear reasonable:usual, result package contraction belief set (H p 1). Moreover, non-tautologyset believed following contraction (H p 2), formulas added (H p 3).Contracting formula originally H effect contraction (H p 4),attempting contract tautology (H p 5). empty contraction unsurprisinglyeffect (H p 5b). knowledge-level accounts, contraction independentsyntactic expression formulas contracted (H p 6). last postulate (H p 7)corresponds maximality postulate contraction based weak remainders.package contraction H p nontrivial nontautologies appearH satisfy maximality condition formula contraction regularHorn contraction based weak remainder sets. is, package contraction essentiallyextends contraction set formulas. result expected, given Theorem 9related elements Hp weak remainders.discussed, characteristic maxichoice package contraction largenumber members Hp , may logically quite weak. However provescase eliminate candidates via pragmatic concerns.package contraction H p belief set H 0 Hp that, informally, models H 0contain countermodel along models H. general, interpretations countermodels one member ,pragmatically, oneselect minimal sets countermodels. Hencecase(M od(>) \ od(i )) 6= ,single countermodel, (M od(>) \ od(i )), would sufficient yieldpackage contraction.Now, may (M od(>) \ od(i )) empty. simple example illustratescase:Example 8 Let H = Cnh (a b, b a) P = {a, b}. H p {a b, b a} =Cnh (>). is, sole countermodel b {a} b {b}.intersection closure interpretations H {ab, a, b, } = od(>).Informally one get around simply selecting minimal set modelscountermodel member set. considerations yield followingdefinition:Definition 12 Let H Horn belief set, let = {1 , . . . , n } set Hornformulas.HS(), set (minimal) hitting sets interpretations respect , definedby:HS() iff1. |>|2. every 1 n 6` H, (|>| \ |i |) 6=3. 0 S, 0 (|>| \ |i |) = 1 n.Thus look sets sets interpretations; elements set interpretations represented maximal consistent sets formulas (Condition 1). well, set495fiDelgrande & Wassermanncontains countermodel member (Condition 2) moreover subsetminimal set satisfies conditions (Condition 3). Thus HS() correspondsminimal set countermodels members . aside, notednotion hitting set new general (Garey & Johnson, 1979) AI (Reiter,1987).Definition 13 Hph set sets formulasH 0 Hph iff H 0 = H mS HS().Definition 14 Let selection function H (Hph ) = {H 0 }H 0 Hph .Define:H ph = (Hph )=6 H 6 Cnh (>); H otherwise.following result follows straightforwardly.Theorem 11 H ph operator maxichoice Horn package contraction.Example 9 Consider case H = Cnh (a, b), P = {a, b, c}.1. Let = {a, b}.verified hitting sets given by:{ {ac, bc}, {a, bc}, {ac, b}, {a, b}, {c}, {} }corresponding elements Hph given by:Hph = { Cnh (>),Cnh (c a),Cnh (c b),Cnh (c a, c b),Cnh (a b, b a),Cnh (a b, b a, c a, c b) }.Compare Example 7, 14 candidate package contractions.2. Let = {a, b}. obtainHph = { Cnh (b),Cnh (b (c a)),Cnh (a b, b a),Cnh (a b, b a, c a, c b) }.496fiHorn Clause Contraction Functionsset formulas satisfies Definition 13 clearly also satisfies Definition 11. Onerestrict set candidate package contractions replacing 0 |S 0 | <|S| third part Definition 12. case, package contraction Example 9,Part 1 would yield two candidates Cnh (a b, b a) Cnh (a b, b a, ca, c b). well, course, one could continue obvious fashion define notionpartial meet Horn package contraction. Given limited use operator, omitdetails.6.2 Forgetting Horn Formulassection examines another means removing beliefs agents belief set,forgetting (Lin & Reiter, 1994; Lang & Marquis, 2002). Forgetting operationbelief sets atoms language; result forgetting atom regardeddecreasing language atom.addressing forgetting, easier work set Horn clauses, ratherHorn formulas. Since confusion, freely switch sets Hornclauses corresponding Horn formula comprising conjunction clausesset. Thus time set appears element formula, understoodstanding conjunction membersVset. Thus sets clauses S1VS2 , S1 S2 stand formula ( S1 ) ( S2 ). course, setsguaranteed finitely representable, since language finite.introduce following notation section, set Horn clauses,> taken distinguished atom true interpretations.{, >}, S[p/t] result uniformly substituting atom p everyS.Sp = { | p occur }Assume without loss generality Horn clause S, head () 6 body().following definition adapts standard definition, attributed George Boole,forgetting Horn clauses.Definition 15 set Horn clauses atom p, define f orget(S, p) S[p/]S[p/>].immediately useful us, since disjunction generally Horn. However,next result shows definition nonetheless leads Horn-definable forget operator.Recall clauses c1 c2 , expressed sets literals p c1 p c2 ,resolvent c1 c2 clause (c1 \ {p}) (c2 \ {p}). well, recall c1c2 Horn, resolvent.following, Res(S, p) set Horn clauses obtained carryingpossible resolutions respect p.Definition 16 Let set Horn clauses p atom. DefineRes(S, p) = { | 1 , 2p body(1 ) p = head (2 ),= (body(1 ) \ {p} body(2 )) head (1 )}497fiDelgrande & WassermannTheorem 12 f orget(S, p) Sp Res(S, p).Corollary 3 Let set Horn clauses p atom. f orget(S, p) equivalentset Horn clauses.Corollary 4 Let S1 S2 sets Horn clauses p atom. S1 S2 impliesf orget(S1 , p) f orget(S2 , p).several points interest results. theorem expressedterms arbitrary sets Horn clauses, deductively-closed Horn belief sets.Hence second corollary states principle irrelevance syntax case forgetting belief bases. well, expression Sp Res(S, p) readily computable,theorem fact provides means computing f orget. Further, approach clearlyiterates one atom. obtain additional result:8Corollary 5f orget(f orget(S, p), q) f orget(f orget(S, q), p).Given this, define set atoms f orget(S, ) =f orget(S, A) = f orget(f orget(S, a), \ {a})A. hand, forgetting atom may result quadratic blowupknowledge base.Finally, might seem approach allows definition revision operatorprocedure computing revision using something akin Levi Identity.Let A() set atoms appearing (formula set formulas) . Then:defFRevise(S, ) = f orget(S, A()) + .fact, yield revision operator, operator general far drasticuseful. see this, consider taxonomic knowledge base asserts whalesfish, whale f ish. course, whales mammals, using definitionrepair knowledge base, one would first forget knowledge involving whales,example, whales fins, breathe air, give live birth, on. exampledoesnt prove reasonable revision operators definable via forget,show nave approach problematic. Moreover, problems particularHorn formulas, rather revision operator defined terms forgetting respectunderlying logic would similarly problematic.7. Comparison among Constructions Horn Contractionsection provides technical summary differences various contractionoperations defined Horn belief sets:Every e-remainder weak remainder, converse true.8. fact, easy consequence definition forget.498fiHorn Clause Contraction Functionsclearly seen Figure 1. Horn theory H formula , e-remaindersmaximal subsets H imply . weak remainders characterisedmodels H together single countermodel , closed intersection.propositional logic notions would coincide; not. well, meansweak remainders partial meet distinct notions, latter correspondingintersections weak remainders.Similarly, obtain following:Every e-remainder infra-remainder, converse true.clear Definition 3, illustrated Example 1.also have:infra-remainders weak-remainders.Looking Figure 1, see set Cnh ({c a, c b, b}) infraremainder weak remainder. however obtained intersection tworemainders.Consider Example 3.2 presented Booth, Meyer Varzinczak (2009), H =hCn ({p q, q r}) one wants contract p r: case, weak remainderscoincide remainders. set {p q r, p r q} infra-remaindercannot obtained intersection weak-remainders. authors claim setdesirable result contraction, give strong motivation.Last, have:weak remainders infra-remainders.Infra-remainders, definition, must contain full-meet contained remainder.Weak remainders contained remainder (or remainder) alwayscontain full meet, seen table Figure 1. Full-meet example wouldcontain {c a, c b} two weak remainders (Cnh (a) Cnh (b))contain formulas.last two items show weak remainders infra-remainders independentconcepts relation studied detail. various relationsillustrated Figure 2.weak remainderseremaindersFigure 2499infra remaindersfiDelgrande & Wassermannaforecited example Booth, Meyer, Varzinczak (2009) raises another pointdeserves attention: H = Cnh ({p q, q r}), H e (p r) = (pr) = {Cnh ({p q}), Cnh ({q r, p r q})}. asymmetrypossible obtain Cnh ({p q}) result contraction, e-remainders, weakremainders infra-remainders allow Cnh ({q r}) possible outcome.motivated study Horn belief base contraction (Delgrande & Wassermann, 2010),one may obtain Cnh ({q r}), think may find interestingalternatives.Zhuang Pagnucco studied several forms Horn Contraction,Epistemic Entrenchment Horn Contraction (EEHC) mentioned earlier (2010a), TransitivelyRelational Partial-Meet Horn Contraction (TRPMHC) (2011) Model-based Horn Contraction (MHC) (2012). different operations compared Zhuang (2012). Partialmeet based weak remainders general EEHC MHC. However,selection function required transitively relational, obtain TRPMHC,equivalent MHC.8. Conclusionpaper explored belief contraction, operators related belief contraction,respect Horn theories. AGM approach two principal meansconstructing contraction functions, via remainders maximal subsets belief set failimply formula, epistemic entrenchment, incorporates preference orderingformulas. focus Horn contraction functions defined remainderlike constructions.proves case basing contraction directly remainder sets, yieldingcall e-remainders, problematic, resulting approach inexpressiveundesirable properties. also show alternative proposed, infraremainders suffers problems. Based examination model-theoreticconsiderations developed account maxichoice Horn contraction terms weakremainder sets. idea models contraction Horn belief set HHorn formula given models H together countermodel , closedintersection (so yield Horn theory). provided representation resultsmaxichoice Horn contraction well partial meet contraction, comparedproposals literature.also study two kinds operators giving beliefs Horn theories: packagecontraction forgetting. former involves contracting set formulas,formula set believed, Again, give construction postulate set, alongcorresponding representation result. second operator, forgetting, thoughtshrinking language discourse.work interesting since Horn clauses found widespread use areaslogic programming, rule-based systems, deductive databases, description logics.well, since Horn reasoning weaker classical reasoning, work sheds lightfoundations belief change. natural topic future work consider Horn revisionoperators study relation Horn contraction. second topic future workconsider belief change logics contain classical propositional logic.500fiHorn Clause Contraction FunctionsAcknowledgmentsfirst author partially supported Canadian NSERC Discovery Grant.second author partially supported Brazilian National Research Council (CNPq),grants 304043/2010-9 471666/2010-6. thank Tommie Meyer, MarcioRibeiro, David Makinson, anonymous reviewers helpful comments.Appendix A. Proofs Main ResultsTheorem 2: Let Horn contraction function defined via selection function(2) based (infra) remainder sets.H p inessential H, obtain (H ) + p ` .Proof: Let = 1 n Horn clause. Horn conjunct ,H e |= p . (To see this, note first Horn clause,form body conjunction atoms body atom a. Since body H HHorn belief set, also p body H. Since assumption p body, followsp body remainder set H respect . Then, p bodylogically equivalent p (body a), whence H e |= p .) Thus (H e ) {p} |=(H e ) + p |= conjunct , (H e ) + p |= .Theorem 3: Let P atom, let H Horn belief set H.Let maxichoice Horn contraction function based remainder sets. everyatom b, least one b b H (a ) + a.Proof: Suppose atom b, neither b b H (a ) + a,H. Since atom, tautology, H,construction, H (a ) element H e (a ). This, togetherassumption b, b 6 H (a ), gives us (1) (H (a )) {b} ` (2)(H (a )) {b } ` . (Results (1) (2) consequence factsince H (a ) remainder set, maximal set fails imply .)(1) (2) together, (H (a )) ` , contradicting successpostulate.Lemma 1 Let set propositional formulas.Cl (M od(T )) = od(Horn(Cn(T ))).Proof:Cl (M od(T )) least set models od(T )Cl (M od(T )) Cl (M od(T )) = od(H) Horn theory H.theory least upper Horn approximation h (Selman & Kautz, 1996), givenh = { | ` Horn prime implicate }.Cnh (T h ) = Horn(Cn(T )) result follows.Theorem 4: H Horn belief set Horn formula:= H ||e .Proof:501fiDelgrande & Wassermann1. H ||e :6 H ` = H ||e = {H}.assume H 6` .Let H 0 ; show H 0 H ||e .Since H 0 , definition H 0 = H |>|\||, od(H 0 ) =od(H m). H Horn theories, thus H Horn theory.Using fact Horn belief set , = Horn(Cn(T )), H =Horn(Cn(H m)) od(H 0 ) = od(Horn(Cn(H m)).Applying Lemma 1 Hm obtain od(Horn(Cn(Hm)) = Cl (M od(Cn(Hm))). Now, Cl (M od(Cn(Hm))) = Cl (M od(Hm)) = Cl ((M od(H)M od(m))).definition maximal consistent Horn theory, od(>)od(m) = {M }. Putting together get od(H 0 ) =Cl ((M od(H) )), is, H 0 H ||e .2. H ||e :part follows immediately essentially taking preceding part reverse order.Lemma 2 Maximality (H w 7) equivalent following property, call(H w 70 ):H 6= H w |>| \ || s.t. H w H 0 s.t. H w H 0 HH 0 6 m.Proof: straightforward show property implies (H w 7): Letconjunction literals appearing m. language finite, well-defined formula.Cnh () = m, thus (H w 7) holds.direction, assume (H w 7) holds.Claim: given H , satisfies conditions (H w 7) p P,either p (p ) also satisfies conditions (H w 7).Proof Claim: Clearly, {, } inconsistent {, l} l{p, p }; H Cnh () H 0 Cnh ( l) l {p, p }.need show Horn theory H 0 H H 0 H, eitherH 0 6 Cnh ( p) H 0 6 Cnh ( (p )).Towards contradiction, assume otherwise. H 0 Cnh ( p) H 0Cnh ( (p )) H 0 Cnh ( p) Cnh ( (p )). Cnh () =Cnh ( p) Cnh ( p ), consequently H 0 Cnh (). contradictssatisfies (H w 7) H .Hence assumption incorrect, H 0 6 Cnh ( p) H 0 6 Cnh ((p )).502fiHorn Clause Contraction Functionsshown satisfies (H w 7) given H , onep (p ). induction (the finite set) P establishes satisfies(H w 7) given H , 0 0 ` p 0 ` (p ) everyp P. Hence 0 Cnh ( 0 ) |>| \ ||, thus taking = Cnh ( 0 ) satisfiesproperty.Theorem 5: Let H Horn belief set. w operator maxichoice Horncontraction based weak remainders iff w satisfies following postulates.(H w 1) H w Horn belief set.(closure)(H w 2) ` , 6 H w .(success)(H w 3) H w H.(inclusion)(H w 4) 6 H, H w = H.(vacuity)(H w 5) ` H w = H(failure)(H w 6) , H w = H w .(extensionality)(H w 7) H 6= H w LHC {, } ` , H w Cnh () H 0s.t H w H 0 H H 06 Cnh ().(maximality)Proof:1. Construction Postulates:construction satisfies first five postulates follows directly definitions weak remainders selection functions. see satisfies (H w 6)note implies = since function,H w = H w .see construction satisfies (H w 7), suppose H 6= H w . means6= hence, |>| \ || H w = H m. Letconjunction literals appearing m. Then, since Cnh () = m,{, } inconsistent, H w Cnh () H 0 s.t H w H 0 HH 0 6 Cnh ().2. Postulates Construction:proof uses (H w 70 ) rather (H w 7), shown equivalentLemma 2.Let w operator satisfies Cn(H w 1) (H w 70 ).Let defined (He ) = {H w }.show function:503fiDelgrande & WassermannAssume = ; need show (He ) = (He ).6 H, = {H} since = ,= H, hence 6 H ` . Then, (H w 4) (H w 5),H w = H w = H definition (He ) = (He ).let us consider case , H. Since ={H | |>| \ ||} = {H | |>| \ ||}.follows |>| \ || = |>| \ ||. see this, suppose {H ||>| \ ||} = {H | |>| \ ||} |>| \ || =6 |>| \ ||. Without lossgenerality, suppose m0 |>| \ || m0 6 |>| \ ||.m0 maximal consistent theory contains . Since H, knowH m0 . means H m0 {H | |>| \ ||},H m0 6 {H | |>| \ ||}, |>| \ || definition6 m. contradicts initial hypothesis.Since |>| \ || = |>| \ || get || = || . (H w 6)H w = H w , (He ) = (He ).6 H, (H w 4) H w = H. Similarly, ` ,(H w 5) H w = H.Consequently assume H ` . need show H w ,is, H w = H |>| \ ||.Since ` , (H w 2) 6 H w ; since HH 6= H w .Since H 6= H w , (H w 70 ) get |>| \ || H wm.well, (H w 3) gives H w H, H w implies H w(m H).need show H w = (m H). Towards contradiction assumeH w 6= (m H), say, H w (m H).Let (m H) \ (H w ).H w Cnh (H w {}) H H.But, substituting Cnh (H w {}) H 0 (H w 70 ) get Cnh (H w{}) 6 m, contradiction.Hence assumption H w 6= (m H) incorrect; hence H w = (m H)(m H) , shown.Theorem 6: Let H Horn belief set. pm operator partial meet Horncontraction based weak remainders iff pm satisfies postulates (H w 1) (H w 6)and:(H pm 7) H \(H pm ), H 0 H pm H 0 , 6 Cnh (H 0 )Cnh (H 0 {})(weak relevance)504fiHorn Clause Contraction FunctionsProof:1. Construction Postulates:(H w 1) follows fact intersection Horn theories Horn theory. Postulates (H w 2) (H w 6) follow immediately definitions weakremainder, selection function partial meet contraction.see construction satisfies weak relevance, note H \ H ,X (He ) 6 X. Since H,|>| \ || 6 X = H m. Take H 0 = m. H H 0 ,6 Cnh (H 0 ) Cnh (H 0 {}) = Cnh ().2. Postulates Construction:Let (He ) = {X | H pm X} 6= (He ) = {H}otherwise.show that: (1) function; (2) selection function;(3) (He ) = H .6 H, (H w 4), H pm = H = (He ). Assume H.(1) Let 1 = 2 . must show (He 1 ) = (He 2 ).proof maxichoice contraction, 1 = 2 implies 1 2 then,Postulate (H w 6), H1 = H2 . construction , (He 1 ) = (He 2 ).(2) construction , know (He ) . show6= , (He ) 6= , otherwise (He ) = {H}.(i) 6= , H 6= |>| \ || =6 . (H w 1) (H w 2),6 Cn(H ). |>| \ || H m. (H w 3),H H, hence, H H (H).(ii) = , ` (H w 5), H = H.(3) know thatTH (He ). Suppose (He )6 H . Since (He ) H, H \ (H ) weak relevance knowH 0 H H 0 , 6 Cnh (H 0 ) Cnh (H 0 {}).|>| \ || H 0 6 m. Take X = H m.X (H w 3) H X hence, X (He ).6 X, leads contradiction.Theorem 7: Partial meet based weak remainders transitive relational selectionfunction satisfies (K 7) (K 8).Proof:Let selection function based transitive relation .Since |>| \ | | = (|>| \ ||) (|>| \ ||) hence, = ,order show PMWR satisfies postulate (K-7), suffices show(*) (He ) (He ) (He ).99. called Choice-distributivity literature.505fiDelgrande & WassermannTake X (He ). know X X . SupposeX , show X (He ). X 6 (He ), X 0X X 0 . X 0 X 6 (He ). caseX analogous, thus X (He ) X (He ), proves (*).order show PMWR satisfies postulate (K-8), let 6 H .show(**) (He ) (He )6 H know (He ) contains least one element .Since based , (He ) (He ).Theorem 8: PMWR satisfies (H 5).Proof:see PMWR satisfies (H 5), first note .H , know X every X (He ). showX every X (He ). Let X (He ). X 6 ,X = H maximal, consistent Horn theory containcontains . Hence, X. Otherwise, i.e., X , showX (He ). Suppose X 6 (He ), X 0X 0 < X. X 0 X cannot element (He ).Hence, every X (He ), know X therefore, H .Theorem 9: Let H Horn belief set let = {1 , . . . , n } set Horn formulas1 n 6` .H 0 Hp iff 1 n Hi H 0 = ni=1 Hi .Proof: Let H Horn belief set = {1 , . . . , n } LHC .= Let H 0 Hp .Definition 10 m1 , . . . , mn H 0 =Tni=1 (Hmi )1. H 6` mi |>| \ |i |;2. otherwise mi = LHC .i, 1 n, above,1. H 6` Definition 4, Hi = H mi satisfies conditionsHi ;2. otherwise mi = LHC Hi = H mi = H LHC = H satisfiesconditions Hi , Definition 4.= Consider let Hi .Definition 41. H 6` Hi = H |>| \ |i |;506fiHorn Clause Contraction Functions2. 6 H ` Hi = H equivalently Hi = H mi mi = LHC .Consequently i, 1 n, above, H 0 = ni=1 Hi satisfies conditionsH 0 Hp Definition 10.Theorem 10: Let H Horn belief set. p operator maxichoice Hornpackage contraction based weak remainders iff p satisfies following postulates:(H p 1) H p belief set.(closure)(H p 2) , ` , 6 H p(success)(H p 3) H p H(inclusion)(H p 4) H p = H p (H )(vacuity)(H p 5) H p = H p ( \ Cnh (>))(failure)(H p 5b) H p = H(triviality)(H p 6) ,H p ( {}) = H p ( {})(extensionality)(H p 7) H 6= H p0 = ( \ Cnh (>)) H = {1 , . . . , n }= {1 , . . . , n } 1 n,{i , } ` H p Cnh (i )H 0 s.t H p H 0 H, H 0 6 Cnh ()(maximality)Proof:1. Construction Postulates:(H p 1) obvious.(H p 2), H, Definition 10 ensures H 0 Hp H 0 6`6 H 0 .(H p 3) H 0 Hp implies H 0 form H X; consequently H 0 H.(H p 4) (H p 5) direct consequence special cases Definition 106 H ` respectively.(H p 5b) vacuously satisfied Definition 10, (H p 6), formH 0 Hp easily seen independent syntactic form members .(H p 7), let X Hp = {1 , . . . , n }. appeal (H p 4)(H p 5) assume without loss generality implies 6`H. Let m1 , . . . , mn specified Definition 10. m1 , . . . , mn satisfy507fiDelgrande & Wassermannconditions1 , . . . , n (H p 7): Since mi |>| \ |i |, {i , } ` . SinceX = H ni=1 mi , X Cnh (mi ) = mi . Last, need show belief0hset H 0 X H 0 H, mi list,Tn H 6 Cn (mi ) = mi .direct consequence fact X = H i=1 mi .2. Postulates Construction:Let p satisfy Postulates (H p 1)(H p 7), let H Horn belief setLHC . Let specified (H p 7) , define H by:(a) ` 6 H H = H.(b) Otherwise corresponding , H maximum setformulas H p H H H Cnh ().Using Theorem 5, easily shown operator maxichoice Horn contraction.implies selection function H = (He ) every.Therefore, Theorem 9 H p = H = H 0H 0 Hp .Theorem 12: f orget(S, p) Sp Res(S, p).Proof: Let finite set nontautological Horn clauses. p P, define:Sh = {c | p = head(c)}Sb = {c | p body(c)}well, already defined: Sp = {c | p occur c}.obtain:f orget(S, p) S[p/] S[p/>](Sh [p/] Sb [p/] Sp [p/])(Sh [p/>] Sb [p/>] Sp [p/>])(Sh [p/] {>} Sp ) ({>} Sb [p/>] Sp )(Sh [p/] Sp ) (Sb [p/>] Sp )Sp (Sh [p/] Sb [p/>])Sp {c1 c2 | c1 Sh [p/] c2 Sb [p/>]}Sp Res(S, p)ReferencesAlchourron, C. E., & Makinson, D. (1985). logic theory change: Safe contraction.Studia Logica, 44 (4), 405422.508fiHorn Clause Contraction FunctionsAlchourron, C., Gardenfors, P., & Makinson, D. (1985). logic theory change:Partial meet contraction revision functions. Journal Symbolic Logic, 50 (2),510530.Anderson, A., & Belnap Jr., N. (1975). Entailment: Logic Relevance Necessity,Vol. I. Princeton University Press.Baader, F., Calvanese, D., McGuiness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2007).Description Logic Handbook (second edition). Cambridge University Press.Booth, R., Meyer, T., Varzinczak, I., & Wassermann, R. (2011). Link PartialMeet, Kernel, Infra Contraction Application Horn Logic. JournalArtificial Intelligence Research, 42, 3153.Booth, R., Meyer, T., & Varzinczak, I. (2009). Next steps propositional Horn contraction.Proceedings International Joint Conference Artificial Intelligence, pp.702707, Pasadena, CA.Creignou, N., Papini, O., Pichler, R., & Woltran, S. (2012). Belief revision within fragmentspropositional logic. Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), ProceedingsThirteenth International Conference Principles Knowledge Representation Reasoning. AAAI Press.Delgrande, J., & Wassermann, R. (2010). Horn clause contraction functions: Belief setbelief base approaches. Lin, F., & Sattler, U. (Eds.), Proceedings Twelfth International Conference Principles Knowledge Representation Reasoning,pp. 143152, Toronto. AAAI Press.Delgrande, J. (2008). Horn clause belief change: Contraction functions. Brewka, G., &Lang, J. (Eds.), Proceedings Eleventh International Conference PrinciplesKnowledge Representation Reasoning, pp. 156165, Sydney, Australia. AAAIPress.Delgrande, J., & Peppas, P. (2011). Revising Horn Theories. Twenty-Second InternationalJoint Conference Artificial Intelligence, pp. 839844.Delgrande, J., & Wassermann, R. (2011). Topics Horn contraction: Supplementary postulates, package contraction, forgetting. IJCAI-11 Workshop NonmonotonicReasoning, Action Change (NRAC-11), pp. 8794, Barcelona, Spain.Eiter, T., & Gottlob, G. (1992). complexity propositional knowledge base revision,updates, counterfactuals. Artificial Intelligence, 57 (2-3), 227270.Flouris, G., Plexousakis, D., & Antoniou, G. (2004). Generalizing AGM postulates: Preliminary results applications. Proceedings 10th International WorkshopNon-Monotonic Reasoning (NMR-04), pp. 171179, Whistler BC, Canada.Gardenfors, P. (1988). Knowledge Flux: Modelling Dynamics Epistemic States.MIT Press, Cambridge, MA.Gardenfors, P., & Makinson, D. (1988). Revisions knowledge systems using epistemicentrenchment. Proc. Second Theoretical Aspects Reasoning KnowledgeConference, pp. 8395, Monterey, Ca.509fiDelgrande & WassermannGarey, M., & Johnson, D. (1979). Computers Intractability: Guide TheoryNP-Completeness. W.H. Freeman Co., New York.Grove, A. (1988). Two Modellings Theory Change. Journal Philosophical Logic, 17,157170.Hansson, S. O. (1999). Textbook Belief Dynamics. Applied Logic Series. KluwerAcademic Publishers.Khardon, R. (1995). Translating Horn representations characteristicmodels. Journal Artificial Intelligence Research, 3, 349372.Lakemeyer, G., & Levesque, H. (2000). Logic Knowledge Bases. MIT Press, Cambridge, MA.Lang, J., & Marquis, P. (2002). Resolving inconsistencies variable forgetting. Proceedings Eighth International Conference Principles Knowledge Representation Reasoning, pp. 239250, San Francisco. Morgan Kaufmann.Langlois, M., Sloan, R., Szorenyi, B., & Turan, G. (2008). Horn complements: TowardsHorn-to-Horn belief revision. Proceedings AAAI National ConferenceArtificial Intelligence, Chicago, Il.Liberatore, P. (2000). Compilability compact representations revision Horn knowledge bases. ACM Transactions Computational Logic, 1 (1), 131161.Lin, F., & Reiter, R. (1994). Forget it!. AAAI Fall Symposium Relevance, NewOrleans.Makinson, D. (2009) Personal communication.Peppas, P. (2008). Belief revision. van Harmelen, F., Lifschitz, V., & Porter, B. (Eds.),Handbook Knowledge Representation, pp. 317359. Elsevier Science, San Diego,USA.Reiter, R. (1987). theory diagnosis first principles. Artificial Intelligence, 32 (1),5796.Rott, H. (1992). logic theory change: maps different kindscontraction functions. Gardenfors, P. (Ed.), Belief Revision, No. 29 CambridgeTracts Theoretical Computer Science, pp. 122141. Cambridge University Press.Selman, B., & Kautz, H. (1996). Knowledge compilation theory approximation. JournalACM, 43 (2), 193224.Zhuang, Z., & Pagnucco, M. (2010a). Horn contraction via epistemic entrenchment.Janhunen, T., & Niemela, I. (Eds.), Logics Artificial Intelligence - 12th EuropeanConference (JELIA 2010), Vol. 6341 Lecture Notes Artificial Intelligence, pp.339351. Springer Verlag.Zhuang, Z., & Pagnucco, M. (2010b). Two methods constructing Horn contractions.Li, J. (Ed.), AI 2010: Advances Artificial Intelligence - 23rd Australasian JointConference, Vol. 6464 Lecture Notes Artificial Intelligence, pp. 7281. SpringerVerlag.510fiHorn Clause Contraction FunctionsZhuang, Z. (2012). Belief Change Horn Fragment Propositional Logic. Ph.D.thesis, School Computer Science Engineering University New South Wales.Zhuang, Z., & Pagnucco, M. (2011). Transitively relational partial meet Horn contraction.Proceedings Twenty-Second International Joint Conference Artificial Intelligence, pp. 11321138, Barcelona, Spain.Zhuang, Z., & Pagnucco, M. (2012). Model based Horn contraction. ProceedingsThirteenth International Conference Principles Knowledge RepresentationReasoning, Rome, Italy.511fiJournal Artificial Intelligence Research 48 (2013) 671-715Submitted 04/13; published 11/13Generating Natural Language Descriptions OWLOntologies: NaturalOWL SystemIon Androutsopoulosion@aueb.grDepartment Informatics,Athens University Economics Business, GreeceDigital Curation Unit Institute Management Information Systems,Research Centre Athena, Athens, GreeceGerasimos Lampouraslampouras06@aueb.grDepartment Informatics,Athens University Economics Business, GreeceDimitrios Galanisgalanisd@aueb.grDepartment Informatics,Athens University Economics Business, GreeceInstitute Language Speech Processing,Research Centre Athena, Athens, GreeceAbstractpresent Naturalowl, natural language generation system produces textsdescribing individuals classes owl ontologies. Unlike simpler owl verbalizers,typically express single axiom time controlled, often entirely fluent naturallanguage primarily benefit domain experts, aim generate fluent coherent multi-sentence texts end-users. system like Naturalowl, one publishinformation owl Web, along automatically produced corresponding textsmultiple languages, making information accessible computer programsdomain experts, also end-users. discuss processing stages Naturalowl,optional domain-dependent linguistic resources system use stage,useful. also present trials showing domain-dependentlinguistic resources available, Naturalowl produces significantly better texts comparedsimpler verbalizer, resources created relatively light effort.1. IntroductionOntologies play central role Semantic Web (Berners-Lee, Hendler, & Lassila, 2001;Shadbolt, Berners-Lee, & Hall, 2006). ontology provides conceptualizationknowledge domain (e.g., consumer electronics) defining classes subclassesindividuals (entities) domain, types possible relations etc.current standard specify Semantic Web ontologies owl (Horrocks, Patel-Schneider,& van Harmelen, 2003), formal language based description logics (Baader, Calvanese,McGuinness, Nardi, & Patel-Schneider, 2002), rdf, rdf schema (Antoniou & vanHarmelen, 2008), owl2 latest version owl (Grau, Horrocks, Motik, Parc2013AI Access Foundation. rights reserved.fiAndroutsopoulos, Lampouras, & Galanissia, Patel-Schneider, & Sattler, 2008). Given owl ontology knowledge domain, onepublish Web machine-readable data pertaining domain (e.g., cataloguesproducts, features etc.), data formally defined semantics basedconceptualization ontology.1 Following common practice Semantic Web research,actually use term ontology refer jointly terminological knowledge (TBox)establishes conceptualization knowledge domain, assertional knowledge (ABox)describes particular individuals.Several equivalent owl syntaxes developed, people unfamiliar formalknowledge representation often difficulties understanding (Rector, Drummond,Horridge, Rogers, Knublauch, Stevens, Wang, & Wroe, 2004). example, followingstatement defines class St. Emilion wines, using functional-style syntax owl,one easiest understand, also adopt throughout article.2EquivalentClasses(:StEmilionObjectIntersectionOf(:BordeauxObjectHasValue(:locatedIn :stEmilionRegion) ObjectHasValue(:hasColor :red)ObjectHasValue(:hasFlavor :strong)ObjectHasValue(:madeFrom :cabernetSauvignonGrape)ObjectMaxCardinality(1 :madeFrom)))make ontologies easier understand, several ontology verbalizers developed(Schwitter, 2010a). Verbalizers usually translate axioms (in case, owl statements)ontology one one controlled, often entirely fluent English statements, typicallywithout considering coherence resulting texts, mostly benefit domainexperts. contrast, article present system aims produce fluentcoherent multi-sentence texts describing classes individuals owl ontologies,texts intended read end-users (e.g., customers on-line retail sites). example,system generate following text owl statement above, ontologyannotated domain-dependent linguistic resources discussed below.St. Emilion kind Bordeaux St. Emilion region. red color strong flavor.made exactly one grape variety: Cabernet Sauvignon grapes.system, called Naturalowl, open-source supports English Greek.Hence, Greek texts also generated owl statements, followingproduct description, provided appropriate Greek linguistic resources also available.contrast, owl verbalizers typically produce English (or English-like) sentences.ClassAssertion(:Laptop :tecraA8)ObjectPropertyAssertion(:manufacturedBy :tecraA8 :toshiba)ObjectPropertyAssertion(:hasProcessor :tecraA8 :intelCore2)DataPropertyAssertion(:hasMemoryInGB :tecraA8 "2"^^xsd:nonNegativeInteger)DataPropertyAssertion(:hasHardDiskInGB :tecraA8 "110"^^xsd:nonNegativeInteger)DataPropertyAssertion(:hasSpeedInGHz :tecraA8 "2"^^xsd:float)DataPropertyAssertion(:hasPriceInEuro :tecraA8 "850"^^xsd:nonNegativeInteger)[English description:] Tecra A8 laptop, manufactured Toshiba. Intel Core 2 processor,2 gb ram 110 gb hard disk. speed 2 ghz costs 850 Euro.1. See http://owl.cs.manchester.ac.uk/repository/ repository owl ontologies.2. Consult http://www.w3.org/TR/owl2-primer/ introduction functional-style syntax owl.672fiGenerating Natural Language Descriptions OWL Ontologies[Greek description:] Tecra A8 , Toshiba.Intel Core 2, 2 gb ram 110 gb. 2 ghz850 .examples illustrate system like Naturalowl help publish information Web owl statements texts generated owl statements.way, information becomes easily accessible computers, processowl statements, end-users speaking different languages; changes owlstatements automatically reflected texts regenerating them. producefluent, coherent multi-sentence texts, Naturalowl relies natural language generation(nlg) methods (McKeown, 1985; Reiter & Dale, 2000) larger extent compared existing owl verbalizers; example, includes mechanisms avoid repeating information,order facts expressed, aggregate smaller sentences longer ones, generatereferring expressions etc. Although nlg established area, first articlediscuss detail nlg system owl ontologies, excluding simpler verbalizers.propose novel algorithms theoretical nlg perspective, showseveral particular issues need considered generating owl ontologies.example, owl statements lead overly complicated sentences, unlessconverted simpler intermediate representations first; also several owl-specificopportunities aggregate sentences (e.g., expressing axioms cardinalitiesproperties); referring expression generation exploit class hierarchy.Naturalowl used owl ontology, obtain texts high qualitydomain-dependent generation resources required; example, classes ontologymapped natural language names, properties sentence plans etc. Similarlinguistic resources used nlg systems, though different systems adopt differentlinguistic theories algorithms, requiring different resources. little consensusexactly information nlg resources capture, apart abstract specifications(Mellish, 2010). domain-dependent generation resources Naturalowl createddomain author, person familiar owl, system configured newontology. domain author uses Protege ontology editor Protege plug-inallows editing domain-dependent generation resources invoking Naturalowl viewresulting texts.3 discuss plug-in article, since similarauthoring tool m-piro (Androutsopoulos, Oberlander, & Karkaletsis, 2007).owl ontologies often use English words concatenations words (e.g., manufacturedBy)identifiers classes, properties, individuals. Hence, domain-dependentgeneration resources often extracted ontology guessing, example,class identifier like Laptop earlier example noun used referclass, statement form ObjectPropertyAssertion(:manufacturedBy X) expressed English sentence form X manufactured .owl verbalizers follow strategy. Similarly, domain-dependent generation resources provided, Naturalowl attempts extract ontology, uses3. Consult http://protege.stanford.edu/ information Protege. Naturalowl Protege plugin freely available http://nlp.cs.aueb.gr/software.html. describe Naturalowl version 2article; version 1 (Galanis & Androutsopoulos, 2007) used less principled representationdomain-dependent generation resources, without supporting owl2.673fiAndroutsopoulos, Lampouras, & Galanisgeneric resources. resulting texts, however, lower quality; also, non-English textscannot generated, identifiers ontology English-like. tradeoffreducing effort construct domain-dependent generation resources owlontologies, obtaining higher-quality texts multiple languages, tradeoffinvestigated previous work. present trials performed measureeffort required construct domain-dependent generation resources Naturalowlextent improve resulting texts, also comparing simplerverbalizer requires domain-dependent generation resources. trials showdomain-dependent generation resources help Naturalowl produce significantly bettertexts, resources constructed relatively light effort, comparedeffort typically needed construct ontology.Overall, main contributions article are: (i) first detailed discussioncomplete, general-purpose nlg system owl ontologies particular issuesarise generating owl ontologies; (ii) shows system relies nlgmethods larger extent, compared simpler owl verbalizers, produce significantlybetter natural language descriptions classes individuals, provided appropriatedomain-dependent generation resources available; (iii) shows descriptionsgenerated one languages, provided appropriate resourcesavailable; (iv) shows domain-dependent generation resources constructedrelatively light effort. already noted, article present novel algorithmstheoretical nlg perspective. fact, algorithms Naturalowl usesnarrower scope, compared fully-fledged nlg algorithms. Nevertheless,trials show system produces texts reasonable quality, especially domaindependent generation resources provided. hope Naturalowl contributestowards wider adoption nlg methods Semantic Web, researchers maywish contribute improved components, given Naturalowl open-source.Naturalowl based ideas ilex (ODonnell, Mellish, Oberlander, & Knott,2001) m-piro (Isard, Oberlander, Androutsopoulos, & Matheson, 2003). ilexproject developed nlg system demonstrated mostly museum exhibits,support owl.4 m-piro project produced multilingual extension systemilex, tested several domains (Androutsopoulos et al., 2007). Attemptsuse generator m-piro owl, however, ran problems (Androutsopoulos,Kallonis, & Karkaletsis, 2005). contrast, Naturalowl especially developed owl.remainder article, assume reader familiar rdf, rdfschema, owl. Readers unfamiliar Semantic Web may wish consultintroductory text first (Antoniou & van Harmelen, 2008).5 also note recentlypopular Linked Data published interconnected using Semantic Web technologies.6 Linked Data currently use rdf rdf schema, owl effectsuperset rdf schema and, hence, work paper also applies Linked Data.4. Dale et al. (1998) Dannels (2008, 2012) also discuss nlg museums.5. longer version article, background readers unfamiliar owlSemantic Web, available technical report (Androutsopoulos, Lampouras, & Galanis, 2012); seehttp://nlp.cs.aueb.gr/publications.html.6. Consult http://linkeddata.org/. See also work Duma Klein (2013).674fiGenerating Natural Language Descriptions OWL OntologiesSection 2 briefly discusses related work; provide pointersrelated work subsequent sections. Section 3 explains Naturalowl generatestexts, also discussing domain-dependent generation resources processing stage.Section 4 describes trials performed measure effort required constructdomain-dependent generation resources impact quality generatedtexts. Section 5 concludes proposes future work.2. Related Workuse functional-style syntax owl article, several equivalent owl syntaxes exist. also work develop controlled natural languages (cnls), mostlyEnglish-like, used alternative owl syntaxes. Sydney owl Syntax (sos) (Cregan,Schwitter, & Meyer, 2007) English-like cnl bidirectional mappingfunctional-style syntax owl; sos based peng (Schwitter & Tilbrook, 2004).similar bidirectional mapping defined Attempto Controlled English (ace)(Kaljurand, 2007). Rabbit (Denaux, Dimitrova, Cohn, Dolbear, & Hart, 2010) clone(Funk, Tablan, Bontcheva, Cunningham, Davis, & Handschuh, 2007) owl cnls,mostly intended used domain experts authoring ontologies (Denaux, Dolbear,Hart, Dimitrova, & Cohn, 2011). also note owl cnls cannot expresskinds owl statements (Schwitter, Kaljurand, Cregan, Dolbear, & Hart, 2008).Much work owl cnls focuses ontology authoring querying (Bernardi, Calvanese, & Thorne, 2007; Kaufmann & Bernstein, 2010; Schwitter, 2010b); emphasismostly direction cnl owl query languages.7 relevant workcnls like sos ace, automatic mappings normative owl syntaxesavailable. feeding owl ontology expressed, example, functional-style syntaxmapping translates English-like cnl, axioms ontologyturned English-like sentences. Systems kind often called ontology verbalizers.term, however, also includes systems translate owl English-like statements belong explicitly defined cnl (Halaschek-Wiener, Golbeck, Parsia,Kolovski, & Hendler, 2008; Schutte, 2009; Power & Third, 2010; Power, 2010; Stevens,Malone, Williams, Power, & Third, 2011; Liang, Stevens, Scott, & Rector, 2011b).Although verbalizers viewed performing kind light nlg, typicallytranslate axioms one one, already noted, without considering coherence (or topical cohesion) resulting texts, usually without aggregating sentences generatingreferring expressions, often producing sentences entirely fluent natural. example, ace sos occasionally use variables instead referring expressions(Schwitter et al., 2008). Also, verbalizers typically employ domain-dependent generation resources typically support multiple languages. Expressing exactmeaning axioms ontology unambiguous manner considered important verbalizers composing fluent coherent text multiple languages,partly verbalizers typically intended used domain experts.7. Conceptual authoring wysiwym (Power & Scott, 1998; Hallett, Scott, & Power, 2007),applied owl (Power, 2009), round-trip authoring (Davis, Iqbal, Funk, Tablan, Bontcheva, Cunningham, & Handschuh, 2008) bidirectional, focus mostly ontology authoring querying.675fiAndroutsopoulos, Lampouras, & GalanisFigure 1: processing stages sub-stages Naturalowl.verbalizers use ideas methods nlg. example, verbalizers includesentence aggregation (Williams & Power, 2010) text planning (Liang, Scott, Stevens, &Rector, 2011a). Overall, however, nlg methods used limited extentowl ontologies. notable exception ontosum (Bontcheva, 2005), generatesnatural language descriptions individuals, apparently classes, rdf schemaowl ontologies. extension miakt (Bontcheva & Wilks, 2004),used generate medical reports. implemented gate (Bontcheva, Tablan,Maynard, & Cunningham, 2004) provide graphical user interfaces manipulatedomain-dependent generation resources (Bontcheva & Cunningham, 2003). detaileddescription ontosum appears published, however, systemseem publicly available, unlike Naturalowl. Also, trials ontosumindependently created ontologies seem published. informationontosum compares Naturalowl found elsewhere (Androutsopoulos et al., 2012).Mellish Sun (2006) focus lexicalization sentence aggregation, aimingproduce single aggregated sentence input collection rdf triples; contrast,Naturalowl produces multi-sentence texts. complementary work, Mellish et al. (2008)consider content selection texts describing owl classes. Unlike Naturalowl, systemexpress facts explicit ontology, also facts deducedontology. Nguyen et al. (2012) discuss proof trees facts deduced owlontologies explained natural language. would particularly interestingexamine deduction explanation mechanisms could added Naturalowl.3. Processing Stages Resources NaturalOWLNaturalowl adopts pipeline architecture, common nlg (Reiter & Dale, 2000),though number purpose components often vary (Mellish, Scott, Cahill, Paiva,Evans, & Reape, 2006). system generates texts three stages, document planning,micro-planning, surface realization, discussed following sections; see Figure 1.3.1 Document PlanningDocument planning consists content selection, system selects informationconvey, text planning, plans structure text generated.3.1.1 Content Selectioncontent selection, system first retrieves ontology owl statementsrelevant class individual described, converts selected676fiGenerating Natural Language Descriptions OWL Ontologiesowl statements message triples, easier express sentences, finallyselects among message triples ones expressed.OWL statements individual targetsLet us first consider content selection Naturalowl asked describe individual(an entity), let us call individual target. system scans owl statementsontology, looking statements forms listed left column Table 1.8effect, retrieves statements describe target directly, opposedstatements describing another individual (named) class target related to.owl allows arbitrarily many nested ObjectUnionOf ObjectIntersectionOf operators, may lead statements difficult express natural language.simplify text generation ensure resulting texts easy comprehend,allow nested ObjectIntersectionOf ObjectUnionOf operators ontologies texts generated from. Table 1, restriction enforced requiring classidentifiers appear points owl also allows expressions construct unnamed classes using operators. ontology uses unnamed classes points Table 1requires class identifiers (named classes), easily modified comply Table 1defining new named classes nested unnamed ones.9 practice, nested ObjectUnionOfObjectIntersectionOf operators rare; see work Power et al. (Power, 2010;Power & Third, 2010; Power, 2012) information frequencies different typesowl statements.10Statements form ClassAssertion(Class target ) may quite complex,Class necessarily class identifier. may also expression constructingunnamed class, following example. multiple rowsClassAssertion Table 1.ClassAssertion(ObjectIntersectionOf(:WineObjectHasValue(:locatedIn :stEmilionRegion)ObjectHasValue(:hasColor :red)ObjectHasValue(:madeFrom :cabernetSauvignonGrape):chateauTeyssier2007)ObjectHasValue(:hasFlavor :strong)ObjectMaxCardinality(1 :madeFrom))Naturalowl would express owl statement generating text like following.2007 Chateau Teyssier wine St. Emilion region. red color strong flavor.made exactly one grape variety: Cabernet Sauvignon grapes.Recall texts Naturalowl intended read end-users. Hence,prefer generate texts may emphasize enough subtleties owl8. owl statements shown Table 1 two arguments actually arguments,converted forms shown.9. also easy automatically detect nested unnamed classes replace them, automatically,new named classes (classes owl identifiers). domain author would consulted,though, provide meaningful owl identifiers new classes (otherwise arbitrary identifiers wouldused) natural language names new classes (see Section 3.2.1 below).10. One could also refactor nested operators; example, ((A B) (C D)) equivalent(A B) (C D). conversion message triples, discussed below, effectalso performs refactoring, cannot cope possible nested union intersectionoperators, disallow general rule.677fiAndroutsopoulos, Lampouras, & Galanisowl statementsMessage triplesClassAssertion(NamedClass target )ClassAssertion(ObjectComplementOf(NamedClass ) target )ClassAssertion(ObjectOneOf(indiv1 indiv2 ...) target )ClassAssertion(ObjectHasValue(objProp indiv ) target )ClassAssertion(ObjectHasValue(dataProp dataValue ) target )ClassAssertion(ObjectHasSelf(objProp ) target )ClassAssertion(ObjectMaxCardinality(number prop [NamedClass ])target )ClassAssertion(ObjectMinCardinality(number prop [NamedClass ])target )ClassAssertion(ObjectExactCardinality(number prop [NamedClass ])target )ClassAssertion(ObjectSomeValuesFrom(objProp NamedClass ) target )ClassAssertion(ObjectAllValuesFrom(objProp NamedClass ) target )ClassAssertion(ObjectIntersectionOf(C1 C2 ...) target )<target, instanceOf, NamedClass >ClassAssertion(ObjectUnionOf(C1 C2 ...)target )ObjectPropertyAssertion(objProp target indiv )DataPropertyAssertion(dataProp target dataValue )NegativeObjectPropertyAssertion(objProp target indiv )NegativeDataPropertyAssertion(dataProp target dataValue )DifferentIndividuals(target indiv )DifferentIndividuals(indiv target )SameIndividual(target indiv )SameIndividual(indiv target )<target, not(instanceOf), NamedClass ><target, oneOf,or(indiv1, indiv2, ...)><target, objProp, indiv ><target, dataProp, dataValue ><target, objProp, target ><target, maxCardinality(prop ),number [:NamedClass ]><target, minCardinality(prop ),number [:NamedClass ]><target, exactCardinality(prop ),number [:NamedClass ]><target, someValuesFrom(objProp ),NamedClass ><target, allValuesFrom(objProp ),NamedClass >convert (ClassAssertion(C1 target ))convert (ClassAssertion(C2 target )) ...or(convert (ClassAssertion(C1 target )),convert (ClassAssertion(C2 target )),...)<target, objProp, indiv ><target, dataProp, dataValue ><target, not(objProp ), indiv ><target, not(dataProp ), dataValue ><target,<target,<target,<target,differentIndividuals, indiv >differentIndividuals, indiv >sameIndividual, indiv >sameIndividual, indiv >Notation: Square brackets indicate optional arguments, convert () recursive applicationconversion . NamedClass class identifier; objProp , dataProp , prop identifiers objectproperties, datatype properties, properties; indiv , indiv1 , . . . identifiers individuals;dataValue datatype value; C , C1 , . . . class identifiers, expressions constructing classeswithout ObjectIntersectionOf ObjectUnionOf.Table 1: owl statements individual target, corresponding message triples.statements, order produce readable texts. owl expert might prefer,example, following description chateauTeyssier2007, mirrors closelycorresponding owl statements.2007 Chateau Teyssier member intersection of: (a) class wines, (b) classindividuals (not necessarily exclusively) St. Emilion region, (c) class individuals(not necessarily exclusively) red color, (d) class individuals (not necessarily exclusively)strong flavor, (e) class individuals made exclusively Cabernet Sauvignon grapes.678fiGenerating Natural Language Descriptions OWL OntologiesStricter texts kind, however, seem inappropriate end-users. fact, couldargued even mentioning wine made exactly one grape varietytext Naturalowl produces inappropriate end-users. system instructedavoid mentioning information via user modeling annotations, discussed below.OWL statements class targetssystem asked describe class, rather individual, scans ontologystatements forms listed left column Table 2. class describedmust named one, meaning must owl identifier, Target denotesidentifier. Again, simplify generation process avoid producing complicatedtexts, Table 2 requires class identifiers appear points owl also allowsexpressions construct unnamed classes using operators. ontology uses unnamedclasses points Table 2 requires class identifiers, easily modified.texts describing classes, difficult express informally differenceEquivalentClasses SubClassOf. EquivalentClasses(C1 C2 ) means individualC1 also belongs C2 , vice versa. contrast, SubClassOf(C1 C2 ) meansmember C1 also belongs C2 , reverse necessarily true. replaceEquivalentClasses SubClassOf definition StEmilion page 672, memberStEmilion still necessarily also member intersection, winecharacteristics intersection necessarily member StEmilion. Consequently,one perhaps add sentences like ones shown italics below, expressingEquivalentClasses SubClassOf, respectively.St. Emilion kind Bordeaux St. Emilion region. red color strong flavor.made exactly one grape variety: Cabernet Sauvignon grapes. Every St. Emilion properties,anything properties St. Emilion.St. Emilion kind Bordeaux St. Emilion region. red color strong flavor.made exactly one grape variety: Cabernet Sauvignon grapes. Every St. Emilion properties,something may properties without St. Emilion.Naturalowl produces texts, without sentences italics, SubClassOfEquivalentClasses, avoid generating texts sound formal. Also, maymention information ontology target class (e.g., St.Emilion strong flavor), user modeling indicates information alreadyknown text exceed particular length. Hence, generated textsexpress necessary, sufficient conditions individuals belong target class.OWL statements second-level targetsapplications, expressing additional owl statements indirectly relatedtarget may desirable. Let us assume, example, target individualexhibit24, following directly relevant statements retrievedontology. Naturalowl would express generating text like one below.ClassAssertion(:Aryballos :exhibit24)ObjectPropertyAssertion(:locationFound :exhibit24 :heraionOfDelos)ObjectPropertyAssertion(:creationPeriod :exhibit24 :archaicPeriod)ObjectPropertyAssertion(:paintingTechniqueUsed :exhibit24 :blackFigureTechnique)ObjectPropertyAssertion(:currentMuseum :exhibit24 :delosMuseum)679fiAndroutsopoulos, Lampouras, & Galanisowl statementsMessage triplesEquivalentClasses(Target C )EquivalentClasses(C Target )SubClassOf(Target NamedClass )SubClassOf(Target ObjectComplementOf(NamedClass ))SubClassOf(TargetObjectOneOf(indiv1 indiv2 ...))SubClassOf(Target ObjectHasValue(objProp indiv ))SubClassOf(TargetObjectHasValue(dataProp dataValue ))SubClassOf(Target ObjectHasSelf(objProp ))SubClassOf(TargetObjectMaxCardinality(number prop [NamedClass ]))SubClassOf(TargetObjectMinCardinality(number prop [NamedClass ]))SubClassOf(TargetObjectExactCardinality(number prop [NamedClass ]))SubClassOf(TargetObjectSomeValuesFrom(objProp NamedClass ))SubClassOf(TargetObjectAllValuesFrom(objProp NamedClass ))SubClassOf(TargetObjectIntersectionOf(C1 C2 ...))convert (SubClassOf(Target C ))convert (SubClassOf(Target C ))<Target, isA, NamedClass ><Target, not(isA), NamedClass ><Target, oneOf,or(indiv1, indiv2, ...)><Target, objProp, indiv >SubClassOf(TargetObjectUnionOf(C1 C2 ...))DisjointClasses(Target NamedClass )DisjointClasses(NamedClass Target )<Target, dataProp, dataValue ><Target, objProp, Target ><Target, maxCardinality(prop ),number [:NamedClass ]><Target, minCardinality(prop ),number [:NamedClass ]><Target, exactCardinality(objProp ),number [:NamedClass ]><Target, someValuesFrom(objProp ),NamedClass ><Target, allValuesFrom(objProp ),NamedClass >convert (SubClassOf(C1 Target ))convert (SubClassOf(C2 Target )) ...or(convert (SubClassOf(C1 Target )),convert (SubClassOf(C2 Target )),...)<Target, not(isA), NamedClass ><Target, not(isA), NamedClass >Notation: Square brackets indicate optional arguments, convert () recursive applicationconversion . NamedClass class identifier; objProp , dataProp , prop identifiers objectproperties, datatype properties, properties; indiv , indiv1 , . . . identifiers individuals;dataValue datatype value; C , C1 , . . . class identifiers, expressions constructing classeswithout ObjectIntersectionOf ObjectUnionOf.Table 2: owl statements class target, corresponding message triples.aryballos, found Heraion Delos. created archaic perioddecorated black-figure technique. currently Museum Delos.names classes individuals shown hyperlinks indicateused subsequent targets. Clicking hyperlink would request describecorresponding class individual. Alternatively, may retrieve advance owlstatements subsequent targets add current target.precisely, assuming target individual, subsequent targets, calledsecond-level targets, targets class, provided named one, individuals target directly linked via object properties. Naturalowl considers second-leveltargets current target individual, class targets, second-leveltargets often lead complicated texts. retrieve owl statements currentsecond-level targets (when applicable), current target, setmaximum fact distance 2 1, respectively. Returning exhibit24, let us assumemaximum fact distance 2 following owl statements second-leveltargets retrieved.1111. Consult http://www.w3.org/TR/owl-time/ principled representations time owl.680fiGenerating Natural Language Descriptions OWL OntologiesSubClassOf(:Aryballos :Vase)SubClassOf(:AryballosObjectHasValue(:exhibitTypeCannedDescription"An aryballos small spherical vase narrow neck, athleteskept oil spread bodies with"^^xsd:string))DatatypePropertyAssertion(:periodDuration :archaicPeriod "700 BC 480 BC"^^xsd:string)DatatypePropertyAssertion(:periodCannedDescription :archaicPeriod"The archaic period Greek ancient city-states developed"^^xsd:string)DataPropertyAssertion(:techniqueCannedDescription :blackFigureTechnique"In black-figure technique, silhouettes rendered black palesurface clay, details engraved"^^xsd:string)express retrieved owl statements, including second-level targets,Naturalowl would generate text like following, may preferable,first time user encounters aryballos archaic exhibits.aryballos, kind vase. aryballos small spherical vase narrow neck,athletes kept oil spread bodies with. aryballos found HeraionDelos created archaic period. archaic period Greek ancient citystates developed spans 700 bc 480 bc. aryballos decorated black-figuretechnique. black-figure technique, silhouettes rendered black pale surfaceclay, details engraved. aryballos currently Museum Delos.note many ontologies impractical represent informationlogical terms. example, much easier store information aryballossmall. . . bodies string, i.e., canned sentence, rather definingclasses, properties, individuals spreading actions, bodies, etc. generatingsentence logical meaning representation. Canned sentences, however,entered multiple versions, several languages user types need supported.Converting OWL statements message triplesTables 1 2 also show retrieved owl statements rewritten triplesform hS, P, Oi, target second-level target; individual,datatype value, class, set individuals, datatype values, classes mappedto; P specifies kind mapping. call semantic subject ownertriple, semantic object filler ; triple also viewed field namedP , owned S, filled O. example, owl statements exhibit24 shownabove, including second-level targets, converted following triples.<:exhibit24, instanceOf, :Aryballos><:exhibit24, :locationFound, :heraionOfDelos><:exhibit24, :creationPeriod, :archaicPeriod><:exhibit24, :paintingTechniqueUsed, :blackFigureTechnique><:exhibit24, :currentMuseum, :delosMuseum><:Aryballos, isA, :Vase><:Aryballos, :exhibitTypeCannedDescription, "An aryballos a... bodies with"^^xsd:string><:archaicPeriod, :periodDuration, "700 BC 480 BC"^^xsd:string><:archaicPeriod, :periodCannedDescription, "The archaic period was..."^^xsd:string><:blackFigureTechnique, :techniqueCannedDescription, "In black-figure..."^^xsd:string>precisely, P be: (i) property ontology; (ii) one keywords isA,instanceOf, oneOf, differentIndividuals, sameIndividuals; (iii) expressionform modifier(), modifier may not, maxCardinality etc. (see Tables 1 2)681fiAndroutsopoulos, Lampouras, & GalanisFigure 2: Graph view message triples.property ontology. hereafter call properties three types P ,though types (ii) (iii) strictly properties terminology owl.need distinguish three types, use terms property ontology,domain-independent property, modified property, respectively.Every owl statement collection owl statements represented set rdftriples.12 triples Tables 12 similar, rdf triples. notably, expressions form modifier() cannot used P rdf triples. avoid confusion, call message triples triples Tables 12, distinguish rdf triples.rdf triples, message triples viewed forming graph. Figure 2 showsgraph message triples exhibit24; triple linking blackFigureTechniquecanned sentence shown save space. second-level targets classesindividuals distance one target (exhibit24).13 contrast, graphrdf triples representing owl statements would complicated, second-leveltargets would always distance one target.message triple intended easily expressible simple sentence,always case rdf triples representing owl statements. message triples alsocapture similarities sentences generated may less obvious lookingoriginal owl statements rdf triples representing them. example,ClassAssertion SubClassOf statements mapped identical message triples,apart identifiers individual class, similarity messagetriples reflects similarity resulting sentences, also shown below.ClassAssertion(ObjectMaxCardinality(1 :madeFromGrape) :product145)<:product145, maxCardinality(:madeFromGrape), 1>Product 145 made one grape.12. See http://www.w3.org/TR/owl2-mapping-to-rdf/.13. Instead retrieving owl statements target second-level targets convertingmessage triples, one could equivalently convert owl statements ontology messagetriples select message triples connecting target nodes distance two target.682fiGenerating Natural Language Descriptions OWL OntologiesSubClassOf(:StEmilion ObjectMaxCardinality(1 :madeFromGrape))<:StEmilion, maxCardinality(:madeFromGrape), 1>St. Emilion made one grape.contrast, without conversion message triples, owl statements rdftriples representing would lead difficult follow sentences like following:Product 145 member class individuals made one grape.St. Emilion subclass class individuals made one grape.example, Tables 1 2 discard ObjectIntersectionOf operators, producingmultiple message triples instead. example, EquivalentClasses statement definingStEmilion page 672 would converted following message triples.<:StEmilion,<:StEmilion,<:StEmilion,<:StEmilion,<:StEmilion,<:StEmilion,isA, :Bordeaux>:locatedIn, :stEmilionRegion>:hasColor, :red>:hasFlavor, :strong>:madeFromGrape, :cabernetSauvignonGrape>maxCardinality(:madeFromGrape), 1>resulting message triples correspond sentences below, subsequent references StEmilion replaced pronouns improve readability; sentencescould also aggregated longer ones, discussed later sections.St. Emilion kind Bordeaux. St. Emilion region. red color. strong flavor.made Cabernet Sauvignon grape. made one grape variety.contrast original owl statement page 672 rdf triples representingwould lead stricter text page 678, inappropriate end-users, alreadynoted. Notice, also, Table 2 converts EquivalentClasses SubClassOf statementsidentical triples, P isA, since Naturalowl produces texts kindsstatements, already discussed.Tables 1 2 also replace ObjectUnionOf operators disjunctions message triples.following owl statement mapped message triple shown below:ClassAssertion(UnionOf(ObjectHasValue(:hasFlavor :strong) ObjectHasValue(:hasFlavor :medium)):houseWine)or(<:houseWine, :hasFlavor, :strong>, <:houseWine, :hasFlavor, :medium>)leads first sentence below; sentence shortened aggregation, leading second sentence below.house wine strong flavor medium flavor.house wine strong medium flavor.683fiAndroutsopoulos, Lampouras, & Galaniscontrast, owl statement corresponding rdf triples effect say that:house wine member union of: (i) class wines strong flavor, (ii)class wines medium flavor.Interest scores repetitionsExpressing message triples retrieved owl statements alwaysappropriate. Let us assume, example, maximum fact distance 2description exhibit24 Figure 2 requested museum visitor. maycase visitor already encountered archaic exhibits, durationarchaic period mentioned previous descriptions. Repeating durationperiod may, thus, undesirable. may also want exclude message triplesuninteresting particular types users. example, may message triplesproviding bibliographic references, children would probably find uninteresting.Naturalowl provides mechanisms allowing domain author assign importancescore every possible message triple, possibly different scores different user types(e.g., adults, children). score non-negative integer indicating interesting usercorresponding type presumably find information message triple,information already conveyed user. museum projects Naturalowloriginally developed for, interest scores ranged 0 (completely uninteresting)3 (very interesting), different range also used. scores specifiedmessage triples involve particular property P (e.g., P = madeFrom),message triples involve semantic subjects particular class (e.g., Statue= Statue) particular property P , message triples involve particularsemantic subjects (e.g., =exhibit37) particular property P . example, maywish specify materials exhibits collection generally mediuminterest (P = madeFrom, score 2), materials statues lower interest (Sstatue, P = madeFrom, score 1), perhaps statues collection madestone, material particular statue exhibit24 important (S =exhibit10, P = madeFrom, score 3), perhaps exhibit24 gold statue.discuss mechanisms used assign interest scores messagetriples article, detailed description mechanisms found elsewhere(Androutsopoulos et al., 2012). also note human-authored texts describingindividuals classes ontology available along owl statements or,generally, logical facts express, statistical machine learning methodsemployed learn automatically select assign interest scores logical facts (Duboue &McKeown, 2003; Barzilay & Lapata, 2005; Kelly, Copestake, & Karamanis, 2010). Anotherpossibility (Demir, Carberry, & McCoy, 2010) would compute interest scoresgraph algorithms like PageRank (Brin & Page, 1998).domain author also specify many times message triplerepeated, assumed users different types assimilated it.triple assimilated, never repeated texts user. example,domain author specify children assimilate duration historical periodmentioned twice; hence, system may repeat, example, durationarchaic period two texts. Naturalowl maintains personal model end-user.model shows message triples conveyed particular user previous684fiGenerating Natural Language Descriptions OWL Ontologiestexts, many times. Again, information user modeling mechanismsNaturalowl found elsewhere (Androutsopoulos et al., 2012).Selecting message triples conveyasked describe target, Naturalowl first retrieves ontology relevant owl statements, possibly also second-level targets. converts retrievedstatements message triples, consults interest scores personal user models rank message triples decreasing interest score, discarding triplesalready assimilated. message triple target assimilated,message triples second-level targets connected assimilated triplealso discarded; example, creationPeriod triple (edge) Figure 2 assimilated, triples archaic period (the edges leaving archaicPeriod)also discarded. system selects maxMessagesPerPage triplesinteresting remaining ones; maxMessagesPerPage parameter whose value setsmaller larger values types users prefer shorter longer texts, respectively.Limitations content selectionowl allows one define broadest possible domain range particular property,using statements like following.ObjectPropertyDomain(:madeFrom :Wine)ObjectPropertyRange(:madeFrom :Grape)practice, specific range restrictions imposed particular subclassespropertys domain. example, following statements specify madeFromused individuals subclass GreekWine Wine, range (possible values)madeFrom restricted individuals subclass GreekGrape Grape.SubClassOf(:GreekWine :Wine) SubClassOf(:GreekGrape :Grape)SubClassOf(:GreekWine AllValuesFrom(:madeFrom :GreekGrape))Naturalowl considers AllValuesFrom similar restrictions (see Tables 1 2),ObjectPropertyDomain ObjectPropertyRange statements. latter typically providegeneral and, hence, uninteresting information perspective end-users.generally, Naturalowl consider owl statements express axiomsproperties, meaning statements declaring property symmetric, asymmetric,reflexive, irreflexive, transitive, functional, inverse functional, propertyinverse of, disjoint another property, subsumed chainproperties, subproperty (more specific) another property. Statementskind mostly useful consistency checks, deduction, generating textsdescribing properties (e.g., grandparent somebody means).143.1.2 Text Planningtarget, previous mechanisms produce message triples expressed,triple intended easily expressible single sentence. text plannerNaturalowl orders message triples, effect ordering corresponding sentences.14. Subproperties without sentence plans, discussed below, could inherit sentence plans superproperties, case automatically extract sentence plans ontology instead.685fiAndroutsopoulos, Lampouras, & GalanisGlobal local coherenceconsidering global coherence, text planners attempt build structure, usuallytree, shows clauses, sentences, larger segments text relatedother, often terms rhetorical relations (Mann & Thompson, 1998). allowedpreferred orderings sentences (or segments) often follow, least partially,global coherence structure. texts, however, Naturalowl intendedgenerate, global coherence structures tend rather uninteresting,sentences simply provide additional information target second-leveltargets, global coherence considered Naturalowl.15considering local coherence, text planners usually aim maximize measuresexamine whether adjacent sentences (or segments) continue focus entities or, focus changes, smooth transition is. Many local coherence measuresbased Centering Theory (ct) (Grosz, Joshi, & Weinstein, 1995; Poesio, Stevenson,& Di Eugenio, 2004). Consult work Karamanis et al. (2009) introductionct ct-based analysis m-piros texts, also applies texts Naturalowl.maximum fact distance Naturalowl 1, sentence-to-sentence transitions type known ct continue, preferred type. maximumfact distance 2, however, transitions always continue. repeatlong aryballos description page 681 without sentence aggregation. readers familiarct, show italics salient noun phrase sentence un , realizesdiscourse entity known preferred center cp (un ). underlined noun phrases realize backward looking center cb (un ), roughly speaking salient discourse entityprevious sentence also mentioned current sentence.(1) (exhibit) aryballos. (2) aryballos kind vase. (3) aryballos smallspherical vase narrow neck, athletes kept oil spread bodies with.(4) aryballos found Heraion Delos. (5) created archaic period. (6)archaic period Greek ancient city-states developed. (7) spans 700 bc 480bc. (8) aryballos decorated black-figure technique. (9) black-figure technique,silhouettes rendered black pale surface clay, details engraved. (10)aryballos currently Museum Delos.sentence 4, cp (u4 ) target exhibit, cb (u4 ) undefined transitionsentence 3 4 nocb, type transition avoided; mark nocb transitionsbullets. sentence 6, cp (u6 ) = cb (u6 ) 6= cb (u5 ), kind transition knownsmooth-shift (Poesio et al., 2004), less preferred continue, better nocb.Another nocb occurs sentence 7 8, followed smooth-shift sentence 89, another nocb sentence 9 10. transitions continue.text planner Naturalowl groups together sentences (message triples) describe particular second-level target (e.g., sentences 23, 67, 9) placesgroup immediately sentence introduces corresponding second-level target(immediately sentences 1, 5, 8). Thus transition sentence introduces second-level target first sentence describes second-level target (e.g.,15. Liang et al. (2011a) Power (2011) seem agree rhetorical relations relevantgenerating texts owl ontologies.686fiGenerating Natural Language Descriptions OWL Ontologiessentence 1 2, 5 6, 8 9) smooth-shift (or continuespecial case initial sentence 1 2). nocb occurs sentences returnproviding information primary target, group sentences provideinformation second-level target. transitions type continue.simple strategy avoid nocb transitions would end generated textmessage triples describe second-level target reported, recorduser model message triples content selection providedactually conveyed. example, would generate sentences 1 3; userrequested information exhibit, sentences 4 7 would generated etc.Topical orderordering sentences, also need consider topical similarity adjacentsentences. Compare, example, following two texts.{locationSection Stoa Zeus Eleutherios located western part Agora. locatednext Temple Apollo Patroos.} {buildSection built around 430 bc. built Doricstyle. built porous stone marble.} {useSection used Classical period,Hellenistic period, Roman period. used religious place meeting point.}{conditionSection destroyed late Roman period. excavated 1891 1931. Todaygood condition.}Stoa Zeus Eleutherios built Doric style. excavated 1891 1931.built porous stone marble. located western part Agora. destroyedlate Roman period. used religious place meeting point. located nextTemple Apollo Patroos. built around 430 bc. Today good condition. usedClassical period, Hellenistic period, Roman period.Even though texts contain sentences, second text difficultfollow, acceptable. first one better, groups together topicallyrelated sentences. mark sentence groups first text curly brackets,brackets would shown end-users. longer texts, sentence groups may optionallyshown separate paragraphs sections, call sections.allow message triples (and corresponding sentences) grouped topic,domain author may define sections (e.g., locationSection, buildSection) assignproperty single section (e.g., assign properties isInArea isNextTolocationSection). message triple placed section property.ordering sections properties inside section also specified, causing message triples ordered accordingly (e.g., may specify locationSectionprecede buildSection, inside locationSection, isInArea propertyexpressed isNextTo). sections, assignments properties sections,order sections properties defined domain-dependent generation resources (Androutsopoulos et al., 2012).overall text planning algorithmNaturalowls text planning algorithm summarized Figure 3. message triplesordered include triples describe second-level targets, i.e., triples hS, P, Oi whoseowner second-level target, triples primary second-level target687fiAndroutsopoulos, Lampouras, & Galanisprocedure orderMessageTriplesinputs:t[0]: primary targett[1], ..., t[n]: second-level targetsL[0]: unordered list triples describing t[0]...L[n]: unordered list triples describing t[n]SMap: mapping properties sectionsSOrder: partial order sectionsPOrder: partial order properties within sectionsoutput:ordered list message triplessteps::= 0 n { orderMessageTriplesAux(L[i], SMap, SOrder, POrder) }:= 1 n { insertAfterFirst(<t[0], _, t[i]>, L[0], L[i]) }return L[0]procedure orderMessageTriplesAuxinputs:L: unordered list triples single targetSMap: mapping properties sectionsSOrder: partial order sectionsPOrder: partial order properties within sectionslocal variables:S[1], ..., S[k]: lists, triples one sectionoutput:ordered list message triples single targetsteps:<S[1], ..., S[k]> := splitInSections(L, SMap):= 1 k { S[i] := orderTriplesInSection(S[i], POrder) }<S[1], ..., S[k]> := reorderSections(S[1], ..., S[k], SOrder)return concatenate(S[1], ..., S[k])Figure 3: overall text planning algorithm Naturalowl.ordered separately, using ordering properties sections. ordered triplessecond-level target inserted ordered list primary target triples,immediately first triple introduces second-level target, i.e., immediatelyfirst triple whose second-level target.related work text planningordering properties sections similar text schemata (McKeown, 1985),roughly speaking domain-dependent patterns specify possible arrangements different types sentences (or segments). Sentence ordering studied extensivelytext summarization (Barzilay, Elhadad, & McKeown, 2002). Duboue McKeown (2001)discuss methods could used learn order sentences segmentsnlg semantically tagged training corpora. Consult also work Barzilay Lee(2004), Elsner et al. (2007), Barzilay Lapata (2008), Chen et al. (2009).688fiGenerating Natural Language Descriptions OWL OntologiesFigure 4: lexicon entry verb find.3.2 Micro-planningprocessing stages discussed far select order message triplesexpressed. next stage, micro-planning, consists three sub-stages: lexicalization,sentence aggregation, generation referring expressions; see also Figure 1 page 676.3.2.1 Lexicalizationlexicalization, nlg systems usually turn output content selection (in case,message triples) abstract sentence specifications. Naturalowl, every propertyontology every supported natural language, domain author may specify onetemplate-like sentence plans indicate message triples involving propertyexpressed. discuss sentence plans specified, first slightdeviation necessary, briefly discuss lexicon entries Naturalowl.Lexicon entriesverb, noun, adjective domain author wishes use sentenceplans, lexicon entry provided, specify inflectional forms word.16lexicon entries multilingual (currently bilingual); allows sentence plansreused across similar languages better option available, discussed elsewhere(Androutsopoulos et al., 2007). Figure 4 shows lexicon entry verb whose Englishbase form find, viewed domain author using Protege plug-inNaturalowl. identifier lexicon entry toFindLex. English part entryshows base form find, simple past found etc. Similarly, Greekpart lexicon entry would show base form corresponding verb ()inflectional forms various tenses, persons etc. lexicon entries nounsadjectives similar.English inflectional forms could automatically produced baseforms using simple morphology rules. hope exloit existing English morphologycomponent, simplenlg (Gatt & Reiter, 2009), future work. Similarmorphology rules Greek used authoring tool m-piro (Androutsopoulos etal., 2007), hope include future version Naturalowl. Rules kindwould reduce time domain author spends creating lexicon entries. ontologiesconsidered, however, dozens lexicon entries verbs, nouns, adjectives16. lexicon entries need provided closed-class words, like determiners prepositions.689fiAndroutsopoulos, Lampouras, & Galanissuffice. Hence, even without facilities automatically produce inflectional forms, creatinglexicon entries rather trivial. Another possibility would exploit general-purposelexicon lexical database, like WordNet (Fellbaum, 1998) celex, though resourceskind often cover highly technical concepts ontologies.17lexicon entries and, generally, domain-dependent generation resourcesNaturalowl stored instances owl ontology (other ontology textsgenerated from) describes linguistic resources system (Androutsopouloset al., 2012). domain author, however, interacts plug-in needaware owl representation resources. representing domain-dependentgeneration resources owl, becomes easier publish Web, checkinconsistencies etc., owl ontologies.Sentence plansNaturalowl, sentence plan sequence slots, along instructions specifying fill in. Figure 5 shows English sentence plan propertyusedDuringPeriod, viewed domain author using Protege plug-in Naturalowl. sentence plan expresses message triples form hS, usedDuringPeriod, Oiproducing sentences like following.[slot1 stoa] [slot2 used] [slot3 during] [slot4 Classical period].[slot1 Stoa Zeus Eleutherios] [slot2 used] [slot3 during] [slot4 Classical period, Hellenisticperiod, Roman period].first slot sentence plan Figure 5 filled automaticallygenerated referring expression owner (S) triple. example, tripleexpress <:stoaZeusEleutherios, :usedDuringPeriod, :classicalPeriod>, appropriatereferring expression may demonstrative noun phrase like stoa, pronoun(it), monuments natural language name (the Stoa Zeus Eleutherios).discuss generation referring expressions below, along mechanisms specifynatural language names. sentence plan also specifies referring expression mustnominative case (e.g., stoa, opposed genitive case expressionsstoas, stoas height 5 meters).second slot filled form verb whose lexicon identifiertoUseVerb. verb form must simple past passive voice, positive polarity(as opposed used). number must agree number expressionfirst slot; example, want generate Stoa Zheus Eleutheriosused, Stoas used. third slot filled preposition during.fourth slot filled expression filler (O) message triple,accusative case.18 <:stoaZeusEleutherios, :usedDuringPeriod, :classicalPeriod>,slot would filled natural language name classicalPeriod.19sentence plan also allows resulting sentence aggregated sentences.17. See http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC96L14 celex.18. English prepositions usually require noun phrase complements accusative (e.g., him). Greeklanguages, cases noticeable effects.19. Future versions Naturalowl may allow referring expression natural languagename produced (e.g., pronoun), S.690fiGenerating Natural Language Descriptions OWL OntologiesFigure 5: sentence plan property usedDuringPeriod.generally, instructions sentence plan may indicate slotfilled one following (ivii):(i) referring expression (owner) message triple. sentence plan mayspecify particular type referring expression use (e.g., always use natural languagename S) or, example Figure 5, may allow system automaticallyproduce appropriate type referring expression depending context.(ii) verb lexicon entry, particular form, possibly formagrees another slot. polarity verb also manually specified or,filler (O) message triple Boolean value, polarity automatically setmatch value (e.g., produce built-in flash false).(iii) noun adjective lexicon, particular form (e.g., case, number),form agrees another slot.(iv) preposition (v) fixed string.(vi) expression (filler) triple. individual class,expression natural language name O; datatype value (e.g., integer),value inserted slot; similarly disjunction conjunctiondatatype values, individuals, classes.(vii) concatenation property values O, provided individual.example, may need express message triple like first one below, whose (anonymousrdf sense) object :n linked numeric value (via hasAmount)individual standing currency (via hasCurrency).<:tecra8, :hasPrice, _:n><_:n, :hasAmount, "850"^^xsd:float><_:n, :hasCurrency, :euroCurrency>would want sentence plan include slot filled concatenationhasAmount value :n natural language name hasCurrency value :n (e.g.,850 Euro English, 850 Greek).Default sentence plansentence plan provided particular property ontology, Naturalowl uses default sentence plan, consisting three slots. first slot filledautomatically generated referring expression owner (S) triple,nominative case. second slot filled tokenized form owl identifierproperty. third slot filled appropriate expression filler (O)691fiAndroutsopoulos, Lampouras, & Galanistriple, discussed above, accusative case (if applicable). following messagetriple, default sentence plan would produce sentence shown below:<:stoaZeusEleutherios, :usedDuringPeriod, and(:classicalPeriod, :hellenisticPeriod, :romanPeriod)>Stoa zeus eleutherios used period classical period, hellenistic period, roman period.Notice use single message triple and(...) filler, instead differenttriple period. kind triple merging effect form aggregation, discussedbelow, takes place content selection. Also, assumed sentencenatural language names individuals provided either;case, Naturalowl uses tokenized forms owl identifiers individuals instead.tokenizer Naturalowl handle CamelCase (e.g., :usedDuringPeriod)underscore style (e.g., :used period). styles used identifiersproperties, classes, individuals, output tokenizer may worseexample suggests, resulting sentences improved providing sentence plansassociating classes individuals natural language names, discussed below.Using rdfs:label stringsowl properties (and elements owl ontologies) labeled stringsmultiple natural languages using rdfs:label annotation property, defined rdfowl standards. example, usedDuringPeriod property could labeledused shown below; could similar labels Greek languages.AnnotationAssertion(rdfs:label :usedDuringPeriod "was used during"@en)rdfs:label string specified property message triple, Naturalowluses string second slot default sentence plan. quality resultingsentences can, thus, improved, rdfs:label strings natural phrasestokenized property identifiers. rdfs:label shown above, default sentenceplan would produce following sentence.Stoa zeus eleutherios used classical period, hellenistic period, roman period.Even rdfs:label strings, default sentence plan may produce sentences disfluencies. Also, rdfs:label strings indicate grammatical categorieswords, allow system apply many sentence aggregation rulesdiscussed below. limitation default sentence plan allowslots preceded followed, respectively, phrase.Sentence plans domain-independent modified propertiesdomain author need provide sentence plans domain-independentproperties (e.g., instanceOf, isA, see Tables 12). properties fixed, domainindependent semantics; hence, built-in sentence plans used. English built-in sentence plans, also serve examples sentence plans, summarizedTable 3; Greek built-in sentence plans similar. save space show sentenceplans templates Table 3, show sentence plans negated domainindependent properties (e.g., not(isA)), similar. Additional slot restrictions692fiGenerating Natural Language Descriptions OWL OntologiesForms message triplesExample message triplescorresponding built-in sentence planspossible resulting sentences<S, instanceOf, ><:eos450d, instanceOf, :PhotographicCamera>ref(S) toBeVerb name(indef, O)eos 450d photographic camera.<S, instanceOf, ><:eos450d, instanceOf, :Cheap>ref(S) toBeVerb name(adj, O)eos 450d cheap.<S, oneOf, ><:WineColor, oneOf, or(:white, :rose, :red)>ref(S) toBeVerb name(O)wine color white, rose, red.<S, differentIndividuals, ><:n97, differentIndividuals, :n97mini>ref(S) toBeVerb identical name(O)n97 identical n97 mini.<S, sameIndividual, ><:eos450d, sameIndividual, :rebelXSi>ref(S) toBeVerb identical name(O)identical Rebel xsi.<S, isA, ><:StEmilion, isa, :Bordeaux>ref(S) toBeVerb kind name(noarticle, O)St. Emilion kind Bordeaux.<S, isA, ><:StEmilion, isa, :Red>ref(S) toBeVerb name(adj, O)St. Emilion red.Notation: ref() stands referring expression ; name() natural language name ;name(indef, ) name(noarticle, ) mean name noun phrase indefinitearticle. Sentence plans involving name(adj, ) used natural language namesequence one adjectives; otherwise sentence plan previous row used.Table 3: Built-in English sentence plans domain-independent properties.shown Figure 3 require, example, subject-verb number agreement verb forms(is was) present tense. Information provided specifying naturallanguage names individuals classes, discussed below, shows definite indefinitearticles articles used (e.g., n97 mini, exhibit 24, St. Emilion St. Emilion simply St. Emilion), default numbername (e.g., wine color Wine colors are). also possible modifybuilt-in sentence plans; example, museum context may wish generatearyballos kind vase instead aryballos kind vase.sentence plans modified properties (e.g., minCardinality(manufacturedBy), seeTables 12) also automatically produced, sentence plans unmodifiedproperties (e.g., manufacturedBy).Specifying appropriateness sentence plansMultiple sentence plans may provided property ontologylanguage. Different appropriateness scores (similar interest scores properties)assigned alternative sentence plans per user type. allows specifying,example, sentence plan generates sentences like amphora depicts Miltiades less appropriate interacting children, compared alternative sentenceplan common verb (e.g., shows). Automatically constructed sentence plansinherit appropriateness scores sentence plans constructed from.Related work sentence planssentence plans Naturalowl similar expressions sentence planning languages like spl (Kasper & Whitney, 1989) used generic surface realizers,fuf/surge (Elhadad & Robin, 1996), kpml (Bateman, 1997), realpro (Lavoie & Ram693fiAndroutsopoulos, Lampouras, & Galanisbow, 1997), nitrogen/halogen (Langkilde, 2000), openccg (White, 2006).sentence plans Naturalowl, however, leave fewer decisions subsequent stages.disadvantage sentence plans often include information could obtained large-scale grammars corpora (Wan, Dras, Dale, & Paris, 2010).hand, input generic surface realizers often refers non-elementary linguistic concepts (e.g., features particular syntax theory) concepts upper model(Bateman, 1990); latter high-level domain-independent ontology may usedifferent conceptualization ontology texts generated from. Hence,linguistic expertise, example Systemic Grammars (Halliday, 1994) case kpml(Bateman, 1997), effort understand upper model required. contrast,sentence plans Naturalowl require domain author familiar elementary linguistic concepts (e.g., tense, number), require familiarityupper model. sentence plans simpler than, example, templates Busemann Horacek (1999) McRoy et al. (2003), allow, instance,conditionals recursive invokation templates. See also work Reiter (1995)van Deemter et al. (2005) discussion template-based vs. principled nlg.corpora texts annotated message triples express available,templates also automatically extracted (Ratnaparkhi, 2000; Angeli, Liang, & Klein,2010; Duma & Klein, 2013). Statistical methods jointly perform content selection,lexicalization, surface realization also proposed (Liang, Jordan, & Klein,2009; Konstas & Lapata, 2012a, 2012b), currently limited generating singlesentences flat records.Specifying natural language namesdomain author assign natural language (nl) names individualsnamed classes ontology; recall named classes mean classes owlidentifiers. individual named class assigned nl name, rdfs:labeltokenized form identifier used instead. nl names domain authorprovides specified much sentence plans, i.e., sequences slots. example,may specify English nl name class ItalianWinePiemonte concatenationfollowing slots; explain slots below.[indef an] [adj Italian] [headnoun wine] [prep from] [def the] [noun Piemonte] [noun region]would allow Naturalowl generate sentence shown followingmessage triple; tokenized form identifier wine32 used.<:wine32, instanceOf, :ItalianWinePiemonte>Wine 32 Italian wine Piemonte region.Similarly, may assign following nl names individuals classicalPeriod, stoaZeusEleutherios, gl2011, classes ComputerScreen Red. Naturalowl makesdistinction common proper nouns; entered nouns lexicon,may multi-word (e.g., Zeus Eleutherios). Naturalowl also instructedcapitalize words particular slots (e.g., Classical).694fiGenerating Natural Language Descriptions OWL Ontologies[def the] [adj Classical] [headnoun period] , [def the] [headnoun stoa] [prep of] [noun Zeus Eleutherios],[headnoun GL-2011] , [indef a] [noun computer] [headnoun screen] , [headadj red]nl names could used express message triples shown below:<:stoaZeusEleutherios, :usedDuringPeriod,:classicalPeriod>Stoa Zeus Eleutherios used Classical period.<:gl2011, instanceOf, :ComputerScreen><:gl2011, instanceOf, :Red>GL-2011 computer screen. GL-2011 red.precisely, nl name sequence slots, accompanying instructionsspecifying slots filled in. slot filled with:(i) article, definite indefinite. article first slot (if present) treatedarticle overall nl name.(ii) noun adjective flagged head (main word) nl name. Exactly onehead must specified per nl name must lexicon entry. number casehead, also taken number case overall nl name,automatically adjusted per context. example, different sentence plans may requirenl name nominative case used subject, accusative usedobject verb; aggregation rules, discussed below, may require singularnl name turned plural. Using lexicon entries, list inflectional formsnouns adjectives, Naturalowl adjust nl names accordingly. genderhead adjectives also automatically adjusted, whereas gender head nounsfixed specified lexicon entries.(iii) noun adjective, among listed lexicon. nl name mayrequire particular inflectional form used, may require inflectional formagrees another slot nl name.(iv) preposition, (v) fixed string.sentence plans, domain author specifies nl names using Protegeplug-in Naturalowl. Multiple nl names specified individual class,assigned different appropriateness scores per user type; hence, differentterminology (e.g., common names diseases) used generating texts nonexperts, opposed texts experts (e.g., doctors). domain author also specify,using plug-in, nl names particular individuals classes involvedefinite, indefinite, articles, nl names singular pluraldefault. example, may prefer texts mention class aryballoi singleparticular generic object, using indefinite singular plural form, shown below.aryballos kind vase. aryballos kind vase. Aryballoi kind vase.695fiAndroutsopoulos, Lampouras, & Galanis3.2.2 Sentence Aggregationsentence plans previous section lead separate sentence message triple.nlg systems often aggregate sentences longer ones improve readability. Naturalowl, maximum number sentences aggregated form single longersentence specified per user type via parameter called maxMessagesPerSentence.museum contexts system originally developed for, setting maxMessagesPerSentence3 4 led reasonable texts adult visitors, whereas value 2 used children. sentence aggregation Naturalowl performed set manually craftedrules, intended domain-independent. claim set rules,initially based aggregation rules m-piro (Melengoglou, 2002), complete,hope extended future work; see, example, work Dalianis(1999) rich set aggregation rules.20 Nevertheless, current rules Naturalowlalready illustrate several aggregation opportunities arise generating textsowl ontologies.save space, discuss English sentence aggregation; Greek aggregation similar. show mostly example sentences aggregation, rules actuallyoperate sentence plans also consider message triples expressed.rules intended aggregate short single-clause sentences. Sentence plans producecomplicated sentences may flagged (using tickbox bottom Figure5) signal aggregation affect sentences. aggregation rules apply almost exclusively sentences adjacent ordering produced textplanner; exception aggregation rules involve sentences cardinalityrestrictions. Hence, depending ordering text planner mayfewer aggregation opportunities; see work Cheng Mellish (2000) related discussion. Also, aggregation rules Naturalowl operate sentences topicalsection, aggregating topically unrelated sentences often sounds unnatural.aggregation Naturalowl greedy. rules discussed below, startingdiscussed first, system scans original (ordered) sentences firstlast, applying rule wherever possible, provided rules application leadsentence expressing maxMessagesPerSentence original sentences. ruleapplied multiple ways, example aggregate two three sentences, applicationaggregates sentences without violating maxMessagesPerSentence preferred.Avoid repeating noun multiple adjectives: Message triples form hS, P, O1 i, . . . ,hS, P, aggregated single message triple hS, P, and(O1 , . . . , )i.nl names O1 , . . . , are, apart possible initial determiners, sequencesadjectives followed head noun, head noun need repeated.Let us consider following message triple. Assuming nl names threeperiods first sentence below, original sentence repeat period threetimes. aggregation rule omits last occurrence head noun.<:stoaZeusEleutherios, :usedDuringPeriod, and(:classicalPeriod, :hellenisticPeriod, :romanPeriod)>used Classical period, Hellenistic period, Roman period.usedClassical, Hellenistic, Roman period.20. appropriate corpora available, may also possible train aggregation modules (Walker,Rambow, & Rogati, 2001; Barzilay & Lapata, 2006).696fiGenerating Natural Language Descriptions OWL OntologiesCardinality restrictions values: set rules aggregate sentences (notnecessarily adjacent) express message triples form hS, (P ), Oi hS, P, Oi,P , minCardinality, maxCardinality, exactCardinality.rules applied, MaxMessagesPerSentence ignored. example, rulesperform aggregations like following.Model 35 sold three countries. Model 35 sold least three countries. Model 35 soldSpain, Italy, Greece.Model 35 sold exactly three countries: Spain, Italy, Greece.Class passive sentence: rule aggregates (i) sentence expressing message triplehS, instanceOf, Ci hS, isA, Ci (ii) passive immediately subsequent sentence expressing single triple form hS, P, Oi, S, P (unmodified) propertyontology. subject auxiliary verb second sentence omitted.Bancroft Chardonnay kind Chardonnay. made Bancroft.Bancroft Chardonnaykind Chardonnay made Bancroft.Class prepositional phrase: second sentence involves verbactive simple present, immediately followed preposition; conditionsprevious rule. subject verb second sentence omitted.Bancroft Chardonnay kind Chardonnay. Bancroft.Bancroft Chardonnay kindChardonnay Bancroft.Class multiple adjectives: rule aggregates (i) sentence formprevious two rules, (ii) one immediately preceding subsequent sentences,expressing single message triple hS, Pi , Oi i, S, Pi (unmodified)properties ontology. preceding subsequent sentences must involveverb active simple present, immediately followed adjective.adjectives absorbed sentence (i) maintaining order.motorbike. red. expensive.red, expensive motorbike.verb conjunction/disjunction: sequence sentences involving verbform, expressing single message triple hS, Pi , Oi i,triples Pi (unmodified) properties ontology, conjunction formedmentioning subject verb once. omitted preposition follows.medium body moderate flavor.born Athens. born 1918. born Athens 1918.medium body. moderate flavor.similar rule applies sentences produced disjunctions message triples, illustrated below. variant first aggregation rule also applied.house wine strong flavor medium flavor.flavor.house wine strong medium flavor.697house wine strong flavor mediumfiAndroutsopoulos, Lampouras, & GalanisDifferent verbs conjunction: sequence sentences, involvingverb form, expressing message triple hS, Pi , Oi i, triplesPi (unmodified) properties ontology, conjunction formed:Bancroft Chardonnay dry. moderate flavor. comes Napa.Bancroft Chardonnaydry, moderate flavor, comes Napa.3.2.3 Generating Referring Expressionssentence plan may require referring expression generated messagetriple hS, P, Oi. Depending context, may better, example, use nlname (e.g., Stoa Zeus Eleutherios), pronoun (e.g., it), demonstrativenoun phrase (e.g., stoa) etc. Similar alternatives could made available O,Naturalowl currently uses itself, datatype value; nl name O,tokenized identifier, rdfs:label, entity class; similarly conjunctionsdisjunctions O. Hence, focus referring expressions S.Naturalowl currently uses limited range referring expressions, includesnl names (or tokenized identifiers rdfs:label strings), pronouns, noun phrasesinvolving demonstrative nl name class (e.g., vase). example,referring expressions mention properties (e.g., vase Rome)generated. Although current referring expression generation mechanisms Naturalowlwork reasonably well, best viewed placeholders elaborate algorithms(Krahmer & van Deemter, 2012), especially algorithms based description logics (Areces,Koller, & Striegnitz, 2008; Ren, van Deemter, & Pan, 2010).Let us consider following generated text, expresses triples hSi , Pi , Oishown below. aggregate sentences section, illustrate casesreferring expressions needed; aggregation would reduce, however, number pronouns, making text less repetitive. readers familiar ct (Section 3.1.2),show italics noun phrase realizing cp (un ), show underlined noun phraserealizing cb (un ), mark nocb transitions bullets.(1) Exhibit 7 statue. (2) sculpted Nikolaou. (3) Nikolaou born Athens. (4)born 1918. (5) died 1998. (6) Exhibit 7 National Gallery. (7) excellentcondition.<:exhibit7,<:nikolaou,<:nikolaou,<:exhibit7,instanceOf, :Statue><:exhibit7, :hasSculptor, :nikolaou>:cityBorn, :athens><:nikolaou, :yearBorn, "1918"^^xsd:integer>:yearDied, "1998"^^xsd:integer><:exhibit7, :currentLocation, :nationalGallery>:currentCondition, :excellentCondition>Naturalowl pronominalizes Sn (for n > 1) Sn = Sn1 , sentences 2, 4, 5,7. Since typically cp (ui ) = Si , obtain cp (un ) = cp (un1 ), whenever Sn pronominalized, pronoun resolved reader intended. People tend prefer readingscp (un ) = cp (un1 ), restriction violated (e.g., gender, number, worldknowledge). helps pronouns Naturalowl generates correctly resolvedreaders, even would appear potentially ambiguous. example,pronoun sentence 7 naturally understood referring exhibit, intended to, gallery, even though neuter excellent condition.698fiGenerating Natural Language Descriptions OWL OntologiesNote referents, transition sentence 6 7 continue; hence,transition type preferences play role. gender generated pronoun gender(most appropriate) nl name pronoun realizes.21nl name, Naturalowl uses gender (most appropriate) nl namespecific class includes nl name (or one classes, many).nl names also associated sets genders, give rise pseudo-pronounslike he/she; may desirable nl name class like Person.individuals classes, may wish use nl names, tokenizedidentifiers rdfs:label strings. common, example, museum ontologies,exhibits known particular names, many exhibits anonymousowl identifiers particularly meaningful. Naturalowl allows domain authormark individuals classes anonymous, indicate nl names, tokenizedidentifiers, rdfs:label strings avoided. primary target markedanonymous, Naturalowl uses demonstrative noun phrase (e.g., statue) referit. demonstrative phrase involves nl name specific class subsumesprimary target, nl name, marked anonymous. Especiallysentences express isA instanceOf message triples primary target,demonstrative phrase simply this, avoid generating sentences like statuestatue. marking anonymous individuals classes currently affectsreferring expressions primary target.3.3 Surface Realizationmany nlg systems, sentences end micro-planning underspecified;example, order constituents exact forms words may unspecified.Large-scale grammars statistical models used fill missing informationsurface realization, already discussed (Section 3.2.1). contrast, Naturalowl(and template-based nlg systems) (ordered aggregated) sentence plansend micro-planning already completely specify surface (final) form sentence.Hence, surface realization Naturalowl mostly process converting internal,fully specified ordered sentence specifications final text. Punctuationcapitalization also added. Application-specific markup (e.g., html tags, hyperlinks)images also added modifying surface realization code Naturalowl.4. Trialsprevious work, Naturalowl used mostly describe cultural heritage objects.xenios project, tested owl version ontology createdm-piro document approximately 50 archaeological exhibits (Androutsopoulos etal., 2007).22 owl version comprised 76 classes, 343 individuals (including cities, personsetc.), 41 properties. xenios, Naturalowl also embedded robotic avatarpresented exhibits m-piro virtual museum (Oberlander, Karakatsiotis,21. languages like Greek use grammatical instead natural genders, pronouns genders cannotdetermined consulting ontology (e.g., check referent animate inanimate).22. xenios co-funded European Union Greek General Secretariat Research Technology; see http://www.ics.forth.gr/xenios/.699fiAndroutsopoulos, Lampouras, & GalanisIsard, & Androutsopoulos, 2008). recently, indigo project, Naturalowlembedded mobile robots acting tour guides exhibition ancient AgoraAthens.23 owl ontology documenting 43 monuments used; 49 classes,494 individuals, 56 properties total.xenios indigo, texts Naturalowl eventually indistinguishablehuman-authored texts. participated, however, development ontologies,may biased towards choices (e.g., classes, properties) made easierNaturalowl generate high-quality texts. Hence, trials discussed below, wantedexperiment independently developed ontologies. also wanted experimentdifferent domains, opposed cultural heritage.goal compare texts Naturalowl simpler verbalizer. used owl verbalizer swat project (Stevens et al., 2011; Williams,Third, & Power, 2011), found particularly robust useful.24 verbalizer produces alphabetical glossary entry named class, property,individual, without requiring domain-dependent generation resources. glossary entrysequence English-like sentences expressing corresponding owl statementsontology. swat verbalizer uses predetermined partial order statementsglossary entry; example, describing class, statements equivalent classessuper-classes mentioned first, individuals belonging target class mentioned last.25 verbalizer actually translates owl ontology Prolog, extractslexicon entries owl identifiers rdfs:label strings, uses predetermined sentence plans specified dcg grammar. also aggregates, effect, message triplesproperty share one argument (S O) (Williams & Power, 2010).hypothesis domain-dependent generation resources would help Naturalowl produce texts end-users would consider fluent coherent, comparedproduced swat verbalizer, also produced Naturalowl withoutdomain-dependent generation resources. also wanted demonstrate high-qualitytexts could produced English Greek, measure effort requiredcreate domain-dependent generation resources Naturalowl existing ontologies. effort measured previous work, developmentdomain-dependent generation resources combined developmentontologies. Since time needed create domain-dependent generation resourcesdepends ones familiarity Naturalowl Protege plug-in, exact timesparticularly informative. Instead, report figures number sentence plans,lexicon entries etc. required, along approximate times. evaluate23. indigo fp6 ist project European Union; consult http://www.ics.forth.gr/indigo/.Videos robots xenios indigo available http://nlp.cs.aueb.gr/projects.html.Two aueb students, G. Karakatsiotis V. Pterneas, Interoperability Challenge 2011Microsoft Imagine Cup similar mobile phone application, called Touring Machine, usesNaturalowl; see http://www.youtube.com/watch?v=PaNAmNC7dZw.24. swat verbalizer used on-line http://swat.open.ac.uk/tools/. used generalpurpose version on-line July August 2011; similar verbalizer owl ace (Section2) available http://attempto.ifi.uzh.ch/site/docs/owl ace.html. domain-specific versionswat snomed biomedical ontology also developed (Liang et al., 2011a, 2011b).25. verbalizer also organizes English-like sentences glossary entry sub-headings likeDefinition, Taxonomy, Description, Distinctions (Williams et al., 2011). discarded subheadings, whose meanings entirely clear us, retained order sentences.700fiGenerating Natural Language Descriptions OWL Ontologiesusability Protege plug-in Naturalowl, since similar authoringtool m-piro. Previous experiments (Androutsopoulos et al., 2007) showed computerscience graduates expertise nlg could learn use effectively authoring toolm-piro create necessary domain-dependent generation resources existingnew ontologies, receiving equivalent full-day introduction course.4.1 Trials Wine Ontologyfirst trial, experimented Wine Ontology, often used SemanticWeb tutorials.26 comprises 63 wine classes, 52 wine individuals, total 238 classesindividuals (including wineries, regions, etc.), 14 properties.submitted Wine Ontology swat verbalizer obtain glossary Englishlike descriptions classes, properties, individuals. retained descriptions63 wine classes 52 wine individuals. Subsequently, also discarded 2063 wine class descriptions, trivial classes (e.g., RedWine)stating obvious (e.g., red wine defined wine color Red).27descriptions remaining 43 wine classes 52 wine individuals, discardedsentences expressing axioms Naturalowl consider, example sentencesproviding examples individuals belong class described. remainingsentences express owl statements Naturalowl expresses maximumfact distance set 1. Two examples texts produced swat verbalizer follow.Chenin Blanc (class): chenin blanc defined something wine, made grapeChenin Blanc Grape, made grape one thing. chenin blanc flavorModerate, color White. chenin blanc sugar Dry Dry,body Full Medium.Foxen Chenin Blanc (individual): Foxen Chenin Blanc chenin blanc. Foxen CheninBlanc body Full. Foxen Chenin Blanc flavor Moderate. Foxen Chenin Blancmaker Foxen. Foxen Chenin Blanc sugar Dry. Foxen Chenin Blanc locatedSanta Barbara Region.Subsequently, generated texts 43 classes 52 individuals using Naturalowlwithout domain-dependent generation resources, hereafter called Naturalowl(), settingmaximum fact distance 1; resulting texts similar swats.constructed domain-dependent generation resources NaturalowlWine Ontology. resources summarized Table 4. constructedsecond author, devoted three days construction, testing, refinement.28experience takes weeks (if longer) develop owl ontology sizeWine Ontology (acquire domain knowledge, formulate axioms owl, checkinconsistencies, populate ontology individuals etc.); hence, period days26. See http://www.w3.org/TR/owl-guide/wine.rdf.27. Third (2012) discusses owl axioms leading undesirable sentences kind might detected.28. resources constructed editing directly owl representations, rather usingProtege plug-in, fully functional time. using fully functionalplug-in, time create domain-dependent generation resources would shorter.701fiAndroutsopoulos, Lampouras, & GalanisResourcesSectionsProperty assignments sectionsInterest score assignmentsSentence plansLexicon entriesNatural language namesEnglishGreek27856741Table 4: Domain-dependent generation resources created Wine Ontology.relatively light effort, compared time needed develop owl ontology size.English texts generated trial; hence, Greek resources constructed.defined one user type, used interest scores block sentences statingobvious, assigning zero interest scores corresponding message triples; alsoset maxMessagesPerSentence 3. 7 14 properties Wine Ontology usedowl statements describe 43 classes 52 individuals. defined 5sentence plans, 7 properties could expressed sentence plans.define multiple sentence plans per property. also assigned 7 properties2 sections, ordered sections properties. created nl namesautomatically extracted ones causing disfluencies. extracted nl namesobtained owl identifiers classes individuals; rdfs:label stringsavailable. reduce number manually constructed nl names further, declared52 individual wines anonymous (and provided nl names them).67 lexicon entries used remaining 41 nl names classes individuals;nl names simple, 2 slots average. used Naturalowldomain-dependent resources, hereafter called Naturalowl(+), re-generate 95 texts,setting maximum fact distance 1; example texts follow.Chenin Blanc (class): Chenin Blanc moderate, white wine. full medium body.off-dry dry. made exactly one wine grape variety: Chenin Blanc grapes.Foxen Chenin Blanc (individual): wine moderate, dry Chenin Blanc. full body.made Foxen Santa Barbara County.resulting 285 texts (95 3) three systems (swat verbalizer, Naturalowl(),Naturalowl(+)) shown 10 computer science students (both undergraduatesgraduate students), involved development Naturalowl;fluent English, though native English speakers, considerwine experts. students told glossary wines developed peopleinterested wines knew basic wine terms (e.g., wine colors, wine flavors),otherwise wine experts. one 285 texts given exactlyone student. student given approximately 30 texts, approximately 10 randomlyselected texts system. owl statements texts generatedshown, students know system generated text.student shown his/her texts random order, regardless systemgenerated them. students asked score text stating stronglyagreed disagreed statements S1 S5 below. scale 1 3 used (1:disagreement, 2: ambivalent, 3: agreement).702fiGenerating Natural Language Descriptions OWL OntologiesCriteriaSentence fluencyReferring expressionsText structureClarityInterestswat2.00 0.151.40 0.132.15 0.162.66 0.132.30 0.15Naturalowl()1.76 0.151.15 0.092.20 0.162.55 0.132.14 0.16Naturalowl(+)2.80 0.102.72 0.132.94 0.052.74 0.112.68 0.12Table 5: Results texts generated Wine Ontology swat verbalizerNaturalowl (+) without () domain-dependent generation resources.(S1 ) Sentence fluency: sentences text fluent, i.e., sentence grammaticalsounds natural. two smaller sentences combined form single, longer sentence,resulting longer sentence also grammatical sounds natural.(S2 ) Referring expressions: use pronouns referring expressions (e.g., wine)appropriate. choices referring expressions (e.g., use pronoun expression insteadname object) sound natural, easy understand expressions refer to.(S3 ) Text structure: order sentences appropriate. text presents information movingreasonably one topic another.(S4 ) Clarity: text easy understand, provided reader familiar basic wine terms.(S5 ) Interest: People interested wines, wine experts, would find informationinteresting. Furthermore, redundant sentences text (e.g., sentences stating obvious).29S5 assesses content selection, first processing sub-stage; expected differencesacross three systems small, reported information,exception redundant sentences blocked using zero interest assignments Naturalowl.S3 assesses text planning, second sub-stage; expected small differences, manywine properties mentioned order, though properties (e.g.,maker, location) naturally reported separately others (e.g., color, flavor),used two sections (Table 4). S1 assesses lexicalization aggregation;decided use separate statements two stages, since might difficultstudents understand exactly aggregation takes place. S2 assesses referringexpression generation. S4 measures overall perceived clarity texts.statement surface realization, stage rather trivial effect.Table 5 shows average scores three systems, averages computed95 texts system, along 95% confidence intervals (of sample means).criterion, best score shown bold; confidence interval best score alsoshown bold overlap confidence intervals.30expected, domain-dependent generation resources clearly help Naturalowl produce fluent sentences much better referring expressions. text structure scoresshow assignment ontologys properties sections orderingsections properties greater impact perceived structure textsexpected. highest score swat verbalizer obtained clarity criterion, agrees experience one usually understand textsswat verbalizer mean, even sentences often entirely fluent, particularly well ordered, keep repeating proper names. Naturalowl(+)had highest clarity29. students told consider whether additional information included.30. two intervals overlap, difference statistically significant. overlap,difference may still statistically significant; performed paired two-tailed t-tests ( = 0.05)cases. pilot study, also measured inter-annotator agreement two students sample30 texts (10 system). Agreement high (sample Pearson correlation r 0.91)five criteria. similar pilot study performed next trial, also indicating high agreement.703fiAndroutsopoulos, Lampouras, & Galanisscore, difference swat verbalizer, second highest score,statistically significant. Naturalowl(+)also obtained higher interest scorestwo systems, statistically significant differences both; differences,larger expected, attributed zero interest score assignmentsdomain-dependent generation resources, blocked sentences stating obvious,otherwise three systems report information.swat verbalizer obtained higher scores Naturalowl(), text structure score exception. difference referring expression scorestwo systems, though, statistically significant. systems, however, receivedparticularly low scores referring expressions, surprising, givenalways refer individuals classes extracted names; slightly higherscore swat verbalizer probably due better tokenization owl identifiers.4.2 Trials Consumer Electronics Ontologysecond trial, experimented Consumer Electronics Ontology, owl ontology consumer electronics products services.31 ontology comprises 54 classes441 individuals (e.g., printer types, paper sizes, manufacturers), informationparticular products. added 60 individuals describing 20 digital cameras, 20 camcorders, 20 printers. 60 individuals randomly selected publicly availabledataset 286 digital cameras, 613 camcorders, 58 printers, whose instances complyConsumer Electronics Ontology.32submitted Consumer Electronics Ontology additional 60 individualsswat verbalizer, retained descriptions 60 individuals. Again,removed sentences expressing axioms Naturalowl consider. also renamedstring values datatype properties make texts easier understand (e.g.,cmt became cm). example description follows.Sony Cyber-shot DSC-T90 digital camera.Sony Cyber-shot DSC-T90 manufacturer Sony.Sony Cyber-shot DSC-T90 data interface type Usb2 0.Sony Cyber-shot DSC-T90 depth Depth. Depth unit measurement cm. Depth valuefloat 9.4.Sony Cyber-shot DSC-T90 digital zoom factor Digital Zoom Factor. Digital Zoom Factorvalue float 12.1. [. . . ]Sony Cyber-shot DSC-T90 feature Video Recording, Microphone Automatic Picture Stabilizer.Sony Cyber-shot DSC-T90 self timer true. [. . . ]ontology, many properties composite values, expressed using auxiliary individuals. example above, property (hasDepth) connects digital cameraauxiliary individual Depth (similar anonymous node :n property concatenationprice example page 691), connected via two properties (hasValueFloat31. Consult http://www.ebusiness-unibw.org/ontologies/consumerelectronics/v1.32. See http://rdf4ecommerce.esolda.com/ dataset used. list similar datasetsavailable http://wiki.goodrelations-vocabulary.org/Datasets.704fiGenerating Natural Language Descriptions OWL OntologieshasUnitOfMeasurement) float value 9.4 unit measurement (centimeters), respectively. obtained descriptions auxiliary individuals (e.g., Depth),different entries glossary swat verbalizer, copied immediately corresponding sentences introduce auxiliary individuals.also formatted text list sentences, above, improve readability.generated texts 60 products using Naturalowl(), setting maximumfact distance 1. Descriptions auxiliary individuals also generated copiedimmediately sentences introducing them. texts similarswat verbalizer, formatted manner.trial, also wanted consider scenario set individualsdescribed changes frequently (e.g., products sold reseller change, new products arrive etc.) along changes connected individuals (e.g., new manufacturers mayadded), nothing else ontology changes, i.e., assertional knowledgechanges. case, may impractical update domain-dependent generationresources whenever population individuals changes. hypothesis considering sample individuals types described (printers, cameras, camcorders,case), would possible construct domain-dependent generation resources (e.g.,sections, ordering sections properties, sentence plans, nl names classes)would help Naturalowl generate reasonably good descriptions new (unseen) individuals (products), without updating domain-dependent generation resources, usingtokenized owl identifiers rdfs:label strings new individuals nl names.simulate scenario, randomly split 60 products two non-overlapping sets,development set test set, consisting 10 digital cameras, 10 camcorders,10 printers. Again, second author constructed refined domain-dependentgeneration resources Naturalowl, time considering version ontologyincluded 30 development products, 30 test products, viewinggenerated texts 30 development products only. took approximately six days (fortwo languages).33 Hence, relatively light effort needed, compared timetypically takes develop ontology size, terminology two languages. Texts30 products test set also generated using Naturalowldomain-dependent generation resources development set.previous trial, defined one user type, used interest scoresblock sentences stating obvious. maximum messages per sentence 3.constructed domain-dependent generation resources English Greek;resources summarized Table 6. created sentence plans 42 propertiesontology used development set (one sentence plan per property);test set uses two additional properties, default sentence plans Naturalowl(for English Greek) used. also assigned 42 properties 6 sections,ordered sections properties. created nl names automaticallyextracted ones causing disfluencies development texts. Unlike previoustrial, products described declared anonymous individuals,number nl names provided roughly previous trial,33. Again, domain-dependent generation resources constructed editing owl representations. test, second author later reconstructed domain-dependent generation resourcesscratch using fully functional Protege plug-ing, time four days.705fiAndroutsopoulos, Lampouras, & GalanisResourcesSectionsProperty assignments sectionsInterest score assignmentsSentence plansLexicon entriesNatural language namesEnglishGreek64212421936421936Table 6: Domain-dependent generation resources Consmer Electronics Ontology.since fewer automatically extracted names causing disfluencies; particular,products reasonably good rdfs:label strings providing English names.example description development set produced Naturalowl(+)follows.formatted sentences section separate paragraph, headed namesection (e.g., features:); easy, Naturalowl automaticallymark sections texts. maximum fact distance 1, sentenceplans caused Naturalowl automatically retrieve additional message triples describingauxiliary individuals distance 1; hence, retrieve informationmanually, unlike texts swat verbalizer Naturalowl().Type: Sony Cyber-shot DSC-T90 digital camera.Main features: focal length range 35.0 140.0 mm, shutter lag 2.0 0.0010 secoptical zoom factor 4.0. digital zoom factor 12.1 display diagonal 3.0 in.features: features automatic picture stabilizer, microphone, video recordingself-timer.Energy environment: uses batteries.Connectivity, compatibility, memory: supports USB 2.0 connections data exchangeinternal memory 11.0 GB.Dimensions weight: 5.7 cm high, 1.5 cm wide 9.4 cm deep. weighs 128.0 grm.180 English texts generated three systems 30 development30 test products shown 10 students first trial. studentstold texts would used on-line descriptions products Website retailer. Again, owl statements texts generatedshown students, students know system generatedtext. student shown 18 randomly selected texts, 9 products developmentset (3 texts per system) 9 products test set (again 3 texts per system).student shown his/her texts random order, regardless systemgenerated them. students asked score texts previous trial.Table 7 shows results English texts development set.34 previous trial, domain-dependent generation resources clearly help Naturalowl producemuch fluent sentences, much better referring expressions sentence orderings.text structure scores swat verbalizer Naturalowl()are much lowerprevious trial, message triples express per individual topics, texts systems jump one topic anothermaking texts look incoherent; example, sentence width cameramay separated sentence height sentence shutter lag.34. confidence interval 0.00, means students gave score texts.706fiGenerating Natural Language Descriptions OWL OntologiesCriteriaSentence fluencyReferring expressionsText structureClarityInterestswat1.97 0.151.10 0.061.67 0.151.97 0.151.77 0.14Naturalowl()1.93 0.271.10 0.111.33 0.192.07 0.261.73 0.29Naturalowl(+)2.90 0.082.87 0.082.97 0.043.00 0.003.00 0.00Table 7: English development results Consumer Electronics Ontology.CriteriaSentence fluencyReferring expressionsText structureClarityInterestswat2.03 0.151.10 0.061.57 0.132.07 0.151.83 0.17Naturalowl()1.87 0.151.10 0.061.37 0.121.93 0.151.60 0.14Naturalowl(+)2.87 0.082.87 0.082.93 0.052.97 0.042.97 0.04Table 8: English test results Consumer Electronics Ontology.incoherence may also contributed much lower clarity scores two systems, compared previous trial. interest scores two systems also muchlower previous trial; may due verbosity texts, causedfrequent references auxiliary individuals second trial, combined lack(or little use) sentence aggregation pronoun generation. contrast, clarity interest Naturalowl(+)were judged perfect; poor clarity interesttwo systems may contributed perfect scores though. Again,swat verbalizer obtained slightly better scores Naturalowl without domain-dependentgeneration resources, except clarity, differences statistically significant.Table 8 shows results English texts test set. results swatverbalizer Naturalowl()are similar Table 7, one would expect.Also, marginal decrease scores Naturalowl(+), comparedscores system development set Table 7. statisticallysignificant difference, however, corresponding cells two tables,three systems. results support hypothesis considering sampleindividuals types described one construct domain-dependent generationresources used produce high-quality texts new individualstypes, rest ontology remains unchanged. fact products (butindividuals) rdfs:label strings providing English names probablycontributed high results Naturalowl(+)in test set, rdfs:label stringskind common owl ontologies.showed 60 Greek texts generated Naturalowl(+)to10 students, native Greek speakers; swat verbalizer Naturalowl()cannotCriteriaSentence fluencyReferring expressionsText structureClarityInterestNaturalowl(+),development data2.87 0.122.77 0.203.00 0.003.00 0.002.97 0.06Naturalowl(+),test data2.83 0.092.80 0.113.00 0.002.93 0.053.00 0.00Table 9: Greek results Consumer Electronics Ontology.707fiAndroutsopoulos, Lampouras, & GalanisNo.1234567System ConfigurationNaturalowl(+)interest scoresref. expr. gen.nl namesaggregationsentence planssections, orderingSentence Fluency4.80 0.124.53 0.163.93 0.283.71 0.293.64 0.332 .07 0 .371.89 0.36Ref. Expressions5.00 0.004.95 0.061 .53 0 .221.48 0.211.33 0.191.33 0.191.33 0.19Text Structure4.82 0.154.78 0.124.80 0.124.71 0.154.67 0.164.60 0.181 .53 0 .24Clarity4.78 0.124.62 0.174.51 0.244.24 0.254.24 0.252 .49 0 .362.33 0.33Interest4.89 0.094.20 0.194.07 0.223.98 0.263.93 0.262 .38 0.351.89 0.28Table 10: Ablation English test results Consumer Electronics Ontology.configuration removes one component resource previous configuration.generate Greek texts Consumer Electronics ontology. Table 9 shows resultsobtained Greek texts development test sets. statisticallysignificant difference corresponding results English (cf. last columns Tables 7 8). also statistically significant difference results Greektexts development test sets (Table 9). note, however, commonuse English names electronics products Greek texts, made using Englishrdfs:label names products Greek texts acceptable. domains,example cultural heritage, might unacceptable use English names individuals;hence, one would provide Greek nl names new individuals.4.3 Ablation Trials Consumer Electronics Ontologylast trial, studied quality generated texts affected various components domain-dependent generation resources Naturalowl graduallyremoved. used Consumer Electronics Ontology, domain-dependent generation resources constructed 30 development products previoustrial. also used 45 new test products (15 digital cameras, 15 camcorders, 15 printers, publicly available dataset), 30 development 30test products previous trial.generated English texts 45 new test products, using 7 configurationsNaturalowl Table 10. resulting 457 = 315 texts shown 7 students,background previous trials. student shown 7 texts 6 7test products (42 49 texts per student). product, 7 texts shown sideside random order, students instructed take account differences7 texts. students know system generated text.criteria (statements S1 S5 Section 4.1) used again, scale 1 5used time (1: strong disagreement, 2: disagreement, 3: ambivalent, 4: agreement,5: strong agreement), make easier distinguish 7 configurations.first configuration (Naturalowl(+)) Naturalowl components enabled, using available domain-dependent generation resources. previoustrial (see Table 8), texts configuration judged near-perfectcriteria. second configuration same, without interest score assignments.results second configuration close results first one, sinceinterest score assignments used avoid generating sentences stating obvious(e.g., Sony Cyber-shot DSC-T90 manufactured Sony). biggest decreaseinterest criterion, one would expect, scores sentence fluency clarityalso affected, presumably sentences state obvious sound unnatural708fiGenerating Natural Language Descriptions OWL Ontologiesseem introduce noise. small differences scores referringexpressions text structure, seem suggest overall qualitytexts decreases, judges biased towards assigning lower scores criteria.35third configuration second one, component generates pronouns demonstrative noun phrases disabled, causing Naturalowl alwaysuse nl names individuals classes, names extracted ontology.big decrease score referring expresions, showing despite simplicity, referring expression generation methods Naturalowl noticeable effect;mark big decreases italics Table 10. scores sentence fluency, interest,clarity also affected, presumably repeating names individualsclasses made sentences look less natural, boring, difficult follow.almost difference (a small positive one) text structure score.fourth configuration, nl names individuals classes also removed, forcing Naturalowl always use automatically extracted names.decrease score referring expressions, decrease small,referring expressions already poor third configuration. Note, also,nl names necessary Naturalowl produce pronouns demonstrative nounphrases; hence, higher referring expression score third configuration wouldpossible without nl names. sentence fluency clarity scores alsoaffected fourth configuration, presumably automatically extracted namesmade texts difficult read understand. also small decreasesscores interest even text structure, suggesting overall qualitytexts decreases, judges biased towards lower scores criteria.fifth configuration, aggregation turned off, causing Naturalowl produceseparate sentence message triple. sentences sharing subjectlonger aggregated, referring expressions subjects generated. Sincecomponent generates pronouns demonstrative noun phrases switchednl names removed, repetitions automatically extracted namesused, score referring expressions decreased further. Sentencefluency also affected, since obvious aggregations longer made,made sentences look less natural. also small decrease scoreperceived text structure interest, difference score clarity. Overall,contribution aggregation perceived quality texts seems rather small.sixth configuration, sentence plans removed, forcing Naturalowluse default sentence plan tokenized property identifiers. sharp decreasesentence fluency clarity, one would expect, also perceived interesttexts. also small decrease perceived text structure, differencescore referring expressions. Overall, results indicate sentence plansimportant part domain-dependent generation resources.seventh configuration, sections, assignments properties sections,ordering sections properties removed, causing Naturalowl produce random35. criteria, differences one configuration next one statistically significant, exceptions differences clarity configurations 4 5,differences scores referring expressions configurations 56 67. Again,95% confidence intervals overlapped, performed paired two-tailed t-tests ( = 0.05).709fiAndroutsopoulos, Lampouras, & Galanisorderings message triples. sharp decrease score textstructure. scores perceived interest, clarity, also sentence fluency alsoaffected, suggesting overall quality texts decreases, judgesbiased towards lower scores criteria.conclude sections ordering information domain-dependent generation resources are, along sentece plans, particularly important. note, however,best scores obtained enabling components using availabledomain-dependent generation resources.5. Conclusions Future Workprovided detailed description Naturalowl, open-source nlg system produces English Greek texts describing individuals classes owl ontologies. Unlikesimpler verbalizers, typically express single axiom time controlled, oftenentirely fluent English primarily benefit domain experts, Naturalowl aimsgenerate fluent coherent multi-sentence texts end-users one languages.discussed processing stages Naturalowl, optional domain-dependent generation resources stage, well particular nlg issues arise generatingowl ontologies. also presented trials performed measure effort requiredconstruct domain-dependent generation resources extent improve resulting texts, also comparing simpler owl verbalizer requiresdomain-dependent generation resources employs nlg methods lesser extent.trials showed domain-dependent generation resources help Naturalowl producesignificantly better texts, resources constructed relatively lighteffort, compared effort typically needed develop owl ontology.Future work could compare effort needed construct domain-dependent generation resources effort needed manually edit lower quality texts producedwithout domain-dependent generation resources. experience manually editingtexts generated verbalizer (or Naturalowl()) tedious largenumber individuals (e.g., products) types described, editorrepeat (or similar) fixes. may be, however, particular applicationspost-editing texts simpler verbalizer may preferable.also aim replace future work pipeline architecture Naturalowl globaloptimization architecture consider nlg processing stages parallel, avoidgreedy stage-specific decisions (Marciniak & Strube, 2005; Lampouras & Androutsopoulos,2013a, 2013b). Finally, hope test Naturalowl biomedical ontologies,Gene Ontology snomed.36ReferencesAndroutsopoulos, I., Kallonis, S., & Karkaletsis, V. (2005). Exploiting OWL ontologiesmultilingual generation object descriptions. 10th European Workshop NLG, pp. 150155, Aberdeen, UK.36. See http://www.geneontology.org/ http://www.ihtsdo.org/snomed-ct/.710fiGenerating Natural Language Descriptions OWL OntologiesAndroutsopoulos, I., Lampouras, G., & Galanis, D. (2012). Generating natural language descriptionsOWL ontologies: detailed presentation NaturalOWL system. Tech. rep., NLPGroup, Department Informatics, Athens University Economics Business, Greece.Androutsopoulos, I., Oberlander, J., & Karkaletsis, V. (2007). Source authoring multilingualgeneration personalised object descriptions. Nat. Language Engineering, 13 (3), 191233.Angeli, G., Liang, P., & Klein, D. (2010). simple domain-independent probabilistic approachgeneration. Conf. Empirical Methods NLP, pp. 502512, Cambridge, MA.Antoniou, G., & van Harmelen, F. (2008). Semantic Web Primer. MIT Press.Areces, C., Koller, A., & Striegnitz, K. (2008). Referring expressions formulas description logic.5th Int. Nat. Lang. Generation Conf., pp. 4249, Salt Fork, OH.Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2002).Description Logic Handbook. Cambridge Univ. Press.Barzilay, R., Elhadad, N., & McKeown, K. (2002). Inferring strategies sentence orderingmultidocument news summarization. Journal AI Research, 17, 3555.Barzilay, R., & Lapata, M. (2005). Collective content selection concept-to-text generation.Human Lang. Technology Conf. Conf. Empirical Methods Nat. Language Processing,pp. 331338, Vancouver, British Columbia, Canada.Barzilay, R., & Lapata, M. (2006). Aggregation via set partitioning natural language generation.Human Lang. Technology Conf. NAACL, pp. 359366, New York, NY.Barzilay, R., & Lapata, M. (2008). Modeling local coherence: entity-based approach. Comput.Linguistics, 34 (1), 134.Barzilay, R., & Lee, L. (2004). Catching drift: Probabilistic content models, applicationsgeneration & summarization. 43rd Annual Meeting ACL, pp. 113120, Ann Arbor, MI.Bateman, J. (1990). Upper modelling: general organisation knowledge nat. lang. processing.5th Int. Workshop NLG, pp. 5461, Dawson, PA.Bateman, J. (1997). Enabling technology multilingual nat. lang. generation: KPML development environment. Nat. Lang. Engineering, 3 (1), 1556.Bernardi, R., Calvanese, D., & Thorne, C. (2007). Lite natural language. 7th Int. WorkshopComput. Semantics, Tilburg, Netherlands.Berners-Lee, T., Hendler, J., & Lassila, O. (2001). Semantic Web. Sc. American, May, 3443.Bontcheva, K. (2005). Generating tailored textual summaries ontologies. 2nd EuropeanSemantic Web Conf., Heraklion, Greece.Bontcheva, K., & Cunningham, H. (2003). Semantic Web: new opportunity challengehuman language technology. Workshop Human Lang. Tech. SW WebServices, 2nd Int. Semantic Web Conf., Sanibel Island, FL.Bontcheva, K., Tablan, V., Maynard, D., & Cunningham, H. (2004). Evolving GATE meet newchallenges language engineering. Nat. Lang. Eng., 10 (3/4), 349373.Bontcheva, K., & Wilks, Y. (2004). Automatic report generation ontologies: MIAKTapproach. 9th Int. Conf. Applications Nat. Language Information Systems, pp.324335, Manchester, UK.Brin, S., & Page, L. (1998). anatomy large-scale hypertextual Web search engine. ComputerNetworks ISDN Systems, 30 (1-7), 107117.Busemann, S., & Horacek, H. (1999). flexible shallow approach text generation. 9th Int.Workshop Nat. Lang. Generation, pp. 238247, New Brunswick, NJ.711fiAndroutsopoulos, Lampouras, & GalanisChen, H., Branavan, S., Barzilay, R., & Karger, D. (2009). Content modeling using latent permutations. Journal Artificial Intelligence Research, 36, 129163.Cheng, H., & Mellish, C. (2000). Capturing interaction aggregation text planningtwo generation systems. 1st Int. Conf. Nat. Lang. Generation, pp. 186193, MitzpeRamon, Israel.Cregan, A., Schwitter, R., & Meyer, T. (2007). Sydney OWL syntax towards controlled naturallanguage syntax OWL. OWL Experiences Directions Workshop, Innsbruck, Austria.Dale, R., Green, S., Milosavljevic, M., Paris, C., Verspoor, C., & Williams, S. (1998). Dynamicdocument delivery: generating natural language texts demand. 9th Int. Conf.Workshop Database Expert Systems Applications, pp. 131136, Vienna, Austria.Dalianis, H. (1999). Aggregation nat. lang. generation. Comput. Intell., 15 (4), 384414.Dannells, D. (2012). generating coherent multilingual descriptions museum objectsSemantic Web ontologies. 7th International NLG Conf., pp. 7684, Utica, IL.Dannels, D. (2008). Generating tailored texts museum exhibits. Workshop LanguageTechnology Cultural Heritage Data Language Resources Evaluation Conf., Marrakech, Morocco.Davis, B., Iqbal, A., Funk, A., Tablan, V., Bontcheva, K., Cunningham, H., & Handschuh, S. (2008).Roundtrip ontology authoring. 7th Int. Conf. Semantic Web, pp. 5065, Karlsruhe,Germany.Demir, S., Carberry, S., & McCoy, K. (2010). discourse-aware graph-based content-selectionframework. 6th Int. NLG Conf., pp. 1725, Trim, Co. Meath, Ireland.Denaux, R., Dimitrova, V., Cohn, A., Dolbear, C., & Hart, G. (2010). Rabbit OWL: Ontologyauthoring CNL-based tool. Fuchs, N. (Ed.), Controlled Nat. Language, Vol. 5972Lecture Notes Computer Science, pp. 246264. Springer.Denaux, R., Dolbear, C., Hart, G., Dimitrova, V., & Cohn, A. (2011). Supporting domain expertsconstruct conceptual ontologies. Web Semantics, 9 (2), 113127.Duboue, P., & McKeown, K. (2001). Empirically estimating order constraints content planninggeneration. 39th Meeting ACL, pp. 172179, Toulouse, France.Duboue, P., & McKeown, K. (2003). Statistical acquisition content selection rules naturallanguage generation. Conf. Empirical Methods Nat. Language Processing, pp. 121128, Sapporo, Japan.Duma, D., & Klein, E. (2013). Generating nat. lang. Linked Data: Unsupervised templateextraction. 10th Int. Conf. Computational Semantics, pp. 8394, Potsdam, Germany.Elhadad, M., & Robin, J. (1996). SURGE: reusable comprehensive syntactic realization component. 8th Int. NLG Workshop, Herstmonceux Castle, Sussex, UK.Elsner, M., Austerweil, J., & Charniak, E. (2007). unified local global model discoursecoherence. Human Lang. Technologies Conf. North American Chapter ACL, pp.436443, Rochester, New York.Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.Funk, A., Tablan, V., Bontcheva, K., Cunningham, H., Davis, B., & Handschuh, S. (2007). CLOnE:Controlled language ontology editing. 6th Int. Semantic Web 2nd Asian SemanticWeb Conf., pp. 142155, Busan, Korea.Galanis, D., & Androutsopoulos, I. (2007). Generating multi-lingual descriptions linguisticallyannotated OWL ontologies: NaturalOWL system. 11th European Workshop Nat.Lang. Generation, Schloss Dagstuhl, Germany.712fiGenerating Natural Language Descriptions OWL OntologiesGatt, A., & Reiter, E. (2009). SimpleNLG: realisation engine practical applications. 12thEuropean Workshop NLG, pp. 9093, Athens, Greece.Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P., & Sattler, U. (2008). OWL 2:next step OWL. Web Semantics, 6, 309322.Grosz, B., Joshi, A., & Weinstein, S. (1995). Centering: framework modelling local coherencediscourse. Comput. Linguistics, 21 (2), 203225.Halaschek-Wiener, C., Golbeck, J., Parsia, B., Kolovski, V., & Hendler, J. (2008). Image browsingnatural language paraphrases semantic web annotations. 1st Workshop SemanticInterop. European Digital Library, 5th European Semantic Web Conf., Tenerife, Spain.Hallett, C., Scott, D., & Power, R. (2007). Composing questions conceptual authoring.Comput. Linguistics, 33, 105133.Halliday, M. (1994). Introduction Functional Grammar (2nd edition). Edward Arnold.Horrocks, I., Patel-Schneider, P., & van Harmelen, F. (2003). SHIQ RDF OWL:making Web Ontology Language. Web Semantics, 1 (1), 726.Isard, A., Oberlander, J., Androutsopoulos, I., & Matheson, C. (2003). Speaking users languages.IEEE Intelligent Systems, 18 (1), 4045.Kaljurand, K. (2007). Attempto Controlled English Semantic Web Language. Ph.D. thesis,Faculty Mathematics Computer Science, University Tartu, Estonia.Karamanis, N., Mellish, C., Poesio, M., & Oberlander, J. (2009). Evaluating centering informationordering using corpora. Comput. Linguistics, 35 (1), 2946.Kasper, R., & Whitney, R. (1989). SPL: sentence plan language text generation. Tech. rep.,Information Sciences Institute, University Southern California.Kaufmann, E., & Bernstein, A. (2010). Evaluating usability nat. lang. query languagesinterfaces Semantic Web knowledge bases. Web Semantics, 8, 377393.Kelly, C., Copestake, A., & Karamanis, N. (2010). Investigating content selection languagegeneration using machine learning. 12th European Workshop Nat. Lang. Generation,pp. 130137, Athens, Greece.Konstas, I., & Lapata, M. (2012a). Concept-to-text generation via discriminative reranking. 50thAnnual Meeting ACL, pp. 369378, Jeju Island, Korea.Konstas, I., & Lapata, M. (2012b). Unsupervised concept-to-text generation hypergraphs.Human Lang. Technology Conf. NAACL, pp. 752761, Montreal, Canada.Krahmer, E., & van Deemter, K. (2012). Computational generation referring expressions:survey. Comput. Linguistics, 38 (1), 173218.Lampouras, G., & Androutsopoulos, I. (2013a). Using integer linear programming content selection, lexicalization, aggregation produce compact texts OWL ontologies.14th European Workshop Nat. Lang. Generation, 51st Annual Meeting ACL, pp. 5160,Sofia, Bulgaria.Lampouras, G., & Androutsopoulos, I. (2013b). Using integer linear programming concept-to-textgeneration produce compact texts. 51st Annual Meeting ACL (short papers),pp. 561566, Sofia, Bulgaria.Langkilde, I. (2000). Forest based statistical sentence generation. 1st Conf. North AmericanChapter ACL, pp. 170177, Seattle, WA.Lavoie, B., & Rambow, O. (1997). fast portable realizer text generation systems. 5thConf. Applied Nat. Language Processing, pp. 265268, Washington DC.713fiAndroutsopoulos, Lampouras, & GalanisLiang, P., Jordan, M., & Klein, D. (2009). Learning semantic correspondences less supervision.47th Meeting ACL 4th AFNLP, pp. 9199, Suntec, Singapore.Liang, S., Scott, D., Stevens, R., & Rector, A. (2011a). Unlocking medical ontologies non-ontologyexperts. 10th Workshop Biomedical NLP, Portland, OR.Liang, S., Stevens, R., Scott, D., & Rector, A. (2011b). Automatic verbalisation SNOMED classesusing OntoVerbal. 13th Conf. AI Medicine, pp. 338342, Bled, Slovenia.Mann, W., & Thompson, S. (1998). Rhetorical structure theory: theory text organization. Text,8 (3), 243281.Marciniak, T., & Strube, M. (2005). Beyond pipeline: Discrete optimization NLP. 9thConf. Comput. Nat. Language Learning, pp. 136143, Ann Arbor, MI.McKeown, K. (1985). Text Generation. Cambridge Univ. Press.McRoy, S., Channarukul, S., & Ali, S. (2003). augmented template-based approach textrealization. Nat. Language Engineering, 9 (4), 381420.Melengoglou, A. (2002). Multilingual aggregation M-PIRO system. Masters thesis, SchoolInformatics, University Edinburgh, UK.Mellish, C. (2010). Using Semantic Web technology support NLG case study: OWL finds RAGS.6th Int. NLG Conf., pp. 8593, Trim, Co. Meath, Ireland.Mellish, C., & Pan, J. (2008). Nat. lang. directed inference ontologies. Artificial Intelligence,172, 12851315.Mellish, C., Scott, D., Cahill, L., Paiva, D., Evans, R., & Reape, M. (2006). reference architecturenat. lang. generation systems. Nat. Language Engineering, 12, 134.Mellish, C., & Sun, X. (2006). Semantic Web linguistic resource: opportunities nat.lang. generation. Knowledge Based Systems, 19, 298303.Nguyen, T., Power, R., Piwek, P., & Williams, S. (2012). Planning accessible explanationsentailments OWL ontologies. 7th International NLG Conf., pp. 110114, Utica, IL.Oberlander, J., Karakatsiotis, G., Isard, A., & Androutsopoulos, I. (2008). Building adaptivemuseum gallery Second Life. Museums Web, Montreal, Canada.ODonnell, M., Mellish, C., Oberlander, J., & Knott, A. (2001). ILEX: architecture dynamichypertext generation system. Nat. Language Engineering, 7 (3), 225250.Poesio, M., Stevenson, R., & Di Eugenio, B. (2004). Centering: parameter theory instantiations. Comput. Linguistics, 30 (3), 309363.Power, R. (2009). Towards generation-based semantic web authoring tool. 12th EuropeanWorkshop Nat. Lang. Generation, pp. 915, Athens, Greece.Power, R. (2010). Complexity assumptions ontology verbalisation. 48th Annual MeetingACL (short papers), pp. 132136, Uppsala, Sweden.Power, R. (2011). Deriving rhetorical relationships semantic content. 13th European Workshop Nat. Lang. Generation, Nancy, France.Power, R. (2012). OWL simplified english: finite-state language ontology editing. 3rdInternational Workshop Controlled Natural Language, pp. 4460, Zurich, Switzerland.Power, R., & Scott, D. (1998). Multilingual authoring using feedback texts. 17th Int. Conf.Comp. Ling. 36th Meeting ACL, pp. 10531059, Montreal, Canada.Power, R., & Third, A. (2010). Expressing OWL axioms English sentences: Dubious theory,feasible practice. 23rd Int. Conf. Comput. Linguistics, pp. 10061013, Beijing, China.714fiGenerating Natural Language Descriptions OWL OntologiesRatnaparkhi, A. (2000). Trainable methods surface natural language generation. 1st Conf.North American Chapter ACL, pp. 194201, Seattle, WA.Rector, A., Drummond, N., Horridge, M., Rogers, J., Knublauch, H., Stevens, R., Wang, H., &Wroe, C. (2004). OWL pizzas: Practical experience teaching OWL-DL: Common errorscommon patterns. 14th Int. Conf. Knowledge Engineering Knowledge Management,pp. 6381, Northamptonshire, UK.Reiter, E. (1995). NLG vs. templates. 5th European Workshop Nat. Lang. Generation, Leiden,Netherlands.Reiter, E., & Dale, R. (2000). Building Natural Lang. Generation Systems. Cambridge Univ. Press.Ren, Y., van Deemter, K., & Pan, J. (2010). Charting potential description logicgeneration referring expressions. 6th Int. Nat. Lang. Generation Conf., pp. 115123,Trim, Co. Meath, Ireland.Schutte, N. (2009). Generating nat. language descriptions ontology concepts. 12th EuropeanWorkshop Nat. Lang. Generation, pp. 106109, Athens, Greece.Schwitter, R. (2010a). Controlled nat. languages knowledge representation. 23rd Int. Conf.Comput. Linguistics (posters), pp. 11131121, Beijing, China.Schwitter, R. (2010b). Creating querying formal ontologies via controlled nat. language. AppliedArtificial Intelligence, 24, 149174.Schwitter, R., Kaljurand, K., Cregan, A., Dolbear, C., & Hart, G. (2008). comparison threecontrolled nat. languages OWL 1.1. 4th OWL Experiences Directions Workshop,Washington DC.Schwitter, R., & Tilbrook, M. (2004). Controlled natural language meets Semantic Web.Australasian Language Technology Workshop, pp. 5562, Sydney, Australia.Shadbolt, N., Berners-Lee, T., & Hall, W. (2006). Semantic Web revisited. IEEE Intell. Systems,21, 96101.Stevens, R., Malone, J., Williams, S., Power, R., & Third, A. (2011). Automatic generationtextual class definitions OWL English. Biomedical Semantics, 2 (S 2:S5).Third, A. (2012). Hidden semantics: learn names ontology?. 7thInternational NLG Conf., pp. 6775, Utica, IL.van Deemter, K., Krahmer, E., & Theune, M. (2005). Real versus template-based natural languagegeneration: false opposition?. Comput. Linguistics, 31 (1), 1524.Walker, M., Rambow, O., & Rogati, M. (2001). Spot: trainable sentence planner. 2nd AnnualMeeting North American Chapter ACL, pp. 1724, Pittsburgh, PA.Wan, S., Dras, M., Dale, R., & Paris, C. (2010). Spanning tree approaches statistical sentencegeneration. Krahmer, E., & Theune, M. (Eds.), Empirical Methods Nat. Lang. Generation, pp. 1344. Springer.White, M. (2006). CCG chart realization disjunctive inputs. 4th Int. Nat. Lang. GenerationConf., pp. 1219, Sydney, Australia.Williams, S., & Power, R. (2010). Grouping axioms coherent ontology descriptions. 6thInt. Nat. Lang. Generation Conf., pp. 197201, Trim, Co. Meath, Ireland.Williams, S., Third, A., & Power, R. (2011). Levels organization ontology verbalization.13th European Workshop Nat. Lang. Generation, pp. 158163, Nancy, France.715fiJournal Artificial Intelligence Research 48 (2013) 415473Submitted 05/13; published 11/13Defeasible Inheritance-Based Description LogicsGiovanni CasiniGCasini@csir.co.zaCentre Artificial Intelligence Research (CAIR)CSIR Meraka Institute UKZN, South AfricaUmberto Stracciaumberto.straccia@isti.cnr.itIstituto di Scienze e Tecnologie dellInformazione (ISTI)CNR, ItalyAbstractDefeasible inheritance networks non-monotonic framework deals hierarchical knowledge. hand, rational closure acknowledged landmarkpreferential approach non-monotonic reasoning. combine two approachesdene new non-monotonic closure operation propositional knowledge basescombines advantages both. redene procedure Description Logics(DLs), family logics well-suited model structured information. casesprovide simple reasoning method built top classical entailment relationand, thus, amenable implementation based existing reasoners. Eventually,evaluate approach well-known landmark test examples.1. Introductionnotion rational closure (Lehmann & Magidor, 1992) acknowledged landmarknon-monotonic reasoning due rm logical properties, limited inferencecapabilities: e.g. exceptional class inherit typical propertiessuperclass. Consider penguins: atypical birds, since y, stillshare lot typical properties birds, e.g. wings. However, rationalclosure may infer penguins wings. hand, Defeasible Inheritance Networks (INs) (Horty, 1994) non-monotonic framework appropriateformalisation hierarchical knowledge limitation, exhibitquestionable logical properties (see Section 3.1).combine two approaches dene new non-monotonic closure operationpropositional knowledge bases combines advantages both, applymethod Description Logics (DLs) (Baader, Calvanese, McGuinness, Nardi, & PatelSchneider, 2003), family logics known well-suited model structuredinformation.1.1 Contributions RoadmapSection 2.1 brief recap classical approach inheritance nets, Hortys (1994)skeptical extension, Section 2.2 describes classical rational closure propositionallogic, generalising method presented Freund (1998). remaining material addressescontributions summarised follows.c2013AI Access Foundation. rights reserved.fiCasini & Straccia1. Section 3 propose new method reason INs relies procedure rational closure, present Boolean extension INs, called Booleandefeasible Inheritance Networks (BINs).2. Using BINs, develop Section 4 defeasible inheritance-based propositional closurecombines advantages inheritance nets rational closure.3. Eventually, Section 5 apply latter procedure case defeasible inheritancebased description logics.major feature procedures propose propositional logic DLsstill maintain desired logical properties rational closure, inferentialpower respect exceptional classes. Moreover method requires existencedecision procedure classical entailment and, thus, implemented topexisting propositional SAT solvers DL reasoners.Please note present paper substantially revised extended versionprevious work (Casini & Straccia, 2011). Specically,1. provide in-depth description reasoning model;2. extensively validate approach w.r.t. series landmark (test) examplesillustrated Horty (1994) Sandewall (2010) (see Appendix A);3. provide computational complexity results related reasoning procedures;4. include proofs supporting major claims (see Appendix B); duecomplexity notation, add also table summarising meaningsymbols use frequently (Appendix C).1.2 Related Workrefer two main approaches non-monotonic reasoning: inheritance networks onehand preferential approach other.Inheritance networks developed formalism reasoning taxonomicinformation. original ideas Touretzky (1986), approach richly developed(Sandewall, 1986; Touretzky, Horty, & Thomason, 1987; Horty, Thomason, & Touretzky,1987; Touretzky, Thomason, & Horty, 1991; Makinson, 1991; Simonet, 1996). See worksHorty (1994) Thomason (1992) overview, Gabbay Schlecta (2009)Sandewall (2010) recently contributed eld. particular, orderevaluate proposal, shall refer skeptical approach described Horty (1987,1994) landmark classical approach inheritance networks.hand, Lehmann Magidors rational closure falls preferentialapproach non-monotonic reasoning; approach, since rst formulationShoham (1988), become main representative non-monotonic reasoning (Kraus,Lehmann, & Magidor, 1990; Lehmann & Magidor, 1992; Makinson, 1994, 2005; Freund,1998; Bochman, 2001; Rott, 2001; Schlechta, 2004), particularly appreciated solidlogical characterization consequence relation.Eventually, proposal shall applied eld description logics (Baader et al.,2003). Several non-monotonic DLs exist (Baader & Hollunder, 1993; Bonatti, Faella, &416fiDefeasible Inheritance-Based Description LogicsSauro, 2011c, 2011b; Brewka & Augustin, 1987; Britz, Heidema, & Meyer, 2008; Donini,Nardi, & Rosati, 2002; Giordano, Olivetti, Gliozzi, & Pozzato, 2009; Giordano, Gliozzi,Olivetti, & Pozzato, 2012b; Grimm & Hitzler, 2009; Knorr, Alferes, & Hitzler, 2011; Quantz& Royer, 1992; Straccia, 1993), integrate several kinds non-monotonic reasoningmechanism DLs. Somewhat related proposal works Britz et al. (2008)Giordano et al. (2009, 2012b), address application preferential methods DL framework, refer rational closure. previous publication (Casini & Straccia, 2010) present procedure apply rational closure DLs,procedure basis actual proposal, modify approachorder amplify inferential power.2. Preliminariescompleteness, start basic notions INs propositional rationalclosure shall rely on.2.1 Defeasible Inheritance NetworksINs (Horty, 1994; Sandewall, 2010) classes (nodes), strict subsumption relationdefeasible subsumption relation among classes (links). shall indicate nodesletters p, q . . ., shall describe pair N = hS, U i, setstrict links, U set defeasible links. Every link N direct link,strict defeasible, positive negative. Specically,1. p q: class p subsumed class q [positive strict link];2. p 6 q: class p class q disjoint [negative strict link];3. p q: element class p usually element class q [positive defeasiblelink];4. p 6 q: element class p usually element class q [negativedefeasible link].Example 2.1. typical penguin example could represented N = h{p b}, {p 6f, b f, b w}i, reading b Bird, p Penguin, f Flying w Wings.presence strict links only, subsumption relation classes wouldcorrespond simply transitive closure links: p subsumed q qsubsumed r, p subsumed r. Instead, presence defeasible links impliespossibility potential inconsistencies hierarchy classes, penguinexample: transitive closure subsumption relation would force us concludetime penguins ying non-ying creatures. Hence reasoninginheritance networks consists mainly deciding conclusions consideredvalid faced potential contradictions. classical approachesdecisions based notions potential path preemption (a procedure that,given two conicting paths, allows choose one resting specic information,invalidating other). Among various proposals, shall briey present Hortys (1994)417fiCasini & Stracciaclassical approach, shall refer landmark. First, recall notion pathshared classical approaches INs. refer paths meansGreek letters (, , , , . . .), tuples indicating sequence nodes involved.example, triple = hp, , qi indicates path starts node p, passespath , ends node q.Definition 2.1 (Potential Paths, Horty, 1994, p. 117). Given net N = hS, U i,define iteratively paths (where 99K {, } 699K {6, 6})1 :Every direct link N simple path:p 99K q S, hp, qi positive path.p 699K q S, hp, qi negative path.Assume path = ht, , pi.positive,p 99K q S, h, qi positive path.p 699K q S, h, qi negative path.negative,q p S, h, qi negative path.path composed strict links, strict path, otherwise defeasibleone.potential path represents potential argumentation, decidevalid not. path strict, automatically considered valid, otherwise, casepotential conicts conclusions distinct defeasible paths, chooseones considered valid.Using notions path preemption, Horty denes iterative constructionextension net, is, set paths considered valid net. Duecomplexity formal denition, describe roughly procedure, referringHortys work (1994, sect. 3), better insight.inductive construction paths based notion degree paths.identied potential paths, dene also notion generalised path.is, generalisation defeasible paths that, given {, 6, , , 6}, dened as:1. every direct link generalised path;2. = h, pi generalised path, p q N , h, qi generalised path.associate every defeasible path hp, , qi number corresponds numberlinks longest generalised path p q, denoted degN (hp, , qi).inheritance nets, notion contradiction corresponds notion conflict:say path conicted another path starting endpoints, opposite polarity (i.e., two potential paths moving node p1. assume link 6 symmetric, is, p 6 q S, q 6 p too.418fiDefeasible Inheritance-Based Description Logicsnode q, one positive negative). dealing strict paths,presence conict points actual contradiction, while, dealing defeasiblepaths, contradiction potential could resolved notion preemption(Horty, 1994, Def. 3.2.2) allows prefer path relying specic information.procedure dened Horty start net N , and, working iterativelydegree paths, dene sequence pairs hN , Pi i, Pi set pathsconsidered valid i-th step. brief, given net N = hS, U i, Hortys procedure resultsconstruction sequence sets valid paths P0 , P1 , P2 , . . . where:1. P0 = N ;2. Pn+1 Pn united paths degree n + 1 extensionvalid path degree n preempt eventual conicting paths.skeptical extension net N denedn=1 Pn (Horty, 1994, sect. 2.2.23.3.2). indicate (positive negative, strict defeasible) connection twonodes p q considered valid Hortys skeptical extension write Np q,{, 6, , 6}.Example 2.2. Consider example 2.1; following Hortys procedure obtain skepticalextension net composed valid paths p b, p 6 f, b f, b w, p b w.Consistency. net considered inconsistent validate two conicting paths,is, pair nodes p q derive positive connection(Np q Np q) negative one (Np 6 q Np 6 q)them.2.2 Propositional Rational ClosureNon-monotonic systems analysed point view properties consequence relations dene (Makinson, 1994). perspective INs satisfydesirable logical properties, presented below, (CM) (CT) (Makinson, 1994,pp. 56-57).Even satisfaction structural properties going present unanimously considered necessary condition formalization defeasible reasoning,since interesting nonmonotonic logics satisfy them, still consider satisfaction desiderata: properties intuitive give back stronglogical characterization consequence relation, solid semantic characterization based preferential interpretation (Kraus et al., 1990; Lehmann & Magidor, 1992),strong connections classic AGM approach belief revision (Alchourron,Gardenfors, & Makinson, 1985). Moreover decision problem often reducedprocedure based classical decision problem (as proposal goingpresent) allowing implement procedure top existing reasoners, systemsbased preferential approach rarely give back counter-intuitive results. mainproblems inferential power approaches often weak (we oftenpoint conclusions would like obtain, system able derive), preferential approach developed propositional logics,attempts extend rst-order languages turned quite problematic.419fiCasini & Stracciaproposal going present tries overcome inferential weaknesses, characteristic classical preferential approaches. presenting propositionallanguages, readapt procedure expressivity DLs since, even preferential approach cannot easily reformulated rst-order logics, turns stillappropriate fragments rst-order logic DLs.follows, shall present procedure building rational closureknowledge base using default-assumption approach (Poole, 1988; Makinson, 2005);approach reduces construction rational closure series checks basedclassical consequence relations. procedure presenting heavily reliesone Freund (1998).Specically, consider classical propositional language built nite set propositional letters P = {p1 , . . . , pn }, using classical connectives , , , , ; sentencesdenoted capital letters C, D, E . . ., sets sentences capital Greek letters, , . . ., usual meaning true false; knowledgebases, shall indicate consequential information means C C|D, respectively strict defeasible conditionals, read respectively C,always C, typically D. denotes classical entailment relation, and,given set formulae set strict conditionals , indicatemonotonic entailment relations obtained adding , respectively, set propositionalformulae set conditionals {C |= | C } extra information; shalluse | also indicate generic non-monotonic consequence relation.knowledge base (KB) agent represented means conditionalsmeans formulae; call conditional knowledge base pair hT , Di, niteset strict conditionals nite set defeasible conditionals.Example 2.3. penguin example encoded as: K = hT , Di = {p b}= {p|f, b|f, b|w}.Another way formalize defeasible information may based simply formulae, usingdefault-assumption approach: default-assumption knowledge base pair h, i,sets formulae representing respectively agent considersnecessarily true typically true.Example 2.4. penguin example could encoded as: K = h, = {p b}= {b f, p f, b w}.shall use Greek letter distinguish default-assumption formulae (i.e.,members ). next show map conditional knowledge base defaultassumption knowledge base (we transform KBs kind one Example 2.3KBs kind Example 2.4), show simple procedure reason withinlatter, relying decision procedure |=.proceed follows: (i) dene notions rational consequence relationrational closure, (ii) then, describe procedure build rational closure usingdefault-assumption knowledge base.420fiDefeasible Inheritance-Based Description Logicsconsequence relation | rational satises following properties (Lehmann &Magidor, 1992):(REF)(CT)(CM)(LLE)(RW)C|C every CC|DReexivityC D|FC|FC|DC|FC D|FC|FC|DCautious Monotony|= CD|F|= FC|FCut (Cumulative Transitivity)Left Logical EquivalenceRight Weakening(OR)C|FD|FC D|FLeft Disjunction(RM)C|FC6|DC D|FRational Monotonyrst six properties, (REF)(OR), characterise class preferential consequence relations: is, given conditional base= {C1 |E1 , . . . , Cn |En } ,say conditional C|D preferential closure P(D) derivableusing rules (REF)(OR) (Kraus et al., 1990). However, preferential closuregenerally considered inferentially weak satisfactory, natural lookstronger forms closure.closure rule (RM) considered, interesting rules,strongest one. However, given form rule (we negated conditionalpremises), rational extension conditional base unique. Indeed,multiple possibilities close condition: example,= {C|F }, neither C D|F C|D choose addeither order satisfy (RM); moreover, simple example shows,possible consequence relation obtained intersection dierent rationalextensions knowledge base satisfy (RM) anymore (in particular case,intersection would contain neither C D|F C|D). Hence, dene rationalclosure conditional base D, choose one possible rational extensions D. Lehmann Magidor dened rational closure operation R satisesset desiderata (Lehmann & Magidor, 1992, sect. 5.1-5.3).1. P(D) R(D). is, conditional base every conditional preferentiallyderivable rational closure D.2. every conditional form C|, C| R(D) C| P(D). Analogously,every conditional form |C, |C R(D) |C P(D). conditionals421fiCasini & Stracciaform C| dene situations simply considered impossible, conditionals form |C indicate considered typical general. kindsinformation properly managed preferential closure.3. C|F P(D), C|D, C D|F/ P(D), prefer closure operationadding C D|F instead C|D: sense rule (RM) employ constrained form monotonicity (given C|F , add C D|F ), arbitrarily addnew defaults (the addition C|D); hence, whenever possible, given conditionalC|F want consider strengthening C D|F instead unmotivatedaddition conditional C|D.shall describe Lehmann Magidors rational closure referring originalformulation (Lehmann & Magidor, 1992). Instead, shall directly refer correspondentconstruction, heavily relying procedure dened Freund (1998), basedtranslation conditional KB default-assumption KB, illustrate next.start conditional KB K = hT , Di. rst steps (Steps 1-3) deneexceptionality ranking conditionals KB, following analogous procedure Lehmann Magidor (1992): ranking permit distinguish correctlystrict defeasible knowledge contained KB (Step 4), since partstrict knowledge could implicitly contained D. allow us constructcorrespondent default-assumption KB (Steps 5-6). Specically:Step 1. translate strict knowledge defeasible conditionals, is, moveKB hT , Di h, i,= {C D| | C } .Intuitively, preferential setting, saying C valid equivalentsaying negation absurdity ((C D)|) (Bochman, 2001, sect. 6.5).Step 2. dene set materialisations conditionals , i.e.,material implications corresponding conditionals:= {C | C|D } .Also, indicate AD set antecedents conditionals :AD = {C | C|D } .Step 3. dene exceptionality ranking conditionals (Lehmann & Magidor, 1992, sect. 2.6). build ranking following notion exceptionality.Given set conditionals D, formula C exceptional preferentiallyentails |C (i.e., |C P(D)); recall conditional |C R(D) |CP(D).conditional C|D said exceptional antecedent C exceptional D. exceptionality proposition decided based |=(Lehmann & Magidor, 1992, Corol. 5.22), C exceptional set conditionals422fiDefeasible Inheritance-Based Description Logics(i.e., |C P(D)) |= C, set materialisationsconditionals D.Let E(AD ) indicate set antecedents result exceptional w.r.t. D,E(AD ) = {C AD | |= C} ,E(D) exceptional conditionals D, i.e.,E(D) = {C|D | C E(AD )} .Obviously, every D, E(D) D.Step 3.1. Taking consideration knowledge base h, i, constructiteratively sequence E0 , E1 . . . subsets conditional base following way:E0 =Ei+1 = E(Ei ) .Since nite set, construction terminate (empty nonempty) xed point E, i.e., set composed exceptional conditionals,materialisations negate antecedents.Step 3.2. Using sequence, dene ranking function r associatesevery conditional number, representing level exceptionality:r(C|D) =C|D Ei C|D/ Ei+1C|D Ei every .Step 4. Step 3 dened materialisation rank every conditional it. Now,Step 4.1. determine inconsistent. conditional base inconsistent derive conditional |. knowconditional form |C rational closure preferential closure, is, given result recalled Step 3.1, checkconsistency using : | P(D ) |= ;Step 4.2. consistent, dene background theory Te agent as2Te = { C | C|D r(C|D) = } .Moreover, one may verify every conditional logicallyequivalent conditional Te ;2. One may easily verify correctness definition referring results work Bochman(2001, sect. 7.5.3, Definition 7.5.1, definition clash p.178, Corollary 7.5.7, Definition 7.5.2,Lemma 7.5.5). suffices show set conditionals ranking value representsgreatest clash (the proof quite immediate definition exceptionality ranking).423fiCasini & Stracciae i.e.,Step 4.3. Te , also identify set conditionals D,defeasible part information contained : i.e.,e = {C|D | r(C|D) < } (obviously,e D) .Essentially, far moved non-defeasible knowledge hidden D,e Moreover, ranking valuesobtaining new conditional base hTe , Di.econditionals D.Step 5. build default-assumption characterization rational closuree so, translate Te set correspondent formulae ,e i.e.,hTe , Di.e = {C | C Te } ,e sequence default-assumptions (i.e., formulae) .e Specically, givenerank value conditionals D, construct sequence default assumptionse = h0 , . . . , n ,en highest rank-value D,^e r(C|D) i} .= {C | C|D(1)Dening default-assumptions way, presented Freund (1998), obtainset default formulae, one associated rank value, s.t. every defaultformula classically derivable preceding ones, is,|= i+1 , 0 < n .e default-assumption set ,e accordingStep 6. Given background theoryeesteps dened far, associate agent pair h, i. Combiningsteps main theorem Freunds work (1998, Thm. 24), showneedefault-assumption characterisation agent means pair h,equivalent rational closure pair hT , Di dened Lehmann Magidor(1992). is,Proposition 2.1. Given knowledge base K = hT , Di,C|D R(K) ,R rational closure operation defined Lehmann Magidor (1992), iffe {i } |= D,{C}e (we indicatefirst formula h0 , . . . , n consistent {C}C|h,ee D).424fiDefeasible Inheritance-Based Description Logicsconsequence, using following knowledge base transformationshT , DiehTe , Dih,ee ,h,()ee means Proposition 2.1,characterise rational closure hT , Di via h,i.e.,C|D R(hT , Di) C|h,ee .So, method decide defeasible consequence rational closure. Specically, given defeasible knowledge base hT , Di propositions C D,1. all, apply hT , Di transformations () obtain defeasible knowle i;eedge base h,ee =2. given C, determine rst ({C} )-consistentformula sequenceh0 , . . . , n i.3. decide follows rational closure C w.r.t. hT , Di determininge {i } D.whether {C}Example 2.5. Consider case penguin, knowledge base Example 2.3. First (Step1), move strict knowledge defeasible part, obtaining= {p b|, p|f, b|f, b|w} .(Step2) define set materialisations= {p b , p f, b f, b w} ,correspondent set antecedentsAD = {p b, b, p} .use set materialisations determine ranking value formulaeAD conditionals (Step3), obtaining0 = r(b) = r(b|f ) = r(b|w)1 = r(p) = r(p|f )= r(p b) = r(p b|) .(Step4), define conditional basee = hTe , Die ,KTe = { p b}e = {p|f, b|f, b|w}(since case strict defeasible part conditional base correctlye K).separated already initial base K, obtain K425fiCasini & Stracciaconditional base translated knowledge base(Step5),eeh,e = {p b}e = {0 , 1 } ,0 = (p f ) (b f ) (b w)1 = p f .Using default information, conclude (Step6) penguins fly, birds flybirds wings.Remark 1. Considering Example 2.5, would intuitive also conclude penguinswings (p|w), rational closure category recognized atypical,category penguins present case (they birds, dont fly, consequentlyr(p) = 1), cannot inherit typical characteristics super classes. Henceallowed conclude that, presumably, penguins wings. weak inferentialpower generally considered main limit rational closure. hand,going see next section, INs manage successfully kind problems.procedure determine rational closure maintains computationalcomplexity classical decision procedure, since easily veried transformations () require O(|K|) entailment tests and, given also proposition 2.1fact strict part encode (monotone) propositional theory,Proposition 2.2. Deciding propositional defeasible consequence rational closure (|, )coNP-complete.Lehmann Magidor (1992) specify also semantic characterization propositional rational closure, alternative correspondent construction recentlypresented Giordano et al. (2012a). move propositional logic DLs, versionrational closure language ALC previously proposed (Casini & Straccia,2010), procedure semantically characterized means preferential DLinterpretations (Britz, Casini, Meyer, Moodley, & Varzinczak, 2013).seen above, rational closure denes non-monotonic consequence relation intuitive behaviour strong logical properties; however, Remark 1, also somewhatweak, often conclusions exceptional situations that, despite intuitive,cannot derive. behaviour due fact procedure associates setpremises conditionals least exceptional.Next, going rene rational closure order avoid loss inferentialpower w.r.t. exceptional premises. proposal based modication initialknowledge base: add new conditionals give information exceptional cases426fiDefeasible Inheritance-Based Description Logicswould lost rational closure procedure. renement obtained usingranking procedure, applying locally, is, order decide conditionalC|D added KB apply procedure rational closure,consider information relevant inferential connection CD. example, assume knowledge base composed set conditionals= {p|q, q|r, q|t, p|t}; now, following procedure rational closure obtainp exceptional proposition, one, cannot derive neither p|rp|t. But, want derive p|t, already p|t, intuitivelyreason avoid conclusion p|r. fact, conclusion woulddesirable, since p q, p r generate conict restinformation knowledge base.So, aim specify way decide information KB relevantw.r.t. particular connection (in case, p|r). order determine localrelatedness going consider INs: use graphical characterisation orderidentify relevant information w.r.t. connection want investigate,apply ranking procedure pieces information recognised relevant.3. Boolean Defeasible Inheritance Networkspresent new decision procedures INs, based classical propositional decisions,that, addition main step nonmonotonic construction goingpresent later on, turns interesting decision procedure per se.following, proceed follows. rst, dene procedure INs,map propositional logic, obtaining desired renement rational closure.3.1 Exceptionality Levels Inheritance Netsrst aim apply INs modied version decision procedure rationalclosure; order dene method deciding validity INs relypropositional calculus allow easily (i) extend method order includelanguage also propositional connectives , , , (ii) integrate rationalclosure, order extend inferential power rational closure without compromisinglogical properties. non-negligible side product propositional SAT-basedreasoning procedure.shall briey review case purely strict nets showing case easilymanageable using propositional calculus, shall focus mixed nets.3.1.1 Strict Netsstrict part nets want obtain valid connections classicalproposals. net composed strict links, i.e., N = hS, i, valid connectionsconsistency easily checked using propositional calculus. Indeed, dene classicalpropositional language using nodes N propositional letters (call PN setpropositional letters), connectives, translate set linksset corresponding propositional implications= {p q | p q S} {p q | p 6 q S} .427fiCasini & Stracciaindicate l literal (being literal propositional letter negation),dene set antecedents implications ,= {p | p l } .derive valid paths using classical consequence relation 3 .Proposition 3.1. Consider net N = hS, translate set propositionalimplications . following properties hold:1. N consistent net, valid strict positive (resp., negative) path hp, , qip q, Np q (resp., Np 6 q), iff p q (resp., p q).2. N inconsistent iff p p .3. Deciding strict consequence done polynomial time.treat decision problem strict nets means classical propositionalcalculus, obtaining exactly valid strict paths classical approaches nets.Note dierence notion inconsistency INs propositional logic. seen Section 2.1, net considered inconsistent node pthat, simultaneously, positively negatively connected another node q: pnot, simultaneously, subclass q. inheritance nets, situation interpreted contradiction, propositional logic correspondent situation( |= (p q) (p q)) would force negation propositional letter (i.e.,node) p ( p), would correspond saying individual fallclass p.3.1.2 Mixed Netsconsider nets strict defeasible links. follows assumestrict part net N = hS, U inferentially closed, is, Np q (resp.,Np 6 q) p q (resp. p 6 q S).procedure diers classical approaches INs mainly basednotion potential path; instead, translate nets links propositionalformulae, build exceptionality ranking using procedure similarone dened rational closure. main dierence procedure denedrational closure lays local characterization exceptionality rankings: checkvalid connection pair nodes p q proceed deningexceptionality ranking nodes; however, consider nodes net,related p q. relation determined means notioncourse, generalisation potential path.Roughly, courses simply routes net following direction arrows,without considering positive negative arrow.Definition 3.1 (Course). Courses defined follows (where {, 6, , 6}):3. Note strict links encoded 2-CNF formulae, also called Krom formulae,propositional 2-SAT problem P .428fiDefeasible Inheritance-Based Description Logics1. every link p q N course = hp, qi N ;2. = h, qi course q r link N already appear ,= h, ri course N .omission repetitions courses needed guarantee niteness courseseven net contains cycles. So, given net N dened nite number links,nite set C N courses, that, turn, nite sequences nodes. denoteN set courses N going node p node q, i.e.,Cp,qNCp,q= { C N | = hp, , qi } .next provide procedure denes validity defeasible connectiontwo nodes p q, via mapping propositional logic. Given net N = hS, U i,dene correspondent knowledge baseKN = hN , N ,N = {p q | p q S} {p q | p 6 q S}N = {p q | p q U } {p q | p 6 q U } .following, may omit N clear context.dene exceptionality ranking nodes, depends decision problemrespect p q only.4So, letp,q = {r | r , Cp,q }{r | r 6 , Cp,q } ,consider set relative antecedents (l literal)Ap,q = {a | l N p,q } .following, N denote supra classical entailment relation obtained addingset propositional formulae N extra axioms. strict part net,p N q (resp., p N q), say q (resp., q) follows strictly p N ,indicate p N q (resp., p N q).hand, |N indicate inference relation defeasible part,is, p|N q read member class p typically also member classq N . Analogously p|N q negative case.4. main difference w.r.t. procedure propositional rational closure: rankinformation KB once, rank information related connectioninterested in, p q.429fiCasini & StracciaNow, use N p,q determine exceptionality level. investigating connection p q, node Ap,q exceptional negatedinformation contained p,q (compare Step 3.2 Section 2.2):E(Ap,q ) = {a Ap,q | p,q N a}E(p,q ) = {a b p,q | E(Ap,q )} .Therefore, like Step 3.3 Section 2.2, build sequence0 = Ap,q= E(i1 ) ,corresponding sequenceE0 = p,qEi = E(Ei1 ) .Since Ap,q p,q nite, every i+1 Ei+1 Ei , sequencesterminate (empty non-empty) xed point function E, Section 2.2.Dene ranking function (like Step 3.4) r associates every implicationp,q number, representing level exceptionality:rp,q (a) =/ i+1rp,q (a) =rp,q (a b) = (a b) Ei (a b)/ Ei+1rp,q (a b) = (a b) Ei .Clearly, r(a b) = r(a) every b p,q . following, assumeobtain node ranking value (that is, function E terminatesempty set). see later (Proposition 3.6) latter case netinconsistent.b p,q implications b p,q leastconsider setexceptional p,b p,q = {a b p,q | r(a b) r(p)} ,eventually deneb p,q p qp|N qNbp|N q p,q N p q .language nets, indicate inference relation generatedprocedure symbol . is,Np qp N qNp 6 qp N qNNp qp 6 qp|N qp|N q .So, given N = hS, U pair nodes hp, qi, inference procedure INssummarised follows:430fiDefeasible Inheritance-Based Description Logics1. Close strict validity.2. Check direct (and hence valid) link N connecting p q. is,connection valid. Otherwise, proceed.3. Determine set Cp,q courses N connecting p q, map linksCp,q sets implications p,q , dene set Ap,q antecedentsimplications p,q .4. Determine ranking value every proposition Ap,q every implicationp,q .b p,q implications least exceptional p.5. Dene set6. decide Nb p,q p q).(p q (Nb p,q p qp 6 q) determining whetherPlease note rely decision procedure only. examplesillustrate behaviour method.Example 3.1. Consider Example 2.1 additional links b p (readtweety).wbfpFigure 1: Example 3.1translate net following knowledge baseK = h, ,= {t b, p, p b}= {p f, b f, b w} .Suppose now, want decide connected f (i.e., Tweety flies).Since link b w appear course f ,t,f= {p f, b f }At,f= {t, b, p} ,431fiCasini & Stracciaobtaint,f p t,f .Thus,0 = r(b) = r(b f )1 = r(t) = r(p) = r(p f )So,b t,f f ,and,b t,f = {p f }t|N f ,expected.next, ask connected w (i.e., Tweety wings). Now,t,w = {b w}At,w = {t, b, p} .t,w imply negation members At,w ,0 = r(t) = r(p) = r(b) = r(b w)b t,w = t,w .b t,w w,t|N w ,expected.Example 3.2. Consider Nixon Diamond (see Figure 2), n Nixon, r republican, q quaker, p pacifist; another classical problem nonmonotonicreasoning, similar previous one informed pathspecific (while link p b tells us informationpenguins specific information birds). So, want neithern p n 6 p validated.qpnrFigure 2: Nixon diamond.432fiDefeasible Inheritance-Based Description Logicsknowledge base K corresponding net composed= {n r, n q}= {r p, q p} .want check n connected p. So, n,p = , negated antecedent nn,p = . Sincen,p 6 n p(n,p n): r(q p) = r(r p) = 0 r(n) = 1, i.e.,n,p 6 n p, concluden 6 |Nn 6 |Npp .two following examples illustrate procedure Hortys skeptical closure,notwithstanding often manifest similar results, always give backresults, one included other.Example 3.3. Consider net Figure 3.fgxpnFigure 3: Example 3.3want investigate valid connection p. AccordingHortys skeptical closure, cannot conclude anything p (N 6 p). Instead,p), since r(a) = r(f ) = r(g) =approach obtain a|K p (Nr(x) = 1.Example 3.4. Consider net Figure 4.pcefbFigure 4: Example 3.4want investigate valid connection p b. AccordingHorty (1994) conclude Np 6 b, approach cannot conclude anything.433fiCasini & StracciaSo, even many situations results two approaches same,obtain dierent results them. dierent outcomes mainly due dierenceconicts interpreted. Consider Example 3.4, unresolved conicttwo paths p f , is, one two paths preempts other,none considered valid, Hortys approach. Hortysinterpretation, conict prevents also construction paths starting ppassing f : order constructible path built augmenting validshorter path, thus cannot construct path starting p passingf (Horty, 1994, Def. 2.1.1). So, unresolved conict totally eliminates possibilityconsider paths ample argumentations, could play role.hand, approach radical conicts: factcannot conclude neither p f p 6 f eliminate possibilityactual world one connections true; simply enough informationdecide. possibility p f eectively valid invites us take considerationpotential argumentation moving p b. So, looking connectionp b Example 3.4, Horty cannot consider path hp, c, t, f, bi, avoidingrise conict path hp, e, bi, approach still consider possibilityhp, c, t, f eectively true, allowing path hp, c, t, f, bi play role decidingwhether valid connection p b. way potentialconict hp, e, bi prevents validity latter. signicant examplesapproach see Appendix A.Notice that, even built notion courses, procedure respectsclassical notion potential path, is, every valid connection corresponds potentialpath net (Denition 2.1).Proposition 3.2. Consider net N . every connection p|N q (resp., p|N q) validatedprocedure, corresponding positive (resp., negative) potential path pq net N .3.1.3 Inference RelationTalking nets, structural properties characterizing rational consequence relations,REF , CT , CM , RM , take following form5 :(REF)(CT)(CM)(RM)NNNNp q every p q NpqN,p qrsNrspqNrsN,p qrsrsN 6 pqN , p6 qrsmeaning properties still propositional case, simply readaptedexpressivity INs: net represents information disposal,premises derivation, links informational atoms language.5. , {, 6}; N , b premises indicates addition direct link b net N ;6 indicates opposite arrow (e.g. 6 iff 6)434fiDefeasible Inheritance-Based Description LogicsHence, sense rules before. (REF) indicates whatever pieceinformation (link) premises, appears also conclusions. (CT) cutcondition, states validity link derived links restnet, link eliminated without aecting set conclusions derivablenet. (CM) form constrained monotony, opposite (CT), stateswhatever conclusion derived net, added premises withoutaecting conclusions. (CT) (CM) intuitive appeal,logical point view characterizeclosure operation. translation (RM) lessintuitive, since INs classical notion negation,notion conict; hence sense rule that, p q consequence N ,addition information conicting p q, i.e., p 6 q, aectdefeasible consequences net N . fact INs share classical logicnotions contradiction negation makes formulation (RM) less intuitiveinteresting.proprieties (REF), (CT) (CM) often considered proprieties nonmonotonic consequence relation satisfy (Kraus et al., 1990; Makinson, 1994),interesting check satised formalism. know classicalapproaches inheritance nets satisfy (CT) (CM) (Makinson, 1994, pp. 56-57),approach logically appealing.Proposition 3.3.satisfies (REF), (CT) (CM).(RM) satised.Proposition 3.4.satisfy (RM).following example proves propositionExample 3.5. Consider net Figure 5. net composed links p f ,f b, p t. Np b N 6 b, N , 6 b 6 p b.bfpFigure 5: Counterexample RM.example actually shows that, dealing notion negation consistencycharacterize INs, (RM) look desirable property anymore, since addition 6 b net creates Nixon Diamond want derivep b (see Example 3.2).435fiCasini & Stracciaproperties logical consequence relations, left equivalence right weakening,analogous following properties:(LE)(RW)Npq Npr NNrqpqNqrNprNrpalso introduce property corresponds logical property supra classicality (if C D, C|D), rule satised non-monotonic consequence relations:(Sup)NNProposition 3.5.pqpqNNp 6 qp 6 qsatisfies (LE), (RW ), (Sup).3.1.4 Consistencyindicated end Section 2.1, net considered inconsistent forcedconclude pair nodes p, q p q positively negativelyconnected. Since, seen above, satises (Sup), say mixed net consistentcannot conclude N p q N p 6 q pair nodes p, q.going see order check consistency mixed net useranking procedure: sucient apply whole net. propositionalcase (see Section 2.2), ranking procedures dened nodes net terminates,nite number steps, either empty set n xed point functionE, i.e., set nodes result always exceptional. case, saynodes innite ranking value (r(p) = ). want check whether net Nconsistent, sucient apply ranking procedure entire net, seenodes innite ranking.Proposition 3.6. net N consistent iff node p r(p) = ,is, conclude N p q N p 6 q pair p, q.Example 3.6. net Figure 6 example inconsistent net,f N6 f . net translatedwould conclude NbfpFigure 6: example inconsistent net.436fiDefeasible Inheritance-Based Description Logicsknowledge base= {t b, p, p b, b p}= {p f, b f } .proceed ranking entire net, obtain p, bt, is, E1 = . Hence, fixed-point exceptionality ranking function,p, b, ranking value.3.1.5 Propertieseld inheritance networks, taxonomy dierent approaches developed basis relevant properties (Horty, 1994). briey checksatised approach.Purely defeasible / mixed nets. Cyclic / acyclic nets. procedure deals easilytwo properties often create problems traditional approaches: presencestrict defeasible links (mixed nets), presence cycles (cyclicnets).Credulous / skeptical / directly skeptical approaches. approach correspondsdirectly skeptical approach: given net, obtain unique set valid connections(vs. credulous approach, allows dierent sets valid paths, possiblyconict other), unique set obtained intersectiondierent possible extensions (as skeptical approaches), obtainedsingle closure operation.Upward / downward chaining. denition valid paths, useform induction length, neither starting initial node towardterminal node (upward chaining), reverse direction (downward chaining);hence, form chaining used procedure.On-path / off-path preemption. O-path preemption classical form preemption, used also Horty (1994, Def. 3.2.2), on-path preemptionbinding, requiring preempting node lie initial segment path preempts (Horty, 1994, sect. 4.2.4). exactly formalise form preemption,since confront directly dierent paths two nodes. However,procedure behaviour analogous use o-path preemption.3.1.6 Computational Complexitydene overall complexity decision procedure nets, considercomplexity course-identication procedure, is, given net N = hS, Utwo nodes s, N , computational cost identify s,t (note easilycomputed polynomial time), whose size bounded polynomially size N .Given construction courses independent nature links (eitherpositive negative, defeasible strict), analyse problem using simpledirected graphs. Given net N = hS, U i, sucient dene correspondent directedgraph G = hV, Ei following way:437fiCasini & StracciaV set nodes N .E set directed links ha, bi, a, b V , s.t. ha, bi E one followingholds:b , 6 b , b U , 6 b U .Recall stated presence 6 b implies b 6 too. So,6 b ha, bi hb, ai E.dened G, let us recall well-known result graph theory sayingdirected graph, given two nodes p q, determining path p qdetermined time O(|V | + |E|), e.g. using BFS (Breadth First Search) (Cormen,Stein, Rivest, & Leiserson, 2001). Now, following argument shows indeed s,tdetermined polynomial time 6 . rst, check path t.not, s,t = . Otherwise, call procedure Delta(s) below:Delta(s): outgoing edge hs, xi s, hs, xi x marked,do: path x mark hs, xi x, recursively,call Delta(x).nished, s,t immediately build marked edges. Note edgemarked node marked (i.e., explored) and, thus, algorithmbounded polynomially size graph.found set s,t , apply decision procedure basedpropositional rational closure decide valid connection pq (as Section 2.2, number entailment tests polynomially bounded sizenet). formulae 2-CNF, like Proposition 3.1, obtain decisionprocedure w.r.t. net respects complexity costs related propositional calculus.Proposition 3.7. Deciding defeasible consequence inheritance networks (done polynomial time.)Eventually, want determine valid links net N considerpairs nodes net N . So, obtained graph G = hV, Ei repeatprocedure elements set pairs nodes graph, whose cardinality|V |(|V | 1). Hence,Proposition 3.8. Computing valid connections net done polynomialtime.3.2 Boolean Inheritance Netsnext extend INs introducing classical propositional connectives , , .Despite extension felt desirable, aware attemptdirection (Horty & Thomason, 1990).6. interested figuring tight bound.438fiDefeasible Inheritance-Based Description LogicscccdcdFigure 7: DisjunctionFigure 8: Conjunction3.2.1 Negationfar, used link 6 indicate two classes disjoint: p 6 q p qlogical meaning. change notation substitute 6 , indicatingp q class p class q complementary (i.e., p q), generalindicate complementary class class p p. Hence, substitute everylink p 6 q net four links: p p, q q, p q, q p. Moreover,eliminate negative defeasible links, since p 6 q expressed p q q.So, transform net using arrows , , . shallcontinue use 6 macro indicating valid negative strict connections obtainedcomposition , is, indicate p 6 q presence path. . } q ,p| {z. . . b} |c .{zn arrowsn, 0.arrows3.2.2 Conjunction DisjunctionNext, extend inheritance nets support conjunction disjunction well, allowinglinks a, b c (conjunction b equivalent c) c a, b (disjunctionb equivalent c). assume inheritance nets containing kindlinks closed according following rule: a, b c (resp., a, b c)net, also c c b (resp., c b c) net. callnets Boolean Defeasible Inheritance Networks (BINs). shall use b bindicate, respectively, node represents conjunction disjunctionb. Graphically, indicate disjunctive conjunctive links Figure 7 Figure 8,respectively.extend reasoning method BINs. so, need amplify notioncourse, introducing notion duct: consider linear routes one pointanother, also parallel routes, order model introduction conjunctionconsequent introduction disjunction antecedent. Roughly,= hs,, tiindicate duct starts node develops ducts ,reaching node t.439fiCasini & StracciaDefinition 3.2 (Duct). Ducts defined follows (where {, 6, , 6}):1. every link p q N duct = hp, qi N ;2. = h, qi duct q r link N already appear ,= h, ri duct N ;3. = hq, duct r q link N already appear ,= hr, duct N ;t,4. ht, , pi hr, , pi ducts, t, r S, hs, r,, pi duct;5. hp, , ti hp, , ri ducts, t, r S, hp, ,t,r , si duct.reasoning method BINs follows. Given net N = hS, U i, denecorrespondent knowledge base K = h, i,= {p q | p q S}{p q | p q S}{p q r | q, r p S}{p q r | p q, r S}{p q | p 6 q S}= {p q | p q U } .Now, may proceed denition |N Section 3.1, simply considering C NN (or simply C ) set ducts p q.set ducts N , Cp,qp,qExample 3.7. Consider net N illustrated Figure 9. net N mappedegbcfFigure 9: Example 3.7KB K = h, i,7= {c g, f g}= {a b, b c, b d, b e, f } .7. ease reading, omitted redundant implications g c, obtained c,g, g c N .440fiDefeasible Inheritance-Based Description LogicsNow, ask whether connected c. verifieda,c = {a b, b c, b d, f } .Note b a,c , duct c passes c orderreach g, back towards c. Now, negated antecedent (a,c a)and, thus,b a,c = {a b, f } .b a,c 6 cb a,c 6 c,Sincea6|N c a6|N c .similar way, may show a6|N a6|N d. desirable result: sincef direct link, a|N f (i.e., a|N (c d)), hence knowcannot conclude a|N c a|N d. But, since evidence whether oneconclusions preferred other, conclude either them.result skeptical approach a6|N c, a6|N c, a6|N d, a6|N d.hand, since duct connecting e ha, b, ei (that is, nodes c, d, g, fplay role possible argumentation connecting e), conclude a|N e.3.2.3 Propertiescall BIN inference relation dened dened closure operation BINshave:NBINp q p qNBINp 6 q p qNBINp q p|N qNBINp 6 q p|N q .BINs inherit structural properties INs, is, (REF ), (CM ),(CT ). Analogously, (LE), (RW ), (Sup) still valid.Proposition 3.9.Proposition 3.10.BINsatisfies (REF ), (CM ), (CT ).BINsatisfies (LE), (RW ), (Sup).Since procedure dened Section 3.1 simply special case procedureBINs, (RM) falsied also BINs counter-example proposition 3.4. Moreover, introduced conjunction disjunction, express analogous rulesdisjunction premises (OR) conjunction consequent (AND):(OR)(AND)NBINpq NNNBINsq NBIN qBINpq NNps NBIN pBIN441BINp,BINq,fiCasini & Stracciasense propositional case, remains intuitive alsoBIN environment: (OR) represents validity reasoning cases, (AND)represents conjunction distinct conclusions still valid conclusionnet.Proposition 3.11.BINsatisfies (OR) (AND).3.2.4 ConsistencyAlso w.r.t. consistency, obtain result INs, i.e., net consistentranking procedure terminates empty set.Proposition 3.12. BIN N consistent iff node p r(p) = ,is, cannot conclude p q p 6 q pair p, q.Remark 2. seen Section 3.1.4 notion consistency inheritance nets different notion consistency propositional logic. Using procedure netinconsistent if, applying ranking function entire net, obtain noderanking value (see Proposition 3.6). point, find net inconsistent,simply stop decision procedure.follows going work BINs framework propositional logic.So, order assimilate notion consistency one propositional logic,shall consider modified version procedure BINs. Supposedecide validity connection two nodes p, q net N . N resultsconsistent, proceed above, otherwise, net results inconsistent (some nodeinfinite ranking value) simply stop, but, case r(p) < , still applyprocedure. Otherwise, p infinite ranking value (r(p) = ), proceedfurther.3.2.5 Computational ComplexityINs, determine s,t (computing immediate) ductsBIN. Now, dicult see recursive BFS graph travelling procedureone devised INs worked BINs well. illustration, referFigure 9 assume processing node b. Since b, c reachabled, c g S, and, recursively, path g c, mark hb, ci hb, di,mark conjunction d, c g visited. Again, nodes aggregatednodes visited ones, guaranteeing polynomial cost computing s,t .Now, s,t determined polynomial time, Section 2.2,number entailment tests polynomially bounded size net, strictpart may encode propositional formula and, thus, unlike case INs, have:Proposition 3.13. Deciding defeasible consequence BINs (problem.BIN )coNP-complete4. Defeasible Inheritance Propositional LogicNow, depart BINs apply similar reasoning procedure frameworkpropositional logic, show obtain kind closure knowledge base442fiDefeasible Inheritance-Based Description Logicsresults rational consequence relation, informative classicalrational closure (Lehmann & Magidor, 1992).consider propositional language , , connectives. So, startconditional KB K = hT , Di (see Section 2.2), = {C1 D1 , . . . , Cn Dn }= {E1 |F1 , . . . , Em |Fm }.Step 1. Given conditional base K = hT , Di, check K preferentially consistent (thatis, check materialisation consistent; Section 2.2, Step 4.1). consistent,dene BIN K, i.e., net NK = hSK , UK i, modelling information Kfollowing way:(i) consider every formula C appears antecedent succedentconditionals K, create node C representing them, modulological equivalence (that is, node C represents class formulae logicallyequivalent C).(ii) node add also, already present, complementary node (thenode representing negation), link ;(iii) add strict links: C add strict link C net;also add SK strict links correspond logical dependenciesformulae represented nodes w.r.t. consequence relation. appears conditional, add net correspondent node ,and, every node n net, add strict node n . Analogously,add n every node n net.(iv) eventually, C|D D, add defeasible link node C node D.Step 2. apply reasoning procedure BINs NK (Section 3.2) identify validdefeasible connections C|N add C|D conditional base Kobtain new conditional base K = hT , 8 .Step 3. Finally, apply K rational closure (Section 2.2) dene nonmonotonic consequence relation |KC|K C|D R(K ) .Now, showProposition 4.1. |K rational consequence relation containing K.Example 4.1. Consider penguin example. modify slightly orderconsider also use connectives. birds (b) typically fly, live trees,wings (f , t, w), penguins fly live trees (f t). So, knowledgebase K = hT , Di be:= {p b}= {b|f, b|t, b|w, p|f t} .8. modify , since strict connections valid net classically derivable .443fiCasini & StracciaNotwithstanding penguins atypicality birds, penguins wings, would likeable derive information disposal, is, would like concludep|w. Please note possible using classical preferential approaches,obtain conclusion passing trough first step closure operation, is,defining corresponding net.Specifically, knowledge base K define net Figure 10 (the dashed arrowsstrict arrows explicit conditional base, logically validadded SK construction net NK ).fftfpfpbwwbFigure 10: Example 4.1Using procedure defined BINs, net obtain new knowledge baseK = hT , ,= {b|f, b|t, b|w, p|f t, p|w, p|f, p|t, b|f t} .Note that, new conditionalsp|f,p|t,b|fwould present also simple rational closure K (we obtain Right Weakening), obtained also conditionalp|w ,444fiDefeasible Inheritance-Based Description Logicswould present rational closure K (see Remark 1).Now, following procedure defined Section 2.2, compute rational closurenew knowledge base K , obtaining rational consequence relation containsoriginal K.Please note that, using BINs, could derived anything else,since vocabulary would limited propositions expressed nodes; however,relying rational closure propositional knowledge bases, reason usingfull expressivity propositional language, deriving new conditionals as, example,b green|f ,derived using BINs green appear net.next example shows another characteristic approach. preferentialapproach typicality absolute property proposition w.r.t. agents knowledgebase, is, class results atypical w.r.t. class (as penguins w.r.t. birds),results atypical w.r.t. entire knowledge base. approach instead, typicalitycomparative notion: consider class exceptional respect superclass,absolutely typical respect another.Example 4.2. Consider red fish (r). fish (f ) pet (p). Typically, fishgills (g) scales (s), pets docile (d) play kids (k). Red fishestypical pets, since play kids. So, K = hT , Di= {r f, r p}= {r|k, p|k, p|d, f |g, f |s} .rational closure red fishes, since atypical pets (they play kids),result atypical general, cannot inherit typical propertiessuper classes.Instead, want red fishes inherit, besides properties pets compatible(d), also typical properties fishes (g s), since considertypical fishes.so, translate knowledge base net Figure 11. netobtain new knowledge base K = hT , i,= {r|k, p|k, p|d, f |g, f |s, r|d, r|g, r|s}derived exactly desired conditionals.Next, compute rational closure K , following procedure defined Section2.2, obtain rational consequence relation containing K informationred fishes, information that, intuitive is, would able derivesimple rational closure K.Therefore, dened new rational consequence relation K extends K,K R(K ), contains intuitive conditionals rational closure K.445fiCasini & StracciaggffprrpkkFigure 11: Example 4.2Consistency. Dened inference procedure, conditional base K consistentcannot derive |. seen (Section 2.2) rational closure conditional baseconsistent preferential closure consistent (| R(K) | P(K)). Here,given base K, obtain procedure preserves preferential consistency K:seen Section 2.2, K preferentially consistent rational closure consistent (i.e.,6 |rcK ), prove following.Proposition 4.2. Given conditional base K, |K iff | R(K).results Section 2.2, corresponds saying |K K (assumingK = hT , Di, K = D).Computational Complexity. Considering procedures dened BINs,conclude dened procedure complexity rational closure,composition procedure dened BINs (Proposition 3.13) nal rationalclosure operation (Proposition 2.2).Proposition 4.3. propositional conditional base K, deciding C|K coNPcomplete problem.5. Defeasible Inheritance DLsNext, apply method signicant DL representative, namely ALC (Baader et al.,2003, ch. 2). ALC corresponds fragment rst order logic, using monadic predicates,called concepts, dyadic ones, called roles.order stress parallel procedure presented Section 2.2proposal ALC, going use notation components playinganalogous role two construction: use p, q, r, . . . concept names, C, D, E, . . .indicate concepts general, instead, respectively, atomic propositions propositions, |= | indicate, respectively, classical consequence relation ALC446fiDefeasible Inheritance-Based Description Logicsnon-monotonic consequence relation ALC. indicate default concept, is,concept assume applying every individual, informed contrary.nite set concept names C = {p, q, r, . . .}, nite set role names R ={R, S, T, . . .} set L ALC-concepts dened inductively follows: (i) C L;(ii) , L; (iii) C, L C D, C D, C L; (iv) C L, R RR.C, R.C L. Concept C used shortcut C D. symbolscorrespond, respectively, conjunction disjunction classical logic.Given set individuals O, indicated bold letters a, b, c, . . ., assertionform a:C (C L) form (a, b):R (R R), respectively indicatingindividual instance concept C, individuals b connectedrole R.general inclusion axiom (GCI) form C (C, L) indicatesinstance C also instance D. use C = shortcut pairC C.FOL point view, assertions inclusion axioms easily mappedFOL following transformation:(a:C) = (a, C),(C D) = x.( (x, C) (x, D)),(x, A) = A(x),(x, C D) = (x, C) (x, D),(x, R.C) = y.(R(x, y) (y, C))(x, R.C) = y.(R(x, y) (y, C))((a, b):R) = R(a, b),(x, ) = (x), (x, ) = (x),(x, C) = (x, C),(x, C D) = (x, C) (x, D),new variable,new variable .Now, classical knowledge base dened pair K = hA, i, niteset GCIs (the TBox ) nite set assertions (the ABox ), whereas defeasibleknowledge base represented triple K = hA, , Di, additionally nite(an instance concept C typically instance conceptset conditionals C9D), C, L .Example 5.1. Consider penguin example. add role P rey vocabulary,role instantiation (a, b):P rey read preys b, add also twoconcepts, (insect) f (fish). example defeasible KBK = hA, , Di= {a:p, b:b, (a, c):P rey, (b, c):P rey}= {p b, f i}f, b f, p P rey.f i, b P rey.i} .= {pparticular structure defeasible KB allows isolation pair hT , Di,could call conceptual system agent, informationindividuals (formalized A).9. Since monotonic part substitute meta-linguistic conditionals C formulae C D,substitute also defeasible part knowledge base conditionals C|D conditionalD, could call defeasible inclusion axioms.formulae C447fiCasini & Stracciafollows going work information concepts hT , Di rst,exploiting immediate analogy homonymous pair propositional setting,address case involving individuals well. show usingmethod overcome limits classical rational closure, already presentedALC (Casini & Straccia, 2010), similar way propositional case. Please noteprocedure presented based slightly modied version procedurerational closure previously presented Casini Straccia (2010), i.e., onepresented Britz et al. (2013). latter accompanied semantic characterization,based DL interpretations preferential relation dened individuals.semantic characterization rational closure ALC characterises steps procedure (the local applications rational closure nal one). However, still lacksemantic characterization overall procedure, accounting also modularizationknowledge base done using INs.Step 1. Given conceptual system K = hT , Di, check preferential consistency, is,dene= {C | C }D}= {C | Cconstruct BIN NK K. process one Section 4,treat concepts propositions: nodes NK represent concepts appearingantecedents consequents inclusion axioms (modulo logicalequivalence); every node add complementary node, already present,connect ; every GCI C becomes strict link C D;becomes defeasible link C D.every defeasible inclusion axiom CMoreover, consider consequence relation monotonic consequence relationobtained adding GCIs , add net strict links representinglogical dependencies nodes respect 10 .Step 2. Apply reasoning procedure BINs NK (Section 3.2) identifyconditional basevalid defeasible connections C D, add CK obtain new conditional base K = hT , i.Now, augmented knowledge base new defeasible conditionals,proceed follows.| C }.Step 3. Dene = {C} let = {C | C }.Step 4. Dene = {C | CStep 5. Determine exceptionality ranking conditionals using setantecedents AD materialisations, concept C exceptionalw.r.t. set conditionals |= C. steps propositional case (Step3 Section 2.2) replacing expression |= Cexpression |= C. way dene ranking function r.10. order create strict part net possible use techniques introduced procedureclassification DL knowledge bases (Baader et al., 2003, ch. 9).448fiDefeasible Inheritance-Based Description LogicsStep 6. Step 4.1, Section 2, verify KB consistent, checking consistency. (as Steps 4.2 - 4.3 Section 2.2), dene setsr(C D) = }Te = { C | Ce= {C | C r(C D) < } .e ,eStep 7. Dene (similarly Step 5 Section 2.2) sets concepts=le = {C | C }e = {0 , . . . , n } ,e r(C{C | CD) i} .Section 2, every , 0 < n, |= i+1 .Step 8. Now, dene inference relation |KC|K |= Cle D,erst {C} -consistentformula11 sequence h0 , . . . , n i.DL analogue Step 6, Section 2.2.Again, steps require decision procedure classical entailment relation |=DLs. redene properties characterizing rational consequence relationframework DLs.showProposition 5.1. |K rational consequence relation containing K.is, analogous properties propositional rational consequence relationsatised, namely:(REF) C|K C(LLE)(CT)C |K E|=T C =|K EC |K E(OR)11. is, 6|= C(RW)C |K(CM)C |K EC |K E|K E(RM)C |K Ede.449C |K|=T EC |K EC |K EC |KC |K EC |KC 6 |K EC E |KfiCasini & StracciaExample 5.2. Consider example 5.1, additionally also add role Born (Born(x, y)read x born y), concept e (Egg). ConsiderK = hT , Di ,= {p b, f i}P rey.f P rey., b P rey.i P rey., b Born.e} .= {ppbBorn.epBorn.ebP rey.f P rey.P rey.i P rey.P rey.f P rey.P rey.i P rey.Figure 12: Example 5.2(Step 1), build correspondent net NK (figure 12), obtain (Step 2)Born.e} .= {pmove rational closure. pair hT , changed (Step 3), f ,= { p bP rey.f P rey., b P rey.i P rey.,pBorn.e, p Born.e} .bset materialisations (Step 4)= {p b , f , p P rey.f P rey.,b P rey.i P rey., b Born.e, p Born.e}AD = {p b, f i, p, b} .obtain (Step 5) exceptionality ranking conditionals:, f , p P rey.f P rey.,E0 = {p bP rey.i P rey., b Born.e, p Born.e}b, f , p P rey.f P rey., p Born.e}E1 = {p b, f }E2 = {p b, f } = E .E3 = {p b2450fiDefeasible Inheritance-Based Description Logicsget ranking values every conditional : namely,P rey.i P rey.) = r(b Born.e)0 = r(bP rey.f P rey.) = r(p Born.e)1 = r(p) = r(i f ) .= r(p branking, obtain (Steps 6-7) background theoryTe = { (p b), (i f i)} ,e = {0 , 1 },default-assumption set0 = (b P rey.i P rey.) (b Born.e)1(p P rey.f P rey.) (p Born.e)= (p P rey.f P rey.) (p Born.e)used definition consequence relation |K (Step 8).example, derive typical penguins preys fishes, i.e.,p|K P rey.f ,insects, i.e.,p|K P rey.i ,also penguins born eggs, i.e.,p|K Born.e ,would derivable rational closure, presented Casini Straccia (2010).Computational Complexity. computational complexity point view, deciding entailment ALC ExpTime-complete (Donini & Massacci, 2000) and,Section 2.2, number entailment tests polynomially bounded sizeknowledge base, following exactly procedures dened propositional case,concludeProposition 5.2. Deciding C|K ALC ExpTime-complete problem.5.1 Closure Operation Individualsfar left ABox, going consider here. procedureABox built procedure TBox, is, consider knowledge basehA, , Di strict knowledge already moved , i.e.,axioms ranking value (that is, correspond sets Tee obtained using procedure previous section). basic idea followingprocedure consider individual named ABox much typical possible,is, associate possible defeasible information consistentrest knowledge base. order apply defeasible information locally451fiCasini & Stracciaindividual, encode information using materialisations inclusion axioms,e sectioni.e., set = h1 , . . . , n i, s.t. |= i+1 1 < n (the set12). want able associate individual (with setindividuals named ABox) strongest formula consistente = , i, callknowledge base. way dene new knowledge base KABox extension knowledge base hA, , Di.Definition 5.1 (ABox extension). Given knowledge base K = hA, , Di, knowledgebase , ABox extension K = hA, , Di iff, classically consistent AD .AD \ composed assertions a:C C = i,every h s.t. h < i,hT , AD {a:h }i |=denition identies extensions original ABox s.t. everyindividual associated defeasible information consistent restknowledge base. Still, main problem that, since individuals relatedroles, possibility associating default concept individualinuenced default information associated individuals, shownfollowing example.AR.A}Example 5.3. Consider K = hA, Di, = {(a, b):R} = D0 = {(hence = h0 = hA R.Ai). associate 0 a, obtain b:Acannot associate 0 b; hand, apply 0 b, derive b:Aanymore able associate 0 a. Hence, define two possible rational extensionsK.implies that, given knowledge base hA, , Di, even closure hT , Dialways unique possibility one ABox extensions.simple procedure obtain possible extensions knowledge base hA, , Di,set individuals named A, following:Definition 5.2 (Procedure ABox extensions).Consider set linear orders individuals O;= ha1 , . . . , do:Set j := 1Set AsD :=Repeat j = + 1:Find first default hAsD {aj :i }, }i 6|= .AsD := AsD {aj :i }.ee must done12. Note that, given conditional knowledge base, procedure determine Te ,all.452fiDefeasible Inheritance-Based Description Logicsj =j+1return hAsD ,indicate AsD ABox extension obtained using sequence s.Now, shownProposition 5.3. Given linear order individuals K, procedure determines ABox extension K. Vice-versa, every ABox extension K correspondsknowledge base generated linear order individuals O.e1 = hA{a:A R.A},instance, related Example 5.3, obtain extension Keorder ha, bi, K2 = hA {b:A R.A}, obtained order hb, ai.Example 5.4. Refer Example 5.1, let K = {A, , D},= {a:p, b:b, (a, c):P rey, (b, c):P rey}= {p b, f i}f, b P rey.i, p f, p P rey.f i}= {bknowledge base define set = {0 , 1 }0 = (b f ) (b P rey.i) (p f ) (p P rey.f i)1 = (p f ) (p P rey.f i) .consider order comes b, associate 1 a, consequently c presumed fish prevented association 0 b.consider b a, c fish cannot apply 1 a.Now, x priori linear order individuals, say a:Ce |= a:C, Kedefeasible consequence K w.r.t. order s, written K a:C, KABox extension generated K based order s.instance, related Example 5.3 order s1 = ha, bi, may infer K s1 a:A,order s2 = hb, ai, may infer K s2 b:A.interesting point consequence relation still satises propertiesrational consequence relation following way.453fiCasini & Straccia(REFDL )hA, , Di a:C every a:C(LLEDL )hA {b:D}, , Di a:CD=EhA {b:E}, , Di a:C(RWDL )hA, , Di a:CCDhA, , Di a:D(CTDL )(CMDL )hA {b:D}, , Di a:ChA, , Di b:DhA, , Di a:ChA, , Di a:ChA, , Di b:DhA {b:D}, , Di a:C(ORDL )hA {b:D}, , Di a:ChA {b:E}, , Di a:ChA {b:D E}, , Di a:C(RMDL )hA, , Di a:ChA, , Di 6 b:DhA {b:D}, , Di a:CshowProposition 5.4. Given K linear order individuals K, consequencerelation satisfies properties (REFDL ) (RMDL ).Note computational complexity point view, entailment w.r.t. ALC TBoxExpTime-complete (Donini & Massacci, 2000) number individuals K linearly bounded |K|, get immediatelyProposition 5.5. Deciding K a:C ALC ExpTime-complete problem.presence multiple ABox extensions, also dene inference relation, conservative inference relation independent order individuals,corresponds intersection inference relations modelling rationalextension.=\{ | linear order elements O}However, possibility lose property rational monotonicity,shown following example.Example 5.5. Consider knowledge base hA, Di s.t. = {(a, b):R} = D0 D1 ,R.A, B} = {A B, R.A B} (whereD0 = {101sets conditionals rank 0 conditionals rank 1, respectively).define two sequences individuals, = ha, bi = hb, ai,defining different rational extension ( ), let = .BhA, Di a:B, since extensions a:B holds (in axioma:A.6axiom r.A B) hA, Di 6 a:A, since hA, DiHowever, hA {a:A}, Di 6 a:B, since hA {a:A}, Di 6 a:B.454fiDefeasible Inheritance-Based Description Logicsincrease computational complexity decision procedure: assuming number individuals named ABox n, perform -testpossible sequences dened n individuals. is, worst casekneed n! tests, done time O(2|K| ) k. Now,2shown that13 n! < 2n and, thus, decision problem remains ExpTime.Proposition 5.6. Deciding K a:C ALC ExpTime-complete problem.Notwithstanding, conjecture many (probably most) real-world cases,knowledge base would single rational ABox extension, cases (RMDL )still valid. check whether knowledge base hA, , Di single rational ABox extension, sucient associate individual strongest modulo consistencyw.r.t hA, , Di, exactly procedure Denition 5.2, consistencycheck aj :i w.r.t. original instead w.r.t. AsD . end, check whetherhAsD , consistent; case obtained rational ABox extensionhA, , Di.following knowledge base unique ABox extension.Example 5.6. Consider KB Example 5.4, (b, c):P rey replaced (b, d):P rey.Then, whatever order individuals, obtain following associationdefault formulae individuals: a:1 , b:0 , c:0 , d:0 . Using informationdefaults, obtain unique default-assumption extension.semantic characterization , making use preferential DL models,presented Casini et al. (2013).lets briey consider heuristics useful case want presentsystem specic ABox queries. Assume want know particular individualpresumably falls concept C, want draw safest possible conclusion.presence multiple acceptable extensions, classical solution use skepticalapproach, i.e., use inference relation , corresponding intersectioninference relations associated possible ordering individuals appearing A.seen above, case multiple rational extensions computational complexity decision problem rise w.r.t classical ALC decision problem.Moreover, case multiple extensions, amount defeasible information associableindividual inuenced individuals related means role:immediate see role-connection ABox two individualsb, information associated inuence amountdefeasible information associate b, way around. Hence,ease decisions w.r.t. ABox introducing notion cluster, i.e., setindividuals named ABox linked means sequence role connections.so, given ABox A, indicate Q symmetric transitiveclosureroles vocabulary, i.e., symmetric transitive closure R.Definition5.3 (Cluster). Define Q reflexive, symmetric transitive closureR. Given individual O, call cluster set [a] individuals13. shown induction n, see e.g.http://lifecs.likai.org/2012/06/better-upper-bound-forfactorial.html.455fiCasini & Stracciaconnected Q:[a] = {b | Q(a, b)} .Hence, order know presumably conclude a, sucientdetermine w.r.t. sequence individuals [a]. Let A[a] ABox obtained restricting statements containing individuals [a]; query a:C clearly decidableusing A[a] .Proposition 5.7. hA, , Di a:C iff hA[a] , , Di a:C every orderingindividuals A[a] .query individual s.t. named ABox (a/ O),constraints dened ABox a, i.e., know a:; hence,individual appearing ABox, associate strongest defaultconcept consistent , 0 : s.t./ O, derive presumablya:C holds hAa , |= a:C, Aa = {a:0 }.6. Comparison Related Worknon-monotonic logics, so-called preferential approach distinguishedvarious proposals (as Reiters defaults, modal approaches, defeasible inheritance. . . ) mainly due logical properties, since former approach committedsatisfaction desirable structural properties consequence relation (see Section2.2). hand, considering point view inferential capacitypreferential approach often results weaker proposals, since oftendesirable, intuitive conclusions cannot derive (see Remark 1).proposal tried combine classical rational closure inheritancenetworks order overcome inferential limits without prejudicing logical propertiesconsequence relation.Section 3 also present alternative way reason defeasible inheritance.Despite proposal presented mainly integrate propositional languagerational closure, results interesting approach per se, Appendixcompare Hortys classical skeptical extension (Horty, 1994, sect. 2-3) Sandewallslandmark examples (Sandewall, 2010).indicated introduction, many papers aimed implementation non-monotonic reasoning DL formalisms. proposalscomparison approach done considering dierent non-monotonic formalisms, independently DL-environment. refer Makinsons work (1994)comparison various non-monotonic approaches.last years main proposals implementation nonmonotonic reasoningDLs connected two approaches: preferential one circumscription.preferential approach, work Britz al. (Britz et al., 2008; Britz, Meyer,& Varzinczak, 2011) preferential DL semantics strongly connected approach,one results semantic characterization rational closure cited(Britz et al., 2013).Still close approach work Giordano et al. (2012b), basedpreferential approach. conclusions derive using logic ALC+Tmin456fiDefeasible Inheritance-Based Description Logicsintuitive, complexity decision problem ABox co-NExpNP (Giordano et al., 2012b, Thm. 13), procedure cannot reduced classical entailment.Among proposals based circumscription, work Bonatti et al. (2009)particularly representative. point view quality inferences,proposal results dicult w.r.t. preferential approach draw expectedconclusions. example, assume knowledge base contains informationmammals typically live land, whales abnormal mammals liveland, ABox contains information a:M ammal W hale. knowinganything else individual a, would like reasoning system reasonassumption dealing typical mammal (since, moreover, speciedwhale) hence able derive lives land. However, usingcircumscription, conclusions draw changes w.r.t. concepts user decideskeep fixed varying (a non-trivial choice), results ablederive a:Habitat.Land, able derive it, even derive whalesexist (Bonatti et al., 2009, sect. 2.1). proposal instead, formalizeproblem knowledge base hA, , Di = {a:M ammal} (we need specifywhale), = {W hale ammal, W hale Habitat.Land} =Habitat.Land}; without needing kind choice user, system{M ammalderive automatically a:Habitat.Land. Moreover, seen computationalcost procedure involving ABox exponential, circumscription case,languages analogous ALC, complexity instance problem co-NExpNP (Bonattiet al., 2009, sect. 4.1.1). issues discussed addressed solvedBonatti et al. (2010, 2011a), circumscriptive systems specicallybuilt low-complexity DLs EL.hand, procedures based circumscription able derive defeasibleinformation individuals implicit ABox, is, can, exam presumably a:R.D. procedure involvingple, conclude a:R.C CABoxes able make kind derivation, since add defeasible informationindividuals named ABox. working renementprocedure order deal also implicit individuals; rst attempt takeconsideration also individuals proposed previous publications (Casini &Straccia, 2010, pp. 9-10), adding completion procedure ABox order explicitlyname ABox implicit individuals, procedure needs rened.Among proposals regarding introduction probabilistic reasoning DLs,Lukasiewicz (2008) presents combination nonmonotonic probabilistic reasoning.nonmonotonic part based preferential approach, presents construction augments inferential power rational closure. Let us consider proposal eliminating probabilistic part, i.e., considering conditionals associatedprobability 1 (conditionals (|)[1, 1] notation), since convey meaningdefeasible conditionals. procedure seems give back resultscases, two proposals dier general approach: proceedsrenement ranking structure, use renement content knowledge base. fact, behaviour two dier. Consider example knowledgeb, d, b c, c e, e} (in Lukasiewiczs notation wouldbase h, Di, = {acorrespond knowledge base empty TBox set P containing conditionals457fiCasini & Straccia(b|a)[1, 1], . . . ).14 construction derive a|c, since simply duct ha, b, cic, Lukasiewiczs approach cannot, since consider follows three preferred subset knowledge base consistentb, d, b c, c e}, {a b, d, b c, e}, {a b, d, c e, e}),(that {aa|c follow latter.7. Conclusionscombining classical rational closure ideas defeasible inheritance networks,proposed new rational consequence relation overcomes limitsformalisms. so, extended defeasible inference capabilities allowingatypical class still inherit properties superclass maintainingdesired logical properties rational closure. table summarizes structuralproperties satised systems taken consideration:15REFCTCMLERWRMHortyBINPLDLsee, proposals defeasible inheritance-based propositional logicDescription Logics still satisfy axioms classical rational closure. Another featuremethod uniquely require existence decision procedure classicalentailment and, thus, implemented top exiting propositional SAT solversDL reasoners. Since introduced procedure interesting also pointview inheritance nets, presented comprehensive procedure logicalknowledge bases making use nets formalism; notwithstanding, condentprocedure reformulated avoiding shift one formalism another.procedure presented ALC straightly extended languages expressive ALC; hand, present procedure needs languageclosed propositional connectives, hence need augment expressivitylanguage order apply less expressive DL languages EL, forcing wayincrease computational costs w.r.t. classical decision problem.Hence, looking adaptation procedure appropriate dealing tractable,less expressive DLs one main open problems present proposal, togetherproper semantic characterization procedure ability reasoningdefeasible way implicit individuals, discussed previous section.14. example corresponds structure Example.A.4 Appendix A, eliminating linkcenter figure.15. IN, BIN, PL, DL stand proposals INs, Boolean INs, propositional logic DLs, respectively.458fiDefeasible Inheritance-Based Description LogicsAppendix A. Examplesgoing validate decision procedures inheritance nets signicantexamples proposed Horty (1994) Sandewall (2010). shall use BINs, but, ordersimplify graphical representation, shall also use 6 macro, explainedSection 3.2. side eect, obtain analogue behaviour propositional DLcases too.Example A.1 (Horty, 1994, ex. 12 ). Consider net Figure 13. Horty claimsdesirable conclusion a|p, since environment mixed nets [. . . ] certainkinds compound defeasible paths legitimately thought carry immediate information - namely, paths consisting single defeasible link followed strict endsegment, length (Horty, 1994, p. 143). corresponds condition (RW ),that, seen Section 3.1.2, procedure satisfies.rpFigure 13: Example A.1Indeed, translate net KB K = h, i, = {a s, t, r p}= {s r, p}. a,p = , a,p |= s, a,p |= a.implication a,p ranking grater 0 r, and, since r p,a|K p.Example A.2 (Horty, 1994, ex. 18 ). Consider net Figure 14. example Hortyclaims prefer negative path ha, p, si positive path ha, r, si,since negative link 6 q nullifies path p r. procedure satisfiesclaim.pqrFigure 14: Example A.2Indeed, translate net KB K = h, i, = = {a p,r, p q, q r, r s, q, p s}. a,s = , a,s p, a,p a.459fiCasini & StracciaTherefore, E1 = {a p, r, q, p q, p s}. Now, E1 a,node rank 2, E2 = {a p, r, q}. conclude neither a|sa|s.Example A.3 (Sandewall, 2010, ex. B.1, Double Diamond). Consider net N = {S, U }defined (see Figure 15)= {s s, r r}U= {a t, p, s, p s, r, p q, q r}rrqpFigure 15: Double diamond.net N translated knowledge base K = {, } = {} = {at, p, s, p s, r, p q, q r}.Using method, net derive neither a|r, a|r, be.However, derive a|q, desirable result derivable Sandewallsapproach.Example A.4 (Sandewall, 2010, ex. B.2, Simonets Scenario). Consider net N = {S, U }defined (Figure 16)= {e e}U = {a b, d, b c, c e, c, e} .cebeFigure 16: Simonets scenario.460fiDefeasible Inheritance-Based Description Logicsnet N translated knowledge base K = {, } = {} = {ab, d, b c, c e, c, e}.Using method, net derive a|e, be, Sandewall cannotderive it. Moreover, derives neither a|c b|e.Example A.5 (Sandewall, 2010, ex. B.3, On-Path vs. O-Path Preemption). Considernet N = {S, U } defined (see Figure 17)= {wa 6 ga}U= {c re, c ce, e, ce e, e ga, wa} .ececgawaFigure 17: On-Path vs O-Path.net N translated knowledge base K = {, } = {wa ga}= {c re, c ce, e, ce e, e ga, wa}.connection c wa valid use form off-path preemption,on-path preemptions derivable. setting, derive c|wa: even cannotconsider method form preemption (at least explicitly), said Section 3.1.5method gives back results analogous off-path preemption.following examples use also conjunctions, going use BINs.Example A.6 (Sandewall, 2010, ex. B.4, Juvenile Oender). original Juvenile offender example represented following net (see Figure 18) N = {S, U },= {b g, b m, p p, m, g g}U= {g p, p} ,p read punished, g guilty, minor, b Billy. wantexpress priority g, b N w. Since netspossible express conjunctions, solve problem adding link g pm, g g.net translated knowledge base K = {, } = {b g, b m}= {g p, p, g p}. knowledge base derive b|p, expected.Example A.7 (Sandewall, 2010, ex. B.5, Campus Residence Scenario). example A.6,Campus residence example necessity include priorities. solve461fiCasini & StracciamgbpgpFigure 18: Juvenile oender.problem inserting conjunctions. Indeed, net N = {S, U } composed (seeFigure 19)= {t ma, e, m, w 6 n, ma, e e}U = {ma e w, n} ,read married, e employed, w lives west apartments,male, n lives northern apartments, Tom. want express epriority m, w results valid. so, add links mmae w,m, e e.eweenFigure 19: Campus residence scenario.net translated knowledge base K = {, } = {t ma, e,m, w n} = {ma e w, n, e w}. knowledge basederive t|w, expected.treat last example propositional problem. simpler netgraphical point view put negation nodes uselessresolution, use three-place conjunction link, macro constructionconjunction three nodes.462fiDefeasible Inheritance-Based Description LogicsExample A.8 (Sandewall, 2010, ex. B.6, Good Math Student Scenario). knowledgebase K = {T , D} composed (see Figure 20)= {t gs, f m, mb, aa ag}= {mb|ag, f ag|mm, mm mb gs|aa, f m|pm} ,gsmbmm mb gsagaaf agmmfmpmFigure 20: Good math student.Sandewall identifies following candidates valid conclusions:t|ag t|aa t|f agt|mm t|mm mb t|mm mb gst|pm .cannot conclude conditionals above. point net highlyinterconnected, especially strict links, nodes ranking 0, exceptmm mb gs ranking 1. Therefore, conclusions above,premise, cannot obtained.Appendix B. ProofsProposition 3.1. Consider net N = hS, translate set propositionalimplications . following properties hold:1. N consistent net, valid strict positive (resp., negative) path hp, , qip q, Np q (resp., Np 6 q), iff p q (resp., p q).2. N inconsistent iff p p .463fiCasini & Straccia3. Deciding strict consequence done polynomial time.Proof. First, easy prove induction length paths that,positive (resp., negative) path p q, p q (resp., p q). Hencehave:1a) N consistent net valid strict positive (negative) path (p, , q)p q, p q ( p q).Moreover, N inconsistent, conclude, p, q it, p qp 6 q, p q p q, is, p. one halfsecond statement.2a) 6 p every p , N consistent.move show halves statements.1b) N consistent net p q (resp., p q), valid strictpositive (resp., negative) path (p, , q) p q.order model classical consequence relation , use propositional resolution method. use symbol R indicate inferences resolutionmethod.Every element forms p q p q. implications correspond,clause form, respectively clauses (i.e., sets literals) {p, q} {p, q}.Call set clauses corresponding . example, assumeR {p, q} (that is, p q), set clauses , {p}, {q} resolvesempty set (that is, , (p q) ).1. Positive case (Np q): Assume p q p, q PN . orderapply refutational approach, p q negated, and, since (p q)equivalent p q, introduce clauses {p}, {q}. So, resolutionapproach p q translated , {p}, {q} R . also assumedconsistent, hence 6R . Since set composedpairs literals (of form {p, q} {p, q}), every reduction stepgives back pair literals (for example, {p, q} {q, r},obtain {p, r}). Therefore, order obtain empty set need use{p} {q} refutation procedure. So, clause {q} musteliminated using clause containing q. clause must necessarilyform {ri , q} ri , obtain new clause {ri }. Again, {ri }eliminated clause form {rj , ri }, obtaining clause {rj }. Sinceset nite, procedure terminate, doneobtain clause {p}, resolved {p}. is,clause {p, rl }, rl , s.t. {rl } appears reduction procedure.Now, clauses used reduction process correspond chainlinks net S: p rl , . . . , ri q, dene valid path (p, , q) S.2. Negative case (Np 6 q): assume p q p, q PS . p qnegated translated clauses {p}, {q}. {q} combined464fiDefeasible Inheritance-Based Description Logicsclause form {q, ri } (that represents q ri , that, turn, representslink q 6 ri ) (case 1), clause form {q, ri } (i.e., q ri , that,turn, q ri ) (case 2).Case 1. obtain clause {ri }, procedure positivecase. clauses used reduction process correspond strict negative path form: p rl , . . . , rj ri , ri 6 q.Case 2. obtain clause {ri }, would combine clauseform {ri , rj }, form {ri , rj }. former case move case1, ending strict negative path p rh , . . . , rl rj , rj 6 ri , ri q.latter case new clause {rj }, case2; however, procedure terminate, and, order terminate,reduction procedure point move case 1.clauses used reduction procedure correspond path form pri , . . . , rj rk , rk 6 rl , rl rm , . . . , rn q, link 6 correspondsshift case 1.2b) Automatically, also N consistent, 6 p every p .Otherwise, would p q p q nodes p q,would imply, procedure above, inconsistency N .3) third point, consider strict links encoded 2-CNF formulae,also called Krom formulae, propositional 2-SAT problem P .Proposition 3.2. Consider net N . every connection p|N q (resp., p|N q) validatedprocedure, corresponding positive (resp., negative) potential path pq net N .b p,q . procedure states p| qb p,q p q (andProof. Dene setNb p,q p q). Following procedure analogous one proofp|N qproposition 3.1, show derivation implications connectedpresence positive (negative) potential paths net.Proposition 3.3.satisfies (REF ), (CT ) (CM ).prove proposition need rst prove following lemma.p q ( {, 6}). Call setLemma B.1. Consider net N = hS, U s.t. Nmaterial implications corresponding links S, consequence relationobtained adding formulae extra-axioms. Consider net N obtainedNadding link p q N . Then, every pair nodes r, N ,r,sNr,s equivalent w.r.t. .Proof. two nets contain nodes. Given two nodes r s, two possibleN , C N = C N , consequentlyN p q C N . p q/ Cr,scases: p q/ Cr,sr,sr,sr,sN . p q C N , C N C N , consequently C N C N ,N =r,sp,qr,sr,sr,sr,sr,s465fiCasini & StracciaN C N one hand C N C Ncorrespondent sets courses two nets (Cp,qp,qr,sr,sother) contain exactly nodes links, apart, possibly, p q.NNNcorresponds saying Np,q p,q one hand, r,s r,scontain exactly implications, apart, possibly, p lq (where lq {q, q}),exactly set antecedents.p q,NNNSince Np,q p lq , consequently p,q p lq . So, p,qNNNNp,q -equivalent, also r,s r,s -equivalent. So, since r,sNr,s -equivalent contain set antecedents, generateN equivalent w.r.t. .Nranking, is,r,sr,sprove Proposition 3.3.Proof. satisfaction (REF ) trivial, since every direct link trivially valid.(CT ): assume N p q N , p q r s. call N net obtained adding p qNN . N r meansr,s r ls (ls {s, s}). lemma B.1N r ls , is, N r s.N equivalent w.r.t. , alsoNr,sr,sr,s(CM ): assume Np q Nr s. call N net obtained adding p qN r ls . lemma B.1NNN . N r meansr,sr,sr,sr s.Nequivalent w.r.t. , alsorl,is,N,pqr,sProposition 3.5.satisfies (LE), (RW ), (Sup).Proof. proofs immediate, considering procedure fact implications corresponding strict links always present decision procedure.Proposition 3.6 net N consistent iff node p r(p) = , is,conclude N p q N p 6 q pair p, q.proved combining following two lemmas.Lemma B.2. Given net N , two nodes p q it, conclude p|N qp|N q iff r(p) = .Proof. left right. immediate denition ranking procedure:r(p) 6= , set implications least ranking p imply p,cannot imply p q p q q.right left. r(p) = , set E 16 implications (with pantecedents) E p. implies E p q E p qwhichever q, is, p|N q p 6 |N q.Lemma B.3. Every node ranking set courses C ranking valueevery set courses extending C.Proof. immediate monotony consequence relations N .16. E set -ranked defaults.466fiDefeasible Inheritance-Based Description Logicslemmas conclude node p innite ranking net, netinconsistent (p|N q p|N q p q), and, conversely, net inconsistent,node p innite ranking.Proposition 3.9BINsatisfies (REF ), (CM ), (CT ).Proof. proof retraces one proposition 3.3.Proposition 3.10BINsatisfies (LE), (RW ), (Sup).Proof. proof retraces one proposition 3.5.Proposition 3.11BINsatisfies (OR) (AN D).Proof. (OR): N BIN p N BIN q t, p, q S. Assume=. case =6 analogous.p,t pq,t q t.N BIN p N BIN q t, is,Since N BIN p N BIN q t, proposition 3.2 least ductp one q t. So, since p, q s, duct t. Moreover,since least duct p q s, every duct connectingallows duct connecting p q t. Hence, s,t p,t s,t q,tholds.hand, that, since p, q s, connected connections moving p q. So, p,t s,t q,t s,t . Hence,p,t = q,t = s,t holds ranking functions rs,t , rp,t , rq,twork sets material implications.propositional level, (p q) either rs,t (p) rs,t (q)rs,t (q) rs,t (p). Assume former (the reasoning applies case).q,tp,t . Moreover, least set exceptionality orderrs,t (p) rs,t (q)negating p negate also q, least set exceptionality order negatingp,t =s,t . So,s,t p t, and, sinceq,tp,t ,t, is, rs,t (t) = rs,t (p),also s,t q t; hence classical reasoning obtain s,t (p q) t,s,t t. Eventually, N BIN t.i.e.,(AN D): N BIN p q N BIN p s, s, q S. Everyduct connecting p q p part duct connecting p addictions, q t, every duct connecting p part duct connecting p q (resp.,p s) addiction q (resp., s). So, p,q = p,s = p,t , obtainN BIN p t.Proposition 3.12 BIN N consistent iff node p r(p) = ,is, cannot conclude p q p 6 q pair p, q.Proof. proof similar one proposition 3.6.Proposition 4.1. |K rational consequence relation containing K.Proof. |K obtained rational closure K , rational consequencerelation. Moreover, R(K ) contains K , and, since K K , contains K too.467fiCasini & StracciaProposition 4.2. Given conditional base K, |K iff | R(K).Proof. sucient prove that, given conditional base K = hT , U i, obtainedextension K = hT , U (Steps 1-3), K preferentially consistent conditional base Kpreferentially consistent too.right left proof immediate, since K K . left right, K6 . Given every conditional added K correspondsconsistent meansimplication classically derivable subset D,C(T D) (where C classical closure operation associated ).C(T ) = C(T D). Then,/ C(K ), K consistent knowledge base too.Proposition 5.1 |K rational consequence relation containing K.Proof. sucient show (1) every inclusion axiom valid |K ,(2) |K satises properties characterizing rational consequence relation.(1) construction, C , C Te (that is, modulo logicalequivalence, Te ). Assume C|D D. either r(C) = r(C|D) = ,r(C) = r(C|D)= i, < . rst case C Te ,e D, implies C| D. second case,{C}KC D, default-assumption associated premise C.e {i } D, is, C| D.Hence {C}K(2) consequence relation |K satises properties rational consequence relation.(REF ) obviously satised, (LLE) is. (RW ), assume C|K D.e {i } D, that, givenmeans rst C-consistent , {C}ee E, implies {C} {i } E, i.e., C|K E.e E, rst C(CT ) C D|K E corresponds {C D}D-consistent formula . Analogously, C|K means rst Ce {j } D. Since C C,consistent j , {C}j i, is, j . Hence, {C D} {j } Ee {j } D, and, since satises (CT ), {C}e {j } E,{C}is, C|K E.e {i } E.(CM ) C|K E means rst C-consistent , {C}e {i } . Hence,Analogously, C|K means {C}econsistent CD, and, monotony , {CD}{i}E, is, C D|K E.e {i } E.(OR) C|K E means rst C-consistent , {C}eAnalogously, D|K E means {D} {j } E rst D-consistentj . three options: j = i, j < < j. rst case,default-assumption associated C , and, since satises OR,{C D} {i } E, is, C D|K E. j < i, j ,ej rst CD-consistent default . Hence {D}{j}eE, and, monotonicity, {C} {j } E. Since satises OR,e {j } E, is, C D| E.{C D}K468fiDefeasible Inheritance-Based Description Logicse(RM ) C 6 |K E corresponds 6 {C} {} E, rst C-consistente {i },formula . means E consistent {C}erst C E-consistent formula . Since {C} {i } D,e {i } D, is, C E| D.monotonicity {C E}KProposition 5.3 Given linear order individuals K, procedure determinesABox extension K. Vice-versa, every ABox extension K corresponds knowledgebase generated linear order individuals O.Proof. rst statement quite immediate. second statement, assumerational extension hA , hA, , Di cannot generated sequenceelements O. associates every individual x default concept ,indicate x .Now, consider generic rational extension , hA, , Di generatedusing sequence elements O. following procedure allows dene sequenceelements s.t. , generated using s, i.e., , = hAsD , i.Take element associate strongest default concept consistentknowledge base hA, (call x ). Look individual x s.t. x = x ,consider x rst element sequence s. Update x:x , repeat procedure, every individual associated default formula. proceduregenerate sequence dominion individuals generates ,hA, , Di.Since sequence generate hA , i, procedure fail,is, point possible associate x default x s.t. x = x .means that, remaining x, x 6= x ; x, either x xx x . rst case possible, since hA , would inconsistent ( xmaximally consistent default). Hence x x x 6= x remaining x.case, hA , would rational extension hA, , Di, since could anotherconsistent model stronger defaults associated individuals.Proposition 5.4 Given K linear order individuals K, consequencerelation satisfies properties (REFDL ) (RMDL ).Proof. REFDL , LLEDL RWDL proof quite immediate. CTDLCMDL , assume hA, , Di b:D, hAsD , b:D. Hence, b:D consistenthAsD , i; implies rst individual sequence s, let a,every , a:i consistent hA, consistent hA {b:D}, i,procedure associates default formula either start A{b:D}.happens following individuals sequence s.hAsD {b:D}, = h(A {b:D})sD , hAsD {b:D}, a:C h(A {b:D})sD ,a:C. Since satises CT CM , hAsD , a:C h(A{b:D})sD , a:C,is, hA, , Di a:C hA {b:D}, , Di a:C.ORDL , assume hA {b:D}, , Di a:C, hA {b:E}, , Di a:C,b nth position sequence s. So, rst n 1 elements associationdefault-formulae models. b, assume procedure469fiCasini & Stracciaassigns b:i case b:D, b:j case b:E. either = j , j ,j . rst case procedure assignment defaults continuesway knowledge bases, also b:D E,is, hA {b:D}, , Di, hA {b:E}, , Di, hA {b:D E}, , Di completed exactlydefaults, obtaining, respectively, ABoxes (A {b:D})sD = {b:D},(A {b:E})sD , = {b:E}, (A {b:D E})sD = {b:D E}, ABox. {b:D} a:C {b:E} a:C, and, since satises OR,obtain {b:D E} a:C, is, h(A {b:D E})sD , a:C. jb:D E, procedure associates b strongest two defaults, is, . Sinceconsistent b:E, every following consistency check procedureforced consider b:D holds, assignment defaults individualsproceed case b:D holds, hA {b:D E}, , Di entailformulae hA {b:D}, , Di. Analogously, j , default-assumption extensionhA {b:D E}, , Di correspond one hA {b:E}, , Di.Finally, RMDL , b:D consistent hAsD , i, presence b:Dknowledge base inuence association defaults individuals,AsD (A {b:D})sD . Eventually, hAsD , a:C implies h(A {b:D})sD , a:C, i.e.hA {b:D}, , Di a:C.Proposition 5.7 hA, , Di a:C iff hA[a] , , Di a:C every ordering individuals A[a].Proof. proof quite immediate. Assume hA[a] , , Di 6 a:C s. Letsequence individuals named obtained using initial segment. HencehA, , Di 6 a:C, implies hA, , Di 6 a:C.assume hA, , Di 6 a:C. Hence, sequence hA, , Di6 a:C. Letrestriction individuals named A[a] ; hA, , Di6 a:C.Appendix C. Table Main SymbolsSince paper considers dierent elds, notation turned quite complex.add table summarize main symbols used paper.NCp,qIN/BINnodesstrict conditionaldefeasible conditionalconsequence relationconjunction, disjunctionlinks courses/ductsp qPLatomspropositionsstrict conditionaldefeasible conditionallinks ductsp qDLconcept namesconceptsindividualsstrict conditionaldefeasible conditionallinks ductsp qNp,qmaterialisationsNlinks Cp,qmaterialisationsNlinks Cp,qmaterialisationsNlinks Cp,qNp,qconditionals Np,qexceptional pconditionals Np,qexceptional pconditionals Np,qexceptional pp, q, . . .C, D, . . .a, b, . . .|,470fiDefeasible Inheritance-Based Description LogicsReferencesAlchourron, C., Gardenfors, P., & Makinson, D. (1985). logic theory change:Partial meet contraction revision functions. Journal Symbolic Logic, 50, pp.510530.Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).(2003). Description Logic Handbook: Theory, Implementation, Applications.Cambridge University Press.Baader, F., & Hollunder, B. (1993). prefer specic defaults terminologicaldefault logic. Proceedings IJCAI-93, pp. 669674. Morgan Kaufmann Publishers.Bochman, A. (2001). logical theory nonmonotonic inference belief change. SpringerVerlag.Bonatti, P. A., Lutz, C., & Wolter, F. (2009). complexity circumscription description logic. Journal Artificial Intelligence Research, 35, pp. 717773.Bonatti, P. A., Faella, M., & Sauro, L. (2010). EL default attributes overriding.Patel-Schneider, P. F., Pan, Y., Hitzler, P., Mika, P., Zhang, L., Pan, J. Z., Horrocks,I., & Glimm, B. (Eds.), International Semantic Web Conference (1), Vol. 6496Lecture Notes Computer Science, pp. 6479. Springer.Bonatti, P. A., Faella, M., & Sauro, L. (2011a). Adding default attributes EL++ .Burgard, W., & Roth, D. (Eds.), Proceedings AAAI-11. AAAI Press.Bonatti, P. A., Faella, M., & Sauro, L. (2011b). Defeasible inclusions low-complexityDLs. Journal Artificial Intelligence Research, 42, pp. 719764.Bonatti, P. A., Faella, M., & Sauro, L. (2011c). complexity el defeasibleinclusions. Proceedings IJCAI-11, pp. 762767. AAAI Press/IJCAI.Brewka, G., & Augustin, D. S. (1987). logic inheritance frame systems.Proceedings IJCAI-87, pp. 483488. Morgan Kaufmann Publishers.Britz, K., Casini, G., Meyer, T., Moodley, K., & Varzinczak, I. (2013). Ordered Interpretations Entailment Defeasible Description Logics. Tech. rep., CAIR, CSIRMeraka UKZN, South Africa.Britz, K., Heidema, J., & Meyer, T. A. (2008). Semantic preferential subsumption.Brewka, G., & Lang, J. (Eds.), Proceedings KR-08, pp. 476484. AAAI Press.Britz, K., Meyer, T., & Varzinczak, I. J. (2011). Semantic foundation preferentialdescription logics. Wang, D., & Reynolds, M. (Eds.), Australasian ConferenceArtificial Intelligence, Vol. 7106 Lecture Notes Computer Science, pp. 491500.Springer.Casini, G., Meyer, T., Moodley, K., & Varzinczak, I. (2013). Nonmonotonic reasoningdescription logics. Rational closure ABox. Proceedings DL-13, pp. 7790.CEUR Workshop Proceedings.Casini, G., & Straccia, U. (2010). Rational closure defeasible description logics.Janhunen, T., & Niemela, I. (Eds.), Proceedings JELIA-10, Vol. 6341 LectureNotes Computer Science, pp. 7790. Springer.471fiCasini & StracciaCasini, G., & Straccia, U. (2011). Defeasible inheritance-based description logics. Proceedings IJCAI-11, pp. 813818.Cormen, T. H., Stein, C., Rivest, R. L., & Leiserson, C. E. (2001). Introduction Algorithms(2nd edition). McGraw-Hill Higher Education.Donini, F. M., & Massacci, F. (2000). EXPTIME tableaux ALC. Artificial Intelligence,124 (1), pp. 87138.Donini, F. M., Nardi, D., & Rosati, R. (2002). Description logics minimal knowledgenegation failure. Transactions Computational Logic, 3 (2), pp. 177225.Freund, M. (1998). Preferential reasoning perspective Poole default logic. ArtificialIntelligence, 98 (1-2), pp. 209235.Gabbay, D. M., & Schlechta, K. (2009). Defeasible inheritance systems reactive diagrams. Logic Journal IGPL, 17 (1), pp. 154.Giordano, L., Gliozzi, V., Olivetti, N., & Pozzato, G. L. (2012a). minimal model semanticsnonmonotonic reasoning. Proceedings JELIA-12, Vol. 7519 Lecture NotesComputer Science, pp. 228241. Springer.Giordano, L., Gliozzi, V., Olivetti, N., & Pozzato, G. L. (2012b). non-monotonic description logic reasoning typicality. Artificial Intelligence, 195, pp. 165202.Giordano, L., Olivetti, N., Gliozzi, V., & Pozzato, G. L. (2009). ALC+T: preferentialextension description logics. Fundam. Inform., 96 (3), 341372.Grimm, S., & Hitzler, P. (2009). preferential tableaux calculus circumscriptive ALCO.Proceedings RR-09, pp. 4054. Springer-Verlag.Horty, J. F. (1994). direct theories nonmonotonic inheritance. Handbooklogic artificial intelligence logic programming: nonmonotonic reasoninguncertain reasoning, Vol. 3, pp. 111187. Oxford University Press.Horty, J. F., & Thomason, R. H. (1990). Boolean extensions inheritance networks.Proceedings AAAI-90, pp. 633639. AAAI Press.Horty, J. F., Thomason, R. H., & Touretzky, D. S. (1987). skeptical theory inheritancenonmonotonic semantic networks. Proceedings AAAI-87. AAAI Press.Knorr, M., Alferes, J. J., & Hitzler, P. (2011). Local closed world reasoning descriptionlogics well-founded semantics. Artificial Intelligence, 175 (9-10), 15281554.Kraus, S., Lehmann, D., & Magidor, M. (1990). Nonmonotonic reasoning, preferentialmodels cumulative logics. Artificial Intelligence, 44 (1-2), pp. 167207.Lehmann, D., & Magidor, M. (1992). conditional knowledge base entail?.Artificial Intelligence, 55 (1), pp. 160.Lukasiewicz, T. (2008). Expressive probabilistic description logics. Artificial Intelligence,172 (6-7), pp. 852883.Makinson, D. (1994). General patterns nonmonotonic reasoning. Handbook logicartificial intelligence logic programming: nonmonotonic reasoning uncertainreasoning, Vol. 3, pp. 35110. Oxford University Press.472fiDefeasible Inheritance-Based Description LogicsMakinson, D. (2005). Bridges Classical Nonmonotonic Logic. Kings College Publications.Makinson, D., & Schlechta, K. (1991). Floating conclusions zombie paths. ArtificialIntelligence, 48, pp. 199209.Poole, D. (1988). logical framework default reasoning. Artificial Intelligence, 36 (1),2747.Quantz, J., & Royer, V. (1992). preference semantics defaults terminological logics.Proceedings KR-92, pp. 294305.Rott, H. (2001). Change, Choice Inference: study belief revision nonmonotonicreasoning. Oxford University Press.Sandewall, E. (1986). Nonmonotonic inference rules multiple inheritance exceptions.Proceedings IEEE-86, pp. 13451353.Sandewall, E. (2010). Defeasible inheritance doubt index axiomatic characterization. Artificial Intelligence, 18 (174), pp. 14311459.Schlechta, K. (2004). Coherent Systems. Elsevier.Shoham, Y. (1988). Reasoning change: time causation standpointartificial intelligence. MIT Press.Simonet, G. (1996). sandewalls paper: Nonmonotonic inference rules multiple inheritance exceptions. Artificial Intelligence, 86, pp. 359374.Straccia, U. (1993). Default inheritance reasoning hybrid KL-ONE style logics. Proceedings IJCAI-93, 676681.Thomason, R. H. (1992). NETL subsequent path-based inheritance theories.Lehmann, F. (Ed.), Semantic Networks Artificial Intelligence, pp. 179204. Pergamon Press.Touretzky, D. S. (1986). mathematics inheritance systems. Pitman.Touretzky, D. S., Horty, J. F., & Thomason, R. H. (1987). clash intuitions: currentstate nonmonotonic multiple inheritance systems. Proceedings IJCAI-87 Vol. 1, pp. 476482. Morgan Kaufmann Publishers.Touretzky, D. S., Thomason, R. H., & Horty, J. F. (1991). skeptics menagerie: conictors,preemptors, reinstates, zombies nonmonotonic inheritance. ProceedingsIJCAI-91, pp. 478483. Morgan Kaufmann Publishers.473fiJournal Artificial Intelligence Research 48 (2013) 253-303Submitted 11/12; published 10/13Optimizing SPARQL Query Answering OWL OntologiesIlianna Kolliailianna2@mail.ntua.grUniversity Ulm, GermanyNational Technical University Athens, GreeceBirte Glimmbirte.glimm@uni-ulm.deUniversity Ulm, GermanyAbstractSPARQL query language currently extended World Wide WebConsortium (W3C) so-called entailment regimes. entailment regime definesqueries evaluated expressive semantics SPARQLs standard simpleentailment, based subgraph matching. queries expressive sincevariables occur within complex concepts also bind concept role names.paper, describe sound complete algorithm OWL Direct Semantics entailment regime. propose several novel optimizations strategiesdetermining good query execution order, query rewriting techniques, showspecialized OWL reasoning tasks concept role hierarchy used reducequery execution time. determining good execution order, propose cost-basedmodel, costs based information instances concepts rolesextracted model abstraction built OWL reasoner. present twoordering strategies: static dynamic one. dynamic case, improveperformance exploiting individual clustering approach allows computingcost functions based one individual sample cluster.provide prototypical implementation evaluate efficiency proposedoptimizations. experimental study shows static ordering usually outperformsdynamic one accurate statistics available. changes, however,statistics less accurate, e.g., due nondeterministic reasoning decisions. queriesgo beyond conjunctive instance queries observe improvement threeorders magnitude due proposed optimizations.1. IntroductionQuery answering important context Semantic Web since provides mechanism via users applications interact ontologies data. Several querylanguages designed purpose, including RDQL (Seaborne, 2004), SeRQL(Broekstra & Kampman, 2006) and, recently, SPARQL. paper, considerSPARQL query language (Prudhommeaux & Seaborne, 2008), standardized2008 World Wide Web Consortium (W3C) currently extendedSPARQL 1.1 (Harris & Seaborne, 2013). Since 2008, SPARQL developed mainquery language Semantic Web supported RDF triple stores.query evaluation mechanism defined SPARQL Query specification basedsubgraph matching. form query evaluation also called simple entailment sinceequally defined terms simple entailment relation RDF graphs(Hayes, 2004). SPARQL 1.1 includes several entailment regimes (Glimm & Ogbuji, 2013)c2013AI Access Foundation. rights reserved.fiKollia & Glimmorder use elaborate entailment relations, induced RDF Schema(RDFS) (Brickley & Guha, 2004) OWL (Motik, Patel-Schneider, & Cuenca Grau, 2012b;Schneider, 2012). Query answering entailment regimes complexmay involve retrieving answers follow implicitly queried graph,seen OWL ontology using OWL entailment. several implementationsSPARQLs RDFS entailment regime available (e.g., Oracle 11g (Oracle, 2013), ApacheJena (The Apache Software Foundation, 2013), Stardog (Clark & Parsia, 2013b)),development tools provide full SPARQL support OWL semantics stillongoing effort.Since consider OWL Direct Semantics entailment regime SPARQL 1.1paper, talk SPARQL queries evaluation SPARQL queries,always assume OWL Direct Semantics entailment regime used. setting,clause query seen set extended OWL axioms (an extendedOWL ontology), variables place concept, role individual names.query answers contain instantiation variables leads OWL axiomsentailed queried ontology. Thus, naive query evaluation procedurerealized OWLs standard reasoning task entailment checking.Please note two types individual variables SPARQL; standard (distinguished) variables anonymous individuals (aka blank nodes). anonymous individuals treated like distinguished variables difference cannotselected and, hence, bindings cannot appear query answer. contrastconjunctive queries, anonymous individuals treated existential variables.hand, anonymous individuals occur query answer bindings distinguished variables, i.e., SPARQL treats anonymous individuals queried ontologyconstants. treatment anonymous individuals chosen compatibilitySPARQLs standard subgraph matching semantics. example, order implementRDF(S) entailment regime, systems simply extend queried graph inferredinformation (materialization) use SPARQLs standard evaluation mechanismmaterialized graph order compute query results. Similarly, usersmove systems support OWL RL profile (Motik, Cuenca Grau, Horrocks, Wu,Fokoue, & Lutz, 2012a), OWL RL rule set OWL 2 specification usedcompute query answers (again via materialization). one change semanticsblank nodes SPARQLs entailment regimes reflect conjunctive query semantics, onecould longer use materialization plus standard SPARQL query processor implemententailment regime. one change semantics blank nodesOWL Direct Semantics entailment regime, materialization cannot used implement regime, users would simply get answers moving systemssupport RDF(S) systems support OWLs Direct Semantics, could also happenget less answers using expressive logic, counter-intuitive.last decade, much effort spent optimizing standard reasoning tasksentailment checking, classification, realization (i.e., computation instancesconcepts roles) (Sirin, Cuenca Grau, & Parsia, 2006; Tsarkov, Horrocks, & PatelSchneider, 2007; Glimm, Horrocks, Motik, Shearer, & Stoilos, 2012). optimizationquery answering algorithms has, however, mostly addressed conjunctive queriesOWL profiles, notably OWL 2 QL profile (Calvanese, Giacomo, Lembo, Lenzerini,254fiOptimizing SPARQL Query Answering OWL Ontologies& Rosati, 2007; Kontchakov, Lutz, Toman, Wolter, & Zakharyaschev, 2010; Perez-Urbina,Motik, & Horrocks, 2010; Rodriguez-Muro & Calvanese, 2012). exceptionworks nRQL SPARQL-DL. query language nRQL supported RacerPro (Haarslev, Moller, & Wessel, 2004) SPARQL-DL implemented Pelletreasoner (Sirin, Parsia, Grau, Kalyanpur, & Katz, 2007). discuss greater detailSection 8.paper, address problem efficient SPARQL query evaluation OWL 2DL ontologies proposing range novel optimizations deal particularexpressive features SPARQL variables place concepts roles.adapt common techniques databases cost-based query planning. costscost model based information instances concepts rolesextracted model abstraction built OWL reasoner. presentstatic dynamic algorithm finding optimal near optimal execution orderdynamic case, improve performance exploiting individual clusteringapproach allows computing cost functions based one individual samplecluster. propose query rewriting techniques show specialized OWLreasoning tasks concept role hierarchy used reduce query executiontime. provide prototypical implementation evaluate efficiency proposedoptimizations. experimental study shows static ordering usually outperformsdynamic one accurate statistics available. changes, however,statistics less accurate, e.g., due non-deterministic reasoning decisions. queriesgo beyond conjunctive SPARQL instance queries, observe improvementthree orders magnitude due proposed optimizations.Note paper combines extends two conference papers: I. Kollia B.Glimm: Cost based Query Ordering OWL Ontologies. Proceedings 11th International Semantic Web Conference, 2012 I. Kollia, B. Glimm I. Horrocks: SPARQLQuery Answering OWL Ontologies. Proceedings 8th Extended Semantic WebConference, 2011. current paper have, additionally first mentionedpaper, defined cost functions general SPARQL queries (i.e., conjunctive instance queries) added experimental results expressive queries. comparisonsecond mentioned papers, defined notion concept rolepolarity presented theorems let us prune search space possible mappingsaxiom templates based polarity together algorithm shows wayuse optimization. Moreover, experimental results added complexqueries make use optimization.remainder paper organized follows: next present preliminaries,present general query evaluation algorithm Section 3 serves basisoptimization. Section 4, present foundations cost model,specify Section 5. Section 6, present optimizations complex queriescannot directly mapped specialized reasoner tasks. Finally, evaluate approachSection 7 discuss related work Section 8 conclude Section 9.255fiKollia & Glimm2. Preliminariessection, first give brief introduction Description Logics since OWLDirect Semantics based Description Logic SROIQ (Horrocks, Kutz, & Sattler,2006). optimizations present need features SROIQ. Hence,present SHOIQ, allows shorter easier follow presentation.introducing SHOIQ, clarify relationship RDF, SPARQLOWL, present SPARQLs OWL Direct Semantics entailment regime giveoverview model building tableau hypertableau calculi.2.1 Description Logic SHOIQfirst define syntax semantics roles, go SHOIQ-concepts,individuals, ontologies/knowledge bases.Definition 1 (Syntax SHOIQ ). Let NC , NR , NI countable, infinite,pairwise disjoint sets concept names, role names, individual names, respectively.call = (NC , NR , NI ) signature. set rol(S) SHOIQ-roles (or rolesshort) NR {r | r NR } {r , r }, roles form r called inverseroles, r top role (analogous owl:topObjectProperty), r bottom role(analogous owl:bottomObjectProperty). role inclusion axiom form rr, roles. transitivity axiom form trans(r) r role. role hierarchy Hfinite set role inclusion transitivity axioms.role hierarchy H, define function inv roles inv(r) := r r NRinv(r) := r = role name NR . Further, define H smallesttransitive reflexive relation roles r H implies r H inv(r) Hinv(s). write r H r H H r. role r transitive w.r.t. H (notationr + H r) role exists r H s, H r, trans(s) H trans(inv(s)) H.role called simple w.r.t. H role r r transitive w.r.t. Hr H s.Given signature = (NC , NR , NI ) role hierarchy H, set SHOIQconcepts (or concepts short) smallest set built inductively symbolsusing following grammar, NI , NC , n IN0 , simple rolew.r.t. H, r role w.r.t. H:C ::= | | {o} | | C | C C | C C | r.C | r.C | 6 n s.C | > n s.C.define semantics SHOIQ concepts:Definition 2 (Semantics SHOIQ-concepts). interpretation = (I , ) consistsnon-empty set , domain I, function , maps every concept nameNC subset AI , every role name r NR binary relation r ,every individual name NI element aI . top role r interpreted{h, | , } bottom role r . role name r NR ,interpretation inverse role (r ) consists pairs h,h , r .256fiOptimizing SPARQL Query Answering OWL Ontologiessemantics SHOIQ-concepts signature defined follows:(C)I(r.C)I(r.C)I(6 n s.C)I(> n s.C)I======\ C{{{{=({o})I = {oI }(C D) = C(C D)I = C| h, r , C }| h, r C }| (sI (, C)) n}| (sI (, C)) n}(M ) denotes cardinality set sI (, C) defined{ | h, sI C }.Definition 3 (Syntax Semantics Axioms Ontologies, Entailment).C, concepts, (general) concept inclusion axiom (GCI) expression C D.introduce C abbreviation C C. finite set GCIs calledTBox. (ABox) (concept role) assertion axiom expression form C(a),r(a, b), r(a, b), b, 6 b, C NC concept, r NR role, a, b NIindividual names. ABox finite set assertion axioms. ontologytriple (T , H, A) TBox, H role hierarchy, ABox. use NCO , NRO ,NIO denote, respectively, set concept, role, individual names occurringO.Let = (I , ) interpretation. satisfies role inclusion axiom rr sI , satisfies transitivity axiom trans(r) r transitive binary relation,role hierarchy H satisfies role inclusion transitivity axioms H.interpretation satisfies GCI C C ; satisfies TBox satisfiesGCI . interpretation satisfies assertion axiom C(a) aI C , r(a, b)haI , bI r , r(a, b) haI , bI/ r , b aI = bI , 6 b aI 6= bI ; satisfiesABox satisfies assertion A. say satisfies satisfies , H,A. case, say model write |= O. sayconsistent model.Given axiom , say entails (written |= ) every modelsatisfies .Description Logics extended concrete domains, correspondOWLs datatypes. case, one distinguishes abstract roles relate twoindividuals concrete roles relate individual data value. DescriptionLogic SROIQ allows number features role chains formhasFather hasBrother hasUncle, support special concept Self,used axioms form Narcissist loves.Self, defining roles reflexive,irreflexive, symmetric, asymmetric.Description Logic ontologies equally expressed terms OWL ontologies,turn mapped RDF graphs (Patel-Schneider & Motik, 2012). direction is, however, always possible, i.e., mapping RDF graphs OWL ontologiesdefined certain well-formed RDF graphs correspond OWL 2 DL ontology.257fiKollia & Glimm2.2 Relationship RDF, SPARQL, OWLSPARQL queries evaluated RDF graphs remain basic data structureeven adopting elaborate semantic interpretation.Definition 4 (RDF Graphs). RDF based set International ResourceIdentifiers (IRIs), set L RDF literals, set B blank nodes. setRDF terms L B. RDF graph set RDF triples form(subject, predicate, object) (I B) .generally abbreviate IRIs using prefixes rdf, rdfs, owl, xsd refer RDF,RDFS, OWL, XML Schema Datatypes namespaces, respectively. empty prefixused imaginary example namespace, completely omit Description Logicsyntax.example SPARQL querySELECT ?x <ontologyIRI> { ?x rdf:type :C . ?x :r ?y }clause SPARQL query consists basic graph pattern (BGP):RDF graph written Turtle syntax (Beckett, Berners-Lee, Prudhommeaux, & Carothers,2013), nodes edges replaced variables. basic graph patternprecisely defined follows:Definition 5 (Basic Graph Pattern). Let V countably infinite set query variablesdisjoint . triple pattern member set (T V ) (I V ) (T V ),basic graph pattern (BGP) set triple patterns.recall complete surface syntax SPARQL since partspecific evaluation SPARQL queries OWLs Direct Semanticsevaluation BGPs. complex clauses, use operatorsUNION alternative selection criteria OPTIONAL query optional bindings(Prudhommeaux & Seaborne, 2008), evaluated simply combining resultsobtained BGP evaluation. Similarly, operations projection variablesSELECT clause straightforward operation results evaluationclause. Therefore, focus BGP evaluation only. detailedintroduction SPARQL queries algebra refer interested readers workHitzler, Krotzsch, Rudolph (2009) Glimm Krotzsch (2010).Since Direct Semantics OWL defined terms OWL structural objects, i.e.,OWL axioms, map BGPs SPARQL queries structural objects,variables place class (concept), object data property (abstract concrete role),individual names literals. Since direct mapping OWL axiomsDescription Logic axioms, BGPs expressed Description Logic axiomsvariables occur place concept, role individual names. example, BGPprevious example mapped ClassAssertion(C ?x) ObjectPropertyAssertion(r?x ?y) functional-style syntax C(?x) r(?x, ?y) Description Logic syntax.details, refer interested readers W3C specification definesmapping OWL structural objects RDF graphs (Patel-Schneider & Motik,2012) specification OWL Direct Semantics entailment regime SPARQL258fiOptimizing SPARQL Query Answering OWL Ontologies(Glimm & Ogbuji, 2013) defines extension mapping BGPsOWL objects variables.2.3 SPARQL Queriesfollowing, directly write BGPs Description Logic notation extended allow variables place concept, role individual names axioms. worthreminding SPARQL support existentially quantified variables,contrast database-style conjunctive queries, one typically also existential/nondistinguished variables.brevity without loss generality, assume neither queryqueried ontology contains anonymous individuals. consider dataproperties literals, presented optimizations easily transferredcase.Definition 6 (Query). Let = (NC , NR , NI ) signature. query signature Sq w.r.t.six-tuple (NC , NR , NI , VC , VR , VI ), VC , VR , VI countable, infinite,pairwise disjoint sets concept variables, role variables, individual variablesdisjoint NC , NR , NI . concept term element NC VC . role termelement NR VR . individual term element NI VI . axiomtemplate Sq SROIQ axiom S, one also use concept variablesVC place concept names, role variables VR place role names, individualvariables VI place individual names. query q w.r.t. query signature Sqnon-empty set axiom templates Sq . use Vars(q) (Vars(at) axiom templateat) denote set variables q (at) |q| denote number axiom templatesq. Let t, individual terms; call axiom templates form A(t) NC ,r(t, ) r NR , query atoms. conjunctive instance query q w.r.t. querysignature Sq non-empty set query atoms.function , use dom() denote domain . Let ontologyq = {at1 , . . . , atn } query Sq consisting n axiom templates. mappingq total function : Vars(q) NCO NRO NIO1. (v) NCO v VC dom(),2. (v) NRO v VR dom(),3. (v) NIO v VI dom(),4. (q) SROIQ ontology.write (q) ((at)) denote result replacing variable v q (at)(v). setq compatible mappings q defined q := { |mapping q O}. mapping solution mapping certain answerq |= (q). denote set containing solution mappings qq . result size number answers query q givencardinality setq .Note last condition definition mappings required ensure decidability query entailment. example, without condition, reasoner might259fiKollia & Glimmtest instantiated axiom templates role variable replaced non-simplerole number restriction, allowed Description Logic axioms. Note alsoindicate variables selected since considerstraightforward task projection here.Examples queries according definition following (where ?xconcept variable, ?y role variable, ?z individual variable):C ?y.?x(r.?x)(?z)remainder, use signature (NC , NR , NI ), denote SROIQ ontologyS, A, B NC concept names O, r, NR role names O, a, b NIindividual names O, ?x, ?y variables, c1 , c2 concept terms, r1 , r2 roleterms, t, individual terms, q = {at1 , . . . , atn } query n axiom templatesquery signature Sq = (NC , NR , NI , VC , VR , VI ),q compatible mappingssolutionmappingsqO.q2.4 Model-building (Hyper)Tableau Calculisection, give brief overview main reasoning techniques OWL DLontologies since cost-based query planning relies techniques.order check whether ontology entails axiom , one typically checks whether{} model. case, every model satisfies|= . example, check whether individual a0 instance concept Cw.r.t. ontology O, check whether adding concept assertion C(a0 ) leadsinconsistency. check this, OWL reasoners use model construction calculustableau hypertableau. remainder, focus hypertableau calculus(Motik, Shearer, & Horrocks, 2009), tableau calculus could equally usedstate results transferred tableau calculi.hypertableau calculus starts initial set ABox assertions and, applyingderivation rules, tries construct (an abstraction of) model O. Derivation rulesusually add new concept role assertion axioms, may introduce new individuals,nondeterministic, leading need choose several alternative assertionaxioms add lead clash contradiction detected. showontology (in)consistent, hypertableau calculus constructs derivation, i.e.,sequence sets assertions A0 , . . . , , A0 contains ABox assertions O,Ai+1 result applying derivation rule Ai final set assertionsrules applicable. derivation exists containclash, consistent called pre-model O. Otherwise inconsistent.assertion set assertions Ai derived either deterministically nondeterministically. assertion derived deterministically derived applicationdeterministic derivation rule assertions derived deterministically.derived assertion derived nondeterministically. easy know whether assertion derived deterministically dependency directed backtracking(hyper)tableau reasoners employ. pre-model, individual s0 assignedlabel L(s0 ) representing concepts (non)deterministically instance260fiOptimizing SPARQL Query Answering OWL Ontologiespair individuals hs0 , s1 assigned label L(hs0 , s1 i) representing rolesindividual s0 (non)deterministically related individual s1 .3. Motivationstraightforward algorithm compute answers query q test,mapping , whether |= (q). Since terms used occur rangemapping q O, finitely many mappings test. worst case,however, number mappings tested still exponential numbervariables query. algorithm sound complete reasoner useddecide entailment sound complete since check mappings variablesconstitute actual solution mappings.Optimizations cannot easily integrated sketched algorithm since usesreasoner check entailment instantiated query whole and, hence,take advantage relations dependencies may exist individualaxiom templates q. optimized evaluation, one evaluate query axiomtemplate axiom template. Initially, solution set contains identity mapping,map variable value. One picks first axiom template,extends identity mapping cover variables chosen axiom templateuses reasoner check mappings instantiate axiom templateentailed axiom. One picks next axiom template extends mappingsprevious round cover variables checks mappings leadentailed axiom. Thus, axiom templates selective satisfiedsolutions reduce number intermediate solutions. Choosing good executionorder, therefore, significantly affect performance.example, let q = {A(?x), r(?x, ?y)} ?x, ?y VI . query belongsclass conjunctive instance queries. assume queried ontology contains 100individuals, 1 belongs concept A. instance 1 r-successor,overall 200 pairs individuals related role r. first evaluateA(?x), test 100 mappings (since ?x individual variable), 1 mappingsatisfies axiom template. evaluate r(?x, ?y) extending mapping100 possible mappings ?y. 1 mapping yields solution. reverseaxiom template order, first axiom template requires test 100 100 mappings.those, 200 remain checked second axiom template perform 10, 200tests instead 200. Note also number intermediate results queryevaluated order A(?x), r(?x, ?y) smaller evaluated reverseorder (2 versus 201).context databases triple stores, cost-based ordering techniques findingoptimal near optimal join ordering widely applied (Steinbrunn, Moerkotte, &Kemper, 1997; Stocker, Seaborne, Bernstein, Kiefer, & Reynolds, 2008). techniquesinvolve maintenance set statistics relations indexes, e.g., numberpages relation, number pages index, number distinct values column,together formulas estimation selectivity predicates estimationCPU I/O costs query execution depends amongst others, numberpages read written secondary memory. formulas261fiKollia & Glimmestimation selectivities predicates (result output size axiom templates) estimatedata distributions using histograms (Ioannidis & Christodoulakis, 1993), parametricsampling methods combinations them. Ordering strategies implementeddatabases triple stores are, however, directly applicable setting. presenceexpressive schema level axioms, cannot rely counting number occurrencestriples. also cannot, general, precompute relevant inferences base statisticsmaterialized inferences. Furthermore, aim decreasing numberintermediate results, also take account cost checking computingsolutions. cost significant OWL reasoning precise estimationquery evaluation difficult cost takes values wide range, e.g., duenondeterminism high worst-case complexity standard reasoning tasks.1several kinds axiom templates can, however, directly retrieve solutionsreasoner instead checking entailment. example, C(?x), reasoners typically method retrieve concept instances. Although might internally triggerseveral tests, methods reasoners highly optimized avoid many testspossible. Furthermore, reasoners typically cache several results computed concept hierarchy retrieving sub-concepts realized cache lookup. Thus,actual execution cost might vary significantly. Notably, straight correlation number results axiom template actual cost retrievingsolutions typically case triple stores databases. requires cost modelstake account cost specific reasoning operations (depending statereasoner) well number results.motivated above, distinguish simple complex axiom templates. Simple axiom templates correspond dedicated reasoning tasks. Let c1concept term, C, C (complex) concepts concept variables, r1 , r2 role terms role inverses t, individual terms. set simple axiom templates contains templatesform: C C , r1 . c1 (domain restriction template), r1 .c1 (range restrictiontemplate), r1 r2 , C(t), r1 (t, ), , 6 . Complex axiom templates can, contrast, evaluated dedicated reasoning tasks might require iteratingcompatible mappings checking entailment instantiated axiom template.example complex axiom template (r.?x)(?y).4. Preprocessing Extracting Information Queriessection, describe way preprocessing queried ontology extract information useful ordering axiom templates query. preprocessing usefulaxiom templates form c1 (t), r1 (t, ), , c1 concept term, r1role term t, individual terms.4.1 Extracting Individual Information Reasoner Modelsfirst step ordering query atoms extraction statistics exploitinginformation generated reasoners. use labels initial pre-model pro1. example, description logic SROIQ, underpins OWL 2 DL standard, worst casecomplexity 2-NExpTime (Kazakov, 2008) typical implementations worst case optimal.262fiOptimizing SPARQL Query Answering OWL OntologiesAlgorithm 1 initializeKnownAndPossibleConceptInstances(O)Input: consistent SROIQ ontology1: := buildM odelF or(O)2: NIO3:C LAn (a)4:C derived deterministically5:K[C] := K[C] {a}6:else7:P [C] := P [C] {a}8:end9:end10: endvide information concepts individuals belong roles oneindividual connected another one. exploit information similarly suggested determining known possible (non-)subsumers concepts classification(Glimm et al., 2012). hypertableau calculus, following two properties holdontology constructed pre-model O:(P1) concept name C (role name r), individual s0 (pair individuals hs1 , s2 i), C LAn (s0 ) (r LAn (hs1 , s2 i)) assertion C(s0 ) (r(s1 , s2 ))derived deterministically, holds |= C(s0 ) (O |= r(s1 , s2 )).(P2) arbitrary individual s0 (pair individuals hs1 , s2 ) arbitraryconcept name C (simple role name r), C 6 LAn (s0 ) (r 6 LAn (hs1 , s2 i)),6|= C(s0 ) (O 6|= r(s1 , s2 )).simplicity, assume equality () axiomatized treatedreflexive, symmetric, transitive role. use properties extract informationpre-model satisfiable ontology O.Definition 7 (Known Possible Instances). Let pre-model ontologyO. individual known (possible) instance concept name C , denotedKAn [C] (a PAn [C]), C LAn (a) C(a) derived deterministically (nondeterministically) . pair individuals ha1 , a2 known (possible) instance simplerole name r , denoted ha1 , a2 KAn (r), r LAn (ha1 , a2 i) r(a1 , a2 ) deriveddeterministically (nondeterministically) . individual a1 (possibly) equalindividual a2 , written a1 K [a2 ] a2 K [a1 ] (a1 P [a2 ] a2 P [a1 ])a1 a2 deterministically (nondeterministically) derived O.remainder, assume known possible instances defined w.r.t.arbitrary pre-model simply write K[C], K[r], K [a], P [C], P [r],P [a]. Intuitively, K[C] contains individuals safely considered instancesconcept name C. hand, possible instances require costly consistencychecks order decide whether real instances concept, individualsneither belong K[C] P [C] safely assumed non-instances C.263fiKollia & GlimmAlgorithm 1 outlines procedure initialize relations known possible concept instances. information extract involves maintenance sets knownpossible instances concepts O. One define similar algorithm initializing known possible instances simple roles (possibly) equal individuals.implementation, use involved procedure store direct typesindividual, concept name C direct type individual ontology|= C(a) concept name |= C, |= D(a)6|= C.Hypertableau tableau reasoners typically deal transitivity directly.order deal non-simple roles, expanded additional axioms capturesemantics transitive relations pre-model built. particular,individual non-simple role r, new concepts Ca Car introduced axiomsCa (a) Ca r.Car added O. consequent application transitivityencoding (Motik et al., 2009) produces axioms propagate Car individual breachable via r-chain. known possible r-successorsdetermined Car instances.technique presented paper used (hyper)tableau calculusproperties (P1) (P2) hold. (hyper)tableau calculi used practiceaware satisfy property (P1). Pre-models produced tableau algorithms presentedliterature also satisfy property (P2); however, commonly used optimizations,lazy unfolding, compromise property (P2), illustrate followingexample. Let us assume ontology containing axiomsr.(C D)(1)B r.C(2)A(a)(3)obvious ontology subconcept B (hence, |= B(a)) since everyindividual r-related individual instance intersection Calso r-related individual instance concept C. However,even though assertion A(a) occurs ABox, assertion B(a) addedpre-model use lazy unfolding. lazy unfolding, instead treating (2) twodisjunctions B r.C B r.(C) typically done general concept inclusionaxioms, B lazily unfolded definition r.C B occurs labelindividual. Thus, although (r.(C D))(a) would derived, leadaddition B(a).Nevertheless, (if all) implemented calculi produce pre-models satisfyleast following weaker property:(P3) arbitrary individual s0 arbitrary concept name C Cprimitive O,2 C 6 LAn (s0 ), 6|= C(s0 ).Hence, properties (P2) (P3) used extract (non-)instance informationpre-models. tableau calculi satisfy (P3), non-primitive concept name2. concept C considered primitive unfoldable (Tsarkov et al., 2007) containsaxiom form C E264fiOptimizing SPARQL Query Answering OWL OntologiesC need add P [C] individuals include concept Clabel.proposed technique determining known possible instances conceptrole names used way tableau hypertableau reasoners.Since tableau algorithms often introduce nondeterminism hypertableau, onemight, however, find less deterministic derivations, results less accurate statistics.4.1.1 Individual Clusteringsection, describe procedure creating clusters individuals withinontology using constructed pre-model O. Two types clusters created:concept clusters role clusters. Concept clusters contain individualsconcepts label role clusters contain individuals concept rolelabels. Role clusters divided three categories, based firstindividual role instances, based second individual basedindividuals.Definition 8 (Concept Role Clusters). Let ontology pre-modelO. define following two relations P1 P2 map individualroles least one successor predecessor, respectively:P1 (a) = {r | r LAn (ha, bi) b NIO }P2 (a) = {r | r LAn (hb, ai) b NIO }Based relations, build three different partitions NIO : concept clusters CC,role successor clusters P C1 , role predecessor clusters P C2 clusters satisfy:C CC.(for a1 , a2 C.(LAn (a1 ) = LAn (a2 )))C P C1 .(for a1 , a2 C.(LAn (a1 ) = LAn (a2 ) P1 (a1 ) = P1 (a2 )))C P C2 .(for a1 , a2 C.(LAn (a1 ) = LAn (a2 ) P2 (a1 ) = P2 (a2 ))).partition NIO NIO role clusters P C12 clusters satisfy:C P C12 .(for ha1 , a2 i, ha3 , a4 C.(LAn (a1 ) = LAn (a3 ), LAn (a2 ) = LAn (a4 )LAn (ha1 , a2 i) = LAn (ha3 , a4 i))).use clusters next section optimize dynamic query ordering strategy.5. Query Answering Axiom Template Orderingsection, describe two different algorithms (a static dynamic one) orderingaxiom templates query based costs deal formulationcosts. first introduce abstract graph representation query q meanslabeled graph Gq define computed statistical costs.Definition 9 (Query Join Graph). query join graph Gq query q tuple(V, E, EL ),265fiKollia & GlimmV = q set vertices (one axiom template);E V V set edges; hat1 , at2 E Vars(at1 ) Vars(at2 ) 6=at1 6= at2 ;EL function assigns set variables hat1 , at2 EEL (at1 , at2 ) = Vars(at1 ) Vars(at2 ).remainder, use Gq query join graph q.goal find query execution plan, determines evaluation orderaxiom templates q. Since number possible execution plans order |q|!,ordering task quickly becomes impractical. following, focus greedy algorithmsdetermining execution order, prune search space considerably. Roughlyspeaking, proceed follows: define cost function, consists two components(i) estimate costs reasoning tasks needed evaluation axiomtemplate (ii) estimate intermediate result size, i.e., number resultsevaluation axiom template incur. components combinedinduce order among axiom templates. paper, simply build sumtwo cost components, different combinations weighted sum two valuescould also used. query plan construction distinguish static dynamicplanning. former, start constructing plan adding minimal templateaccording order. Variables template considered bound,changes cost function might induce different order among remaining axiomtemplates. Considering updated order, select minimal axiom templateyet plan update costs. process continues plan containstemplates. complete plan determined templates evaluated.dynamic case differs selecting template plan, immediatelydetermine solutions chosen template, used update costfunction. yields accurate cost estimates, costly solutionsconsidered updating cost function. Sampling techniques used testsubset solutions, show Section 7 random sampling, i.e., randomlychoosing percentage individuals far computed solutions, adequate.reason, propose alternative sampling approach based usepreviously described individual clusters. first present example make differencestatic dynamic planning clearer justify dynamic orderingbeneficial setting.Example 1. Let ontology q = {C(?x), r(?x, ?y), D(?y)} conjunctive instancequery O. Suppose known possible instances query conceptsrolesK[C] = {a}K[r] =K[D] = {b}P [C] = {c, e}P [r] = {hc, di, he, f i}P [D] = {f, g, h}let us assume possible instances C, r are, fact, real instances (noteinformation beginning). Please mindpossible instances concepts roles costly evaluate known instances266fiOptimizing SPARQL Query Answering OWL Ontologiessince require expensive consistency checks order decide whether realinstances.According static planning, ordering query atoms first determined. particular, atom r(?x, ?y) chosen first since least number known possible instances (0 known 2 possible versus 1 known 2 possible C(?x)1 known 3 possible D(?y)). atom C(?x) chosen since lessknown possible instances D(?y), i.e., 1 known 2 possible versus 1 known3 possible D(?y). Hence chosen execution plan static planning P =(r(?x, ?y), C(?x), D(?y)). Afterwards, query evaluated according chosen execution plan, i.e., atom r(?x, ?y) evaluated first, gives solution mappings1 = {{?x 7 c, ?y 7 d}, {?x 7 e, ?y 7 f }}. requires 2 consistency checks 2possible instances r. Afterwards, check ?x mappings, c e, knownpossible instances C. Since c e possible instances, check whetherreal instances C (this requires 2 consistency checks). Hence, solution mappings2 = 1 = {{?x 7 c, ?y 7 d}, {?x 7 e, ?y 7 f }}. end, check?y mappings, f , known possible instances D. possibleinstance, f , find one consistency check f indeed instance D. Hence,solution mappings qq = {{?x 7 e, ?y 7 f }} finding solution required5 consistency checks.According dynamic planning, ordering determined evaluate query.reasons before, atom r(?x, ?y) chosen evaluated firstsolution mappings are, before, 1 = {{?x 7 c, ?y 7 d}, {?x 7 e, ?y 7 f }} (this requires2 consistency checks). afterwards check ?y mappings, f , knownpossible instances D. Note requires look-up since find famong possible instances, check whether individual indeed instancenot. f possible instance. also check ?x mappings, ce, known possible instances C. Here, c e possible instances, i.e.,2 relevant possible instances C(?x) 1 D(?y). Hence, atom D(?y)chosen evaluated next, resulting solution sequence 2 = {{?x 7 e, ?y 7 f }}(partial) execution plan (r(?x, ?y), D(?y)), requiring 1 consistency check. end,check whether ?x mapping, e, known possible instance C. Since epossible instance, check whether real instance (this requires 1 consistency check).Hence, solution mappings qq = {{?x 7 e, ?y 7 f }}, foundperforming 4 consistency checks, one less static case.Note dynamic ordering perform less checks static ordering, sincecase exploit results joins query atoms information regardingpossible instances atoms (i.e., real instances), determinedresult evaluating atoms ordering them.make process query plan construction precise, leaveexact details defining cost function ordering induces later.Definition 10 (Static Dynamic Ordering). static (dynamic) cost function w.r.t.q function : q 2V ars(q) R R (d : q 2q R R),qdenote set compatible mappings q O. two costs hEcat , Rsat (hEcat , Rsdat i)axiom template q combined yield static ordering ( dynamic ordering267fiKollia & Glimm), total order axiom templates q that, at, q, say(at ) iff Ecsat + Rssat Ecsat + Rssat (Ecdat + Rsdat Ecdat + Rsdat ).execution plan q duplicate-free sequence axiom templates q.initial execution plan empty sequence complete execution plan sequencecontaining templates q. Let Pi = (at1 , . . . , ati ) < |q| execution planq query join graph Gq = (V, E, EL ). set bound variables ati within PiVb (ati ) = Vars(ati ) Vars({at1 , . . . , ati1 }). Let Cq set complex axiom templatesq. next define axiom templates used extend incomplete executionplan. Let axiom template Pi , set suci (at) contains axiom templatesconnected yet Pi , i.e., suci (at) = {at q | hat, E,/{at1 , . . . ati }}. Based this, define set connected successor axiom templatesPi Si = {at | {at1 , . . . , ati } suci (at )}. allow includingaxiom templates connected complex axiom template Si definepotential next templates qi Pi w.r.t. Gq qi = q Pi initial execution planotherwise[q = Sisuci (at).Cq SiGiven Pi , static (dynamic) ordering induces execution plan Pi+1 = (at1 , . . . , ati , ati+1 )ati+1 qi ati+1 (ati+1 at) qi 6= ati+1 .Note according definition, Pi execution plan, caseqi contains templates assigned minimal cost cost function.case, one choose atoms add Pi . Moreover, accordingdefinition case queries containing simple axiom templatesthat, > 0, set potential next templates contains templatesconnected template already plan since unconnected templates causeunnecessary blowup number intermediate results. queries complextemplates set potential next axiom templates additionally contain templatesshare common variables template already plan. differenthandling queries complex templates reasonable since, evaluating complexaxiom template requires many consistency checks, want reduce numbercandidate bindings, first evaluating simple (cheaper) templates bind variablesappear complex one.Example 2. Let ontology q = {?x A, ?y r, B ?y.?x} queryO. Assuming systems usually precompute concept role hierarchiesaccept queries, evaluation first two templates, i.e., ?x ?y r, requirecheap cache lookups, whereas axiom template B ?y.?x, requires costly consistencychecks. Hence, reasonable first evaluate first two (cheap) templates reducemappings ?x ?y evaluate third (expensive) template, checkingreduced mappings yield entailed axiom.example shows actual gain get handling ordering complexaxiom templates way presented Section 7.Let n = |q| Pn = (at1 , . . . , atn ) complete execution plan q determinedstatic ordering. procedure find solution mappingsq Pn recursively268fiOptimizing SPARQL Query Answering OWL Ontologiesdefined follows: Initially, solution set contains identity mapping 0 = {0 },map variable value. Assuming evaluatedsequence Pi = (at1 , . . . , ati ), < n found set solution mappings ,order find solution mappings i+1 Pi+1 , use specific reasoning tasksextend mappings cover new variables ati+1 ati+1 simple axiomtemplate entailment check service reasoners ati+1 contain new variablesati+1 complex axiom template. dynamic planning differenceexecution plan construction interleaved query evaluation. particular, let n = |q|Pi = (at1 . . . ati ) < n (partial) execution plan q determined dynamicordering let solution mappings Pi . order find Pi+1 extend Pinew template, ati+1 , q, i.e., Pi+1 = (at1 , . . . ati+1 ), which, according dynamiccost function, minimal cost among remaining templates q \ {at1 , . . . ati }.dynamic cost function assigns costs templates iteration + 1 taking accountsolution mappings . afterwards evaluate atom ati+1 , i.e., find solutionmappings i+1 Pi+1 extending solution mappings Pi waystatic case. Section 6.3 Algorithm 3, show complete procedure followanswer query.define cost functions precisely, estimate costrequired reasoner operations (first component) estimated result output size (secondcomponent) evaluating axiom template. intuition behind estimated valuereasoner operation costs evaluation possible instances muchcostly evaluation known instances since possible instances require expensiveconsistency checks whereas known instances require cheap cache lookups. estimatedresult size takes account number known possible instances probabilitypossible instances actual instances.time needed entailment check change considerably ontologieseven within ontology (depending involved concepts, roles individuals).order accurately determine entailment cost use different entailment costvalues depending whether template consideration template form i)c1 (t), ii) r1 (t, ), iii) , c1 concept term, r1 role term t, individualterms, iv) one rest simple axiom templates (that require consistency checksevaluated) complex axiom template. following write CL denote costcache lookup internal structures reasoner, CE placeholder relevantentailment cost value PIS possible instance success, i.e, estimated percentagepossible instances actual instances. costs CL CE determinedrecording average time previously performed lookups entailment checksqueried ontology, e.g., initial consistency check, classification, previousqueries. possible instance success, PIS , determined testing several ontologieschecking many initial possible instances real ones, around50% nearly ontologies.Apart relations known possible instances Section 4.1, usefollowing auxiliary relations:Definition 11 (Successor Predecessor Relations). Let r role individual. define sucK[r] preK[r] set individuals known r-successors269fiKollia & Glimmr-predecessors, respectively:sucK[r] := {a | b.ha, bi K[r]}preK[r] := {a | b.hb, ai K[r]}.Similarly, define sucK[r, a] preK[r, a] known r-successors knownr-predecessors a, respectively:sucK[r, a] := {b | ha, bi K[r]}preK[r, a] := {b | hb, ai K[r]}.analogously define functions sucP[r], preP[r], sucP[r, a], preP[r, a] replacingK[r] P [r].Next, define cost functions case conjunctive instance queries, i.e.,queries containing query atoms. Section 5.2 extend cost functions dealgeneral queries.5.1 Static Dynamic Cost Functions Conjunctive Instance Queriesstatic cost function takes two components input: query atom set containingvariables query atom considered bound. function returns pairreal numbers reasoning cost result size query atom.Initially, variables unbound use number known possible instances successors/predecessors estimate number required lookups consistency checks evaluating query atom resulting number mappings.input form hC(?x), hr(?x, ?y), resulting pair real numberscomputational cost estimated result size computedh|K[at]| CL + |P [at]| CE , |K[at]| + PIS |P [at]|i,denotes predicate query atom (C r). concept (role) atom,factor represents depth concept (role) concept (role) hierarchy.use factor since store direct types individual (rolesindividuals instances) and, order find instances concept (role), mayneed check subconcepts (subroles) known possible instances. queryatom role atom constant first place, i.e., input cost functionform hr(a, ?x), i, use relations known possible successors estimatecomputational cost result size:h|sucK[r, a]| CL + |sucP[r, a]| CE , |sucK[r, a]| + PIS |sucP[r, a]|i.Analogously, use preK preP instead sucK sucP input formhr(?x, a), i. Finally, atom contains constants, i.e., input cost functionform hC(a), i, hr(a, b), i, function returns hd CL , 1i individualknown instance concept role, hd CE , PIS individual possible instancehd CL , 0i otherwise, i.e., individual known non-instance.equality atoms form ?x ?y, ?x, ?x b, exploitinformation initial pre-model described Section 4.1. Based cardinalityK [a] P [a], define cost functions different cases query atoms270fiOptimizing SPARQL Query Answering OWL Ontologiesbound variables. inputs form h?x a, ha ?x, i, cost functiondefined as:h|K [a]| CL + |P [a]| CE , |K [a]| + PIS |P [a]|i.inputs form h?x ?y, i, cost function computed as:*+XX(|K [a]| CL + |P [a]| CE )/2,(|K [a]| + PIS |P [a]|)/2 .aNIOaNIOinputs form ha b, i, function returns hCL , 1i b K [a], hCE , PISb P [a], hCL , 0i otherwise (i.e., b equivalent a).determining cost initial query atom, least one variable consequently considered atom bound, since query plan construction moveatoms sharing common variable assume query connected. define cost functions atoms least one variable bound. make assumptionatoms unbound variables costly evaluate atomsvariables bound. query atom r(?x, ?y) ?x bound, i.e., function inputsform hr(?x, ?y), {?x}i, use average number known possible successorsrole estimate computational cost result size:|P [r]||K[r]||P [r]||K[r]|CL +CE ,+PIS .|sucK[r]||sucP[r]||sucK[r]| |sucP[r]|case ?y r(?x, ?y) bound, use predecessor functions preK preP insteadsucK sucP. Note work estimated average number successors(predecessors) one individual.atoms variables bound, use formulas comparableones initial plan, normalized estimate values one individual.input query atom form C(?x) ?x bound variable use|K[C]| CL + |P [C]| CE |K[C]| + PIS |P [C]|,.|NIO ||NIO |simple normalization always accurate, leads good resultscases show Section 7. Similarly, normalize formulas role atomsform r(?x, ?y) {?x, ?y} set bound variables atom. two costcomponents atoms computed|K[r]| CL + |P [r]| CE |K[r]| + PIS |P [r]|,.|NIO | |NIO ||NIO | |NIO |role atoms constant bound variable, i.e., atoms form r(a, ?x)(r(?x, a)) ?x bound variable, use sucK[r, a] sucP[r, a] (preK[r, a] preP[r, a])instead K[r] P [r] formulas normalize |NIO |.Similarly, normalize cost functions inputs equality atoms boundvariables, depending whether atoms contain one two bound variables. inputsform h?x a, {?x}i, ha ?x, {?x}i, divide cost function components inputs271fiKollia & Glimmalready executed1234567r(?x, ?y)r(?x, ?y)r(?x, ?y), D(?y)r(?x, ?y), C(?x)current atomC(?x)r(?x, ?y)D(?y)C(?x)D(?y)C(?x)D(?y)K[at]200200700100504545P [at]350200600150503540real P [at]20050400100402525Table 1: Query Ordering Exampleform h?x a, ha ?x, |NIO |. input form h?x y, {?x, ?y}i,divide cost function components input form h?x ?y, |NIO | |NIO |.inputs form h?x ?y, {?x}i, h?x ?y, {?y}i, divide cost functioncomponents input form h?x ?y, |NIO |.dynamic cost function based static function s, uses firstequations, atom contains unbound variables constants. functiontakes pair hat, input, query atom set solution mappingsatoms already evaluated, returns pair real numbers usingmatrix addition follows:Xd(at, ) =s((at), )sampling techniques used, compute costs potential nextatoms execution plan considering one individual relevant cluster.cluster relevant depends query atom compute cost functionpreviously computed bindings. instance, compute cost role atomr(?x, ?y) already determined bindings ?x, use role successor clusterP C1 . Among ?x bindings, check cost one binding per clusterassign cost ?x bindings cluster.Example 3. Let us assume conjunctive instance query qfind cost (using dynamic function) atom C(?x) within execution planq. assume evaluation previous query atoms planalready determined set intermediate solutions mappings a, b, c?x. Let us assume a, b, c belong concept cluster. Accordingdynamic ordering need find cost instantiated atom using static costfunction, i.e., d(C(?x), ) = s(C(a), ) + s(C(b), ) + s(C(c), ). additionally usecluster based sampling, find cost one individual cluster, let us say a,assign cost individuals cluster mappings?x . Hence, cost atom C(?x) sampling used, computedd(C(?x), ) = 3 s(C(a), ) avoiding computation s(C(b), ) s(C(c), ).example similar Example 1 (but greater number instances)shows ordering achieved use defined static dynamic functionsshown below. assume q query consisting three query atoms: C(?x),272fiOptimizing SPARQL Query Answering OWL Ontologiesr(?x, ?y), D(?y). Table 1 gives information known possible instancesatoms within sequence. second column shows already executed sequencesPi1 = (at1 , . . . , ati1 ) atoms q. Column 3 gives current atom ati column4 (5) gives number mappings known (possible) instances satisfytime atoms (at1 , . . . , ati1 ) column 2. Column 6 gives number realinstances possible instances current atom. example, row 4 saysevaluated atom r(?x, ?y) and, order evaluate C(?x), consider100 known 150 possible instances C also mappings ?x. assume10,000 individuals ontology O. explain, using example,described formulas work. assume CL CE , alwayscase since cache lookup less expensive consistency check CE valuesquery concepts roles. ease presentation,consider factor depth concept (role) within concept (role) hierarchy.techniques (static dynamic) atom r(?x, ?y) chosen first sinceleast number possible instances (200) (or smaller) number knowninstances (200) atoms (0 initial solution mapping mapvariable):s(r(?x, ?y), ) = d(r(?x, ?y), {0 }) = h200 CL + 200 CE , 200 + PIS 200i,s(C(?x), ) = d(C(?x), {0 }) = h200 CL + 350 CE , 200 + PIS 350i,s(D(?y), ) = d(D(?y), {0 }) = h700 CL + 600 CE , 700 + PIS 600i.case static ordering, atom C(?x) chosen r(?x, ?y) since C lesspossible (and known) instances (350 versus 600):350200 + 350 PIS200CL +CE ,,s(C(?x), {?x}) =10, 00010, 00010, 000700600700 + 600 PISs(D(?y), {?y}) =CL +CE ,.10, 00010, 00010, 000Hence, order evaluation case P = (r(?x, ?y), C(?x), D(?y)) leading200 (row 2) + 150 (row 4) + 40 (row 7) entailment checks. dynamic case,evaluation r(?x, ?y), gives set solutions 1 , atom D(?y) fewer knownpossible instances (50 known 50 possible) atom C(?x) (100 known150 possible) and, hence, lower cost:d(D(?y), 1 ) = h50 CL + 150 CL + 50 CE , 50 + 0 + 50 PIS i,d(C(?x), 1 ) = h100 CL + 0 CL + 150 CE , 100 + 0 + 150 PIS i.Note applying solution 1 D(?y) (C(?x)) results query atomconstant place ?y (?x). D(?y), case 250 r-instances, 200handled look-up (50 turn known instances 150 turninstances D), 50 require entailment check. Similarly, consideringC(?x), need 100 lookups 150 entailment checks. Note assume worstcase example, i.e., values ?x ?y take different. Therefore,atom D(?y) chosen next, leading execution query atoms orderP = (r(?x, ?y), D(?y), C(?x)) execution 200 (row 2) + 50 (row 5) + 35 (row 6)entailment checks.273fiKollia & Glimm5.2 Cost Functions General Queriesexplain order remaining simple complex axiom templates.use statistics reasoner, whenever available. case reasonercannot give estimates, one still work statistics computed explicitly statedinformation use upper bounds estimate reasoner costs result size axiomtemplates.first consider general concept assertion axiom template. Let KC [a] conceptsknown instance, PC [a] concepts possible instance.sets computed sets known possible instances concepts. inputform h?x(a), cost function definedh|KC [a]| CL + |PC [a]| CE , |KC [a]| + PIS |PC [a]|i.input form h?x(?y), i, cost function defined*+XX(|K[C]| CL + |P [C]| CE ),(|K[C]| + PIS |P [C]|) .CNCOCNCOinputs form h?x(a), {?x}i h?x(?y), {?x, ?y}i, normalize functions|NCO | |NIO ||NCO | respectively. inputs form h?x(?y), {?x}i h?x(?y), {?y}inormalize function inputs form h?x(?y), |NCO | |NIO | respectively.general role assertion axiom templates, several cases cost functions depending bound variables. next define cost functions cases. costfunctions cases similarly defined. input form h?z(?x, ?y), i,cost function defined :*+XX(|K[r]| CL + |P [r]| CE ),(|K[r]| + PIS |P [r]|) .rNROrNROinputs form h?z(a, ?y), i, cost function defined as:*+XX(|sucK[r, a]| CL + |sucP[r, a]| CE ),(|sucK[r, a]| + PIS |sucP[r, a]|) .rNROrNROinput form h?z(?x, ?y), {?z}i, cost function defined as:+*X |K[r]| CL + |P [r]| CE X |K[r]| + PIS |P [r]|,.|NRO ||NRO |rNRrNRLast, inputs form h?z(?x, ?y), {?x}i, two cost components computed as:*+X |K[r]|X |K[r]||P [r]||P [r]|(CL +CE ),+PIS ) .(|sucK[r]||sucP[r]||sucK[r]| |sucP[r]|rNRrNR274fiOptimizing SPARQL Query Answering OWL Ontologiesconcept (role) inclusion axiom templates form c1 c2 (r1 r2 ), c1 , c2concept terms (r1 , r2 role terms), contain concept (role) names variablesneed lookups computed concept (role) hierarchy order compute answers(assuming concept (role) hierarchy precomputed).One define similar cost functions types axiom templates either usingavailable statistics relying told information ontology. paper,however, define cost function based assumption iteratepossible values respective variables one consistency check value.Hence, define following general cost function cases:h|N | CE , |N |i,N {NCO , NRO , NIO } appropriate variable tested. discussedSection 5.1, dynamic function based static one applieddescribed cases empty set bound variables.Proposition 1. Let q query ontology O, static dynamic costfunctions defined Sections 5.1 5.2. ordering induced total orderaxiom templates q.Proof. cost functions defined kinds axiom templates returntwo real numbers possible input. Since, according Definition 10, ordersbased addition two real numbers, addition reals yieldsreal number, since total order reals, immediately gettotal orders.obvious ordering axiom templates affect soundness completeness query evaluation algorithm.6. Complex Axiom Template Optimizationssection, first describe optimizations developed complexaxiom templates (Sections 6.1, 6.2) present procedure evaluating queries(Section 6.3).6.1 Axiom Template Rewritingcostly evaluate axiom templates rewritten axiom templatesevaluated efficiently yield equivalent result. go describeaxiom template rewriting technique, define concept template is, usefulthroughout section.Definition 12 (Concept Template). Let Sq = (NC , NR , NR , VC , VR , VI ) querysignature w.r.t. signature = (NC , NR , NI ). concept template Sq SROIQconcept S, one also use concept variables VC place concept names,role variables VR place role names individual variables VI placeindividual names.275fiKollia & GlimmDefinition 13 (Rewriting). Let ontology, q axiom template Sq ,t, t1 , . . . tn individuals individual variables Sq , C, C1 , . . . , Cn concept templatesSq . function rewrite takes axiom template returns set axiom templatesfollows:= (C1 . . . Cn )(t), rewrite(at) = {C1 (t), . . . , Cn (t)};= C C1 . . . Cn , rewrite(at) = {C C1 , . . . , C Cn };= C1 . . . Cn C, rewrite(at) = {C1 C, . . . , Cn C};= t1 . . . tn , rewrite(at) = {t1 t2 , t2 t3 , . . . , tn1 tn }.understand intuition behind transformation, consider queryaxiom template: ?x r.?y A. evaluation requires quadratic number consistency checks number concepts (since ?x ?y concept variables).rewriting yields: ?x ?x r.?y. first axiom template evaluatedcheap cache lookup (assuming concept hierarchy precomputed).second one, check usually resulting bindings ?x combinedconcept names ?y.Note Description Logics typically support n-ary equality axioms t1 . . .tn , binary ones, whereas OWL, one typically also write n-ary equality axioms.Since cost functions defined binary equality axioms, equivalently rewriten-ary one several binary ones. One could even optimize evaluationatoms evaluating one binary equality axiom template propagatingbinding found equivalent individuals equality axioms. validsince equality congruence relation.6.2 Concept Role Hierarchy Exploitationnumber consistency checks required evaluate query reducedtaking concept role hierarchies account. concepts rolesclassified (this ideally done system accepts queries), hierarchiesstored reasoners internal structures. use hierarchies prunesearch space solutions evaluation certain axiom templates. illustrateintuition example Infection hasCausalLinkTo.?x. solutionB holds, B also solution. Thus, searching solutions ?x,choose next binding test traversing concept hierarchy top-down. findnon-solution A, subtree rooted concept hierarchy safely pruned.Queries ontologies large number concepts deep concept hierarchycan, therefore, gain maximum advantage optimization. employ similaroptimizations using role hierarchies.example above, prune subconcepts ?x positivepolarity axiom template Infection hasCausalLinkTo.?x., i.e., ?x occurs positivelyright hand side axiom template. case variable ?x negative polarityaxiom template form C1 C2 , i.e., ?x occurs directly indirectlynegation right hand side axiom template positively left-hand sideaxiom template, one can, instead, prune superconcepts.276fiOptimizing SPARQL Query Answering OWL Ontologiesnext specify precisely polarity concept variable concept axiomtemplate.Definition 14 (Concept Polarity). Let ?x VC concept variable C, C1 , C2 ,concept templates, r role, n IN0 . define polarity ?x C follows: ?xoccurs positively ?x. Furthermore, ?x occurs positively (negatively)?x occurs negatively (positively) D,C1 C2 C1 C2 ?x occurs positively (negatively) C1 C2 ,r.D, r.D, > n r.D ?x occurs positively (negatively) D,6 n r.D ?x occurs negatively (positively)= n r.D ?x occurs D.say ?x occurs positively (negatively) C1 C2 ?x occurs negatively(positively) C1 positively (negatively) C2 . Note ?x occur positivelynegatively concept template. define partial function polc mapsconcept variable ?x concept template C (axiom template form C1 C2 )pos ?x occurs positively C (C1 C2 ) neg ?x occurs negativelyC (C1 C2 ).Note matter whether ?x occurs positively negatively concept template D,concept template C form = n r.D, ?x occurs positively well negatively.due fact C equivalent concept template 6 n r.D > n r.D?x occurs positively well negatively. Since function polc definedvariables appear positively negatively, concept hierarchy cannotexploited case. example, consider concept template ?x r.?x, (axiomtemplate ?x r.?x), ?x appears negatively ?x positively r.?x. Now,let arbitrary element model = (I , ) ontology. obviousinstance r.A either B B holds, cannot deduceinstance B r.B.proving correctness proposed optimization, first show relationship entailment concept membership, used subsequent proofs.Lemma 1. Let q query w.r.t. query signature Sq = (NC , NR , NI , VC , VR , VI ),q axiom template form C1 C2 C1 C2 concept templateslet mapping O. holds 6|= (C1 C2 ) iff existsinterpretation = (I , ) element |= 6 (C1 C2 )I .Proof. 6|= (C1 C2 ) holds iff exists interpretation = (I , ) element|= (C1 )I 6 (C2 )I , holds iff (C1 )I(C2 )I , equivalent (C1 C2 )I , equivalent((C1 C2 ))I , holds iff 6 (C1 C2 )I .following theorem holds every axiom template form C1 C2 . Noteassume concept assertion templates form C(a) expressedequivalent axiom templates {a} C. use C(?x)=A , concept name,denote concept obtained applying extension also maps ?x A.277fiKollia & GlimmTheorem 1. Let ontology, A, B concept names |= B, C1 , C2concept templates, C1 C2 axiom template, C = C1 C2 , ?x VC concept variableoccurring C mapping covers variables C apart ?x.1. polc (?x, C) = pos holds 6|= (C1 C2 )(?x)=B , 6|= (C1 C2 )(?x)=A .2. polc (?x, C) = neg holds 6|= (C1 C2 )(?x)=A , 6|= (C1 C2 )(?x)=B .Proof. Due Lemma 1, suffices show model = (I , )element following (which formalized contrapositive form):1. polc (?x, C) = pos holds (C(?x)=A )I , (C(?x)=B )I .2. polc (?x, C) = neg holds (C(?x)=B )I , (C(?x)=A )I .prove claim induction structure concept template C:C =?x, ?x occurs positively C. Now, (?x(?x)=A )I , AI ,easy see B since |= B assumption. Hence, (?x(?x)=B )I .C = polc (?x, C) = pos, (D(?x)=A )I , show(D(?x)=B )I . Note polc (?x, D) = neg. contrary shown,assume (D(?x)=B )I . Since |= B induction hypothesis(D(?x)=A )I (D(?x)=A )I contradiction. proof analogouspolc (?x, C) = neg.C = C1 C2 polc (?x, C) = pos, ((C1 C2 )(?x)=A )I ,(C1(?x)=A )I (C2(?x)=A )I . Since |= B induction hypothesis, (C1(?x)=B )I (C2(?x)=B )I . Thus, ((C1 C2 )(?x)=B )I .proof analogous polc (?x, C) = neg.proof C1 C2 analogous one C1 C2 .C = r.D polc (?x, C) = pos, ((r.D)(?x)=A )I , least one rsuccessor, say , instance D(?x)=A . Since |= B inductionhypothesis, D(?x)=B . Hence, (r.(D(?x)=B ))I = ((r.D)(?x)=B )I .proof analogous polc (?x, C) = neg.C = r.D polc (?x, C) = pos, ((r.D)(?x)=A )I , (r.(D)(?x)=A )Ir-successors instance D(?x)=A . Since |= B induction hypothesis, r-successors also instances D(?x)=B . Hence,(r.(D(?x)=B ))I = ((r.D)(?x)=B )I . proof analogous polc (?x, C) = neg.C = > n r.D polc (?x, C) = pos, ((> n r.D)(?x)=A )I ,least n distinct r-successors instances D(?x)=A . Since |= Binduction hypothesis, successors instances D(?x)=B . Hence,least n distinct r-successors instances D(?x)=B and, therefore, (>n r.(D)(?x)=B )I = ((> n r.D)(?x)=B )I . proof analogous polc (?x, C) = neg.278fiOptimizing SPARQL Query Answering OWL OntologiesC = 6 n r.D polc (?x, C) = pos, ((6 n r.D)(?x)=A )I , show((6 n r.D)(?x)=B )I . Note polc (?x, D) = neg. contraryshown, assume ((6 n r.D)(?x)=B )I , i.e., ((> n + 1 r.D)(?x)=B )I .Hence, least n + 1 distinct r-successors instances D(?x)=B .Since polc (?x, D) = neg induction hypothesis, D(?x)=B instancesalso D(?x)=A instances (> n + 1 r.(D)(?x)=A )I = ((> n + 1 r.D)(?x)=A )I ,contradiction. proof analogous polc (?x, C) = neg.C = (= n r.D), polarity ?x C always positive negative,polc (?x, C) undefined case cannot occur.extend optimization case role variables first definepolarity role variable concept axiom template.Definition 15 (Role Polarity). Let ?x VR role variable, C, C1 , C2 , concepttemplates, r role, n IN0 . define polarity ?x C follows: ?x occurspositively ?x.D, ?x .D, > n ?x.D, > n ?x .D, = n ?x.D, = n ?x .D; ?xoccurs negatively ?x.D, ?x .D, 6 n ?x.D, 6 n ?x .D, = n ?x.D, = n ?x .D.Furthermore, ?x occurs positively (negatively)?x occurs negatively (positively) D,C1 C2 C1 C2 ?x occurs positively (negatively) C1 C2 ,r.D, ?x.D, ?x .D, > n r.D, > n ?x.D, > n ?x .D, r.D, ?x.D, ?x .D?x occurs positively (negatively) D,6 n r.D, 6 n ?x.D, 6 n ?x .D ?x occurs negatively (positively) D,= n r.D ?x occurs D.say ?x occurs positively (negatively) C1 C2 ?x occurs negatively(positively) C1 positively (negatively) C2 . define partial function polrmaps role variable ?x concept template C (axiom template form C1 C2 )pos ?x occurs positively C (C1 C2 ) neg ?x occurs negativelyC (C1 C2 ).Note also make assumption occurrences ?x firstpart definition.show, hierarchy optimization also applicable role variables, provided occur positively negatively.Theorem 2. Let ontology, r, role names |= r s, C1 , C2 concepttemplates, C1 C2 axiom template, C = C1 C2 , ?x VR role variable occurringC mapping covers variables C apart ?x.1. polr (?x, C) = pos holds 6|= (C1 C2 )(?x)=s , 6|= (C1 C2 )(?x)=r .2. polr (?x, C) = neg holds 6|= (C1 C2 )(?x)=r , 6|= (C1 C2 )(?x)=s .279fiKollia & GlimmProof. Due Lemma 1, suffices show model = (I , )element following (which formalized contrapositive form):1. polr (?x, C) = pos holds (C(?x)=r )I , (C(?x)=s )I .2. polr (?x, C) = neg holds (C(?x)=s )I , (C(?x)=r )I .prove claim induction structure concept template C:C = ?x.D, concept template contain ?x.polr (?x, C) = pos. Assume, ((?x.D)(?x)=r )I , is, (r.(D))I .h, r (D)I . Since |= r s, alsoh, sI and, therefore, (s.(D))I = ((?x.D)(?x)=s )I .C = ?x.D, concept template contain ?x.polr (?x, C) = neg. ((?x.D)(?x)=s )I , show ((?x.D)(?x)=r )I .contrary shown, assume ((?x.D)(?x)=r )I , i.e.,(r.(D))I . Hence, h, r (D)I .Since |= r s, also h, sI and, therefore,/ (s.(D))I =((?x.D)(?x)=s ) , contradiction.C = > n ?x.D concept template contain ?x.polr (?x, C) = pos. Assume, ((> n ?x.D)(?x)=r )I , (> n r.(D))Ileast n distinct r-successors instances (D). Since |= rr-successors also s-successors and, therefore, (> n s.(D))I = ((>n ?x.D)(?x)=s )I .C = 6 n ?x.D C concept template contain ?x.polr (?x, C) = neg. ((6 n ?x.D)(?x)=s )I , show((6 n ?x.D)(?x)=r )I . contrary shown, assume ((6n ?x.D)(?x)=r )I , i.e., (> n + 1 r.(D))I . Hence, least n + 1 distinctr-successors, instances (D). Since |= r s, r-successorsalso s-successors ((> n + 1 s.(D)))I = ((> n + 1 ?x.D)(?x)=s )I ,contradiction.C = C1 C2 polr (?x, C) = pos, ((C1 C2 )(?x)=r )I , (C1 (?x)=r )I(C2 (?x)=r )I . Since |= r induction hypothesis,(C1(?x)=s )I (C2(?x)=s )I . Thus, ((C1 C2 )(?x)=s )I . proofanalogous polr (?x, C) = neg.proof C1 C2 analogous one C1 C2 .C = polr (?x, C) = pos, (D(?x)=r )I , show(D(?x)=s )I . Note polr (?x, D) = neg. contrary shown,assume (D(?x)=s )I . Since |= r induction hypothesis(D(?x)=r )I (D(?x)=r )I contradiction. proof analogouspolr (?x, C) = neg.280fiOptimizing SPARQL Query Answering OWL OntologiesC = p.D polr (?x, C) = pos, also polr (?x, D) = pos. Now,((p.D)(?x)=r )I , least one p-successor instance D(?x)=r .Since |= r induction hypothesis, p-successor instanceD(?x)=s . Hence, ((p.D)(?x)=s )I . proof analogous polr (?x, C) = neg.C = ?x.D polr (?x, C) = pos, also polr (?x, D) = pos. Note?x occurs since otherwise case handled already above. Now,((?x.D)(?x)=r )I , least one r-successor instanceD(?x)=r . Since |= r induction hypothesis, least one s-successorinstance D(?x)=s . Hence, ((?x.D)(?x)=s )I .C = p.D polr (?x, C) = pos, also polr (?x, D) = pos. Now,((p.D)(?x)=r )I , (p.(D)(?x)=r )I p-successor instanceD(?x)=r . Since |= r induction hypothesis, p-successors alsoinstances D(?x)=s . Hence, (p.(D(?x)=s ))I = ((p.D)(?x)=s )I . proofanalogous polr (?x, C) = neg.C = ?x.D polr (?x, C) = neg, also polr (?x, D) = neg. Note?x occurs since otherwise case handled already above. Now,((?x.D)(?x)=s )I , show ((?x.D)(?x)=r )I . contraryshown, assume/ ((?x.D)(?x)=r )I , i.e., (r.(D)(?x)=r )I .Hence, h, r ((D)(?x)=r )I . Since|= r s, also s-successor and, induction hypothesis,((D)(?x)=s )I contradiction.C = > n p.D polr (?x, C) = pos, (( > n p.D)(?x)=r )I ,least n distinct p-successors instances D(?x)=r . Since |= rinduction hypothesis, p-successors also instances D(?x)=s . Hence,(( > n p.D)(?x)=s )I . proof analogous polr (?x, C) = negC = > n ?x.D polr (?x, C) = pos, also polr (?x, D) = pos. Note?x occurs since otherwise case handled already above. Now,(( > n ?x.D)(?x)=r )I , least n distinct r-successorsinstances D(?x)=r . Since |= r induction hypothesis, least ndistinct s-successors instances D(?x)=s . Hence, (( > n ?x.D)(?x)=s )I .C = 6 n p.D polr (?x, C) = pos, ((6 n p.D)(?x)=r )I , show((6 n p.D)(?x)=s )I . Note polr (?x, D) = neg. contraryshown, assume ((6 n p.D)(?x)=s )I , i.e., ((> n + 1 p.D)(?x)=s )I .Hence, least n + 1 distinct p-successors instances D(?x)=s .Since polr (?x, D) = neg induction hypothesis, D(?x)=s instancesalso D(?x)=r instances (> n + 1 p.(D)(?x)=r )I = ((> n + 1 p.D)(?x)=r )I ,contradiction. proof analogous polr (?x, C) = neg.C = 6 n ?x.D polr (?x, C) = neg, polr (?x, D) = pos. Note?x occurs since otherwise case handled already above. ((6n ?x.D)(?x)=s )I show ((6 n ?x.D)(?x)=r )I . contrary281fiKollia & GlimmAlgorithm 2 getPossibleMappings(O, ?x, at, )Input: O: queried SROIQ ontology?x: concept role variableat: axiom template ?x occurs: mapping ?x dom()Output: set mappings1: :=2: ?x VC3:polc (?x, at) = pos4::= { | (?x) = A, direct subconcept (?x) O,(?y) = (?y) ?y dom() \ {?x}}5:else6::= { | (?x) = A, direct superconcept (?x) O,(?y) = (?y) ?y dom() \ {?x}}7:end8: else9:polr (?x, at) = pos10::= { | (?x) = r, r direct subrole (?x) O,(?y) = (?y) ?y dom() \ {?x}}11:else12::= { | (?x) = r, r direct superrole (?x) O,(?y) = (?y) ?y dom() \ {?x}}13:end14: end15: returnshown, assume ((6 n ?x.D)(?x)=r )I , i.e., ((>n + 1 ?x.D)(?x)=r )I . Hence, least n + 1 distinct r-successors instances D(?x)=r . Since |= r s, induction hypothesis, r-successorsalso s-successors instances D(?x)=s . Hence, ((> n + 1 ?x.D)(?x)=s )I((6 n ?x.D)(?x)=s )I , contradiction.C = (= n ?x.D) C = (= n r.D), polarity ?x C always positivenegative, polr (?x, C) undefined case cannot occur.cases ?x occurring form inverse (?x ) analogous, given|= r iff |= r .Algorithm 2, explain detail Section 6.3, shows usetheorems create possible concept role mappings concept role variable ?xappears positively negatively axiom template C1 C2 .6.3 Query Answering AlgorithmAlgorithm 3 shows optimized way evaluating queries using static ordering. First,axiom templates simplified possible (method rewrite line 1). Next, method282fiOptimizing SPARQL Query Answering OWL OntologiesAlgorithm 3 evaluate(O, q)Input: O: queried SROIQ ontologyq: queryOutput: set solutions evaluating q1: := rewrite(q)2: At1 , . . . , Atm :=connectedComponents(At)3: j=1, . . . ,4:Rj := {0 | dom(0 ) = }5:at1 , . . . , atn := order(Atj )6:= 1, . . . , n7:R :=8:Rj9:isSimple(ati ) Vars(ati ) \ dom() 6=10:R := R { | callSpecificReasonerTask((ati ))}11:else Vars(ati ) \ dom() =12:|= (ati )13:R := R {}14:end15:else16:Vopt := {?x |?x 6 dom(), Theorem 1 2 applies ?x ati }17:B := initializeVariableMappings(O, ati , , Vopt )18:B 6=19::= removeMapping(B)20:|= (ati )21:R := R { | (?x) = (?x) ?x/ Vopt(?x) = C ?x Vopt VC , |= C (?x)(?x) = r ?x Vopt VR , |= r (?x)}22:?x Vopt23:B := B getPossibleMappings(O, ?x, ati , )24:end25:end26:end27:end28:end29:Rj := R30:end31: end32: Rans := {1 . . . | j Rj , 1 j m}33: return RansconnectedComponents (line 2) partitions axiom templates sets connected components, i.e., within component templates share common variables, whereascomponents shared variables. Unconnected components unnecessarily increaseamount intermediate results and, instead, one simply combine results283fiKollia & GlimmAlgorithm 4 initializeVariableMappings(O, at, , Vopt )Input: O: queried SROIQ ontologyat: axiom template: partial mappingVopt : variables Theorem 1 2 appliesOutput: set mappings1: := {}2: ?x Vars(at) \ dom()3:R :=4:?x VC ?x Vopt5:6:polc (?x, at) = pos7:(?x) :=8:else9:(?x) :=10:end11:R := R { }12:end13:else ?x VR ?x Vopt14:15:polr (?x, at) = pos16:(?x) := r17:else18:(?x) := r19:end20:R := R { }21:end22:else23:R := { | (?x) = a, NCO NRO NIO (?y) = 1 (?y)1 ?y dom(1 )}24:end25::= R26: end27: returncomponents end (line 32). component, proceed described below:first determine order (method order line 5) described Section 5. simple axiom template, contains far unbound variables, call specialized reasoner methodretrieve entailed results, i.e., mappings unbound variables (callSpecificReasonerTaskline 10). Note mappings assign values variables coveredalready computed (partial) solution since instantiate atom ati .allows defining union setting ( )(v) = (v) v dom(),( )(v) = (v) otherwise. templates variables bound, checkwhether mappings lead entailed axioms (lines 11 14). cases, i.e.,284fiOptimizing SPARQL Query Answering OWL Ontologiescomplex axiom templates unbound variables, check compatible mappingsyield entailed axiom (lines 15 27). particular, first initialize set B candidate mappings unbound variables axiom template (line 17, refersAlgorithm 4). Algorithm 4 initializes unbound variables axiom templatesTheorem 1 2 applies (r ) (r ) depending whether respective polarityfunction returns pos neg. template variables optimization applicable, compatible mappings returned. method removeMapping (line 19) returnsmapping B deletes mapping B. instantiate axiom template check entailment. case entailment holds, first extend set Rcurrent mapping mappings map optimization variables equivalentconcepts roles respective variable mappings (line 21) afterwards extend set B possible mappings variables hierarchy optimizationapplicable (getPossibleMappings line 23). example, checked mappingmaps concept variable ?x concept ?x occurs positively axiomtemplate, add set B mappings map ?x direct subconcept3(see Algorithm 2 line 4). implementation use involved procedure, i.e.,order avoid checking entailment instantiated axiom templatemapping, case concept (role) hierarchy traversalperform, keep track already processed mappings checkchecked previous iteration loop (lines 18 26). easepresentation, shown Algorithm 3. repeat procedure Bempty (lines 18 26).dynamic ordering, Algorithm 3 changed follows: first computenumber axiom templates Atj ; n := |Atj |. swap line 5 line 6, i.e.,instead ordering axiom templates loop evaluates axiom templates,order within loop. function order gets additional input parameterset currently computed solutions returns next cheapest axiom templateaccording dynamic ordering function. Hence, ati := order(Atj , Rj ) insteadat1 , . . . , atn := order(Atj ). insert line calling order removecheapest axiom template current component: Atj := Atj \ {ati }. result,next iteration loop compute cheapest axiom template amongstyet evaluated templates until, last iteration, one axiom template left.Algorithm 3 sound complete. soundness completeness algorithmbased following facts:method rewrite (see Definition 13) affect answers query q, sincerewrites axiom templates templates set answers.method connectedComponents affect answers q; splitsquery several components evaluated separately takecartesian product answers.method order change query way; reorders axiomtemplates.3. say concept name direct subconcept concept name B w.r.t. O, |= Bconcept name |= B, |= 6|= . similarway define direct superconcept, direct subrole direct superrole.285fiKollia & Glimmactual axiom template evaluation, iterate templatesquery taking account mappings already computedevaluation previous templates distinguish three cases:1. axiom template simple one contains unbound variables. usespecialized reasoner tasks compute entailed mappings since usesound complete reasoner result indeed sound complete.2. axiom template contain unbound variables. case, simplycheck entailment using sound complete reasoner.3. axiom template complex template least one variable unbound.variables optimization Section 6.2 applicable, initializevariables /r (/r ) traverse concept/role hierarchy topdown (bottom-up). prune mappings according Theorems 1 2 casechecked mapping constitute solution mapping. case,extend set possible mappings B. variables axiom templateshierarchy optimization applicable, check compatiblemappings. Thus, due Theorem 1 2 procedure sound complete.Although algorithm implemented HermiT reasoner, one compute answers query using (hyper)tableau reasoner.7. Evaluationtested developed optimizations standard benchmarks range customqueries test complex axiom template evaluation expressive ontologies.experiments performed Mac OS X Lion machine 2.53 GHz Intel Core i7processor Java 1.6 allowing 1GB Java heap space. measure time one-offtasks classification separately since tasks usually performedsystem accepts queries. ontologies code required perform experimentsavailable online (Kollia & Glimm, 2013). developed system (Glimm & Kollia,2013), called OWL-BGP, implemented SPARQL Wrapper usedreasoner implements OWLReasoner interface OWL API (Horridge &Bechhofer, 2009). Section 7.1 compare different ordering strategiesdeveloped two benchmarks (LUBM UOBM) contain queries variablesplace individuals (query atoms). also show effect ordering LUBM usingcustom queries simple axiom templates created SPARQL-DL (Kremen &Sirin, 2008). Section 7.2 show effect proposed optimizations queriescomplex axiom templates. evaluation used HermiT hypertableaureasoner (Motik, Shearer, Glimm, Stoilos, & Horrocks, 2013). reasonersPellet (Clark & Parsia, 2013a) Racer Pro (Racer Systems GmbH & Co. KG, 2013) couldequally well used implementation long provide interfacerequired statistics, i.e., number known possible instances concepts rolescomputation cost functions used query ordering. Without optimizations,providing interface statistics easily realized described currentpaper. presented query ordering techniques also used optimizations286fiOptimizing SPARQL Query Answering OWL Ontologiescaching, pseudo model merging techniques, binary instance retrieval, absorptionemployed. cost functions might, however, require adaptation take reductionrequired number consistency checks account. example, Pellet uses binaryinstance retrieval, testing possible instances concept realized splittingcandidate instances two partitions. partition, single consistency checkperformed. consistency check successful, safe consider individualsbelonging partition non-instances tested concept A. Otherwise,split partition process resulting partitions way. case, oneperforms one consistency check potentially determine several (non-)instances A,reflected cost functions.also worth noting TrOWL reasoning framework (Thomas, Pan, & Ren,2013) started use SPARQL wrapper provide SPARQL support. adaptationalso provide statistics is, best knowledge, still outstanding, althoughstraightforward. TrOWL based two approximate reasoners: one underapproximates (computation concept role instances sound, incomplete) (Ren, Pan, &Zhao, 2010) one overapproximates (computation concept role instancescomplete, unsound) (Pan, Thomas, & Zhao, 2009). setting, underapproximation straightforwardly seen known instances overapproximationminus underapproximation possible instances.7.1 Query Orderingtested ordering techniques Lehigh University Benchmark (LUBM) (Guo,Pan, & Heflin, 2005) case disjunctive information presentexpressive University Ontology Benchmark (UOBM) (Ma, Yang, Qiu, Xie, Pan, &Liu, 2006).first used 14 conjunctive ABox queries provided LUBM. these, queries2, 7, 8, 9 interesting ones setting since contain many atomsordering effect running time. tested queries LUBM(1,0)LUBM(2,0) contain data one two universities respectively, starting index0. LUBM(1,0) contains 17,174 individuals LUBM(2,0) contains 38,334 individuals.LUBM(1,0) took 19 load 0.092 classification initialization knownpossible instances concepts roles. clustering approach concepts took 1resulted 16 clusters. clustering approach roles lasted 4.9 resulted17 role successor clusters, 29 role predecessor clusters 87 role clusters. LUBM(2,0)took 48.5 load 0.136 classification initialization known possibleinstances. clustering approach concepts took 3.4 resulted 16 clusters.clustering approach roles lasted 16.3 resulted 17 role successor clusters, 31 rolepredecessor clusters 102 role clusters. Table 2 shows execution timefour queries LUBM(1,0) LUBM(2,0) four cases: i) use staticalgorithm (columns 2 6), ii) use dynamic algorithm (columns 3 7), iii)use random sampling, i.e., taking half individuals returned (fromevaluation previous query atoms) run, decide next cheapest atomevaluated dynamic case iv) using proposed sampling approachbased clusters constructed individuals queried ontology (columns 4287fiKollia & GlimmQ2789Static51254851,099LUBM(1,0)Dynamic RSampling119390298526446392,9353,021CSampling3720551769Static162706226,108LUBM(2,0)Dynamic RSampling4421,036772,73386663123,20214,362CSampling153646603,018Table 2: Query answering times milliseconds LUBM(1,0) LUBM(2,0) using i)static algorithm ii) dynamic algorithm, iii) 50% random sampling (RSampling),iv) constructed individual clusters sampling (CSampling)QPlansNo27893361456336Chosen Plan OrderStatic Dynamic Sampling211111111173160150Pellet PlanWorst Plan51254951,2354,9307,5191,7825,388Table 3: Statistics constructed plans chosen orderings running timesmilliseconds orderings chosen Pellet worst constructed plans8). queries marked (*) queries static dynamic algorithmsresult different ordering. Queries 7 8 observe increase runningtime dynamic technique used (in comparison static) especiallyevident Query 8 LUBM(2,0), number individuals ontologyintermediate result sizes larger. Dynamic ordering also behaves worse staticQueries 2 9. happens because, although dynamic algorithm choosesbetter ordering static algorithm, intermediate results (that need checkediteration determine next query atom executed) quite largehence cost iterating possible mappings dynamic case far outweighsbetter ordering obtained. also observe random sampling collectingordering statistics dynamic case (checking 50% individuals i1 randomlydetecting next query atom executed) leads much worse resultsqueries plain static dynamic ordering. happens since random sampling oftenleads choice worse execution order. use cluster based sampling methodperforms better plain dynamic algorithm queries. Queries 2 9,gain better ordering dynamic algorithm sampling usedmuch evident. case since use one individual every clustercost functions computation number clusters much smallernumber otherwise tested individuals run.order show effectiveness proposed cost functions comparedrunning times valid plans (plans comply connectedness conditionDefinition 10, i.e., plans consecutive atoms share least one common variable)288fiOptimizing SPARQL Query Answering OWL OntologiesLUBM(3,0) LUBM(4,0) LUBM(5,0) LUBM(6,0) LUBM(7,0) LUBM(8,0) LUBM(9,0)55,66478,579102,368118,500144,612163,552183,425Table 4: Number individuals LUBM increasing number universitiesrunning time plan chosen method. following show resultsLUBM(1, 0), results LUBM(2,0) comparable. Table 3 show,query, number valid plans constructed according Definition 10(column 2), order plan chosen static, dynamic, cluster based samplingmethods order valid plans execution time (columns 3,4,5; e.g., value2 indicates ordering method chose second best plan), running timeHermiT plan created Pellet (column 6) well running timeworst constructed plan (column 7).comparison ordering approach approach followed reasonerssupport conjunctive query answering Pellet Racer Pro straightforward. case Pellet Racer many optimizations instanceretrieval (Sirin et al., 2007; Haarslev & Moller, 2008), HermiT have. Thus,comparison execution times reasoners HermiT would conveymuch information effectiveness proposed query ordering techniques.idea comparing orderings computed reasoners computedmethods also informative since orderings chosen different reasonersdepend much way queries evaluated costs specific tasksreasoners and, hence, reasoner dependent, i.e., ordering good onereasoner leads efficient evaluation may good another reasoner.note searching orderings according Pellet, switchedsimplification optimization Pellet implements regarding exploitation domain range axioms queried ontology reducing number query atomsevaluated (Sirin & Parsia, 2006). done order better evaluatedifference plain ordering obtained Pellet HermiT since cost functions takeaccount query atoms.observe queries apart Query 9 orderings chosen algorithms(near)optimal ones. Queries 2 7, Pellet chooses orderingalgorithms. Query 8, Pellet chooses ordering which, evaluated HermiT,results higher execution time. Query 9, algorithms choose plansmiddle order valid plans w.r.t. query execution time, meansalgorithms perform well query. greedy techniquesused find execution plan take account local informationchoose next query atom executed. Interestingly, use cluster based samplingled finding better ordering, see running time Table 2better ordering plan found cluster based sampling techniques comparedstatic plain dynamic ordering (Table 3). ordering chosen Pellet Query 9also perform well. see that, queries, worst running times manyorders magnitude greater running times achieved ordering algorithms.general, observe LUBM static techniques adequate use dynamicordering improve execution time much compared static ordering.289fiKollia & GlimmQ LUBM(3,0) LUBM(4,0) LUBM(5,0) LUBM(6,0) LUBM(7,0) LUBM(8,0) LUBM(9,0)20.350.621.261.712.263.114.1870.110.160.230.320.330.330.4080.770.911.271.291.341.441.65918.4942.9885.54116.88181.07235.06312.7120.6455.1690.99138.84213.59241.85323.15Table 5: Query answering times seconds LUBM increasing number universitiesQ Static Dynamic CSampling PlansNo4 13.359 186.30110.98120.0114 94.61q1 191.07q2 47.0413.40188.580.840.0190.6098.2422.2013.41185.401.670.0193.40100.2522.511483041466Chosen Plan OrderPelletStatic Dynamic SamplingPlan11113.40111636.911110.981110.01211 > 30 min211 > 30 min21122.2WorstPlan271.56636.91> 30 min> 30 min> 30 min> 30 min> 30 minTable 6: Query answering times seconds UOBM (1 university, 3 departments)statisticsorder show scalability system, next run LUBM queriesdifferent numbers universities, i.e., LUBM(i,0) ranges 3 9. Table 4 showsnumber individuals appearing ABox different university size. runningtimes Queries 2, 7, 8, 9 well running time 14 LUBM queriesshown Table 5. results LUBM(1,0) LUBM(2,0) shown Table 2. Noteresults shown case static ordering performed. tablesee queries, running time increases number individualsABox increases, reasonable. observe query answering ontologiesstill scalable query answering databases so,expressive schema taken account fact incompleteinformation contrast databases complete information.Unlike LUBM, UOBM ontology contains disjunctions reasoner makes alsonondeterministic derivations. order reduce reasoning time, removed nominals used first three departments containing 6,409 individuals. resultingontology took 16 load 0.1 classify initialize known possible instances. clustering approach concepts took 1.6 resulted 356 clusters.clustering approach roles lasted 6.3 resulted 451 role successor clusters, 390role predecessor clusters 4,270 role clusters. present results static dynamic algorithms Queries 4, 9, 11, 12 14 provided UOBM,interesting ones consist many atoms. queries contain oneatom possible instances. see Table 6, static dynamic ordering show290fiOptimizing SPARQL Query Answering OWL Ontologiessimilar performance Queries 4, 9, 11 12. Since available statistics casequite accurate, methods find optimal plans intermediate result set sizessmall. ordering methods, atoms possible instances queriesexecuted last. Query 14, dynamic algorithm finds better ordering resultscomparable performance. effect cluster based sampling techniquerunning time obvious case LUBM. happens currentexperiment intermediate result sizes large and, importantly,gain obtained due sampling order milliseconds whereas total queryanswering times order seconds obscuring small improvement runningtime due sampling. queries orderings created Pellet resultworse running times orderings created algorithms.order illustrate dynamic ordering performs better static, also createdtwo custom queries:q1 = { isAdvisedBy(?x,?y), GraduateStudent(?x), Woman(?y) }q2 = { SportsFan(?x), GraduateStudent(?x), Woman(?x) }queries, P [GraduateStudent], P [Woman] P [isAdvisedBy] non-empty, i.e.,query concepts roles possible instances. running times dynamicordering smaller since accurate statistics result smaller number possibleinstances checked query execution. particular, staticordering, 151 41 possible instances checked query q1 q2 , respectively,compared 77 23 dynamic ordering. Moreover, intermediate resultsgenerally smaller dynamic ordering static leading significant reductionrunning time queries. Interestingly, query q2 could answered withintime limit 30 minutes transformed three query concepts conjunction,i.e., asked instances intersection three concepts.complex concepts reasoner longer use information known possibleinstances falls back naive way computing concept instances. Again,reasons before, sampling techniques apparent effect runningtime queries.query SPARQL-DL tests issued LUBM(1,0) (Kremen & Sirin, 2008)(cf. Table 7), Table 8 shows running time plan chosen method (column2), number valid plans, i.e., plans comply connectedness conditionDefinition 10 (column 3), order chosen plan order valid plansexecution times (column 4), running time HermiT plan createdPellet (column 5) well running time worst constructed plan (column 6).queries shown Table 7 ordered according static ordering algorithm. Sincereasoning LUBM deterministic, use static planning order axiom templates.Dynamic planning improve execution times (actually makes worse)since, explained before, deterministic reasoningimportant information ordering beginning overhead caused dynamicordering results worse query execution time.results Table 8 one observe Queries 1, 2, 3, 4 8proposed ordering chooses optimal plan among valid plans. Queries 5, 6, 7, 910 optimal plan chosen according proposed cost estimation algorithm.Queries 5, 7, 9 10 due greedy techniques used finding291fiKollia & GlimmGraduateStudent(?x)?y(?x, ?z)Course(?w)1GraduateStudent(?x)?y(?x, ?w)?z(?w)GraduateCourse ?z7?c?c(?x)teachingAssistantOf(?x, ?y)takesCourse(?x, ?y)8?c Person?c(?x)advisor(?x, ?y)9?c Person?c(?x)teachingAssistantOf(?x, ?y)Course(?y)10?p worksFor?p(?y, ?w)?c(?y)?c Facultyadvisor(?x, ?y)GraduateStudent(?x)memberOf(?x, ?w)6?c Employee?c(?x)Student(?x)undergraduateDegreeFrom(?x, ?y)3?y memberOf?y(?x, University0)Person(?x)4?y(GraduateStudent5, ?w)?z(?w)?z Course2?z Course?z(?w)?y(?x, ?w)GraduateStudent(?x)5Table 7: Queries used SPARQL-DL testsQuery12345678910Chosen OrderingTime0.360.030.050.0126.1010.490.420.230.190.80PlansNo21444881448812Chosen PlanOrder11115261421Pellet PlanTime0.360.375.440.010.9510.492.680.230.190.80Worst PlanTime0.580.615.4511.46454.25499.652.680.800.47992.77Table 8: Query answering times seconds queries Table 7 LUBM(1,0)statisticsiteration ordering algorithm next cheapest axiom template evaluated.example, optimal plan Query 10 starts template GraduateStudent(?x),cheapest one according cost based technique then, moving292fiOptimizing SPARQL Query Answering OWL Ontologiesconnected templates, different order chosen order chosen algorithm.turns valid plans beginning atom GraduateStudent(?x) lead betterexecution times plan chosen algorithm resulting existence severalbetter plans chosen one.Query 6 find optimal plan overestimated costdisjoint axiom template hence missed optimal ordering. Nevertheless,chosen plans lead execution times queries three orders magnitudelower worst plans chosen. queries proposedordering lead optimal plan, one additionally take accounttime saved computing costs |q!| possible orderings,high. Apart Queries 4, 6 8, observe plans produced Pelletoptimal evaluated HermiT. discussed before, happensstatistics created ordering reasoner specific hence good orderingone reasoner may good another reasoner.7.2 Complex Axiom Template Optimizationsabsence suitable standard benchmarks arbitrary SPARQL queries, createdcustom set queries shown Tables 10 12 GALEN FBbt XPontology, respectively. Systems fully support SPARQL Direct Semantics entailmentregime still development, makes hard compare resultskinds queries systems.GALEN biomedical ontology. expressivity (Horn-)SHIF consists2,748 concepts 413 abstract roles. FBbt XP ontology taken OpenBiological Ontologies (OBO) Foundry (OBO Foundry, 2013). falls SHI fragmentSROIQ consists 7,221 concepts 21 abstract roles. considerTBox part FBbt XP since ABox relevant showing different effectsproposed optimizations execution times considered queries. GALEN took3.7 load 11.1 classify (concepts roles), FBbt XP took 1.5 load7.4 classify.execution times queries Tables 10 12 shown right-handside Tables 9 11, respectively. set time limit 30 minutesquery. query, tested execution without optimizationscombination applicable optimizations Sections 5 6. Tables 9 11,one also see number consistency checks performed evaluationquery combination applicable optimizations well numberresults query. tables taken time worst orderingquery atoms cases ordering optimization applicable enabled.Note complex axiom templates require consistency checks evaluated;simple ones (subsumption axiom templates case) need cache lookupsreasoners internal structures since concepts roles already classified.GALEN Queries: expected, increase number variables within axiomtemplate leads significant increase query execution time numbermappings checked grows exponentially number variables. can,particular, observed difference execution time Query 1 2.293fiKollia & GlimmQuery11223334444555ReorderingHierarchyExploitationRewritingxxxxxxxxxxxxxConsistencyChecks2,750501,141,2501,291xx19,2503,073xxx16,1351971,8831,883xTimeAnswersNo1.680.18578.989.85>30 min102.372.69> 30 min> 30 min7.681.12> 30 min0.670.810102142142,8162,81651514,3924,392Table 9: Query answering times seconds queries Table 10 withoutoptimizations1Infection hasCausalLinkTo.?x2Infection ?y.?x3?x Infection hasCausalAgent.?y4 NAMEDLigament NAMEDInternalBodyPart ?x?x hasShapeAnalagousTo?y ?z.linear5?x NonNormalCondition?z ModifierAttributeBacterium ?z.?w?y StatusAttribute?w AbstractStatus?x ?y.StatusTable 10: Sample complex queries GALEN ontologytwo queries, evident use hierarchy exploitation optimizationleads decrease execution time two orders magnitude. Query 3completed time limit least query rewriting optimization enabled.get improvement three orders magnitude query, using rewritingcombination hierarchy exploitation. Query 4 completed giventime limit least reordering rewriting enabled. Rewriting splits first axiomtemplate following two simple axiom templates, evaluated muchefficiently:NAMEDLigament NAMEDInternalBodyPart294NAMEDLigament ?xfiOptimizing SPARQL Query Answering OWL Ontologiesrewriting, ordering optimization even pronounced effect sincerewritten axiom templates evaluated simple cache lookup. Withoutordering, complex axiom template could executed simple ones,leads inability answering query within time limit 30 min. Withoutgood ordering, Query 5 also answered within time limit. orderingchosen algorithm shown below. Note query consists two connectedcomponents: one axioms containing ?z ?w another one axiomscontaining ?x ?y.?z ModifierAttribute?w AbstractStatusBacterium ?z.?w?y StatusAttribute?x NonNormalCondition?x ?y.Statusquery, hierarchy exploitation optimization improve execution timesince, due chosen ordering, variables hierarchy optimizationapplied, already bound comes evaluation complex templates.Hence, running times without hierarchy exploitation similar.number consistency checks significantly lower number answersoverall results computed taking cartesian products results twoconnected components. Interestingly, queries complex axiom templates,make sense require next axiom template evaluate shares variablepreviously evaluated axiom templates, case simple axiom templates.example, would require that, first connected component query wouldexecuted following order:?z ModifierAttributeBacterium ?z.?w?w AbstractStatusresults 294,250 instead 1,498 consistency checks since longer use cheapcache look-up check determine bindings ?w, first iterate possible?w bindings check entailment complex axiom template reducecomputed candidates processing last axiom template.Although optimizations significantly improve query execution time,required time still quite high. practice, is, therefore, advisable add manyrestrictive axiom templates (axiom templates require cache lookups) queryvariables possible. example, addition ?y Shape Query 4 reducesruntime 1.12 0.65 s. observe, expected, execution timequery applicable optimization analogous number consistency checksperformed evaluation query.FBbt XP Queries: Queries 1, 2, 3, 5 6, ordering optimizationapplicable, observe decrease execution time two orders magnitude295fiKollia & GlimmQueryReordering1112223334455556666xxxxxxHierarchyExploitationRewritingxx11,26214,446xx12,63772,230xx54,186166,1291335166,12921,6699073xxxxxxxxxxxConsistencyChecks151,683xxx43,33832,490TimeAnswersNo44.13> 30 min5.6437.38> 30 min39.20357.59> 30 min252.41486.8117.03457.8419.6811.740.01> 30 min183.66> 30 min152.387,2437,2437,2247,2241881886868000043,33843,338Table 11: Query answering times seconds queries Table 12 withoutoptimizations1 ?x part of.?y?x FBbt 000057892 ?y part?x ?y.FBbt 000016063 ?x ?y.FBbt 00025990?y overlaps4 FBbt 00001606 ?y.?x5 FBbt 00001606 ?y.?x?y develops6?y FBbt 00001884?p part?x ?p.?y ?wTable 12: Sample complex queries FBbt XP ontologyordering optimization used. ordering optimization important answeringQueries 1, 2 3 within time limit. queries, additional use hierarchyexploitation optimization leads improvement three orders magnitude.observe queries effect hierarchy exploitation profoundothers. precisely, smaller ratio result size numberconsistency checks without hierarchy optimization, pronounced effectenabling optimization. words, tested mappings indeedsolutions, one prune fewer parts hierarchy since pruning performedfind non-solution. Query 2, even observe slight increase running296fiOptimizing SPARQL Query Answering OWL Ontologiestime hierarchy optimization used. optimizationprune candidate mappings, outweigh overhead caused maintaininginformation hierarchy parts already tested. Query 6, rewritingoptimization important answer query within time limit. optimizationsenabled, number consistency checks less result size (32,490 versus43,338) since complex axiom template requires consistency checks.8. Related Workyet standardized commonly implemented query language OWL ontologies. Several widely deployed systems support, however, query language.Pellet supports SPARQL-DL (Sirin & Parsia, 2007), subset SPARQL, adaptedwork OWLs Direct Semantics. kinds SPARQL queries supportedSPARQL-DL directly mapped reasoner tasks. Therefore, SPARQLDL understood queries use simple axiom templates terminology.Similarly, KAON2 (Hustadt, Motik, & Sattler, 2004) supports SPARQL queries, restricted ABox queries/conjunctive instance queries. best knowledge,publications describe ordering strategies KAON2. Racer Pro (Haarslev& Moller, 2001) proprietary query language, called nRQL (Haarslev et al., 2004),allows queries go beyond ABox queries, e.g., one retrieve sub- superconcepts given concept. TrOWL (Thomas et al., 2013) another system supportsSPARQL queries, reasoning TrOWL approximate, i.e., OWL DL ontologyrewritten ontology uses less expressive language reasoning applied (Thomas, Pan, & Ren, 2010). TrOWL based SPARQL framework presentedhere, instead using HermiT background reasoner, uses approximate reasoners OWL 2 EL OWL 2 QL profiles. Furthermore, systemsQuOnto (Acciarri, Calvanese, De Giacomo, Lembo, Lenzerini, Palmieri, & Rosati, 2013)Requiem (Perez-Urbina, Motik, & Horrocks, 2013), support profiles OWL 2,support conjunctive queries, e.g., written SPARQL syntax, proper nondistinguished variables. systems support OWL 2 DL, Pellet supportsnon-distinguished variables long used cycles, since decidability cyclicconjunctive queries best knowledge still open problem.problem finding good orderings templates query issued ontology already preliminarily studied (Sirin & Parsia, 2006; Kremen & Sirin, 2008;Haarslev & Moller, 2008). Similarly work, Sirin Parsia well KremenSirin exploit reasoning techniques information provided reasoner models createstatistics cost result size axiom template evaluations within executionplans. difference use cached models cheaply finding obvious conceptrole (non-)instances, whereas case cache model model parts.Instead process pre-model constructed initial ontology consistency checkextract known possible instances concepts roles it. subsequentlyuse information create update query atom statistics. Moreover, SirinParsia Kremen Sirin compare costs complete execution plans heuristically reducing huge number possible complete plans choose onepromising beginning query execution. different cheap297fiKollia & Glimmgreedy algorithm finds, iteration, next promising axiom template.experimental study shows equally effective investigation possible execution orders. Moreover, work additionally used dynamic orderingcombined clustering techniques, apart static ones, showntechniques lead better performance particularly ontologies contain disjunctionsallow purely deterministic reasoning.Haarslev Moller discuss means example ordering criteria usefind efficient query execution plans Racer Pro. particular, use traditionaldatabase cost based optimization techniques, means take accountcardinality concept role atoms decide promising ordering.previously discussed, inadequate especially ontologies disjunctiveinformation.significant amount work estimation cost metrics search optimalorders evaluating joins performed context databases. discussedSection 3, databases, cost formulas defined estimate CPU I/O costs(similar reasoning costs) number returned tuples (similar resultsizes). estimates used find good join orders. System R query optimizer,example, among first works use extended statistics novel dynamic programming algorithm find effective (minimal) join orders query atoms (Selinger, Astrahan,Chamberlin, Lorie, & Price, 1979). heuristic similar (for case conjunctiveinstance queries) used work, according join order permutationsreduced avoiding Cartesian products result sets query atoms. Regarding join order selection, apart dynamic programming, also algorithmic paradigms basedbranch-and-bound simulated annealing have, since then, presented literature.Dynamic ordering also explored literature context adaptive queryprocessing techniques (Gounaris, Paton, Fernandes, & Sakellariou, 2002),proposed overcome problems caused lack necessary statistics, good selectivity estimates, knowledge runtime mappings query compile time.techniques take account changes happen evaluation environment runtimemodify execution plan runtime (i.e., change used operators joinsorder (remaining) query atoms evaluated).9. Conclusionscurrent paper, presented sound complete query answering algorithmnovel optimizations OWL Direct Semantics entailment regime SPARQL 1.1.prototypical query answering system combines existing tools ARQ, OWL API,HermiT OWL reasoner. Apart query ordering optimizationwhich uses(reasoner dependent) statistics provided HermiTthe system independent reasoner used, could employ reasoner supports OWL API.propose two cost-based ordering strategies finding (near-)optimal execution orders conjunctive instance queries. cost formulas based information extractedmodels reasoner (in case HermiT). show experimental studystatic techniques quite adequate ontologies reasoning deterministic.reasoning nondeterministic, however, dynamic techniques often perform better.298fiOptimizing SPARQL Query Answering OWL Ontologiesuse cluster based sampling techniques improve performance dynamicalgorithm intermediate result sizes queries sufficiently large, whereas random sampling beneficial often leads suboptimal query execution plans.presented approach used find answers queries issued SROIQontologies. Since based entailment checking finding answers conjunctiveinstance queries scalable techniques, query rewriting,applied ontologies lower expressivity, DL-Lite. words,trade-off scalability ontology expressivity one needs considerimportant ones application use scalable query answering systemless expressive ontology less scalable system expressive ontology.module ordering based extraction statistics reasoner model,computed off-line. update ontology ABox would cause construction new model scratch consequent recompilation knownpossible instances concepts roles unless incremental reasoner used. incremental reasoner could, example, find modules pre-model affectedupdate recompute model parts. One could also incrementally updatestatistics used ordering. best knowledge, OWL DL reasonerspartially support incremental reasoning considered case currentpaper.queries go beyond conjunctive instance queries provide optimizations rewriting equivalent, simpler queries. Another highly effectivefrequently applicable optimization prunes number candidate solutionschecked exploiting concept role hierarchies. One can, usually, assumehierarchies computed system accepts queries. empirical evaluationshows optimization reduce query evaluation times three ordersmagnitude.Acknowledgmentswork done within Transregional Collaborative Research Centre SFB/TRR62 Companion-Technology Cognitive Technical Systems funded GermanResearch Foundation (DFG).ReferencesAcciarri, A., Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Palmieri, M., &Rosati, R. (2013). QuOnto. Available http://www.dis.uniroma1.it/quonto/.Beckett, D., Berners-Lee, T., Prudhommeaux, E., & Carothers, G. (Eds.). (19 February2013). Turtle Terse RDF Triple Language. W3C Candidate Recommendation.Available http://www.w3.org/TR/turtle/.Brickley, D., & Guha, R. V. (Eds.). (10 February 2004). RDF Vocabulary DescriptionLanguage 1.0: RDF Schema. W3C Recommendation. Available http://www.w3.org/TR/rdf-schema/.299fiKollia & GlimmBroekstra, J., & Kampman, A. (2006). RDF query transformation language.Staab, S., & Stuckenschmidt, H. (Eds.), Semantic Web Peer-to-Peer, pp. 2339.Springer.Calvanese, D., Giacomo, G. D., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractablereasoning efficient query answering description logics: DL-Lite family.Journal Automated Reasoning, 39 (3), 385429.Clark & Parsia (2013a). Pellet. Available http://clarkparsia.com/pellet/.Clark & Parsia (2013b). Stardog. Available http://stardog.com.Glimm, B., Horrocks, I., Motik, B., Shearer, R., & Stoilos, G. (2012). novel approachontology classification. Journal Web Semantics: Science, Services AgentsWorld Wide Web, 14, 84101. Special Issue Dealing MessinessWeb Data.Glimm, B., & Kollia, I. (2013). OWL-BGP. Available http://code.google.com/p/owl-bgp/.Glimm, B., & Krotzsch, M. (2010). SPARQL beyond subgraph matching. Proceedings9th International Semantic Web Conference (ISWC10), Vol. 6414 LectureNotes Computer Science, pp. 241256. Springer.Glimm, B., & Ogbuji, C. (2013). SPARQL 1.1 entailment regimes. W3C Recommendation.Available http://www.w3.org/TR/sparql11-entailment/.Gounaris, A., Paton, N. W., Fernandes, A. A., & Sakellariou, R. (2002). Adaptive queryprocessing: survey. 19th BNCOD, pp. 1125. Springer.Guo, Y., Pan, Z., & Heflin, J. (2005). LUBM: benchmark OWL knowledge basesystems. Journal Web Semantics, 3 (2-3), 158182.Haarslev, V., & Moller, R. (2001). Racer system description. Gor, R., Leitsch, A., & Nipkow, T. (Eds.), Proceedings 1st International Joint Conference AutomatedReasoning (IJCAR01), Vol. 2083 LNCS, pp. 701705. Springer.Haarslev, V., & Moller, R. (2008). scalability description logic instance retrieval.Journal Automated Reasoning, 41 (2), 99142.Haarslev, V., Moller, R., & Wessel, M. (2004). Querying semantic web Racer+ nRQL. Proceedings KI-2004 International Workshop ApplicationsDescription Logics.Harris, S., & Seaborne, A. (Eds.). (2013). SPARQL 1.1 Query Language. W3C Recommendation. Available http://www.w3.org/TR/sparql11-query/.Hayes, P. (Ed.). (10 February 2004). RDF Semantics. W3C Recommendation. Availablehttp://www.w3.org/TR/rdf-mt/.Hitzler, P., Krotzsch, M., & Rudolph, S. (2009). Foundations Semantic Web Technologies.Chapman & Hall/CRC.Horridge, M., & Bechhofer, S. (2009). OWL API: Java API working OWL2 ontologies. Patel-Schneider, P. F., & Hoekstra, R. (Eds.), ProceedingsOWLED 2009 Workshop OWL: Experiences Directions, Vol. 529 CEURWorkshop Proceedings. CEUR-WS.org.300fiOptimizing SPARQL Query Answering OWL OntologiesHorrocks, I., Kutz, O., & Sattler, U. (2006). even irresistible SROIQ. Doherty, P., Mylopoulos, J., & Welty, C. A. (Eds.), Proceedings 10th InternationalConference Principles Knowledge Representation Reasoning (KR06), pp.5767. AAAI Press.Hustadt, U., Motik, B., & Sattler, U. (2004). Reducing SHIQ description logic disjunctive datalog programs. Proceedings 9th International ConferencePrinciples Knowledge Representation Reasoning (KR04), pp. 152162. AAAIPress.Ioannidis, Y. E., & Christodoulakis, S. (1993). Optimal histograms limiting worst-caseerror propagation size join results. ACM Transactions Database Systems,18 (4), 709748.Kazakov, Y. (2008). RIQ SROIQ harder SHOIQ. Brewka, G., & Lang, J.(Eds.), Proceedings 11th International Conference Principles KnowledgeRepresentation Reasoning (KR08), pp. 274284. AAAI Press.Kollia, I., & Glimm, B. (2013). Evaluation sources. Available http://code.google.com/p/query-ordering/.Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2010). combined approach query answering DL-Lite. Lin, F., & Sattler, U. (Eds.),Proceedings 12th International Conference Principles Knowledge Representation Reasoning (KR10). AAAI Press.Kremen, P., & Sirin, E. (2008). SPARQL-DL implementation experience. Clark, K.,& Patel-Schneider, P. F. (Eds.), Proceedings 4th OWLED Workshop OWL:Experiences Directions Washington, Vol. 496 CEUR Workshop Proceedings.CEUR-WS.org.Ma, L., Yang, Y., Qiu, Z., Xie, G., Pan, Y., & Liu, S. (2006). Towards complete OWLontology benchmark. Semantic Web: Research Applications, Lecture NotesComputer Science, chap. 12, pp. 125139. Springer.Motik, B., Cuenca Grau, B., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (Eds.). (11December 2012a). OWL 2 Web Ontology Language: Profiles. W3C Recommendation.Available http://www.w3.org/TR/owl2-profiles/.Motik, B., Patel-Schneider, P. F., & Cuenca Grau, B. (Eds.). (11 December 2012b). OWL 2Web Ontology Language: Direct Semantics. W3C Recommendation. Availablehttp://www.w3.org/TR/owl2-direct-semantics/.Motik, B., Shearer, R., Glimm, B., Stoilos, G., & Horrocks, I. (2013). HermiT. Availablehttp://www.hermit-reasoner.com/.Motik, B., Shearer, R., & Horrocks, I. (2009). Hypertableau reasoning description logics.Journal Artificial Intelligence Research, 36, 165228.OBO Foundry (2013). open biological biomedical ontologies. Available http://www.obofoundry.org/.Oracle (2013). Oracle database documentation library 11g release 2. Available http://www.oracle.com/pls/db112/homepage.301fiKollia & GlimmPan, J. Z., Thomas, E., & Zhao, Y. (2009). Completeness guaranteed approximation owldl query answering. Proceedings 2009 International Workshop DescriptionLogics (DL09).Patel-Schneider, P. F., & Motik, B. (Eds.). (11 December 2012). OWL 2 Web OntologyLanguage: Mapping RDF Graphs. W3C Recommendation. Available http://www.w3.org/TR/owl2-mapping-to-rdf/.Perez-Urbina, H., Motik, B., & Horrocks, I. (2010). Tractable query answering rewritingdescription logic constraints. Journal Applied Logic, 8 (2), 186209.Perez-Urbina, H., Motik, B., & Horrocks, I. (2013). Requiem. Available http://www.comlab.ox.ac.uk/projects/requiem/home.html.Prudhommeaux, E., & Seaborne, A. (Eds.). (15 January 2008). SPARQL Query Language RDF. W3C Recommendation. Available http://www.w3.org/TR/rdf-sparql-query/.Racer Systems GmbH & Co. KG (2013).racer-systems.com.RacerPro 2.0.Available http://www.Ren, Y., Pan, J. Z., & Zhao, Y. (2010). Soundness preserving approximation tboxreasoning. Proceedings 25th National Conference Artificial Intelligence(AAAI10). AAAI Press.Rodriguez-Muro, M., & Calvanese, D. (2012). High performance query answeringdl-lite ontologies. Proc. 13th Int. Conf. Principles KnowledgeRepresentation Reasoning (KR 2012), pp. 308318.Schneider, M. (Ed.). (11 December 2012). OWL 2 Web Ontology Language: RDFBased Semantics. W3C Recommendation. Available http://www.w3.org/TR/owl2-rdf-based-semantics/.Seaborne, A. (9 January 2004). RDQL Query Language RDF. W3C MemberSubmission. Available http://www.w3.org/Submission/RDQL/.Selinger, P. G., Astrahan, M. M., Chamberlin, D. D., Lorie, R. A., & Price, T. G. (1979).Access path selection relational database management system. Bernstein,P. A. (Ed.), Proceedings 1979 ACM SIGMOD International ConferenceManagement Data, Boston, Massachusetts, May 30 - June 1, pp. 2334. ACM.Sirin, E., Cuenca Grau, B., & Parsia, B. (2006). wine water: Optimizing description logic reasoning nominals. Doherty, P., Mylopoulos, J., & Welty, C. A.(Eds.), Proceedings 10th International Conference Principles KnowledgeRepresentation Reasoning (KR06), pp. 9099. AAAI Press.Sirin, E., & Parsia, B. (2006). Optimizations answering conjunctive ABox queries: Firstresults. Proceedings 2006 International Workshop Description Logics(DL06), Vol. 189 CEUR Workshop Proceedings. CEUR-WS.org.Sirin, E., & Parsia, B. (2007). SPARQL-DL: SPARQL query OWL-DL. Golbreich,C., Kalyanpur, A., & Parsia, B. (Eds.), Proceedings OWLED 2007 WorkshopOWL: Experiences Directions, Vol. 258 CEUR Workshop Proceedings. CEURWS.org.302fiOptimizing SPARQL Query Answering OWL OntologiesSirin, E., Parsia, B., Grau, B. C., Kalyanpur, A., & Katz, Y. (2007). Pellet: practicalOWL-DL reasoner. Journal Web Semantics, 5 (2), 5153.Steinbrunn, M., Moerkotte, G., & Kemper, A. (1997). Heuristic randomized optimization join ordering problem. VLDB Journal, 6, 191208.Stocker, M., Seaborne, A., Bernstein, A., Kiefer, C., & Reynolds, D. (2008). SPARQL basicgraph pattern optimization using selectivity estimation. Proceedings 17thInternational Conference World Wide Web (WWW08), pp. 595604. ACM.Apache Software Foundation (2013). Apache jena. Available http://jena.apache.org.Thomas, E., Pan, J. Z., & Ren, Y. (2010). TrOWL: Tractable OWL 2 reasoning infrastructure. Proceedings Extended Semantic Web Conference (ESWC10).Thomas, E., Pan, J. Z., & Ren, Y. (2013). TrOWL. Available http://trowl.eu.Tsarkov, D., Horrocks, I., & Patel-Schneider, P. F. (2007). Optimizing terminological reasoning expressive description logics. Journal Automated Reasoning, 39 (3), 277316.303fiJournal Artificial Intelligence Research 48 (2013) 583-634Submitted 4/13; published 11/13Protecting Moving Targets Multiple Mobile ResourcesFei FangAlbert Xin JiangMilind Tambefeifang@usc.edujiangx@usc.edutambe@usc.eduUniversity Southern CaliforniaLos Angeles, CA 90089 USAAbstractrecent years, Stackelberg Security Games successfully applied solve resource allocation scheduling problems several security domains. However, previouswork mostly assumed targets stationary relative defenderattacker, leading discrete game models finite numbers pure strategies. papercontrast focuses protecting mobile targets leads continuous set strategiesplayers. problem motivated several real-world domains including protecting ferries escort boats protecting refugee supply lines. contributions include:(i) new game model multiple mobile defender resources moving targetsdiscretized strategy space defender continuous strategy space attacker.(ii) efficient linear-programming-based solution uses compact representationdefenders mixed strategy, accurately modeling attackers continuous strategy using novel sub-interval analysis method. (iii) Discussion analysis multipleheuristic methods equilibrium refinement improve robustness defenders mixedstrategy. (iv) Discussion approaches sample actual defender schedules defenders mixed strategy. (iv) Detailed experimental analysis algorithms ferryprotection domain.1. Introductionlast years, game-theoretic decision support systems successfully deployed several domains assist security agencies (defenders) protecting critical infrastructure ports, airports air-transportation infrastructure (Tambe, 2011; Gatti,2008; Marecki, Tesauro, & Segal, 2012; Jakob, Vanek, & Pechoucek, 2011). decision support systems assist defenders allocating scheduling limited resourcesprotect targets adversaries. particular, given limited security resourcespossible cover secure target times; simultaneously, attacker observe defenders daily schedules, deterministic schedule defenderexploited attacker (Paruchuri, Tambe, Ordonez, & Kraus, 2006; Kiekintveld,Islam, & Kreinovich, 2013; Vorobeychik & Singh, 2012; Conitzer & Sandholm, 2006).One game-theoretic model deployed schedule security resourcesdomains Stackelberg game leader (the defender) follower (theattacker). model, leader commits mixed strategy, randomizedschedule specified probability distribution deterministic schedules; followerobserves distribution plays best response (Korzhyk, Conitzer, & Parr, 2010).Decision-support systems based model successfully deployed, includingARMOR LAX airport (Pita, Jain, Marecki, Ordonez, Portway, Tambe, Western,c2013AI Access Foundation. rights reserved.fiFang, Jiang, & TambeParuchuri, & Kraus, 2008), IRIS US Federal Air Marshals service (Tsai, Rathi,Kiekintveld, Ordonez, & Tambe, 2009), PROTECT US Coast Guard (Shieh,An, Yang, Tambe, Baldwin, DiRenzo, Maule, & Meyer, 2012).previous work game-theoretic models security assumed either stationarytargets airport terminals (Pita et al., 2008), targets stationary relativedefender attacker, e.g., trains (Yin, Jiang, Johnson, Kiekintveld, Leyton-Brown,Sandholm, Tambe, & Sullivan, 2012) planes (Tsai et al., 2009), playersmove along targets protect attack them). stationary nature leadsdiscrete game models finite numbers pure strategies. paper focussecurity domains defender needs protect mobile set targets.attacker attack targets point time movement, leadingcontinuous set strategies. defender deploy set mobile escort resources (calledpatrollers short) protect targets. assume game zero-sum, allowvalues targets vary depending locations time. defendersobjective schedule mobile escort resources minimize attackers expected utility.call problem Multiple mobile Resources protecting Moving Targets (MRMT).first contribution paper novel game model MRMT called MRMTsg .MRMTsg attacker-defender Stackelberg game model continuous set strategies attacker. contrast, defenders strategy space also continuous,discretize MRMTsg three reasons. Firstly, let defenders strategy spacecontinuous, space mixed strategies defender would infinitedimensions, makes exact computation infeasible. Secondly, practice, patrollersable fine-grained control vehicles, makes actualdefenders strategy space effectively discrete one. Finally, discretized defender strategy space subset original continuous defender strategy space, optimalsolution calculated formulation feasible solution original gamegives lower-bound guarantee defender terms expected utility original continuous game. hand, discretizing attackers strategy spacehighly problematic illustrate later paper. particular, deployrandomized schedule defender assumption attacker couldattack certain discretized time points, actual attacker could attacktime point, leading possibly worse outcome defender.second contribution CASS (Solver Continuous Attacker Strategies), efficientlinear program exactly solve MRMTsg . Despite discretization, defender strategyspace still exponential number pure strategies. overcome shortcomingcompactly representing defenders mixed strategies marginal probability variables.attacker side, CASS exactly efficiently models attackers continuous strategyspace using sub-interval analysis, based observation given defendersmixed strategy, attackers expected utility piecewise-linear function. Along waypresenting CASS, present DASS (Solver Discretized Attacker Strategies),finds minimax solutions MRMTsg games constraining attacker attackdiscretized time points. clarity exposition first derive DASS CASScase targets move one-dimensional line segment. later show DASSCASS extended case targets move two-dimensional space.584fiProtecting Moving Targets Multiple Mobile Resourcesthird contribution focused equilibrium refinement. game multipleequilibria, defender strategy found CASS suboptimal respect uncertainties attackers model, e.g., attacker attack certain timeintervals. present two heuristic equilibrium refinement approaches game.first, route-adjust, iteratively computes defender strategy dominates earlier strategies. second, flow-adjust, linear-programming-based approach. experimentsshow flow-adjust computationally faster route-adjust route-adjusteffective selecting robust equilibrium strategies.Additionally, provide several sampling methods generating practical patrol routesgiven defender strategy compact representation. Finally present detailed experimental analyses algorithm ferry protection domain. CASS deployedUS Coast Guard since April 2013.rest article organized follows: Section 2 provides problem statement.Section 3 presents MRMTsg model initial formulation DASS CASSone-dimensional setting. Section 4 discusses equilibrium refinement, followed Section5 gives generalized formulation DASS CASS two-dimensional settings.Section 6 describes sample patrol route Section 7 provides experimentalresults ferry protection domain. Section 8 discusses related work, followed Section9, provides concluding remarks, Section 10, discusses future work.end article, Appendix provides table listing notations used article,Appendix B provides detailed calculation finding intersection points2-D case.2. Problem StatementOne major example practical domains motivating paper problem protecting ferries carry passengers many waterside cities. Packed hundredspassengers, may present attractive targets attacker. example, attackermay ram suicide boat packed explosives ferry happened attacksFrench supertanker Limburg USS Cole (Greenberg, Chalk, & Willis, 2006).case, intention attacker detected gets close ferry.Small, fast well-armed patrol boats (patrollers) provide protection ferries(Figure 1(a)), detecting attacker stopping armed weapons. However, often limited numbers patrol boats, i.e., cannot protect ferriestimes locations. first focus case ferries patrol boats moveone-dimensional line segment (this realistic setting also simplifies exposition);discuss two-dimensional case Section 5.2.1 Domain Descriptionproblem, L moving targets, F1 , F2 , ..., FL . assume targetsmove along one-dimensional domain, specifically straight line segment linking two terminal points name B. sufficient capture real-world domainsferries moving back-and-forth straight line two terminalsmany ports around world; example green line shown Figure 1(b).provide illustration geometric formulation problem Figure 2.1.585fiFang, Jiang, & Tambe(a)(b)Figure 1: (a) Protecting ferries patrol boats; (b) Part map New York Harbor Commuter Ferry Routes. straight line linking St. George TerminalWhitehall Terminal indicates public ferry route run New York City Department Transportation.targets fixed daily schedules. schedule target described continuous function Sq : q = 1, ..., L index target, = [0, 1]continuous time interval (e.g., representing duration typical daily patrol shift)= [0, 1] continuous space possible locations (normalized) 0 correspondingterminal 1 terminal B. Thus Sq (t) denotes position target Fqspecified time t. assume Sq piecewise linear.defender W mobile patrollers move along protect targets,denoted P1 , P2 , ..., PW . Although capable moving faster targets,maximum speed vm . defender attempts protect targets, attackerchoose certain time certain target attack. (In rest paper, denotedefender attacker he). probability attack success dependspositions patrollers time. Specifically, patroller detect tryintercept anything within protection radius cannot detect attacker priorradius. Thus, patroller protects targets within protective circle radius(centered current position), shown Figure 2.1.Figure 2: example three targets (triangles) two patrollers (squares).protective circles patrollers shown protection radius . patrollerprotects targets protective circle. Patroller P1 protecting F2 P2protecting F3 .586fiProtecting Moving Targets Multiple Mobile ResourcesSymmetrically, target protected patrollers whose protective circles coverit. attacker attacks protected target, probability successful attackdecreasing function number patrollers protecting target. Formally,use set coefficients {CG } describe strength protection.Definition 1. Let G {1, ..., W } total number patrollers protecting target Fq ,i.e., G patrollers Fq within radius G patrollers.CG [0, 1] specifies probability patrollers successfully stop attacker.require CG1 CG2 G1 G2 , i.e., patrollers offer better protection.previous work security games (Tambe, 2011; Yin et al., 2012; Kiekintveld,Jain, Tsai, Pita, Ordonez, & Tambe, 2009), model game Stackelberg game,defender commits randomized strategy first, attacker respondstrategy. patrol schedules domains previously created hand;hence suffer drawbacks hand-drawn patrols, including lack randomness (inparticular, informed randomness) reliance simple patrol patterns (Tambe, 2011),remedy paper.2.2 Defender Strategypure strategy defender designate movement schedule patroller.Analogous targets schedule, patrollers schedule written continuousfunction Ru : u = 1, ..., W index patroller. Ru must compatiblepatrollers velocity range. mixed defender strategy randomizationpure strategies, denoted f .2.3 Attacker Strategyattacker conducts surveillance defenders mixed strategy targets schedules; may execute pure strategy response attack certain target certaintime. attackers pure strategy denoted hq, ti q index targetattack time attack.2.4 Utility Functionassume game zero-sum. attacker performs successful attack target Fqlocation x time t, gets positive reward Uq (x, t) defender gets Uq (x, t),otherwise players get utility zero. positive reward Uq (x, t) known functionaccounts many factors practice. example, attacker may effectiveattack target stationary (such terminal point) targetmotion. targets position decided schedule, utility functionwritten Uq (t) Uq (Sq (t), t). assume target Fq , Uq (t) representedpiecewise linear function t.2.5 EquilibriumSince game zero-sum, Strong Stackelberg Equilibrium calculated findingminimax/maximin strategy (Fudenberg & Tirole, 1991; Korzhyk et al., 2010). is,587fiFang, Jiang, & Tambefind optimal defender strategy finding strategy minimizes maximumattackers expected utility.Definition 2. single patroller case, attacker expected utility attacking target Fqtime given defender mixed strategy fAttEUf (Fq , t) = (1 C1 f (Fq , t))Uq (t)(1)Uq (t) reward successful attack, f (Fq , t) probability patrollerprotecting target Fq time C1 protection coefficient single patroller.drop subscript f obvious context. C1 Uq (t) constants givenattackers pure strategy hq, ti, AttEU(Fq , t) purely decided (Fq , t). definitionmultiple patrollers given Section 3.4. denote attackersmaximum expected utilityAttEUmf = max AttEUf (Fq , t)q,t(2)optimal defender strategy strategy f AttEUmf minimized, formallyf arg minf 0 AttEUmf0(3)2.6 Assumptionsproblem, following assumptions made based discussions domainexperts. provide justifications assumptions. appropriatecurrent domain application, relaxing assumptions future applications remainsissue future work; provide initial discussion Section 10.attackers plan decided off-line, i.e., attacker take accountpatrollers current partial route (partial pure strategy) executing attack:assumption similar assumption made applications security gamesjustified elsewhere (An, Kempe, Kiekintveld, Shieh, Singh, Tambe, & Vorobeychik, 2012; Pita, Jain, Ordonez, Portway, Tambe, Western, Paruchuri, & Kraus,2009; Tambe, 2011). One key consideration given attackers limitedresources well, generate execute complex conditional planschange based on-line observations defenders pure strategy difficultrisky.single attacker assumed instead multiple attackers: assumption arisesperforming even single attack already costly attacker. Thus,coordinating attackers time even harder thereforesignificantly less likely attacker.game assumed zero-sum: case, objectives defenderattacker direct conflict: preventing attack higher potential damagebigger success defender game.588fiProtecting Moving Targets Multiple Mobile Resourcesschedules targets deterministic: domains focus on, potentialdelays targets schedules usually within several minutes any,targets try catch fixed schedules soon possible. Therefore,even delays occur, deterministic schedule target viewedgood approximation actual schedule.3. Modelssection, introduce MRMTsg model uses discretized strategy spacedefender continuous strategy space attacker. clarity exposition,introduce DASS approach compute minimax solution discretized attackerstrategy space (Section 3.2), followed CASS attackers continuous strategy space(Section 3.3). first assume single patroller Sections 3.1 3.3generalize multiple patrollers Section 3.4.3.1 Representing Defenders Strategiessubsection, introduce discretized defender strategy space compactrepresentation used represent defenders mixed strategy. show compactrepresentation equivalent intuitive full representation, followed several propertiescompact representation.Since defenders strategy space discretized, assume patrollermakes changes finite set time points = {t1 , t2 , ..., tM }, evenly spaced acrossoriginal continuous time interval. t1 = 0 starting time tM = 1 normalizedending time. denote distance two adjacent time points: = tk+1tk = M11 . set small enough target Fq , schedule Sq (t)utility function Uq (t) linear interval [tk , tk+1 ] k = 1, . . . , 1, i.e.,target moving uniform speed utility successful attack changeslinearly intervals. Thus, t0 breakpoint Sq (t) Uq (t)q, represented t0 = tK0 K0 integer.addition discretization time, also discretize line segment ABtargets move along set points = {d1 , d2 , ..., dN } restrict patrollerlocated one discretized points di discretized time point tk . Notenecessarily evenly distributed targets locations restricted tk .time interval [tk , tk+1 ], patroller moves constant speed location ditime tk location dj time tk+1 . movements compatible speed limitvm possible. points d1 , d2 , ..., dN ordered distance terminal A,d1 refers dN refers B. Since time interval discretized points,patrollers route Ru represented vector Ru = (dru (1) , dru (2) , ..., dru (M ) ). ru (k)indicates index discretized distance point patroller located timetk .explained Section 1, discretized defenders strategy spacecomputational reasons. even clear whether equilibrium exists originalgame continuous strategy space players. discretization made alsopractical constraint patrollers.589fiFang, Jiang, & Tambeexpository purpose, first focus case single defender resourcegeneralize larger number resources later. single defender resource,defenders mixed strategy full representation assigns probability patrolroutes executed. Since time step patroller choose goN different locations, N possible patrol routes total numberachievable speed limit (or vm large enough). exponentially growingnumber routes make analysis based full representation intractable.Therefore, use compact representation defenders mixed strategy.Definition 3. compact representation single defender resource compactway represent defenders mixed strategy using flow distribution variables {f (i, j, k)}.f (i, j, k) probability patroller moving di time tk dj time tk+1 .complexity compact representation O(M N 2 ), much efficientcompared full representation.Proposition 1. strategy full representation mapped compact representation.Proof sketch: H possible patrol routes R1 , R2 , ..., RH , mixed defenderstrategy represented full representation probability vector (p(R1 ), ...p(RH ))p(Ru ) probability taking route Ru . Taking route Ru means patrollermoves dru (k) dru (k+1) time [tk , tk+1 ], edge ERu (k),Ru (k+1),k takenroute Ru chosen. total probability taking edge Ei,j,k sumprobabilities routes Ru Ru (k) = Ru (k + 1) = j. Therefore, givenstrategy full presentation specified probability vector (p(R1 ), ...p(RH )),construct compact representation consisting set flow distribution variables{f (i, j, k)}Xf (i, j, k) =p(Ru ).(4)Ru :Ru (k)=i Ru (k+1)=jFigure 3 shows simple example illustrating compact representation. Numbersedges indicate value f (i, j, k). use Ei,j,k denote directed edge linkingnodes (tk , di ) (tk+1 , dj ). example, f (2, 1, 1), probability patroller movingd2 d1 time t1 t2 , shown edge E2,1,1 node (t1 , d2 ) node(t2 , d1 ). similar compact representation used earlier Yin et al. (2012),use continuous setting.Note different mixed strategies full representation mappedcompact representation. Table 1 shows two different mixed defender strategies full representations mapped mixed strategy compact representationshown Figure 3. probability route labeled edges route fullrepresentation. Adding numbers particular edge Ei,j,k routes fullrepresentation together, get f (i, j, k) compact representation.Theorem 1. Compact representation lead loss solution quality.590fiProtecting Moving Targets Multiple Mobile ResourcesFigure 3: Compact representation: x-axis shows time intervals; y-axis discretizeddistance-points one-dimensional movement space.R1 = (d1 , d1 , d1 )R1 = (d1 , d1 , d1 )Full Representation 1R2 = (d1 , d1 , d2 )R3 = (d2 , d1 , d1 )R4 = (d2 , d1 , d2 )Full Representation 2R2 = (d1 , d1 , d2 )R3 = (d2 , d1 , d1 )R4 = (d2 , d1 , d2 )Table 1: Two full representations mapped compact representationshown Figure 3.Proof sketch: complete proof theorem relies calculations Section3.2 3.3. provide sketch. Recall goal find optimal defenderstrategy f minimizes maximum attacker expected utility AttEUmf .show next subsections, (Fq , t) calculated compact representation{f (i, j, k)}. two defender strategies full representation mappedcompact representation {f (i, j, k)}, functionAttEU function according Equation 1. Thus value AttEUmftwo defender strategies. optimal mixed defender strategy compact representationstill optimal corresponding defender strategies full representation.exploit following properties compact representation.Property 1.ForPany time interval [tk , tk+1 ], sum flow distribution variablesPNNequals 1:i=1j=1 f (i, j, k) = 1.Property 2. sum flows go particular node equals sumP flowsgo node. Denote sum node (tk , di ) p(i, k), p(i, k) = Nj=1 f (j, i, kPN1) = j=1 f (i, j, k). p(i, k) equal marginal probability patrollerlocation di time tk .PProperty 3. Combining Property 1 2, Ni=1 p(i, k) = 1.591fiFang, Jiang, & Tambe3.2 DASS: Discretized Attacker Strategiessubsection, introduce DASS, mathematical program efficiently finds minimax solutions MRMTsg -based games assumption attacker attackone discretized time points tk . problem, need minimize v vmaximum attackers expected utility. Here, v maximum AttEU(Fq , t)target Fq discretized time point tk .Equation (1), know AttEU(Fq , t) decided (Fq , t), probabilitypatroller protecting target Fq time t. Given position target Sq (t),define protection range q (t) = [max{Sq (t)re , d1 }, min{Sq (t)+re , dN }]. patrollerlocated within range q (t), distance target patrollerthus patroller protecting Fq time t. (Fq , t) probabilitypatroller located within range q (t) time t. discretized time pointstk , patroller located discretized distance point di , definefollowing.Definition 4. I(i, q, k) function two values. I(i, q, k) = 1 di q (tk ),otherwise I(i, q, k) = 0.words, I(i, q, k) = 1 means patroller located di time tk protect target Fq . Note value I(i, q, k) calculated directly inputparameters (di , Sq (t) ) stored look-up table. particular, I(i, q, k)variable formulations follow. simply encodes relationship dilocation target Fq tk . probability patroller di time tkp(i, k).(Fq , tk ) =XAttEU(Fq , tk ) =1 C1i:I(i,q,k)=1p(i, k),Xi:I(i,q,k)=1p(i, k) Uq (tk ).(5)(6)Equation (6) follows Equations (1) (5), expressing attackers expected utilitydiscretized time points. Finally, must address speed restrictions patroller. setflows corresponding actions achievable zero,1 is, f (i, j, k) = 0|dj di | > vm t. Thus, DASS formulated linear program. linear program1. Besides speed limit, also model practical restrictions domain placing constraintsf (i, j, k).592fiProtecting Moving Targets Multiple Mobile Resourcessolves number targets one defender resource.minz(7)f (i,j,k),p(i,k)f (i, j, k) [0, 1],i, j, k(8)f (i, j, k) = 0,i, j, k |dj di | > vm(9)p(i, k) =NXf (j, i, k 1),i, k > 1(10)f (i, j, k),i, k <(11)k(12)q, k(13)j=1p(i, k) =NXj=1NXp(i, k) = 1,i=1z AttEU(Fq , tk ),Constraint 8 describes probability range. Constraint 9 describes speed limit. Constraints 1011 describes Property 2. Constraint 12 exactly Property 3. Property 1derived Property 2 3, listed constraint. Constraint (13) showsattacker chooses strategy gives maximal expected utility amongpossible attacks discretized time points; AttEU() described Equation (6).3.3 CASS: Continuous Attacker Strategiessubsection, generalize problem one continuous attacker strategy setprovides linear-programming-based solution CASS. CASS efficiently finds optimalmixed defender strategy assumption attacker attack timecontinuous time interval = [0, 1]. assumption, DASSs solution qualityguarantee may fail: attacker chooses attack tk tk+1 , may gethigher expected reward attacking tk tk+1 . Consider following example,defenders compact strategy tk tk+1 shown Figure 4.defenders strategy three non-zero flow variables f (3, 4, k) = 0.3, f (3, 1, k) = 0.2,f (1, 3, k) = 0.5, indicated set three edges E + = {E3,4,k , E3,1,k , E1,3,k }.target Fq moves d3 d2 constant speed [tk , tk+1 ]. schedule depictedstraight line segment Sq . dark lines L1q L2q parallel Sq distance. area indicates protection range q (t) time (tk , tk+1 ).Consider time points edge E + intersects one L1q , L2q , labelr , r = 1 . . . 4 Figure 4). Intuitively, time points defenderqkpatrol could potentially enter leave protection range target. simplify05notation, denote tk qkk+1 qk . example, patroller moving d30 1d4 (or equivalently, taking edge E3,4,k ) protects target qkqk0 , 1 ], distance target lessE3,4,k L11 L21 [qkqkr r+1 ,equal protection radius . Consider sub-intervals qkqkr = 0 . . . 4. Since within five sub-intervals, patroller enters leaves593fiFang, Jiang, & Tambeprotection range, probability target protected constantsub-interval, shown Figure 5(a).Figure 4: example show target moving d3 d2 [tk , tk+1 ] pror , r+1 ], patroller either always protects targettected. sub-interval [qkqknever protects target. Equivalently, target either always withinprotective circle patroller always outside circle.Suppose Uq (t) decreases linearly 2 1 [tk , tk+1 ] C1 = 0.8.calculate attackers expected utility function AttEU(Fq , t) (tk , tk+1 ), plottedFigure 5(b). AttEU(Fq , t) linear sub-interval function discontinuous1 , . . . , 4 patroller leaving entering protectionintersection points qkqkr left as:range target. denote limit AttEU approaches qkrlim AttEU(Fq , t) = AttEU(Fq , qk)rtqkSimilarly, right limit denoted as:r+lim AttEU(Fq , t) = AttEU(Fq , qk)r+tqk2 ,Fq target, attacker choose attack time immediately qkgetting expected utility arbitrarily close 1.70. According Equation (6),2+get AttEU(Fq , tk ) = 1.20 AttEU(Fq , tk+1 ) = 1.00, lower AttEU(Fq , qk).Thus, attacker get higher expected reward attacking tk tk+1 ,violating DASSs quality guarantee.However, discontinuities attackers expected utility function, maximum might exist. implies minimax solution concept might welldefined game. thus define solution concept minimizing supremumAttEU(Fq , t).594fiProtecting Moving Targets Multiple Mobile Resources0.501.701.431.201.000.200.00(a) Probability target protectedconstant sub-interval.(b) attackers expected utility linearsub-interval.Figure 5: Sub-interval analysis (tk , tk+1 ) example shown Figure 4.]Definition 5. supremum attackers expected utility smallest real numbergreater equal elements infinite set {AttEU(Fq , t)}, denotedsup AttEU(Fq , t).supremum least upper bound function AttEU(Fq , t). CASSmodel, Equation 2 modifiedAttEUmf = sup AttEUf (Fq , t)(14)q,tdefender strategy f minimax AttEUmf maximized, i.e.,f arg minf 0 sup AttEUf 0 (Fq , t)2+)=example, supremum attackers expected utility (tk , tk+1 ) AttEU(Fq , qk1.70. rest paper, specify supremum used instead maximum easily inferred context.deal possible attacks discretized points findoptimal defender strategy? generalize process (called sub-interval analysis)possible edges Ei,j,k . make use piecewise linearity AttEU(Fq , t)fact potential discontinuity points fixed, allows us constructlinear program solves problem optimality. name approach CASS(Solver Continuous Attacker Strategies).first introduce general sub-interval analysis. target Fq timeinterval (tk , tk+1 ), calculate time points edges Ei,j,k L1q , L2q intersect,denoted intersection points. sort intersection points increasing order, denotedr , r = 1 . . . ,0qkqkqk total number intersection points. Set qk = tkqkqk+1r , r+1 ), r = 0, ..., .= tk+1 . Thus (tk , tk+1 ) divided sub-intervals (qkqkqkLemma 1. given target Fq , AttEU(Fq , t) piecewise-linear t. Furthermore,exists fixed set time points, independent defenders mixed strategy,AttEU(Fq , t) linear adjacent pair points. Specifically, pointsr defined above.intersection points qk595fiFang, Jiang, & Tamber , r+1 ) target F , feasible edge EProof: sub-interval (qkqi,j,k eitherqk12totally Lq , similarly Lq . Otherwise new intersectionpoint contradicts definition sub-intervals. edge Ei,j,k L1qL2q , distance patroller taking edge target Fq less , meaningtarget protected patroller. edge Ei,j,k taken probability f (i, j, k),total probability target protected ((Fq , t)) sum f (i, j, k) whosecorresponding edge Ei,j,k two lines sub-interval. (Fq , t) constantsub-interval thus attackers expected utility AttEU(Fq , t) linearsub-interval according Equation 2 Uq (t) linear [tk , tk+1 ]. Discontinuityexist intersection points upper bound number pointstarget Fq N 2 .r , r+1 ), 0Define coefficient Arqk (i, j) C1 edge Ei,j,k L1q L2q (qkqkotherwise. According Equation (1) fact (Fq , t) sum f (i, j, k) whoser , r+1 ).corresponding coefficient Arqk (i, j) = C1 , following equation (qkqkN XNX(15)AttEU(Fq , t) = 1Arqk (i, j)f (i, j, k) Uq (t)i=1 j=1Piecewise linearity AttEU(Fq , t) means function monotonic sub-intervalsupremum found intersection points. linearity, supremumr , r+1 ) chosen one-sided limits endpoints,AttEU (qkqk(r+1)r+) AttEU(Fq , qk). Furthermore, Uq (t) decreasing [tk , tk+1 ],AttEU(Fq , qksupremum(r+1)r+) otherwise AttEU(Fq , qk). words, attackersAttEU(Fq , qkr+1r r+1 . Thus, CASSrstrategies (qk , qk ) dominated attacking time close qkqkadds new constraints Constraints 813 consider attacks occur (tk , tk+1 ).add one constraint sub-interval respect possible supremum valuesub-interval:minz(16)f (i,j,k),p(i,k)subject constraints (8 . . . 13)(r+1)r+z max{AttEU(Fq , qk), AttEU(Fq , qk)}(17)k {1 . . . }, q {1 . . . L}, r {0 . . . Mqk }linear program stands core CASS differentiate namesolver name linear program following. linear constraints included Constraint 17 added CASS using Algorithm 1. inputalgorithm include targets schedules {Sq }, protection radius , speed limitvm , set discretized time points {tk } set discretized distance points {di }.Function CalInt(L1q , L2q , vm ) Line 1 returns list intersection time points0possible edges Ei,j,k parallel lines L1q , L2q , additional points tk qk+1r , r+1 ) Line 1 returns coefficienttk+1 qkqk . Function CalCoef(L1q , L2q , vm , qkqkrrmatrix Aqk . Aqk easily decided checking status midpoint time. Set596fiProtecting Moving Targets Multiple Mobile Resourcesr + r+1 )/2 denote patrollers positiontmid = (qkmid edge Ei,j,k takenqkEi,j,tmid , thus Arqk (i, j) = C1 Ei,j,tmid q (tmid ). Lines 11 add constraint respect(r+1)r+larger value AttEU(Fq , qk) AttEU(Fq , qk) CASS sub-intervalr+1r(qk , qk ). means attacker chooses attack Fq sub-interval, bestr , r+1 ).choice decided larger value two side-limits AttEU (qkqkAlgorithm 1: Add constraints described Constraint 17Input: Sq , , vm , {tk }, {di };k 1, . . . , 1q 1, . . . , LL1q Sq + , L2q Sq ;+10 , . . . , qkCalInt(L1q , L2q , vm );qkqkr 0, . . . , Mqkr , r+1 );Arqk CalCoef(L1q , L2q , vm , qkqkUq (t) decreasing [tk , tk+1 ]r+add constraint z AttEU(Fq , qk)endelse(r+1)add constraint z AttEU(Fq , qk)endendendendTheorem 2. CASS computes (in polynomial time) exact solution (minimax)game discretized defender strategies continuous attacker strategies.Proof: According Lemma 1, AttEU(Fq , t) piecewise linear discontinuityr . intersection points divide time spaceoccur intersection points qksub-intervals. piecewise linearity, supremum AttEU(Fq , t) equalslimit endpoint least one sub-interval. defenders strategy ffeasible, feasible z linear program 16-17 less limit valuesintersection points according Constraint 17 values discretized time points tkaccording Constraint 13, thus v upper bound AttEU(Fq , t) f . zminimized objective function, z greater supremum AttEU(Fq , t)given defender strategy f , z minimum set supremumcorresponding defender strategies. Thus get optimal defender strategy f .total number variables linear program O(M N 2 ). number constraints represented Algorithm 1 O(M N 2 L) number intersection points2(M 1)N 2 target. number constraints represented Constraints813 O(M N 2 ). Thus, linear program computes solution polynomial time.Corollary 1. solution CASS provides feasible defender strategy originalcontinuous game gives exact expected value strategy.597fiFang, Jiang, & Tambe3.4 Generalized Model Multiple Defender Resourcessubsection, generalize DASS CASS solve problem multiple defender resources. multiple patrollers, patrollers coordinateother. Recall protection coefficient CG Definition 1, target better protectedpatrollers close (within radius ). protection providedtarget determined patrollers locations known. Thus sufficientcalculate probability individual edge taken single patroller case.presence multiple patrollers, need complex representation explicitly describe defender strategy. illustrate generalization multiple defenderresources case, first take two patrollers example. two patrollers,patrol strategy represented using flow distribution variables {f (i1 , j1 , i2 , j2 , k)}.flow distribution variables defined Cartesian product two duplicated setsfeasible edges {Ei,j,k }. f (i1 , j1 , i2 , j2 , k) joint probability first patrollermoving di1 dj1 second patroller moving di2 di2 time tktk+1 , i.e., taking edge Ei1 ,j1 ,k Ei2 ,j2 ,k respectively. corresponding marginal distribution variable p(i1 , i2 , k) represents probability first patroller di1second di2 time tk . Protection coefficients C1 C2 used one twopatrollers protecting target respectively.attackers expected utility writtenAttEU(Fq , t) = (1 (C1 1 (Fq , t) + C2 2 (Fq , t))) Uq (t)1 (Fq , t) probability one patroller protecting target Fq time2 (Fq , t) probability patrollers protecting target. attackshappen discretized points tk , make use I(i, q, k) Definition 4I(i1 , q, k) + I(i2 , q, k) total number patrollers protecting ferry time tk .X1 (Fq , tk ) =p(i1 , i2 , k)i1 ,i2 :I(i1 ,q,k)+I(i2 ,q,k)=1X2 (Fq , tk ) =p(i1 , i2 , k)i1 ,i2 :I(i1 ,q,k)+I(i2 ,q,k)=2Constraints attacks occurring (tk , tk+1 ) calculated algorithmlooks Algorithm 1. main difference coefficient matrix Arqkexpression AttEU. set values coefficient matrix Arqk (i1 , j1 , i2 , j2 ) C2edges Ei1 ,j1 ,k Ei2 ,j2 ,k L1q L2q , C1 one edgesr , r+1 )protects target. attackers expected utility function (qkqkXAttEU(Fq , t) = (1Arqk (i1 , j1 , i2 , j2 )f (i1 , j1 , i2 , j2 , k)) Uq (t)i1 ,j1 ,i2 ,j2general case W defender resources, use {f (i1 , j1 , ..., iW , jW , k)} represent patrol strategy.Definition 6. compact representation multiple defender resources compact wayrepresent defenders mixed strategy using flow distribution variables {f (i1 , j1 , ..., iW , jW , k)}.{f (i1 , j1 , ..., iW , jW , k)} joint probability patroller moving diu time tkdju time tk+1 u = 1 . . . W .598fiProtecting Moving Targets Multiple Mobile ResourcesGiven generalized compact representation, get following equations calculating attackers expected utility function protection probability:WXAttEU(Fq , t) = 1CQ Q (Fq , t) Uq (t)Q=1Q (Fq , tk ) =Xi1 ,...,iW :WPp(i1 , . . . , iW , k)I(iu ,q,k)=Qu=1Q number patrollers protecting target. modify Algorithm 1 applymultiple defender resource case. Set Arqk (i1 , j1 , ..., iW , jW ) CQ Q edges{Eiu ,ju ,k } L1q L2q .conclude linear program generalized CASS multiple patrollers follows.minz(18)f (i1 ,j1 ,...,iW ,jW ,k),p(i1 ,...,iW ,k)f (i1 , j1 , . . . , iW , jW , k) = 0, i1 , . . . , iW , j1 , . . . , jW u, |dju diu | > vm(19)nnXXp(i1 , . . . , iW , k) =...f (j1 , i1 , . . . , jW , iW , k 1), i1 , . . . , iW , k > 1j1 =1jW =1nXnX(20)p(i1 , . . . , iW , k) =...j1 =1f (i1 , j1 , . . . , iW , jW , k), i1 , . . . , iW , k <jW =1(21)nXi1 =1...nXp(i1 , . . . , iW , k) = 1, k(22)iW =1z AttEU(Fq , tk ), q, kz(r+1)r+max{AttEU(Fq , qk), AttEU(Fq , qk)}, k, q, r(23)(24)number variables linear program O(M N 2W ) number constraints O(M N W ). reasonable examine potentially efficient alternatives.summarize results examination concluding using currentlinear program would appear currently offer best tradeoff terms solution quality time least current domains application; although discussed below,significant future work might reveal alternatives approaches future domains.first question examine computational complexity problemhand: generating optimal patrolling strategies multiple patrollers graph. Unfortunately, despite significant attention paid topic, currently, complexity remainsunknown. specifically, question computational complexity generating patrolsmultiple defenders graphs different types received significant attention (Letchford, 2013; Korzhyk et al., 2010). studies illustrate several cases problemNP-hard, cases problem known polynomial time, despite significant effort, problem complexity many cases remains unknown (Letchford & Conitzer,599fiFang, Jiang, & Tambe2013). Unfortunately, graph turns different cases consideredwork. Indeed, DASS model explained game homogeneous defenderresources patrolling graph, similar cases already considered.However, prior results cannot explain complexity game structuregraph fit prior graphs.Given computational complexity results directly available, may examineapproaches provide efficient approximations. provide overview twoapproaches (providing experimental results Section 7.1.6). first approach attemptsprovide compact representation hope providing speedup. end,apply intuitive approach uses individual strategy profile patrollercalculates best possible mixed strategy combination. Unfortunately, approachinefficient run-time even DASS model may result suboptimal solution.Thus, although compact, approach fails achieve goal; explainapproach next.Assume patroller independently follows mixed strategy. Denote individual mixed strategy patroller u fu (iu , ju , tk ), probability targetprotected Q players represented polynomial expression {fu (iu , ju , tk )}order Q. optimization problem converted minimizing objective function znon-linear constraints. Assume two patrollers, potential attacktarget q time tk , denote probability patroller u protecting target$u . $u linear fu , attackers expected utility attack representedAttEU(Fq , tk ) = (1 C1 ((1 $1 )$2 + (1 $2 )$1 ) C2 $1 $2 )Uq (tk )constraint z AttEU(Fq , tk ) quadratic f , due fact joint probabilityrepresented product individual probability patroller. constraints ensured convex feasible region known polynomialalgorithms solving kind non-convex optimization problems. attempt solveproblem converting mathematical program non-convex objective function linear constraints, i.e., instead minimizing z constraints z AttEU(Fq , tk ),incorporate constraints objective functionz = max{AttEU(Fq , tk )}q,k(25)results Section 7.1.6 show solve mathematical program MATLAB using function fmincon interior-point method DASS model, algorithmfails get feasible solution efficiently even enough time given, solutionstill suboptimal may get stuck local minimum. conclude, althoughapproach compact helps saving memory, inefficient run-time mayresult loss solution quality.second approach takes step reduce run-time complexity, makingpolynomial approximation algorithm, lead high degradation solution quality. approach, iteratively compute optimal defender strategynewly added resource unit given existing strategies previous defender resources.Namely, first calculate f1 (i1 , j1 , tk ) one patroller available calculate600fiProtecting Moving Targets Multiple Mobile Resourcesf2 (i2 , j2 , tk ) given value f1 (i1 , j1 , tk ). way, need solve W linear programscomplexity O(M N 2 ) approach much faster compared former one. Unfortunately, approach fails capture coordination patrollers effectivelythus may result high degradation solution quality. example, supposetwo targets constant utility U , one target stays terminal onestays terminal B. Further, suppose protection coefficient always 1 targetprotected one patrollers. two patrollers available, optimal solution would protect one targets way, targets protectedprobability 1 expected utility function attacker 0. defenderstrategy calculated patroller sequentially discussed above, solution wouldprotect target probability 0.5 players, making attackers expectedutility 0.25%U . words, reach suboptimal solution, wasting resourcespatrollers end protecting target probability 0.25. case,already see 0.25 probability target unprotected clearlyoptimal solution existed protected targets probability 1. Thus, eventwo patrollers solution leads potentially significant loss expected utility;therefore, solution clearly appears inadequate purposes.Given discussion, would appear fast approximation may leadsignificant losses solution quality may efficient enough. Fortunately currentapplication domains, current deployment CASS protecting ferries (e.g.,Staten Island Ferry New York), number defender resources limited.lack resources main reason optimization using security games becomes critical.result, current approach CASS adequate current domains ferryprotection. research scale-up issue future work.4. Equilibrium Refinementgame often multiple equilibria. Since game zero-sum, equilibria achieveobjective value. However, attacker deviates best response,equilibrium strategies defender may provide better results others.Consider following example game. two targets moving [t1 , t2 ] (nodiscretization): one moves d3 d2 moves d1 d2 (SeeFigure 6(a)). Suppose d3 d2 = d2 d1 = = 0.5d. one patrolleravailable protection coefficient C1 = 1. targets utility functions decrease10 1 [t1 , t2 ] (See Figure 6(b)). one equilibrium, f3,2,1 = f1,2,1 = 0.5, i.e.,patroller randomly chooses one target follows way. another equilibrium,f3,3,1 = f1,1,1 = 0.5, i.e., patroller either stays d1 d3 . either equilibrium,attackers best response attack t1 , maximum expected utility 5.However, attacker physically constrained (e.g., due launch point locations)attack earlier t0 t0 > 11 (where 11 intersection time point11 = (t1 + t2 )/2), defender strategies choose attack eithertargets t0 . attackers expected utility Uq (t0 )/2 first equilibrium50% probability patroller following target. However secondequilibrium, assured succeed get utility Uq (t0 ) distancechosen target d1 (or d3 ) larger t0 , i.e., chosen target unprotected601fiFang, Jiang, & Tambet0 . case, defender strategy first equilibrium preferable onesecond; indeed, first defender strategy dominates second one, meanfirst equally good better second matter strategy attackerchooses. provide formal definition dominance Section 4.1.101(a) Two targets moves schedules S1S2 .(b) Utility functiontargets decreasing linearlytime.Figure 6: example show one equilibrium outperforms another attackerconstrained attack [t0 , t2 ] t0 > 11 .goal improve defender strategy robust constrainedattackers keeping defenders expected utility unconstrained attackerssame. task selecting one multiple equilibria game instanceequilibrium refinement problem, received extensive study game theory(van Damme, 1987; Fudenberg & Tirole, 1991; Miltersen & Srensen, 2007). finitesecurity games, An, Tambe, Ordonez, Shieh, Kiekintveld (2011) proposed techniquesprovide refinement Stackelberg equilibrium. However little priorresearch computation equilibrium refinements continuous games.section, introduce two equilibrium refinement approaches: route-adjust(Section 4.1) flow-adjust (Section 4.2). approaches applied improvefeasible defender strategy applied optimal defender strategyexisting equilibrium, get new equilibria robust optimal defenderstrategies.expository simplicity, still use single-resource case example,methods applicable multiple-resources case. results shown evaluationsection experimentally illustrates two refinement methods significantly improveperformance.4.1 Route AdjustGiven f defender strategy one equilibrium game, find defenderstrategy f 0 attacker strategy (q, t), defenders expected utilityf 0 equal higher one f , one f 0 strictly higherone f least one specific attacker strategy, say f 0 dominates f .Intuitively, defender choose f 0 instead f f 0 least good f602fiProtecting Moving Targets Multiple Mobile Resourcesattacker strategy achieve better performance attacker strategies.equilibrium strategy f 0 robust unknown deviations attacker side.give formal definition dominance follows.Definition 7. Defender strategy f dominates f 0 q, t, DefEUf (Fq , t) DefEUf 0 (Fq , t),q, t, DefEUf (Fq , t) > DefEUf 0 (Fq , t); equivalently zero-sum game, q, t,AttEUf (Fq , t) AttEUf 0 (Fq , t), q, t, AttEUf (Fq , t) < AttEUf 0 (Fq , t).Corollary 2. Defender strategy f dominates f 0 q, t, (Fq , t) 0 (Fq , t) q, t,(Fq , t) > 0 (Fq , t).Definition 7 simply restates commonly used weak dominance definition gametheory specific game. Corollary 2 follows Equation (1).section, introduce route-adjust approach gives procedurefinding defender strategy f 1 dominates given defender strategy f 0 . Route-adjustprovides final routes using steps: (i) decompose flow distribution f 0 componentroutes; (ii) route, greedily find route provides better protection targets;(iii) combine resulting routes new flow distribution, f 1 , dominates f 0f 1 different f0 . detailed process listed Algorithm 2. illustrateapproach using simple dominated strategy shown Figure 3.accomplish step (i), decompose flow distribution iteratively finding routecontains edge minimum probability. shown Figure 7, first randomlychoose route contains edge E1,2,2 , f (1, 2, 2) = 0.4 minimum among flowvariables. choose R2 = (d1 , d1 , d2 ), set p(R2 ) = f (1, 2, 2) = 0.4.edge route R2 subtract 0.4 original flow, resulting residual flow.continue extract routes residual flow route left. Denote Znumber non-zero edges flow distribution graph, Z decreased least1 iteration. algorithm terminate Z steps Zroutes found. result step (i) sparse description defender mixed strategyfull representation. discuss Section 6, decomposition constitutes onemethod executing compact strategy.step (ii), adjust routes greedily. end, first introducer coefficientdominance relation edges routes, using intersection points qkrmatrix Aqk (i, j) defined Section 3.3.Definition 8. Edge Ei,j,k dominates edge Ei0 ,j 0 ,k [tk , tk+1 ] Arqk (i, j) Arqk (i0 , j 0 ),q = 1..L, r = 0..Mqk , q, r Arqk (i, j) > Arqk (i0 , j 0 ).dominance relation edges based comparison protection providedtargets sub-interval. following dominance relation routes, denoteedge Eru (k),ru (k+1),k E(u, k) simplify notation, .Definition 9. Route Ru = (dru (1) , . . . , dru (M ) ) dominates Ru0 = (dru0 (1) , . . . , dru0 (M ) )k = 1 . . . 1, E(u, k) = E(u0 , k) E(u, k) dominates E(u0 , k) k E(u, k)dominates E(u0 , k).Route Ru dominates Ru0 edge Ru either dominatescorresponding edge Ru0 least one edge Ru dominates corresponding edgeRu0 .603fiFang, Jiang, & TambeAlgorithm 2: Route-AdjustInput: mixed defender strategy fOutput: updated mixed defender strategy f 0(i) Decompose f multiple routes iteratively finding route containsedge minimum probability:(a) Initialize remaining flow distribution f = f route set = .Initialize probability distribution routes p(Ru ) = 0, u.(b) max f(i, j, k) > 0i. Set (i0 , j0 , k0 ) = arg mini,j,k:f(i,j,k)>0 f(i, j, k).ii. Set fmin = f(i0 , j0 , k0 ).iii. Find arbitrary route Ru0 ru0 (k0 1) = i0 ru0 (k0 ) =j0 (i.e., edge Ei0 ,j0 ,k0 route) f(ru0 (k), ru0 (k+1), k) > 0,k (i.e., edges route non-zero remaining flow).iv. Add Ru0 set p(Ru0 ) = fmin .v. Set f(i, j, k) = f(i, j, k) fmin ru0 (k 1) = ru0 (k) = j.end(ii) Adjust route greedily get new set routes 0 corresponding new probability distribution p0 :(a) Initialize new set 0 = new probability distribution p0 (Ru ) = 0,u.(b) 6=i. Pick route Ru S.ii. Adjust Ru get new route Ru0 : given Ru specifiedk , set ru0 (k) = ru (k) k 6= k . Set ru0 (k ) = i0 that: 1)E(u1 , k 1) E(u1 , k ) meet speed constraint; 2) Ru0 dominates Ru choice i0 ; 3) Ru0 dominated routechoice i0 . i0 exists, set ru0 (k ) = ru (k )iii. Add Ru 0 set p0 (Ru0 ) = p(Ru ).iv. Remove Ru S.end(iii) Reconstruct new compact representation f 0 0 p0 accordingEquation 4.604fiProtecting Moving Targets Multiple Mobile Resources, ,p R 0.4, ,p R 0.2, ,p R 0.4Figure 7: Step (i): decomposition. Every time route containing minimal flow variablesubtracted residual graph left decomposition. original flow distribution thus decomposed three routes R2 , R1 , R3probability 0.4, 0.2 0.4 respectively.Denote original route adjusted Ru new route Ru0 . greedyway improve route replace one node route. want replacenode time tk , ru0 (k) = ru (k), k 6= k dru (k ) originalroute replaced dru0 (k ) . patrollers route changes [tk 1 , tk +1 ]. Thus,edges E(u, k 1) E(u, k ) original route replaced E(u0 , k 1)E(u0 , k ) new route.trying find new route Ru0 dominates original route provide equalprotection targets. selection ru0 (k ) needs meet requirementsspecified Algorithm 2. first one describes speed limit constraint. secondone actually requires changed edges E(u0 , k 1) E(u0 , k ) either equaldominate corresponding edges original route (and dominance relation existleast one edge). third requirement attains local maximum. new nodeexist specified k , return original route Ru .iterate process new route get final route denoted Ru0several iterations state convergence reached. state convergencereached, resulting route Ru0 keeps unchanged matter k chosennext iteration.example Figure 7, assume targets moving schedule d1 d1 d2 ,d3 d2 = d2 d1 = d, = 0.1d utility function constant. adjust routeone iteration changing patrollers position time t3 , i.e., ru (3). t3last discretized time point, edge E(u, 2) may changed. R1 = (d1 , d1 , d1 ),enumerate possible patrollers positions time t3 choose one accordingthree constraints mentioned above. case, candidates d1 d2 ,corresponding new routes R1 (unchanged) R2 = (d1 , d1 , d2 ) respectively. Noteedge Ed1 ,d2 ,2 dominates Ed1 ,d1 ,2 former one protects target way[t2 , t3 ] thus R2 dominates R1 . d2 chosen patrollers position t3 R2605fiFang, Jiang, & Tambechosen new route. adjustment routes non-zero probabilitydecomposition shown Table 2.RuR1 = (d1 , d1 , d1 )R2 = (d1 , d1 , d2 )R3 = (d2 , d1 , d1 )p(Ru ) decomposition0.20.40.4Adjusted Routes(d1 , d1 , d2 ) = R2(d1 , d1 , d2 ) = R2(d2 , d1 , d2 ) = R4Table 2: Step (ii): Adjust route greedily.R1R2R3R4Ru= (d1 , d1 , d1 )= (d1 , d1 , d2 )= (d2 , d1 , d1 )= (d2 , d1 , d2 )p0 (Ru ) adjustment00.600.4Composed Flow DistributionTable 3: Step (iii): compose new compact representation.new routes get step (ii) original routes dominateoriginal routes. is, whenever route Ru chosen according defender mixedstrategy resulting step (i), always equally good better choose corresponding new route Ru0 instead, Ru0 provides equal protectiontargets Ru . Suppose H possible routes defender strategy step(i), denoted R1 , ..., RH . adjusting routes, get new defender strategy(p0 (R1 ), p0 (R2 ), ..., p0 (RH )) full representation (See Table 3). routes takenhigher probability (e.g. p0 (R2 ) = 0.2 + 0.4 = 0.6) lower probability(e.g. p0 (R3 ) = 0) compared original strategy. step (iii), reconstruct newcompact representation according Equation 4. accomplished via processinverse decomposition exactly map strategy fullrepresentation compact representation. example above, result shownTable 3.Theorem 3. steps (i)(iii), get new defender strategy f 1 dominatesoriginal one f 0 f 1 different f 0 .Proof: continue use notation decomposition step (i) yieldsroutes R1 , ..., RH . flow distribution variable original distribution f 0 (i, j, k),decomposed H sub-flows {fu0 (i, j, k)} according route decomposition. fu0 (i, j, k) =p(Ru ) = ru (k), j = ru (k + 1) fu0 (i, j, k) = 0 otherwise. Thus followingequation.XHf 0 (i, j, k) =f 0 (i, j, k)(26)u=1 uX=fu0 (i, j, k)(27)u:ru (k)=i,ru (k+1)=jadjust route separately, non-zero sub-flow fu0 (i, j, k) edge E(u, k) movededge E(u0 , k) route Ru adjusted Ru0 . Reconstructing flow distribution f 1606fiProtecting Moving Targets Multiple Mobile Resourcesalso regarded adding sub-flows adjustment together edge.means, f 1 composed set sub-flows adjustment, denoted {fu1 (i0 , j 0 , k)}.subscript u represents index original route indicate movededge E(u, k). fu1 (i0 , j 0 , k) = fu0 (ru (k), ru (k + 1), k), i0 = Ru0 (k) j 0 = Ru0 (k + 1);otherwise fu1 (i0 , j 0 , k) = 0. Similarly Equation 27, following equation f 1 .f 1 (i0 , j 0 , k) ==XHf 1 (i0 , j 0 , k)u=1 uXu0 :ru0 (k)=i0 ,ru0 (k+1)=j 0(28)fu1 (i0 , j 0 , k)(29)Based adjustment made, Ru0 dominates Ru thus E(u0 , k)dominates E(u, k). edge E(u, k) protects target Fq time t,corresponding edge E(u0 , k) adjustment also protects target Fq time t.Recall Section 3.3 (Fq , t) sum f (i, j, k) whose corresponding edgeEi,j,k protect target Fq time t. denote 0 (Fq , t) 1 (Fq , t) probabilities protection corresponding f 0 f 1 respectively. According Equation 27,0 (Fq , t) viewed sum non-zero sub-flows fu0 (i, j, k) corresponding E(u, k) protects target Fq time t. fu0 (i, j, k) term summationcalculate 0 (Fq , t), means E(u, k) protects Fq thus corresponding E(u0 , k)protects Fq t, corresponding sub-flow fu1 (ru0 (k), ru0 (k + 1), k) f 1 also termsummation calculate 1 (Fq , t). leads conclusion 0 (Fq , t) 1 (Fq , t). Noteq, t, 0 (Fq , t) = 1 (Fq , t), routes kept unchanged step (ii) otherwisecontradicts fact new route dominates original route. AccordingCorollary 2, f 1 dominates f 0 different f 0 .example Figure 7, f 0 (1, 1, 2) decomposed two non-zero terms f10 (1, 1, 2) =0.2 f30 (1, 1, 2) = 0.4 along routes R1 R3 (See Figure 7). adjustment,get corresponding subflows f11 (1, 2, 2) = 0.2, f31 (1, 2, 2) = 0.4. Recall targetsschedule d1 d1 d2 . flow distribution adjustment (See Table 5) givesprotection target [t2 , t3 ]. Since flow equal t1 t2 (and thereforeprotection same), overall new strategy dominates old strategy.Therefore, apply route-adjust optimal defender strategy calculated CASSget robust equilibrium. step (iii) allows us prove Theorem 3, noticeend step (ii), probability distribution set routessample actual patrol routes. two defender resources, generalizedversion Definition 8 used define dominance relation edge tuple(Ei1 ,j1 ,k , ..., EiW ,jW ,k ) coefficient matrix multiple patrollers Arqk (i1 , j1 , ..., iW , jW ).ways adjust route. Instead adjusting one noderoute, adjust consecutive nodes time, example, adjustru0 (k ) ru0 (k + 1) checking edges E(u0 , k 1), E(u0 , k ) E(u0 , k + 1). However,need tradeoff performance efficiency algorithm. tradeoffdiscussed Section 7.4.2 Flow AdjustWhereas route-adjust tries select equilibrium robust attackers playingsuboptimal strategies, second approach, flow-adjust, attempts select new equilibri607fiFang, Jiang, & Tambeum robust rational attackers constrained attack time interval [tk , tk+1 ]. discuss below, flow-adjust focuses weaker form dominance,implies larger set strategies dominated (and thus could potentiallyeliminated) compared standard notion dominance used route-adjust; however flow-adjust guarantee elimination dominated strategies.denote DefEUkf defender expected utility attacker constrained attacktime interval [tk , tk+1 ] attacker provides best response given defender strategy f . Formally, DefEUkf = minq{1...L},t[tk ,tk+1 ] {DefEUf (Fq , t)}. givefollowing definition local dominance.Definition 10. Defender strategy f locally dominates f 0 DefEUkf DefEUkf 0 , k.2Corollary 3. Defender strategy f locally dominates f 0minq{1...L},t[tk ,tk+1 ]{DefEUf (Fq , t)}minq{1...L},t[tk ,tk+1 ]{DefEUf 0 (Fq , t)}, k,equivalently zero-sum game,maxq{1...L},t[tk ,tk+1 ]{AttEUf (Fq , t)}maxq{1...L},t[tk ,tk+1 ]{AttEUf 0 (Fq , t)}, k.Corollary 3 follows fact attacker plays best response givendefender strategy, means f locally dominates f 0 maximum attackerexpected utilities time interval [tk , tk+1 ] given f greater f 0 .Compared Definition 7, gives standard condition dominance, localdominance weaker condition; is, f dominates f 0 f locally dominates f 0 ,however converse necessarily true. Intuitively, whereas Definition 7 attackerplay (possibly suboptimal) strategy, attackers possible deviationsbest response restricted. result, set locally dominated strategiesincludes set dominated strategies. Definition 10, f locally dominates f 0 ,attacker rational (i.e., still playing best response) constrained attacktime interval [tk , tk+1 ], f preferable f 0 defender. corollaryeven rational attacker constrained attack unionintervals, f still preferable f 0 f locally dominates f 0 . One intuition localdominance concept following: suppose suspect attacker restricted(unknown) subset time, due logistical constraints. logistical constraintswould likely make restricted time subset contiguous union small numbercontiguous sets. Since sets well-approximated unions intervals [tk , tk + 1],local dominance serve approximate notion dominance respectattackers.Flow-adjust looks defender strategy f 1 locally dominates original defenderstrategy f 0 . achieve this, simply adjust flow distribution variables f (i, j, k)keeping marginal probabilities p(i, k) same. Figure 8 shows example gametwo discretized intervals [t1 , t2 ] [t2 , t3 ] (only first interval shown). Supposemaximal attacker expected utility 5U0 equilibrium attained second2. dont require exists least one k DefEUkf > DefEUkf 0 .608fiProtecting Moving Targets Multiple Mobile Resourcesinterval [t2 , t3 ]. attackers utility success constant U0 first interval[t1 , t2 ], defender strategy [t1 , t2 ] could arbitrarily chosen attackersexpected utility [t1 , t2 ] worst case smaller attackers best response[t2 , t3 ]. However, attacker constrained attack [t1 , t2 ] only, defender strategyfirst interval make difference. example, one target movingd1 d2 [t1 , t2 ]. schedule ferry shown dark lines parallellines L11 L21 respect protection radius = 0.2(d2 d1 ) shown dashedlines. marginal distribution probabilities p(i, k) 0.5 protection coefficientC1 = 1. f 0 , defenders strategy taking edges E1,1,1 E2,2,1 probability0.5 attackers maximum expected utility U0 , achieved around time(t1 + t2 )/2 neither two edges E1,1,1 E2,2,1 within targets protectionrange. adjust flows edge E1,2,1 E2,1,1 , shown Figure 8(b), attackersmaximum expected utility [t1 , t2 ] reduced 0.5U0 edge E1,2,1 within targetsprotection range way. rational attacker constrained attack[t1 , t2 ] get lower expected utility given defender strategy f 1 given f 0 , thusequilibrium f 1 robust kind deviation attacker side.(a) f 0 : patroller takingedges E1,1,1 E2,2,1probability 0.5.(b) f 1 : patroller takingedges E1,2,1 E2,1,1probability 0.5.Figure 8: example flow adjust. rational attacker constrained attack[t1 , t2 ] choose attack around time (t1 + t2 )/2 get utility U0 given f 0attack around t1 t2 get utility 0.5U0 given f 1 .flow-adjust, construct 1 new linear programs, one time interval[tk , tk +1 ], k = 1 . . . 1 find new set flow distribution probabilities f (i, j, k )achieve lowest local maximum [tk , tk +1 ] unchanged p(i, k ) p(i, k + 1).609fiFang, Jiang, & Tambelinear program interval [tk , tk +1 ] shown below.min vf (i,j,k )f (i, j, k ) = 0, |dj di | > vmnXp(i, k + 1) =f (j, i, k ), {1 . . . n}j=1p(i, k ) =nXf (i, j, k ), {1 . . . n}j=1v AttEU (Fq , tk ), q {1 . . . L}, k {k , k + 1}(r+1)r+v max{AttEU (Fq , qk), AttEU (Fq , qk)}q {1 . . . L}, r {0 . . . Mqk }linear program appears similar linear program CASS,significant differences. Unlike CASS, marginal probabilities p(i, k ) knownconstants provided input mentioned above, separate program[tk , tk +1 ]. Thus, get f (i, j, k ) local maximum [tk , tk +1 ]minimized. Denote minimum vk1 . original flow distribution f 0 , getAttEUf 0 (Fq , t) denote original local maximum value [tk , tk +1 ] vk0 .subset {f 0 (i, j, k )} original flow distribution f 0 feasible solution linearprogram above, vk1 vk0 , noting equality happens intervalattackers best response chosen.Note change made f (i, j, k) interval [tk , tk +1 ] affectperformance f intervals marginal probabilities p(i, k) kept same,i.e., changing f (i, j, k ) based linear program independent changef (i, j, k), k 6= k . solve 1 linear programs independently.calculating f (i, j, k ) k = 1..M 1, get new defender strategy f 1combining solutions f (i, j, k ) different linear programs together. vk1 vk0 ,maxq{1...L},t[tk ,tk +1 ]AttEUf 0 (Fq , t)maxq{1...L},t[tk ,tk +1 ]AttEUf 1 (Fq , t)k = 1..M 1, i.e., f 1 locally dominates f 0 .hand, restricted strategies p(i, k),may exist another strategy f 2 different set p(i, k) locally dominates f 1 .Finding locally dominating strategies different p(i, k) original topicfuture research.Although two refinement approaches provide necessarily lead nondominated strategy corresponding dominance definition, two approachesguaranteed find robust (or least indifferent) equilibrium facedconstrained attackers compared original equilibrium obtain CASS. Clearly,two refinement approaches exhaust space refinement approachesrefinement approaches possible may lead equilibria better610fiProtecting Moving Targets Multiple Mobile Resources(e.g. dominate) one found CASS. However, likely different defenderstrategies resulting different equilibrium refinements comparableterms dominance, i.e., constrained attackers, one equilibrium might turnbetter constrained attackers, another equilibrium might better.computational costs may differ well. Thus, understanding space refinementapproaches terms computational cost output quality, determiningapproach adopted circumstances important challenge futurework.5. Extension Two-Dimensional SpaceDASS CASS presented Section 3 based assumptiontargets patrollers move along straight line. However, complex modelneeded practical domains. example, Figure 9 shows part route mapWashington State Ferries, several ferry trajectories. number patrollerboats tasked protect ferries area, necessarily optimal simplyassign ferry trajectory patroller boat calculate patrolling strategiesseparately according CASS described Section 3. ferry trajectories closeother, patrolling strategy take account ferries areamuch efficient, e.g., patroller protect ferry moving Seattle Bremertonfirst, change direction halfway protect another ferry moving BainbridgeIsland back Seattle.Figure 9: Part route map Washington State Ferriessection, extend previous model complex case, targets patrollers move two-dimensional space provide corresponding linearprogram-based solution. use single defender resource example, generalize multiple defenders end section.5.1 Defender Strategy 2-Done-dimensional case, need discretize time space defendercalculate defenders optimal strategy. time interval discretized settime points = {tk }. Let G = (V, E) represents graph set vertices Vcorresponds locations patrollers may at, discretized time points, E set feasible edges patrollers take. edge e E satisfies611fiFang, Jiang, & Tambemaximum speed limit patroller possibly practical constraints (e.g., smallisland may block edges).5.2 DASS 2-Dattack occurs discretized time points, linear program DASSdescribed Section 3 applied two-dimensional settings distanceConstraint 9 substituted Euclidean distance 2-D space nodes Vi Vj .minv(30)f (i,j,k),p(i,k)f (i, j, k) [0, 1], i, j, k(31)f (i, j, k) = 0, i, j, k ||Vj Vi || > vm(32)p(i, k) =NXf (j, i, k 1), i, k > 1(33)f (i, j, k), i, k <(34)j=1p(i, k) =NXj=1NXp(i, k) = 1, k(35)i=1v AttEU(Fq , tk ), q, k(36)Note f (i, j, k) represents probability patroller moving node ViVj [tk , tk+1 ]. Recall Figure 2.1, patroller protects targets within protectivecircle radius . However, one-dimensional space, care straightline AB, used q (t) = [max{Sq (t) , d1 }, min{Sq (t) + , dN }] protectionrange target Fq time t, essence line segment. contrast, wholecircle needs considered protection range two-dimensional spaceextended protection range written q (t) = {V = (x, y) : ||V Sq (t)|| }.change affects value I(i, q, k) thus value AttEU (Fq , tk ) Constraint 36.5.3 CASS 2-Dattacking time chosen continuous time interval , needanalyze problem similar way Section 3.3. protection radius ,means patrollers located within circle whose origin Sq (t) radiusprotect target Fq . assume target change speed directiontime [tk , tk+1 ], circle also move along line 2-D space. trackcircle 3-D space x axes indicate position 2-D z axistime, get oblique cylinder, similar cylinder except topbottom surfaces displaced (See Figure 10). patroller movesvertex Vi ( V ) vertex Vj time [tk , tk+1 ], protects targetwithin surface. 3-D space described above, patrollers movementrepresented straight line.612fiProtecting Moving Targets Multiple Mobile ResourcesVVrFigure 10: illustration calculation intersection points two-dimensionalsetting. x axes indicates position 2-D z axistime. simplify illustration, z axis starts time tk . example,two intersection points occurring time points ta tb .Intuitively, two intersection points patrollers route3-D space surface. proved analytically calculating exacttime intersection points. Assume patroller moving V1 = (x1 , y1 )V2 = (x2 , y2 ) target moving Sq (tk ) = (x1 , y1 ) Sq (tk+1 ) = (x2 , y2 )[tk , tk+1 ] (an illustration shown Figure 10). get time intersection points,solve quadratic equation coordination parameters protection radius. present detailed calculation Appendix B. root quadratic equationwithin interval [tk , tk+1 ], indicates patrollers route intersectssurface time point. two intersection points. findintersection points, analysis Section 3.3 applies claimLemma 1. conclude need consider attackers strategiesr one-dimensional case denoteintersection points. use notation qksorted intersection points get following linear program 2-D case.minv(37)f (i,j,k),p(i,k)subject constraints(31 . . . 36)(r+1)r+v max{AttEU(Fq , qk), AttEU(Fq , qk)}(38)k {1 . . . }, q {1 . . . L}, r {0 . . . Mqk }Algorithm 1 still used add constraints linear program CASS2-D case. main difference compared CASS 1-D case since Euclideandistance 2-D used Constraint 32 need use extended definition q (t)2-D deciding entries coefficient matrix Arqk (i, j).multiple defender resources, linear program described Section 3.4 applicable extended definition q (t) used calculate AttEU Constraint 19613fiFang, Jiang, & Tambesubstituted following constraint:f (i1 , j1 , . . . , iW , jW , k) = 0, i1 , . . . , iW , j1 , . . . , jW u, kVju Viu k > vm t.6. Route Samplingdiscussed generate optimal defender strategy compact representation; however, defender strategy executed taking complete route.need sample complete route compact representation. section,give two methods sampling show corresponding defender strategy fullrepresentation methods applied.first method convert strategy compact representation Markovstrategy. Markov strategy setting defender strategy patrollersmovement tk tk+1 depends location patroller tk . denote(i, j, k) conditional probability moving di dj time tk tk+1 givenpatroller located di time tk . words (i, j, k) represents chancetaking edge Ei,j,k given patroller already located node (tk , di ). Thus, givencompact defender strategy specified f (i, j, k) p(i, k),(i, j, k) = f (i, j, k)/p(i, k), p(i, k) > 0.(39)(i, j, k) arbitrary number p(i, k) = 0. get sampled route firstdetermining start patrolling according p(i, 1); tk , randomly choosego tk tk+1 according conditional probability distribution (i, j, k).distribution sampling procedure matches given marginal variablesedge Ei,j,k sampled probability p(i, k)(i, j, k) = f (i, j, k). sampling methodactually leads full representation route Ru = (dru (1) , dru (2) , ..., dru (M ) ) sampledQ 1probability p(ru (1), 1)k=1 (ru (k), ru (k + 1), k), product probabilityinitial distribution probability taking step. method intuitivelystraightforward patrol route decided online patrol, i.e.,position patroller tk+1 decided patroller reaches position tk ,makes defender strategy unpredictable. downside methodnumber routes chosen non-zero probability high N .2-D case, patroller located node Vi time tk . sampling process exactly(i, j, k) used denote probability moving Vi Vj[tk , tk+1 ].second method sampling based decomposition process mentionedSection 4.1 (step (i)). discussed first sampling method, samplingessentially restoring full representation compact representation. shownTable 1, multiple ways assign probabilities different routes decomposition process route-adjust constructively defines one them. make useinformation get process, sample route according probabilityassigned decomposed route. number routes chosen non-zero probabilityN 2 , much less first method thus becomes feasible describestrategy full representation, providing routes chosen positive probability. Different sampling approaches may necessitated different application614fiProtecting Moving Targets Multiple Mobile Resourcesrequirements. applications might require defender obtain strategy fullrepresentation presented small number pure strategies. However,applications, strategy decided on-line, potentially hand-held smartphone (Luber, Yin, Fave, Jiang, Tambe, & Sullivan, 2013) may preferred.Therefore, based needs application, different sampling strategies mightselected.7. Evaluationuse different settings ferry protection domain compare performanceterms attackers expected utility AttEU(Fq , t). zero-sum game, lowervalue AttEU indicates higher value defenders expected utility.run experiments 1-D 2-D setting. evaluate performanceCASS show sampling results. also evaluate improvement tworefinement approaches 1-D. Section 7.1 shows results 1-D setting; Section7.2 2-D setting.7.1 Experiments One Dimensional Setting1-D setting, first evaluate performance solvers show muchperformance improved using refinement methods. also show sampledroutes example setting evaluate CASS varying number patrollers.7.1.1 Experimental Settingsused following setting experiments one dimensional case. complexspatio-temporal game; rather discrete security game previous work.three ferries moving terminals B total distance AB = 1.simulation time 30 minutes. schedules ferries shown Figure 11,x-axis indicates time y-axis distance terminal A. Ferry 1Ferry 3 moving B Ferry 2 moving B A. maximum speedpatrollers vm = 0.1/min protection radius = 0.1. Experimentsone-dimensional case using 2 patrollers (where C1 = 0.8, C2 = 1.0), exceptSection 7.1.5 report experiments different numbers patrollers.distance10.500Ferry1Ferry2Ferry310time2030Figure 11: Schedules ferries615fiFang, Jiang, & Tambe7.1.2 Performance Solverscompare strategies calculated CASS DASS baseline strategy.baseline strategy, two patrollers choose ferry probability 1/3 (uniformlyrandom) move alongside offer full protection, leaving two unprotected(strategy observed practice). First wished stress-test CASS using complexutility functions realistic case follows. Therefore, tested 4 differentdiscretization levels (details discretization levels included Table 4) randomutilities, discretization level, created 20 problem instances. probleminstances different across levels. ferry protection domain, utility functionferry usually depends ferrys position, instance utilities uniformlyrandomly chosen [0, 10] discretized distance points; example shownFigure 12(a). chosen discretization levels ensured Uq (t) lineartime interval [tk , tk+1 ] target Fq . Figure 12(a), x-axis indicates distanceterminal A, y-axis indicates utility successful attack ferry locateddistance d. Figure 12(b), x-axis plots four discretization levels y-axis plotsaverage attacker expected utility plays best response 20 instances baseline,DASS CASS. CASS shown outperform DASS baseline differencesstatistically significant (p < 0.01). Note different sets instances generateddifferent discretization levels, cannot compare results across levels directly.However, helpful better understanding models. figure, findsolution quality DASS varies lot sometimes worse naive strategy(e.g., level 1). DASS calculates optimal solution considersattacks discretized time points. Figure 12(b), solution quality measuredAttEU , calculated maximum continuous attacker strategy set.gap optimal objective function DASS actual AttEU givenoptimal solution DASS may vary different strategies different discretization levels.Another interesting observation average solution quality CASS almostdiscretization levels. Despite difference instance sets, result impliesimprovement finer discretization may limited CASS.Level1234(minutes)1052.524713160.50.250.1250.1N35911Table 4: Details discretization levels. experiments mentioned section,distance space evenly discretized, parameterized = di+1 di .Next turn realistic utility function ferry domain, U -shapeinverse U -shape. Figure 13(a) shows sample utility curve attacker gainshigher utility closer shore. fix utility shore 10, vary utilitymiddle (denoted Umid ), value floor U -shape topinverse U -shape evaluate strategies. Figure 13(b), Umid shown x-axis616fiProtecting Moving Targets Multiple Mobile Resources8Ave(AttEUm)U utility105000.5distance1(a) Randomized attacker utility functionNAIVEDASSCASS6420Level1 Level2 Level3 Level4(b) Average solution quality differentstrategiesFigure 12: Performance different randomized utility function settings. utilityfunction set experiments function distance Terminal A.utility function piece-wise linear value discretized distancepoints di chosen randomly [0,10].15Sup(AttEU)U utility1086400.5distance5001(a) Realistic attacker utility functionUmid = 510NAIVEDASSCASS510Umid1520(b) Solution quality different strategiesFigure 13: Performance different realistic utility function settings. utility function U-shape inverse U-shape. utility around distance 0.5 denotedUmid . compare defender strategy given DASS CASSbaseline Umid changing 1 20.compare performance strategies terms attackers expected utilityplays best response y-axis. conclude 1) strategy calculated CASSoutperforms baseline DASS; 2) DASS may actually achieve worse resultsbaseline.Among different experiment settings discretization utility function,choose one instance provide detailed analysis it. refer instanceexample setting following section. example setting, discretizationlevel 4 used utility curve shown Figure 13(a), parameters involveddescribed Section 7.1.1. Figure 14 compares attacker expected utility functionDASS CASS used respectively. x-axis indicates time t, y-axisindicates attackers expected utility attacks Ferry 1 time t. strategy calculated DASS, worst performance discretized time points 3.50 (AttEU(F1 , 20)),however, supremum AttEU(F1 , t), [0, 30] high 4.99 (AttEU(F1 , 4+ )),617fiFang, Jiang, & Tambe5AttEU43210DASSCASS1020time30Figure 14: attackers expected utility function given defender strategy calculatedDASS vs CASS example setting. expected utilities discretizedtime points indicated squares CASS dots DASS. maximumAttEU CASS 3.82, 30%less maximum AttEUDASS, 4.99.experimentally shows taking consideration attacks discretized time points necessary. strategy calculated CASS, supremumAttEU(F1 , t) reduced 3.82.7.1.3 Improvement Using Refinement Methodscompare refinement approaches described Section 4 analyze tradeoffperformance improvement runtime. Three approaches considered comparison: route-adjust, flow-adjust variation route-adjust, denoted route-adjust2.step (ii) route-adjust, replace every node route one-by-one sequence.3step (ii) route-adjust2, replace every consecutive pair nodes route sequence.first show results example setting. Figure 15(a), compare AttEU(Fq , t)function defender strategy given CASS one route-adjust Ferry1. shows attack aiming target time, defender strategyroute-adjust refinement equally good better one original equilibrium,thus defender performs equally better matter attacker constrainedtime, i.e., defender strategy route-adjust dominates original strategy. Figure15(b) comparison AttEU function defender strategy route-adjustone route-adjust2 Ferry 1. one route-adjust2 dominateone route-adjust overall former appears perform better latterfrequently larger amounts. use average value AttEU functionmetric performance, show route-adjust2 better route-adjustexample setting later Table 5. Figure 15(c) shows comparison AttEUfunction defender strategy given CASS defender strategy3. supplementary experiments, also tested route-adjust iterations, e.g., repeatingprocess replacing every node sequence five times. extra benefit insignificantruntime increases proportionally number iterations. light this, choose replacenode experiments reported article.618fi4433AttEUAttEUProtecting Moving Targets Multiple Mobile Resources2CASSRouteAdjust1010time2021030(a) AttEU function Ferry 1route-adjust (one node time)RouteAdjustRouteAdjust210time2030(b) AttEU function Ferry 1route-adjust2 (two nodes time)AttEU43210CASSFlowAdjust10time2030(c) Performance flow-adjustFigure 15: Performance equilibrium refinement approaches.flow-adjust Ferry 1. strategy given CASS dominated oneflow-adjust Definition 7, investigate maximum AttEU timeinterval [tk , tk+1 ], shown Table 6, find defender strategy flow-adjustlocally dominates original strategy.list worst case performance average performance AttEU functionferries example setting four defender strategies (CASS, route-adjust, routeadjust2, flow-adjust) Table 5, conclude 1) worst case performancestrategies flow-adjust same, means defender achieves exactlyexpected utility towards unconstrained rational attacker; 2) average performanceflow-adjust slightly better CASS, outperformed route-adjustroute-adjust2, takes much less time run compared two; 3)example setting, adjust two consecutive nodes time, performance betteradjusting one node time, difference significant muchexpensive terms run-time.StrategiesCASSRoute-AdjustRoute-Adjust2Flow-AdjustWorst Case Performance3.823.823.823.82Average Performance3.402.882.763.34Runtime (minutes)8.9632.310.50Table 5: Comparison different refinement approaches terms average performanceruntime. runtime refinement process calculated.619fiFang, Jiang, & Tambetime interval [tk , tk+1 ][2, 4][4, 6][6, 8][8, 10][10, 12][12, 14][14, 16]maximum3.75873.81823.81533.81373.80523.80503.7800maximum3.66753.81823.61643.63163.63163.56643.2100time interval [tk , tk+1 ][16, 18][18, 20][20, 22][22, 24][24, 26][26, 28][28, 30]maximum3.81113.81823.81823.81823.81823.81823.8182maximum3.72913.81823.81823.81823.81823.81823.8182Table 6: maximum attackers expected utility time interval decreasesflow-adjust used.Figure 16(a) Figure 16(b) shows maximum average improvementroute-adjust, route-adjust2 flow-adjust, averaged 20 instances Level 4randomized utilities used Figure 12(b); Figure 16(c) showsaverage runtime. maximum improvement largest difference AttEUfunction given defender strategy calculated CASS one refinement.average improvement average difference two functions. standarddeviations instances shown error bars. Figure 16 confirms refinement approaches improve defender strategy calculated CASS termsmaximum performance average performance thus provide better defender strategies given possible constrained attackers. Route-adjust2 achieves improvement,route-adjust, flow-adjust least. Flow-adjust achieves much less improvementcompared two approaches. One explanation constraintsstrong require marginal probabilities unchanged likelylittle changes made original defender strategy. difference routeadjust2 route-adjust significant. terms run-time, flow-adjust leastexpensive, route-adjust second route-adjust2 most. Route-adjust2 significantly expensive compared two. conclude route-adjustbetter choice considering tradeoff improvement runtime.7.1.4 Sampled Routesfirst convert defender strategy example setting Markov strategysample 1000 pair patrol routes. defender strategy used oneroute-adjust. sample, pair routes chosen step step two patrolboats according joint conditional probability distribution {(i1, j1, i2, j2, k)}.routes two patrol routes chosen simultaneously coordinatingother. cannot show pair separately 1000 samples. Instead, Figure17(a) shows frequency taken 1000 samples edge. x-axisindicates time y-axis distance terminal A. width edgeindicates frequency chosen least one patroller. Although Figure 17(a)precisely depict samples, provides rough view routes takenpatrol boats.620fiProtecting Moving Targets Multiple Mobile Resources0.6routeadjustrouteadjust2flowadjust2Ave ImprovementMax Improvement2.51.510.5routeadjustrouteadjust2flowadjust0.40.200Runtime (minutes)(a) Average maximal improvement(b) Average average improvementrouteadjustrouteadjust2flowadjust403020100(c) Average runtimeFigure 16: Comparison refinement approaches.Figure 17(b) shows pair routes highest probability usedecomposition method sampling. solid lines show patrol boats routesdashed lines show ferries schedules. get 3958 different pair patrol routes totaldecomposition process shown pair routes chosen probability 1.57%.1distancedistance10.80.60.40.2005101520250.80.60.20030timePatrol Boat 1Patrol Boat 20.451015202530time(a)(b)Figure 17: Results sampling example setting: (a) Frequency edgechosen first sampling method based Markov strategy used. (b)Decomposed routes highest probability superimposed ferry schedulessecond sampling method based decomposition used.621fiAttacker EU64log(Runtime (seconds))Fang, Jiang, & Tambe1 patroller2 patrollers3 patrollers4 patrollers201013Attacker EU4(b) Runtime Level 1log(Runtime (seconds))(a) Solution quality Level 151 patroller2 patrollers3 patrollers4 patrollers21 patroller2 patrollers3 patrollers3210(c) Solution quality Level 2321 patroller2 patrollers3 patrollers1012(d) Runtime Level 2Figure 18: Performance varying number patrollers.7.1.5 Number PatrollersFigure 18(a) shows improvement performance CASS increasing numberpatrollers discretization Level 1. x-axis shows number patrollersy-axis indicates average attackers maximal expected utility, i.e., expected rewardplays best response. results averaged 20 random utility settingsdiscretization Level 1. fewer patrollers, performance defender varies lotdepending randomized utility function (as indicated standard deviation shownerror bar). variance gets much smaller patrollers, meansdefender sufficient resources different instances. Figure 18(b) shows run-timeCASS. y-axis indicates average natural logarithm runtime. surprisingly,run-time increases number patrollers increases.Figure 18(c) 18(d) show average performance run-time CASS discretization Level 2, using set utility settings used Level 1. results1 3 patrollers shown. program runs memory 4 patrollersN 8 = 2734375 flow distribution variables least N 4 = 8757 constraints. Noteaverage solution quality Level 2 better result Level 1 (e.g.,average attacker EU 1 patroller 4.81 Level 1 4.13 Level 2), indicateshigher level granularity improve solution quality. However, granularity clearlyaffect ability scale-up; means need consider tradeoffsolution quality memory used one way combat scaling-up problemreduce level granularity. Nonetheless, number patrollers encounteredreal-world scenarios New York order 3 4, CASS capableleast key real-world scenarios.622fiProtecting Moving Targets Multiple Mobile Resources7.1.6 Approximation Approach Multiple Defender Resourcestested first approximation approach multiple defender resources describedSection 3.4 example setting. used fmincon function interior-pointmethod MATLAB minimize non-linear objective function (Equation 25). Table7 lists different run-time value objective function achieved given differentiteration number (denoted MaxIter ). function ensured provide feasiblesolution iteration number large enough, shown first two rows.compared result LP formulation DASS, implementedMATLAB using linprog function. DASS solved within 8.032 seconds providesoptimal solution AttEUm = 3.5, approximation approach outperformedrun-time efficiency solution quality. approach fails provide feasible solutionefficiently even sufficient time given (more 400 times run-timeLP formulation), maximum attacker expected utility 18% larger optimalsolution. mainly new formulation approximation approachlonger linear convex, making difficult find global maximum.axIter300010000900000Run time(sec)4.1417.213298AttEUminfeasibleinfeasible4.0537Table 7: Performance approximation approach.7.2 Experiments Two Dimensional Settingsettings 2-D space complex even single patroller. showexample setting motivated ferry system Seattle, Bainbridge islandBremerton shown Figure 9. example setting, three terminals (denoted A,BC) non-collinear 2-D space shown Figure 19(a). Ferry 1 Ferry2 moving trajectory Terminal B C (denoted Trajectory 1)Ferry 3 Ferry 4 moving trajectory Terminal B (denotedTrajectory 2). schedules four ferries shown Figure 19(b), x-axistime y-axis distance common terminal B. Ferry 1 movesC B, Ferry 2 moves B C, Ferry 3 moves B Ferry 4 movesB. Similar one-dimensional scenario ferry domain, assume utilitydecided ferrys position utility function shown Figure 19(c).x-axis distance common terminal B y-axis utility twotrajectories respectively. 2-D space discretized grid shown Figure 19(d)x = 1.5 = 1 indicating interval x-axis y-axis. patrollerlocated one intersection points grid graph discretized time points.simulation time 60 minutes = 13, i.e., tk+1 tk = 5 minutes. speed limitpatroller = 0.38 available edges patroller take[tk , tk+1 ] shown Figure 19(d). one patroller involved. protection radiusset = 0.5, protection coefficient C1 = 0.8.623fiFang, Jiang, & TambeC2distance Terminal BTerminals 2DTrajectory 1B1Trajectory 2001.534.5xFerry Schedules10.60.40.20060Edges Available2Ferry Trajectory1Ferry Trajectory26utility40(b) Ferry schedulesUtility Function820time(a) Three terminals10Ferry1Ferry2Ferry3Ferry40.8142000.20.40.60.80011.534.5xdistance Terminal B(c) Utility function(d) Available edgesFigure 19: example setting two-dimensional spaceFigure 20(a) compares performance DASS CASS Ferry 2. Ferry 2 chosenstrategies, attackers best response attack Ferry 2. x-axistime t, y-axis attacker expected utility attacking Ferry 1 time t.maximum AttEU CASS 6.1466, 12% lower compared result DASS,6.9817. Figure 20(b) 20(c) show two sampled route given strategy calculatedCASS 2-D map dashed lines represents ferry trajectories.patroller starts node text start follows arrowed route, endsnode text end end patrol. may stay nodes textstay. patrol routes shown intuitive way ambiguous. exactroute listed table time position. routes sampled basedconverted Markov strategy, total number patrol routes may chosennon-zero probability 4.49 1010 .8. Related Worksection discuss literature related work. first discuss workcomputation game-theoretic patrolling strategies, discuss work continuousgames, finally discuss work equilibrium refinement.mentioned introduction, Stackelberg games widely applied security domains, although work considered static targets (e.g., Korzhyket al., 2010; Krause, Roper, & Golovin, 2011; Letchford & Vorobeychik, 2012; Kiekintveld624fiProtecting Moving Targets Multiple Mobile ResourcesSampled Route CASS7stay6AttEU240staystaystartend1DASSCASS502040time6001.534.5x(a) Solution quality DASSand CASSFerry 2(b) Sampled route 1 superimposed ferrytrajectoriesSampled Route CASSstaystart2stayend1001.534.5x(c) Sampled route 2 superimposed ferrytrajectoriesFigure 20: Experimental results two-dimensional settingset al., 2013). Agmon, Kraus, Kaminka (2008) proposed algorithms computingmixed strategies setting perimeter patrol adversarial settings mobile robotpatrollers. Similarly, Basilico, Gatti, Amigoni (2009) computed randomized leader strategies robotic patrolling environments arbitrary topologies. Evenplayers mobile, e.g., hider-seeker games (Halvorson, Conitzer, & Parr, 2009),infiltration games (Alpern, 1992) search games (Gal, 1980), targets (if any) assumed static. Tsai et al. (2009) applied Stackelberg games domain schedulingfederal air marshals board flights. targets (i.e., flights) domain mobile,players restricted move along targets protect attack them.stationary nature leads discrete game models finite numbers pure strategies.Bosansky, Lisy, Jakob, Pechoucek (2011) Vanek, Jakob, Hrstka, Pechoucek(2011) studied problem protecting moving targets. However, consideredmodel defender, attacker targets discretized movementsdirected graph. discretization attacker strategy spaces introduce suboptimalitysolutions, shown DASS. We, work, generalize strategy space attacker continuous realm compute optimal strategies evensetting. Furthermore, provide efficient scalable linear formulation,Bosansky et al. presented formulation non-linear constraints, faced problemsscaling larger games even single defender resource.625fiFang, Jiang, & TambeYin et al. (2012) considered domain patrolling public transit networks (suchLA Metro subway train system) order catch fare evaders. playersride along trains follow fixed schedule, domain inherently discretemodeled patrolling problem finite zero-sum Bayesian game. Yin et al. proposedcompact representation defender mixed strategies flows network. adaptcompact representation idea continuous domain. particular, domain needmodel interaction defenders flow attackers continuous strategyspace. proposed sub-interval analysis used spatio-temporal reasoning efficientlyreduce problem finite LP.Games continuous strategy spaces well-studied game theory. Mucheconomics literature focused games whose equilibria solved analytically(and thus question computation arise), example classical theoryauctions (see e.g., Krishna, 2009). Recent computational approaches analysisdesign auctions focused discretized versions auction games (e.g.,Thompson & Leyton-Brown, 2009; Daskalakis & Weinberg, 2012). researchefficiently solving two-player continuous games specific types utility functions,zero-sum games convex-concave utility functions (Owen, 1995) separablecontinuous games polynomial utility functions (Stein, Ozdaglar, & Parrilo, 2008).Johnson, Fang, Tambe (2012) studied continuous game model protecting forestsillegal logging. model target (i.e., forest) stationary,simplifying assumptions (e.g., forest circular shape) able solvegame efficiently. contrast existing work, game model moving targetscontinuous domain, resulting utility functions discontinuous thus existingapproaches applicable. CASS algorithm solves game optimally withoutneeding discretize attackers strategy space.extensive literature equilibrium refinement; however existing workcomputation equilibrium refinement focuses finite games. simultaneousmove finite games, solution concepts perfect equilibrium proper equilibriumproposed refinements Nash equilibrium (Fudenberg & Tirole, 1991). MiltersenSrensen (2007) proposed efficient algorithm computing proper equilibria finite zero-sum games. finite security games, et al. (2011) proposed refinementStackelberg equilibrium techniques computing refinements. resultingdefender strategy robust possibilities constrained capabilities attacker.existing approaches rely finiteness action sets, thus applicablesetting. Simon Stinchcombe (1995) proposed definitions perfect equilibriumproper equilibrium infinite games continuous strategy sets, howeverpropose computational procedure resulting solution concepts. Exact computation equilibrium refinements continuous games MRMTsg remains challengingopen problem.9. Conclusionpaper makes several contributions computing optimal strategies given moving targets mobile patrollers. First, introduce MRMTsg , novel Stackelberg game modeltakes consideration spatial temporal continuity. model, targets move626fiProtecting Moving Targets Multiple Mobile Resourcesfixed schedules attacker chooses attacking time continuous timeinterval. Multiple mobile defender resources protect targets within protectionradius, bring continuous space analysis. Second, develop fast solutionapproach, CASS, based compact representation sub-interval analysis. Compact representations dramatically reduce number variables designing optimal patrolstrategy defender. Sub-interval analysis reveals piece-wise linearity attackerexpected utility function shows finite set dominating strategies attacker. Third, propose two approaches equilibrium refinement CASSs solutions:route-adjust flow-adjust. Route-adjust decomposes patrol routes, greedily improvesroutes composes new routes together get new defender strategy. Flowadjust fast simple algorithm adjusts flow distribution achieve optimalitytime interval keeping marginal probability discretized time pointsunchanged. Additionally, provide detailed experimental analyses ferry protectiondomain. CASS deployed US Coast Guard since April 2013.10. Future Workseveral important avenues future work. include: (i) use decreasingfunction model protection provided targets instead using fixed protectionradius; (ii) handle practical constraints patrol boat schedule easily implementable; (iii) efficiently handle complex uncertain target schedules utilityfunctions.provide initial discussion relaxation assumptionslisted Section 2 used throughout paper:allow complex uncertain target schedules, may model problemgame targets follow stochastic schedules. framework may still applymay need enriched (e.g., using approaches use MDPs representdefender strategies, see Jiang, Yin, Zhang, Tambe, & Kraus, 2013). Coordinatingmultiple defenders becomes important challenge. may helpfulcases appeal prior work multi-agent teamwork, givensignificant uncertainty cases leading need on-line coordination(Tambe, 1997; Stone, Kaminka, Kraus, & Rosenschein, 2010; Kumar & Zilberstein,2010; Yin & Tambe, 2011).focus environments multiple attackers coordinate attacks,may need enhance framework. Prior results Korzhyk,Conitzer, Parr (2011) stationary targets discrete time would helpfuladdressing challenge, although case moving targets continuous spacetime cases provides significant challenge. Combiningprevious item future work, complex multiple defender multiple attacker scenariowould appear significant computational challenge.627fiFang, Jiang, & TambeAcknowledgmentsthank USCG officers, particularly Craig Baldwin, Joe Direnzo FrancisVarrichio officers sector New York, exceptional collaboration. viewsexpressed herein author(s) construed official reflectingviews Commandant U.S. Coast Guard. research supportedUS Coast Guard grant HSHQDC-10-D-00019 MURI grant W911NF-11-1-0332.also thank anonymous reviewers valuable suggestions.preliminary version work appears conference paper (Fang, Jiang, &Tambe, 2013). several major advances article: (i) Whereas earlier workconfined targets move 1-D space, provide significant extension algorithms(DASS CASS) article enable targets patrollers move 2D space; also provide detailed experimental results 2-D extension. (ii)provide additional novel equilibrium refinement approaches experimentally compareperformance equilibrium refinement approach offered earlier work;allows us offer improved understanding equilibrium refinement space. (iii)discuss several sampling methods detail sample actual patrol routes mixedstrategies generate discussion missing earlier work. (iv) providedetailed proofs omitted previous version work.628fiProtecting Moving Targets Multiple Mobile ResourcesAppendix A. Notation TableNotationMRMTMRMTsgLFqA, BSq (t)WPuvmCGUq (t)NtkdiRuru (k)f (i, j, k)p(i, k)Ei,j,kp(Ru )AttEU(Fq , t)q (t)(Fq , t)I(i, q, k)L1q ,L2qrqkrAttEU(Fq , qk)MqkArqk (i, j)E(u, k)Meaningproblem multiple Mobile Resources protecting Moving TargetsGame model continuous set strategies attacker MRMT.Number ferries.Ferry index q.Terminal points.Continuous time interval finite set time points.Continuous space possible locations set distance points.Ferry schedule. Position target Fq specified time t.Number patrollers.Patroller index u.Speed limit patroller.Protection radius patroller.Probability attacker stopped G patrollers.Positive reward successful attack target Fq time attacker.Number discretized time points.Number discretized distance points.Discretized time point.Discretized distance point.Distance two adjacent time points.Patrol route patroller Pu . discretization defenders strategy space,Ru described vector.patroller located dru (k) time tk .Flow distribution variable. Probability patroller moves di djtime [tk , tk+1 ].Marginal distribution variable. Probability patroller located di tk .directed edge linking nodes (tk , di ) (tk+1 , dj ).Probability taking route Ru .Attacker expected utility attacking target Fq time t.Protection range target Fq timeProbability patroller protecting target Fq time t.Whether patroller located di time tk protecting target Fq .Lines Sq (t) .rth intersection point [tk , tk+1 ] respect target Fq .r .Left/right-side limit AttEU(Fq , t) qkNumber intersection points [tk , tk+1 ] respect target Fq .r , r+1 ]; 0 otherwise.C1 patroller taking edge Ei,j,k protect target Fq [qkqkShort Eru (k),ru (k+1),k .Table 8: Summary notations involved paper.629fiFang, Jiang, & TambeAppendix B. Calculation Intersection Points CASS 2-D Settingscalculate time patrollers route intersects protection rangetarget patroller moving V1 = (x1 , y1 ) V2 = (x2 , y2 ) targetmoving Sq (tk ) = (x1 , y1 ) Sq (tk+1 ) = (x2 , y2 ) [tk , tk+1 ]. patrollersposition given time [tk , tk+1 ] denoted (x, y) targets position denoted(x, y).tk(x2 x1 ) + x1 ,tk+1 tktk(x2 x1 ) + x1 ,x =tk+1 tktk(y2 y1 ) + y1tk+1 tktk=(y2 y1 ) + y1tk+1 tkx=y=(40)(41)intersection point, distance patrollers position targets positionequals protection radius , looking time(x x)2 + (y y)2 = re2(42)substituting variables Equation 42 Equations 4041, denoting(x2 x1 ) (x2 x1 ),tk+1 tk(y2 y1 ) (y2 y1 )A2 =,tk+1 tkA1 =B1 = x1 x1 ,B2 = y1 y1 ,Equation 42 simplified(A1 A1 tk + B1 )2 + (A2 A2 tk + B2 )2 = re2 .(43)Denote C1 = B1 A1 tk C2 = B2 A2 tk , easily get two rootsquadratic equation,p2(A1 C1 + A2 C2 ) 2 (A1 C1 + A2 C2 )2 (A21 + A22 )(C12 + C22 re2 )ta,b =.(44)2(A21 + A22 )ta tb time valid intersection point within time intervalconsideration ([tk , tk+1 ]).ReferencesAgmon, N., Kraus, S., & Kaminka, G. A. (2008). Multi-robot perimeter patrol adversarialsettings. IEEE International Conference Robotics Automation (ICRA), pp.23392345.Alpern, S. (1992). Infiltration Games Arbitrary Graphs. Journal Mathematical Analysis Applications, 163, 286288.630fiProtecting Moving Targets Multiple Mobile ResourcesAn, B., Kempe, D., Kiekintveld, C., Shieh, E., Singh, S. P., Tambe, M., & Vorobeychik, Y.(2012). Security games limited surveillance. Proceedings Twenty-SixthAAAI Conference Artificial Intelligence, pp. 12411248.An, B., Tambe, M., Ordonez, F., Shieh, E., & Kiekintveld, C. (2011). Refinement strongstackelberg equilibria security games. Proceedings Twenty-Fifth AAAIConference Artificial Intelligence (AAAI), pp. 587593.Basilico, N., Gatti, N., & Amigoni, F. (2009). Leader-follower strategies robotic patrolling environments arbitrary topologies. Proceedings 8th International Conference Autonomous Agents Multiagent Systems (AAMAS) Volume 1, pp. 5764.Bosansky, B., Lisy, V., Jakob, M., & Pechoucek, M. (2011). Computing time-dependentpolicies patrolling games mobile targets. 10th International ConferenceAutonomous Agents Multiagent Systems (AAMAS) - Volume 3, pp. 989996.Conitzer, V., & Sandholm, T. (2006). Computing optimal strategy commit to.Proceedings 7th ACM Conference Electronic Commerce, EC 06, pp. 8290.Daskalakis, C., & Weinberg, S. M. (2012). Symmetries optimal multi-dimensional mechanism design. Proceedings 13th ACM Conference Electronic Commerce,EC 12, pp. 370387.Fang, F., Jiang, A. X., & Tambe, M. (2013). Optimal patrol strategy protecting movingtargets multiple mobile resources. Proceedings 2013 International Conference Autonomous Agents Multi-agent Systems, AAMAS 13, pp. 957964.Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.Gal, S. (1980). Search Games. Academic Press, New York.Gatti, N. (2008). Game theoretical insights strategic patrolling: Model algorithmnormal-form. Proceedings 18th European Conference Artificial Intelligence(ECAI), pp. 403407.Greenberg, M., Chalk, P., & Willis, H. (2006). Maritime terrorism: risk liability. RandCorporation monograph series. RAND Center Terrorism Risk Management Policy.Halvorson, E., Conitzer, V., & Parr, R. (2009). Multi-step Multi-sensor Hider-Seeker Games.IJCAI.Jakob, M., Vanek, O., & Pechoucek, M. (2011). Using agents improve internationalmaritime transport security. Intelligent Systems, IEEE, 26 (1), 9096.Jiang, A. X., Yin, Z., Zhang, C., Tambe, M., & Kraus, S. (2013). Game-theoretic randomization security patrolling dynamic execution uncertainty. Proceedings2013 international conference Autonomous agents multi-agent systems,AAMAS 13, pp. 207214.Johnson, M. P., Fang, F., & Tambe, M. (2012). Patrol strategies maximize pristine forestarea. Proceedings Twenty-Sixth AAAI Conference Artificial Intelligence(AAAI), pp. 295301.631fiFang, Jiang, & TambeKiekintveld, C., Islam, T., & Kreinovich, V. (2013). Security games interval uncertainty. Proceedings 2013 International Conference Autonomous AgentsMulti-agent Systems, AAMAS 13, pp. 231238.Kiekintveld, C., Jain, M., Tsai, J., Pita, J., Ordonez, F., & Tambe, M. (2009). Computingoptimal randomized resource allocations massive security games. Proceedings8th International Conference Autonomous Agents Multiagent Systems- Volume 1, AAMAS 09, pp. 689696.Korzhyk, D., Conitzer, V., & Parr, R. (2010). Complexity computing optimal Stackelbergstrategies security resource allocation games. Proceedings 24th NationalConference Artificial Intelligence (AAAI), pp. 805810.Korzhyk, D., Conitzer, V., & Parr, R. (2011). Security games multiple attacker resources. Proceedings Twenty-Second international joint conference Artificial Intelligence - Volume Volume One, IJCAI11, pp. 273279. AAAI Press.Krause, A., Roper, A., & Golovin, D. (2011). Randomized sensing adversarial environments. Proceedings 22nd International Joint Conference ArtificialIntelligence (IJCAI), pp. 21332139.Krishna, V. (2009). Auction theory. Academic press.Kumar, A., & Zilberstein, S. (2010). Anytime planning decentralized POMDPs usingexpectation maximization. Proceedings Twenty-Sixth Conference Uncertainty Artificial Intelligence, pp. 294301.Letchford, J. (2013). Computational Aspects Stackelberg Games. Ph.D. thesis, DukeUniversity.Letchford, J., & Conitzer, V. (2013). Solving security games graphs via marginal probabilities. Proceedings Twenty-Seventh AAAI Conference Artificial Intelligence (AAAI), pp. 591597.Letchford, J., & Vorobeychik, Y. (2012). Computing optimal security strategies interdependent assets. Conference Uncertainty Artificial Intelligence (UAI),pp. 459468.Luber, S., Yin, Z., Fave, F. D., Jiang, A. X., Tambe, M., & Sullivan, J. P. (2013). Gametheoretic patrol strategies transit systems: trusts system mobile app(demonstration). International Conference Autonomous Agents MultiagentSystems (AAMAS)[Demonstrations Track], pp. 13771378.Marecki, J., Tesauro, G., & Segal, R. (2012). Playing repeated stackelberg games unknown opponents. Proceedings 11th International Conference AutonomousAgents Multiagent Systems, AAMAS 12, pp. 821828.Miltersen, P. B., & Srensen, T. B. (2007). Computing proper equilibria zero-sum games.Proceedings 5th International Conference Computers Games, CG06,pp. 200211.Owen, G. (1995). Game Theory (3rd ed.). Academic Press.632fiProtecting Moving Targets Multiple Mobile ResourcesParuchuri, P., Tambe, M., Ordonez, F., & Kraus, S. (2006). Security multiagent systemspolicy randomization. Proceedings fifth international joint conferenceAutonomous agents multiagent systems, AAMAS 06, pp. 273280.Pita, J., Jain, M., Marecki, J., Ordonez, F., Portway, C., Tambe, M., Western, C., Paruchuri,P., & Kraus, S. (2008). Deployed ARMOR protection: application game theoretic model security Los Angeles International Airport. Proceedings7th International Joint Conference Autonomous Agents Multiagent Systems:Industrial Track, AAMAS 08, pp. 125132.Pita, J., Jain, M., Ordonez, F., Portway, C., Tambe, M., Western, C., Paruchuri, P., &Kraus, S. (2009). Using game theory los angeles airport security.. AI Magazine,30, 4357.Shieh, E., An, B., Yang, R., Tambe, M., Baldwin, C., DiRenzo, J., Maule, B., & Meyer,G. (2012). PROTECT: deployed game theoretic system protect portsUnited States. Proceedings 11th International Conference AutonomousAgents Multiagent Systems - Volume 1, AAMAS 12, pp. 1320.Simon, L. K., & Stinchcombe, M. B. (1995). Equilibrium refinement infinite normal-formgames. Econometrica, 63 (6), 14211443.Stein, N. D., Ozdaglar, A., & Parrilo, P. A. (2008). Separable low-rank continuousgames. International Journal Game Theory, 37 (4), 475504.Stone, P., Kaminka, G. A., Kraus, S., & Rosenschein, J. S. (2010). Ad hoc autonomousagent teams: Collaboration without pre-coordination. Proceedings 24th AAAIConference Artificial Intelligence, pp. 15041509.Tambe, M. (1997). Towards flexible teamwork. JOURNAL ARTIFICIAL INTELLIGENCE RESEARCH, 7, 83124.Tambe, M. (2011). Security Game Theory: Algorithms, Deployed Systems, LessonsLearned. Cambridge University Press.Thompson, D. R. M., & Leyton-Brown, K. (2009). Computational analysis perfectinformation position auctions. Proceedings 10th ACM conference Electronic commerce, EC 09, pp. 5160.Tsai, J., Rathi, S., Kiekintveld, C., Ordonez, F., & Tambe, M. (2009). IRIS - toolstrategic security allocation transportation networks. Eighth InternationalConference Autonomous Agents Multiagent Systems - Industry Track, AAMAS09, pp. 3744.van Damme, E. (1987). Stability Perfection Nash equilibria. Springer-Verlag.Vanek, O., Jakob, M., Hrstka, O., & Pechoucek, M. (2011). Using multi-agent simulationimprove security maritime transit. Proceedings 12th InternationalWorkshop Multi-Agent-Based Simulation (MABS), pp. 116.Vorobeychik, Y., & Singh, S. (2012). Computing stackelberg equilibria discounted stochastic games. Proceedings Twenty-Sixth Conference Artificial Intelligence (AAAI), pp. 14781484.633fiFang, Jiang, & TambeYin, Z., Jiang, A. X., Johnson, M. P., Kiekintveld, C., Leyton-Brown, K., Sandholm, T.,Tambe, M., & Sullivan, J. P. (2012). TRUSTS: Scheduling randomized patrolsfare inspection transit systems. Proceedings Twenty-Fourth ConferenceInnovative Applications Artificial Intelligence (IAAI), pp. 23482355.Yin, Z., & Tambe, M. (2011). Continuous time planning multiagent teams temporalconstraints. Proceedings Twenty-Second international joint conferenceArtificial Intelligence - Volume Volume One, IJCAI11, pp. 465471. AAAI Press.634fiJournal Artificial Intelligence Research 48 (2013) 1-22Submitted 12/12; published 10/13Natural Language Inference Arabic Using Extended TreeEdit Distance SubtreesMaytham Alabbasmaytham.alabbas@gmail.comDepartment Computer Science, University Basrah,Basrah, IraqAllan RamsayAllan.Ramsay@manchester.ac.ukSchool Computer Science, University Manchester,Manchester, M13 9PL, UKAbstractMany natural language processing (NLP) applications require computation similarities pairs syntactic semantic trees. Many researchers used tree editdistance task, technique suffers drawback deals single node operations only. extended standard tree edit distance algorithmdeal subtree transformation operations well single nodes. extended algorithm subtree operations, TED+ST, effective flexible standardalgorithm, especially applications pay attention relations among nodes (e.g.linguistic trees, deleting modifier subtree cheaper sum deletingcomponents individually). describe use TED+ST checking entailmenttwo Arabic text snippets. preliminary results using TED+ST encouraging compared two string-based approaches standard algorithm.1. IntroductionTree edit distance widely used component natural language processing (NLP)systems attempt determine whether one text snippet supports inference another(roughly speaking, whether first entails second), distance pairsdependency trees taken measure likelihood one entails other.extend standard algorithm calculating distance two trees allowingoperations apply subtrees, rather single nodes. extension improvesperformance technique Arabic around 5% F-score around 4%accuracy compared number well-known techniques. relative performancestandard techniques Arabic testset replicates results reportedtechniques English testsets. also applied extended version tree editdistance, TED+ST, English RTE-2 testset, outperforms standardalgorithm.Tree edit distance generalisation standard string edit distance metric,measures similarity two strings. used underpin several NLPapplications information extraction (IE), information retrieval (IR) naturallanguage inference (NLI). edit distance two trees defined minimumcost sequence edit operations transform one tree another. numerousapproaches calculating edit distance trees, reported Selkow (1977), Taic!2013AI Access Foundation. rights reserved.fiAlabbas & Ramsay(1979), Zhang Shasha (1989), Klein (1998), Demaine, Mozes, Rossman, Weimann(2009) Pawlik Augsten (2011). chosen work Zhang-Shashasalgorithm (Zhang & Shasha, 1989) intermediate structures producedalgorithm allow us detect respond operations subtrees. referstandard tree edit distance algorithm throughout rest article, mean ZhangShashas algorithm, use short form ZS-TED.ultimate goal develop NLI system Arabic (Alabbas, 2011).1 NLIproblem determining whether natural language hypothesis h reasonably inferrednatural language premise p. challenges NLI quite differentencountered formal deduction: emphasis informal reasoning, lexical semanticknowledge, variability linguistic expression, rather long chains formalreasoning (MacCartney, 2009). recent, better-known, formulation NLItask recognising textual entailment challenge (RTE), described Dagan Glickman(2004) task determining, two text snippets premise p hypothesis h, whether. . . typically, human reading p would infer h likely true. Accordingauthors, entailment holds truth h, interpreted typical language user,inferred meaning p. popular method used recent yearstasks use tree edit distance, compares sentence pairs finding minimalcost sequence editing operations transform tree representation one sentencetree (Kouylekov, 2006; Heilman & Smith, 2010). Approximate tree matchingkind allows users match parts two trees, rather demanding completematch every element tree. However, one main drawbacks tree editdistance transformation operations applied solely single nodes (Kouylekov,2006). Kouylekov Magnini (2005) used standard tree edit distance, usestransformation operations (insert, delete exchange) solely single nodes, checkentailment two dependency trees. hand, Heilman Smith(2010) extended available operations standard tree edit distance INSERT-CHILD,INSERT-PARENT, DELETE-LEAF, DELETE-&-MERGE, RELABEL-NODE RELABEL-EDGE.authors also identify three new operations, MOVE-SUBTREE, means move node Xtree last child left/right side node (s.t.descendant X ), NEW-ROOT MOVE-SIBLING, enable succinct edit sequencescomplex transformation. extended set edit operations allows certain combinationsbasic operations treated single steps, hence provides shorter (and thereforecheaper) derivations. fine-grained distinctions between, instance, different kindsinsertions also make possible assign different weights different variationsoperation. Nonetheless, operations continue operate individual nodes rathersubtrees (despite name, even MOVE-SUBTREE appears defined operationnodes rather subtrees). solved problem extending basicversion algorithm costs operations insert/delete/exchange subtreesderived appropriate function costs operations parts.makes TED+ST effective flexible standard algorithm, especiallyapplications pay attention relations among nodes (e.g deleting modifier subtree,linguistic trees, cheaper sum deleting components individually).1. particular, Modern standard Arabic (MSA). refer Arabic throughout article,mean MSA.2fiNatural Language Inference Arabicrest paper organised follows: Zhang-Shashas algorithm, ZS-TED,explained Section 2. Section 3 presents TED+ST. Section 4 describes dependency treesmatching. Dataset preparation explained Section 5. experimental resultsdiscussed Section 6. Conclusions given Section 7.2. Zhang-Shashas TED Algorithmapproach extends ZS-TED, uses dynamic programming provide O(n4 )algorithm finding optimal sequence node-based edit operations transformingone tree another. section contains brief recapitulation algorithmadetailed description given Bille (2005).Ordered trees trees left-to-right order among siblings significant.Approximate tree matching allows us match tree parts another tree.three operations, namely deleting, inserting exchanging node,transform one ordered tree another. nonnegative real cost associatededit operation. costs changed match requirements specific applications.Deleting node x means attaching children parent x. Insertion inversedeletion. means inserted node becomes parent consecutive sub-sequenceleft right order parent. Exchanging node alters label. editingoperations illustrated Figure 1 (Bille, 2005).(a)l1l2(b)l1l1l2(b)l1l1l2Figure 1: (a) Relabeling node label (l1 l2 ). (b) Deleting node labeled (l2 ).(c) Inserting node labeled l2 child node labeled l1 ( l2 ).operation associated cost allowed single nodes only. Selectinggood set costs operations hard dealing complex problems.3fiAlabbas & Ramsayalterations costs choosing different combination leaddrastic changes tree edit distance performance (Mehdad & Magnini, 2009).ZS-TED algorithm, tree nodes compared using postorder traversal,visits nodes tree starting leftmost leaf descendant root proceedingleftmost descendant right sibling leaf, right siblings,parent leaf tree root. last node visited alwaysroot. example postorder traversal leftmost leaf descendant treeshown Figure 2. figure, two trees, T1 m=7 nodes T2 n=7nodes. subscript node considered order node postordertree. So, postorder T1 e,f,b,g,c,d,a postorder T2 g,c,y,z,x,d,a.leftmost leaf descendant subtrees T1 headed nodes e,f,b,g,c,d,a1,2,1,4,4,6,1 respectively, similarly leftmost leaf descendants g,c,y,z,x,d,a T21,1,3,4,3,3,1.a7a7c5b3e1f2d6g4c2d6g1x5y3T1z4T2Figure 2: Two trees T1 T2 postorder traversal.descendants node, least cost mapping calculatednode encountered, order least cost mapping selected right away.achieve this, algorithm pursues keyroots tree, definedset contains root tree plus nodes left sibling. Concentratingkeyroots critical dynamic nature algorithm, since subtreesrooted keyroots allow problem split independent subproblemsgeneral kind. keyroots tree decided advance, permitting algorithmdistinguish tree distance (the distance two nodes consideredcontext left siblings trees T1 T2 ) forest distance (the distancetwo nodes considered separately siblings ancestorsdescendants) (Kouylekov, 2006). illustration, keyroots tree Figure 2marked bold.node, computation find least cost mapping (the tree distance)node first tree one second depends solely mapping nodeschildren. find least cost mapping node, then, one needs recognise least costmapping keyroots among children, plus cost leftmost child.nodes numbered according postorder traversal, algorithm proceedsfollowing steps (Kouylekov, 2006): (i) mappings leaf keyroots determined;(ii) mappings keyroots next higher level decided recursively; (iii)root mapping found. Algorithm 1 shows pseudocode ZS-TED algorithm (Zhang& Shasha, 1989). matrices F used recording results individual4fiNatural Language Inference Arabicsubproblems: used store tree distance trees rooted pairs nodestwo trees, F used store forest distance sequences nodes.F used temporary store tree edit distance pairs keyrootscalculated. extended standard algorithm, computes costcheapest edit sequence, also records edit operations themselves.involves adding two new matrices, DPATH FDPATH, hold appropriate sequencesedit operationsDPATH hold edit sequences trees rooted pairs nodesFDPATH hold edit sequences forests. DPATH permanent arrays,whereas F FDPATH reinitialised pair keyroots.algorithm iterates keyroots, split two main stages pairkeyroots: initialisation phase (lines 312) deals first row column,assume every cell first row reached appending insert operationcell left every cell first column reached appending delete operationcell it, appropriate costs. exactly parallel initialisationstandard dynamic time warping algorithm calculating string edit distance, thoughtreating task matching subsets subtrees rooted T1 [x] T2 [y]string matching problem nodes two trees sequences enumeratedpost-order.second stage (lines 1337) traces cost edit sequence transformingsub-sequence sequence nodes dominated T1 [x] sub-sequence sequencenodes dominated T2 [x], considering whether nodes reachedcell left insert, cell delete, cell diagonallyleft either match exchange x. two cases consideredhere:two sequences consideration trees (tested line 15), knowconsidered every possible way exchanging one other, hencerecord cost F D, edit sequence FDPATHDPATH. case, calculate cost moving along diagonal inspectiontwo nodes. See Figure 3 illustration notion.ii one sequences forest retrieve cost moving along diagonalDPATH, store cost F edit sequence FDPATH.cases, gather set {cost, path} pairs result considering insert/delete/exchange operations preceding sub-sequences, choose bestpair store various arrays. similar corresponding elementstring edit algorithm, added complication calculating tree edit costssequences pair keyroots involves calculating costs edit sequencespairs sub-sequences nodes roots. results pairs keyrootsstored permanently, utilised calculations sub-sequences nextstage.Bille (2005) provides detailed worked examples calculation costs transforming one tree another. Figure 4 shows FDPATH grows algorithm iterateskeyroots trees T1 T2 Figure 2. figure, cells representing5fiAlabbas & RamsayAlgorithm 1 pseudocode Zhang-Shashas TED algorithm edit sequences[i, j]ith jth nodes post-order enumeration tree (T [i, i] written [i])l(i)leftmost leaf descendant subtree rootedK(T )keyroots tree T, K(T ) = {k : k1 > k l(k1 ) = l(k)}D[i, j]tree distance two nodes T1 [i] T2 [j]F D[T1 [i, i1 ], T2 [j, j1 ]]forest distance nodes i1 T1 nodes j j1 T2DP H[i, j]edit sequence trees rooted two nodes T1 [i] T2 [j]F DAT H[T1 [i, i1 ], T2 [j, j1 ]] edit sequence forests covered nodes i1 T1 nodes j j1 T2(T1 [i] )cost deleting ith node T1( T2 [j])cost inserting jth node T2 T1(T1 [i] T2 [j])cost exchanging ith node T1 jth node T2m, nnumber nodes T1 T2 respectivelybestchoose best cost path set options1: x 1 |K1 (T1 )|2:1 |K2 (T2 )|3:F D[, ] 04:F DP H[, ]5:l1 (x) x6:F D[T1 [l1 (x), i], ] F D[T1 [l1 (x), i-1], ] + (T1 [i] )7:F DP H[T1 [l1 (x), i], ] F DP H[T1 [l1 (x), i-1], ] +8:end9:j l2 (y)10:F D[, T2 [l2 (y), j]] F D[, T2 [l2 (y), y-1]] + ( T2 [j])11:F DP H[, T2 [l2 (y), j]] F DP H[, T2 [l2 (y), y-1]] +12:end13:l1 (x) x14:j l2 (y)15:(l1 (i) == l1 (x) l2 (j) == l2 (y))16:cost, path best({F D[T1 [l1 (x), i-1], T2 [l2 (y), j]] + (T1 [i] ),17:F DP H[T1 [l1 (x), i-1], T2 [l2 (y), j]] + d},18:{F D[T1 [l1 (x), i], T2 [l2 (y), j-1]] + ( T2 [j]),19:F DP H[T1 [l1 (x), i], T2 [l2 (y), j-1]] + i},20:{F D[T1 [l1 (x), i-1], T2 [l2 (y), j-1]] + (T1 [i] T2 [j])),21:F DP H[T1 [l1 (x), i-1], T2 [l2 (y), j-1]] + m/x})22:F D[T1 [l1 (x), i], T2 [l2 (y), j]] cost23:D[i, j] cost24:F DP H[T1 [l1 (x), i], T2 [l2 (y), j]] path25:DP H[i, j] path26:else27:cost, path best({F D[T1 [l1 (x), i-1], T2 [l2 (y), j]] + (T1 [i] ),28:F DP H[T1 [l1 (x), i-1], T2 [l2 (y), j]] + d},29:{F D[T1 [l1 (x), i], T2 [l2 (y), j-1]] + ( T2 [j]),30:F DP H[T1 [l1 (x), i], T2 [l2 (y), j-1]] + i},31:{F D[T1 [l1 (x), i-1], T2 [l2 (y), j-1]] + D[i, j]),32:F DP H[T1 [l1 (x), i-1], T2 [l2 (y), j-1]] + DP H[i][j]})33:F D[T1 [l1 (x), i], T2 [l1 (y), j]] cost34:F DP H[T1 [l1 (x), i], T2 [l1 (y), j]] path35:end36:end37:end38:end39: end40: return D[n, m], DP H[n, m]6fiNatural Language Inference Arabici-1,j-1i,j-1i-1,jx/mi,jFigure 3: edit operation direction used algorithm. arc implies editoperation labeled: insertion, deletion, x exchangingoperation (matching).optimal sequence edit operations transform T1 T2 highlighted bold,final optimal path shown last cell (at final row column).T1efbgcT2dddddddddddddddddddddddddddgxxdxdddddmdddmddddmdddddmdddciixixidxdxdddmidddmmdddmmddddmmddiiiiixxixxdxixdxxdddmmidddmmxdddmmxdziiiiiiixiixxiixxdxdxxidddmmiidddmmxidddmmxidxiiiiiiiixixiiixiixxxxdxixidddmmiiidddmmxiidddmmxiidiiiiiiiiixiixiiiixiixxxixdxixiixdxixixdddmmiiimdddmmiiimdiiiiiiiiiixiiixiiiixiiixxxiiiixxxiidxdxixixidddmmiiimidddmmiiimmFDPATHFigure 4: Computing optimal path trees Figure 2.mapping two trees found final sequence edit operationsmapping nodes corresponding match operation only.final distance 6 represents final values (at final row column) D.2last value DPATH represents final sequence edit operations, namely dddmmiiimm. According path, define alignment two postorder trees.alignment two trees T1 T2 obtained inserting gap symbol (i.e. _)either T1 T2 , according type edit operation, resulting strings 12 length sequence edit operations. gap symbol inserted2 edit operation delete (d), whereas inserted 1 editoperation insert (i). Otherwise, nodes T1 T2 inserted 1 2respectively. following optimal alignment T1 T2 :1:2:e_f_b_ggcc__z_x2. simplicity here, assume single operation cost 1 except matching cost0, described Zhang Shasha (1989).7fiAlabbas & Ramsaymeans:d:d:d:m:m:i:i:i:m:m:Delete (e) T1Delete (f ) T1Delete (b) T1Leave (g) without changeLeave (c) without changeInsert (y) T1Insert (z ) T1Insert (x ) T1Leave (d) without changeLeave (a) without changefinal mapping T1 T2 shown Figure 5. mapping figureinsertion, deletion, matching exchanging operations shown single, double,single dashed double dashed outline respectively. matching nodes (or subtrees)linked dashed arrows.a7c5b3e1f2a7d6g4c2d6g1x5y3T1z4T2Figure 5: ZS-TED, mapping T1 T2 .3. Extended TED Subtree Operationsmain weakness ZS-TED algorithm able perform transformationssubtrees (i.e. delete subtree, insert subtree exchange subtree). output ZSTED lowest cost sequence operations single nodes. extend findlowest cost sequence operations nodes subtrees, TED+ST, follows:1. Run ZS-TED compute standard alignment results (Algorithm 1);2. Go alignment group subtree operations. sequence identicaloperations applies set nodes comprising subtree, replaced8fiNatural Language Inference Arabicsingle operation, whose cost determined appropriate function costsindividual nodes (Algorithm 2). variety functions could applied here,depending application. using algorithm textual entailmentuse costs Figure 8, derived used Punyakanok, Roth,Yih (2004), illustration current section simply take costsubtree operation half sum costs individual operationsmake up.noted apply technique modify Zhang-ShashasO(n4 ) algorithm, could also applied algorithm finding tree edit distance,e.g. Kleins O(n3 logn ) algorithm (Klein, 1998), Demaine et al. O(n3 ) algorithm (Demaine,Mozes, Rossman, & Weimann, 2009) Pawlik Augsten O(n3 ) algorithm (Pawlik &Augsten, 2011), since extension operates output original algorithm.additional time cost O(n2 ) negligible since less time cost availabletree edit distance algorithm.3.1 Find Sequence Subtree Edit OperationsExtending ZS-TED cover subtree operations give us flexibility comparingtrees (especially linguistic trees). key algorithm find maximalsequences identical edit operations correspond subtrees. sequence nodespostorder corresponds subtree following conditions satisfied: (i) firstnode leaf; (ii) leftmost sibling last node sequence (i.e. rootsubtree) first node sequence. two conditionschecked constant time, since leftmost sibling node determined nodeadvance. hence find maximal sequences corresponding subtrees scanningforwards sequence node operations find sequences identical operations,scanning backwards sequence find pointcovers subtree. involves potentially O(n2 ) stepsn forward steps find sequencesidentical operations, possibly n-1 backward steps time find sub-sequencescorresponding subtrees. example, sequence nodes e,f,b tree T1 Figure 2subtree e leaf leftmost last node b 1, representsfirst node e. hand, sequence nodes g,c,d treesubtree g leaf, leftmost last node 6, represents itself,first node g.Algorithm 2 contains pseudocode find optimal sequence single subtreeedit operations transforming T1 T2 . Ep=1..L {d, i, x, m} algorithmoptimal sequence node edits transforming T1 T2 , obtained applyingtechnique Section 2, 1 2 alignments T1 T2 obtainedapplying sequence node edits.shown Algorithm 2, find optimal single subtree edit operations sequencetransforms T1 T2 , maximal sequence identical operations checked seewhether contains subtree(s) not. Checking whether sequence correspondssubtree depends type edit operation, according following rules: (i)operation d, sequence checked first tree; (ii) operation i,sequence checked second tree; (iii) otherwise, sequence checked9fiAlabbas & RamsayAlgorithm 2 pseudocode find subtree edit operationsELS1, S2sequence edit operations transform tree T1 tree T2 , Ep=1..L {d,i,x,m}length sequence edit operations Eoptimal alignment T1 T2 respectively, length 1 = 2 = L1: repeat2:ERoot EL3:F L4:repeat5:(F 2 EF 1 == ERoot)6:F F 17:end8:(F == L)9:LL110:ERoot EL11:F L12:end13:(F < L F 2 EF 1 )= ERoot) (L = 0)14:F0 F15:(F < L)16:(F < L)17:IsSubtree true18:(F < L IsSubtree)119:(ERoot =d SF1 ..SLsubtree)2220:(ERoot =i SF ..SL subtree)1221:((ERoot {x,m}) (SF1 ..SLSF2 ..SLsubtrees))22:Replace EF ..EL1 +23:LF 124:F F025:else26:IsSubtree f alse27:end28:end29:F F +130:end31:L L132:F F033:end34:L F0 135: (L 0)36: return Etrees. that, sequence operations corresponds subtree, symbolssequence replaced + except last one (which represents rootsubtree). Otherwise, checking starts sub-sequence original, explained below.instance, let us consider Eh , ..., Et , 1 h < L, 1 < L, h < t, sequenceedit operation, i.e. Ek=h..t {d, i, x, m}. Let us consider h0 = h, firstlycheck nodes Sh1 , ..., St1 Sh2 , ..., St2 see whether heads subtrees.Ek d, nodes Sh1 , ..., St1 checked, nodes Sh2 , ..., St2 checked,otherwise, nodes Sh1 , ..., St1 Sh2 , ..., St2 checked. edit operations Eh , ..., Et1replaced + sequence corresponds subtree. Then, start checkingbeginning another sequence left subtree Eh , ..., Et , i.e. = h 1.10fiNatural Language Inference ArabicOtherwise, checking applied sequence starting next position, i.e.h = h+ 1. checking continued h = t. that, (t h) sequencesstart different positions end position contain subtree, checkingstarts beginning new sequence, i.e. h = h0 = 1. processrepeated h = t.explain subtree operations applied, let us consider two trees T1T2 Figure 2.According TED+ST, cost 3 sequence operation follows:sequence d, result. sequences consist three subtrees(i.e. three deleted nodes, first two matched nodes three inserted nodes):ddd mm iii mm. So, final result is: ++d +m ++i mm. means:++d:+m:++i:m:m:Delete subtree (e,f,b) T1Leave subtree (g,c) without changeInsert subtree (y,z,x ) T1Leave (d) without changeLeave (a) without changefinal mapping T1 T2 obtained using TED+ST shown Figure 6.a7c5b3e1f2a7d6g4c2d6g1x5y3T1z4T2Figure 6: TED+ST, mapping T1 T2 .4. Matching Dependency Treesmentioned above, main goal design textual entailment (TE) system Arabiccheck whether one text snippet (i.e. premise p) entails another text (i.e. hypothesis h).match p h dependency tree pairs effectively, use TED+ST. enables usfind minimum edit operations transform one tree another. allows ussensitive fact links dependency tree carry linguistic informationrelations complex units, hence ensure paying attentionrelations compare two trees. instance, enables us pay attention11fiAlabbas & Ramsayfact operations involving modifiers, particular, applied subtreewhole rather individual elements. Thus, transform tree D1 tree D2Figure 7 deleting park single operation, removing modifier whole,rather three operations removing in, park one one, using costsFigure 8 initial test edit operations experiments. costsupdated version costs used Punyakanok et al. (2004).3 authors foundusing tree edit distance gives better results bag-of-word scoring methods,applied question answering.4sawsawmanparkmanD1D2Figure 7: Two dependency trees, D1 D2 .using costs Figure 8, cost transferring D1 D2 according ZS-TED19 (i.e. one stop word (5) two words (14)), whereas according TED+SToperations 0. Therefore, easy decide D1 entails D2 , whereas reversetrue. also exploited subset/superset relations encoded Arabic WordNet (AWN)(Black, Elkateb, Rodriguez, Alkhalifa, Vossen, Pease, & Fellbaum, 2006) comparingitems tree. Roughly speaking, comparing one tree another requires us swaptwo lexical items, happier item source tree synonymhyponym one target treesince wombat hyponym animal, swappingwombat premise saw wombat zoo animal saw animalzoo truth-preserving exchange.Approaches make use lexical relations kind cope factwords often multiple meanings. follow Hobbs (2005) assuming W1sense hyponym sense W2 sentence involving W1 entailsimilar sentence involving W2 shown (1).(1) p.h.saw peach yesterdays party.saw attractive woman yesterdays party.3. stop words list contains common Arabic words (e.g. particle( +,- ./0# "#! $ Almdyr mwl director indeed busy"#! $ indeed). instance, %&'! )*( +,- ./0# Almdyr mwl director busy.entails %&'! )*4. transcription Arabic examples document follows Habash-Soudi-Buckwalter (HSB) transliteration scheme (Habash, Soudi, & Buckwalter, 2007) transcribing Arabic symbols.12fiNatural Language Inference ArabicCostSingle nodeSubtree (more one node)Delete:X stop word cost 5,else cost 7stop word cost5,else cost 100X subsumed cost 0,elseif X stop word cost 5,elseif subsumed (orantonym of) X cost 100else cost 500Insert:Exchange:double sum costs partsS1 identical S2 cost 0else half sum costs partsFigure 8: Edit operation costs.(1p), instance, word peach ambiguous,5 shade pink tinged yellow(hypernym: Pink) Downy juicy fruit sweet yellowish whitish flesh (hypernym:Drupe, edible fruit, stone fruit) attractive seductive looking woman (hypernym:Adult female, women) cultivated temperate regions (hypernym: Fruit tree).context (1h), however, human reader would assume second interpretationpeach intended, despite fact general fairly unusual usage.reflects widely accepted view contextual information key lexicaldisambiguation. Within RTE task, premise provides context disambiguationhypothesis, hypothesis provides context disambiguation premise.Almost human reader would, instance, accept (2p) entails (2h), despitepotential ambiguity word bank.(2) p.h.money tied bank.cannot easily spend money.5. Dataset Preparationorder train test TE system Arabic, need appropriate dataset.knowledge, datasets available Arabic, develop one.followed one procedures used collecting premise-hypothesis pairsRTE tasks, slight alteration. premises RTE collected varietysources, e.g. newswire text. contain one two sentences tend fairly long(e.g. averaging 25 words RTE1, 28 words RTE2, 30 words RTE3 39 wordsRTE4). contrast, hypotheses quite short single sentences (averaging 11 wordsRTE1, 8 words RTE2 7 words RTE3 RTE4), manually constructedpremise. first three RTE Challenges presented binary classificationtask yes balanced numbers yes problems. Beginning RTE4,three-way classifications (yes, no, contradict, distinguish casesh contradicts p h compatible with, entailed p).5. See Sages dictionary online: http://www.sequencepublishing.com/thesageonline.php. WordNet alsoprovides senses (and more) peach.13fiAlabbas & Ramsaydataset, want produce set p-h pairs handpartlylengthy tedious process, importantly hand-coded datasets liableembody biases introduced developer. dataset used training system,rules extracted little unfolding information explicitlysupplied developers. used testing test examplesdevelopers chosen, likely biased, albeit unwittingly, towardsway think problem.set Arabic p-h pairs TE task created semi-automatic techniquetwo stages. first stage (Section 5.1) responsible automatically collectingp-h pairs news websites, second stage (Section 5.2) uses online annotationsystem allows annotators annotate collected pairs manually. stagesexplained detail below.5.1 Collecting p-h Pairscollected candidate p-h pairs automatically so-called headline-lead paragraphtechnique (Burger & Ferro, 2005) web (e.g. newspaper corpora, pairingfirst paragraph article, p, headline, h). based observationnews articles headline often partial paraphrase first paragrapharticle, conveying thus comparable meaning. use updated version headlinelead paragraph strategy improve quality p-h pair.key idea pose queries search engine automatically filterresponses text snippets might entail query. pairs manually annotated entailment/non-entailment, texts automatically collectedfreely occurring natural texts. eliminates possibility (indeed likelihood)unconscious bias introduced hypotheses manually generated.built corpus p-h pairs using headlines websites Arabic newspapersTV channels queries input Google via standard Google API,selecting first paragraph, usually represents related text snippet(s)article headline (Burger & Ferro, 2005), first 10 returned pages.technique produces large number potential pairs without bias eitherpremises hypotheses. improve quality pairs resulted query,use two conditions filter results: (i) length headline must least fivewords, avoid small headlines; (ii) fewer 80% words headlineappear premise, avoid similar sentences.problem p h similar would littlelearn used training phase TE system; wouldalmost worthless test pairvirtually TE system get pair right,serve discriminatory test pair. therefore eliminate excessively similarp-h pairs training testing, assess terms number shareduncommon words.order overcome problem, matched headlines one source storiesanother. Major stories typically covered range outlets, usually variations emphasis wording. Stories different sources linked lookingcommon words headlinesit unlikely two stories about, in14fiNatural Language Inference Arabicstance, neanderthals news time, straightforward matching basedlow frequency words proper names likely find articles topic.terminology structure first text snippets articles, however, likelyquite different. Thus using headline one source first text snippetarticle story another source likely produce p-h pairsunduly similar. therefore link headline one newspaper relatedsentences another.5.2 Annotating p-h Pairspairs collected first stage still marked-up human annotators,least process collecting nearly bias-free possible. pairs covernumber subjects politics, business, sport general news. annotationperformed eight expert non-expert human annotators identify different pairspositive entailment examples yes, p judged entails h, negative examplesno, entailment hold. annotators follow nearly annotationguidelines used building RTE task dataset (Dagan, Glickman, & Magnini,2006).pair annotated three annotators. inter-annotator agreement (whereannotators agree) around 74% compared 89% annotator agreesleast one co-annotator. suggests annotators found difficult task.fact 74% agreement annotations produced three independentannotators taken account sets upper bound reasonable expectautomatic system carrying task. human annotators agreethree quarters cases, unlikely computer-based system achievemuch 75% agreement given pair annotators.66. Experimentscheck effectiveness TED+ST, used check entailment p-hArabic pairs text snippets compared results two string-based approaches(bag-of-words Levenshtein distance) ZS-TED set pairs. Checkingwhether one Arabic text snippet entails another, however, particularly challengingArabic ambiguous languages, English. instance, Arabicwritten without diacritics (short vowels), often leading multiple ambiguities. makesmorphological analysis difficult (i.e. single written form may easily correspondmany ten different lexemes, see Alabbas & Ramsay, 2011a, 2011b, 2012a, 2012c).preliminary testing dataset contains 600 pairs, binary annotated yes (a 50-50split) using technique explained Section 5. distribution pairs p lengthsummarised Table 1, h average length around 10 words averagecommon words p h around 4 words. average length sentencedataset 25 words per sentence, sentences containing 40+ words.6. dataset, including dependency-tree analysis CoNLL format, available online appendices article http://www.cs.man.ac.uk/~ramsay/ArabicTE/15fiAlabbas & Ramsayps length<2020-2930-39>39Total#pairs175329879600yes8317143330092158446300Table 1: Distribution sentence lengths testset.order check entailment p-h pairs, follow three steps. First,sentence preprocessed tagger parser order convert elements p-hpair dependency trees. dependency tree tree words vertices syntacticrelations dependency relations. vertex therefore single parent, exceptroot tree. dependency relation holds dependent, i.e. syntacticallysubordinate vertex, head, i.e. another vertex dependent. Thusdependency structure represented head-dependent relation verticesclassified dependency types SBJ subject, OBJ object, ATT attribute, etc.carried number experiments state-of-the-art taggersAMIRA (Diab, 2009), MADA (Habash, Rambow, & Roth, 2009) in-house maximumlikelihood (MXL) tagger (Ramsay & Sabtan, 2009) parsers MALTParser (Nivre,Hall, Nilsson, Chanev, Eryigit, Kbler, Marinov, & Marsi, 2007) MSTParser (McDonald, Lerman, & Pereira, 2006).7 experiments show particular merging MADA(97% accuracy) MSTParser gives better results (around 81% labelled accuracy)tagger:parser combinations (Alabbas & Ramsay, 2012b). therefore useMADA+MSTParser current experiments.converting p-h pairs dependency trees, matched dependency trees usingZS-TED TED+ST algorithms, two string-based algorithms (bag-of-wordsLevenshtein distance) provide baseline. tree edit distance algorithms used editoperation costs defined Figure 8 find cost matching p-h pairs.bag-of-words measures similarity p h number common words(either surface forms lemma forms), divided length h.four algorithms use AWN lexical resource order take account synonymyhyponymy relations calculating cost edit.carried two kinds experiments using algorithms: first simpleyes/no experiment, using single threshold decide whether premise similar enoughhypothesis safe say entailed it, second two thresholdscould say yes/dont know/no. results experiments given below.7. parsers data-driven dependency parsers. Arabic usually trained Arabicdependency treebank, Prague Arabic Dependency Treebank (PADT) (Smr, Bielicky, Kouilov,Krmar, Haji, & Zemnek, 2008), version Penn Arabic Treebank (PATB) (Maamouri& Bies, 2004) converted dependency trees: scoring parsers matter countingdependency links.16fiNatural Language Inference Arabic6.1 Binary Decision (yes no)p entails h cost matching less (more case bag-of-words) threshold.results experiments, terms precision (P), recall (R) F-score (F)yes class overall accuracy, shown Table 2. table shows substantialimprovement obtained using TED+ST bag-of-words (F-score TED+STaround 1.16 times F-score bag-of-words, accuracy 1.09 times better)ZS-TED (around 1.06 times better F-score 1.04 times better total accuracy).MethodBag-of-wordsLevenshtein distanceZS-TEDTED+STPyes63.5%64.7%65.9%69.7%Ryes43.7%44.1%51.2%54.5%Fyes0.5180.5250.5760.612Accuracy59.3%60.2%62.5%65.5%Table 2: Performance TED+ST compared string-based algorithms ZS-TED,binary decision.Although primarily interested Arabic, carried parallel sets experiments English RTE2 testset, using Princeton English WordNet (PWN)resource deciding whether word premise may exchanged one hypothesis. tree edit distance algorithms work dependency tree analyses input texts, used set analysed using Minipar (Lin, 1998), downloadedhttp://u.cs.biu.ac.il/~nlp/RTE2 Datasets/RTE2 Preprocessed Datasets.html.RTE2 testset contains around 800 p-h pairs, number Minipar analysesmultiple heads hence correspond well-formed trees, alsonumber cases segmentation algorithm used produces multi-word expressions. eliminating problematic pairs kind left 730 pairs, splitevenly positive negative examples. Since mainly concerneddifference ZS-TED TED+ST, omitted Levenshtein distancesimply kept basic bag-of-words algorithm baseline. Previous authorsshown tree edit distance consistently outperforms string-based approachesdataset, need replicate result here.MethodBag-of-wordsZS-TEDTED+STPyes53.2%52.9%53.2%Ryes50.1%62.5%66.8%Fyes0.5160.5730.59Accuracy52.1%53.5%55.8%Table 3: Performance TED+ST compared simple bag-of-words ZS-TED,binary decision, RTE2 dataset.pattern Table 3 similar Table 2. ZS-TED better bag-of-words,TED+ST improvement ZS-TED. experiments textual entailmenttasks report accuracy: certain situations may important decisionstrustworthy (high precision, Table 2) sure captured17fiAlabbas & Ramsaymany positive examples possible (high recall8 , Table 3), good balance(high F-score). easy change balance precision recall,simply changing threshold used determining whether safe sayp entails hwe could chosen thresholds Table 3 increased precisiondecreased recall, results closely matched Table 2. key pointsets experiments, F-scores improve move string-basedmeasures ZS-TED use TED+ST; remarkablysimilar two datasets, despite fact collected different means,different languages, parsed using different parsers.6.2 Making Three-way Decision (yes, unknown)task use two thresholds, one trigger positive answer cost matchinglower lower threshold (exceeds higher one bag-of-words algorithm)trigger negative answer cost matching exceeds higher one (mutatismutandis bag-of-words). Otherwise, result unknown. reason makingthree-way decision drive systems make precise distinctions. Notedistinguishing {h entails p, h p compatible, h contradicts p},{h entails p, dont know whether h entails p, h entail p}.subtle distinction, reflecting systems confidence judgement,extremely useful deciding act decision.results experiment, terms precision (P), recall (R) F-score (F),shown Table 4. Again, shows large improvement using TED+STbag-of-words (F-score around 1.10 times better) ZS-TED (F-score around 1.06 timesbetter).MethodBag-of-wordsLevenshtein distanceZS-TEDTED+STP58.9%61.4%65.1%67.4%R56.7%58.0%56.0%60.2%F0.5780.5970.6020.636Table 4: Performance TED+ST compared string-based algorithms ZS-TED,three-way decision.scores three-way decision RTE2 dataset lower Arabicdataset, TED+ST outperforms ZS-TED three measures.7. Conclusionpresented extended version, TED+ST, tree edit distance solvedone main drawbacks standard tree edit distance, supports8. might useful, instance, TED+ST used low cost filter question-answeringsystem, results query search engine might filtered TED+ST passedsystem employing full semantic analysis deep reasoning, high precision alsotime-consuming.18fiNatural Language Inference ArabicMethodBag-of-wordsZS-TEDTED+STP50.8%52.3%54.3%R48.3%50.2%52.7%F0.4950.5120.535Table 5: Performance TED+ST compared simple bag-of-word ZS-TED,three-way decision, RTE2 dataset.edit operations (i.e. delete, insert exchange) single nodes. TED+ST dealssubtree transformation operations well operations single nodes: leads usefulimprovements performance standard algorithm determining entailment.key subtrees tend correspond single information units. treatingoperations subtrees less costly corresponding set individual node operations,TED+ST concentrates entire information units, appropriate granularityindividual words considering entailment relations.current findings, preliminary, quite encouraging. fact resultsoriginal testset, particularly improvement F-score, replicated testsetcontrol parser used produce dependency treesp-h pairs provides evidence robustness approach. anticipatecases accurate parser (our parser Arabic attains around 81%accuracy PATB, Minipar reported attain 80% Suzanne corpus)would improve performance ZS-TED TED+ST.currently experimenting different scoring algorithms ZS-TED TED+ST. performance variant tree edit distance depends critically costsvarious operations, thresholds used deciding whether h entailsp, therefore investigating use various optimisation algorithms choosingweights thresholds. also intend use Arabic lexical resources,OpenOffice Arabic dictionary MS Word Arabic dictionary, provide usinformation relations words, information AWN,useful, sparse comparison PWN (Habash, 2010).Acknowledgmentswould like thank reviewers valuable comments, particular reviewersuggested evaluating approach English dataset well Arabic one.extra work provided support belief robustness approachdegree anticipate.would like extend thanks annotators time effortput annotating experimental dataset. Maytham Alabbas owes deepest gratitudeIraqi Ministry Higher Education Scientific Research financial supportPhD study. Allan Ramsays contribution work partially supported QatarNational Research Fund (grant NPRP 09 - 046 - 6 - 001).19fiAlabbas & RamsayReferencesAlabbas, M. (2011). ArbTE: Arabic textual entailment. Proceedings Second StudentResearch Workshop associated RANLP 2011, pp. 4853, Hissar, Bulgaria. RANLP2011 Organising Committee.Alabbas, M., & Ramsay, A. (2011a). Evaluation combining data-driven dependencyparsers Arabic. Proceeding 5th Language & Technology Conference: HumanLanguage Technologies (LTC11), pp. 546550, Pozna, Poland.Alabbas, M., & Ramsay, A. (2011b). Evaluation dependency parsers long Arabicsentences. Proceeding International Conference Semantic TechnologyInformation Retrieval (STAIR11), pp. 243248, Putrajaya, Malaysia. IEEE.Alabbas, M., & Ramsay, A. (2012a). Arabic treebank: phrase-structure trees dependency trees. META-RESEARCH Workshop Advanced Treebanking 8thInternational Conference Language Resources Evaluation (LREC), pp. 6168,Istanbul, Turkey.Alabbas, M., & Ramsay, A. (2012b). Combining black-box taggers parsers modern standard Arabic. Federated Conference Computer Science InformationSystems (FedCSIS-2012), pp. 19 26, Wroclaw, Poland. IEEE.Alabbas, M., & Ramsay, A. (2012c). Improved POS-tagging Arabic combining diversetaggers. Proceedings 8th Artificial Intelligence Applications Innovations(AIAI), pp. 107116, Halkidiki, Greece. Springer.Bille, P. (2005). survey tree edit distance related problems. Theoretical ComputerScience, 337 (1-3), 217239.Black, W., Elkateb, S., Rodriguez, H., Alkhalifa, M., Vossen, P., Pease, A., & Fellbaum, C.(2006). Introducing Arabic WordNet project. Proceedings 3rd International WordNet Conference (GWC-06), pp. 295299, Jeju Island, Korea.Burger, J., & Ferro, L. (2005). Generating entailment corpus news headlines.Proceedings ACL Workshop Empirical Modeling Semantic EquivalenceEntailment, pp. 4954, Ann Arbor, Michigan, USA. Association ComputationalLinguistics.Dagan, I., & Glickman, O. (2004). Probabilistic textual entailment: generic applied modeling language variability. PASCAL Workshop Learning Methods TextUnderstanding Mining, pp. 2629, Grenoble, France.Dagan, I., Glickman, O., & Magnini, B. (2006). PASCAL recognising textual entailment challenge. Quionero-Candela, J., Dagan, I., Magnini, B., & dAlch Buc, F.(Eds.), Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, Recognising Tectual Entailment, Vol. 3944 Lecture NotesComputer Science, pp. 177190. Springer Berlin, Heidelberg.Demaine, E., Mozes, S., Rossman, B., & Weimann, O. (2009). optimal decompositionalgorithm tree edit distance. ACM Transactions Algorithms (TALG), 6 (1),2:12:19.20fiNatural Language Inference ArabicDiab, M. (2009). Second generation tools (AMIRA 2.0): fast robust tokenization, POStagging, base phrase chunking. Proceedings 2nd International ConferenceArabic Language Resources Tools, pp. 285288, Cairo, Eygpt. MEDARConsortium.Habash, N. (2010). Introduction Arabic Natural Language Processing. Synthesis LecturesHuman Language Technologies. Morgan & Claypool Publishers.Habash, N., Rambow, O., & Roth, R. (2009). MADA+TOKAN: toolkit Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemminglemmatization. Proceedings 2nd International Conference Arabic LanguageResources Tools, Cairo, Eygpt. MEDAR Consortium.Habash, N., Soudi, A., & Buckwalter, T. (2007). Arabic transliteration. Arabic Computational Morphology, 1522.Heilman, M., & Smith, N. (2010). Tree edit models recognizing textual entailments, paraphrases, answers questions. Human Language Technologies: 2010 AnnualConference North American Chapter Association Computational Linguistics, pp. 10111019, Los Angeles, California, USA. Association ComputationalLinguistics.Hobbs, J. R. (2005). handbook pragmatics, chap. Abduction Natural LanguageUnderstanding, pp. 724740. Blackwell Publishing.Klein, P. (1998). Computing edit-distance unrooted ordered trees. Proceedings 6th Annual European Symposium Algorithms (ESA 98), pp. 91102,Venice, Italy. Springer-Verlag.Kouylekov, M. (2006). Recognizing Textual Entailment Tree Edit Distance: applicationQuestion Answering Information Extraction. Ph.D. thesis, DIT, UniversityTrento, Italy.Kouylekov, M., & Magnini, B. (2005). Recognizing textual entailment tree edit distance algorithms. Proceedings the1st Challenge Workshop Recognising TextualEntailment, pp. 1720, Southampton, UK.Lin, D. (1998). Dependency-based evaluation minipar. Workshop EvaluationParsing systems, pp. 317330. Springer.Maamouri, M., & Bies, A. (2004). Developing Arabic treebank: methods, guidelines,procedures, tools. Proceedings Workshop Computational ApproachesArabic Script-based Languages, pp. 29, Geneva, Switzerland.MacCartney, B. (2009). Natural Language Inference. Ph.D. thesis, Department ComputerScience, Stanford University, USA.McDonald, R., Lerman, K., & Pereira, F. (2006). Multilingual dependency parsingtwo-stage discriminative parser. 10th Conference Computational Natural Language Learning (CoNLL-X), New York, USA.Mehdad, Y., & Magnini, B. (2009). Optimizing textual entailment recognition using particleswarm optimization. Proceedings 2009 Workshop Applied Textual Inference (TextInfer 09), pp. 3643, Suntec, Singapore. Association ComputationalLinguistics.21fiAlabbas & RamsayNivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit, G., Kbler, S., Marinov, S., & Marsi,E. (2007). MaltParser: language-independent system data-driven dependencyparsing. Natural Language Engineering, 13 (02), 95135.Pawlik, M., & Augsten, N. (2011). RTED: robust algorithm tree edit distance.Proceedings VLDB Endowment, 5 (4), 334345.Punyakanok, V., Roth, D., & Yih, W. (2004). Natural language inference via dependencytree mapping: application question answering. Computational Linguistics, 6,110.Ramsay, A., & Sabtan, Y. (2009). Bootstrapping lexicon-free tagger Arabic. Proceedings 9th Conference Language Engineering (ESOLEC2009), pp. 202215,Cairo, Egypt.Selkow, S. (1977). tree-to-tree editing problem. Information Processing Letters, 6 (6),184186.Smr, O., Bielicky, V., Kouilov, I., Krmar, J., Haji, J., & Zemnek, P. (2008). PragueArabic dependency treebank: word million words. Proceedings Workshop Arabic Local Languages (LREC 2008), pp. 1623, Marrakech, Morocco.Tai, K. (1979). tree-to-tree correction problem. Journal ACM (JACM), 26 (3),422433.Zhang, K., & Shasha, D. (1989). Simple fast algorithms editing distancetrees related problems. SIAM Journal Computing, 18 (6), 12451262.22fiJournal Artificial Intelligence Research 48 (2013) 813-839Submitted 05/13; published 11/13Single Network Relational Transductive LearningAmit DhurandharJun Wangadhuran@us.ibm.comwangjun@us.ibm.comIBM T.J. Watson Research1101 Kitchawan Road, Yorktown Heights, NY-10598 USAAbstractRelational classification single connected network particular interestmachine learning data mining communities last decade so.mainly due explosion popularity social networking sites Facebook,LinkedIn Google+ amongst others. statistical relational learning, many techniquesdeveloped address problem, connected unweighted homogeneous/heterogeneous graph partially labeled goal propagatelabels unlabeled nodes. paper, provide different perspective enablingeffective use graph transduction techniques problem. thus exploitstrengths class methods relational learning problems. accomplishproviding simple procedure constructing weight matrix serves input richclass graph transduction techniques. procedure multiple desirable properties.example, weights assigns edges unlabeled nodes naturally relatemeasure association commonly used statistics, namely Gamma test statistic.portray efficacy approach synthetic well real data, comparingstate-of-the-art relational learning algorithms, graph transduction techniquesadjacency matrix real valued weight matrix computed using available attributes input. experiments see approach consistently outperformsapproaches graph sparsely labeled, remains competitivebest proportion known labels increases.1. IntroductionGiven affluence large connected relational graphs across diverse domains, singlewithin network classification one popular endeavours statistical relationallearning (SRL) research (Getoor & Taskar, 2007). Ranging social networking websitesmovie databases citation networks, large connected relational graphs1 banal.single network classification, partially labeled data graph goal extendlabeling, accurately possible, unlabeled nodes. nodes maymay associated attributes. example within network classificationcould useful forming common interest groups social networking websites.instance, group people geography may interested playing soccerwould interested finding people likely interest.different domain entertainment, one might interested estimatingnew movies likely make splash box office. Based successmovies actors and/or director, one could providereasonable estimate movies likely successful.1. least large connected component.c2013AI Access Foundation. rights reserved.fiDhurandhar & WangMany methods learn infer data graph developed SRL literature. effective methods perform collective classification (Chakrabarti,Dom, & Indyk, 1998), is, besides using attributes unlabeled node inferlabel, also use attributes labels related nodes/entities. thus generalization methods assume data independently identically distributed(i.i.d.). Examples methods relational markov networks (RMNs) (Taskar, Abbeel,& Koller, 2002), relational dependency networks (RDNs) (Neville & Jensen, 2007), markovlogic networks (MLNs) (Richardson & Domingos, 2006), probabilistic relational models(PRMs) (Getoor, Koller, & Small, 2004). fall umbrella markov networks. simpler models suggested baselines relational neighborclassifiers (RN) (Macskassy & Provost, 2003, 2007; Chakrabarti et al., 1998; Sen, Namata,Bilgic, Getoor, Gallagher, & Eliassi-Rad, 2008), simply choose numerousclass label amongst neighbors involved variants using relaxationlabeling. Interestingly, simple models perform quite well auto-correlationhigh, even though graph maybe sparsely labeled. Recently, pseudo-likelihood expectation maximization (PL-EM) (Xiang & Neville, 2008) method introduced, seemsperform favorably methods graph moderate number (around20-30%) labeled nodes.different class methods could potentially address problem handgraph transduction methods (Zhu, Ghahramani, & Lafferty, 2003; Zhou, Bousquet, Lal,Weston, & Schlkopf, 2004; Wang, Jebara, & fu Chang, 2008), part semisupervised learning methods. methods typically perform well givenweighted graph linked nodes mostly labels unless apriori dissimilarnodes explicitly specified (Goldberg, Zhu, & Wright, 2007; Tong & Jin, 2007) , evensmall fraction labels known. weighted graph readily available,constructed (explanatory) attributes nodes. unweighted graphattributes given, adjacency matrix passed input.multiple methods learn weights based attributes. simplestuse lp norm. sophisticated techniques use specialized optimization functions basedgaussian kernels (Jebara, Wang, & Chang, 2009; Jebara & Shchogolev, 2006) loglikelihood (Xiang, Neville, & Rogati, 2010) determine weights. methods however,unsupervised (i.e. ignore labels) based fundamental assumptionhomophily, is, linked closeby datapoints labels. assumptionnecessarily satisfied real-world relational graphs.methods, determine weights based linkage (Gallagher,Tong, Eliassi-Rad, & Faloutsos, 2008). Here, besides edges input graph, edgesadded labeled unlabeled nodes. weights determined makingmultiple even length random walks starting unlabeled node. strategy workswell binary labeled graphs may may satisfy homophily assumption,necessarily effective two labels. Moreover, methodstill unsupervised computationally expensive.literature concerning methods learn weights given unweightedgraph, seen appropriate weighting scheme help leverage graph semanticsmake prediction algorithms robust noise compared unweighted counterparts. fact, well recognized (Maier, Von Luxburg, & Hein, 2008; Jebara814fiSingle Network Relational Transductive Learninget al., 2009; Zhu, Lafferty, & Rosenfeld, 2005) accurate edge weighting significantinfluence various graph based machine learning tasks clustering classification. Moreover, experiments see using unweighted graph leadssubstantially inferior results cases, opposed using weighting scheme.comparing scheme methods learn weights, preferableleast following reasons. First, method uses attribute information, linkinformation alongwith labels determine weights. methods use eitherlink information attribute information tend ignore specific labeling.Hence, method comprehensive thus robust takes accountthree sources information. Second, method used data heterogenouslimited homogenous datasets. Third, see experiments,method performs well across varied labeling percentages, methodstend higher bias able fully exploit settings labelinformation available.relational learning, graphs typically unweighted sometimes mayattributes. many cases, attributes may accurately predict labels,case, weighting edges solely may provide acceptable results. linkslabeling could viewed additional source information predict unlabelednodes. intuitions captured relational gaussian process model (Chu,Sindhwani, Ghahramani, & Keerthi, 2007), limited undirected graphsmentioned prior works (Xiang & Neville, 2008) suggested kernel function easyadapt relational settings may heterogeneous data.paper, provide lucid way effectively leverage rich class graph transduction methods, namely based graph laplacian regularization framework,within network relational classification. Among existing graph transduction methods,class methods considered one efficient accurate real applications (Jebara et al., 2009; Zhu et al., 2003; Zhou et al., 2004). particular, provideprocedure learn weight matrix graph may directed undirected,may exhibit positive negative auto-correlation edges graph maylabeled nodes, unlabeled nodes labeled unlabelednode. method semi-supervised sense uses label information welllinks attribute information determine weights. semi-supervised,learned weights robust effectively capture dependencies muchunsupervised weight learning methods described above. Moreover, learning procedurenaturally relates commonly used statistical measures making principled previous approaches. first provide solution graph nodes attributes,class labels. extend solution include attributes (and heterogenous data)incorporating conical weighting scheme weighs importance links relativeattributes. construction weight matrix assumes binary labeling, however,recursive application chosen graph transduction method reconstructionweight matrix accomplish multi-class classification witnessed experimentsreal data.rest paper organized follows: Section 2, describe constructionweight matrix nodes labels attributes. Section 3,discuss interesting characteristics weighting scheme. Section 4, extend815fiDhurandhar & Wang-1??11Figure 1: Example input graph (T ) construction method.construction described section 2 able model attributes data heterogeneity.Section 5, suggest modification graph transduction methodsFigure 1effectively exploit richness input weight matrix. Section 6, showefficacy ideas experiments synthetic real data. Section 7, discusspromising future directions summarize contributions made current work.2. Weight Matrix Constructionsection elucidate way constructing weight matrix partially labeledgraph G(V, E, ) V set nodes, E set edges set labels.assume labeling binary, i.e labeled node label Yi {1, 1}.mentioned before, procedure constructing weight matrix W , servesinput graph transduction technique, could applied recursively iteratively(binary) classified portion, attain multi-class classification. Hence, input runweight matrix construction method partially (binary) labeled graph shownFigure 1.weights Wij learn edge node node j signify degreesimilarity/dissimilarity labels nodes. weights lie interval[1, 1], positive sign indicates nodes tend similar labels,negative sign indicates tend different labels. numerical valueignoring sign indicates magnitude tendencies. Formally,Wij = f (Yi , Yj , G)(1)Yi Yj maybe known unknown f () [1, 1]. case, value f ()depends type nodes edge connecting, i.e., nodes labeled, unlabeledone labeled unlabeled, along labeled portion graph.exact assignments f () edges connecting 3 different types nodes givensubsection 2.2. Loosely speaking, f () would negative close -1 edgesconnect nodes different labels, would positive close +1edges connect nodes label. semantics consistent evendiscuss extensions later paper, involve learning weights presenceattributes heterogenous data.816fiSingle Network Relational Transductive LearningSymbolNqNqrGraph TypeD, UNqrUNpD, UPsameD, UPoppD, UD, USemantics# nodes label q# edges nodelabel q nodelabel rq = r,# edges nodelabel q nodelabel rq 6= r,Half # edgesnode labelq node label rTotal # labeled edgesi.e. edgesnodes labeledRatio # edgesnodeslabel total #labeled edgesRatio # edgesnodesdifferent labels total# labeled edgesDistribution labelededgesTable 1: notation used paper. graph type, stands directedU stands undirected.Given setup, partially labeled graph G 3 types nodes consequently 9types edges directed graph 6 types edges undirected one. nodecould labeled 1 1 may unlabeled. edge could two nodeslabel (i.e. (1 1) (1 1)) two oppositely labeled nodes (i.e.(1 1) (1 1)) labeled unlabeled node (i.e. (1 ?) (1 ?)(? 1) (? 1)) two unlabeled nodes (i.e. (? ?)). undirectedexample graph shown Figure 1. task assign weightstypes edges.817fiDhurandhar & Wang-1??11Figure 2: Tw weighted version graph shown figure 1.2.1 NotationFigureto2the different types edges, introducedescribe weights assignnotation. Given graph G, let Nq denote number nodes label q. Let Nqr denotenumber edges node label q node label r. undirected graph,would number edges nodes labeled q r, q = r. q 6= r,Nqr would half number edges q r. Let Np denote total numberlabeled edges, i.e. total number edges nodes labeled.words, Np = N11 + N11 + N11 + N11 . let,Psame =N11 + N11N11 + N11, Popp =NpNp(2)Hence, Psame + Popp = 1. denote empirical distribution derived labeled edgesD. summary notation directed undirected graphs shown table 1.2.2 Assignment Weightsdescribe weight matrix construction applies directed undirected graphs. partition types edges 5 categories suggest wayassigning weights edges categories.Edges nodes label: edge nodeslabel, node node j label, assign weight Wij = Psameedge. makes intuitive sense since want weigh edge basedlikely nodes label connected.worth mentioning one might think assigning label specific weights edges.instance, one strategy would assign Wij = NN11Wij = N11dependingNpplabels +1 1 respectively. However, assignment seemsconceptual issue even simple case, graph connected nodesmostly labels. case, weights edges connectingnodes labels would devalued undesirably. example considergraph where, N11 = 10, N11 = 10 N11 = 1. basically graph twolarge clusters connected nodes labels one link connecting two818fiSingle Network Relational Transductive Learningclusters. case, label specific strategy would give weights Wij = 1021 0.48,strategy would give weights Wij = 200.95edgesnodes21labels. clear latter strategy preferable since, eitherlabels high tendency connected nodes labels. Notereasoning also applies N11 might different N11 ,graph exhibits high positive auto-correlation.Edges nodes opposite/different labels: edge nodesopposite labels, node node j different labels, assign weightWij = Popp edge. also intuitive since, want weigh edge basedlikely nodes opposite labels connected. assign negativesign since simply assigning magnitude create distinction nodeslabeled alike different labels.Edges unlabeled nodes: edge unlabeled nodes, nodenode j labels, assign weight Wij = ED [Yi , Yj ] edge.ED [Yi , Yj ] denotes expectation labeled edges distribution D. YiYj {1, 1} hence,ED [Yi , Yj ] =XqrP [Yi = q, Yj = r]q,r{1,1}= P [Yi = 1, Yj = 1] P [Yi = 1, Yj = 1]+P [Yi = 1, Yj = 1] P [Yi = 1, Yj = 1]N11 N11 N11 N11=+NpNpNpNp(3)Since know labels nodes edges category,assign unbiased estimate indicated expected value.Edges unlabeled node node label 1: edgeunlabeled node node label 1, assign weight Wij = ED [Yi |Yj = 1]edge. Yi {1, 1}. case,ED [Yi |Yj = 1] =N11 N11 + N11N1N1(4)unbiased estimate given one nodes label 1.Edges unlabeled node node label 1: edgeunlabeled node node label 1, assign weight Wij = ED [Yi |Yj = 1]edge. Yi {1, 1}. case,ED [Yi |Yj = 1] =N11 N11 + N11N1N1unbiased estimate given one nodes label 1.819(5)fiDhurandhar & Wangweighted version example graph Figure 1, shown graph Tw Figure2.3. Characteristics Matrix Constructionprevious section, elucidated way constructing weight matrix partiallylabeled graph. section, discuss certain characteristics construction.discuss aspects relationships suggested weights standard statistical measures tendencies weight matrix function connectivity labelinggraph. see, construction seems desirable properties.3.1 Relation Standard Measures Associationprevious section, described provided brief justification procedureassign weights. turns weights assign edges least oneunlabeled node, besides unbiased, (statistical) semantics.Remark: weights assigned edges unlabeled nodes equate gammatest statistic () relational setting i.e. ED [Yi , Yj ] = .gamma test statistic (Goodman & Kruskal, 1954), standard measure association used statistics. value statistic ranges [1, 1], positivevalues indicate agreement, negative values indicate disagreement/inversion zero indicates absence association. statistic historically used compare sorted orderobservations based values two attributes. However, also used measureauto-correlation relational data graphs. Hence, assignment weight edges unlabeled nodes auto-correlation graph, makes intuitive sense.turns out, statistic also interesting relationship Student distribution(Goodman & Kruskal, 1954).weights assigned edges one labeled one unlabeled node i.e. ED [Yi |Yj =1] ED [Yi |Yj = 1], based equations 4 5 written as: (Psame |1)(Popp |1) = 1(Psame | 1) (Popp | 1) = 1 . could considered gamma test statisticsconditioned one particular type label could referred conditional gammatest statistics.3.2 Behavior Weight matrixanalyze behavior weight matrix labeled edges input graphtend towards connecting nodes labels analogously connectingnodes different labels.input graph tends nodes labels connected,following effect weight matrix. weight edges nodeslabel tends one, i.e. Psame 1. weight edges nodes different labelstends zero, i.e. Popp 0. weight edges unlabeled nodes tends 1, i.e.1. weight remaining set edges also tends one, i.e. 1 , 1 1. Hence,situation weight matrix becomes adjacency matrix extreme case,different labeled edges vanishing (i.e. weighted 0) edges getting weight820fiSingle Network Relational Transductive Learning-1??11Figure 3: Ts instantiation graph Tw , labeled edges nodeslabels.-1Figure 3??11Figure 4: instantiation graph Tw , labeled edges nodesdifferent labels.Figure4 graph Figure 2 becomes graphone. Consequently, exampleweightedwFigure 3.input graph tends nodes different labels connected,following effect weight matrix. weight edges nodeslabel tends zero, i.e. Psame 0. weight edges nodesdifferent labels tends -1, i.e. Popp 1. weight edges unlabeled nodestends -1, i.e. 1. weight remaining set edges also tends -1, i.e.1 , 1 1. Since graph extreme case positive weights, negativesign weights superfluous terms graph structure eliminated. Hence,situation weight matrix becomes adjacency matrix extreme case,labeled edges vanishing (i.e. weighted 0) edges gettingweight one. Consequently, example weighted graph Tw Figure 2 becomes graphFigure 4.thus Ts = , labeled edges Ts complementlabeled portion respect base graph . intuitively expect labelededges differently labeled nodes slowly disappear edges remain821fiDhurandhar & WangPaperPaperAuthorPaperAuthorTitleAreaAgePaper TitleAuthora)Paperb)Figure 5: a) represents relational schema node types, Paper Author. relationship many-to-many. rounded boxes linked nodetypes denote respective attributes. b) corresponding data graph,shows authors linked papers authored co-authored.present, edges connecting nodes label become predominant. alsoexpect analogous behavior diametric case. seen, intuitionscaptured implicitly, modeling weight matrix, thus making constructionprocedure acceptable.4. Extensionsprevious sections, described procedure constructing weight matrixpartially labeled graph attributes. section, extend weightingscheme include attribute information. Moreover, also present solution handledata heterogeneity using ideas relational learning.4.1 Modeling Attributesdata graphs attributes, want able leverage informationaddition information learned connectivity graph, possiblyimprove performance procedure. particular, need extendweight assignment procedure able encapsulate attribute information. simple waycombining already modeled connectivity information attributes, assignweight edge conical combination weight based connectivityweight based affinity attribute values connected nodes. Hence, wcweight assigned based connectivity particular edge type waweight assigned based attributes, wc + wa new weight edge,, 0. wc essentially weight assignment described section 2, viz. Psame etc.wa function attributes nodes connected corresponding edge,soon define. parameters determined standardmodel selection techniques cross-validation. reasonable indicator valuecould absolute value auto-correlation graph. reasonableestimate value could absolute value cross-correlation walabeling corresponding nodes i.e. labels different.822fiSingle Network Relational Transductive LearningFigure 6: figure shows transformed data graph Paper node type,obtained data graph Figure 5b.absence attributes, weight assignment wc type edge, valueinterval [1, 1]. effectively combine aforementioned two sources information,wa needs scale wc . One obvious choice could cosine similaritycommonly used text analytics (Belkin, Niyogi, & Sindhwani, 2005). Cosine similaritylies [1, 1], values close 1 imply nodes similar values close1 imply nodes dissimilar. choices could kernel functions (K)gaussian kernel (Wang et al., 2008), normalize popular distance metricseuclidean distance lp norms value [0, 1]. Here, values close 1 implysimilarity values close 0 imply dissimilarity. range easily transformedusual range [-1,1] semantics before, simple linear transformationform, 2K 1.4.2 Modeling Heterogeneous Datadata graph multiple types entities, resulting different types nodes,procedure previously described cannot directly applied construct weight matrix.cases, standard relational learning strategies collapsing portions graphusing aggregation applied reduce graph single type nodeattributes (Getoor & Taskar, 2007; Dhurandhar & Dobra, 2012). new graphextended procedure applied.823fiDhurandhar & Wanginstance, citation graph may authors linked papers, papersmultiple authors vice-versa. example shown Figure 5. Figure5a, see node type Paper two attributes, Title Area, denotetitle paper research area belongs respectively. Let attribute Areaclass label, i.e. want classify papers based research area. nodetype Author attributes Paper Title Age, relates particular paperages authors wrote it. Title attribute (a primary key) PaperPaper Title attribute (a foreign key) Author. Hence, Paper node threeattributes namely; Title, Area Age. attributes Title Area called intrinsicattributes belong node type Paper attribute Age called relationalattribute since belongs different linked node type Author. papervariable number authors thus paper would associated multiple valuesAge. popular solution problem aggregate values attribute AgeAuthor single value paper associated single Age value.aggregation function average ages related authors paperused. instead Age attribute introduce new attribute AvgAgedenotes average age. attributes Paper node are; Title, AreaAvgAge. Linking papers co-authored author, data graphlinks Paper node type, node two attributes class labelshown Figure 6.see example weight assignment transformed data graphextended method. Let us assume Paper 1 Paper 2 area AI encoded1 Paper 3 systems encoded 1. Let average age correspondingthree papers (i.e. Paper 1, Paper 2 Paper 3) 30, 30.5 (average two authors)31 respectively. Let ascii value titles 10, 11 15. Also let = 0.1||xi xj ||2= 0.5. use gaussian radial basis kernel K = e 22= 1 compute wa ,weights two edges W12 W23 follows:W12 = Psame + (2e(3030.5)2 +(1011)221)= 0.1 0.5 + 0.5 0.071= 0.086W23 = (Popp ) + (2e(30.535)2 +(1113)221)= 0.1 0.5 + 0.5 1= 0.55weights would passed enhanced graph transduction frameworkdescribe next. important note heterogeneous link types,described procedures applied independently graphs formedlink type final result could obtained aggregating individual decisionsstandard ensemble label consolidation techniques taking majority voteweighted majority based corresponding auto-correlations.824fiSingle Network Relational Transductive Learning5. Enhancing Graph Transduction Techniquesgraph laplacian regularization based framework one efficient popularframeworks semi-supervised learning practical machine learning systems. showneffectiveness many applications, including challenging cases agnostics settings(Jebara et al., 2009). particular, graph based transductive learning approaches imposetrade fitting accuracy prediction function labeled datasmoothness function graph. Typically, smoothness measureprediction function f graph G calculated (Zhou et al., 2004):XXkf k2G =Wij kf (xi ) f (xj )k2j11= f (X)> (D W )f (X) = f (X)> Lf (X),22(6)Wij weight edge nodes xi xPj , X input matrix denotingnodes, f (xi ) label node xi , = {Dii }, Dii = j Wij diagonal matrixf (X) = [f (x1 ), , f (xn )]> . quantity L called graph Laplacian,viewed operator space functions f (Chung, 1997).Given measure function smoothness, graph laplacian based regularizationframework estimates unknown function f follows:f opt = arg min Q(Xl , Yl , f ) + kf k2G(7)Q(Xl , Yl , f ) loss function measuring accuracy labeled set (Xl , Yl ).example, Q(Xl , Yl , f ) = kf (Xl ) Yl k2 i.e. squared loss, popular choice (Belkin,Niyogi, & Sindhwani, 2006; Zhou et al., 2004).Note graph regularization framework directly applied predictionmodeling relational networks directly. because, smoothness measure usinggraph laplacian based assumption connected nodes tendclass labels hence weights non-negative (i.e. Wij 0 i, j). However,well-known edges relational networks could connect type nodes, describedearlier. typical example observed WEBKB dataset (Craven, DiPasquo,Freitag, McCallum, Mitchell, Nigam, & Slattery, 1998), faculty nodes mostlylinked student nodes instead type nodes, i.e. faculty nodes. Althoughrecent work modeled dissimilarity incorporated similarity measurederive called mix graph based prediction models (Tong & Jin, 2007; Goldberg et al.,2007), assumed similarity/dissimilary relations apriori known. However,case automatically estimate positive negative correlations givenlink attribute information partial labeling.indicated Section 2, derive weighted graph containing positive weightednegative weighted edges. ensure compatibility graph Laplacian basedregularization framework, modify smoothness term (Goldberg et al., 2007) usingderived relational edges follows,XXkf k2G =Wij kf (xi ) sgn(Wij )f (xj )k2 ,(8)j825fiDhurandhar & Wangset Wij = |Wij | degree matrix = {Dii } computed Dii =accordingly. positive semidefinite matrix defined as:= (D W ) + (1 sgn(W )) W.PjDij(9)symbol represents Hadamard product. easy see modified smoothnessmeasure Eq. 9 written matrix form as,1(10)kf k2G = f (X)> f (X).2new smoothness measure, extend existing approaches usingderived weighted graph prediction tasks.6. Experimentsprevious sections, described method construct weight matrix relationaldata serves input rich class graph based transductive learning algorithms.section, assess efficacy approach empirical studies syntheticreal data. studies, compare methods across three broad categories, namely:a) sophisticated relational learning (RL) methods, b) sophisticated graph transductionmethods weight matrix computed using available attributes adjacency matrix(if attributes) input (GTA) c) relational transductive methods learnedweight matrix passed input (enhanced/modified) graph transduction techniques.situations methods category c) perform favorably methods twocategories would conditions which, using procedure would justified.attributes available compute weights using well accepted method (Jebara et al., 2009). relational learning methods consider are: MLNs, RDNs, PL-EMRN. learn MLNs using discriminative learning inference performed usingMarkov Chain Monte Carlo (1000 runs). conditional probability distributions (CPDs)RDNs learned using relational probability trees (RPTs), since generallybetter performance relational bayesian classifiers especially number features large (Neville & Jensen, 2007)). inference performed sample obtainedperforming Gibbs sampling (burn-in 100, number samples 1000) usinglearned CPDs. graph transduction methods consider are: local global consistency(LGC) method harmonic functions gaussian fields (HFGF) method. considermethods, since well recognized robust across varied settingscomparison transduction label propagation methods (Liu & Chang, 2009),thus considered suitable baselines (Wang, Jebara, & Chang, 2013). parameter settings use methods prior works (Zhou et al.,2004; Zhu et al., 2003). parameters method (, ) datasets heterogenous attributes found using 10 fold cross-validation, combination, [0, 1] varied independently steps 0.1.experiments, vary percentage known labels training 5%10% 30% 70%. errors methods obtained randomlyselecting (100 times) labeled nodes specified proportions followed averagingcorresponding errors. avoid clutter figures reporting results, plotfollowing 4 curves (rather 8),826fiSingle Network Relational Transductive Learning1649Best RL4815Best RL141312Percentage ErrorPercentage Error47Best GTAHFGFWLGCW46Best GTA454443HFGFWLGCW421141100204060Percentage Labeled40080Figure 7: Performance methods3 categoriesgraph generated using preferential attachmentauto-correlation high,shown above.204060Percentage Labeled80Figure 8: Performance methods3 categoriesgraph generated using preferential attachmentauto-correlation low,shown above.best performance labeled percentage methods category a) (BESTRL)2 ,best performance labeled percentage methods category b) (BESTGTA),LGC method constructed weight matrix input (LGCW)HFGF method constructed weight matrix input (HFGFW) i.e. methods category c).6.1 Synthetic Experimentsgenerate graphs using well accepted random graph generation procedures createreal world graphs, namely: forest fire (FF) (Leskovec, Kleinberg, & Faloutsos, 2007)2. percentage labeled instances 10% RL methods roughly accuracies,though RN obviously efficient. moderate labeling i.e. 30% PL-EM usually best.high labeling i.e. 70% RDNs best cases.827fiDhurandhar & Wang16Best RL45Best GTA14Percentage ErrorPercentage Error1550Best RL1312111090HFGFWBest GTA4035LGCWLGCW204060Percentage LabeledHFGFW30080Figure 9: Performance methods3 categoriesgraph generated using forestfire auto-correlationhigh, shown above.204060Percentage Labeled80Figure 10: Performance methods 3 categoriesgraph generated using forest fire autocorrelation low, shownabove.preferential attachment (PA) (Barabasi & Albert., 1999). procedures add one nodetime nodes get added, assign label based intuitive label generationprocedure described below.6.1.1 Setupgenerate 100 graphs consisting 1000 nodes two generation techniques mentioned above. parameter settings forest fire (forward probability = 0.37, backwardprobability = 0.32) preferential attachment (exponent = 1.6) derived studies(Leskovec et al., 2007; Barabasi & Albert., 1999) indicate settings leadrealistic graphs.labeling front, generate binary labeling {1, 1} simple proceduregraphs. Whenever new node added, probability p assignmajority class amongst labeled neighbors probability 1 p assign onetwo labels uniformly random. Hence, labels generated dependentparticular graph generation procedure consequently connectivity graph,desired. easy see p 1 auto-correlation graph increases, leadinghomogeneity less entropy amongst connected nodes. two graph828fiSingle Network Relational Transductive LearningYearsPublicationPublicationadvisedbytaughtbyPersonPnameTitletaNameCourseStatusIdInphaseCTPersonHNLevelHaspositionCourseCIdLb)a)Figure 11: a) represents relational schema real dataset UW-CSE types, Person, Course Publication. relationship related typesmany-to-many. rounded boxes denote respective attributes. b)corresponding model graph depicts conditional dependenciesrelevant attributes three types namely; Name (N), Status (S), Inphase(I), Hasposition (H), Concatenated Titles (CT), Concatenated course Ids (CId)Level (L).generation procedures, create graphs p low i.e. 0.3 p high i.e. 0.8.low p leads auto-correlation 0.2 i.e. 0.2 high p leadsauto-correlation 0.7 i.e. 0.7, calculated generated graphs.model graph relational methods case trivial since,attributes hence labels unknown nodes generated based known labelsneighbors.6.1.2 ObservationsFigures 7, 8, 9 10 see given particular graph generation procedureirrespective level auto-correlation relative performance 3 different classmethods qualitatively similar. GTAs known perform particularly wellnodes labeled (Zhou et al., 2004; Wang et al., 2008) confirmedexperiments. percentage known labels increases however, relational learningmethods start performing better standard graph transduction techniques.probably due fact sophisticated relational learning methods low biasrelatively high variance, however, increasing number labeled nodes variancedrops rapidly.829fiDhurandhar & WangFigure 12: a) represents relational schema real dataset BREAD Store type.rounded boxes denote respective attributes. b) correspondingmodel graph depicts conditional dependencies relevantattributes namely; Sales Target (ST), Promotions (P), Orders (O) Reclaims(R).interesting part though, weight matrix construction technique seemscapture enough complexity labeling network structure besidesperforming exceedingly well graph sparsely labeled, remains competitiverelational learning methods percentage known labels moderate high.6.2 Real Data Experimentsexperiments real data choose three datasets, namely: UW-CSE (Richardson &Domingos, 2006), WEBKB (Craven et al., 1998) real industrial dataset, BREAD,obtained large consumer retail company.6.2.1 SetupUW-CSE dataset contains information UW computer science department.dataset consists 442 people either students professors. datasetinformation regarding course taught whom, teaching assistantscourse, publication record person, phase person (i.e. prequalifier, post-qualifier), position person (i.e. faculty, affiliate faculty etc.), yearsprogram advisor (or temporary advisor) student (advisedby links).relational schema dataset given Figure 11a. classification task findperson Student Professor. dataset divided five parts; ai.db,graphics.db, theory.db, language.db systems.db. run experiments part830fiSingle Network Relational Transductive Learning2170Best RLHFGFW65Best RL60191817Percentage ErrorPercentage Error20Best GTAHFGFWLGCW55Best GTA5045LGCW40351630150204060Percentage Labeled25080Figure 13: Performance methods3 categoriesUW-CSE dataset, shownabove.204060Percentage Labeled80Figure 14: Multi-class transductive performance methods3 categoriesWEBKB dataset, shownabove.report error averaged parts. model graph showing various conditionaldependencies shown Figure 11b. model graph introduce two new attributespresent relational schema namely, CT CId formed concatenatingtitles papers written person concatenating Ids courses taught (or ta)person. Year attribute eliminated since particularly discriminative.relational methods trained based model graph, besides offcourse takingaccount labels neighbors.WEBKB dataset collection webpages obtained computer sciencedepartments 4 US universities. webpage belongs one 7 categories namely;course, faculty, student, staff, project, department other. category webpagesused input classification task, used link webpagesremaining 6 classes (Macskassy & Provost, 2007). performed experiments fourgraphs formed one university computed average error fouruniversities learning methods. WEBKB, commonly useddataset, use model graph constructed prior works (Neville & Jensen, 2007)train relational methods.BREAD dataset sales information bread products sold different storesnortheastern United States. dataset information 2347 stores.831fiDhurandhar & Wang5045Best RLPercentage Error40Best GTA35302520HFGFWLGCW15100204060Percentage Labeled80Figure 15: Performance methods3 categoriesBREAD dataset, shownabove.store know location, know store met3 underachieved target quarterlysales, know amounts promotion period, know quantityordered period know amount reclaimed period. Basedlocation, form graph linking closest stores together. this,dataset size 2347 node graph 4 attributes. Setting attributeindicating whether sales met underachieved expected amount class label,obtain graph node three explanatory attributes. relational schemadataset given Figure 12a. corresponding model graph showing variousconditional dependencies shown Figure 12b. again, relational methodstrained based model graph, besides taking account labels neighbors.6.2.2 ObservationsUW-CSE WEBKB datasets see best GTA better relational methods small percentage (< 20%) labels known, relationalmethods quickly close gap start outperforming GTAs label information. weight matrix construction method however, performs bettertwo classes methods low label proportions remains competitive rela3. also includes cases sales exceeded expected amount.832fiSingle Network Relational Transductive Learningtional methods proportion increases, unlike GTAs. favorable behaviorlikely attributed method able effectively model strength (i.e.numerical value) direction (i.e. + ) dependencies linked entities,something GTAs seemingly fail capture.BREAD dataset see GTAs much worse classmethods. possible reason stores near one another typically competetype products hence, input graph exhibits strongnegative auto-correlation. Since, GTAs predominantly model similarity linkedentities, performance practically unchanged even percentage knownlabels increased. relational methods perform much better GTAs setting.contrast GTAs, effectively capture dissimilarity linked nodesnumber known labels increases. However, weight matrix construction method seemscapture relationship much earlier small percentage labels known.7. Discussionpaper, provided simple yet novel way constructing weight matrixpartially labeled relational graphs may directed undirected, may mayattributes may homogeneous heterogeneous. describedmanner weight matrix serve input rich class graphtransduction methods modified graph laplacian based regularization framework.portrayed desirable properties construction method showcasedeffectiveness capturing complex dependencies experiments synthetic realdata.primary focus paper learn effectively unweighted graphs.However, many real world problems, might given weighted graph.instance, genome sequence analysis connection strength gene expressionsestimated experiments coupled expert knowledge. situationsquestion arises incorporate known weights methodology?logical consistent way incorporating weights modeling wouldcombine computed connectivity based weight wc attribute based weightwa conical combination. consistent methodology describedcombine connectivity based weight attribute based weight. Thus, wkknown (normalized) weight, weight edge would wc + wa + wk ,, , 0. before, free parameters computed using standard modelselection techniques based graph properties domain knowledge.future, would interesting extend procedure perform multi-classclassification single shot, rather perform multiple binary classificationtasks. would likely improve actual running time, though necessarilytime complexity terms O(.). would also interesting learn weights basedlocal neighborhood graph entire graph. Thus, would computebased local structure around datapoint assign weights. Determininglocality however, tricky especially multiple link types.theory side, might interest analyze synthetic label generationprocedure introduced paper, different types graphs. One could use ideas833fiDhurandhar & Wangtheory random walks determine tendencies label generation procedure.learning theory perspective, one could potentially derive error bounds functions p(amongst parameters), one express p terms auto-correlation ,one would error bounds functions . would interest sincecomputed static graphs given snapshot evolving graph, oneknow order nodes attached, thus making error boundapplicable graphs larger set applications.related orthogonal research problem studying influence spreadsocial networks (Castillo, Chen, & Lakshmanan, 2012; Kempe, Kleinberg, & Tardos, 2003).interesting research problem, one primary goals studyinformation flows real networks. end interesting findnodes/people network likely influential targettingpeople lead rapid information spread. something marketing departmentsconsumer product companies interested obvious reasons. Thoughcommonalities research problem ours, learnperform inference real graphs, objectives quite different. case, mainlycare correctly labeling unknown nodes based connectivity attributes.really interested information flow would fastest consequentlynodes target achieve efficient manner. certain sense,influence spread problem probably could formulated active learning versionproblem, want choose small number nodes query would maximizeperformance particular class within network classification algorithms.definitely something interesting pursue going forward.Acknowledgmentswould like thank Katherine Dhurandhar proofreading paper. would alsolike thank editor reviewers constructive comments.Appendixprovide figures synthetic real data experiments plots methodsbest. Figures 16, 17, 18 19 correspond synthetic experiments,figures 20, 21 22 correspond real data experiments.834fiSingle Network Relational Transductive Learning5016RNRNPLEM48MLNPercentage ErrorPercentage Error15LGC14HFGF13HFGFWRDNLGCW12LGC46HFGFHFGFWRDN44LGCWMLN4211100PLEM204060Percentage Labeled40080204060Percentage Labeled80Figure 16: Performance methodsFigure 17: Performance methodsPA high autocorrelation.PA low autocorrelation.161550RNRDNPercentage ErrorPercentage ErrorPLEM14LGC13MLN12HFGF111090HFGFWLGCW45LGCHFGFPLEM40MLNRN35HFGFWLGCWRDN204060Percentage Labeled30080204060Percentage Labeled80Figure 18: Performance methodsFigure 19: Performance methodsFF high autocorrelation.FF low autocorrelation.835fiDhurandhar & Wang21RN70MLN1918PLEMRN60HFGFHFGFWRDN17HFGFW65LGCPercentage ErrorPercentage Error20LGCWHFGF55PLEM5045LGCLGCWMLN403516RDN30150204060Percentage Labeled25080Figure 20: Performance methods204060Percentage LabeledFigure 21: Multi-class transductive perfor-UW-CSE dataset.mance methodsWEBKB dataset.50RN45Percentage ErrorPLEM40MLNHFGF,LGC35302520RDNHFGFWLGCW1510080204060Percentage Labeled80Figure 22: Performance methodsBREAD dataset.836fiSingle Network Relational Transductive LearningReferencesBarabasi, A., & Albert., R. (1999). Emergence scaling random networks. Science, 286,509512.Belkin, M., Niyogi, P., & Sindhwani, V. (2005). manifold regularization. Int. Workshop Artificial Intelligence Statistics.Belkin, M., Niyogi, P., & Sindhwani, V. (2006). Manifold Regularization: GeometricFramework Learning Labeled Unlabeled Examples. Journal MachineLearning Research, 7, 23992434.Castillo, C., Chen, W., & Lakshmanan, L. (2012).Kdd2012 tutorial: Information influence spread social networks. http://research.microsoft.com/enus/people/weic/kdd12tutorial inf.aspx.Chakrabarti, S., Dom, B., & Indyk, P. (1998). Enhanced hypertext categorization usinghyperlinks. Proceedings SIGMOD-98, ACM International Conference Management Data, pp. 307318, Seattle, US. ACM Press, New York, US.Chu, W., Sindhwani, V., Ghahramani, Z., & Keerthi, S. (2007). Relational learninggaussian processes. Advances Neural Information Processing Systems 19, pp.289296. MIT Press.Chung, F. (1997). Spectral graph theory. No. 92. Amer Mathematical Society.Craven, M., DiPasquo, D., Freitag, D., McCallum, A., Mitchell, T., Nigam, K., & Slattery,S. (1998). Learning extract symbolic knowledge world wide web. Proceedings fifteenth national/tenth conference Artificial intelligence/Innovativeapplications artificial intelligence, AAAI, pp. 509516. American AssociationArtificial Intelligence.Dhurandhar, A., & Dobra, A. (2012). Distribution free bounds relational classification.Knowledge Information Systems, 1.Gallagher, B., Tong, H., Eliassi-Rad, T., & Faloutsos, C. (2008). Using ghost edges classification sparsely labeled networks. KDD 08: Proc. 14th ACM SIGKDDIntl. conf. Knowledge discovery data mining, pp. 256264, New York, NY,USA. ACM.Getoor, L., Koller, D., & Small, P. (2004). Understanding tuberculosis epidemiology usingprobabilistic relational models. Journal Artificial Intelligence Medicine, 30, 233256.Getoor, L., & Taskar, B. (2007). Introduction Statistical Relational Learning. MIT Press.Goldberg, A., Zhu, X., & Wright, S. (2007). Dissimilarity graph-based semi-supervisedclassification. Artificial Intelligence Statistics (AISTATS).Goodman, L., & Kruskal, W. (1954). Measures association cross classifications. Journal American Statistical Association, 49, 732764.Jebara, T., & Shchogolev, V. (2006). B-matching spectral clustering. Proc. 17thEuropean conf. Machine Learning, ECML06, Berlin, Heidelberg. Springer-Verlag.837fiDhurandhar & WangJebara, T., Wang, J., & Chang, S. (2009). Graph construction b-matching semisupervised learning. Proc. 26th Annual Intl. Conf. Machine Learning,ICML 09, pp. 441448, New York, NY, USA. ACM.Kempe, D., Kleinberg, J., & Tardos, E. (2003). Maximizing spread influencesocial network. Proceedings ninth ACM SIGKDD international conferenceKnowledge discovery data mining, KDD 03, pp. 137146, New York, NY,USA. ACM.Leskovec, J., Kleinberg, J., & Faloutsos, C. (2007). Graph evolution: Densificationshrinking diameters. ACM Trans. Knowl. Discov. Data, 1 (1), 2.Liu, W., & Chang, S. (2009). Robust multi-class transductive learning graphs.Computer Vision Pattern Recognition, 2009., pp. 381388. IEEE.Macskassy, A., & Provost, F. (2003). simple relational classifier..Macskassy, S., & Provost, F. (2007). Classification networked data: toolkitunivariate case study. J. Mach. Learn. Res., 8, 935983.Maier, M., Von Luxburg, U., & Hein, M. (2008). Influence graph construction graphbased clustering measures. Proc. Neural Infor. Proc. Sys.Neville, J., & Jensen, D. (2007). Relational dependency networks. J. Mach. Learn. Res., 8,653692.Richardson, M., & Domingos, P. (2006). Markov logic networks. Mach. Learn., 62 (1-2),107136.Sen, P., Namata, G. M., Bilgic, M., Getoor, L., Gallagher, B., & Eliassi-Rad, T. (2008).Collective classification network data. AI Magazine, 29 (3).Taskar, B., Abbeel, P., & Koller, D. (2002). Discriminative probabilistic models relationaldata. Proc. 18th Conference Uncertainty AI, pp. 485492.Tong, W., & Jin, R. (2007). Semi-supervised learning mixed label propagation.Proceedings National Conference Artificial Intelligence.Wang, J., Jebara, T., & Chang, S. (2013). Semi-supervised learning using greedy max-cut.Journal Machine Learning Research, 14, 729758.Wang, J., Jebara, T., & fu Chang, S. (2008). Graph transduction via alternating minimization. Proceedings International Conference Machine Learning.Xiang, R., & Neville, J. (2008). Pseudolikelihood em within-network relational learning.Proceedings 2008 Eighth IEEE International Conference Data Mining,pp. 11031108, Washington, DC, USA. IEEE Computer Society.Xiang, R., Neville, J., & Rogati, M. (2010). Modeling relationship strength online socialnetworks. Proc. 19th Intl. conf. World wide web, New York, NY, USA.ACM.Zhou, D., Bousquet, O., Lal, T., Weston, J., & Schlkopf, B. (2004). Learning localglobal consistency. Advances Neural Information Processing Systems 16,pp. 321328. MIT Press.838fiSingle Network Relational Transductive LearningZhu, X., Ghahramani, Z., & Lafferty, J. (2003). Semi-supervised learning using gaussianfields harmonic functions. Proceedings ICML, pp. 912919.Zhu, X., Lafferty, J., & Rosenfeld, R. (2005). Semi-supervised learning graphs. Ph.D.thesis, Carnegie Mellon University, Language Technologies Institute, School Computer Science.839fiJournal Artificial Intelligence Research 48 (2013) 717-732Submitted 6/13; published 11/13Research NoteCase Pathology Multiobjective Heuristic SearchJose Luis Perez de la CruzLawrence MandowEnrique Machucaperez@lcc.uma.eslawrence@lcc.uma.esmachuca@lcc.uma.esDpto. Lenguajes Ciencias de la ComputacionUniversidad de MalagaBulevar Louis Pasteur, 35. Campus de Teatinos, 29071Malaga (Spain)Abstractarticle considers performance MOA* multiobjective search algorithmheuristic information. shown certain cases blind searchefficient perfectly informed search, terms node label expansions.class simple graph search problems defined number nodes growslinearly problem size number nondominated labels grows quadratically.proved problems number node expansions performed blind MOA*grows linearly problem size, number expansions performedperfectly informed heuristic grows quadratically. also proved number labelexpansions grows quadratically blind case cubically informed case.1. IntroductionHeuristic search algorithms central problem solving Artificial Intelligencemany practical applications Operations Research. heuristic search additionalinformation provided algorithm aim reducing computational effortneeded find solution.However, sometimes goal achieved. contrary, certain casesshown use better heuristics implies worsening performance.example, well-known fact arising bipersonal games lookahead pathology,is, deeper exploration performed (and hence better suppossedlyheuristic minimax value assigned position), worse decision taken (Nau,1982). Recently, phenomenon described one-agent real-time search (Lustrek& Bulitko, 2008; Nau, Lustrek, Parker, Bratko, & Gams, 2010).Even statically precomputed heuristics pathologies found.instance, let us consider standard A* algorithm (Hart, Nilsson, & Raphael, 1968).known algorithm admissible provided optimistic heuristic costestimates that, estimates also consistent, informed heuristicsalways result equally efficient search (see Pearl, 1984, especially pp. 7585).However, heuristic optimistic consistent, algorithm A* performO(2n ) node expansions search performed graph n nodes arc costsbounded (Martelli, 1977). Notice heuristic used, A* performs likeDijkstras algorithm never exhibit exponential performance.c2013AI Access Foundation. rights reserved.fiPerez de la Cruz, Mandow, & Machucapaper deals pathology arising certain extension A* multiobjectivesearch problems. decision making situations one criterion involved,concept optimal solution frequently replaced Pareto optimality, is, solution better improves respect least one criterion withoutworsening others (Ehrgott, 2005). Since Pareto optimality partial order relation,solving problems usually results set Pareto-optimal solutions, representoptimal trade-offs criteria optimized. importance researchmultiobjective search algorithms two-fold. first place, many graph search problems benefit directly multiobjective analysis (De Luca Cardillo & Fortuna, 2000;Gabrel & Vanderpooten, 2002; Refanidis & Vlahavas, 2003; Muller-Hannemann & Weihe,2006; DellOlmo, Gentili, & Scozzari, 2005; Ziebart, Dey, & Bagnell, 2008; Wu, Campbell,& Merz, 2009; Delling & Wagner, 2009; Fave, Canu, Iocchi, Nardi, & Ziparo, 2009; Mouratidis, Lin, & Yiu, 2010; Caramia, Giordani, & Iovanella, 2010; Boxnick, Klopfer, Romaus, &Klopper, 2010; Klopper, Ishikawa, & Honiden, 2010; Wu, Campbell, & Merz, 2011; Machuca& Mandow, 2011). hand, multicriteria preference models used graphsearch typically look subset Pareto-optimal solutions (Mandow & Perez de la Cruz,2003; Perny & Spanjaard, 2005; Galand & Perny, 2006; Galand & Spanjaard, 2007; Galand,Perny, & Spanjaard, 2010). Therefore, improvements performance multiobjective algorithms guide development efficient algorithms multicriteria decisionrules.Two direct extensions A* accept general (multiobjective) heuristic functionsproposed literature: MOA* (Stewart & White, 1991) NAMOA* (Mandow& Perez de la Cruz, 2005). NAMOA* uses label selection guide exploration.formal point view, recent analysis (Mandow & Perez de la Cruz, 2010a) shownalgorithm admissible optimistic heuristics, efficiency, measurednumber label expansions, improves informed consistent heuristics.Furthermore, number expansions optimal respect class admissiblealgorithms. words, NAMOA* inherits beneficial properties A*.MOA* uses node selection (as opposed label selection) guide exploration,also known admissible optimistic heuristics (Stewart & White, 1991).development MOA* prompted number related formal developments extensions(Dasgupta, Chakrabarti, & DeSarkar, 1995, 1999; Perny & Spanjaard, 2002; Mandow &Perez de la Cruz, 2003; Perny & Spanjaard, 2005), still cited algorithmchoice recent applications (De Luca Cardillo & Fortuna, 2000; Fave et al., 2009; Klopperet al., 2010). previous formal analysis showed exist problems blindMOA* performs (2n ) node expansions graphs n nodes (Mandow & Perez de laCruz, 2010b). However, formal analysis MOA* remained incomplete. particular,efficiency algorithm never related precision consistent heuristics.recent empirical analysis (Machuca, Mandow, Perez de la Cruz, & Ruiz-Sepulveda, 2010)shown that, certain cases, MOA* performs much worse NAMOA* biobjective random problems different correlations. Quite surprisingly, analysisalso revealed heuristic MOA* actually performs consistently worse uninformedMOA*.paper part investigation formal properties MOA* NAMOA*.formally show performance MOA*, measured terms number718fiA Case Pathology Multiobjective Heuristic Searchlabel node expansions, improve general informed heuristics.precisely, define class simple graph search problems prove useperfect heuristic information MOA* yields computational effort useheuristic information. words, blind MOA* cases efficientperfectly informed MOA*, terms node label expansions.article organized follows. First, necessary concepts presentedalgorithm MOA* briefly described (Section 2). Section 3 class simplemultiobjective search problems defined. performance MOA* classproblems analyzed blind perfectly informed cases Sections 4 5 termsnode expansions, Section 6 terms label expansions. Finally, conclusionsfuture work described.2. Backgroundmultiobjective decision problems alternative evaluated according set qdifferent objectives usually grouped vector ~y = (y1 , y2 , . . . yq ), ~y Rq . Preferencevectors ~x, ~y defined so-called Pareto order dominance relation ()follows: ~x ~y objectives holds xi yi leastobjective j holds xj < yj . Given set vectors , subset nondominatedvectors nd(Y ) defined nd(Y ) = {~y | @~x ~x ~y }.solution multiobjective problem consists set Pareto-optimal nondominated solutions, is, set solutions costs nondominatedset solution costs.multiobjective graph search problem, single source set destination nodesdesignated given graph G = (N, A). Pairs nodes n, n0 N may joineddirected arcs (n, n0 ) labelled vector costs ~c(n, n0 ) Rq . path P graphsequence nodes joined consecutive arcs cost ~c(P ) P sumcosts component arcs. solution problem set paths P joiningsource destination nodes ~c(P ) nondominated set solutioncosts.2.1 MOA* AlgorithmMOA* well-known algorithm performs multiobjective heuristic graph search (Stewart & White, 1991). pseudocode (slightly adapted original: Stewart & White,1991) presented Table 1. MOA* presents many similarities A*. Two sets nodesOP EN CLOSED used control search. Initially, source nodeopen node. Newly generated nodes create pointer parents. However, MOA*construct search tree like A*, rather acyclic directed graph. duefact node may reached several optimal (nondominated) paths. scalarcost functions g, h, f generalized functions G, H, F return sets vectorsnode. Additionally, LABEL(n0 , n) sets keep subsets vectors G(n0 ) arisepaths n0 coming n.Function G(n) refers set nondominated cost vectors among paths alreadyfound n. heuristic function H(n) returns also set vectors, estimating costs719fiPerez de la Cruz, Mandow, & Machuca1. INITIALIZE set OP EN start node s, empty sets, SOLN , C, CLOSED LABEL.2. CALCULATE set N nodes n OP EN least one estimate f~ F (n)dominated estimates open nodes solution cost C.3. N empty,Terminate returning set solution paths reach nodes SOLN costs C.elseChoose node n N using domain-specific heuristic, breaking ties favour goal nodes,move n OP EN CLOSED.4. bookkeeping maintain accrued costs node selection function values.5. IDENTIFY SOLUTIONS. n solution node,Include n SOLN current costs C.Remove dominated costs C.Go back step 2.6. EXPAND n examine successors. successors nodes n do:(a) newly generated node,i.ii.iii.iv.Establish pointer n.Set G(m) = LABEL(m, n).Compute F (m).Add OP EN .(b) Otherwise, new, following,i. potentially nondominated paths discovered, then, one,following.Ensure cost LABEL(m, n), therefore G(m).new cost added G(m) then, purge LABEL(m, n) dominated costs,CLOSED, move OP EN .7. Go back step 2.Table 1: MOA Algorithm.nondominated paths n destination nodes. evaluation function F (n) returnsset cost estimates n, F (n) = nd{~g + ~h | ~g G(n) ~h H(n)}.Later useful define H (n) function returns set costsactual nondominated paths n destinations nodes.iteration MOA* computes ND, subset open nodes nondominatedcost estimate, selects node subset. admissibility algorithmdepend particular selection procedure among nodes ND. following,additional selection procedure nondominated nodes called nd-selection rule.destination node selected, added SOLN, costs C. ValuesF (n) dominated vectors C never considered ND. Search terminates NDempty, is, candidate nodes dominated explored.expansion n generates successors n0 n graph adequate G(n0 )values them. n0 new, placed OPEN, sets G(n0 ) LABEL(n0 , n)store costs paths extended n n0 . n0 new, MOA* checks720fiA Case Pathology Multiobjective Heuristic Searchnew nondominated value G(n0 ) generated current step;case, G(n0 ) LABEL(n0 , n) properly updated, and, n0 CLOSED, mustmoved back OPEN.pair (n, ~g ) n node ~g G(n) usually called label. MOA*labels node n expanded simultaneously n selected. Therefore, labelsreaching single node given time either simultaneously open closed.original paper (Stewart & White, 1991), interesting properties MOA*proved. example, proved MOA* admissible H(n) optimistic.Regarding comparison admissible heuristics, function H(n) defined leastinformed another H 0 (n) whenever ~h0 H 0 (n) exists ~h H(n)~h0 ~h. case, proved set nodes expanded MOA* Hsubset expanded H 0 (theorem 4, p. 805). However, authors recognizednodes may reopened even heuristic function consistent, henceset expanded nodes significant measure analysis performanceMOA*.Figure 1: Graph (3, 10, 10, 2)3. Class Multiobjective Search Problemsevery n N, let us consider problem graphs (Figure 1) 2n nodes labeled 1, . . . , 2n1, 2n 3n 2 arcs. every even node 2i (1 < n) outgoing arcsform (2i, 2i + 1) (2i, 2i + 2). every odd node 2i + 1 (1 < n 1)outgoing arc form (2i + 1, 2i + 2). also arc (1, 2). start node1 goal node 2n. cost arc ~c(i, j) defined follows: chooseeither 2 4; every > 0, ~c(2i, 2i + 2) = (, 6 ); every > 0,~c(2i, 2i + 1) = ~c(2i + 1, 2i + 2) = (3 /2, /2). way, define two possiblesets costs arcs, exception ~c(1, 2) = (1 , 2 ) subjectrestriction.shall refer problem graphs multiobjective chain graphs. every n,corresponding set multiobjective chain graphs denoted Mn . denote(n, 1 , 2 , ) graph Mn ~c(1, 2) = (1 , 2 ) c(2i, 2i + 2) = (, 6 ).example, Figure 1 shows (3, 10, 10, 2).graph Mn always 2n1 different paths start goalnode. fact, go 2i 2i + 2 choose either simple path < 2i, 2i + 2 >(with cost (, 6 )) two-arc path < 2i, 2i + 1, 2i + 2 > (with cost (6 , ))n 1 independent choices like that. hand, n differentpath costs given c~n k = (1 , 2 ) + (2(n 1) + 2k, 4(n 1) 2k), every k721fiPerez de la Cruz, Mandow, & MachucaIt. #123456OPEN12344566G(n) = F (n)(0, 0)(10, 10)(12, 11)(12, 14)(12, 14)(14, 12)(14, 15)(16, 13)(14, 18)(16, 16)(14, 18)(16, 16)(18, 14)Table 2: Trace uninformed MOA* (3, 10, 10, 2)0 k n 1. (n, 1 , 2 , 2) graph, cost c~n k corresponds path n 1 karcs (2i, 2i + 2) k paths < 2i, 2i + 1, 2i + 2 >. denote Cn = {c~n 0 , . . . , c~n n1 }.Notice every solution cost (y1 , y2 ) Cn holds y1 + y2 = 1 + 2 + 6(n 1),solution costs given lie line negative slope hence nonecosts dominates another, is, nd(Cn ) = Cn .4. Blind MOA* Mnsample run MOA* (3, 10, 10, 2) (Figure 1) H(n) = {~0} (uninformed case)provided Table 2. Values G(n) include nondominated costs generated pathsnode. Since performing blind search, values F (n)G(n).observe trace node selected nodes j <selected. means every node i, 1 2n 1 expandedonce. way, example MOA* performs exactly 2n node selections 2n 1node expansions. result general proved induction numberiterations every graph (n, 1 , 2 , ).Lemma 1 input MOA* graph (n, 1 , 2 , ) n H(n) = {~0}, MOA*performs exactly 2n 1 node expansions.Proof. Let us consider OPEN set certain iteration (s = 1, . . .) executionMOA*. Let us call level integer L = bs/2c. easily proved inductionthat:(i) every odd iteration algorithm (s = 3, . . .), OPEN = {s, s+1}, labels{(a+3/2, b+/2) | (a, b) CL }; labels s+1 {(a+, b+6)| (a, b) CL };selected node s.(ii) every even iteration algorithm (s = 4, . . .), OPEN = {s}, labelsexactly CL selected node s.follows every node selected exactly therefore numbernode expansions exactly 2n 1./722fiA Case Pathology Multiobjective Heuristic Searchn123456H (n)(14,18),(16,16),(18,14)(4,8),(6,6),(8,4)(4,5),(6,3)(2,4),(4,2)(2,1)(0,0)Table 3: Heuristic values (3, 10, 10, 2)Figure 2: Search graph iteration 25. Perfectly Informed MOA* Mnsample run MOA* (3, 10, 10, 2) (Figure 1) H(n) = H (n) (perfect information) provided Figures 2-9 Table 4. Figures 2-9 show trace search graphiteration. Closed nodes shown gray. Values G(n) shown nodeFigures created, change previous iteration.Table 4, iteration nodes OPEN displayed also G(n)values (that is, nondominated costs paths generated start node). ValuesF (n) computed adding G(n) values estimations Table 3. Sinceassuming perfect heuristic information, F (n) values always optimal solution costs,is, nondominated costs solution paths node 1 node 6. Then, every iterationevery node n, F (n) C3 .Notice also labels C3 (and hence F (n)) nondominated, generalseveral open nondominated nodes. necessary provide additionalheuristic rule (step 3-else algorithm Table 1) nd-selection rule. exampleTable 4, following nd-selection rule applied: select node best lexicographicnondominated alternative (remember lexicographic order total order definedbiobjective case (y1 , y2 ) < (z1 , z2 ) y1 < z1 , y1 = z1 y2 < z2 ).also possible nondominated cost appears several open nodes.another procedure must provided breaking ties open nodesf~-label. done, instance, random, selecting newest node,oldest node OPEN breadth-first fashion. latter procedure followedTable 4.observe order node selection example124634656pattern is: MOA* selects even nodes goal node reached; selectsodd node 2i + 1 selects even nodes 2j, j > i; done everyodd node selected once. way even nodes general selected severaltimes. example node 4 selected twice node 6 selected three times.723fiPerez de la Cruz, Mandow, & MachucaIt#123456789OPEN123435635455656G(n)(0, 0)(10, 10)(12, 11)(12, 14)(12, 11)(14, 15)(14, 18)(12, 11)(14, 15)(12, 14)(14, 12)(14, 15)(14, 15)(16, 13)(14, 18)(16, 16)(14, 15)(16, 13)(14, 18)(16, 16)(18, 14))F (n)(14, 18)(16, 16)(18, 14)(14, 18)(16, 16)(18, 14)(16, 16)(18, 14)(14, 18)(16, 16)(16, 16)(18, 14)(16, 16)(14, 18)(16, 16)(18, 14)(16, 16)(14, 18)(16, 16)(18, 14)(16, 16)(16, 16)(18, 14)(14, 18)(16, 16)(16, 16)(18, 14)(14, 18)(16, 16)(18, 14)Table 4: Trace perfectly informed MOA* (3, 10, 10, 2)Figure 3: Search graph iteration 3Figure 4: Search graph iteration 4Figure 5: Search graph iteration 5724fiA Case Pathology Multiobjective Heuristic SearchFigure 6: Search graph iteration 6Figure 7: Search graph iteration 7Figure 8: Search graph iteration 8Figure 9: Search graph iteration 9725fiPerez de la Cruz, Mandow, & Machucaconcrete order expansion depend nd-selection tie-breaking rules.However, prove general results valid nd-selection rule usesheuristic f~ values (irrespective tie-breaking rule). Notice nd-selection rulesusually applied (as best lexicographic best linear) kind.Lemma 2 (i) Let (n, 1 , 2 , 2) Mn . Let nondominated solution costs C ={~c 0 , . . . , c~ n1 }. nd-selection rule c~ 0 selected preference{~c 1 , . . . , c~ n1 }, MOA* performs least n + n(n1)node expansions.2(ii) Analogously, let (n, 1 , 2 , 4) Mn . nd-selection rule c~ n1selected preference {~c 0 , . . . , c~ n2 }, MOA* performs least n + n(n1)node2expansions.Proof. prove part (i) (proof part (ii) entirely analogous). Let (n, 1 , 2 , 2)Mn . First remember node j iteration algorithm, setF (j) estimated costs j subset Cn = {~c 0 , . . . , c~ k , . . . , c~ n1 } = {(1 + (2(n1) + 2k, 2 + 4(n 1) 2k)) | 0 k n 1}. Let us trace first n + 1 iterationsalgorithm. shown every > 0, c~ 0 appear F (2i), c~ 0 neverappear F (2i + 1). Therefore, first iteration node 1 selected expanded.even nodes selected expanded sequentially node 2i selected.amounts first n expansions.Elementary computations show step, every 0 < n, F (2i) = {~c 0, . . . ,ni1nic~}; every 0 < n1, F (2i+1) = {~c , . . . , c~}; OP EN = {3, 5, . . . , 2n1}.Two observations made step: i) Let us consider odd nodes. Since every oddnode 2i + 1 (0 < < n) optimal path going it, open nodes mustselected expanded termination. amounts least n 1 expansions,even reexpansion assumed nodes; ii) Let us consider nongoal evennodes {2, 4, . . . , 2n 2}. iteration number labels associated 2i n + 1 i.Since every even node exist n optimal costs, termination numberlabels every even node must exactly n, is, 0 + 1 + . . . + (n 1) = n(n1)2labels even nodes missing moment. prove labels generatedone time, is, one every expansion, proved least anothern(n1)expansions needed.2Let us call episode subsequence node expansions comprised two consecutive odd node expansions (or last odd node expansion last expansion).example Table 4, episodes < 1, 2, 4, 6 >, < 3, 4, 6 > < 5, 6 >.episode starts, even node OPEN (since even node always ndselected label c~ 0 , odd node never nd-selected label c~ 0 , odd nodeselected). end episode, reason, even nodeOPEN. Let us consider episode e =< 2j 1, 2j, 2j 0 , . . . , 2j n >. easily seenexpansion even node 2j originates opening even node, namely 2(j + 1),episode always form E =< 2j 1, 2j, 2(j + 1), 2(j + 2), . . . , 2(j + m) >.prove induction episodes every episode starts, every evennode 2i (1 < n) exist integers p, q that: i) F (2i) = {~c 0 , . . . , ~cp }; ii)F (2i 2) = {~c 0 , . . . , c~ q } = F (2i 1) {~c 0 }; iii) either q = p q = p + 1.obviously true first episode finishes second episode starts. Assume726fiA Case Pathology Multiobjective Heuristic Searchtrue episode e starts. show remains true finishes, is,episode e + 1 starts.Consider odd node 2j 1 starts episode assume q = p, is,F (2j 1) = F (2j) {~c 0 }. new label added F (2j), reexpansionneeded modification done, hence stated relation among labels continuestrue. contrary, assume q = p + 1, is, F (2j 1) = {~c 1 , . . . , c~ p+1 },0p+10pF (2j 2) = {~c , . . . , c~}, F (2j) = {~c , . . . , c~ }. Therefore one label c~ p+1 added2j. stated relation labels remain true 2j 2, 2j 1 2j (onlyalternative p = q). However, F (2j) modified mustcheck relation F (2j), F (2j + 1) F (2j + 2). hypothesis F (2j + 1) ={~c 1 , . . . , c~ p }, expansion 2j adds c~ p+1 it, also becomes F (2j) = F (2j +1).Concerning relation F (2j) F (2j + 2), F (2j + 2) = {~c 0 , . . . , c~ p },label added F (2j + 2), relation holds (since F (2j + 2) exactly onelabel less F (2j) now) episode finishes since even node remains open.F (2j + 2) = {~c 0 , . . . , c~ p1 }, one label c~ p added F (2j + 2) relation alsobecomes true, F (2j + 2) modified. induction length episodecould proved finally relation holds modified nodes.case, see new added label immediately triggers expansionnode labels added even nodes one time. Therefore, least n(n1)expansions2needed complete labels even nodes. first expansion episode oneodd node, must computed; last expansion episodeadd label, compute n(n1)additional node expansions.2expansions,Now, adding togheter performed expansions, least n + n(n1)2q. e. d./Figure 10: Construction (4, 1 , 2 , 2) graph Lemma 3727fiPerez de la Cruz, Mandow, & Machucaprove main lemma:Lemma 3 nd-selection rule depends f~ values, every n Nexists graph (n, 1 , 2 , ) input MOA* H(n) = H (n),MOA* performs least n + n(n1)node expansions.2Proof. idea proof follows: Lemma 2 asserts order lexicographically optimal costs graph (n, 1 , 2 , 2) Mn selected one turnsfirst one, MOA* performs least n + n(n1)node expansions (and analogously2every graph (n, 1 , 2 , 4) Mn selected one last one).Lemma 3 proved every n every selection rule depending f~ values showgraph (n, 1 , 2 , 2) Mn satisfying condition (or graph (n, 1 , 2 , 4) Mnsatisfying analogous condition).Let n N. Let us consider line y1 + y2 = 8(n 1) + 2 sequence = 2n 1nondominated points it, (F 0 , . . . , F m1 ) given F = (2(n1)+2i+1, 6(n1)2i+1)(Figure 10 shows line seven points F 0 , . . . , F 6 n = 4). nd-selectionrule select one them, say F j , preference others; case,F j extract (F 0 , . . . , F m1 ) subsequence n points (F j , F j+1 , . . . , F j+n1 )(F jn+1 , . . . , F j1 , F j ). Assume first case (that depicted Figure 10 j = 3;subsequence F 3 , F 4 , F 5 , F 6 ). consider graph (n, 1 , 2 , 2)1 = 2j + 1 2 = 2(n 1 j) + 1 (in Figure 10 point K (1 , 2 ) = (7, 1)).always possible, is, every n, j 1 , 2 > 0.every k, 0 k n 1, F j+k = (1 , 2 ) + (2(n 1) + 2k, 4(n 1) 2k) = c~ k ,is, extracted subsequence (F j , F j+1 , . . . , F j+n1 ) exactly set solution costs(n, 1 , 2 , 2), (~c 0 , c~ 1 , . . . , c~ n1 ). Since F j nd-selected (F j+1 , . . . , F j+n1 ),Lemma 2 MOA* performs least n + n(n1)node expansions (n, 1 , 2 , 2).2Analogously prove case considering graph form (n, 3 , 4 , 4)./Lemmas 1 3 obtain immediately following result.Theorem 1 every nd-selection rule depending f~ values, exists sequencegraphs M1 , . . . , Mn , . . . that:(i) Every Mn 2n nodes 3n 2 arcs.(ii) MOA performs (n) node expansions applied Mn heuristic information given.(iii) MOA performs (n2 ) node expansions applied Mn perfect heuristicinformation given.6. Label Countsbasic operation MOA node expansion every node expansion impliesgeneral case joint expansion several labels. However, algorithms (e.g.NAMOA , Mandow & Perez de la Cruz, 2010a) use label expansion basic operation.reason could interesting analyse MOA also terms label expansions.Lemma 4 input MOA* graph (n, 1 , 2 , ) H(n) = {~0}, MOA*performs exactly n2 n + 1 label expansions.728fiA Case Pathology Multiobjective Heuristic SearchProof. Consider reasoning proof Lemma 1. proved that,selected expansion, every even node 2i labelsPand every odd node 2i + 1(with 1) labels. Adding together 1 + 2 1in1 = 1 + n(n 1) =n2 n + 1 label expansions./Lemma 5 every n N exists graph (n, 1 , 2 , ) inputMOA* H(n) = H (n), MOA* performs least n2 + n(n+1)(n1)label expansions.4Proof. Every odd node 1, . . . , 2n 1 must termination n labels, mustexpanded least once. amounts least n2 label expansions. hand,consider even nodes 2, 4, . . . , 2n 2. must also n labels termination. Considerreasoning proof Lemma 2. proved that: (i) first timeeven node 2i expanded, (n + 1) labels; (ii) time even node 2iexpanded, exactly one label. Therefore thePnumber label expansions node2i (n + 1) + (n + 2) + . . . + (n + i) = 1ji (n + j) = (n+1)i2 . AddingP(n+1)itogheter even nodes 2i 1 n 1 1in1 2 = n(n+1)(n1)4label expansions. Adding expansions odd even nodes leastn2 + n(n+1)(n1)q. e. d.4/Lemmas 4 5 obtain immediately following result.Theorem 2 every nd-selection rule depending f~ values, exists sequencegraphs M1 , . . . , Mn , . . . that:(i) Every Mn 2n nodes 3n 2 arcs.(ii) MOA performs (n2 ) label expansions applied Mn heuristic information given.(iii) MOA performs (n3 ) label expansions applied Mn perfect heuristicinformation given.7. Conclusions Future Workpaper considers performance MOA* multiobjective heuristic search algorithm. Results show performance degrade better heuristic information.class problems presented (multiobjective chain graphs) use perfect heuristic information (a trivially consistent informed heuristic) result reductionnumber node label expansions performed algorithm. contrary,performance perfectly informed version algorithm worse performanceblind version.Multiobjective chain graphs formalize infrequent situation practical multiobjective search, sequence nodes traversed least two conflicting paths.analysis revealed MOA* combined H , best possible heuristic,number node expansions grows quadratically, number grows linearlyheuristic information used; number label expansions grows cubically,grows quadratically heuristic information provided.729fiPerez de la Cruz, Mandow, & Machucapathology, together results theoretical (Mandow & Perez de laCruz, 2010b) empirical (Machuca, Mandow, Perez de la Cruz, & Ruiz-Sepulveda, 2012),casts doubts suitability MOA* performing heuristic multiobjective search.general, alternatives (such NAMOA*) proved (Mandow& Perez de la Cruz, 2010a) pathological behaviours cannot arise,preferred.AcknowledgmentsPartially supported Gobierno de Espana, grant TIN2009-14179. Partially fundedConsejera de Innovacion, Ciencia Empresa. Junta de Andaluca (Espana), P07-TIC03018.ReferencesBoxnick, S., Klopfer, S., Romaus, C., & Klopper, B. (2010). Multiobjective searchmanagement hybrid energy storage system. IEEE International ConferenceIndustrial Informatics (INDIN), pp. 745750.Caramia, M., Giordani, S., & Iovanella, A. (2010). selection k routes multiobjective hazmat route planning. IMA Journal Management Mathematics, 21,239251.Dasgupta, P., Chakrabarti, P., & DeSarkar, S. (1995). Utility pathmax partial orderheuristic search. Information Processing Letters, 55, 317322.Dasgupta, P., Chakrabarti, P., & DeSarkar, S. (1999). Multiobjective Heuristic Search.Vieweg, Braunschweig/Wiesbaden.De Luca Cardillo, D., & Fortuna, T. (2000). Dea model efficiency evaluationnondominated paths road network. European Journal Operational Research,121 (3), 549558.Delling, D., & Wagner, D. (2009). Pareto paths SHARC. SEA, pp. 125136.DellOlmo, P., Gentili, M., & Scozzari, A. (2005). finding dissimilar Pareto-optimalpaths. European Journal Operational Research, 162, 7082.Ehrgott, M. (2005). Multicriteria Optimization. Springer.Fave, F. M. D., Canu, S., Iocchi, L., Nardi, D., & Ziparo, V. A. (2009). Multi-objectivemulti-robot surveillance. 4th International Conference Autonomous RobotsAgents (ICARA), pp. 6873. IEEE.Gabrel, V., & Vanderpooten, D. (2002). Enumeration interactive selection efficientpaths multiple criteria graph scheduling earth observing satellite. EuropeanJournal Operational Research, 139, 533542.Galand, L., & Perny, P. (2006). Search compromise solutions multiobjective statespace graphs. Proc. XVII European Conference Artificial Intelligence(ECAI2006), pp. 9397.730fiA Case Pathology Multiobjective Heuristic SearchGaland, L., Perny, P., & Spanjaard, O. (2010). Choquet-based optimisation multiobjective shortest path spanning tree problems. European Journal OperationalResearch, 204 (2), 303315.Galand, L., & Spanjaard, O. (2007). Owa-based search state space graphs multiplecost functions. FLAIRS Conference 2007, pp. 8691.Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determinationminimum cost paths. IEEE Trans. Systems Science Cybernetics SSC-4, 2,100107.Klopper, B., Ishikawa, F., & Honiden, S. (2010). Service composition pareto-optimalitytime-dependent QoS attributes. Lecture Notes Computer Science, LNCS 6470,635640.Lustrek, M., & Bulitko, V. (2008). Thinking much: Pathology path finding. et al.,M. G. (Ed.), Proceedings European 18th Conference Artificial Intelligence,pp. 899900. IOS Press.Machuca, E., Mandow, L., Perez de la Cruz, J. L., & Ruiz-Sepulveda, A. (2010).empirical comparison multiobjective graph search algorithms. KI2010 LNAI 6359, pp. 238245.Machuca, E., Mandow, L., Perez de la Cruz, J. L., & Ruiz-Sepulveda, A. (2012). comparison heuristic best-first algorithms bicriterion shortest path problems. EuropeanJournal Operational Research, 217, 4453.Machuca, E., & Mandow, L. (2011). Multiobjective route planning precalculatedheuristics. Proc. 15th Portuguese Conference Artificial Intelligence (EPIA2011), pp. 98107.Mandow, L., & Perez de la Cruz, J. L. (2003). Multicriteria heuristic search. EuropeanJournal Operational Research, 150, 253280.Mandow, L., & Perez de la Cruz, J. L. (2005). new approach multiobjective A*search. Proc. XIX Int. Joint Conf. Artificial Intelligence (IJCAI05), pp.218223.Mandow, L., & Perez de la Cruz, J. L. (2010a). Multiobjective A* search consistentheuristics. Journal ACM, 57 (5), 27:125.Mandow, L., & Perez de la Cruz, J. L. (2010b). note complexity multiobjective A* search algorithms. ECAI 2010, pp. 727731.Martelli, A. (1977). complexity admissible search algorithms. Artificial Intelligence, 8, 113.Mouratidis, K., Lin, Y., & Yiu, M. (2010). Preference queries large multi-cost transportation networks. Proceedings - International Conference Data Engineering,pp. 533544.Muller-Hannemann, M., & Weihe, K. (2006). cardinality Pareto set bicriteria shortest path problems. Annals OR, 147 (1), 269286.Nau, D. S. (1982). investigation causes pathology games. Artificial Intelligence, 19 (3), 257278.731fiPerez de la Cruz, Mandow, & MachucaNau, D. S., Lustrek, M., Parker, A., Bratko, I., & Gams, M. (2010). betterlook ahead?. Artificial Intelligence, 174 (1617), 13231338.Pearl, J. (1984). Heuristics. Addison-Wesley, Reading, Massachusetts.Perny, P., & Spanjaard, O. (2002). preference-based search state space graphs.Proc. Eighteenth Nat. Conf. AI, pp. 751756. AAAI Press.Perny, P., & Spanjaard, O. (2005). preference-based approach spanning treesshortest paths problems. European Journal Operational Research, 162, 584601.Refanidis, I., & Vlahavas, I. (2003). Multiobjective heuristic state-space search. ArtificialIntelligence, 145, 132.Stewart, B. S., & White, C. C. (1991). Multiobjective A*. Journal ACM, 38 (4),775814.Wu, P.-Y., Campbell, D., & Merz, T. (2009). On-board multi-objective mission planningunmanned aerial vehicles. IEEE Aerospace Conference Proceedings, pp. 110.Wu, P.-Y., Campbell, D., & Merz, T. (2011). Multi-objective four-dimensional vehiclemotion planning large dynamic environments. IEEE Transactions Systems,Man, Cybernetics, Part B: Cybernetics, 41 (3), 621634.Ziebart, B., Dey, A., & Bagnell, J. (2008). Fast planning dynamic preferences. ICAPS2008 - Proceedings 18th International Conference Automated PlanningScheduling, pp. 412419.732fiJournal Artificial Intelligence Research 48 (2013) 953-1000Submitted 09/13; published 12/13Constraint Solver Flexible Protein ModelsFederico Campeottocampe8@nmsu.eduDept. Computer Science, New Mexico State UniversityDepts. Math. & Computer Science, University UdineAlessandro Dal Palualessandro.dalpalu@unipr.itDept. Math. & Computer Science, University ParmaAgostino Dovieragostino.dovier@uniud.itDept. Math. & Computer Science, University UdineFerdinando Fiorettoffiorett@cs.nmsu.eduDept. Computer Science, New Mexico State UniversityDepts. Math. & Computer Science, University UdineEnrico Pontelliepontell@cs.nmsu.eduDept. Computer Science, New Mexico State UniversityAbstractpaper proposes formalization implementation novel class constraints aimed modeling problems related placement multi-body systems3-dimensional space. multi-body system composed body elements, connectedjoint relationships constrained geometric properties. emphasis investigation use multi-body systems model native conformations proteinstructureswhere body represents entity protein (e.g., amino acid,small peptide) geometric constraints related spatial propertiescomposing atoms. paper explores use proposed class constraints supportvariety different structural analysis proteins, loop modeling structureprediction.declarative nature constraint-based encoding provides elaboration toleranceability make use additional knowledge analysis studies. filteringcapabilities proposed constraints also allow control number representativesolutions withdrawn conformational space protein, meanscriteria driven uniform distribution sampling principles. scenario possibleselect desired degree precision and/or number solutions. filtering componentautomatically excludes configurations violate spatial geometric propertiescomposing multi-body system. paper illustrates implementation constraintsolver based multi-body perspective empirical evaluation protein structureanalysis problems.1. IntroductionConstraint Programming (CP) declarative programming methodology gainedpredominant role addressing large scale combinatorial optimization problems.paradigm, CP provides tools necessary guide modeling resolution searchproblemsin particular, offers declarative problem modeling (in terms variablesconstraints), ability rapidly propagate effects search decisions, flexibleefficient procedures explore search space possible solutions. field CPc2013AI Access Foundation. rights reserved.fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelliroots seminal work Sutherland (1963) Sketchpad system,successive efforts systems like CONSTRAINTS (Sussmann & Steele, 1980) ThingLab(Borning, 1981). years, CP become paradigm choice address hard searchproblems, drawing integrating ideas diverse domains, like Artificial IntelligenceOperations Research (Rossi, van Beek, & Walsh, 2006). declarative natureCP enables fast natural modeling problems, facilitating development,rapid exploration different models resolution techniques (e.g., modeling choices,search heuristics).recent years, several research groups started appreciating potential constraint programming within realm Bioinformatics. field Bioinformatics presentsnumber open research problems grounded critical exploration combinatorial search space, highly suitable manipulated constraint-based search.Constraint methodologies applied analyze DNA sequences instance,locate Cis-regulatory elements (Guns, Sun, Marchal, & Nijssen, 2010), DNA restrictionmaps construction (Yap & Chuan, 1993), pair-wise multiple sequence alignment (Yang, 1998; Yap, 2001; Tsai, Huang, Yu, & Lu, 2004). Constraint methodologiesapplied biological networks (Corblin, Trilling, & Fanchon, 2005; Larhlimi &Bockmayr, 2009; Ray, Soh, & Inoue, 2010; Gay, Fages, Martinez, & Soliman, 2011; Gebser,Schaub, Thiele, & Veber, 2011) biological inference problems, Haplotype inference (Graca, Marques-Silva, Lynce, & Oliveira, 2011; Erdem & Ture, 2008),phylogenetic inference (Erdem, 2011).particular area Bioinformatics witnessed extensive use CP techniquesdomain structural biologyi.e., branch molecular biology biochemistrydeals molecular structure nucleic acids proteins, structureaffects behavior functions. Constraint Programming progressively gained pivotalrole providing effective ways explore space conformations macromolecules,address problems like secondary tertiary structure prediction, flexibility, motif discovery, docking (Backofen, Will, & Bornberg-Bauer, 1999; Krippahl & Barahona, 2002;Thebault, de Givry, Schiex, & Gaspin, 2005; Dal Palu, Dovier, & Pontelli, 2007; Mann& Dal Palu, 2010; Shih & Hwang, 2011; Krippahl & Barahona, 2005; Dal Palu, Spyrakis,& Cozzini, 2012b; Chelvanayagam, Knecht, Jenny, Benner, & Gonnet, 1998; Yue & Dill,2000). Two comprehensive surveys use constraint-based methods structuralBioinformatics recently proposed (Dal Palu, Dovier, Fogolari, & Pontelli, 2012a;Barahona & Krippahl, 2008).focus work use constraint-based technology support structuralstudies proteins. Proteins macromolecules fundamental importance wayregulate vital functions biological processes. structural properties criticaldetermining biological functions proteins (Skolnick, Fetrow, & Kolinski, 2000; Baker& Sali, 2001) investigating protein-protein interactions, central virtually cellular processes (Alberts, Johnson, Lewis, Raff, Roberts, & Walter, 2007).refer Protein Structure Prediction (PSP) problem problem determiningtertiary structure protein knowledge primary structure and/or knowledge structures (e.g., secondary structure components, templates homologousproteins). PSP problem also often broken specialized classes problemsrelated specific aspects tertiary structure protein, side-chain geometry954fiA Constraint Solver Flexible Protein Modelsprediction (Dunbrack, 2002), loop modeling prediction (Go & Scheraga, 1970; Xiang, Soto,& Honig, 2002; Rufino, Donate, Canard, & Blundell, 1997; Soto, Fasnacht, Zhu, Forrest, &Honig, 2008), protein flexibility investigation (Bennett & Huber, 1984).classes problems share common rootsthe need track possible conformations chains amino acids. variations problem relate factors likelength chain considered (from short peptides case loop modelingentire proteins general PSP case) diverse criteria employed selectionsolutions, as, instance, lowest basin effective energy surface, composedintra-molecular energy protein plus solvation free energy (Karplus &Shakhnovich, 1992; Lazaridis, Archontis, & Karplus, 1995).Modeling variability protein chain involves many degrees freedomneeded represent different protein conformations. Tracking variability requiresexploration vast conformational space. Model simplifications adopted reducecomputational cost, instance backbone-only models represent backboneproteins, side-chain representation could simplified single central point (centroid)describing center mass, one adopt approximated representation spacethough lattice models.Nevertheless, even strong simplifications, search space remains intractableprevents use brute-force search methods space possible conformations(Crescenzi, Goldman, Papadimitriou, Piccolboni, & Yannakakis, 1998).Constraint programming methodologies found natural use addressing PSPrelated problemswhere structural chemical properties modeled termsconstraints spatial positions atoms, transforming search conformationsconstraint satisfaction/optimization problem. proposed approaches rangepure ab initio methods (Backofen et al., 1999; Dal Palu et al., 2007) methods basedNMR data (Krippahl & Barahona, 1999) methods based fragments assembly (DalPalu, Dovier, Fogolari, & Pontelli, 2010). spite efforts, design effectiveapproaches filter space conformations lead feasible search remainschallenging open problem.work present constraint solver targeted modeling general class proteinstructure studies. particular solution suitable address protein structure analysisstudy, requiring generation set unbiased sampled diverse conformationssatisfy certain given restraints. One unique features solution presentedwork capability generate uniformly distributed sampling target protein regionsamong given portion Cartesian space selected granularityaccountingspatial rotational properties.abstract problem general multi-body system, composing bodyconstrained means geometric properties related bodies jointrelationships. body represent entity protein, individual aminoacid small peptide (e.g., protein fragment). Bodies relate spatial positionsorganization individual atoms composing it.view exploration protein structures multi-body systems suggests number different constraints, used model different classes structural studiesapplied filter infeasible (or unlikely) conformations. propose investigationseveral classes constraints, terms theoretical properties practical use955fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontellifiltering. Particular emphasis given Joined-Multibody (JM) constraint, whosesatisfaction prove NP-complete. Realistic protein models require assemblyhundreds different body versions, making problem intractable. study efficientapproximated propagator, called JM filtering (JMf), allows us efficiently computeclasses solutions, partitioned structural similarity controlled tolerance error.perspective novel holds strong potential. structural problems investigating computationally intractable; use global constraints specifically designedmeet needs enables effective exploration search space greaterpotential effective approximations.multi-body model provides interesting perspective exploring spaceconformationswhile actual search operates discrete sets alternatives (e.g., setsfragments), filtering process avails reasoning processes operates continuousdomain; allows propagation filtering effective.proposed multi-body constraints filtering techniques constitute coreresolution engine FIASCO (Fragment-based Interactive Assembly protein Structureprediction Constraints), efficient C++-based constraint solver. demonstrateflexibility efficiency FIASCO using engine model solve classproblems derived loop modeling instances. Throughout paper showability FIASCO providing uniform efficient modeling platform studyingdifferent structural properties (that been, far, addressed using significantlydistinct methods tools). declarative nature constraint-based methods supportslevel elaboration tolerance offered frameworks protein structureprediction, facilitating integration additional knowledge guiding studies (e.g.,availability information secondary structure elements).rest paper organized follows. Section 2, provide high-levelbackground biological chemical properties proteins review commonly used approaches address structural studies. Section 3, develop constraintframework dealing fragments multi-body structures. Section 4 describesimplementation constraints propagation schemes FIASCO system.Section 5 report experimental results using FIASCO collectionbenchmarks loop modeling. Section 6 provides concluding remarks.preliminary version research pursued paper presented (Campeotto,Dal Palu, Dovier, Fioretto, & Pontelli, 2012). work Campeotto et al. focusedone new class constraints targeting problem loop closure, work presentedpaper provides comprehensive constraint system, focused modeling structuralprotein properties investigating different types problems (e.g., structure prediction,studies flexibility). present manuscript includes also precise detailedformalization extensive experimentation comparison.2. Background, General Context, Related Worksection briefly review basic Biology notions, introduce problemstackling paper refer selection related literature.956fiA Constraint Solver Flexible Protein ModelsHsidechainNCHHNC'HCC'sidechainFigure 1: schematic sequence two amino acids showing amino acid backboneside chains. arrow C 0 N denotes peptidic bond.2.1 General Backgroundprotein molecule made smaller building blocks, called amino acids. One amino acidconnected another one peptidic bond. Several amino acids pairwiseconnected linear chain forms whole protein. backbone protein,illustrated Figure 1, formed sequence N C C 0 atoms contained aminoacid. backbone rather flexible allows large degree freedom protein.amino acid characterized variable group atoms influences specificphysical chemical properties. group, named side chain, ranges 1 18 atomsconnects C atom amino acid. 20 kinds amino acids foundcommon eukaryotic organisms.Proteins made 10 1, 000 amino acids, average globular protein300 amino acids long. amino acid contains 724 atoms, therefore numberatoms arrangements space grow easily beyond computationalpower. Since beginning protein simulation studies, different algorithms exploring conformations devised, molecular dynamics, local search, MonteCarlo, genetic algorithms, constraint approaches, well different geometric representations (Neumaier, 1997).literature, several geometric models proteins proposed. One choiceinfluences quality complexity computational approaches numberpoints describe single amino acid.simplest representation one amino acid represented onepoint, typically C atom, given robust geometric property: distanceC atoms two consecutive amino acids preserved low variance (roughly 3.81A).Usually, volumetric constraints enforced points, order simulateaverage occupancy amino acid. representation visualized chainbeads moved space.refined representation models store (or all) points backbone, pluscentroid mass (CG) represents whole side chain connects C atom.models, amino acid described different C CG distances CG volumes.centroid approximation side-chain flexibility allows refinedenergetic models, number points taken care still low. paper957fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliFigure 2: native structure intact influenza virus M1 protein (indexed 1EA3PDB) modeled full atom, 5@ model, simple C C model(from left right). secondary structures (-helices) emphasized.Figure 3: Amino acid concatenation 5@ modeluse particular case simplified models, 5@ model, described preciselybelow. particular instance coarse-grained protein models (Clementi, 2008;Shehu, 2010). end spectrum, atom amino acid representedone point. representation accurate, time allowsaccurate energetic considerations. drawback computational demandhandling backbone side-chain flexibility increases significantly.Figure 2 report three representations protein.paper select intermediate representation amino acids atomsN, C , C 0 backbone centroid side chain (CG) accounted for.also include oxygen (O) atom attached C 0 atom, atom togetherC 0 N identifies triangle chemically stable along backbone usedassembly amino acids (see complete formalization). positiontwo H atoms backbone deduced position atomsdeal explicitly. conclusion, deal 5 atomic elements per aminoacid: 4 atoms N C C 0 centroid CG. briefly refer representation5@ model. Figure 3 illustrates atoms involved concatenationtwo consecutive amino acids. Inter-atomic distances consecutive atoms fixeddue chemical bonds; thus, differences structures identifieddifferences angles involved. common find substructures958fiA Constraint Solver Flexible Protein Modelsprotein consecutive amino acids arranged according repeated characteristicpatterns. property found almost every protein; refer typical patternssecondary structure elements. common examples -helices -sheets(see Figure 2).2.2 Context Proposed Workpaper present tool assembling reasoning amino acidsspace. similar approaches (e.g, Simons, Kooperberg, Huang, & Baker, 1997),system relies set admissible elementary shapes (or fragments) representsspatial dictionary arrangements every part protein.element dictionary general enough describe specific atomic structureeither single amino acid longer sequence (even hundreds amino acids long).amino acid sequence, several alternative arrangements expected populatedatabase, offer various hypothesis local shape sequence.protein partitioned contiguous fragments arranged according onepossible shapes recorded database.sequence amino acids free rotate bonds space (typically two degreesfreedom along backbone several others along side chain); however, duechemical properties physical occupancy specific types amino acidsinvolved surrounding environment, arrangements impossible and/or unlikely. core assumption assembling approaches rely statistical databasearrangements describe local feasible behavior, order direct search candidates high probability energetically favorable. presence multiplecandidate fragments every part protein requires combinatorial search amongpossible choices that, assembled together, leads alternative putative configurationsprotein. search process charge verifying feasibility assembly,since combination local arrangements could generate non-feasible global shape, e.g.,one leads spatial clash atoms different fragments. one (or more)fragment described one single arrangement, part protein rigidly imposed.particular degenerate case exploited describe rigid parts protein.specific combination fragment length number instances fragment determines type protein problem modeled. range complete backboneflexibility (fragments made hundreds choices amino acids) secondary structure - loop models (interleaving longer fragments modeling helices/-strands shorterfragments).library fragments usually derived content Protein Data Bank(PDB, www.pdb.org) contains 96,000 protein structures. design adoptedstudy parametric choice library fragments use. example,experiments use library fragments derived subset PDB knowntop-500 (Lovell, Davis, Arendall, de Bakker, Word, Prisant, Richardson, & Richardson,2003), contains non redundant proteins preserves statistical relevance. Alternative libraries fragments obtained use sophisticated protein databasesearch algorithms, FREAD (Choi & Deane, 2010). retrieve information depending specific amino acid sequence, since local properties greatly influence typical959fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelliarrangements observed. Moreover, build libraries different sequences lengths h, evenlonger sequences statistical coverage becomes weak. Nevertheless, Micheletti, Seno,Maritan (2000) conjectured relatively small set fragment shapes (a dozens)length 5, able describe virtually protein. Handl, Knowles, Vernon, Baker,Lovell (2012) demonstrate size structure search space affectedchoice fragment length used optimize search process. Similar considerations explored others (Hegler, Latzer, Shehu, Clementi,& Wolynes, 2009). Recent work show efficiently build dictionaries (Fogolari,Corazza, Viglino, & Esposito, 2012). models easily accommodatedframework.considered sequence associated several configurations 5@ models, placedaccording standardized coordinate system. activity, also consider C 0group preceding amino acid N atom following amino acid.extra information needed fragments combination, assuming fragmentconnected two peptidic bonds. Therefore, specific sequence, storeoccurrencesC 0 N C C 0 N| {z }h timesrelative positions. order reduce impact specific propertiesdatabase used, cluster set way two fragments RMSD1 lessgiven threshold, one stored. example, length h = 1RMSD threshold .2A, derive fragment database roughly 90 fragments peramino acid.CG information added later using statistical considerations side-chain mobility, accounted clustering described (Fogolari, Esposito,Viglino, & Cattarinussi, 1996).2.3 Protein Structure Predictionprotein structure prediction problem, sequence amino acids composing protein (known primary structure) given input; task predict threedimensional (3D) shape (known native conformation tertiary structure)protein standard conditions.common assumption, based Anfinsens work (1973), 3D structureminimizes given energy function modeling atomic force fields, candidate best approximates functional state protein. setting, choicenumber atoms used represent amino acid controls qualitycomputational complexity.Moreover, spatial domains proteins points (e.g., atoms, centroids)placed impact type algorithms search performed.domain either continuous, often represented floating point coordinates,discrete, often derived discretization space based crystal lattice structure.1. Root Mean Square Deviation captures overall similarity space corresponding atoms,performing optimal roto-translation best overlap two structures.960fiA Constraint Solver Flexible Protein Modelsgeometric model determined, necessary introduce energyfunction, mostly based atoms considered distances. structure prediction problem, energy function used assign score geometrically feasiblecandidate; candidate optimal score represents solution predictionproblem.Let us briefly review popular approaches problem, particular emphasis solutions rely constraint programming technology.natural approach investigating protein conformations simulationsphysical movements atoms molecules is, unfortunately, beyond current computational capabilities (Jauch, Yeo, Kolatkar, & Clarke, 2007; Ben-David, Noivirt-Brik, Paz,Prilusky, Sussman, & Levy, 2009; Kinch, Yong Shi, Cong, Cheng, Liao, & Grishin, 2011).originated variety alternative approaches, many based comparative modelingi.e., small structures related protein family members used templatesmodel global structure protein interest (Jones, 2006; Fujitsuka, Chikenji, &Takada, 2006; Simons et al., 1997; Lee, Kim, Joo, Kim, & Lee, 2004; Karplus, Karchin,Draper, Casper, Mandel-Gutfreund, Diekhans, & Source., 2003). methods, oftenreferred fragments assembly, protein structure assembled using small protein subunits templates present relevant sequence similarities (homologous affinity) w.r.t.target sequence.literature, Constraint Programming (CP) techniques shown potential:structural variability protein modeled constraints, constraint solvingperformed order deduce optimal structure (Backofen & Will, 2006; Barahona& Krippahl, 2008; Dal Palu, Dovier, & Fogolari, 2004; Dal Palu et al., 2010). CPused provide approximated solutions ab-initio lattice-based modeling proteinstructures, using local search large neighboring search (Shmygelska & Hoos, 2005;Dotu, Cebrian, Van Hentenryck, & Clote, 2011); exact resolution problem latticespaces using CP, along clever symmetry breaking techniques, also investigated (Backofen & Will, 2006). approaches solve constraint optimization problembased simple energy function (HP). precise energy function usedDal Palu et al. (2004, 2007), information secondary structures (i.e., -helices,-sheets) also taken consideration. Due approximation errors introducedlattice discretization, approaches scale medium-size proteins. Off-latticemodels, based idea fragment assembly, implemented using Constraint LogicProgramming Finite Domains, presented (Dal Palu et al., 2010; Dal Palu,Dovier, Fogolari, & Pontelli, 2011), applied structure prediction alsostructural analysis problems. instance, Dal Palu et al. (2012b) use approach generate sets feasible conformations studies protein flexibility. useCP analyze NMR data related problem protein docking alsoinvestigated (Barahona & Krippahl, 2008).context ab-initio prediction, recent work (Olson, Molloy, & Shehu, 2011)shown increasing complexity conformational search spaceby usingrefined fragment libraryin combination sampling strategy, enhancesgeneration near-native structure sets. work Shehu (2009) Molloy, Saleh,Shehu (2013) illustrates various enhancement fragment-based assembly process leadingfaster computations improved sampling conformation spacee.g., using961fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontellitree-based methods inspired motion planning guarantee progress towards minimalenergy conformations maintaining geometrically separate conformations. termsenergy landscape, native state generally lower free energy non-native structures,extremely difficult locate. Hence, targeted conformational sampling may aidprotein structure prediction different near-native structure used guidesearch; several schemes based Monte Carlo movements sampling conformationspace fragments assembly proposed (Shmygelska & Levitt, 2009; Xu &Zhang, 2012; Debartolo, Hocky, Wilde, Xu, Freed, & Sosnick, 2010). Methods basednon-uniform probabilistic mass functions (derived previously generated decoys)proposed aid problem (Simoncini, Berenger, Shrestha, & Zhang, 2012).Sampling, however, remains great challenge protein complex topologies and/orlarge sizes (Kim, Blum, Bradley, & Baker, 2009; Shmygelska & Levitt, 2009).widely accepted proteins, native state, considered dynamicentities instead steady rigid structures. Indeed, recent years research focusshifted towards prediction schemes take account non-static nature proteins,supported recent observations based magnetic resonance techniques. Processesenzyme catalysis, protein transport antigen recognition rely ability proteinschange conformation according required conditions. dynamic naturevisualized set different structures coexist time. generationsets capture non-redundant structures (in pure geometric terms) greatchallenge (Kim et al., 2009). Robotics inverse kinematics methods extensivelyexplored sampling proteins conformational space (Zhang & Kavraki, 2002; Cortes& Al-Bluwi, 2012) molecular simulations (Al-Bluwi, Simeon, & Cortes, 2012; Moll,Schwarz, & Kavraki, 2007; Noonan, OBrien, & Snoeyink, 2005; Kirillova, Cortes, Stefaniu,& Simeon, 2008).motivation work provide ability generating protein setcontains optimal sub-optimal candidates, order capture dynamic informationbehavior protein. desirable property conformations returnedpool sufficiently diverse uniformly distributed 3D space.2.4 Protein Loop Modelingprotein loop modeling problem restricted version structure prediction problem. use problem working example remaining part paper.context, protein structure already partially defined, e.g., large numberatoms already placed space. Usually, common scenario derives Xray crystallography analysis, spatial resolution atoms degenerates presenceregions protein exposed surface presents increasedinstability. Since crystal contains several copies protein order perform measurement, regions appear fuzzy, therefore placement atomsregions may ambiguous. Usually, regions, referred loops, involvedsecondary structures, instead stable. dealing homology modeling, protein found another organism, typically shows variationssequence due evolution, especially loop regions, since less essentialprotein stability functionality. Starting homologous protein structure, usually962fiA Constraint Solver Flexible Protein Modelsloops need recomputed specialized loop modeling approach useminimization techniques.length loop typically range 2 20 amino acids; nevertheless,compared secondary structures, flexibility loops produces large, physicallyconsistent, conformation search spaces. Constraints mutual positions orientations (dihedral angles) loop atoms deduced used simplify search.restrictions defined loop closure constraints. Figure 2, (simple)possible scenario two macro-structures (two helices) connected loop.setting, assume know position two helices, loop atomsdetermined.procedure protein loop modeling typically consists 3 phases: sampling, filtering,ranking (Jamroz & Kolinski, 2010). Sampling commonly based loop candidate generation, using dihedral angles sampled structural databases (Felts, Gallicchio,Chekmarev, Paris, Friesner, & Levy, 2008), subsequent candidate modification ordersatisfy loop closure constraints. conformations checked w.r.t. loop constraints geometries rest structure, loops detectedphysically infeasible, e.g., causing steric clashes, discarded filtering procedure.Popular methods used loop modeling include Cyclic Coordinate Descent (CCD)method (Canutescu & Dunbrack, 2003), algorithms based inverse kinematics (Kolodny,Guibas, Levitt, & Koehl, 2005; Shehu & Kavraki, 2012), Self-Organizing (SOS) algorithm (Liu, Zhu, Rassokhin, & Agrafiotis, 2009), simultaneously satisfy loopclosure steric clash restrictions iteratively superimposing small fragments (amideC ) adjusting distances atoms, Wriggling method (Cahill, Cahill,& Cahill, 2003), employs suitably designed Monte Carlo local moves satisfy loopclosure constraints. Multi-method approaches also proposede.g., Lee, Lee,Park, Coutsias, Seok (2010) propose loop sampling method combines fragmentassembly analytical loop closure, based set torsion angles satisfying imposedconstraints. Ab initio methods (Rapp & Friesner, 1999; Fiser, Do, & Sali, 2000; Jacobson,Pincus, Rapp, Day, Honig, Shaw, & Friesner, 2004; Spassov, Flook, & Yan, 2008; Deane& Blundell, 2001; Felts et al., 2008; Xiang et al., 2002) methods based templatesextracted structural databases (Choi & Deane, 2010) explored.Finally, ranking stepe.g., based statistical potential energy, like DOPE (Shen& Sali, 2006), DFIRE (Zhou & Zhou, 2002), one proposed Fogolari et al. (2007),used select best loop candidates.sampling filtering procedures work together direct search towards structurally diverse admissible loop conformations, order maximizeprobability including candidate close native one reduce time neededanalyze candidates. work motivated need controlling properties resulting set candidates. particular, model structural diversitydistance orientation backbone make sampling phase guided loopconstraints.Fragment-based assembly methods also investigated context loopmodeling (Lee et al., 2010; Zhang & Hauser, 2013). Shehu Kavraki (2012) reviewgreat detail loop modeling techniques.963fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliFigure 4: left: two fragments B1 (light grey) B2 (dark grey)points(B1 ) = ((0, 0), (1, 0), (1, 1), (2, 1)) points(B2 ) = ((4, 0), (3, 0), (3, 1), (4, 1), (4, 2)).arrows address initial points. right: observe rotating B2 90 degrees translating -3 units x-axis, last three points B1 (last(B1 ))first three points B2 (first(B2 )) perfectly overlap. Thus, end(B1 ) _ front(B2 ).3. Constraint Solving 3D Fragmentsassume reader familiarity basic principles constraint programming constraint satisfaction problems (CSP); reader referred, e.g., Handbook Constraint Programming (Rossi et al., 2006). Section, introduceformalization effective solution tackle practical applications concerningplacement 3D fragments. applications described combinatorial problems,modeled set variables, representing entities problem deals with, setconstraints, representing relationships among entities. context constraintprogramming system, variables constraints adopted provide solutionCSP, is, assignment variables satisfies constraints. extendconcept enabling constraint solver find representative solution CSPsatisfies additional properties expressed among variables whole solution set.3.1 Terminologyfragment B composed ordered list least three (distinct) 3D points, denotedpoints(B). number points fragment referred length. front-end-anchors fragment B, denoted front(B) end(B), two lists containingfirst three last three points points(B). B(i) denote i-th pointfragment B. two ordered lists points p~ ~q, write p~ _ ~qperfectly overlapped rigid coordinate translation and/or rotation (briefly, rototranslation)see Figure 4 (let us assume z coordinate 0 points omittedsimplicity).non-empty set fragments length called body. bodyused model set possible shapes sequence points. say bodylength k fragment contains length k.multi-body sequence S1 , . . . , Sn bodies.964fiA Constraint Solver Flexible Protein ModelsFigure 5: left right: body S1 composed unique fragment, bodiesS2 S3 composed two fragments each. Arrows address initial points fragments.~ = S1 , S2 , S3 constitutes multi-body. rightmostthree bodies length 4.figure report spatial shapes associated four rigid bodies obtained~ One identified full lines, three dashedmulti-body S.lines. Observe rigid body identified ((0, 0), (1, 0), (1, 1), (2, 1), (2, 0), (3, 0))obtained rotation 180 degrees fragment ((2, 0), (3, 0), (3, 1), (4, 1)) S2x axis (flipping) translation 1 units x +1 units y. Observemoreover rigid body identified ((0, 0), (1, 0), (1, 1), (2, 1), (2, 0), (1, 0)) containspoint (1, 0) twice.~ = S1 , . . . , Sn , rigid body~ sequence fragmentsGiven multibodyB1 , . . . , Bn , Bi Si = 1, . . . , n end(Bi ) _ front(Bi+1 ), = 1, . . . , n1.rigid body uniquely identified sequence B1 , . . . , Bn ; however, consecutivefragments overlapped, rigid body alternatively identified list pointsform spatial shape. Figure 5 report examples bodies, multi-bodies, rigidbodies. previous example, assume z coordinate 0 points.Remark 3.1 (Working Example) concepts related loop-modeling problem. Points atoms. fragment spatial shape atoms. last three atomsone fragment overlap first three atoms another fragment, join them.body set admissible shapes given list atoms. multi-body S1 , . . . , Snsequence elements, corresponding sequence atoms (of amino acids).idea last three atoms body Si first three successivebody Si+1 . rigid body possible complete shape atoms, provided last threeatoms fragment selected set Si overlap first three atoms fragmentselected Si+1 .overlapping points end(Bi ) front(Bi+1 ) constitute i-th joint rigidbody. number rigid bodies obtained single multi-body S1 , . . . , Snbounded ni=1 |Si |. Figure 6 provides schematic general representation rigidbody.rigid body defined overlap joints, relies chain relative rototranslations fragments. points points(Bi ) therefore positioned according(homogeneous) coordinate system associated fragment Bi1 . Notereference system B1 defined, whole rigid body completely positioned.22. exception case points joint collinear. Points p1 , . . . , pn , n 3collinear points p3 , p4 , . . . , pn belongs straight line containing two points p1 p2 .965fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliFigure 6: schematic representation rigid body. joints connecting two adjacentfragments emphasized. points points(B) fragment representedcircles. fragment extends first point joint last pointsuccessive joint.relative positions two consecutive fragments Bi1 Bi rigid body (2 n)defined transformation matrix Ti R44 . matrix depends standardDenavit-Hartenberg parameters (Hartenberg & Denavit, 1995) obtained startend fragmentsthe reader referred work LaValle (2006) details.denote product T1 T2 . . . Ti (x, y, z, 1)T Ti (x, y, z).Let us analyze first matrix T1 . fragment B1 forced start givenpoint oriented given way; case matrix T1 defines roto-translationB1 fulfilling constraints. absence constraints, assume B1normalized T1 i.e.,its first point (0, 0, 0), second point aligned along z axisthird belongs plane formed x z axes. orientation referredreference system 0 .= 1, . . . , n, coordinate system conversion (x0 , 0 , z 0 ), point (x, y, z)points(Bi ) coordinate system B1 , obtained by:(x0 , 0 , z 0 , 1)T = T1 T2 . . . Ti (x, y, z, 1)T = Ti (x, y, z)(1)Homogeneous transformations last value tuple always 1.rest paper, focus 5@ model; however proposed formalizationmethods used also models, e.g., C C model. lattercase, points(Bi ) contains least 3 amino acids, joints guaranteed noncolinear, due chemical properties backbone. combining C fragments,specific rotational angles full-atom backbone lost imprecise multibody assembly produced.fragment body associated sequence amino acids. fragment sequenceh 1 amino acids described body length 4h + 3, modeling concatenationatoms represented regular expression: C 0 O(N C C 0 O)h N . representation first last sequence C 0 atoms coincide front- end-anchor,respectively, employed process assembling consecutive fragments (i.e.,used roto-translation).discretized R3 space represented regular lattice, composed cubic cellsside length equal given parameter k. cell referred 3D voxel(or, simply, voxel ); assume voxel receives unique identifier. denotevoxel(p, k) identifier voxel contains 3D point p contextdiscretization space using cubes side length equal k. spatial quantizationallows efficient treatment approximated propagation requiredgeometric constraints introduced following sections.966fiA Constraint Solver Flexible Protein Models3.2 Variables DomainsLet us define variables adopted describe entities problem fragments.domain variable V set allowable values V , denotedDV . deal fragments placements 3D space adopt two distinct typesvariables:Finite Domain Variables (FDVs): domain finite domain variable finiteset non negative integer numbers.Point Variables (PVs): variables assume coordinates 3D point R3 .domains are, initially, 3D boxes identified two opposite vertices hmin, maxi,done discrete solver COLA (Dal Palu, Dovier, & Pontelli, 2005, 2007).Remark 3.2 (Working Example) Following Remark 3.1, FDVs identifiersvarious fragments body, PVs used represent 3D coordinates assignedvarious structural points (e.g., atoms, centroids) interest moleculeconsidered. Clearly, values PVs depend deterministically values FDVs(and vice-versa).variable assigned domain contains unique value; case point variables,happens DV = hmin, maxi min = max.3.3 Constraintssection, formalize constraints define fragments placement,used describe Protein Structure problems context fragment assembly.3.3.1 Distance ConstraintsDistance constraints model spatial properties point variables operating 3D space.Point variables P Q related distance constraint formkP Qk op(2)k k Euclidean norm, R+ op .built-in global constraint alldistant associates minimal radius di pointvariable Pi (i = 1, . . . , n) ensures spheres surrounding pair point variablesintersect:alldistant(P1 , . . . , Pn , d1 , . . . , dn ),(3)constraint equivalent constraints kPi Pj k di +dj i, j {1, . . . , n}, <j. used avoid steric clashes among different atoms (and centroids),different volumes. Checking consistency alldistant constraint (given domainsvariables Pi ) NP-complete (Dal Palu, Dovier, & Pontelli, 2010)the proof basedencoding bin-packing problem using alldistant constraint, holds trueeven particular setting, point variables intervals R3 domains.Remark 3.3 (Working Example) alldistant constraint introduced avoid clashesrigid body obtained multi-body S1 , . . . , Sn . distance constraints967fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliFigure 7: Fragments assembled overlapping plane R , described rightmostC 0 , O, N atoms first fragment (left), plane L , described leftmostC 0 , O, N atoms second fragment (right), common nitrogen atomuseful extra information known (e.g., one might inferred biologicalarguments pair amino acid stay within certain distance).3.3.2 Fragment ConstraintFragment constraints relate finite domain variables point variables. Let us assumedatabase F fragments, F [i] represents i-th fragment database.Thus, given FDV variable V , F [V ] denotes fragment indexed V Vinstantiated. fragments stored F ordered list 3D points.Given list point variables P~ , constraint:fragment(V, P~ , F )(4)states exists roto-translation Rot P~ = Rot F [V ]namely, V =list points P~ take form fragment F [i]. simplicity,omit database F clear context. Intuitively, constraints ensurefragment choice reproduce correct shape associated 3D point,regardless space orientation fragment. orientation determinedjoined multi-body constraint presented following section.3.3.3 Centroid Constraintcentroid constraint enforces relation among four PVs. Intuitively, first threeassociated atoms N, C , C 0 amino acid fourth relatedcentroid CG. constraint parametric w.r.t. type amino aciddeterministically establishes position CG depending position points:centroid(PN , PC , PC 0 , PCG , a)(5)Figure 7 centroids displayed along backbone purple circles labeledCG. constraint used database fragment contains full backbone information. centroid information used place missing full-atom sidechain. side-chain centroid computed taking account average C -side-chaincenter mass distance, average bend angle formed side-chain center-of-massC -C 0 , torsional angle formed N -C -C 0 -side-center mass (Fogolari et al.,968fiA Constraint Solver Flexible Protein Models1996). abstraction allows us reduce number fragments consider, removingfragments would geometrically conflict position CG. Considersingle side chain may 100 main configurations (rotamers).3.3.4 Table Constraintconstraint used restrict assignments set FDVs (representing fragments)specific tuples choices. useful modeling specific local collaborativebehavior involves one fragment; example, happens modelingsecondary structure multiple arrangements underlying amino acids and/or specificapproximation strategies employed.~ k-tuple FDVs. table (orLet F set k-tuples integer values Vcombinatorial) constraint, form~ ,F)table(V(6)~ assumes values restricted tuples listed F , i.e.,requires list variables V~exists F V [i] = t[i], 0, . . . , k 1.Remark 3.4 (Working Example) Going back loop-modeling problem, rolefragment constraint evident: relates (IDs the) selected fragments multibody 3D positions various atoms involved. centroid constraintinstead introduced add position centroid represents side chain5@ representation. table constraint common constraint constraint languagesuseful info consecutive fragments rigid body known due externalknowledge.3.3.5 Joined Multibody ConstraintJoined Multibody (JM) constraint enforces relation list FDVs encodingmultibody. limits spatial domains various fragments composing multibodyorder retain fragments assemble properly compenetrate.~ V~ , A,~ E,~ i, where:joined-multibody (JM) constraint described tuple: J = hS,~ = S1 , . . . , Sn multi-body. Let B = {B1 , . . . , Bk } set fragments S,~Sni.e., B = i=1 Si .~ = V1 , . . . , Vn list FDVs, domains DVi = {j : Bj Si }.V~ = A1 , A2 , A3 , E~ = E1 , . . . , E3n lists sets 3D points that:A1 A2 A3 set admissible points front(B), B S1 ;E3i2 E3i1 E3i set admissible points end(B), B Si , = 1, . . . , n;constant, used express minimal distance constraint different point.~ {1, . . . , |B|} s.t. existsolution JM constraint J assignment : Vmatrices T1 , . . . , Tn (used ) following properties:Domain: = 1, . . . , n, (Vi ) DVi .Joint: = 1, . . . , n 1, let (a1 , a2 , a3 ) = end(B(Vi ) ) (b1 , b2 , b3 ) = front(B(Vi+1 ) ),holds (for j = 1, 2, 3):Ti (ajx , ajy , ajz ) = Ti+1 (bjx , bjy , bjz )969fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliSpatial Domain: Let (a1 , a2 , a3 ) = front(B(V1 ) ), T1 aj Aj {1}.3 =1, . . . , n, let (e1 , e2 , e3 ) = end(B(Vi ) )Ti (ejx , ejy , ejz ) E3(i1)+j {1}1 j 3 T2 , . . . , Ti (in Ti ) matrices overlap end(B(Vi1 ) )front(B(Vi ) )Minimal Distance: j, ` = 1, . . . , n, j < `, points points(B(Vj ) )b points(B(V` ) ), holds that:4kTj (ax , ay , az ) T` (bx , , bz )kproved establishing consistencyi.e., existence solutionof JMconstraints NP-complete (Campeotto et al., 2012). also proved remainsNP complete even assuming fragments problem threeatoms spatial position, holds last three atoms (ofcourse fragments allowed contain three atoms otherwise problemtrivial). proof reported www.cs.nmsu.edu/fiasco/.Remark 3.5 (Working Example) JM constraint contains exactly ingredients~ corresponding FDsneeded modeling loop problem. multi-body S,~ , set possible 3D points loop starts~ set possible 3DV~points loop ends E weak version alldistant constraint pair~atoms avoid clashes, solutions (non clashing) rigid bodies starts~ends E.Let us observe JM constraint explicitly forbid spatial positions PVsvariables (save first three last three points loop). However,additional constraints explicitly required domain definition PVs variablesused encoding.Remark 3.6 choice using three points overlap resembles method proposedKolodny, Guibas, Levitt, Koehl (2005). hand, observetechnical exercise modify JM constraints allow parametricoverlap contiguous fragments.4. FIASCO Constraint Solverpresent overall structure implementation hybrid constraint solver capablehandling classes constraints described previous section.4.1 Constraint Solvingdistinctive feature FIASCO possibility handle continuous domains costkeeping discrete library choices (finite domain variables). handling fragmentsallows us reason spatial properties efficient descriptive waypure 3D domain modeling adopted previous proposals. Moreover, FIASCO allows3. product {1} necessary use homogeneous coordinates.4. Let us observe weak form alldistant constraint different distancespoint allowed. is, sense, closer alldifferent constraint.970fiA Constraint Solver Flexible Protein Modelssolver uniformly sample search space means spatial equivalence relationused control tradeoff accuracy efficiency. particularlyeffective finite domains heavily populated, critical componentmodel real-world problems.constraint solver builds classical prop-labeling tree exploration constraint propagation phases interleaved non-deterministic branching phases usedexplore different value assignments variables (Apt, 2009). solver able handlepoint variables finite domain variablesthis reason referhybrid solver. particular, assignments finite domain variables guide search;values imply assignments point variables, turn may propagate reducedomains point variables finite domain variables. Moreover, propagation technique implemented JM constraint classical filtering techniqueitapproximated technique describe later.presence point variables allows, principle, infinite number domain valuesR3 . However, noted information carried assembling fragments (encodedfinite domain variables) much informative complex demandingmodel 3D continuous space (e.g., Oct-trees, CSG, no-goods). particular, directkinematics encoded JM constraint able efficiently identify set admissibleregions point variable fast, approximated, controlled way. Therefore,point variables seen internal aid propagation. variables updatedJM propagation phase interact JM propagator prunecorresponding fragment variables. Distance constraints point variables includedstandard AC3 propagation loop domains updates.aspect extends classical solver structure capability controlling amount search tree explored. search tree contains large numberbranches similar, point view geometric distancecorresponding point variables. goal produce subset feasible solutionsexhibit significant 3D differences themselves. accomplished introducingpossibility explore subtree given depth, enumerating specific limitednumber branches, rather following standard recursion propagation expansion. achieve behavior, necessary selectively interfere standardrecursive call solver, implement non-deterministic assignment partial tuplesfinite domain variables. resembles implementation table constraint,dynamically created search. strategy allows us significantly reducenumber branches explored subtree, produces significant resultsselection branches controlled adequate partitioning function. work,propose effective partitioning function based measure 3D similarity pointvariables; used direct search along specific branches controlled depthadequately separated partitioning function. practically realizedintroducing form look-ahead, controlled JM propagator, returns setpartial assignments well filtered domains finite domain variables.4.1.1 Hybrid Solver971fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli~ , P~ , D,~ C, `)Algorithm 1 search(V~ , P~ , D,~ C, `Require: V~ |1: ` > |V2:output (P~ )3:return4: end5: fragment index f Dv`~ , P~ , D)~6:AC-3(C {v` = f }, Vnm7:get table JM()8:n > 09:Non-deterministically select 1..n10:j = 1..m11:C C {v`+j = [i][j]}12:end~ , P~ , D,~ C, ` + m)13:search(V14:else~ , P~ , D,~ C, ` + 1)15:search(V16:end17:end18: endgeneral structure solver highlighted Algorithm 1. solver designed~ = v1 , . . . , vn finite domains variables, together domainsprocess list VDv1 , . . . , Dvn them. Intuitively, domain set indices set fragments.Moreover, solver receives list P~ = p1 , . . . , p5n 5 n point variables,variables p4i , . . . , p4i+4 related fragment domain Dvi . point variable~pj has, turn, spatial domain Dpj . C represents constraints elements VP~ . Finally, solver receives also input current level ` explorationsearch tree (set 1 first time procedure called). sake simplicity,choice variables assigned based ordering input list (moresophisticated selection strategies easily introduced). enter level `,assume variables v1 , . . . , v`1 already assigned.~ already assignedLet us briefly describe algorithm. variables V(lines 14), search algorithm terminates returns computed solution, represented values assigned variables P~ . Otherwise, non-deterministically selectfragment index domain variable v` assign variable. Lines 67indicate execution standard constraint propagation step (using AC-3). propagation step fails, assume another non-deterministic choice made, possible.Every reference non-deterministic choice algorithm corresponds creationchoice-point target backtracking case failure (for simplicity,assume chronological backtracking). succeeds, leading possible reduction~ computation proceed. table constraint might produceddomains D,propagation JM constraint AC-3 procedure (see details).case (lines 89), (m) variables non-deterministically assignedvalues table (lines 912), search continues less variables972fiA Constraint Solver Flexible Protein Modelsassigned (line 13). case, search continue one lessvariable (v` ) assigned (line 15).peculiar feature constraint solver (not reported abstract algorithmdefined) used avoid search solutions similar others.Let us assume 3D space partitioned cubic voxels size k A. Then, given list~ list PVs P~ , user state:FDVs V~ , P~ , k)uniqueseq(V(7)constraint forces solver prune search tree following way. Given~ variable assigned next steppartial assignment , let v V~p1 , . . . , ph P PVs consequently instantiated. constraint ensurestwo assignments 1 , 2 extending v, p1 , . . . , ph holds exists leastone {1, . . . , h} 1 (pi ) 2 (pi ) belong voxel.4.2 Constraint Propagationsection, discuss propagation rules associated various constraints introduced Section 3.3; applied within call AC-3 procedure (line 6Algorithm 1). constraint propagation used reduce domain size PVsFDVs, ensuring constraint consistency. AC-3 standard implementation fixpointpropagation loop (Apt, 2009; Rossi et al., 2006).4.2.1 Joined Multibody ConstraintJM constraint complex constraint triggered leftmost points involved constraint (anchors) instantiated. JM propagation (JMf) basedanalysis distribution space points involved. goal propagation reduce domains FDVs identification fragmentscannot contribute generation rigid body compatible corresponding Point Variable domains. viewed form hyper-arc consistencyset fragments. Moreover, due complexity precision considerations,propagator approximated use spatial equivalence relation (), identifiesclasses tuples fragments; classes property spatially differentone another.allows compact handling combinatorics multi-body, controllederror threshold allows us select precision filtering. equivalence relationcaptures rigid bodies geometrically similar, allowing search compactsmall differences among them.~ V~ , A,~ E,~ i, alongJMf algorithm receives input JM-constraint hS,set G points available placement bodies,equivalence relation .sake readability, assume domain information variables avail~ , Tab). process, algorithmable. algorithm builds table constraint table (Vmakes use function (lines 7 8); function takes input two lists ~a ~b3D points, computes homogeneous transformation overlap ~b ~a. call973fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliAlgorithm 2 JMf algorithm.~ V~ , A,~ E,~ , G,Require: S,Ensure: Tab~ |; Tab =1: n |VT1 start(B) A1 A2 A3T1 end(B) E1 E2 E32: R1 (B, T1 ) B S1 , T1p points(B).q G. k(T1 p) qkc C involving p.consistent(c))3: P1 {T1 end(B) | (B, T1 ) R1 }4: = 2, . . . , n5:Pi = ; Ri = ;6:EPi1 /= (E, start(B)) 6= failend(B) E3i2 E3i1 E3i7:Ri Ri B Sip points(B).q G. k(T p) qkc C involving p.consistent(c))8:Pi {(E, start(B)) end(B) | B Ri }9:end10:compute Pi / filter Ri accordingly11: end12: representative L Pn /13:Tab = Tab (L)14: endfunction fail ~a 6_ ~b. simplicity, fourth component (always 1)homogeneous transformation explicitly reported algorithm.~ |, algorithm computes sets Ri Pi , respectively= 1, . . . , n = |Vcontain fragments Si still lead solution, corresponding allowed3D positions end-points. fragment B Ri+1 denote parent(B)set fragments B 0 Ri end(B 0 ) _ front(B) via . fragment B,denote label(B) corresponding FD value associated.computing/updating Ri Pi , fragments end-anchors containedbounds E3i2 , E3i1 , E3i kept. Fragments would cause points collapsei.e.,due distance smaller previously placed pointsare filtered (lines 27). Moreover, spatial positions points first fragment validated(line 2); finally, enforce consistency check constraint c C involving pointspoints(B) Si retain points potentially reach admissible positions(lines 2 7).~ | 1 iterations (lines 411). First Ri Pi computedalgorithm performs |Vbasis sets end-anchors previous level Pi1 starting pointselected fragment B, filtering overlapping leadwrong portions space (lines 78). filtering based applied (line 10).step, set triples 3D points Pi clustered using . representativeequivalence class chosen (within Pi ) corresponding fragment Ri identified;(non-identified) fragments filtered Ri . Let us also note974fiA Constraint Solver Flexible Protein Modelsfiltering based clustering performed initial step P1 , typicallyalready captured restrictions imposed A.fragments reachable last iteration determined representativesselected, populate Tab set tuples associated representative L.~ allows us overlap last pointfunction (L) returns assignments VL.JMf algorithm parametric w.r.t. clustering relation function selectingrepresentative; express degree approximation rigid bodiesbuilt. proposed clustering relation loop modeling takes account two factors: (a) positions end-anchors 3D space (b) orientationplane formed fragments anchor L w.r.t. fixed reference system 0 adoptedFIASCO (c.f. Figure 7). combination clusterings allows capture local geometricalsimilarities, since spatial rotational features taken account.spatial clustering (a) used following. Given set fragments, three endpoints C 0 (end anchors) cluster considered, centroid triangleC 0 computed. use three parameters: kmin , kmax N, kmin kmax , r R,r 0. start selecting set kmin fragments, pairwise distant least 2r.fragments selected representatives equivalence class fragments fallwithin sphere radius r centered centroid representative. clusteringensures rather even initial distribution clusters, however fragments may fallwithin kmin clusters. allow create kmax kmin new clusters,covering sphere radius r. Remaining fragments assigned closestcluster. employed technique variant k-means clustering algorithm calledleader clustering algorithm; allows fast implementation acceptable results.orientation clustering (b) partitions fragments according relative orientation planes R w.r.t. 0 . plane spatial orientation described Euler angles, , frame w.r.t. 0 . algorithm produces variable number partitions depending . particular, given threshold > 0 3 (360/) possible partitionsdescribing equal regions sphere though interval ( / 2 , / 2 , / 2 ).fragment allotted partition determined .final cluster intersection two partitioning algorithms. definesequivalence relation depending kmin , kmax , r, . representative selectionfunction selects fragment partition according preferences (e.g.,frequent fragment, closest center, etc.).Note r = 0, = 360, kmax unbounded, clustering performedwould cause combinatorial explosion every possible end-anchor wholeproblem. spatial error introduced depends r . = 360, errorintroduced step bounded 2r dimension. iterationerrors linearly increased, since new fragment placed initial error gatheredprevious iterations, thus resulting 2nr bound last end-anchor. Clearlybound coarse, average experimental results show better performances.Similar considerations argued rotational errors, however intersectiontwo clusterings, provide, general, much tighter bound.975fiF. Campeotto et al.Campeotto, Dal Palu, Dovier, Fioretto, & PontelliFigure8: graphicalrepresentationpropagationJM constrainttheconstraintvariables Viover, . . . , Vthei+3 .variables Vi , . . . , Vi+Figure9: graphicalrepresentationtheofpropagationJM(a) simultaneous placement elements domain variable Vi+1 simulated,(a) simultaneous placement elements domain variable Vi+1 simulated,overlapping corresponding fragment end-anchor fragment associated elementoverlappingeachsetcorrespondingfragmentend-anchorfragmentelementdomainVi .points Pi+1computedwithclusteredusing theofrelation(pointsassociatedwithindomainVieach.set onepointsPi+1computedandchosenclusteredusing fragmentsrelation (pointsdottedellipses). ofForclusterfragmentrepresentativehence(highlighteddottedellipses).Forcollectionclusterone fragmentrepresentativehence(highlighted fragmenfilledrightmostcircle).representativesconstitutesset Ri+1(b) Thechosenpreviousstep iswithperformedcircle).basisend-anchorsfragmentsconstitutesrepresentativesini+1 (b) previofilledrightmostcollectionrelatedrepresentativesthechosenset Rpreviousfilledbox,onrepresentssetthepointsGavailableplacementstep islevel.performedbasisend-anchorsrelatedtheforfragmentsrepresentatives chosenbodies (for instance due distance constraint). fragment falling area discarded.previouslevel.filledbox,representssetpointsGavailableplaceme(c) last iteration JMf algorithm set points Pi+3 clustered,bodies(forinstanceduedistanceconstraint).fragmentfallingareadiscardereach desired position retained, instance front-anchor associated fragment next(c)lastiterationJMfalgorithmsetpointsPclustered,thvariable, sequence fragments able lead condition (marked thicki+3 lines) selectedpopulatetableTab.reach desired position retained, instance front-anchor associated fragment nevariable, sequence fragments able lead condition (marked thick lines) selectpopulate table Tab.976fiA Constraint Solver Flexible Protein ModelsP||P-Q||dQFigure 9: effect distance constraint ||P Q|| propagation. Empty boxesrepresent original PVs domains full boxes represent reduced PVs domainseffect constraint propagation.4.2.2 Distance Constraintspropagation distance constraints approximated technique reducessize box domains. introduce following operations PVs box domainstwo variables P Q used describe propagation rulefollowing subsections:Domain intersection: DP DQ = hmax(Pmin , Qmin ), min(Pmax , Qmax )iDomain union: DP DQ = hmin(Pmin , Qmin ), max(Pmax , Qmax )iDomain dilatation:DP + = hPmin d, Pmin + dimax(P, Q) = (max(Px , Qx ), max(Py , Qy ), max(Pz , Qz )), (and similarly min),P + = (Px + d, Py + d, Pz + d).Given two point variables P Q, domains DP DQ , respectively, simplification rule constraint ||P Q|| updates domains follows:DP = ((DQ + d) DP )DQ = ((DP + d) DQ )(8)ensures points DP DQ positioned within approximationsphere radius d. sphere approximated considering box inscribing (a cubeside 2d), illustrated Figure 9.propagation constraint ||P Q|| harder coarse representationbox domains adopted work model PVs allow descriptioncomplex polyhedron. hence apply simple form bound consistency describedfollowing rule:(DP DQ ) = hl, ui, ||u l|| <P||P Q|| := , DQ =establishes unsatisfiability constraint.977(9)fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli4.2.3 Fragment Constraintpropagation fragment constraints fragment(V, P~ , ) exploited solutionsearch enforce assembly process fragment [V ] along point variablesP1 , . . . , Pn P~ . Recall DV domain V containing references {j1 , . . . , jk }database fragments .P1= {p1 }, DP2 = {p2 }, DP3 = {p3 }, DV = {j1 , . . . , jk }~fragment(V, P , ) :jkn^[{((p1 , p2 , p3 ), [f ]) [f ](i)}DPi = DPii=1(10)f =j1((p1 , p2 , p3 ), [f ]) roto-translation applied overlap first threepoints fragment [f ] start-anchor (p1 , p2 , p3 ).conjunction bottom part rule re-evaluates domains P1 , P2 , P3 ,may reduce singleton domains empty whenever compatibleselected fragment.4.2.4 Centroid Constraintpositions atoms N , C C 0 amino acids determined,propagation algorithm enforces value PV PCG involved centroid constraint.PN = {pN }, DC = {pC }, DPC 0 = {pC 0 }centroid(PN , PC , PC 0 , PCG , a) : P(11)CG = (DPCG {cg(pN , pC , pC 0 , a)})cg(pN , pC , pC 0 , a) support function returns center massside chain amino acid considering points pN , pC , pC 0 , describedSect. 3.3.3.4.2.5 Implementation Detailsproposed solver relies efficient C++ implementation, carefully designedallow additional tailored solving capability without need reshaping core structures.internal representation domains finite domain variables abstracted two arrays length size initial domain. One array pointsvalues Boolean bit-mask states whether value stilldomain. flags set 0, current partial assignment cannot part solutionoverall constraint; exactly one set 1, variable assigned value.representation implies linear scan domains propagationjustified reasonably small size domains target application (typically less100 values). internal representation domains point variables simplypair hmin, maxi uniquely characterizes 3D box R3 . Since variables usedmostly distance constraints, representation expressive enough (Oct-treesconsidered significant advantage).Point Variables propagation described above; variables instantiatedfragment selection.978fiA Constraint Solver Flexible Protein Modelsmanagement uniqueseq property (7) implemented dedicated datastructure based hash tables. Every time PV assigned, value mapped 3Dvoxel fixed size. 3D grid implemented via Hash Table voxel indexes keyspoints contained voxels values. operations performed O(1)(amortized complexity).4.3 One JM Constraintsbriefly describe modeled two problems FIASCO. JM constraintable model geometrically assembly fragments therefore used everyprotein model. single JM covers protein ensures flexibility, however longproteins computational precision issues arise. beneficial modelprotein multiple JM constraints, e.g. JM (i, j) JM (j, k) amino acidsj covered JM constraints overlap common amino acid.practical choice improves approximate search allows increase numberdifferent solutions produced. practice, protein section handled JM constraintpotentially combined different arrangements sections. Therefore,expected number solutions found grows exponentially number JMconstraints. JM constraint parameters used control clustering precisionnumber conformations found.5. Experimental Resultsreport experimental results obtained FIASCO system (availablehttp://www.cs.nmsu.edu/fiasco). Experiments conducted Linux Intel Core i7860, 2.5 GHz, memory 8 GB, machine. solver implemented C++.fragment database adopted FREAD database shown effective loop structure prediction (Choi & Deane, 2010). parameters analysis 5.1.4use database fragments length 1. fragments classified aminoacid class frequency occurrence whole top-500.set system model two applications described below. particular,Section 5.1 analyze loop modeling scenario focus performances JMfiltering examining filtering power computational costs. Next, comparequality loop conformations generated, measuring RMSD proposed looprespect native conformation. present relationships amongJM parameters control quality efficiency.Section 5.2 show examples ab-initio protein structure predictionconclude comparison FIASCO constraint solvers, protein modelsdescribed common subset constraints.5.1 Loop Modelingloop modeling problem formalized presence two known (large) fragmentsfixed space. sequence amino acids length n given connectingtwo parts protein. JM constraint defined sequence, particularattention starting ending points fixed. start first fragment979fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliFigure 10: example loop computed toolend last fragment, namely sequence C 0 (initial points) coordinates~a = (a1 , a2 , a3 ), sequence C 0 (final points) coordinates ~e = (e1 , e2 , e3 )known. one caveat end points: due discrete nature fragmentassembly, unlikely exactly reach final points. accommodate errors,require JM constraint produces results fall within thresholdcorresponding final points.Figure 10 show Example loop computed tool (the parts proteinconnected shown left connecting loop right).Additional spatial constraints points (e.g. no-good regions determined presenceatoms) given. constant (now = 1.5A) asserts minimum distancepairs atoms.5.1.1 Filtered Search Space Performancesselected 30 protein targets set non-redundant X-ray crystallography structuresdone Canutescu Dunbrack (2003). partitioned proteins 3 classesaccording loop region lengths (n = 4, 8, 12). model CSP usesfragment assembly model loop, particular using JM constraint loopregion.assess filtering capabilities FIASCO, perform exhaustive search generating solution protein targets. Using clusterization 0.2A, numberdifferent fragments length 1 found amino acid (see Fig. 11). sizedomains corresponding FDVs bound 100this adequate samplingdescribe reasonable amino acid flexibility. cases number fragmentsexceeds 100, 100 frequent ones kept.increases likelihood generating loop structure similar nativeone. loop length n generates exponential search space size bounded 100n .selected variable leftmost one. Fragments selected decreasing frequency order.imposed JM constraint every 4 consecutive amino acids. clusteringparameters set follows: kmin value equal size domains,980fi100 120 140806040020N. different FragmentsConstraint Solver Flexible Protein ModelsCEFGHKLNPQRVWAmino acidsFigure 11: Number different fragments (after clustering) per amino acid datasetused different values kmax based loop lengths. values r set120 0.5 setting. summary parameters listed Table 1.Table 1 report average times needed exhaustively explore loop searchspace, average number solutions generated.n4812# JM123JM Parameterskmin kmax100 1000 120100500 120100100 120r0.50.50.5Full JM# Solutions Time (s)5973.139850710.1232830928.87Table 1: Loop Modeling settings average running times (in seconds) numbersolutions generated.5.1.2 JM Approximated Propagator QualityEven approximated JM produces small set solutions, showgood representation overall variability protein structure. test,compare solutions means RMSD original structures. experimentscarried 30 protein targets settings described Table 1,exception kmax loop set size 12, set 500.Figure 12 show bar chart RMSD predictions proteinloop within group targets analyzed. Precisely, x-axis 30 (10loop length) protein targets. bar reports best RMSD (dark), averageRMSD (grey), worst RMSD (light grey) found. Numbers bars representnumber loops found (multiplied factors indicated underneath). resultsbiased fragment database use: excluded fragments belong981fi2.10.151.13.70.30.3865.64.71.30.780.590.961.31.31.10.670.650.830.640.680.310.770.710.20.520.191.360.928Best RmsdAvg RmsdWorst Rmsd402RMSD (Angstrm)2.9Campeotto, Dal Palu, Dovier, Fioretto, & Pontelli. 105. 103Length 4Length 8. 107Length 12Figure 12: RMSD comparison Loop Set (x-axis: 30 protein targets)deposited protein targets. Therefore, possible reconstruct originaltarget loop none searches expected reach RMSD equal 0.loops length 8 12, exploration whole conformational search spaceusing simple search procedure would result excessively long computation time.enforces need propagator JM, filtering algorithm successfully removesredundant conformations allows us cover whole search space short periodtime.Fig. 12 loop predictions calculated using fragments length 1. studychoice affects time accuracy sampling also model loops length12 using fragment length 3, 6, 9. Best RMSDs reported Figure 13.experiments kept settings used (kmax = 500). Moreover, JM constraintimposed fragments order cover whole fragment (e.g, fragmentslength 3 set JM constraint every three consecutive amino acids) set time-out3600 Seconds.Notice increasing length fragments accuracy decreases duereduced size domains. Nevertheless, time also reduced since samplingperformed smaller search space JM constraints cover longer sequences aminoacids. average times are: 1580.14, 0.98, 0.74 seconds using fragments length 3,6, 9 respectively.982fi3201RMSD (Angstrm)45Constraint Solver Flexible Protein ModelsLen3Len6Len9Figure 13: RMSD comparison loop sampling loops length 12 using fragmentslength 3, 6, 9.983fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli5.1.3 Comparison State-of-the-art Loop Samplerssection, compare method three state-of-the-art loop samplers: CyclicCoordinate Descent (CCD) algorithm (Canutescu & Dunbrack, 2003), Self-Organizingalgorithm (SOS) (Liu, Zhu, Rassokhin, & Agrafiotis, 2009), FALCm method (Lee,Lee, Park, Coutsias, & Seok, 2010).Table 2 shows average best RMSD benchmarks length 4, 8 12computed four programs. report results given Table 2 CanutescuDunbrack CCD, Table 1 Liu et al. SOS, Table II Lee et al.FALCm method, RMSDs obtained adopting settings JMf providedbest results previous section (see also Subsection 5.1.5). notedresults line produced systems.LoopLength4812Average (best) RMSDCCDSOSFALCmJMf0.560.200.220.271.591.190.720.933.052.251.811.58Table 2: Comparison loop sampling methodsexecution time reported appear competitive (e.g., consideredresults reported Soto et al., 2008).5.1.4 JM Parameters Analysissection, analyze impact JM parameters quality bestsolutions found execution times. particular, aim experimentsshed light relationship JM constraint settings results.Figure 14, analyze impact kmax execution times (left)precision (right) filtering JM constraint. top bottom, use= 60, 120, 360. tests performed protein loops length 4 (see sectionabove), adopting cluster parameters, r {0.5, 1.0, 3.0, 5.0}, kmin = 100. dotplots represents average best RMSD found predictions (left)average execution time (right). RMSD values tend decrease smaller clusteringparameters r number clusters increases, filtering time increaseskmax increases.Figure 15 study relation RMSD number JMscover given target loop protein Voxel-side parameter. experimentsused values {100, 250, 500, 800, 1000} kmax , set r = 1, = 120,averaged RMSDs values resulting sample set structures. relationRMSD number JM well average worst computational timesshown Fig. 15 left. use medium-length loop taken protein 1XPC(res. 216-230) vary number JMs cover loop (the side voxelset 3A). figure observe increasing number JMs (i.e.covering less amino acids single JM) RMSD decreases computationalcost higher. Notice best RMSD given loop covered 4 JM984fiA Constraint Solver Flexible Protein Models0.950.00.80.610.0r0.5rTime (s))RMSD (A0.70.51.03.05.00.40.51.03.05.01.00.30.50.2100001000.11001000500010005000JM kmax10000JM kmax50.00.90.810.00.6r0.51.03.05.00.5Time (s))RMSD (A0.7r1.00.40.51.03.05.00.50.30.210010005000100001001000JM kmax500010000JM kmax0.90.80.6r0.50.51.03.05.0Time (s))RMSD (A0.7r1.00.50.51.03.05.00.40.35000100000.21001000100JM kmax1000500010000JM kmaxFigure 14: Comparison best RMSD values execution times varyingkmax clustering parameter = 60 (top), 120 (center), 360 (bottom)985fi7RMSD1LE01MXN1FDF001232091.72 (3216.94)1105.63 (2057.83)194.70 (411.95)9.53 (19.54)11.73 (18.42)42RMSD45668Campeotto, Dal Palu, Dovier, Fioretto, & Pontelli123450N.of JM20406080100VoxelSideFigure 15: Left: RMSD (best average) Time (average worst) values increasingnumber JM constraints completely cover target loop length 15. Right:Average (dotted line) best (solid line) RMSD targets 1LE0 length 12 (top),1MXN length 16 (medium), 1FDF length 25 (low). JM-Voxel-side parametervoxels clustering varies 3 100. JM kmax parameter varies100 1000. targets completely covered multiple JM-constraints.constraint (i.e., JM constraint four consecutive amino acids). rule thumbsuggest use JM constraint cover 3 4 consecutive amino acids sincesetting produces best results within acceptable time. Fig. 15 right reportbest RMSD (solid line) average RMSD (dotted line) structures found usingmultiple JM constraints cover sequences 4 consecutive amino acids wholetarget proteins. Namely, protein target length n, set JM constraints+ 3, = 3 j, 0 j < n/3. experiments, considered three proteinsrelatively short length, order obtain complete exploration search spacereasonable computational time: 1LE0 (length 12), 1MXN (length 16), 1FDF (length24). Moreover used values {3, 5, 10, 20, 30, 50, 100} side voxels usedclustering.Figure 15 observe voxel size (enabled uniqueseq)impact clustering values lower 30A (recall proteins diameterless 30A). voxel sides lower 3A observe substantial improvementterms quality, time required solver compute solutions increasesexponentially.5.1.5 Results Summary Default Parametersprovide guidelines may helpful tune JM parameters givenprotein modeling problem. suggest several levels parametrization might usedaccording user needs respect running time prediction accuracy. stress986fiA Constraint Solver Flexible Protein Modelsmerely guidelines, outlined empirical evaluations, severaltests done establish desired tuning.suggest set JM model sequence least 3 amino acids generallonger 8, payoff computational load JM clustering. default choicekmin set average size variable domains involved JM constraint,suggest set kmax least kmin greater 10000. latter,together number consecutive JM constraints, greatest impactcomputational cost prediction accuracy. Computational costs grow numberconsecutive JM increases, time also produce general higheraccuracy. trend exhibited growing kmax parameter. Table 3 illustratesfive basic settings could used incrementally establish trade runningtimes prediction accuracy. first level (Lev. 1) associated faster computationaltimes lower accuracy last one (Lev. 5) slowest also accurate.second column table indicates length amino acid sequence modeledsingle JM.Lev.12345n.JM88644kmin|D||D||D||D||D|kmax50010001005001000120120120120120r53331SpeedAccuracyTable 3: JM default parameters5.2 Application Protein Structure Predictionprotein structure prediction problem, model generic backbone multipleJM constraints. principle, unique JM constraint model whole problem.previous cases, split smaller parts, moreover, presence secondarystructure valid help placement JM constraints handle loopsconsecutive pair. simple search generate pool conformations, energyscoring select best candidate. used statistical energy function developed5@ model, energy function used instead.section, study applicability FIASCO protein structure predictionproblem. particular, consider prediction problems secondary structureelements protein given. Furthermore, order assess potential structure,introduce energy functionthe adopted previous studies,precisely described http://www.cs.nmsu.edu/fiasco.modeling, used information location typesecondary structure elements primary sequence provided directly ProteinData Bank. imposed sequence JM constraints every consecutivepair secondary structure elements. number consecutive JM constraints variedaccording length unstructured sequence modeled, covering 5amino acids single JM constraint. addition one JM constraint imposedfirst amino acid beginning first secondary structure element another987fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelliend last secondary structure element last amino acid (the tailsprotein). domains initial end points JM constraints setadmissible points (a box large enough contain protein). search phase,first secondary structure deterministically set space. labeling proceedsJM constraint attached leading next secondary structure on.Tails instantiated end.propagation constraints generates set admissible structures, represents possible folds target protein. set, select structureminimum energy; extract also structure minimum RMSD, order evaluatequality energy function. tests adopt FREAD database. Table 4reports best energy values found FIASCO. RMSD columns reportedcorresponding RMSD associated conformation best energy found solver.#JM column reports total number JM used model protein, togethermaximum number consecutive JM adopted model contiguous sequenceamino acids (within parentheses).Protein ID1ZDD2GP82K9D1ENH2IGD1SN11AIL1B4R1JHGLen.3540445460636979100# JM4(2)4(2)5(2)4(1)7(2)7(3)4(1)11(2)7(1)Energy100513138110204693309896295882358874411077313590572950RMSD2.056.282.528.2110.505.554.596.114.51Time (Min.)11.428.552.6931.6726.4714.824.468.414.50Table 4: Ab initio prediction FIASCO.results show quality predictions computed FIASCO (6.3 averageRMSD) competitive (and, shown following section, par betterproduced methods). results particularly encouraging proteins longerlength, sampling search space aids development admissible structures.time required FIASCO completely explore search space depends stronglytype mutual arrangement secondary structure elements target.example, protein 2K9D protein 1ENH length, FIASCOsignificantly faster first protein second one. observationmade proteins 2IGD 1SN1. results reported Table 4 promisingsuggest feasible approach solve ab initio prediction problem.future work, explore integration local search techniques (e.g., largeneighboring search), order sample search space decrease timeneeded explore it.988fiA Constraint Solver Flexible Protein Models5.3 Comparison FIASCO State-of-the-Art Constraint Solverssection, motivate choice designing ad-hoc solver instead usinggeneral-purpose constraint solver. particular provide comparison FIASCOstate-of-the-art constraint solving. results justify choice implementing newsolver scratch instead using available constraint programming library constraint programming language. solver chosen comparison Gecode (GecodeTeam, 2013), efficient solver winner recent MiniZinc challenges (Stuckey, Becket, & Fischer, 2010).Gecode recently introduced (in version 4.0) handling floating point variables.Nevertheless, since Gecode fastest solver FD variables, first encodedPSP discretizing fragments positions. particular, multiplied realnumber scaling factor (100) obtain integer values. spatial position encodedtriple variables, representing coordinates point. operation (e.g.,multiplications) applied variables requires re-scaling result; unfortunatelyleads ineffective propagation. particularly evident dealing distanceconstraints, require implementation Euclidean distance pairs triplesvariables.order understand solvers capabilities propagate constraints placement overlapping fragment implemented three versions code, considereddifferent number constraints, precisely:1. implementation uses JM constraint (JM only)2. implementation adds alldistant constraint3. implementation adds alldistant centroid constraintscases use complete search (in particular, clustering tabling constraintslines 10 1214 Algorithm 2 disabled).Table 5, report execution times required FIASCO Gecode (withconsidered constraints) determine increasing number solutions, 1, 0001, 000, 000. solutions computed target protein 1ZDD length35. Table 5 shows execution time solvers increases proportionallynumber solutions found. However, FIASCO one order magnitude faster Gecodeunconstrained case, two orders magnitude faster cases. mainreason FIASCO specifically developed handle finite domains 3D pointvariables, approximated FD variables Gecode. Constraintsapproximations propagate poorly slowly. Moreover, approximation fragmentsusing finite domain variables introduces approximation errors, grow searchphase (and consequently, less solutions returned constrained cases). errorsmay result final structures relatively imprecise coordinates atomsconverted back real values.Table 6, consider small sequence four amino acids (SER TRP THR TRPthefirst four amino acids protein 1LE0), generate solutions. reportvalues best average RMSD among structures sets solutions computed using FIASCO Gecode implementation complete enumeration989fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliNumbersolutions1000100001000001000000JM0.0300.3123.00629.859FIASCOalldistant alldistant + centroid0.0510.0590.4760.6124.7946.04047.66961.385JM0.3582.57125.407252.815Gecodealldistant alldistant + centroid2.5313.80721.05635.370209.569347.8312186.833632.39Table 5: Comparison execution times FIASCO Gecode, increasing numbersolutions different sets considered constraints.domains. observe FIASCO significantly faster exploring search space,moreover, approximation introduces errors leads loss feasible solutions.JMalldistantalldistant + centroidN. sol.810000805322805322FIASCOTime (sec.) RMSD20.4930.16733.4930.16738.9530.167Avg. RMSD1.5701.5641.564N. sol810000774463169441GecodeTime (sec.) RMSD181.1020.190252.9740.190140.6440.580Avg. RMSD1.5961.5911.880Table 6: Number solutions, time, best RMSD, average RMSD set structuresfound FIASCO Gecode complete enumeration solution space usingdifferent constraintsencoded constraint satisfaction problem using new versionGecode allows employ float variables. labeled finite domain variablesallow select fragments, values point variables obtained constraintpropagation. Since constraint propagation float variables based interval arithmetics,turns amino acids intervals large able reconstructing protein evaluating energy value. instance, completeassignment variables related fragments protein 1ZDD, domainsfloat variables associated position first two amino acids singletons,related tenth amino acids intervals size one two A; even worse,domains atoms eleventh amino acids unbounded. stagelabeling float variables required computational time orders magnitude higherreported Table 6 finite domain Gecode implementation.Constraint solvers like ECLiPSe (Cheadle, Harvey, Sadler, Schimpf, Shen, & Wallace, 2003) Choco (Choco Team, 2008) also support mixed use integerreal variables. ECLiPSe Prolog-based language handles integer real variables together. However, great number matrix operations required application fit well Prolog implementation. Furthermore, current trendECLiPSe replace direct constraint solving translation FlatZinc.case Choco, current support Real Variables still development (c.f.http://choco.sourceforge.net/userguide.pdfpage 3). Things may changenext releases.also experimented another constraint solver, implementing multi-bodyconstraints using JaCoP library (JaCoP Team, 2012), similar way doneGecode. Eventually, tested protein used results reported Table 5,990fiA Constraint Solver Flexible Protein Modelsobserve substantial difference terms execution time,Gecode implementation.terms protein structure prediction, design FIASCO influencedprevious work TUPLES system (Dal Palu et al., 2011). TUPLES alsoconstraint solver protein structure prediction, based fragments assembly. Figure 16compares performance TUPLES FIASCO set proteins discussedSection 5.2. make comparison fair, make use energy functionsystems assume secondary structure elements known. Noteimportant differences two systems. TUPLES implemented usingconstraint logic programming techniques, specifically, SICStus Prolog (Swedish InstituteComputer Science, 2012); TUPLES make use floating point variables;hand, TUPLES introduces heuristic search mechanism based large neighboringsearch.results show quality predictions computed FIASCO (6.3 averageRMSD) better quality predictions computed TUPLES (9.4 averageRMSD). complete sampling search space allows us obtain better resultsproteins longer length benchmark ( 63). Instead, shorter proteins,obtain comparable results. similarity quality depends useenergy function systems. Notice energy function used designedsimpler model adopted TUPLES (C C ). Moreover, TUPLES based Prologimplementation provide floating point variables hence value mustrounded approximated. aspects explain quality differencesRMSD Best RMSD found FIASCO behaviorproteins (e.g., 1ZDD, 2GP8 ) (energy) RMSD values better FIASCO evencorresponding energy (RMSD) values higher TUPLES. execution timesFIASCO significantly faster TUPLES, spite FIASCOs lack sophisticatedsearch heuristic.also performed comparison state-of-the-art online Robetta predictor (Raman, Vernon, Thompson, Tyka, Sadreyev, Pei, Kim, Kellogg, DiMaio, Lange, Kinch, Sheffler, Kim, Das, Grishin, & Baker, 2009) first four proteins Table 6. builtdictionary 3 9 amino acid long peptides Robetta interface,disabled homology inference, order maintain fair comparison. results are:1ZDD computed 21s 5.92 RMSD, 2GP8 computed 16s 5.44 RMSD, 2K9Dcomputed 22s 4.65 RMSD, 1ENH computed 39s 2.74 RMSD. notedresults line Robetta predictor.Let us conclude section mentioning results reported previous section(where compared FIASCO TUPLES) provide also implicit comparison another off-the-shelf solver, SICStus Prolog constraint logic programming solver (SwedishInstitute Computer Science, 2012).6. Conclusionspaper, presented novel constraint (joined-multibody) model rigid bodiesconnected joints, constrained degrees freedom 3D space. presentedpolynomial time approximated filtering algorithm joined-multibody constraint,991fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliFigure 16: Comparison RMSD Execution Time TUPLES FIASCO992fiA Constraint Solver Flexible Protein Modelsexploits geometrical features rigid bodies. particular, filtering algorithmcombined search heuristics produce pool admissible solutionsuniformly sampled. allows direct control quality number solutions.filtering algorithm based 3D clustering procedure able copehigh variability rigid bodies, preserving computational cost. practicaladvantages joined-multibody constraint shown extensive set real proteinsimulations two main categories: protein loop reconstruction structure prediction(ab-initio). tests showed parameters constraint able controleffectively quality computational cost search. conclusion, constraintsolver FIASCO able model effectively various common protein case-studies analyses.future work, applications side, plan explore protein loop closureproblem, use specific databases scoring functions. close problemprotein flexibility, plan use FIASCO solver generate conformational spacelong scale movements nuclear receptors (Dal Palu et al., 2012b). Finally, plan useFIASCO general context protein structure prediction combination localsearch methods protein-ligand spatial constraints. constraint side, planintegrate JM filtering algorithm distance constraints, order generateaccurate clusters; plan integrate spatial constraints inferred boundsenergy terms (e.g., favorable contributions provided pairing secondary structureelements translate energy bounds distance constraints). plan investigateuse multiple JM constraints model super-secondary structures placement,useful capture important functional structural protein features. latterthought imposing several spatial path preferences given chain points. Finally,intend integrate constraint solver visual interface make easily availableBiologist practitioners porting parts tool within GPU-basedframework recently explored Campeotto, Dovier, Pontelli (2013).Acknowledgmentsthank Federico Fogolari comments several parts work. authorswould like express gratitude JAIR reviewers helped us sensibly improvepresentation.ReferencesAl-Bluwi, I., Simeon, T., & Cortes, J. (2012). Motion Planning Algorithms MolecularSimulations: Survey. Computer Science Review, 6 (4), 125143.Alberts, B., Johnson, A., Lewis, J., Raff, M., Roberts, K., & Walter, P. (2007). MolecularBiology Cell (5th Edition edition). Garland Science.Anfinsen, C. B. (1973). Principles Govern Folding Protein Chains. Science, 181,223230.Apt, K. (2009). Principles Constraint Programming. Cambridge University Press.Backofen, R., & Will, S. (2006). Constraint-Based Approach Fast Exact StructurePrediction 3-Dimensional Protein Models. Constraints, 11 (1), 530.993fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliBackofen, R., Will, S., & Bornberg-Bauer, E. (1999). Application Constraint Programming Techniques Structure Prediction Lattice Proteins Extended Alphabet.Bioinformatics, 15(3), 234242.Baker, D., & Sali, A. (2001). Protein Structure Prediction Structual Genomics. Science,294 (5540), 9396.Barahona, P., & Krippahl, L. (2008). Constraint Programming Structural Bioinformatics.Constraints, 13 (1-2), 320.Ben-David, M., Noivirt-Brik, O., Paz, A., Prilusky, J., Sussman, J. L., & Levy, Y. (2009).Assessment CASP8 Structure Predictions Template Free Targets. Proteins, 77,5065.Bennett, W., & Huber, R. (1984). Structural Functional Aspects Domain MotionsProteins. Crit. Rev. Biochem., 15, 291384.Borning, A. (1981). Programming Language Aspects ThingLab, ConstraintOriented Simulation Laboratory. ACM Transactions Programming LanguagesSystems, 3 (4), 353387.Cahill, S., Cahill, M., & Cahill, K. (2003). Kinematics Protein Folding. JournalComputational Chemistry, 24 (11), 13641370.Campeotto, F., Dovier, A., & Pontelli, E. (2013). Protein Structure Prediction GPU:Declarative Approach Multi-agent Framework. International ConferenceParallel Processing (ICPP), pp. 474479. IEEE Computer Society Press.Campeotto, F., Dal Palu, A., Dovier, A., Fioretto, F., & Pontelli, E. (2012). FilteringTechnique Fragment Assembly-Based Proteins Loop Modeling Constraints.Milano, M. (Ed.), CP, Vol. 7514 Lecture Notes Computer Science, pp. 850866.Springer.Canutescu, A., & Dunbrack, R. (2003). Cyclic coordinate descent: robotics algorithmprotein loop closure. Protein Sci, 12, 963972.Cheadle, A. M., Harvey, W., Sadler, A. J., Schimpf, J., Shen, K., & Wallace, M. G. (2003).ECLiPSe: Introduction. Technical report IC-Parc 031, IC-Parc, Imperial CollegeLondon.Chelvanayagam, G., Knecht, L., Jenny, T., Benner, S., & Gonnet, G. (1998). Combinatorial Distance-Constraint Approach Predicting Protein Tertiary ModelsKnown Secondary Structure. Folding Design, 3, 149160.Choco Team (2008). Choco: Open Source Java Constraint Programming Library.Workshop Open-Source Software Integer Constraint Programming. Available http://www.emn.fr/z-info/choco-solver/.Choi, Y., & Deane, C. M. (2010). FREAD Revisited: Accurate Loop Structure PredictionUsing Database Search Algorithm. Proteins, 78 (6), 143140.Clementi, C. (2008). Coarse-grained Models Protein Folding: Toy Models PredictiveTools?. Curr Opin Struct Biol, 18, 1015.994fiA Constraint Solver Flexible Protein ModelsCorblin, F., Trilling, L., & Fanchon, E. (2005). Constraint Logic Programming ModelingBiological System Described Logical Network. Workshop ConstraintBased Methods Bioinformatics.Cortes, J., & Al-Bluwi, I. (2012). Robotics Apporach Enhance Conformational Sampling Proteins. International Design Engineering Technical ConferencesComputers Information Engineering Conference, Vol. 4, pp. 11771186. ASME.Crescenzi, P., Goldman, D., Papadimitriou, C., Piccolboni, A., & Yannakakis, M. (1998).Complexity Protein Folding. Proceedings Thirtieth Annual ACMSymposium Theory Computing, pp. 597603. ACM Press.Dal Palu, A., Dovier, A., Fogolari, F., & Pontelli, E. (2012a). Protein Structure AnalysisConstraint Programming. Cozzini, P., & Kellogg, G. (Eds.), ComputationalApproaches Nuclear Receptors, chap. 3, pp. 4059. Royal Society Chemistry.Dal Palu, A., Spyrakis, F., & Cozzini, P. (2012b). New Approach Investigating ProteinFlexibility Based Constraint Logic Programming: First Application CaseEstrogen Receptor. European Journal Medicinal Chemistry, 49, 127140.Dal Palu, A., Dovier, A., & Fogolari, F. (2004). Constraint Logic Programming ApproachProtein Structure Prediction. BMC Bioinformatics, 5, 186.Dal Palu, A., Dovier, A., Fogolari, F., & Pontelli, E. (2010). CLP-based protein fragmentassembly. Theory Practice Logic Programming, 10 (4-6), 709724.Dal Palu, A., Dovier, A., Fogolari, F., & Pontelli, E. (2011). Exploring Protein FragmentAssembly Using CLP. Walsh, T. (Ed.), Proceedings International JointConference Artificial Intelligence, IJCAI, pp. 25902595. IJCAI/AAAI.Dal Palu, A., Dovier, A., & Pontelli, E. (2005). New Constraint Solver 3D LatticesApplication Protein Folding Problem. International Conference LogicProgramming Artificial Intelligence Reasoning, pp. 4863. Springer Verlag.Dal Palu, A., Dovier, A., & Pontelli, E. (2007). Constraint Solver Discrete Lattices,Parallelization, Application Protein Structure Prediction. Software PracticeExperience, 37 (13), 14051449.Dal Palu, A., Dovier, A., & Pontelli, E. (2010). Computing Approximate SolutionsProtein Structure Determination Problem using Global Constraints DiscreteCrystal Lattices. International Journal Data Mining Bioinformatics, 4 (1),120.Deane, C., & Blundell, T. (2001). CODA. Combined Algorithm Predicting Structurally Variable Regions Protein Models. Protein Sci, 10, 599612.Debartolo, J., Hocky, G., Wilde, M., Xu, J., Freed, K., & Sosnick, T. (2010). ProteinStructure Prediction Enhanced Evolutionary Diversity: SPEED. Protein Science,19 (3), 520534.Dotu, I., Cebrian, M., Van Hentenryck, P., & Clote, P. (2011). Lattice Protein StructurePrediction Revisited. IEEE/ACM Trans. Comput. Biology Bioinform, 8 (6), 16201632.995fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliDunbrack, R. (2002). Rotamer Libraries 21st Century. Curr. Opin. Struct. Biol.,12 (4), 431440.Erdem, E. (2011). Applications Answer Set Programming Phylogenetic Systematics.Logic Programming, Knowledge Representation, Nonmonotonic Reasoning, pp.415431. Springer Verlag.Erdem, E., & Ture, F. (2008). Efficient Haplotype Inference Answer Set Programming.National Conference Artificial Intelligence (AAAI), pp. 436441. AAAI/MITPress.Felts, A., Gallicchio, E., Chekmarev, D., Paris, K., Friesner, R., & Levy, R. (2008). Prediction Protein Loop Conformations using AGBNP Implicit Solvent ModelTorsion Angle Sampling. J Chem Theory Comput, 4, 855868.Fiser, A., Do, R., & Sali, A. (2000). Modeling Loops Protein Structures. Protein Sci,9, 17531773.Fogolari, F., Esposito, G., Viglino, P., & Cattarinussi, S. (1996). Modeling PolypeptideChains C Chains, C Chains C , C Chains Ellipsoidal LateralChains. Biophysical Journal, 70, 11831197.Fogolari, F., Pieri, L., Dovier, A., Bortolussi, L., Giugliarelli, G., Corazza, A., Esposito, G.,& Viglino, P. (2007). Scoring Predictive Models using Reduced RepresentationProteins: Model Energy Definition. BMC Structural Biology, 7 (15), 117.Fogolari, F., Corazza, A., Viglino, P., & Esposito, G. (2012). Fast Structure SimilaritySearches among Protein Models: Efficient Clustering Protein Fragments. AlgorithmsMolecular Biology, 7, 16.Fujitsuka, Y., Chikenji, G., & Takada, S. (2006). SimFold Energy Function De NovoProtein Structure Prediction: Consensus Rosetta. Proteins, 62, 381398.Gay, S., Fages, F., Martinez, T., & Soliman, S. (2011). Constraint Program SubgraphEpimorphisms Application Identifying Model Reductions Systems Biology.Workshop Constraint-Based Methods Bioinformatics.Gebser, M., Schaub, T., Thiele, S., & Veber, P. (2011). Detecting Inconsistencies LargeBiological Networks Answer Set Programming. Theory Practice LogicProgramming, 11 (2-3), 323360.Gecode Team (2013). Gecode: Generic Constraint Development Environment. Availablehttp://www.gecode.org.Go, N., & Scheraga, H. (1970). Ring Closure Local Conformational DeformationsChain Molecules. Macromolecules, 3, 178187.Graca, A., Marques-Silva, J., Lynce, I., & Oliveira, A. (2011). Haplotype InferencePseudo-Boolean Optimization. Annals OR, 184 (1), 137162.Guns, T., Sun, H., Marchal, K., & Nijssen, S. (2010). Cis-regulatory Module Detection UsingConstraint Programming. IEEE International Conference BioinformaticsBiomedicine (BIBM), pp. 363368.996fiA Constraint Solver Flexible Protein ModelsHandl, J., Knowles, J., Vernon, R., Baker, D., & Lovell, S. (2012). Dual RoleFragments Fragment-Assembly Methods De Novo Protein Structure Prediction.Proteins: Structure, Function Bioinformatics, 80 (2), 490504.Hartenberg, R., & Denavit, J. (1995). Kinematic Notation Lower Pair MechanismsBased Matrices. Journal Applied Mechanics, 77, 215221.Hegler, J., Latzer, J., Shehu, A., Clementi, C., & Wolynes, P. (2009). Restriction VersusGuidance Protein Structure Prediction. Proc Natl Acad Sci U.S.A., 106 (36), 1530215307.Jacobson, M., Pincus, D., Rapp, C., Day, T., Honig, B., Shaw, D., & Friesner, R. (2004).Hierarchical Approach All-atom Protein Loop Prediction. Proteins, 55, 351367.JaCoP Team (2012). JaCoP web page, visited November 2012..http://www.jacop.eu.AvailableJamroz, M., & Kolinski, A. (2010). Modeling Loops Proteins: Multi-method Approach. BMC Struct. Biol., 10 (5).Jauch, R., Yeo, H., Kolatkar, P. R., & Clarke, N. D. (2007). Assessment CASP7 StructurePredictions Template Free Targets. Proteins, 69, 5767.Jones, D. (2006). Predicting Novel Protein Folds using FRAGFOLD. Proteins, 45,127132.Karplus, K., Karchin, R., Draper, J., Casper, J., Mandel-Gutfreund, Y., Diekhans, M.,& Source., R. H. (2003). Combining local structure, fold-recognition, new foldmethods protein structure prediction. Proteins, 53 (6), 491497.Karplus, M., & Shakhnovich, E. (1992). Protein Folding: Theoretical Studies Thermodynamics Dynamics. Protein Folding, pp. 127195. WH Freeman.Kim, D. E., Blum, B., Bradley, P., & Baker, D. (2009). Sampling Bottlenecks De novoProtein Structure Prediction. Journal Molecular Biology, 393 (1), 249 260.Kinch, L., Yong Shi, S., Cong, Q., Cheng, H., Liao, Y., & Grishin, N. V. (2011). CASP9assessment free modeling target predictions. Proteins, 79, 5973.Kirillova, S., Cortes, J., Stefaniu, A., & Simeon, T. (2008). NMA-Guided Path Planning Approach Computing Large-Amplitude Conformational Changes Proteins.Proteins: Structure, Function, Bioinformatics, 70 (1), 131143.Kolodny, R., Guibas, L., Levitt, M., & Koehl, P. (2005). Inverse Kinematics Biology:Protein Loop Closure Problem. International Journal Robotics Research,24 (2-3), 151163.Krippahl, L., & Barahona, P. (2002). Psico: Solving Protein Structures ConstraintProgramming Optimization. Constraints, 7 (4-3), 317331.Krippahl, L., & Barahona, P. (2005). Applying Constraint Programming Rigid BodyProtein Docking. Principles Practice Constraint Programming, pp. 373387. Springer Verlag.Krippahl, L., & Barahona, P. (1999). Applying Constraint Programming Protein Structure Determination. Principles Practice Constraint Programming, pp. 289302. Springer Verlag.997fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliLarhlimi, A., & Bockmayr, A. (2009). New Constraint-Based Description SteadyState Flux Cone Metabolic Networks. Discrete Applied Mathematics, 157 (10),22572266.LaValle, S. (2006). Planning Algorithms. Cambridge University Press.Lazaridis, T., Archontis, G., & Karplus, M. (1995). Enthalpic Contribution ProteinStability: Atom-Based Calculations Statistical Mechanics. Adv. Protein Chem.,47, 231306.Lee, J., Kim, S., Joo, K., Kim, I., & Lee, J. (2004). Prediction Protein Tertiary Structureusing Profesy, Novel Method Based Fragment Assembly ConformationalSpace Annealing. Proteins, 56 (4), 704714.Lee, J., Lee, D., Park, H., Coutsias, E., & Seok, C. (2010). Protein Loop Modeling UsingFragment Assembly Analytical Loop Closure. Proteins, 78 (16), 34283436.Liu, P., Zhu, F., Rassokhin, D., & Agrafiotis, D. (2009). Self-organizing AlgorithmModeling Protein Loops. PLOS Comput Biol, 5 (8).Lovell, S., Davis, I., Arendall, W., de Bakker, P., Word, J., Prisant, M., Richardson, J., &Richardson, D. (2003). Structure Validation C Geometry: , C Deviation.Proteins, 50, 437450.Mann, M., & Dal Palu, A. (2010). Lattice Model Refinement Protein Structures.Workshop Constraint-Based Methods Bioinformatics.Micheletti, C., Seno, F., & Maritan, A. (2000). Recurrent oligomers proteins: optimal scheme reconciling accurate concise backbone representations automatedfolding design studies. proteins, 40 (4), 662674.Moll, M., Schwarz, D., & Kavraki, L. (2007). Roadmap Methods Protein Folding. Humana Press.Molloy, K., Saleh, S., & Shehu, A. (2013). Probabilistic Search Energy GuidanceBiased Decoy Sampling Ab-Initio Protein Structure Prediction. IEEE/ACM Trans.Comput. Biology Bioinform, PrePrint.Neumaier, A. (1997). Molecular Modeling Proteins Mathematical PredictionProtein Structure. SIAM Review, 39, 407460.Noonan, K., OBrien, D., & Snoeyink, J. (2005). Protein Backbone Motion InverseKinematics. International Journal Robotics Research, 24 (11), 971982.Olson, B. S., Molloy, K., & Shehu, A. (2011). Search Protein Native StateProbabilistic Sampling Approach. J. Bioinformatics Computational Biology,9 (3), 383398.Raman, S., Vernon, R., Thompson, J., Tyka, M., Sadreyev, R., Pei, J., Kim, D., Kellogg, E.,DiMaio, F., Lange, O., Kinch, L., Sheffler, W., Kim, B.-H., Das, R., Grishin, N. V.,& Baker, D. (2009). Structure Prediction CASP8 All-atom Refinement usingRosetta. Proteins, 77 (Suppl. 9), 8999.Rapp, C. S., & Friesner, R. A. (1999). Prediction Loop Geometries using GeneralizedBorn Model Solvation Effects. Proteins, 35, 173183.998fiA Constraint Solver Flexible Protein ModelsRay, O., Soh, T., & Inoue, K. (2010). Analyzing Pathways Using ASP-Based Approaches.Algebraic Numeric Biology, 4th International Conference, pp. 167183. SpringerVerlag.Rossi, F., van Beek, P., & Walsh, T. (2006). Handbook Constraint Programming. ElsevierScience Inc.Rufino, S., Donate, L., Canard, L., & Blundell, T. (1997). Predicting ConformationalClass Short Medium Size Loops Connecting Regular Secondary Structures:Application Comparative Modeling. J. Mol. Biol., 267, 352367.Shehu, A. (2009). Ab-Initio Tree-Based Exploration Enhance Sampling Low-EnergyProtein Conformations. Proceedings Robotics: Science Systems V.Shehu, A. (2010). Conformational Search Protein Native State, pp. 431452. JohnWiley & Sons. Inc.Shehu, A., & Kavraki, L. (2012). Modeling Structures Motions Loops ProteinMolecules. Entropy, 14, 252290.Shen, M., & Sali, A. (2006). Statistical Potential Assessment Prediction ProteinStructures. Protein Sci, 15, 25072524.Shih, E., & Hwang, M.-J. (2011). Use Distance Constraints Protein-ProteinDocking Computations. Proteins: Structure, Function, Bioinformatics, 80 (1),194205.Shmygelska, A., & Hoos, H. (2005). Ant Colony Optimisation Algorithm 2D3D Hydrophobic Polar Protein Folding Problem. BMC Bioinformatics, 6, 3052.Shmygelska, A., & Levitt, M. (2009). Generalized Ensemble Methods De Novo StructurePrediction. Proceedings National Academy Science (USA), 106 (5), 14151420.Simoncini, D., Berenger, F., Shrestha, R., & Zhang, K. (2012). Probabilistic FragmentBased Protein Structure Prediction Algorithm. PLOS One, 7 (7), e38799.Simons, K., Kooperberg, C., Huang, E., & Baker, D. (1997). Assembly Protein TertiaryStructures Fragments Similar Local Sequences using Simulated AnnealingBayesian Scoring Functions. J. Mol. Biol., 268, 209225.Skolnick, J., Fetrow, J., & Kolinski, A. (2000). Structural Genomics ImportanceGene Function Analysis. Nat. Biotechnology, 18 (3), 283287.Soto, C., Fasnacht, M., Zhu, J., Forrest, L., & Honig, B. (2008). Loop Modeling: Sampling,Filtering, Scoring. Proteins: Structure, Function, Bioinformatics, 70, 834843.Spassov, V., Flook, P., & Yan, L. (2008). LOOPER: Molecular Mechanics-based Algorithm Protein Loop Prediction. Protein Eng, 21, 91100.Stuckey, P. J., Becket, R., & Fischer, J. (2010). Philosophy MiniZinc Challenge.Constraints, 15 (3), 307316.Sussmann, G., & Steele, G. (1980). CONSTRAINTS: Language Expressing AlmostHierarchical Descriptions. Artificial Intelligence, 14 (1), 139.999fiCampeotto, Dal Palu, Dovier, Fioretto, & PontelliSutherland, I. (1963). Sketchpad: Man-Machine Graphical Communication System. Tech.rep. 296, Lincoln Laboratory, MIT.Swedish Institute Computer Science (2012). SICStus Prolog Home Page. http://www.sics.se/sicstus/.Thebault, P., de Givry, S., Schiex, T., & Gaspin, C. (2005). Combining Constraint Processing Pattern Matching Describe Locate Structured Motifs GenomicSequences. Fifth Workshop Modeling Solving Problems Constraints,pp. 5360.Tsai, Y., Huang, Y., Yu, C., & Lu, C. (2004). MuSiC: Tool Multiple SequenceAlignment Constraints. Bioinformatics, 20 (14), 23092311.Xiang, Z., Soto, C., & Honig, B. (2002). Evaluating Conformal Energies: Colony EnergyApplication Problem Loop Prediction. PNAS, 99, 74327437.Xu, D., & Zhang, Y. (2012). Ab Initio Protein Structure Assembly Using ContinuousStructure Fragments Optimized Knowledge-based Force Field. Proteins, 80 (7),17151735.Yang, R. (1998). Multiple Protein/DNA Sequence Alignment Constraints. International Conference Practical Applications Constraint Programming.Yap, R. (2001). Parametric Sequence Alignment Constraints. Constraints, 6, 157172.Yap, R., & Chuan, H. (1993). Constraint Logic Programming Framework ConstructingDNA Restriction Maps. Artificial Intelligence Medicine, 5 (5), 447464.Yue, K., & Dill, K. (2000). Constraint Based Assembly Tertiary Protein StructuresSecondary Structure Elements. Proteins Science, 9 (10), 19351946.Zhang, M., & Kavraki, L. (2002). New Method Fast Accurate DerivationMolecular Conformations. Journal Chemical Information Computer Sciences,42 (1), 6470.Zhang, Y., & Hauser, K. (2013). Unbiased, Scalable Sampling Protein Loop Conformations Probabilistic Priors. BMC Structural Biology, (to appear http:// www. indiana. edu/ ~ motion/ slikmc/ papers/ BMC_ Zhang. pdf ).Zhou, H., & Zhou, Y. (2002). Distance-scaled, Finite Ideal-gas Reference State ImprovesStructure-derived Potentials Mean Force Structure Selection Stability Prediction. Protein Sci, 11, 27142726.1000fiJournal Artificial Intelligence Research 48 (2013) 841-883Submitted 06/13; published 11/13Scalable Efficient Bayes-Adaptive ReinforcementLearning Based Monte-Carlo Tree SearchArthur Guezaguez@gatsby.ucl.ac.ukGatsby Computational Neuroscience UnitUniversity College LondonLondon, WC1N 3AR, UKDavid Silverd.silver@cs.ucl.ac.ukDept. Computer ScienceUniversity College LondonLondon, WC1E 6BT, UKPeter Dayandayan@gatsby.ucl.ac.ukGatsby Computational Neuroscience UnitUniversity College LondonLondon, WC1N 3AR, UKAbstractBayesian planning formally elegant approach learning optimal behaviourmodel uncertainty, trading exploration exploitation ideal way. Unfortunately,planning optimally face uncertainty notoriously taxing, since search spaceenormous. paper introduce tractable, sample-based method approximateBayes-optimal planning exploits Monte-Carlo tree search. approach avoids expensive applications Bayes rule within search tree sampling models currentbeliefs, furthermore performs sampling lazy manner. enables outperform previous Bayesian model-based reinforcement learning algorithms significantmargin several well-known benchmark problems. show, approach evenwork problems infinite state space lie qualitatively reach almostprevious work Bayesian exploration.1. Introductionkey challenge sequential decision-making understand agents learncollect rewards avoid costs interactions world. natural waycharacterize interactions Markov Decision Process (mdp). mdps consistset states, set possible actions, transition model stochastically decidessuccessor state given state action. addition, cost reward associatedstate action. problem learning arises aspectstransition model unknown agent, implying uncertainty best strategygathering rewards avoiding costs. Exploration therefore necessary reduceuncertainty ensure appropriate exploitation environment. Weighing benefitsexploring, identify potentially better actions, benefits exploiting knownsources rewards generally referred exploration-exploitation trade-off.trade-off formalized various different ways. One possible objectivecontrol number suboptimal actions agent ever performs; algorithms high2013 AI Access Foundation. rights reserved.fiGuez, Silver, & Dayanprobability bound number suboptimal steps polynomial numberstates actions said pac-mdp (Strehl, Li, & Littman, 2009). Insteadfocusing suboptimal actions, another objective minimize so-called regret,expected loss relative optimal policy mdp (Jaksch, Ortner, & Auer,2010). Lastly, Bayesian decision theory prescribes maximizing expected discountedsum rewards light prior distribution transition models; one wayachieve solving augmented mdp, called Bayes-Adaptive mdp (bamdp),corresponding augmented dynamics known (Martin, 1967; Duff, 2002).augmentation posterior belief distribution dynamics, given data farobserved. agent starts belief state corresponding prior and, executinggreedy policy bamdp whilst updating posterior, acts optimally (with respectbeliefs) original mdp. Bayes-optimal policy optimal policybamdp; integrates exploration exploitation ideal manner.general, different objectives compatible see example workKolter Ng (2009) incompatibility pac-mdp Bayes-optimal solution. pac-mdp regret frameworks gained considerable traction recent years,Bayesian exploration comparatively ignored. However, Bayesian framework attractive structured prior knowledge incorporated solutionprincipled manner, providing means tackle, least theory, large complexunknown environments. Methods tailored objectives pac-mdp regret minimization cannot far easily adapted exploit priors. assumptionenvironment, thus forced explore every state action least once,hopeless large environments.Unfortunately, exact Bayesian reinforcement learning (RL) computationally intractable.Various algorithms devised approximate optimal learning, often ratherlarge cost. computational barrier restricted Bayesian RL small domainssimple priors. paper, present tractable approach exploits extendsrecent advances Monte-Carlo tree search (mcts) (Kocsis & Szepesvari, 2006), notablypartially-observable mdps (Silver & Veness, 2010) bamdp seenspecial case. mcts capable tackling large mdp problems dynamicsknown (Gelly, Kocsis, Schoenauer, Sebag, Silver, Szepesvari, & Teytaud, 2012), shownaive application mcts bamdp tractable general proposeset principled modifications obtain practical algorithm, called bamcpBayes-Adaptive Monte-Carlo Planner.iteration bamcp, pomcp algorithm (Silver & Veness, 2010), singlemdp sampled agents current beliefs. mdp used simulate singleepisode whose outcome used update value node search tree traversedsimulation. integrating many simulations, therefore many samplemdps, optimal value future sequence obtained respect agentsbeliefs. prove process converges Bayes-optimal policy, given infinitesamples. Since many priors appropriate Bayesian RL setting requireform approximate inference, extend convergence proof show bamcpalso converges combined Markov Chain Monte Carlo-based inference scheme.algorithm efficient previous sparse sampling methods Bayes-adaptiveplanning (Wang, Lizotte, Bowling, & Schuurmans, 2005; Castro, 2007; Asmuth & Littman,842fiBayes-Adaptive Monte-Carlo Planning2011), partly update posterior belief state coursesimulation. thus avoids repeated applications Bayes rule, expensivesimplest priors mdp. increase computational efficiency further,introduce additional innovation: lazy sampling scheme samples posteriordistribution states traversed simulation.applied bamcp representative sample benchmark problems competitive algorithms literature. consistently significantly outperformed existingBayesian RL methods, also recent non-Bayesian approaches, thus achieving state-ofthe-art performance.Further, bamcp particularly well suited support planning large domainsrichly structured prior knowledge makes lazy sampling possible effective.offers prospect applying Bayesian RL realistically complex scale. illustratepossibility showing bamcp tackle domain infinite numberstates structured prior dynamics, challenging, radically intractable,task existing approaches. example exploits bamcps ability use Markov chainMonte Carlo methods inference associated posterior distribution models.paper organized follows. First, formally define Bayesian model-basedRL problem review existing methods; present new algorithm contextprevious suggestions; finally report empirical results existing domainsnew, infinite, task. results appeared short conference versionpaper (Guez, Silver, & Dayan, 2012).2. Model-Based Reinforcement Learningfirst briefly review model-based reinforcement learning search algorithmscase model known. introduce formalism Bayesian model-basedRL provide survey existing approximation algorithms motivate approach.2.1 Model-Based Reinforcement Learning Known Modelmdp described 5-tuple = hS, A, P, R, i, discrete set states,finite set actions, P : R state transition probability kernel,R : R bounded reward function, discount factor (Szepesvari,2010). deterministic stationary mdp policy defined mapping :states actions. value function policy state expected return,defined as:"#XV (s) Ert |s0 = ,(1)t=0rt random reward obtained time following policy stateE denotes expectation operator averages possible paths policyimplies. related quantity action-value function policy executing843fiGuez, Silver, & Dayanparticular action state executing :"#XX0t10Q (s, a) R(s, a) +P(s, a, )EMrt |s1 =s0= R(s, a) +X(2)t=1P(s, a, s0 )V (s0 ),(3)s0implying relation V (s) = Q (s, (s)). optimal action-value function, denoted Q ,provides maximum expected return Q (s, a) obtained executing actionstate s. optimal value function, V , similarly defined related QV (s) = maxaA Q (s, a), S. optimal policy achieves maximum expectedreturn states, obtained Q as: (s) = argmaxaA Q (s, a),breaking ties arbitrarily.components mdp tuple known including model Pstandard mdp planning algorithms used estimate optimal policy off-line,Value Iteration Policy Iteration (Bellman, 1954; Ross, 1983; Sutton & Barto,1998; Szepesvari, 2010). However, always practical find optimal policystates large mdps one fell swoop. Instead, methods concentratesearching online best action current state st . particularly commonmodel-based Bayesian RL algorithms. therefore introduce relevant existing onlinesearch methods mdps used building blocks Bayesian RL algorithms.2.1.1 Online SearchOnline search methods evaluate tree possible future sequences. tree rootedcurrent state composed state action nodes. state node, includingroot, children actions legal state. turn, actionnode children successor states resulting action. goalforward search algorithm recursively estimate value state action nodetree. Ultimately, value possible action root used selectnext action, process repeats using new state root.Online search methods may categorised firstly backup methodvalue node updated, secondly order nodes treetraversed backups applied.2.1.2 Full-Width SearchClassical online search methods based full-width backups, consider legalactions possible successor states, example using Bellman backup,V (s) max R(s, a) +aAXP(s, a, s0 )V (s0 )(4)s0Search efficiency largely determined order nodes traversed.One example best-first, current best usually determined accordingoptimistic criterion. leads algorithm resembling (Hart, Nilsson, & Raphael,844fiBayes-Adaptive Monte-Carlo Planning1968), applies deterministic case. search tree may also truncated,using knowledge extreme reward discount factor ensureprovably benign (Davies, Ng, & Moore, 1998). one prepared give guaranteesoptimality, approximate value function (typically described online search literatureheuristic function evaluation function) applied leaf nodes substitutevalue truncated subtree.2.1.3 Sample-Based SearchRather expanding every tree node completely, sample-based search methods overcomecurse dimensionality sampling successor states transition distribution.generic advantage full-width search expend little effortunlikely paths tree.Sparse SamplingSparse Sampling (Kearns, Mansour, & Ng, 1999) sample-based online search algorithm.key idea sample C successor nodes action node, apply Bellmanbackup sampled transitions, update value parent state nodevalues child nodes:CXVd (s) = max R(s, a) +Vd+1 (child(s, a, i)).aAC(5)i=1search tree traversed depth-first manner, approximate value functionemployed truncated leaf nodes, pre-defined depth D. Sparse Samplingconverges near-optimal policy given appropriate choice parameters C D.FSSSAlthough Sparse Sampling concentrates likely transitions, focus searchnodes relatively high values returns. work Walsh, Goschin,Littman (2010), Forward Search Sparse Sampling (fsss) extends regular Sparse Samplingmaintaining lower upper bounds value node:Ld (s, a) = R(s, a) +Ud (s, a) = R(s, a) +CCXLd+1 (s0 )Count(s, a, s0 ),(6)Ud+1 (s0 )Count(s, a, s0 ),(7)s0 Child(s,a)Xs0 Child(s,a)Ld (s) = max Ld (s, a),(8)Ud (s) = max Ud (s, a),(9)aAaAChild(s, a) set successor states sampled C draws P(s, a, ),Count(s, a, s0 ) number times set element sampled. Whenever nodecreated, lower upper bounds initialized according Ld (s, a) = Vmin845fiGuez, Silver, & DayanUd (s, a) = Vmax , i.e., worst best possible returns. tree traversed bestfirst manner according value bounds, starting root simulationtree. state node, promising action selected maximisingupper bound value. action (or chance) node, successor states selectedsampled set C candidates maximising uncertainty (upper minus lowerbound). effectively prunes branches tree low upper boundsexhaustively explored, still maintaining theoretical guarantees SparseSampling.Monte-Carlo Tree SearchDespite theoretical guarantees, practice, sparse sampling fsss sufferfact truncate search tree particular depth, experience biasassociated approximate value function use leaves. Monte-Carlo TreeSearch (mcts) provides way reducing bias evaluating leaves exactly usingmodel, employing sub-optimal, rollout policy. formally, mcts, statesevaluated averaging many simulations. simulation starts roottraverses current tree leaf reached, using tree policy (e.g., greedy actionselection) based information far gathered nodes tree.results (locally) best-first tree traversal, step tree policy selectsbest child (best according exploration criterion) given current values tree.Rather truncating search relying potentially biased value function leafnodes, different policy, called rollout policy (e.g., uniform random) employedleaf node termination search horizon. node traversed simulationupdated Monte-Carlo backup, simply evaluates node meanoutcome simulations passed node. Specifically, Monte-Carlobackups update value action node follows:Qd (s, a) Qd (s, a) + (R Qd (s, a))/Nd (s, a),(10)R sampled discounted return obtained traversed action node s,depth Nd (s, a) visitation count action node s, (i.e., update computesmean sampled returns obtained action node simulations).particular tree policy mcts received much attention, indeed underliesalgorithm, uct (Upper Confidence bounds applied Trees) policy (Kocsis &Szepesvari, 2006). uct employs ucb (Upper Confidence Bounds) algorithm (Auer,Cesa-Bianchi, & Fischer, 2002), designed multi-armed bandit problems, select adaptively actions every state node according to:pargmax Qd (s, a) + c log(Nd (s))/Nd (s, a),(11)aAc exploration constant needs set appropriately Nd (s)visitation count state node s. tree policy treats forward search metaexploration problem, preferring exploit regions tree currently appear betterothers, continuing explore unknown less known parts tree. leadsgood empirical results even small numbers simulations, effort expended846fiBayes-Adaptive Monte-Carlo Planningsearch seems fruitful. Nevertheless parts tree eventually visited infinitelyoften, therefore algorithm shown converge optimal policylong run.Despite negative theoretical results showing uct slow find optimalpolicies carefully designed counterexample mdps (Coquelin & Munos, 2007), uctsuccessful many large mdp domains (Gelly et al., 2012).2.2 Model-Based Bayesian Reinforcement Learningmethods far discussed depend agent model world, i.e.,dynamics P. key concern Bayesian RL acting model fullyknown. first describe generic Bayesian formulation optimal decision-makingunknown mdp, following Martin (1967) Duff (2002), consider approximationsinspired intractability full problem.2.2.1 FormalismGiven dynamics P P (coming set possible models)incompletely known, Bayesian RL treats latent random variable followsprior distribution P (P). Observations dynamics contained history ht (attime t) actions states: ht s1 a1 s2 a2 . . . at1 st , duly lead posterior distributionP via likelihood.history ht influences posterior distribution. Thus policies integrateexploration exploitation (called EE policies) Bayesian RL problem genericallytake history account, along current state, order specifyaction take. is, whereas P known, policy defined mapping: [0, 1] current state actions probability (of execution),Bayesian RL, EE policies defined mappings history, current state, actionprobability : H [0, 1], H set possible histories.1 denoteset EE policies.objective EE policy Bayesian formulation maximizeexpected return (sum discounted rewards), expectation taken distribution environments P (P) = P (P |), addition taking usual expectationstochasticity return induced dynamics. Formally, define expecteddiscounted return v starting state seeing history h following EEpolicy as:"#Xv(s, h, ) = Ert |s0 = s, h0 = h(12)t=0Z=P="dP P (P |h) EM (P)Xa0X#rt |s0 = s, h0 = h(13)t=0R(s, a0 ) +X(s, h, a0 )v(s0 , ha0 s0 , )P(s, a0 , s0 , h),(14)s01. redundancy state-history notation throughout paper, namely current state couldextracted history, present ensure clarity exposition.847fiGuez, Silver, & DayanRP(s, a, s0 , h) P dP P (P |h) P(s, a, s0 ) denotes probability transitioningstate s0 executing distribution dynamics P (P |h), (P) denotesmdp associated dynamics P.Definition 1 Given S, A, R, , prior distribution P (P) dynamicsmdp , letv (s, ) = sup v(s, , ).(15)Martin (1967, Thm. 3.2.1) shows exists strategy achievesexpected return (i.e., v(s, , ) = v (s, )). EE strategy called Bayesoptimal policy.2formulation prescribes natural recipe computing Bayes-optimal policy.observing history ht mdp, posterior belief P updated using Bayesrule P (P|ht ) P (ht |P)P (P) (or recursive form P (P|ht ) P(st1 , at1 , st )P (P|ht1 )).Thus, uncertainty dynamics model transformed certaintycurrent state inside augmented state space + = H, statespace original problem H set possible histories. dynamics associatedaugmented state space describedZP + (hs, hi, a, hs0 , h0 i) = 1[h0 = has0 ]P(s, a, s0 )P (P|h) dP,(16)Preward function simply projected reward function original mdp:R+ (hs, hi, a) = R(s, a).(17)Together, 5-tuple + = hS + , A, P + , R+ , forms Bayes-Adaptive mdp (bamdp)mdp problem . Since dynamics bamdp known, principlesolved obtain optimal value function associated action:"#X 0Q (hst , ht i, a) = max EM +rt0 |at =(18)t0 =toptimal action state readily derived. Optimal actionsbamdp executed greedily real mdp constitute best course actionBayesian agent respect prior belief P:Proposition 1 (Silver, 1963; Martin, 1967) optimal policy bamdpBayes-optimal policy, defined Definition 1.obvious expected performance bamdp policy mdpbounded optimal policy obtained fully-observable model,2. proof Martin (1967) covers finite state spaces, extended specific kindsinfinite state spaces one consider Section 4.2, see Appendix details.848fiBayes-Adaptive Monte-Carlo Planningequality occurring, example, degenerate case prior supporttrue model.Bayes-optimal policy stationary function augmented state, evolvestime observed context original mdp function stateonly. Since uncertainty dynamics taken account optimizationreturn, Bayes-optimal policy integrates exploration exploitation optimally.useful observe bamdp particular form Partially Observablemdp (pomdp). state space pomdp P, P set possiblemodels P. second component state space static hidden, partiallyobserved experienced transitions. Planning conducted belief space,equivalently space sufficient statistics belief distribution, allowing decisionstaken light likely outcomes gathering exploitable informationhidden state. case bamdp, actions gather information hiddenmodel P. However, pomdp discrete pomdp since state space continuous(with discrete observations). Therefore, pointed Duff (2002), many classicalsolutions pomdps cannot directly applied bamdp.practical perspective, solving bamdp exactly computationally intractable,even small state spaces. First, augmented state space contains possible historiestherefore infinite. Second, transitions bamdp, described Equation 16,require integration transition models posterior. Although operationtrivial simple probabilistic models (e.g., independent Dirichlet-Multinomial),intractable priors interest (see Section 4.2 example). However, certainspecial cases bamdp known somewhat tractable. example,celebrated Gittins indices provide shortcut solution bandit problems (Gittins, Weber,& Glazebrook, 1989), although calculating indices remains challenge general.Further, optimal solution least finite-horizon linear-Gaussian control problems computed exactly (Tonk & Kappen, 2010). Nevertheless, appears unlikelyexists tractable exact algorithm solve general bamdps, justifying searchsound efficient approximations.2.2.2 Approximate Bayes-Adaptive AlgorithmsThree coarse classes approximation methods developed, review.Note analogues solution methods pomdps.First offline methods toil mightily provide execution policies usedobserved augmented state. Second third two sets online methodsconcentrate current augmented state. One set methods uses sparse samplingfull tree future states actions associated bamdp, startingcurrent augmented state. samples solves one mdps currentposterior P, possibly correcting bias towards exploitation typicallyleads.describing classes, highlight currently lack, establishbasis new algorithm, bamcp.849fiGuez, Silver, & DayanOffline MethodsOne idea solve entire bamdp offline, every state belief (or history).obviates need anything simple value/policy lookup execution.However, avenue approximation led much practical success presumablydifficulties associated size bamdp, including factgargantuan amounts computation may performed find good policies partsspace histories actually sampled practice.Existing approaches class include actor-critic algorithm (Duff, 2003),learning, point-based value iteration algorithm, called beetle (Bayesian Exploration Exploitation Tradeoff LEarning) (Poupart, Vlassis, Hoey, & Regan, 2006).beetle builds approximate policy off-line exploiting facets structurevalue functions bamdps, inherit broader, parent, class pomdps.recently, Wang, Won, Hsu, Lee (2012) propose solve offline pomdp representing latent dynamics discrete partially-observed state component,value state component corresponds one K possible models sampledprior. approach fail true model well-represented K sampledmodels.Offline methods particularly poorly suited problems infinite state taskconsider section 4.2.Online Methods: Sparse SamplingOnline methods reduce dependency size bamdp approximatingbamdp solution around current (augmented) state agent running planningalgorithm step.One idea perform forms forward search current state. Althoughmethods concentrate current state, search tree still large expensiveevaluate given path tree. partial alleviation problem, approachesrely form sparse, non-uniform, tree exploration minimize search effort(but see also Fonteneau, Busoniu, & Munos, 2013). Section 2.1.3 described searchalgorithms mdps, present existing extensions bamdp setting. Analogousmethods pomdps reviewed Ross, Pineau, Paquet, Chaib-Draa (2008).Wang et al. applied Sparse Sampling search online bamdps (Wang et al., 2005),expanding tree non-uniformly according sampled trajectories. state node,promising action selected via Thompson sampling (Thompson, 1933; Agrawal & Goyal,2011) i.e., sampling mdp belief-state, solving mdp taking optimalaction. Sparse Sampling, fails exploit information values nodesprioritizing sampling process. chance (action) node, successor belief-statesampled transition dynamics bamdp.Castro et al. applied Sparse Sampling define relevant region bamdpcurrent decision step. leads optimization problem solved using LinearProgramming (Castro & Precup, 2007).Asmuth Littmans bfs3 algorithm (Asmuth & Littman, 2011) adapts ForwardSearch Sparse Sampling (Walsh et al., 2010) bamdp (treated particular mdp).Although bfs3 described Monte-Carlo tree search, fact uses Bellman backup850fiBayes-Adaptive Monte-Carlo Planningrather Monte-Carlo evaluation. fsss, Bellman backup updates lowerupper bounds value node.Online Methods: Dual OptimismInstead applying sparse sampling methods tree future states actions,alternative collection methods derives one simpler mdps posteriorcurrent augmented state, whose solution often computationally straightforward. itself,leads over-exploitation: corrections thus necessary generate sufficient exploration. Exploration seen coming optimism face uncertainty actionsyet tried sufficiently must look attractive current mean.Indeed, various heuristic forms exploration bonus (Sutton, 1990; Schmidhuber,1991; Dayan & Sejnowski, 1996; Kearns et al., 1999; Meuleau & Bourgine, 1999; Brafman& Tennenholtz, 2003) generalize optimism inherent optimal solutionsGittins indices.One approximation first derived work Cozzolino, Gonzalez-Zubieta,Miller (1965), mean estimate transition probabilities (i.e., meanposterior) employed certainty equivalence approximation. Solving corresponding mean mdp induces form optimism, always sufficient driveexploration. idea revisited linked reinforcement learning formulationsDayan Sejnowski (1996).Another way induce optimism exploit variance posterior samplingmdps augmented state. One approaches Bayesian DP algorithm (Strens,2000). step (or every couple steps), single model sampledposterior distribution transition models, action optimal modelexecuted. Although popular approach practice, algorithm knowntheoretical guarantee relating Bayes-optimal solution. Bandit case,reduces Thompson Sampling. Optimism generated solving posterior sampleslikely yield optimistic values unknown parts mdp (where posteriorentropy large) force agent visit regions. Best SampledSet (boss) algorithm generalizes idea (Asmuth, Li, Littman, Nouri, & Wingate, 2009).boss samples number models posterior combines optimistically.drives sufficient exploration guarantee finite-sample performance guarantees,theoretical guarantees cannot easily related Bayes-optimal solution. bossquite sensitive parameter governs sampling criterion, unfortunatelydifficult select. Castro Precup proposed variant, referred sboss,provides effective adaptive sampling criterion (Castro & Precup, 2010).One also see certain non-Bayesian methods light. instance, BayesianExploration Bonus (beb) solves posterior mean mdp, additional rewardbonus depends visitation counts (Kolter & Ng, 2009). bonus tailoredmethod satisfies so-called pac-bamdp property, generalizes pac-mdpmentioned introduction, implies limiting polynomial factor numbersteps EE policy different Bayes-optimal policy. recentapproach bolt algorithm, merges ideas beb boss, enforces optimismtransitions (temporarily) adding fictitious evidence currently poorly-known851fiGuez, Silver, & Dayanactions lead currently poorly-known states (Araya-Lopez, Buffet, & Thomas, 2012). boltalso pac-bamdp property.Discussion Existing MethodsDespite recent progress approximation algorithms, tackling large domains complex structured priors remains computational reach existing Bayesian RL methods.Unfortunately, exactly structured domains Bayesian methods shine,since statistical capacity take advantage priors.Methods tackle bamdp directly forward-search methods, sufferrepeated computation bamdp dynamics inside search tree priors.is, compute single bamdp transition Equation 16, one needs apply Bayesrule perform integration possible models. done cheaplysimple priors, rather expensive arbitrary priors.hand, optimism-based methods attractive appeartractable since dealing smaller mdps. However, turns hardtranslate sophisticated prior knowledge form bonus existing methodscompatible simple Dirichlet-Multinomial models. Moreover, behaviorearly steps exploration sensitive precise parameter inducingoptimism.therefore developed approximation algorithm Bayesian RL compatiblecomplex priors, maintaining efficiency (Bayesian) soundness, largeEE tasks approached principled way. approach adapts pomcp MonteCarlo tree search algorithm (Silver & Veness, 2010), avoids expensive applicationsBayes rule sampling root tree. also extends approachintroducing novel scheme lazy sampling. makes possible search locallyfinite portions large even infinite domains.3. BAMCP Algorithmreiterate, goal bamdp planning method find, decision point hs, hiencountered, action least approximately maximizes future expected return(i.e., find optimal EE policy (s, h)). algorithm, Bayes-Adaptive Monte-CarloPlanning (BAMCP), performing forward-search space possiblefuture histories bamdp using tailored Monte-Carlo tree search.employ uct algorithm, presented Section 2.1.3, allocate search effortpromising branches state-action tree, use sample-based rollouts provide valueestimates node. clarity, let us denote Bayes-Adaptive uct (ba-uct)algorithm applies vanilla uct bamdp (i.e., particular mdp dynamicsdescribed Equation 16).3 Sample-based search bamdp using ba-uct requiresgeneration samples P + every step simulation expensive proceduresimplest generative models P (P). avoid cost samplingsingle transition model P posterior root search tree start3. using uct solve bamdps mentioned Asmuth Littman (2011), awarepublished work evaluated performance ba-uct.852fiBayes-Adaptive Monte-Carlo Planningsimulation i, using P generate necessary samples simulation.Sample-based tree search acts filter, ensuring correct distribution statesuccessors obtained tree nodes, sampled P + . rootsampling method originally introduced pomcp algorithm (Silver & Veness, 2010),developed solve discrete-state pomdps.Combining ba-uct version root sampling forms basis proposedbamcp algorithm; detailed Section 3.1. addition, bamcp also takes advantagelazy sampling reduce sampling complexity root, detailed Section 3.2.Finally, bamcp integrates rollout learning improve rollouts online, detailedSection 3.3. Section 3.4, show bamcp converges Bayes-optimal solution.Subsequent sections provide empirical results.3.1 BA-UCT Root Samplingroot node search tree decision point represents current statebamdp. tree composed state nodes representing belief states hs, hi actionnodes representing effect particular actions parent state node. visitcounts: N (hs, hi) state nodes, N (hs, hi, a) action nodes, initialized 0updated throughout search. value, Q(hs, hi, a), initialized 0, also maintainedaction node.simulation traverses tree without backtrackingfollowing uct policypstate nodes defined argmaxa Q(hs, hi, a) + c log(N (hs, hi))/N (hs, hi, a), cexploration constant needs set appropriately. Given action, transitiondistribution P corresponding current simulation used sample next state.is, action node (hs, hi, a), s0 sampled P (s, a, ), new state nodeset hs0 , has0 i.simulation reaches leaf, tree expanded attaching new state nodeconnected action nodes, rollout policy ro used control mdp definedcurrent P . policy followed fixed total depth (determined usingdiscount factor). rollout provides estimate value Q(hs, hi, a) leafaction node. estimate used update value action nodes traversedsimulation: R sampled discounted return obtained traversedaction node (hs, hi, a) given simulation, update value action nodeQ(hs, hi, a)+ (R Q(hs, hi, a))/N (hs, hi, a) (i.e., mean sampled returns obtainedaction node simulations).detailed description bamcp algorithm provided Algorithm 1. diagramexample bamcp simulations presented Figure 1. Section 3.4, show bamcpeventually converges Bayes-optimal policy.Finally, note history transitions h generally compact sufficient statistic belief fully observable mdps. can, instance, replacedunordered transition counts , considerably reducing number states bamdpand, potentially complexity planning. bamcp search reduced search space,takes form expanding lattice rather tree. found versionbamcp offer marginal improvement. common finding mcts, stemming tendency concentrate search effort one several equivalent paths (up853fiGuez, Silver, & Dayan1.2.PastPastPlanningPlanningTree policyTree policyRolloutpolicyRolloutpolicy0004.3.02PastPastPlanningPlanningTree policyTree policyRolloutpolicyRolloutpolicy0020Figure 1: diagram presents first 4 simulations bamcp mdp 2 actions statehst , ht i. rollout trajectories represented dotted lines (green current rollouts,greyed past rollouts). 1. root node expanded two action nodes. Actiona1 chosen root (random tie-breaking) rollout executed P 1 resultingvalue estimate 0. Counts N (hst , ht i) N (hst , ht i, a1 ), value Q(hst , ht i, a1 ) get updated.2. Action a2 chosen root rollout executed value estimate 0. Countsvalue get updated. 3. Action a1 chosen (tie-breaking), s0 sampled P 3 (st , a1 , ).State node hs0 , ht a1 s0 gets expanded action a1 selected, incurring reward 2, followedrollout. 4. UCB rule selects action a1 top, successor state s0 sampledP 4 (st , a1 , ). Action a2 chosen internal node hs0 , ht a1 s0 i, followed rollout usingP 4 ro . reward 2 obtained 2 steps tree node. Counts traversednodes updated MC backup updates Q(hs0 , ht a1 s0 i, a1 ) R = 0+0+ 2 2+ 3 0+ =2 2 Q(hst , ht i, a1 ) + 2 3 /3 = 32 ( + 3 ).854fiBayes-Adaptive Monte-Carlo PlanningAlgorithm 1: BAMCPprocedure Search( hs, hi )repeatP P (P|h)Simulate(hs, hi, P, 0)Timeout()return argmax Q(hs, hi, a)end procedureprocedure Rollout(hs, hi, P, )Rmax <return 0endro (hs, hi, )s0 P(s, a, )r R(s, a)returnr+Rollout(hs0 , has0 i, P, d+1)end procedureprocedure Simulate( hs, hi, P, d)Rmax < return 0N (hs, hi) = 0N (hs, hi, a) 0,Q(hs, hi, a)) 0endro (hs, hi, )s0 P(s, a, )r R(s, a)R r + Rollout(hs0 , has0 i, P, d)N (hs, hi) 1, N (hs, hi, a) 1Q(hs, hi, a) Rreturn Rendq(hs,hi))argmax Q(hs, hi, b) + c log(NN (hs,hi,b)bs0 P(s, a, )r R(s, a)R r + Simulate(hs0 , has0 i, P, d+1)N (hs, hi) N (hs, hi) + 1N (hs, hi, a) N (hs, hi, a) + 1Q(hs, hi, a) Q(hs, hi, a) +RQ(hs,hi,a)N (hs,hi,a)return Rend proceduretransposition), implying limited effect performance reducing numberpaths.Note algorithm similar ba-uct root sampling also appearedwork Vien Ertel (2012), shortly bamcp originally published Guez et al.(2012).3.1.1 Root Sampling Work Simple Exampleillustrate workings bamcp, particular root sampling, simulated exampleshowcases crucial component Bayes-adaptivity.Consider simple prior distribution two mdps (P 0 P 1 ), illustrated Figure 2,P (P = P 0 ) = P (P = P 1 ) = 21 . mdps episodic stop leaves,episode starts s0 . state s1 s2 , action expected reward0 prior distribution mdps. Nevertheless, outcome transitionaction a0 state s0 carries information identity mdp, allows Bayesadaptive agent take informed decision state s1 s2 . Using Bayes-rule,P (P = P 0 |s0 a0 s1 ) P (s0 a0 s1 | P = P 0 )P (P = P 0 ) = 0.8.855fiGuez, Silver, & Dayans0a0a1s1 p = 0.8 s2 p = 0.2a0a1s3 p = 1+2a0s4 p = 12(a)s5 p = 10a1s4 p = 12s3 p = 1+2P = P0s0a0a1s1 p = 0.2 s2 p = 0.8a0a1s4 p = 12a0s3 p = 1+2(b)s3 p = 1+2a1s5 p = 10s4 p = 12P = P1Figure 2: two mdps Section 3.1.1, prior probability P (P = P 0 ) = P (P = P 1 ) =12 . Differences two mdps highlighted blue.therefore compute optimal values:(2P (P = P 0 |h) 2P (P = P 1 |h)V (h = s0 a0 s1 ) = max2P (P = P 1 |h) 2P (P = P 0 |h)(19)= 2 0.8 2 0.2 = 1.2 (= V (h = s0 a0 s2 ))V (h = s0 ) = max{0, 1.2} = 1.2.simulate bamcp simple example first decision state s0 .root sampling, bamcp samples either P 0 P 1 equal probability roottree, perform explicit posterior update inside tree. Yet, suggestedLemma 1, expect find correct distribution P (P = P 0 |s0 a0 s1 ) samplesP tree node hs0 a0 s1 i. Moreover, bamcp converge optimal values Vaccording Theorem 1. observed empirically Figure 3.second row Figure 3, observe Q(s0 a0 s1 , a1 ) slower convergecompared values. time ticking slowly non-optimalnode (i.e., small fraction simulations reach node) value stays put manysimulations.3.2 Lazy Samplingprevious work sample-based tree search, indeed including pomcp (Silver & Veness,2010), complete sample state drawn posterior root search tree.However, computationally costly. Instead, sample P lazily, generating856fiBayes-Adaptive Monte-Carlo Planning221.51.5Value1VVVV0.500.510200400600800100012001400( 0a 0s 1)( 0a 0s 1)( 0)( 0)1600180010.500.52000105104x 100.50.5Q( 0 0 1 , 1 )Q ( 0a 0s 1, 1)Value11.5211.50200400600800100012001400160018002000205104x 1011P 0 0 s(1 P = P 1 )P ( P = P 1| 0a 0s 1)Probability0.80.60.80.6P 0 0 s(1 P = P 0 )P ( P = P 0| 0a 0s 1)0.40.40.200.2020040060080010001200Number simulations1400160018002000005104x 10Figure 3: Tracking different internal variables bamcp example Section 3.1.1 = 0.9.bamcp run starting state number simulations (x-axis) c = 20.first two rows show evolution values tree nodes corresponding different histories,along target values computed Equation 20. bottom row shows evolutionPs0 a1 s1 (P = P 0 ) = 1 Ps0 a1 s1 (P = P 1 ), empirical distribution mdps seen goingPN (hs0 a1 s1 i)1[P = P 0 ]). (Left) first 2000 simulationstree node hs0 a1 s1 (i.e., N (hs01a1 s1 i) i=0(Right) Zoomed view 100,000 simulations, displaying empirical convergence targetvalues.857fiGuez, Silver, & Dayanparticular transition probabilities required simulation traversestree, also rollout.Consider P(s, a, ) parametrized latent variable s,a state actionpair. may depend other, wellR additional set latent variables .posterior P written P (|h) = P (|, h)P (|h), = {s,a |sS, A}. Define = {s1 ,a1 , , st ,at } (random) set parameters requiredcourse bamcp simulation starts time 1 ends time t. Usingchain rule, rewriteP (|, h) =P (s1 ,a1 |, h)P (s2 ,a2 |1 , , h)...P (sT ,aT |T 1 , , h)P ( \ |T , , h)length simulation \ denotes (random) set parametersrequired simulation. simulation i, sample P (|ht )root lazily sample st ,at parameters required, conditionedt1 parameters sampled current simulation. process stopped endsimulation, typically long parameters sampled. example,transition parameters different states actions independent, simplydraw necessary parameters individually state-action pair encounteredsimulation. general, transition parameters independent different states,dependencies likely structured. example, mdp dynamics could arisemixture model denotes mixture component P (|h) specifies posteriormixture proportion. Then, transition parameters conditionally independentgiven mixture component, sampling root simulation allows us samplerequired parameters s,a independently P (s,a |i , h) requiredi-th simulation. leads substantial performance improvement, especiallylarge mdps single simulation requires small subset parameters (seeexample domain Section 4.2 concrete illustration). lazy sampling schemelimited shallow latent variable models; deeper models, also benefitconditional independencies save sampling operations simulation samplingnecessary latent variables opposed sampling .3.3 Rollout Policy Learningchoice rollout policy ro important simulations few, especially domaindisplay substantial locality rewards require carefully selected sequenceactions obtained. Otherwise, simple uniform random policy chosen providenoisy estimates. work, learn Qro , optimal Q-value real mdp, modelfree manner, using Q-learning, samples (st , , rt , st+1 ) obtained off-policy resultinteraction bamcp agent mdp time t. real transition858fiBayes-Adaptive Monte-Carlo Planning(st , , rt , st+1 ) observed, updateQro (st , ) Qro (st , ) + (rt + max Qro (st+1 , a) Qro (st , )),(20)learning rate parameter; standard Q-learning rule (Watkins,1989). Acting greedily according Qro translates pure exploitation gathered knowledge. rollout policy bamcp following Qro could therefore over-exploit. Instead, similarwork Gelly Silver (2007), select -greedy policy respect Qrorollout policy ro . words, steps mdp, updated Qrotimes use following stochastic rollout policy mcts simulations + 1decision step:(1 + |A|= argmaxa0 Qro (s, a0 )ro (s, a) =(21)otherwise,|A|ro (s, a) probability selecting action mdp state (i.e., historyignored) rollout. biases rollouts towards observed regions high rewards.method provides valuable direction rollout policy negligible computationalcost. complex rollout policies considered, example rollout policiesdepend sampled model P history ht . However, usually incur computational overhead, may less desired running simulations worseestimates.3.4 Theoretical Propertiessection, show bamcp converges Bayes-optimal policy. first presenttheoretical results case exact posterior inference conducted obtainposterior samples dynamics (Section 3.4.1), extend convergence guaranteecase approximate inference (MCMC-based) necessary produce posteriorsamples (Section 3.4.2).proof Theorem 1 present supplementary material Guez et al. (2012).Theorem 2 novel contribution paper.3.4.1 Exact Inference Casemain step proving root sampling alter behavior ba-uct.proof adaptation pomcp proof Silver Veness (2010). provideintuition empirical evidence convergence simple Bandit problemsBayes-optimal solution known.Consider ba-uct algorithm: uct applied Bayes-Adaptive mdp (its dynamicsdescribed Equation 16). Let (hT ) rollout distribution ba-uct:probability history hT generated running ba-uct search hst , ht i,ht prefix hT , effective horizon search tree, arbitrary EEpolicy. Similarly define quantities (hT ): probability history hT generatedrunning bamcp algorithm, Ph (P): distribution P node h859fiGuez, Silver, & Dayanrunning bamcp. following lemma shows rollout statisticsbamcp ba-uct.4Lemma 1 (hT ) = (hT ) EE policies : H A.Proof Let arbitrary. show induction suffix histories h ht , (a)(h) = (h); (b) P (P |h) = Ph (P), P (P |h) denotes (as before) posteriordistribution dynamics given h.Base case: root (h = ht , suffix history size 0), clear Pht (P) = P (P |ht )since sampling posterior root node (ht ) = (ht ) = 1 sincesimulations go root node.Step case:Assume proposition true suffices size j. Consider suffix has0 size j + 1,s0 arbitrary h arbitrary suffix size j ending s.following relation holds:Z0(has ) = (h)(h, a)dP P (P |h) P(s, a, s0 )(22)ZP= (h)(h, a)dP Ph (P) P(s, a, s0 )(23)P= (has0 ),(24)second line obtained using induction hypothesis, restdefinitions. addition, match distribution samples P node has0 :P (P |has0 ) = P (has0 | P)P (P)/P (has0 )(25)00(26)00(27)= P (h| P)P (P) P(s, a, )/P (has )= P (P |h)P (h) P(s, a, )/P (has )0= ZP (P |h) P(s, a, )0(28)= Z Ph (P) P(s, a, )(29)0= Z Pha (P) P(s, a, )(30)= Phas0 (P),(31)Equation 29 obtained induction hypothesis, Equation 30 obtainedfact choice action node made independently samples P.Finally, obtain Equation 31 Equation 30, consider probability sampleP arrives node has0 , first needs traverse node ha (this occurs probabilityPha (P)) then, node ha, state s0 needs sampled (this occurs probability P(s, a, s0 )); therefore, Phas0 (P) Pha (P) P(s, a, s0 ). Z normalization constant:RRZ = 1/( P P P(s, a, s0 )P (P |h)) = 1/( P P P(s, a, s0 )Ph (P)). completes induction.4. ease notation, refer node history only, opposed state historyrest paper.860fiBayes-Adaptive Monte-Carlo Planningproof Lemma 1 make explicit use lazy sampling, since methodrealizing values relevant random variables affect rollout distributionaffect computed, how.Define V (hs, hi) = max Q(hs, hi, a) hs, hi H. show bamcp conaAverges Bayes-optimal solution.Theorem 1 > 0 (the numerical precision, see Algorithm 1) suitably chosenmaxc (e.g. c > R1), state hst , ht i, bamcp constructs value function root nodepconverges probability 0 -optimal value function, V (hst , ht i) V0 (hst , ht i),. Moreover, large enough N (hst , ht i), bias V (hst , ht i) decreases0 = 1O(log(N (hst , ht i))/N (hst , ht i)).Proof uct analysis Kocsis Szepesvari (2006) applies ba-uct algorithm,since vanilla uct applied bamdp (a particular mdp). also applies arbitrary rollout policies, including one developed Section 3.3. Lemma 1, bamcpsimulations equivalent distribution ba-uct simulations. nodes bamcptherefore evaluated exactly ba-uct, providing result.Lemma 1 provides intuition belief updates unnecessary searchtree: search tree filters samples root node distributionsamples node equivalent distribution obtained explicitly updatingbelief. particular, root sampling pomcp (Silver & Veness, 2010) bamcpdifferent evaluating tree using posterior mean. illustrated empiricallyFigures 4 5 case simple Bandit problems.3.4.2 Approximate Inference CaseTheorem 1, made implicit assumption bamcp provided true samplesdrawn iid posterior. However, sophisticated priors require formapproximate sampling scheme (see, example, task Section 4.2), MarkovChain Monte Carlo (MCMC), generally deliver correlated posterior sampleschain converges stationary distribution (Neal, 1993). Thus, necessary extendproof convergence bamcp deal samples nature.Theorem 2 using approximate sampling procedure based MCMC chainstationary distribution P (P |ht ) (e.g., Metropolis-Hastings Gibbs sampling) producesample sequence P 1 , P 2 , . . . root node bamcp, value V (hst , ht i) foundbamcp converges probability (near-)optimal value function.Proof Let > 0 chosen numerical accuracy algorithm. choosefinite depth search tree function , rmax , guarantees sumtotal return depth amountsPto less . consider leaf Q-nodetree, mean value = n1 nm=1 rm n simulations, rm rewardobtained node m-th simulation going node. Since ucbused throughout tree, exploration never ceases guarantees n (seeexample Kocsis & Szepesvari, 2006, Thm. 3).861fiGuez, Silver, & Dayan80Discounted sum rewardsUndiscounted sum rewards26024022020018075706560160BayesoptimalBAMCPPosterior Mean(a)55BayesoptimalBAMCPPosterior Mean(b)Figure 4: Performance comparison bamcp (50000 simulations, 100 runs) posterior meandecision 8-armed Bernoulli bandit = 0.99 300 steps. arms successprobabilities 0.6 except one arm success probability 0.9. Bayes-optimalresult obtained 1000 runs Gittins indices (Gittins et al., 1989). a. Mean sumrewards 300 steps. b. Mean sum discounted rewards 300 steps.Root sampling filtering (Lemma 1) still holds despite approximate samplingroot node; since statement distribution samples, ordersamples arrive. Therefore, distribution dynamics node convergesright stationary distribution P (P |hi ), hi history corresponding node i.Asymptotic results Markov Chains (Law large numbers Markov Chains) guaranteeus a.s., true expected reward leaf node i.Given convergence leaves, work way tree backward inductionshow values node converge (near-)optimal values. particularvalue root converges optimal value.3.5 Possible Misuse Latent Variable Information: Counter-Exampleplanning bamdp using sample-based forward-search algorithm bamcp,could tempting use knowledge available sampler producing samples(such value latent variables model) take better planning decisions.example, generating sample P iR dynamics according posterior distribution P (P |h) written P (P |)P (|h), P might generatedsampling P (|h) sampling P P (P |i ). Since value availablecontain high-level information, one natural question ask whether searchinformed value .862fiBayes-Adaptive Monte-Carlo PlanningBAMCP Number simulations: 5000BAMCP Number simulations: 25000020201515105551015205BAMCP Number simulations: 2500000201515201510510551015205101520Posterior mean decisionProbability correct decision1510BAMCP Number simulations: 50000002020101000.20.40.60.8155101520Figure 5: Evaluation bamcp Bayes-optimal policy, case = 0.95, choosingdeterministic arm reward 0.5 stochastic arm reward 1 posteriorprobability p Beta(, ). result tabulated range values , , cell valuecorresponds probability making correct decision (computed 50 runs)compared Gittins indices (Gittins et al., 1989) corresponding posterior. firstfour tables corresponds different number simulations bamcp last table showsperformance acting according posterior mean. range , values,Gittins indices stochastic arm larger 0.5 (i.e., selecting stochastic armoptimal) + 1 also = + 2 6. Acting according posterior meandifferent Bayes-optimal decision >= Gittins index larger 0.5.bamcp guaranteed converge Bayes-optimal decision cases, convergenceslow edge cases Gittins index close 0.5 (e.g., = 17, = 19,Gittins index 0.5044 implies value 0.5044/(1) = 10.088 stochasticarm versus value 10 0.5 + 10.088 = 10.0836 deterministic arm).863fiGuez, Silver, & DayanHere, outline one incorrect way using latent variable value search.Suppose would want split search tree value (this would occur implicitlyconstructing history features based value ), provide simplecounter-example shows valid search approach.Consider simple prior distribution two 5-state mdps, illustrated Figure 6,P ( = 0|h0 ) = P ( = 1|h0 ) = 21 , P (P |) delta function illustrated mdp.s0a0s0a1a0s1 p = 1a0s2 p = 1+1s1 p = 1a1s3 p = 1+2a0s4 p = 12(a)a1s2 p = 1+1a1s4 p = 12=0s3 p = 1+2(b)=1Figure 6: two possible mdps corresponding two settings .h0 = s0a0a1s1s2+1a012s312+2a112s4122Figure 7: bamdp, nodes correspond belief(or history)-states.2 deterministic actions (a0 , a1 ) mdp, episode length 1 2 steps.difference two mdps outcome taking action a0 a1 states1 , illustrated Figure 6, a0 rewarding = 0 costly = 1,vice-versa a1 . rewards obtained executing actionterminal states (s2 , s3 , s4 ).Observing first transition informative, implies posterior distribution unchanged first transition: P (P |h0 ) = P (P |h0 a0 s1 ) = P (P |h0 a1 s2 ).bamdp corresponding problem illustrated Figure 7.864fiBayes-Adaptive Monte-Carlo Planninghistory-state h0 = s0 , Bayes-optimal Q values easily computed:Q (h0 , a1 ) = ,(32)Q (h0 a0 s1 , a0 ) = 0 + (2 P (s3 |h0 a0 s1 a0 ) 2 P (s4 |h0 a0 s1 a0 ))= (1 1) = 0,(33)(34)Q (h0 a0 s1 , a1 ) = 0 + (2 P (s3 |h0 a0 s1 a0 ) 2 P (s4 |h0 a0 s1 a0 ))(35)= (1 1) = 0,(36)(37)Q (h0 , a0 ) = 0 + max Q (h0 a0 s1 , a) = 0,implies a1 = (h0 ) . [We used fact P (s3 |h0 a0 s1 a0 ) = P ( =0|h0 a0 s1 a0 ) P (s3 | = 0, s1 a0 ) + P ( = 1|h0 a0 s1 a0 ) P (s3 | = 1, s1 a0 ) = 21 1 + 12 0 = 12 ,similarly P (s4 |h0 a0 s1 a0 )].Note that, since belief updates occur terminal states, forward-searchwithout root sampling equivalent. would construct search treeFigure 7 compute right value right decision.problem comes decide split search tree chance nodes basedvalue generated samples going tree. example, takingaction a0 state s0 , would using either mdp = 0 w.p 0.5 mdp= 1 w.p 0.5. Since multiple values go node h0 a0 , wouldbranch tree illustrated Figure 8. search tree problematic valuecomputed Q (h0 , a0 ) becomes 2 2 , larger Q (h0 , a1 ) = > 0.5.Therefore, policy computed root longer Bayes-optimal.h0 = s0a0= 0, s1a1s2= 1, s1+1a0s3a1a0s4+2a1s32s4+22Figure 8: problematic search tree.branching latent variable value, creating spurious observations:implying latent variable past observed future,case.summarize, Bayes-adaptive policy optimized must function futurehistories (i.e., things well actually observe future), cannot function futureunobserved latent variables. Ignoring causes problems simple domainsone illustrated above, similar scenarios would occur complex latent variablemodels reasons.865fiGuez, Silver, & Dayan4. Experimentsfirst present empirical results bamcp set standard problems comparisonspopular algorithms. showcase bamcps advantages large scale task:infinite 2D grid complex correlations reward locations.4.1 Standard Domainsfollowing algorithms run standard domains: bamcp, sboss, beb, bfs3.Details implementation parametrization found Appendix B.addition, report results work Strens (2000) several algorithms.following domains, fix = 0.95.Double-loop domain 9-state deterministic mdp 2 actions (Dearden,Friedman, & Russell, 1998), 1000 steps executed domain. illustratedFigure 9(a).Grid5 5 5 grid reset state one corner, single reward statediametrically opposite reset state. Actions cardinal directions executedsmall probability failure (pfailure = 0.2) 1000 steps.expectacan approxias derived fairlyGrid10 10 10 grid designed way Grid5. collect 2000 stepsdomain.b,2a,10a,0a,0a,0a,01 b,2 Maze234Deardens264-statesmaze53 flags collect (Dearden et al., 1998).b,2special state providesrewardequivalentnumber flags collected sinceb,2b,2last visit. 20000 steps executed domain5 . illustrated Figure 9(b).ed two possiblefirst, modate, mightture update,ation.(a) Task 1 [11].b,2a,1016b,0b,0a,02b,2b,2b,2a,0b,25a,0a,03b,0a,014a,b,052a,0a,0a,00a,b,0Figure 1. Chain problemalgorithms conces showes,case,-VPI strategiesnvergence prooftried infinitely0earning rate.shows-values.use momentect mean.finitely oftenupdating,every stateprovefinitely oftena,b,276.1 ProblemDescriptionsFigurea,b,134 arcsFigure 1b,0shows 85-state Chain problem.a,b,0arelabeled actions cause state transition,associated rewards.HoweveragentFigure 3. Maze problem.(b) Task(a) 2 [14].(b)abstract actions {1,2} available. Usually abstract action 1causes real-world action take place, abstractdirection perpendicular intended (with2 causes real-world action b. probability 0.2,standardF domainsGeffect.9: actionTwodescribedSection4.1: a)Double-loopdomain,b) Deardensprobability0.1).33 reachablelocationsagentslipsactionoppositemaze (including goal) 8maze.FiguresworkStrensoptimalbehaviortothealwayschooseaction 1(2000).(evencombinations status flags time.though sometimes results transitions labeledyields 264 discrete states. agent given limitedb). state 5 reached, reward 10 usuallylayout information (identifying immediate successorsreceived several times agent slips, startsorder tothereducecomplexitystate performance1. problemofrequiresquantifyeffectivealgorithm,westate)measuredtotalundiscountedposterior distribution Bayesian DP approach.exploration accurate estimation discounted reward.reward many steps. chose measure performance enable fair comparisonsFigure 2 shows Loop problem involves two6.2 Resultsdrawnpriorwork.fact,TwoareFactionsoptimisingdifferent criterion discountedloopslength5 joinedsinglestart state.available transitions deterministic. Takingexperimental results show accumulated totalsrewardDPreceivedoverStrenslearning(2000)phases5. resultreportedmazetheloop,Bayesianalg.consistdifferentactionrepeatedlycauses traversalrightF Deardenssteps Chain Loop, 20000 stepsyieldingreward1themazeevery layout5 actionstaken. 1000versiontaskgivenagent.Maze. Averages taken 256 runs ChainConversely,taking actionproblem.b repeatedly causesistraversal(c) Task 3.navigationstartstate.Loop,16 runs Maze. Table 1 summarizesleft loop, yielding reward 2 every 5 actionscomparativeperformance 1, 2, 8 phasesagent receivesrewarduponreachingbasednumbertaken. problem requires difficult compromise866learning. (Note results pessimisticexploration exploitation.flags collected.show rewards actually received learningFigure 3 shows Maze problem. agent moverather rewards could receivedinstantaneous greedy policy.) Bayesian DP method,one squareFigureleft,3: right,Theupthreedomainsusedmaze.experiments.new hypothesis (for MDP) drawn timeattempts move wall, action effect.problem move start (top-left) goal(top-right) collecting flags way.system entered starting state. Maze, newhypothesis also obtained every 24 stepsfiBayes-Adaptive Monte-Carlo Planningreward start state might expect evaluation unfavourablealgorithm.Although one major advantage Bayesian RL one specify priorsdynamics, domains, used rather generic priors enable comparisons previous work. Double-loop domain, Bayesian RL algorithms run simple1Dirichlet-Multinomial model symmetric Dirichlet parameter = |S|. gridsmaze domain, algorithms run sparse Dirichlet-Multinomial model,described Friedman Singer (1999). models, efficient collapsedsampling schemes available; employed ba-uct bfs3 algorithmsexperiments compress posterior parameter sampling transition samplingsingle transition sampling step. considerably reduces cost belief updatesinside search tree using simple probabilistic models. Unfortunately, efficient collapsed sampling schemes available general (see example modelSection 4.2).Sum Rewards 1000 steps9090BAMCP80807070606050504040303020201031021109010010108070706060505040403030202031021031021090SBOSS8010BFS311001010110110100BEB310210Average Time per Step (s)100Figure 10: Performance algorithm Grid5 domain function planning time.point corresponds single run algorithm associated setting parameters.Increasing brightness inside points codes increasing value parameter (bamcpbfs3: number simulations, beb: bonus parameter , sboss: number samples K).second dimension variation coded size points (bfs3: branching factor C,sboss: resampling parameter ). range parameters specified Appendix B.summary results presented Table 1. Figures 10 11 report planningtime/performance trade-off different algorithms Grid5 Maze domain.867fiUndiscounted sum rewards 20000 stepsGuez, Silver, & Dayan11001000900BAMCP (BAUCT+RS+LS+RL)BEBBFS3SBOSS80070060050040030020010001010Average Time per Step (s)10Figure 11: Performance algorithm, Figure 10 Deardens Maze domain.BAMCPBFS3 (Asmuth & Littman, 2011)SBOSS (Castro & Precup, 2010)BEB (Kolter & Ng, 2009)Bayesian DP* (Strens, 2000)Bayes VPI+MIX* (Dearden et al., 1998)IEQL+* (Meuleau & Bourgine, 1999)QL Boltzmann*Double-loop387.6 1.5382.2 1.5371.5 3386 0377 1326 31264 1186 1Grid572.9 366 559.3 467.5 3-Grid1032.7 310.4 221.8 210 1-Deardens Maze965.2 73240.9 46671.3 126184.6 35817.6 29269.4 1195.2 20Table 1: Experiment results summary. algorithm, report mean sum rewardsconfidence interval best performing parameter within reasonable planning time limit(0.25 s/step Double-loop, 1 s/step Grid5 Grid10, 1.5 s/step Maze).bamcp, simply corresponds number simulations achieve planning timeimposed limit. * Results Strens (2000) reported without timing information.domains tested, bamcp performed best. algorithms came closetasks, parameters tuned specific domain.particularly evident beb, required different value exploration bonus achievemaximum performance domain. bamcps performance stable respectchoice exploration constant (c = 3) require fine tuning obtainresults.bamcps performance scaled well function planning time, evident Figures 10 11. contrast, sboss follows opposite trend. samples employedbuild merged model, sboss actually becomes optimistic over-explores, de868fi(a)Undiscounted sum rewards 20000 stepsBayes-Adaptive Monte-Carlo Planning11001000BAUCT + RLBAUCT9008007006005004003002001000110Average Time per Step (s)0101100BAUCT + RS + RL1000BAUCT + RS900800700600(b)500400300200100011011001000010BAUCT + RS + LS + RL (BAMCP)BAUCT + RS + LS900800700600(c)5004003002001000110010Figure 12: Evolution performance ba-uct bamcp Deardens Maze domain. bamcp presentplots comparison, also displayed Figure 11. a. Performance vanilla ba-uctwithout rollout policy learning (RL) presented Section 3.3. b. Performanceba-uct Root Sampling (RS), presented Section 3.1, without rolloutlearning. c. Performance ba-uct Root Sampling Lazy Sampling (LS), presentedSection 3.2. addition rollout policy learning, bamcp algorithm.869fiGuez, Silver, & Dayangrading performance. beb cannot take advantage prolonged planning time all.performance bfs3 generally improves planning time, given appropriatechoice parameters, obvious trade-off branching factor, depth,number simulations domain. bamcp greatly benefited lazy samplingscheme experiments, providing 35 speed improvement naive approachmaze domain example; illustrated Figure 12.Deardens maze aptly illustrates major drawback forward search sparse samplingalgorithms bfs3. Like many maze problems, rewards zero least k steps,k solution length. Without prior knowledge optimal solution length,upper bounds higher true optimal value tree fullyexpanded depth k even simulation happens solve maze. contrast,bamcp discovers successful simulation, Monte-Carlo evaluation immediately biassearch tree towards successful trajectory.Figure 12 confirms that, even moderate-sized domain simple prior (Independent Sparse Dirichlet-Multinomial), bamcp amply benefits root sampling, lazysampling, rollout learning. complex priors, following section, ba-uctbecomes computationally intractable. Root sampling lazy sampling mandatorycomponents.4.2 Infinite 2D Grid Task......Figure 13: portion infinite 2D grid task generated Beta distribution parameters 1 = 1, 1 = 2(columns) 2 = 2, 2 = 1 (rows). Black squares location (i,j) indicates reward 1,circles represent corresponding parameters pi (blue) qj (orange) rowcolumn (area circle proportional parameter value). One way interpretparameters following column implies collection 2pi /3 reward average (2/3mean Beta(2, 1) distribution) whereas following row j implies collection qj /3reward average; high values parameters pi less likely high values parametersqj . parameters employed results presented Figure 14-c).870fiBayes-Adaptive Monte-Carlo Planningperhaps unfair characterize domains previous sectionlimited scale. Indeed, seen correct reflection stateart Bayesian RL. However, bamcp, root-based lazy sampling,applied considerably larger challenging domains. therefore designed newproblem well beyond capabilities prior algorithms since infinitecombinatorially structured state space, even challenging belief space. Althoughstill abstract, new task illustrates something bamcps power.4.2.1 Problem Descriptionnew problem class complex mdps infinite grid. draw particularmdp, column associated latent parameter pi Beta(1 , 1 ) row jassociated latent parameter qj Beta(2 , 2 ). probability grid cell ijreward 1 pi qj , otherwise reward 0. agent knows gridalways free move four cardinal directions. Rewards consumedvisited; returning location subsequently results reward 0. opposedindependent Dirichlet priors employed standard domains, here, dynamics tightlycorrelated across states (i.e., observing state transition provides informationstate transitions).domain illustrated Figure 13. Although uncertainty appears concernreward function mdp rather dynamics, viewed formally uncertainty dynamics state augmented binary variable indicateswhether reward present.6Formally, since rewards disappear one visit, description statemdp needs include information state rewards (for exampleform set grid locations previously visited) addition position agentinfinite grid. state therefore combination current agents location(i, j), unordered set previously visited locations V , binary variable R = rij .dynamics P deterministically updates position agent visitedlocations based agents action, updates R according reward map.known reward function simply R(s, a) = s(R) (i.e., described before,agent gets reward position ij rij = 1).4.2.2 InferencePosterior inference (of dynamics P) model requires approximationnon-conjugate coupling variables. see this, consider posterior probabilityparticular grid cell kl reward 1 (denote event rkl = 1),Zpk ql P (pk , ql |O) dpk dql ,P (rkl = 1|O) =(38)pk ,ql= {(i, j)} set observed reward locations, associated observedreward rij {0, 1}. Sampling rkl straightforward given access posterior samples6. fact, bamdp framework straightforwardly extended deal general, partiallyobserved, reward functions (Duff, 2002).871fiGuez, Silver, & Dayanpk ql . However, posterior distribution pk ql , P (pk , ql |O), cannot easilysampled from, given by:P (pk , ql |O) P (O|pk , ql )P (pk )P (ql )ZP (O|PO , QO )P (p)P (q)=PO \pk ,QO \qlZ=pPO(40)qQO(pi qj )rij (1 pi qj )1rijPO \pk ,QO \ql (i,j)O(39)Beta(p; 1 , 1 )pPOBeta(q; 2 , 2 ),qQO(41)PO denotes set parameters pi observed columns (columns leastone observation exists) similarly QO rows. posterior suffers nonconjugacy (because multiplicative interaction two Beta distribution)also complicated dependence structure (pk ql depend observations outsidecolumn k row l). reasons, inference done approximately via MetropolisHastings (details Appendix C).4.2.3 ResultsPlanning algorithms attempt solve mdp based sample(s) (or mean)posterior (e.g., boss, beb, Bayesian DP) cannot directly handle large combinatorialstate space. Previous forward-search methods (e.g., ba-uct, bfs3) deal statespace, complex belief space: every node search tree must solveapproximate inference problem estimate posterior beliefs. contrast, bamcplimits posterior inference root search tree directly affectedsize state space belief space, allows algorithm perform well evenlimited planning time. Note lazy sampling required setup since fullsample dynamics involves infinitely many parameters.Figure 14 demonstrates planning performance bamcp complex domain.Performance improves additional planning time. quality prior clearly affectsagents performance, bamcp take advantage correct prior information gainrewards. addition, behavior agent qualitatively different dependingprior parameters employed.example, case Figure 14-a, rewards often found relatively denseblocks map agent exploits fact exploring; explains highfrequency short dwell times. Figure 14-b, good rewards rates obtainedfollowing rare rows high qj parameters, finding good rows expensiveleast two reasons: 1) good rows far agents current position 2)takes longer decide value row observations lack rewards;entropy posterior larger given observations rewards (which explainedeither rows columns poor, time) given observationsrewards (which explained high probability rows columnsgood, since rij Bernoulli(pi qj )). Hence, agent might settle sub-optimal rowslarge periods time, example gains enough confidence better row likelyfound nearby (as Bandit problems Bayes-optimal agent might settle872fi605030110010Planning time (s)45403530252015110010Planning time (s)8654210706050403020110010Planning time (s)110110010Planning time (s)11070.050501001502005010015020050100150200Dwell Time (Horizontal)0.056.565.554.5403.532.52210180071090102109150102101010Discounted sum rewards 200 stepsUndiscounted sum rewards 200 steps20210Undiscounted sum rewards 200 stepsBAMCPBAMCP Wrong priorRandom11Frequency70BAMCPBAMCP Wrong prior12Frequency800.0513110010Planning time (s)110130.050Dwell Time (Horizontal)0.05121110Frequency904014Discounted sum rewards 200 steps100Discounted sum rewards 200 stepsUndiscounted sum rewards 200 stepsBayes-Adaptive Monte-Carlo Planning98706543210110010Planning time (s)1100.050Dwell Time (Horizontal)Figure 14: Performance bamcp function planning time Infinite 2D grid task, = 0.97,row corresponds different set parameters generating grid. performance first 200 steps environment averaged 50 sampled environments(5 runs sample) reported terms undiscounted (left) discounted(center) sum rewards. bamcp run either correct generative model prior (solidgreen) incorrect prior (dotted green). performance uniform random policyalso reported (blue). small sample portion grid generated parametersdisplayed row, presented Figure 13. frequency histogram dwell timesnumber consecutive steps agent stays row switching reportedscenario. grids generated Beta parameters a) 1 =0.5, 1 =0.5, 2 =0.5, 2 =0.5,b) 1 =0.5, 1 =0.5, 2 =1, 2 =3, c) 1 =2, 1 =1, 2 =1, 2 =2. case wrong priors (dot-dashed lines), bamcp given parameters a) 1 =4, 1 =1, 2 =0.5, 2 =0.5, b)1 =1, 1 =3, 2 =0.5, 2 =0.5, c) 1 =1, 1 =2, 2 =2, 2 =1.873fiGuez, Silver, & Dayansub-optimal arm believes likely best arm given past data). heavier-taildistribution dwell times scenario, Figure 14-b, reflects behavior.case Figure 14-c consists mixture rich poor rows. agentdetermine moderately quickly row good enough, given expects find,switches nearby row. good enough row found, agent sticklarge periods time. reflected bimodal nature distributiondwell times Figure 14-c. many cases, agent satisfied one first rowsvisits, since likely agent starts good row. decides stayentire duration episode, explains peak towards 200.bamcps prior belief dynamics generative modelsdistribution (Wrong prior dot-dashed lines Figure 14), maladaptive behaviorobserved. instance, Figure 14-a, deluded agent expects columns rich,rows rich others poor. Hence, good strategy given priorbelief find one good rows exploit travelling horizontally. However,since lot columns actually poor generative model, agent never encounterscontinuous sequence rewards expects find good rows. Given wrong prior,even actually good row, explains observation row poorrather column switches different row. behavior reflectedshorter horizontal dwell times plotted Figure 14-a. Similar effects observedWrong prior cases Figure 14-b,c.pointed actual Bayes-optimal strategy domainknown behavior bamcp finite planning time might qualitatively matchBayes-optimal strategy. Nevertheless, speculate behavior observebamcp, including apparently maladaptive behaviors, would also foundBayes-optimal solution.5. DiscussionBayesian model-based reinforcement learning addresses problem optimizing discounted return agent dynamics uncertain. solving augmentedmdp called bamdp, agent optimally trade-off exploration exploitationmaximize expected discounted return according prior beliefs. formally attractive, framework suffers major drawback: computationally intractablesolve bamdp exactly. aware formal complexity analysissolving bamdps, bamdps mapped continuous-state pomdps discrete observations (Duff, 2002). general, solving discrete pomdps known challenging(Mundhenk, Goldsmith, Lusena, & Allender, 2000; Madani, Hanks, & Condon, 2003).approximate Bayes-optimal solution efficiently, suggested sample-basedalgorithm Bayesian RL called bamcp significantly surpassed performanceexisting algorithms several standard tasks. showed bamcp tackle largercomplex tasks generated structured prior, existing approaches scalepoorly. addition, bamcp provably converges Bayes-optimal solution, evenMCMC-based posterior sampling employed.main idea employ Monte-Carlo tree search explore augmented Bayesadaptive search space efficiently. naive implementation idea algorithm874fiBayes-Adaptive Monte-Carlo Planningcalled ba-uct. However, ba-uct cannot cope priors employsexpensive belief updates inside search tree. therefore introduced three modificationsobtain computationally tractable sampled-based algorithm: root sampling,requires beliefs sampled start simulation (as Silver & Veness, 2010);model-free RL algorithm learns rollout policy; lazy sampling schemeenables posterior beliefs sampled cheaply.5.1 Future Work: AlgorithmsDespite excellent empirical performance many domains (Gelly et al., 2012), uctalgorithm known suffer several drawbacks. First, finite-time regret bound.possible construct malicious environments, example optimal policyhidden generally low reward region tree, uct misled longperiods (Coquelin & Munos, 2007). course, setting, appropriate prior distributionsmight help structure search effectively. But, issue convergence MCMCchain approximate inference settings may hinder effort get finite-time guarantees.Second, uct algorithm treats every action node multi-armed bandit problem.However, actual benefit accruing reward planning,theory appropriate use pure exploration bandits (Bubeck, Munos, & Stoltz, 2009).focused learning dynamics (and implicitly rewards infinite gridtask) fully observable mdp. states observed directly, bamcp couldextended maintain beliefs dynamics state. state dynamicswould sampled posterior distribution, start simulation.setting known Bayes-Adaptive Partially Observable mdp (bapomdp) (Ross, Pineau,Chaib-draa, & Kreitmann, 2011).work, limited case discrete-state mdps, since alreadypresent significant challenges Bayesian exploration. bamcp cannot straightforwardlyconverted deal continuous-state mdps, remains see whether ingredients make bamcp successful discrete setting could reassembledcontinuous-state solution, example using form value function approximationsimulation-based search (Silver, Sutton, & Muller, 2008).5.2 Future Work: Priorsbamcp able exploit prior knowledge dynamics principled manner.possible encode many aspects domain knowledge prior distribution,important avenue future work explore rich, structured priors dynamicsmdp. showed, prior knowledge matches class environmentsagent encounter, exploration significantly accelerated. thereforeimportant understand select learn appropriate priors large real-worldtasks tackled Bayesian RL. One promising category rich priors contextnon-parametric priors. example, Doshi-Velez, Wingate, Roy, Tenenbaum (2010)Wingate, Goodman, Roy, Kaelbling, Tenenbaum (2011) investigateddirection, yet combination myopic planning algorithms, ratherBayes-Adaptive planning.875fiGuez, Silver, & Dayan5.3 Evaluation Bayesian RL AlgorithmsBayesian RL algorithms traditionally tested small, hand-crafted, domains.Even though domains contain substantial structure, priors given agentusually independent Dirichlet distributions generate unstructured randomworlds. mismatch prior distribution domains problematicevaluate, since perfect Bayesian RL algorithm guaranteed perform well givenincorrect priors. Since tractable compute Bayes-optimal solution exactly,becomes impossible decide whether algorithm obtains low return comparedalgorithms hand-crafted domain better worse approximationBayes-optimal solution.purpose algorithmic evaluation, obvious solution design priorsactually generate tasks solve. Bayesian RL algorithm givengenerative model prior also tested many generated tasks,Bayes-optimal solution guaranteed obtain best discounted return average.case, higher mean return becomes synonymous better approximation givengoal matching Bayes-optimal solutions performance. employed methodaveraging across generated domains Section 4.2 evaluate bamcp algorithmInfinite Grid task. understand exploration performance proposed algorithmic solutions truly, future comparisons algorithms would likely benefitevaluation schemes.5.4 ConclusionBayes-adaptive planning conceptually appealing computationally challenging.enormous computation required prior approaches largely due factroot values computed expectations and/or maximisations complete treepossible actions states follows current history. addition,values must integrate distribution transition potentially reward modelsstate search tree. result, computation typically grows exponentiallysearch depth, rate determined action space, successor state space, modelspace.new algorithm, bamcp, builds previous work (Kearns et al., 1999; Kocsis &Szepesvari, 2006; Silver & Veness, 2010) solves problems systematically sampling expectations, notably pomcp algorithm Silver Veness (2010).tiny fraction future tree actually explored, chosen sampled actions according likely worth; sampling transitions dynamics. Additionally,bamcp also solves requirement integrating models: sampling modelsbelief distribution; root node, avoid need computeposteriors throughout tree; lazily avoiding realizing random choiceslast possible moment.result efficient algorithm outperforms previous Bayesian model-basedreinforcement learning algorithms significant margin several well-known benchmarkproblems, scale problems infinite state space complex priorstructure.876fiBayes-Adaptive Monte-Carlo PlanningAcknowledgmentsacknowledge support project Gatsby Charitable Foundation (AG, PD),Natural Sciences Engineering Research Council Canada (AG), Royal Society(DS), European Communitys Seventh Framework Programme (FP7/20072013) grant agreement n 270327 (DS).Appendix A: List AcronymsBAMCPBAMDPBA-UCTBEBBEETLEBFS3BOLTBOSSFSSSIEQLMCMCMCTSMDPPACPOMCPPOMDPRLSBOSSUCB1UCTBayes-Adaptive Monte-Carlo Planner (Algorithm name)Bayes-Adaptive Markov Decision ProcessBayes-Adaptive UCT (Algorithm name)Bayesian Exploration Bonus (Algorithm name)Bayesian Exploration Exploitation Tradeoff LEarning (Algorithm name)Bayesian Forward Search Sparse Sampling (Algorithm name)Bayesian Optimistic Local Transitions (Algorithm name)Best Sampled Set (Algorithm name)Forward Search Sparse Sampling (Algorithm name)Interval Estimation Q-learning (Algorithm name)Monte-Carlo Markov ChainMonte-Carlo Tree Search (Algorithm name)Markov Decision ProcessProbably Approximately CorrectPartially-Observable Monte-Carlo Planner (Algorithm name)Partially Observable Markov Decision ProcessReinforcement LearningSmarter Best Sampled Set (Algorithm name)Upper Confidence Bound 1 (Algorithm name)Upper Confidence bounds applied Trees (Algorithm name)Appendix B: Algorithms Implementationalgorithms implemented C++ (code components shared across algorithms much possible):BAMCP - algorithm presented Section 3, implemented root sampling,lazy sampling, rollout learning. algorithm run different numbersimulations (10 10000) span different planning times. experiments,set ro -greedy policy = 0.5. uct exploration constantleft unchanged experiments (c = 3), experimented valuesc {0.5, 1, 5} similar results.SBOSS (Castro & Precup, 2010): domain, varied number samplesK {2, 4, 8, 16, 32} resampling threshold parameter {3, 5, 7}.BEB (Kolter & Ng, 2009): domain, varied bonus parameter{0.5, 1, 1.5, 2, 2.5, 3, 5, 10, 15, 20}.877fiGuez, Silver, & DayanBFS3 (Asmuth & Littman, 2011) domain, varied branching factorC {2, 5, 10, 15} number simulations (10 2000). depth searchset 15 domains except larger grid maze domainset 50. also tuned Vmax parameter domain Vmin always set0.Code paper found online first authors website, directlyfollowing GitHub link https://github.com/acguez/bamcp.Appendix C: Inference Details Infinite 2D Grid Taskconstruct Markov Chain using Metropolis-Hastings algorithm sampleposterior distribution row column parameters given observed transitions, followingnotation introduced Section 4.2. Let = {(i, j)} set observed rewardlocations, associated observed reward rij {0, 1}. proposal distributionchooses row-column pair (ip , jp ) uniformly random,Pand samples pip Beta(1 +m1 , 1 + n1 ) qjp Beta(2 + m2 , 2 + n2 ), m1 = (i,j)OP 1i=ip rij (i.e., sum2rewards observed column) n1 = (1 /2(2 + 2 )) (i,j)O 1i=ip (1 rij ),similarly m2 , n2 (mutatis mutandis). n1 term proposed column parameterpi rough correction term, based prior mean failure row parameters,account observed 0 rewards column due potentially low row parameters.Since proposal biased respect true conditional distribution (fromcannot sample), also prevent proposal distribution getting peaked. Betterproposals (e.g., taking account sampled row parameters) could devised,would likely introduce additional computational cost proposal generatedlarge enough acceptance probabilities (generally 0.5 experiments).parameters pi , qj j present kept last accepted samples(i.e., pi = pi qj = pj js), parameters pi , qj linkedobservations (lazily) resampled prior influence acceptanceprobability. denote Q(p, q p, q) probability proposing set parametersp q last accepted sample column/row parameters p q. acceptanceprobability computed = min(1, A0 ) where:P (p, q |h)Q(p, q p, q)P (p, q |h)Q(p, q p, q)P (p, q)Q(p, q p, q)P (h| p, q)=P (p, q)Q(p, q p, q)P (h| p, q)Qn1 m2n2rij1rij1pm(i,j)O 1[i = ip j = jp ](pi qj ) (1 pi qj )ip (1 pip ) qjp (1 qjp )Q= m12rij1rij .n2pip (1 pip )n1 qm(i,j)O 1[i = ip j = jp ](pi qj ) (1 pi qj )jp (1 qjp )A0 =last accepted sampled employed whenever sample rejected. Finally, reward valuesRij resampled lazily based last accepted sample parameters pi , qj ,observed already. omit implicit deterministic mapping obtaindynamics P parameters.878fiBayes-Adaptive Monte-Carlo PlanningAppendix D: Existence Bayes-Optimal Policydescribed Definition 1, Martin (1967) proves following statement mdpsfinite state spaces.Theorem 3 (Martin, 1967, Thm. 3.2.1) Let v(s, h, ) expected discounted returnmdp (with |S| |A| finite) process starts augmented state hs, hiEE policy used. Letv (s, h) = sup v(s, h, ).(42)policy v (s, h) = v(s, h, ).proof Theorem 3 consists proving set EE policies mappedcompact subset real line, mapping function v(s, h, )continuous set. proof requires ordering histories reliesfiniteness state space. Let N = |S|, history ordering employed Martin(1967) is:t0Xst N t+1 ,(43)z(ht0 )t=1z(h) number corresponding history h order. general |S|finite, scenarios may bound Nt number states agenttime (for example Infinite Grid scenario). scenarios considerfollowing ordering histories:0w(ht0 )Xstt=1Nk1 ,(44)k=0reduces w(h) = z(h) Nt = N t. ordering, proof Theorem 3carried Martin (1967) (with minimal modifications) provestatement general mdps - state space infinite possible statesagent grows controlled manner time.Although reassuring know Bayes-optimal policy exists additional cases, practice satisfied approximation Bayes-optimal policyexistence -Bayes-optimal policies likely guaranteed even generalscenarios.879fiGuez, Silver, & DayanReferencesAgrawal, S., & Goyal, N. (2011). Analysis Thompson sampling multi-armedbandit problem. Arxiv preprint.Araya-Lopez, M., Buffet, O., & Thomas, V. (2012). Near-optimal BRL using optimisticlocal transitions. Proceedings 29th International Conference MachineLearning.Asmuth, J., Li, L., Littman, M., Nouri, A., & Wingate, D. (2009). Bayesian samplingapproach exploration reinforcement learning. Proceedings Twenty-FifthConference Uncertainty Artificial Intelligence, pp. 1926.Asmuth, J., & Littman, M. (2011). Approaching Bayes-optimality using Monte-Carlo treesearch. Proceedings 27th Conference Uncertainty Artificial Intelligence,pp. 1926.Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis multiarmedbandit problem. Machine learning, 47 (2), 235256.Bellman, R. (1954). theory dynamic programming. Bull. Amer. Math. Soc, 60 (6),503515.Brafman, R., & Tennenholtz, M. (2003). R-max-a general polynomial time algorithmnear-optimal reinforcement learning. Journal Machine Learning Research, 3,213231.Bubeck, S., Munos, R., & Stoltz, G. (2009). Pure exploration multi-armed banditsproblems. Proceedings 20th international conference Algorithmic learningtheory, pp. 2337. Springer-Verlag.Castro, P., & Precup, D. (2010). Smarter sampling model-based Bayesian reinforcementlearning. Machine Learning Knowledge Discovery Databases, pp. 200214.Springer.Castro, P. (2007). Bayesian exploration Markov decision processes. Ph.D. thesis, McGillUniversity.Castro, P., & Precup, D. (2007). Using linear programming Bayesian explorationMarkov decision processes. Proceedings 20th International Joint ConferenceArtificial Intelligence, pp. 24372442.Coquelin, P., & Munos, R. (2007). Bandit algorithms tree search. Proceedings23rd Conference Uncertainty Artificial Intelligence, pp. 6774.Cozzolino, J., Gonzalez-Zubieta, R., & Miller, R. (1965). Markov decision processesuncertain transition probabilities. Tech. rep., 11, Operations Research Center, MIT.Davies, S., Ng, A., & Moore, A. (1998). Applying online search techniques reinforcementlearning. Proceedings National Conference Artificial Intelligence, pp.753760.Dayan, P., & Sejnowski, T. (1996). Exploration bonuses dual control. Machine Learning,25 (1), 522.880fiBayes-Adaptive Monte-Carlo PlanningDearden, R., Friedman, N., & Russell, S. (1998). Bayesian Q-learning. ProceedingsNational Conference Artificial Intelligence, pp. 761768.Doshi-Velez, F., Wingate, D., Roy, N., & Tenenbaum, J. (2010). Nonparametric bayesianpolicy priors reinforcement learning. Advances Neural Information ProcessingSystems (NIPS).Duff, M. (2003). Design optimal probe. Proceedings 20th InternationalConference Machine Learning, pp. 131138.Duff, M. (2002). Optimal Learning: Computational Procedures Bayes-Adaptive MarkovDecision Processes. Ph.D. thesis, University Massachusetts Amherst.Fonteneau, R., Busoniu, L., & Munos, R. (2013). Optimistic planning belief-augmentedMarkov decision processes. IEEE International Symposium Adaptive DynamicProgramming reinforcement Learning (ADPRL 2013).Friedman, N., & Singer, Y. (1999). Efficient Bayesian parameter estimation large discretedomains. Advances Neural Information Processing Systems (NIPS), 1 (1), 417423.Gelly, S., Kocsis, L., Schoenauer, M., Sebag, M., Silver, D., Szepesvari, C., & Teytaud, O.(2012). grand challenge computer Go: Monte Carlo tree search extensions.Communications ACM, 55 (3), 106113.Gelly, S., & Silver, D. (2007). Combining online offline knowledge UCT. Proceedings 24th International Conference Machine learning, pp. 273280.Gittins, J., Weber, R., & Glazebrook, K. (1989). Multi-armed bandit allocation indices.Wiley Online Library.Guez, A., Silver, D., & Dayan, P. (2012). Efficient Bayes-adaptive reinforcement learningusing sample-based search. Advances Neural Information Processing Systems(NIPS), pp. 10341042.Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determinationminimum cost paths. Systems Science Cybernetics, IEEE Transactions on,4 (2), 100107.Jaksch, T., Ortner, R., & Auer, P. (2010). Near-optimal regret bounds reinforcementlearning. Journal Machine Learning Research, 99, 15631600.Kearns, M., Mansour, Y., & Ng, A. (1999). sparse sampling algorithm near-optimalplanning large Markov decision processes. Proceedings 16th internationaljoint conference Artificial intelligence-Volume 2, pp. 13241331.Kocsis, L., & Szepesvari, C. (2006). Bandit based Monte-Carlo planning. MachineLearning: ECML, pp. 282293. Springer.Kolter, J., & Ng, A. (2009). Near-Bayesian exploration polynomial time. Proceedings26th Annual International Conference Machine Learning, pp. 513520.Madani, O., Hanks, S., & Condon, A. (2003). undecidability probabilistic planningrelated stochastic optimization problems. Artificial Intelligence, 147 (1), 534.Martin, J. (1967). Bayesian decision problems Markov chains. Wiley.881fiGuez, Silver, & DayanMeuleau, N., & Bourgine, P. (1999). Exploration multi-state environments: Local measures back-propagation uncertainty. Machine Learning, 35 (2), 117154.Mundhenk, M., Goldsmith, J., Lusena, C., & Allender, E. (2000). Complexity finitehorizon markov decision process problems. Journal ACM (JACM), 47 (4),681720.Neal, R. M. (1993). Probabilistic inference using markov chain monte carlo methods. Tech.rep., University Toronto.Poupart, P., Vlassis, N., Hoey, J., & Regan, K. (2006). analytic solution discreteBayesian reinforcement learning. Proceedings 23rd international conferenceMachine learning, pp. 697704. ACM.Ross, S., Pineau, J., Chaib-draa, B., & Kreitmann, P. (2011). Bayesian approachlearning planning Partially Observable Markov Decision Processes. JournalMachine Learning Research, 12, 17291770.Ross, S., Pineau, J., Paquet, S., & Chaib-Draa, B. (2008). Online planning algorithmsPOMDPs. Journal Artificial Intelligence Research, 32 (1), 663704.Ross, S. (1983). Introduction stochastic dynamic programming: Probability mathematical. Academic Press, Inc.Schmidhuber, J. (1991). Curious model-building control systems. IEEE InternationalJoint Conference Neural Networks, pp. 14581463.Silver, D., & Veness, J. (2010). Monte-Carlo planning large POMDPs. AdvancesNeural Information Processing Systems (NIPS), pp. 21642172.Silver, D., Sutton, R. S., & Muller, M. (2008). Sample-based learning searchpermanent transient memories. Proceedings 25th international conferenceMachine learning, pp. 968975. ACM.Silver, E. (1963). Markovian decision processes uncertain transition probabilitiesrewards. Tech. rep., DTIC Document.Strehl, A., Li, L., & Littman, M. (2009). Reinforcement learning finite MDPs: PACanalysis. Journal Machine Learning Research, 10, 24132444.Strens, M. (2000). Bayesian framework reinforcement learning. Proceedings17th International Conference Machine Learning, pp. 943950.Sutton, R. (1990). Integrated architectures learning, planning, reacting basedapproximating dynamic programming. Proceedings Seventh InternationalConference Machine Learning, Vol. 216, p. 224. Citeseer.Sutton, R., & Barto, A. (1998). Reinforcement learning. MIT Press.Szepesvari, C. (2010). Algorithms reinforcement learning. Synthesis Lectures Artificial Intelligence Machine Learning. Morgan & Claypool Publishers.Thompson, W. (1933). likelihood one unknown probability exceeds anotherview evidence two samples. Biometrika, 25 (3/4), 285294.Tonk, S., & Kappen, H. (2010). Optimal exploration symmetry breaking phenomenon.Tech. rep., Radboud University Nijmegen.882fiBayes-Adaptive Monte-Carlo PlanningVien, N. A., & Ertel, W. (2012). Monte carlo tree search bayesian reinforcement learning.Machine Learning Applications (ICMLA), 2012 11th International Conferenceon, Vol. 1, pp. 138143. IEEE.Walsh, T., Goschin, S., & Littman, M. (2010). Integrating sample-based planningmodel-based reinforcement learning. Proceedings 24th Conference Artificial Intelligence (AAAI).Wang, T., Lizotte, D., Bowling, M., & Schuurmans, D. (2005). Bayesian sparse samplingon-line reward optimization. Proceedings 22nd International ConferenceMachine learning, pp. 956963.Wang, Y., Won, K., Hsu, D., & Lee, W. (2012). Monte Carlo Bayesian reinforcementlearning. Proceedings 29th International Conference Machine Learning.Watkins, C. (1989). Learning delayed rewards. Ph.D. thesis, Cambridge.Wingate, D., Goodman, N., Roy, D., Kaelbling, L., & Tenenbaum, J. (2011). Bayesian policysearch policy priors. Proceedings International Joint ConferencesArtificial Intelligence.883fiJournal Articial Intelligence Research 48 (2013) 783-812Submitted 08/13; published 11/13Complexity Optimal Monotonic Planning:Bad, Good, Causal GraphCarmel DomshlakAnton Nazarenkodcarmel@ie.technion.ac.ilanton.nazarenko@gmail.comFaculty Industrial Engineering & Management,Technion - Israel Institute Technology,Haifa, IsraelAbstractalmost two decades, monotonic, delete free, relaxation onekey auxiliary tools practice domain-independent deterministic planning.particular contexts satiscing optimal planning, underlies state-of-theart heuristic functions. satiscing planning monotonic tasks polynomial-time,optimal planning monotonic tasks NP-equivalent. establish negativepositive results complexity wide fragments optimal monotonic planning,fragments dened around causal graph topology. results shedlight link complexity general optimal planning complexityoptimal planning respective monotonic relaxations.1. Introductiondomain-independent deterministic (or classical) planning, world states represented complete assignments set variables, operators allow deterministicmodications assignments, objective nd sequence operatorssequentially modies given initial assignment assignment satises certainpredened goal property. last two decades, solvers problem made spectacular advances empirical eciency, especially context state-spaceheuristic search planning techniques. eciency made possible largelyability exploit monotonic, delete-free, relaxations planning tasks (McDermott,1999; Bonet & Gener, 2001; Homann & Nebel, 2001).high level, monotonic relaxation replaces regular value switching semanticsplanning operators value accumulating semantics. is, operator switchesvalue variable v x y, relaxed version operator extendsvalue v {x} {x, y}. key point applying operators valueaccumulating semantics reduce applicability operators future. Twoproperties monotonic relaxation make especially valuable automated planning. First,deterministic planning PSPACE-complete even rather conservative propositionalformalisms, planning monotonic tasks polynomial-time (Bylander, 1994), thusexploited deriving heuristic estimates. Second, numerous problems practicalinterest, plans monotonic relaxations distant true plansproblems (Homann, 2005; Helmert & Mattmuller, 2007; Helmert & Domshlak, 2009; Bonet& Helmert, 2010). Hence, starting seminal HSP (Bonet & Gener, 2001)FF (Homann & Nebel, 2001) planning systems, exploiting, particular, explicitlyc2013AI Access Foundation. rights reserved.fiDomshlak & Nazarenkoplanning for, monotonic relaxations became important ingredient systemsdomain-independent deterministic planning. (For comprehensive survey, see, e.g., Betz &Helmert, 2009.)Ideally, planner reasoning cost plans monotonic relaxationsreason cost optimal plans monotonic relaxations. Unfortunately,regular planning monotonic tasks polynomial-time, optimal planning tasksNP-equivalent (Bylander, 1994), constant-factor approximations problemprovably hard well (Betz & Helmert, 2009). Still, admissible heuristic estimatesprinciple exploit tractable fragments optimal planning monotonic relaxations (Katz& Domshlak, 2010). However, best knowledge, substantial fragmentstractability revealed monotonic optimal planning date.Identifying signicant fragments tractability optimal monotonic planning precisely focus here. special interest establishing connections complexity optimal planning optimal planning monotonic relaxationsrespective planning tasks. interest motivated new important rolemethods combine tractable fragments regular deterministic planning monotonic relaxation heuristics (Helmert, 2004; Keyder & Gener, 2008a; Katz & Domshlak,2010; Katz, Homann, & Domshlak, 2013a, 2013b; Katz & Homann, 2013). turn,comparative perspective brought us consider planning tasks terms nite-domainrepresentations go beyond standard propositional representation formalismsSTRIPS (Fikes & Nilsson, 1971) ADL (Pednault, 1989). explanation,possibly even justication, choice analysis place here.Due close relationship rst-order propositional logics, propositional representations dominated area automated planning since early days AI research. instance, propositional PDDL language still de facto standard problemdescription language planning community (Fox & Long, 2003). However, propositional languages blur lot important structure present typical planning tasksinterest. discuss later on, nite-domain representations (FDR) go beyond propositional state variables (Backstrom & Klein, 1991; Backstrom & Nebel, 1995; Helmert,2009) allowed much deeper much discriminative analysis automated planning complexity. turn, formal developments already translatedpractical advances planning, allowing introduction eective enhancementsmonotonic relaxation heuristics (Fox & Long, 2001; Helmert, 2004; Helmert & Gener,2008; Keyder & Gener, 2008a; Cai, Homann, & Helmert, 2013; Katz et al., 2013a,2013b), abstraction heuristics (Edelkamp, 2001; Helmert, Haslum, & Homann, 2007; Katz& Domshlak, 2010), decomposition-based planning (Nissim, Brafman, & Domshlak, 2010;Nissim & Brafman, 2012), search-topology analysis (Homann, 2011), many others.Nonetheless, value accumulating semantics monotonic relaxation, FDRlonger maintains key advantage propositional representations,longer seems reason prefer explicit representation certain mutual exclusionrelationships propositions. Therefore, principle, results presentedfollows phrased, sometimes even extended, context propositionalrepresentations STRIPS. Why, then, chosen view complexityoptimal relaxed planning lens FDR? primary reason interestcomparative complexity analysis optimal planning optimal planning respective784fiThe Complexity Optimal Monotonic Planningmonotonic relaxations. Departing previously discovered fragments tractabilitysatiscing optimal planning, approach following two high-level questions:1. fragments deterministic planning, any, optimal planning hardoptimal (monotonically) relaxed planning easy?2. fragments deterministic planning, any, optimal planning easy yetoptimal (monotonically) relaxed planning hard?regular planning, best classication days worst-case timecomplexity exploits properties graphical structures induced planningtasks, together various properties FDR state variables sizedomains. Hence, discussing optimal relaxed planning FDR tasks keeps us directrelation well-explored complexity map regular deterministic planning. Moreover,show known links planning complexity graph-topologicalproperties FDR tasks even stronger case optimal relaxed planning.second reason choice recent work already revealed interestinginterplays (either complete partial) monotonic relaxation nite-domain variables graphical structures induced FDR tasks. respective resultscontext computational complexity non-optimal planning (Katz et al., 2013b), heuristic estimates (Keyder & Gener, 2008b; Katz et al., 2013a; Katz & Homann, 2013),search-topology analysis (Homann, 2011). instance, Homann (2011) showed that,causal graph FDR task acyclic, every variable transition invertible,h+ heuristic induced optimal relaxed plans evaluated states localminima. result particular testies examining monotonic relaxationslens FDR lead crisp concisely formulated results.found work here: show results easily reformulated, sometimes even generalized, terms STRIPS, resultsconform easily STRIPS reformulation.Finally, immediate value results work mostly theoretical,would like see step towards exploiting optimal relaxed planning deviseheuristic functions deterministic planning. results, Theorem 4,fact directly used within framework implicit abstractions (Katz & Domshlak,2010), results possibly exploited within various frameworkspartial monotonic relaxation as, e.g., recent red-black planning framework Katzet al. (2013b, 2013a). extent actually happen remains, course,seen. However, focus implicit abstractions red-black planningnite-domain task representations make much easier assess relevanceresults frameworks.2. Formalism, Background, Related Resultsuse notation [n] refer set {1, . . . , n}. directed graphs, edge xdenoted (x, y), undirected graphs, edge x denoted{x, y}. ||x|| refer representation size object x, confused |x|,denotes number elements set x.785fiDomshlak & Nazarenko2.1 FDR, MFDR, Monotonic Relaxationadopt terminology notation Katz et al. (2013b). planning tasknite-domain representation (FDR) given quintuple = V, A, I, G, cost,where:V set state variables, v V associated nite domainD(v). partial variable assignment p function variable subset V(p) Vassigns v V(p) value p[v] D(v) domain. partial variable assignmentcalled state V(s) = V .initial state. goal G partial variable assignment V .nite set actions. action pair pre(a), e(a) partial variableassignments V called precondition eect, respectively.cost : R0+ real-valued, nonnegative action cost function.Auxiliary notation:partial assignment p variable subset V V(p), p[V ] denoteassignment provided p V . ease presentation, sometimes also specifypartial assignments sets constructs v d, v V D(v).variable v V , Av denote actions aecting value v,is, Av = {a | v V(e(a))}. sequence actions state variable v V ,v denote restriction actions Av .semantics FDR tasks follows. action applicable states[v] = pre(a)[v] v V(pre(a)). Applying state changes value every vV(e(a)) e(a)[v]; resulting state denoted sJaK. sJa1 , . . . , ak K denotestate obtained sequential application (respectively applicable) actions a1 , . . . , akstarting state s. action sequence s-plan sJa1 , . . . , ak K[V(G)] = G,optimal s-plan sum action costs minimal among s-plans.computational task (optimal) planning nding (optimal) I-plan. follows,(optimal) I-plans often referred simply (optimal) plans .monotonic nite-domain representation (MFDR) planning task givenquintuple = V, A, I, G, cost exactly FDR tasks, semantics dierent.1Informally, MFDR tasks state variables accumulate values, rather switchingthem. specically, MFDR state function assigns v Vnon-empty subset s[v] D(v) domain. MFDR action applicable statepre(a)[v] s[v] v V(pre(a)). Applying MFDR action state changesvalue v V(e(a)) s[v] s[v] {e(a)[v]}. Respectively, MFDR action sequencea1 , . . . , ak applicable state s-plan G[v] sJa1 , . . . , ak K[v] v V(G).respects, MFDR FDR semantics identical.1. entirely clear original formulation monotonic relaxation multi-valued variabledomains attributed, traced back least work Helmert (2006)Fast Downward planning system.786fiThe Complexity Optimal Monotonic PlanningFDR planning PSPACE-complete even propositional state variables, planning MFDR tasks polynomial time (Bylander, 1994). Starting HSP (Bonet& Gener, 2001) FF (Homann & Nebel, 2001) planning systems, exploiting attractive property MFDR deriving heuristic estimates via notion monotonicrelaxation became key ingredient many planning systems. Given FDR planning task= V, A, I, G, monotonic relaxation MFDR task + = .state , optimal relaxation heuristic h+ (s) dened cost optimalplan MFDR task V, A, s, G, (optimal) relaxed planning refer(optimal) planning + . + plan + , + referred relaxedplan .Finally, FDR MFDR, sometimes distinguish planning tasksterms pair standard graphical structures induced description tasks.causal graph CG digraph nodes V . arc (v, v ) CG v =v exists action (v, v ) V(e(a))V(pre(a))V(e(a)).case, say (v, v ) induced a. succ(v) pred(v) respectivelydenote sets immediate successors predecessors v CG .domain transition graph DTG(v, ) variable v V arc-labeleddigraph nodes D(v) arc (d, ) labeled pre(a)[V \ {v}]cost(a) belongs graph e(a)[v] = , either pre(a)[v] =v V(pre(a)).2.2 Causal Graph Treewidth Planning ComplexityIntroduced Halin (1976), tree-width went unnoticed independently rediscovered Robertson Seymour (1984) Arnborg, Cornell, Proskurowski (1987).since received widespread attention due numerous graph-theoretic algorithmicapplications. Informally, tree-width graph measure close structuregraph tree. example, tree-width tree 1, regardless size,whereas tree-width complete graph n nodes n 1. Formally, tree-widthgraph dened via notion tree decomposition follows.tree decomposition connected undirected graph G = (V, E) pair T, ,= (V , E ) tree, i.e., connected acyclic graph, : V 7 2V that:1. every v V , set {t V | v (t)} non-empty connected.2. every (v, u) E V {v, u} (t).width tree decomposition T, G max{|(t)| | V } 1, treewidth G, tw(G), minimum width tree decompositions G. Followingappears standard terminology, tree-width digraph G refertree-width undirected graph induced G (Berwanger, Dawar, Hunter, Kreutzer, &Obdzralek, 2012).development parametrized complexity analysis (Downey & Fellows, 1999;Flum & Grohe, 2006), shown many NP-hard problems solvedpolynomial time restricted induce certain problem-specic graphical structuresxed tree-width. particular, constraint satisfaction constraint optimization787fiDomshlak & Nazarenkoproblems nite-domain variables solved time polynomial sizeexplicit description problems, exponential tree-width inducedconstraint graph (Dechter, 2003). Since causal graph captures high-level structureplanning problems, one would expect tree-width play similar roleworst-case time complexity satiscing optimal FDR planning. Unfortunately,results direction mostly negative.standard assumption parametric complexity hierarchy W[1] nu-FPT(Flum & Grohe, 2006), Chen Gimenez (2010) proved that, familydigraphs C, FDR planning tasks inducing causal graphs C polynomial-timesize connected components C bounded constant.family digraphs tree-width 1 trivially fails satisfy latter condition,immediate corollary result even satiscing FDR planning restrictedcausal graphs tree-width 1 polynomial.construction proof Chen Gimenez (2010) uses FDR tasksvariable domains parametric size, work Gimenez Jonsson (2009b)shows negative result causal graphs tree-width 1 holds evenrestricted planning tasks xed variable domains. Specically, GimenezJonsson show FDR planning chain causal graphs NP-hard evenrestricted variables domains size 5.negative results role causal graphs tree-width computationaltractability FDR planning strong, apparently tell part story.shown Brafman Domshlak (2006, 2013), causal graphs tree-width playrole worst-case time complexity FDR planning, tight interplayanother parameter called tasks local depth. Informally, local depth FDR taskcaptures minmax amount work required single variable order solve . Sincelater refer result Brafman Domshlak, precise specication local depthwarranted here: denoting P lans() (possibly innite) set plans FDRtask , local depth=minmax {|v |},P lans() vVis, maximal number value changes single state variable, along planminimizes quantity among plans . Theorem 6 Brafman Domshlak(2013), FDR tasks solved time polynomial |||| exponentialO(tw(CG ) ). possible stratications based succinct representationinternal variable domain dynamics, suggested Fabre, Jezequel, Haslum,Thiebaux (2010), appears strongest link discovered farcomplexity general FDR planning graph-topological properties causalgraphs. Note also positive result applies satiscing planning; applicableoptimal planning limited settings (Fabre et al., 2010; Brafman & Domshlak,2013).788fiThe Complexity Optimal Monotonic Planning3. Negative Results: Bottleneck Variable Domainsfocus connections worst-case time complexity optimal relaxedplanning structure problems causal graphs. Note causal graphsFDR tasks trivially invariant monotonic relaxation: since + = ,CG+ = CG . mentioned, previous works already revealed certain connections structure causal graphs, particular tree-width,complexity FDR planning. follows, show link even strongersomewhat intriguing case optimal relaxed planning. said that,begin set negative results which, least rst glance, suggestlink actually likely.Denition 1 connected digraph G = (N, E) fork contains exactly one noder N non-zero out-degree, is, E = {(r, n) | n N \ {r}}. Similarly, Ginverted fork contains exactly one node r N non-zero in-degree, is,E = {(n, r) | n N \ {r}}. respective special nodes r fork inverted-fork graphscalled roots graphs.Considering FDR planning tasks fork inverted fork causal graphs, rstimpression might FDR fragments restricted enough allow polynomialtime planning. This, however, case: even non-optimal planning FDR taskssimple causal graphs hard, even variables rootsrestricted binary-valued (Domshlak & Dinitz, 2001). hand, especiallysince non-optimal relaxed planning FDR polynomial-time, results directinuence complexity optimal relaxed planning respective FDR fragments.Nonetheless, surprisingly not, problem hard.Theorem 1 Optimal relaxed planning NP-equivalent even restricted FDR tasksfork inverted-fork structured causal graphs. Moreover, result holds evenstate variables roots restricted binary domains.Proof: proof polynomial reductions NP-equivalent problems (minimum) Directed Steiner Tree (minimum) Set Cover (Karp, 1972).Directed Steiner Tree: Given digraph G = (N, E) arc weights w : E R0+ ,set terminals Z N , root vertex nr , nd minimum weight arborescence(directed tree) rooted nr N terminals Z included .Set Cover: Given collection C subsets nite set S, nd minimum cardinalitysubset C C every element belongs least one member C .Fragment I: Given Directed Steiner Tree problem G = (N, E), w, Z, nr , corresponding fork-structured FDR task = V, A, I, G, cost constructed follows.variable set V contains variable per terminal node G, plus extra variable r,is, V = {r} VZ VZ = {vz | z Z}. domain r, D(r) = N , correspondsnodes G, variables binary-valued, D(vz ) = {0, 1}.initial state, I[r] = nr I[v] = 0 v VZ . goal achieve value 1v VZ . arc e = (x, y) E, action set contains root-changing action789fiDomshlak & Nazarenkoae pre(ae ) = {r x}, e(ae ) = {r y}, cost(ae ) = w(e). Likewise,terminal z Z, contains vz -changing action az pre(az ) = {r z, vz 0},e(az ) = {vz 1}, cost(az ) = 0. construction clearly polynomial, causalgraph forms fork rooted r, variable domains requiredtheorem. also holds that:(i) relaxed plan , set arcs {e | ae r } G induces connectedsub-graph G containing nr terminals Z included G (or otherwiseleast one leaf variables could changed goal value).Likewise, directed path G nr every node n G (orotherwise respective value n root variable r could achievedalong ). Hence, particular, G contains arborescence rooted nr includesterminals Z.(ii) Vice versa, let arborescence G rooted nr includes terminalsZ = {z1 , . . . , zm }, let {e1 , . . . , ek } topological ordering arcs .ae1 , . . . , aek , az1 , . . . , azm relaxed plan , cost preciselyweight .Hence, optimal relaxed plans induce minimum directed Steiner trees G = (N, E), w, Z, nr ,vice versa.Fragment II: Given Set Cover problem S, C = {1, 2, . . . , m} |C| = n,corresponding inverted-fork structured FDR task = V, A, I, G, cost constructedfollows. variable set V contains variable per member C, plus extra variable r,is, V = {r}{vc | c C}. domain r D(r) = {0}S, variablesbinary-valued, D(vc ) = {0, 1}. initial state, I[v] = 0 v V . goalachieve value special variable r. S, subset c C c,action set contains root-changing action ai;c pre(ai;c ) = {r (i 1), vc 1},e(ai;c ) = {r i}, cost(ai;c ) = 0. Likewise, c C, contains vc -changingaction ac pre(ac ) = {vc 0}, e(ac ) = {vc 1}, cost(ac ) = 1. constructionpolynomial, causal graph forms inverted fork rooted r, variabledomains required theorem. Note that, due chain-like structuredomain transition graphs , dierence plans+ , easy verify plan induces cover cost,vice versa. Hence, optimal relaxed plans induce minimum set covers S, C,vice versa.Corollary 1 Optimal relaxed planning NP-equivalent even restricted FDR taskscausal-graph tree-width 1.corollary immediate Theorem 1 undirected graphs inducedforks inverted forks special case trees thus tree-width 1. rstglance, message Corollary 1 discouraging respect agenda: structurecausal graphs seem play major role complexity optimalrelaxed planning FDR. next result, however, seems even discouragingrespect prospects tractability optimal relaxed planning FDR.790fiThe Complexity Optimal Monotonic PlanningTheorem 2 Optimal relaxed planning NP-equivalent even restricted FDR taskstwo state variables.Proof: proof polynomial reduction (minimum) directed Steiner treeproblem, fact, proof similar fork case Theorem 1. GivenDirected Steiner Tree problem G = (N, E), w, Z, nr Z = {z1 , . . . , zm }, compileFDR task = {v1 , v2 }, A, I, G, cost follows. domain v1 correspondsnodes G, domain v2 corresponds terminal nodes rootnode, is, D(v1 ) = N D(v2 ) = Z {nr }. initial state, I[v1 ] = nrI[v2 ] = nr , goal achieve value zm v2 . arc e = (x, y) E,action set contains v1 -changing action ae pre(ae ) = {v1 x}, e(ae ) = {v1 y},cost(ae ) = w(e). Likewise, denoting nr z0 , 1 m, contains v2 -changingaction azi pre(azi ) = {v1 zi , v2 zi1 }, e(azi ) = {v2 zi }, cost(azi ) = 0.construction polynomial, correctness stems analysis identicalproof Theorem 1.Theorem 2 shows even dimensionality FDR state spaces plays secondaryrole, any, complexity optimal relaxed planning. that, however, answersone two macro-questions agenda:Corollary 2 exist fragments FDR optimal planning polynomial-time,optimal relaxed planning NP-equivalent.corollary immediate Theorem 2 polynomial-time solvability optimalplanning FDR tasks xed number state variables.4. Positive Results I: State Variables Fixed-Size DomainsDepending readers background intuitions, Theorem 2 either surprise seemsomewhat predictable. case, Theorem 2 led us consider dierent(and time fruitful) fragmentation optimal relaxed planning.closer look Theorem 1 Lemma 2 reveals size FDR variabledomains might crucial complexity optimal relaxed planning: proofstwo claims rely heavily parametric domain size state variables,proofs also imply optimal relaxed planning hard even single FDRvariable comes parametric domain size. Departing observation,show that, all, topology causal graph play interesting role worstcase time complexity classication optimal relaxed planning FDR tasks. particular,turns bounding tree-width causal graph constant takesachieve polynomial-time optimal relaxed planning FDR tasks xed-size variabledomains.Theorem 3 family directed graphs C, tree-width C boundedconstant, optimal relaxed planning FDR tasks xed-size variable domainscausal graphs C polynomial-time.791fiDomshlak & NazarenkoProof: proof Theorem 3 inspired closely resembles approach BrafmanDomshlak (2013) discussed Section 2.2. Given FDR task = V, A, I, G, cost,compile constraint optimization problem COP+ = (X , ) nite-domainvariables X , functions , global objective minimize (X ). + unsolvable, assignments X evaluate objective function . Otherwise,optimum objective obtained assignments X correspondoptimal plans + , is, optimal relaxed plans .Let |V | = n, = maxvV |D(v)|, recall pred(v) state variable v V denotesset vs immediate predecessors causal graph.state variable v V , X contains variable xv represents choicesubset actions Av participate plan looking for. possiblechoices form domain D(xv ) xv , choice represented setsize smaller equal , element set quadruple(d, id, a, t),D(v), id {v} pred(v), {1, . . . , n}, and, id = v, {aAv | e(a )[v] = d} otherwise =. high level, view state variablesactive decision makers, value relaxed variable v aims achieveaccumulate time point t, either using action a, delegatingtask another variable id. show later on, variable accumulatevalues, since accumulated values never lost, optimal plans +cannot longer n actions.state variable v V , contains non-negative, extended real-valued function v D(xv ). Likewise, pair state variables {v, w}causal graph CG contains either arc (v, w) arc (w, v), contains indicatorfunction v,w : D(xv ) D(xw ) {0, }. simplify specication v,w ,dene set auxiliary constraints follows.(S1) [Precondition Constraint] assignment v1 , . . . , vn X satises S1 i,v V , (d, v, a, t) v implies that, w V(pre(a)),pre(a)[w] {I[w]} {d | (d , , , ) w , < t}.(1)(S2) [Delegation Constraint] assignment v1 , . . . , vn X satises S2 i, v V ,(d, w, , t) v implies that, Av Aw e(a)[v], (, w, a, t) w .(S3) [Goal Achievement Constraint] assignment v1 , . . . , vn X satises S3 i,v V(G),G[v] {I[v]} {d | (d, , , ) v }.(2)Constraint S1 ensures preconditions actions variable committedprovided time. Constraint S2 ensures outsourced value achievementsfullled required time points. Finally, constraint S3 simply veries valuev induced v goal value. Importantly, S3 corresponds set n unary constraints,792fiThe Complexity Optimal Monotonic PlanningS1 S2 represented set binary constraints X . Given that,functions speciedv (v ) =cost(a),(,v,a,)v{0, {v , w } satises S1(xv , xw ), S2(xv , xw ), S3(xv ) S3(xw )v,w (v , w ) =,, otherwise(3)S1(xv , xw ), S2(xv , xw ), S3(xv ) correspond binary unary constraintsinduced respectively S1, S2, S3 COP variables xv xw .Let us take closer look COP+ constructed problemsscope Theorem 3.(1) constraint network COP+ corresponds undirected graph inducedcausal graph CG+ (= CG ). Hence, since tree-width latter boundedconstant scope theorem, tree-width constraint networkCOP+ . nding optimal tree decomposition graph G NP-hard,tree decomposition G width c tw(G) low constant c foundtime polynomial size G (Robertson & Seymour, 1991; Becker & Geiger,1996; Amir, 2010). Hence, COP+ solved time polynomial sizerepresentation using standard message-passing algorithm constraint optimizationtrees (Dechter, 2003).(2) Recall values COP variable xv sets quadruples (d, id, a, t) size. Then, size xv domain D(xv ) upper-bounded(|D(v)| |{z}n (|Av | + 1) (n)) ,| {z }| {z } | {z }id(4)since = O(1), |D(xv )| = O(poly(||||)). Together (1), impliesCOP+ solved time O(poly(||||)).prove correctness compilation showing that, + unsolvable,assignments X evaluate objective function (X ) , otherwise, objective minimized assignments X correspondoptimal plans + .First, given assignment = v1 , . . . , vn X () < , showinduces valid plan + cost (). Note that, since () < ,Eq. 3 satisfying constraintsS1-S3.Consider multi-set Z = vV {(a, t) | (, v, a, t) v } induced , letZ = {(a1 , t1 ), . . . , (am , tm )}, = |Z|,arbitrary ordering Z that, 1 j < m, holds tj ti .1 m, let v variable charge performing action ai , is,(e(ai )[v], v, ai , ti ) v . w V(pre(ai )), Eq. 1 constraint S1 implies eitherpre(ai )[w] = I[w] (pre(ai )[w], id, a, t) w < ti . latter case,793fiDomshlak & Nazarenkoid = w, construction , = aj j < i, denitionD(xw ), pre(ai )[w] = e(aj )[w]. Otherwise, id = w w = w,denition S2, pre(ai )[w] = e(a )[w] (, w , , t) w . Thus, = ajj < i, pre(ai )[w] = e(aj )[w]. Therefore, action sequence= a1 , . . . ,applicable initial state + , given that, satisfying S3 impliesplan + . Finally, immediate construction Eq. 3cost( ) = ().show optimal plan = a1 , . . . , + induces assignment= v1 , . . . , vn X ( ) = cost(). denition MFDR,1 state variable v V , IJa1 , . . . , ai1 K[v] IJa1 , . . . , ai K[v].optimality , 1 m, exists least one variable v VIJa1 , . . . , ai1 K[v] IJa1 , . . . , ai K[v]. particular, implies actions {a1 , . . . , }dierent, variable v V ,/v = {ai | IJa1 , . . . , ai1 K[v] IJa1 , . . . , ai K[v]},then2 |/v| .Adopting arbitrary ordering {v1 , . . . , vn } state variables V , 1 j n, let/vj = {aj1 , . . . , ajmj }. turn, 1 l mj , let djl D(vj ) value achievedaccumulated vj action ajl , i.e., IJa1 , . . . , ajl K[vj ] \ IJa1 , . . . , ajl 1 K[vj ] = {djl }.1 l mj , ajl j1k=1 /vk , vj set contain (djl , vj , ajl , jl ), otherwise, vjset contain (djl , vk , , jl ) k = min {k | ajl /vk }.construction , action present value exactly onevariable xv , Eq. 3, v (v ) sums cost exactly once. Hence, satisesconstraints S1-S3 enforced step functions v,w , ( ) =cost(). former also directly follows construction . 1 j n,mjlet vj = {(djl , idjl , ajl , tjl )}l=1. denition values djl above, setvalues {dj1 , . . . , djmj } exactly set values vj gets accumulated relaxedplan I, thus satisfaction S3 follows plan + . Again,construction , sequence time points {tj1 , . . . , tjmj } corresponds timepoints rst achievements {dj1 , . . . , djmj }, respectively, along ,rst achiever ajl along , captured properly scheduled either vi neighborvi causal graph. Hence, constraints S1 S2 satised well.nalizes proof compilation correctness, thus Theorem 3.Note Theorem 3 particular answers second macro-question agenda:Corollary 3 exist fragments FDR (even satiscing) planning NPequivalent, optimal relaxed planning polynomial-time.2. correspondswell-known fact that, notation, optimal plans MFDR cannotlonger vV |D(v)| n.794fiThe Complexity Optimal Monotonic Planningcorollary immediate Theorem 3 discovery Gimenez Jonsson(2009b) FDR planning chain causal graphs NP-hard even restricted variablesdomains size 5.Following STRIPS vs. FDR discussion introduction, Theorem 3 triviallyimplies optimal relaxed planning STRIPS tasks done time polynomial|||| exponential tree-width causal graph. actuallyexample tractability fragment one benet switchingpropositional representation: formulation result remains same,coverage result grows because, size FDR variable domains bounded= O(1), tree-width causal graph STRIPS representationtimes larger FDR. However, also smaller, identical.Later, however, present results directly benet nite-domain inputrepresentation planning tasks.discussion remainder section addresses readers familiarwork Brafman Domshlak (2013) detail. discussion skipped withoutloss continuity.rst view, compilation proof Theorem 3 brings mind compilation algorithmv1v2vnbehind proof Theorem 6 Brafman Domshlak (2013). One might thus naturally ask whetheralgorithm cannot used directly proofTheorem 3. stands, however, algorithmvn+1Brafman Domshlak yield polynomialtime complexity tasks Theorem 3concerned. see why, consider FDR task = V, A, I, G, cost V = {v1 , . . . , vn+1 },where, vi , D(vi ) = {0, 1}, I[vi ] = 0, G[vi ] = 1, = {a1 , . . . , }V(pre(ai )) = , V(e(ai )) = {vi , vn+1 }, e(ai )[vi ] = e(ai )[vn+1 ] = 1. causal graph, depicted above, tree-width 1. However, plan + , maxvV {|v |}n, local depth + n. + cannot solved without applyingn actions least once, actions aects value variablevn+1 . Hence, nding even non-optimal plan + using algorithm BrafmanDomshlak (2013) take time exponential ||||.nal note, actually shown algorithm Brafman Domshlak(2013) optimal polynomial-time sub-class MFDR tasks like Theorem 3also restricted single-eect operators. multiple-eect actions complicatematters, require dierent algorithmic approach guarantee planning tractability.5. Positive Results II: M-unfoldable State VariablesDespite discouraging results Section 3, return consider FDR tasksparametric-size domains. Recall that, variable values monotonic relaxation +correspond sets values respective variables , large variable domains+ given implicitly, via variable domains D(v1 ), . . . , D(vn ) . concisenessrepresentation, however, hides many aspects problem structure + otherwisemight exploited planning eciency. particular, reasoning domain795fiDomshlak & Nazarenkotransition graphs induced FDR tasks successfully exploited complexityanalysis FDR (Jonsson & Backstrom, 1998; Domshlak & Dinitz, 2001; Katz & Domshlak,2008; Gimenez & Jonsson, 2009a). contrast, monotonic relaxations, true domaintransition graphs + , denoted henceforth DTG(v, + ), cannot always representedexplicitly number nodes graphs exponential ||||. However,always, always be, case, focus planningaccessible monotonic variables.Denition 2 Given FDR planning task = V, A, I, G, cost, eective domain(v) v V + consists value subsets D+ (v) = 2D(v) \ reachable{I[v]} DTG(v, + ). is, (v)(i) I[v] ,(ii) value , directed path I[v] unlabeled digraphinduced DTG(v, ) values along path belong .elements (v) called eective values v + .Denition 3 Let innite set FDR tasks, property state variablesthat, task , partitions state variables satisfy(referred -variables), satisfy . say -variablesmonotonically unfoldable (M-unfoldable) exists integer k N that,every task every -variable v , |D (v)| = O(||||k ).Denition 3 property FDR state variables, particular,property dened respect tasks causal graphs, root, sink, nodeswhose causal graph in-degrees larger causal graph out-degrees, etc. Informally, -variables set FDR tasks M-unfoldable if, every taskevery -variable v , relevant subgraph DTG(v, + ) described explicitlyspace polynomial representation size . instance, v trivially M-unfoldablesize domain bounded constant, even O(log(||||)). generally, let DTG (v, ) digraph obtained (labels ignored) domain transitiongraph DTG(v, ) unifying parallel edges. hard verify Denitions 2 3v M-unfoldable number arborescence subgraphs DTG (v, )rooted I[v], covering G[v] v V(G), O(poly(||||)).return consider fork-structured FDR tasks. Theorem 1 consideredfork-structured FDR tasks root variables unrestricted, considerinverse fragment, corresponding fork-structured FDR tasks root variablesrestricted. Optimal FDR planning tasks polynomial-time |D(r)| =2 (Katz & Domshlak, 2010), NP-equivalent |D(r)| > 2 (Katz & Keyder, 2012).contrast, Theorem 4 shows optimal relaxed planning fork-structured FDRtasks polynomial-time much wider class root variables. Moreover, simpleobservation behind construction proof Theorem 4 later generalized capturemuch richer fragment causal graphs.Theorem 4 Optimal relaxed planning polynomial time set FDR tasksfork-structured causal graphs M-unfoldable root variables.796fiThe Complexity Optimal Monotonic PlanningProof: Let = V, A, I, G, cost fork-structured FDR task root r Vleafs =V \ {r} = {v1 , . . . , vn }. assume goal values specied variables Vleafs ;none leaves Vleafs \ V(G) need change value all, thusschematically removed problem. Given FDR task , optimalplan relaxation + constructed follows.eective values (r) consistent goal (that is, G[r]r V(G)) processed one one, independently. eective value ,extract following information.(1) root variable r, determine cheapest path I[r] DTG(r, + ),() denoting action sequence inducing path.(2) leaf variable v Vleafs ,(a) schematically remove domain transition graph DTG(v, )arcs labeled actions supported , is, actionsr V(pre(a)) pre(a)[r] ,(b) determine cheapest path I[v] G[v] arc-reduced domaintransition graph, (G[v]) denoting action sequence inducingpath.Return concatenation action sequences ( ) (G[v1 ]) . . . (G[vn ])[]ncost( (G[vi ])) ,= argmin cost(()) +(r)i=1cost() action sequence sum costs actions along .algorithm polynomial-time given explicit description eective partDTG(r, + ), thus polynomial-time r M-unfoldable. Recall that, sincecausal graph acyclic, action aects one variable. correctnessalgorithm stems simple observation that, relaxed plan fork-structuredFDR task , = r v1 . . . vn also relaxed plan , (trivially) identicalcost. Hence, searching optimal relaxed plan , restrictplans latter form, immediate description algorithmnds cheapest plan.aside, following STRIPS vs. FDR discussion, note Theorem 4 providesexample exploiting value grouping induced FDR representation planning tasks. simple algorithm proof exploits fact actionschange value leaf variable depend value, prevents restrictionsplaced either size leaf domains, structure domaintransition graphs. course, Theorem 4 also reformulated terms STRIPS, yetwould require respective partition propositions given/discovered,essentially equivalent starting something like FDR input rst place.scope tractability result Theorem 4 fairly limited termscausal graph structure, nice property sets optimal relaxed plans forkstructured FDR tasks, exploited proof Theorem 4, generalized much797fiDomshlak & Nazarenkowider fragment causal graphs. turn, generalization allows us provide nexttractability result wide fragment optimal relaxed planning FDR.Lemma 1 Let = V, A, I, G, cost FDR task directed acyclic causal graph,let {v1 , . . . , vn } arbitrary topological ordering V respect CG . Then,plan + , = v1 . . . vn also plan + .Proof: Directed acyclicity causal graph particular implies action aectsone variable. proof lemma stems combining property(i) fact preserves order actions vi , (ii) core propertymonotonic relaxations + that, variable v, value D(v), staterelaxed task + , s[v], [v] reachable + .Since CG forms DAG state variables ordered according topologicalordering CG , V(pre(a)[v1 ]) {v1 } actions v1 . Thus, order preservationv1 along respect implies v1 applicable I, IJv1 K[v1 ] = IJK[v1 ],IJv1 K[vi ] = I[vi ] > 1. Assume that, 1, v1 . . . vi applicableI,{IJK[vj ], jIJv1 . . . vi K[vj ] =.(5)I[vj ],j>iTogether topological ordering V order preservation vi +1 alongrespect , Eq. 5 implies vi+1 applicable IJv1 . . . vi K,IJv1{IJK[vj ],j =i+1.. . . vi KJvi+1 K[vj ] =IJv1 . . . vi K[vj ], otherwise(6)Putting Eqs. 5 6 together proves induction hypothesis, = n, Eq. 5boilsIJ K = IJv1 . . . vn K = IJK.Denition 4 Let innite set FDR tasks. say tasks Munfoldable state variables M-unfoldable.Theorem 5 Let innite set M-unfoldable FDR tasks directed acyclic causalgraphs. tree-width node in-degree causal graphs boundedconstant, optimal relaxed planning polynomial-time.Proof: Similarly proof Theorem 3, proof Theorem 5 based planningto-COP compilation. Given FDR task = V, A, I, G, cost, compile monotonicrelaxation + constraint optimization problem COP+ = (X , ) variables X ,functions , global objective minimize (X ) that, + unsolvable,assignments X evaluate objective function , otherwise,optimum objective obtained assignments X correspondoptimal plans + . specic construction COPs relies propertymonotonic relaxations DAG-structured FDR tasks expressed Lemma 1.798fiThe Complexity Optimal Monotonic PlanningGiven FDR task = V, A, I, G, cost theorem, COP+ = (X , ) speciedfollows. state variable v V ,X contains variable xv domain D(xv ) = (v), is, eective domainv + ,contains non-negative, extended real-valued function v v immediateancestors causal graph, is, {xv } {xw | w pred(v)}.Assuming arbitrary xed ordering {w1 , . . . , wk } vs immediate ancestors pred(v),pred (pred(v)) = (w1 ) (wk ), let DTG(v, + |pred ) denoterestriction DTG(v, + ) edges supported pred : edge markedaction remains DTG(v, + |pred ) if, w V(pre(a))\{v}, pre(a)[w]pred [w]. Given that, eective value (v) pred (pred(v)),v (, pred ) = reachable I[v] DTG(v, + |pred ), G[v] speciedyet G[v] . Otherwise, v (, pred ) equals cost cheapest path I[v]DTG(v, + |pred ). follows, action sequence inducing cheapest pathdenoted (|pred ).properties COP+ constructed problems scope Theorem 5 follows.(1) constraint network COP+ corresponds (the undirected graph induced by)moral graph causal graph CG . Since in-degree tree-width CGbounded constant, tree-width COP constraint network.mentioned before, given graph G, tree decomposition graph G widthc tw(G) low constant c found time polynomial size Gexponential tw(G). Hence, since COP = O(1), COP+ solved timepolynomial size explicit representation.(2) Since M-unfoldable, domain size COP variable O(poly(||||)). Together (1), implies COP+ solved time O(poly(||||)).(3) denitions monotonic relaxation domain transition graphs, explicitdescription DTG(v, + ) M-unfoldable FDR task polynomial ||||.Hence, construction functional components , thus entire COP+ ,done time O(poly(||||)).(4) construction COP+ Lemma 1, topological ordering {v1 , . . . , vn }V , every complete assignment COP variables Xv ([v], [pred(v)]) = =vVinduces relaxed plan([v1 ]|[pred(v1 )]) . . . ([vn ]|[pred(vn )])cost , vice versa. Thus, + solvable, then, given assignmentX minimization objective COP+ obtained, derive(in O(poly(||||)) time) optimal relaxed plan . Otherwise, + unsolvable,assignments X evaluate objective function .799fiDomshlak & NazarenkoONMLHIJKv1HIJKONMLv2HIJKONMLv3/ d1,2d1,1d1,1d2,1d1,2d1,3d2,2d2,3ONMLHIJKHIJKHIJK/ ONML/ ONMLd1,1 Qo Qd1,3BB QQQ | 1,2 BBmm|BB Q|Q|QB|BB|| QQQmmmmBBB||||BmmQQQQ ||BB|| BBmmmQ| B}|||mmmmm BB! }||| QQQQQBB!mv(HIJKONMLHIJKHIJK/ ONML/ ONMLd2,1 Qo Qd2,3BB QQQ | 2,2 BB|BB Q|Q|QB|BB|| QQQmmmmBBB||||BmmQQQQ ||BB|| BBmmmQ| B}|||mmmmm BB! }||| QQQQQBB!mv(HIJKONMLHIJKHIJK/ ONML/ ONMLd3,1d3,2d3,3d1,1#/ d2,2;d2,1d3,1/ d1,3d1,2d1,3#/ d2,3;d2,1#/ d3,2;d2,2d2,3#/ d3,3;(a)(b)Figure 1: Illustration example used discussion Theorem 5.nalizes proof Theorem 5, Corollary 4 generalizes digraphsalmost DAGs.Note Theorem 5 provides yet another example exploiting value grouping induced FDR representation planning tasks. Consider planning task family(n) = V, A, I, G, cost V = {v1 , . . . , vn }; 1 n, D(vi ) = {di,1 , . . . , di,n };I[vi ] = di,1 ; G[vi ] = di,n ; actionsA={ai,k,j = {vi di,k , vi1 di1,j }, {vi di,k+1 }}.|{z} |{z}1i,jn1kn1preeFigure 1a illustrates causal graph domain transition graphs task (3) .causal graphs (n) form directed chains, thus tree-width nodein-degree causal graphs (n) equal 1. Likewise, eective domain (vi )vi +(n) size n, thus tractability optimal relaxed planning (n)directly covered Theorem 5. contrast, variable value di,j representedseparate propositional variable, inducing causal graph Figure 1b,tree-width node in-degree family induced causal graphs ordern, fact, causal graph even acyclic. Therefore, Theorem 5 longerdirectly applicable.Corollary 4 Let innite set M-unfoldable FDR tasks. size stronglyconnected components, tree-width, node in-degree causal graphsbounded constant, optimal relaxed planning polynomial-time.Proof: FDR task causal graph whose strongly connected components (SCCs)size k compiled equivalent FDR task directed acycliccausal graph merging variables SCC single variable (Seipp & Helmert,2011). compilation done time polynomial |||| exponential k.causal graph CGm obtained causal graph CG contracting nodesSCC. Since node contraction decrease tree-width, tw(CGm )800fiThe Complexity Optimal Monotonic Planningtw(CG ), thus tw(CGm ) = O(1). Likewise, maximal node in-degree CGc, maximal node in-degree CGm ck, thus also O(1). Finally,domain variable u obtained merging SCC {v1u , . . . , vku },k k, corresponds cross-product domains SCCs variables.easy verify Denition 2 (u) (v1u ) (vku ), thus, togetherk = O(1), M-unfoldability v1u , . . . , vku implies |D (u)| = O(poly(||||)), tting3Denition 3.Returning statement Theorem 5, comments extensions beyondCorollary 4 place. First, note Theorem 5 generalize Theorem 4fork-structured FDR tasks latter allows general, M-unfoldable,leaf variables. However, easy see Theorem 5 stratied allowgeneralization. Since variable depends leaf v, care vachieving G[v]. Thus, optimal relaxed plan , v induces simple path,general arborescence, DTG(v, ). Hence, using binary-valued (G[v] achieved: yes/no)COP variables xv DAG leaf v, specifying respective functions v usingprocedure proof Theorem 4, scope Theorem 5 extended generalizeTheorem 4.Second, case DAG-structured causal graphs, Denition 2 eective domains,notion M-unfoldability based, overly conservative. Instead derivingeective domains variables isolation, derive topological ordercausal graph, given already derived eective domains immediate ancestors.specic domains, substantially extend scope M-unfoldability FDR tasksDAG causal graphs.Finally, Theorem 5 requires tree-width, also in-degree causalgraph bounded constant. extra condition, latter sucient,necessary. Below, notion prevail decomposability, list two localproperties state variables guarantee polynomial-time optimal relaxed planningarbitrary acyclic causal graphs xed tree-width. likelyhelpful properties exist, thus boundaries prevail decomposabilityextended. Nicely, optimal relaxed planning remain polynomial-time even dierentstate variables satisfy dierent properties, even state variablesprevail decomposable, xed in-degree.Denition 5 Let innite set FDR tasks, property state variablesthat, task , partitions state variables satisfy(referred -variables), satisfy . say -variablesprevail decomposable if, every task every -variable v , either(i) set PRv = {pre(a)[pred(v)] | Av } preconditions actions Av variablesv size O(log(||||)),3. Note possible (u) (v1u ) (vku ). instance, V = {x, y}, D(x) = D(y) ={0, 1}, = {x 0, 0}, G = {x 1, 1}, = {{x 0, 1}, {x 1}, {y 0, x1}, {y 1}}, (x) = {{0}, {0, 1}}, (y) = {{0}, {0, 1}}, (xy) = {{x 0, 0}}.fact, example easily extended merged variable xy M-unfoldable,x not.801fiDomshlak & Nazarenko(ii) set ARBv arborescence subgraphs DTG(v, ) rooted I[v], covering G[v]v V(G), size O(log(||||)).say tasks prevail decomposable state variablesprevail decomposable.Note prevail decomposability type (i) tangential notion M-unfoldability:neither former imply latter, way around. contrast, prevaildecomposability type (ii) direct stratication M-unfoldability latterconsiders compacted version DTG (v, ) DTG(v, ), furthermore, allowspolynomial (rather logarithmic) bound number arborescence subgraphs.Theorem 6 Let innite set M-unfoldable, prevail decomposable FDR tasksdirected acyclic causal graphs. tree-width causal graphs boundedconstant, optimal relaxed planning polynomial-time.Proof: well, proof Theorem 6 follows planning-to-COP compilationmethodology. However, compilation prevail decomposability must dierone proof Theorem 5 since longer rely xed in-degree causalgraphs derive xed tree-width constraint networks xed tree-widthcausal graphs. ease presentation, rst specify compilation assumingstate variables satisfy specic condition (i) Denition 5. extendspecication cover alternative condition (ii) Denition 5 well.construction need establish certain graph-theoretic formalismrespective notation. Let G = (V, E) graph, let N : V 2V node neighborhood function G, is, N (v) = {u | {v, u} E}. splitting v Vsupport N (v) transforms G adding new vertex v edge {v, v }, and,u S, removing edge {v, u} adding edge {v , u}. Informally, splitting seen(non-unique) reverse process edge contraction, nodes added G splittingscalled stretch nodes. example, Figure 2b depicts graph obtained graphFigure 2a splitting node v support {x, y, u} N (v) = {x, y, u, w},adding stretch node v(1) .graph G expansion G G transformed G sequence splittings.example, Figure 2c depicts graph obtained graph Figure 2a rstsplitting node v support {x, y} N (v) = {x, y, u, w}, splitting vsupport {u, w} N (v) = {v(1) , u, w}. specically, G = (V , E )expansion G = (V, E) exist functions f : V V g : E E(a) v V , subgraph G induced f 1 (v) = {v V | f (v ) = v} tree,(b) {v, u} E, g({v, u}) = {v , u } f (v ) = v f (u ) = u.tree subgraph (v) G induced f 1 (v) called stretch tree v.bijective correspondence leaves (v) neighbors N (v) v viafunction g: {v, u} E, exactly one edge E , g({v, u}), directlyconnects (v) (u). words, f induces partition V ,part stretch tree (v) v V , g maps edges G edges802fiThe Complexity Optimal Monotonic Planningx-----v//////uw(a)xv(1)x ====v(1)vx(1)vwu(1)@@@u(b)v(1)y(1)vv(2)uxw(c)v(2)uw(1)w(d)Figure 2: Node splitting graph expansions.G connect parts partition. Finally, node degree Gbounded 3, G called sub-cubic. example, expansion Figure 2csub-cubic, expansion Figure 2b not.terminology mostly adopted Markov Shi (2011). addition,call expansion G G fully separating stretch trees G connectedstretch nodes, original nodes, G. is, G = (V , E ) fullyseparating expansion G = (V, E) exists function : V V that,v V , holds (v) f 1 (v) and, edge { (v), v } E , v f 1 (v).example, expansion Figure 2c fully separating, expansion Figure 2dis.4Given notion graph expansion, problem solvedgraph G, eciency solving problem depends badly, possibly exponentially,node degree G, try reformulate problem sub-cubic expansionG G. However, eciency problem question also depends badlygraphs tree-width, tree-width G close possible G.(The tree-width G cannot smaller tree-width G G Gminor.) numerous ecient schemes sub-cubic graph expansion,create expansions arbitrarily larger tree-width expandees.Recently, however, Markov Shi (2011) showed negative side-eect alwayseliminated, sometimes even eciently.Theorem 3.1 Markov Shi (2011) states main result: polynomialtime algorithm that, given graph G tree decomposition width w, computessub-cubic expansion G G tw(G ) w + 1. particular, result impliesgraph G admits sub-cubic expansion whose tree-width tw(G) + 1,expansion constructed eciently arbitrary graph families xed4. Without eective loss generality, one assume (v) = v, is, nodes V nevermapped stretch nodes, mirrors V . However, decided stickexplicit use function avoid confusion nodes V identically named nodesV .803fiDomshlak & Nazarenkotree-width.5 Moreover, straightforward verify expansion transformedlinear time fully separating expansion, without increasing tree-width nodedegrees. Therefore, Theorem 3.1 Markov Shi (2011) holds even request fullyseparating sub-cubic expansions.6COP compilation exploits tree-width friendly expansions causal graphs.Since focus Theorem 5 digraph families C tree-width Cbounded constant, Theorem 3.1 Markov Shi (2011), digraph G Ceciently associated fully separating sub-cubic expansion G tree-widthtw(G) + 1. Note, however, construction G ignores orientation arcsG: G digraph, G undirected graph, construction basedtree decomposition undirected graph induced G. Since COP compilationdepend direction arcs causal graph, restore Grelevant bits information G.rst give auxiliary notation.Given fully separating expansion G (the undirected graph induced by) digraphG = (V, E), consider stretch trees (v) rooted respective nodes (v),Tv (v) denote subtree (v) rooted v (v).Recalling graphs G interest DAGs, leaves (v)bijectively associated neighbors N (v) v G, let N (v), N (v)N (v) partition vs neighbors G immediate ancestors immediatedescendants v, respectively.(v) denote respective neighborsNvin (v) N (v) Nvout(v) Nv associated leaves stretch subtree Tv (v). is,(v) u (u),u Nvin (v) Nvout(v) if, leaf vvG contains edge {v , u } (i.e., g({v, u}) = {v , u }).proceed specifying COP compilation FDR tasks Theorem 6.Given task = V, A, I, G, cost, let G = (V , E ) fully separating, sub-cubicexpansion G causal graph CG tree-width tw(CG ) + 1. respectiveconstraint optimization problem COP+ = (X , ) specied follows.v V , X contains variable xv schematically associated root(v) (v), variable xv /v stretch-tree node v (v) \ { (v)}.domain variable xv{{ | (v), G[v] }, v V(G)D(xv ) =.(7)(v),otherwisedomain variable xv /vD(xv /v ) = {0, 1}mv D(xv ),(8)5. determining optimal tree decomposition graph NP-hard, done polynomial timegraph families xed tree-width (Bodlaender, 1996).6. Requiring expansions fully separating luxury need: relying propertysimplies compilation scheme described next, scheme also modied requirefull separation.804fiThe Complexity Optimal Monotonic Planningmv = |PRv |. is, D(xv /v ) set pairs , , {0, 1}mv(v). {0, 1}mv , DTG(v, + |) denote restriction DTG(v, + )edges supported : Assuming arbitrarily xed numbering elementsPRv = {pr1 , . . . , prmv }, edge marked action pre(a)[pred(v)] = priPRv remains DTG(v, + |) [i] = 1. (v), c(|) denotecost cheapest path I[v] DTG(v, + |); case unreachability,c(|) = .Similarly way node causal graphs expansion G associatedCOP variable, also associated non-negative, extended real-valued function.state variable v V :(I) stretch tree root (v) associated function v . scope vQ(v ) = {xv } {xv /v | v N ( (v))},N : V 2V node neighborhood function G . Note |N ( (v))|3 G sub-cubic. D(xv ) assignment = {v , v }v N ( (v))Q(v ) \ {xv },{c(|H() ), v N ( (v)) : v =v (, ) =,(9),otherwiseH() Hadamard, entrywise, product indicator vectors{v }v N ( (v)) .(II) leaf stretch node v (v) associated 0/ indicator function v /v ,scopeQ(v /v ) = {xv /v , xu /u },u leaf node (u) g({v, u}) = {v , u }. orientationarcs within causal graph CG matters. Specically, {v, u} representscausal graph arc u v , v /v zeroes assignments (v , , u , u )vector v enables preconditions PRv are, passivelyactively, supported value u u,vector u enables preconditions PRu , since v conditionu-changing actions DAG-structured planning task .is, v , v D(xv /v ) u , u D(xu /u ),0,u = 1v /v (v , v , u , u ) =1 mv : (v [i] = 0) (u V(pri ) pri [u] u ) ., otherwise(10)Note value v /v independent v component v , v , u , u .805fiDomshlak & NazarenkoOtherwise, {v, u} represents causal graph arc7 v u, conversely,0, v = 1v /v (v , v , u , u ) =1 mu : (u [i] = 0) (v V(pri ) pri [v] v ) ., otherwise(11)(III) internal stretch node v (v) also associated 0/ indicator functionv /v , scope comprises variable xv /v , together variables xv /vcorrespond immediate descendants v Tv (v). is,Q(v /v ) = {xv /v } {xv /v | v N (v ) Tv (v)}.v , v D(xv /v ) assignment = {v , v } Q(v /v ) \{xv /v }, v /v zeroes (v , v , ) vector v enablespreconditions PRv (passively actively) supportedimmediate ancestors u Nvin (v) via values ancestors commitrespective stretch-tree root COP variables xu . is,{0, v = H() v N (v ) Tv (v) : v = vv /v (v , v , ) =. (12), otherwisewords, starting Eqs. 10 11, support provided Nvin (v) vcommunicated v indicator vectors v , aggregated/summarizedEq. 12 Hadamard vector product H() .Complexity-wise, properties COP+ constructed follows.(1) constraint network COP+ obtained expansion G replacinggraphsubgraph G induced nodes Q( ) clique Q( ). Let GQnodes Q( ), x X , edges {Q( ), Q( )} (only) pairs ,isomorphicQ( ) Q( ) = . construction COP functions , GQG . Likewise, since |Q( ) Q( )| 1 pairs functions , , tree-width) max |Q( )|. Together impliesCOP constraint network tw(GQCOP tw(GQ)max |Q( )| = tw(G )max |Q( )| 4tw(G ) 4(tw(CG )+1) = O(1).Hence, since nding constant-factor approximation graphs tree-width polynomial size graph exponential tree-width, COP+solved time polynomial size representation.(2) M-unfoldability Eq. 7, v V , domain size COP variable xvO(poly(||||)). domain stretch node variable xv /v cross-producttwo sets. size second set Eq. 8 O(poly(||||)) domainsize respective variable xv . size rst set Eq. 8 2|PRv | and,Denition 5, 2|PRv | = 2O(log(||||)) = O(poly(||||)). Together (1), impliesCOP+ solved time O(poly(||||)).7. Since Theorem 6 devoted directed acyclic causal graphs, address caseCG contains (v, u) (u, v).806fiThe Complexity Optimal Monotonic Planning(3) denition monotonic relaxation denition domain transition graphs,explicit description DTG(v, + ) M-unfoldable FDR task polynomial||||. Hence, construction functional components Eqs. 9-12, thusentire COP+ , done time O(poly(||||)).proceed prove correctness COP+ = (X , ). is, prove+ unsolvable, assignments X evaluate objective function(X ) , otherwise, objective minimized assignmentsX correspond optimal plans + .First, given assignment X () < , show inducesvalid plan + cost (). Eqs. 9 12, () < implies that,v V xv /v stretch tree (v), [xv /v ] {, [xv ]}. is,relaxation value v assigned xv consistently propagated nodes(v), particular, leaves.Let leaf node xv /v (v) connect (v) (u) causal graph neighboru N (v), let [xv /v ] = v , [xv ]. u N (v), Eq. 10, v /v () =implies v encodes preconditions PRv disabledrelaxation value [xu ] u. Otherwise, u N (v), then, DAG structure CG ,u nothing preconditions actions aecting v + , Eq. 11, v /v () =implies v = 1 trivially enables preconditions PRv .Given that, (leaf internal) stretch node (v), let [xv /v ] = v , [xv ].conjunctive structure preconditions FDR Hadamard vector productEq. 12, v /v () = implies v encodes preconditions PRvnotdisabled values [xu ] u Nvin (v). Finally, denition graphexpansion, v N ( (v)) Nvin (v) = N (v). Thus, Eq. 11, v () = implies [xv ]reachable I[v] properly restricted domain transition graph DTG(v, + |[Xv ])Xv = {xu | u pred(v)}, v () equals cost cheapestpath. rest follows DAG structure CG Lemma 1. proofopposite direction straightforward construction COP+ serializationLemma 1.nal note return denition prevail decomposability, specically,second sucient condition that, state variables v V , set ARBv arborescencesugraphs DTG(v, ) rooted I[v], covering G[v] v V(G), size O(log(||||)).condition addressed COP construction far, switchingrst second sucient condition prevail decomposability requiressemantics indicator vectors changed: Instead encoding supportpred(v) provide individual preconditions actions Av , encode supportpred(v) provide entire arborescences ARBv DTG(v, ). Since conditionrequires |ARBv | = O(log(||||)), support encoded reasoned eciently.Note choice two conditions made variable-by-variable basis,thus two conditions mutually exclusive complementary.807fiDomshlak & Nazarenkocausal graphextra conditionxed size= O(1)|D(v)| = O(1)= O(1) & DAG |D(v)| = O(1)= O(1) & DAG in-degree = O(1)= O(1) & DAGFDRP?YesMFDRP?YesYesYes, M-unfoldableYes, M-unfoldableprevail decomposableTh.Th.Th.Th.Th.23356Table 1: summary main results optimal MFDR planning, contrastedpreviously established complexity corresponding fragments optimal FDRplanning. table, fragment FDR/MFDR planning, characterizedterms causal graph tree-width , causal graph in-degree, upperbound |D(v)| size variable domains. M-unfoldability prevaildecomposability two properties MFDR tasks introducedexploited work.6. Summary Future Worktook step towards ne-grained classication worst-case time complexity optimalmonotonic planning, focus gets harder gets easierswitching optimal planning optimal relaxed planning, context nite-domainplanning task representations. Along way, established negative positiveresults complexity wide fragments problem, negative resultsemphasizing role structure state variable domains, positive resultsemphasizing role causal graph topology. Table 1 lists main results optimalmonotonic planning, contrasted complexity corresponding fragmentsoptimal FDR planning. key conclusions follows.1. Optimal planning monotonic relaxations hard even restricted simplecausal graph structures, complexity stems size statevariable domains.2. Restricted planning tasks constant-bounded state variable domains, problem becomes solvable time exponential tree-width causal graph,known much even non-optimal regular planning.3. tree-width digraphs independent edge directions, exploitingdirected structure causal graph together tree-width allowscomputational tractability expanded beyond xed-size state variable domains.latter conclusion opens interesting venue investigation.addressed directed acyclic causal graphs, scope tractability perhaps expanded exploiting existing directed notions graph width (Johnson, Robertson,Seymour, & Thomas, 2001; Hunter & Kreutzer, 2008; Berwanger et al., 2012). might808fiThe Complexity Optimal Monotonic Planningespecially appealing tree-width standard planning benchmarks natural FDR encodings appear xed across respective familiestasks.Another interesting direction would examine results techniques introduced wider context: recently introduced framework red-blackrelaxations (Katz et al., 2013b). red-black (RB) planning, variables partitionedtwo sets: black set adopts regular, value switching semantics FDR,red set adopts monotonic, value accumulating semantics MFDR. contextsatiscing planning, complexity analysis RB planning complexity lenscausal graph topology already led advances practice heuristic-searchplanning (Katz et al., 2013a; Katz & Homann, 2013). take similar step optimalplanning, admissible heuristics based RB relaxations must devised. That,turn, calls identifying tractable fragments optimal RB planning. cautiouslyoptimistic results techniques presented paper found valuable context RB planning well. instance, positive result Theorem 4fork-structured MFDR tasks straightforwardly extended RB tasksroot variables taking monotonic semantics leaves keeping regular, FDRsemantics. Similarly, positive result Theorem 5 DAG-structured MFDR tasksstraightforwardly extended RB tasks black-painted leaf variables. interestingquestion respect whether computational tractability optimal RB planningextended causal graphs internal nodes get keep original FDRsemantics.Acknowledgmentswork partly supported Israel Science Foundation (ISF) grant 1045/12,EOARD grant FA8655-12-1-2096.ReferencesAmir, E. (2010). Approximation algorithms treewidth. Algorithmica, 56 (4), 448479.Arnborg, S., Cornell, D. G., & Proskurowski, A. (1987). Complexity nding embeddingsk-tree. SIAM Journal Algebraic Discrete Methods, 8, 277284.Backstrom, C., & Klein, I. (1991). Planning polynomial time: SAS-PUBS class.Computational Intelligence, 7 (3), 181197.Backstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. ComputationalIntelligence, 11 (4), 625655.Becker, A., & Geiger, D. (1996). suciently fast algorithm nding close optimaljunction trees. Proceedings 12th Conference Uncertainty ArticialIntelligence (UAI), pp. 8189.Berwanger, D., Dawar, A., Hunter, P., Kreutzer, S., & Obdzralek, J. (2012). DAG-widthdirected graphs. Journal Combinatorial Theory, Series B, 102 (4), 900923.Betz, C., & Helmert, M. (2009). Planning h+ theory practice. Proceedings32nd Annual German Conference Articial Intelligence (KI), pp. 916.809fiDomshlak & NazarenkoBodlaender, H. L. (1996). linear-time algorithm nding tree-decompositions smalltreewidth. SIAM Journal Computing, 25 (6), 13051317.Bonet, B., & Gener, H. (2001). Planning heuristic search. Articial Intelligence, 129 (12), 533.Bonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets.Proceedings 19th European Conference Articial Intelligence, pp. 329334,Lisbon, Portugal.Brafman, R. I., & Domshlak, C. (2006). Factored planning: How, when, not.Proceedings 18th National Conference Articial Intelligence (AAAI), pp.809814, Boston, MA.Brafman, R. I., & Domshlak, C. (2013). complexity planning agent teamsimplications single agent planning. Articial Intelligence, 198, 5271.Bylander, T. (1994). computational complexity propositional STRIPS planning.Articial Intelligence, 69 (1-2), 165204.Cai, D., Homann, J., & Helmert, M. (2013). Enhancing context-enhanced additiveheuristic precedence constraints. Proceedings 19th International Conference Automated Planning Scheduling (ICAPS), pp. 5057.Chen, H., & Gimenez, O. (2010). Causal graphs structurally restricted planning. Journal Computer System Sciences, 76 (7), 579592.Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.Domshlak, C., & Dinitz, Y. (2001). Multi-agent o-line coordination: Structure complexity. Proceedings Sixth European Conference Planning (ECP), pp. 277288.Downey, R. G., & Fellows, M. R. (1999). Parameterized Complexity. Springer-Verlag, NewYork.Edelkamp, S. (2001). Planning pattern databases. Proceedings EuropeanConference Planning (ECP), pp. 1324.Fabre, E., Jezequel, L., Haslum, P., & Thiebaux, S. (2010). Cost-optimal factored planning:Promises pitfalls. Proceedings International Conference AutomatedPlanning Scheduling (ICAPS), pp. 6572.Fikes, R. E., & Nilsson, N. (1971). STRIPS: new approach application theoremproving problem solving. Articial Intelligence, 2, 189208.Flum, J., & Grohe, M. (2006). Parameterized Complexity Theory. Springer-Verlag.Fox, M., & Long, D. (2001). Stan4: hybrid planning strategy based subproblemabstraction. AI Magazine, 22 (3), 8184.Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporalplanning problems. Journal Articial Intelligence Research, 20, 61124.Gimenez, O., & Jonsson, A. (2009a). inuence k-dependence complexityplanning. Proceedings 19th International Conference Automated PlanningScheduling (ICAPS), pp. 138145.810fiThe Complexity Optimal Monotonic PlanningGimenez, O., & Jonsson, A. (2009b). Planning chain causal graphs variablesdomains size 5 NP-hard. Journal Articial Intelligence Research, 34, 675706.Halin, R. (1976). s-functions graphs. Journal Geometry, 8, 171186.Helmert, M. (2004). planning heuristic based causal graph analysis. ProceedingsFourteenth International Conference Automated Planning Scheduling(ICAPS), pp. 161170.Helmert, M. (2006). Fast Downward planning system. Journal Articial IntelligenceResearch, 26, 191246.Helmert, M. (2009). Concise nite-domain representations PDDL planning tasks. Articial Intelligence, 173, 503535.Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whatsdierence anyway?. Proceedings 19th International Conference Automated Planning Scheduling (ICAPS), pp. 162169.Helmert, M., & Gener, H. (2008). Unifying causal graph additive heuristics. Proceedings 18th International Conference Automated Planning Scheduling(ICAPS), pp. 140147.Helmert, M., Haslum, P., & Homann, J. (2007). Flexible abstraction heuristics optimalsequential planning. Proceedings 17th International Conference AutomatedPlanning Scheduling (ICAPS), pp. 200207.Helmert, M., & Mattmuller, R. (2007). Accuracy admissible heuristic functions selected planning domains. Proceedings 23rd AAAI Conference ArticialIntelligence, pp. 938943.Homann, J. (2005). ignoring delete lists works: Local search topology planningbenchmarks. Journal Articial Intelligence Research, 24, 685758.Homann, J. (2011). Analyzing search topology without running search: connection causal graphs h+ . Journal Articial Intelligence Research, 41,155229.Homann, J., & Nebel, B. (2001). FF planning system: Fast plan generationheuristic search. Journal Articial Intelligence Research, 14, 253302.Hunter, P., & Kreutzer, S. (2008). Digraph measures: Kelly decompositions, games,orderings. Theoretical Computer Science, 399 (3), 206219.Johnson, T., Robertson, N., Seymour, P. D., & Thomas, R. (2001). Directed tree-width.Journal Combinatorial Theory, Series B, 82 (1), 138154.Jonsson, P., & Backstrom, C. (1998). State-variable planning structural restrictions:Algorithms complexity. Articial Intelligence, 100 (12), 125176.Karp, R. (1972). Reducibility among combinatorial problems. Complexity ComputerComputations, pp. 85103. Plenum Press, New York.Katz, M., & Domshlak, C. (2008). New islands tractability cost-optimal planning.Journal Articial Intelligence Research, 32, 203288.811fiDomshlak & NazarenkoKatz, M., & Domshlak, C. (2010). Implicit abstraction heuristics. Journal ArticialIntelligence Research, 39, 51126.Katz, M., & Homann, J. (2013). Red-black relaxed plan heuristics reloaded. Proceedings6th Annual Symposium Combinatorial Search (SOCS), pp. 105113.Katz, M., Homann, J., & Domshlak, C. (2013a). Red-black relaxed plan heuristics.Proceedings 27th AAAI Conference Articial Intelligence (AAAI), pp. 489495.Katz, M., Homann, J., & Domshlak, C. (2013b). said need relax variables?. Proceedings 23rd International Conference Automated PlanningScheduling (ICAPS), pp. 126134.Katz, M., & Keyder, E. (2012). Structural patterns beyond forks: Extending complexityboundaries classical planning. Proceedings 26th AAAI ConferenceArticial Intelligence (AAAI), pp. 17791785.Keyder, E., & Gener, H. (2008a). Heuristics planning action costs revisited.Proceedings 18th European Conference Articial Intelligence (ECAI), pp.588592.Keyder, E., & Gener, H. (2008b). Heuristics planning action costs revisited.Proceedings 18th European Conference Articial Intelligence, pp. 588592.Markov, I. L., & Shi, Y. (2011). Constant-degree graph expansions preserve treewidth.Algorithmica, 59, 461470.McDermott, D. V. (1999). Using regression-match graphs control search planning.Articial Intelligence, 109 (1-2), 111159.Nissim, R., & Brafman, R. I. (2012). Multi-agent parallel distributed systems.Proceedings 11th International Conference Autonomous Agents Multiagent Systems (AAMAS), pp. 12651266.Nissim, R., Brafman, R. I., & Domshlak, C. (2010). general, fully distributed multiagent planning algorithm. Proceedings 9th International ConferenceAutonomous Agents Multiagent Systems (AAMAS), pp. 13231330.Pednault, E. (1989). ADL: Exploring middle ground STRIPS situation calculus. Proceedings 1st International Conference PrinciplesKnowledge Representation Reasoning, pp. 324331.Robertson, N., & Seymour, P. D. (1984). Graph minors III: Planar tree-width. JournalCombinatorial Theory, 36, 4963.Robertson, N., & Seymour, P. D. (1991). Graph minors X: Obstructions tree decomposition. Journal Combinatorial Theory, Series B, 52 (2), 153190.Seipp, J., & Helmert, M. (2011). Fluent merging classical planning problems. Proceedings ICAPS-2011 Workshop Knowledge Engineering PlanningScheduling (KEPS), pp. 4753.812fiJournal Artificial Intelligence Research 48 (2013) 923-951Submitted 7/13; published 12/13Smooth Transition Powerlessness Absolute PowerElchanan Mosselmossel@stat.berkeley.eduUniversity California, BerkeleyBerkeley, CA 94720 USAWeizmann Institute ScienceRehovot, IsraelAriel D. Procacciaarielpro@cs.cmu.eduCarnegie Mellon UniversityPittsburgh, PA 15213 USAMiklos Z. Raczracz@stat.berkeley.eduUniversity California, BerkeleyBerkeley, CA 94720 USAAbstractstudy phase transition coalitional manipulation problem generalizedscoring rules. Previously shown that,conditions distributionvotes, number manipulators ( n), n number voters,probability random profile manipulable coalition goes zeronumber voters goes infinity, whereas number manipulators ( n),probability random profile manipulablegoes one. considercritical window, coalition size c n, show c goes zeroinfinity, limiting probability random profile manipulable goes zeroone smooth fashion, i.e., smooth phase transition two regimes.result analytically validates recent empirical results, suggests decidingcoalitional manipulation problem may limited computational hardness practice.1. IntroductionFinding good voting systems satisfy natural requirements one maingoals social choice theory. problem increasingly relevant area artificial intelligence computer science broadly, virtual electionsestablished tool preference aggregation (see, e.g., Caragiannis & Procaccia, 2011).naturally desirable property voting system strategyproofness (a.k.a. nonmanipulability): voter benefit voting strategically, i.e., voting accordingtrue preferences. However, Gibbard (1973) Satterthwaite (1975) showedreasonable voting system strategyproof. stating result, let us specifyproblem formally.consider n voters electing single winner among candidates. voters specifyopinion ranking candidates, winner determined accordingn [m] voters rankings,predefined social choice function (SCF) f : Smdenotes set possible total orderings candidates. call collectionrankings voters ranking profile. say SCF manipulable existsc2013AI Access Foundation. rights reserved.fiMossel, Procaccia, & Raczranking profile voter achieve desirable outcome election accordingtrue preferences voting way reflect true preferences.Gibbard-Satterthwaite theorem states SCF dictatorship(i.e., function single voter), allows least three candidateselected, manipulable. contributed realization unlikely expecttruthfulness voting. Consequently, many branches research devotedunderstanding extent manipulability voting systems, finding wayscircumventing negative results.One approach, introduced Bartholdi, Tovey, Trick (1989), suggests computationalcomplexity barrier manipulation: SCF may manipulable practicehard voter compute manipulative vote. significant body work focusesworst-case complexity manipulation (see survey Faliszewski Procaccia, 2010).interested specifically coalitional manipulation problem, groupvoters change votes unison, goal making given candidatewin. Various variations problem known N P-hard manycommon SCFs (Conitzer, Sandholm, & Lang, 2007; Xia, Zuckerman, Procaccia, Conitzer,& Rosenschein, 2009; Betzler, Niedermeier, & Woeginger, 2011).Crucially, line work focuses worst-case complexity. worst-case hardnessmanipulation desirable property SCF have, tell us muchtypical instances problemis usually easy hard manipulate? recent lineresearch average-case manipulability questioning validity worstcase complexity results. goal alternative line work showreasonable voting rules computationally hard manipulate average.Specifically, goal rule following informal statement: good votingrules hard manipulate average sufficiently rich distributionvotes.Taking point view, showing easiness manipulation restricted classdistributionssuch i.i.d. votes even uniform votes (the impartial culture assumption)interesting, even necessarily capture possible real-world elections.Specifically, show manipulation easy distributions, averagecase hardness result would necessarily make unnatural technical assumptionsavoid distributions. Studying restricted distributions votes indeedexactly recent papers done.coalitional manipulation problem, Procaccia Rosenschein (2007a) first suggested trivial determine whether manipulation possible coalitionalmanipulation instances, typical-case computational point view; one makehighly informed guess purely based number manipulators. Specifically, studied setting distribution votes (which satisfies conditions),concentrated family SCFs known positional scoring rules. showedsize coalition ( n), probability converging 1 n , coalitionpowerless, i.e., cannot change outcome election. contrast, sizecoalition ( n) (and (n)), probability converging 1 n , coalition all-powerful, i.e., elect candidate. Later Xia Conitzer (2008b) provedanalogous result so-called generalized scoring rules, family contains almostcommon voting rules. See also related work Peleg (1979), Slinko (2004), Pritchard924fiA Smooth Transition Powerlessness Absolute PowerSlinko (2006), Pritchard Wilson (2009). discuss additional related workSection 1.2.primary interest paper understand critical window papersleave open, size coalition ( n). Specifically, interestedphase transition probability coalitional manipulation, size coalitionc n c varies zero infinity, i.e., transition powerlessness absolutepower.past decades much research connection phasetransitions computationally hard problems (see, e.g., Fu & Anderson, 1986; Cheeseman,Kanefsky, & Taylor, 1991; Achlioptas, Naor, & Peres, 2005). particular, oftencase computationally hardest problems found critical valuessharp phase transition (see, e.g., Gomes & Walsh, 2006, overview).hand, smooth phase transitions often found connection computationally easy(polynomial) problems, 2-coloring (Achlioptas, 1999) 1-in-2 SAT (Walsh, 2002).Thus understanding phase transition critical window may shed lightcomputationally hardest problems lie.Recently, Walsh (2011) empirically analyzed two well-known voting rulesvetosingle transferable vote (STV)and found smooth phase transitiontwo regimes. Specifically, Walsh studied coalitional manipulation unweighted votesSTV weighted votes veto, sampled number distributionsexperiments, including i.i.d. distributions, correlated distributions, votes sampledreal-world elections. main result complements improves upon Walshs analysistwo ways; Walshs results show phase transition looks like concretelyveto STV, analytically show phase transition indeed smoothgeneralized scoring rule (including veto STV) votes i.i.d. suggestsdeciding coalitional manipulation problem may computationally hardpractice.1.1 Resultspresent results, first let us formally specify setup problem.n , candidate a, definedenote ranking profile = (1 , . . . , n ) Smn | f () = a}, set ranking profiles outcome f a.Wa = { Smsetup assumptions following.Assumption 1. assume number candidates, m, constant.Assumption 2. assume SCF f anonymous, i.e., treats voter equally.Assumption 3. assume votes voters i.i.d., according distributionp Sm . Furthermore, assume exists > 0 every Sm ,p () (necessarily 1/m!).assume these, setup would include uninteresting cases,f constanti.e., matter votes are, specific candidate wins.Another less interesting case probability given candidate winning vanishesn essentially forget candidate large n (in sense925fiMossel, Procaccia, & Raczcoalition size (n) would necessary make candidate win). excludefocus interesting cases, make additional assumption concernsSCF distribution votes.Assumption 4. assume exists > 0 every n everycandidate [m], probability elected least > 0, i.e., P (Wa )(necessarily 1/m).four assumptions satisfied distribution uniform (i.e., impartial culture assumption) SCF close neutral (i.e., neutraltie-breaking rules); particular, hold commonly used SCFs. assumptionssomewhat general this, although i.i.d. assumption remains restrictiveone. However, discussed earlier, even showing easiness manipulationrestricted class distributions interesting.mentioned before, interested case coalition size c nconstant c. Define probabilitiesq n (c) := P coalition size c n elect candidate ,q n (c) := P coalition size c n change outcome election ,rn (c) := P specific coalition size c n elect candidate ,rn (c) := P specific coalition size c n change outcome election ,letq (c) := lim q n (c) ,nq (c) := lim q n (c) ,nr (c) := lim rn (c) ,nr (c) := lim rn (c) ,nprovided limits exist. Clearly q n (c) q n (c), rn (c) rn (c), rn (c) q n (c),rn (c) q n (c).describe results, deal quantities, first explainrelate various variants coalitional manipulation problem. coalitional manipulation problem coalition fixed, thus relevant quantities rn (c)rn (c). Closely related problem determining margin victory,minimum number voters need change votes change outcomeelection. Also related problem bribery, minimum number voters needchange votes make given candidate win. main differenceproblems coalitional manipulation coalition fixed, whereas latter twoproblems coalition fixed. Hence relevant quantities studying lattertwo q n (c) q n (c). tools also allow us deal related quantities (suchmicrobribery, Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009), focusattention four quantities described above.first result analyzes case size coalition c n large c.show c large enough, probability close 1, specific coalition size c nelect candidate. holds SCF satisfies (mild) restrictions.Theorem 1.1. Assume Assumptions 1, 2, 3, 4 hold. > 0 existsconstant c = c (, , , m) rn (c) 1 every n. particular, choosehppc = (4/) log (2m!/)log (2m/) + log (2/) .926fiA Smooth Transition Powerlessness Absolute Powerfollowslim lim inf rn (c) = 1.cnresult extends previous theorems Procaccia Rosenschein (2007a), XiaConitzer (2008b), scoring rules generalized scoring rules, respectively,anonymous SCFs.second result deals case size coalition c n smallc, transition c goes 0 . assume additionally fgeneralized scoring rule (to defined Section 3.1.1); needed exist(pathological) anonymous SCFs result hold (see beginningSection 3 example).Theorem 1.2. Assume Assumptions 1, 2, 3, 4 hold, furthermore fgeneralized scoring rule. Then:(1) limits q (c), q (c), r (c) r (c) exist.(2) exists constant K = K (f, ) < q (c) Kc; particular,limc0 q (c) = 0.(3) 0 < c < , 0 < q (c) q (c) < 1 0 < r (c) r (c) < 1, furthermoreq (c), q (c), r (c) r (c) continuously differentiable c bounded derivative.words, Part 2 means c small enough probability close 1coalition size c n change outcome election, statementsr r Part 3 mean coalitional manipulation problem smooth phasetransition: number manipulators increases, probabilities coalitionpower, absolute power, increase smoothly. Parts 1 2 theoremsimply make result Xia Conitzer (2008b) precise, extending analysis( n) regime. importantly, proofs statements introducemachinery needed establish Part 3, main result.Since coalitional manipulation problem sharp phase transition, Theorem 1.2 interpreted suggesting realistic distributions votes likelyyield coalitional manipulation instances tractable practice, even sizecoalition concentrates previously elusive ( n) regime; truegeneralized scoring rule, particular almost common social choice functions (anexception Dodgsons rule). interpretation negative flavor strengthening conclusion worst-case complexity poor barrier manipulation.However, complexity glass fact half empty. probability marginvictory c n captured quantity q n , hence Part 3 Theorem 1.2 alsoimplies margin victory problem smooth phase transition. recentlypointed Xia (2012a), efficiently solving margin victory problem could helppost-election auditsused determine whether electronic elections resultedincorrect outcome due software hardware bugsand tractability fact desirable.methods use flexible, extended various setups interestdirectly satisfy assumptions above, instance single-peaked preferences. Considerone-dimensional political spectrum represented interval [0, 1], fix location927fiMossel, Procaccia, & Raczcandidates. Assume voters uniformly distributed interval, independentlyother. technical reasons, distribution satisfy assumptions,since rankings Sm p () = 0; however, tools allow ussettingwell. instance, locations candidates1deal32m1, results hold (with appropriate quantitative modifications).2m , 2m , . . . , 2mSimilarly, locations something else, would exist subset candidatesasymptotically nonvanishing probability winning, results holdrestricted subset candidates.Finally, discuss role tie-breaking setup, since often importantissue studying manipulation. However, since consider manipulation coalitionssize c n, ties exist constant number voters voteschanged appropriately longer tie, relevant. Indeed, tools allow usextend results Theorem 1.2 class SCFs slightly beyond generalized scoringrules, and, particular, allow arbitrary tie-breaking rules (see Section 3.2.1details).1.2 Additional Related Workrecent line research average-case algorithmic flavor also suggests manipulation indeed typically easy; see, e.g., work Kelly (1993), Conitzer Sandholm (2006), Procaccia Rosenschein (2007b), Zuckerman et al. (2009) resultscertain restricted classes SCFs. different approach, initiated Friedgut, Kalai,Keller Nisan (2008, 2011), studied fraction ranking profiles manipulable, also suggests manipulation easy average; see work XiaConitzer (2008a), Dobzinski Procaccia (2008), Isaksson, Kindler Mossel (2012),Mossel Racz (2012). refer survey Faliszewski Procaccia (2010)detailed history surrounding literature. See also related literature economics,e.g., work Good Mayer (1975), Chamberlain Rothschild (1981), Myatt(2007).Recent work Xia (2012a) independent from, closely related to, work.mentioned above, Xias paper concerned computing margin victoryelections. focuses computational complexity questions approximation algorithms,one results similar Parts 1 2 Theorem 1.2. However, analysiscompletely different; approach facilitates proof Part 3 theorem,main contribution. even recent (and also independent) manuscript Xia (2012b)considers similar questions generalized scoring rules captures additional typesstrategic behavior (such control), again, crucially, work attemptunderstand phase transition (nor subsume Theorem 1.1).2. Large CoalitionsWithout ado, prove Theorem 1.1. main idea observe i.i.d. distributions, Hamming distance random ranking profile fixed subset rankingprofiles concentrates around mean. theorem follows standard concentrationinequalities.928fiA Smooth Transition Powerlessness Absolute Powern , defineProof Theorem 1.1. , 0 Sm,01X=1 6= i0 ,nni=1i.e., (, 0 ) 1/n times Hamming distance 0 . U subset rankingprofiles specific ranking profile define dU () = min0 U (, 0 ).function dU Lipschitz constant 1/n, therefore McDiarmids inequalityfollowing concentration inequality:P (|dU () EdU | c) 2 exp 2c2 n(1)n . Suppose U n measure least , i.e., Uc > 0 U SmpP ( U ) , take 2 exp 2 2 n < , e.g., let = log (2/)/ n.(1) implies exists U |dU () EdU | , since dU () = 0,means EdU . set U ,P (dU () > + c) exp 2c2 npc > 0. Choosing c = B/ n defining B 0 = B + log (2/) get(2)P dU () > B 0 / n exp 2B 2 .language usual Hamming distance, means probability0 n coordinates Uranking profileneedschangedleastBexp 2B 2 , made arbitrarily small choosing B large enough.assumption, P ( Wa ) every a, therefore (2) union boundgetP : dWa () > B 0 / n exp 2B 2 .pchoosing B = log (2m/), probability /2.Consider specific coalition size DB 0 n, = (, m) chosen later.Using Chernoffs bound union bound, probability close one, every possibleranking coalition least B 0 n voters ranking :P Sm : coalition size DB 0 n less B 0 n voters rankingm!P Bin DB 0 n, < B 0 n m! exp (1 1/D)2 DB 0 n/2m! exp (1 1/D)2 D/2 ,Bin (DB 0 n, ) denotes binomial random variable parameters DB 0 n ,used assumption every voter probability every rankingleast > 0. Choosing = (4/) log (2m!/), probability /2.anonymity f , outcome depends number voters votingaccording ranking. Consequently, distanceB 0 / n away Wa , ranking least B 0 n voterscoalition ranking , coalition able achieve outcome. Usingunion bound happens probability least 1 .929fiMossel, Procaccia, & Racz3. Small Coalitions Phase Transitionsection almost entirely devoted proof Theorem 1.2, also includeshelpful definitions, examples, intuitions.Consider following example SCF. [m] let na () denote numbervotersPwho ranked candidate top ranking profile . Define SCF ff () =a=1 ana () mod m. SCF clearly anonymous (since dependsnumber voters voting according specific rankings), moreover easy seesingle voter always elect candidate.example shows that, general, cannot matching lower boundsize manipulating coalition order n. However, artificial example(one would consider voting system real life), expect matchinglower bound holds reasonable SCFs.Xia Conitzer (2008b) introduced large class SCFs called generalized scoringrules, include commonly occurring SCFs. following introduceclass SCFs, provide alternative way looking (as so-called hyperplanerules), show class SCFs coalition size c n small enoughc, probability able change outcome election arbitrarilyclose zero. end section prove smooth transition statedPart 3 Theorem 1.2.3.1 Generalized Scoring Rules, Hyperplane Rules, Equivalencesection introduce generalized scoring rules hyperplane rules showequivalence.3.1.1 Generalized Scoring Rulesdefine generalized scoring rules.Definition 1. y, z Rk , say z equivalent, denoted z,every i, j [k], yi yj zi zj .Definition 2. function g : Rk [m] compatible z, g (y) = g (z).is, function g compatible, g (y) completely determinedtotal preorder {y1 , . . . , yk } (a total preorder ordering ties allowed).Definition 3 (Generalized scoring rules). Let k N, f : Sm Rk (called generalizedscoring function), g : Rk [m] g compatible (g called decision function).functions f g determine generalized scoring rule GS (f, g) follows:n , letSm!nXGS (f, g) () := gf (i ) .i=1definition clear every generalized scoring rule (GSR) anonymous.930fiA Smooth Transition Powerlessness Absolute Power3.1.2 Hyperplane RulesPreliminaries notation. following, SCF let us write f fn , i.e., let usexplicitly note f function n voters; also let us write n . Since SCFfn anonymous, outcome depends numbers voters vote accordingparticular rankings. Let Dn denote set points probability simplex m!coordinates integer multiples 1/n. Let us denote typical elementprobability simplex m! x = {x }Sm . ranking profile n , let us denotecorresponding element probability simplex x ( n ), i.e., Sm ,1X1 [i = ] .nnx ( n ) =i=1assumptions outcome fn depends x ( n ), abuse notationmay write fn : m! |Dn [m] fn ( n ) = fn (x ( n )).ready define hyperplane rules.Definition 4 (Hyperplane rules). Fix finite set affine hyperplanes simplex m! :H1 , . . . , H` . affine hyperplane partitions simplex three parts: affine hyperplane two open halfspaces either side affine hyperplane. Thus affinehyperplanes H1 , . . . , H` partition simplex finitely many (at 3` ) regions. LetF : m! [m] function constant region. sequencen [m], definedSCFs {fn }n1 , fn : Smfn ( n ) = F (x ( n ))called hyperplane rule induced affine hyperplanes H1 , . . . , H` function F .function F : m! [m] naturally partitions simplex m! parts basedoutcome F . (For hyperplane rules partition coarser partition m!induced affine hyperplanes H1 , . . . , H` .) abuse notation denote parts{Wa }a[m] . following definition useful us.Definition 5 (Interior boundaries partition induced F ). say x m!interior point partition {Wa }a[m] induced F exists > 0m! |x y| , F (x) = F (y). Otherwise, sayx m! boundary partition, denote B.hyperplane rule boundary B contained union correspondingaffine hyperplanes. Conversely, suppose F : m! [m] arbitrary functionn [m] defined f ( n ) = F (x ( n )).sequence (anonymous) SCFs {fn }n1 , fn : Smnboundary B F contained union finitely many affine hyperplanesm! , F necessarily hyperplane rule, exists hyperplane rule FF F agree everywhere except perhaps union finitely many affinehyperplanes.931fiMossel, Procaccia, & Racz3.1.3 EquivalenceXia Conitzer (2009) gave characterization generalized scoring rules: SCFgeneralized scoring rule anonymous finitely locally consistent (see Xia& Conitzer, 2009, Definition 5). characterization related saying generalizedscoring rules hyperplane rules, yet believe spelling explicitlyimportant, geometric viewpoint hyperplane rules somewhat different,probabilistic context also flexible.Lemma 3.1. class generalized scoring rules coincides class hyperplanerules.Proof. First let us show every hyperplane rule generalized scoring rule. Let usconsider hyperplane rule induced affine hyperplanes H1 , . . . , H` simplex m! ,function F : m! [m]. affine hyperplanes m! thoughthyperplanes Rm! go originabusing notation also denoteH1 , . . . , H` . Let u1 , . . . , u` denote unit normal vectors hyperplanes.n,need define functions f g every ranking profile n Sm`+1`+1nnGS (f, g) ( ) = F (x ( )). f : Sm Rg : R[m]. Coordinates1, . . . , ` f correspond hyperplanes H1 , . . . , H` , last coordinate f always0 (this technical necessity make sure function g compatible). Let uslook coordinate corresponding hyperplane Hj normal vector uj . Smdefine(f ())j (f ())Hj (f ())uj := (uj ) ,coordinates Rm! indexed elements Sm .n(f ( ))j :=nXi=1(f (i ))j =nX(uj )i = n (uj x ( n )) .i=1sign (f ( n ))j thus tells us side hyperplane Hj point x ( n ) lieson. define g (y) R`+1 y`+1 = 0; requirement gcompatible defines g R`+1 . x R, define sgn (x) 1 x > 0, 1 x < 0,0 x = 0.define g (y1 , . . . , y` , 0), look vector (sgn (y1 ) , . . . , sgn (y` )). vector determines region m! following way: sgn (yj ) = 1, region liesopen halfspace uj , sgn (yj ) = 1 region lies open halfspacecontain uj , finally yj = 0, region lies hyperplaneHj . define g (y1 , . . . , y` , 0) value F region m! defined(sgn (y1 ) , . . . , sgn (y` )). value g (y1 , . . . , y` , 0) well-defined since F constantregion. Moreover, take z y`+1 = z`+1 = 0, necessarily(sgn (y1 ) , . . . , sgn (y` )) = (sgn (z1 ) , . . . , sgn (z` )), thus g (y) = g (z): g compatible(this used extra coordinate).let us show every generalized scoring rule hyperplane rule. Supposegeneralized scoring rule given Pfunctions f : Sm PRk g : Rk [m]. rankingnnnnprofile Sm , define f ( ) := i=1 f (i ) = n Sm f () (x ( n )) ; waykview f function mapping Nm!0 \ {0} R (and hence also view GS (f, g)932fiA Smooth Transition Powerlessness Absolute Powerfunction mapping Nm!0 \ {0} [m]). Since mapping homogeneous, may extenddomain f (and hence GS (f, g)) Qm!0 \ {0} natural way.m!total preorder O, let RO = x Q0 \ {0} : f (x) . definition, x, ROg (f (x)) = g (f (y)), i.e., GS (f, g) constant region RO . region ROQ-convex cone, i.e. x, RO Q [0, 1], x + (1 ) RO ,furthermore Q>0 , x RO (both properties follow immediatelyDefinition 1). Thus write Qm!0 \ {0} disjoint union Q-convex cones{RO }O . way taking finitely many hyperplanes Rm! cuttingQm!0 \ {0} using hyperplanes; precise statement found Appendix A.essentially follows result Kemperman (1986, Thm. 2)to keep paper selfcontained reproduce Appendix results proof, show statementfollows results. Since function homogeneous, need lookm!valuesGSm!(f,g) simplex . Bym!the above, simplex dividedregions ROvia affine hyperplanes , function GS (f, g) constantm!RO total preorder O, GS (f, g) indeed hyperplane rule.3.1.4 Examplescommonly used SCFs generalized scoring rules / hyperplane rules, includingpositional scoring rules, instant-runoff voting, Coombs method, contingent vote,Kemeny-Young method, Bucklin voting, Nansons method, Baldwins method, Copelandsmethod, maximin, ranked pairs. examples already shown XiaConitzer (2008b, 2009), nevertheless Appendix B detail explanations manyexamples. main reason perspective hyperplane rulearguably makes explanations simpler clearer. rule fitframework Dodgsons rule, homogeneous (see, e.g., Brandt, 2009), therefore hyperplane rule.3.2 Small Coalitions Generalized Scoring Rulesshow generalized scoring rules, coalition size c n small enoughc change outcome election small probability. equivalenceabove, work framework hyperplane rules.consider two metrics m! : L1 metric, denoted d1 kk1 , L2metric, denoted d2 kk2 . L1 metric important setting, since changingvotes voters corresponds moving L1 metric m! ; connectionformalized following lemma.n . (x ( n ) , x ( n )) 2 ( n , n ),Lemma 3.2. Let n , n Sm1H denotesn HPHamming distance, i.e., dH ( n , n ) = ni=1 1 [i 6= ]. Furthermore, Dn ,n x ( n ) = (x ( n ) , y) = 2 ( n , n ).exists n Sm1n HProof. Let 0 = n , = 1, . . . , n, define ranking profile= (1 , . . . , , i+1 , . . . , n ) .933fiMossel, Procaccia, & Raczdefinition, n = n . desired inequality follows triangle inequality:nXd1 (x ( n ) , x ( n )) = d1 x 0 , x ( n )d1 x i1 , xi=1nX221 [i 6= ] = dH ( n , n ) .=nni=1second part lemma, construct n follows. Sm , let :={i [n] : = }. x ( n ) , every , let = . x ( n ) > ,choose subset indices I0 size |I0 | = ny , every I0 , let = . Finally,define rest coordinates n x ( n ) = y. construction guaranteesd1 (x ( n ) , y) = n2 dH ( n , n ).therefore natural define distances boundary B using L1 metric:Definition 6 (Blowup boundary). > 0, define blowup boundary BnB + = m! : x B kx yk1 .order coalition able change outcome election givenranking profile, point simplex corresponding ranking profile needssufficiently close boundary B; formulated following lemma.Lemma 3.3. Suppose n voters, coalition size k, ranking profilen , corresponds point x ( n ) m! probability simplex.n Smnecessary condition coalition able change outcome electionposition x ( n ) B +2k/n . Conversely, x ( n ) B +(2km!)/n , existscoalition size k change outcome election.Proof. ranking profile n coalition reach, dH ( n , n ) k,n / B +2k/n , everyLemma 3.2 d1 (x ( n ) , x ( n )) 2kn . x ( )ranking profile n coalition reach, x ( n ) x ( n ) regiondetermined hyperplanes, F (x ( n )) = F (x ( n )), i.e., coalition cannotchange outcome election.suppose x ( n ) B +(2km!)/n . definition exists Bm!d1 (x ( n ) , y) 2km!n . Since B, exists Dn d1 (y, y) nF (y) 6= F (x ( n )). triangle inequality, d1 (x ( n ) , y) 2kn , secondn x ( n ) = ( n , n ) k.part Lemma 3.2 exists n SmHcoalition consisting voters indices := {i [n] : 6= } thus changeoutcome election.Corollary 3.4. n voters, probability coalition size k changeoutcome election bounded P x ( n ) B +(2km!)/nP x ( n ) B +2k/n , n drawn according probability distributionsatisfying conditions setup.934fiA Smooth Transition Powerlessness Absolute PowerGaussian limit. Due i.i.d.-ness votes, multinomial random variable x ( n )concentrates around expectation, rescaled random variablex ( n ) :=n (x ( n ) E (x ( n )))converges normal distribution, zero mean specific covariance structure.analysis better use Gaussian picture, thus reformulatepreliminaries limiting setting. First, let us determine limiting distribution.Lemma 3.5. x ( n ) n N (0, ), covariance structure given= diag (p) ppT , recall p distribution vote.Proof. clear PE (x ( n )) = 0. Computingcovariance structure, first12E x = n2 ni,j=1 P (i = , j = ) = 1 n1 p ()2 + n1 p (),Var (x ) = n1 p () p ()2 thus Var (x ) = p () p ()2 . similarly 6= 0n1 X1 X100E (x x0 ) = 2p () p 0 ,P = , j = = 2p () p = 1nnni,j=1i6=jCov (x , x0 ) = n1 p () p ( 0 ) thusCov (x , x0 ) = p () p 0 .Note: concentration x ( n ) around mean, assumptionevery n every candidate [m], P (f ( n ) = a) , necessary every> 0 every candidate [m] exists m! ky E (x (1 ))k1F (y) = a.Denote distribution N (0, ) let X denote random variable distributedaccording . Note degenerate multivariate normal distribution, supportconcentrates hyperplane H0 coordinates sum zero. (ThisPnSm x ( ) = 0.)underlying function F : m! [m] corresponds function F : Rm! |H0 [m]Gaussian limit, functionnF partitionsRm! |H0 parts basedoutcome F . denote parts Wa. need following definitionsa[m]properties boundaries, analogous above.Definition 7 (Interior boundariespartition). say x Rm! |H0ninterior point partition Wainduced F exists > 0a[m]R |H0 kx yk1 , F (x) = F (y). Otherwise, sayx Rm! |H0 boundary partition, denote B.m!Lemma 3.6. boundary B comes hyperplane rule, i.e., B containedunion ` affine hyperplanes m! , B contained union ` hyperplanesRm! |H0 , ` `.935fiMossel, Procaccia, & RaczProof. Two things happen affine hyperplane H m! take Gaussianlimit: (1) E (x ()) H, translation E (x ()) takes H hyperplane HRm! |H0 , since H goes origin, scaling (in particular n) movehyperplane; (2) E (x ())/ H, translation E (x ()) takes H affinem!hyperplane H R |H0 go origin, scaling n movesH affine hyperplane Rm! |H0 whose L2 distance origin proportionaln, n limit affine hyperplane vanishes.Definition 8 (Blowup boundary). > 0, define blowup boundary BnB + = Rm! |H0 : x B kx yk1 .Let us focus specifically coalition size c n (small) constant c. Corollary 3.4 implies following.Corollary 3.7. hyperplane rules limit probability electionnvoters coalition size c n change outcome election X B +2c .following claim, together Corollary 3.7, tells us hyperplane rulescoalition size c n change outcome election small probability,given c sufficiently small, proving Part 2 Theorem 1.2.n oMClaim 3.8. Suppose SCF hyperplane rule, particular let Hii=1collection hyperplanes Rm! |H0 Bi=1 Hi .r 2 Mc+c.X BProof. condition union boundX+cX BX Hi+c .i=1hyperplane H Rm! |H0 , denote (one of) corresponding unit normal vector(s) (inhyperplane H0 ) u.nH = x Rm! |H0 : u x = 0since L1 distance always greater L2 distance,nnH +c x Rm! |H0 : H kx yk2 c = x Rm! |H0 : |u x| c .Since X multidimensional Gaussian r.v., u X one-dimensional Gaussian r.v. (whichcentered). Therefore2cX H +c u X [c, c] r.2 Var u X936fiA Smooth Transition Powerlessness Absolute Power2Var u X = E u X = E uT X X u = uT u,remains showminu:kuk=1,u1uT u ,1 m!-dimensional vector 1 every coordinate.Let 1 () 2 () m! () denote eigenvalues . Since positivesemidefinite, eigenvalues nonnegative. know 0 eigenvalue (thecorresponding eigenvector 1), m! () = 0. variational characterizationeigenvaluesminuT u = m!1 () ,u:kuk=1,u1need show m!1 () . use Weyls inequalities.Lemma 3.9 (Weyls inequalities). n n matrix let 1 (M ) 2 (M )n (M ) denote eigenvalues. C n n symmetric matricesj (A + C) (A) + ji+1 (C)j,j (A + C) (A) + ji+n (C)j.use Weyls inequality = diag (p) C = ppT . eigenvalues{p ()}Sm , less . Since C rank 1, eigenvalues onezero, single nonzero eigenvalue m! (C) = pT p. Since = diag (p)ppT = A+C,Weyls inequality tells usm!1 () m! (diag (p)) + m!1 ppT + 0 = .implies lower bound ( n) size coalition neededorder change outcome election hyperplane rules. mentioned before,commonly occurring SCFs class rules: see Appendix B many examples.3.2.1 Almost Hyperplane RulesFurthermore, Gaussian limiting setting sensitive small changesvoting rule finite n. Consequently, SCFs almost hyperplane rules (insense make precise below), conclusion holds: coalition size ( n) neededorder able change outcome election non-negligible probability.particular, result holds SCFs arbitrary tie-breaking rules rankingprofiles lie one hyperplanes (e.g., tie-breaking rule dependnumber voters n).Definition 9 (Almost hyperplane rules). Fix finite set affine hyperplanessimplex m! : H1 , . . . , H` . partition simplex finitely many regions. Let F :m! [m] function constant region, let B denote937fiMossel, Procaccia, & Raczn [m], calledinduced boundary. sequence SCFs {fn }n1 , fn : Smalmost hyperplane rule every n x ( n )/ B +o(1/ n) ,fn ( n ) = F (x ( n )) .SCF called almost hyperplane rule induced affine hyperplanes H1 , . . . , H`function F .n [m], almostLemma 3.10. Suppose sequence SCFs {fn }n1 , fn : Smhyperplane rule defined ` hyperplanes. Gaussian limiting setting boundaryB contained union ` hyperplanes Rm! |H0 , ` `.Proof. finite n, induced boundary fn simplex m! contained B +o(1/ n) ,definition. Since Gaussian limit scale n, blowup (1/ n)boundary B disappears limit, hence back situation Lemma 3.6.Consequently, affine hyperplanes corresponding almost hyperplane rule eitherdisappear infinity become hyperplanes Rm! |H0 .Corollary 3.11. Corollary 3.7 Claim 3.8 hold almost hyperplane rules well.3.3 Smoothness Phase Transitionfinal subsection goal show Parts 1 3 Theorem 1.2. existencelimits Part 1 follows immediately Gaussian limit described above;detail this, rather give formulas limiting probabilities. implyproperties described Part 3 theorem.following let hyperplane rule given affine hyperplanes H1 , . . . , H`m! function F : m! [m]; limiting setting denote H1 , . . . , H`corresponding hyperplanes Rm! |H0 denote F : Rm! |H0 [m] correspondingfunction.3.3.1 Quantities q qx Rm! |H0 define(x) :=infd1 (x, y) ,(x) := maxinfa[m] y:F (y)=ay:F (y)6=F (x)d1 (x, y) .previous subsection immediate writeq (c) = X : X 2c ,q (c) = X : X 2c .important note boundary B contained union finitely manyhyperplanes, H1 , . . . , H`, thus regions F constant convex conesintersection finitely many halfspaces.Consequently (x) either d1 (x,0),m!d1 x, Hj =0 denotes origin R , d1 x, Hj 1 j `,938fiA Smooth Transition Powerlessness Absolute Powerinf yHj d1 (x, y). scale x positive constant , distance originevery hyperplane scales well (i.e., d1 (x, 0) = d1 (x, 0) d1 x, Hj =d1 x, Hj ), thus every > 0, (x) = (x). Consequently,write x = kxk2 s, m!1 , m!1 denotes (m! 1)-sphere (notn , set ranking profiles n voters candidates),confused Sm(x) = kxk2 (s).scaling property holds well, hence(3)q (c) = X : X 2c ,2q (c) = X : X 2c .(4)2Recall condition every [m], P (f () = a) , implies every> 0 every [m] exists x Rm! |H0 kxk2 F (x) = a.Consequently every x Rm! |H0 must(x) d1(x, 0) (x) d1 (x, 0).m!1particular,d1 (s, 0) m!d2 (s, 0) = m! (s) , (s) m!.immediately implies every c > 02cq (c) X : X> 0.2m!show q (c) < 1, note since boundary contained union finitelymany hyperplanes, exists m!1 (s ) > 0. continuity ,exists neighborhood U m!1 every U , (s) (s ) /2.4cx x/ kxk2 U kxk2 > (s) ,(x) = kxk2 (x/ kxk2 ) >4c (s )= 2c.(s ) 2consequentlyq (c) 1 X : X/ X U, X >224c(s )< 1.Finally, fact q (c) q (c) continuously differentiable followsformulas (3) (4), since q (c) q (c) written Gaussian volumesubset Rm! |H0 , cases subset grows continuously c increases.derivative q (c) q (c) bounded zero (by Corollary 3.7 Claim 3.8),c derivative approaches zero, since derivative continuous, mustbounded constant whole half-line.3.3.2 Quantities r rprevious setup coalition size c n specified, ranking profilecould changed arbitrarily within Hamming ball radius c n. probabilitysimplex m! corresponded L1 ball radius 2c/ n, rescaled limiting939fiMossel, Procaccia, & Raczsetting corresponded L1 ball Rm! |H0 radius 2c. coalition sizec n specified, things slightly different. particular, look probabilitydistribution probability simplex m! induced distribution ranking profiles(or, limiting setting, Gaussian distribution Rm! |H0 ), lost trackvotes specific coalition. Nonetheless, Gaussian limiting setting still providesformulas limiting probabilities r (c) r (c).firstdraw random ranking profile n c n voterscoalition, nc n , voters coalition set votes arbitrarily.question is, coalition affect outcome vote? particular, (a)change outcome election, (b)candidate?electranking profile nc n corresponds point x nc n probability simplex m! , setting votesmove point probabilitythecoalitionncnsimplex neighborhood x. omit calculation finite npresent result limiting setting.Suppose limiting ranking profile voters coalition correspondspoint x Rm! |H0 . set points coalition reach following:nRc (x) := Rm! |H0 : Sm : x + cp () 0 .definen(x) := inf : R (x) F (y) 6= F (x) ,n(x) := inf : [m] R (x) F (y) = ,follows immediately writer (c) = X : X c ,r (c) = X : X c .way Section 3.3.1 one argue scale: > 0(x) = (x) (x) = (x). Hencer (c) = X : X c ,(5)2r (c) = X : X c .(6)2every 0 < c < r (c) q (c) < 1 (using Section 3.3.1). Let us showalso r (c) > 0. claim m!1 |H0 , (s) 2 . follows factm!1 |H0 m!1 |H0 R 2 (s), true m!1 |H0Sm , + 2 p () 1 1 + 2 = 0. Thuscr (c) X : X>022claimed.940fiA Smooth Transition Powerlessness Absolute PowerFinally, fact r (c) r (c) continuously differentiable followsformulas (5) (6) using argument given above: r (c) r (c) writtenGaussian volume subsets Rm! |H0 , subsets grow continuously c increases.derivative r (c) r (c) bounded zero (by Corollary 3.7 Claim 3.8),c derivative approaches zero, since derivative continuous,must bounded constant whole half-line.Acknowledgmentsthank anonymous referees helpful comments. E.M. supported NSF (DMS1106999) DOD ONR grant N000141110140, M.Z.R. supported UCBerkeley Graduate Fellowship NSF (DMS 0548249).Appendix A. Decomposing Rd Disjoint Union Finitely ManyConvex Cones: Via Hyperplanesorder paper self-contained, reproduce main definitionsresults Kemperman (1986) make precise claim used proof Lemma 3.1way decompose Qd0 \ {0} disjoint union finitely many Q-convexcones via hyperplanes. Kempermans paper deals convex sets general,summarize results convex cones relevant us. Kempermans resultspertain finite dimensional linear spaces state form; endshow results Rd0 follow immediately these, consequence alsoobtain claim used proof Lemma 3.1.Let us start main definitions. following, linear spacesreals finite dimensional. Let X linear space. convex cone subset K Xx, K > 0 imply x + K x K. (We require0 K.) set X, denote affine hull aff (A), convex hull cvx (A),closure cl (A). Note K X convex cone, aff (K) linear subspaceX.define two special types convex cones: basic convex cones elementary convexcones.Definition 10 (Basic convex cone). Let K convex cone finite dimensional linearspace X. say K basic convex cone (in X) K member K = K0partition1 . . . KrX = K0 KX finitely many disjoint convex cones {Ki }ri=0 .Note linear subspace X basic convex cone, immediatelyfollows K basic convex cone X basic convex cone aff (K).order define elementary convex cones, need definitions.Definition 11 (Open polyhedral convex cone). Let K convex cone finite dimensional linear space X. say K open polyhedral convex cone relative X K941fiMossel, Procaccia, & Raczexpressed intersection finitely many open halfspaces H1 , . . . , H` X,origin boundary. whole linear space X open polyhedralconvex cone ` = 0.Definition 12 (Relatively open polyhedral convex cone). Let K convex conefinite dimensional linear space X. K relatively open polyhedral convex coneeither K = K open polyhedral convex cone relative aff (K).Definition 13 (Elementary convex cone). Let K convex cone finite dimensionallinear space X. say K elementary convex cone K representeddisjoint union finitely many relatively open polyhedral convex cones.main result Kemperman concerning convex cones following (Kemperman,1986, Thm. 2).Theorem A.1. Let K convex cone Rd . K basic convex coneelementary convex cone.Lemma 3.1 use direction, thus leave proofdirection exercise reader.Proof direction. Let X finite dimensional linear space let K basicconvex cone X dimension = dim (K) = dim (Y ), = aff (K). proveinduction following:(i) relative interior K, denoted K 0 , relatively open polyhedral convex cone.(ii) K 0 6= , denote F1 , . . . , F` (d 1)-dimensional hyperplanescor0responding finitely many faces polyhedron cl (K) = cl K .convex cones Fi K, = 1, . . . , `, elementary convex cones dimension1 (but need disjoint).(iii) convex cone K also elementary convex cone.K = , properties (i) - (iii) hold. = 0, necessarily K = {0}, since Kconvex cone, K satisfies properties (i) - (iii) above.may assume 1 basic convex cone dimension1 satisfies properties (i) - (iii) above. Since K basic convex cone, existspartition1 . . . Kr= K0 K(7)finitely many disjoint convex cones {Kj }rj=0 , K0 = K. may assumer 0 minimal, hence Kj non-empty. Note K 0 also non-empty sincedim (K) = dim (Y ).r = 0 K = K0 = properties (i) - (iii) immediately satisfied,may assume r 1. j = 1, . . . , r, let Hj hyperplane separatesconvex cone K = K0 non-empty interior K 0 non-empty convex cone Kj .(Such hyperplanes exist hyperplane separation theorem, and, moreover,hyperplane goes origin, Kj contains least one point every942fiA Smooth Transition Powerlessness Absolute Poweropen ball around origin, since Kj cone.) Let Hj0 associated open halfspace contains interior K 0 K. LetL0 = H10 Hr0 .L0 polyhedral convex cone, open relative , contains interiorK 0 K.claim L0 = K 0 . enough show L0 K, L0 K 0follows definition K 0 . Suppose contrary exists x L0x/ K. partition (7) must exist index 1 j r x Kj ./ L0 , contradiction. proves (i).implies x/ Hj0 thus xlet us show (ii). (7), write linear space Fi disjoint unionconvex cones Fi Kj , j = 0, . . . , r, thus Fi K basic convex cone hence,induction, elementary convex cone.Finally, let us show K elementary convex cone. Since K 0 polyhedralconvex cone open relative , remains show K \ K 0 writtenfinite disjoint union relatively open polyhedral convex cones. (ii), writeK \ K 0 finite union elementary convex cones:K \ K 0 = `i=1 (Fi K) ,remains show write finite disjoint union relativelyopen polyhedral convex cones. may assume w.l.o.g. Fi K 6=(Fi K) * (Fj K) 6= j (otherwise leave Fi K union).claim every i,[(Fj Fi K) ,(8)rel int (Fi K) (Fi K) \j6=iimmediately follows rel int (Fi K) rel int (Fj K) = 6= j.show (8), let two open halfspaces either side hyperplane Fj denotedFj+ Fj . W.l.o.g. assume K Fj = . Since (Fi K) * (Fj K), must(Fi K) Fj+ 6= . Let x (Fi K) Fj+ let Fj Fi K. Since Fi K convex,interval x contained Fi K, (Fi K) Fj = , pointsline past point Fi K; hence/ rel int (Fi K).Since Fi K basic convex cone, rel int (Fi K) relatively open polyhedralconvex cone induction. Fi K = aff (Fi K) rel int (Fi K) = Fi K. not,denote Fi,1 , . . . , Fi,`i hyperplanes aff (Fi K) corresponding finitelymany faces polyhedron cl (Fi K). induction, convex cones Fi,j Fi K,j = 1, . . . , `i , elementary convex cones, write[``K \ K 0 = i=1 rel int (Fi K)i=1 `j=1(Fi,j Fi K) .remains shown `i=1 `j=1(Fi,j Fi K) written finite disjointunion relatively open polyhedral convex cones; follows iterating previousargument.943fiMossel, Procaccia, & RaczLet us show Rd0 basic convex cone Rd . = 1, . . . , d, defineclosed halfspace Hi0 = x Rd : xi 0 complement Hi<0 = x Rd : xi < 0 ,define convex cones0Ki = H10 Hi1Hi<0 ,= 1, . . . , d.write Rd disjoint union convex cones Rd0 K1 , . . . , Kd , showingindeed Rd0 basic convex cone. implies write Rd0 disjointunion convex cones C1 , . . . , Cr , Ci basic convex cone, hence,Theorem A.1, elementary convex cone.let us turn claim proof Lemma 3.1. Lemma 3.1, write Qm!0 \ {0}m!1 . . . Cr .disjoint union finitely many Q-convex cones: Q0 \ {0} = C0 Cm!= 0, . . . , r, let Ci = cvx (Ci ). known (see, e.g., Young, 1975) Ci = Q Ci .Ci therefore disjoint convex cones satisfyC0 C1 . . . Cr Rm!0(9)cl C0 cl C1 cl Cr = Rm!0 .(10)goal show Ci elementary convex cone. Conditions (10) (9)similar definition basic convex cone; spirit let us introducefollowing definition.Definition 14 (Basic convex cone closure). Let K0 convex cone finitedimensional linear space X. say K0 basic convex cone closure (in X)exist disjoint convex cones K1 , . . . , Kr1 . . . KrXK0 Kcl (K0 ) cl (K1 ) cl (Kr ) = X.Since Rd0 basic convex cone, Ci basic convex cones closure.fact, every basic convex cone closure elementary convex cone; proofexactly one shown direction Theorem A.1, oneneeds replace basic convex cone basic convex cone closure everywhereproof, make appropriate changes. Moreover, direction Theorem A.1implies actually every basic convex cone closure basic convex cone.Hence Ci elementary convex cones, need Lemma 3.1.Appendix B. Voting Rules Hyperplane Rules: Examplesfollowing show positional scoring rules, instant-runoff voting, Coombsmethod, contingent vote, Kemeny-Young method, Bucklin voting, Nansons method,Baldwins method, Copelands method hyperplane rules.944fiA Smooth Transition Powerlessness Absolute PowerPositional scoring rules. Let w Rm weight vector. Givenranking profilePvector , (normalized) score candidate [m] sa = n1 ni=1 w i1 (a) .positional scoring rule associated weight vector w elects candidatehighest score. (In case tie, tie-breaking rule,care here.) denote SCF n voters fnw . Examples includeplurality (with weight vector w = (1, 0, 0, . . . , 0)), Borda count (with weight vectorw = (m 1, 2, . . . , 0)) veto (with weight vector w = (1, 1, . . . , 1, 0)).sequence SCFs {fnw }n1 associate function F w : m! [m]followingPway. candidate[m] x m! , define (normalized) scoresa (x) = Sm x w 1 (a) , letF w (x) := arg max sa (x) ,a[m]arg max unique, unique, tie-breaking rule.construction guarantees fnw = F w |Dn . candidates 6= b, definenHa,b := x m! : sa (x) = sb (x) ,affine hyperplaneprobability simplex m! . Clearly boundaryB w contained union 2 affine hyperplanes:[BwHa,b .a6=b[m]Instant-runoff voting. candidate receives absolute majority first preferencevotes, candidate wins. candidate receives absolute majority,candidate fewest top votes eliminated. next round votescounted again, ballot counted one vote advancing candidateranked highest ballot. repeated winning candidate receivesmajority vote remaining candidates.boundary corresponds two kinds situations: either (1) tietop end, two candidates remain; (2) tie eliminatingcandidate end one rounds. Technically situation (1) also containedsituation (2), since end one view choosing winner eliminatingsecond placed candidate. One see candidates b tiedelimination candidates C [m] \ {a, b} (where C = allowed)eliminated, necessarilyXXXXx =x .(11)C 0 C {(1),...,(|C 0 |)}=C 0 ,(|C 0 |+1)=aC 0 C {(1),...,(|C 0 |)}=C 0 ,(|C 0 |+1)=bConsequently, denoting sa,C (x) quantity left hand side (11),boundary B contained union m2 2m affine hyperplanes:n[[Bx m! : sa,C (x) = sb,C (x) .a6=b C[m]\{a,b}945fiMossel, Procaccia, & RaczCoombs method. similar IRV, elimination rule different.candidate receives absolute majority first preference votes, candidatewins. candidate receives absolute majority, candidate rankedlast voters eliminated. next round votes counted again,ballot counted one vote advancing candidate ranked highestballot. repeated winning candidate receives majorityvote remaining candidates.boundary corresponds two kinds situations: either (1) tietop end, two candidates remain; (2) tie eliminatingcandidate end one rounds. Technically situation (1) also containedsituation (2), since end one view choosing winner eliminatingsecond placed candidate. One see candidates b tiedelimination candidates C [m] \ {a, b} (where C = allowed)eliminated, necessarilyXXXXx =x .(12)C 0 C {(m),...,(m|C 0 |+1)}=C 0 ,(m|C 0 |)=aC 0 C {(m),...,(m|C 0 |+1)}=C 0 ,(m|C 0 |)=bConsequently, denoting sa,C (x) quantity left hand side (12),boundary B contained union m2 2m affine hyperplanes:B[[x m! : sa,C (x) = sb,C (x) .a6=b C[m]\{a,b}Contingent vote. also similar IRV, except two candidatesget eliminated first round. candidate receives absolute majority firstpreference votes, he/she wins. candidate receives absolute majority,top two leading candidates eliminated second count,votes supported eliminated candidate redistributed amongtwo remaining candidates. candidate achieves absolute majoritywins.boundary B corresponds two kinds situations: either (1)two distinct top candidates, votes voters votedcandidates redistributed, two top candidates dead heat; (2)two candidates receive equal number votes firstround. situations described subsets affine hyperplanes,B contained union (m 1) affine hyperplanes:[XXXXm!Bx :x +x =x +xa6=b:(1)=a:(1)=b:(1){a,b},a/>b:(1){a,b},b/>a[XXx m! :x =x .a6=b:(1)=a:(1)=b946fiA Smooth Transition Powerlessness Absolute PowerKemeny-Young method. Denote K Kendall tau distance, metricpermutations counts number pairwise disagreements twopermutations, i.e.,XK (1 , 2 ) =1 [a b opposite order 1 2 ] ,{a,b}sum unordered pairs distinct candidates. Given rankingprofile n , Kemeny-Young method selects ranking minimizes sumKendall tau distances votes:= arg minnXK (i , ) ,i=1winner election declared (1). us convenientwriteX= arg minx ( n ) K (, ) .boundaryBPP must exist two rankings 1 21 (1) 6= 2 (1) x K (, 1 ) = x K (, 2 ). Thus B containedunion (m!)2 affine hyperplanes:()XX[m!x :x K (, 1 ) =x K (, 2 ) .B1 6=2Bucklin voting. First every candidate gets point voters rankedtop. candidate majority (i.e., n/2points), candidate wins. not, every candidate gets pointvoters ranked second. candidaten/2 points this, candidate points wins (there mightmultiple candidates n/2 points given round). processiterated candidate n/2 points.point boundary B corresponds situation pair candidates number points number rounds. Therefore Bcontained union m2 (m 1) /2 affine hyperplanes:kk[ [XXXXBx m! :x =x .a6=b k=1i=1 :(i)=ai=1 :(i)=bNansons method. Borda count combined variation instantrunoff voting procedure. First, Borda scores candidates computed,candidates Borda score greater average Borda scoreeliminated. Borda scores remaining candidate recomputed,eliminated candidates ballot. repeatedfinal candidate left.947fiMossel, Procaccia, & Raczboundary corresponds situations candidates Borda score exactly equalsaverage score candidates eliminated. C [m], denotesa,C (x) score candidate exactly candidates C eliminated(sa,C (x) linear function {x }Sm ), denote sC (x) average scoreremaining candidates exactly candidates C eliminated.boundary B contained union m2m affine hyperplanes:n[[Bx m! : sa,C (x) = sC (x) .a[m] C[m]\{a}Baldwins method. essentially Borda count combined instantrunoff voting procedure. First, Borda scores candidates computed,candidate lowest score eliminated. Borda scoresremaining candidate recomputed, eliminated candidateballot. repeated final candidate left.boundary corresponds ties eliminating candidate end onerounds. Borrow notation sa,C (x) previous example. boundary Bthus contained union m2 2m affine hyperplanes:n[[Bx m! : sa,C (x) = sb,C (x) .a6=b C[m]\{a,b}Copelands method. pairwise aggregation method: every candidate gets1 point candidate beats pairwise majority election, 1/2point candidate ties pairwise majority election. winnercandidate receives points. methodcorresponds cuttingsimplex m! finitely many regions viaaffinehyperplanes,2region winner candidate points.previous examples tie-breaking rules issue,become important. care tie-breaking rulesaffine hyperplane two candidates tie pairwise majority election.However, open regions intersection halfspaces defined affinehyperplanes candidates tied top scores.case, order Copeland hyperplane rule, need break tiesfavor candidate whole region. (This also ties brokenCopelands method Xia & Conitzer, 2008b.)Using tie-breaking rule Copelands method indeed hyperplane rule, sinceboundary contained union2 affine hyperplanes:[XXBx m! :x =x .a6=b:a>b948:b>afiA Smooth Transition Powerlessness Absolute PowerReferencesAchlioptas, D. (1999). Threshold phenomena random graph colouring satisfiability.Ph.D. thesis, Department Computer Science, University Toronto.Achlioptas, D., Naor, A., & Peres, Y. (2005). Rigorous location phase transitions hardoptimization problems. Nature, 435 (7043), 759764.Bartholdi III, J., Tovey, C., & Trick, M. (1989). Computational Difficulty Manipulating Election. Social Choice Welfare, 6 (3), 227241.Betzler, N., Niedermeier, R., & Woeginger, G. J. (2011). Unweighted coalitional manipulation Borda rule NP-hard. Proceedings 22nd International JointConference Artificial Intelligence (IJCAI), pp. 5560.Brandt, F. (2009). remarks Dodgsons voting rule. Mathematical Logic Quarterly,55 (4), 460463.Caragiannis, I., & Procaccia, A. D. (2011). Voting almost maximizes social welfare despitelimited communication. Artificial Intelligence, 175 (910), 16551671.Chamberlain, G., & Rothschild, M. (1981). note probability casting decisivevote. Journal Economic Theory, 25 (1), 152162.Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). really hard problems are.Proceedings 12th International Joint Conference Artificial Intelligence(IJCAI), pp. 331337.Conitzer, V., & Sandholm, T. (2006). Nonexistence Voting Rules UsuallyHard Manipulate. Proceedings 21st National Conference ArtificialIntelligence, Vol. 21, pp. 627634.Conitzer, V., Sandholm, T., & Lang, J. (2007). elections candidateshard manipulate?. Journal ACM, 54 (3), 133.Dobzinski, S., & Procaccia, A. (2008). Frequent Manipulability Elections: CaseTwo Voters. Proceedings 4th International Workshop InternetNetwork Economics, pp. 653664. Springer.Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). LlullCopeland Voting Computationally Resist Bribery Constructive Control. Journal Artificial Intelligence Research, 35, 275341.Faliszewski, P., & Procaccia, A. (2010). AIs War Manipulation: Winning?. AIMagazine, 31 (4), 5364.Friedgut, E., Kalai, G., Keller, N., & Nisan, N. (2011). Quantitative VersionGibbard-Satterthwaite Theorem Three Alternatives. SIAM J. Comput., 40 (3),934952.Friedgut, E., Kalai, G., & Nisan, N. (2008). Elections manipulated often. Proceedings 49th Annual Symposium Foundations Computer Science, pp.243249. IEEE.949fiMossel, Procaccia, & RaczFu, Y., & Anderson, P. (1986). Application statistical mechanics NP-complete problemscombinatorial optimisation. Journal Physics A: Mathematical General, 19,16051620.Gibbard, A. (1973). Manipulation Voting Schemes: General Result. Econometrica:Journal Econometric Society, 587601.Gomes, C., & Walsh, T. (2006). Randomness Structure. Rossi, F., van Beek, P.,& Walsh, T. (Eds.), Handbook Constraint Programming, Foundations ArtificialIntelligence, pp. 639664. Elsevier.Good, I., & Mayer, L. (1975). Estimating efficacy vote. Behavioral Science, 20 (1),2533.Isaksson, M., Kindler, G., & Mossel, E. (2012). Geometry Manipulation: Quantitative Proof Gibbard-Satterthwaite Theorem. Combinatorica, 32 (2), 221250.Kelly, J. (1993). Almost social choice rules highly manipulable, arent.Social Choice Welfare, 10 (2), 161175.Kemperman, J. (1986). Decomposing Rd finitely many semigroups. IndagationesMathematicae (Proceedings), Vol. 89, pp. 7178. Elsevier.Mossel, E., & Racz, M. (2012). quantitative Gibbard-Satterthwaite theorem withoutneutrality. Proceedings 44th ACM Symposium Theory Computing(STOC), pp. 10411060. ACM. Full version appear Combinatorica, availablearXiv preprint arXiv:1110.5888.Myatt, D. (2007). theory strategic voting. Review Economic Studies,74 (1), 255281.Peleg, B. (1979). note manipulability large voting schemes. Theory Decision,11 (4), 401412.Pritchard, G., & Slinko, A. (2006). average minimum size manipulating coalition.Social Choice Welfare, 27 (2), 263277.Pritchard, G., & Wilson, M. (2009). Asymptotics minimum manipulating coalitionsize positional voting rules impartial culture behaviour. Mathematical SocialSciences, 58 (1), 3557.Procaccia, A., & Rosenschein, J. (2007a). Average-case tractability manipulation voting via fraction manipulators. Proceedings 6th International ConferenceAutonomous Agents Multi-Agent Systems (AAMAS), pp. 718720.Procaccia, A., & Rosenschein, J. (2007b). Junta Distributions Average-case Complexity Manipulating Elections. Journal Artificial Intelligence Research, 28,157181.Satterthwaite, M. (1975). Strategy-proofness Arrows Conditions: Existence Correspondence Theorems Voting Procedures Social Welfare Functions. JournalEconomic Theory, 10 (2), 187217.Slinko, A. (2004). large coalition manipulate election?. MathematicalSocial Sciences, 47 (3), 289293.950fiA Smooth Transition Powerlessness Absolute PowerWalsh, T. (2002). Interface P NP: COL, XOR, NAE, 1-in-k, HornSAT. Proceedings 17th National Conference AI (AAAI 2002), pp. 695700.Walsh, T. (2011). Hard Manipulation Problems?. Journal ArtificalIntelligence Research, 42, 129.Xia, L. (2012a). Computing margin victory various voting rules. Proceedings13th ACM Conference Electronic Commerce (EC), pp. 982999. ACM.Xia, L. (2012b). Many Vote Operations Needed Manipulate Voting System?.Arxiv preprint arXiv:1204.1231.Xia, L., & Conitzer, V. (2008a). Sufficient Condition Voting Rules FrequentlyManipulable. Proceedings 9th ACM Conference Electronic Commerce(EC), pp. 99108. ACM.Xia, L., & Conitzer, V. (2008b). Generalized Scoring Rules Frequency CoalitionalManipulability. Proceedings 9th ACM Conference Electronic Commerce(EC), pp. 109118. ACM.Xia, L., & Conitzer, V. (2009). Finite Local Consistency Characterizes Generalized ScoringRules. Proceedings 9th International Joint Conference Artificial Intelligence (IJCAI), pp. 336341.Xia, L., Zuckerman, M., Procaccia, A. D., Conitzer, V., & Rosenschein, J. S. (2009). Complexity unweighted coalitional manipulation common voting rules.Proceedings 21st International Joint Conference Artificial Intelligence (IJCAI), pp. 348353.Young, H. (1975). Social choice scoring functions. SIAM Journal Applied Mathematics,824838.Zuckerman, M., Procaccia, A. D., & Rosenschein, J. S. (2009). Algorithms coalitionalmanipulation problem. Artificial Intelligence, 173 (2), 392412.951fiJournal Artificial Intelligence Research 48 (2013)Submitted 5/2013; published 12/2013Exact Query Reformulation DatabasesFirst-order Description Logics OntologiesEnrico FranconiVolha KerhetNhung Ngofranconi@inf.unibz.itkerhet@inf.unibz.itngo@inf.unibz.itFree University Bozen-Bolzano, ItalyAbstractstudy general framework query rewriting presence arbitraryfirst-order logic ontology database signature. framework supports decidingexistence safe-range first-order equivalent reformulation query termsdatabase signature, so, provides effective approach construct reformulation based interpolation using standard theorem proving techniques (e.g., tableau).Since reformulation safe-range formula, effectively executable SQL query.end, present non-trivial application framework ontologiesexpressive ALCHOIQ description logic, providing effective means computesafe-range first-order exact reformulations queries.1. Introductionaddress problem query reformulation expressive ontologies databases.ontology provides conceptual view database composed constraintsvocabulary extending basic vocabulary data. Querying database usingterms richer ontology allows flexibility using basicvocabulary relational database directly.paper study develop query rewriting framework applicable knowledgerepresentation systems data stored classical finite relational database, wayliterature called locally-closed world assumption (Etzioni, Golden,& Weld, 1997), exact views (Marx, 2007; Nash, Segoufin, & Vianu, 2010; Fan, Geerts,& Zheng, 2012), DBox (Seylan, Franconi, & de Bruijn, 2009; Franconi, Ibanez-Garcia,& Seylan, 2011). DBox set ground atoms semantically behaves likedatabase, i.e., interpretation database predicates DBox exactly equaldatabase relations. DBox predicates closed, i.e., extensionsevery interpretation, whereas predicates ontology open, i.e.,extensions may vary among different interpretations. consideropen interpretation database predicates (also called ABox sound views).ABox, interpretation database predicates contains database relations possiblymore. notion less faithful representation database semantics since wouldallow spurious interpretations database predicates additional unwanted tuplespresent original database.general framework ontology set first-order formulas, queries(possibly open) first-order formulas. Within setting, framework provides precisesemantic conditions decide existence safe-range first-order equivalent reformulac2013AI Access Foundation. rights reserved.fiFranconi, Kerhet, & Ngotion query terms database signature. also provides effective approachconstruct reformulation sufficient conditions. interested safe-range reformulations queries range-restricted syntax needed reduce originalquery answering problem relational algebra evaluation (e.g., via SQL) originaldatabase (Abiteboul, Hull, & Vianu, 1995). framework points several conditionsontologies queries guarantee existence safe-range reformulation.show conditions feasible practice also provide efficient methodensure validation. Standard theorem proving techniques used computereformulation.order complete, framework applicable ontologies queries expressedfragment first-order logic enjoying finitely controllable determinacy (Nash et al.,2010), stronger property finite model property logic. employed logicenjoy finitely controllable determinacy approach would become soundincomplete, still effectively implementable using standard theorem proving techniques.explored non-trivial applications framework complete; paper,application ALCHOIQ ontologies concept queries discussed. show(i) check whether answers given query ontology solely determinedextension DBox predicates and, so, (ii) find equivalent rewritingquery terms DBox predicates allow use standard database technologyanswering query. means benefit low computational complexitysize data answering queries relational databases. addition, possiblereuse standard techniques description logics reasoning find rewritings,paper Seylan et al. (2009).query reformulation problem received strong interest classical relationaldatabase research well modern knowledge representation studies. Differentlymainstream research query reformulation (Halevy, 2001), mostly basedperfect maximally contained rewritings sound views relatively inexpressive constraints (see, e.g., DL-Lite approach Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009), focus exact rewritings exact views, since characterisesprecisely query answering problem ontologies databases, caseexact semantics database must preserved. example, consider ground negative query given standard relational database; adding ontology top it,answer supposed changesince query uses signature databaseadditional constraints supposed change meaning querywhereasdatabase treated ABox (sound views) answer may change presenceontology. may important application perspective: DBox preservesbehaviour legacy application queries relational database. Moreover,focussing exact reformulations definable queries (as opposed considering certainanswer semantics arbitrary queries, DL-Lite), guarantee answersqueries subsequently composed arbitrary way: may important legacydatabase applications.work extends works exact rewritings exact views Marx (2007)Nash et al. (2010) focussing safe-range reformulations conditions ensuringexistence, considering general first-order ontologies extending databasesignature, rather local view constraints database predicates (Halevy,886fiExact Query Reformulation DBs FO DL Ontologies2001). paper extends papers Franconi, Kerhet, Ngo (2012a, 2012b)providing precise semantic characterisation existence exact reformulation(Theorem 4) opposed sufficient conditions, considering much expressive description logic ALCHOIQ, providing proofs.paper organised follows: section 2 provides necessary formal backgrounddefinitions; section 3 introduces notion query determined database; section 4 introduces characterisation query reformulation problem; sections 5 6conditions allowing effective reformulation analysed, sound complete algorithm compute reformulation introduced. Finally, present caseALCHOIQ ontologies. proofs presented details Appendix.2. PreliminariesLet FOL(C, P) classical function-free first-order language equality signature= (C, P), C finite set constants P set predicates associatedarities. rest paper refer arbitrary fragment FOL(C, P),called L.denote P{1 ,...,n } set predicates occurring formulas 1 , . . . , n ,C{1 ,...,n } set constants occurring formulas 1 , . . . , n ; sakebrevity, instead P{} (resp. C{} ) write P (resp. C ). denote (1 , . . . , n )signature formulas 1 , . . . , n , namely union P{1 ,...,n } C{1 ,...,n } .denote arity predicate P ar(P ). Given formula , denote setvariables appearing var(), set free variables appearingfree(); may use notation [X] , X = free() (possibly empty)set free variables formula.database (instance) DB finite set ground atoms form P (c1 , . . . , cn ),P P, n-ary predicate, ci C (1 n). set predicates appearingdatabase DB denoted PDB , set constants appearing DB calledactive domain DB, denoted CDB . (possibly empty) finite set KB closedformulas called ontology.usual, interpretation = hI , includes non-empty setthe domaininterpretation function defined constants predicates signature. sayinterpretations = hI , J = hJ , J equal, written = J , = J= J . interpretation embeds database DB, holds aI = everydatabase constant CDB (the standard name assumption (SNA), customary databases,see Abiteboul et al., 1995) (c1 , . . . , cn ) P P (c1 , . . . , cn ) DB.denote set interpretations embedding database DB E(DB).words, every interpretation embedding DB interpretationdatabase predicate always given exactly content database;is, general, case interpretation non-database predicates.say database predicates closed, predicates openmay interpreted differently different interpretations. consider openworld assumption (the ABox ) embedding database interpretation. openworld, interpretation soundly embeds database holds (c1 , . . . , cn ) P(but if) P (c1 , . . . , cn ) DB.887fiFranconi, Kerhet, & Ngoorder allow arbitrary database embedded, generalise standardname assumption constants C; implies domain interpretationnecessarily includes set constants C. finiteness C correspondsfinite ability database system represent distinct constant symbols; C meantunknown advance, since different database systems may different limits.see framework introduced depend choice C.Given interpretation = hI , i, denote I|S interpretation restrictedsmaller signature P C, i.e., interpretation domaininterpretation function defined constants predicates setS. semantic active domain signature 0 P C interpretation I, denotedadom( 0 , I), set elements domain occurring interpretationspredicates constants 0 I:adom( 0 , I) :=[[P 0 (a1 ,...,an )P{a1 , . . . , }[{cI }.c 00 PDB C, interpretations J embedding DB have:adom( 0 , I) = adom( 0 , J ); so, case introduce notation adom( 0 , DB) :=adom( 0 , I), interpretation embedding database DB. Intuitivelyadom( 0 , DB) includes constants 0 DB appearing relations corresponding predicates 0 .Let X set variable symbols set; substitution total function : X 7assigning element variable X, including empty substitutionX = . Domain image (range) substitution written dom() rng()respectively. Given subset set constants C0 C, write formula [X]true interpretation free variables substituted according substitution: X 7 C0 (I |= [X/] ). Given interpretation = hI , subsetdomain , write formula [X] true free variablesinterpreted according substitution : X 7 (I, |= ). extension domainaSformula [X] respect interpretation defined set domain elements{rng() | dom() = X, rng() , I, |= [X] }.usual, interpretation closed formula true called modelformula; set models formula (resp. KB) denoted () (resp.(KB)). database DB legal ontology KB exists model KB embeddingDB. following, consider consistent non-tautological ontologies legaldatabases.2.1 Queriesquery (possibly closed) formula. Given query Q[X] , define certain answerKB DB follows:Definition 1 (Certain Answer). (certain) answer query Q[X] database DBontology KB set substitutions constants:{ | dom() = X, rng() C, (KB) E(DB) : |= Q[X/] }.888fiExact Query Reformulation DBs FO DL OntologiesQuery answering defined entailment problem, going(high) complexity entailment.Note, query Q closed (i.e., Boolean query), certain answer {}Q true models ontology embedding database, otherwise.following, assume closed formula Q[X/] neither valid inconsistentontology KB, given substitution : X 7 C assigning variables distinct constantsappearing Q, KB, CDB : would lead trivial reformulations.show weaken standard name assumption constantsassuming unique names, without changing certain answers. said before,interpretation satisfies standard name assumption cI = c c C. Alternatively, interpretation satisfies unique name assumption (UNA) aI 6= bIdifferent a, b C. denote set interpretations satisfying standard nameassumption I(SNA). denote set interpretations satisfying unique nameassumption I(UNA). following proposition allows us freely interchange standard name unique name assumptions interpretations embedding databases.practical advantage, since encode unique name assumption classicalfirst-order logic reasoners, many description logics reasoners support nativelyunique name assumption extension OWL.Proposition 1 (SNA vs UNA). query Q[X] , ontology KB database DB,{ | dom() = X, rng() C, I(SNA) (KB) E(DB) : |= Q[X/] } ={ | dom() = X, rng() C, I(UNA) (KB) E(DB) : |= Q[X/] }.Since query arbitrary first-order formula, answer may dependdomain, know advance. example, query Q(x) = Student(x)database Student(a), Student(b), domain {a, b, c} answer {x = c},domain {a, b, c, d} answer {x = c, x = d}. Therefore, notiondomain independent queries introduced relational databases. adaptclassical definitions (Avron, 2008; Abiteboul et al., 1995) framework: needgeneral version domain independence, namely domain independence w.r.tontology, i.e., restricted models ontology.Definition 2 (Domain Independence). formula Q[X] domain independentrespect ontology KB iff every two models J KB (i.e., = hI ,J = hJ , J i) agree interpretation predicates constants (i.e.= J ), every substitution : X 7 J have:rng() I, |= Q[X] iffrng() J J , |= Q[X] .definition reduces classical definition domain independence wheneverontology empty.weaker version domain independencewhich relevant open formulasisfollowing.Definition 3 (Ground Domain Independence). formula Q[X] ground domain independent iff Q[X/] domain independent every substitution : X 7 C.889fiFranconi, Kerhet, & Ngoexample, formula P (x) ground domain independent, domain independent.problem checking whether FOL formula domain independent undecidable(Abiteboul et al., 1995). well known safe-range syntactic fragment FOL introducedCodd equally expressive language; indeed safe-range formula domain independent, domain independent formula easily transformed logicallyequivalent safe-range formula. Intuitively, formula safe-range variablesbounded positive predicates equalities (for full details see Appendix A.3).example, formula A(x) B(x) safe-range, queries A(x) x. A(x)not. check whether formula safe-range, formula transformed logicallyequivalent safe-range normal form range restriction computed according setsyntax based rules; range restriction formula subset free variables,coincides free variables formula said safe-range (Abiteboulet al., 1995). Similar domain independence, formula ground safe-range grounding formula safe-range. ontology KB safe-range (domain independent),every formula KB safe-range (domain independent).safe-range fragment first-order logic standard name assumptionequally expressive relational algebra, core SQL (Abiteboul et al.,1995).3. Determinacycertain answer query includes substitutions make query truemodels ontology embedding database: so, substitution would makequery true model, would discarded certain answer.words, may case answer query necessarily amongmodels ontology embedding database. case, query fullydetermined given source data; indeed, answer possible,certain. Due indeterminacy query respect data, complexitycompute certain answer general increases complexity entailmentlogic. paper focus case query answermodels ontology embedding database, namely, information requestedquery fully available source data without ambiguity. way,indeterminacy disappears, complexity process may decrease (see section 4).determinacy query w.r.t. source database (Nash et al., 2010; Marx, 2007; Fanet al., 2012) called implicit definability formula (the query) setpredicates (the database predicates) Beth (1953).Definition 4 (Finite Determinacy Implicit Definability). query Q[X] (finitely)determined (or implicitly definable from) database predicates PDB KB ifftwo models J ontology KBboth finite interpretationdatabase predicates PDB whenever I|PDB C = J |PDB C every substitution: X 7 have: I, |= Q[X] iff J , |= Q[X] .Intuitively, answer implicitly definable query depend interpretation non-database predicates. database domain fixed, never890fiExact Query Reformulation DBs FO DL Ontologiescase substitution would make query true model ontologyfalse others, since truth value implicitly defined query dependsinterpretation database predicates constants domain (whichfixed). practice, focussing finite determinacy queries guarantee useralways interpret answers certain, also exactnamelywhatever answer never part answer possible world.following focus ontologies queries fragments FOL(C, P)determinacy models finite interpretation database predicates (finitedeterminacy) determinacy models unrestricted interpretation databasepredicates (unrestricted determinacy) coincide. say fragments finitelycontrollable determinacy: require whenever query finitely determinedalso determined unrestricted models (the reverse trivially true). Indeed, resultspaper would fail finite determinacy unrestricted determinacy coincide:shown (Gurevich, 1984) Theorem 1 fails consider modelsfinite interpretation database predicates.Example 1 (Example database theory). Let P = {P, R, A}, PDB = {P, R},KB = {x, y, z. R(x, y) R(x, z) = z,x, y. R(x, y) z. R(z, x),(x, y. R(x, y) z. R(y, z)) (x. A(x) P (x))}.formula x, y. R(x, y) z. R(y, z) entailed first two formulasfinite interpretations R. query Q = A(x) finitely determined P (it equivalentP (x) models finite interpretation R), determineddatabase predicate models unrestricted interpretation R. knowledgebase enjoy finitely controllable determinacy.exact reformulation query (Nash et al., 2010) (also called explicit definitionBeth, 1953) formula logically equivalent query makes use databasepredicates constants.Definition 5 (Exact Reformulation Explicit Definability). query Q[X] explicitly definable database predicates PDB ontology KB iffb[X] FOL(C, P), KB |= X.Q[X] Qb[X] (Q)b PDB . callformula Qb[X] exact reformulation Q[X] KB PDB .formula QDeterminacy query completely characterised existence exact reformulation query: well known first-order query determined databasepredicates exists first-order exact reformulation.Theorem 1 (Projective Beth definability, Beth, 1953). query Q implicitly definable database predicates PDB ontology KB, iff explicitly definableb FOL(C, P) PDB KB.formula Qe formula obtained uniformly replacingLet Q formula L Qevery occurrence non-database predicate P new predicate Pe. extendrenaming operator e set formulas natural way. One check whether queryimplicitly definable using following theorem.Theorem 2 (Testing Determinacy, Beth, 1953). query Q[X] implicitly definableg |= X.Q[X] Qe[X] .database predicates PDB ontology KB iff KB KB891fiFranconi, Kerhet, & Ngo4. Exact Safe-Range Query Reformulationsection analyse conditions original query answering problemcorresponding entailment problem reduced systematically model checkingproblem safe-range formula database (e.g., using database systemSQL). Given database signature PDB , ontology KB, query Q[X] expressed Ldetermined database predicates, goal find safe-range reformulationb[X] Q[X] FOL(C, P), evaluated relational algebra expressionQlegal database instance, gives answer certain answer Q[X] databaseKB. reformulated following problem:Problem 1 (Exact safe-range Query Reformulation). Find exact reformulationb[X] Q[X] KB safe-range query FOL(C, P) PDB .QSince exact reformulation equivalent ontology original query,certain answer original query reformulated query identical.precisely, following proposition holds.Proposition 2. Given database DB, let Q[X] implicitly definable PDB KBb[X] exact reformulation Q[X] KB PDB , then:let Q{ | dom() = X, rng() C, (KB) E(DB) : |= Q[X/] } =b[X/] }.{ | dom() = X, rng() C, (KB) E(DB) : |= Qequation clear order answer exactly reformulated query,one may still need consider models ontology embedding database, i.e.,still entailment problem solve. following theorem states conditionreduce original query answering problembased entailmentto problemchecking validity exact reformulation single model: conditionreformulation domain independent. Indeed one interpretation(with particular domain) embedding database signature restricteddatabase predicates.Theorem 3 (Adequacy Exact safe-range Query Reformulation). Let DBb[X] exact domaindatabase legal KB, let Q[X] query. Qindependent (or safe-range) reformulation Q[X] KB PDB , then:{ | dom() = X, rng() C, (KB) E(DB) : |= Q[X/] } =b DB), = hC, E(DB) : I|P{ | dom() = X, rng() adom((Q),DB Cb[X/] }.|= Qsafe-range reformulation necessary transform first-order query relationalalgebra query evaluated using SQL techniques. theoremshows addition safe-range also sufficient property exact reformulation correctly evaluated SQL query. Let us see examplecannot reduce problem answering exact reformulation model checkingdatabase, exact reformulation safe-range.Example 2. Let P = {P, A}, PDB = {P }, C = {a},DB = {P (a, a)}, KB = {y. P (a, y) A(y)},b[X] = y. P (x, y) (i.e., X = {x}).Q[X] = Q892fiExact Query Reformulation DBs FO DL OntologiesC includes active domain CDB (it actually equal).DB legal KB = h{a}, P = {(a, a)}, AI =obviously, (KB).{ | dom() = X, rng() C, (KB) E(DB) : |= Q[X/] } =one take = h{a, b}, P = {(a, a)}, AI = {b}; (KB)E(DB), possible substitution {x a} have: 6|= P (a, y).However,b DB), = hC, E(DB) : I|P C |= Qb[X/] } ={ | dom() = X, rng() adom((Q),DB{x a}seen, answers query reformulation exists containconstants active domain database query; therefore, ground statements ontology involving non-database predicates non-active domain constants(for example, ABox statements) play role final evaluationreformulated query database.5. Conditions Exact Safe-Range Reformulationseen importance getting exact safe-range query reformulation.section going study conditions exact safe-range queryreformulation exists.First all, focus semantic notion safe-range namely domain independence. implicit definability isas already knowa sufficient conditionexistence exact reformulation, guarantee alone existence domainindependent reformulation.Example 3. Let P = {A, B}, PDB = {A}, KB = {x.B(x) A(x)}, Q = B(x).Q implicitly definable PDB KB, every exact reformulation QPDB KB logically equivalent A(x) domain independent.looking example, seems reason non domain independentreformulation lies fact ontology, domain independent, cannot guarantee existence exact domain independent reformulation non domain independentquery. However, let us consider following example:Example 4. Let PDB = {A, C}, KB = {A(a),B(y) C(x). easy see KBimplicitly definable PDB KB,independent reformulation Q.x. A(x) B(x)} let query Q =domain independent Q not. Qb = A(a) C(x) exact domainQobvious spite fact query Q domain independent,domain independent respect ontology KB. words, caseontology guarantees existence exact domain independent reformulation.queries domain independent respect ontology, followingtheorem holds, giving semantic requirements existence exact domainindependent reformulation.893fiFranconi, Kerhet, & NgoTheorem 4 (Semantic Characterisation). Given set database predicates PDB ,domain independent ontology KB, query Q[X] , domain independent exact reformub[X] Q[X] PDB KB exists Q[X] implicitly definablelation QPDB KB domain independent respect KB.theorem shows us semantic conditions exact domain independent reformulation query, give us method compute reformulation equivalent safe-range form. following theorem gives us sufficientconditions existence exact safe-range reformulation decidable fragmentFOL(C, P) finite unrestricted determinacy coincide, gives us constructiveway compute it, exists.Theorem 5 (Constructive). If:g |= X. Q[X] Qe[X] (that is, Q[X] implicitly definable),1. KB KB2. Q[X] safe-range (that is, Q[X] domain independent),3. KB safe-range (that is, KB domain independent),b[X] Q[X] safe-range query FOL(C, P)exists exact reformulation QPDB KB, obtained constructively.order constructively compute exact safe-range query reformulation useb[X]tableau based method find Craigs interpolant (Fitting, 1996) compute QgQe[X] ). See Section 6 full details.validity proof implication (KB Q[X] ) (KBLet us consider fully worked example, adapted paper Nash et al.(2010).Example 5. Given: P = {R, V1 , V2 , V3 , A}, PDB = {V1 , V2 , V3 , Adom} Adomactive domain DB,KB = { x, y. V1 (x, y) z, v. R(z, x) R(z, v) R(v, y),x, y. V2 (x, y) z. R(x, z) R(z, y),x, y. V3 (x, y) z, v. R(x, z) R(z, v) R(v, y),Q(x, y) = z, v, u. R(z, x) R(z, v) R(v, u) R(u, y)}.conditions theorem satisfied: Q(x, y) implicitly definable PDBKB; Q(x, y) safe-range; KB safe-range.b y)Therefore, tableau method one finds Craigs interpolant compute Q(x,gebvalidity proof implication (KB Q[X] ) (KB Q[X] ) obtain Q(x, y) =z. V1 (x, z) v. (V2 (v, z) V3 (v, y))an exact ground safe-range reformulation. Sinceb y) Adom(x)answer Q active domain, also KB |= Q(x,bAdom(y). KB |= Q(x, y) Q(x, y) Adom(x) Adom(y). Therefore, z. V1 (x, z)v. (V2 (v, z) V3 (v, y))Adom(x)Adom(y) exact safe-range reformulation Q(x, y)PDB KB.894fiExact Query Reformulation DBs FO DL Ontologies6. Constructing Safe-Range Reformulationsection introduce method compute safe-range reformulation implicitlydefinable query conditions theorem 5 satisfied. method basednotion interpolant introduced Craig (1957).Definition 6 (Interpolant). sentence interpolant sentenceFOL(C, P), predicate constant symbols set predicateconstant symbols , valid sentencesFOL(C, P).Theorem 6 (Craigs interpolation). valid sentence FOL(C, P),neither valid, exists interpolant.Note, Beth definability (Theorem 1) Craigs interpolation theoremhold fragments FOL(C, P): interpolant may always expressedfragment itself, obviously FOL(C, P) (because Theorem 6).interpolant used find exact reformulation given implicitly definablequery follows.Theorem 7 (Interpolant definition). Let Q[X] query n 0 free variablesimplicitly definable database predicates PDB ontology KB. Then,closed formula c1 , ..., cn distinct constant symbols C appearing KB Q[X] :^^g Qe[X/c ,...,c ] )(( KB) Q[X/c1 ,...,cn ] ) (( KB)(1)n1b[c ,...,c /X] exact reformulation Q[X] KBvalid, interpolant Qn1PDB .Therefore, find exact reformulation implicitly definable query termsdatabase predicates enough find interpolant implication (1)substitute constants c1 , . . . , cn back free variables X original query.interpolant constructed validity proof (1) using automated theoremproving techniques tableau resolution. order guarantee safe-rangeproperty reformulation, use tableau method book Fitting (1996).6.1 Tableau-based Method Compute Interpolantsection recall context tableau based method compute interpolant (Fitting, 1996).Assume valid, therefore unsatisfiable. closed tableaucorresponding . order compute interpolant tableau one needsmodify biased tableau.Definition 7 (Biased tableau). biased tableau formulas tree = (V, E)where:V set nodes, node labelled set biased formulas. biased formulaexpression form L() R() formula. node n,S(n) denotes set biased formulas labelling n.895fiFranconi, Kerhet, & Ngoroot tree labelled {L(), R()}E set edges. Given 2 nodes n1 n2 , (n1 , n2 ) E iff biasedcompletion rule n1 n2 . say biased completion rule n1n2() result applying rule X(), X refer L R(for rules, two possibilities choosing ()),S(n2 ) = (S(n1 ) \ {X()}) {Y ()}.Let C set constants input formulas tableau. C par extends Cinfinite set new constants. constant new occur anywheretableau. notations, following rules :Propositional rulesX()X()Negation rulesX(>)X()ruleX(1 2 )X()X(>)X(1 )X(2 )ruleX((1 2 ))X(1 ) | X(2 )First order rulesruleX(x.)ruleX(x.)X((t))C parX((c))new constant cEquality rulesreflexivity rulereplacement ruleX(t = u)((t))X()X(t = t)occurs((u))C parnode tableau closed contains X() (). node closed,rule applied. words, becomes leaf tree. branch closedcontains closed node tableau closed branches closed. Obviously,standard tableau FOL closed biased tableau vice versa.Given closed biased tableau, interpolant computed applying interpolant rules.intinterpolant rule written I, formula= {L(1 ), L(2 ), ..., L(n ), R(1 ), R(2 ), ..., R(m )}.Rules closed branchesintintr1. {L(), L()}r2. {R(), R()} >intintr3. {L()}r4. {R()} >intintr5. {L(), R()}r6. {R(), L()}896fiExact Query Reformulation DBs FO DL OntologiesRules propositional casesint{X()}p1.p4.p6.intp2.int{X()}int{X(1 ), X(2 )}{X(>)}intintp3.{X()}int{X()}{X(>)}intint{L(1 )} I1 {L(2 )} I2p5.intint{X(1 2 )}{L((1 2 ))} I1 I2intint{R(1 )} I1 {R(2 )} I2int{R((1 2 ))} I1 I2Rules first order cases :int{X((p))}f1.f2.f3.f4.f5.p parameter occurint{X(x.(x))}int{L((c))}intc occurs {1 , ..., n }{L(x.(x))}int{R((c))}intc occurs {1 , ..., }{R(x.(x))}int{L((c))}int{L(x.(x))} x.I[c/x]int{R((c))}intc occur {1 , ..., n }c occur {1 , ..., }{R(x.(x))} x.I[c/x]Rules equality casesint{X((p)), X(t = t)}e1.e3.intinte2.{X((p))}int{L((u)), R(t = u)}int{X((u)), X(t = u)}int{X((t)), X(t = u)}u occurs (t), 1 , ...,{L((t)), R(t = u)} = uint{R((u)), L(t = u)}u occurs (t), 1 , ...,int{R((t)), L(t = u)} = uint{L((u)), R(t = u)}e5.u occur (t), 1 , ...,int{L((t)), R(t = u)} I[u/t]int{R((u)), L(t = u)}e6.u occur (t), 1 , ...,int{R((t)), L(t = u)} I[u/t]e4.summary, order compute interpolant , one first need generatebiased tableaux proof unsatisfiability using biased completion rulesapply interpolant rules bottom leaves root.Let us consider example demonstrate method works.Example 6. Let P = {S, G, U }, PDB = {S, U },897fiFranconi, Kerhet, & NgoKB = { x(S(x) (G(x) U (x)))x(G(x) S(x))x(U (x) S(x))x(G(x) U (x))}Q(x) = G(x)Obviously, Q implicitly definable U , since ontology states GU partition S. follow tableau method find exact reformulation.intcompactness, use notation instead I.S0 = {L(x(S(x) (G(x) U (x)))),L(x(G(x) S(x))),L(x(U (x) S(x))),L(x(G(x) U (x))),L(G(c)),R(x(S(x) (G1 (x) U (x)))),R(x(G1 (x) S(x))),R(x(U (x) S(x))),R(x(G1 (x) U (x))),R(G1 (c))}applying rule removing implication, have:S1 = {L(S(c) G(c) U (c)),L(G(c) S(c))),L(U (c) S(c)),L(G(c) U (c)),L(G(c)),R(S(c) G1 (c) U (c)),R(G1 (c) S(c)),R(U (c) S(c)),R(G1 (c) U (c)),R(G1 (c))}interpolant S1 computed follows:S4 {R(S(c)}S(c)S4 {R(U (c))}U (c)S4 = S3 {R(S(c) U (c))}(S(c)U (c))S3 {R(G1 (c))}>(S(c)U (c))B.7S2 {L(G(c))}S3 = S2 {L(U (c))}(S(c)U (c))S2 = S1 {L(S(c))}(S(c)U (c))B.5S1 {L(G(c))}B.3S1bTherefore, S(c)U (c) interpolant Q(x)= S(x)U (x) exact reformulationQ(x).898fiExact Query Reformulation DBs FO DL OntologiesAlgorithm 1 Safe-range ReformulationInput: safe-range KB, safe-range implicitly definable query Q[X] .Output: exact safe-range reformulation.b[X] Theorem 71: Compute interpolant Qb[X]2: free variable x bounded positive predicate Qb[X] := Qb[X] Adom b (x)QQb[X]3: Return Q6.2 Safe-Range Reformulationwant show reformulation computed tableau based methodcondition Theorem 5 generates ground safe-range query.Theorem 8 (Ground safe-range Reformulation). Let KB ontology, let Qquery implicitly definable PDB . KB Q safe-range rewrittenb obtained using tableau method described Section 6.1 ground safe-range.query Qwords, conditions Theorem 8 guarantee quantified variablesreformulation range-restricted. need consider still unsafe free variables.theorem help us deal non-range-restricted free variables. Let us firstdefine active domain predicate query Q safe-range formula:Adom Q (x) :=WWP PQ z1 , . . . , zar(P )1 . P (x, z1 , . . . , zar(P )1 ) . . . P (z1 , . . . , zar(P )1 , x)cCQ (x = c).Theorem 9 (Range query). Let KB domain independent ontology, letQ[x1 ,...,xn ] query domain independent respect KB.KB |= x1 , . . . , xn . Q[x1 ,...,xn ] Adom Q (x1 ) . . . Adom Q (xn ).Given safe-range ontology, safe-range implicitly definable query obviouslydomain independent respect ontology. case, Theorem 9 saysanswer reformulation include active domain elements. Therefore, activedomain predicate used guard free variables boundedpositive predicate.Based Theorem 8 Theorem 9, propose complete procedure constructsafe-range reformulation Algorithm 1.7. Guarded Negation Fragment ALCHOIQALCHOIQ extension description logic ALC role hierarchies, individuals,inverse roles, qualified cardinality restrictions: corresponds SHOIQ description logic without transitive roles; logic basis OWL. syntaxsemantics ALCHOIQ concept expressions summarised Figure 1,atomic concept, C concepts, individual name, P atomic role,R either P P . forall qualified unqualified atmost operatorsderived using negation atleast operator usual way. TBox ALCHOIQ899fiFranconi, Kerhet, & NgoSyntax{o}PPCC uDC tDnRnR.CSemanticsAI{oI }P{(y, x)|(x, y) P }\CC DIC DI{x|#({y|(x, y) RI }) n}{x|#({y|(x, y) RI } C ) n}Figure 1: Syntax semantics ALCHOIQ concepts rolesset concept inclusion axioms C v role inclusion axioms R v (where C,concepts R, roles) usual description logics semantics.section, present application Theorem 5, introducing ALCHOIQGNdescription logic, guarded negation syntactic fragment ALCHOIQ (Figure 2)happens express exactly domain independent concepts TBoxes ALCHOIQ.language restricts ALCHOIQ prescribing negated conceptsguarded generalised atom (an atomic concept, nominal, unqualified atleastnumber restriction), i.e., absolute negation forbidden. Similarly, derived forallatmost operators would guarded using standard definition dualatleast operator, guarded negation. ALCHOIQGN actually intersection GNFO fragment (Barany, ten Cate, & Otto, 2012) ALCHOIQ (seeAppendix A.5 details GNFO).ALCHOIQGN important property coinciding domain independent fragment ALCHOIQ, therefore providing excellent candidate languageontologies queries satisfying conditions Theorem 5.Theorem 10 (Expressive power equivalence). domain independent fragmentALCHOIQ ALCHOIQGN equally expressive.words theorem says domain independent TBox axiomdomain independent concept query ALCHOIQ logically equivalent, respectively,TBox axiom concept query ALCHOIQGN , vice-versa. theorem providesdescription logics version Codds theorem. Codds theorem states safe-rangesyntactic fragment FOL domain-independent fragment FOL preciselyequivalent expressive power; is, database query formulated one languageexpressed other.RBC::=::=::=P | P| {o} | nRB | nR.C | nR.C | B u C | C u | CFigure 2: Syntax ALCHOIQGN concepts roles900fiExact Query Reformulation DBs FO DL Ontologies7.1 Applying Constructive Theoremwant reformulate concept queries ontology DBox reformulated query evaluated SQL query database represented DBox.context, database DBox, ontology ALCHOIQGN TBox,query ALCHOIQGN concept query. concept query either ALCHOIQGNconcept expression denoting open formula one free variable, ALCHOIQGNABox concept assertion denoting boolean query. expected, DBox includes groundatomic statements form A(a) P (a, b) (where atomic concept Patomic role). Theorem 10 draw following corollary.Corollary 1. ALCHOIQGN TBoxes concept queries domain independent.also prove following theorem.Theorem 11. ALCHOIQGN TBoxes concept queries finitely controllable determinacy.Therefore, satisfy conditions Theorem 5, language likeexpressive ALCHOIQ description logic, guarded negation.argue non-guarded negation appear cleanly designed ontology,and, present, fixed. Indeed, use absolute negative informationsuch as,e.g., non-male female ( male v female)should discouraged cleandesign methodology, since subsumer would include sorts objects universe(but ones subsumee type) without obvious control. guarded negativeinformation subsumee allowedsuch axiom non-male personfemale (person u male v female).observation suggests fix non-guarded negations: every non-guarded negation users asked replace guarded one, guard may arbitraryatomic concept, nominal, non-qualified existential. Therefore, user asked makeexplicit type concept, way make domain independent; notetype could also fresh new atomic concept. believe fix proposingALCHOIQ reasonable one, would make ALCHOIQ ontologies eligibleused framework.7.2 Complete ProcedureALCHOIQGN decidable logic feasible application general framework.Given ALCHOIQGN ontology KB concept query Q, apply proceduregenerate safe-range reformulation database concepts roles (basedconstructive theorem, conditions satisfied), exists.Input: ALCHOIQGN TBox KB, concept query Q ALCHOIQGN ,database signature (database atomic concepts roles).g |= Q Qe using1. Check implicit definability query Q testing KB KBstandard OWL2 reasoner (ALCHOIQGN sublanguage OWL2). Continueholds.901fiFranconi, Kerhet, & Ngob tableau proof generated step 1 (see2. Compute safe-range reformulation QSection 6). implemented simple extension standard DL reasonereven presence important optimisation techniques semanticbranching, absorption, backjumping explained Seylan et al. (2009) tenCate, Franconi, Seylan (2011).b expressed database signature.Output: safe-range reformulation QNote procedure checking determinacy computing reformulationcould run offline mode compile time. Indeed, could run atomic conceptontology, store persistently outcome reformulationsuccessful. pre-computation may expensive operation, sinceasseenit based entailment, complexity involves size ontologydata.order get idea size reformulations, ALCF descriptionlogic tableau-based algorithm computing explicit definitions doubleexponential size (ten Cate et al., 2011; ten Cate, Franconi, & Seylan, 2013); algorithmoptimal also shown smallest explicit definition implicitly definedconcept may double exponentially long size input TBox.Clearly, similarly DL-Lite reformulations, research needed order optimisereformulation step order make practical. However, note frameworkpresented clear advantage point view conceptual modelling sinceimplicit definitions (that is, queries) general TBoxes double exponentiallysuccinct acyclic concept definitions (that is, explicit queries database).also another interesting open problem checking given databaselegal respect given ontology. Remember database DB legalontology KB exists model KB embedding DB. check involves heavycomputations optimised algorithm still unknown: matter fact,known method today reduce problem satisfiability problemdatabase embedded TBox using nominals (Franconi et al., 2011). researchneeded order optimise reasoning nominals special case.Appendix A.5 contains definitions theorems needed prove theorems 1011.8. Conclusionintroduced framework compute exact reformulation first-order queriesdatabase ontologies. found exact conditions guaranteesafe-range reformulation exists, show evaluated relationalalgebra query database give answer original queryontology. non-trivial case study presented field description logics,ALCHOIQ language.also implemented tool based Prover9 theorem prover (McCune, 2011).Given arbitrary first-order ontology, database signature, arbitrary first-orderquery TPTP syntax, tool performs tests check whether reformulation computed, computes optimal safe-range reformulation.902fiExact Query Reformulation DBs FO DL Ontologiesframework useful data exchange-like scenarios, target database(made determined relations) materialised proper database,arbitrary queries performed. achieved context non-exactrewritings preserving certain answers. scenario description logics ontologies,rewritings concept queries pre-computed offline once. shown framework works theory also case arbitrary safe-range first-order queries, toolshows possible practice. case description logics, workingextending theoretical framework conjunctive queries: need finitely controllabledeterminacy conjunctive queries, seems follow description logicworks Barany, Gottlob, Otto (2010) Rosati (2011).future work, would like study optimisations reformulations. practical perspective, since might many rewritten queries one original query,problem selecting optimised query terms query evaluation important.fact, one take account criteria used optimise, as:size rewritings, numbers used predicates, priority predicates, numberrelational operators, clever usage duplicates. tool, plan evaluateproposed technique real context.Concurrently, exploring problem fixing real ontologies order enforcedefinability known case (Franconi, Ngo, & Sherkhonov, 2012c).happens intuitively obvious answer query foundavailable data (that is, query definable database), mediatingontology entail definability. introduce novel problem definabilityabduction solve completely data exchange scenario.thank anonymous reviewers useful comments got earlier versions paper. wish thank Alex Borgida, Tommaso Di Noia, Umberto Straccia,David Toman, Grant Weddell fruitful discussions topicspaper.Appendix A. ProofsA.1 Proofs Section 2Proposition 1Proof.LetAsna = { | dom() = X, rng() C, I(SNA)M (KB)E(DB) : |= Q[X/] }Auna = { | dom() = X, rng() C, I(UNA)M (KB)E(DB) : |= Q[X/] }Since SNA stricter UNA, i.e. I(SNA) I(UNA), have: Auna Asna trivially.Let Asna ./ Auna interpretation = hI , embeddingDB satisfying UNA (KB) 6|= Q[X/] . Let us construct newinterpretation J = hJ , J embedding DB follows:J := (I \ {aI | C}) C;903fiFranconi, Kerhet, & Ngoconstant C, aJ := a;every predicate P P, PJ constructed PI replacing elementaI PI , constant, a.Obviously, J satisfies SNA J isomorphic. Since first-order logic sentences cannot distinguish two isomorphic structures, J 6|= Q[X/] contradictsassumption Asna . Therefore Auna .A.2 Proofs Section 4Proposition 2.b[X] exact reformulation Q[X] , KB |= X.Q[X] Qb[X] . Then,Proof. Since Qb[X] ,model (KB) substitution : X 7 have: I, |= Q[X] Qb[X] ).equivalent (I, |= Q[X] I, |= QNow, let substitution { | dom() = X, rng() = C, (KB)E(DB) : |= Q[X/] }, = h, model KB embedding DB (ifany). Let := composition substitution interpretationfunction (i.e. (x) = iff (x) = c C cI = a). I, |= Q[X]b[X] |= Qbb|= Q[X/] I, |= Q[X/] . Summing up: |= Q[X/] |= Q[X/] .b[X/] }.Hence, { | dom() = X, rng() = C, (KB) E(DB) : |= Qinverse inclusion proved similarly.Theorem 3.Proof. First recall assume SNA. order prove theorem, one needsfollowing two propositions.Proposition 3 (Domain Independence). query Q[X] domain independent iffevery two interpretations = hI , J = hJ , J agree interpretationpredicates PQ (and constants C), every substitution : X 7 Jhave:rng() I, |= Q[X]iffrng() J J , |= Q[X] .Proof. () Obviously, second part proposition holds, query domainindependent.() Suppose, query domain independent. Let = hI , J = hJ , Jtwo interpretations, agree interpretation predicates PQ(and constants C), I|PQ C = J |PQ C . Let us fix substitution : X 7 J(if query closed, omit everything, concerns substitutionproof) that:rng() I, |= Q[X] .(2)904fiExact Query Reformulation DBs FO DL Ontologies00Let us consider interpretations 0 = hI , J 0 = hJ , J i, I|PQ C =0000 |PQ C= J |PQ C = J |PQ C , P P \ PQ : P = = P J . Let us consider0 . domain interpret predicates constants, occurringQ[X] equally. Therefore, since I, |= Q[X] (by (2)), 0 , |= Q[X] .Let us consider interpretations 0 J 0 . construction, agree interpretation predicates constants. Therefore, apply definition domainindependence them. Then, sincerng() 0 , |= Q[X] ,(3)rng() J J 0 , |= Q[X] .(4)have,interpretations J J 0 domain interpret predicatesconstants, occurring Q[X] equally. Thus, (4),rng() J J , |= Q[X] .(5)Therefore, (2) = (5). Similarly (5) = (2), proposition proved.Proposition 4. Q[X] domain independent, interpretation = h,substitution : X 7 , I, |= Q[X] , following holds:rng() adom((Q[X] ), I).Proof. Assume, X = {x}, Q one free variable x (the proof easilyextended general case).Let us prove contradiction. Suppose, exists substitution {x b}I, {x b} |= Q(x) b \ adom((Q(x)), I). Let us consider interpretation 0 =h {a}, i, brand-new element, appear . 0 , {xb} |= Q(x) domain independence Q(x). Consider another interpretation0000 = h {a}, occurrence b interpretation predicate replacedelement a. words, n-ary predicate P P \ (Q(x)), (. . . , a, . . .)000P iff (. . . , b, . . .) P (since supposition b appear interpretationspredicates query). Interpretations predicates constantssame. 00 satisfies SNA (even b C). Then, since 0 , {x b} |= Q[X] ,construction 00 have: 00 , {x a} |= Q(x), changed interpretationspredicates, appear query. since 0 00 domainagree interpretations predicates Q(x) constants, followingholds: 0 , {x a} |= Q(x).Let us consider interpretations = h, 0 = h {a}, i.interpretation function. Therefore, since Q(x) domain independent 0 , {xa} |= Q(x), have: rng({x a}) . . contradiction,supposition 6 .905fiFranconi, Kerhet, & Ngoprove theorem itself.L := { | dom() = X, rng() C, (KB) E(DB) : |= Q[X/] };b DB), = hC, E(DB) : I|PR := { | dom() = X, rng() adom((Q),DB Cb[X/] }.|= QbLet L. (KB) E(DB) have: |= Q[X/] |= Q[X/] ,Proposition 2.Consider J = hC, embedding DB. J agree interpretations Cb P subset PDB .(since SNA) predicates set (Q)b[X] domain independent, Proposition 3 have: J |= QbThen, since Q[X/] . Sincebbb(Q[X] ) PDB C, J |P C |= Q. Since Q[X] domain independent, PropositionDB[X/]b[X] ), J ). adom((Qb[X] ), J ) = adom((Qb[X] ), DB),4 have: rng() adom((Qb[X] ) PDB C. Therefore, rng() adom((Qb[X] ), DB).assume SNA (QR and, hence, L R.b[X] ), DB)). J = hC, embedding DBLet R (rng() adom((Qbbhave: J |PDB C |= Q[X/] . J |= Q[X/] . Consider (KB) E(DB). Jbagree interpretations C (since SNA) PDB . Since (Q) PDB C[X/]b[X] domain independent, Proposition 3 have: |= QbbQ[X/] . Since Q[X] exactreformulation Q[X] KB PDB , Proposition 2 have: |= Q[X/] .L and, hence, R L.Theorem 3 proved completely.A.3 Definitions Proofs Section 5Proposition 5. Let KB domain independent ontology. interpretation = hI ,model KB, J = hJ , J i, = J , also model KB.Proof. Let sentence KB. Then, since model KB, |= . domainindependent, KB domain independent. Hence, since = J , J |= . Thus, Jmodel sentence KB. means, J model KB.Proposition 6. Let KB ontology, let Q[X] query domain independentrespect KB. exact reformulation Q[X] KB (over set predicates)also domain independent respect KB.b[X] exact reformulation Q[X] KB (over set predicates),Proof. Let Q= h , J = hJ , J two models KB = J ,: X 7 J substitutionb[X] .rng() I, |= QbThen, since Q[X] exact reformulation Q[X] , have: I, |= Q[X] . Then, since Q[X]domain independent respect KB, have:rng() J J , |= Q[X] .b[X] exact reformulation Q[X] , have: J , |= Qb[X] . Thus, Qb[X]again, since Qdomain independent respect KB definition.906fiExact Query Reformulation DBs FO DL OntologiesLemma 1. Let KB domain independent ontology, let Q[X] querydomain independent respect KB. = h, model KBsubstitution : X 7 I, |= Q[X] following holds:rng() adom((Q[X] ), I).Proof. Without loss generality assume, X = {x}, Q one free variable x(the proof easily extended general case).Let us prove contradiction. Suppose I, {x b} |= Q(x), b \adom((Q[X] ), I). Since KB domain independent, brand-new element a,appear , interpretation = h {a}, also model KB Proposition5. Then, since Q(x) domain independent respect KBinterpretation function, I, {x b} |= Q(x).1Consider new interpretation 1 = h {a}, constructed occurrence b interpretation predicate replaced element a. words,1n-ary predicate P P \ PQ , (. . . , a, . . .) P iff (. . . , b, . . .) P (since suppositionb appear interpretations predicates query).Then, since I, {x b} |= Q(x) construction 1 have: 1 , {x a} |= Q(x)(since simply replace b, appear neither constant Q(x)interpretations predicates Q(x), a). Then, since 1 domain{a} agree interpretations predicates Q(x) constants(since assume SNA), have: I, {x a} |= Q(x).Let us consider interpretations = h, = h {a}, i.models KB interpretation function . So, since Q(x) domainindependent respect KB I, {x a} |= Q(x), have: I, {xa} |= Q(x) definition domain independence respect ontology.contradiction, supposition 6 . lemma proved.Let set formulas. Adom defined similarly Adom Q , Qquery.Lemma 2. Let KB domain independent ontology, let Q[X] (X = {x1 , ..., xn })query domain independent respect KB. following holds:KB |= X.Q[X] Q[X] |Adom KBQQ[X] |Adom KBQ Q0 [X] Adom KBQ (x1 ) ... Adom KBQ (xn ), Q0 [X] Q[X]that:Every sub-formula Q[X] form x.(x) replaced x.(x)Adom KBQ (x)Every sub-formula Q[X] form x.(x) replaced x.Adom KBQ (x)(x)Proof. Without loss generality, prove lemma n = 1. case,write Q(x) instead Q[X] . prove contradiction.Assume model = hI , KB element I, {xa} |= Q(x) I, {x a} 6|= Q(x)|Adom KBQ .907fiFranconi, Kerhet, & Ngoconstruct new interpretation J = hAdom IKBQ C, J predicateP PKBQ , P J := P , predicate P P \ PKBQ , P J := .Since KB domain independent, J also model KB Proposition 5. Then,J , {x a} |= Q(x) Q domain independent respect KB. consequence, however, J , {x a} |= Q(x)|Adom KBQ definition Q(x)|Adom KBQ .Q(x)|Adom KBQ safe-range construction (see Definition 10). Hence, domainindependent. Therefore I, {x a} |= Q(x)|Adom KBQ . Contradiction.Assume model = hI , KB element I, {xa} |= Q(x)|Adom KBQ I, {x a} 6|= Q(x). One lead contradiction similarlyabove. Therefore, lemma proved.Theorem 4.Proof. theorem proved Theorem 5.direction. Based Lemma 2, one see exact reformulations Q[X]also exact reformulations Q[X] |Adom KBQ . Since Q[X] |Adom KBQ safe-rangeKB always transformed logically equivalent safe-range ontology KB 0 ,b[X] found Theorem 5 takesobviously exact safe-range reformulation Q0KB Q[X] |Adom KBQ input exact domain independent reformulationQ[X] .direction.b[X] Q[X]Suppose, exists exact domain independent reformulation QPDB KB. domain independent respect KB. Hence,Proposition 6, Q[X] domain independent respect KB. Since existsexact reformulation Q[X] , Q[X] implicitly definable PDB KBTheorem 1.theorem proved completely.order help readers follow easier, recall formal definitions safe-rangesafe-range normal form (Abiteboul et al., 1995).Definition 8 (safe-range normal form). denoted SRNFfirst order formula transformed SRNF following steps :Variable substitution: distinct pair quantifiers may employ variable.Remove universal quantifiersRemove implicationsPush negationFlatten and/or908fiExact Query Reformulation DBs FO DL OntologiesDefinition 9 (Range restriction formula). denoted rrInput : formula SRNFOutput : subset free()CaseR(e1 , ..., en ) : rr() = set variables e1 , ..., enx = = x, constant : rr() = {x}x = : rr() =1 2 : rr() = rr(1 ) rr(2 )1 2 : rr() = rr(1 ) rr(2 )1 x = : rr() = rr(1 ) {x, y} rr(1 ) = ; rr() = rr(1 ) {x, y} otherwise1 : rr() = rr(1 )x1 : rr() = rr(1 )\{x} x rr(1 ); rr() = otherwiseNote : Z = Z = \Z = Z\ =Definition 10 (safe-range). formula safe-range iff rr(SRNF()) = free().Definition 11 (ground safe-range). formula ground safe-range iff substitution free variables constants becomes safe-range.Observation 1.1. query Q[X] interpretation = h, following holds:Adom IQ = adom((Q[X] ), I).2. Adom Q (x) safe-range.Theorem 5.Proof. theorem proved Theorem 8 Theorem 9.use following lemma proof.Lemma 3. KB ontology, Q[X] (X = {x1 , . . . , xn }) ground safe-range queryKB |= X. Q[X] 1 (x1 ) . . . n (xn ),(6)b[X] := Q[X] 1 (x1 ). . .n (xn )1 , . . . , n n safe-range formulas, query Qbsafe-range KB |= X. Q[X] Q[X] .909fiFranconi, Kerhet, & NgoProof. Let Q0[X] safe-range normal form query Q[X] , i.e. Q0[X] := SRNF(Q[X] ) =Y. [XY] , [XY] conjunctive normal form (the safe-range normal formquery prenex normal form). Q0[X] ground safe-range, KB |= Q0[X] Q[X] .Hence, KB |= X. Q0[X] 1 (x1 ) . . . n (xn ). Let Q00[X] := Q0[X] 10 (x1 ) . . . n0 (xn ),i0 (xi ) = SRNF(i (xi )). 6, KB |= Q0[X] Q00[X] . handb[X] Q00 construction. Summing everything, have: KB |= Qb[X]KB |= X. Q[X]b[X] safe-range.Q[X] thing need prove Q000One see, Q[X] Y. ([XY] 1 (x1 ). . .n0 (xn )) safe-range normalb[X] . Since Q0 = Y. [XY] ground safe-range, rr([XY] ) \ X = Y0 Y,form Q[X]Y\Y0 exists conjunct x = [XY] , x X. Then, sincei0 (xi ) safe-range, definition range restriction rr([XY] 10 (x1 ). . .n0 (xn )) =X Y, rr(Y. ([XY] 10 (x1 ) . . . n0 (xn ))) = X = free(Q00[X] ). Therefore,b[X] safeY. ([XY] 10 (x1 ) . . . n0 (xn )) safe-range definition, hence Qrange.Let us continue prove theorem.b usingX = , (Q closed) build exact safe-range reformulation QTheorem 8.Suppose now, X = {x1 , . . . , xn }. Since Q[X] safe-range implicitly definablePDB , apply Theorem 8 Q[X] construct ground safe-range rewriting Q0[X] expressed PDB KB |= X. Q[X] Q0[X] . Since Q[X] domain independent(since safe-range), also domain independent respect KB. Hence, Proposition 6, Q0[X] also domain independent respect KB. Moreover, KB safe-rangeand, hence, domain independent. Theorem 9:KB |= X. Q0[X] Adom Q (x1 ) . . . Adom Q (xn ).second item Observation 1 Adom Q (x) safe-range formula. Lemma 3b[X] := Q0 Adom Q0 (x1 ) . . . Adom Q0 (xn ) safe-range KB |= X. Q0query Q[X][X]b[X] . Since KB |= X. Q[X] Q0 , have: KB |= X. Q[X] Qb[X] . Therefore,Q[X]b[X] one looking for.constructed query QTheorem 5 proved completely.A.4 Proofs Section 6Theorem 7.Proof. First prove Q implicitly definable formula (1) valid.g |= X.Q[X] QgApplying syntactic definition implicit definability: KB KB[X] . Therefore,replaceXsetconstantsc,...,c,followingformulavalid1nVVge[X/c ,...,c ] ). consequence, (1) valid.( KB KB)(Q[X/c1 ,...,cn ] Qn1b[c ,...,c /X] ) Qb[X/c ,...,c ] Craig interNext, prove KB |= (Q[X] Qn0n11b[X/c ,...,c ] interpolant:polant (1). Since Q1n910fiExact Query Reformulation DBs FO DL OntologiesVb[X/c ,...,c ]1. (( KB) Q[X/c1 ,...,cn ] ) Qn1b[X/c ,...,c ] ): KB |= (Q[X/c1 ,...,cn ] Qn1g Qe[X/c ,...,c ] )b[X/c ,...,c ] ((V KB)2. Qnn11g |= (Qb[X/c ,...,c ] Qe[X/c ,...,c ] ).: KBnn11b PDB , relation KB |= (Qb[X/c ,...,c ] Q[X/c ,...,c ] ) holds wellSince (Q)nn11From(1)(2) expected statement.b[X/c ,...,c ] ) PDB (Qb[c ,...,c /X] ) PDB .Last least, since (Qnn11b[c ,...,c /X] really explicit definition Qstatements, Qn1Theorem 8.Proof. need following propositions prove theorem.Proposition 7. 1 2 safe-range closed iff 1 2 safe-range closed.Proof. have:rr(1 2 ) = rr(1 ) rr(2 )free(1 2 ) = free(1 ) free(2 )rr(1 ) = rr(1 ) free(1 )rr(2 ) = rr(2 ) free(2 )1 2 closed iff f ree(1 ) = free(2 ) =1 closed iff free(1 ) =2 closed iff free(2 ) =1 2 safe-range iff rr(1 2 ) = free(1 2 )1 safe-range iff rr(1 ) = free(1 )1 safe-range iff rr(2 ) = free(2 )Therefore:1 2 closed iff 1 2 closed1 2 closed, safe-range iff 1 2 closed, safe-range.Proposition 8. 1 2 safe-range closed iff 1 2 safe-range closed.Proof. have:rr(1 2 ) = rr(1 ) rr(2 )911fiFranconi, Kerhet, & Ngofree(1 2 ) = free(1 ) free(2 )rr(1 ) = rr(1 ) free(1 )rr(2 ) = rr(2 ) free(2 )1 2 closed iff free(1 ) = free(2 ) =1 closed iff free(1 ) =2 closed iff free(2 ) =1 2 safe-range iff rr(1 2 ) = free(1 2 )1 safe-range iff rr(1 ) = free(1 )1 safe-range iff rr(2 ) = free(2 )Therefore:1 2 closed iff 1 2 closed1 2 closed, safe-range iff 1 2 closed, safe-range.Proposition 9. ~x(~x) closed safe-range (~t) closed safe-range~t constants.Proof. Obviously, ~x(~x) closed (~t) closed.Assume (~t) safe-range. Since closed rr(SRNF((~t))) =SRNF((~t)) must contain subformula form ~z0 (~t, ~z)~z 6 rr(SRNF(0 (~t, ~z)))SRNF((~x)) must contain subformula form ~z0 (~x, ~z)~z 6 rr(SRNF(0 (~x, ~z)))SRNF((~x)) must contain subformula form ~z0 (~x, ~z)~z 6 rr(SRNF(0 (~x, ~z))) pushing negation effect formularr(SRNF((~x))) =rr(SRNF(~x(~x))) =rr(SRNF(~x(~x))) =~x(~x) safe-rangecontradiction.Proposition 10. ~x(~x) closed safe-range (~t) closed safe-range~t constants.Proof. Undoubtedly, ~x(~x) closed (~t) closed.Assume (~t) safe-range. Since closed, rr(SRNF((~t))) =SRNF((~t)) must contain subformula form ~z0 (~t, ~z)~z 6 rr(SRNF(0 (~t, ~z)))912fiExact Query Reformulation DBs FO DL OntologiesSRNF((~x)) must contain subformula form ~z0 (~x, ~z)~z 6 rr(SRNF(0 (~x, ~z)))rr(SRNF((~x))) =rr(SRNF(~x(~x))) =~x(~x) safe-rangecontradiction.Based propositions, prove Theorem 8 follows.First, show closed safe-range validinterpolant. Assume biased tableau . Therefore root node= {L(), R()}. Based tableau expansion rules propositions, every expansion step = {L(1 ), ..., L(n ), R(1 ), ..., R(m )}, 1 , ..., n1 , ..., safe-range closed(*) .need prove interpolant step safe-range closed (**)induction shape proof set rules Section 6.Rules closed branches: trivial safe-range closed(*)Rules propositional case :rule (p1)(p2)(p3)(p4) nothing changes, one need prove.rule (p5), apply Proposition 8, (**) holds.rule (p6), apply Proposition 7,(**) holds.Rules first order case :rule (f1) (f2) (f3) nothing changes, one need prove.rule (f4), since c occur {1 , ..., n } case cintcontains R((c)). Therefore {L((c))} = (c). Since x.(x)safe-range (due (*)) x.I[c/x] safe-rangerule (f5), since c occur {1 , ..., } case cintcontains L((c)). Therefore {R((c))} = (c). Since x.(x)safe-range (due (*)) x.I[c/x] safe-rangeRules equality : input formulas closed containfunction symbols, equations ground. Therefore, influencesafe-range property interpolant step.consequence, Q(~c), KB, KB 0 ,Q0 (~c) closed safe-rangeb c) KB Q(~c) KB 0 Q0 (~c).interpolant Q(~Theorem 9.Proof. consequence Lemma 1, Theorem 9 holds.913fiFranconi, Kerhet, & NgoA.5 Definitions Proofs Section 7safe-range fragment ALCHOIQ. call axiom (concept) ALCHOIQ(ground) safe-range, corresponding logically equivalent (open) formula FOL(C, P)(ground) safe-range. concept C denote corresponding logically equivalentformula FOL(C, P) one free variable x C(x). Unfortunately concept inclusionaxioms ALCHOIQ ontologies may safe-range: example, axiom male vfemale safe-range. easy see axiom C v safe-rangeC(x) safe-range D(x) safe-range: observe axiom logicallyequivalent formula x. C(x) D(x) FOL(C, P) (which actually saferange normal form). following proposition provides recursive rules deciding whetherALCHOIQ concept safe-range.Proposition 11. Let atomic concept, let C ALCHOIQ concepts,let R either atomic role inverse atomic role. Then:1. A, {o}, nR, nR.C safe-range;2. C u safe-range C safe-range safe-range;3. C safe-range C safe-range safe-range;4. C safe-range C safe-range.Proof. enough prove proposition atomic roles ordervariables binary atoms first-order logic translation ALCHOIQ conceptaffect safe-range property translation. Therefore hereafter assume Ratomic role.Since atomic concept, A(x) safe-range.{o}(x) = (x = o) - safe-range.( nR)(x) = x1 , . . . , xn . R(x, x1 ) . . . R(x, xn ) (x1 6= x2 ) . . . (xn1 6= xn ) safe-range.( nR.C)(x) = x1 , . . . , xn . R(x, x1 ) . . . R(x, xn ) C(x1 ) . . . C(xn ) (x1 6=x2 ) . . . (xn1 6= xn ) - safe-range.Let us prove, (C u D)(x) = C(x) D(x) safe-range C(x)safe-range D(x) safe-range.) Let C(x) D(x) safe-range let safe-range normalforms.C(x) D(x) safe-range definition.) Let C(x) D(x) safe-range safe-range normal form (i.e. C(x)D(x) safe-range normal form). Let us prove contradiction. Suppose,C(x) D(x) safe-range. C(x) D(x) safe-range definition.contradiction. Therefore, C(x) safe-range D(x) safe-range.914fiExact Query Reformulation DBs FO DL OntologiesLet us prove, (C D)(x) = C(x) D(x) safe-range C(x)safe-range D(x) safe-range.) Let C(x) D(x) safe-range safe-range normal forms.C(x) D(x) safe-range definition.) Let C(x) D(x) safe-range safe-range normal form (i.e. C(x)D(x) safe-range normal form). Let us prove contradiction. Suppose, C(x)D(x) safe-range. C(x) D(x) safe-range definition.contradiction. Therefore, C(x) safe-range D(x) safe-range.Let us prove, C(x) safe-range C(x) safe-range.) Let C(x) safe-range. Let us prove contradiction. Let C(x) also saferange. C(x) C(x) domain independent. one easily see(looking definition domain independence), impossible. Therefore,C(x) safe-range. ) need prove, C(x) safe-range,C(x) safe-range.Let us prove induction structure formula. Suppose, item truesubformula formula C(x).Suppose, C(x) safe-range. Let us consider (using already proved items)possible cases, C(x) safe-range.C(x) = (R.D)(x) = y. R(x, y) D(y) y.R(x, y)D(y) - safe-range,(possibly complex) concept. C(x) = y.R(x, y) D(y)safe-range definition.Suppose, C(x) = (D u F )(x) safe-range. D(x) safe-rangeF (x) safe-range. Since D(x) F (x) subformulas C(x),applying current item get: D(x) F (x) safe-range. C(x)(D(x) F (x)) D(x) F (x) - safe-range, D(x) F (x)safe-range.Suppose, C(x) = (D F )(x) safe-range. D(x) safe-rangeF (x) safe-range. Since D(x) F (x) subformulas C(x),applying current item get: either D(x) F (x) safe-range. C(x)(D(x) F (x)) D(x) F (x) - safe-range, either D(x) F (x)safe-range.Suppose, C(x) = D(x) safe-range. need prove, C(x) D(x)safe-range. Let us prove contradiction. Suppose, D(x) safe-range.Then, since D(x) subformula C(x), applying current item get:D(x) C(x) safe-range. contradiction. Hence, C(x) safe-range.item proved completely.proposition proved completely.Proposition 12. ALCHOIQ role inclusion axioms safe-range.915fiFranconi, Kerhet, & NgoProof. Let v R role inclusion axiom ALCHOIQ. formula x, y. S(x, y)R(x, y) first-order logic translation axiom, (x, y) stands (x, y)preceding role atomic (x, y) stands (y, x) preceding role inverse atomic.formula safe-range.Guarded negation first-order logic. recall definition guarded negation firstorder logic (GNFO) given paper Barany et al. (2012). GNFO fragmentfirst-order logic consisting formulas generated following recursive definition:::= R(t1 , . . . , tn ) | t1 = t2 | 1 2 | 1 2 | x. |(7)ti either variable constant, atomic formula (possiblyequality statement) containing free variables .Guarded negation fragment ALCHOIQ. consider ALCHOIQGN -guarded negation fragment ALCHOIQ (i.e. intersection GNFO ALCHOIQ).say,concept C ALCHOIQGN concept C ALCHOIQ conceptcorresponding first-order logic translation C(x) expressed GNFO;concept inclusion axiom C v ALCHOIQGN concept inclusion axiomC ALCHOIQ concepts formula x. C(x) D(x) (whichequivalent first-order translation C v D) expressed GNFO;role inclusion axiom v R ALCHOIQGN role inclusion axiom Rroles (atomic inverse atomic) formula x, y. S(x, y) R(x, y) ,(x, y) stands (x, y) preceding role atomic (x, y) stands (y, x)preceding role inverse atomic, expressed GNFO.easy see, ALCHOIQ role inclusion axiom ALCHOIQGN role inclusionaxiom. Proposition 12 following holds.Proposition 13.ALCHOIQGN role inclusion axioms safe-range.safe-range role inclusion axioms ALCHOIQ ALCHOIQGN .definition GNFO ALCHOIQ follows, complex concept Clogic ALCHOIQGN recursively defined follows:B ::= | {o} | nRC ::= B | nR.C | nR.C | B u C | C u | C(8)atomic concept, R atomic role inverse atomic role, CALCHOIQGN concepts (possibly complex).Note, general, according definition (7) GNFO formulas atleastoperator n 2 GNFO non-guarded inequality statements xi 6=xj . fix assuming inequality relation actually special binary databasepredicate. assumption usual databases.916fiExact Query Reformulation DBs FO DL OntologiesAlso strictly speaking nR u C GNFO. Indeed, formula(x1 , . . . , xn . R(x, x1 ) . . . R(x, xn ) (x1 6= x2 ) . . . (xn1 6= xn )) C(x)GNFO (R(x, y) stands P (x, y) R stands atomic role P , R(x, y) standsP (y, x) R stands inverse atomic role P ), easily transformedlogically equivalent GNFO one simply shifting parentheses: x1 , . . . , xn . (R(x, x1 ). . . R(x, xn ) (x1 6= x2 ) . . . (xn1 6= xn ) C(x)). So, assume, formulanR u C ALCHOIQGN .Proposition 14. ALCHOIQGN concepts safe-range.Proof. Let us prove induction structure ALCHOIQGN concepts defined(8).1. A, {o}, nR, nR.C, nR.C (C ALCHOIQGN concept) safe-rangeitem 1 Proposition 11.2. atomic concept A, individual role R natural number nconcepts u C, {o} u C nR u C safe-range item 3Proposition 11 since A, {o} nR safe-range first item.3. Suppose, ALCHOIQGN concepts C safe-range. conceptsC u C safe-range items 2 3 Proposition 11 respectively.proposition proved.Lemma 4. safe-range concept C ALCHOIQ following holds:C v B1 . . . Bn ,Bi appears subconcept C one following concepts:atomic concept A;{o}, individual name;nR, R atomic role inverse atomic role, n natural number.Proof. Let us prove proposition induction safe-range concepts ALCHOIQ.A, {o}, nR, nR.C safe-range Proposition 11. v A, {o} v {o}, nR vnR, nR.C v nR.Suppose C complex safe-range concept proposition holdssafe-range subconcepts C.1. C = C1 u C2 - safe-range. either C1 C2 safe-range. Let C1 safe-range.Hence, C1 v B1 . . . Bm , Bi concept aforementioned type.C1 u C2 v C1 v B 1 . . . B .2. C = C1 C2 - safe-range. C1 C2 safe-range. Hence, C1 v B1 . . . BkC2 v Bk+1 . . . Bm , Bi concept aforementioned type.C1 C2 v (B1 . . . Bk ) (Bk+1 . . . Bm ) v B1 . . . Bm .917fiFranconi, Kerhet, & Ngo3. C = safe-range. Proposition 11 possible saferange. one following cases takes place.= D1 u D2 . D1 D2 . reduced case item 2.= D1 D2 . D1 u D2 . reduced case item 1.= D1 . D1 D1 . Hence, D1 safe-range subconcept D.proposition holds D1 and, hence, also C, C D1 .lemma proved completely.Lemma 5. ALCHOIQ concept C exists ALCHOIQGN concept C 0either C C 0 C C 0 .Proof. Suppose lemma holds ALCHOIQ subconcepts ALCHOIQconcept C. Let us prove C.1. Base. A, {o}, nR ALCHOIQGN concepts definition ALCHOIQGNconcept (8).2. C = nR.D D0 ALCHOIQGN concept D0D0 . C nR.D0 C nR.D0 . nR.D0 nR.D0ALCHOIQGN concepts. Hence, item proved.3. C = D0 ALCHOIQGN concept D0 D0 .C D0 C D0 D0 . item proved.4. C = C1 u C2 C10 ALCHOIQGN concept C1 C10 C1 C10 , C20ALCHOIQGN concept C2 C20 C2 C20 . Consider possiblecases.(a) C1 C10 C2 C20 . C C 0 , C 0 = C10 u C20 ALCHOIQGNconcept (because C10 C20 ALCHOIQGN concepts).(b) C1 C10 C2 C20 . C C10 u C20 (C10 C20 ) = C 0 ,C 0 = C10 tC20 ALCHOIQGN concept (because C10 C20 ALCHOIQGNconcepts).(c) C1 C10 C2 C20 (the case C1 C10 C2 C20 similarone). C C10 u C20 . Since C10 ALCHOIQGN concept Proposition14 safe-range and, hence, Lemma 4 C10 v B1 . . . Bn , Bieither atomic concept {o} R. C10 C10 u (B1 . . . Bn ) and,hence, C C10 u (B1 . . . Bn ) u C20 C10 u (B1 u C20 . . . Bn u C20 ).disjunct Bi u C20 ALCHOIQGN concept (because C2 ALCHOIQGNconcept definition (8) ALCHOIQGN concepts). C 0 = C10 u(B1 u C20 . . . Bn u C20 ) ALCHOIQGN concept. C C 0 . itemproved.5. C = C1 C2 (C1 u C2 ). case reduced items 3 4.918fiExact Query Reformulation DBs FO DL Ontologieslemma proved completely.Corollary 2. ALCHOIQ concept C concept B, either atom{o} nR, concept B u C equivalent ALCHOIQGN concept.Proof. Lemma 5 exists ALCHOIQGN concept C 0 either C C 0C C 0 . B u C B u C 0 B u C B u C 0 . B u C 0 B u C 0ALCHOIQGN concepts (by definition (8) ALCHOIQGN concepts). Hence,corollary proved.Proposition 15. safe-range ALCHOIQ concept equivalent ALCHOIQGNconcept.Proof. Let C safe-range ALCHOIQ concept. Lemma 4 C v B1 . . . Bn ,Bi either atom {o} nR. C C u (B1 . . . Bn )B1 uCt. . .tBn uC. corollary 2 disjunct Bi uC exists ALCHOIQGNconcept Di Bi u C Di . C D1 . . . Dn . concept D1 . . . DnALCHOIQGN concept disjunction ALCHOIQGN concepts. Hence, propositionproved.Proposition 16. ALCHOIQGN concept inclusion axioms safe-range.Proof. Let C v concept inclusion axiom ALCHOIQGN . meanscorresponding first-order logic translation x. C(x) D(x) GNFO. Hence, C(x)D(x) GNFO or, same, C u ALCHOIQGN . easy see,x. C(x) D(x) safe-range formula C(x) D(x) safe-range,corresponding ALCHOIQGN concept C u safe-range.Proposition 14 ALCHOIQGN concept safe-range. proposition proved.Lemma 6. safe-range ALCHOIQ concept C ALCHOIQ conceptconcept C u equivalent ALCHOIQGN concept C 0 u D0 , C 0 D0ALCHOIQGN concepts.Proof. Since C safe-range Lemma 4 C v B1 . . . Bn , Bi eitheratomic concept {o} R. C C u (B1 . . . Bn ) and, hence, C uC u (B1 . . . Bn ) u C u (B1 u . . . Bn u D). corollary 2 disjunctBn uD ALCHOIQGN concept. Hence, D0 := B1 uDt. . .tBn uD ALCHOIQGNconcept. Since C safe-range Proposition 15 exists ALCHOIQGN conceptC 0 C C 0 . C u C 0 u D0 , C 0 u D0 ALCHOIQGN concept,C 0 D0 ALCHOIQGN concepts.Proposition 17. safe-range ALCHOIQ concept inclusion axiom C v transformed concept inclusion axiom C 0 v D0 , C 0 D0 ALCHOIQGN .Proof. Let C v safe-range ALCHOIQ concept inclusion axiom. corresponding formula x. C(x) D(x) safe-range. first-order logic formulaC(x) D(x) safe-range, or, same, ALCHOIQ concept C u saferange. Proposition 11 C safe-range safe-range.919fiFranconi, Kerhet, & NgoC safe-range. Lemma 6 exist two ALCHOIQGN concepts C 0D0 C u logically equivalent ALCHOIQGN concept C 0 u D0 .x. C(x) D(x) logically equivalent x. C 0 (x) D0 (x). Hence, C vlogically equivalent C 0 v D0 (C 0 D0 ALCHOIQGN concepts).safe-range. proof similar previous item.proposition proved completely.Proposition 18. two ALCHOIQGN concepts C axiom C vALCHOIQGN concept inclusion axiom.Proof. axiom C v logically equivalent first-order logic formula x. C(x)D(x), C(x) D(x) GNFO. x. C(x) D(x) also GNFO. Hence,definition ALCHOIQGN concept inclusion axiom axiom C vALCHOIQGN concept inclusion axiom.Propositions 17 18 imply following.Proposition 19. safe-range ALCHOIQ concept inclusion axiom equivalentALCHOIQGN concept inclusion axiom.consider connection safe-range fragment ALCHOIQ guarded negation fragment ALCHOIQ, ALCHOIQGN . say fragment, meanset TBox assertions (concept role inclusion axioms) concepts (open formulas)ALCHOIQ satisfying particular property (e.g safe-range guarded negation). Takingaccount propositions 14, 15, 16, 19 13, following theorem.Proposition 20. safe-range fragment ALCHOIQ ALCHOIQGN equallyexpressive.proves Theorem 10:Theorem 10 (Expressive power equivalence). domain independent fragmentALCHOIQ ALCHOIQGN equally expressive.Theorem 11. ALCHOIQGN TBoxes finitely controllable determinacy conceptqueries.Proof. need prove, ALCHOIQGN TBox (ontology), conceptquery Q ALCHOIQGN set database predicates PDB , whenever queryfinitely determined database predicates ontology also determinedunrestricted models.Suppose, Q finitely determined PDB . Theorem 2e |=fin P means entailment modelsfollows, Te |=fin PDB Q v Q,DBefinite interpretation database predicates. Hence, particular Te |=fin Q v Q,|=fin means entailment finite models. Hereafter let one sentence,first-order logic translation conjunction axioms TBox .aforementioned entailment have:e|=fin ( e) (x. Q(x) Q(x)).920(9)fiExact Query Reformulation DBs FO DL OntologieseProposition 14 Q(x) safe-range. Hence, Q(x) Q(x)safe-range, hencee safe-range and, hence, Proposition 15 existsALCHOIQ concept Q u Qe C 0 . x. Q(x) Q(x)eALCHOIQGN concept C 0 Q u Qx.C 0 (x)following holds:|=fin ( e) (x.C 0 (x)).(10)x.C 0 (x) GNFO, C 0 (x) GNFO. Since axioms ALCHOIQGNTBox axioms, sentences e GNFO. sentence eGNFO.Therefore right hand side entailment (10) GNFO. (( e)(x.C 0 (x))) also GNFO entailment (10) finite model.Then, since GNFO finite model property, (( e) (x.C 0 (x))) unsatisfiable. Hence, have:|= ( e) (x.C 0 (x)).eSince x.C 0 (x) x. Q(x) Q(x),following holds:e|= ( e) (x. Q(x) Q(x)).e Theorem 2 means, query Q determinedTe |= Q v Q.unrestricted models database predicates PDB ontology .proposition proved.ReferencesAbiteboul, S., Hull, R., & Vianu, V. (1995). Foundations Databases. Addison-Wesley.Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). DL-Lite familyrelations. J. Artif. Intell. Res. (JAIR), 36, 169.Avron, A. (2008). Constructibility decidability versus domain independence absoluteness. Theor. Comput. Sci., 394, 144158.Barany, V., Gottlob, G., & Otto, M. (2010). Querying guarded fragment. Proceedings25th Annual IEEE Symposium Logic Computer Science (LICS 2010), pp.110.Barany, V., ten Cate, B., & Otto, M. (2012). Queries guarded negation (full version).CoRR, abs/1203.0077.Beth, E. (1953). Padoas method theory definition. Indagationes Mathematicae,15, 330339.Craig, W. (1957). Three uses Herbrand-Gentzen theorem relating model theoryproof theory. J. Symb. Log., 22 (3), 269285.Etzioni, O., Golden, K., & Weld, D. S. (1997). Sound efficient closed-world reasoningplanning. Artif. Intell., 89, 113148.Fan, W., Geerts, F., & Zheng, L. (2012). View determinacy preserving selected information data transformations. Inf. Syst., 37, 112.Fitting, M. (1996). First-order logic automated theorem proving (2nd edition). Springer.921fiFranconi, Kerhet, & NgoFranconi, E., Ibanez-Garcia, Y. A., & Seylan, Inanc. (2011). Query answering DBoxeshard. Electronic Notes Theoretical Computer Science, Elsevier, 278, 7184.Franconi, E., Kerhet, V., & Ngo, N. (2012a). Exact query reformulation SHOQ DBoxes.Proc. 2012 International workshop Description Logics (DL-2012).Franconi, E., Kerhet, V., & Ngo, N. (2012b). Exact query reformulation first-order ontologies databases. Logics Artificial Intelligence - 13th European Conference,JELIA 2012, pp. 202214.Franconi, E., Ngo, N., & Sherkhonov, E. (2012c). definability abduction problemdata exchange. Web Reasoning Rule Systems - 6th International ConferenceRR 2012.Gurevich, Y. (1984). Toward logic tailored computational complexity. ComputationProof Theory, Vol. 1104, pp. 175216. Springer.Halevy, A. Y. (2001). Answering queries using views: survey. VLDB Journal, 10,270294.Marx, M. (2007). Queries determined views: pack views. Proceedings 26thACM symposium Principles Database Systems, PODS 07, pp. 2330.McCune, W. (20052011).prover9.Prover9 Mace4.http://www.cs.unm.edu/~mccune/Nash, A., Segoufin, L., & Vianu, V. (2010). Views queries: Determinacy rewriting.ACM Trans. Database Syst., 35, 21:121:41.Rosati, R. (2011). finite controllability conjunctive query answering databasesopen-world assumption. J. Comput. Syst. Sci., 77 (3), 572594.Seylan, Inanc., Franconi, E., & de Bruijn, J. (2009). Effective query rewriting ontologies DBoxes. Proc. 21st International Joint Conference ArtificialIntelligence (IJCAI 2009), pp. 923925.ten Cate, B., Franconi, E., & Seylan, Inanc. (2011). Beth definability expressive description logics. Proc. 22nd International Joint Conference ArtificialIntelligence (IJCAI 2011), pp. 10991106.ten Cate, B., Franconi, E., & Seylan, Inanc. (2013). Beth definability expressive description logics. Journal Artificial Intelligence Research (JAIR), 48, 347414.922fiJournal Artificial Intelligence Research 48 (2013) 347-414Submitted 05/13; published 11/13Beth Definability Expressive Description LogicsBalder ten Catebtencate@ucsc.eduUC Santa CruzEnrico Franconifranconi@inf.unibz.itFree University Bozen-BolzanoInanc Seylanseylan@informatik.uni-bremen.deUniversity BremenAbstractBeth definability property, well-known property classical logic, investigated context description logics: general L -TBox implicitly definesL -concept terms given signature, L description logic,always exist signature explicit definition L concept? propertystudied used optimize reasoning description logics. papercomplete classification Beth definability provided extensions basic description logic ALC transitive roles, inverse roles, role hierarchies, and/or functionalityrestrictions, arbitrary finite structures. Moreover, present tableaubased algorithm computes explicit definitions double exponential size.algorithm optimal also shown smallest explicit definitionimplicitly defined concept may double exponentially long size input TBox.Finally, explicit definitions allowed expressed first-order logic, showcompute single exponential time.1. Introductionaddress Beth definability property (Beth, 1953) context description logics(DLs). Beth definability property relates two notions definability logic L ,implicit definability explicit definability. Implicit definability semantic notion:asks whether interpretation given L -formula fully determined universediscourse interpretation given predicates models theory. Explicit definability hand syntactic: asks whetherL -formula set predicates equivalent . Clearly, explicitdefinability implies implicit definability. converse holds well, logic Lsaid Beth definability property. Logics property consideredwell-balanced terms syntax semantics since connects model-theoreticnotion implicit definability explicit definability.Beth definability property naturally formulated DLs slightly changing terminology paragraph above: formulas become concepts, theories becomeTBoxes, consists unary binary predicates (respectively called concept namesrole names).c2013AI Access Foundation. rights reserved.fiTen Cate, Franconi, & SeylanExample 1.1. Consider following ALC-TBox .ParentParentFatherMotherManvvvhasChild.>Father MotherManWomanWomanconcept name Mother implicitly definable = {hasChild, Woman} .Precisely mean clear present Definition 4.1; intuitively,mean instances Mother model exactly determined knowdomain instances I. fact, spell implicit definitionALC-concept Woman u hasChild.>. concept explicit definition Mother|= Mother Woman u hasChild.> (cf. Definition 4.5).Beth definability DLs found applications optimizing reasoning. first application related extracting equivalent acyclic L -terminology general TBoxL (Baader & Nutt, 2003; ten Cate, Conradie, Marx, & Venema, 2006). acyclicterminology consists acyclic definitions concept names particularinterest reasoning easier general TBoxes. example, satisfiability ALC-terminology PSpace-complete problem whereas problemgeneral ALC-TBoxes ExpTime-complete (Donini, 2003). second applicationrelated ontology-based data access setting, assumes existence databaseinstance (also referred DBox context) TBox may speakpredicates database instance (Seylan, Franconi, & de Bruijn, 2009).setting, user may ask concept queries signature TBox; ideafind equivalent rewriting original query terms predicates appearDBox. rewriting exists, determining certain answers queryreduced query answering relational databases, known AC0data complexity contrast general coNP-completeness concept querying ALCDBoxes (Seylan et al., 2009).applications involve computing explicit definitions basis implicitdefinitions. Here, problem may always possible DLs, i.e.,DLs may lack Beth definability property.Example 1.2. example, model scenario cars, owners,relationships owners cars. Consider following ALCH-TBoxconsisting concept inclusion axiomsSportsCarFuelEfficientCarSportsCarproudOwner.CarvvvvCarCarFuelEfficientCar(loves.SportsCar u owns.SportsCar))(loves.FuelEfficientCar u owns.FuelEfficientCar))role inclusion axiomsproudOwner v ownsproudOwner v loves348fiBeth Definability Expressive Description Logicsconcept proudOwner.Car implicitly definable = {owns, loves} ,sense instances proudOwner.Car model exactly determinedknow domain instances roles . Indeed, individualproud owner car individual owns something he/she loves.fact left-to-right direction equivalence holds every model followsimmediately role inclusion axioms, similarly, fact (contrapositivethe) right-to-left direction holds models follows immediately TBoxaxioms. implicit definition made explicit using role conjunction operatorconcept (owns u loves).>. However, shown ALCH-conceptexplicit definition proudOwner.Car . formally provehere, see proof Theorem 4.18 Section 4.2 similar example. particular,shows ALCH lacks Beth definability property.natural research agenda case identify DLs Beth definabilityproperty. Since property useful computing explicit definitions basisimplicit definitions, vital question complexity task, termstime needed compute explicit definitions, terms size explicitdefinitions obtained. question first studied ten Cate et al. (2006) weakerBeth definability property, considers concept names signature.paper interested general Beth definability property takesaccount role names signature. believe natural DLsDL knowledge base, role names considered part signature. presentworst-case optimal algorithm constructing explicit definitions.Since work Craig (1957), customary establish Beth definabilityvia interpolation theorem; work exception. particular, obtainpositive results Beth definability worst-case optimal algorithm constructinginterpolants description logics consider.contributions paper follows.obtain complete classification Beth definability property extensionsALC transitive roles, inverse roles, role hierarchies, and/or functionality restrictions, arbitrary structures (BP) finite structures (BPF).results summarized Table 1. Note finite model property (FMP)sub-logics SHOQ shown Lutz, Areces, Horrocks, Sattler (2005); FMPsub-logics SHIO+ Duc Lamolle (2010); failure FMPALCF extensions well-known (cf. Calvanese & Giacomo, 2003).present constructive algorithm based interpolating tableau calculuscompute explicit definitions ALC considered extensionsBeth definability property. algorithm runs double exponential time computes worst case explicit definition double exponential size conceptimplicitly definable. respect, algorithm optimal also showsmallest explicit definition implicitly defined concept may doubleexponentially long size input TBox DLs.consider case explicit definitions allowed expressed firstorder logic. particularly relevant use case computing certain answers349fiTen Cate, Franconi, & SeylanHFFMP++++++++++++-BP++++++++-BPF++++++-Table 1: BP BPF ALC SHIFquery given DBox TBox. present algorithm computes firstorder explicit definition implicitly defined concept single exponential timeDLs BP BPF.1.1 Related WorkBeth definability property, general sense, first shown hold firstorder logic (Beth, 1953). Beth definability comes different flavors oneinterested related projective Beth definability. Here, projective refersability specify set predicates . projective version known strongerBeths original formulation (cf. Hoogland, 2001) first shown hold first-order logicCraig (1957). Since seminal works Beth Craig, Beth definabilitystudied many logics.Lang Marquis (2008), also motivated AI, study propositional variant.modal temporal variants extensively studied (cf. Gabbay & Maksimova,2005). k-variable fragment first-order logic, k 2, known lack Bethdefinability property, whereas Guarded Packed Fragments satisfy non-projectiveversion Beth property (cf. Hoogland, 2001). guarded-negation fragmentrecently shown Beth definability property well (Barany, Benedikt, & tenCate, 2013).Beth definability practical applications relational databases query rewritingusing exact views (Nash, Segoufin, & Vianu, 2010; Afrati, 2011; Marx, 2007; Pasaila, 2011;Barany et al., 2013). Here, idea decide answers given query inferredcontent collection views (that is, whether theory consisting viewdefinitions implicitly defines query terms view predicates), and, indeedcase, rewrite query query schema consisting view predicates350fiBeth Definability Expressive Description Logics(that is, explicit definition query terms view predicates). View-basedquery rewriting naturally arises various settings, including query optimization, queryingaccess restrictions, data integration, privacy analysis.Beth definability also studied DL literature. Similarly relationaldatabase case, finds applications computing explicit definitions basis implicitdefinitions (Baader & Nutt, 2003; ten Cate et al., 2006; Seylan et al., 2009; Seylan, Franconi, & de Bruijn, 2010). papers also present results size explicitdefinitions obtained implicitly defined concepts. Ten Cate et al. establishsingle exponential lower bound triple exponential upper bound ALC.hard see lower bound proof ten Cate et al. carries Beth definabilityproperty consider. matching single exponential upper bound size explicitdefinitions claimed established Seylan et al. (2010) Theorem 1; however,theorem wrong since crucial step proof, namely Lemma 1, erroneous.paper, improve single exponential lower bound ten Cate et al. doubleexponential correct single exponential upper bound Seylan et al. double exponential, thus obtaining tight complexity bounds. bounds DLs sharp contrastfirst-order logic since recursive bound minimal number quantifieralternations explicit definitions first-order logic (Friedman, 1976). BP firstshown hold ALC Seylan et al. (2009) stronger variant studiedten Cate et al.. Specifically, show DLs consider support role hierarchiesactually lack BP, whereas satisfy variant BP studied ten Cate et al..respect, Theorem 10 Seylan et al. (2010) claiming DLs BP erroneous.mistake proof Theorem 9, presents reduction conceptsatisfiability problem w.r.t. TBoxes SHI problem ALC, actuallyused computing SHI-interpolants.Since work Craig (1957), customary establish Beth definabilityvia interpolation lemma; work exception. interpolation lemmausually established model-theoretic proof-theoretic argument (Hoogland, 2001).advantage latter former yields procedure constructinterpolant. Several interpolation properties formulated general TBoxes studiedALC- (ten Cate et al., 2006; Ghilardi, Lutz, & Wolter, 2006; Konev, Lutz, Walther,& Wolter, 2009a; Seylan et al., 2009; Konev, Lutz, Ponomaryov, & Wolter, 2010; Lutz& Wolter, 2011) EL-family DLs (Konev, Walther, & Wolter, 2009b; Lutz, Piro,& Wolter, 2010; Nikitina & Rudolph, 2012; Lutz, Seylan, & Wolter, 2012a). notablevariant uniform interpolation property. uniform interpolant given L -TBoxset predicates another L -TBox 0 0 uses predicateslogical consequences 0 formulated coincide. paper,consider uniform interpolation right interpolation propertyestablishing tight bounds size explicit definitions. witnessedfollowing observations. Deciding existence uniform interpolant given ALCTBox set predicates known 2-ExpTime-complete (Lutz & Wolter, 2011),whereas problem formulated interpolation property studyExpTime. simpler DL EL, uniform interpolants also expensivenon-uniform ones. particular, deciding existence uniform interpolants ELExpTime-complete (Lutz et al., 2012a); Nikitina Rudolph (2012) establish triple351fiTen Cate, Franconi, & Seylanexponential tight bounds size uniform interpolants. hand, decidingexistence interpolants, consider paper description logic EL,PTime problem reduced concept subsumption w.r.t. TBoxEL Lemma 3 Lutz, Seylan, Wolter (2012b).results paper announced ten Cate, Franconi, Seylan(2011) extended abstract. current paper extends work full proofsclaimed results new material Section 4.4.1.2 Outlinestart introducing Section 2 DLs study BP reasoningproblems relevant us paper. also fix section first-ordernotation standard translation DLs first-order logic. using firstorder logic extensively Section 3.3. hammer nail positiveresults, i.e., +, columns BP BPF Table 1, worst-case optimal algorithmconstructing interpolants. Section 3 dedicated interpolation result. Finally,results BP presented Section 4. Since interpolation results usedprove BP, Section 3 naturally comes Section 4, reader less interestedinterpolation results may prefer skip Section 3 initially.2. Preliminariessection, introduce description logics study. frequentlyused logics expressive ALC-family description logics.2.1 Description LogicsLet NC NR countably infinite mutually disjoint sets concept namesrole names, respectively. reasons become clear moment, also assumecountably infinite subset NR , denoted NR+ , NR \ NR+ also countably infinite.role names NR+ are, intuitively, designated transitive, allowedused description logics transitive roles. element NC NR alsocalled predicate, set NC NR concept role names called signature.ease exposition, first introduce description logic ALCF I,define description logics study. concept language ALCF definedfollows:Concepts:Roles:C, ::= > | | C | C u | R.C | 1RR::= P | PNC P NR \ NR+ . concept constructors , t, R.C, 2Rdefined abbreviations usual way. Also, slight abuse notation,sometimes write (P ) , P NR , case refers role name P itself.ALCFI-TBox finite set concept inclusion axioms (CIAs) C v D, CALCF I-concepts.semantics ALCF I-concepts roles given terms interpretations.interpretation pair = hI , non-empty set called domain352fiBeth Definability Expressive Description Logics>I(C)I(C u D)I(R.C)I( 1R)I(P )I======,\ C ,C DI ,{s | exists hs, ti RI C },{s | t, u , hs, ti RI hs, ui RI = u},{hs, ti | ht, si P }.Table 2: Semantics complex ALCF I-concepts rolesI, function maps concept name NC subset AIrole name P NR binary relation P . anticipation discussiondescription logics transitive roles below, require also P NR+ ,relation P transitive. map extended complex concepts roles meansinductive definitions provided Table 2.interpretation satisfies (or, model of) CIA C v C DI , satisfies(or, model of) TBox satisfies every CIA . use notation |= C v|= express satisfies C v D, respectively, satisfies .description logic ALCF defined member larger familydescription logics. basic description logic ALC defined ALCF without inverseroles (i.e., without roles form P ) without functionality restrictions (i.e., withoutconcepts form 1R). X {S, H, I, F}, description logic ALCX extendsALC1. Functionality restrictions (as ALCF I) F X,2. Inverse roles (as ALCF I) X,3. Transitive roles X. this, mean role names NR+ allowedused.4. Role hierarchies H X. this, mean TBox may contain role inclusionaxioms (RIAs) form R v S, R roles, satisfiedinterpretation RI .DLs include transitive roles (S) functionality restrictions (F),syntactic restriction imposed: whenever 1R occurs concept, R requiredsimple role respect TBox hand (Horrocks, Sattler, & Tobies, 2000).simple role is, intuitively, role transitive subrole. formal definitionsimplicity follows: let us write R vT either R = roles R1 , . . . , RnR1 = R, Rn = S, 1 < n, contains either RIA Ri v Ri+1RIA Ri v Ri+1. say R simple respect existrole vT R form P P P NR+ .motivation standard syntactic restriction that, without it, basic decisionproblems satisfiability concept subsumption respect TBox (definedbelow) quickly become undecidable (Horrocks et al., 2000).353fiTen Cate, Franconi, & SeylanX {S, H, I, F} X, customary omit prefix ALC notation ALCX. particular, description logic ALCSHIF (which expressivedescription logic consider paper) referred simply SHIF. SHIF alsotheoretical basis Web Ontology Language OWL-Lite (Horrocks, Patel-Schneider,& van Harmelen, 2003), makes important DL practical viewpoint.L -concept C, set sub(C) consists C subconcepts. conceptC TBox , rol(C, ) denotes set roles occurring C ; sig(C, )denotes set concept names role names occurring C , i.e., signatureC . use sig(C) abbreviation sig(C, ). size L -concept C(L -role R), written |C| (resp. |R|), number occurrences symbols needed writeC (resp. R). size L -TBox , written |T |, defined analogously. Later on,Section 3.3 also consider other, succinct, ways representing concepts.alternative ways represent functionality restrictions transitive rolesDL literature. example, functionality transitivity axioms form funct RTrans(R) sometimes treated axioms TBox. Although syntactic differences considered minor far standard reasoning tasks (cf. Section 2.2)concerned, interpolation results sensitive changes language. example,opting TBox axioms form funct R instead freely allowing 1R constructconcept language would change expressive power languages consider.Section 3.1, show ALCF interpolation property. interested readerinvited check proof adapted case allow functionalityrestrictions TBox axioms.2.2 Decision Problemsconcept C satisfiable respect TBox exists modelC 6= . CIA C v follows TBox (denoted |= C v D), every modelmodel C v D. write |= C |= C v |= v C holdtrue.following decision problems relevant us:Concept satisfiability respect TBox :Given C , determine C satisfiable w.r.t. .Concept subsumption respect TBox :Given C v , determine |= C v D.problems parametrized description logic L , input concept(s)TBox specified. two problems reducible (or, accurately,others complement) logics consider, due fact conceptlanguages closed negation. fact, problems ExpTime-completedescription logics consider (Tobies, 2001).decision problems also considered restricted class finiteinterpretations, i.e., interpretations whose domain finite set. refervariants decision problems finite concept satisfiability finite conceptsubsumption. Thus, finite concept satisfiability respect TBox problemdeciding whether given concept non-empty denotation finite model354fiBeth Definability Expressive Description Logicsgiven TBox. known (Lutz, Sattler, & Tendera, 2005) finite concept satisfiabilityfinite concept subsumption also ExpTime-complete description logicsconsider here.finite concept satisfiability problem coincides unrestricted satisfiability problem, say description logic question finite modelproperty.Definition 2.1 (Finite model property). DL L said finite model property(FMP) every L -concept C every L -TBox , C satisfiable w.r.t. ,finite interpretation model C 6= .well-known ALCF extensions lack finite model property (Calvanese& Giacomo, 2003).2.3 First-Order Translationwell-known correspondence theory modal/description logics descriptionlogic concepts can, general, translated first-order logic formulae one freevariable (Sattler, Calvanese, & Molitor, 2003). translation, concept nameviewed unary predicate symbol role name R viewed binary predicatesymbol first-order language. interpretation, then, corresponds first-orderstructure.assume reader familiar basic notation terminology first-orderlogic. particular, use notation I, |= express first-order formulasatisfied structure first-order variable assignment . Sometimes,convenient use different notation express thing: (x1 , . . . , xn )first-order formula whose free variables x1 , . . . , xn , a1 , . . . , elementsdomain structure I, write |= [a1 , . . . , ] express satisfiedvariable assignment sends variable xi corresponding element ai .Note notation implicitly assumes order free variables ,always clear context.Definition 2.2. mapping x SHIF-concepts first-order formulae definedfollows:x (>) = >,x (A) = A(x),x (C) = x (C),x (C u D) = x (C) x (D),x (P.C) = y[P (x, y) (C)],x (P .C) = y[P (y, x) (C)],x ( 1P ) = z1 z2 [P (x, z1 ) P (x, z2 ) z1 = z2 ],x ( 1P ) = z1 z2 [P (z1 , x) P (z2 , x) z1 = z2 ],355fiTen Cate, Franconi, & Seylanobtained definition replacingoccurrences xVvice versa. SHIF-TBox , (T ) defined (),(C v D) = x[x (C) x (D)](R v S) = xy[xy (R) xy (S)]where, P NR , xy (P ) = P (x, y) xy (P ) = P (y, x).translation model-preserving, i.e., SHIF-concepts C, interpretationsI, first-order assignments I, (x) C iff I, |= x (C); similarlyCIAs, RIAs, TBoxes.3. Constructive Interpolation Tableauxsection provides constructive proof interpolation property DLsinterested in. property essential part proof BP DLs (cf.Definition 4.7). Resorting interpolation show Beth definability property logicstandard technique since seminal work Craig (1957). start defininginterpolation property.Definition 3.1 (Interpolation property). DL L said interpolation propertyL -concepts C1 , C2 L -TBoxes T1 , T2 , T1 T2 |= C1 v C2 ,L -conceptsig(I) sig(C1 , T1 ) sig(C2 , T2 ),T1 T2 |= C1 v I,T1 T2 |= v C2 .concept called interpolant C1 C2 hT1 , T2 i.interpolation property consider defined specifically prove BP. Normally,Craig interpolation property first-order logic stated follows: first-orderformulae , |= , exists first-order formula sig()sig() sig(), |= , |= . however relate interpolation propertyconsider first-order Craig interpolation using standard translation Definition 2.2.Given L -concepts C1 , C2 L -TBoxes T1 , T2 , standard translationfollowing equivalences:T1 T2 |= C1 v C2(T1 ) (T2 ) |= x (C1 ) x (C2 )(T1 ) x (C1 ) |= (T2 ) x (C2 )Thus, setting = (T1 ) x (C1 ) = (T2 ) x (C2 ), know CraigsInterpolation Theorem first-order logic always first-order interpolant, |= (Craig, 1957). However, know general whetherinterpolant expressed L -concept. reason work356fiBeth Definability Expressive Description LogicsDL setting instead full first-order. proofs constructive sensepresent effective procedures computing interpolants. also allows us establishupper bounds size interpolants.section organized follows. Section 3.1, show directly interpolation property holds ALC ALCF using worst-case optimal tableau (plural:tableaux) algorithm style Gore Nguyen (2007). Section 3.2, showinterpolation property also holds extensions ALC ALCF transitive inverse roles. Instead establishing results directly using tableaux,make use satisfiability signature preserving reductions ALC ALCF.main result says interpolants logics computed double exponential time. Section 3.3, study happens interpolants allowedexpressed full first-order logic show first-order interpolants computedsingle exponential time.3.1 Direct Algorithm Computing Interpolants ALCFsection, assume ALCF-concepts defined recursively Section 2.1 usingalso , t, R.C, 2R primitives, i.e., assume that, e.g., 2R constructorconcept language abbreviation ( 1R) anymore. Moreover, assumeconcepts negation normal form (NNF), i.e., negation occurs frontconcept names. well-known every ALCF-concept easily transformedequivalent one NNF pushing negation inwards using dualitiesconcept constructors (Tobies, 2001), e.g., R.C R.C. NNF complementconcept C written C.Another assumption make ALCF-TBoxesconsist axioms form > v C. assumptions make tableau notationcompatible standard tableau notation DLs. precisely, wantseparate rule concept constructor language (Horrocks et al., 2000).main result present section, namely Theorem 3.10, easily shownhold case make assumptions.Definition 3.2. Let C ALCF-concept let ALCF-TBox. conceptclosure cl(C, ) C smallest set concepts satisfying following conditions:C cl(C, );> v , cl(C, );cl(C, ) E sub(D), E cl(C, );R.D cl(C, ) R.D cl(C, ).rest section, fix two ALCF-concepts C0 , D0 two ALCF-TBoxes Tl ,Tr . denote union Tl Tr . l stands left r rightnaming scheme adopted Fitting (1996). allow us identify TBox(Tl Tr ) concept (C0 D0 ) inference made. biased concept expressionform C , C ALCF-concept {l, r} bias. Two relevant biasedconcept closures cll clr defined follows.cll = {C l | C cl(C0 , Tl )} clr = {C r | C cl(D0 , Tr )}.357fiTen Cate, Franconi, & Seylanuse Greek letters , denote bias.tableau rules producing subsets cll clr systematic way. aim,make use metaphor burden relief. Intuitively, subset cll clrburden satisfiability depends satisfiability one subsetscll clr call reliefs .Definition 3.3. Let cll clr.(C1 u C2 ) u-burden iff (C1 u C2 ) {(C1 ) , (C2 ) } 6 ;(C1 C2 ) t-burden iff (C1 C2 ) {(C1 ) , (C2 ) } = ;( 1R) 1-burden iff ( 1R) {(R.C) | (R.C) } 6 ;(R.C) -burden iff (R.C) ;( 2R) 2-burden iff ( 2R) .burden type burden above.Definition 3.4. Let cll clr, C burden , = {Dl | > v Tl } {Dr |> v Tr }. cll clr called C -reliefC = (C1 u C2 ) = {(C1 ) , (C2 ) } ;C = (C1 C2 ) either = {(C1 ) } = {(C2 ) };C = ( 1R) = {(R.C) | (R.C) };C = (R.C) = {C } {D | (R.D) } S;C = ( 2R) = {D | (R.D) } S.biased hC0 v D0 , i-tableau (hC0 v D0 , i-tableau short) vertex-labeleddirected graph hV, Ei labeling content : V 2cllclr . Intuitively, edges hg, g 0constructed algorithm, g 0 .content correspond C -relief g.content. Notetableau neither required tree directed acyclic graph (DAG)cycles may occur general. say node g tableau contains clasheither one following holds.g.content,{A , (A) } g.content,{( 1R) , ( 2R) } g.content.tableau expansion rules given Figure 1 expand tableau making usesemantics concepts. rule said applicable node g conditionsatisfied g, rule applied g before, g contain clash. orderguarantee finite expansion, use proxies following way. Whenever rule createsnew node g 0 g, attaching edge hg, g 0 E, tableau searched358fiBeth Definability Expressive Description LogicsRu ruleCondition:Action:Rt ruleCondition:Action:(C1 u C2 ) u-burden g.content.E E {hg, g 0 i} g 0 .content , (C1 u C2 ) -reliefg.content.(C1 C2 ) t-burden g.content.E E {hg, g1 i, hg, g2 i}, g1 .content 1 , g2 .content 2 ,1 , 2 (C1 C2 ) -reliefs g.content.R1 ruleCondition:( 1R) 1-burden g.content.Action:E E {hg, g 0 i} g 0 .content , ( 1R) -reliefg.content.R ruleCondition:= {(C1 )1 , . . . , (Cn )n } C iff C - 2-burdeng.content.Action:E E {hg, gi | 1 n} 1 n,gi .content , (Ci )i -relief g.content.Figure 1: Tableau expansion rules ALCFnode g 00 V g 0 .content = g 00 .content. g 00 found, edge hg, g 00added E g 0 discarded.interested deciding |= C0 v D0 . tableau algorithm consists twophases. first phase starts initial hC0 v D0 , i-tableau = h{g0 }, i,g0 .content = {(C0 )l , (D0 )r } {E l | > v E Tl } {E r | > v E Tr }.expanded repeatedly applying tableau expansion rules wayone rule applicable node time, first applicable rulelist [Ru , Rt , R1 , R ] chosen. first phase continues long rule applicableT. hC0 v D0 , i-tableau called complete output firstphase tableau algorithm.Lemma 3.5. first phase tableau algorithm terminates time 2O(n) ,n = |cll clr|. Moreover complete hC0 v D0 , i-tableau = hV, Ei produces,|V| 2n |E| 2O(n) .Proof. definition, first phase continues long rule applicablenode tableau. definition applicability, one ruleapplied node tableau.Let n = |cll clr|. definition proxy, |V| 2n since 2ndistinct subsets cll clr. Combining fact one ruleapplication per node, obtain 2n bound number rule applications. Now,easy see rule executes time polynomial n, i.e., execution timerule bounded nk , k constant. whole running359fiTen Cate, Franconi, & Seylantime first phase 2n nk . is,k2n nk = 2n+log n= 2n+klog n2O(n) .remains show bound |E|. definition tableau rules, out-degreenode cannot exceed n. Therefore, |E| n 2n , i.e., |E| 2O(n) .Let complete hC0 v D0 , i-tableau obtained first phase algorithm. purpose second phase tableau algorithm, i.e., Algorithm 1,construct following functions:1. status : V {sat, unsat} total function,2. int partial function V ALCF-concepts.g V, values assigned g thesedfunctions denoted g.statusint(g). Intuitively, status node g denotes C g.content C satisfiablew.r.t. ; int(g), defined, interpolant g.content following sense.Definition 3.6. Let cll clr. concept called interpolantF|= C l C v |= v C r CFsig(I) sig( C l C) sig( C r C),definition Algorithm 1, g V, int(g) defined if,if, g.status = unsat. order compute int(g) node g V g.status = unsat,Algorithm 1 uses interpolant calculation rules presented Figures 2, 3, 4.rules Figure 2 compute int(g) based solely g.content; ones Figure 3 take accountg.content successor g 0 g, values g 0 .content int(g 0 ); finally, onesFigure 4 take account g.content every successor g 0 g, values g 0 .contentint(g 0 ). invite reader verify that, indeed, whenever Algorithm 1 assigns unsatg.status, node g tableau, interpolant calculation ruleapplied compute int(g). Furthermore, interpolant calculation rule easilyseen sound. example, interpolant calculation rule Cu Figure 3 soundbecause, successor g 0 g, g 0 .content (C1 u C2 ) -relief g.content int(g 0 )interpolant g 0 .content (in sense Definition 3.6) necessarily alsointerpolant g.content.Let = hV, Ei complete hC0 v D0 , i-tableau output secondphase. said open g0 .status = sat; said closedg0 .status = unsat. determined open second phase,tableau algorithm returns 6|= C0 v D0 , otherwise returns |= C0 v D0 .next three results establish important properties tableau algorithmuse prove Theorem 3.10. proofs results require introductionstandard substantial amount notation DL modal logic literature.order present Theorem 3.10 clearly, defer proofs Appendix C.360fiBeth Definability Expressive Description LogicsAlgorithm 1 Second phase tableau algorithmPropagate:done true.every g V g.status 6= unsat:g contains clash,1. g.status unsat,lrrl2. apply one {Cl , Cr , Cll , Crr, C , C }, one whose condition satisfied,calculate int(g),3. done false.g 0 V hg, g 0 E, g 0 .status = unsat, g 0 .content (C1 u C2 ) -,( 1R) -, (R.C) , ( 2R) -relief g.content,1. g.status unsat,r6RRRl6RrRlRrR2. apply one {Cu , Cl61, Cr61, ClR1 , C1 , C , C , C , C }, one whose condition satisfied, calculate int(g),3. done false.g1 , g2 V g1 6= g2 , hg, g1 i, hg, g2 E, gi .status = unsat{1, 2}, gi .content (C1 C2 ) -relief g.content {1, 2},1. g.status unsat,2. apply one {Clt , Crt }, one whose condition satisfied, calculate int(g),3. done false.done = false.Assign:every g V g.status 6= unsat, g.status sat.361fiTen Cate, Franconi, & SeylanCl ruleCondition:Action:Cr ruleCondition:Action:Cll ruleCondition:Action:CrrruleCondition:Action:ClrruleCondition:Action:CrlruleCondition:Action:l g.content.int(g)r g.content.int(g) >{C l , (C)l } g.content, C form 1R.int(g){C r , (C)r } g.content, C form 1R.int(g) >{C l , (C)r } g.content, C form 1R.int(g) C{C r , (C)l } g.content, C form 1R.int(g) CFigure 2: Interpolant calculation rules ALCF (content dependent rules)Lemma 3.7. Let = hV, Ei output second phase. g V, g.status =unsat,1. g.content unsatisfiable w.r.t. ;2. int(g) defined interpolant g.content;n3. |int(g)| O(22 ), n = |cll clr|.next lemma establishes double exponential upper bound runtime Algorithm 1. consequence interpolant calculation double exponential upperbound size interpolants (cf. Lemma 3.7).Lemma 3.8. second phase tableau algorithm, i.e., Algorithm 1, runs timenO(22 ), n = |cll clr|.next proposition establishes soundness completeness algorithmconcept subsumption w.r.t. TBoxes ALCF.Proposition 3.9. closed hC0 v D0 , i-tableau |= C0 v D0 .tableau algorithm presented section two phases actuallyalgorithm compute interpolants double exponential size ALCF.upper bound optimal results establish Section 4 imply smallestinterpolants double exponential size.362fiBeth Definability Expressive Description LogicsCu ruleCondition:g 0 .content (C1 u C2 ) -relief g.content.Action:int(g) int(g 0 ).RCl61ruleCondition:g 0 .content ( 1R)l -relief g.contentbiased concept form (R.C)r g.content.Action:int(g) int(g 0 ).r6RC1 ruleCondition:g 0 .content ( 1R)r -relief g.contentbiased concept form (R.C)l g.content.Action:int(g) int(g 0 ).lRC1 ruleCondition:g 0 .content ( 1R)l -relief g.contentbiased concept form (R.C)r g.content.Action:int(g) int(g 0 )u 1R.CrR1 ruleCondition:g 0 .content ( 1R)r -relief g.contentbiased concept form (R.C)l g.content.Action:int(g) int(g 0 )t 2R.l6RC ruleCondition:g 0 .content (R.C)l - ( 2R)l -relief g.content,biased concept form (R.D)r g.content.Action:int(g) .Cr6R ruleCondition:g 0 .content (R.C)r - ( 2R)r -relief g.content,biased concept form (R.D)l g.content.Action:int(g) >.ClRruleCondition:g 0 .content (R.C)l - ( 2R)l -relief g.content,biased concept form (R.D)r g.content.Action:int(g) R.int(g 0 ).CrRruleCondition:g 0 .content (R.C)r - ( 2R)r -relief g.content,biased concept form (R.D)l g.content.Action:int(g) R.int(g 0 ).Figure 3: Interpolant calculation rules ALCF (single successor dependent rules)Theorem 3.10. ALCF-concepts C, ALCF-TBoxes T1 , T2 T1 T2 |=C v exists interpolant C hT1 , T2 computedtime double exponential |T1 | + |T2 | + |C| + |D|.Proof. Suppose C, ALCF-concepts T1 , T2 , ALCF-TBoxesT1 T2 = |= C v D. Proposition 3.9, closed hC v D, i363fiTen Cate, Franconi, & SeylanClt ruleCondition:Action:Crt ruleCondition:Action:g1 .content, g2 .content (C1 C2 )l -reliefs g.content.int(g) int(g1 ) int(g2 ).g1 .content, g2 .content (C1 C2 )r -reliefs g.content.int(g) int(g1 ) u int(g2 ).Figure 4: Interpolant calculation rules ALCF (multiple successor dependent rules)tableau = hV, Ei. means g0 .status = unsat, thus Lemma 3.7,ALCF-concept suchFthat int(g0 ) = interpolant g0 .content. LetX = >vET1 E = >vET2 E. Since interpolant g0 .content,|= C u X v I, |= v , sig(I) sig(C u X) sig(D ).fact |= X > |= , obtain |= C v |= v D;sig(I) sig(C u X) sig(D ), obtain sig(I) sig(C, T1 ) sig(D, T2 ). Henceinterpolant C hT1 , T2 i. Finally Lemma 3.8, computed timedouble exponential |T1 | + |T2 | + |C| + |D|.end section discussion techniques used. tableau algorithmdefined based tableau algorithm Gore Nguyen (2007). extendedalgorithm ALCF added machinery compute interpolants. generalinterpolation follows corollary cut-free sequent tableau calculus1 logic (e.g.,see Rautenberg, 1983; Fitting, 1996; Kracht, 2007); corollary give upperbounds size computation time interpolants unless calculus combineddecision procedure. section, goal obtain tight upper boundssize computation time interpolants ALCF. traditional tableau algorithmsDLs, e.g., one Horrocks et al. (2000), also used establish similar results(Seylan et al., 2009). crucial idea tableau algorithm provideexplicit representation tableau rule applications interpolantcalculated induction rule applications. chose non-traditional DL tableaualgorithm purposes based non-labeled2 tableau calculuscalculi actually commonly used proving interpolation results modal logics(e.g., Rautenberg, 1983).3.2 Extending Interpolation Transitive Inverse Rolessection, extend Theorem 3.10 logics order obtain main interpolation result Theorem 3.22. aim, present various polynomial reductionsreasoning one DL another. purpose reductions eliminate constructors language. technique use reductions well-known DLliterature called axiom schema instantiation technique (Calvanese, Giacomo,& Rosati, 1998; Calvanese, Giacomo, Lenzerini, & Nardi, 2001). Similar techniques also1. tableau calculus defined set tableau rules.2. non-labeled tableau calculus provides explicit representation individuals interpretations.364fiBeth Definability Expressive Description Logicsappear modal logic (Kracht, 2007). idea behind technique summarizedfollows.DLs syntactic variants modal logics. well-known axiom schemavalid modal logic corresponds certain condition accessibility relationframes logic (Blackburn, de Rijke, & Venema, 2001). example axiom schema4 : 2 22 defines class transitive frames. axiom schema instantiationtechnique based instantiating axiom schema finite number timesconcept cl relevant concept closure, adding instances TBoxobtain equi-satisfiable TBox. resulting TBox free constructorlanguage instantiated axiom schema.note input reductions normally concept TBox;interpolation, given pair concepts C1 , C2 pair TBoxes T1 , T2 .Therefore, require reductions mix signature sig(C1 , T1 )sig(C2 , T2 ) uncontrolled way. exactly mean clearLemma 3.14 Lemma 3.19. Naturally, calls extra notation.Definition 3.11. injective function : X NR , X finite subset NR{P | P NR }, called role renaming P NR , {P, P } 6 X. rolerenaming called safe signature range() = .Given L -concept C role renaming , Z (C) concept obtained Creplacing every occurrence every R dom() (R).Intuitively, use role renamings, name suggests, rename roles concepts.need make sure renaming operation well-defined thus, avoid mappingsrole inverse domain mapping. Safeness mappingw.r.t. signature property desire following reductions. starttransitive roles thus, instantiate axiom schema 2 22.Definition 3.12. Let C0 SIF-concept, SIF-TBox, safe rolerenaming sig(C0 , ) dom() = sig(C0 , ) NR+ range() NR+ = .(C0 , , ) defined ALCF I-TBox S1 (C0 , , )S2 (C0 , , ), S1 (C0 , , ) ={> v Z (C) | > v C }S2 (C0 , , ) = {Z (R.C) v Z (R.R.C) | R.C cl(C0 , ) {R, R } NR+ 6= }Note definition above, signature resulting ALCF I-TBoxequal signature original SIF-TBox C0 contains transitive roles.Introducing new non-transitive role names necessary alloweduse symbols NR+ logics without transitive roles (cf. Section 2.1). Althoughformulation following proposition slightly different one Lemma 6.23Tobies (2001), proof idea same.Proposition 3.13. SIF-concept C0 satisfiable w.r.t. SIF-TBoxALCF I-concept Z (C0 ) satisfiable w.r.t. ALCF I-TBox (C0 , , ),safe role renaming sig(C0 , ) dom() = sig(C0 , ) NR+ range() NR+ = .reduction (for concept satisfiability w.r.t. TBoxes) Definition 3.12 satisfiesfollowing property essential extending interpolation results logics365fiTen Cate, Franconi, & Seylantransitive roles. respect, also resembles splitting reduction functionsKracht (2007).Lemma 3.14. Let T1 , T2 SIF-TBoxes let C1 , C2 SIF-concepts.T1 T2 |= C1 v C2 iff (C1 , T1 , ) (C2 , T2 , ) |= Z (C1 ) v Z (C2 )safe role renaming sig(C1 u C2 , T1 T2 ) dom() = sig(C1 u C2 , T1T2 ) NR+ range() NR+ = .Proof. Let safe role renaming sig(C1 u C2 , T1 T2 ) specified lemma.use following claims proof.Claim 3.15. (C1 u C2 , T1 T2 , ) = (C1 u C2 , T1 , ) (C1 u C2 , T2 , ).Proof claim. () Suppose C v (C1 u C2 , T1 T2 , ). either C v212 , T1 T2 , ). former holds,2 , T1 T2 , ) C v (C1 u C(C1 u Cimmediately obtain desired result; thus, suppose latter holds. C vform Z (R.C) v Z (R.R.C), R.C cl(C1 u C2 , T1 T2 ), {R, R } NR+ 6= .Definition 3.2 R.C cl(C1 u C2 , T1 T2 ), obtain R.C cl(C1 u C2 , T1 )cl(C1 u C2 , T2 ). Definition 3.12 fact either R NR+ R NR+ ,Z (R.C) v Z (R.R.C) (C1 u C2 , T1 , ) (C1 u C2 , T2 , ),wanted show.() rather easy see direction claim holds.Claim 3.16. (C1 u C2 , T1 , ) (C1 u C2 , T2 , ) = (C1 , T1 , ) (C2 , T2 , ).Proof claim. () Suppose C v (C1 , T1 , ) (C2 , T2 , ). desired result0follows immediately C v = > v Z (C ), > v C 0 T1 T2 . Otherwise,Definition 3.12 C v form Z (R.C) v Z (R.R.C), R.C cl(C1 , T1 )cl(C2 , T2 ) either R NR+ R NR+ . R.C cl(C1 , T1 ) cl(C2 , T2 )cl(C1 , T1 ) cl(C2 , T2 ) cl(C1 u C2 , T1 ) cl(C1 u C2 , T2 ), obtain R.C cl(C1 uC2 , T1 ) cl(C1 u C2 , T2 ). Definition 3.12 fact either R NR+R NR+ , Z (R.C) v Z (R.R.C) (C1 u C2 , T1 , ) (C1 u C2 , T2 , ),wanted show.() Suppose C v (C1 u C2 , T1 , ) (C1 u C2 , T2 , ). desired result followsimmediately C v = > v Z (C 0 ), > v C 0 T1 T2 . Otherwise,Definition 3.12 C v form Z (R.C) v Z (R.R.C), R.C cl(C1 uC2 , T1 ) cl(C1 u C2 , T2 ), either R NR+ R NR+ . R.C cl(C1 uC2 , T1 ) cl(C1 u C2 , T2 ), fact C1 u C2 6= R.C, Definition 3.2, obtainR.C cl(C1 , T1 ) cl(C2 , T2 ). Definition 3.12 fact either R NR+R NR+ , Z (R.C) v Z (R.R.C) (C1 , T1 , ) (C2 , T2 , ),wanted show.lemma shown following way.T1 T2 |= C1 v C2 , iff366fiBeth Definability Expressive Description LogicsC1 u C2 unsatisfiable w.r.t. T1 T2 , iffZ (C1 u C2 ) unsatisfiable w.r.t. (C1 u C2 , T1 T2 , ) (Proposition 3.13), iffZ (C1 u C2 ) unsatisfiable w.r.t. (C1 u C2 , T1 , ) (C1 u C2 , T2 , ) (firstclaim), iffZ (C1 u C2 ) unsatisfiable w.r.t. (C1 , T1 , ) (C2 , T2 , ) (second claim), iff(C1 , T1 , ) (C2 , T2 , ) |= Z (C1 ) v Z (C2 ).need similar reduction eliminate inverse roles. De Giacomo (1996) presentsmethod reduce converse-PDL satisfiability PDL satisfiability using axiom schemainstantiation technique. Since DLs notational variants PDLs, techniqueeasily adapted DLs done Calvanese et al. (1998, 2001). idea instantiateconverse-PDL axiom schemas []h [ ]hi.Definition 3.17. Let C0 ALCF I-concept, let ALCF I-TBox,safe role renaming sig(C0 , ) dom() consisting inverse roles appearingC0 , range() NR+ = . (C0 , , ) defined ALCF-TBoxI1 (C0 , , ) I2 (C0 , , ), I1 (C0 , , ) = {> v Z (C) | > v C }I2 (C0 , , ) = {Z (C)v Z (R .R.C)| R.C cl(C0 , )}Note definition above, signature resulting ALCF-TBoxequal signature original ALCF I-TBox C0 contains inverse roles.Proposition 3.18 establishes correctness reduction concept satisfiability w.r.t.TBoxes. full proof proposition given Seylan (2012).Proposition 3.18. ALCF I-concept C0 satisfiable w.r.t. ALCF I-TBoxALCF-concept Z (C0 ) satisfiable w.r.t. ALCF-TBox (C0 , , ),safe role renaming sig(C0 , ) dom() consisting inverse roles appearingC0 range() NR+ = .following property reduction useful interpolation results.Lemma 3.19. Let T1 , T2 ALCF I-TBoxes let C1 , C2 ALCF I-concepts.T1 T2 |= C1 v C2 iff (C1 , T1 , ) (C2 , T2 , ) |= Z (C1 ) v Z (C2 )safe role renaming sig(C1 u C2 , T1 T2 ) dom() consistinginverse roles appearing C1 u C2 T1 T2 range() NR+ = .Proof. following claims shown analogously Claim 3.15 Claim 3.16, respectively.Claim 3.20. (C1 u C2 , T1 T2 , ) = (C1 u C2 , T1 , ) (C1 u C2 , T2 , ).Claim 3.21. (C1 u C2 , T1 , ) (C1 u C2 , T2 , ) = (C1 , T1 , ) (C2 , T2 , ).367fiTen Cate, Franconi, & Seylanargument last step proof Lemma 3.14.Theorem 3.22. Let L ALC extensions constructors {S, I, F}.L -concepts C1 , C2 L -TBoxes T1 , T2 , T1 T2 |= C1 v C2 , existsinterpolant C1 C2 hT1 , T2 computed time double exponential|T1 | + |T2 | + |C1 | + |C2 |.Proof. Theorem 3.10 already covers case L = ALCF.L = ALC. tableau algorithm ALCF (with proved Theorem 3.10)used without modification decide concept satisfiability w.r.t. TBox ALC.words, given ALC-concepts C1 , C2 ALC-TBox = T1 T2 , check|= C1 v C2 using algorithm. Observe executionalgorithm, R1 never applied clashes involving conceptform 1R. algorithm constructs closed hC1 v C2 , i-tableau, interpolantcalculation algorithm calculate interpolant ALCF. Since R1 never appliedfirst phase clash involving concept form 1R resultingtableau, interpolant calculation rules producing concepts form 1R 2R,rRnamely ClR1 , C1 , ones Figure 2, never applied second phase. Henceresulting interpolant actually ALC-concept. always interpolantT1 T2 |= C1 v C2 double exponential upper bound computation timeshown Theorem 3.10.L {ALCI, ALCF I}. Let C1 , C2 L -concepts T1 , T2 L -TBoxesT1 T2 |= C1 v C2 . Let role renaming specified Lemma 3.19: rolerenaming always exists. Lemma 3.19, (C1 , T1 , ) (C2 , T2 , ) |= Z (C1 ) vZ (C2 ), C1 , C2 ALC-concepts (ALCF-concepts) (C1 , T1 , ), (C2 , T2 , )ALC-TBoxes (respectively ALCF-TBoxes). compute interpolant Z (C1 )Z (C2 ) hI (C1 , T1 , ), (C2 , T2 , )i time double exponentialsize input.1. sig(I) sig(Z (C1 ), (C1 , T1 , )) sig(Z (C2 ), (C2 , T2 , )),2. (C1 , T1 , ) (C2 , T2 , ) |= Z (C1 ) v I,3. (C1 , T1 , ) (C2 , T2 , ) |= v Z (C2 ).Let 1 restriction rol(C1 , T1 ) 2 restriction rol(C2 , T2 );set 1 = range(1 ) 2 = range(2 ). Intuitively, 1 2 exactly setsnew role names introduced (C1 , T1 , ) (C2 , T2 , ), respectively. easysee sig(Z (C1 ), (C1 , T1 , )) sig(C1 , T1 ) 1 , sig(Z (C2 ), (C2 , T2 , ))sig(C2 , T2 ) 2 . item 1 above,sig(I) (sig(C1 , T1 ) 1 ) (sig(C2 , T2 ) 2 )simple distributivity argument, obtainsig(I) (sig(C1 , T1 ) sig(C2 , T2 )) (1 2 )(sig(C1 , T1 ) 2 ) (sig(C2 , T2 ) 1 )368fiBeth Definability Expressive Description LogicsSince sig(C1 , T1 ) 2 = sig(C2 , T2 ) 1 = ,sig(I) (sig(C1 , T1 ) sig(C2 , T2 )) (1 2 )let L -concept obtained replacing occurrencesrole name P 1 2 role R (R ) = P . Since injective,well defined. Moreover, Z (D) = I.claim every P 1 2 , role name R (R ) = P sig(C1 , T1 )sig(C2 , T2 ). Suppose P 1 2 . P range(1 ) range(2 ). Since 1 2defined restrictions rol(C1 , T1 ) rol(C2 , T2 ), respectively,R rol(C1 , T1 ) rol(C2 , T2 ) (R ) = P . R sig(C1 , T1 ) sig(C2 , T2 ).claim shown, sig(I) (sig(C1 , T1 ) sig(C2 , T2 )) (1 2 ),construction D, sig(D) sig(C1 , T1 ) sig(C2 , T2 ). Moreover, Z (D) = I,items 2 3 above, Lemma 3.19, obtain T1 T2 |= C1 v T1 T2 |= v C2 .Hence interpolant C1 C2 hT1 , T2 i. easy see timerequired compute stated theorem.L {S, SI, SF, SIF}. follows, let L 0 L without transitive roleconstructor, e.g., L = SIF, L 0 = ALCF I. know L 0 satisfiesstated theorem. Suppose C1 , C2 L -concepts T1 , T2 L -TBoxesT1 T2 |= C1 v C2 . proof proceeds analogously inverse role case,except course use Lemma 3.14.conclude, shown logic L stated theorem constructive waycompute interpolant, one exists, time double exponential size input.Hence theorem follows.3.3 Shorter First-Order Interpolantsshow interpolation algorithm adapted compute first-orderinterpolants single exponential time. proof proceed along following lines.First show double exponential size interpolants duerepeated occurrence subformulas algorithm yields single exponential sizeinterpolants using succinct (DAG-shaped opposed tree-shaped) concept representation. Next apply idea implicit work Avigad (2003), namely succinctlyrepresented first-order formulas transformed polynomial time equivalent ordinary tree-shaped first-order formulas structures least two elements.allows us compute single exponential first-order interpolants structures leasttwo elements. that, show single exponential interpolants structuresone element constructed reduction propositional logic. combining interpolants obtained via two methods, finally obtain desired single exponentialfirst-order interpolant arbitrary structures.Step 1: Singly-exponential interpolants via succinct representation startdefining notions allow us represent DAG-shaped concepts.Definition 3.23. Fix description logic L . axiom form C, NCC L -concept, called concept definition axiom L (or, L -CDA). Let369fiTen Cate, Franconi, & Seylansignature. acyclic terminology L set L -CDAs= {A1 C1 , . . . , Cn }{A1 , . . . , } = sig(Ci ) {A1 , . . . , Ai1 } {1, . . . , n}.succinct-L -concept pair hA, i, acyclic terminologyL concept name belonging sig(T ) \ . unfolding succinct-L concept hA, L -concept obtained repeatedly applyingCDAs , i.e., replacing occurrences left-hand side right-hand side,CDA applied.Note acyclic terminologies well-known DL literature (Baader & Nutt,2003).Example 3.24. Let consist following.Woman Person u FemaleMan Person u MaleHuman Woman Manacyclic terminology {Person, Female, Male}. unfolding succinctconcept hHuman,(Person u Female) (Person u Male).unfolding succinct-concept general exponentially longer.Proposition 3.25. Let L description logic. succinct-L -concept hA,O(1)unfolding C, |C| 2|T | .Theorem 3.26. Let L ALC extensions constructors {S, I, F}.L -concepts C1 , C2 L -TBoxes T1 , T2 , T1 T2 |= C1 v C2 , existssuccinct-L -concept hA, sig(C1 , T1 ) sig(C2 , T2 )unfolding hA, interpolant C1 C2 hT1 , T2 i,hA, computed time single exponential |T1 | + |T2 | + |C1 | + |C2 |.Proof. Let L one DLs mentioned theorem, let T1 T2 |= C1 v C2 ,T1 , T2 L -TBoxes C1 , C2 L -concepts, let = |T1 | + |T2 | + |C1 | + |C2 |.proof Theorem 3.22, first reduce T1 T2 |= C1 v C2 T10 T20 |= D1 v D2 ,T10 , T20 ALC-TBoxes (ALCF-TBoxes) D1 , D2 ALC-concepts (resp. ALCFconcepts).show interpolant calculation step Algorithm 1 ALCF (and thus ALC,see Figures 2, 3, 4) modified compute succinct-concept single exponential sizeinterpolant, instead concept.associate every node g tableau distinct fresh concept name Xg .new algorithm still uses interpolation calculation rules instead directlyassigning interpolant every node g g.status = unsat, construct acyclicterminology 0 sig(D1 , T10 ) sig(D2 , T20 ), acyclic terminology makes use370fiBeth Definability Expressive Description Logicsnew concept names Xg , unfolding succinct-concept hXg , 0interpolant g.content whenever g.status = unsat. set 0 initialized emptyset, throughout computation algorithm, 0 extended natural way.instance, suppose Clt applied g. Clt adds 0 CDA Xg Xg1 Xg2 ,g1 g2 successors node g tableau. Another examplelr } g.content. Clr addsclash rule. Suppose Clrapplied g {C , (C)00CDA Xg C. Lemma 3.5, follows |T | 2O(m) ; definitionAlgorithm 1, follows 0 acyclic terminology sig(D1 , T10 ) sig(D2 , T20 ).Moreover, T10 T20 |= D1 v D2 , Xg0 C 0 . hXg0 , 0succinct-concept sig(D1 , T10 ) sig(D2 , T20 ) unfolding easily showninterpolant D1 D2 T10 T20 .way similar proof Theorem 3.22, i.e., replacing back newly introduced role names inverse transitive roles 0 originals, obtain newterminology 00 . unfolding hXg0 , 00 guaranteed interpolant C1C2 hT1 , T2 i. Moreover, hXg0 , 00 size single exponential m.rest section, purpose obtain equivalent first-order formulagiven succinct-concept polynomial time. make use standard translation(see Definition 2.2). following, distinguish DL interpretationsfirst-order structures (we choose unary binary predicates first-orderlanguage symbols NC NR , respectively).Step 2: Singly-exponential FO interpolants interpretations two elements first-order formula (x) interpretation = hI , ,write I, |= (x) first-order assignment (x) =I, |= (x). |=2 , denote restriction relation |= considersinterpretations = hI , |I | 2. Similarly, |==1 , denote restrictionrelation |= considers interpretations = hI , |I | = 1.proof following theorem inspired result Avigad (2003),states that, structures least two elements, one efficiently eliminate acyclicdefinitions proofs. Theorem 3.27 viewed adaptation resultfirst-order translation succinct-concepts description logic.Theorem 3.27. Given succinct-SHIF-concept hB, signature , construct polynomial time first-order formula (x) , |=2 (x) x (C),C unfolding hB, i.proof Theorem 3.27 based lemma state next. expositoryreasons, convenient state lemma terms structures constantsymbols. constant symbols needed Theorem 3.27. usedmake statement proof following lemma readable.Lemma 3.28. Given acyclic terminology = {A1 C1 , . . . , Cn } SHIF,construct polynomial time first-order formula (x, y1 , . . . , yn , z) additionalconstant symbols 0 1, that, interpretations satisfying 0I 6= 1I ,elements a, ~b, c (where ~b = b1 , . . . , bn ),371fiTen Cate, Franconi, & Seylan|=(1I[a, ~b, c] ~b = k k {1, . . . , n}, c =0ICkIotherwise0I} 1I |0I {zk = 00I} Ck unfolding succinct-concept hAk , i.| {zk1 timesnk timesProof. define induction number n CDAs . n = 1,simply define (x, y, z)(x, y, z) = (y = 1) ((x (C1 ) z = 1) (x (C1 ) z = 0))Now, let n > 1 let 0 obtained removing last CDA. words,let = 0 {An Cn }. induction hypothesis, formula 0 (u, ~v , w) satisfyingrequired conditions w.r.t. 0 (where ~v = v1 , . . . , vn1 ). distinguish followingcases:1. Cn atomic concept functionality restriction signature . case,define follows, ~y = y1 . . . yn ~v = v1 . . . vn1 .(x, ~y , z) = u, ~v , w(T 0 (u, ~v , w) x = u ~y = ~v 0 z = w)(~y = 0 01 ((x (Cn ) z = 1) (x (Cn ) z = 0))))VHere, ~y = ~v 0 shorthand formulai<n yi = vi yn = 0, and, similarly,V~y = 0 01 shorthand formula i<n yi = 0 yn = 1.2. Cn form Ai < n. case, define follows:(x, ~y , z) = u, ~v , w 0 (u, ~v , w)((x = u ~y = ~v 0 z = w)(~y = 0 01 u = x ~v = ((w = 1 z = 0) (w = 0 z = 1))))Here, notation conventions apply Vprevious item. addition, ~v =used shorthand formula vi = 1 j6=i vj = 0. notations alsoused following items.3. Cn form Ai uAj i, j < n. first attempt, define (x, ~y , z) follows:(x, ~y , z) = u, ~v , w u0 , ~v 0 , w0 0 (u, ~v , w) 0 (u0 , ~v 0 , w0 )((x = u ~y = ~v 0 z = w)(~y = 0 01 u = u0 = x ~v = ~v 0 = j(w = w0 = z = 1 ((w = 0 w0 = 0) z = 0))))works, except fact 0 occurs twice formula, mayresult exponential blowup. solve problem replacing conjunction0 (u, ~v , w) 0 (u0 , ~v 0 , w0 )u00 , ~v 00 , w00 ((u00 = u~v 00 = vw00 = w)(u00 = u0 ~v 00 = v 0 w00 = w0 ) 0 (u00 , ~v 00 , w00 ))372fiBeth Definability Expressive Description Logics4. Cn form P.Ai < n. difficult case. followingformula expresses required property:(x, ~y , z) = u, ~v , w 0 (u, ~v , w)((x = u ~y = ~v 0 z = w)(~y = 0 01 z = 1 P xu ~v = w = 1)(~y = 0 01 z = 0u0 , ~v 0 , w0 (T 0 (u0 , ~v 0 , w0 ) P xu0 ~v 0 = w0 = 0)))However, before, formula still problem contains two copies0 . fix two steps. First, bring universal quantifiers front,transform formula following equivalent formula:u, ~v , w u0 , ~v 0 , w0 0 (u, ~v , w) 0 (u0 , ~v 0 , w0 )((x = u ~y = ~v 0 z = w)(~y = 0 01 z = 1 P xu ~v = w = 1)(~y = 0 01 z = 0 (P xu0 ~v 0 = w0 = 0)))Finally, before, replace conjunction 0 (u, ~v , w) 0 (u0 , ~v 00 , w0 )u00 , ~v 00 , w00 ((u00 = u~v 00 = vw00 = w)(u00 = u0 ~v 00 = v 0 w00 = w0 ) 0 (u00 , ~v 00 , w00 ))5. Cn form P .Ai < n. case handled like previous one.Note that, general, Cn could complex concept various Ai < n occur.However, complex CDAs always decomposed multiple simpler CDAskinds, cost polynomial increase size terminology.clear construction formula obtained satisfiesconditions stated lemma. obtained polynomial-time followsfact that, inductive definition , previously constructedformula 0 occurs once.ready proof Theorem 3.27.Proof Theorem 3.27. Let succinct-concept hAi , given, = {A1 C1 , . . . ,Cn }. Let (x) = (x, i, 1) let (x) = u, v(u 6= v 0 (x)), 0 (x)obtained (x) replacing 0 1 u v, respectively. that,every interpretation domain least two elements, every ,following conditions equivalent:1. I, |= (x)2. 0 , |= (x), interpretation 0 extends mapping constantsymbols 0 1 distinct elements .3. 0 , |= C, C unfolding hAi , i.4. I, |= C, C unfolding hAi , i.373fiTen Cate, Franconi, & Seylanequivalence 1 2 immediate construction . equivalence 23 follows Lemma 3.28. equivalence 3 4 immediate, since 0 1occur C. concludes proof.Definition 3.29. Let C, L -concepts let T1 , T2 L -TBoxes T1 T2 |=C v D. first-order formula (x) called FO interpolant C hT1 , T2following conditions hold:sig((x)) sig(C, T1 ) sig(D, T2 ),(T1 ) (T2 ) |= x.x (C) (x),(T1 ) (T2 ) |= x.(x) x (D).FO |=2 -interpolant FO |==1 -interpolant defined way above, exceptreplace occurrences |= |=2 |==1 , respectively.Proposition 3.30. Let L ALC extensions constructors {S, I, F}.L -concepts C1 , C2 L -TBoxes T1 , T2 , T1 T2 |= C1 v C2 , existsFO |=2 -interpolant C1 C2 hT1 , T2 computed time singleexponential |T1 | + |T2 | + |C1 | + |C2 |.Proof. Suppose T1 T2 |= C1 v C2 . Theorem 3.26, succinct-concepthA, sig(C1 , T1 ) sig(C2 , T2 ) unfolding hA, interpolantC1 C2 hT1 , T2 i, hA, computed time single exponential|T1 |+|T2 |+|C1 |+|C2 |. Theorem 3.27, first-order formula (x)constructed time polynomial |T | (hence single exponential |T1 |+|T2 |+|C1 |+|C2 |)sig((x)) sig(I),|=2 (x) x (I).follows (x) FO |=2 -interpolant C1 C2 hT1 , T2 whose size singleexponential |T1 | + |T2 | + |C1 | + |C2 |.Step 3: Singly-exponential FO interpolants interpretations one elementstill obtain interpolants structures one element. showProposition 3.35. essential idea interpolants structuressingleton domains much different propositional interpolants. First,give reduction concept subsumption w.r.t. TBoxes interpretations singletondomains entailment propositional logic.Definition 3.31. Let C SHIF-concept. mapping PL (C) defined inductively follows.PL (>) = >,PL (A) = A,PL (C) = PL (C),PL (C u D) = PL (C) u PL (D),PL (R.C) = AR u PL (C),PL ( 1R) = >,374fiBeth Definability Expressive Description LogicsAP = AP fresh concept name every role name P NR . SHIF-TBox, definePL (T ) = {PL (C) v PL (D) | C v } {AR v | R v }.Here, concept name AP , intuitively, expresses non-emptiness role P .Note transitive roles ignored translation, semanticstrivially satisfied interpretations whose domain singleton set. SHIF-conceptC, PL (C) ALC-concept without role constructors. view PL (C) propositionalformula (where concept names propositions, identify upropositional connectives , respectively). Similarly, SHIF-TBox , PL (T )set ALC CIAs without role constructors, view set propositionalformulae.Proposition 3.32. Let SHIF-TBox let C, SHIF-concepts.|==1 C vPL (T ) |= PL (C) v PL (D).Proof. () Let |==1 C v D, suppose |= PL (T ), PL (C)I . needshow PL (D)I . Let J obtained restricting domain elementreading interpretation role name P concept name AP .Formally,J = {s};NC , AJ iff AI ;P NR , P J = {hs, si} AIP , P J = , otherwise.definition above, trivially follows every P NR+ P J transitive. Moreover, every role R,hs, si RJ AIR .(1)see this, suppose first hs, si RJ . R = P P NR , AIP , i.e., AIR ;R = P P NR , AIP fact AP = AP(see Definition 3.31), obtain AIR . Hence AIR . direction, supposeAIR . R = P P NR , hs, si P J , i.e., hs, si RJ ; R = PP NR , fact AP = AP , hs, si P J thus, hs, si RJ .Hence (1) follows.Claim 3.33. every SHIF-concept C 0 , PL (C 0 )I (C 0 )J .Proof claim. proof induction structure C 0 . base case,C 0 = C 0 = >, trivial, boolean cases follow immediately inductivehypothesis. C 0 = R.D0 , following equivalences:PL (C 0 )I ;AIR PL (D0 )I (by semantics);375fiTen Cate, Franconi, & Seylanhs, si RJ (D0 )J (by (1) inductive hypothesis);(C 0 )J (by semantics).Finally, C 0 = 1R, since PL (C 0 ) = > , PL (C 0 )I . Moreover,definition J , (C 0 )J . PL (C 0 )I iff (C 0 )J ,wanted show.show J |= , i.e., J satisfies every CIA RIA . J satisfiesevery CIA direct consequence previous claim; proceed caseRIAs. Let R v hs, ti RJ . definition J , = t. Hencew.l.o.g. suppose hs, si RJ . (1), AIR . Since AR v PL (T )|= PL (T ), AIS . (1) again, implies hs, si J . Hence J satisfiesR v S.proceed towards goal PL (D)I follows. |= PL (T ), PL (C)I ,previous claim, obtain C J . Since J |= , follows |= C vDJ . using previous claim, conclude PL (D)I .() Let PL (T ) |= PL (C) v PL (D), suppose |= , C , = {s}.need show DI . Define interpretation J follows:J = {s};NC , AJ = AIP NR , (AP )J = {s} P = {hs, si}, (AP )J = otherwisefirst show every role RAJR hs, si R .(2)left-to-right, suppose AJR . R = P P NR , hs, si P , i.e.,hs, si RI ; R = P P NR , fact AP = AP ,AJP , implies hs, si R . Hence hs, si R . direction, supposeJhs, si RI . R = P P NR , AP , i.e., AJR ; R = PJP NR , hs, si P , implies AP = AP AR . Hence (2) follows.Claim 3.34. every SHIF-concept C 0 , (C 0 )I PL (C 0 )J .Proof claim. proof induction structure C 0 . base case,C 0 = C 0 = >, trivial, boolean cases follow immediately inductivehypothesis. C 0 = R.D0 , following equivalences(C 0 )I ;hs, si RI (D0 )I (by semantics = {s});0 JAJR PL (D ) (by (2) inductive hypothesis).PL (C 0 )J (by semantics).376fiBeth Definability Expressive Description LogicsFinally, C 0 = 1R, since = {s}, |= > v 1R, thus, (C 0 )I .Moreover, PL (C 0 ) = >, PL (C 0 )J . (C 0 )I iff PL (C 0 )J .show J |= PL (T ). definition, every CIA PL (T ) form (i)PL (C 0 ) v PL (D0 ), C 0 v D0 ; form (ii) AR v , R v .J satisfies CIAs form (i) direct consequence previous claim;focus CIAs form (ii). Let AR v PL (T ) AJR . (2),hs, si RI . Since |= R v , hs, si . (2) again,AJ. Hence J satisfies AR v .proceed towards goal DI follows. C previous claim,PL (C)J . J |= PL (T ) PL (T ) |= PL (C) v PL (D), obtainPL (D)J . Using previous claim again, conclude DI .Proposition 3.35. Let L ALC extensions constructors {S, H, I, F}.L -concepts C1 , C2 L -TBoxes T1 , T2 , T1 T2 |= C1 v C2 , existsFO |==1 -interpolant C1 C2 hT1 , T2 computed time singleexponential |T1 | + |T2 | + |C1 | + |C2 |.Proof. Let L one DLs mentioned theorem let C1 , C2 L -conceptslet T1 , T2 L -TBoxes T1 T2 |= C1 v C2 . immediately followsT1 T2 |==1 C1 v C2 . Proposition 3.32, T1 T2 |==1 C1 v C2 implies PL (T1 )PL (T2 ) |= PL (C1 ) v PL (C2 ). Theorem 3.10, interpolant PL (C1 )PL (C2 ) hPL (T1 ), PL (T2 )i computed time double exponential|T1 |+|T2 |+|C1 |+|C2 |. However, case dealing propositional formulaetableau algorithm easily modified construct tree-shaped proof insteadgeneral graph-shaped one eliminating use proxies. fact, describedstandard tableau algorithm propositional logic. well-known nodetree polynomial out-degree size input height treepolynominal size input. inspecting proof Theorem 3.10, one easilysee case computed time single exponential |T1 | + |T2 | + |C1 | + |C2 |.Finally let concept obtained replacing occurrence concept nameAR R.>. x (D) FO |==1 -interpolant C1 C2 hT1 , T2 i.easy see time required compute x (D) stated proposition.Step 4: Putting together result follows, puttingFO |==1 -interpolants FO |=2 -interpolants together:Theorem 3.36. Let L ALC extensions constructors {S, I, F}.L -concepts C, L -TBoxes T1 , T2 , T1 T2 |= C v D, existsFO interpolant (x) C hT1 , T2 (x) computed time singleexponential |T1 | + |T2 | + |C| + |D|.Proof. Let L one DLs mentioned theorem, let C, L -concepts,let T1 , T2 L -TBoxes T1 T2 |= C v D. Proposition 3.35,FO |==1 -interpolant (x) C hT1 , T2 computed timesingle exponential |T1 | + |T2 | + |C| + |D|; Proposition 3.30, FO |=2 interpolant (x) C hT1 , T2 computed time single exponential377fiTen Cate, Franconi, & Seylan|T1 | + |T2 | + |C| + |D|. Let(x) = (yz(y 6= z) (x)) (yz(y = z) (x)).Claim 3.37. (T1 ) (T2 ) |= x.x (C) (x)(T1 ) (T2 ) |= x.(x) x (D).Proof claim. prove first part. proof second part analogous.Let = hI , model T1 T2 , i.e., (T1 ) (T2 ), first-ordervariable assignment I, |= x (C). need show I, |= (x). aim,show I, |= (yz(y 6= z) (x)) I, |= (yz(y = z) (x)).First suppose I, |= yz(y 6= z). done prove I, |= (x).I, |= yz(y 6= z) implies |I | 2. I, |= x (C) (T1 ) (T2 ) |=2x.x (C) (x), obtain I, |= (x), done.suppose I, |= yz(y = z). done prove I, |= (x).I, |= yz(y = z) implies |I | = 1. I, |= x (C) (T1 ) (T2 ) |==1x.x (C) (x), obtain I, |= (x), done.Thus, conjuncts (x) satisfied I, . I, |= (x).assumption sig((x)), sig((x)) sig(C, T1 ) sig(D, T2 ). Sinceformulas yz(y 6= z) yz(y = z) introduce new predicates,sig((x)) sig(C, T1 ) sig(D, T2 ). Therefore, (x) FO interpolant ChT1 , T2 i. Moreover, since conjuncts computed single exponential time,(x). Hence theorem follows.4. Results Beth Definabilitysection, present main technical contributions paper. first introducenotions implicit explicit definability concepts define (projective) Bethdefinability property, fact primary notions interest paper.follows, L denotes description logics ALCX X {S, H, I, F}.Definition 4.1 (Implicit definability). Let C L -concept, L -TBox,sig(C, ). C implicitly definable if, every two models Jsatisfying = J and, P , P = P J , holds C = C J .words, given TBox, concept C implicitly definable setinstances depends extension predicates domain discourse.Deciding implicit definability L means, given L -concept C, L -TBox , setpredicates sig(C, ), check whether C implicitly definable .every predicate P sig(C, ) \ , introduce new predicate P 0 sig(C, ).e (respectively, Te ) concept (respectively, TBox) obtained replacing everylet Coccurrence predicate P 6 C (respectively, ) P 0 . Lemma 4.2, whose proofroutine adaptation analogous result first-order logic (Boolos, Burgess, & Jeffrey,2007), provides characterization implicit definability terms entailment. wellknown characterization often used definition implicit definability (Hoogland &Marx, 2002; Conradie, 2002).Lemma 4.2. Let C L -concept, L -TBox, sig(C, ). Ceimplicitly definable Te |= C C.378fiBeth Definability Expressive Description Logicsparticular, Lemma 4.2 reduces implicit definability L concept subsumptionproblem L w.r.t. TBoxes. also possible reduce concept subsumption problemL w.r.t. TBoxes problem deciding implicit definability L .Lemma 4.3. Let C v L -CIA, L -TBox, = sig(C u D, ),A0 NC \ . |= C v A0 implicitly definable{A0 v C u D}.Proof. () Suppose |= C v D. Let J models {A0 v C u D}= J P , P = P J . Obviously, J also models .|= C v D, (C u D)I = (C u D)J = . AI0 = AJ0 = .Hence, A0 implicitly definable {A0 v C u D}.() show contrapositive, i.e., 6|= C v D, A0 implicitly definable{A0 v C u D}. Suppose 6|= C v D. model(C u D)I . Let I1 = hI1 , I1 I2 = hI2 , I2I1 = I2 = ;AI1 = AI2 = AI , (NC \ A0 );RI1 = RI2 = RI , R NR ;AI0 1 = {s} AI0 2 = .easy see I1 I2 models {A0 v C u D}. Also observe I1I2 two interpretations domain agree assignpredicates . AI0 1 6= AI0 2 . Hence A0 implicitly definable{A0 v C u D}.Using Lemma 4.2 (for upper bound) Lemma 4.3 (for lower bound), following theorem follows immediately, since concept subsumption problem w.r.t. TBoxesExpTime-complete description logics question (Tobies, 2001).Theorem 4.4. ALC extensions constructors {S, H, I, F},implicit definability ExpTime-complete.Explicit definability syntactic counterpart implicit definability. Given conceptC, signature , TBox , asks existence concept formulatedC denote set every model .Definition 4.5 (Explicit definability). Let C L -concept, L -TBox,sig(C, ). say C explicitly definable L -concept|= C sig(D) . concept called explicit definitionC .Proposition 4.6. Let C L -concept, L -TBox, sig(C, ). Cexplicitly definable , C implicitly definable .379fiTen Cate, Franconi, & SeylanProof. Suppose C explicitly definable . concepte sig(D)|= C D. implies definition Te C,eeeee|= C D. |= C |= C monotonicitye Lemma 4.2, C implicitly definable|=. yield Te |= C C..Definition 4.7 (Beth definability property). L Beth definability property (BP)L -concepts C, L -TBoxes , signatures sig(C, ), C implicitlydefinable , C explicitly definable .Observe that, definition, restricts concept names role namesallowed appear explicit definition. obtain weaker versionBeth definability property restricting concept names occurring explicitdefinition. called concept-name Beth definability property (CBP). words,CBP refers existence explicit definitions signatures form NR .explain later, also reasons interested whether descriptionlogics satisfy Beth definability property restricted class finite interpretations. known Beth definability property, restricted finite structures,fails first-order logic (see e.g., Hoogland, 2001), spite fact holdsunrestricted case. specifically investigate Beth definability description logicsrestricted finite interpretations. call Beth definability property finite(BPF). Formally, BPF defined way BP, except replace,definition, occurrences word interpretation model finite interpretationfinite model, replace symbol |= |=f , |=f considers finite interpretations. addition, speak f-implicit definability f-explicit definability.follows Lemma 4.2 that, L FMP, BP BPF equivalent L .Hence makes sense specifically study BPF logics without FMP.4.1 Bounds Size Explicit Definitionsstart positive result BP direct application interpolation theorem,i.e., Theorem 3.22.Theorem 4.8 (BP). Let L ALC extensions constructors{S, I, F}. L -concepts C, L -TBoxes , signatures sig(C, ),C implicitly definable , C explicitly definable ,explicit definition C computed time double exponential |T | + |C|.Proof. Let L one DLs stated theorem, C L -concept,L -TBox, sig(C, ) C implicitly definable .e (where Te Ce obtained C,Lemma 4.2, Te |= C Crespectively, replacing occurrences predicates P 6 fresh predicates P 0esig(C, )). Theorem 3.22, interpolant C CeeehT , computed time double exponential |T | + |T | + |C| + |C|. Sincee Te ) = , (a) Te |= C vinterpolant, sig(I) sig(C, ) sig(C,e (b) Te |= Ce v C, Te |= v C,(b) Te |= v C.e|= C follows (a). structure Te , straightforwardly follows|= C I.380fiBeth Definability Expressive Description Logicse = 2 (|T | + |C|).time needed compute I, observe |T | + |Te | + |C| + |C|Hence computed time double exponential |T | + |C|.proof Theorem 4.8 uses Theorem 3.22. Similarly, use Theorem 3.36 instead, show first-order explicit definitions implicitly defined conceptscomputed single exponential time. Note Theorem 4.8 also establishes doubleexponential upper bound size explicit definitions considered logics. upperbound optimal show Theorem 4.11 explicit definitions Lmay need double exponentially big.essential tool proof Theorem 4.11, path-set constructionpreviously used Lutz (2006) characterize succinctness public announcementlogic compared epistemic logic. path-set construction also used Ghilardiet al. (2006) establish lower bound size concepts witnessing TBoxconservative extension another TBox.Definition 4.9. C ALC-concept, path-set PC C defined structuralinduction follows, denotes empty sequence denotes concatenationfinite sequences:P> = PA = {}, NC ;PC = PC ;PCuD = PC PD ;PR.C = {} {R p | p PC }.Intuitively, PC describes nestings role constructors C. use PC toolestablishing lower bounds size concepts.Lemma 4.10. every ALC-concept C, |C| |PC |.Proof. proof induction structure C.C atomic concept form > (with NC ), then, definition, |C| = 1|PC | = 1 since PC = {}. Hence |C| |PC |.Next, let C = D. inductive hypothesis, |D| |PD |. |PD | =|PD |, obtain |D| |PD |. Finally, fact |D| = |D| + 1, obtain |D||PD |. Hence |C| |PC |.Next, let C = C1 u C2 . inductive hypothesis, |C1 | |PC1 | |C2 ||PC2 |. implies |C1 |+|C2 | |PC1 |+|PC2 |. fact |C1 uC2 | = |C1 |+|C2 |+1,obtain |C1 uC2 | |PC1 |+|PC2 |. Finally, |PC1 |+|PC2 | |PC1 uC2 |, |C1 uC2 ||PC1 uC2 |. Hence |C| |PC |.Finally, let C = R.D. inductive hypothesis, |D| |PD |. implies|D| + 2 |PD | + 2. fact |R.D| = |D| + 2, obtain |R.D| |PD | + 2.Finally, |PD | + 1 = |PR.D |, |R.D| |PR.D |. Hence |C| |PC |.Theorem 4.11 (Explicit definition lower bound). Let = {R, S} NR . everyn N, ALC-concept Cn ALC-TBox Tn sig(Cn , Tn ), |Tn ||Cn | polynomial n, Cn implicitly definable Tn , smallestexplicit definition Cn Tn double exponentially long n.381fiTen Cate, Franconi, & SeylanProof. Fix n N. Let A1 , . . . , pairwise distinct concept names. useconcept names negations represent binary format number {0, . . . , 2n 1}.precisely, u . . . u A1 represents 0, u An1 . . . u A1 represents 1,on. Obviously, implies least significant bit position 1. every{0, . . . , 2n 1}, denote concept represents Ci . Note Ci ,either Aj Aj conjunct Ci , j {1, . . . , n}.k {1, . . . , n},let Xk = A1 u . . . u Ak1 u Aklet Yk = A1 u . . . u Ak1 u Ak .Note Xk Yk concept names use abbreviateCIAs. define Tn ALC-TBox consisting following CIAs.u . . . u A1 v R. u S.A1 . . . v R.> S.>every k {1, . . . , n} ,Xk v .Yk ul((Al u .Al ) (Al u .Al ))k<lnIntuitively, last item allows us decrease counter value one flippingrespective bits. Note |Tn | polynomial n Tn satisfiable. fact,present models Tn Claim 4.14. model Tn , everyevery {1, . . . , n}, either AIi (Ai )I virtue interpretation.Therefore, every exactly one {0, . . . , 2n 1} CiI .Claim 4.12. Let {1, . . . , 2n 1}.1. Tn |= Ci v R.Ci1 u S.Ci12. Tn |= Ci R.Ci1 S.Ci1Proof claim. 1, suppose = hI , model Tn , CiI ,{R, S} = . suffices show (.Ci1 )I .hs, ti done immediately; therefore, suppose hs, ti . need show.Ci1Ci = Bn u . . . u B1 , Bj = Aj Bj = Aj , j {1, . . . , n}.Denote B j concept Aj Bj = Aj , else concept Aj Bj = Aj . SinceCiI , exactly one k {1, . . . , n} XkI . CIAXk v .Yk ul((Al u .Al ) (Al u .Al ))k<ln382fiBeth Definability Expressive Description LogicsTn , (Bn u . . . u Bk+1 u B k u . . . u B 1 )I . hard seeCi1 = Bn u . . . u Bk+1 u B k u . . . u B 1 ., wanted show.Hence conclude Ci12. () Suppose = hI , model Tn CiI . Since 6= 0,A1 . . . v R.> S.> Tn , Ci (R.>)I Ci (S.>)I . is,either hs, ti RI hs, ti . cases, Ci11. Hence(R.Ci1 S.Ci1 ) .() Suppose = hI , model Tn (R.Ci1 S.Ci1 )I .means Ci1either hs, ti RI hs, ti .proceed towards contradiction suppose 6 CiI , i.e., (Ci )I .definition interpretation, CjI , j 6= j {0, . . . , 2n 1}. j = 0,u . . . u A1 v R. u S. Tn , immediately get contradiction. j 6= 0,. Since 6= j, (i 1) 6= (j 1). Thus, binary representation1, Cj11 j 1 must differ least one bit. implies Cj1Ci1k {1, . . . , n} Ak 6 Ak . Hence contradiction.define concepts D0 . . . D2n 1 inductively follows.D0 = R. u S.Di = R.Di1 S.Di1Intuitively, Di shape binary tree (due role names R, S) heighttree O(i). implies |C2n 1 | double exponential n.Claim 4.13. every {0, . . . , 2n 1}, Tn |= Ci Di .Proof claim. proof induction i. base case = 0.axioms Tn , trivially follows Tn |= A1 u . . . R. u S.. words,Tn |= C0 D0 . Hence claim holds base case.inductive step, suppose > 0. previous claim, Tn |= Ci R.Ci1S.Ci1 ; inductive hypothesis, Tn |= Ci1 Di1 . Tn |= CiR.Di1 S.Di1 wanted show.previous claim, {0, . . . , 2n 1}, Di explicit definitionCi = {R, S} Tn . Proposition 4.6, Ci implicitly definableTn . rest proof, show explicit definition CiTn least double exponentially long. aim, introduce interpretationsbased elements , denotes set strings symbols. precisely, every p 0 |p| 2n 1, define interpretation Ipfollows.Ip = {p0 | p0 prefix p};NC ,383fiTen Cate, Franconi, & Seylan= Aj j {1, . . . , n},AIp = {p0 Ip | conjunct C|p||p0 | },6= Aj j {1, . . . , n}, AIp = ;NR ,Ip = {hp1 , p2 Ip Ip | p2 = p1 }, ,Ip = , NR \ .following claim easy show.Claim 4.14. every p 0 |p| 2n 1,Ip |= Tn ,C|p|p .Denote every {0, . . . , 2n 1}, set p |p| = .Claim 4.15. Let {0, . . . , 2n 1} let C ALC-concept sig(C) ={R, S} Tn |= Ci C. PC .Proof claim. Suppose first = 0. = {}. Moreover, definitionPC . Hence PC , wanted show.suppose > 0. proceed towards contradiction. Supposepa \ PC . Let pb prefix pa |pb | = 1. Since > 0, pb well-defined.claim Ipb Ipa sub(C) {s p | p PD } PC ,DIpb DIpa .(3)proof induction structure D. Since base boolean casestrivial, treat case = .E, .(). Suppose DIpb . Ipb hs, ti Ipb E Ipb .Since Ipa well, former yields hs, ti Ipa . thus remains showE Ipa . definition, = PD = {} { p | p PE }. Thus,assumption {s p | p PD } PC yields {s} {t p | p PE } PC . implies{t p | p PE } PC . induction hypothesis E Ipb , obtainE Ipa , wanted show direction proof.(). Suppose DIpa . Ipa hs, ti IpaE Ipa . definition, = PD = {} { p | p PE }. Thus,assumption {s p | p PD } PC yields {s} {t p | p PE } PC . implies{t p | p PE } PC .(4)(4) PE , obtain PC . Since pa 6 PC , means 6= pa .Ipb hs, ti Ipb . (4), E Ipa , induction hypothesis,E Ipb . Hence (.E)Ipb .384fiBeth Definability Expressive Description LogicsThus, shown (3) holds. arrive contradiction follows.IpClaim 4.14, Ci pa Ci1b , since |pa | = |pb | = 1, respectively.IpIpCi1b implies definition Ci 6 Ci b . Tn |= Ci C Claim 4.14,obtain C Ipa 6 C Ipb . contradicts immediate consequence(3), namely C Ipb iff C Ipa . Hence contradiction. Thus, conclude PC> 0.show theorem, argue follows. Suppose C ALC-conceptnTn |= C2n 1 C sig(C) . previous claim 2 1 PC .nnnndefinition, |2 1 | = 22 1 thus, 22 1 |PC |. Lemma 4.10, 22 1 |C|.Hence theorem follows.Remark 4.16. role disjunction constructor, present ALC, Cnwould admit single exponentially long explicit definition Tn Theorem 4.11.Remark 4.17. lower bound argument Theorem 4.11 works CBP wellsetting = .Combined Theorem 4.8, Theorem 4.11 implies implicit definitions using general TBoxes exactly double exponentially succinct explicit definitions usingacyclic terminologies. closes open problem ten Cate et al. (2006) sizeexplicit definitions. Moreover, theorems establish exact bound sizeequivalent rewritings concept queries considered Seylan et al. (2009). Theorem 4.11also shows Theorem 1 Seylan et al. (2010), claims single exponential upperbound size explicit definitions ALC, wrong. source problemproof Theorem 1 Lemma 1, claims single exponential upper boundsize interpolants ALC.4.2 Failure Beth Definability Presence Role Hierarchiesshow BP fails description logics consider include rolehierarchies (H). shows BP indeed stronger property CBPlogics CBP (ten Cate et al., 2006).Theorem 4.18. Let L ALCH extensions constructors {S, I, F}.L BP.Proof. Let = {R1 , R2 } consider ALCH-TBox consistsv R1v R2R1 .A u S. v R2 .AR1 .A u S. v R2 .Aeasy see satisfiable. fact, present two models below.Claim 4.19. S.> implicitly definable .385fiTen Cate, Franconi, & SeylanR1R1wR2R2R2vbR1Figure 5: Interpretations J used disproving BP ALCHProof claim. Define XI = {s | .hs, ti R1I R2I }. show that,whenever |= , (S.>)I = XI . establishes claim, since XI dependsR1I R2I .First, show (S.>)I XI . Suppose (S.>)I .hs, ti . RIAs , hs, ti R1I R2I . Hence XI .Next, show XI (S.>)I . contradiction, suppose XI 6 (S.>)I ,i.e., (S.)I . hs, ti R1I R2I hs, ti 6 .definition interpretation, either (i) AI (ii) (A)I . (i),R1 .A u S. v R2 .A , (A)I , contradiction. (ii),R1 .A u S. v R2 .A , AI , contradiction. HenceXI (S.>)I .Let = hI , interpretation= {s, t},R1I = R2I = = {hs, ti};RI = , R NR \ ( {S});B = , B NC .Let J = hJ , J interpretationJ = {w, v, a, b},R1J = {hw, ai, hv, bi}, R2J = {hw, bi, hv, ai};RJ = , R NR \ ;AJ = {a};B J = , B (NC \ {A}).interpretations J depicted Figure 5. hard see Jmodels . Furthermore, two structures indistinguishable conceptssignature , following sense:Claim 4.20. SHIF-concepts C sig(C) = {R1 , R2 },1. C w C J ;386fiBeth Definability Expressive Description Logics2. C v C J ;3. C C J ;4. C b C Jproof claim straightforward, simultaneous induction structureconcept C (alternatively, bisimulations used establish result).Since (S.>)I w 6 (S.>)J , follows SHIF-concept Csig(C) |= S.> C. summary, ALCH-concept S.>implicitly definable ALCH-TBox , S.> explicitly definableSHIF. conclude BP fails every description logicincludes ALCH included SHIF.Theorem 4.18 shows Theorem 10 Seylan et al. (2010), claims ALCHextensions and/or BP, incorrect. mistake proofTheorem 9, presents reduction concept satisfiability problem w.r.t.TBoxes SHI problem ALC, actually used computingSHI-interpolants.4.3 Failure Beth Definability Finiteconsider BPF (the analogue Beth Definability finite structures).start, explain motivations. Seylan et al. (2009) consider ontology-based dataaccess setting, traditional ABoxes replaced DBoxes. Syntactically, DBoxesdefined way ABoxes, semantics different: ABoxmerely assumed express true facts, DBox assumed list true factsspecified subset signature (known set data predicates). Thus, example,= {A(a), R(a, b)} DBox data predicates R, and, definitionsemantics DBoxes, that, every model D, AI = {aI } RI = {haI , bI i}.setting, TBox may contain predicates data predicatesauthors use BP determine whether concept query signature TBoxrewritten equivalent first-order query data predicates. possible,computing certain answers original query reduced computinganswers rewriting DBox, viewed database. settingdescribed here, DLs without FMP, natural consider BPF BP.reason that, every interpretation DBox, data predicates are, definition, finiterelations. fact, appropriate analogue BP setting one restrictedinterpretations data predicates finite rest signatureunrestricted. variant BP viewed common generalization BP BFP.study here, negative results present BFP applywell.Theorem 4.21 establishes BPF fails L , L DL (amongones consider) lacking FMP. precisely, show L -TBox , L concept C, signature C f-implicitly definable ,f-explicit definition L , i.e., L -concept sig(D)|=f C D. Intuitively, reason failure BPF logics387fiTen Cate, Franconi, & Seylanexpress transitive closure role (see also discussionproof Theorem 4.21).Theorem 4.21. Let L ALCF extensions constructors {S, H}.L BPF.Proof. will, fact, prove something stronger: construct implicit definitioncorresponding explicit definition even full first-order logic.Let A, B, X concept names let R role name. Suppose = {R, A}. ConsiderALCF I-TBox consists following.> v 1R u 1RB v R.Bv XR.(A u B) v XR.X v Xshow concept f-implicitly definablef-explicitly definable . concept question u B. Noteconcept finitely satisfiable w.r.t. , i.e., finite model(A u B)I 6= . fact, provide model below.interpretation = hI , i. sequence s0 , . . . , sn elements calledfinite R-path n > 0 hsi , si+1 RI < n. infinite R-path definedanalogously. R-path start end nodes calledR-cycle. show two claims useful proof theorem.Claim 4.22. Let finite model . B , hs, si (RI )+ , (RI )+transitive closure RI .Proof claim. Suppose B . axiom B v R.B implies existencefollowing infinite R-path:p = s0 , s1 , . . .s0 = 0, si B .Since finite, 0 n < sn = sm . n = 0, immediatelyhs, si (RI )+ . Otherwise, claim pairs hsi , sj sequencehsn , sm i, hsn1 , sm1 i, . . . , hs0 , smn i, si = sj . base case follows immediatelysn = sm . inductive step, inductive hypothesis sni =smi = t, . definition p, hsni1 , ti RI hsmi1 , tiRI , imply axiom > v 1R |= sni1 = smi1 . Hence= s0 = smn . hs, si (RI )+ , wanted show.Claim 4.23. u B f-implicitly definable .Proof claim. interpretations I, defineYI = {s | hs, si (RI )+ AI }.388fiBeth Definability Expressive Description Logicsshow that, finite models , (A u B)I = YI . implies claim, sinceYI depends RI AI .() Suppose (A u B)I . Claim 4.22, know hs, si (RI )+ .YI .() Suppose YI . AI hs, si (RI )+ . Since AI , enoughshow B . definition interpretation, either B (B)I .(B)I , hs, si (RI )+ , (A u B)I , axioms R.(A u B) v XR.X v X , (X)I would contradiction AIv X . must B .rest proof showing f-explicit definition u B. aim, start defining interpretation , parameterizednatural number n > 0.= {s0 , . . . , s2n+1 } {t0 , . . . , t2n+1 }RIn = {hsi , si+1 | 0 2n} {hti , ti+1 | 0 2n} {ht2n+1 , t0 , }i= {sn , tn }B = X = {ti | 0 2n + 1}Claim 4.24. every first-order formula (x) n > 0 |= [sn ]|= [tn ].Proof. apply Gaifman locality theorem (cf. Libkin, 2004). present setting,unary binary relations, concerned formulassingle free variable, Gaifman locality theorem particularly easy state. Giveninterpretation elements a, b , say b distance n relativesignature , sequence s0 , . . . , sm 0 n s0 = a, sm = b,0 < m, pair hsi , si+1 belongs P (P ) binary relation (i.e.,role name) P . interpretation I, element , natural number n 0,denote a,n interpretation whose domain consists elementsdistance n a, whose relations ones restrictedsubset domain. Gaifman locality theorem stated follows:every first-order formula (x), natural number n > 0 that, structureselements a, b , a,n isomorphic b,n , via isomorphism mapsb, |= [a] |= [b].Now, let (x) first-order formula, let n > 0 natural number givenGaifman locality theorem. Consider instance constructed earlier.immediately clear construction sn ,n isomorphic tn ,n , viaisomorphism maps sn tn . Therefore, |= [sn ] |= [tn ].Claim 4.25. SHIF-concept C sig(C) |=f (A u B) C.Proof claim. proceed towards contradiction. Suppose C ALCF I-conceptsig(C) = {R, A} |=f u B C. Let (x) = x (C). Since sn (A u B)In ,|=f u B C, fact finite model , 6|= [sn ];389fiTen Cate, Franconi, & Seylanreasoning, |= [tn ]. previous claim, |= [sn ]|= [tn ], contradiction.summary, ALCF I-concept u B f-implicitly definableALCF I-TBox , u B f-explicitly definable evenSHIF (or first-order logic, matter). follows that, L proper extensionALCF constructors {S, H}, L BPF.point specific counterexample BPF described proof Theorem 4.21 actually admits explicit definition one allow use transitiveclosure. Specifically, shown u (R.>) u (R+ .R.>) explicitdefinition, R+ denotes transitive closure role R.4.4 Transitive Closure Operatorproof Theorem 4.21 suggests failure BPF considered logics maycaused fact express transitive closure. raises questionwhether one regain BPF adding transitive closure constructor ALCF I.section, show ALCF extended transitive closure constructor stilllacks BPF.following, denote L+ language obtained L additionally allowing R+ role every role R L . allows us include rolesinductive definition concepts. However, L includes functionality restrictions, then,usual, forbid use transitive closure inside functionality restrictions.words, concepts form 1R, R allowed make use transitive closureconstructor.semantics transitive closure construct expected, namely, (R+ )Irelation{hs, ti | s1 , . . . , sn (n > 1) s1 = s, sn = t, hsi , si+1 RI 1 < n}.Theorem 4.26. ALCF + BPF.Proof. Consider following ALCF + -TBox .> v 1R> v R.>> v R+ .Av BR.B v BR.B v Beasy see finitely satisfiable, i.e., finite model. fact, providefinite models , n > 0, below. first show concept name B f-implicitlydefinable = {R, A} show f-explicit definitionconcept language. interpretation I, R-path R-cycledefined proof Theorem 4.21.Claim 4.27. Let finite model . , hs, si (R+ )I .Proof claim. Identical proof Claim 4.22 proof Theorem 4.21.390fiBeth Definability Expressive Description LogicsClaim 4.28.(a) |=f > v R .>,(b) |=f > v 1R.Proof claim. Part (a) follows immediately previous claim.prove part (b), suppose, sake contradiction 6|=f > v 1R.finite model s, t, u 6= u hs, ti, hs, ui RI .Since > v 1R |= part (a) claim, (R )Igraph total function . Since 6= u hs, ti, hs, ui RI , also knowcardinality image function must strictly smaller cardinalitydomain, i.e., |Y | < |I |. implies ( . contradicts> v R.> |= . Hence conclude |=f > v 1R.s, , write odd(s, t) R-path odd length t, is,R-path s0 , . . . , sn = s0 , = sn , n odd. Note R-path likes0 , . . . , sn , always n > 0.Claim 4.29. finite models ,B = {s | .odd(s, t) AI }.Proof claim. () Suppose B . first claim, R-cycle p = s0 , . . . , sn ,s0 = sn = n > 0. Since > v R+ .A |= ,hs, ti (R+ )I AI . claim = si , {1, . . . , n 1}.show this, proceed towards contradiction.Suppose claim hold. R-path t0 , . . . , tm ,t0 = tm = t. Obviously, path different R-cycle p sinceoccur p. using |=f > v 1R previous claim, show everyindividual ti actually appears p, contradicts fact appearp. Hence conclude {1, . . . , n 1} si = t.show odd. v B , |= , AI , 6 B .using B axioms R.B v B , R.B v B , one easilyshow induction odd. implies odd(s, t).Hence odd(s, t) AI , wantedshow.() Suppose odd(s, t) AI . impliesR-path s0 , . . . , sn s0 = s, sn = t, n odd. AI ,v B , |= , 6 B . using fact n odd, axiomsR.B v B R.B v B , one easily show induction B .Claim 4.29 implies B f-implicitly definable . rest proofshows f-explicit definition B . n 0, letfollowing interpretation:= {s0 , . . . , s2n+3 }391fiTen Cate, Franconi, & SeylanRIn = {hsi , si+1 | 0 < 2n + 3} {hs2n+3 , s0 i}= {sn+2 },B = {si | 0 2n + 3 odd(si , sn+2 )}.Intuitively, R-cycle (even) length 2n + 4, whose elements satisfyconcept name and/or B. Observe , n 0, model . Define function: N follows.(n + 2) n + 2d(si ) =(n + 2) < n + 2words, d(s) distance sn+2 .ALCF + -concept C, denote md(C) modal depth C, is,maximal nesting depth role constructors C. Formally,md(A) = md(>) = md( 1R) = 0md(C) = md(C)md(C u D) = max{md(C), md(D)}md(R.C) = md(R+ .C) = md(C) + 1CA R form P P P NR .Claim 4.30. {0, . . . , n} s, s0 \ {s | d(s) i},C iff s0 CALCF + -concepts md(C) sig(C) .Proof claim. Let {0, . . . , n}, s, s0 \ {s | d(s) i}, CALCFI + -concept md(C) sig(C) . proof induction i.= 0. Since md(C) = 0 sig(C) , C obeys following grammar:C ::= > | | 1S | C | C u C= R = R (recall forbid use transitive closure inside functionality restrictions).induction structure C, show C iff s0 C .C = >. definition interpretation, >In s0 >In . Hence>In iff s0 >In .C = (recall concept name ). assumption, 6= sn+2s0 6= sn+2 . definition , obtain 6 s0 6 . Henceiff s0 .C = 1S. definition , |S (t)| = 1.particular, |S (s)| = |S (s0 )| = 1. Hence ( 1S)In iff s0 ( 1S)In .392fiBeth Definability Expressive Description LogicsC = D. Follows easily inductive hypothesis C.C = C1 u C2 . Follows easily inductive hypothesis C.Hence conclude C iff s0 C , = 0.Next, consider case > 0, let md(C) sig(C) .C obeys following grammar:C ::= S.E | + .E | C | C u Cmd(E) 1, = R = R . induction structure C, showC iff s0 C .C = S.E md(E) 1, = R = R . definition ,exactly one hs, ti exactly one t0hs0 , t0 . Moreover, t, t0 \ {s | d(s) 1}.following equivalent:(S.E)InE (since individual hs, ti )t0 E (by inductive hypothesis i)s0 (S.E)In (since individual hs0 , t0 ).= + .E md(E) = 1, = R = R . Suppose first (S + .E)In .hs, ti (S )+ E . distinguishfollowing cases:6= s0 . definition , immediately obtain hs0 , ti (S )+ ;E implies s0 (S + .E)In .= s0 . definition , hs0 , si (S )+ . Moreover, s, s0\ {s | d(s) 1}. inductive hypothesiss0 E , E , implies hs0 , si (S )+ s0 (S + .E)In .Hence s0 (S + .E)In cases, wanted show. directionright left shown analogously.cases shown easily inductive hypothesis C.Hence claim follows.Claim 4.31. ALCF + -concept C sig(C) {A, R} |=f B C.Proof claim. proceed towards contradiction suppose existenceconcept C. definition, md(C) = n, n 0; s0 , s1 \ {s | d(s)n}. previous claim, s0 C iff s1 C . factfinite model |=f B C, s0 B iff s1 B . impliesdefinition Claim 4.29 odd(s0 , sn+2 ) iff odd(s1 , sn+2 ), contradiction.Hence conclude exists ALCF + -concept C sig(C) {A, R}|= B C.393fiTen Cate, Franconi, & Seylanproof theorem follows. Claim 4.29, B f-implicitly definable= {A, R} . Claim 4.31, B f-explicitly definable. Hence ALCF + BPF.5. Concluding Remarkspaper, studied BP expressive DLs commonly used concept constructors.constructors appear Web Ontology Language OWL-Lite (Horrocks et al.,2003). OWL-Lite superseded OWL 2, supports important constructors nominals, denoted language, qualified number restrictions,denoted Q language. already results available regarding BPlogics Q O.Q generalization F ten Cate et al. (2006) show via model-theoretic argumentCBP holds ALCQ. believe BP also shown hold ALCQALCQI using model-theoretic argument; although argument gives upperbound size explicit definitions. Extending upper bound results sizeexplicit definitions logics appears difficult unavailabilitynatural optimal tableau algorithm logics.logics O, besides concept role names, assume set NI = {i, j, . . .}nominals. Syntactically, nominals treated atomic concepts semanticallynominal interpreted singleton set. presence nominals gives rise two different Beth definability properties. first one, allowed restrict nominalsappearing implicit/explicit definitions making part signature ;second one, definitions allowed use nominal NI . Obviously, first onestronger property. Ten Cate et al. (2006) show even second property failsALCO. also observe extending ALCO concepts form @i C enoughregain CBP. Intuitively, @i C says point satisfying nominal also satisfiesconcept C.similar way, one try identify extension ALCH BP.proof Theorem 4.18, argument failure BP considered logicsexpress role conjunction. remains open ALCH extended roleconjunction constructor BP. Another interesting open question identify minimalextension ALCF BPF.Theorem 3.36, know compute first-order explicit definitions singleexponential size, given concept implicitly defined TBox. leaveanother open problem existence matching lower bound, i.e., familyTBoxes implicitly defining concept smallest explicit definitions first-orderlogic single exponentially big?Acknowledgmentsgrateful Carsten Lutz Maarten Marx helpful discussions topic.substantial part research carried extended visit Inanc Seylan394fiBeth Definability Expressive Description LogicsUC Santa Cruz 2010, thank Phokion Kolaitis hospitality. also thankanonymous reviewers extensive comments.Balder ten Cate supported NSF grants IIS-0905276 IIS-1217869.Appendix A. QuasimodelsDecision procedures based semantic tableau construct model given formula/concept, finite representation model model unfolded.paper, use term quasimodel denote finite representation following Andreka, Nemeti, van Benthem (1998). Various names usedliterature, including Hintikka structures (Schwendimann, 1998), model graph (Gore, 1999),even tableau (Horrocks & Sattler, 2007). Modulo differences, building blocksstructures sets finite concepts subset relevant conceptclosure. using definition concept closure cl(C, ) given Section 3.1.Remark A.1. rest appendix, assume ALCF-concepts definedrecursively Section 2.1 using also , t, R.C, 2R primitives; conceptsNNF; ALCF-TBoxes consist axioms form > v C. discussionassumptions, refer reader beginning Section 3.1.every subset concept closure suitable take part quasimodel. Depending logic hand, sets satisfy basic consistency requirements. Following,e.g., Lutz et al. (2005), use term type denote sets satisfying requirements. Note, however, non-membership concept type implymembership negation concept type. respect, typessimilar Hintikka sets, also called downward-saturated sets (cf. Fitting, 1996).Definition A.2. Let C0 ALCF-concept let ALCF-TBox.cl(C0 , ) called hC0 , i-type ALCF A, C, C1 , C2 , R.C,2R, 1R cl(C0 , ),(P ) 6 ;(P ) {A, A} 6 ;(Pu ) C1 u C2 , C1 C2 ;(Pt ) C1 C2 , C1 C2 ;(Pv ) > v C , C ;(P./ ) { 1R, 2R} 6 ;(P1 ) { 1R, R.C} , R.C .type belongs quasimodel, may force type also belongquasimodel, instance witness existential statement. fact, quasimodelcollection types coherent sense.395fiTen Cate, Franconi, & SeylanDefinition A.3. Let C0 ALCF-concept, ALCF-TBox , two hC0 , itypes ALCF.R.Cwrite === R.C {C} {C 0 | R.C 0 } .2Rwrite === 2R {C 0 | R.C 0 } .set Q hC0 , i-types ALCF hC0 , i-quasimodel ALCF satisfies:(a) 0 Q C0 0 ;R.C(b) every Q R.C , type Q === ;2R(c) every Q 2R , type Q === .following theorem useful soundness completeness proofs tableauinterpolation algorithms. proof inspired Marx Venema (2007).Theorem A.4. ALCF-concept C0 satisfiable w.r.t. ALCF-TBoxhC0 , i-quasimodel ALCF.Proof. () Given model = hI , C0I 6= , carve ,set concepts L(s) cl(C0 , ) follows.L(s) = {C cl(C0 , ) | C }.let Q = {L(s) | }.Claim A.5. Q hC0 , i-type ALCF.Proof claim. Suppose Q. = L(s) . verify conditionsDefinition A.2.definition, = thus 6 thus 6 L(s). Hence (P ) satisfied.virtue interpretation, case AI (A)I .Hence (P ) satisfied.C1 u C2 L(s), (C1 u C2 )I . Since interpretation, C1IC2I . C1 , C2 L(s). Hence (Pu ) satisfied.C1 C2 L(s), (C1 C2 )I . Since interpretation, C1I C2I .C1 L(s) C2 L(s). Hence (Pt ) satisfied.> v C , C thus C . C L(s). Hence (Pv )satisfied.Suppose contradiction (P./ ) hold. ( 1R)I (2R)I . contradiction. Hence (P./ ) satisfied.Suppose { 1R, R.C} . assumption ( 1R)I (R.C)I .follows exactly one hs, ti RI C .(R.C)I . Hence (P1 ) satisfied.396fiBeth Definability Expressive Description LogicsSince shown conditions Definition A.2 satisfied, concludehC0 , i-type ALCF.claim Q hC0 , i-quasimodel. Claim A.5, Q, hC0 , i-typeALCF. Thus remains show condition (a), (b), (c) Definition A.3satisfied.(a), since C0I 6= , s0 s0 C0I constructionQ, L(s0 ) Q. Hence, condition (a) satisfied.condition (b), suppose R.C L(s) . means (R.C)I ,i.e., individual hs, ti RI C . constructionQ, C L(t). let R.D L(s). construction Q,(R.D)I . implies hs, ti RI DI . construction Q again,R.Cobtain L(t). Hence, L(s) === L(t); conclude (b) satisfied.proof (c) analogous.() Suppose Q hC0 , i-quasimodel ALCF. idea proof constructinterpretation inductively using Q show |= C0I 6= .construction, need introduce notation first.Let interpretation let L : Q. pair hs, CiC cl(C0 , ) called defect w.r.t. L, if,R.C L(s) hs, ti RI C L(t),2R L(s) |{t | hs, ti RI }| < 2.Fix map f : cl(C0 , ) N, let linear order Cartesian product N Norder type (recall countably infinite linear order said order typeelement order finitely many elements less it;well known linear orders N N order type ).ready define induction interpretations Ii = hIi , Ii Ii Nmappings Li : Ii Q, N.Base case. condition (a) Definition A.3, type 0 Q C0 0 .Define interpretation I0 follows.I0 = {s}, N;NC ,0 , AI0 = {s},6 0 , AI0 = ;R NR , RI0 = .Set L0 = {s 7 0 }.Inductive step. defect Ii w.r.t. Li , set Ii+1 = Ii Li+1 = Li ;otherwise, let hs, Ci least defect Ii w.r.t. Li , i.e., every defect ht, Di Ii w.r.t.Li , hs, f (C)i ht, f (D)i (using fact order type ). Li (s) QCconditions (b) (c) Definition A.3, Q Li (s) =.C = R.D, let N \ Ii define397fiTen Cate, Franconi, & SeylanIi+1 = Ii {t},NC ,, AIi+1 = AIi {t},6 , AIi+1 = AIi ;NR ,= R, Ii+1 = {hs, ti} Ii ,6= R, Ii+1 = Ii .Also set Li+1 = Li {t 7 }. C = 2R, let t1 , t2 N \ Ii t1 6= t2 defineIi+1 = Ii {t1 , t2 },NC ,, AIi+1 = AIi {t1 , t2 },6 , AIi+1 = AIi ;NR ,= R, Ii+1 = {hs, t1 i, hs, t2 i} Ii ,6= R, Ii+1 = Ii .Also set Li+1 = Li {t1 7 , t2 7 }. finishes inductive construction. defineinterpretation follows:= i0 Ii ,P NC NR , P = i0 P Ii .Also set L = i0 Li . Observe L total mapping Q.Claim A.6. concepts C cl(C0 , ) , C L(s) C .Proof claim. Let C stated claim. Suppose C L(s).definition L, N C Li (s); let smallest natural numbersatisfying C Li (s), i.e., Ii interpretation introduced s. inductionstructure C, show C . Since Q, 6 (P ), followsC 6= . Hence, consider remaining cases C.C = >. assumption , i.e., >I .C = A, NC . definition Ii Li (s), immediatelyfollows AIi . implies definition AI .C = A, NC . Since Li (s) Q, Li (s) satisfies (P ). Li (s),6 Li (s). One easily show induction k i,6 AIk . implies assumption k N, 6 AIk .definition I, obtain 6 AI , i.e., (A)I .398fiBeth Definability Expressive Description LogicsC = C1 u C2 . Follows easily inductive hypothesis (Pu ).C = C1 C2 . Follows easily inductive hypothesis (Pt ).C = R.D. Let hs, ti RI . need show DI .hs, ti RI , k N hs, ti RIk ; w.l.o.g. assume Ikinterpretation introduced t. follows E cl(C0 , )Ehs, Ei defect Ik1 w.r.t. Lk1 Lk (s) =Lk (t). implies Lk (t).definition L, obtain L(t). inductive hypothesis,implies DI . Hence, (R.D)I .C = R.D. assumption i, hs, Ci defect Ii w.r.t. Li .definition , finitely many pairs ht, Ei N E cl(C0 , )ht, f (E)i hs, f (C)i. implies k >fix defect hs, Ci step k. Ik hs, ti RIkLk (t). definition I, hs, ti RI L(t).inductive hypothesis, latter implies DI . Hence, (R.D)I .C = 1R. Suppose contradiction t1 , t2 t1 6= t2hs, t1 i, hs, t2 RI . k1 , k2 N hs, ti RIki Ikiinterpretation introduced ti , {1, 2}. construction,implies concepts C1 , C2 cl(C0 , ) hs, Ci defectCIki 1 w.r.t. Lki 1 Lki (s) =Lki (ti ), {1, 2}. followsCi 6= 2R, {1, 2}; otherwise, would obtain contradiction (P./ ).Thus, C1 = R.D1 C2 = R.D2 . definition cl(C0 , ),R.D1 , R.D2 cl(C0 , ); (P1 ), implies R.D1 , R.D2 Lk1 (s) =Lk2 (s). Suppose w.l.o.g. k1 < k2 . D2 Lk1 (t1 ). contradictsfact hs, R.D2 defect Ik2 1 w.r.t Lk2 1 .C = 2R. case shown similarly case C = R.D.Since considered possible cases, conclude claim holds.Using Claim A.6, direction Theorem shown easily follows.base case inductive construction, I0 C0 L0 (s).implies C0 L(s) Claim A.6, obtain C0I . Moreover, Claim A.6(Pv ), |= . Hence C0 satisfiable w.r.t. .Appendix B. Useful Lemmas Tableau Correctness Interpolationcll clr, define(l) = {C | C l cll} (r) = {C | C r clr}.() shorthand (l) (r). followingsignature set ALCFconcepts concern. define sig(S) = CS sig(C). Let finite setALCF-conceptsALCF-TBox. say satisfiable w.r.t.satisfiable w.r.t. . Moreover cll clr satisfiable w.r.t.() satisfiable w.r.t. .399fiTen Cate, Franconi, & SeylanLemma B.1. Let cll clr satisfiable w.r.t. .?-burden ? {u, 1, , 2} -relief ,satisfiable w.r.t. ;t-burden , -relief satisfiablew.r.t. .Proof. Suppose stated Theorem, i.e., satisfiable w.r.t. .means () satisfiable w.r.t. . TheoremA.4hC, i-quasimodel Q ALCF, C = D() D. means Q() . also use term h, i-quasimodel Q.Assume (C1 u C2 ) u-burden . (C1 u C2 ) -relief ={(C1 ) , (C2 ) }. (Pu ), {C1 , C2 } thus () . Hence () satisfiablew.r.t. .Assume ( 1R) 1-burden . ( 1R) -relief ={(R.C) | (R.C) }. (R.C) , R.C (P1 ), R.C .Hence () satisfiable w.r.t. .Assume (R.C) -burden . () condition (b) Definition A.3, Q {C} {D | R.D ()}; (Pv ),{E | > v E } . Let (R.C) -relief . () . Hencesatisfiable w.r.t. .Assume ( 2R) 2-burden . () condition (b) Definition A.3, Q {D | R.D ()}; (Pv ),{E | > v E } . Let 2R-relief . () .Hence satisfiable w.r.t. .Assume (C1 C2 ) t-burden . (C1 C2 ) -relief ,() (Pt ). Hence, (C1 C2 ) -relief satisfiable w.r.t.T.Proposition B.2. Let ALCF-TBox C0 , C1 , . . . , Cn , ALCF-concepts.1. |= C0 u C1 u . . . u Cn v D,|= R.C0 u R.C1 u . . . u R.Cn v R.D.2. |= v C1 . . . Cn ,|= R.D v R.C1 . . . R.Cn .3. |= C1 u . . . u Cn v D,|= 2R u R.C1 u . . . u R.Cn v R.D.Proof. 1, proceed towards contradiction. Suppose |= C0 u C1 u . . . u Cn v6|= R.C0 u R.C1 u . . . u R.Cn v R.D. model|= C0 u C1 u . . . u Cn v 6|= R.C0 u R.C1 u . . . u R.Cn v R.D. latter400fiBeth Definability Expressive Description Logics(R.C0 u R.C1 u . . . u R.Cn )I 6 (R.D)I .is, hs, ti RI , (C0 u C1 u . . . u Cn )I , (D)I .contradicts |= C0 u C1 u . . . u Cn v D.2, proceed towards contradiction. Suppose |= v C1 . . . Cn6|= R.D v R.C1 . . . R.Cn . model |= vC1 . . . Cn 6|= R.D v R.C1 . . . R.Cn . latter(R.D)I 6 (R.C1 . . . R.Cn )I .hs, ti RI , DI , (C1 u . . . u Cn )I . contradicts|= v C1 . . . Cn .Proposition B.3. Let C ALCF-concept R role name.|= 1R u R.C u R.C 1R u R.C.Proof. |= 1R u R.C u R.C v 1R u R.C trivial. direction,suppose contradiction 6|= 1R u R.C v 1R u R.C u R.C. meansinterpretation = hI , 6|= 1R u R.C v 1R u R.C u R.C.Thus ( 1R)I , (R.C)I , (R.C)I .last two t1 , t2 hs, t1 i, hs, t2 RI , t1 C , t2 (C)I .( 1R)I , t1 = t2 contradiction.Lemma B.4. Let cll clr.1. l , interpolant ;2. r , > interpolant ;3. concept C form 1R,(a) {C l , (C)l } , interpolant ;(b) {C r , (C)r } , > interpolant ;(c) {C l , (C)r } , C interpolant ;(d) {C r , (C)l } , Cinterpolant ;4. (C1 u C2 ) -relief interpolant , interpolant;5. 1 2 (C1 C2 )l -reliefs , I1 , I2 interpolants 1 , 2 respectively, I1 I2 interpolant ;6. 1 2 (C1 C2 )r -reliefs , I1 , I2 interpolants 1 , 2 respectively, I1 u I2 interpolant ;7. ( 1R)l -relief , biased concept form (R.C)r ,interpolant , interpolant ;8. ( 1R)r -relief , biased concept form (R.C)l ,interpolant , interpolant ;401fiTen Cate, Franconi, & Seylan9. ( 1R)l -relief , biased concept form (R.C)r ,interpolant , Iu 1R interpolant ;10. ( 1R)r -relief , biased concept form (R.C)l ,interpolant , 2R interpolant ;11. (R.C)l -relief , interpolant , biased conceptform (R.D)r , interpolant ;12. (R.C)r -relief , interpolant , biased conceptform (R.D)l , > interpolant ;13. (R.C)l -relief , interpolant , biasedconcept form (R.D)r , R.I interpolant ;14. (R.C)r -relief , interpolant , biasedconcept form (R.D)l , R.I interpolant ;15. ( 2R)l -relief , interpolant , biased conceptform (R.D)r , interpolant ;16. ( 2R)r -relief , interpolant , biased conceptform (R.D)l , > interpolant ;17. ( 2R)l -relief , interpolant , biasedconcept form (R.D)r , R.I interpolant ;18. ( 2R)r -relief , interpolant , biasedconcept form (R.D)l , R.I interpolant .Proof. 1. Suppose (l) = {X1 , . . . , Xn } {} (r) = {Y1 , . . . , Ym }. |=u X1 u . . . , Xn v |= v Y1 . . . Ym hold trivially. Since logicalconstant, = sig() sig((l)) sig((r)). Hence 1 satisfied.2. Suppose (r) = {Y1 , . . . , Ym } {} (l) = {X1 , . . . , Xn }. |=X1 u . . . , Xn v > |= > v Y1 . . . Ym > hold trivially. Since > logicalconstant, = sig(>) sig((l)) sig((r)). Hence 2 satisfied.3a. Suppose (l) = {X1 , . . . , Xn } {C, C}(r) = {Y1 , . . . , Ym }. |=X1 u . . . , Xn u C u Cv |= v Y1 . . . Ym hold trivially. Since logicalconstant, = sig() sig((l)) sig((r)). Hence 3a satisfied.argument 3b analogous previous case.3c. Suppose (l) = {X1 , . . . , Xn } {C} (r) = {Y1 , . . . , Ym } {C}.|=X1 u . . . u Xn u C v C, |= C v Y1 . . . Ym (C),sig(C) sig((l)) sig((r))hold trivially. Hence 3c satisfied.argument 3d analogous previous case.4. Suppose (C1 uC2 )l -relief , interpolant , (l) = {X1 , . . . , Xn }{C1 uC2 }, (r) = {Y1 , . . . , Ym }. assumption, |= X1 u. . .uXn u(C1 uC2 )uC1 uC2 vI, i.e., |= X1 u. . .uXn u(C1 uC2 ) v |= v Y1 t. . .tYm . assumption again,sig((l)) = sig((l)) sig((r)) = sig((r)), thus sig(I) sig((l)) sig((r)).402fiBeth Definability Expressive Description LogicsTherefore interpolant . case (C1 u C2 )r -reliefinterpolant shown analogously. Hence 4 satisfied.5. Suppose 1 2 (C1 C2 )l -reliefs , I1 , I2 interpolants 1 , 2respectively, (l) = {X1 , . . . , Xn } {C1 C2 }, (r) = {Y1 , . . . , Ym }. assumption,|= X1 u . . . u Xn u (C1 C2 ) u C1 v I1 |= X1 u . . . u Xn u (C1 C2 ) u C2 v I2 .following.|= I1 I2 w (X1 u . . . u Xn u (C1 C2 ) u C1 )(X1 u . . . u Xn u (C1 C2 ) u C2 )|= I1 I2 w (X1 u . . . u Xn u (C1 C2 )) u (C1 C2 )|= I1 I2 w X1 u . . . u Xn u (C1 C2 )half, assumption |= I1 v Y1 . . . Ym |= I2 v Y1 . . . Ym .|= I1 I2 v Y1 . . . Ym . Clearly, sig(I1 I2 ) sig((l)) sig((r)). Hence5 satisfied.argument 6 analogous previous case.7. Suppose( 1R)l -relief ,interpolant ,(l) = {X1 , . . . , Xn } { 1R} {R.C1 , . . . , R.Ck }, {R.C1 , . . . , R.Ck } ={R.C (l)},(r) = {Y1 , . . . , Ym },biased concept form (R.C)r .Let E = R.C1 u . . . u R.Ck . assumption,|= X1 u . . . u Xn u 1R u R.C1 u . . . u R.Ck u E v I.Proposition B.3|= X1 u . . . u Xn u 1R u R.C1 u . . . u R.Ck vwanted show. half, since biased conceptform (R.C)r , (r) = (r).|= v Y1 . . . Ymwanted show. assumption sig((l)) = sig((l)) sig((r)) =sig((r)), thus sig(I) sig((l)) sig((r)). Therefore interpolant . Hence7 satisfied.8 shown analogously previous case.9. Suppose( 1R)l -relief ,403fiTen Cate, Franconi, & Seylaninterpolant ,(l) = {X1 , . . . , Xn } { 1R} {R.C1 , . . . , R.Ck }, {R.C1 , . . . , R.Ck } ={R.C (l)},(r) = {Y1 , . . . , Ym } {R.D1 , . . . , R.Dl }, {R.D1 , . . . , R.Dl } = {R.C(r)},biased concept form (R.C)r .Let E = R.C1 u . . . u R.Ck . assumption,|= X1 u . . . u Xn u 1R u R.C1 u . . . u R.Ck u E v I.Also, trivially following.|= X1 u . . . u Xn u 1R u R.C1 u . . . u R.Ck u E v 1R.Combining two, get|= X1 u . . . u Xn u 1R u R.C1 u . . . u R.Ck u E v Iu 1Rwanted show.half, let F = R.C1 . . . R.Cl . assumption I,|= v Y1 . . . Ym R.C1 . . . R.Cl F.this, trivially get|= v Y1 . . . Ym R.C1 . . . R.Cl F 2R.Proposition B.3,|= v Y1 . . . Ym R.C1 . . . R.Cl 2R,implies|= Iu 1R v Y1 . . . Ym R.C1 . . . R.Cl ,wanted show. assumption sig((l)) = sig((l)) sig((r)) =sig((r)), thus sig(I) sig((l)) sig((r)). Moreover, since biasedconcept form (R.C)r , R sig((l)) sig((r)). conclusion, sig(Iu 1R)sig((l)) sig((r)). Hence 9 satisfied.10 shown analogously previous case.11. Suppose following:(R.C)l -relief ,interpolant ,FE = >vCTl C, F = >vCTr C,404fiBeth Definability Expressive Description Logics(l) = {X1 , . . . , Xn } {R.C} {R.D1 , . . . , R.Dk }, {R.D1 , . . . , R.Dk } ={R.D (l)},biased concept form (R.D)r .last assumption, (r) = {C | > v C Tr }. assumption, |= v F . Since|= F v , |= thus |= R.I . assumption|= C u D1 u . . . u Dk u E v I. Since |= > v E, |= C u D1 u . . . u Dk v I.Proposition B.2|= R.C u R.D1 u . . . u R.Dk v R.I.However |= R.I means|= R.C u R.D1 u . . . u R.Dk v .Hence,|= X1 u . . . u Xn u R.C u R.D1 u . . . u R.Dk vwanted show. half, let (r) = {Y1 , . . . , Ym }. Since|= , trivially|= v Y1 . . . Ymfinal step, need showsig() sig((l)) sig((r)).follows easily since logical constant. Hence 11 satisfied.12. Suppose following:(R.C)r -relief ,interpolant ,FE = >vCTl C, F = >vCTr C,(r) = {Y1 , . . . , Ym } {R.C} {R.D1 , . . . , R.Dk }, {R.D1 , . . . , R.Dk } ={R.D (r)},biased concept form (R.D)l .last assumption, (l) = {C | > v C Tl }. assumption, |= E v I. Since|= > v E, |= > thus |= R.I >. assumption |=v CtD1 t. . .tDk tF . Since |= F v , |= v CtD1 t. . .tDk .Proposition B.2|= R.I v R.C R.D1 . . . R.Dk .However |= > R.I, means|= > v R.C R.D1 . . . R.Dk .405fiTen Cate, Franconi, & SeylanHence,|= > v Y1 . . . Ym R.C R.D1 . . . R.Dkwanted show. half, let (l) = {X1 , . . . , Xn }. Since|= >, trivially|= X1 u . . . u Xn v >final step, need showsig(>) sig((l)) sig((r)).follows easily since > logical constant. Hence 12 satisfied.13. Suppose following:(R.C)l -relief ,interpolant ,FE = >vCTl C, F = >vCTr C,(l) = {X1 , . . . , Xn } {R.C} {R.D1 , . . . , R.Dk }, {R.D1 , . . . , R.Dk } ={R.D (l)},(r) = {Y1 , . . . , Ym } {R.C1 , . . . , R.Cl }, l 1 {R.C1 , . . . , R.Cl } ={R.C (r)}.assumption, |= C u D1 u . . . u Dk u E v I. Since |= > v E,|= C u D1 u . . . u Dk v I. Proposition B.2|= R.C u R.D1 u . . . u R.Dk v R.I(5)(5),|= X1 u . . . u Xn u R.C u R.D1 u . . . u R.Dk v R.Iwanted show. argue half. assumption,|= v C1 . . . Cl F . Since |= F v , |= v C1 . . . Cl .Proposition B.2|= R.I v R.C1 . . . R.Cl(6)(6),|= R.I v Y1 . . . Ym R.C1 . . . R.Clwanted show. final step, need showsig(R.I) sig((l)) sig((r)).follows easily since assumption sig(I) sig((l)) sig((r)) R sig((l))sig((r)), latter consequence l 1.argument 14 analogous previous case. Moreover 15, 16, 17, 18shown similarly 11, 12, 13, 14, respectively.406fiBeth Definability Expressive Description LogicsAppendix C. Tableau Correctness, Termination, InterpolationLemma C.1. Let = hV, Ei output second phase. every node g V:1. g.status either sat unsat.2. g.status = unsat, either one following holds.g sink node3 containing clash;exactly one successor g 0 g (C1 uC2 ) g.content,g 0 .content (C1 u C2 ) -relief g.content g 0 .status = unsat;exactly one successor g 0 g ( 1R) g.content,g 0 .content ( 1R) -relief g.content g 0 .status = unsat;exactly n successors g1 , . . . , gn g, n cardinalityset {(C1 )1 , . . . , (Cn )n } - 2-burdens g.content, gi .content(Ci )i -relief g.content {1, . . . , n}, {1, . . . , n}gi .status = unsat;exactly two successors g1 , g2 g (C1 C2 )g.content, gi .content (C1 tC2 ) -relief g.content {1, 2}, g1 .content 6=g2 .content, gi .status = unsat {1, 2}.3. g.status = sat, either one following holds.g sink node containing clash,exactly one successor g 0 g (C1 uC2 ) g.content,g 0 .content (C1 u C2 ) -relief g.content g 0 .status = sat;exactly one successor g 0 g ( 1R) g.content,g 0 .content ( 1R) -relief g.content g 0 .status = sat;exactly n successors g1 , . . . , gn g, n cardinalityset {(C1 )1 , . . . , (Cn )n } - 2-burdens g.content, gi .content(Ci )i -relief g.content {1, . . . , n}, {1, . . . , n}gi .status = sat;exactly two successors g1 , g2 g (C1 C2 )g.content, gi .content (C1 tC2 ) -relief g.content {1, 2}, g1 .content 6=g2 .content, {1, 2} gi .status = sat.Proof. 1 clearly follows fact every node assigned status unsatPropagate step Algorithm 1 gets status sat end (Assign) Algorithm 1.Let g V. definition tableau algorithm, g satisfies exactly onefollowing structural conditions:g sink node;3. node outgoing edges407fiTen Cate, Franconi, & Seylanexactly two successors g1 , g2 g (C1 C2 ) g.content,gi .content (C1 C2 ) -relief g.content {1, 2};exactly one successor g 0 g (C1 u C2 ) g.content,g 0 .content (C1 u C2 ) -relief g.content;exactly one successor g 0 g ( 1R) g.content,g 0 .content ( 1R) -relief g.content;exactly n successors g1 , . . . , gn g, n cardinality set{(C1 )1 , . . . , (Cn )n } - 2-burdens g.content, gi .content (Ci )n relief g.content {1, . . . , n}.Suppose first g.status = unsat g clearly respects 2 waysnode get status unsat Propagate. Suppose g.status = sat. g.statusdetermined Assign Algorithm 1. distinguish structural propertiesg above.Suppose g sink node. means rule applied g. g.status =sat, immediately obtain g.status contain clash; g.statuscontains clash, would g.status = unsat contradicts factevery node g V, value g.status calculated once.Suppose exactly two successors g1 , g2 g (C1 C2 )g.content, gi .content (C1 C2 ) -relief g.content {1, 2}. Since g.status = sat,g.status undefined right Assign. implies {1, 2}gi .status undefined otherwise g.status = unsat. Assign,gi .status = sat. Hence, g satisfies 3.Suppose exactly one successor g 0 g (C1 u C2 )g.content, g 0 .content (C1 u C2 ) -relief g.content. Since g.status = sat, g.statusundefined right Assign. implies that, g 0 .status undefined Assignotherwise g.status = unsat. Assign, g 0 .status = sat. Hence, g satisfies3.remaining cases shown analogously. Hence lemma follows.Proof Lemma 3.7. start observations. Algorithm 1 assigns statusunsat nodes V Propagate phase. status assignment steps inducesequence 0, 1, 2. . . .. assignment step i, associate set Vi Vinodes status unsat far. Observe step step + 1, extendVi single node only. induction number status assignment steps, firstshow g Vi ,g.content unsatisfiable w.r.t. ;ALCF-concept Cint(g) = C,C interpolant g.content,|int(g)| 2i+2 1.408fiBeth Definability Expressive Description Logicsbase case, V0 = {g} sink node g V containing clash.Obviously, g.content unsatisfiable. interpolant calculation rules Figure 2 covercases clash thus, ALCF-concept assigned int(g). Lemma B.4,int(g) interpolant g.content. claim |int(g)| 2. int(g) form >,, A, A, clear; int(g) form 1R (or 2R), observeencoded using one symbol 1 (resp. 2) one symbol R. Hence,|int(g)| 2 2i+2 1 inductive hypothesis holds base case.inductive step, let Vi+1 = Vi {g}. inductive hypothesis holds everyg 0 Vi , trivially; thus, consider case g. Lemma C.1, five casesdistinguish:1. g sink node containing clash. shown analogously base case.2. exactly one successor g 0 g (C1 u C2 ) g.content,g 0 .content (C1 u C2 ) -relief g.content g 0 .status = unsat. inductivehypothesis, g 0 .content unsatisfiable w.r.t. , int(g 0 ) interpolant g 0 ,|int(g 0 )| 2i+2 1. (the contrapositive version of) Lemma B.1, g.contentunsatisfiable w.r.t. . Moreover, Cu applied calculate int(g) int(g) =int(g 0 ). Lemma B.4, int(g) interpolant g.content. int(g) =int(g 0 ) |int(g 0 )| 2i+2 1 |int(g)| 2i+3 1. Hence inductive hypothesisholds case.3. exactly two successors g1 , g2 g (C1 C2 ) g.content,gj .content (C1 C2 ) -relief g.content j {1, 2}, g1 .content 6= g2 .content,gj .status = unsat j {1, 2}. inductive hypothesis, gj .contentunsatisfiable w.r.t. , int(gj ) interpolant gj , |int(gj )| 2i+2 1, j{1, 2}. (the contrapositive version of) Lemma B.1, g.content unsatisfiablew.r.t. . Moreover, depending , either Clt Crt applied calculate int(g).Lemma B.4, int(g) interpolant g.content. |int(g)| = |int(g1 )|+|int(g2 )| + 1. inductive hypothesis, obtain|int(g)| (2i+2 1) + (2i+2 1) + 1 = 2i+3 1.Thus, inductive hypothesis holds case.4. cases shown similarly.Hence, claim follows.Now, use claim shown prove lemma. Let g Vg.status = unsat. Lemma 3.5, |V| 2n , n = |cll clr|. Thus,worst case, 2n status assignment steps Propagate V2n = V. Sinceg.status = unsat, follows g V2n . claim, g.content unsatisfiable w.r.t.nn, int(g) defined interpolant g.content, |int(g)| 22 +2 1 = 422 1.nint(g) O(22 ). Hence lemma follows.Proof Lemma 3.8. Algorithm 1 consists two stages: Propagate Assign.Assign, make 2n assignments Lemma 3.5 number nodestableau bounded number. Moreover, assignment step takes constant time.whole Assign stage takes time O(2n ).409fiTen Cate, Franconi, & SeylanPropagate, loop inside do-while loop. loop iterates 2nnodes assigns, possible, status unsat node checking worst case ndirect successors node. algorithm assigns status unsat node g,nalso assigns concept int(g). Lemma 3.7, |int(g)| O(22 ). Thus spendstime calculating int(g). Suppose loop finished iteration nodestableau. execution, none nodes got status unsat, do-whileloop terminates done = true. worst case, status assignedone node iteration do-while loop. Hence do-while loop iterates 2nntimes discussed iteration takes time O(22 ) interpolationcalculation. Since dominates runtime Algorithm 1, lemma follows.Lemma C.2 (Soundness). closed hC0 v D0 , i-tableau, |= C0 v D0 .Proof. Let closed hC0 v D0 , i-tableau. Since closed, g0 .content = unsat.g0 .content = {(C0 )l , (D0 )r } {E l | > v E Tl } {E r | > v E Tr } Lemma 3.7,impliesGl|= C0 uE v D0E>vETl>vETr|= C0 v D0 .Definition C.3. Let = hV, Ei output second phase. say nodeg V saturatedg.status = sat g sink node,g.status = sat R applied g.g, g 0 V , g 0 called saturation g g 0 saturated, pathg = g0 , g1 , . . . , gk = g 0 k 0 0 < k, gi .status = satedge hgi , gi+1 created application rule {Ru , Rt , R1 }.Lemma C.4. Let = hV, Ei complete tableau hC0 v D0 , i.1. g V saturated, g.content() hC0 u0 , i-type.2. g V g.status = sat, saturation g 0 g g 0 .contentg.content.Proof. 1, suppose g saturated. need show g.content() satisfiesDefinition A.2. start g.content() cl(C0 u0 , ). Let C g.content().follows C g.content, {l, r}. Since g.content cll clr,C cll clr. implies C cl(C0 , Tl ) cl(D0 , Tr ). C cl(C0 u0 , ),wanted show.show properties Definition A.2 satisfied. definition, g.status =sat. g contain clash otherwise g.status = unsat would contradictassumption. Hence, (P ), (P ), (P./ ) satisfied. definition, g sink nodeR applied g. cases, none {Ru , Rt , R1 } applicableg: former, follows fact rule applicable g;410fiBeth Definability Expressive Description Logicslatter, follows rule precedence. Hence, (Pu ), (Pt ), (P1 ) satisfied. Finally,{C | > v C } g.content() easy consequence definitiontableau algorithm. means (Pv ) satisfied. Hence, conclude 1 holds.2, suppose g V g.status = sat. saturation g 0 gg 0 .content g.content follows easily Lemma C.1.Lemma C.5 (Completeness). open hC0 v D0 , i-tableau, 6|= C0 v D0 .Proof. Suppose = hV, Ei open hC0 v D0 , i-tableau. Since open,g0 .status = sat. Lemma C.4, saturation g? g0 g? .contentg0 .content. Since g? saturated, follows Lemma C.4 g? .content() hC0 u0 , i-type. Let 0 = {C0 u0 }g? .content(). claim 0 also hC0 u0 , itype. Suppose contradiction not. Since g? .content() type, follows(Pu ) violated C0 u0 0 , i.e., {C0 ,0 } 6 0 . definition 0 ,means {C0 ,0 } 6 g? .content(). know {C0 ,0 } g0 .content()g? .content g0 .content, implies {C0 ,0 } g? .content(), i.e., contradiction.Hence conclude 0 hC0 u0 , i-type. DefineQ = {0 } {g.content() | g V saturated}.show Q hC0 u0 , i-quasimodel 6|= C0 v D0 followsTheorem A.4. easy see Q set hC0 u0 , i-types: alreadyshown 0 type; g.content() g V saturated, factfollows immediately Lemma C.4. remains show conditions (a), (b), (c)Definition A.3 hold.Condition (a) holds since 0 Q C0 u0 0 .Suppose R.C Q. distinguish = 0 =g.content() saturated g V. first argue latter. Since g saturatedR.C g.content(), R applied g; since g saturated, g.status = sat.Lemma C.1, successor g 0 g {C} {D | R.D} g 0 .content() g 0 .status = sat. Lemma C.4, saturation g 00g 0 g 00 .content g 0 .content. Since g 00 saturated, g 00 .content() QR.Cg 00 .content() hC0 u0 , i-type. === g 00 .content(). case= 0 follows analogously using fact successor g 0 g?{C} {D | R.D } g 0 .content() g 0 .status = sat. Hence condition (b)Definition A.3 satisfied.condition (c) holds shown similarly previous case; leavereader verify this. Hence, conclude Q hC0 u0 , i-quasimodel.Proposition 3.9 follows immediately Lemma C.2 Lemma C.5.ReferencesAfrati, F. N. (2011). Determinacy query rewriting conjunctive queries views.Theoretical Computer Science, 412 (11), 10051021.Andreka, H., Nemeti, I., & van Benthem, J. (1998). Modal languages bounded fragmentspredicate logic. Journal Philosophical Logic, 27, 217274.411fiTen Cate, Franconi, & SeylanAvigad, J. (2003). Eliminating definitions skolem functions first-order logic. ACMTransactions Computational Logic, 4, 402415.Baader, F., & Nutt, W. (2003). Basic description logics. Description Logic Handbook,pp. 4395. Cambridge University Press.Barany, V., Benedikt, M., & ten Cate, B. (2013). Rewriting guarded negation queries.MFCS13, pp. 98110.Beth, E. W. (1953). Padoas methods theory definitions. Indagationes Mathematicae, 15, 330339.Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal logic. Cambridge UniversityPress.Boolos, G. S., Burgess, J. P., & Jeffrey, R. C. (2007). Computability Logic. CambridgeUniversity Press.Calvanese, D., & Giacomo, G. D. (2003). Expressive description logics. DescriptionLogic Handbook, pp. 178218. Cambridge University Press.Calvanese, D., Giacomo, G. D., Lenzerini, M., & Nardi, D. (2001). Reasoning expressivedescription logics. Handbook Automated Reasoning, pp. 15811634.Calvanese, D., Giacomo, G. D., & Rosati, R. (1998). note encoding inverse rolesfunctional restrictions ALC knowledge bases. Description Logics, Vol. 11.CEUR-WS.org.ten Cate, B., Conradie, W., Marx, M., & Venema, Y. (2006). Definitorially complete description logics. KR, pp. 7989.ten Cate, B., Franconi, E., & Seylan, I. (2011). Beth definability expressive descriptionlogics. IJCAI, pp. 10991106.Conradie, W. (2002). Definability changing perspectives: beth property threeextensions modal logic. Masters thesis, University Amsterdam.Craig, W. (1957). Three uses Herbrand-Gentzen theorem relating model theoryproof theory. Journal Symbolic Logic, 22 (3), 269285.De Giacomo, G. (1996). Eliminating converse Converse PDL. Journal Logic,Language Information, 5 (2), 193208.Donini, F. M. (2003). Complexity reasoning. Description Logic Handbook, pp.96136. Cambridge University Press.Duc, C. L., & Lamolle, M. (2010). Decidability description logics transitive closureroles concept role inclusion axioms. Description Logics, Vol. 573, pp.372383. CEUR-WS.org.Fitting, M. (1996). First-order logic automated theorem proving (2nd ed.). SpringerVerlag.Friedman, H. (1976). complexity explicit definitions. Advances Mathematics,20 (1), 1829.Gabbay, D. M., & Maksimova, L. (2005). Interpolation Definability Modal Logics(Oxford Logic Guides). Clarendon Press.412fiBeth Definability Expressive Description LogicsGhilardi, S., Lutz, C., & Wolter, F. (2006). damage ontology? case conservative extensions description logics. KR, pp. 187197.Gore, R. (1999). Tableau methods modal temporal logics. Handbook TableauMethods, pp. 297396. Kluwer.Gore, R., & Nguyen, L. A. (2007). Exptime tableaux global caching descriptionlogics transitive roles, inverse roles role hierarchies. TABLEAUX, pp.133148.Hoogland, E. (2001). Definability Interpolation: Model-theoretic investigations. Ph.D.thesis, University Amsterdam.Hoogland, E., & Marx, M. (2002). Interpolation definability guarded fragments.Studia Logica, 70 (3), 373409.Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDFOWL: making web ontology language. Journal Web Semantics, 1 (1),726.Horrocks, I., & Sattler, U. (2007). tableau decision procedure SHOIQ. JournalAutomated Reasoning, 39 (3), 249276.Horrocks, I., Sattler, U., & Tobies, S. (2000). Practical reasoning expressive description logics. Logic Journal IGPL, 8 (3), 239264.Konev, B., Lutz, C., Ponomaryov, D., & Wolter, F. (2010). Decomposing description logicontologies. KR, pp. 236246.Konev, B., Lutz, C., Walther, D., & Wolter, F. (2009a). Formal properties modularisation.Modular Ontologies, pp. 2566. Springer.Konev, B., Walther, D., & Wolter, F. (2009b). Forgetting uniform interpolationlarge-scale description logic terminologies. IJCAI, pp. 830835.Kracht, M. (2007). Modal consequence relations. Handbook Modal Logic, pp. 491545.Elsevier.Lang, J., & Marquis, P. (2008). propositional definability. Artificial Intelligence, 172,9911017.Libkin, L. (2004). Elements Finite Model Theory. Springer.Lutz, C. (2006). Complexity succinctness public announcement logic. AAMAS,pp. 137143.Lutz, C., Areces, C., Horrocks, I., & Sattler, U. (2005). Keys, nominals, concretedomains. Journal Artificial Intelligence Research, 23, 667726.Lutz, C., Piro, R., & Wolter, F. (2010). Enriching EL-concepts greatest fixpoints.ECAI, pp. 4146.Lutz, C., Sattler, U., & Tendera, L. (2005). complexity finite model reasoningdescription logics. Information Computation, 199 (1-2), 132171.Lutz, C., Seylan, I., & Wolter, F. (2012a). automata-theoretic approach uniforminterpolation approximation description logic EL. KR.413fiTen Cate, Franconi, & SeylanLutz, C., Seylan, I., & Wolter, F. (2012b). Mixing open closed world assumptionontology-based data access: Non-uniform data complexity. Description Logics, Vol.846, pp. 268278. CEUR-WS.org.Lutz, C., & Wolter, F. (2011). Foundations uniform interpolation forgettingexpressive description logics. IJCAI, pp. 989995.Marx, M. (2007). Queries determined views: pack views. PODS, pp. 2330.Marx, M., & Venema, Y. (2007). Local variations loose theme: Modal logicdecidability. Finite Model Theory Applications, pp. 371426. Springer.Nash, A., Segoufin, L., & Vianu, V. (2010). Views queries: Determinacy rewriting.ACM Transactions Database Systems, 35 (3).Nikitina, N., & Rudolph, S. (2012). Expexpexplosion: Uniform interpolation general ELterminologies. ECAI, pp. 618623.Pasaila, D. (2011). Conjunctive queries determinacy rewriting. ICDT, pp. 220231.Rautenberg, W. (1983). Modal tableau calculi interpolation. Journal PhilosophicalLogic, 12 (4), 403423.Sattler, U., Calvanese, D., & Molitor, R. (2003). Relationships formalisms.Description Logic Handbook, pp. 137177. Cambridge University Press.Schwendimann, S. (1998). new one-pass tableau calculus PLTL. TABLEAUX, pp.277292.Seylan, I. (2012). DBoxes Beth Definability Description Logics. Ph.D. thesis, FreeUniversity Bozen-Bolzano.Seylan, I., Franconi, E., & de Bruijn, J. (2009). Effective query rewriting ontologiesDBoxes. IJCAI, pp. 923929.Seylan, I., Franconi, E., & de Bruijn, J. (2010). Optimal rewritings definitorially completedescription logics. Description Logics, Vol. 573, pp. 125136. CEUR-WS.org.Tobies, S. (2001). Complexity Results Practical Algorithms Logics KnowledgeRepresentation. Ph.D. thesis, RWTH-Aachen.414fiJournal Artificial Intelligence Research 48 (2013) 67-113Submitted 3/13; published 10/13Survey Multi-Objective Sequential Decision-MakingDiederik M. Roijersd.m.roijers@uva.nlInformatics InstituteUniversity AmsterdamAmsterdam, NetherlandsPeter Vamplewp.vamplew@ballarat.edu.auSchool Science,Information Technology EngineeringUniversity BallaratBallarat, Victoria, AustraliaShimon Whitesons.a.whiteson@uva.nlInformatics InstituteUniversity AmsterdamAmsterdam, NetherlandsRichard Dazeleyr.dazeley@ballarat.edu.auSchool Science,Information Technology EngineeringUniversity BallaratBallarat, Victoria, AustraliaAbstractSequential decision-making problems multiple objectives arise naturally practice pose unique challenges research decision-theoretic planning learning,largely focused single-objective settings. article surveys algorithms designed sequential decision-making problems multiple objectives. Thoughgrowing body literature subject, little makes explicit circumstances special methods needed solve multi-objective problems. Therefore,identify three distinct scenarios converting problem single-objectiveone impossible, infeasible, undesirable. Furthermore, propose taxonomyclassifies multi-objective methods according applicable scenario, naturescalarization function (which projects multi-objective values scalar ones), typepolicies considered. show factors determine nature optimal solution, single policy, convex hull, Pareto front. Using taxonomy,survey literature multi-objective methods planning learning. Finally,discuss key applications methods outline opportunities future work.1. IntroductionSequential decision problems, commonly modeled Markov decision processes (MDPs)(Bellman, 1957a), occur range real-world tasks robot control (Kober &Peters, 2012), game playing (Szita, 2012), clinical management patients (Peek, 1999),military planning (Aberdeen, Thiebaux, & Zhang, 2004), control elevators (Crites& Barto, 1996), power systems (Ernst, Glavic, & Wehenkel, 2004), water supplies(Bhattacharya, Lobbrecht, & Solomantine, 2003). Therefore, development algorithmsc2013AI Access Foundation. rights reserved.fiRoijers, Vamplew, Whiteson & Dazeleyautomatically solving problems, either planning given model MDP (e.g.,via dynamic programming methods, Bellman, 1957b) learning interactionunknown MDP (e.g., via temporal-difference methods, Sutton & Barto, 1998),important challenge artificial intelligence.research topics, desirability undesirability actionseffects codified single, scalar reward function. Typically, objectiveautonomous agent interacting MDP maximize expected (possiblydiscounted) sum rewards time. many tasks, scalar reward functionnatural, e.g., financial trading agent could rewarded based monetary gainloss holdings recent time period. However, also many tasksnaturally described terms multiple, possibly conflicting objectives, e.g.,traffic control system minimize latency maximize throughput; autonomousvehicle minimize travel time fuel costs. Multi-objective problemswidely examined many areas decision-making (Zeleny & Cochrane, 1982; Vira &Haimes, 1983; Stewart, 1992; Diehl & Haimes, 2004; Roijers, Whiteson, & Oliehoek, 2013)growing, albeit fragmented, literature addressing multi-objective decisionmaking sequential settings.article, present survey algorithms devisedsettings. begin Section 2 formalizing problem multi-objective MDP(MOMDP). Then, Section 3, motivate multi-objective perspective decisionmaking. Little existing literature multi-objective algorithms makes explicitmulti-objective approach beneficial and, crucially, cases cannot trivially reducedsingle-objective problem solved standard algorithms. address this,describe three motivating scenarios multi-objective algorithms.Then, Section 4, present novel taxonomy organizes multi-objective problemsterms underlying assumptions nature resulting solutions. keydifficulty existing literature authors considered many different typesproblems, often without making explicit assumptions involved, differauthors, scope applicability resulting methods. taxonomyaims fill void.Sections 5 6 survey MOMDP planning learning methods, respectively, organizing according taxonomy identifying key differencesapproaches examined planning learning areas. Section 7 surveys applicationsmethods, covering specific applications general classes problemsMOMDP methods applied. Section 8 discusses future directions fieldbased gaps literature identified Sections 5 6, Section 9 concludes.2. Backgroundfinite single-objective Markov decision process (MDP) tuple hS, A, T, R, , where:finite set states,finite set actions,: [0, 1] transition function specifying, state, action,next state, probability next state occurring,68fiA Survey Multi-Objective Sequential Decision-MakingR : reward function, specifying, state, action, nextstate, expected immediate reward,: [0, 1] probability distribution initial states,[0, 1) discount factor specifying relative importance immediate rewards.goal agent acts environment maximize expected returnRt , function rewards received timestep onwards. Typically,return additive (Boutilier, Dean, & Hanks, 1999), i.e., sum rewards.infinite horizon MDP, return typically infinite sum, term discountedaccording :Xk rt+k+1 ,Rt =k=0rt reward obtained time t. parameter thus quantifies relativeimportance short-term long-term rewards.contrast, finite horizon MDP, return typically undiscounted finite sum,i.e., certain number timesteps, process terminates rewardobtained. single- multi-objective methods developed finite horizon,discounted infinite horizon, average reward settings (Puterman, 1994), sakebrevity formalize infinite horizon discounted reward MDPs article.1agents policy determines actions selects timestep. broadestsense, policy condition everything known agent. state-indepedentvalue function V specifies expected return following initial state:V = E[R0 | ].(1)policy stationary, i.e., conditions current state,formalized : [0, 1]: specifies, state action, probabilitytaking action state. specify state value function policy :V (s) = E[Rt | , st = s],st = s. Bellman equation restates expectation recursivelystationary policies:XX(s, a, )[R(s, a, ) + V (s )].V (s) =(s, a)Note Bellman equation, forms heart standard solution algorithmsdynamic programming (Bellman, 1957b) temporal-difference methods (Sutton& Barto, 1998), explicitly relies assumption additive returns. importantbecause, explain Section 4.2.2, multi-objective settings interfereadditivity property, making planning learning methods rely Bellmanequation inapplicable.1. formalizations settings, see example overview Van Otterlo Wiering (2012).69fiRoijers, Vamplew, Whiteson & DazeleyState value functions induce partial ordering policies, i.e., betterequal value greater states:s, V (s) V (s).special case stationary policy deterministic stationary policy, oneaction chosen probability 1 every state. deterministic stationary policyseen mapping states actions: : A. single-objective MDPs,always least one optimal policy , i.e., : , stationary deterministic.Theorem 1. additive infinite-horizon single-objective MDP, exists deterministic stationary optimal policy (see e.g., Howard, 1960; Boutilier et al., 1999).one optimal policy exists, share value function, knownoptimal value function V (s) = max V (s). Bellman optimality equation definesoptimal value function recursively:X(s, a, )[R(s, a, ) + V (s )].V (s) = maxNote that, maximizes actions, equation makes use factoptimal deterministic stationary policy. optimal policy maximizesvalue every state, policy optimal regardless initial state distribution .However, state-independent value (Equation 1) may well different differentinitial state distributions. Using , state value function translated backstate-independent value function (Equation 1):XV =(s)V (s).sSmulti-objective MDP (MOMDP)2 MDP reward function R :n describes vector n rewards, one objective, instead scalar.Similarly, value function V MOMDP specifies expected cumulative discountedreward vector:Xk rk+1 | ],(2)V = E[k=0rt vector rewards received time t. difference singleobjective value (Equation 1) multi-objective value (Equation 2) policyreturn, underlying sum rewards, vector rather scalar.stationary policies, also define multi-objective value state:V (s) = E[Xk rt+k+1 | , st = s].(3)k=0single-objective MDP, state value functions impose partial orderingpolicies compared different states, e.g., possible V (s) > V (s) V (s ) <2. Multi-objective MDPs confused mixed-observability MDPs (Ong, Png, Hsu, & Lee,2010), also sometimes abbreviated MOMDP.70fiA Survey Multi-Objective Sequential Decision-MakingV (s ). given state, ordering complete, i.e., V (s) must greater than,equal to, less V (s). true state-independent value functions.contrast, MOMDP, presence multiple objectives means valuefunction V (s) state vector expected cumulative rewards instead scalar.value functions supply partial ordering, even given state. example,possible that, state s, Vi (s) > Vi (s) Vj (s) < Vj (s). Similarly,state-independent value functions, may Vi > Vi Vj < Vj . Consequently,unlike MDP, longer determine values optimal without additionalinformation prioritize objectives. information providedform scalarization function, discuss following sections.Though focus article, also MOMDP variants constraintsspecified objectives (see e.g., Feinberg & Shwartz, 1995; Altman, 1999).goal agent maximize regular objectives meeting constraintsobjectives. Constrained objectives fundamentally different regularobjectives explicitly prioritized regular objectives, i.e., policyfails meet constraint inferior policy meets constraints, regardlesswell policies maximize regular objectives.3. Motivating ScenariosMOMDP setting received considerable attention, immediately obvious useful addition standard MDP specialized algorithmsneeded. fact, researchers argue modeling problems explicitly multiobjective necessary, scalar reward function adequate sequentialdecision-making tasks. direct formulation perspective Suttons rewardhypothesis, states mean goals purposes wellthought maximization expected value cumulative sum received scalarsignal (reward).3view imply multi-objective problems exist. Indeed,would difficult claim, since easy think problems naturally possessmultiple objectives. Instead, implication reward hypothesis resultingMOMDPs always converted single-objective MDPs additive returns.conversion process would involve two steps. first step specify scalarizationfunction.Definition 1. scalarization function f , function projects multi-objectivevalue V scalar value.Vw (s) = f (V (s), w),w weight vector parameterizing f .example, f may compute linear combination values, case elementw quantifies relative importance corresponding objective (this setting discussed Section 4.2.1). second step define single-objective MDP3. http://rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html71fiRoijers, Vamplew, Whiteson & DazeleyFigure 1: three motivating scenarios MOMDPs: (a) unknown weights scenario,(b) decision support scenario, (c) known weights scenario.additive returns that, s, expected return equals scalarized valueVw (s).Though rarely, ever, makes issue explicit, research MOMDPs restspremise exist tasks one conversion steps impossible,infeasible, undesirable. section, discuss three scenarios occur(see Figure 1).first scenario, call unknown weights scenario (Figure 1a), occursw unknown moment planning learning must occur. Consider examplepublic transport system aims minimize latency (i.e., time commutersneed reach destinations) pollution costs. addition, assume resultingMOMDP scalarized converting objective monetary cost: economistscompute cost lost productivity due commuting pollution incurs taxmust paid pollution credits purchased given price. Assume also creditstraded open market therefore price constantly fluctuates. transportsystem complex, may infeasible compute new plan every day given latestprices. scenario, preferable use multi-objective planning methodcomputes set policies that, price, one policies optimal(see planning learning phase Figure 1a). computationallyexpensive computing single optimal policy given price, needs donedone advance, computational resources available.Then, time select policy, current weights, i.e., price pollution72fiA Survey Multi-Objective Sequential Decision-Makingcredits, used determine best policy set (the selection phase). Finally,selected policy employed task (the execution phase).unknown weights scenario, scalarization impossible planning learningtrivial policy actually needs used w known time.contrast, second scenario, call decision support scenario (Figure1b), scalarization infeasible throughout entire decision-making processdifficulty specifying w, even f . example, economists may able accuratelycompute cost lost productivity due commuting. user may also fuzzypreferences defy meaningful quantification. example, transport system couldmade efficient building new train line obstructs beautiful view,human designer may able quantify loss beauty. difficulty specifyingexact scalarization especially apparent designer single personcommittee legislative body whose members different preferences agendas.system, MOMDP method used calculate optimal solution setrespect known constraints f w. Figure 1b shows, decision supportscenario proceeds similarly unknown weights scenario except that, selectionphase, user users select policy set according arbitrary preferences,rather explicit scalarization according given weights.cases, one still argue scalarization planning learningpossible principle. example, loss beauty quantified measuringresulting drop housing prices neighborhoods previously enjoyed unobstructedview. However, difficulty scalarization may impracticalbut, importantly, forces users express preferences way mayinconvenient unnatural. selecting w requires weighing hypotheticaltrade-offs, much harder choosing set actual alternatives.well understood phenomenon field decision analysis (Clemen, 1997),standard workflow involves presenting alternatives soliciting preferences.subfields decision analysis multiple criteria decision-making multiattribute utility theory focus multiple objectives (Dyer, Fishburn, Steuer, Wallenius, &Zionts, 1992). reasons, algorithms MOMDPs provide critical decisionsupport. Rather forcing users specify w advance, algorithmsprune policies would optimal w. Then, offer users rangealternatives select according preferences whose relative importanceeasily quantified.third scenario, call known weights scenario (Figure 1c), assumew known time planning learning thus scalarization possiblefeasible. However, may undesirable difficulty second stepconversion. particular, f nonlinear, resulting single-objective MDPmay additive returns (see Section 4.2.2). result, optimal policy maynon-stationary (see Section 4.3.2) stochastic (see Section 4.3.3), cannot occursingle-objective, additive, infinite-horizon MDPs (see Theorem 1). Consequently, MDPdifficult solve, standard methods applicable. Converting MDPone additive returns may help either cause blowup state space,73fiRoijers, Vamplew, Whiteson & Dazeleyalso leaves problem intractable.4 Therefore, even though scalarization possiblew known, may still preferable use methods specially designed MOMDPsrather convert problem single-objective MDP. contrast unknownweights decision support scenarios, known weights scenario, MOMDPmethod produces one policy, executed, i.e., separate selectionphase, shown Figure 1c.Note Figure 1 assumes off-line scenario: planning learning occurs once,execution. However, multi-objective methods also employed on-line settingsplanning learning interleaved execution. on-line versionunknown weights scenario, weights better characterized dynamic, ratherunknown. on-line scenario, agent must already seen weights timesteps> 1 since prerequisite execution timesteps 1, . . . , 1. However,weights change time, agent may yet know weights usedtimestep planning learning phase timestep.4. Problem Taxonomyfar, described MOMDP formalism proposed three motivating scenariosit. section, discuss constitutes optimal solution. Unfortunately,simple answer question, depends several critical factors. Therefore,propose problem taxonomy, shown Table 1, categorizes MOMDPs accordingfactors describes nature optimal solution category.taxonomy based call utility-based approach, contrast manymulti-objective papers follow axiomatic approach optimality MOMDPs.utility-based approach rests following premise: execution phasesscenarios Section 3, one policy selected collapsing value vector policyscalar utility, using scalarization function. application scalarizationfunction may implicit hidden, e.g., may embedded thought-processuser, nonetheless occurs. scalarization function part notion utility,i.e., agent maximize. Therefore, find set optimal solutionpossible weight setting scalarization function, solved MOMDP.utility-based approach derives optimal solution set assumptionsmade scalarization function, policies user allows, whether needone multiple policies.contrast, axiomatic approach begins axiom optimal solution setPareto front (see Section 4.2.2).5 approach limiting because, demonstratesection, settings solution concepts suitable.Thus, take utility-based approach makes possible derive solutionconcept, rather assuming it. Pareto front fact correct solution4. Since non-additive returns depend agents entire history, immediate reward functionconverted MDP may also depend history thus state representation convertedMDP must augmented include it.5. example axiomatic approach multi-objective reinforcement learning, see surveyLiu, Xu, Hu (2013).74fiA Survey Multi-Objective Sequential Decision-Makingsingle policy(known weights)deterministiclinearscalarizationmultiple policies(unknown weights decision support)stochasticone deterministic stationarypolicy (1)monotonically oneincreasingdeterministicscalarizationnon-stationarypolicy (3)deterministicstochasticconvex coverage setdeterministic stationary policies(2)one mixturepolicy twodeterministicstationarypolicies (4)Paretocoverage setdeterministicnon-stationarypolicies (5)convexcoverage setdeterministicstationarypolicies (6)Table 1: MOMDP problem taxonomy showing critical factors problemnature resulting optimal solution. columns describe whetherproblem necessitates single policy multiple ones, whether policiesmust deterministic (by specification) allowed stochastic. rowsdescribe whether scalarization function linear combination rewardsor, whether cannot assumed scalarization function merelymonotonically increasing function them. contents cell describeoptimal solution given setting looks like.concept, utility-based approach provides justification it. not, allowsappropriate solution concept derived instead.taxonomy categorizes problem classes based assumptions scalarization function, policies user allows, whether one multiple policiesrequired. show leads different solution concepts, underscoring importancecarefully considering choice solution concept based available information.discuss three factors constitute taxonomy following order.Section 4.1, discuss first factor: whether one multiple policies sought, choicefollows directly motivating scenario applicable. known weightsscenario (Figure 1c) implies single-policy approach unknown weights decisionsupport scenarios (Figure 1a 1b) imply multiple-policy approach. Section 4.2,discuss second factor: whether scalarization function linear combinationrewards merely monotonically increasing function them. Section 4.3, discussthird factor: whether stochastic deterministic policies permitted.goal taxonomy cover research MOMDPs remaining simpleintuitive. However, due diversity research MOMDPs, researchfit neatly taxonomy. note discrepancies discussing researchSections 5 6.75fiRoijers, Vamplew, Whiteson & Dazeley4.1 Single versus Multiple PoliciesFollowing approach Vamplew et al. (2011), first distinguish problemsone policy sought ones multiple policies sought. case holdsdepends three motivating scenarios discussed Section 3 applies.unknown weights decision support scenarios, solution MOMDPconsists multiple policies. Though two scenarios conceptually quite different,algorithmic perspective identical. reason characterized strict separation decision-making process two phases: planninglearning phase execution phase (though on-line settings, agent may goback forth two).planning learning phase, w unavailable. Consequently, planning learning algorithm must return single policy set policies (and correspondingmulti-objective values). set contain policies suboptimalscalarizations, i.e. interested undominated policies.Definition 2. MOMDP scalarization function f , set undominatedpolicies, U (m ), subset possible policies exists wscalarized value maximal:U (m ) = { : w( ) Vw Vw }.(4)U (m ) sufficient solve m, i.e., w, contains policy optimalscalarized value. However, may contain redundant policies that, optimalweights, optimal policy set w. policies removedstill ensuring set contains optimal policy w. fact, order solvem, need subset undominated policies that, possible w,least one policy set optimal. sometimes called coverage set (CS) (Becker,Zilberstein, Lesser, & Goldman, 2003).Definition 3. MOMDP scalarization function f , set CS(m )coverage set subset U (m ) if, every w, contains policy maximalscalarized value, i.e., if:CS( ) U ( ) (w)() CS( ) ( ) Vw Vw .(5)Note U (m ) automatically coverage set. However, U (m ) unique, CS(m )need be. multiple policies value, U (m ) containsthem, coverage set need contain one. addition, given CS(m ),may exist policy/ CS(m ) V different V CS(m )scalarized value CS(m ) w optimal.contrast single-objective MDPs, MOMDPs whether policy CS(m )depend initial state distribution . thus important accurately specifyformulating MOMDP.Ideally, MOMDP algorithm find smallest CS(m ). However,might harder finding one smaller U (m ). Section 4.2, specializecoverage set two classes scalarization functions.76fiA Survey Multi-Objective Sequential Decision-Makingexecution phase, single policy chosen set returned planninglearning phase executed. unknown weights scenario, assume w revealedplanning learning complete execution begins. Selecting policyrequires maximizing scalarized value policy returned set:= argmax Vw .CS(m )decision support scenario, set manually inspected user(s), selectpolicy execution informally, making implicit trade-off objectives.known weights scenario, w known planning learning begins. Therefore,returning multiple policies unnecessary. However, mentioned Section 3 discussedSection 4.2.2, scalarization yield single-objective MDP difficultsolve.4.2 Linear versus Monotonically Increasing Scalarization Functionssecond critical factor affecting constitutes optimal solution MOMDPnature scalarization function. section, discuss two types scalarizationfunction: linear combinations rewards merelymonotonically increasing functions them.4.2.1 Linear Scalarization Functionscommon assumption scalarization function (e.g., Natarajan & Tadepalli, 2005;Barrett & Narayanan, 2008), f linear, i.e., computes weighted sumvalues objective.Definition 4. linear scalarization function computes inner product weight vectorw value vector VVw = w V .(6)element w specifies much one unit value corresponding objectivecontributes scalarized value. elements weight vector w positive realnumbers constrained sum 1.Linear scalarization functions simple intuitive way scalarize. One commonsituation applicable rewards easily translated monetaryvalue. example, consider mining task different policies yield different expectedquantities various minerals. prices per kilo minerals fluctuate daily,task formulated MOMDP, objective corresponding differentmineral. element V reflects expected number kilos mineralmined scalarized value Vw corresponds monetary valueeverything mined. Vw computed w, corresponding(normalized) current price per kilo mineral, becomes known.single-policy setting, w known, presence multiple objectives posesdifficulties given linear f . Instead, f simply applied reward vector77fiRoijers, Vamplew, Whiteson & DazeleyMOMDP. inner product computed f distributes addition, resultsingle-objective MDP additive returns. infinite horizon setting leads to:Vw = w V = w E[Xk rt+k+1 ] = E[Xk (w rt+k+1 )].(7)k=0k=0Since single-objective MDP additive returns, solved standard methods, yielding single policy, reflected box labeled (1) Table 1. Due Theorem1, determinstic stationary policy suffices. However, multi-objective approach stillpreferable case, e.g., V may easier estimate Vw large continuousMOMDPs function approximation required (see Section 6.1).multiple policy setting, however, know w planning learningtherefore want find coverage set. f linear, U (m ), automaticallycoverage set, consists convex hull. Substituting Equation 6 definitionundominated set (Definition 2), obtain definition convex hull:Definition 5. MOMDP m, convex hull (CH) subsetexists w linearly scalarized value maximal:CH(m ) = { : w( ) w V w V }.(8)Figure 2a illustrates concept convex hull stationary deterministic policies.point plot represents multi-objective value given policy two-objectiveMOMDP. axes represent reward dimensions. convex hull shown setfilled circles, connected lines form convex surface.6 Given linear f ,scalarized value policy linear function weights. illustratedFigure 2b, x-axis represents weight dimension 0 (w[1] = 1 w[0]),y-axis scalarized value policies. select policy, need knowvalues convex hull policies, form upper surface scalarized value,illustrated black solid lines, correspond three convex hull policiesFigure 2a. upper surface forms piecewise linear convex function. functionsalso well-known literature partially-observable Markov decision processes(POMDPs), whose relationship MOMDPs discuss Section 5.2.Like U (m ), CH(m ) contain superfluous policies. However, also defineconvex coverage set (CCS) specification coverage set f linear.reflected box (2) Table 1 (we explain policies set deterministicstationary Section 4.3.1).Definition 6. MOMDP m, set CCS(m ) convex coverage setsubset CH(m ) if, every w, contains policy whose linearly scalarized valuemaximal, i.e., if:CCS(m ) CH(m ) (w)() CCS(m ) ( ) w V w V . (9)6. Note term convex hull slightly different meaning multi-objective literaturestandard geometric definition. geometry, convex hull finite set points Euclideanspace minimal subset points expressed convexcombination points convex hull. multi-objective setting, interestedparticular subset geometric convex hull; points convex combinations strictlybigger (in dimensions) point S, i.e., points optimal weight.78fiA Survey Multi-Objective Sequential Decision-Making(a)(b)Figure 2: Example convex hull Pareto front. point (a) representsmulti-objective value given policy line (b) represents linearlyscalarized value policy across values w. convex hull shown blackfilled circles (a), black lines (b). Pareto front consists filledpoints (circles squares) (a), dashed solid black lines(b). unfilled points (a) (grey lines (b)) dominated.deterministic stationary policies, difference CH(m ) CCS(m ) mayoften small. Therefore, terms often used interchangeably. However, casenon-stationary stochastic policies, difference quite significant, CHcontain infinitely many policies, possible construct finite CCS, showSection 4.3.1.4.2.2 Monotonically Increasing Scalarization Functionslinear scalarization functions intuitive simple, always adequateexpressing users preferences. example, suppose mining task mentionedabove, two minerals mined three policies available: 1sends mining equipment location first mineral mined, 2location second mineral mined, 3 locationminerals mined. Suppose owner equipment prefers 3 , e.g.,least partially appeases clients different interests. However, may case that,location corresponding 3 fewer minerals, convex hull contains1 2 . Thus, owners preference 3 implies she, implicitly explicitly,employs nonlinear scalarization function.Here, consider case f nonlinear, corresponds commonnotion relationship reward utility. class possibly nonlinear scalarizations strictly monotonically increasing scalarization functions. functionsadhere constraint policy changed way value increases79fiRoijers, Vamplew, Whiteson & Dazeleyone objectives, without decreasing objectives, scalarizedvalue also increases.Definition 7. scalarization function f strictly monotonically increasing if:(i, Vi Vi i, Vi > Vi ) (w, Vw > Vw ).(10)Linear scalarization functions (with non-zero positive weights) included classfunctions. condition left-hand side Equation 10 commonly knownPareto dominance (Pareto, 1896).Definition 8. policy Pareto-dominates another policy value leasthigh objectives strictly higher least one objective:V P V i, Vi Vi i, Vi > Vi .(11)Demanding f strictly monotonically increasing quite minimal constraint,requires that, things equal, getting reward certain objectivealways better. fact, difficult think f violates constraint withoutemploying highly unnatural notion reward.7Three observations order strictly monotonically increasing scalarizationfunctions related concept Pareto dominance. First, unlike linear case,necessarily know exact shape f . Instead, know belongsparticular class functions. solution concept follows thus applies strictlymonotonically increasing f . cases stronger assumptions f made,specific solution concepts possible. However, except linearity, awareproperties f exploited solving MOMDPs.Second, notions optimality introduced Section 4.2.1 longer appropriate.reason that, even though vector-valued returns still additive (Equation 2),scalarized returns may f may longer linear. example, considerwell-known Tchebycheff scalarization function (Perny & Weng, 2010)8 :X(V , p, w) = max wi |pi Vi |wi |pi Vi |,(12)i1...ni1...np optimistic reference point, w weights, arbitrarily small positiveconstant greater 0. Note sum righthand side makes functionstrictly monotonically increasing. Now, p = (3, 3),P = 0.01, r1 = (0, 3), r2 = (3, 0),kw = (0.5, 0.5) = 1, f (V , w) = 0 E[k=0 f (rt+k+1 , w)] = (1.515) +(1.515) = 3.03. loss additivity scalarized returns applying nonlinearf important consequences methods applied, show Section 4.3.2.Third, still identify prune policies optimal wstrictly monotonically increasing f , even though may nonlinear. Consider three7. addition, f strictly monotonically increasing assumptions made,policies pruned coverage set. Thus, computing value every policy coverageset, required selection phase, likely intractable.8. definition differs slightly Perny Weng (2010): multiplied 1 expressmaximization instead minimization, sake consistency rest article.80fiA Survey Multi-Objective Sequential Decision-Makinglabeled policies Figure 2a (note Figure 2b apply, scalarizationfunction longer linear). B higher value one objective, lowervalue other. therefore cannot tell whether B ought preferred withoutknowing w. However, C lower value objectives, thus Paretodominates C: P C. f strictly monotonically increasing, scalarized valuegreater C w thus discard C.now, defer full discussion constitutes optimal solutionMOMDP strictly monotonically increasing scalarization function (i.e., boxes (3)-(6)Table 1) depends, whether single multiple policy settingapplies, also whether deterministic also stochastic policies considered,addressed Section 4.3.However, already observe that, given strictly monotonically increasing f ,use Pareto front set viable policies. Pareto front consists policiesPareto dominated.Definition 9. MOMDP m, Pareto front set policiesPareto dominated policy :P F (m ) = { : ( ), V P V }.(13)Note P F (m ) set undominated policies U (m ) specific strictlymonotonically increasing f . already seen special case linear f ,U (m ) = CH(m ), subset P F (m ). (For example, Figure 2, Paretofront consists convex hull plus B.) However, strictly monotonically increasingf , know policy P F (m ) dominated respect f , i.e.,6 P F (m ) 6 U (m ). because, strictly monotonically increasing f/ P F (m ), cannot exist w optimal, since definition existsV P V and, since f strictly monotonically increasing, impliesVw > Vw .However, know f strictly monotonically increasing, cannot settlesubset P F (m ) either, exist strictly monotonically increasing fU (m ) = P F (m ). Perny Weng (2010) show U (m ) = P F (m )Tchebycheff function (Equation 12), strictly monotonically increasing. Therefore,cannot discard policies P F (m ) retain undominated set U (m )strictly monotonically increasing f .Pareto coverage set (PCS) minimal size constructed retaining onepolicy policies identical vector values P F (m ). formally definePCS follows:Definition 10. MOMDP m, set P CS(m ) Pareto coverage set subsetP F (m ) if, every policy , contains policy either dominatesequal value , i.e., if:P CS(m ) P F (m ) ( )() P CS(m ) (V P V V = V ) . (14)Again, deterministic stationary policies difference P CS(m ) P F (m )may minor. Note P F (m ) automatically P CS(m ). papersliterature therefore take P F (m ) solution.81fiRoijers, Vamplew, Whiteson & Dazeleyalso slightly relax constraint f , without change policiesP CS(m ). Specifically, define monotonically increasing scalarizationfunction function following property holds: (i, Vi Vi ) (w, VwVw ). relaxation influences set undominated policies: policiesP F (m ) always dominated strictly monotonically increasing f , needmonotonically increasing f . Consider example f (V , w) = 0,monotonically increasing strictly monotonically increasing. functiondominated policies, every policy scalarized value. However,scalarized value policy 6 P F (m ) cannot greater scalarizedfunction policy P CS(m ), use P CS(m ) (non-strict) monotonicallyincreasing f . Therefore, article, focus monotonically increasing f ,broader class functions.P F (m ), even P CS(m ), may prohibitively large containmany policies whose values differ negligible amounts, Chatterjee et al. (2006) Brazdilet al. (2011) introduce slack parameter , use define -approximate Paretofront, P F (m ). P F (m ) contains values policies every possible policypolicy P F (m ) Vi (s) + Vi (s). weakeningrequirements domination, approach yields smaller set calculatedefficiently.Another option finding smaller set P F (m ) making additional assumptionsscalarization function. example, Perny, Weng, Goldsmith, Hanna (2013)introduce notion fairness objectives, leading Lorentz optimality.additional assumption sum values objectives stays same,making difference two objectives smaller yields higher scalarized value.course strong assumption apply broadly Pareto optimality. However,apply, help reduce size optimal solution set.4.3 Deterministic versus Stochastic Policiesthird critical factor affecting constitutes optimal solution MOMDPwhether deterministic polices considered stochastic ones also allowed.applications reason exclude stochastic policies priori,cases stochastic policies clearly undesirable even unethical. example,policy determines clinical treatment patient, e.g., work Lizotte, Bowling,Murphy (2010) Shortreed, Laber, Lizotte, Stroup, Pineau, Murphy (2011),flipping coin determine course action may inappropriate. denoteset deterministic policiesset stationary policies . sets subsetspolicies: . Finally set policies deterministicstationary intersection sets, denotedDS = .single-objective MDPs, factor critical because, due Theorem 1,restrict search deterministic stationary policies, i.e. optimal attainable valueV . However,attainable deterministic stationary policy: maxm V = maxDSsituation complex MOMDPs. section, discuss focusstochastic deterministic policies affects setting considered taxonomy.82fiA Survey Multi-Objective Sequential Decision-Making4.3.1 Deterministic Stochastic Policies Linear ScalarizationFunctionsf linear, result similar Theorem 1 holds MOMDPs due followingcorollary:Corollary 1. MOMDP m, CCS(mDS ) also CCS( ).Proof. f linear, translate MOMDP single-objective MDP,possible w. done treating inner product reward vector wnew rewards, leaving rest problem is. Since inner product distributesaddition, scalarized returns remain additive (Equation 7). Thus, every wexists translation single-objective MDP, optimal deterministicstationary policy must exist, due Theorem 1. Hence, w exists optimaldeterministic stationary policy. Therefore, exists CCS(mDS ) optimalw. Consequently, cannot exist \ DS w V > w Vthus CCS(mDS ) also CCS( ).CCS(mDS ) thus sufficient solving MOMDPs linear f , even stochastic non-stationary policies allowed. reflected box (2) Table 1. alsoapplies box (1) since optimal policy case member CCS(mDS ),i.e., one best given known w.Unfortunately, result analogous Corollary 1 holds MOMDPs monotonically increasing f . rest section, discuss consequencesnature optimal MOMDP solution boxes (3)-(6) Table 1.4.3.2 Multiple Deterministic Policies Monotonically IncreasingScalarization Functionsmultiple-policy setting deterministic policies allowed f nonlinear,non-stationary policies may better best stationary ones.Theorem 2. infinite-horizon MOMDPs, deterministic non-stationary policies Paretodominate deterministic stationary policies undominated deterministic stationary policies (White, 1982).see why, consider following MOMDP, denoted m1, adapted exampleWhite (1982). one state three actions a1 , a2 , a3 , yield rewards(3, 0), (0, 3), (1, 1), respectively. allow deterministic stationary policies,three possible policies 1 , 2 , 3 m1DS , corresponding always takingone actions, Pareto optimal. policies followingstate-independent values (Equation 2): V1 = (3/(1 ), 0), V2 = (0, 3/(1 )),V3 = (1/(1 ), 1/(1 )). However, consider set possibly non-stationarym1m1policies m1(including non-stationary ones), construct policy ns \ DSalternates a1 a2 , starting a1 , whose value Vns = (3/(12 ), 3/(1 2 )). Consequently, ns P 3 > 0.5 thus cannot restrict83fiRoijers, Vamplew, Whiteson & Dazeleyattention stationary policies.9 Consequently, multiple deterministic policies casemonotonically increasing f , need find P CS(m), includes non-stationarypolicies, shown box (5) Table 1.addition consider broader class policies, another consequencedefining policy indirectly via value function longer possible. standardsingle-objective methods, optimal policy found local action selectionrespect value function: i.e., every state, policy selects actionmaximizes expected value. However, local selection yield non-stationary policy, value function must also non-stationary, i.e., must condition currenttimestep. standard finite-horizon setting, different value function computed timestep, possible infinite-horizon setting.discuss address difficulty Sections 5 6.4.3.3 Multiple Stochastic Policies Monotonically IncreasingScalarization Functionsmultiple policy setting stochastic non-stationary policies, i.e., full set ,allowed, cannot consider deterministic stationary policies. However,employ stochastic stationary policies instead deterministic non-stationary ones.particular, employ mixture policy (Vamplew, Dazeley, Barker, & Kelarev, 2009)takes set N deterministicpolicies, selects i-th policy set,Pprobability pi , Np=1.leads values linear combinationi=0values constituent policies. previous example, replace nspolicy chooses 1 probability p1 2 otherwise, resulting followingvalues:3p1 3(1 p1 )12V = p1 V + (1 p1 )V =,.11Fortunately, necessary explicitly represent entire P CS(m ) explicitly.Instead, sufficient compute CCS(mDS ). necessary stochastic policiescreate P CS(m ) easily constructed making mixture policiespolicies CCS(mDS ).Corollary 2. infinite horizon discounted MOMDP, infinite set mixture policiesPM constructed policies CCS(mDS ), set PM ,P CS( ) (Vamplew et al., 2009).Proof. construct policy value vector convex surface, e.g.,10 Thereblack lines Figure 2a, mixing policies CCS(mDS ), e.g., black dots.fore, always construct mixture policy dominates policy valuesurface, e.g., B. show contradiction cannot policy9. White (1982) shows infinite-horizon discounted setting, arguments hold alsofinite-horizon average-reward settings.10. Note always mix policies adjacent; line pair policiesmix convex surface. E.g. mixing policy represented leftmost black dotFigure 2a policy represented rightmost black dot lead optimal policies,line connecting two points convex surface.84fiA Survey Multi-Objective Sequential Decision-Makingconvex surface. was, would optimal w f linear. Consequently, Corollary 1, would deterministic stationary policy leastequal value. since convex surface spans values CCS(mDS ), leadscontradiction. Thus, policy Pareto-dominate mixture policy convexsurface.Thanks Corollary 2, sufficient compute CCS(mDS ) solve MOMDP,reflected box (6) Table 1. surprising consequence fact, knowledgemade explicit literature, Pareto optimality, though commonsolution concept associated multi-objective problems, actually necessary onespecific problem setting:Observation 1. multiple policy setting f monotonically increasingdeterministic policies considered (box (5) Table 1), requires computing Pareto coverage set. either f linear stochastic policies allowed, CCS(mDS ) suffices.Wakuta (1999) proves sufficiency CCS(mDS ) monotonically increasingscalarizations multiple stochastic policies (box (6) Table 1) infinite horizonMOMDPs, different way. Instead mixture policies Corollary 2, uses stationary randomizations deterministic stationary policies. Wakuta Togawa (1998)provide similar proof average reward case.Note that, common consider non-stationary stochastic policies fnonlinear, policies typically condition current state, current statetime, agents reward history. However, setting, policies conditionreward history dominate not. example, suppose twoobjectives take positive values f simply selects smaller two, i.e.,f (V , w) = mini Vi . Suppose also that, given state, two actions available,yields rewards (4, 4) (0, 5) respectively. Finally, suppose agent arrivestate one two reward histories, whose discounted sums either (5, 0) (3, 3).policy conditions discounted reward histories outperform policiesnot, i.e., optimal policy selects action yielding (4, 4) reward history sums(3, 3) action yielding (0, 5) reward history sums (5, 0). So,single objective MDPs Markov property additive returns sufficient restrictattention policies ignore history, multi-objective case, scalarized returnslonger additive therefore optimal policy depend history. Examplesmethods exploit fact steering approach (Mannor & Shimkin, 2001)reward-augmented-state thresholded lexicographic ordering method Geibel (2006),discussed Section 6.1.4.3.4 Single Deterministic Stochastic Policies MonotonicallyIncreasing Scalarization Functionsremains address single-policy setting monotonically increasing f .nature optimal solution case follows directly reasoning givenmultiple-policy setting.deterministic policies considered, single policy sought maynon-stationary, reflected box (3) Table 1, reasons elucidated Whites85fiRoijers, Vamplew, Whiteson & Dazeleyexample. Again, hard define non-stationary policy local action selection,due risk circular dependencies Q-values.stochastic policies allowed, optimal policy may stochastic,represented mixture policy two deterministic stationary policies,reflected box (4) Table 1, reasons given Corollary 2. cases,policies potentially benefit conditioning reward history.5. Planning MOMDPssection, survey key approaches planning MOMDPs, i.e., computingoptimal policy coverage set undominated policies given complete modelMOMDP. Following taxonomy presented Section 4, first consider single-policymethods turn multiple-policy methods linear monotonically increasingscalarization functions.5.1 Single-Policy Planningknown weights scenario, w known planning begins, singlepolicy, optimal w, must discovered. Since MOMDP transformedsingle-objective MDP f linear (see Section 4.2.1), focus single-policyplanning nonlinear f .discussed Section 4.2.2, nonlinear f cause scalarized return nonadditive. Consequently, single-objective dynamic programming linear programmingmethods, exploit assumption additive returns employing Bellman equation, applicable. However, different linear programming formulations singlepolicy planning MOMDPs possible. key feature methodsproduce stochastic policies, which, discussed Section 4, optimalscalarization function nonlinear. aware single-policy planningmethods work arbitrary nonlinear f , methods developed two specialcases. particular, Perny Weng (2010) propose linear programming methodMOMDPs scalarized using Tchebycheff function mentioned Section 4.2.2.Tchebycheff function always w Pareto-optimal policy optimal,approach find (single) policy Pareto front. addition, Ogryczak, Perny,Weng (2011) propose analogous method ordered weighted regret metric.metric calculates regret objective respect estimated ideal referencepoint, sorts descending order, calculates weighted sum weightsalso descending order.researchers proposed single-policy methods MOMDPs constraints.Feinberg Shwartz (1995) consider MOMDPs one regular objective objectives inequality constraints. show feasible policy exists setting,deterministic stationary finite number timesteps N that,prior timestep N , random actions must performed. call (M, N )policy, show Pareto-optimal values achieved (M, N ) policies, proposelinear programming algorithm finds -approximate policies setting.general MOMDPs constraints also considered. particular, Altman (1999)proposes several linear programming approaches settings.86fiA Survey Multi-Objective Sequential Decision-MakingFurnkranz, Hullermeier, Cheng, Park (2012) propose framework MDPsqualitative reward signals, related MOMDPs fit neatlytaxonomy. Qualitative reward signals indicate preference policies actionswithout directly ascribing numeric value them. Since preferences induce partialordering policies, policy iteration method authors propose settingmay applicable MOMDPs nonlinear f , Pareto dominance also induces partialorderings. However, authors note multi-objective tasks generally numericfeedback exploited. Thus, suggest quantitative MOMDPsviewed subset preference-based MDPs, methods designed specificallyMOMDPs may efficient general preference-based methods.5.2 Multiple-Policy Planning Linear Scalarization Functionsmultiple-policy setting linear f , seek CCS(mDS ). Note however,distinction convex hull convex coverage set usually madeliterature.One might argue explicitly multi-objective methods necessary setting, one could repeatedly run single-objective methods obtain CCS(mDS ).However, since infinitely many possible w, obvious possible values w covered. might possible devise way run single-objectivemethods finite number times still guarantee CCS(mDS ) produced. However,would nontrivial result corresponding algorithm would essencemulti-objective method happens use single-objective methods subroutines.One approach attempted find minimally sized CCS(m), i.e., convexcoverage set deterministic necessarily stationary policies, originally proposedWhite Kim (1980), translate MOMDP partially observable Markovdecision process (POMDP) (Sondik, 1971). intuitive way think translationimagine fact one true objective agent unawareobjectives MOMDP is. modeled POMDP defining statetuple hs1 , s2 s1 state MOMDP s2 {1 . . . n} indicatestrue objective. observations thus identify s1 exactly give informations2 . Note translation MOMDPs POMDPs one-way only. everyPOMDP translated equivalent MOMDP.Typically, agent interacting POMDP maintains belief, i.e., probabilitydistribution states. POMDP derived MOMDP, belief decomposed belief s1 belief s2 . former degenerative s1known. latter vector size n i-th element specifies probabilityi-th objective true one. vector analogous w linear f .fact, reason Figure 2b resembles piecewise linear value functions oftendepicted POMDPs; difference whether x-axis interpreted wbelief.White Kim (1980) show that, finite horizon case, solution every beliefexactly solution w, solutions resulting POMDPexactly original MOMDP. infinite horizon case difficultinfinite horizon POMDPs undecidable (Madani, Hanks, & Condon, 1999). However,87fiRoijers, Vamplew, Whiteson & Dazeleysufficiently large horizon, solution finite horizon POMDP usedapproximate solution infinite horizon MOMDP.solve resulting POMDP, White Kim (1980) propose combination Sondiksone-pass algorithm (Smallwood & Sondik, 1973) policy iteration POMDPs (Sondik,1978). However, POMDP planning method used long (1)require initial belief POMDP state (which would correspond initializingMOMDP state also w) (2) computes optimal policy everypossible belief. recently developed exact methods, e.g., Cassandra, Littman,Zhang (1997) Kaelbling, Littman, Cassandra (1998), meet conditionscould thus employed. Approximate point-based POMDP methods (Spaan & Vlassis,2005; Pineau, Gordon, & Thrun, 2006) meet conditions (1) (2) couldadapted compute approximate convex hull, choosing prior distributionweights could sample. Online POMDP planning methods (Ross, Pineau,Paquet, & Chaib-draa, 2008) applicable plan given belief.Converting POMDP thus allows use POMDP methodssolving MOMDPs linear f . However, approach inefficientexploit characteristics distinguish MOMDPs general POMDPs, i.e.,part state, s1 , known observations give informations2 . example, methods compute policies trees, e.g., (Kaelbling et al., 1998)exploit fact deterministic policies stationary functions stateneeded MOMDPs linear f . Furthermore, mentioned before, general infinitehorizon POMDPs undecidable, MOMDPs fact possible computeCCS(mDS ) exactly.reasons, researchers also developed specialized planning methodssetting. Viswanathan, Aggarwal, Nair (1977) propose linear programming approachepisodic MOMDPs. Wakuta Togawa (1998) propose policy iteration approachthree phases. first phase uses policy iteration narrow setpossibly optimal policies. second phase uses linear programs check optimality.Since necessarily give definitive answer, third phase uses another linearprogram handle undetermined solutions left second phase.Barrett Narayanan (2008) propose convex hull value iteration (CHVI), computes CH(m), every state. CHVI extends conventional value iteration storingDSset vectors, Q (s, a) state-action pair, representing convex hull policies involving action. sets vectors correspond Q-values single-objectivesetting; contain optimal Q-values possible w. backup operationperformed, Q-hulls next state propagated back s. possible nextstate , possible actions considered (i.e. union convex hulls Q (s , )taken), weighted probability occurring taking action state s.procedure similar witness algorithm (Kaelbling et al., 1998) POMDPs.Lizotte et al. (2010) propose value-iteration approach finite-horizon settingcomputes different value function timestep. addition, uses piecewiselinear spline representation value functions. authors prove offers asymptotictime space complexity improvements representation used CHVI alsoenables application algorithm MOMDPs continuous states. However,88fiA Survey Multi-Objective Sequential Decision-Makingalgorithm applicable problems two objectives. limitation addressedauthors subsequent work (Lizotte, Bowling, & Murphy, 2012) extendsalgorithm arbitrary number objectives provides detailed implementationcase three objectives.5.3 Multiple-Policy Planning Monotonically Increasing ScalarizationFunctionssection, consider planning MOMDPs monotonically increasing f . discussed Section 4.3, stochastic policies allowed, mixture policies deterministicstationary policies sufficient. Therefore, focus case deterministicpolicies allowed consider methods compute P CS(m), includenon-stationary policies. distinction P F (m)PCS(m) usuallymade literature.linear case, scalarizing every w obtaining P CS(m) singleobjective methods problematic. infinitely many w consider but, unlikelinear case, additional difficulty scalarized returns may longeradditive, make single-objective methods inapplicable.Daellenbach Kluyver (1980) present algorithm multi-objective routing tasks(essentially deterministic MOMDPs). approach uses dynamic programming conjunction augmented state space find non-Pareto-dominated policies iteratively,number iterations equals maximum number steps route. algorithm finds undominated sub-policies parallel. authors use two alternative explicitscalarization functions, call weighted minsum weighted minmax operators. First, values solutions translated : objective, new valuebecomes fractional difference optimal values objective acrosssolutions. Then, value objective multiplied positive weight. Finally,either minimum sum (minsum) minimum maximal value (minmax )new weighted fractional differences chosen scalarization. Notescalarization functions monotonically increasing objectives, optimal valueobjective individually depend scalarization function.White (1982) extends work proposing dynamic programming methodapproximately solves infinite horizon MOMDPs. repeatedly backing according multi-objective version Bellman equation. Since policiesnon-stationary, size Pareto front grows rapidly number backups applied.However, White notes number need large acceptable approximations reached. Nonetheless, approach feasible small MOMDPs.Wiering De Jong (2007) address difficulty dynamic programming methodcalled CON-MODP deterministic MOMDPs computes optimal stationary policies.CON-MODP works enforcing consistency DP updates: policy consistentsuggests action timesteps given state. inconsistent policyinconsistent one state-action pair, CON-MODP makes consistent forcingcurrent action taken time current state visited. inconsistency runsdeeper, policy discarded.89fiRoijers, Vamplew, Whiteson & Dazeleycontrast, Gong (1992) proposes linear programming approach finds Paretofront stationary policies. However, authors note, approach also suitablesmall MOMDPs number constraints decision variableslinear program increase rapidly state space grows.mentioned Section 4.2.2, one way cope intractably large Pareto frontscompute instead -approximate Pareto front, much smaller. Chatterjeeet al. (2006) propose linear programming method computes -approximate frontinfinite horizon MOMDP, Chatterjee (2007) propose analogous algorithmaverage reward setting. cases, stationary stochastic policies shownsufficient.Another way improve scalability setting give planning wholestate space instead plan on-line agents current state, using Monte Carlo treesearch approach (Kocsis & Szepesvari, 2006). approaches, provensuccessful, e.g., game Go (Gelly & Silver, 2011), increasingly popularsingle-objective MDPs. Wang Sebag (2013) propose Monte Carlo tree search methoddeterministic MOMDPs. Single-objective tree search methods typically optimisticallyexplore tree selecting actions maximize upper confidence boundvalue estimates. multi-objective variant same, respect scalarmulti-objective value function whose definition based hypervolume indicator induced proposed action together set Pareto optimal policies computedfar. hypervolume indicator (Zitzler, Thiele, Laumanns, Fonseca, & da Fonseca, 2003)measures hypervolume Pareto-dominated set points. Since Paretofront maximizes hypervolume indicator, optimistic action selection strategy focusestree search branches likely compliment existing archive.6. Learning MOMDPsmethods reviewed Section 5 assume model transition rewarddynamics MOMDP known. cases model directly available,multi-objective reinforcement learning (MORL) used instead.One way carry MORL take model-based approach, i.e., use agentsinteraction environment learn model transition reward functionMOMDP apply multi-objective planning methods describedSection 5. Though approach seems well suited MORL, papersconsidered it, (e.g., Lizotte et al., 2010, 2012). discuss opportunities future workmodel-based MORL Section 8.1. Instead, work MORL focusedmodel-free methods, model transition reward function never explicitlylearned.section, survey key MORL approaches. majoritymethods single-policy setting, multiple-policy methods also developed. first glance, may seem multiple-policy methods unlikely effectivelearning setting, since finding policies would increase sample costs,computational costs, former typically much scarcer resource. However, modelbased methods obviate issue: enough samples gathered learnuseful model, finding policies optimal weights requires computation. Model90fiA Survey Multi-Objective Sequential Decision-Makingfree methods also practical multiple-policy setting employ off-policylearning (Sutton & Barto, 1998; Precup, Sutton, & Dasgupta, 2001), makes possible learn one policy using data gathered another. way, policiesmultiple weight settings optimized using data.6.1 Single-Policy Learning Methodsknown weights scenario, MORL algorithm aims learn single policyoptimal given weights. discussed Section 5.1, linear scalarizationequivalent learning optimal policy single-objective MDP standardtemporal-difference (TD) methods (Sutton, 1988) Q-learning (Watkins, 1989)easily applied.However, even though specialized methods needed address setting,nonetheless commonly studied setting MORL. Linear scalarizationuniform weights, i.e., elements w equal, forms basis work Karlsson(1997), Ferreira, Bianchi, Ribeiro (2012), Aissani, Beldjilali, Trentesaux (2008)Shabani (2009) amongst others, non-uniform weights used authorsCastelletti et al. (2002), Guo et al. (2009) Perez et al. (2009). majoritywork uses TD methods, work on-line, although Castelletti et al. (2010) extend off-lineFitted Q-Iteration (Ernst, Geurts, & Wehenkel, 2005) multiple objectives.cases, change made underlying RL algorithm that, ratherscalarizing reward function learning scalar value function resultingsingle-objective MDP, vector-valued value function learned original MOMDPscalarized selecting actions. argument approachvalues individual objectives may easier learn scalarized value, particularlyfunction approximation employed (Tesauro et al., 2007). example, functionapproximator ignore state variables irrelevant objective, reducingsize state space thereby speeding learning.discussed Section 4.2.2, linear scalarization may appropriate scenarios. Vamplew, Yearwood, Dazeley, Berry (2008) demonstrate empiricallypractical consequences MORL. Therefore, MORL methods worknonlinear scalarization functions substantial importance. Unfortunately, illustratedSection 4.2.2, coping setting especially challenging, since algorithmsTD methods based Bellman equation inherently incompatiblenonlinear scalarization functions due non-additive nature scalarized returns.Four main classes single-policy MORL methods using non-linear scalarizationarisen, differ deal issue. first class simply applies TDmethods without modification. approaches either resign heuristics guaranteed converge impose restrictions environment ensureconvergence. second class modifies either TD algorithm state representationissue non-additive returns avoided. third class uses TD methodslearn multiple policies using linear scalarization different values w,forms stochastic non-stationary meta-policy optimal respectnonlinear scalarization. fourth class uses policy-search methods,91fiRoijers, Vamplew, Whiteson & Dazeleymake use Bellman equation hence directly applied combinationnonlinear scalarizations.first class includes methods model problem multi-agent system,one agent per objective. agent learns recommends actions basisreturn objective. global switch selects winning agent, whose recommended action followed current state. Examples include simple winner-takes-allapproach agent whose recommended action highest Q-value selected,sophisticated approaches W-learning (Humphrys, 1996) selectedaction one incur loss followed. One key weaknessapproaches pointed Russell Zimdars (2003): allowselection actions that, optimal single objective, offer good compromisemultiple objectives. Another key weakness that, since actions selecteddifferent timesteps may recommended different agents, resulting behavior corresponds policy combines elements learned agent. combinationmay optimal even single objective, i.e., may Pareto dominated performarbitrarily poorly.TD also used directly nonlinear scalarization functions allowconsideration actions, optimal regards individualobjectives. Scalarization functions based fuzzy logic proposed problemsdiscrete actions Zhao, Chen, Hu (2010) problems continuousactions Lin Chung (1999). widely cited approach nonlinear scalarizationGabor, Kalmar, Szepesvari (1998), designed tasks constraintsmust satisfied objectives. lexicographic ordering objectives definedthreshold value specified objectives except last. State-action valuesobjective exceed corresponding threshold clamped threshold valueprior applying lexicographic ordering. Thus, thresholded lexicographic ordering(TLO) approach scalarization maximizes performance last objective subjectmeeting constraints objectives specified thresholds.methods combining TD nonlinear scalarization may converge suitablepolicy certain conditions, also converge suboptimal policy even failconverge conditions. example, Issabekov Vamplew (2012) demonstrate empirically TLO fail converge suitable policy episodic tasksconstrained objective receives non-zero rewards timestep endepisode. general, methods based combination TD nonlinear scalarizationmust regarded heuristic nature, applicable restricted classes problems.second class avoids problems caused non-additive scalarized returns modifying either TD algorithm state representation. knowledge, two approachesproposed Geibel (2006) address limitations TLO membersclass. require reward accumulated objective current episodestored. first algorithm, local decision-making based scalarized valuesum cumulative reward current state-action values. eliminatesproblem non-additive returns, yields policy non-stationary respectobserved state, meaning algorithm may converge. second approach augments state representation cumulative reward. approach convergescorrect policy learns slowly, due increase size state space.92fiA Survey Multi-Objective Sequential Decision-Makingthird class uses TD methods learn policies based linear scalarizations.policy selection mechanism based nonlinear scalarization used formmeta-policy base policies. Multiple Directions Reinforcement Learning(MDRL) algorithm Mannor Shimkin (2001, 2004) uses approachcontext on-line learning non-episodic tasks. user specifies target region withinlong-term average reward lie. initial active policy chosen arbitrarilyfollowed average reward moves outside target region agentspecified reference state. point, direction current average rewardvector closest point target set calculated, policy whose direction bestmatches target direction selected active policy. way, average rewardsteered towards users specified target region. underlying base policiesutilize linear scalarization, nature policy-selection mechanism meansoverall non-stationary policy formed base policies optimal nonlinearscalarization specified users defined target set. Vamplew et al. (2009) suggestsimilar approach episodic tasks, TD used first learn policies optimallinear scalarization range different w, stochastic mixture policyconstructed optimal regards nonlinear scalarization.fourth class uses policy-search algorithms directly learn policy without learning value function. single-policy MORL, research policy-search approachesfocused policy-gradient methods (Sutton, McAllester, Singh, & Mansour, 2000; Kohl &Stone, 2004; Kober & Peters, 2011). methods, policy iteratively adjusteddirection gradient value respect parameters (usually probabilitydistributions actions per state) policy. Shelton (2001) proposes algorithmfirst learns optimal policy individual objective. used base policies form initial mixture policy stochastically selects base policy startepisode. hill-climbing method based weighted convex combinationnormalized objective gradients iteratively improves mixture policy. approachdirectly fit taxonomy returns never scalarized. Instead,weights used find step direction relative current policy parameters.practical perspective, behavior akin single-policy RL using nonlinearscalarization function, converges single Pareto-optimal policy need lieconvex hull. Uchibe Doya (2009) also propose policy-gradient methodMORL called Constrained Policy Gradient RL (CPGRL) uses gradient projection technique find policies whose average reward satisfies constraints oneobjectives. Like Sheltons approach, CPGRL learns stochastic policies worksnonlinear scalarization functions.6.2 Multiple-Policy Learning Linear Scalarization Functionsunknown weights decision support scenarios, f linear, MORL algorithmsaim learn CCS possible policies. simple inefficient approach usedCastelletti et al. (2002) run TD multiple times different values w.simplest case, runs conducted sequentially gradually build approximateCCS. Natarajan Tadepalli (2005) showed approach made efficientreusing policies learned earlier runs similar w. show93fiRoijers, Vamplew, Whiteson & Dazeleyimproves greatly sample costs learning policy w similar alreadyvisited previous runs. However, many samples typically still required goodapproximate CCS obtained.sophisticated approach approximating convex coverage set learn multiple policies parallel. Several algorithms proposed achieve withinTD learning framework. approach Hiraoka, Yoshida, Mishima (2009) similarCHVI planning algorithm Barrett Narayanan (2008) (see Section 5.2)learns parallel optimal value function w, using convex hull representation.approach prone infinite growth number vertices convex hull polygons,threshold margin applied hull representations iteration, eliminating points contribute little hulls hypervolume. Hiraoka et al. (2009) presentalgorithm adapt margins learning improve efficiency, note manyparameters must tuned effective performance. Mukai, Kuroe, Iima (2012) presentsimilar extension CHVI learning context. address problematic growthnumber values stored pruning vectors Q-value update: vectorselected random set vectors stored given state-action pairothers lying within threshold distance deleted.approaches Hiraoka et al. (2009) Mukai et al. (2012) designedon-line learning. contrast, Multi-Objective Fitted Q-Iteration (MOFQI) (Castelletti,Pianosi, & Restelli, 2011, 2012) off-line approach learning multiple policies. MOFQImulti-objective extension Fitted Q-Iteration (FQI) algorithm (Ernst et al., 2005)uses combination historical data single-step transition dynamicsenvironment, initial function approximator, Q-learning update rule constructdataset maps state-action pairs expected return. dataset usedtrain improved function approximator process repeats valuesfunction approximator converge. MOFQI provides computationally efficient extensionFQI multiple objectives including w input function approximatorconstructing expanded training data set containing training instances randomlygenerated ws. Since learned function generalizes across weight space additionstate-action space, used construct policy w.discussed Section 5.3, Lizotte et al. (2010) Lizotte et al. (2012) describe valueiteration algorithm find convex hull policies finite horizon tasks. notemethod applied learning context estimating model state transitionprobabilities immediate rewards basis experience environment.approach demonstrated task analyzing randomized drug trial data producingestimates historical data gathered clinical trials.6.3 Multiple-Policy Learning Monotonically Increasing ScalarizationFunctionsf nonlinear, MORL algorithms unknown weights decision support scenarios aim learn PCS. linear scalarization case, simplest approachrun single-objective algorithms multiple times varying w. Shelton (2001) demonstrates approach policy-gradient algorithm, Vamplew et al. (2011)TLO method Gabor et al. (1998). approach however, requires94fiA Survey Multi-Objective Sequential Decision-Makingf explicitly known learning algorithm, may undesirable decisionsupport scenario.knowledge, currently methods learning multiple policiesnonlinear f using value-function approach. might seem possible adapt convexhull methods CHVI using Pareto-dominance operators place convex-hullcalculations, straightforward. scalarized values policiescertain state non-additive, cannot restrict stationary policies wantfind deterministic Pareto-optimal policies (as mentioned Section 4.3.2). However,Bellman equation CHVI work, additivity, resulting sufficiencydeterministic policies, required. discuss options developing multiple-policy learningmethods nonlinear f Sections 8.1 8.2.Given extensive research multi-objective evolutionary algorithms (MOEAs)(Coello Coello, Lamont, & Van Veldhuizen, 2002; Tan, Khor, Lee, & Sathikannan, 2003;Drugan & Thierens, 2012) evolutionary methods RL (Whiteson, 2012), surprisingly little work evolutionary approaches MORL. methods populationbased, well suited approximating Pareto fronts, would thus seem naturalfit f nonlinear. knowledge, Handa (2009b) first apply MOEAsMORL, extending Estimation Distribution (EDA) evolutionary algorithms handle multiple objectives. EDA-RL (Handa, 2009a) uses Conditional Random Fields (CRF)represent probabilistic policies. initial set policies used generate setepisodes. best episodes set selected CRFs likely produce trajectories generated. policies formed CRFs constitutenext generation. Handa (2009b) extends EDA-RL MOMDPs using Paretodominance based fitness metric select best episodes.Soh Demiris (2011) also apply MOEAs MORL. Policies representedStochastic Finite State Controllers (SFSC) optimized using two different MOEAs:NSGA2, standard evolutionary algorithm, MCMA, EDA. use SFSCs givesrise large search space, necessitating addition local search operator. localsearch generates random w, uses scalarize rewards, performs gradient-basedsearch SFSC. Empirical comparisons multi-objective variants three POMDPbenchmarks demonstrate evolutionary methods generally superior purelylocal-search approach, local search combined evolution usually outperformspurely evolutionary methods. one papers directly consider partiallyobservable MOMDPs.7. MOMDP ApplicationsMulti-objective methods planning learning employed wide rangeapplications, simulation real-world settings. section, surveyapplications. sake brevity, list comprehensive instead aimsprovide illustrative range examples. First, discuss use multi-objectivemethods specific applications. Second, discuss research identified broaderclasses problems multi-objective methods play useful role.95fiRoijers, Vamplew, Whiteson & Dazeley7.1 Specific Applicationsimportant factor driving interest multi-objective decision-making increasingsocial political emphasis environmental concerns. more, decisions mustmade trade economic, social, environmental objectives. reflectedfact substantial proportion applications multi-objective methodsenvironmental component.Perhaps extensively researched application water reservoir control problem considered Castelletti et al. (2002), Castelletti, Pianosi, Soncini-Sessa (2008),Castelletti et al. (2011, 2012) Castelletti, Pianosi, Restelli (2013). generaltask find control policy releasing water dam balancing multiple usesreservoir, including hydroelectric production flood mitigation. Managementhydroelectric power production also examined Shabani (2009). Another environmental application forest management balance economic benefitstimber harvesting environmental aesthetic objectives, demonstratedsimulation Gong (1992) Bone Dragicevic (2009).Several researchers also considered environmentally-motivated applications concerning management energy consumption. SAVES system developed Kwaket al. (2012) controls various aspects commercial building (lighting, heating, airconditioning, computer systems) provide suitable trade-off energy consumption comfort buildings occupants. Simulation results indicateSAVES reduce energy consumption approximately 30% compared manual controlsystem, maintaining slightly improving occupant comfort. Tesauro et al.(2007) Liu et al. (2010) consider problem controlling computing server,objectives minimizing response time user requests power consumption.Guo et al. (2009) apply MORL develop broker agent electricity market.broker sets caps group agents sit hierarchy manage energyconsumption device level, must balance energy cost system stability.Shelton (2001) also examines application MORL developing broker agents.However, case agents task financial rather environmental, actingmarket maker sets buy sell prices resources market. aim balanceobjectives maximizing profit minimizing spread (the difference buysell prices) lead larger volume trades11 .Computing communications applications also widely considered. Perezet al. (2009) apply MORL allocation resources jobs cloud computing scenario, objectives maximizing system responsiveness, utilization resources,fairness amongst different classes user. Comsa et al. (2012) consider maximizesystem throughput ensure user equity context Long Term Evolution mobilecommunications packet scheduling protocol. Tong Brown (2002) use constraint-basedscalarization address tasks call access control routing broadband multimedia network. system aims maximize profit (a function throughput)satisfying constraints quality service metrics (capacity constraints fairness constraints), uses methods similar Gabor et al. (1998). Zheng, Li, Qiu, Gong11. Sheltons model market directly model trading volume, spread used proxyvolume.96fiA Survey Multi-Objective Sequential Decision-Making(2012) also use constrained MORL methods make routing decisions cognitive radionetwork, aiming minimize average transmission delay maintaining acceptablylow packet loss rate.Industrial mechanical control, important application single-objective MDPmethods, also explored MOMDP researchers. Aoki, Kimura, Kobayashi(2004) apply distributed RL control sewage flow system, exploiting systems hierarchical structure find solution minimizes violation stock levels nodeflow system, smoothing variation flow source. Aissani et al. (2008)apply MORL maintenance scheduling within manufacturing plant minimize timetaken complete maintenance tasks machine downtime. Aissani, Beldjilali,Trentesaux (2009) build work applying simulation real petroleum refinery demonstrating ability adapt unscheduled corrective maintenance requireddue equipment failures. control wet clutch heavy-duty transmission systemsexamined Van Vaerenbergh et al. (2012). twin objectives minimizingengagement time, also making transition smooth.Robotics also popular application MOMDPs, though work farsimulation rather real robots. Maravall de Lope (2002) consider controltwo-limbed brachiating robot, objectives moving desired directionavoiding collisions12 . Nojima, Kojima, Kubota (2003) also attempt balanceobjectives progress target collision avoidance. agent makes usepredefined behavioral modules target tracing, collision avoidance, wall following,MORL used dynamically adjust weighting modules. Meisner (2009)identifies social robots promising application MOMDP methods: behaviorinherently multi-objective must carry task without causing anxietydiscomfort humans.MORL also applied control traffic infrastructure. Yang Wen(2010) apply control freeway on-ramps vehicle management systems, aimingmaximize throughput equity freeway system. Multiple agentsshared policies used, action selection occurring via negotiation agents.Similarly, Dusparic Cahill (2009) apply MORL control traffic lights intersectionsurban environment minimize waiting time two different classes vehicles. Yin,Duan, Li, Zhang (2010) Houli, Zhiheng, Yi (2010) also apply MORL trafficlight control. novelty approach lies considering different objectives basedcurrent state road system; minimizing vehicle stops prioritized trafficfree-flowing; minimizing waiting time emphasized system medium load;minimizing queue length intersections targeted system congested.Lizotte et al. (2010, 2012) consider medical application: prescribing appropriatedrug regime patient achieve acceptable trade-off drugs effectiveness severity side effects. system learns multiple policies based12. many robotic applications may ideal avoid collisions completely, environmentsmay possible (e.g., presence moving obstacles whose velocity fasterrobot, difficult predict, may case humans human-controlled vehicles)reducing likelihood impact collisions may reasonable attemptingfind collision-free policy. See example Holenstein Badreddin (1991) Pervez Ryu(2008).97fiRoijers, Vamplew, Whiteson & Dazeleystatic data produced randomized controlled drug trials. selection besttreatment specific patient made doctor based patients individual circumstances. application excellent example problem stochasticapproaches like mixture policies inappropriate. policy maximizes symptom relief also side effects one patient minimizes side effects alsosymptom relief next patient may appear give excellent results averagedacross episodes. However, experience individual patient likely regardedundesirable.7.2 Applications within Broader Planning Learning Tasksaddition specific applications discussed above, several authors identifiedgeneral classes tasks multi-objective sequential decision-making applied.7.2.1 Probabilistic Risk-Aware PlanningCheng, Subrahmanian, Westerberg (2005) argue decision-making uncertainty inherently multi-objective nature. Even single rewardconsidered (such profit), environmental uncertainty means expected valuealone insufficient support good decision-making; decision-maker must also considervariance return. Similarly, Bryce (2008) states probabilistic planninginherently multi-objective due need optimize cost probabilitysuccess plan. criticizes approaches either aggregate factors bound oneoptimize other, arguing favor explicitly multi-objective methods.aptly named Probabilistic Planning Multi-objective! paper Bryce, Cushing,Kambhampati (2007) demonstrates might achieved, describing method basedmulti-objective dynamic programming belief states, multi-objective extensionLooping AO* search algorithm find set Pareto-optimal plans. Recent workKolobov, Mausam, Weld (2012) Teichteil-Konigsbuch (2012b) examine extension stochastic shortest path (SSP) methods problems dead-end states exist.SSP methods assume least one policy exists guaranteed reach goal;presence dead-ends policy exists, authors propose algorithmsaim maximize probability reaching goal minimize costpaths found goal.Bryce (2008) notes probabilistic plan fails environment enters non-goalabsorbing state. Hence, multi-objective probabilistic planning strong parallelsresearch risk-aware RL carried Geibel (2001) Geibel Wysotzki (2005),add second reward signal indicating transition environment errorstate. Defourny, Ernst, Wehenkel (2008) also provide useful insights incorporation risk-awareness MDP methods. review range criteria proposedconstraining risk, note many nonlinear produce non-additivescalarized returns incompatible local decision-making methods basedBellman equation. recommend custom risk-control requirementsmostly enforced heuristically, altering policy optimization procedures checkingcompliance policies initial requirements. Multi-policy MOMDP methods treating risk additional objective would satisfy requirement: iden98fiA Survey Multi-Objective Sequential Decision-Makingtified coverage set, risk-aware metric used select best policy.However, measures risk may expressed directly discounted cumulativerewards. example, agent may wish minimize variance expected return particular reward signal rather discounted cumulative value. Methodsbased multi-objective probabilistic model checking (Courcoubetis & Yannakakis, 1998;Forejt, Kwiatkowska, Norman, Parker, & Qu, 2011; Forejt, Kwiatkowska, & Parker, 2012;Teichteil-Konigsbuch, 2012a), evaluate whether system modelled MDP satisfies multiple, possibly conflicting, properties, may suitable tasks.7.2.2 Multi-Agent Systemsuse MDPs within multi-agent systems widely explored (Bosoniu, Babuska,& Schutter, 2008), several authors proposed approaches strongly relatedMOMDPs. multi-agent system, agent objective, effectiveoverall performance must also consider actions affect agents.agents completely self-interested, problem framed MOMDPtreating effects agents additional objectives. example, Mouaddib (2006)uses multi-objective dynamic programming facilitate cooperation multiple agentswhose underlying goals may conflicting. state-action pair, agent storesthree values: local utility, gain agents receive, penalty inflictsagents. policy agent established converting vector valuesregret ratios applying leximin ordering ratios.Dusparic Cahill (2010) compare application MORL multi-agent tasksmulti-agent methods evolutionary ant-colony algorithms. DusparicCahill (2009) extend W-Learning algorithm Humphrys (1996). agent learnslocal policies (one objectives) remote policies (onelocal policy neighboring agents). timestep, local policiesactive remote policies agent nominate actions, winning action selectedcombining action values across nominating policies. weighting term appliedvalues remote policies determine level cooperation agent offers neighbor.Experimental results urban traffic control simulator show substantial improvementlevel cooperation non-zero. work similar Schneider, Wong,Moore, Riedmiller (1999), addresses use multiple agents distributednetwork power distribution grid, aim maximize global rewardformed combination agents local reward. demonstrateagent focuses local reward, policies learned may maximize globalreward, performance improved agent perform linearly scalarizedlearning using local reward rewards neighboring agents.7.2.3 Multi-Objective Optimization using Reinforcement LearningReinforcement learning primarily applied sequential decision-making tasks dynamicenvironment. However also employed control search mechanisms static optimization tasks scheduling (Carchrae & Beck, 2005). Multi-objective optimizationstatic tasks design well-established field and, majority work99fiRoijers, Vamplew, Whiteson & Dazeleyemployed mathematical evolutionary approaches (Coello Coello et al., 2002),authors explored application reinforcement learning contexts.Mariano Morales (1999, 2000b, 2000a) investigate use RL methods (Ant-QQ-learning) search mechanism optimization multi-objective design tasks.values decision variables considered current state, actions definedalter values variables. Multiple agents explore state space parallel.Agents divided families, family focuses single objective.end episode, final states found agent evaluated. Undominated solutions kept archive agents discovered solutions rewarded,increasing likelihood similar policies followed future. methodshown work small number test problems evolutionary multi-objectiveoptimization literature. Liao, Wu, Jiang (2010) apply RL search static controlsettings power generation system objectives reducing fuel usage ensuring voltage stability. propose RL algorithm formulated specifically taskshigh-dimensional state spaces, compare performance evolutionarymulti-objective algorithm, finding RL method discovers frontsaccurate better distributed, also improving speed search.Note effectively apply RL multi-objective optimization, assumptions usually made nature environment. example, Liao et al. (2010) requireaction increases decreases value precisely one state variable. result,methods likely limited applicability general MORL problemsdescribed earlier.8. Future Worksection, enumerate possibilities future research multi-objectiveplanning learning.8.1 Model-Based Methodsmentioned Section 6, little work model-based approachesMORL. Given breadth planning methods MOMDPs, could employedmodel-based MORL methods subroutines, surprising. knowledge,work area Lizotte et al. (2010, 2012), model MOMDPstransition probabilities reward function derived historical data,spline-based multi-objective value iteration approach applied model. general,learning models seems negligibly harder single-objective setting, sinceestimates reward function learned separately. problem learningtransition function, generally considered hard part model learning, identicalsingle-objective setting. Especially multiple-policy scenarios, model-based approachesMORL could greatly reduce sample costs: model learned, entireCCS PCS computed off-line, without requiring additional samples.100fiA Survey Multi-Objective Sequential Decision-Making8.2 Learning Multiple Policies Monotonically Increasing ScalarizationFunctions using Value Functionsmentioned Section 6.3, aware methods use value functionapproach learn multiple policies PCS. stochastic policies permitted,problem easier learn CCS(mDS ), use either mixture policies(Vamplew et al., 2009) stationary randomizations (Wakuta, 1999) policiesCCS (see Section 4.3.3). However, deterministic policies permitted,problem difficult. One option could use finite-horizon approximationinfinite horizon problem. planning backwards planning horizon, expectedreward timesteps go approximates infinite-horizon value better better. mentioned Section 5.2, similar approaches used POMDPsetting. Another way find good approximations non-stationary policies couldlearn stationary policies (perhaps extending CON-MDP (Wiering & De Jong, 2007)learning setting), prefix timesteps non-stationary policy.8.3 Many-Objective Sequential Decision-Makingmajority research reviewed article, theoretical applied, dealsMOMDPs objectives. mirrors state early evolutionary multiobjective research, focused almost exclusively problems two threeobjectives. However, last decade growing interest evolutionarymethods so-called many-objective problems, least four sometimesfifty objectives (Ishibuchi, Tsukamoto, & Nojima, 2008). researchshown many algorithms perform well objectives scale poorlynumber objectives, necessitating special algorithms many-objective setting.many-objective MDPs received little consideration far, numerous real-world control problems naturally modeled way. example,Fleming et al. (2005) point many-objective control problems commonly ariseengineering, give example jet engine control system eight objectives.many-objective problems considered evolutionary computation, seems likelyleast methods explored far scale poorly number objectives. example, multi-policy MOMDP planning algorithm described Lizotteet al. (2010) limited problems two objectives.key challenge posed many-objective problems number undominatedsolutions typically grows exponentially number objectives. particularlyproblematic multiple-policy MOMDP methods. Fleming et al. (2005) note oneeffective approaches used many-objective evolutionary computationincorporate user preferences restrict search space small region interest.particular, recommend interactive preference articulation user interactively steers system towards desirable solution optimization. Vamplewet al. (2011) raise possibility incorporating approach MORL,aware research actually done so.101fiRoijers, Vamplew, Whiteson & Dazeley(3,0)(0,3)BC(1,1)Figure 3: MOMDP two objectives four states.8.4 Expectation Scalarized ReturnSection 3, defined scalarized value Vw (s) result applying scalarization function f multi-objective value V (s) according w, i.e., Vw (s) = f (V (s), w).Since V (s) expectation, means scalarization function appliedexpectation computed, i.e.,Vw (s) = f (V (s), w) = f (E[Xk rk | , s0 = s], w).k=0formulation, refer scalarization expected return (SER)standard literature. However, option. also possible defineVw (s) expectation scalarized return (ESR):Vw (s) = E[f (Xk rk , w) | , s0 = s]k=0definition used critically affect policies preferred. example,consider following MOMDP, illustrated Figure 3. four states (A, B, C,D) two objectives. agent starts state two possible actions: a1 transitsstate B C, probability 0.5, a2 transits state probability 1.actions lead (0, 0) reward. states B, C one action,leads deterministic reward (3, 0) B, (0, 3) C, (1, 1) D.scalarization function multiplies two objectives together. Thus, SER,Vw (s) = V1 (s)V2 (s),ESR,Vw (s) = E[Xk=0k rk1Xk=0102k rk2 | , s0 = s],fiA Survey Multi-Objective Sequential Decision-Makingrki reward i-th objective timestep k (w needed examplesince f involves constants). 1 (A) = a1 2 (A) = a2 , multi-objectivevalues V1 (A) = (1.5/(1 ), 1.5/(1 )) V2 (A) = (/(1 ), /(1 )).SER, leads scalarized values V 1 (A) = (1.5/(1 ))2 V 2 (A) =(/(1 ))2 consequently 1 preferred. ESR, however, V 1 (A) = 0V 2 (A) = (/(1 ))2 thus 2 preferred.Intuitively, SER formulation appropriate policy used many timesreturn accumulates across episodes, e.g., user using policytime. Then, scalarizing expected reward makes sense 1 preferableexpectation accumulate return objectives. However, policyused times return accumulate across episodes, e.g.,episode conducted different user, ESR formulation appropriate.case, expected return scalarization interest 2 preferable1 always yield zero scalarized return given episode.knowledge, literature MOMDPs employs ESR formulation,even though many real-world scenarios seems appropriate.example, medical application Lizotte et al. (2010) mentioned Section 7,patient gets one episode treat illness, thus clearly interestedmaximizing ESR, SER. Thus, believe developing methods MOMDPsESR formulation critical direction future research.9. Conclusionsarticle presented survey algorithms designed sequential decision-making problems multiple objectives.order make explicit circumstances special methods neededsolve multi-objective problems, identified three distinct scenarios convertingproblem single-objective one impossible, infeasible, undesirable. wellproviding motivation need multi-objective methods, scenarios also representthree main ways methods applied practice.proposed taxonomy classifies multi-objective methods according applicable scenario, scalarization function (which projects multi-objective values scalarones), type policies considered. showed factors determinenature optimal solution, single policy, coverage set (convexPareto). taxonomy based utility-based approach, sees scalarizationfunction part utility, thus part problem definition. contrastsso-called axiomatic approach, usually assumes Pareto front appropriatesolution. showed utility-based approach used justify choicesolution set. Following line thought, observed (Observation 1) computingPareto front often necessary, many cases convex coverage setdeterministic stationary policies sufficient.Using taxonomy, surveyed literature multi-objective methods planninglearning. interesting observation learning methods use modelfree rather model based approach, identifying latter understudied class103fiRoijers, Vamplew, Whiteson & Dazeleymethods. Another part taxonomy yet widely studied learningcase monotonically increasing scalarization functions.discussed key applications MOMDP methods motivation importancemethods. Applications identified diverse range fields including environmental management, financial markets, information communications technology,control industrial processes, robotic systems traffic infrastructure. addition connections identified multi-objective sequential decision-making broadareas research probabilistic planning model-checking, multi-agent systemsgeneral multi-objective optimization.Finally, outlined several opportunities future work, include understudiedareas (model-based methods, learning monotonically increasing scalarization settings,many-objective sequential decision-making), reformulation objectiveMOMDPs Expectation Scalarized Return particularly importantoptimize policy executed once.Acknowledgmentswould like thank Matthijs Spaan, Frans Oliehoek, Matthijs Snel, Marie D. MannerSamy Sa, well anonymous reviewers, valuable feedback. worksupported Netherlands Organisation Scientific Research (NWO): DecisionTheoretic Control Network Capacity Allocation Problems (#612.001.109) project.ReferencesAberdeen, D., Thiebaux, S., & Zhang, L. (2004). Decision-theoretic military operationsplanning. Proc. ICAPS, Vol. 14, pp. 402411.Aissani, N., Beldjilali, B., & Trentesaux, D. (2008). Efficient effective reactive scheduling manufacturing system using Sarsa-multi-objective agents. MOSIM08: 7thConference Internationale de Modelisation et Simulation, pp. 698707.Aissani, N., Beldjilali, B., & Trentesaux, D. (2009). Dynamic scheduling maintenancetasks pretroleum industry: reinforcement approach. Engineering ApplicationsArtificial Intelligence, 22, 10891103.Altman, E. (1999). Constrained Markov Decision Processes. Chapman Hall/CRC,London.Aoki, K., Kimura, H., & Kobayashi, S. (2004). Distributed reinforcement learning usingbi-directional decision making multi-criteria control multi-stage flow systems.8th Conference Intelligent Autonomous Systems, Vol. 2004.03, pp. 281290.Barrett, L., & Narayanan, S. (2008). Learning optimal policies multiple criteria.Proceedings 25th International Conference Machine Learning, pp. 4147,New York, NY, USA. ACM.Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-IndependentDecentralized Markov Decision Processes. Proc. 2nd Intl Joint Conf.Autonomous Agents & Multi-Agent Systems.104fiA Survey Multi-Objective Sequential Decision-MakingBellman, R. E. (1957a). Markov decision process. Journal Mathematical Mech., 6,679684.Bellman, R. (1957b). Dynamic Programming. Princeton University Press.Bhattacharya, B., Lobbrecht, A. H., & Solomantine, D. P. (2003). Neural networks reinforcement learning control water systems. Journal Water Resources PlanningManagement, 129 (6), 458465.Bone, C., & Dragicevic, S. (2009). GIS intelligent agents multiobjective naturalresource allocation: reinforcement learning approach. Transactions GIS, 13 (3),253272.Bosoniu, L., Babuska, R., & Schutter, B. D. (2008). comprehensive survey multiagentreinforcement learning. IEEE Transactions Systems, Man, Cybernetics - PartC: Applications Reviews, 38 (2), 156172.Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,11, 194.Brazdil, T., Brozek, V., Chatterjee, K., Forejt, V., & Kucera, A. (2011). Two viewsmultiple mean-payoff objectives Markov decision processes. CoRR, abs/1104.3489.Bryce, D. (2008). value(s) probabilistic plans. Workshop Reality CheckPlanning Scheduling Uncertainty, ICAPS-08.Bryce, D., Cushing, W., & Kambhampati, S. (2007). Probabilistic planning multiobjective!. Technical report 08-006, Arizona State University.Carchrae, T., & Beck, J. C. (2005). Applying machine learning low-knowledge controloptimization algorithms. Computational Intelligence, 21 (4), 372387.Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple, fast,exact method partially observable markov decision processes. ProceedingsThirteenth conference Uncertainty artificial intelligence, pp. 5461.Castelletti, A., Pianosi, F., & Restelli, M. (2013). multiobjective reinforcement learningapproach water resources systems operation: Pareto frontier approximationsingle run. Water Resources Research.Castelletti, A., Corani, G., Rizzolli, A., Soncini-Sessa, R., & Weber, E. (2002). Reinforcement learning operational management water system. IFAC WorkshopModeling Control Environmental Issues, pp. 325330.Castelletti, A., Galelli, S., Restelli, M., & Soncini-Sessa, R. (2010). Tree-based reinforcement learning optimal water reservoir operation. Water Resources Research,46 (W09507).Castelletti, A., Pianosi, F., & Restelli, M. (2011). Multi-objective Fitted Q-Iteration: Paretofrontier approximation one single run. International Conference Networking,Sensing Control, pp. 260265.Castelletti, A., Pianosi, F., & Restelli, M. (2012). Tree-based Fitted Q-iteration multiobjective Markov decision processes. IEEE World Congress ComputationalIntelligence.105fiRoijers, Vamplew, Whiteson & DazeleyCastelletti, A., Pianosi, F., & Soncini-Sessa, R. (2008). Water reservoir control economic, social environmental constraints. Automatica, 44, 15951607.Chatterjee, K. (2007). Markov decision processes multiple long-run average objectives.FSTTCS, Vol. LNCS 4855, pp. 473484.Chatterjee, K., Majumdar, R., & Henzinger, T. A. (2006). Markov decision processesmultiple objectives. Proceedings 23rd Annual conference TheoreticalAspects Computer Science, STACS06, pp. 325336, Berlin, Heidelberg. SpringerVerlag.Cheng, L., Subrahmanian, E., & Westerberg, A. (2005). Multiobjective decision processesuncertainty: Applications, formulations solution strategies. IndustrialEngineering Chemistry Research, 44 (8), 24052415.Clemen, R. T. (1997). Making Hard Decisions: Introduction Decision Analysis (2edition). South-Western College Pub.Coello Coello, C. A., Lamont, G. B., & Van Veldhuizen, D. A. (2002). Evolutionary Algorithms Solving Multi-Objective Problems. Kluwer Academic Publishers.Comsa, I., Aydin, M., Zhang, S., Kuonen, P., & Wagen, J.-F. (2012). Multi objective resource scheduling LTE networks using reinforcement learning. International JournalDistributed Systems Technologies, 3 (2), 3957.Courcoubetis, C., & Yannakakis, M. (1998). Markov decision processes regular events.IEEE Transactions Automatic Control, 43 (10), 13991418.Crites, R. H., & Barto, A. G. (1996). Improving elevator performance using reinforcementlearning. Touretzky, D. S., Mozer, M. C., & Hasselmo, M. E. (Eds.), AdvancesNeural Information Processing Systems 8, pp. 10171023. MIT Press.Daellenbach, H. G., & Kluyver, C. A. D. (1980). Note multiple objective dynamicprogramming. Journal Operational Research Society, 31, 591594.Defourny, B., Ernst, D., & Wehenkel, L. (2008). Risk-aware decision making dynamicprogramming. NIPS 2008 Workshop Model Uncertainty Risk RL.Diehl, M., & Haimes, Y. Y. (2004). Influence diagrams multiple objectives tradeoff analysis. Systems, Man Cybernetics, Part A: Systems Humans, IEEETransactions on, 34 (3), 293304.Drugan, M. M., & Thierens, D. (2012). Stochastic pareto local search: Pareto neighbourhoodexploration perturbation strategies. Journal Heuristics, 18 (5), 727766.Dusparic, I., & Cahill, V. (2009). Distributed W-learning: Multi-policy optimization selforganizing systems. Third IEEE International Conference Self-AdaptiveSelf-Organizing Systems, pp. 2029.Dusparic, I., & Cahill, V. (2010). Multi-policy optimization self-organizing systems.SOAR 2009, LNCS 6090, pp. 101126.Dyer, J. S., Fishburn, P. C., Steuer, R. E., Wallenius, J., & Zionts, S. (1992). Multiple criteria decision making, multiattribute utility theory: next ten years. ManagementScience, 38 (5), 645654.106fiA Survey Multi-Objective Sequential Decision-MakingErnst, D., Geurts, P., & Wehenkel, L. (2005). Tree-based batch mode reinforcement learning.Journal Machine Learning Research, 6, 503556.Ernst, D., Glavic, M., & Wehenkel, L. (2004). Power systems stability control: Reinforcement learning framework. IEEE Transactions Power Systems, 19 (1), 427435.Feinberg, E. A., & Shwartz, A. (1995). Constrained Markov decision models weighteddiscounted rewards. Mathematics Operations Research, 20 (2), 302320.Ferreira, L., Bianchi, R., & Ribeiro, C. (2012). Multi-agent multi-objective reinforcementlearning using heuristically accelerated reinforcement learning. 2012 BrazilianRobotics Symposium Latin American Robotics Symposium, pp. 1420.Fleming, P., Purshouse, R., & Lygoe, R. (2005). Many-objective optimization: engineering design perspective. Evolutionary Multi-Criterion Optimization: Lecture NotesComputer Science, Vol. 3410, pp. 1432.Forejt, V., Kwiatkowska, M., Norman, G., Parker, D., & Qu, H. (2011). Quantitativemulti-objective verification probabilistic systems. Tools AlgorithmsConstruction Analysis Systems, pp. 112127. Springer Berlin Heidelberg.Forejt, V., Kwiatkowska, M., & Parker, D. (2012). Pareto curves probabilistic modelchecking. Automated Technology Verification Analysis, pp. 317332.Springer Berlin Heidelberg.Furnkranz, J., Hullermeier, E., Cheng, W., & Park, S.-H. (2012). Preference-based reinforcement learning: formal framework policy iteration algorithm. MachineLearning, 89 (1-2), 123156.Gabor, Z., Kalmar, Z., & Szepesvari, C. (1998). Multi-criteria reinforcement learning.Fifteenth International Conference Machine Learning, pp. 197205.Geibel, P., & Wysotzki, F. (2005). Risk-sensitive reinforcement learning applied controlconstraints. Journal Artificial Intelligence Research, 24, 81108.Geibel, P. (2001). Reinforcement learning bounded risk. Proceeding 18thInternational Conference Machine Learning, pp. 162169.Geibel, P. (2006). Reinforcement learning MDPs constraints. European Conference Machine Learning, Vol. 4212, pp. 646653.Gelly, S., & Silver, D. (2011). Monte-carlo tree search rapid action value estimationcomputer go. Artificial Intelligence, 175 (11), 18561875.Gong, P. (1992). Multiobjective dynamic programming forest resource management.Forest Ecology Management, 48, 4354.Guo, Y., Zeman, A., & Li, R. (2009). reinforcement learning approach setting multiobjective goals energy demand management. International Journal Agent Technologies Systems, 1 (2), 5570.Handa, H. (2009a). EDA-RL: Estimation distribution algorithms reinforcement learning problems. ACM/SIGEVO Genetic Evolutionary Computation Conference,pp. 405412.107fiRoijers, Vamplew, Whiteson & DazeleyHanda, H. (2009b). Solving multi-objective reinforcement learning problems EDA-RL acquisition various strategies. Proceedings Ninth Internatonal ConferenceIntelligent Sysems Design Applications, pp. 426431.Hiraoka, K., Yoshida, M., & Mishima, T. (2009). Parallel reinforcement learning weightedmulti-criteria model adaptive margin. Cognitive Neurodynamics, 3, 1724.Holenstein, A. A., & Badreddin, E. (1991). Collision avoidance behavior-based mobilerobot design. Robotics Automation, 1991. Proceedings., 1991 IEEE International Conference on, pp. 898903. IEEE.Houli, D., Zhiheng, L., & Yi, Z. (2010). Multiobjective reinforcement learning trafficsignal control using vehicular ad hoc network. EURASIP Journal AdvancesSignal Processing.Howard, R. A. (1960). Dynamic programming Markov decision processes. MIT Press.Humphrys, M. (1996). Action selection methods using reinforcement learning. Proceedings Fourth International Conference Simulation Adaptive Behavior, pp.135144.Ishibuchi, H., Tsukamoto, N., & Nojima, Y. (2008). Evolutionary many-objective optimisation: short review. IEEE Congress Evolutionary Computation, pp. 24192426.Issabekov, R., & Vamplew, P. (2012). empirical comparison two common multiobjective reinforcement learning algorithms. AI2012: 25th Australasian JointConference Artificial Intelligence, pp. 626636.Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning actingpartially observable stochastic domains. Artificial Intelligence, 101, 99134.Karlsson, J. (1997). Learning Solve Multiple Goals. Ph.D. thesis, University Rochester.Kober, J., & Peters, J. (2011). Policy search motor primitives robotics. MachineLearning, 12, 171203.Kober, J., & Peters, J. (2012). Reinforcement learning robotics: survey. Wiering,M., & Otterlo, M. (Eds.), Reinforcement Learning, Vol. 12 Adaptation, Learning,Optimization, pp. 579610. Springer Berlin Heidelberg.Kocsis, L., & Szepesvari, C. (2006). Bandit based monte-carlo planning. 17th EuropeanConference Machine Learning, pp. 282293. Springer.Kohl, N., & Stone, P. (2004). Policy gradient reinforcement learning fast quadrupedallocomotion. Proceedings IEEE International Conference RoboticsAutomation, pp. 26192624.Kolobov, A., Mausam, & Weld, D. S. (2012). theory goal-oriented mdps deadends. Proceedings Twenty-Eighth Conference Uncertainty ArtificialIntelligence.Kwak, J., Varakantham, P., Maheswarn, R., Tambe, M., Jazizadeh, F., Kavulya, G., Klein,L., Becerik-Gerber, B., Hayes, T., & Wood, W. (2012). SAVES: sustainable multiagent application conserve building energy considering occupants. 11th International Conference Autonomous Agents Multiagent Systems, pp. 2128.108fiA Survey Multi-Objective Sequential Decision-MakingLiao, H., Wu, Q., & Jiang, L. (2010). Multi-objective optimization reinforcement learningpower system dispatch voltage stability. Innovative Smart Grid TechnologiesConference Europe.Lin, C.-T., & Chung, I.-F. (1999). reinforcement neuro-fuzzy combiner multiobjectivecontrol. IEEE Transactions Systems, Man Cyberbetics - Part B, 29 (6), 726744.Liu, C., Xu, X., & Hu, D. (2013). Multiobjective reinforcement learning: comprehensiveoverview. Systems, Man, Cybernetics, Part C: Applications Reviews, IEEETransactions on, PP (99), 113.Liu, W., Tan, Y., & Qiu, Q. (2010). Enhanced q-learning algorithm dynamic powermanagement performance constraints. DATE10, pp. 602605.Lizotte, D. J., Bowling, M., & Murphy, S. A. (2010). Efficient reinforcement learningmultiple reward functions randomized clinical trial analysis. 27th InternationalConference Machine Learning, pp. 695702.Lizotte, D. J., Bowling, M., & Murphy, S. A. (2012). Linear fitted-q iteration multiplereward functions. Journal Machine Learning Research, 13, 32533295.Madani, O., Hanks, S., & Condon, A. (1999). undecidability probabilistic planninginfinite-horizon partially observable Markov decision problems. ProceedingsNational Conference Artificial Intelligence (AAAI), pp. 541548.Mannor, S., & Shimkin, N. (2001). steering approach multi-criteria reinforcementlearning. Neural Information Processing Systems, pp. 15631570.Mannor, S., & Shimkin, N. (2004). geometric approach multi-criterion reinforcementlearning. Journal Machine Learning Research, 5, 325360.Maravall, D., & de Lope, J. (2002). reinforcement learning method dynamic obstacleavoidance robotic mechanisms. Computational Intelligent Systems AppliedResearch: Proceedings 5th International FLINS Conference, pp. 485494, Singapore. World Scientific.Mariano, C., & Morales, E. (1999). MOAQ Ant-Q algorithm multiple objectiveoptimization problems. GECCO-99: Proceedings Genetic EvolutionaryComputation Conference, pp. 894901.Mariano, C., & Morales, E. (2000a). new approach solution multiple objectiveoptimization problems based reinforcement learning. Advances ArtificialIntelligence, International Joint Conference, 7th Ibero-American Conference AI,15th Brazilian Symposium. Springer.Mariano, C., & Morales, E. (2000b). new distributed reinforcement learning algorithmmultiple objective optimisation problems. Lecture Notes AI Vol 1952: Proceedings Mexican International Conference Artficial Intelligence, pp. 212223.Springer.Meisner, E. M. (2009). Learning Controllers Human-Robot Interaction. Ph.D. thesis,Rensselaer Polytechnic Institute.109fiRoijers, Vamplew, Whiteson & DazeleyMouaddib, A.-I. (2006). Collective multi-objective planning. Proceedings IEEEWorkshop Distributed Intelligent Systems: Collective Intelligence Applications (DIS06), pp. 4348, Washington, DC, USA. IEEE Computer Society.Mukai, Y., Kuroe, Y., & Iima, H. (2012). Multi-objective reinforcement learning methodacquiring Pareto optimal policies simultaneously. IEEE International Conference Systems, Man Cybernetics, pp. 19171923.Natarajan, S., & Tadepalli, P. (2005). Dynamic preferences multi-criteria reinforcementlearning. International Conference Machine Learning, pp. 601608.Nojima, Y., Kojima, F., & Kubota, N. (2003). Local episode-based learning multiobjective behavior coordination mobile robot dynamic environments.12th IEEE International Conference Fuzzy Systems, Vol. 1, pp. 307312.Ogryczak, W., Perny, P., & Weng, P. (2011). minimizing ordered weighted regretsmultiobjective Markov decision processes. 2nd International ConferenceAlgorithmic Decision Theory, pp. 190204.Ong, S. C., Png, S. W., Hsu, D., & Lee, W. S. (2010). Planning uncertainty robotictasks mixed observability. International Journal Robotics Research, 29 (8),10531068.Pareto, V. (1896). Manuel dEconomie Politique. Giard, Paris.Peek, N. B. (1999). Explicit temporal models decisiontheoretic planning clinicalmanagement. Artificial Intelligence Medicine, 15 (2), 135154.Perez, J., Germain-Renaud, C., Kegl, B., & Loomis, C. (2009). Responsive elastic computing. International Conference Autonomic Computing, pp. 5564.Perny, P., & Weng, P. (2010). finding compromise solutions multiobjective Markovdecision processes. ECAI Multidisciplinary Workshop Advances PreferenceHandling, pp. 5560.Perny, P., Weng, P., Goldsmith, J., & Hanna, J. P. (2013). Approximation lorenz-optimalsolutions multiobjective markov decision processes. Workshops TwentySeventh AAAI Conference Artificial Intelligence.Pervez, A., & Ryu, J. (2008). Safe physical human robot interaction-past, presentfuture. Journal Mechanical Science Technology, 22 (3), 469483.Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations largePOMDPs. Journal Artificial Intelligence Research, 27 (1), 335380.Precup, D., Sutton, R. S., & Dasgupta, S. (2001). Off-policy temporal-difference learningfunction approximation. Proceedings 18th International ConferenceMachine Learning, pp. 417424.Puterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons, Inc.Roijers, D. M., Whiteson, S., & Oliehoek, F. A. (2013). Computing convex coverage setsmulti-objective coordination graphs. ADT 2013: Proceedings Third International Conference Algorithmic Decision Theory. appear.110fiA Survey Multi-Objective Sequential Decision-MakingRoss, S., Pineau, J., Paquet, S., & Chaib-draa, B. (2008). Online planning algorithmsPOMDPs. Journal Artificial Intelligence Research, 32, 663704.Russell, S., & Zimdars, A. L. (2003). Q-decomposition reinforcement learning agents.Proceedings 20th International Conference Machine Learning, pp. 656663.Schneider, J., Wong, W.-K., Moore, A., & Riedmiller, M. (1999). Distributed value functions. Proceedings 16th International Conference Machine Learning, pp.371378, San Francisco, CA. Morgan Kaufmann.Shabani, N. (2009). Incorporating flood control rule curves Columbia River hydroelectric system multireservoir reinforcement learning optimization model. Mastersthesis, University British Columbia.Shelton, C. R. (2001). Importance sampling reinforcement learning multiple objectives. AI Technical Report 2001-003, MIT.Shortreed, S., Laber, E., Lizotte, D., Stroup, T., Pineau, J., & Murphy, S. (2011). Informingsequential clinical decision-making reinforcement learning: empirical study.Machine Learning, 84, 109136.Smallwood, R., & Sondik, E. (1973). optimal control partially observable Markovprocesses finite horizon. Operations Research, 21 (5), 10711088.Soh, H., & Demiris, Y. (2011). Evolving policies multi-reward partially observableMarkov decision processes (MR-POMDPs). GECCO11 Proceedings 13thAnnual Conference Genetic Evolutionary Computation, pp. 713720.Sondik, E. (1971). optimal control partially observable processes finite horizon.Ph.D. thesis, Stanford University, Stanford, California.Sondik, E. (1978). optimal control partially observable Markov processesinfinite horizon: Discounted costs. Operations Research, 26 (2), 282304.Spaan, M., & Vlassis, N. (2005). Perseus: Randomized point-based value iterationPOMDPs. Journal Artificial Intelligence Research, 24 (1), 195220.Stewart, T. J. (1992). critical survey status multiple criteria decision makingtheory practice. Omega, 20 (5), 569586.Sutton, R. S. (1988). Learning predict methods temporal differences. MachineLearning, 3 (1), 944.Sutton, R. S., & Barto, A. G. (1998). Introduction Reinforcement Learning (1st edition).MIT Press, Cambridge, MA, USA.Sutton, R., McAllester, D., Singh, S., & Mansour, Y. (2000). Policy gradient methodsreinforcement learning function approximation. NIPS, pp. 10571063.Szita, I. (2012). Reinforcement learning games. Wiering, M., & Otterlo, M. (Eds.),Reinforcement Learning, Vol. 12 Adaptation, Learning, Optimization, pp. 539577. Springer Berlin Heidelberg.Tan, K. C., Khor, E. F., Lee, T. H., & Sathikannan, R. (2003). evolutionary algorithmadvanced goal priority specification multi-objective optimization. JournalArtificial Intelligence Research, 18, 183215.111fiRoijers, Vamplew, Whiteson & DazeleyTeichteil-Konigsbuch, F. (2012a). Path-constrained markov decision processes: bridginggap. Proceedings Twentieth European Conference Artificial Intelligence.Teichteil-Konigsbuch, F. (2012b). Stochastic safest shortest path problems. Proceedings Twenty-Sixth AAAI Conference Artificial Intelligence.Tesauro, G., Das, R., Chan, H., Kephart, J. O., Lefurgy, C., Levine, D. W., & Rawson, F.(2007). Managing power consumption performance computing systems usingreinforcement learning. Neural Information Processing Systems.Tong, H., & Brown, T. X. (2002). Reinforcement learning call admission controlrouting quality service constraints multimedia networks. Machine Learning, 49, 111139.Uchibe, E., & Doya, K. (2009). Constrained Reinforcement Learning IntrinsicExtrinsic Rewards, pp. 155166. Theory Novel Applications Machine Learning.I-Tech, Vienna, Austria.Vamplew, P., Dazeley, R., Barker, E., & Kelarev, A. (2009). Constructing stochastic mixturepolicies episodic multiobjective reinforcement learning tasks. AI09: 22ndAustralasian Conference Artificial Intelligence, pp. 340349.Vamplew, P., Dazeley, R., Berry, A., Dekker, E., & Issabekov, R. (2011). Empirical evaluation methods multiobjective reinforcement learning algorithms. Machine Learning,84 (1-2), 5180.Vamplew, P., Yearwood, J., Dazeley, R., & Berry, A. (2008). limitations scalarisation multi-objective reinforcement learning Pareto fronts. AI08: 21stAustralasian Joint Conference Artificial Intelligence, pp. 372378. Springer.Van Otterlo, M., & Wiering, M. (2012). Reinforcement learning markov decision processes. Reinforcement Learning: State Art, chap. 1, pp. 342. Springer.Van Vaerenbergh, K., Rodriguez, A., Gagliolo, M., Vrancx, P., Nowe, A., Stoev, J.,Goossens, S., Pinte, G., & Symens, W. (2012). Improving wet clutch engagementreinforcement learning. International Joint Conference Neural Networks,IJCNN 2012.Vira, C., & Haimes, Y. Y. (1983). Multiobjective decision making: theory methodology.No. 8. North-Holland.Viswanathan, B., Aggarwal, V. V., & Nair, K. P. K. (1977). Multiple criteria Markovdecision processes. TIMS Studies Management Science, 6, 263272.Wakuta, K., & Togawa, K. (1998). Solution procedures Markov decision processes. Optimization: Journal Mathematical Programming Operations Research, 43 (1),2946.Wakuta, K. (1999). note structure value spaces vector-valued Markov decisionprocesses.. Mathematical Methods Operations Research, 49 (1), 7785.Wang, W., & Sebag, M. (2013). Hypervolume indicator dominance reward based multiobjective monte-carlo tree search. Machine Learning, 127.Watkins, C. J. C. H. (1989). Learning Delayed Rewards. Ph.D. thesis, CambridgeUniversity.112fiA Survey Multi-Objective Sequential Decision-MakingWhite, C. C., & Kim, K. M. (1980). Solution procedures solving vector criterion Markovdecision processes. Large Scale Systems, 1, 129140.White, D. (1982). Multi-objective infinite-horizon discounted Markov decision processes.Journal Mathematical Analysis Applications, 89 (2), 639 647.Whiteson, S. (2012). Evolutionary computation reinforcement learning. Wiering,M. A., & van Otterlo, M. (Eds.), Reinforcement Learning: State Art, chap. 10,pp. 325352. Springer, Berlin.Wiering, M., & De Jong, E. (2007). Computing optimal stationary policies multiobjective Markov decision processes. IEEE International Symposium Approximate Dynamic Programming Reinforcement Learning, pp. 158165. IEEE.Yang, Z., & Wen, K. (2010). Multi-objective optimization freeway traffic flow viafuzzy reinforcement learning method. 3rd International Conference AdvancedComputer Theory Engineering, Vol. 5, pp. 530534.Yin, S., Duan, H., Li, Z., & Zhang, Y. (2010). Multi-objective reinforcement learningtraffic signal coordinate control. 1th World Conference Transport Research.Zeleny, M., & Cochrane, J. L. (1982). Multiple criteria decision making, Vol. 25. McGrawHill New York.Zhao, Y., Chen, Q., & Hu, W. (2010). Multi-objective reinforcement learning algorithmMOSDMP unknown environment. Proceedings 8th World CongressIntelligent Control Automation, pp. 31903194.Zheng, K., Li, H., Qiu, R. C., & Gong, S. (2012). Multi-objective reinforcement learningbased routing cognitive radio networks: Walking random maze. InternationalConference Computing, Networking Communications, pp. 359363.Zitzler, E., Thiele, L., Laumanns, M., Fonseca, C. M., & da Fonseca, V. G. (2003). Performance assessment multiobjective optimizers: analysis review. EvolutionaryComputation, IEEE Transactions on, 7 (2), 117132.113fiJournal Artificial Intelligence Research 48 (2013) 231 252Submitted 4/2013; published 10/2013Optimal Implementation Watched LiteralsGeneral TechniquesIan P. Gentian.gent@st-andrews.ac.ukSchool Computer Science, St Andrews UniversitySt Andrews, Fife KY16 9SX, UKAbstractprove implementation technique scanning lists backtracking searchalgorithms optimal. result applies simple general framework, present:applications include watched literal unit propagation SAT number examplesconstraint satisfaction. Techniques like watched literals known highly spaceefficient effective practice. implemented circular approach describedhere, techniques also optimal run time per branch big-O terms amortizedacross search tree. also applies multiple list elements must found.constant factor overhead worst case 2. Replacing existing non-optimalimplementation unit propagation MiniSat speeds propagation 29%, thoughenough improve overall run time significantly.1. Introductionmany backtrack search procedures, given list must contain element satisfyingproperty. call acceptable element. acceptable element exists list,relevant action triggered, assigning unit clause SAT (Boolean Satisfiability). paper considers monotonic acceptability properties: i.e. elementunacceptable node search tree, remains unacceptable descendant nodes.description general, applies vital components modern search techniques.well unit propagation SAT, typical application CSP (Constraint SatisfactionProblem) support variable-value pair general purpose arc consistency algorithm,e.g. MGAC2001/3.1 (Bessiere, Regin, Yap, & Zhang, 2005). acceptable element(support) available, triggered action variable-value pair must pruned.common technique maintaining acceptability keep pointer knownacceptable element. element becomes unacceptable search process,scan list new acceptable element. find one change pointer. not,trigger necessary action. backtracking, must guarantee pointerpoints acceptable element. several methods achieving this. Onereset pointer list scan. second store current valuepointer restore value backtracking. paper focuses third method,backtrack-stable approach, used watched literals SAT.backtrack-stable approach simple: changes made pointer exceptscanning list new acceptable element. Watched literals SAT,classic example approach, proven highly effective practice (Moskewicz,Madigan, Zhao, Zhang, & Malik, 2001). correctness depends acceptabilitymonotonic. move pointer new acceptable element particular node,c2013AI Access Foundation. rights reserved.fiGentnew element must acceptable ancestor nodes. backtrack thus stillacceptable element, even different one entered node with.Unfortunately, branch (i.e. sequence nodes root leaf node),longer guarantee single pass list enough. given node pointer maymove due moves children nodes, meaning may missed elements. Indeed,single branch may need check every value list many times.paper shows simple, circular, implementation backtrack-stable approach optimal big-O terms amortised across branches tree, significantlybetter theoretical property previously suggested. amortisation appliessearch tree explored depth-first style, independent size search tree.constant increase worst case complexity 2. Applicability existing code dependslow-level implementation details. discuss cases detail. cases existing implementations shown optimal, others optimal implementationwatched literals used. example show empirically implementing watchedliterals optimally speeds unit propagation MiniSat 29%, although effectlead statistically significant improvement overall MiniSat solution time.Section 2 describes simple framework scanning lists backtracking searchgives pseudocode three methods compared paper, gives worked example motivate proof circular approach optimal. Section 3 gives proofrelated results, including detailed comparisons state restoration method. Section 4 generalises result multiple acceptable elements. Later sections discuss applications constraint satisfaction, watched literal unit propagation SAT, includingexperiments MiniSat. Appendices gives proofs omitted main text, detailedmethodology SAT experiments, results another method list scanning,summary Online Appendix paper.2. Simple Framework List Scanning Algorithmslist list length N .1 assume boolean function acceptable(list,i)check list elements, returning true list[i] acceptable false not. Throughoutpaper, function required monotonic, following sense.Definition 1 (Monotonicity Acceptability). Acceptability monotonic if, wheneveracceptable(list,i) fails node, acceptable(list,i) would also fail calledremainder search process node descendant nodes.Note definition allows elements list moved, long unacceptability index maintained: flexibility important Section 4.unacceptability detected must find new acceptable element, guarantee noneexists. achieved one three variants function findNewElement (sometimes abbreviated FNE). variant updates value pointer last, eithersucceeds acceptable value list[last], fails guarantee acceptable value list exists. execution environment assumed guarantee that,1. paper assume computational model word size least log N bits,standard operations words take O(1) time. ideal complexity-theoretic pointview, standard assumption literature, although unfortunately usually clearly stated.232fiOptimal Implementation Watched Literalsnode search tree, list[last] changes acceptable unacceptable,findNewElement(list) called next node visited, except backtracking occurs node descendant node visited. purposespaper, detection unacceptability counted call acceptable: whatevercost detection approach studied here. assume initialisation phase O(N ) calls acceptable made find initial value last:calls considered preprocessing charged node search tree.also assume calls findNewElement made initialisationunacceptability detected.variant findNewElement ensure following invariant maintained.Note invariant means one first two cases holds, value list[last]unacceptable iff acceptable element list.Invariant 2. times least one following true:value list[last] acceptable;acceptable element list;initialisation process completed;value list[last] become unacceptable current node findNewElement yet called completed.Many calls findNewElement may happen single node, factsacceptability may become known time. example, case unitpropagations SAT, current watched literal (i.e. list[last]) may become unsatisfiable(unacceptable), scan new watched literal (value last). laterpropagations node make new value unsatisfiable, leading new scans.paper compares three implementations findNewElement shows goodproperties last one. differences arise last dealtinvocations findNewElement. simplest variant shown Procedure 1:time called previous value last discarded list searchedbeginning.Procedure 1: FNE-NoState(list)1: last := -12: repeat3:last := last + 14:acceptable(list,last) return true5: last = N6: return falseProcedure 1 simple, certainly maintains Invariant 2, highly space-efficient.significant disadvantage worst case require (N 2 ) calls acceptableevery leaf node, stated Proposition 3 proved Appendix E. (Appendix Eomitted main text available online, see Appendix D.)233fiGentProposition 3. procedure FNE-NoState maintains Invariant 2. makes O(N 2 )calls acceptable per branch search tree, requires (N 2 ) worst case.(Proof Appendix E online.)remaining two variants use FNE-NoState initialise last, make differentcalls thereafter. second variant based state restoration. call continues searchrecent value last found current node ancestors. Sincevalue last may changed descendent nodes, mechanism backtrackingsearch solver must exploited restore value last backtracking occurs.method vary solvers, critical paper. Given this,method shown Procedure 2.Procedure 2: FNE-RestoreState(list)1: repeat2:last := last + 13:acceptable(list,last) return true4: last = N5: return falseSince acceptability monotonic last always restored value previously,invariant guaranteed. following, proof necessary.Proposition 4. procedure FNE-RestoreState maintains Invariant 2. givenbranch search tree root leaf node, N calls acceptable made.final, backtrack-stable, variant, restore former values lastbacktracking. problem branch, even single node,value last moved later nodes restored return currentnode. deal correctly, allow every element list checkedeven may already checked higher current branch evennode. focus paper circular method checking elements:end list circle around end list zero, continue checkinghit value last call made. initialisation callprocedure FNE-NoState. subsequent calls following.Procedure 3: FNE-Circular(list)1: last-cache := last2: repeat3:last := last + 14:last = N last := 05:acceptable(list,last) return true6: last = last-cache7: return falseclaim made originality circular method: used Gent, Jefferson,Miguel (2006b) probably earlier authors. Unlike previous variants,invariant hold search algorithms. Correctness proved Section 3below, context downwards-explored search trees, defined Definition 5.234fiOptimal Implementation Watched Literals.One difference Procedures 2 3 noted. given nodesearch tree, Procedure 2 make N calls acceptable. However, Procedure3 can, perhaps counterintuitively, require almost 2N calls single node. example,suppose given node last = 0. value becomes unacceptableacceptable value N 1, requires N 1 calls set last = N 1.propagation may later make N 1 unacceptable also. resulting new call Procedure3 cannot find acceptable value, must still check values 0 N 2memory previous call. (The variable last-cache local call Procedure.)N 1 calls acceptable, total 2N 2 node.2.1 Worked Examplepresent worked example Procedure 3, showing amortize count callsacceptable way form basis optimality results paper.Figure 1 shows example search using circular approach. example assumenode id = 0 :last=14, cost = 14Xlast=178 :14, 17X1 :14, 0last=192 :17, 3Xlast=165 :17, 18X9 :16, 21X12 :14, 18Xlast=183 :17, 19P= 360,1,2,34 :19, 2XP0,1,2,4= 196 :18, 20XP0,1,5,6= 52last=157 :17, 19XP0,1,5,710 :16, 0P= 510,8,9,10= 5211 :16, 0P0,8,9,11= 5213 :15, 1XP0,8,12,13= 5014 :14, 38XP= 870,8,12,14Figure 1: search using circular approach. See main text description.list searched 20 elements, indexed 0 19. initialisationsets last = 0. box node indicates node number (italics), value lastcalls FNE-Circular (bold), cost calls total node(measured number calls acceptable), including failed call one. Xindicates successful call made, indicates unsuccessful call. occurnode, value last moved successful call, value laterbecomes unacceptable, another call FNE-Circular fails. branch annotatedvalue last search left changed value last. example, node5 value last 17, left child set 18, meaning changedbranched right. total cost branch listed, e.g. 87 last branch.example, first 14 elements become unacceptable root, calls FNEmust check 14 elements order set last = 14. restored last maximum cost235fiGentbranch would number elements list, case 20. usedFNE-Circular, searched one branch, maximum cost would almost double,38. could happen eventually settle last = 19, becomes unacceptablefailed call costs 19. contrast, extreme right hand branch costs14 + 17 + 18 + 38 = 87, twice maximum search took single branch.Summing number checks FNE branch, get 36 + 19 + 52 + 51 + 52 +52 + 50 + 87 = 399, almost 50 times number branches, 8. However, countsmany costs twice. summing node, total number checks across tree14 + 0 + 3 + 19 + 2 + 18 + 20 + 19 + 17 + 21 + 0 + 0 + 18 + 1 + 38 = 190. 20times number branches, less 40 times number branches. showlatter bound always true: never look list elements 2kN ,N number list elements k number branches.left branch segment (LBS) segment branch left branching decisionsending leaf node. paper, left child given internal node taken meanwhichever child node explored first.2 Figure 2 shows search tree Figure 1,showing LBSs cost across LBS. Crucial points note are:left branch segment contains consecutively numbered nodes; cost left branchsegment never 38 = 2N 2; total cost summed left branchsegments (36 + 2 + 38 + 19 + 38 + 0 + 19 + 38 = 190) identical total cost acrosstree calculated earlier. observations tree provable general.3. Formal Resultsworked example showed binary branching, necessary. lefthand child node may number later nodes (including zero unarynode.) leaf node simply node children. prove correctness optimalityFNE-Circular search algorithms build trees following type:Definition 5 (Downwards-Explored Search Tree). search tree Downwards-Exploredif: node n0 tree, nodes descending n0 visited later n0 ,node n1 descended n0 visited later n0 , nodes treedescend n0 visited n0 n1 .Depth-first search certainly defines downwards-explored tree, many variantsconflict-directed backjumping (Prosser, 1993). less obvious case single restartmodern Conflict-Driven Clause Learning (CDCL) SAT solver (Marques-Silva, Lynce, &Malik, 2009). backtracking, search returns level tree chosen conflictanalysis, guaranteed intermediate parts search tree unsatisfiable,never visited again, resulting downwards-explored search tree. CDCL solversundertake restarts, major algorithms Iterative Deepening (Korf, 1985)Limited Discrepancy Search (Harvey & Ginsberg, 1995; Prosser & Unsworth, 2011).three examples iteration considered separately gives downward-exploredtree. results current paper therefore apply single iteration restartalgorithm. also apply multiple restarts iterations one counts separately2. algorithm may regard first branching choice right branch, e.g. limited discrepancy searchgoing heuristic, paper left child defined whichever one explored first.236fiOptimal Implementation Watched Literalsnode id = 0 : last=14, cost = 14Xlast=178 :14, 17X1 :14, 0last=192 :17, 3Xlast=165 :17, 18X9 :16, 21X12 :14, 18Xlast=183 :17, 19P0,1,2,3= 364 :19, 2XP4=26 :18, 20XP5,6= 38last=157 :17, 19XP7= 1910 :16, 011 :16, 0PP8,9,10= 3811=013 :15, 1XP12,13= 1914 :14, 38XP= 3814Figure 2: example Figure 1 additional annotations. double line indicatesleft hand branch, wavy line right hand branch. sequence doublelines indicates left branch segment. solid box around node indicatesstart left branch segment. Finally, sums costs madeleft branch segment ending leaf node.iteration, i.e. counting branch twice even duplicate branchprevious iteration. major algorithms explore downwards: e.g. breadth-firstbest-first search. algorithms, consecutive nodes last, containedsearch state, moves pointing acceptable unacceptable value.results paper therefore apply way algorithms.first result correctness FNE-circular method, slightly generalisedalso apply middle-out Procedure 5 Appendix B.Definition 6 (Locally Correct). findNewElement procedure locally correct iff:1. element list acceptable procedure sets last point acceptablevalue succeeds;2. element list acceptable procedure fails exits last setvalue entry.Specifically, FNE-Circular locally correct design, therefore global correctness corollary following theorem.Theorem 7. (Correctness) search algorithm defines downwards-explored searchtree, procedure findNewElement locally correct, Invariant 2 true times.(Proof Appendix E online.)Definition 8 (LBS). left branch segment ending leaf node n LBS(n) definedrecursively follows:237fiGentn LBS(n)m1 LBS(n) m1 left hand child parent node m2 , m2 LBS(n).convenience, parent node one child call left hand child.LBS(n) minimal set nodes satisfying properties.Note leaf node left hand child parent, definitionstrivially give LBS(m) = {m}. Every internal node exactly one left child,proceed follows.Lemma 9. Every node tree contained exactly one left branch segment.downwards-explored search tree, nodes left branch segment visited consecutivelywithout search visiting nodes. (Proof Appendix E online.)Theorem 10. downwards-explored search tree, N 1 callsacceptable made successful calls FNE-Circular left branch segment.Proof. proof relies monotonicity acceptability tree.nodes LBS explored sequentially without interruption, Lemma 9, callsFNE-Circular therefore changes last, consecutive. definition FNECircular means check N 1 elements single LBS, last mustincremented least N times, meaning every value checked least LBS,including original value last entry LBS. Call value i. Say root LBSm1 , element checked node m2 either m1 = m2 m1 ancestorm2 . entry m1 call acceptable(list, i) m2 , every list elementmust checked. Furthermore, value j, either check j FNECircular failed, succeeded later value become unacceptable causinganother call FNE-Circular. case value last wouldmoved j. Therefore, j, list element j cannot acceptablecheck made node m2 . Therefore, call FNE-Circular makessecond check must fail. required, shownN 1 calls acceptable LBS, unsuccessful call FNE-Circular.Corollary 11. downwards-explored search tree, calls FNE-Circular LBSmake 2N 2 calls acceptable.Proof. Theorem 10, maximum number list elements checked successful callsLBS N 1. first unsuccessful call FNE-Circular check N 1:elements list except list[last]. calls necessary LBS, sinceelement become acceptable again. total cost bounded 2N 2.Theorem 12. downwards-explored search tree containing k branches, calls FNECircular make k(2N 2) calls acceptable.Proof. call FNE-Circular occurs node search tree. nodesearch tree exactly one LBS. Therefore every call FNE-Circular occursexactly one LBS. 2N 2 acceptability checks LBS. treek branches exactly k LBSs, meaning total number list elementschecked tree bounded k(2N 2).238fiOptimal Implementation Watched Literalstherefore optimality following sense:Theorem 13. (Optimality) downwards-explored search tree, circular approachrequires space one last pointer worst case O(N ) calls acceptable perbranch tree, algorithm require o(N ) calls per branch. (Proof Appendix Eonline.)results show restoring state circular equivalent worst casetime complexity big-O terms across tree, compare precisely.Proposition 14. circular, many 2k(N 2) calls acceptabledownwards-explored search tree. state restoration, number calls acceptablebounded kN k(N 1) calls acceptable tree. (ProofAppendix E online.)Thus worst case number list element checks backtrack last twice worst case number list checks backtrack it.apply instance instance basis, following result shows.Proposition 15. (Non Dominance) techniques check less acrossdownwards-explored search tree. circular method take (k) times fewer callsacceptable across tree state restoration, state restoration need (N )times fewer calls circular. (Proof Appendix E online.)language Likitvivatanavong, Zhang, Shannon, Bowen, Freuder (2007),LBS, circular positive repeats (duplicate successful calls acceptable).negative repeats (duplicate failed calls) last call FNE-Circularfails, failed call FNE acceptable element leafnode. reduces significantly chance N calls LBS. example,SAT, consider clause r literals. random boolean assignment, 21rchance literals valid full assignment, 2rr exactly one literalvalid. two valid literals, failed call. chance callFNE failing r+12r LBS. clauses 10 literals randomassignments maximum 1% chance negative repeats LBS.4. Generalisation Multiple Acceptable Elementsimportant case need maintain multiple acceptable elements list.Specifically, must ensure least W different elements list acceptable,trigger action less W are. classic example watched literals SAT,W = 2 action unit propagation, examples arise constraintshigher W discussed Section 5. circular approach generalised,bound number calls acceptable per branch independent W .implementation technique maintain two lists. first, called watched,length W 1, second list, unwatched, length N . union two listsoriginal list list length N + W 1. Initialisation assumed either makeelements watched plus unwatched[last] acceptable, (if possible) triggernecessary action. solver infrastructure assumed ensure correct notification239fiGentfindNewElement: must maintain position element watched,done O(1) time per move element using O(N + W ) space. assumeelement watched changes acceptable unacceptable, singlevalue unwatched[last] does, FNE-Circular-W called appropriateparameters next node visited, unless backtracking occurs then. alsoassume one event happens node, separate call happensevent. Given assumptions, implementation almost trivial, follows.FNE-Circular-W(list,elt,i)1:// elt value list newly unacceptable2:// index elt watched unless elt = unwatched[last ]3: elt 6= watched[i]4:watched[i] := unwatched[last]5:unwatched[last] := elt6: return FNE-Circular(unwatched)Everything follows depends fact that, despite swapping elements, acceptability monotonic list unwatched.Procedure 4:Proposition 16. Acceptability unwatched monotonic acceptability list is.Proof. acceptability list monotonic, way nonmonotonicity couldoccur element unwatched replaced one watched.happen FNE-Circular-W Line 5. value unwatched[last] replacedelt, elt becoming unacceptable reason FNE-Circular-W called.Therefore, call acceptable(list,last) must return false, whether wouldsucceeded replacement. Definition 1, monotonicity therefore respected.Proposition 16 Corollary 11 make following immediate. remarkablebound Corollary 17 independent W , number acceptable elements required.Initialisation need O(N + W ) calls acceptable, done once.Corollary 17. downwards-explored search tree, calls FNE-Circular-WLBS make 2N 2 calls acceptable.must also show correctness, done revised invariant. firstclause holds, W required acceptable elements, second does,W 1 acceptable elements trigger necessary action.Invariant 18. times least one following true:elements watched {unwatched[last]} acceptable;acceptable element unwatched;initialisation process completed;current node, least one element watched{unwatched[last]} becomeunacceptable corresponding call findNewElement yet completed.240fiOptimal Implementation Watched LiteralsLemma 19. downwards-explored search tree, call FNE-Circular-W eitherreturns false reduces one number unacceptable elements set watched{unwatched[last]}. (Proof Appendix E online.)Theorem 20. (Correctness) downwards-explored search tree, FNE-Circular-Wmaintains Invariant 18. (Proof Appendix E online.)Chai Kuehlmann (2003) described multiple watches pseudo-boolean solver,although current results apply number watches variedsearch. Chai Kuehlmann give implementation details: implementationdescribed follows Gent et al. (2006b) sum boolean variables.5. Application Constraint Satisfactionfirst application constraint propagation, specifically maintaining generalised arcconsistency. optimal algorithm GAC2001/3.1 easily turned algorithmMGAC2001/3.1 maintains GAC search (Bessiere et al., 2005).Corollary 21. constraint arity r variable domain size d,downwards-explored search tree circular approach maintaining last pointerMGAC2001/3.1 achieved using space store O(dr) last pointers (beyond storagespace constraint ), requires time check O(rdr ) tuples per branch. (ProofAppendix E online.)reasonable assumption takes time O(r) check tuple since arityr (Bessiere et al., 2005).3 basis get time O(r2 dr ) per branch. Bessiere et al.report time complexity O(r2 dr ) GAC2001/3.1 require numberlast pointers. shows amortized worst case big-O time per branchMGAC2001/3.1 needed simply one-off algorithm GAC2001/3.1,using space. Bessiere (2004) reports using state restoration techniquesimplementation MAC2001 (Bessiere & Regin, 2001). therefore used additionalspace, since many copies last must stored instead one.results improve given van Dongen (2004). binary constraints (r = 2),gives upper bound space complexity O(d min (n, d)) per constraint timeoptimal implementation using time O(d2 ) per branch, domain size nnumber variables problem. Corollary 21 gives O(d2 ) time improvedO(d) space, although van Dongens results remain valid given upper bounds.several studies time complexity maintaining arc consistencybranch, suggestion leaving last pointers alone good theoretically.existing circular implementations seen optimal. example, Gentet al. (2006b, p. 185) wrote: one general disadvantage mentionedwatched triggers, . . . often possible use propagation algorithmoptimal worst case terms propagation work performed single branch.3. constraints stored extensionally, true, space requirement store constraintO(rdr ). However constraints may also stored intensionally procedurally, casechecking time either larger smaller, space requirement arbitrarily small.241fiGentexample variant GAC-2001/3.1 below. fact, implementation MGAC2001/3.1 time optimal amortized across branches better space complexityvan Dongen (2004) reports optimal implementations MAC-2001/3.1.Regin (2005) studied maintaining arc consistency search without backtrackinglast pointer. writes: last values restored backtrackingtime complexity AC-6 AC-7 algorithms O(d3 ) (Regin, 2005, p. 528),gives example similar right hand branch Figure 1. contextdomain size constraints binary maximum length list N = d2 . Reginentirely correct. contribution show lack optimality branchcompensated amortization factor d. Regin writes also: Currently [i.e.paper], MAC version algorithms capable keep optimal timecomplexity every branch tree search (O(d2 ) per constraint, sizelargest domain), without sacrificing space complexity. method recomputecorrect value last backtracking without storing it, comparing current valuelast values restored domain backtracking. elegant generaliseobvious way non-binary constraints number combinations restoredvalues exponential.Likitvivatanavong et al. (2007) discuss cost Arc Consistency search.give ACS-resOpt, uses instance list scanning framework given here,report optimal given node, optimal branchtree (which call path-forward complexity). reordering domains search,Adaptive Domain Ordering (ADO) enforces MAC binary constraints optimalproperty O(ed2 ) worst-case time complexity branch search tree (Likitvivatanavong, Zhang, Bowen, & Freuder, 2005). Unfortunately perform wellempirical tests (Likitvivatanavong et al., 2007), combined reordering domains search, militates widespread adoption constraint solvers.applications constraints. Nightingale, Gent, Jefferson, Miguel(2013) use circular technique avoid restoring state GAC algorithms exploitingshort supports. Jefferson, Moore, Nightingale, Petrie (2010) use propagatorgeneralised constraint. Gent et al. (2006b) used circular approach propagatingelement constraint. also used generalisation W literals sum booleansconstraint. examples seen code Minion constraint solver(Gent, Jefferson, & Miguel, 2006a), version 0.15, described papersconstraint litsumgeq. seen excellent theoretical properties.6. Application Satisfiabilitysecond application watched literal unit propagation SAT (Moskewicz et al., 2001).clause list literals. acceptable element one represents either unassignedsatisfied literal. standard two-literal watching must maintain two acceptable elements. Modern CDCL SAT solvers quickly learn large numbers large clauses, thusbenefit greatly maintain two pointers clause. approach Section 4 applied easily. However, normally done SAT solvers.4 Successfulsolvers often implement search new watches non-optimal way. see this,4. grateful reviewer paper pointing fact me.242fiOptimal Implementation Watched Literalsmust examine code, necessary level implementation detail givenpapers. example MiniSat (Een & Sorensson, 2003) tinisat (Huang, 2007) implement watched literals following non-optimal way. core implementationshown Procedure 5: key change watched elements list watchedinstead one Procedure 4. details MiniSats implementationProcedure 5: important maintenance blocked literals,discuss below. Proposition 3 technically apply, nevertheless statewithout proof approach leads (N 2 ) calls acceptable per branchworst case.FNE-NoState-W(list,elt,i)// elt value list newly unacceptable// index elt watchedresult = FNE-NoState(unwatched)resultwatched[i] := unwatched[last]unwatched[last] := eltreturn resultProcedure 5:1:2:3:4:5:6:7:compare practical performance, adapted MiniSat version 2.2.0 optimal unitpropagation using Procedure 4. call Circular MiniSat original StockMiniSat. Appendix gives full methodology detailed results. key resultsfollows. First, Circular unit propagates notably faster Stock featuresMiniSat related unit propagation removed. searching 594 instances10 million conflicts, Circulars mean time 141.6s, compared mean 182.6s Stock,Stock takes 29.1% time. Median performance much closer, 73.1s Circularcompared 78.1s Stock. larger disparity mean Stock never15% faster, Circular much 9.5 times faster Stock. Second,features MiniSat restored, improved propagation speed Circulartranslate improved performance. 330 instances, statistical supportreject null hypothesis two solvers equivalent performance.interesting look many watched literal scans blocked, term usedMiniSats code. clause scan blocked watched literal (at time watchset up) valid literal now, clause satisfied new watch needed.saves accessing memory relating clause. instance measured ratiob/u blocked unblocked watched scans. higher b/u better since resultsless watched scans. computed ratio b/u values obtained CircularStock MiniSat, = (bc /uc )/(bs /us ). Figure 3 shows plotted obtained speedup.Behaviour different two regions. 1.2 (123 instances) median speedup1.48 mean 1.86. high correlation speedup, r2 = 0.88.< 1.2 (471 instances) get correlation speedup, r2 = 0.01. ComparedStock MiniSat median speedup 0.97 mean 0.98, i.e. slight slowdowns.analysis indicates speedup occurs instances Circular muchbetter Stock MiniSat setting watches literals likely valid latersearch thus block watched literal scans. would interesting investigateresult theoretically, since follow results paper. may related243fiGent1086543210.8Circular:Stock0.548x + 0.5230.60.50.5 0.60.811.22345681020Figure 3: Scatterplot (x-axis) speedup ratio conflicts per second CircularStock MiniSat (y-axis). vertical line shows = 1.2. line 0.548x +0.523 best-fit line region 1.2.fact Circular sets watches arbitrary literals clause, Stock MiniSattend set watches literals appearing early clause. Therefore good blockingliteral late clause, Circular chance watching it.Head-tail lists important advance implementation unit propagationSAT (Zhang & Stickel, 2000). Pointers first unassigned literal clause (head)last (tail) maintained state-restoration. Head-tail lists led watched literals(Moskewicz et al., 2001), two pointers arbitrary (but different) unassigned literals. Watched literals (or variants thereof) become standard techniqueefficient implementation unit propagation SAT solvers. implemented described paper, theoretical properties watched literals seennearly good head-tail lists time, much reduced space overheads.variant implementation watched literals JQuest (Lynce & Marques-Silva,2005). Scans go last end list, restart last backwards 0.middle-out search big-O optimal provided current direction searchpersistent calls. However, direction search always initially one direction, require (N 2 ) checks per branch, case JQuest. Full algorithmicdetails proofs statements Appendix B. Lynce Marques-Silva (2005)also introduced literal sifting attempt get best worlds: literals clausereordered search avoid repeating checks backtracking pointers. LynceMarques-Silva report slightly better empirical performance literal sifting (althoughnoted comparison non-optimal watched literal implementation). Generalising literal sifting arbitrary number acceptable elements performingtheoretical experimental comparisons circular approach open future work.244fiOptimal Implementation Watched LiteralsAnother application seen optimal, implemented circular style,Van Gelders (2002) three literal watching detect binary clauses.7. Conclusionsshown circular approach scanning lists backtracking search desirabletheoretical properties. big-O optimal time (measured number acceptabilitychecks) amortized across search tree. worst case constant factortwo. results average case apply every search tree numberbranches. results generalise maintaining multiple acceptable elements single list,complexity independent number elements required. result relevantpractically important algorithms applications SAT CSP. Techniques likewatched literals SAT known successful practice, certainly reducedspace overheads compared state restoration methods. implemented appropriately,newly understood essentially theoretical disadvantages timeeither. existing implementations seen optimal even thoughrealised implementers. implementations unit propagations real worldSAT solvers optimal, e.g. MiniSat. Replacing optimal implementationMiniSat improve propagation speed mean 29%. Experiments suggestedcircular approach better able find watched literals likely true future nodes.However, improved propagation speed result improved speed full solver.Acknowledgmentsthank Chris Jefferson Peter Nightingale help paper many ways,example C++ coding advice suggestions implement variants watchedliterals MiniSat. thank JAIR editor paper, Holger Hoos, anonymousreviewers suggestions leading comparison MiniSat study middleout approach, requiring much precise presentation results. thankauthors MiniSat, tinisat, JQuest making code available study.Appendix A. Experiments MiniSatappendix describes methodology gives detailed results experiments watchedliteral implementation MiniSat version 2.2.0. Two variants MiniSat implemented.first, Circular, implements two literal watching algorithm using approach describedSection 4. second, TwoPointer, variant two independent pointersmaintained, based method preprint paper. Since worse propertiespractice theory, TwoPointer described Appendix C.Timings reported performed single Apple MacPro (MacPro4,1),two Quad-Core Intel Xeon chips 2.26GHz, L2 Cache 256KB per core, L3 Cache 8MB perprocessor, 32GB DDR3 RAM 1066MHz, 7200 RPM hard drive, MacOS 10.6. MiniSat 2.2.0used codebase, compile time flags distribution. Instances245fiGent100Circular ratioTwoPointer ratio1010.10.010.010.11101001000Figure 4: Scatterplot relative performance Circular TwoPointer variants comparedStock MiniSat. x-axis gives run time Stock MiniSat seconds.y-axis gives ratio Stock time Circular/TwoPointer time. Ratios 1mean alternative faster, 1 Stock MiniSat faster.SAT 2005 competition (Le Berre & Simon, 2006) used.5 reasons choiceMiniSat well competition, unlikely straw man,provides large manageable set benchmarks: using entire availableset chance selection bias. significant advances SATsolvers since 2005, aware major changes propagation, focuscurrent paper. Code results available online, see Appendix D.first experiment, features MiniSat cut out, e.g. clause learning,conflict analysis, heuristics. eliminating aspects solver, variantsearches identical spaces differences speed must due differing speedspropagation. optimisations applied exploit cut-down solver,propagators tested used second experiment. However,since must propagated running full MiniSat, set learnt clausesincluded. this, standard MiniSat run instance 60s (on differentLinux machine). 594 instances unsolved 60s, clause set saved giverealistic static instance cut-down versions MiniSat. Two versionspropagator created: one instrumentation switched off, maximisespeed; one several additional counters added provide metricsnature search watched literals. reporting cpu times first versionused (with times median three runs). Search performed limit107 conflicts reached (excepting one instance solved 7.75 106 conflicts).tests unrestricted MiniSat, three runs performed algorithm-instancecombination. MiniSat default settings used cpu timeout 1200s memory5. http://www.lri.fr/~simon/contest/results/download/distrib-benchs-random-sat2005.tar.bz2,distrib-benchs-crafted-sat2005.tar.bz2, distrib-benchs-industrial-sat2005.tar.bz2.246fiOptimal Implementation Watched Literals1GB. 87 instances, taking 0.01s max-to-min deviationalgorithm 10% median, another 18 runs performed algorithm,median 21 runs used. Instances algorithm solved within timeout,less 0.01s, discarded. 330 instances remained. Results shownCircular TwoPointer Figure 4. huge variation runtimesinstance, almost 100 times. propagation method find differentconflicting clauses, leading different sets learnt clauses heuristics. instancesvertically diagonally x = 1200 one method timed-outnot. paired t-test performed Circular Stock MiniSat,null hypothesis distributions mean. gave = 0.127,p = 0.899, i.e. highly insignificant result. assumption normality invalid,t-test randomised 100,000 times (Cohen, 1995). these, 48.3% gave lower tvalue 51.7% higher value. Similar results obtained TwoPointer StockMiniSat. conclusion must statistical evidence either CircularTwoPointer either better worse Stock MiniSat fully featured solver.Appendix B. Middle-Out List Scanningappendix gives formal presentation algorithms proofs middle-out scanningwatched literals discussed Section 6.Procedure 5: FNE-MiddleOut-Helper(list,delta)Require: delta equals 1 +11: last-cache := last2: repeat3:last := last + delta4:acceptable(list,last) return true5: last = 0 last = N6: last:= last-cache7: return falseProcedure 6: FNE-MiddleOut(list)Require: delta persistent calls equals 1 +1, initialised either1: FNE-MiddleOut-Helper(delta)2:return true3: else4:delta := delta5:FNE-Helper(delta)6:return true7:else8:return falseFirst, note FNE-MiddleOut locally correct (Definition 6). Therefore Theorem 7, FNE-MiddleOut maintains Invariant 2 times. FNE-MiddleOut turnsoptimal big-O terms, follows analogue Theorem 10.247fiGentTheorem 22. downwards-explored search tree, total number calls acceptable made successful calls FNE-MiddleOut LBS 2N . (ProofAppendix E online.)result follow similar development circular, analogous results,omit results except important, state without proof.Theorem 23. (Optimality) downwards-explored search tree, Middle-Out approach requires space one last pointer worst case O(N ) calls acceptableper branch tree.persistence delta executions critical. add line 0 : delta = +1Procedure 6 give FNE-MiddleOut-Fixed, get following worse result.Proposition 24. downwards-explored search tree, total number calls acceptable made FNE-MiddleOut-Fixed (N 2 ) per branch search tree.(Proof Appendix E online.)solver JQuest Lynce Marques-Silva (2005) implements watched literalsstyle FNE-MiddleOut-Fixed, non-optimal. cannot deducedcited paper seen http://sat.inesc.pt/sat/soft/jquest/jquest-src.tgz file ClauseSCImplWL.java: flag controls direction move first in,swapped search first watch clause never second.Appendix C. Maintaining Multiple Pointers: Theory ExperimentCompared described Section 4, naive approach implementing multiplewatches separate last pointer one. unit propagate correctly SATtwo watched literals. Crucially, cannot allow two pointers settleelement. correct method achieve unit propagation follows. pointerbecomes unacceptable, store current value call FNE-Circular. failsclause entirely false. succeeds different value pointer nothing.succeeds value pointer call FNE-Circular again.second call fails must reset value first pointer stored valueunit propagate literal represented second pointer. prove optimalityapproach need general version Theorem 10.Theorem 25. Suppose W pointers last1 , last2 , . . . lastW list maintainedsimultaneously, definition acceptability, calls FNE pointerlasti made points unacceptable element value anotherpointer currently has. Then: cN calls acceptable made LBSdownwards-explored search tree, either least one calls FNE-Circular failsleast two pointers take value. (Proof Appendix E online.)Theorem 25 leads correctness unit propagation procedure described above.guarantees one satisfiable literal remains clause, pointerssettle unit propagation performed. space requirement O(1) perlast pointer. Detection unacceptability also done O(1) time maintaininglist occurrences literals, consulted literal set false. gives:248fiOptimal Implementation Watched LiteralsCorollary 26. Unit propagation using watched literals clause N literalsimplemented O(1) space using O(N ) time per branch search tree.1086543210.8TwoPointer:Stock0.437x + 0.5720.60.50.5 0.60.811.22345681020Figure 5: Scatterplot (x-axis) speedup ratio conflicts per second TwoPointer Stock MiniSat (y-axis). line 0.437x + 0.572 best-fit lineregion 1.2. vertical line shows = 1.2.development following Theorem 10 follows before. state without proof:Theorem 27. conditions Theorem 25, search tree containing k branches,calls FNE-Circular make k((c + 1)N 1) calls acceptable.TwoPointer unit propagates faster Stock. searching 594 instances cutdown MiniSat 10 million conflicts, TwoPointer took mean 155.9s 182.6sStock, Stock takes 17.1% time. 10% slower Circularsmean time 141.6s. Circular never 13% slower TwoPointer35% faster. mean median speedups Circular TwoPointer 1.10.median performance Stock slightly better TwoPointer (78.1s 81.1s)Stock never 22% faster TwoPointer much 7.7 times faster.see similar results effect blocked watches Circular. ResultsCircular definition given main paper Section 6. 1.2 (122instances), correlates strongly speedup conflicts per second, correlationcoefficient r2 = 0.86. Median speedup region 1.32 mean 1.65. best fitline shown Figure 5. < 1.2 (472 instances), correlationspeedup, r2 = 0.07. Median mean speedups 0.88 0.90 (so slowdownsspeedups.) regions, extremely high correlation TwoPointerCircular, r2 > 0.996. Circular, analysis indicates speedupoccurs instances TwoPointer much better Stock MiniSat setting watchesliterals likely valid later search thus block watched literal search.249fiGentResults TwoPointer full version MiniSat similar Circular.methodology described Appendix A, raw t-value 1.54, p = 0.124.Randomisation 100,000 times gave 26.2% lower t-values 73.8% higher values.Appendix D. Description Online AppendicesTwo Online Appendices available. first textual Appendix E proofs omittedmain text (Gent13a-appendix1.pdf).6 second contains results tables, fullMiniSat outputs, graphs used paper (Gent13a-appendix2.tgz).7 fuller versionappendix, including code variant MiniSat scripts run analyseexperiments, available separately.8 file 4MB unpacks 12MB.Separately, 2.4GB compressed tar file available containing clausesets written60s failed search.9ReferencesBessiere, C., Regin, J.-C., Yap, R., & Zhang, Y. (2005). optimal coarse-grained arcconsistency algorithm. Artificial Intelligence, 165, 165185.Bessiere, C. (2004). Personal communication Marc van Dongen.. Described (vanDongen, 2004).Bessiere, C., & Regin, J.-C. (2001). Refining basic constraint propagation algorithm.Nebel, B. (Ed.), Proceedings Seventeenth International Joint ConferenceArtificial Intelligence, IJCAI 2001, Seattle, Washington, USA, August 4-10, 2001, pp.309315. Morgan Kaufmann.Chai, D., & Kuehlmann, A. (2003). fast pseudo-boolean constraint solver. Proceedings40th Design Automation Conference, DAC 2003, Anaheim, CA, USA, June2-6, 2003, pp. 830835. ACM.Cohen, P. R. (1995). Empirical methods artificial intelligence. MIT Press.Een, N., & Sorensson, N. (2003). extensible SAT-solver. Giunchiglia, E., & Tacchella, A. (Eds.), SAT, Vol. 2919 Lecture Notes Computer Science, pp. 502518.Springer.Gent, I. P., Jefferson, C., & Miguel, I. (2006a). Minion: fast scalable constraint solver.Brewka, G., Coradeschi, S., Perini, A., & Traverso, P. (Eds.), ECAI, Vol. 141Frontiers Artificial Intelligence Applications, pp. 98102. IOS Press.Gent, I. P., Jefferson, C., & Miguel, I. (2006b). Watched literals constraint propagationMinion. Benhamou, F. (Ed.), CP, Vol. 4204 Lecture Notes ComputerScience, pp. 182197. Springer.Harvey, W. D., & Ginsberg, M. L. (1995). Limited discrepancy search. ProceedingsFourteenth International Joint Conference Artificial Intelligence, IJCAI 95,6.7.8.9.Also available http://ipg.host.cs.st-andrews.ac.uk/JAIR/Gent13a-appendix1.pdfAlso available http://ipg.host.cs.st-andrews.ac.uk/JAIR/Gent13a-appendix2.tgzhttp://ipg.host.cs.st-andrews.ac.uk/JAIR/Gent13a-appendix2-full.tgzhttp://ipg.host.cs.st-andrews.ac.uk/JAIR/writtenclausesets.tgz250fiOptimal Implementation Watched LiteralsMontreal Quebec, Canada, August 20-25 1995, 2 Volumes, Vol. 1, pp. 607615. MorganKaufmann.Huang, J. (2007). case simple SAT solvers. Bessiere, C. (Ed.), PrinciplesPractice Constraint Programming - CP 2007, 13th International Conference, CP2007, Providence, RI, USA, September 23-27, 2007, Proceedings, Vol. 4741 LectureNotes Computer Science, pp. 839846. Springer.Jefferson, C., Moore, N. C. A., Nightingale, P., & Petrie, K. E. (2010). Implementing logicalconnectives constraint programming. Artificial Intelligence, 174 (16-17), 14071429.Korf, R. E. (1985). Depth-first iterative-deepening: optimal admissible tree search.Artificial Intelligence, 27 (1), 97109.Le Berre, D., & Simon, L. (2006). Special volume SAT 2005 competitionsevaluations. JSAT, 2 (1-4).Likitvivatanavong, C., Zhang, Y., Bowen, J., & Freuder, E. C. (2005). Maintaining arc consistency using adaptive domain ordering. Kaelbling, L. P., & Saffiotti, A. (Eds.),IJCAI-05, Proceedings Nineteenth International Joint Conference Artificial Intelligence, Edinburgh, Scotland, UK, July 30-August 5, 2005, pp. 15271528.Professional Book Center.Likitvivatanavong, C., Zhang, Y., Shannon, S., Bowen, J., & Freuder, E. C. (2007). Arcconsistency search. Veloso, M. M. (Ed.), IJCAI 2007, Proceedings 20thInternational Joint Conference Artificial Intelligence, Hyderabad, India, January6-12, 2007, pp. 137142.Lynce, I., & Marques-Silva, J. P. (2005). Efficient data structures backtrack search SATsolvers. Ann. Math. Artif. Intell., 43 (1), 137152.Marques-Silva, J. P., Lynce, I., & Malik, S. (2009). Conflict-driven clause learning satsolvers. Biere, A., Heule, M., van Maaren, H., & Walsh, T. (Eds.), HandbookSatisfiability, Vol. 185 Frontiers Artificial Intelligence Applications, pp.131153. IOS Press.Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: engineering efficient SAT solver. Proceedings 38th annual Design AutomationConference, DAC 01, pp. 530535, New York, NY, USA. ACM.Nightingale, P., Gent, I. P., Jefferson, C., & Miguel, I. (2013). Short long supportsconstraint propagation. J. Artif. Intell. Res. (JAIR), 46, 145.Prosser, P. (1993). Hybrid algorithms constraint satisfaction problem. ComputationalIntelligence, 9(3), 268299.Prosser, P., & Unsworth, C. (2011). Limited discrepancy search revisited. J. Exp. Algorithmics, 16, 1.6:1.11.6:1.18.Regin, J.-C. (2005). MAC algorithms search without additional space cost.Proc. 11th Principles Practice Constraint Programming (CP 2005), pp. 520533.van Dongen, M. R. C. (2004). Saving support-checks always save time. Artif.Intell. Rev., 21 (3-4), 317334.251fiGentVan Gelder, A. (2002). Generalizations watched literals backtracking search.Seventh Intl Symposium AI Mathematics.Zhang, H., & Stickel, M. E. (2000). Implementing Davis-Putnam method. J. Autom.Reasoning, 24 (1/2), 277296.252fiJournal Artificial Intelligence Research 48 (2013) 513-582Submitted 12/12; published 11/13AI Methods Algorithmic Composition:Comprehensive SurveyJose David FernndezFrancisco Vicojosedavid@geb.uma.esfjv@geb.uma.esUniversidad de Mlaga, Calle Severo Ochoa, 4, 119Campanillas, Mlaga, 29590 SpainAbstractAlgorithmic composition partial total automation process music composition using computers. Since 1950s, different computational techniques relatedArtificial Intelligence used algorithmic composition, including grammaticalrepresentations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming evolutionary algorithms. survey aims comprehensiveaccount research algorithmic composition, presenting thorough view fieldresearchers Artificial Intelligence.1. IntroductionMany overly optimistic, ultimately unfulfilled predictions made early daysArtificial Intelligence, computers able pass Turing test seemed decadesaway. However, field Artificial Intelligence grown got matured, developingacademic research reaching many industrial applications. time, keyprojects challenges captivated public attention, driverless cars, naturallanguage speech processing, computer players board games.introduction formal methods instrumental consolidation manyareas Artificial Intelligence. However, presents disadvantage areas whose subjectmatter difficult define formal terms, naturally tend become marginalized.case Computational Creativity (also known Artificial Creativity),loosely defined computational analysis and/or synthesis works art,partially fully automated way. Compounding problem marginalization, twocommunities naturally interested field (AI arts) speak different languages(sometimes different!) different methods goals1 , creating great difficulties collaboration exchange ideas them. spite this, smallsometimes fragmented communities active research different aspectsComputational Creativity.purpose survey review bring together existing research specificstyle Computational Creativity: algorithmic composition. Interpreted literally, algorithmic composition self-explanatory term: use algorithms compose music.broad definition, centuries musicians proposing methodsconsidered algorithmic sense, even human creativity plays key1. Related problem, uncommon engineering concepts become bent strange waysinterpreted artists. See Footnote 28 page 550 particularly remarkable example.c2013AI Access Foundation. rights reserved.fiFernndez & Vicorole. commonly cited examples include dArezzos Micrologus, species counterpoint,Mozarts dice games, Schoenbergs twelve-tone technique, Cages aleatoric music. Readers interested pre-computer examples algorithmic compositionreferred introductory chapters almost thesis book subject,Daz-Jerezs (2000), Aschauers (2008) Nierhauss (2009). survey, useterm algorithmic composition restricted way, partial total automationmusic composition formal, computational means. course, pre-computer examplesalgorithmic composition implemented computer, approachesreviewed survey implement classical methodology. general, focusAI techniques, self-similarity cellular automata also reviewed moderncomputational techniques used generating music material without creativehuman input.1.1 Motivationuseful starting points researching past present computer musicComputer Music Journal, International Computer Music Conference 2 annually organized International Computer Music Association 3 , books MachineModels Music (Schwanauer & Levitt, 1993), Understanding music AI (Balabanet al., 1992), Music Connectionism (Todd & Loy, 1991), anthologies selectedarticles Computer Music Journal (Roads & Strawn, 1985; Roads, 1992). However,resources algorithmic composition, computer music general.specific information algorithmic composition, surveys better option.many surveys reviewing work algorithmic composition. reviewanalysis composition computer AI methods (Roads, 1985), others discussalgorithmic composition point view related music theory artistic considerations (Collins, 2009), personal perspective composer (Langston, 1989;Dobrian, 1993; Pope, 1995; Maurer, 1999). provide depth comprehensive view specific technique algorithmic composition, Anders Miranda(2011) constraint programming, Ames (1989) Markov chains, Santoset al. (2000) evolutionary techniques, others specialized comparison paradigms computational research music, Toiviainen (2000). Othersoffer wide-angle (but relatively shallow) panoramic field (Papadopoulos & Wiggins,1999), review early history field (Loy & Abbott, 1985; Ames, 1987; Burns, 1994),analyze methodologies motivations algorithmic composition (Pearce et al., 2002).also works combine depth comprehensive reviews wide rangemethods algorithmic composition, Nierhauss (2009) book.context, natural question arises: yet another survey? answerexisting survey article fulfills following criteria: (a) cover methodscomprehensive way, point view primarily focused AI research,(b) centered algorithmic composition.4 Nierhauss (2009) book algorithmic2. archives available http://quod.lib.umich.edu/i/icmc/3. http://www.computermusic.org/4. Many surveys conflate discussion algorithmic composition (synthesis music) computational analysis music, work Roads (1985), Nettheim (1997) Toiviainen (2000).become somewhat distracting reader interested algorithmic composition.514fiAI Methods Algorithmic Compositioncomposition comes close fulfilling criteria long, detailed expositionsmethod comprehensive reviews state art. contrast, surveyintended reasonably short article, without lengthy descriptions: referenceguide AI researchers. aims mind, survey primarily structuredaround methods used implement algorithmic composition systems, though earlysystems also reviewed separately.second, practical motivation accessibility. Since Computational Creativitybalances edge AI arts, relevant literature scattered acrossmany different journals scholarly books, broad spectrum topics computerscience music theory. unfortunate consequence, many differentpaywalls researchers relevant content, translating sometimes lot hassle,partially mitigated relatively recent trends like self-archiving. survey bringstogether substantial body research algorithmic composition, intentionconveying effectively AI researchers.2. Introducing Algorithmic CompositionTraditionally, composing music involved series activities, definitionmelody rhythm, harmonization, writing counterpoint voice-leading, arrangementorchestration, engraving (notation). Obviously, list intended exhaustivereadily applicable every form music, reasonable starting point, especiallyclassical music. activities automated computer varying degrees,techniques languages suitable others (Loy &Abbott, 1985; Pope, 1993).relatively small degrees automation, focus languages, frameworksgraphical tools provide support specific and/or monotone tasks composition process, provide raw material composers, order bootstrap compositionprocess, source inspiration. commonly known computer-aided algorithmic composition (CAAC), constitutes active area research commercialsoftware development: many software packages programming environmentsadapted purpose, SuperCollider (McCartney, 2002), Csound (Boulanger,2000), MAX/MSP (Puckette, 2002), Kyma (Scaletti, 2002), Nyquist (Simoni & Dannenberg, 2013) AC Toolbox (Berg, 2011). development experimental CAACsystems IRCAM5 (such PatchWork, OpenMusic various extensions)also emphasized (Assayag et al., 1999). Arizas comprehensive repository software tools research resources algorithmic composition6 constitutes good startingpoint (Ariza, 2005a) explore ecosystem, well algorithmic composition general.Earlier surveys (such Pennycook, 1985 Pope, 1986) also useful understandingevolution field, especially evolution graphical tools aid composers.survey, hand, concerned algorithmic compositionhigher degrees automation compositional activities, rather typical CAAC.words, focus techniques, languages tools computationally encode humanmusical creativity automatically carry creative compositional tasks minimal5. http://www.ircam.fr/6. http://www.flexatone.net/algoNet/515fiFernndez & Vicohuman intervention, instead languages tools whose primary aim aid humancomposers creative processes.Obviously, divide ends spectrum automation (CAAC representing low degree automation, algorithmic composition high degree automation)clear, method automates generation creative works usedtool aid composers, systems higher degrees automation custombuilt top many CAAC frameworks.7 Furthermore, human composer naturallyinclude computer languages tools integral part composition process,Brian Enos concept generative music (Eno, 1996). conclude considerations,survey computer systems automating compositional tasks userexpected main source creativity (at most, user expected setparameters creative process, encode knowledge compose, provide examples music composed humans processed computer). alsoincludes real-time automatic systems music improvisation, jazz performance,experimental musical instruments automate certain extent improvisationmusic.Finally, considerations, describe survey about:Although music defined organized sound, composition written traditional staff notation fully specify music actually sounds:piece music performed, musicians add patterns small deviations nuancespitch, timing musical parameters. patterns account musicalconcept expressiveness gesture, necessary music soundnatural. problem automatically generating expressive music importantitself, involves creativity, clearly within boundaries algorithmiccomposition reviewed survey. reader referred Kirke Mirandas(2009) review area information.computational synthesis musical sounds, algorithmic sound synthesis,understood logical extension algorithmic composition small timescales;involves use languages tools specifying synthesizing sound waveforms,rather abstract specification music associated traditional staffnotation. line algorithmic composition algorithmic sound synthesisblurred previously mentioned CAAC systems, surveyconcerned sound synthesis; interested readers may refer Roadss (2004) booksubject.computer games (and interactive settings), music frequently requiredgracefully adapt state game, according rules. kindmusic commonly referred non-linear music (Buttram, 2003) proceduralaudio (Farnell, 2007). Composing non-linear music presents challenges own,specifically related problem algorithmic composition, reviewliterature kind music.7. case many systems algorithmic composition described here. example,PWConstraints (described Section 3.2.3) built top PatchWork, described Assayag et al.(1999).516fiAI Methods Algorithmic Compositionthree scenarios (automated expressiveness, algorithmic sound synthesis nonlinear music) sparingly mentioned survey, mentioned innovative(or otherwise notable) techniques involved.2.1 Early Yearssection, review early research published algorithmic compositioncomputers, clear computational approach. references mightdiscussed methodology following sections, useful group togetherhere, since difficult find survey discussing them.earliest use computers compose music dates back mid-1950s, roughlytime concept Artificial Intelligence coined DarmouthConference, though two fields converge time later. Computersexpensive slow, also difficult use, operated batch mode.One commonly cited examples Hiller Isaacsons (1958) Illiac Suite,composition generated using rule systems Markov chains, late 1956.designed series experiments formal music composition. followingdecade, Hillers work inspired colleagues university experimentalgorithmic composition, using library computer subroutines algorithmic composition written Baker (also collaborator Hiller), MUSICOMP (Ames, 1987).library provided standard implementation various methods used Hillerothers.Iannis Xenakis, renowned avant-garde composer, profusely used stochastic algorithmsgenerate raw material compositions, using computers since early 1960sautomate methods (Ames, 1987). Though work better described CAAC,still deserves mentioned pioneer. Koenig, well knownXenakis, also composer 1964 implemented algorithm (PROJECT1) usingserial composition (a musical theory) techniques (as Markov chains) automategeneration music (Ames, 1987).However, also several early examples algorithmic composition, thoughprofusely cited Hiller Xenakiss. Push Button Bertha, composed 1956(Ames, 1987) around time Hillers Illiac Suite, perhaps thirdcited example: song whose music algorithmically composed publicity stuntBurroughs (an early computer company), generating music similar previously analyzedcorpus. However, least one earlier, unpublished work Caplin Prinz 1955(Ariza, 2011), used two approaches: implementation Mozarts dice damegenerator melodic lines using stochastic transitional probabilities various aspectscomposition. Another commonly cited example Brooks et al. (1957) exploredpotential Markoff 8 chain method.Several early examples also notable. Olsons (1961) dedicated computerable compose new melodies related previously fed ones, using Markov processes.work submitted publication 1960, claimed built machineearly 1950s. Also interest Gills (1963) algorithm, implemented request8. Markov Markoff alternative transliterations Russian surname . spellingMarkov prevalent decades, many older papers used Markoff.517fiFernndez & VicoBBC, represents hallmark application classical AI techniques algorithmiccomposition: used hierarchical search backtracking guide compositional process inspired Schoenbergs twelve-tone technique. Finally, worth mentioningmay represent first dissertation algorithmic composition: Padbergs (1964) Ph.D.thesis implemented compositional framework (based formal music theory) computercode. work unusual that, instead using random number generators, usedraw text input drive procedural techniques order generate parameterscomposition system.Non-scholarly early examples also exist, though difficult assesssparsity published material, fact mostly peer-reviewed.example, Pinkerton (1956) described Scientific American Banal Tune-Maker,simple Markov chain created several tens nursery tunes, Sowa (1956) usedGENIAC machine9 implement idea (Cohen, 1962), Raymond Kurzweilimplemented 1965 (Rennie, 2010) custom-made device generated music styleclassical composers. Another example, unfortunately shrouded mystery, RaymondScotts Electronium (Chusid, 1999), electronic device whose development spannedseveral decades, reportedly able generate abstract compositions. Unfortunately, Scottnever published otherwise explained work.machines became less expensive, powerful cases interactive, algorithmic composition slowly took off. However, aside researchers Urbana (Hillersuniversity), little continuity research, reinventing wheel algorithmiccomposition techniques common. problem compounded fact initiatives algorithmic composition often came artists, tended develop ad hocsolutions, communication computer scientists difficult many cases.3. Methodsrange methodological approaches used implement algorithmic compositionnotably wide, encompassing many, different methods Artificial Intelligence,also borrowing mathematical models Complex Systems even Artificial Life.survey structured methodology, devoting subsection one:3.1Grammars3.2Symbolic, Knowledge-Based Systems3.3Markov Chains3.4Artificial Neural Networks3.5Evolutionary Population-Based Methods3.6Self-Similarity Cellular AutomataFigure 1 summarizes taxonomy methods reviewed survey. Together,Sections 3.1 3.2 describe work using symbolic techniques characterizedclassical good old-fashioned AI. Although grammars (Section 3.1) symbolic9. GENIAC Electric Brain, electric-mechanic machine promoted educational toy. Despitemarketed computer device, computing performed human operator.518fiAI Methods Algorithmic CompositionArtificial intelligenceSymbolic AIOptimization(Knowledge-based, Rule-based)Sections 3.1, 3.2Computational methodsautomatic generationmusic material(not based modelshuman creativity)Population-based methodsGrammarsRule learningSection 3.1Section 3.2.1Evolutionary algorithmsL-systemsSections 3.1.2, 3.2.2, 3.4.1, 3.5Section 3.1.1ConstraintsatisfactionRelatedmethodsAutomaticInteractiveSection 3.5.1Section 3.5.2Section 3.2.3Section 3.1.3Complex systemsCase-basedreasoningConcurrencymodelsSection 3.2.4Section 3.2.5population-based methodsSection 3.5.3Self-similaritySection 3.6Machine learningMarkov chainsRelated statistical methodsSection 3.3Cellular automataArtificial neural networksSection 3.6.1Section 3.4Figure 1: Taxonomy methods reviewed surveyknowledge-based, thus included part Section 3.2, segregated separate subsection relative historical importance algorithmiccomposition. Sections 3.3 3.4 describe work using various methodologies machinelearning, Section 3.5 evolutionary algorithms populationbased optimization methods. Although methodologies described Section 3.6really form Artificial Intelligence, included importance algorithmic composition automatic sources music material (i.e.,depend model human creativity generating music material).attempts systematize algorithmic composition,taxonomies Papadopoulos Wiggins (1999) Nierhaus (2009). taxonomyroughly similar Nierhauss, differences, including L-systemsgrammars instead self-similar systems. reader may surprised find manymethods machine learning optimization missing taxonomy.several reasons this. cases, methods subsumed others. example,machine learning, many different methods formulated mathematicalframework artificial neural networks. cases, method used rarely,almost always together methods. example, optimization, casetabu search, used times context constraint satisfactionproblems (Section 3.2.3), simulated annealing, occasionally combinedconstraint satisfaction, Markov processes artificial neural networks.difficult neatly categorize existing literature algorithmic compositionhierarchical taxonomy, methods frequently hybridized, giving risemany possible combinations. specially true evolutionary methods,519fiFernndez & Vicocombined almost every method. Additionally, papers considered belong different methodologies, depending selected theoretical framework10 , others unique approaches11 , complicating issue. Finally, lines methods (as rule systems, grammars Markov chains)frequently blurred: cases, ascribing work one becomes, end,largely arbitrary exercise depending terminology, intentions domainresearchers. method presented separately (but also presenting existing hybridizations methods), describing state art mostly chronologicalorder method.Although classification fully comprehensive, found one (arguablyremote) example using method related ones listed above: Amiot et al.(2006), applied Discrete Fourier Transform (DFT) generate variations musicalrhythms. Given rhythm sequence numerical symbols, representedfrequency domain computing DFT. Variations rhythm generatedslightly perturbing coefficients transform converting back time domain.3.1 Grammars Related Methodsbroad terms, formal grammar may defined set rules expand high-levelsymbols detailed sequences symbols (words) representing elements formallanguages. Words generated repeatedly applying rewriting rules, sequenceso-called derivation steps. way, grammars suited represent systems hierarchical structure, reflected recursive application rules. hierarchicalstructures recognized styles music, hardly surprising formalgrammar theory applied analyze compose music long time12 , despite recurring concerns grammars fail capture internal coherency subtletiesrequired music composition (Moorer, 1972).compose music using formal grammars, important step define setrules grammar, drive generative process. rules traditionallymulti-layered, defining several subsets (maybe even separated distinct grammars) rulesdifferent phases composition process: general themes composition,arrangement individual notes. early authors derived rules handprinciples grounded music theory, methods possible, like examining corpus pre-existing musical compositions distill grammar able generate compositionsgeneral style corpus, using evolutionary algorithms. Another importantaspect mapping formal grammar musical objects generates, usually relates symbols derived sequences elements musiccomposition, notes, chords melodic lines. However, mappings possible,using derivation tree define different aspects musical composition. Anotherimportant aspect automatic composition process election grammatical10. example, Markov chains formulated stochastic grammars; self-similar systemscharacterized L-system grammars; rule learning case-based reasoning also machine learningmethods; etc.11. example, Kohonens method (Kohonen et al., 1991), neither grammatical neuralMarkovian, framed either way, according creator.12. See, e.g., survey Roads (1979).520fiAI Methods Algorithmic CompositionReferenceComposition taskCommentsLidov & Gabura, 1973melodyearly proposalRader, 1974melodyearly proposal,detailed grammarUlrich, 1977jazz chord identificationintegrated ad hoc system(to produce jazz improvisations)Baroni & Jacoboni, 1978grammar Bach choralesearly proposalLeach & Fitch, 1995(XComposer)structure, rhythm melodyuses chaotic non-linear systems(self-similarity)Hamanaka et al., 2008generate variations two melodies(by altering derivation tree)inspired Lerdahl et al.s (1983)GTTMRoads, 1977structure, rhythm melodygrammar compilerHoltzman, 1981structure, rhythm melodygrammar compilerJones, 1980structurespace grammars(uses derivation tree)Bel, 1992(Bol Processor)improvisation tabla rhythmstool field researchKippen & Bel, 1989improvisation tabla rhythmsgrammatical inferenceCruz-Alczar & Vidal-Ruiz,1998melodygrammatical inferenceGillick et al., 2009jazz improvisationgrammatical inference.Implemented extensionKeller Morrisons (2007)ImprovGeneratorKitani & Koike, 2010(ImprovGenerator)real-time drum rhythm improvisationonline grammatical inferenceKeller & Morrison, 2007(Impro-Visor)jazz improvisationsophisticated GUI interfaceQuick, 2010classical three-voice counterpointintegrated SchenkerianframeworkChemillier, 2004jazz chord sequencesimplemented OpenMusicMAXTable 1: References Section 3.1 (algorithmic composition grammars), orderappearance.rules applied. many approaches possible, use activation probabilitiesrules (stochastic grammars) common. process compiling informationsurvey, noted almost research done regularcontext-free grammars, context-sensitive general grammars seemdifficult implement effectively, except simple toy systems.Lidov Gabura (1973) implemented early example formal grammar compose simple rhythms. Another early example implemented Rader (1974): definedgrammar hand rather simple music concepts, enriching rules grammaractivation probabilities. early examples used grammars driven rulesmusic theories, either small part synthesis engine, Ulrichs (1977) grammar521fiFernndez & Vicoenumerating jazz chords, inferring rules classical works, BaroniJacobonis (1978) grammar generate melodies. Generative Theory Tonal Music(Lerdahl et al., 1983), book presenting grammatical analysis tonal music, relatively early theoretical work said influenced use grammarsalgorithmic composition, though directly concerned algorithmic composition,grammatical approach analysis music. book widely popular, lasting impact field high citation rates. Examples laterwork inspired book include Popes (1991) T-R Trees, Leach Fitchs (1995)event trees, Hamanaka et al.s (2008) melody morphing.1980s, proposed approaches line computer science, abstractingprocess generate grammars instead codifying hand, though costproducing less interesting compositions. Roads (1977) proposed framework define,process use grammars compose music, Holtzman (1981) described languagedefine music grammars automatically compose music them. Meanwhile, Jones(1980) proposed concept space grammars, conjunction novel mappingtechnique: instead using terminal symbols building blocks composition,used derivation tree terminal sequence define characteristicscomposition. approach unfortunately developed far enough yield significantresults. spite early efforts, research grammatical representationsmusic focused analysis rather synthesis. instances, Steedmans(1984) influential grammar analysis jazz chord progressions, later adaptedsynthesis (see below).problem grammatical approach algorithmic composition difficultymanually define set grammatical rules produce good compositions. problemsolved generating rules grammar (and way applied) automatically. example, although Bel (1992) implemented BOL processor facilitatecreation hand less sophisticated music grammars13 , also exploredautomated inference regular grammars (Kippen & Bel, 1989). Later, Cruz-AlczarVidal-Ruiz (1998) implemented several methods grammatical inference: analyze corpus pre-existing classical music compositions, represented suitable set symbols,inducing stochastic regular grammars (Markov chains) able parse compositionscorpus, finally applying grammars generate new compositionssimilar style compositions corpus. Gillick et al. (2009) used similarapproach (also Markovian) synthesize jazz solos, elaborated synthesisphase. Kitani Koike (2010) provide another example grammatical inference,case used real-time improvised accompaniment.However, others still designed grammars hand, carefully choosing mapping terminal symbols musical objects, Keller Morrison (2007)jazz improvisations. Another approach take pre-existing music theorystrong hierarchical methodology, designing grammar inspired Schenkerian analysis(Quick, 2010), using Lerdhals grammatical analysis derive new compositions twopreviously existing ones altering derivation tree (Hamanaka et al., 2008), even de13. Initially represent analyze informal knowledge Indian tabla drumming, later alsorepresent music styles.522fiAI Methods Algorithmic CompositionReferenceComposition taskCommentsPrusinkiewicz, 1986melodymapping turtle graphics music scoresNelson, 1996melodymapping turtle graphics music scoresMason & Saffle, 1994melody (counterpoint suggested)mapping turtle graphics music scoresSoddell & Soddell, 2000aural representationsbiological dataL-system modulates pitch intervalsMorgan, 2007composition largeinstrumental ensemblead hoc symbolic mappingLangston, 1989melodyL-system interpreted arrangepre-specified fragmentsWorth & Stepney, 2005melodyseveral mappings L-system typesManousakis, 2006melody (sound synthesis)complex, multi-dimensional mapping.Implemented MAX.McCormack, 1996melody, polyphoniescontex-sensitive L-systemsDuBois, 2003real-time accompanimentimplemented MAXWilson, 2009melodymapping turtle graphics music scoresMcGuire, 2006arpeggiatorsimple symbolic mappingWatson, 2008base chord progressionL-systems used contextlarger, multi-stage systemGogins, 2006voice leadingMusical theory (pitch spaces).Implemented CsoundBulley & Jones, 2011arpeggiatorpart real-time art installation.Implemented MAXPestana, 2012real-time accompanimentimplemented MAXTable 2: References Section 3.1.1, order appearance.veloping jazz on-the-fly improviser (Chemillier, 2004) adapting Steedmans grammar,previously implemented analysis purposes.3.1.1 L-SystemsLindenmayer Systems, commonly abbreviated L-systems, specific variant formalgrammar, whose distinctive feature parallel rewriting, i.e., derivation step,one possible rewriting rules applied once. successfullyapplied different scenarios, specially model microbial, fungi plant growthshapes, particularly well-suited represent hierarchical self-similaritycharacteristic organisms. ability represent self-similar structures, togetherfact L-systems easier understand apply traditional formalgrammars, made L-systems fairly popular algorithmic composition.Arguably, visually stunning way use L-systems synthesis2D 3D renderings plants, using mapping sequences symbols graphicsbased turtle graphics (Prusinkiewicz & Lindenmayer, 1990). naturalfirst application L-systems algorithmic composition used turtle graphics renderimage interpreted musical score (Prusinkiewicz, 1986), mapping523fiFernndez & Vicocoordinates, angles edge lengths musical objects. approach usedmusic composers, Nelsons (1996) Summer Song Mason Saffles (1994) ideausing different rotations stretchings image implement counterpoint.funny side note, Soddell Soddell (2000) generated aural renditions biologicalL-system models, explore new ways understand them. Additionally, composersused new approaches dependent upon graphical interpretation L-systems,Morgans (2007) symbolic mapping. One popular pre-generate collectionshort fragments and/or musical objects, define algorithm interpretfinal sequence symbols instructions transform arrange fragmentscomposition. approach used Langston (1989) Kyburz (Supper, 2001),Edwards (2011) used convoluted ultimately similar mapping.However, two approaches (the graphics-to-music pre-generated sequences)scratch surface technical possibilities generate music L-systems; manymappings possible (Worth & Stepney, 2005). cases, mappingsbecome exceedingly complex, implementation Manousakis (2006), whoseL-systems drove multidimensional automata whose trajectory interpretedmusic. composers researchers experimented context-free L-systems,McCormack (1996, 2003a) used context-sensitive, parametric L-systems increaseexpressiveness compositions enable implementation polyphony. alsoused rich comprehensive mapping symbol sequence musical score,interpreting symbols sequence instructions modulate parametersautomata driving MIDI synthesizer, though grammars ultimately specifiedhand. DuBois (2003) used simpler also rich approach, mapping symbolselemental musical objects (as notes instruments) simple transformations appliedthem, using brackets encode polyphony. also used L-systems drive real-timesynthetic accompaniment, extracting features audio signal performer (aspitch loudness notes), encoding symbols expanded L-systemrules, using resulting symbol sequences drive MIDI synthesizers. spitedevelopments, new mappings based images rendered turtle method stillinvestigated (Wilson, 2009).L-systems also used implement tools assist compositional processsolving part it, generating complex arpeggios off-the-shelf arpeggiators(McGuire, 2006), providing base chord progression composition (Watson,2008), sometimes applying elements music theory implement rules (Gogins, 2006).Another area research implementation real-time improvisers, either limitedparts composition process (Bulley & Jones, 2011), accompaniment (Pestana,2012).3.1.2 Grammars Evolutionary AlgorithmsEvolutionary methods also used together grammars. case, commonapproach evolve grammatical rules, GeNotator (Thywissen, 1999),genomes grammars specified GUI fitness function interactive(the user assigns fitness grammars). exotic example Khalifa et al.524fiAI Methods Algorithmic CompositionReferenceComposition taskCommentsThywissen, 1999(GeNotator)structuregrammar genotypeinteractive evolutionary algorithmKhalifa et al., 2007melodygrammar part fitness functionOrtega et al., 2002melodygrammatical evolutionReddin et al., 2009melodygrammatical evolutionShao et al., 2010(Jive)melodyinteractive grammatical evolutionBryden, 2006melodyinteractive evolutionary algorithm L-systemsFox, 2006melodyinteractive evolutionary algorithm L-systemsPeck, 2011melodyevolutionary algorithm L-systemsDalhoum et al., 2008melodygrammatical evolution L-systemsTable 3: References Section 3.1.2, order appearance.(2007) uses grammar part fitness function instead generationcompositions.evolutionary methods specifically adapted handle grammars.case grammatical evolution, method genomes sequences numberssymbols controlling application rules pre-defined (and possibly stochastic)grammar. common approach represent music outputgrammar, range general specific given music style. Severalinstances method developed: early, bare-bones implementation(Ortega et al., 2002) elaborated one using simple fitness function based general concepts music theory (Reddin et al., 2009). However, approaches,system implemented Shao et al. (2010), whose grammar used produceintermediate code, used generate music.general case formal grammars, evolutionary algorithmsused create L-systems. However, examples use interactive fitness function (thefitness assigned human), like basic implementation Bryden (2006)approach based genetic programming used Fox (2006). Others use simplisticfitness functions, modest results (Peck, 2011). sophisticated approachused Dalhoum et al. (2008), using grammatical evolution fitness function baseddistance metric synthesized compositions pre-specified corpus compositions.3.1.3 Related MethodsFinally, subsection presents examples exactly use grammars,utilize similar borderline approaches.first one application Kohonens Dynamically Expanding Context (DEC)method algorithmic composition (Kohonen et al., 1991). DEC, set music examplesfed algorithm, infers model structure examples mayconstrued stochastic context-sensitive grammar. model parsimoniouspossible, is, rules little contextual information possible. Then,inferred grammar used generate new compositions. Drewes Hgbergs (2007)525fiFernndez & VicoReferenceComposition taskCommentsKohonen et al., 1991melodyUses Kohonens Dynamically Expanding ContextDrewes & Hgberg, 2007generate variationsmelodyapplies tree-based algebraic transformationsCope, 1992 (EMI),2000 (SARA, ALICE),2005 (Emily Howell)melodyEMI uses Augmented Transition NetworksTable 4: References Section 3.1.3, order appearance.work also borderline, using regular tree grammars generate basic scaffoldmodified algebraic operations generate final music composition.famous example category Copes (1992) Experiments MusicalIntelligence (EMI), software application able analyze set musical compositionsspecific style (for example, Bachs) derive Augmented Transition Network (ATN),i.e., finite state automaton able parse relatively complex languages. EMI appliespattern-matching algorithms extract signatures short musical sequences characteristicstyle set examples analyzed, determining usesignatures compositions style. analysis, synthesis phase generatesnew music compositions comply specifications encoded inferred ATN,quite impressive results. iterated EMIs design applications, like SARAALICE (Cope, 2000), ultimately tried new approach yet another application,Emily Howell. Cope (2005) reported Emily Howell developed unique styleprocess trial error guided human input; however, researchers (Wiggins,2008) disputed validity methodology.3.2 Symbolic, Knowledge-Based Systems Related MethodsHere, knowledge-based system used umbrella term encompassing various rule-basedsystems several different paradigms, common denominator representingknowledge less structured symbols. Since knowledge musical compositiontraditionally structured sets less formalized rules manipulatingmusical symbols (Anders & Miranda, 2011), knowledge-based rule systems comenatural way implement algorithmic composition. fact, extremely commonalgorithmic composition systems include kind composition rules pointworkflow. known early work algorithmic composition example:classical rules counterpoint used generation first second movementsIlliac Suite (Hiller & Isaacson, 1958). this, subsection mostlyconfined description systems strong foundations AI (as expert systems),sidestepping certain degree works composers difficult categorize,ad hoc nature approaches different language use.Starting exposition early work, Gills (1963) paper, already cited Section 2.1, presented first application classical AI heuristics algorithmic composition: used hierarchical search backtracking guide set compositional rulesSchoenbergs twelve-tone technique. Another notable example Rothgebs (1968)Ph.D. thesis: encoded SNOBOL set rules extracted eighteenth century526fiAI Methods Algorithmic CompositionReferenceComposition taskCommentsGill, 1963Schoenbergs twelve-tonetechniquehierarchical search backtrackingRothgeb, 1968unfigured bassimplemented SNOBOLThomas, 1985(Vivace)four-part harmonizationimplemented LISPThomas et al., 1989(Cantabile)Indian raga styleimplemented LISPSteels, 1986four-part harmonizationuses Minskys framesRiecken, 1998(Wolfgang)melodyuses Minskys SOMHorowitz, 1995jazz improvisationuses Minskys SOMFry, 1984(Flavors Band)jazz improvisationstylesphrase processing networks (networksagents encoding musical knowledge)Gjerdingen, 1988(Praeneste)species counterpointimplements theory composers workSchottstaedt, 1989species counterpointconstraint-based search backtrackingLthe, 1999piano minuetsset rules extracted classical textbookUlrich, 1977jazz improvisationalso uses grammar (for jazz chords)Levitt, 1981jazz improvisationCriticized Horowitz (1995)overly primitiveHirata & Aoyagi, 1988jazz improvisationuses logic programmingRowe, 1992(Cypher)interactive jazzimprovisationuses Minskys SOMWalker, 1994(ImprovisationBuilder)interactive jazzimprovisationimplemented SmallTalkAmes & Domino, 1992(Cybernetic Composer)jazz, rockalso uses Markov chains rhythmTable 5: References Section 3.2, order appearance.music treatises harmonize unfigured bass, say, determine adequate chordssequence bass notes.14 discovered classical rules incompleteincoherent certain extent.recurring problems many others implementing rules compositionstraight musical theory. example, Thomas (1985) designed rule-based systemfour-part chorale harmonization implemented Lisp15 , intent clarifyingmusical rules taught students. Later, designed another rule system (Thomaset al., 1989) simple melody generation Indian raga style. Another exampleharmonization use Minskys paradigm frames one students encodeset constraints solve relatively simple problem tonal harmony, finding passing chord two others (Steels, 1979), later tackle problem four-partharmonization (Steels, 1986). Minsky developed paradigms, K-lines14. noted stems practice completely specifying harmonization,problem performers expected solve improvisation.15. systems discussed paragraph implemented Lisp.527fiFernndez & VicoReferenceComposition taskCommentsSchwanauer, 1993(MUSE)four-partharmonizationpresents learning techniques similar wayRoads (1985, sect. 8.2)Widmer, 1992harmonizationbased user evaluations training corpusSpangler, 1999real-time four-partharmonizationprioritizes harmonic errors severity,order refine resultsMorales & Morales, 1995species counterpointuses logic programmingTable 6: References Section 3.2.1, order appearance.Society Mind (SOM), also influenced work algorithmic composition twostudents, Riecken Horowitz. Riecken (1998) used system composed monophonic melodies according user-specified emotional criteria, Horowitz(1995) used system improvised jazz solos. Frys (1984) phrase processingnetworks, directly based SOM, specialized procedural representationsnetworks agents implementing musical transformations encode knowledge jazzimprovisation styles.researchers also explored different ways generate species counterpointrule-based systems: Gjerdingen (1988) implemented system based useseveral pre-specified musical schemata, implementing theory composers work,Schottstaedt (1989) used formal approach: constraint-based searchbacktracking. followed classical rulebook species counterpoint, pointbending rules creating new ones order get close possible scoresserving examples book. Also formal side, Lthe (1999) extracted setrules classical textbook composing minuets.music styles demanded different approaches: jazz performances improvisations existing melodies, knowledge-based systems jazz structuredless sophisticated analysis-synthesis engines. example, work Ulrich (1977):system analyzed melody fitted harmonic structure. Another student Minsky(Levitt, 1981) implemented rule-based jazz improviser formulating rulesconstraints, Hirata Aoyagi (1988) encoded rules logic programming, tryingdesign flexible system. Rowe (1992) used SOM architecture16 Cypher,analysis-synthesis engine able play jazz interactively human performer, notableflexibility musical knowledge encoded it. Also, Walker (1994) implementedobject-oriented analysis-synthesis engine able play jazz interactively humanperformer, Ames Domino (1992) implemented hybrid system (using rulesMarkov chains) generation music several popular genres.3.2.1 Rule Learningknowledge implemented rule-based systems usually static, part knowledge may dynamically changed learned. natural term concept machinelearning, meaning unfortunately vague, used catch-all manymethods, including neural networks Markov chains.16. student Minsky, though.528fiAI Methods Algorithmic Compositionexamples rule-based learning systems developed. example,Schwanauer (1993) implemented MUSE, rule-based system solving several tasks fourpart harmonization. core ruleset static, series constraints directivescomposition process also built system, application alsoused dynamically change rule priorities. Additionally, system successfullysolved task, able deduce new composite rules extracting patterns ruleapplication. Widmer (1992) implemented another example: system harmonizationsimple melodies. based user evaluations training corpus: hierarchicalanalysis training melodies evaluations, extracted rules harmonization.Spangler (1999) implemented system generating rule systems harmonizing fourpart chorales style Bach, constraint harmonization realtime. rulesets generated analyzing databases examples algorithmsapplied formal concepts information theory distilling rules, violationsharmonic rules prioritized order refine results. Using framework logicprogramming, Morales Morales (1995) designed system learned rules classicalcounterpoint musical examples rule templates.3.2.2 Rule-Based Methods Evolutionary Algorithmsintuitive way hybridize rule-based knowledge systems evolutionary algorithms craft fitness function ruleset. done efficiently domainswhose rules adequately codified, compliance rules expressedgraduated scale, instead binary (yes/no) compliance.good example four-part baroque harmonization pre-specified melody,lends particularly well approach. McIntyre (1994) extracted set rulesperforming harmonization classical works, codified set scoringfunctions. fitness weighted sum scores, tiered structure:scores added unless specific scores values thresholds (because critical prerequisites produce good harmonizations). slightlydifferent approach used Horner Ayers (1995): defined two classes rules:one defining acceptable voicings individual chords, used enumerate possiblevoicings, another defining voices allowed change successive chords. evolutionary algorithm used find music compositions, whose searchspace constructed enumeration voicings (first class rules). fitnesscandidate solution simply amount violated rules second class.Phon-Amnuaisuk et al. (1999) also four-part harmonization using set rules buildfitness function musical knowledge design genotype mutationcrossover operators, lack global considerations fitness function led modest results. contrast, Maddox Otten (2000) got good results implementing systemsimilar McIntyres (1994), using flexible representation, resultinglarger search space possible individuals, without tiered structure fitnessfunction, enabling less constrained search process.Another good example species counterpoint: Polito et al. (1997) extracted rulesspecies counterpoint classic eighteenth century music treatise, using definefitness functions multi-agent genetic programming system: agent performed529fiFernndez & VicoReferenceComposition taskCommentsMcIntyre, 1994four-part harmonizationexplores several schemes combinerules fitness functionHorner & Ayers, 1995four-part harmonizationtwo stages: enumeration possiblechord voicings, evolutionary algorithmvoice-leading rulesPhon-Amnuaisuk et al., 1999four-part harmonizationcriticizes vanilla evolutionary algorithmsgenerating unstructuredharmonizationsMaddox & Otten, 2000four-part harmonizationsimilar McIntyres (1994)Polito et al., 1997species counterpointmulti-agent genetic programming systemGwee, 2002species counterpointfuzzy rulesTable 7: References Section 3.2.2, order appearance.set composition transformation operations given melody specified seed,cooperated produce composition. Gwee (2002) exhaustively studiedcomputational complexity problems related generation species counterpointrulesets, implemented evolutionary algorithm whose fitness function basedset fuzzy rules (although also experimented trained artificial neural networksfitness functions).3.2.3 Constraint SatisfactionGradually (in process spanned 1980s 1990s), researchers algorithmiccomposition rule-based systems adopted formal techniques based logic programming. example, Boenn et al. (2008) used answer set programming encode rulesmelodic composition harmonization. However, work logic programmingdifferent paradigm: formulation algorithmic composition tasksconstraint satisfaction problems (CSPs). Previously referenced work, Steelss (1979),Levitts (1981), Schottstaedts (1989) Lthes (1999) seen part gradualtrend towards formulation musical problems CSPs17 , although constraint logicprogramming (CLP) came tool choice solve CSPs. Good surveys CLPalgorithmic composition written Pachet Roy (2001) AndersMiranda (2011).Ebciolu worked many years area, achieving notable results. first workimplemented Lisp (Ebciolu, 1980), translated rules fifth-species strict counterpointcomposable Boolean functions (he add rules bring systemproducing acceptable results, though), used algorithm produced exhaustiveenumeration compositions satisfying previously arranged set rules: basically,implemented custom engine logic programming Lisp. next decade,tackled problem writing four-part chorales style J. S. Bach. Finally,produced CHORAL, monumental expert system (Ebciolu, 1988), distilling 350rules guide harmonization process melody generation. keep problem17. Gills (1963) implementation formulated CSP, somewhat primitive later standards.530fiAI Methods Algorithmic Compositiontractable, designed custom logic language (BSL) optimizations standard logiclanguages, backjumping. system received substantial publicity, supposedreach level talented music student, words.Following Ebciolus work, many constraint systems implemented harmonization counterpoint. Tsang Aitken (1991) implemented CLP system using Prologharmonize four-part chorales. However, system grossly inefficient.18 OvansDavison (1992) described interactive CSP system first-species counterpoint,human user drove search process, system constrained possible outputs(according counterpoint rules) search progressed. took care efficiencyusing arc-consistency resolution constraints. Ramrez Peralta (1998)solved different problem: given monophonic melody, CLP system generatedchord sequence harmonize it. Phon-Amnuaisuk (2002) implemented constraint systemharmonizing chorales style J. S. Bach, innovation previoussystems: add knowledge system apply rules controlharmonization process explicitly, thus modulating search process explicitflexible way. Anders Miranda (2009) analyzed Schoenbergs textbook theoryharmony, programming system Strasheela (see below) produce self-contained harmonic progressions, instead harmonizing pre-existing melodies, constraintsystems do.many CLP systems implemented solve classical problems harmonization counterpoint, researchers studied application CLP techniquesdifferent problems. simple application, Wiggins (1998) used CLP system generate short fragments serial music. Zimmermann (2001) described two-stage method,stages used CLP: first stage (AARON) took input storyboard specify mood composition function time, generated harmonic progressionsequence directives. second (COMPOzE) generated four-part harmonizationaccording previously arranged progression directives; result intendedbackground music. Laurson Kuuskankare (2000) studied constraints instrumentation19 guitars trumpets (i.e., constraints composing music easily playableinstruments). Chemillier Truchet (2001) analyzed two CSPs: style Central African harp music, Ligeti textures. used heuristic search analyzesinstead backtracking, heralding OMClouds approach constraint programming (seebelow). Sandred (2004) proposed application constraint programming rhythm.Several general-purpose constraint programming systems algorithmic compositionproposed (i.e., languages environments program constraints). Oneearliest examples Courtots (1990) CARLA, CLP system generating polyphonies visual front-end rich, extendable type system designed representrelationships different musical concepts. Pachet Roy (1995) implemented another general-purpose musical CLP (Backtalk) object-oriented framework (MusES),designing generator four-part harmonizations top it. key contributionhierarchical arrangement constraints notes chords, dramatically decreasing (both cognitive computational) complexity resulting constraint system.18. spite using 20 rules, required 70 megabytes memory harmonize phrase 11notes.19. say, take account way instrument played composing part.531fiFernndez & VicoReferenceComposition taskCommentsBoenn et al., 2008melody harmonizationanswer set programmingEbciolu, 1980species counterpointimplemented LISPEbciolu, 1988(CHORAL)four-part harmonizationimplemented custom logic language (BSL)Tsang & Aitken, 1991four-part harmonizationinefficientOvans & Davison, 1992species counterpointinteractive searchRamrez & Peralta, 1998melody harmonizationsimpler constraint solverPhon-Amnuaisuk, 2002four-part harmonizationexplicit control search processAnders & Miranda, 2009Schoenbergs TheoryHarmonyimplemented StrasheelaWiggins, 1998Schoenbergs twelve-tonetechniquesimple demonstrationZimmermann, 2001(Coppelia)structure, melody,harmonization, rhythmtwo stages: harmonic plan (Aaron)execution (Compoze)Laurson & Kuuskankare,2000guitar trumpetinstrumentationimplemented PWConstraintsChemillier & Truchet, 2001African harp Ligetitexturesimplemented OpenMusicSandred, 2004rhythmimplemented OpenMusicCourtot, 1990(CARLA)polyphony, general purposeearly general-purpose systemPachet & Roy, 1995(BackTalk)four-part harmonizationimplemented MusEsRueda et al., 1998polyphony, general purposedescribes PWConstraints (implementedPatchWork) Situation (implementedOpenMusic)Rueda et al., 2001general purposedescribes PiCOdescribes ntccOlarte et al., 2009general purposeAllombert et al., 2006interactive improvisationuses ntccRueda et al., 2006interactive improvisationuses ntcc Markovian modelsPachet et al., 2011melodyintegrates Markovian models constraintsTruchet et al., 2003general purposedescribes OMCloudsAnders, 2007general purposedescribes StrasheelaSandred, 2010general purposedescribes PWMC.Implemented PatchWorkCarpentier & Bresson, 2010orchestrationuses multi-objective optimizationdiscover candidate solutions.Interfaces OpenMusic MAXYilmaz & Telatar, 2010harmonizationfuzzy logicAguilera et al., 2010species counterpointprobabilistic logicGeis & Middendorf, 2008four-part harmonizationmulti-objective Ant Colony OptimizationHerremans & Sorensena,2012species counterpointvariable neighborhood tabu searchDavismoon & Eccles, 2010melody, rhythmuses simulated annealing combineconstraints Markov processesMartin et al., 2012interactive improvisationimplemented MAXTable 8: References Section 3.2.3, order appearance.532fiAI Methods Algorithmic CompositionRueda et al. (1998) reviewed two early general-purpose systems, PWConstraintsSituation. PWConstraints able (relatively easily) handle problems polyphoniccomposition subsystem (score-PMC), Situation flexible implemented optimizations search procedures. PiCO (Rueda et al., 2001)experimental language music composition seamlessly integrated constraints,object-oriented programming calculus concurrent processes. idea useconstraint programming specify voices composition, use concurrentcalculus harmonize them. authors also implemented visual front-end PiCOease use, Cordial. similar way PiCO, ntcc another language constraintprogramming implemented primitives defining concurrent systems, althoughspecifically designed algorithmic composition. ntcc proposed generaterhythm patterns expressive alternative PiCO (Olarte et al., 2009),mainly used machine improvisation: Allombert et al. (2006) usedimprovisation stage two-stage system (the first stage used temporal logic systemcompose abstract temporal relationships musical objects, ntcc stagegenerated concrete music realizations), Rueda et al. (2006) used ntcc implementreal-time system learned Markovian model (using Factor Oracle) musicians concurrently applied generate improvisations. related ntcc, Pachetet al. (2011) also proposed framework combine constraint satisfaction Markovprocesses.OMClouds (Truchet et al., 2003) another general-purpose (but purely visual) constraint system composition, implementation set apart formalsystems: internally, constraints translated cost functions. Instead optimized tree search backtracking usual CLP, adaptive tabu search performed,seeking minimize solution minimal cost. avoids problems inherentconstraint programming, overconstraining, cannot guaranteed completely navigate search space. Anders (2007) implemented Strasheela, systemexpressly designed highly flexible programmable, aiming overcome perceived limitation previous general-purpose systems: difficulty implement complexconstraints related multiple aspects compositions process. Finally, anotherpurely visual constraint system, PWMC, proposed Sandred (2010) overcome perceived limitations score-PMC. able handle constraints concerning pitchstructure score-PMC, also rhythm metric structure.stressed that, CLP become tool choice solve CSPs,approaches also used. Previously cited OMClouds one these. CarpentierBresson (2010) implemented mixed system orchestration worked curious way: user fed system target sound set symbolic constraints;multi-objective evolutionary algorithm found set orchestration solutions matchingtarget sound, local search algorithm filtered solutions complyingconstraints. Yilmaz Telatar (2010) implemented system simple constraintharmonization fuzzy logic, Aguilera et al. (2010) used probabilistic logic solvefirst-species counterpoint. exotic solutions proposed, use AntColony Optimization multi-objective approach solve constraints Baroqueharmonization (Geis & Middendorf, 2008), variable neighborhood tabu search solvesoft constraints first-species counterpoint (Herremans & Sorensena, 2012), simulated533fiFernndez & VicoReferenceComposition taskCommentsPereira et al., 1997Baroque musichierarchical analysis representationRibeiro et al., 2001(MuzaCazUza)Baroque musicgenerates melody harmonic lineRamalho & Ganascia, 1994jazz improvisationuses rule-based system analysisParikh, 2003jazz improvisationalso uses rule system analize musicEigenfeldt & Pasquier, 2010jazz chord progressionsalso uses Markovian modelsSabater et al., 1998harmonizationalso uses rule systemTable 9: References Section 3.2.4, order appearance.annealing combine constraints Markov processes (Davismoon & Eccles, 2010). Finally, Martin et al. (2012) presented even exotic approach: real-time musicperformer reacted environment. aspects musiccontrolled Markov chains, others expressed CSP. solve CSP realtime, solution calculated random (but quickly) using binary decision diagrams.3.2.4 Case-Based ReasoningCase-based reasoning (CBR) another formal framework rule-based systems.CBR paradigm, system database cases, defined instancesproblem corresponding solutions. Usually, case also contains structuredknowledge problem solved case. faced new problem,system matches case database. Unless new problem identicalone recorded case database, system select case similar newproblem, adapt corresponding solution new problem. new solutiondeemed appropriate, new case (recording new problem along new solution)may included database.Several researchers used CBR algorithmic composition. Pereira et al. (1997)implemented system generated case database three Baroque musicpieces, analyzed hierarchical structures; cases nodes.system composed soprano melodic line piece, searching similar casescase database. results comparable output first-year student, accordingmusic experts consulted authors. intersecting set researchers implementedsimpler CBR composing system (Ribeiro et al., 2001) generated melodyharmonic line, time case database generated six Baroque pieces. Casesrepresented different way, tough: case represented rhythm, melodyattributes associated chord given context. generate new music piece,harmonic line specified, system fleshed music piece matchingcases harmonic line.Hybrid systems also proposed. Ramalho Ganascia (1994) proposed jazzimproviser used rule system analyze incoming events (for example, ongoingsequence chords) CBR engine improvise. case database assembledextracting patterns transcriptions jazz recordings, consisted descriptionscontexts play contexts. improvisation, current context534fiAI Methods Algorithmic CompositionReferenceComposition taskCommentsHaus & Sametti, 1991(Scoresynth)melodyPetri netsLyon, 1995melodyPetri nets encode Markov chainsHolm, 1990melody, sound synthesisinspired CSP process algebraRoss, 1995(MWSCCS)melodycustom process algebraRueda et al., 2001general purposedescribes PiCOAllombert et al. (2006)interactive improvisationuses ntccRueda et al., 2006interactive improvisationuses ntcc Markovian models(Olarte et al., 2009)general purpose, rhythmsdescribes ntccTable 10: References Section 3.2.5, order appearance.analyzed rule system, cases applying current contextextracted database combined determine output improviser. Considering Ramalho Ganascias system inflexible, Parikh (2003) implementedanother jazz improviser, intending use large case database containing jazz fragmentsvarious sources, order get system style own. EigenfeldtPasquier (2010) used case-based system generate variable-order Markov modelsjazz chord progressions.Outside jazz domain, hybrid system harmonizing melodies popular songsimplemented Sabater et al. (1998): given melody, system sequentially decidedchords harmonize it. CBR module failed match case, system fell backsimple heuristic rule system select appropriate chord. harmonized outputadded case database, CBR module gradually learned time rulesystem.3.2.5 Concurrency ModelsConcurrency models described formal languages specify, model and/or reasondistributed systems. provide primitives precisely define semanticsinteraction synchronization several entities. main applicationmodeling designing distributed concurrent computer systems, alsoused languages partially fully model composition process, musiccomposition formulated endeavor carefully synchronize streams musicevents produced several interacting entities. Concerning algorithmic composition,used concurrency models Petri nets several kinds process algebras,also known process calculi. Detailed descriptions models beyond scopesurvey; see example Reisigs (1998) book Petri nets Baetens (2005) surveyprocess algebras information.Petri nets used basis Scoresynth (Haus & Sametti, 1991), visualframework algorithmic composition Petri nets used describe transformations musical objects (sequences notes musical attributes), synchronization535fiFernndez & Vicomusical objects implicit structure net. Petri nets alsoused efficient compact way implement Markov chains (Lyon, 1995).Process algebras first used algorithmic composition Holm (1990), althoughmodel (inspired Hoares algebra, CSP) geared towards sound synthesismusic composition. proper example Rosss (1995) MWSCCS, extension (adding concepts music composition) previously existing algebra (WSCCS).Specifications algorithmic composition written MWSCCS meant resemblegrammatical specifications, richer expressive power. Later examples alsocited Section 3.2.3, PiCO language (Rueda et al., 2001), integratedlogical constraints process algebra. Also cited Section, ntcc process algebra used implement machine improvisation (Allombert et al., 2006)drive Markovian model (using Factor Oracle) real-time machine learningimprovisation (Rueda et al., 2006). also proposed generate rhythm patterns,expressive alternative PiCO (Olarte et al., 2009).3.3 Markov Chains Related MethodsConceptually, Markov chain simple idea: stochastic process, transiting discretetime steps finite (or countable) set states, without memory: nextstate depends current state, sequence states precededtime step. simplest incarnations, Markov chains representedlabeled directed graphs: nodes represent states, edges represent possible transitions,edge weights represent probability transition states. However, Markov chainscommonly represented probability matrices.Markov chains applied music composition, probability matrices mayeither induced corpus pre-existing compositions (training), derived handmusic theory trial-and-error. former common way useresearch, latter used software tools composers. important designdecision map states Markov chain musical objects. simplest(but fairly common) mapping assigns sequential group notes state,choice one note (instead larger sequence) fairly common.also common extend consideration current state: n-th orderMarkov chain, next state depends last n states, last one.consequence, probability matrix n + 1 dimensions. algorithmic composition,Markov chains mostly used generative devices (generating sequence states),also used analysis tools (evaluating probability sequence states).latter case, term n-gram also used, though strictly speaking referssequence N states.Markov chains popular method early years algorithmic composition.Early examples already reviewed Section 2.1; additionally, Ames (1989) alsoprovides good survey. However, Markov chains generated corpus pre-existingcompositions captured local statistical similarities, limitations soon becameapparent (Moorer, 1972): low-n Markov chains produced strange, unmusical compositionswandered aimlessly, high-n ones essentially rehashed musical segmentscorpus also computationally expensive train.536fiAI Methods Algorithmic CompositionReferenceComposition taskCommentsTipei, 1975melodyMarkov chains part larger,ad hoc systemJones, 1981melodysimple introduction composersLangston, 1989melodydynamic weightsNorth, 1991melodyMarkov chains part larger,ad hoc systemAmes & Domino, 1992(Cybernetic Composer)jazz, rockuses Markov chains rhythmsVisell, 2004real-time generative artinstallationLiberal use concept HMM.Implemented MAXZicarelli, 1987(M Jam Factory)interactive improvisationcommercial GUI applicationsalternative representation transitionmatrices. Implemented athenaCLAriza, 2006Ponsford et al., 1999composing sarabande piecesadds symbols expose structurepiecesLyon, 1995melodyimplements Markov chainsPetri netsVerbeurgt et al., 2004melodytwo stages: Markovian modelartificial neural networkThom, 2000(BoB)interactive jazz improvisationstatistical machine learningLo & Lucas, 2006melodyevolutionary algorithm, Markov chainsused fitness functionWerner & Todd, 1997melodyco-evolutionary algorithm, Markovchains used evolvable fitness functionsThornton, 2009melodygrammar-like hierarchyMarkov modelsCruz-Alczar Vidal-Ruiz(1998)melodyanalysis grammatical inference,generation Markovian modelsGillick et al. (2009)jazz improvisationanalysis grammatical inference,generation Markovian modelsEigenfeldt & Pasquier, 2010jazz chord progressionsuses case-based reasoningDavismoon & Eccles, 2010melody, rhythmuses simulated annealing combineconstraints Markov processesPachet et al., 2011melodyintegrates Markovian modelsconstraintsGrachten, 2001jazz improvisationintegrates Markovian modelsconstraintsManaris et al., 2011interactive melody improvisationMarkov chains generate candidatesevolutionary algorithmWooller & Brown, 2005transitioning two melodiesalternates Markov chainstwo melodiesTable 11: References Section 3.3, order appearance.this, Markov chains came seen source raw material, insteadmethod truly compose music automated way, except specialized tasks537fiFernndez & Vicorhythm selection (McAlpine et al., 1999). Therefore, research interest Markovchains receded subsequent years limitations became apparent methodsdeveloped, remained popular among composers. However, citing even relevantsubset works composers use Markov chains part compositionalprocess would inflate reference list beyond reasonable length. typical examplesMarkov chains used composers (sometimes part larger automatic compositionalframework software system) papers Tipei (1975), Jones (1981), Langston (1989,although used dynamically computed weights), North (1991) Ames Domino(1992). noted composers sometimes deconstruct formal methods adaptpurposes, Visell (2004) used concept Hidden MarkovModel (described below) implement manually-tuned real-time generative art system.addition, many software suites use Markov chains provide musical ideas composers, even probabilities specified hand instead generated corpus.Ariza (2006) gives compact list software suites experimental programs using Markovchains, thesis (Ariza, 2005b) provides comprehensive view field. Muchdone usability field, using GUI interfaces (Zicarelli, 1987), alsodeveloping effective ways encode probability matrices, example compactstring specifications (Ariza, 2006).However, novel research Markov chains algorithmic compositions stillcarried several ways. example, Ponsford et al. (1999) used corpus sarabandepieces (relatively simple dance music) generate new compositions using Markov models20 ,pre-processing stage automatically annotate compositions corpussymbols make explicit structure, post-processing stage using templateconstrain structure synthesized composition, order generate minimallyacceptable results. Another way hybridization Markov chains methods.example, Lyon (1995) used Petri nets efficient compact way implementMarkov chains, Verbeurgt et al. (2004) used Markov chains21 generate basicpattern melody, refined artificial neural network.BoB system (Thom, 2000), Markov chains trained statistical learning: setjazz solos, statistical signatures extracted pitches, melodic intervals contours.Then, signatures used define transition probabilities Markov chainwhose output sampled generate acceptable solos. Lo Lucas (2006) trainedMarkov chains classic music pieces, but, instead generating compositions them,used fitness evaluators evolutionary algorithm evolve melodies encodedsequences pitches. Werner Todd (1997) also used Markov chains evaluatesimple (32-note) melodies, particularity chains alsosubject evolution, investigate sexual evolutionary dynamics. Thornton (2009) definedset grammar-like rules existing composition, inferring hierarchy Markovmodels use statistical patterns analyzed composition multiple levels. alreadymentioned Section 3.1, Cruz-Alczar Vidal-Ruiz (1998) Gillick et al. (2009) usedgrammatical inference Markovian models. Regarding symbolic methods, EigenfeldtPasquier (2010) used case-based system generate Markov processes jazz chord20. work commonly cited literature grammatical, methodology thoroughlystatistical.21. Also reviewed Section 3.4.538fiAI Methods Algorithmic Compositionprogressions, (Davismoon & Eccles, 2010) used simulated annealing combine constraintsMarkov processes, Pachet et al. (2011) proposed framework combine Markoviangeneration music rules (constraints) produce better results.Additionally, Markov chains remained feasible option restricted problems (for example, real-time performances, jazz improvisation), limitations less apparentcases generation whole compositions. example, Grachten (2001)developed jazz improviser Markov chains generated duration pitches,system constraints refined output, pre-defined licks (short musical patterns)inserted appropriate times. Manaris et al. (2011) also implemented improviser, usingMarkov model trained user input generate population candidate melodies, feeding evolutionary algorithm, whose fitness function rewarded melodies whosemetrics similar user inputs metrics. different (but also restricted) problemstudied Wooller Brown (2005): applying Markov chains generate musicaltransitions (morphings) two different pieces simple application non-linearmusic, stochastically alternating two Markov chains, one trained onepieces.3.3.1 Related Methodssophisticated Markovian models (and related statistical methods; see surveyConklin, 2003) also applied algorithmic composition, Pachets (2002)Continuator, real-time interactive music system. Continuator departs commonMarkov chain implementations uses variable-order (also known mixed-order)Markov chains22 , constrained fixed n value, used get bestlow high-n chains. Conklin Witten (1995) implemented sophisticated variableorder scheme23 , whose main feature consideration parallel multiple viewpointssequences events compositions (for example, pitches, durations, contours, etc.),instead integrating unique sequence symbols, commonimplementations Markov chains. Variable-order Markov chains also usedpart larger real-time music accompaniment system (Martin et al., 2012). variableorder schemes used algorithmic composition, formulated machine learning framework,Prediction Suffix Trees (PSTs, Dubnov et al., 2003), space-efficient structures likeFactor Oracles 24 (Assayag & Dubnov, 2004), Multiattribute Prediction Suffix Graphs(MPSGs, Trivio Rodrguez & Morales-Bueno, 2001), considered extensionPSTs consider multiple viewpoints Conklin Wittens work. Sastry (2011)also used multiple viewpoints PSTs modelize Indian tabla compositions, thoughmodel could also used generate new compositions.Hidden Markov Models (HMMs) also generalizations Markov chainsused algorithmic composition. HMM Markov chain whose state unobservable, state-dependent output visible. Training HMM involves22. noted Kohonens method (Kohonen et al., 1991), reviewed Section 3.1.3, similar(in ways) variable-order chains.23. Conklin Wittens method also described grammatical, includedemphasis formal statistical analysis.24. Also implemented concurrent constraint paradigm Rueda et al. (2006). See Section 3.2.3Section 3.2.5 details.539fiFernndez & VicoReferenceComposition taskCommentsPachet, 2002(Continuator)interactive improvisationvariable-orderConklin & Witten, 1995Bach choralesmultiple viewpoint systemsMartin et al., 2012interactive improvisationvariable-order; implemented MAXDubnov et al., 2003melodyPrediction Suffix Trees.Implemented OpenMusicRueda et al., 2006interactive improvisationuses ntcc Factor OraclesAssayag & Dubnov, 2004melodyFactor Oracles.Implemented OpenMusicTrivio Rodrguez &Morales-Bueno, 2001melodyMultiattribute Prediction Suffix GraphsSastry, 2011improvisation tabla rhythmsmultiple viewpoints PredictionSuffix Trees.Implemented MAXFarbood & Schoner, 2001species counterpointHidden Markov ModelsBiyikoglu, 2003four-part harmonizationHidden Markov ModelsAllan, 2002four-part harmonizationHidden Markov ModelsMorris et al., 2008(SongSmith)melody harmonizationHidden Markov ModelsSchulze, 2009(SuperWillow)melody, rhythm, two-voiceharmonizationHidden Markov ModelsPrediction Suffix TreesYi & Goldsmith, 2007four-part harmonizationMarkov Decision ProcessesMartin et al., 2010interactive improvisationPartially ObservableMarkov Decision ProcessesTable 12: References Section 3.3.1, order appearance.determining matrix transition probabilities, also matrix output probabilities(that is, state, probability possible output). Then, given sequenceoutputs, possible compute likely sequence states produce sequence outputs, using Viterbi dynamic programming algorithm. way, HMMsfind globally optimized sequence states, simpler Markov methods perform local optimization. applied algorithmic composition, HMMs appropriate addelements existing composition (most commonly, counterpoint harmonization),given set pre-existing examples: composition modeled sequence outputsHMM, additions computed likely sequence statesHMM.Farbood Schoner (2001) implemented earliest example HMM algorithmiccomposition: trained second-order HMM generate Palestrina-style first-speciescounterpoint (the simplest way write counterpoint), defining training set rulesused teach counterpoint. related problem train HMMs set choraleharmonizations style J.S. Bach order get Bach-like harmonizations.problem researched Biyikoglu (2003) Allan (2002); latter dividedproblem harmonization three subtasks HARMONET (Hild et al.,1992). Microsofts SongSmith software, Morris et al. (2008) trained HMM540fiAI Methods Algorithmic Composition300 lead sheets (specifications song melodies) generate chords accompany userspecified vocal melody, parametrizing resulting system intuitive interfacenon-technical users. Schulze (2009) generated music several styles using mixed-orderMarkov chains generate melodies, HMMs harmonize them.Markov Decision Processes (MDPs) another generalization Markov models,agent maximizes utility function taking actions probabilistically influence next state, Partially Observable MDPs (POMDPs) represent correspondinggeneralization HMMs. Experimental systems algorithmic composition implemented MDPs (Yi & Goldsmith, 2007) POMDPs (Martin et al., 2010), thoughclear sophisticated models offer definitive advantages simpler ones.3.4 Artificial Neural Networks Related MethodsArtificial Neural Networks (ANNs) computational models inspired biological neuralnetworks, consisting interconnected sets artificial neurons: simple computationaldevices aggregate numeric inputs single numeric output using (generally)simple nonlinear function. neurons connections set externally(input connections), output signals intended read resultnetworks computation (output connections). Typically, neurons organizedrecurrent networks (some neurons inputs come neurons)several interconnected layers, many variations found literature. ANNstypically used machine learning method, using set examples (input patterns)train network (i.e., set weights connections neurons), orderuse recognize generate similar patterns. Effectively, means neuralnetworks need pre-existing corpus music compositions (all similarstyle, generally); therefore imitate style training examples.papers use supervised learning approach, meaning examples training setassociated signal, ANN learns association. important aspectANN design modelization musical composition, is, mappingmusic music notation inputs outputs network. Another importantaspect way compositions fed ANNs: may presentedtemporal patterns network inputs, usually windowed segments,cases fed (as wholes) ANNs (these implementationsscale well, though, big ANNs needed model long compositions).ANNs first used 1970s 1980s analyze musical compositions, creating artificial models cognitive theories music (Todd & Loy, 1991), lateradapted music composition. first example implemented Todd (1989),used three-layered recurrent ANN designed produce temporal sequence outputsencoding monophonic melody, output signal network representing absolutepitch. Given set one composition examples, ANN trained associatesingle input configuration output temporal sequence corresponding composition. Then, feeding input configurations different ones used trainingcreated melodies interpolated ones used training. one melodyused training, result extrapolation it. Later year, Duff(1989) published another early example, using different approach, encoding relative541fiFernndez & VicoReferenceComposition taskCommentsTodd, 1989melodythree layers, recurrentDuff, 1989melodytwo layers, recurrentMozer, 1991(CONCERT)melodypsychologically-grounded representationpitchfeedforward model, used fitnessfunction optimization algorithmLewis, 1991Shibata, 1991harmonizationfeedforward modelBellgard & Tsang, 1992harmonizationeffective Boltzmann machineMelo, 1998harmonizationANN trainedmodel music tensionToiviainen, 1995jazz improvisationrecurrent modelNishijimi & Watanabe, 1993jazz improvisationfeedforward modelFranklin, 2001jazz improvisationrecurrent modelHild et al., 1992(HARMONET)four-part harmonizationthree-layered architecture(two ANNs constraint system)Feulner & Hrnel, 1994(MELONET)four-part harmonizationuses HARMONET another ANNmelodic variationsGoldman et al., 1996(NETNEG)species counterpointANN basic melody, ensembleagents refine melodyVerbeurgt et al., 2004melodytwo stages: Markovian modelANNAdiloglu & Alpaslan, 2007species counterpointfeedforward modelBrowne & Fox, 2009melodysimulated annealing ANNmeasure musical tensionCoca et al., 2011melodyrecurrent model, uses chaotic non-linearsystems introduce variationTable 13: References Section 3.4, order appearance.instead absolute pitches (as Todds work) mapping, composing music Bachsstyle.machine learning paradigm, ANNs used many different ways, Toddsapproach possible; indeed, early papers provide different examples.example, Mozer (1991) developed recurrent ANN training program devised capture local global patterns set training examples. model also featuredoutput mapping sophisticated multidimensional space pitch representation,capture formal psychological notion similarity different pitches. way,similar output signals mapped similar pitches, order facilitate learning phaseimprove composition phase. Lewis (1991) proposed another ANN framework: creation refinement, feedforward ANN trained set patterns rangingrandom good music, associating pattern (possibly) multidimensional musicality score. way, training phase generated mapping functionpatterns musicality scores. Then, create new compositions, mapping inverted:starting purely random pattern, gradient-descent algorithm used ANN542fiAI Methods Algorithmic Compositioncritique, reshaping random pattern maximize musicality score hopefinally producing pleasant composition. Unfortunately, paradigm prohibitivecomputational cost, tested fairly simple short compositions.early examples described experiments composing lessfull-fledged monophonic compositions. However, ANNs also used automatetasks music composition, harmonization pre-existing melodies. Shibata (1991) implemented early example: feedforward ANN represented chords using component tones, trained harmonizing simple MIDI music, whose performance measuredhuman listeners. sophisticated ANN used harmonization, effective Boltzmann machine (EBM), also provided measure quality output relativetraining set (Bellgard & Tsang, 1992). Melo (1998) also harmonized classical music,notable twist: order model tension25 music harmonized,measured tension curve reported several human subjects listening music,used averaged tension curve train ANN, chord progressionsgenerated ANN matched tension level suggested curve. seen,harmonization popular test case, problems also tried. example,Toiviainen (1995) used ANNs generated jazz improvisations based set trainingexamples. ANNs able create new jazz melodic patterns based trainingset. similar way, Nishijimi Watanabe (1993) trained set feedforward ANNsproduce jazz improvisations jam session, modeling several music features jazzusing examples modeled jazz improvisations train ANNs. Franklin (2001)used recurrent ANNs improvise jazz (trade four solos jazz performer), trainedtwo phases: first phase training ANN set pre-specified examples,second phase ANN reconfigured trained reinforcement learning,reinforcement values obtained applying set heuristic rules.researchers came use hybrid systems, combining ANNs methods.One first examples HARMONET (Hild et al., 1992): model designed solvecomplex task: four-part choral harmonization Bachs style. HARMONETthree-layered architecture: first component feedforward ANN sophisticated encoding musical information (optimized harmonization functions insteadindividual pitches), used extract harmonization information. outputfed second component, rule-based constraint satisfaction algorithm generatechords, final component another ANN designed add quaver ornamentspreviously generated chords. evolution HARMONET, MELONET (Feulner &Hrnel, 1994; improved Hrnel & Degenhardt, 1997) harmonized chorales,also generated melodic variations voices, using HARMONET first processingstage harmonization, used another neural network generate melodicvariations.NETNEG (Goldman et al., 1996) another hybrid system used ANN trainedsixteenth century classical music compositions. ANN generated basic melodysegments. segment created, ensemble agents generated polyphonicelaboration segment. agents rule-based systems crafted music theoret25. important property music, rather difficult define. point time, tension relatedinterplay structure uncertainty perceived listener flow music.Informally, defined unfinishedness music stopped point.543fiFernndez & Vicoical considerations, coordinated maintain coherent global output. ANNs alsocombined probabilistic methods: work Verbeurgt et al. (2004), settraining sequences decomposed musical motifs, encoded relative pitch. Then,Markov chain constructed, whose states motifs. New compositions generated Markov chain, assign absolute pitches motifs resultingcomposition, trained ANN. Adiloglu Alpaslan (2007) used feedforward ANNsgenerate two-voice counterpoint, applying notions music theory representationmusical information networks. Browne Foxs (2009) system, simulatedannealing used arrange small fragments (motifs) classical music, trying getprofile musical tension (the metric Melo, 1998) similar profilepre-specified composition, measured using ANN specialized music perception. Finally,another hybrid system implemented Coca et al. (2011), used ANNs trainedpre-existing compositions together pseudo-random musical input generatedchaotic system, order generate complex compositions synthesis phase.3.4.1 ANNs Evolutionary AlgorithmsAmong hybrid systems, combining ANNs evolutionary algorithms quickly became popular. Usually, ANN trained act fitness functionevolutionary algorithm. case earliest example hybrid systems,NEUROGEN (Gibson & Byrne, 1991). fitness function composed result twoANNs, one judging intervals pitches overall structure. genetic algorithm rather rigid, classical binary representation used,severely limiting applicability whole implementation. However, alsoinverted frameworks evolving individuals ANNs. example, HrnelRagg (1996) evolved HARMONET networks, fitness network performancetraining harmonization. another example (Chen & Miikkulainen, 2001), recurrentthree-layered ANNs evolved compose music, fitness computed setrules music theory.Given modular nature evolutionary algorithms perceived complexityANNs, uncommon evolutionary framework laid first researchwork, subsequent developments ANNs used replace original fitnessfunction.26 example, Spector Alpern (1994) developed genetic programming (GP)framework jazz improvisations: individuals programs composed collectionstransformations produced improvisations upon fed previously existing jazzmelodies, fitness function aggregated several simple principles jazz musictheory. following year (Spector & Alpern, 1995), updated model trainANN used fitness function. However, scheme always fare well,specially initial framework uses interactive fitness. case GenJam (Biles,1994), evolutionary algorithm generating jazz melodies interactive fitnessfunction. Later (Biles et al., 1996), interactive fitness evaluation representedsevere fitness bottleneck, ANNs tried partially offload evaluation human users,success, ANNs failed satisfactorily generalize evaluations26. approach risky, though, evolutionary algorithms tend find exploit unexpectedundesired quirks fitness evaluation function; evaluator ANN.544fiAI Methods Algorithmic CompositionReferenceComposition taskCommentsGibson & Byrne, 1991melody, rhythmTwo ANNs define fitness functionHrnel & Ragg, 1996melody harmonizationevolves HARMONET, fitness ANNsperformance training harmonizationChen & Miikkulainen, 2001melodyevolves recurrent networks,fitness computed set rulesSpector & Alpern, 1994melodygenetic programming, ANNs fitness functionsBiles et al., 1996jazz improvisationANNs fitness functionsJohanson & Poli, 1998(GP-Music)melodygenetic programming, ANNs fitness functionsKlinger & Rudolph, 2006melodyANNs decision trees fitness functionsManaris et al., 2007melodygenetic programming, ANNs fitness functionsBurton, 1998drum rhythmsAdaptive Resonance Theory(unsupervised learning) fitness functionPhon-Amnuaisuk et al., 2007melodygenetic programming, self-organinzing map(unsupervised learning) fitness functionTable 14: References Section 3.4.1, order appearance.training sets. case GP-Music System (Johanson & Poli, 1998), usedGP procedural representations short melodies, trained ANNs failure,decidedly par respect performance algorithm interactivefitness. Klinger Rudolph (2006) compared performance feedforward ANNslearned decision trees, finding latter performed better ratings easierunderstand. spite examples, successful instances exist: Manaris et al.(2007) extracted several statistical metrics music compositions trained ANNrecognize compositions whose metrics distributions featured Zipfs law, usedfitness function GP framework whose individuals procedural representationspolyphonic compositions. results validated aesthetically pleasing humantesters.research described point uses ANNs supervised learning. However,methods using unsupervised learning also exist. Burton (1998) proposed genetic algorithmclassical binary representation generating multi-voice percussion rhythms, whosefitness function presented unconventional feature mix. used Adaptive Resonance Theory (ART), ANN unsupervised learning, initially trained perform automaticclustering set drum rhythms. Then, execution genetic algorithm,unsupervised learning continued. fitness measure near individual cluster. individual represented brand new rhythm different addnew cluster, rhythm presented user decide musically acceptable.way, Burton tried get best interactive automatic fitness evaluation.different example unsupervised learning presented Phon-Amnuaisuk et al.(2007), able generate variations pre-specified composition. used GP whose individuals procedural representations melodies, fitness similaritypre-specified composition, measured self-organizing map (SOM) previouslytrained musical elements pre-specified composition.545fiFernndez & VicoReferenceComposition taskCommentsBaggi, 1991jazz improvisationad hoc connectionist expert systemLaine, 2000simple rhytmsuses central pattern generatorsDorin, 2000poly-rhythmic musical patternsuses Boolean networksHoover et al., 2012acompanimentuses CPPNsTable 15: References Section 3.4.2, order appearance.3.4.2 Related MethodsFinally, subsection presents methods may considered roughly similar ANNs,generally, connectionist. example, Neurswing (Baggi, 1991) may describedad hoc expert system jazz improvisation crafted connectionist framework.Surprisingly, many research papers cite Neurswing ANN framework, despite Baggisdisclaimer: [Neurswing], though vaguely resembling neural net connectionistsystem, [. . . ] (Baggi, 1991). Laine (2000) used simple ANNs implement CentralPattern Generators, whose output patterns interpreted less simplerhythms (motion patterns terminology).Boolean networks another connectionist paradigm node binarystate edges nodes directed; nodes state changes discrete stepsaccording (usually randomly chosen) Boolean function whose inputs statesnodes connections node. may considered generalizationscellular automata, depending wiring distribution Boolean functions,states change complex patterns, potentially complex responses external forcing. properties, Dorin (2000) used Boolean networks generatecomplex poly-rhythmic musical patterns, modulable real time user.Hoover et al. (2012) proposed another connectionist approach: compositional patternproducing networks (CPPNs). feedforward networks neuron mayuse different, arbitrary function, instead classical sigmoid function. usuallydesigned interactive evolutionary methods, used generate modulatehighly complex patterns. cited paper, fed pre-existing simple compositioninput, order generate accompaniment it.3.5 Evolutionary Population-Based Methodsevolutionary algorithms (EAs) approximately follow common pattern: changingset candidate solutions (a population individuals) undergoes repeated cycle evaluation, selection reproduction variation. first step generate candidatesolutions initial set, either user-specified examples less randomway. candidate evaluated using fitness function, heuristic rule measurequality. next phase selection: new set candidate solutions generatedcopying candidate solutions old one; candidate solution copied numbertimes probabilistically proportional fitness. step decreases diversitypopulation, restored applying (to fraction candidate solutions)operators designed increase variation (for example, mutation recombination546fiAI Methods Algorithmic Compositionoperators). steps applied iteratively; result, best mean fitness graduallytend increase.algorithmic pattern common EAs, exist many different algorithms using different sets selection rules, variation operators solution encoding.EA, encoded form candidate solution genotype, phenotypetranslation coded form solution. Hollands original formulation genetic algorithms strongly associated plain direct encoding genotypesbinary strings, case papers using EAs. this,term evolutionary preferred genetic paper. Another popular variant, Kozasgenetic programming, represents genotypes tree structures, often encoding expressionsprogramming language, phenotype result evaluating expressions.Since EAs particularly prone hybridized methods, alsoreviewed sections: grammars Section 3.1.2, ANNs Section 3.4.1,Markov chains Section 3.3 , rule-based systems Section 3.2.2,cellular automata Section 3.6.1 . general, papers cited sectionsdiscussed here. also recommend literature learn evolutionarycomputer music: Burton Vladimirova (1999) wrote survey long, thorough descriptions referenced papers, survey Santos et al. (2000) similarstyle, packed brief descriptions. Miranda Biless (2007) bookrecent, also contains work optimization methods (as swarm optimization)algorithmic composition techniques (as cellular automata).3.5.1 Evolution Automatic Fitness Functionsdifficulty define automatic fitness functions constant issue, frequentlylimiting application evolutionary methods well-defined restricted problemscomposition. Horner Goldberg (1991) provided one first examples:implemented EA thematic bridging, composition technique consisting definingsequence (with pre-specified preferred length) small musical patternsfirst last ones pre-specified, pattern result applying simpletransformation previous one. Naturally, individuals EA definedlists operations applied initial pattern generate sequence patterns.fitness measured close final pattern (generated operations)pre-specified final pattern, plus difference actual preferred lengthssequence. Essentially work (with differences underlying representationoperation set) reported Ricanek et al. (1993).common way implement fitness function weighted sum featurescomposition (although tuning weights optimize EA prove difficult excepttoy problems). example, Marques et al. (2000) composed short polyphonic melodiesusing direct representation genotypes also simple fitness function,rather simple, ad hoc evaluation harmony melodic value. resultsreportedly acceptable. Johnson et al. (2004) also composed short melodies using EA. examples evolutionary algorithms described section, warranting dedicatedsubsection them.. case previous note.547fiFernndez & VicoReferenceComposition taskCommentsHorner & Goldberg, 1991thematic bridgingfitness: distance original melodiesRicanek et al., 1993thematic bridgingfitness: distance original melodiesMarques et al., 2000polyphonyfitness: combination featuresJohnson et al., 2004melodyfitness: combination featuresPapadopoulos & Wiggins,1998jazz improvisationfitness: combination featuresHarris, 2008(JazzGen)jazz improvisatorfitness: combination featuresTowsey et al., 2001melodic extensionfitness: combination features(only fitness, evolutionary algorithm)Birchfield, 2003melody, rhythmfitness: combination featuresGaray Acevedo, 2004species counterpointfitness: combination featuresLozano et al., 2009melodyharmonizationfitness: combination featuresDe Prisco et al., 2010unfigured bassmulti-objective optimizationFreitas & Guimares, 2011melodyharmonizationmulti-objective optimizationGartland-Jones, 2002generate variationstwo melodiesfitness: distance melodyAlfonseca et al., 2005melodyfitness: distance corpus melodieszcan & Eral, 2008(AMUSE)generate variationsmelodyfitness: combination featuresWolkowicz et al., 2009melodyfitness: distance corpus melodiesLaine & Kuuskankare, 1994melodyfitness: distance melody. Genetic programmingSpector & Alpern, 1994jazz improvisationfitness: combination features. Genetic programmingDahlstedt, 2007contemporaryclassical musicfitness: combination features. Genetic programmingEsp et al., 2007melodyfitness: combination features extracted fropmcorpus melodies. Genetic programmingJensen, 2011melodyfitness: distance corpus melodies.Genetic programmingDaz-Jerez, 2011;Snchez-Quintana et al.,2013contemporaryclassical music,genressophisticated indirect encodingTable 16: References Section 3.5.1, order appearance.fitness function weighted sum series basic, local featuresmelody. Papadopoulos Wiggins (1998) implemented system that, givenchord progression, evolved jazz melodies relative pitch encoding, using fitness functionweighted sum eight evaluations characteristics melody, rangingsimple heuristics speed position notes user-specified contoursimilarity user-specified music fragments. similar approach implemented Harris(2008), modest (if promising) results. Towsey et al. (2001) actuallyimplement EA, discussed build fitness function melodic extension548fiAI Methods Algorithmic Composition(given composition, extend bars): proposed extract 21 statisticalcharacteristics corpus pre-specified compositions, defining fitness functionweighted sum distance individual mean characteristic.similar vein, Birchfield (2003) implemented fitness function giant weighted summany features hierarchical EA, multiple population levels, individualpopulation composed individuals lower populations (similar modeldescribed Biles, 1994; see Section 3.5.2). used output EA materialarrange long composition ten instruments. Garay Acevedo (2004) implementedsimple EA compose first species counterpoint, features weighted fitnessfunction simplistic, leading modest results. Lozano et al. (2009) generated chordsequences harmonize pre-specified melody two steps: first simple EA generatedset possible solutions according simple local considerations (appropriatenesschord corresponding part melody) chords notesmelody, variable neighborhood search used establish chord progressionaccording global considerations.alternative implementing fitness function weighted sum musical features use multi-objective evolutionary algorithms (MOEAs). However,rarely used, probably harder implement, conceptually practice. MOEAs used De Prisco et al. (2010) harmonizeunfigured bass27 Freitas Guimares (2011) harmonization.Another approach consists measuring fitness distance target composition corpus compositions. example, Gartland-Jones (2002) implemented EAcompose hybrids two pre-specified compositions (a goal similar Hamanakaet al.s, 2008, cited Section 3.1) using one seed initial population,distance (sum difference pitch note) fitness function. Alfonseca et al. (2005) used sophisticated evaluation: fitness compositionpopulation sum distances corpus pre-specified target compositions. metric normalized compression distance, measure different twosymbol strings are, based compressed lengths (both concatenated separated).zcan Eral (2008) used simple genetic representation fitness function basedweighted sum long list simple musical characteristics, reportedly ablegenerate improvisations pre-specified melody given harmonic context,specify evolved melodies related pre-specified music.EA implemented Wolkowicz et al. (2009), individuals encoded using relativepitches, sophisticated statistical analysis n-gram sequences pre-specified corpuscompositions used implement fitness function.Genetic programming also used fitness functions based comparisonspre-specified music. work Laine Kuuskankare (1994), individualsdiscrete functions time whose output (phenotype) interpreted sequencepitches. fitness simply sum differences pitches pre-specifiedtarget composition phenotype individual, similar way GartlandJones (2002). However, frequent find fitness functions based analysischaracteristics compositions evolving algorithm, often comparing27. description unfigured bass, see discussion Rothgebs (1968) work Section 3.2.549fiFernndez & Vicocharacteristics pre-specified training set compositions. Spector Alpern (1994)used genetic programming jazz improvisation, trading fours: individualsfunctions took input four produced another one applying seriestransformations it. fitness determined applying series score functionsmeasured rhythm, tonality features produced four, comparingdatabase high-quality examples renowned artists.Also using genetic programming, Dahlstedt (2007) composed relatively short contemporary classical music pieces, simple fitness function: set target valuesassigned several statistics compositions (as note density pitch standard deviation, among others), fitness weighted sum differencestarget values values individual. individuals trees whose nodesrepresented notes different operations musical sequences (using developmentalprocess genotype phenotype). Reportedly, generated pieces acceptablequality, genotype model especially well suited contemporary classicalmusic. Esp et al. (2007) used simple tree representation compositions (but withoutindirect encoding), defining fitness function weighted sum sophisticated statistical models melody description (measuring distance values setpre-specified compositions) several relatively simple characteristics composition.Jensen (2011) also used simple tree representation compositions; fitness calculated measuring frequency distributions simple events compositions, ratingaccording Zipfs law similarity pre-specified compositions.One latest successful results evolutionary computer music followsevo-devo strategy. Iamus computer cluster hybridizes bioinspired techniques:compositions evolve environment ruled formal constraints aesthetic principles (Daz-Jerez, 2011). compositions also develop genomic encodings wayresembles embryological development (hence evo-devo), providing high structuralcomplexity relatively low computational cost. composition resultevolutionary process instruments involved preferred durationspecified, included fitness function. Iamus write professional scorescontemporary classical music, published debut album September 2012(Ball, 2012; Coghlan, 2012), ten works interpreted first-class musicians (including LSO orchestra piece). Melomics, technology behind avant-gardecomputer-composer, also mastering genres transferring result industry(Snchez-Quintana et al., 2013). compiling myriad musical fragmentsessential styles browsable, web-based repository (Stieler, 2012). first time,Melomics offering music real commodity (priced size MIDI representation),ownership piece directly transferred buyer.3.5.2 Musical IGAsprevious exposition, apparent designing objective convenientfitness function evaluating music compositions difficult problem.28 musicevaluated terms subjective aesthetic quality, may become impractical28. point artists sometimes find unconventional ways around problem: Waschka (1999)argued solved problem assigning purely random fitness value individuals.550fiAI Methods Algorithmic CompositionReferenceComposition taskCommentsHartmann, 1990melodyinspired Dawkins biomorphsNelson, 1993melodyinspired Dawkins biomorphsHorowitz, 1994rhythmsinspired Dawkins biomorphsPazos et al., 1999rhythmsbinary genotypeDegazio, 1996melodybinary genotype; graphical representationBiles, 1994(GenJam)jazz improvisationtwo hierarchically structured populations(measures jazz phrases)Tokui & Iba, 2000rhythmstwo hierarchically structured populations(short long rhythmic patterns).Genetic programmingJacob, 1995melodyuser trains critics act fitness functionsSchmidl, 2008melodyuser trains critics act fitness functionsPutnam, 1994melodygenetic programmingAndo & Iba, 2007melodygenetic programmingMacCallum et al., 2012(DarwinTunes)melodygenetic programmingKaliakatsos-Papakostaset al., 2012melody, 8-bit soundsynthesisgenetic programmingHoover et al., 2012acompanimentuses CPPNsMcDermott & OReilly, 2011interactive generativemusicsimilar CPPNsRalley, 1995melodyminimizes user input clustering candidatesUnehara & Onisawa, 2001melodyminimizes user input elitismDaz-Jerez, 2011contemporary classicalmusicminimizes user fatigue producing small, goodcompositionsBeyls, 2003melodyuses cellular automata; graphical representationMoroni et al., 2000(Vox Populi)melodycomplex graphical representationVentrella, 2008melodywhole population comprises melodyMarques et al., 2010melodyminimizes evolutionary iterationsTable 17: References Section 3.5.2, order appearance.directly impossible define formal fitness function. inconveniences,many researchers resorted implement fitness function human evaluators.common term describing class EAs musical IGA (interactive genetic algorithm29 , MIGA short). MIGAs represent substantial percentage total bodywork EAs algorithmic composition, subsection devoted them.first MIGAs implemented composers intrigued concept evolutionary computing, resulting less peculiar architectures perspectivecommon practice evolutionary computing, also computer scientists exploring29. research tagged IGA use binary genotypes commonly associated termgenetic algorithm, term common.551fiFernndez & Vicofield. Hartmann (1990), inspired Dawkins biomorphs, presented one firstapplications evolutionary computing composition, MIGA unfortunately described notoriously laconic obscure language, resulting low citation ratework. Also inspired biomorphs, Nelson (1993) described toy MIGAevolving short rhythms fixed melodic structure, simple binary genotypes (eachbit simply denoted presence absence sound). formal models similar scopeNelsons designed Horowitz (1994) Pazos et al. (1999). implementedrhythm generators multiple instruments, one independent rhythmpattern encoded genotype (Horowitzs genotypes parametric, Pazos etal. used direct binary encodings). Degazio (1996) implemented systemgenotype set parameters (in later iterations, mini-language describeparameters) instruct CAAC software generate melodies.best known MIGA may GenJam, system generating jazz solos, developedseveral years. first incarnation (Biles, 1994), formulated MIGAtwo hierarchically structured populations: one measures, jazz phrases, constructed sequences measures. Given chord progression several parameters,jazz solos emerged concatenating selected phrases evolutionary process,fitness integrated time accumulating fixed increments decrementssimple good/bad indications evaluator. iterations system includedalready discussed use ANNs fitness functions (Biles et al., 1996) possibilitytrade fours human performer, dynamically introducing populationmusic performed human (Biles, 1998). Tokui Iba (2000) used similar solutioncreating rhythms multiple instruments: population short sequences specifiedlist notes, another population tree structures representing functions simplemacro language used short sequences building blocks. Another example hierarchical structuring MIGA Jacobs (1995) system general-purpose composition,three inter-dependent evolutionary processes: one involving human user trainears evaluate short musical sequences, another one compose musical phrases usingears (filters) fitness functions, another also involving human user trainarranger reorders resulting phrases final output system. Schmidl(2008) implemented similar system, without high-level arranger module,ears automatically trained set examples, order minimize user interactionenable real-time composition.Genetic programming interactive evaluation used several times. Putnam(1994) implemented early example: individual coded set functionsgenerated melody result iterated function system. Tokui Ibas (2000)example already cited. Ando Iba (2007) implemented fully interactivesystem (not selection, also reproduction mutation user-guided),genotype model similar Dahlstedts (2007). MacCallum et al. (2012) usedtrees encode Perl expressions generated polyphonic short loops, concentratedanalyzing interactive evolution point view theoretical biology. KaliakatsosPapakostas et al. (2012) used rather different approach generate 8-bit melodies:individual function composed bitwise operators generated waveformiterating function. fact, work might described sound synthesis largetime scales, rather music composition.552fiAI Methods Algorithmic CompositionMIGAs graph-based genetic representations also exist, implementationbased CPPNs (Hoover et al., 2012) already cited Section 3.4.2. McDermottOReilly (2011) used similar paradigm: genotypes sequences integersindirectly encoded graphs, whose nodes represented functions, connectionscompositions functions. output nodes generated musical output onevoices, modulated user inputs.problem common MIGAs user fatigue: candidate solution evaluationcomparatively slow monotone task rapidly leads user fatigue. Even smallpopulation sizes small numbers generations, remains significant problemsolved many researchers different ways. example, Ralley (1995) used binaryrepresentation relative pitch encoding genotypes, classified populationclustering algorithm, deriving similarity metrics rudimentary spectral analysisscores. user simply required evaluate closest composition centroidcluster. exotic solution employed previously cited Tokui Iba(2000): training neural network filter candidates low fitness, thus presentinguser individuals acceptable quality. Unehara Onisawa (2001) presented10% candidate melodies human user, parts genomes bestrated ones dispersed population horizontal gene transfer. McDermottOReilly (2011) also limited number candidates exposed user rating, filteringworst ones heuristic functions. Another option minimize user fatigueproduce small compositions already reasonably good. Melomics system (seelast paragraph Section 3.5.1) used way (Daz-Jerez, 2011).common ways manage problem include low population sizes and/or hierarchical structuring algorithm (Biles, 1994), providing statistical information and/orrendering graphical representations compositions order make possibleevaluation without actually listening (Degazio, 1996). Graphical representationsalso particularly useful using generative methods L-systems cellular automata(Beyls, 2003). Putnam (1994) used web interface reach volunteers. Moroniet al. (2000) tried solve problem using sophisticated GUI abstractions, complexnon-linear mappings graphic controls parameters fitness functionaspects evolutionary process, produce highly modulable systemreal-time interactive composition melodies. mitigate user fatigue, Ventrella (2008)presented population short melodies continuous stream sound; fitness obtained binary signal set user. Marques et al. (2010) limited user fatiguegeneration short, simple melodies severely limiting number generationsalgorithm, generating reasonably good starting population drawing notesusing Zipfs law.3.5.3 Population-Based MethodsFinally, subsection presents methods also population-based. example, metaheuristic method Harmony Search inspired improvisation processmusicians, though practice framed evolutionary method specific waystructure candidate solutions perform selection, crossover mutation operations.Geem Choi (2007) used method harmonize Gregorian chants (i.e., write or553fiFernndez & VicoReferenceComposition taskCommentsGeem & Choi, 2007harmonizeGregorian chantsharmony searchGeis & Middendorf, 2008four-partharmonizationmulti-objective Ant Colony OptimizationTominaga & Setomoto, 2008polyphony,counterpointartificial chemistryWerner & Todd, 1997melodyco-evolutionary algorithm, Markov chains usedevolvable fitness functionsBown & Wiggins, 2005melodyindividuals Markov chainscompose evaluate musicMiranda, 2002melodyindividuals agree common setintonation patternsMiranda et al., 2003,sect. IVmelodyindividuals grammars compose musicMcCormack, 2003binteractive soundscapeindividuals indirectly compete users attentionDahlstedt & Nordahl, 2001soundscapemusic emerges collective interactionsBeyls, 2007soundscapemusic emerges collective interactionsBlackwell & Bentley, 2002soundscapemusic emerges collective interactionsEldridge & Dorin, 2009soundscape,sound synthesismusic emerges collective interactions,individuals exist frequency domainBown & McCormack, 2010interactive soundscape,sound synthesismusic emerges collective interactions,individuals exist frequency domainTable 18: References Section 3.5.3, order appearance.ganum lines chants). methods also population-based properlyevolutionary. example use Ant Colony Optimization (ACO) solve constraintharmonization problems (Geis & Middendorf, 2008), already mentioned Section 3.2.3.ACO, candidate solutions represented paths graph, populationagents (ants) traverse graph, cooperating find optimal path.exotic example, Artificial Chemistry generative system consistingmultiset strings symbols. strings (analogues molecules) react accordingpre-specified set rules (analogues chemical reactions), generating new stringsexisting ones. Tominaga Setomoto (2008) used method, encoding polyphoniccompositions strings musical rules counterpoint reaction rulesartificial chemistry: starting set simple strings, system generated progressivelycomplex ones, though aesthetical value resulting compositions varied widely.popular method based populations individuals Artificial Ecosystem.artificial ecosystem, compositions emerge interaction individualssimulation evolutionary and/or cultural interactions, taking inspiration evolutionary origins music humans (Wallin & Merker, 2001). Frequently, complexitysimulations severely limited available computational power,cases goal music composition per se, study evolutionary dynamics,emergence shared cultural traits avant-garde artistic experimentation.554fiAI Methods Algorithmic Compositionearly example Werner Todd (1997) investigated sexual evolutionary dynamicspopulation males (small compositions) females (Markov chains initially generatedcorpus songs) evaluated much males deviated expectations.However, studies use one kind agent, work Bown Wiggins (2005),whose agents used Markov chains compose analyze music. Miranda (2002)Miranda, Kirby, Todd (2003, sect. IV) implemented models similar goal:study emergence common structures (shared cultural knowledge). first casepopulation agents strove imitate others intonation patterns (short sequencespitches); second case agents learned compose music inferring musical grammarsagents songs. Bosma (2005) extended Mirandas (2002) model, using neuralnetworks agents learn compose music, although tiny population sizes.part art installation, McCormack (2003b) proposed virtual ecosystem evolvingagents able compose music using rule-based system, competing resourcesindirectly determined interest human observers.alternative music composed agents, emergesepiphenomenon whole ecosystem. models Dahlstedt Nordahl (2001)Beyls (2007) used simple organisms two-dimensional space whose collective behaviormapped complex compositions, Blackwell Bentleys (2002) modelsimilar three-dimensional, dynamics agents inspired swarmflocking simulations. examples use either homogeneous spatiallystructured ecosystems, recent trend use sound environment itself.Eldridge Dorins (2009) model, agents dwelt one-dimensional spaceFourier transform sample ambient sound, feeding moving energy acrossfrequencies. Bown McCormack (2010) implemented similar model,agents neural networks generated sound competed room spacefrequencies ambient sound.3.6 Self-Similarity Cellular Automatalate 1970s, two notable results music reported Voss Clarke (1978).first that, music many different styles, spectral density audiosignal (approximately) inversely proportional frequency; words, approximately follows 1/f distribution. surprising: many different data seriesfollow property, meteorological data stock market prices; usually referred1/f noise pink noise. second result random compositions seemedmusical pleasing (for wide range evaluators, unskilled people professional musicians composers) pitches determined source 1/f noise,rather common random processes white (uncorrelated) noise Brownian motion (random walks). Although first result since challenged30 , second oneused composers source raw material. Bolognesi (1983) implementedearly example influenced Voss Clarkes results, composers used data series1/f noise raw material even results, early 1970s (Doornbusch, 2002).30. main criticism data samples used Voss Clarke hours long, mergingsample many different compositions (and even non-musical sounds radio station). view555fiFernndez & VicoReferenceComposition taskCommentsVoss & Clarke, 1978melodyfirst reference 1/f noise musicBolognesi, 1983melodyearly deliberate use 1/f noise musicDoornbusch, 2002melodyreference early non-deliberate use 1/f noise musicGogins, 1991melodyiterated function systemsPressing, 1988melody,sound synthesischaotic non-linear mapsHerman, 1993melodychaotic non-linear dynamical systemsLangston, 1989melodyfractional Brownian motionDaz-Jerez, 2000melodyfractals self-similar systemsBidlack, 1992melodyvarious fractal chaotic systemsLeach & Fitch, 1995(XComposer)melody, rhythmuses various fractal chaotic systemsHinojosa-Chapel, 2003melodyuses various fractal chaotic systems fillCoca et al., 2011melodyuses chaotic systems add variationTable 19: References Section 3.6, order appearance.remains question 1/f noise produces musical resultsrandom processes. consensus research artistic communities self-similarity:structure 1/f noise statistically similar across several orders magnitude (Farrellet al., 2006). Self-similarity common feature classical music compositions (Hs &Hs, 1991), also one defining features fractals (in fact, 1/f noise alsofractal characteristics). this, fractals extensively used sourceinspiration raw material compositions CAAC software. general, selfsimilar musical patterns multiple levels structure, pleasing regularitiesalso dotted sudden changes. characteristics also presentoutput chaotic systems (whose attractors also fractal structures), also usedgenerate musical patterns. Commonly used techniques generate self-similar musicalpatterns include chaotic systems iterated function systems (Gogins, 1991), nonlinear maps (Pressing, 1988) non-linear dynamical systems (Herman, 1993), alsofractional Brownian motion (Langston, 1989), cellular automata (discussed below) Lsystems (already discussed Section 3.1.1). exotic methods also possible,musical renderings fractal images number sequences fractal characteristics (DazJerez, 2000). methods widely regarded suitable produce melodiescompositions right, source inspiration raw material (Bidlack,1992). this, extensive review provided here.31 However, full-fledgedalgorithmic composition methods use part creative process, LeachFitchs (1995) XComposer, chaotic systems used fill structures laidcritics, Nettheim (1992), samples could possibly representative single musicalpieces (see also discussion Daz-Jerez, 2000, pp. 136138).31. Good (if somewhat outdated) reviews found work Jones (1989), Daz-Jerez (2000)Nierhaus (2009). list composers using fractals chaotic systems CAAC longimpractical consistently describe existing relevant work.556fiAI Methods Algorithmic Compositionhierarchical model, Hinojosa-Chapels (2003) paradigm interactivesystems, also used source musical material. also usedadd complexity compositions generated means.323.6.1 Cellular Automatacellular automaton (CA) discrete (in time, space state) dynamic system composedsimple computational units (cells) usually arranged ordered n-dimensional (andpotentially unbounded) grid (or regular tiling). cell one finitenumber states. discrete time step, cells state deterministically updated,using set transition rules take account state neighbors states.Although definition generalized multiple ways, represents good firstapproximation. Cellular automata used many disciplines across ScienceHumanities dynamical models complex spatial temporal patterns emerginglocal interaction many simple units; music composition one disciplines.Cellular automata used generate fractal patterns discrete versions chaoticdynamical systems, also represent alternative computational paradigm realizealgorithmic composition.33 Unfortunately, like fractals chaotic systems, CA alsotend produce interesting somewhat unmusical patterns used inspirationraw material rather directly music compositions. Although CA arguedbetter suited sound synthesis algorithmic composition (Miranda, 2007),latter application reviewed here.Xenakis known deeply interested application CA music.orchestral composition Horos, released 1986, widely regarded used CAconfigure structure composition, though heavily edited hand (Hoffmann, 2002). Early, better documented explorations CA music composition includeimplementations Beyls (1989), Millen (1990), Hunt et al. (1991), mappedpatterns generated user-defined CA MIDI output. Beyls (1989) presented CAgenerative system real-time composition avant-garde music, exploring several wayscomplexify generated musical patterns (as changing transition rules accordingmeta-rules), Millen (1990) presented minimalist CAAC system. Hunt et al. (1991)implemented another CAAC system designed give composer controlcomposition process. Echoing Beylss (1989) early work, Ariza (2007) proposed bendtransition rules order increase space parameters available composer experimentation, either randomly changing state isolated cells dynamicallychanging transition rules one generation next.CAMUS (Miranda, 1993) known CA system algorithmic compositioninnovative design using two bidimensional CA: Conways Game Life (used determinemusical sequences) Griffeaths Crystalline Growths (used determine instrumentation notes generated first CA). activated cell Game Lifemapped sequence three notes, whose instrument selected according corresponding cell second CA (the Crystalline Growths system). Unfortunately, accordingcreator (Miranda, 2007), CAMUS produce musical results: out32. See, e.g., description work Coca et al. (2011) Section 3.4.33. detailed survey, see e.g. work Burraston Edmonds (2005).557fiFernndez & VicoReferenceComposition taskCommentsHoffmann, 2002structurereference early use CA music (Xenakiss Horos)Beyls, 1989melodyearly use CA musicMillen, 1990melodyearly use CA musicHunt et al., 1991melodyearly use CA musicAriza, 2007melodydynamically changing CA rulesMiranda, 1993(CAMUS)melody,instrumentationtwo CA: one melody,instrumentationMcAlpine et al., 1999(CAMUS 3D)melody, rhythm,instrumentationabove, plus Markov chains select rhythmBilotta & Pantano, 2001melodyexplores several mappings CA music eventsDorin, 2002(Liquiprism)rhythmic patternsseveral interacting CABall, 2005, Miljkovic, 2007melody, rhythmreferences WolframTonesPhon-Amnuaisuk, 2010melodyuses ANNs learn CA rulesBeyls, 2003melodyinteractive evolutionary algorithmBilotta & Pantano, 2002melodyextends Bilotta Pantanos (2001) workevolutionary algorithmLo, 2012melodyevolutionary algorithm,Markov chains used fitness functionTable 20: References Section 3.6.1, order appearance.put properly considered raw material edited hand. CAMUS latergeneralized, using Markov chain determine note durations three-dimensionalversions Game Life Crystalline Growths (McAlpine et al., 1999).recently, Bilotta Pantano (2001) explored several different mappings generate music CA: local codes (mapping cells pitches, usual mappingpapers), global codes (mapping entropy whole pattern generation musical events) mixed codes (mapping groups cells musical events). Dorin (2002) usedsix bidimensional finite CA arranged cube (their edges connected), running different speeds, generate complex poly-rhythmic patterns.34 Finally, WolframTones35 (Ball,2005) commercial application CA music composition, using database fourbillions transition rules one-dimensional CA (all possible transition rules takingaccount five neighbors). WolframTones searches rules produce chaotic complexpatterns. patterns mapped musical events, system able searchpatterns whose musical mapping resembles one set pre-defined musical styles(Miljkovic, 2007).Although CA commonly used generate musical material uncontrolled way(i.e., composer tunes parameters CA hand), possible usemethods design CA (states, transition rules, etc.). example, Phon-Amnuaisuk34. Section 3.4.2, similar work (Dorin, 2000) Boolean networks (a connectionist paradigm usuallyseen generalization CA) mentioned.35. http://tones.wolfram.com/558fiAI Methods Algorithmic Composition(2010) used artificial neural networks trained learn transition rules CA: givenmelody, piano-roll notation interpreted temporal pattern CA,network trained learn transition rules temporal pattern. Then, giveninitial conditions, network produced new compositions piano-roll notation.Evolutionary algorithms also used design parameters (transition rules, states,etc.) CA. cases, previous work hand-designed CA adapted useevolutionary algorithm. case Beyls (2003), used interactive evolutionary algorithm evolve parameters CA, Bilotta Pantano (2002),adapted previously discussed work (Bilotta & Pantano, 2001) use evolutionaryalgorithm, although fitness function poorly described. Lo (2012) applied evolutionary algorithms generate CA algorithmic composition comprehensive way,experimenting various fitness functions based extracting statistical modelscorpus pre-existing compositions, including metrics based Markov models Zipfslaw.4. Conclusionssurvey, several hundreds papers algorithmic composition briefly reviewed. Obviously, none described detail. Rather, surveyintended short reference guide various methods commonly used algorithmiccomposition. Pearce et al. (2002) noted, papers algorithmic compositionadequately (a) specify precise practical theoretical aims research; (b) usemethodology achieve aims; (c) evaluate results controlled, measurablerepeatable way. Researchers algorithmic composition diverse backgrounds,and, many cases, present work way enables comparisonothers. considerations, presented literature narrativestyle, classifying existing work several broad categories, providing brief descriptions papers approximately chronological order category.4.1 CreativityAlgorithmic composition automates (to varying degrees) various tasks associatedmusic composition, generation melodies rhythms, harmonization, counterpoint orchestration. tasks applied two ways: (a) generate musicimitating corpus compositions specific style, (b) automate compositiontasks varying degrees, designing mere tools human composers, generatingcompositions without human intervention:Generating music imitating corpus compositions specific style.instances kind problem (including real-time improvisation systems elaborate input human musicians) considered solved: imitation problemstackled many different methods, many cases reasonable success(such Copes EMI, 1992, Pachets Continuator, 2002). fact, since originscomputational algorithmic composition, bias research community towards imitation problems (Nierhaus, 2009). may attributed559fiFernndez & Vicodifficulty merge mindset computer science (clear-cut definitions, precise algorithms, straight methodologies) mindset artistic work (intuition, vagueness,cultural heritage artistic influences). two mindsets may comparedcommon cultural divide Artificial Intelligence neats scruffies.Unfortunately, neats reign supreme Artificial Intelligence, yetgain upper hand Artificial Creativity.Automating composition tasks varying degrees. case automatedsystems algorithmic composition intended reproduce human creativityway, problem evaluating performance: concept artisticcreativity eludes formal, unambiguous effective definition. makes difficultevaluate systems completely rigorous way. Certainly, many frameworksproposed assessing computational creativity36 , one easilyuniformly applied computers humans alike, way sparkcontroversy. may seem simple measure computational creativity humanstandards: simply ask people listen human machine compositions,declare algorithmic composition system creative people cannot tellapart compositions human ones. Ariza (2009) noted, kind musicalTuring Test performed many different researchers trying validatesystems, valid algorithmic composition system aspires imitate,truly creative create truly innovative work art.also view systems algorithmic composition cannot attain true creativity, even principle. fact, suggested (Kugel, 1990) Turing-equivalentformalism truly simulate human creativity, i.e., musical creativity effectively computable, thus preventing computer systems completely imitating human composers,even theory. argument without merits, open debate, preciselylacks rigorous, unambiguous definition creativity.4.2 MethodsRegardless (more less abstract) considerations true creativity, surveypresented existing work algorithmic composition, organized several categories.described beginning Section 3, categories grouped classes:Symbolic AI (grammars rule-based systems). umbrella,grouped different techniques. techniques used imitation (bestyle specific composer, generally musical style) automationcomposition tasks. proved effective, popular (at least,sheer volume reviewed work), cases, labor-intensive,require musical knowledge encoded maintained symbolicframework choice. also clear trend towards formalsystems, gradually moving ad hoc rule systems constraint satisfactionvarious formalisms.36. example, Geros (2000), Pearce Wigginss (2001), Ritchies (2007) Bodens (2009).560fiAI Methods Algorithmic CompositionMachine learning (Markov chains artificial neural networks).nature, machine learning techniques used primarily imitation, althoughMarkov chains (and related statistical methods) artificial neural networksalso used automate composition tasks (such harmonization).noted techniques described symbolic AI also machine learning(like Copes ATNs, rule learning case-based reasoning).Optimization techniques (evolutionary algorithms). case machinelearning, optimization techniques (mostly evolutionary algorithms) profuselyused imitation, since natural express objective optimization (thefitness function) distance musical style imitated. However,automation composition tasks also explored, casemachine learning techniques.Self-similarity cellular automata. Strictly speaking, techniquesform AI. explained beginning Section 3, represent convenientway generate novel musical material without resorting human musical knowledge,problem musical material generated way rough;commonly used human composers raw material build upon.reviewing literature, becomes apparent silver bullet: exceptstrict, limited imitation specific musical styles real-time improvisation systemselaborate input human musicians, almost approaches algorithmic compositionseem unable produce content deemed par professionalhuman composers, even without taking account problem creativity discussedSection 4.1. examples stand out, niche applications,contemporary classical music composed Iamus (Ball, 2012).silver bullet, one obvious way forward hybridization twomethods. fact, review existing work, seems apparent manyresearchers already following route, hybridizationsrarely explored. example, music material produced systems basedself-similarity CA commonly regarded mere source inspiration humancomposers, rather proper way automate composition music,music material generally lacks structure. However, used first stageprocess algorithmic composition, modified refined subsequent stages,probably based form machine learning goal produce music specificmusical style, knowledge-based system (such Leach Fitchs XComposer,1995). case evolutionary algorithms, self-similarity systems may used seedinitial population, introduce variety avoid premature convergence, CA mayused individuals evolved (such work Lo, 2012, also featuresmachine learning techniques). research required explore potentialkind approach, combining self-similarity CA-based systems methods.case optimization techniques, multi-objective paradigm rarelyused, least comparison traditional single-objective approach. Composingmusic usually requires balancing set many different, sometimes conflicting objectives,configure various aspects music, multi-objective optimization seems natural561fiFernndez & Vicoway tackle problem. often, researchers use weighted sum parametersconflate objectives single fitness function. researchers usemulti-objective optimization, Geis Middendorf (2008), Carpentier Bresson(2010), De Prisco et al. (2010), Freitas Guimares (2011). multi-objectiveoptimization harder (both conceptually practice), represents natural waydealing complexity many different objectives, exploredresearch community.Finally, specific case evolutionary algorithms, issue encoding individuals also examined. Looking algorithmic composition optimizationproblem, search spaces musical compositions tend huge high-dimensional.Direct encodings (such directly representing music sequence pitches) makedifficult explore search space effective way, problems scalability(the performance degrades significantly size problem increases) solutionstructure (the solutions generated algorithm tend unstructured, hard adaptfragile). problem mitigated indirect encodings, genotypedirectly represent phenotype, rather list instructions build it. Manydifferent types indirect encoding used algorithmic composition,L-systems, types grammars, various encoding styles used genetic programming. However, advanced techniques indirect encoding rarelyapplied algorithmic composition, order overcome aforementioned problemsscalability solution structure, related artificial embryogenies (Stanley& Miikkulainen, 2003), inspired biological developmental processes.Adding evolutionary toolkit may way enable complexcompositional tasks tackled.4.3 Final ThoughtsComputers come stay: use CAAC software prevalent among many composers, artistic scenes (as generative music) embrace computer-generated musicpart identity. However, creativity still hands composers part.argued Section 4.1, creativity inherently subjective concept, arguablydebatable point computational system may become truly creative. However,even precise definition cannot agreed upon, easy see developmentalgorithmic composition systems capable independent creativity radically changeprocess music composition, consequently market music.seen yet another case computers replacing humans ever sophisticatedactivity, potentially radical disruption way composers perform work:like pedagogical expert system supersedes role human teachers, enablenew ways work.music one arts stronger mathematical background, surprisingdebate whether machines make original creative workscentered subfield computational creativity. Hybridization different techniques,bioinspiration, use high performance computing might bring new realms(computer-) creativity. science writer Philip Ball put analysis Melomics562fiAI Methods Algorithmic Compositionmusic composition technology: . . . unfolding complex structure mutable coreenabled kind dramatic invention found biological evolution (Ball, 2012).Acknowledgmentsauthors wish thank Ilias Bergstrom comments preliminary versionmanuscript. Also, critical review anonymous referees greatly improvedfinal version. study partially supported grant MELOMICS project(IPT-300000-2010-010) Spanish Ministerio de Ciencia e Innovacin, grantCAUCE project (TSI-090302-2011-8) Spanish Ministerio de Industria, TurismoComercio. first author supported grant GENEX project (P09-TIC5123) Consejera de Innovacin Ciencia de Andaluca. first author alsowishes thank wife Elisa daughter Isabel day day, spitelong hours spent writing manuscript, family invaluable supportprovided.ReferencesAdiloglu, K., & Alpaslan, F. N. (2007). machine learning approach two-voice counterpoint composition. Knowledge-Based Systems, 20 (3), 300309.Aguilera, G., Galn, J. L., Madrid, R., Martnez, A. M., Padilla, Y., & Rodrguez, P. (2010).Automated generation contrapuntal musical compositions using probabilistic logicDerive. Mathematics Computers Simulation, 80 (6), 12001211.Alfonseca, M., Cebrin, M., & Ortega, A. (2005). Evolving computer-generated musicmeans normalized compression distance. Proceedings WSEASInternational Conference Simulation, Modelling Optimization, pp. 343348,Stevens Point, Wisconsin, USA.Allan, M. (2002). Harmonising chorales style Johann Sebastian Bach. Mastersthesis, University Edinburgh.Allombert, A., Assayag, G., Desainte-Catherine, M., & Rueda, C. (2006). Concurrent constraints models interactive scores. Proceedings Sound Music Computing Conference.Ames, C. (1987). Automated composition retrospect: 1956-1986. Leonardo, 20 (2), 169185.Ames, C. (1989). Markov process compositional model: survey tutorial.Leonardo, 22 (2), 175187.Ames, C., & Domino, M. (1992). Understanding music AI, chap. Cybernetic composer:overview, pp. 186205. MIT Press, Cambridge.Amiot, E., Noll, T., Agon, C., & Andreatta, M. (2006). Fourier oracles computer-aidedimprovisation. Proceedings International Computer Music Conference.Anders, T. (2007). Composing Music Composing Rules: Design Usage GenericMusic Constraint System. Ph.D. thesis, Queens University Belfast.563fiFernndez & VicoAnders, T., & Miranda, E. R. (2009). computational model generalises Schoenbergsguidelines favourable chord progressions. Proceedings Sound MusicComputing Conference.Anders, T., & Miranda, E. R. (2011). Constraint programming systems modeling musictheories composition. ACM Computing Surveys, 43 (4), 30:130:38.Ando, D., & Iba, H. (2007). Interactive composition aid system means tree representation musical phrase. Proceedings IEEE Conference EvolutionaryComputation, pp. 42584265.Ariza, C. (2005a). Navigating landscape computer aided algorithmic compositionsystems: definition, seven descriptors, lexicon systems research.Proceedings International Computer Music Conference.Ariza, C. (2005b). Open Design Computer-Aided Algorithmic Music Composition:athenaCL. Ph.D. thesis, New York University.Ariza, C. (2006). Beyond transition matrix: language-independent, string-based inputnotation incomplete, multiple-order, static Markov transition values. Unpublishedmanuscript.Ariza, C. (2007). Automata bending: Applications dynamic mutation dynamic rulesmodular One-Dimensional cellular automata. Computer Music Journal, 31 (1),2949.Ariza, C. (2009). interrogator critic: Turing test evaluation generativemusic systems. Computer Music Journal, 33 (2), 4870.Ariza, C. (2011). Two pioneering projects early history computer-aided algorithmic composition. Computer Music Journal, 35 (3), 4056.Aschauer, D. (2008). Algorithmic composition. Masters thesis, Vienna University Technology.Assayag, G., & Dubnov, S. (2004). Using factor oracles machine improvisation. SoftComputing - Fusion Foundations, Methodologies Applications, 8 (9), 604610.Assayag, G., Rueda, C., Laurson, M., Agon, C., & Delerue, O. (1999). Computer-Assistedcomposition IRCAM: PatchWork OpenMusic. Computer Music Journal,23 (3), 5972.Baeten, J. C. M. (2005). brief history process algebra. Theoretical Computer Science,335 (23), 131146.Baggi, D. L. (1991). Neurswing: intelligent workbench investigation swingjazz. Computer, 24 (7), 6064.Balaban, M., Ebciolu, K., & Laske, O. E. (1992). Understanding music AI : perspectives music cognition. MIT Press, Cambridge.Ball, P. (2005). Making music numbers online. Nature News Online(http://dx.doi.org/10.1038/050919-14).Ball, P. (2012). Computer science: Algorithmic rapture. Nature, 488 (7412), 458.564fiAI Methods Algorithmic CompositionBaroni, M., & Jacoboni, C. (1978). Proposal grammar melody : Bach chorales.Les Presses de lUniversit de Montral.Bel, B. (1992). Modelling improvisatory compositional processes. Languages Design,Formalisms Word, Image Sound, 1, 1126.Bellgard, M. I., & Tsang, C. P. (1992). Harmonizing music using network Boltzmannmachines. Proceedings Annual Conference Artificial Neural NetworksApplications, pp. 321332, France.Berg, P. (2011). Using AC Toolbox. Institute Sonology, Royal Conservatory,Hague.Beyls, P. (1989). musical universe cellular automata. Proceedings International Computer Music Conference, pp. 3441.Beyls, P. (2003). Selectionist musical automata: Integrating explicit instruction evolutionary algorithms. Proceedings Brazilian Symposium Computer Music.Beyls, P. (2007). Interaction self-organisation society musical agents. Proceedings European Conference Artificial Life.Bidlack, R. (1992). Chaotic systems simple (but complex) compositional algorithms.Computer Music Journal, 16 (3), 3347.Biles, J. A. (1994). GenJam: genetic algorithm generating jazz solos. ProceedingsInternational Computer Music Conference.Biles, J. A. (1998). Interactive GenJam: Integrating real-time performance geneticalgorithm. Proceedings International Computer Music Conference.Biles, J. A., Anderson, P., & Loggi, L. (1996). Neural network fitness functionsmusical IGA. Proceedings International Symposium Intelligent IndustrialAutomation Soft Computing.Bilotta, E., & Pantano, P. (2001). Artificial life music tells complexity. ProceedingsEuropean Conference Artificial Life.Bilotta, E., & Pantano, P. (2002). Synthetic harmonies: approach musical semiosismeans cellular automata. Leonardo, 35 (2), 153159.Birchfield, D. A. (2003). Evolving intelligent musical materials. Ph.D. thesis, ColumbiaUniversity, New York.Biyikoglu, K. M. (2003). Markov model chorale harmonization. ProceedingsTriennial ESCOM Conference.Blackwell, T. M., & Bentley, P. (2002). Improvised music swarms. ProceedingsIEEE Conference Evolutionary Computation, pp. 14621467.Boden, M. A. (2009). Computer models creativity. AI Magazine, 30 (3), 2334.Boenn, G., Brain, M., De Vos, M., & Fitch, J. (2008). Automatic composition melodicharmonic music Answer Set Programming. Proceedings InternationalConference Logic Programming, pp. 160174.Bolognesi, T. (1983). Automatic composition: Experiments self-similar music. Computer Music Journal, 7 (1), 2536.565fiFernndez & VicoBosma, M. (2005). Musicology virtual world: bottom approach studymusical evolution. Masters thesis, University Groningen University Plymouth.Boulanger, R. C. (Ed.). (2000). Csound Book: Perspectives Software Synthesis,Sound Design, Signal Processing, Programming. MIT Press.Bown, O., & McCormack, J. (2010). Taming nature: tapping creative potentialecosystem models arts. Digital Creativity, 21 (4), 215231.Bown, O., & Wiggins, G. A. (2005). Modelling musical behaviour cultural-evolutionarysystem. Proceedings International Joint Conference Artificial Inteligence.Brooks, F. P., Hopkins, A. L., Neumann, P. G., & Wright, W. V. (1957). experimentmusical composition. IRE Transactions Electronic Computers, EC-6 (3), 175182.Browne, T. M., & Fox, C. (2009). Global Expectation-Violation fitness function evolutionary composition. Proceedings Conference Applications EvolutionaryComputation, pp. 538546.Bryden, K. (2006). Using Human-in-the-Loop evolutionary algorithm create DataDriven music. Proceedings IEEE Conference Evolutionary Computation,pp. 20652071.Bulley, J., & Jones, D. (2011). Variable 4: dynamical composition weather systems.Proceedings International Computer Music Conference.Burns, K. (1994). History Development Algorithms Music Composition,1957-1993. Ph.D. thesis, Ball State University.Burraston, D., & Edmonds, E. (2005). Cellular automata generative electronic musicsonic art: historical technical review. Digital Creativity, 16 (3), 165185.Burton, A. R. (1998). Hybrid Neuro-Genetic Pattern Evolution System Applied MusicalComposition. Ph.D. thesis, University Surrey.Burton, A. R., & Vladimirova, T. (1999). Generation musical sequences genetictechniques. Computer Music Journal, 23 (4), 5973.Buttram, T. (2003). DirectX 9 Audio Exposed: Interactive Audio Development, chap. Beyond Games: Bringing DirectMusic Living Room. Wordware Publishing Inc.Carpentier, G., & Bresson, J. (2010). Interacting symbol, sound, feature spacesorchide, computer-aided orchestration environment. Computer Music Journal,34 (1), 1027.Chemillier, M. (2004). Toward formal study jazz chord sequences generated Steedmans grammar. Soft Computing - Fusion Foundations, Methodologies Applications, 8 (9), 617622.Chemillier, M., & Truchet, C. (2001). Two musical CSPs. Proceedings InternationalConference Principles Practice Constraint Programming.Chen, C. C. J., & Miikkulainen, R. (2001). Creating melodies evolving recurrent neuralnetworks. Proceedings International Joint Conference Neural Networks,pp. 22412246.566fiAI Methods Algorithmic CompositionChusid, I. (1999). Beethoven-in-a-box: Raymond scotts electronium. Contemporary MusicReview, 18 (3), 914.Coca, A. E., Romero, R. A. F., & Zhao, L. (2011). Generation composed musical structures recurrent neural networks based chaotic inspiration. ProceedingsInternational Joint Conference Neural Networks, pp. 32203226.Coghlan, A. (2012).215 (2872), 7.Computer composer honours Turings centenary.New Scientist,Cohen, J. E. (1962). Information theory music. Behavioral Science, 7 (2), 137163.Collins, N. (2009). Musical form algorithmic composition. Contemporary Music Review,28 (1), 103114.Conklin, D. (2003). Music generation statistical models. Proceedings Symposium Artificial Intelligence Creativity Arts Science.Conklin, D., & Witten, I. (1995). Multiple viewpoint systems music prediction. JournalNew Music Research, 24 (1), 5173.Cope, D. (1992). Computer modeling musical intelligence EMI. Computer MusicJournal, 16 (2), 6983.Cope, D. (2000). Algorithmic Composer. A-R Editions.Cope, D. (2005). Computer Models Musical Creativity. MIT Press, Cambridge.Courtot, F. (1990). constraint-based logic program generating polyphonies. Proceedings International Computer Music Conference, pp. 292294.Cruz-Alczar, P. P., & Vidal-Ruiz, E. (1998). Learning regular grammars model musical style: Comparing different coding schemes. Proceedings InternationalColloquium Grammatical Inference, pp. 211222.Dahlstedt, P. (2007). Autonomous evolution complete piano pieces performances.Proceedings European Conference Artificial Life.Dahlstedt, P., & Nordahl, M. G. (2001). Living melodies: Coevolution sonic communication. Leonardo, 34 (3), 243248.Dalhoum, A. A., Alfonseca, M., Cebrin, M., Snchez-Alfonso, R., & Ortega, A. (2008).Computer-generated music using grammatical evolution. Proceedings MiddleEast Simulation Multiconference, pp. 5560.Davismoon, S., & Eccles, J. (2010). Combining musical constraints Markov transitionprobabilities improve generation creative musical structures. ProceedingsEuropean Conference Applications Evolutionary Computation.De Prisco, R., Zaccagnino, G., & Zaccagnino, R. (2010). EvoBassComposer: multiobjective genetic algorithm 4-voice compositions. Proceedings GeneticEvolutionary Computation Conference.Degazio, B. (1996). evolution musical organisms. Leonardo Music Journal, 7, 2733.Daz-Jerez, G. (2000). Algorithmic Music Using Mathematical Models. Ph.D. thesis, Manhattan School Music.567fiFernndez & VicoDaz-Jerez, G. (2011). Composing Melomics: Delving computational worldmusical inspiration. Leonardo Music Journal, 21, 1314.Dobrian, C. (1993). Music artificial intelligence. Unpublished manuscript. Availablehttp://music.arts.uci.edu/dobrian/CD.music.ai.htm.Doornbusch, P. (2002). brief survey mapping algorithmic composition. ProceedingsInternational Computer Music Conference.Dorin, A. (2000). Boolean networks generation rhythmic structure. ProceedingsAustralasian Computer Music Conference, pp. 3845.Dorin, A. (2002). Liquiprism : Generating polyrhythms cellular automata. Proceedings International Conference Auditory Display.Drewes, F., & Hgberg, J. (2007). algebra tree-based music generation. ProceedingsInternational Conference Algebraic Informatics, pp. 172188.Dubnov, S., Assayag, G., Lartillot, O., & Bejerano, G. (2003). Using machine-learningmethods musical style modeling. Computer, 36 (10), 7380.DuBois, R. L. (2003). Applications Generative String-Substitution Systems ComputerMusic. Ph.D. thesis, Columbia University.Duff, M. O. (1989). Backpropagation Bachs 5th cello suite (Sarabande). ProceedingsInternational Joint Conference Neural Networks, p. 575.Ebciolu, K. (1980). Computer counterpoint. Proceedings International ComputerMusic Conference.Ebciolu, K. (1988). expert system harmonizing four-part chorales. Computer MusicJournal, 12 (3), 4351.Edwards, M. (2011). Algorithmic composition: computational thinking music. Communications ACM, 54 (7), 5867.Eigenfeldt, A., & Pasquier, P. (2010). Realtime generation harmonic progressions using controlled Markov selection. Proceedings International ConferenceComputational Creativity.Eldridge, A., & Dorin, A. (2009). Filterscape: Energy recycling creative ecosystem.Proceedings Conference Applications Evolutionary Computation.Eno, B. (1996). Generative Music, speech Imagination Conference. Availablehttp://www.inmotionmagazine.com/eno1.html.Esp, D., Ponce de Len, P. J., Prez-Sancho, C., Rizo, D., nesta, J. I., Moreno-Seco, F., &Pertusa, A. (2007). cooperative approach style-oriented music composition.Proceedings International Joint Conference Artificial Inteligence.Farbood, M., & Schoner, B. (2001). Analysis synthesis Palestrina-style counterpointusing Markov chains. Proceedings International Computer Music Conference.Farnell, A. (2007). introduction procedural audio itsapplication computergames. Proceedings Audio Mostly Conference.Farrell, S., Jan Wagenmakers, E., & Ratcliff, R. (2006). 1/f noise human cognition:ubiquitous, mean?. Psychonomic Bulletin & Review, 13 (4), 737741.568fiAI Methods Algorithmic CompositionFeulner, J., & Hrnel, D. (1994). MELONET: neural networks learn harmony-basedmelodic variations. Proceedings International Computer Music Conference,pp. 121124, San Francisco.Fox, C. W. (2006). Genetic hierarchical music structures. Proceedings InternationalFlorida Artificial Research Society Conference.Franklin, J. (2001). Multi-phase learning jazz improvisation interaction. Proceedings Biennial Symposium Arts Technology.Freitas, A., & Guimares, F. (2011). Melody harmonization evolutionary music usingmultiobjective genetic algorithms. Proceedings Sound Music ComputingConference.Fry, C. (1984). Flavors band: language specifying musical style. Computer MusicJournal, 8 (4), 2034.Garay Acevedo, A. (2004). Fugue composition counterpoint melody generation usinggenetic algorithms. Proceedings International Conference Computer MusicModeling Retrieval, pp. 96106.Gartland-Jones, A. (2002). genetic algorithm think like composer?. ProceedingsGenerative Art Conference.Geem, Z. W., & Choi, J. Y. (2007). Music composition using harmony search algorithm.Proceedings Conference Applications Evolutionary Computation, pp.593600.Geis, M., & Middendorf, M. (2008). Creating melodies baroque harmonies antcolony optimization. International Journal Intelligent Computing Cybernetics,1 (2), 213218.Gero, J. S. (2000). Computational models innovative creative design processes.Technological Forecasting Social Change, 64 (23), 183196.Gibson, P. M., & Byrne, J. A. (1991). NEUROGEN, musical composition using geneticalgorithms cooperating neural networks. Proceedings International Conference Artificial Neural Networks, pp. 309313.Gill, S. (1963). technique composition music computer. ComputerJournal, 6 (2), 129133.Gillick, J., Tang, K., & Keller, R. M. (2009). Learning jazz grammars. ProceedingsSound Music Computing Conference, pp. 125130.Gjerdingen, R. (1988). Explorations Music, Arts, Ideas: Essays HonorLeonard B. Meyer, chap. Concrete musical knowledge computer programspecies counterpoint, pp. 199228. Pendragon Press.Gogins, M. (1991). Iterated functions systems music. Computer Music Journal, 15 (1),4048.Gogins, M. (2006). Score generation voice-leading chord spaces. ProceedingsInternational Computer Music Conference.569fiFernndez & VicoGoldman, C., Gang, D., Rosenschein, J., & Lehmann, D. (1996). NETNEG: hybridinteractive architecture composing polyphonic music real time. ProceedingsInternational Computer Music Conference, pp. 133140.Grachten, M. (2001). JIG : jazz improvisation generator. Proceedings WorkshopCurrent Research Directions Computer Music, pp. 16. Audiovisual Institute-UPF.Gwee, N. (2002). Complexity Heuristics Rule-Based Algorithmic Music Composition.Ph.D. thesis, Louisiana State University.Hamanaka, M., Hirata, K., & Tojo, S. (2008). Melody morphing method based GTTM.Proceedings International Computer Music Conference, pp. 155158.Harris, R. (2008). Algorithmic composition jazz. Masters thesis, University Bath.Hartmann, P. (1990). Natural selection musical identities. Proceedings International Computer Music Conference.Haus, G., & Sametti, A. (1991). Scoresynth: system synthesis music scoresbased Petri nets music algebra. IEEE Computer, 24 (7), 5660.Herman, M. (1993). Deterministic chaos, iterative models, dynamical systemsapplication algorithmic composition. Proceedings International ComputerMusic Conference.Herremans, D., & Sorensena, K. (2012). Composing first species counterpoint variableneighbourhood search algorithm. Journal Mathematics Arts, 6 (4), 169189.Hild, H., Feulner, J., & Menzel, D. (1992). HARMONET: neural net harmonisingchorales style J.S. Bach. Proceedings Conference Neural Information Processing Systems.Hiller, L. A., & Isaacson, L. M. (1958). Musical composition High-Speed digitalcomputer. Journal Audio Engineering Society, 6 (3), 154160.Hinojosa-Chapel, R. (2003). Realtime algorithmic music systems fractals chaoticfunctions: Toward active musical instrument. Masters thesis, Universitat PompeuFabra.Hirata, K., & Aoyagi, T. (1988). realize jazz feelings: logic programming approach. Proceedings International Conference Fifth Generation ComputerSystems.Hoffmann, P. (2002). Towards "automated art": Algorithmic processes xenakis compositions. Contemporary Music Review, 21 (2-3), 121131.Holm, F. (1990). CESAM: concept engine synthesis audio music. ProceedingsInternational Computer Music Conference.Holtzman, S. R. (1981). Using generative grammars music composition. ComputerMusic Journal, 5 (1), 5164.Hoover, A. K., Szerlip, P. A., Norton, M. E., Brindle, T. A., Merritt, Z., & Stanley, K. O.(2012). Generating complete multipart musical composition single monophonic melody functional scaffolding. Proceedings International Conference Computational Creativity.570fiAI Methods Algorithmic CompositionHrnel, D., & Degenhardt, P. (1997). neural organist improvising baroque-style melodicvariations. Proceedings International Computer Music Conference, pp. 430433.Hrnel, D., & Ragg, T. (1996). connectionist model evolution styles harmonization. Proceedings International Conference Music PerceptionCognition.Horner, A., & Ayers, L. (1995). Harmonization musical progressions genetic algorithms. Proceedings International Computer Music Conference.Horner, A., & Goldberg, D. E. (1991). Genetic algorithms computer-assisted musiccomposition. Proceedings International Conference Genetic Algorithms,pp. 337441.Horowitz, M. D. (1994). Generating rhythms genetic algorithms. ProceedingsAAAI National Conference Artificial intelligence, Menlo Park.Horowitz, M. D. (1995). Representing musical knowledge. Ph.D. thesis, Columbia University.Hs, K. J., & Hs, A. (1991). Self-similarity "1/f noise" called music. ProceedingsNational Academy Sciences United States America, 88 (8), 35073509.Hunt, A., Kirk, R., & Orton, R. (1991). Musical applications cellular automata workstation. Proceedings International Computer Music Conference.Jacob, B. L. (1995). Composing genetic algorithms. Proceedings InternationalComputer Music Conference.Jensen, J. H. (2011). Evolutionary music composition: quantitative approach. Mastersthesis, Norwegian University Science Technology.Johanson, B., & Poli, R. (1998). GP-music: interactice genetic programming systemmusic generation automated fitness raters. Proceedings AnnualConference Genetic Programming, pp. 181186.Johnson, M., Tauritz, D. R., & Wilkerson, R. (2004). Evolutionary computation appliedmelody generation. Proceedings Artificial Neural Networks Engineering(ANNIE) Conference.Jones, K. (1980). space grammar stochastic generation Multi-Dimensionalstructures. Proceedings International Computer Music Conference.Jones, K. (1981). Compositional applications stochastic processes. Computer MusicJournal, 5 (2), 4561.Jones, K. (1989). Generative models computer-assisted musical composition. Contemporary Music Review, 3 (1), 177196.Kaliakatsos-Papakostas, M. A., Epitropakis, M. G., Floros, A., & Vrahatis, M. N. (2012).Interactive evolution 8-Bit melodies genetic programming towards finding aesthetic measures sound evolutionary biologically inspired music, sound, artdesign. Proceedings International Conference Evolutionary Biologically Inspired Music, Sound, Art Design, pp. 141152.571fiFernndez & VicoKeller, R. M., & Morrison, D. R. (2007). grammatical approach automatic improvisation. Proceedings Sound Music Computing Conference, pp. 330337.Khalifa, Y. M. A., Khan, B. K., Begovic, J., Wisdom, A., & Wheeler, A. M. (2007). Evolutionary music composer integrating formal grammar. Proceedings GeneticEvolutionary Computation Conference, pp. 25192526, New York.Kippen, J., & Bel, B. (1989). identification modelling percussion language,emergence musical concepts machine-learning experimental set-up.Computers Humanities, 23 (3), 199214.Kirke, A., & Miranda, E. (2009). survey computer systems expressive musicperformance. ACM Computing Surveys, 42 (1), 3:13:41.Kitani, K. M., & Koike, H. (2010). ImprovGenerator: Online grammatical induction onthe-fly improvisation accompaniment. Proceedings International ConferenceNew Interfaces Musical Expression.Klinger, R., & Rudolph, G. (2006). Evolutionary composition music learned melodyevaluation. Proceedings WSEAS International Conference ComputationalIntelligence, Man-Machine Systems Cybernetics, pp. 234239, Stevens Point, Wisconsin, USA.Kohonen, T., Laine, P., Tiits, K., & Torkkola, K. (1991). Music Connectionism, chap.Nonheuristic Automatic Composing Method, pp. 229242. MIT Press, Cambridge.Kugel, P. (1990). Myhills Thesis: Theres computing musical thinking. Computer Music Journal, 14 (3), 1225.Laine, P. (2000). Method Generating Musical Motion Patterns. Ph.D. thesis, University Helsinki.Laine, P., & Kuuskankare, M. (1994). Genetic algorithms musical style oriented generation. Proceedings IEEE Conference Evolutionary Computation, pp.858862.Langston, P. (1989). Six techniques algorithmic music composition. ProceedingsInternational Computer Music Conference.Laurson, M., & Kuuskankare, M. (2000). Towards idiomatic instrumental writing: constraint based approach. Proceedings Annual Symposium Systems ResearchArts.Leach, J., & Fitch, J. (1995). Nature, music, algorithmic composition. Computer MusicJournal, 19 (2), 2333.Lerdahl, F., Jackendoff, R., & Jackendoff, R. S. (1983). Generative Theory TonalMusic. MIT Press, Cambridge.Levitt, D. A. (1981). melody description system jazz improvisation. Masters thesis,Massachusetts Institute Technology.Lewis, J. P. (1991). Music Connectionism, chap. Creation refinement problem algorithmic music composition. MIT Press, Cambridge.572fiAI Methods Algorithmic CompositionLidov, D., & Gabura, J. (1973). melody writing algorithm using formal language model.Computer Studies Humanities Verbal Behavior, 4 (34), 138148.Lo, M. Y. (2012). Evolving Cellular Automata Music Composition Trainable FitnessFunctions. Ph.D. thesis, University Essex.Lo, M., & Lucas, S. M. (2006). Evolving musical sequences N-Gram based trainable fitness functions. Proceedings IEEE Conference Evolutionary Computation,pp. 601608.Lthe, M. (1999). Knowledge based automatic composition variation melodiesminuets early classical style. Proceedings Annual German ConferenceArtificial Intelligence, pp. 159170.Loy, G., & Abbott, C. (1985). Programming languages computer music synthesis, performance, composition. ACM Computing Surveys, 17 (2), 235265.Lozano, L., Medaglia, A. L., & Velasco, N. (2009). Generation Pop-Rock chord sequencesusing genetic algorithms variable neighborhood search. ProceedingsConference Applications Evolutionary Computation, pp. 573578.Lyon, D. (1995). Using stochastic Petri nets real-time Nth-order stochastic composition.Computer Music Journal, 19 (4), 1322.MacCallum, R. M., Mauch, M., Burt, A., & Leroi, A. M. (2012). Evolution musicpublic choice. Proceedings National Academy Sciences United StatesAmerica, 109 (30), 1208112086.Maddox, T., & Otten, J. (2000). Using evolutionary algorithm generate Four-Part18th century harmony. Proceedings WSEAS International ConferenceMathematics Computers Business Economics.Manaris, B., Hughes, D., & Vassilandonakis, Y. (2011). Monterey mirror: CombiningMarkov models, genetic algorithms, power laws. Proceedings IEEEConference Evolutionary Computation.Manaris, B., Roos, P., Machado, P., Krehbiel, D., Pellicoro, L., & Romero, J. (2007).corpus-based hybrid approach music analysis composition. ProceedingsAAAI National Conference Artificial intelligence, pp. 839845.Manousakis, S. (2006). Musical L-Systems. Masters thesis, Royal Conservatory,Hague.Marques, V. M., Oliveira, V., Vieira, S., & Rosa, A. C. (2000). Music composition using genetic evolutionary algorithms. Proceedings IEEE Conference EvolutionaryComputation, pp. 714719.Marques, V. M., Reis, C., & Machado, J. A. T. (2010). Interactive evolutionary computationmusic. Proceedings IEEE International Conference Systems, ManCybernetics, pp. 35013507.Martin, A., Jin, C. T., & Bown, O. (2012). Implementation real-time musical decisionmaker. Proceedings Australasian Computer Music Conference.573fiFernndez & VicoMartin, A., Jin, C. T., van Schaik, A., & Martens, W. L. (2010). Partially observable Markovdecision processes interactive music systems. Proceedings InternationalComputer Music Conference.Mason, S., & Saffle, M. (1994). L-Systems, melodies musical structure. Leonardo MusicJournal, 4, 3138.Maurer, J. (1999). brief history algorithmic composition. Unpublished manuscript.Available https://ccrma.stanford.edu/~blackrse/algorithm.html.McAlpine, K., Miranda, E., & Hoggar, S. (1999). Making music algorithms: CaseStudy system. Computer Music Journal, 23 (2), 1930.McCartney, J. (2002). Rethinking computer music language: SuperCollider. ComputerMusic Journal, 26 (4), 6168.McCormack, J. (1996). Grammar-Based music composition. Complexity International, 3,320336.McCormack, J. (2003a). Application L-systems Developmental Models Computer Art, Animation Music Synthesis. Ph.D. thesis, Monash University.McCormack, J. (2003b). Evolving sonic ecosystems. Kybernetes, 32 (12), 184202.McDermott, J., & OReilly, U. M. (2011). executable graph representation evolutionary generative music. Proceedings Genetic Evolutionary ComputationConference, pp. 403410, New York.McGuire, K. (2006). ArpEgg: rewriting grammar complex arpeggios. ProceedingsGenerative Art Conference.McIntyre, R. A. (1994). Bach box: evolution four part baroque harmony using genetic algorithm. Proceedings IEEE Conference EvolutionaryComputation, pp. 852857.Melo, A. F. (1998). connectionist model tension chord progressions. Masters thesis,University Edinburgh.Miljkovic, K. (2007). Mathematica live performance: Mapping simple programsmusic. Proceedings International Conference Mathematics Computation Music.Millen, D. (1990). Cellular automata music. Proceedings International ComputerMusic Conference.Miranda, E. R. (1993). Cellular automata music: interdisciplinary project. JournalNew Music Research, 22 (1), 321.Miranda, E. R. (2002). Mimetic development intonation. Proceedings International Conference Music Artificial Intelligence.Miranda, E. R. (2007). Evolutionary Computer Music, chap. Cellular Automata Music:Sound Synthesis Musical Forms, pp. 170193. Springer-Verlag London.Miranda, E. R., & Biles, J. A. (Eds.). (2007). Evolutionary computer music. Springer-VerlagLondon.574fiAI Methods Algorithmic CompositionMiranda, E. R., Kirby, S., & Todd, P. M. (2003). computational models evolution music: origins musical taste emergence grammars.Contemporary Music Review, 22 (3), 91111.Moorer, J. A. (1972). Music computer composition. Communications ACM,15 (2), 104113.Morales, E., & Morales, R. (1995). Learning musical rules. Proceedings International Joint Conference Artificial Inteligence.Morgan, N. (2007). Transformation mapping L-Systems data compositionlarge-scale instrumental work. Proceedings European Conference ArtificialLife.Moroni, A., Manzolli, J., Zuben, F. V., & Gudwin, R. (2000). Vox Populi: interactiveevolutionary system algorithmic music composition. Leonardo Music Journal, 10,4954.Morris, D., Simon, I., & Basu, S. (2008). Exposing parameters trained dynamic modelinteractive music creation. Proceedings AAAI National ConferenceArtificial intelligence, pp. 784791.Mozer, M. (1991). Music Connectionism, chap. Connectionist music composition basedmelodic, stylistic, psychophysical constraints, pp. 195211. MIT Press,Cambridge.Nelson, G. L. (1993). Sonomorphs: application genetic algorithms growthdevelopment musical organisms. Proceedings Biennial Symposium ArtsTechnology, pp. 155169.Nelson, G. L. (1996). Real time transformation musical material fractal algorithms.Computers & Mathematics Applications, 1, 109116.Nettheim, N. (1992). spectral analysis melody. Journal New Music Research,21 (2), 135148.Nettheim, N. (1997). bibliography statistical applications musicology. MusicologyAustralia, 20 (1), 94106.Nierhaus, G. (2009). Algorithmic Composition: Paradigms Automated Music Generation.Springer Berlin / Heidelberg.Nishijimi, M., & Watanabe, K. (1993). Interactive music composer based neural networks. Fujitsu Scientific Technical Journal, 29 (2), 189192.North, T. (1991). technical explanation theme variations: computer music workutilizing network compositional algorithms. Ex Tempore, 5 (2).Olarte, C., Rueda, C., & Valencia, F. D. (2009). New Computational Paradigms Computer Music, chap. Concurrent Constraint Calculi: Declarative Paradigm Modeling Music Systems. Editions Delatour France.Olson, H. F. (1961). Aid music composition employing random probability system.Journal Acoustical Society America, 33, 11631170.575fiFernndez & VicoOrtega, A., Snchez, R., & Alfonseca, M. (2002). Automatic composition music meansgrammatical evolution. Proceedings Conference APL.Ovans, R., & Davison, R. (1992). interactive Constraint-Based expert assistant musiccomposition. Proceedings Canadian Conference Artificial Intelligence, pp.7681.zcan, E., & Eral, T. (2008). genetic algorithm generating improvised music.Proceedings International Conference Artificial Evolution, pp. 266277.Pachet, F. (2002). Interacting musical learning system: Continuator. Proceedings International Conference Music Artificial Intelligence, pp. 103108.Pachet, F., & Roy, P. (1995). Mixing constraints objects: case study automaticharmonization. Proceedings Conference Technology Object-OrientedLanguages Systems.Pachet, F., & Roy, P. (2001). Musical harmonization constraints: survey. Constraints,6 (1), 719.Pachet, F., Roy, P., & Barbieri, G. (2011). Finite-length Markov processes constraints.Proceedings International Joint Conference Artificial Inteligence.Padberg, H. A. (1964). Computer-composed canon free-fugue. Ph.D. thesis, Saint LouisUniversity, St. Louis.Papadopoulos, G., & Wiggins, G. (1998). genetic algorithm generation jazzmelodies. Proceedings Finnish Conference Artificial Intelligence (STeP).Papadopoulos, G., & Wiggins, G. (1999). AI methods algorithmic composition: survey,critical view future prospects. Proceedings Symposium MusicalCreativity, pp. 110117.Parikh, T. (2003). Iris: artificially intelligent real-time improvisation system. Mastersthesis, Emory University.Pazos, A., Santos del Riego, A., Dorado, J., & Romero Caldalda, J. J. (1999). Genetic musiccompositor. Proceedings IEEE Conference Evolutionary Computation, pp.885890.Pearce, M., Meredith, D., & Wiggins, G. (2002). Motivations methodologies automation compositional process. Music Scienti, 6 (2), 119147.Pearce, M., & Wiggins, G. (2001). Towards framework evaluation machinecompositions. Proceedings Symposium Artificial Intelligence CreativityArts Science, pp. 2232.Peck, J. M. (2011). Explorations algorithmic composition: Systems compositionexamination several original works. Masters thesis, State University New York,College Oswego.Pennycook, B. (1985). Computer-music interfaces: survey. ACM Computing Surveys,17 (2), 267289.Pereira, F., Grilo, C., Macedo, L., & Cardoso, A. (1997). Composing music case-basedreasoning. Proceedings Conference Computational Models CreativeCognition.576fiAI Methods Algorithmic CompositionPestana, P. (2012). Lindenmayer systems harmony fractals. Chaotic ModelingSimulation, 1 (1), 9199.Phon-Amnuaisuk, S. (2002). Control language harmonisation process. ProceedingsInternational Conference Music Artificial Intelligence, pp. 155167.Phon-Amnuaisuk, S. (2010). Investigating music pattern formations heterogeneouscellular automata. Journal New Music Research, 39 (3), 253267.Phon-Amnuaisuk, S., Law, E. H., & Kuan, H. C. (2007). Evolving music generationSOM-fitness genetic programming. Proceedings Conference ApplicationsEvolutionary Computation, pp. 557566.Phon-Amnuaisuk, S., Tuson, A., & Wiggins, G. (1999). Evolving musical harmonisation.Proceedings International Conference Artificial Neural Nets GeneticAlgorithms.Pinkerton, R. C. (1956). Information theory melody. Scientific American, 194 (2),7787.Polito, J., Daida, J. M., & Bersano Begey, T. F. (1997). Musica ex machina: Composing16th-Century counterpoint genetic programming symbiosis. ProceedingsInternational Conference Evolutionary Programming, pp. 113124.Ponsford, D., Wiggins, G., & Mellish, C. (1999). Statistical learning harmonic movement.Journal New Music Research, 28 (2), 150177.Pope, S. T. (1986). Music notations representation musical structure knowledge. Perspectives New Music, 24 (2), 156189.Pope, S. T. (1991). tool manipulating expressive structural hierarchies music(or: "T-R trees MODE: tree editor based loosely Freds theory").Proceedings International Computer Music Conference.Pope, S. T. (1993). Music Processing, chap. Music composition editing computer,pp. 2572. Oxford University Press.Pope, S. T. (1995). Fifteen years computer-assisted composition. ProceedingsBrazilian Symposium Computer Music.Pressing, J. (1988). Nonlinear maps generators musical design. Computer MusicJournal, 12 (2), 3546.Prusinkiewicz, P. (1986). Score generation L-systems. Proceedings International Computer Music Conference, pp. 455457.Prusinkiewicz, P., & Lindenmayer, A. (1990). algorithmic beauty plants. SpringerVerlag New York.Puckette, M. (2002). Max Seventeen. Computer Music Journal, 26 (4), 3143.Putnam, J. (1994). Genetic programming music. Tech. rep., New mexico institutemining technology.Quick, D. (2010). Generating music using concepts Schenkerian analysis chordspaces. Tech. rep., Yale University.577fiFernndez & VicoRader, G. M. (1974). method composing simple traditional music computer.Communications ACM, 17 (11), 631638.Ralley, D. (1995). Genetic algorithms tool melodic development. ProceedingsInternational Computer Music Conference, pp. 501502.Ramalho, G., & Ganascia, J.-G. (1994). Simulating creativity jazz performance.Proceedings AAAI National Conference Artificial intelligence, pp. 108113,Menlo Park.Ramrez, R., & Peralta, J. (1998). constraint-based melody harmonizer. ProceedingsWorkshop Constraints Artistic Applications.Reddin, J., McDermott, J., & ONeill, M. (2009). Elevated pitch: Automated grammaticalevolution short compositions applications evolutionary computing. ProceedingsConference Applications Evolutionary Computation, pp. 579584.Reisig, W. (1998). Elements Distributed Algorithms: Modeling Analysis PetriNets. Springer.Rennie, J. (2010). Ray Kurzweils slippery futurism. IEEE Spectrum, 47 (12), 2428.Ribeiro, P., Pereira, F. C., Ferrand, M., & Cardoso, A. (2001). Case-based melody generationMuzaCazUza. Proceedings Symposium Artificial IntelligenceCreativity Arts Science, pp. 6774.Ricanek, K., Homaifar, A., & Lebby, G. (1993). Genetic algorithm composes music.Proceedings Southeastern Symposium System Theory, pp. 223227.Riecken, D. (1998). WOLFGANG: "emotions" architecture enable learningcompose music. Proceedings International Conference SocietyAdaptive Behavior.Ritchie, G. (2007). empirical criteria attributing creativity computer program.Journal Artificial Intelligence, Philosophy Cognitive Science, 17 (1), 6799.Roads, C. (1977). Composing grammars. Proceedings International ComputerMusic Conference.Roads, C. (1979). Grammars representations music. Computer Music Journal, 3 (1),4855.Roads, C. (1985). Research music artificial intelligence. ACM Computing Surveys,17 (2), 163190.Roads, C. (Ed.). (1992). Music Machine: Selected Readings Computer MusicJournal. MIT Press.Roads, C. (2004). Microsound. MIT Press.Roads, C., & Strawn, J. (Eds.). (1985). Foundations computer music. MIT Press.Ross, B. J. (1995). process algebra stochastic music composition. ProceedingsInternational Computer Music Conference.Rothgeb, J. (1968). Harmonizing unfigured bass: computational Study. Ph.D. thesis,Yale University.578fiAI Methods Algorithmic CompositionRowe, R. (1992). Interactive Music Systems: Machine Listening Composing. MITPress, Cambridge.Rueda, C., lvarez, G., Quesada, L. O., Tamura, G., Valencia, F., Daz, J. F., & Assayag,G. (2001). Integrating constraints concurrent objects musical applications:calculus visual language. Constraints, 6 (1), 2152.Rueda, C., Assayag, G., & Dubnov, S. (2006). concurrent constraints factor oracle modelmusic improvisation. Proceedings Latin American Informatics Conference.Rueda, C., Lindberg, M., Laurson, M., Bloch, G., & Assayag, G. (1998). Integrating constraint programming visual musical composition languages. ProceedingsWorkshop Constraints Artistic Applications.Sabater, J., Arcos, J., & Lpez de Mntaras, R. (1998). Using rules support case-basedreasoning harmonizing melodies. Proceedings AAAI Spring SymposiumMultimodal Reasoning, pp. 147151.Snchez-Quintana, C., Moreno-Arcas, F., Albarracn-Molina, D., Fernndez, J. D., & Vico,F. (2013). Melomics: case-study AI Spain. AI Magazine, 34 (3), 99103.Sandred, O. (2004). Interpretation everyday gestures composing rules. Proceedings Music Music Science Conference.Sandred, O. (2010). PWMC, constraint-solving system generating music scores. Computer Music Journal, 34 (2), 824.Santos, A., Arcay, B., Dorado, J., Romero, J. J., & Rodrguez, J. A. (2000). Evolutionarycomputation systems musical composition. Proceedings InternationalConference Acoustic Music: Theory Applications.Sastry, A. (2011). N-gram modeling tabla sequences using variable-length hidden Markovmodels improvisation composition. Masters thesis, Georgia Institute Technology.Scaletti, C. (2002). Computer music languages, Kyma, future. Computer MusicJournal, 26 (4), 6982.Schmidl, H. (2008). Pseudo-Genetic algorithmic composition. Proceedings International Conference Genetic Evolutionary Methods.Schottstaedt, W. (1989). Current directions computer music research, chap. AutomaticCounterpoint, pp. 199214. MIT Press, Cambridge.Schulze, W. (2009). Formal Language Theory Approach Music Generation. Ph.D.thesis, Stellenbosch University.Schwanauer, S. (1993). Machine models music, chap. learning machine tonal composition, pp. 511532. MIT Press, Cambridge.Schwanauer, S. M., & Levitt, D. A. (1993). Machine Models Music. MIT Press,Cambridge.Shao, J., McDermott, J., ONeill, M., & Brabazon, A. (2010). Jive: generative, interactive, virtual, evolutionary music system applications evolutionary computation.Proceedings Conference Applications Evolutionary Computation, pp.341350.579fiFernndez & VicoShibata, N. (1991). neural network-based method chord/note scale associationmelodies. NEC Research Development, 32 (3), 453459.Simoni, M., & Dannenberg, R. B. (2013). Algorithmic Composition: Guide ComposingMusic Nyquist. University Michigan Press.Soddell, F., & Soddell, J. (2000). Microbes music. Proceedings Pacific RimInternational Conference Artificial Intelligence.Sowa, J. F. (1956). machine compose music. Instruction manual GENIAC, OliverGarfield Company, Inc.Spangler, R. R. (1999). Rule-Based Analysis Generation Music. Ph.D. thesis, California Institute Technology.Spector, L., & Alpern, A. (1994). Criticism, culture, automatic generationartworks. Proceedings AAAI National Conference Artificial intelligence,pp. 38, Menlo Park.Spector, L., & Alpern, A. (1995). Induction recapitulation deep musical structure.Proceedings International Joint Conference Artificial Inteligence, pp. 4148.Stanley, K., & Miikkulainen, R. (2003). taxonomy artificial embryogeny. ArtificialLife, 9 (2), 93130.Steedman, M. J. (1984). generative grammar jazz chord sequences. Music Perception:Interdisciplinary Journal, 2 (1), 5277.Steels, L. (1979). Reasoning modeled society communicating experts. Mastersthesis, Massachusetts Institute Technology, Cambridge.Steels, L. (1986). Learning craft musical composition. Proceedings International Computer Music Conference.Stieler, W. (2012). Die mozart-Maschine. Technology Review (German edition), 12/2012,2634.Supper, M. (2001). remarks algorithmic composition. Computer Music Journal,25 (1), 4853.Thom, B. (2000). BoB: interactive improvisational music companion. ProceedingsInternational Conference Autonomous Agents, pp. 309316, New York.Thomas, M. T. (1985). Vivace: rule based AI system composition. ProceedingsInternational Computer Music Conference, pp. 267274.Thomas, M. T., Chatterjee, S., & Maimone, M. W. (1989). Cantabile: rule-based systemcomposing melody. Proceedings International Computer Music Conference.Thornton, C. (2009). Hierarchical Markov modelling generative music. ProceedingsInternational Computer Music Conference.Thywissen, K. (1999). GeNotator: environment exploring application evolutionary techniques computer-assisted composition. Organised Sound, 4 (2), 127133.Tipei, S. (1975). MP1: computer program music composition. ProceedingsAnnual Music Computation Conference.580fiAI Methods Algorithmic CompositionTodd, P. M. (1989). connectionist approach algorithmic composition. Computer MusicJournal, 13 (4), 2743.Todd, P. M., & Loy, D. G. (1991). Music Connectionism. MIT Press, Cambridge.Toiviainen, P. (1995). Modeling target-note technique bebop-style jazz improvisation:artificial neural network approach. Music Perception: Interdisciplinary Journal,12 (4), 399413.Toiviainen, P. (2000). Readings Music Artificial Intelligence, chap. Symbolic AIversus Connectionism Music Research, pp. 4767. Harwood Academic Publishers.Tokui, N., & Iba, H. (2000). Music composition interactive evolutionary computation.Proceedings Generative Art Conference.Tominaga, K., & Setomoto, M. (2008). artificial-chemistry approach generatingpolyphonic musical phrases. Proceedings Conference ApplicationsEvolutionary Computation, pp. 463472.Towsey, M. W., Brown, A. R., Wright, S. K., & Diederich, J. (2001). Towards melodicextension using genetic algorithms. Educational Technology & Society, 4 (2), 5465.Trivio Rodrguez, J. L., & Morales-Bueno, R. (2001). Using multiattribute predictionsuffix graphs predict generate music. Computer Music Journal, 25 (3), 6279.Truchet, C., Assayag, G., & Codognet, P. (2003). OMClouds, heuristic solver musicalconstraints. Proceedings International Conference Metaheuristics.Tsang, C. P., & Aitken, M. (1991). Harmonizing music discipline constraint logicprogramming. Proceedings International Computer Music Conference.Ulrich, J. W. (1977). analysis synthesis jazz computer. ProceedingsInternational Joint Conference Artificial Inteligence, pp. 865872.Unehara, M., & Onisawa, T. (2001). Composition music using human evaluation.Proceedings IEEE International Conference Fuzzy Systems, pp. 12031206.Ventrella, J. J. (2008). Art Artificial Evolution, chap. Evolving Structure LiquidMusic, pp. 269288. Springer Berlin / Heidelberg.Verbeurgt, K., Fayer, M., & Dinolfo, M. (2004). hybrid Neural-Markov approachlearning compose music example. Proceedings Canadian ConferenceAdvances Artificial Intelligence, pp. 480484.Visell, Y. (2004). Spontaneous organisation, pattern models, music. Organised Sound,9 (02), 151165.Voss, R. F., & Clarke, J. (1978). 1/f noise music: Music 1/f noise. JournalAcoustical Society America, 63, 258263.Walker, W. F. (1994). conversation-based framework musical improvisation. Ph.D.thesis, University Illinois.Wallin, N. L., & Merker, B. (2001). Origins Music. MIT Press.Waschka, R. (1999). Avoiding fitness bottleneck: Using genetic algorithms composeorchestral music. Proceedings International Computer Music Conference, pp.201203.581fiFernndez & VicoWatson, L. A. (2008). Algorithmic composition flute accompaniment. Mastersthesis, University Bath.Werner, M., & Todd, P. M. (1997). many love songs: Sexual selection evolutioncommunication. Proceedings European Conference Artificial Life.Widmer, G. (1992). Qualitative perception modeling intelligent musical learning. Computer Music Journal, 16 (2), 5168.Wiggins, G. A. (1998). use constraint systems musical composition. ProceedingsWorkshop Constraints Artistic Applications.Wiggins, G. A. (2008). Computer models musical creativity: review computer modelsmusical creativity David Cope. Literary Linguistic Computing, 23 (1), 109116.Wilson, A. J. (2009). symbolic sonification L-systems. Proceedings International Computer Music Conference, pp. 203206.Wolkowicz, J., Heywood, M., & Keselj, V. (2009). Evolving indirectly represented melodiescorpus-based fitness evaluation. Proceedings Conference ApplicationsEvolutionary Computation, pp. 603608.Wooller, R., & Brown, A. R. (2005). Investigating morphing algorithms generativemusic. Proceedings International Conference Generative SystemsElectronic Arts.Worth, P., & Stepney, S. (2005). Growing music: Musical interpretations L-Systems.Proceedings Conference Applications Evolutionary Computation, pp.545550.Yi, L., & Goldsmith, J. (2007). Automatic generation four-part harmony. ProceedingsConference Uncertainty Artificial Intelligence.Yilmaz, A. E., & Telatar, Z. (2010). Note-against-note two-voice counterpoint meansfuzzy logic. Knowledge-Based Systems, 23 (3), 256266.Zicarelli, D. (1987). Jam factory. Computer Music Journal, 11 (4), 1329.Zimmermann, D. (2001). Modelling musical structures. Constraints, 6 (1), 5383.582fiJournal Artificial Intelligence Research 48 (2013) 305-346Submitted 04/13; published 10/13Global Model Concept-to-Text GenerationIoannis KonstasMirella LapataIKONSTAS @ INF. ED . AC . UKMLAP @ INF. ED . AC . UKInstitute Language, Cognition Computation,School Informatics, University Edinburgh,10 Crichton Street, EH8 9AB, Edinburgh UKAbstractConcept-to-text generation refers task automatically producing textual outputnon-linguistic input. present joint model captures content selection (what say)surface realization (how say) unsupervised domain-independent fashion. Ratherbreaking generation process sequence local decisions, define probabilistic context-free grammar globally describes inherent structure input (a corpusdatabase records text describing them). recast generation task findingbest derivation tree set database records describe algorithm decodingframework allows intersect grammar additional information capturing fluencysyntactic well-formedness constraints. Experimental evaluation several domains achieves results competitive state-of-the-art systems use domain specific constraints, explicit featureengineering labeled data.1. IntroductionConcept-to-text generation broadly refers task automatically producing textual outputnon-linguistic input (Reiter & Dale, 2000). Depending application domain hand,input may assume various representations including databases records, expert system knowledge bases, simulations physical systems on. Figure 1 shows input examplescorresponding text three domains: air travel, sportscasting weather forecast generation.typical concept-to-text generation system implements pipeline architecture consistingthree core stages, namely content planning (selecting appropriate content inputdetermining structure target text), sentence planning (determining structure lexical content individual sentences), surface realization (rendering specification chosensentence planner surface string). Traditionally, components hand-engineeredorder generate high quality text, expense portability scalability. thus surprise recent years witnessed growing interest automatic methods creating trainablegeneration components. Examples include learning database records presenttext (Duboue & McKeown, 2002; Barzilay & Lapata, 2005) verbalized(Liang, Jordan, & Klein, 2009). Besides concentrating isolated components, approachesemerged tackle concept-to-text generation end-to-end. Due complexity task,models simplify generation process, e.g., creating output consists sentences, thus obviating need content planning, treating sentence planning surfacerealization one component. common modeling strategy break generation processsequence local decisions, learned separately (Reiter, Sripada, Hunter, & Davy, 2005a;Belz, 2008; Chen & Mooney, 2008; Angeli, Liang, & Klein, 2010; Kim & Mooney, 2010).c2013AI Access Foundation. rights reserved.fiKONSTAS & L APATAPassBad PassTurnDatabase:pink3 pink7pink7 purple3pink7 purple3Text:pink3 passes ball pink7(a) ROBO CDatabase:TemperatureCloud Sky Covertimemin mean max06:00-21:00 9 15 21timepercent (%)06:00-09:0025-5009:00-12:0050-75Wind SpeedWind Directiontimemin mean max06:00-21:00 15 20 30Text:timemode06:00-21:00Cloudy, temperatures 10 20 degrees. South wind around 20 mph.(b) W EATHER G OVDatabase:Text:FlightDay NumberMonthdenver bostonnumber dep/ar9departuremonthdep/araugust departureConditionSearcharg1arg2 typearrival time 16:00 <typequery flightGive flights leaving Denver August ninth coming back Boston 4pm.(c) ATISFigure 1: Input-output examples (a) sportscasting, (b) weather forecast generation, (c) querygeneration air travel domain.paper focus problem generating text database describeend-to-end generation model performs content selection surface realization jointly.specifically, input model set database records collocated textual descriptions.Consider example Figure 1b. Here, records provide structured representationweather specific time interval (e.g., temperature, wind speed direction)text renders information natural language. formulate task creating textcorresponding database following generative process: database consists306fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATIONset typed tuples (record, field, value), aim choose subset talk about.naturally decomposes selecting sequence records, sequence fields withinrecord. Finally, field generate sequence words according valuefield. Central approach jointly optimize process, rather breaking variousdecisions local problems greedily trying solve one them.this, define probabilistic context-free grammar (PCFG) captures structuredatabase verbalized. Generation boils finding best stringoutput captured best derivation tree licensed grammar. order ensuregeneration output coherent, intersect grammar additional information capturingfluency syntactic well-formedness constraints. Specifically, experiment n-gram language model dependency model based work Klein Manning (2004). followChiangs (2007) integration framework show extended intersecting CFGgrammar arbitrary number models (see Huang, 2008 similar proposal). workclosest Liang et al. (2009) learn align database records text segmentsusing hierarchical hidden semi-Markov generative model (see Section 3.1 details). recastmodel PCFG develop decoding algorithm allows us go beyond alignments,i.e., generate multi-sentence text corresponding database input.model conceptually simpler previous approaches (e.g., Angeli et al., 2010; Kim &Mooney, 2010); encodes information domain structure globally, consideringinput space simultaneously generation. thus need train single model (on givendomain) without separately optimize different content selection surface realization components. importantly, recasting generation parsing allows us optimize jointobjective (hence finding likely grammar derivation also yields grammatical outputtext) principled manner, rather approximating greedy search local decisions. assumption input must set records essentially correspondingdatabase-like tables whose columns describe fields certain type. Experimental evaluationthree domains obtains results competitive state-of-the-art without using domain specificconstraints, explicit feature engineering labeled data.1remainder paper structured follows. Section 2 provides overview relatedwork. Section 3 presents generation model; defines PCFG used experimentspresents decoding algorithm Section 4 discusses experimental set-up Section 5 presentsresults. Discussion future work concludes paper.2. Related Workliterature reveals many examples generation systems produce high quality text, almostindistinguishable human writing (Dale, Geldof, & Prost, 2003; Reiter, Sripada, Hunter, Yu,& Davy, 2005b; Green, 2006; Turner, Sripada, & Reiter, 2009). systems often implementpipeline architecture involve great deal manual effort. instance, typical contentselection module involves manually engineered rules based analysis large numbertexts domain-relevant corpus, consultation domain experts. Analogously, surface1. preliminary version work published proceedings NAACL 2012. current article presentsgeneral model, formulates explicitly decoding algorithm shows intersect PCFG arbitrary number external knowledge sources. addition, present several novel experiments, comprehensiveerror analysis.307fiKONSTAS & L APATArealization often based grammar written hand cover syntactic constructsvocabulary domain.One earliest systems exemplifies approach FOG (Goldberg, Driedger, & Kittredge, 1994), weather forecast generator used Environment Canada, Canadian weatherservice. FOG takes input numerical simulations meteorological maps uses expertsystem decide structure document optional human intervention viagraphical interface. sentence planning surface realization, generator uses grammarspecific weather domain, well canned syntactic structures written expert linguistsencoded Backus Naur Form (BNF). recently, Reiter et al. (2005a) developedUM IME -M OUSAM, text generator produces marine weather forecasts offshore oilrig applications. content planner system based linear segmentation input(i.e., time series data) informed pragmatic (Gricean) analysis communicated weather forecasts (Sripada, Reiter, Hunter, & Yu, 2003). Sentence planning relies rulesselect appropriate time phrases, based empirical study human-written forecasts. Surface realization relies special grammar rules emulate weather sub-language interest,based corpus analysis.existing generation systems engineered obtain good performance particulardomains, often difficult adapt across different domains. alternative adoptdata-driven approach try automatically learn individual generation components evenend-to-end system. example class methods described work BarzilayLapata (2005) view content selection instance collective classification. Givencorpus database records texts describing them, first use simple anchor-basedalignment technique obtain records-to-text alignments. Then, use alignments trainingdata (records present text positive labels, records negative) learn contentselection model simultaneously optimizes local label assignments pairwise relations.Building work, Liang et al. (2009) present hierarchical hidden semi-Markov generativemodel first determines facts discuss generates words predicatesarguments chosen facts. model decomposed three tiers HMMs correspondchains records, fields words. use Expectation Maximization (EM) trainingdynamic programming inference (see Section 3.1 thorough description).approaches emerged recently combine content selection surface realization. Kim Mooney (2010) present generator two-stage pipeline architecture: usinggenerative model similar model work Liang et al. (2009), first decidesay verbalize selected input WASP1 , existing generation system (Wong &Mooney, 2007). contrast, Angeli et al. (2010) propose unified content selection surfacerealization model also operates alignment output produced model Lianget al.. model decomposes sequence discriminative local decisions. first determine records database talk about, fields records mention,finally words use describe chosen fields. decisions implementedlog-linear model features learned training data. surface realization componentperforms decisions based automatically extracted templates filtered domain-specificconstraints order guarantee fluent output.related work focused mapping meaning representations (e.g., logical formnumeric weather data) natural language, using explicitly aligned sentence/meaning pairstraining data. example, Wong Mooney (2007) learn mapping using synchronous308fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATIONcontext-free grammar (SCFG). also integrate language model SCFG decodemeaning representation input text, using left-to-right Early chart generator. Belz (2008)creates CFG hand (using set template-based domain-specific rules) estimates probabilities rule application automatically development corpus. Ratnaparkhi (2002) usesdependency-style grammar phrase fragments context dialogue system, incorporatingamong others long-range dependencies. recently, Lu Ng (2011) propose workmodel performs joint surface realization lexical acquisition input representedtyped lambda calculus. present novel SCFG forest-to-string generation algorithm,captures correspondence natural language logical form represented hybridtrees.Similar work Angeli et al. (2010), also present end-to-end system performscontent selection surface realization. However, rather breaking generation tasksequence local decisions, optimize say say simultaneously.learn mappings logical form, rather focus input less structured possiblynoisy. key insight convert set database records serving input generatorPCFG neither hand crafted domain specific simply describes structureinput. training, estimate weights grammar rules using EM algorithmdynamic program similar inside-outside algorithm (Li & Eisner, 2009). testinggiven set database search best derivation tree licensed grammar.searching, intersect grammar external linguistically motivated models createk-best lists derivations, thus optimizing say say time.3. Problem Formulationassume generator takes input set database record tuples (r, f , v) outputstext g verbalizes records. record token ri , 1 |d|, type ri .t,thought name table relational database schema. Notetotal number records |d| vary examples. Figure 1b illustrates instances recordtypes Temperature, Wind Speed, Wind Direction. record token also setfields ri .f associated it. example, record type Wind Direction two fields, namelywindDir1 .time windDir1 .mode. henceforth abbreviate fields names (e.g., timemode) record type apparent context. Fields different values fk .v;Figure 1b value field mode S. Fields also associated type fk .t, definesrange possible values take; model supports integer categorical value types.example, top right table Figure 1b named Cloud Sky Cover (sc short), corresponds fourdatabase record tuples: (sc1 , time, 06:00-09:00), (sc1 , percent, 25-50), (sc2 , time, 09:00-12:00)(sc2 , percent, 50-75). time percent categorical type.training corpus consists several scenarios, i.e., database records paired texts w2like shown Figure 1. weather forecast domain, scenario corresponds weatherrelated measurements temperature, wind, speed, collected specific day time(e.g., day night). sportscasting, scenarios describe individual events soccer game(e.g., passing kicking ball). air travel domain, scenarios comprise flight-relateddetails (e.g., origin, destination, day, time).2. use w denote gold-standard text g refer string words system generates.309fiKONSTAS & L APATA...r1...r1 . f 1w1...w...ri . f 1w...ri...w...ri . f|f|w...r|r|wr|r| . f|f|w...wNFigure 2: Graphical model representation generative alignment model Liang et al. (2009).Shaded nodes represent observed variables (i.e., database collocated text w), unshadednodes indicate latent variables. Arrows indicate conditional dependencies variables. Starting database d, model emits sequence records; record emitssequence fields, specific type particular record. Finally, record uniformlyselects number c emits words w1 . . . wc .goal first define model naturally captures (hidden) relationsdatabase records observed text w. trained, use model generate text gcorresponding new records d. model extension hierarchical hidden semi-Markovmodel Liang et al. (2009) describe detail next section. key idea recastmodel probabilistic context-free grammar, therefore reducing tasks content selectionsurface realization common parsing problem.3 Arguably, could implementedmodel using finite-state representation. However, conceptualization generation parsing,allows us use well-known CYK algorithm (Kasami, 1965; Younger, 1967) order findbest g licensed grammar. also affords wider range extensions go beyondexpressivity cascade HMMs model Liang et al. furthermore ensureresulting text fluent intersecting grammar externally trained surface level models,namely n-gram language model dependency model. Thus, model generate parseimportantly text deemed likely grammar surface models.following, first describe approach Liang et al. move describe grammardecoding algorithm, i.e., procedure finding best g given input d.310fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION3.1 Model Inducing AlignmentsLiang et al. (2009) present generative semi-hidden Markov model learns correspondenceworld state unsegmented string text without, however, generating outputstring words g describing world state. case, world state represented setdatabase records, associated fields values. model defined generativeprocess summarized three steps:1. Record choice. Choose sequence records r describe. Consecutive records selectedbasis types.2. Field choice. record ri emit sequence fields ri .f.3. Word choice. chosen field ri . fk generate number words c, c > 0 chosenuniformly.process implemented hierarchy Markov chains correspond records, fields,values input database. captured Markov chain records conditioned recordtypes; given record type, record chosen uniformly set records type.way, model essentially captures rudimentary notions local coherence salience,respectively. formally:|r|p(r | d) = p(ri .t | ri1 .t)1|s(ri .t)|(1)s(t) defined function returns set records type t: = {r : r.t = t},r0 .t START record type. Liang et al. (2009) also include special null record type,accounts words particularly align record present database. Fieldchoice modeled analogously Markov chain fields given record choice ri type t:|ri .f|p(f | ri .t) = p(ri . fk | ri . fk1 )(2)kalso implement special start stop fields model transitions boundariescorresponding phrase. Finally, chosen record ri , field fk uniformly chosen number c,0 < c < N, emit words independently given field value type. Note sincemodel always observes words, simplistic representation surface level adequate(however, relaxing independence assumption, e.g., additionally conditioning previousword(s), could potentially yield powerful model):|w|p(w |ri , ri . fk , ri . fk .t) = p(w j | ri .t, ri . fk .v)(3)jmodel supports three different types fields, namely string, categorical integer.adopt specific generation strategy word level. string-typed fields,3. alternative would learn SFCG database input accompanying text. However, wouldinvolve considerable overhead terms alignment (as database text together constitute cleanparallel corpus, rather noisy comparable corpus), well grammar training decoding using state-of-theart statistical machine translation (SMT) methods, manage avoid simpler approach.311fiKONSTAS & L APATAEvents:Fields:Text:skyCover1percent=0-25cloudy ,kNwithgtemperature1time=6am-9pmtemperaturesmin=910gandmax=2120 degrees .kwindDir1mode=SNsouthgwindgkwindSpeed1Nmean=20aroundg20 mph .Figure 3: Example alignment output model Liang et al. (2009) weather domain.Subscripts refer record tokens (e.g., skyCover1 first record type Cloud Sky Cover).emit single word (possibly) multi-word value, chosen uniformly. categorical fields,maintain separate multinomial distribution words field value. Finally, integerfields, wish capture intuition numeric quantity database renderedtext word possibly numerical value due stylistic factors.allow several ways generating word given field value. include generating exact value,rounding rounding multiple 5, rounding closest multiple 5,adding subtracting unexplained noise + , respectively. noise modeledgeometric distribution, parameters trained given value ri . fk .v.example models output weather domain shown Figure 3. toprow contains database records selected model (subscripts correspond record tokens;e.g., temperature1 refers first record type temperature Figure 1b). second row contains selected fields record associated values. special field null alignswords directly refer values database records, with, windaround. Finally, last row shows segmentation alignment original text w producedmodel.stands, Liang et al.s (2009) model generates alignment sequences wordsfacts database, falling short creating meaningful sentence document. Kim Mooney(2010) address problem interfacing alignments WASP1 (Wong & Mooney, 2007).latter publicly available generation system takes alignment input findslikely string using widely popular noisy-channel model. Angeli et al. (2010) proposemodel different spirit nevertheless also operates alignments Liang et al. Usingtemplate extraction method, post-process alignments order obtain sequencerecords, fields, words spanned chosen records fields. generationprocess modeled series local decisions, arranged hierarchically traineddiscriminatively. record choose talk about, choose subset fields,finally suitable template render chosen content. process repeats decidesgenerate special STOP record.treat model Liang et al. (2009) black box order obtain alignments.Rather, demonstrate generation seamlessly integrated semi-hidden Markovmodel re-interpreting CFG rewrite rules providing appropriate decoding algorithm.model simultaneously learns records fields talk about, textual unitscorrespond to, creatively rearrange coherent document.3.2 Grammar Definitionmentioned earlier, recast model Liang et al. (2009) series CFG rewrite rules,corresponding first two layers HMMs Figure 2. also include set grammarrules emit chains words, rather words isolation. viewed additional312fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION1. R(start)GCSGSURF2. R(ri .t) FS(r j , start) R(r j .t)[Pr = 1]hP(r j .t | ri .t) |s(r1i .t)|3. R(ri .t) FS(r j , start)hP(r j .t | ri .t) |s(r1i .t)|4. FS(r, r. fi ) F(r, r. f j ) FS(r, r. f j )[P( f j | fi )]5. FS(r, r. fi ) F(r, r. f j )[P( f j | fi )]6. F(r, r. f ) W(r, r. f ) F(r, r. f )[P(w | w1 , r, r. f )]7. F(r, r. f ) W(r, r. f )[P(w | w1 , r, r. f )]8. W(r, r. f )[P( | r, r. f , f .t, f .v, f .t = {cat, null})]9. W(r, r. f ) gen( f .v)[P(gen( f .v).mode | r, r. f , f .t = int)P( f .v | gen( f .v).mode)]Table 1: Grammar rules GGEN weights shown square brackets.HMM words field original model. modification important generation; since observe set database records d, need better informed modeldecoding captures word-to-word dependencies directly. also pointPCFG extend underlying expressivity model presented Liang et al., namelyalso describes regular language.grammar GGEN defined Table 1 (rules (1)(9)) contains two types rules. GCSrules perform content selection, whereas GSURF rules perform surface realization. typesrules purely syntactic (describing intuitive relationship records, records fields,fields corresponding words), could apply database similar structure irrespectively semantics domain. Rule weights governed underlying multinomialdistribution shown square brackets. Non-terminal symbols capitals denote intermediate states; terminal symbol corresponds words seen training set, gen( f .v)function generating integer numbers given value field f . non-terminals, savestart symbol S, one features (shown parentheses) act constraints, similar number gender agreement constraints augmented syntactic rules. Figure 4 shows twoderivation trees licensed grammar sentence Cloudy, temperatures 1020 degrees. (see example Figure 1b).first rule grammar denotes expansion start symbol record R,special start record type (hence notation R(start)). Rule (2) defines chaintwo consecutive records, i.e., going record ri r j . Here, FS(r j , start) represents setfields record r j following record R(ri ). example, Figure 4a, top branching ruleR(start) FS(sc2 , start)R(sc2 .t) (sc stands Cloud Sky Cover) interpreted follows.Given beginning document, hence record R(start), talk313fiKONSTAS & L APATAR(start)R(sc2 .t)FS(sc2 ,start)F(sc2 ,%)FS(sc2 ,%)W(sc2 ,%)F(sc2 ,null)R(t1 .t)FS(t1 ,start)FS(t1 ,min)F(t1 ,min)W(sc2 ,null) W(t1 ,min)F(t1 ,min)W(t1 ,min)FS(t1 ,max)F(t1 ,max)F(t1 ,min)W(t1 ,min)W(t1 ,max)F(t1 ,null)F(t1 ,max)W(t1 ,max) W(t1 ,null)F(t1 ,min)W(t1 ,null)W(t1 ,min)Cloudy,10temperaturesF(t1 ,null).degrees20...(a)R(start)R(sc2 .t)FS(sc2 ,start)F(sc2 ,%)FS(sc2 ,%)W(sc2 ,%)F(sc2 ,null)R(t1 .t)FS(t1 ,start)FS(t1 ,null)F(t1 ,null)W(sc2 ,null) W(t1 ,null)F(t1 ,null)W(t1 ,null)FS(t1 ,min)F(t1 ,min)W(t1 ,min)F(t1 ,max)F(t1 ,min)F(t1 ,max)W(t1 ,min) W(t1 ,max)W(t1 ,max)F(t1 ,max)W(t1 ,max)F(t1 ,max)W(t1 ,max)Cloudy,temperatures1020degrees....(b)Figure 4: Two derivation trees using grammar Table 1 sentence Cloudy, temperatures 10 20 degrees.. use sc shorthand record type Cloud SkyCover, Temperature. Subscripts refer record tokens (e.g., sc2 second Cloud SkyCover record, t1 first Temperature record, on).314fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATIONpart forecast refers Cloud Sky Cover, i.e., emit set fields spannednon-terminal FS(sc2 , start). field start FS acts special boundary consecutiverecords. Note input database example 1b, two records type Cloud SkyCover (see second box example). Given value percent (%) fieldsecond record 50-75, likely lexicalize phrase Cloudy ,. differentscenario, equivalent phrase Mostly sunny , first record value 25-50 wouldappropriate. Rule R(sc2 .t) FS(t1 , start)R(t1 .t) (t stands Temperature)interpreted similarly: talk sky coverage forecast movedescribe temperature outlook, via field set spanned non-terminal FS(t1 , start) (seesecond sub-tree Figure 4a). weight rule bigram probability two recordsconditioned record type, multiplied normalization factor |s(r1i .t)| , s(t)function returns set records type (Liang et al., 2009). also defined nullrecord type i.e., record fields acts smoother words may correspondparticular record. Rule (3) simply escape rule, parsing process (on recordlevel) finish.Rule (4) equivalent rule (2) field level, i.e., describes chaining twoconsecutive fields fi f j . Non-terminal F(r, r. f ) refers field f record r. example,tree Figure 4a, rule FS(t1 , min) F(t1 , max) FS(t1 , max) specifies talkfield max record t1 (i.e., temperature record), talking field min. Analogouslyrecord level, also included special null field type emission wordscorrespond specific record field (e.g., see emission two last tokens degrees .end phrase derivation tree. Rule (6) defines expansion field F sequence(binarized) words W, weight equal bigram probability current word givenprevious word, current record, field. See consecutive application rulederivation tree emission phrase temperatures 10 .Rules (8) (9) responsible surface generation; define emission wordsintegers W , given field type value, thus regarded lexical rulesgrammar (see pre-terminal expansions derivation tree Figure 4a examples).Rule (8) emits single word vocabulary training set. weight defines multinomialdistribution seen words, every value field f , given field type categorical(denoted cat grammar) special null field. Rule (9) identical fields whosetype integer. Function gen( f .v) generates integer number given field value, using eitherfollowing six ways (Liang et al., 2009): identical field value, rounding roundingmultiple 5, rounding closest multiple 5 finally adding subtractingunexplained noise + respectively. noise modeled geometric distribution,parameters trained given value f .v. weight multinomial sixinteger generation function choices, given record field f , times P( f .v | gen( f .v).mode),set geometric distribution noise + , 1 otherwise.Naturally, grammar yield several derivation trees given input string. Noticedifference Figure 4a Figure 4b emitting phrases temperatures 1020 degrees .. Figure 4a, field min (whose record Temperature) spans entirephrase, whereas Figure 4b phrase split two parts. null field emits temperaturesmin field emits 10 . Analogously, derivation tree Figure 4a, field maxemits first three words, 20 degrees, null emits full-stop null field315fiKONSTAS & L APATArecord (very common situation case punctuation marks). derivation treeFigure 4b, however, whole phrase spanned field max.3.3 Generationfar defined probabilistic grammar captures structure databaserecords fields intermediate non-terminals, words w (from associated text) terminals. mapping w unknown thus intermediate multinomials (see ruleweights GGEN Table 1) define distribution hidden correspondences h records,fields values. Given input scenario database generate correspondingtext using grammar Table 1.high-level generation procedure described follows. first select lengthN output text (we defer discussion achieve Section 4.3). Then, applygrammar empty document building derivation trees bottom-up fashion, startinglexical rules r GSURF . word position document emit k-best listcandidate words drawn corresponding distributions, given values fieldsrecords d; then, apply rest rules r GCS , keeping list k-best partial derivationspartially generated text node4 , reach root symbol spanning wholedocument. Finally, reconstruct top-scoring generated string root tree, following pointers best derivation, lexical rules emit words finaldocument. order guarantee grammaticality final output text, rescore k-bestlists node applying external linguistic knowledge, n-gram language modelshead dependency-style models, partially generated substrings.analogy parsing, procedure amounts finding likely derivation, i.e., sequencerewrite rules given input. Note, subtle difference syntactic parsinggeneration. former case, observe string words goal findprobable syntactic structure, i.e., hidden correspondence h. generation, however, describedabove, string observed; instead, must thus find best text g, maximizingh g (the latter achieved use external linguistic knowledge via rescoring),g = g1 . . . gN sequence words licensed GCS GSURF . formally:g = f arg max P (g, h)(4)g,hf function takes input derivation tree (g, h) returns g. use modifiedversion CYK parser (Kasami, 1965; Younger, 1967) find g. Optimizing h gintractable, approximate f pruning search space explain Section 3.5.following, use framework deductive proof systems (Shieber, Schabes,& Pereira, 1995) order describe decoder. first present basic adaptation CYKalgorithm task give concrete decoding procedure generates text, using chart datastructure (Section 3.4). extend basic decoder k-best decoder, integrating external linguistic knowledge attempt improve quality output. basic decodernaively optimizes function f h, whereas extended version maximizes h g,approximately. Note framework deductive proof systems used convenience.4. use efficient method compresses stored substrings considerably, following work Chiang (2007);see equation (12) Section 3.6.316fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATIONItems:[A, i, j]R(A B)R(A BC)Axioms:[W, i, + 1] :W gi+1 , gi+1 {, gen()}Inference rules:(1)(2)Goal:R(A B) : [B, i, j] : s1[A, i, j] : s1R(A B C) : [B, i, k] : s1 [C, k, j] : s2[A, i, j] : s1 s2[S, 0, N]Figure 5: basic decoder deductive system. Productions B B CGCS rules Figure 1; features grammar non-terminals omitted sake clarity.provides level abstract generalization number algorithms. Examples include recogntion sentence according grammar, learning inside outside weights, Viterbi search,case generating text (see Goodman, 1999 details).3.4 Basic DecoderAnalogously parser, decoder generally defined set weighted items (somedesignated axioms others goals, i.e., items proven) set inferencerules form:I1 : s1 . . . Ik : skI:sinterpreted follows: items Ii (i.e., antecedents) first provenweight (or score) si , item (i.e., consequent) provable, weight provided sidecondition holds. decoding process begins set axioms, progressively appliesinference rules, order prove items reaches one designated goals.basic decoder specified Figure 5 consists four components, class items,set axioms, set inference rules subclass items, namely goal items. Followingwork Goodman (1999), items system take two forms: [A, i, j] indicates generated spanj, rooted non-terminal A; R(A B) R(A B C) corresponds contentselection production rules GCS one two non-terminals right hand side. Axiomscorrespond individual word generated surface realization grammar rules GSURF (see(8) (9) Table 1). inference rules follow two forms, one grammar production rulesone non-terminal right hand side, another one rules two non-terminals.example, inference rule (1) Figure 5 combines two items, namely rule form Bweight generated span [B, i, j] weight s1 rooted B, results new generatedspan [A, i, j] weight s1 , rooted A. Finally, system one goal, [S, 0, N],root node grammar N (predicted) length generated text. time complexity317fiKONSTAS & L APATAO(n3 ), case CYK algorithm. could converted grammar rules Chomskynormal form (CNF) implemented original CYK algorithm. Note grammarCNF, since contains unary productions type B, i.e., non-terminal symbolsright-hand side well. chose directly implement inference rules (1) (2) instead (seeFigure 5), since know arity grammar 2 thus able avoidblow-up number derived rules.defined parsing strategy, need way find likely derivation;pseudocode Figure 6 gives generation algorithm basic decoder. uses arraychart[A, i, j], cells get filled sets weights items. also uses identical arraybp[A, i, j] stores back-pointers antecedents item rooted A, well actualgenerated words processing lexical rules r GSURF (abusing somewhat traditionalinterpretation back-pointer array, storage pointers antecedent chart items). sizechart back-pointer array set pre-defined number N words wantgenerate (Section 4.3). procedure begins first filling diagonal cells chartunary spans rooted W , weights lexical rules r GSURF . Equivalently,back-pointers array takes corresponding generated word. Note conventionial parsingprocedure, always assume diagonal cells chart already filledactual words underlying sentence. case, assume fixed-size chartempty diagonal, gets filled top scoring words emitted lexical rulesgrammar. Next, items visited combined order, i.e., smaller spans come largerspans. Given way grammar constructed, items rooted F (corresponding fields)come items rooted R (records) ultimately S. particular point chart,algorithm considers antecedent items proven given rules GCS storeshighest scoring combination. Finally, construct resulting string g recursivelyvisiting bp[S, 0, N]. trace back-pointers item antecedents words giemitted axioms.3.5 k-best Decodingbasic decoder described far produce best derivation tree input givengrammar GGEN unfortunately may correspond best generated text. fact,output often poor model notion constitutes fluent language. grammar encodes little knowledge regard syntactic well-formedness grammatical coherence.Essentially, surface realization boils word bigram rules (6) (7) lexical rulesGSURF . word bigram rules inject knowledge word combinations model,kind information usually sparse cannot capture longer range dependencies.generation process Figure 6 picks top scoring words emitted lexical productionrules (lines 35), order produce best derivation root node S. Instead, wouldpreferable added chart list top k words (as well list top k items [B, i, j],[C, j, k] production rule r GCS ), thus produced k-best list derivations (withassociated strings) root node. done efficiently using lazy algorithm foundwork Huang Chiang (2005). Then, generation process finished,use language model higher order n-grams, head dependency-style rules rescorek-best lists generated strings directly (see also Charniak & Johnson, 2005 Liang, BouchardCote, Klein, & Taskar, 2006 application similar idea parsing machine translation,318fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:function ECODE(GGEN ,d,N)0 . . . Nr : W gi+1 GSURFchart[W, i, + 1] [W, i, + 1] :bp[W, i, + 1] gi+1. store actual word gi+1endendl 2 . . . Ni, k, j j = l < k < jitems [B, i, j] [B, i, k], [C, k, j] inferable chart rules r GCSr form Bchart[A, i, j] max ([B, i, j] : s1 P(r))bp[A, i, j] argmax ([B, i, j] : s1 P(r))endr form B Cchart[A, i, j] max (chart[B, i, k] chart[C, k, j] P(r))bp[A, i, j] argmax ([B, i, k] : s1 [C, k, j] : s2 P(r))endendendendreturn chart[S, 0, N], bp[S, 0, N]end functionFigure 6: Generation procedure basic decoder.respectively). Although method fast, i.e., linear k, would practically set khigh search among exponentially many possible generations given input.better solution, common practice machine translation, rescore derivationtrees online. Chiang (2007) intersects PCFG grammar weighted finite state automaton(FSA), represents n-gram language model; states FSA correspond n 1 terminal symbols. resulting grammar also PCFG incorporates FSA. Similarly,intersect grammar ensemble external probabilistic models, provided expressregular language. probable generation g calculated as:g = f arg max p(g) p( g, h | d)(5)g,hp(g, h | d) decoding likelihood sequence words g = g1 . . . gN length Nhidden correspondence h emits it, i.e., likelihood grammar given databaseinput scenario d. p(g) measure quality output could instance providedlanguage model (see Section 4.2 details estimate p(g, h | d) p(g)). theory,function f optimize h g jointly, thus admitting search errors. practice,however, resulting grammar intersection prohibitively large, calls pruningsearch space. following show extend basic generation decoder Figure 5intersecting (linearly) ensemble external probabilistic models.319fiKONSTAS & L APATAPPADVPRBNPPPNPNPNNSNNSQPCD CC CDCloudy temperatures1020CD10CCdegrees(a)ROOTRBCloudyNNStemperaturesCD20NNSdegrees(b)Figure 7: Phrase structure tree dependency graph sentence.addition n-gram language models routinely used means ensuring lexical fluency rudimentary grammaticality, also inject syntactic knowledgegenerator. represent syntactic information form directed dependencies couldpotentially capture long range relationships beyond horizon language model. Figure 7shows dependency-style representation sentence Cloudy temperatures 1020 degrees corresponding phrase structure. dependency graph Figure 7b capturesgrammatical relations words via directed edges syntactic heads dependents(e.g., verb subject noun modifying adjective). Edges labeledindicate type head-dependent relationship (e.g., subject object) unlabeled shownfigure. Formally, dependency structure set dependency pairs hwh , wa head whargument word wa , respectively. general, argument modifier, object complement; head times determines behavior pair. Figure 7b, cloudyhead with, head temperature, on. D(wh ) returns set dependency pairswhose head wh , e.g., D(10) = {and, 20}.Previous work (Ratnaparkhi, 2002) incorporated dependency information surface realization directly generating syntactic dependency tree rather word sequence.underlying probabilistic model predicts word conditioning syntactically related words320fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION(i.e., parent, grandparent, siblings). Importantly, approach requires corpusannotated dependency tree structures. obviate need manual annotation considering dependency structures induced automatically unsupervised fashion.this, use Dependency Model Valence (DMV; Klein & Manning, 2004), however,nothing inherent formulation restricts us model. unsupervised modellearns dependency structures broadly similar fashion (e.g., captures attachment likelihood argument head) could used instead proviso operatesstructures isomorphic derivation trees generated grammar. necessaryintersecting dependency model expresses (up to) context-free language, since formulatemodel also CFG5 .Finally, note although work two external information sources (i.e., language modelsdependencies), framework propose applies arbitrary number models expressingregular language. instance, could incorporate models capture dependencies relatingcontent selection field n-grams, however leave future work.3.6 Extended Decoderbegin introducing notation. define two functions p q operatesurface-level models strings = a1 . . . al , length l, ai V {?}. V vocabularyobserved text w (obtained training corpus), ? symbol represents elidedpart string. Recall k-best decoder needs keep list generated sub-stringsnode, rescoring purposes. Note sub-strings (potentially) differentobserved text w; top-scoring string root node essentially collapses final generatedtext g. Storing lists whole sub-strings generated far node, would require considerableamounts memory. avoid define function q(a) stores essential minimumstring information needed surface-level models (the ? symbol stands omittedparts string) step, order correctly compute rescoring weight. Function p(a)essentially calculates rescoring weight given string, linearly interpolating scoresindividual model mi weight . Therefore applying p(a) bottom-up fashion (seeextended decoder Figure 8) output q(a) allows us correctly compute rescoringweight model whole document incrementally. formally:p(a) = pmi (a)s.t.= 1(6)setting, make use language model (pm1 ) dependency model (pm2 ):pm1 (a1 . . . al ) =PLM (ai |ain+1 . . . ai1 )(7)nil?{a/ in+1 ,...,ai }pm2 (a1 . . . al ) = PDEP D(ah ) , ah {a1 , . . . , al }(8)function pm1 computes LM probabilities complete n-grams string; PLM returnsprobability observing word given previous n1 words. pm2 returns probability5. Intersecting two CFGs undecidable, PSPACE-complete one CFG finite (Nederhof & Satta, 2004).321fiKONSTAS & L APATAa1 . . . almostly cloudy ,mostly cloudy ? cloudy ,pm1 (a1 . . . al )PLM (,|mostly cloudy)1PLM (with|cloudy ,) PLM (a|, with)qm1 (a1 . . . al )mostly cloudy ? cloudy ,mostly cloudy ?Table 2: Example values functions pm1 qm1 phrase mostly cloudy, a.assume 3-gram language model.dependency model dependency structure headed word ah . dependency structure D,word ah dependants depsD (ah , le f t) attach left dependents depsD (ah , right)attach right. Equation (9) recursively defines probability dependency D(ah )rooted ah (Klein & Manning, 2004):PDEP D(ah ) =PSTOP (STOP|ah , dir, ad j)dir[le f t,right] depsD (ah ,dir)(9)PCHOOSE (aa |ah , dir)PDEP D(aa )PSTOP (STOP|ah , dir, ad j)PSTOP binary multinomial indicating whether stop attaching arguments head word ahgiven direction, i.e., left right, adjacency, i.e., whether directly adjacentah not. PCHOOSE multinomial possible argument words given ah directionattachment. next define function q(a) returns set strings, one model mi(we use shortly expand lexical items [A, i, j] basic decoder Figure 5).q(a) = hqm1 (a), . . . , qmM (a)i(10)(11)(a1 . . . an1 ? aln+2 . . . alqm1 (a1 . . . al ) =a1 . . . all notherwise(12)(13)all = 1q (a . . . )pm2 (a1 . . . ak )m2 1kqm2 (a1 . . . ak ak+1 . . . al ) =pm2 (ak+1 . . . al )1klq (a . . . ) otherwisem2 k+1l(14)Function qm1 (a) compresses string a, eliding words n-gramsrecognized. thus avoid storing whole sub-generation string, produced decoder far,mentioned earlier. Table 2 gives example values pm1 (a) qm1 (a) phrase mostlycloudy, a. Function qm2 (a) returns head string a. progressively combine substrings (a1 . . . ak ) (ak+1 . . . al ) together, 1 k l, head words ah1 {a1 , . . . , ak }ah2 {ak+1 , . . . , al }, function qm2 (a) returns either ah1 ah2 . probability PDEP decideswhether ah1 attaches ah2 vice versa, thus augmenting D(ah1 ) pair hah1 , ah2 D(ah2 )hah2 , ah1 i, respectively.322fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATIONjItems:[A, i, j; q(gi )]R(A B)R(A BC)Axioms:i+1[W, i, + 1; q(gi+1)] : p(gi )W gi+1 , gi+1 {, gen()}Inference rules:j(1)R(A B) : [B, i, j; q(gi )] : s1jj[A, i, j; q(gi )] : s1 p(gi )(2)R(A B C) : [B, i, k; q(gki )] : s1 [C, k, j; q(gk )] : s2jj[A, i, j; q(gi )] : s1 s2 p(gi )jGoal:[S, 0, N; q(hsin1 gN0 h/si)]Figure 8: Extended decoder using rescoring function p(g). Productions B B CGCS rules Figure 1; features grammar non-terminals omitted sakeclarity.Note equation (14) evaluates whether every word attach left right everyhead word, therefore essentially collapses to:Pmdep = PDEP D(ah ) = PSTOP (STOP|ah , dir, ad j)PCHOOSE (aa |ah , dir)(15)PSTOP (STOP|ah , dir, ad j)example, case pm2 (a1 . . . ak ), ah becomes one a1 . . . ak , aa one ak+1 . . . al ,dir = right ad j true ah = ak aa = ak+1 .ready extend basic decoder Figure 5, includes rescoring funcjtion p(gi ) generated sub-string gi . . . g j . new deduction system specified Figure 8.jItems [A, i, j] become [A, i, j; q(gi )]; represent derivations gi g j rooted nonterminal augmented model-specific strings defined above; words, includecompressed sub-generationselidedN parts head word. Analogously, goal itemn1Nincludes q hsi g0 h/si . Note g0 augmented (n 1) start symbols hsi endsymbol h/si. necessary correctly computing n-gram probabilities beginningend sentence. Figure 9 shows example instantiations inference rules extendeddecoder.generation procedure identical procedure described basic decoder Figure 6, save exponential items need deducted. Recall chart Figure 6stores cell chart[A, i, j] set combined weights cells correspond proved antecedents item [A, i, j]. new chart 0 extended decoder equivalently stores set listsjweights cell position chart 0 [A, i, j]. list contains items [A, i, j; q(gi )]jroot non-terminal span j, different set q(gi ), sorted best-first.running time integrating LM DMV models (N 3 |V |4(n1) |P|), V outputvocabulary P vocabulary used DMV. using lexicalized dependency model,323fiKONSTAS & L APATAR (R(skyCover1 .t) FS(temp1 , start) R(temp1 .t)) :[FS(temp1 , start), 1, 2; hwith, INi] : s1 [R(temp1 .t), 2, 8; ha low ? 15 degrees, JJi] : s2[R(skyCover1 .t), 1, 8; hwith ? 15 degrees, JJi] : s1 s2 p(hwith ? 15 degrees, JJi)R (FS(windSpeed1 , min) F(windSpeed1 , max) FS(windSpeed1 , max)) :[F(windSpeed1 , max), 3, 4; hhigh, JJi] : s1 [FS(windSpeed1 , max), 4, 5; h15, CDi] : s2[FS(windSpeed1 , min), 3, 5; hhigh 15, JJi] : s1 s2R (F(windDir1 , mode) W(windDir1 , mode)) : [W(windDir1 , mode), 3, 4; hsoutheast, JJi] : s1[F(windDir1 , mode), 3, 4; hsoutheast, JJi] : s1Figure 9: Inference rules extended decoder productions (2), (4), (7) Table 1(W EATHER G OV domain). strings h. . .i, correspond output functions qmlmqmdep . adopt unlexicalized dependency model, trained POS tags derivedPenn Treebank project (Marcus et al., 1993). first example corresponds wordJJ word low, second example JJ corresponds word high CDnumber 15, whereas third example JJ corresponds word southeast.P collapses V , otherwise contains part-of-speech (POS) tags every gi V . Noticerule (2) Figure 8 combines two items contain 2(n 1) words, hence exponent4(n 2). running time slow use practice, explain must adoptform pruning order able explore search space efficiently.3.7 Approximate SearchjjConsider task deriving k-best list items L([A, i, j; q(gi )]) deducted item [A, i, j; q(gi )]jrule (2) extended decoder Figure 8. item Lm ([A, i, j; q(gi )]) position list,j1 k, takes form [A, i, j; q(gm |i )]. example procedure shown Figure 10.jgrid depicts possible combinations items [B, i, k; q(gki )] [C, k, j; q(gk )] inferredrule form R(A B C) corresponding weights. k2 combinationsused create resulting k-best list shown bottom figure, store cellchart 0 [A, i, j]. However, want keep k items, going prunedaway. fact, grid example worst case cube, i.e., hold two threedimensions, one rules B C left hand-side non-terminal A, twocorresponding items rooted B C6 ; calls calculation k3 combinations.better approach apply cube pruning (Chiang, 2007; Huang & Chiang, 2005), i.e., computesmall corner grid prune items fly, thus obviating costly computationk3 combinations.6. deducted item [R(skyCover1 .t); q(g81 )] Figure 10 also inferred rule R(R(skyCover1 .t)R(windSpeed1 .t) FS(windSpeed1 , start)) (and corresponding antecedent items) rule R(R(skyCover1 .t)R(rainChance1 .t) FS(rainChance1 , start)), on. illustrate slice cube, depicting enumeration k-best lists fixed grammar rule, sake clarity.324fi[FS(temp1 , start), 1, 2; hwith, INi][FS(temp1 , start), 1, 2; ha, DTi][FS(temp1 , start), 1, 2; haround, RBi]G LOBAL ODEL C ONCEPT- -T EXT G ENERATION.95.93.91[R(temp1 .t), 2, 8; ha low ? 15 degrees, JJi] .56.40.25.20[R(temp1 .t), 2, 8; hlow around ? 15 degrees, JJi] .54.35.30.17[R(temp1 .t), 2, 8; ha low ? around 17, RBi] .44.15.08.10[R(skyCover1 .t), 1, 8; hwith ? 15 degrees, JJi[R(skyCover1 .t), 1, 8; hwith low ? 15 degrees, JJi][R(skyCover1 .t), 1, 8; ha ? 15 degrees, JJi][R(skyCover1 .t), 1, 8; haround low ? 15 degrees, RBi][R(skyCover1 .t), 1, 8; hwith ? around 17, RBi]: .40: .35: .25: .17: .15Figure 10: Computing exhaustive list deducted item [R(skyCover1 .t); q(g81 )] via application inference rule (2) extended decoder Figure 9. antecedent itemsrule R (R(skyCover1 .t) R(temp1 .t) FS(temp1 , start)) items [R(temp1 .t), 2, 8; q(g82 )],FS(temp1 , start), 1, 2; q(g21 )]. figure shows slice cube, particular rule;side grid lists top three candidate items antecedent item, sorted bestfirst. Numbers grid represent total score combination.Consider Figure 11 example. side grid shows lists top three itemsantecedent item. Numbers grid represent total score combination.Figures 11b11d illustrate enumeration top three combinations best-first order. Cellsgray represent frontiers iteration; cells black resulting top three items.basic intuition behind cube pruning pair antecedent items u1 = [B, i, k; q(gki )], u2 =j[C, k, j; q(gk )] sorted k-best lists L(u1 ), L(u2 ), best combinations lie closeupper-left corner grid. example, 3-best list nodes u1 = [R(temp1 .t), 2, 8; q(g82 )]325fi[FS(temp1 , start), 1, 2; hwith, INi][FS(temp1 , start), 1, 2; ha, DTi][FS(temp1 , start), 1, 2; haround, RBi][FS(temp1 , start), 1, 2; hwith, INi][FS(temp1 , start), 1, 2; ha, DTi][FS(temp1 , start), 1, 2; haround, RBi][FS(temp1 , start), 1, 2; hwith, INi][FS(temp1 , start), 1, 2; ha, DTi][FS(temp1 , start), 1, 2; haround, RBi]KONSTAS & L APATA.95.93.91.95.93.91.95.93.91[R(temp1 .t), 2, 8; ha low ? 15 degrees, JJi] .56.40.25.20.40.25.20.40.25.20[R(temp1 .t), 2, 8; hlow around ? 15 degrees, JJi] .54.35.30.17.35.30.17.35.30.17[R(temp1 .t), 2, 8; ha low ? around 17, RBi] .44.15.08.10.15.08.10.15.08.10(a)(b)(c)Figure 11:Computing item combinations u1 = [R(temp1 .t), 2, 8; q(g82 )]u2 = [FS(temp1 , start), 1, 2; q(g21 )] using cube pruning. (a)(c) enumerate combinations items order construct resulting k-best list described text.u2 = [FS(temp1 , start), 1, 2; q(g21 )] are:hL(u1 ) = ha low ? 15 degrees, JJi, hlow around ? 15 degrees, JJi, ha low ? around 17, RBihL(u2 ) = hwith, INi, ha, DTi, haround, RBiintuitively best combination derivation top left corner7 :L1 (u1 ), L1 (u2 ) = ha low ? 15 degrees, JJi, hwith, INi = hwith ? 15 degrees, INicases combination cost, i.e., score grammar rule multipliedrescoring weight p(g), negligible, could start enumerating item combinations order shown Figures 11b11c, starting (L1 (u1 ), L1 (u2 )) stopping k. Since twolists sorted guaranteed L2 (u1 ), i.e., second item k-best list u1 either(L1 (u1 ), L2 (u2 )) (L2 (u1 ), L1 (u2 )) (in example Figure 11b latter). thus selectmove compute neighboring combinations, on.8 computationk-best lists axioms [W, i, + 1; q(gii+1 )], enumerate top-k terminal symbols gi+1 .take account combination cost, grid non-monotonic, therefore bestfirst guarantee longer holds enumerate neighbors fashion described. HuangChiang (2007) argue loss incurred search error insignificant comparedspeedup gained. case, overcome this, compute resulting k-best list, first adding7. Note head sub-generation fragment shifted head L2 .8. Contrary Huang Chiang (2007) use probabilities instead log scores computation itemcombinations, hence select biggest scoring combinations.326fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATIONcomputed item combinations temporary buffer, resort enumeratedtotal k combinations.3.8 Learningrepresent grammar input scenario weighted hypergraph (Gallo, Longo, Pallottino, & Nguyen, 1993). follow procedure proposed Klein Manning (2001)allows transform CFG hypergraph. order learn weights grammar rulesdirectly estimate hypergraph representation using EM algorithm. Formally,objective trying optimize factorizes into:P(r, ri .f, w|d) = P(ri |d) P(ri . f j |ri ) P(w j |ri . f j , ri )j(16)kr set record tokens ri . Given training set scenarios database recordsobserved text w maximize marginal likelihood data, summing recordtokens ri fields ri .f, regarded latent variables:arg maxp(r, ri .f, w|d; ),(17)(w,d) r,ri .fmultinomial distributions weights GGEN . EM algorithm alternatesE-step M-step. E-step compute expected counts rules usingdynamic program similar inside-outside algorithm (Li & Eisner, 2009). M-step,optimise normalising counts computed E-step. initialise EM uniformdistribution multinomial distribution applied add-0.001 smoothing multinomialM-step. average, EM converged datasets 15 iterations. Note n-gramlanguage model dependency model trained externally, hence parametersoptimized alongside model. generation procedure extended decoder Figure 8implemented using dynamic programming. choice hypergraph representation merelyone several alternatives. example, could adopted representation based weightedfinite state transducers (de Gispert, Iglesias, Blackwood, Banga, & Byrne, 2010) since modeldescribes regular language terms PCFG surface level models intersectwith. also possible represent grammar pushdown automaton (Iglesias, Allauzen,Byrne, de Gispert, & Riley, 2011) intersect finite automata representing language modeldependency-related information, respectively. choice hypergraph representationmotivated compactness9 fact allows future extensions PCFGrules capture global aspects generation problem (e.g., document planning)unavoidably result context-free languages.4. Experimental Designsection present experimental setup assessing performance model.give details datasets used, explain model trained, describe modelsused comparison approach, discuss system output evaluated.9. Hypergraphs commonly used machine translation literature allow compact encoding SCFGseven though cases also describe regular languages. example, true SCFGs employedhierarchical phrase-based SMT (Chiang, 2007) assume finite input language permit infiniterecursions.327fiKONSTAS & L APATA4.1 Dataused system generate soccer commentaries, weather forecasts, spontaneous utterances relevant air travel domain (examples given Figure 1). first domainused dataset described work Chen Mooney (2008), consists 1,539 scenarios 20012004 Robocup game finals (henceforth ROBO C UP). scenario containsaverage |d| = 2.4 records, paired short sentence (5.7 words). domainsmall vocabulary (214 words) simple syntax (e.g., transitive verb subject object).Records dataset aligned manually corresponding sentences (Chen & Mooney,2008). Given relatively small size dataset, performed cross-validation following previous work (Chen & Mooney, 2008; Angeli et al., 2010). trained system three ROBO Cgames tested fourth, averaging four train/test splits.weather forecast generation, used dataset presented work Liang et al. (2009),consists 29,528 weather scenarios 3,753 major US cities (collected four days).vocabulary domain (henceforth W EATHER G OV) comparable ROBO C (345 words),however, texts longer (N = 29.3) varied. average, forecast 4 sentencescontent selection problem challenging; 5.8 36 records per scenariomentioned text roughly corresponds 1.4 records per sentence. used 25,000scenarios W EATHER G OV training, 1,000 scenarios development 3,528 scenariostesting. partition used work Angeli et al. (2010).air travel domain used ATIS dataset (Dahl, Bates, Brown, Fisher, Hunicke-Smith,Pallett, Pao, Rudnicky, & Shriberg, 1994), consisting 5,426 scenarios. transcriptionsspontaneous utterances users interacting hypothetical online flight booking system.used dataset introduced work Zettlemoyer Collins (2007)10 automatically converted lambda-calculus expressions attribute-value pairs following conventions adopted study Liang et al. (2009).11 Figure 1c shows output conversion process original lambda expression x. f light(x) f rom(x, denver) to(x, boston)day number departure(x, 9) month departure(x, august) < (arrival time(x), 16:00). Givenexpression, first create record variable (e.g., x). assign record typesaccording corresponding class types (e.g., variable x class type flight). Next, fieldsvalues added predicates two arguments class type first argument matching record type. name predicate denotes field, second argumentdenotes value (e.g., f rom(x, denver) used fill record type Flight, since typefirst argument also f light). name function becomes field name, (i.e., from)second argument set value, (i.e., denver). Note functions namesmonth departure, month arrival, day number arrival, day number departure on. orderreduce resulting number record types, created aggregate record types embedcommon information (i.e., departure, arrival) special field. example, functionday number departure split value departure field dep/ar record Day,field number value 9. also defined special record types, ConditionSearch. latter introduced every lambda operator assigned categorical fieldvalue flight refers record type variable x.10. original corpus contains user utterances single dialogue turns would result trivial scenarios. Zettlemoyer Collins (2007) concatenate user utterances referring dialogue act, (e.g., book flight), thusyielding complex scenarios longer sentences.11. See Konstas (2013) resulting dataset.328fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATIONcontrast two previous datasets, ATIS much richer vocabulary (927 words);scenario corresponds single sentence (average length 11.2 words) 2.65 19 recordtypes mentioned average. Note original lambda expressions created basedutterance, thus contain necessary information conveyed meaning text.result, converted records scenario mentioned corresponding text.Following work Zettlemoyer Collins (2007), trained 4,962 scenarios testedATIS NOV93 contains 448 examples.4.2 Model TrainingGeneration model amounts finding best derivation (g, h) maximizes producttwo likelihoods, namely p(g, h | d) p(g) (see equation (5)). p(g, h | d) correspondsrules GGEN generate word sequence g, whereas p(g) likelihood g independentlyd. estimate p(g, h | d) described Section 3.8. Examples top scoring itemsmultinomial distributions grammar rules GGEN given Table 3. obtainestimate p(g) linearly interpolating score language model DMV (Klein &Manning, 2004).Specifically, language models trained SRI toolkit (Stolcke, 2002) using add-1smoothing.12 ROBO C domain, used bigram language model given averagetext length relatively small. W EATHER G OV ATIS, used trigram language model.obtained unlexicalized version DMV13 domains. datasetstagged automatically using Stanford POS tagger (Toutanova, Klein, Manning, & Singer, 2003)words augmented part speech, e.g., low becomes low/JJ, around becomesaround/RB on; words several parts speech duplicated many timesnumber different POS tags assigned tagger. example, gust may actnoun verb, given context, hence keep augmented forms, i.e., gust/NNSgust/VBS. initialized EM uniform distributions small amount noise14 addedmultinomials (i.e., PSTOP PCHOOSE ) break initial symmetry. Klein Manning (2004)use harmonic distribution instead, probability one word heading another higherappear closer one another. Preliminary results development set showedformer initialization scheme robust across datasets.model two hyperparameters: number k-best derivations considered decoder vector weights model integration. Given interpolate two models whose weights sum one, need modulate single interpolation parameter 0 LM 1. LM 0, decoder influenced DMV converselyLM 1 decoder influenced language model. general case, could learninterpolation parameters using minimum error rate training (Och, 2003), howevernecessary experiments. performed grid search k LM held-out data taken12. Adopting complex smoothing technique Good-Turing (Good, 1953) usually applicablesmall vocabularies. statistics computing called count-of-counts, i.e., number words occurring once,twice on, sufficient lead poor smoothing estimates.13. trained WSJ-10 corpus, implementation DMV obtained accuracy reportedwork Klein Manning (2004). WSJ-10 consists 7,422 sentences 10 words removingpunctuation.14. Repeated runs different random noise WSJ-10 corpus yielded results; accuracy stabilized around60th iteration (out 100).329fiKONSTAS & L APATAWeight DistributionP( | pass, from, purple2)P( | steal, null, NULL)P( | turnover, null, NULL)Top-5 scoring itemspurple2, a, makes, pink10, shortball, the, steals, from, purple8to, the, ball, kicks, loses(a) ROBO CWeight DistributionP(ri .t | temperature)P(ri .t | windSpeed)P(ri .t | skyCover)P( fi | temperature.time)P( fi | windSpeed.min)P( fi | gust.max)P( | skyCover, percent, 0-25)P( | skyCover, percent, 25-50)P( | rainChance, mode, Definitely)Top-5 scoring itemswindDir, sleetChance, windSpeed,freezingRainChance, windChillgust, null, precipPotential,windSpeed, snowChancetemperature, skyCover, thunderChance,null, rainChancemin, max, mean, null, timemax, time, percent, mean, nullmin, mean, null, time, max,, clear, mostly, sunny, mid,, cloudy, partly, clouds, increasingrain, of, and, the, storms(b) W EATHER G OVWeight DistributionP(ri .t | search)P(ri .t | flight)P(ri .t | day)P( | flight, to, mke)P( | search, what, flight)P( | search, type, query)Top-5 scoring itemsflight, search, when, day, conditionsearch, day, flight, month, conditionwhen, search, flight, month, conditionmitchell, general, international, takeoffs, departI, a, like, to, flightlist, the, me, please, show(c) ATISTable 3: Top-5 scoring items multinomial distributions record rules, field rulescategorical word rewrite rule GGEN (see rules (2), (4), (8) Table 1, respectively). firstcolumn table shows underlying multinomial distribution corresponding rule.example P( | pass, from, purple2), corresponds distribution emitting word givenvalue purple2 field record type pass.W EATHER G OV, ROBO C UP, ATIS, respectively. optimal values k LMthree domains (when evaluating system performance BLEU-4) shown Table 4.conducted two different tuning runs, one version model takes LMaccount (k- BEST- LM; LM = 1) another one LM DMV integrated(k- BEST- LM - DMV). seen, optimal values k generally larger k- BEST- LM - DMV.probably due noise introduced DMV; result, decoder exploresearch space thoroughly. effort investigate impact DMV further, fixed330fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATIONk- BEST- LMROBO CW EATHER G OVATISk251540(a) Interpolation LMk- BEST- LM - DMVROBO CW EATHER G OVATISk856540LM0.90.30.6(b) Interpolation LM DMVTable 4: Optimal values parameters k LM calculated performing grid searchBLEU-4 development set. LM Table (a) set 1.LM = 0 development set performed grid search DMV own. Modelperformance dropped significantly (by 58% BLEU points) entirely surprising givenDMV alone cannot guarantee fluent output. contribution rather rests capturingglobal dependencies outwith local horizon language model.4.3 Determining Output LengthUnlike generation systems operate surface realization level word templates,emit word individually bottom-up fashion. Therefore, need decide numberwords N wish generate beginning decoding process. common approachfix N average text length training set (Banko, Mittal, & Witbrock, 2000). However,would good choice case, since text length follow normal distribution.shown Figure 12 distribution N across domains mostly skewed.avoid making unwarranted assumptions output, trained linear regressionmodel determines text length individually scenario. input model,used flattened version database, features record-field pairs. underlying ideascenario contains many records fields, use words expressthem. contrast, number records fields small, likely outputlaconic. attempt capture number words needed communicate specific record-fieldpairs, experimented different types feature values, e.g., setting feature actualvalue (categorical numerical) frequency training data. former scheme workedbetter denser datasets, W EATHER G OV ROBO C whereas latter adoptedATIS sparser database, means smooth infrequent values. trainedtraining set tested development set regression model obtained correlationcoefficient 0.64 ROBO C UP, 0.84 W EATHER G OV, 0.73 ATIS (using Pearsons r).4.4 System Comparisonevaluated three configurations system. baseline uses top scoring derivationsubgeneration (1- BEST) two versions model make better use decodingalgorithm. One version integrates k-best derivations LM (k- BEST- LM), version additionally takes DMV account (k- BEST- LM - DMV). Preliminary experimentsmodel integrates k-best derivations DMV exhibit satisfactory results (seeSection 4.2) omit sake brevity. compared output models331fiKONSTAS & L APATA6000700600500400300200100FrequencyFrequency700050004000300020010003 5 7 9 11 13 15 179(a) Text length N ROBO C21 33 45 57 69 81(b) Text length N W EATHER G OVFrequency240020001600120080040026 10 14 18 22 26 30 34 38 44 48(c) Text length N ATISFigure 12: Text length distribution ROBO C UP, W EATHER G OV, ATIS (training set).Angeli et al. (2010) whose approach closest state-of-the-art W EATHER G OV.15ROBO C UP, also compared best-published results (Kim & Mooney, 2010).4.5 Evaluationevaluated system output automatically, using BLEU-4 modified precision score (Papineni,Roukos, Ward, & Zhu, 2002) human-written text reference. addition, evaluatedgenerated text via judgment elicitation study. Participants presented scenariocorresponding verbalization asked rate latter along two dimensions: fluency(is text grammatical overall understandable?) semantic correctness (does meaningconveyed text correspond database input?). subjects used five point rating scalehigh number indicates better performance. randomly selected 12 documentstest set (for domain) generated output models (1- BEST k- BEST- LM - DMV)Angeli et al.s (2010) model. also included original text (H UMAN) gold standard. thus15. grateful Gabor Angeli providing us code system.332fiF IXEDJ OINTG LOBAL ODEL C ONCEPT- -T EXT G ENERATIONSystem1- BESTk- BEST- LMk- BEST- LM - DMV1- BESTk- BEST- LMk- BEST- LM - DMVNGELIK IM -M OONEYBLEU8.01.24.8823.1410.79.30.9029.7328.7047.27.System1- BESTk- BEST- LMk- BEST- LM - DMVNGELIBLEU8.64.33.7034.18.38.40.(b) W EATHER G OVSystem1- BESTk- BEST- LMk- BEST- LM - DMVNGELIBLEU11.85.29.3030.3728.70(c) ATIS(a) ROBO CTable 5: BLEU-4 scores ROBO C UP, W EATHER G OV, ATIS ( : significantly different1- BEST; : significantly different NGELI; . significantly different k- BEST- LM; : significantly different k- BEST- LM - DMV; : significantly different K IM -M OONEY.obtained ratings 48 (12 4) scenario-text pairs domain. study conductedInternet using Amazon Mechanical Turkand involved 305 volunteers (104 ROBO C UP, 101W EATHER G OV, 100 ATIS), self reported native English speakers. experimentalinstructions given Appendix A.5. Resultsconducted two experiments ROBO C domain. first assessed performancegenerator joint content selection surface realization obtained results shownupper half Table 5a (see J OINT). second experiment forced generator usegold-standard records database. necessary order compare previouswork (Angeli et al., 2010; Kim & Mooney, 2010).16 results summarized lower halfTable 5a (see F IXED).Overall, generator performs better 1- BEST baseline comparably Angeli et al.(2010). k- BEST- LM - DMV slightly worse k- BEST- LM. due fact sentencesROBO C short (their average length 5.7 words) result model cannotrecover meaningful dependencies. Using Wilcoxon signed-rank test find differencesBLEU scores among k- BEST- LM - DMV, k- BEST- LM NGELI statistically significant. Kim Mooney (2010) significantly outperform three models 1- BEST baseline(p < 0.01). entirely surprising, however, model requires considerablesupervision (e.g., parameter initialization) includes post-hoc re-ordering component.Finally, also observe substantial increase performance compared joint content selection surface realization setting. expected generator faced easier taskless scope error.regard W EATHER G OV, model (k- BEST- LM k- BEST- LM - DMV) significantly improves 1- BEST baseline (p < 0.01) lags behind Angeli et al. (2010) difference16. Angeli et al. (2010) Kim Mooney (2010) fix content selection record field level. letgenerator select appropriate fields, since two per record type level complexityeasily tackled decoding.333fiKONSTAS & L APATAF1 (%)BLEU-4 (%)1009080706050403020105 000 10 000 15 000 20 000 25 000Number training scenarios50454035302520151055 000 10 000 15 000 20 000 25 000Number training scenarios(a) Alignment(b) Generation outputFigure 13: Learning curves displaying quality alignments generated output varyfunction size training data.statistically significant (p < 0.01). Since system emits words based language model rathertemplate, displays freedom word order lexical choice, thus likelierproduce creative output, sometimes even overly distinct compared reference. Dependencies seem play important role here, yielding overall better performance.17 Interestingly,k- BEST- LM - DMV significantly better k- BEST- LM domain (p < 0.01). SentencesW EATHER G OV longer ROBO C allows k- BEST- LM - DMV learn dependencies capture information complementary language model.ATIS, k- BEST- LM - DMV model significantly outperforms 1- BEST (p < 0.01)NGELI (p < 0.05), whereas k- BEST- LM performs comparably. Furthermore, k- BEST- LM - DMVsignificantly better k- BEST- LM (p < 0.01). ATIS domain challengingrespect surface realization. vocabulary larger ROBO C factor 4.3W EATHER G OV factor 2.7. increased vocabulary model learns richerdependencies improve fluency overall performance.also examined amount training data required model. performed learningexperiments W EATHER G OV since contains training scenarios ROBO C ATISchallenging regard content selection. Figures 13(a) (b) show number training instances influnces quality alignment generation output, respectively.measure alignment F-score following methodology outlined work Liang et al.(2009) using gold alignments. graphs show 5,000 scenarios enough obtainingreasonable alignments generation output. small upward trend detected increasing training instances, however seems considerably larger amounts would requiredobtain noticeable improvements.17. DMV commonly trained sentence-by-sentence basis. ROBO C ATIS datasets, scenario-textpair corresponds single sentence. W EATHER G OV, however, text may include multiple sentences.latter case trained DMV multi-sentence text without presegmenting individual sentences.non-standard training regime seem pose difficulty domain, safely assumeexamples elided root head, namely weather (e.g., weather mostly cloudy, lowaround 30).334fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATIONSystem1- BESTk- BEST- LM - DMVNGELIH UMANROBO CFSC2.142.094.053.554.013.474.173.97W EATHER G OVFSC2.252.533.893.543.823.724.013.58ATISF2.403.963.864.16SC2.493.823.313.96Table 6: Mean ratings fluency (F) semantic correctness (SC) system output elicitedhumans ROBO C UP, W EATHER G OV, ATIS ( : significantly different 1- BEST; : significantly different NGELI; : significantly different k- BEST- LM - DMV; : significantlydifferent H UMAN).results human evaluation study shown Table 6. report mean ratingssystem gold-standard human authored text. experimental participants rated outputtwo dimensions, namely fluency (F) semantic correctness (SC). elicited judgmentsk- BEST- LM - DMV generally performed better k- BEST- LM automatic evaluation(see Table 5). carried Analysis Variance (A NOVA) examine effect systemtype (1- BEST, k- BEST- LM - DMV, NGELI, H UMAN) fluency semantic correctnessratings. used Tukeys Honestly Significant differences (HSD) test, explained Yandell(1997) assess whether means differences statistically significant.three domains system (k- BEST- LM - DMV) significantly better 1- BEST baseline (a < 0.01) terms fluency. output indistinguishable gold-standard (H UMAN)NGELI (pair-wise differences among k- BEST- LM - DMV, NGELI H UMAN statistically significant). respect semantic correctness, ROBO C UP, k- BEST- LM - DMV significantly better 1- BEST (a < 0.01) significantly worse H UMAN (a < 0.01). Althoughratings k- BEST- LM - DMV numerically higher NGELI, difference statistically significant. NGELI also significantly worse H UMAN (a < 0.01). W EATHER G OV,semantic correctness k- BEST- LM - DMV NGELI significantly different. twosystems also indistinguishable H UMAN. ATIS, k- BEST- LM - DMV best performing model respect semantic correctness. significantly better 1- BEST NGELI(a < 0.01) significantly different H UMAN.sum, observe performance improves k-best derivations taken account(the 1- BEST system consistently worse). results also show taking dependency-basedinformation account boosts model performance achievedlanguage model. model par NGELI ROBO C W EATHER G OV performsbetter ATIS evaluated automatically humans. Error analysis suggestsreason NGELIs poorer performance ATIS might inability create good qualitysurface templates. due lack sufficient data fact templates cannotfully express database configurations many different ways. especially trueATIS consists transcriptions spontaneous spoken utterances meaningrendered many different ways. example, phrases show flights,flights, flights, please give flights, convey exactmeaning stemming Search record.model learns domain specific conventions say say data,without hand-engineering manual annotation. Porting system different domain335fiKONSTAS & L APATABad PassInput:1- BEST:k- BEST- LM - DMV:pink11 purple5pink11 pass purple5 purple5 pink11 pass purple5 purple5 purple5pink11 made pass intercepted purple5NGELI:pink11 made bad pass missed target pickedpurple5H UMAN:pink11 tries pass intercepted purple5(a) ROBO CInput:1- BEST:k- BEST- LM - DMV:TemperatureCloud Sky Covertimemin mean max06:00-21:00 32 39 46timepercent (%)06:00-21:0075-100Wind SpeedWind Directiontimemin mean max06:00-21:00 6710timemode06:00-21:00 SENear 46. Near 46. Near 46. Near 46. Near 46. near 46. Southeast wind.Mostly cloudy, high near 46. South southeast wind 6 10 mph.NGELI :chance rain drizzle, high near 46. Southeast wind 610 mph. mph. Chance precipitation 60%.H UMAN:Mostly cloudy, high near 46. South southeast wind 6 10 mph.(b) W EATHER G OVInput:1- BEST:k- BEST- LM - DMV:FlightDaySearchmilwaukee phoenixdaydep/ar/retsaturday departuretypequery flightMilwaukee Phoenix Saturday Saturday Saturday SaturdayShow flights Milwuakee Phoenix SaturdayNGELI :Show flights Milwuakee Phoenix SaturdayH UMAN:Milwuakee Phoenix Saturday(c) ATISFigure 14: Example output (a) sportscasting, (b) weather forecasting, (c) air travel domainscorrect content selection.straightforward, assuming database corresponding (unaligned) text. long databaseobeys structure grammar GGEN , need retrain model obtain weightsgrammar rules; addition, system requires domain specific language model optionally336fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATIONGold:Output:TemperatureCloud Sky CoverChance Raintimemin mean max06:00-21:00 30 38 44timepercent (%)06:00-21:0075-100TimeMode06:00-21:00 Slight ChanceWind SpeedWind DirectionPrecipitation Potential (%)timemin mean max06:00-21:00 667timemode06:00-21:00 ENEtimemin mean max06:00-21:00 9 20 3540 percent chance showers 10am. Mostly cloudy, highnear 44. East northeast wind around 7 mph.(a) Gold standard content selection verbalizationContentSelection:Output:TemperatureCloud Sky CoverChance Raintimemin mean max06:00-21:00 30 38 44timepercent (%)06:00-21:0075-100timemode06:00-09:00 ChanceWind SpeedWind DirectionChance Thunderstormtimemin mean max06:00-21:00 667timemode06:00-21:00 ENEtimemode06:00-13:00 -13:00-21:00 --chance showers. Patchy fog noon. Mostly cloudy,high near 44. East wind 6 7 mph.(b) k- BEST- LM - DMV content selectionContentSelection:Output:TemperaturePrecipitation Potential (%)Chance Raintimemin mean max06:00-21:00 30 38 44timemin mean max06:00-21:00 9 20 35timemode06:00-09:00 ChanceWind SpeedWind DirectionChance Thunderstormtimemin mean max06:00-21:00 667timemode06:00-21:00 ENEtimemode06:00-21:00 --chance showers. Patchy fog noon. Mostly cloudy, highnear 44. East wind 6 7 mph. Chance precipitation 35%(c) NGELI content selectionFigure 15: Example output W EATHER G OV domain incorrect content selection (in gray).337fiKONSTAS & L APATAROOTshowPhoenixPhoenixshow flights Milwaukee Phoenix SaturdayFigure 16: Dependency structure sentence Show flights Milwaukee PhoenixSunday generated k- BEST- LM - DMV (see Figure 14c). Intermediate nodes tree denotehead words subtree.information heads dependents DMV learns unsupervised fashion.latter case, also need tune hyperparameter LM , cases k. Note,fine-tuning k becomes less important integrating language model only. explainSection 4.2, DMV possibly introduces noise, therefore modulate k carefullyallow decoder search bigger space.Examples system output correct content selection record level given Figure 14. Note case ROBO C UP, content selection fixed gold standard.seen, generated text close human authored text. Also note outputsystem improves considerably taking k-best derivations account (compare 1- BESTk- BEST- LM - DMV figure). Figure 15a shows examples incorrect content selectionrecord level W EATHER G OV domain. Figure 15a shows gold standard content selectioncorresponding verbalization. Figures 15b 15c show output k- BEST- LM - DMVsystem NGELI. Tables black denote record selection identical gold standard, whereastables grey denote false positive recall. k- BEST- LM - DMV identifies incorrect valueMode field Chance Rain record; addition, fails select Precipitation Potential (%) record altogether. former mistake affect correctness generatorsoutput, whereas latter (i.e., fails mention exact likelihood rain, 40% goldstandard 35% NGELIs output). Finally, Figure 16 shows dependency structuremodel produced sentence Show flights Milwaukee Phoenix SaturdayFigure 14c; notice long range dependency flights on, would otherwiseinaccessible language model.338fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION6. Conclusionspresented end-to-end generation system performs content selection surfacerealization simultaneously. Central approach encoding generation parsing problem. reformulate input (a set database records text describing them)PCFG show approximately find best generated string licensed grammar.evaluated model three domains (ROBO C UP, W EATHER G OV, ATIS) showedable obtain performance comparable superior state-of-the-art. experimentsalso designed assess several aspects proposed framework use k-best decoding intersection grammar multiple information sources. observed k-bestdecoding essential producing good quality output. Across domains, performance increasesfactor least two multiple derivations taken account. addition, intersectinggrammar dependency-based information seems capture syntactic information complementary language model. argue approach computationally efficient viablepractical applications. Outwith generation, hope work described mightrelevance fields summarization machine translation.Future extensions many varied. obvious extension concerns porting frameworkchallenging domains richer vocabulary longer texts (e.g., product descriptions,user manuals, sports summaries). related question extend PCFG-based approachadvocated capture discourse-level document structure. future directions involveexploiting information available database directly. model takes accountk-best derivations decoding time, however inspection indicates often failsselect best one. Initial work (Konstas & Lapata, 2012) shows model presentedadapted use forest reranking, technique approximately reranks packed forestexponentially many derivations (Huang, 2008). reranker essentially structured perceptron(Collins, 2002) enriched local non-local features. therefore allows explicitly modeldependencies across fields, records, interactions.Finally, although focus paper, worth pointing model describedalso perform semantic parsing, i.e., convert text formal meaning representation.done trivially modifying grammar Table 1. Instead observing words terminals(rules (8) (9)), observe values fields, given particular word w, field, record:W(r, r. f ) f .v[P( f .v | r, r. f , f .t, w)]W(r, r. f ) gen(w)[P(gen(w).mode | r, r. f , f .t=int)]decoding, prior p(g) equation (4) becomes p( f .v) naively obtainedcreating n-gram language model alignments meaning representationstext. alignments principle hidden could estimated using model Lianget al. (2009).Acknowledgmentsgrateful anonymous referees whose feedback helped substantially improvepresent paper. Thanks Luke Zettlemoyer Tom Kwiatkowski help ATISdataset well Giorgio Satta Frank Keller helpful comments suggestions. also339fiKONSTAS & L APATAthank members Probabilistic Models reading group University Edinburghfeedback. preliminary version work published proceedings NAACL 2012.Appendix A. Experimental InstructionsA.1 Instructionsexperiment given tables contain facts weather (e.g., Temperature, Chance Rain, Wind Direction, Cloud Coverage on) translationnatural language. Example 1 tabulates weather related information translationRainy high near 47. Windy, east wind 5 15 mph.Example 1CategoryTemperatureFieldstime: 17.0006.00(+1 day) min: 30mean: 40 max: 47Wind Direction time: 17.0006.00(+1 day) mode: SECloud Sky Cover time: 17.0006.00(+1 day) percent: 2550Chance Rain time: 17.0021.00mode: LikelyRainy high near 47. Windy , east wind 5 15 mph.row table contains different weather-related event. first row talks temperature, second one wind direction, etc. Different event types instantiate different fields.example, Temperature four fields, time, min, mean, max. Fields turn values,either numbers (e.g., 47 degrees Fahrenheit event Temperature), words (e.g.,Likely Slight Chance event Chance Rain).specifically, read table follows. Temperature, field timevalue 17.00-06.00(+1 day) refers temperatures measured 5pm 6amfollowing day. minimum temperature recorded time period 30 degrees Fahrenheit(field min), maximum 47 degrees (field max) average temperature 40 degrees(field mean). time period, wind blow south east direction (the modeWind Direction SE). 2550% sky covered clouds (see field percent value25-50 Cloud Sky Cover), may interpreted slightly cloudy outlook. Finally,5pm 9pm likely rain, indicated mode field value Likely ChanceRain event.Note temperature values Fahrenheit scale. Fahrenheit scale alternativetemperature scale Celsius, proposed 1724 physicist Daniel Gabriel Fahrenheit.formula converts Fahrenheit degrees Celsius [F] = [C] 59 + 32. So, instance, 1 C =30 F. Also note, measure speed used throughout experiment miles per hour, mphshort.natural language translations generated computer program. taskrate translations two dimensions, namely Fluency Semantic Correctness scale340fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION1 5. far Fluency concerned, judge whether translation grammaticalwell-formed English gibberish. translation grammatical, ratehigh terms fluency. lot repetition translation seems like word salad,give low number.Semantic Correctness refers meaning conveyed translation whether corresponds reported tabular data. words, translation conveycontent table not? translation nothing categories, fields valuesdescribed table, probably give low number Semantic Correctness.translation captures information listed table, give high number. Bear mind slight numerical deviations normal penalized (e.g.,common weather forecasters round wind speed values closest 5, i.e., 50 mph instead47 mph).A.2 Rating ExamplesExample 1, would probably give translation high score Fluency (e.g., 4 5), sincecoherent contain grammatical errors. However, give lowscore Semantic Correctness (e.g., 13), conveys information table.example, windy wind 5 15 mph relate wind speedmentioned table. Let us consider following example:Example 2CategoryFieldsTemperaturetime: 17.0006.00(+1 day) min: 40mean: 45 max: 50Wind Direction time: 17.0006.00(+1 day) mode:Wind Speedtime: 17.00 06.00(+1 day) min: 5mean: 7 max: 15Cloud Sky Cover time: 17.0006.00(+1 day) percent: 025Sunny, low around 40. South wind 5 15 mph.Here, give translation high scores dimensions, namely Fluency Semantic Correctness. text grammatical succinctly describes content table.example, 4 5 would appropriate numbers.Example 3CategoryFieldsTemperaturetime: 17.0006.00(+1 day) min: 30mean: 40 max:47Wind Direction time: 17.0006.00(+1 day) mode: ESEAround 40. Around 40. Around 40. East wind.341fiKONSTAS & L APATAexample 3, translation scores poorly Fluency Semantic Correctness. textmany repetitions clear correspondence translation table. around40 probably refers temperature, clear context text. eastwind refers wind direction, missing verb preposition would relateweather outlook. Appropriate scores dimensions would 1 2.Finally, judging translation pay attention values fields tableaddition event categories. example, may event Chance Rain valueNone mode field. means likely rain, penalize mentionrain text, unless another event Chance Rain different time perioddifferent value mode field.A.3 Rating Procedurestart experiment asked enter personal details. Next,presented 15 table-translation pairs evaluate manner described above.shown one pair time. finish rating, click button bottom rightadvance next response.Things remember:unsure rate translation, click top right window Helplink. may also leave open course experiment reference.Higher numbers represent positive opinion translation lower numbers negativeone.spend long analyzing translations; able rateread first time.right wrong answer, use judgment rating translation.A.4 Personal Detailspart experiment ask couple personal details. informationtreated confidentially made available third party. addition, noneresponses associated name way. ask supply followinginformation.name email address.age sex.specify, Language Region, place (city, region/state/province, country)learnt first language.enter code provided end experiment Mechanical Turk HIT.342fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATIONReferencesAmazon Mechanical Turk (2012). Retrieved https://www.mturk.com..Angeli, G., Liang, P., & Klein, D. (2010). simple domain-independent probabilistic approachgeneration. Proceedings 2010 Conference Empirical Methods NaturalLanguage Processing, pp. 502512, Cambridge, MA.Banko, M., Mittal, V. O., & Witbrock, M. J. (2000). Headline generation based statistical translation. Proceedings Association Computational Linguistics, pp. 318325, Hong Kong.Barzilay, R., & Lapata, M. (2005). Collective content selection concept-to-text generation.Proceedings Human Language Technology Empirical Methods Natural LanguageProcessing, pp. 331338, Vancouver, British Columbia.Belz, A. (2008). Automatic generation weather forecast texts using comprehensive probabilisticgeneration-space models. Natural Language Engineering, 14(4), 431455.Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-best parsing maxent discriminativereranking. Proceedings 43rd Annual Meeting Association ComputationalLinguistics, pp. 173180, Ann Arbor, Michigan.Chen, D. L., & Mooney, R. J. (2008). Learning sportscast: test grounded language acquisition. Proceedings International Conference Machine Learning, pp. 128135,Helsinki, Finland.Chiang, D. (2007). Hierarchical phrase-based translation. Computational Linguistics, 33(2), 201228.Collins, M. (2002). Discriminative training methods hidden Markov models: Theory experiments perceptron algorithms. Proceedings 2002 Conference EmpiricalMethods Natural Language Processing, pp. 18, Philadelphia, Pennsylvania.Dahl, D. A., Bates, M., Brown, M., Fisher, W., Hunicke-Smith, K., Pallett, D., Pao, C., Rudnicky,A., & Shriberg, E. (1994). Expanding scope ATIS task: ATIS-3 corpus.Proceedings Workshop Human Language Technology, pp. 4348, Plainsboro, NewJersey.Dale, R., Geldof, S., & Prost, J.-P. (2003). Coral: Using natural language generation navigationalassistance. Proceedings 26th Australasian Computer Science Conference, pp. 3544,Adelaide, Australia.de Gispert, A., Iglesias, G., Blackwood, G., Banga, E. R., & Byrne, W. (2010). Hierarchical phrasebased translation weighted finite-state transducers shallow-n grammars. Computational Linguistics, 36(3), 505533.Duboue, P. A., & McKeown, K. R. (2002). Content planner construction via evolutionary algorithmscorpus-based fitness function. Proceedings International Natural Language Generation, pp. 8996, Ramapo Mountains, NY.Gallo, G., Longo, G., Pallottino, S., & Nguyen, S. (1993). Directed hypergraphs applications.Discrete Applied Mathematics, 42, 177201.Goldberg, E., Driedger, N., & Kittredge, R. (1994). Using natural-language processing produceweather forecasts. IEEE Expert, 9(2), 4553.343fiKONSTAS & L APATAGood, I. J. (1953). population frequencies species estimation population parameters. Biometrika, 40(3/4), pp. 237264.Goodman, J. (1999). Semiring parsing. Computational Linguistics, 25(4), 573605.Green, N. (2006). Generation biomedical arguments lay readers. Proceedings 5thInternational Natural Language Generation Conference, pp. 114121, Sydney, Australia.Huang, L. (2008). Forest reranking: Discriminative parsing non-local features. ProceedingsACL-08: HLT, pp. 586594, Columbus, Ohio.Huang, L., & Chiang, D. (2005). Better k-best parsing. Proceedings 9th InternationalWorkshop Parsing Technology, pp. 5364, Vancouver, British Columbia.Huang, L., & Chiang, D. (2007). Forest rescoring: Faster decoding integrated language models.Proceedings 45th Annual Meeting Association Computational Linguistics,pp. 144151, Prague, Czech Republic.Iglesias, G., Allauzen, C., Byrne, W., de Gispert, A., & Riley, M. (2011). Hierarchical phrase-basedtranslation representations. Proceedings 2011 Conference Empirical MethodsNatural Language Processing, pp. 13731383, Edinburgh, Scotland, UK. AssociationComputational Linguistics.Kasami, T. (1965). efficient recognition syntax analysis algorithm context-free languages. Tech. rep. AFCRL-65-758, Air Force Cambridge Research Lab, Bedford, Massachusetts.Kim, J., & Mooney, R. (2010). Generative alignment semantic parsing learning ambiguous supervision. Proceedings 23rd Conference Computational Linguistics,pp. 543551, Beijing, China.Klein, D., & Manning, C. (2004). Corpus-based induction syntactic structure: Models dependency constituency. Proceedings 42nd Meeting Association Computational Linguistics, pp. 478485, Barcelona, Spain.Klein, D., & Manning, C. D. (2001). Parsing hypergraphs. Proceedings 7th International Workshop Parsing Technologies, pp. 123134, Beijing, China.Konstas, I. (2013). ATIS dataset retrieved http://homepages.inf.ed.ac.uk/ikonstas/index.php?page=resources..Konstas, I., & Lapata, M. (2012). Concept-to-text generation via discriminative reranking.Proceedings 50th Annual Meeting Association Computational Linguistics:Human Language Technologies, pp. 369378, Jeju, South Korea.Li, Z., & Eisner, J. (2009). First- second-order expectation semirings applicationsminimum-risk training translation forests. Proceedings 2009 ConferenceEmpirical Methods Natural Language Processing, pp. 4051, Suntec, Singapore.Liang, P., Bouchard-Cote, A., Klein, D., & Taskar, B. (2006). end-to-end discriminative approach machine translation. Proceedings 21st International Conference Computational Linguistics 44th Annual Meeting Association ComputationalLinguistics, pp. 761768, Sydney, Australia.344fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATIONLiang, P., Jordan, M., & Klein, D. (2009). Learning semantic correspondences less supervision.roceedings Joint Conference 47th Annual Meeting ACL 4thInternational Joint Conference Natural Language Processing AFNLP, pp. 9199,Suntec, Singapore.Lu, W., & Ng, H. T. (2011). probabilistic forest-to-string model language generationtyped lambda calculus expressions. Proceedings 2011 Conference EmpiricalMethods Natural Language Processing, pp. 16111622, Edinburgh, Scotland, UK.Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building large annotated corpusEnglish: Penn treebank. Comput. Linguist., 19(2), 313330.Nederhof, M.-J., & Satta, G. (2004). language intersection problem non-recursive contextfree grammars. Information Computation, 192(2), 172 184.Och, F. J. (2003). Minimum error rate training statistical machine translation. ProceedingsAnnual Meeting Association Computational Linguistics, pp. 160167, Sapporo,Japan.Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: method automatic evaluation machine translation. Proceedings 40th Annual Meeting AssociationComputational Linguistics, pp. 311318, Philadelphia, Pennsylvania.Ratnaparkhi, A. (2002). Trainable approaches surface natural language generationapplication conversational dialog systems. Computer Speech & Language, 16(3-4), 435455.Reiter, E., & Dale, R. (2000). Building natural language generation systems. Cambridge UniversityPress, New York, NY.Reiter, E., Sripada, S., Hunter, J., & Davy, I. (2005a). Choosing words computer-generatedweather forecasts. Artificial Intelligence, 167, 137169.Reiter, E., Sripada, S., Hunter, J., Yu, J., & Davy, I. (2005b). Choosing words computer-generatedweather forecasts. Artificial Intelligence, 167, 137169.Shieber, S. M., Schabes, Y., & Pereira, F. C. N. (1995). Principles implementation deductiveparsing. Logic Programming, 24, 336.Sripada, S. G., Reiter, E., Hunter, J., & Yu, J. (2003). Generating English summaries time seriesdata using gricean maxims. Proceedings Ninth ACM SIGKDD InternationalConference Knowledge Discovery Data Mining, pp. 187196. ACM Press.Stolcke, A. (2002). SRILM extensible language modeling toolkit. Hansen, J. H. L., &Pellom, B. L. (Eds.), Proceedings 7th International Conference Spoken LanguageProcessing, pp. 901904, Denver, Colorado. ISCA.Toutanova, K., Klein, D., Manning, C. D., & Singer, Y. (2003). Feature-rich part-of-speech taggingcyclic dependency network. Proceedings 2003 Conference NorthAmerican Chapter Association Computational Linguistics Human LanguageTechnology - Volume 1, pp. 173180, Edmonton, Canada.Turner, R., Sripada, Y., & Reiter, E. (2009). Generating approximate geographic descriptions.Proceedings 12th European Workshop Natural Language Generation, pp. 4249,Athens, Greece.345fiKONSTAS & L APATAWong, Y. W., & Mooney, R. (2007). Generation inverting semantic parser uses statisticalmachine translation. Proceedings Human Language Technology ConferenceNorth American Chapter Association Computational Linguistics, pp. 172179, Rochester, NY.Yandell, B. S. (1997). Practical Data Analysis Designed Experiments. Chapman & Hall/CRC.Younger, D. H. (1967). Recognition parsing context-free languages time n3 . InformationControl, 10(2), 189208.Zettlemoyer, L., & Collins, M. (2007). Online learning relaxed CCG grammars parsinglogical form. Proceedings 2007 Joint Conference Empirical Methods NaturalLanguage Processing Computational Natural Language Learning, pp. 678687, Prague,Czech Republic.346fiJournal Artificial Intelligence Research 48 (2013) 115174Submitted 11/12; published 10/13Taming Infinite Chase: Query AnsweringExpressive Relational ConstraintsAndrea Calandrea@dcs.bbk.ac.ukDepartment Computer Science Information SystemsUniversity London, Birkbeck College, UKGeorg Gottlobgeorg.gottlob@cs.ox.ac.ukDepartment Computer ScienceUniversity Oxford, UKMichael Kiferkifer@cs.stonybrook.eduDepartment Computer ScienceStony Brook University, USAAbstractchase algorithm fundamental tool query evaluation testing querycontainment tuple-generating dependencies (TGDs) equality-generating dependencies (EGDs). far, research topic focused caseschase procedure terminates. paper introduces expressive classes TGDs defined viasyntactic restrictions: guarded TGDs (GTGDs) weakly guarded sets TGDs (WGTGDs). classes, chase procedure guaranteed terminate thus mayinfinite outcome. Nevertheless, prove problems conjunctive-queryanswering query containment TGDs decidable. provide decisionprocedures tight complexity bounds problems. show EGDsincorporated results providing conditions EGDsharmfully interact TGDs affect decidability complexity queryanswering. show applications aforesaid classes constraints problemanswering conjunctive queries F-Logic Lite, object-oriented ontology language,tractable Description Logics.1. Introductionpaper studies simple yet fundamental rule-based language ontological reasoningquery answering: language tuple-generating dependencies (TGDs). formalism captures wide variety logics far considered unrelated other:OWL-based languages EL (Baader, Brandt, & Lutz, 2005) DL-Lite (Calvanese,De Giacomo, Lembo, Lenzerini, & Rosati, 2007; Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009) one hand object-based languages like F-Logic Lite (Cal &Kifer, 2006) other. present paper significant extension earlier work(Cal, Gottlob, & Kifer, 2008), since applied contexts gave riseDatalog family (Cal, Gottlob, & Pieris, 2011) ontology languages. presentpaper focuses fundamental complexity results underlying one key fragmentsfamily. Subsequent work focused study various special casesformalism (Cal, Gottlob, & Lukasiewicz, 2012a), complexity, extensions basedparadigms (Cal, Gottlob, & Pieris, 2012b).c2013AI Access Foundation. rights reserved.fiCal, Gottlob & Kiferwork also closely related work query answering query containment (Chandra & Merlin, 1977), central problems database theory knowledge representation and, cases, reducible other. especiallyinteresting presence integrity constraintsor dependencies, database parlance.databases, query containment used query optimization schema integration (Aho, Sagiv, & Ullman, 1979; Johnson & Klug, 1984; Millstein, Levy, & Friedman,2000), knowledge representation often used object classification, schemaintegration, service discovery, (Calvanese, De Giacomo, & Lenzerini, 2002; Li &Horrocks, 2003).practically relevant instance containment problem first studied Johnson Klug (1984) functional inclusion dependencies later Calvanese,De Giacomo, Lenzerini (1998). Several additional decidability results obtainedfocusing concrete applications. instance, work Cal Martinenghi (2010)considers constraints arising Entity-Relationship diagrams, CalKifer (2006) considers constraints derived relevant subset F-logic (Kifer, Lausen,& Wu, 1995), called F-Logic Lite.literature studies variants subclasses tuple-generating dependencies (TGDs)purpose reasoning query answering. TGD Horn-like rule existentially-quantified variables head. early works subject dubbed resultinglanguage Datalog value invention (Mailharrow, 1998; Cabibbo, 1998). formally,TGD XY(X, Y) Z(X, Z) first-order formula, (X, Y) (X, Z)conjunctions atoms, called body head TGD, respectively. TGD satisfied relational instance B whenever body TGD satisfied BB also satisfies head TGD. possible enforce TGD satisfiedadding new facts B head, thus TGD itself, become satisfied.new facts contain labeled null values (short: nulls) positions correspondingvariables Z. nulls similar Skolem constants. chase databasepresence set TGDs process iterative enforcement dependencies, fixpoint reached. result process, also call chase,infinite and, case, procedure cannot used without modificationsdecision algorithms. Nevertheless, result chase serves fundamental theoreticaltool answering queries presence TGDs (Cal, Lembo, & Rosati, 2003a; Fagin,Kolaitis, Miller, & Popa, 2005) representative models .present paper, focus specific logical theory. Instead, tacklecommon issue possibly non-terminating chase underlying several earlierstudies, including works Johnson Klug (1984), Cal Martinenghi (2010),Cal Kifer (2006). works study constraints language TGDsequality-generating dependencies (EGDs) using chase technique, faceproblem chase procedure might generate infinite result. dealproblem much general way carving large class constraintsinfinite chase tamed, i.e., modified would become decisionprocedure query answering.Section 3, define notions sets guarded TGDs (GTGDs) weaklyguarded sets TGDs (WGTGDs). TGD guarded body contains atom called116fiTaming Infinite Chaseguard covers variables occurring body. WGTGDs generalize guarded TGDsrequiring guards cover variables occurring so-called affected positions(predicate positions may contain labeled nulls generated chase). Noteinclusion dependencies (or IDs) viewed trivially guarded TGDs. importance guards lies Theorem 3.5, shows fixed set u GTGDs plussingle non-guarded TGD, query evaluation u undecidable. However,show WGTGDs (possibly infinite) result chase finite treewidth(Theorem 3.14). use result together well-known results generalized tree-model property (Goncalves & Gradel, 2000; Gradel, 1999) show evaluatingBoolean conjunctive queries decidable WGTGDs (and thus also GTGDs). Unfortunately, result directly provide useful complexity bounds.Section 4, show lower complexity bounds conjunctive query answeringweakly guarded sets TGDs. prove, Turing machine simulations, query evaluation weakly guarded sets TGDs exptime-hard case fixed set TGDs,2exptime-hard case TGDs part input.Section 5, address upper complexity bounds query answering weaklyguarded sets TGDs. Let us first remark showing |= Q equivalentshowing theory = {Q} unsatisfiable. Unfortunately, generalguarded Q WGTGDs generally non-guarded first-ordersentences (while GTGDs are). Therefore, cannot (as one might think first glance)directly use known results guarded logics (Goncalves & Gradel, 2000; Gradel, 1999)derive complexity results query evaluation. thus develop completely new algorithmsprove problem question exptime-complete case boundedpredicate arities and, even case TGDs fixed, 2exptime-complete general.Section 6, derive complexity results reasoning GTGDs. generalcase, complexity WGTGDs but, interestingly, reasoning fixed setdependencies (which usual setting data exchange description logics),get much better results: evaluating Boolean queries np-complete ptime casequery atomic. Recall Boolean query evaluation np-hard even casesimple database without integrity constraints (Chandra & Merlin, 1977). Therefore,np upper bound general Boolean queries optimal, i.e., class TGDsquery evaluation (or query containment) efficient.Section 7, describe semantic condition weakly guarded sets TGDs.prove whenever set WGTGDs fulfills condition, answering Boolean queriesnp, answering atomic queries, well queries bounded treewidth, ptime.Section 8 extends results case TGDs multiple-atom heads.extension trivial cases except case bounded predicate arity.Section 9 deals equality generating dependencies (EGDs), generalization functional dependencies. Unfortunately, shown works Chandra Vardi (1985),Mitchell (1983), Johnson Klug (1984), Koch (2002), Cal et al. (2003a), query answering many problems become undecidable case admit TGDsEGDs. remains undecidable even mix simplest class guarded TGDs, namely,inclusion dependencies, simplest type EGDs, namely functional dependencies,even key dependencies (Chandra & Vardi, 1985; Mitchell, 1983; Johnson & Klug, 1984;Cal et al., 2003a). Section 9, present sufficient semantic condition decidabil117fiCal, Gottlob & KiferBCQ typeGTGDs WGTGDsgeneral2exptime 2exptimeatomic fixed 2exptime 2exptimeQuery answering variable TGDs.BCQ typeGTGDsgeneralnpatomic fixedptimeQuery answering fixedWGTGDsexptimeexptimeTGDs.BCQ typeGTGDs WGTGDsgeneralexptimeexptimeatomic fixed exptimeexptimeQuery answering fixed predicate arity.Figure 1: Summary results. complexity bounds tight.ity query-answering sets TGDs general EGDs. call EGDs innocuouswhen, roughly speaking, application (i.e., enforcement) introduce new atoms,eliminates atoms. show innocuous EGDs essentially ignoredconjunctive query evaluation query containment testing.TGD-based ontology languages paper part larger family ontologylanguages called Datalog (Cal et al., 2011). results subsume main decidabilitynp-complexity result Johnson Klug (1984), decidability complexityresults F-Logic Lite Cal Kifer (2006), DL-Lite special cases.fact, Section 10 shows results even general that.complexity results paper, together immediate consequences, summarized Figure 1, complexity bounds tight. Noticecomplexity case fixed queries fixed TGDs so-called data complexity,i.e., complexity respect data only, particular interest databaseapplications. complexity variable Boolean conjunctive queries (BCQs) variableTGDs called combined complexity. easy see (but prove formallyclasses) complexity results atomic fixed queries extend queriesbounded width, width mean treewidth even hypertree width (Gottlob,Leone, & Scarcello, 2002)see also works Adler, Gottlob, Grohe (2007),Gottlob, Leone, Scarcello (2001).2. Preliminariessection define basic notions use throughout paper.2.1 Relations, Instances Queriesrelational schema R set relational predicates, aritya non-negativeinteger represents number arguments predicate takes. write r/n say118fiTaming Infinite Chaserelational predicate r arity n. Given n-ary predicate r R, position r[k],1 6 k 6 n, refers k-th argument r. assume underlying relationalschema R postulate queries constraints use predicates R.schema R sometimes omitted clear context immaterial.introduce following pairwise disjoint sets symbols: (i) (possibly infinite) setdata constants, constitute normal domain databases schemaR; (ii) set N labeled nulls, i.e., fresh Skolem constants; (iii) infinite set Vvariables, used queries constraints. Different constants represent differentvalues (unique name assumption), different nulls may represent value.also assume lexicographic order N , every labeled null N followingconstant symbols . Sets variables (or sequences, order relevant)denoted X, i.e., X = X1 , . . . , Xk , k. notation X shorthandX1 . . . Xk , similarly X.instance relational predicate r/n (possibly infinite) set atomic formulas (atoms) form r(c1 , . . . , cn ), {c1 , . . . , cn } N . atoms alsocalled facts. fact r(c1 , . . . , cn ) true, say tuple hc1 , . . . , cn belongsinstance r (or r, confusion arise). instancerelational schema R = {r1 , . . . , rm } set comprised instances r1 , . . . , rm .instances treated first-order formulas, labeled null viewed existential variable name, relational instances nulls correspondconjunction atoms preceded existential quantification nulls. instance, {r(a, z1 , z2 , z1 ), s(b, z2 , z3 )}, {z1 , z2 , z3 } N {a, b} , expressedz1 z2 z3 r(a, z1 , z2 , z1 ) s(b, z2 , z3 ). following, omit quantifiers.fact r(c1 , . . . , cn ) said ground ci {1, . . . , n}. case,also tuple hc1 , . . . , cn said ground. relation schema instance whosefacts ground said ground, ground instance R also called database.sequence atoms ha1 , . . . , ak conjunction atoms a1 . . . ak , useatoms(A) denote set atoms A: atoms(A) = {a1 , . . . , ak }. Given (groundnon-ground) atom a, domain a, denoted dom(a), set values (variables,constants orSlabeled nulls) appear arguments a. set atoms, definedom(A) = aA dom(a). sequence conjunction atoms definedom(A) = dom(atoms(A)). atom, set, sequence, conjunction atoms,write vars(A) denote set variables A.Given instance B relational schema R, Herbrand Base B, denoted HB (B),set atoms formed using predicate symbols R argumentsdom(B). Notice extension classical notion Herbrand Base,includes ground atoms only.n-ary conjunctive query (CQ) R formula form q(X1 , . . . , Xn ) (X),q predicate appearing R, variables X1 , . . . , Xn appear X,(X), called body query, conjunction atoms constructed predicatesR. arity query arity head predicate q. q arity 0,conjunctive query called Boolean (BCQ). BCQs, convenient drop headpredicate simply view query set atoms (X). stated otherwise,assume queries contain constants, since constants eliminated queriessimple polynomial time transformation. also sometimes refer conjunctive119fiCal, Gottlob & Kiferqueries queries. size conjunctive query Q denoted |Q|; representsnumber atoms Q.2.2 Homomorphismsmapping set symbols S1 another set symbols S2 seen function: S1 S2 defined follows: (i) (the empty mapping) mapping; (ii)mapping, {X }, X S1 S2 mapping alreadycontain X 6= . X mapping , write (X) = .notion mapping naturally extended atoms follows. = r(c1 , . . . , cn )atom mapping, define (a) = r((c1 ), . . . , (cn )). set atoms,= {a1 , . . . , }, (A) = {(a1 ), . . . , (am )}. set atoms (A) also called imagerespect . conjunction atoms C = a1 . . . , (C) shorthand(atoms(C)), is, (C) = {(a1 ), . . . , (am )}.homomorphism set atoms A1 another set atoms A2 , dom(A1A2 ) N V mapping dom(A1 ) dom(A2 ) followingconditions hold: (1) c (c) = c; (2) (A1 ) A2 , i.e., atom, a, A1 ,atom (a) A2 . case, say A1 maps A2 via .answer conjunctive query Q form q(X1 , . . . , Xn ) (X) instanceB R, denoted Q(B), defined follows: tuple ( N )n , Q(B) iffhomomorphism maps (X) atoms B, hX1 , . . . , Xn t. case,abuse notation, also write q(t) Q(B). Boolean conjunctive query Qpositive answer B iff hi (the tuple elements) Q(B); otherwise, saidnegative answer.2.3 Relational Dependenciesdefine main type dependencies used paper, tuple-generatingdependencies, TGDs.Definition 2.1. Given relational schema R, TGD R first-order formulaform XY(X, Y) Z(X, Z), (X, Y) (X, Z) conjunctionsatoms R, called body head TGD, respectively; denoted body()head (). dependency satisfied instance B R if, wheneverhomomorphism h maps atoms (X, Y) atoms B, exists extensionh2 h (i.e., h2 h) maps atoms (X, Z) atoms B.simplify notation, usually omit universal quantifiers TGDs.also sometimes call TGDs rules implication symbol them. Noticethat, general, constants appear body, also headsTGDs. simplicity without loss generality, assume constantsappear head TGDs also appear body TGD.symbol |= used henceforth usual logical entailment, setsatoms TGDs viewed first-order theories. theories, restrictfinite models: consider arbitrary models could finite infinite.aspect discussed Section 11.120fiTaming Infinite Chase2.4 Query Answering Containment TGDsdefine notion query answering TGDs. similar notion used dataexchange (Fagin et al., 2005; Gottlob & Nash, 2006) query answering incompletedata (Cal et al., 2003a). Given database satisfy constraints ,first define set completions (or repairssee Arenas, Bertossi, & Chomicki, 1999)database, call solutions.Definition 2.2. Consider relational schema R, set TGDs , databaseR. set instances {B | B |= } called set solutions given ,denoted sol(D, ).following definition problem, denote CQAns, answeringconjunctive queries TGDs. answers defined also referred certainanswers (see Fagin et al., 2005).Definition 2.3. Consider relational schema R, set TGDs , database R,conjunctive query Q R. answer conjunctive query Q given ,denoted ans(Q, D, ), set tuples every B sol(D, ), Q(B)holds.Notice components definition necessarily constants. ans(Q, D, ), also write {Q} |= q(t), Q representedrule body(Q) q(X).Containment queries relational databases long considered fundamentalproblem query optimization, especially query containment constraintsTGDs. formally define problem, call CQCont.Definition 2.4. Consider relational schema R, set TGDs R, two conjunctivequeries Q1 , Q2 expressed R. say Q1 contained Q2 , denotedQ1 Q2 , every instance B R B |= Q1 (B) subsetQ2 (B).2.5 Chasechase introduced procedure testing implication dependencies (Maier,Mendelzon, & Sagiv, 1979), later also employed checking query containment (Johnson & Klug, 1984) query answering incomplete data relational dependencies (Cal et al., 2003a). Informally, chase procedure process repairing databaserespect set dependencies, result chase satisfies dependencies. chase may refer either chase procedure output. chaseworks database so-called TGD chase rule, defines resultapplications TGD comes two flavors: oblivious restricted.Definition 2.5. [Oblivious Applicability] Consider instance B schema R,TGD = (X, Y) Z (X, Z) R. say obliviously applicable Bexists homomorphism h h((X, Y)) B.121fiCal, Gottlob & KiferDefinition 2.6. [Restricted Applicability] Consider instance B schema R,TGD = (X, Y) Z (X, Z) R. say restrictively applicable Bexists homomorphism h h((X, Y)) B, extension hh|X h ((X, Z)) B.1oblivious form applicability called way forgets checkwhether TGD already satisfied. contrast, TGD restrictively applicablealready satisfied.Definition 2.7. [TGD Chase Rule] Let TGD form (X, Y) Z (X, Z)suppose obliviously (resp., restrictively) applicable instance B viahomomorphism h. Let h extension h|X that, Z Z, h (Z) freshlabeled null N occurring B, following lexicographically B.result oblivious (resp., restricted ) application B h B = Bh ((X, Z)).,h,hwrite B B (resp., B R B ) denote B obtained Bsingle oblivious (resp., restricted) chase step.TGD chase rule, defined above, basic building block construct chasedatabase set TGDs. Depending notion applicability useobliviousrestrictedwe get oblivious restricted chase. formal definitionchase given below.Definition 2.8. [Oblivious Restricted Chase] Let database setTGDs. oblivious (resp., restricted) chase sequence respect sequence,hiinstances B0 , B1 , B2 , . . . B0 = and, > 0, Bi Bi+1 (resp.,,hiBi R Bi+1 ) . also assume chase sequence pairhi , hi never applied once. oblivious (resp., restricted) chaserespect , denoted Ochase(D, ) (resp., Rchase(D, )), defined follows:finite oblivious (resp., restricted) chase respect finite oblivious,hi,hi(resp., restricted) chase sequence B0 , . . . , Bm Bi Bi+1 (resp., Bi RBi+1 ) 0 6 < m, application yieldsinstance B 6= Bm . define Ochase(D, ) = Bm (resp., Rchase(D, ) = Bm ).,hiinfinite oblivious (resp., restricted) chase sequence B0 , B1 , . . ., Bi Bi+1,hi(resp., Bi R Bi+1 ) > 0, fair whenever TGD = (X, Y)Z (X, Z) obliviously (resp., restrictedly) applicable Bi homomorphism h, exists extension h h|X k > 0 h (head ())Bk . infinite oblivious chase respect fair infinite chase sequence,hi,hiB0 , B1 , . . . Bi Bi+1 (resp., Bi R Bi+1 ) > 0. case,define Ochase(D, ) = limi Bi (resp., Rchase(D, ) = limi Bi ).easy see chase infinite, sequence applications chaserule infinite. remark chase defined databases ground tuples. However, definition straightforwardly applies also arbitrary instances, possibly containing1. h|X denotes restriction h set variables X.122fiTaming Infinite Chaselabeled nulls. assume fair deterministic strategy constructing chase sequences.use Ochase [i] (D, ) (resp., Rchase [i] (D, )) denote result i-th step oblivious (resp., restricted) chase respect . Notice Ochase [i] (D, ) (resp.,Rchase [i] (D, )) called oblivious (resp., restricted) chase respectderivation level i, work Cal et al. (2012a).Example 2.9. example, show oblivious chase procedure. Consider following set = {1 , 2 , 3 , 4 } TGDs.1 :2 :3 :4 :r3 (X, )r1 (X, )r1 (X, ), r2 (Y )r1 (X, )r2 (X)Z r3 (Y, Z)Z r1 (Y, Z)r2 (Y )let = {r1 (a, b)}. chase procedure adds following sequence atoms:r3 (b, z1 ) via 2 , r2 (b) via 4 , r1 (b, z2 ) via 3 , r3 (z2 , z3 ) via 2 , r2 (z2 ) via 4 , on.2.6 Query Answering Chaseproblems query containment answering TGDs closely relatednotion chase, explained below.Theorem 2.10 (see Nash, Deutsch, & Remmel, 2006). Consider relational schema R,database R, set TGDs R, n-ary conjunctive query Q head-predicateq, n-ary ground tuple (with values ). ans(Q, D, ) iff existshomomorphism h h(body(Q)) Rchase(D, ) h(head (Q)) = q(t).Notice fact h(body(Q)) Rchase(D, ) h(head (Q)) = q(t) equivalent saying q(t) Q(Rchase(D, )), Rchase(D, ) {Q} |= q(t).result Theorem 2.10 important, holds (possibly infinite) restrictedchase universal solution (Fagin et al., 2005), i.e., representative instancessol(D, ). formally, universal solution (possibly infinite) instanceU that, every instance B sol(D, ), exists homomorphism maps UB. work Nash et al. (2006) shown chase constructed respectTGDs universal solution.freezing homomorphism query homomorphism maps every distinctvariable query distinct labeled null N . following well known resultslight extension result Chandra Merlin (1977).Theorem 2.11. Consider relational schema R, set TGDs R, two conjunctive queries Q1 , Q2 R. Q1 Q2 iff (head (Q1 )) Q2 (Rchase((body(Q1 )), )freezing homomorphism Q1 .results Johnson Klug (1984) Nash et al. (2006),easily obtain following result, considered folklore.Corollary 2.12. problems CQAns CQCont mutually logspace-reducible.123fiCal, Gottlob & Kifer2.7 Oblivious vs. Restricted Chaseobserved Johnson Klug (1984) case functional inclusion dependencies, things complicated restricted chase used instead obliviousone, since applicability TGD depends presence atoms previously addeddatabase chase. technically easier use oblivious chaseused lieu restricted chase because, shall prove now, result similarTheorem 2.10 holds oblivious chase, i.e., also universal. result, bestknowledge, never explicitly stated before. sake completeness,present full proof here.Theorem 2.13. Consider set TGDs relational schema R, letdatabase R. exists homomorphism (Ochase(D, ))Rchase(D, ).Proof. proof induction number applications TGD chase ruleconstruction oblivious chase Ochase(D, ). want prove that,> 0, homomorphism Ochase [m] (D, ) Rchase(D, ).Base case. base case, = 0, TGD rule yet applied,Ochase [0] (D, ) = Rchase(D, ) required homomorphism simply identity homomorphism 0 .Inductive case. Assume applied TGD chase rule times obtainedOchase [m] (D, ). induction hypothesis, exists homomorphism mapsOchase [m] (D, ) Rchase(D, ). Consider (m + 1)-th application TGD chaserule, TGD form (X, Y) Z(X, Z). definition applicability TGDs,homomorphism maps (X, Y) atoms Ochase(D, )suitably extended another homomorphism, , maps variablesZ fresh null N already present Ochase [m] (D, ). result application TGD, atoms ((X, Z)) added Ochase [m] (D, ), thus obtainingOchase [m+1] (D, ). Consider homomorphism R = , maps (X, Y)atoms Rchase(D, ). Since Rchase(D, ) satisfies dependencies (andOchase(D, )), extension R R maps (X, Z) tuples Rchase(D, ).Denoting Z = Z1 , . . . , Zk , define m+1 = {O (Zi ) R (Zi )}16i6k . complete proof, need show m+1 indeed homomorphism. addition (Zi ) R (Zi ), 1 6 6 k, compatible none(Zi ) appears . Therefore m+1 well-defined mapping. Now, consider atomr(X, Z) (X, Z). atom (r(X, Z)) added Ochase(D, ) (m+1)-thstep m+1 (r(X, Y)) = m+1 (r(O (X), (Z))) = r(m+1 (O (X), m+1 (O (Z)). Notice m+1 (O (X)) = m+1 (O (X)) = R (X) = R (X), m+1 (O (Z)) = R (Z).Therefore, m+1 (r(X, Z)) = r(R (X), R (Z)) = R (r(X, Z)), Rchase(D, ),construction.desired homomorphism Ochase(D, ) Rchase(D, ) therefore=.i=0Corollary 2.14. Given set TGDs relational schema R databaseR, Ochase(D, ) universal solution .124fiTaming Infinite ChaseCorollary 2.15. Given Boolean query Q schema R, database R,set TGDs , Ochase(D, ) |= Q Rchase(D, ) |= Q.following, unless explicitly stated otherwise, chase mean oblivious chase,chase(D, ) stand Ochase(D, ).2.8 Decision ProblemsRecall that, Theorem 2.10, |= Q iff chase(D, ) |= Q. Based this, definetwo relevant decision problems prove logspace-equivalence.Definition 2.16. conjunctive query evaluation decision problem CQeval definedfollows. Given conjunctive query Q n-ary head predicate q, set TGDs ,database ground n-tuple t, decide whether ans(Q, D, ) or, equivalently,whether chase(D, ) {Q} |= q(t).Definition 2.17. Boolean conjunctive query evaluation problem BCQeval definedfollows. Given Boolean conjunctive query Q, set TGDs , database D, decidewhether chase(D, ) |= Q.following result implicit work Chandra Merlin (1977).Lemma 2.18. problems CQeval BCQeval logspace-equivalent.Proof. Notice BCQeval trivially made special instance CQeval, e.g.,adding propositional atom head atom. thus suffices show CQeval polynomiallyreduces BCQeval. Let hQ, D, , q(t)i instance CQeval, q/n headpredicate Q ground n-tuple. Assume head atom Q q(X1 , . . . , Xn )= hc1 , . . . , cn i. define Q Boolean conjunctive query whose bodybody(Q) q (X1 , . . . , Xn ), q fresh predicate symbol occurring D, Q,easy see q(t) Q(chase(D, )) iff chase(D {q (c1 , . . . , cn )}, ) |= Q .lemma well-known equivalence problem query containment TGDs CQeval problem (Corollary 2.12), three following problemslogspace-equivalent: (1) CQ-eval TGDs, (2) BCQeval TGDs, (3) querycontainment TGDs. Henceforth, consider one problems,BCQ-eval problem. above, complexity results carry problems.Dealing multiple head-atoms. turns dealing multiple atomsTGD heads complicates proof techniques, assume TGDs singleatom head. proving results single-headed TGDs, extendresults case multiple-atom heads Section 8.2.9 Tree Decomposition Related Notionsintroduce required notions tree decompositions. hypergraph pairH = hV, Hi, V set nodes H 2V . elements H thus subsetsV ; called hyperedges. Gaifman graph hypergraph H = hV, Hi, denoted125fiCal, Gottlob & KiferGH , undirected graph V set nodes edge (v1 , v2 )graph v1 v2 jointly occur hyperedge H.Given graph G = hV, Ei, tree decomposition G pair hT, i, = hN, Aitree, labeling function : N 2V that:(i) v V n N v (n); is, (N ) = nN (n) = V ;(ii) every edge e = (v1 , v2 ) E n N (n) {v1 , v2 };(iii) every v V , set {n N | v (n)} induces connected subtree .width tree decomposition hT, integer value max{|(n)| 1 | n N }.treewidth graph G = hV, Ei, denoted tw(G), minimum width treedecompositions G. Given hypergraph H, treewidth tw(H) defined treewidthGaifman graph: tw(H) = tw(GH ). Notice notion treewidth immediatelyextends relational structures.3. Guarded Weakly-Guarded TGDs: Decidability Issuessection introduces guarded TGDs (GTGDs) weakly guarded sets TGDs (WGTGDs), enjoy several useful properties. particular, show query answeringTGDs decidable.Definition 3.1. Given TGD form (X, Y) (X, Z), say (fully)guarded TGD (GTGD) exists atom body, called guard, containsuniversally quantified variables , i.e., variables X, occur (X, Y).define weakly guarded sets TGDs, first give notion affected positionpredicate relational schema, given set TGDs . Intuitively, positionaffected set TGDs exists database labeled null appearsatom chase(D, ) position . importance affected positionsdefinitions labeled null appear non-affected positions. define notionbelow.Definition 3.2. Given relational schema R set TGDs R, positionpredicate p R affected respect either:(base case) , existentially quantified variable appears head (),(inductive case) , variable appearing position head () alsoappears body(), affected positions.Example 3.3. Consider following set TGDs:1 : p1 (X, ), p2 (X, ) Z p2 (Y, Z)2 : p2 (X, ), p2 (W, X) p1 (Y, X)Notice p2 [2] affected since Z existentially quantified 1 . variable 1appears p2 [2] (which affected position) also p1 [2] (which affectedposition). Therefore 1 make position p2 [1] affected one. Similarly,126fiTaming Infinite Chase2 , X appears affected position p2 [2] also non-affected position p2 [1].Therefore, p1 [2] affected. hand, 2 appears p2 [2] nowhereelse. Since already established p2 [2] affected position, makes p1 [1]also affected position.Definition 3.4. Consider set TGDs schema R. TGD form(X, Y) (X, Z) said weakly guarded respect (W GT GD)atom body(), called weak guard, contains universally quantifiedvariables appear affected positions respect also appearnon-affected positions respect . set said weakly guarded setTGDs TGD weakly guarded respect .GTGD WGTGD may one guard. case, picklexicographically first guard use criterion fixing guard rule.actual choice affect proofs results.following theorem shows undecidability conjunctive query answeringTGDs. result, general form, follows undecidability results TGD implication (see Beeri & Vardi, 1981; Chandra, Lewis, & Makowsky, 1981b). showCQ answering problem remains undecidable even case fixed set singleheaded TGDs single non-guarded rule, ground atom query. prooffirst principles reduces well-known halting problem Turing machinesquery-answering TGDs. recently, Baget, Leclere, Mugnier, Salvat (2011a)showed CQ answering undecidable also case contains single TGD, which,however, contains multiple atoms head.Theorem 3.5. exists fixed atomic BCQ Q fixed set TGDs u ,TGDs u guarded except one, undecidable determine whetherdatabase D, u |= Q or, equivalently, whether chase(D, u ) |= Q.Proof. proof hinges observation that, appropriate input facts D, usingfixed set TGDs consists guarded TGDs single unguarded TGD, possibleforce infinite grid appear chase(D, u ). set guarded rules, oneeasily simulate deterministic universal Turing machine (TM) M, executesevery deterministic TM empty input tape, whose transition table specifieddatabase D. done using infinite grid, i-th horizontal linegrid represents tape content instant i. assume transitions Turingmachine encoded relation trans D, example, ground atomtrans(s1 , a1 , s2 , a2 , right) means current state s1 symbol a1 read, switchstate s2 , write a2 , move right.show infinite grid defined. Let contain (among initializationatoms specify initial configuration M) atom index (0), definesinitial point grid. Also, make use three constants right, left, stay encodingthree types moves. Consider following TGDs:index (X) next(X, )next(X, ) index (Y )trans(T), next(X1 , X2 ), next(Y1 , Y2 ) grid (T, X1 , Y1 , X2 , Y2 )127fiCal, Gottlob & Kiferstands sequence argument variables S1 , A1 , S2 , A2 , , appropriatepredicate trans. Note last three TGDs non-guarded.TGDs define infinite grid whose points co-ordinates X (horizontalvertical, respectively) point horizontal vertical successorsalso encoded. addition, point appears together possible transitionrule. hard see simulate progress Turing machine usingsuitable initialization atoms guarded TGDs. end, need additionalpredicates cursor (Y, X), meaning cursor position X time , state(Y, S),expressing state time , content(X, Y, A), expressing time, content position X tape A. following rule encodes behaviortransition rules move cursor right:grid (S1 , A1 , S2 , A2 , right, X1 , Y1 , X2 , Y2 ),cursor (Y1 , X1 ), state(Y1 , S1 ), content(X1 , Y1 , A1 )cursor (Y2 , X2 ), content(X1 , Y2 , A2 ), state(Y2 , S2 ), mark (Y1 , X1 )rule also obvious sibling rules left stay moves. sakebrevity only, rule contains multiple atoms head. problem,rules existentially quantified variables head. Therefore, TGDmultiple head-atoms replaced equivalent set TGDs single-atom headsidentical bodies.Notice mark predicate head marks tape cell modifiedinstant Y1 . need additional inertia rules, ensure positionstape modified Y1 following time instant Y2 . end,use two different markings: keep f tape positions follow one markedmark , keep p preceding tape positions. way, able, makinguse guarded rules only, ensure that, every instant Y1 , every tape cell X,keep p (Y1 , X) keep f (Y1 , X) true, keeps symbol instant Y2 followingY1 . rules propagate aforementioned markings forward backwards,respectively, starting marked tape positions.mark (Y1 , X1 ), grid (T, X1 , Y1 , X2 , Y2 ) keep f (Y1 , X2 )keep f (Y1 , X1 ), grid (T, X1 , Y1 , X2 , Y2 ) keep f (Y1 , X2 )mark (Y1 , X2 ), grid (T, X1 , Y1 , X2 , Y2 ) keep p (Y1 , X1 )keep p (Y1 , X2 ), grid (T, X1 , Y1 , X2 , Y2 ) keep p (Y1 , X1 )also inertia rules {a1 , . . . , , }, {a1 , . . . , , } tape alphabet:keep f (Y1 , X1 ), grid (T, X1 , Y1 , X2 , Y2 ), content(X1 , Y1 , a) content(X1 , Y2 , a)keep p (Y1 , X1 ), grid (T, X1 , Y1 , X2 , Y2 ), content(X1 , Y1 , a) content(X1 , Y2 , a)Notice use constant instead variable rules orderguardedness property. therefore need two rules every tape symbol,is, 2 + 2 inertia rules altogether.Finally, assume, without loss generality, Turing machine singlehalting state s0 encoded atom halt(s0 ) D. add guarded128fiTaming Infinite Chaserule state(Y, S), halt(S) stop. clear machine halts iff chase(D, u ) |=stop, i.e., iff u |= stop. thus reduced halting problem problemanswering atomic queries database u . latter problem thereforeundecidable.Definition 3.6. [Guarded chase forest, restricted GCF] Given set WGTGDsdatabase D, guarded chase forest (GCF) , denoted gcf(D, ), constructedfollows.(a) atom (fact) D, add node labeled d.(b) every node v labeled chase(D, ) every atom b obtained(and possibly atoms) one-step application TGD ,image guard add one node v labeled b arc going vv .Assuming chase forest gcf(D, ) built inductively, following precisely strategyfixed deterministic chase procedure, set non-root nodes chase foresttotally ordered relation reflects order generation. restricted GCF, denoted rgcf(D, ), obtained gcf(D, ) eliminating subtreerooted node w whose label duplicate earlier generated node. Thus, vw nodes labeled atom, v w, w nodes subtree rootedw eliminated gcf(D, ) obtain rgcf(D, ). Note rgcf(D, )label occurs once, therefore identify nodes labels say,instance, node instead node v labeled a.Example 3.7. Consider Example 2.9 page 123. corresponding (infinite)guarded chase forest shown Figure 2. Every edge a-node b-node labeledTGD whose application causes introduction b. Notice atoms (e.g.,r2 (b) r2 (z2 )) label one node forest. nodes belonging alsorestricted GCF shaded figure.r1 (a, b)2r3 (b, z1 )43r2 (b)r1 (b, z2 )21r2 (b)34r3 (z2 , z3 )r2 (z2 )r1 (z2 , z4 )21r2 (z2 )43Figure 2: Chase forest Example 3.7.goal following material show that, weakly guarded sets TGDs,possibly infinite set atoms chase(D, ) finite treewidth (Lemma 3.13).used show decidability query-answering WGTGDs (Theorem 3.14).first step towards proving chase(D, ) finite treewidth, generalize notion129fiCal, Gottlob & Kiferacyclicity instance, point relationship notiontreewidth. show chase(D, ) enjoys (a specific version of) generalizedform acyclicity (Lemma 3.11), finite treewidth result immediately follows.Definition 3.8. Let B (possibly infinite) instance schema R let dom(B).[S]-join forest hF, B undirected labeled forest F = hV, Ei (finiteinfinite), whose labeling function : V B that:(1) epimorphism, i.e., (V ) = B;(2) F [S]-connected, i.e., c dom(B) S, set {v Vdom((v))} induces connected subtree F .|csay B [S]-acyclic B [S]-join forest.Notice dealing relational instance, definition worksrelational structure, including queries. Definition 3.8 generalizes classical notionhypergraph acyclicity (Beeri, Fagin, Maier, Mendelzon, Ullman, & Yannakakis, 1981)instance query: instance query, seen hypergraph, hypergraph-acyclic(which -acyclic according Fagin, 1983) []-acyclic .following Lemma follows definitions [S]-acyclicity.Lemma 3.9. Given instance B schema R, set dom(B), B [S]acyclic, tw(B) 6 |S| + w, w maximum predicate arity R tw(B)treewidth B.Proof. hypothesis, B [S]-acyclic therefore [S]-join forest hF, i,F = hV, Ei. tree decomposition hT, = hN, Ai, constructed follows. First,take N = V {n0 }, n0 auxiliary node. Let Vr V set nodesroots [S]-join forest F let Ar set edges n0 node Vr .define = E Ar . labeling function defined follows: (n0 ) = S,nodes v 6= n0 , (v) = dom((v)) S. show hT, tree decomposition.Recalling definition tree decompositions Section 2.9, (i) holds trivially Fjoin forest (V ) = B. (ii), notice edges Gaifman graph Batom = r(c1 , . . . , cm ) B clique among nodes c1 , . . . , cm .Since atom exists v V (v) = (v) dom((v)), (ii)holds immediately. Finally consider connectedness. Let us take value c appearingB argument. c S, set {v N | c (v)} entire N , construction,connectedness holds. c 6 S, set {v N | c (v)} induces connected subtree Ftherefore , since (v) = (v) S. Therefore, (iii) holds. Notice also widthtree decomposition |S| + w construction.Definition 3.10. Let database schema R, HB (D) Herbrand Basedefined Section 2. define:chase (D, ) = chase(D, ) HB (D),chase + (D, ) = chase(D, ) chase (D, )130fiTaming Infinite Chaseplain words, chase (D, ) finite set null-free atoms chase(D, ).contrast, chase + (D, ) may infinite; set atoms chase(D, )least one null argument. Note chase (D, ) chase + (D, ) = chase(D, )chase (D, ) chase + (D, ) = .Lemma 3.11. weakly guarded set TGDs database, chase + (D, )[dom(D)]-acyclic, therefore chase(D, ).order prove result, resort auxiliary lemma.Lemma 3.12. Let database weakly guarded set TGDs. Let nodergcf(D, ) null value N first introduced, let af descendantnode rgcf(D, ) argument. Then, appears every node (=atom)(unique) path af .Proof. Let a1 = , a2 , . . . , = af path af . definition affectedpositions, appears affected positions atoms chase. Suppose,contrary, appear intermediate atom path. Then,i, 2 6 6 n 1, appear ai , appears ai+1 . Sinceappears affected positions, order ai+1 must either appear aiinvented ai+1 added. first case ruled assumption,second impossible first introduced a1 , ai+1 contradiction.come back proof Lemma 3.11.Proof. proof constructive, exhibiting [dom(D)]-join forest F = hV, Eichase(D, ). take F rgcf(D, ) define, atom rgcf(D, ),labeling function F (d) = d. Since every atom chase(D, ) coveredcorresponding node F , remains show chase(D, ) [dom(D)]-connected.Take pair distinct atoms a1 , a2 rgcf(D, ) value c Nargument. atoms a1 a2 must common ancestor rgcf(D, ) cfirst invented: not, value c would introduced twice chase(D, ).Lemma 3.12, c appears atoms paths a1 a2 . thusfollows set {v V | c (v)} induces connected subtree F .Lemma 3.13. weakly guarded set TGDs database schema R,tw(chase(D, )) 6 |dom(D)| + w, w maximum predicate arity R.Proof. claim follows Lemmas 3.9 3.11.Theorem 3.14. Given relational schema R, weakly guarded set TGDs , Booleanconjunctive query Q, database R, problem checking whether |= Q,equivalently chase(D, ) |= Q, decidable.Proof (sketch). first remind key result Courcelle (1990), generalizes earlierresult Rabin (1969). Courcelles result states satisfiability problem decidableclasses first-order theories (more generally, theories monadic second-order logic)131fiCal, Gottlob & Kiferenjoy finite treewidth model property. class C theories finite-treewidthmodel property satisfiable theory C possible compute integer f (T )model treewidth f (T )see also works GoncalvesGradel (2000) Gradel (1999), general property, called generalizedtree-model property, discussed. apply prove theorem.Let Q universal sentence obtained negating existentially quantified conjunctive query Q. classes TGDs, |= Q iff chase(D, ) |= Q iff Qunsatisfiable. Trivially, deciding whether |= Q equivalent Turing reductions deciding whether 6|= Q. latter holds iff {Q} satisfiableor, equivalently, iff chase(D, ) model Q which, turn, holds iff chase(D, )model {Q}. Lemma 3.13, WGTGDs, chase(D, ) finite treewidth.decision problem thus amounts checking whether theory belonging class Cfirst-order theories (of form {Q}) satisfiable, guaranteedwhenever theory class satisfiable, model finite treewidth(namely, chase(D, )), C therefore enjoys finite treewidth model property.Decidability thus follows Courcelles result.Determining precise complexity query answering sets guarded weaklyguarded sets TGDs require new techniques, subject next sections.4. Complexity: Lower Boundssection prove several lower bounds complexity decision problemanswering Boolean conjunctive queries guarded weakly guarded sets TGDs.Theorem 4.1. problem BCQeval WGTGDs exptime-hard case TGDsfixed. problem 2exptime-hard predicate arity bounded.hardness results also hold fixed atomic ground queries.Proof. start exptime-hardness result fixed WGTGD sets . wellknown apspace (alternating pspace, see Chandra, Kozen, & Stockmeyer, 1981a)equals exptime. Notice apspace-hard languages accepted alternating polynomial-space machines use n worktape cells, n inputsize, input initially placed worktape. (This well-knownshown trivial padding arguments). prove claim, thus suffices simulatebehavior restricted linear space (linspace) Alternating Turing Machine (ATM)input bit string means weakly guarded set TGDs databaseD. Actually, show stronger result: fixed set WGTGDs simulateuniversal ATM turn simulates every linspace ATM uses n tapecells every input. ATM transition table ATM input stringstored database D. |= Q atomic ground query Q iff ATMaccepts given input.2Without loss generality, assume ATM exactly one acceptingstate sa . also assume never tries read beyond tape boundaries. Let2. technique proposed Cal et al. (2008). similar technique later described Hernich,Libkin, Schweikardt (2011) proof undecidability existence so-called CWA (closedworld assumption) universal solutions data exchange.132fiTaming Infinite Chasedefined= (S, , , , s0 , {sa })set states, = {0, 1, } tape alphabet, blank tape symbol,transition function, defined : (S {, r, })2 ( denotes stayhead move, r denote left right, respectively), s0 initial state,{sa } singleton-set accepting states. Since alternating Turing machine(ATM), set states partitioned two sets: (universal existentialstates, respectively). general idea encoding configurations (exceptinitial configuration ) represented fresh nulls vi , > 1, generatedchase.relational schema. describe predicates schemause reduction. Notice schema fixed depend particularATM encode. schema predicates follows.(1) Tape. ternary predicate symbol (a, c, v) denotes configuration v cellc contains symbol a, . Also, binary predicate succ(c1 , c2 ) denotesfact cell c1 follows cell c2 tape. Finally, neq(c1 , c2 ) says two cellsdistinct.(2) States. binary predicate state(s, v) says configuration v ATMstate s. use three additional unary predicates: existential , universal , accept.atom existential (s) (resp., universal (s)) denotes state existential(resp., universal), accept(c) says c accepting configuration, is,one whose state accepting state.(3) Configurations. unary predicate config(v) expresses fact value videntifies configuration. ternary predicate next(v, v1 , v2 ) used sayconfigurations v1 v2 derived v. Similarly, use follows(v, v ) sayconfiguration v derived v. Finally, unary predicate init(v) statesconfiguration v initial.(4) Head (cursor). use fact cursor (c, v) say head (cursor)ATM cell c configuration v.(5) Marking. Similarly done proof Theorem 3.5, use mark (c, v)say cell c marked configuration v. TGDs ensurenon-marked cells keep symbols transition one configuration another.(6) Transition function. represent transition function M, use single8-ary predicate transition: every transition rule (s, a) = ((s1 , a1 , m1 ), (s2 , a2 , m2 ))transition(s, a, s1 , a1 , m1 , s2 , a2 , m2 ).database D. data constants database used identify cells,configurations, states on. particular, use accepting state sainitial state s0 plus special initial configuration . database describes initialconfiguration ATM technicalities.(a) assume, without loss generality, n symbols input occupycells numbered 1 n, i.e., c1 , . . . , cn . technical reasons, order obtainsimpler TGD set below, also use dummy cell constants c0 cn+1 ,intuitively represent border cells without symbols. i-th symbol ai I,database fact symbol (a, ci , ), {1, . . . , n}.133fiCal, Gottlob & Kifer(b) atom state(s0 , ) specifies state s0 initial configuration .(c) every existential state sE universal state sU , facts existential (sE )universal (sU ). accepting state, database fact accept(sa ).(d) atom cursor (c1 , ) indicates that, initial configuration, cursor pointsfirst cell.(e) atoms succ(c1 , c2 ), . . . , succ(cn1 , cn ) encode fact cells c1 , . . . , cntape (beyond ATM operate) adjacent. technicalreasons, also use analogous facts succ(c0 , c1 ) succ(cn , cn+1 ). Also, atomsform neq(ci , cj ), i, j 1 6 6 n, 1 6 j 6 n 6= j, denotefact cells c1 , . . . , cn pairwise distinct.(f ) atom config() says valid configuration.(g) database atoms form transition(s, a, s1 , a1 , m1 , s2 , a2 , m2 ),encode transition function , described above.TGDs. describe TGDs define transitions acceptingconfigurations ATM.(a) Configuration generation. following TGDs say that, every configuration(halting non haltingwe mind configurations derivedhalting one), two configurations follow it, configurationfollows another configurations also valid configuration:config(V ), V1 V2 next(V, V1 , V2 )next(V, V1 , V2 ) config(V1 ), config(V2 )next(V, V1 , V2 ) follows(V, V1 )next(V, V1 , V2 ) follows(V, V2 )(b) Configuration transition. following TGD encodes transitionATM starts existential state, moves right first configuration leftsecond. C denotes current cell, C1 C2 new cells firstsecond configuration (on right left C, respectively),constants r, , represent right, left, stay moves,respectively.transition(S, A, S1 , A1 , r, S2 , A2 , ), next(V, V1 , V2 ),state(S, V ), cursor (C, V ), symbol (A, C, V ), succ(C1 , C), succ(C, C2 )state(S1 , V1 ), state(S2 , V2 ), symbol (A1 , C1 , V1 ), symbol (A2 , C2 , V2 ),cursor (C1 , V1 ), cursor (C2 , V2 ), mark (C, V ),nine rules like one, corresponding possible moveshead child configurations C1 C2 . moves encoded viasimilar TGDs. rules suitably mark cells written transitionmeans predicate mark . cells involved transitionmust retain symbols, specified following TGD:config(V ), follows(V, V1 ), mark (C, V ), symbol (C1 , A, V ), neq(C1 , C) symbol (C1 , A, V1 )134fiTaming Infinite Chase(c) Termination. rule state(sa , V ) accept(V ) defines configuration Vaccepting state accepting state. following TGDs state that,existential state, least one configuration derived must accepting.universal states, configurations must accepting.next(V, V1 , V2 ), state(S, V ), existential (S), accept(V1 ) accept(V )next(V, V1 , V2 ), state(S, V ), existential (S), accept(V2 ) accept(V )next(V, V1 , V2 ), state(S, V ), universal (S), accept(V1 ), accept(V2 ) accept(V )Note that, brevity, TGDs used multiple atoms head.However, heads existentially quantified variables, multi-headed TGDsreplaced sets TGDs one head-atom. Note alsodatabase constants (r, , , sa ) appearing rules eliminatedintroducing additional predicate symbols database atoms. example, addpredicate acceptstate signature fact acceptstate(sa ) database D,rule state(sa , V ) accept(V ) replaced equivalent constant-free ruleacceptstate(X), state(X, V ) accept(V ).hard show encoding described sound complete. is,accepts input chase(D, ) |= accept(). also easy verifyset TGDs used weakly guardedthis done checkingvariable appearing affected positions also appears guard atom. instance,take rule next(V, V1 , V2 ), state(S, V ), existential (S), accept(V1 ) accept(V ).immediate see state[1] existential [1] non-affected (the TGDs never inventnew states), variables appearing affected positions only, namely V, V1 , V2 ,appear guard atom next(V, V1 , V2 ). proves claim.turn case fixed unbounded predicate arities.obtaining 2exptime lower bound, sufficient adapt proofsimulate ATM 2n worktape cells, i.e., aexpspace machine whose spacerestricted 2n tape cells. Actually, accommodate two dummy cells left right2n effective tape cells, used technical reasons, feature 2n+1 tapecells instead 2n .make sure input string put cells 1, . . . , n worktape. Givenmany n worktape cells, fill cells rightinput string blank symbol .time, WGTGD set fixed, depend n. Since muchstronger result shown Section 6 (Theorem 6.2), belabor detailsfollows, explain proof fixed sets TGDs needschanged.Rather representing tape cell data constant, tape cell represented vector (b0 , b1 , b2 , . . . , bn ) Boolean values {0, 1}. databasebefore, except following changes:contains additional facts bool (0), bool (1), zero(0), one(1).fact symbol (a, ci , ) replaced fact symbol (a, b0 , b1 , b2 , . . . , bn , ),(b0 , b1 , b2 , . . . , bn ) Boolean vector length n representing integer i,0 6 6 n 6 2n+1 .135fiCal, Gottlob & Kiferfact cursor (c1 , ) replaced (n + 2)-ary fact cursor (0, 0, , 0, 1, ).succ neq facts described item (e) eliminated. (Vectorized versionspredicates defined via Datalog rulessee below).TGD set changed follows. rules, cell-variable Creplaced vector C n variables. example, atom succ(C1 , C) becomessucc(C1 , C) = succ(C10 , C11 , . . . C1n , C 0 , C 1 , . . . , C n ).add Datalog rules n-ary succ neq predicates. example, n-ary predicate succ implemented following rules:bool (X0 ), . . . , bool (Xn1 ) succ(X0 , . . . , Xn1 , 0 , X0 , . . . , Xn1 , 1),bool (X0 ), . . . , bool (Xn2 ) succ(X0 , . . . , Xn2 , 0, 1 , X0 , . . . , Xn2 , 1, 0),...bool (X0 ), . . . , bool (Xni ) succ(X0 , . . . , Xni , 0, 1 . . . 1 , X0 , . . . , Xni , 1, 0, . . . , 0),...succ(0, 1, . . . , 1 , 1, 0, . . . , 0)rules contain constants easily eliminated use zeroone predicates, extensional database (EDB) predicates. add simple Datalog rules use vectorized succ predicate define vectorized versionsless neq predicates. Using less than, add single rule that,initial configuration , puts blanks tape cells beyond last cell n input:less than(n, C) symbol (, C, ), n n-ary binary vector representing number n (i.e., input size).resulting set rules weakly guarded correctly simulates aexpspace (alternating exponential space) Turing machine whose transition table stored databaseD. reduction polynomial time. Since aexpspace=2exptime, immediately follows arity bounded problem 2exptime-hard.5. Complexity: Upper Boundssection present upper bounds query answering weakly guarded TGDs.5.1 Squid Decompositionsdefine notion squid decomposition, prove lemma called SquidLemma useful tool proving complexity results followingsub-sections.Definition 5.1. Let Q Boolean conjunctive query n body atoms schemaR. R-cover Q Boolean conjunctive query Q+ R contains bodybody atoms Q. addition, Q+ may contain n R-atoms.Example 5.2. Let R = {r/2, s/3, t/3} Q Boolean conjunctive querybody atoms {r(X, ), r(Y, Z), t(Z, X, X)}. following query Q+ R-cover Q:Q+ = {r(X, ), r(Y, Z), t(Z, X, X), t(Y, Z, Z), s(Z, U, U )}.136fiTaming Infinite ChaseLemma 5.3. Let B instance schema R Q Boolean conjunctive queryB. B |= Q iff exists R-cover Q+ Q B |= Q+ .Proof. only-if direction follows trivially fact Q R-cover itself.direction follows straightforwardly fact whenever homomorphismh : vars(Q+ ) dom(B), h(Q+ ) B, then, given Q subset Q+ ,restriction h h vars(Q) homomorphism vars(Q) dom(B) h (Q) =h(Q) B. Therefore B |= Q+ implies B |= Q.Definition 5.4. Let Q Boolean conjunctive query schema R. squid decomposition = (Q+ , h, H, ) Q consists R-cover Q+ Q, mapping h : vars(Q+ )vars(Q+ ), partition h(Q+ ) two sets H , = h(Q+ ) H,exists set variables V h(vars(Q+ )) that: (i) H = {a h(Q+ ) |vars(a) V }, (ii) [V ]-acyclic. appropriate set V given togethersquid decomposition = (Q+ , h, H, ), then, slight terminology overloading, mayspeak squid decomposition (Q+ , h, H, T, V ).Note squid decomposition = (Q+ , h, H, ) Q necessarily definequery folding (Chandra & Merlin, 1977; Qian, 1996) Q+ , h needendomorphism Q+ ; terms, require h(Q+ ) Q+ . However, htrivially homomorphism Q+ h(Q+ ).Intuitively, squid decomposition = (Q+ , h, H, T, V ) describes way queryQ may mapped homomorphically chase(D, ). First, instead mapping Qchase(D, ), equivalently map h(Q+ ) = H chase(D, ). set V specifies variables h(Q+ ) ought mapped constants, i.e., elementsdom(D). atoms set H thus mapped ground atoms, is, elements finite set chase (D, ), may highly cyclic. [V ]-acyclic atom set shallmapped possibly infinite set chase + (D, ) which, however, [dom(D)]-acyclic.acyclicities chase + (D, ) exploited designing appropriate decisionprocedures determining whether chase(D, ) |= Q. made formalsequel.One think set H squid decomposition = (Q+ , h, H, T, V ) headsquid, set join-forest tentacles attached head. becomeclear following example associated Figure 3.Example 5.5. Consider following Boolean conjunctive query:Q = {r(X, ), r(X, Z), r(Y, Z),r(Z, V1 ), r(V1 , V2 ), r(V2 , V3 ), r(V3 , V4 ), r(V4 , V5 ),r(V1 , V6 ), r(V6 , V5 ), r(V5 , V7 ), r(Z, U1 ), s(U1 , U2 , U3 ),r(U3 , U4 ), r(U3 , U5 ), r(U4 , U5 )}.Let Q+ following Boolean query: Q+ = Q {s(U3 , U4 , U5 )}. possible squiddecomposition (Q+ , h, H, T, V ) based homomorphism h, defined follows:h(V6 ) = V2 , h(V4 ) = h(V5 ) = h(V7 ) = V3 , h(X) = X variable X Q+ .result squid decomposition V = {X, Y, Z} query shown Figure 3.137fiCal, Gottlob & Kifercyclic head H (encircled oval) represented join graph,3 [V ]acyclic tentacle set depicted [V ]-join forest. Moreover, forest representingrooted bag H-atoms, entire decomposition takes shapesquid. Note eliminated additional atom s(U3 , U4 , U5 ), original setatoms {r(U3 , U4 ), r(U3 , U5 ), r(U4 , U5 )} would form non-[V ]-acyclic cycle, thereforewould part tentacles.r(X, )r(X, Z)headr(Y, Z)r(Z, V1 )r(Z, U1 )r(V1 , V2 )s(U1 , U2 , U3 )r(V2 , V3 )s(U3 , U4 , U5 )tentaclesr(V3 , V3 )r(U3 , U4 )r(U3 , U5 )r(U4 , U5 )Figure 3: Squid decomposition Example 5.5. Atoms h(Q+ ) shown.following two lemmas auxiliary technical results.Lemma 5.6. Let Q Boolean conjunctive query let U (possibly infinite) [A]acyclic instance, dom(U ). Assume U |= Q, i.e., homomorphismf : dom(Q) dom(U ) f (Q) U . Then:(1) [A]-acyclic subset W U that: (i) f (Q) W (ii) |W | < 2|Q|.(2) cover Q+ Q |Q+ | < 2|Q|, homomorphism gextends f g(Q+ ) = W .Proof.Part (1). assumption,4 U [A]-acyclic f : dom(Q) dom(U ) homomorphism f (Q) U . Since U [A]-acyclic, (possibly infinite) [A]-joinforest = hhV, Ei, i. assume, without loss generality, distinct vertices u, vdifferent labels, i.e., (u) 6= (v). assumption made removingsubforests rooted nodes labeled duplicate atoms. Let TQ finite subforest3. join graph H atoms nodes. edge two atoms exists iff atoms shareleast one variable.4. One may tempted conjecture W = f (Q), work acyclicity (and thusalso [A]-acyclicity) hereditary property: may well case U acyclic,subset f (Q) U not. However, taking W = f (Q) works case arities 2.138fiTaming Infinite Chasecontains ancestors nodes (s) f (Q). Let F = hhV , E i,forest obtained follows.V = {v V | (v) f (Q)} K, K set vertices TQleast two children.v, w V edge v w E iff w descendant v ,unique shortest path v w contain nodeV .Finally, v V , (v) = (v).Let us define W = (V ). claim forest F [A]-join forest W . SinceCondition (1) Definition 3.8 ([S]-join forest) immediately satisfied, suffices showCondition (2), is, F satisfies [A]-connectedness condition. Assume,pair distinct vertices v1 v2 F , value b dom(U ) holdsb dom( (v1 )) dom( (v2 )). order prove aforementioned [A]-connectednesscondition, need show least one path F v1 v2 (hereview F undirected graph), every node v V lyingpath b dom( (v)). construction F , v1 v2 connected vlies (unique) path v1 v2 . Since [A]-join forest,b dom((v)) = dom( (v)). Thus F [A]-join forest W .Moreover, construction F , number children inner vertex Fleast 2, F |Q| leaves. follows F 2|Q| 1 vertices.Therefore W [A]-acyclic set atoms |W | 6 2|Q| W f (Q).Part (2). Q extended Q+ follows. atom r(t1 , . . . , tk ) W f (Q),add Q new query atom r(1 , . . . , k ) 1 6 6 k, newly inventedvariable. Obviously, W |= Q+ thus homomorphism g extending fg(Q+ ) = W . Moreover, construction |Q+ | < 2|Q|.Lemma 5.7. Let G [A]-acyclic instance let G instance obtained Geliminating set atoms dom(S) A. G [A]-acyclic.Proof. = hhV, Ei, [A]-join forest G [A]-join forest Gobtained G repeatedly eliminating vertex v (v) S.construction, atom e eliminated G way property dom(e) A.Hence, every value b dom(G) A, node u V (u) = e cannot belonginduced (connected) subtree {v V | b dom((v))}. thus get G enjoys[A]-connectedness property.following Lemma used main tool subsequent complexity analysis.Lemma 5.8 (Squid Lemma). Let weakly guarded set TGDs schema R,database R, Q Boolean conjunctive query. chase(D, ) |= Q iffsquid decomposition = (Q+ , h, H, ) homomorphism : dom(h(Q+ ))dom(chase(D, )) that: (i) (H) chase (D, ), (ii) (T ) chase + (D, ).Proof. If. squid decomposition = (Q+ , h, H, ) Q homomorphismdescribed, composition h homomorphism ( h)(Q+ ) =139fiCal, Gottlob & Kifer(h(Q+ )) chase(D, ). Hence, chase(D, ) |= Q+ and, Lemma 5.3, chase(D, ) |=Q.if. Assume U = chase(D, ) |= Q. Then, exists homomorphism f :vars(Q) dom(U ) f (Q) chase(D, ). Lemma 3.11, chase + (D, ) [dom(D)]acyclic. Lemma 5.6, follows Boolean query Q+ < 2|Q| atoms,atoms Q contained Q+ , homomorphism g : dom(Q+ )dom(U ) g(Q+ ) U , g(Q+ ) [dom(D)]-acyclic.Partition vars(Q+ ) two sets vars (Q+ ) vars + (Q+ ) follows:vars (Q+ ) = {X vars(Q+ ) | g(X) dom(D)}vars + (Q+ ) = vars(Q+ ) vars (Q+ ).Define mapping h : vars(Q+ ) vars(Q+ ) follows. X vars(Q+ ), let h(X)lexicographically first variable set {Y vars(Q+ ) | g(Y ) = g(X)}. Letus define V V = h(vars (Q+ )). Moreover, let H set atomsh(Q+ ), vars(a) V = h(vars (Q+ )), let = h(Q+ ) H. Note that,definition H, g(H) chase (D, ) and, definition , g(T ) chase + (D, ). Letrestriction g dom(h(Q+ )). Clearly, , h, H, fulfill conditions (i)(ii) statement lemma. thus remains prove = (Q+ , h, H, )actually squid decomposition Q. this, need show [V ]-acyclic.prove this, first observe pair variables X, vars(Q+ )g(X) = g(Y ) h(X) = h(Y ). Therefore is, construction, bijectionh(dom(Q+ )) dom((Q+ )). particular, h(Q+ ) isomorphic (T ) viarestriction dom(T ). Since (T ) = (T ) obtained [dom(D)]-acyclicinstance (Q+ ) eliminating atoms whose arguments dom(D) (namelyatoms (H)), Lemma 5.7, (T ) [dom(D)]-acyclic and, therefore, triviallyalso [dom(D) dom((T ))]-acyclic. Now, since every X dom(T ) holds X Viff (X) dom(D), immediately follows that, since (T ) [dom(D)]-acyclic,[V ]-acyclic.5.2 Clouds Complexity Query Answering WGTGDsgoal subsection prove following theorem:Theorem 5.9. Let weakly guarded set TGDs, database schema R,Q Boolean conjunctive query. problem determining whether |= Q or,equivalently, whether chase(D, ) |= Q exptime case bounded arity,2exptime general.general case (of unbounded arities), first outline short high-level proofaimed specialists Computational Logic. proof makes sophisticated use previousresults. give much longer, self-contained proof, worksgeneral case case bounded arities. self-contained proof also introducesconcepts used following sections.High Level Proof Sketch Theorem 5.9 (General Case). transform original probleminstance (D, , Q) guarded first-order theory = (D, , Q) chase(D, ) |=140fiTaming Infinite ChaseQ iff unsatisfiable. signature uses set constants plus constantelement dom(D). Moreover, includes predicate symbols occurring D, ,Q, plus special nullary (i.e., propositional) predicate symbol q.contains ground facts D, plus instances rule r obtainedr replacing variables r occur non-affected positions constants. Noteresulting rules universally quantified guarded sentences. Moreover,squid decomposition = (Q+ , h, H, T, V ), possible replacement setvariables V constants signature , contains guarded sentence obtainedfollows. Notice Q := (H) Boolean acyclic conjunctive query. resultsGottlob, Leone, Scarcello (2003),5 Q thus rewritten (in polynomial time)equivalent guarded sentence . define ( q), obviouslyguarded, too. Let denote sentences mentioned far. constructionSquid Lemma (Lemma 5.8), follows chase(D, ) |= Q iff |= q. let= {q}. Obviously, unsatisfiable iff chase(D, ) |= Q.Note reduction arity-preserving exptime-reduction. Let exponential upper bound runtime required reduction (and thus also size(D, , Q)). deterministic version algorithm work Gradel (1999)deciding whether guarded theory unbounded arity satisfiable unsatisfiable runswdouble-exponential time O(2O(sw ) ), size theory w maximumpredicate arity. Therefore, overall runtime first computing = (D, , Q) input (D, , Q) size n maximum arity w, checking whether unsatisfiablewO(2O(t(n)w ) ), still double-exponential. Deciding |= Q thus2exptime.Note case bounded w, similar proof provide exptime bound,w2t(n)w would still doubly exponential due exponential term t(n), even wconstant. Actually, noted Barany, Gottlob, Otto (2010), evaluating non-atomicconjunctive queries guarded first-order theories bounded predicate arity fact2exptime-complete. Surprisingly, remains true even guarded theories(variable) database fixed guarded theory simple form involvingdisjunctions (Bourhis, Morak, & Pieris, 2013). therefore needed develop differentproof ideas.rest section present independent self-contained proof Theorem 5.9 developing tools analyzing complexity query answering WGTGDs. end introduce notion cloud atom chase databaseset WGTGDs. Intuitively, cloud set atoms chase(D, )whose arguments belong dom(a) dom(D). words, atoms cloudcannot nulls appear a. cloud important showsubtree gcf(D, ) rooted depends cloud.Definition 5.10. Let weakly guarded set TGDs schema Rdatabase R. every atom chase(D, ) cloud respectfollowing set: cloud (D, , a) = {b chase(D, ) | dom(b) dom(a) dom(D)}. Notice5. See Theorem 3 paper, proof, remark proof, Corollary 3.141fiCal, Gottlob & Kiferevery atom chase(D, ) cloud (D, , a). Moreover, defineclouds(D, ) = {cloud (D, , a) | chase(D, )}clouds + (D, ) = {(a, cloud (D, , a)) | chase(D, )}subset cloud (D, , a) called subcloud (with respect D).set subclouds atom denoted subclouds(D, , a). Finally, definesubclouds + (D, ) = {(a, C) | chase(D, ) C cloud (D, , a)}.Definition 5.11. Let B instance (possibly nulls) schema, R,database R. Let atoms Herbrand Base HB (B). sayD-isomorphic, denoted , simply case understood,bijective homomorphism6 f : dom() dom() f () = (and thusalso f 1 () = ). definition extends directly cases setsatoms atom-set pairs (in similar fashion clouds + (D, )).Example 5.12. {a, b} dom(D) {1 , 2 , 3 , 4 } N , have: p(a, 1 , 2 )p(a, 3 , 4 ) (p(a, 3 ), {q(a, 3 ), q(3 , 3 ), r(3 )}) (p(a, 1 ), {q(a, 1 ), q(1 , 1 ), r(1 )}).hand, p(a, 1 , 2 ) 6 p(a, 1 , 1 ) p(a, 1 , 2 ) 6 p(3 , 1 , 2 ).Lemma 5.13. Given database schema R instance B R, Disomorphism relation HB (B) (resp., 2HB (B) HB (B) 2HB (B) , Definition 5.11) equivalence relation.lemma follows directly definitions; lets us define, every setatoms HB (B), quotiont set A/D respect defined equivalencerelation . notion quotient set naturally extends sets sets atomsclouds(D, ), sequences (pairs, particular) thereof.Lemma 5.14. Let weakly guarded set TGDs let database schemaR. Let |R| denote number predicate symbols R, w maximum aritysymbols R. Then:(1) every atom chase(D, ), |cloud (D, , a)| 6 |R| (|dom(D)| + w)w .Thus, cloud (D, , a) polynomial size database arity wbounded exponential otherwise.w(2) atom chase(D, ), |subclouds(D, , a)| 6 2|R|(|dom(D)|+w) .w(3) |clouds(D, )/ | 6 2|R|(|dom(D)|+w) , i.e., areup isomorphismatexponentially many possible clouds subclouds chase, arity w bounded,otherwise doubly exponentially many.w(4) |clouds + (D, )/ | 6 |subclouds + (D, )/ | 6 |R|(|dom(D)|+w)w 2|R|(|dom(D)|+w) .Proof. claims proved combinatorial arguments follows.(1) distinct atoms cloud obtained placing symbols a, plus possiblysymbols dom(D), w arguments predicate symbol R.predicate R, number symbols thus placed |dom(D)| + w.6. Recall that, definition, restriction homomorphism dom(D) identity mapping.142fiTaming Infinite Chase(2) different ways choose subclouds(D, , a) clearly determines setsubsets cloud (D, , a).(3) easy see size set non-pairwise-isomorphic cloudschase bounded number possible subclouds fixed atom.(4) Here, counting number possible subclouds, associatedgenerating atom. inequality holds because, choose non-pairwiseisomorphic clouds, possible generating atoms arguments|dom(D)| + w symbols construct subclouds.Definition 5.15. Given database set WGTGDs, let atomchase(D, ). define following notions:set atoms label nodes subtrees gcf(D, ) rooted a;= cloud (D, , a);subset atoms gcf(D, ), gcf[a, S]7 inductively defined follows:(i) {a} gcf[a, S];(ii) b gcf[a, S] b , b obtained via chase rule applied using TGDbody head-atom , homomorphism , () = b() gcf[a, S].Theorem 5.16. database schema R, weakly guarded set TGDs,chase(D, ), = gcf[a, cloud (D, , a)].Proof. definitions gcf[a, cloud (D, , a)], gcf[a, cloud (D, , a)]a. remains show converse inclusion: gcf[a, cloud (D, , a)]. Definelevela (a) = 0 fact b cloud (D, , a) also define levela (b) = 0.every atom c , levela (c) defined distance (i.e., length path)c gcf(D, ).first show following facts parallel induction levela (b):(1) b cloud (D, , b) gcf[a, cloud (D, , a)].(2) b b gcf[a, cloud (D, , a)].Statement (2) converse inclusion after.Induction basis. levela (b) = 0, either (a) b cloud (D, , a) {a},(b) b = a. case (a), cloud (D, , a) gcf[a, cloud (D, , a)] therefore bgcf[a, cloud (D, , a)], proves (1). Moreover, since b cloud (D, , a), b cannot contain labeled nulls a, dom(b) dom(D) dom(a) dom(D). Thereforecloud (D, , b) cloud (D, , a) gcf[a, cloud (D, , a)], proves (2). case (b),b = thus cloud (D, , a) = cloud (D, , b) gcf[a, cloud (D, , a)], proves (1).Since b = gcf[a, cloud (D, , a)], (2) follows well.Induction step. Assume (1) (2) satisfied clevela (c) 6 assume levela (b) = + 1, > 0. atom b produced TGDwhose guard g matches atom b level i, is, induction hypothesis,gcf[a, cloud (D, , a)]. body atoms TGD match atoms whose arguments7. implicit here, avoid clutter.143fiCal, Gottlob & Kifermust cloud (D, , b) thus also gcf[a, cloud (D, , a)], induction hypothesis. Therefore, (2) holds b. show (1), consider atom b cloud (D, , b).case dom(b ) dom(b ), cloud (D, , b ) cloud (D, , b ) gcf[a, cloud (D, , a)].Otherwise, b contains least one new labeled null introduced generation b. Given weakly guarded set labeled null N introducedchase, must path b b gcf(D, ) (and therefore alsob). simple additional induction levelb (b ) shows applications TGDspath must fired elements gcf[a, cloud (D, , a)] only. Therefore,b gcf[a, cloud (D, , a)], proves (1).corollary follows directly theorem.Corollary 5.17. database schema R, weakly guarded set TGDs,a, b chase(D, ), (a, cloud (D, , a)) (b, cloud (D, , b)), b.Definition 5.18. Let database atom. canonical renaming :dom(a) dom(D) dom(D), = {1 , . . . , h } N set labeled nullsappearing a, 1-1 substitution maps element dom(D)null-argument first unused element . cloud (D, , a)(S) well-defined pair (can (a), (S)) denoted can(a, S).Example 5.19. Let = g(d, 1 , 2 , 1 ) = {p(1 ), r(2 , 2 ), s(1 , 2 , b)}, {d, b}dom(D) {1 , 2 } N . (a) = g(d, 1 , 2 , 1 ), (S) = {p(1 ), r(2 , 2 ),s(1 , 2 , b)}.Definition 5.20. database schema R, weakly guarded set TGDsR, set atoms S, write (D, , a, S) |= Q iff existshomomorphism (Q) .following result straightforwardly follows Theorem 5.16 previous definitions.Corollary 5.21. database schema R, weakly guarded set TGDs,chase(D, ), Q Boolean conjunctive query, following statementsequivalent:(1)(2)(3)(4)|= Q(D, , a, cloud (D, , a)) |= Q(D, , (a), cana (cloud (D, , a))) |= Qsubset cloud (D, , a) (D, , (a), (S )) |= Q.use pair can(a, cloud (D, , a)) unique canonical representativeequivalence class {(b, cloud (D, , b)) | (b, cloud (D, , b)) (a, cloud (D, , a))}clouds + (D, ). Therefore, set {can(a, cloud (D, , a)) | chase(D, )}quotient set clouds + (D, )/ isomorphic. Note that, Lemma 5.14, setsfinite size exponential |D| + || schema fixed (and double exponentialotherwise).Now, given database schema R, weakly guarded set TGDs R,atomic Boolean conjunctive query Q, describe alternating algorithm Acheck(D, , Q)144fiTaming Infinite Chasedecides whether |= Q. assume Q form Y1 , . . . , , q(t1 , t2 , . . . , tr ),t1 , . . . , tr , r > , terms (constants variables) dom(D){Y1 , Y2 , . . . , }.algorithm Acheck returns true accepts configuration, accordingcriteria explained below; otherwise, returns false. Acheck uses tuples form(a, S, , , b) basic data structures (configurations). Intuitively, configuration corresponds atom derived step chase computation togetherset already derived atoms belonging cloud a. informal meaningparameters configuration follows.(1) root atom chase subtree consideration.(2) cloud (D, , a); intuitively subcloud containing set atoms cloud (D, , a)that, computing chase(D, ), originally derived outside subtreeguarded chase forest rooted (and thus outside subtree rootedrgcf(D, )). expect atoms serve side atoms (i.e., atoms matchingnon-guard atoms TGD) deriving desired atom b starting a.(3) contains, every step computation, subset cloud (D, , a)computed far, assumed valid, verified anotherbranch computation.(4) total ordering atoms consistent order atomsproved algorithm (by simulating chase procedure).(5) b atom needs derived. cases (namely, main pathproof tree developed Acheck), algorithm try derive specificatom, match query atom q(t1 , . . . , tr ) atoms path.case, use symbol place b.ready describe algorithm Acheck sufficiently detailed level. However,omit many low-level details.Acheck first checks |= Q. so, Acheck returns true halts. Otherwise,algorithm attempts guess path, called main branch, contains atom qinstance Q. done follows.Initialization. algorithm Acheck starts guesses atom D,expand main branch eventually lead atom q matchingquery Q. end, algorithm guesses set cloud (D, , a) total orderS, generates configuration c0 = (a, S, , , ). set initialized= S.Form configurationadditional specifications. configuration,set set implicitly partitioned two sets + , + ={a1 , a2 , . . . , ak } disjoint D. total order elements precede+ . + , defined a1 a2 ak .Summary tasks Acheck performs configuration. AssumeAcheck algorithm generates configuration c = (a, S, , , b), b might . Acheckperforms following tasks c:Acheck verifies guessed set c actually subset cloud (D, , a).achieved massive universal branching described145fiCal, Gottlob & Kiferheading Universal Branching. Let us, however, anticipate works,may contribute understanding steps. Acheck verifyatoms a1 , . . . , ak chase(D, ), where, {1, . . . , k},proof ai chase(D, ) use premises atoms precede ai ,according . algorithm thus finds suitable atoms d1 , . . . , dk buildsproof trees a1 , . . . , ak . 1 6 6 k, generates configurationsform (di , S, {a1 , a2 , . . . , ai1 }, , ai ). configuration usedstarting point proof ai chase(D, ) assuming a1 , . . . , ai1 chase(D, )already established. Acheck thus simulates sequential proof atomscloud (D, , a) via parallel universal branching c.Acheck tests whether c final configuration (i.e., accepting rejecting one).described heading Test final Configuration below.c final configuration Acheck, means first componentyet one matched b (or query, b = ). Acheckmoves chase tree one step replacing child a. stepdescribed heading Existential Branching.following, let c = (a, S, , , b) configuration, b may .Test final configuration. b D, Acheck accepts configuration,expand further. b = , Acheck checks (via simple subroutine)whether Q matches a, i.e., homomorphic image query atom q(t1 , . . . , tr ).so, Acheck accepts c (and thus returns true) expand further. b 6= ,Acheck checks whether = b. true, Acheck accepts configuration cexpand further. Otherwise, configuration tree expanded described next.Existential Branching. Acheck guesses TGD body headatom , whose guard g matches via substitution (that is, (g) = a)() . () corresponds newly generated atom (possibly containingfresh labeled nulls N ). Note that, guess made, existentialbranching automatically fails Acheck returns false. define configuration c1Acheck creates c, first introduce intermediate auxiliary configurationb), where:c = (a, S, , ,(a) = () new atom generated application substitution .(b) contains atom dom(d) dom(a) dom(D). Thus,addition new atom a, inherits atoms subcloudparent configuration c compatible a. addition, includes setnewatoms(c) new atoms guessed Acheck algorithm. argumentsatom newatoms(c) must elements set dom(a) dom(D).(c) = S.total order obtained eliminating atoms(d)ordering atoms newatoms(c) atoms set oldproven(c) =(these assumed already proven parent configuration c).(e) b defined b = b.Next, Acheck constructs configuration c1 c canonicalization: c1 = (c),(b)), ()totalc1 = (can (a), (S), (S ), (),order atoms (S ) derived .146fiTaming Infinite ChaseIntuitively, c1 main child c way deriving query atom q(t1 , . . . , tr )assuming atoms guessed subcloud derivable.Universal Branching. generated configuration c, set equalS. already said, means assumed configuration setatoms derivable. verify indeed case, Acheck generates parallel,using universal computation branching, set auxiliary configurations provingguessed atoms (newatoms(c)) indeed derivable chaserespect .concateLet (newatoms(c)) = {n1 , . . . , nm } let linear ordern.nation order , restricted oldproven(c), order n1 n2(i)1 6 6 m, Acheck generates configuration c2 defined(i)ni ).c2 = (can (a), (S), (oldproven(c)) {n1 , . . . , ni1 }, (),completes description Acheck algorithm.Theorem 5.22. Acheck algorithm correct runs exponential time casebounded arities, double exponential time otherwise.Proof.Soundness. easy see algorithm sound respect standardchase, i.e., Acheck(D, , Q) returns true, chase(D, ) |= Q. fact, modulovariable renaming, preserves soundness according Corollary 5.21, algorithmnothing chasing respect , even chase steps necessarilyorder standard chase. Thus, atom derived Acheck occurschase. Since every chase computes universal solution complete respectconjunctive query answering, whenever Acheck returns true, Q entailedchase, thus also standard chase, chase(D, ).Completeness. completeness Acheck respect chase(D, ) shownfollows. Whenever chase(D, ) |= Q, finite proof Q, i.e., finite sequenceproof Q generated atoms ends atom q, instance Q.proof simulated alternating computation Acheck follows: (i) steer mainbranch Acheck towards (a variant of) q choosing successively TGDssubstitutions (modulo appropriate variable renamings) used standardchase branch q; (ii) whenever subcloud chosen atomAcheck, choose set atoms cloud (D, , a) (D atoms(proof Q )), modulo appropriatevariable renaming; (iii) ordering , always choose one given proof Q .fact Q-instance lost replacing configurations canonical versionsguaranteed Corollary 5.21.Computational cost. case bounded arity, size configuration cpolynomial . Thus, Acheck describes alternating pspace (i.e., apspace)computation. well-known apspace = exptime. case arity bounded,configuration requires exponential space. algorithm describescomputation Alternating expspace, equal 2exptime.147fiCal, Gottlob & KiferCorollary 5.23. Let weakly guarded set TGDs, let databaseschema R. Then, computing chase (D, ) done exponential time casebounded arity, double exponential time otherwise.Proof. sufficient start empty set cycle ground atoms bHerbrand base HB (D) checking whether chase(D, ) |= b. holds, addb A. result chase (D, ). claimed time bounds follow straightforwardly.finally state independent proof Theorem 5.9.Proof Theorem 5.9. construct algorithm Qcheck Qcheck(D, , Q) outputstrue iff |= Q (i.e., iff chase(D, ) |= Q). algorithm relies notion squiddecompositions, Lemma 5.8; works follows.(1) Qcheck starts computing chase (D, ).(2) Qcheck nondeterministically guesses squid decomposition = (Q+ , h, H, ) Qbased set V vars(h(Q+ )), H = {a h(Q+ ) | vars(a) V }[V ]-acyclic. Additionally, Qcheck guesses substitution 0 : V dom(D)0 (H) chase (D, ). Note np guess, number atomsQ+ twice number atoms Q.(3) Qcheck checks whether 0 extended homomorphism (T )chase + (D, ). Lemma 5.8, equivalent check chase(D, ) |= Q.exists iff connected subgraph 0 (T ), homomorphism(t) chase + (D, ). Qcheck algorithm thus identifies connectedcomponents 0 (T ). component [dom(D)]-acyclic conjunctive query,whose arguments may contain constants dom(D). componentthus represented [dom(D)]-join tree t. join tree t, Qchecktests whether exists homomorphism (t) chase + (D, ).done subroutine Tcheck, takes TGD set , database D,connected subgraph (i.e., subtree) 0 (T ) input. inner workingsTcheck(D, , t) described below.(4) Qcheck outputs true iff check (3) gives positive result.correctness Qcheck follows Lemma 5.8. Given step (2) nondeterministic, complexity Qcheck npX , i.e., np oracle X, Xcomplexity class sufficiently powerful for: (i) computing chase (D, ), (ii)performing tests Tcheck(D, , t).describe Tcheck subroutine.General notions. Tcheck(D, , t) obtained Acheck via following modifications. configuration Tcheck maintains pointer Tpoint vertex (an atomaq ). Intuitively, provides link root subtree still needsmatched descendant configurations c. addition data structures carriedconfiguration Acheck, configuration Tcheck also maintains array substlength w, w maximum predicate arity R. Informally, subst encodessubstitution maps current atom (the canonicalized version of) currentatom chase(D, ).148fiTaming Infinite ChaseTcheck works like Acheck, instead nondeterministically constructing main configuration path configuration tree eventually atom matches query,nondeterministically constructs main configuration (sub)tree configuration tree,eventually atoms join tree get consistently translated vertices. important component main configuration c Tcheck current atom a.Initially, nondeterministically chosen atom D. subsequent configurationsalternating computation tree, take nodes gcf(D, ).Initialization. Similarly Acheck, computation starts generating initialconfiguration (a, S, S, , , Tpoint, subst), nondeterministically chosendatabase D, Tpoint points root r t, subst homomorphic substitutionsubst(r) = a, r homomorphically mappable a; otherwise subst empty.configuration root main configuration tree.general, pointer Tpoint main configuration c = (a, S, , , , Tpoint, subst)points atom aq t, yet matched. algorithm attemptsexpand configuration successively guessing subtree configurations, mimickingsuitable subtree gcf(D, ) satisfies subquery rooted aq .Whenever Tcheck generates configuration, Acheck, Tcheck generatesvia universal branching number configurations whose joint task verifyelements indeed provable. (We provide details branchingdone.)Expansion. expansion main configuration c = (a, S, , , , Tpoint, subst)works follows. configuration c, Tcheck first checks whether exists homomorphism (subst(aq )) = a.1. ( exists.) exists, two cases:1.1. aq leaf t, current configuration turns accepting one.1.2. aq leaf t, Tcheck nondeterministically guesses whethergood match, i.e., one contributes global query answerexpanded map entire tree gcf(D, ).1.2.1. (Good match). case good match, Tcheck branches universallyfollowing child aqs aq t. nondeterministically (i.e.,via existential branching) creates new configurationcs = (as , Ss , Ss , , , Tpoints , substs )Tpoints points aqs , substs encodes compositionsubsts . atom guessed, analogously done Acheck,guessing TGD body head atom ,guard atom g matches via homomorphism (that is, (g) = a)() . cloud subsets Ss Ss chosenAcheck. Intuitively, Tcheck, found good match aq a, triesmatch children aq children (and, eventually, descendants)gcf(D, ). Finally, function indicates appropriatecanonizations made obtain cs c (we omit tedious details).149fiCal, Gottlob & Kifer1.2.2. (No good match). case good match exists, child configurationcnew = cananew (anew , S, , , , Tpoint, subst)c nondeterministically created, whose first component represents childanew a, cnew inherits remaining components c.Intuitively, failed matching aq (to which, remind, Tpointpoints) a, Tcheck attempts matching aq childgcf(D, ). analogy previous case, anew obtained guessingTGD body head atom , guardatom g matches via homomorphism (that is, (g) = a), () ,anew := (). Again, function term cananew indicatesappropriate canonizations applied (which describe detail).2. ( exist.) case, Tcheck proceeds exactly case 1.2.2, namely,attempts matching aq child (or eventually descendant)gcf(D, ).Correctness. correctness Tcheck shown along similar linesAcheck. important additional point consider Tcheck that, given queryacyclic, actually sufficient remember configuration c latestatom substitution subst. correctness Qcheck follows correctnessTcheck Lemma 5.8.Computational cost. complexity Qcheck, note case aritybounded, Tcheck runs apspace = exptime, computing chase (D, ) exptime Corollary 5.23. Thus, Qcheck runs time npexptime = exptime. caseunbounded arities, computing chase (D, ) running Tcheck 2exptime,therefore Qcheck runs time np2exptime = 2exptime.combining Theorems 4.1 5.9 immediately get following characterizationcomplexity reasoning weakly guarded sets TGDs.Theorem 5.24. Let weakly guarded set TGDs schema R, databaseR, Q Boolean conjunctive query. Determining whether |= Q or, equivalently,whether chase(D, ) |= Q exptime-complete case bounded predicate arities, evenfixed Q atomic. general case unbounded predicate arities,problem 2exptime-complete. completeness results hold problemquery containment weakly guarded sets TGDs.Generalization. definition WGTGDs generalized classes TGDs whoseunguarded positions guaranteed contain controlled finite number null-valuesonly. Let f computable integer function two variables. Call predicate positionTGD set f -bounded f (|D|, ||) null values appear chase(D, )arguments position ; otherwise call f -unbounded. set TGDs f -weakly guardedrule contains atom body covers variables occurwithin rule f -unbounded positions only. minor adaptation proofTheorem 3.14, seen CQ-answering class f -weakly guarded TGDsdecidable. Moreover, simple modification Qcheck Tcheck procedures,150fiTaming Infinite Chaseallowing polynomial number nulls enter unguarded positions, shownCQ-answering fixed sets WGTGDs exptime-complete worst case,class WGTGD sets defined follows. set TGDs belongs classf -weakly guarded function f exists function g,f (|D|, ||)| 6 |D|g(||) .6. Guarded TGDsturn attention GTGDs. first consider case variable databaseinput. Later, prove part complexity bounds stronger conditionfixed database.6.1 ComplexityVariable DatabaseTheorem 6.1. Let set GTGDs schema R database R. Let,before, w denote maximum predicate arity R |R| total number predicatesymbols R. Then:(1) Computing chase (D, ) done polynomial time w |R|bounded and, thus, also case fixed set . problem exptime(and thus exptime-complete) w bounded, 2exptime otherwise.(2) Q atomic fixed Boolean query checking whether chase(D, ) |= Qptime-complete w |R| bounded. problem remains ptimecomplete even case fixed. problem exptime-complete w bounded2exptime-complete general. remains 2exptime-complete even |R|bounded.(3) Q general conjunctive query, checking chase(D, ) |= Q np-completecase w |R| bounded and, thus, also case fixed set . Checkingchase(D, ) |= Q exptime-complete w bounded 2exptime-completegeneral. remains 2exptime-complete even |R| bounded.(4) BCQ answering GTGDs np-complete w |R| bounded, evencase set GTGDs fixed.(5) BCQ answering GTGDs exptime-complete w bounded 2exptimecomplete general. remains 2exptime-complete even |R| bounded.Proof. First, note items (4) (5) immediately follow first three items,given chase(D, ) universal model. therefore need prove items (1)-(3).first explain hardness results obtained, deal matchingmembership results.Hardness Results. ptime-hardness checking chase(D, ) |= Q atomic (andthus also fixed) queries Q fixed follows fact ground atom inferencefixed fully guarded Datalog program variable databases ptime-hard. fact,proof Theorem 4.4 work Dantsin, Eiter, Gottlob, Voronkov (2001)shown fact inference single-rule Datalog program whose body guardatom contains variables ptime-hard. np-hardness item (3) immediatelyderived hardness CQ containment (which turn polynomially equivalent151fiCal, Gottlob & Kiferquery answering) without constraints (Chandra & Merlin, 1977). hardness resultsexptime 2exptime derived via minor variations proof Theorem 4.1.example, |R| unbounded w bounded, tape cells polynomialworktape simulated using polynomially many predicate symbols. example,fact configuration v cell 5 contains symbol 1 encoded S51 (v). omitdetails, given much stronger hardness result established via fullproof Theorem 6.2.Membership results. membership results proved exactly weaklyguarded sets TGDs, except instead using concept cloud, usesimilar concept restricted cloud, coincides type atomwork Cal et al. (2012a). restricted cloud rcloud (D, , a) atom chase(D, )set atoms b chase(D, ) dom(b) dom(a). proofalmost identical one Theorem 5.16, show database,set GTGDs, chase(D, ), r = gcf[a, rcloud (D, , a)], rdefined r = {a } rcloud (D, , a). follows that, main computationaltasks, use algorithms rAcheck, rQcheck, rTcheck, differ alreadyfamiliar Acheck, Qcheck, Tcheck restricted clouds instead ordinaryclouds used. However, unlike case |R| w bounded cloud(or subcloud) polynomial size |D |, restricted cloud rcloud (D, , a)constant number atoms, storing canonical version (rcloud (D, , a)) thusrequires logarithmic space only. total, case |R| w bounded, dueuse restricted clouds (and subsets thereof) configuration c rAcheck rTcheckrequires logarithmic space. Since alogspace = ptime, ptime-results atomicqueries items (1) (2) follow. Moreover, |R| w bounded, general(non-atomic non-fixed) queries, rQcheck algorithm decides chase(D, ) |= Qnp guessing squid decomposition (in nondeterministic polynomial time) checking(in alogspace=ptime) homomorphism squid decompositionchase(D, ). Thus, case, rQcheck runs npptime = np, proves np upperbound Item (3). If, addition, Q fixed, Q constant number squiddecompositions, therefore rQcheck runs ptimeptime = ptime, proves ptimeupper bound fixed queries mentioned item (2). exptime 2exptime upperbounds inherited upper bounds WGTGDs.Note one main results Johnson Klug (1984), namely, querycontainment inclusion dependencies bounded arities np-complete, specialcase Item (3) Theorem 6.1.6.2 ComplexityFixed Databasenext result tightens parts Theorem 6.1 showing exptime2exptime-completeness results hold even case fixed input database.Theorem 6.2. Let set GTGDs schema R. before, let w denotemaximum arity predicate R |R| total number predicate symbols. Then,fixed databases D, checking whether chase(D, ) |= Q exptime-complete w152fiTaming Infinite Chasebounded 2exptime-complete unbounded w. unbounded w, problem remains2exptime-complete even |R| bounded.Proof. First, observe upper bounds (i.e., membership results exptime2exptime) inherited Theorem 6.1, suffices prove hardness resultscases Q fixed atomic query.start proving checking chase(D, ) |= Q exptime-hard w bounded.well-known apspace (alternating pspace) equals exptime.already noted proof Theorem 4.1, sufficient simulate linspacealternating Turing machine (ATM) uses n worktape cells every input(bit string) size n, input string initially present worktape.particular, show accepts input iff chase(D, ) |= Q.Without loss generality, assume (i) ATM exactly one accepting state,a, also halting state; (ii) initial state existential state; (iii)alternates transition existential universal states; (iv) nevertries read beyond tape boundaries.Let defined = (S, , , q0 , {sa }), set states, = {0, 1, }tape alphabet, blank tape symbol, : (S {, r, })2transition function ( denotes stay head move, r denote leftright respectively), q0 initial state, {sa } singleton set final(accepting) states. Since alternating TM, set states partitionedtwo sets, universal existential states, respectively. general ideaencoding different configurations input length n representedfresh nulls generated construction chase.Let us describe schema R. First, integer 1 6 6 n, R containspredicate head /1, head (c) true iff configuration c headtape cell i. R also predicates zero /1, one /1, blank /1, zero (c),one (c), blank (c) true configuration c tape cell contains symbol 0, 1,, respectively. Furthermore, state S, R predicate state /1,state (c) true iff state configuration c s. R also contains: predicate start/1,start(c) true iff c starting configuration; predicate config/1, trueiff argument identifies configuration; predicate next/3, next(c, c1 , c2 )true c1 c2 two successor configurations c. also predicatesuniversal /1 existential /1, universal (c) existential (c) true cuniversal (respectively, existential) configuration. Finally, predicate accepting/1,accepting(c) true accepting configurations c, propositional symbolaccept, true iff Turing Machine accepts input I.describe set (M, I) GTGDs simulates behavior inputI. rules (M, I) follows.1. Initial configuration generation rules. following rule creates initial state:X init(X). also add rule init(X) config(X), says initialconfiguration is, fact, configuration.2. Initial configuration rules. following set rules encodes tape contentinitial configuration, is, input string I. 1 6 6 n, i-th celltape contains 0, add rule init(X) zero (X); contains 1,153fiCal, Gottlob & Kiferadd init(X) one (X). also add rule init(X) existential (X) ordersay, without loss generality, initial configuration existential one.Moreover, add rules init(X) head 1 (X) init(X) state s0 (X) defineinitial values state head position input I.3. Configuration generation rules. add rule creates two successor configurationidentifiers configuration identifier. Moreover, add rules statingnew configuration identifiers indeed identify configurations:config(X) X1 ,X2 next(X, X1 , X2 ),next(X, Y, Z) config(Y ),next(X, Y, Z) config(Z).4. Transition rules. show example transition rules generatedtransition finite control. Assume, instance, transition table containsspecific transition form: (s, 0) ( (s1, 1, r) , (s2, 0, ) ). assertfollowing rules, 1 6 6 n:head (X), zero (X), state (X), next(X, X1 , X2 ) state s1 (X1 )head (X), zero (X), state (X), next(X, X1 , X2 ) state s2 (X2 ).Moreover, 1 6 < n rules:head (X), zero (X), state (X), next(X, X1 , X2 ) one (X1 )head (X), zero (X), state (X), next(X, X1 , X2 ) head i+1 (X1 ),1 < 6 n add rules:head (X), zero (X), state (X), next(X, X1 , X2 ) zero (X2 )head (X), zero (X), state (X), next(X, X1 , X2 ) head i1 (X2 )types transition rules constructed analogously. Note totalnumber rules added 6n times number transition rules. Hence linearlybounded size n input string M.5. Inertia rules. rules state tape cells positions head keepvalues. Thus, 1 6 i, j 6 n 6= j add rules:head (X), zero j (X), next(X, X1 , X2 ) zero j (X1 )head (X), one j (X), next(X, X1 , X2 ) one j (X1 )head (X), blank j (X), next(X, X1 , X2 ) blank j (X1 ),6. Configuration-type rules. rules say immediate successor configurationsexistential configuration universal, vice-versa:existential (X), next(X, X1 , X2 ) universal (X1 )existential (X), next(X, X1 , X2 ) universal(X2 )universal (X), next(X, X1 , X2 ) existential (X1 )universal (X), next(X, X1 , X2 ) existential (X2 ).154fiTaming Infinite Chase7. Acceptance rules. recursive rules state configuration accepting:state sa (X) accepting(X)existential (X), next(X, X1 , X2 ), accepting(X1 ) accepting(X)existential (X), next(X, X1 , X2 ), accepting(X2 ) accepting(X)universal (X), next(X, X1 , X2 ), accepting(X1 ), accepting(X2 ) accepting(X)init(X), accepting(X) accept.completes description set TGDs (M, I). Note set guarded,maximum predicate arity 3, obtained logarithmic spaceconstant machine description M. faithfully simulates behavior alternatinglinear space machine input I. follows (M, I) |= accept iff accepts input I.Let D0 denote empty database, let Q0 ground-atom query accept.(M, I) D0 |= Q0 iff accepts input I. shows answering groundatom queries fixed databases constrained bounded arity GTGDs exptime-hard.Let us illustrate obtain 2exptime hardness result guarded TGDsarities unbounded, number |R| predicate symbols schema Rbounded constant. Given aexpspace=2exptime (aexpspace alternatingaexpspace), aim simulate aexpspace Turing machine. sufficientsimulate one uses 2n worktape cells, since acceptance problemmachines already 2exptime-hard. fact, trivial padding arguments,acceptance problem every aexpspace machine transformed polynomial timeacceptance problem one using 2n worktape cells.problem is, however, longer construct polynomial numberrules explicitly address worktape cell i, pair cells i, j, sinceexponential number worktape cells. idea encode tape cell indexesvectors symbols (v1 , . . . , vk ) vi {0, 1}. proof Theorem 4.1,could define, polynomial number rules, successor relation succ stores pairsindexes succ(v1 , . . . , vk , w1 , . . . , wk ). However, difficulty:two different types variables: variables Vi , Wj range bits vi , wiabove-described bit vectors, variables X, Y, Z range configurations.major difficulty that, given rules guarded, must make suretwo types variables, whenever occur elsewhere rule body, also occurguard. end, use fixed database D01 contains single factzeroone(0, 1), construct guard relation g vector vn bits binary successor w, configuration x two successorconfigurations z, relation g contains tuple g(v, w, x, y, z). use severalauxiliary relations construct g.technical reasons, first two arguments atoms dummyvariables T0 T1 always forced take values 0 1, respectively.way, convenient, values 0 1 available implicitly formvariables, need use constants explicitly rules.Given database non-empty, need create initial configuration identifier via existential rule before. simply take 0 identifier155fiCal, Gottlob & Kiferinitial configuration: zeroone(T0 , T1 ) init(T0 , T1 , T0 ). (Here, first two arguments init(T0 , T1 , T0 ) serve, explained, carry values 0 1 along.)also add: init(T0 , T1 , T0 ) config(T0 , T1 , T0 ) assert 0 identifier initialconfiguration. Next present new configuration generation rules.config(T0 , T1 , X) Y, Z next(T0 , T1 , X, Y, Z),next(T0 , T1 , X, Y, Z) config(T0 , T1 , ),next(T0 , T1 , X, Y, Z) config(T0 , T1 , Z).use rules create relation b atom b(0, 1, v, x, y, z) containstuple vector v n bits, configuration x. better readability,whenever useful, use superscripts indicating arity vector variables:instance, V(n) denotes V1 , . . . , Vn . Moreover, 0(j) denotes vector j zeros 1(j)vector j ones. start rule next(T0 , T1 , X, Y, Z) b(T0 , T1 , T0 (n) , X, Y, Z),defines atom b(0, 1, 0(n) , x, y, z), configuration x next-successorsz.following n rules, 1 6 6 n, generate exponential number new atoms,triple X, Y, Z, swapping 0s 1s possible ways. Eventually, chasegenerate possible prefixes n bits.b(T0 , T1 , U1 , . . . , Ui1 , T0 , Ui+1 , . . . , Un , X, Y, Z)b(T0 , T1 , U1 , . . . , Ui1 , T1 , Ui+1 , . . . , Un , X, Y, Z).ready define guard-relation g another group guarded rules.0 6 r < n, add:b(T0 , T1 , U(r) , T0 , T1 (nr1) , X, Y, Z) g(U(r) , T0 , T1 (nr1) , U(r) , T1 , T0 (nr1) , X, Y, Z).n rules define exponential number cell-successor pairs tripleconfiguration identifiers X, Y, Z, Z next configurations followingX. particular, relation g contains precisely tuples g(v, w, x, y, z), vn-ary bit vector, w binary successor, x configuration identifier, firstsuccessor via next relation, z second successor via next relation.ready simulate aexpspace Turing machine input stringset GTGDs (M , I). Since simulation similar one presentedfirst part proof, sketch point main differences.simulation, use (in addition aforementioned auxiliary predicates)predicates similar ones used earlier simulation exptime Turing machineM. However, use constant number predicates. So, rather using, atomshead (x), zero (x) on, use vectorized versions head (v, x), zero(v, x)on, v bit vector length n takes role exponential index. Thus,example, equivalent earlier rulehead (X), zero (X), state (X), next(X, X1 , X2 ) one (X1 )g(V, W, X, X1 , X2 ), head (V, X), zero(V, X), state(X, s) one(V, X1 ). earlier rulehead (X), zero (X), state (X), next(X, X1 , X2 ) head i1 (X2 )156fiTaming Infinite Chasebecomes g(V, W, X, X1 , X2 ), head (W, X), zero(W, X), state(X, s) head (V, X2 ).straightforward see initialization rules written. Informally,copying input string worktape, place n input bits tapewriting rule bit. add rules fill positions n + 1 2nblanks. done similar way second part proofTheorem 4.1, omit details.remaining issue specification inertia rules. rules dealpairs i, j different, necessary adjacent, tape cell positions earlier simulation.adjacent cell positions available far. problem solveddifferent ways. One possibility described below.simply modify definition predicate b adding second vectorn bits b-atoms b-atoms actually form b(T0 , T1 , v, u, x, y, z),v u range possible distinct pairs bit vectors length n. u vectorcarried g-atoms. thus assume g-atoms formg(v, w, u, x, y, z). former inertia rule head (X), zero j (X), next(X, X1 , X2 ) zero j (X1 )would become g(V, W, U, X, X1 , X2 ), head (W, X), zero(U, X) zero(U, X1 ).remains defined configuration acceptance rules. configuration rules similar ones used previous reduction, hence leaveexercise. acceptance rules follows:state(X, sa ) accepting(X)existential (X), g(V, W, X, X1 , X2 ), accepting(X1 ) accepting(X)existential (X), g(V, W, X, X1 , X2 ), accepting(X2 ) accepting(X)universal (X), g(V, W, X, X1 , X2 ), accepting(X1 ), accepting(X2 ) accepting(X)zeroone(T0 , T1 ), accepting(T0 ) accept.completes description set TGDs (M , I). Note setguarded constant number predicates. obtained logspaceconstant machine description M. also faithfully simulates behavioralternating exponential space machine input I. follows (M , I) |= acceptiff accepts input I. Let Q0 BCQ defined Q0 = {accept}.D01 (M , I) |= Q0 iff accepts input I. shows answering ground atomicqueries fixed databases guarded TGDs fixed number predicate symbols(but unbounded arity) 2exptime-hard.7. Polynomial Clouds Criterionprevious section seen that, case bounded arity, query answeringweakly guarded sets TGDs exptime-complete, query answering GTGDsnp-complete. Note that, unrestricted queries databases, np-completenessbest obtain. fact, even absence constraints, BCQ answering problemnp-complete (Chandra & Merlin, 1977).section, establish criterion used tool recognizing relevantcases query answering np even weakly guarded sets TGDsfully guarded. Note consider setting weakly guarded set157fiCal, Gottlob & KiferTGDs fixed setting classes TGD sets considered. classes,require uniform polynomial bounds.Definition 7.1. [Polynomial Clouds Criterion] fixed weakly guarded set TGDssatisfies Polynomial Clouds Criterion (PCC ) following conditions hold:1. exists polynomial () database D, |clouds(D, )/ | 6(|D|). words, isomorphism, polynomially manyclouds.2. polynomial () that, database atom a:cloud (D, , a) computed time (|D|, ||),6 cloud (D, , a) computed time (|D|, ||) startingD, a, cloud (D, , b), b predecessor gcf(D, ).also say satisfies PCC respect . Noteabove, || constant omitted. However, use || justifiedfollowing. class C weakly guarded TGD sets satisfies PCC fixedpolynomials TGD set C satisfies PCC uniformlyrespect (i.e., TGD set class , bound).Theorem 7.2. Let fixed weakly guarded set TGDs schema R,enjoys Polynomial Clouds Criterion. Then:Deciding database atomic fixed Boolean conjunctive query Q whether|= Q (equivalently, whether chase(D, ) |= Q) ptime.Deciding database general Boolean conjunctive query Q whether |=Q (equivalently, chase(D, ) |= Q) np.Proof. polynomial algorithm Acheck2 atomic queries Q works follows. startproduce chase forest gcf(D, ) using standard chase. addition, immediatelygenerating node cloud cloud (D, , a) (in polynomial time), store(a, cloud (D, , a)) buffer, call cloud-store. Whenever branchforest reaches vertex b b (cloud (D, , b)) already cloud-store,expansion branch b blocked. Since polynomial numberpairs (a, cloud (D, , a)), algorithm stops polynomial number chase steps,step requiring polynomial time. Now, Corollary 5.17, cloud-store alreadycontains possible atoms chase(D, ) clouds, isomorphism. checkwhether chase(D, ) |= Q holds atomic query Q, thus sufficient test whetherevery atom c occurs cloud-store matches Q. summary, Acheck2 runs ptime.algorithm Qcheck2 conjunctive queries works like Qcheck, exceptcalls algorithm Tcheck2 subroutine instead Tcheck. input Tcheck2 D,Q, also cloud-store computed Acheck2. assume cloud-storeidentifies entry e = (a, cloud (D, , a)) unique integer e# using O(log n) bitsonly. Tcheck2 alternating algorithm works essentially like Tcheck, exceptfollowing modifications:Tcheck always guesses full cloud = cloud (D, , a), instead possibly guessingsubcloud. contrast, Tcheck2 guesses entry number e# correspondingentry (a, cloud (D, , a)) cloud-store.158fiTaming Infinite ChaseTcheck2 verifies correctness cloud guess alogspace using D, a, e# , wellb e# , b main atom predecessor configuration eentry cloud-store featuring b (b, cloud (D, , b)). Note verificationeffectively possible due condition (2) Definition 7.1.Tcheck2 needs compute main configuration treethe one whose configurations contain . algorithm compute auxiliary branches, sincelonger necessary, correctness check done different way.configurations Tcheck2 need guess memorize linear ordersset + .Given Tcheck2 alogspace algorithm, Qcheck2 npalogspace procedure. Sincenpalogspace = npptime = np, query answering np. case fixed conjunctive queryQ, since Q constant number squid decompositions, Qcheck2 runs ptimeptime =ptime.Note Polynomial Clouds Criterion syntactic. Nevertheless, usefulproving query answering weakly guarded sets TGDs np, evenpolynomial time atomic queries. application criterion illustratedSection 10.following direct corollary Theorem 6.1.Theorem 7.3. (1) Every set GTGDs satisfies PCC. (2) constant k,class GTGD sets arity bounded k satisfies PCC.following result obtained minor adaptation proof Theorem 7.2.Theorem 7.4. Let fixed weakly guarded set TGDs enjoys PolynomialClouds Criterion, let k constant. Then:(1) database Boolean conjunctive query treewidth 6 k, decidingwhether |= Q (equivalently, chase(D, ) |= Q) ptime.(2) tractability result holds acyclic Boolean conjunctive queries.analogy PCC, one may define various criteria based bounds.particular, define Exponential Clouds Criterion (ECC) classes TGDsets, use next section, follows:Definition 7.5. [Exponential Clouds Criterion] Let C class weakly guarded TGDsets. C satisfies Exponential Clouds Criterion (ECC) following conditionssatisfied:1. polynomial () every database set TGDsC size n, |clouds(D, )/ | 6 2(|D|+n) .2. exists polynomial () every database D, set TGDsC size n, atom a:D, cloud (D, , a) computed time 2 (|D|+n) ,6 D, cloud (D, , a) computed time 2 (|D|+n) D, a,cloud (D, , b), b predecessor gcf(D, ).159fiCal, Gottlob & Kiferfollowing result sets TGDs enjoying ECC:Theorem 7.6. weakly guarded set TGDs class C enjoys Exponential Clouds Criterion, deciding database Boolean conjunctive queryQ (atomic not) whether |= Q exptime.Proof (sketch). proof similar Theorem 7.2. main differenceptime alogspace replaced exptime apspace, respectively.get query answering atomic queries apspace = exptime, answeringnon-atomic queries npapspace = npexptime = exptime. Thus, case,difference atomic non-atomic query answering: exptime.8. TGDs Multiple-Atom Headsmentioned Section 2, complexity results proved far single-headed TGDs alsocarry general case, multiple atoms may appear rule heads. makeclaim formal here.Theorem 8.1. complexity results derived paper sets TGDs whose headssingle-atoms equally valid sets multi-atom head TGDs.Proof (sketch). suffices show upper bounds carry setting TGDsmultiple-atom heads. exhibit transformation arbitrary set TGDsschema R set single-headed TGDs schema R extends Rauxiliary predicate symbols.TGD set obtained replacing rule form r : body(X)head 1 (Y), head 2 (Y), . . . , head k (Y), k > 1 set variablesappear head, following set rules:body(X) V (Y)V (Y) head 1 (Y)V (Y) head 2 (Y)...V (Y) head k (Y),V fresh predicate symbol, arity number variablesY. Note that, general, neither contained X way around.easy see that, except atoms form V (Y), chase(D, ) chase( , D)coincide. atoms form V (Y) completely new predicates thusmatch predicate symbol conjunctive query Q. Therefore, chase(D, ) |= Q iffchase( , D) |= Q.Obviously, constructed logspace . Therefore, extensioncomplexity results general case immediate, except case bounded arity.Notice arity auxiliary predicate construction dependsnumber head-variables corresponding transformed TGD, which, general,bounded.160fiTaming Infinite Chasecase bounded-arity WGTGDs, exptime upper bound still derivedtransformation showing class TGD sets obtainedtransformation satisfies Exponential Clouds Criterion Section 7. seedatabase exponential number clouds, noticeevery large atom V (Y) derived rule small weak guard g body,i.e., weak guard g bounded arity. cloud cloud (D, , g) weak guard gclearly determines everything g guarded chase forest; particular, cloudV (Y). Thus set clouds(D, ) clouds atoms determinedclouds atoms bounded arity. immediately verifiable combinatorial reasons,singly-exponentially many clouds. shows |clouds(D, )/ |singly-exponentially bounded. Therefore, first condition Definition 7.5 satisfied.hard verify second condition Definition 7.5, too. Thus, query-answeringbased bounded-arity WGTGDs exptime. Given GTGDs subclassWGTGDs, exptime bound holds bounded-arity GTGDs, well.completely different proof theorem follows directly resultsGottlob, Manna, Pieris (2013a) class GTGDs, Gottlob,Manna, Pieris (2013b) class WGTGDs.9. EGDssection deal equality generating dependencies (EGDs), generalizationfunctional dependencies, which, turn, generalize key dependencies (Abiteboul, Hull, &Vianu, 1995).Definition 9.1. Given relational schema R, EGD first-order formula formX(X) X = Xk , (X) conjunction atoms R, X , Xk X.dependency satisfied instance B if, whenever homomorphism hmaps atoms (X) atoms B, h(X ) = h(Xk ).possible repair, chase, instance according EGDs analogychase based TGDs. start defining EGD chase rule.Definition 9.2. [EGD Applicability] Consider instance B schema R, EGDform (X) Xi = Xj R. say applicable Bhomomorphism h h((X)) B h(Xi ) 6= h(Xj ).Definition 9.3. [EGD Chase Rule] Let EGD form (X) Xi = Xjsuppose applicable instance B via homomorphism h. resultapplication B h failure {h(Xi ), h(Xj )} (because unique nameassumption). Otherwise, result application instance B obtained Breplacing occurrence h(Xj ) h(Xi ) h(Xi ) precedes h(Xj ) lexicographicalorder. h(Xj ) precedes h(Xi ) occurrences h(Xi ) replaced h(Xj ),hinstead. write B B say B obtained B via single EGD chase step.Definition 9.4. [Chase sequence respect TGDs EGDs] Let database= E , set TGDs E set EGDs. (possiblyinfinite) chase sequence respect sequence instances B0 , B1 , . . .161fiCal, Gottlob & Kifer,hiBi Bi+1 , B0 = E > 0. chase sequence saidfailing last step failure. chase sequence said fair every TGDEGD applicable certain step eventually applied.case fair chase sequence happens finite, B0 , . . . , Bm , rule application change Bm , chase well defined Bm , denoted chase(D, ).purposes, order application TGDs EGDs irrelevant. followingtherefore, saying fair chase sequence, refer fair chase sequence,chosen according order application dependencies.well-known (see Johnson & Klug, 1984) EGDs cause problems combinedTGDs, even simple types EGDs, plain key constraints,implication problem EGDs plus TGDs query answering problem undecidable.remains true even EGDs together GTGDs. fact, even though inclusiondependencies fully guarded TGDs, implication problem, query answering, querycontainment undecidable keys used EGDs inclusion dependenciesTGDs (Chandra & Vardi, 1985; Mitchell, 1983; Cal et al., 2003a).Moreover, result infinite chase using TGDs well-defined limitinfinite, monotonically increasing sequence (or, equivalently, least fixed-pointmonotonic operator), sequence sets obtained infinite chase databaseTGDs EGDs is, general, neither monotonic convergent. Thus, even thoughdefine chase procedure TGDs plus EGDs, clear resultinfinite chase involving TGDs EGDs defined.reasons, cannot hope extend positive results weakly guardedsets TGDs, even GTGDs, previous sections include arbitrary EGDs.Therefore, looking suitable restrictions EGDs, would allow us to: (i)use (possibly infinite) chase procedure obtain query-answering algorithm,(ii) transfer decidability results upper complexity bounds derived previoussections extended formalism.class fulfills desiderata subclass EGDs, call innocuousrelative set TGDs. EGDs enjoy property query answering insensitivethem, provided chase fail. words, = E ,set TGDs, E set EGDs, E innocuous relative , simplyignore EGDs non-failing chase sequence. possible because, intuitively,non-failing sequence generate atom entailed chase(D, ).specifically, start notion innocuous application EGD. Intuitively, making two symbols equal, innocuous EGD application makes atomequal existing atom a0 ; way, consequence EGDapplication, original atom lost, new atom whatsoever introduced.concept innocuous EGD application formally defined follows.Definition 9.5. [Innocuous EGD application] Consider (possibly infinite) non-failingchase sequence = B0 , B1 , . . ., starting database D, respect set =E , set TGDs E set EGDs. say EGD,happlication Bi Bi+1 , E > 0, innocuous Bi+1 Bi .162fiTaming Infinite ChaseNotice innocuousness semantic, syntactic, property. desirableinnocuous EGD applications applications cannot trigger new TGD applications, i.e., TGD applications possible EGD applied.Given might undecidable whether set dependencies certain classguarantees innocuousness EGD applications, one either give direct proofinnocuousness concrete set dependencies, Section 10.2, definesufficient syntactic conditions guarantee innocuousness EGD applicationsentire class dependencies, done, e.g., Cal et al. (2012a).Definition 9.6. Let = E , set TGDs E set EGDs,= E . E innocuous if, every database fairchase sequence respect non-failing, application EGDsequence respect innocuous.Theorem 9.7. Let = E , set TGDs E set EGDsinnocuous . Let database fair chase sequence respectnon-failing. |= Q iff chase(D, ) |= Q.Proof. Consider fair chase sequence B0 , B1 , . . . = B0 presence ,,hiBi Bi+1 > 0 E . Let us define modified chase procedurecall blocking chase, denoted blockchase(D, ). blocking chase uses two sets:set C blocked atoms set (unblocked) atoms A. started database|= E (the case 6|= E possible implies immediate chasefailure), C initialized empty set (C = ) initialized D.initialization, blocking chase attempts apply dependencies E exactlyway standard fair chase sequence, following caveats.trying application hi , hi i:TGD, hi (body(i )) C = , apply hi , hi add new atomgenerated application A.TGD hi (body(i )) C 6= , application hi , hi blocked,nothing done.EGD, application hi , hi proceeds follows. Add Cfacts standard chase disappear step (because Bi Bi1 , dueinnocuousness), i.e., add C set Bi Bi1 . Thus, instead eliminating tuplesA, blocking chase simply bans used putting C.Note that, construction blockchase(D, ), whenever block chase encountersEGD , hi , hi actually applicable, blockchase(D, ) well-defined. Let us use CiAi denote values C step i, respectively. Initially, C0 = A0 =explained before. Observe = C0 C1 C2 = A0 A1 A2monotonically increasing sequences least upper bounds C = Ci = Ai ,respectively. Clearly, (C , ) least fixpoint transformation performedblockchase(D, ) (with respect component-wise set inclusion).Now, let defined = C . definition S, have: |= .Moreover, homomorphism h maps chase(D, ) S. Note hlimit homomorphism sequence h1 , h2 , h3 , . . . (these hi homomorphisms163fiCal, Gottlob & Kiferused computing block chase), defined set pairs (x, y)exists > 0 hi (hi1 ( h1 (x))) = alteredhomomorphism hj j > i. Note every instance B contains D,B |= D. particular, |= D. Putting everything together, conclude |= .also well-known (see Nash et al., 2006) set atoms|= , homomorphism hM hM (chase(D, )) .assume |= Q. |= Q and, chase(D, ), alsochase(D, ) |= Q. Conversely, chase(D, ) |= Q, homomorphism g,g(Q) chase(D, ). Therefore, set atoms |= ,since hM (chase(D, )) , hM (g(Q)) . latter means |= Q.come problem checking, given database set = E ,set WGTGDs E EGDs innocuous , whether fair chase,hsequence (denoted B0 , B1 , . . .) respect fails. Consider application BiBi+1 , E form (X) X = Xk . application causes chasefail, h(X ) h(Xk ) distinct values dom(D). Notice Bj existsj 6 i, exist j > i.Lemma 9.8. Consider database set dependencies = E ,weakly guarded set TGDs E EGDs innocuous .fair chase sequence respect fails iff EGD Eform (X) X = Xk homomorphism h h((X)) chase(D, ),h(X ) 6= h(Xk ), {h(X ), h(Xk )} dom(D).Proof (sketch).If. Let B0 , B1 , . . . fair chase sequence respect . First,difficult show that, since E innocuous relative , failure occurs step,hiEGD applications Bi Bi+1 , E < 1, innocuous(see similar proof Cal, Console, & Frosini, 2013) sequence B0 , . . . , B1 .this, direction follows straightforwardly.if. assumption, fails Bk , k > 1. Since applications innocuousEGDs remove tuples chase, easily seen that, applicable Bk viahomomorphism h, also applicable chase(D, ) via homomorphismh, settles only-if part.Theorem 9.9. Consider database set dependencies = E ,GTGDs (resp., WGTGDs) E EGDs innocuous . Checkingwhether fair chase sequence respect fails decidable,complexity query answering GTGDs (resp., WGTGDs) alone.Proof (sketch). Let neq new binary predicate, serve inequality.extension neq defined dom(D) dom(D) {(d, d) | dom(D)}constructed time quadratic |dom(D)|. Now, every EGD form (X)X1 = X2 , X1 , X2 X, define following Boolean conjunctive query (expressedset atoms): Q = (X) {neq(X1 , X2 )}. Since, construction, new factsform neq(1 , 2 ) introduced chase, immediate see, Lemma 9.8,164fiTaming Infinite Chaseleast one Q positive answer fair chase sequencerespect fails. Theorem 9.7, answering query Q donerespect chase alone, decidable.Let = E theorem, database, let Q query.theorem, check |= Q help following algorithm:1. check whether fair chase sequence respect fails algorithmdescribed Theorem 9.9;2. fair chase sequence respect fails, return true halt;3. |= Q return true; otherwise return false.gives us following corollary:Corollary 9.10. Answering general conjunctive queries weakly guarded sets TGDsinnocuous EGDs ptime reducible answering queries classweakly guarded sets TGDs alone, thus complexity.10. Applicationssection discuss applications results weakly guarded sets TGDsDescription Logic languages object-oriented logic languages.10.1 DL-LiteDL-Lite (Calvanese et al., 2007; Artale et al., 2009) prominent family ontologylanguages tractable query answering. Interestingly, restriction GTGDs calledlinear TGDs (which exactly one body-atom one head-atom) properly extendsDL-Lite languages, shown Cal et al. (2012a). complexity query answeringlinear TGDs lower GTGDs, refer reader work Calet al. (2012a) details.Furthermore, Cal et al. (2012a) also show language GTGDs properly extendsdescription logic EL well extension ELf , allows inverse functionalroles. fact TGDs capture important DL-based ontology languages confirmsTGDs useful tools ontology modeling querying.10.2 F-Logic LiteF-Logic Lite expressive subset F-logic (Kifer et al., 1995), well-known formalismintroduced object-oriented deductive languages. refer reader work CalKifer (2006) details F-Logic Lite. Roughly speaking, compared full FLogic, F-Logic Lite excludes negation default inheritance, allows limitedform cardinality constraints. F-Logic Lite encoded set twelve TGDsEGDs, below, denote FLL :1 : type(O, A, ), data(O, A, V ) member(V, ).2 : sub(C1 , C3 ), sub(C3 , C2 ) sub(C1 , C2 ).3 : member(O, C), sub(C, C1 ) member(O, C1 ).165fiCal, Gottlob & Kifer4 : data(O, A, V ), data(O, A, W ), funct(A, O) V = W .Note EGD axiomatization.5 : mandatory(A, O) V data(O, A, V ).Note TGD existentially quantified variable head.6 : member(O, C), type(C, A, ) type(O, A, ).7 : sub(C, C1 ), type(C1 , A, ) type(C, A, ).8 : type(C, A, T1 ), sub(T1 , ) type(C, A, ).9 : sub(C, C1 ), mandatory(A, C1 ) mandatory(A, C).10 : member(O, C), mandatory(A, C) mandatory(A, O).11 : sub(C, C1 ), funct(A, C1 ) funct(A, C).12 : member(O, C), funct(A, C) funct(A, O).results paper apply set constraints, since FLL weaklyguarded set, single EGD 4 innocuous. innocuousness 4 shownobserving that, whenever EGD applied, turns one atom another; moreover,new data atoms created chase (see rule 5 ) new labeled nulls exactlyposition data[3], symbols equated also reside.prove relevant complexity results. start showing BCQ answeringF-Logic Lite np-complete.Theorem 10.1. Conjunctive query answering F-Logic Lite rules np-hard.Proof (sketch). proof reduction 3-colorability problem. Encodegraph G = (V, E) conjunctive query Q which, edge (vi , vj ) E, two atomsdata(X, Vi , Vj ) data(X, Vj , Vi ), X unique variable. Let database= {data(o, r, g), data(o, g, r), data(o, r, b), data(o, b, r), data(o, g, b), data(o, b, g)}. Then,G three-colorable iff |= Q, case iff FLL |= Q. transformationG (Q, D) obviously polynomial, proves claim.Theorem 10.2. Conjunctive query answering F-Logic Lite rules np.Proof (sketch). mentioned before, ignore EGD FLL since,innocuous, interfere query answering. Let FLL denote set TGDsresulting FLL eliminating rule 4 , i.e., let FLL = FLL {4 }. establishmembership np, sufficient show that: (1) FLL weakly guarded; (2) FLL enjoysPCC (see Definition 7.1). condition, membership npproved exhibiting following. (i) algorithm, analogous Acheck, constructscanonical versions atoms chase clouds (which storedcloud store), polynomial time. algorithm check whether atomic(Boolean) query satisfied atom cloud store. (ii) algorithm, analogousQcheck, guesses (by calling analogous version Tcheck) entire cloudsguessing cloud index (a unique integer) cloud store. algorithmcheck, alternating logarithmic space (alogspace), correctness cloud guess.check, use cloud main atom predecessor configuration.complexity running algorithm shown npalogspace = np.(1) easy: affected positions data[3], member[1], type[1], mandatory[2], funct[2]data[1]. easy see every rule FLL weakly guarded, thus FLLweakly guarded.166fiTaming Infinite Chaselet us sketch (2 ). need show FLL satisfies two conditionsDefinition 7.1. prove first condition holds FLL follows. Let fullFLL =FLL {5 }. full TGDs (no existentially-quantified variables) application alter domain. chase(D, FLL ) = chase(chase(D, fullFLL ), FLL ).fullLet us closer look D+ = chase(D, FLL ). Clearly, dom(D+ ) = dom(D).predicate symbol p, let Rel (p) denote relation consisting p-atoms D+ .Let family relations obtained relations Rel (p)performing arbitrary selection followed projection (we forbid disjunctionsselection predicate). example, let c, dom(D). contain relations1,2 ({1=c} Rel (data)), 2 ({1=d3=c} Rel (data)), on, numbers representattributes selection applied. Given D+ size polynomialmaximum arity relation Rel (p) 3, set size polynomial D+thus polynomial D. shown preserved precise sense,going final result chase(D+ , FLL ): relation Rel (p) corresponding predicate p final chase result, performing selection values outside dom(D)projecting columns used selection, set tuples dom(D)elements result relation . example, v5 labeled null, setdom(D), member(v5 , ) element final result, relation. Similarly, v7 v8 new values, set values A, data(v7 , A, v8 )chase, relation . follows FLL satisfies (2). fact,possible clouds determined polynomially many ways choosing threeelements predicate. proof preservation property doneinduction i-th new labeled null added. Roughly, labeled null, createdrule 5 , analyze sets values (or tuples) attached via rules 4 ,6 , 7 , 8 , 10 , on, conclude sets already presentnext lower level, thus, induction hypothesis, .second condition Definition 7.1 proved similar arguments.Theorems 10.1 10.2 immediately get following result.Corollary 10.3. Conjunctive query answering F-Logic Lite rules np-completegeneral conjunctive queries, ptime fixed-size atomic conjunctive queries.11. Conclusions Related Workpaper identified large non-trivial class tuple-generating equalitygenerating dependencies problems conjunctive query containment answering decidable, provided relevant complexity results. Applicationsresults span databases knowledge representation. particular, shownclass constraints subsumes classical work Johnson Klug (1984) wellrecent results Cal Kifer (2006). Moreover, able capture relevantontology formalisms Description Logics (DL) family, particular DL-Lite EL.problem query containment non-terminating chase addresseddatabase context Johnson Klug (1984), ontological theory contains inclusion dependencies key dependencies particular form. introductionDL-Lite family description logics works Calvanese et al. (2007) Artale et al.167fiCal, Gottlob & Kifer(2009) significant leap forward ontological query answering due expressiveness DL-Lite languages tractable data complexity. Conjunctive query answeringDL-Lite advantage first-order rewritable, i.e., pair hQ, i, QCQ DL-Lite ontology (TBox), rewritten first-order query Qthat, every database (ABox) D, answer Q logical theorycoincides answer Q D. Since first-order query writtenSQL, practical terms means pair hQ, rewritten SQL queryoriginal database D.Rewritability widely adopted ontology querying. works Cal, Calvanese,De Giacomo, Lenzerini (2001), Cal, Lembo, Rosati (2003b) present queryrewriting techniques deal Entity-Relationship schemata inclusion dependencies, respectively. work Perez-Urbina, Motik, Horrocks (2010) presents Datalog rewriting algorithm expressive DL ELHIO , comprises limited formconcept role negation, role inclusion, inverse roles, nominals, i.e., conceptsinterpreted singletons. Conjunctive query answering ELHIO ptime-completedata complexity, proposed algorithm also optimal ontology languagesDL-Lite. Optimizations rewriting linear TGDs (TGDs exactly oneatom body) presented Gottlob, Orsi, Pieris (2011), Orsi Pieris(2011). Gottlob Schwentick (2012) showed rewriting conjunctive queryset linear TGDs polynomial size query TGD set.rewriting techniques ptime-complete languages (in data complexity)proposed description logic EL (Rosati, 2007; Lutz, Toman, & Wolter, 2009; Krotzsch& Rudolph, 2007). Another approach worth mentioning combination rewritingchase (see Kontchakov, Lutz, Toman, Wolter, & Zakharyaschev, 2010); techniqueintroduced DL-Lite order tackle performance problems ariserewriting according ontology large.Recent works concentrate semantic characterization sets TGDsquery answering decidable (Baget et al., 2011a). notion first-order rewritabilitytightly connected finite unification set (FUS). FUS semantically characterizedset TGDs enjoy following property: every conjunctive query Q,rewriting Q Q obtained backward-chaining unification, accordingrules , terminates. Another semantic characterization TGDs boundedtreewidth set (BTS), i.e., set TGDs chase TGDs boundedtreewidth. seen Section 3, every weakly guarded set TGDs BTS. finiteexpansion set (FES) set TGDs guarantees, every database, terminationrestricted chase, therefore decidability query answering.Datalog family (Cal et al., 2011) proposed purpose providingtractable query answering algorithms general ontology languages. Datalog ,fundamental constraints TGDs EGDs. Clearly, TGDs extension Datalogrules. absence value invention (existential quantification head), thoroughlydiscussed Patel-Schneider Horrocks (2007), main shortcoming plain Datalogmodeling ontologies even conceptual data formalisms Entity-Relationshipmodel (Chen, 1976). Sets GTGDs WGTGDs Datalog ontologies. Dataloglanguages easily extend common tractable ontology languages; particular,168fiTaming Infinite Chasemain DL-Lite languages (see Cal et al., 2012a). fundamental decidability paradigmsDatalog family following:Chase termination. chase terminates, finite instance produced; obviously, Theorem 2.10, query answering case decidable. notablesyntactic restriction guaranteeing chase termination weak acyclicity TGDs,refer reader milestone paper Fagin et al. (2005). generalsyntactic restrictions studied Deutsch, Nash, Remmel (2008), Marnette(2009), Greco, Spezzano, Trubitsyna (2011), Baget et al. (2011a), Grau,Horrocks, Krotzsch, Kupke, Magka, Motik, Wang (2012). semantic propertyTGDs, called parsimony, introduced Leone, Manna, Terracina, Veltri(2012). Parsimony ensures decidability query answering termination specialversion chase, called parsimonious chase.Guardedness. paradigm studied paper. thorough studydata complexity query answering GTGDs linear TGDs, subsetguarded class, found work Cal et al. (2012a). interestingclasses frontier guarded (FGTGDs) weakly frontier-guarded TGDs (WFGTGDs) considered studied Baget et al. (2011a), Baget, Mugnier, Rudolph,Thomazo (2011b), Krotzsch Rudolph (2011). idea underlyingclasses that, obtain decidability, sufficient guard frontier variables,is, variables occur body head rule.8 WFGTGDs syntactically liberal succinct WGTGDs, conjunctive query answering WFGTGDs computationally expensive casebounded arities. seen querying WFGTGDs expressivequerying WGTGDs. fact, every WFGTGD set CQ Q,exists WGTGD set CQ Q every database D, |= Q iff|= Q . generalization WFGTGDs, called greedy bounded-treewidth TGDs,proposed Baget et al. (2011b), together complexity analysis. guardedness paradigm combined acyclicity Krotzsch Rudolph (2011),generalization WFGTGDs weakly acyclic TGDs proposed.Stickiness. class sticky sets TGDs (or sticky Datalog , see Cal et al., 2012b)defined means syntactic restriction rule bodies, ensuresticky set TGDs first-order rewritable, FUS, according Baget et al.(2011a). Civili Rosati (2012) proposed extension sticky sets TGDs.interaction equality generating dependencies TGDs subject several works, starting work Johnson Klug (1984), dealsfunctional inclusion dependencies, proposing class inclusion dependencies calledkey-based, which, intuitively, interaction key dependencies thanks syntacticrestrictions. absence interaction EGDs TGDs captured notionseparability, first introduced Cal et al. (2003a) key inclusion dependencies,also adopted, though sometimes explicitly stated, instance, Cal, Gottlob,Pieris (2012a), Artale et al. (2009) Calvanese et al. (2007)see work Cal,Gottlob, Orsi, Pieris (2012b) survey topic.8. FGTGDs independently discovered Mantas Simkus working doctoral thesis.169fiCal, Gottlob & Kifershown Cal et al. (2012a), stratified negation added straightforwardlyDatalog . recently, guarded Datalog extended two versions well-foundednegation (see Gottlob, Hernich, Kupke, & Lukasiewicz, 2012; Hernich, Kupke, Lukasiewicz,& Gottlob, 2013).ontological query answering, normally finite infinite models theoriesconsidered. cases, restricting attention finite solutions (models)always equivalent general approach. property equivalence queryanswering finite models query answering arbitrary models (finiteinfinite) called finite controllability, proved restricted classes functionalinclusion dependencies Johnson Klug (1984). Finite controllability provedclass arbitrary inclusion dependencies pioneering work Rosati (2011).even general result appears work Barany et al. (2010), shownfinite controllability holds guarded theories.related previous approach guarded logic programming guarded open answer setprogramming (Heymans, Nieuwenborgh, & Vermeir, 2005). easy see setGTGDs interpreted guarded answer set program, defined Heymans et al.(2005), guarded answer set programs expressive GTGDsallow negation.Implementations ontology-based data access systems take advantage query answering techniques tractable ontologies; particular, mention DLV (Leone et al., 2012),Mastro (Savo, Lembo, Lenzerini, Poggi, Rodriguez-Muro, Romagnoli, Ruzzi, & Stella, 2010)NYAYA (De Virgilio, Orsi, Tanca, & Torlone, 2012).Acknowledgmentsextended version results authors, published KR 2008 Conference DL 2008 Workshop. Andrea Cal Georg Gottlob also affiliatedOxford-Man Institute Quantitative Finance, University Oxford, UK. AndreaCal acknowledges support EPSRC project Logic-based Integration QueryingUnindexed Data (EP/E010865/1). Georg Gottlob acknowledges funding European Research Council European Communitys Seventh Framework Program(FP7/2007-2013) / ERC grant agreement DIADEM no. 246858. Michael Kifer partially supported NSF grant 0964196. authors grateful Andreas Pieris,Marco Manna, Michael Morak anonymous reviewers valuable commentssuggestions improve paper.ReferencesAbiteboul, S., Hull, R., & Vianu, V. (1995). Foundations Databases. Addison-Wesley.Adler, I., Gottlob, G., & Grohe, M. (2007). Hypertree width related hypergraph invariants. Eur. Journal Combinatorics, 28 (8), 21672181.Aho, A., Sagiv, Y., & Ullman, J. D. (1979). Equivalence relational expressions. SIAMJournal Computing, 8 (2), 218246.Arenas, M., Bertossi, L. E., & Chomicki, J. (1999). Consistent query answers inconsistentdatabases. Proc PODS 1999, pp. 6879.170fiTaming Infinite ChaseArtale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). DL-lite familyrelations. J. Artif. Intell. Res., 36, 169.Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Proc. IJCAI 2005,pp. 364369.Baget, J.-F., Leclere, M., Mugnier, M.-L., & Salvat, E. (2011a). rules existentialvariables: Walking decidability line. Artif. Intell., 175 (910), 16201654.Baget, J.-F., Mugnier, M.-L., Rudolph, S., & Thomazo, M. (2011b). Walking complexitylines generalized guarded existential rules. Proc. IJCAI 2011, pp. 712717.Barany, V., Gottlob, G., & Otto, M. (2010). Querying guarded fragment. Proc.LICS 2010, pp. 110.Beeri, C., Fagin, R., Maier, D., Mendelzon, A. O., Ullman, J. D., & Yannakakis, M. (1981).Properties acyclic database schemes. Proc. STOC 1981, pp. 355362.Beeri, C., & Vardi, M. Y. (1981). implication problem data dependencies.Proc. ICALP 1981, pp. 7385.Bourhis, P., Morak, M., & Pieris, A. (2013). impact disjunction query answeringguarded-based existential rules. Proc. IJCAI 2013.Cabibbo, L. (1998). expressive power stratified logic programs value invention.Inf. Comput., 147 (1), 2256.Cal, A., Calvanese, D., De Giacomo, G., & Lenzerini, M. (2001). Accessing data integrationsystems conceptual schemas. Proc. ER 2001, pp. 270284.Cal, A., Console, M., & Frosini, R. (2013). separability ontological constraints.Forthcoming.Cal, A., Gottlob, G., & Kifer, M. (2008). Taming infinite chase: Query answeringexpressive relational constraints. Proc. KR 2008, pp. 7080.Cal, A., Gottlob, G., & Lukasiewicz, T. (2009). general datalog-based frameworktractable query answering ontologies. Proc. PODS 2009, pp. 7786.Cal, A., Gottlob, G., & Lukasiewicz, T. (2012a). general datalog-based frameworktractable query answering ontologies. J. Web Semantics, 14, 5783. Extendedversion (Cal, Gottlob, & Lukasiewicz, 2009).Cal, A., Gottlob, G., Orsi, G., & Pieris, A. (2012b). interaction existential rulesequality constraints ontology querying. Proc. Correct Reasoning 2012,pp. 117133.Cal, A., Gottlob, G., & Pieris, A. (2011). New expressive languages ontological queryanswering. Proc. AAAI 2011.Cal, A., Gottlob, G., & Pieris, A. (2012a). Ontological query answering expressiveentity-relationship schemata. Inf. Syst., 37 (4), 320335.Cal, A., Gottlob, G., & Pieris, A. (2012b). Towards expressive ontology languages:query answering problem. Artif. Intell., 193, 87128.Cal, A., & Kifer, M. (2006). Containment conjunctive object meta-queries. Proc.VLDB 2006, pp. 942952.171fiCal, Gottlob & KiferCal, A., Lembo, D., & Rosati, R. (2003a). decidability complexity queryanswering inconsistent incomplete databases. PODS 2003, pp. 260271.Cal, A., Lembo, D., & Rosati, R. (2003b). Query rewriting answering constraintsdata integration systems. Proc. IJCAI 2003, pp. 1621.Cal, A., & Martinenghi, D. (2010). Querying incomplete data extended er schemata.TPLP, 10 (3), 291329.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractablereasoning efficient query answering description logics: DL-lite family. J.Autom. Reasoning, 39 (3), 385429.Calvanese, D., De Giacomo, G., & Lenzerini, M. (2002). Description logics informationintegration. Computational Logic: Logic Programming Beyond, Vol. 2408LNCS, pp. 4160. Springer.Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998). decidability querycontainment constraints. Proc. PODS 1998, pp. 149158.Chandra, A. K., Kozen, D., & Stockmeyer, L. J. (1981a). Alternation. J. ACM,28 (1), 114133.Chandra, A. K., Lewis, H. R., & Makowsky, J. A. (1981b). Embedded implicational dependencies inference problem. Proc. STOC 1981, pp. 342354.Chandra, A. K., & Merlin, P. M. (1977). Optimal implementation conjunctive queriesrelational data bases. Proc. STOC 1977, pp. 7790.Chandra, A. K., & Vardi, M. Y. (1985). implication problem functional inclusiondependencies undecidable. SIAM J. Comput., 14, 671677.Chen, P. P. (1976). entity-relationship model - toward unified view data. Trans.Database Syst., 1 (1), 936.Civili, C., & Rosati, R. (2012). broad class first-order rewritable tuple-generatingdependencies. Proc. Datalog 2.0 2012, pp. 6880.Courcelle, B. (1990). monadic second-order logic graphs. I. recognizable sets finitegraphs. Information Computation, 85 (1), 1275.Dantsin, E., Eiter, T., Gottlob, G., & Voronkov, A. (2001). Complexity expressivepower logic programming. ACM Computing Surveys, 33 (3), 374425.De Virgilio, R., Orsi, G., Tanca, L., & Torlone, R. (2012). NYAYA: system supportinguniform management large sets semantic data. Proc. ICDE 2012, pp.13091312.Deutsch, A., Nash, A., & Remmel, J. B. (2008). chase revisited. Proc. PODS 2008,pp. 149158.Fagin, R. (1983). Degrees acyclicity hypergraphs relational database schemes.J. ACM, 30 (3), 514550.Fagin, R., Kolaitis, P. G., Miller, R. J., & Popa, L. (2005). Data exchange: semanticsquery answering. Theor. Comput. Sci., 336 (1), 89124.172fiTaming Infinite ChaseGoncalves, M. E., & Gradel, E. (2000). Decidability issues action guarded logics.Proc. DL 2000, pp. 123132.Gottlob, G., Hernich, A., Kupke, C., & Lukasiewicz, T. (2012). Equality-friendly wellfounded semantics applications description logics. Proc. AAAI 2012.Gottlob, G., Leone, N., & Scarcello, F. (2001). Hypertree decompositions: survey.Proc. MFCS 2001, pp. 3757.Gottlob, G., Leone, N., & Scarcello, F. (2002). Hypertree decompositions tractablequeries. J. Comp. Syst. Sci., 64 (3).Gottlob, G., Leone, N., & Scarcello, F. (2003). Robbers, marshals, guards: game theoretic logical characterizations hypertree width. J. Comput. Syst. Sci., 66 (4),775808.Gottlob, G., Manna, M., & Pieris, A. (2013a). Combining decidability paradigms existential rules. appear TPLP.Gottlob, G., Manna, M., & Pieris, A. (2013b). Querying hybrid fragments existentialrules. Forthcoming.Gottlob, G., & Nash, A. (2006). Data exchange: computing cores polynomial time.Proc. PODS 2006, pp. 4049.Gottlob, G., Orsi, G., & Pieris, A. (2011). Ontological queries: Rewriting optimization.Proc. ICDE 2011, pp. 213.Gottlob, G., & Schwentick, T. (2012). Rewriting ontological queries small nonrecursivedatalog programs. Proc. KR 2012.Gradel, E. (1999). restraining power guards. J. Symb. Log., 64 (4), 17191742.Grau, B. C., Horrocks, I., Krotzsch, M., Kupke, C., Magka, D., Motik, B., & Wang, Z.(2012). Acyclicity conditions application query answering descriptionlogics. Proc. KR 2012.Greco, S., Spezzano, F., & Trubitsyna, I. (2011). Stratification criteria rewriting techniques checking chase termination. PVLDB, 4 (11), 11581168.Hernich, A., Kupke, C., Lukasiewicz, T., & Gottlob, G. (2013). Well-founded semanticsextended datalog ontological reasoning. Proc. PODS 2013, pp. 225236.Hernich, A., Libkin, L., & Schweikardt, N. (2011). Closed world data exchange. ACMTrans. Database Syst., 36 (2), 1453.Heymans, S., Nieuwenborgh, D. V., & Vermeir, D. (2005). Guarded open answer set programming. Proc. LPNMR 2005.Johnson, D. S., & Klug, A. (1984). Testing containment conjunctive queriesfunctional inclusion dependencies. J. Comp. Syst. Sci., 28, 167189.Kifer, M., Lausen, G., & Wu, J. (1995). Logical foundations object-oriented framebased languages. J. ACM, 42, 741843.Koch, C. (2002). Query rewriting symmetric constraints. Proc. FoIKS 2002, pp.130147.173fiCal, Gottlob & KiferKontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2010). combined approach query answering dl-lite. Proc. KR 2010.Krotzsch, M., & Rudolph, S. (2007). Conjunctive queries EL composition roles.Proc. DL 2007.Krotzsch, M., & Rudolph, S. (2011). Extending decidable existential rules joining acyclicity guardedness. Proc. IJCAI 2011, pp. 963968.Leone, N., Manna, M., Terracina, G., & Veltri, P. (2012). Efficiently computable datalog;programs. Proc. KR 2012.Li, L., & Horrocks, I. (2003). software framework matchmaking based semanticweb technology. Proc. WWW 2003.Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering descriptionlogic EL using relational database system. Proc. IJCAI 2009, pp. 20702075.Maier, D., Mendelzon, A. O., & Sagiv, Y. (1979). Testing implications data dependencies.Trans. Database Syst., 4 (4), 455469.Mailharrow, D. (1998). classification constraint-based framework configuration.Artif. Intell. Eng. Design, Anal. Manuf., 12 (4), 383397.Marnette, B. (2009). Generalized schema-mappings: termination tractability.Proc. PODS 2009, pp. 1322.Millstein, T., Levy, A., & Friedman, M. (2000). Query containment data integrationsystems. PODS 2000.Mitchell, J. C. (1983). implication problem functional inclusion dependencies.Inf. Control, 56, 154173.Nash, A., Deutsch, A., & Remmel, J. (2006). Data exchange, data integration, chase.Tech. rep. CS2006-0859, UCSD.Orsi, G., & Pieris, A. (2011). Optimizing query answering ontological constraints.PVLDB, 4 (11), 10041015.Patel-Schneider, P. F., & Horrocks, I. (2007). comparison two modelling paradigmssemantic web. J. Web Semantics, 5 (4), 240250.Perez-Urbina, H., Motik, B., & Horrocks, I. (2010). Tractable query answering rewritingdescription logic constraints. J. Appl. Logic, 8 (2), 186209.Qian, X. (1996). Query folding. Proc. ICDE 1996, pp. 4855.Rabin, M. O. (1969). Decidability second-order theories automata infinite trees.Trans. Am. Math. Soc., 141 (135), 4.Rosati, R. (2007). conjunctive query answering EL. Proc. DL 2007.Rosati, R. (2011). finite controllability conjunctive query answering databasesopen-world assumption. J. Comput. Syst. Sci., 77 (3), 572594.Savo, D. F., Lembo, D., Lenzerini, M., Poggi, A., Rodriguez-Muro, M., Romagnoli, V.,Ruzzi, M., & Stella, G. (2010). Mastro work: Experiences ontology-based dataaccess. Proc. Description Logics.174fiJournal Artificial Intelligence Research 48 (2013) 23-65Submitted 04/13; published 10/13Learning Optimal Bayesian Networks:Shortest Path PerspectiveChanghe Yuanchanghe.yuan@qc.cuny.eduDepartment Computer ScienceQueens College/City University New YorkQueens, NY 11367 USABrandon Malonebrandon.malone@cs.helsinki.fiDepartment Computer ScienceHelsinki Institute Information TechnologyFin-00014 University Helsinki, FinlandAbstractpaper, learning Bayesian network structure optimizes scoring functiongiven dataset viewed shortest path problem implicit state-space searchgraph. perspective highlights importance two research issues: developmentsearch strategies solving shortest path problem, design heuristic functions guiding search. paper introduces several techniques addressingissues. One A* search algorithm learns optimal Bayesian network structuresearching promising part solution space. others mainlytwo heuristic functions. first heuristic function represents simple relaxationacyclicity constraint Bayesian network. Although admissible consistent, heuristic may introduce much relaxation result loose bound. second heuristicfunction reduces amount relaxation avoiding directed cycles within groupsvariables. Empirical results show methods constitute promising approachlearning optimal Bayesian network structures.1. IntroductionBayesian networks graphical models represent uncertain relationsrandom variables domain compactly intuitively. Bayesian network directedacyclic graph nodes represent random variables, arcs lackrepresent dependence/conditional independence relations variables.relations quantified set conditional probability distributions, onevariable conditioning parents. Overall, Bayesian network represents jointprobability distribution variables.Applying Bayesian networks real-world problems typically requires building graphicalrepresentations problems. One popular approach use score-based methodsfind high-scoring structures given dataset (Cooper & Herskovits, 1992; Heckerman,1998). Score-based learning shown NP-hard, however (Chickering, 1996).Due complexity, early research area mainly focused developing approximation algorithms greedy hill climbing approaches (Heckerman, 1998; Bouckaert,1994; Chickering, 1995; Friedman, Nachman, & Peer, 1999). Unfortunately solutionsfound methods unknown quality. recent years, several exact learning algoc2013AI Access Foundation. rights reserved.fiYuan & Malonerithms developed based dynamic programming (Koivisto & Sood, 2004; Ott,Imoto, & Miyano, 2004; Silander & Myllymaki, 2006; Singh & Moore, 2005), branchbound (de Campos & Ji, 2011), integer linear programming (Cussens, 2011; Jaakkola,Sontag, Globerson, & Meila, 2010; Hemmecke, Lindner, & Studeny, 2012). methodsguaranteed find optimal solutions able finish successfully. However,efficiency scalability leave much room improvement.paper, view problem learning Bayesian network structure optimizes scoring function given dataset shortest path problem. idearepresent solution space learning problem implicit state-space search graph,shortest path start goal nodes graph correspondsoptimal Bayesian network. perspective highlights importance two orthogonalresearch issues: development search strategies solving shortest path problem,design admissible heuristic functions guiding search. present severaltechniques address issues. Firstly, A* search algorithm developed learnoptimal Bayesian network focusing searching promising parts solutionspace. Secondly, two heuristic functions introduced guide search. tightnessheuristic determines efficiency search algorithm. first heuristic represents simple relaxation acyclicity constraint Bayesian networksvariable chooses optimal parents independently. result, heuristic estimate maycontain many directed cycles result loose bound. second heuristic, namedk-cycle conflict heuristic, based form relaxation tightens boundavoiding directed cycles within groups variables. Finally, traversingsearch graph, need calculate cost arc visited, correspondsselecting optimal parents variable candidate set. present two datastructures storing querying costs candidate parent sets. One setfull exponential-size data structures called parent graphs stored hash tablesanswer query constant time. sparse representation parentgraph stores optimal parent sets improve space efficiency.empirically evaluated A* algorithm empowered different combinationsheuristic functions parent graph representations set UCI machine learningdatasets. results show even simple heuristic full parent graph representation, A* often achieve better efficiency and/or scalability existing approacheslearning optimal Bayesian networks. k-cycle conflict heuristic sparse parentgraph representation enabled algorithm achieve even greater efficiencyscalability. results indicate proposed methods constitute promising approachlearning optimal Bayesian network structures.remainder paper structured follows. Section 2 reviews problemlearning optimal Bayesian networks reviews related work. Section 3 introducesshortest path perspective learning problem. formulation search graphdiscussed detail. Section 4 introduces two data structures developed computestore optimal parent sets pairs variables candidate sets. data structures used query cost arc search graph. Section 5 presentsA* search algorithm. developed two heuristic functions guiding algorithmstudied theoretical properties. Section 6 presents empirical results evaluatingalgorithm several existing approaches. Finally, Section 7 concludes paper.24fiLearning Optimal Bayesian Networks2. Backgroundfirst provide brief summary related work learning Bayesian networks.2.1 Learning Bayesian Network StructuresBayesian network directed acyclic graph (DAG) G represents joint probabilitydistribution set random variables V = {X1 , X2 , ..., Xn }. directed arc XiXj represents dependence two variables; say Xi parent Xj .use PAj stand parent set Xj . dependence relation Xj PAjquantified using conditional probability distribution, P (Xj |PAj ). joint probabilitydistribution represented G factorized productQ conditional probabilitydistributions network, i.e., P (X1 , ..., Xn ) = ni=1 P (Xi |PAi ). additioncompact representation, Bayesian networks also provide principled approaches solvingvarious inference tasks, including belief updating, probable explanation, maximumPosteriori assignment (Pearl, 1988), relevant explanation (Yuan, Liu, Lu, & Lim,2009; Yuan, Lim, & Littman, 2011a; Yuan, Lim, & Lu, 2011b).Given dataset = {D1 , ..., DN }, data point Di vector valuesvariables V, learning Bayesian network task finding network structurebest fits D. work, assume variable discrete finite numberpossible values, data point missing values.roughly three main approaches learning problem: score-based learning,constraint-based learning, hybrid methods. Score-based learning methods evaluatequality Bayesian network structures using scoring function selects onebest score (Cooper & Herskovits, 1992; Heckerman, 1998). methods basicallyformulate learning problem combinatorial optimization problem. work welldatasets many variables, may fail find optimal solutions largedatasets. discuss approach detail next section,approach take. Constraint-based learning methods typically use statistical testingsidentify conditional independence relations data build Bayesian networkstructure best fits independence relations (Pearl, 1988; Spirtes, Glymour, &Scheines, 2000; Cheng, Greiner, Kelly, Bell, & Liu, 2002; de Campos & Huete, 2000; Xie &Geng, 2008). Constraint-based methods mostly rely results local statistical testings,often scale large datasets. However, sensitive accuracystatistical testings may work well insufficient noisy data.comparison, score-based methods work well even datasets relatively datapoints. Hybrid methods aim integrate advantages previous two approachesuse combinations constraint-based and/or score-based methods solving learningproblem (Dash & Druzdzel, 1999; Acid & de Campos, 2001; Tsamardinos, Brown, & Aliferis,2006; Perrier, Imoto, & Miyano, 2008). One popular strategy use constraint-basedlearning create skeleton graph use score-based learning find high-scoringnetwork structure subgraph skeleton (Tsamardinos et al., 2006; Perrier et al.,2008). work, consider Bayesian model averaging methods aimestimate posterior probabilities structural features edges rather modelselection (Heckerman, 1998; Friedman & Koller, 2003; Dash & Cooper, 2004).25fiYuan & Malone2.2 Score-Based LearningScore-based learning methods rely scoring function Score(.) evaluating qualityBayesian network structure. search strategy used find structure G optimizesscore. Therefore, score-based methods two major elements, scoring functionssearch strategies.2.2.1 Scoring FunctionsMany scoring functions used measure quality network structure.Bayesian scoring functions define posterior probability distributionnetwork structures conditioning data, structure highestposterior probability presumably best structure. scoring functions bestrepresented Bayesian Dirichlet score (BD) (Heckerman, Geiger, & Chickering, 1995)variations, e.g., K2 (Cooper & Herskovits, 1992), Bayesian Dirichlet scorescore equivalence (BDe) (Heckerman et al., 1995), Bayesian Dirichlet score scoreequivalence uniform priors (BDeu) (Buntine, 1991). scoring functions oftenform trading goodness fit structure data complexitystructure. goodness fit measured likelihood structure givendata amount information compressed structure data.Scoring functions belonging category include minimum description length (MDL)(or equivalently Bayesian information criterion, BIC) (Rissanen, 1978; Suzuki, 1996; Lam& Bacchus, 1994), Akaike information criterion (AIC) (Akaike, 1973; Bozdogan, 1987),(factorized) normalized maximum likelihood function (NML/fNML) (Silander, Roos, Kontkanen, & Myllymaki, 2008), mutual information tests score (MIT) (de Campos,2006). scoring functions decomposable, is, score networkdecomposed sum node scores (Heckerman, 1998).optimal structure G may unique multiple Bayesian network structures may share optimal score1 . Two network structures said belongequivalence class (Chickering, 1995) represent set probability distributions possible parameterizations. Score-equivalent scoring functions assignscore structures equivalence class. scoring functionsscore equivalent.mainly use MDL score work. Let ri number states Xi , Npainumber data points consistent PAi = pai , Nxi ,pai number datapoints constrained Xi = xi . MDL defined follows (Lam & Bacchus, 1994).DL(G) =XDL(Xi |PAi ),1. often use optimal instead optimal throughout paper.26(1)fiLearning Optimal Bayesian Networkslog NK(Xi |PAi ),2XNxi ,paiH(Xi |PAi ) =Nxi ,pai log,Npaixi ,paiK(Xi |PAi ) = (ri 1)rl .Xl PAiDL(Xi |PAi ) = H(Xi |PAi ) +(2)(3)(4)goal find Bayesian network minimum MDL score. However,methods means restricted MDL; decomposable scoring function,BIC, BDeu, fNML, used instead without affecting search strategy.demonstrate that, test BDeu experimental section. One slight differenceMDL scoring functions latter scores need maximizedorder find optimal solution. rather straightforward translatemaximization minimization problems simply changing sign scores. Also,sometimes use costs refer scores, also represent distancesnodes search graph.2.2.2 Local Search StrategiesGiven n variables, O(n2n(n1) ) directed acyclic graphs (DAGs). sizesolution space grows exponentially number variables. surprisingscore-based structure learning shown NP-hard (Chickering, 1996). Duecomplexity, early research focused mainly developing approximation algorithms (Heckerman, 1998; Bouckaert, 1994). Popular search strategies used include greedy hillclimbing, stochastic search, genetic algorithm, etc..Greedy hill climbing methods typically begin initial network, e.g., emptynetwork randomly generated structure, repeatedly apply single edge operations,including addition, deletion, reversal, finding locally optimal network. Extensions approach include tabu search random restarts (Glover, 1990), limitingnumber parents parameters variable (Friedman et al., 1999), searchingspace equivalence classes (Chickering, 2002), searching space variableorderings (Teyssier & Koller, 2005), searching constraints extracteddata (Tsamardinos et al., 2006). optimal reinsertion algorithm (OR) (Moore & Wong,2003) adds different operator: variable removed network, optimal parentsselected, variable reinserted network parents.parents selected ensure new network still valid Bayesian network.Stochastic search methods Markov Chain Monte Carlo simulated annealingalso applied find high-scoring structure (Heckerman, 1998; de Campos &Puerta, 2001; Myers, Laskey, & Levitt, 1999). methods explore solution spaceusing non-deterministic transitions neighboring network structures favoringbetter solutions. stochastic moves used hope escape local optima findbetter solutions.optimization methods genetic algorithms (Hsu, Guo, Perry, & Stilson,2002; Larranaga, Kuijpers, Murga, & Yurramendi, 1996) ant colony optimization meth27fiYuan & Maloneods (de Campos, Fernndez-Luna, Gmez, & Puerta, 2002; Daly & Shen, 2009)applied learning Bayesian network structures well. Unlike previous methodswork one solution time, population-based methods maintain set candidate solutions throughout search. step, create next generationsolutions randomly reassembling current solutions genetic algorithms,generating new solutions based information collected incumbent solutionsant colony optimization. hope obtain increasingly better populations solutionseventually find good network structure.local search methods quite robust face large learning problemsmany variables. However, guarantee find optimal solution. worse,quality solutions typically unknown.2.2.3 Optimal Search StrategiesRecently multiple exact algorithms developed learning optimal Bayesian networks. Several dynamic programming algorithms proposed based observationBayesian network least one leaf (Ott et al., 2004; Singh & Moore, 2005).leaf variable child variables Bayesian network. order find optimalBayesian network set variables V, sufficient find best leaf. leafchoice X, best possible Bayesian network constructed letting X choose optimalparent set PAX V\{X} letting V\{X} form optimal subnetwork.best leaf choice one minimizes sum Score(X, PAX ) Score(V\{X})scoring function Score(.). formally, have:Score(V) = min {Score(V \ {X}) + BestScore(X, V \ {X})},XV(5)BestScore(X, V \ {X}) =minScore(X, PAX ).PAX V\{X}(6)Given recurrence relation, dynamic programming algorithm works follows. first finds optimal structures single variables, trivial. Startingbase cases, algorithm builds optimal subnetworks increasingly larger variablesets optimal network found V. dynamic programming algorithmsfind optimal Bayesian network O(n2n ) time space (Koivisto & Sood, 2004; Ottet al., 2004; Silander & Myllymaki, 2006; Singh & Moore, 2005). Recent algorithmsimproved memory complexity either trading longer running times reduced memory consumption (Parviainen & Koivisto, 2009) taking advantage layered structurepresent within dynamic programming lattice (Malone, Yuan, & Hansen, 2011b; Malone,Yuan, Hansen, & Bridges, 2011a).branch bound algorithm (BB) proposed de Campos Ji (2011)learning Bayesian networks. algorithm first creates cyclic graph allowingvariable obtain optimal parents variables. best-first search strategyused break cycles removing one edge time. algorithm usesapproximation algorithm estimate initial upper bound solution pruning.algorithm also occasionally expands worst nodes search frontier hope find28fiLearning Optimal Bayesian NetworksFigure 1: order graph four variables.better networks update upper bound. completion, algorithm finds optimalnetwork structure subgraph initial cyclic graph. algorithm ranmemory finding solution, switch using depth-first search strategyfind suboptimal solution.Integer linear programming (ILP) also used learn optimal Bayesian networkstructures (Cussens, 2011; Jaakkola et al., 2010). learning problem cast integerlinear program polytope exponential number facets. outer boundapproximation polytope solved. solution relaxed problemintegral, guaranteed optimal structure. Otherwise, cutting planes branchbound algorithms subsequently applied find optimal structure. Recentlysimilar method proposed find optimal structure searching spaceequivalence classes (Hemmecke et al., 2012).Several methods considered optimal constraints enforcenetwork structure. example, optimal parents selected variable, K2finds optimal network structure particular variable ordering (Cooper & Herskovits,1992). methods developed (Ordyniak & Szeider, 2010; Kojima, Perrier, Imoto, &Miyano, 2010) find optimal network structure must subgraph given supergraph.3. Shortest Path Perspectivesection introduces shortest path perspective problem learning Bayesiannetwork structure given dataset.3.1 Order Graphstate space graph learning Bayesian networks basically Hasse diagram containingsubsets variables domain. Figure 1 visualizes state space graphlearning problem four variables. top-most node empty set layer29fiYuan & Malone0 start search node, bottom-most node complete set layer ngoal node, n number variables domain. arc U U {X}represents generating successor node adding new variable {X} existing setvariables U; U called predecessor U {X}. cost arc equal scoreselecting optimal parent set X U, i.e., BestScore(X, U). example, arc{X1 , X2 } {X1 , X2 , X3 } cost equal BestScore(X3 , {X1 , X2 }). node layerni successors many ways add new variable, predecessorsmany leaf choices. define expanding node U generating successorsnodes U.search graph thus defined, path start node goal node definedsequence nodes arc nodes next nodesequence. path also corresponds ordering variables orderappearance. example, path traversing nodes , {X1 }, {X1 , X2 }, {X1 , X2 , X3 },{X1 , X2 , X3 , X4 } stands variable ordering X1 , X2 , X3 , X4 . also callsearch graph order graph. cost path defined sum costsarcs path. shortest path path minimum total costorder graph.Given shortest path, reconstruct Bayesian network structure notingarc path encodes choice optimal parents one variablespreceding variables, complete path represents orderingvariables. Therefore, putting together optimal parent choices generates validBayesian network. construction, Bayesian network structure optimal.3.2 Finding Shortest PathVarious methods applied solve shortest path problem. Dynamic programmingconsidered evaluate order graph using top sweep order graph (Silander& Myllymaki, 2006; Malone et al., 2011b). Layer layer, dynamic programming findsoptimal subnetwork variables contained node order graph basedresults previous layers. example, three ways construct Bayesiannetwork node {X1 , X2 , X3 }: using {X2 , X3 } subnetwork X1 leaf, using{X1 , X3 } subnetwork X2 leaf, using {X1 , X2 } subnetwork X3leaf. top-down sweep makes sure optimal subnetworks already found{X2 , X3 }, {X1 , X3 }, {X1 , X2 }. need select optimal parentsleaves identify leaf produces optimal network {X1 , X2 , X3 }.evaluation reaches node last layer, shortest path and, equivalently, optimalBayesian network found global variable set.drawback dynamic programming approach need computeBestScore(.) candidate parent sets variable. n variables,2n nodes order graph, also 2n1 parent scores computedvariable, totally n2n1 scores. number variables increases, computing storingorder parent graphs quickly becomes infeasible.paper, propose apply A* algorithm (Hart, Nilsson, & Raphael, 1968)solve shortest path problem. A* uses heuristic function evaluate qualitysearch nodes expand promising search node search step.30fiLearning Optimal Bayesian Networksguidance heuristic functions, A* needs explore part searchgraph finding optimal solution. However, comparison dynamic programming,A* overhead calculating heuristic values maintaining priority queue.actual relative performance dynamic programming A* thus dependsefficiency calculating heuristic values tightness values (Felzenszwalb& McAllester, 2007; Klein & Manning, 2003).4. Finding Optimal Parent Setsintroducing algorithm solving shortest path problem, first discussobtain cost BestScore(X, U) arc U U {X} visitorder graph. Recall arc involves selecting optimal parents variablecandidate set. need consider subsets candidate set finding subsetbest score. section, introduce two data structures related methodscomputing storing optimal parent sets scores pairs variable candidateparent set.exact algorithms learning Bayesian network structures need calculateoptimal parent sets scores. present reasonable approach calculationpaper. Note, however, approach applicable algorithms, vice versa.4.1 Parent Graphuse data structure called parent graph compute costs arcs order graph.variable parent graph. parent graph variable X Hasse diagramconsisting subsets variables V \ {X}. node U stores optimal parentset PAX U minimizes Score(X, PA X ) well BestScore(X, U) itself.example, Figure 2(b) shows sample parent graph X1 contains best scoressubsets {X2 , X3 , X4 }. obtain Figure 2(b), however, first need calculatepreliminary graph Figure 2(a) contains raw score subset U parentset X1 , i.e., Score(X1 , U). Equation 3 shows, scores calculated basedcounts particular instantiations parent child variables.use AD-tree (Moore & Lee, 1998) collect counts datasetcompute scores. AD-tree unbalanced tree structure contains two typesnodes, AD-tree nodes varying nodes. AD-tree node stores number data pointsconsistent particular variable instantiation; varying node used instantiatestate variable. full AD-tree stores counts data points consistentpartial instantiations variables. sample AD-tree two variables shownFigure 3. n variables states each, number AD-tree nodes AD-tree(d+1)n . grows even faster size order parent graph. Moore Lee (1998)also described sparse AD-tree significantly reduces space complexity. Readersreferred paper details. pseudo code assumes sparse AD-treeused.Given AD-tree, ready calculate raw scores Score(X1 , .) Figure 2(a).exponential number scores parent graph. However, parentsets possibly optimal Bayesian network; certain parent sets discardedwithout ever calculating values according following theorems Tian (2000).31fiYuan & MaloneFigure 2: sample parent graph variable X1 . (a) raw scores Score(X1 , .)parent sets. first line node gives parent set, secondline gives score using set parents X1 . (b) optimalscores BestScore(X1 , .) candidate parent set. second linenode gives optimal score using subset variables first lineparents X1 . (c) optimal parent sets scores. pruned parentsets shown gray. parent set pruned predecessorsbetter score.X1 = *X2 = *C = 50VaryVX1VaryVX2X1 = 0X2 = *X1 = 1X2 = *X1 = *X2 = 0X1 = *X2 = 1C = 20C = 30C = 25C = 25VaryX2VaryX2X1 = 0X2 = 0X1 = 0X2 = 1X1 = 1X2 = 0X1 = 1X2 = 1C = 15C=5C = 10C = 20Figure 3: AD-tree.use theorems compute necessary MDL scores. scoring functionsBDeu also similar pruning rules (de Campos & Ji, 2011). Algorithm 1 providespseudo code calculating raw scores.Theorem 1 optimal Bayesian network based MDL scoring function,2Nvariable log( logN ) parents, N number data points.32fiLearning Optimal Bayesian NetworksAlgorithm 1 Score Calculation AlgorithmInput: AD sparse AD-tree input data; V input variables.Output: Score(X, U) pair X V U V \ {X}1: function calculateMDLScores(AD, V)2:Xi V3:calculateScores(Xi , AD)4:end5: end function6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:27:function calculateScores(Xi, AD)2Nk 0 log( logPrune due Theorem 1N )U U V \ {X}& |U| == kparent sets size kprune f alseUK(Xi |U) - Score(Xi , U \ {Y }) > 0prune truePrune due Theorem 2breakendendprune ! = trueScore(Xi , U) log2 N K(Xi |U)Complexity terminstantiation xi , u Xi , ULog likelihood termcF amily GetCount({xi } u,AD)cP arents GetCount(u, AD)Score(Xi , U) Score(Xi , U) - cF amily log cF amilyScore(Xi , U) Score(Xi , U) + cF amily log cP arentsendendendendend functionTheorem 2 Let U two candidate parent sets X, U S, K(Xi |S)DL(Xi |U) > 0. supersets cannot possibly optimal parent setsX.computing raw scores, compute parent graph according followingtheorem appeared many earlier papers, e.g., see work TeyssierKoller (2005), de Campos Ji (2010). theorem simply means parent setoptimal subset better score.Theorem 3 Let U two candidate parent sets X U S,Score(X, U) Score(X, S). optimal parent set X candidate set.33fiYuan & MaloneAlgorithm 2 Computing parent graphsInput: necessary Score(X, U), X V&U V \ {X}Output: Full parent graphs containing BestScore(X, U)1: function calculateFullParentGraphs(V, Score(., .))2:X V3:layer 0 nPropagate best scores graph4:U U V \ {X}& |U| == layer5:calculateBestScore(X, U, Score(., .))6:end7:end8:end9: end function10:11:12:13:14:15:16:17:function calculateBestScore(X, U, Score(., .))BestScore(X, U) Score(X, U)UBestScore(X, U \ {Y }) < BestScore(X, U)BestScore(X, U) BestScore(X, U \ {Y })endendend functionfunction getBestScore(X, U)19:return BestScore(X, U)20: end functionPropagate best scoresQuery BestScore(X, U)18:Therefore, generate successor node U{Y } U parent graph X,check whether Score(X, U {Y }) smaller BestScore(X, U). so, let parentgraph node U{Y } record optimal parent set. Otherwise BestScore(X, U)smaller, propagate optimal parent set U U{Y }. propagation,must following (Teyssier & Koller, 2005).Theorem 4 Let U two candidate parent sets X U S. mustBestScore(X, S) BestScore(X, U).pseudo code propagating scores computing parent graph outlinedAlgorithm 2. Figure 2(b) shows parent graph optimal scores propagatingbest scores top bottom.search order graph, whenever visit new arc U U {X},find score looking parent graph variable X. example, need findoptimal parents X1 {X2 , X3 }, look node {X2 , X3 } X1 parent graphfind optimal parent set score. make look-ups efficient, use hashtables organize parent graphs query answered constant time.34fiLearning Optimal Bayesian NetworksparentsX1scoresX1{X2 , X3 }5{X3 }6{X2 }8{}10Table 1: Sorted scores parent sets X1 pruning parent setspossibly optimal.parentsX12parentsXX1X3parentsX14parentsXX1{X2 , X3 }110{X3 }010{X2 }100{}000Table 2: parentsX (Xi ) bit vectors X1 . 1 line Xi indicates corresponding parent set includes variable Xi , 0 indicates otherwise. Notethat, pruning, none optimal parent sets include X4 .4.2 Sparse Parent Graphsfull parent graph variable X exhaustively enumerates subsets V \ {X}stores BestScore(X, U) subsets. Naively, approach requires storingn2n1 scores parent sets (Silander & Myllymaki, 2006). Theorem 3, however,number optimal parent sets often far smaller full size. Figure 2(b) showsoptimal parent set may shared several candidate parent sets. full parentgraph representation allocate space repetitive information candidate sets,resulting waste time space.address limitations, introduce sparse representation parent graphsrelated scanning techniques querying optimal parent sets. full parentgraphs, begin calculating pruning scores described last Section. DueTheorems 1 2, parent sets pruned without evaluated.Therefore, create full parent graphs. Also, instead creatingHasse diagrams, sort optimal parent scores variable X list, alsomaintain parallel list stores associated optimal parent sets. call sortedlists scoresX parentsX . Table 1 shows sorted lists optimal scoresparent graph Figure 2(b). essence, allows us store efficiently processscores Figure 2(c).find optimal parent set X candidate set U, simply scanlist X starting beginning. soon find first parent set subsetU, find optimal parent score BestScore(X, U). trivially true duefollowing theorem.Theorem 5 first subset U parentsX optimal parent set X U.Scanning lists find optimal parent sets inefficient done properly.Since scanning arc visited order graph, inefficiencyscanning large impact search algorithm.35fiYuan & MaloneparentsX1validX13parentsXX1newvalidX1{X2 , X3 }100{X3 }100{X2 }111{}111Table 3: result performing bitwise operation exclude parent setsinclude X3 . 1 validX1 bit vector means parent setinclude X3 used selecting optimal parents. first set bitindicates best possible score parent set.parentsX1validX13parentsXX1newvalidX1{X2 , X3 }000{X3 }010{X2 }100{}111Table 4: result performing bitwise operation exclude parent setsinclude either X3 X2 . 1 validnewX1 bit vector means parentset includes neither X2 X3 . initial validX1 bit vector already excludedX3 , finding validnewX1 required excluding X2 .ensure efficiency, propose following scanning technique. variableX, first initialize working bit vector length kscoresX k called validX 1s.indicates parent scores scoresX usable. Then, create n 1 bit vectorsalso length kscoresX k, one variable V \ {X}. bit vector variabledenoted parentsYX contains 1s parent sets contain 0s others.Table 2 shows bit vectors example Table 1. Then, exclude variablecandidate parent, perform bit operation validnewvalidX & parentsYX . newXvalidX bit vector contains 1s parent sets subsets V \ {Y }.first set bit corresponds BestScore(X, V \ {Y }). Table 3 shows example excludingX3 set possible parents X1 , first set bit new bit vectorcorresponds BestScore(X1 , V \ {X3 }). want exclude X2 candidateparent, new bit vector last step becomes current bit vector step,2bit operation applied: validnewvalidX & parentsXXX1 . first setbit result corresponds BestScore(X1 , V \ {X2 , X3 }). Table 4 demonstratesoperation. Also, important note exclude one variable time. example,if, excluding X3 , wanted exclude X4 rather X2 , could take validnewX4validX & parentsX.operationsdescribedcreateSparseParentGraphXgetBestScore functions Algorithm 3.pruning duplicate scores, sparse representation requires much lessmemory storing possible parent sets scores. long kscores(X)k <C(n 1, n2 ), also requires less memory memory-efficient dynamic programmingalgorithm (Malone et al., 2011b).Experimentally, show kscoresX k almost36fiLearning Optimal Bayesian NetworksAlgorithm 3 Sparse Parent Graph AlgorithmsInput: necessary Score(X, U), X V&U V \ {X}Output: Sparse parent graphs containing optimal parent sets scores1: function createSparseParentGraph(X, Score(., .))2:X V3:scorest , parentst sort(Score(X, ))Sort scores, preferring low cardinality4:scoresX , parentsXInitialize possibly optimal scores5:= 0 |scorest |6:prune f alse7:j = 0 |scoresX |Check better subset pattern exists8:contains(parentst(i), parentsX (j))&scoresX (i) scorest (i)9:prune true10:Break11:end12:end13:prune ! = true14:Append scoresX , parentsX parentst(i), parentst (i)15:end16:end17:= 0 |scoresX |Set bit vectors efficient querying18:parentsX (i)19:set(parentsYX (i))20:end21:end22:end23: end function24:25:26:27:28:29:30:31:function getBestScore(X, U)valid allScoresXV \ Uvalid valid& parentsYXendf sb f irstSetBit(valid)return scoresX [f sb]end functionQuery BestScore(X, U)Return first score set bitalways smaller C(n 1, n2 ) several orders magnitude. approach offers(usually substantial) memory savings compared previous best approaches.sparse representation extra benefit improving time efficiency well.full representation, create complete exponential-size parent graphs,even though many nodes parent graph share optimal parent choices.sparse representation, avoid creating nodes, makes creating sparseparent graphs much efficient.37fiYuan & Malone5. A* Search Algorithmready tackle shortest path problem order graph. sectionpresents search algorithm well two admissible heuristic functions guidingalgorithm.5.1 Algorithmapply well known state space search method, A* algorithm (Hart et al., 1968),solve shortest path problem order graph. main idea algorithmuse evaluation function f measure quality search nodes always expandone lowest f cost exploration order graph. node U,f (U) decomposed sum exact past cost, g(U), estimated future cost,h(U). g(U) cost measures shortest distance start node U,h(U) cost estimates far away U goal node. Therefore, f cost providesestimated total cost best possible path passes U.A* uses open list (usually priority queue) store search frontier,closed list store expanded nodes. Initially open list contains start node,closed list empty. search step, node lowest f -costopen list, say U, selected expansion generate successor nodes. expandingU, however, need first check whether goal node. yes, shortest pathgoal found; construct Bayesian network path terminatesearch.U goal, expand generate successor nodes. successorconsiders one possible way adding new variable, say X, leaf existingsubnetwork variables U, = U {X}. g cost calculatedsum g-cost U cost arc U S. arc cost welloptimal parent set PAX X U retrieved Xs parent graph. h costcomputed heuristic function describe shortly. recordfollowing information2 : g cost, h cost, X, PAX .clear order graph multiple paths node.perform duplicate detection see whether node representing set variablesalready generated before. check duplicates, search space blowsorder graph size 2n order tree size n!. first check whetherduplicate already exists closed list. so, check whether duplicatebetter g cost S. yes, discard immediately, represents worse path.Otherwise, remove duplicate closed list, place open list.happens found better path lower g cost, reopen node futuresearch.duplicate found closed list, also need check open list.duplicate found, simply add open list. Otherwise, compareg costs duplicate S. duplicate lower g cost, discarded.Otherwise, replace duplicate S. Again, lower g cost means betterpath found.2. also delay calculation h duplicate detection avoid unnecessary calculationsnodes pruned.38fiLearning Optimal Bayesian NetworksAlgorithm 4 A* Search AlgorithmInput: full sparse parent graphs containing BestScore(X, U)Output: optimal Bayesian network G1: function main(D)2:start3:Score(start) 0 P4:push(open, start, V BestScore(Y, V \ {Y })5:!isEmpty(open)6:U pop(open)7:U goalshortest path found8:print(The best score + Score(V))9:G construct network shortest path10:return G11:end12:put(closed, U)13:X V \ UGenerate successors14:g BestScore(X, U) + Score(U)15:contains(closed, U {X})Closed list DD16:g < Score(U {X})reopen node17:delete(closed, U {X})18:push (open, U {X}, g + h)19:Score(U {X}) g20:end21:else22:contains(open, U {X}) & g < Score(U {X}) Open list DD23:update(open, U {X}, g + h)24:Score(U {X}) g25:end26:end27:end28:end29: end functionsuccessor nodes generated, place node U closedlist, indicates node already expanded. Expanding top node openlist called one search step. A* algorithm performs step repeatedly goalnode selected expansion. moment shortest path start stategoal state found.shortest path found, reconstruct optimal Bayesian networkstructure starting goal node tracing back shortest path reachingstart node. Since node path stores leaf variable optimal parent set,putting optimal parent sets together generates valid Bayesian network structure.pseudo code A* algorithm shown Algorithm 4.39fiYuan & Malone5.2 Simple Heuristic FunctionA* algorithm provides different theoretical guarantees depending propertiesheuristic function h. function h admissible h cost never greatertrue cost goal; words, optimistic. Given admissible heuristicfunction, A* algorithm guaranteed find shortest path goal nodeselected expansion (Pearl, 1984). Let U node order graph. first considerfollowing simple heuristic function h.Definition 1h(U) =XBestScore(X, V\{X}).(7)XV\Uheuristic function allows remaining variable choose optimal parentsvariables. design reflects principle exact cost relaxed problemused admissible bound original problem (Pearl, 1984). case,original problem learn Bayesian network directed acyclic graph. Equation 7relaxes problem ignoring acyclicity constraint, directed cyclic graphsallowed. heuristic function easily proven admissible following theorem.proofs theorems paper found Appendix A.Theorem 6 h admissible.turns h even nicer property. heuristic function consistent if,node U successor S, h(U) h(S) + c(U, S), c(U, S) stands costarc U S. Given consistent heuristic, f cost monotonically non-decreasingfollowing path order graph. result, f cost node lessequal f cost goal node. follows immediately consistent heuristicguaranteed admissible. consistent heuristic, A* algorithm guaranteedfind shortest path node U U selected expansion. duplicatefound closed list, duplicate must optimal g cost, new nodediscarded immediately. show following simple heuristic Equation 7also consistent.Theorem 7 h consistent.heuristic may seem expensive compute requires computing BestScore(X, V\{X}) variable X. However, scores easily found querying parentgraphs stored array repeated use. takes linear time calculateheuristic start node. subsequent computation h, however, takes constanttime simply subtract best score newly added variableheuristic value parent node.5.3 Improved Admissible Heuristicsimple heuristic function defined Equation 7, referred hsimple hereafter, relaxesacyclicity constraint Bayesian networks completely. result, hsimple may introducemany directed cycles result loose bound. introduce another heuristicsection tighten heuristic. first use toy example motivate new heuristic,describe two specific approaches computing heuristic.40fiLearning Optimal Bayesian NetworksX1X2X3X4Figure 4: directed graph representing heuristic estimate start search node.5.3.1 Motivating Examplehsimple , heuristic estimate start node order graph allows variablechoose optimal parents variables. Suppose optimal parent setsX1 , X2 , X3 , X4 {X2 , X3 , X4 }, {X1 , X4 }, {X2 }, {X2 , X3 } respectively. parentchoices shown directed graph Figure 4. Since acyclicity constraintignored, directed cycles introduced, e.g., X1 X2 . However, knowfinal solution cannot cycles; three cases possible X1 X2 : (1) X2parent X1 (so X1 cannot parent X2 ), (2) X1 parent X2 , (3) neithertrue. Based Theorem 4, third case cannot provide better valuefirst two cases one variables must fewer candidate parents.(1) (2), unclear one better, take minimumget lower bound. Consider case (1). delete arc X1 X2 ruleX1 parent X2 . let X2 rechoose optimal parents remainingvariables {X3 , X4 }, is, must check parent sets including X1 . deletionarc alone cannot produce new bound best parent set X2{X3 , X4 } necessarily {X4 }. total bound X1 X2 computed summingtogether original bound X1 new bound X2 . call total boundb1 . Case (2) handled similarly; call total bound b2 . joint costX1 X2 , c(X1 , X2 ), must optimistic, compute minimum b1 b2 .Effectively considered possible ways break cycle obtained tighterheuristic value. new heuristic clearly admissible, still allow cycles amongvariables.Often, hsimple introduces multiple cycles heuristic estimate. Figure 4 alsocycle X2 X4 . cycle shares X2 earlier cycle X1 X2 ;say cycles overlap. One way break cycles set parent set X2{X3 }; however, introduces new cycle X2 X3 . described detailshortly, partition variables exclusive groups break cycles withingroup. example, X2 X3 different groups, break cycle.41fiYuan & Malone5.3.2 K-Cycle Conflict Heuristicidea generalized compute joint cost variable groupsize k avoiding cycles within group. node U order graph,calculate heuristic value partitioning variables V \ U several exclusivegroups sum costs together. name resulting technique k-cycle conflictheuristic. Note simple heuristic hsimple special case new heuristic,simply contains costs individual variables (k=1).new heuristic application additive pattern database technique (Felner,Korf, & Hanan, 2004). Pattern databases (Culberson & Schaeffer, 1998) approachcomputing admissible heuristic problem solving relaxed problem. Consider15-puzzle problem. 15 square tiles numbered 1 15 randomly placed 44 box one position left empty. configuration tiles called state.goal slide tiles one time destination configuration. tile slideempty position beside position. 15 puzzle relaxedcontain tiles 1-8 tiles removed. relaxation, multiplestates original problem map one state abstract state space relaxedproblem share positions remaining tiles. abstract state calledpattern; cost pattern equal smallest cost sliding remainingtiles destination positions. cost provides lower bound stateoriginal state space maps pattern. costs patterns storedpattern database.relax problem different ways obtain multiple pattern databases.solutions several relaxed problems independent, problems saidexclusive. 15-puzzle, also relax contain tiles 9-15. relaxationsolved independently previous one share puzzlemovements. concrete state original state space, positions tiles 1-8map pattern first pattern database, positions tiles 9-15 mapdifferent pattern second pattern database. costs patterns addedtogether obtain admissible heuristic, hence name additive pattern databases.learning problem, pattern defined group variables, costoptimal joint cost variables avoiding directed cycles them.decomposability scoring function implies costs two exclusive patternsadded together obtain admissible heuristic.explicitly break cycles computing cost pattern.following theorem offers straightforward approach so.Theorem 8 cost pattern U, c(U), equal shortest distance V \ Ugoal node order graph.consider example Figure 4. cost pattern {X1 , X2 } equalshortest distance {X3 , X4 } goal order graph Figure 1.Furthermore, difference c(U) sum simple heuristic valuesvariables U indicates amount improvement brought avoiding cycles withinpattern. differential score, called h , thus used quality measureordering patterns choosing patterns likely result tighterheuristic.42fiLearning Optimal Bayesian Networks5.3.3 Dynamic K-Cycle Conflict Heuristictwo slightly different versions k-cycle conflict heuristic. first versionnamed dynamic k-cycle conflict heuristic, compute costs groups variablessize k store single pattern database. According Theorem 8,heuristic computed finding shortest distances nodeslast k layers order graph goal.compute heuristic using breadth-first search backward searchorder graph k layers. search starts goal node expands ordergraph backward layer layer. reverse arc U {X} U cost arcU U {X}, i.e., BestScore(X, U). reverse g cost U updated whenever newpath lower cost found. Breadth-first search ensures node U obtainexact reverse g cost previous layer expanded. g cost cost patternV \ U. also compute differential score, h , pattern time.pattern better differential score subset patternsdiscarded. pruning significantly reduce size pattern database improvequery efficiency. algorithm computing dynamic k-cycle conflict heuristicshown Algorithm 5.heuristic created, calculate heuristic value search nodefollows. node U, partition remaining variables V \ U set exclusivepatterns, sum costs together heuristic value. Since prune supersetpatterns, always find partition. However, potentially many wayspartition. Ideally want find one highest total cost, representstightest heuristic value. problem finding optimal partition formulatedmaximum weighted matching problem (Felner et al., 2004). k = 2, defineundirected graph vertex represents variable, edge twovariables represents pattern containing variables weight equalcost pattern. goal select set edges graph twoedges share vertex total weight edges maximized. matching problemsolved O(n3 ) time, n number vertices (Papadimitriou & Steiglitz,1982).k > 2, add hyperedges matching graph connectingk vertices represent larger patterns. goal becomes select set edgeshyperedges maximize total weight. However, three-dimensional higher-ordermaximum weighted matching problem NP-hard (Garey & Johnson, 1979). meanssolve NP-hard problem calculating heuristic value.alleviate potential inefficiency, greedily select patterns based quality.Consider node U unsearched variables V \ U. choose pattern highestdifferential cost patterns subsets V \ U. repeat stepremaining variables variables covered. total cost chosen patternsused heuristic value U. hdynamic function Algorithm 5 gives pseudocodecomputing heuristic value.dynamic k-cycle conflict heuristic introduced example dynamically partitioned pattern database (Felner et al., 2004) patterns dynamicallyselected search algorithm. refer dynamic pattern database short.43fiYuan & MaloneAlgorithm 5 Dynamic k-cycle Conflict HeuristicInput: full sparse parent graphs containing BestScore(X, U)Output: pattern database P patterns size k1: function createDynamicPD(k)2:P D0 (V) 03:h (V) 04:l = 1 kPerform BFS k levels5:U P Dl16:expand(U, l)7:checkSave(U)8:P D(V \ U) P Dl1 (U)9:end10:end11:X P \ saveRemove superset patterns improvement12:delete P D(X)13:end14:sort(P : h )Sort patterns decreasing costs15: end function16:17:18:19:20:21:22:23:24:25:26:27:28:29:30:31:32:33:34:35:36:37:38:function expand(U, l)X Ug P Dl1 (U) + BestScore(X, U \ {X})g < P Dl (U \ {X}) P Dl (U \ {X}) gendend functionDuplicate detectionfunction checkSave(U)Ph (U) g V\U BestScore(Y, V \ {Y })X V \ UCheck improvement subset patternsh (U) > h (U {X}) save(U)endend functionfunction hdynamic (U)h0RUPRRR\Sh h + P D(S)endendreturn hend functionCalculate heuristic value UGreedily find best subset pattern R44fiLearning Optimal Bayesian Networkspotential drawback dynamic pattern databases that, even greedymethod, computing heuristic value still much expensive simple heuristicEquation 7. Consequently, search time longer even though tighter patterndatabase heuristic results pruning fewer expanded nodes.5.3.4 Static K-Cycle Conflict Heuristicaddress inefficiency dynamic pattern database computing heuristic values,introduce another version named static k-cycle conflict heuristic based staticallypartitioned pattern database technique (Felner et al., 2004). idea partitionvariables several static exclusive groups, create separate pattern databasegroup. Consider problem variables {X1 , ..., X8 }. divide variablestwo groups, {X1 , ..., X4 } {X5 , ..., X8 }. group, say {X1 , ..., X4 }, createpattern database contains costs subsets {X1 , ..., X4 } storehash table. refer heuristic static pattern database short.Screate static pattern databases follows. static grouping V = Vi , needcompute pattern database group Vi resembles order graph containingsubsets Vi . use breadth first search create graph startingnodeVi . cost arc U{X} U graph equal BestScore(X, ( j6=i Vj )U),means variables groups valid candidate parents. ensureefficient retrieval, static pattern databases stored hashtables; nothing prunedthem. Algorithm 6 gives pseudocode creating static pattern databases.much simpler use static pattern databases compute heuristic value. Considersearch node {X1 , X4 , X8 }; unsearched variables {X2 , X3 , X5 , X6 , X7 }. simplydivide variables two patterns {X2 , X3 } {X5 , X6 , X7 } according staticgrouping, look respective pattern databases, sum costs togetherheuristic value. Moreover, since search step processes one variable,one pattern affected requires new score lookup. Therefore, heuristic valuecalculated incrementally. hstatic function Algorithm 6 provides pseudocodenaively calculating heuristic value.5.3.5 Properties K-Cycle Conflict Heuristicversions k-cycle conflict heuristic remain admissible. Although avoidcycles within pattern, cannot prevent cycles across different patterns. following theorem proves result.Theorem 9 k-cycle conflict heuristic admissible.Understanding consistency new heuristic slightly complex. firstlook static pattern database involve selecting patterns dynamically.following theorem shows static pattern database still consistent.Theorem 10 static pattern database version k-cycle conflict heuristic remainsconsistent.dynamic pattern database, search step needs solve maximum weightedmatching problem select set patterns compute heuristic value.45fiYuan & MaloneAlgorithm 6 Static k-cycle Conflict HeuristicsInput: full sparse parent graphs containing BestScore(X, U), Vi partition VOutput: full pattern database P Vi1: function createStaticPD(Vi )2:P D0i () 0 fi fi3:l = 1 fiVi fiPerform BFS Vi4:U P Dl15:expand(U, l, Vi )(U)6:P (U) P Dl17:end8:end9: end function10:11:12:13:14:15:16:17:18:19:20:21:22:function expand(U, l, Vi )X Vi \ U(U) + BestScore(X, Ug P Dl1j6=i Vj )g < P Dl (U X) P Dl (U X) gendend functionfunction hstatic (U)h0Vi Vh h + P (U Vi )endreturn hend functionDuplicate detectionSum P separatelyfollowing, show dynamic k-cycle conflict heuristic also consistent closelyfollowing Theorem 4.1 work Edelkamp Schrodl (2012).Theorem 11 dynamic pattern database version k-cycle conflict heuristic remains consistent.However, theorem assumes use shortest distances nodesabstract space. use greedy method solve maximum weightedmatching problem, longer guarantee find shortest paths. result,may lose consistency property dynamic pattern database. thus necessaryA* reopen duplicate node closed list better path found.6. Experimentsevaluated A* search algorithm set benchmark datasets UCI repository (Bache & Lichman, 2013). datasets 29 variables 30, 162 datapoints. discretized variables two states using mean values deleted46fiLearning Optimal Bayesian Networks1.00E+101.00E+09FullLargest LayerSparse1.00E+081.00E+07Size1.00E+061.00E+051.00E+041.00E+031.00E+021.00E+011.00E+00Figure 5: number parent sets scores stored full parent graphs(Full), largest layer parent graphs memory-efficient dynamic programming (Largest Layer), sparse representation (Sparse).data points missing values. A* search algorithm implemented Java3 .compared algorithm branch bound (BB)4 (de Campos & Ji, 2011), dynamic programming (DP)5 (Silander & Myllymaki, 2006), integer linear programming(GOBNILP) algorithms6 (Cussens, 2011). used latest versions softwaresource code time experiments well default parameter settings;version 1.1 GOBNILP 2.1.1 SCIP. BB DP calculate MDL,use BIC score, uses equivalent calculation MDL. results confirmedalgorithms found Bayesian networks either belongequivalence class. experiments performed 2.66 GHz Intel Xeon 16GBRAM running SUSE Linux Enterprise Server version 10.6.1 Full vs Sparse Parent Graphsfirst evaluated memory savings made possible sparse parent graphs comparison full parent graphs. particular, compared maximum numberscores stored variables algorithm. typical dynamic programming algorithm stores scores possible parent sets variables.memory-efficient dynamic programming (Malone et al., 2011b) stores possible parentsets one layer parent graphs variables, size largest layer3. software package source code named URLearning (You Learning) implementing A*algorithm downloaded http://url.cs.qc.cuny.edu/software/URLearning.html.4. http://www.ecse.rpi.edu/cvrl/structlearning.html5. http://b-course.hiit.fi/bene6. http://www.cs.york.ac.uk/aig/sw/gobnilp/47fiYuan & Maloneparent graphs indication space requirement. sparse representationstores optimal parent sets variables.Figure 5 shows memory savings sparse representation benchmarkdatasets. clear number optimal parent scores stored sparse representation typically several orders magnitude smaller full representation.Furthermore, due Theorem 1, increasing number data points increases maximum number candidate parents. Therefore, number candidate parent sets increasesnumber data points increases; however, many new parent sets prunedsparse representation Theorem 3. number variables also affectsnumber candidate parent sets. Consequently, number optimal parent scoresincreases function number data points number variables.results show, amount pruning data-dependent, though, easily predictable.practice, find number data points affect number unique scores muchnumber variables.6.2 Pattern Database Heuristicsnew pattern database heuristic two versions: static dynamic pattern databases;parameterized different ways. tested various parameterizationsnew heuristics A* algorithm two datasets named Autos Flag. chosetwo datasets large enough number variables betterdemonstrate effect pattern database heuristics. dynamic pattern database,varied k 2 4. static pattern databases, tried groupings 9-9-8 13-13Autos dataset groupings 10-10-9 15-14 Flag dataset. obtainedgroupings simply dividing variables datasets several consecutive blocks.results based sparse parent graphs shown Figure 6. showresults full parent graphs A* ran memory datasetsfull parent graphs used. sparse representations, A* achieved much betterscalability, able solve Autos heuristic Flagbest heuristics using sparse parent graphs. Hereafter experiments resultsassume use sparse parent graphs.Also, pattern database heuristics improved efficiency scalability A* significantly. A* either simple heuristic static pattern database grouping10-10-9 ran memory Flag dataset. pattern database heuristics enabled A* finish successfully. dynamic pattern database k = 2 helped reducenumber expanded nodes significantly datasets. Setting k = 3 helped evenmore. However, increasing k 4 resulted increased search time, sometimeseven increased number expanded nodes (not shown). believe larger k alwaysresults better pattern database; occasional increase expanded nodesgreedy strategy used choose patterns fully utilize better heuristic.longer search time understandable though, less efficient computeheuristic value larger pattern databases, inefficiency gradually overtookbenefit. Therefore, k = 3 seems best parametrization dynamic patterndatabase general. static pattern databases, able test much larger48fiLearning Optimal Bayesian Networks1.00E+04Running TimeSize Pattern Database1.00E+051.00E+031.00E+021.00E+011.00E+00500450400350300250200150100500Autos1.00E+04Running TimeSize Pattern Database1.00E+051.00E+031.00E+021.00E+011.00E+00500450400350300250200150100500XXF lagFigure 6: comparison A* enhanced different heuristics (hsimple , hdynamic k =2, 3, 4, hstatic groupings 9-9-8 13-13 Autos datasetgroupings 10-10-9 15-14 Flag dataset). Size Pattern Databasemeans number patterns stored. Running Time means search time(in seconds) using indicated pattern database strategy. X meansmemory.groups need enumerate groups certain size. resultssuggest fewer larger groups tend result tighter heuristic.sizes static pattern databases typically much larger dynamicpattern databases. However, time needed create pattern databases still negligible comparison search time cases. thus cost effective try computelarger affordable-size static pattern databases achieve better search efficiency.results show best static pattern databases typically helped A* achieve betterefficiency dynamic pattern databases, even number expanded nodeslarger. reason calculating heuristic values much efficientusing static pattern databases.49fiYuan & Malone10000BB ScoringDP ScoringA* ScoringScoring Time10001001010.1Figure 7: comparison scoring time BB, DP, A* algorithms. labelX-axis consists dataset name, number variables, numberdata points.6.3 A* Simple Heuristicfirst tested A* hsimple heuristic. competing algorithm roughly twophases, computing optimal parent sets/scores (scoring phase) searching Bayesiannetwork structure (searching phase). therefore compare algorithms based twoparts running time: scoring time search time. Figure 7 shows scoring timesBB, DP, A*. GOBNILP included assumes optimal scoresprovided input. label horizontal axis shows dataset, numbervariables, number data points. results show AD-tree method usedA* algorithm seems efficient approach computing parent scores.scoring part DP often order magnitude slower others.result somewhat misleading, however. scoring searching parts DPtightly integrated algorithms. result, work DP donescoring part; little work left search. show shortly, search timeDP typically short.Figure 8(a) reports search time algorithms. benchmarkdatasets difficult algorithms take long even fail find optimalsolutions. therefore terminate algorithm early runs 7,200 secondsdataset. results show BB succeeded two datasets, VotingHepatitis, within time limit. datasets, A* algorithm several ordersmagnitude faster BB. major difference A* BB formulationsearch space. BB searches space directed cyclic graphs, A* alwaysmaintains directed acyclic graph search. results indicate bettersearch space directed acyclic graphs.results also show search time needed DP algorithm often shorterA*. explained earlier, reason heavy lifting DP done50fiLearning Optimal Bayesian Networks10000BBDPGOBNILPA*Search Time1000100101X XXXXXXXX XX(a)10000Total Running TimeDP Total TimeA* Total Time1000100101(b)Figure 8: comparison (a) search time (in seconds) BB, DP, GOBNILP, A*(b) total running time DP A*. X means correspondingalgorithm finish within time limit (7,200 seconds) ran memorycase A*..scoring part. add scoring search time together, shown Figure 8(b),A* several times faster DP datasets except Adult Voting (Again,GOBNILP left search part). main difference A*DP A* explores part order graph, dynamic programming fullyevaluates graph. However, step A* search algorithm overheadcost computing heuristic function maintaining priority queue. One step51fiYuan & MaloneA* expensive similar dynamic programming step. pruningoutweigh overhead, A* slower dynamic programming. AdultVoting large number data points, makes pruning techniqueTheorem 1 less effective. Although DP algorithm perform pruning, duesimplicity, algorithm highly streamlined optimized performingcalculations. DP algorithm faster A* search twodatasets. However, A* algorithm efficient DP datasets.datasets, number data points large comparison numbervariables. pruning significantly outweighs overhead A*. example,A* runs faster Mushroom dataset comparing total running time even thoughMushroom 8,000 data points.comparison GOBNILP A* shows advantages. A* able find optimal Bayesian networks datasets well withintime limit. GOBNILP failed learn optimal Bayesian networks three datasets,including Letter, Image, Mushroom. reason GOBNILP formulateslearning problem integer linear program whose variables correspond optimalparent sets variables. Even though datasets many variables,many optimal parent sets, integer programs many variablessolvable within time limit. hand, results also show GOBNILPquite efficient many datasets. Even though dataset may manyvariables, GOBNILP solve efficiently long number optimal parent setssmall. much efficient A* datasets Hepatitis Heart, althoughopposite true datasets Adult Statlog.6.4 A* Pattern Database HeuristicsSince static pattern databases seem work better dynamic pattern databasescases, tested A* static pattern database (A*,SP) A*, DP, GOBNILPdatasets used Figure 8 well several larger datasets. used simplestatic grouping n2 n2 datasets, n number variables.results BB excluded solve additional dataset. resultsshown Figure 9.benefits brought pattern databases A* rather obvious.datasets A* able finish, A*,SP typically order magnitudefaster. addition, A*,SP able solve three larger datasets: Sensor, Autos, Flag,A* failed them. running time datasets pretty short,indicates memory consumption parent graphs reduced, A*able use memory order graph solve search problems rathereasily.DP able solve one dataset, Autos, A* able solve.somewhat surprising given A* pruning capability. explanation A*stores search information RAM, fail RAM exhausted. DPalgorithm described Silander Myllymaki (2006) stores intermediate resultscomputer files hard disks, able scale larger datasets A*.52fiLearning Optimal Bayesian Networks1000Search TimeDPGOBNILPA*A*, SP100101XXXXXXXX XX X XFigure 9: comparison search time (in seconds) DP, GOBNILP, A*, A*,SP.X means corresponding algorithm finish within timelimit (7,200 seconds) ran memory case A*.GOBNILP able solve Autos, Horse, Flag, failed Sensors. Sensorsdataset 5, 456 data points. number optimal parent sets large, almost106 shown Figure 5. GOBNILP begins difficulty solving datasets8, 000 optimal parent scores particular computing environment. again,GOBNILP quite efficient datasets able solve Autos Flag.algorithm solve Horse dataset. Figure 5, clearreason number optimal parent sets small dataset.6.5 Pruning A*gain insight performance A*, also looked amount pruningA* different layers order graph. plot Figure 10 detailed numbersexpanded nodes versus numbers unexpanded nodes layer ordergraph two datasets: Mushroom Parkinsons. use datasetslargest datasets solved A* A*,SP, manifest differentpruning behaviors. top two figures show results A* simple heuristic,bottom two show A*,SP algorithm.Mushroom, plain A* needed expand small portion search nodeslayer, indicates heuristic function quite tight dataset. effectivepruning started early 6th layer. Parkinsons, however, plain A*successful pruning nodes. first 13 layers, heuristic function appearedloose. A* expand nodes layers. heuristic function becametighter latter layers enabled A* prune increasing percentage searchnodes. help pattern database heuristic, however, A*,SP helped prune many53fi1.60E+06Expanded1.40E+06UnexpandedExpandedvsUnexpandedNodesExpandedvsUnexpandedNodesYuan & Malone1.20E+061.00E+068.00E+056.00E+054.00E+052.00E+050.00E+00024681012 14Layer1618201.60E+06Expanded1.40E+061.20E+061.00E+068.00E+056.00E+054.00E+052.00E+050.00E+00022(a) A* Mushroom246810 12 14Layer16182022182022(b) A* Parkinsons1.60E+061.60E+06ExpandedUnexpandedExpandedvsUnexpandedNodesExpandedvsUnexpandedNodesUnexpanded1.40E+061.20E+061.00E+068.00E+056.00E+054.00E+052.00E+050.00E+000246810 12Layer1416182022(c) A*,SP MushroomExpandedUnexpanded1.40E+061.20E+061.00E+068.00E+056.00E+054.00E+052.00E+050.00E+000246810 12 14Layer16(d) A*,SP ParkinsonsFigure 10: number expanded unexpanded nodes A* layer ordergraph Mushroom Parkinsons using different heuristics.search nodes Parkinsons; pruning became effective early 6th layer.A*,SP also helped prune nodes Mushroom, although benefit clearA* already quite effective dataset.6.6 Factors Affecting Learning DifficultySeveral factors may affect difficulty dataset Bayesian network learningalgorithms, including number variables, number data points, numberoptimal parent sets. analyzed correlation factors searchtimes algorithms. replaced occurrence time 7,200 ordermake analysis possible (we caution though may results underestimation).Figure 11 shows results. excluded results BB finished twodatasets. DP, A*, A*,SP, important factor determining efficiencynumber variables, correlations search time numbersvariables greater 0.58. However, seems negative correlationsearch time number data points. Intuitively, increasing numberdata points make dataset difficult. explanation preexisting negative correlation number data points number variablesdatasets tested; analysis shows correlation 0.61.54fiLearning Optimal Bayesian Networks1VariablesData!RecordsOptimal!Parent!Sets0.8Correlation0.60.40.200.2DPGOBNILPA*A*,!SP0.40.6Figure 11: correlation search time algorithms several factorsmay affect difficulty learning problem, including numbervariables, number data points dataset, number optimalparent sets.Since search time strong positive correlation number variables,seemingly negative correlation search time number data pointsbecomes less surprising.comparison, efficiency GOBNILP affected number optimalparent sets; correlation high close 0.8. Also, positive correlationnumber data points efficiency. because, explained earlier,data points often leads optimal parent sets. Finally, correlationnumber variables almost zero, means difficulty dataset GOBNILPdetermined number variables.insights quite important, provide guideline choosing suitablealgorithm given characteristic dataset. many optimal parent setsmany variables, A* better algorithm; way around true, GOBNILPbetter.6.7 Effect Scoring Functionsanalyses far based mainly MDL score. decomposable scoringfunctions also used A* algorithm, correctness search strategiesheuristic functions affected scoring function. However, different scoringfunctions may different properties. example, Theorem 1 property MDLscore. cannot use pruning technique scoring functions. Consequently,number optimal parent sets, tightness heuristic, practical performancevarious algorithms may affected.verify hypothesis, also tested BDeu scoring function (Heckerman, 1998)equivalent sample size set 1.0. Since scoring phase commonexact algorithms, focus experiment comparing number optimal parentsets resulted scoring functions, search time A*,SP GOBNILP55fiYuan & MaloneOptimal PS, MDLOptimal PS, BdeuSize100000001000000100000100001000100101(a)10000GOBNILP, MDLGOBNILP, BDeuA*, MDLA*, BDeuSearch Time1000100101XXXXXXXXXX(b)Figure 12: comparison (a) number optimal parent sets, (b) search timeA*,SP GOBNILP various datasets two scoring functions, MDLBDeu.datasets; Horse Flag included optimal parent setsunavailable. Figure 12 shows results.main observation number optimal parent sets differ MDLBDeu. BDeu score tends allow larger parent sets MDL resultslarger number optimal parent sets datasets. difference aroundorder magnitude datasets Imports Autos.comparison search time shows A*,SP affected much GOBNILP. increase number optimal parent sets, efficiency findingoptimal parent set affected, A*,SP slowed slightlydatasets. significant change Mushroom dataset. took A*,SP 2seconds solve dataset using MDL, 115 seconds using BDeu. comparison,GOBNILP affected much more. able solve datasets Imports Autos effi56fiLearning Optimal Bayesian Networksciently using MDL, failed solve within 3 hours using BDeu. remainedunable solve Letter, Image, Mushroom, Sensors within time limit.7. Discussions Conclusionspaper presents shortest-path perspective problem learning optimal Bayesiannetworks optimize given scoring function. uses implicit order graph representsolution space learning problem shortest path startgoal nodes graph corresponds optimal Bayesian network. perspectivehighlights importance two orthogonal directions research. One directiondevelop search algorithms solving shortest path problem. main contributionmade line A* algorithm solving shortest path problem learningoptimal Bayesian network. Guided heuristic functions, A* algorithm focusessearching promising parts solution space finding optimal Bayesiannetwork.second equally important research direction development search heuristics.introduced two admissible heuristics shortest path problem. first heuristicestimates future cost completely relaxing acyclicity constraint Bayesian networks. shown admissible also consistent. second heuristic,k-cycle conflict heuristic, developed based additive pattern database technique.Unlike simple heuristic variable allowed choose optimal parents independently, new heuristic tightens estimation enforcing acyclicity constraintwithin small groups variables. two specific approaches computingnew heuristic. One approach named dynamic k-cycle conflict heuristic computes costsgroups variables size k. search, dynamically partitionremaining variables exclusive patterns calculating heuristic value.approach named static k-cycle conflict heuristic partitions variables several staticexclusive groups, computes separate pattern database group. sumcosts static pattern databases obtain admissible heuristic. heuristicsremain admissible consistent, although consistency dynamic k-cycle conflictmay sacrificed due greedy method used select patterns.tested A* algorithm empowered different search heuristics set UCImachine learning datasets. results show pattern database heuristicscontributed significant improvements efficiency scalability A* algorithm. results also show A* algorithm typically efficient dynamicprogramming shares similar formulation. comparison GOBNILP, integerprogramming algorithm, A* less sensitive number optimal parent sets, numberdata points, scoring functions, sensitive number variablesdatasets. advantages, believe methods represent promising approachlearning optimal Bayesian network structures.Exact algorithms learning optimal Bayesian networks still limited relativelysmall problems. scaling learning needed, e.g., incorporating domainexpert knowledge learning. also means approximation methods still usefuldomains many variables. Nevertheless, exact algorithms valuableserve basis evaluate different approximation methods57fiYuan & Malonequality assurance. Also, promising research direction develop algorithmsbest properties approximation exact algorithms, is,find good solutions quickly and, given enough resources, converge optimalsolution (Malone & Yuan, 2013).Acknowledgmentsresearch supported NSF grants IIS-0953723, EPS-0903787, IIS-1219114Academy Finland (Finnish Centre Excellence Computational Inference ResearchCOIN, 251170). Part research previously presented IJCAI-11 (Yuan,Malone, & Wu, 2011) UAI-12 (Yuan & Malone, 2012).Appendix A. Proofsfollowing proofs theorems paper.A.1 Proof Theorem 5Proof: Note optimal parent set X U subset U,subset best score. Sorting unique parent scores makes surefirst found subset must satisfy requirements stated theorem.A.2 Proof Theorem 6Proof: Heuristic function h clearly admissible, allows remaining variablechoose optimal parents variables V. chosen parent set mustsuperset parent set variable optimal directed acyclic graphconsisting remaining variables. Due Theorem 4, heuristic results lowerbound cost.A.3 Proof Theorem 7Proof: successor node U, let \ U.Xh(U) =BestScore(X, V\{X})XV\UXBestScore(X, V\{X})XV\U,X6=Y+BestScore(Y, U)= h(S) + c(U, S).inequality holds fewer variables used select optimal parents . Hence,h consistent.A.4 Proof Theorem 8Proof: theorem proven noting avoiding cycles variablesU equivalent finding optimal ordering variables best joint score.58fiLearning Optimal Bayesian Networksdifferent paths V \ U goal node correspond different orderingsvariables, among shortest path hence corresponds optimal ordering.A.5 Proof Theorem 9Proof: node U, assume remaining variables V \ U partitioned exclusivesets V1 , ..., Vp . decomposability scoring function, h(U) =pPc(Vi ). computing c(Vi ), allow directed cycles within Vi .i=1variables V \ Vi valid candidate parents, however. cost pattern, c(Vi ),must optimal definition pattern databases. argument usedproof Theorem 6, h(U) cost cannot worse total cost V \ U, is,cost optimal directed acyclic graph consisting variables (with U allowableparents also). Otherwise, simply arrange variables patternsorder optimal directed acyclic graph get cost. Therefore, heuristicstill admissible.Note previous argument relies optimality pattern costs,patterns chosen. greedy strategy used dynamic pattern databaseaffects patterns selected. Therefore, theorem holds dynamicstatic pattern databases.A.6 Proof Theorem 10Proof: Recall using static pattern databases node partitions V = Vi ,heuristic value node U follows.h(U) =Xc((V \ U) Vi ),(V \U) Vi pattern ith static pattern database. Then, successornode U, let \ U. Without lost generality, let (V \ U) Vj . heuristicvalue nodeh(S) =Xc((V \ U) Vi ) + c((V \ U) (Vj \ {Y })).i6=jAlso, cost Uc(U, S) = BestScore(Y, U).definition pattern database, know c((V\U)Vj ) best possiblejoint score variables pattern U searched. Therefore,c((V \ U) Vj ) c(V \ U) Vj \ {Y }) + BestScore(Y, (i6=j Vi ) (Vj \ (V \ U))c((V \ U) (Vj \ {Y })) + BestScore(Y, U).last inequality holds U (i6=j Vi ) (Vj \ (V \ U)). followingimmediately follows.h(U) h(S) + c(U, S).59fiYuan & MaloneHence, static k-cycle conflict heuristic consistent.A.7 Proof Theorem 11Proof: heuristic values calculated dynamic pattern database considered shortest distances nodes abstract space. abstract space consistsset nodes, i.e., subsets V. However, additional arcs addednode nodes k additional variables.Consider shortest path p two nodes U goal V original solutionspace. path remains valid path, may longer shortest path UV additional arcs.Let g (U, V) shortest distance U V abstract space.successor node U, must following.g (U, V) g (U, S) + g (S, V).(8)Now, recall g (U, V) g (S, V) heuristic values original solutionspace, g (U, S) equal arc cost c(U, S) original space. thereforefollowing.h(U) c(U, S) + h(S).(9)Hence, dynamic k-cycle conflict heuristic consistent.ReferencesAcid, S., & de Campos, L. M. (2001). hybrid methodology learning belief networks:BENEDICT. International Journal Approximate Reasoning, 27 (3), 235262.Akaike, H. (1973). Information theory extension maximum likelihood principle.Proceedings Second International Symposium Information Theory, pp.267281.Bache, K., & Lichman, M.http://archive.ics.uci.edu/ml.(2013).UCImachinelearningrepository.Bouckaert, R. R. (1994). Properties Bayesian belief network learning algorithms.Proceedings Tenth Conference Uncertainty Artificial Intelligence, pp.102109, Seattle, WA. Morgan Kaufmann.Bozdogan, H. (1987). Model selection Akaikes information criterion (AIC): generaltheory analytical extensions. Psychometrika, 52, 345370.Buntine, W. (1991). Theory refinement Bayesian networks. Proceedings seventhconference (1991) Uncertainty artificial intelligence, pp. 5260, San Francisco,CA, USA. Morgan Kaufmann Publishers Inc.Cheng, J., Greiner, R., Kelly, J., Bell, D., & Liu, W. (2002). Learning Bayesian networksdata: information-theory based approach. Artificial Intelligence, 137 (1-2),4390.60fiLearning Optimal Bayesian NetworksChickering, D. (1995). transformational characterization equivalent Bayesian networkstructures. Proceedings 11th annual conference uncertainty artificialintelligence (UAI-95), pp. 8798, San Francisco, CA. Morgan Kaufmann Publishers.Chickering, D. M. (1996). Learning Bayesian networks NP-complete. LearningData: Artificial Intelligence Statistics V, pp. 121130. Springer-Verlag.Chickering, D. M. (2002). Learning equivalence classes Bayesian-network structures.Journal Machine Learning Research, 2, 445498.Cooper, G. F., & Herskovits, E. (1992). Bayesian method induction probabilisticnetworks data. Machine Learning, 9, 309347.Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,14, 318334.Cussens, J. (2011). Bayesian network learning cutting planes. ProceedingsTwenty-Seventh Conference Annual Conference Uncertainty Artificial Intelligence (UAI-11), pp. 153160, Corvallis, Oregon. AUAI Press.Daly, R., & Shen, Q. (2009). Learning Bayesian network equivalence classes ant colonyoptimization. Journal Artificial Intelligence Research, 35, 391447.Dash, D., & Cooper, G. (2004). Model averaging prediction discrete Bayesiannetworks. Journal Machine Learning Research, 5, 11771203.Dash, D. H., & Druzdzel, M. J. (1999). hybrid anytime algorithm constructioncausal models sparse data. Proceedings Fifteenth Annual ConferenceUncertainty Artificial Intelligence (UAI99), pp. 142149, San Francisco, CA.Morgan Kaufmann Publishers, Inc.de Campos, C. P., & Ji, Q. (2011). Efficient learning Bayesian networks using constraints.Journal Machine Learning Research, 12, 663689.de Campos, C. P., & Ji, Q. (2010). Properties Bayesian Dirichlet scores learn Bayesiannetwork structures. Fox, M., & Poole, D. (Eds.), AAAI, pp. 431436. AAAI Press.de Campos, L. M. (2006). scoring function learning Bayesian networks basedmutual information conditional independence tests. Journal Machine LearningResearch, 7, 21492187.de Campos, L. M., Fernndez-Luna, J. M., Gmez, J. A., & Puerta, J. M. (2002). Ant colonyoptimization learning Bayesian networks. International Journal ApproximateReasoning, 31 (3), 291311.de Campos, L. M., & Huete, J. F. (2000). new approach learning belief networksusing independence criteria. International Journal Approximate Reasoning, 24 (1),11 37.61fiYuan & Malonede Campos, L. M., & Puerta, J. M. (2001). Stochastic local algorithms learning beliefnetworks: Searching space orderings. Benferhat, S., & Besnard, P.(Eds.), ECSQARU, Vol. 2143 Lecture Notes Computer Science, pp. 228239.Springer.Edelkamp, S., & Schrodl, S. (2012). Heuristic Search - Theory Applications. MorganKaufmann.Felner, A., Korf, R., & Hanan, S. (2004). Additive pattern database heuristics. JournalArtificial Intelligence Research, 22, 279318.Felzenszwalb, P. F., & McAllester, D. A. (2007). generalized A* architecture. JournalArtificial Intelligence Research, 29, 153190.Friedman, N., & Koller, D. (2003). Bayesian network structure: Bayesianapproach structure discovery Bayesian networks. Machine Learning, 50 (1-2),95125.Friedman, N., Nachman, I., & Peer, D. (1999). Learning Bayesian network structuremassive datasets: sparse candidate algorithm. Laskey, K. B., & Prade, H.(Eds.), Proceedings Fifteenth Conference Conference Uncertainty ArtificialIntelligence (UAI-99), pp. 206215. Morgan Kaufmann.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: GuideTheory NP-Completeness. W. H. Freeman & Co., New York, NY, USA.Glover, F. (1990). Tabu search: tutorial. Interfaces, 20 (4), 7494.Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determination minimum cost paths. IEEE Trans. Systems Science Cybernetics, 4 (2),100107.Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks:combination knowledge statistical data. Machine Learning, 20, 197243.Heckerman, D. (1998). tutorial learning Bayesian networks. Holmes, D., & Jain,L. (Eds.), Innovations Bayesian Networks, Vol. 156 Studies ComputationalIntelligence, pp. 3382. Springer Berlin / Heidelberg.Hemmecke, R., Lindner, S., & Studeny, M. (2012). Characteristic imsets learningBayesian network structure. International Journal Approximate Reasoning, 53 (9),13361349.Hsu, W. H., Guo, H., Perry, B. B., & Stilson, J. A. (2002). permutation genetic algorithmvariable ordering learning Bayesian networks data. Langdon, W. B.,Cant-Paz, E., Mathias, K. E., Roy, R., Davis, D., Poli, R., Balakrishnan, K., Honavar,V., Rudolph, G., Wegener, J., Bull, L., Potter, M. A., Schultz, A. C., Miller, J. F.,Burke, E. K., & Jonoska, N. (Eds.), GECCO, pp. 383390. Morgan Kaufmann.62fiLearning Optimal Bayesian NetworksJaakkola, T., Sontag, D., Globerson, A., & Meila, M. (2010). Learning Bayesian networkstructure using LP relaxations. Proceedings 13th International ConferenceArtificial Intelligence Statistics (AISTATS), pp. 358365, Chia Laguna Resort,Sardinia, Italy.Klein, D., & Manning, C. D. (2003). A* parsing: Fast exact Viterbi parse selection.Proceedings Human Language Conference North American AssociationComputational Linguistics (HLT-NAACL), pp. 119126.Koivisto, M., & Sood, K. (2004). Exact Bayesian structure discovery Bayesian networks.Journal Machine Learning Research, 5, 549573.Kojima, K., Perrier, E., Imoto, S., & Miyano, S. (2010). Optimal search clusteredstructural constraint learning Bayesian network structure. Journal MachineLearning Research, 11, 285310.Lam, W., & Bacchus, F. (1994). Learning Bayesian belief networks: approach basedMDL principle. Computational Intelligence, 10, 269293.Larranaga, P., Kuijpers, C. M. H., Murga, R. H., & Yurramendi, Y. (1996). LearningBayesian network structures searching best ordering genetic algorithms. IEEE Transactions Systems, Man, Cybernetics, Part A, 26 (4), 487493.Malone, B., & Yuan, C. (2013). Evaluating anytime algorithms learning optimal Bayesiannetworks. Proceedings 29th Conference Uncertainty Artificial Intelligence (UAI-13), pp. 381390, Seattle, Washington.Malone, B., Yuan, C., Hansen, E., & Bridges, S. (2011a). Improving scalability optimal Bayesian network learning frontier breadth-first branch bound search.Proceedings 27th Conference Uncertainty Artificial Intelligence (UAI-11),pp. 479488, Barcelona, Catalonia, Spain.Malone, B., Yuan, C., & Hansen, E. A. (2011b). Memory-efficient dynamic programminglearning optimal Bayesian networks. Proceedings 25th AAAI ConferenceArtificial Intelligence (AAAI-11), pp. 10571062, San Francisco, CA.Moore, A., & Lee, M. S. (1998). Cached sufficient statistics efficient machine learninglarge datasets. Journal Artificial Intelligence Research, 8, 6791.Moore, A., & Wong, W.-K. (2003). Optimal reinsertion: new search operator accelerated accurate Bayesian network structure learning. InternationalConference Machine Learning, pp. 552559.Myers, J. W., Laskey, K. B., & Levitt, T. S. (1999). Learning Bayesian networksincomplete data stochastic search algorithms. Laskey, K. B., & Prade, H.(Eds.), Proceedings Fifteenth Conference Conference Uncertainty ArtificialIntelligence (UAI-99), pp. 476485. Morgan Kaufmann.63fiYuan & MaloneOrdyniak, S., & Szeider, S. (2010). Algorithms complexity results exact Bayesianstructure learning. Gruwald, P., & Spirtes, P. (Eds.), Proceedings 26thConference Conference Uncertainty Artificial Intelligence (UAI-10), pp. 401408. AUAI Press.Ott, S., Imoto, S., & Miyano, S. (2004). Finding optimal models small gene networks.Pacific Symposium Biocomputing, pp. 557567.Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial optimization: algorithmscomplexity. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.Parviainen, P., & Koivisto, M. (2009). Exact structure discovery Bayesian networksless space. Proceedings Twenty-Fifth Conference Uncertainty ArtificialIntelligence, Montreal, Quebec, Canada. AUAI Press.Pearl, J. (1984). Heuristics: intelligent search strategies computer problem solving.Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.Pearl, J. (1988). Probabilistic reasoning intelligent systems: networks plausible inference. Morgan Kaufmann Publishers Inc.Perrier, E., Imoto, S., & Miyano, S. (2008). Finding optimal Bayesian network givensuper-structure. Journal Machine Learning Research, 9, 22512286.Rissanen, J. (1978). Modeling shortest data description. Automatica, 14, 465471.Silander, T., & Myllymaki, P. (2006). simple approach finding globally optimal Bayesian network structure. Proceedings 22nd Annual ConferenceUncertainty Artificial Intelligence (UAI-06), pp. 445452. AUAI Press.Silander, T., Roos, T., Kontkanen, P., & Myllymaki, P. (2008). Factorized normalizedmaximum likelihood criterion learning Bayesian network structures. Proceedings4th European Workshop Probabilistic Graphical Models (PGM-08), pp. 257272.Singh, A., & Moore, A. W. (2005). Finding optimal Bayesian networks dynamic programming. Tech. rep. CMU-CALD-05-106, Carnegie Mellon University.Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, prediction, search (secondedition). MIT Press.Suzuki, J. (1996). Learning Bayesian belief networks based minimum descriptionlength principle: efficient algorithm using B&B technique. InternationalConference Machine Learning, pp. 462470.Teyssier, M., & Koller, D. (2005). Ordering-based search: simple effective algorithmlearning Bayesian networks. Proceedings Twenty-First Annual ConferenceUncertainty Artificial Intelligence (UAI-05), pp. 584590. AUAI Press.64fiLearning Optimal Bayesian NetworksTian, J. (2000). branch-and-bound algorithm MDL learning Bayesian networks.UAI 00: Proceedings 16th Conference Uncertainty Artificial Intelligence,pp. 580588, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.Tsamardinos, I., Brown, L., & Aliferis, C. (2006). max-min hill-climbing Bayesiannetwork structure learning algorithm. Machine Learning, 65, 3178.Xie, X., & Geng, Z. (2008). recursive method structural learning directed acyclicgraphs. Journal Machine Learning Research, 9, 459483.Yuan, C., Lim, H., & Littman, M. L. (2011a). relevant explanation: Computationalcomplexity approximation methods. Annals Mathematics Artificial Intelligence, 61, 159183.Yuan, C., Lim, H., & Lu, T.-C. (2011b). relevant explanation Bayesian networks.Journal Artificial Intelligence Research (JAIR), 42, 309352.Yuan, C., Liu, X., Lu, T.-C., & Lim, H. (2009). Relevant Explanation: Properties,algorithms, evaluations. Proceedings 25th Conference UncertaintyArtificial Intelligence (UAI-09), pp. 631638, Montreal, Canada.Yuan, C., & Malone, B. (2012). improved admissible heuristic learning optimalBayesian networks. Proceedings 28th Conference Uncertainty ArtificialIntelligence (UAI-12), pp. 924933, Catalina Island, CA.Yuan, C., Malone, B., & Wu, X. (2011). Learning optimal Bayesian networks using A*search. Proceedings 22nd International Joint Conference Artificial Intelligence (IJCAI-11), pp. 21862191, Helsinki, Finland.65fi