Journal Artificial Intelligence Research 48 (2013) 635-669

Submitted 11/12; published 11/13

Reasoning Explanations
Negative Query Answers DL-Lite
Diego Calvanese

calvanese@inf.unibz.it

Free University Bozen-Bolzano, Italy

Magdalena Ortiz

ortiz@kr.tuwien.ac.at

Vienna University Technology, Austria

Mantas Simkus

simkus@dbai.tuwien.ac.at

Vienna University Technology, Austria

Giorgio Stefanoni

Giorgio.Stefanoni@cs.ox.ac.uk

University Oxford, United Kingdom

Abstract
order meet usability requirements, logic-based applications provide explanation facilities reasoning services. holds also Description Logics, research
focused explanation TBox reasoning and, recently, query answering. Besides explaining presence tuple query answer, important explain
also given tuple missing. address latter problem instance conjunctive query answering DL-Lite ontologies adopting abductive reasoning; is,
look additions ABox force given tuple result. reasoning
tasks consider existence recognition explanation, relevance necessity
given assertion explanation. characterize computational complexity
problems arbitrary, subset minimal, cardinality minimal explanations.

1. Introduction
Ontology-based data access (OBDA) systems new form information systems
use ontology, set logical constraints, mediate access data. role
ontology OBDA system twofold. one hand, intermediate layer
domain user physical data providing unified view information
held various data sources. many cases, ontology extends data vocabulary
introducing new intensional predicates used query information
succinct declarative way. hand, ontology provides constraints,
taken account answering queries may contribute enrich
obtained answers. Hence, potentially relevant implicit knowledge derived
data, plus ontology, made explicit using specifically tailored reasoning
algorithms. existing OBDA systems based DL-Lite family lightweight
Description Logics (DLs), introduced Calvanese, De Giacomo, Lembo, Lenzerini,
Rosati (2007), also basis QL profile OWL 2 ontology language
(Motik, Fokoue, Horrocks, Wu, Lutz, & Grau, 2009).
argued McGuinness Patel-Schneider (1998), order meet usability requirements set domain users, knowledge-based systems equipped explanation algorithms reasoning services. holds also Description Logics,
c
2013
AI Access Foundation. rights reserved.

fiCalvanese, Ortiz, Simkus & Stefanoni

research focused explanation TBox reasoning (cf., McGuinness & Borgida,
1995; Borgida, Franconi, & Horrocks, 2000; Penaloza & Sertkaya, 2010; Horridge, Parsia, &
Sattler, 2008). Additionally, Borgida, Calvanese, Rodriguez-Muro (2008) studied
problem explaining positive query answers conjunctive queries DL-Lite ontologies. particular, outlined procedure computing reasons tuple
answer query, minimizing corresponding explanation shown
user. addition, Borgida et al. (2008) suggested OBDA systems, besides explaining
positive query answers, also explain negative query answers; is, tuples
user expects result actually occur there. OBDA systems
answer queries ontological constraints, explaining negative query answers trivial: constraints need taken account understand required tuple
missing answers. procedure explaining negative query answers would
improve usability OBDA systems.
reason, formalize explanation problem context query answering
DL ontologies. Following Eiter Gottlob (1995), adopt abductive reasoning;
is, explanations set facts need asserted ABox force required
tuple result. explanations help users debugging negative answer
giving effective way repairing OBDA system terms updates data layer.
Since ontologies used enrich data vocabulary, consider also restrictions
vocabulary additional assertions constructed. precisely,
given DL TBox , ABox A, query q, set predicates, explanation
given tuple ~c new ABox E, whose predicates occur , answer
q ontology hT , Ei contains ~c. According Occams razor principle,
important aspect explanations provide users solutions simple
understand free redundancy, hence small possible. address requirement,
study various restrictions explanations, particular, focus subset minimal
cardinality minimal ones. consider standard decision problems associated logic-based
abduction: (i) existence explanation, (ii) recognition given ABox
explanation, (iii) relevance (iv) necessity ABox assertionthat is, whether
occurs explanations. first, latter two problems may appear rather
artificial, however, provide valuable information user debugging negative
answers. Relevance used test whether assertion user deems related
negative answer indeed so; whereas, necessity used test whether assertion
intrinsically related negative answer.
idea restricting vocabulary explanations adaptation concept
introduced Baader, Bienvenu, Lutz, Wolter (2010), study among others
query emptiness problem. is, given query q TBox decide whether
ABoxes given signature , evaluating q hT , Ai leads
empty result. Section 3, shall see framework deciding existence
explanation relates query non-emptiness problem. fact, many DLs, deciding
whether query non-empty w.r.t. TBox reduces checking whether exists
explanation missing answer.
purpose paper shed light computational complexity explaining
missing answers queries ontologies formulated DL-Lite expressive member
DL-Lite family DLs. end, consider two important classes queries
636

fiReasoning Explanations Negative Query Answers DL-Lite

is, instance queries unions conjunctive queries (UCQs)and provide computational complexity results four decision problems defined above. Moreover,
perform complexity analysis two different explanation settings. consider
case explanation vocabulary strict subset vocabulary ontology
data, well case explanations constructed arbitrary
predicates. Section 4, show consider instance queries input,
relevant decision problems NL-complete, irrespective chosen explanation setting
particular minimality criterion applied explanations. Section 5, analyze complexity problem admit UCQs input, show
complexity varies respect chosen explanation setting minimality
criterion. complexity results UCQs summarized Table 5.1.

2. Preliminaries
section, first introduce ontologies formulated DLs, particular focus
DL DL-Lite . introduce languages querying ontologies consider,
recall important properties DL-Lite used throughout
paper. Finally, briefly present less known complexity classes
mentioned later.
2.1 Description Logic Ontologies
usual DLs, consider countably infinite sets NC , NR , NI atomic concepts,
atomic roles, individuals, respectively. Whenever distinction atomic concepts roles immaterial, call element NC NR predicate.
DL TBox finite set axioms, whose form depends specific DL
considered; DL-Lite , DL adopted paper, definition given below.
DL ABox finite set ABox assertions, expressions form A(c)
P (c, d), atomic concept, P atomic role, c individuals.
DL ontology pair = hT , Ai, DL TBox DL ABox.
semantics DL ontologies based first-order interpretations = hI , i,

non-empty set called domain interpretation function mapping
individual c NI object cI , atomic concept NC set AI ,
atomic role P NR binary relation P .
interpretation satisfies ABox assertion A(c) cI AI , satisfies
assertion P (c, d) hcI , dI P . Satisfaction TBox axioms also defined according
form specific DL; define DL-Lite . interpretation
model hT , Ai, satisfies axioms assertions A. call hT , Ai
consistent admits least one model, inconsistent otherwise. Also, ABox
consistent TBox ontology hT , Ai consistent.
2.1.1 DL-Lite
DL-Lite member DL-Lite family DLs (Calvanese et al., 2007; Calvanese,
De Giacomo, Lembo, Lenzerini, Poggi, Rodriguez-Muro, & Rosati, 2009),
designed dealing efficiently large amounts extensional information. DL-Lite ,
637

fiCalvanese, Ortiz, Simkus & Stefanoni

concept expressions (or, concepts) C, denoting sets objects, role expressions (or,
roles) R, denoting binary relations objects, formed according following
syntax, denotes atomic concept P atomic role.1
R P | P

C | R

DL-Lite TBox consists axioms following form.
C1 v C2
R1 v R2

C1 v C2
R1 v R2

(funct R)

Axioms first column called positive inclusions (among concepts roles, respectively), second column disjointness axioms, third column
functionality assertions roles. order retain tractability reasoning, DL-Lite
TBoxes must satisfy additional restriction roles functional inverse functional cannot specialized. Formally, DL-Lite TBox contains (funct P ) (funct P ),
role R contain R v P R v P (Calvanese et al., 2007).
semantics concept expressions specified follows.
(R)I

= {o | o0 : ho, o0 RI }

(P )I

= {ho, o0 | ho0 , oi P }

interpretation satisfies axiom 1 v 2 1I 2I , satisfies axiom 1 v 2
1I 2I = , satisfies axiom (funct R) RI partial functionthat is,
set objects {o, o1 , o2 } , ho, o1 RI ho, o2 RI , o1 = o2 .
Following common practice DLs DL-Lite family (Calvanese et al.,
2007), usually adopt unique name assumption (UNA)that is, interpretation individual pair c 6= d, require cI 6= dI . Whenever drop
assumption, explicitly say so. UNA, problem checking whether
DL-Lite ontology consistent NL-complete, whereas without UNA, problem
becomes PTime-complete (Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009).
2.2 Instance Queries Conjunctive Queries
Let NV countably infinite set variables. Together NI NV form set terms.
Expressions form A(t) P (t, t0 ), atomic concept, P atomic role,
t, t0 terms, called atoms.
conjunctive query (CQ) q arity n 0 expression q(x1 , . . . , xn ) a1 , . . . , ,
where, {1, . . . , m}, ai atom. tuple hx1 , . . . , xn
tuple answer variables q. Let NV (q) set variables occurring q, let NI (q)
set individuals q, let at(q) = {a1 , . . . , }, let |q| number terms
occurring q. consider safe CQsthat is, answer variable xi q occurs
least one atoms q. Boolean conjunctive query CQ arity 0, shall
write simply set atoms. instance query q(x) conjunctive query whose body
consists single unary atom A(x). union conjunctive queries (UCQ) set CQs
1. ignore distinction data values objects present DL-Lite OWL 2 QL,
since immaterial results. is, consider value domains attributes.

638

fiReasoning Explanations Negative Query Answers DL-Lite

arity, assume w.l.o.g. CQs UCQ tuple
answer variables. following, denote IQ set instance queries
CQ set UCQs.
match n-ary CQ q interpretation mapping : NV (q) NI (q)

(i) (c) = cI , c NI (q),
(ii) (t) AI , A(t) at(q),
(iii) h(t), (t0 )i P , P (t, t0 ) at(q).
n-tuple individuals hc1 , . . . , cn answer q I, exists match
q hcI1 , . . . , cIn = h(x1 ), . . . , (xn )i. let ans(q, I) denote set
answers q I. Boolean CQ returns answer either , representing
value false,S empty tuple hi, representing value true. UCQ q, let
ans(q, I) = q0 q ans(q 0 , I). certain answer UCQ q arity n ontology hT , Ai
defined
cert(q, , A) = {~c (NI )n | ~c ans(q, I), model hT , Ai}.
2.3 Query Answering DL-Lite
problem query answering DLs problem computing certain answer
given query given DL ontology. Formulated way, query answering
computation problem decision problem. Since paper interested
establishing computational complexity results, identify query answering decision
problem, sometimes called recognition problem, input constituted
DL ontology hT , Ai, query q(~x), tuple ~c arity |~x|, task determine
whether ~c cert(q, , A). special case instance queries, problem also known
instance checking. Notice that, since consider ontology query
part input, considering so-called combined complexity (Vardi, 1982).
many DLs, instance checking reduced problem deciding ontology
consistency. holds also DL-Lite and, thus, answering instance query
done nondeterministic logarithmic space. contrast, problem answering UCQ
(and hence CQ) q DL-Lite ontology hT , Ai solved nondeterministic
polynomial time adopting pure query rewriting approach (Calvanese et al., 2007, 2009).
technique works two steps. first step, compute perfect reformulation
Rq,T q w.r.t. is, rewrite input query q respect TBox
UCQ Rq,T . rewriting step, portion TBox relevant answering q
compiled Rq,T . second step, simply evaluate computed rewriting Rq,T
ABox Aseen first order interpretation. captured proposition
below, makes use notion interpretation associated ABox, formalized
following definition.
Definition 2.1. Given ABox A, let DB interpretation whose domain DB
set individuals occurring A,
(i) cDB = c, individuals c occurring A;
639

fiCalvanese, Ortiz, Simkus & Stefanoni

(ii) ADB = {c | A(c) A}, NC ;
(iii) P DB = {hc, di | P (c, d) A}, P NR .
following proposition summarizes results query answering based rewriting shown logics DL-Lite family (and DL-Lite particular) exploit following.
Proposition 2.1. (Calvanese et al., 2007, 2009) Let hT , Ai DL-LiteA ontology, let q
UCQ, let max(q) = maxqi q |at(qi )|. possible construct UCQ Rq,T , called
perfect reformulation q w.r.t. ,
cert(q, , A) = ans(Rq,T , DB ).
Moreover, Rq,T satisfies following properties.
predicates occurring Rq,T occur q.
qr Rq,T max(q) atoms 2 max(q) terms.
q consists single instance query, qr Rq,T one atom.
qr Rq,T obtained nondeterministic polynomial time combined
size q.
Deciding whether given tuple individuals ans(Rq,T , DB ) also achieved
nondeterministic polynomial time combined size q.
2.4 Complexity Theory
briefly outline definition non-canonical complexity classes used paper;
details, refer reader standard textbooks computational complexity
(e.g., Papadimitriou, 1994). class P2 member Polynomial Hierarchy:
class decision problems solvable nondeterministic polynomial time using NP
oracle. class PNP
k contains decision problems solved polynomial time
NP oracle, oracle calls must first prepared issued parallel.
class DP contains problems that, considered languages, characterized
intersection language NP language coNP. Additionally, class NL
contains decision problems solved nondeterministic Turing machine using
P
logarithmic amount space. believed NL PTime NP DP PNP
k 2
strict hierarchy inclusions. make assumption.
usual, use reductions problems infer complexity bounds throughout
paper. Unless stated otherwise, many-one logarithmic space reductions.

3. Explaining Negative Query Answers
section, formalize abductive task problem finding explanations
negative answers queries DL ontologies.
DL TBox , DL ABox A, query q IQ CQ, let (T , A, q) denote
set predicates occur , A, q. signature non-empty finite
subset NC NR . Furthermore, ABox -ABox assertions use
predicates ; is, (, A, ) .
640

fiReasoning Explanations Negative Query Answers DL-Lite

Definition 3.1. Let hT , Ai DL ontology, q(~x) query IQ CQ, ~c tuple
individuals arity |~x|, signature. call P = hT , A, q, ~c, Query Abduction
Problem (QAP). explanation (or, solution to) P -ABox E
(i) ontology hT , Ei consistent,
(ii) ~c cert(q, , E).
set explanations P denoted expl(P). predicates ones
allowed explanations, hence call abducible predicates. (T , A, q) , say
P unrestricted explanation signature; otherwise, contain symbols
(T , A, q), say P restricted explanation signature.
QAP, call tuple ~c negative answer q hT , Ai, ~c
/ cert(q, , A).
Clearly, query q ontology hT , Ai admits negative answer hT , Ai consistent.
Also, condition (i), ontology inconsistent, P admit explanations.
Ontology languages, DL-Lite , allow specification existential
restrictions negative constraints (e.g., disjointness axioms), sometimes require explanations introduce fresh individuals occur within QAP. next precisely
characterize individuals.
Definition 3.2. Let P = hT , A, q, ~c, QAP let E solution P. arbitrary
individual u occurring E anonymous occur , A, q, ~c.
Now, use example highlight query abduction problems useful
debugging negative query answers.
Example 3.1. Let Au following set assertions particular university.
DPhil(Anna)
enroll(Anna, KR)
enroll(Luca, IDB )

DPhil(Beppe)
teach(Marco, KR)
teach(Carlo, IDB )

is, Anna Beppe doctoral students, Anna enrolled KR course,
taught Marco, Luca enrolled introductory DB course (IDB ),
taught Carlo. Now, consider following DL-Lite TBox Tu formalizing university
domain, Au (partial) instance.
enroll v Student
enroll v Course
DPhil v Student

teach v Lecturer
teach v Course
Course v teach

Tu models objects domain enroll Students, objects domain
teach Lecturers, whereas objects range enroll teach Courses. Among
students DPhil students. Finally, every Course must taught someone.
Now, assume university administration interested finding
teaching course least one enrolled students doctoral student,
captured following query.
qu (x) teach(x, y), enroll(z, y), DPhil(z)
641

fiCalvanese, Ortiz, Simkus & Stefanoni

Assume Carlo expected part result. case, Luca
student Carlo known DPhil student. Hence Carlo
/ cert(q, , A)
Carlo negative answer. Suppose complete information
predicates enroll teachthat is, latter predicates abducible. easy
see
Eu = {teach(Carlo, c), enroll(Beppe, c), enroll(Luca, c)}
explanation QAP Pu = hTu , Au , qu , Carlo, {enroll, teach}i, suggests
existence course, represented anonymous individual c, occur
ABox Au .
example shows certain explanations may assumptive
include assertions required solve problem. Indeed, examples
explanation reason assume Luca enrolled anonymous course
c. following, examine various restrictions expl(P) reduce redundancy
explanations, achieved introducing preference relation among explanations.
relation reflexive transitivethat is, pre-order among explanations.
pre-order expl(P), write E E 0 E E 0 E 0 E.
Definition 3.3. preferred explanations expl (P) QAP P pre-order ,
called -explanations (-solutions), defined follows.
expl (P) = { E expl(P) | E 0 expl(P) E 0 E }
consider two preference orders commonly adopted comparing abductive solutions: subset-minimality order, denoted , minimum explanation
size order, denoted . latter order defined E E 0 iff |E| |E 0 |. Considering
that, definition, explanations finite, arbitrary QAP P,
-solution P also -solution P; is, expl (P) expl (P).
Example 3.2. already argued, ABox Eu redundant solution QAP Pu
introduced Example 3.1. Next, introduce two minimal solutions. First, consider
solution asserting Carlo teach anonymous course c Beppe enrolled
course. ABox Eu0 = {teach(Carlo, c), enroll(Beppe, c)} -explanation. Second,
consider solution asserting Beppe enrolled IDB course. ABox
Eu00 = {enroll(Beppe, IDB )} -explanation (and hence also -explanation).
context logic-based abduction, four main decision problems considered interest (Eiter & Gottlob, 1995), parametrized according chosen
preference order .
~ abducible predicate ,
Definition 3.4. Given QAP P, ABox assertion (d)
ABox E, define following decision problems.
-exist(ence): exist -explanation P?
~ occur -explanations P?
-nec(essity): assertion (d)
~ occur -explanation P?
-rel(evance): assertion (d)
642

fiReasoning Explanations Negative Query Answers DL-Lite

-rec(ognition): ABox E -explanation P?
Whenever preference applied (i.e., identity), omit write
front problems names.
paper, study complexity reasoning problems query abduction. start highlighting, remaining part section, interesting properties
query abduction problems important connections reasoning tasks.
3.1 Reductions Reasoning Problems
show introduced problems reduced other. Unless
otherwise stated, reductions present work DLs, instance queries
UCQs, restricted unrestricted explanation signatures.
start showing nec least hard non-exist (i.e., complement
exist problem).
Proposition 3.1. every DL, non-exist reducible nec.
~ arbitrary ABox assertion,
Proof. Assume QAP P = hT , A, q, ~c, let (d)
~
d~ occur P. following holds: P explanation iff (d)
0
necessary P = hT , A, q, ~c, {}i. construction, follows solution
P also solution P 0 ; furthermore, solution E 0 P 0 , 6 (, E 0 , ) implies
E 0 solution P. definition P 0 since d~ globally fresh,
~
ABox E, E explanation P 0 E \ {(d)}
0
explanation P . correctness reduction immediately follows.
QAPs restricted explanation signatures, next show nec reduces
non-exist. reduction works every DL allows disjointness axioms.
Proposition 3.2. every DL allows concept role disjointness axioms,
restricted explanation signatures, nec reducible non-exist.
Proof. Consider instance nec given QAP P = hT , A, q, ~c, might
~ Next, show construct QAP P 0
restricted, ABox assertion (d).
0
~
(d) necessary P iff P admit solutions. end, let 0
two globally fresh predicates arity ; furthermore, let TBox 0 , ABox A0 ,
signature 0 follows.
0 := {0 v } { v 0 }

~
A0 := {(d)}

0 := { | 6= } {0 }

Finally, let P 0 := hT 0 , A0 , q, ~c, 0 i. Now, show correctness reduction; is,
~ necessary P iff P 0 admit solutions.
(d)
() prove contrapositive. Suppose P 0 solution E 0 . definition
~ 6 E 0 predicate occur E 0 .
hT 0 , A0 0 , 0 (d)
Let ABox E defined follows.
E := {(~t) E 0 | 6= 0 } {(~t) | 0 (~t) E 0 }
643

fiCalvanese, Ortiz, Simkus & Stefanoni

~ remains show
construction, E -ABox contain (d).
E solution P. end, please observe model J hT 0 , A0 E 0
model hT , Ei, since 0 v 0 . addition, model hT , Ei
extended model J hT 0 , A0 E 0 setting 0J := {(~t)J | 0 (~t) E 0 }
~ J }. follows hT 0 , A0 E 0 conservative extension hT , Ei. Given
J := {(d)
~c cert(q, 0 , A0 E 0 ) q hT , Ai, obtain ~c cert(q, , E).
Furthermore, since hT 0 , A0 E 0 consistent, also hT , Ei consistent;
~ required.
E solution P contain assertion (d),
() prove contrapositive. Suppose solution E P exists
~
(d) 6 E. Let ABox E 0 defined follows.
E 0 := {(~t) E | 6= } {0 (~t) | (~t) E}
~ remains show
construction, E 0 0 -ABox contain 0 (d).
0
0
0
0
0
E solution P . seen before, hT , E conservative extension hT , Ei. Given ~c cert(q, , E), obtain ~c cert(q, 0 , A0 E 0 ).
~ 6 E 0 , also hT 0 , A0 E 0
Furthermore, since hT , Ei consistent 0 (d)
consistent; E 0 solution P 0 , required.
simple modification Proposition 3.2 shows result applies also DLs
allow negative ABox assertions form A(c) P (c, c0 ) instead disjointness
axioms. next show rel exist mutually reducible.
Proposition 3.3. every DL, rel exist mutually reducible.
Proof. First, show reduce rel exist. Let P arbitrary QAP
~ arbitrary ABox assertion .
form hT , A, q, ~c, let (d)
0
~ relevant P P 0 admits solution.
construct QAP P (d)
~
end, let A0 ABox defined A0 = {(d)}.
Then, define QAP P 0
0
0
P = hT , , q, ~c, i. Next, prove correctness reduction. only-if direction
immediate. direction, suppose P 0 admits solution E 0 . follows,
~ consistent TBox . Moreover, latter
definition P 0 , -ABox E 0 {(d)}
ABox also solution P and, therefore, given assertion relevant.
Second, prove exist reducible rel. Let P arbitrary QAP
form hT , A, q, ~c, i, let arbitrary predicate , let d~ arbitrary tuple
individuals occurring P d~ arity predicate . prove
~ relevant P. direction follows definition
P admits solution iff (d)
~
relevance. show only-if direction, suppose P admits solution E. (d)
~
occurs E, relevant P. Otherwise, since individuals occur P ,
~ also solution P, hence (d)
~ relevant P.
ABox E {(d)}
Moreover, -nec nec also mutually reducible.
Proposition 3.4. every DL, -nec nec mutually reducible.
~ (d)
~
Proof. arbitrary QAP P arbitrary ABox assertion (d),
~
occurs -minimal explanations P iff (d) occurs explanations P. Thus,
nec -nec equivalent problems.
644

fiReasoning Explanations Negative Query Answers DL-Lite

Finally, since preference orders prefer smaller explanations and, definition,
explanations finite, orders well-founded. immediately follows exists
explanation arbitrary QAP P P admits minimal explanation
preference orders.
Proposition 3.5. every DL, -exist, -exist, exist mutually reducible.
3.2 QAPs Query Emptiness Problem
mentioned introduction, deciding existence explanation related
query emptiness problem studied Baader et al. (2010). Since rely problem
infer complexity bounds throughout paper, briefly introduce here.
Definition 3.5. Let DL TBox, Q {IQ, CQ} query language, signature.
say Q-query q empty given every -ABox consistent
cert(q, , A) = . Otherwise, say q non-empty given
. Q non-emptiness problem consists deciding, input , q, , whether q
non-empty given .
Next, first show that, every DL, instance queries Boolean UCQs,
query non-emptiness reduces exist. Then, show DL-Lite case holds
even arbitrary UCQs.
Proposition 3.6. every DL instance queries Boolean UCQs, Q nonemptiness reducible exist.
Proof. Let arbitrary DL TBox, let q IQ CQ arbitrary query
q CQ implies q Boolean UCQ, let arbitrary signature. show
construct QAP P q non-empty given iff P admits solution.
end, let ~c arbitrary tuple q CQ implies ~c = hi, q IQ
implies ~c = hai globally fresh individual. Clearly, q
non-empty given iff P = hT , , q, ~c, admits solution.
relationship CQ non-emptiness exist tightened, restrict
attention DL-Lite TBoxes.
Proposition 3.7. DL-LiteA , CQ non-emptiness reducible exist.
Proof. Consider DL-Lite TBox , signature , time n-ary query q CQ.
W.l.o.g., assume q CQ. Then, cannot immediately extend proof given
Boolean CQs introducing n (distinct) individuals since might forced match
distinct answer variables q individual ABox witnessing non-emptiness
q. However, adapt proof case follows. let N fresh atomic
concept occurring (T , , q). define 0 = {N } let q 0 Boolean
CQ at(q 0 ) = at(q) {N (x1 ), . . . , N (xn )}. Finally, let P = hT , , q 0 , hi, 0
QAP. following, show q non-empty given iff P admits solution.
() Suppose q non-empty given . is, exists -ABox
hT , Ai consistent exists n-ary tuple ~a = ha1 , . . . , individuals
645

fiCalvanese, Ortiz, Simkus & Stefanoni

~a cert(q, , A). Now, consider 0 -ABox E = {N (ai ) | 1 n}.
Since N fresh predicate, hT , Ei conservative extension hT , Ai.
is, model hT , Ai extended model hT , Ei, every model
hT , Ei also model hT , Ai. assumption hT , Ai consistent
~a cert(q, , A), conclude E solution P.
() Suppose P admits solution E. follows hT , Ei consistent
model hT , Ei, exists match q 0 |= q 0 . Since N
fresh predicate occurring answer variable xi q atom N (xi )
contained q 0 , (xi ) = aIi ai NI N (ai ) E.
follows ~a cert(q, , E). Consider -ABox obtained E removing
assertions N ; immediately follows hT , Ei conservative extension hT , Ai.
Therefore, also ~a cert(q, , A) and, thus, q non-empty given .
Proposition 3.7 generalized Horn DLsthat is, DLs
answering instance conjunctive queries reduces evaluating input query
single, canonical model ontology. follows DL-Lite and, general,
Horn-DLs, deciding exist generalizes query non-emptiness problem. Hence,
hardness results non-emptiness obtained Baader et al. (2010) hold
instance queries UCQs apply also exist problem restricted explanation
signatures. However, since also consider ABoxes require specific tuple
query answer, converse hold always transfer upper
bounds setting.
3.3 Canonical Explanations
studying complexity reasoning query abduction problems, first show
restrict search explanations. order so, define notion
instantiation conjunctive query.
Definition 3.6. Let q n-ary CQ answer variables hx1 , . . . , xn i; furthermore, let
~c = hc1 , . . . , cn tuple individuals. Let mapping terms q NI
identity NI answer variable xj q (xj ) = cj .
Then, call ABox
E = {A((t)) | A(t) at(q)} {R((s), (t)) | R(s, t) at(q)}
~c-instantiation q. Given DL ontology O, additionally that,
quantified variable y, (y) distinct anonymous individual uy occurring q O,
say E direct O.
Note following distinguish instantiations differ
assignment anonymous individuals variables. Hence, CQ finite
number distinct instantiations, unique direct one.
3.3.1 Unrestricted Explanation Signature
obtain explanation QAP P unrestricted explanation signature,
iterate set possible instantiations input query, searching one
646

fiReasoning Explanations Negative Query Answers DL-Lite

instantiation consistent input ontology. absence UNA,
even consider one single instantiation CQ: direct instantiation,
existentially quantified variables mapped distinct anonymous individuals.
presence UNA, underlying DL expressive enough enforce inequalities
individuals occurring P (e.g., means disjointness axioms), reduce
problem searching CQ whose direct instantiation consistent input
ontology, UNA dropped.
Proposition 3.8. Let = hT , Ai arbitrary DL ontology let P = hT , A, q, ~c,
arbitrary QAP unrestricted explanation signature. Furthermore, qi q,
let Eqi direct ~c-instantiation qi O. following hold:
1. UNA, solution P exists iff ~c-instantiation E qi q exists
hT , E consistent.
2. Without UNA, solution P exists iff query qi q exists hT , Eqi
consistent.
3. Furthermore, suppose DL supports concept disjointness axioms.
UNA, solution P exists iff query qi q exists hT 0 , A0 Eqi consistent without UNA, A0 0 extend quadratic number
assertions axioms, respectively.
Proof. Consider arbitrary qi q let E arbitrary ~c-instantiation qi .
first prove consistency hT , E (with without UNA) implies E
solution P (with without UNA, resp.). shows direction 1 2. Let
mapping generating E suppose hT , E consistent. Let
arbitrary model hT , E i. build match qi setting (t) = (t)I
term qi . (xj ) = (xj )I = cIj answer variable xj , match
witnesses ~c ans(q, I). Hence ~c cert(q, , E ) E solution P, desired.
only-if direction 1, assume arbitrary solution E P, use
show exists ~c-instantiation E qi q hT , E consistent.
Since E solution P, definition, exists model hT , Ei
UNA. Without loss generality, assume = NI c NI
cI = c. Moreover, interpretation admits match qi q witnessing
~c ans(qi , I). define mapping , let (t) = (t) term occurring qi .
model E . Since also model hT , Ai, model hT , E
shows latter consistent, desired.
only-if direction 2 shown similarly. Suppose P admits solution E.
exists model hT , Ei (without UNA) admits match
qi q witnessing ~c ans(qi , I). obtain interpretation J model
hT , Eqi i, extend follows. every anonymous individual uy introduced
Eqi due existentially quantified variable y, let uy J = (y). resulting
interpretation model Eqi , since individuals uy occur ontology,
modelhood hT , Ai preserved.
3, use extended ABox A0 TBox 0 enforce UNA individuals occurring P. ABox A0 extends assertion Ac (c) individual
647

fiCalvanese, Ortiz, Simkus & Stefanoni

c occurring P, Ac fresh concept name. TBox 0 consists axioms
Ac v Ac0 pairs c 6= c0 individuals occurring P. Since interpretations
hT , Ai UNA hT 0 , A0 without UNA coincide, claim easily follows
statement 2 above.
direct consequence proposition that, DLs, restrict search
explanations result instantiating input query.
Corollary 3.9. Let P = hT , A, q, ~c, QAP unrestricted explanation signature,
let max(q) = maxqi q |at(qi )|, let max-terms(q) = maxqi q |qi |. P explanation,
P explanation concepts roles q, max(q) atoms,
max-terms(q) individuals.
3.3.2 Restricted Explanation Signature
allow restricted explanation signatures, Proposition 3.8 hold anymore,
search space possible explanations becomes significantly larger. see
following sections, notable effect complexity different decision
problems. However, case DL-Lite , still show weaker version
proposition allows us restrict search instantiations queries
perfect reformulation input query q. Moreover, every -minimal explanation
obtained way.
Proposition 3.10. Let P = hT , A, q, ~c, QAP hT , Ai DL-LiteA ontology,
let Rq,T perfect reformulation q w.r.t. . solution P exists
~c-instantiation E qr Rq,T exists (i) hT , E consistent,
(ii) E \ -ABox. Moreover, E 0 -minimal explanation implies query
qr Rq,T ABox E exist E ~c-instantiation qr E 0 = E \ A.
Proof. first part claim shown analogously item 1 Proposition 3.8 (recall
DL-Lite make UNA). direction, consider arbitrary qr Rq,T
let E ~c-instantiation qr generated mapping . assume hT , E
consistent E \ -ABox. Then, show E \ solution P,
suffices show existence match qr DB AE witnessing ~c cert(q, , AE ).
easily obtained setting (t) = (t)I term qr . only-if
direction, assume arbitrary solution E P use show exists ~cinstantiation E qr Rq,T satisfies conditions (i) (ii). Since E solution
P, definition, E -ABox, hT , Ei consistent, ~c cert(q, , E).
Proposition 2.1, follows exists query qr Rq,T match qr
DB AE witness ~c ans(qr , DB AE ). define mapping setting (t) = (t)
term qr . Then, resulting ~c-instantiation E E E A,
implies consistency hT , E E \ also -ABox desired.
show second part claim, suppose E -minimal solution P.
Proposition 2.1, exists qr Rq,T exists match
witnessing ~c ans(qr , DB AE ). construct ~c-instantiation E qr follows:
E = {A((t)) E | A(t) at(qr )} {R((t), (t0 )) E | R(t, t0 ) at(qr )}
minimality E, E = E \ A.
648

fiReasoning Explanations Negative Query Answers DL-Lite

Similarly above, implies consider small explanations whose size
linear size input query q, signature depends q,
also signature input TBox .
Corollary 3.11. Let P = hT , A, q, ~c, QAP hT , Ai DL-LiteA ontology.
Furthermore, let max(q) = maxqi q |at(qi )|. P = hT , A, q, ~c, explanation,
P explanation concepts roles q, max(q) atoms,
2 max(q) individuals.

4. Complexity Instance Queries
study complexity reasoning query abduction problems. consider
complexity unrestricted restricted explanation signatures, consider
different minimality criteria abductive solutions. measure complexity
QAP P = hT , A, q, ~c, terms combined size , A, q, is,
consider combined complexity. section, investigate complexity reasoning
QAPs body input query consists single unary atomthat is,
consider instance queries. following section, shall turn attention UCQs.
4.1 Existence Explanations
giving first complexity results, show that, instance queries, -minimal
-minimal explanations coincide. see this, consider arbitrary QAP P = hT , A, q, c,
q IQ let qr arbitrary CQ perfect reformulation Rq,T . Propositions 2.1 3.10, follows ~c-instantiation qr consistent hT , Ai
contains explanation P; moreover, -minimal explanation P obtained
way. explanations contain one assertion (cf. Proposition 2.1), -minimal explanations size one, obtain following result.
Proposition 4.1. Let P = hT , A, q, ~c, QAP hT , Ai DL-LiteA ontology
q IQ, let E arbitrary -ABox. Then, E solution P implies
solution E 0 E P exists |E 0 | 1. Hence, expl (P) = expl (P).
consider complexity deciding existence explanation.
Theorem 4.2. DL-LiteA , instance queries, unrestricted restricted
explanation signatures, exist, -exist, -exist NL-complete.
Proof. Proposition 3.5, suffices show result exist. first provide
algorithm yields desired upper bound, even restricted explanation signatures.
show problem NL-hard already case unrestricted signatures.
(membership) Let P = hT , A, q, c, QAP q IQ. decide exist
non-deterministic logarithmic space, exploit Proposition 4.1 test candidate
singleton explanations iterating , individuals occurring P,
two anonymous individuals. results polynomially many candidate solutions E constant size. test whether hT , Ei consistent
c cert(q, , E). Since DL-Lite ontology consistency instance checking
solved non-deterministic logarithmic space, exist NL.
649

fiCalvanese, Ortiz, Simkus & Stefanoni

Algorithm 1 isNEC
~ hT , Ai DL-Lite ontology,
Input: QAP P = hT , A, q, ~c, assertion (d)
q IQ CQ, unrestricted, .
~ necessary P.
Output: yes iff (d)
1: Let globally fresh predicate arity .
~
2: Let 0 := { v } let A0 := {(d)}.
0
0
3: hT , , q, ~
c, admits solution, return no.
~
4: Let set individuals occurring P d.
Let u globally fresh anonymous individual.
~ 6 E
-ABoxes E individuals {u} s.t. |E | 1 (d)
~ hT , E , q, ~c, admits solution, return no.
7:
hT , E |= (d)
8: end
9: Return yes.

5:
6:

(hardness) reduce DL-Lite ontology consistency problem (under UNA)
exist. Consider arbitrary DL-Lite ontology hT , Ai. Furthermore, consider
arbitrary atomic concept occurring hT , Ai, let q = A(x), let c NI arbitrary
individual, let P = hT , A, q, c, QAP unrestricted . show hT , Ai
consistent P admits solution. direction trivial. onlyif direction, suppose hT , Ai consistent, consider E = {A(c)}. Since hT , Ai
consistent fresh, hT , Ei also consistent. model hT , Ei
satisfies assertion A(c), E solution P.
4.2 Deciding Necessity
Section 3.1, seen QAPs restricted explanation signatures
DLs allow disjointness axioms, nec reduces non-exist. case QAPs
unrestricted explanation signatures ontologies restricted DL-Lite , provide
Algorithm 1 Turing reduction (non-)exist; is, procedure solves nec
employing subroutine solving exist. following proposition proves correctness.
Proposition 4.3. DL-LiteA , instance queries UCQs, unrestricted explanation signatures, algorithm isNEC decides nec.
Proof. Let P = hT , A, q, ~c, QAP hT , Ai DL-Lite ontology, query
~ assertion
q IQ CQ, signature unrestricted; furthermore, let (d)
~
abducible predicate . prove (d) necessary P iff isNEC returns yes.
only-if direction, prove contrapositive. Suppose isNEC returns
~ 6 E. According
given instance; show solution E P exists (d)
construction isNEC, consider two alternative cases.
QAP hT 0 , A0 , q, ~c, admits solution E. DL-Lite , Calvanese et al. (2009)
showed negative inclusion axioms affect consistency given ontology,
contribute towards computing certain answer; is, ~c cert(q, 0 , A0 )
~
iff hT 0 , A0 consistent ~c cert(q, , A0 ). Then, since assertion (d)
0
0
predicate occurring P hT , consistent, E also solution
650

fiReasoning Explanations Negative Query Answers DL-Lite

~ since
P = hT , A, q, ~c, i. definition, solution contain (d),
0
0
0
~ v .
hT , |= (d)
QAP hT 0 , A0 , q, ~c, solution. Since isNEC returns no, -ABox E ex~ 6 E , hT , E |= (d),
~ QAP hT , E , q, ~c,
ists |E | 1, (d)
~ entailed hT , E
solution E. Given assertion (d)
0
~
E := E \ {(d)} also solution hT , E , q, ~c, i. conclude E 0 E
~ required.
solution hT , A, q, ~c, contain (d),
direction, prove contrapositive. Suppose -ABox E exists
~ 6 E; show isNEC returns no. W.l.o.g.,
E solution P (d)
~ E
individual u Algorithm 1 occur E. Now, hT , Ei 6|= (d),
0
0
solution QAP hT , , q, ~c, i, isNEC returns no, required. Otherwise, consider
~ take conjunctive query q 0 (~x) (~x).
case hT , Ei |= (d)
assumption, d~ cert(q 0 , , E). Proposition 2.1, query r Rq0 ,T
match r exist r contains single atom d~ ans(r, DB AE ) witnessed
. Let (~y ) unique atom occurring r ~x ~y let (~t)
assertion obtained (~y ) replacing variable ~y (y). Clearly,
(~t) E. Next, distinguish among two cases.
variable ~y (y) I. Then, let E := , (~t) A,
~ 6 E ,
let E := {(~t)}, (~t) E. either case, (d)


~ E E. Hence, E solution QAP hT , E , q, ~c, i;
hT , E |= (d),
isNEC returns no, required.
Variable ~y exists (y) 6 I. Given d~ I, d~ ans(r, DB AE ),
predicates arity 2, d~ form d~ := hdi, NC ,
NR . follows CQ r form r(x) (x, y) r(x) (y, x). Next,
consider former case only, case symmetrical. Then, assertion
(~t) form (d, (y)). Since (y) 6 I, (d, (y)) E. Now, let
E 0 ABox obtained E replacing occurrence individual (y)
individual u Algorithm 1. Since E 0 obtained solution E uniformly
replacing anonymous individual individual occur E P,
E 0 also solution P. definition, (d) 6 E 0 (d, u) E 0 .
Now, let E := {(d, u)}. Since ans(r, DB AE ) witnessed
definition E , hT , E |= (d). last, since E E 0 E 0
solution P, conclude ABox E 0 solution hT , E , q, ~c, i. Hence,
isNEC returns no, required.
Next, use Algorithm 1 Propositions 3.1 3.2 characterize complexity
nec presence instance queries.
Theorem 4.4. DL-LiteA , instance queries, unrestricted restricted
explanation signatures, nec, -nec, -nec NL-complete.
Proof. NL upper bound nec restricted signatures, observe that,
Proposition 3.2, nec reduces non-exist. Theorem 4.2, proved exist NL.
651

fiCalvanese, Ortiz, Simkus & Stefanoni

Given NL = coNL, nec NL well. NL upper bound
case unrestricted signature established using algorithm isNEC Proposition 4.3.
Indeed, given NL = coNL, non-exist coNL, checking whether
assertion entailed DL-Lite ontology coNL well, immediately obtain
isNEC runs nondeterministic logarithmic space. coNL-hardness thus also NLhardness nec follows Proposition 3.1 Theorem 4.2. addition, Proposition 3.4
states nec -nec equivalent and, thus, also -nec NL-complete. Finally,
Proposition 4.1, conclude -nec NL-complete.
4.3 Deciding Relevance
Proposition 3.3, deciding relevance assertion QAP equivalent assessing
whether QAP admits solution. already showed latter problem NL-complete
(see Theorem 4.2). Therefore, following result easily follows.
Theorem 4.5. DL-LiteA , instance queries, unrestricted restricted
explanation signatures, rel NL-complete.
next theorem, show complexity problem change even
apply minimality criterion solutions.
Theorem 4.6. DL-LiteA , instance queries, restricted unrestricted
explanation signatures, -rel -rel NL-complete.
Proof. Proposition 4.1, suffices show -rel NL-complete.
~
(membership) Let P = hT , A, q, c, QAP q IQ let (d)
~ -relevant P
ABox assertion abducible predicate . argue (d)
~
~
(i) c 6 cert(q, , A), (ii) hT , {(d)}i consistent, (iii) c cert(q, , {(d)}).
show only-if direction, since direction directly follows Proposition 4.1
~ -relevant P. definition
definition solution. Suppose (d)
minimal solution, follows c 6 cert(q, , A). Also, Proposition 4.1, follows
~ -solution P. then, c cert(q, , {(d)})
~
{(d)}

~
ontology hT , {(d)}i consistent. Since conditions (i-iii) decided
non-deterministic logarithmic space DL-Lite ontologies, conclude that, instance
queries (un)restricted explanation signatures, -rel NL.
(hardness) Hardness proved employing reduction Theorem 4.2
taking A(c) assertion shown relevant. Proposition 4.1,
hT , Ai consistent A(c) -relevant P.
4.4 Deciding Recognition
Finally, consider problem deciding whether given ABox solution QAP.
Theorem 4.7. DL-LiteA , instance queries, unrestricted restricted
explanation signatures, rec NL-complete.
Proof. (membership) Let P = hT , A, q, c, QAP (where may restricted)
q IQ let E ABox. definition solution QAP, decide
652

fiReasoning Explanations Negative Query Answers DL-Lite



-exist
unrestr.

restr.

none



PTime

NP

-nec

-rel

-rec

unrestr.

restr.

unrestr.

restr.

PTime

coNP

PTime

NP

PNP
k
PTime

PNP
k
coNP

P2

unrestr.

restr.

NP
DP

P2

DP

Table 5.1: Complexity reasoning QAPs UCQs DL-Lite . entries
table denote completeness results, except -rel unrestricted explanation signatures.

whether E expl(P) three steps: (i) check E -ABox, (ii) check hT , Ei
consistent, (iii) check c cert(q, , E). DL-Lite ontologies,
perform three steps non-deterministic logarithmic space. Thus, instance queries
restricted unrestricted signatures, rec NL.
(hardness) provide reduction consistency problem DL-Lite ontologies. Consider arbitrary ontology hT , Ai. Then, let fresh concept name
occurring ontology let c fresh individual. Furthermore, let q(x) A(x)
instance query. Finally, let P = hT , A, q, c, query abduction problem
unrestricted explanation signature let E = {A(c)} target ABox.
difficult see hT , Ai consistent iff E solution P.
Unsurprisingly, complexity change consider minimality criterion
solutions.
Theorem 4.8. DL-LiteA , instance queries, unrestricted restricted
explanation signatures, -rec -rec NL-complete
Proof. Proposition 4.1, focus -rec.
(membership) order decide whether E expl (P) first check E indeed
solution P, non-deterministic logarithmic space (see Theorem 4.7).
Then, Proposition 4.1, need check |E| 1 E empty ABox
whenever c cert(q, , A). Since instance checking DL-Lite NL, conclude
-rec NL well.
(hardness) reuse reduction consistency DL-Lite provided Theorem 4.7 show that, instance queries unrestricted explanation signatures,
-nec NL-hard. conclude that, restricted unrestricted explanation
signature, -nec -nec NL-complete.

5. Complexity Unions Conjunctive Queries
section, consider general problem reasoning query abduction
problems admit UCQs input. establish complexity various rea653

fiCalvanese, Ortiz, Simkus & Stefanoni

Algorithm 2 someExplanation
Input: QAP P = hT , A, q, ~c, i.
Output: yes iff P explanation.
1: Guess CQ qr perfect reformulation Rq,T q w.r.t. .
2: Guess ~
c-instantiation E qr .
3: E \ -ABox hT , E consistent, return yes.
4: Return no.
soning tasks problems DL-Lite , unrestricted restricted explanation signatures, different minimality criteria. results section
summarized Table 5.1.
5.1 Existence Explanations
first focus problem deciding whether query abduction problem unrestricted signature admits least one explanation.
follows Proposition 3.8 complexity problem coincides
complexity deciding consistency without UNA underlying DL. Proposition 3.5, extends -exist, -exist. Since reasoning without UNA
PTime-complete DL-Lite (Artale et al., 2009), obtain following result.
Theorem 5.1. every DL L, UCQs, unrestricted explanation signatures,
exist, -exist, -exist complexity consistency checking without
UNA L. Hence DL-LiteA , mentioned problems PTime-complete.
allow restricted explanation signatures, deciding exist becomes harder.
DL-Lite , complexity increases PTime NP.
Theorem 5.2. DL-LiteA , UCQs, restricted explanation signatures, exist,
-exist, -exist NP-complete. NP-hardness holds already following restricted settings:
1. QAPs TBox contains concept inclusions forms A1 v A2
A1 vA2 concept names A1 A2 , ABox empty, query Boolean
CQ consisting conjunction unary atoms single quantified variable.
2. QAPs empty TBox.
Proof. Proposition 3.5, sufficient show exist NP-complete.
(membership) upper bound follows guess-and-check Algorithm 2,
immediate Proposition 3.10. guesses non-deterministically CQ qr perfect
reformulation Rq,T q w.r.t. , ~c-instantiation E qr . algorithm checks
polynomial time E \ -ABox ontology hT , E consistent;
shown Calvanese et al. (2009) latter check polynomial.
(hardness) Next, provide two hardness results. first one follows directly
Proposition 3.7 hardness proof CQ query emptiness sublogic
654

fiReasoning Explanations Negative Query Answers DL-Lite

DL-LiteA known DL-Litecore given Theorem 17 Baader et al. (2010). showing hardness second setting, reduce following NP-complete problem: given
pair directed graphs G = (V, E) G0 = (V 0 , E 0 ), decide whether exists homomorphism G G0 . end, let = {e(ca , cb ) | (a, b) E 0 } ABox.
Furthermore, B arbitrary atomic concept c globally fresh individual, let
q = {e(xa , xb ) | (a, b) E} {B(c)} Boolean CQ = {B} signature. Finally, let PG,G0 = h, A, q, QAP; show exists homomorphism
G G0 iff solution PG,G0 . Indeed, homomorphism G G0 ,
{B(c)} solution P. direction, assume explanation E
P. Since binary atoms prohibited occurring E selection ,
must exist match q DB . mapping also witnesses existence
homomorphism G G0 .
5.2 Deciding Necessity
Now, consider problem checking whether assertion occurs solutions
QAP P; is, whether assertion necessary P. case restricted
explanation signatures, use reductions Section 3.1 Theorem 5.2 derive
nec -nec coNP-complete. case unrestricted explanation signatures, use procedure solving nec described Algorithm 1 show nec
-nec PTime-complete.
Theorem 5.3. DL-LiteA , UCQs, unrestricted explanation signatures, nec
-nec PTime-complete. Furthermore, restricted explanation signatures,
nec -nec coNP-complete.
Proof. Theorem 5.1 Theorem 5.2, proved problems deciding existence solution QAP unrestricted restricted explanation signatures
PTime-complete NP-complete, respectively. applying reduction Proposition 3.1, nec PTime-hard unrestricted coNP-hard
restricted explanation signatures.
upper bound, first consider case restricted explanation signatures.
Proposition 3.2, nec reduces non-exist. Theorem 5.2, latter problem
solved nondeterministic polynomial time. readily obtain nec coNP.
case unrestricted signatures, Proposition 4.3 states algorithm isNEC solves nec,
even consider UCQs input. definition, isNEC requires checking whether
polynomially many QAPs admit solution, whether polynomially many DLLite ontologies entail given assertion. Since DL-Lite , instance checking PTime
and, Theorem 5.1, non-exist PTime, conclude isNEC runs polynomial
time. Thus, nec unrestricted signatures PTime.
conclude nec PTime-complete unrestricted coNP-complete
restricted explanation signatures.
Finally, Proposition 3.4 states nec -nec equivalent and, thus, also -nec
PTime-complete unrestricted coNP-complete restricted explanation
signatures.
655

fiCalvanese, Ortiz, Simkus & Stefanoni

Now, consider complexity -nec show that, common assumptions, problem harder nec. Intuitively, one first compute
minimal size explanation, inspect explanations size.
following, use [i..j] denote integer interval {i, . . . , j}.
Theorem 5.4. DL-LiteA , UCQs, unrestricted restricted explanation signatures, -nec PNP
k -complete. hardness holds already QAPs
empty TBox CQ.
Proof. structure proof follows. First, show -nec PNP
k . Then,
NP
prove problem Pk -hard restricted signatures. Finally, argue
reduction also used particular case unrestricted signatures.
(membership) Consider arbitrary QAP P = hT , A, q, ~c, (where signature
may restricted) let arbitrary ABox assertion. Corollary 3.9, know
P explanation, exists explanation whose size bounded
max(q) = maxqi q |at(qi )|. Observe hP, negative instance -nec iff
[0..m] (a) P explanation E |E| = 6 E, (b) E
-minimal. Thus, use auxiliary problem size-out, decide given
tuple hP 0 , 0 , n0 i, P 0 QAP, 0 assertion, n0 integer, whether
exists explanation E 0 P 0 |E 0 | = n0 0 6 E 0 . Furthermore, problem
no-smaller decide, given tuple hP 0 , n0 QAP integer, whether
explanation E 0 P 0 |E 0 | < n0 . Observe size-out NP,
no-smaller coNP. Take tuple = hA0 , B0 , . . . , , Bm i, Ai = hP, , ii
Bi = hP, ii, [0..m]. Due observation, occurs -minimal
explanations E P iff [0..m], one following holds: (i) Ai negative
instance size-out, (ii) Bi negative instance no-smaller. built
polynomial time size input, whether instances instances satisfy (i)
(ii) decided making 2m parallel calls NP oracle. Thus obtain
membership PNP
k .
(hardness) give reduction OddMinVertexCover, PNP
k -complete
(Wagner, 1987). instance problem given graph G = (V, E),
asked whether least cardinality vertex covers G odd. is,
odd integer k [1..|V |] G vertex cover C |C| = k,
vertex cover C 0 G |C 0 | < k?
reduction exploit following property. Given integer k directed
graph G = (V, E) vertices, construct new graph G0 = ([1..m], E 0 )
exist two symmetric edges [1..k] j [1..m]. following holds:
injective homomorphism h G G0 , G vertex cover size k.
Indeed, take C = {v V | h(v) k}. Due injectivity, |C| = k. Assume arbitrary
edge {v1 , v2 } E. Since h homomorphism, due selection edges must
h(v1 ) k h(v2 ) k. {v1 , v2 } C 6= selection C.
Assume arbitrary graph G = (V, E) vertices V = {v1 , . . . , vm }. W.l.o.g.,
G connected, directed, least 2 nodes. construct next QAP PG =
h, A|V | , qG , hi, G assertion G G positive instance OddMinVertexCover iff G -necessary PG . reduction use individuals odd , even, cij ,
i, j [0..m], concept names , L, roles P , 6=, Edge.
656

fiReasoning Explanations Negative Query Answers DL-Lite

odd
A0
L

L

A1
L

L

L

A2
L

L

L

A3
L

A4

L

even

Figure 1: structure A|V | graph G = (V, E) 4 vertices. Solid arcs
A` represent assertions Egde(a, b) A` introduced (b). dashed arc
ABox A` individual par (`) represents collection assertions
relate individual A` par (`) via role P .
Let qG Boolean query consisting atoms
(i) Edge(xi1 , xi2 ), edge (vi1 , vi2 ) E,
(ii) 6=(xi1 , xi2 ), i1 , i2 [1..m], i1 6= i2 ,
(iii) L(x1 ), . . . , L(xm ) P (x1 , y), (y).
Intuitively, (i) represent graph G query. use atoms (ii)
ensure different variables mapped distinct elements. atoms L(xi )
used measure size vertex covers, atoms P (x1 , y) (y) used
determine parity. allow explanations concept names, thus set
G = {M, L}.
define A|V | , first construct collection A0 , . . . , ABoxes, Aj
consists assertions
(a) L(cji ), [j..m],
(b) Edge(cji1 , cji2 ), i1 , i2 [1..m] i1 j i2 j,
(c) 6=(cji1 , cji2 ), i1 , i2 [1..m] i1 6= i2 .
integer k, let par (k) = odd k odd, par (k) = even, otherwise. Let A0 =
{P (cji , par (j)) | i, j [0..m]}. A|V | = A0 A0 . See Figure 1 example.
Finally, let G = (odd ). prove correctness reduction, define
up(k) = {L(ck1 ), . . . , L(ckk ), (par (k))}, claim following:
claim 1: C vertex cover G size k, up(k) explanation PG .
Let = A|V | up(k). suffices show existence match qG DB . Take
enumeration z1 , . . . , zm variables x1 , . . . , xm {z1 , . . . , zk } = {xi | vi C}.
Take mapping (zi ) = cki [1..m], (y) = par (k). Assume
atom Edge(xi1 , xi2 ) qG . Due (b) definition Aj , suffices show
(xi1 ) = ck` (xi2 ) = ck` ` k. Indeed, since C vertex cover, vi1 C
vi2 C. due enumeration variables, xi1 = z` xi2 = z` ` k.
Due definition , (xi1 ) = ck` (xi2 ) = ck` ` k. atoms 6=(xi1 , xi2 )
qG properly mapped due (c) construction Aj fact injective
657

fiCalvanese, Ortiz, Simkus & Stefanoni

construction. atom L(xi ) qG two options. (xi ) = ck` ` k,
L(ck` ) up(k) definition up(k). Otherwise, ` > k, L(ck` ) Ak
definition Ak . atom P ((x1 ), (y)) belongs due definition A0 ,
((y)) up(k) construction up(k).
claim 2: Assume up(k) explanation PG . G vertex cover size k.
Let = A|V | up(k) let match qG DB . Observe due irreflexivity
role 6= atoms (ii) qG , must injective. Observe also ` [1..m],
` 6= k, |{c`i | L(c`i ) A` }| < m. Due connectedness G atoms
L(x1 ), . . . , L(xm ) qG , must use atoms Ak A0 up(k). is, also
match qG DB Ak A0 up(k) . Let C = {vi V | (xi ) = ckn , n [1..k]}. |C| = k
due injectivity . see C vertex cover, assume edge (vi1 , vi2 ) E.
construction, qG atom Edge(xi1 , xi2 ). Since match DB Ak A0 up(k) ,
Edge((xi1 ), (xi2 )) Ak . Then, construction Ak , (xi1 ) = ckn (xi2 ) = ckn
n k. selection C, {(xi1 ), (xi2 )} C 6= .
claim 3: Assume E -minimal explanation PG size k. E = up(k
1). Since G connected E -minimal, exist index ` [1..m]
E {L(c`1 ), . . . , L(c`m ), (par (`))} match qG A` A0 E. Since
L(c`i ) A` [`+1..m] definition A` , cardinality minimality
E {L(c`1 ), . . . , L(c`` ), (par (`))}. definition A` , |{c`i | L(c`i ) A` }| = `.
Thus, due injectivity match qG , must |{c`i | L(c`i ) E}| `.
Hence, E = {L(c`1 ), . . . , L(c`` ), (par (`))} = up(`). Since |E| = k, ` = k 1.
finalize correctness proof:
() Suppose exists odd integer k [1..|V |] G vertex cover C
|C| = k, vertex cover C 0 G |C 0 | < k. claim 1, up(k)
explanation PG . make sure up(k) -minimal. Suppose exists
explanation E 0 size |E 0 | < |up(k)|, i.e., |E 0 | = ` ` k. assume
E 0 -minimal. claim 3, E 0 = up(` 1). follows claim 2 G
vertex cover size ` 1. Since ` 1 < k, arrive contradiction assumption
G vertex cover size < k. Thus up(k) -minimal. Since k odd,
(odd ) up(k). claim 3, apart up(k) -minimal explanation
PG . is, (odd ) occurs -minimal explanations PG .
() Assume (odd ) occurs -minimal explanations PG . claim 3,
know up(k) unique -minimal explanation, integer k. Since (odd )
up(k), get k odd. Then, claim 2, vertex cover C size k.
remains ensure vertex cover C 0 size ` < k. Assume opposite.
claim 1 up(`) explanation size |up(`)| < |up(k)|,
contradicts assumption up(k) -minimal. Thus G positive instance
OddMinVertexCover.
definition G prohibits binary atoms occurring -minimal explanations.
effect achieved using G = (, A|V | , qG ) modifying A|V | qG
make prohibitively expensive binary atoms -minimal explanations. Simply
replace binary assertion r(c, d) A|V | fresh assertions r1 (c, d), . . . , rm+2 (c, d),
binary r(x, y) qG r1 (x, y), . . . , rm+2 (x, y). way lower-bound
shown unrestricted explanation signatures.
658

fiReasoning Explanations Negative Query Answers DL-Lite

5.3 Deciding Relevance
Using Theorems 5.1 5.2, reductions Section 3, obtain following results.
Theorem 5.5. DL-LiteA , UCQs, unrestricted explanation signatures, rel
PTime-complete. restricted explanation signatures, rel NP-complete.
Unsurprisingly, UCQs, -rel complexity -nec. Indeed, two
problems share source complexity, namely need inspect explanations
computed size, allows us reduce OddMinVertexCover problem.
fact, PNP
k -hardness shown using reduction proof Theorem 5.4,
matching upper bound obtained slightly modifying algorithm -nec.
Theorem 5.6. DL-LiteA , UCQs, unrestricted restricted explaNP
nation signatures, -rel PNP
k -complete. Pk -hardness holds already QAPs
empty TBox CQ.
Proof. First, show that, restricted explanation signatures, problem -rel
NP
PNP
k . Second, argue that, unrestricted explanation signatures, -rel Pk -hard.
(membership) -rel tackled way similar -nec. fact, algorithm
described Theorem 5.4 modified order solve problem. Let size-in solve
following problem: given tuple hP, , ni, P QAP, assertion, n
integer, decide whether exists explanation E, |E| = n E. Then,
change positivity condition -nec algorithm follows: occurs
-minimal explanation E P iff [0..m] holds that: (i) Ai positive
instance size-in, (ii) Bi positive instance no-smaller. easy see
size-in solvable NP, hence whole problem PNP
k .
(hardness) Recall reduction OddMinVertexCover -nec proof
Theorem 5.4. argue exactly reduction also shows PNP
k -hardness
-rel. Assume directed graph G let PG G QAP assertion
resulting reduction. prove claim suffices show following equivalence:
G -necessary PG iff G -relevant PG . equivalence follows directly
claim 3, states PG unique -minimal explanation.
turn attention -rel. problem obtain precise complexity
characterization case restricted explanation signatures, leave open
whether unrestricted signatures P2 upper bound shown tight.2 note
latter case, coNP lower bound easily shown, instance,
reduction non-existence homomorphism two graphs.
Theorem 5.7. DL-LiteA , UCQs, unrestricted restricted explanation signatures, -rel P2 . restricted explanation signatures, -rel P2 -hard,
hardness holds already QAPs empty TBox CQ.
Proof. (membership) Let P = hT , A, q, ~c, QAP let ABox assertion.
provide extended version algorithm solving existence, decides whether
2. proof P2 lower bound unrestricted signatures Theorem 2 Calvanese, Ortiz, Simkus,
Stefanoni (2011) incorrect.

659

fiCalvanese, Ortiz, Simkus & Stefanoni

-relevant P. Let has-subexpl solve problem deciding whether given explanation E subset explanation. modified algorithm, similarly
Algorithm 2, first non-deterministically guess CQ qr perfect reformulation Rq,T
q w.r.t. ~c-instantiation E qr E . Additionally consistency
test checking E -ABox, also check complement has-subexpl
E, order assure E -minimal. follows -relevant. Since checking
complement has-subexpl done coNP, problem solvable P2 .
(hardness) reduce P2 -complete problem non-cert3col (Stewart, 1991, see
also Bonatti, Lutz, & Wolter, 2009). instance non-cert3col given graph
G = (V, E) vertices V = {1, . . . , n} every edge labelled disjunction
two literals Boolean propositions {p(i,j) | 1 i, j n}. say edge e E
evaluates true truth assignment satisfies disjunction labelling e. Then,
graph G positive instance non-cert3col iff truth assignment exists
graph (G)obtained G including edges evalute true
3-colorable. Assume instance G non-cert3col. show build
polynomial time QAP PG = hTG , AG , qG , ~cG , G ABox assertion G . first
present relevant definitions, discuss intuition behind reduction
prove correctness.
construction, use empty TBox Boolean CQ, thus TG = ~cG = hi.
order define ABox AG , let L function assigns edge e E
set {l1 , l2 } literals occurring label. Moreover, let T(e) (resp., F(e)) set
containing truth assignment literals L(e) edge e evaluates true
(resp., false) . Finally, truth assignment literal l occurring
G, define image l w.r.t. , written img (l), follows.
(
l (l) =
img (l) :=
l otherwise
ready define ABox AG . definition, use individuals a1 , . . . , a4 ;
moreover, literal l G, use individuals l l denote ls truth value. Also,
1 k ` 4, edge e E, truth assignment T(e) F(e), let
e,
k,`
fresh individual. ABox AG consists four distinct components , AtT , AfT ,
AC introduce next.
={d(l, l), d(l, l) | literal l occurs G}
{B(ak ) | 1 k 3}
e,
e,
AtT ={Re (ak , k,`
), (k,`
, a` ) | e E, T(e), 1 k < ` 3}
e,
{P (k,`
, img (l)) | e E, T(e), l L(e), 1 k < ` 3}
e,
e,
AfT ={Re (ak , k,`
), (k,`
, a` ) | e E, F(e), 1 k ` 3}
e,
{P (k,`
, img (l)) | e E, F(e), l L(e), 1 k ` 3}
e,
e,
AC ={Re (a4 , 4,4
), (4,4
, a4 ) | e E, T(e) F(e)}
e,
{P (4,4
, img (l)) | e E, T(e) F(e), l L(e)}

660

fiReasoning Explanations Negative Query Answers DL-Lite

Next, define Boolean query qG . end, vertex V , let xi
distinct variable; edge hi, ji E, let yi,j distinct variable; and, literal
l occurring G, let zl zl two distinct variables. Then, edge hi, ji E, let
qG contain following atoms.
{B(xi ), (xi , yi,j ), (yi,j , xj ), B(xj )} {P (yi,j , zl ), Al (zl ), d(zl , zl ) | l L(e)}
Finally, let G = B(a4 ) assertion want show relevant let
G = {Al | literal l occurs G} {B} signature.
Now, outline main idea behind construction. ABox AG encodes two structures: triangular structure AtT AfT cyclic structure AC . former structure
individuals a1 , a2 , a3 edges G evaluate true according
arbitrary truth assignment mapped non-reflexive edges (cf. AtT ). contrast, edges G evaluate false according mapped arbitrary edge
(cf. AfT ). latter, cyclic, structure AC individual a4 (which asserted
member B) G mapped AC possible truth assignments.
Query qG obtained graph G requiring vertex graph
member concept B, reifying edges graph, incorporating disjunction
literals. particular, literal l G, variables zl zl represent truth
values l atom Al (zl ) used enforce particular truth assignment. Since ABox AG
contain assertions concept Al , minimal explanation E PG corresponds
truth assignment G. is, E contains, literal l G, either Al (l)
Al (l). Also, definition ABox, query qG mapped AtT AfT
minimal explanation E implies (G) 3-colorable. contrast, every truth
assignment , map query qG cyclic structure AC , provided explanation
E asserts individual a4 member B. ready formally prove
correctness reduction.
() Suppose truth assignment (G) 3-colorable; show
assertion B(a4 ) -relevant PG . Consider -ABox E = {B(a4 )} E ,
E = {Al (l) | (l) = t} {Al (l) | (l) = f }. Clearly, E explanation. Indeed,
match query qG cyclic structure AC mapping variables xi qG
(interpretation of) a4 . Suppose smaller explanation E 0 E. Observe E E 0 .
because, literal l, concept Al occur AG occur qG .
Then, E \ {B(a4 )} must explanation. qG matched triangular
structure encoded AG . Thus, (G) 3-colorable contradicts assumption.
() Let E -minimal explanation PG containing B(a4 ); show
exists truth assignment (G) 3-colorable. first argue
literal l either Al (l) E Al (l) E. follows three considerations.
First, due signature restriction, predicate cannot occur E. Second, literal
l, query qG contains atoms Al (zl ) d(zl , zl ), whereas ABox AG contains assertions d(l, l)
d(l, l). Third, literal l, concept Al occurs qG one variable
zl . Therefore, since E minimal solution, know exactly one Al (l) E
Al (l) E holds. Next, define truth assignment literals occurring G.
literal l G, let (l) = Al (l) E, (l) = f Al (l) E. difficult
argue t(G) 3-colorable thus G positive instance non-cert3col.
661

fiCalvanese, Ortiz, Simkus & Stefanoni

Indeed, (G) 3-colorable, qG could mapped triangle structure AG
making E \ {B(a4 )} smaller explanation, contradiction.
5.4 Recognizing Explanations
Unsurprisingly, UCQs unrestricted restricted explanation signatures,
rec NP. Indeed, order solve problem, need check consistency
explanation ontology, check whether given tuple certain answer
query. former polynomial latter NP.
Theorem 5.8. DL-LiteA , UCQs, restricted unrestricted explanation signatures, rec NP-complete. NP-hardness holds already QAPs
empty TBox CQ.
Proof. usual, first show that, (un)restricted explanation signatures, rec
NP. Then, argue that, unrestricted explanation signatures, problem
NP-hard.
(membership) Given QAP P = hT , A, q, ~c, ABox E, devise algorithm
deciding rec follows. Firstly, procedure checks E indeed -ABox; check
linear E. makes sure extending ontology ABox E lead
inconsistent theory; checked polynomial time (Artale et al., 2009).
last, decides whether ~c occurs cert(q, , E); Proposition 2.1 feasible
NP. Hence overall algorithm runs non-deterministic polynomial time.
(hardness) use essentially reduction existence homomorphism directed graphs G G0 proof Theorem 5.2, difference
instead reducing existence explanation signature
= {B}, leave signature unrestricted (that is, = (T , A, q)), reduce
problem deciding whether E = {B(c)} explanation.
case preference order place, recognize explanation one check minimality well. check coNP-hard - -minimality, leading completeness
DP.
Theorem 5.9. DL-LiteA , UCQs, restricted unrestricted explanation signatures, -rec -rec DP-complete. DP-hardness holds
already QAPs empty TBox CQ.
Proof. first argue that, (un)restricted explanation signatures, two problems
DP. Then, unrestricted explanation signatures, prove -rec
-rec DP-hard.
(membership) Membership problem DP shown providing two
languages L1 NP L2 coNP, set yes-instances L1 L2 .
-rec, simply let
L1 = {(P, E) | E expl(P)}
L2 = {(P, E) | P explanation E 0 s.t. |E 0 | < |E|}
-rec, take L1 L2 = {(P, E) | P explanation E 0 s.t. E 0 ( E}.
662

fiReasoning Explanations Negative Query Answers DL-Lite

(hardness) DP-hardness shown reduction problem HP-noHP.
instance HP-noHP given two directed graphs G = (V, E) G0 = (V 0 , E 0 ),
hG, G0 positive instance iff G Hamilton path G0 one.
pair hG, G0 i, define QAP P = h, A, q, hi, -ABox E that:
(a) hG, G0 positive instance HP-noHP iff E -minimal explanation P,
(b) hG, G0 positive instance HP-noHP iff E -minimal explanation P.
W.l.o.g., nodes G G0 disjoint ordinary individuals. Construct ABox
AG = {e(vi , vj ) | (vi , vj ) E} {d(vi , vj ) | vi , vj V, vi 6= vi }. Intuitively, assertion
e(vi , vj ) encodes edge (vi , vj ) graph G, whereas assertion d(vi , vj ) encodes
nodes vi vj distinct. ABox AG0 encodes G0 similar way before, using roles
e0 instead e, addition assertion A(vi0 ) vi0 V 0 . Take set fresh
individuals = {o1 , . . . , o|V 0 | } ABox AC = {e0 (oi , oj ), d(oi , oj ) | 1 6= j |V 0 |}.
ABox P defined = AG AG0 AC .
Let q = q1 q10 q2 q20 q3 Boolean CQ
q1
q10
q2
q20
q3

=
=
=
=
=

{e(x1 , x2 ), e(x2 , x3 ), . . . , e(x|V |1 , x|V | ))},
{d(xi , xj ) | vi , vj V, vi 6= vj },
{e0 (y1 , y2 ), e0 (y2 , y3 ), . . . , e0 (y|V 0 |1 , y|V 0 | )},
{d(yi , yj ) | vi0 , vj0 V 0 , vi0 6= vj0 },
{A(y1 ), . . . , A(y|V 0 | )}.

Intuitively, q1 q10 asks simple path |V | vertices related via role e. Analogously,
q2 q20 asks simple path |V 0 | vertices related via role e0 . Additionally, q3 asks
node latter path satisfies A.
Finally, let E = {A(oi ) | oi O} let = (T , A, q).
() Assume hG, G0 positive instance HP-noHP, let a1 , . . . a|V |
Hamilton path G. show E -minimal -minimal explanation P.
end, first take mapping variables q (x1 ) = a1 , . . . , (x|V | ) = a|V |
(y1 ) = o1 , . . . , (y|V 0 | ) = o|V 0 | . clearly match q DB AE , hence
E explanation P. Indeed, subquery q1 q10 q fulfilled a1 , . . . a|V |
Hamilton path G, q2 q20 fulfilled AC clique size |V 0 |, q3
fulfilled E. assure minimality, assume towards contradiction
explanation E 0 |E 0 | < |E| |E 0 | |E|. case, |E 0 | < |V 0 |. Assume 0 match
q DB AE 0 . Note AG AG0 share individuals. Since q3 q20 asks
|V 0 | elements satisfying |E 0 | < |V 0 |, 0 must map variables y1 , . . . , y|V 0 | |V 0 |
distinct individuals AG0 . presence q2 q implies existence Hamilton
path G0 . Contradiction.
() Assume E expl (P) (resp., E expl (P)) match q DB AE .
Note e0 occur AG e occur AG0 AC . construction
q1 q10 AG , maps variables x1 , . . . , x|V | |V | distinct constants AG
G must Hamilton path. Towards contradiction suppose G0 also Hamilton
path. construction AG0 , q2 q20 q3 match DB AG0 . means
build match 0 q DB AG0 , turn means explanation P.
contradicts assumption E -minimal (resp., -minimal).
663

fiCalvanese, Ortiz, Simkus & Stefanoni

6. Discussion
section, discuss issues remain investigation.
6.1 Computing Explanations
complexity analysis DL-Lite , considered problem computing
solutions query abduction problems. Nevertheless, infer upper bounds
complexity computing solutions QAP P presented results. input
query P instance query, computing arbitrary solution computing
minimal3 solutions easy, since Proposition 3.10, need consider singleton candidate explanations, number polynomially bounded. problem
computing arbitrary solution E remains polynomial UCQs signature P
unrestricted, since always obtain E creating suitable direct instantiation
one CQs input (see Section 3.3). Instead, restricted signatures, total
number (minimal) solutions general exponential size signature
maximal size query occurring input UCQ; computing
requires general exponential time. remains investigated whether solutions
enumerated polynomial delay (cf., Penaloza & Sertkaya, 2010). case
restricted signature, however, NP-harness result established Theorem 5.2 implies
compute solution E one essentially essentially cannot better guessing
ABox E deciding whether E expl (P).
6.2 Data Complexity
work focused combined complexity. respect data complexity
(i.e., complexity measured respect size ABox only,
query TBox considered fixed) ontology complexity (i.e.,
query considered fixed), observe inference tasks shown
NP-complete essentially rely checking ontology consistency, hence AC0 data
complexity (Calvanese et al., 2009). Moreover, Corollaries 3.9 3.11, one restrict
attention explanations bounded size query, follows
fixed query, polynomially many explanations considered. Hence
reasoning tasks polynomial data complexity ontology complexity.
6.3 Description Logics.
lower bounds proved paper rely properties exclusive
DL-Lite , hence hold DLs well. fact, mentioned, many
lower bounds hold even absence TBox. upper bounds,
relied DL-Lite existence perfect reformulation given query (see
Proposition 2.1) argue canonical explanations small restricted
signature (more specifically, obtained instantiating CQs perfect
reformulation input query) query answering done NP.
reason, expect results carry DLs admit small explanations
3. Since every ABox superset solution solution, dont impose minimality
condition, always exponential number solutions, provided one exists.

664

fiReasoning Explanations Negative Query Answers DL-Lite

query answering NP. instance, lower upper bounds
established hold OWL 2 QL, obtained DL-Lite forbidding
functionality assertions dropping unique name assumption (as results
rely functionality axioms, unique name assumption irrelevant).
expressive DLs, bounds complexity reasoning tasks also
inferred. Corollary 5.1, showed QAPs unrestricted explanation signatures, deciding existence explanation complexity ontology consistency without UNA. Hence, problem ExpTime-complete extensions
ALC standard reasoning (with without UNA) also ExpTime-complete,
like well known SHIQ. consider restricted explanation signatures, problem
becomes significantly harder. witnessed lower bounds Baader et al. (2010)
stemming CQ-emptiness (see Proposition 3.6): exist already 2ExpTime hard
ALCI (Theorem 28 Baader et al., 2010), undecidable ALCF (Theorem 29).
ALC, authors recently improved lower bound CQ-emptiness ExpTime
NExpTime (personal communication). mentioned Section 3, upper bounds
apply directly setting (although expect extend),
precise characterization reasoning problems considered paper expressive
DLs remains open.

7. Related Work
problem explaining missing query answers first considered database
community (Jagadish, Chapman, Elkiss, Jayapandian, Li, Nandi, & Yu, 2007).
literature, found three different models explanation missing answers, differ
notion solution. First, Chapman Jagadish (2009) proposed model
explanations relational operations (e.g., natural joins selections)
responsible preventing given tuple returned answers. Second, Tran
Chan (2010) defined solutions refinements input query given
tuple answer relaxed query database. Third, Huang, Chen, Doan,
Naughton (2008) defined solutions sequences database update operations
result answering given conjunctive query updated relational
instance includes missing answer. Herschel Hernandez (2010) generalized
latter model considering UCQs aggregation grouping. Although
explanation model closely related ours, work Huang et al. Herschel
Hernandez tackle problem point view computing solutions, whereas
interested outlining computational complexity problem. Moreover,
spirit abductive reasoning, solutions declarative rather operational
naturethat is, solutions databases rather sequence database operations.
classical logic, abductive reasoning form non sequitur argument,
conclusion B logical consequence premises ( 6|= B), even though B
assumed follow theory (Eiter & Gottlob, 1995). aim find set
formulas |= B. Abductive reasoning important also context
Description Logics (Elsenbroich, Kutz, & Sattler, 2006), three orthogonal abductive
problems studied. First, abduction studied explain conceptsthat
is, given two concepts C TBox , concept abduction amounts finding
665

fiCalvanese, Ortiz, Simkus & Stefanoni

concept H |= C u H v C u H satisfiable w.r.t. (Noia, Sciascio, &
Donini, 2009; Bienvenu, 2008). Second, Hubauer, Grimm, Lamparter, Roshchin (2012)
applied TBox abductive reasoning diagnosis complex systems. particular,
given TBox , set abducible axioms Ax , set axioms O, TBox abduction
amounts finding subset Ax |= O. Third, Klarman, Endriss,
Schlobach (2011) studied problem ABox abduction ALC ontologies.
problem consists finding additions need made ABox order force
set ABox assertions logically entailed ontology. Along line, Du,
Qi, Shen, Pan (2011a) considered problem practical perspective.
recently, Du, Wang, Qi, Pan, Hu (2011b) defined problem abductive
conjunctive query answering, use basis new approach semantic
matchmaking. Given satisfiable DL ontology CQ q, tuple ~c called abductive
answer q w.r.t. exists set E ABox assertions E |= q(~c).
Similarly approach, authors allow restrict signature abductive
solutions constructed. addition, one limit impact E specifying
set closed predicates; assertion closed predicate require
E |= |= . main contribution paper procedure
computing abductive answers CQs ontologies formulated DLP fragment
OWL 2, fragment orthogonal DL-Lite terms expressiveness. Considering
closed predicates context DL-Lite QAPs interesting research direction.

8. Conclusions
paper studied problem explaining negative answers user queries
DL-Lite ontologies. formalized problem abductive task: given
(U)CQ q, consistent ontology tuple constants ~c ~c
certain answers q O, explanation defined set ABox assertions that,
added O, preserve consistency result ~c certain answers.
considered special cases allowing restricted signature assertions
explanation, instance query rather full (U)CQ
input. also considered preference orders explanations, studied two
orders: subset minimal cardinality minimal explanations. cases,
obtained complexity bounds four decision problems inspired knowledge base
abduction: deciding existence explanation (exist), deciding whether given assertion
occurs (nec) (rel) explanations, recognizing explanations (rec).
complexity bounds tight, exception rel subset minimal explanations
unrestricted signatures, leave open gap coNP-hardness
membership P2 .
Specifically, shown case instance queries decision problems tractable, fact NL-complete, even restricted explanations signatures
preference orders simultaneously considered. picture significantly different
(U)CQs, results Table 5.1 show. Indeed, tractability always lost soon
one considers restricted explanations signatures. signatures restricted, considering preference order also results intractability cases, exceptions
exist, always tractable, nec, polynomial subset minimal
666

fiReasoning Explanations Negative Query Answers DL-Lite

explanations PNP
k cardinality minimal ones. contrast nec, rel harder,
common assumptions, subset minimal cardinality minimal explanations. rec
hard even explanations signature restricted preference order
considered.
would interesting apply framework lightweight description logics,
starting EL-family. Also, would like investigate minimality
criteria. instance, semantic criteria allow one reward explanations less/more
constraining terms models ontology.
Acknowledgments
authors would like thank anonymous referees careful reading
submitted manuscript valuable comments. work partially supported
Austrian Science Fund (FWF) grants P20840 T515, EU FP7 projects ACSI
(Artifact-Centric Service Interoperation), grant agreement n. FP7-257593, Optique
(Scalable End-user Access Big Data), grant agreement n. FP7-318338, AlcatelLucent EPSRC.

References
Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). DL-Lite family
relations. J. Artificial Intelligence Research, 36, 169.
Baader, F., Bienvenu, M., Lutz, C., & Wolter, F. (2010). Query predicate emptiness
description logics. Proc. 12th Int. Conf. Principles Knowledge
Representation Reasoning (KR 2010).
Bienvenu, M. (2008). Complexity abduction EL family lightweight description
logics. Proc. 11th Int. Conf. Principles Knowledge Representation
Reasoning (KR 2008), pp. 220230. AAAI Press.
Bonatti, P. A., Lutz, C., & Wolter, F. (2009). complexity circumscription description logics. J. Artificial Intelligence Research, 35, 717773.
Borgida, A., Franconi, E., & Horrocks, I. (2000). Explaining ALC subsumption. Proc.
14th Eur. Conf. Artificial Intelligence (ECAI 2000).
Borgida, A., Calvanese, D., & Rodriguez-Muro, M. (2008). Explanation DL-Lite family description logics. Proc. 7th Int. Conf. Ontologies, DataBases,
Applications Semantics (ODBASE 2008), Vol. 5332 Lecture Notes Computer
Science, pp. 14401457. Springer.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodriguez-Muro, M.,
& Rosati, R. (2009). Ontologies databases: DL-Lite approach. Tessaris,
S., & Franconi, E. (Eds.), Semantic Technologies Informations Systems 5th Int.
Reasoning Web Summer School (RW 2009), Vol. 5689 Lecture Notes Computer
Science, pp. 255356. Springer.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning efficient query answering description logics: DL-Lite family. J.
Automated Reasoning, 39 (3), 385429.
667

fiCalvanese, Ortiz, Simkus & Stefanoni

Calvanese, D., Ortiz, M., Simkus, M., & Stefanoni, G. (2011). complexity conjunctive
query abduction DL-Lite. Proc. 24th Int. Workshop Description Logic
(DL 2011), Vol. 745 CEUR Electronic Workshop Proceedings, http://ceur-ws.
org/.
Chapman, A., & Jagadish, H. V. (2009). not?. Proc. ACM SIGMOD Int.
Conf. Management Data, pp. 523534.
Du, J., Qi, G., Shen, Y.-D., & Pan, J. Z. (2011a). Towards practical ABox abduction
large OWL DL ontologies. Proc. 25th AAAI Conf. Artificial Intelligence
(AAAI 2011). AAAI Press.
Du, J., Wang, S., Qi, G., Pan, J. Z., & Hu, Y. (2011b). new matchmaking approach
based abductive conjunctive query answering. Proc. Joint Int. Semantic
Tech. Conf. (JIST 2011), pp. 144159.
Eiter, T., & Gottlob, G. (1995). complexity logic-based abduction. J. ACM,
42 (1), 342.
Elsenbroich, C., Kutz, O., & Sattler, U. (2006). case abductive reasoning
ontologies. Proc. 2nd Int. Workshop OWL: Experiences Directions (OWLED 2006), Vol. 216. CEUR Electronic Workshop Proceedings, http:
//ceur-ws.org/.
Herschel, M., & Hernandez, M. A. (2010). Explaining missing answers SPJUA queries.
Proc. VLDB Endowment, 3 (1), 185196.
Horridge, M., Parsia, B., & Sattler, U. (2008). Laconic precise justifications OWL.
Proc. 7th Int. Semantic Web Conf. (ISWC 2008), Vol. 5318 Lecture Notes
Computer Science, pp. 323338. Springer.
Huang, J., Chen, T., Doan, A., & Naughton, J. (2008). provenance non-answers
queries extracted data. Proc. VLDB Endowment, 1 (1), 736747.
Hubauer, T., Grimm, S., Lamparter, S., & Roshchin, M. (2012). diagnostics framework
based abductive description logic reasoning. Proc. IEEE Int. Conf.
Industrial Technology, (ICIT 2012), pp. 1047 1054.
Jagadish, H. V., Chapman, A., Elkiss, A., Jayapandian, M., Li, Y., Nandi, A., & Yu, C.
(2007). Making database systems usable. Proc. ACM SIGMOD Int. Conf.
Management Data, pp. 1324.
Klarman, S., Endriss, U., & Schlobach, S. (2011). ABox abduction description logic
ALC. J. Automated Reasoning, 46 (1), 4380.
McGuinness, D. L., & Borgida, A. (1995). Explaining subsumption description logics.
Proc. 14th Int. Joint Conf. Artificial Intelligence (IJCAI 1995), pp. 816821.
McGuinness, D. L., & Patel-Schneider, P. F. (1998). Usability issues knowledge representation systems. Proc. 15th Nat. Conf. Artificial Intelligence (AAAI 1998),
pp. 608614. AAAI Press/The MIT Press.
Motik, B., Fokoue, A., Horrocks, I., Wu, Z., Lutz, C., & Grau, B. C. (2009). OWL 2 Web
Ontology Language Profiles. W3C Recommendation, World Wide Web Consortium.
668

fiReasoning Explanations Negative Query Answers DL-Lite

Noia, T. D., Sciascio, E. D., & Donini, F. M. (2009). tableaux-based calculus abduction
expressive description logics: Preliminary results. Proc. 22rd Int. Workshop
Description Logic (DL 2009), Vol. 477. CEUR Electronic Workshop Proceedings,
http://ceur-ws.org/.
Papadimitriou, C. H. (1994). Computational Complexity. Addison Wesley Publ. Co.
Penaloza, R., & Sertkaya, B. (2010). Complexity axiom pinpointing DL-Lite
family description logics. Proc. 19th Eur. Conf. Artificial Intelligence
(ECAI 2010), pp. 2934. IOS Press.
Stewart, I. A. (1991). Complete problems involving boolean labelled structures projection transactions. J. Logic Computation, 1 (6), 861882.
Tran, Q. T., & Chan, C.-Y. (2010). ConQueR why-not questions. Proc.
ACM SIGMOD Int. Conf. Management Data, pp. 1526.
Vardi, M. Y. (1982). complexity relational query languages. Proc. 14th
Symp. Theory computing (STOC 1982), pp. 137146.
Wagner, K. W. (1987). complicated questions maxima minima,
closures NP. Theoretical Computer Science, 51 (12), 5380.

669

fiJournal Artificial Intelligence Research 48 (2013) 733-782

Submitted 04/13; published 11/13

Unsupervised Sub-tree Alignment
Tree-to-Tree Translation
Tong Xiao
Jingbo Zhu

xiaotong@mail.neu.edu.cn
zhujingbo@mail.neu.edu.cn

College Information Science Engineering
Northeastern University
3-11, Wenhua Road, Heping District
Shenyang, China

Abstract
article presents probabilistic sub-tree alignment model application
tree-to-tree machine translation. Unlike previous work, resort surface heuristics expensive annotated data, instead derive unsupervised model infer
syntactic correspondence two languages. importantly, developed model
syntactically-motivated rely word alignments. by-product,
model outputs sub-tree alignment matrix encoding large number diverse alignments
syntactic structures, machine translation systems efficiently extract translation rules often filtered due errors 1-best alignment.
Experimental results show proposed approach outperforms three state-of-the-art
baseline approaches alignment accuracy grammar quality. applied
machine translation, approach yields +1.0 BLEU improvement -0.9 TER reduction NIST machine translation evaluation corpora. tree binarization
fuzzy decoding, even outperforms state-of-the-art hierarchical phrase-based system.

1. Introduction
Recent years witnessed increasing interest syntax-based methods many Artificial Intelligence (AI) Natural Language Processing (NLP) applications ranging
text summarization Machine Translation (MT). particular, syntax-based models
intensively investigated Statistical Machine Translation (SMT). Approaches include
string-to-tree MT (Galley, Hopkins, Knight, & Marcu, 2004; Galley, Graehl, Knight, Marcu,
DeNeefe, Wang, & Thayer, 2006), tree-to-string MT (Liu, Liu, & Lin, 2006; Huang, Kevin,
& Joshi, 2006) tree-to-tree MT (Eisner, 2003; Zhang, Jiang, Aw, Li, Tan, & Li, 2008;
Liu, Lu, & Liu, 2009a; Chiang, 2010), train tree-string/tree-tree pairs
seek model translation equivalency relations learned parsed data. part
focus syntax-based MT, tree-to-tree models use synchronous context free grammars synchronous tree substitution grammars received growing interest, showing
promising results several well-established evaluation tasks (Zhang et al., 2008; Liu
et al., 2009a; Chiang, 2010). example, recent studies (Chiang, 2010) demonstrated
modern tree-to-tree systems significantly outperform hierarchical phrase-based
counterpart large scale Chinese-English Arabic-English translation.
tree-to-tree MT, translation problem broadly regarded transformation
source-language syntax tree target-language syntax tree. model process,
c
2013
AI Access Foundation. rights reserved.

fiXiao & Zhu

tree-to-tree systems resort general framework synchronous grammars,
pair trees generated derivations synchronous grammar rules (or translation
rules). model, goal translation build underlying derivations
pairs trees output target string encoded likely derivation. Figure
1 shows intuitive example illustrate generation process tree pair using
sample grammar, source target-language sentences associated
phrase structure trees generated using automatic parsers.1
Previous work shown acquisition good translation rules one essential factors contributing success syntax-based systems (DeNeefe, Knight, Wang,
& Marcu, 2007). date, several research groups addressed issue rule acquisition designed effective algorithms extract high-coverage grammars bilingual
parsed data (Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010). Despite differences
detailed modeling, approaches rely syntactic alignments align tree nodes
syntactic parse tree one language tree nodes other, alignments
could employed standard tree-to-tree rule extraction algorithms (Liu et al., 2009a;
Chiang, 2010).
current tree-to-tree models heavily depend syntactic alignments two
languages, alignments induced indirectly word alignments tree-to-tree
systems sensitive word alignment behavior. Unfortunately, word alignments
general far perfect viewpoint syntactic alignment (Fossum, Knight,
& Abney, 2008). cases, even one spurious word alignment prevent large
number desirable rules extraction. example, Figure 2(a) shows tree-totree translation rules extracted using word alignment produced GIZA++.
alignment incorrectly aligns source word (a past tense marker Chinese)
target word the. spurious word alignment produces incorrect rule AS()
DT(the) blocks extraction high-level syntactic transfer rules,
IP(NN1 VP2 ) S(NP1 VP2 ).
Obviously, desirable solution directly infer node correspondences
source target parse trees, namely sub-tree alignment. syntactic parse trees
explain underlying structure sentences well, performing alignment sub-tree level
make benefits high-level structural information syntactic categorization.
example, consider alignment Figure 2(b). links nodes two parse
trees (in Chinese English), rather aligning word level. example,
confident align VP sub-tree (spanning ) source tree
VP sub-tree (spanning drastically fallen) target tree.2 therefore
1. phrase structure tree, leaf nodes words sentence. internal tree nodes followed
leaf nodes labeled Part-Of-Speech (POS) tags, tree nodes labeled syntactic
categories defined treebanks (see appendix meanings POS tags syntactic categories used
work). NLP, many well-developed parsers available automatic parsing. Also, several
good-quality phrase structure treebanks across languages used train parsing models,
Penn English Chinese Treebanks (Marcus, Santorini, & Marcinkiewicz, 1993; Xue, Xia, Chiou,
& Palmer, 2005). Note addition phrase structure syntax, popular formalisms
(e.g., dependency syntax) used syntax-based MT. discussion different formalisms
syntactic parsing beyond scope article. instead focus tree-to-tree MT based
phrase structure trees throughout work.
2. Chinese English follow subject-verb-object structure. verb phrases Chinese
sentence frequently aligned verb phrases English translation.

734

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

NP

VP

NP

VP

PRP

VBD





Target-language Side
(English)



VP
VBN

PP

satisfied
PP


NP



r1

r4

DT

NNS



answers

r2


(ta)


(huida)

P

NN


(biaoshi)


(manyi)

VV

NN

PP

PN

PP

VP

NP

VP

NP

VP

Source-language Side
(Chinese)

r3

(dui)

IP

Synchronous Grammar Used
ID
r1
r2
r3
r4

Source-language Side
NP(PN())
PP(P() NN())
VP(PP1 VP(VV() NN()))
IP(NP1 VP2 )

Target-language Side
NP(PRP(he))
PP(IN(with) NP(DT(the) NNS(answers)))
VP(VBD(was) VP(VBN(satisfied) PP1 ))
S(NP1 VP2 )

Figure 1: Example derivation tree-to-tree translation rules. rules represented
aligned pairs tree-fragments (linked dotted lines). subscripts
language sides grammar rules indicate alignments frontier nonterminals. language side derivation, round-head lines link
frontier non-terminals rewritten translation.
know child nodes source-language VP likely aligned child
nodes target-language VP. means two VPs aligned,
children aligned outside VP sub-tree structure, i.e., prevent
alignment Chinese tree node English tree node DT due
inconsistency VP-VP alignment. case, correctly aligned VBP.
735

fiXiao & Zhu




NP
DT

NP

VP

NNS

VBP

DT

ADVP

imports

RB

VP

NNS

VBP

ADVP

imports

VBN

RB

drastically fallen

drastically fallen








VV



AD

NN

VBN




VP



VV



AD

NN

VP



VP
VP

IP

IP

(Minimal) Rules Extracted

(Minimal) Rules Extracted
r1

AS() DT(the)

r3

AD() RB(drastically)

r2

NN() NNS(imports)

r4

VV() VBN(fallen)

r3

AD() RB(drastically)

r6

AS() VBP(have)

r4

VV() VBN(fallen)

r7

NN() NP(DT(the) NNS(imports))

r5

IP(NN1 VP(AD2 VP(VV3 AS4 )))

r8

VP(AD1 VP(VV2 AS3 ))
VP(VBP3 ADVP(RB1 VBN2 ))

S(NP(DT4 NNS1 ) VP(VBP(have) ADVP(RB2 VBN3 )))

r9

(a) word alignment extracted rules

IP(NN1 VP2 ) S(NP1 VP2 )

(b) sub-tree alignment extracted rules

Figure 2: Tree-to-tree translation rules extracted via word alignment (a) sub-tree alignment (b). dashed lines represent word alignment links, dotted lines
represent sub-tree alignment (or node alignment) links.
result, bad rule AS() DT(the) ruled out, desirable rules
extracted using sub-tree alignment (including desirable rules blocked
Figure 2(a)).
Actually, researchers aware sub-tree alignment problem tried
explore solutions (Tinsley, Zhechev, Hearne, & Way, 2007; Sun, Zhang, & Tan, 2010b,
2010a). example, proposed judge whether two nodes aligned not.
work, alignment confidence first calculated using lexical translation probabilities
classifiers trained labeled data, final alignment determined according
node-level alignment score. However, inference sub-tree alignment approaches
relies heuristic algorithms, models essentially optimized within unified
probabilistic framework.
Moreover, alignment result applied tree-to-tree translation, systems
suffer another problem translation rules extracted using 1-best alignment
(Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010). problem significantly affects
736

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

rule-set coverage rate due alignment errors. simple solution issue use
k-best alignments instead. However, k-best alignments often variations many
redundancies. differ alignment links. obviously inefficient
extract rules similar alignments.
article address sub-tree alignment issue principled way investigate
methods effectively apply sub-tree alignment result tree-to-tree MT. particular,
develop unsupervised approach learning probabilistic sub-tree alignment
model bi-lingual parsed data.
investigate different methods integrating sub-tree alignment tree-to-tree
machine translation. Specifically, develop sub-tree alignment matrix encoding
exponentially large number diverse sub-tree alignments, extract multiple
alternative translation rules using alignment posteriors sub-tree alignment
matrix.
advantages approach three-fold. First, approach rely
heuristic algorithms labeled data. Second, developed sub-tree alignment model
structure model used MT, i.e., based synchronous tree
substitution grammars. means MT systems directly make benefits subtree alignment model, especially rule extraction MT parameter estimation. Third,
accessing sub-tree alignment matrix encodes large number alignments,
efficiently obtain rules often filtered due errors within 1best/k-best alignment result. experiment approach Chinese-English subtree alignment translation tasks. sub-tree alignment, significantly outperforms
three state-of-the-art baselines. machine translation, approach obtains significant
improvements tree-to-tree system rule quality translation quality.
example, yields +1.0 BLEU improvement -0.9 TER reduction NIST MT
evaluation corpora. Finally, system even outperforms state-of-the-art hierarchical
phrase-based system equipped tree binarization (Wang, Knight, & Marcu, 2007b)
fuzzy decoding (Chiang, 2010) techniques.
rest article structured follows. Section 2 briefly introduces subtree alignment task. Section 3 describes unsupervised approach sub-tree alignment.
Then, Section 4 investigates effective methods applying alignment model tree-totree translation. Then, Section 5 presents experimental evaluation approach.
reviewing related work Section 6, interesting issues discussed Section 7.
Finally, article concluded summary Section 8.

2. Problem Statement
general, sub-tree alignment defined task find alignment
nodes tree nodes another tree.3 restrict machine translation article, sub-tree alignment actually task must tightly coupled
specific applications. example, addition machine translation,
3. work term tree refers data structure defined recursively collection nodes
starting root node. node list edges pointing nodes (or children),
constraint edge duplicated points root (Knuth, 1997).

737

fiXiao & Zhu

NLP tasks make benefits sub-tree alignment, including sentence simplification (Cohn & Lapata, 2009; Woodsend & Lapata, 2011), paraphrasing (Das & Smith,
2009), question answering (Wang, Smith, & Mitamura, 2007a), parser adaptation
projection (Smith & Eisner, 2009).
Ideally, would like sub-tree alignment system language independent
application independent. Given parallel corpus training examples, able
learn alignment model use infer syntactic correspondence tree
pairs. Broadly speaking, alignments paired linguistic tree structures
regarded instances sub-tree alignment. example, alignment performed
dependency trees (Eisner, 2003; Nakazawa & Kurohashi, 2011) phrase structure
trees (Tinsley et al., 2007; Sun et al., 2010b).
Although sub-tree alignment problem includes number tasks seek alignments syntactic tree structures, particularly interested aligning tree
nodes phrase structure trees work. focus phrase structure sub-tree alignment because: 1) phrase structure parsing one popular syntactic analysis
formalisms. Several state-of-the-art full parsing models/tools developed many
languages; 2) phrase structure trees basis many successful syntax-based MT systems. alternatives, dependency trees, also benefit MT systems,
constituency-based models interest relatively larger portion MT community show state-of-the-art performance recent tree-to-tree systems (Zhang et al.,
2008; Liu et al., 2009a; Chiang, 2010).
natural language processing, phrase structure parse tree ordered rooted
tree. represents syntactic structure sentence according phrase structure grammars (or constituency grammars) describe way words combine form
phrases sentences (Chiswell & Hodges, 2007). Generally, phrase structure parse trees
distinguish terminal non-terminal nodes. leaf nodes labeled terminal categories (or words), internal nodes labeled non-terminal categories
grammar (or phrasal categories). example, English parse tree Figure 2(b),
imports terminal, nodes NP NNS two non-terminals indicating
noun phrase plural form nouns respectively.4 following description
experiments, take Penn Treebank standard tree annotation.
choose Penn Treebank one popular tree-annotated corpora
used syntactic parsing good quality quantity several languages,
Chinese English.
Based definition, sub-tree alignment defined alignments
non-terminals source target-language (phrase structure) parse trees.5
formally, given source-language parse tree target-language parse tree ,
sub-tree alignment (denoted A(S, ) short) set node-to-node links
. node pair (u, v) (S, ), good alignment follow three criteria
(Tinsley et al., 2007):
1. u (or v) aligned (indicating 1-to-1 alignment).
4. Note non-terminals always followed leaf nodes also called pre-terminals
labeled part-of-speech tags. E.g., NNS node followed terminal node imports
thus pre-terminal.
5. contrast, word alignment regarded alignments terminals two languages.

738

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

2. u aligned v, descendants u aligned descendants v.
3. u aligned v, ancestors u aligned ancestors v.
criteria prevent aligning constituents cross other. property
similar bi-parsing formalisms, synchronous context free
grammars synchronous tree substitution grammars. advantage enables
use powerful synchronous grammars modeling sub-tree alignment problem.
shown next section, based constraints take synchronous
tree substitution grammars basis proposed model.
According Tinsley et al.s (2007) work, alignments satisfying criteria
called well-formed alignments. alignment ill-formed violates
criteria. work focus well-formed alignments. Hence sub-tree alignment
task stated as: given pair parse trees (S, ), search likely wellformed alignment
= arg max P(A | S, )

(1)

A(S,T )

(S, ) set well-formed alignments, P(A | S, ) viewed
alignment model predicts probability every alignment given .
follows, describe approach sub-tree alignment tree-to-tree translation, including alignment model, training inference methods, effective
use model tree-to-tree MT systems.

3. Unsupervised Sub-tree Alignment
section present unsupervised sub-tree alignment model. first define
base model sub-tree alignment framework synchronous tree substitution
grammars, describe model parameterization, training inference methods.
3.1 Base Model
fundamental question sub-tree alignment define correspondence
nodes source-language parse tree nodes target-language parse tree.
address issue using Synchronous Tree Substitution Grammars (STSGs)
widely adopted model transformation process source target-language
parse trees MT (Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010). general
framework STSGs (Chiang & Knight, 2006), assumed pair source
target parse trees simultaneously generated using derivation STSG rules (or
tree-to-tree transfer rules). example, grammar Figure 1 STSG
rules used generate pair sentences. formally, STSG system
hNs , Nt , Ws , Wt , i, Ns Nt sets non-terminals source target
languages, Ws Wt sets terminals (or words) source target languages,
finite set productions. production STSG rewrite rule (denoted r)
pair source target-language non-terminals (snt , tnt ):
hsnt , tnt hsr , tr , r
739

fiXiao & Zhu

sr source-language tree-fragment, whose frontier nodes either words Ws
non-terminals Ns (labeled x); tr corresponding target-language tree-fragment;
r set 1-to-1 alignments connect frontier non-terminals sr
frontier non-terminals tr . example, r5 Figure 2(a),
snt = IP
tnt =
sr = IP(NN:x VP(AD:x VP(VV:x AS:x)))
tr = S(NP(DT:x NNS:x) VP(VBP(have) ADVP(RB:x VBN:x)))
r = {1-2, 2-3, 3-4, 4-1}
Note non-terminals left-hand side rule actually roots
corresponding tree-fragments right hand side. means rule contains
exactly information matter whether root nodes (snt , tnt ) explicitly
represented not. following parts article use hsr , tr , r simpler representation STSG rules. Beyond this, STSG rules written
compact form alignment r encoded numbers assigned frontier
non-terminals sr tr . example, Figures 1 2, subscripts language
sides STSG rules indicate aligned pairs frontier non-terminals.
STSG model, frontier non-terminals also called substitution nodes.
applying STSGs, rewrite aligned pair substitution nodes tree-fragment
pair encoded STSG rule. constraint operation labels
substituted non-terminals must match root labels rewrite rules. example,
round-head lines Figure 1 show substitution operations used derivation.
using STSG rules, parse tree pair generate corresponding derivations. generation process trivial: start pair root symbols repeatedly rewrite pairs non-terminal symbols using STSG rules. example, tree pair
Figure 2(b), start root labels source target-language parse trees
(the superscript indicates node index tree)
h IP[1] , S[1]
apply rule r9 .
IP[1] S[1]

h IP(NN[2] VP[3] ), S(NP[2] VP[3] )
r9

IP[1] S[1]

represents operation rewrites aligned node pair IP[1]
r9

S[1] r9 (denoted IP[1] S[1] ). process proceeds repeatedly rewriting
remaining frontier non-terminals get complete source target-language trees,
like so:
740

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

NN[2] NP[2]


r7

VP[3] VP[3]


r8

h IP(NN() VP[3] ), S(NP(DT(the) NNS(imports)) VP[3] )
h IP(NN() VP(AD

[4]

[5]

VP(VV

AS[6] ))),

S(NP(DT(the) NNS(imports)) VP(VBP
AD[4] RB[4]


r3

h IP(NN() VP(AD() VP(VV

[5]

S(NP(DT(the) NNS(imports)) VP(VBP
[5]

VV

[6]

ADVP(RB

[4]

VBN[5] )))

AS[6] ))),

[6]

ADVP(RB(drastically) VBN[5] )))

[5]

VBN

h IP(NN() VP(AD() VP(VV() AS[6] ))),
r4

S(NP(DT(the) NNS(imports)) VP(VBP

[6]

ADVP(RB(drastically) VBN(fallen))))
AS[6] VBP[6]


r6

h IP(NN() VP(AD() VP(VV() AS()))),
S(NP(DT(the) NNS(imports)) VP(VBP(have)
ADVP(RB(drastically) VBN(fallen))))

process, rewrite rule indicates node alignment. importantly,
derivations model two nice properties: first, node u
source-language (or target-language) parse tree, one node targetlanguage (or source-language) parse tree aligned u; second, hierarchical
structure behind alignment avoids links constituents cross other.
Consequently, well-formed sub-tree alignment A, always find derivation
encodes alignment A. means sub-tree alignment problem essentially
problem finding likely STSG derivation. Thus sub-tree
alignment task (see Equation (1)) restated finding likely derivation
given pair parse trees.
model derivation probability, follow formulation adopted statistical
word alignment (Brown, Pietra, Pietra, & Mercer, 1993; Vogel, Ney, & Tillmann, 1996).
transformation source-language tree target-language tree described
following equation.
X
P(T | S) =
P (T, d|S)
(2)
dD(S,T )

D(S, ) set derivations transforming (say, aligning nodes
nodes ), P (T, d|S) probability transforming using
derivation D(S, ), parameters model. use notation
P () express dependence model parameters. general, optimal
value learned parsed parallel data training criteria. example,
context unsupervised learning, optimize model parameters maximizing
probability observed data (known maximum likelihood training).
Given set optimal parameters , best sub-tree alignment (S, ) determined
P (T,d|S)
choosing derivation P (d | S, ) greatest. Since P (d | S, ) = P (T |S)


741

fiXiao & Zhu

P (T | S) constant given (S, ) , finding best derivation
finding derivation make P (T, | S) large possible. Hence reach
fundamental equation sub-tree alignment.
= arg max P (T, | S)

(3)

dD(S,T )

formulation implies three fundamental issues sub-tree alignment, including
modeling derivation probability (i.e., P (T, d|S)), learning model parameters (i.e., )
finding best alignment given learned model (i.e., arg max operation).
following parts section, describe solutions issues.
3.2 Parameterization
simplest case, alignment model one parameter instance derivation.
However, model would unmanageable set parameters since number
derivations exponential length input sentences. choose simple
solution issue decomposes base model product trainable submodels. start assumption rules conditionally independent given
source-language parse tree S, probability P(T, | S) defined product
rule probabilities (for conciseness, drop subscript on).

P(T, | S)
P(r | S)
(4)
rd

Nevertheless complex tree-to-tree mappings still result extremely large number
rules, causes computational problem degenerate analysis
data.6 control number parameters reasonable level, decompose
rule probability simpler probability factors independence assumptions.
First assume generation rule r independent input tree S,
conditioned source-language side rule, is,
P(r | S) P(r | sr )

(5)

Note strong assumption generation synchronous grammar
rule depends source-language side. similar used statistical
modeling machine translation (Brown et al., 1993; Koehn, Och, & Marcu, 2003; Galley
et al., 2004; Chiang, 2005) generation atomic alignment/translation units
conditioned associated source-language words tree-fragments, rather
whole input sentence tree. SMT, independence assumptions based phrases
translation rules generally used decompose parallel corpus manageable units
parameter estimation. successfully used modern SMT
systems, adopt similar assumption ease parameter estimation process
model.
decompose P(r | sr ) additional assumptions. Since r = hsr , tr , r i,
P(r | sr ) written another form using chain rule:
6. degenerate analysis refers case using models complex results overfitting
poor generalization ability unseen data.

742

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

P(r | sr ) = P(sr , tr , r | sr )
= P(r | sr , tr ) P(tr | sr )

(6)

Equation (6) indicates two sub-models, including reordering model frontier nonterminals P(r | sr , tr ), tree-fragment translation model P(tr | sr ).
model P(r | sr , tr ), view frontier non-terminal reordering problem aligning
elements two vectors non-terminals. Let vnt() function returns
vector leaf non-terminals given tree-fragment. r defines 1-to-1 alignment
non-terminals vnt(sr ) vnt(tr ). example, r5 Figure 2(a),
frontier non-terminal vectors sr5 tr5 are:
vnt(sr5 ) = (NN, AD, VV, AS)
vnt(tr5 ) = (DT, NNS, RB, VBN)
r5 = {1-2, 2-3, 3-4, 4-1} indicates alignment vnt(sr5 ) vnt(tr5 ), say,
NN aligned NNS, AD aligned RB on. opt simple model
selecting r . models non-terminal reordering probability condition
frontier non-terminal vectors language sides, follows7
P(r | tr , sr ) Preorder (r | vnt(sr ), vnt(tr ))

(7)

turn problem modeling tree-fragment translation P(tr | sr ) (i.e.,
second sub-model defined Equation (6)). define tree-fragment consists
two parts: words lex() (i.e., terminals ) tree structure tree() without lexicons
involved. example, r5 Figure 2(a), target-language tree-fragment contains
two elements lex(tr5 ) tree(tr5 ):
lex(tr5 ) =
tree(tr5 ) = S(NP(DT:x NNS:x) VP(VBP ADVP(RB:x VBN:x)))
Let root() function returns root given tree-fragment. write
P(tr | sr ) as:
P(tr | sr ) = P(lex(tr ), tree(tr ) | sr )
= P(root(tr ) | sr )
P(tree(tr ) | root(tr ), sr )
P(lex(tr ) | tree(tr ), root(tr ), sr )

(8)

worth noting Equation (8) approximation. choose
one many ways P(tr | sr ) written product series
7. reordering model defined ensures arbitrary 1-to-1 alignments handled. might
result large model sparse parameter distributions big tree-fragments involved.
considering issue, choose several pruning methods better control rule size sub-tree
alignment system. See Section 5.2.2 pruning settings work.

743

fiXiao & Zhu

conditional probabilities. simply assert equation generating targetlanguage tree-fragment source-language tree-fragment, first choose root
symbol target-language tree-fragment given source-language tree-fragment (in
probability P(root(tr ) | sr )). choose tree-structure target-language
tree-fragment given root symbol source-language tree-fragment (in probability
P(tree(tr ) | root(tr ), sr )). choose target-language terminals associated
tree-fragment given target-language tree-structure, target-language root symbol
source-language tree-fragment (in probability P(lex(tr ) | tree(tr ), root(tr ), sr )).
Another note Equation (8) actually reduce model complexity.
example, P(lex(tr ) | tree(tr ), root(tr ), sr ) essentially indicates combinations source
target-language tree-fragments. simpler model required feasible solution
parameter estimation. this, introduce additional assumptions relax
conditions probabilities reduce number parameters reasonable level.
1. P(root(tr ) | sr ) depends root(sr ), i.e.,
P(root(tr ) | sr ) Pnt (root(tr ) | root(sr ))

(9)

assumption implies node correspondence source targetlanguage parse trees.
2. P(tree(tr ) | root(tr ), sr ) depends root(tr ), i.e.,
P(tree(tr ) | root(tr ), sr ) Ptree (tree(tr ) | root(tr ))

(10)

second assumption results monolingual model generating target-language
tree-structures, generation tree-fragment conditioned
root. viewed analogy generative model used standard TSGs.
3. P(lex(tr ) | tree(tr ), root(tr ), sr ) depends source words lex(sr ), i.e.,
P(lex(tr ) | tree(tr ), root(tr ), sr ) Plex (lex(tr ) | lex(sr ))

(11)

allows us directly model terminal correspondence two languages.
Then, substitute Equations (9)-(11) Equation (8), get
P(tr | sr ) Pnt (root(tr ) | root(sr ))
Ptree (tree(tr ) | root(tr ))
Plex (lex(tr ) | lex(sr ))
using Equations (6), (7) (12), Equation (4) finally written as:
744

(12)

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

id

rule

probability

r3

AD() RB(drastically)

Pnt (RB | AD) Plex (drastically | )

r4

VV() VBN(fallen)

Pnt (VBN | VV) Plex (fallen | )

r6

AS() VBP(have)

Pnt (VBP | AS) Plex (have | )

r7

NN()

Pnt (NP | NN) Plex (the imports | )

NP(DT(the) NNS(imports))

Ptree (NP(DT NNS) | NP)

VP(AD1 VP(VV2 AS3 ))

Pnt (VP | VP) Ptree (VP(VBP ADVP(RB VBN)) | VP)

VP(VBP3 ADVP(RB1 VBN2 ))

Preorder (1-2, 2-3, 3-1 | (AD, VV, AS), (VBP, RB, VBN))

IP(NN1 VP2 ) S(NP1 VP2 )

Pnt (S | IP) Ptree (S(NP VP) | S)

r8
r9

Preorder (1-1, 2-2 | (NN, VP), (NP, VP))

Table 1: Rule probabilities sample derivation = {r3 , r4 , r6 , r7 , r8 , r9 } Figure 2(b)
P(T, | S)



Pnt (root(tr ) | root(sr ))

rd

Ptree (tree(tr ) | root(tr ))
Plex (lex(tr ) | lex(sr ))
Preorder (r | vnt(sr ), vnt(tr ))

(13)

simplified model generative story described section. takes
rule generation probability product four probability factors: Pnt () nonterminal mapping probability, roughly captures syntactic correspondence subtrees two languages; Ptree () probability generating tree structure
; Plex () probability terminal mappings two language sides
rule; Preorder () probability frontier non-terminal reordering encoded
rule. See Table 1 rule probabilities sample derivation.
model parameters assumed multinomial distributions. calculation Pnt (), Ptree () Preorder () straightforward: directly used
without decompositions assumptions. calculate Plex (), choose
form adopted popular models word alignment (Och & Ney, 2004; Thayer, Ettelaie, Knight, Marcu, Munteanu, Och, & Tipu, 2004), probability defined
product word-based translation probabilities:
l


1 X
Plex (t1 ...tl | s1 ...sm ) Plength (l | m)
Pw (ti | sj )

i=1

(14)

j=1

ti target word, sj source word. Plength () used control number
target words produced given number source words. Pw () word translation
probability. sub-model principle something rather similar conventional
word-based translation tables, IBM Models (Brown et al., 1993).
3.3 Node Deletion Insertion
Word (or sub-tree) deletion/insertion common real-world alignment translation
tasks. add flexibility modeling problem, allow production empty
745

fiXiao & Zhu

sub-trees either source target-language side rule model. formally,
rule whose target-language side empty sub-tree, probability defined as:
P(r | S) Pnt (root() | root(sr ))
Ptree (tree() | root())
Plex (lex() | lex(sr ))
Preorder ( | vnt(sr ), vnt())

(15)

special symbol indicates nothing. Factors Pnt (root() | root(sr ))
Plex (lex() | lex(sr )) model deletion probability different levels tree-fragment.
Ptree (tree() | root()) probability generating empty tree-fragment. Factor
Preorder ( | vnt(sr ), vnt()) regards special reordering pattern aligns
frontier non-terminals source side virtual node NULL. Obviously, values
Ptree (tree() | root()) Preorder ( | vnt(sr ), vnt()) simply 1.
Similarly, rule whose source side empty sub-tree, probability defined as:
P(r | S) Pnt (root(tr ) | root())
Ptree (tree(tr ) | root(tr ))
Plex (lex(tr ) | lex())
Preorder ( | vnt(), vnt(tr ))

(16)

value Preorder ( | vnt(), vnt(tr )) also 1.
worthwhile note word deletion insertion problems important
MT spite relatively less discussion recent studies tree-to-tree translation.
actually analogy NULL-alignment used IBM Models
(Brown et al., 1993). word/phrase-based models, removing words alignment
leave space correctly aligning words sentence.8 even
necessary (1-to-1) sub-tree alignment alignment respect syntactic
constraints language sides, e.g., sub-tree alignments allowed break
constraints imposed neighbouring parts tree. cases, cannot obtain
correct 1-to-1 alignment tree pair due one two bad nodes
necessarily aligned valid node counterpart tree. Instead, nodes
skipped alignment thus impose bad constraints parts
tree node deletion/insertion allowed. especially true align sentence
pairs flat tree structures free translations. work found node
deletion insertion operations necessary achieve satisfactory sub-tree alignment
result. therefore used implementation default.
3.4 Training
turn training problem. discussed Section 3.1, focus unsupervised
learning model parameters, is, optimal values parameters estimated given
8. Note current phrase-based approaches (Koehn et al., 2003; Och & Ney, 2004) allow NULL-aligned
words appear boundary phrase, viewed way implicit modeling
word insertion/deletion problem

746

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

collection tree pairs without annotation sub-tree level alignment. work
choose two approaches estimating parameters sub-tree alignment model, including
Maximum Likelihood Estimation (MLE) approach Bayesian approach.
3.4.1 Maximum Likelihood Training
MLE one popular methods parameter estimation statistical models.
basic idea that, given model set parameters, MLE method selects values
parameters generate distribution gives highest probability observed
data. MLE general approach parameter estimation widely adopted
many AI NLP tasks, part-of-speech tagging. case sub-tree alignment,
MLE simply described finding optimal values parameters lead
maximum probability aligning tree nodes source-language parse tree
target-language parse tree. formally, given set tree pairs {(S1 , T1 ), ..., (Sn , Tn )},
objective MLE-based training defined be:
= arg max


n


X

P (Ti , | Si )

(17)

i=1 dD(Si ,Ti )

choose Expectation Maximization (EM, Dempster, Laird, & Rubin, 1977)
algorithm solve optimization problem. Basically EM algorithm
iterative training method finding maximum likelihood estimates model parameters,
assumed observed data depends latent variables. algorithm
performs iteratively calling two sub-routines, namely Expectation (E)-step
Maximization (M)-step. E-step, calculates expected value likelihood
function associated parameters observed data, respect distribution
latent variables given observed data current estimates parameters.
M-step, seeks parameters maximize expected likelihood found
E-step.
applying EM algorithm case, view input pairs parse
trees {(S1 , T1 ), ..., (Sn , Tn )} observed data, underlying derivations rules latent
variables, distributions Pnt (), Ptree (), Preorder () Plex () (i.e., Plength ()
Pw ()) unknown parameters. See Figure 3 pseudo-code training algorithm
Pnt () (denoted tnt |snt ). algorithm directly applicable estimation
parameters model, skip description learning remaining parameters
here. detailed description EM-based training model parameters
refer reader appendix.
algorithm, snt tnt represent source-language non-terminal symbol
target-language non-terminal symbol, u v represent source-language tree node
target-language tree node, EC() represents expected count given variable,
P(k) (T, | S) represents derivation probability based parameters obtained
k-th round. EM iteration, E-step algorithm accumulates expected
count pairs parse trees. Then, M-step finds maximum likelihood estimate
using quantity. nontrivial part algorithm computation
expected count E-step. Roughly speaking, physical meaning right-hand
side line 9 relative probability derivation contains rule r (with root node
747

fiXiao & Zhu

1: Function TrainModelWithEM ({(S1 , T1 ), ..., (Sn , Tn )})
(0)
2: Set {tnt |snt } = initial model
3: k = 0 K 1
4:
Foreach non-terminal symbol pair (snt , tnt )
5:
EC(tnt |snt ) = 0
6: E-step:
6:
Foreach tree pair (S, ) sequence {(S1 , T1 ), ..., (Sn , Tn )}
7:
Foreach node pair (u, v) symbol pair (tnt , snt ) (S, )
8:
Foreach rule r rooted
P (u, v)
P

rooted (u,v)
9:
EC(tnt |snt ) + = d: rd r P
0
d0 P (k) (T,d |S)
8: M-step:
10:
Foreach non-terminal symbol pair (snt , tnt )

11:
12:

(k+1)

tnt |snt =

P

(k) (T,d|S)

EC(tnt |snt )
EC(t0 |s )

t0nt

nt

nt

(K)

return {tnt |snt }
Figure 3: EM-based training algorithm (for Pnt ())

P
pair (u, v)). numerator d: rd r rooted (u,v) P(k) (T, | S) probability sum
P
derivations involve r , denominator d0 P(k) (T, d0 | S)
overall probability alignment . However, brute-force computation
expected counts inefficient requires sum possible derivations
whose number exponential length input sentences.
work use bilingual version inside outside probabilities (Manning
& Schutze, 1999) avoid naive enumeration possible derivations computing
various probabilities. inside probability (u, v) (denoted (u, v)) measures
likely generate sub-tree pair inside node pair (u, v). outside probability
(denoted (u, v)) dual inside probability. measures likely generate
remaining parts tree pair (S, ) start symbols. Like formulation
used monolingual parsing (Manning & Schutze, 1999), (u, v) (u, v) defined
using following recursive forms:


X

(u, v) =
P(r | S)
(p, q)
(18)
r:root(r)=(u,v)

(u, v) =

X

(p,q)yield(r)



(root(r)) P(r | S)

r:(u,v)yield(r)



(p, q)



(19)

(p,q)yield(r)
(p,q)6=(u,v)

root(r) abbreviation node pair (root(sr ), root(tr )), yield(r)
set aligned frontier non-terminal pairs yielded r. Based recursive
definitions, (u, v) (u, v) efficiently computed dynamic programming.
using inside outside probabilities, easy address computation
problem mentioned above. Let (u, v) denote probability tree node u aligned
tree node v. probability expressed inside-outside fashion:
748

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

(u, v) =

X

P(T, | S)

d: (u,v)
aligned

= (u, v) (u, v)

(20)

way, overall alignment probability (i.e., denominator
right-hand side line 9) simply written as:
X

P(T, | S) = (root(S), root(T )) (root(S), root(T ))

(21)



numerator right-hand side line 9, let us view another angle.
E-Step algorithm, expected count accumulated rules whose root (u, v).
rules rooted (u, v) indicate node alignment u v, lines
8-9 principle imply probability derivations aligning u v, precisely
node alignment probability (u, v). probability written simple
form using inside outside probabilities:
X
X
P(T, | S) = (u, v)
r: r rooted d: rd
(u,v)

= (u, v) (u, v)

(22)

Together result Equation (21), E-step efficiently implemented
replacing lines 8-9 following equation
EC(tnt |snt ) + =

(u, v) (u, v)
(root(S), root(T )) (root(S), root(T ))

(23)

(snt , tnt ) symbol pair (u, v). Note (snt , tnt ), E-step step
(u,v)(u,v)
increases EC(tnt |snt ) sum (root(S),root(T
))(root(S),root(T )) node pairs (u, v)
whose symbols snt tnt . means (snt , tnt ) aligned different positions
input tree pair, method considers alignment (snt , tnt ) multiple
times updates EC(tnt |snt ) accordingly.
also worth noting several methods initializing model parameters EM-style training begins. example, model initialized
uniform random distributions. work initialize parameters sub-tree
alignment model model obtained using word alignment result. standard way adopted many unsupervised models simpler model used good
starting point training process. helpful optimization procedure
sensitive initial setting model parameters (e.g., EM non-convex objective functions). experiments found using GIZA++ word alignment parameter
initialization resulted better performance fewer iterations convergence
uniform initial distributions. word alignment obtained unsupervised
manner, change training condition approach. Thus chose
method initializing model parameters implementation.
749

fiXiao & Zhu

3.4.2 Bayesian Approach
MLE one standard approaches training unsupervised models, well
known tendency overfit data. overfitting problem becomes severe
complex models since parameters fit training data better.
case STSGs, likely result degenerate analysis data, i.e., rare
big rules dominate ML solution STSGs, considered noisy
generalize poorly unseen data (Cohn & Blunsom, 2009; Liu & Gildea, 2009).
natural solution problem incorporate constraints proper priors
training process. take Bayesian approach alternative solution
training problem.
Unlike MLE, Bayesian approach plug single optimum point estimate
parameter distribution data point, instead account uncertainty
value parameter. Bayesian models, parameters assumed
drawn probability distributions priors. parameters extra prior
distributions called hyperparameters, denoted . parameters
model viewed mathematically multinomials, choose Dirichlet distributions
(Ferguson, 1973) prior model parameters. advantage using Dirichlet
distributions conjugate multinomial distributions inference
priors easier.
Following previous description, use denote model parameters
multinomial outcomes {1, ..., K} (i.e., k probability outcome k {1, ..., K}).
multinomial distribution sample set outcomes {x1 , ..., xn } probability
P(xi = k) = k . Dirichlet prior distribution multinomials, sample
prior actually set parameter values . Therefore distribution
modeled as:
xi | Multinomial()

(24)

| Dirichlet()

(25)

Equation (24) means xi distributed according multinomial parameters
. Similarly, Equation (25) read distributed according Dirichlet distribution parameters . = {1 , ..., K } hyperparameter vector corresponding
outcomes. work use symmetric Dirichlet prior ( i.e., 1 , ..., K share
value), use represent single hyperparameter instead hyperparameter
vector.
Using model, compute conditional distribution new observation
xn+1 given previous observations {x1 , ..., xn } hyperparameter , follows:
Z
P(xn+1 | x1 , ..., xn , ) = P(xn+1 | x1 , ..., xn , ) P( | )
(26)
big advantage Bayesian approach introduce prior distribution
unknown parameters model, meant capture knowledge beliefs
model seeing data (Neal, 1998). especially important case
need bias towards preferred situations. example, expect
model favor high frequency rules dislike rare big rules. goal
750

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

easily achieved using Bayesian approach appropriate choice priors, say,
Dirichlet prior low concentration parameter . However, introduction priors
generally makes intractable estimate posterior analytically. practical systems
based Bayesian approach, widely-used solution use approximate methods
seek compromise exact inference computational resources. work
choose Variational Bayes approximate inference. Variational Bayes good method
preserves benefits introducing prior tractable inference procedure
(Attias, 2000; Beal, 2003). successfully applied several NLP-related models,
Hidden Markov Models (HMMs) IBM Models (Beal, 2003; Riley & Gildea,
2010). One good thing Variational Bayes seen extension
EM algorithm resembles usual forms used EM. resulting procedure looks lot
like EM algorithm modified M-step, convenient implementation.
follow approach presented previous work (Beal, 2003; Riley & Gildea, 2010)
variational Bayesian algorithms applied similar tasks. need
slight change M-step original EM algorithm presented Section 3.4.1.
original EM algorithm (see Figure 3), M-step normalizes expected counts
collected E-step standard MLE. variational Bayesian version M-step
slightly modifies formula performs inexact normalization passing counts
function f (x) = exp((x)).

tnt |snt =

f (EC(tnt |snt ) + )
P
f ( t0 (EC(t0nt |snt ) + ))

(27)

nt

(x) digamma function (Johnson, 2007). approximate effect
subtracting 0.5 argument. choice controls behavior estimation.
set low value, performs estimation way anti-smoothing.
0.5 subtracted rule counts, small counts corresponding rare events
penalized heavily, large counts corresponding frequent events affected
much. example, low values make Equation (27) favor non-terminal pairs
aligned frequently distrust non-terminal pairs aligned rarely.
way, variational Bayesian method could control overfitting caused abusing
rare events. hand, larger used smoothing required.
method applicable training parameters model.
requires replacement M-step Figure 3 variational Bayesian M-step (as
Equation (27)). implementation, variational Bayes-based training,
perform additional round normalization without variational Bayes normalize rule
probabilities sum one.9
9. additional normalization process makes posterior probabilities directly comparable
obtained training methods, EM-based training. Note convert result
Bayesian inference probability distributions good explanation various probability
factors model. hand, technical trick results pseudo-Bayesian procedure
Bayesian inference exactly though shows good results empirical study. One
remove additional round normalization pure Bayesian approach. changes
affect overall pipeline approach (from practical standpoint).

751

fiXiao & Zhu

1: Function Decode
(S, )

2:
[], [] = GetInsideOutsideProbabilities (S, )
3:
Foreach node u bottom-up order
4:
Foreach node v bottom-up order
5:
[u, v] = [u, v] [u, v]
6:
Foreach tree-fragment sr rooted u
7:
Foreach tree-fragment tr rooted v
8:
Foreach frontier non-terminal alignment sr ts
9:
r = CreateRule(s
Q r , tr , a)
10:
score = P(r | S) (p,q)yield(r) P(d[p, q])
11:
score > P(d[u, v])
12:
d[u, v] = CreateDerivation(r, {d[p, q] : (p, q) yield(r)})
13: return (d[], [])
14: Function GetInsideOutsideProbabilities (S, )
15: Foreach node u bottom-up order
16:
Foreach node v bottom-up order
17:
Set [u, v] according Equation (18)
18: Foreach node u top-down order
19:
Foreach node v top-down order
20:
Set [u, v] according Equation (19)
21: return ([], [])
Figure 4: Decoding algorithm proposed sub-tree alignment model 1-best
posterior-based outputs
3.5 Decoding
Inference model straightforward. simplest case inferring 1-best subtree alignment. Given set learned parameters, first visit every node pair (u, v)
bottom-up fashion, compute posterior probability aligning sub-tree pair
rooting (u, v). procedure dynamic program used trainer.
select derivation maximum sub-tree alignment probability
input tree pair. Also, generate list k-best derivations similar manner.
addition 1-best/k-best output, model able output alignment
posterior probability every pair tree nodes. this, need record
probability (u, v) node pair obtain inside outside probabilities.
Note outputting alignment posterior probabilities also commonly used statistical
word phrasal aligners. provides flexible way making use alignment result
downstream components, rule extraction system. presented
next sections, tree-to-tree MT systems make great benefits posteriorbased alignment output, results effective rule extraction method well
better translation results.
Figure 4 depicts pseudo-code decoding algorithm 1-best posteriorbased outputs. algorithm, d[x, y] data structure records best derivation rooted (x, y). [x, y], [x, y] [x, y] data structures record inside
probabilities, output probabilities alignment posterior probabilities, respectively. Cre752

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

ateRule() creates rule pair tree-fragments (sr , tr ) frontier non-terminal
alignment a, calculates rule probability. CreateDerivation() builds derivation
using input rules. output, access 1-best alignment traversing
d[root(S), root(T )], access alignment posterior [].
Given pair trees (S, ), outer two loops algorithm iterates pair
nodes two trees, resulting time complexity O(|S| |T |) | | represents
2 ), N
size input tree. Generating pairs tree-fragments requires O(Ntree
tree
maximum number tree-fragments given tree node. Computing alignment
(sr , tr ) requires O(L!) L maximum number leaf non-terminals
2
rule. Therefore time complexity algorithm O(|S| |T | Ntree
L!), quadratic
size input trees. Note actual time complexity algorithm could
high potential alignments considered. example, Ntree generally
exponential function depth input tree-fragment, deep tree could
results extremely large space alignments. make practical sub-tree alignment
systems, pruning techniques taken account work. example,
implementation, restrict depth tree-fragment reasonable number (see Section
5.2.2). addition, commonly-used phrasal alignment related tasks, consider
word alignments pruning discard sub-tree alignments violate certain
number word alignments. example, throw away sub-tree alignments
two word alignment links outside spans covered aligned sub-trees.

4. Applying Sub-tree Alignment Tree-to-Tree Translation
sub-tree alignment obtained, current tree-to-tree systems directly learn translation rules node-aligned tree pairs. section investigate methods applying
sub-tree alignment tree-to-tree rule extraction.
4.1 Rule Extraction Using 1-best/k-best Sub-tree Alignments
data, several methods developed tree-to-tree rule extraction (Zhang et al.,
2008; Liu et al., 2009a; Chiang, 2010). popular GHKM-like
method extends idea extracting syntactic translation rules string-tree pairs
(Galley et al., 2004). GHKM-like extraction, first compute set minimallysized translation rules explain mappings source-language tree
target-language tree respecting alignment reordering
two languages. Larger rules learned composing two minimal rules.
example, Figure 2(b), r7 r9 two minimal rules extracted according sub-tree
alignment. compose rules form larger rule, like this:
IP(NN() VP1 ) S(NP(DT(the) NNS(imports)) VP1 )
work use tree-to-tree version GHKM-like extraction described
Liu et al.s (2009a) work. See Figure 5(a) pseudo-code rule extraction
1-best sub-tree alignment. choose method widely used
tree-to-tree systems. Note rule extraction tree-to-tree translation generally
restricted performed 1-best sub-tree alignment result. GHKM-like
753

fiXiao & Zhu

1: Function OneBestExtract (S, , A)
2:
Foreach node u
3:
Foreach node v
4:
Foreach tree-fragment pair (sr , tr )
4:
rooted (u, v)
5:
= OneToOneAlign(sr , tr , A)
6:
empty
7:
r = CreateRule(sr , tr , a)
8:
rules.Add(r)
9:
return rules
10: Function OneToOneAlign(sr , tr , A)
11: frontier non-terminals (sr , tr )
11:
1-to-1 alignments
12:
return frontier alignment (sr , tr )
13: Else
14:
return

1: Function MatrixExtract (S, , )
2:
Foreach node u
3:
Foreach node v
4:
IsExtractable({(u, v)}, )
5:
next loop
6:
Foreach tree-fragment pair (sr , tr )
6:
rooted (u, v)
7:
Foreach frontier alignment
7:
(sr , tr )
8:
IsExtractable(a, )
9:
r = CreateRule(sr , tr , a)
10:
rules.Add(r)
11: return rules
12: Function IsExtractable(a, )
13: Foreach alignment (p, q)
14:
probability (p, q) < Pmin
15:
return false
16: return true

(a) 1-best Extraction

(b) Matrix-based Extraction

Figure 5: 1-best matrix-based rule extraction algorithms
extraction method employed list k-best sub-tree alignments provided.
k-best extraction need repeat procedure 1-best extraction
sub-tree alignment k-best list.
4.2 Rule Extraction Using Sub-tree Alignment Matrices
Previous work pointed current MT systems suffer error propagation due
alignment errors made within 1-best alignment (Venugopal, Zollmann, Smith, &
Stephan, 2008). sub-tree alignment early-stage step training pipeline,
errors 1-best alignment likely propagated translation rule extraction
parameter estimation translation model. Though problem alleviated
using k-best alignments, limited scope k-best alignments still results inefficient
learning translation rules. example, preliminary experiment shows 95.8%
extracted rules redundant 100-best alignments involved.
instead present simple efficient method, namely matrix-based rule extraction. method, use posterior-based output aligner represent
sub-tree alignment compact structure - call sub-tree alignment matrix alignment
matrix short (Liu, Xia, Xiao, & Liu, 2009b; de Gispert, Pino, & Byrne, 2010).
See Figure 6(a) two example sub-tree alignment matrices made pair sentence segments. matrices, entry indexed pair source target nodes.
score entry posterior probability alignment corresponding node pair, i.e., (u, v) probability defined Equation (20). probability
straightforwardly accessible output inference algorithm described Section
3.5. principle (u, v) viewed measure sub-tree alignment confidence: higher
value indicates confident alignment two nodes. way
754

fihave

RB

[4]

VBN

drastically



1

fallen





VV[4]

AS[5]

AD[2]

.1

AS[5]

4]

5]

VP[1]

.1

AD[2]

.8

.1

.1

.6

.1

.2

VP[3]

.3

.7

VV[4]

.4

AS[5]

1 VV[4]
1

VB
N[

1]

VB
P [2]
AD
VP [
3]
RB [

.9

AD[2]
VP[3]

VP[3]
VP[1]

VP[1]

1

[5]

VP [

ADVP[3]

VB

VBP[2]

VP [

1]

VP[1]

P [2]
AD
VP [
3]
RB [
4]
VB
N [5]

Unsupervised Sub-tree Alignment Tree-to-Tree Translation

.6

= fixed alignment

= possible alignment

Matrix 1: 1-best alignment

Matrix 2: posterior

(a) Sub-tree alignment matrices sample sub-tree pair
Minimal Rules
Extracted Matrix 2 (posterior)
r3
AD() RB(drastically)
r4
VV() VBN(fallen)
r6
AS() VBP(have)
r8
VP(AD1 VP(VV2 AS3 ))
VP(VBP3 ADVP(RB1 VBN2 ))
r10 VP(VV() AS()) VBN(fallen)
r11 VP(AD1 VP2 ) VP(VBP1 ADVP2 )

Minimal Rules
Extracted Matrix 1 (1-best)
r3 AD() RB(drastically)
r4 VV() VBN(fallen)
r6 AS() VBP(have)
r8 VP(AD1 VP(VV2 AS3 ))
VP(VBP3 ADVP(RB1 VBN2 ))

...
(b) Rules extracted using 1-best alignment alignment posterior
Figure 6: Matrix-based representation sub-tree alignment sample rules extracted.
Matrix 1 shows case 1-best sub-tree alignment, Matrix 2 shows
case sub-tree alignment posterior.
access possible sub-tree alignments (with different probabilities), rather limited
number them.
extract rules using sub-tree alignment matrix. method simple:
collect rules associated entry matrix. core algorithm
method essential used 1-best/k-best extraction. difference
1-best/k-best extraction matrix-based method considers possible node
pairs extraction, rather visiting only. See Figure 5(b) pseudocode sub-tree alignment matrix-based rule extraction algorithm, represents
sub-tree alignment matrix pair trees (S, ). Compared extracting rules
k-best alignments, method efficiently obtain additional rules whose extraction
blocked k-best extraction. example, right side Figure 6(b) two new rules
r10 r11 extracted, cannot obtained 1-best alignment result.
prevent extraction great number noisy rules low alignment probabilities,
755

fiXiao & Zhu

prune away rules whose alignment probabilities pre-specified threshold.
formally, given pair nodes (u, v), rule extraction executed (u, v)
satisfies:
(u, v)
< Pmin
(28)
(root(S), root(T ))
expression measures relative probability alignment (u, v) respect
sum probabilities possible derivations. Pmin empirical threshold control
often rules pruned (a larger Pmin means rules thrown away).
work, set 107 default. Therefore, entries zero score Figure
6(a) (denoted dot) excluded rule extraction.
However, discarding rules relatively low probabilities turn results incompleteness problem, is, extracted rules might unable transform given source
parse-tree, even training set. Nonetheless, problem severe
case. experiments observed parse-tree pairs (over 90%) training
corpus could recovered extracted rules Pmin chose default value,
contribution translation accuracy low confidence rules limited
(generally less 0.1 BLEU points).
Another note sub-tree alignment matrix-based extraction. advantage
method follows general well-developed framework syntax-based MT,
i.e., word/syntactic alignment + rule extraction/parameter estimation + MT decoding.
need replace rule extraction component sub-tree alignment matrixbased system, preserve components pipeline. means still
use heuristics obtain additional useful rules result sub-tree alignment
matrix-based extraction, rule composing (Galley et al., 2006) SPMT extraction
(Marcu, Wang, Echihabi, & Knight, 2006). Also, posterior probability encoded
matrix used better estimation various MT-oriented features.10
Note basis approach STSG model, rules sub-tree
alignment model resemble general forms translation rules used tree-to-tree MT
systems. So, alternative simple way rule induction, directly infer translation rules sub-tree alignment model take corresponding rule probabilities
features translation model MT decoding. However, tree-to-tree MT
method suffers several problems. First, sub-tree alignment model requires computation possible aligned tree-fragments, results high time complexity
training decoding procedures. result, aggressive pruning used
reasonable size search space, e.g., consider relatively small tree-fragments
implementation acceptable running speed. side effect, many relatively large
rules (e.g., composed rules SPMT rules) absent sub-tree alignment model,
available use traditional alignment + extraction heuristics pipeline.
engineering standpoint, efficient directly infer translation rules
sub-tree alignment model, compared inferring rules using pruned fixed subtree alignment matrix plus heuristics. Second, rule probability optimization
objective sub-tree alignment different used MT systems. example, use generative model maximum-likelihood/Bayesian approach sub-tree
10. See Section 4.3 detailed discussion parameter estimation issue.

756

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

alignment, use discriminative model minimum error rate training MT. Many
features employed MT decoder considered sub-tree alignment model.
issues might lead unsatisfactory MT performance. shown experiments (see Section 5.3.5), directly inferring translation rules sub-tree alignment
model achieve promising results.
4.3 Learning Features Machine Translation
previous work syntax-based MT, proved syntax-based systems make great
benefits MT-oriented features, even necessarily well explained
syntactic parsing viewpoint (e.g., phrase-based translation probabilities). However,
features available word/sub-tree alignment model. Instead
need learn features using additional step parameter estimation MT.
this, follow commonly-used framework estimates values various MToriented features extracted rule set using MLE. procedure simple:
translation rules extracted, obtain maximum-likelihood (or relative-frequency)
estimate parameters according definition feature function.
However, traditional tree-to-tree systems rule extracted tree pair
count unit one, used calculate values various features.
approach might enlarge influence noisy rules extracted sub-tree alignment
matrices. E.g., rule high alignment probability equal weight rule
low alignment probability, thus unreasonably large impact MT systems.
desired solution rule extracted derivation low probability
penalized accordingly feature learning. Motivated idea, use fractional counts
estimate appearance rule (Mi & Huang, 2008). Given node pair (u, v)
(S, ), alignment probability rule r rooted (u, v) defined (denoted
(r; u, v)):
X
(r; u, v) =
P(T, | S)
(29)
dD(S,T )
rd

(r; u, v) regarded probability sum derivations involving r (u, v).
Also, rewrite Equation (29) inside-outside fashion:

(r; u, v) = (u, v)
(p, q) P(r | S)
(30)
(p,q)yield(r)

define probability r involved derivations (S, ) as:
X
(r) =
(r; u, v)

(31)

u,v

Equation (31) sum probabilities r node pairs. means
rule probability considered multiple times particular derivations contain
r once. using (r), fractional count r defined be:
c(r) =

(r)
(root(S), root(T ))

757

(32)

fiXiao & Zhu

Equation (32) reflects probability likely r involved derivation given
pair trees. set bilingual parse trees, c(r) accumulated tree pair.
Obviously, c(r) used estimate parameters MT model, is,
translation rules weighted, parameter estimation procedure proceed usual,
weight counts. work c(r) employed learn five features used
MT decoder, including bi-directional phrase-based conditional translation probabilities
(Marcu et al., 2006) three syntax-based conditional probabilities (Mi & Huang, 2008).
Let () function returns sequence frontier nodes input tree-fragment.
probabilities computed following equations:
P
00
r00 :(sr00 )=(sr )(tr00 )=(tr ) c(r )
P
Pphrase (tr | sr ) =
(33)
0
r0 :(sr0 )=(sr ) c(r )
P
00
r00 :(sr00 )=(sr )(tr00 )=(tr ) c(r )
P
Pphrase (sr | tr ) =
(34)
0
r0 :(t 0 )=(tr ) c(r )
r

c(r)

P(r | root(r)) =

P

P(r | sr ) =

P

r0 :root(r0 )=root(r) c(r

c(r)
r0 :sr0 =sr

P(r | tr ) =

c(r0 )

c(r)
P

r0 :tr0 =tr

c(r0 )

0)

(35)
(36)
(37)

5. Experiments
evaluation, first experimented approach Chinese-English sub-tree
alignment task, tested effectiveness state-of-the-art tree-to-tree MT system.
5.1 Baselines
Three unsupervised sub-tree alignment methods chosen baselines experiments.
WordAlign-1 : WordAlign-1 based GHKM-like method (Galley et al., 2004)
uses word alignments infer syntactic correspondences. implementation,
GIZA++ toolkit grow-diag-final-and method used obtain
symmetric word alignment sentence pairs. sub-tree alignments
heuristically induced selecting node correspondences consistent
word alignment result (i.e., sub-tree alignments violate word
alignments). chose method widely adopted modern
tree-to-tree systems.
WordAlign-2 : second baseline essentially WordAlign-1.
difference WordAlign-1 improved word alignment system using
link-deletion techniques (Fossum et al., 2008). basic idea delete harmful
alignment links initial word alignment result (e.g., deleting link
Figure 2(a)). experiments considered likely
deletion top-10 common Chinese words (including {, , , ,
758

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

, , , , , }) top-10 common English words (including {the,
of, and, to, in, a, is, that, for, on}).
HeuristicAlgin: HeuristicAlgin re-implementation approach proposed
Tinsley et al.s (2007) work. method alignment confidence every node
pair first computed lexical translation probabilities, used obtain
node correspondences via heuristic algorithm. method require
training process successfully adopted several translation tasks,
French-English translation, chosen another baseline comparison.
5.2 Experimental Setup
settings experiments described follows.
5.2.1 Data Preparation
bilingual corpus consists 1.06 million sentence pairs.11 mentioned above,
used GIZA++ grow-diag-final-and heuristics generate 1-best/k-best word
alignments, used baseline word alignment results. parse trees
Chinese English generated using Berkeley Parser.12 publicly available
corpus used evaluate sub-tree alignment result.13 consists 736 node-aligned
sentence pairs (with gold-standard parse trees language sides) LDC2003E07
also included bilingual data. corpus divided two parts:
held-out set used finding appropriate setting hyperparameters (99 sentences
articles 301-309), test set used evaluating sub-tree alignment systems (637
sentences articles 001-066). MT experiments, 5-gram language model trained
Xinhua portion Gigaword corpus addition English part LDC
bilingual training data.14 used NIST 2003 MT evaluation corpus development
set (919 sentences) newswire portion NIST 2004-2006 MT evaluation corpora
test set (3,486 sentences).
5.2.2 Sub-tree Alignment
parameters sub-tree alignment model initialized add-one smoothing
rule-set extracted using word alignments (i.e., WordAlign-1 baseline). Then,
model trained parse trees bilingual corpus using EM algorithm
Variational Bayes (VB) approach. implementation VB-based training,
hyperparameters assumed share value.15 leads setting = 0.01
11. LDC category: LDC2003E14, LDC2005T10, LDC2003E07, LDC2005T06, LDC2005E83, LDC2006E26,
LDC2006E34, LDC2006E85, LDC2006E92 LDC2004T08. See http://www.ldc.upenn.edu/
details.
12. Note LDC2003E07 corpus reused gold-standard parse trees provided Chinese
English treebanks.
13. Available http://www.nlplab.com/resources/nodealigned-bitreebank.html
14. LDC category English Gigaword corpus: LDC2003T05
15. Although could adopt different hyperparameters finer control priors model parameters, found setting hyperparameters value could also lead satisfactory
performance.

759

fiXiao & Zhu

optimal value held-out set. default, trained model 5 EM
Variational EM iterations. speed-up training process avoid degenerate
analysis caused large rules, restricted rules reasonable sizes rules five frontier non-terminals depth three. rules
five frontier non-terminals, considered tree-fragments depth one
restrict number frontier non-terminals involved, is, flat tree structures,
used associated height-one tree-fragments. Besides, discarded sub-tree
alignment every node pair whose terminals aligned outside corresponding
spans two times WordAlign-1.
5.2.3 Machine Translation
used NiuTrans open-source toolkit (Xiao, Zhu, Zhang, & Li, 2012) build
tree-to-tree MT system. rule extraction, used extension GHKM method
extract minimal tree-to-tree transformation rules (Liu et al., 2009a) obtained larger
rules composing two three minimal rules (Galley et al., 2006). used CKY-style
decoder cube pruning (Huang & Chiang, 2005) beam search decode Chinese
sentences. default beam size set 50. addition features described
Equations (33)-(37), also used several features MT system, including
5-gram language model, rule number bonus, target length bonus two binary
features - lexicalized rule low frequency rule (Marcu et al., 2006). features
combined log-linear fashion optimized using Minimum Error Rate Training (MERT,
Och, 2003).
5.3 Results
following part section, present experimental results, including evaluations sub-tree alignments, extracted rules, MT systems. Also, show results
several improved methods effective use approach tree-to-tree MT.
5.3.1 Evaluation Alignments
First evaluated alignment quality various sub-tree alignment approaches terms
precision (P), recall (R) F-1 score.16 See Table 2 results three baseline
systems sub-tree alignment system. measures, VB-based system
significantly improves overall recall F-1 score, slightly degrading precision
compared WordAlign-1/2. Also, VB-based training outperforms EM-based counterpart due priors introduced learning process. interesting observation
that, though EM training model suffers degenerate analysis
data, show extremely bad results experiment. phenomenon due
restriction size tree-fragment training. described Section 5.2.2,
restricted translation rules reasonable-size tree-fragments several ways (e.g.,
16. Let predicted number alignments system output, correct number correct alignments
system output, gold number alignments gold-standard. measure precision, recall
2
)precisionrecall
correct
F- score defined as: precision = predicted
, recall = correct
F- = (1+
.
gold
2 precision+recall
parameter controls preference recall (i.e., > 1) precision (i.e., 0 < 1).
NLP tasks set 1, indicating equal weights recall precision.

760

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

Entry
Overall
NP NP
NN NN
VP VP
PU ,
IP
PU .
NP NN
NP PP
NN NNS
NR NNP
NN NP
PP PP
NN JJ
P
QP NP

WordAlign-1
P
R
F-1
75.4 63.6 69.0
86.9 59.0 70.3
83.9 75.6 79.5
75.3 61.9 68.0
84.7 76.5 80.4
92.5 87.5 89.9
98.5 98.7 98.6
77.6 71.5 74.4
46.8 63.8 54.0
84.2 76.0 79.9
69.4 41.2 51.8
65.1 59.8 62.4
84.6 68.4 75.7
90.7 76.5 83.0
81.5 69.8 75.2
72.2 64.9 68.3

WordAlign-2
P
R
F-1
76.3 64.5 69.9
87.0 62.1 72.5
83.6 77.0 80.2
74.9 61.9 67.8
84.7 76.5 80.4
92.3 87.7 89.9
98.5 98.7 98.6
83.0 71.5 76.8
49.7 63.5 55.7
83.8 76.5 80.0
69.9 41.7 52.3
65.9 61.0 63.3
85.1 69.2 76.3
90.4 76.3 82.7
82.3 68.3 74.7
72.6 65.2 68.7

HeuristicAlgin
P
R
F-1
65.7 67.7 66.7
79.1 73.6 76.3
76.2 74.1 75.1
71.3 71.8 71.6
69.6 71.3 70.3
90.6 90.7 90.6
98.5 98.7 98.6
59.3 77.5 67.2
53.1 31.9 39.8
77.8 75.5 76.7
63.4 57.4 60.2
71.8 50.4 59.2
79.3 72.6 75.8
83.9 81.7 82.8
81.2 72.2 76.4
67.1 65.6 66.4

(EM)
P
R
F-1
79.8 46.2 58.5
84.2 48.2 61.3
81.8 63.7 71.6
80.5 49.0 60.9
82.7 67.7 74.5
90.4 76.8 83.0
94.7 86.8 90.6
75.4 62.3 68.2
43.2 42.1 42.6
83.7 58.3 68.7
57.7 41.5 48.3
70.5 48.1 57.2
85.6 56.9 68.4
84.2 69.1 75.9
79.9 57.9 67.1
78.3 42.2 54.8

(VB)
P
R
F-1
72.6 75.1 73.8
88.7 75.3 81.4
81.1 79.9 80.5
75.7 75.8 75.7
82.5 80.7 81.6
90.0 92.4 91.2
98.5 98.5 98.6
75.9 78.9 77.4
54.5 70.1 61.3
81.0 77.6 79.2
67.2 56.3 61.3
71.1 67.7 69.4
85.4 82.5 83.9
85.9 81.2 83.5
84.7 76.1 80.2
74.9 71.7 73.3

Table 2: Evaluation results sub-tree alignment system baselines.
measures reported percentage.
set parameter maximum depth). constraints reduce number rules
involved training, prevents use rare large rules. result indicates
fact tree-fragment size constraint actually important efficiency
also crucial learning. discussed previous work, without constraints
imposing proper prior, solution EM degenerate (Marcu & Wong, 2002; DeNero,
Gillick, Zhang, & Klein, 2006).
addition, Table 2 shows result 15 common types sub-tree alignment. expected, VB-based system achieves best F-1 score cases.
interestingly, observed approach obtains significantly better performance
handling PP (Prepositional Phrase) alignment seems difficult problem
baselines due unclear boundary indicators aligning PP structures. attribute
better use syntactic information language sides model,
generally ignored traditional models based surface heuristics word alignments.
5.3.2 Evaluation Extracted Rules
applied sub-tree alignment result tree-to-tree system study impact
sub-tree alignment MT. discussed Section 4, rule extraction downstream
component sub-tree alignment current tree-to-tree MT pipeline. therefore
chose evaluate quality rules obtained various sub-tree alignment results.
determine goodness extracted grammars, computed rule precision, recall,
F-1 scores approach baseline approaches test set used
(1-best) alignment quality evaluation. make gold-standard grammar, chose
method used Fossum et al.s (2008) work grammar automatically
generated manually-annotated alignment result, is, rules extracted using
annotated sub-tree alignments regarded gold-standard computing various
evaluation scores. Table 3 shows evaluation result grammars extracted
761

fimatrix

1best

Xiao & Zhu

Entry
WordAlign-1
WordAlign-2
HeuristicAlgin
(EM)
(VB)
(VB + Pmin
(VB + Pmin
(VB + Pmin
(VB + Pmin

= 105 )
= 106 )
= 107 )
= 108 )

Rule P
51.9
52.3
55.8
61.9
54.9
79.6
53.0
41.3
34.9

Rule R
60.8
61.8
55.3
49.2
65.2
34.5
70.0
75.6
79.5

Rule F-1
55.9
56.6
55.5
54.8
59.6
48.2
60.3
53.4
48.5

Table 3: Evaluation results rules obtained various sub-tree alignment approaches.
measures reported percentage.
different sub-tree alignment approaches. see improvements persist
sub-tree alignments employed translation rule extraction. VB-based approach
produces grammars higher rule F-1 score three baselines.
addition 1-best extraction, studied rule extraction behaves
sub-tree alignment matrix-based extraction method. Table 3 also shows result
sub-tree matrix-based extraction method different choices pruning parameter
Pmin . see smaller values Pmin result grammars higher rule recall. Also,
better rule F-1 scores achieved adjusting Pmin seeking good balance
rule precision rule recall , e.g., Pmin = 106 107 .
scores informative measure grammar quality, also investigated differences rule sets obtained model compared
baseline approaches, following Levenberg, Dyer, Blunsoms (2012) method. Figure 7
shows probable rules (frequency 2) obtained bilingual corpus using
VB-based alignment approach appear model WordAlign-2
alignment vice versa. asked two annotators sub-tree alignment estimate
rule quality based syntactic correspondence adequacy frontier node sequence
two languages sides. rule labeled good judges considered good quality. figure, see eight top-10 rules extracted
using approach absent WordAlign-2 grammar good rules. contrast,
four top-10 rules baseline model good quality sense human
preference. Furthermore, examined top-100 probable rules appear
two grammars individually. Again, top-100 rules extracted using proposed model
better quality. results 61 good rules. contrast, 44% top-ranking
rules induced using WordAlign-2 alignment good translation rules.
5.3.3 Evaluation Translations
also evaluated translations generated MT system different sub-tree alignment approaches. Since VB-based training shows best performance previous
experiments, chose default setting approach following experiments.
Table 4 shows evaluation result translation quality estimated using caseinsensitive IBM-version BLEU4 (Papineni, Roukos, Ward, & Zhu, 2002) TER (Snover,
762

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation


1*
2*
3*
4
5*
6*
7*
8*
9*
10

top-10 highest probability rules (for MT) approach absent WordAlign-2
NP(DNP1 NN()) VP(ADVP1 VP(VBD(improved)))
NP(PU( ) NP(CD() NN()) PU()) NP(DT(the) CD(two) NNS(sessions))
NP(NP(QP1 NP(NN())) NP2 ) NP(X2 NP(CD1 NP(NNS(represents))))
NP(NN() NP1 ) VP(VB(desire) NNP1 )
NP(PU() NP(NR1 NN()) PU()) NP(() NP(NNP1 NN(independence)) ())
NP(VP1 DEC() NP(NN() NN()))
NP(ADJP(ADJP1 JJ(ideological)) NN(struggle))
NP(NP(QP1 NP2 ) NP(ADJP(JJ()) NP(NN())))
NP(NP(DT(the) JJ(important) NN(thinking)) IN(of) SBAR(WHNP1 S2 ))
NP(IP1 DEG() NP(NN() NN())) NP(ADJP(ADJP1 JJ(practical))
NN(significance))
NP(NP(PU() NP(QP1 NN()) PU()) NP(ADJP2 NN()))
NP(NP(DT(the) JJ2 NN(idea)) PP(IN(of) NP(DT(the) CD1 NNS(represents))))
NP(PU() NN() PU1 ) VP(VBG(joining) NP(DT(the) NN1 ))

top-10 highest probability rules (for MT) WordAlign-2 absent approach
1
LCP(QP1 LC()) ADJP(JJ1 )
2*
NP(DNP1 NN()) VP(ADVP1 VBP(changes))
3*
NP(NN() NN1 ) NP(CD(three) NNS(links) X1 )
4
NP(DNP(IP1 DEC()) NP(NN())) NP(ADJP1 NN(significance))
5
VP(ADVP1 VP(VV2 NP(NN() NN3 )))
VP(ADVP1 VP(VP(VV2 ) NP(DT(the) JJ(mass) NN3 )))
6
IP(NP1 VP(VV() NP(NN() NN2 ))) NP(NP(NNS1 ) PP(IN(for) NP2 ))
7
NP(VP1 DEG() NN()) ADJP(JJ1 )
8
NP(NP(PU() NT1 PU()) NP(NN()) NR())
NP(NP(PRP$(his)) QP(CD1 ) NN(speech))
9*
VP(VP(ADVP1 VP(VV() CC() VV())) NP2 )
VP(ADVP1 VP(VP(VB(strengthen) CC(and) VB(improve)) NP2 ))
10* NP(NP(PU() NN() PU()) NP1 )
NP(NP(() NP(NN(taiwan) NN(independence)) ()) NNS1 )

Figure 7: top-10 highest probability rules built proposed sub-tree alignment
approach WordAlign-2 baseline grammar, top-10 rules
WordAlign-2 baseline grammar obtained using proposed
sub-tree alignment approach. * = good translation rule.
Dorr, Schwartz, Makhoul, Micciula, & Weischedel, 2005), significance test performed using bootstrap resampling method (Koehn, 2004). Moreover, efficiency
rule extraction reported terms rule-set-size/extraction-time. comparison,
also report result rule extraction using word alignment matrices (Liu et al., 2009b)
WordAlign-1 WordAlign-2.
Table 4 indicates approach outperforms baselines BLEU TER
measures 1-best 30-best extraction. addition, matrix-based method
much efficient k-best method. example, compared 30-best extraction, extracting rules sub-tree alignment matrices 9 times efficient. However,
rules counted unit one parameter estimation translation model,
using alignment matrices show significant BLEU improvements TER reductions comparison 30-best counterpart (see rows marked unitcount).
many additionally extracted rules utilized real translation.
example, observed 7.3% rules used generating final (1-best)
763

fiXiao & Zhu

Entry
WordAlign-1 (1-best)
WordAlign-2 (1-best)
HeuristicAlgin (1-best)
WordAlign-1 (30-best)
WordAlign-2 (30-best)
HeuristicAlgin (30-best)
WordAlign-1 (matrix)
WordAlign-2 (matrix)
(1-best + unitcount)
(30-best + unitcount)
(matrix + unitcount)
(1-best + posterior)
(30-best + posterior)
(matrix + posterior)

Dev

Test

BLEU4[%] TER[%]

BLEU4[%] TER[%]

36.2
36.2
35.7
36.3
36.4
35.4
36.9*
36.8*
36.7*
36.8*
36.9*
36.9*
37.0*
37.4**

34.2
34.2
33.8
34.4
34.6*
33.9
35.0*
35.1*
34.9*
35.0*
35.3**
35.0*
35.2**
35.6**

57.0
56.9
57.2
57.2
57.0
57.2
56.5
56.6
56.6
56.6
56.3*
56.4*
56.2**
55.9**

58.3
58.1
58.3
58.2
58.0
58.0
57.9*
57.7*
57.8
57.6*
57.5*
57.4*
57.0**
57.1**

Rule-set
size
24.8M
25.3M
22.7M
32.7M
33.0M
32.4M
50.2M
53.8M
27.0M
37.4M
54.9M
27.0M
37.4M
54.9M

Efficiency
(rule/sec)

75.4
75.9
72.0
3.8
3.9
3.8
35.8
37.9
78.8
4.1
37.7
78.8
4.1
37.7

Table 4: Evaluation translations different alignment approaches. BLEU, higher
better. TER, lower better. unitcount means take rule
occurrence unit one parameter estimation, posterior means use
rule posterior probabilities fractional counts parameter estimation. * **
= significantly better three 1-best baselines (p < 0.05 0.01).
translations indeed extracted alignments seen 30-best
alignments. thus indicates fact naively increasing number rules might
effective improving translation quality.
last three rows Table 4 show result using alignment posterior probabilities
parameter estimation (i.e., method described Section 4.3). see alignment
posterior probabilities helpful improving translation quality system
weight rules confidence (entries unitcount vs. entries
posterior ). using sub-tree alignment matrices rule extraction alignment
posterior probabilities parameter estimation, approach finally achieves +1.0 BLEU
improvement -0.9 TER reduction 30-best case baselines. even
outperforms word alignment matrix-based counterpart +0.5 BLEU points -0.6
TER points (both significant p < 0.05).
Further, effectiveness proposed approach demonstrated terms BLEU
TER scores rule-set size. Figure 8 compares approach
baseline approaches different numbers unique rules extracted.17 Clearly,
number unique rules, proposed sub-tree alignment approach leads better translations baselines.
5.3.4 Impact Alignment Grammar Quality MT Performance
experiments demonstrate effectiveness proposed approach terms
different measures individually. next natural question sub-tree alignment
17. this, adjusted Pmin obtain grammars different sizes approach.
approaches, used different k-best lists rule extraction.

764

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

43

1 - TER[%]

BLEU4[%]

36

35

34

HeuristicAlgin
WordAlign-2
WordAlign-1


42

HeuristicAlgin
WordAlign-2
WordAlign-1

41

33
10

20

30

40

50

60

70

10

20

30

40

50

60

70

Rule-set size (million)

Rule-set size (million)

Figure 8: BLEU 1-TER rule-set size
rule extraction affect translation quality. study issue important
optimize upstream systems MT decoding select appropriate evaluation
metrics good prediction MT performance.
therefore carried another set experiments compares translation
quality different sub-tree alignment rule extraction settings. generate diverse
sub-tree alignment rule extraction results, varied values Pmin
sub-tree alignment rule extraction respectively. way, obtained ensembles
sub-tree alignments grammars different precision recall scores.18 chose
F- score evaluation metric sub-tree alignment system rule
extraction system. Instead fixing 1, varied value 0.5 3. Since
parameter control bias towards precision recall, choosing different values
helpful seeking good tradeoff precision recall. find
appropriate evaluation measure sub-tree alignment rule extraction predicting
MT performance well.
Figures 9 10 plot F- scores measures MT performance sub-tree alignment rule extraction. Figure 10, see rule F-3 score correlates best
translation quality measures, indicates MT system prefers rule
recall-biased metrics. agrees observation Figure 8 MT system
make benefits rules. hand, curves Figure 9 show
better correlation sub-tree alignment F-2/F-3 score translation quality measures, implying preference relatively higher sub-tree alignment recall. result
reasonable framework node alignment links result aligned
tree-fragments (or rules) extracted. high-recall sub-tree alignment generally results
big grammar high rule recall, thus better BLEU TER results. also com18. example, larger value generally results higher alignment precision, small value
prefers higher alignment recall. rule extraction, larger value Pmin generally leads grammar
higher rule precision, choosing smaller Pmin generate grammar higher rule recall.

765

fiXiao & Zhu

80

sub-tree alignment F-[%]

sub-tree alignment F-[%]

80

70

60

F-0.50
F-0.75
F-1.00
F-2.00
F-3.00

50

40
33.5

34

34.5

35

35.5

70

60

F-0.50
F-0.75
F-1.00
F-2.00
F-3.00

50

40

36

41

BLEU4[%]

42

43

1 - TER[%]

70

70

60

60

rule F-[%]

rule F-[%]

Figure 9: BLEU 1-TER sub-tree alignment F- measure

50
40

F-0.50
F-0.75
F-1.00
F-2.00
F-3.00

30
20
34

34.5

35

50
40

F-0.50
F-0.75
F-1.00
F-2.00
F-3.00

30
20

35.5

41.5

BLEU4[%]

42

42.5

43

1 - TER[%]

Figure 10: BLEU 1-TER rule F- measure

puted Pearsons correlation coefficients sub-tree alignment/rule F-3 score
BLEU/TER. sub-tree alignment F-3, correlation coefficients BLEU TER
0.971 -0.962 respectively. rule F-3, correlation coefficients BLEU
TER 0.983 -0.963 respectively. show good correlations translation quality measures. Another interesting observation MT performance
sensitive change rule F- score change sub-tree alignment
F- score. may lie rule extraction direct upstream step decoding
impacts output MT systems. contrast, sub-tree alignment front-end
step MT pipeline indirect effect actual translation process.
766

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

System
Hierarchical phrase-based
Tree 1-best word alignment (WordAlign-2)

Word alignment matrix
tree Sub-tree alignment matrix

Dev

Test

BLEU4[%] TER[%]

BLEU4[%] TER[%]

37.2
36.8
37.0
37.9*

35.2
35.3
35.2
36.0**

57.3
56.4*
56.3*
55.8**

58.0
57.7
57.5*
56.8**

Table 5: MT Evaluation results rules obtained various alignment approaches.
BLEU, higher better. TER, lower better. * ** = significantly better
hierarchical phrase-based baseline (p < 0.05 0.01).
5.3.5 Improvements
Previous work pointed straightforward implementation tree-to-tree MT
suffers problems rules derivations either rule extraction
decoding process (Chiang, 2010). advance tree-to-tree system compare
state-of-the-art, employed tree binarization (Wang et al., 2007b) fuzzy
decoding (Chiang, 2010) system. alignment approach equipped
general framework tree-to-tree translation, trivial conduct another set
experiments investigate effectiveness approach stronger system.19 Table
5 shows BLEU TER scores system enhanced methods.20
comparison, also report result state-of-the-art MT system implements
hierarchical phrase-based model (Chiang, 2007) tree-to-tree system extracts
rules using word alignment matrices (Liu et al., 2009b). Table 5 indicates superiority
approach tree binarization fuzzy decoding involved. significantly
outperforms hierarchical phrase-based system (+0.7 BLEU points -1.2 TER points)
tree-to-tree system based word alignment matrices (+0.8 BLEU points
-0.7 TER points).
discussed Section 4, transfer rules sub-tree alignment model resembles
general form STSGs directly used MT. Instead resorting
explicit step rule extraction, use rules sub-tree alignment model
MT decoding, i.e., sub-tree alignment cast grammar induction step. therefore
built another system directly acquires translation rules sub-tree alignment
step. this, need output rules derivation forest generated
alignment model. rule probabilities obtained using inside output
probabilities, pruning performed throwing away rules whose probability
Pmin . addition rule probability, reused n-gram language model, rule number
bonus, target length bonus, lexicalized rule low frequency rule indicators
base tree-to-tree system additional features fair comparison. obtain good
reasonable result, employed fuzzy decoding tree binarizaiton experiment.
19. choose setting previous experiments gold-standard alignment annotation Penn Treebank-style trees only. difficult evaluate alignment grammar
quality binarized trees due lack benchmark data. experiments first conducted
experiments individual tasks (see Sections 5.3.1-5.3.3), studied correlations simple
reasonable setting consistent result sub-tree alignment MT (see Section 5.3.4).
investigated effectiveness approach advanced tree-to-tree system (see Section 5.3.5).
20. implementation parse trees binarized head-out fashion.

767

fiXiao & Zhu

Entry
Baseline (explicit rule extraction)
Rules sub-tree alignment model
Rules sub-tree alignment model + MERT
Baseline + sub-tree alignment features

Dev

Test

BLEU4[%] TER[%]

BLEU4[%] TER[%]

37.9
36.2
36.7
38.2

36.0
34.3
34.9
36.1

55.8
57.9
57.3
55.8

56.8
58.8
58.0
56.9

Table 6: MT Evaluation results obtaining rules sub-tree alignment model
obtaining rules traditional rule extraction pipeline
Table 6 compares results sub-tree alignment matrix-based rule extraction inducing rules alignment model (Row 1 vs. Row 2). Unfortunately, straightforwardly
inferring rules probabilities sub-tree alignment model underperforms
baseline. might attributed several reasons. First, due large derivation
space, cannot enumerate relatively large tree-fragments sub-tree alignment
step, instead access tree-fragments limited depths. contrast, baseline system extracts basic rules using sub-tree alignment matrices obtains
large rules heuristics (e.g., rule composing). additional rules obtained
baseline framework rule extraction general useful modern syntax-based
systems (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007). Second, rule
probability sub-tree alignment model defined product probability factors
good generation story. However, MT systems usually use features required form generative model, features shown Equations (33)-(37).
consequence, many well-developed features used baseline system
available sub-tree alignment model. Third, sub-tree alignment model
trained maximizing likelihood criteria, consistent
adopted MT system (i.e., minimizing evaluation-related error rate function).
study issues, improved system two ways. First, treated four
probability factors sub-tree alignment model (See Equation (13)) different
features MT decoder, tuned weights using MERT. Row 3 Table 6 shows
method achieves better results system employing unweighted probability
factors. However, performance still worse baseline, indicates
MT-oriented features rule extraction heuristics crucial success
tree-to-tree system. Finally added probabilistic factors sub-tree alignment
model baseline system additional features. shown last row Table 6,
enhanced system yields modest BLEU improvements baseline, TER
improvement achieved. results give us two interesting messages - 1) rule
extraction heuristics, MT-oriented features objectives learning key factors contributing good tree-to-tree system; 2) better use sub-tree alignment
model upstream module rule extraction decoding, rather using
simple step grammar induction.
last issue investigate whether sub-tree alignment model make
benefits labeled data. Although focus unsupervised learning work,
proposed model require strictly unsupervised condition. Instead
enhanced use labeled data. idea simple: combine probability
factors sub-tree alignment model log-linear weighted fashion. means
768

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

Entry
Unweighted
Weighted (weights learned labeled data)

Dev

Test

BLEU4[%] TER[%]

BLEU4[%] TER[%]

37.9
38.3

36.0
36.1

55.8
55.6

56.8
56.9

Table 7: Comparison unweighted weighted sub-tree alignment models
probability factors sub-tree alignment model taken real valued feature
functions, feature weights learned labeled data supervised methods.
way, unweighted generative model (i.e., factor weight one)
transformed weighted model (i.e., factor individual weight). Note
weighted model almost form used SMT systems.
difference SMT model language model needed targetlanguage side fixed sub-tree alignment step. avoid bias towards
many rules, also added rule number additional feature new model.
training test, divided node-aligned gold-standard data two parts -
first 310 sentences selected weight training, remaining 327 sentences
selected testing system. learn feature weights supervised manner, chose
MERT one powerful tools training log-linear models. error
function used MERT defined one minus sub-tree alignment F-1 score.
327-sentence test data (with tree annotation Penn Treebanks),
weighted model achieves alignment F-1 score 75.4% rule F-1 score 60.0%,
respectively. result better unweighed (and unsupervised) model
obtains alignment F-1 score 72.4% rule F-1 score 59.2%
data set. Finally tested MT performance best setting (i.e., sub-tree alignment
matrix-based rule extraction + tree binarization + fuzzy decoding).21 Table 7 shows
weighted sub-tree alignment model leads better BLEU score tuning set
show promising improvements test data. size labeled corpus
small, expect better results labeled data available. Also worth studying
sophisticated supervised methods learn better weights, kernel-based
methods (Sun et al., 2010b). supervised/semi-supervised learning focus
work, leave interesting issues future investigations.

6. Related Work
Syntax-based approaches widely adopted machine translation last
ten years. Many successful syntactic MT systems developed shown good
results several translation tasks (Eisner, 2003; Galley et al., 2004, 2006; Liu et al.,
2006; Huang et al., 2006; Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010). Despite
differences modeling implementation details, models require alignment
step acquire syntactic correspondence source target languages.
standard SMT, syntax-based MT systems use word alignments infer syntactic
alignments string-tree/tree-tree pairs. However, word alignments generally
good quality viewpoint syntactic alignment. makes sense directly
21. sub-tree-aligned data binarized trees, reused weights learned Penn
Treebank-style trees four probability factors sub-tree alignment model.

769

fiXiao & Zhu

induce sub-tree level alignments pairs sentences syntactic information
either language side both. especially true tree-to-tree MT
actually need alignment sub-trees two languages, rather surface
alignment words. several lines work address syntactic alignment
problem make better use various alignment results tree-to-tree translation.
6.1 Word Sub-tree Alignment Machine Translation
earliest efforts syntactic alignment focus enhancing word alignment models
syntactic information. date, several research groups (Fraser & Marcu, 2007; DeNero
& Klein, 2007; May & Knight, 2007; Fossum et al., 2008; Haghighi, Blitzer, DeNero, &
Klein, 2009; Burkett, Blitzer, & Klein, 2010; Riesa, Irvine, & Marcu, 2011) proposed
syntax-augmented models advance word alignment systems. Although models
achieved promising improvements, still address alignment problem word level.
discussed Section 1, methods might desirable choices learning
correspondence tree nodes two languages. alternative straightforward solution, researchers tried infer sub-tree level alignments pairs syntactic
trees. example, Imamura (2001), Groves, Hearne, Way (2004), Tinsley et al.
(2007) defined several scoring functions measure similarity source
target sub-trees, aligned tree nodes greedy algorithms. approaches,
though simple implement, derived principled way. example,
models explicit optimization procedure, general framework statistical learning. Instead, model parameters obtained using additional alignment
models lexicons. another line research, Sun et al. (2010a, 2010b) attempted address sub-tree alignment problem supervised/semi-supervised models. used
tree kernels various syntactic features advance sub-tree alignment system
showed promising results Chinese-English translation tasks. However, approach still
relies heuristic algorithms inferring node correspondences two parse trees.
Beyond this, train tree kernels, approach requires additional labeled data
generally expensive build. Unlike studies, derive sub-tree model
principled way develop unsupervised sub-tree alignment framework tree-to-tree
MT.
6.2 Unsupervised Syntactic Alignment
also previous studies resort labeled data sub-tree alignment.
earliest Eisners (2003) work. designed unsupervised approach
modeling sub-tree alignment problem STSG formalism. However, since
detailed derivation model decomposition provided, model computationally
expensive, even difficult applied current tree-to-tree systems complex tree
structures involved. Gildea (2003) also applied STSGs tree-to-tree/tree-to-string
alignment. developed loosely tree-based alignment method address issue
parse-tree isomorphism bitext. work targets word alignment rather
modern syntactic MT systems. Recently Nakazawa Kurohashi (2011) proposed
Bayesian approach sub-tree alignment dependency trees, tested
Japanese-English MT system. Actually model much common model
770

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

presented work. example, apply unsupervised learning methods
Bayesian models sub-tree alignment. hand, two studies differ
important aspects. First, Nakazawa Kurohashi (2011) restricted sub-tree
alignment dependency trees, different aligning tree nodes
phrase structure trees. Since phrase structure trees involve complex structures
syntactic categories, alignment problem phrase structure trees relatively
difficult dependency-based counterpart. Second, model makes benefits
recent advances STSGs directly applicable current state-of-the-art tree-totree systems.
Another related work presented Pauls, Klein, Chiang, Knights
(2010) work. factored node-to-string alignment model components
generates target side synchronous rule source side. Moreover, probability
rule fragment factored lexical structural component work. Actually,
model proposed model two variants theme. appear
obvious differences them. First, focus sub-tree alignment tree-to-tree
translation, Pauls et al. (2010) addressed alignment issue tree-to-string/stringto-tree translation. model, parse language sides independently, rather
parsing one side projecting syntactic categories. result, inference faster
work since need consider possible parse trees unparsed side
alignment. Second, permutation model presented work general
order handle non-ITG trees. Third, investigate methods effective use
sub-tree alignment MT. particular, present rule extraction approach obtaining
additional translation rules using sub-tree alignment posteriors, rather learning rules
1-best sub-tree alignment.
6.3 Rule Extraction Using Various Alignment Results
machine translation, word syntactic alignments used extract translation rules
phrases. traditional pipeline rule phrase extraction, 1-best alignment result considered, suffers limited scope single alignment.
efficiently obtain diverse alignment/parsing results, packed data structures adopted
improve 1-best pipeline MT systems recent years (Mi & Huang, 2008; Liu et al., 2009a;
Zhang, Zhang, Li, Aw, & Tan, 2009). example, Liu et al. (2009b) de Gispert et al.
(2010) used alignment posterior probabilities phrase hierarchical phrase extraction.
development sub-tree alignment matrices actually motivated similar idea
word alignment matrices. difference work use sub-tree
language sides infer alignment posterior probabilities, probabilities
calculated word/phrase-level previous work (Liu et al., 2009b; de Gispert et al.,
2010). Moreover, knowledge, effectiveness sub-tree alignment matrix
systematically studied case tree-to-tree translation.
Note approach presented work also something similar synchronous grammar induction. example, model results STSG
formalism used MT. Recent studies Bayesian models (Blunsom, Cohn,
Dyer, & Osborne, 2009; Cohn & Blunsom, 2009; Levenberg et al., 2012) shown
promising results directly learning synchronous grammars bilingual data hierar771

fiXiao & Zhu

chical phrase-based string-to-tree systems, rather extracting synchronous grammar
rules based explicit word/syntactic alignment step. However rare see related
work tree-to-tree MT. principle article different previous work synchronous grammar induction. example, aim work learn sub-tree
alignment model, applied many potential applications except MT,
sentence compression paraphrasing test summarization (Jing, 2000; Cohn & Lapata,
2009). Unlike synchronous grammar induction alignment implicitly encoded
learning process, treat sub-tree alignment separate task. eases
development tuning alignment system actually resort MT
systems slow difficult optimize. Another advantage approach
make benefits compact models, rather used MT great
number rules involved. Take implementation instance. alignment model
learned relatively small set grammar rules (rules limited depths),
MT system accesses much larger grammar many additional rules involved
rule composing. method result efficient alignment system likely
alleviate degenerate analysis data, cost degrading MT performance.

7. Discussion
underlying assumption proposed model 1-to-1 sub-tree alignments
achieved based constraints imposed neighboring parts tree (see Section
2). makes sense standpoint linguistically-motivated models, yet turn
faces problem constraints make difficult align sentences/trees correctly,
particularly free translations. several reasons explain
approach works nice tree-to-tree MT suffer greatly
structure divergence languages. First, model flexible allows
node deletion/insertion alignment. means levels tree
necessary require every node aligned valid node language side,
instead nodes dropped needed. advantage method
cannot confidently align node node counterpart tree, align
virtual node enforce bad constraints aligning parts tree.
useful flat tree structures partial translations
syntactically well-formed. Second, main purpose approach infer
sub-tree alignment probabilities used pruning sub-tree alignment matrices
extracting rules MT systems. Though 1-to-1 alignment required training
sub-tree alignment model, actually access large number alignment
alternatives rule extraction, even cannot appear derivation
due alignment constraints. Third, model work phrase structure
trees. Instead Penn Treebank-style trees difficult alignment
cases, sub-tree alignment system works well binarized trees shows promising
improvements various baselines. Note tree binarization effective method
alleviate structure divergence problem, especially Chinese-English translation.
Also, might interesting investigate methods dealing differences
syntactic structures languages, forest-based methods (Mi & Huang,
2008; Liu et al., 2009a).
772

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

Another note approach. implemented naively, speed sub-tree alignment system slow since model needs calculation alignment probability pairs tree-fragments. Fortunately, thought computation, several
optimizations make system much efficient practice. First, described
work, pruning methods employed restrict number tree-fragments
reasonable level. experiments, system pruning achieved speed 1.82.2
sentence/second single core Intel Xeon 3.16-GHz CPU. Another way speed
improvement parallel processing. good property EM-style algorithms
E-step easily implemented parallel computation environment.
need divide training data set number smaller parts, run
inside-outside algorithm parts parallel (i.e., Map procedure). expected
counts model parameters accumulated results parts (i.e.,
Reduce procedure). M-step performed usual. implementation
used 40 threads parallel training. running time one training iteration
1-million-sentence corpus 14-17 hours. Note system speed-up
expected powerful distributed infrastructures available (e.g., clusters +
Hadoop), difficult scale approach handle millions sentence pairs
using current training framework.

8. Conclusions
proposed unsupervised probabilistic sub-tree alignment approach tree-totree translation. factoring alignment model several components, resulting
model easily learned using EM algorithm variational Bayesian approach.
Also, investigated different ways applying proposed model tree-to-tree
translation. particular, developed sub-tree alignment matrix encodes
exponentially large number alignments. representation sub-tree alignment,
desirable rules extracted efficiently using k-best sub-tree alignment
result. experiments showed proposed model achieved significant improvements
alignment quality grammar quality several baselines. NIST ChineseEnglish evaluation corpora, achieved +1.0 BLEU improvement -0.9 TER reduction
top state-of-the-art tree-to-tree system. improved MT system even significantly
outperformed state-of-the-art hierarchical phrase-based system equipped tree
binarization fuzzy decoding.

Acknowledgments
work supported part National Science Foundation China (Grants
61073140 61272376), Natural Science Foundation Youth China (Grant
61300097), China Postdoctoral Science Foundation (Grant 2013M530131), Specialized Research Fund Doctoral Program Higher Education (Grant 20100042110031),
Fundamental Research Funds Central Universities (Grant N100204002).
authors would like thank anonymous reviewers pertinent insightful comments, Keh-Yih Su great help improving early version article, Ji
773

fiXiao & Zhu

helpful discussions, Chunliang Zhang Tongran Liu language refinement.
corresponding author article Jingbo Zhu.

Appendix A. Part-of-speech Tags Phrase Structure Labels
work annotation POS tagging phrase structure parsing follows standard defined Penn English Chinese Treebanks (Marcus et al., 1993; Xue et al.,
2005). See Tables 8-11 lists POS tags constituent labels used example
trees article.
POS Tag
AD

NN
NR
P
PN
PU
VV

Description
Adverb
Aspect Particle
Noun (except proper nouns temporal nouns)
Proper Noun
Preposition
Pronoun
Punctuation
Verb (except stative verbs, copulas, main
verbs , )

Table 8: Chinese POS tags used examples
POS Tag
DT

JJ
NNP
NNS
PRP
RB
VBD
VBN
VBP
,
.

Description
Determiner
Preposition
Adjective
Proper noun (singular)
Noun (plural)
Personal Pronoun
Adverb
Verb (past tense)
Verb (past participle)
Verb (non-3rd person singular present)
Comma
Period

Table 9: English POS tags used examples
Syntactic Label
IP
NP
PP
QP
VP

Description
Single Clause
Noun Phrase
Preposition Phrase
Quantity Phrase
Verb Phrase

Table 10: Chinese constituent labels used examples

774

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

Syntactic Label
ADVP
NP
PP

VP

Description
Adverb Phrase
Noun Phrase
Preposition Phrase
Sentence
Verb Phrase

Table 11: English constituent labels used examples
Distribution
Pnt ()
Ptree ()
Preorder ()

Notation
tnt |snt
ttree |tnt
tvnt |svnt

Plength ()
Pw ()

l|m
tw |sw

Description
snt tnt source target-language non-terminal symbols
ttree target-language tree-fragment
svnt tvnt vectors non-terminal symbols source
target-languages
l numbers source target terminals (or words)
sw tw source target terminals (or words)

Table 12: Notations model parameters

Appendix B. EM-based Training Sub-tree Alignment Model
described Section 3, proposed sub-tree alignment model five types parameters, including non-terminal mapping probability Pnt (), target-language treefragment generation probability Ptree (), frontier non-terminal reordering probability
Preorder (), word number probability Plength () word mapping probability Pw ().
convenience use new set notations denote model parameters following description. See Table 12 symbol list.
follow framework EM-based training described Figure
3. See Figure 11 complete version EM algorithm parameters model.
algorithm, EC() represents expected count input variable. (X = x)
0-1 function returns 1 variable X takes value x, 0 otherwise. (k) (r; u, v)
(k) (S, ) rule probability (see Equation (29)) probability subtree alignment (see Equation (20)), k indicates
probabilities calculated based parameters k-th iteration. tree(), vnt()
lex() functions return tree-structure, frontier non-terminal vector,
terminal sequence input tree-fragment, respectively (see Section 3.2).
basic idea E-step check rule r (given pair tree nodes u
(r;u,v)
v) update EC() relative probability (root(S),root(T
)) . applied
update rules parameters tnt |snt , ttree |tnt , tvnt |svnt l|m (see lines 8-11).
exception tw |sw . defined Equation (14), Pw (ti | sj ) direct
product factor,Pinstead use sum terminals source-language treefragment (i.e.,
j=1 Pw (ti | sj )). follow result IBM Model 1 make
P|lex(s )|
update magnitude proportional Pw (ti | sj )/ j 0 =1 r Pw (ti | sj 0 ). refer reader
Brown et al.s (1993) work detailed derivation expected count IBM Model
1. also worth noting algorithm performs parameter update based
different choices (u, v) r E-step. means rule instance involved
particular derivation one time (e.g., tree-fragment appears
775

fiXiao & Zhu

1: Function TrainModelWithEM ({(S1 , T1 ), ..., (Sn , Tn )})
(0)
(0)
(0)
(0)
(0)
2: Initialize {tnt |snt , ttree |tnt , tvnt |svnt , l|m , tw |sw }
3: k = 0 K 1
4:
Set EC() = 0 model parameters
6: E-step:
5:
Foreach tree pair (S, ) sequence {(S1 , T1 ), ..., (Sn , Tn )}
6:
Foreach node pair (u, v) (S, )
7:
Foreach rule r rooted (u, v)
(k) (r;u,v)(snt =u)(tnt =v)
(k) (root(S),root(T ))

8:

EC(tnt |snt )

+=

9:

EC(ttree |tnt ) + =

(k) (r;u,v)(tnt =v)(ttree =tree(tr ))
(k) (root(S),root(T ))

10:

EC(tvnt |svnt ) + =

(k) (r;u,v)(svnt =vnt(sr ))(tvnt =vnt(tr ))
(k) (root(S),root(T ))

11:

EC(l|m )

(k) (r;u,v)(m=|lex(sr )|)(l=|lex(tr )|)
(k) (root(S),root(T ))

12:

Foreach word pair (sj , ti ) position (j, i) (lex(sr ), lex( tr ))

+=

(k)
P
(ti |sj )
(sw =sj )(tw =ti )
r P(k) (t |s )
w
j0
j 0 =1

(k) (r;u,v) P|lex(sw)|

13:

EC(tw |sw ) + =

(k) (root(S),root(T ))

10: M-step:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:

Foreach non-terminal symbol pair (snt , tnt )
(k+1)

tnt |snt

=

EC(tnt |snt )
P

t0nt



EC t0

nt |snt

Foreach target non-terminal symbol tnt tree-fragment structure ttree
EC(ttree |tnt )

(k+1)

ttree |tnt =

P

t0tree



EC t0

tree |tnt

Foreach pair non-terminal symbol vectors (svnt , tvnt )
EC(tvnt |svnt )

(k+1)

tvnt |svnt =

P

t0vnt



EC t0

vnt |svnt

Foreach pair word numbers (m, l)
(k+1)

l|m

=

EC(l|m )
P

l0

EC l0 |m



Foreach pair words (sw , tw )
(k+1)

tw |sw

(K)

=

EC(tw |sw )
P

t0w

(K)

EC t0



w |sw

(K)

(K)

(K)

return {tnt |snt , ttree |tnt , tvnt |svnt , l|m , tw |sw }
Figure 11: EM-based training algorithm model parameters

different positions), update corresponding parameters would carried
multiple times.
Another note EM algorithm. expected counts parameters
efficiently calculated using inside outside probabilities according lines 8-11
776

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

13. parameters efficient ways. example, discussed
Section 3.4.1, expected count tnt |snt obtained without checking individual
rule, is, omit loop r case. technique considered
speed-up sub-tree alignment system.

References
Attias, H. (2000). variational bayesian framework graphical models. Solla, S. A.,
Leen, T. K., & K., M. (Eds.), Advances Neural Information Processing Systems 12,
pp. 209215. MIT Press.
Beal, M. J. (2003). Variational algorithms approximate bayesian inference. Masters
thesis, University College London.
Blunsom, P., Cohn, T., Dyer, C., & Osborne, M. (2009). gibbs sampler phrasal
synchronous grammar induction. Proceedings Joint Conference 47th
Annual Meeting ACL 4th International Joint Conference Natural
Language Processing AFNLP (ACL-IJCNLP), pp. 782790, Suntec, Singapore.
Brown, P. E., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1993). mathematics
statistical machine translation: Parameter estimation. Computational Linguistics,
19, 263311.
Burkett, D., Blitzer, J., & Klein, D. (2010). Joint parsing alignment weakly synchronized grammars. Human Language Technologies: 2010 Annual Conference North American Chapter Association Computational Linguistics
(HLT:NAACL), pp. 127135, Los Angeles, California, USA.
Chiang, D. (2005). hierarchical phrase-based model statistical machine translation.
Proceedings 43rd Annual Meeting Association Computational Linguistics (ACL), pp. 263270, Ann Arbor, Michigan, USA.
Chiang, D. (2007). Hierarchical phrase-based translation. Computational Linguistics, 33,
4560.
Chiang, D. (2010). Learning translate source target syntax. Proceedings
48th Annual Meeting Association Computational Linguistics (ACL),
pp. 14431452, Uppsala, Sweden.
Chiang, D., & Knight, K. (2006). introduction synchronous grammars. Tutorials
21st International Conference Computational Linguistics 44th Annual
Meeting Association Computational Linguistics (COLING-ACL).
Chiswell, I., & Hodges, W. (2007). Mathematical Logic. Oxford University Press.
Cohn, T., & Blunsom, P. (2009). Bayesian model syntax-directed tree string grammar induction. Proceedings 2009 Conference Empirical Methods Natural
Language Processing (EMNLP), pp. 352361, Singapore.
Cohn, T., & Lapata, M. (2009). Sentence compression tree transduction. Journal
Artificial Intelligence Research, 34, 637674.
Das, D., & Smith, N. A. (2009). Paraphrase identification probabilistic quasi-synchronous
recognition. Proceedings Joint Conference 47th Annual Meeting
777

fiXiao & Zhu

ACL 4th International Joint Conference Natural Language Processing
AFNLP (ACL-IJCNLP), pp. 468476, Suntec, Singapore.
de Gispert, A., Pino, J., & Byrne, W. (2010). Hierarchical phrase-based translation grammars extracted alignment posterior probabilities. Proceedings 2010
Conference Empirical Methods Natural Language Processing (EMNLP), pp.
545554, Cambridge, MA, USA.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data via
em algorithm. Journal Royal Statistical Society. Series B (Methodological),
39, 138.
DeNeefe, S., Knight, K., Wang, W., & Marcu, D. (2007). syntax-based MT
learn phrase-based MT?. Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language
Learning (EMNLP-CoNLL), pp. 755763, Prague, Czech Republic.
DeNero, J., Gillick, D., Zhang, J., & Klein, D. (2006). generative phrase models underperform surface heuristics. Proceedings Workshop Statistical Machine
Translation (WMT), pp. 3138, New York city, USA.
DeNero, J., & Klein, D. (2007). Tailoring word alignments syntactic machine translation. Proceedings 45th Annual Meeting Association Computational
Linguistics (ACL), pp. 1724, Prague, Czech Republic.
Eisner, J. (2003). Learning non-isomorphic tree mappings machine translation.
Companion Volume Proceedings 41st Annual Meeting Association
Computational Linguistics (ACL), pp. 205208, Sapporo, Japan.
Ferguson, T. S. (1973). bayesian analysis nonparametric problems. Annals
Statistics, 1, 209230.
Fossum, V., Knight, K., & Abney, S. (2008). Using syntax improve word alignment
precision syntax-based machine translation. Proceedings Third Workshop
Statistical Machine Translation (WMT), pp. 4452, Columbus, Ohio, USA.
Fraser, A., & Marcu, D. (2007). Getting structure right word alignment: LEAF.
Proceedings 2007 Joint Conference Empirical Methods Natural Language
Processing Computational Natural Language Learning (EMNLP-CoNLL), pp. 51
60, Prague, Czech Republic.
Galley, M., Graehl, J., Knight, K., Marcu, D., DeNeefe, S., Wang, W., & Thayer, I. (2006).
Scalable inference training context-rich syntactic translation models. Proceedings 21st International Conference Computational Linguistics
44th Annual Meeting Association Computational Linguistics (COLINGACL), pp. 961968, Sydney, Australia.
Galley, M., Hopkins, M., Knight, K., & Marcu, D. (2004). Whats translation rule?.
Susan Dumais, D. M., & Roukos, S. (Eds.), Proceedings 2004 Human Language
Technology Conference North American Chapter Association Computational Linguistics (HLT:NAACL), pp. 273280, Boston, Massachusetts, USA.
778

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

Gildea, D. (2003). Loosely tree-based alignment machine translation. Proceedings
41st Annual Meeting Association Computational Linguistics (ACL), pp.
8087, Sapporo, Japan.
Groves, D., Hearne, M., & Way, A. (2004). Robust sub-sentential alignment phrasestructure trees. Proceedings 20th International Conference Computational
Linguistics (COLING), pp. 10721078, Geneva, Switzerland.
Haghighi, A., Blitzer, J., DeNero, J., & Klein, D. (2009). Better word alignments
supervised itg models. Proceedings Joint Conference 47th Annual
Meeting ACL 4th International Joint Conference Natural Language
Processing AFNLP (ACL-IJCNLP), pp. 923931, Suntec, Singapore.
Huang, L., & Chiang, D. (2005). Better k-best parsing. Proceedings Ninth International Workshop Parsing Technology (IWPT), pp. 5364, Vancouver, British
Columbia, Canada.
Huang, L., Kevin, K., & Joshi, A. (2006). Statistical syntax-directed translation
extended domain locality. Proceedings 7th Conference Association
Machine Translation Americas (AMTA), pp. 6673, Cambridge, Massachusetts,
USA.
Imamura, K. (2001). Hierarchical phrase alignment harmonized parsing. Proceedings
6th NLP Pacific Rim Symposium, pp. 377384.
Jing, H. (2000). Sentence reduction automatic text summarization. Proceedings
6th Applied Natural Language Processing Conference, pp. 310315.
Johnson, M. (2007). doesnt EM find good HMM POS-taggers?. Proceedings
2007 Joint Conference Empirical Methods Natural Language Processing
Computational Natural Language Learning (EMNLP-CoNLL), pp. 296305, Prague,
Czech Republic.
Knuth, D. (1997). Art Computer Programming: Fundamental Algorithms. AddisonWesley.
Koehn, P. (2004). Statistical significance tests machine translation evaluation. Lin,
D., & Wu, D. (Eds.), Proceedings 2004 Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 388395, Barcelona, Spain.
Koehn, P., Och, F., & Marcu, D. (2003). Statistical phrase-based translation. Proceedings 2003 Human Language Technology Conference North American
Chapter Association Computational Linguistics (HLT:NAACL), pp. 4854,
Edmonton, Canada.
Levenberg, A., Dyer, C., & Blunsom, P. (2012). bayesian model learning scfgs
discontiguous rules. Proceedings 2012 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning
(EMNLP-CoNLL), pp. 223232, Jeju Island, Korea.
Liu, D., & Gildea, D. (2009). Bayesian learning phrasal tree-to-string templates. Proceedings 2009 Conference Empirical Methods Natural Language Processing
(EMNLP), pp. 13081317, Singapore.
779

fiXiao & Zhu

Liu, Y., Liu, Q., & Lin, S. (2006). Tree-to-string alignment template statistical machine
translation. Proceedings 21st International Conference Computational
Linguistics 44th Annual Meeting Association Computational Linguistics (COLING-ACL), pp. 609616, Sydney, Australia.
Liu, Y., Lu, Y., & Liu, Q. (2009a). Improving tree-to-tree translation packed forests.
Proceedings Joint Conference 47th Annual Meeting ACL
4th International Joint Conference Natural Language Processing AFNLP
(ACL-IJCNLP), pp. 558566, Suntec, Singapore.
Liu, Y., Xia, T., Xiao, X., & Liu, Q. (2009b). Weighted alignment matrices statistical
machine translation. Proceedings 2009 Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 10171026, Singapore.
Manning, C. D., & Schutze, H. (1999). Foundations Statistical Natural Language Processing. MIT Press.
Marcu, D., Wang, W., Echihabi, A., & Knight, K. (2006). Spmt: Statistical machine translation syntactified target language phrases. Proceedings 2006 Conference
Empirical Methods Natural Language Processing (EMNLP), pp. 4452, Sydney,
Australia.
Marcu, D., & Wong, D. (2002). phrase-based,joint probability model statistical machine translation. Proceedings 2002 Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 133139.
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building large annotated
corpus english: penn treebank. Computational Linguistics, 19, 313330.
May, J., & Knight, K. (2007). Syntactic re-alignment models machine translation.
Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning (EMNLP-CoNLL),
pp. 360368, Prague, Czech Republic.
Mi, H., & Huang, L. (2008). Forest-based translation rule extraction. Proceedings
2008 Conference Empirical Methods Natural Language Processing (EMNLP),
pp. 206214, Honolulu, Hawaii, USA.
Nakazawa, T., & Kurohashi, S. (2011). Bayesian subtree alignment model based dependency trees. Proceedings 5th International Joint Conference Natural Language
Processing (IJCNLP), pp. 794802, Chiang Mai, Thailand.
Neal, R. (1998). Philosophy Bayesian Inference. http://www.cs.toronto.edu/radford/
res-bayes-ex.html.
Och, F. (2003). Minimum error rate training statistical machine translation. Proceedings 41st Annual Meeting Association Computational Linguistics
(ACL), pp. 160167, Sapporo, Japan.
Och, F., & Ney, H. (2004). alignment template approach statistical machine translation. Computational Linguistics, 30, 417449.
Papineni, K., Roukos, S., Ward, T., & Zhu, W. (2002). Bleu: method automatic
evaluation machine translation. Proceedings 40th Annual Meeting
780

fiUnsupervised Sub-tree Alignment Tree-to-Tree Translation

Association Computational Linguistics (ACL), pp. 311318, Philadelphia, Pennsylvania, USA.
Pauls, A., Klein, D., Chiang, D., & Knight, K. (2010). Unsupervised syntactic alignment
inversion transduction grammars. Proceedings Human Language Technologies: 2010 Annual Conference North American Chapter Association
Computational Linguistics (HLT:NAACL), pp. 118126, Los Angeles, California,
USA.
Riesa, J., Irvine, A., & Marcu, D. (2011). Feature-rich language-independent syntax-based
alignment statistical machine translation. Proceedings 2011 Conference
Empirical Methods Natural Language Processing (EMNLP), pp. 497507, Edinburgh, Scotland, UK.
Riley, D., & Gildea, D. (2010). Improving performance giza++ using variational
bayes. Tech. rep., University Rochester.
Smith, D. A., & Eisner, J. (2009). Parser adaptation projection quasi-synchronous
grammar features. Proceedings 2009 Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 822831, Singapore.
Snover, M., Dorr, B., Schwartz, R., Makhoul, J., Micciula, L., & Weischedel, R. (2005).
Study Translation Error Rate Targeted Human Annotation. Tech. rep.
LAMP-TR-126,CS-TR-4755,UMIACS-TR-2005-58, University Maryland, College
Park BBN Technologies.
Sun, J., Zhang, M., & Tan, C. L. (2010a). Discriminative induction sub-tree alignment
using limited labeled data. Proceedings 23rd International Conference
Computational Linguistics (COLING), pp. 10471055, Beijing, China.
Sun, J., Zhang, M., & Tan, C. L. (2010b). Exploring syntactic structural features sub-tree
alignment using bilingual tree kernels. Proceedings 48th Annual Meeting
Association Computational Linguistics (ACL), pp. 306315, Uppsala, Sweden.
Thayer, I., Ettelaie, E., Knight, K., Marcu, D., Munteanu, D., Och, F., & Tipu, Q. (2004).
isi/usc mt system. Proceedings International Workshop Spoken Language
Translation 2004, pp. 5960.
Tinsley, J., Zhechev, V., Hearne, M., & Way, A. (2007). Robust language pair-independent
sub-tree alignment. Proceedings Machine Translation Summit XI, pp. 467474,
Copenhagen, Denmark.
Venugopal, A., Zollmann, A., Smith, N. A., & Stephan, V. (2008). Wider pipelines: n-best
alignments parses mt training. Proceedings Eighth Conference
Association Machine Translation Americas (AMTA), pp. 192201.
Vogel, S., Ney, H., & Tillmann, C. (1996). Hmm-based word alignment statistical translation. Proceedings 16rd International Conference Computational Linguistics (COLING), pp. 836841.
Wang, M., Smith, N. A., & Mitamura, T. (2007a). Jeopardy model? quasisynchronous grammar QA. Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language
Learning (EMNLP-CoNLL), pp. 2232, Prague, Czech Republic.
781

fiXiao & Zhu

Wang, W., Knight, K., & Marcu, D. (2007b). Binarizing syntax trees improve syntaxbased machine translation accuracy. Proceedings 2007 Joint Conference
Empirical Methods Natural Language Processing Computational Natural
Language Learning (EMNLP-CoNLL), pp. 746754, Prague, Czech Republic.
Woodsend, K., & Lapata, M. (2011). Learning simplify sentences quasi-synchronous
grammar integer programming. Proceedings 2011 Conference Empirical Methods Natural Language Processing (EMNLP), pp. 409420, Edinburgh,
Scotland, UK.
Xiao, T., Zhu, J., Zhang, H., & Li, Q. (2012). Niutrans: open source toolkit phrasebased syntax-based machine translation. Proceedings 50th Annual Meeting Association Computational Linguistics System Demonstrations (ACL),
pp. 1924, Jeju Island, Korea.
Xue, N., Xia, F., Chiou, F.-d., & Palmer, M. (2005). penn chinese treebank: Phrase
structure annotation large corpus. Natural Language Engineering, 11, 207238.
Zhang, H., Zhang, M., Li, H., Aw, A., & Tan, C. L. (2009). Forest-based tree sequence
string translation model. Proceedings Joint Conference 47th Annual
Meeting ACL 4th International Joint Conference Natural Language
Processing AFNLP (ACL-IJCNLP), pp. 172180, Suntec, Singapore.
Zhang, M., Jiang, H., Aw, A., Li, H., Tan, C. L., & Li, S. (2008). tree sequence alignmentbased tree-to-tree translation model. Proceedings 46th Annual Meeting
Association Computational Linguistics: Human Language Techonologies
(ACL:HLT), pp. 559567, Columbus, Ohio, USA.

782

fiJournal Artificial Intelligence Research 48 (2013) 175-230

Submitted 05/13; published 10/13

Online Mechanism Multi-Unit Demand
Application Plug-in Hybrid Electric Vehicle Charging
Valentin Robu
Enrico H. Gerding
Sebastian Stein

vr2@ecs.soton.ac.uk
eg@ecs.soton.ac.uk
ss2@ecs.soton.ac.uk

University Southampton, SO17 1BJ, Southampton, UK

David C. Parkes

parkes@eecs.harvard.edu

Harvard University, Cambridge, 02138, USA

Alex Rogers

acr@ecs.soton.ac.uk

University Southampton, SO17 1BJ, Southampton, UK

Nicholas R. Jennings

nrj@ecs.soton.ac.uk

University Southampton, SO17 1BJ, Southampton, UK
King Abdulaziz University, Jeddah, Saudi Arabia

Abstract
develop online mechanism allocation expiring resource dynamic agent population. agent non-increasing marginal valuation function
resource, upper limit number units allocated
period. propose two versions truthful allocation mechanism. modifies
decisions greedy online assignment algorithm sometimes cancelling allocation
resources. One version makes modification immediately upon allocation decision
second waits point agent departs market. Adopting
prior-free framework, show second approach better worst-case allocative
efficiency scalable. hand, first approach (with immediate
cancellation) may easier practice need reclaim units previously allocated. consider application recharging plug-in hybrid electric vehicles
(PHEVs). Using data real-world trial PHEVs UK, demonstrate higher
system performance fixed price system, performance comparable standard,
non-truthful scheduling heuristic, ability support 50% vehicles
fuel cost simple randomized policy.

1. Introduction
Designing mechanisms allocating scarce resources self-interested agents central
research topic artificial intelligence (Sandholm, 2002; Engel & Wellman, 2010).
aim work devise mechanisms satisfy certain desirable properties,
truthfulness efficiency. Many settings mechanisms applied
characterised dynamic supply demand, i.e., agents arrive leave market
time, availability supply also changes time. led field online
mechanism design, agents incentivised report truthfully value
c
2013
AI Access Foundation. rights reserved.

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

given allocation, also period available market (Parkes, 2007).
However, date, existing work field assumes valuations
agents certain allocation described single parameter, so-called
single-valued domains. Existing approaches consider multi-valued domains rely
access probabilistic model supply demand ability compute optimal
allocation policy, becomes computationally infeasible realistic settings.
address shortcomings, extend state art developing novel
model-free mechanism (i.e., assumes knowledge future demand supply)
multi-valued demand. particular, consider domains multi-unit demand
agents non-increasing marginal values. domains, first units allocated
agent higher (or equal) marginal value agent compared subsequent
units. online settings consider, resources continuously produced perishable,
thus available supply must allocated period. Moreover, supply available
period known advance, start period.
Examples settings non-increasing marginal values perishable resources occur many real-life settings. One example cloud computing, jobs arrive
time perishable computational resources must allocated jobs (Porter,
2004; Stein, Gerding, Rogers, Larson, & Jennings, 2011). particular, non-increasing
marginal value model applies naturally large-scale data processing optimisation
any-time computation. setting, first unit computation provides solution
certain quality, subsequent units allow improving solution, level
computation longer useful. Hence, first units valuable,
already provide good approximation desired solution, subsequent units
increase value, marginally non-increasing amount. Another example online
advertising, impressions need allocated soon users visit webpage (Constantin, Feldman, Muthukrishnan, & Pal, 2009). non-increasing marginal values also
applies setting, since additional exposure set users ad likely
decreasing impact.
third example, studied extensively paper, allocation electricity
charging plug-in hybrid electric vehicles (PHEVs). Similar pure (non-hybrid) electric
vehicles (pure EVs), vehicles charged directly electric charging point.
difference PHEVs electric motor internal combustion engine,
widely seen solution problem range anxiety, i.e., fear car run
electricity middle nowhere (Eberle & von Helmolt, 2010).1 However,
associated increase demand electricity, significant concerns within
electricity distribution industries regarding widespread use vehicles, since
high charging rates PHEVs require (up three times maximum current demand
typical home) could overload local electricity distribution networks peak times (Fairley,
2010). One approach address concern (e.g., adopted Pacific Gas Electric
Company California) introduce time-of-use pricing plans seek shift demand.
sophisticated approach takes account valuations self-interested
owners, design online mechanism schedules access dynamically order
prevent network overload. assumption decreasing marginal values justified
1. Practical examples PHEVs include versions cars Toyota Prius Honda Insight,
drive petrol, whose batteries also charged directly electrical charging point.

176

fiAn Online Mechanism Multi-Unit Demand

vehicle owner likely use first units electricity, always
use combustion engine alternative case runs electricity (and
car still used even fully charged). background, main
contributions work are:
develop new model-free online mechanism settings participating agents
non-increasing marginal values units perishable good. adopt greedy
algorithm coupled method modify allocation ensure incentive compatibility. involves cancelling part proposed allocation, explore
two ways performing cancellation: immediately, i.e., time step resource actually allocated, departure agent market
(at point must take back allocated items). variants (weakly)
dominant-strategy incentive compatible (DSIC), participants incentive
misreport valuations arrival-departure dynamics.
analyse worst-case performance achieved mechanisms relative
optimal offline allocation, considering number units need remain
unallocated order achieve incentive compatibility total value
allocation (i.e., allocative efficiency). variation on-departure cancellation
results higher allocative efficiency, tractable, may involve additional
practical challenges. example, PHEV charging domain, occasionally
requires vehicles battery partially discharged prior departure.
evaluate online mechanism numerical simulations abstract domain PHEV charging domain, compare results several benchmarks
assume non-strategic agents, including optimal offline solution, scheduling
heuristic greedy algorithm without cancellation. simulations PHEV
domain based real data large-scale trial (pure) EVs UK. Valuations derived real monetary savings, considering factors fuel prices,
distance owner expects travel, energy efficiency vehicle. results establish mechanism outperforms fixed-price mechanism
terms allocative efficiency domains, performing slightly worse
non-incentive compatible scheduling solutions. addition, mechanism
on-departure cancellation scales easily hundreds agents.
focus allocative efficiency rather revenue, appropriate many
domains interest. example, PHEV charging domain, reasonable
goal allocate capacity efficiently order maximise value user base
power company, given significant constraints charging capacity. Moreover, many
cloud computing applications (for example, large-scale scientific computing), goal
allocate capacity jobs urgent important. However, practice
seller wants guarantee minimum revenue unit sold, would easy
include reserve price. minimum price set units time
points, would affect properties mechanism.
remainder paper organised follows. first discuss related work
(Section 2), formally introducing model (Section 3). Section 4, define
177

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

online mechanisms study strategic properties. Then, Section 5, develop
worst-case analytical results, followed Section 6 discussion compute
allocations payments practice. Using real synthetic input data, present
results experimental evaluation mechanisms Section 7, conclude
Section 8.

2. Related Work
section, first review existing work online mechanism design (Section 2.1),
provide background PHEV charging application, along overview
previous work considers problem (Section 2.2).
2.1 Online Mechanism Design
One line work online mechanism design aims develop online variants VickreyClarke-Groves (VCG) mechanism. context, Parkes Singh (2003) consider
problem maximising long-term allocative efficiency system self-interested agents
arrive depart dynamically. model online mechanism design problem
Markov decision process (MDP), whose solutions used implement optimal policies
truth-revealing Bayes-Nash equilibrium. related work, Gershkov Moldovanu
(2010) examine allocation set goods dynamic population randomly arriving
buyers. consider two settings: one common deadline allocating
objects buyers, second one without firm deadline, buyers
impatient, assigning higher value items allocated sooner.
Unlike Parkes Singh (2003), Gershkov Moldovanu (2010), mechanism proposed paper model-free (which advantage prior knowledge distribution required agents types future allocations),
focus stronger concept dominant-strategy incentive compatibility (where reporting truthfully best response regardless agents doing, even
irrational). approach requires fewer assumptions, makes computing allocations
tractable compared VCG-like approaches. VCG generally requires
allocations optimal expectation (perhaps constrained space policies),
whereas, show, use greedy heuristics.
Model-free settings considered Hajiaghayi et al. (2005), Parkes (2007) Porter
(2004). work Porter examines scheduling jobs single machine proposes
incentive compatible mechanism setting. However, work assumes setting
results job released agent completion agents
reported deadline. assumption reasonable scheduling computational jobs
server, suitable setting, since goods (i.e. electricity units) must
allocated instantly become available.
work considers online setting similar one consider
Hajiaghayi et al. (2005). study problem online scheduling single, re-usable
resource finite time period, agent arrival-departure window
active market. Agents may misreport valuation, well
arrival departure, subject assumption limited misreports (i.e.,
early arrival later departure misreports possible). setting, characterise
178

fiAn Online Mechanism Multi-Unit Demand

truthful allocation payment policies, prove worst-case approximation ratios
respect optimal offline allocation. key limitation mechanism proposed
Hajiaghayi et al. concerns single-valued domains, whereas consider multi-unit setting
decreasing marginal values. show mechanism directly apply
multi-unit case, requiring, cases, additional cancellation rules applied
ensure truthfulness.
Multi-unit demand considered work Lavi Nisan (2004), propose
online auction model mechanism required make decisions bid
received. provide characterisation incentive compatibility domains
terms supply curves, concept relates closely threshold mechanism
characterisation. However, online auctions model, auctioneer must respond
bid immediately, considering bids. response, mechanism
determines quantity sold price paid. would
applicable setting presented paper, limit number
perishable units allocated time interval. Moreover, window-based
allocation allows prices determined dynamically, based bids observed
agents departure. similar vein, Babaioff, Blumrosen, Roth (2010)
consider online auction model future supply unknown, characterise several
subclasses truthful mechanisms. domain different ours, bidders
model specify multi-dimensional demands non-increasing marginal values.
related work online mechanism design adapts consensus algorithm
online stochastic optimisation proposed Bent Van Hentenryck (2004) setting
self-interested agents. context, Parkes Duong (2007), Constantin
Parkes (2009) first propose idea modifying decision algorithm cancelling
part allocation order ensure incentive compatibility. Unlike present paper,
setting assumes single-valued private information approaches applicable
agents non-increasing marginal values. Also single-valued settings
pure EV domain, Stein, Gerding, Robu, Jennings (2012) propose model-based online
mechanism assumes knowledge future supply uses pre-commitment ensure
online allocations truthful.
2.2 Electric Vehicle Charging
Multi-agent systems AI techniques increasingly used address challenges
Smart Grid (e.g., Vytelingum, Voice, Ramchurn, Rogers, & Jennings, 2011; Robu, Kota,
Chalkiadakis, Rogers, & Jennings, 2012), EV charging one important
application areas. Work automatic scheduling EV charging typically allows individual vehicle owners indicate times car available charging,
enabling automatic scheduling satisfying constraints distribution network.
vein, Clement, Haesen, Driesen (2009) propose centralised scheduler,
makes optimal use network capacity vehicle owners report expected future
vehicle use system. Sundstrom Binding (2012) tackle problem charging
multiple electric vehicles considering distribution grid constraints, formalise underlying optimisation problem propose novel method based load flow solve it.
strategic behaviour remains possible approaches; e.g., owner may indicate
179

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

earlier departure time travel distances order receive preferential charging.
result high cognitive load car owners, may lead inefficient schedules
based actual user requirements, leading efficiency loss.
potential speculation strategic agents identified crucial problem
scheduling problems, scheduling computational jobs cluster (Porter,
2004), scheduling computation-intensive services cloud (Stein et al., 2011)
market-based scheduling loads transportation logistics (Robu, Noot, La Poutre, & van
Schijndel, 2011). increase number EVs requiring charging, potential
manipulation become increasingly pressing problem PHEV scheduling well.
approaches EV scheduling include lottery-based solution proposed Vasirani Ossowski (2011), decision whether charge vehicle
determined lottery system, designed ensure level fairness resulting
allocation. Unlike work, however, authors use game-theoretic principles
prove participating vehicles incentive report preferences truthfully,
thus scheme agents may incentive speculate. Moreover, experimental
analysis reported Vasirani Ossowski adopt real data derive EV driving
patterns, charging capacities network constraints.
recent work investigates using grid-integrated electric vehicles (GIVs) sell power
storage capacity back grid concept known vehicle-to-grid (V2G) (c.f.
Kamboj, Kempton, & Decker, 2011). work different ours, study
problem coordinated charging PHEVs local network capacity constraints.
subsequent work model present paper (which first appeared Gerding,
Robu, Stein, Parkes, Rogers, & Jennings, 2011, Robu, Stein, Gerding, Parkes, Rogers,
& Jennings, 2011), study problem charging pure EVs, must receive set
amount charge, otherwise derive value allocation (Stein et al., 2012).
second paper studies problem two-sided markets, PHEVs charging
stations compete matched (Gerding, Stein, Robu, Zhao, & Jennings, 2013). Unlike
present model, papers consider single-minded bidders, work Stein
et al. assumes access probabilistic model environment.

3. Model
consider online mechanism design setting discrete time steps,
period, multiple indivisible units perishable good sold, agent requires
multiple units within certain period. show Section 3.1, model also
used continuously available resources, electricity computational resources.
case, allocation decision consists amount resource consumed
agent next period following time point.
convenience, overview notation provided Table 1. Formally, let S(t)
denote supply available time t. Let I(t) = {1, 2, . . .} denote set agents
market time already left market. assume access
probabilistic model future arrivals, departures future supply beyond current
time period t. Agents numbered according arrival time. agent I(t)s
type described tuple = hvi , ai , di , ri , vi marginal valuation
vector, ai di , di ai , arrival departure times (the earliest latest
180

fiAn Online Mechanism Multi-Unit Demand

times agent available market), ri maximum consumption rate (i.e.,
maximum number units agent consume time t), set
admissible types. Upon arrival, agent needs report valuation function
maximum consumption rate. two aspects agents reported type required
remain unchanged agent present, although departure time modified
(it becomes known mechanism actual departure).
element vi,k valuation vi called marginal valuation, represents
agents willingness pay k th unit good, given acquired k 1 units.
require:
Assumption 1. Marginal valuations non-increasing, i.e., i, k : vi,k vi,k+1 .
Given this, agents utility functionPis U (k, x, ) = V (k, ) x, x agents
payment mechanism, V (k, ) = kj=1 vi,j total value derived given type,
k number units allocated agent arrival departure.
mechanism asks agents report types and, based information, decides
allocation supply payment units received. Since agents misreport
type, aim design mechanism incentivises agents make truthful
reports. denote reported type = hvi , ai , di , ri i. results use common
assumption online mechanism design literature (Hajiaghayi et al., 2005), agents
cannot report earlier arrival later departure. addition, assume agents
cannot misreport higher maximum consumption rate.
Assumption 2. Limited Misreports: Agents cannot report earlier arrival, later
departure, higher maximum consumption rate, i.e., ai ai , di di , ri ri must hold.
following, reports satisfy Assumption 1 Assumption 2 said
admissible. Given assumption, aim develop mechanism dominantstrategy incentive compatible (DSIC), i.e., agents best reporting = , matter
agents report.
Formally, let = {i |i I(t)} denote types agents time t, =
{j |j I(t), j 6= i} types agents except i, similarly, denote
corresponding reported types. Note that, brevity, remove dependence
hti
notation. Furthermore, ki denotes endowment (or number units allocated far)
hti hti
beginning time t, including allocation time t, khti = hk1 , k2 , . . .i.

hd+1i

Furthermore, ki = ki

denotes agent endowment upon reported departure.
hti

mechanism defined allocation policy, (I |khti ), I(t), determines
number units allocated agent time given current endowment,
payment policy, xi (i |ki ), I(t), calculates total payment allocated
units. allocation made online, payment needs finalised upon
ht+1i
hti
hti
reported departure agent. Note ki
= ki + (I |khti ). also use (i ) =
Pdi
hti
hti
t=ai (I |k ) denote total number units allocated agent i, given reported
type.
aim find mechanism satisfying following property:
181

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Definition 1. (Dominant-Strategy Incentive Compatible (DSIC)) mechanism DSIC
reporting truthfully, i.e., = , weakly dominant strategy. Formally, agents i,
admissible , :
V (i (i ), ) xi (i |i (i )) V (i (i ), ) xi (i |i (i ))

(1)

3.1 Application Plug-In Hybrid Electric Vehicle Charging
applying model PHEV charging domain, agents compete limited charging
capacity behalf EV owners within neighbourhood. assume market
electricity PHEV charging separate regular household consumption.
Given this, available supply, S(t), charging PHEV residual supply
regular household consumption removed. supply also include electricity
uncertain sources, shared renewable generator, e.g., shared neighbourhood
wind turbine solar panel installation.
scenario, unit electricity defined amount kWh charging
lowest rate interval (e.g., lowest rate 6.5 A, 230 V
hourly slots, unit 6.5 230 V 1 h = 1.495 kWh). Charging points typically allow
charging occur different rates, maximum consumption rate, ri ,
refers maximum charging rate charging point given battery. Since units
indivisible, means charging rate needs multiple lowest rate.
example, agent allocated 2 units single time step, charging rate
twice lowest charging rate time interval. Given focus network capacity,
supply assumed perishable capacity left unused time cannot allocated later.
time arrival, ai , departure, di , refers interval vehicle
available charging (i.e., home used). However, agent
believes benefit delaying arrival, wait plugging
vehicle electricity network. Therefore, arrival report ai time
owner physically plugs vehicle electricity network, misreport consists
plugging arrival. Similarly, reported departure, di , simply represents time
vehicle unplugged electricity network. Although arrival departure
modeled part reported type, practice need communicated
advance mechanism, simply observed occur, i.e., owner
plugs unplugs vehicle.
limited misreports assumption (Assumption 2) reasonable context, since
agents cannot physically plug EV home. requirement ri ri
also natural PHEV charging. electric batteries configured charge
slower rate, charging faster rate one allowed manufacturer
might destroy them.
time arrival, agent needs report marginal valuation vector vi .
clear interpretation marginal valuation PHEVs always use petrol
substitute electricity. marginal value additional unit charge
expected money saved incur cost petrol.2 determining exact
value, also need consider amount purchased electricity owner use
2. Note implicitly assume petrol (i.e. gasoline) always available substitute,
sufficient refueling points vehicle run petrol.

182

fiAn Online Mechanism Multi-Unit Demand

v1 = h10, 4i
v2 = h5i

Agent 1
Agent 2
Agent 3

v3 = h2i
S(t1 ) = 1

S(t2 ) = 1

Figure 1: Example showing arrivals, departures, valuation vectors 3 agents.
expectation. example, certain use first unit electricity, maximum
willingness pay would equal equivalent cost petrol. units become less likely
used, expected value decreases. value marginal unit expected
savings compared using petrol alternative. Section 7 provide detailed analysis,
confirming non-increasing marginal value (Assumption 1) hybrid EVs.

4. Online Mechanism
section, first present simple greedy allocation policy, show cannot
coupled payment rule provide truthfulness. Continuing, combine two
variations idea modifying allocation generated greedy rule, provide
theoretical analysis properties.
4.1 Greedy Allocation Policy
hti

Let vector vi

= hvi,khti +1 , . . . , vi,khti +r denote agent reported marginal values






hti

next ri units, given endowment ki time t. agents reported willingness
hti
pay units available time and, follows, refer vector vi
active (reported) marginal valuations time t. Furthermore, let V hti denote multiset
values agents present market time t, i.e., I(t)
ai di . Next, define set operator maxhki V return highest k
elements multiset V (or, |V| < k, return V). Then, greedy allocation policy is:
Definition 2 (Greedy Allocation Policy). time step t, allocate S(t) units
every agent receives one unit good active marginal valuations
included maxhS(t)i V hti .
ignore issues tie breaking throughout paper simplify exposition.
note, however, results presented hold implementing either random tie
breaking rule, first come, first served rule, breaks ties favour agent
arrived market first.
provide example greedy allocation, consider active marginal valuations
given Table 2. case, multiset active marginal valuations consists V hti =
h7, 6i h10, 6, 6i h8i = h7, 6, 10, 6, 6, 8i (in particular order). Then, S(t) = 3,
highest active marginal values allocate resources agents marginal
values 10, 8, 7. Thus, example, agent receive 1 unit.
order show greedy always DSIC, consider example illustrated
Figure 1, involving 2 time steps 3 agents. Suppose supply S(t) = 1 time
183

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Supply, Agents Preferences (Section 3)
Supply perishable units time
set agents arrived far time
Type agent
Marginal valuation vector, vi,k value k th unit
Arrival time, departure time, maximum consumption rate
Set admissible types, i.e., subject Assumptions 1 2
types agents I(t)

S(t)
I(t) = {1, 2, ...}
= hvi , ai , di , ri
vi = hvi,1 , vi,2 , ...i
, , ri

= {i |i I(t)}
= {j |j I(t),
j 6= i}
, ,
P
V (k, ) = kj=1 vi,j
U (k, x, ) =
V (k, ) x
hti
ki
hti hti
khti = hk1 , k2 , ...i

hd+1i
ki = ki

types agents j I(t) except
Reported types
Total value given k units allocated ai di
Agent utility, x payment mechanism
General Mechanisms (Section 3)
Agent endowment beginning time
endowment agents I(t) beginning time

Agent endowment reported departure
Allocation policy, i.e., number units allocated time
hti
(I |khti )
given agents reports current endowments khti
Payment policy, i.e., agent payment given reported types
xi (i |ki )
agents, agent endowment departure.
Greedy Policy DSIC Mechanism (Sections 4 4.2)
hti
vi =
Agent reported active marginal values
hvi,khti +1 , ..., vi,khti +r






Vi0

Multiset reported active marginal values (where union operator used multiset operator throughout paper)
Active reported marginal values agent would never
present market
hti
Zeros added multiset ensure |Vi0 | S(t)

max V, min V

Returns highest respectively lowest k elements multiset V

V hti =



hti
iI(t) vi

hti

Vi
hti

hki

hki

Externality imposed agent others, i.e., marginal valuations missing due agent allocated
= min max
hri
hS(t)i
min(ri , S(t)) units time


hti
Vector marginal payments time t, pi,k price

[

hti
ht
pi = incr
Ei agent charged k th unit, operator incr orders
=ai
elements multiset increasing order

hti
Ei



hti
Vi0

hdi

pi = pi

Marginal payment vector reported departure agent

Table 1: Main notation references sections first introduced.
184

fiAn Online Mechanism Multi-Unit Demand

agent (i)
1
2
3

ri
2
3
1

vi
h8, 7, 6, 5i
h10, 6, 6, 4, 4i
h9, 8, 8, 7i

hti

ki
1
0
2

hti

vi
h7, 6i
h10, 6, 6i
h8i
hti

Table 2: Example three agents active marginal valuations vi , given marginal
hti
valuations vi , endowments ki , maximum consumption rates ri .

step ri = 1 agents i. Assuming truthful agents, greedy would allocate
units agent 1, agent 1 highest active marginal value time steps
h1i
h1i
h2i
h2i
(v1 = h10i > v2 = h5i, v1 = h4i > v3 = h2i).
Now, consider question finding payment policy makes greedy allocation
policy DSIC. much agent 1 pay? answer this, note payment
unit allocated time = 1 least 5. Otherwise, agent 1 present
market time = 1 valuation v1,1 (5 , 5), would incentive
misreport v1,1 > 5 still win. Similarly, payment unit allocated time
= 2 least 2. Thus, minimum payment agent 1 allocated two units
x1 (|1 = 2) = 7.
hand, much agent 1 pay allocated one unit
instead? argue 2. Suppose, contradiction, payment set
larger value x1 (|1 = 1) = 2 + (where > 0). agents first marginal
value v1,1 instead 2 < v1,1 < 2 + (with remaining marginal values zero),
would win period 2, would pay 2 + hence negative utility. However,
x1 (|1 = 2) 7 x1 (|1 = 1) 2, agent 1 wants one unit, two,
allocated greedy mechanism (its utility one unit greater two,
10 2 > 10 + 4 7). Hence, greedy allocation policy cannot made DSIC setting
payments.
example shows problem particular payment policy,
intrinsic greedy allocation policy. particular, problem allocation
policy satisfy necessary property W-MON (Bikhchandani, Chatterji, Lavi,
Mualem, Nisan, & Sen, 2006):
Definition 3 (Weak Monotonicity (W-MON)). allocation policy , W-MON if,
every I(t), = hvi , ai , di , ri i, = hvi , ai , di , ri , N 1 , set
types subject non-increasing marginal valuations, following equation holds:
V (i (i ), ) V (i (i ), ) V (i (i ), ) V (i (i ), )

(2)

words, changing agent type (while keeping types agents fixed)
type another type changes allocation (i ) (i ),
resulting difference utilities new original outcomes evaluated new
type agent (denoted function V (, )) must less difference utilities
185

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

evaluated original type agent (denoted function V (, )). Using notion
demonstrate greedy allocation policy DSIC setting.
Theorem 1. greedy allocation policy DSIC multi-valued domains nonincreasing marginal valuations.
Proof. Bikhchandani et al. (2006, Lemma 1) know necessary condition
DSIC allocation policy satisfy W-MON (see Definition 3). Equation 2
true, shown following must hold:
(i )



(i )

> (i )

X

(i )

vi,k



k=i (i )+1

X

vi,k

(3)

k=i (i )+1

words, units allocated one type compared another type,
type also higher marginal values units. Consider example
Figure 1, look W-MON condition agent 1 varying type. keep
arrival departure fixed, i.e., ai = ai , di = di . Suppose v1 = h10, 4i
example, v1 = h4 + , 4 + i, 0 < < 1. Note 1 (1 ) = 2, (1 ) = 1
(agent 1 allocated unit first time step type changed 1
1 ). Since 1 allocated additional unit (compared 1 ), W-MON requires 1
< v , thereby
values second unit higher equal 1 . However, see v1,2
1,2
violating Equation 3. example demonstrates greedy allocation policy
W-MON, therefore DSIC.
4.2 Achieving Truthfulness Cancellation
Addressing problem W-MON, consider two types modifications allocation decision greedy policy, designed achieve monotonicity. first
immediate cancellation, units simply left unallocated, i.e., none agents
receive unit, even demand. second on-departure cancellation,
units initially allocated using greedy approach, departure agent,
overallocated units removed.3
model on-departure cancellation efficient generally requires
fewer cancellations, also computationally efficient calculating payments
allocations. However, depending domain, may always possible
remove units allocated.
following, detail allocation policies explain payments computed. give example shows difference two mechanisms
and, lastly, provide analysis economic properties mechanisms.
defining allocation policies, show compute agents marginal
payment vector, determines, additional unit, price agent would need
pay unit. marginal payments used determine
cancel allocation, well agents total payment given allocation.
3. PHEV setting, corresponds first charging battery later discharging overallocated
units.

186

fiAn Online Mechanism Multi-Unit Demand

necessary condition truthfulness payments cannot depend agents
report except effect allocation; e.g., see work Nisan, Roughgarden,
hti
Tardos, Vazirani (2007, Proposition 9.27). end, let Vi denote multiset
active marginal valuations agents market time t, agent removed
market rerun ai onwards.
hti
cannot simply derive Vi V hti since removing agent could affect endowments agents earlier time steps. example, setting Table 2,
agents 1 3 endowments time therefore removing either agents
likely increase endowments agents, thereby changing dynamics
hti
entire market. ensure Vi truly independent agent i, market needs
re-run point agent first entered market, process
needs repeated agent I(t) ai di , whose payments
computing.
hti
case |Vi | < S(t), furthermore add number zero-valued bids refer
hti

hti

enlarged set Vi0 , ensure |Vi0 | S(t). Next, similar operator maxhki ,
define set operator minhki V return lowest k elements multiset V (or,
|V| < k, return V). define externality agent would impose agents
min(ri , S(t)) S(t) units time as:


hti
hti
Ei = min max Vi0
hri

hS(t)i

hti

hti

Note cardinality Ei equal |Ei | = min(ri , S(t)). Intuitively, multiset
hti
Ei contains marginal values agents would lose agent
hti
win ri units time t. example, let V1 = h1, 4, 5, 7, 9, 10i (sorted convenience),
hti

S(t) = 4, r1 = 2. Then, E1 = h5, 7i.
agent active single time step, externality would specify
payment unit. is, using example, agent 1 allocated single
unit mechanism, payment would 5. hand, allocated 2
units, payment would 5 + 7 = 12. intuition regular
Vickrey-Clarke-Groves (VCG) mechanism, total payment corresponds
sum externalities.
compute overall payments online, need combine externalities across
time steps agents active period current time t. this, define
hti
ordered vector marginal payments, pi , follows:


ht
hti

E
,
pi = incr

=ai
incr operator orders elements multiset increasing order,
use union symbol denote union multisets (and element appear
multiple times).
hti
Now, pi,k price agent charged k th unit good. Intuitively,
minimum valuation agent could report winning unit time t.
prices adjusted time step. particular, since vector increasing order,
187

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

hti

elements added time increases, pi,k either stays decreases
given k, never increase. following, use pi,k denote agent marginal
payment k th unit time di .
Given this, decision payment policies defined follows:
Allocation Policy decision allocate consists two stages:
Stage 1 time step t, pre-allocate using greedy allocation policy (see
Definition 2).
Stage 2 consider two variations decide cancel pre-allocation:
Immediate Cancellation (IM). Leave unit unallocated whenever marginal payment time unit greater marginal value, i.e.,
whenever:
hti

hti

hti

hti

vi,k < pi,k ki < k ki +

On-Departure Cancellation (OD). departing agent, cancel allocation unit k ki vi,k < pi,k .
Payment Policy Payment always occurs reported departure. Given ki units
allocated agent i, payment collected is:
xi (i |ki ) =

Xki

k=1

pi,k

(4)

following, refer two mechanisms immediate on-departure
cancellation IM OD respectively, corresponding allocation policies im
od . distinction two mechanisms made, used.
payment policy mirrors allocation policy. example, immediate cancellation
hti
used, agent times t, values pi vector computed
re-running market, absence agent using immediate cancellation, based
reports agents. Conversely, on-departure cancellation used,
policy used computing pi prices.
4.3 Examples
demonstrate two mechanisms work, present two examples. aim
first example show difference immediate on-departure cancellation.
second example illustrates effect changing maximum consumption rates.
example shows specific instances, increasing agents maximum consumption rate
actually increase number cancellations.
4.3.1 Example 1
first example extends setting shown Figure 1 include third time step, = 3.
agents 1 3 remain market = 3 (i.e., d1 = d3 = 3) new agents
arrive. Furthermore, S(t) = 1 {1, 2, 3}, three units
allocated total. before, suppose ri = 1 agents. Table 3 shows
188

fiAn Online Mechanism Multi-Unit Demand

t=1

t=2

t=3
IM

t=3
OD

agent 1:
a1 = 1, d1 = 3,
v1 = h10, 4i, r1 = 1
h1i
h1i
k1 = 0, v1 = h10i
h1i
h1i
V1 = E1 = h5i
h1i
p1 = h5i
imh1i
odh1i
1
= 1
=1
h2i
h1i
k1 = 1, v1 = h4i
h2i
h2i
V1 = E1 = h2i
h2i
p1 = h2, 5i
imh2i
1
=0
odh2i
1
=1
h3i
h1i
k1 = 1, v1 = h4i
h3i
h3i
V10 = E1 = h0i
h3i
p1 = h0, 2, 5i
imh3i
1
=1
h3i

agent 2:
a2 = 1, d2 = 1,
v2 = h5i, r2 = 1
h1i
h1i
k2 = 0, v2 = h5i
h1i
h1i
V2 = E2 = h10i
h1i
p2 = h10i
imh1i
odh1i
2
= 2
=0

agent 3:
a3 = 2, d3 = 3,
v3 = h2i, r3 = 1

h2i

h1i

h3i

h1i

h3i

h1i

k3 = 0, v3 = h2i
h2i
h2i
V3 = E3 = h4i
h2i
p3 = h4i
imh2i
odh2i
3
= 3
=0
k3 = 0, v3 = h2i
h3i
h3i
V3 = E3 = h4i
h3i
p3 = h4, 4i
imh3i
3
=0

h1i

k1 = 2, v3 = hi
h3i
h3i
V10 = E1 = h0i
h3i
p1 = h0, 2, 5i
odh3i
1
=0

k3 = 0, v3 = h2i
h3i
h3i
V30 = E3 = h0i
h3i
p3 = h0, 4i
odh3i
3
=1

Table 3: Example run mechanism 3 agents 3 time steps IM
OD mechanisms. Grey cells indicate different values IM OD policies.
hti

hti

hti

endowments ki , active marginal valuations Vi , externalities, Ei , marginal
hti

hti

payments pi , allocation decisions different time steps.
start considering allocations payments using immediate cancellation.
time = 1, Stage 1 mechanism pre-allocates unit agent 1, since
h1i
v1,1 = 10 p1,1 = 5, pre-allocation cancelled second stage. time
= 2, unit gets pre-allocated agent 1 since active marginal value greater
h2i
h2i
h2i
agent 3, i.e., v1 = h4i > v3 = h2i. However, V1 = h2i inserted
h2i

h2i

beginning p1 vector, result v1,2 = 4 < p1,2 = 5 (at prices, agent
1 prefers allocated one unit instead two). Consequently, pre-allocation gets
cancelled unit goes neither agents.
h3i
time = 3, active marginal value agent 1 still v1 = h4i, since endowment
unchanged, since agent 1 still highest active marginal value, preallocated unit. calculate marginal payment agent 1, recall allocation
policy needs recomputed agent 1 entirely removed market. case
agent 3 would allocated unit time = 2, thus time = 3 active
h3i
marginal value agent 0. Thus, value 0 inserted p1 vector.
h3i
= 3, however, v1,2 = 4 p1,2 = 2, therefore pre-allocation cancelled.
interesting exercise see happens marginal payment vector agent
3 = 3. calculate this, remove agent 3 rerun market = 2.
189

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

agent1
agent2
agent3

v1 = h10, 8, 3i
v2 = h7i
v3 = h1i
S(t1 ) = 2

S(t2 ) = 1

Figure 2: Example showing arrivals, departures, valuation vectors 3 agents.

h2i

case, time = 2, marginal payment vector agent 1 becomes p1 = h0, 5i. Since
h2i
marginal payment second unit, p1,2 , remains unchanged, pre-allocation
still cancelled! Therefore, even agent 3 market, second unit remains
h3i
unallocated, agent 1s active marginal value = 3 v1 = h4i,
h3i
h3i
V3 = h4i. Given this, agent 3s marginal payments become p3 = h4, 4i. Note
marginal payment first unit (4) higher marginal value unit (2),
consistent allocation. Otherwise, marginal payment
lower, agent 3 would incentive overreport win unit = 3.
So, case immediate cancellation, two three units allocated agent 1,
h3i
h3i
agent pays x1 = p1,1 + p1,2 = 0 + 2 = 2. third unit allocated
agent. Note unit cannot go agent 3, payment would
h3i
p3,1 = 4, resulting negative utility agent 3.
consider setting on-departure cancellation. first two time
steps before, except cancellation = 2 (since done
departure needed). changes endowment state agent 1 = 3, therefore
h3i
active marginal value agent 1 = 3 equal v1 = hi,
agent 3 removed market. Therefore, unit pre-allocated agent 3,
h3i
h3i
payment unit p3,1 = p3,1 = 0. vector p1 remains unchanged compared
immediate case. point, longer need cancel one pre-allocations
agent 1, since received k = 2 units, allocation immediate
cancellation policy, note v1,2 > p1,2 .
pre-allocations cancelled on-departure policy, policy
efficient. show remainder paper, on-departure policy never
cancels more, typically cancels fewer pre-allocations compared immediate one.
Still, possible construct examples where, worst case, half pre-allocations
need cancelled, even on-departure policy.
Furthermore, note units given away free (i.e., payment
units zero). standard problem auctions insufficient competition,
trivially resolved e.g. introducing minimum price reserve price
unit good. However, reduce efficiency since units remain unallocated
fall reserve price. consider reserve prices paper,
economic properties mechanism continue hold reserve prices, long
reserve prices time points (otherwise could incentive
misreport arrival time).
190

fiAn Online Mechanism Multi-Unit Demand

4.3.2 Example 2
next example, depicted Figure 2, shows setting two time steps three
agents, different preferences supply first step two units,
change maximum consumption rate agent 1. consider two cases:
maximum consumption rate agent 1 r1 = 1.4 case, one marginal
value taken agent per time step. time t1 , marginal valuations v1,1 = 10
agent 1, v2,1 = 7 agent 2 pre-allocated, time t2 , marginal value v1,2 = 8
agent 1 pre-allocated. prices charged agent 1 are: p1 = h0, 1i, without
agent 1 market, would free, spare unit time t1 available unit
t2 would sell agent 3 1. pre-allocation gets cancelled case, actual
allocation equivalent optimal offline allocation.
maximum consumption rate agent 1 r1 = 2. Then, time t1 , greedy
policy described allocates two marginal values agent 1: v1,2 = 10 v1,2 = 8,
higher v2,1 = 7, agent 2 drops market. time t2 ,
unit pre-allocated agent 1 (due marginal value 3 higher 1).
However, marginal payments vector required agent 1 p1 = h0, 1, 7i,
marginal valuations v1 = h10, 8, 3i. Given prices, agent 1 prefers two units
three (because 10 + 8 1 > 10 + 8 + 3 1 7), third cancelled. overall
efficiency lower, pre-allocation third available unit cancelled, whereas
r1 = 1 allocated agent 2. Note, however, although efficiency much
lower, agent 1 incentive declare true maximum consumption rate r1 = 2 as,
case, payment change.
4.4 Establishing Truthfulness
section prove mechanisms DSIC assumptions
non-increasing marginal valuations (Assumption 1) limited misreports (Assumption 2).
following, first establish DSIC respect valuations only, prove
truthful reporting arrival departure times separately. detail, proceed
following 3 stages:
(i) define concept threshold policy, show that, coupled
appropriate payment policy, given admissible pair hai , di i, allocation policy
threshold policy, mechanism DSIC respect valuations (Lemma 1).
(ii) show allocation policy threshold policy (Lemma 2).
(iii) show agents truthfully report valuations, reporting ai = ai , di = di ,
ri = ri weakly dominant strategy (Lemma 3).
results combined Theorem 2 show mechanism DSIC.
Definition 4 (Threshold Policy). allocation policy threshold policy if, given
agent fixed hai , di , ri , exists marginally non-decreasing threshold
vector , independent report vi made agent i, following holds: k, vi :
(i , ) k vi,k k .
4. Note two agents desire one unit, maximum consumption rate irrelevant
example.

191

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

words, threshold policy potentially different threshold k k,
agent receive least k units reported valuation k th
item least k .
threshold policy satisfies W-MON, sufficient DSIC domain since
bounded agent valuations domain completely ordered, meaning
payoff types agree weak preference ordering allocations (i.e.,
always weakly better less), indifference way goods allocated
agents (Bikhchandani et al., 2006). show allocation policy threshold property, thus satisfies W-MON, also handles misreports arrivals,
departures maximum charging rates.
Importantly, vector non-decreasing, i.e., k+1 k ,
independent reported valuation vector vi . properties satisfied
pi vector, use show mechanism threshold policy.
First, however, show threshold policy appropriate payments DSIC
respect valuations:
Lemma 1. Fixing admissible haj , dj , rj j fixing , threshold
policy coupled payment policy:
xi (i , ) =

Pi (i ,i )
k=1

k ,

vi marginally non-increasing, reporting vi truthfully weakly dominant strategy.
Proof. Agent utility rewritten as:
Pi (i ,i )
ui (i ; ) = k=1
(vi,k k )

Since independent vi , agent potentially benefit changing allocation,
(i , ). Since values k+1 k (non-decreasing threshold vector) vi,k+1 vi,k
(non-increasing marginal values), Definition 4 vi,k k 0 k (i )
vi,k k 0 k > (i ). Suppose that, misreporting agent allocated
(i ) > (i ), ui (i ; ) < ui (i ; ) since:
Pi (i ,i )

k=i (i ,i )+1

(vi,k k ) < 0

Similarly, misreporting (i , ) < (i , ) results ui (i ; ) < ui (i ; )
since:
Pi (i ,i )
(vi,k k ) 0
k=i (i ,i )+1

misreporting effect allocation, utility remains same. Therefore,
incentive agent misreport valuations.

Note greedy allocation policy threshold policy. Indeed, shown
already satisfy W-MON. next lemma shows threshold condition
holds cancel allocations according policies, set threshold
values k = pi,k .
192

fiAn Online Mechanism Multi-Unit Demand

Lemma 2. Given non-increasing marginal valuations, allocation policy Section 4.2
(for either cancellation policy) threshold policy k = pi,k .
hti

hti

Proof. First, definition vector pi pi Section 4.2, values pi
independent reports vi made agent i. component values
ha
hti
Vii , . . . , Vi computed based reports agents, first removing
hti

agent market. Note pi pi also affected reported arrival
time, departure time, maximum consumption rate agent i, lemma
concerned truthful reporting agents valuations, take reported
arrival deadline given, require truthful point.
Second, need show two inequalities, thus proof done two parts. Part 1:
Whenever vi,k pi,k , allocates least k units agent i. Part 2: Whenever vi,k < pi,k ,
allocates strictly fewer k units agent i.
Part 1: Let vi,k pi,k . Suppose agent uniform marginal values, vi,k ,
hti
first k units (i.e., vi,1 = vi,2 = . . . = vi,k ). Note externality Ei contains marginal
values time agent displace winning ri units good time
step (that is, marginal values reappear next time step agents
remain market). Given this, long agent fewer k units then, Stage
1 mechanism, time step agent market, win exactly
hti
units marginal values Ei less vi,k , i.e. win units
hti
hti
1 j |Ei | vi,k Ei,j (ignoring tie breaking). Note externalities (and
thus marginal payments) calculated removing agent market
first time entered, contain displaced marginal values. However,
even when, winning unit, agent displaces losing marginal value future time
step, since value less equal vi,k , affect allocations first k
units future time steps agent since continue higher marginal value.
Now, pi,j pi,k j k (by definition), must least k units
hti
hti
pi,k Ei,j , 1 j |Ei | period ai di , since vi,k pi,k , agent wins
least k units Stage 1.
Furthermore, whenever j units particular time step, marginal payments
units appear within first k + j first elements pti vector, k
number units earlier time steps (since values lowest
clearing payment, ordered ascendingly). agent wins unit
hti
hti
externality Ei,j Stage 1 vi,k Ei,j (given uniform valuations), follows
vi,k = vi,j pi,j whenever wins unit Stage 1. Therefore, pre-allocations
cancelled Stage 2.
holds agent uniform marginal values vi,k first k units.
fact, however, non-increasing valuations, vi,j vi,k , 1 j k,
thus allocation policy allocate least k units agent i.
Part 2: Let vi,k < pi,k . First consider on-departure cancellation case. per
definition Stage 2 mechanism, allocation unit k cancelled. However, still
need show pre-allocated units j > k cancelled well. Since pi,j pi,k (by
definition) vi,j vi,k (since valuations marginally non-increasing assumption)
j > k, follows vi,j < pi,j j > k. Therefore even Stage 1 pre-allocates
193

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

k units, cancelled Stage 2, thus strictly fewer k units
remain.
hti
consider immediate cancellation case. Note pi,k pi,k tk di ,
tk time k th unit allocated. is, marginal payment values
ht
decrease time. Since vi,k < pi,k (by assumption) pi,k pi,kk , follows
ht

hti

vi,k pi,kk . Thus follows vi,k < pi,k (ai + k 1) di . Consider case
where, time tk , k th unit allocated Stage 1. result, pre-allocation k th
unit always cancelled time tk Stage 2 allocation policy. Therefore,
final result allocation strictly fewer k units.
setting k = pi,k , payment policy Equation 4 corresponds payment
policy Lemma 1. Therefore proposed mechanism shown DSIC valuations.
complete proof showing truthful reporting arrival departure
times also DSIC given limited misreports, assuming truthful reporting vi .
Lemma 3. Given limited misreports, assuming truthfully reporting vi = vi
dominant strategy given arrival,departure maximum consumption rate reports
hai , di , ri i, (subject limited misreports) dominant strategy report ai = ai ,
di = di , ri = ri .
ha ,d ,r

Proof. Let pi denote vector increasingly ordered marginal clearing values (computed without i), given agent reports = hvi , ai , di , ri i. reporting type , agent
Pi (i ) hai ,di ,ri
allocated (i ) items, total payment is:
. agent i, misj=1 pi,j
reporting results one two cases:
(i ) = (i ): Misreporting agent cannot change values pi ,
ever decrease size pi vector. particular, due limited misreports
ha ,d ,r
ai ai , di di ri ri , thus p contains subset elements


hai ,di ,ri

pi

. vectors definition increasingly ordered, follows
hai ,di ,ri
hai ,di ,ri
pi,j
pi,j
, j (di ai + 1). Since payment consists first ki = ki
elements, increase misreporting.
(i ) 6= (i ): First, show (i ) > (i ) could never occur. Since
threshold values remain same, agent win fewer units per time step (when
reporting lower maximum consumption rate), and/or number time steps
allocations occur decreases (when reporting later arrival and/or earlier deadline), Stage
1 mechanism allocate fewer equal numbers units. Furthermore, since
ha ,d ,r

ha ,d ,r

pi,ji pi,ji , possibility cancelling increase Stage 2. Thus,
always holds (i ) (i ).
Now, consider case (i ) < (i ). First, shown case (i ) = (i )
Pi (i ) hai ,di ,ri Pi (i ) hai ,di ,ri
above, know j=1
pi,j
j=1 pi,k
(i.e., payment units
increase misreporting arrival and/or departure). Furthermore, know
allocation (i ) preferable allocation (i ) < (i ), otherwise
reporting true valuation vector vi would dominant strategy. Since payment
items potentially even higher misreporting, agent cannot benefit
winning fewer items.
194

fiAn Online Mechanism Multi-Unit Demand

Theorem 2. Given non-increasing marginal valuations limited misreports, ondeparture cancellation immediate cancellation policies payment policy according
Equation 4 DSIC.
Proof. proof theorem follows directly lemmas. Lemmas 1
2 show that, triple arrival/departure/consumption rate (mis)-reports, hai di , ri i,
allocation policy truthful terms valuation vector vi , given appropriate
payment policy. Furthermore, payments Equation 4 correspond Lemma 2,
therefore truthfully implement mechanism. Finally, Lemma 3 completes
reasoning, showing that, truthful report valuation vector vi , agents cannot benefit
misreporting arrivals/departures.
4.5 Implications PHEV Domain
Since assume units perishable, may always possible cancel units
allocated. Whereas problem immediate cancellation, since
units never allocated begin with, on-departure cancellation policy requires
battery partially discharged departure vehicle. Although may
undesirable, agents best interest avoid paying units given design
mechanism, since marginal value units less marginal payment.
agent could avoid discharging unplugging units discharged,
agent end paying relatively expensive units. noted, Section 7,
addition comparing IM OD mechanisms, also evaluate extent
mechanisms would manipulable designed simple greedy allocation
policy, without assuming cancellation.

5. Theoretical Bounds Allocative Efficiency
important question given online nature allocation allocative efficiency compares optimal offline allocation, assuming full knowledge
future. discussed Sections 3 4.2, online setting, possible achieve
optimal allocation, agents arrive leave market continuously. Moreover,
multi-dimensional online setting, allocation units needs cancelled order
maintain truthfulness. Nevertheless, optimal offline allocation represents useful
upper bound could achieved terms allocative efficiency, preferences
availability constraints agents known advance.
end, following, study theoretical worst-case performance
IM OD. precisely, policies, consider two types inefficiencies:
Worst-case cancellation ratio. fraction units allocated
single agent need cancelled worst case (and, maintain incentive
properties, cannot allocated agent). Formally, let set
agents present market time point time interval
bound computed. Denote ipre (I ) total number units pre-allocated
agent Stage 1 policy entire active period agent,
icanc (I ) = ipre (I ) (I ) number units cancelled Stage 2.
195

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Given this, cancellation ratio specific agent RC,i (I ) =

icanc (I )
.
ipre (I )



define worst-case cancellation ratio agents types |I|
as:
canc (I )
max
.
RC
= max max ipre
|I| iI (I )
Competitive ratio allocative efficiency. Whereas cancellation ratio considers
worst-case individual agent, competitive ratio compares social welfare
mechanism social welfare achieved optimal offline mechanism,
full information future arrivals. Here, social welfare defined sum
valuations obtained agents (i.e., sum utilities excluding payments).
detail, following work Parkes (2007), competitive ratio setting
defined follows. Let ion (I ) denote number units allocated online
mechanism departure agent given types agents, |I| , iof f (I )
denote number units allocated optimal offline mechanism agent i.
P Pion
social welfare allocations defined as: V (I ) = iI k=1
vi,k
f
P
P

online case, respectively V f (I ) =
iI
k=1 vi,k offline case. Now,
competitive analysis assumes existence adversary choose
set inputs, case adversary choose set agent types
|I| . Given this, online mechanism said onc-competitive efficiency,
exists constant c 1 that: |I| : VVof f((I )) 1c . also say


online mechanism guaranteed achieve within fraction
optimal offline algorithm.

1
c

value

motivation studying two metrics follows. First, outlined Section
4.2, variants mechanism propose require part allocation
agents sometimes cancelled, order ensure truthfulness. natural ask
worst-case fraction number units allocated agent need
cancelled, types mechanisms (i.e., immediate on-departure cancellation), market set-up. second criteria (i.e., competitive ratio allocative
efficiency), follow metric proposed Hajiaghayi et al. (2005) Parkes (2007)
online domains, caveat deriving bound multi-dimensional case
considerably involved single-dimensional one, due required cancellations. Section 5.1 study issues mechanism immediate cancellation,
Section 5.2 mechanism on-departure cancellation.
5.1 Worst-Case Bounds Mechanism Immediate Cancellation
following theorem shows that, using online mechanism immediate cancellation, worst-case cancellation ratio goes 1 number units required
single agent goes infinity.
max = 1, n maximum
Theorem 3. Using IM allocation policy, limn RC
demand.

Proof. proof example. Consider following setting consisting agent
marginal valuation vector vA = hv1 , v2 , ...vn i, values strictly decreasing,
196

fiAn Online Mechanism Multi-Unit Demand

i.e., v1 > v2 > .... > vn . assume agent arrives time aA = 1, departs dA ,
dA = n (n + 1)/2, maximum charging speed rA = 1. Agent faces
sequence cursory (i.e. local) agents, agents desires exactly one unit,
present market one timestep departs immediately afterwards.
one time exactly one cursory agents market. valuations
agents follows. first agent valuation v1 = hv1 i, next two agents,
= 2 = 3, valuations vi = hv2 i, next three agents, {4, 5, 6},
valuations vi = hv3 i, next four agents, {7, 8, 9, 10}, vi = hv4 i, etc. Thus,
total, sequence n (n + 1)/2 agents. Here, sufficiently small
v1 > v2 . result, agent imposes following externality timestep:
hv1 , v2 , v2 , v3 , v3 , v3 , v4 , v4 , v4 , v4 , ...i (noting that, since
cursory agents present market single time step, cancelling allocation
affect externality, marginal payment).
allocation settings proceeds follows. first time step, unit
pre-allocated agent (since v1 > v1 ) cancellation. second
time step, unit pre-allocated agent (since v2 > v2 ), point
h2i
marginal payment pi = hv2 , v1 i. Since marginal value second unit
less marginal payment unit, i.e., v2 < v1 , unit gets cancelled.
Therefore, time = 3, marginal value agent still v2 , third unit also
gets allocated agent, time cancelled. However, next two
time-steps, units pre-allocated cancelled times. see this, note
h2i
marginal payment time = 5 pi = hv3 , v3 , v2 , v2 , v1 i. Since
h2i
v3 < pi,3 = v2 , unit gets cancelled.
generally, every k th unit allocated cancelled, marginal
value agent becomes vk+1 , next k units first pre-allocated (since
marginal value cursory agents vk+1 ), subsequently cancelled (since
marginal payment vk ). (k + 1)th unit allocated
cancelled, next k + 1 units cancelled, on.
pre
= 1 + 2 + 3 + 4 + . . . + n = n (n + 1)/2 units pre-allocated.
result,
canc = 0 + 1 + 2 + 3 + . . . + (n 1) = (n 1) n/2 cancelled,
units
= n remain allocated. Therefore, ratio number units cancelled n
is:
canc

n1
n2 n
= lim
=1
pre = lim
n
n n + 1
n n2 + n


RC,A = lim

theorem shows worst-case result individual agent unbounded. use result (and example constructed proof) derive
similarly negative result allocative efficiency (i.e., overall efficiency system):
Theorem 4. mechanism immediate cancellation, competitive ratio
allocation efficiency unbounded. is, exists finite c, that:
|I| :

1
V (I )


f
c
V
(I )
197

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Proof. Generally, two potential sources inefficiency w.r.t. offline allocation:
either units pre-allocated subsequently cancelled, units allocated
agents less utility agents would allocated offline
case. proof based former source inefficiency uses example
given Theorem 3.
example Theorem 3 showed possible construct example
where, given n (n + 1)/2 units supply, (n 1) n cancelled (and thus
allocated agent) n allocated. Now, suppose valuations
units agents (including agent A) [v, v], v/v = r finite constant.
Since using optimal offline allocation units allocated, total value
least: V f v n (n + 1)/2. hand, online allocation using online
allocation using immediate cancellation value most: V v n.
Given this, following holds:
2r
vn
V
lim
= lim
=0
n v n (n + 1)/2
n n + 1
n V f
lim

Therefore, constant c, always possible find counter example
worst-case efficiency lower 1/c.
Thus, theoretical bound efficiency loss using immediate
cancellation allocation policy. However, proof relies agent
infinitely patient, infinite demand, higher valuation bidders
unit. practice, extreme situation would never occur. consider
practical scenarios, therefore, Section 7 use simulations investigate realistic settings. showing worst-case bounds immediate cancellation, remainder
Section 5 derive theoretical bounds on-departure cancellation mechanism.
Specifically, show mechanism provides much better bounds. fact,
competitive bounds efficiency single-unit demand settings,
cancellation occurs.
5.2 Worst-Case Bounds Mechanism On-Departure Cancellation
section divided two parts: Section 5.2.1 discuss worst-case cancellation
ratio particular agent provide tight bound, Section 5.2.2 consider
bound allocative efficiency entire market.
5.2.1 Worst-Case Cancellation Ratio
section organised follows. First, show half units
cancelled particular agent. go show exist examples
half cancelled. Note that, convenience, following lemma formulated
terms units retained instead units cancelled.
Lemma 4. Using on-departure cancellation, suppose agent pre-allocated n units
departure time di , k units kept Stage 2 (and mechanism
cancels n k units). Then, type profile |I| , agent I, k n/2
(i.e., least half units allocated).
198

fiAn Online Mechanism Multi-Unit Demand

Proof. prove property, start deriving two inequalities hold value
k. First, since k defined number units kept, remaining ones (n k)
cancelled, must hold vi,k+1 < pi,k+1 (otherwise (k + 1)th unit would
cancelled, contradicting definition).
second inequality given vi,k+1 pi,nk less obvious. see
always holds, need observation greedy allocation works. Recall n
number units pre-allocated greedy allocation policy. Therefore,
active marginal values, vi,1 , . . . , vi,n , point [ai , di ] among top S(t)
highest marginal values. Consequently, lowest marginal value ones pre-allocated,
vi,n , must greater marginal payment least one unit (otherwise could
unit). Since marginal payments sorted increasing order,
must therefore hold vi,n pi,1 . Similarly, next-lowest value, must hold
vi,n1 pi,2 , on. general write vi,nj+1 pi,j , j {1, n}. set
j = n k, get vi,k+1 pi,nk .
Therefore, order greedy policy allocate n units mechanism
subsequently cancel units positions k + 1 n (assuming k + 1 < n, otherwise
cancelling take place departure agent i), following inequalities must
satisfied:
(
vi,k+1 < pi,k+1
(5)
vi,k+1 pi,nk
Given this, show k n/2 contradiction. Suppose k = n/2 1,
i.e., strictly n/2 cancelled. conditions become:
(
vi,n/2 < pi,n/2
(6)
vi,n/2 pi,nn/2+1
show contradiction, need consider separately cases n even
n odd. n even, n = n/2 + n/2, system
becomes:
(
vi,n/2 < pi,n/2
(7)
vi,n/2 pi,n/2+1
implies pi,n/2+1 < pi,n/2 , since marginal price vector pi weakly increasing
definition, leads contradiction. case n odd,
n = n/2 + n/2 1, conditions become:
(
vi,n/2 < pi,n/2
(8)
vi,n/2 pi,n/2
Clearly, equations cannot satisfied simultaneously, leading contradiction.
Note value k < n/2 would lead contradiction due pi
increasing, hence necessarily k n/2, completing proof.
complete analysis, show bound tight, i.e., exist settings
half units allocated agent cancelled.
199

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Lemma 5. exist settings mechanism on-departure cancellation
cancels allocation n/2 units, n/2 units kept, departure
agent i, n number pre-allocated units.
Proof. proof done constructing worst-case example. Consider
single agent, A, market n time periods, demand n units,
n even. first n/2 marginal valuations equal 4, remaining ones 2.
example, n = 8, marginal valuation vector becomes vA = h4, 4, 4, 4, 2, 2, 2, 2i. Similar
proof Theorem 3, agent faced, time step, different, single cursory
agent, participates time step. valuations first n/2 cursory
agents sequence given vi = h3i, second half agents vi = h1i.
Thus, n = 8, marginal payment agent would pA = h1, 1, 1, 1, 3, 3, 3, 3i.
setting, mechanism on-departure cancellation would pre-allocate units
agent (since vA,k = 4 > vk,1 = 3 k n/2 vA,k = 2 > vk,1 = 1 k > n/2).
However, departure agent A, exactly half units allocated cancelled (since
vA,k = 2 < pA,k = 3 k > n/2).
Finally, unify results Lemmas 4 5 following theorem.
Theorem 5. setting on-departure cancellation non-increasing marginal values, number units agents present, worst case cancellation-ratio
max = 1 .
number units allocated agent RC
2
Proof. Lemma 4 shows that, regardless set-up, half units allocated
max 1 , regardless setting
agent cancelled departure, thus RC
2
(i.e., possible input types agents). Lemma 5 shows exist settings
max = 1 , completing proof.
cancellation ratio exactly RC
2
Note practice smaller settings, significantly fewer half units
cancelled. worst case cancellation ratio 1/2 allocations occurs specifically constructed example, and, shown experimental analysis, realistic
distributions application domain, actual performance much better.
5.2.2 Competitive Bound Allocative Efficiency
previous section discusses cancellation problem perspective single
agents, whole market. section, show that, case agents
weakly decreasing marginal values, allocation returned on-departure cancellation
mechanism 2-competitive optimal offline allocation. result means
multi-unit demand case on-departure cancellation worse terms worst-case
competitive bound single-unit demand problem discussed Hajiaghayi et al.
(2005), Parkes (2007), despite fact single unit demand need
cancellation ensure incentive compatibility. Formally, state
following theorem:
Theorem 6. mechanism on-departure cancellation 2-competitive optimal offline allocation, setting non-increasing marginal values.
200

fiAn Online Mechanism Multi-Unit Demand

Proof. order establish competitive bound optimal offline allocation, use
charging argument similar Hajiaghayi et al. (2005).5 basic idea
charge (or match) marginal value units agent allocated
offline case another, higher-valued unit allocated offline online.
either unit itself, higher value unit causes allocated
online market. Formally, consider units vi,p (belonging agent position p)
allocated offline online case. unit vi,p charged
twice, itself, lower valued unit allocated offline online,
follows worst-case social welfare ratio online vs. offline
allocation cannot drop 1:2.
Now, agents single-unit demand (such case discussed Hajiaghayi et al.,
2005), easy see property always holds, unit vi,p allocated
online once, thus displace one unit vj,q . Crucially, units
cancelled. multi-unit demand setting, argument becomes involved,
unit vi,p (allocated online offline) affect online market several ways:
displace another unit vj,q would allocated offline, displace
mean specifically unit vj,q never pre-allocated online (hence cancellation
apply it).
cause cancellation another unit vj,q . second case, unit vj,q
pre-allocated, allocation cancelled due presence unit vi,p
market (meaning pre-allocation would cancelled departure
agent j, unit vi,p present).
main issue remains shown unit vi,p displace cause
cancellation one unit would allocated offline. Thus, cannot
displace two units allocated offline, allocated online, due
presence unit vi,p .
show contradiction. Formally, suppose three units: vi,p , vj,q
vk,r allocated offline case (with vj,q < vi,p vk,r < vi,p ). Unit vi,p allocated
online case (i.e., pre-allocated cancelled). Units vj,q vk,r allocated
online case unit vi,p present, allocated online unit vi,p present.
Given set-up, three possible cases:
1. Neither units vj,q vk,r pre-allocated online unit vi,p present (hence,
cancellation either vj,q vk,r ).
2. Unit vj,q never pre-allocated online, unit vk,r pre-allocated allocation
cancelled later (i.e., departure agent k market), unit vi,p present.
3. units vj,q vk,r pre-allocated, pre-allocations cancelled
departure market agents j, respectively k, unit vi,p present.
5. term charging refer electricity charging, represents name proof device
used online mechanism design.

201

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

cases, unit vi,p present, units vj,q vk,r pre-allocated cancelled online case. order complete proof need show, contradiction,
three cases could occur.
Case 1 similar case single unit demand discussed Hajiaghayi et al.
(2005), cancellation occurs units. relatively straightforward see
cannot occur, unit vi,p pre-allocated (at time t), thus
displace one unit would allocated otherwise.
either unit allocated online time time t, unit allocated online later on,
unit is, turn, displaced it.
Case 2: Suppose vj,q (belonging agent j active [aj , dj ])
unit assumed pre-allocated vi,p present, unit vk,r unit
allocated cancelled. two subcases consider here, require
separate discussion.
Case 2A: First, consider vj,q > vk,r . case, agent k lower marginal
value agent j, value vk,r still pre-allocated essentially greedy
allocation policy, vj,q not. means agent k must patient
agent j, hence dj < dk , otherwise vj,q would pre-allocated instead.
Now, denote pk payment vector agent k, defined Section 4.2.
unit vk,r cancelled must hold vk,r < pk,r . Now, denote pk<i> vector
marginal payments agent k agent present market, recall
assumption value vj,q allocated. Thus, have:
pk<i> = incr(pk \{vi,p } {vj,q })
incr operator orders elements increasing order. Since vj,q > vk,r ,
<i>
follows vk,r
< pk,r , thus allocation unit vk,r would still cancelled, even
without unit vi,p .
Case 2B: second subcase, consider vj,q vk,r , i.e., value unit
displaced agent lower one pre-allocated cancelled. First
note that, case occur, unit vj,q allocated online within [ak , dk ], active
window agent k. obvious condition: agent j allocated online outside
window (and displaced agent market, displacement occurs
outside [ak , dk ]), units vj,q vi,p cannot influence cancellation unit vk,r (because
unit pre-allocated once, case pre-allocation vi,p would
happen outside [ak , dk ]).
previously, recall condition unit vk,r < pk,r , required unit vk,r
cancelled. Note means least k units [ak , dk ] higher
value unit vk,r , thus, offline allocation (which benchmark) would need
take priority it. setting, one units vi,p . even removing
vi,p it, vector pk<i> = incr(pk \{vi,p } {vj,q }) must contain least k 1 values higher
vk,r . offline allocation without unit vi,p , k 1 values must given priority,
together least unit vk,r . However, means unit vj,q cannot allocated
offline [ak , dk ], lower value vk,r . gives contradiction
initial assumption units vi,j vk,r allocated offline (as well online)
without unit vi,p present.
202

fiAn Online Mechanism Multi-Unit Demand

explain intuitively, means unit vi,p cause displacement (non-allocation) unit vj,q cancellation another one vk,r ,
possible units high enough value allocated offline
case well. Thus, one offline-allocated unit allocated online
presence unit vi,p market.
Case 3: final case, units would need pre-allocated cancelled,
absence value vi,p . contradiction case shown similarly Case 2A
above. Considering marginal price vectors agents j k without vi,p
market:
pj<i> = incr(pj \{vi,p } {vk,r })
pk<i> = incr(pk \{vi,p } {vj,q })
easy see that, regardless whether value vj,q vk,r lower, value
cancellation would still occur departure market without agent i, leading
contradiction.
summarise, exhaustively shown contradiction holds
possible cases. Thus, unit vi,p allocated online offline displace
(or lead cancellation of) one unit allocated offline. Thus, two units
allocated offline charged unit allocated online, completing proof.

6. Computational Aspects
section, consider implications implementing mechanisms practice, including computational complexity algorithms. examine on-departure
immediate cancellation separately, differ fundamentally complexity.
6.1 Implementing On-Departure Cancellation
Algorithm 1 briefly outlines implementation mechanism on-departure cancellation (OD). Here, assume first time step denoted t0 , second
t1 = t0 + 1, on. simplicity, use throughout section denote
full set agents arriving time points, note algorithm explicitly
uses information future arrivals. Initially, algorithm sets endowments
agents 0 (line 2), units allocated. Then, every time step t,
algorithm first pre-allocates units using greedy allocation policy (line 4).
done O(N rmax ) using well known linear-time selection algorithm described Blum,
Floyd, Pratt, Rivest, Tarjan (1973), N = |I| total number agents
rmax = maxiI ri maximum consumption rate.
Next, algorithm computes marginal payments time step rerunning
market without active agent (line 6). rerunning market particular
hti
agent i, important note pre-allocations effect pi
cancellations irrelevant, affect future development market.
Therefore, necessary compute greedy allocation agent removed,
total run-time, active agents, O(N 2 rmax ) (assuming results
hti
previous time steps re-used). Updating pi new marginal payments
203

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Algorithm 1 Mechanism On-Departure Cancellation (OD).
1: procedure OnDepartureMechanism(I , S)
ht
2:
kht0 h0, 0, . . . , 0i
Initial endowments, ki 0 = 0,
3:
{t0 , t1 , . . .}
4:
kht+1i GreedyAllocation(I , S(t), khti )
Run greedy allocation

5:
{j I|aj dj t}
Iterate active agents
hti
6:
update pi using
Run market without
7:
di =
agent departing
ht+1i hti
8:
k , pi k
, pi
Final pre-allocation marginal payments
9:
vi,ki < pi,ki
10:
ki ki 1
Cancel units necessary
11:
end P

pi,k
Final payment agent
12:
xi (i |ki ) kk=1
13:
end
14:
end
15:
end
16: end procedure

done using simple insertion algorithm, and, noting lowest |vi | payments
agent need kept, done, agents, O(N rmax vmax ),
vmax maximum length agents valuation vector.
Finally, departure, lines 811, units lower valuation
corresponding marginal payments cancelled, final payment calculated
line 12. done O(N vmax ) simply iterating values.
summary, time complexity algorithm OD mechanism O(N 2 rmax +
N rmax vmax + N vmax ) time step. rmax vmax assumed constant6 ,
simplifies O(N 2 ). Generally, means algorithm executed quickly,
even large numbers agents.
6.2 Implementing Immediate Cancellation
Next, consider mechanism immediate cancellation (IM), shown Algorithm 2. key difference OD mechanism units potentially cancelled
every time step agent active (lines 79), rather departure.
small modification significant impact computational tractability mechanism. Unlike previous mechanism, computing marginal payments line 6,
cancellations agents affect payments. feature already highlighted
example Section 4.3.1, cancellation second unit pre-allocated
agent 1 causes change marginal payments agent 3. generally, cancellations
immediate effect endowments agents directly affects active
marginal valuations V hti subsequent time steps.
6. reasonable, limited technological constraints practice. particular, rmax
limited battery infrastructure constraints, vmax related petrol savings achievable
EV (as detailed Section 7.3.3).

204

fiAn Online Mechanism Multi-Unit Demand

Algorithm 2 Mechanism Immediate Cancellation (IM).
1: procedure ImmediateMechanism(I , S)
ht
2:
kht0 h0, 0, . . . , 0i
Initial endowments, ki 0 = 0,
3:
{t0 , t1 , . . .}
4:
kht+1i GreedyAllocation(I , S(t), khti )
Run greedy allocation

5:
{j I|aj dj t}
Iterate active agents
hti
6:
update pi using
Run market without
hti
7:
vi,kht+1i < p ht+1i


8:
9:
10:
11:
12:
13:
14:
15:
16:

ht+1i
ki

i,ki
ht+1i
ki
1


end
di =
ht+1i hti
k , pi k
,p
P ii
xi (i |ki ) kk=1
pi,k
end
end
end
end procedure

Cancel units immediately
agent departing
Final allocation marginal payments
Final payment agent

Now, order determine cancellations rerunning market without active
agent i, necessary compute marginal payments agents
markets (effectively executing full algorithm ). Clearly, leads
recursion potentially sees possible subsets agents evaluated. worst case,
therefore, cancellation decisions need executed every agent every possible
subset I, N 2(N 1) times.7 Simplifying assuming vmax rmax
constant, leads runtime complexity O(N 2N ).
runtime exponential number agents clearly problem applying
mechanism realistic settings handful agents. However, tackle
problems, possible use technique akin branch-and-bound enters
recursion line 6 necessary. present following section.
6.3 Speeding Immediate Cancellation Using Bounds
obtain faster algorithm IM mechanism, instead calculating marginal
hti
payments pi every time step, find iteratively refine lower upper bounds
payments. intuition behind approach choose initial bounds
easily calculated without resorting recursion. agents reported valuation
pre-allocated unit, vi,k , lies outside bounds, immediately determine whether
unit cancelled not. hand, vi,k lies bounds, refine
7. practice, recursion occurs set agents active time
agent evaluated, previous decisions affected agent presence similarly
agents arriving di affect i. means settings large numbers agents may
still tractable little overlap active agents, sake analysis
section, assume worst case, N agents active concurrently.

205

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

iteratively calculating actual marginal payments agents active
time steps cancellation decision unambiguous.
ht,si
ht,si
detail, use pi
pi
denote lower upper bounds, respechti
tively, pi . Here, indicates level refinement, time step actual
marginal payments calculated, ai 1 t. Analogous actual
hti
ht,si
ht,si
pi vector, pi
pi
vectors increasing order, represent bounds
hti
ht,si
hti
ht,si
pi , pi,k pi,k pi,k , k s. level refinement, s,
hti

increased, bounds become tighter, eventually converging pi,k . following,
describe detail calculate initial bounds (with = ai 1, indicating
actual payments calculated yet):
ht,a 1i

order calculate initial lower bounds, pi , rerun market without
ai using greedy allocation policy without cancellations.
marginal payments market (as described Section 4.2) used lower
hti
bounds. payments necessarily actual payments pi ,
may cancellations latter, cause active marginal valuations
change subsequent time steps. However, seen easily indeed
represent lower bound. Specifically, cancellations either influence
cause increase one elements externality vector E hti (since agents
lower endowment cancellations therefore equal higher value
obtaining additional units).
ht,a 1i

calculate initial upper bounds, pi , consider actual market including agent time step ai t, calculate externality agent
would impose others winning available unit throughout time interval use derive upper bounds payments. formally,
hti
define new multiset set valuations, Vi0 , derived simply removing
elements corresponding agent V hti padding zeroes, necessary, size least S(t). Then, proceed similar manner
Section 4.2 defining externality agent would impose others time step
hti
hti
Ei = minhri (maxhS(t)i Vi0 ). Given this, final vector upper bounds

ht,a 1i
ht
pi
= incr( tt =ai Ei ).

Unlike actual marginal payments, upper bounds include effect agent
market, may allocated units would allocated
others, or, presence, cause cancellation units, which, turn,
affect active valuations agents subsequent time steps. However,
effect reduce supply available agents, including agent
ht,a 1i
market increase active valuations agents, therefore pi
hti
upper bound pi .

Given bounds, quickly test agents marginal valuation vi,k
ht,a 1i
pre-allocated unit falls outside bounds. vi,k < pi,k , unit canht,ai 1i

celled immediately, vi,k pi,k

, definitely cancelled. However,

206

fiAn Online Mechanism Multi-Unit Demand

Algorithm 3 Mechanism Immediate Cancellation (IM) Bounds.
1: procedure BoundedImmediateMechanism(I , S)
ht
2:
kht0 h0, 0, . . . , 0i
Initial endowments, ki 0 = 0,
3:

4:
si ai 1
Initial refinement bounds
5:
end
6:
{t0 , t1 , . . .}
7:
kht+1i GreedyAllocation(I , S(t), khti )
Run greedy allocation
8:
{j I|aj dj t}
Iterate active agents
ht,s
ht,s
9:
Calculate pi pi
Add initial bounds time
10:
repeat
ht,si

11:
vi,kht+1i < p ht+1i


ht+1i
ki

12:



else vi,kht+1i

13:



15:
16:
17:



19:
20:
21:
22:
23:
24:
25:
26:
27:

Unit definitely cancelled

i,ki

Refine bounds

i,ki

di =
agent departing
ht+1i
ki ki
Final allocation
ht,si
ht,si
x {1, 2, . . . , ki }, pi,x
6= pi,x

si si + 1
Refine final payments
end P
ht,s

Final payment agent
pi,k
xi (i |ki ) kk=1
end
end
end
end procedure

ht,ai 1i

pi,k

1
ht,si
< p ht+1i


si si + 1
ht,s
ht,s
Update pi pi
end
ht,si
vi,kht+1i p ht+1i

14:

18:

i,ki
ht+1i
ki

ht,ai 1i

vi,k < pi,k

, bounds ambiguous need refined.
ht,si

ht,si

obtain refined bounds pi
pi
computing actual marginal payments specified time s, bounds calculated above.
effectively replaces initial bounds actual marginal payments, resulting
ht,si
hti
ht,si
accurate overall bounds. Eventually, = t, pi
= pi = pi
.

ht,si
ht
formally, refined lower bounds calculated pi
= incr(( st =ai Ei )

ht
hti
( tt =ai +1 Ei )), Ei externality vector market without i, using immehti

diate cancellations (as used actual payments), Ei corresponding vector
market without without cancellations. Similarly, refined upper bounds


ht,si
ht
ht
hti
calculated pi
= incr(( st =ai Ei ) ( tt =ai +1 Ei )), Ei
defined
above.
207

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Full details IM mechanism using bounds given Algorithm 3. keeps
track level refinement bounds agent (as si , initialised
ai 1 line 4). Then, instead updating actual payments, lines 1017 repeatedly
compare marginal valuation last unit pre-allocated (vi,kht+1i )

current upper lower bounds, refining necessary. Note here, previous
calculations bounds reused, algorithm checks iteratively, either
increasing (when next time step calculated) si 1 (when bounds refined).
case, means active valuations one additional time step
added existing bounds. loop repeats statement line 17 becomes
true, captures cases last pre-allocated unit definitely cancelled
(in case pre-allocated units also cancelled, marginal
valuations least high, respective payments equal lower),
pre-allocated units time step cancelled.
Finally, agent departs, final allocation payments calculated
lines 1824. Here, important calculate actual marginal payments
ki allocated units. achieved refining bounds first ki upper
lower bounds equal. incur computational effort,
hti
equivalent computing complete pi vector Algorithm 2. payments
need calculated units actually allocated departure, upper
lower bounds may equal without needing compute actual payments, importantly,
required full set agents recursively subsets agents.
practice, algorithm bounds significantly reduces computational runtime mechanism (typically 99% throughout experiments conducted
Section 7), often avoids re-running market possible subsets agents.
However, important note worst-case run-time still equivalent Algorithm 2, i.e., O(N 2N ), exponential number agents. best case,
recursion necessary, run-time reduces O(N 2 ).

7. Experimental Evaluation
section, quantify performance mechanisms, compared number
benchmarks, applying range settings. investigated theoretical
performance bounds mechanisms Section 5, purpose section evaluate
performance realistic settings.
Specifically, Section 7.2, consider general setting easily reproducible
show mechanisms perform vary supply demand good. Then,
Section 7.3, turn PHEV domain. this, first show derive
agents preferences based vehicle owners driving behaviour. Then, use real data
collected first large-scale trial pure electric vehicles (EVs) UK show
trends continue hold realistic application setting. Furthermore, look
gradual introduction fast-charging PHEVs would affect neighbourhood
limited electricity supply, terms social welfare (which translates overall fuel
savings within neighbourhood) financial savings individuals. Throughout
experiments, also consider simpler greedy allocation mechanism without cancellation
quantify potential benefits agent would able achieve misreporting
208

fiAn Online Mechanism Multi-Unit Demand

mechanism. demonstrates whether actually scope strategic misreporting
realistic settings whether cancellation needed practice.
consider two specific settings, briefly outline common parameters
benchmarks used experiments.
7.1 Experimental Setup
evaluate mechanisms, simulate different settings number agents compete limited supply good allocated hourly basis 24-hour
period. order test scenarios varying supply demand, sample agents
randomly fixed probability distributions use range supply functions (these
outlined detail Sections 7.2 7.3). order ensure statistical significance results, re-sample agents 1,000 times setting, plot 95%
confidence intervals throughout section.
addition two mechanisms proposed paper, immediate cancellation
(IM) on-departure cancellation (OD), evaluate number benchmark mechanisms:
Fixed fixed-price mechanism allocates units agents
valuation least given constant p, price pay unit p.
demand exceeds supply, unit allocated agent chosen uniformly
random set agents sufficiently high valuation. Here, agent
may receive multiple units, maximum consumption rate, ri . mechanism
DSIC constitutes direct comparison mechanisms. However,
optimise performance fixed-price mechanism, p must carefully chosen.
Thus, given setting, test possible values (in steps 0.01) select
p achieves highest average efficiency (over 1,000 trials). Thus, showing
results Fixed, constitutes upper bound could achieved
mechanism.
Random special case Fixed, p = 0. Thus, using baseline benchmark,
units allocated randomly agents pay anything.
Greedy simple greedy allocation policy, described Section 4. Payments
calculated using pi prices (as IM OD), cancellations.
Thus, mechanism truthful, constitutes interesting comparison
mechanisms, allows us quantify loss efficiency caused
cancellations, well potential benefits agents misreporting
absence cancellations.
Heuristic allocates units weighted combination agents valuation
urgency (proximity departure time) maximised. Here, [0, 1]
parameter denotes importance urgency, = 1 corresponds
well-known earliest-deadline-first heuristic scheduling (Pinedo, 2008), = 0
indicates units always allocated agent highest valuation.
truthful mechanism impose payments here, primary
purpose benchmark approach. Again, always select best
testing values steps 0.01 setting.
209

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Optimal assumes complete knowledge future arrivals supply, allocates
units agents maximise overall allocative efficiency. Clearly, mechanism
possible assumes knowledge future also truthful
(again impose payments), serves upper bound efficiency
could achieved.
7.2 General Allocation Setting
First, consider general synthetic setting, generate agents supply
function simple distributions. main reason examining scenario
turning realistic setup generate results easily reproducible
tied specific application domain. following, outline distributions
sample supply agents (Section 7.2.1), discuss results
(Section 7.2.2).
7.2.1 Synthetic Setup
setting, generate supply function S(t) randomly drawing discrete
uniform distribution {1, 2, 3, . . . , s}, vary experiments represent
different amounts good produced. agent i, sample arrival
time ai discrete uniform distribution {0, 1, 2, . . . , 23} departure time
{ai , ai + 1, . . . , 23}. sample maximum consumption rate ri {1, 2, 3, 4, 5},
finally, generate vi first selecting number required units uniformly random {1, 2, 3, . . . , 20}. Then, first valuation vi,1 sampled exponential
distribution rate 1, remaining valuations drawn uniformly random
continuous interval [0, vi,1 ] (ordered appropriately ensure non-increasing marginal
valuations).
7.2.2 Synthetic Results
Figure 3, examine allocative efficiency mechanisms increase number agents competing limited supply electricity. figure shows allocative
efficiency setting low supply (left), = 1, i.e., one unit available per
time step, setting high supply (right), = 20, i.e., 20 units
available per time step. choose two extreme settings show full spectrum
potential supply scenarios (and focus supply settings based real data Section 7.3).
Note due run-time complexity, plot IM smaller setting one
unit supply. supply high (s = 20), agent typically allocated large
hti
number units, causing IM require frequent refinements bounds pi
vectors thus leading computational bottleneck. non-truthful Heuristic approach consistently achieves around 99% Optimal full information = 1,
plot readability, also use approximation Optimal
= 20 (where Optimal also becomes computationally infeasible).
Several trends emerge results. First, = 1, simple truthful
Greedy approach performs well (around 99% Optimal). Next, note
truthful mechanisms, IM OD also perform well, achieving around 95% 96%
210

fiAn Online Mechanism Multi-Unit Demand

Allocative Efficiency (% Optimal)

Approximate Allocative Efficiency (% Heuristic)

100%

100%

90%

90%

80%

80%
Greedy
OD
IM
Fixed
Random

70%
60%
50%

70%
60%
50%

40%

40%

30%

30%

20%

20%
0

25

50 75 100 125 150 175 200
Number Agents

0

25

50 75 100 125 150 175 200
Number Agents

Figure 3: Allocative efficiency synthetic setting (low supply, = 1, left,
high supply, = 20, right).

Optimal, respectively. slightly lower Greedy, indicates 3
4% loss efficiency incurred due cancellations. difference performance
IM OD expected here, IM cancel units (see Section 5); however, despite
unfavourable worst-case performance IM, surprising difference
significant practice. Overall, results promising, indicating mechanisms
work well settings, specific conditions cause cancellations (i.e.,
valuations allocated units effectively cross marginal payments)
occur frequently practice.
fixed price mechanism, Fixed, performs significantly worse proposed mechanisms, achieving 81% 83% Optimal. because, order remain
truthful, mechanism sets single fixed price respond dynamically
changes supply demand time step time step. also explains
mechanism performs worst some, much, competition, e.g., around
30 agents. Here, fixed price starts rise, ensure agents higher valuations
allocated first, still considerable variance valuations time step,
sometimes leaving high-value agents unallocated, times units allocated
all. contrast this, IM OD always allocate available units agents
highest valuations. Furthermore, noted Fixed, unlike IM OD,
assumes priori knowledge distributions agents drawn. may
always available practice, decrease performance.
Finally, Random mechanism performs worst all, surprising, uses
information agents valuations all. However, poor performance demon211

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

10%

Proportion PreAllocated Units Cancelled
10%
OD
IM

OD

5%

5%

0%

0%
0

50

100

150

200

Number Agents

0

50

100

150

200

Number Agents

Figure 4: Proportion initially allocated units cancelled (s = 1 left, = 20
right).

strates clearly potential perils using poorly designed non-truthful mechanisms,
strategically misreported valuations may relation agents actual valuations.
setting abundant supply, = 20, broad trends
observed. OD mechanism still achieves around 97% near-optimal Heuristic
slightly less Greedy, Fixed Random perform significantly worse. However,
gap performance smaller time, less competition often
sufficient units satisfy agents. settings, benefit always
allocating agents highest valuations generally lower. Note IM
infeasible settings due high run-time complexity, OD took, average,
less 100ms execute 24 time steps even complex settings = 20
200 agents.
Next, illustrate actual cancellations IM OD mechanisms far
worst-case bounds established Section 5, Figure 4 shows average proportion
pre-allocated units cancelled. generally low, ranging 0% 7%. expected,
OD mechanism cancels fewer units IM mechanism. Furthermore,
general trend cancel units competition low (as usually sufficient
hti
units satisfy agents, leading mostly 0 valuations pi vectors). Cancellations
peak medium levels competition, start drop slightly.
hti
peak explained large variations pi vectors settings
also agents generally allocated units settings
competition (leading similarly higher variation valuations agents allocated
units).
Figure 5, next consider potential benefits misreporting cancellations
used (as Greedy mechanism). measure computing utility
agent on-departure cancellations used compare actual utility gained
Greedy mechanism. constitutes best deviation single agent could
achieved perfect hindsight actual pi prices. plot proportion
cases agent achieve gain misreporting (light blue), conditional
212

fiAn Online Mechanism Multi-Unit Demand

Potential Gain Utility Misreporting
25%

25%

20%

20%

15%

15%

10%

10%

5%

5%

0%

Proportion Gains
Conditional Utility Increase
Overall Utility Increase

0%
0

25

50

75 100 125 150 175 200
Number Agents

0

25

50

75 100 125 150 175 200
Number Agents

Figure 5: Potential gains misreporting Greedy mechanism (s = 1 left,
= 20 right).

proportional increase utility gain misreporting exists (dark red),
product two, i.e., overall average proportional increase utility, including cases
gain (light red).
results indicate without cancellations, often cases agent
benefit misreporting 67% cases = 1 20% cases
= 20. provides clear motivation using incentive compatible mechanisms
settings. However, although individual gains lead average increase
utility 1520%, considering overall average utility increase (including
cases agents benefit), 12% significantly less many
specific settings. offers promise settings cancellations infeasible,
example immediate cancellations computationally feasible large
settings, on-departure cancellations cannot practically implemented.
low expected gains 12% less, agent may wish exert additional efforts
strategise. Furthermore, gains represents upper bound achieved
perfect foresight prices, likely unavailable practice. also
note expected gain misreporting fluctuates significantly, depending
specific setting similar cancellations, fluctuation caused variations
pi vectors also valuations allocated units.
conclude section, Figure 6 explores performance gap
proposed mechanisms benchmarks supply increased. Here, fix number
agents 50 vary maximum number available units good per
time step, s, 1 70. Due complexity setting, omit Optimal
IM analysis. clear relative benefit OD mechanism
truthful benchmarks decreases supply increased. surprising,
eventually agents completely satisfied, even random allocation policy.
213

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Approximate Allocative Efficiency (% Heuristic)
100%
90%
80%
Greedy
OD
Fixed
Random

70%
60%
50%
40%
30%
0

10

20

30
40
Supply (Maximum Units)

50

60

70

Figure 6: Approximate allocative efficiency maximum supply per time step (s)
increased.

However, still significant benefit using OD relatively high supply levels
around = 45, never outperformed truthful benchmarks, consistently
performing close near-optimal Heuristic (over 99% cases) close
Greedy mechanism without cancellations. terms potential gains deviations, similar
trends discussed previously observed omit detailed figure.
far, concentrated describing general performance mechanisms
synthetic, easily reproducible setting. following, apply data real
EV setting.
7.3 PHEV Setting
section, use data real (pure) EV trial simulate typical charging patterns.8 allows us verify trends discussed previous section
continue hold realistic setting. Furthermore, basing experiments actual
PHEV characteristics enables us quantify actual utility drivers real terms (i.e.,
monetary gain fuel saved). also investigate whether introduction
faster charging speeds lead benefits settings consider.
investigation interesting, fast chargers already available domestic settings
allow vehicles charge twice normal rate (or faster).9 However, impact
settings unclear, still constrained overall supply electricity.
following, first present principled approach deriving agents marginal
valuation vector show approach satisfies non-increasing marginal valuation
assumption (Section 7.3.1). describe real-world data use
8. Note that, whereas simulation based PHEVs, uses real-world experimental data pure
EVs. However, believe reasonable assume charging behaviour would similar.
9. See, example, http://www.pod-point.com/ http://www.charging-solutions.com/.

214

fiAn Online Mechanism Multi-Unit Demand

experiments (Section 7.3.2), followed outline used sample PHEVs
(Section 7.3.3). Finally, discuss results (Section 7.3.4).
7.3.1 Deriving Agents Marginal Valuation Vector
important part overall model method computing marginal valuation
vector, vi , based real data. so, combine data sampled cars actual
journey distances principled approach calculating expected economic benefit
charging PHEVs. detail, first derive probability density function, p(m),
data, describes probability distance travelled miles (described
Sections 7.3.2 7.3.3). Given distribution, price fuel (in /litre), pp ,
internal combustion engine efficiency (in miles/litre), ep , efficiency electric
engine (in miles/kWh), ee , calculate expected utility certain amount
charge (in kWh), ce , follows:
Z
Z
pp
pp
p(m)dm
(m ce ee ) p(m)dm,
(9)
E(u(ce )) =
e
p
ce ee ep
0
first term expected fuel cost without charge, second term
expected cost battery charge ce . Therefore, utility function represents
expected savings terms real money given battery charge (without taking cost
charge account). Given this, unit size (in kWh), se , straight-forward
calculate marginal valuation k th unit follows:
vk = E(u(k se )) E(u((k 1) se ))

(10)

Recall that, model, assume valuations marginally non-increasing.
show that, using approach, assumption automatically satisfied.
so, need show Equation 9 non-decreasing (i.e., first derivative positive)
concave (i.e., second derivative negative). first derivative given by:
Z
pp
dE(u(ce ))
=
ee
p(m)
(11)
dce
ep
ce ee
second derivative given by:
pp
d2 E(u(ce ))
= ee 2 p(ce ee )
dce 2
ep

(12)

Clearly, conditions satisfied, means valuations always positive
marginally non-increasing. follows, describe derive experimental
settings, supply function, arrival departure agents, travel
distance probability distributions real-world dataset. also provide examples
marginal valuation vectors using data.
7.3.2 CABLED Dataset
base experiments data gathered CABLED (Coventry Birmingham
Low Emissions Demonstration) project,10 first large-scale endeavour
10. See http://cabled.org.uk/.

215

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Cars

Frequency

15 %

Arrivals
Departures

10 %

5%

0%

:
22

:
20

:
18

:
16

:
14

:
12

:
10

:
08

:
06

:
04

:
02

:
00

00

00

00

00

00

00

00

00

00

00

00

00

Time

Figure 7: Distributions arrival departure times 56 EVs CABLED dataset
(assigning equal weight EV).

UK record study driving charging behaviours EV owners. part
project, 110 EVs loaned public equipped GPS data loggers
record comprehensive usage information, trip durations distances, home
charging patterns energy consumption.
data, focus period March June 2011,
provided information 63 distinct vehicles, total 13,273 journeys.
journey, includes times ignition turned again, total
mileage (as derived GPS readings taken every 60 seconds), well labels
starting end location, available (such home work).
vehicles CABLED trial charged various locations mostly home,
also work. purpose experiments, assume charging
vehicles takes place single location. work focuses coordinating charging EVs within specific neighbourhood considering effect
multiple markets electricity beyond scope work. available,
choose charging location one labeled home data.11 Given this,
since interested arrival departure times charging location,
well consumption patterns visits charging location, aggregate
intermediate journeys departure charging location next return
location single journey.
Aggregating data way discarding vehicles without clear charging location results 4,302 distinct journeys 56 different EVs, covering total distance travelled
close 72,500 miles. overall distribution recorded arrival departure times
11. vehicles dataset lack this, used shared fleet vehicles organisation
cases, use appropriate alternative location label, charging took place,
discard vehicle suitable label identified.

216

fiAn Online Mechanism Multi-Unit Demand

Frequency

EV 1

EV 2

Arrivals
Departures

30 %

30 %

20 %

20 %

10 %

10 %

0%

0%

Frequency

EV 4

30 %

30 %

20 %

20 %

10 %

10 %

0%

0%

0

:0

20
0

:0

16
0

:0

12
0

:0

08
0

:0

04
0

:0

00

0

:0

20
0

:0

16
0

:0

12
0

:0

08
0

:0

04
0

:0

00

EV 5
Frequency

0

:0

20
0

:0

16
0

:0

12
0

:0

08
0

:0

04
0

:0

00

0

:0

20
0

:0

16
0

:0

12
0

:0

08
0

:0

04
0

:0

00

EV 3

EV 6
60 %

40 %

40 %
20 %

20 %

0%

0%

Time

Figure 8: Example distributions arrival departure times six EVs.

217

0

0

:0

20

0

:0

16

0

:0

12

0

:0

08

0

:0

04

:0

00

0

0

:0

20

0

:0

16

0

:0

12

0

:0

08

0

:0

04

:0

00

Time

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Empirical Distribution Function (CDF)

1
0.9
0.8
0.7
0.6

Cars
Car 1
Car 2
Car 3
Car 4
Car 5
Car 6

0.5
0.4
0.3
0.2
0.1
0
0.01

0.1

1

10

100

1000

Miles per Journey

Figure 9: Empirical distribution functions distances travelled successive visits
home charging locations. Six example EVs combined summary
56 EVs, equal weights assigned cars (in red), shown.

charging location shown Figure 7.12 Here, almost arrivals happen
9am 11pm, clear peak late afternoon evening (4pm 9pm). Departures take place throughout day peak around 8am 10am. show
individual driving patterns vary recorded cars, Figure 8 details arrival
departure time distributions six individual EVs data set. reflect
general trends shown previous figure, arrivals generally occurring
evening departures morning. However, EVs 2 3 deviate
pattern. vehicles shared fleet vehicles, collected
returned main charging location throughout day.
Figure 9 shows distribution distances vehicles travelled visits
home charging location (red line), well corresponding functions six
EVs Figure 8 (interrupted lines).13 Overall, average distance travelled 41
miles, median around 9 miles (assigning uniform probabilities car type).
six sample EVs show significantly different typical travel distances, ranging
average 4.19 miles (EV 1) 100 miles (EV 6).
following, discuss use data CABLED project instantiate
EV charging simulations.
12. allocate electricity hourly units, arrival departure times rounded
next full hour.
13. Note distance journeys exceeds range typical electric cars.
charged alternative locations CABLED trial, ignore experiments.
Since focus PHEVs work, practice, shortfall would made combustion
engine.

218

fiAn Online Mechanism Multi-Unit Demand

Units (3 kWh each)

15

High Supply
Low Supply

10
5
0

0
:0
00
0
:0
22
0
:0
20
0
:0
18
0
:0
16
0
:0
14
0
:0
12
0
:0
10
0
:0
08
0
:0
06
0
:0
04
0
:0
02
0
:0
00
Time

Figure 10: Number 3 kWh units available PHEV charging two scenarios
varying supply (high low ), based neighbourhood 30 households.

7.3.3 Generating Experiments
experimental run, simulate small neighbourhood 30 households
variable number PHEVs 24-hour period, starting 8:00 morning
8:00 following day. assume electricity allocated hourly time steps,
unit corresponds 3 kWh (which approximate energy obtained
standard 13 BS 1363 household socket UK charging hour).
obtain supply function S(t), first compute overall average electricity
consumption throughout neighbourhood (without PHEVs), based average consumption single UK household weekday high summer.14 Then, assume
overall electricity supply limited capacity local transformer,
electricity available PHEV charging, S(t), difference capacity
(possibly including additional safety margin) current overall consumption.
detail, consider two possible scenarios: (1) high supply scenario,
capacity limit set covers 150% peak consumption (at 10:00 pm),
resulting 615 kWh available PHEV charging; (2) low supply scenario,
capacity limit 80% peak consumption, resulting 99 kWh available charging.15
corresponding units available PHEV charging two scenarios shown
Figure 10.
Furthermore, use specific empirical distribution journey distances corresponding cars type cars travel distance distribution, p(m) (for example,
sampled car based car 3, use dashed dark blue distribution function Figure 9). use Equations 9 10 derive marginal valuations. case,
empirical distribution function discrete, integrals Equation 9 replaced
sums data points. Furthermore, initially assume ri drawn uniformly
14. used data available http://data.ukedc.rl.ac.uk/browse/edc/Electricity/LoadProfile/
data.
15. Local transformers often undersized way since prior PHEV use, could cool
overnight periods low demand.

219

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

random discrete set ri {1, 2, 3, 4}, is, cars charge one four
units electricity per hour (corresponding 3 12 kWh).
generate agents variety marginal valuations, note ee ep depend
specific make type PHEV. simulate this, draw ee uniformly
random 2 4 miles/kWh ep drawn 9 18 miles/litre. Next, draw
capacity car battery 15 25 kWh. realistic values modelled
Chevrolet Volt, first mass-produced PHEV. However, include variance
account vehicle types. Throughout experiments, hold price petrol
constant pp = 1.30 per litre.
Table 4 shows example valuations corresponding six cars considered previously (fixing ep = 13.5 miles/litre, ee = 3 miles/kWh battery capacity 20 kWh).
highlight longer expected journeys generally translate higher marginal valuations, also variable valuations individual agent. example,
car 4 values first 3 kWh electricity 0.67, seventh unit worth
0.038, far less likely used.
Car
1
2
3
4
5
6
..
.

vi,1
0.340
0.304
0.481
0.670
0.727
0.839
..
.

vi,2
0.136
0.178
0.157
0.453
0.620
0.797
..
.

vi,3
0.001
0.162
0.073
0.333
0.582
0.767
..
.

vi,4

vi,5

vi,6

vi,7

0.114
0.062
0.312
0.540
0.711
..
.

0.033
0.035
0.263
0.498
0.630
..
.

0.134
0.445
0.555
..
.

0.038
0.445
0.540
..
.

Table 4: Example marginal valuations (in ).
set experimental run, randomly generate set N PHEV agents,
vary N 1 60 simulate different levels demand.16 agent i,
first choose one 56 available cars uniformly random CABLED dataset,
base agents type on. Then, randomly select one cars recorded
journeys use time day cars arrival charging location ai (at
8:00 time window consider). ensure correlation arrival
times subsequent departure times dataset preserved, use departure
time journey immediately following sampled journey di (or 10 hours
arrival, whichever sooner).
7.3.4 PHEV Results
First, interested general trends mechanisms whether similar
trends discussed Section 7.2.2. end, Figure 11 shows allocative efficiency
mechanisms setting low supply (where feasible run Optimal
16. Note realistic number PHEVs within neighbourhood served single distribution
transformer (Huang & Infield, 2010).

220

fiAn Online Mechanism Multi-Unit Demand

Allocative Efficiency (% Optimal)
100%

90%
Optimal
Heuristic
Greedy
OD
IM
Fixed
Random

80%

70%

60%
0

5

10

15

20

25

30

35

40

45

50

55

60

Number EVs

Figure 11: Allocative efficiency small neighbourhood 30 households.
IM mechanisms). demonstrates broad trends previous
synthetic setup OD IM mechanisms clearly dominate truthful mechanism
(with IM achieving slightly worse results due higher cancellation rates). This, again,
due ability mechanisms always allocate agents highest
valuations.
However, although still consistently achieve around 90% Optimal, relative
performance OD IM mechanisms slightly lower. drop performance,
also witnessed Heuristic Greedy mechanism, due constrained
real-world settings, electricity available abundance certain times (i.e.,
night), agents significantly less patient others.
settings, often pay delay patient agents, even higher valuations,
favour less patient ones. Furthermore, valuations directly related
fuel costs saved unit electricity, less variance real-world valuations,
causing gap OD IM mechanisms truthful benchmarks
narrow slightly. Turning potential gains misreporting setting, Figure 12
confirms patterns observed previously. magnitude gains
slightly higher due different setting (reaching 3% terms overall gains
30% conditional case).
key advantage applying mechanisms real-world data allows us
determine actual fuel savings agents could achieve settings. Thus, Figure 13
shows average fuel savings agent various mechanisms, or,
words, average amount agent would spent fuel, allocated
electricity. Initially, high (around 1.15), little competition, starts
dropping PHEV owners compete amount electricity. key interest
horizontal separation different mechanisms. given fuel saving
per agent, mechanism sustain significantly larger number agents
truthful mechanisms. example, save least 0.40 per agent, Random support
221

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Potential Gain Utility Misreporting
35%

Proportion Gains
Expected Benefit Gain
Expected Benefit Overall

30%
25%
20%
15%
10%
5%
0%
5

10

15

20

25

30

35

40

45

50

55

60

Number EVs

Figure 12: Potential gains misreporting Greedy mechanism small neighbourhood 30 households.

Fuel Savings per Agent per Day (in )
Optimal
Heuristic
Greedy
OD
IM
Fixed
Random

1.25

1

0.75

0.5

0.25
0

5

10

15

20

25

30

35

40

45

50

55

60

Number EVs

Figure 13: Saving per agent per day small neighbourhood 30 households.
40 PHEV owners, IM OD achieve threshold around 60 PHEV
owners (an approximately 50% improvement).
Finally, consider detail presence fast-charging vehicles affects
overall neighbourhood, terms overall fuel savings, occurrence cancellations
utilities individual agents. end, fix number PHEVs 60
222

fiAn Online Mechanism Multi-Unit Demand

Social Welfare / Fuel Savings (per Day, )
90
30

Optimal
Heuristic
Greedy
OD

IM
Fixed Price
Random

85
80

25

75
20

70
65
0

10
20 30 40 50 60
Number FastCharging EVs

0

10 20 30 40 50 60
Number FastCharging EVs

Figure 14: Social welfare introducing fast-charging cars neighbourhood
low supply (left) high supply (right).

Units Cancelled per Day (% Total Allocated)
30%

30%

IM
OD

20%

20%

10%

10%

0%

0%
0

10 20 30 40 50 60
Number FastCharging EVs

0

10
20
30
40
50
Number FastCharging EVs

60

Figure 15: Cancellations introducing fast-charging cars neighbourhood
low supply (left) high supply (right).

consider low high demand settings shown Figure 10. Due
computational cost, test Optimal IM mechanisms setting
low supply. investigate impact fast-charging, assume two agent types
first, normal, charge single unit 3 kWh per time step, second,
fast, equipped fast chargers charge four units per time step.
223

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Utility Per Agent (per Day, p = 0.01)
1.25

14p

12p

1

10p
OD (Fast)
OD (Normal)
Greedy (Fast)
Greedy (Normal)

8p

0.75

0.5
0

10 20 30 40 50 60
Number FastCharging EVs

0

10 20 30 40 50 60
Number FastCharging EVs

Figure 16: Individual agent utility introducing fast-charging cars neighbourhood low supply (left) high supply (right).

Throughout experiments, vary number fast-charging PHEVs (out total
60). Figure 14 first shows resulting social welfare (i.e., overall fuel savings)
supply scenarios low supply (left) high supply (right). First, note
trends two scenarios different supply low, introduction
fast-charging vehicles little effect overall social welfare mechanisms,
supply high, mechanisms display increased savings. happens
first scenario highly constrained, low supply resulting occasions
agent could charge single unit per time step. contrast, supply high,
agents often allocated multiple units, thus enabling impatient agents particular
achieve higher overall fuel savings.
addition this, interesting note proposed mechanisms OD IM
benefit settings (achieving additional fuel savings almost two litres per day
low supply setting, seven litres high supply setting). reason
becomes evident considering proportion units cancelled fastcharging PHEVs introduced mechanisms settings, number
units cancelled consistently reduced around 7080% cars replaced
fast-charging PHEVs (shown Figure 15). occurs mainly
hti
active marginal valuations time step populate pi vectors, thus reducing
number cancellations. also causes gap mechanisms Greedy
mechanism shrink, fewer cancellations take place.
respect utility individual agents (including payments mechanism),
Figure 16 shows agents settings always incentive switch fast224

fiAn Online Mechanism Multi-Unit Demand

Potential Gain Utility Misreporting
40%

40%

Proportion Gains
Conditional Utility Increase
Overall Utility Increase

35%

35%

30%

30%

25%

25%

20%

20%

15%

15%

10%

10%

5%

5%

0%

0%
0

10
20 30 40 50 60
Number FastCharging EVs

0

10 20 30 40 50 60
Number FastCharging EVs

Figure 17: Potential gains misreporting Greedy mechanism neighbourhood
low supply (left) high supply (right).

charging PHEVs (e.g., purchasing domestic fast charger), applies
OD mechanism Greedy mechanism. low supply, expected daily saving
switching fast-charging PHEV approximately 0.020.03, high supply,
around 0.200.25. cases, benefit result increasing available
supply per time step, well increasing size price vector. Furthermore, even
entire population switch slow charging PHEVs fast-charging PHEVs,
individuals would, average, achieve higher utility. Note differences
utility OD Greedy mechanisms significantly smaller fast-charging vehicles,
indicating fast-charging agents expect lower gains misreporting
cancellations.
Figure 17 investigates individual gains misreporting
cancellations. shows interesting trend initial gains high (reaching
8% one setting), decrease significantly fast-charging PHEVs introduced
(to around 0.2% setting). clearly due significant reduction
cancellations witnessed settings. Furthermore, note, comparing OD
Greedy Figure 16, agents gain misreporting tend
slow-charging ones. Overall, promising result settings cancellations
feasible increasing consumption rate PHEVs (within realistic parameters
achievable current technological trends), scope potential benefits
strategising simple greedy mechanism reduced significantly. particular
example, supply high, fast-charging PHEV expect gain less 1
course entire year strategising optimally.
225

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

8. Conclusions
contributions paper theoretical practical. theoretical
side, propose novel online, model-free mechanism perishable goods agents
multi-unit demand non-increasing marginal valuations. show that, order
ensure dominant strategy incentive compatibility setting, mechanism occasionally requires units remain unallocated (we say pre-allocation cancelled),
even demand units. define two ways cancellation
performed: immediate, i.e., actual allocation, departure agent
market. study properties two variants, terms incentives
allocative efficiency. Furthermore, present algorithms computing payments
allocations mechanisms, analyse computational tractability.
on-departure cancellation mechanism better computational tractability,
worst-case competitive bound, terms allocation efficiency, single-unit
demand case. However, mechanism requires cancellations done departure
agent market always feasible. nave approach computing
payments mechanism immediate cancellations requires time exponential
number agents. address this, proposed branch-and-bound algorithm
allows payments computed immediate cancellation policy many realistically
sized settings. Another potential problem immediate cancellation policy
worst-case bound terms efficiency allocation.
practical side, show mechanism applied within smart grid
solve important problem integrating increasing number high-consumption
PHEVs electricity grid. addition synthetic setting, empirically evaluate
mechanism using real-world data large-scale trial electric vehicles UK.
show proposed mechanism highly robust, scalable (in particular, ondeparture variant) achieves better allocative efficiency fixed-price benchmark,
slightly less efficient established cooperative scheduling heuristic.
Specifically, demonstrate mechanism sustain 50% vehicles
fuel cost achieved using simple randomised mechanism. variants
also consistently achieve efficiency around 90%, compared hypothetical optimal
offline solution. Given theoretical results regarding bounds, surprising
result, suggesting specific conditions cause cancellations often occur
practice allocation policies perform well realistic settings. Furthermore,
consider introduction fast chargers within neighbourhood, show
leads significant increase overall fuel savings reduces
occurrence cancellations. Finally, since on-departure cancellation requires discharging
battery PHEV, consider potential gains misreporting units
cancelled (and assuming full knowledge types agents).
settings considered, average potential gain 1% 8% overall, could
go 30% average considering cases misreporting beneficial,
could even higher individual cases. gain becomes smaller number
competing agents increases and, interestingly, fast-charging PHEVs particularly low
incentives misreporting.
226

fiAn Online Mechanism Multi-Unit Demand

Taken together, mechanisms represent versatile range tools, may
suitable specific scenarios others. example, medium-sized settings
allocations cannot cancelled departure, IM mechanism may
suitable (e.g., low-supply PHEV settings outlined Section 7.3). settings
on-departure cancellation feasible, OD mechanism leads higher average
worst-case efficiency, also scalable. Here, also important emphasize
on-departure cancellation occurs users best interest thus,
entirely possible achieve optional action. Finally, show results,
even IM OD infeasible, mechanism without cancellations may still
viable settings, may even possible significantly reduce scope
manipulation adjusting system parameters; e.g., introducing fast chargers
PHEV setting increasing supply.
several directions extending work. related work, Stein et al. (2012)
discuss alternative model, uses probabilistic information future arrivals
designed elicit truthful reporting pure EVs, rather PHEVs. model,
however, requires knowledge future supply assumes single-minded bidders; i.e.,
preferences single-dimensional possible specify different values
different amounts charge received. future work, intend explore mechanisms
combine benefits approaches.
addition, intend test mechanism using real-world trial. seen
(see Section 7.3), design agent elicits information regarding intended use
(a probability distribution driving distance), combines information
price petrol efficiency vehicle, derive owners marginal
valuation vector. agent also participate mechanism owners behalf,
avoiding need owner understand details mechanism.
spirit work hidden market design (Seuken, Parkes, Horvitz, Jain, Czerwinski, &
Tan, 2012), aim design user interface users cognitive
load reduced hiding details underlying market. trial, intend
approach participants owning regular (non EV) cars, install GPS trackers cars.
Participants asked predict driving requirements, agent
use information, well learned historic driving patterns, derive users utility
function participate mechanism behalf. Although trial
regular cars, users able see much would hypothetically saved
providing accurate (and truthful) information intended use.

Acknowledgments
work supported iDEaS (www.ideasproject.info) ORCHID (www.orchid.ac.uk )
projects University Southampton. David Parkes supported part
Harvard SEAS TomKat fund.

References
Babaioff, M., Blumrosen, L., & Roth, A. (2010). Auctions online supply. Proceedings
11th ACM Conference Electronic Commerce (EC10), pp. 1322.
227

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Bent, R., & Van Hentenryck, P. (2004). value consensus online stochastic scheduling. Proceedings 14th International Conference Automated Planning
Scheduling (ICAPS04), pp. 219226.
Bikhchandani, S., Chatterji, S., Lavi, R., Mualem, A., Nisan, N., & Sen, A. (2006). Weak
monotonicity characterizes deterministic dominant-strategy implementation. Econometrica, 74 (4), 11091132.
Blum, M., Floyd, R. W., Pratt, V., Rivest, R. L., & Tarjan, R. E. (1973). Time bounds
selection. Journal Computer System Sciences, 7 (4), 448 461.
Clement, K., Haesen, E., & Driesen, J. (2009). Coordinated charging multiple plug-in hybrid electric vehicles residential distribution grids. Proceedings IEEE/PES
Power Systems Conference Exposition (PSCE09), pp. 17.
Constantin, F., Feldman, J., Muthukrishnan, S., & Pal, M. (2009). online mechanism
ad slot reservations cancellations. Proceedings ACM-SIAM Symposium
Discrete Algorithms (SODA09), pp. 12651274.
Constantin, F., & Parkes, D. C. (2009). Self-correcting sampling-based dynamic multi-unit
auctions. Proceedings 10th ACM Conference Electronic Commerce (EC09),
pp. 8998.
Eberle, U., & von Helmolt, R. (2010). Sustainable transportation based electric vehicle
concepts: brief overview. Energy & Environmental Science, 3, 689699.
Engel, Y., & Wellman, M. P. (2010). Multiattribute auctions based generalized additive
independence. Journal Artificial Intelligence Research (JAIR), 37, 479525.
Fairley, P. (2010). Speed bumps ahead electric-vehicle charging. IEEE Spectrum, 47 (1),
1314.
Gerding, E., Stein, S., Robu, V., Zhao, D., & Jennings, N. R. (2013). Two-sided online
markets electric vehicle charging. Proceedings 12th International Confernece
Autonomous Agents Multiagent Systems (AAMAS13), pp. 989996.
Gerding, E., Robu, V., Stein, S., Parkes, D., Rogers, A., & Jennings, N. (2011). Online
mechanism design electric vehicle charging. Proceedings 10th International
Conference Autonomous Agents Multi-Agent Systems (AAMAS11), pp. 811
818.
Gershkov, A., & Moldovanu, B. (2010). Efficient sequential assignment incomplete
information. Games Economic Behavior, 68 (1), 144154.
Hajiaghayi, M., Kleinberg, R., Mahdian, M., & Parkes, D. C. (2005). Online auctions
re-usable goods. Proceedings 6th ACM Conference Electronic Commerce
(EC05), pp. 165174.
Huang, S., & Infield, D. (2010). impact domestic plug-in hybrid electric vehicles
power distribution system loads. Proceedings International Conference
Power System Technology (POWERCON 2010), pp. 17.
Kamboj, S., Kempton, W., & Decker, K. S. (2011). Deploying power grid-integrated electric
vehicles multi-agent system. Proceedings 10th International Conference
Autonomous Agents Multiagent Systems (AAMAS11), pp. 1320.
228

fiAn Online Mechanism Multi-Unit Demand

Lavi, R., & Nisan, N. (2004). Competitive analysis incentive compatible on-line auctions.
Theoretical Computer Science, 310, 159180.
Nisan, N., Roughgarden, T., Tardos, E., & Vazirani, V. (2007). Algorithmic Game Theory.
Cambridge University Press.
Parkes, D. C. (2007). Online mechanisms. Nisan, N., Roughgarden, T., Tardos, E., &
Vazirani, V. (Eds.), Algorithmic Game Theory, pp. 411439.
Parkes, D. C., & Duong, Q. (2007). ironing-based approach adaptive online mechanism design single-valued domains. Proceedings 22nd National Conference
Artificial Intelligence (AAAI07), pp. 94101.
Parkes, D. C., & Singh, S. (2003). MDP-based approach online mechanism design. Proceedings 17th Conference Neural Information Processing Systems
(NIPS03), pp. 791798.
Pinedo, M. (2008). Scheduling: Theory, Algorithms, Systems (3rd edition). Springer.
Porter, R. (2004). Mechanism design online real-time scheduling. Proceedings
5th ACM Conference Electronic Commerce (EC04), pp. 6170.
Robu, V., Stein, S., Gerding, E., Parkes, D., Rogers, A., & Jennings, N. (2011). online mechanism multi-speed electric vehicle charging. Proceedings 2nd
International Conference Auctions, Market Mechanisms Applications
(AMMA11), pp. 100112.
Robu, V., Kota, R., Chalkiadakis, G., Rogers, A., & Jennings, N. R. (2012). Cooperative
virtual power plant formation using scoring rules. Proceedings 22nd AAAI
Conference Artificial Intelligence (AAAI12).
Robu, V., Noot, H., La Poutre, J. A., & van Schijndel, W. (2011). multi-agent platform
auction-based allocation loads transportation logistics. Expert Systems
Applications, 38 (4), 34833491.
Sandholm, T. (2002). Algorithm optimal winner determination combinatorial auctions. Artificial Intelligence, 135 (1-2), 154.
Seuken, S., Parkes, D. C., Horvitz, E., Jain, K., Czerwinski, M., & Tan, D. (2012). Market user interface design. Proceedings 13th ACM Conference Electronic
Commerce, pp. 898915. ACM.
Stein, S., Gerding, E., Robu, V., & Jennings, N. R. (2012). model-based online mechanism
pre-commitment application electric vehicle charging. Proceedings
11th International Conference Autonomous Agents Multiagent Systems
(AAMAS12), pp. 669676.
Stein, S., Gerding, E., Rogers, A., Larson, K., & Jennings, N. R. (2011). Algorithms
mechanisms procuring services uncertain durations using redundancy. Artificial Intelligence, 175, 20212060.
Sundstrom, O., & Binding, C. (2012). Flexible charging optimization electric vehicles
considering distribution grid constraints. IEEE Transactions Smart Grid, 3 (1),
2637.
229

fiRobu, Gerding, Stein, Parkes, Rogers & Jennings

Vasirani, M., & Ossowski, S. (2011). computational monetary market plug-in electric
vehicle charging. Proceedings 2nd International Conference Auctions,
Market Mechanisms Applications (AMMA11), pp. 8899.
Vytelingum, P., Voice, T., Ramchurn, S. D., Rogers, A., & Jennings, N. R. (2011). Theoretical practical foundations large-scale agent-based micro-storage smart
grid. Journal Artificial Intelligence Research (JAIR), 42, 765813.

230

fiJournal Artificial Intelligence Research 48 (2013) 475-511

Submitted 04/13; published 11/13

Horn Clause Contraction Functions
James P. Delgrande

jim@cs.sfu.ca

School Computing Science,
Simon Fraser University,
Burnaby, B.C., V5A 1S6
Canada

Renata Wassermann

renata@ime.usp.br

Dept. Computer Science
University Sao Paulo
05508-090 Sao Paulo,
Brazil

Abstract
classical, AGM-style belief change, assumed underlying logic contains
classical propositional logic. clearly limiting assumption, particularly Artificial
Intelligence. Consequently recent interest studying belief change approaches full expressivity classical propositional logic obtained.
paper investigate belief contraction Horn knowledge bases. point
obvious extension Horn case, involving Horn remainder sets starting point,
problematic. Horn remainder sets undesirable properties, also
desirable Horn contraction functions captured approach. Horn belief set
contraction, develop account terms model-theoretic characterisation involving
weak remainder sets. Maxichoice partial meet Horn contraction specified,
show problems arising earlier work resolved approaches.
well, constructions specific operators sets postulates provided, representation results obtained. also examine Horn package contraction, contraction
set formulas. Again, give construction postulate set, linking via
representation result. Last, investigate closely-related notion forgetting Horn
clauses. work arguably interesting since Horn clauses found widespread use
AI; well, results given may potentially extended areas make
use Horn-like reasoning, logic programming, rule-based systems, description
logics. Finally, since Horn reasoning weaker classical reasoning, work sheds
light foundations belief change.

1. Introduction
area belief change knowledge representation studies rational agent may
alter beliefs presence new information. best-known approach area
so-called AGM paradigm (Alchourron, Gardenfors, & Makinson, 1985; Gardenfors,
1988), named original developers. work focused primarily two belief
change operations, belief contraction, agent may reduce stock beliefs,
belief revision, new information consistently incorporated agents belief
corpus. fundamental assumption approach underlying logic governing
c
2013
AI Access Foundation. rights reserved.

fiDelgrande & Wassermann

agents beliefs subsumes classical propositional logic. However, artificial intelligence
(AI) major concern efficient, limited, ideally tractable reasoning. Hence
significant effort studying limited reasoners, including Horn clause based
approaches, limited epistemic reasoning involving explicit belief (Lakemeyer & Levesque,
2000), description logics (Baader, Calvanese, McGuiness, Nardi, & Patel-Schneider,
2007). Moreover, since knowledge base evolve, crucially important change
knowledge base managed principled fashion. However, AGM approach
cannot used guide change approach, mentioned above,
subsume classical propositional logic.
paper address belief change expressively-weak language Horn clauses,
Horn clause written rule form a1 a2 n 0,
a, ai (1 n) atoms. (Thus, expressed conjunctive normal form,
Horn clause one positive literal.) approach, agents beliefs
represented Horn clause knowledge base, input conjunction Horn
clauses. focus belief contraction (and, later, operators related contraction)
agents stock beliefs decreases.
topic Horn clause contraction (and general topic Horn belief change
general) interesting several reasons. First, Horn clause reasoners constitute important class AI systems, Horn clauses found extensive use artificial intelligence
database theory, areas logic programming, truth maintenance systems,
deductive databases. Horn clause belief change also sheds light theoretical underpinnings belief change, weakens assumption underlying logic contains
propositional logic. Hence results obtained may relevant belief change
areas limited reasoning. example, approaches explicit belief often derives much
inspiration relevance logic (Anderson & Belnap Jr., 1975); description logics,
constituting fragments classical first-order logic, nonetheless many cases
support full propositional reasoning.1
Creignou, Papini, Pichler, Woltran (2012) provide motivation study
belief change tractable fragments propositional logic:
many applications, language restricted priori. instance, rulebased formalization expert knowledge much easier handle standard
users. case users want revise rules, indeed expect
outcome still easy-to-read format used to. Many fragments
propositional logic allow efficient reasoning methods. Suppose agent
frequently answer queries beliefs. done
efficiently thus beliefs stored formula known tractable
class. case beliefs agent undergoing revision, desired
result operation yields formula fragment. Hence,
agent still use dedicated solving method equipped
fragment. case changes performed rarely, bother whether
revision performed efficiently, important
outcome still evaluated efficiently.
1. fact, Booth, Meyer, Varzinczak (2009) point out, results also relevant belief change
description logics, topic also elicited recent interest.

476

fiHorn Clause Contraction Functions

Horn clause contraction become topic interest belief change recent years
(Delgrande, 2008; Delgrande & Wassermann, 2010, 2011; Booth et al., 2009; Booth, Meyer,
Varzinczak, & Wassermann, 2011; Zhuang & Pagnucco, 2010a, 2011, 2012). discuss
next section, work centers notion remainder set, maximal
subset knowledge base fails imply given formula. show remainder
sets Horn case restricted cannot give feasible contraction operators.
well yield contraction operators undesirable properties.
propose notion weak remainder set serves basis generating
Horn maxichoice contraction operators. Contraction also considered terms
underlying model theory, viewpoint proves highly enlightening studying Horn
belief change. Given specification maxichoice contraction based weak remainders,
go develop specification partial meet Horn contractions, consider package
contraction forgetting. contraction operators developed, provide postulate
sets along constructions, show representation results. Consequently present
comprehensive exploration landscape Horn contraction.
next section introduces belief change following section discusses reasoning
Horn clause theories. main approach presented Section 4, Section 5
discusses considerations pertaining supplementary contraction postulates. Section 6
covers related operators package contraction forgetting Horn theories.
paper concludes discussion concluding section. Proofs given appendix.
material presented previously Delgrande (2008) Delgrande
Wassermann (2010, 2011).

2. Background
section, introduce main concepts area Belief Change
need throughout paper.
2.1 Belief Change
previously mentioned, AGM approach (Alchourron et al., 1985; Gardenfors, 1988)
original best-known approach belief change.2 goal approach
describe belief change knowledge level, is, abstract level independent
beliefs represented manipulated. Belief states modelled sets sentences,
called belief sets, closed logical consequence operator logic includes
classical propositional logic language L. Thus belief set K satisfies constraint:
K logically entails K.
central operators3 addressed contraction, agent reduces set beliefs,
revision, agent consistently incorporates new belief. revision, since
new belief may inconsistent agents beliefs, beliefs may need dropped
order maintain consistent set beliefs. third operator, belief expansion also
2. well, Peppas (2008) provides excellent survey.
3. paper, use terms operator f unction interchangeably refering belief change
operations.

477

fiDelgrande & Wassermann

introduced: belief set K formula , expansion K , denoted K + ,
deductive closure K {}. Expansion captures simplest form belief change;
reasonably applied new information consistent belief set
operators characterised two means. one hand, set rationality
postulates belief change function may provided; postulates stipulate constraints govern rational belief change function. hand, specific
constructions belief change function given. Representation results provided, showing set rationality postulates exactly captures operator given
particular construction.
review notions belief contraction. Informally, contraction belief set
formula belief set formula believed. Formally, contraction
function function 2L L 2L satisfying following postulates.
(K 1) K belief set.
(K 2) K K.
(K 3) 6 K, K = K.
(K 4) 6` , 6 K .
(K 5) K, K (K ) + .
(K 6) , K = K .
(K 7) K K K ( ).
(K 8) 6 K ( ) K ( ) K .
Thus, contraction yields belief set (K 1) sentence contraction
believed (unless tautology) (K 4). new sentences believed (K 2),
formula originally believed contraction effect (K 3). fifth
postulate, so-called recovery postulate, states nothing lost one contracts
expands sentence. postulate controversial, discussed, example
Hansson (1999). sixth postulate asserts contraction independent
sentence syntactically expressed. last two postulates express relations
contracting conjunctions contracting constituent conjuncts. Hence (K 7)
says formula result contracting two formulas
result contracting conjunction. (K 8) says conjunct
result contracting conjunction, then, presence (K 7), contracting
conjunct contracting conjunction. first six postulates referred
basic postulates last two referred supplementary postulates.
Revision represents situation new information may inconsistent
reasoners beliefs K, needs incorporated consistent manner, one
exception formula revision inconsistent. revision function
function 2L L 2L satisfying set postulates analogous
contraction. Contraction usually taken fundamental operator belief
478

fiHorn Clause Contraction Functions

change. Moreover, revision contraction interdefinable. Revision defined
terms contraction means Levi Identity:
K = (K ) + .

(1)

Thus, revise , make K consistent expand . Contraction
similarly defined terms revision Harper identity:
K = K (K ).
Since consider revision functions paper, refer reader work
Gardenfors (1988) Peppas (2008) details.
Various constructions proposed characterise belief change. original
construction terms remainder sets, -remainder K maximal subset
K fails imply . Formally:
Definition 1 Let K L let L.
K set sets formulas s.t. K 0 K iff
1. K 0 K
2. K 0 6`
3. K 00 s.t. K 0 K 00 K, holds K 00 ` .
K 0 K -remainder K.
Thus K class maximal -nonimplying subsets K.
ambiguity, also refer K 0 K simply remainder K.
Two classes contraction functions relevant concerns. maxichoice contraction, contraction defined correspond single selected remainder. partial meet
contraction, contraction corresponds intersection subset remainders.
Consequently, maxichoice contraction partial meet contraction vice versa.
logical point view, -remainders comprise equally-good candidates
contraction K. Selection functions introduced reflect extra-logical
factors need taken account, obtain best plausible remainders.
maxichoice contraction, selection function determines single selected remainder
contraction. partial meet contraction, selection function returns subset
remainders, intersection constitutes contraction. Thus selection
function denoted (), contraction K formula expressed
\
K =
(K ).
(2)
belief set K function 2L L 2L , proves case
partial meet contraction function iff satisfies basic contraction postulates (K 1)
(K 6). Last, let transitive relation 2K , let selection function defined
by:
(K ) = {K 0 K | K 00 K , K 00 K 0 }.
transitively relational selection function, defined terms
transitively relational partial meet contraction function. have:
479

fiDelgrande & Wassermann

Theorem 1 (Alchourron et al., 1985) Let K belief set let function
2L L 2L .
1. partial meet contraction function iff satisfies contraction postulates
(K 1)(K 6).
2. transitively relational partial meet contraction function iff satisfies contraction postulates (K 1)(K 8).
second major construction contraction functions called epistemic entrenchment. general idea extra-logic factors related contraction given
ordering formulas agents belief set, reflecting willing agent would
give formula. contraction function defined terms removing less
entrenched formulas belief set. Gardenfors Makinson (1988) show
logics including classical propositional logic, two types constructions, selection functions remainder sets epistemic entrenchment orderings, capture class
contraction functions.
Two constructions also proposed literature shown equivalent
transitively relational partial meet contraction: safe contraction (Alchourron & Makinson, 1985; Rott, 1992) systems spheres (Grove, 1988). address either
construction paper.
2.2 Belief Change Horn Clause Theories
Earlier work belief change Horn theories focussed specific aspects problem,
rather general characterisation Horn clause belief change. example,
complexity specific approaches revising knowledge bases addressed Eiter
Gottlob (1992). includes case knowledge base formula
revision conjunctions Horn clauses, although results revision may Horn.
unexpectedly, results generally better Horn case. Liberatore (2000) considers
problem compact representation revision Horn case. Given knowledge
base K formula , Horn, main problem addressed whether knowledge
base, revised according given operator, expressed propositional formula
whose size polynomial respect sizes K .
Langlois, Sloan, Szorenyi, Turan (2008) approach study revising Horn formulas characterising existence complement Horn consequence; complement corresponds result contraction operator. work may seen specific
instance general framework developed Flouris, Plexousakis Antoniou (2004).
study belief change broad notion logic. particular, give criterion
existence contraction operator satisfying basic AGM postulates terms
decomposability.
present paper builds extends (Delgrande, 2008; Delgrande & Wassermann,
2010, 2011). Delgrande (2008) addresses maxichoice belief contraction Horn clause theories, contraction defined terms remainder sets, using Definition 1, expressed terms derivations among Horn clauses. Booth, Meyer, Varzinczak (2009)
Booth, Meyer, Varzinczak, Wassermann (2011) develop area,
480

fiHorn Clause Contraction Functions

considering versions contraction, based remainder sets: partial meet contraction, generalisation partial meet, package contraction. Horn contraction based
remainders found inadequate Delgrande Wassermann (2010), instead
developed notion weak remainder. work Zhuang Pagnucco (2010a,
2012) follows another line, focusing epistemic entrenchment model-based constructions. approaches discussed compared detail introduced
overall approach.
Recently, revision operations Horn theories also developed (Delgrande &
Peppas, 2011), revision fragments propositional logic also explored
(Creignou et al., 2012). However relation work contraction operations
described paper still unclear.

3. Horn Clause Theories
deal languages based finite sets atoms, propositional letters P =
{a, b, c, . . . }, P includes distinguished atom . L language propositional
logic P usual connectives , , , .4 LHC restriction L
Horn formulas, Horn formula finite conjunction Horn clauses. LHC
least set given by:
1. a1 a2 a, n 0, a, ai (1 n) atoms, Horn clause.
2. Every Horn clause Horn formula.
3. Horn formulas .
well, convenience, > taken denoting specific atom a.
Horn clause r 1 above, n = 0 r fact, also written a.
Horn clause r 1 above, head(r) a, body(r) set {a1 , a2 , . . . , }.
fact, head(r) a, body(r) empty. Horn clause r
head(r) = , r integrity constraint. Allowing conjunctions clauses, given
3, adds nothing interest expressibility language respect reasoning.
However, adds expressibility contraction, able contract
single Horn clause.
Semantics: interpretation L function P {true, f alse}
assigned f alse. Sentences L true false interpretation according
standard rules propositional logic. interpretation model sentence (or set
sentences), written |= , true . od() set models formula
(or set formulas) ; thus od(>) set interpretations L. interpretation
usually identified atoms true interpretation. Thus, language given
P = {p, q, r, s}, interpretation expressed {p, q} p q true
r false. convenience, also express interpretations juxtaposition
atoms. Thus set interpretations {{p, q}, {p}, {}} usually written {pq, p, }.
notions inherited corresponding Horn formula language LHC .
key point concerning Horn theories theories characterised semantically
4. avoid clutter, ambiguity results, dont parameterize L P.

481

fiDelgrande & Wassermann

fact models closed intersections positive atoms interpretation. is, Horn theory H satisfies constraint:
M1 , M2 od(H) M1 M2 od(H).
leads notion characteristic models (Khardon, 1995) Horn formula
set formulas: characteristic model formula every M1 , M2 od(),
M1 M2 = implies = M1 = M2 . Thus example, {p q , r}
models {pr, qr, r} characteristic models {pr, qr}. Since pr qr = r, r isnt
characteristic model .
Proof Theory: assume suitable inference relation ` classical propositional logic.
following axioms rules give inference relation Horn formulas,
simplicity, b, possibly subscripted, taken ranging atoms.
Axioms:



aa

Rules:
1. a1 b1 bn a1
infer b1 bn a2
2. a1 infer a1 b
3. Horn clauses r1 , r2 , body(r1 ) = body(r2 ) head (r1 ) = head (r2 )
r1 infer r2 .
4. (a)
(b)

infer
infer

Rule 1 extended version modus ponens, Rule 2 strengthening antecedent. Rule 3 states order atoms body Horn clause irrelevant,
repeated atoms.
formula derived set formulas A, written `HC ,
obtained finite number applications rules axioms;
simplicity drop subscript write ` . = {} singleton set
write ` . set formulas LHC inconsistent ` . use
represent logical equivalence, ` ` .
Notation: collect reference notation used paper. Lower-case
Greek characters , , . . ., possibly subscripted, denote arbitrary formulas either L
LHC . Upper case Roman characters A, B, . . . , possibly subscripted, denote arbitrary sets
formulas. H, H1 , H 0 , etc. denote Horn belief sets, H iff H `HC .
Cn(A) (classical, propositional) deductive closure formula
set formulas propositional logic. Cnh (A) deductive closure Horn formula
set formulas Horn derivability. set formulas A, Horn(A) = { |
Horn formula}.
use (possibly subscripted) denote maximal consistent Horn theory;
is, 6` every Horn formula , either {} ` . Hence
482

fiHorn Clause Contraction Functions

exactly one model. often use maximal consistent sets formulas place
interpretations, makes statement proof various results easier. || set
maximal, consistent Horn theories contain .
(M1 , 0 , etc.) denote (classical, propositional) interpretations understood language. od(A) set models A. Arbitrary sets interpretations
denoted (M0 etc.). Cl (M) intersection closure set interpretations M;
is, Cl (M) least set interpretations
1. Cl (M)
2. M1 , M2 Cl (M) implies M1 M2 Cl (M).
Note denotes interpretation expressed set atoms, denotes
maximal consistent set Horn formulas. Thus logical content same,
interpretation defines maximal consistent set Horn formulas, vice versa.
retain two interdefinable notations, since useful subsequent development.
Similar comments apply od() vs. ||; also make use fact 1-1
correspondence elements || od().
Last, since P finite, (Horn propositional logic) belief set may finitely represented, is, X belief set, formula Cn() = X.

4. Horn Clause Belief Set Contraction
section, examine possible constructions operation contraction
Horn belief sets. begin operations based remainde sets proceed introducing
concept weak remainder set.
4.1 Horn Clause Contraction Remainder Sets
straightforward way define Horn contraction function adapting construction used classical logic contraction. end, Delgrande (2008) developed
remainder-set approach Horn contraction, subsequently generalised Booth,
Meyer Varzinczak (2009). proves case approaches sufficiently expressive general Horn contraction; well, contraction based remainder
sets shown undesirable properties. review pertinent aspects
approaches here, particular consider results (classical, AGM) contraction
readily extend Horn case.
definition remainder sets Horn clause belief sets (called e-remainder sets
Delgrande, 2008) remainder set (Definition 1) respect
Horn clauses Horn derivability. H Horn belief set LHC , set
e-remainders respect H denoted H e .
Definition 2 Let H LHC let LHC .
H e set sets formulas H 0 H e iff
1. H 0 H
2. H 0 6`
483

fiDelgrande & Wassermann

3. H 00 H 0 H 00 H holds H 00 ` .
H 0 H e -e-remainder respect H.
Usually H 0 referred simply remainder, since Horn context
underlying formula clear.
Observation 1 H e 1 = H e 2 , H 0 H, 1 Cnh (H 0 ) iff 2
Cnh (H 0 ).
Observation 2 (Upper bound property) X H 6 Cnh (X),
X 0 X X 0 H e .
Horn remainders given Definition 2 regarded comprising set candidate
contractions H formula ; single remainder could selected
maxichoice contraction H . Booth, Meyer, Varzinczak (2009) subsequently argue
maxichoice contraction sufficient account Horn contraction functions.
classical AGM contraction, set partial meet contraction functions defined
taking intersection remainders. However, Booth, Meyer, Varzinczak
also argue set Horn partial meet contractions sufficient capture full
range possible contraction functions. Instead define infra remainder sets, follows:
Definition 3 belief sets H X, X H e 5 iff X 0 H e
\

H e X X 0 .
elements H e infra e-remainder sets H respect .
Thus infra e-remainder set belief set contains intersection Horn remainders, contained Horn remainder. e-remainder sets clearly infra
e-remainder sets, intersection set e-remainder sets. is:

Observation 3 Let H LHC , LHC , let X H e . ( X) H e .
Example 1 P = {a, b, c}, let H = Cnh (a b).
Consider candidates H (a b).
verified three remainder sets:
Cnh (a (c b)),
Cnh (b (c a)),
Cnh ((a



b) (b a) (c a) (c b)).

well, remainder set infra remainder set must contain closure
(c a) (c b).
5. Booth, Meyer, Varzinczak (2009) write X H e set Horn clauses.

484

fiHorn Clause Contraction Functions

see last part example, note (c a) (c b) remainders,
intersection remainders. however leads significant blemish. Call
p inessential H conjunction atoms body containing p, H ` p body
implies either ` p body H ` body a. contraction defined terms
remainder sets, intersections remainder sets, infra remainder sets,
result:6
Theorem 2 Let Horn contraction function defined via selection function
(2) based (infra) remainder sets.
H p inessential H, obtain (H ) + p ` .
following example (based example Hansson, 1999) illustrates problem:
1. believe Cleopatra son daughter (s d).
2. learn source information unreliable, remove belief; i.e.
compute contraction H (s d).
3. learn raining outside (r).
4. conclude Cleopatra son daughter (s d)
behaviour clearly undesirable. However, consider example implies
Horn contraction point: H (s d) + r entails d. Hence,
regardless defined terms (infra) remainders, models H (s d)
r true must also true. turn means sdr
cannot model d-remainder. last point curious, sdr clearly
counter-model d, yet take part remainder, take
part contraction.
AGM contraction, -remainder belief set K characterized set models K together single countermodel , vice versa
(e.g., see Gardenfors, 1988, p. 86). example shows equivalence
proof-theoretic notion remainders semantic notion minimallyextended sets models breaks Horn case.
So, consider going Horn case: Assume H |= wish find
maximal belief set H 0 H 0 H H 0 6|= . is, H 0 -remainder set
H. described, classical AGM (maxichoice) contraction, semantic side one
adds countermodel models H; set models characterises candidate
theory maxichoice contraction.
Consider analogous process Horn theories. Since remainder set must
Horn theory, models Horn theory closed intersection, would
need make sure constraint holds here. So, intuitively, carry maxichoice
Horn contraction, would add countermodel formula contraction, close
result intersection. However, theories resulting approach
correspond obtained via remainder sets. see this, consider Example 1,
pertinent results summarised Figure 1.
6. result would also obtained package contraction, discussed Section 6, package contraction
defined terms infra remainder sets.

485

fiDelgrande & Wassermann

countermodel
ac

bc
b
c


induced
models

b


resulting KB

remainder
set?


(c b)
b
b (c a)
(a b) (b a)
(a b) (b a) (c a) (c b)





Figure 1: Example: Candidates Horn contraction
ac (viz. abc) countermodel = ab; given first entry
first row table. Since H model ab, intersection models,
ab ac = must also included; item second column. resulting
belief set characterised interpretations od(H) {ac, a} = {abc, ab, ac, a},
set models formula a, given third column. result isnt remainder
set, since Cnh (a (c b)) logically stronger belief set fails imply b. last
belief set, Cnh (a (c b)) appears second row table. observed
models belief set made models H together countermodel
a, is, induced model first row.
previously noted, three remainder sets, indicated last column.
discussed, result problematic approaches Delgrande (2008)
Booth, Meyer, Varzinczak (2009). example, none approaches
papers possible obtain H e (a b) a, possible obtain H e (a b)
((a b) (b a)). possibilities would desirable potential contractions.
diagnosis problem presumably clear. example,
countermodel given abc, possible set interpretations satisfying:
1. closed intersections
2. = od(H) abc
solution also seems clear: semantic point view, one wants characteristic
models maxichoice candidates H e consist characteristic models H
together single interpretation od(>) \ od(). resulting theories, called
weak remainder sets, would correspond theories given third column Figure 1;
explore notion next subsection.
conclude note shown (maxichoice) contraction based
remainder sets alone suffers triviality result analogous AGM contraction.
Theorem 3 (Makinson, 2009) Let P atom, let H Horn belief set
H. Let maxichoice Horn contraction function based remainder sets.
every atom b, least one b b H (a ) + a.
Hence false according H, contracting expanding
yields belief set every atom believed true believed false.
clearly far unrealistic useful.
486

fiHorn Clause Contraction Functions

4.2 Horn Clause Contraction Weak Remainder Sets
previous section showed basing Horn contractions solely remainder sets (or infra
remainder sets) problematic. suggested adequate version contraction
based weak remainder sets belief set H formula H,
1-1 correspondence countermodels weak remainder sets. section
develop Horn contraction based weak remainder sets. first give two constructions
weak remainder sets, terms belief sets terms sets models, show
constructions equivalent. characterise maxichoice Horn contraction
terms weak remainder sets, showing via representation result characterisations
equivalent. Following similarly characterise partial meet contraction.
Definition 4 Let H Horn belief set, let Horn formula.
set sets formulas
1. H then: H 0 iff H 0 = H |>| \ ||.
2. Otherwise = {H}.
H 0 weak remainder set H .
Observation 4 H 0 , H 0 belief set, i.e., H 0 = Cnh (H 0 ).
definition, maximal consistent set formulas, corresponds
set formulas true interpretation. case underlying interpretation
would belong od(>)\M od(), say underlying interpretation would
countermodel .
Example 2 P = {a, b, c}, let H = Cnh (a b) = b.
m1 = Cnh (a b c) |>| \ ||, H m1 = Cnh (a (c b)).
m2 = Cnh (a b c) |>| \ ||, H m2 = Cnh (a).
Note (H m2 ) (H m1 ), also full propositional closure gives Cn(H
m2 ) = Cn(a (b c)).
previous definition specifies weak remainder sets terms maximal consistent
sets formulas. next definition similar, expressed directly terms countermodels formula.
Definition 5 Let H Horn belief set, let Horn formula. Define H ||e by:
1. H then: H ||e set sets formulas H 0 H ||e iff
6 od() od(H 0 ) = Cl (M od(H) {M }).
2. Otherwise H ||e = {H}.
running example, H ||e given closure formulas column 3
Figure 1.
Perhaps surprisingly, two characterisations prove equivalent:
487

fiDelgrande & Wassermann

Theorem 4 H Horn belief set Horn formula:
= H ||e .
position define Horn contraction operator. start defining
selection function, basically done AGM approach. Given selection function,
straightforward define maxichoice contraction operator, following this, partial
meet contraction operator.
Definition 6 Let H Horn belief set. selection function H if, every
LHC ,
1. 6= =
6 (He ) .
2. = (He ) = {H}.
Definition 7 Let selection function H (He ) = {H 0 }
H 0 .
maxichoice Horn contraction based weak remainders given by:
H w = (He )
Hence result maxichoice contraction characterised single weak remainder
set.
obtain following representation result, relating construction postulate
set characterising contraction:
Theorem 5 Let H Horn belief set. w operator maxichoice Horn
contraction based weak remainders iff w satisfies following postulates.
(H w 1) H w Horn belief set.

(closure)

(H w 2) ` , 6 H w .

(success)

(H w 3) H w H.

(inclusion)

(H w 4) 6 H, H w = H.

(vacuity)

(H w 5) ` H w = H

(failure)

(H w 6) , H w = H w .

(extensionality)

(H w 7) H 6= H w LHC {, } ` , H w Cnh () H 0
s.t H w H 0 H H 0
6 Cnh ().
(maximality)
first four postulates (H w 6) obvious counterparts AGM contraction
postulates. Notably, obtain recovery postulate. following provides
counterexample.
488

fiHorn Clause Contraction Functions

Example 3 Let H = Cn(p q) = p r q.
H w 6` p q, since p q ` p r q.
Thus H w Cn({p r q | P \ {p, r}}.)
H w + Cn({p r q | P \ {p, r}) + 6` p q
hence H w + 6` p q.
Postulate (H w 5) derivable using AGM postulates, relies recovery postulate (K 5) proof. Since lack recovery postulate, required
postulate, covering special case, right.
Postulate (H w 7) complicated others, expresses basic defining characteristic maxichoice revision: contraction nontrivial (viz. H 6= H w ),
countermodel model H w . expressed ,
mutually inconsistent, H w subset closure , H w maximal
set formulas holds. turn means that, even though recovery
postulate hold, nonetheless trivial contraction, entire belief set
discarded, excluded legal contraction operator. verified Example 1 (see also Figure 1) countermodels abc abc fulfill conditions
(H w 7), say, postulate captures notion weak remainder set.
turn next partial meet Horn contraction. definition partial meet Horn
contraction analogous AGM contraction, based weak remainder sets:
Definition 8 Let selection function H (He ) (He ).
partial meet Horn contraction based weak remainders given by:
H pm =

\

(He )

representation result involves modification last postulate maxichoice contraction:
Theorem 6 Let H Horn belief set. pm operator partial meet Horn
contraction based weak remainders iff pm satisfies postulates (H w 1) (H w 6)
and:
(H pm 7) H \(H pm ), H 0 H pm H 0 , 6 Cnh (H 0 )
Cnh (H 0 {})
(weak relevance)
Example 4 running example, partial meet given first last weak
remainder sets Figure 1 given
Cnh ((b a) (c a)).
terms models, characterised models b, together two countermodels given atoms ac , closed intersections.
489

fiDelgrande & Wassermann

5. Supplementary Postulates
section investigate different proposals Horn contraction operations
behave respect supplementary postulates (K 7) (K 8). Throughout
section, assume selection functions transitively relational.
First consider operation Horn partial meet e-contraction (Delgrande, 2008).
following example shows that, considering e defined Delgrande (see also Definition 2), Horn partial meet e-contraction satisfy (K 7):
Example 5 Let H = Cnh ({a b, b c, d, c}).

H e (a c) = {H1 , H2 , H3 , H4 }
H e (b c) = {H5 }
where:
H1
H2
H3
H4
H5

= Cnh ({a b, d}),
= Cnh ({a b, c d, c}),
= Cnh ({b c, c b, d}),
= Cnh ({a c b, b c, c d, c, b, b d}),
= Cnh ({a b, d, c})

Note two first elements H e (a c) subsets single element
H e (b c) hence, cannot belong H e (a c b c).
H e (a c b c) = {H3 , H4 , H5 }
take selection function based transitive relation remainder sets
gives priority order appear example, i.e., H5 H4 H3
H2 H1 , have:
H (a c) = H1
H (b c) = H5
H (a c b c) = H3
see
H (a c) H (b c) = H1 6 H3 = H (a c b c)
example shows operation satisfy (K 8):
c 6 H (a c b c) H (a c b c) 6 H (a c).
restrictions selection function, example also
shows contraction based infra-remainders satisfy supplementary postulates. Note remainder set example also infra-remainder
selection function always selects single element. suffices assign remaining
infra-remainders lower priority.
show operation partial meet based weak remainders (PMWR)
better behaviour respect supplementary postulates:
490

fiHorn Clause Contraction Functions

Theorem 7 Partial meet based weak remainders transitive relational selection
function satisfies (K 7) (K 8).
recently, Zhuang Pagnucco (2010a) addressed Horn contraction
point view epistemic entrenchment. compare AGM contraction via epistemic
entrenchment classical propositional logic contraction Horn logics. postulate
set provided shown characterise entrenchment-based Horn contraction. fact
AGM contraction allows disjunctions formulas, general Horn,
handled considering Horn strengthenings postulate set, say, logically
weakest Horn formulas subsume disjunction. contrast earlier work,
postulate set includes equivalents supplemental postulates, goes beyond
set basic postulates. detail, Zhuang Pagnucco (2010a) following:
Definition 9 given clause , set Horn strengthenings ()H set
()H Horn clause Horn clause 0
0 .
ten postulates given Zhuang Pagnucco (2010a) characterize epistemic
entrenchment Horn contraction (EEHC), postulates (H 1), (H 2), (H 4), (H 6), (H 7)
(H 8) correspond exactly AGM postulates numbers. (H 1),
(H 2), (H 3), (H 4) (H 6) correspond postulates (H w 1)-(H w 6) characterizing partial meet contraction based weak remainders defined. three new postulates are:
(H 5) H H
(H 9) H \ H ( )H , 6 H
(H 10) ( )H , 6 H 6 H \ H
Subsequently, Zhuang Pagnucco (2010b) shown transitively relational
PMWR defined general EEHC. means operation
satisfying set 10 postulates (which include (K 7) (K 8)) PMWR.
seen PMWR satisfies (K 7) (K 8), hence, order compare PMWR
EEHC, need know whether PMWR satisfies (H 5), (H 9) (H 10).
Theorem 8 PMWR satisfies (H 5).
Zhuang (2012) shown weak relevance implies (H 9), hence, PMWR satisfies
(H 9). PMWR general satisfy (H 10), following example shows:
Example 6 Let H = Cnh ({a, b}).

= {H1 , H3 }
(a b) = {H1 , H2 , H3 },

491

fiDelgrande & Wassermann

H1 = Cnh ({b a, b}),
H2 = Cnh ({a})
H3 = Cnh ({b}).
Assuming selection function based transitive relation H1 H2
H1 H3 (and H2 H3 H3 H2 ),
H = H3 H (a b) = H2 H3
Since (ab)H = {a, b}, (ab)H , 6 H (ab), b H a.

6. Operators
section consider two contraction-like operators. first, package contraction,
like contraction, defined respect set formulas. second operator,
forget, regarded removal atom set atoms language
discourse.
6.1 Package Contraction
AGM-style belief change propositional logic, given belief set K set formulas
, package contraction K p form contraction (non-tautological)
member K p . propositional logic effect package contraction may
nearly, quite, obtained contracting disjunction elements . see
difference, consider = {, }. Clearly, 6 K ( ) whereas seems
simultaneous contraction K p {, } allow possibility
true outcome.
Booth, Meyer, Varzinczak (2009) note, package contraction interest Horn
clause theories, given limited expressivity theories. is, , Horn
formulas, H ( ) undefined whenever non-Horn (which, course,
time). hand, expressing contraction
H p {, } seems perfectly fine.
development Horn package contraction analogous maxichoice Horn
contraction based weak remainders. Essentially, package contraction H p ,
ensure countermodel among models H p .
Definition 10 Let H Horn belief set, let = {1 , . . . , n } finite7 set Horn
formulas.
Hp set sets formulas H 0 Hp iff
every 1 n, mi that:
6` H mi |>| \ |i |; otherwise mi = LHC ;

H 0 = H ni=1 mi .
next definition, notion selection function H (Definition 6) extended
obvious fashion apply set Horn formulas.

7. Since assume underlying language finite, set formulas equivalent finite
set formulas, logical equivalence formulas.

492

fiHorn Clause Contraction Functions

Definition 11 Let selection function H (Hp ) = {H 0 }
H 0 Hp .
(maxichoice) package Horn contraction based weak remainders given by:
H p = (Hp )
=
6 H 6 Cnh (>); H otherwise.
following result relates elements Hp weak remainders.
Theorem 9 Let H Horn belief set let = {1 , . . . , n } set Horn formulas
1 n 6` .

H 0 Hp iff 1 n Hi H 0 = ni=1 Hi .
follows immediately maxichoice Horn contraction defines package
contraction, vice versa.
Corollary 1 Let p operator maxichoice Horn package contraction.
H = H p

= {}

operator maxichoice Horn contraction based weak remainders.
Corollary 2 Let operator maxichoice Horn contraction based weak remainders.
\
H p =
H


operator maxichoice Horn package contraction.
Example 7 Consider Horn belief set H = Cnh ({a, b}) P = {a, b, c}. want
determine elements
Hp = Cnh ({a, b})p {a, b}.
total 14 elements Hp 14 candidate package contractions.
candidates described follows:
1. 4 countermodels a, given by:
= {bc, b, c, }.
Thus four weak remainders corresponding countermodels,
four candidates maxichoice Horn contraction a.
2. Similarly 4 countermodels b:
B = {ac, a, c, }.
493

fiDelgrande & Wassermann

3. Members Hp given
Cl (M od(H) {x} {y})
x B.
example, x = bc, = , Cl (M od(H) {x} {y}) = {abc, ab, bc, b, },
set models (c b) (a b).
x = bc, = ac, Cl (M od(H) {x} {y}) = Cnh (>); holds
choice x y.
example indicates informally great deal choice respect
candidates package contraction. extent, combinatorial explosion
possibilities expected, given fact formula general large
number countermodels, compounded fact formula
package contraction associated countermodel. However, also
noted candidate package contractions contain redundancies, selected
countermodel may also countermodel b, case seems
reason allow possible incorporation separate countermodel b. Consequently,
also consider versions package contraction sense yield maximal belief
set. However, first provide results regarding package contraction.
following result:
Theorem 10 Let H Horn belief set. p operator maxichoice Horn
package contraction based weak remainders iff p satisfies following postulates:
(H p 1) H p belief set.

(closure)

(H p 2) , ` , 6 H p

(success)

(H p 3) H p H

(inclusion)

(H p 4) H p = H p (H )

(vacuity)

(H p 5) H p = H p ( \ Cnh (>))

(failure)

(H p 5b) H p = H

(triviality)

(H p 6) ,
H p ( {}) = H p ( {})

(extensionality)

(H p 7) H 6= H p
0 = ( \ Cnh (>)) H = {1 , . . . , n }
= {1 , . . . , n } 1 n,
{i , } ` H p Cnh (i )
H 0 s.t H p H 0 H, H 0 6 Cnh ()
494

(maximality)

fiHorn Clause Contraction Functions

exception last postulate, postulates clear reasonable:
usual, result package contraction belief set (H p 1). Moreover, non-tautology
set believed following contraction (H p 2), formulas added (H p 3).
Contracting formula originally H effect contraction (H p 4),
attempting contract tautology (H p 5). empty contraction unsurprisingly
effect (H p 5b). knowledge-level accounts, contraction independent
syntactic expression formulas contracted (H p 6). last postulate (H p 7)
corresponds maximality postulate contraction based weak remainders.
package contraction H p nontrivial nontautologies appear
H satisfy maximality condition formula contraction regular
Horn contraction based weak remainder sets. is, package contraction essentially
extends contraction set formulas. result expected, given Theorem 9
related elements Hp weak remainders.
discussed, characteristic maxichoice package contraction large
number members Hp , may logically quite weak. However proves
case eliminate candidates via pragmatic concerns.
package contraction H p belief set H 0 Hp that, informally, models H 0
contain countermodel along models H. general, interpretations countermodels one member ,
pragmatically, one
select minimal sets countermodels. Hence


case

(M od(>) \ od(i )) 6= ,

single countermodel, (M od(>) \ od(i )), would sufficient yield
package contraction.

Now, may (M od(>) \ od(i )) empty. simple example illustrates
case:
Example 8 Let H = Cnh (a b, b a) P = {a, b}. H p {a b, b a} =
Cnh (>). is, sole countermodel b {a} b {b}.
intersection closure interpretations H {ab, a, b, } = od(>).
Informally one get around simply selecting minimal set models
countermodel member set. considerations yield following
definition:
Definition 12 Let H Horn belief set, let = {1 , . . . , n } set Horn
formulas.
HS(), set (minimal) hitting sets interpretations respect , defined
by:
HS() iff
1. |>|
2. every 1 n 6` H, (|>| \ |i |) 6=
3. 0 S, 0 (|>| \ |i |) = 1 n.
Thus look sets sets interpretations; elements set interpretations represented maximal consistent sets formulas (Condition 1). well, set
495

fiDelgrande & Wassermann

contains countermodel member (Condition 2) moreover subsetminimal set satisfies conditions (Condition 3). Thus HS() corresponds
minimal set countermodels members . aside, noted
notion hitting set new general (Garey & Johnson, 1979) AI (Reiter,
1987).
Definition 13 Hph set sets formulas

H 0 Hph iff H 0 = H mS HS().
Definition 14 Let selection function H (Hph ) = {H 0 }
H 0 Hph .
Define:
H ph = (Hph )
=
6 H 6 Cnh (>); H otherwise.
following result follows straightforwardly.
Theorem 11 H ph operator maxichoice Horn package contraction.
Example 9 Consider case H = Cnh (a, b), P = {a, b, c}.
1. Let = {a, b}.
verified hitting sets given by:
{ {ac, bc}, {a, bc}, {ac, b}, {a, b}, {c}, {} }
corresponding elements Hph given by:
Hph = { Cnh (>),
Cnh (c a),
Cnh (c b),
Cnh (c a, c b),
Cnh (a b, b a),
Cnh (a b, b a, c a, c b) }.
Compare Example 7, 14 candidate package contractions.
2. Let = {a, b}. obtain
Hph = { Cnh (b),
Cnh (b (c a)),
Cnh (a b, b a),
Cnh (a b, b a, c a, c b) }.
496

fiHorn Clause Contraction Functions

set formulas satisfies Definition 13 clearly also satisfies Definition 11. One
restrict set candidate package contractions replacing 0 |S 0 | <
|S| third part Definition 12. case, package contraction Example 9,
Part 1 would yield two candidates Cnh (a b, b a) Cnh (a b, b a, c
a, c b). well, course, one could continue obvious fashion define notion
partial meet Horn package contraction. Given limited use operator, omit
details.
6.2 Forgetting Horn Formulas
section examines another means removing beliefs agents belief set,
forgetting (Lin & Reiter, 1994; Lang & Marquis, 2002). Forgetting operation
belief sets atoms language; result forgetting atom regarded
decreasing language atom.
addressing forgetting, easier work set Horn clauses, rather
Horn formulas. Since confusion, freely switch sets Horn
clauses corresponding Horn formula comprising conjunction clauses
set. Thus time set appears element formula, understood
standing conjunction members
V
set. Thus sets clauses S1
V
S2 , S1 S2 stand formula ( S1 ) ( S2 ). course, sets
guaranteed finitely representable, since language finite.
introduce following notation section, set Horn clauses,
> taken distinguished atom true interpretations.
{, >}, S[p/t] result uniformly substituting atom p every
S.
Sp = { | p occur }
Assume without loss generality Horn clause S, head () 6 body().
following definition adapts standard definition, attributed George Boole,
forgetting Horn clauses.
Definition 15 set Horn clauses atom p, define f orget(S, p) S[p/]
S[p/>].
immediately useful us, since disjunction generally Horn. However,
next result shows definition nonetheless leads Horn-definable forget operator.
Recall clauses c1 c2 , expressed sets literals p c1 p c2 ,
resolvent c1 c2 clause (c1 \ {p}) (c2 \ {p}). well, recall c1
c2 Horn, resolvent.
following, Res(S, p) set Horn clauses obtained carrying
possible resolutions respect p.
Definition 16 Let set Horn clauses p atom. Define
Res(S, p) = { | 1 , 2
p body(1 ) p = head (2 ),
= (body(1 ) \ {p} body(2 )) head (1 )}
497

fiDelgrande & Wassermann

Theorem 12 f orget(S, p) Sp Res(S, p).
Corollary 3 Let set Horn clauses p atom. f orget(S, p) equivalent
set Horn clauses.
Corollary 4 Let S1 S2 sets Horn clauses p atom. S1 S2 implies
f orget(S1 , p) f orget(S2 , p).
several points interest results. theorem expressed
terms arbitrary sets Horn clauses, deductively-closed Horn belief sets.
Hence second corollary states principle irrelevance syntax case forgetting belief bases. well, expression Sp Res(S, p) readily computable,
theorem fact provides means computing f orget. Further, approach clearly
iterates one atom. obtain additional result:8
Corollary 5
f orget(f orget(S, p), q) f orget(f orget(S, q), p).
Given this, define set atoms f orget(S, ) =
f orget(S, A) = f orget(f orget(S, a), \ {a})
A. hand, forgetting atom may result quadratic blowup
knowledge base.
Finally, might seem approach allows definition revision operator
procedure computing revision using something akin Levi Identity.
Let A() set atoms appearing (formula set formulas) . Then:
def

FRevise(S, ) = f orget(S, A()) + .
fact, yield revision operator, operator general far drastic
useful. see this, consider taxonomic knowledge base asserts whales
fish, whale f ish. course, whales mammals, using definition
repair knowledge base, one would first forget knowledge involving whales,
example, whales fins, breathe air, give live birth, on. example
doesnt prove reasonable revision operators definable via forget,
show nave approach problematic. Moreover, problems particular
Horn formulas, rather revision operator defined terms forgetting respect
underlying logic would similarly problematic.

7. Comparison among Constructions Horn Contraction
section provides technical summary differences various contraction
operations defined Horn belief sets:
Every e-remainder weak remainder, converse true.
8. fact, easy consequence definition forget.

498

fiHorn Clause Contraction Functions

clearly seen Figure 1. Horn theory H formula , e-remainders
maximal subsets H imply . weak remainders characterised
models H together single countermodel , closed intersection.
propositional logic notions would coincide; not. well, means
weak remainders partial meet distinct notions, latter corresponding
intersections weak remainders.
Similarly, obtain following:
Every e-remainder infra-remainder, converse true.
clear Definition 3, illustrated Example 1.
also have:
infra-remainders weak-remainders.
Looking Figure 1, see set Cnh ({c a, c b, b}) infraremainder weak remainder. however obtained intersection two
remainders.
Consider Example 3.2 presented Booth, Meyer Varzinczak (2009), H =
h
Cn ({p q, q r}) one wants contract p r: case, weak remainders
coincide remainders. set {p q r, p r q} infra-remainder
cannot obtained intersection weak-remainders. authors claim set
desirable result contraction, give strong motivation.
Last, have:
weak remainders infra-remainders.
Infra-remainders, definition, must contain full-meet contained remainder.
Weak remainders contained remainder (or remainder) always
contain full meet, seen table Figure 1. Full-meet example would
contain {c a, c b} two weak remainders (Cnh (a) Cnh (b))
contain formulas.
last two items show weak remainders infra-remainders independent
concepts relation studied detail. various relations
illustrated Figure 2.

weak remainders

eremainders

Figure 2
499

infra remainders

fiDelgrande & Wassermann

aforecited example Booth, Meyer, Varzinczak (2009) raises another point
deserves attention: H = Cnh ({p q, q r}), H e (p r) = (p
r) = {Cnh ({p q}), Cnh ({q r, p r q})}. asymmetry
possible obtain Cnh ({p q}) result contraction, e-remainders, weak
remainders infra-remainders allow Cnh ({q r}) possible outcome.
motivated study Horn belief base contraction (Delgrande & Wassermann, 2010),
one may obtain Cnh ({q r}), think may find interesting
alternatives.
Zhuang Pagnucco studied several forms Horn Contraction,
Epistemic Entrenchment Horn Contraction (EEHC) mentioned earlier (2010a), Transitively
Relational Partial-Meet Horn Contraction (TRPMHC) (2011) Model-based Horn Contraction (MHC) (2012). different operations compared Zhuang (2012). Partial
meet based weak remainders general EEHC MHC. However,
selection function required transitively relational, obtain TRPMHC,
equivalent MHC.

8. Conclusion
paper explored belief contraction, operators related belief contraction,
respect Horn theories. AGM approach two principal means
constructing contraction functions, via remainders maximal subsets belief set fail
imply formula, epistemic entrenchment, incorporates preference ordering
formulas. focus Horn contraction functions defined remainderlike constructions.
proves case basing contraction directly remainder sets, yielding
call e-remainders, problematic, resulting approach inexpressive
undesirable properties. also show alternative proposed, infra
remainders suffers problems. Based examination model-theoretic
considerations developed account maxichoice Horn contraction terms weak
remainder sets. idea models contraction Horn belief set H
Horn formula given models H together countermodel , closed
intersection (so yield Horn theory). provided representation results
maxichoice Horn contraction well partial meet contraction, compared
proposals literature.
also study two kinds operators giving beliefs Horn theories: package
contraction forgetting. former involves contracting set formulas,
formula set believed, Again, give construction postulate set, along
corresponding representation result. second operator, forgetting, thought
shrinking language discourse.
work interesting since Horn clauses found widespread use areas
logic programming, rule-based systems, deductive databases, description logics.
well, since Horn reasoning weaker classical reasoning, work sheds light
foundations belief change. natural topic future work consider Horn revision
operators study relation Horn contraction. second topic future work
consider belief change logics contain classical propositional logic.
500

fiHorn Clause Contraction Functions

Acknowledgments
first author partially supported Canadian NSERC Discovery Grant.
second author partially supported Brazilian National Research Council (CNPq),
grants 304043/2010-9 471666/2010-6. thank Tommie Meyer, Marcio
Ribeiro, David Makinson, anonymous reviewers helpful comments.

Appendix A. Proofs Main Results
Theorem 2: Let Horn contraction function defined via selection function
(2) based (infra) remainder sets.
H p inessential H, obtain (H ) + p ` .
Proof: Let = 1 n Horn clause. Horn conjunct ,
H e |= p . (To see this, note first Horn clause,
form body conjunction atoms body atom a. Since body H H
Horn belief set, also p body H. Since assumption p body, follows
p body remainder set H respect . Then, p body
logically equivalent p (body a), whence H e |= p .) Thus (H e ) {p} |=
(H e ) + p |= conjunct , (H e ) + p |= .
Theorem 3: Let P atom, let H Horn belief set H.
Let maxichoice Horn contraction function based remainder sets. every
atom b, least one b b H (a ) + a.
Proof: Suppose atom b, neither b b H (a ) + a,
H. Since atom, tautology, H,
construction, H (a ) element H e (a ). This, together
assumption b, b 6 H (a ), gives us (1) (H (a )) {b} ` (2)
(H (a )) {b } ` . (Results (1) (2) consequence fact
since H (a ) remainder set, maximal set fails imply .)
(1) (2) together, (H (a )) ` , contradicting success
postulate.
Lemma 1 Let set propositional formulas.
Cl (M od(T )) = od(Horn(Cn(T ))).
Proof:
Cl (M od(T )) least set models od(T )
Cl (M od(T )) Cl (M od(T )) = od(H) Horn theory H.
theory least upper Horn approximation h (Selman & Kautz, 1996), given

h = { | ` Horn prime implicate }.
Cnh (T h ) = Horn(Cn(T )) result follows.
Theorem 4: H Horn belief set Horn formula:
= H ||e .
Proof:
501

fiDelgrande & Wassermann

1. H ||e :
6 H ` = H ||e = {H}.
assume H 6` .
Let H 0 ; show H 0 H ||e .
Since H 0 , definition H 0 = H |>|\||, od(H 0 ) =
od(H m). H Horn theories, thus H Horn theory.
Using fact Horn belief set , = Horn(Cn(T )), H =
Horn(Cn(H m)) od(H 0 ) = od(Horn(Cn(H m)).
Applying Lemma 1 Hm obtain od(Horn(Cn(Hm)) = Cl (M od(Cn(H
m))). Now, Cl (M od(Cn(Hm))) = Cl (M od(Hm)) = Cl ((M od(H)M od(m))).
definition maximal consistent Horn theory, od(>)
od(m) = {M }. Putting together get od(H 0 ) =
Cl ((M od(H) )), is, H 0 H ||e .
2. H ||e :
part follows immediately essentially taking preceding part reverse order.


Lemma 2 Maximality (H w 7) equivalent following property, call
(H w 70 ):
H 6= H w |>| \ || s.t. H w H 0 s.t. H w H 0 H
H 0 6 m.
Proof: straightforward show property implies (H w 7): Let
conjunction literals appearing m. language finite, well-defined formula.
Cnh () = m, thus (H w 7) holds.
direction, assume (H w 7) holds.
Claim: given H , satisfies conditions (H w 7) p P,
either p (p ) also satisfies conditions (H w 7).
Proof Claim: Clearly, {, } inconsistent {, l} l
{p, p }; H Cnh () H 0 Cnh ( l) l {p, p }.
need show Horn theory H 0 H H 0 H, either
H 0 6 Cnh ( p) H 0 6 Cnh ( (p )).
Towards contradiction, assume otherwise. H 0 Cnh ( p) H 0
Cnh ( (p )) H 0 Cnh ( p) Cnh ( (p )). Cnh () =
Cnh ( p) Cnh ( p ), consequently H 0 Cnh (). contradicts
satisfies (H w 7) H .
Hence assumption incorrect, H 0 6 Cnh ( p) H 0 6 Cnh (
(p )).
502

fiHorn Clause Contraction Functions

shown satisfies (H w 7) given H , one
p (p ). induction (the finite set) P establishes satisfies
(H w 7) given H , 0 0 ` p 0 ` (p ) every
p P. Hence 0 Cnh ( 0 ) |>| \ ||, thus taking = Cnh ( 0 ) satisfies
property.
Theorem 5: Let H Horn belief set. w operator maxichoice Horn
contraction based weak remainders iff w satisfies following postulates.
(H w 1) H w Horn belief set.

(closure)

(H w 2) ` , 6 H w .

(success)

(H w 3) H w H.

(inclusion)

(H w 4) 6 H, H w = H.

(vacuity)

(H w 5) ` H w = H

(failure)

(H w 6) , H w = H w .

(extensionality)

(H w 7) H 6= H w LHC {, } ` , H w Cnh () H 0
s.t H w H 0 H H 0
6 Cnh ().
(maximality)
Proof:
1. Construction Postulates:
construction satisfies first five postulates follows directly definitions weak remainders selection functions. see satisfies (H w 6)
note implies = since function,
H w = H w .
see construction satisfies (H w 7), suppose H 6= H w . means
6= hence, |>| \ || H w = H m. Let
conjunction literals appearing m. Then, since Cnh () = m,
{, } inconsistent, H w Cnh () H 0 s.t H w H 0 H
H 0 6 Cnh ().
2. Postulates Construction:
proof uses (H w 70 ) rather (H w 7), shown equivalent
Lemma 2.
Let w operator satisfies Cn(H w 1) (H w 70 ).
Let defined (He ) = {H w }.
show function:
503

fiDelgrande & Wassermann

Assume = ; need show (He ) = (He ).
6 H, = {H} since = ,
= H, hence 6 H ` . Then, (H w 4) (H w 5),
H w = H w = H definition (He ) = (He ).
let us consider case , H. Since =
{H | |>| \ ||} = {H | |>| \ ||}.
follows |>| \ || = |>| \ ||. see this, suppose {H |
|>| \ ||} = {H | |>| \ ||} |>| \ || =
6 |>| \ ||. Without loss
generality, suppose m0 |>| \ || m0 6 |>| \ ||.
m0 maximal consistent theory contains . Since H, know
H m0 . means H m0 {H | |>| \ ||},
H m0 6 {H | |>| \ ||}, |>| \ || definition
6 m. contradicts initial hypothesis.
Since |>| \ || = |>| \ || get || = || . (H w 6)
H w = H w , (He ) = (He ).
6 H, (H w 4) H w = H. Similarly, ` ,
(H w 5) H w = H.
Consequently assume H ` . need show H w ,
is, H w = H |>| \ ||.
Since ` , (H w 2) 6 H w ; since H
H 6= H w .
Since H 6= H w , (H w 70 ) get |>| \ || H w
m.
well, (H w 3) gives H w H, H w implies H w
(m H).
need show H w = (m H). Towards contradiction assume
H w 6= (m H), say, H w (m H).
Let (m H) \ (H w ).
H w Cnh (H w {}) H H.
But, substituting Cnh (H w {}) H 0 (H w 70 ) get Cnh (H w
{}) 6 m, contradiction.
Hence assumption H w 6= (m H) incorrect; hence H w = (m H)
(m H) , shown.
Theorem 6: Let H Horn belief set. pm operator partial meet Horn
contraction based weak remainders iff pm satisfies postulates (H w 1) (H w 6)
and:
(H pm 7) H \(H pm ), H 0 H pm H 0 , 6 Cnh (H 0 )
Cnh (H 0 {})
(weak relevance)
504

fiHorn Clause Contraction Functions

Proof:
1. Construction Postulates:
(H w 1) follows fact intersection Horn theories Horn theory. Postulates (H w 2) (H w 6) follow immediately definitions weak
remainder, selection function partial meet contraction.
see construction satisfies weak relevance, note H \ H ,
X (He ) 6 X. Since H,
|>| \ || 6 X = H m. Take H 0 = m. H H 0 ,
6 Cnh (H 0 ) Cnh (H 0 {}) = Cnh ().
2. Postulates Construction:
Let (He ) = {X | H pm X} 6= (He ) = {H}
otherwise.
show that: (1) function; (2) selection function;
(3) (He ) = H .

6 H, (H w 4), H pm = H = (He ). Assume H.
(1) Let 1 = 2 . must show (He 1 ) = (He 2 ).
proof maxichoice contraction, 1 = 2 implies 1 2 then,
Postulate (H w 6), H1 = H2 . construction , (He 1 ) = (He 2 ).
(2) construction , know (He ) . show
6= , (He ) 6= , otherwise (He ) = {H}.
(i) 6= , H 6= |>| \ || =
6 . (H w 1) (H w 2),
6 Cn(H ). |>| \ || H m. (H w 3),
H H, hence, H H (H).
(ii) = , ` (H w 5), H = H.


(3) know thatTH (He ). Suppose (He )
6 H . Since (He ) H, H \ (H ) weak relevance know
H 0 H H 0 , 6 Cnh (H 0 ) Cnh (H 0 {}).
|>| \ || H 0 6 m. Take X = H m.
X (H w 3) H X hence, X (He ).
6 X, leads contradiction.
Theorem 7: Partial meet based weak remainders transitive relational selection
function satisfies (K 7) (K 8).
Proof:
Let selection function based transitive relation .
Since |>| \ | | = (|>| \ ||) (|>| \ ||) hence, = ,
order show PMWR satisfies postulate (K-7), suffices show
(*) (He ) (He ) (He ).9
9. called Choice-distributivity literature.

505

fiDelgrande & Wassermann

Take X (He ). know X X . Suppose
X , show X (He ). X 6 (He ), X 0
X X 0 . X 0 X 6 (He ). case
X analogous, thus X (He ) X (He ), proves (*).
order show PMWR satisfies postulate (K-8), let 6 H .
show
(**) (He ) (He )
6 H know (He ) contains least one element .
Since based , (He ) (He ).
Theorem 8: PMWR satisfies (H 5).
Proof:
see PMWR satisfies (H 5), first note .
H , know X every X (He ). show
X every X (He ). Let X (He ). X 6 ,
X = H maximal, consistent Horn theory contain
contains . Hence, X. Otherwise, i.e., X , show
X (He ). Suppose X 6 (He ), X 0
X 0 < X. X 0 X cannot element (He ).
Hence, every X (He ), know X therefore, H .

Theorem 9: Let H Horn belief set let = {1 , . . . , n } set Horn formulas
1 n 6` .

H 0 Hp iff 1 n Hi H 0 = ni=1 Hi .
Proof: Let H Horn belief set = {1 , . . . , n } LHC .
= Let H 0 Hp .
Definition 10 m1 , . . . , mn H 0 =

Tn

i=1 (H

mi )

1. H 6` mi |>| \ |i |;
2. otherwise mi = LHC .
i, 1 n, above,
1. H 6` Definition 4, Hi = H mi satisfies conditions
Hi ;
2. otherwise mi = LHC Hi = H mi = H LHC = H satisfies
conditions Hi , Definition 4.
= Consider let Hi .
Definition 4
1. H 6` Hi = H |>| \ |i |;
506

fiHorn Clause Contraction Functions

2. 6 H ` Hi = H equivalently Hi = H mi mi = LHC .

Consequently i, 1 n, above, H 0 = ni=1 Hi satisfies conditions
H 0 Hp Definition 10.
Theorem 10: Let H Horn belief set. p operator maxichoice Horn
package contraction based weak remainders iff p satisfies following postulates:
(H p 1) H p belief set.

(closure)

(H p 2) , ` , 6 H p

(success)

(H p 3) H p H

(inclusion)

(H p 4) H p = H p (H )

(vacuity)

(H p 5) H p = H p ( \ Cnh (>))

(failure)

(H p 5b) H p = H

(triviality)

(H p 6) ,
H p ( {}) = H p ( {})

(extensionality)

(H p 7) H 6= H p
0 = ( \ Cnh (>)) H = {1 , . . . , n }
= {1 , . . . , n } 1 n,
{i , } ` H p Cnh (i )
H 0 s.t H p H 0 H, H 0 6 Cnh ()

(maximality)

Proof:
1. Construction Postulates:
(H p 1) obvious.
(H p 2), H, Definition 10 ensures H 0 Hp H 0 6`
6 H 0 .
(H p 3) H 0 Hp implies H 0 form H X; consequently H 0 H.
(H p 4) (H p 5) direct consequence special cases Definition 10
6 H ` respectively.
(H p 5b) vacuously satisfied Definition 10, (H p 6), form
H 0 Hp easily seen independent syntactic form members .
(H p 7), let X Hp = {1 , . . . , n }. appeal (H p 4)
(H p 5) assume without loss generality implies 6`
H. Let m1 , . . . , mn specified Definition 10. m1 , . . . , mn satisfy
507

fiDelgrande & Wassermann

conditions
1 , . . . , n (H p 7): Since mi |>| \ |i |, {i , } ` . Since
X = H ni=1 mi , X Cnh (mi ) = mi . Last, need show belief
0
h
set H 0 X H 0 H, mi list,
Tn H 6 Cn (mi ) = mi .
direct consequence fact X = H i=1 mi .
2. Postulates Construction:
Let p satisfy Postulates (H p 1)(H p 7), let H Horn belief set
LHC . Let specified (H p 7) , define H by:
(a) ` 6 H H = H.
(b) Otherwise corresponding , H maximum set
formulas H p H H H Cnh ().
Using Theorem 5, easily shown operator maxichoice Horn contraction.
implies selection function H = (He ) every
.

Therefore, Theorem 9 H p = H = H 0
H 0 Hp .
Theorem 12: f orget(S, p) Sp Res(S, p).
Proof: Let finite set nontautological Horn clauses. p P, define:
Sh = {c | p = head(c)}
Sb = {c | p body(c)}
well, already defined: Sp = {c | p occur c}.
obtain:
f orget(S, p) S[p/] S[p/>]
(Sh [p/] Sb [p/] Sp [p/])
(Sh [p/>] Sb [p/>] Sp [p/>])
(Sh [p/] {>} Sp ) ({>} Sb [p/>] Sp )
(Sh [p/] Sp ) (Sb [p/>] Sp )
Sp (Sh [p/] Sb [p/>])
Sp {c1 c2 | c1 Sh [p/] c2 Sb [p/>]}
Sp Res(S, p)

References
Alchourron, C. E., & Makinson, D. (1985). logic theory change: Safe contraction.
Studia Logica, 44 (4), 405422.
508

fiHorn Clause Contraction Functions

Alchourron, C., Gardenfors, P., & Makinson, D. (1985). logic theory change:
Partial meet contraction revision functions. Journal Symbolic Logic, 50 (2),
510530.
Anderson, A., & Belnap Jr., N. (1975). Entailment: Logic Relevance Necessity,
Vol. I. Princeton University Press.
Baader, F., Calvanese, D., McGuiness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2007).
Description Logic Handbook (second edition). Cambridge University Press.
Booth, R., Meyer, T., Varzinczak, I., & Wassermann, R. (2011). Link Partial
Meet, Kernel, Infra Contraction Application Horn Logic. Journal
Artificial Intelligence Research, 42, 3153.
Booth, R., Meyer, T., & Varzinczak, I. (2009). Next steps propositional Horn contraction.
Proceedings International Joint Conference Artificial Intelligence, pp.
702707, Pasadena, CA.
Creignou, N., Papini, O., Pichler, R., & Woltran, S. (2012). Belief revision within fragments
propositional logic. Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), Proceedings
Thirteenth International Conference Principles Knowledge Representation Reasoning. AAAI Press.
Delgrande, J., & Wassermann, R. (2010). Horn clause contraction functions: Belief set
belief base approaches. Lin, F., & Sattler, U. (Eds.), Proceedings Twelfth International Conference Principles Knowledge Representation Reasoning,
pp. 143152, Toronto. AAAI Press.
Delgrande, J. (2008). Horn clause belief change: Contraction functions. Brewka, G., &
Lang, J. (Eds.), Proceedings Eleventh International Conference Principles
Knowledge Representation Reasoning, pp. 156165, Sydney, Australia. AAAI
Press.
Delgrande, J., & Peppas, P. (2011). Revising Horn Theories. Twenty-Second International
Joint Conference Artificial Intelligence, pp. 839844.
Delgrande, J., & Wassermann, R. (2011). Topics Horn contraction: Supplementary postulates, package contraction, forgetting. IJCAI-11 Workshop Nonmonotonic
Reasoning, Action Change (NRAC-11), pp. 8794, Barcelona, Spain.
Eiter, T., & Gottlob, G. (1992). complexity propositional knowledge base revision,
updates, counterfactuals. Artificial Intelligence, 57 (2-3), 227270.
Flouris, G., Plexousakis, D., & Antoniou, G. (2004). Generalizing AGM postulates: Preliminary results applications. Proceedings 10th International Workshop
Non-Monotonic Reasoning (NMR-04), pp. 171179, Whistler BC, Canada.
Gardenfors, P. (1988). Knowledge Flux: Modelling Dynamics Epistemic States.
MIT Press, Cambridge, MA.
Gardenfors, P., & Makinson, D. (1988). Revisions knowledge systems using epistemic
entrenchment. Proc. Second Theoretical Aspects Reasoning Knowledge
Conference, pp. 8395, Monterey, Ca.
509

fiDelgrande & Wassermann

Garey, M., & Johnson, D. (1979). Computers Intractability: Guide Theory
NP-Completeness. W.H. Freeman Co., New York.
Grove, A. (1988). Two Modellings Theory Change. Journal Philosophical Logic, 17,
157170.
Hansson, S. O. (1999). Textbook Belief Dynamics. Applied Logic Series. Kluwer
Academic Publishers.
Khardon, R. (1995). Translating Horn representations characteristic
models. Journal Artificial Intelligence Research, 3, 349372.
Lakemeyer, G., & Levesque, H. (2000). Logic Knowledge Bases. MIT Press, Cambridge, MA.
Lang, J., & Marquis, P. (2002). Resolving inconsistencies variable forgetting. Proceedings Eighth International Conference Principles Knowledge Representation Reasoning, pp. 239250, San Francisco. Morgan Kaufmann.
Langlois, M., Sloan, R., Szorenyi, B., & Turan, G. (2008). Horn complements: Towards
Horn-to-Horn belief revision. Proceedings AAAI National Conference
Artificial Intelligence, Chicago, Il.
Liberatore, P. (2000). Compilability compact representations revision Horn knowledge bases. ACM Transactions Computational Logic, 1 (1), 131161.
Lin, F., & Reiter, R. (1994). Forget it!. AAAI Fall Symposium Relevance, New
Orleans.
Makinson, D. (2009) Personal communication.
Peppas, P. (2008). Belief revision. van Harmelen, F., Lifschitz, V., & Porter, B. (Eds.),
Handbook Knowledge Representation, pp. 317359. Elsevier Science, San Diego,
USA.
Reiter, R. (1987). theory diagnosis first principles. Artificial Intelligence, 32 (1),
5796.
Rott, H. (1992). logic theory change: maps different kinds
contraction functions. Gardenfors, P. (Ed.), Belief Revision, No. 29 Cambridge
Tracts Theoretical Computer Science, pp. 122141. Cambridge University Press.
Selman, B., & Kautz, H. (1996). Knowledge compilation theory approximation. Journal
ACM, 43 (2), 193224.
Zhuang, Z., & Pagnucco, M. (2010a). Horn contraction via epistemic entrenchment.
Janhunen, T., & Niemela, I. (Eds.), Logics Artificial Intelligence - 12th European
Conference (JELIA 2010), Vol. 6341 Lecture Notes Artificial Intelligence, pp.
339351. Springer Verlag.
Zhuang, Z., & Pagnucco, M. (2010b). Two methods constructing Horn contractions.
Li, J. (Ed.), AI 2010: Advances Artificial Intelligence - 23rd Australasian Joint
Conference, Vol. 6464 Lecture Notes Artificial Intelligence, pp. 7281. Springer
Verlag.
510

fiHorn Clause Contraction Functions

Zhuang, Z. (2012). Belief Change Horn Fragment Propositional Logic. Ph.D.
thesis, School Computer Science Engineering University New South Wales.
Zhuang, Z., & Pagnucco, M. (2011). Transitively relational partial meet Horn contraction.
Proceedings Twenty-Second International Joint Conference Artificial Intelligence, pp. 11321138, Barcelona, Spain.
Zhuang, Z., & Pagnucco, M. (2012). Model based Horn contraction. Proceedings
Thirteenth International Conference Principles Knowledge Representation
Reasoning, Rome, Italy.

511

fiJournal Artificial Intelligence Research 48 (2013) 671-715

Submitted 04/13; published 11/13

Generating Natural Language Descriptions OWL
Ontologies: NaturalOWL System
Ion Androutsopoulos

ion@aueb.gr

Department Informatics,
Athens University Economics Business, Greece
Digital Curation Unit Institute Management Information Systems,
Research Centre Athena, Athens, Greece

Gerasimos Lampouras

lampouras06@aueb.gr

Department Informatics,
Athens University Economics Business, Greece

Dimitrios Galanis

galanisd@aueb.gr

Department Informatics,
Athens University Economics Business, Greece
Institute Language Speech Processing,
Research Centre Athena, Athens, Greece

Abstract
present Naturalowl, natural language generation system produces texts
describing individuals classes owl ontologies. Unlike simpler owl verbalizers,
typically express single axiom time controlled, often entirely fluent natural
language primarily benefit domain experts, aim generate fluent coherent multi-sentence texts end-users. system like Naturalowl, one publish
information owl Web, along automatically produced corresponding texts
multiple languages, making information accessible computer programs
domain experts, also end-users. discuss processing stages Naturalowl,
optional domain-dependent linguistic resources system use stage,
useful. also present trials showing domain-dependent
linguistic resources available, Naturalowl produces significantly better texts compared
simpler verbalizer, resources created relatively light effort.

1. Introduction
Ontologies play central role Semantic Web (Berners-Lee, Hendler, & Lassila, 2001;
Shadbolt, Berners-Lee, & Hall, 2006). ontology provides conceptualization
knowledge domain (e.g., consumer electronics) defining classes subclasses
individuals (entities) domain, types possible relations etc.
current standard specify Semantic Web ontologies owl (Horrocks, Patel-Schneider,
& van Harmelen, 2003), formal language based description logics (Baader, Calvanese,
McGuinness, Nardi, & Patel-Schneider, 2002), rdf, rdf schema (Antoniou & van
Harmelen, 2008), owl2 latest version owl (Grau, Horrocks, Motik, Parc
2013
AI Access Foundation. rights reserved.

fiAndroutsopoulos, Lampouras, & Galanis

sia, Patel-Schneider, & Sattler, 2008). Given owl ontology knowledge domain, one
publish Web machine-readable data pertaining domain (e.g., catalogues
products, features etc.), data formally defined semantics based
conceptualization ontology.1 Following common practice Semantic Web research,
actually use term ontology refer jointly terminological knowledge (TBox)
establishes conceptualization knowledge domain, assertional knowledge (ABox)
describes particular individuals.
Several equivalent owl syntaxes developed, people unfamiliar formal
knowledge representation often difficulties understanding (Rector, Drummond,
Horridge, Rogers, Knublauch, Stevens, Wang, & Wroe, 2004). example, following
statement defines class St. Emilion wines, using functional-style syntax owl,
one easiest understand, also adopt throughout article.2
EquivalentClasses(:StEmilion
ObjectIntersectionOf(:Bordeaux
ObjectHasValue(:locatedIn :stEmilionRegion) ObjectHasValue(:hasColor :red)
ObjectHasValue(:hasFlavor :strong)
ObjectHasValue(:madeFrom :cabernetSauvignonGrape)
ObjectMaxCardinality(1 :madeFrom)))

make ontologies easier understand, several ontology verbalizers developed
(Schwitter, 2010a). Verbalizers usually translate axioms (in case, owl statements)
ontology one one controlled, often entirely fluent English statements, typically
without considering coherence resulting texts, mostly benefit domain
experts. contrast, article present system aims produce fluent
coherent multi-sentence texts describing classes individuals owl ontologies,
texts intended read end-users (e.g., customers on-line retail sites). example,
system generate following text owl statement above, ontology
annotated domain-dependent linguistic resources discussed below.
St. Emilion kind Bordeaux St. Emilion region. red color strong flavor.
made exactly one grape variety: Cabernet Sauvignon grapes.

system, called Naturalowl, open-source supports English Greek.
Hence, Greek texts also generated owl statements, following
product description, provided appropriate Greek linguistic resources also available.
contrast, owl verbalizers typically produce English (or English-like) sentences.
ClassAssertion(:Laptop :tecraA8)
ObjectPropertyAssertion(:manufacturedBy :tecraA8 :toshiba)
ObjectPropertyAssertion(:hasProcessor :tecraA8 :intelCore2)
DataPropertyAssertion(:hasMemoryInGB :tecraA8 "2"^^xsd:nonNegativeInteger)
DataPropertyAssertion(:hasHardDiskInGB :tecraA8 "110"^^xsd:nonNegativeInteger)
DataPropertyAssertion(:hasSpeedInGHz :tecraA8 "2"^^xsd:float)
DataPropertyAssertion(:hasPriceInEuro :tecraA8 "850"^^xsd:nonNegativeInteger)

[English description:] Tecra A8 laptop, manufactured Toshiba. Intel Core 2 processor,
2 gb ram 110 gb hard disk. speed 2 ghz costs 850 Euro.
1. See http://owl.cs.manchester.ac.uk/repository/ repository owl ontologies.
2. Consult http://www.w3.org/TR/owl2-primer/ introduction functional-style syntax owl.

672

fiGenerating Natural Language Descriptions OWL Ontologies

[Greek description:] Tecra A8 , Toshiba.
Intel Core 2, 2 gb ram 110 gb. 2 ghz
850 .

examples illustrate system like Naturalowl help publish information Web owl statements texts generated owl statements.
way, information becomes easily accessible computers, process
owl statements, end-users speaking different languages; changes owl
statements automatically reflected texts regenerating them. produce
fluent, coherent multi-sentence texts, Naturalowl relies natural language generation
(nlg) methods (McKeown, 1985; Reiter & Dale, 2000) larger extent compared existing owl verbalizers; example, includes mechanisms avoid repeating information,
order facts expressed, aggregate smaller sentences longer ones, generate
referring expressions etc. Although nlg established area, first article
discuss detail nlg system owl ontologies, excluding simpler verbalizers.
propose novel algorithms theoretical nlg perspective, show
several particular issues need considered generating owl ontologies.
example, owl statements lead overly complicated sentences, unless
converted simpler intermediate representations first; also several owl-specific
opportunities aggregate sentences (e.g., expressing axioms cardinalities
properties); referring expression generation exploit class hierarchy.
Naturalowl used owl ontology, obtain texts high quality
domain-dependent generation resources required; example, classes ontology
mapped natural language names, properties sentence plans etc. Similar
linguistic resources used nlg systems, though different systems adopt different
linguistic theories algorithms, requiring different resources. little consensus
exactly information nlg resources capture, apart abstract specifications
(Mellish, 2010). domain-dependent generation resources Naturalowl created
domain author, person familiar owl, system configured new
ontology. domain author uses Protege ontology editor Protege plug-in
allows editing domain-dependent generation resources invoking Naturalowl view
resulting texts.3 discuss plug-in article, since similar
authoring tool m-piro (Androutsopoulos, Oberlander, & Karkaletsis, 2007).
owl ontologies often use English words concatenations words (e.g., manufacturedBy)
identifiers classes, properties, individuals. Hence, domain-dependent
generation resources often extracted ontology guessing, example,
class identifier like Laptop earlier example noun used refer
class, statement form ObjectPropertyAssertion(:manufacturedBy X
) expressed English sentence form X manufactured .
owl verbalizers follow strategy. Similarly, domain-dependent generation resources provided, Naturalowl attempts extract ontology, uses
3. Consult http://protege.stanford.edu/ information Protege. Naturalowl Protege plugin freely available http://nlp.cs.aueb.gr/software.html. describe Naturalowl version 2
article; version 1 (Galanis & Androutsopoulos, 2007) used less principled representation
domain-dependent generation resources, without supporting owl2.

673

fiAndroutsopoulos, Lampouras, & Galanis

generic resources. resulting texts, however, lower quality; also, non-English texts
cannot generated, identifiers ontology English-like. tradeoff
reducing effort construct domain-dependent generation resources owl
ontologies, obtaining higher-quality texts multiple languages, tradeoff
investigated previous work. present trials performed measure
effort required construct domain-dependent generation resources Naturalowl
extent improve resulting texts, also comparing simpler
verbalizer requires domain-dependent generation resources. trials show
domain-dependent generation resources help Naturalowl produce significantly better
texts, resources constructed relatively light effort, compared
effort typically needed construct ontology.
Overall, main contributions article are: (i) first detailed discussion
complete, general-purpose nlg system owl ontologies particular issues
arise generating owl ontologies; (ii) shows system relies nlg
methods larger extent, compared simpler owl verbalizers, produce significantly
better natural language descriptions classes individuals, provided appropriate
domain-dependent generation resources available; (iii) shows descriptions
generated one languages, provided appropriate resources
available; (iv) shows domain-dependent generation resources constructed
relatively light effort. already noted, article present novel algorithms
theoretical nlg perspective. fact, algorithms Naturalowl uses
narrower scope, compared fully-fledged nlg algorithms. Nevertheless,
trials show system produces texts reasonable quality, especially domaindependent generation resources provided. hope Naturalowl contributes
towards wider adoption nlg methods Semantic Web, researchers may
wish contribute improved components, given Naturalowl open-source.
Naturalowl based ideas ilex (ODonnell, Mellish, Oberlander, & Knott,
2001) m-piro (Isard, Oberlander, Androutsopoulos, & Matheson, 2003). ilex
project developed nlg system demonstrated mostly museum exhibits,
support owl.4 m-piro project produced multilingual extension system
ilex, tested several domains (Androutsopoulos et al., 2007). Attempts
use generator m-piro owl, however, ran problems (Androutsopoulos,
Kallonis, & Karkaletsis, 2005). contrast, Naturalowl especially developed owl.
remainder article, assume reader familiar rdf, rdf
schema, owl. Readers unfamiliar Semantic Web may wish consult
introductory text first (Antoniou & van Harmelen, 2008).5 also note recently
popular Linked Data published interconnected using Semantic Web technologies.6 Linked Data currently use rdf rdf schema, owl effect
superset rdf schema and, hence, work paper also applies Linked Data.
4. Dale et al. (1998) Dannels (2008, 2012) also discuss nlg museums.
5. longer version article, background readers unfamiliar owl
Semantic Web, available technical report (Androutsopoulos, Lampouras, & Galanis, 2012); see
http://nlp.cs.aueb.gr/publications.html.
6. Consult http://linkeddata.org/. See also work Duma Klein (2013).

674

fiGenerating Natural Language Descriptions OWL Ontologies

Section 2 briefly discusses related work; provide pointers
related work subsequent sections. Section 3 explains Naturalowl generates
texts, also discussing domain-dependent generation resources processing stage.
Section 4 describes trials performed measure effort required construct
domain-dependent generation resources impact quality generated
texts. Section 5 concludes proposes future work.

2. Related Work
use functional-style syntax owl article, several equivalent owl syntaxes exist. also work develop controlled natural languages (cnls), mostly
English-like, used alternative owl syntaxes. Sydney owl Syntax (sos) (Cregan,
Schwitter, & Meyer, 2007) English-like cnl bidirectional mapping
functional-style syntax owl; sos based peng (Schwitter & Tilbrook, 2004).
similar bidirectional mapping defined Attempto Controlled English (ace)
(Kaljurand, 2007). Rabbit (Denaux, Dimitrova, Cohn, Dolbear, & Hart, 2010) clone
(Funk, Tablan, Bontcheva, Cunningham, Davis, & Handschuh, 2007) owl cnls,
mostly intended used domain experts authoring ontologies (Denaux, Dolbear,
Hart, Dimitrova, & Cohn, 2011). also note owl cnls cannot express
kinds owl statements (Schwitter, Kaljurand, Cregan, Dolbear, & Hart, 2008).
Much work owl cnls focuses ontology authoring querying (Bernardi, Calvanese, & Thorne, 2007; Kaufmann & Bernstein, 2010; Schwitter, 2010b); emphasis
mostly direction cnl owl query languages.7 relevant work
cnls like sos ace, automatic mappings normative owl syntaxes
available. feeding owl ontology expressed, example, functional-style syntax
mapping translates English-like cnl, axioms ontology
turned English-like sentences. Systems kind often called ontology verbalizers.
term, however, also includes systems translate owl English-like statements belong explicitly defined cnl (Halaschek-Wiener, Golbeck, Parsia,
Kolovski, & Hendler, 2008; Schutte, 2009; Power & Third, 2010; Power, 2010; Stevens,
Malone, Williams, Power, & Third, 2011; Liang, Stevens, Scott, & Rector, 2011b).
Although verbalizers viewed performing kind light nlg, typically
translate axioms one one, already noted, without considering coherence (or topical cohesion) resulting texts, usually without aggregating sentences generating
referring expressions, often producing sentences entirely fluent natural. example, ace sos occasionally use variables instead referring expressions
(Schwitter et al., 2008). Also, verbalizers typically employ domain-dependent generation resources typically support multiple languages. Expressing exact
meaning axioms ontology unambiguous manner considered important verbalizers composing fluent coherent text multiple languages,
partly verbalizers typically intended used domain experts.
7. Conceptual authoring wysiwym (Power & Scott, 1998; Hallett, Scott, & Power, 2007),
applied owl (Power, 2009), round-trip authoring (Davis, Iqbal, Funk, Tablan, Bontcheva, Cunningham, & Handschuh, 2008) bidirectional, focus mostly ontology authoring querying.

675

fiAndroutsopoulos, Lampouras, & Galanis

Figure 1: processing stages sub-stages Naturalowl.
verbalizers use ideas methods nlg. example, verbalizers include
sentence aggregation (Williams & Power, 2010) text planning (Liang, Scott, Stevens, &
Rector, 2011a). Overall, however, nlg methods used limited extent
owl ontologies. notable exception ontosum (Bontcheva, 2005), generates
natural language descriptions individuals, apparently classes, rdf schema
owl ontologies. extension miakt (Bontcheva & Wilks, 2004),
used generate medical reports. implemented gate (Bontcheva, Tablan,
Maynard, & Cunningham, 2004) provide graphical user interfaces manipulate
domain-dependent generation resources (Bontcheva & Cunningham, 2003). detailed
description ontosum appears published, however, system
seem publicly available, unlike Naturalowl. Also, trials ontosum
independently created ontologies seem published. information
ontosum compares Naturalowl found elsewhere (Androutsopoulos et al., 2012).
Mellish Sun (2006) focus lexicalization sentence aggregation, aiming
produce single aggregated sentence input collection rdf triples; contrast,
Naturalowl produces multi-sentence texts. complementary work, Mellish et al. (2008)
consider content selection texts describing owl classes. Unlike Naturalowl, system
express facts explicit ontology, also facts deduced
ontology. Nguyen et al. (2012) discuss proof trees facts deduced owl
ontologies explained natural language. would particularly interesting
examine deduction explanation mechanisms could added Naturalowl.

3. Processing Stages Resources NaturalOWL
Naturalowl adopts pipeline architecture, common nlg (Reiter & Dale, 2000),
though number purpose components often vary (Mellish, Scott, Cahill, Paiva,
Evans, & Reape, 2006). system generates texts three stages, document planning,
micro-planning, surface realization, discussed following sections; see Figure 1.
3.1 Document Planning
Document planning consists content selection, system selects information
convey, text planning, plans structure text generated.
3.1.1 Content Selection
content selection, system first retrieves ontology owl statements
relevant class individual described, converts selected
676

fiGenerating Natural Language Descriptions OWL Ontologies

owl statements message triples, easier express sentences, finally
selects among message triples ones expressed.
OWL statements individual targets
Let us first consider content selection Naturalowl asked describe individual
(an entity), let us call individual target. system scans owl statements
ontology, looking statements forms listed left column Table 1.8
effect, retrieves statements describe target directly, opposed
statements describing another individual (named) class target related to.
owl allows arbitrarily many nested ObjectUnionOf ObjectIntersectionOf operators, may lead statements difficult express natural language.
simplify text generation ensure resulting texts easy comprehend,
allow nested ObjectIntersectionOf ObjectUnionOf operators ontologies texts generated from. Table 1, restriction enforced requiring class
identifiers appear points owl also allows expressions construct unnamed classes using operators. ontology uses unnamed classes points Table 1
requires class identifiers (named classes), easily modified comply Table 1
defining new named classes nested unnamed ones.9 practice, nested ObjectUnionOf
ObjectIntersectionOf operators rare; see work Power et al. (Power, 2010;
Power & Third, 2010; Power, 2012) information frequencies different types
owl statements.10
Statements form ClassAssertion(Class target ) may quite complex,
Class necessarily class identifier. may also expression constructing
unnamed class, following example. multiple rows
ClassAssertion Table 1.
ClassAssertion(
ObjectIntersectionOf(:Wine
ObjectHasValue(:locatedIn :stEmilionRegion)
ObjectHasValue(:hasColor :red)
ObjectHasValue(:madeFrom :cabernetSauvignonGrape)
:chateauTeyssier2007)

ObjectHasValue(:hasFlavor :strong)
ObjectMaxCardinality(1 :madeFrom))

Naturalowl would express owl statement generating text like following.
2007 Chateau Teyssier wine St. Emilion region. red color strong flavor.
made exactly one grape variety: Cabernet Sauvignon grapes.

Recall texts Naturalowl intended read end-users. Hence,
prefer generate texts may emphasize enough subtleties owl
8. owl statements shown Table 1 two arguments actually arguments,
converted forms shown.
9. also easy automatically detect nested unnamed classes replace them, automatically,
new named classes (classes owl identifiers). domain author would consulted,
though, provide meaningful owl identifiers new classes (otherwise arbitrary identifiers would
used) natural language names new classes (see Section 3.2.1 below).
10. One could also refactor nested operators; example, ((A B) (C D)) equivalent
(A B) (C D). conversion message triples, discussed below, effect
also performs refactoring, cannot cope possible nested union intersection
operators, disallow general rule.

677

fiAndroutsopoulos, Lampouras, & Galanis
owl statements

Message triples

ClassAssertion(NamedClass target )
ClassAssertion(
ObjectComplementOf(NamedClass ) target )
ClassAssertion(
ObjectOneOf(indiv1 indiv2 ...) target )
ClassAssertion(
ObjectHasValue(objProp indiv ) target )
ClassAssertion(
ObjectHasValue(dataProp dataValue ) target )
ClassAssertion(ObjectHasSelf(objProp ) target )
ClassAssertion(
ObjectMaxCardinality(number prop [NamedClass ])
target )
ClassAssertion(
ObjectMinCardinality(number prop [NamedClass ])
target )
ClassAssertion(
ObjectExactCardinality(number prop [NamedClass ])
target )
ClassAssertion(
ObjectSomeValuesFrom(objProp NamedClass ) target )
ClassAssertion(
ObjectAllValuesFrom(objProp NamedClass ) target )
ClassAssertion(
ObjectIntersectionOf(C1 C2 ...) target )

<target, instanceOf, NamedClass >

ClassAssertion(
ObjectUnionOf(C1 C2 ...)

target )

ObjectPropertyAssertion(objProp target indiv )
DataPropertyAssertion(dataProp target dataValue )
NegativeObjectPropertyAssertion(
objProp target indiv )
NegativeDataPropertyAssertion(
dataProp target dataValue )
DifferentIndividuals(target indiv )
DifferentIndividuals(indiv target )
SameIndividual(target indiv )
SameIndividual(indiv target )

<target, not(instanceOf), NamedClass >
<target, oneOf,
or(indiv1, indiv2, ...)>
<target, objProp, indiv >
<target, dataProp, dataValue >
<target, objProp, target >
<target, maxCardinality(prop ),
number [:NamedClass ]>
<target, minCardinality(prop ),
number [:NamedClass ]>
<target, exactCardinality(prop ),
number [:NamedClass ]>
<target, someValuesFrom(objProp ),
NamedClass >
<target, allValuesFrom(objProp ),
NamedClass >
convert (ClassAssertion(C1 target ))
convert (ClassAssertion(C2 target )) ...
or(convert (ClassAssertion(C1 target )),
convert (ClassAssertion(C2 target )),
...)
<target, objProp, indiv >
<target, dataProp, dataValue >
<target, not(objProp ), indiv >
<target, not(dataProp ), dataValue >
<target,
<target,
<target,
<target,

differentIndividuals, indiv >
differentIndividuals, indiv >
sameIndividual, indiv >
sameIndividual, indiv >

Notation: Square brackets indicate optional arguments, convert () recursive application
conversion . NamedClass class identifier; objProp , dataProp , prop identifiers object
properties, datatype properties, properties; indiv , indiv1 , . . . identifiers individuals;
dataValue datatype value; C , C1 , . . . class identifiers, expressions constructing classes
without ObjectIntersectionOf ObjectUnionOf.

Table 1: owl statements individual target, corresponding message triples.
statements, order produce readable texts. owl expert might prefer,
example, following description chateauTeyssier2007, mirrors closely
corresponding owl statements.
2007 Chateau Teyssier member intersection of: (a) class wines, (b) class
individuals (not necessarily exclusively) St. Emilion region, (c) class individuals
(not necessarily exclusively) red color, (d) class individuals (not necessarily exclusively)
strong flavor, (e) class individuals made exclusively Cabernet Sauvignon grapes.

678

fiGenerating Natural Language Descriptions OWL Ontologies

Stricter texts kind, however, seem inappropriate end-users. fact, could
argued even mentioning wine made exactly one grape variety
text Naturalowl produces inappropriate end-users. system instructed
avoid mentioning information via user modeling annotations, discussed below.
OWL statements class targets
system asked describe class, rather individual, scans ontology
statements forms listed left column Table 2. class described
must named one, meaning must owl identifier, Target denotes
identifier. Again, simplify generation process avoid producing complicated
texts, Table 2 requires class identifiers appear points owl also allows
expressions construct unnamed classes using operators. ontology uses unnamed
classes points Table 2 requires class identifiers, easily modified.
texts describing classes, difficult express informally difference
EquivalentClasses SubClassOf. EquivalentClasses(C1 C2 ) means individual
C1 also belongs C2 , vice versa. contrast, SubClassOf(C1 C2 ) means
member C1 also belongs C2 , reverse necessarily true. replace
EquivalentClasses SubClassOf definition StEmilion page 672, member
StEmilion still necessarily also member intersection, wine
characteristics intersection necessarily member StEmilion. Consequently,
one perhaps add sentences like ones shown italics below, expressing
EquivalentClasses SubClassOf, respectively.
St. Emilion kind Bordeaux St. Emilion region. red color strong flavor.
made exactly one grape variety: Cabernet Sauvignon grapes. Every St. Emilion properties,
anything properties St. Emilion.
St. Emilion kind Bordeaux St. Emilion region. red color strong flavor.
made exactly one grape variety: Cabernet Sauvignon grapes. Every St. Emilion properties,
something may properties without St. Emilion.

Naturalowl produces texts, without sentences italics, SubClassOf
EquivalentClasses, avoid generating texts sound formal. Also, may
mention information ontology target class (e.g., St.
Emilion strong flavor), user modeling indicates information already
known text exceed particular length. Hence, generated texts
express necessary, sufficient conditions individuals belong target class.
OWL statements second-level targets
applications, expressing additional owl statements indirectly related
target may desirable. Let us assume, example, target individual
exhibit24, following directly relevant statements retrieved
ontology. Naturalowl would express generating text like one below.
ClassAssertion(:Aryballos :exhibit24)
ObjectPropertyAssertion(:locationFound :exhibit24 :heraionOfDelos)
ObjectPropertyAssertion(:creationPeriod :exhibit24 :archaicPeriod)
ObjectPropertyAssertion(:paintingTechniqueUsed :exhibit24 :blackFigureTechnique)
ObjectPropertyAssertion(:currentMuseum :exhibit24 :delosMuseum)

679

fiAndroutsopoulos, Lampouras, & Galanis
owl statements

Message triples

EquivalentClasses(Target C )
EquivalentClasses(C Target )
SubClassOf(Target NamedClass )
SubClassOf(Target ObjectComplementOf(NamedClass ))
SubClassOf(Target
ObjectOneOf(indiv1 indiv2 ...))
SubClassOf(Target ObjectHasValue(objProp indiv ))
SubClassOf(Target
ObjectHasValue(dataProp dataValue ))
SubClassOf(Target ObjectHasSelf(objProp ))
SubClassOf(Target
ObjectMaxCardinality(number prop [NamedClass ]))
SubClassOf(Target
ObjectMinCardinality(number prop [NamedClass ]))
SubClassOf(Target
ObjectExactCardinality(number prop [NamedClass ]))
SubClassOf(Target
ObjectSomeValuesFrom(objProp NamedClass ))
SubClassOf(Target
ObjectAllValuesFrom(objProp NamedClass ))
SubClassOf(Target
ObjectIntersectionOf(C1 C2 ...))

convert (SubClassOf(Target C ))
convert (SubClassOf(Target C ))
<Target, isA, NamedClass >
<Target, not(isA), NamedClass >
<Target, oneOf,
or(indiv1, indiv2, ...)>
<Target, objProp, indiv >

SubClassOf(Target
ObjectUnionOf(C1 C2 ...))
DisjointClasses(Target NamedClass )
DisjointClasses(NamedClass Target )

<Target, dataProp, dataValue >
<Target, objProp, Target >
<Target, maxCardinality(prop ),
number [:NamedClass ]>
<Target, minCardinality(prop ),
number [:NamedClass ]>
<Target, exactCardinality(objProp ),
number [:NamedClass ]>
<Target, someValuesFrom(objProp ),
NamedClass >
<Target, allValuesFrom(objProp ),
NamedClass >
convert (SubClassOf(C1 Target ))
convert (SubClassOf(C2 Target )) ...
or(convert (SubClassOf(C1 Target )),
convert (SubClassOf(C2 Target )),
...)
<Target, not(isA), NamedClass >
<Target, not(isA), NamedClass >

Notation: Square brackets indicate optional arguments, convert () recursive application
conversion . NamedClass class identifier; objProp , dataProp , prop identifiers object
properties, datatype properties, properties; indiv , indiv1 , . . . identifiers individuals;
dataValue datatype value; C , C1 , . . . class identifiers, expressions constructing classes
without ObjectIntersectionOf ObjectUnionOf.

Table 2: owl statements class target, corresponding message triples.
aryballos, found Heraion Delos. created archaic period
decorated black-figure technique. currently Museum Delos.

names classes individuals shown hyperlinks indicate
used subsequent targets. Clicking hyperlink would request describe
corresponding class individual. Alternatively, may retrieve advance owl
statements subsequent targets add current target.
precisely, assuming target individual, subsequent targets, called
second-level targets, targets class, provided named one, individuals target directly linked via object properties. Naturalowl considers second-level
targets current target individual, class targets, second-level
targets often lead complicated texts. retrieve owl statements current
second-level targets (when applicable), current target, set
maximum fact distance 2 1, respectively. Returning exhibit24, let us assume
maximum fact distance 2 following owl statements second-level
targets retrieved.11
11. Consult http://www.w3.org/TR/owl-time/ principled representations time owl.

680

fiGenerating Natural Language Descriptions OWL Ontologies

SubClassOf(:Aryballos :Vase)
SubClassOf(:Aryballos
ObjectHasValue(:exhibitTypeCannedDescription
"An aryballos small spherical vase narrow neck, athletes
kept oil spread bodies with"^^xsd:string))
DatatypePropertyAssertion(:periodDuration :archaicPeriod "700 BC 480 BC"^^xsd:string)
DatatypePropertyAssertion(:periodCannedDescription :archaicPeriod
"The archaic period Greek ancient city-states developed"^^xsd:string)
DataPropertyAssertion(:techniqueCannedDescription :blackFigureTechnique
"In black-figure technique, silhouettes rendered black pale
surface clay, details engraved"^^xsd:string)

express retrieved owl statements, including second-level targets,
Naturalowl would generate text like following, may preferable,
first time user encounters aryballos archaic exhibits.
aryballos, kind vase. aryballos small spherical vase narrow neck,
athletes kept oil spread bodies with. aryballos found Heraion
Delos created archaic period. archaic period Greek ancient citystates developed spans 700 bc 480 bc. aryballos decorated black-figure
technique. black-figure technique, silhouettes rendered black pale surface
clay, details engraved. aryballos currently Museum Delos.

note many ontologies impractical represent information
logical terms. example, much easier store information aryballos
small. . . bodies string, i.e., canned sentence, rather defining
classes, properties, individuals spreading actions, bodies, etc. generating
sentence logical meaning representation. Canned sentences, however,
entered multiple versions, several languages user types need supported.
Converting OWL statements message triples
Tables 1 2 also show retrieved owl statements rewritten triples
form hS, P, Oi, target second-level target; individual,
datatype value, class, set individuals, datatype values, classes mapped
to; P specifies kind mapping. call semantic subject owner
triple, semantic object filler ; triple also viewed field named
P , owned S, filled O. example, owl statements exhibit24 shown
above, including second-level targets, converted following triples.
<:exhibit24, instanceOf, :Aryballos>
<:exhibit24, :locationFound, :heraionOfDelos>
<:exhibit24, :creationPeriod, :archaicPeriod>
<:exhibit24, :paintingTechniqueUsed, :blackFigureTechnique>
<:exhibit24, :currentMuseum, :delosMuseum>
<:Aryballos, isA, :Vase>
<:Aryballos, :exhibitTypeCannedDescription, "An aryballos a... bodies with"^^xsd:string>
<:archaicPeriod, :periodDuration, "700 BC 480 BC"^^xsd:string>
<:archaicPeriod, :periodCannedDescription, "The archaic period was..."^^xsd:string>
<:blackFigureTechnique, :techniqueCannedDescription, "In black-figure..."^^xsd:string>

precisely, P be: (i) property ontology; (ii) one keywords isA,
instanceOf, oneOf, differentIndividuals, sameIndividuals; (iii) expression
form modifier(), modifier may not, maxCardinality etc. (see Tables 1 2)
681

fiAndroutsopoulos, Lampouras, & Galanis

Figure 2: Graph view message triples.
property ontology. hereafter call properties three types P ,
though types (ii) (iii) strictly properties terminology owl.
need distinguish three types, use terms property ontology,
domain-independent property, modified property, respectively.
Every owl statement collection owl statements represented set rdf
triples.12 triples Tables 12 similar, rdf triples. notably, expressions form modifier() cannot used P rdf triples. avoid confusion, call message triples triples Tables 12, distinguish rdf triples.
rdf triples, message triples viewed forming graph. Figure 2 shows
graph message triples exhibit24; triple linking blackFigureTechnique
canned sentence shown save space. second-level targets classes
individuals distance one target (exhibit24).13 contrast, graph
rdf triples representing owl statements would complicated, second-level
targets would always distance one target.
message triple intended easily expressible simple sentence,
always case rdf triples representing owl statements. message triples also
capture similarities sentences generated may less obvious looking
original owl statements rdf triples representing them. example,
ClassAssertion SubClassOf statements mapped identical message triples,
apart identifiers individual class, similarity message
triples reflects similarity resulting sentences, also shown below.
ClassAssertion(ObjectMaxCardinality(1 :madeFromGrape) :product145)
<:product145, maxCardinality(:madeFromGrape), 1>

Product 145 made one grape.

12. See http://www.w3.org/TR/owl2-mapping-to-rdf/.
13. Instead retrieving owl statements target second-level targets converting
message triples, one could equivalently convert owl statements ontology message
triples select message triples connecting target nodes distance two target.

682

fiGenerating Natural Language Descriptions OWL Ontologies

SubClassOf(:StEmilion ObjectMaxCardinality(1 :madeFromGrape))
<:StEmilion, maxCardinality(:madeFromGrape), 1>

St. Emilion made one grape.

contrast, without conversion message triples, owl statements rdf
triples representing would lead difficult follow sentences like following:
Product 145 member class individuals made one grape.
St. Emilion subclass class individuals made one grape.

example, Tables 1 2 discard ObjectIntersectionOf operators, producing
multiple message triples instead. example, EquivalentClasses statement defining
StEmilion page 672 would converted following message triples.
<:StEmilion,
<:StEmilion,
<:StEmilion,
<:StEmilion,
<:StEmilion,
<:StEmilion,

isA, :Bordeaux>
:locatedIn, :stEmilionRegion>
:hasColor, :red>
:hasFlavor, :strong>
:madeFromGrape, :cabernetSauvignonGrape>
maxCardinality(:madeFromGrape), 1>

resulting message triples correspond sentences below, subsequent references StEmilion replaced pronouns improve readability; sentences
could also aggregated longer ones, discussed later sections.
St. Emilion kind Bordeaux. St. Emilion region. red color. strong flavor.
made Cabernet Sauvignon grape. made one grape variety.

contrast original owl statement page 672 rdf triples representing
would lead stricter text page 678, inappropriate end-users, already
noted. Notice, also, Table 2 converts EquivalentClasses SubClassOf statements
identical triples, P isA, since Naturalowl produces texts kinds
statements, already discussed.
Tables 1 2 also replace ObjectUnionOf operators disjunctions message triples.
following owl statement mapped message triple shown below:
ClassAssertion(
UnionOf(ObjectHasValue(:hasFlavor :strong) ObjectHasValue(:hasFlavor :medium))
:houseWine)
or(<:houseWine, :hasFlavor, :strong>, <:houseWine, :hasFlavor, :medium>)

leads first sentence below; sentence shortened aggregation, leading second sentence below.
house wine strong flavor medium flavor.
house wine strong medium flavor.

683

fiAndroutsopoulos, Lampouras, & Galanis

contrast, owl statement corresponding rdf triples effect say that:
house wine member union of: (i) class wines strong flavor, (ii)
class wines medium flavor.

Interest scores repetitions
Expressing message triples retrieved owl statements always
appropriate. Let us assume, example, maximum fact distance 2
description exhibit24 Figure 2 requested museum visitor. may
case visitor already encountered archaic exhibits, duration
archaic period mentioned previous descriptions. Repeating duration
period may, thus, undesirable. may also want exclude message triples
uninteresting particular types users. example, may message triples
providing bibliographic references, children would probably find uninteresting.
Naturalowl provides mechanisms allowing domain author assign importance
score every possible message triple, possibly different scores different user types
(e.g., adults, children). score non-negative integer indicating interesting user
corresponding type presumably find information message triple,
information already conveyed user. museum projects Naturalowl
originally developed for, interest scores ranged 0 (completely uninteresting)
3 (very interesting), different range also used. scores specified
message triples involve particular property P (e.g., P = madeFrom),
message triples involve semantic subjects particular class (e.g., Statue
= Statue) particular property P , message triples involve particular
semantic subjects (e.g., =exhibit37) particular property P . example, may
wish specify materials exhibits collection generally medium
interest (P = madeFrom, score 2), materials statues lower interest (S
statue, P = madeFrom, score 1), perhaps statues collection made
stone, material particular statue exhibit24 important (S =
exhibit10, P = madeFrom, score 3), perhaps exhibit24 gold statue.
discuss mechanisms used assign interest scores message
triples article, detailed description mechanisms found elsewhere
(Androutsopoulos et al., 2012). also note human-authored texts describing
individuals classes ontology available along owl statements or,
generally, logical facts express, statistical machine learning methods
employed learn automatically select assign interest scores logical facts (Duboue &
McKeown, 2003; Barzilay & Lapata, 2005; Kelly, Copestake, & Karamanis, 2010). Another
possibility (Demir, Carberry, & McCoy, 2010) would compute interest scores
graph algorithms like PageRank (Brin & Page, 1998).
domain author also specify many times message triple
repeated, assumed users different types assimilated it.
triple assimilated, never repeated texts user. example,
domain author specify children assimilate duration historical period
mentioned twice; hence, system may repeat, example, duration
archaic period two texts. Naturalowl maintains personal model end-user.
model shows message triples conveyed particular user previous
684

fiGenerating Natural Language Descriptions OWL Ontologies

texts, many times. Again, information user modeling mechanisms
Naturalowl found elsewhere (Androutsopoulos et al., 2012).
Selecting message triples convey
asked describe target, Naturalowl first retrieves ontology relevant owl statements, possibly also second-level targets. converts retrieved
statements message triples, consults interest scores personal user models rank message triples decreasing interest score, discarding triples
already assimilated. message triple target assimilated,
message triples second-level targets connected assimilated triple
also discarded; example, creationPeriod triple (edge) Figure 2 assimilated, triples archaic period (the edges leaving archaicPeriod)
also discarded. system selects maxMessagesPerPage triples
interesting remaining ones; maxMessagesPerPage parameter whose value set
smaller larger values types users prefer shorter longer texts, respectively.
Limitations content selection
owl allows one define broadest possible domain range particular property,
using statements like following.
ObjectPropertyDomain(:madeFrom :Wine)

ObjectPropertyRange(:madeFrom :Grape)

practice, specific range restrictions imposed particular subclasses
propertys domain. example, following statements specify madeFrom
used individuals subclass GreekWine Wine, range (possible values)
madeFrom restricted individuals subclass GreekGrape Grape.
SubClassOf(:GreekWine :Wine) SubClassOf(:GreekGrape :Grape)
SubClassOf(:GreekWine AllValuesFrom(:madeFrom :GreekGrape))

Naturalowl considers AllValuesFrom similar restrictions (see Tables 1 2),
ObjectPropertyDomain ObjectPropertyRange statements. latter typically provide
general and, hence, uninteresting information perspective end-users.
generally, Naturalowl consider owl statements express axioms
properties, meaning statements declaring property symmetric, asymmetric,
reflexive, irreflexive, transitive, functional, inverse functional, property
inverse of, disjoint another property, subsumed chain
properties, subproperty (more specific) another property. Statements
kind mostly useful consistency checks, deduction, generating texts
describing properties (e.g., grandparent somebody means).14
3.1.2 Text Planning
target, previous mechanisms produce message triples expressed,
triple intended easily expressible single sentence. text planner
Naturalowl orders message triples, effect ordering corresponding sentences.
14. Subproperties without sentence plans, discussed below, could inherit sentence plans superproperties, case automatically extract sentence plans ontology instead.

685

fiAndroutsopoulos, Lampouras, & Galanis

Global local coherence
considering global coherence, text planners attempt build structure, usually
tree, shows clauses, sentences, larger segments text related
other, often terms rhetorical relations (Mann & Thompson, 1998). allowed
preferred orderings sentences (or segments) often follow, least partially,
global coherence structure. texts, however, Naturalowl intended
generate, global coherence structures tend rather uninteresting,
sentences simply provide additional information target second-level
targets, global coherence considered Naturalowl.15
considering local coherence, text planners usually aim maximize measures
examine whether adjacent sentences (or segments) continue focus entities or, focus changes, smooth transition is. Many local coherence measures
based Centering Theory (ct) (Grosz, Joshi, & Weinstein, 1995; Poesio, Stevenson,
& Di Eugenio, 2004). Consult work Karamanis et al. (2009) introduction
ct ct-based analysis m-piros texts, also applies texts Naturalowl.
maximum fact distance Naturalowl 1, sentence-to-sentence transitions type known ct continue, preferred type. maximum
fact distance 2, however, transitions always continue. repeat
long aryballos description page 681 without sentence aggregation. readers familiar
ct, show italics salient noun phrase sentence un , realizes
discourse entity known preferred center cp (un ). underlined noun phrases realize backward looking center cb (un ), roughly speaking salient discourse entity
previous sentence also mentioned current sentence.
(1) (exhibit) aryballos. (2) aryballos kind vase. (3) aryballos small
spherical vase narrow neck, athletes kept oil spread bodies with.
(4) aryballos found Heraion Delos. (5) created archaic period. (6)
archaic period Greek ancient city-states developed. (7) spans 700 bc 480
bc. (8) aryballos decorated black-figure technique. (9) black-figure technique,
silhouettes rendered black pale surface clay, details engraved. (10)
aryballos currently Museum Delos.

sentence 4, cp (u4 ) target exhibit, cb (u4 ) undefined transition
sentence 3 4 nocb, type transition avoided; mark nocb transitions
bullets. sentence 6, cp (u6 ) = cb (u6 ) 6= cb (u5 ), kind transition known
smooth-shift (Poesio et al., 2004), less preferred continue, better nocb.
Another nocb occurs sentence 7 8, followed smooth-shift sentence 8
9, another nocb sentence 9 10. transitions continue.
text planner Naturalowl groups together sentences (message triples) describe particular second-level target (e.g., sentences 23, 67, 9) places
group immediately sentence introduces corresponding second-level target
(immediately sentences 1, 5, 8). Thus transition sentence introduces second-level target first sentence describes second-level target (e.g.,
15. Liang et al. (2011a) Power (2011) seem agree rhetorical relations relevant
generating texts owl ontologies.

686

fiGenerating Natural Language Descriptions OWL Ontologies

sentence 1 2, 5 6, 8 9) smooth-shift (or continue
special case initial sentence 1 2). nocb occurs sentences return
providing information primary target, group sentences provide
information second-level target. transitions type continue.
simple strategy avoid nocb transitions would end generated text
message triples describe second-level target reported, record
user model message triples content selection provided
actually conveyed. example, would generate sentences 1 3; user
requested information exhibit, sentences 4 7 would generated etc.
Topical order
ordering sentences, also need consider topical similarity adjacent
sentences. Compare, example, following two texts.
{locationSection Stoa Zeus Eleutherios located western part Agora. located
next Temple Apollo Patroos.} {buildSection built around 430 bc. built Doric
style. built porous stone marble.} {useSection used Classical period,
Hellenistic period, Roman period. used religious place meeting point.}
{conditionSection destroyed late Roman period. excavated 1891 1931. Today
good condition.}
Stoa Zeus Eleutherios built Doric style. excavated 1891 1931.
built porous stone marble. located western part Agora. destroyed
late Roman period. used religious place meeting point. located next
Temple Apollo Patroos. built around 430 bc. Today good condition. used
Classical period, Hellenistic period, Roman period.

Even though texts contain sentences, second text difficult
follow, acceptable. first one better, groups together topically
related sentences. mark sentence groups first text curly brackets,
brackets would shown end-users. longer texts, sentence groups may optionally
shown separate paragraphs sections, call sections.
allow message triples (and corresponding sentences) grouped topic,
domain author may define sections (e.g., locationSection, buildSection) assign
property single section (e.g., assign properties isInArea isNextTo
locationSection). message triple placed section property.
ordering sections properties inside section also specified, causing message triples ordered accordingly (e.g., may specify locationSection
precede buildSection, inside locationSection, isInArea property
expressed isNextTo). sections, assignments properties sections,
order sections properties defined domain-dependent generation resources (Androutsopoulos et al., 2012).
overall text planning algorithm
Naturalowls text planning algorithm summarized Figure 3. message triples
ordered include triples describe second-level targets, i.e., triples hS, P, Oi whose
owner second-level target, triples primary second-level target
687

fiAndroutsopoulos, Lampouras, & Galanis

procedure orderMessageTriples
inputs:
t[0]: primary target
t[1], ..., t[n]: second-level targets
L[0]: unordered list triples describing t[0]
...
L[n]: unordered list triples describing t[n]
SMap: mapping properties sections
SOrder: partial order sections
POrder: partial order properties within sections
output:
ordered list message triples
steps:
:= 0 n { orderMessageTriplesAux(L[i], SMap, SOrder, POrder) }
:= 1 n { insertAfterFirst(<t[0], _, t[i]>, L[0], L[i]) }
return L[0]
procedure orderMessageTriplesAux
inputs:
L: unordered list triples single target
SMap: mapping properties sections
SOrder: partial order sections
POrder: partial order properties within sections
local variables:
S[1], ..., S[k]: lists, triples one section
output:
ordered list message triples single target
steps:
<S[1], ..., S[k]> := splitInSections(L, SMap)
:= 1 k { S[i] := orderTriplesInSection(S[i], POrder) }
<S[1], ..., S[k]> := reorderSections(S[1], ..., S[k], SOrder)
return concatenate(S[1], ..., S[k])

Figure 3: overall text planning algorithm Naturalowl.

ordered separately, using ordering properties sections. ordered triples
second-level target inserted ordered list primary target triples,
immediately first triple introduces second-level target, i.e., immediately
first triple whose second-level target.
related work text planning
ordering properties sections similar text schemata (McKeown, 1985),
roughly speaking domain-dependent patterns specify possible arrangements different types sentences (or segments). Sentence ordering studied extensively
text summarization (Barzilay, Elhadad, & McKeown, 2002). Duboue McKeown (2001)
discuss methods could used learn order sentences segments
nlg semantically tagged training corpora. Consult also work Barzilay Lee
(2004), Elsner et al. (2007), Barzilay Lapata (2008), Chen et al. (2009).
688

fiGenerating Natural Language Descriptions OWL Ontologies

Figure 4: lexicon entry verb find.
3.2 Micro-planning
processing stages discussed far select order message triples
expressed. next stage, micro-planning, consists three sub-stages: lexicalization,
sentence aggregation, generation referring expressions; see also Figure 1 page 676.
3.2.1 Lexicalization
lexicalization, nlg systems usually turn output content selection (in case,
message triples) abstract sentence specifications. Naturalowl, every property
ontology every supported natural language, domain author may specify one
template-like sentence plans indicate message triples involving property
expressed. discuss sentence plans specified, first slight
deviation necessary, briefly discuss lexicon entries Naturalowl.
Lexicon entries
verb, noun, adjective domain author wishes use sentence
plans, lexicon entry provided, specify inflectional forms word.16
lexicon entries multilingual (currently bilingual); allows sentence plans
reused across similar languages better option available, discussed elsewhere
(Androutsopoulos et al., 2007). Figure 4 shows lexicon entry verb whose English
base form find, viewed domain author using Protege plug-in
Naturalowl. identifier lexicon entry toFindLex. English part entry
shows base form find, simple past found etc. Similarly, Greek
part lexicon entry would show base form corresponding verb ()
inflectional forms various tenses, persons etc. lexicon entries nouns
adjectives similar.
English inflectional forms could automatically produced base
forms using simple morphology rules. hope exloit existing English morphology
component, simplenlg (Gatt & Reiter, 2009), future work. Similar
morphology rules Greek used authoring tool m-piro (Androutsopoulos et
al., 2007), hope include future version Naturalowl. Rules kind
would reduce time domain author spends creating lexicon entries. ontologies
considered, however, dozens lexicon entries verbs, nouns, adjectives
16. lexicon entries need provided closed-class words, like determiners prepositions.

689

fiAndroutsopoulos, Lampouras, & Galanis

suffice. Hence, even without facilities automatically produce inflectional forms, creating
lexicon entries rather trivial. Another possibility would exploit general-purpose
lexicon lexical database, like WordNet (Fellbaum, 1998) celex, though resources
kind often cover highly technical concepts ontologies.17
lexicon entries and, generally, domain-dependent generation resources
Naturalowl stored instances owl ontology (other ontology texts
generated from) describes linguistic resources system (Androutsopoulos
et al., 2012). domain author, however, interacts plug-in need
aware owl representation resources. representing domain-dependent
generation resources owl, becomes easier publish Web, check
inconsistencies etc., owl ontologies.
Sentence plans
Naturalowl, sentence plan sequence slots, along instructions specifying fill in. Figure 5 shows English sentence plan property
usedDuringPeriod, viewed domain author using Protege plug-in Naturalowl. sentence plan expresses message triples form hS, usedDuringPeriod, Oi
producing sentences like following.
[slot1 stoa] [slot2 used] [slot3 during] [slot4 Classical period].
[slot1 Stoa Zeus Eleutherios] [slot2 used] [slot3 during] [slot4 Classical period, Hellenistic
period, Roman period].

first slot sentence plan Figure 5 filled automatically
generated referring expression owner (S) triple. example, triple
express <:stoaZeusEleutherios, :usedDuringPeriod, :classicalPeriod>, appropriate
referring expression may demonstrative noun phrase like stoa, pronoun
(it), monuments natural language name (the Stoa Zeus Eleutherios).
discuss generation referring expressions below, along mechanisms specify
natural language names. sentence plan also specifies referring expression must
nominative case (e.g., stoa, opposed genitive case expressions
stoas, stoas height 5 meters).
second slot filled form verb whose lexicon identifier
toUseVerb. verb form must simple past passive voice, positive polarity
(as opposed used). number must agree number expression
first slot; example, want generate Stoa Zheus Eleutherios
used, Stoas used. third slot filled preposition during.
fourth slot filled expression filler (O) message triple,
accusative case.18 <:stoaZeusEleutherios, :usedDuringPeriod, :classicalPeriod>,
slot would filled natural language name classicalPeriod.19
sentence plan also allows resulting sentence aggregated sentences.
17. See http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC96L14 celex.
18. English prepositions usually require noun phrase complements accusative (e.g., him). Greek
languages, cases noticeable effects.
19. Future versions Naturalowl may allow referring expression natural language
name produced (e.g., pronoun), S.

690

fiGenerating Natural Language Descriptions OWL Ontologies

Figure 5: sentence plan property usedDuringPeriod.
generally, instructions sentence plan may indicate slot
filled one following (ivii):
(i) referring expression (owner) message triple. sentence plan may
specify particular type referring expression use (e.g., always use natural language
name S) or, example Figure 5, may allow system automatically
produce appropriate type referring expression depending context.
(ii) verb lexicon entry, particular form, possibly form
agrees another slot. polarity verb also manually specified or,
filler (O) message triple Boolean value, polarity automatically set
match value (e.g., produce built-in flash false).
(iii) noun adjective lexicon, particular form (e.g., case, number),
form agrees another slot.
(iv) preposition (v) fixed string.
(vi) expression (filler) triple. individual class,
expression natural language name O; datatype value (e.g., integer),
value inserted slot; similarly disjunction conjunction
datatype values, individuals, classes.
(vii) concatenation property values O, provided individual.
example, may need express message triple like first one below, whose (anonymous
rdf sense) object :n linked numeric value (via hasAmount)
individual standing currency (via hasCurrency).
<:tecra8, :hasPrice, _:n>

<_:n, :hasAmount, "850"^^xsd:float>

<_:n, :hasCurrency, :euroCurrency>

would want sentence plan include slot filled concatenation
hasAmount value :n natural language name hasCurrency value :n (e.g.,
850 Euro English, 850 Greek).
Default sentence plan
sentence plan provided particular property ontology, Naturalowl uses default sentence plan, consisting three slots. first slot filled
automatically generated referring expression owner (S) triple,
nominative case. second slot filled tokenized form owl identifier
property. third slot filled appropriate expression filler (O)
691

fiAndroutsopoulos, Lampouras, & Galanis

triple, discussed above, accusative case (if applicable). following message
triple, default sentence plan would produce sentence shown below:
<:stoaZeusEleutherios, :usedDuringPeriod, and(:classicalPeriod, :hellenisticPeriod, :romanPeriod)>

Stoa zeus eleutherios used period classical period, hellenistic period, roman period.

Notice use single message triple and(...) filler, instead different
triple period. kind triple merging effect form aggregation, discussed
below, takes place content selection. Also, assumed sentence
natural language names individuals provided either;
case, Naturalowl uses tokenized forms owl identifiers individuals instead.
tokenizer Naturalowl handle CamelCase (e.g., :usedDuringPeriod)
underscore style (e.g., :used period). styles used identifiers
properties, classes, individuals, output tokenizer may worse
example suggests, resulting sentences improved providing sentence plans
associating classes individuals natural language names, discussed below.
Using rdfs:label strings
owl properties (and elements owl ontologies) labeled strings
multiple natural languages using rdfs:label annotation property, defined rdf
owl standards. example, usedDuringPeriod property could labeled
used shown below; could similar labels Greek languages.
AnnotationAssertion(rdfs:label :usedDuringPeriod "was used during"@en)

rdfs:label string specified property message triple, Naturalowl
uses string second slot default sentence plan. quality resulting
sentences can, thus, improved, rdfs:label strings natural phrases
tokenized property identifiers. rdfs:label shown above, default sentence
plan would produce following sentence.
Stoa zeus eleutherios used classical period, hellenistic period, roman period.

Even rdfs:label strings, default sentence plan may produce sentences disfluencies. Also, rdfs:label strings indicate grammatical categories
words, allow system apply many sentence aggregation rules
discussed below. limitation default sentence plan allow
slots preceded followed, respectively, phrase.
Sentence plans domain-independent modified properties
domain author need provide sentence plans domain-independent
properties (e.g., instanceOf, isA, see Tables 12). properties fixed, domainindependent semantics; hence, built-in sentence plans used. English built-in sentence plans, also serve examples sentence plans, summarized
Table 3; Greek built-in sentence plans similar. save space show sentence
plans templates Table 3, show sentence plans negated domainindependent properties (e.g., not(isA)), similar. Additional slot restrictions
692

fiGenerating Natural Language Descriptions OWL Ontologies

Forms message triples
Example message triples
corresponding built-in sentence plans
possible resulting sentences
<S, instanceOf, >
<:eos450d, instanceOf, :PhotographicCamera>
ref(S) toBeVerb name(indef, O)
eos 450d photographic camera.
<S, instanceOf, >
<:eos450d, instanceOf, :Cheap>
ref(S) toBeVerb name(adj, O)
eos 450d cheap.
<S, oneOf, >
<:WineColor, oneOf, or(:white, :rose, :red)>
ref(S) toBeVerb name(O)
wine color white, rose, red.
<S, differentIndividuals, >
<:n97, differentIndividuals, :n97mini>
ref(S) toBeVerb identical name(O)
n97 identical n97 mini.
<S, sameIndividual, >
<:eos450d, sameIndividual, :rebelXSi>
ref(S) toBeVerb identical name(O)
identical Rebel xsi.
<S, isA, >
<:StEmilion, isa, :Bordeaux>
ref(S) toBeVerb kind name(noarticle, O)
St. Emilion kind Bordeaux.
<S, isA, >
<:StEmilion, isa, :Red>
ref(S) toBeVerb name(adj, O)
St. Emilion red.
Notation: ref() stands referring expression ; name() natural language name ;
name(indef, ) name(noarticle, ) mean name noun phrase indefinite
article. Sentence plans involving name(adj, ) used natural language name
sequence one adjectives; otherwise sentence plan previous row used.

Table 3: Built-in English sentence plans domain-independent properties.
shown Figure 3 require, example, subject-verb number agreement verb forms
(is was) present tense. Information provided specifying natural
language names individuals classes, discussed below, shows definite indefinite
articles articles used (e.g., n97 mini, exhibit 24, St. Emilion St. Emilion simply St. Emilion), default number
name (e.g., wine color Wine colors are). also possible modify
built-in sentence plans; example, museum context may wish generate
aryballos kind vase instead aryballos kind vase.
sentence plans modified properties (e.g., minCardinality(manufacturedBy), see
Tables 12) also automatically produced, sentence plans unmodified
properties (e.g., manufacturedBy).
Specifying appropriateness sentence plans
Multiple sentence plans may provided property ontology
language. Different appropriateness scores (similar interest scores properties)
assigned alternative sentence plans per user type. allows specifying,
example, sentence plan generates sentences like amphora depicts Miltiades less appropriate interacting children, compared alternative sentence
plan common verb (e.g., shows). Automatically constructed sentence plans
inherit appropriateness scores sentence plans constructed from.
Related work sentence plans
sentence plans Naturalowl similar expressions sentence planning languages like spl (Kasper & Whitney, 1989) used generic surface realizers,
fuf/surge (Elhadad & Robin, 1996), kpml (Bateman, 1997), realpro (Lavoie & Ram693

fiAndroutsopoulos, Lampouras, & Galanis

bow, 1997), nitrogen/halogen (Langkilde, 2000), openccg (White, 2006).
sentence plans Naturalowl, however, leave fewer decisions subsequent stages.
disadvantage sentence plans often include information could obtained large-scale grammars corpora (Wan, Dras, Dale, & Paris, 2010).
hand, input generic surface realizers often refers non-elementary linguistic concepts (e.g., features particular syntax theory) concepts upper model
(Bateman, 1990); latter high-level domain-independent ontology may use
different conceptualization ontology texts generated from. Hence,
linguistic expertise, example Systemic Grammars (Halliday, 1994) case kpml
(Bateman, 1997), effort understand upper model required. contrast,
sentence plans Naturalowl require domain author familiar elementary linguistic concepts (e.g., tense, number), require familiarity
upper model. sentence plans simpler than, example, templates Busemann Horacek (1999) McRoy et al. (2003), allow, instance,
conditionals recursive invokation templates. See also work Reiter (1995)
van Deemter et al. (2005) discussion template-based vs. principled nlg.
corpora texts annotated message triples express available,
templates also automatically extracted (Ratnaparkhi, 2000; Angeli, Liang, & Klein,
2010; Duma & Klein, 2013). Statistical methods jointly perform content selection,
lexicalization, surface realization also proposed (Liang, Jordan, & Klein,
2009; Konstas & Lapata, 2012a, 2012b), currently limited generating single
sentences flat records.
Specifying natural language names
domain author assign natural language (nl) names individuals
named classes ontology; recall named classes mean classes owl
identifiers. individual named class assigned nl name, rdfs:label
tokenized form identifier used instead. nl names domain author
provides specified much sentence plans, i.e., sequences slots. example,
may specify English nl name class ItalianWinePiemonte concatenation
following slots; explain slots below.
[indef an] [adj Italian] [headnoun wine] [prep from] [def the] [noun Piemonte] [noun region]

would allow Naturalowl generate sentence shown following
message triple; tokenized form identifier wine32 used.
<:wine32, instanceOf, :ItalianWinePiemonte>

Wine 32 Italian wine Piemonte region.

Similarly, may assign following nl names individuals classicalPeriod, stoa
ZeusEleutherios, gl2011, classes ComputerScreen Red. Naturalowl makes
distinction common proper nouns; entered nouns lexicon,
may multi-word (e.g., Zeus Eleutherios). Naturalowl also instructed
capitalize words particular slots (e.g., Classical).
694

fiGenerating Natural Language Descriptions OWL Ontologies

[def the] [adj Classical] [headnoun period] , [def the] [headnoun stoa] [prep of] [noun Zeus Eleutherios],
[headnoun GL-2011] , [indef a] [noun computer] [headnoun screen] , [headadj red]

nl names could used express message triples shown below:
<:stoaZeusEleutherios, :usedDuringPeriod,

:classicalPeriod>

Stoa Zeus Eleutherios used Classical period.
<:gl2011, instanceOf, :ComputerScreen>

<:gl2011, instanceOf, :Red>

GL-2011 computer screen. GL-2011 red.

precisely, nl name sequence slots, accompanying instructions
specifying slots filled in. slot filled with:
(i) article, definite indefinite. article first slot (if present) treated
article overall nl name.
(ii) noun adjective flagged head (main word) nl name. Exactly one
head must specified per nl name must lexicon entry. number case
head, also taken number case overall nl name,
automatically adjusted per context. example, different sentence plans may require
nl name nominative case used subject, accusative used
object verb; aggregation rules, discussed below, may require singular
nl name turned plural. Using lexicon entries, list inflectional forms
nouns adjectives, Naturalowl adjust nl names accordingly. gender
head adjectives also automatically adjusted, whereas gender head nouns
fixed specified lexicon entries.
(iii) noun adjective, among listed lexicon. nl name may
require particular inflectional form used, may require inflectional form
agrees another slot nl name.
(iv) preposition, (v) fixed string.
sentence plans, domain author specifies nl names using Protege
plug-in Naturalowl. Multiple nl names specified individual class,
assigned different appropriateness scores per user type; hence, different
terminology (e.g., common names diseases) used generating texts nonexperts, opposed texts experts (e.g., doctors). domain author also specify,
using plug-in, nl names particular individuals classes involve
definite, indefinite, articles, nl names singular plural
default. example, may prefer texts mention class aryballoi single
particular generic object, using indefinite singular plural form, shown below.
aryballos kind vase. aryballos kind vase. Aryballoi kind vase.

695

fiAndroutsopoulos, Lampouras, & Galanis

3.2.2 Sentence Aggregation
sentence plans previous section lead separate sentence message triple.
nlg systems often aggregate sentences longer ones improve readability. Naturalowl, maximum number sentences aggregated form single longer
sentence specified per user type via parameter called maxMessagesPerSentence.
museum contexts system originally developed for, setting maxMessagesPerSentence
3 4 led reasonable texts adult visitors, whereas value 2 used children. sentence aggregation Naturalowl performed set manually crafted
rules, intended domain-independent. claim set rules,
initially based aggregation rules m-piro (Melengoglou, 2002), complete,
hope extended future work; see, example, work Dalianis
(1999) rich set aggregation rules.20 Nevertheless, current rules Naturalowl
already illustrate several aggregation opportunities arise generating texts
owl ontologies.
save space, discuss English sentence aggregation; Greek aggregation similar. show mostly example sentences aggregation, rules actually
operate sentence plans also consider message triples expressed.
rules intended aggregate short single-clause sentences. Sentence plans produce
complicated sentences may flagged (using tickbox bottom Figure
5) signal aggregation affect sentences. aggregation rules apply almost exclusively sentences adjacent ordering produced text
planner; exception aggregation rules involve sentences cardinality
restrictions. Hence, depending ordering text planner may
fewer aggregation opportunities; see work Cheng Mellish (2000) related discussion. Also, aggregation rules Naturalowl operate sentences topical
section, aggregating topically unrelated sentences often sounds unnatural.
aggregation Naturalowl greedy. rules discussed below, starting
discussed first, system scans original (ordered) sentences first
last, applying rule wherever possible, provided rules application lead
sentence expressing maxMessagesPerSentence original sentences. rule
applied multiple ways, example aggregate two three sentences, application
aggregates sentences without violating maxMessagesPerSentence preferred.
Avoid repeating noun multiple adjectives: Message triples form hS, P, O1 i, . . . ,
hS, P, aggregated single message triple hS, P, and(O1 , . . . , )i.
nl names O1 , . . . , are, apart possible initial determiners, sequences
adjectives followed head noun, head noun need repeated.
Let us consider following message triple. Assuming nl names three
periods first sentence below, original sentence repeat period three
times. aggregation rule omits last occurrence head noun.
<:stoaZeusEleutherios, :usedDuringPeriod, and(:classicalPeriod, :hellenisticPeriod, :romanPeriod)>

used Classical period, Hellenistic period, Roman period.

used

Classical, Hellenistic, Roman period.
20. appropriate corpora available, may also possible train aggregation modules (Walker,
Rambow, & Rogati, 2001; Barzilay & Lapata, 2006).

696

fiGenerating Natural Language Descriptions OWL Ontologies

Cardinality restrictions values: set rules aggregate sentences (not
necessarily adjacent) express message triples form hS, (P ), Oi hS, P, Oi,
P , minCardinality, maxCardinality, exactCardinality.
rules applied, MaxMessagesPerSentence ignored. example, rules
perform aggregations like following.
Model 35 sold three countries. Model 35 sold least three countries. Model 35 sold
Spain, Italy, Greece.

Model 35 sold exactly three countries: Spain, Italy, Greece.

Class passive sentence: rule aggregates (i) sentence expressing message triple
hS, instanceOf, Ci hS, isA, Ci (ii) passive immediately subsequent sentence expressing single triple form hS, P, Oi, S, P (unmodified) property
ontology. subject auxiliary verb second sentence omitted.
Bancroft Chardonnay kind Chardonnay. made Bancroft.

Bancroft Chardonnay

kind Chardonnay made Bancroft.

Class prepositional phrase: second sentence involves verb
active simple present, immediately followed preposition; conditions
previous rule. subject verb second sentence omitted.
Bancroft Chardonnay kind Chardonnay. Bancroft.

Bancroft Chardonnay kind

Chardonnay Bancroft.

Class multiple adjectives: rule aggregates (i) sentence form
previous two rules, (ii) one immediately preceding subsequent sentences,
expressing single message triple hS, Pi , Oi i, S, Pi (unmodified)
properties ontology. preceding subsequent sentences must involve
verb active simple present, immediately followed adjective.
adjectives absorbed sentence (i) maintaining order.
motorbike. red. expensive.

red, expensive motorbike.

verb conjunction/disjunction: sequence sentences involving verb
form, expressing single message triple hS, Pi , Oi i,
triples Pi (unmodified) properties ontology, conjunction formed
mentioning subject verb once. omitted preposition follows.
medium body moderate flavor.
born Athens. born 1918. born Athens 1918.
medium body. moderate flavor.

similar rule applies sentences produced disjunctions message triples, illustrated below. variant first aggregation rule also applied.
house wine strong flavor medium flavor.
flavor.

house wine strong medium flavor.
697

house wine strong flavor medium

fiAndroutsopoulos, Lampouras, & Galanis

Different verbs conjunction: sequence sentences, involving
verb form, expressing message triple hS, Pi , Oi i, triples
Pi (unmodified) properties ontology, conjunction formed:
Bancroft Chardonnay dry. moderate flavor. comes Napa.

Bancroft Chardonnay

dry, moderate flavor, comes Napa.

3.2.3 Generating Referring Expressions
sentence plan may require referring expression generated message
triple hS, P, Oi. Depending context, may better, example, use nl
name (e.g., Stoa Zeus Eleutherios), pronoun (e.g., it), demonstrative
noun phrase (e.g., stoa) etc. Similar alternatives could made available O,
Naturalowl currently uses itself, datatype value; nl name O,
tokenized identifier, rdfs:label, entity class; similarly conjunctions
disjunctions O. Hence, focus referring expressions S.
Naturalowl currently uses limited range referring expressions, includes
nl names (or tokenized identifiers rdfs:label strings), pronouns, noun phrases
involving demonstrative nl name class (e.g., vase). example,
referring expressions mention properties (e.g., vase Rome)
generated. Although current referring expression generation mechanisms Naturalowl
work reasonably well, best viewed placeholders elaborate algorithms
(Krahmer & van Deemter, 2012), especially algorithms based description logics (Areces,
Koller, & Striegnitz, 2008; Ren, van Deemter, & Pan, 2010).
Let us consider following generated text, expresses triples hSi , Pi , Oi
shown below. aggregate sentences section, illustrate cases
referring expressions needed; aggregation would reduce, however, number pronouns, making text less repetitive. readers familiar ct (Section 3.1.2),
show italics noun phrase realizing cp (un ), show underlined noun phrase
realizing cb (un ), mark nocb transitions bullets.
(1) Exhibit 7 statue. (2) sculpted Nikolaou. (3) Nikolaou born Athens. (4)
born 1918. (5) died 1998. (6) Exhibit 7 National Gallery. (7) excellent
condition.
<:exhibit7,
<:nikolaou,
<:nikolaou,
<:exhibit7,

instanceOf, :Statue>
<:exhibit7, :hasSculptor, :nikolaou>
:cityBorn, :athens>
<:nikolaou, :yearBorn, "1918"^^xsd:integer>
:yearDied, "1998"^^xsd:integer>
<:exhibit7, :currentLocation, :nationalGallery>
:currentCondition, :excellentCondition>

Naturalowl pronominalizes Sn (for n > 1) Sn = Sn1 , sentences 2, 4, 5,
7. Since typically cp (ui ) = Si , obtain cp (un ) = cp (un1 ), whenever Sn pronominalized, pronoun resolved reader intended. People tend prefer readings
cp (un ) = cp (un1 ), restriction violated (e.g., gender, number, world
knowledge). helps pronouns Naturalowl generates correctly resolved
readers, even would appear potentially ambiguous. example,
pronoun sentence 7 naturally understood referring exhibit, intended to, gallery, even though neuter excellent condition.
698

fiGenerating Natural Language Descriptions OWL Ontologies

Note referents, transition sentence 6 7 continue; hence,
transition type preferences play role. gender generated pronoun gender
(most appropriate) nl name pronoun realizes.21
nl name, Naturalowl uses gender (most appropriate) nl name
specific class includes nl name (or one classes, many).
nl names also associated sets genders, give rise pseudo-pronouns
like he/she; may desirable nl name class like Person.
individuals classes, may wish use nl names, tokenized
identifiers rdfs:label strings. common, example, museum ontologies,
exhibits known particular names, many exhibits anonymous
owl identifiers particularly meaningful. Naturalowl allows domain author
mark individuals classes anonymous, indicate nl names, tokenized
identifiers, rdfs:label strings avoided. primary target marked
anonymous, Naturalowl uses demonstrative noun phrase (e.g., statue) refer
it. demonstrative phrase involves nl name specific class subsumes
primary target, nl name, marked anonymous. Especially
sentences express isA instanceOf message triples primary target,
demonstrative phrase simply this, avoid generating sentences like statue
statue. marking anonymous individuals classes currently affects
referring expressions primary target.
3.3 Surface Realization
many nlg systems, sentences end micro-planning underspecified;
example, order constituents exact forms words may unspecified.
Large-scale grammars statistical models used fill missing information
surface realization, already discussed (Section 3.2.1). contrast, Naturalowl
(and template-based nlg systems) (ordered aggregated) sentence plans
end micro-planning already completely specify surface (final) form sentence.
Hence, surface realization Naturalowl mostly process converting internal,
fully specified ordered sentence specifications final text. Punctuation
capitalization also added. Application-specific markup (e.g., html tags, hyperlinks)
images also added modifying surface realization code Naturalowl.

4. Trials
previous work, Naturalowl used mostly describe cultural heritage objects.
xenios project, tested owl version ontology created
m-piro document approximately 50 archaeological exhibits (Androutsopoulos et
al., 2007).22 owl version comprised 76 classes, 343 individuals (including cities, persons
etc.), 41 properties. xenios, Naturalowl also embedded robotic avatar
presented exhibits m-piro virtual museum (Oberlander, Karakatsiotis,
21. languages like Greek use grammatical instead natural genders, pronouns genders cannot
determined consulting ontology (e.g., check referent animate inanimate).
22. xenios co-funded European Union Greek General Secretariat Research Technology; see http://www.ics.forth.gr/xenios/.

699

fiAndroutsopoulos, Lampouras, & Galanis

Isard, & Androutsopoulos, 2008). recently, indigo project, Naturalowl
embedded mobile robots acting tour guides exhibition ancient Agora
Athens.23 owl ontology documenting 43 monuments used; 49 classes,
494 individuals, 56 properties total.
xenios indigo, texts Naturalowl eventually indistinguishable
human-authored texts. participated, however, development ontologies,
may biased towards choices (e.g., classes, properties) made easier
Naturalowl generate high-quality texts. Hence, trials discussed below, wanted
experiment independently developed ontologies. also wanted experiment
different domains, opposed cultural heritage.
goal compare texts Naturalowl simpler verbalizer. used owl verbalizer swat project (Stevens et al., 2011; Williams,
Third, & Power, 2011), found particularly robust useful.24 verbalizer produces alphabetical glossary entry named class, property,
individual, without requiring domain-dependent generation resources. glossary entry
sequence English-like sentences expressing corresponding owl statements
ontology. swat verbalizer uses predetermined partial order statements
glossary entry; example, describing class, statements equivalent classes
super-classes mentioned first, individuals belonging target class mentioned last.25 verbalizer actually translates owl ontology Prolog, extracts
lexicon entries owl identifiers rdfs:label strings, uses predetermined sentence plans specified dcg grammar. also aggregates, effect, message triples
property share one argument (S O) (Williams & Power, 2010).
hypothesis domain-dependent generation resources would help Naturalowl produce texts end-users would consider fluent coherent, compared
produced swat verbalizer, also produced Naturalowl without
domain-dependent generation resources. also wanted demonstrate high-quality
texts could produced English Greek, measure effort required
create domain-dependent generation resources Naturalowl existing ontologies. effort measured previous work, development
domain-dependent generation resources combined development
ontologies. Since time needed create domain-dependent generation resources
depends ones familiarity Naturalowl Protege plug-in, exact times
particularly informative. Instead, report figures number sentence plans,
lexicon entries etc. required, along approximate times. evaluate
23. indigo fp6 ist project European Union; consult http://www.ics.forth.gr/indigo/.
Videos robots xenios indigo available http://nlp.cs.aueb.gr/projects.html.
Two aueb students, G. Karakatsiotis V. Pterneas, Interoperability Challenge 2011
Microsoft Imagine Cup similar mobile phone application, called Touring Machine, uses
Naturalowl; see http://www.youtube.com/watch?v=PaNAmNC7dZw.
24. swat verbalizer used on-line http://swat.open.ac.uk/tools/. used generalpurpose version on-line July August 2011; similar verbalizer owl ace (Section
2) available http://attempto.ifi.uzh.ch/site/docs/owl ace.html. domain-specific version
swat snomed biomedical ontology also developed (Liang et al., 2011a, 2011b).
25. verbalizer also organizes English-like sentences glossary entry sub-headings like
Definition, Taxonomy, Description, Distinctions (Williams et al., 2011). discarded subheadings, whose meanings entirely clear us, retained order sentences.

700

fiGenerating Natural Language Descriptions OWL Ontologies

usability Protege plug-in Naturalowl, since similar authoring
tool m-piro. Previous experiments (Androutsopoulos et al., 2007) showed computer
science graduates expertise nlg could learn use effectively authoring tool
m-piro create necessary domain-dependent generation resources existing
new ontologies, receiving equivalent full-day introduction course.
4.1 Trials Wine Ontology
first trial, experimented Wine Ontology, often used Semantic
Web tutorials.26 comprises 63 wine classes, 52 wine individuals, total 238 classes
individuals (including wineries, regions, etc.), 14 properties.
submitted Wine Ontology swat verbalizer obtain glossary Englishlike descriptions classes, properties, individuals. retained descriptions
63 wine classes 52 wine individuals. Subsequently, also discarded 20
63 wine class descriptions, trivial classes (e.g., RedWine)
stating obvious (e.g., red wine defined wine color Red).27
descriptions remaining 43 wine classes 52 wine individuals, discarded
sentences expressing axioms Naturalowl consider, example sentences
providing examples individuals belong class described. remaining
sentences express owl statements Naturalowl expresses maximum
fact distance set 1. Two examples texts produced swat verbalizer follow.
Chenin Blanc (class): chenin blanc defined something wine, made grape
Chenin Blanc Grape, made grape one thing. chenin blanc flavor
Moderate, color White. chenin blanc sugar Dry Dry,
body Full Medium.
Foxen Chenin Blanc (individual): Foxen Chenin Blanc chenin blanc. Foxen Chenin
Blanc body Full. Foxen Chenin Blanc flavor Moderate. Foxen Chenin Blanc
maker Foxen. Foxen Chenin Blanc sugar Dry. Foxen Chenin Blanc located
Santa Barbara Region.

Subsequently, generated texts 43 classes 52 individuals using Naturalowl
without domain-dependent generation resources, hereafter called Naturalowl(), setting
maximum fact distance 1; resulting texts similar swats.
constructed domain-dependent generation resources Naturalowl
Wine Ontology. resources summarized Table 4. constructed
second author, devoted three days construction, testing, refinement.28
experience takes weeks (if longer) develop owl ontology size
Wine Ontology (acquire domain knowledge, formulate axioms owl, check
inconsistencies, populate ontology individuals etc.); hence, period days
26. See http://www.w3.org/TR/owl-guide/wine.rdf.
27. Third (2012) discusses owl axioms leading undesirable sentences kind might detected.
28. resources constructed editing directly owl representations, rather using
Protege plug-in, fully functional time. using fully functional
plug-in, time create domain-dependent generation resources would shorter.

701

fiAndroutsopoulos, Lampouras, & Galanis
Resources
Sections
Property assignments sections
Interest score assignments
Sentence plans
Lexicon entries
Natural language names

English

Greek
2
7
8

5
67
41





Table 4: Domain-dependent generation resources created Wine Ontology.
relatively light effort, compared time needed develop owl ontology size.
English texts generated trial; hence, Greek resources constructed.
defined one user type, used interest scores block sentences stating
obvious, assigning zero interest scores corresponding message triples; also
set maxMessagesPerSentence 3. 7 14 properties Wine Ontology used
owl statements describe 43 classes 52 individuals. defined 5
sentence plans, 7 properties could expressed sentence plans.
define multiple sentence plans per property. also assigned 7 properties
2 sections, ordered sections properties. created nl names
automatically extracted ones causing disfluencies. extracted nl names
obtained owl identifiers classes individuals; rdfs:label strings
available. reduce number manually constructed nl names further, declared
52 individual wines anonymous (and provided nl names them).
67 lexicon entries used remaining 41 nl names classes individuals;
nl names simple, 2 slots average. used Naturalowl
domain-dependent resources, hereafter called Naturalowl(+), re-generate 95 texts,
setting maximum fact distance 1; example texts follow.
Chenin Blanc (class): Chenin Blanc moderate, white wine. full medium body.
off-dry dry. made exactly one wine grape variety: Chenin Blanc grapes.

Foxen Chenin Blanc (individual): wine moderate, dry Chenin Blanc. full body.
made Foxen Santa Barbara County.

resulting 285 texts (95 3) three systems (swat verbalizer, Naturalowl(),
Naturalowl(+)) shown 10 computer science students (both undergraduates
graduate students), involved development Naturalowl;
fluent English, though native English speakers, consider
wine experts. students told glossary wines developed people
interested wines knew basic wine terms (e.g., wine colors, wine flavors),
otherwise wine experts. one 285 texts given exactly
one student. student given approximately 30 texts, approximately 10 randomly
selected texts system. owl statements texts generated
shown, students know system generated text.
student shown his/her texts random order, regardless system
generated them. students asked score text stating strongly
agreed disagreed statements S1 S5 below. scale 1 3 used (1:
disagreement, 2: ambivalent, 3: agreement).
702

fiGenerating Natural Language Descriptions OWL Ontologies
Criteria
Sentence fluency
Referring expressions
Text structure
Clarity
Interest

swat
2.00 0.15
1.40 0.13
2.15 0.16
2.66 0.13
2.30 0.15

Naturalowl()
1.76 0.15
1.15 0.09
2.20 0.16
2.55 0.13
2.14 0.16

Naturalowl(+)
2.80 0.10
2.72 0.13
2.94 0.05
2.74 0.11
2.68 0.12

Table 5: Results texts generated Wine Ontology swat verbalizer
Naturalowl (+) without () domain-dependent generation resources.
(S1 ) Sentence fluency: sentences text fluent, i.e., sentence grammatical
sounds natural. two smaller sentences combined form single, longer sentence,
resulting longer sentence also grammatical sounds natural.
(S2 ) Referring expressions: use pronouns referring expressions (e.g., wine)
appropriate. choices referring expressions (e.g., use pronoun expression instead
name object) sound natural, easy understand expressions refer to.
(S3 ) Text structure: order sentences appropriate. text presents information moving
reasonably one topic another.
(S4 ) Clarity: text easy understand, provided reader familiar basic wine terms.
(S5 ) Interest: People interested wines, wine experts, would find information
interesting. Furthermore, redundant sentences text (e.g., sentences stating obvious).29

S5 assesses content selection, first processing sub-stage; expected differences
across three systems small, reported information,
exception redundant sentences blocked using zero interest assignments Naturalowl.
S3 assesses text planning, second sub-stage; expected small differences, many
wine properties mentioned order, though properties (e.g.,
maker, location) naturally reported separately others (e.g., color, flavor),
used two sections (Table 4). S1 assesses lexicalization aggregation;
decided use separate statements two stages, since might difficult
students understand exactly aggregation takes place. S2 assesses referring
expression generation. S4 measures overall perceived clarity texts.
statement surface realization, stage rather trivial effect.
Table 5 shows average scores three systems, averages computed
95 texts system, along 95% confidence intervals (of sample means).
criterion, best score shown bold; confidence interval best score also
shown bold overlap confidence intervals.30
expected, domain-dependent generation resources clearly help Naturalowl produce fluent sentences much better referring expressions. text structure scores
show assignment ontologys properties sections ordering
sections properties greater impact perceived structure texts
expected. highest score swat verbalizer obtained clarity criterion, agrees experience one usually understand texts
swat verbalizer mean, even sentences often entirely fluent, particularly well ordered, keep repeating proper names. Naturalowl(+)had highest clarity
29. students told consider whether additional information included.
30. two intervals overlap, difference statistically significant. overlap,
difference may still statistically significant; performed paired two-tailed t-tests ( = 0.05)
cases. pilot study, also measured inter-annotator agreement two students sample
30 texts (10 system). Agreement high (sample Pearson correlation r 0.91)
five criteria. similar pilot study performed next trial, also indicating high agreement.

703

fiAndroutsopoulos, Lampouras, & Galanis

score, difference swat verbalizer, second highest score,
statistically significant. Naturalowl(+)also obtained higher interest scores
two systems, statistically significant differences both; differences,
larger expected, attributed zero interest score assignments
domain-dependent generation resources, blocked sentences stating obvious,
otherwise three systems report information.
swat verbalizer obtained higher scores Naturalowl(), text structure score exception. difference referring expression scores
two systems, though, statistically significant. systems, however, received
particularly low scores referring expressions, surprising, given
always refer individuals classes extracted names; slightly higher
score swat verbalizer probably due better tokenization owl identifiers.
4.2 Trials Consumer Electronics Ontology
second trial, experimented Consumer Electronics Ontology, owl ontology consumer electronics products services.31 ontology comprises 54 classes
441 individuals (e.g., printer types, paper sizes, manufacturers), information
particular products. added 60 individuals describing 20 digital cameras, 20 camcorders, 20 printers. 60 individuals randomly selected publicly available
dataset 286 digital cameras, 613 camcorders, 58 printers, whose instances comply
Consumer Electronics Ontology.32
submitted Consumer Electronics Ontology additional 60 individuals
swat verbalizer, retained descriptions 60 individuals. Again,
removed sentences expressing axioms Naturalowl consider. also renamed
string values datatype properties make texts easier understand (e.g.,
cmt became cm). example description follows.
Sony Cyber-shot DSC-T90 digital camera.
Sony Cyber-shot DSC-T90 manufacturer Sony.
Sony Cyber-shot DSC-T90 data interface type Usb2 0.
Sony Cyber-shot DSC-T90 depth Depth. Depth unit measurement cm. Depth value
float 9.4.
Sony Cyber-shot DSC-T90 digital zoom factor Digital Zoom Factor. Digital Zoom Factor
value float 12.1. [. . . ]
Sony Cyber-shot DSC-T90 feature Video Recording, Microphone Automatic Picture Stabilizer.
Sony Cyber-shot DSC-T90 self timer true. [. . . ]

ontology, many properties composite values, expressed using auxiliary individuals. example above, property (hasDepth) connects digital camera
auxiliary individual Depth (similar anonymous node :n property concatenation
price example page 691), connected via two properties (hasValueFloat
31. Consult http://www.ebusiness-unibw.org/ontologies/consumerelectronics/v1.
32. See http://rdf4ecommerce.esolda.com/ dataset used. list similar datasets
available http://wiki.goodrelations-vocabulary.org/Datasets.

704

fiGenerating Natural Language Descriptions OWL Ontologies

hasUnitOfMeasurement) float value 9.4 unit measurement (centimeters), respectively. obtained descriptions auxiliary individuals (e.g., Depth),
different entries glossary swat verbalizer, copied immediately corresponding sentences introduce auxiliary individuals.
also formatted text list sentences, above, improve readability.
generated texts 60 products using Naturalowl(), setting maximum
fact distance 1. Descriptions auxiliary individuals also generated copied
immediately sentences introducing them. texts similar
swat verbalizer, formatted manner.
trial, also wanted consider scenario set individuals
described changes frequently (e.g., products sold reseller change, new products arrive etc.) along changes connected individuals (e.g., new manufacturers may
added), nothing else ontology changes, i.e., assertional knowledge
changes. case, may impractical update domain-dependent generation
resources whenever population individuals changes. hypothesis considering sample individuals types described (printers, cameras, camcorders,
case), would possible construct domain-dependent generation resources (e.g.,
sections, ordering sections properties, sentence plans, nl names classes)
would help Naturalowl generate reasonably good descriptions new (unseen) individuals (products), without updating domain-dependent generation resources, using
tokenized owl identifiers rdfs:label strings new individuals nl names.
simulate scenario, randomly split 60 products two non-overlapping sets,
development set test set, consisting 10 digital cameras, 10 camcorders,
10 printers. Again, second author constructed refined domain-dependent
generation resources Naturalowl, time considering version ontology
included 30 development products, 30 test products, viewing
generated texts 30 development products only. took approximately six days (for
two languages).33 Hence, relatively light effort needed, compared time
typically takes develop ontology size, terminology two languages. Texts
30 products test set also generated using Naturalowl
domain-dependent generation resources development set.
previous trial, defined one user type, used interest scores
block sentences stating obvious. maximum messages per sentence 3.
constructed domain-dependent generation resources English Greek;
resources summarized Table 6. created sentence plans 42 properties
ontology used development set (one sentence plan per property);
test set uses two additional properties, default sentence plans Naturalowl
(for English Greek) used. also assigned 42 properties 6 sections,
ordered sections properties. created nl names automatically
extracted ones causing disfluencies development texts. Unlike previous
trial, products described declared anonymous individuals,
number nl names provided roughly previous trial,
33. Again, domain-dependent generation resources constructed editing owl representations. test, second author later reconstructed domain-dependent generation resources
scratch using fully functional Protege plug-ing, time four days.

705

fiAndroutsopoulos, Lampouras, & Galanis
Resources
Sections
Property assignments sections
Interest score assignments
Sentence plans
Lexicon entries
Natural language names

English

Greek

6
42
12
42
19
36

42
19
36

Table 6: Domain-dependent generation resources Consmer Electronics Ontology.
since fewer automatically extracted names causing disfluencies; particular,
products reasonably good rdfs:label strings providing English names.
example description development set produced Naturalowl(+)follows.
formatted sentences section separate paragraph, headed name
section (e.g., features:); easy, Naturalowl automatically
mark sections texts. maximum fact distance 1, sentence
plans caused Naturalowl automatically retrieve additional message triples describing
auxiliary individuals distance 1; hence, retrieve information
manually, unlike texts swat verbalizer Naturalowl().
Type: Sony Cyber-shot DSC-T90 digital camera.
Main features: focal length range 35.0 140.0 mm, shutter lag 2.0 0.0010 sec
optical zoom factor 4.0. digital zoom factor 12.1 display diagonal 3.0 in.
features: features automatic picture stabilizer, microphone, video recording
self-timer.
Energy environment: uses batteries.
Connectivity, compatibility, memory: supports USB 2.0 connections data exchange
internal memory 11.0 GB.
Dimensions weight: 5.7 cm high, 1.5 cm wide 9.4 cm deep. weighs 128.0 grm.

180 English texts generated three systems 30 development
30 test products shown 10 students first trial. students
told texts would used on-line descriptions products Web
site retailer. Again, owl statements texts generated
shown students, students know system generated
text. student shown 18 randomly selected texts, 9 products development
set (3 texts per system) 9 products test set (again 3 texts per system).
student shown his/her texts random order, regardless system
generated them. students asked score texts previous trial.
Table 7 shows results English texts development set.34 previous trial, domain-dependent generation resources clearly help Naturalowl produce
much fluent sentences, much better referring expressions sentence orderings.
text structure scores swat verbalizer Naturalowl()are much lower
previous trial, message triples express per individual topics, texts systems jump one topic another
making texts look incoherent; example, sentence width camera
may separated sentence height sentence shutter lag.
34. confidence interval 0.00, means students gave score texts.

706

fiGenerating Natural Language Descriptions OWL Ontologies
Criteria
Sentence fluency
Referring expressions
Text structure
Clarity
Interest

swat
1.97 0.15
1.10 0.06
1.67 0.15
1.97 0.15
1.77 0.14

Naturalowl()
1.93 0.27
1.10 0.11
1.33 0.19
2.07 0.26
1.73 0.29

Naturalowl(+)
2.90 0.08
2.87 0.08
2.97 0.04
3.00 0.00
3.00 0.00

Table 7: English development results Consumer Electronics Ontology.
Criteria
Sentence fluency
Referring expressions
Text structure
Clarity
Interest

swat
2.03 0.15
1.10 0.06
1.57 0.13
2.07 0.15
1.83 0.17

Naturalowl()
1.87 0.15
1.10 0.06
1.37 0.12
1.93 0.15
1.60 0.14

Naturalowl(+)
2.87 0.08
2.87 0.08
2.93 0.05
2.97 0.04
2.97 0.04

Table 8: English test results Consumer Electronics Ontology.
incoherence may also contributed much lower clarity scores two systems, compared previous trial. interest scores two systems also much
lower previous trial; may due verbosity texts, caused
frequent references auxiliary individuals second trial, combined lack
(or little use) sentence aggregation pronoun generation. contrast, clarity interest Naturalowl(+)were judged perfect; poor clarity interest
two systems may contributed perfect scores though. Again,
swat verbalizer obtained slightly better scores Naturalowl without domain-dependent
generation resources, except clarity, differences statistically significant.
Table 8 shows results English texts test set. results swat
verbalizer Naturalowl()are similar Table 7, one would expect.
Also, marginal decrease scores Naturalowl(+), compared
scores system development set Table 7. statistically
significant difference, however, corresponding cells two tables,
three systems. results support hypothesis considering sample
individuals types described one construct domain-dependent generation
resources used produce high-quality texts new individuals
types, rest ontology remains unchanged. fact products (but
individuals) rdfs:label strings providing English names probably
contributed high results Naturalowl(+)in test set, rdfs:label strings
kind common owl ontologies.
showed 60 Greek texts generated Naturalowl(+)to
10 students, native Greek speakers; swat verbalizer Naturalowl()cannot

Criteria
Sentence fluency
Referring expressions
Text structure
Clarity
Interest

Naturalowl(+),
development data
2.87 0.12
2.77 0.20
3.00 0.00
3.00 0.00
2.97 0.06

Naturalowl(+),
test data
2.83 0.09
2.80 0.11
3.00 0.00
2.93 0.05
3.00 0.00

Table 9: Greek results Consumer Electronics Ontology.
707

fiAndroutsopoulos, Lampouras, & Galanis
No.
1
2
3
4
5
6
7

System Configuration
Naturalowl(+)
interest scores
ref. expr. gen.
nl names
aggregation
sentence plans
sections, ordering

Sentence Fluency
4.80 0.12
4.53 0.16
3.93 0.28
3.71 0.29
3.64 0.33
2 .07 0 .37
1.89 0.36

Ref. Expressions
5.00 0.00
4.95 0.06
1 .53 0 .22
1.48 0.21
1.33 0.19
1.33 0.19
1.33 0.19

Text Structure
4.82 0.15
4.78 0.12
4.80 0.12
4.71 0.15
4.67 0.16
4.60 0.18
1 .53 0 .24

Clarity
4.78 0.12
4.62 0.17
4.51 0.24
4.24 0.25
4.24 0.25
2 .49 0 .36
2.33 0.33

Interest
4.89 0.09
4.20 0.19
4.07 0.22
3.98 0.26
3.93 0.26
2 .38 0.35
1.89 0.28

Table 10: Ablation English test results Consumer Electronics Ontology.
configuration removes one component resource previous configuration.
generate Greek texts Consumer Electronics ontology. Table 9 shows results
obtained Greek texts development test sets. statistically
significant difference corresponding results English (cf. last columns Tables 7 8). also statistically significant difference results Greek
texts development test sets (Table 9). note, however, common
use English names electronics products Greek texts, made using English
rdfs:label names products Greek texts acceptable. domains,
example cultural heritage, might unacceptable use English names individuals;
hence, one would provide Greek nl names new individuals.
4.3 Ablation Trials Consumer Electronics Ontology
last trial, studied quality generated texts affected various components domain-dependent generation resources Naturalowl gradually
removed. used Consumer Electronics Ontology, domain-dependent generation resources constructed 30 development products previous
trial. also used 45 new test products (15 digital cameras, 15 camcorders, 15 printers, publicly available dataset), 30 development 30
test products previous trial.
generated English texts 45 new test products, using 7 configurations
Naturalowl Table 10. resulting 457 = 315 texts shown 7 students,
background previous trials. student shown 7 texts 6 7
test products (42 49 texts per student). product, 7 texts shown side
side random order, students instructed take account differences
7 texts. students know system generated text.
criteria (statements S1 S5 Section 4.1) used again, scale 1 5
used time (1: strong disagreement, 2: disagreement, 3: ambivalent, 4: agreement,
5: strong agreement), make easier distinguish 7 configurations.
first configuration (Naturalowl(+)) Naturalowl components enabled, using available domain-dependent generation resources. previous
trial (see Table 8), texts configuration judged near-perfect
criteria. second configuration same, without interest score assignments.
results second configuration close results first one, since
interest score assignments used avoid generating sentences stating obvious
(e.g., Sony Cyber-shot DSC-T90 manufactured Sony). biggest decrease
interest criterion, one would expect, scores sentence fluency clarity
also affected, presumably sentences state obvious sound unnatural
708

fiGenerating Natural Language Descriptions OWL Ontologies

seem introduce noise. small differences scores referring
expressions text structure, seem suggest overall quality
texts decreases, judges biased towards assigning lower scores criteria.35
third configuration second one, component generates pronouns demonstrative noun phrases disabled, causing Naturalowl always
use nl names individuals classes, names extracted ontology.
big decrease score referring expresions, showing despite simplicity, referring expression generation methods Naturalowl noticeable effect;
mark big decreases italics Table 10. scores sentence fluency, interest,
clarity also affected, presumably repeating names individuals
classes made sentences look less natural, boring, difficult follow.
almost difference (a small positive one) text structure score.
fourth configuration, nl names individuals classes also removed, forcing Naturalowl always use automatically extracted names.
decrease score referring expressions, decrease small,
referring expressions already poor third configuration. Note, also,
nl names necessary Naturalowl produce pronouns demonstrative noun
phrases; hence, higher referring expression score third configuration would
possible without nl names. sentence fluency clarity scores also
affected fourth configuration, presumably automatically extracted names
made texts difficult read understand. also small decreases
scores interest even text structure, suggesting overall quality
texts decreases, judges biased towards lower scores criteria.
fifth configuration, aggregation turned off, causing Naturalowl produce
separate sentence message triple. sentences sharing subject
longer aggregated, referring expressions subjects generated. Since
component generates pronouns demonstrative noun phrases switched
nl names removed, repetitions automatically extracted names
used, score referring expressions decreased further. Sentence
fluency also affected, since obvious aggregations longer made,
made sentences look less natural. also small decrease score
perceived text structure interest, difference score clarity. Overall,
contribution aggregation perceived quality texts seems rather small.
sixth configuration, sentence plans removed, forcing Naturalowl
use default sentence plan tokenized property identifiers. sharp decrease
sentence fluency clarity, one would expect, also perceived interest
texts. also small decrease perceived text structure, difference
score referring expressions. Overall, results indicate sentence plans
important part domain-dependent generation resources.
seventh configuration, sections, assignments properties sections,
ordering sections properties removed, causing Naturalowl produce random
35. criteria, differences one configuration next one statistically significant, exceptions differences clarity configurations 4 5,
differences scores referring expressions configurations 56 67. Again,
95% confidence intervals overlapped, performed paired two-tailed t-tests ( = 0.05).

709

fiAndroutsopoulos, Lampouras, & Galanis

orderings message triples. sharp decrease score text
structure. scores perceived interest, clarity, also sentence fluency also
affected, suggesting overall quality texts decreases, judges
biased towards lower scores criteria.
conclude sections ordering information domain-dependent generation resources are, along sentece plans, particularly important. note, however,
best scores obtained enabling components using available
domain-dependent generation resources.

5. Conclusions Future Work
provided detailed description Naturalowl, open-source nlg system produces English Greek texts describing individuals classes owl ontologies. Unlike
simpler verbalizers, typically express single axiom time controlled, often
entirely fluent English primarily benefit domain experts, Naturalowl aims
generate fluent coherent multi-sentence texts end-users one languages.
discussed processing stages Naturalowl, optional domain-dependent generation resources stage, well particular nlg issues arise generating
owl ontologies. also presented trials performed measure effort required
construct domain-dependent generation resources extent improve resulting texts, also comparing simpler owl verbalizer requires
domain-dependent generation resources employs nlg methods lesser extent.
trials showed domain-dependent generation resources help Naturalowl produce
significantly better texts, resources constructed relatively light
effort, compared effort typically needed develop owl ontology.
Future work could compare effort needed construct domain-dependent generation resources effort needed manually edit lower quality texts produced
without domain-dependent generation resources. experience manually editing
texts generated verbalizer (or Naturalowl()) tedious large
number individuals (e.g., products) types described, editor
repeat (or similar) fixes. may be, however, particular applications
post-editing texts simpler verbalizer may preferable.
also aim replace future work pipeline architecture Naturalowl global
optimization architecture consider nlg processing stages parallel, avoid
greedy stage-specific decisions (Marciniak & Strube, 2005; Lampouras & Androutsopoulos,
2013a, 2013b). Finally, hope test Naturalowl biomedical ontologies,
Gene Ontology snomed.36

References
Androutsopoulos, I., Kallonis, S., & Karkaletsis, V. (2005). Exploiting OWL ontologies
multilingual generation object descriptions. 10th European Workshop NLG, pp. 150
155, Aberdeen, UK.
36. See http://www.geneontology.org/ http://www.ihtsdo.org/snomed-ct/.

710

fiGenerating Natural Language Descriptions OWL Ontologies

Androutsopoulos, I., Lampouras, G., & Galanis, D. (2012). Generating natural language descriptions
OWL ontologies: detailed presentation NaturalOWL system. Tech. rep., NLP
Group, Department Informatics, Athens University Economics Business, Greece.
Androutsopoulos, I., Oberlander, J., & Karkaletsis, V. (2007). Source authoring multilingual
generation personalised object descriptions. Nat. Language Engineering, 13 (3), 191233.
Angeli, G., Liang, P., & Klein, D. (2010). simple domain-independent probabilistic approach
generation. Conf. Empirical Methods NLP, pp. 502512, Cambridge, MA.
Antoniou, G., & van Harmelen, F. (2008). Semantic Web Primer. MIT Press.
Areces, C., Koller, A., & Striegnitz, K. (2008). Referring expressions formulas description logic.
5th Int. Nat. Lang. Generation Conf., pp. 4249, Salt Fork, OH.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2002).
Description Logic Handbook. Cambridge Univ. Press.
Barzilay, R., Elhadad, N., & McKeown, K. (2002). Inferring strategies sentence ordering
multidocument news summarization. Journal AI Research, 17, 3555.
Barzilay, R., & Lapata, M. (2005). Collective content selection concept-to-text generation.
Human Lang. Technology Conf. Conf. Empirical Methods Nat. Language Processing,
pp. 331338, Vancouver, British Columbia, Canada.
Barzilay, R., & Lapata, M. (2006). Aggregation via set partitioning natural language generation.
Human Lang. Technology Conf. NAACL, pp. 359366, New York, NY.
Barzilay, R., & Lapata, M. (2008). Modeling local coherence: entity-based approach. Comput.
Linguistics, 34 (1), 134.
Barzilay, R., & Lee, L. (2004). Catching drift: Probabilistic content models, applications
generation & summarization. 43rd Annual Meeting ACL, pp. 113120, Ann Arbor, MI.
Bateman, J. (1990). Upper modelling: general organisation knowledge nat. lang. processing.
5th Int. Workshop NLG, pp. 5461, Dawson, PA.
Bateman, J. (1997). Enabling technology multilingual nat. lang. generation: KPML development environment. Nat. Lang. Engineering, 3 (1), 1556.
Bernardi, R., Calvanese, D., & Thorne, C. (2007). Lite natural language. 7th Int. Workshop
Comput. Semantics, Tilburg, Netherlands.
Berners-Lee, T., Hendler, J., & Lassila, O. (2001). Semantic Web. Sc. American, May, 3443.
Bontcheva, K. (2005). Generating tailored textual summaries ontologies. 2nd European
Semantic Web Conf., Heraklion, Greece.
Bontcheva, K., & Cunningham, H. (2003). Semantic Web: new opportunity challenge
human language technology. Workshop Human Lang. Tech. SW Web
Services, 2nd Int. Semantic Web Conf., Sanibel Island, FL.
Bontcheva, K., Tablan, V., Maynard, D., & Cunningham, H. (2004). Evolving GATE meet new
challenges language engineering. Nat. Lang. Eng., 10 (3/4), 349373.
Bontcheva, K., & Wilks, Y. (2004). Automatic report generation ontologies: MIAKT
approach. 9th Int. Conf. Applications Nat. Language Information Systems, pp.
324335, Manchester, UK.
Brin, S., & Page, L. (1998). anatomy large-scale hypertextual Web search engine. Computer
Networks ISDN Systems, 30 (1-7), 107117.
Busemann, S., & Horacek, H. (1999). flexible shallow approach text generation. 9th Int.
Workshop Nat. Lang. Generation, pp. 238247, New Brunswick, NJ.
711

fiAndroutsopoulos, Lampouras, & Galanis

Chen, H., Branavan, S., Barzilay, R., & Karger, D. (2009). Content modeling using latent permutations. Journal Artificial Intelligence Research, 36, 129163.
Cheng, H., & Mellish, C. (2000). Capturing interaction aggregation text planning
two generation systems. 1st Int. Conf. Nat. Lang. Generation, pp. 186193, Mitzpe
Ramon, Israel.
Cregan, A., Schwitter, R., & Meyer, T. (2007). Sydney OWL syntax towards controlled natural
language syntax OWL. OWL Experiences Directions Workshop, Innsbruck, Austria.
Dale, R., Green, S., Milosavljevic, M., Paris, C., Verspoor, C., & Williams, S. (1998). Dynamic
document delivery: generating natural language texts demand. 9th Int. Conf.
Workshop Database Expert Systems Applications, pp. 131136, Vienna, Austria.
Dalianis, H. (1999). Aggregation nat. lang. generation. Comput. Intell., 15 (4), 384414.
Dannells, D. (2012). generating coherent multilingual descriptions museum objects
Semantic Web ontologies. 7th International NLG Conf., pp. 7684, Utica, IL.
Dannels, D. (2008). Generating tailored texts museum exhibits. Workshop Language
Technology Cultural Heritage Data Language Resources Evaluation Conf., Marrakech, Morocco.
Davis, B., Iqbal, A., Funk, A., Tablan, V., Bontcheva, K., Cunningham, H., & Handschuh, S. (2008).
Roundtrip ontology authoring. 7th Int. Conf. Semantic Web, pp. 5065, Karlsruhe,
Germany.
Demir, S., Carberry, S., & McCoy, K. (2010). discourse-aware graph-based content-selection
framework. 6th Int. NLG Conf., pp. 1725, Trim, Co. Meath, Ireland.
Denaux, R., Dimitrova, V., Cohn, A., Dolbear, C., & Hart, G. (2010). Rabbit OWL: Ontology
authoring CNL-based tool. Fuchs, N. (Ed.), Controlled Nat. Language, Vol. 5972
Lecture Notes Computer Science, pp. 246264. Springer.
Denaux, R., Dolbear, C., Hart, G., Dimitrova, V., & Cohn, A. (2011). Supporting domain experts
construct conceptual ontologies. Web Semantics, 9 (2), 113127.
Duboue, P., & McKeown, K. (2001). Empirically estimating order constraints content planning
generation. 39th Meeting ACL, pp. 172179, Toulouse, France.
Duboue, P., & McKeown, K. (2003). Statistical acquisition content selection rules natural
language generation. Conf. Empirical Methods Nat. Language Processing, pp. 121
128, Sapporo, Japan.
Duma, D., & Klein, E. (2013). Generating nat. lang. Linked Data: Unsupervised template
extraction. 10th Int. Conf. Computational Semantics, pp. 8394, Potsdam, Germany.
Elhadad, M., & Robin, J. (1996). SURGE: reusable comprehensive syntactic realization component. 8th Int. NLG Workshop, Herstmonceux Castle, Sussex, UK.
Elsner, M., Austerweil, J., & Charniak, E. (2007). unified local global model discourse
coherence. Human Lang. Technologies Conf. North American Chapter ACL, pp.
436443, Rochester, New York.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.
Funk, A., Tablan, V., Bontcheva, K., Cunningham, H., Davis, B., & Handschuh, S. (2007). CLOnE:
Controlled language ontology editing. 6th Int. Semantic Web 2nd Asian Semantic
Web Conf., pp. 142155, Busan, Korea.
Galanis, D., & Androutsopoulos, I. (2007). Generating multi-lingual descriptions linguistically
annotated OWL ontologies: NaturalOWL system. 11th European Workshop Nat.
Lang. Generation, Schloss Dagstuhl, Germany.
712

fiGenerating Natural Language Descriptions OWL Ontologies

Gatt, A., & Reiter, E. (2009). SimpleNLG: realisation engine practical applications. 12th
European Workshop NLG, pp. 9093, Athens, Greece.
Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P., & Sattler, U. (2008). OWL 2:
next step OWL. Web Semantics, 6, 309322.
Grosz, B., Joshi, A., & Weinstein, S. (1995). Centering: framework modelling local coherence
discourse. Comput. Linguistics, 21 (2), 203225.
Halaschek-Wiener, C., Golbeck, J., Parsia, B., Kolovski, V., & Hendler, J. (2008). Image browsing
natural language paraphrases semantic web annotations. 1st Workshop Semantic
Interop. European Digital Library, 5th European Semantic Web Conf., Tenerife, Spain.
Hallett, C., Scott, D., & Power, R. (2007). Composing questions conceptual authoring.
Comput. Linguistics, 33, 105133.
Halliday, M. (1994). Introduction Functional Grammar (2nd edition). Edward Arnold.
Horrocks, I., Patel-Schneider, P., & van Harmelen, F. (2003). SHIQ RDF OWL:
making Web Ontology Language. Web Semantics, 1 (1), 726.
Isard, A., Oberlander, J., Androutsopoulos, I., & Matheson, C. (2003). Speaking users languages.
IEEE Intelligent Systems, 18 (1), 4045.
Kaljurand, K. (2007). Attempto Controlled English Semantic Web Language. Ph.D. thesis,
Faculty Mathematics Computer Science, University Tartu, Estonia.
Karamanis, N., Mellish, C., Poesio, M., & Oberlander, J. (2009). Evaluating centering information
ordering using corpora. Comput. Linguistics, 35 (1), 2946.
Kasper, R., & Whitney, R. (1989). SPL: sentence plan language text generation. Tech. rep.,
Information Sciences Institute, University Southern California.
Kaufmann, E., & Bernstein, A. (2010). Evaluating usability nat. lang. query languages
interfaces Semantic Web knowledge bases. Web Semantics, 8, 377393.
Kelly, C., Copestake, A., & Karamanis, N. (2010). Investigating content selection language
generation using machine learning. 12th European Workshop Nat. Lang. Generation,
pp. 130137, Athens, Greece.
Konstas, I., & Lapata, M. (2012a). Concept-to-text generation via discriminative reranking. 50th
Annual Meeting ACL, pp. 369378, Jeju Island, Korea.
Konstas, I., & Lapata, M. (2012b). Unsupervised concept-to-text generation hypergraphs.
Human Lang. Technology Conf. NAACL, pp. 752761, Montreal, Canada.
Krahmer, E., & van Deemter, K. (2012). Computational generation referring expressions:
survey. Comput. Linguistics, 38 (1), 173218.
Lampouras, G., & Androutsopoulos, I. (2013a). Using integer linear programming content selection, lexicalization, aggregation produce compact texts OWL ontologies.
14th European Workshop Nat. Lang. Generation, 51st Annual Meeting ACL, pp. 5160,
Sofia, Bulgaria.
Lampouras, G., & Androutsopoulos, I. (2013b). Using integer linear programming concept-to-text
generation produce compact texts. 51st Annual Meeting ACL (short papers),
pp. 561566, Sofia, Bulgaria.
Langkilde, I. (2000). Forest based statistical sentence generation. 1st Conf. North American
Chapter ACL, pp. 170177, Seattle, WA.
Lavoie, B., & Rambow, O. (1997). fast portable realizer text generation systems. 5th
Conf. Applied Nat. Language Processing, pp. 265268, Washington DC.
713

fiAndroutsopoulos, Lampouras, & Galanis

Liang, P., Jordan, M., & Klein, D. (2009). Learning semantic correspondences less supervision.
47th Meeting ACL 4th AFNLP, pp. 9199, Suntec, Singapore.
Liang, S., Scott, D., Stevens, R., & Rector, A. (2011a). Unlocking medical ontologies non-ontology
experts. 10th Workshop Biomedical NLP, Portland, OR.
Liang, S., Stevens, R., Scott, D., & Rector, A. (2011b). Automatic verbalisation SNOMED classes
using OntoVerbal. 13th Conf. AI Medicine, pp. 338342, Bled, Slovenia.
Mann, W., & Thompson, S. (1998). Rhetorical structure theory: theory text organization. Text,
8 (3), 243281.
Marciniak, T., & Strube, M. (2005). Beyond pipeline: Discrete optimization NLP. 9th
Conf. Comput. Nat. Language Learning, pp. 136143, Ann Arbor, MI.
McKeown, K. (1985). Text Generation. Cambridge Univ. Press.
McRoy, S., Channarukul, S., & Ali, S. (2003). augmented template-based approach text
realization. Nat. Language Engineering, 9 (4), 381420.
Melengoglou, A. (2002). Multilingual aggregation M-PIRO system. Masters thesis, School
Informatics, University Edinburgh, UK.
Mellish, C. (2010). Using Semantic Web technology support NLG case study: OWL finds RAGS.
6th Int. NLG Conf., pp. 8593, Trim, Co. Meath, Ireland.
Mellish, C., & Pan, J. (2008). Nat. lang. directed inference ontologies. Artificial Intelligence,
172, 12851315.
Mellish, C., Scott, D., Cahill, L., Paiva, D., Evans, R., & Reape, M. (2006). reference architecture
nat. lang. generation systems. Nat. Language Engineering, 12, 134.
Mellish, C., & Sun, X. (2006). Semantic Web linguistic resource: opportunities nat.
lang. generation. Knowledge Based Systems, 19, 298303.
Nguyen, T., Power, R., Piwek, P., & Williams, S. (2012). Planning accessible explanations
entailments OWL ontologies. 7th International NLG Conf., pp. 110114, Utica, IL.
Oberlander, J., Karakatsiotis, G., Isard, A., & Androutsopoulos, I. (2008). Building adaptive
museum gallery Second Life. Museums Web, Montreal, Canada.
ODonnell, M., Mellish, C., Oberlander, J., & Knott, A. (2001). ILEX: architecture dynamic
hypertext generation system. Nat. Language Engineering, 7 (3), 225250.
Poesio, M., Stevenson, R., & Di Eugenio, B. (2004). Centering: parameter theory instantiations. Comput. Linguistics, 30 (3), 309363.
Power, R. (2009). Towards generation-based semantic web authoring tool. 12th European
Workshop Nat. Lang. Generation, pp. 915, Athens, Greece.
Power, R. (2010). Complexity assumptions ontology verbalisation. 48th Annual Meeting
ACL (short papers), pp. 132136, Uppsala, Sweden.
Power, R. (2011). Deriving rhetorical relationships semantic content. 13th European Workshop Nat. Lang. Generation, Nancy, France.
Power, R. (2012). OWL simplified english: finite-state language ontology editing. 3rd
International Workshop Controlled Natural Language, pp. 4460, Zurich, Switzerland.
Power, R., & Scott, D. (1998). Multilingual authoring using feedback texts. 17th Int. Conf.
Comp. Ling. 36th Meeting ACL, pp. 10531059, Montreal, Canada.
Power, R., & Third, A. (2010). Expressing OWL axioms English sentences: Dubious theory,
feasible practice. 23rd Int. Conf. Comput. Linguistics, pp. 10061013, Beijing, China.
714

fiGenerating Natural Language Descriptions OWL Ontologies

Ratnaparkhi, A. (2000). Trainable methods surface natural language generation. 1st Conf.
North American Chapter ACL, pp. 194201, Seattle, WA.
Rector, A., Drummond, N., Horridge, M., Rogers, J., Knublauch, H., Stevens, R., Wang, H., &
Wroe, C. (2004). OWL pizzas: Practical experience teaching OWL-DL: Common errors
common patterns. 14th Int. Conf. Knowledge Engineering Knowledge Management,
pp. 6381, Northamptonshire, UK.
Reiter, E. (1995). NLG vs. templates. 5th European Workshop Nat. Lang. Generation, Leiden,
Netherlands.
Reiter, E., & Dale, R. (2000). Building Natural Lang. Generation Systems. Cambridge Univ. Press.
Ren, Y., van Deemter, K., & Pan, J. (2010). Charting potential description logic
generation referring expressions. 6th Int. Nat. Lang. Generation Conf., pp. 115123,
Trim, Co. Meath, Ireland.
Schutte, N. (2009). Generating nat. language descriptions ontology concepts. 12th European
Workshop Nat. Lang. Generation, pp. 106109, Athens, Greece.
Schwitter, R. (2010a). Controlled nat. languages knowledge representation. 23rd Int. Conf.
Comput. Linguistics (posters), pp. 11131121, Beijing, China.
Schwitter, R. (2010b). Creating querying formal ontologies via controlled nat. language. Applied
Artificial Intelligence, 24, 149174.
Schwitter, R., Kaljurand, K., Cregan, A., Dolbear, C., & Hart, G. (2008). comparison three
controlled nat. languages OWL 1.1. 4th OWL Experiences Directions Workshop,
Washington DC.
Schwitter, R., & Tilbrook, M. (2004). Controlled natural language meets Semantic Web.
Australasian Language Technology Workshop, pp. 5562, Sydney, Australia.
Shadbolt, N., Berners-Lee, T., & Hall, W. (2006). Semantic Web revisited. IEEE Intell. Systems,
21, 96101.
Stevens, R., Malone, J., Williams, S., Power, R., & Third, A. (2011). Automatic generation
textual class definitions OWL English. Biomedical Semantics, 2 (S 2:S5).
Third, A. (2012). Hidden semantics: learn names ontology?. 7th
International NLG Conf., pp. 6775, Utica, IL.
van Deemter, K., Krahmer, E., & Theune, M. (2005). Real versus template-based natural language
generation: false opposition?. Comput. Linguistics, 31 (1), 1524.
Walker, M., Rambow, O., & Rogati, M. (2001). Spot: trainable sentence planner. 2nd Annual
Meeting North American Chapter ACL, pp. 1724, Pittsburgh, PA.
Wan, S., Dras, M., Dale, R., & Paris, C. (2010). Spanning tree approaches statistical sentence
generation. Krahmer, E., & Theune, M. (Eds.), Empirical Methods Nat. Lang. Generation, pp. 1344. Springer.
White, M. (2006). CCG chart realization disjunctive inputs. 4th Int. Nat. Lang. Generation
Conf., pp. 1219, Sydney, Australia.
Williams, S., & Power, R. (2010). Grouping axioms coherent ontology descriptions. 6th
Int. Nat. Lang. Generation Conf., pp. 197201, Trim, Co. Meath, Ireland.
Williams, S., Third, A., & Power, R. (2011). Levels organization ontology verbalization.
13th European Workshop Nat. Lang. Generation, pp. 158163, Nancy, France.

715

fiJournal Artificial Intelligence Research 48 (2013) 415473

Submitted 05/13; published 11/13

Defeasible Inheritance-Based Description Logics
Giovanni Casini

GCasini@csir.co.za

Centre Artificial Intelligence Research (CAIR)
CSIR Meraka Institute UKZN, South Africa

Umberto Straccia

umberto.straccia@isti.cnr.it

Istituto di Scienze e Tecnologie dellInformazione (ISTI)
CNR, Italy

Abstract
Defeasible inheritance networks non-monotonic framework deals hierarchical knowledge. hand, rational closure acknowledged landmark
preferential approach non-monotonic reasoning. combine two approaches
dene new non-monotonic closure operation propositional knowledge bases
combines advantages both. redene procedure Description Logics
(DLs), family logics well-suited model structured information. cases
provide simple reasoning method built top classical entailment relation
and, thus, amenable implementation based existing reasoners. Eventually,
evaluate approach well-known landmark test examples.

1. Introduction
notion rational closure (Lehmann & Magidor, 1992) acknowledged landmark
non-monotonic reasoning due rm logical properties, limited inference
capabilities: e.g. exceptional class inherit typical properties
superclass. Consider penguins: atypical birds, since y, still
share lot typical properties birds, e.g. wings. However, rational
closure may infer penguins wings. hand, Defeasible Inheritance Networks (INs) (Horty, 1994) non-monotonic framework appropriate
formalisation hierarchical knowledge limitation, exhibit
questionable logical properties (see Section 3.1).
combine two approaches dene new non-monotonic closure operation
propositional knowledge bases combines advantages both, apply
method Description Logics (DLs) (Baader, Calvanese, McGuinness, Nardi, & PatelSchneider, 2003), family logics known well-suited model structured
information.
1.1 Contributions Roadmap
Section 2.1 brief recap classical approach inheritance nets, Hortys (1994)
skeptical extension, Section 2.2 describes classical rational closure propositional
logic, generalising method presented Freund (1998). remaining material addresses
contributions summarised follows.
c
2013
AI Access Foundation. rights reserved.

fiCasini & Straccia

1. Section 3 propose new method reason INs relies procedure rational closure, present Boolean extension INs, called Boolean
defeasible Inheritance Networks (BINs).
2. Using BINs, develop Section 4 defeasible inheritance-based propositional closure
combines advantages inheritance nets rational closure.
3. Eventually, Section 5 apply latter procedure case defeasible inheritancebased description logics.
major feature procedures propose propositional logic DLs
still maintain desired logical properties rational closure, inferential
power respect exceptional classes. Moreover method requires existence
decision procedure classical entailment and, thus, implemented top
existing propositional SAT solvers DL reasoners.
Please note present paper substantially revised extended version
previous work (Casini & Straccia, 2011). Specically,
1. provide in-depth description reasoning model;
2. extensively validate approach w.r.t. series landmark (test) examples
illustrated Horty (1994) Sandewall (2010) (see Appendix A);
3. provide computational complexity results related reasoning procedures;
4. include proofs supporting major claims (see Appendix B); due
complexity notation, add also table summarising meaning
symbols use frequently (Appendix C).
1.2 Related Work
refer two main approaches non-monotonic reasoning: inheritance networks one
hand preferential approach other.
Inheritance networks developed formalism reasoning taxonomic
information. original ideas Touretzky (1986), approach richly developed
(Sandewall, 1986; Touretzky, Horty, & Thomason, 1987; Horty, Thomason, & Touretzky,
1987; Touretzky, Thomason, & Horty, 1991; Makinson, 1991; Simonet, 1996). See works
Horty (1994) Thomason (1992) overview, Gabbay Schlecta (2009)
Sandewall (2010) recently contributed eld. particular, order
evaluate proposal, shall refer skeptical approach described Horty (1987,
1994) landmark classical approach inheritance networks.
hand, Lehmann Magidors rational closure falls preferential
approach non-monotonic reasoning; approach, since rst formulation
Shoham (1988), become main representative non-monotonic reasoning (Kraus,
Lehmann, & Magidor, 1990; Lehmann & Magidor, 1992; Makinson, 1994, 2005; Freund,
1998; Bochman, 2001; Rott, 2001; Schlechta, 2004), particularly appreciated solid
logical characterization consequence relation.
Eventually, proposal shall applied eld description logics (Baader et al.,
2003). Several non-monotonic DLs exist (Baader & Hollunder, 1993; Bonatti, Faella, &
416

fiDefeasible Inheritance-Based Description Logics

Sauro, 2011c, 2011b; Brewka & Augustin, 1987; Britz, Heidema, & Meyer, 2008; Donini,
Nardi, & Rosati, 2002; Giordano, Olivetti, Gliozzi, & Pozzato, 2009; Giordano, Gliozzi,
Olivetti, & Pozzato, 2012b; Grimm & Hitzler, 2009; Knorr, Alferes, & Hitzler, 2011; Quantz
& Royer, 1992; Straccia, 1993), integrate several kinds non-monotonic reasoning
mechanism DLs. Somewhat related proposal works Britz et al. (2008)
Giordano et al. (2009, 2012b), address application preferential methods DL framework, refer rational closure. previous publication (Casini & Straccia, 2010) present procedure apply rational closure DLs,
procedure basis actual proposal, modify approach
order amplify inferential power.

2. Preliminaries
completeness, start basic notions INs propositional rational
closure shall rely on.
2.1 Defeasible Inheritance Networks
INs (Horty, 1994; Sandewall, 2010) classes (nodes), strict subsumption relation
defeasible subsumption relation among classes (links). shall indicate nodes
letters p, q . . ., shall describe pair N = hS, U i, set
strict links, U set defeasible links. Every link N direct link,
strict defeasible, positive negative. Specically,
1. p q: class p subsumed class q [positive strict link];
2. p 6 q: class p class q disjoint [negative strict link];
3. p q: element class p usually element class q [positive defeasible
link];
4. p 6 q: element class p usually element class q [negative
defeasible link].
Example 2.1. typical penguin example could represented N = h{p b}, {p 6
f, b f, b w}i, reading b Bird, p Penguin, f Flying w Wings.
presence strict links only, subsumption relation classes would
correspond simply transitive closure links: p subsumed q q
subsumed r, p subsumed r. Instead, presence defeasible links implies
possibility potential inconsistencies hierarchy classes, penguin
example: transitive closure subsumption relation would force us conclude
time penguins ying non-ying creatures. Hence reasoning
inheritance networks consists mainly deciding conclusions considered
valid faced potential contradictions. classical approaches
decisions based notions potential path preemption (a procedure that,
given two conicting paths, allows choose one resting specic information,
invalidating other). Among various proposals, shall briey present Hortys (1994)
417

fiCasini & Straccia

classical approach, shall refer landmark. First, recall notion path
shared classical approaches INs. refer paths means
Greek letters (, , , , . . .), tuples indicating sequence nodes involved.
example, triple = hp, , qi indicates path starts node p, passes
path , ends node q.
Definition 2.1 (Potential Paths, Horty, 1994, p. 117). Given net N = hS, U i,
define iteratively paths (where 99K {, } 699K {6, 6})1 :
Every direct link N simple path:
p 99K q S, hp, qi positive path.
p 699K q S, hp, qi negative path.
Assume path = ht, , pi.
positive,
p 99K q S, h, qi positive path.
p 699K q S, h, qi negative path.
negative,
q p S, h, qi negative path.
path composed strict links, strict path, otherwise defeasible
one.
potential path represents potential argumentation, decide
valid not. path strict, automatically considered valid, otherwise, case
potential conicts conclusions distinct defeasible paths, choose
ones considered valid.
Using notions path preemption, Horty denes iterative construction
extension net, is, set paths considered valid net. Due
complexity formal denition, describe roughly procedure, referring
Hortys work (1994, sect. 3), better insight.
inductive construction paths based notion degree paths.
identied potential paths, dene also notion generalised path.
is, generalisation defeasible paths that, given {, 6, , , 6}, dened as:
1. every direct link generalised path;
2. = h, pi generalised path, p q N , h, qi generalised path.
associate every defeasible path hp, , qi number corresponds number
links longest generalised path p q, denoted degN (hp, , qi).
inheritance nets, notion contradiction corresponds notion conflict:
say path conicted another path starting end
points, opposite polarity (i.e., two potential paths moving node p
1. assume link 6 symmetric, is, p 6 q S, q 6 p too.

418

fiDefeasible Inheritance-Based Description Logics

node q, one positive negative). dealing strict paths,
presence conict points actual contradiction, while, dealing defeasible
paths, contradiction potential could resolved notion preemption
(Horty, 1994, Def. 3.2.2) allows prefer path relying specic information.
procedure dened Horty start net N , and, working iteratively
degree paths, dene sequence pairs hN , Pi i, Pi set paths
considered valid i-th step. brief, given net N = hS, U i, Hortys procedure results
construction sequence sets valid paths P0 , P1 , P2 , . . . where:
1. P0 = N ;
2. Pn+1 Pn united paths degree n + 1 extension
valid path degree n preempt eventual conicting paths.

skeptical extension net N dened
n=1 Pn (Horty, 1994, sect. 2.2.2
3.3.2). indicate (positive negative, strict defeasible) connection two
nodes p q considered valid Hortys skeptical extension write N
p q,
{, 6, , 6}.
Example 2.2. Consider example 2.1; following Hortys procedure obtain skeptical
extension net composed valid paths p b, p 6 f, b f, b w, p b w.
Consistency. net considered inconsistent validate two conicting paths,
is, pair nodes p q derive positive connection
(N
p q N
p q) negative one (N
p 6 q N
p 6 q)
them.
2.2 Propositional Rational Closure
Non-monotonic systems analysed point view properties consequence relations dene (Makinson, 1994). perspective INs satisfy
desirable logical properties, presented below, (CM) (CT) (Makinson, 1994,
pp. 56-57).
Even satisfaction structural properties going present unanimously considered necessary condition formalization defeasible reasoning,
since interesting nonmonotonic logics satisfy them, still consider satisfaction desiderata: properties intuitive give back strong
logical characterization consequence relation, solid semantic characterization based preferential interpretation (Kraus et al., 1990; Lehmann & Magidor, 1992),
strong connections classic AGM approach belief revision (Alchourron,
Gardenfors, & Makinson, 1985). Moreover decision problem often reduced
procedure based classical decision problem (as proposal going
present) allowing implement procedure top existing reasoners, systems
based preferential approach rarely give back counter-intuitive results. main
problems inferential power approaches often weak (we often
point conclusions would like obtain, system able derive), preferential approach developed propositional logics,
attempts extend rst-order languages turned quite problematic.
419

fiCasini & Straccia

proposal going present tries overcome inferential weaknesses, characteristic classical preferential approaches. presenting propositional
languages, readapt procedure expressivity DLs since, even preferential approach cannot easily reformulated rst-order logics, turns still
appropriate fragments rst-order logic DLs.
follows, shall present procedure building rational closure
knowledge base using default-assumption approach (Poole, 1988; Makinson, 2005);
approach reduces construction rational closure series checks based
classical consequence relations. procedure presenting heavily relies
one Freund (1998).
Specically, consider classical propositional language built nite set propositional letters P = {p1 , . . . , pn }, using classical connectives , , , , ; sentences
denoted capital letters C, D, E . . ., sets sentences capital Greek letters
, , . . ., usual meaning true false; knowledge
bases, shall indicate consequential information means C C|D, respectively strict defeasible conditionals, read respectively C,
always C, typically D. denotes classical entailment relation, and,
given set formulae set strict conditionals , indicate
monotonic entailment relations obtained adding , respectively, set propositional
formulae set conditionals {C |= | C } extra information; shall
use | also indicate generic non-monotonic consequence relation.
knowledge base (KB) agent represented means conditionals
means formulae; call conditional knowledge base pair hT , Di, nite
set strict conditionals nite set defeasible conditionals.
Example 2.3. penguin example encoded as: K = hT , Di = {p b}
= {p|f, b|f, b|w}.
Another way formalize defeasible information may based simply formulae, using
default-assumption approach: default-assumption knowledge base pair h, i,
sets formulae representing respectively agent considers
necessarily true typically true.
Example 2.4. penguin example could encoded as: K = h, = {p b}
= {b f, p f, b w}.
shall use Greek letter distinguish default-assumption formulae (i.e.,
members ). next show map conditional knowledge base defaultassumption knowledge base (we transform KBs kind one Example 2.3
KBs kind Example 2.4), show simple procedure reason within
latter, relying decision procedure |=.
proceed follows: (i) dene notions rational consequence relation
rational closure, (ii) then, describe procedure build rational closure using
default-assumption knowledge base.
420

fiDefeasible Inheritance-Based Description Logics

consequence relation | rational satises following properties (Lehmann &
Magidor, 1992):
(REF)
(CT)

(CM)

(LLE)

(RW)

C|C every C
C|D

Reexivity

C D|F
C|F

C|D
C|F
C D|F
C|F

C|D

Cautious Monotony

|= C
D|F

|= F
C|F

Cut (Cumulative Transitivity)

Left Logical Equivalence

Right Weakening

(OR)

C|F
D|F
C D|F

Left Disjunction

(RM)

C|F
C6|D
C D|F

Rational Monotony

rst six properties, (REF)(OR), characterise class preferential consequence relations: is, given conditional base
= {C1 |E1 , . . . , Cn |En } ,
say conditional C|D preferential closure P(D) derivable
using rules (REF)(OR) (Kraus et al., 1990). However, preferential closure
generally considered inferentially weak satisfactory, natural look
stronger forms closure.
closure rule (RM) considered, interesting rules,
strongest one. However, given form rule (we negated conditional
premises), rational extension conditional base unique. Indeed,
multiple possibilities close condition: example,
= {C|F }, neither C D|F C|D choose add
either order satisfy (RM); moreover, simple example shows,
possible consequence relation obtained intersection dierent rational
extensions knowledge base satisfy (RM) anymore (in particular case,
intersection would contain neither C D|F C|D). Hence, dene rational
closure conditional base D, choose one possible rational extensions D. Lehmann Magidor dened rational closure operation R satises
set desiderata (Lehmann & Magidor, 1992, sect. 5.1-5.3).
1. P(D) R(D). is, conditional base every conditional preferentially
derivable rational closure D.
2. every conditional form C|, C| R(D) C| P(D). Analogously,
every conditional form |C, |C R(D) |C P(D). conditionals
421

fiCasini & Straccia

form C| dene situations simply considered impossible, conditionals form |C indicate considered typical general. kinds
information properly managed preferential closure.
3. C|F P(D), C|D, C D|F
/ P(D), prefer closure operation
adding C D|F instead C|D: sense rule (RM) employ constrained form monotonicity (given C|F , add C D|F ), arbitrarily add
new defaults (the addition C|D); hence, whenever possible, given conditional
C|F want consider strengthening C D|F instead unmotivated
addition conditional C|D.
shall describe Lehmann Magidors rational closure referring original
formulation (Lehmann & Magidor, 1992). Instead, shall directly refer correspondent
construction, heavily relying procedure dened Freund (1998), based
translation conditional KB default-assumption KB, illustrate next.
start conditional KB K = hT , Di. rst steps (Steps 1-3) dene
exceptionality ranking conditionals KB, following analogous procedure Lehmann Magidor (1992): ranking permit distinguish correctly
strict defeasible knowledge contained KB (Step 4), since part
strict knowledge could implicitly contained D. allow us construct
correspondent default-assumption KB (Steps 5-6). Specically:
Step 1. translate strict knowledge defeasible conditionals, is, move
KB hT , Di h, i,
= {C D| | C } .
Intuitively, preferential setting, saying C valid equivalent
saying negation absurdity ((C D)|) (Bochman, 2001, sect. 6.5).
Step 2. dene set materialisations conditionals , i.e.,
material implications corresponding conditionals:
= {C | C|D } .
Also, indicate AD set antecedents conditionals :
AD = {C | C|D } .
Step 3. dene exceptionality ranking conditionals (Lehmann & Magidor, 1992, sect. 2.6). build ranking following notion exceptionality.
Given set conditionals D, formula C exceptional preferentially
entails |C (i.e., |C P(D)); recall conditional |C R(D) |C
P(D).
conditional C|D said exceptional antecedent C exceptional D. exceptionality proposition decided based |=
(Lehmann & Magidor, 1992, Corol. 5.22), C exceptional set conditionals
422

fiDefeasible Inheritance-Based Description Logics

(i.e., |C P(D)) |= C, set materialisations
conditionals D.
Let E(AD ) indicate set antecedents result exceptional w.r.t. D,
E(AD ) = {C AD | |= C} ,
E(D) exceptional conditionals D, i.e.,
E(D) = {C|D | C E(AD )} .
Obviously, every D, E(D) D.
Step 3.1. Taking consideration knowledge base h, i, construct
iteratively sequence E0 , E1 . . . subsets conditional base following way:
E0 =
Ei+1 = E(Ei ) .
Since nite set, construction terminate (empty nonempty) xed point E, i.e., set composed exceptional conditionals,
materialisations negate antecedents.
Step 3.2. Using sequence, dene ranking function r associates
every conditional number, representing level exceptionality:
r(C|D) =




C|D Ei C|D
/ Ei+1
C|D Ei every .

Step 4. Step 3 dened materialisation rank every conditional it. Now,
Step 4.1. determine inconsistent. conditional base inconsistent derive conditional |. know
conditional form |C rational closure preferential closure, is, given result recalled Step 3.1, check
consistency using : | P(D ) |= ;
Step 4.2. consistent, dene background theory Te agent as2
Te = { C | C|D r(C|D) = } .

Moreover, one may verify every conditional logically
equivalent conditional Te ;

2. One may easily verify correctness definition referring results work Bochman
(2001, sect. 7.5.3, Definition 7.5.1, definition clash p.178, Corollary 7.5.7, Definition 7.5.2,
Lemma 7.5.5). suffices show set conditionals ranking value represents
greatest clash (the proof quite immediate definition exceptionality ranking).

423

fiCasini & Straccia

e i.e.,
Step 4.3. Te , also identify set conditionals D,

defeasible part information contained : i.e.,
e = {C|D | r(C|D) < } (obviously,
e D) .


Essentially, far moved non-defeasible knowledge hidden D,
e Moreover, ranking values
obtaining new conditional base hTe , Di.
e
conditionals D.

Step 5. build default-assumption characterization rational closure
e so, translate Te set correspondent formulae ,
e i.e.,
hTe , Di.
e = {C | C Te } ,


e sequence default-assumptions (i.e., formulae) .
e Specically, given

e
rank value conditionals D, construct sequence default assumptions
e = h0 , . . . , n ,


e
n highest rank-value D,
^
e r(C|D) i} .
= {C | C|D

(1)

Dening default-assumptions way, presented Freund (1998), obtain
set default formulae, one associated rank value, s.t. every default
formula classically derivable preceding ones, is,
|= i+1 , 0 < n .
e default-assumption set ,
e according
Step 6. Given background theory
e
e
steps dened far, associate agent pair h, i. Combining
steps main theorem Freunds work (1998, Thm. 24), shown
e
e
default-assumption characterisation agent means pair h,
equivalent rational closure pair hT , Di dened Lehmann Magidor
(1992). is,
Proposition 2.1. Given knowledge base K = hT , Di,
C|D R(K) ,
R rational closure operation defined Lehmann Magidor (1992), iff
e {i } |= D,
{C}

e (we indicate
first formula h0 , . . . , n consistent {C}
C|h,
e
e D).
424

fiDefeasible Inheritance-Based Description Logics

consequence, using following knowledge base transformations
hT , Di

e
hTe , Di

h,

e
e ,
h,

()

e
e means Proposition 2.1,
characterise rational closure hT , Di via h,
i.e.,
C|D R(hT , Di) C|h,
e
e .
So, method decide defeasible consequence rational closure. Specically, given defeasible knowledge base hT , Di propositions C D,
1. all, apply hT , Di transformations () obtain defeasible knowle i;
e
edge base h,

e
e =
2. given C, determine rst ({C} )-consistent
formula sequence
h0 , . . . , n i.
3. decide follows rational closure C w.r.t. hT , Di determining
e {i } D.
whether {C}

Example 2.5. Consider case penguin, knowledge base Example 2.3. First (Step1), move strict knowledge defeasible part, obtaining
= {p b|, p|f, b|f, b|w} .
(Step2) define set materialisations
= {p b , p f, b f, b w} ,
correspondent set antecedents
AD = {p b, b, p} .

use set materialisations determine ranking value formulae
AD conditionals (Step3), obtaining
0 = r(b) = r(b|f ) = r(b|w)
1 = r(p) = r(p|f )
= r(p b) = r(p b|) .
(Step4), define conditional base



e = hTe , Di
e ,
K
Te = { p b}
e = {p|f, b|f, b|w}


(since case strict defeasible part conditional base correctly
e K).
separated already initial base K, obtain K
425

fiCasini & Straccia

conditional base translated knowledge base

(Step5),



e
e
h,
e = {p b}

e = {0 , 1 } ,

0 = (p f ) (b f ) (b w)
1 = p f .

Using default information, conclude (Step6) penguins fly, birds fly
birds wings.
Remark 1. Considering Example 2.5, would intuitive also conclude penguins
wings (p|w), rational closure category recognized atypical,
category penguins present case (they birds, dont fly, consequently
r(p) = 1), cannot inherit typical characteristics super classes. Hence
allowed conclude that, presumably, penguins wings. weak inferential
power generally considered main limit rational closure. hand,
going see next section, INs manage successfully kind problems.
procedure determine rational closure maintains computational
complexity classical decision procedure, since easily veried transformations () require O(|K|) entailment tests and, given also proposition 2.1
fact strict part encode (monotone) propositional theory,

Proposition 2.2. Deciding propositional defeasible consequence rational closure (|, )
coNP-complete.
Lehmann Magidor (1992) specify also semantic characterization propositional rational closure, alternative correspondent construction recently
presented Giordano et al. (2012a). move propositional logic DLs, version
rational closure language ALC previously proposed (Casini & Straccia,
2010), procedure semantically characterized means preferential DL
interpretations (Britz, Casini, Meyer, Moodley, & Varzinczak, 2013).
seen above, rational closure denes non-monotonic consequence relation intuitive behaviour strong logical properties; however, Remark 1, also somewhat
weak, often conclusions exceptional situations that, despite intuitive,
cannot derive. behaviour due fact procedure associates set
premises conditionals least exceptional.
Next, going rene rational closure order avoid loss inferential
power w.r.t. exceptional premises. proposal based modication initial
knowledge base: add new conditionals give information exceptional cases
426

fiDefeasible Inheritance-Based Description Logics

would lost rational closure procedure. renement obtained using
ranking procedure, applying locally, is, order decide conditional
C|D added KB apply procedure rational closure,
consider information relevant inferential connection C
D. example, assume knowledge base composed set conditionals
= {p|q, q|r, q|t, p|t}; now, following procedure rational closure obtain
p exceptional proposition, one, cannot derive neither p|r
p|t. But, want derive p|t, already p|t, intuitively
reason avoid conclusion p|r. fact, conclusion would
desirable, since p q, p r generate conict rest
information knowledge base.
So, aim specify way decide information KB relevant
w.r.t. particular connection (in case, p|r). order determine local
relatedness going consider INs: use graphical characterisation order
identify relevant information w.r.t. connection want investigate,
apply ranking procedure pieces information recognised relevant.

3. Boolean Defeasible Inheritance Networks
present new decision procedures INs, based classical propositional decisions,
that, addition main step nonmonotonic construction going
present later on, turns interesting decision procedure per se.
following, proceed follows. rst, dene procedure INs,
map propositional logic, obtaining desired renement rational closure.
3.1 Exceptionality Levels Inheritance Nets
rst aim apply INs modied version decision procedure rational
closure; order dene method deciding validity INs rely
propositional calculus allow easily (i) extend method order include
language also propositional connectives , , , (ii) integrate rational
closure, order extend inferential power rational closure without compromising
logical properties. non-negligible side product propositional SAT-based
reasoning procedure.
shall briey review case purely strict nets showing case easily
manageable using propositional calculus, shall focus mixed nets.
3.1.1 Strict Nets
strict part nets want obtain valid connections classical
proposals. net composed strict links, i.e., N = hS, i, valid connections
consistency easily checked using propositional calculus. Indeed, dene classical
propositional language using nodes N propositional letters (call PN set
propositional letters), connectives, translate set links
set corresponding propositional implications
= {p q | p q S} {p q | p 6 q S} .
427

fiCasini & Straccia

indicate l literal (being literal propositional letter negation),
dene set antecedents implications ,
= {p | p l } .
derive valid paths using classical consequence relation 3 .
Proposition 3.1. Consider net N = hS, translate set propositional
implications . following properties hold:
1. N consistent net, valid strict positive (resp., negative) path hp, , qi
p q, N
p q (resp., N
p 6 q), iff p q (resp., p q).
2. N inconsistent iff p p .
3. Deciding strict consequence done polynomial time.
treat decision problem strict nets means classical propositional
calculus, obtaining exactly valid strict paths classical approaches nets.
Note dierence notion inconsistency INs propositional logic. seen Section 2.1, net considered inconsistent node p
that, simultaneously, positively negatively connected another node q: p
not, simultaneously, subclass q. inheritance nets, situation interpreted contradiction, propositional logic correspondent situation
( |= (p q) (p q)) would force negation propositional letter (i.e.,
node) p ( p), would correspond saying individual fall
class p.
3.1.2 Mixed Nets
consider nets strict defeasible links. follows assume
strict part net N = hS, U inferentially closed, is, N
p q (resp.,
N
p 6 q) p q (resp. p 6 q S).
procedure diers classical approaches INs mainly based
notion potential path; instead, translate nets links propositional
formulae, build exceptionality ranking using procedure similar
one dened rational closure. main dierence procedure dened
rational closure lays local characterization exceptionality rankings: check
valid connection pair nodes p q proceed dening
exceptionality ranking nodes; however, consider nodes net,
related p q. relation determined means notion
course, generalisation potential path.
Roughly, courses simply routes net following direction arrows,
without considering positive negative arrow.
Definition 3.1 (Course). Courses defined follows (where {, 6, , 6}):
3. Note strict links encoded 2-CNF formulae, also called Krom formulae,
propositional 2-SAT problem P .

428

fiDefeasible Inheritance-Based Description Logics

1. every link p q N course = hp, qi N ;
2. = h, qi course q r link N already appear ,
= h, ri course N .
omission repetitions courses needed guarantee niteness courses
even net contains cycles. So, given net N dened nite number links,
nite set C N courses, that, turn, nite sequences nodes. denote
N set courses N going node p node q, i.e.,
Cp,q
N
Cp,q
= { C N | = hp, , qi } .

next provide procedure denes validity defeasible connection
two nodes p q, via mapping propositional logic. Given net N = hS, U i,
dene correspondent knowledge base
KN = hN , N ,

N = {p q | p q S} {p q | p 6 q S}

N = {p q | p q U } {p q | p 6 q U } .
following, may omit N clear context.
dene exceptionality ranking nodes, depends decision problem
respect p q only.4
So, let
p,q = {r | r , Cp,q }
{r | r 6 , Cp,q } ,
consider set relative antecedents (l literal)
Ap,q = {a | l N p,q } .
following, N denote supra classical entailment relation obtained adding
set propositional formulae N extra axioms. strict part net,
p N q (resp., p N q), say q (resp., q) follows strictly p N ,
indicate p N q (resp., p N q).
hand, |N indicate inference relation defeasible part,
is, p|N q read member class p typically also member class
q N . Analogously p|N q negative case.
4. main difference w.r.t. procedure propositional rational closure: rank
information KB once, rank information related connection
interested in, p q.

429

fiCasini & Straccia

Now, use N p,q determine exceptionality level. investigating connection p q, node Ap,q exceptional negated
information contained p,q (compare Step 3.2 Section 2.2):
E(Ap,q ) = {a Ap,q | p,q N a}
E(p,q ) = {a b p,q | E(Ap,q )} .
Therefore, like Step 3.3 Section 2.2, build sequence
0 = Ap,q
= E(i1 ) ,
corresponding sequence
E0 = p,q
Ei = E(Ei1 ) .
Since Ap,q p,q nite, every i+1 Ei+1 Ei , sequences
terminate (empty non-empty) xed point function E, Section 2.2.
Dene ranking function (like Step 3.4) r associates every implication
p,q number, representing level exceptionality:
rp,q (a) =
/ i+1
rp,q (a) =
rp,q (a b) = (a b) Ei (a b)
/ Ei+1
rp,q (a b) = (a b) Ei .
Clearly, r(a b) = r(a) every b p,q . following, assume
obtain node ranking value (that is, function E terminates
empty set). see later (Proposition 3.6) latter case net
inconsistent.
b p,q implications b p,q least
consider set
exceptional p,
b p,q = {a b p,q | r(a b) r(p)} ,


eventually dene

b p,q p q
p|N q
N
b
p|N q p,q N p q .

language nets, indicate inference relation generated
procedure symbol . is,
N



p q

p N q

N



p 6 q

p N q

N
N



p q
p 6 q

p|N q
p|N q .



So, given N = hS, U pair nodes hp, qi, inference procedure INs
summarised follows:
430

fiDefeasible Inheritance-Based Description Logics

1. Close strict validity.
2. Check direct (and hence valid) link N connecting p q. is,
connection valid. Otherwise, proceed.
3. Determine set Cp,q courses N connecting p q, map links
Cp,q sets implications p,q , dene set Ap,q antecedents
implications p,q .
4. Determine ranking value every proposition Ap,q every implication
p,q .
b p,q implications least exceptional p.
5. Dene set

6. decide N
b p,q p q).
(



p q (N



b p,q p q
p 6 q) determining whether

Please note rely decision procedure only. examples
illustrate behaviour method.
Example 3.1. Consider Example 2.1 additional links b p (read
tweety).
w
b
f



p

Figure 1: Example 3.1
translate net following knowledge base
K = h, ,

= {t b, p, p b}

= {p f, b f, b w} .
Suppose now, want decide connected f (i.e., Tweety flies).
Since link b w appear course f ,
t,f

= {p f, b f }

At,f

= {t, b, p} ,
431

fiCasini & Straccia

obtain
t,f p t,f .
Thus,
0 = r(b) = r(b f )
1 = r(t) = r(p) = r(p f )
So,
b t,f f ,
and,

b t,f = {p f }

t|N f ,

expected.
next, ask connected w (i.e., Tweety wings). Now,
t,w = {b w}
At,w = {t, b, p} .
t,w imply negation members At,w ,
0 = r(t) = r(p) = r(b) = r(b w)


b t,w = t,w .


b t,w w,


t|N w ,

expected.
Example 3.2. Consider Nixon Diamond (see Figure 2), n Nixon, r republican, q quaker, p pacifist; another classical problem nonmonotonic
reasoning, similar previous one informed path
specific (while link p b tells us information
penguins specific information birds). So, want neither
n p n 6 p validated.
q
p

n

r

Figure 2: Nixon diamond.
432

fiDefeasible Inheritance-Based Description Logics

knowledge base K corresponding net composed
= {n r, n q}
= {r p, q p} .
want check n connected p. So, n,p = , negated antecedent n
n,p = . Since
n,p 6 n p
(n,p n): r(q p) = r(r p) = 0 r(n) = 1, i.e.,
n,p 6 n p, conclude

n 6 |N
n 6 |N

p
p .

two following examples illustrate procedure Hortys skeptical closure,
notwithstanding often manifest similar results, always give back
results, one included other.
Example 3.3. Consider net Figure 3.
f

g

x

p





n



Figure 3: Example 3.3
want investigate valid connection p. According
Hortys skeptical closure, cannot conclude anything p (N 6 p). Instead,
p), since r(a) = r(f ) = r(g) =
approach obtain a|K p (N
r(x) = 1.
Example 3.4. Consider net Figure 4.
p
c



e

f

b

Figure 4: Example 3.4
want investigate valid connection p b. According
Horty (1994) conclude N
p 6 b, approach cannot conclude anything.
433

fiCasini & Straccia

So, even many situations results two approaches same,
obtain dierent results them. dierent outcomes mainly due dierence
conicts interpreted. Consider Example 3.4, unresolved conict
two paths p f , is, one two paths preempts other,
none considered valid, Hortys approach. Hortys
interpretation, conict prevents also construction paths starting p
passing f : order constructible path built augmenting valid
shorter path, thus cannot construct path starting p passing
f (Horty, 1994, Def. 2.1.1). So, unresolved conict totally eliminates possibility
consider paths ample argumentations, could play role.
hand, approach radical conicts: fact
cannot conclude neither p f p 6 f eliminate possibility
actual world one connections true; simply enough information
decide. possibility p f eectively valid invites us take consideration
potential argumentation moving p b. So, looking connection
p b Example 3.4, Horty cannot consider path hp, c, t, f, bi, avoiding
rise conict path hp, e, bi, approach still consider possibility
hp, c, t, f eectively true, allowing path hp, c, t, f, bi play role deciding
whether valid connection p b. way potential
conict hp, e, bi prevents validity latter. signicant examples
approach see Appendix A.
Notice that, even built notion courses, procedure respects
classical notion potential path, is, every valid connection corresponds potential
path net (Denition 2.1).
Proposition 3.2. Consider net N . every connection p|N q (resp., p|N q) validated
procedure, corresponding positive (resp., negative) potential path p
q net N .
3.1.3 Inference Relation
Talking nets, structural properties characterizing rational consequence relations,
REF , CT , CM , RM , take following form5 :
(REF)
(CT)
(CM)
(RM)

N
N
N
N

p q every p q N
pq
N,p q
rs
N
rs
pq
N
rs
N,p q
rs
rs
N 6 pq
N , p6 q
rs

meaning properties still propositional case, simply readapted
expressivity INs: net represents information disposal,
premises derivation, links informational atoms language.
5. , {, 6}; N , b premises indicates addition direct link b net N ;
6 indicates opposite arrow (e.g. 6 iff 6)

434

fiDefeasible Inheritance-Based Description Logics

Hence, sense rules before. (REF) indicates whatever piece
information (link) premises, appears also conclusions. (CT) cut
condition, states validity link derived links rest
net, link eliminated without aecting set conclusions derivable
net. (CM) form constrained monotony, opposite (CT), states
whatever conclusion derived net, added premises without
aecting conclusions. (CT) (CM) intuitive appeal,
logical point view characterize
closure operation. translation (RM) less
intuitive, since INs classical notion negation,
notion conict; hence sense rule that, p q consequence N ,
addition information conicting p q, i.e., p 6 q, aect
defeasible consequences net N . fact INs share classical logic
notions contradiction negation makes formulation (RM) less intuitive
interesting.
proprieties (REF), (CT) (CM) often considered proprieties nonmonotonic consequence relation satisfy (Kraus et al., 1990; Makinson, 1994),
interesting check satised formalism. know classical
approaches inheritance nets satisfy (CT) (CM) (Makinson, 1994, pp. 56-57),
approach logically appealing.
Proposition 3.3.



satisfies (REF), (CT) (CM).

(RM) satised.
Proposition 3.4.



satisfy (RM).

following example proves proposition
Example 3.5. Consider net Figure 5. net composed links p f ,
f b, p t. N
p b N 6 b, N , 6 b 6 p b.
b
f



p

Figure 5: Counterexample RM.
example actually shows that, dealing notion negation consistency
characterize INs, (RM) look desirable property anymore, since addition 6 b net creates Nixon Diamond want derive
p b (see Example 3.2).
435

fiCasini & Straccia

properties logical consequence relations, left equivalence right weakening,
analogous following properties:
(LE)
(RW)

N

pq N
pr N
N
rq
pq
N
qr
N
pr

N

rp

also introduce property corresponds logical property supra classicality (if C D, C|D), rule satised non-monotonic consequence relations:
(Sup)

N
N

Proposition 3.5.

pq
pq


N
N

p 6 q
p 6 q

satisfies (LE), (RW ), (Sup).

3.1.4 Consistency
indicated end Section 2.1, net considered inconsistent forced
conclude pair nodes p, q p q positively negatively
connected. Since, seen above, satises (Sup), say mixed net consistent
cannot conclude N p q N p 6 q pair nodes p, q.
going see order check consistency mixed net use
ranking procedure: sucient apply whole net. propositional
case (see Section 2.2), ranking procedures dened nodes net terminates,
nite number steps, either empty set n xed point function
E, i.e., set nodes result always exceptional. case, say
nodes innite ranking value (r(p) = ). want check whether net N
consistent, sucient apply ranking procedure entire net, see
nodes innite ranking.
Proposition 3.6. net N consistent iff node p r(p) = ,
is, conclude N p q N p 6 q pair p, q.
Example 3.6. net Figure 6 example inconsistent net,
f N
6 f . net translated
would conclude N
b
f



p

Figure 6: example inconsistent net.
436

fiDefeasible Inheritance-Based Description Logics

knowledge base
= {t b, p, p b, b p}
= {p f, b f } .
proceed ranking entire net, obtain p, b
t, is, E1 = . Hence, fixed-point exceptionality ranking function,
p, b, ranking value.
3.1.5 Properties
eld inheritance networks, taxonomy dierent approaches developed basis relevant properties (Horty, 1994). briey check
satised approach.
Purely defeasible / mixed nets. Cyclic / acyclic nets. procedure deals easily
two properties often create problems traditional approaches: presence
strict defeasible links (mixed nets), presence cycles (cyclic
nets).
Credulous / skeptical / directly skeptical approaches. approach corresponds
directly skeptical approach: given net, obtain unique set valid connections
(vs. credulous approach, allows dierent sets valid paths, possibly
conict other), unique set obtained intersection
dierent possible extensions (as skeptical approaches), obtained
single closure operation.
Upward / downward chaining. denition valid paths, use
form induction length, neither starting initial node toward
terminal node (upward chaining), reverse direction (downward chaining);
hence, form chaining used procedure.
On-path / off-path preemption. O-path preemption classical form preemption, used also Horty (1994, Def. 3.2.2), on-path preemption
binding, requiring preempting node lie initial segment path preempts (Horty, 1994, sect. 4.2.4). exactly formalise form preemption,
since confront directly dierent paths two nodes. However,
procedure behaviour analogous use o-path preemption.
3.1.6 Computational Complexity
dene overall complexity decision procedure nets, consider
complexity course-identication procedure, is, given net N = hS, U
two nodes s, N , computational cost identify s,t (note easily
computed polynomial time), whose size bounded polynomially size N .
Given construction courses independent nature links (either
positive negative, defeasible strict), analyse problem using simple
directed graphs. Given net N = hS, U i, sucient dene correspondent directed
graph G = hV, Ei following way:
437

fiCasini & Straccia

V set nodes N .
E set directed links ha, bi, a, b V , s.t. ha, bi E one following
holds:
b , 6 b , b U , 6 b U .
Recall stated presence 6 b implies b 6 too. So,
6 b ha, bi hb, ai E.
dened G, let us recall well-known result graph theory saying
directed graph, given two nodes p q, determining path p q
determined time O(|V | + |E|), e.g. using BFS (Breadth First Search) (Cormen,
Stein, Rivest, & Leiserson, 2001). Now, following argument shows indeed s,t
determined polynomial time 6 . rst, check path t.
not, s,t = . Otherwise, call procedure Delta(s) below:
Delta(s): outgoing edge hs, xi s, hs, xi x marked,
do: path x mark hs, xi x, recursively,
call Delta(x).
nished, s,t immediately build marked edges. Note edge
marked node marked (i.e., explored) and, thus, algorithm
bounded polynomially size graph.
found set s,t , apply decision procedure based
propositional rational closure decide valid connection p
q (as Section 2.2, number entailment tests polynomially bounded size
net). formulae 2-CNF, like Proposition 3.1, obtain decision
procedure w.r.t. net respects complexity costs related propositional calculus.
Proposition 3.7. Deciding defeasible consequence inheritance networks (
done polynomial time.

)



Eventually, want determine valid links net N consider
pairs nodes net N . So, obtained graph G = hV, Ei repeat
procedure elements set pairs nodes graph, whose cardinality
|V |(|V | 1). Hence,
Proposition 3.8. Computing valid connections net done polynomial
time.
3.2 Boolean Inheritance Nets
next extend INs introducing classical propositional connectives , , .
Despite extension felt desirable, aware attempt
direction (Horty & Thomason, 1990).
6. interested figuring tight bound.

438

fiDefeasible Inheritance-Based Description Logics

c

c

cd

cd





Figure 7: Disjunction

Figure 8: Conjunction

3.2.1 Negation
far, used link 6 indicate two classes disjoint: p 6 q p q
logical meaning. change notation substitute 6 , indicating
p q class p class q complementary (i.e., p q), general
indicate complementary class class p p. Hence, substitute every
link p 6 q net four links: p p, q q, p q, q p. Moreover,
eliminate negative defeasible links, since p 6 q expressed p q q.
So, transform net using arrows , , . shall
continue use 6 macro indicating valid negative strict connections obtained
composition , is, indicate p 6 q presence path

. . } q ,
p
| {z. . . b} |c .{z
n arrows

n, 0.

arrows

3.2.2 Conjunction Disjunction
Next, extend inheritance nets support conjunction disjunction well, allowing
links a, b c (conjunction b equivalent c) c a, b (disjunction
b equivalent c). assume inheritance nets containing kind
links closed according following rule: a, b c (resp., a, b c)
net, also c c b (resp., c b c) net. call
nets Boolean Defeasible Inheritance Networks (BINs). shall use b b
indicate, respectively, node represents conjunction disjunction
b. Graphically, indicate disjunctive conjunctive links Figure 7 Figure 8,
respectively.
extend reasoning method BINs. so, need amplify notion
course, introducing notion duct: consider linear routes one point
another, also parallel routes, order model introduction conjunction
consequent introduction disjunction antecedent. Roughly,
= hs,


, ti


indicate duct starts node develops ducts ,
reaching node t.
439

fiCasini & Straccia

Definition 3.2 (Duct). Ducts defined follows (where {, 6, , 6}):
1. every link p q N duct = hp, qi N ;
2. = h, qi duct q r link N already appear ,
= h, ri duct N ;
3. = hq, duct r q link N already appear ,
= hr, duct N ;
t,
4. ht, , pi hr, , pi ducts, t, r S, hs, r,
, pi duct;

5. hp, , ti hp, , ri ducts, t, r S, hp, ,t
,r , si duct.
reasoning method BINs follows. Given net N = hS, U i, dene
correspondent knowledge base K = h, i,
= {p q | p q S}
{p q | p q S}
{p q r | q, r p S}
{p q r | p q, r S}
{p q | p 6 q S}
= {p q | p q U } .
Now, may proceed denition |N Section 3.1, simply considering C N
N (or simply C ) set ducts p q.
set ducts N , Cp,q
p,q
Example 3.7. Consider net N illustrated Figure 9. net N mapped
e


g





b
c
f

Figure 9: Example 3.7
KB K = h, i,

7

= {c g, f g}
= {a b, b c, b d, b e, f } .
7. ease reading, omitted redundant implications g c, obtained c,
g, g c N .

440

fiDefeasible Inheritance-Based Description Logics

Now, ask whether connected c. verified
a,c = {a b, b c, b d, f } .
Note b a,c , duct c passes c order
reach g, back towards c. Now, negated antecedent (a,c a)
and, thus,
b a,c = {a b, f } .

b a,c 6 c
b a,c 6 c,
Since

a6|N c a6|N c .

similar way, may show a6|N a6|N d. desirable result: since
f direct link, a|N f (i.e., a|N (c d)), hence know
cannot conclude a|N c a|N d. But, since evidence whether one
conclusions preferred other, conclude either them.
result skeptical approach a6|N c, a6|N c, a6|N d, a6|N d.
hand, since duct connecting e ha, b, ei (that is, nodes c, d, g, f
play role possible argumentation connecting e), conclude a|N e.
3.2.3 Properties
call BIN inference relation dened dened closure operation BINs
have:
N

BIN

p q p q

N

BIN

p 6 q p q

N

BIN

p q p|N q

N

BIN

p 6 q p|N q .

BINs inherit structural properties INs, is, (REF ), (CM ),
(CT ). Analogously, (LE), (RW ), (Sup) still valid.
Proposition 3.9.
Proposition 3.10.

BIN

satisfies (REF ), (CM ), (CT ).

BIN

satisfies (LE), (RW ), (Sup).

Since procedure dened Section 3.1 simply special case procedure
BINs, (RM) falsied also BINs counter-example proposition 3.4. Moreover, introduced conjunction disjunction, express analogous rules
disjunction premises (OR) conjunction consequent (AND):
(OR)

(AND)

N

BIN

pq N
N

N

BIN

sq N
BIN q

BIN

pq N
N

ps N
BIN p

BIN

441

BIN

p,

BIN

q,

fiCasini & Straccia

sense propositional case, remains intuitive also
BIN environment: (OR) represents validity reasoning cases, (AND)
represents conjunction distinct conclusions still valid conclusion
net.
Proposition 3.11.

BIN

satisfies (OR) (AND).

3.2.4 Consistency
Also w.r.t. consistency, obtain result INs, i.e., net consistent
ranking procedure terminates empty set.
Proposition 3.12. BIN N consistent iff node p r(p) = ,
is, cannot conclude p q p 6 q pair p, q.
Remark 2. seen Section 3.1.4 notion consistency inheritance nets different notion consistency propositional logic. Using procedure net
inconsistent if, applying ranking function entire net, obtain node
ranking value (see Proposition 3.6). point, find net inconsistent,
simply stop decision procedure.
follows going work BINs framework propositional logic.
So, order assimilate notion consistency one propositional logic,
shall consider modified version procedure BINs. Suppose
decide validity connection two nodes p, q net N . N results
consistent, proceed above, otherwise, net results inconsistent (some node
infinite ranking value) simply stop, but, case r(p) < , still apply
procedure. Otherwise, p infinite ranking value (r(p) = ), proceed
further.
3.2.5 Computational Complexity
INs, determine s,t (computing immediate) ducts
BIN. Now, dicult see recursive BFS graph travelling procedure
one devised INs worked BINs well. illustration, refer
Figure 9 assume processing node b. Since b, c reachable
d, c g S, and, recursively, path g c, mark hb, ci hb, di,
mark conjunction d, c g visited. Again, nodes aggregated
nodes visited ones, guaranteeing polynomial cost computing s,t .
Now, s,t determined polynomial time, Section 2.2,
number entailment tests polynomially bounded size net, strict
part may encode propositional formula and, thus, unlike case INs, have:
Proposition 3.13. Deciding defeasible consequence BINs (
problem.

BIN )

coNP-complete

4. Defeasible Inheritance Propositional Logic
Now, depart BINs apply similar reasoning procedure framework
propositional logic, show obtain kind closure knowledge base
442

fiDefeasible Inheritance-Based Description Logics

results rational consequence relation, informative classical
rational closure (Lehmann & Magidor, 1992).
consider propositional language , , connectives. So, start
conditional KB K = hT , Di (see Section 2.2), = {C1 D1 , . . . , Cn Dn }
= {E1 |F1 , . . . , Em |Fm }.
Step 1. Given conditional base K = hT , Di, check K preferentially consistent (that
is, check materialisation consistent; Section 2.2, Step 4.1). consistent,
dene BIN K, i.e., net NK = hSK , UK i, modelling information K
following way:
(i) consider every formula C appears antecedent succedent
conditionals K, create node C representing them, modulo
logical equivalence (that is, node C represents class formulae logically
equivalent C).
(ii) node add also, already present, complementary node (the
node representing negation), link ;
(iii) add strict links: C add strict link C net;
also add SK strict links correspond logical dependencies
formulae represented nodes w.r.t. consequence relation
. appears conditional, add net correspondent node ,
and, every node n net, add strict node n . Analogously,
add n every node n net.
(iv) eventually, C|D D, add defeasible link node C node D.
Step 2. apply reasoning procedure BINs NK (Section 3.2) identify valid
defeasible connections C|N add C|D conditional base K
obtain new conditional base K = hT , 8 .
Step 3. Finally, apply K rational closure (Section 2.2) dene nonmonotonic consequence relation |K
C|K C|D R(K ) .
Now, show
Proposition 4.1. |K rational consequence relation containing K.
Example 4.1. Consider penguin example. modify slightly order
consider also use connectives. birds (b) typically fly, live trees,
wings (f , t, w), penguins fly live trees (f t). So, knowledge
base K = hT , Di be:


= {p b}

= {b|f, b|t, b|w, p|f t} .
8. modify , since strict connections valid net classically derivable .

443

fiCasini & Straccia

Notwithstanding penguins atypicality birds, penguins wings, would like
able derive information disposal, is, would like conclude
p|w. Please note possible using classical preferential approaches,
obtain conclusion passing trough first step closure operation, is,
defining corresponding net.
Specifically, knowledge base K define net Figure 10 (the dashed arrows
strict arrows explicit conditional base, logically valid
added SK construction net NK ).
f



f


tf



p





f



p

b

w

w





b

Figure 10: Example 4.1
Using procedure defined BINs, net obtain new knowledge base
K = hT , ,

= {b|f, b|t, b|w, p|f t, p|w, p|f, p|t, b|f t} .
Note that, new conditionals
p|f,

p|t,

b|f

would present also simple rational closure K (we obtain Right Weakening), obtained also conditional
p|w ,
444

fiDefeasible Inheritance-Based Description Logics

would present rational closure K (see Remark 1).
Now, following procedure defined Section 2.2, compute rational closure
new knowledge base K , obtaining rational consequence relation contains
original K.
Please note that, using BINs, could derived anything else,
since vocabulary would limited propositions expressed nodes; however,
relying rational closure propositional knowledge bases, reason using
full expressivity propositional language, deriving new conditionals as, example,
b green|f ,
derived using BINs green appear net.
next example shows another characteristic approach. preferential
approach typicality absolute property proposition w.r.t. agents knowledge
base, is, class results atypical w.r.t. class (as penguins w.r.t. birds),
results atypical w.r.t. entire knowledge base. approach instead, typicality
comparative notion: consider class exceptional respect superclass,
absolutely typical respect another.
Example 4.2. Consider red fish (r). fish (f ) pet (p). Typically, fish
gills (g) scales (s), pets docile (d) play kids (k). Red fishes
typical pets, since play kids. So, K = hT , Di


= {r f, r p}

= {r|k, p|k, p|d, f |g, f |s} .
rational closure red fishes, since atypical pets (they play kids),
result atypical general, cannot inherit typical properties
super classes.
Instead, want red fishes inherit, besides properties pets compatible
(d), also typical properties fishes (g s), since consider
typical fishes.
so, translate knowledge base net Figure 11. net
obtain new knowledge base K = hT , i,
= {r|k, p|k, p|d, f |g, f |s, r|d, r|g, r|s}
derived exactly desired conditionals.
Next, compute rational closure K , following procedure defined Section
2.2, obtain rational consequence relation containing K information
red fishes, information that, intuitive is, would able derive
simple rational closure K.
Therefore, dened new rational consequence relation K extends K,
K R(K ), contains intuitive conditionals rational closure K.
445

fiCasini & Straccia

g

g
f
f













p


r



r





p

k



k

Figure 11: Example 4.2
Consistency. Dened inference procedure, conditional base K consistent
cannot derive |. seen (Section 2.2) rational closure conditional base
consistent preferential closure consistent (| R(K) | P(K)). Here,
given base K, obtain procedure preserves preferential consistency K:
seen Section 2.2, K preferentially consistent rational closure consistent (i.e.,
6 |rc
K ), prove following.
Proposition 4.2. Given conditional base K, |K iff | R(K).
results Section 2.2, corresponds saying |K K (assuming
K = hT , Di, K = D).
Computational Complexity. Considering procedures dened BINs,
conclude dened procedure complexity rational closure,
composition procedure dened BINs (Proposition 3.13) nal rational
closure operation (Proposition 2.2).
Proposition 4.3. propositional conditional base K, deciding C|K coNPcomplete problem.

5. Defeasible Inheritance DLs
Next, apply method signicant DL representative, namely ALC (Baader et al.,
2003, ch. 2). ALC corresponds fragment rst order logic, using monadic predicates,
called concepts, dyadic ones, called roles.
order stress parallel procedure presented Section 2.2
proposal ALC, going use notation components playing
analogous role two construction: use p, q, r, . . . concept names, C, D, E, . . .
indicate concepts general, instead, respectively, atomic propositions propositions, |= | indicate, respectively, classical consequence relation ALC
446

fiDefeasible Inheritance-Based Description Logics

non-monotonic consequence relation ALC. indicate default concept, is,
concept assume applying every individual, informed contrary.
nite set concept names C = {p, q, r, . . .}, nite set role names R =
{R, S, T, . . .} set L ALC-concepts dened inductively follows: (i) C L;
(ii) , L; (iii) C, L C D, C D, C L; (iv) C L, R R
R.C, R.C L. Concept C used shortcut C D. symbols
correspond, respectively, conjunction disjunction classical logic.
Given set individuals O, indicated bold letters a, b, c, . . ., assertion
form a:C (C L) form (a, b):R (R R), respectively indicating
individual instance concept C, individuals b connected
role R.
general inclusion axiom (GCI) form C (C, L) indicates
instance C also instance D. use C = shortcut pair
C C.
FOL point view, assertions inclusion axioms easily mapped
FOL following transformation:
(a:C) = (a, C),
(C D) = x.( (x, C) (x, D)),
(x, A) = A(x),
(x, C D) = (x, C) (x, D),
(x, R.C) = y.(R(x, y) (y, C))
(x, R.C) = y.(R(x, y) (y, C))

((a, b):R) = R(a, b),
(x, ) = (x), (x, ) = (x),
(x, C) = (x, C),
(x, C D) = (x, C) (x, D),
new variable,
new variable .

Now, classical knowledge base dened pair K = hA, i, nite
set GCIs (the TBox ) nite set assertions (the ABox ), whereas defeasible
knowledge base represented triple K = hA, , Di, additionally nite
(an instance concept C typically instance concept
set conditionals C
9
D), C, L .
Example 5.1. Consider penguin example. add role P rey vocabulary,
role instantiation (a, b):P rey read preys b, add also two
concepts, (insect) f (fish). example defeasible KB
K = hA, , Di

= {a:p, b:b, (a, c):P rey, (b, c):P rey}
= {p b, f i}
f, b f, p P rey.f i, b P rey.i} .
= {p



particular structure defeasible KB allows isolation pair hT , Di,
could call conceptual system agent, information
individuals (formalized A).
9. Since monotonic part substitute meta-linguistic conditionals C formulae C D,
substitute also defeasible part knowledge base conditionals C|D conditional
D, could call defeasible inclusion axioms.
formulae C

447

fiCasini & Straccia

follows going work information concepts hT , Di rst,
exploiting immediate analogy homonymous pair propositional setting,
address case involving individuals well. show using
method overcome limits classical rational closure, already presented
ALC (Casini & Straccia, 2010), similar way propositional case. Please note
procedure presented based slightly modied version procedure
rational closure previously presented Casini Straccia (2010), i.e., one
presented Britz et al. (2013). latter accompanied semantic characterization,
based DL interpretations preferential relation dened individuals.
semantic characterization rational closure ALC characterises steps procedure (the local applications rational closure nal one). However, still lack
semantic characterization overall procedure, accounting also modularization
knowledge base done using INs.
Step 1. Given conceptual system K = hT , Di, check preferential consistency, is,
dene


= {C | C }

D}
= {C | C

construct BIN NK K. process one Section 4,
treat concepts propositions: nodes NK represent concepts appearing
antecedents consequents inclusion axioms (modulo logical
equivalence); every node add complementary node, already present,
connect ; every GCI C becomes strict link C D;
becomes defeasible link C D.
every defeasible inclusion axiom C
Moreover, consider consequence relation monotonic consequence relation
obtained adding GCIs , add net strict links representing
logical dependencies nodes respect 10 .
Step 2. Apply reasoning procedure BINs NK (Section 3.2) identify
conditional base
valid defeasible connections C D, add C


K obtain new conditional base K = hT , i.
Now, augmented knowledge base new defeasible conditionals,
proceed follows.
| C }.
Step 3. Dene = {C
} let = {C | C }.
Step 4. Dene = {C | C



Step 5. Determine exceptionality ranking conditionals using set
antecedents AD materialisations
, concept C exceptional

w.r.t. set conditionals |= C. steps propositional case (Step
3 Section 2.2) replacing expression |= C

expression |= C. way dene ranking function r.
10. order create strict part net possible use techniques introduced procedure
classification DL knowledge bases (Baader et al., 2003, ch. 9).

448

fiDefeasible Inheritance-Based Description Logics

Step 6. Step 4.1, Section 2, verify KB consistent, checking consistency
. (as Steps 4.2 - 4.3 Section 2.2), dene sets
r(C D) = }
Te = { C | C





e
= {C | C r(C D) < } .

e ,
e
Step 7. Dene (similarly Step 5 Section 2.2) sets concepts


=

l

e = {C | C }

e = {0 , . . . , n } ,



e r(C
{C | C




D) i} .

Section 2, every , 0 < n, |= i+1 .
Step 8. Now, dene inference relation |K
C|K |= C

l

e D,


e
rst {C} -consistent
formula11 sequence h0 , . . . , n i.
DL analogue Step 6, Section 2.2.
Again, steps require decision procedure classical entailment relation |=
DLs. redene properties characterizing rational consequence relation
framework DLs.
show
Proposition 5.1. |K rational consequence relation containing K.
is, analogous properties propositional rational consequence relation
satised, namely:
(REF) C|K C
(LLE)

(CT)

C |K E

|=T C =

|K E

C |K E

(OR)
11. is, 6|= C

(RW)

C |K

(CM)

C |K E
C |K E

|K E

(RM)

C |K E
de
.

449

C |K

|=T E

C |K E
C |K E

C |K

C |K E
C |K

C 6 |K E

C E |K

fiCasini & Straccia

Example 5.2. Consider example 5.1, additionally also add role Born (Born(x, y)
read x born y), concept e (Egg). Consider
K = hT , Di ,



= {p b, f i}

P rey.f P rey., b P rey.i P rey., b Born.e} .
= {p



p

b




Born.e

p



Born.e

b

P rey.f P rey.

P rey.i P rey.



P rey.f P rey.

P rey.i P rey.

Figure 12: Example 5.2
(Step 1), build correspondent net NK (figure 12), obtain (Step 2)

Born.e} .
= {p
move rational closure. pair hT , changed (Step 3)
, f ,
= { p b

P rey.f P rey., b P rey.i P rey.,
p

Born.e, p Born.e} .
b


set materialisations (Step 4)
= {p b , f , p P rey.f P rey.,
b P rey.i P rey., b Born.e, p Born.e}
AD = {p b, f i, p, b} .
obtain (Step 5) exceptionality ranking conditionals:
, f , p P rey.f P rey.,
E0 = {p b


P rey.i P rey., b Born.e, p Born.e}
b


, f , p P rey.f P rey., p Born.e}
E1 = {p b



, f }
E2 = {p b

, f } = E .
E3 = {p b
2


450

fiDefeasible Inheritance-Based Description Logics

get ranking values every conditional : namely,
P rey.i P rey.) = r(b Born.e)
0 = r(b

P rey.f P rey.) = r(p Born.e)
1 = r(p

) = r(i f ) .
= r(p b


ranking, obtain (Steps 6-7) background theory
Te = { (p b), (i f i)} ,

e = {0 , 1 },
default-assumption set

0 = (b P rey.i P rey.) (b Born.e)

1

(p P rey.f P rey.) (p Born.e)
= (p P rey.f P rey.) (p Born.e)

used definition consequence relation |K (Step 8).
example, derive typical penguins preys fishes, i.e.,
p|K P rey.f ,
insects, i.e.,
p|K P rey.i ,
also penguins born eggs, i.e.,
p|K Born.e ,
would derivable rational closure, presented Casini Straccia (2010).
Computational Complexity. computational complexity point view, deciding entailment ALC ExpTime-complete (Donini & Massacci, 2000) and,
Section 2.2, number entailment tests polynomially bounded size
knowledge base, following exactly procedures dened propositional case,
conclude
Proposition 5.2. Deciding C|K ALC ExpTime-complete problem.
5.1 Closure Operation Individuals
far left ABox, going consider here. procedure
ABox built procedure TBox, is, consider knowledge base
hA, , Di strict knowledge already moved , i.e.,
axioms ranking value (that is, correspond sets Te
e obtained using procedure previous section). basic idea following

procedure consider individual named ABox much typical possible,
is, associate possible defeasible information consistent
rest knowledge base. order apply defeasible information locally
451

fiCasini & Straccia

individual, encode information using materialisations inclusion axioms,
e section
i.e., set = h1 , . . . , n i, s.t. |= i+1 1 < n (the set
12
). want able associate individual (with set
individuals named ABox) strongest formula consistent
e = , i, call
knowledge base. way dene new knowledge base K
ABox extension knowledge base hA, , Di.
Definition 5.1 (ABox extension). Given knowledge base K = hA, , Di, knowledge
base , ABox extension K = hA, , Di iff
, classically consistent AD .
AD \ composed assertions a:C C = i,
every h s.t. h < i,
hT , AD {a:h }i |=
denition identies extensions original ABox s.t. every
individual associated defeasible information consistent rest
knowledge base. Still, main problem that, since individuals related
roles, possibility associating default concept individual
inuenced default information associated individuals, shown
following example.
AR.A}
Example 5.3. Consider K = hA, Di, = {(a, b):R} = D0 = {
(hence = h0 = hA R.Ai). associate 0 a, obtain b:A
cannot associate 0 b; hand, apply 0 b, derive b:A
anymore able associate 0 a. Hence, define two possible rational extensions
K.


implies that, given knowledge base hA, , Di, even closure hT , Di
always unique possibility one ABox extensions.
simple procedure obtain possible extensions knowledge base hA, , Di,
set individuals named A, following:
Definition 5.2 (Procedure ABox extensions).
Consider set linear orders individuals O;
= ha1 , . . . , do:
Set j := 1
Set AsD :=
Repeat j = + 1:
Find first default hAsD {aj :i }, }i 6|= .
AsD := AsD {aj :i }.
e
e must done
12. Note that, given conditional knowledge base, procedure determine Te ,
all.

452

fiDefeasible Inheritance-Based Description Logics

j =j+1
return hAsD ,
indicate AsD ABox extension obtained using sequence s.
Now, shown
Proposition 5.3. Given linear order individuals K, procedure determines ABox extension K. Vice-versa, every ABox extension K corresponds
knowledge base generated linear order individuals O.
e1 = hA{a:A R.A},
instance, related Example 5.3, obtain extension K
e
order ha, bi, K2 = hA {b:A R.A}, obtained order hb, ai.
Example 5.4. Refer Example 5.1, let K = {A, , D},
= {a:p, b:b, (a, c):P rey, (b, c):P rey}
= {p b, f i}
f, b P rey.i, p f, p P rey.f i}
= {b




knowledge base define set = {0 , 1 }

0 = (b f ) (b P rey.i) (p f ) (p P rey.f i)
1 = (p f ) (p P rey.f i) .
consider order comes b, associate 1 a, consequently c presumed fish prevented association 0 b.
consider b a, c fish cannot apply 1 a.
Now, x priori linear order individuals, say a:C
e |= a:C, K
e
defeasible consequence K w.r.t. order s, written K a:C, K
ABox extension generated K based order s.

instance, related Example 5.3 order s1 = ha, bi, may infer K s1 a:A,
order s2 = hb, ai, may infer K s2 b:A.
interesting point consequence relation still satises properties
rational consequence relation following way.
453

fiCasini & Straccia

(REFDL )

hA, , Di a:C every a:C

(LLEDL )

hA {b:D}, , Di a:C
D=E
hA {b:E}, , Di a:C

(RWDL )

hA, , Di a:C
CD
hA, , Di a:D

(CTDL )
(CMDL )

hA {b:D}, , Di a:C
hA, , Di b:D
hA, , Di a:C
hA, , Di a:C
hA, , Di b:D
hA {b:D}, , Di a:C

(ORDL )

hA {b:D}, , Di a:C
hA {b:E}, , Di a:C
hA {b:D E}, , Di a:C

(RMDL )

hA, , Di a:C
hA, , Di 6 b:D
hA {b:D}, , Di a:C

show
Proposition 5.4. Given K linear order individuals K, consequence
relation satisfies properties (REFDL ) (RMDL ).
Note computational complexity point view, entailment w.r.t. ALC TBox
ExpTime-complete (Donini & Massacci, 2000) number individuals K linearly bounded |K|, get immediately
Proposition 5.5. Deciding K a:C ALC ExpTime-complete problem.
presence multiple ABox extensions, also dene inference relation
, conservative inference relation independent order individuals,
corresponds intersection inference relations modelling rational
extension.
=

\

{ | linear order elements O}

However, possibility lose property rational monotonicity,
shown following example.
Example 5.5. Consider knowledge base hA, Di s.t. = {(a, b):R} = D0 D1 ,
R.A, B} = {A B, R.A B} (where
D0 = {
1
0
1



sets conditionals rank 0 conditionals rank 1, respectively).
define two sequences individuals, = ha, bi = hb, ai,


defining different rational extension ( ), let = .
B
hA, Di a:B, since extensions a:B holds (in axiom





a:A.
6
axiom r.A B) hA, Di 6 a:A, since hA, Di
However, hA {a:A}, Di 6 a:B, since hA {a:A}, Di 6 a:B.
454

fiDefeasible Inheritance-Based Description Logics

increase computational complexity decision procedure: assuming number individuals named ABox n, perform -test
possible sequences dened n individuals. is, worst case
k
need n! tests, done time O(2|K| ) k. Now,
2
shown that13 n! < 2n and, thus, decision problem remains ExpTime.
Proposition 5.6. Deciding K a:C ALC ExpTime-complete problem.
Notwithstanding, conjecture many (probably most) real-world cases,
knowledge base would single rational ABox extension, cases (RMDL )
still valid. check whether knowledge base hA, , Di single rational ABox extension, sucient associate individual strongest modulo consistency
w.r.t hA, , Di, exactly procedure Denition 5.2, consistency
check aj :i w.r.t. original instead w.r.t. AsD . end, check whether
hAsD , consistent; case obtained rational ABox extension
hA, , Di.
following knowledge base unique ABox extension.
Example 5.6. Consider KB Example 5.4, (b, c):P rey replaced (b, d):P rey.
Then, whatever order individuals, obtain following association
default formulae individuals: a:1 , b:0 , c:0 , d:0 . Using information
defaults, obtain unique default-assumption extension.

semantic characterization , making use preferential DL models,
presented Casini et al. (2013).
lets briey consider heuristics useful case want present
system specic ABox queries. Assume want know particular individual
presumably falls concept C, want draw safest possible conclusion.
presence multiple acceptable extensions, classical solution use skeptical
approach, i.e., use inference relation , corresponding intersection
inference relations associated possible ordering individuals appearing A.
seen above, case multiple rational extensions computational complexity decision problem rise w.r.t classical ALC decision problem.
Moreover, case multiple extensions, amount defeasible information associable
individual inuenced individuals related means role:
immediate see role-connection ABox two individuals
b, information associated inuence amount
defeasible information associate b, way around. Hence,
ease decisions w.r.t. ABox introducing notion cluster, i.e., set
individuals named ABox linked means sequence role connections.
so, given ABox A, indicate Q symmetric transitive
closure
roles vocabulary, i.e., symmetric transitive closure R.
Definition
5.3 (Cluster). Define Q reflexive, symmetric transitive closure

R. Given individual O, call cluster set [a] individuals

13. shown induction n, see e.g.http://lifecs.likai.org/2012/06/better-upper-bound-forfactorial.html.

455

fiCasini & Straccia

connected Q:
[a] = {b | Q(a, b)} .
Hence, order know presumably conclude a, sucient
determine w.r.t. sequence individuals [a]. Let A[a] ABox obtained restricting statements containing individuals [a]; query a:C clearly decidable
using A[a] .
Proposition 5.7. hA, , Di a:C iff hA[a] , , Di a:C every ordering
individuals A[a] .
query individual s.t. named ABox (a
/ O),
constraints dened ABox a, i.e., know a:; hence,
individual appearing ABox, associate strongest default
concept consistent , 0 : s.t.
/ O, derive presumably
a:C holds hAa , |= a:C, Aa = {a:0 }.

6. Comparison Related Work
non-monotonic logics, so-called preferential approach distinguished
various proposals (as Reiters defaults, modal approaches, defeasible inheritance. . . ) mainly due logical properties, since former approach committed
satisfaction desirable structural properties consequence relation (see Section
2.2). hand, considering point view inferential capacity
preferential approach often results weaker proposals, since often
desirable, intuitive conclusions cannot derive (see Remark 1).
proposal tried combine classical rational closure inheritance
networks order overcome inferential limits without prejudicing logical properties
consequence relation.
Section 3 also present alternative way reason defeasible inheritance.
Despite proposal presented mainly integrate propositional language
rational closure, results interesting approach per se, Appendix
compare Hortys classical skeptical extension (Horty, 1994, sect. 2-3) Sandewalls
landmark examples (Sandewall, 2010).
indicated introduction, many papers aimed implementation non-monotonic reasoning DL formalisms. proposals
comparison approach done considering dierent non-monotonic formalisms, independently DL-environment. refer Makinsons work (1994)
comparison various non-monotonic approaches.
last years main proposals implementation nonmonotonic reasoning
DLs connected two approaches: preferential one circumscription.
preferential approach, work Britz al. (Britz et al., 2008; Britz, Meyer,
& Varzinczak, 2011) preferential DL semantics strongly connected approach,
one results semantic characterization rational closure cited
(Britz et al., 2013).
Still close approach work Giordano et al. (2012b), based
preferential approach. conclusions derive using logic ALC+Tmin
456

fiDefeasible Inheritance-Based Description Logics

intuitive, complexity decision problem ABox co-NExpNP (Giordano et al., 2012b, Thm. 13), procedure cannot reduced classical entailment.
Among proposals based circumscription, work Bonatti et al. (2009)
particularly representative. point view quality inferences,
proposal results dicult w.r.t. preferential approach draw expected
conclusions. example, assume knowledge base contains information
mammals typically live land, whales abnormal mammals live
land, ABox contains information a:M ammal W hale. knowing
anything else individual a, would like reasoning system reason
assumption dealing typical mammal (since, moreover, specied
whale) hence able derive lives land. However, using
circumscription, conclusions draw changes w.r.t. concepts user decides
keep fixed varying (a non-trivial choice), results able
derive a:Habitat.Land, able derive it, even derive whales
exist (Bonatti et al., 2009, sect. 2.1). proposal instead, formalize
problem knowledge base hA, , Di = {a:M ammal} (we need specify
whale), = {W hale ammal, W hale Habitat.Land} =
Habitat.Land}; without needing kind choice user, system
{M ammal
derive automatically a:Habitat.Land. Moreover, seen computational
cost procedure involving ABox exponential, circumscription case,
languages analogous ALC, complexity instance problem co-NExpNP (Bonatti
et al., 2009, sect. 4.1.1). issues discussed addressed solved
Bonatti et al. (2010, 2011a), circumscriptive systems specically
built low-complexity DLs EL.
hand, procedures based circumscription able derive defeasible
information individuals implicit ABox, is, can, exam presumably a:R.D. procedure involving
ple, conclude a:R.C C
ABoxes able make kind derivation, since add defeasible information
individuals named ABox. working renement
procedure order deal also implicit individuals; rst attempt take
consideration also individuals proposed previous publications (Casini &
Straccia, 2010, pp. 9-10), adding completion procedure ABox order explicitly
name ABox implicit individuals, procedure needs rened.
Among proposals regarding introduction probabilistic reasoning DLs,
Lukasiewicz (2008) presents combination nonmonotonic probabilistic reasoning.
nonmonotonic part based preferential approach, presents construction augments inferential power rational closure. Let us consider proposal eliminating probabilistic part, i.e., considering conditionals associated
probability 1 (conditionals (|)[1, 1] notation), since convey meaning
defeasible conditionals. procedure seems give back results
cases, two proposals dier general approach: proceeds
renement ranking structure, use renement content knowledge base. fact, behaviour two dier. Consider example knowledge
b, d, b c, c e, e} (in Lukasiewiczs notation would
base h, Di, = {a




correspond knowledge base empty TBox set P containing conditionals
457

fiCasini & Straccia

(b|a)[1, 1], . . . ).14 construction derive a|c, since simply duct ha, b, ci
c, Lukasiewiczs approach cannot, since consider follows three preferred subset knowledge base consistent
b, d, b c, c e}, {a b, d, b c, e}, {a b, d, c e, e}),
(that {a











a|c follow latter.

7. Conclusions
combining classical rational closure ideas defeasible inheritance networks,
proposed new rational consequence relation overcomes limits
formalisms. so, extended defeasible inference capabilities allowing
atypical class still inherit properties superclass maintaining
desired logical properties rational closure. table summarizes structural
properties satised systems taken consideration:15

REF
CT
CM
LE
RW

RM

Horty












BIN







PL








DL








see, proposals defeasible inheritance-based propositional logic
Description Logics still satisfy axioms classical rational closure. Another feature
method uniquely require existence decision procedure classical
entailment and, thus, implemented top exiting propositional SAT solvers
DL reasoners. Since introduced procedure interesting also point
view inheritance nets, presented comprehensive procedure logical
knowledge bases making use nets formalism; notwithstanding, condent
procedure reformulated avoiding shift one formalism another.
procedure presented ALC straightly extended languages expressive ALC; hand, present procedure needs language
closed propositional connectives, hence need augment expressivity
language order apply less expressive DL languages EL, forcing way
increase computational costs w.r.t. classical decision problem.
Hence, looking adaptation procedure appropriate dealing tractable,
less expressive DLs one main open problems present proposal, together
proper semantic characterization procedure ability reasoning
defeasible way implicit individuals, discussed previous section.
14. example corresponds structure Example.A.4 Appendix A, eliminating link
center figure.
15. IN, BIN, PL, DL stand proposals INs, Boolean INs, propositional logic DLs, respectively.

458

fiDefeasible Inheritance-Based Description Logics

Appendix A. Examples
going validate decision procedures inheritance nets signicant
examples proposed Horty (1994) Sandewall (2010). shall use BINs, but, order
simplify graphical representation, shall also use 6 macro, explained
Section 3.2. side eect, obtain analogue behaviour propositional DL
cases too.
Example A.1 (Horty, 1994, ex. 12 ). Consider net Figure 13. Horty claims
desirable conclusion a|p, since environment mixed nets [. . . ] certain
kinds compound defeasible paths legitimately thought carry immediate information - namely, paths consisting single defeasible link followed strict end
segment, length (Horty, 1994, p. 143). corresponds condition (RW ),
that, seen Section 3.1.2, procedure satisfies.
r


p





Figure 13: Example A.1
Indeed, translate net KB K = h, i, = {a s, t, r p}
= {s r, p}. a,p = , a,p |= s, a,p |= a.
implication a,p ranking grater 0 r, and, since r p,
a|K p.
Example A.2 (Horty, 1994, ex. 18 ). Consider net Figure 14. example Horty
claims prefer negative path ha, p, si positive path ha, r, si,
since negative link 6 q nullifies path p r. procedure satisfies
claim.
p


q



r

Figure 14: Example A.2
Indeed, translate net KB K = h, i, = = {a p,
r, p q, q r, r s, q, p s}. a,s = , a,s p, a,p a.
459

fiCasini & Straccia

Therefore, E1 = {a p, r, q, p q, p s}. Now, E1 a,
node rank 2, E2 = {a p, r, q}. conclude neither a|s
a|s.
Example A.3 (Sandewall, 2010, ex. B.1, Double Diamond). Consider net N = {S, U }
defined (see Figure 15)
= {s s, r r}
U

= {a t, p, s, p s, r, p q, q r}

r







r

q


p

Figure 15: Double diamond.
net N translated knowledge base K = {, } = {} = {a
t, p, s, p s, r, p q, q r}.
Using method, net derive neither a|r, a|r, be.
However, derive a|q, desirable result derivable Sandewalls
approach.
Example A.4 (Sandewall, 2010, ex. B.2, Simonets Scenario). Consider net N = {S, U }
defined (Figure 16)
= {e e}
U = {a b, d, b c, c e, c, e} .
c
e

b




e


Figure 16: Simonets scenario.
460

fiDefeasible Inheritance-Based Description Logics

net N translated knowledge base K = {, } = {} = {a
b, d, b c, c e, c, e}.
Using method, net derive a|e, be, Sandewall cannot
derive it. Moreover, derives neither a|c b|e.
Example A.5 (Sandewall, 2010, ex. B.3, On-Path vs. O-Path Preemption). Consider
net N = {S, U } defined (see Figure 17)
= {wa 6 ga}
U

= {c re, c ce, e, ce e, e ga, wa} .
e
ce

c

ga

wa


Figure 17: On-Path vs O-Path.
net N translated knowledge base K = {, } = {wa ga}
= {c re, c ce, e, ce e, e ga, wa}.
connection c wa valid use form off-path preemption,
on-path preemptions derivable. setting, derive c|wa: even cannot
consider method form preemption (at least explicitly), said Section 3.1.5
method gives back results analogous off-path preemption.
following examples use also conjunctions, going use BINs.
Example A.6 (Sandewall, 2010, ex. B.4, Juvenile Oender). original Juvenile offender example represented following net (see Figure 18) N = {S, U },
= {b g, b m, p p, m, g g}
U

= {g p, p} ,

p read punished, g guilty, minor, b Billy. want
express priority g, b N w. Since nets
possible express conjunctions, solve problem adding link g p
m, g g.
net translated knowledge base K = {, } = {b g, b m}
= {g p, p, g p}. knowledge base derive b|p, expected.
Example A.7 (Sandewall, 2010, ex. B.5, Campus Residence Scenario). example A.6,
Campus residence example necessity include priorities. solve
461

fiCasini & Straccia


mg

b



p

g

p

Figure 18: Juvenile oender.
problem inserting conjunctions. Indeed, net N = {S, U } composed (see
Figure 19)
= {t ma, e, m, w 6 n, ma, e e}
U = {ma e w, n} ,
read married, e employed, w lives west apartments,
male, n lives northern apartments, Tom. want express e
priority m, w results valid. so, add links mmae w,
m, e e.

e
w



e

e

n


Figure 19: Campus residence scenario.
net translated knowledge base K = {, } = {t ma, e,
m, w n} = {ma e w, n, e w}. knowledge base
derive t|w, expected.
treat last example propositional problem. simpler net
graphical point view put negation nodes useless
resolution, use three-place conjunction link, macro construction
conjunction three nodes.
462

fiDefeasible Inheritance-Based Description Logics

Example A.8 (Sandewall, 2010, ex. B.6, Good Math Student Scenario). knowledge
base K = {T , D} composed (see Figure 20)


= {t gs, f m, mb, aa ag}

= {mb|ag, f ag|mm, mm mb gs|aa, f m|pm} ,

gs

mb
mm mb gs



ag
aa

f ag
mm
fm
pm

Figure 20: Good math student.
Sandewall identifies following candidates valid conclusions:
t|ag t|aa t|f ag
t|mm t|mm mb t|mm mb gs

t|pm .

cannot conclude conditionals above. point net highly
interconnected, especially strict links, nodes ranking 0, except
mm mb gs ranking 1. Therefore, conclusions above,
premise, cannot obtained.

Appendix B. Proofs
Proposition 3.1. Consider net N = hS, translate set propositional
implications . following properties hold:
1. N consistent net, valid strict positive (resp., negative) path hp, , qi
p q, N
p q (resp., N
p 6 q), iff p q (resp., p q).
2. N inconsistent iff p p .
463

fiCasini & Straccia

3. Deciding strict consequence done polynomial time.
Proof. First, easy prove induction length paths that,
positive (resp., negative) path p q, p q (resp., p q). Hence
have:
1a) N consistent net valid strict positive (negative) path (p, , q)
p q, p q ( p q).
Moreover, N inconsistent, conclude, p, q it, p q
p 6 q, p q p q, is, p. one half
second statement.
2a) 6 p every p , N consistent.
move show halves statements.
1b) N consistent net p q (resp., p q), valid strict
positive (resp., negative) path (p, , q) p q.
order model classical consequence relation , use propositional resolution method. use symbol R indicate inferences resolution
method.
Every element forms p q p q. implications correspond,
clause form, respectively clauses (i.e., sets literals) {p, q} {p, q}.
Call set clauses corresponding . example, assume
R {p, q} (that is, p q), set clauses , {p}, {q} resolves
empty set (that is, , (p q) ).
1. Positive case (N
p q): Assume p q p, q PN . order
apply refutational approach, p q negated, and, since (p q)
equivalent p q, introduce clauses {p}, {q}. So, resolution
approach p q translated , {p}, {q} R . also assumed
consistent, hence 6R . Since set composed
pairs literals (of form {p, q} {p, q}), every reduction step
gives back pair literals (for example, {p, q} {q, r},
obtain {p, r}). Therefore, order obtain empty set need use
{p} {q} refutation procedure. So, clause {q} must
eliminated using clause containing q. clause must necessarily
form {ri , q} ri , obtain new clause {ri }. Again, {ri }
eliminated clause form {rj , ri }, obtaining clause {rj }. Since
set nite, procedure terminate, done
obtain clause {p}, resolved {p}. is,
clause {p, rl }, rl , s.t. {rl } appears reduction procedure.
Now, clauses used reduction process correspond chain
links net S: p rl , . . . , ri q, dene valid path (p, , q) S.
2. Negative case (N
p 6 q): assume p q p, q PS . p q
negated translated clauses {p}, {q}. {q} combined
464

fiDefeasible Inheritance-Based Description Logics

clause form {q, ri } (that represents q ri , that, turn, represents
link q 6 ri ) (case 1), clause form {q, ri } (i.e., q ri , that,
turn, q ri ) (case 2).
Case 1. obtain clause {ri }, procedure positive
case. clauses used reduction process correspond strict negative path form: p rl , . . . , rj ri , ri 6 q.
Case 2. obtain clause {ri }, would combine clause
form {ri , rj }, form {ri , rj }. former case move case
1, ending strict negative path p rh , . . . , rl rj , rj 6 ri , ri q.
latter case new clause {rj }, case
2; however, procedure terminate, and, order terminate,
reduction procedure point move case 1.
clauses used reduction procedure correspond path form p
ri , . . . , rj rk , rk 6 rl , rl rm , . . . , rn q, link 6 corresponds
shift case 1.
2b) Automatically, also N consistent, 6 p every p .
Otherwise, would p q p q nodes p q,
would imply, procedure above, inconsistency N .
3) third point, consider strict links encoded 2-CNF formulae,
also called Krom formulae, propositional 2-SAT problem P .

Proposition 3.2. Consider net N . every connection p|N q (resp., p|N q) validated
procedure, corresponding positive (resp., negative) potential path p
q net N .
b p,q . procedure states p| q
b p,q p q (and
Proof. Dene set
N
b p,q p q). Following procedure analogous one proof
p|N q
proposition 3.1, show derivation implications connected
presence positive (negative) potential paths net.
Proposition 3.3.



satisfies (REF ), (CT ) (CM ).

prove proposition need rst prove following lemma.
p q ( {, 6}). Call set
Lemma B.1. Consider net N = hS, U s.t. N
material implications corresponding links S, consequence relation
obtained adding formulae extra-axioms. Consider net N obtained
N
adding link p q N . Then, every pair nodes r, N ,
r,s

N


r,s equivalent w.r.t. .

Proof. two nets contain nodes. Given two nodes r s, two possible
N , C N = C N , consequently
N p q C N . p q
/ Cr,s
cases: p q
/ Cr,s
r,s
r,s
r,s
N . p q C N , C N C N , consequently C N C N ,
N =

r,s
p,q
r,s
r,s
r,s
r,s
r,s
465

fiCasini & Straccia





N C N one hand C N C N
correspondent sets courses two nets (Cp,q
p,q
r,s
r,s
other) contain exactly nodes links, apart, possibly, p q.
N
N
N
corresponds saying N
p,q p,q one hand, r,s r,s
contain exactly implications, apart, possibly, p lq (where lq {q, q}),
exactly set antecedents.
p q,
N
N
N
Since N
p,q p lq , consequently p,q p lq . So, p,q


N
N
N
N
p,q -equivalent, also r,s r,s -equivalent. So, since r,s

N
r,s -equivalent contain set antecedents, generate
N equivalent w.r.t. .
N
ranking, is,
r,s
r,s

prove Proposition 3.3.
Proof. satisfaction (REF ) trivial, since every direct link trivially valid.
(CT ): assume N p q N , p q r s. call N net obtained adding p q

N
N . N r means
r,s r ls (ls {s, s}). lemma B.1
N r ls , is, N r s.
N equivalent w.r.t. , also
N

r,s
r,s
r,s


(CM ): assume N
p q N
r s. call N net obtained adding p q
N r ls . lemma B.1
N
N
N . N r means
r,s
r,s
r,s

r s.
N
equivalent w.r.t. , also

r

l
,

is,
N
,
p

q

r,s
Proposition 3.5.



satisfies (LE), (RW ), (Sup).

Proof. proofs immediate, considering procedure fact implications corresponding strict links always present decision procedure.
Proposition 3.6 net N consistent iff node p r(p) = , is,
conclude N p q N p 6 q pair p, q.
proved combining following two lemmas.
Lemma B.2. Given net N , two nodes p q it, conclude p|N q
p|N q iff r(p) = .
Proof. left right. immediate denition ranking procedure:
r(p) 6= , set implications least ranking p imply p,
cannot imply p q p q q.
right left. r(p) = , set E 16 implications (with p
antecedents) E p. implies E p q E p q
whichever q, is, p|N q p 6 |N q.
Lemma B.3. Every node ranking set courses C ranking value
every set courses extending C.
Proof. immediate monotony consequence relations N .
16. E set -ranked defaults.

466

fiDefeasible Inheritance-Based Description Logics

lemmas conclude node p innite ranking net, net
inconsistent (p|N q p|N q p q), and, conversely, net inconsistent,
node p innite ranking.
Proposition 3.9

BIN

satisfies (REF ), (CM ), (CT ).

Proof. proof retraces one proposition 3.3.
Proposition 3.10

BIN

satisfies (LE), (RW ), (Sup).

Proof. proof retraces one proposition 3.5.
Proposition 3.11

BIN

satisfies (OR) (AN D).

Proof. (OR): N BIN p N BIN q t, p, q S. Assume
=. case =6 analogous.
p,t p
q,t q t.
N BIN p N BIN q t, is,
Since N BIN p N BIN q t, proposition 3.2 least duct
p one q t. So, since p, q s, duct t. Moreover,
since least duct p q s, every duct connecting
allows duct connecting p q t. Hence, s,t p,t s,t q,t
holds.
hand, that, since p, q s, connected connections moving p q. So, p,t s,t q,t s,t . Hence,
p,t = q,t = s,t holds ranking functions rs,t , rp,t , rq,t
work sets material implications.
propositional level, (p q) either rs,t (p) rs,t (q)
rs,t (q) rs,t (p). Assume former (the reasoning applies case).
q,t
p,t . Moreover, least set exceptionality order
rs,t (p) rs,t (q)
negating p negate also q, least set exceptionality order negating
p,t =
s,t . So,
s,t p t, and, since
q,t
p,t ,
t, is, rs,t (t) = rs,t (p),


also s,t q t; hence classical reasoning obtain s,t (p q) t,
s,t t. Eventually, N BIN t.
i.e.,
(AN D): N BIN p q N BIN p s, s, q S. Every
duct connecting p q p part duct connecting p addiction
s, q t, every duct connecting p part duct connecting p q (resp.,
p s) addiction q (resp., s). So, p,q = p,s = p,t , obtain
N BIN p t.
Proposition 3.12 BIN N consistent iff node p r(p) = ,
is, cannot conclude p q p 6 q pair p, q.
Proof. proof similar one proposition 3.6.
Proposition 4.1. |K rational consequence relation containing K.
Proof. |K obtained rational closure K , rational consequence
relation. Moreover, R(K ) contains K , and, since K K , contains K too.
467

fiCasini & Straccia

Proposition 4.2. Given conditional base K, |K iff | R(K).
Proof. sucient prove that, given conditional base K = hT , U i, obtained
extension K = hT , U (Steps 1-3), K preferentially consistent conditional base K
preferentially consistent too.
right left proof immediate, since K K . left right, K
6 . Given every conditional added K corresponds
consistent means
implication classically derivable subset D,
C(T D) (where C classical closure operation associated ).
C(T ) = C(T D). Then,
/ C(K ), K consistent knowledge base too.
Proposition 5.1 |K rational consequence relation containing K.
Proof. sucient show (1) every inclusion axiom valid |K ,
(2) |K satises properties characterizing rational consequence relation.
(1) construction, C , C Te (that is, modulo logical
equivalence, Te ). Assume C|D D. either r(C) = r(C|D) = ,
r(C) = r(C|D)= i, < . rst case C Te ,
e D, implies C| D. second case,
{C}
K
C D, default-assumption associated premise C.
e {i } D, is, C| D.
Hence {C}
K

(2) consequence relation |K satises properties rational consequence relation.
(REF ) obviously satised, (LLE) is. (RW ), assume C|K D.
e {i } D, that, given
means rst C-consistent , {C}
e
e E, implies {C} {i } E, i.e., C|K E.

e E, rst C
(CT ) C D|K E corresponds {C D}
D-consistent formula . Analogously, C|K means rst Ce {j } D. Since C C,
consistent j , {C}
j i, is, j . Hence, {C D} {j } E
e {j } D, and, since satises (CT ), {C}
e {j } E,
{C}
is, C|K E.
e {i } E.
(CM ) C|K E means rst C-consistent , {C}
e {i } . Hence,
Analogously, C|K means {C}
e
consistent CD, and, monotony , {CD}{
i}
E, is, C D|K E.
e {i } E.
(OR) C|K E means rst C-consistent , {C}
e
Analogously, D|K E means {D} {j } E rst D-consistent
j . three options: j = i, j < < j. rst case,
default-assumption associated C , and, since satises OR,
{C D} {i } E, is, C D|K E. j < i, j ,
e
j rst CD-consistent default . Hence {D}{
j}
e
E, and, monotonicity, {C} {j } E. Since satises OR,
e {j } E, is, C D| E.
{C D}
K
468

fiDefeasible Inheritance-Based Description Logics

e
(RM ) C 6 |K E corresponds 6 {C} {
} E, rst C-consistent
e {i },
formula . means E consistent {C}
e
rst C E-consistent formula . Since {C} {i } D,
e {i } D, is, C E| D.
monotonicity {C E}
K
Proposition 5.3 Given linear order individuals K, procedure determines
ABox extension K. Vice-versa, every ABox extension K corresponds knowledge
base generated linear order individuals O.
Proof. rst statement quite immediate. second statement, assume
rational extension hA , hA, , Di cannot generated sequence
elements O. associates every individual x default concept ,
indicate x .
Now, consider generic rational extension , hA, , Di generated
using sequence elements O. following procedure allows dene sequence
elements s.t. , generated using s, i.e., , = hAsD , i.
Take element associate strongest default concept consistent
knowledge base hA, (call x ). Look individual x s.t. x = x ,
consider x rst element sequence s. Update x:x , repeat procedure, every individual associated default formula. procedure
generate sequence dominion individuals generates ,
hA, , Di.
Since sequence generate hA , i, procedure fail,
is, point possible associate x default x s.t. x = x .
means that, remaining x, x 6= x ; x, either x x
x x . rst case possible, since hA , would inconsistent ( x
maximally consistent default). Hence x x x 6= x remaining x.
case, hA , would rational extension hA, , Di, since could another
consistent model stronger defaults associated individuals.
Proposition 5.4 Given K linear order individuals K, consequence
relation satisfies properties (REFDL ) (RMDL ).
Proof. REFDL , LLEDL RWDL proof quite immediate. CTDL
CMDL , assume hA, , Di b:D, hAsD , b:D. Hence, b:D consistent
hAsD , i; implies rst individual sequence s, let a,
every , a:i consistent hA, consistent hA {b:D}, i,
procedure associates default formula either start A{b:D}.
happens following individuals sequence s.
hAsD {b:D}, = h(A {b:D})sD , hAsD {b:D}, a:C h(A {b:D})sD ,
a:C. Since satises CT CM , hAsD , a:C h(A{b:D})sD , a:C,
is, hA, , Di a:C hA {b:D}, , Di a:C.
ORDL , assume hA {b:D}, , Di a:C, hA {b:E}, , Di a:C,
b nth position sequence s. So, rst n 1 elements association
default-formulae models. b, assume procedure
469

fiCasini & Straccia

assigns b:i case b:D, b:j case b:E. either = j , j ,
j . rst case procedure assignment defaults continues
way knowledge bases, also b:D E,
is, hA {b:D}, , Di, hA {b:E}, , Di, hA {b:D E}, , Di completed exactly
defaults, obtaining, respectively, ABoxes (A {b:D})sD = {b:D},
(A {b:E})sD , = {b:E}, (A {b:D E})sD = {b:D E}, ABox
. {b:D} a:C {b:E} a:C, and, since satises OR,
obtain {b:D E} a:C, is, h(A {b:D E})sD , a:C. j
b:D E, procedure associates b strongest two defaults, is, . Since
consistent b:E, every following consistency check procedure
forced consider b:D holds, assignment defaults individuals
proceed case b:D holds, hA {b:D E}, , Di entail
formulae hA {b:D}, , Di. Analogously, j , default-assumption extension
hA {b:D E}, , Di correspond one hA {b:E}, , Di.
Finally, RMDL , b:D consistent hAsD , i, presence b:D
knowledge base inuence association defaults individuals,
AsD (A {b:D})sD . Eventually, hAsD , a:C implies h(A {b:D})sD , a:C, i.e.
hA {b:D}, , Di a:C.
Proposition 5.7 hA, , Di a:C iff hA[a] , , Di a:C every ordering individuals A[a].
Proof. proof quite immediate. Assume hA[a] , , Di 6 a:C s. Let
sequence individuals named obtained using initial segment. Hence

hA, , Di 6 a:C, implies hA, , Di 6 a:C.
assume hA, , Di 6 a:C. Hence, sequence hA, , Di
6 a:C. Let

restriction individuals named A[a] ; hA, , Di
6 a:C.

Appendix C. Table Main Symbols
Since paper considers dierent elds, notation turned quite complex.
add table summarize main symbols used paper.

N
Cp,q

IN/BIN
nodes
strict conditional
defeasible conditional
consequence relation
conjunction, disjunction
links courses/ducts
p q

PL
atoms
propositions
strict conditional
defeasible conditional
links ducts
p q

DL
concept names
concepts
individuals
strict conditional
defeasible conditional
links ducts
p q

N
p,q

materialisations
N
links Cp,q

materialisations
N
links Cp,q

materialisations
N
links Cp,q

N

p,q

conditionals N
p,q
exceptional p

conditionals N
p,q
exceptional p

conditionals N
p,q
exceptional p

p, q, . . .
C, D, . . .
a, b, . . .

|
,

470

fiDefeasible Inheritance-Based Description Logics

References
Alchourron, C., Gardenfors, P., & Makinson, D. (1985). logic theory change:
Partial meet contraction revision functions. Journal Symbolic Logic, 50, pp.
510530.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). Description Logic Handbook: Theory, Implementation, Applications.
Cambridge University Press.
Baader, F., & Hollunder, B. (1993). prefer specic defaults terminological
default logic. Proceedings IJCAI-93, pp. 669674. Morgan Kaufmann Publishers.
Bochman, A. (2001). logical theory nonmonotonic inference belief change. SpringerVerlag.
Bonatti, P. A., Lutz, C., & Wolter, F. (2009). complexity circumscription description logic. Journal Artificial Intelligence Research, 35, pp. 717773.
Bonatti, P. A., Faella, M., & Sauro, L. (2010). EL default attributes overriding.
Patel-Schneider, P. F., Pan, Y., Hitzler, P., Mika, P., Zhang, L., Pan, J. Z., Horrocks,
I., & Glimm, B. (Eds.), International Semantic Web Conference (1), Vol. 6496
Lecture Notes Computer Science, pp. 6479. Springer.
Bonatti, P. A., Faella, M., & Sauro, L. (2011a). Adding default attributes EL++ .
Burgard, W., & Roth, D. (Eds.), Proceedings AAAI-11. AAAI Press.
Bonatti, P. A., Faella, M., & Sauro, L. (2011b). Defeasible inclusions low-complexity
DLs. Journal Artificial Intelligence Research, 42, pp. 719764.
Bonatti, P. A., Faella, M., & Sauro, L. (2011c). complexity el defeasible
inclusions. Proceedings IJCAI-11, pp. 762767. AAAI Press/IJCAI.
Brewka, G., & Augustin, D. S. (1987). logic inheritance frame systems.
Proceedings IJCAI-87, pp. 483488. Morgan Kaufmann Publishers.
Britz, K., Casini, G., Meyer, T., Moodley, K., & Varzinczak, I. (2013). Ordered Interpretations Entailment Defeasible Description Logics. Tech. rep., CAIR, CSIR
Meraka UKZN, South Africa.
Britz, K., Heidema, J., & Meyer, T. A. (2008). Semantic preferential subsumption.
Brewka, G., & Lang, J. (Eds.), Proceedings KR-08, pp. 476484. AAAI Press.
Britz, K., Meyer, T., & Varzinczak, I. J. (2011). Semantic foundation preferential
description logics. Wang, D., & Reynolds, M. (Eds.), Australasian Conference
Artificial Intelligence, Vol. 7106 Lecture Notes Computer Science, pp. 491500.
Springer.
Casini, G., Meyer, T., Moodley, K., & Varzinczak, I. (2013). Nonmonotonic reasoning
description logics. Rational closure ABox. Proceedings DL-13, pp. 7790.
CEUR Workshop Proceedings.
Casini, G., & Straccia, U. (2010). Rational closure defeasible description logics.
Janhunen, T., & Niemela, I. (Eds.), Proceedings JELIA-10, Vol. 6341 Lecture
Notes Computer Science, pp. 7790. Springer.
471

fiCasini & Straccia

Casini, G., & Straccia, U. (2011). Defeasible inheritance-based description logics. Proceedings IJCAI-11, pp. 813818.
Cormen, T. H., Stein, C., Rivest, R. L., & Leiserson, C. E. (2001). Introduction Algorithms
(2nd edition). McGraw-Hill Higher Education.
Donini, F. M., & Massacci, F. (2000). EXPTIME tableaux ALC. Artificial Intelligence,
124 (1), pp. 87138.
Donini, F. M., Nardi, D., & Rosati, R. (2002). Description logics minimal knowledge
negation failure. Transactions Computational Logic, 3 (2), pp. 177225.
Freund, M. (1998). Preferential reasoning perspective Poole default logic. Artificial
Intelligence, 98 (1-2), pp. 209235.
Gabbay, D. M., & Schlechta, K. (2009). Defeasible inheritance systems reactive diagrams. Logic Journal IGPL, 17 (1), pp. 154.
Giordano, L., Gliozzi, V., Olivetti, N., & Pozzato, G. L. (2012a). minimal model semantics
nonmonotonic reasoning. Proceedings JELIA-12, Vol. 7519 Lecture Notes
Computer Science, pp. 228241. Springer.
Giordano, L., Gliozzi, V., Olivetti, N., & Pozzato, G. L. (2012b). non-monotonic description logic reasoning typicality. Artificial Intelligence, 195, pp. 165202.
Giordano, L., Olivetti, N., Gliozzi, V., & Pozzato, G. L. (2009). ALC+T: preferential
extension description logics. Fundam. Inform., 96 (3), 341372.
Grimm, S., & Hitzler, P. (2009). preferential tableaux calculus circumscriptive ALCO.
Proceedings RR-09, pp. 4054. Springer-Verlag.
Horty, J. F. (1994). direct theories nonmonotonic inheritance. Handbook
logic artificial intelligence logic programming: nonmonotonic reasoning
uncertain reasoning, Vol. 3, pp. 111187. Oxford University Press.
Horty, J. F., & Thomason, R. H. (1990). Boolean extensions inheritance networks.
Proceedings AAAI-90, pp. 633639. AAAI Press.
Horty, J. F., Thomason, R. H., & Touretzky, D. S. (1987). skeptical theory inheritance
nonmonotonic semantic networks. Proceedings AAAI-87. AAAI Press.
Knorr, M., Alferes, J. J., & Hitzler, P. (2011). Local closed world reasoning description
logics well-founded semantics. Artificial Intelligence, 175 (9-10), 15281554.
Kraus, S., Lehmann, D., & Magidor, M. (1990). Nonmonotonic reasoning, preferential
models cumulative logics. Artificial Intelligence, 44 (1-2), pp. 167207.
Lehmann, D., & Magidor, M. (1992). conditional knowledge base entail?.
Artificial Intelligence, 55 (1), pp. 160.
Lukasiewicz, T. (2008). Expressive probabilistic description logics. Artificial Intelligence,
172 (6-7), pp. 852883.
Makinson, D. (1994). General patterns nonmonotonic reasoning. Handbook logic
artificial intelligence logic programming: nonmonotonic reasoning uncertain
reasoning, Vol. 3, pp. 35110. Oxford University Press.
472

fiDefeasible Inheritance-Based Description Logics

Makinson, D. (2005). Bridges Classical Nonmonotonic Logic. Kings College Publications.
Makinson, D., & Schlechta, K. (1991). Floating conclusions zombie paths. Artificial
Intelligence, 48, pp. 199209.
Poole, D. (1988). logical framework default reasoning. Artificial Intelligence, 36 (1),
2747.
Quantz, J., & Royer, V. (1992). preference semantics defaults terminological logics.
Proceedings KR-92, pp. 294305.
Rott, H. (2001). Change, Choice Inference: study belief revision nonmonotonic
reasoning. Oxford University Press.
Sandewall, E. (1986). Nonmonotonic inference rules multiple inheritance exceptions.
Proceedings IEEE-86, pp. 13451353.
Sandewall, E. (2010). Defeasible inheritance doubt index axiomatic characterization. Artificial Intelligence, 18 (174), pp. 14311459.
Schlechta, K. (2004). Coherent Systems. Elsevier.
Shoham, Y. (1988). Reasoning change: time causation standpoint
artificial intelligence. MIT Press.
Simonet, G. (1996). sandewalls paper: Nonmonotonic inference rules multiple inheritance exceptions. Artificial Intelligence, 86, pp. 359374.
Straccia, U. (1993). Default inheritance reasoning hybrid KL-ONE style logics. Proceedings IJCAI-93, 676681.
Thomason, R. H. (1992). NETL subsequent path-based inheritance theories.
Lehmann, F. (Ed.), Semantic Networks Artificial Intelligence, pp. 179204. Pergamon Press.
Touretzky, D. S. (1986). mathematics inheritance systems. Pitman.
Touretzky, D. S., Horty, J. F., & Thomason, R. H. (1987). clash intuitions: current
state nonmonotonic multiple inheritance systems. Proceedings IJCAI-87 Vol. 1, pp. 476482. Morgan Kaufmann Publishers.
Touretzky, D. S., Thomason, R. H., & Horty, J. F. (1991). skeptics menagerie: conictors,
preemptors, reinstates, zombies nonmonotonic inheritance. Proceedings
IJCAI-91, pp. 478483. Morgan Kaufmann Publishers.

473

fiJournal Artificial Intelligence Research 48 (2013) 253-303

Submitted 11/12; published 10/13

Optimizing SPARQL Query Answering OWL Ontologies
Ilianna Kollia

ilianna2@mail.ntua.gr

University Ulm, Germany
National Technical University Athens, Greece

Birte Glimm

birte.glimm@uni-ulm.de

University Ulm, Germany

Abstract
SPARQL query language currently extended World Wide Web
Consortium (W3C) so-called entailment regimes. entailment regime defines
queries evaluated expressive semantics SPARQLs standard simple
entailment, based subgraph matching. queries expressive since
variables occur within complex concepts also bind concept role names.
paper, describe sound complete algorithm OWL Direct Semantics entailment regime. propose several novel optimizations strategies
determining good query execution order, query rewriting techniques, show
specialized OWL reasoning tasks concept role hierarchy used reduce
query execution time. determining good execution order, propose cost-based
model, costs based information instances concepts roles
extracted model abstraction built OWL reasoner. present two
ordering strategies: static dynamic one. dynamic case, improve
performance exploiting individual clustering approach allows computing
cost functions based one individual sample cluster.
provide prototypical implementation evaluate efficiency proposed
optimizations. experimental study shows static ordering usually outperforms
dynamic one accurate statistics available. changes, however,
statistics less accurate, e.g., due nondeterministic reasoning decisions. queries
go beyond conjunctive instance queries observe improvement three
orders magnitude due proposed optimizations.

1. Introduction
Query answering important context Semantic Web since provides mechanism via users applications interact ontologies data. Several query
languages designed purpose, including RDQL (Seaborne, 2004), SeRQL
(Broekstra & Kampman, 2006) and, recently, SPARQL. paper, consider
SPARQL query language (Prudhommeaux & Seaborne, 2008), standardized
2008 World Wide Web Consortium (W3C) currently extended
SPARQL 1.1 (Harris & Seaborne, 2013). Since 2008, SPARQL developed main
query language Semantic Web supported RDF triple stores.
query evaluation mechanism defined SPARQL Query specification based
subgraph matching. form query evaluation also called simple entailment since
equally defined terms simple entailment relation RDF graphs
(Hayes, 2004). SPARQL 1.1 includes several entailment regimes (Glimm & Ogbuji, 2013)
c
2013
AI Access Foundation. rights reserved.

fiKollia & Glimm

order use elaborate entailment relations, induced RDF Schema
(RDFS) (Brickley & Guha, 2004) OWL (Motik, Patel-Schneider, & Cuenca Grau, 2012b;
Schneider, 2012). Query answering entailment regimes complex
may involve retrieving answers follow implicitly queried graph,
seen OWL ontology using OWL entailment. several implementations
SPARQLs RDFS entailment regime available (e.g., Oracle 11g (Oracle, 2013), Apache
Jena (The Apache Software Foundation, 2013), Stardog (Clark & Parsia, 2013b)),
development tools provide full SPARQL support OWL semantics still
ongoing effort.
Since consider OWL Direct Semantics entailment regime SPARQL 1.1
paper, talk SPARQL queries evaluation SPARQL queries,
always assume OWL Direct Semantics entailment regime used. setting,
clause query seen set extended OWL axioms (an extended
OWL ontology), variables place concept, role individual names.
query answers contain instantiation variables leads OWL axioms
entailed queried ontology. Thus, naive query evaluation procedure
realized OWLs standard reasoning task entailment checking.
Please note two types individual variables SPARQL; standard (distinguished) variables anonymous individuals (aka blank nodes). anonymous individuals treated like distinguished variables difference cannot
selected and, hence, bindings cannot appear query answer. contrast
conjunctive queries, anonymous individuals treated existential variables.
hand, anonymous individuals occur query answer bindings distinguished variables, i.e., SPARQL treats anonymous individuals queried ontology
constants. treatment anonymous individuals chosen compatibility
SPARQLs standard subgraph matching semantics. example, order implement
RDF(S) entailment regime, systems simply extend queried graph inferred
information (materialization) use SPARQLs standard evaluation mechanism
materialized graph order compute query results. Similarly, users
move systems support OWL RL profile (Motik, Cuenca Grau, Horrocks, Wu,
Fokoue, & Lutz, 2012a), OWL RL rule set OWL 2 specification used
compute query answers (again via materialization). one change semantics
blank nodes SPARQLs entailment regimes reflect conjunctive query semantics, one
could longer use materialization plus standard SPARQL query processor implement
entailment regime. one change semantics blank nodes
OWL Direct Semantics entailment regime, materialization cannot used implement regime, users would simply get answers moving systems
support RDF(S) systems support OWLs Direct Semantics, could also happen
get less answers using expressive logic, counter-intuitive.
last decade, much effort spent optimizing standard reasoning tasks
entailment checking, classification, realization (i.e., computation instances
concepts roles) (Sirin, Cuenca Grau, & Parsia, 2006; Tsarkov, Horrocks, & PatelSchneider, 2007; Glimm, Horrocks, Motik, Shearer, & Stoilos, 2012). optimization
query answering algorithms has, however, mostly addressed conjunctive queries
OWL profiles, notably OWL 2 QL profile (Calvanese, Giacomo, Lembo, Lenzerini,
254

fiOptimizing SPARQL Query Answering OWL Ontologies

& Rosati, 2007; Kontchakov, Lutz, Toman, Wolter, & Zakharyaschev, 2010; Perez-Urbina,
Motik, & Horrocks, 2010; Rodriguez-Muro & Calvanese, 2012). exception
works nRQL SPARQL-DL. query language nRQL supported Racer
Pro (Haarslev, Moller, & Wessel, 2004) SPARQL-DL implemented Pellet
reasoner (Sirin, Parsia, Grau, Kalyanpur, & Katz, 2007). discuss greater detail
Section 8.
paper, address problem efficient SPARQL query evaluation OWL 2
DL ontologies proposing range novel optimizations deal particular
expressive features SPARQL variables place concepts roles.
adapt common techniques databases cost-based query planning. costs
cost model based information instances concepts roles
extracted model abstraction built OWL reasoner. present
static dynamic algorithm finding optimal near optimal execution order
dynamic case, improve performance exploiting individual clustering
approach allows computing cost functions based one individual sample
cluster. propose query rewriting techniques show specialized OWL
reasoning tasks concept role hierarchy used reduce query execution
time. provide prototypical implementation evaluate efficiency proposed
optimizations. experimental study shows static ordering usually outperforms
dynamic one accurate statistics available. changes, however,
statistics less accurate, e.g., due non-deterministic reasoning decisions. queries
go beyond conjunctive SPARQL instance queries, observe improvement
three orders magnitude due proposed optimizations.
Note paper combines extends two conference papers: I. Kollia B.
Glimm: Cost based Query Ordering OWL Ontologies. Proceedings 11th International Semantic Web Conference, 2012 I. Kollia, B. Glimm I. Horrocks: SPARQL
Query Answering OWL Ontologies. Proceedings 8th Extended Semantic Web
Conference, 2011. current paper have, additionally first mentioned
paper, defined cost functions general SPARQL queries (i.e., conjunctive instance queries) added experimental results expressive queries. comparison
second mentioned papers, defined notion concept role
polarity presented theorems let us prune search space possible mappings
axiom templates based polarity together algorithm shows way
use optimization. Moreover, experimental results added complex
queries make use optimization.
remainder paper organized follows: next present preliminaries,
present general query evaluation algorithm Section 3 serves basis
optimization. Section 4, present foundations cost model,
specify Section 5. Section 6, present optimizations complex queries
cannot directly mapped specialized reasoner tasks. Finally, evaluate approach
Section 7 discuss related work Section 8 conclude Section 9.

255

fiKollia & Glimm

2. Preliminaries
section, first give brief introduction Description Logics since OWL
Direct Semantics based Description Logic SROIQ (Horrocks, Kutz, & Sattler,
2006). optimizations present need features SROIQ. Hence,
present SHOIQ, allows shorter easier follow presentation.
introducing SHOIQ, clarify relationship RDF, SPARQL
OWL, present SPARQLs OWL Direct Semantics entailment regime give
overview model building tableau hypertableau calculi.
2.1 Description Logic SHOIQ
first define syntax semantics roles, go SHOIQ-concepts,
individuals, ontologies/knowledge bases.
Definition 1 (Syntax SHOIQ ). Let NC , NR , NI countable, infinite,
pairwise disjoint sets concept names, role names, individual names, respectively.
call = (NC , NR , NI ) signature. set rol(S) SHOIQ-roles (or roles
short) NR {r | r NR } {r , r }, roles form r called inverse
roles, r top role (analogous owl:topObjectProperty), r bottom role
(analogous owl:bottomObjectProperty). role inclusion axiom form r
r, roles. transitivity axiom form trans(r) r role. role hierarchy H
finite set role inclusion transitivity axioms.
role hierarchy H, define function inv roles inv(r) := r r NR
inv(r) := r = role name NR . Further, define H smallest
transitive reflexive relation roles r H implies r H inv(r) H
inv(s). write r H r H H r. role r transitive w.r.t. H (notation
r + H r) role exists r H s, H r, trans(s) H trans(inv(s)) H.
role called simple w.r.t. H role r r transitive w.r.t. H
r H s.
Given signature = (NC , NR , NI ) role hierarchy H, set SHOIQconcepts (or concepts short) smallest set built inductively symbols
using following grammar, NI , NC , n IN0 , simple role
w.r.t. H, r role w.r.t. H:
C ::= | | {o} | | C | C C | C C | r.C | r.C | 6 n s.C | > n s.C.
define semantics SHOIQ concepts:
Definition 2 (Semantics SHOIQ-concepts). interpretation = (I , ) consists
non-empty set , domain I, function , maps every concept name
NC subset AI , every role name r NR binary relation r ,
every individual name NI element aI . top role r interpreted
{h, | , } bottom role r . role name r NR ,

interpretation inverse role (r ) consists pairs h,
h , r .
256

fiOptimizing SPARQL Query Answering OWL Ontologies

semantics SHOIQ-concepts signature defined follows:

(C)I
(r.C)I
(r.C)I
(6 n s.C)I
(> n s.C)I

=
=
=
=
=
=


\ C
{
{
{
{

=
({o})I = {oI }



(C D) = C
(C D)I = C




| h, r , C }
| h, r C }
| (sI (, C)) n}
| (sI (, C)) n}

(M ) denotes cardinality set sI (, C) defined
{ | h, sI C }.
Definition 3 (Syntax Semantics Axioms Ontologies, Entailment).
C, concepts, (general) concept inclusion axiom (GCI) expression C D.
introduce C abbreviation C C. finite set GCIs called
TBox. (ABox) (concept role) assertion axiom expression form C(a),
r(a, b), r(a, b), b, 6 b, C NC concept, r NR role, a, b NI
individual names. ABox finite set assertion axioms. ontology
triple (T , H, A) TBox, H role hierarchy, ABox. use NCO , NRO ,
NIO denote, respectively, set concept, role, individual names occurring
O.
Let = (I , ) interpretation. satisfies role inclusion axiom r
r sI , satisfies transitivity axiom trans(r) r transitive binary relation,
role hierarchy H satisfies role inclusion transitivity axioms H.
interpretation satisfies GCI C C ; satisfies TBox satisfies
GCI . interpretation satisfies assertion axiom C(a) aI C , r(a, b)
haI , bI r , r(a, b) haI , bI
/ r , b aI = bI , 6 b aI 6= bI ; satisfies
ABox satisfies assertion A. say satisfies satisfies , H,
A. case, say model write |= O. say
consistent model.
Given axiom , say entails (written |= ) every model
satisfies .
Description Logics extended concrete domains, correspond
OWLs datatypes. case, one distinguishes abstract roles relate two
individuals concrete roles relate individual data value. Description
Logic SROIQ allows number features role chains form
hasFather hasBrother hasUncle, support special concept Self,
used axioms form Narcissist loves.Self, defining roles reflexive,
irreflexive, symmetric, asymmetric.
Description Logic ontologies equally expressed terms OWL ontologies,
turn mapped RDF graphs (Patel-Schneider & Motik, 2012). direction is, however, always possible, i.e., mapping RDF graphs OWL ontologies
defined certain well-formed RDF graphs correspond OWL 2 DL ontology.
257

fiKollia & Glimm

2.2 Relationship RDF, SPARQL, OWL
SPARQL queries evaluated RDF graphs remain basic data structure
even adopting elaborate semantic interpretation.
Definition 4 (RDF Graphs). RDF based set International Resource
Identifiers (IRIs), set L RDF literals, set B blank nodes. set
RDF terms L B. RDF graph set RDF triples form
(subject, predicate, object) (I B) .
generally abbreviate IRIs using prefixes rdf, rdfs, owl, xsd refer RDF,
RDFS, OWL, XML Schema Datatypes namespaces, respectively. empty prefix
used imaginary example namespace, completely omit Description Logic
syntax.
example SPARQL query
SELECT ?x <ontologyIRI> { ?x rdf:type :C . ?x :r ?y }
clause SPARQL query consists basic graph pattern (BGP):
RDF graph written Turtle syntax (Beckett, Berners-Lee, Prudhommeaux, & Carothers,
2013), nodes edges replaced variables. basic graph pattern
precisely defined follows:
Definition 5 (Basic Graph Pattern). Let V countably infinite set query variables
disjoint . triple pattern member set (T V ) (I V ) (T V ),
basic graph pattern (BGP) set triple patterns.
recall complete surface syntax SPARQL since part
specific evaluation SPARQL queries OWLs Direct Semantics
evaluation BGPs. complex clauses, use operators
UNION alternative selection criteria OPTIONAL query optional bindings
(Prudhommeaux & Seaborne, 2008), evaluated simply combining results
obtained BGP evaluation. Similarly, operations projection variables
SELECT clause straightforward operation results evaluation
clause. Therefore, focus BGP evaluation only. detailed
introduction SPARQL queries algebra refer interested readers work
Hitzler, Krotzsch, Rudolph (2009) Glimm Krotzsch (2010).
Since Direct Semantics OWL defined terms OWL structural objects, i.e.,
OWL axioms, map BGPs SPARQL queries structural objects,
variables place class (concept), object data property (abstract concrete role),
individual names literals. Since direct mapping OWL axioms
Description Logic axioms, BGPs expressed Description Logic axioms
variables occur place concept, role individual names. example, BGP
previous example mapped ClassAssertion(C ?x) ObjectPropertyAssertion(r
?x ?y) functional-style syntax C(?x) r(?x, ?y) Description Logic syntax.
details, refer interested readers W3C specification defines
mapping OWL structural objects RDF graphs (Patel-Schneider & Motik,
2012) specification OWL Direct Semantics entailment regime SPARQL
258

fiOptimizing SPARQL Query Answering OWL Ontologies

(Glimm & Ogbuji, 2013) defines extension mapping BGPs
OWL objects variables.
2.3 SPARQL Queries
following, directly write BGPs Description Logic notation extended allow variables place concept, role individual names axioms. worth
reminding SPARQL support existentially quantified variables,
contrast database-style conjunctive queries, one typically also existential/nondistinguished variables.
brevity without loss generality, assume neither query
queried ontology contains anonymous individuals. consider data
properties literals, presented optimizations easily transferred
case.
Definition 6 (Query). Let = (NC , NR , NI ) signature. query signature Sq w.r.t.
six-tuple (NC , NR , NI , VC , VR , VI ), VC , VR , VI countable, infinite,
pairwise disjoint sets concept variables, role variables, individual variables
disjoint NC , NR , NI . concept term element NC VC . role term
element NR VR . individual term element NI VI . axiom
template Sq SROIQ axiom S, one also use concept variables
VC place concept names, role variables VR place role names, individual
variables VI place individual names. query q w.r.t. query signature Sq
non-empty set axiom templates Sq . use Vars(q) (Vars(at) axiom template
at) denote set variables q (at) |q| denote number axiom templates
q. Let t, individual terms; call axiom templates form A(t) NC ,
r(t, ) r NR , query atoms. conjunctive instance query q w.r.t. query
signature Sq non-empty set query atoms.
function , use dom() denote domain . Let ontology
q = {at1 , . . . , atn } query Sq consisting n axiom templates. mapping
q total function : Vars(q) NCO NRO NIO
1. (v) NCO v VC dom(),
2. (v) NRO v VR dom(),
3. (v) NIO v VI dom(),
4. (q) SROIQ ontology.
write (q) ((at)) denote result replacing variable v q (at)

(v). set
q compatible mappings q defined q := { |
mapping q O}. mapping solution mapping certain answer
q |= (q). denote set containing solution mappings q

q . result size number answers query q given
cardinality set
q .
Note last condition definition mappings required ensure decidability query entailment. example, without condition, reasoner might
259

fiKollia & Glimm

test instantiated axiom templates role variable replaced non-simple
role number restriction, allowed Description Logic axioms. Note also
indicate variables selected since consider
straightforward task projection here.
Examples queries according definition following (where ?x
concept variable, ?y role variable, ?z individual variable):
C ?y.?x
(r.?x)(?z)
remainder, use signature (NC , NR , NI ), denote SROIQ ontology
S, A, B NC concept names O, r, NR role names O, a, b NI
individual names O, ?x, ?y variables, c1 , c2 concept terms, r1 , r2 role
terms, t, individual terms, q = {at1 , . . . , atn } query n axiom templates
query signature Sq = (NC , NR , NI , VC , VR , VI ),
q compatible mappings



solution
mappings

q

O.
q
2.4 Model-building (Hyper)Tableau Calculi
section, give brief overview main reasoning techniques OWL DL
ontologies since cost-based query planning relies techniques.
order check whether ontology entails axiom , one typically checks whether
{} model. case, every model satisfies
|= . example, check whether individual a0 instance concept C
w.r.t. ontology O, check whether adding concept assertion C(a0 ) leads
inconsistency. check this, OWL reasoners use model construction calculus
tableau hypertableau. remainder, focus hypertableau calculus
(Motik, Shearer, & Horrocks, 2009), tableau calculus could equally used
state results transferred tableau calculi.
hypertableau calculus starts initial set ABox assertions and, applying
derivation rules, tries construct (an abstraction of) model O. Derivation rules
usually add new concept role assertion axioms, may introduce new individuals,
nondeterministic, leading need choose several alternative assertion
axioms add lead clash contradiction detected. show
ontology (in)consistent, hypertableau calculus constructs derivation, i.e.,
sequence sets assertions A0 , . . . , , A0 contains ABox assertions O,
Ai+1 result applying derivation rule Ai final set assertions
rules applicable. derivation exists contain
clash, consistent called pre-model O. Otherwise inconsistent.
assertion set assertions Ai derived either deterministically nondeterministically. assertion derived deterministically derived application
deterministic derivation rule assertions derived deterministically.
derived assertion derived nondeterministically. easy know whether assertion derived deterministically dependency directed backtracking
(hyper)tableau reasoners employ. pre-model, individual s0 assigned
label L(s0 ) representing concepts (non)deterministically instance
260

fiOptimizing SPARQL Query Answering OWL Ontologies

pair individuals hs0 , s1 assigned label L(hs0 , s1 i) representing roles
individual s0 (non)deterministically related individual s1 .

3. Motivation
straightforward algorithm compute answers query q test,
mapping , whether |= (q). Since terms used occur range
mapping q O, finitely many mappings test. worst case,
however, number mappings tested still exponential number
variables query. algorithm sound complete reasoner used
decide entailment sound complete since check mappings variables
constitute actual solution mappings.
Optimizations cannot easily integrated sketched algorithm since uses
reasoner check entailment instantiated query whole and, hence,
take advantage relations dependencies may exist individual
axiom templates q. optimized evaluation, one evaluate query axiom
template axiom template. Initially, solution set contains identity mapping,
map variable value. One picks first axiom template,
extends identity mapping cover variables chosen axiom template
uses reasoner check mappings instantiate axiom template
entailed axiom. One picks next axiom template extends mappings
previous round cover variables checks mappings lead
entailed axiom. Thus, axiom templates selective satisfied
solutions reduce number intermediate solutions. Choosing good execution
order, therefore, significantly affect performance.
example, let q = {A(?x), r(?x, ?y)} ?x, ?y VI . query belongs
class conjunctive instance queries. assume queried ontology contains 100
individuals, 1 belongs concept A. instance 1 r-successor,
overall 200 pairs individuals related role r. first evaluate
A(?x), test 100 mappings (since ?x individual variable), 1 mapping
satisfies axiom template. evaluate r(?x, ?y) extending mapping
100 possible mappings ?y. 1 mapping yields solution. reverse
axiom template order, first axiom template requires test 100 100 mappings.
those, 200 remain checked second axiom template perform 10, 200
tests instead 200. Note also number intermediate results query
evaluated order A(?x), r(?x, ?y) smaller evaluated reverse
order (2 versus 201).
context databases triple stores, cost-based ordering techniques finding
optimal near optimal join ordering widely applied (Steinbrunn, Moerkotte, &
Kemper, 1997; Stocker, Seaborne, Bernstein, Kiefer, & Reynolds, 2008). techniques
involve maintenance set statistics relations indexes, e.g., number
pages relation, number pages index, number distinct values column,
together formulas estimation selectivity predicates estimation
CPU I/O costs query execution depends amongst others, number
pages read written secondary memory. formulas
261

fiKollia & Glimm

estimation selectivities predicates (result output size axiom templates) estimate
data distributions using histograms (Ioannidis & Christodoulakis, 1993), parametric
sampling methods combinations them. Ordering strategies implemented
databases triple stores are, however, directly applicable setting. presence
expressive schema level axioms, cannot rely counting number occurrences
triples. also cannot, general, precompute relevant inferences base statistics
materialized inferences. Furthermore, aim decreasing number
intermediate results, also take account cost checking computing
solutions. cost significant OWL reasoning precise estimation
query evaluation difficult cost takes values wide range, e.g., due
nondeterminism high worst-case complexity standard reasoning tasks.1
several kinds axiom templates can, however, directly retrieve solutions
reasoner instead checking entailment. example, C(?x), reasoners typically method retrieve concept instances. Although might internally trigger
several tests, methods reasoners highly optimized avoid many tests
possible. Furthermore, reasoners typically cache several results computed concept hierarchy retrieving sub-concepts realized cache lookup. Thus,
actual execution cost might vary significantly. Notably, straight correlation number results axiom template actual cost retrieving
solutions typically case triple stores databases. requires cost models
take account cost specific reasoning operations (depending state
reasoner) well number results.
motivated above, distinguish simple complex axiom templates. Simple axiom templates correspond dedicated reasoning tasks. Let c1
concept term, C, C (complex) concepts concept variables, r1 , r2 role terms role inverses t, individual terms. set simple axiom templates contains templates
form: C C , r1 . c1 (domain restriction template), r1 .c1 (range restriction
template), r1 r2 , C(t), r1 (t, ), , 6 . Complex axiom templates can, contrast, evaluated dedicated reasoning tasks might require iterating
compatible mappings checking entailment instantiated axiom template.
example complex axiom template (r.?x)(?y).

4. Preprocessing Extracting Information Queries
section, describe way preprocessing queried ontology extract information useful ordering axiom templates query. preprocessing useful
axiom templates form c1 (t), r1 (t, ), , c1 concept term, r1
role term t, individual terms.
4.1 Extracting Individual Information Reasoner Models
first step ordering query atoms extraction statistics exploiting
information generated reasoners. use labels initial pre-model pro1. example, description logic SROIQ, underpins OWL 2 DL standard, worst case
complexity 2-NExpTime (Kazakov, 2008) typical implementations worst case optimal.

262

fiOptimizing SPARQL Query Answering OWL Ontologies

Algorithm 1 initializeKnownAndPossibleConceptInstances(O)
Input: consistent SROIQ ontology
1: := buildM odelF or(O)
2: NIO
3:
C LAn (a)
4:
C derived deterministically
5:
K[C] := K[C] {a}
6:
else
7:
P [C] := P [C] {a}
8:
end
9:
end
10: end
vide information concepts individuals belong roles one
individual connected another one. exploit information similarly suggested determining known possible (non-)subsumers concepts classification
(Glimm et al., 2012). hypertableau calculus, following two properties hold
ontology constructed pre-model O:
(P1) concept name C (role name r), individual s0 (pair individuals hs1 , s2 i)
, C LAn (s0 ) (r LAn (hs1 , s2 i)) assertion C(s0 ) (r(s1 , s2 ))
derived deterministically, holds |= C(s0 ) (O |= r(s1 , s2 )).
(P2) arbitrary individual s0 (pair individuals hs1 , s2 ) arbitrary
concept name C (simple role name r), C 6 LAn (s0 ) (r 6 LAn (hs1 , s2 i)),
6|= C(s0 ) (O 6|= r(s1 , s2 )).
simplicity, assume equality () axiomatized treated
reflexive, symmetric, transitive role. use properties extract information
pre-model satisfiable ontology O.
Definition 7 (Known Possible Instances). Let pre-model ontology
O. individual known (possible) instance concept name C , denoted
KAn [C] (a PAn [C]), C LAn (a) C(a) derived deterministically (nondeterministically) . pair individuals ha1 , a2 known (possible) instance simple
role name r , denoted ha1 , a2 KAn (r), r LAn (ha1 , a2 i) r(a1 , a2 ) derived
deterministically (nondeterministically) . individual a1 (possibly) equal
individual a2 , written a1 K [a2 ] a2 K [a1 ] (a1 P [a2 ] a2 P [a1 ])
a1 a2 deterministically (nondeterministically) derived O.
remainder, assume known possible instances defined w.r.t.
arbitrary pre-model simply write K[C], K[r], K [a], P [C], P [r],
P [a]. Intuitively, K[C] contains individuals safely considered instances
concept name C. hand, possible instances require costly consistency
checks order decide whether real instances concept, individuals
neither belong K[C] P [C] safely assumed non-instances C.
263

fiKollia & Glimm

Algorithm 1 outlines procedure initialize relations known possible concept instances. information extract involves maintenance sets known
possible instances concepts O. One define similar algorithm initializing known possible instances simple roles (possibly) equal individuals.
implementation, use involved procedure store direct types
individual, concept name C direct type individual ontology
|= C(a) concept name |= C, |= D(a)
6|= C.
Hypertableau tableau reasoners typically deal transitivity directly.
order deal non-simple roles, expanded additional axioms capture
semantics transitive relations pre-model built. particular,
individual non-simple role r, new concepts Ca Car introduced axioms
Ca (a) Ca r.Car added O. consequent application transitivity
encoding (Motik et al., 2009) produces axioms propagate Car individual b
reachable via r-chain. known possible r-successors
determined Car instances.
technique presented paper used (hyper)tableau calculus
properties (P1) (P2) hold. (hyper)tableau calculi used practice
aware satisfy property (P1). Pre-models produced tableau algorithms presented
literature also satisfy property (P2); however, commonly used optimizations,
lazy unfolding, compromise property (P2), illustrate following
example. Let us assume ontology containing axioms
r.(C D)

(1)

B r.C

(2)

A(a)

(3)

obvious ontology subconcept B (hence, |= B(a)) since every
individual r-related individual instance intersection C
also r-related individual instance concept C. However,
even though assertion A(a) occurs ABox, assertion B(a) added
pre-model use lazy unfolding. lazy unfolding, instead treating (2) two
disjunctions B r.C B r.(C) typically done general concept inclusion
axioms, B lazily unfolded definition r.C B occurs label
individual. Thus, although (r.(C D))(a) would derived, lead
addition B(a).
Nevertheless, (if all) implemented calculi produce pre-models satisfy
least following weaker property:
(P3) arbitrary individual s0 arbitrary concept name C C
primitive O,2 C 6 LAn (s0 ), 6|= C(s0 ).
Hence, properties (P2) (P3) used extract (non-)instance information
pre-models. tableau calculi satisfy (P3), non-primitive concept name
2. concept C considered primitive unfoldable (Tsarkov et al., 2007) contains
axiom form C E

264

fiOptimizing SPARQL Query Answering OWL Ontologies

C need add P [C] individuals include concept C
label.
proposed technique determining known possible instances concept
role names used way tableau hypertableau reasoners.
Since tableau algorithms often introduce nondeterminism hypertableau, one
might, however, find less deterministic derivations, results less accurate statistics.
4.1.1 Individual Clustering
section, describe procedure creating clusters individuals within
ontology using constructed pre-model O. Two types clusters created:
concept clusters role clusters. Concept clusters contain individuals
concepts label role clusters contain individuals concept role
labels. Role clusters divided three categories, based first
individual role instances, based second individual based
individuals.
Definition 8 (Concept Role Clusters). Let ontology pre-model
O. define following two relations P1 P2 map individual
roles least one successor predecessor, respectively:
P1 (a) = {r | r LAn (ha, bi) b NIO }
P2 (a) = {r | r LAn (hb, ai) b NIO }
Based relations, build three different partitions NIO : concept clusters CC,
role successor clusters P C1 , role predecessor clusters P C2 clusters satisfy:
C CC.(for a1 , a2 C.(LAn (a1 ) = LAn (a2 )))
C P C1 .(for a1 , a2 C.(LAn (a1 ) = LAn (a2 ) P1 (a1 ) = P1 (a2 )))
C P C2 .(for a1 , a2 C.(LAn (a1 ) = LAn (a2 ) P2 (a1 ) = P2 (a2 ))).
partition NIO NIO role clusters P C12 clusters satisfy:
C P C12 .(for ha1 , a2 i, ha3 , a4 C.(LAn (a1 ) = LAn (a3 ), LAn (a2 ) = LAn (a4 )
LAn (ha1 , a2 i) = LAn (ha3 , a4 i))).
use clusters next section optimize dynamic query ordering strategy.

5. Query Answering Axiom Template Ordering
section, describe two different algorithms (a static dynamic one) ordering
axiom templates query based costs deal formulation
costs. first introduce abstract graph representation query q means
labeled graph Gq define computed statistical costs.
Definition 9 (Query Join Graph). query join graph Gq query q tuple
(V, E, EL ),
265

fiKollia & Glimm

V = q set vertices (one axiom template);
E V V set edges; hat1 , at2 E Vars(at1 ) Vars(at2 ) 6=
at1 6= at2 ;
EL function assigns set variables hat1 , at2 E
EL (at1 , at2 ) = Vars(at1 ) Vars(at2 ).
remainder, use Gq query join graph q.
goal find query execution plan, determines evaluation order
axiom templates q. Since number possible execution plans order |q|!,
ordering task quickly becomes impractical. following, focus greedy algorithms
determining execution order, prune search space considerably. Roughly
speaking, proceed follows: define cost function, consists two components
(i) estimate costs reasoning tasks needed evaluation axiom
template (ii) estimate intermediate result size, i.e., number results
evaluation axiom template incur. components combined
induce order among axiom templates. paper, simply build sum
two cost components, different combinations weighted sum two values
could also used. query plan construction distinguish static dynamic
planning. former, start constructing plan adding minimal template
according order. Variables template considered bound,
changes cost function might induce different order among remaining axiom
templates. Considering updated order, select minimal axiom template
yet plan update costs. process continues plan contains
templates. complete plan determined templates evaluated.
dynamic case differs selecting template plan, immediately
determine solutions chosen template, used update cost
function. yields accurate cost estimates, costly solutions
considered updating cost function. Sampling techniques used test
subset solutions, show Section 7 random sampling, i.e., randomly
choosing percentage individuals far computed solutions, adequate.
reason, propose alternative sampling approach based use
previously described individual clusters. first present example make difference
static dynamic planning clearer justify dynamic ordering
beneficial setting.
Example 1. Let ontology q = {C(?x), r(?x, ?y), D(?y)} conjunctive instance
query O. Suppose known possible instances query concepts
roles
K[C] = {a}

K[r] =

K[D] = {b}

P [C] = {c, e}

P [r] = {hc, di, he, f i}

P [D] = {f, g, h}

let us assume possible instances C, r are, fact, real instances (note
information beginning). Please mind
possible instances concepts roles costly evaluate known instances
266

fiOptimizing SPARQL Query Answering OWL Ontologies

since require expensive consistency checks order decide whether real
instances.
According static planning, ordering query atoms first determined. particular, atom r(?x, ?y) chosen first since least number known possible instances (0 known 2 possible versus 1 known 2 possible C(?x)
1 known 3 possible D(?y)). atom C(?x) chosen since less
known possible instances D(?y), i.e., 1 known 2 possible versus 1 known
3 possible D(?y). Hence chosen execution plan static planning P =
(r(?x, ?y), C(?x), D(?y)). Afterwards, query evaluated according chosen execution plan, i.e., atom r(?x, ?y) evaluated first, gives solution mappings
1 = {{?x 7 c, ?y 7 d}, {?x 7 e, ?y 7 f }}. requires 2 consistency checks 2
possible instances r. Afterwards, check ?x mappings, c e, known
possible instances C. Since c e possible instances, check whether
real instances C (this requires 2 consistency checks). Hence, solution mappings
2 = 1 = {{?x 7 c, ?y 7 d}, {?x 7 e, ?y 7 f }}. end, check
?y mappings, f , known possible instances D. possible
instance, f , find one consistency check f indeed instance D. Hence,
solution mappings q
q = {{?x 7 e, ?y 7 f }} finding solution required
5 consistency checks.
According dynamic planning, ordering determined evaluate query.
reasons before, atom r(?x, ?y) chosen evaluated first
solution mappings are, before, 1 = {{?x 7 c, ?y 7 d}, {?x 7 e, ?y 7 f }} (this requires
2 consistency checks). afterwards check ?y mappings, f , known
possible instances D. Note requires look-up since find f
among possible instances, check whether individual indeed instance
not. f possible instance. also check ?x mappings, c
e, known possible instances C. Here, c e possible instances, i.e.,
2 relevant possible instances C(?x) 1 D(?y). Hence, atom D(?y)
chosen evaluated next, resulting solution sequence 2 = {{?x 7 e, ?y 7 f }}
(partial) execution plan (r(?x, ?y), D(?y)), requiring 1 consistency check. end,
check whether ?x mapping, e, known possible instance C. Since e
possible instance, check whether real instance (this requires 1 consistency check).
Hence, solution mappings q
q = {{?x 7 e, ?y 7 f }}, found
performing 4 consistency checks, one less static case.
Note dynamic ordering perform less checks static ordering, since
case exploit results joins query atoms information regarding
possible instances atoms (i.e., real instances), determined
result evaluating atoms ordering them.
make process query plan construction precise, leave
exact details defining cost function ordering induces later.
Definition 10 (Static Dynamic Ordering). static (dynamic) cost function w.r.t.

q function : q 2V ars(q) R R (d : q 2q R R),
q



denote set compatible mappings q O. two costs hEcat , Rsat (hEcat , Rsdat i)
axiom template q combined yield static ordering ( dynamic ordering
267

fiKollia & Glimm

), total order axiom templates q that, at, q, say
(at ) iff Ecsat + Rssat Ecsat + Rssat (Ecdat + Rsdat Ecdat + Rsdat ).
execution plan q duplicate-free sequence axiom templates q.
initial execution plan empty sequence complete execution plan sequence
containing templates q. Let Pi = (at1 , . . . , ati ) < |q| execution plan
q query join graph Gq = (V, E, EL ). set bound variables ati within Pi
Vb (ati ) = Vars(ati ) Vars({at1 , . . . , ati1 }). Let Cq set complex axiom templates
q. next define axiom templates used extend incomplete execution
plan. Let axiom template Pi , set suci (at) contains axiom templates
connected yet Pi , i.e., suci (at) = {at q | hat, E,
/
{at1 , . . . ati }}. Based this, define set connected successor axiom templates
Pi Si = {at | {at1 , . . . , ati } suci (at )}. allow including
axiom templates connected complex axiom template Si define
potential next templates qi Pi w.r.t. Gq qi = q Pi initial execution plan
otherwise
[
q = Si
suci (at).
Cq Si

Given Pi , static (dynamic) ordering induces execution plan Pi+1 = (at1 , . . . , ati , ati+1 )
ati+1 qi ati+1 (ati+1 at) qi 6= ati+1 .
Note according definition, Pi execution plan, case
qi contains templates assigned minimal cost cost function.
case, one choose atoms add Pi . Moreover, according
definition case queries containing simple axiom templates
that, > 0, set potential next templates contains templates
connected template already plan since unconnected templates cause
unnecessary blowup number intermediate results. queries complex
templates set potential next axiom templates additionally contain templates
share common variables template already plan. different
handling queries complex templates reasonable since, evaluating complex
axiom template requires many consistency checks, want reduce number
candidate bindings, first evaluating simple (cheaper) templates bind variables
appear complex one.
Example 2. Let ontology q = {?x A, ?y r, B ?y.?x} query
O. Assuming systems usually precompute concept role hierarchies
accept queries, evaluation first two templates, i.e., ?x ?y r, require
cheap cache lookups, whereas axiom template B ?y.?x, requires costly consistency
checks. Hence, reasonable first evaluate first two (cheap) templates reduce
mappings ?x ?y evaluate third (expensive) template, checking
reduced mappings yield entailed axiom.
example shows actual gain get handling ordering complex
axiom templates way presented Section 7.
Let n = |q| Pn = (at1 , . . . , atn ) complete execution plan q determined
static ordering. procedure find solution mappings
q Pn recursively
268

fiOptimizing SPARQL Query Answering OWL Ontologies

defined follows: Initially, solution set contains identity mapping 0 = {0 },
map variable value. Assuming evaluated
sequence Pi = (at1 , . . . , ati ), < n found set solution mappings ,
order find solution mappings i+1 Pi+1 , use specific reasoning tasks
extend mappings cover new variables ati+1 ati+1 simple axiom
template entailment check service reasoners ati+1 contain new variables
ati+1 complex axiom template. dynamic planning difference
execution plan construction interleaved query evaluation. particular, let n = |q|
Pi = (at1 . . . ati ) < n (partial) execution plan q determined dynamic
ordering let solution mappings Pi . order find Pi+1 extend Pi
new template, ati+1 , q, i.e., Pi+1 = (at1 , . . . ati+1 ), which, according dynamic
cost function, minimal cost among remaining templates q \ {at1 , . . . ati }.
dynamic cost function assigns costs templates iteration + 1 taking account
solution mappings . afterwards evaluate atom ati+1 , i.e., find solution
mappings i+1 Pi+1 extending solution mappings Pi way
static case. Section 6.3 Algorithm 3, show complete procedure follow
answer query.
define cost functions precisely, estimate cost
required reasoner operations (first component) estimated result output size (second
component) evaluating axiom template. intuition behind estimated value
reasoner operation costs evaluation possible instances much
costly evaluation known instances since possible instances require expensive
consistency checks whereas known instances require cheap cache lookups. estimated
result size takes account number known possible instances probability
possible instances actual instances.
time needed entailment check change considerably ontologies
even within ontology (depending involved concepts, roles individuals).
order accurately determine entailment cost use different entailment cost
values depending whether template consideration template form i)
c1 (t), ii) r1 (t, ), iii) , c1 concept term, r1 role term t, individual
terms, iv) one rest simple axiom templates (that require consistency checks
evaluated) complex axiom template. following write CL denote cost
cache lookup internal structures reasoner, CE placeholder relevant
entailment cost value PIS possible instance success, i.e, estimated percentage
possible instances actual instances. costs CL CE determined
recording average time previously performed lookups entailment checks
queried ontology, e.g., initial consistency check, classification, previous
queries. possible instance success, PIS , determined testing several ontologies
checking many initial possible instances real ones, around
50% nearly ontologies.
Apart relations known possible instances Section 4.1, use
following auxiliary relations:
Definition 11 (Successor Predecessor Relations). Let r role individual. define sucK[r] preK[r] set individuals known r-successors
269

fiKollia & Glimm

r-predecessors, respectively:
sucK[r] := {a | b.ha, bi K[r]}



preK[r] := {a | b.hb, ai K[r]}.

Similarly, define sucK[r, a] preK[r, a] known r-successors known
r-predecessors a, respectively:
sucK[r, a] := {b | ha, bi K[r]}



preK[r, a] := {b | hb, ai K[r]}.

analogously define functions sucP[r], preP[r], sucP[r, a], preP[r, a] replacing
K[r] P [r].
Next, define cost functions case conjunctive instance queries, i.e.,
queries containing query atoms. Section 5.2 extend cost functions deal
general queries.
5.1 Static Dynamic Cost Functions Conjunctive Instance Queries
static cost function takes two components input: query atom set containing
variables query atom considered bound. function returns pair
real numbers reasoning cost result size query atom.
Initially, variables unbound use number known possible instances successors/predecessors estimate number required lookups consistency checks evaluating query atom resulting number mappings.
input form hC(?x), hr(?x, ?y), resulting pair real numbers
computational cost estimated result size computed
h|K[at]| CL + |P [at]| CE , |K[at]| + PIS |P [at]|i,
denotes predicate query atom (C r). concept (role) atom,
factor represents depth concept (role) concept (role) hierarchy.
use factor since store direct types individual (roles
individuals instances) and, order find instances concept (role), may
need check subconcepts (subroles) known possible instances. query
atom role atom constant first place, i.e., input cost function
form hr(a, ?x), i, use relations known possible successors estimate
computational cost result size:
h|sucK[r, a]| CL + |sucP[r, a]| CE , |sucK[r, a]| + PIS |sucP[r, a]|i.
Analogously, use preK preP instead sucK sucP input form
hr(?x, a), i. Finally, atom contains constants, i.e., input cost function
form hC(a), i, hr(a, b), i, function returns hd CL , 1i individual
known instance concept role, hd CE , PIS individual possible instance
hd CL , 0i otherwise, i.e., individual known non-instance.
equality atoms form ?x ?y, ?x, ?x b, exploit
information initial pre-model described Section 4.1. Based cardinality
K [a] P [a], define cost functions different cases query atoms
270

fiOptimizing SPARQL Query Answering OWL Ontologies

bound variables. inputs form h?x a, ha ?x, i, cost function
defined as:
h|K [a]| CL + |P [a]| CE , |K [a]| + PIS |P [a]|i.
inputs form h?x ?y, i, cost function computed as:
*
+
X
X
(|K [a]| CL + |P [a]| CE )/2,
(|K [a]| + PIS |P [a]|)/2 .
aNIO

aNIO

inputs form ha b, i, function returns hCL , 1i b K [a], hCE , PIS
b P [a], hCL , 0i otherwise (i.e., b equivalent a).
determining cost initial query atom, least one variable consequently considered atom bound, since query plan construction move
atoms sharing common variable assume query connected. define cost functions atoms least one variable bound. make assumption
atoms unbound variables costly evaluate atoms
variables bound. query atom r(?x, ?y) ?x bound, i.e., function inputs
form hr(?x, ?y), {?x}i, use average number known possible successors
role estimate computational cost result size:


|P [r]|
|K[r]|
|P [r]|
|K[r]|
CL +
CE ,
+
PIS .
|sucK[r]|
|sucP[r]|
|sucK[r]| |sucP[r]|
case ?y r(?x, ?y) bound, use predecessor functions preK preP instead
sucK sucP. Note work estimated average number successors
(predecessors) one individual.
atoms variables bound, use formulas comparable
ones initial plan, normalized estimate values one individual.
input query atom form C(?x) ?x bound variable use


|K[C]| CL + |P [C]| CE |K[C]| + PIS |P [C]|
,
.
|NIO |
|NIO |
simple normalization always accurate, leads good results
cases show Section 7. Similarly, normalize formulas role atoms
form r(?x, ?y) {?x, ?y} set bound variables atom. two cost
components atoms computed


|K[r]| CL + |P [r]| CE |K[r]| + PIS |P [r]|
,
.
|NIO | |NIO |
|NIO | |NIO |
role atoms constant bound variable, i.e., atoms form r(a, ?x)
(r(?x, a)) ?x bound variable, use sucK[r, a] sucP[r, a] (preK[r, a] preP[r, a])
instead K[r] P [r] formulas normalize |NIO |.
Similarly, normalize cost functions inputs equality atoms bound
variables, depending whether atoms contain one two bound variables. inputs
form h?x a, {?x}i, ha ?x, {?x}i, divide cost function components inputs
271

fiKollia & Glimm

already executed
1
2
3
4
5
6
7

r(?x, ?y)
r(?x, ?y)
r(?x, ?y), D(?y)
r(?x, ?y), C(?x)

current atom
C(?x)
r(?x, ?y)
D(?y)
C(?x)
D(?y)
C(?x)
D(?y)

K[at]
200
200
700
100
50
45
45

P [at]
350
200
600
150
50
35
40

real P [at]
200
50
400
100
40
25
25

Table 1: Query Ordering Example
form h?x a, ha ?x, |NIO |. input form h?x y, {?x, ?y}i,
divide cost function components input form h?x ?y, |NIO | |NIO |.
inputs form h?x ?y, {?x}i, h?x ?y, {?y}i, divide cost function
components input form h?x ?y, |NIO |.
dynamic cost function based static function s, uses first
equations, atom contains unbound variables constants. function
takes pair hat, input, query atom set solution mappings
atoms already evaluated, returns pair real numbers using
matrix addition follows:
X
d(at, ) =
s((at), )


sampling techniques used, compute costs potential next
atoms execution plan considering one individual relevant cluster.
cluster relevant depends query atom compute cost function
previously computed bindings. instance, compute cost role atom
r(?x, ?y) already determined bindings ?x, use role successor cluster
P C1 . Among ?x bindings, check cost one binding per cluster
assign cost ?x bindings cluster.
Example 3. Let us assume conjunctive instance query q
find cost (using dynamic function) atom C(?x) within execution plan
q. assume evaluation previous query atoms plan
already determined set intermediate solutions mappings a, b, c
?x. Let us assume a, b, c belong concept cluster. According
dynamic ordering need find cost instantiated atom using static cost
function, i.e., d(C(?x), ) = s(C(a), ) + s(C(b), ) + s(C(c), ). additionally use
cluster based sampling, find cost one individual cluster, let us say a,
assign cost individuals cluster mappings
?x . Hence, cost atom C(?x) sampling used, computed
d(C(?x), ) = 3 s(C(a), ) avoiding computation s(C(b), ) s(C(c), ).
example similar Example 1 (but greater number instances)
shows ordering achieved use defined static dynamic functions
shown below. assume q query consisting three query atoms: C(?x),
272

fiOptimizing SPARQL Query Answering OWL Ontologies

r(?x, ?y), D(?y). Table 1 gives information known possible instances
atoms within sequence. second column shows already executed sequences
Pi1 = (at1 , . . . , ati1 ) atoms q. Column 3 gives current atom ati column
4 (5) gives number mappings known (possible) instances satisfy
time atoms (at1 , . . . , ati1 ) column 2. Column 6 gives number real
instances possible instances current atom. example, row 4 says
evaluated atom r(?x, ?y) and, order evaluate C(?x), consider
100 known 150 possible instances C also mappings ?x. assume
10,000 individuals ontology O. explain, using example,
described formulas work. assume CL CE , always
case since cache lookup less expensive consistency check CE values
query concepts roles. ease presentation,
consider factor depth concept (role) within concept (role) hierarchy.
techniques (static dynamic) atom r(?x, ?y) chosen first since
least number possible instances (200) (or smaller) number known
instances (200) atoms (0 initial solution mapping map
variable):
s(r(?x, ?y), ) = d(r(?x, ?y), {0 }) = h200 CL + 200 CE , 200 + PIS 200i,
s(C(?x), ) = d(C(?x), {0 }) = h200 CL + 350 CE , 200 + PIS 350i,
s(D(?y), ) = d(D(?y), {0 }) = h700 CL + 600 CE , 700 + PIS 600i.
case static ordering, atom C(?x) chosen r(?x, ?y) since C less
possible (and known) instances (350 versus 600):


350
200 + 350 PIS
200
CL +
CE ,
,
s(C(?x), {?x}) =
10, 000
10, 000
10, 000


700
600
700 + 600 PIS
s(D(?y), {?y}) =
CL +
CE ,
.
10, 000
10, 000
10, 000
Hence, order evaluation case P = (r(?x, ?y), C(?x), D(?y)) leading
200 (row 2) + 150 (row 4) + 40 (row 7) entailment checks. dynamic case,
evaluation r(?x, ?y), gives set solutions 1 , atom D(?y) fewer known
possible instances (50 known 50 possible) atom C(?x) (100 known
150 possible) and, hence, lower cost:
d(D(?y), 1 ) = h50 CL + 150 CL + 50 CE , 50 + 0 + 50 PIS i,
d(C(?x), 1 ) = h100 CL + 0 CL + 150 CE , 100 + 0 + 150 PIS i.
Note applying solution 1 D(?y) (C(?x)) results query atom
constant place ?y (?x). D(?y), case 250 r-instances, 200
handled look-up (50 turn known instances 150 turn
instances D), 50 require entailment check. Similarly, considering
C(?x), need 100 lookups 150 entailment checks. Note assume worst
case example, i.e., values ?x ?y take different. Therefore,
atom D(?y) chosen next, leading execution query atoms order
P = (r(?x, ?y), D(?y), C(?x)) execution 200 (row 2) + 50 (row 5) + 35 (row 6)
entailment checks.
273

fiKollia & Glimm

5.2 Cost Functions General Queries
explain order remaining simple complex axiom templates.
use statistics reasoner, whenever available. case reasoner
cannot give estimates, one still work statistics computed explicitly stated
information use upper bounds estimate reasoner costs result size axiom
templates.
first consider general concept assertion axiom template. Let KC [a] concepts
known instance, PC [a] concepts possible instance.
sets computed sets known possible instances concepts. input
form h?x(a), cost function defined
h|KC [a]| CL + |PC [a]| CE , |KC [a]| + PIS |PC [a]|i.
input form h?x(?y), i, cost function defined
*
+
X
X
(|K[C]| CL + |P [C]| CE ),
(|K[C]| + PIS |P [C]|) .
CNCO

CNCO

inputs form h?x(a), {?x}i h?x(?y), {?x, ?y}i, normalize functions
|NCO | |NIO ||NCO | respectively. inputs form h?x(?y), {?x}i h?x(?y), {?y}i
normalize function inputs form h?x(?y), |NCO | |NIO | respectively.
general role assertion axiom templates, several cases cost functions depending bound variables. next define cost functions cases. cost
functions cases similarly defined. input form h?z(?x, ?y), i,
cost function defined :
*
+
X
X
(|K[r]| CL + |P [r]| CE ),
(|K[r]| + PIS |P [r]|) .
rNRO

rNRO

inputs form h?z(a, ?y), i, cost function defined as:
*
+
X
X
(|sucK[r, a]| CL + |sucP[r, a]| CE ),
(|sucK[r, a]| + PIS |sucP[r, a]|) .
rNRO

rNRO

input form h?z(?x, ?y), {?z}i, cost function defined as:
+
*
X |K[r]| CL + |P [r]| CE X |K[r]| + PIS |P [r]|
,
.
|NRO |
|NRO |


rNR

rNR

Last, inputs form h?z(?x, ?y), {?x}i, two cost components computed as:
*
+
X |K[r]|
X |K[r]|
|P [r]|
|P [r]|
(
CL +
CE ),
+
PIS ) .
(
|sucK[r]|
|sucP[r]|
|sucK[r]| |sucP[r]|


rNR

rNR

274

fiOptimizing SPARQL Query Answering OWL Ontologies

concept (role) inclusion axiom templates form c1 c2 (r1 r2 ), c1 , c2
concept terms (r1 , r2 role terms), contain concept (role) names variables
need lookups computed concept (role) hierarchy order compute answers
(assuming concept (role) hierarchy precomputed).
One define similar cost functions types axiom templates either using
available statistics relying told information ontology. paper,
however, define cost function based assumption iterate
possible values respective variables one consistency check value.
Hence, define following general cost function cases:
h|N | CE , |N |i,
N {NCO , NRO , NIO } appropriate variable tested. discussed
Section 5.1, dynamic function based static one applied
described cases empty set bound variables.
Proposition 1. Let q query ontology O, static dynamic cost
functions defined Sections 5.1 5.2. ordering induced total order
axiom templates q.
Proof. cost functions defined kinds axiom templates return
two real numbers possible input. Since, according Definition 10, orders
based addition two real numbers, addition reals yields
real number, since total order reals, immediately get
total orders.
obvious ordering axiom templates affect soundness completeness query evaluation algorithm.

6. Complex Axiom Template Optimizations
section, first describe optimizations developed complex
axiom templates (Sections 6.1, 6.2) present procedure evaluating queries
(Section 6.3).
6.1 Axiom Template Rewriting
costly evaluate axiom templates rewritten axiom templates
evaluated efficiently yield equivalent result. go describe
axiom template rewriting technique, define concept template is, useful
throughout section.
Definition 12 (Concept Template). Let Sq = (NC , NR , NR , VC , VR , VI ) query
signature w.r.t. signature = (NC , NR , NI ). concept template Sq SROIQ
concept S, one also use concept variables VC place concept names,
role variables VR place role names individual variables VI place
individual names.
275

fiKollia & Glimm

Definition 13 (Rewriting). Let ontology, q axiom template Sq ,
t, t1 , . . . tn individuals individual variables Sq , C, C1 , . . . , Cn concept templates
Sq . function rewrite takes axiom template returns set axiom templates
follows:
= (C1 . . . Cn )(t), rewrite(at) = {C1 (t), . . . , Cn (t)};
= C C1 . . . Cn , rewrite(at) = {C C1 , . . . , C Cn };
= C1 . . . Cn C, rewrite(at) = {C1 C, . . . , Cn C};
= t1 . . . tn , rewrite(at) = {t1 t2 , t2 t3 , . . . , tn1 tn }.
understand intuition behind transformation, consider query
axiom template: ?x r.?y A. evaluation requires quadratic number consistency checks number concepts (since ?x ?y concept variables).
rewriting yields: ?x ?x r.?y. first axiom template evaluated
cheap cache lookup (assuming concept hierarchy precomputed).
second one, check usually resulting bindings ?x combined
concept names ?y.
Note Description Logics typically support n-ary equality axioms t1 . . .
tn , binary ones, whereas OWL, one typically also write n-ary equality axioms.
Since cost functions defined binary equality axioms, equivalently rewrite
n-ary one several binary ones. One could even optimize evaluation
atoms evaluating one binary equality axiom template propagating
binding found equivalent individuals equality axioms. valid
since equality congruence relation.
6.2 Concept Role Hierarchy Exploitation
number consistency checks required evaluate query reduced
taking concept role hierarchies account. concepts roles
classified (this ideally done system accepts queries), hierarchies
stored reasoners internal structures. use hierarchies prune
search space solutions evaluation certain axiom templates. illustrate
intuition example Infection hasCausalLinkTo.?x. solution
B holds, B also solution. Thus, searching solutions ?x,
choose next binding test traversing concept hierarchy top-down. find
non-solution A, subtree rooted concept hierarchy safely pruned.
Queries ontologies large number concepts deep concept hierarchy
can, therefore, gain maximum advantage optimization. employ similar
optimizations using role hierarchies.
example above, prune subconcepts ?x positive
polarity axiom template Infection hasCausalLinkTo.?x., i.e., ?x occurs positively
right hand side axiom template. case variable ?x negative polarity
axiom template form C1 C2 , i.e., ?x occurs directly indirectly
negation right hand side axiom template positively left-hand side
axiom template, one can, instead, prune superconcepts.
276

fiOptimizing SPARQL Query Answering OWL Ontologies

next specify precisely polarity concept variable concept axiom
template.
Definition 14 (Concept Polarity). Let ?x VC concept variable C, C1 , C2 ,
concept templates, r role, n IN0 . define polarity ?x C follows: ?x
occurs positively ?x. Furthermore, ?x occurs positively (negatively)
?x occurs negatively (positively) D,
C1 C2 C1 C2 ?x occurs positively (negatively) C1 C2 ,
r.D, r.D, > n r.D ?x occurs positively (negatively) D,
6 n r.D ?x occurs negatively (positively)
= n r.D ?x occurs D.
say ?x occurs positively (negatively) C1 C2 ?x occurs negatively
(positively) C1 positively (negatively) C2 . Note ?x occur positively
negatively concept template. define partial function polc maps
concept variable ?x concept template C (axiom template form C1 C2 )
pos ?x occurs positively C (C1 C2 ) neg ?x occurs negatively
C (C1 C2 ).
Note matter whether ?x occurs positively negatively concept template D,
concept template C form = n r.D, ?x occurs positively well negatively.
due fact C equivalent concept template 6 n r.D > n r.D
?x occurs positively well negatively. Since function polc defined
variables appear positively negatively, concept hierarchy cannot
exploited case. example, consider concept template ?x r.?x, (axiom
template ?x r.?x), ?x appears negatively ?x positively r.?x. Now,
let arbitrary element model = (I , ) ontology. obvious
instance r.A either B B holds, cannot deduce
instance B r.B.
proving correctness proposed optimization, first show relationship entailment concept membership, used subsequent proofs.
Lemma 1. Let q query w.r.t. query signature Sq = (NC , NR , NI , VC , VR , VI ),
q axiom template form C1 C2 C1 C2 concept templates
let mapping O. holds 6|= (C1 C2 ) iff exists
interpretation = (I , ) element |= 6 (C1 C2 )I .
Proof. 6|= (C1 C2 ) holds iff exists interpretation = (I , ) element
|= (C1 )I 6 (C2 )I , holds iff (C1 )I
(C2 )I , equivalent (C1 C2 )I , equivalent
((C1 C2 ))I , holds iff 6 (C1 C2 )I .
following theorem holds every axiom template form C1 C2 . Note
assume concept assertion templates form C(a) expressed
equivalent axiom templates {a} C. use C(?x)=A , concept name,
denote concept obtained applying extension also maps ?x A.
277

fiKollia & Glimm

Theorem 1. Let ontology, A, B concept names |= B, C1 , C2
concept templates, C1 C2 axiom template, C = C1 C2 , ?x VC concept variable
occurring C mapping covers variables C apart ?x.
1. polc (?x, C) = pos holds 6|= (C1 C2 )(?x)=B , 6|= (C1 C2 )(?x)=A .
2. polc (?x, C) = neg holds 6|= (C1 C2 )(?x)=A , 6|= (C1 C2 )(?x)=B .
Proof. Due Lemma 1, suffices show model = (I , )
element following (which formalized contrapositive form):
1. polc (?x, C) = pos holds (C(?x)=A )I , (C(?x)=B )I .
2. polc (?x, C) = neg holds (C(?x)=B )I , (C(?x)=A )I .
prove claim induction structure concept template C:
C =?x, ?x occurs positively C. Now, (?x(?x)=A )I , AI ,
easy see B since |= B assumption. Hence, (?x(?x)=B )I .
C = polc (?x, C) = pos, (D(?x)=A )I , show
(D(?x)=B )I . Note polc (?x, D) = neg. contrary shown,
assume (D(?x)=B )I . Since |= B induction hypothesis
(D(?x)=A )I (D(?x)=A )I contradiction. proof analogous
polc (?x, C) = neg.
C = C1 C2 polc (?x, C) = pos, ((C1 C2 )(?x)=A )I ,
(C1(?x)=A )I (C2(?x)=A )I . Since |= B induction hypothesis, (C1(?x)=B )I (C2(?x)=B )I . Thus, ((C1 C2 )(?x)=B )I .
proof analogous polc (?x, C) = neg.
proof C1 C2 analogous one C1 C2 .
C = r.D polc (?x, C) = pos, ((r.D)(?x)=A )I , least one rsuccessor, say , instance D(?x)=A . Since |= B induction
hypothesis, D(?x)=B . Hence, (r.(D(?x)=B ))I = ((r.D)(?x)=B )I .
proof analogous polc (?x, C) = neg.
C = r.D polc (?x, C) = pos, ((r.D)(?x)=A )I , (r.(D)(?x)=A )I
r-successors instance D(?x)=A . Since |= B induction hypothesis, r-successors also instances D(?x)=B . Hence,
(r.(D(?x)=B ))I = ((r.D)(?x)=B )I . proof analogous polc (?x, C) = neg.
C = > n r.D polc (?x, C) = pos, ((> n r.D)(?x)=A )I ,
least n distinct r-successors instances D(?x)=A . Since |= B
induction hypothesis, successors instances D(?x)=B . Hence,
least n distinct r-successors instances D(?x)=B and, therefore, (>
n r.(D)(?x)=B )I = ((> n r.D)(?x)=B )I . proof analogous polc (?x, C) = neg.
278

fiOptimizing SPARQL Query Answering OWL Ontologies

C = 6 n r.D polc (?x, C) = pos, ((6 n r.D)(?x)=A )I , show
((6 n r.D)(?x)=B )I . Note polc (?x, D) = neg. contrary
shown, assume ((6 n r.D)(?x)=B )I , i.e., ((> n + 1 r.D)(?x)=B )I .
Hence, least n + 1 distinct r-successors instances D(?x)=B .
Since polc (?x, D) = neg induction hypothesis, D(?x)=B instances
also D(?x)=A instances (> n + 1 r.(D)(?x)=A )I = ((> n + 1 r.D)(?x)=A )I ,
contradiction. proof analogous polc (?x, C) = neg.
C = (= n r.D), polarity ?x C always positive negative,
polc (?x, C) undefined case cannot occur.
extend optimization case role variables first define
polarity role variable concept axiom template.
Definition 15 (Role Polarity). Let ?x VR role variable, C, C1 , C2 , concept
templates, r role, n IN0 . define polarity ?x C follows: ?x occurs
positively ?x.D, ?x .D, > n ?x.D, > n ?x .D, = n ?x.D, = n ?x .D; ?x
occurs negatively ?x.D, ?x .D, 6 n ?x.D, 6 n ?x .D, = n ?x.D, = n ?x .D.
Furthermore, ?x occurs positively (negatively)
?x occurs negatively (positively) D,
C1 C2 C1 C2 ?x occurs positively (negatively) C1 C2 ,
r.D, ?x.D, ?x .D, > n r.D, > n ?x.D, > n ?x .D, r.D, ?x.D, ?x .D
?x occurs positively (negatively) D,
6 n r.D, 6 n ?x.D, 6 n ?x .D ?x occurs negatively (positively) D,
= n r.D ?x occurs D.
say ?x occurs positively (negatively) C1 C2 ?x occurs negatively
(positively) C1 positively (negatively) C2 . define partial function polr
maps role variable ?x concept template C (axiom template form C1 C2 )
pos ?x occurs positively C (C1 C2 ) neg ?x occurs negatively
C (C1 C2 ).
Note also make assumption occurrences ?x first
part definition.
show, hierarchy optimization also applicable role variables, provided occur positively negatively.
Theorem 2. Let ontology, r, role names |= r s, C1 , C2 concept
templates, C1 C2 axiom template, C = C1 C2 , ?x VR role variable occurring
C mapping covers variables C apart ?x.
1. polr (?x, C) = pos holds 6|= (C1 C2 )(?x)=s , 6|= (C1 C2 )(?x)=r .
2. polr (?x, C) = neg holds 6|= (C1 C2 )(?x)=r , 6|= (C1 C2 )(?x)=s .
279

fiKollia & Glimm

Proof. Due Lemma 1, suffices show model = (I , )
element following (which formalized contrapositive form):
1. polr (?x, C) = pos holds (C(?x)=r )I , (C(?x)=s )I .
2. polr (?x, C) = neg holds (C(?x)=s )I , (C(?x)=r )I .
prove claim induction structure concept template C:
C = ?x.D, concept template contain ?x.
polr (?x, C) = pos. Assume, ((?x.D)(?x)=r )I , is, (r.(D))I .
h, r (D)I . Since |= r s, also
h, sI and, therefore, (s.(D))I = ((?x.D)(?x)=s )I .
C = ?x.D, concept template contain ?x.
polr (?x, C) = neg. ((?x.D)(?x)=s )I , show ((?x.D)(?x)=r )I .
contrary shown, assume ((?x.D)(?x)=r )I , i.e.,
(r.(D))I . Hence, h, r (D)I .
Since |= r s, also h, sI and, therefore,
/ (s.(D))I =

((?x.D)(?x)=s ) , contradiction.
C = > n ?x.D concept template contain ?x.
polr (?x, C) = pos. Assume, ((> n ?x.D)(?x)=r )I , (> n r.(D))I
least n distinct r-successors instances (D). Since |= r
r-successors also s-successors and, therefore, (> n s.(D))I = ((>
n ?x.D)(?x)=s )I .
C = 6 n ?x.D C concept template contain ?x.
polr (?x, C) = neg. ((6 n ?x.D)(?x)=s )I , show
((6 n ?x.D)(?x)=r )I . contrary shown, assume ((6
n ?x.D)(?x)=r )I , i.e., (> n + 1 r.(D))I . Hence, least n + 1 distinct
r-successors, instances (D). Since |= r s, r-successors
also s-successors ((> n + 1 s.(D)))I = ((> n + 1 ?x.D)(?x)=s )I ,
contradiction.
C = C1 C2 polr (?x, C) = pos, ((C1 C2 )(?x)=r )I , (C1 (?x)=r )I
(C2 (?x)=r )I . Since |= r induction hypothesis,
(C1(?x)=s )I (C2(?x)=s )I . Thus, ((C1 C2 )(?x)=s )I . proof
analogous polr (?x, C) = neg.
proof C1 C2 analogous one C1 C2 .
C = polr (?x, C) = pos, (D(?x)=r )I , show
(D(?x)=s )I . Note polr (?x, D) = neg. contrary shown,
assume (D(?x)=s )I . Since |= r induction hypothesis
(D(?x)=r )I (D(?x)=r )I contradiction. proof analogous
polr (?x, C) = neg.
280

fiOptimizing SPARQL Query Answering OWL Ontologies

C = p.D polr (?x, C) = pos, also polr (?x, D) = pos. Now,
((p.D)(?x)=r )I , least one p-successor instance D(?x)=r .
Since |= r induction hypothesis, p-successor instance
D(?x)=s . Hence, ((p.D)(?x)=s )I . proof analogous polr (?x, C) = neg.
C = ?x.D polr (?x, C) = pos, also polr (?x, D) = pos. Note
?x occurs since otherwise case handled already above. Now,
((?x.D)(?x)=r )I , least one r-successor instance
D(?x)=r . Since |= r induction hypothesis, least one s-successor
instance D(?x)=s . Hence, ((?x.D)(?x)=s )I .
C = p.D polr (?x, C) = pos, also polr (?x, D) = pos. Now,
((p.D)(?x)=r )I , (p.(D)(?x)=r )I p-successor instance
D(?x)=r . Since |= r induction hypothesis, p-successors also
instances D(?x)=s . Hence, (p.(D(?x)=s ))I = ((p.D)(?x)=s )I . proof
analogous polr (?x, C) = neg.
C = ?x.D polr (?x, C) = neg, also polr (?x, D) = neg. Note
?x occurs since otherwise case handled already above. Now,
((?x.D)(?x)=s )I , show ((?x.D)(?x)=r )I . contrary
shown, assume
/ ((?x.D)(?x)=r )I , i.e., (r.(D)(?x)=r )I .
Hence, h, r ((D)(?x)=r )I . Since
|= r s, also s-successor and, induction hypothesis,
((D)(?x)=s )I contradiction.
C = > n p.D polr (?x, C) = pos, (( > n p.D)(?x)=r )I ,
least n distinct p-successors instances D(?x)=r . Since |= r
induction hypothesis, p-successors also instances D(?x)=s . Hence,
(( > n p.D)(?x)=s )I . proof analogous polr (?x, C) = neg
C = > n ?x.D polr (?x, C) = pos, also polr (?x, D) = pos. Note
?x occurs since otherwise case handled already above. Now,
(( > n ?x.D)(?x)=r )I , least n distinct r-successors
instances D(?x)=r . Since |= r induction hypothesis, least n
distinct s-successors instances D(?x)=s . Hence, (( > n ?x.D)(?x)=s )I .
C = 6 n p.D polr (?x, C) = pos, ((6 n p.D)(?x)=r )I , show
((6 n p.D)(?x)=s )I . Note polr (?x, D) = neg. contrary
shown, assume ((6 n p.D)(?x)=s )I , i.e., ((> n + 1 p.D)(?x)=s )I .
Hence, least n + 1 distinct p-successors instances D(?x)=s .
Since polr (?x, D) = neg induction hypothesis, D(?x)=s instances
also D(?x)=r instances (> n + 1 p.(D)(?x)=r )I = ((> n + 1 p.D)(?x)=r )I ,
contradiction. proof analogous polr (?x, C) = neg.
C = 6 n ?x.D polr (?x, C) = neg, polr (?x, D) = pos. Note
?x occurs since otherwise case handled already above. ((6
n ?x.D)(?x)=s )I show ((6 n ?x.D)(?x)=r )I . contrary
281

fiKollia & Glimm

Algorithm 2 getPossibleMappings(O, ?x, at, )
Input: O: queried SROIQ ontology
?x: concept role variable
at: axiom template ?x occurs
: mapping ?x dom()
Output: set mappings
1: :=
2: ?x VC
3:
polc (?x, at) = pos
4:
:= { | (?x) = A, direct subconcept (?x) O,
(?y) = (?y) ?y dom() \ {?x}}
5:
else
6:
:= { | (?x) = A, direct superconcept (?x) O,
(?y) = (?y) ?y dom() \ {?x}}
7:
end
8: else
9:
polr (?x, at) = pos
10:
:= { | (?x) = r, r direct subrole (?x) O,
(?y) = (?y) ?y dom() \ {?x}}
11:
else
12:
:= { | (?x) = r, r direct superrole (?x) O,
(?y) = (?y) ?y dom() \ {?x}}
13:
end
14: end
15: return
shown, assume ((6 n ?x.D)(?x)=r )I , i.e., ((>
n + 1 ?x.D)(?x)=r )I . Hence, least n + 1 distinct r-successors instances D(?x)=r . Since |= r s, induction hypothesis, r-successors
also s-successors instances D(?x)=s . Hence, ((> n + 1 ?x.D)(?x)=s )I
((6 n ?x.D)(?x)=s )I , contradiction.
C = (= n ?x.D) C = (= n r.D), polarity ?x C always positive
negative, polr (?x, C) undefined case cannot occur.
cases ?x occurring form inverse (?x ) analogous, given
|= r iff |= r .
Algorithm 2, explain detail Section 6.3, shows use
theorems create possible concept role mappings concept role variable ?x
appears positively negatively axiom template C1 C2 .
6.3 Query Answering Algorithm
Algorithm 3 shows optimized way evaluating queries using static ordering. First,
axiom templates simplified possible (method rewrite line 1). Next, method
282

fiOptimizing SPARQL Query Answering OWL Ontologies

Algorithm 3 evaluate(O, q)
Input: O: queried SROIQ ontology
q: query
Output: set solutions evaluating q
1: := rewrite(q)
2: At1 , . . . , Atm :=connectedComponents(At)
3: j=1, . . . ,
4:
Rj := {0 | dom(0 ) = }
5:
at1 , . . . , atn := order(Atj )
6:
= 1, . . . , n
7:
R :=
8:
Rj
9:
isSimple(ati ) Vars(ati ) \ dom() 6=
10:
R := R { | callSpecificReasonerTask((ati ))}
11:
else Vars(ati ) \ dom() =
12:
|= (ati )
13:
R := R {}
14:
end
15:
else
16:
Vopt := {?x |?x 6 dom(), Theorem 1 2 applies ?x ati }
17:
B := initializeVariableMappings(O, ati , , Vopt )
18:
B 6=
19:
:= removeMapping(B)
20:
|= (ati )
21:
R := R { | (?x) = (?x) ?x
/ Vopt
(?x) = C ?x Vopt VC , |= C (?x)
(?x) = r ?x Vopt VR , |= r (?x)}
22:
?x Vopt
23:
B := B getPossibleMappings(O, ?x, ati , )
24:
end
25:
end
26:
end
27:
end
28:
end
29:
Rj := R
30:
end
31: end
32: Rans := {1 . . . | j Rj , 1 j m}
33: return Rans

connectedComponents (line 2) partitions axiom templates sets connected components, i.e., within component templates share common variables, whereas
components shared variables. Unconnected components unnecessarily increase
amount intermediate results and, instead, one simply combine results
283

fiKollia & Glimm

Algorithm 4 initializeVariableMappings(O, at, , Vopt )
Input: O: queried SROIQ ontology
at: axiom template
: partial mapping
Vopt : variables Theorem 1 2 applies
Output: set mappings
1: := {}
2: ?x Vars(at) \ dom()
3:
R :=
4:
?x VC ?x Vopt
5:

6:
polc (?x, at) = pos
7:
(?x) :=
8:
else
9:
(?x) :=
10:
end
11:
R := R { }
12:
end
13:
else ?x VR ?x Vopt
14:

15:
polr (?x, at) = pos
16:
(?x) := r
17:
else
18:
(?x) := r
19:
end
20:
R := R { }
21:
end
22:
else
23:
R := { | (?x) = a, NCO NRO NIO (?y) = 1 (?y)
1 ?y dom(1 )}
24:
end
25:
:= R
26: end
27: return

components end (line 32). component, proceed described below:
first determine order (method order line 5) described Section 5. simple axiom template, contains far unbound variables, call specialized reasoner method
retrieve entailed results, i.e., mappings unbound variables (callSpecificReasonerTask
line 10). Note mappings assign values variables covered
already computed (partial) solution since instantiate atom ati .
allows defining union setting ( )(v) = (v) v dom(),
( )(v) = (v) otherwise. templates variables bound, check
whether mappings lead entailed axioms (lines 11 14). cases, i.e.,
284

fiOptimizing SPARQL Query Answering OWL Ontologies

complex axiom templates unbound variables, check compatible mappings
yield entailed axiom (lines 15 27). particular, first initialize set B candidate mappings unbound variables axiom template (line 17, refers
Algorithm 4). Algorithm 4 initializes unbound variables axiom templates
Theorem 1 2 applies (r ) (r ) depending whether respective polarity
function returns pos neg. template variables optimization applicable, compatible mappings returned. method removeMapping (line 19) returns
mapping B deletes mapping B. instantiate axiom template check entailment. case entailment holds, first extend set R
current mapping mappings map optimization variables equivalent
concepts roles respective variable mappings (line 21) afterwards extend set B possible mappings variables hierarchy optimization
applicable (getPossibleMappings line 23). example, checked mapping
maps concept variable ?x concept ?x occurs positively axiom
template, add set B mappings map ?x direct subconcept3
(see Algorithm 2 line 4). implementation use involved procedure, i.e.,
order avoid checking entailment instantiated axiom template
mapping, case concept (role) hierarchy traversal
perform, keep track already processed mappings check
checked previous iteration loop (lines 18 26). ease
presentation, shown Algorithm 3. repeat procedure B
empty (lines 18 26).
dynamic ordering, Algorithm 3 changed follows: first compute
number axiom templates Atj ; n := |Atj |. swap line 5 line 6, i.e.,
instead ordering axiom templates loop evaluates axiom templates,
order within loop. function order gets additional input parameter
set currently computed solutions returns next cheapest axiom template
according dynamic ordering function. Hence, ati := order(Atj , Rj ) instead
at1 , . . . , atn := order(Atj ). insert line calling order remove
cheapest axiom template current component: Atj := Atj \ {ati }. result,
next iteration loop compute cheapest axiom template amongst
yet evaluated templates until, last iteration, one axiom template left.
Algorithm 3 sound complete. soundness completeness algorithm
based following facts:
method rewrite (see Definition 13) affect answers query q, since
rewrites axiom templates templates set answers.
method connectedComponents affect answers q; splits
query several components evaluated separately take
cartesian product answers.
method order change query way; reorders axiom
templates.
3. say concept name direct subconcept concept name B w.r.t. O, |= B
concept name |= B, |= 6|= . similar
way define direct superconcept, direct subrole direct superrole.

285

fiKollia & Glimm

actual axiom template evaluation, iterate templates
query taking account mappings already computed
evaluation previous templates distinguish three cases:
1. axiom template simple one contains unbound variables. use
specialized reasoner tasks compute entailed mappings since use
sound complete reasoner result indeed sound complete.
2. axiom template contain unbound variables. case, simply
check entailment using sound complete reasoner.
3. axiom template complex template least one variable unbound.
variables optimization Section 6.2 applicable, initialize
variables /r (/r ) traverse concept/role hierarchy topdown (bottom-up). prune mappings according Theorems 1 2 case
checked mapping constitute solution mapping. case,
extend set possible mappings B. variables axiom templates
hierarchy optimization applicable, check compatible
mappings. Thus, due Theorem 1 2 procedure sound complete.
Although algorithm implemented HermiT reasoner, one compute answers query using (hyper)tableau reasoner.

7. Evaluation
tested developed optimizations standard benchmarks range custom
queries test complex axiom template evaluation expressive ontologies.
experiments performed Mac OS X Lion machine 2.53 GHz Intel Core i7
processor Java 1.6 allowing 1GB Java heap space. measure time one-off
tasks classification separately since tasks usually performed
system accepts queries. ontologies code required perform experiments
available online (Kollia & Glimm, 2013). developed system (Glimm & Kollia,
2013), called OWL-BGP, implemented SPARQL Wrapper used
reasoner implements OWLReasoner interface OWL API (Horridge &
Bechhofer, 2009). Section 7.1 compare different ordering strategies
developed two benchmarks (LUBM UOBM) contain queries variables
place individuals (query atoms). also show effect ordering LUBM using
custom queries simple axiom templates created SPARQL-DL (Kremen &
Sirin, 2008). Section 7.2 show effect proposed optimizations queries
complex axiom templates. evaluation used HermiT hypertableau
reasoner (Motik, Shearer, Glimm, Stoilos, & Horrocks, 2013). reasoners
Pellet (Clark & Parsia, 2013a) Racer Pro (Racer Systems GmbH & Co. KG, 2013) could
equally well used implementation long provide interface
required statistics, i.e., number known possible instances concepts roles
computation cost functions used query ordering. Without optimizations,
providing interface statistics easily realized described current
paper. presented query ordering techniques also used optimizations
286

fiOptimizing SPARQL Query Answering OWL Ontologies

caching, pseudo model merging techniques, binary instance retrieval, absorption
employed. cost functions might, however, require adaptation take reduction
required number consistency checks account. example, Pellet uses binary
instance retrieval, testing possible instances concept realized splitting
candidate instances two partitions. partition, single consistency check
performed. consistency check successful, safe consider individuals
belonging partition non-instances tested concept A. Otherwise,
split partition process resulting partitions way. case, one
performs one consistency check potentially determine several (non-)instances A,
reflected cost functions.
also worth noting TrOWL reasoning framework (Thomas, Pan, & Ren,
2013) started use SPARQL wrapper provide SPARQL support. adaptation
also provide statistics is, best knowledge, still outstanding, although
straightforward. TrOWL based two approximate reasoners: one underapproximates (computation concept role instances sound, incomplete) (Ren, Pan, &
Zhao, 2010) one overapproximates (computation concept role instances
complete, unsound) (Pan, Thomas, & Zhao, 2009). setting, underapproximation straightforwardly seen known instances overapproximation
minus underapproximation possible instances.
7.1 Query Ordering
tested ordering techniques Lehigh University Benchmark (LUBM) (Guo,
Pan, & Heflin, 2005) case disjunctive information present
expressive University Ontology Benchmark (UOBM) (Ma, Yang, Qiu, Xie, Pan, &
Liu, 2006).
first used 14 conjunctive ABox queries provided LUBM. these, queries
2, 7, 8, 9 interesting ones setting since contain many atoms
ordering effect running time. tested queries LUBM(1,0)
LUBM(2,0) contain data one two universities respectively, starting index
0. LUBM(1,0) contains 17,174 individuals LUBM(2,0) contains 38,334 individuals.
LUBM(1,0) took 19 load 0.092 classification initialization known
possible instances concepts roles. clustering approach concepts took 1
resulted 16 clusters. clustering approach roles lasted 4.9 resulted
17 role successor clusters, 29 role predecessor clusters 87 role clusters. LUBM(2,0)
took 48.5 load 0.136 classification initialization known possible
instances. clustering approach concepts took 3.4 resulted 16 clusters.
clustering approach roles lasted 16.3 resulted 17 role successor clusters, 31 role
predecessor clusters 102 role clusters. Table 2 shows execution time
four queries LUBM(1,0) LUBM(2,0) four cases: i) use static
algorithm (columns 2 6), ii) use dynamic algorithm (columns 3 7), iii)
use random sampling, i.e., taking half individuals returned (from
evaluation previous query atoms) run, decide next cheapest atom
evaluated dynamic case iv) using proposed sampling approach
based clusters constructed individuals queried ontology (columns 4
287

fiKollia & Glimm

Q
2
7
8
9

Static
51
25
485
1,099

LUBM(1,0)
Dynamic RSampling
119
390
29
852
644
639
2,935
3,021

CSampling
37
20
551
769

Static
162
70
622
6,108

LUBM(2,0)
Dynamic RSampling
442
1,036
77
2,733
866
631
23,202
14,362

CSampling
153
64
660
3,018

Table 2: Query answering times milliseconds LUBM(1,0) LUBM(2,0) using i)
static algorithm ii) dynamic algorithm, iii) 50% random sampling (RSampling),
iv) constructed individual clusters sampling (CSampling)

Q

PlansNo

2
7
8
9

336
14
56
336

Chosen Plan Order
Static Dynamic Sampling
2
1
1
1
1
1
1
1
1
173
160
150

Pellet Plan

Worst Plan

51
25
495
1,235

4,930
7,519
1,782
5,388

Table 3: Statistics constructed plans chosen orderings running times
milliseconds orderings chosen Pellet worst constructed plans

8). queries marked (*) queries static dynamic algorithms
result different ordering. Queries 7 8 observe increase running
time dynamic technique used (in comparison static) especially
evident Query 8 LUBM(2,0), number individuals ontology
intermediate result sizes larger. Dynamic ordering also behaves worse static
Queries 2 9. happens because, although dynamic algorithm chooses
better ordering static algorithm, intermediate results (that need checked
iteration determine next query atom executed) quite large
hence cost iterating possible mappings dynamic case far outweighs
better ordering obtained. also observe random sampling collecting
ordering statistics dynamic case (checking 50% individuals i1 randomly
detecting next query atom executed) leads much worse results
queries plain static dynamic ordering. happens since random sampling often
leads choice worse execution order. use cluster based sampling method
performs better plain dynamic algorithm queries. Queries 2 9,
gain better ordering dynamic algorithm sampling used
much evident. case since use one individual every cluster
cost functions computation number clusters much smaller
number otherwise tested individuals run.
order show effectiveness proposed cost functions compared
running times valid plans (plans comply connectedness condition
Definition 10, i.e., plans consecutive atoms share least one common variable)
288

fiOptimizing SPARQL Query Answering OWL Ontologies

LUBM(3,0) LUBM(4,0) LUBM(5,0) LUBM(6,0) LUBM(7,0) LUBM(8,0) LUBM(9,0)
55,664
78,579
102,368
118,500
144,612
163,552
183,425
Table 4: Number individuals LUBM increasing number universities
running time plan chosen method. following show results
LUBM(1, 0), results LUBM(2,0) comparable. Table 3 show,
query, number valid plans constructed according Definition 10
(column 2), order plan chosen static, dynamic, cluster based sampling
methods order valid plans execution time (columns 3,4,5; e.g., value
2 indicates ordering method chose second best plan), running time
HermiT plan created Pellet (column 6) well running time
worst constructed plan (column 7).
comparison ordering approach approach followed reasoners
support conjunctive query answering Pellet Racer Pro straightforward. case Pellet Racer many optimizations instance
retrieval (Sirin et al., 2007; Haarslev & Moller, 2008), HermiT have. Thus,
comparison execution times reasoners HermiT would convey
much information effectiveness proposed query ordering techniques.
idea comparing orderings computed reasoners computed
methods also informative since orderings chosen different reasoners
depend much way queries evaluated costs specific tasks
reasoners and, hence, reasoner dependent, i.e., ordering good one
reasoner leads efficient evaluation may good another reasoner.
note searching orderings according Pellet, switched
simplification optimization Pellet implements regarding exploitation domain range axioms queried ontology reducing number query atoms
evaluated (Sirin & Parsia, 2006). done order better evaluate
difference plain ordering obtained Pellet HermiT since cost functions take
account query atoms.
observe queries apart Query 9 orderings chosen algorithms
(near)optimal ones. Queries 2 7, Pellet chooses ordering
algorithms. Query 8, Pellet chooses ordering which, evaluated HermiT,
results higher execution time. Query 9, algorithms choose plans
middle order valid plans w.r.t. query execution time, means
algorithms perform well query. greedy techniques
used find execution plan take account local information
choose next query atom executed. Interestingly, use cluster based sampling
led finding better ordering, see running time Table 2
better ordering plan found cluster based sampling techniques compared
static plain dynamic ordering (Table 3). ordering chosen Pellet Query 9
also perform well. see that, queries, worst running times many
orders magnitude greater running times achieved ordering algorithms.
general, observe LUBM static techniques adequate use dynamic
ordering improve execution time much compared static ordering.
289

fiKollia & Glimm

Q LUBM(3,0) LUBM(4,0) LUBM(5,0) LUBM(6,0) LUBM(7,0) LUBM(8,0) LUBM(9,0)
2
0.35
0.62
1.26
1.71
2.26
3.11
4.18
7
0.11
0.16
0.23
0.32
0.33
0.33
0.40
8
0.77
0.91
1.27
1.29
1.34
1.44
1.65
9
18.49
42.98
85.54
116.88
181.07
235.06
312.71

20.64
55.16
90.99
138.84
213.59
241.85
323.15
Table 5: Query answering times seconds LUBM increasing number universities
Q Static Dynamic CSampling PlansNo
4 13.35
9 186.30
11
0.98
12
0.01
14 94.61
q1 191.07
q2 47.04

13.40
188.58
0.84
0.01
90.60
98.24
22.20

13.41
185.40
1.67
0.01
93.40
100.25
22.51

14
8
30
4
14
6
6

Chosen Plan Order
Pellet
Static Dynamic Sampling
Plan
1
1
1
13.40
1
1
1
636.91
1
1
1
0.98
1
1
1
0.01
2
1
1 > 30 min
2
1
1 > 30 min
2
1
1
22.2

Worst
Plan
271.56
636.91
> 30 min
> 30 min
> 30 min
> 30 min
> 30 min

Table 6: Query answering times seconds UOBM (1 university, 3 departments)
statistics

order show scalability system, next run LUBM queries
different numbers universities, i.e., LUBM(i,0) ranges 3 9. Table 4 shows
number individuals appearing ABox different university size. running
times Queries 2, 7, 8, 9 well running time 14 LUBM queries
shown Table 5. results LUBM(1,0) LUBM(2,0) shown Table 2. Note
results shown case static ordering performed. table
see queries, running time increases number individuals
ABox increases, reasonable. observe query answering ontologies
still scalable query answering databases so,
expressive schema taken account fact incomplete
information contrast databases complete information.
Unlike LUBM, UOBM ontology contains disjunctions reasoner makes also
nondeterministic derivations. order reduce reasoning time, removed nominals used first three departments containing 6,409 individuals. resulting
ontology took 16 load 0.1 classify initialize known possible instances. clustering approach concepts took 1.6 resulted 356 clusters.
clustering approach roles lasted 6.3 resulted 451 role successor clusters, 390
role predecessor clusters 4,270 role clusters. present results static dynamic algorithms Queries 4, 9, 11, 12 14 provided UOBM,
interesting ones consist many atoms. queries contain one
atom possible instances. see Table 6, static dynamic ordering show
290

fiOptimizing SPARQL Query Answering OWL Ontologies

similar performance Queries 4, 9, 11 12. Since available statistics case
quite accurate, methods find optimal plans intermediate result set sizes
small. ordering methods, atoms possible instances queries
executed last. Query 14, dynamic algorithm finds better ordering results
comparable performance. effect cluster based sampling technique
running time obvious case LUBM. happens current
experiment intermediate result sizes large and, importantly,
gain obtained due sampling order milliseconds whereas total query
answering times order seconds obscuring small improvement running
time due sampling. queries orderings created Pellet result
worse running times orderings created algorithms.
order illustrate dynamic ordering performs better static, also created
two custom queries:
q1 = { isAdvisedBy(?x,?y), GraduateStudent(?x), Woman(?y) }
q2 = { SportsFan(?x), GraduateStudent(?x), Woman(?x) }
queries, P [GraduateStudent], P [Woman] P [isAdvisedBy] non-empty, i.e.,
query concepts roles possible instances. running times dynamic
ordering smaller since accurate statistics result smaller number possible
instances checked query execution. particular, static
ordering, 151 41 possible instances checked query q1 q2 , respectively,
compared 77 23 dynamic ordering. Moreover, intermediate results
generally smaller dynamic ordering static leading significant reduction
running time queries. Interestingly, query q2 could answered within
time limit 30 minutes transformed three query concepts conjunction,
i.e., asked instances intersection three concepts.
complex concepts reasoner longer use information known possible
instances falls back naive way computing concept instances. Again,
reasons before, sampling techniques apparent effect running
time queries.
query SPARQL-DL tests issued LUBM(1,0) (Kremen & Sirin, 2008)
(cf. Table 7), Table 8 shows running time plan chosen method (column
2), number valid plans, i.e., plans comply connectedness condition
Definition 10 (column 3), order chosen plan order valid plans
execution times (column 4), running time HermiT plan created
Pellet (column 5) well running time worst constructed plan (column 6).
queries shown Table 7 ordered according static ordering algorithm. Since
reasoning LUBM deterministic, use static planning order axiom templates.
Dynamic planning improve execution times (actually makes worse)
since, explained before, deterministic reasoning
important information ordering beginning overhead caused dynamic
ordering results worse query execution time.
results Table 8 one observe Queries 1, 2, 3, 4 8
proposed ordering chooses optimal plan among valid plans. Queries 5, 6, 7, 9
10 optimal plan chosen according proposed cost estimation algorithm.
Queries 5, 7, 9 10 due greedy techniques used finding
291

fiKollia & Glimm

GraduateStudent(?x)
?y(?x, ?z)
Course(?w)

1

GraduateStudent(?x)
?y(?x, ?w)
?z(?w)
GraduateCourse ?z
7
?c
?c(?x)
teachingAssistantOf(?x, ?y)
takesCourse(?x, ?y)
8
?c Person
?c(?x)
advisor(?x, ?y)
9
?c Person
?c(?x)
teachingAssistantOf(?x, ?y)
Course(?y)
10
?p worksFor
?p(?y, ?w)
?c(?y)
?c Faculty
advisor(?x, ?y)
GraduateStudent(?x)
memberOf(?x, ?w)
6

?c Employee
?c(?x)
Student(?x)
undergraduateDegreeFrom(?x, ?y)
3
?y memberOf
?y(?x, University0)
Person(?x)
4
?y(GraduateStudent5, ?w)
?z(?w)
?z Course
2

?z Course
?z(?w)
?y(?x, ?w)
GraduateStudent(?x)

5

Table 7: Queries used SPARQL-DL tests
Query
1
2
3
4
5
6
7
8
9
10

Chosen Ordering
Time
0.36
0.03
0.05
0.01
26.10
10.49
0.42
0.23
0.19
0.80

PlansNo
2
14
4
4
8
8
14
4
8
812

Chosen Plan
Order
1
1
1
1
5
2
6
1
4
21

Pellet Plan
Time
0.36
0.37
5.44
0.01
0.95
10.49
2.68
0.23
0.19
0.80

Worst Plan
Time
0.58
0.61
5.45
11.46
454.25
499.65
2.68
0.80
0.47
992.77

Table 8: Query answering times seconds queries Table 7 LUBM(1,0)
statistics

iteration ordering algorithm next cheapest axiom template evaluated.
example, optimal plan Query 10 starts template GraduateStudent(?x),
cheapest one according cost based technique then, moving
292

fiOptimizing SPARQL Query Answering OWL Ontologies

connected templates, different order chosen order chosen algorithm.
turns valid plans beginning atom GraduateStudent(?x) lead better
execution times plan chosen algorithm resulting existence several
better plans chosen one.
Query 6 find optimal plan overestimated cost
disjoint axiom template hence missed optimal ordering. Nevertheless,
chosen plans lead execution times queries three orders magnitude
lower worst plans chosen. queries proposed
ordering lead optimal plan, one additionally take account
time saved computing costs |q!| possible orderings,
high. Apart Queries 4, 6 8, observe plans produced Pellet
optimal evaluated HermiT. discussed before, happens
statistics created ordering reasoner specific hence good ordering
one reasoner may good another reasoner.
7.2 Complex Axiom Template Optimizations
absence suitable standard benchmarks arbitrary SPARQL queries, created
custom set queries shown Tables 10 12 GALEN FBbt XP
ontology, respectively. Systems fully support SPARQL Direct Semantics entailment
regime still development, makes hard compare results
kinds queries systems.
GALEN biomedical ontology. expressivity (Horn-)SHIF consists
2,748 concepts 413 abstract roles. FBbt XP ontology taken Open
Biological Ontologies (OBO) Foundry (OBO Foundry, 2013). falls SHI fragment
SROIQ consists 7,221 concepts 21 abstract roles. consider
TBox part FBbt XP since ABox relevant showing different effects
proposed optimizations execution times considered queries. GALEN took
3.7 load 11.1 classify (concepts roles), FBbt XP took 1.5 load
7.4 classify.
execution times queries Tables 10 12 shown right-hand
side Tables 9 11, respectively. set time limit 30 minutes
query. query, tested execution without optimizations
combination applicable optimizations Sections 5 6. Tables 9 11,
one also see number consistency checks performed evaluation
query combination applicable optimizations well number
results query. tables taken time worst ordering
query atoms cases ordering optimization applicable enabled.
Note complex axiom templates require consistency checks evaluated;
simple ones (subsumption axiom templates case) need cache lookups
reasoners internal structures since concepts roles already classified.
GALEN Queries: expected, increase number variables within axiom
template leads significant increase query execution time number
mappings checked grows exponentially number variables. can,
particular, observed difference execution time Query 1 2.
293

fiKollia & Glimm

Query
1
1
2
2
3
3
3
4
4
4
4
5
5
5

Reordering

Hierarchy
Exploitation

Rewriting

x
x
x

x
x
x
x
x

x
x
x
x
x

Consistency
Checks
2,750
50
1,141,250
1,291

x
x

19,250
3,073

x
x
x

16,135
197
1,883
1,883

x

Time

AnswersNo

1.68
0.18
578.98
9.85
>30 min
102.37
2.69
> 30 min
> 30 min
7.68
1.12
> 30 min
0.67
0.8

10
10
214
214
2,816
2,816

51
51
4,392
4,392

Table 9: Query answering times seconds queries Table 10 without
optimizations
1
Infection hasCausalLinkTo.?x
2
Infection ?y.?x
3
?x Infection hasCausalAgent.?y
4 NAMEDLigament NAMEDInternalBodyPart ?x
?x hasShapeAnalagousTo?y ?z.linear
5
?x NonNormalCondition
?z ModifierAttribute
Bacterium ?z.?w
?y StatusAttribute
?w AbstractStatus
?x ?y.Status
Table 10: Sample complex queries GALEN ontology
two queries, evident use hierarchy exploitation optimization
leads decrease execution time two orders magnitude. Query 3
completed time limit least query rewriting optimization enabled.
get improvement three orders magnitude query, using rewriting
combination hierarchy exploitation. Query 4 completed given
time limit least reordering rewriting enabled. Rewriting splits first axiom
template following two simple axiom templates, evaluated much
efficiently:
NAMEDLigament NAMEDInternalBodyPart
294



NAMEDLigament ?x

fiOptimizing SPARQL Query Answering OWL Ontologies

rewriting, ordering optimization even pronounced effect since
rewritten axiom templates evaluated simple cache lookup. Without
ordering, complex axiom template could executed simple ones,
leads inability answering query within time limit 30 min. Without
good ordering, Query 5 also answered within time limit. ordering
chosen algorithm shown below. Note query consists two connected
components: one axioms containing ?z ?w another one axioms
containing ?x ?y.
?z ModifierAttribute
?w AbstractStatus
Bacterium ?z.?w
?y StatusAttribute
?x NonNormalCondition
?x ?y.Status
query, hierarchy exploitation optimization improve execution time
since, due chosen ordering, variables hierarchy optimization
applied, already bound comes evaluation complex templates.
Hence, running times without hierarchy exploitation similar.
number consistency checks significantly lower number answers
overall results computed taking cartesian products results two
connected components. Interestingly, queries complex axiom templates,
make sense require next axiom template evaluate shares variable
previously evaluated axiom templates, case simple axiom templates.
example, would require that, first connected component query would
executed following order:
?z ModifierAttribute
Bacterium ?z.?w
?w AbstractStatus
results 294,250 instead 1,498 consistency checks since longer use cheap
cache look-up check determine bindings ?w, first iterate possible
?w bindings check entailment complex axiom template reduce
computed candidates processing last axiom template.
Although optimizations significantly improve query execution time,
required time still quite high. practice, is, therefore, advisable add many
restrictive axiom templates (axiom templates require cache lookups) query
variables possible. example, addition ?y Shape Query 4 reduces
runtime 1.12 0.65 s. observe, expected, execution time
query applicable optimization analogous number consistency checks
performed evaluation query.
FBbt XP Queries: Queries 1, 2, 3, 5 6, ordering optimization
applicable, observe decrease execution time two orders magnitude
295

fiKollia & Glimm

Query

Reordering

1
1
1
2
2
2
3
3
3
4
4
5
5
5
5
6
6
6
6

x
x
x
x
x
x

Hierarchy
Exploitation

Rewriting

x
x

11,262
14,446

x
x

12,637
72,230

x
x

54,186
166,129
1335
166,129
21,669
907
3

x
x
x
x
x
x

x
x
x
x
x

Consistency
Checks
151,683

x
x
x

43,338
32,490

Time

AnswersNo

44.13
> 30 min
5.64
37.38
> 30 min
39.20
357.59
> 30 min
252.41
486.81
17.03
457.84
19.68
11.74
0.01
> 30 min
183.66
> 30 min
152.38

7,243
7,243
7,224
7,224
188
188
68
68
0
0
0
0
43,338
43,338

Table 11: Query answering times seconds queries Table 12 without
optimizations

1 ?x part of.?y
?x FBbt 00005789
2 ?y part
?x ?y.FBbt 00001606
3 ?x ?y.FBbt 00025990
?y overlaps

4 FBbt 00001606 ?y.?x
5 FBbt 00001606 ?y.?x
?y develops
6
?y FBbt 00001884
?p part
?x ?p.?y ?w

Table 12: Sample complex queries FBbt XP ontology
ordering optimization used. ordering optimization important answering
Queries 1, 2 3 within time limit. queries, additional use hierarchy
exploitation optimization leads improvement three orders magnitude.
observe queries effect hierarchy exploitation profound
others. precisely, smaller ratio result size number
consistency checks without hierarchy optimization, pronounced effect
enabling optimization. words, tested mappings indeed
solutions, one prune fewer parts hierarchy since pruning performed
find non-solution. Query 2, even observe slight increase running
296

fiOptimizing SPARQL Query Answering OWL Ontologies

time hierarchy optimization used. optimization
prune candidate mappings, outweigh overhead caused maintaining
information hierarchy parts already tested. Query 6, rewriting
optimization important answer query within time limit. optimizations
enabled, number consistency checks less result size (32,490 versus
43,338) since complex axiom template requires consistency checks.

8. Related Work
yet standardized commonly implemented query language OWL ontologies. Several widely deployed systems support, however, query language.
Pellet supports SPARQL-DL (Sirin & Parsia, 2007), subset SPARQL, adapted
work OWLs Direct Semantics. kinds SPARQL queries supported
SPARQL-DL directly mapped reasoner tasks. Therefore, SPARQLDL understood queries use simple axiom templates terminology.
Similarly, KAON2 (Hustadt, Motik, & Sattler, 2004) supports SPARQL queries, restricted ABox queries/conjunctive instance queries. best knowledge,
publications describe ordering strategies KAON2. Racer Pro (Haarslev
& Moller, 2001) proprietary query language, called nRQL (Haarslev et al., 2004),
allows queries go beyond ABox queries, e.g., one retrieve sub- superconcepts given concept. TrOWL (Thomas et al., 2013) another system supports
SPARQL queries, reasoning TrOWL approximate, i.e., OWL DL ontology
rewritten ontology uses less expressive language reasoning applied (Thomas, Pan, & Ren, 2010). TrOWL based SPARQL framework presented
here, instead using HermiT background reasoner, uses approximate reasoners OWL 2 EL OWL 2 QL profiles. Furthermore, systems
QuOnto (Acciarri, Calvanese, De Giacomo, Lembo, Lenzerini, Palmieri, & Rosati, 2013)
Requiem (Perez-Urbina, Motik, & Horrocks, 2013), support profiles OWL 2,
support conjunctive queries, e.g., written SPARQL syntax, proper nondistinguished variables. systems support OWL 2 DL, Pellet supports
non-distinguished variables long used cycles, since decidability cyclic
conjunctive queries best knowledge still open problem.
problem finding good orderings templates query issued ontology already preliminarily studied (Sirin & Parsia, 2006; Kremen & Sirin, 2008;
Haarslev & Moller, 2008). Similarly work, Sirin Parsia well Kremen
Sirin exploit reasoning techniques information provided reasoner models create
statistics cost result size axiom template evaluations within execution
plans. difference use cached models cheaply finding obvious concept
role (non-)instances, whereas case cache model model parts.
Instead process pre-model constructed initial ontology consistency check
extract known possible instances concepts roles it. subsequently
use information create update query atom statistics. Moreover, Sirin
Parsia Kremen Sirin compare costs complete execution plans heuristically reducing huge number possible complete plans choose one
promising beginning query execution. different cheap
297

fiKollia & Glimm

greedy algorithm finds, iteration, next promising axiom template.
experimental study shows equally effective investigation possible execution orders. Moreover, work additionally used dynamic ordering
combined clustering techniques, apart static ones, shown
techniques lead better performance particularly ontologies contain disjunctions
allow purely deterministic reasoning.
Haarslev Moller discuss means example ordering criteria use
find efficient query execution plans Racer Pro. particular, use traditional
database cost based optimization techniques, means take account
cardinality concept role atoms decide promising ordering.
previously discussed, inadequate especially ontologies disjunctive
information.
significant amount work estimation cost metrics search optimal
orders evaluating joins performed context databases. discussed
Section 3, databases, cost formulas defined estimate CPU I/O costs
(similar reasoning costs) number returned tuples (similar result
sizes). estimates used find good join orders. System R query optimizer,
example, among first works use extended statistics novel dynamic programming algorithm find effective (minimal) join orders query atoms (Selinger, Astrahan,
Chamberlin, Lorie, & Price, 1979). heuristic similar (for case conjunctive
instance queries) used work, according join order permutations
reduced avoiding Cartesian products result sets query atoms. Regarding join order selection, apart dynamic programming, also algorithmic paradigms based
branch-and-bound simulated annealing have, since then, presented literature.
Dynamic ordering also explored literature context adaptive query
processing techniques (Gounaris, Paton, Fernandes, & Sakellariou, 2002),
proposed overcome problems caused lack necessary statistics, good selectivity estimates, knowledge runtime mappings query compile time.
techniques take account changes happen evaluation environment runtime
modify execution plan runtime (i.e., change used operators joins
order (remaining) query atoms evaluated).

9. Conclusions
current paper, presented sound complete query answering algorithm
novel optimizations OWL Direct Semantics entailment regime SPARQL 1.1.
prototypical query answering system combines existing tools ARQ, OWL API,
HermiT OWL reasoner. Apart query ordering optimizationwhich uses
(reasoner dependent) statistics provided HermiTthe system independent reasoner used, could employ reasoner supports OWL API.
propose two cost-based ordering strategies finding (near-)optimal execution orders conjunctive instance queries. cost formulas based information extracted
models reasoner (in case HermiT). show experimental study
static techniques quite adequate ontologies reasoning deterministic.
reasoning nondeterministic, however, dynamic techniques often perform better.
298

fiOptimizing SPARQL Query Answering OWL Ontologies

use cluster based sampling techniques improve performance dynamic
algorithm intermediate result sizes queries sufficiently large, whereas random sampling beneficial often leads suboptimal query execution plans.
presented approach used find answers queries issued SROIQ
ontologies. Since based entailment checking finding answers conjunctive
instance queries scalable techniques, query rewriting,
applied ontologies lower expressivity, DL-Lite. words,
trade-off scalability ontology expressivity one needs consider
important ones application use scalable query answering system
less expressive ontology less scalable system expressive ontology.
module ordering based extraction statistics reasoner model,
computed off-line. update ontology ABox would cause construction new model scratch consequent recompilation known
possible instances concepts roles unless incremental reasoner used. incremental reasoner could, example, find modules pre-model affected
update recompute model parts. One could also incrementally update
statistics used ordering. best knowledge, OWL DL reasoners
partially support incremental reasoning considered case current
paper.
queries go beyond conjunctive instance queries provide optimizations rewriting equivalent, simpler queries. Another highly effective
frequently applicable optimization prunes number candidate solutions
checked exploiting concept role hierarchies. One can, usually, assume
hierarchies computed system accepts queries. empirical evaluation
shows optimization reduce query evaluation times three orders
magnitude.

Acknowledgments
work done within Transregional Collaborative Research Centre SFB/TRR
62 Companion-Technology Cognitive Technical Systems funded German
Research Foundation (DFG).

References
Acciarri, A., Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Palmieri, M., &
Rosati, R. (2013). QuOnto. Available http://www.dis.uniroma1.it/quonto/.
Beckett, D., Berners-Lee, T., Prudhommeaux, E., & Carothers, G. (Eds.). (19 February
2013). Turtle Terse RDF Triple Language. W3C Candidate Recommendation.
Available http://www.w3.org/TR/turtle/.
Brickley, D., & Guha, R. V. (Eds.). (10 February 2004). RDF Vocabulary Description
Language 1.0: RDF Schema. W3C Recommendation. Available http://www.w3.
org/TR/rdf-schema/.
299

fiKollia & Glimm

Broekstra, J., & Kampman, A. (2006). RDF query transformation language.
Staab, S., & Stuckenschmidt, H. (Eds.), Semantic Web Peer-to-Peer, pp. 2339.
Springer.
Calvanese, D., Giacomo, G. D., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning efficient query answering description logics: DL-Lite family.
Journal Automated Reasoning, 39 (3), 385429.
Clark & Parsia (2013a). Pellet. Available http://clarkparsia.com/pellet/.
Clark & Parsia (2013b). Stardog. Available http://stardog.com.
Glimm, B., Horrocks, I., Motik, B., Shearer, R., & Stoilos, G. (2012). novel approach
ontology classification. Journal Web Semantics: Science, Services Agents
World Wide Web, 14, 84101. Special Issue Dealing Messiness
Web Data.
Glimm, B., & Kollia, I. (2013). OWL-BGP. Available http://code.google.com/p/owl-bgp/.
Glimm, B., & Krotzsch, M. (2010). SPARQL beyond subgraph matching. Proceedings
9th International Semantic Web Conference (ISWC10), Vol. 6414 Lecture
Notes Computer Science, pp. 241256. Springer.
Glimm, B., & Ogbuji, C. (2013). SPARQL 1.1 entailment regimes. W3C Recommendation.
Available http://www.w3.org/TR/sparql11-entailment/.
Gounaris, A., Paton, N. W., Fernandes, A. A., & Sakellariou, R. (2002). Adaptive query
processing: survey. 19th BNCOD, pp. 1125. Springer.
Guo, Y., Pan, Z., & Heflin, J. (2005). LUBM: benchmark OWL knowledge base
systems. Journal Web Semantics, 3 (2-3), 158182.
Haarslev, V., & Moller, R. (2001). Racer system description. Gor, R., Leitsch, A., & Nipkow, T. (Eds.), Proceedings 1st International Joint Conference Automated
Reasoning (IJCAR01), Vol. 2083 LNCS, pp. 701705. Springer.
Haarslev, V., & Moller, R. (2008). scalability description logic instance retrieval.
Journal Automated Reasoning, 41 (2), 99142.
Haarslev, V., Moller, R., & Wessel, M. (2004). Querying semantic web Racer
+ nRQL. Proceedings KI-2004 International Workshop Applications
Description Logics.
Harris, S., & Seaborne, A. (Eds.). (2013). SPARQL 1.1 Query Language. W3C Recommendation. Available http://www.w3.org/TR/sparql11-query/.
Hayes, P. (Ed.). (10 February 2004). RDF Semantics. W3C Recommendation. Available
http://www.w3.org/TR/rdf-mt/.
Hitzler, P., Krotzsch, M., & Rudolph, S. (2009). Foundations Semantic Web Technologies.
Chapman & Hall/CRC.
Horridge, M., & Bechhofer, S. (2009). OWL API: Java API working OWL
2 ontologies. Patel-Schneider, P. F., & Hoekstra, R. (Eds.), Proceedings
OWLED 2009 Workshop OWL: Experiences Directions, Vol. 529 CEUR
Workshop Proceedings. CEUR-WS.org.
300

fiOptimizing SPARQL Query Answering OWL Ontologies

Horrocks, I., Kutz, O., & Sattler, U. (2006). even irresistible SROIQ. Doherty, P., Mylopoulos, J., & Welty, C. A. (Eds.), Proceedings 10th International
Conference Principles Knowledge Representation Reasoning (KR06), pp.
5767. AAAI Press.
Hustadt, U., Motik, B., & Sattler, U. (2004). Reducing SHIQ description logic disjunctive datalog programs. Proceedings 9th International Conference
Principles Knowledge Representation Reasoning (KR04), pp. 152162. AAAI
Press.
Ioannidis, Y. E., & Christodoulakis, S. (1993). Optimal histograms limiting worst-case
error propagation size join results. ACM Transactions Database Systems,
18 (4), 709748.
Kazakov, Y. (2008). RIQ SROIQ harder SHOIQ. Brewka, G., & Lang, J.
(Eds.), Proceedings 11th International Conference Principles Knowledge
Representation Reasoning (KR08), pp. 274284. AAAI Press.
Kollia, I., & Glimm, B. (2013). Evaluation sources. Available http://code.google.com/
p/query-ordering/.
Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2010). combined approach query answering DL-Lite. Lin, F., & Sattler, U. (Eds.),
Proceedings 12th International Conference Principles Knowledge Representation Reasoning (KR10). AAAI Press.
Kremen, P., & Sirin, E. (2008). SPARQL-DL implementation experience. Clark, K.,
& Patel-Schneider, P. F. (Eds.), Proceedings 4th OWLED Workshop OWL:
Experiences Directions Washington, Vol. 496 CEUR Workshop Proceedings.
CEUR-WS.org.
Ma, L., Yang, Y., Qiu, Z., Xie, G., Pan, Y., & Liu, S. (2006). Towards complete OWL
ontology benchmark. Semantic Web: Research Applications, Lecture Notes
Computer Science, chap. 12, pp. 125139. Springer.
Motik, B., Cuenca Grau, B., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (Eds.). (11
December 2012a). OWL 2 Web Ontology Language: Profiles. W3C Recommendation.
Available http://www.w3.org/TR/owl2-profiles/.
Motik, B., Patel-Schneider, P. F., & Cuenca Grau, B. (Eds.). (11 December 2012b). OWL 2
Web Ontology Language: Direct Semantics. W3C Recommendation. Available
http://www.w3.org/TR/owl2-direct-semantics/.
Motik, B., Shearer, R., Glimm, B., Stoilos, G., & Horrocks, I. (2013). HermiT. Available
http://www.hermit-reasoner.com/.
Motik, B., Shearer, R., & Horrocks, I. (2009). Hypertableau reasoning description logics.
Journal Artificial Intelligence Research, 36, 165228.
OBO Foundry (2013). open biological biomedical ontologies. Available http:
//www.obofoundry.org/.
Oracle (2013). Oracle database documentation library 11g release 2. Available http:
//www.oracle.com/pls/db112/homepage.
301

fiKollia & Glimm

Pan, J. Z., Thomas, E., & Zhao, Y. (2009). Completeness guaranteed approximation owl
dl query answering. Proceedings 2009 International Workshop Description
Logics (DL09).
Patel-Schneider, P. F., & Motik, B. (Eds.). (11 December 2012). OWL 2 Web Ontology
Language: Mapping RDF Graphs. W3C Recommendation. Available http:
//www.w3.org/TR/owl2-mapping-to-rdf/.
Perez-Urbina, H., Motik, B., & Horrocks, I. (2010). Tractable query answering rewriting
description logic constraints. Journal Applied Logic, 8 (2), 186209.
Perez-Urbina, H., Motik, B., & Horrocks, I. (2013). Requiem. Available http://www.
comlab.ox.ac.uk/projects/requiem/home.html.
Prudhommeaux, E., & Seaborne, A. (Eds.). (15 January 2008). SPARQL Query Language RDF. W3C Recommendation. Available http://www.w3.org/TR/
rdf-sparql-query/.
Racer Systems GmbH & Co. KG (2013).
racer-systems.com.

RacerPro 2.0.

Available http://www.

Ren, Y., Pan, J. Z., & Zhao, Y. (2010). Soundness preserving approximation tbox
reasoning. Proceedings 25th National Conference Artificial Intelligence
(AAAI10). AAAI Press.
Rodriguez-Muro, M., & Calvanese, D. (2012). High performance query answering
dl-lite ontologies. Proc. 13th Int. Conf. Principles Knowledge
Representation Reasoning (KR 2012), pp. 308318.
Schneider, M. (Ed.). (11 December 2012). OWL 2 Web Ontology Language: RDFBased Semantics. W3C Recommendation. Available http://www.w3.org/TR/
owl2-rdf-based-semantics/.
Seaborne, A. (9 January 2004). RDQL Query Language RDF. W3C Member
Submission. Available http://www.w3.org/Submission/RDQL/.
Selinger, P. G., Astrahan, M. M., Chamberlin, D. D., Lorie, R. A., & Price, T. G. (1979).
Access path selection relational database management system. Bernstein,
P. A. (Ed.), Proceedings 1979 ACM SIGMOD International Conference
Management Data, Boston, Massachusetts, May 30 - June 1, pp. 2334. ACM.
Sirin, E., Cuenca Grau, B., & Parsia, B. (2006). wine water: Optimizing description logic reasoning nominals. Doherty, P., Mylopoulos, J., & Welty, C. A.
(Eds.), Proceedings 10th International Conference Principles Knowledge
Representation Reasoning (KR06), pp. 9099. AAAI Press.
Sirin, E., & Parsia, B. (2006). Optimizations answering conjunctive ABox queries: First
results. Proceedings 2006 International Workshop Description Logics
(DL06), Vol. 189 CEUR Workshop Proceedings. CEUR-WS.org.
Sirin, E., & Parsia, B. (2007). SPARQL-DL: SPARQL query OWL-DL. Golbreich,
C., Kalyanpur, A., & Parsia, B. (Eds.), Proceedings OWLED 2007 Workshop
OWL: Experiences Directions, Vol. 258 CEUR Workshop Proceedings. CEURWS.org.
302

fiOptimizing SPARQL Query Answering OWL Ontologies

Sirin, E., Parsia, B., Grau, B. C., Kalyanpur, A., & Katz, Y. (2007). Pellet: practical
OWL-DL reasoner. Journal Web Semantics, 5 (2), 5153.
Steinbrunn, M., Moerkotte, G., & Kemper, A. (1997). Heuristic randomized optimization join ordering problem. VLDB Journal, 6, 191208.
Stocker, M., Seaborne, A., Bernstein, A., Kiefer, C., & Reynolds, D. (2008). SPARQL basic
graph pattern optimization using selectivity estimation. Proceedings 17th
International Conference World Wide Web (WWW08), pp. 595604. ACM.
Apache Software Foundation (2013). Apache jena. Available http://jena.apache.org.
Thomas, E., Pan, J. Z., & Ren, Y. (2010). TrOWL: Tractable OWL 2 reasoning infrastructure. Proceedings Extended Semantic Web Conference (ESWC10).
Thomas, E., Pan, J. Z., & Ren, Y. (2013). TrOWL. Available http://trowl.eu.
Tsarkov, D., Horrocks, I., & Patel-Schneider, P. F. (2007). Optimizing terminological reasoning expressive description logics. Journal Automated Reasoning, 39 (3), 277316.

303

fiJournal Artificial Intelligence Research 48 (2013) 583-634

Submitted 4/13; published 11/13

Protecting Moving Targets Multiple Mobile Resources
Fei Fang
Albert Xin Jiang
Milind Tambe

feifang@usc.edu
jiangx@usc.edu
tambe@usc.edu

University Southern California
Los Angeles, CA 90089 USA

Abstract
recent years, Stackelberg Security Games successfully applied solve resource allocation scheduling problems several security domains. However, previous
work mostly assumed targets stationary relative defender
attacker, leading discrete game models finite numbers pure strategies. paper
contrast focuses protecting mobile targets leads continuous set strategies
players. problem motivated several real-world domains including protecting ferries escort boats protecting refugee supply lines. contributions include:
(i) new game model multiple mobile defender resources moving targets
discretized strategy space defender continuous strategy space attacker.
(ii) efficient linear-programming-based solution uses compact representation
defenders mixed strategy, accurately modeling attackers continuous strategy using novel sub-interval analysis method. (iii) Discussion analysis multiple
heuristic methods equilibrium refinement improve robustness defenders mixed
strategy. (iv) Discussion approaches sample actual defender schedules defenders mixed strategy. (iv) Detailed experimental analysis algorithms ferry
protection domain.

1. Introduction
last years, game-theoretic decision support systems successfully deployed several domains assist security agencies (defenders) protecting critical infrastructure ports, airports air-transportation infrastructure (Tambe, 2011; Gatti,
2008; Marecki, Tesauro, & Segal, 2012; Jakob, Vanek, & Pechoucek, 2011). decision support systems assist defenders allocating scheduling limited resources
protect targets adversaries. particular, given limited security resources
possible cover secure target times; simultaneously, attacker observe defenders daily schedules, deterministic schedule defender
exploited attacker (Paruchuri, Tambe, Ordonez, & Kraus, 2006; Kiekintveld,
Islam, & Kreinovich, 2013; Vorobeychik & Singh, 2012; Conitzer & Sandholm, 2006).
One game-theoretic model deployed schedule security resources
domains Stackelberg game leader (the defender) follower (the
attacker). model, leader commits mixed strategy, randomized
schedule specified probability distribution deterministic schedules; follower
observes distribution plays best response (Korzhyk, Conitzer, & Parr, 2010).
Decision-support systems based model successfully deployed, including
ARMOR LAX airport (Pita, Jain, Marecki, Ordonez, Portway, Tambe, Western,
c
2013
AI Access Foundation. rights reserved.

fiFang, Jiang, & Tambe

Paruchuri, & Kraus, 2008), IRIS US Federal Air Marshals service (Tsai, Rathi,
Kiekintveld, Ordonez, & Tambe, 2009), PROTECT US Coast Guard (Shieh,
An, Yang, Tambe, Baldwin, DiRenzo, Maule, & Meyer, 2012).
previous work game-theoretic models security assumed either stationary
targets airport terminals (Pita et al., 2008), targets stationary relative
defender attacker, e.g., trains (Yin, Jiang, Johnson, Kiekintveld, Leyton-Brown,
Sandholm, Tambe, & Sullivan, 2012) planes (Tsai et al., 2009), players
move along targets protect attack them). stationary nature leads
discrete game models finite numbers pure strategies. paper focus
security domains defender needs protect mobile set targets.
attacker attack targets point time movement, leading
continuous set strategies. defender deploy set mobile escort resources (called
patrollers short) protect targets. assume game zero-sum, allow
values targets vary depending locations time. defenders
objective schedule mobile escort resources minimize attackers expected utility.
call problem Multiple mobile Resources protecting Moving Targets (MRMT).
first contribution paper novel game model MRMT called MRMTsg .
MRMTsg attacker-defender Stackelberg game model continuous set strategies attacker. contrast, defenders strategy space also continuous,
discretize MRMTsg three reasons. Firstly, let defenders strategy space
continuous, space mixed strategies defender would infinite
dimensions, makes exact computation infeasible. Secondly, practice, patrollers
able fine-grained control vehicles, makes actual
defenders strategy space effectively discrete one. Finally, discretized defender strategy space subset original continuous defender strategy space, optimal
solution calculated formulation feasible solution original game
gives lower-bound guarantee defender terms expected utility original continuous game. hand, discretizing attackers strategy space
highly problematic illustrate later paper. particular, deploy
randomized schedule defender assumption attacker could
attack certain discretized time points, actual attacker could attack
time point, leading possibly worse outcome defender.
second contribution CASS (Solver Continuous Attacker Strategies), efficient
linear program exactly solve MRMTsg . Despite discretization, defender strategy
space still exponential number pure strategies. overcome shortcoming
compactly representing defenders mixed strategies marginal probability variables.
attacker side, CASS exactly efficiently models attackers continuous strategy
space using sub-interval analysis, based observation given defenders
mixed strategy, attackers expected utility piecewise-linear function. Along way
presenting CASS, present DASS (Solver Discretized Attacker Strategies),
finds minimax solutions MRMTsg games constraining attacker attack
discretized time points. clarity exposition first derive DASS CASS
case targets move one-dimensional line segment. later show DASS
CASS extended case targets move two-dimensional space.
584

fiProtecting Moving Targets Multiple Mobile Resources

third contribution focused equilibrium refinement. game multiple
equilibria, defender strategy found CASS suboptimal respect uncertainties attackers model, e.g., attacker attack certain time
intervals. present two heuristic equilibrium refinement approaches game.
first, route-adjust, iteratively computes defender strategy dominates earlier strategies. second, flow-adjust, linear-programming-based approach. experiments
show flow-adjust computationally faster route-adjust route-adjust
effective selecting robust equilibrium strategies.
Additionally, provide several sampling methods generating practical patrol routes
given defender strategy compact representation. Finally present detailed experimental analyses algorithm ferry protection domain. CASS deployed
US Coast Guard since April 2013.
rest article organized follows: Section 2 provides problem statement.
Section 3 presents MRMTsg model initial formulation DASS CASS
one-dimensional setting. Section 4 discusses equilibrium refinement, followed Section
5 gives generalized formulation DASS CASS two-dimensional settings.
Section 6 describes sample patrol route Section 7 provides experimental
results ferry protection domain. Section 8 discusses related work, followed Section
9, provides concluding remarks, Section 10, discusses future work.
end article, Appendix provides table listing notations used article,
Appendix B provides detailed calculation finding intersection points
2-D case.

2. Problem Statement
One major example practical domains motivating paper problem protecting ferries carry passengers many waterside cities. Packed hundreds
passengers, may present attractive targets attacker. example, attacker
may ram suicide boat packed explosives ferry happened attacks
French supertanker Limburg USS Cole (Greenberg, Chalk, & Willis, 2006).
case, intention attacker detected gets close ferry.
Small, fast well-armed patrol boats (patrollers) provide protection ferries
(Figure 1(a)), detecting attacker stopping armed weapons. However, often limited numbers patrol boats, i.e., cannot protect ferries
times locations. first focus case ferries patrol boats move
one-dimensional line segment (this realistic setting also simplifies exposition);
discuss two-dimensional case Section 5.
2.1 Domain Description
problem, L moving targets, F1 , F2 , ..., FL . assume targets
move along one-dimensional domain, specifically straight line segment linking two terminal points name B. sufficient capture real-world domains
ferries moving back-and-forth straight line two terminals
many ports around world; example green line shown Figure 1(b).
provide illustration geometric formulation problem Figure 2.1.
585

fiFang, Jiang, & Tambe

(a)

(b)

Figure 1: (a) Protecting ferries patrol boats; (b) Part map New York Harbor Commuter Ferry Routes. straight line linking St. George Terminal
Whitehall Terminal indicates public ferry route run New York City Department Transportation.

targets fixed daily schedules. schedule target described continuous function Sq : q = 1, ..., L index target, = [0, 1]
continuous time interval (e.g., representing duration typical daily patrol shift)
= [0, 1] continuous space possible locations (normalized) 0 corresponding
terminal 1 terminal B. Thus Sq (t) denotes position target Fq
specified time t. assume Sq piecewise linear.
defender W mobile patrollers move along protect targets,
denoted P1 , P2 , ..., PW . Although capable moving faster targets,
maximum speed vm . defender attempts protect targets, attacker
choose certain time certain target attack. (In rest paper, denote
defender attacker he). probability attack success depends
positions patrollers time. Specifically, patroller detect try
intercept anything within protection radius cannot detect attacker prior
radius. Thus, patroller protects targets within protective circle radius
(centered current position), shown Figure 2.1.
















Figure 2: example three targets (triangles) two patrollers (squares).
protective circles patrollers shown protection radius . patroller
protects targets protective circle. Patroller P1 protecting F2 P2
protecting F3 .

586

fiProtecting Moving Targets Multiple Mobile Resources

Symmetrically, target protected patrollers whose protective circles cover
it. attacker attacks protected target, probability successful attack
decreasing function number patrollers protecting target. Formally,
use set coefficients {CG } describe strength protection.
Definition 1. Let G {1, ..., W } total number patrollers protecting target Fq ,
i.e., G patrollers Fq within radius G patrollers.
CG [0, 1] specifies probability patrollers successfully stop attacker.
require CG1 CG2 G1 G2 , i.e., patrollers offer better protection.
previous work security games (Tambe, 2011; Yin et al., 2012; Kiekintveld,
Jain, Tsai, Pita, Ordonez, & Tambe, 2009), model game Stackelberg game,
defender commits randomized strategy first, attacker respond
strategy. patrol schedules domains previously created hand;
hence suffer drawbacks hand-drawn patrols, including lack randomness (in
particular, informed randomness) reliance simple patrol patterns (Tambe, 2011),
remedy paper.
2.2 Defender Strategy
pure strategy defender designate movement schedule patroller.
Analogous targets schedule, patrollers schedule written continuous
function Ru : u = 1, ..., W index patroller. Ru must compatible
patrollers velocity range. mixed defender strategy randomization
pure strategies, denoted f .
2.3 Attacker Strategy
attacker conducts surveillance defenders mixed strategy targets schedules; may execute pure strategy response attack certain target certain
time. attackers pure strategy denoted hq, ti q index target
attack time attack.
2.4 Utility Function
assume game zero-sum. attacker performs successful attack target Fq
location x time t, gets positive reward Uq (x, t) defender gets Uq (x, t),
otherwise players get utility zero. positive reward Uq (x, t) known function
accounts many factors practice. example, attacker may effective
attack target stationary (such terminal point) target
motion. targets position decided schedule, utility function
written Uq (t) Uq (Sq (t), t). assume target Fq , Uq (t) represented
piecewise linear function t.
2.5 Equilibrium
Since game zero-sum, Strong Stackelberg Equilibrium calculated finding
minimax/maximin strategy (Fudenberg & Tirole, 1991; Korzhyk et al., 2010). is,
587

fiFang, Jiang, & Tambe

find optimal defender strategy finding strategy minimizes maximum
attackers expected utility.
Definition 2. single patroller case, attacker expected utility attacking target Fq
time given defender mixed strategy f
AttEUf (Fq , t) = (1 C1 f (Fq , t))Uq (t)

(1)

Uq (t) reward successful attack, f (Fq , t) probability patroller
protecting target Fq time C1 protection coefficient single patroller.
drop subscript f obvious context. C1 Uq (t) constants given
attackers pure strategy hq, ti, AttEU(Fq , t) purely decided (Fq , t). definition
multiple patrollers given Section 3.4. denote attackers
maximum expected utility
AttEUm
f = max AttEUf (Fq , t)
q,t

(2)

optimal defender strategy strategy f AttEUm
f minimized, formally
f arg minf 0 AttEUm
f0

(3)

2.6 Assumptions
problem, following assumptions made based discussions domain
experts. provide justifications assumptions. appropriate
current domain application, relaxing assumptions future applications remains
issue future work; provide initial discussion Section 10.
attackers plan decided off-line, i.e., attacker take account
patrollers current partial route (partial pure strategy) executing attack:
assumption similar assumption made applications security games
justified elsewhere (An, Kempe, Kiekintveld, Shieh, Singh, Tambe, & Vorobeychik, 2012; Pita, Jain, Ordonez, Portway, Tambe, Western, Paruchuri, & Kraus,
2009; Tambe, 2011). One key consideration given attackers limited
resources well, generate execute complex conditional plans
change based on-line observations defenders pure strategy difficult
risky.
single attacker assumed instead multiple attackers: assumption arises
performing even single attack already costly attacker. Thus,
coordinating attackers time even harder therefore
significantly less likely attacker.
game assumed zero-sum: case, objectives defender
attacker direct conflict: preventing attack higher potential damage
bigger success defender game.
588

fiProtecting Moving Targets Multiple Mobile Resources

schedules targets deterministic: domains focus on, potential
delays targets schedules usually within several minutes any,
targets try catch fixed schedules soon possible. Therefore,
even delays occur, deterministic schedule target viewed
good approximation actual schedule.

3. Models
section, introduce MRMTsg model uses discretized strategy space
defender continuous strategy space attacker. clarity exposition,
introduce DASS approach compute minimax solution discretized attacker
strategy space (Section 3.2), followed CASS attackers continuous strategy space
(Section 3.3). first assume single patroller Sections 3.1 3.3
generalize multiple patrollers Section 3.4.
3.1 Representing Defenders Strategies
subsection, introduce discretized defender strategy space compact
representation used represent defenders mixed strategy. show compact
representation equivalent intuitive full representation, followed several properties
compact representation.
Since defenders strategy space discretized, assume patroller
makes changes finite set time points = {t1 , t2 , ..., tM }, evenly spaced across
original continuous time interval. t1 = 0 starting time tM = 1 normalized
ending time. denote distance two adjacent time points: = tk+1
tk = M11 . set small enough target Fq , schedule Sq (t)
utility function Uq (t) linear interval [tk , tk+1 ] k = 1, . . . , 1, i.e.,
target moving uniform speed utility successful attack changes
linearly intervals. Thus, t0 breakpoint Sq (t) Uq (t)
q, represented t0 = tK0 K0 integer.
addition discretization time, also discretize line segment AB
targets move along set points = {d1 , d2 , ..., dN } restrict patroller
located one discretized points di discretized time point tk . Note
necessarily evenly distributed targets locations restricted tk .
time interval [tk , tk+1 ], patroller moves constant speed location di
time tk location dj time tk+1 . movements compatible speed limit
vm possible. points d1 , d2 , ..., dN ordered distance terminal A,
d1 refers dN refers B. Since time interval discretized points,
patrollers route Ru represented vector Ru = (dru (1) , dru (2) , ..., dru (M ) ). ru (k)
indicates index discretized distance point patroller located time
tk .
explained Section 1, discretized defenders strategy space
computational reasons. even clear whether equilibrium exists original
game continuous strategy space players. discretization made also
practical constraint patrollers.
589

fiFang, Jiang, & Tambe

expository purpose, first focus case single defender resource
generalize larger number resources later. single defender resource,
defenders mixed strategy full representation assigns probability patrol
routes executed. Since time step patroller choose go
N different locations, N possible patrol routes total number
achievable speed limit (or vm large enough). exponentially growing
number routes make analysis based full representation intractable.
Therefore, use compact representation defenders mixed strategy.
Definition 3. compact representation single defender resource compact
way represent defenders mixed strategy using flow distribution variables {f (i, j, k)}.
f (i, j, k) probability patroller moving di time tk dj time tk+1 .
complexity compact representation O(M N 2 ), much efficient
compared full representation.
Proposition 1. strategy full representation mapped compact representation.
Proof sketch: H possible patrol routes R1 , R2 , ..., RH , mixed defender
strategy represented full representation probability vector (p(R1 ), ...p(RH ))
p(Ru ) probability taking route Ru . Taking route Ru means patroller
moves dru (k) dru (k+1) time [tk , tk+1 ], edge ERu (k),Ru (k+1),k taken
route Ru chosen. total probability taking edge Ei,j,k sum
probabilities routes Ru Ru (k) = Ru (k + 1) = j. Therefore, given
strategy full presentation specified probability vector (p(R1 ), ...p(RH )),
construct compact representation consisting set flow distribution variables
{f (i, j, k)}
X
f (i, j, k) =
p(Ru ).
(4)
Ru :Ru (k)=i Ru (k+1)=j

Figure 3 shows simple example illustrating compact representation. Numbers
edges indicate value f (i, j, k). use Ei,j,k denote directed edge linking
nodes (tk , di ) (tk+1 , dj ). example, f (2, 1, 1), probability patroller moving
d2 d1 time t1 t2 , shown edge E2,1,1 node (t1 , d2 ) node
(t2 , d1 ). similar compact representation used earlier Yin et al. (2012),
use continuous setting.
Note different mixed strategies full representation mapped
compact representation. Table 1 shows two different mixed defender strategies full representations mapped mixed strategy compact representation
shown Figure 3. probability route labeled edges route full
representation. Adding numbers particular edge Ei,j,k routes full
representation together, get f (i, j, k) compact representation.
Theorem 1. Compact representation lead loss solution quality.
590

fi

Protecting Moving Targets Multiple Mobile Resources













Figure 3: Compact representation: x-axis shows time intervals; y-axis discretized
distance-points one-dimensional movement space.

R1 = (d1 , d1 , d1 )




R1 = (d1 , d1 , d1 )




Full Representation 1
R2 = (d1 , d1 , d2 )
R3 = (d2 , d1 , d1 )


R4 = (d2 , d1 , d2 )



Full Representation 2
R2 = (d1 , d1 , d2 )
R3 = (d2 , d1 , d1 )

R4 = (d2 , d1 , d2 )





Table 1: Two full representations mapped compact representation
shown Figure 3.

Proof sketch: complete proof theorem relies calculations Section
3.2 3.3. provide sketch. Recall goal find optimal defender
strategy f minimizes maximum attacker expected utility AttEUm
f .
show next subsections, (Fq , t) calculated compact representation
{f (i, j, k)}. two defender strategies full representation mapped
compact representation {f (i, j, k)}, function
AttEU function according Equation 1. Thus value AttEUm
f
two defender strategies. optimal mixed defender strategy compact representation
still optimal corresponding defender strategies full representation.
exploit following properties compact representation.
Property 1.
ForPany time interval [tk , tk+1 ], sum flow distribution variables
PN
N
equals 1:
i=1
j=1 f (i, j, k) = 1.
Property 2. sum flows go particular node equals sum
P flows
go node. Denote sum node (tk , di ) p(i, k), p(i, k) = N
j=1 f (j, i, k
PN
1) = j=1 f (i, j, k). p(i, k) equal marginal probability patroller
location di time tk .
P
Property 3. Combining Property 1 2, N
i=1 p(i, k) = 1.
591

fiFang, Jiang, & Tambe

3.2 DASS: Discretized Attacker Strategies
subsection, introduce DASS, mathematical program efficiently finds minimax solutions MRMTsg -based games assumption attacker attack
one discretized time points tk . problem, need minimize v v
maximum attackers expected utility. Here, v maximum AttEU(Fq , t)
target Fq discretized time point tk .
Equation (1), know AttEU(Fq , t) decided (Fq , t), probability
patroller protecting target Fq time t. Given position target Sq (t),
define protection range q (t) = [max{Sq (t)re , d1 }, min{Sq (t)+re , dN }]. patroller
located within range q (t), distance target patroller
thus patroller protecting Fq time t. (Fq , t) probability
patroller located within range q (t) time t. discretized time points
tk , patroller located discretized distance point di , define
following.

Definition 4. I(i, q, k) function two values. I(i, q, k) = 1 di q (tk ),
otherwise I(i, q, k) = 0.

words, I(i, q, k) = 1 means patroller located di time tk protect target Fq . Note value I(i, q, k) calculated directly input
parameters (di , Sq (t) ) stored look-up table. particular, I(i, q, k)
variable formulations follow. simply encodes relationship di
location target Fq tk . probability patroller di time tk
p(i, k).
(Fq , tk ) =

X


AttEU(Fq , tk ) =

1 C1

i:I(i,q,k)=1

p(i, k),

X
i:I(i,q,k)=1


p(i, k) Uq (tk ).

(5)

(6)

Equation (6) follows Equations (1) (5), expressing attackers expected utility
discretized time points. Finally, must address speed restrictions patroller. set
flows corresponding actions achievable zero,1 is, f (i, j, k) = 0
|dj di | > vm t. Thus, DASS formulated linear program. linear program

1. Besides speed limit, also model practical restrictions domain placing constraints
f (i, j, k).

592

fiProtecting Moving Targets Multiple Mobile Resources

solves number targets one defender resource.
min

z

(7)

f (i,j,k),p(i,k)

f (i, j, k) [0, 1],

i, j, k

(8)

f (i, j, k) = 0,

i, j, k |dj di | > vm

(9)

p(i, k) =

N
X

f (j, i, k 1),

i, k > 1

(10)

f (i, j, k),

i, k <

(11)

k

(12)

q, k

(13)

j=1

p(i, k) =

N
X
j=1

N
X

p(i, k) = 1,

i=1

z AttEU(Fq , tk ),

Constraint 8 describes probability range. Constraint 9 describes speed limit. Constraints 1011 describes Property 2. Constraint 12 exactly Property 3. Property 1
derived Property 2 3, listed constraint. Constraint (13) shows
attacker chooses strategy gives maximal expected utility among
possible attacks discretized time points; AttEU() described Equation (6).
3.3 CASS: Continuous Attacker Strategies
subsection, generalize problem one continuous attacker strategy set
provides linear-programming-based solution CASS. CASS efficiently finds optimal
mixed defender strategy assumption attacker attack time
continuous time interval = [0, 1]. assumption, DASSs solution quality
guarantee may fail: attacker chooses attack tk tk+1 , may get
higher expected reward attacking tk tk+1 . Consider following example,
defenders compact strategy tk tk+1 shown Figure 4.
defenders strategy three non-zero flow variables f (3, 4, k) = 0.3, f (3, 1, k) = 0.2,
f (1, 3, k) = 0.5, indicated set three edges E + = {E3,4,k , E3,1,k , E1,3,k }.
target Fq moves d3 d2 constant speed [tk , tk+1 ]. schedule depicted
straight line segment Sq . dark lines L1q L2q parallel Sq distance
. area indicates protection range q (t) time (tk , tk+1 ).
Consider time points edge E + intersects one L1q , L2q , label
r , r = 1 . . . 4 Figure 4). Intuitively, time points defender
qk
patrol could potentially enter leave protection range target. simplify
0
5
notation, denote tk qk
k+1 qk . example, patroller moving d3
0 1
d4 (or equivalently, taking edge E3,4,k ) protects target qk
qk
0 , 1 ], distance target less
E3,4,k L11 L21 [qk
qk
r r+1 ,
equal protection radius . Consider sub-intervals qk
qk
r = 0 . . . 4. Since within five sub-intervals, patroller enters leaves
593

fiFang, Jiang, & Tambe

protection range, probability target protected constant
sub-interval, shown Figure 5(a).
























Figure 4: example show target moving d3 d2 [tk , tk+1 ] pror , r+1 ], patroller either always protects target
tected. sub-interval [qk
qk
never protects target. Equivalently, target either always within
protective circle patroller always outside circle.

Suppose Uq (t) decreases linearly 2 1 [tk , tk+1 ] C1 = 0.8.
calculate attackers expected utility function AttEU(Fq , t) (tk , tk+1 ), plotted
Figure 5(b). AttEU(Fq , t) linear sub-interval function discontinuous
1 , . . . , 4 patroller leaving entering protection
intersection points qk
qk
r left as:
range target. denote limit AttEU approaches qk
r
lim AttEU(Fq , t) = AttEU(Fq , qk
)

r
tqk

Similarly, right limit denoted as:
r+
lim AttEU(Fq , t) = AttEU(Fq , qk
)

r+
tqk

2 ,
Fq target, attacker choose attack time immediately qk
getting expected utility arbitrarily close 1.70. According Equation (6),
2+
get AttEU(Fq , tk ) = 1.20 AttEU(Fq , tk+1 ) = 1.00, lower AttEU(Fq , qk
).
Thus, attacker get higher expected reward attacking tk tk+1 ,
violating DASSs quality guarantee.
However, discontinuities attackers expected utility function, maximum might exist. implies minimax solution concept might welldefined game. thus define solution concept minimizing supremum
AttEU(Fq , t).

594

fiProtecting Moving Targets Multiple Mobile Resources





0.50

1.70
1.43
1.20
1.00

0.20
0.00
















(a) Probability target protected
constant sub-interval.











(b) attackers expected utility linear
sub-interval.

Figure 5: Sub-interval analysis (tk , tk+1 ) example shown Figure 4.]
Definition 5. supremum attackers expected utility smallest real number
greater equal elements infinite set {AttEU(Fq , t)}, denoted
sup AttEU(Fq , t).
supremum least upper bound function AttEU(Fq , t). CASS
model, Equation 2 modified
AttEUm
f = sup AttEUf (Fq , t)

(14)

q,t

defender strategy f minimax AttEUm
f maximized, i.e.,
f arg minf 0 sup AttEUf 0 (Fq , t)
2+
)=
example, supremum attackers expected utility (tk , tk+1 ) AttEU(Fq , qk
1.70. rest paper, specify supremum used instead maximum easily inferred context.
deal possible attacks discretized points find
optimal defender strategy? generalize process (called sub-interval analysis)
possible edges Ei,j,k . make use piecewise linearity AttEU(Fq , t)
fact potential discontinuity points fixed, allows us construct
linear program solves problem optimality. name approach CASS
(Solver Continuous Attacker Strategies).
first introduce general sub-interval analysis. target Fq time
interval (tk , tk+1 ), calculate time points edges Ei,j,k L1q , L2q intersect,
denoted intersection points. sort intersection points increasing order, denoted
r , r = 1 . . . ,
0
qk
qk
qk total number intersection points. Set qk = tk


qkqk

+1

r , r+1 ), r = 0, ..., .
= tk+1 . Thus (tk , tk+1 ) divided sub-intervals (qk
qk
qk

Lemma 1. given target Fq , AttEU(Fq , t) piecewise-linear t. Furthermore,
exists fixed set time points, independent defenders mixed strategy,
AttEU(Fq , t) linear adjacent pair points. Specifically, points
r defined above.
intersection points qk
595

fiFang, Jiang, & Tambe

r , r+1 ) target F , feasible edge E
Proof: sub-interval (qk
q
i,j,k either
qk
1
2
totally Lq , similarly Lq . Otherwise new intersection
point contradicts definition sub-intervals. edge Ei,j,k L1q
L2q , distance patroller taking edge target Fq less , meaning
target protected patroller. edge Ei,j,k taken probability f (i, j, k),
total probability target protected ((Fq , t)) sum f (i, j, k) whose
corresponding edge Ei,j,k two lines sub-interval. (Fq , t) constant
sub-interval thus attackers expected utility AttEU(Fq , t) linear
sub-interval according Equation 2 Uq (t) linear [tk , tk+1 ]. Discontinuity
exist intersection points upper bound number points
target Fq N 2 .
r , r+1 ), 0
Define coefficient Arqk (i, j) C1 edge Ei,j,k L1q L2q (qk
qk
otherwise. According Equation (1) fact (Fq , t) sum f (i, j, k) whose
r , r+1 ).
corresponding coefficient Arqk (i, j) = C1 , following equation (qk
qk


N X
N
X

(15)
AttEU(Fq , t) = 1
Arqk (i, j)f (i, j, k) Uq (t)
i=1 j=1

Piecewise linearity AttEU(Fq , t) means function monotonic sub-interval
supremum found intersection points. linearity, supremum
r , r+1 ) chosen one-sided limits endpoints,
AttEU (qk
qk
(r+1)

r+
) AttEU(Fq , qk
). Furthermore, Uq (t) decreasing [tk , tk+1 ],
AttEU(Fq , qk
supremum
(r+1)
r+
) otherwise AttEU(Fq , qk
). words, attackers
AttEU(Fq , qk
r+1
r r+1 . Thus, CASS
r
strategies (qk , qk ) dominated attacking time close qk
qk
adds new constraints Constraints 813 consider attacks occur (tk , tk+1 ).
add one constraint sub-interval respect possible supremum value
sub-interval:

min

z

(16)

f (i,j,k),p(i,k)

subject constraints (8 . . . 13)
(r+1)

r+
z max{AttEU(Fq , qk
), AttEU(Fq , qk

)}

(17)

k {1 . . . }, q {1 . . . L}, r {0 . . . Mqk }
linear program stands core CASS differentiate name
solver name linear program following. linear constraints included Constraint 17 added CASS using Algorithm 1. input
algorithm include targets schedules {Sq }, protection radius , speed limit
vm , set discretized time points {tk } set discretized distance points {di }.
Function CalInt(L1q , L2q , vm ) Line 1 returns list intersection time points
0
possible edges Ei,j,k parallel lines L1q , L2q , additional points tk qk


+1

r , r+1 ) Line 1 returns coefficient
tk+1 qkqk . Function CalCoef(L1q , L2q , vm , qk
qk
r
r
matrix Aqk . Aqk easily decided checking status midpoint time. Set

596

fiProtecting Moving Targets Multiple Mobile Resources

r + r+1 )/2 denote patrollers position
tmid = (qk
mid edge Ei,j,k taken
qk
Ei,j,tmid , thus Arqk (i, j) = C1 Ei,j,tmid q (tmid ). Lines 11 add constraint respect
(r+1)

r+
larger value AttEU(Fq , qk
) AttEU(Fq , qk
) CASS sub-interval
r+1
r
(qk , qk ). means attacker chooses attack Fq sub-interval, best
r , r+1 ).
choice decided larger value two side-limits AttEU (qk
qk

Algorithm 1: Add constraints described Constraint 17
Input: Sq , , vm , {tk }, {di };
k 1, . . . , 1
q 1, . . . , L
L1q Sq + , L2q Sq ;


+1

0 , . . . , qk
CalInt(L1q , L2q , vm );
qk
qk
r 0, . . . , Mqk
r , r+1 );
Arqk CalCoef(L1q , L2q , vm , qk
qk
Uq (t) decreasing [tk , tk+1 ]
r+
add constraint z AttEU(Fq , qk
)
end
else
(r+1)
add constraint z AttEU(Fq , qk
)
end
end
end
end

Theorem 2. CASS computes (in polynomial time) exact solution (minimax)
game discretized defender strategies continuous attacker strategies.
Proof: According Lemma 1, AttEU(Fq , t) piecewise linear discontinuity
r . intersection points divide time space
occur intersection points qk
sub-intervals. piecewise linearity, supremum AttEU(Fq , t) equals
limit endpoint least one sub-interval. defenders strategy f
feasible, feasible z linear program 16-17 less limit values
intersection points according Constraint 17 values discretized time points tk
according Constraint 13, thus v upper bound AttEU(Fq , t) f . z
minimized objective function, z greater supremum AttEU(Fq , t)
given defender strategy f , z minimum set supremum
corresponding defender strategies. Thus get optimal defender strategy f .
total number variables linear program O(M N 2 ). number constraints represented Algorithm 1 O(M N 2 L) number intersection points
2(M 1)N 2 target. number constraints represented Constraints
813 O(M N 2 ). Thus, linear program computes solution polynomial time.
Corollary 1. solution CASS provides feasible defender strategy original
continuous game gives exact expected value strategy.
597

fiFang, Jiang, & Tambe

3.4 Generalized Model Multiple Defender Resources
subsection, generalize DASS CASS solve problem multiple defender resources. multiple patrollers, patrollers coordinate
other. Recall protection coefficient CG Definition 1, target better protected
patrollers close (within radius ). protection provided
target determined patrollers locations known. Thus sufficient
calculate probability individual edge taken single patroller case.
presence multiple patrollers, need complex representation explicitly describe defender strategy. illustrate generalization multiple defender
resources case, first take two patrollers example. two patrollers,
patrol strategy represented using flow distribution variables {f (i1 , j1 , i2 , j2 , k)}.
flow distribution variables defined Cartesian product two duplicated sets
feasible edges {Ei,j,k }. f (i1 , j1 , i2 , j2 , k) joint probability first patroller
moving di1 dj1 second patroller moving di2 di2 time tk
tk+1 , i.e., taking edge Ei1 ,j1 ,k Ei2 ,j2 ,k respectively. corresponding marginal distribution variable p(i1 , i2 , k) represents probability first patroller di1
second di2 time tk . Protection coefficients C1 C2 used one two
patrollers protecting target respectively.
attackers expected utility written
AttEU(Fq , t) = (1 (C1 1 (Fq , t) + C2 2 (Fq , t))) Uq (t)
1 (Fq , t) probability one patroller protecting target Fq time
2 (Fq , t) probability patrollers protecting target. attacks
happen discretized points tk , make use I(i, q, k) Definition 4
I(i1 , q, k) + I(i2 , q, k) total number patrollers protecting ferry time tk .
X
1 (Fq , tk ) =
p(i1 , i2 , k)
i1 ,i2 :I(i1 ,q,k)+I(i2 ,q,k)=1
X
2 (Fq , tk ) =
p(i1 , i2 , k)
i1 ,i2 :I(i1 ,q,k)+I(i2 ,q,k)=2

Constraints attacks occurring (tk , tk+1 ) calculated algorithm
looks Algorithm 1. main difference coefficient matrix Arqk
expression AttEU. set values coefficient matrix Arqk (i1 , j1 , i2 , j2 ) C2
edges Ei1 ,j1 ,k Ei2 ,j2 ,k L1q L2q , C1 one edges
r , r+1 )
protects target. attackers expected utility function (qk
qk
X
AttEU(Fq , t) = (1
Arqk (i1 , j1 , i2 , j2 )f (i1 , j1 , i2 , j2 , k)) Uq (t)
i1 ,j1 ,i2 ,j2

general case W defender resources, use {f (i1 , j1 , ..., iW , jW , k)} represent patrol strategy.
Definition 6. compact representation multiple defender resources compact way
represent defenders mixed strategy using flow distribution variables {f (i1 , j1 , ..., iW , jW , k)}.
{f (i1 , j1 , ..., iW , jW , k)} joint probability patroller moving diu time tk
dju time tk+1 u = 1 . . . W .
598

fiProtecting Moving Targets Multiple Mobile Resources

Given generalized compact representation, get following equations calculating attackers expected utility function protection probability:


W
X
AttEU(Fq , t) = 1
CQ Q (Fq , t) Uq (t)
Q=1

Q (Fq , tk ) =

X
i1 ,...,iW :

W
P

p(i1 , . . . , iW , k)
I(iu ,q,k)=Q

u=1

Q number patrollers protecting target. modify Algorithm 1 apply
multiple defender resource case. Set Arqk (i1 , j1 , ..., iW , jW ) CQ Q edges
{Eiu ,ju ,k } L1q L2q .
conclude linear program generalized CASS multiple patrollers follows.
min

z

(18)

f (i1 ,j1 ,...,iW ,jW ,k),p(i1 ,...,iW ,k)

f (i1 , j1 , . . . , iW , jW , k) = 0, i1 , . . . , iW , j1 , . . . , jW u, |dju diu | > vm
(19)
n
n
X
X
p(i1 , . . . , iW , k) =
...
f (j1 , i1 , . . . , jW , iW , k 1), i1 , . . . , iW , k > 1
j1 =1

jW =1

n
X

n
X

(20)
p(i1 , . . . , iW , k) =

...

j1 =1

f (i1 , j1 , . . . , iW , jW , k), i1 , . . . , iW , k <

jW =1

(21)
n
X
i1 =1

...

n
X

p(i1 , . . . , iW , k) = 1, k

(22)

iW =1

z AttEU(Fq , tk ), q, k
z

(r+1)
r+
max{AttEU(Fq , qk
), AttEU(Fq , qk
)}, k, q, r

(23)
(24)

number variables linear program O(M N 2W ) number constraints O(M N W ). reasonable examine potentially efficient alternatives.
summarize results examination concluding using current
linear program would appear currently offer best tradeoff terms solution quality time least current domains application; although discussed below,
significant future work might reveal alternatives approaches future domains.
first question examine computational complexity problem
hand: generating optimal patrolling strategies multiple patrollers graph. Unfortunately, despite significant attention paid topic, currently, complexity remains
unknown. specifically, question computational complexity generating patrols
multiple defenders graphs different types received significant attention (Letchford, 2013; Korzhyk et al., 2010). studies illustrate several cases problem
NP-hard, cases problem known polynomial time, despite significant effort, problem complexity many cases remains unknown (Letchford & Conitzer,
599

fiFang, Jiang, & Tambe

2013). Unfortunately, graph turns different cases considered
work. Indeed, DASS model explained game homogeneous defender
resources patrolling graph, similar cases already considered.
However, prior results cannot explain complexity game structure
graph fit prior graphs.
Given computational complexity results directly available, may examine
approaches provide efficient approximations. provide overview two
approaches (providing experimental results Section 7.1.6). first approach attempts
provide compact representation hope providing speedup. end,
apply intuitive approach uses individual strategy profile patroller
calculates best possible mixed strategy combination. Unfortunately, approach
inefficient run-time even DASS model may result suboptimal solution.
Thus, although compact, approach fails achieve goal; explain
approach next.
Assume patroller independently follows mixed strategy. Denote individual mixed strategy patroller u fu (iu , ju , tk ), probability target
protected Q players represented polynomial expression {fu (iu , ju , tk )}
order Q. optimization problem converted minimizing objective function z
non-linear constraints. Assume two patrollers, potential attack
target q time tk , denote probability patroller u protecting target
$u . $u linear fu , attackers expected utility attack represented

AttEU(Fq , tk ) = (1 C1 ((1 $1 )$2 + (1 $2 )$1 ) C2 $1 $2 )Uq (tk )
constraint z AttEU(Fq , tk ) quadratic f , due fact joint probability
represented product individual probability patroller. constraints ensured convex feasible region known polynomial
algorithms solving kind non-convex optimization problems. attempt solve
problem converting mathematical program non-convex objective function linear constraints, i.e., instead minimizing z constraints z AttEU(Fq , tk ),
incorporate constraints objective function
z = max{AttEU(Fq , tk )}
q,k

(25)

results Section 7.1.6 show solve mathematical program MATLAB using function fmincon interior-point method DASS model, algorithm
fails get feasible solution efficiently even enough time given, solution
still suboptimal may get stuck local minimum. conclude, although
approach compact helps saving memory, inefficient run-time may
result loss solution quality.
second approach takes step reduce run-time complexity, making
polynomial approximation algorithm, lead high degradation solution quality. approach, iteratively compute optimal defender strategy
newly added resource unit given existing strategies previous defender resources.
Namely, first calculate f1 (i1 , j1 , tk ) one patroller available calculate
600

fiProtecting Moving Targets Multiple Mobile Resources

f2 (i2 , j2 , tk ) given value f1 (i1 , j1 , tk ). way, need solve W linear programs
complexity O(M N 2 ) approach much faster compared former one. Unfortunately, approach fails capture coordination patrollers effectively
thus may result high degradation solution quality. example, suppose
two targets constant utility U , one target stays terminal one
stays terminal B. Further, suppose protection coefficient always 1 target
protected one patrollers. two patrollers available, optimal solution would protect one targets way, targets protected
probability 1 expected utility function attacker 0. defender
strategy calculated patroller sequentially discussed above, solution would
protect target probability 0.5 players, making attackers expected
utility 0.25%U . words, reach suboptimal solution, wasting resources
patrollers end protecting target probability 0.25. case,
already see 0.25 probability target unprotected clearly
optimal solution existed protected targets probability 1. Thus, even
two patrollers solution leads potentially significant loss expected utility;
therefore, solution clearly appears inadequate purposes.
Given discussion, would appear fast approximation may lead
significant losses solution quality may efficient enough. Fortunately current
application domains, current deployment CASS protecting ferries (e.g.,
Staten Island Ferry New York), number defender resources limited.
lack resources main reason optimization using security games becomes critical.
result, current approach CASS adequate current domains ferry
protection. research scale-up issue future work.

4. Equilibrium Refinement
game often multiple equilibria. Since game zero-sum, equilibria achieve
objective value. However, attacker deviates best response,
equilibrium strategies defender may provide better results others.
Consider following example game. two targets moving [t1 , t2 ] (no
discretization): one moves d3 d2 moves d1 d2 (See
Figure 6(a)). Suppose d3 d2 = d2 d1 = = 0.5d. one patroller
available protection coefficient C1 = 1. targets utility functions decrease
10 1 [t1 , t2 ] (See Figure 6(b)). one equilibrium, f3,2,1 = f1,2,1 = 0.5, i.e.,
patroller randomly chooses one target follows way. another equilibrium,
f3,3,1 = f1,1,1 = 0.5, i.e., patroller either stays d1 d3 . either equilibrium,
attackers best response attack t1 , maximum expected utility 5.
However, attacker physically constrained (e.g., due launch point locations)
attack earlier t0 t0 > 11 (where 11 intersection time point
11 = (t1 + t2 )/2), defender strategies choose attack either
targets t0 . attackers expected utility Uq (t0 )/2 first equilibrium
50% probability patroller following target. However second
equilibrium, assured succeed get utility Uq (t0 ) distance
chosen target d1 (or d3 ) larger t0 , i.e., chosen target unprotected
601

fiFang, Jiang, & Tambe

t0 . case, defender strategy first equilibrium preferable one
second; indeed, first defender strategy dominates second one, mean
first equally good better second matter strategy attacker
chooses. provide formal definition dominance Section 4.1.








10




1
















(a) Two targets moves schedules S1
S2 .







(b) Utility function
targets decreasing linearly
time.

Figure 6: example show one equilibrium outperforms another attacker
constrained attack [t0 , t2 ] t0 > 11 .
goal improve defender strategy robust constrained
attackers keeping defenders expected utility unconstrained attackers
same. task selecting one multiple equilibria game instance
equilibrium refinement problem, received extensive study game theory
(van Damme, 1987; Fudenberg & Tirole, 1991; Miltersen & Srensen, 2007). finite
security games, An, Tambe, Ordonez, Shieh, Kiekintveld (2011) proposed techniques
provide refinement Stackelberg equilibrium. However little prior
research computation equilibrium refinements continuous games.
section, introduce two equilibrium refinement approaches: route-adjust
(Section 4.1) flow-adjust (Section 4.2). approaches applied improve
feasible defender strategy applied optimal defender strategy
existing equilibrium, get new equilibria robust optimal defender
strategies.
expository simplicity, still use single-resource case example,
methods applicable multiple-resources case. results shown evaluation
section experimentally illustrates two refinement methods significantly improve
performance.
4.1 Route Adjust
Given f defender strategy one equilibrium game, find defender
strategy f 0 attacker strategy (q, t), defenders expected utility
f 0 equal higher one f , one f 0 strictly higher
one f least one specific attacker strategy, say f 0 dominates f .
Intuitively, defender choose f 0 instead f f 0 least good f
602

fiProtecting Moving Targets Multiple Mobile Resources

attacker strategy achieve better performance attacker strategies.
equilibrium strategy f 0 robust unknown deviations attacker side.
give formal definition dominance follows.
Definition 7. Defender strategy f dominates f 0 q, t, DefEUf (Fq , t) DefEUf 0 (Fq , t),
q, t, DefEUf (Fq , t) > DefEUf 0 (Fq , t); equivalently zero-sum game, q, t,
AttEUf (Fq , t) AttEUf 0 (Fq , t), q, t, AttEUf (Fq , t) < AttEUf 0 (Fq , t).
Corollary 2. Defender strategy f dominates f 0 q, t, (Fq , t) 0 (Fq , t) q, t,
(Fq , t) > 0 (Fq , t).
Definition 7 simply restates commonly used weak dominance definition game
theory specific game. Corollary 2 follows Equation (1).
section, introduce route-adjust approach gives procedure
finding defender strategy f 1 dominates given defender strategy f 0 . Route-adjust
provides final routes using steps: (i) decompose flow distribution f 0 component
routes; (ii) route, greedily find route provides better protection targets;
(iii) combine resulting routes new flow distribution, f 1 , dominates f 0
f 1 different f0 . detailed process listed Algorithm 2. illustrate
approach using simple dominated strategy shown Figure 3.
accomplish step (i), decompose flow distribution iteratively finding route
contains edge minimum probability. shown Figure 7, first randomly
choose route contains edge E1,2,2 , f (1, 2, 2) = 0.4 minimum among flow
variables. choose R2 = (d1 , d1 , d2 ), set p(R2 ) = f (1, 2, 2) = 0.4.
edge route R2 subtract 0.4 original flow, resulting residual flow.
continue extract routes residual flow route left. Denote Z
number non-zero edges flow distribution graph, Z decreased least
1 iteration. algorithm terminate Z steps Z
routes found. result step (i) sparse description defender mixed strategy
full representation. discuss Section 6, decomposition constitutes one
method executing compact strategy.
step (ii), adjust routes greedily. end, first introduce
r coefficient
dominance relation edges routes, using intersection points qk
r
matrix Aqk (i, j) defined Section 3.3.
Definition 8. Edge Ei,j,k dominates edge Ei0 ,j 0 ,k [tk , tk+1 ] Arqk (i, j) Arqk (i0 , j 0 ),
q = 1..L, r = 0..Mqk , q, r Arqk (i, j) > Arqk (i0 , j 0 ).
dominance relation edges based comparison protection provided
targets sub-interval. following dominance relation routes, denote
edge Eru (k),ru (k+1),k E(u, k) simplify notation, .
Definition 9. Route Ru = (dru (1) , . . . , dru (M ) ) dominates Ru0 = (dru0 (1) , . . . , dru0 (M ) )
k = 1 . . . 1, E(u, k) = E(u0 , k) E(u, k) dominates E(u0 , k) k E(u, k)
dominates E(u0 , k).
Route Ru dominates Ru0 edge Ru either dominates
corresponding edge Ru0 least one edge Ru dominates corresponding edge
Ru0 .
603

fiFang, Jiang, & Tambe

Algorithm 2: Route-Adjust
Input: mixed defender strategy f
Output: updated mixed defender strategy f 0
(i) Decompose f multiple routes iteratively finding route contains
edge minimum probability:
(a) Initialize remaining flow distribution f = f route set = .
Initialize probability distribution routes p(Ru ) = 0, u.
(b) max f(i, j, k) > 0
i. Set (i0 , j0 , k0 ) = arg mini,j,k:f(i,j,k)>0 f(i, j, k).
ii. Set fmin = f(i0 , j0 , k0 ).
iii. Find arbitrary route Ru0 ru0 (k0 1) = i0 ru0 (k0 ) =
j0 (i.e., edge Ei0 ,j0 ,k0 route) f(ru0 (k), ru0 (k+1), k) > 0,
k (i.e., edges route non-zero remaining flow).
iv. Add Ru0 set p(Ru0 ) = fmin .
v. Set f(i, j, k) = f(i, j, k) fmin ru0 (k 1) = ru0 (k) = j.
end
(ii) Adjust route greedily get new set routes 0 corresponding new probability distribution p0 :
(a) Initialize new set 0 = new probability distribution p0 (Ru ) = 0,
u.
(b) 6=
i. Pick route Ru S.
ii. Adjust Ru get new route Ru0 : given Ru specified
k , set ru0 (k) = ru (k) k 6= k . Set ru0 (k ) = i0 that: 1)
E(u1 , k 1) E(u1 , k ) meet speed constraint; 2) Ru0 dominates Ru choice i0 ; 3) Ru0 dominated route
choice i0 . i0 exists, set ru0 (k ) = ru (k )
iii. Add Ru 0 set p0 (Ru0 ) = p(Ru ).
iv. Remove Ru S.
end
(iii) Reconstruct new compact representation f 0 0 p0 according
Equation 4.

604

fiProtecting Moving Targets Multiple Mobile Resources

, ,
p R 0.4

, ,
p R 0.2






































, ,
p R 0.4
















Figure 7: Step (i): decomposition. Every time route containing minimal flow variable
subtracted residual graph left decomposition. original flow distribution thus decomposed three routes R2 , R1 , R3
probability 0.4, 0.2 0.4 respectively.

Denote original route adjusted Ru new route Ru0 . greedy
way improve route replace one node route. want replace
node time tk , ru0 (k) = ru (k), k 6= k dru (k ) original
route replaced dru0 (k ) . patrollers route changes [tk 1 , tk +1 ]. Thus,
edges E(u, k 1) E(u, k ) original route replaced E(u0 , k 1)
E(u0 , k ) new route.
trying find new route Ru0 dominates original route provide equal
protection targets. selection ru0 (k ) needs meet requirements
specified Algorithm 2. first one describes speed limit constraint. second
one actually requires changed edges E(u0 , k 1) E(u0 , k ) either equal
dominate corresponding edges original route (and dominance relation exist
least one edge). third requirement attains local maximum. new node
exist specified k , return original route Ru .
iterate process new route get final route denoted Ru0
several iterations state convergence reached. state convergence
reached, resulting route Ru0 keeps unchanged matter k chosen
next iteration.
example Figure 7, assume targets moving schedule d1 d1 d2 ,
d3 d2 = d2 d1 = d, = 0.1d utility function constant. adjust route
one iteration changing patrollers position time t3 , i.e., ru (3). t3
last discretized time point, edge E(u, 2) may changed. R1 = (d1 , d1 , d1 ),
enumerate possible patrollers positions time t3 choose one according
three constraints mentioned above. case, candidates d1 d2 ,
corresponding new routes R1 (unchanged) R2 = (d1 , d1 , d2 ) respectively. Note
edge Ed1 ,d2 ,2 dominates Ed1 ,d1 ,2 former one protects target way
[t2 , t3 ] thus R2 dominates R1 . d2 chosen patrollers position t3 R2
605

fiFang, Jiang, & Tambe

chosen new route. adjustment routes non-zero probability
decomposition shown Table 2.
Ru
R1 = (d1 , d1 , d1 )
R2 = (d1 , d1 , d2 )
R3 = (d2 , d1 , d1 )

p(Ru ) decomposition
0.2
0.4
0.4

Adjusted Routes
(d1 , d1 , d2 ) = R2
(d1 , d1 , d2 ) = R2
(d2 , d1 , d2 ) = R4

Table 2: Step (ii): Adjust route greedily.

R1
R2
R3
R4

Ru
= (d1 , d1 , d1 )
= (d1 , d1 , d2 )
= (d2 , d1 , d1 )
= (d2 , d1 , d2 )

p0 (Ru ) adjustment
0
0.6
0
0.4

Composed Flow Distribution









Table 3: Step (iii): compose new compact representation.
new routes get step (ii) original routes dominate
original routes. is, whenever route Ru chosen according defender mixed
strategy resulting step (i), always equally good better choose corresponding new route Ru0 instead, Ru0 provides equal protection
targets Ru . Suppose H possible routes defender strategy step
(i), denoted R1 , ..., RH . adjusting routes, get new defender strategy
(p0 (R1 ), p0 (R2 ), ..., p0 (RH )) full representation (See Table 3). routes taken
higher probability (e.g. p0 (R2 ) = 0.2 + 0.4 = 0.6) lower probability
(e.g. p0 (R3 ) = 0) compared original strategy. step (iii), reconstruct new
compact representation according Equation 4. accomplished via process
inverse decomposition exactly map strategy full
representation compact representation. example above, result shown
Table 3.
Theorem 3. steps (i)(iii), get new defender strategy f 1 dominates
original one f 0 f 1 different f 0 .
Proof: continue use notation decomposition step (i) yields
routes R1 , ..., RH . flow distribution variable original distribution f 0 (i, j, k),
decomposed H sub-flows {fu0 (i, j, k)} according route decomposition. fu0 (i, j, k) =
p(Ru ) = ru (k), j = ru (k + 1) fu0 (i, j, k) = 0 otherwise. Thus following
equation.
XH
f 0 (i, j, k) =
f 0 (i, j, k)
(26)
u=1 u
X
=
fu0 (i, j, k)
(27)
u:ru (k)=i,ru (k+1)=j

adjust route separately, non-zero sub-flow fu0 (i, j, k) edge E(u, k) moved
edge E(u0 , k) route Ru adjusted Ru0 . Reconstructing flow distribution f 1
606

fiProtecting Moving Targets Multiple Mobile Resources

also regarded adding sub-flows adjustment together edge.
means, f 1 composed set sub-flows adjustment, denoted {fu1 (i0 , j 0 , k)}.
subscript u represents index original route indicate moved
edge E(u, k). fu1 (i0 , j 0 , k) = fu0 (ru (k), ru (k + 1), k), i0 = Ru0 (k) j 0 = Ru0 (k + 1);
otherwise fu1 (i0 , j 0 , k) = 0. Similarly Equation 27, following equation f 1 .
f 1 (i0 , j 0 , k) =
=

XH

f 1 (i0 , j 0 , k)
u=1 u

X
u0 :ru0 (k)=i0 ,ru0 (k+1)=j 0

(28)
fu1 (i0 , j 0 , k)

(29)

Based adjustment made, Ru0 dominates Ru thus E(u0 , k)
dominates E(u, k). edge E(u, k) protects target Fq time t,
corresponding edge E(u0 , k) adjustment also protects target Fq time t.
Recall Section 3.3 (Fq , t) sum f (i, j, k) whose corresponding edge
Ei,j,k protect target Fq time t. denote 0 (Fq , t) 1 (Fq , t) probabilities protection corresponding f 0 f 1 respectively. According Equation 27,
0 (Fq , t) viewed sum non-zero sub-flows fu0 (i, j, k) corresponding E(u, k) protects target Fq time t. fu0 (i, j, k) term summation
calculate 0 (Fq , t), means E(u, k) protects Fq thus corresponding E(u0 , k)
protects Fq t, corresponding sub-flow fu1 (ru0 (k), ru0 (k + 1), k) f 1 also term
summation calculate 1 (Fq , t). leads conclusion 0 (Fq , t) 1 (Fq , t). Note
q, t, 0 (Fq , t) = 1 (Fq , t), routes kept unchanged step (ii) otherwise
contradicts fact new route dominates original route. According
Corollary 2, f 1 dominates f 0 different f 0 .
example Figure 7, f 0 (1, 1, 2) decomposed two non-zero terms f10 (1, 1, 2) =
0.2 f30 (1, 1, 2) = 0.4 along routes R1 R3 (See Figure 7). adjustment,
get corresponding subflows f11 (1, 2, 2) = 0.2, f31 (1, 2, 2) = 0.4. Recall targets
schedule d1 d1 d2 . flow distribution adjustment (See Table 5) gives
protection target [t2 , t3 ]. Since flow equal t1 t2 (and therefore
protection same), overall new strategy dominates old strategy.
Therefore, apply route-adjust optimal defender strategy calculated CASS
get robust equilibrium. step (iii) allows us prove Theorem 3, notice
end step (ii), probability distribution set routes
sample actual patrol routes. two defender resources, generalized
version Definition 8 used define dominance relation edge tuple
(Ei1 ,j1 ,k , ..., EiW ,jW ,k ) coefficient matrix multiple patrollers Arqk (i1 , j1 , ..., iW , jW ).
ways adjust route. Instead adjusting one node
route, adjust consecutive nodes time, example, adjust
ru0 (k ) ru0 (k + 1) checking edges E(u0 , k 1), E(u0 , k ) E(u0 , k + 1). However,
need tradeoff performance efficiency algorithm. tradeoff
discussed Section 7.
4.2 Flow Adjust
Whereas route-adjust tries select equilibrium robust attackers playing
suboptimal strategies, second approach, flow-adjust, attempts select new equilibri607

fiFang, Jiang, & Tambe

um robust rational attackers constrained attack time interval [tk , tk+1 ]. discuss below, flow-adjust focuses weaker form dominance,
implies larger set strategies dominated (and thus could potentially
eliminated) compared standard notion dominance used route-adjust; however flow-adjust guarantee elimination dominated strategies.
denote DefEUkf defender expected utility attacker constrained attack
time interval [tk , tk+1 ] attacker provides best response given defender strategy f . Formally, DefEUkf = minq{1...L},t[tk ,tk+1 ] {DefEUf (Fq , t)}. give
following definition local dominance.
Definition 10. Defender strategy f locally dominates f 0 DefEUkf DefEUkf 0 , k.2
Corollary 3. Defender strategy f locally dominates f 0
min
q{1...L},t[tk ,tk+1 ]

{DefEUf (Fq , t)}

min
q{1...L},t[tk ,tk+1 ]

{DefEUf 0 (Fq , t)}, k,

equivalently zero-sum game,
max
q{1...L},t[tk ,tk+1 ]

{AttEUf (Fq , t)}

max
q{1...L},t[tk ,tk+1 ]

{AttEUf 0 (Fq , t)}, k.

Corollary 3 follows fact attacker plays best response given
defender strategy, means f locally dominates f 0 maximum attacker
expected utilities time interval [tk , tk+1 ] given f greater f 0 .
Compared Definition 7, gives standard condition dominance, local
dominance weaker condition; is, f dominates f 0 f locally dominates f 0 ,
however converse necessarily true. Intuitively, whereas Definition 7 attacker
play (possibly suboptimal) strategy, attackers possible deviations
best response restricted. result, set locally dominated strategies
includes set dominated strategies. Definition 10, f locally dominates f 0 ,
attacker rational (i.e., still playing best response) constrained attack
time interval [tk , tk+1 ], f preferable f 0 defender. corollary
even rational attacker constrained attack union
intervals, f still preferable f 0 f locally dominates f 0 . One intuition local
dominance concept following: suppose suspect attacker restricted
(unknown) subset time, due logistical constraints. logistical constraints
would likely make restricted time subset contiguous union small number
contiguous sets. Since sets well-approximated unions intervals [tk , tk + 1],
local dominance serve approximate notion dominance respect
attackers.
Flow-adjust looks defender strategy f 1 locally dominates original defender
strategy f 0 . achieve this, simply adjust flow distribution variables f (i, j, k)
keeping marginal probabilities p(i, k) same. Figure 8 shows example game
two discretized intervals [t1 , t2 ] [t2 , t3 ] (only first interval shown). Suppose
maximal attacker expected utility 5U0 equilibrium attained second
2. dont require exists least one k DefEUkf > DefEUkf 0 .

608

fiProtecting Moving Targets Multiple Mobile Resources

interval [t2 , t3 ]. attackers utility success constant U0 first interval
[t1 , t2 ], defender strategy [t1 , t2 ] could arbitrarily chosen attackers
expected utility [t1 , t2 ] worst case smaller attackers best response
[t2 , t3 ]. However, attacker constrained attack [t1 , t2 ] only, defender strategy
first interval make difference. example, one target moving
d1 d2 [t1 , t2 ]. schedule ferry shown dark lines parallel
lines L11 L21 respect protection radius = 0.2(d2 d1 ) shown dashed
lines. marginal distribution probabilities p(i, k) 0.5 protection coefficient
C1 = 1. f 0 , defenders strategy taking edges E1,1,1 E2,2,1 probability
0.5 attackers maximum expected utility U0 , achieved around time
(t1 + t2 )/2 neither two edges E1,1,1 E2,2,1 within targets protection
range. adjust flows edge E1,2,1 E2,1,1 , shown Figure 8(b), attackers
maximum expected utility [t1 , t2 ] reduced 0.5U0 edge E1,2,1 within targets
protection range way. rational attacker constrained attack
[t1 , t2 ] get lower expected utility given defender strategy f 1 given f 0 , thus
equilibrium f 1 robust kind deviation attacker side.




















(a) f 0 : patroller taking
edges E1,1,1 E2,2,1
probability 0.5.










(b) f 1 : patroller taking
edges E1,2,1 E2,1,1
probability 0.5.

Figure 8: example flow adjust. rational attacker constrained attack
[t1 , t2 ] choose attack around time (t1 + t2 )/2 get utility U0 given f 0
attack around t1 t2 get utility 0.5U0 given f 1 .

flow-adjust, construct 1 new linear programs, one time interval
[tk , tk +1 ], k = 1 . . . 1 find new set flow distribution probabilities f (i, j, k )
achieve lowest local maximum [tk , tk +1 ] unchanged p(i, k ) p(i, k + 1).
609

fiFang, Jiang, & Tambe

linear program interval [tk , tk +1 ] shown below.
min v

f (i,j,k )

f (i, j, k ) = 0, |dj di | > vm
n
X

p(i, k + 1) =
f (j, i, k ), {1 . . . n}
j=1

p(i, k ) =

n
X

f (i, j, k ), {1 . . . n}

j=1

v AttEU (Fq , tk ), q {1 . . . L}, k {k , k + 1}
(r+1)

r+
v max{AttEU (Fq , qk
), AttEU (Fq , qk

)}

q {1 . . . L}, r {0 . . . Mqk }
linear program appears similar linear program CASS,
significant differences. Unlike CASS, marginal probabilities p(i, k ) known
constants provided input mentioned above, separate program
[tk , tk +1 ]. Thus, get f (i, j, k ) local maximum [tk , tk +1 ]
minimized. Denote minimum vk1 . original flow distribution f 0 , get
AttEUf 0 (Fq , t) denote original local maximum value [tk , tk +1 ] vk0 .
subset {f 0 (i, j, k )} original flow distribution f 0 feasible solution linear
program above, vk1 vk0 , noting equality happens interval
attackers best response chosen.
Note change made f (i, j, k) interval [tk , tk +1 ] affect
performance f intervals marginal probabilities p(i, k) kept same,
i.e., changing f (i, j, k ) based linear program independent change
f (i, j, k), k 6= k . solve 1 linear programs independently.
calculating f (i, j, k ) k = 1..M 1, get new defender strategy f 1
combining solutions f (i, j, k ) different linear programs together. vk1 vk0 ,

max
q{1...L},t[tk ,tk +1 ]

AttEUf 0 (Fq , t)

max
q{1...L},t[tk ,tk +1 ]

AttEUf 1 (Fq , t)

k = 1..M 1, i.e., f 1 locally dominates f 0 .
hand, restricted strategies p(i, k),
may exist another strategy f 2 different set p(i, k) locally dominates f 1 .
Finding locally dominating strategies different p(i, k) original topic
future research.
Although two refinement approaches provide necessarily lead nondominated strategy corresponding dominance definition, two approaches
guaranteed find robust (or least indifferent) equilibrium faced
constrained attackers compared original equilibrium obtain CASS. Clearly,
two refinement approaches exhaust space refinement approaches
refinement approaches possible may lead equilibria better
610

fiProtecting Moving Targets Multiple Mobile Resources

(e.g. dominate) one found CASS. However, likely different defender
strategies resulting different equilibrium refinements comparable
terms dominance, i.e., constrained attackers, one equilibrium might turn
better constrained attackers, another equilibrium might better.
computational costs may differ well. Thus, understanding space refinement
approaches terms computational cost output quality, determining
approach adopted circumstances important challenge future
work.

5. Extension Two-Dimensional Space
DASS CASS presented Section 3 based assumption
targets patrollers move along straight line. However, complex model
needed practical domains. example, Figure 9 shows part route map
Washington State Ferries, several ferry trajectories. number patroller
boats tasked protect ferries area, necessarily optimal simply
assign ferry trajectory patroller boat calculate patrolling strategies
separately according CASS described Section 3. ferry trajectories close
other, patrolling strategy take account ferries area
much efficient, e.g., patroller protect ferry moving Seattle Bremerton
first, change direction halfway protect another ferry moving Bainbridge
Island back Seattle.

Figure 9: Part route map Washington State Ferries
section, extend previous model complex case, targets patrollers move two-dimensional space provide corresponding linearprogram-based solution. use single defender resource example, generalize multiple defenders end section.
5.1 Defender Strategy 2-D
one-dimensional case, need discretize time space defender
calculate defenders optimal strategy. time interval discretized set
time points = {tk }. Let G = (V, E) represents graph set vertices V
corresponds locations patrollers may at, discretized time points
, E set feasible edges patrollers take. edge e E satisfies
611

fiFang, Jiang, & Tambe

maximum speed limit patroller possibly practical constraints (e.g., small
island may block edges).
5.2 DASS 2-D
attack occurs discretized time points, linear program DASS
described Section 3 applied two-dimensional settings distance
Constraint 9 substituted Euclidean distance 2-D space nodes Vi Vj .
min

v

(30)

f (i,j,k),p(i,k)

f (i, j, k) [0, 1], i, j, k

(31)

f (i, j, k) = 0, i, j, k ||Vj Vi || > vm

(32)

p(i, k) =

N
X

f (j, i, k 1), i, k > 1

(33)

f (i, j, k), i, k <

(34)

j=1

p(i, k) =

N
X
j=1

N
X

p(i, k) = 1, k

(35)

i=1

v AttEU(Fq , tk ), q, k

(36)

Note f (i, j, k) represents probability patroller moving node Vi
Vj [tk , tk+1 ]. Recall Figure 2.1, patroller protects targets within protective
circle radius . However, one-dimensional space, care straight
line AB, used q (t) = [max{Sq (t) , d1 }, min{Sq (t) + , dN }] protection
range target Fq time t, essence line segment. contrast, whole
circle needs considered protection range two-dimensional space
extended protection range written q (t) = {V = (x, y) : ||V Sq (t)|| }.
change affects value I(i, q, k) thus value AttEU (Fq , tk ) Constraint 36.
5.3 CASS 2-D
attacking time chosen continuous time interval , need
analyze problem similar way Section 3.3. protection radius ,
means patrollers located within circle whose origin Sq (t) radius
protect target Fq . assume target change speed direction
time [tk , tk+1 ], circle also move along line 2-D space. track
circle 3-D space x axes indicate position 2-D z axis
time, get oblique cylinder, similar cylinder except top
bottom surfaces displaced (See Figure 10). patroller moves
vertex Vi ( V ) vertex Vj time [tk , tk+1 ], protects target
within surface. 3-D space described above, patrollers movement
represented straight line.
612

fiProtecting Moving Targets Multiple Mobile Resources







V



V









r

Figure 10: illustration calculation intersection points two-dimensional
setting. x axes indicates position 2-D z axis
time. simplify illustration, z axis starts time tk . example,
two intersection points occurring time points ta tb .

Intuitively, two intersection points patrollers route
3-D space surface. proved analytically calculating exact
time intersection points. Assume patroller moving V1 = (x1 , y1 )
V2 = (x2 , y2 ) target moving Sq (tk ) = (x1 , y1 ) Sq (tk+1 ) = (x2 , y2 )
[tk , tk+1 ] (an illustration shown Figure 10). get time intersection points,
solve quadratic equation coordination parameters protection radius
. present detailed calculation Appendix B. root quadratic equation
within interval [tk , tk+1 ], indicates patrollers route intersects
surface time point. two intersection points. find
intersection points, analysis Section 3.3 applies claim
Lemma 1. conclude need consider attackers strategies
r one-dimensional case denote
intersection points. use notation qk
sorted intersection points get following linear program 2-D case.
min

v

(37)

f (i,j,k),p(i,k)

subject constraints(31 . . . 36)
(r+1)

r+
v max{AttEU(Fq , qk
), AttEU(Fq , qk

)}

(38)

k {1 . . . }, q {1 . . . L}, r {0 . . . Mqk }
Algorithm 1 still used add constraints linear program CASS
2-D case. main difference compared CASS 1-D case since Euclidean
distance 2-D used Constraint 32 need use extended definition q (t)
2-D deciding entries coefficient matrix Arqk (i, j).
multiple defender resources, linear program described Section 3.4 applicable extended definition q (t) used calculate AttEU Constraint 19
613

fiFang, Jiang, & Tambe

substituted following constraint:
f (i1 , j1 , . . . , iW , jW , k) = 0, i1 , . . . , iW , j1 , . . . , jW u, kVju Viu k > vm t.

6. Route Sampling
discussed generate optimal defender strategy compact representation; however, defender strategy executed taking complete route.
need sample complete route compact representation. section,
give two methods sampling show corresponding defender strategy full
representation methods applied.
first method convert strategy compact representation Markov
strategy. Markov strategy setting defender strategy patrollers
movement tk tk+1 depends location patroller tk . denote
(i, j, k) conditional probability moving di dj time tk tk+1 given
patroller located di time tk . words (i, j, k) represents chance
taking edge Ei,j,k given patroller already located node (tk , di ). Thus, given
compact defender strategy specified f (i, j, k) p(i, k),
(i, j, k) = f (i, j, k)/p(i, k), p(i, k) > 0.

(39)

(i, j, k) arbitrary number p(i, k) = 0. get sampled route first
determining start patrolling according p(i, 1); tk , randomly choose
go tk tk+1 according conditional probability distribution (i, j, k).
distribution sampling procedure matches given marginal variables
edge Ei,j,k sampled probability p(i, k)(i, j, k) = f (i, j, k). sampling method
actually leads full representation route Ru = (dru (1) , dru (2) , ..., dru (M ) ) sampled
Q 1
probability p(ru (1), 1)
k=1 (ru (k), ru (k + 1), k), product probability
initial distribution probability taking step. method intuitively
straightforward patrol route decided online patrol, i.e.,
position patroller tk+1 decided patroller reaches position tk ,
makes defender strategy unpredictable. downside method
number routes chosen non-zero probability high N .
2-D case, patroller located node Vi time tk . sampling process exactly
(i, j, k) used denote probability moving Vi Vj
[tk , tk+1 ].
second method sampling based decomposition process mentioned
Section 4.1 (step (i)). discussed first sampling method, sampling
essentially restoring full representation compact representation. shown
Table 1, multiple ways assign probabilities different routes decomposition process route-adjust constructively defines one them. make use
information get process, sample route according probability
assigned decomposed route. number routes chosen non-zero probability
N 2 , much less first method thus becomes feasible describe
strategy full representation, providing routes chosen positive probability. Different sampling approaches may necessitated different application
614

fiProtecting Moving Targets Multiple Mobile Resources

requirements. applications might require defender obtain strategy full
representation presented small number pure strategies. However,
applications, strategy decided on-line, potentially hand-held smartphone (Luber, Yin, Fave, Jiang, Tambe, & Sullivan, 2013) may preferred.
Therefore, based needs application, different sampling strategies might
selected.

7. Evaluation
use different settings ferry protection domain compare performance
terms attackers expected utility AttEU(Fq , t). zero-sum game, lower
value AttEU indicates higher value defenders expected utility.
run experiments 1-D 2-D setting. evaluate performance
CASS show sampling results. also evaluate improvement two
refinement approaches 1-D. Section 7.1 shows results 1-D setting; Section
7.2 2-D setting.
7.1 Experiments One Dimensional Setting
1-D setting, first evaluate performance solvers show much
performance improved using refinement methods. also show sampled
routes example setting evaluate CASS varying number patrollers.
7.1.1 Experimental Settings
used following setting experiments one dimensional case. complex
spatio-temporal game; rather discrete security game previous work.
three ferries moving terminals B total distance AB = 1.
simulation time 30 minutes. schedules ferries shown Figure 11,
x-axis indicates time y-axis distance terminal A. Ferry 1
Ferry 3 moving B Ferry 2 moving B A. maximum speed
patrollers vm = 0.1/min protection radius = 0.1. Experiments
one-dimensional case using 2 patrollers (where C1 = 0.8, C2 = 1.0), except
Section 7.1.5 report experiments different numbers patrollers.

distance

1

0.5

0
0

Ferry1
Ferry2
Ferry3

10

time

20

30

Figure 11: Schedules ferries

615

fiFang, Jiang, & Tambe

7.1.2 Performance Solvers
compare strategies calculated CASS DASS baseline strategy.
baseline strategy, two patrollers choose ferry probability 1/3 (uniformly
random) move alongside offer full protection, leaving two unprotected
(strategy observed practice). First wished stress-test CASS using complex
utility functions realistic case follows. Therefore, tested 4 different
discretization levels (details discretization levels included Table 4) random
utilities, discretization level, created 20 problem instances. problem
instances different across levels. ferry protection domain, utility function
ferry usually depends ferrys position, instance utilities uniformly
randomly chosen [0, 10] discretized distance points; example shown
Figure 12(a). chosen discretization levels ensured Uq (t) linear
time interval [tk , tk+1 ] target Fq . Figure 12(a), x-axis indicates distance
terminal A, y-axis indicates utility successful attack ferry located
distance d. Figure 12(b), x-axis plots four discretization levels y-axis plots
average attacker expected utility plays best response 20 instances baseline,
DASS CASS. CASS shown outperform DASS baseline differences
statistically significant (p < 0.01). Note different sets instances generated
different discretization levels, cannot compare results across levels directly.
However, helpful better understanding models. figure, find
solution quality DASS varies lot sometimes worse naive strategy
(e.g., level 1). DASS calculates optimal solution considers
attacks discretized time points. Figure 12(b), solution quality measured
AttEU , calculated maximum continuous attacker strategy set.
gap optimal objective function DASS actual AttEU given
optimal solution DASS may vary different strategies different discretization levels.
Another interesting observation average solution quality CASS almost
discretization levels. Despite difference instance sets, result implies
improvement finer discretization may limited CASS.
Level
1
2
3
4

(minutes)
10
5
2.5
2


4
7
13
16


0.5
0.25
0.125
0.1

N
3
5
9
11

Table 4: Details discretization levels. experiments mentioned section,
distance space evenly discretized, parameterized = di+1 di .

Next turn realistic utility function ferry domain, U -shape
inverse U -shape. Figure 13(a) shows sample utility curve attacker gains
higher utility closer shore. fix utility shore 10, vary utility
middle (denoted Umid ), value floor U -shape top
inverse U -shape evaluate strategies. Figure 13(b), Umid shown x-axis
616

fiProtecting Moving Targets Multiple Mobile Resources

8

Ave(AttEUm)

U utility

10

5

0
0

0.5
distance

1

(a) Randomized attacker utility function

NAIVE
DASS
CASS

6
4
2
0

Level1 Level2 Level3 Level4

(b) Average solution quality different
strategies

Figure 12: Performance different randomized utility function settings. utility
function set experiments function distance Terminal A.
utility function piece-wise linear value discretized distance
points di chosen randomly [0,10].

15
Sup(AttEU)

U utility

10
8
6
4
0

0.5
distance

5
0
0

1

(a) Realistic attacker utility function
Umid = 5

10

NAIVE
DASS
CASS

5

10
Umid

15

20

(b) Solution quality different strategies

Figure 13: Performance different realistic utility function settings. utility function U-shape inverse U-shape. utility around distance 0.5 denoted
Umid . compare defender strategy given DASS CASS
baseline Umid changing 1 20.

compare performance strategies terms attackers expected utility
plays best response y-axis. conclude 1) strategy calculated CASS
outperforms baseline DASS; 2) DASS may actually achieve worse results
baseline.
Among different experiment settings discretization utility function,
choose one instance provide detailed analysis it. refer instance
example setting following section. example setting, discretization
level 4 used utility curve shown Figure 13(a), parameters involved
described Section 7.1.1. Figure 14 compares attacker expected utility function
DASS CASS used respectively. x-axis indicates time t, y-axis
indicates attackers expected utility attacks Ferry 1 time t. strategy calculated DASS, worst performance discretized time points 3.50 (AttEU(F1 , 20)),
however, supremum AttEU(F1 , t), [0, 30] high 4.99 (AttEU(F1 , 4+ )),
617

fiFang, Jiang, & Tambe

5

AttEU

4
3
2
1
0

DASS
CASS
10
20
time

30

Figure 14: attackers expected utility function given defender strategy calculated
DASS vs CASS example setting. expected utilities discretized
time points indicated squares CASS dots DASS. maximum
AttEU CASS 3.82, 30%less maximum AttEU
DASS, 4.99.

experimentally shows taking consideration attacks discretized time points necessary. strategy calculated CASS, supremum
AttEU(F1 , t) reduced 3.82.
7.1.3 Improvement Using Refinement Methods
compare refinement approaches described Section 4 analyze tradeoff
performance improvement runtime. Three approaches considered comparison: route-adjust, flow-adjust variation route-adjust, denoted route-adjust2.
step (ii) route-adjust, replace every node route one-by-one sequence.3
step (ii) route-adjust2, replace every consecutive pair nodes route sequence.
first show results example setting. Figure 15(a), compare AttEU(Fq , t)
function defender strategy given CASS one route-adjust Ferry
1. shows attack aiming target time, defender strategy
route-adjust refinement equally good better one original equilibrium,
thus defender performs equally better matter attacker constrained
time, i.e., defender strategy route-adjust dominates original strategy. Figure
15(b) comparison AttEU function defender strategy route-adjust
one route-adjust2 Ferry 1. one route-adjust2 dominate
one route-adjust overall former appears perform better latter
frequently larger amounts. use average value AttEU function
metric performance, show route-adjust2 better route-adjust
example setting later Table 5. Figure 15(c) shows comparison AttEU
function defender strategy given CASS defender strategy
3. supplementary experiments, also tested route-adjust iterations, e.g., repeating
process replacing every node sequence five times. extra benefit insignificant
runtime increases proportionally number iterations. light this, choose replace
node experiments reported article.

618

fi4

4

3

3

AttEU

AttEU

Protecting Moving Targets Multiple Mobile Resources

2

CASS
RouteAdjust

1
0

10

time

20

2
1
0

30

(a) AttEU function Ferry 1
route-adjust (one node time)

RouteAdjust
RouteAdjust2
10

time

20

30

(b) AttEU function Ferry 1
route-adjust2 (two nodes time)

AttEU

4
3
2
1
0

CASS
FlowAdjust
10

time

20

30

(c) Performance flow-adjust

Figure 15: Performance equilibrium refinement approaches.
flow-adjust Ferry 1. strategy given CASS dominated one
flow-adjust Definition 7, investigate maximum AttEU time
interval [tk , tk+1 ], shown Table 6, find defender strategy flow-adjust
locally dominates original strategy.
list worst case performance average performance AttEU function
ferries example setting four defender strategies (CASS, route-adjust, routeadjust2, flow-adjust) Table 5, conclude 1) worst case performance
strategies flow-adjust same, means defender achieves exactly
expected utility towards unconstrained rational attacker; 2) average performance
flow-adjust slightly better CASS, outperformed route-adjust
route-adjust2, takes much less time run compared two; 3)
example setting, adjust two consecutive nodes time, performance better
adjusting one node time, difference significant much
expensive terms run-time.
Strategies
CASS
Route-Adjust
Route-Adjust2
Flow-Adjust

Worst Case Performance
3.82
3.82
3.82
3.82

Average Performance
3.40
2.88
2.76
3.34

Runtime (minutes)
8.96
32.31
0.50

Table 5: Comparison different refinement approaches terms average performance
runtime. runtime refinement process calculated.

619

fiFang, Jiang, & Tambe

time interval [tk , tk+1 ]
[2, 4]
[4, 6]
[6, 8]
[8, 10]
[10, 12]
[12, 14]
[14, 16]

maximum

3.7587
3.8182
3.8153
3.8137
3.8052
3.8050
3.7800

maximum

3.6675
3.8182
3.6164
3.6316
3.6316
3.5664
3.2100

time interval [tk , tk+1 ]
[16, 18]
[18, 20]
[20, 22]
[22, 24]
[24, 26]
[26, 28]
[28, 30]

maximum

3.8111
3.8182
3.8182
3.8182
3.8182
3.8182
3.8182

maximum

3.7291
3.8182
3.8182
3.8182
3.8182
3.8182
3.8182

Table 6: maximum attackers expected utility time interval decreases
flow-adjust used.

Figure 16(a) Figure 16(b) shows maximum average improvement
route-adjust, route-adjust2 flow-adjust, averaged 20 instances Level 4
randomized utilities used Figure 12(b); Figure 16(c) shows
average runtime. maximum improvement largest difference AttEU
function given defender strategy calculated CASS one refinement.
average improvement average difference two functions. standard
deviations instances shown error bars. Figure 16 confirms refinement approaches improve defender strategy calculated CASS terms
maximum performance average performance thus provide better defender strategies given possible constrained attackers. Route-adjust2 achieves improvement,
route-adjust, flow-adjust least. Flow-adjust achieves much less improvement
compared two approaches. One explanation constraints
strong require marginal probabilities unchanged likely
little changes made original defender strategy. difference routeadjust2 route-adjust significant. terms run-time, flow-adjust least
expensive, route-adjust second route-adjust2 most. Route-adjust2 significantly expensive compared two. conclude route-adjust
better choice considering tradeoff improvement runtime.
7.1.4 Sampled Routes
first convert defender strategy example setting Markov strategy
sample 1000 pair patrol routes. defender strategy used one
route-adjust. sample, pair routes chosen step step two patrol
boats according joint conditional probability distribution {(i1, j1, i2, j2, k)}.
routes two patrol routes chosen simultaneously coordinating
other. cannot show pair separately 1000 samples. Instead, Figure
17(a) shows frequency taken 1000 samples edge. x-axis
indicates time y-axis distance terminal A. width edge
indicates frequency chosen least one patroller. Although Figure 17(a)
precisely depict samples, provides rough view routes taken
patrol boats.
620

fiProtecting Moving Targets Multiple Mobile Resources

0.6

routeadjust
routeadjust2
flowadjust

2

Ave Improvement

Max Improvement

2.5

1.5
1
0.5

routeadjust
routeadjust2
flowadjust

0.4
0.2

0

0

Runtime (minutes)

(a) Average maximal improvement

(b) Average average improvement
routeadjust
routeadjust2
flowadjust

40
30
20
10
0

(c) Average runtime

Figure 16: Comparison refinement approaches.

Figure 17(b) shows pair routes highest probability use
decomposition method sampling. solid lines show patrol boats routes
dashed lines show ferries schedules. get 3958 different pair patrol routes total
decomposition process shown pair routes chosen probability 1.57%.

1

distance

distance

1
0.8
0.6
0.4
0.2
0
0

5

10

15

20

25

0.8
0.6

0.2
0
0

30

time

Patrol Boat 1
Patrol Boat 2

0.4

5

10

15

20

25

30

time

(a)

(b)

Figure 17: Results sampling example setting: (a) Frequency edge
chosen first sampling method based Markov strategy used. (b)
Decomposed routes highest probability superimposed ferry schedules
second sampling method based decomposition used.

621

fiAttacker EU

6
4

log(Runtime (seconds))

Fang, Jiang, & Tambe

1 patroller
2 patrollers
3 patrollers
4 patrollers

2
0

1
0
1

3

Attacker EU

4

(b) Runtime Level 1
log(Runtime (seconds))

(a) Solution quality Level 1
5

1 patroller
2 patrollers
3 patrollers
4 patrollers

2

1 patroller
2 patrollers
3 patrollers

3
2
1
0

(c) Solution quality Level 2

3
2

1 patroller
2 patrollers
3 patrollers

1
0
1
2

(d) Runtime Level 2

Figure 18: Performance varying number patrollers.

7.1.5 Number Patrollers
Figure 18(a) shows improvement performance CASS increasing number
patrollers discretization Level 1. x-axis shows number patrollers
y-axis indicates average attackers maximal expected utility, i.e., expected reward
plays best response. results averaged 20 random utility settings
discretization Level 1. fewer patrollers, performance defender varies lot
depending randomized utility function (as indicated standard deviation shown
error bar). variance gets much smaller patrollers, means
defender sufficient resources different instances. Figure 18(b) shows run-time
CASS. y-axis indicates average natural logarithm runtime. surprisingly,
run-time increases number patrollers increases.
Figure 18(c) 18(d) show average performance run-time CASS discretization Level 2, using set utility settings used Level 1. results
1 3 patrollers shown. program runs memory 4 patrollers
N 8 = 2734375 flow distribution variables least N 4 = 8757 constraints. Note
average solution quality Level 2 better result Level 1 (e.g.,
average attacker EU 1 patroller 4.81 Level 1 4.13 Level 2), indicates
higher level granularity improve solution quality. However, granularity clearly
affect ability scale-up; means need consider tradeoff
solution quality memory used one way combat scaling-up problem
reduce level granularity. Nonetheless, number patrollers encountered
real-world scenarios New York order 3 4, CASS capable
least key real-world scenarios.
622

fiProtecting Moving Targets Multiple Mobile Resources

7.1.6 Approximation Approach Multiple Defender Resources
tested first approximation approach multiple defender resources described
Section 3.4 example setting. used fmincon function interior-point
method MATLAB minimize non-linear objective function (Equation 25). Table
7 lists different run-time value objective function achieved given different
iteration number (denoted MaxIter ). function ensured provide feasible
solution iteration number large enough, shown first two rows.
compared result LP formulation DASS, implemented
MATLAB using linprog function. DASS solved within 8.032 seconds provides
optimal solution AttEUm = 3.5, approximation approach outperformed
run-time efficiency solution quality. approach fails provide feasible solution
efficiently even sufficient time given (more 400 times run-time
LP formulation), maximum attacker expected utility 18% larger optimal
solution. mainly new formulation approximation approach
longer linear convex, making difficult find global maximum.
axIter
3000
10000
900000

Run time(sec)
4.14
17.21
3298

AttEUm
infeasible
infeasible
4.0537

Table 7: Performance approximation approach.

7.2 Experiments Two Dimensional Setting
settings 2-D space complex even single patroller. show
example setting motivated ferry system Seattle, Bainbridge island
Bremerton shown Figure 9. example setting, three terminals (denoted A,B
C) non-collinear 2-D space shown Figure 19(a). Ferry 1 Ferry
2 moving trajectory Terminal B C (denoted Trajectory 1)
Ferry 3 Ferry 4 moving trajectory Terminal B (denoted
Trajectory 2). schedules four ferries shown Figure 19(b), x-axis
time y-axis distance common terminal B. Ferry 1 moves
C B, Ferry 2 moves B C, Ferry 3 moves B Ferry 4 moves
B. Similar one-dimensional scenario ferry domain, assume utility
decided ferrys position utility function shown Figure 19(c).
x-axis distance common terminal B y-axis utility two
trajectories respectively. 2-D space discretized grid shown Figure 19(d)
x = 1.5 = 1 indicating interval x-axis y-axis. patroller
located one intersection points grid graph discretized time points.
simulation time 60 minutes = 13, i.e., tk+1 tk = 5 minutes. speed limit
patroller = 0.38 available edges patroller take
[tk , tk+1 ] shown Figure 19(d). one patroller involved. protection radius
set = 0.5, protection coefficient C1 = 0.8.
623

fiFang, Jiang, & Tambe

C

2



distance Terminal B

Terminals 2D
Trajectory 1
B

1

Trajectory 2
0


0

1.5

3

4.5

x

Ferry Schedules
1

0.6
0.4
0.2
0
0

60

Edges Available
2

Ferry Trajectory1
Ferry Trajectory2

6



utility

40

(b) Ferry schedules

Utility Function
8

20

time

(a) Three terminals
10

Ferry1
Ferry2
Ferry3
Ferry4

0.8

1

4
2
0
0

0.2

0.4

0.6

0.8

0
0

1

1.5

3

4.5

x

distance Terminal B
(c) Utility function

(d) Available edges

Figure 19: example setting two-dimensional space
Figure 20(a) compares performance DASS CASS Ferry 2. Ferry 2 chosen
strategies, attackers best response attack Ferry 2. x-axis
time t, y-axis attacker expected utility attacking Ferry 1 time t.
maximum AttEU CASS 6.1466, 12% lower compared result DASS,
6.9817. Figure 20(b) 20(c) show two sampled route given strategy calculated
CASS 2-D map dashed lines represents ferry trajectories.
patroller starts node text start follows arrowed route, ends
node text end end patrol. may stay nodes text
stay. patrol routes shown intuitive way ambiguous. exact
route listed table time position. routes sampled based
converted Markov strategy, total number patrol routes may chosen
non-zero probability 4.49 1010 .

8. Related Work
section discuss literature related work. first discuss work
computation game-theoretic patrolling strategies, discuss work continuous
games, finally discuss work equilibrium refinement.
mentioned introduction, Stackelberg games widely applied security domains, although work considered static targets (e.g., Korzhyk
et al., 2010; Krause, Roper, & Golovin, 2011; Letchford & Vorobeychik, 2012; Kiekintveld
624

fiProtecting Moving Targets Multiple Mobile Resources

Sampled Route CASS

7

stay

6


AttEU

2

4
0

stay
staystart
end

1

DASS
CASS

5

0

20
40
time

60

0

1.5

3

4.5

x

(a) Solution quality DASSand CASS
Ferry 2

(b) Sampled route 1 superimposed ferry
trajectories

Sampled Route CASS
staystart



2

stay
end

1

0
0

1.5

3

4.5

x
(c) Sampled route 2 superimposed ferry
trajectories

Figure 20: Experimental results two-dimensional settings

et al., 2013). Agmon, Kraus, Kaminka (2008) proposed algorithms computing
mixed strategies setting perimeter patrol adversarial settings mobile robot
patrollers. Similarly, Basilico, Gatti, Amigoni (2009) computed randomized leader strategies robotic patrolling environments arbitrary topologies. Even
players mobile, e.g., hider-seeker games (Halvorson, Conitzer, & Parr, 2009),
infiltration games (Alpern, 1992) search games (Gal, 1980), targets (if any) assumed static. Tsai et al. (2009) applied Stackelberg games domain scheduling
federal air marshals board flights. targets (i.e., flights) domain mobile,
players restricted move along targets protect attack them.
stationary nature leads discrete game models finite numbers pure strategies.
Bosansky, Lisy, Jakob, Pechoucek (2011) Vanek, Jakob, Hrstka, Pechoucek
(2011) studied problem protecting moving targets. However, considered
model defender, attacker targets discretized movements
directed graph. discretization attacker strategy spaces introduce suboptimality
solutions, shown DASS. We, work, generalize strategy space attacker continuous realm compute optimal strategies even
setting. Furthermore, provide efficient scalable linear formulation,
Bosansky et al. presented formulation non-linear constraints, faced problems
scaling larger games even single defender resource.
625

fiFang, Jiang, & Tambe

Yin et al. (2012) considered domain patrolling public transit networks (such
LA Metro subway train system) order catch fare evaders. players
ride along trains follow fixed schedule, domain inherently discrete
modeled patrolling problem finite zero-sum Bayesian game. Yin et al. proposed
compact representation defender mixed strategies flows network. adapt
compact representation idea continuous domain. particular, domain need
model interaction defenders flow attackers continuous strategy
space. proposed sub-interval analysis used spatio-temporal reasoning efficiently
reduce problem finite LP.
Games continuous strategy spaces well-studied game theory. Much
economics literature focused games whose equilibria solved analytically
(and thus question computation arise), example classical theory
auctions (see e.g., Krishna, 2009). Recent computational approaches analysis
design auctions focused discretized versions auction games (e.g.,
Thompson & Leyton-Brown, 2009; Daskalakis & Weinberg, 2012). research
efficiently solving two-player continuous games specific types utility functions,
zero-sum games convex-concave utility functions (Owen, 1995) separable
continuous games polynomial utility functions (Stein, Ozdaglar, & Parrilo, 2008).
Johnson, Fang, Tambe (2012) studied continuous game model protecting forests
illegal logging. model target (i.e., forest) stationary,
simplifying assumptions (e.g., forest circular shape) able solve
game efficiently. contrast existing work, game model moving targets
continuous domain, resulting utility functions discontinuous thus existing
approaches applicable. CASS algorithm solves game optimally without
needing discretize attackers strategy space.
extensive literature equilibrium refinement; however existing work
computation equilibrium refinement focuses finite games. simultaneousmove finite games, solution concepts perfect equilibrium proper equilibrium
proposed refinements Nash equilibrium (Fudenberg & Tirole, 1991). Miltersen
Srensen (2007) proposed efficient algorithm computing proper equilibria finite zero-sum games. finite security games, et al. (2011) proposed refinement
Stackelberg equilibrium techniques computing refinements. resulting
defender strategy robust possibilities constrained capabilities attacker.
existing approaches rely finiteness action sets, thus applicable
setting. Simon Stinchcombe (1995) proposed definitions perfect equilibrium
proper equilibrium infinite games continuous strategy sets, however
propose computational procedure resulting solution concepts. Exact computation equilibrium refinements continuous games MRMTsg remains challenging
open problem.

9. Conclusion
paper makes several contributions computing optimal strategies given moving targets mobile patrollers. First, introduce MRMTsg , novel Stackelberg game model
takes consideration spatial temporal continuity. model, targets move
626

fiProtecting Moving Targets Multiple Mobile Resources

fixed schedules attacker chooses attacking time continuous time
interval. Multiple mobile defender resources protect targets within protection
radius, bring continuous space analysis. Second, develop fast solution
approach, CASS, based compact representation sub-interval analysis. Compact representations dramatically reduce number variables designing optimal patrol
strategy defender. Sub-interval analysis reveals piece-wise linearity attacker
expected utility function shows finite set dominating strategies attacker. Third, propose two approaches equilibrium refinement CASSs solutions:
route-adjust flow-adjust. Route-adjust decomposes patrol routes, greedily improves
routes composes new routes together get new defender strategy. Flowadjust fast simple algorithm adjusts flow distribution achieve optimality
time interval keeping marginal probability discretized time points
unchanged. Additionally, provide detailed experimental analyses ferry protection
domain. CASS deployed US Coast Guard since April 2013.

10. Future Work
several important avenues future work. include: (i) use decreasing
function model protection provided targets instead using fixed protection
radius; (ii) handle practical constraints patrol boat schedule easily implementable; (iii) efficiently handle complex uncertain target schedules utility
functions.
provide initial discussion relaxation assumptions
listed Section 2 used throughout paper:
allow complex uncertain target schedules, may model problem
game targets follow stochastic schedules. framework may still apply
may need enriched (e.g., using approaches use MDPs represent
defender strategies, see Jiang, Yin, Zhang, Tambe, & Kraus, 2013). Coordinating
multiple defenders becomes important challenge. may helpful
cases appeal prior work multi-agent teamwork, given
significant uncertainty cases leading need on-line coordination
(Tambe, 1997; Stone, Kaminka, Kraus, & Rosenschein, 2010; Kumar & Zilberstein,
2010; Yin & Tambe, 2011).
focus environments multiple attackers coordinate attacks,
may need enhance framework. Prior results Korzhyk,
Conitzer, Parr (2011) stationary targets discrete time would helpful
addressing challenge, although case moving targets continuous space
time cases provides significant challenge. Combining
previous item future work, complex multiple defender multiple attacker scenario
would appear significant computational challenge.

627

fiFang, Jiang, & Tambe

Acknowledgments
thank USCG officers, particularly Craig Baldwin, Joe Direnzo Francis
Varrichio officers sector New York, exceptional collaboration. views
expressed herein author(s) construed official reflecting
views Commandant U.S. Coast Guard. research supported
US Coast Guard grant HSHQDC-10-D-00019 MURI grant W911NF-11-1-0332.
also thank anonymous reviewers valuable suggestions.
preliminary version work appears conference paper (Fang, Jiang, &
Tambe, 2013). several major advances article: (i) Whereas earlier work
confined targets move 1-D space, provide significant extension algorithms
(DASS CASS) article enable targets patrollers move 2D space; also provide detailed experimental results 2-D extension. (ii)
provide additional novel equilibrium refinement approaches experimentally compare
performance equilibrium refinement approach offered earlier work;
allows us offer improved understanding equilibrium refinement space. (iii)
discuss several sampling methods detail sample actual patrol routes mixed
strategies generate discussion missing earlier work. (iv) provide
detailed proofs omitted previous version work.

628

fiProtecting Moving Targets Multiple Mobile Resources

Appendix A. Notation Table

Notation
MRMT
MRMTsg
L
Fq
A, B


Sq (t)
W
Pu
vm

CG
Uq (t)

N
tk
di

Ru
ru (k)
f (i, j, k)
p(i, k)
Ei,j,k
p(Ru )
AttEU(Fq , t)
q (t)
(Fq , t)
I(i, q, k)
L1q ,L2q
r
qk
r
AttEU(Fq , qk
)
Mqk
Arqk (i, j)
E(u, k)

Meaning
problem multiple Mobile Resources protecting Moving Targets
Game model continuous set strategies attacker MRMT.
Number ferries.
Ferry index q.
Terminal points.
Continuous time interval finite set time points.
Continuous space possible locations set distance points.
Ferry schedule. Position target Fq specified time t.
Number patrollers.
Patroller index u.
Speed limit patroller.
Protection radius patroller.
Probability attacker stopped G patrollers.
Positive reward successful attack target Fq time attacker.
Number discretized time points.
Number discretized distance points.
Discretized time point.
Discretized distance point.
Distance two adjacent time points.
Patrol route patroller Pu . discretization defenders strategy space,
Ru described vector.
patroller located dru (k) time tk .
Flow distribution variable. Probability patroller moves di dj
time [tk , tk+1 ].
Marginal distribution variable. Probability patroller located di tk .
directed edge linking nodes (tk , di ) (tk+1 , dj ).
Probability taking route Ru .
Attacker expected utility attacking target Fq time t.
Protection range target Fq time
Probability patroller protecting target Fq time t.
Whether patroller located di time tk protecting target Fq .
Lines Sq (t) .
rth intersection point [tk , tk+1 ] respect target Fq .
r .
Left/right-side limit AttEU(Fq , t) qk
Number intersection points [tk , tk+1 ] respect target Fq .
r , r+1 ]; 0 otherwise.
C1 patroller taking edge Ei,j,k protect target Fq [qk
qk
Short Eru (k),ru (k+1),k .
Table 8: Summary notations involved paper.

629

fiFang, Jiang, & Tambe

Appendix B. Calculation Intersection Points CASS 2-D Settings
calculate time patrollers route intersects protection range
target patroller moving V1 = (x1 , y1 ) V2 = (x2 , y2 ) target
moving Sq (tk ) = (x1 , y1 ) Sq (tk+1 ) = (x2 , y2 ) [tk , tk+1 ]. patrollers
position given time [tk , tk+1 ] denoted (x, y) targets position denoted
(x, y).
tk
(x2 x1 ) + x1 ,
tk+1 tk
tk
(x2 x1 ) + x1 ,
x =
tk+1 tk

tk
(y2 y1 ) + y1
tk+1 tk
tk
=
(y2 y1 ) + y1
tk+1 tk

x=

y=

(40)
(41)

intersection point, distance patrollers position targets position
equals protection radius , looking time
(x x)2 + (y y)2 = re2

(42)

substituting variables Equation 42 Equations 4041, denoting
(x2 x1 ) (x2 x1 )
,
tk+1 tk
(y2 y1 ) (y2 y1 )
A2 =
,
tk+1 tk
A1 =

B1 = x1 x1 ,
B2 = y1 y1 ,

Equation 42 simplified
(A1 A1 tk + B1 )2 + (A2 A2 tk + B2 )2 = re2 .

(43)

Denote C1 = B1 A1 tk C2 = B2 A2 tk , easily get two roots
quadratic equation,
p
2(A1 C1 + A2 C2 ) 2 (A1 C1 + A2 C2 )2 (A21 + A22 )(C12 + C22 re2 )
ta,b =
.
(44)
2(A21 + A22 )
ta tb time valid intersection point within time interval
consideration ([tk , tk+1 ]).

References
Agmon, N., Kraus, S., & Kaminka, G. A. (2008). Multi-robot perimeter patrol adversarial
settings. IEEE International Conference Robotics Automation (ICRA), pp.
23392345.
Alpern, S. (1992). Infiltration Games Arbitrary Graphs. Journal Mathematical Analysis Applications, 163, 286288.
630

fiProtecting Moving Targets Multiple Mobile Resources

An, B., Kempe, D., Kiekintveld, C., Shieh, E., Singh, S. P., Tambe, M., & Vorobeychik, Y.
(2012). Security games limited surveillance. Proceedings Twenty-Sixth
AAAI Conference Artificial Intelligence, pp. 12411248.
An, B., Tambe, M., Ordonez, F., Shieh, E., & Kiekintveld, C. (2011). Refinement strong
stackelberg equilibria security games. Proceedings Twenty-Fifth AAAI
Conference Artificial Intelligence (AAAI), pp. 587593.
Basilico, N., Gatti, N., & Amigoni, F. (2009). Leader-follower strategies robotic patrolling environments arbitrary topologies. Proceedings 8th International Conference Autonomous Agents Multiagent Systems (AAMAS) Volume 1, pp. 5764.
Bosansky, B., Lisy, V., Jakob, M., & Pechoucek, M. (2011). Computing time-dependent
policies patrolling games mobile targets. 10th International Conference
Autonomous Agents Multiagent Systems (AAMAS) - Volume 3, pp. 989996.
Conitzer, V., & Sandholm, T. (2006). Computing optimal strategy commit to.
Proceedings 7th ACM Conference Electronic Commerce, EC 06, pp. 8290.
Daskalakis, C., & Weinberg, S. M. (2012). Symmetries optimal multi-dimensional mechanism design. Proceedings 13th ACM Conference Electronic Commerce,
EC 12, pp. 370387.
Fang, F., Jiang, A. X., & Tambe, M. (2013). Optimal patrol strategy protecting moving
targets multiple mobile resources. Proceedings 2013 International Conference Autonomous Agents Multi-agent Systems, AAMAS 13, pp. 957964.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Gal, S. (1980). Search Games. Academic Press, New York.
Gatti, N. (2008). Game theoretical insights strategic patrolling: Model algorithm
normal-form. Proceedings 18th European Conference Artificial Intelligence
(ECAI), pp. 403407.
Greenberg, M., Chalk, P., & Willis, H. (2006). Maritime terrorism: risk liability. Rand
Corporation monograph series. RAND Center Terrorism Risk Management Policy.
Halvorson, E., Conitzer, V., & Parr, R. (2009). Multi-step Multi-sensor Hider-Seeker Games.
IJCAI.
Jakob, M., Vanek, O., & Pechoucek, M. (2011). Using agents improve international
maritime transport security. Intelligent Systems, IEEE, 26 (1), 9096.
Jiang, A. X., Yin, Z., Zhang, C., Tambe, M., & Kraus, S. (2013). Game-theoretic randomization security patrolling dynamic execution uncertainty. Proceedings
2013 international conference Autonomous agents multi-agent systems,
AAMAS 13, pp. 207214.
Johnson, M. P., Fang, F., & Tambe, M. (2012). Patrol strategies maximize pristine forest
area. Proceedings Twenty-Sixth AAAI Conference Artificial Intelligence
(AAAI), pp. 295301.
631

fiFang, Jiang, & Tambe

Kiekintveld, C., Islam, T., & Kreinovich, V. (2013). Security games interval uncertainty. Proceedings 2013 International Conference Autonomous Agents
Multi-agent Systems, AAMAS 13, pp. 231238.
Kiekintveld, C., Jain, M., Tsai, J., Pita, J., Ordonez, F., & Tambe, M. (2009). Computing
optimal randomized resource allocations massive security games. Proceedings
8th International Conference Autonomous Agents Multiagent Systems
- Volume 1, AAMAS 09, pp. 689696.
Korzhyk, D., Conitzer, V., & Parr, R. (2010). Complexity computing optimal Stackelberg
strategies security resource allocation games. Proceedings 24th National
Conference Artificial Intelligence (AAAI), pp. 805810.
Korzhyk, D., Conitzer, V., & Parr, R. (2011). Security games multiple attacker resources. Proceedings Twenty-Second international joint conference Artificial Intelligence - Volume Volume One, IJCAI11, pp. 273279. AAAI Press.
Krause, A., Roper, A., & Golovin, D. (2011). Randomized sensing adversarial environments. Proceedings 22nd International Joint Conference Artificial
Intelligence (IJCAI), pp. 21332139.
Krishna, V. (2009). Auction theory. Academic press.
Kumar, A., & Zilberstein, S. (2010). Anytime planning decentralized POMDPs using
expectation maximization. Proceedings Twenty-Sixth Conference Uncertainty Artificial Intelligence, pp. 294301.
Letchford, J. (2013). Computational Aspects Stackelberg Games. Ph.D. thesis, Duke
University.
Letchford, J., & Conitzer, V. (2013). Solving security games graphs via marginal probabilities. Proceedings Twenty-Seventh AAAI Conference Artificial Intelligence (AAAI), pp. 591597.
Letchford, J., & Vorobeychik, Y. (2012). Computing optimal security strategies interdependent assets. Conference Uncertainty Artificial Intelligence (UAI),
pp. 459468.
Luber, S., Yin, Z., Fave, F. D., Jiang, A. X., Tambe, M., & Sullivan, J. P. (2013). Gametheoretic patrol strategies transit systems: trusts system mobile app
(demonstration). International Conference Autonomous Agents Multiagent
Systems (AAMAS)[Demonstrations Track], pp. 13771378.
Marecki, J., Tesauro, G., & Segal, R. (2012). Playing repeated stackelberg games unknown opponents. Proceedings 11th International Conference Autonomous
Agents Multiagent Systems, AAMAS 12, pp. 821828.
Miltersen, P. B., & Srensen, T. B. (2007). Computing proper equilibria zero-sum games.
Proceedings 5th International Conference Computers Games, CG06,
pp. 200211.
Owen, G. (1995). Game Theory (3rd ed.). Academic Press.
632

fiProtecting Moving Targets Multiple Mobile Resources

Paruchuri, P., Tambe, M., Ordonez, F., & Kraus, S. (2006). Security multiagent systems
policy randomization. Proceedings fifth international joint conference
Autonomous agents multiagent systems, AAMAS 06, pp. 273280.
Pita, J., Jain, M., Marecki, J., Ordonez, F., Portway, C., Tambe, M., Western, C., Paruchuri,
P., & Kraus, S. (2008). Deployed ARMOR protection: application game theoretic model security Los Angeles International Airport. Proceedings
7th International Joint Conference Autonomous Agents Multiagent Systems:
Industrial Track, AAMAS 08, pp. 125132.
Pita, J., Jain, M., Ordonez, F., Portway, C., Tambe, M., Western, C., Paruchuri, P., &
Kraus, S. (2009). Using game theory los angeles airport security.. AI Magazine,
30, 4357.
Shieh, E., An, B., Yang, R., Tambe, M., Baldwin, C., DiRenzo, J., Maule, B., & Meyer,
G. (2012). PROTECT: deployed game theoretic system protect ports
United States. Proceedings 11th International Conference Autonomous
Agents Multiagent Systems - Volume 1, AAMAS 12, pp. 1320.
Simon, L. K., & Stinchcombe, M. B. (1995). Equilibrium refinement infinite normal-form
games. Econometrica, 63 (6), 14211443.
Stein, N. D., Ozdaglar, A., & Parrilo, P. A. (2008). Separable low-rank continuous
games. International Journal Game Theory, 37 (4), 475504.
Stone, P., Kaminka, G. A., Kraus, S., & Rosenschein, J. S. (2010). Ad hoc autonomous
agent teams: Collaboration without pre-coordination. Proceedings 24th AAAI
Conference Artificial Intelligence, pp. 15041509.
Tambe, M. (1997). Towards flexible teamwork. JOURNAL ARTIFICIAL INTELLIGENCE RESEARCH, 7, 83124.
Tambe, M. (2011). Security Game Theory: Algorithms, Deployed Systems, Lessons
Learned. Cambridge University Press.
Thompson, D. R. M., & Leyton-Brown, K. (2009). Computational analysis perfectinformation position auctions. Proceedings 10th ACM conference Electronic commerce, EC 09, pp. 5160.
Tsai, J., Rathi, S., Kiekintveld, C., Ordonez, F., & Tambe, M. (2009). IRIS - tool
strategic security allocation transportation networks. Eighth International
Conference Autonomous Agents Multiagent Systems - Industry Track, AAMAS
09, pp. 3744.
van Damme, E. (1987). Stability Perfection Nash equilibria. Springer-Verlag.
Vanek, O., Jakob, M., Hrstka, O., & Pechoucek, M. (2011). Using multi-agent simulation
improve security maritime transit. Proceedings 12th International
Workshop Multi-Agent-Based Simulation (MABS), pp. 116.
Vorobeychik, Y., & Singh, S. (2012). Computing stackelberg equilibria discounted stochastic games. Proceedings Twenty-Sixth Conference Artificial Intelligence (AAAI), pp. 14781484.
633

fiFang, Jiang, & Tambe

Yin, Z., Jiang, A. X., Johnson, M. P., Kiekintveld, C., Leyton-Brown, K., Sandholm, T.,
Tambe, M., & Sullivan, J. P. (2012). TRUSTS: Scheduling randomized patrols
fare inspection transit systems. Proceedings Twenty-Fourth Conference
Innovative Applications Artificial Intelligence (IAAI), pp. 23482355.
Yin, Z., & Tambe, M. (2011). Continuous time planning multiagent teams temporal
constraints. Proceedings Twenty-Second international joint conference
Artificial Intelligence - Volume Volume One, IJCAI11, pp. 465471. AAAI Press.

634

fiJournal Artificial Intelligence Research 48 (2013) 1-22

Submitted 12/12; published 10/13

Natural Language Inference Arabic Using Extended Tree
Edit Distance Subtrees
Maytham Alabbas

maytham.alabbas@gmail.com

Department Computer Science, University Basrah,
Basrah, Iraq

Allan Ramsay

Allan.Ramsay@manchester.ac.uk

School Computer Science, University Manchester,
Manchester, M13 9PL, UK

Abstract
Many natural language processing (NLP) applications require computation similarities pairs syntactic semantic trees. Many researchers used tree edit
distance task, technique suffers drawback deals single node operations only. extended standard tree edit distance algorithm
deal subtree transformation operations well single nodes. extended algorithm subtree operations, TED+ST, effective flexible standard
algorithm, especially applications pay attention relations among nodes (e.g.
linguistic trees, deleting modifier subtree cheaper sum deleting
components individually). describe use TED+ST checking entailment
two Arabic text snippets. preliminary results using TED+ST encouraging compared two string-based approaches standard algorithm.

1. Introduction
Tree edit distance widely used component natural language processing (NLP)
systems attempt determine whether one text snippet supports inference another
(roughly speaking, whether first entails second), distance pairs
dependency trees taken measure likelihood one entails other.
extend standard algorithm calculating distance two trees allowing
operations apply subtrees, rather single nodes. extension improves
performance technique Arabic around 5% F-score around 4%
accuracy compared number well-known techniques. relative performance
standard techniques Arabic testset replicates results reported
techniques English testsets. also applied extended version tree edit
distance, TED+ST, English RTE-2 testset, outperforms standard
algorithm.
Tree edit distance generalisation standard string edit distance metric,
measures similarity two strings. used underpin several NLP
applications information extraction (IE), information retrieval (IR) natural
language inference (NLI). edit distance two trees defined minimum
cost sequence edit operations transform one tree another. numerous
approaches calculating edit distance trees, reported Selkow (1977), Tai
c
!2013
AI Access Foundation. rights reserved.

fiAlabbas & Ramsay

(1979), Zhang Shasha (1989), Klein (1998), Demaine, Mozes, Rossman, Weimann
(2009) Pawlik Augsten (2011). chosen work Zhang-Shashas
algorithm (Zhang & Shasha, 1989) intermediate structures produced
algorithm allow us detect respond operations subtrees. refer
standard tree edit distance algorithm throughout rest article, mean ZhangShashas algorithm, use short form ZS-TED.
ultimate goal develop NLI system Arabic (Alabbas, 2011).1 NLI
problem determining whether natural language hypothesis h reasonably inferred
natural language premise p. challenges NLI quite different
encountered formal deduction: emphasis informal reasoning, lexical semantic
knowledge, variability linguistic expression, rather long chains formal
reasoning (MacCartney, 2009). recent, better-known, formulation NLI
task recognising textual entailment challenge (RTE), described Dagan Glickman
(2004) task determining, two text snippets premise p hypothesis h, whether
. . . typically, human reading p would infer h likely true. According
authors, entailment holds truth h, interpreted typical language user,
inferred meaning p. popular method used recent years
tasks use tree edit distance, compares sentence pairs finding minimal
cost sequence editing operations transform tree representation one sentence
tree (Kouylekov, 2006; Heilman & Smith, 2010). Approximate tree matching
kind allows users match parts two trees, rather demanding complete
match every element tree. However, one main drawbacks tree edit
distance transformation operations applied solely single nodes (Kouylekov,
2006). Kouylekov Magnini (2005) used standard tree edit distance, uses
transformation operations (insert, delete exchange) solely single nodes, check
entailment two dependency trees. hand, Heilman Smith
(2010) extended available operations standard tree edit distance INSERT-CHILD,
INSERT-PARENT, DELETE-LEAF, DELETE-&-MERGE, RELABEL-NODE RELABEL-EDGE.
authors also identify three new operations, MOVE-SUBTREE, means move node X
tree last child left/right side node (s.t.
descendant X ), NEW-ROOT MOVE-SIBLING, enable succinct edit sequences
complex transformation. extended set edit operations allows certain combinations
basic operations treated single steps, hence provides shorter (and therefore
cheaper) derivations. fine-grained distinctions between, instance, different kinds
insertions also make possible assign different weights different variations
operation. Nonetheless, operations continue operate individual nodes rather
subtrees (despite name, even MOVE-SUBTREE appears defined operation
nodes rather subtrees). solved problem extending basic
version algorithm costs operations insert/delete/exchange subtrees
derived appropriate function costs operations parts.
makes TED+ST effective flexible standard algorithm, especially
applications pay attention relations among nodes (e.g deleting modifier subtree,
linguistic trees, cheaper sum deleting components individually).
1. particular, Modern standard Arabic (MSA). refer Arabic throughout article,
mean MSA.

2

fiNatural Language Inference Arabic

rest paper organised follows: Zhang-Shashas algorithm, ZS-TED,
explained Section 2. Section 3 presents TED+ST. Section 4 describes dependency trees
matching. Dataset preparation explained Section 5. experimental results
discussed Section 6. Conclusions given Section 7.

2. Zhang-Shashas TED Algorithm
approach extends ZS-TED, uses dynamic programming provide O(n4 )
algorithm finding optimal sequence node-based edit operations transforming
one tree another. section contains brief recapitulation algorithma
detailed description given Bille (2005).
Ordered trees trees left-to-right order among siblings significant.
Approximate tree matching allows us match tree parts another tree.
three operations, namely deleting, inserting exchanging node,
transform one ordered tree another. nonnegative real cost associated
edit operation. costs changed match requirements specific applications.
Deleting node x means attaching children parent x. Insertion inverse
deletion. means inserted node becomes parent consecutive sub-sequence
left right order parent. Exchanging node alters label. editing
operations illustrated Figure 1 (Bille, 2005).

(a)

l1

l2

(b)

l1

l1

l2

(b)

l1

l1
l2

Figure 1: (a) Relabeling node label (l1 l2 ). (b) Deleting node labeled (l2 ).
(c) Inserting node labeled l2 child node labeled l1 ( l2 ).

operation associated cost allowed single nodes only. Selecting
good set costs operations hard dealing complex problems.
3

fiAlabbas & Ramsay

alterations costs choosing different combination lead
drastic changes tree edit distance performance (Mehdad & Magnini, 2009).
ZS-TED algorithm, tree nodes compared using postorder traversal,
visits nodes tree starting leftmost leaf descendant root proceeding
leftmost descendant right sibling leaf, right siblings,
parent leaf tree root. last node visited always
root. example postorder traversal leftmost leaf descendant tree
shown Figure 2. figure, two trees, T1 m=7 nodes T2 n=7
nodes. subscript node considered order node postorder
tree. So, postorder T1 e,f,b,g,c,d,a postorder T2 g,c,y,z,x,d,a.
leftmost leaf descendant subtrees T1 headed nodes e,f,b,g,c,d,a
1,2,1,4,4,6,1 respectively, similarly leftmost leaf descendants g,c,y,z,x,d,a T2
1,1,3,4,3,3,1.
a7

a7
c5

b3
e1

f2

d6

g4

c2

d6

g1

x5
y3

T1

z4

T2

Figure 2: Two trees T1 T2 postorder traversal.
descendants node, least cost mapping calculated
node encountered, order least cost mapping selected right away.
achieve this, algorithm pursues keyroots tree, defined
set contains root tree plus nodes left sibling. Concentrating
keyroots critical dynamic nature algorithm, since subtrees
rooted keyroots allow problem split independent subproblems
general kind. keyroots tree decided advance, permitting algorithm
distinguish tree distance (the distance two nodes considered
context left siblings trees T1 T2 ) forest distance (the distance
two nodes considered separately siblings ancestors
descendants) (Kouylekov, 2006). illustration, keyroots tree Figure 2
marked bold.
node, computation find least cost mapping (the tree distance)
node first tree one second depends solely mapping nodes
children. find least cost mapping node, then, one needs recognise least cost
mapping keyroots among children, plus cost leftmost child.
nodes numbered according postorder traversal, algorithm proceeds
following steps (Kouylekov, 2006): (i) mappings leaf keyroots determined;
(ii) mappings keyroots next higher level decided recursively; (iii)
root mapping found. Algorithm 1 shows pseudocode ZS-TED algorithm (Zhang
& Shasha, 1989). matrices F used recording results individual
4

fiNatural Language Inference Arabic

subproblems: used store tree distance trees rooted pairs nodes
two trees, F used store forest distance sequences nodes.
F used temporary store tree edit distance pairs keyroots
calculated. extended standard algorithm, computes cost
cheapest edit sequence, also records edit operations themselves.
involves adding two new matrices, DPATH FDPATH, hold appropriate sequences
edit operationsDPATH hold edit sequences trees rooted pairs nodes
FDPATH hold edit sequences forests. DPATH permanent arrays,
whereas F FDPATH reinitialised pair keyroots.
algorithm iterates keyroots, split two main stages pair
keyroots: initialisation phase (lines 312) deals first row column,
assume every cell first row reached appending insert operation
cell left every cell first column reached appending delete operation
cell it, appropriate costs. exactly parallel initialisation
standard dynamic time warping algorithm calculating string edit distance, though
treating task matching subsets subtrees rooted T1 [x] T2 [y]
string matching problem nodes two trees sequences enumerated
post-order.
second stage (lines 1337) traces cost edit sequence transforming
sub-sequence sequence nodes dominated T1 [x] sub-sequence sequence
nodes dominated T2 [x], considering whether nodes reached
cell left insert, cell delete, cell diagonally
left either match exchange x. two cases considered
here:
two sequences consideration trees (tested line 15), know
considered every possible way exchanging one other, hence
record cost F D, edit sequence FDPATH
DPATH. case, calculate cost moving along diagonal inspection
two nodes. See Figure 3 illustration notion.
ii one sequences forest retrieve cost moving along diagonal
DPATH, store cost F edit sequence FDPATH.
cases, gather set {cost, path} pairs result considering insert/delete/exchange operations preceding sub-sequences, choose best
pair store various arrays. similar corresponding element
string edit algorithm, added complication calculating tree edit costs
sequences pair keyroots involves calculating costs edit sequences
pairs sub-sequences nodes roots. results pairs keyroots
stored permanently, utilised calculations sub-sequences next
stage.
Bille (2005) provides detailed worked examples calculation costs transforming one tree another. Figure 4 shows FDPATH grows algorithm iterates
keyroots trees T1 T2 Figure 2. figure, cells representing
5

fiAlabbas & Ramsay

Algorithm 1 pseudocode Zhang-Shashas TED algorithm edit sequences
[i, j]
ith jth nodes post-order enumeration tree (T [i, i] written [i])
l(i)
leftmost leaf descendant subtree rooted
K(T )
keyroots tree T, K(T ) = {k : k1 > k l(k1 ) = l(k)}
D[i, j]
tree distance two nodes T1 [i] T2 [j]
F D[T1 [i, i1 ], T2 [j, j1 ]]
forest distance nodes i1 T1 nodes j j1 T2
DP H[i, j]
edit sequence trees rooted two nodes T1 [i] T2 [j]
F DAT H[T1 [i, i1 ], T2 [j, j1 ]] edit sequence forests covered nodes i1 T1 nodes j j1 T2
(T1 [i] )
cost deleting ith node T1
( T2 [j])
cost inserting jth node T2 T1
(T1 [i] T2 [j])
cost exchanging ith node T1 jth node T2
m, n
number nodes T1 T2 respectively
best
choose best cost path set options
1: x 1 |K1 (T1 )|
2:
1 |K2 (T2 )|
3:
F D[, ] 0
4:
F DP H[, ]
5:
l1 (x) x
6:
F D[T1 [l1 (x), i], ] F D[T1 [l1 (x), i-1], ] + (T1 [i] )
7:
F DP H[T1 [l1 (x), i], ] F DP H[T1 [l1 (x), i-1], ] +
8:
end
9:
j l2 (y)
10:
F D[, T2 [l2 (y), j]] F D[, T2 [l2 (y), y-1]] + ( T2 [j])
11:
F DP H[, T2 [l2 (y), j]] F DP H[, T2 [l2 (y), y-1]] +
12:
end
13:
l1 (x) x
14:
j l2 (y)
15:
(l1 (i) == l1 (x) l2 (j) == l2 (y))
16:
cost, path best({F D[T1 [l1 (x), i-1], T2 [l2 (y), j]] + (T1 [i] ),
17:
F DP H[T1 [l1 (x), i-1], T2 [l2 (y), j]] + d},
18:
{F D[T1 [l1 (x), i], T2 [l2 (y), j-1]] + ( T2 [j]),
19:
F DP H[T1 [l1 (x), i], T2 [l2 (y), j-1]] + i},
20:
{F D[T1 [l1 (x), i-1], T2 [l2 (y), j-1]] + (T1 [i] T2 [j])),
21:
F DP H[T1 [l1 (x), i-1], T2 [l2 (y), j-1]] + m/x})
22:
F D[T1 [l1 (x), i], T2 [l2 (y), j]] cost
23:
D[i, j] cost
24:
F DP H[T1 [l1 (x), i], T2 [l2 (y), j]] path
25:
DP H[i, j] path
26:
else
27:
cost, path best({F D[T1 [l1 (x), i-1], T2 [l2 (y), j]] + (T1 [i] ),
28:
F DP H[T1 [l1 (x), i-1], T2 [l2 (y), j]] + d},
29:
{F D[T1 [l1 (x), i], T2 [l2 (y), j-1]] + ( T2 [j]),
30:
F DP H[T1 [l1 (x), i], T2 [l2 (y), j-1]] + i},
31:
{F D[T1 [l1 (x), i-1], T2 [l2 (y), j-1]] + D[i, j]),
32:
F DP H[T1 [l1 (x), i-1], T2 [l2 (y), j-1]] + DP H[i][j]})
33:
F D[T1 [l1 (x), i], T2 [l1 (y), j]] cost
34:
F DP H[T1 [l1 (x), i], T2 [l1 (y), j]] path
35:
end
36:
end
37:
end
38:
end
39: end
40: return D[n, m], DP H[n, m]

6

fiNatural Language Inference Arabic

i-1,j-1

i,j-1

i-1,j
x/m





i,j

Figure 3: edit operation direction used algorithm. arc implies edit
operation labeled: insertion, deletion, x exchanging
operation (matching).

optimal sequence edit operations transform T1 T2 highlighted bold,
final optimal path shown last cell (at final row column).
T1
e
f
b
g
c



T2

dd
ddd
dddd
ddddd
dddddd
ddddddd

g

x
xd
xdd
dddm
dddmd
dddmdd
dddmddd

c
ii
xi
xid
xdx
dddmi
dddmm
dddmmd
dddmmdd


iii
iix
xix
xdxi
xdxx
dddmmi
dddmmx
dddmmxd

z
iiii
iiix
iixx
iixxd
xdxxi
dddmmii
dddmmxi
dddmmxid

x
iiiii
iiixi
xiiix
iixxx
xdxixi
dddmmiii
dddmmxii
dddmmxiid


iiiiii
iiixii
xiiiix
iixxxi
xdxixii
xdxixix
dddmmiiim
dddmmiiimd


iiiiiii
iiixiii
xiiiixi
iixxxii
iixxxiid
xdxixixi
dddmmiiimi
dddmmiiimm

FDPATH

Figure 4: Computing optimal path trees Figure 2.
mapping two trees found final sequence edit operations
mapping nodes corresponding match operation only.
final distance 6 represents final values (at final row column) D.2
last value DPATH represents final sequence edit operations, namely dddmmiiimm. According path, define alignment two postorder trees.
alignment two trees T1 T2 obtained inserting gap symbol (i.e. _)
either T1 T2 , according type edit operation, resulting strings 1
2 length sequence edit operations. gap symbol inserted
2 edit operation delete (d), whereas inserted 1 edit
operation insert (i). Otherwise, nodes T1 T2 inserted 1 2
respectively. following optimal alignment T1 T2 :
1:
2:

e

_

f

_

b

_

g

g

c

c

_



_

z

_

x









2. simplicity here, assume single operation cost 1 except matching cost
0, described Zhang Shasha (1989).

7

fiAlabbas & Ramsay

means:
d:
d:
d:
m:
m:
i:
i:
i:
m:
m:

Delete (e) T1
Delete (f ) T1
Delete (b) T1
Leave (g) without change
Leave (c) without change
Insert (y) T1
Insert (z ) T1
Insert (x ) T1
Leave (d) without change
Leave (a) without change

final mapping T1 T2 shown Figure 5. mapping figure
insertion, deletion, matching exchanging operations shown single, double,
single dashed double dashed outline respectively. matching nodes (or subtrees)
linked dashed arrows.

a7
c5

b3
e1

f2

a7
d6

g4

c2

d6

g1

x5
y3

T1

z4

T2

Figure 5: ZS-TED, mapping T1 T2 .

3. Extended TED Subtree Operations
main weakness ZS-TED algorithm able perform transformations
subtrees (i.e. delete subtree, insert subtree exchange subtree). output ZSTED lowest cost sequence operations single nodes. extend find
lowest cost sequence operations nodes subtrees, TED+ST, follows:
1. Run ZS-TED compute standard alignment results (Algorithm 1);
2. Go alignment group subtree operations. sequence identical
operations applies set nodes comprising subtree, replaced
8

fiNatural Language Inference Arabic

single operation, whose cost determined appropriate function costs
individual nodes (Algorithm 2). variety functions could applied here,
depending application. using algorithm textual entailment
use costs Figure 8, derived used Punyakanok, Roth,
Yih (2004), illustration current section simply take cost
subtree operation half sum costs individual operations
make up.
noted apply technique modify Zhang-Shashas
O(n4 ) algorithm, could also applied algorithm finding tree edit distance,
e.g. Kleins O(n3 logn ) algorithm (Klein, 1998), Demaine et al. O(n3 ) algorithm (Demaine,
Mozes, Rossman, & Weimann, 2009) Pawlik Augsten O(n3 ) algorithm (Pawlik &
Augsten, 2011), since extension operates output original algorithm.
additional time cost O(n2 ) negligible since less time cost available
tree edit distance algorithm.
3.1 Find Sequence Subtree Edit Operations
Extending ZS-TED cover subtree operations give us flexibility comparing
trees (especially linguistic trees). key algorithm find maximal
sequences identical edit operations correspond subtrees. sequence nodes
postorder corresponds subtree following conditions satisfied: (i) first
node leaf; (ii) leftmost sibling last node sequence (i.e. root
subtree) first node sequence. two conditions
checked constant time, since leftmost sibling node determined node
advance. hence find maximal sequences corresponding subtrees scanning
forwards sequence node operations find sequences identical operations,
scanning backwards sequence find point
covers subtree. involves potentially O(n2 ) stepsn forward steps find sequences
identical operations, possibly n-1 backward steps time find sub-sequences
corresponding subtrees. example, sequence nodes e,f,b tree T1 Figure 2
subtree e leaf leftmost last node b 1, represents
first node e. hand, sequence nodes g,c,d tree
subtree g leaf, leftmost last node 6, represents itself,
first node g.
Algorithm 2 contains pseudocode find optimal sequence single subtree
edit operations transforming T1 T2 . Ep=1..L {d, i, x, m} algorithm
optimal sequence node edits transforming T1 T2 , obtained applying
technique Section 2, 1 2 alignments T1 T2 obtained
applying sequence node edits.
shown Algorithm 2, find optimal single subtree edit operations sequence
transforms T1 T2 , maximal sequence identical operations checked see
whether contains subtree(s) not. Checking whether sequence corresponds
subtree depends type edit operation, according following rules: (i)
operation d, sequence checked first tree; (ii) operation i,
sequence checked second tree; (iii) otherwise, sequence checked
9

fiAlabbas & Ramsay

Algorithm 2 pseudocode find subtree edit operations
E
L
S1, S2

sequence edit operations transform tree T1 tree T2 , Ep=1..L {d,i,x,m}
length sequence edit operations E
optimal alignment T1 T2 respectively, length 1 = 2 = L

1: repeat
2:
ERoot EL
3:
F L
4:
repeat
5:
(F 2 EF 1 == ERoot)
6:
F F 1
7:
end
8:
(F == L)
9:
LL1
10:
ERoot EL
11:
F L
12:
end
13:
(F < L F 2 EF 1 )= ERoot) (L = 0)
14:
F0 F
15:
(F < L)
16:
(F < L)
17:
IsSubtree true
18:
(F < L IsSubtree)
1
19:
(ERoot =d SF1 ..SL
subtree)
2
2
20:
(ERoot =i SF ..SL subtree)
1
2
21:
((ERoot {x,m}) (SF1 ..SL
SF2 ..SL
subtrees))
22:
Replace EF ..EL1 +
23:
LF 1
24:
F F0
25:
else
26:
IsSubtree f alse
27:
end
28:
end
29:
F F +1
30:
end
31:
L L1
32:
F F0
33:
end
34:
L F0 1
35: (L 0)
36: return E

trees. that, sequence operations corresponds subtree, symbols
sequence replaced + except last one (which represents root
subtree). Otherwise, checking starts sub-sequence original, explained below.
instance, let us consider Eh , ..., Et , 1 h < L, 1 < L, h < t, sequence
edit operation, i.e. Ek=h..t {d, i, x, m}. Let us consider h0 = h, firstly
check nodes Sh1 , ..., St1 Sh2 , ..., St2 see whether heads subtrees.
Ek d, nodes Sh1 , ..., St1 checked, nodes Sh2 , ..., St2 checked,
otherwise, nodes Sh1 , ..., St1 Sh2 , ..., St2 checked. edit operations Eh , ..., Et1
replaced + sequence corresponds subtree. Then, start checking
beginning another sequence left subtree Eh , ..., Et , i.e. = h 1.
10

fiNatural Language Inference Arabic

Otherwise, checking applied sequence starting next position, i.e.
h = h+ 1. checking continued h = t. that, (t h) sequences
start different positions end position contain subtree, checking
starts beginning new sequence, i.e. h = h0 = 1. process
repeated h = t.
explain subtree operations applied, let us consider two trees T1
T2 Figure 2.
According TED+ST, cost 3 sequence operation follows:
sequence d, result. sequences consist three subtrees
(i.e. three deleted nodes, first two matched nodes three inserted nodes):
ddd mm iii mm. So, final result is: ++d +m ++i mm. means:
++d:
+m:
++i:
m:
m:

Delete subtree (e,f,b) T1
Leave subtree (g,c) without change
Insert subtree (y,z,x ) T1
Leave (d) without change
Leave (a) without change

final mapping T1 T2 obtained using TED+ST shown Figure 6.

a7
c5

b3
e1

f2

a7
d6

g4

c2

d6

g1

x5
y3

T1

z4

T2

Figure 6: TED+ST, mapping T1 T2 .

4. Matching Dependency Trees
mentioned above, main goal design textual entailment (TE) system Arabic
check whether one text snippet (i.e. premise p) entails another text (i.e. hypothesis h).
match p h dependency tree pairs effectively, use TED+ST. enables us
find minimum edit operations transform one tree another. allows us
sensitive fact links dependency tree carry linguistic information
relations complex units, hence ensure paying attention
relations compare two trees. instance, enables us pay attention
11

fiAlabbas & Ramsay

fact operations involving modifiers, particular, applied subtree
whole rather individual elements. Thus, transform tree D1 tree D2
Figure 7 deleting park single operation, removing modifier whole,
rather three operations removing in, park one one, using costs
Figure 8 initial test edit operations experiments. costs
updated version costs used Punyakanok et al. (2004).3 authors found
using tree edit distance gives better results bag-of-word scoring methods,
applied question answering.4
saw

saw




man





park

man



D1

D2

Figure 7: Two dependency trees, D1 D2 .

using costs Figure 8, cost transferring D1 D2 according ZS-TED
19 (i.e. one stop word (5) two words (14)), whereas according TED+ST
operations 0. Therefore, easy decide D1 entails D2 , whereas reverse
true. also exploited subset/superset relations encoded Arabic WordNet (AWN)
(Black, Elkateb, Rodriguez, Alkhalifa, Vossen, Pease, & Fellbaum, 2006) comparing
items tree. Roughly speaking, comparing one tree another requires us swap
two lexical items, happier item source tree synonym
hyponym one target treesince wombat hyponym animal, swapping
wombat premise saw wombat zoo animal saw animal
zoo truth-preserving exchange.
Approaches make use lexical relations kind cope fact
words often multiple meanings. follow Hobbs (2005) assuming W1
sense hyponym sense W2 sentence involving W1 entail
similar sentence involving W2 shown (1).
(1) p.
h.

saw peach yesterdays party.
saw attractive woman yesterdays party.

3. stop words list contains common Arabic words (e.g. particle
( +,- ./0# "#! $ Almdyr mwl director indeed busy
"#! $ indeed). instance, %&'! )*
( +,- ./0# Almdyr mwl director busy.
entails %&'! )*
4. transcription Arabic examples document follows Habash-Soudi-Buckwalter (HSB) transliteration scheme (Habash, Soudi, & Buckwalter, 2007) transcribing Arabic symbols.

12

fiNatural Language Inference Arabic

Cost

Single node

Subtree (more one node)

Delete:

X stop word cost 5,
else cost 7
stop word cost
5,
else cost 100
X subsumed cost 0,
elseif X stop word cost 5,
elseif subsumed (or
antonym of) X cost 100
else cost 50

0

Insert:

Exchange:

double sum costs parts

S1 identical S2 cost 0
else half sum costs parts

Figure 8: Edit operation costs.
(1p), instance, word peach ambiguous,5 shade pink tinged yellow
(hypernym: Pink) Downy juicy fruit sweet yellowish whitish flesh (hypernym:
Drupe, edible fruit, stone fruit) attractive seductive looking woman (hypernym:
Adult female, women) cultivated temperate regions (hypernym: Fruit tree).
context (1h), however, human reader would assume second interpretation
peach intended, despite fact general fairly unusual usage.
reflects widely accepted view contextual information key lexical
disambiguation. Within RTE task, premise provides context disambiguation
hypothesis, hypothesis provides context disambiguation premise.
Almost human reader would, instance, accept (2p) entails (2h), despite
potential ambiguity word bank.
(2) p.
h.

money tied bank.
cannot easily spend money.

5. Dataset Preparation
order train test TE system Arabic, need appropriate dataset.
knowledge, datasets available Arabic, develop one.
followed one procedures used collecting premise-hypothesis pairs
RTE tasks, slight alteration. premises RTE collected variety
sources, e.g. newswire text. contain one two sentences tend fairly long
(e.g. averaging 25 words RTE1, 28 words RTE2, 30 words RTE3 39 words
RTE4). contrast, hypotheses quite short single sentences (averaging 11 words
RTE1, 8 words RTE2 7 words RTE3 RTE4), manually constructed
premise. first three RTE Challenges presented binary classification
task yes balanced numbers yes problems. Beginning RTE4,
three-way classifications (yes, no, contradict, distinguish cases
h contradicts p h compatible with, entailed p).
5. See Sages dictionary online: http://www.sequencepublishing.com/thesageonline.php. WordNet also
provides senses (and more) peach.

13

fiAlabbas & Ramsay

dataset, want produce set p-h pairs handpartly
lengthy tedious process, importantly hand-coded datasets liable
embody biases introduced developer. dataset used training system,
rules extracted little unfolding information explicitly
supplied developers. used testing test examples
developers chosen, likely biased, albeit unwittingly, towards
way think problem.
set Arabic p-h pairs TE task created semi-automatic technique
two stages. first stage (Section 5.1) responsible automatically collecting
p-h pairs news websites, second stage (Section 5.2) uses online annotation
system allows annotators annotate collected pairs manually. stages
explained detail below.
5.1 Collecting p-h Pairs
collected candidate p-h pairs automatically so-called headline-lead paragraph
technique (Burger & Ferro, 2005) web (e.g. newspaper corpora, pairing
first paragraph article, p, headline, h). based observation
news articles headline often partial paraphrase first paragraph
article, conveying thus comparable meaning. use updated version headlinelead paragraph strategy improve quality p-h pair.
key idea pose queries search engine automatically filter
responses text snippets might entail query. pairs manually annotated entailment/non-entailment, texts automatically collected
freely occurring natural texts. eliminates possibility (indeed likelihood)
unconscious bias introduced hypotheses manually generated.
built corpus p-h pairs using headlines websites Arabic newspapers
TV channels queries input Google via standard Google API,
selecting first paragraph, usually represents related text snippet(s)
article headline (Burger & Ferro, 2005), first 10 returned pages.
technique produces large number potential pairs without bias either
premises hypotheses. improve quality pairs resulted query,
use two conditions filter results: (i) length headline must least five
words, avoid small headlines; (ii) fewer 80% words headline
appear premise, avoid similar sentences.
problem p h similar would little
learn used training phase TE system; would
almost worthless test pairvirtually TE system get pair right,
serve discriminatory test pair. therefore eliminate excessively similar
p-h pairs training testing, assess terms number shared
uncommon words.
order overcome problem, matched headlines one source stories
another. Major stories typically covered range outlets, usually variations emphasis wording. Stories different sources linked looking
common words headlinesit unlikely two stories about, in14

fiNatural Language Inference Arabic

stance, neanderthals news time, straightforward matching based
low frequency words proper names likely find articles topic.
terminology structure first text snippets articles, however, likely
quite different. Thus using headline one source first text snippet
article story another source likely produce p-h pairs
unduly similar. therefore link headline one newspaper related
sentences another.
5.2 Annotating p-h Pairs
pairs collected first stage still marked-up human annotators,
least process collecting nearly bias-free possible. pairs cover
number subjects politics, business, sport general news. annotation
performed eight expert non-expert human annotators identify different pairs
positive entailment examples yes, p judged entails h, negative examples
no, entailment hold. annotators follow nearly annotation
guidelines used building RTE task dataset (Dagan, Glickman, & Magnini,
2006).
pair annotated three annotators. inter-annotator agreement (where
annotators agree) around 74% compared 89% annotator agrees
least one co-annotator. suggests annotators found difficult task.
fact 74% agreement annotations produced three independent
annotators taken account sets upper bound reasonable expect
automatic system carrying task. human annotators agree
three quarters cases, unlikely computer-based system achieve
much 75% agreement given pair annotators.6

6. Experiments
check effectiveness TED+ST, used check entailment p-h
Arabic pairs text snippets compared results two string-based approaches
(bag-of-words Levenshtein distance) ZS-TED set pairs. Checking
whether one Arabic text snippet entails another, however, particularly challenging
Arabic ambiguous languages, English. instance, Arabic
written without diacritics (short vowels), often leading multiple ambiguities. makes
morphological analysis difficult (i.e. single written form may easily correspond
many ten different lexemes, see Alabbas & Ramsay, 2011a, 2011b, 2012a, 2012c).
preliminary testing dataset contains 600 pairs, binary annotated yes (a 50-50
split) using technique explained Section 5. distribution pairs p length
summarised Table 1, h average length around 10 words average
common words p h around 4 words. average length sentence
dataset 25 words per sentence, sentences containing 40+ words.
6. dataset, including dependency-tree analysis CoNLL format, available online appendices article http://www.cs.man.ac.uk/~ramsay/ArabicTE/

15

fiAlabbas & Ramsay

ps length
<20
20-29
30-39
>39
Total

#pairs
175
329
87
9
600

yes
83
171
43
3
300


92
158
44
6
300

Table 1: Distribution sentence lengths testset.

order check entailment p-h pairs, follow three steps. First,
sentence preprocessed tagger parser order convert elements p-h
pair dependency trees. dependency tree tree words vertices syntactic
relations dependency relations. vertex therefore single parent, except
root tree. dependency relation holds dependent, i.e. syntactically
subordinate vertex, head, i.e. another vertex dependent. Thus
dependency structure represented head-dependent relation vertices
classified dependency types SBJ subject, OBJ object, ATT attribute, etc.
carried number experiments state-of-the-art taggers
AMIRA (Diab, 2009), MADA (Habash, Rambow, & Roth, 2009) in-house maximumlikelihood (MXL) tagger (Ramsay & Sabtan, 2009) parsers MALTParser (Nivre,
Hall, Nilsson, Chanev, Eryigit, Kbler, Marinov, & Marsi, 2007) MSTParser (McDonald, Lerman, & Pereira, 2006).7 experiments show particular merging MADA
(97% accuracy) MSTParser gives better results (around 81% labelled accuracy)
tagger:parser combinations (Alabbas & Ramsay, 2012b). therefore use
MADA+MSTParser current experiments.
converting p-h pairs dependency trees, matched dependency trees using
ZS-TED TED+ST algorithms, two string-based algorithms (bag-of-words
Levenshtein distance) provide baseline. tree edit distance algorithms used edit
operation costs defined Figure 8 find cost matching p-h pairs.
bag-of-words measures similarity p h number common words
(either surface forms lemma forms), divided length h.
four algorithms use AWN lexical resource order take account synonymy
hyponymy relations calculating cost edit.
carried two kinds experiments using algorithms: first simple
yes/no experiment, using single threshold decide whether premise similar enough
hypothesis safe say entailed it, second two thresholds
could say yes/dont know/no. results experiments given below.

7. parsers data-driven dependency parsers. Arabic usually trained Arabic
dependency treebank, Prague Arabic Dependency Treebank (PADT) (Smr, Bielicky, Kouilov,
Krmar, Haji, & Zemnek, 2008), version Penn Arabic Treebank (PATB) (Maamouri
& Bies, 2004) converted dependency trees: scoring parsers matter counting
dependency links.

16

fiNatural Language Inference Arabic

6.1 Binary Decision (yes no)
p entails h cost matching less (more case bag-of-words) threshold.
results experiments, terms precision (P), recall (R) F-score (F)
yes class overall accuracy, shown Table 2. table shows substantial
improvement obtained using TED+ST bag-of-words (F-score TED+ST
around 1.16 times F-score bag-of-words, accuracy 1.09 times better)
ZS-TED (around 1.06 times better F-score 1.04 times better total accuracy).
Method
Bag-of-words
Levenshtein distance
ZS-TED
TED+ST

Pyes
63.5%
64.7%
65.9%
69.7%

Ryes
43.7%
44.1%
51.2%
54.5%

Fyes
0.518
0.525
0.576
0.612

Accuracy
59.3%
60.2%
62.5%
65.5%

Table 2: Performance TED+ST compared string-based algorithms ZS-TED,
binary decision.
Although primarily interested Arabic, carried parallel sets experiments English RTE2 testset, using Princeton English WordNet (PWN)
resource deciding whether word premise may exchanged one hypothesis. tree edit distance algorithms work dependency tree analyses input texts, used set analysed using Minipar (Lin, 1998), downloaded
http://u.cs.biu.ac.il/~nlp/RTE2 Datasets/RTE2 Preprocessed Datasets.html.
RTE2 testset contains around 800 p-h pairs, number Minipar analyses
multiple heads hence correspond well-formed trees, also
number cases segmentation algorithm used produces multi-word expressions. eliminating problematic pairs kind left 730 pairs, split
evenly positive negative examples. Since mainly concerned
difference ZS-TED TED+ST, omitted Levenshtein distance
simply kept basic bag-of-words algorithm baseline. Previous authors
shown tree edit distance consistently outperforms string-based approaches
dataset, need replicate result here.
Method
Bag-of-words
ZS-TED
TED+ST

Pyes
53.2%
52.9%
53.2%

Ryes
50.1%
62.5%
66.8%

Fyes
0.516
0.573
0.59

Accuracy
52.1%
53.5%
55.8%

Table 3: Performance TED+ST compared simple bag-of-words ZS-TED,
binary decision, RTE2 dataset.
pattern Table 3 similar Table 2. ZS-TED better bag-of-words,
TED+ST improvement ZS-TED. experiments textual entailment
tasks report accuracy: certain situations may important decisions
trustworthy (high precision, Table 2) sure captured
17

fiAlabbas & Ramsay

many positive examples possible (high recall8 , Table 3), good balance
(high F-score). easy change balance precision recall,
simply changing threshold used determining whether safe say
p entails hwe could chosen thresholds Table 3 increased precision
decreased recall, results closely matched Table 2. key point
sets experiments, F-scores improve move string-based
measures ZS-TED use TED+ST; remarkably
similar two datasets, despite fact collected different means,
different languages, parsed using different parsers.
6.2 Making Three-way Decision (yes, unknown)
task use two thresholds, one trigger positive answer cost matching
lower lower threshold (exceeds higher one bag-of-words algorithm)
trigger negative answer cost matching exceeds higher one (mutatis
mutandis bag-of-words). Otherwise, result unknown. reason making
three-way decision drive systems make precise distinctions. Note
distinguishing {h entails p, h p compatible, h contradicts p},
{h entails p, dont know whether h entails p, h entail p}.
subtle distinction, reflecting systems confidence judgement,
extremely useful deciding act decision.
results experiment, terms precision (P), recall (R) F-score (F),
shown Table 4. Again, shows large improvement using TED+ST
bag-of-words (F-score around 1.10 times better) ZS-TED (F-score around 1.06 times
better).
Method
Bag-of-words
Levenshtein distance
ZS-TED
TED+ST

P
58.9%
61.4%
65.1%
67.4%

R
56.7%
58.0%
56.0%
60.2%

F
0.578
0.597
0.602
0.636

Table 4: Performance TED+ST compared string-based algorithms ZS-TED,
three-way decision.
scores three-way decision RTE2 dataset lower Arabic
dataset, TED+ST outperforms ZS-TED three measures.

7. Conclusion
presented extended version, TED+ST, tree edit distance solved
one main drawbacks standard tree edit distance, supports
8. might useful, instance, TED+ST used low cost filter question-answering
system, results query search engine might filtered TED+ST passed
system employing full semantic analysis deep reasoning, high precision also
time-consuming.

18

fiNatural Language Inference Arabic

Method
Bag-of-words
ZS-TED
TED+ST

P
50.8%
52.3%
54.3%

R
48.3%
50.2%
52.7%

F
0.495
0.512
0.535

Table 5: Performance TED+ST compared simple bag-of-word ZS-TED,
three-way decision, RTE2 dataset.

edit operations (i.e. delete, insert exchange) single nodes. TED+ST deals
subtree transformation operations well operations single nodes: leads useful
improvements performance standard algorithm determining entailment.
key subtrees tend correspond single information units. treating
operations subtrees less costly corresponding set individual node operations,
TED+ST concentrates entire information units, appropriate granularity
individual words considering entailment relations.
current findings, preliminary, quite encouraging. fact results
original testset, particularly improvement F-score, replicated testset
control parser used produce dependency trees
p-h pairs provides evidence robustness approach. anticipate
cases accurate parser (our parser Arabic attains around 81%
accuracy PATB, Minipar reported attain 80% Suzanne corpus)
would improve performance ZS-TED TED+ST.
currently experimenting different scoring algorithms ZS-TED TED+
ST. performance variant tree edit distance depends critically costs
various operations, thresholds used deciding whether h entails
p, therefore investigating use various optimisation algorithms choosing
weights thresholds. also intend use Arabic lexical resources,
OpenOffice Arabic dictionary MS Word Arabic dictionary, provide us
information relations words, information AWN,
useful, sparse comparison PWN (Habash, 2010).

Acknowledgments
would like thank reviewers valuable comments, particular reviewer
suggested evaluating approach English dataset well Arabic one.
extra work provided support belief robustness approach
degree anticipate.
would like extend thanks annotators time effort
put annotating experimental dataset. Maytham Alabbas owes deepest gratitude
Iraqi Ministry Higher Education Scientific Research financial support
PhD study. Allan Ramsays contribution work partially supported Qatar
National Research Fund (grant NPRP 09 - 046 - 6 - 001).
19

fiAlabbas & Ramsay

References
Alabbas, M. (2011). ArbTE: Arabic textual entailment. Proceedings Second Student
Research Workshop associated RANLP 2011, pp. 4853, Hissar, Bulgaria. RANLP
2011 Organising Committee.
Alabbas, M., & Ramsay, A. (2011a). Evaluation combining data-driven dependency
parsers Arabic. Proceeding 5th Language & Technology Conference: Human
Language Technologies (LTC11), pp. 546550, Pozna, Poland.
Alabbas, M., & Ramsay, A. (2011b). Evaluation dependency parsers long Arabic
sentences. Proceeding International Conference Semantic Technology
Information Retrieval (STAIR11), pp. 243248, Putrajaya, Malaysia. IEEE.
Alabbas, M., & Ramsay, A. (2012a). Arabic treebank: phrase-structure trees dependency trees. META-RESEARCH Workshop Advanced Treebanking 8th
International Conference Language Resources Evaluation (LREC), pp. 6168,
Istanbul, Turkey.
Alabbas, M., & Ramsay, A. (2012b). Combining black-box taggers parsers modern standard Arabic. Federated Conference Computer Science Information
Systems (FedCSIS-2012), pp. 19 26, Wroclaw, Poland. IEEE.
Alabbas, M., & Ramsay, A. (2012c). Improved POS-tagging Arabic combining diverse
taggers. Proceedings 8th Artificial Intelligence Applications Innovations
(AIAI), pp. 107116, Halkidiki, Greece. Springer.
Bille, P. (2005). survey tree edit distance related problems. Theoretical Computer
Science, 337 (1-3), 217239.
Black, W., Elkateb, S., Rodriguez, H., Alkhalifa, M., Vossen, P., Pease, A., & Fellbaum, C.
(2006). Introducing Arabic WordNet project. Proceedings 3rd International WordNet Conference (GWC-06), pp. 295299, Jeju Island, Korea.
Burger, J., & Ferro, L. (2005). Generating entailment corpus news headlines.
Proceedings ACL Workshop Empirical Modeling Semantic Equivalence
Entailment, pp. 4954, Ann Arbor, Michigan, USA. Association Computational
Linguistics.
Dagan, I., & Glickman, O. (2004). Probabilistic textual entailment: generic applied modeling language variability. PASCAL Workshop Learning Methods Text
Understanding Mining, pp. 2629, Grenoble, France.
Dagan, I., Glickman, O., & Magnini, B. (2006). PASCAL recognising textual entailment challenge. Quionero-Candela, J., Dagan, I., Magnini, B., & dAlch Buc, F.
(Eds.), Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, Recognising Tectual Entailment, Vol. 3944 Lecture Notes
Computer Science, pp. 177190. Springer Berlin, Heidelberg.
Demaine, E., Mozes, S., Rossman, B., & Weimann, O. (2009). optimal decomposition
algorithm tree edit distance. ACM Transactions Algorithms (TALG), 6 (1),
2:12:19.
20

fiNatural Language Inference Arabic

Diab, M. (2009). Second generation tools (AMIRA 2.0): fast robust tokenization, POS
tagging, base phrase chunking. Proceedings 2nd International Conference
Arabic Language Resources Tools, pp. 285288, Cairo, Eygpt. MEDAR
Consortium.
Habash, N. (2010). Introduction Arabic Natural Language Processing. Synthesis Lectures
Human Language Technologies. Morgan & Claypool Publishers.
Habash, N., Rambow, O., & Roth, R. (2009). MADA+TOKAN: toolkit Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming
lemmatization. Proceedings 2nd International Conference Arabic Language
Resources Tools, Cairo, Eygpt. MEDAR Consortium.
Habash, N., Soudi, A., & Buckwalter, T. (2007). Arabic transliteration. Arabic Computational Morphology, 1522.
Heilman, M., & Smith, N. (2010). Tree edit models recognizing textual entailments, paraphrases, answers questions. Human Language Technologies: 2010 Annual
Conference North American Chapter Association Computational Linguistics, pp. 10111019, Los Angeles, California, USA. Association Computational
Linguistics.
Hobbs, J. R. (2005). handbook pragmatics, chap. Abduction Natural Language
Understanding, pp. 724740. Blackwell Publishing.
Klein, P. (1998). Computing edit-distance unrooted ordered trees. Proceedings 6th Annual European Symposium Algorithms (ESA 98), pp. 91102,
Venice, Italy. Springer-Verlag.
Kouylekov, M. (2006). Recognizing Textual Entailment Tree Edit Distance: application
Question Answering Information Extraction. Ph.D. thesis, DIT, University
Trento, Italy.
Kouylekov, M., & Magnini, B. (2005). Recognizing textual entailment tree edit distance algorithms. Proceedings the1st Challenge Workshop Recognising Textual
Entailment, pp. 1720, Southampton, UK.
Lin, D. (1998). Dependency-based evaluation minipar. Workshop Evaluation
Parsing systems, pp. 317330. Springer.
Maamouri, M., & Bies, A. (2004). Developing Arabic treebank: methods, guidelines,
procedures, tools. Proceedings Workshop Computational Approaches
Arabic Script-based Languages, pp. 29, Geneva, Switzerland.
MacCartney, B. (2009). Natural Language Inference. Ph.D. thesis, Department Computer
Science, Stanford University, USA.
McDonald, R., Lerman, K., & Pereira, F. (2006). Multilingual dependency parsing
two-stage discriminative parser. 10th Conference Computational Natural Language Learning (CoNLL-X), New York, USA.
Mehdad, Y., & Magnini, B. (2009). Optimizing textual entailment recognition using particle
swarm optimization. Proceedings 2009 Workshop Applied Textual Inference (TextInfer 09), pp. 3643, Suntec, Singapore. Association Computational
Linguistics.
21

fiAlabbas & Ramsay

Nivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit, G., Kbler, S., Marinov, S., & Marsi,
E. (2007). MaltParser: language-independent system data-driven dependency
parsing. Natural Language Engineering, 13 (02), 95135.
Pawlik, M., & Augsten, N. (2011). RTED: robust algorithm tree edit distance.
Proceedings VLDB Endowment, 5 (4), 334345.
Punyakanok, V., Roth, D., & Yih, W. (2004). Natural language inference via dependency
tree mapping: application question answering. Computational Linguistics, 6,
110.
Ramsay, A., & Sabtan, Y. (2009). Bootstrapping lexicon-free tagger Arabic. Proceedings 9th Conference Language Engineering (ESOLEC2009), pp. 202215,
Cairo, Egypt.
Selkow, S. (1977). tree-to-tree editing problem. Information Processing Letters, 6 (6),
184186.
Smr, O., Bielicky, V., Kouilov, I., Krmar, J., Haji, J., & Zemnek, P. (2008). Prague
Arabic dependency treebank: word million words. Proceedings Workshop Arabic Local Languages (LREC 2008), pp. 1623, Marrakech, Morocco.
Tai, K. (1979). tree-to-tree correction problem. Journal ACM (JACM), 26 (3),
422433.
Zhang, K., & Shasha, D. (1989). Simple fast algorithms editing distance
trees related problems. SIAM Journal Computing, 18 (6), 12451262.

22

fiJournal Artificial Intelligence Research 48 (2013) 813-839

Submitted 05/13; published 11/13

Single Network Relational Transductive Learning
Amit Dhurandhar
Jun Wang

adhuran@us.ibm.com
wangjun@us.ibm.com

IBM T.J. Watson Research
1101 Kitchawan Road, Yorktown Heights, NY-10598 USA

Abstract
Relational classification single connected network particular interest
machine learning data mining communities last decade so.
mainly due explosion popularity social networking sites Facebook,
LinkedIn Google+ amongst others. statistical relational learning, many techniques
developed address problem, connected unweighted homogeneous/heterogeneous graph partially labeled goal propagate
labels unlabeled nodes. paper, provide different perspective enabling
effective use graph transduction techniques problem. thus exploit
strengths class methods relational learning problems. accomplish
providing simple procedure constructing weight matrix serves input rich
class graph transduction techniques. procedure multiple desirable properties.
example, weights assigns edges unlabeled nodes naturally relate
measure association commonly used statistics, namely Gamma test statistic.
portray efficacy approach synthetic well real data, comparing
state-of-the-art relational learning algorithms, graph transduction techniques
adjacency matrix real valued weight matrix computed using available attributes input. experiments see approach consistently outperforms
approaches graph sparsely labeled, remains competitive
best proportion known labels increases.

1. Introduction
Given affluence large connected relational graphs across diverse domains, single
within network classification one popular endeavours statistical relational
learning (SRL) research (Getoor & Taskar, 2007). Ranging social networking websites
movie databases citation networks, large connected relational graphs1 banal.
single network classification, partially labeled data graph goal extend
labeling, accurately possible, unlabeled nodes. nodes may
may associated attributes. example within network classification
could useful forming common interest groups social networking websites.
instance, group people geography may interested playing soccer
would interested finding people likely interest.
different domain entertainment, one might interested estimating
new movies likely make splash box office. Based success
movies actors and/or director, one could provide
reasonable estimate movies likely successful.
1. least large connected component.
c
2013
AI Access Foundation. rights reserved.

fiDhurandhar & Wang

Many methods learn infer data graph developed SRL literature. effective methods perform collective classification (Chakrabarti,
Dom, & Indyk, 1998), is, besides using attributes unlabeled node infer
label, also use attributes labels related nodes/entities. thus generalization methods assume data independently identically distributed
(i.i.d.). Examples methods relational markov networks (RMNs) (Taskar, Abbeel,
& Koller, 2002), relational dependency networks (RDNs) (Neville & Jensen, 2007), markov
logic networks (MLNs) (Richardson & Domingos, 2006), probabilistic relational models
(PRMs) (Getoor, Koller, & Small, 2004). fall umbrella markov networks. simpler models suggested baselines relational neighbor
classifiers (RN) (Macskassy & Provost, 2003, 2007; Chakrabarti et al., 1998; Sen, Namata,
Bilgic, Getoor, Gallagher, & Eliassi-Rad, 2008), simply choose numerous
class label amongst neighbors involved variants using relaxation
labeling. Interestingly, simple models perform quite well auto-correlation
high, even though graph maybe sparsely labeled. Recently, pseudo-likelihood expectation maximization (PL-EM) (Xiang & Neville, 2008) method introduced, seems
perform favorably methods graph moderate number (around
20-30%) labeled nodes.
different class methods could potentially address problem hand
graph transduction methods (Zhu, Ghahramani, & Lafferty, 2003; Zhou, Bousquet, Lal,
Weston, & Schlkopf, 2004; Wang, Jebara, & fu Chang, 2008), part semisupervised learning methods. methods typically perform well given
weighted graph linked nodes mostly labels unless apriori dissimilar
nodes explicitly specified (Goldberg, Zhu, & Wright, 2007; Tong & Jin, 2007) , even
small fraction labels known. weighted graph readily available,
constructed (explanatory) attributes nodes. unweighted graph
attributes given, adjacency matrix passed input.
multiple methods learn weights based attributes. simplest
use lp norm. sophisticated techniques use specialized optimization functions based
gaussian kernels (Jebara, Wang, & Chang, 2009; Jebara & Shchogolev, 2006) loglikelihood (Xiang, Neville, & Rogati, 2010) determine weights. methods however,
unsupervised (i.e. ignore labels) based fundamental assumption
homophily, is, linked closeby datapoints labels. assumption
necessarily satisfied real-world relational graphs.
methods, determine weights based linkage (Gallagher,
Tong, Eliassi-Rad, & Faloutsos, 2008). Here, besides edges input graph, edges
added labeled unlabeled nodes. weights determined making
multiple even length random walks starting unlabeled node. strategy works
well binary labeled graphs may may satisfy homophily assumption,
necessarily effective two labels. Moreover, method
still unsupervised computationally expensive.
literature concerning methods learn weights given unweighted
graph, seen appropriate weighting scheme help leverage graph semantics
make prediction algorithms robust noise compared unweighted counterparts. fact, well recognized (Maier, Von Luxburg, & Hein, 2008; Jebara
814

fiSingle Network Relational Transductive Learning

et al., 2009; Zhu, Lafferty, & Rosenfeld, 2005) accurate edge weighting significant
influence various graph based machine learning tasks clustering classification. Moreover, experiments see using unweighted graph leads
substantially inferior results cases, opposed using weighting scheme.
comparing scheme methods learn weights, preferable
least following reasons. First, method uses attribute information, link
information alongwith labels determine weights. methods use either
link information attribute information tend ignore specific labeling.
Hence, method comprehensive thus robust takes account
three sources information. Second, method used data heterogenous
limited homogenous datasets. Third, see experiments,
method performs well across varied labeling percentages, methods
tend higher bias able fully exploit settings label
information available.
relational learning, graphs typically unweighted sometimes may
attributes. many cases, attributes may accurately predict labels,
case, weighting edges solely may provide acceptable results. links
labeling could viewed additional source information predict unlabeled
nodes. intuitions captured relational gaussian process model (Chu,
Sindhwani, Ghahramani, & Keerthi, 2007), limited undirected graphs
mentioned prior works (Xiang & Neville, 2008) suggested kernel function easy
adapt relational settings may heterogeneous data.
paper, provide lucid way effectively leverage rich class graph transduction methods, namely based graph laplacian regularization framework,
within network relational classification. Among existing graph transduction methods,
class methods considered one efficient accurate real applications (Jebara et al., 2009; Zhu et al., 2003; Zhou et al., 2004). particular, provide
procedure learn weight matrix graph may directed undirected,
may exhibit positive negative auto-correlation edges graph may
labeled nodes, unlabeled nodes labeled unlabeled
node. method semi-supervised sense uses label information well
links attribute information determine weights. semi-supervised,
learned weights robust effectively capture dependencies much
unsupervised weight learning methods described above. Moreover, learning procedure
naturally relates commonly used statistical measures making principled previous approaches. first provide solution graph nodes attributes,
class labels. extend solution include attributes (and heterogenous data)
incorporating conical weighting scheme weighs importance links relative
attributes. construction weight matrix assumes binary labeling, however,
recursive application chosen graph transduction method reconstruction
weight matrix accomplish multi-class classification witnessed experiments
real data.
rest paper organized follows: Section 2, describe construction
weight matrix nodes labels attributes. Section 3,
discuss interesting characteristics weighting scheme. Section 4, extend
815

fiDhurandhar & Wang

-1

?

?

1

1
Figure 1: Example input graph (T ) construction method.
construction described section 2 able model attributes data heterogeneity.
Section 5, suggest modification graph transduction methods
Figure 1
effectively exploit richness input weight matrix. Section 6, show
efficacy ideas experiments synthetic real data. Section 7, discuss
promising future directions summarize contributions made current work.

2. Weight Matrix Construction
section elucidate way constructing weight matrix partially labeled
graph G(V, E, ) V set nodes, E set edges set labels.
assume labeling binary, i.e labeled node label Yi {1, 1}.
mentioned before, procedure constructing weight matrix W , serves
input graph transduction technique, could applied recursively iteratively
(binary) classified portion, attain multi-class classification. Hence, input run
weight matrix construction method partially (binary) labeled graph shown
Figure 1.
weights Wij learn edge node node j signify degree
similarity/dissimilarity labels nodes. weights lie interval
[1, 1], positive sign indicates nodes tend similar labels,
negative sign indicates tend different labels. numerical value
ignoring sign indicates magnitude tendencies. Formally,
Wij = f (Yi , Yj , G)

(1)

Yi Yj maybe known unknown f () [1, 1]. case, value f ()
depends type nodes edge connecting, i.e., nodes labeled, unlabeled
one labeled unlabeled, along labeled portion graph.
exact assignments f () edges connecting 3 different types nodes given
subsection 2.2. Loosely speaking, f () would negative close -1 edges
connect nodes different labels, would positive close +1
edges connect nodes label. semantics consistent even
discuss extensions later paper, involve learning weights presence
attributes heterogenous data.
816

fiSingle Network Relational Transductive Learning

Symbol
Nq
Nqr

Graph Type
D, U


Nqr

U

Np

D, U

Psame

D, U

Popp

D, U



D, U

Semantics
# nodes label q
# edges node
label q node
label r
q = r,
# edges node
label q node
label r
q 6= r,
Half # edges
node label
q node label r
Total # labeled edges
i.e. edges
nodes labeled
Ratio # edges
nodes
label total #
labeled edges
Ratio # edges
nodes
different labels total
# labeled edges
Distribution labeled
edges

Table 1: notation used paper. graph type, stands directed
U stands undirected.

Given setup, partially labeled graph G 3 types nodes consequently 9
types edges directed graph 6 types edges undirected one. node
could labeled 1 1 may unlabeled. edge could two nodes
label (i.e. (1 1) (1 1)) two oppositely labeled nodes (i.e.
(1 1) (1 1)) labeled unlabeled node (i.e. (1 ?) (1 ?)
(? 1) (? 1)) two unlabeled nodes (i.e. (? ?)). undirected
example graph shown Figure 1. task assign weights
types edges.
817

fiDhurandhar & Wang

-1

?

?

1

1
Figure 2: Tw weighted version graph shown figure 1.
2.1 Notation

Figureto2the different types edges, introduce
describe weights assign
notation. Given graph G, let Nq denote number nodes label q. Let Nqr denote
number edges node label q node label r. undirected graph,
would number edges nodes labeled q r, q = r. q 6= r,
Nqr would half number edges q r. Let Np denote total number
labeled edges, i.e. total number edges nodes labeled.
words, Np = N11 + N11 + N11 + N11 . let,
Psame =

N11 + N11
N11 + N11
, Popp =
Np
Np

(2)

Hence, Psame + Popp = 1. denote empirical distribution derived labeled edges
D. summary notation directed undirected graphs shown table 1.
2.2 Assignment Weights
describe weight matrix construction applies directed undirected graphs. partition types edges 5 categories suggest way
assigning weights edges categories.
Edges nodes label: edge nodes
label, node node j label, assign weight Wij = Psame
edge. makes intuitive sense since want weigh edge based
likely nodes label connected.
worth mentioning one might think assigning label specific weights edges.
instance, one strategy would assign Wij = NN11
Wij = N11
depending
Np
p
labels +1 1 respectively. However, assignment seems
conceptual issue even simple case, graph connected nodes
mostly labels. case, weights edges connecting
nodes labels would devalued undesirably. example consider
graph where, N11 = 10, N11 = 10 N11 = 1. basically graph two
large clusters connected nodes labels one link connecting two
818

fiSingle Network Relational Transductive Learning

clusters. case, label specific strategy would give weights Wij = 10
21 0.48,
strategy would give weights Wij = 20

0.95


edges

nodes
21
labels. clear latter strategy preferable since, either
labels high tendency connected nodes labels. Note
reasoning also applies N11 might different N11 ,
graph exhibits high positive auto-correlation.
Edges nodes opposite/different labels: edge nodes
opposite labels, node node j different labels, assign weight
Wij = Popp edge. also intuitive since, want weigh edge based
likely nodes opposite labels connected. assign negative
sign since simply assigning magnitude create distinction nodes
labeled alike different labels.
Edges unlabeled nodes: edge unlabeled nodes, node
node j labels, assign weight Wij = ED [Yi , Yj ] edge.
ED [Yi , Yj ] denotes expectation labeled edges distribution D. Yi
Yj {1, 1} hence,

ED [Yi , Yj ] =

X

qrP [Yi = q, Yj = r]

q,r{1,1}

= P [Yi = 1, Yj = 1] P [Yi = 1, Yj = 1]+
P [Yi = 1, Yj = 1] P [Yi = 1, Yj = 1]
N11 N11 N11 N11
=

+

Np
Np
Np
Np

(3)

Since know labels nodes edges category,
assign unbiased estimate indicated expected value.
Edges unlabeled node node label 1: edge
unlabeled node node label 1, assign weight Wij = ED [Yi |Yj = 1]
edge. Yi {1, 1}. case,
ED [Yi |Yj = 1] =

N11 N11 + N11

N1
N1

(4)

unbiased estimate given one nodes label 1.
Edges unlabeled node node label 1: edge
unlabeled node node label 1, assign weight Wij = ED [Yi |Yj = 1]
edge. Yi {1, 1}. case,
ED [Yi |Yj = 1] =

N11 N11 + N11

N1
N1

unbiased estimate given one nodes label 1.
819

(5)

fiDhurandhar & Wang

weighted version example graph Figure 1, shown graph Tw Figure
2.

3. Characteristics Matrix Construction
previous section, elucidated way constructing weight matrix partially
labeled graph. section, discuss certain characteristics construction.
discuss aspects relationships suggested weights standard statistical measures tendencies weight matrix function connectivity labeling
graph. see, construction seems desirable properties.
3.1 Relation Standard Measures Association
previous section, described provided brief justification procedure
assign weights. turns weights assign edges least one
unlabeled node, besides unbiased, (statistical) semantics.
Remark: weights assigned edges unlabeled nodes equate gamma
test statistic () relational setting i.e. ED [Yi , Yj ] = .
gamma test statistic (Goodman & Kruskal, 1954), standard measure association used statistics. value statistic ranges [1, 1], positive
values indicate agreement, negative values indicate disagreement/inversion zero indicates absence association. statistic historically used compare sorted order
observations based values two attributes. However, also used measure
auto-correlation relational data graphs. Hence, assignment weight edges unlabeled nodes auto-correlation graph, makes intuitive sense.
turns out, statistic also interesting relationship Student distribution
(Goodman & Kruskal, 1954).
weights assigned edges one labeled one unlabeled node i.e. ED [Yi |Yj =
1] ED [Yi |Yj = 1], based equations 4 5 written as: (Psame |1)(Popp |1) = 1
(Psame | 1) (Popp | 1) = 1 . could considered gamma test statistics
conditioned one particular type label could referred conditional gamma
test statistics.
3.2 Behavior Weight matrix
analyze behavior weight matrix labeled edges input graph
tend towards connecting nodes labels analogously connecting
nodes different labels.
input graph tends nodes labels connected,
following effect weight matrix. weight edges nodes
label tends one, i.e. Psame 1. weight edges nodes different labels
tends zero, i.e. Popp 0. weight edges unlabeled nodes tends 1, i.e.
1. weight remaining set edges also tends one, i.e. 1 , 1 1. Hence,
situation weight matrix becomes adjacency matrix extreme case,
different labeled edges vanishing (i.e. weighted 0) edges getting weight
820

fiSingle Network Relational Transductive Learning

-1

?

?

1

1
Figure 3: Ts instantiation graph Tw , labeled edges nodes
labels.

-1
Figure 3

?

?

1

1
Figure 4: instantiation graph Tw , labeled edges nodes
different labels.

Figure
4 graph Figure 2 becomes graph
one. Consequently, example
weighted
w

Figure 3.
input graph tends nodes different labels connected,
following effect weight matrix. weight edges nodes
label tends zero, i.e. Psame 0. weight edges nodes
different labels tends -1, i.e. Popp 1. weight edges unlabeled nodes
tends -1, i.e. 1. weight remaining set edges also tends -1, i.e.
1 , 1 1. Since graph extreme case positive weights, negative
sign weights superfluous terms graph structure eliminated. Hence,
situation weight matrix becomes adjacency matrix extreme case,
labeled edges vanishing (i.e. weighted 0) edges getting
weight one. Consequently, example weighted graph Tw Figure 2 becomes graph
Figure 4.
thus Ts = , labeled edges Ts complement
labeled portion respect base graph . intuitively expect labeled
edges differently labeled nodes slowly disappear edges remain
821

fiDhurandhar & Wang

Paper

Paper

Author

Paper
Author

Title

Area

Age

Paper Title
Author

a)

Paper

b)

Figure 5: a) represents relational schema node types, Paper Author. relationship many-to-many. rounded boxes linked node
types denote respective attributes. b) corresponding data graph,
shows authors linked papers authored co-authored.

present, edges connecting nodes label become predominant. also
expect analogous behavior diametric case. seen, intuitions
captured implicitly, modeling weight matrix, thus making construction
procedure acceptable.

4. Extensions
previous sections, described procedure constructing weight matrix
partially labeled graph attributes. section, extend weighting
scheme include attribute information. Moreover, also present solution handle
data heterogeneity using ideas relational learning.
4.1 Modeling Attributes
data graphs attributes, want able leverage information
addition information learned connectivity graph, possibly
improve performance procedure. particular, need extend
weight assignment procedure able encapsulate attribute information. simple way
combining already modeled connectivity information attributes, assign
weight edge conical combination weight based connectivity
weight based affinity attribute values connected nodes. Hence, wc
weight assigned based connectivity particular edge type wa
weight assigned based attributes, wc + wa new weight edge,
, 0. wc essentially weight assignment described section 2, viz. Psame etc.
wa function attributes nodes connected corresponding edge,
soon define. parameters determined standard
model selection techniques cross-validation. reasonable indicator value
could absolute value auto-correlation graph. reasonable
estimate value could absolute value cross-correlation wa
labeling corresponding nodes i.e. labels different.
822

fiSingle Network Relational Transductive Learning

Figure 6: figure shows transformed data graph Paper node type,
obtained data graph Figure 5b.

absence attributes, weight assignment wc type edge, value
interval [1, 1]. effectively combine aforementioned two sources information,
wa needs scale wc . One obvious choice could cosine similarity
commonly used text analytics (Belkin, Niyogi, & Sindhwani, 2005). Cosine similarity
lies [1, 1], values close 1 imply nodes similar values close
1 imply nodes dissimilar. choices could kernel functions (K)
gaussian kernel (Wang et al., 2008), normalize popular distance metrics
euclidean distance lp norms value [0, 1]. Here, values close 1 imply
similarity values close 0 imply dissimilarity. range easily transformed
usual range [-1,1] semantics before, simple linear transformation
form, 2K 1.
4.2 Modeling Heterogeneous Data
data graph multiple types entities, resulting different types nodes,
procedure previously described cannot directly applied construct weight matrix.
cases, standard relational learning strategies collapsing portions graph
using aggregation applied reduce graph single type node
attributes (Getoor & Taskar, 2007; Dhurandhar & Dobra, 2012). new graph
extended procedure applied.
823

fiDhurandhar & Wang

instance, citation graph may authors linked papers, papers
multiple authors vice-versa. example shown Figure 5. Figure
5a, see node type Paper two attributes, Title Area, denote
title paper research area belongs respectively. Let attribute Area
class label, i.e. want classify papers based research area. node
type Author attributes Paper Title Age, relates particular paper
ages authors wrote it. Title attribute (a primary key) Paper
Paper Title attribute (a foreign key) Author. Hence, Paper node three
attributes namely; Title, Area Age. attributes Title Area called intrinsic
attributes belong node type Paper attribute Age called relational
attribute since belongs different linked node type Author. paper
variable number authors thus paper would associated multiple values
Age. popular solution problem aggregate values attribute Age
Author single value paper associated single Age value.
aggregation function average ages related authors paper
used. instead Age attribute introduce new attribute AvgAge
denotes average age. attributes Paper node are; Title, Area
AvgAge. Linking papers co-authored author, data graph
links Paper node type, node two attributes class label
shown Figure 6.
see example weight assignment transformed data graph
extended method. Let us assume Paper 1 Paper 2 area AI encoded
1 Paper 3 systems encoded 1. Let average age corresponding
three papers (i.e. Paper 1, Paper 2 Paper 3) 30, 30.5 (average two authors)
31 respectively. Let ascii value titles 10, 11 15. Also let = 0.1
||xi xj ||2

= 0.5. use gaussian radial basis kernel K = e 22
= 1 compute wa ,
weights two edges W12 W23 follows:

W12 = Psame + (2e

(3030.5)2 +(1011)2
2

1)

= 0.1 0.5 + 0.5 0.071
= 0.086
W23 = (Popp ) + (2e

(30.535)2 +(1113)2
2

1)

= 0.1 0.5 + 0.5 1
= 0.55
weights would passed enhanced graph transduction framework
describe next. important note heterogeneous link types,
described procedures applied independently graphs formed
link type final result could obtained aggregating individual decisions
standard ensemble label consolidation techniques taking majority vote
weighted majority based corresponding auto-correlations.
824

fiSingle Network Relational Transductive Learning

5. Enhancing Graph Transduction Techniques
graph laplacian regularization based framework one efficient popular
frameworks semi-supervised learning practical machine learning systems. shown
effectiveness many applications, including challenging cases agnostics settings
(Jebara et al., 2009). particular, graph based transductive learning approaches impose
trade fitting accuracy prediction function labeled data
smoothness function graph. Typically, smoothness measure
prediction function f graph G calculated (Zhou et al., 2004):
XX
kf k2G =
Wij kf (xi ) f (xj )k2


j

1
1
= f (X)> (D W )f (X) = f (X)> Lf (X),
2
2

(6)

Wij weight edge nodes xi xP
j , X input matrix denoting
nodes, f (xi ) label node xi , = {Dii }, Dii = j Wij diagonal matrix
f (X) = [f (x1 ), , f (xn )]> . quantity L called graph Laplacian,
viewed operator space functions f (Chung, 1997).
Given measure function smoothness, graph laplacian based regularization
framework estimates unknown function f follows:
f opt = arg min Q(Xl , Yl , f ) + kf k2G

(7)

Q(Xl , Yl , f ) loss function measuring accuracy labeled set (Xl , Yl ).
example, Q(Xl , Yl , f ) = kf (Xl ) Yl k2 i.e. squared loss, popular choice (Belkin,
Niyogi, & Sindhwani, 2006; Zhou et al., 2004).
Note graph regularization framework directly applied prediction
modeling relational networks directly. because, smoothness measure using
graph laplacian based assumption connected nodes tend
class labels hence weights non-negative (i.e. Wij 0 i, j). However,
well-known edges relational networks could connect type nodes, described
earlier. typical example observed WEBKB dataset (Craven, DiPasquo,
Freitag, McCallum, Mitchell, Nigam, & Slattery, 1998), faculty nodes mostly
linked student nodes instead type nodes, i.e. faculty nodes. Although
recent work modeled dissimilarity incorporated similarity measure
derive called mix graph based prediction models (Tong & Jin, 2007; Goldberg et al.,
2007), assumed similarity/dissimilary relations apriori known. However,
case automatically estimate positive negative correlations given
link attribute information partial labeling.
indicated Section 2, derive weighted graph containing positive weighted
negative weighted edges. ensure compatibility graph Laplacian based
regularization framework, modify smoothness term (Goldberg et al., 2007) using
derived relational edges follows,
XX
kf k2G =
Wij kf (xi ) sgn(Wij )f (xj )k2 ,
(8)


j

825

fiDhurandhar & Wang

set Wij = |Wij | degree matrix = {Dii } computed Dii =
accordingly. positive semidefinite matrix defined as:
= (D W ) + (1 sgn(W )) W.

P

j

Dij
(9)

symbol represents Hadamard product. easy see modified smoothness
measure Eq. 9 written matrix form as,
1
(10)
kf k2G = f (X)> f (X).
2
new smoothness measure, extend existing approaches using
derived weighted graph prediction tasks.

6. Experiments
previous sections, described method construct weight matrix relational
data serves input rich class graph based transductive learning algorithms.
section, assess efficacy approach empirical studies synthetic
real data. studies, compare methods across three broad categories, namely:
a) sophisticated relational learning (RL) methods, b) sophisticated graph transduction
methods weight matrix computed using available attributes adjacency matrix
(if attributes) input (GTA) c) relational transductive methods learned
weight matrix passed input (enhanced/modified) graph transduction techniques.
situations methods category c) perform favorably methods two
categories would conditions which, using procedure would justified.
attributes available compute weights using well accepted method (Jebara et al., 2009). relational learning methods consider are: MLNs, RDNs, PL-EM
RN. learn MLNs using discriminative learning inference performed using
Markov Chain Monte Carlo (1000 runs). conditional probability distributions (CPDs)
RDNs learned using relational probability trees (RPTs), since generally
better performance relational bayesian classifiers especially number features large (Neville & Jensen, 2007)). inference performed sample obtained
performing Gibbs sampling (burn-in 100, number samples 1000) using
learned CPDs. graph transduction methods consider are: local global consistency
(LGC) method harmonic functions gaussian fields (HFGF) method. consider
methods, since well recognized robust across varied settings
comparison transduction label propagation methods (Liu & Chang, 2009),
thus considered suitable baselines (Wang, Jebara, & Chang, 2013). parameter settings use methods prior works (Zhou et al.,
2004; Zhu et al., 2003). parameters method (, ) datasets heterogenous attributes found using 10 fold cross-validation, combination
, [0, 1] varied independently steps 0.1.
experiments, vary percentage known labels training 5%
10% 30% 70%. errors methods obtained randomly
selecting (100 times) labeled nodes specified proportions followed averaging
corresponding errors. avoid clutter figures reporting results, plot
following 4 curves (rather 8),
826

fiSingle Network Relational Transductive Learning

16

49
Best RL

48

15

Best RL

14
13
12

Percentage Error

Percentage Error

47
Best GTA

HFGFW
LGCW

46

Best GTA

45
44
43

HFGFW

LGCW

42
11
41
10
0

20
40
60
Percentage Labeled

40
0

80

Figure 7: Performance methods
3 categories
graph generated using preferential attachment
auto-correlation high,
shown above.

20
40
60
Percentage Labeled

80

Figure 8: Performance methods
3 categories
graph generated using preferential attachment
auto-correlation low,
shown above.

best performance labeled percentage methods category a) (BEST
RL)2 ,
best performance labeled percentage methods category b) (BEST
GTA),
LGC method constructed weight matrix input (LGCW)
HFGF method constructed weight matrix input (HFGFW) i.e. methods category c).

6.1 Synthetic Experiments
generate graphs using well accepted random graph generation procedures create
real world graphs, namely: forest fire (FF) (Leskovec, Kleinberg, & Faloutsos, 2007)
2. percentage labeled instances 10% RL methods roughly accuracies,
though RN obviously efficient. moderate labeling i.e. 30% PL-EM usually best.
high labeling i.e. 70% RDNs best cases.

827

fiDhurandhar & Wang

16

Best RL

45

Best GTA

14

Percentage Error

Percentage Error

15

50
Best RL

13
12
11
10
9
0

HFGFW

Best GTA

40

35

LGCW

LGCW

20
40
60
Percentage Labeled

HFGFW

30
0

80

Figure 9: Performance methods
3 categories
graph generated using forest
fire auto-correlation
high, shown above.

20
40
60
Percentage Labeled

80

Figure 10: Performance methods 3 categories
graph generated using forest fire autocorrelation low, shown
above.

preferential attachment (PA) (Barabasi & Albert., 1999). procedures add one node
time nodes get added, assign label based intuitive label generation
procedure described below.
6.1.1 Setup
generate 100 graphs consisting 1000 nodes two generation techniques mentioned above. parameter settings forest fire (forward probability = 0.37, backward
probability = 0.32) preferential attachment (exponent = 1.6) derived studies
(Leskovec et al., 2007; Barabasi & Albert., 1999) indicate settings lead
realistic graphs.
labeling front, generate binary labeling {1, 1} simple procedure
graphs. Whenever new node added, probability p assign
majority class amongst labeled neighbors probability 1 p assign one
two labels uniformly random. Hence, labels generated dependent
particular graph generation procedure consequently connectivity graph,
desired. easy see p 1 auto-correlation graph increases, leading
homogeneity less entropy amongst connected nodes. two graph
828

fiSingle Network Relational Transductive Learning

Years

Publication

Publication

advisedby
taughtby

Person

Pname

Title

ta
Name

Course

Status
Id

Inphase

CT

Person




H

N

Level

Hasposition

Course
CId
L

b)

a)

Figure 11: a) represents relational schema real dataset UW-CSE types, Person, Course Publication. relationship related types
many-to-many. rounded boxes denote respective attributes. b)
corresponding model graph depicts conditional dependencies
relevant attributes three types namely; Name (N), Status (S), Inphase
(I), Hasposition (H), Concatenated Titles (CT), Concatenated course Ids (CId)
Level (L).

generation procedures, create graphs p low i.e. 0.3 p high i.e. 0.8.
low p leads auto-correlation 0.2 i.e. 0.2 high p leads
auto-correlation 0.7 i.e. 0.7, calculated generated graphs.
model graph relational methods case trivial since,
attributes hence labels unknown nodes generated based known labels
neighbors.
6.1.2 Observations
Figures 7, 8, 9 10 see given particular graph generation procedure
irrespective level auto-correlation relative performance 3 different class
methods qualitatively similar. GTAs known perform particularly well
nodes labeled (Zhou et al., 2004; Wang et al., 2008) confirmed
experiments. percentage known labels increases however, relational learning
methods start performing better standard graph transduction techniques.
probably due fact sophisticated relational learning methods low bias
relatively high variance, however, increasing number labeled nodes variance
drops rapidly.
829

fiDhurandhar & Wang

Figure 12: a) represents relational schema real dataset BREAD Store type.
rounded boxes denote respective attributes. b) corresponding
model graph depicts conditional dependencies relevant
attributes namely; Sales Target (ST), Promotions (P), Orders (O) Reclaims
(R).

interesting part though, weight matrix construction technique seems
capture enough complexity labeling network structure besides
performing exceedingly well graph sparsely labeled, remains competitive
relational learning methods percentage known labels moderate high.
6.2 Real Data Experiments
experiments real data choose three datasets, namely: UW-CSE (Richardson &
Domingos, 2006), WEBKB (Craven et al., 1998) real industrial dataset, BREAD,
obtained large consumer retail company.
6.2.1 Setup
UW-CSE dataset contains information UW computer science department.
dataset consists 442 people either students professors. dataset
information regarding course taught whom, teaching assistants
course, publication record person, phase person (i.e. prequalifier, post-qualifier), position person (i.e. faculty, affiliate faculty etc.), years
program advisor (or temporary advisor) student (advisedby links).
relational schema dataset given Figure 11a. classification task find
person Student Professor. dataset divided five parts; ai.db,
graphics.db, theory.db, language.db systems.db. run experiments part
830

fiSingle Network Relational Transductive Learning

21
70

Best RL

HFGFW

65

Best RL

60
19
18
17

Percentage Error

Percentage Error

20

Best GTA

HFGFW

LGCW

55

Best GTA

50
45

LGCW

40
35

16

30
15
0

20

40
60
Percentage Labeled

25
0

80

Figure 13: Performance methods
3 categories
UW-CSE dataset, shown
above.

20
40
60
Percentage Labeled

80

Figure 14: Multi-class transductive performance methods
3 categories
WEBKB dataset, shown
above.

report error averaged parts. model graph showing various conditional
dependencies shown Figure 11b. model graph introduce two new attributes
present relational schema namely, CT CId formed concatenating
titles papers written person concatenating Ids courses taught (or ta)
person. Year attribute eliminated since particularly discriminative.
relational methods trained based model graph, besides offcourse taking
account labels neighbors.
WEBKB dataset collection webpages obtained computer science
departments 4 US universities. webpage belongs one 7 categories namely;
course, faculty, student, staff, project, department other. category webpages
used input classification task, used link webpages
remaining 6 classes (Macskassy & Provost, 2007). performed experiments four
graphs formed one university computed average error four
universities learning methods. WEBKB, commonly used
dataset, use model graph constructed prior works (Neville & Jensen, 2007)
train relational methods.
BREAD dataset sales information bread products sold different stores
northeastern United States. dataset information 2347 stores.
831

fiDhurandhar & Wang

50
45

Best RL

Percentage Error

40
Best GTA

35
30
25
20

HFGFW
LGCW

15
10
0

20
40
60
Percentage Labeled

80

Figure 15: Performance methods
3 categories
BREAD dataset, shown
above.

store know location, know store met3 underachieved target quarterly
sales, know amounts promotion period, know quantity
ordered period know amount reclaimed period. Based
location, form graph linking closest stores together. this,
dataset size 2347 node graph 4 attributes. Setting attribute
indicating whether sales met underachieved expected amount class label,
obtain graph node three explanatory attributes. relational schema
dataset given Figure 12a. corresponding model graph showing various
conditional dependencies shown Figure 12b. again, relational methods
trained based model graph, besides taking account labels neighbors.
6.2.2 Observations
UW-CSE WEBKB datasets see best GTA better relational methods small percentage (< 20%) labels known, relational
methods quickly close gap start outperforming GTAs label information. weight matrix construction method however, performs better
two classes methods low label proportions remains competitive rela3. also includes cases sales exceeded expected amount.

832

fiSingle Network Relational Transductive Learning

tional methods proportion increases, unlike GTAs. favorable behavior
likely attributed method able effectively model strength (i.e.
numerical value) direction (i.e. + ) dependencies linked entities,
something GTAs seemingly fail capture.
BREAD dataset see GTAs much worse class
methods. possible reason stores near one another typically compete
type products hence, input graph exhibits strong
negative auto-correlation. Since, GTAs predominantly model similarity linked
entities, performance practically unchanged even percentage known
labels increased. relational methods perform much better GTAs setting.
contrast GTAs, effectively capture dissimilarity linked nodes
number known labels increases. However, weight matrix construction method seems
capture relationship much earlier small percentage labels known.

7. Discussion
paper, provided simple yet novel way constructing weight matrix
partially labeled relational graphs may directed undirected, may may
attributes may homogeneous heterogeneous. described
manner weight matrix serve input rich class graph
transduction methods modified graph laplacian based regularization framework.
portrayed desirable properties construction method showcased
effectiveness capturing complex dependencies experiments synthetic real
data.
primary focus paper learn effectively unweighted graphs.
However, many real world problems, might given weighted graph.
instance, genome sequence analysis connection strength gene expressions
estimated experiments coupled expert knowledge. situations
question arises incorporate known weights methodology?
logical consistent way incorporating weights modeling would
combine computed connectivity based weight wc attribute based weight
wa conical combination. consistent methodology described
combine connectivity based weight attribute based weight. Thus, wk
known (normalized) weight, weight edge would wc + wa + wk ,
, , 0. before, free parameters computed using standard model
selection techniques based graph properties domain knowledge.
future, would interesting extend procedure perform multi-class
classification single shot, rather perform multiple binary classification
tasks. would likely improve actual running time, though necessarily
time complexity terms O(.). would also interesting learn weights based
local neighborhood graph entire graph. Thus, would compute
based local structure around datapoint assign weights. Determining
locality however, tricky especially multiple link types.
theory side, might interest analyze synthetic label generation
procedure introduced paper, different types graphs. One could use ideas
833

fiDhurandhar & Wang

theory random walks determine tendencies label generation procedure.
learning theory perspective, one could potentially derive error bounds functions p
(amongst parameters), one express p terms auto-correlation ,
one would error bounds functions . would interest since
computed static graphs given snapshot evolving graph, one
know order nodes attached, thus making error bound
applicable graphs larger set applications.
related orthogonal research problem studying influence spread
social networks (Castillo, Chen, & Lakshmanan, 2012; Kempe, Kleinberg, & Tardos, 2003).
interesting research problem, one primary goals study
information flows real networks. end interesting find
nodes/people network likely influential targetting
people lead rapid information spread. something marketing departments
consumer product companies interested obvious reasons. Though
commonalities research problem ours, learn
perform inference real graphs, objectives quite different. case, mainly
care correctly labeling unknown nodes based connectivity attributes.
really interested information flow would fastest consequently
nodes target achieve efficient manner. certain sense,
influence spread problem probably could formulated active learning version
problem, want choose small number nodes query would maximize
performance particular class within network classification algorithms.
definitely something interesting pursue going forward.

Acknowledgments
would like thank Katherine Dhurandhar proofreading paper. would also
like thank editor reviewers constructive comments.

Appendix
provide figures synthetic real data experiments plots methods
best. Figures 16, 17, 18 19 correspond synthetic experiments,
figures 20, 21 22 correspond real data experiments.

834

fiSingle Network Relational Transductive Learning

50

16
RN

RN

PLEM

48
MLN

Percentage Error

Percentage Error

15

LGC

14

HFGF

13

HFGFW

RDN
LGCW

12

LGC

46

HFGF
HFGFW

RDN

44

LGCW
MLN

42

11
10
0

PLEM

20
40
60
Percentage Labeled

40
0

80

20
40
60
Percentage Labeled

80

Figure 16: Performance methods

Figure 17: Performance methods

PA high autocorrelation.

PA low autocorrelation.

16
15

50
RN

RDN

Percentage Error

Percentage Error

PLEM

14
LGC

13
MLN

12
HFGF

11
10
9
0

HFGFW
LGCW

45

LGC

HFGF
PLEM

40

MLN
RN

35

HFGFW
LGCW

RDN

20
40
60
Percentage Labeled

30
0

80

20
40
60
Percentage Labeled

80

Figure 18: Performance methods

Figure 19: Performance methods

FF high autocorrelation.

FF low autocorrelation.

835

fiDhurandhar & Wang

21
RN

70
MLN

19
18

PLEM

RN

60
HFGF

HFGFW
RDN

17

HFGFW

65

LGC

Percentage Error

Percentage Error

20

LGCW

HFGF

55

PLEM

50
45

LGC

LGCW

MLN

40
35

16

RDN

30
15
0

20

40
60
Percentage Labeled

25
0

80

Figure 20: Performance methods

20
40
60
Percentage Labeled

Figure 21: Multi-class transductive perfor-

UW-CSE dataset.

mance methods
WEBKB dataset.

50
RN

45
Percentage Error

PLEM

40
MLN

HFGF,LGC

35
30
25
20

RDN
HFGFW
LGCW

15
10
0

80

20
40
60
Percentage Labeled

80

Figure 22: Performance methods
BREAD dataset.

836

fiSingle Network Relational Transductive Learning

References
Barabasi, A., & Albert., R. (1999). Emergence scaling random networks. Science, 286,
509512.
Belkin, M., Niyogi, P., & Sindhwani, V. (2005). manifold regularization. Int. Workshop Artificial Intelligence Statistics.
Belkin, M., Niyogi, P., & Sindhwani, V. (2006). Manifold Regularization: Geometric
Framework Learning Labeled Unlabeled Examples. Journal Machine
Learning Research, 7, 23992434.
Castillo, C., Chen, W., & Lakshmanan, L. (2012).
Kdd2012 tutorial: Information influence spread social networks. http://research.microsoft.com/enus/people/weic/kdd12tutorial inf.aspx.
Chakrabarti, S., Dom, B., & Indyk, P. (1998). Enhanced hypertext categorization using
hyperlinks. Proceedings SIGMOD-98, ACM International Conference Management Data, pp. 307318, Seattle, US. ACM Press, New York, US.
Chu, W., Sindhwani, V., Ghahramani, Z., & Keerthi, S. (2007). Relational learning
gaussian processes. Advances Neural Information Processing Systems 19, pp.
289296. MIT Press.
Chung, F. (1997). Spectral graph theory. No. 92. Amer Mathematical Society.
Craven, M., DiPasquo, D., Freitag, D., McCallum, A., Mitchell, T., Nigam, K., & Slattery,
S. (1998). Learning extract symbolic knowledge world wide web. Proceedings fifteenth national/tenth conference Artificial intelligence/Innovative
applications artificial intelligence, AAAI, pp. 509516. American Association
Artificial Intelligence.
Dhurandhar, A., & Dobra, A. (2012). Distribution free bounds relational classification.
Knowledge Information Systems, 1.
Gallagher, B., Tong, H., Eliassi-Rad, T., & Faloutsos, C. (2008). Using ghost edges classification sparsely labeled networks. KDD 08: Proc. 14th ACM SIGKDD
Intl. conf. Knowledge discovery data mining, pp. 256264, New York, NY,
USA. ACM.
Getoor, L., Koller, D., & Small, P. (2004). Understanding tuberculosis epidemiology using
probabilistic relational models. Journal Artificial Intelligence Medicine, 30, 233
256.
Getoor, L., & Taskar, B. (2007). Introduction Statistical Relational Learning. MIT Press.
Goldberg, A., Zhu, X., & Wright, S. (2007). Dissimilarity graph-based semi-supervised
classification. Artificial Intelligence Statistics (AISTATS).
Goodman, L., & Kruskal, W. (1954). Measures association cross classifications. Journal American Statistical Association, 49, 732764.
Jebara, T., & Shchogolev, V. (2006). B-matching spectral clustering. Proc. 17th
European conf. Machine Learning, ECML06, Berlin, Heidelberg. Springer-Verlag.
837

fiDhurandhar & Wang

Jebara, T., Wang, J., & Chang, S. (2009). Graph construction b-matching semisupervised learning. Proc. 26th Annual Intl. Conf. Machine Learning,
ICML 09, pp. 441448, New York, NY, USA. ACM.
Kempe, D., Kleinberg, J., & Tardos, E. (2003). Maximizing spread influence
social network. Proceedings ninth ACM SIGKDD international conference
Knowledge discovery data mining, KDD 03, pp. 137146, New York, NY,
USA. ACM.
Leskovec, J., Kleinberg, J., & Faloutsos, C. (2007). Graph evolution: Densification
shrinking diameters. ACM Trans. Knowl. Discov. Data, 1 (1), 2.
Liu, W., & Chang, S. (2009). Robust multi-class transductive learning graphs.
Computer Vision Pattern Recognition, 2009., pp. 381388. IEEE.
Macskassy, A., & Provost, F. (2003). simple relational classifier..
Macskassy, S., & Provost, F. (2007). Classification networked data: toolkit
univariate case study. J. Mach. Learn. Res., 8, 935983.
Maier, M., Von Luxburg, U., & Hein, M. (2008). Influence graph construction graphbased clustering measures. Proc. Neural Infor. Proc. Sys.
Neville, J., & Jensen, D. (2007). Relational dependency networks. J. Mach. Learn. Res., 8,
653692.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Mach. Learn., 62 (1-2),
107136.
Sen, P., Namata, G. M., Bilgic, M., Getoor, L., Gallagher, B., & Eliassi-Rad, T. (2008).
Collective classification network data. AI Magazine, 29 (3).
Taskar, B., Abbeel, P., & Koller, D. (2002). Discriminative probabilistic models relational
data. Proc. 18th Conference Uncertainty AI, pp. 485492.
Tong, W., & Jin, R. (2007). Semi-supervised learning mixed label propagation.
Proceedings National Conference Artificial Intelligence.
Wang, J., Jebara, T., & Chang, S. (2013). Semi-supervised learning using greedy max-cut.
Journal Machine Learning Research, 14, 729758.
Wang, J., Jebara, T., & fu Chang, S. (2008). Graph transduction via alternating minimization. Proceedings International Conference Machine Learning.
Xiang, R., & Neville, J. (2008). Pseudolikelihood em within-network relational learning.
Proceedings 2008 Eighth IEEE International Conference Data Mining,
pp. 11031108, Washington, DC, USA. IEEE Computer Society.
Xiang, R., Neville, J., & Rogati, M. (2010). Modeling relationship strength online social
networks. Proc. 19th Intl. conf. World wide web, New York, NY, USA.
ACM.
Zhou, D., Bousquet, O., Lal, T., Weston, J., & Schlkopf, B. (2004). Learning local
global consistency. Advances Neural Information Processing Systems 16,
pp. 321328. MIT Press.
838

fiSingle Network Relational Transductive Learning

Zhu, X., Ghahramani, Z., & Lafferty, J. (2003). Semi-supervised learning using gaussian
fields harmonic functions. Proceedings ICML, pp. 912919.
Zhu, X., Lafferty, J., & Rosenfeld, R. (2005). Semi-supervised learning graphs. Ph.D.
thesis, Carnegie Mellon University, Language Technologies Institute, School Computer Science.

839

fiJournal Artificial Intelligence Research 48 (2013) 717-732

Submitted 6/13; published 11/13

Research Note
Case Pathology Multiobjective Heuristic Search
Jose Luis Perez de la Cruz
Lawrence Mandow
Enrique Machuca

perez@lcc.uma.es
lawrence@lcc.uma.es
machuca@lcc.uma.es

Dpto. Lenguajes Ciencias de la Computacion
Universidad de Malaga
Bulevar Louis Pasteur, 35. Campus de Teatinos, 29071
Malaga (Spain)

Abstract
article considers performance MOA* multiobjective search algorithm
heuristic information. shown certain cases blind search
efficient perfectly informed search, terms node label expansions.
class simple graph search problems defined number nodes grows
linearly problem size number nondominated labels grows quadratically.
proved problems number node expansions performed blind MOA*
grows linearly problem size, number expansions performed
perfectly informed heuristic grows quadratically. also proved number label
expansions grows quadratically blind case cubically informed case.

1. Introduction
Heuristic search algorithms central problem solving Artificial Intelligence
many practical applications Operations Research. heuristic search additional
information provided algorithm aim reducing computational effort
needed find solution.
However, sometimes goal achieved. contrary, certain cases
shown use better heuristics implies worsening performance.
example, well-known fact arising bipersonal games lookahead pathology,
is, deeper exploration performed (and hence better suppossedly
heuristic minimax value assigned position), worse decision taken (Nau,
1982). Recently, phenomenon described one-agent real-time search (Lustrek
& Bulitko, 2008; Nau, Lustrek, Parker, Bratko, & Gams, 2010).
Even statically precomputed heuristics pathologies found.
instance, let us consider standard A* algorithm (Hart, Nilsson, & Raphael, 1968).
known algorithm admissible provided optimistic heuristic cost
estimates that, estimates also consistent, informed heuristics
always result equally efficient search (see Pearl, 1984, especially pp. 7585).
However, heuristic optimistic consistent, algorithm A* perform
O(2n ) node expansions search performed graph n nodes arc costs
bounded (Martelli, 1977). Notice heuristic used, A* performs like
Dijkstras algorithm never exhibit exponential performance.
c
2013
AI Access Foundation. rights reserved.

fiPerez de la Cruz, Mandow, & Machuca

paper deals pathology arising certain extension A* multiobjective
search problems. decision making situations one criterion involved,
concept optimal solution frequently replaced Pareto optimality, is, solution better improves respect least one criterion without
worsening others (Ehrgott, 2005). Since Pareto optimality partial order relation,
solving problems usually results set Pareto-optimal solutions, represent
optimal trade-offs criteria optimized. importance research
multiobjective search algorithms two-fold. first place, many graph search problems benefit directly multiobjective analysis (De Luca Cardillo & Fortuna, 2000;
Gabrel & Vanderpooten, 2002; Refanidis & Vlahavas, 2003; Muller-Hannemann & Weihe,
2006; DellOlmo, Gentili, & Scozzari, 2005; Ziebart, Dey, & Bagnell, 2008; Wu, Campbell,
& Merz, 2009; Delling & Wagner, 2009; Fave, Canu, Iocchi, Nardi, & Ziparo, 2009; Mouratidis, Lin, & Yiu, 2010; Caramia, Giordani, & Iovanella, 2010; Boxnick, Klopfer, Romaus, &
Klopper, 2010; Klopper, Ishikawa, & Honiden, 2010; Wu, Campbell, & Merz, 2011; Machuca
& Mandow, 2011). hand, multicriteria preference models used graph
search typically look subset Pareto-optimal solutions (Mandow & Perez de la Cruz,
2003; Perny & Spanjaard, 2005; Galand & Perny, 2006; Galand & Spanjaard, 2007; Galand,
Perny, & Spanjaard, 2010). Therefore, improvements performance multiobjective algorithms guide development efficient algorithms multicriteria decision
rules.
Two direct extensions A* accept general (multiobjective) heuristic functions
proposed literature: MOA* (Stewart & White, 1991) NAMOA* (Mandow
& Perez de la Cruz, 2005). NAMOA* uses label selection guide exploration.
formal point view, recent analysis (Mandow & Perez de la Cruz, 2010a) shown
algorithm admissible optimistic heuristics, efficiency, measured
number label expansions, improves informed consistent heuristics.
Furthermore, number expansions optimal respect class admissible
algorithms. words, NAMOA* inherits beneficial properties A*.
MOA* uses node selection (as opposed label selection) guide exploration,
also known admissible optimistic heuristics (Stewart & White, 1991).
development MOA* prompted number related formal developments extensions
(Dasgupta, Chakrabarti, & DeSarkar, 1995, 1999; Perny & Spanjaard, 2002; Mandow &
Perez de la Cruz, 2003; Perny & Spanjaard, 2005), still cited algorithm
choice recent applications (De Luca Cardillo & Fortuna, 2000; Fave et al., 2009; Klopper
et al., 2010). previous formal analysis showed exist problems blind
MOA* performs (2n ) node expansions graphs n nodes (Mandow & Perez de la
Cruz, 2010b). However, formal analysis MOA* remained incomplete. particular,
efficiency algorithm never related precision consistent heuristics.
recent empirical analysis (Machuca, Mandow, Perez de la Cruz, & Ruiz-Sepulveda, 2010)
shown that, certain cases, MOA* performs much worse NAMOA* biobjective random problems different correlations. Quite surprisingly, analysis
also revealed heuristic MOA* actually performs consistently worse uninformed
MOA*.
paper part investigation formal properties MOA* NAMOA*.
formally show performance MOA*, measured terms number
718

fiA Case Pathology Multiobjective Heuristic Search

label node expansions, improve general informed heuristics.
precisely, define class simple graph search problems prove use
perfect heuristic information MOA* yields computational effort use
heuristic information. words, blind MOA* cases efficient
perfectly informed MOA*, terms node label expansions.
article organized follows. First, necessary concepts presented
algorithm MOA* briefly described (Section 2). Section 3 class simple
multiobjective search problems defined. performance MOA* class
problems analyzed blind perfectly informed cases Sections 4 5 terms
node expansions, Section 6 terms label expansions. Finally, conclusions
future work described.

2. Background
multiobjective decision problems alternative evaluated according set q
different objectives usually grouped vector ~y = (y1 , y2 , . . . yq ), ~y Rq . Preference
vectors ~x, ~y defined so-called Pareto order dominance relation ()
follows: ~x ~y objectives holds xi yi least
objective j holds xj < yj . Given set vectors , subset nondominated
vectors nd(Y ) defined nd(Y ) = {~y | @~x ~x ~y }.
solution multiobjective problem consists set Pareto-optimal nondominated solutions, is, set solutions costs nondominated
set solution costs.
multiobjective graph search problem, single source set destination nodes
designated given graph G = (N, A). Pairs nodes n, n0 N may joined
directed arcs (n, n0 ) labelled vector costs ~c(n, n0 ) Rq . path P graph
sequence nodes joined consecutive arcs cost ~c(P ) P sum
costs component arcs. solution problem set paths P joining
source destination nodes ~c(P ) nondominated set solution
costs.
2.1 MOA* Algorithm
MOA* well-known algorithm performs multiobjective heuristic graph search (Stewart & White, 1991). pseudocode (slightly adapted original: Stewart & White,
1991) presented Table 1. MOA* presents many similarities A*. Two sets nodes
OP EN CLOSED used control search. Initially, source node
open node. Newly generated nodes create pointer parents. However, MOA*
construct search tree like A*, rather acyclic directed graph. due
fact node may reached several optimal (nondominated) paths. scalar
cost functions g, h, f generalized functions G, H, F return sets vectors
node. Additionally, LABEL(n0 , n) sets keep subsets vectors G(n0 ) arise
paths n0 coming n.
Function G(n) refers set nondominated cost vectors among paths already
found n. heuristic function H(n) returns also set vectors, estimating costs
719

fiPerez de la Cruz, Mandow, & Machuca

1. INITIALIZE set OP EN start node s, empty sets, SOLN , C, CLOSED LABEL.
2. CALCULATE set N nodes n OP EN least one estimate f~ F (n)
dominated estimates open nodes solution cost C.
3. N empty,
Terminate returning set solution paths reach nodes SOLN costs C.
else
Choose node n N using domain-specific heuristic, breaking ties favour goal nodes,
move n OP EN CLOSED.
4. bookkeeping maintain accrued costs node selection function values.
5. IDENTIFY SOLUTIONS. n solution node,
Include n SOLN current costs C.
Remove dominated costs C.
Go back step 2.
6. EXPAND n examine successors. successors nodes n do:
(a) newly generated node,
i.
ii.
iii.
iv.

Establish pointer n.
Set G(m) = LABEL(m, n).
Compute F (m).
Add OP EN .

(b) Otherwise, new, following,
i. potentially nondominated paths discovered, then, one,
following.
Ensure cost LABEL(m, n), therefore G(m).
new cost added G(m) then, purge LABEL(m, n) dominated costs,
CLOSED, move OP EN .
7. Go back step 2.

Table 1: MOA Algorithm.
nondominated paths n destination nodes. evaluation function F (n) returns
set cost estimates n, F (n) = nd{~g + ~h | ~g G(n) ~h H(n)}.
Later useful define H (n) function returns set costs
actual nondominated paths n destinations nodes.
iteration MOA* computes ND, subset open nodes nondominated
cost estimate, selects node subset. admissibility algorithm
depend particular selection procedure among nodes ND. following,
additional selection procedure nondominated nodes called nd-selection rule.
destination node selected, added SOLN, costs C. Values
F (n) dominated vectors C never considered ND. Search terminates ND
empty, is, candidate nodes dominated explored.
expansion n generates successors n0 n graph adequate G(n0 )
values them. n0 new, placed OPEN, sets G(n0 ) LABEL(n0 , n)
store costs paths extended n n0 . n0 new, MOA* checks
720

fiA Case Pathology Multiobjective Heuristic Search

new nondominated value G(n0 ) generated current step;
case, G(n0 ) LABEL(n0 , n) properly updated, and, n0 CLOSED, must
moved back OPEN.
pair (n, ~g ) n node ~g G(n) usually called label. MOA*
labels node n expanded simultaneously n selected. Therefore, labels
reaching single node given time either simultaneously open closed.
original paper (Stewart & White, 1991), interesting properties MOA*
proved. example, proved MOA* admissible H(n) optimistic.
Regarding comparison admissible heuristics, function H(n) defined least
informed another H 0 (n) whenever ~h0 H 0 (n) exists ~h H(n)
~h0 ~h. case, proved set nodes expanded MOA* H
subset expanded H 0 (theorem 4, p. 805). However, authors recognized
nodes may reopened even heuristic function consistent, hence
set expanded nodes significant measure analysis performance
MOA*.

Figure 1: Graph (3, 10, 10, 2)

3. Class Multiobjective Search Problems
every n N, let us consider problem graphs (Figure 1) 2n nodes labeled 1, . . . , 2n
1, 2n 3n 2 arcs. every even node 2i (1 < n) outgoing arcs
form (2i, 2i + 1) (2i, 2i + 2). every odd node 2i + 1 (1 < n 1)
outgoing arc form (2i + 1, 2i + 2). also arc (1, 2). start node
1 goal node 2n. cost arc ~c(i, j) defined follows: choose
either 2 4; every > 0, ~c(2i, 2i + 2) = (, 6 ); every > 0,
~c(2i, 2i + 1) = ~c(2i + 1, 2i + 2) = (3 /2, /2). way, define two possible
sets costs arcs, exception ~c(1, 2) = (1 , 2 ) subject
restriction.
shall refer problem graphs multiobjective chain graphs. every n,
corresponding set multiobjective chain graphs denoted Mn . denote
(n, 1 , 2 , ) graph Mn ~c(1, 2) = (1 , 2 ) c(2i, 2i + 2) = (, 6 ).
example, Figure 1 shows (3, 10, 10, 2).
graph Mn always 2n1 different paths start goal
node. fact, go 2i 2i + 2 choose either simple path < 2i, 2i + 2 >
(with cost (, 6 )) two-arc path < 2i, 2i + 1, 2i + 2 > (with cost (6 , ))
n 1 independent choices like that. hand, n different
path costs given c~n k = (1 , 2 ) + (2(n 1) + 2k, 4(n 1) 2k), every k
721

fiPerez de la Cruz, Mandow, & Machuca

It. #
1
2
3
4
5
6

OPEN
1
2
3
4
4
5
6
6

G(n) = F (n)
(0, 0)
(10, 10)
(12, 11)
(12, 14)
(12, 14)(14, 12)
(14, 15)(16, 13)
(14, 18)(16, 16)
(14, 18)(16, 16)(18, 14)

Table 2: Trace uninformed MOA* (3, 10, 10, 2)

0 k n 1. (n, 1 , 2 , 2) graph, cost c~n k corresponds path n 1 k
arcs (2i, 2i + 2) k paths < 2i, 2i + 1, 2i + 2 >. denote Cn = {c~n 0 , . . . , c~n n1 }.
Notice every solution cost (y1 , y2 ) Cn holds y1 + y2 = 1 + 2 + 6(n 1),
solution costs given lie line negative slope hence none
costs dominates another, is, nd(Cn ) = Cn .

4. Blind MOA* Mn
sample run MOA* (3, 10, 10, 2) (Figure 1) H(n) = {~0} (uninformed case)
provided Table 2. Values G(n) include nondominated costs generated paths
node. Since performing blind search, values F (n)
G(n).
observe trace node selected nodes j <
selected. means every node i, 1 2n 1 expanded
once. way, example MOA* performs exactly 2n node selections 2n 1
node expansions. result general proved induction number
iterations every graph (n, 1 , 2 , ).
Lemma 1 input MOA* graph (n, 1 , 2 , ) n H(n) = {~0}, MOA*
performs exactly 2n 1 node expansions.
Proof. Let us consider OPEN set certain iteration (s = 1, . . .) execution
MOA*. Let us call level integer L = bs/2c. easily proved induction
that:
(i) every odd iteration algorithm (s = 3, . . .), OPEN = {s, s+1}, labels
{(a+3/2, b+/2) | (a, b) CL }; labels s+1 {(a+, b+6)| (a, b) CL };
selected node s.
(ii) every even iteration algorithm (s = 4, . . .), OPEN = {s}, labels
exactly CL selected node s.
follows every node selected exactly therefore number
node expansions exactly 2n 1.
/
722

fiA Case Pathology Multiobjective Heuristic Search

n
1
2
3
4
5
6

H (n)
(14,18),(16,16),(18,14)
(4,8),(6,6),(8,4)
(4,5),(6,3)
(2,4),(4,2)
(2,1)
(0,0)

Table 3: Heuristic values (3, 10, 10, 2)

Figure 2: Search graph iteration 2

5. Perfectly Informed MOA* Mn
sample run MOA* (3, 10, 10, 2) (Figure 1) H(n) = H (n) (perfect information) provided Figures 2-9 Table 4. Figures 2-9 show trace search graph
iteration. Closed nodes shown gray. Values G(n) shown node
Figures created, change previous iteration.
Table 4, iteration nodes OPEN displayed also G(n)
values (that is, nondominated costs paths generated start node). Values
F (n) computed adding G(n) values estimations Table 3. Since
assuming perfect heuristic information, F (n) values always optimal solution costs,
is, nondominated costs solution paths node 1 node 6. Then, every iteration
every node n, F (n) C3 .
Notice also labels C3 (and hence F (n)) nondominated, general
several open nondominated nodes. necessary provide additional
heuristic rule (step 3-else algorithm Table 1) nd-selection rule. example
Table 4, following nd-selection rule applied: select node best lexicographic
nondominated alternative (remember lexicographic order total order defined
biobjective case (y1 , y2 ) < (z1 , z2 ) y1 < z1 , y1 = z1 y2 < z2 ).
also possible nondominated cost appears several open nodes.
another procedure must provided breaking ties open nodes
f~-label. done, instance, random, selecting newest node,
oldest node OPEN breadth-first fashion. latter procedure followed
Table 4.
observe order node selection example
124634656
pattern is: MOA* selects even nodes goal node reached; selects
odd node 2i + 1 selects even nodes 2j, j > i; done every
odd node selected once. way even nodes general selected several
times. example node 4 selected twice node 6 selected three times.
723

fiPerez de la Cruz, Mandow, & Machuca

It#
1
2
3
4

5
6
7
8
9

OPEN
1
2
3
4
3
5
6
3
5
4
5
5
6
5
6

G(n)
(0, 0)
(10, 10)
(12, 11)
(12, 14)
(12, 11)
(14, 15)
(14, 18)
(12, 11)
(14, 15)
(12, 14)(14, 12)
(14, 15)
(14, 15)(16, 13)
(14, 18)(16, 16)
(14, 15)(16, 13)
(14, 18)(16, 16)(18, 14))

F (n)
(14, 18)(16, 16)(18, 14)
(14, 18)(16, 16)(18, 14)
(16, 16)(18, 14)
(14, 18)(16, 16)
(16, 16)(18, 14)
(16, 16)
(14, 18)
(16, 16)(18, 14)
(16, 16)
(14, 18)(16, 16)(18, 14)
(16, 16)
(16, 16)(18, 14)
(14, 18)(16, 16)
(16, 16)(18, 14)
(14, 18)(16, 16)(18, 14)

Table 4: Trace perfectly informed MOA* (3, 10, 10, 2)

Figure 3: Search graph iteration 3

Figure 4: Search graph iteration 4

Figure 5: Search graph iteration 5

724

fiA Case Pathology Multiobjective Heuristic Search

Figure 6: Search graph iteration 6

Figure 7: Search graph iteration 7

Figure 8: Search graph iteration 8

Figure 9: Search graph iteration 9

725

fiPerez de la Cruz, Mandow, & Machuca

concrete order expansion depend nd-selection tie-breaking rules.
However, prove general results valid nd-selection rule uses
heuristic f~ values (irrespective tie-breaking rule). Notice nd-selection rules
usually applied (as best lexicographic best linear) kind.
Lemma 2 (i) Let (n, 1 , 2 , 2) Mn . Let nondominated solution costs C =
{~
c 0 , . . . , c~ n1 }. nd-selection rule c~ 0 selected preference
{~
c 1 , . . . , c~ n1 }, MOA* performs least n + n(n1)
node expansions.
2
(ii) Analogously, let (n, 1 , 2 , 4) Mn . nd-selection rule c~ n1
selected preference {~
c 0 , . . . , c~ n2 }, MOA* performs least n + n(n1)
node
2
expansions.
Proof. prove part (i) (proof part (ii) entirely analogous). Let (n, 1 , 2 , 2)
Mn . First remember node j iteration algorithm, set
F (j) estimated costs j subset Cn = {~
c 0 , . . . , c~ k , . . . , c~ n1 } = {(1 + (2(n
1) + 2k, 2 + 4(n 1) 2k)) | 0 k n 1}. Let us trace first n + 1 iterations
algorithm. shown every > 0, c~ 0 appear F (2i), c~ 0 never
appear F (2i + 1). Therefore, first iteration node 1 selected expanded.
even nodes selected expanded sequentially node 2i selected.
amounts first n expansions.
Elementary computations show step, every 0 < n, F (2i) = {~
c 0, . . . ,
ni
1
ni
c~
}; every 0 < n1, F (2i+1) = {~
c , . . . , c~
}; OP EN = {3, 5, . . . , 2n1}.
Two observations made step: i) Let us consider odd nodes. Since every odd
node 2i + 1 (0 < < n) optimal path going it, open nodes must
selected expanded termination. amounts least n 1 expansions,
even reexpansion assumed nodes; ii) Let us consider nongoal even
nodes {2, 4, . . . , 2n 2}. iteration number labels associated 2i n + 1 i.
Since every even node exist n optimal costs, termination number
labels every even node must exactly n, is, 0 + 1 + . . . + (n 1) = n(n1)
2
labels even nodes missing moment. prove labels generated
one time, is, one every expansion, proved least another
n(n1)
expansions needed.
2
Let us call episode subsequence node expansions comprised two consecutive odd node expansions (or last odd node expansion last expansion).
example Table 4, episodes < 1, 2, 4, 6 >, < 3, 4, 6 > < 5, 6 >.
episode starts, even node OPEN (since even node always ndselected label c~ 0 , odd node never nd-selected label c~ 0 , odd node
selected). end episode, reason, even node
OPEN. Let us consider episode e =< 2j 1, 2j, 2j 0 , . . . , 2j n >. easily seen
expansion even node 2j originates opening even node, namely 2(j + 1),
episode always form E =< 2j 1, 2j, 2(j + 1), 2(j + 2), . . . , 2(j + m) >.
prove induction episodes every episode starts, every even
node 2i (1 < n) exist integers p, q that: i) F (2i) = {~
c 0 , . . . , ~cp }; ii)
F (2i 2) = {~
c 0 , . . . , c~ q } = F (2i 1) {~
c 0 }; iii) either q = p q = p + 1.
obviously true first episode finishes second episode starts. Assume
726

fiA Case Pathology Multiobjective Heuristic Search

true episode e starts. show remains true finishes, is,
episode e + 1 starts.
Consider odd node 2j 1 starts episode assume q = p, is,
F (2j 1) = F (2j) {~
c 0 }. new label added F (2j), reexpansion
needed modification done, hence stated relation among labels continues
true. contrary, assume q = p + 1, is, F (2j 1) = {~
c 1 , . . . , c~ p+1 },
0
p+1
0
p
F (2j 2) = {~
c , . . . , c~
}, F (2j) = {~
c , . . . , c~ }. Therefore one label c~ p+1 added
2j. stated relation labels remain true 2j 2, 2j 1 2j (only
alternative p = q). However, F (2j) modified must
check relation F (2j), F (2j + 1) F (2j + 2). hypothesis F (2j + 1) =
{~
c 1 , . . . , c~ p }, expansion 2j adds c~ p+1 it, also becomes F (2j) = F (2j +1).
Concerning relation F (2j) F (2j + 2), F (2j + 2) = {~
c 0 , . . . , c~ p },
label added F (2j + 2), relation holds (since F (2j + 2) exactly one
label less F (2j) now) episode finishes since even node remains open.
F (2j + 2) = {~
c 0 , . . . , c~ p1 }, one label c~ p added F (2j + 2) relation also
becomes true, F (2j + 2) modified. induction length episode
could proved finally relation holds modified nodes.
case, see new added label immediately triggers expansion
node labels added even nodes one time. Therefore, least n(n1)
expansions
2
needed complete labels even nodes. first expansion episode one
odd node, must computed; last expansion episode
add label, compute n(n1)
additional node expansions.
2
expansions,
Now, adding togheter performed expansions, least n + n(n1)
2
q. e. d.
/

Figure 10: Construction (4, 1 , 2 , 2) graph Lemma 3

727

fiPerez de la Cruz, Mandow, & Machuca

prove main lemma:
Lemma 3 nd-selection rule depends f~ values, every n N
exists graph (n, 1 , 2 , ) input MOA* H(n) = H (n),
MOA* performs least n + n(n1)
node expansions.
2
Proof. idea proof follows: Lemma 2 asserts order lexicographically optimal costs graph (n, 1 , 2 , 2) Mn selected one turns
first one, MOA* performs least n + n(n1)
node expansions (and analogously
2
every graph (n, 1 , 2 , 4) Mn selected one last one).
Lemma 3 proved every n every selection rule depending f~ values show
graph (n, 1 , 2 , 2) Mn satisfying condition (or graph (n, 1 , 2 , 4) Mn
satisfying analogous condition).
Let n N. Let us consider line y1 + y2 = 8(n 1) + 2 sequence = 2n 1
nondominated points it, (F 0 , . . . , F m1 ) given F = (2(n1)+2i+1, 6(n1)2i+1)
(Figure 10 shows line seven points F 0 , . . . , F 6 n = 4). nd-selection
rule select one them, say F j , preference others; case,
F j extract (F 0 , . . . , F m1 ) subsequence n points (F j , F j+1 , . . . , F j+n1 )
(F jn+1 , . . . , F j1 , F j ). Assume first case (that depicted Figure 10 j = 3;
subsequence F 3 , F 4 , F 5 , F 6 ). consider graph (n, 1 , 2 , 2)
1 = 2j + 1 2 = 2(n 1 j) + 1 (in Figure 10 point K (1 , 2 ) = (7, 1)).
always possible, is, every n, j 1 , 2 > 0.
every k, 0 k n 1, F j+k = (1 , 2 ) + (2(n 1) + 2k, 4(n 1) 2k) = c~ k ,
is, extracted subsequence (F j , F j+1 , . . . , F j+n1 ) exactly set solution costs
(n, 1 , 2 , 2), (~
c 0 , c~ 1 , . . . , c~ n1 ). Since F j nd-selected (F j+1 , . . . , F j+n1 ),
Lemma 2 MOA* performs least n + n(n1)
node expansions (n, 1 , 2 , 2).
2
Analogously prove case considering graph form (n, 3 , 4 , 4).
/
Lemmas 1 3 obtain immediately following result.
Theorem 1 every nd-selection rule depending f~ values, exists sequence
graphs M1 , . . . , Mn , . . . that:
(i) Every Mn 2n nodes 3n 2 arcs.
(ii) MOA performs (n) node expansions applied Mn heuristic information given.
(iii) MOA performs (n2 ) node expansions applied Mn perfect heuristic
information given.

6. Label Counts
basic operation MOA node expansion every node expansion implies
general case joint expansion several labels. However, algorithms (e.g.
NAMOA , Mandow & Perez de la Cruz, 2010a) use label expansion basic operation.
reason could interesting analyse MOA also terms label expansions.
Lemma 4 input MOA* graph (n, 1 , 2 , ) H(n) = {~0}, MOA*
performs exactly n2 n + 1 label expansions.
728

fiA Case Pathology Multiobjective Heuristic Search

Proof. Consider reasoning proof Lemma 1. proved that,
selected expansion, every even node 2i labels
Pand every odd node 2i + 1
(with 1) labels. Adding together 1 + 2 1in1 = 1 + n(n 1) =
n2 n + 1 label expansions.
/
Lemma 5 every n N exists graph (n, 1 , 2 , ) input
MOA* H(n) = H (n), MOA* performs least n2 + n(n+1)(n1)
label expansions.
4
Proof. Every odd node 1, . . . , 2n 1 must termination n labels, must
expanded least once. amounts least n2 label expansions. hand,
consider even nodes 2, 4, . . . , 2n 2. must also n labels termination. Consider
reasoning proof Lemma 2. proved that: (i) first time
even node 2i expanded, (n + 1) labels; (ii) time even node 2i
expanded, exactly one label. Therefore thePnumber label expansions node
2i (n + 1) + (n + 2) + . . . + (n + i) = 1ji (n + j) = (n+1)i
2 . Adding
P
(n+1)i
togheter even nodes 2i 1 n 1 1in1 2 = n(n+1)(n1)
4
label expansions. Adding expansions odd even nodes least
n2 + n(n+1)(n1)
q. e. d.
4
/
Lemmas 4 5 obtain immediately following result.
Theorem 2 every nd-selection rule depending f~ values, exists sequence
graphs M1 , . . . , Mn , . . . that:
(i) Every Mn 2n nodes 3n 2 arcs.
(ii) MOA performs (n2 ) label expansions applied Mn heuristic information given.
(iii) MOA performs (n3 ) label expansions applied Mn perfect heuristic
information given.

7. Conclusions Future Work
paper considers performance MOA* multiobjective heuristic search algorithm. Results show performance degrade better heuristic information.
class problems presented (multiobjective chain graphs) use perfect heuristic information (a trivially consistent informed heuristic) result reduction
number node label expansions performed algorithm. contrary,
performance perfectly informed version algorithm worse performance
blind version.
Multiobjective chain graphs formalize infrequent situation practical multiobjective search, sequence nodes traversed least two conflicting paths.
analysis revealed MOA* combined H , best possible heuristic,
number node expansions grows quadratically, number grows linearly
heuristic information used; number label expansions grows cubically,
grows quadratically heuristic information provided.
729

fiPerez de la Cruz, Mandow, & Machuca

pathology, together results theoretical (Mandow & Perez de la
Cruz, 2010b) empirical (Machuca, Mandow, Perez de la Cruz, & Ruiz-Sepulveda, 2012),
casts doubts suitability MOA* performing heuristic multiobjective search.
general, alternatives (such NAMOA*) proved (Mandow
& Perez de la Cruz, 2010a) pathological behaviours cannot arise,
preferred.

Acknowledgments
Partially supported Gobierno de Espana, grant TIN2009-14179. Partially funded
Consejera de Innovacion, Ciencia Empresa. Junta de Andaluca (Espana), P07-TIC03018.

References
Boxnick, S., Klopfer, S., Romaus, C., & Klopper, B. (2010). Multiobjective search
management hybrid energy storage system. IEEE International Conference
Industrial Informatics (INDIN), pp. 745750.
Caramia, M., Giordani, S., & Iovanella, A. (2010). selection k routes multiobjective hazmat route planning. IMA Journal Management Mathematics, 21,
239251.
Dasgupta, P., Chakrabarti, P., & DeSarkar, S. (1995). Utility pathmax partial order
heuristic search. Information Processing Letters, 55, 317322.
Dasgupta, P., Chakrabarti, P., & DeSarkar, S. (1999). Multiobjective Heuristic Search.
Vieweg, Braunschweig/Wiesbaden.
De Luca Cardillo, D., & Fortuna, T. (2000). Dea model efficiency evaluation
nondominated paths road network. European Journal Operational Research,
121 (3), 549558.
Delling, D., & Wagner, D. (2009). Pareto paths SHARC. SEA, pp. 125136.
DellOlmo, P., Gentili, M., & Scozzari, A. (2005). finding dissimilar Pareto-optimal
paths. European Journal Operational Research, 162, 7082.
Ehrgott, M. (2005). Multicriteria Optimization. Springer.
Fave, F. M. D., Canu, S., Iocchi, L., Nardi, D., & Ziparo, V. A. (2009). Multi-objective
multi-robot surveillance. 4th International Conference Autonomous Robots
Agents (ICARA), pp. 6873. IEEE.
Gabrel, V., & Vanderpooten, D. (2002). Enumeration interactive selection efficient
paths multiple criteria graph scheduling earth observing satellite. European
Journal Operational Research, 139, 533542.
Galand, L., & Perny, P. (2006). Search compromise solutions multiobjective state
space graphs. Proc. XVII European Conference Artificial Intelligence
(ECAI2006), pp. 9397.
730

fiA Case Pathology Multiobjective Heuristic Search

Galand, L., Perny, P., & Spanjaard, O. (2010). Choquet-based optimisation multiobjective shortest path spanning tree problems. European Journal Operational
Research, 204 (2), 303315.
Galand, L., & Spanjaard, O. (2007). Owa-based search state space graphs multiple
cost functions. FLAIRS Conference 2007, pp. 8691.
Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determination
minimum cost paths. IEEE Trans. Systems Science Cybernetics SSC-4, 2,
100107.
Klopper, B., Ishikawa, F., & Honiden, S. (2010). Service composition pareto-optimality
time-dependent QoS attributes. Lecture Notes Computer Science, LNCS 6470,
635640.
Lustrek, M., & Bulitko, V. (2008). Thinking much: Pathology path finding. et al.,
M. G. (Ed.), Proceedings European 18th Conference Artificial Intelligence,
pp. 899900. IOS Press.
Machuca, E., Mandow, L., Perez de la Cruz, J. L., & Ruiz-Sepulveda, A. (2010).
empirical comparison multiobjective graph search algorithms. KI2010 LNAI 6359, pp. 238245.
Machuca, E., Mandow, L., Perez de la Cruz, J. L., & Ruiz-Sepulveda, A. (2012). comparison heuristic best-first algorithms bicriterion shortest path problems. European
Journal Operational Research, 217, 4453.
Machuca, E., & Mandow, L. (2011). Multiobjective route planning precalculated
heuristics. Proc. 15th Portuguese Conference Artificial Intelligence (EPIA
2011), pp. 98107.
Mandow, L., & Perez de la Cruz, J. L. (2003). Multicriteria heuristic search. European
Journal Operational Research, 150, 253280.
Mandow, L., & Perez de la Cruz, J. L. (2005). new approach multiobjective A*
search. Proc. XIX Int. Joint Conf. Artificial Intelligence (IJCAI05), pp.
218223.
Mandow, L., & Perez de la Cruz, J. L. (2010a). Multiobjective A* search consistent
heuristics. Journal ACM, 57 (5), 27:125.
Mandow, L., & Perez de la Cruz, J. L. (2010b). note complexity multiobjective A* search algorithms. ECAI 2010, pp. 727731.
Martelli, A. (1977). complexity admissible search algorithms. Artificial Intelligence, 8, 113.
Mouratidis, K., Lin, Y., & Yiu, M. (2010). Preference queries large multi-cost transportation networks. Proceedings - International Conference Data Engineering,
pp. 533544.
Muller-Hannemann, M., & Weihe, K. (2006). cardinality Pareto set bicriteria shortest path problems. Annals OR, 147 (1), 269286.
Nau, D. S. (1982). investigation causes pathology games. Artificial Intelligence, 19 (3), 257278.
731

fiPerez de la Cruz, Mandow, & Machuca

Nau, D. S., Lustrek, M., Parker, A., Bratko, I., & Gams, M. (2010). better
look ahead?. Artificial Intelligence, 174 (1617), 13231338.
Pearl, J. (1984). Heuristics. Addison-Wesley, Reading, Massachusetts.
Perny, P., & Spanjaard, O. (2002). preference-based search state space graphs.
Proc. Eighteenth Nat. Conf. AI, pp. 751756. AAAI Press.
Perny, P., & Spanjaard, O. (2005). preference-based approach spanning trees
shortest paths problems. European Journal Operational Research, 162, 584601.
Refanidis, I., & Vlahavas, I. (2003). Multiobjective heuristic state-space search. Artificial
Intelligence, 145, 132.
Stewart, B. S., & White, C. C. (1991). Multiobjective A*. Journal ACM, 38 (4),
775814.
Wu, P.-Y., Campbell, D., & Merz, T. (2009). On-board multi-objective mission planning
unmanned aerial vehicles. IEEE Aerospace Conference Proceedings, pp. 110.
Wu, P.-Y., Campbell, D., & Merz, T. (2011). Multi-objective four-dimensional vehicle
motion planning large dynamic environments. IEEE Transactions Systems,
Man, Cybernetics, Part B: Cybernetics, 41 (3), 621634.
Ziebart, B., Dey, A., & Bagnell, J. (2008). Fast planning dynamic preferences. ICAPS
2008 - Proceedings 18th International Conference Automated Planning
Scheduling, pp. 412419.

732

fiJournal Artificial Intelligence Research 48 (2013) 953-1000

Submitted 09/13; published 12/13

Constraint Solver Flexible Protein Models
Federico Campeotto

campe8@nmsu.edu

Dept. Computer Science, New Mexico State University
Depts. Math. & Computer Science, University Udine

Alessandro Dal Palu

alessandro.dalpalu@unipr.it

Dept. Math. & Computer Science, University Parma

Agostino Dovier

agostino.dovier@uniud.it

Dept. Math. & Computer Science, University Udine

Ferdinando Fioretto

ffiorett@cs.nmsu.edu

Dept. Computer Science, New Mexico State University
Depts. Math. & Computer Science, University Udine

Enrico Pontelli

epontell@cs.nmsu.edu

Dept. Computer Science, New Mexico State University

Abstract
paper proposes formalization implementation novel class constraints aimed modeling problems related placement multi-body systems
3-dimensional space. multi-body system composed body elements, connected
joint relationships constrained geometric properties. emphasis investigation use multi-body systems model native conformations protein
structureswhere body represents entity protein (e.g., amino acid,
small peptide) geometric constraints related spatial properties
composing atoms. paper explores use proposed class constraints support
variety different structural analysis proteins, loop modeling structure
prediction.
declarative nature constraint-based encoding provides elaboration tolerance
ability make use additional knowledge analysis studies. filtering
capabilities proposed constraints also allow control number representative
solutions withdrawn conformational space protein, means
criteria driven uniform distribution sampling principles. scenario possible
select desired degree precision and/or number solutions. filtering component
automatically excludes configurations violate spatial geometric properties
composing multi-body system. paper illustrates implementation constraint
solver based multi-body perspective empirical evaluation protein structure
analysis problems.

1. Introduction
Constraint Programming (CP) declarative programming methodology gained
predominant role addressing large scale combinatorial optimization problems.
paradigm, CP provides tools necessary guide modeling resolution search
problemsin particular, offers declarative problem modeling (in terms variables
constraints), ability rapidly propagate effects search decisions, flexible
efficient procedures explore search space possible solutions. field CP
c
2013
AI Access Foundation. rights reserved.

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

roots seminal work Sutherland (1963) Sketchpad system,
successive efforts systems like CONSTRAINTS (Sussmann & Steele, 1980) ThingLab
(Borning, 1981). years, CP become paradigm choice address hard search
problems, drawing integrating ideas diverse domains, like Artificial Intelligence
Operations Research (Rossi, van Beek, & Walsh, 2006). declarative nature
CP enables fast natural modeling problems, facilitating development,
rapid exploration different models resolution techniques (e.g., modeling choices,
search heuristics).
recent years, several research groups started appreciating potential constraint programming within realm Bioinformatics. field Bioinformatics presents
number open research problems grounded critical exploration combinatorial search space, highly suitable manipulated constraint-based search.
Constraint methodologies applied analyze DNA sequences instance,
locate Cis-regulatory elements (Guns, Sun, Marchal, & Nijssen, 2010), DNA restriction
maps construction (Yap & Chuan, 1993), pair-wise multiple sequence alignment (Yang, 1998; Yap, 2001; Tsai, Huang, Yu, & Lu, 2004). Constraint methodologies
applied biological networks (Corblin, Trilling, & Fanchon, 2005; Larhlimi &
Bockmayr, 2009; Ray, Soh, & Inoue, 2010; Gay, Fages, Martinez, & Soliman, 2011; Gebser,
Schaub, Thiele, & Veber, 2011) biological inference problems, Haplotype inference (Graca, Marques-Silva, Lynce, & Oliveira, 2011; Erdem & Ture, 2008),
phylogenetic inference (Erdem, 2011).
particular area Bioinformatics witnessed extensive use CP techniques
domain structural biologyi.e., branch molecular biology biochemistry
deals molecular structure nucleic acids proteins, structure
affects behavior functions. Constraint Programming progressively gained pivotal
role providing effective ways explore space conformations macromolecules,
address problems like secondary tertiary structure prediction, flexibility, motif discovery, docking (Backofen, Will, & Bornberg-Bauer, 1999; Krippahl & Barahona, 2002;
Thebault, de Givry, Schiex, & Gaspin, 2005; Dal Palu, Dovier, & Pontelli, 2007; Mann
& Dal Palu, 2010; Shih & Hwang, 2011; Krippahl & Barahona, 2005; Dal Palu, Spyrakis,
& Cozzini, 2012b; Chelvanayagam, Knecht, Jenny, Benner, & Gonnet, 1998; Yue & Dill,
2000). Two comprehensive surveys use constraint-based methods structural
Bioinformatics recently proposed (Dal Palu, Dovier, Fogolari, & Pontelli, 2012a;
Barahona & Krippahl, 2008).
focus work use constraint-based technology support structural
studies proteins. Proteins macromolecules fundamental importance way
regulate vital functions biological processes. structural properties critical
determining biological functions proteins (Skolnick, Fetrow, & Kolinski, 2000; Baker
& Sali, 2001) investigating protein-protein interactions, central virtually cellular processes (Alberts, Johnson, Lewis, Raff, Roberts, & Walter, 2007).
refer Protein Structure Prediction (PSP) problem problem determining
tertiary structure protein knowledge primary structure and/or knowledge structures (e.g., secondary structure components, templates homologous
proteins). PSP problem also often broken specialized classes problems
related specific aspects tertiary structure protein, side-chain geometry
954

fiA Constraint Solver Flexible Protein Models

prediction (Dunbrack, 2002), loop modeling prediction (Go & Scheraga, 1970; Xiang, Soto,
& Honig, 2002; Rufino, Donate, Canard, & Blundell, 1997; Soto, Fasnacht, Zhu, Forrest, &
Honig, 2008), protein flexibility investigation (Bennett & Huber, 1984).
classes problems share common rootsthe need track possible conformations chains amino acids. variations problem relate factors like
length chain considered (from short peptides case loop modeling
entire proteins general PSP case) diverse criteria employed selection
solutions, as, instance, lowest basin effective energy surface, composed
intra-molecular energy protein plus solvation free energy (Karplus &
Shakhnovich, 1992; Lazaridis, Archontis, & Karplus, 1995).
Modeling variability protein chain involves many degrees freedom
needed represent different protein conformations. Tracking variability requires
exploration vast conformational space. Model simplifications adopted reduce
computational cost, instance backbone-only models represent backbone
proteins, side-chain representation could simplified single central point (centroid)
describing center mass, one adopt approximated representation space
though lattice models.
Nevertheless, even strong simplifications, search space remains intractable
prevents use brute-force search methods space possible conformations
(Crescenzi, Goldman, Papadimitriou, Piccolboni, & Yannakakis, 1998).
Constraint programming methodologies found natural use addressing PSP
related problemswhere structural chemical properties modeled terms
constraints spatial positions atoms, transforming search conformations
constraint satisfaction/optimization problem. proposed approaches range
pure ab initio methods (Backofen et al., 1999; Dal Palu et al., 2007) methods based
NMR data (Krippahl & Barahona, 1999) methods based fragments assembly (Dal
Palu, Dovier, Fogolari, & Pontelli, 2010). spite efforts, design effective
approaches filter space conformations lead feasible search remains
challenging open problem.
work present constraint solver targeted modeling general class protein
structure studies. particular solution suitable address protein structure analysis
study, requiring generation set unbiased sampled diverse conformations
satisfy certain given restraints. One unique features solution presented
work capability generate uniformly distributed sampling target protein regions
among given portion Cartesian space selected granularityaccounting
spatial rotational properties.
abstract problem general multi-body system, composing body
constrained means geometric properties related bodies joint
relationships. body represent entity protein, individual amino
acid small peptide (e.g., protein fragment). Bodies relate spatial positions
organization individual atoms composing it.
view exploration protein structures multi-body systems suggests number different constraints, used model different classes structural studies
applied filter infeasible (or unlikely) conformations. propose investigation
several classes constraints, terms theoretical properties practical use
955

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

filtering. Particular emphasis given Joined-Multibody (JM) constraint, whose
satisfaction prove NP-complete. Realistic protein models require assembly
hundreds different body versions, making problem intractable. study efficient
approximated propagator, called JM filtering (JMf), allows us efficiently compute
classes solutions, partitioned structural similarity controlled tolerance error.
perspective novel holds strong potential. structural problems investigating computationally intractable; use global constraints specifically designed
meet needs enables effective exploration search space greater
potential effective approximations.
multi-body model provides interesting perspective exploring space
conformationswhile actual search operates discrete sets alternatives (e.g., sets
fragments), filtering process avails reasoning processes operates continuous
domain; allows propagation filtering effective.
proposed multi-body constraints filtering techniques constitute core
resolution engine FIASCO (Fragment-based Interactive Assembly protein Structure
prediction Constraints), efficient C++-based constraint solver. demonstrate
flexibility efficiency FIASCO using engine model solve class
problems derived loop modeling instances. Throughout paper show
ability FIASCO providing uniform efficient modeling platform studying
different structural properties (that been, far, addressed using significantly
distinct methods tools). declarative nature constraint-based methods supports
level elaboration tolerance offered frameworks protein structure
prediction, facilitating integration additional knowledge guiding studies (e.g.,
availability information secondary structure elements).
rest paper organized follows. Section 2, provide high-level
background biological chemical properties proteins review commonly used approaches address structural studies. Section 3, develop constraint
framework dealing fragments multi-body structures. Section 4 describes
implementation constraints propagation schemes FIASCO system.
Section 5 report experimental results using FIASCO collection
benchmarks loop modeling. Section 6 provides concluding remarks.
preliminary version research pursued paper presented (Campeotto,
Dal Palu, Dovier, Fioretto, & Pontelli, 2012). work Campeotto et al. focused
one new class constraints targeting problem loop closure, work presented
paper provides comprehensive constraint system, focused modeling structural
protein properties investigating different types problems (e.g., structure prediction,
studies flexibility). present manuscript includes also precise detailed
formalization extensive experimentation comparison.

2. Background, General Context, Related Work
section briefly review basic Biology notions, introduce problems
tackling paper refer selection related literature.
956

fiA Constraint Solver Flexible Protein Models

H

side
chain

N

C

H

H

N
C'

H
C


C'
side
chain



Figure 1: schematic sequence two amino acids showing amino acid backbone
side chains. arrow C 0 N denotes peptidic bond.
2.1 General Background
protein molecule made smaller building blocks, called amino acids. One amino acid
connected another one peptidic bond. Several amino acids pairwise
connected linear chain forms whole protein. backbone protein,
illustrated Figure 1, formed sequence N C C 0 atoms contained amino
acid. backbone rather flexible allows large degree freedom protein.
amino acid characterized variable group atoms influences specific
physical chemical properties. group, named side chain, ranges 1 18 atoms
connects C atom amino acid. 20 kinds amino acids found
common eukaryotic organisms.
Proteins made 10 1, 000 amino acids, average globular protein
300 amino acids long. amino acid contains 724 atoms, therefore number
atoms arrangements space grow easily beyond computational
power. Since beginning protein simulation studies, different algorithms exploring conformations devised, molecular dynamics, local search, Monte
Carlo, genetic algorithms, constraint approaches, well different geometric representations (Neumaier, 1997).
literature, several geometric models proteins proposed. One choice
influences quality complexity computational approaches number
points describe single amino acid.
simplest representation one amino acid represented one
point, typically C atom, given robust geometric property: distance
C atoms two consecutive amino acids preserved low variance (roughly 3.81A).
Usually, volumetric constraints enforced points, order simulate
average occupancy amino acid. representation visualized chain
beads moved space.
refined representation models store (or all) points backbone, plus
centroid mass (CG) represents whole side chain connects C atom.
models, amino acid described different C CG distances CG volumes.
centroid approximation side-chain flexibility allows refined
energetic models, number points taken care still low. paper

957

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Figure 2: native structure intact influenza virus M1 protein (indexed 1EA3
PDB) modeled full atom, 5@ model, simple C C model
(from left right). secondary structures (-helices) emphasized.

Figure 3: Amino acid concatenation 5@ model
use particular case simplified models, 5@ model, described precisely
below. particular instance coarse-grained protein models (Clementi, 2008;
Shehu, 2010). end spectrum, atom amino acid represented
one point. representation accurate, time allows
accurate energetic considerations. drawback computational demand
handling backbone side-chain flexibility increases significantly.
Figure 2 report three representations protein.
paper select intermediate representation amino acids atoms
N, C , C 0 backbone centroid side chain (CG) accounted for.
also include oxygen (O) atom attached C 0 atom, atom together
C 0 N identifies triangle chemically stable along backbone used
assembly amino acids (see complete formalization). position
two H atoms backbone deduced position atoms
deal explicitly. conclusion, deal 5 atomic elements per amino
acid: 4 atoms N C C 0 centroid CG. briefly refer representation
5@ model. Figure 3 illustrates atoms involved concatenation
two consecutive amino acids. Inter-atomic distances consecutive atoms fixed
due chemical bonds; thus, differences structures identified
differences angles involved. common find substructures

958

fiA Constraint Solver Flexible Protein Models

protein consecutive amino acids arranged according repeated characteristic
patterns. property found almost every protein; refer typical patterns
secondary structure elements. common examples -helices -sheets
(see Figure 2).
2.2 Context Proposed Work
paper present tool assembling reasoning amino acids
space. similar approaches (e.g, Simons, Kooperberg, Huang, & Baker, 1997),
system relies set admissible elementary shapes (or fragments) represents
spatial dictionary arrangements every part protein.
element dictionary general enough describe specific atomic structure
either single amino acid longer sequence (even hundreds amino acids long).
amino acid sequence, several alternative arrangements expected populate
database, offer various hypothesis local shape sequence.
protein partitioned contiguous fragments arranged according one
possible shapes recorded database.
sequence amino acids free rotate bonds space (typically two degrees
freedom along backbone several others along side chain); however, due
chemical properties physical occupancy specific types amino acids
involved surrounding environment, arrangements impossible and/or unlikely. core assumption assembling approaches rely statistical database
arrangements describe local feasible behavior, order direct search candidates high probability energetically favorable. presence multiple
candidate fragments every part protein requires combinatorial search among
possible choices that, assembled together, leads alternative putative configurations
protein. search process charge verifying feasibility assembly,
since combination local arrangements could generate non-feasible global shape, e.g.,
one leads spatial clash atoms different fragments. one (or more)
fragment described one single arrangement, part protein rigidly imposed.
particular degenerate case exploited describe rigid parts protein.
specific combination fragment length number instances fragment determines type protein problem modeled. range complete backbone
flexibility (fragments made hundreds choices amino acids) secondary structure - loop models (interleaving longer fragments modeling helices/-strands shorter
fragments).
library fragments usually derived content Protein Data Bank
(PDB, www.pdb.org) contains 96,000 protein structures. design adopted
study parametric choice library fragments use. example,
experiments use library fragments derived subset PDB known
top-500 (Lovell, Davis, Arendall, de Bakker, Word, Prisant, Richardson, & Richardson,
2003), contains non redundant proteins preserves statistical relevance. Alternative libraries fragments obtained use sophisticated protein database
search algorithms, FREAD (Choi & Deane, 2010). retrieve information depending specific amino acid sequence, since local properties greatly influence typical

959

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

arrangements observed. Moreover, build libraries different sequences lengths h, even
longer sequences statistical coverage becomes weak. Nevertheless, Micheletti, Seno,
Maritan (2000) conjectured relatively small set fragment shapes (a dozens)
length 5, able describe virtually protein. Handl, Knowles, Vernon, Baker,
Lovell (2012) demonstrate size structure search space affected
choice fragment length used optimize search process. Similar considerations explored others (Hegler, Latzer, Shehu, Clementi,
& Wolynes, 2009). Recent work show efficiently build dictionaries (Fogolari,
Corazza, Viglino, & Esposito, 2012). models easily accommodated
framework.
considered sequence associated several configurations 5@ models, placed
according standardized coordinate system. activity, also consider C 0
group preceding amino acid N atom following amino acid.
extra information needed fragments combination, assuming fragment
connected two peptidic bonds. Therefore, specific sequence, store
occurrences
C 0 N C C 0 N
| {z }
h times
relative positions. order reduce impact specific properties
database used, cluster set way two fragments RMSD1 less
given threshold, one stored. example, length h = 1
RMSD threshold .2A, derive fragment database roughly 90 fragments per
amino acid.
CG information added later using statistical considerations side-chain mobility, accounted clustering described (Fogolari, Esposito,
Viglino, & Cattarinussi, 1996).
2.3 Protein Structure Prediction
protein structure prediction problem, sequence amino acids composing protein (known primary structure) given input; task predict three
dimensional (3D) shape (known native conformation tertiary structure)
protein standard conditions.
common assumption, based Anfinsens work (1973), 3D structure
minimizes given energy function modeling atomic force fields, candidate best approximates functional state protein. setting, choice
number atoms used represent amino acid controls quality
computational complexity.
Moreover, spatial domains proteins points (e.g., atoms, centroids)
placed impact type algorithms search performed.
domain either continuous, often represented floating point coordinates,
discrete, often derived discretization space based crystal lattice structure.
1. Root Mean Square Deviation captures overall similarity space corresponding atoms,
performing optimal roto-translation best overlap two structures.

960

fiA Constraint Solver Flexible Protein Models

geometric model determined, necessary introduce energy
function, mostly based atoms considered distances. structure prediction problem, energy function used assign score geometrically feasible
candidate; candidate optimal score represents solution prediction
problem.
Let us briefly review popular approaches problem, particular emphasis solutions rely constraint programming technology.
natural approach investigating protein conformations simulations
physical movements atoms molecules is, unfortunately, beyond current computational capabilities (Jauch, Yeo, Kolatkar, & Clarke, 2007; Ben-David, Noivirt-Brik, Paz,
Prilusky, Sussman, & Levy, 2009; Kinch, Yong Shi, Cong, Cheng, Liao, & Grishin, 2011).
originated variety alternative approaches, many based comparative modelingi.e., small structures related protein family members used templates
model global structure protein interest (Jones, 2006; Fujitsuka, Chikenji, &
Takada, 2006; Simons et al., 1997; Lee, Kim, Joo, Kim, & Lee, 2004; Karplus, Karchin,
Draper, Casper, Mandel-Gutfreund, Diekhans, & Source., 2003). methods, often
referred fragments assembly, protein structure assembled using small protein subunits templates present relevant sequence similarities (homologous affinity) w.r.t.
target sequence.
literature, Constraint Programming (CP) techniques shown potential:
structural variability protein modeled constraints, constraint solving
performed order deduce optimal structure (Backofen & Will, 2006; Barahona
& Krippahl, 2008; Dal Palu, Dovier, & Fogolari, 2004; Dal Palu et al., 2010). CP
used provide approximated solutions ab-initio lattice-based modeling protein
structures, using local search large neighboring search (Shmygelska & Hoos, 2005;
Dotu, Cebrian, Van Hentenryck, & Clote, 2011); exact resolution problem lattice
spaces using CP, along clever symmetry breaking techniques, also investigated (Backofen & Will, 2006). approaches solve constraint optimization problem
based simple energy function (HP). precise energy function used
Dal Palu et al. (2004, 2007), information secondary structures (i.e., -helices,
-sheets) also taken consideration. Due approximation errors introduced
lattice discretization, approaches scale medium-size proteins. Off-lattice
models, based idea fragment assembly, implemented using Constraint Logic
Programming Finite Domains, presented (Dal Palu et al., 2010; Dal Palu,
Dovier, Fogolari, & Pontelli, 2011), applied structure prediction also
structural analysis problems. instance, Dal Palu et al. (2012b) use approach generate sets feasible conformations studies protein flexibility. use
CP analyze NMR data related problem protein docking also
investigated (Barahona & Krippahl, 2008).
context ab-initio prediction, recent work (Olson, Molloy, & Shehu, 2011)
shown increasing complexity conformational search spaceby using
refined fragment libraryin combination sampling strategy, enhances
generation near-native structure sets. work Shehu (2009) Molloy, Saleh,
Shehu (2013) illustrates various enhancement fragment-based assembly process leading
faster computations improved sampling conformation spacee.g., using
961

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

tree-based methods inspired motion planning guarantee progress towards minimal
energy conformations maintaining geometrically separate conformations. terms
energy landscape, native state generally lower free energy non-native structures,
extremely difficult locate. Hence, targeted conformational sampling may aid
protein structure prediction different near-native structure used guide
search; several schemes based Monte Carlo movements sampling conformation
space fragments assembly proposed (Shmygelska & Levitt, 2009; Xu &
Zhang, 2012; Debartolo, Hocky, Wilde, Xu, Freed, & Sosnick, 2010). Methods based
non-uniform probabilistic mass functions (derived previously generated decoys)
proposed aid problem (Simoncini, Berenger, Shrestha, & Zhang, 2012).
Sampling, however, remains great challenge protein complex topologies and/or
large sizes (Kim, Blum, Bradley, & Baker, 2009; Shmygelska & Levitt, 2009).
widely accepted proteins, native state, considered dynamic
entities instead steady rigid structures. Indeed, recent years research focus
shifted towards prediction schemes take account non-static nature proteins,
supported recent observations based magnetic resonance techniques. Processes
enzyme catalysis, protein transport antigen recognition rely ability proteins
change conformation according required conditions. dynamic nature
visualized set different structures coexist time. generation
sets capture non-redundant structures (in pure geometric terms) great
challenge (Kim et al., 2009). Robotics inverse kinematics methods extensively
explored sampling proteins conformational space (Zhang & Kavraki, 2002; Cortes
& Al-Bluwi, 2012) molecular simulations (Al-Bluwi, Simeon, & Cortes, 2012; Moll,
Schwarz, & Kavraki, 2007; Noonan, OBrien, & Snoeyink, 2005; Kirillova, Cortes, Stefaniu,
& Simeon, 2008).
motivation work provide ability generating protein set
contains optimal sub-optimal candidates, order capture dynamic information
behavior protein. desirable property conformations returned
pool sufficiently diverse uniformly distributed 3D space.
2.4 Protein Loop Modeling
protein loop modeling problem restricted version structure prediction problem. use problem working example remaining part paper.
context, protein structure already partially defined, e.g., large number
atoms already placed space. Usually, common scenario derives Xray crystallography analysis, spatial resolution atoms degenerates presence
regions protein exposed surface presents increased
instability. Since crystal contains several copies protein order perform measurement, regions appear fuzzy, therefore placement atoms
regions may ambiguous. Usually, regions, referred loops, involved
secondary structures, instead stable. dealing homology modeling, protein found another organism, typically shows variations
sequence due evolution, especially loop regions, since less essential
protein stability functionality. Starting homologous protein structure, usually

962

fiA Constraint Solver Flexible Protein Models

loops need recomputed specialized loop modeling approach use
minimization techniques.
length loop typically range 2 20 amino acids; nevertheless,
compared secondary structures, flexibility loops produces large, physically
consistent, conformation search spaces. Constraints mutual positions orientations (dihedral angles) loop atoms deduced used simplify search.
restrictions defined loop closure constraints. Figure 2, (simple)
possible scenario two macro-structures (two helices) connected loop.
setting, assume know position two helices, loop atoms
determined.
procedure protein loop modeling typically consists 3 phases: sampling, filtering,
ranking (Jamroz & Kolinski, 2010). Sampling commonly based loop candidate generation, using dihedral angles sampled structural databases (Felts, Gallicchio,
Chekmarev, Paris, Friesner, & Levy, 2008), subsequent candidate modification order
satisfy loop closure constraints. conformations checked w.r.t. loop constraints geometries rest structure, loops detected
physically infeasible, e.g., causing steric clashes, discarded filtering procedure.
Popular methods used loop modeling include Cyclic Coordinate Descent (CCD)
method (Canutescu & Dunbrack, 2003), algorithms based inverse kinematics (Kolodny,
Guibas, Levitt, & Koehl, 2005; Shehu & Kavraki, 2012), Self-Organizing (SOS) algorithm (Liu, Zhu, Rassokhin, & Agrafiotis, 2009), simultaneously satisfy loop
closure steric clash restrictions iteratively superimposing small fragments (amide
C ) adjusting distances atoms, Wriggling method (Cahill, Cahill,
& Cahill, 2003), employs suitably designed Monte Carlo local moves satisfy loop
closure constraints. Multi-method approaches also proposede.g., Lee, Lee,
Park, Coutsias, Seok (2010) propose loop sampling method combines fragment
assembly analytical loop closure, based set torsion angles satisfying imposed
constraints. Ab initio methods (Rapp & Friesner, 1999; Fiser, Do, & Sali, 2000; Jacobson,
Pincus, Rapp, Day, Honig, Shaw, & Friesner, 2004; Spassov, Flook, & Yan, 2008; Deane
& Blundell, 2001; Felts et al., 2008; Xiang et al., 2002) methods based templates
extracted structural databases (Choi & Deane, 2010) explored.
Finally, ranking stepe.g., based statistical potential energy, like DOPE (Shen
& Sali, 2006), DFIRE (Zhou & Zhou, 2002), one proposed Fogolari et al. (2007),
used select best loop candidates.
sampling filtering procedures work together direct search towards structurally diverse admissible loop conformations, order maximize
probability including candidate close native one reduce time needed
analyze candidates. work motivated need controlling properties resulting set candidates. particular, model structural diversity
distance orientation backbone make sampling phase guided loop
constraints.
Fragment-based assembly methods also investigated context loop
modeling (Lee et al., 2010; Zhang & Hauser, 2013). Shehu Kavraki (2012) review
great detail loop modeling techniques.

963

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Figure 4: left: two fragments B1 (light grey) B2 (dark grey)
points(B1 ) = ((0, 0), (1, 0), (1, 1), (2, 1)) points(B2 ) = ((4, 0), (3, 0), (3, 1), (4, 1), (4, 2)).
arrows address initial points. right: observe rotating B2 90 degrees translating -3 units x-axis, last three points B1 (last(B1 ))
first three points B2 (first(B2 )) perfectly overlap. Thus, end(B1 ) _ front(B2 ).

3. Constraint Solving 3D Fragments
assume reader familiarity basic principles constraint programming constraint satisfaction problems (CSP); reader referred, e.g., Handbook Constraint Programming (Rossi et al., 2006). Section, introduce
formalization effective solution tackle practical applications concerning
placement 3D fragments. applications described combinatorial problems,
modeled set variables, representing entities problem deals with, set
constraints, representing relationships among entities. context constraint
programming system, variables constraints adopted provide solution
CSP, is, assignment variables satisfies constraints. extend
concept enabling constraint solver find representative solution CSP
satisfies additional properties expressed among variables whole solution set.
3.1 Terminology
fragment B composed ordered list least three (distinct) 3D points, denoted
points(B). number points fragment referred length. front-
end-anchors fragment B, denoted front(B) end(B), two lists containing
first three last three points points(B). B(i) denote i-th point
fragment B. two ordered lists points p~ ~q, write p~ _ ~q
perfectly overlapped rigid coordinate translation and/or rotation (briefly, rototranslation)see Figure 4 (let us assume z coordinate 0 points omitted
simplicity).
non-empty set fragments length called body. body
used model set possible shapes sequence points. say body
length k fragment contains length k.
multi-body sequence S1 , . . . , Sn bodies.

964

fiA Constraint Solver Flexible Protein Models

Figure 5: left right: body S1 composed unique fragment, bodies
S2 S3 composed two fragments each. Arrows address initial points fragments.
~ = S1 , S2 , S3 constitutes multi-body. rightmost
three bodies length 4.
figure report spatial shapes associated four rigid bodies obtained
~ One identified full lines, three dashed
multi-body S.
lines. Observe rigid body identified ((0, 0), (1, 0), (1, 1), (2, 1), (2, 0), (3, 0))
obtained rotation 180 degrees fragment ((2, 0), (3, 0), (3, 1), (4, 1)) S2
x axis (flipping) translation 1 units x +1 units y. Observe
moreover rigid body identified ((0, 0), (1, 0), (1, 1), (2, 1), (2, 0), (1, 0)) contains
point (1, 0) twice.
~ = S1 , . . . , Sn , rigid body
~ sequence fragments
Given multibody
B1 , . . . , Bn , Bi Si = 1, . . . , n end(Bi ) _ front(Bi+1 ), = 1, . . . , n1.
rigid body uniquely identified sequence B1 , . . . , Bn ; however, consecutive
fragments overlapped, rigid body alternatively identified list points
form spatial shape. Figure 5 report examples bodies, multi-bodies, rigid
bodies. previous example, assume z coordinate 0 points.
Remark 3.1 (Working Example) concepts related loop-modeling problem. Points atoms. fragment spatial shape atoms. last three atoms
one fragment overlap first three atoms another fragment, join them.
body set admissible shapes given list atoms. multi-body S1 , . . . , Sn
sequence elements, corresponding sequence atoms (of amino acids).
idea last three atoms body Si first three successive
body Si+1 . rigid body possible complete shape atoms, provided last three
atoms fragment selected set Si overlap first three atoms fragment
selected Si+1 .
overlapping points end(Bi ) front(Bi+1 ) constitute i-th joint rigid
body. number rigid bodies obtained single multi-body S1 , . . . , Sn
bounded ni=1 |Si |. Figure 6 provides schematic general representation rigid
body.
rigid body defined overlap joints, relies chain relative rototranslations fragments. points points(Bi ) therefore positioned according
(homogeneous) coordinate system associated fragment Bi1 . Note
reference system B1 defined, whole rigid body completely positioned.2
2. exception case points joint collinear. Points p1 , . . . , pn , n 3
collinear points p3 , p4 , . . . , pn belongs straight line containing two points p1 p2 .

965

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Figure 6: schematic representation rigid body. joints connecting two adjacent
fragments emphasized. points points(B) fragment represented
circles. fragment extends first point joint last point
successive joint.
relative positions two consecutive fragments Bi1 Bi rigid body (2 n)
defined transformation matrix Ti R44 . matrix depends standard
Denavit-Hartenberg parameters (Hartenberg & Denavit, 1995) obtained start
end fragmentsthe reader referred work LaValle (2006) details.
denote product T1 T2 . . . Ti (x, y, z, 1)T Ti (x, y, z).
Let us analyze first matrix T1 . fragment B1 forced start given
point oriented given way; case matrix T1 defines roto-translation
B1 fulfilling constraints. absence constraints, assume B1
normalized T1 i.e.,its first point (0, 0, 0), second point aligned along z axis
third belongs plane formed x z axes. orientation referred
reference system 0 .
= 1, . . . , n, coordinate system conversion (x0 , 0 , z 0 ), point (x, y, z)
points(Bi ) coordinate system B1 , obtained by:
(x0 , 0 , z 0 , 1)T = T1 T2 . . . Ti (x, y, z, 1)T = Ti (x, y, z)

(1)

Homogeneous transformations last value tuple always 1.
rest paper, focus 5@ model; however proposed formalization
methods used also models, e.g., C C model. latter
case, points(Bi ) contains least 3 amino acids, joints guaranteed noncolinear, due chemical properties backbone. combining C fragments,
specific rotational angles full-atom backbone lost imprecise multibody assembly produced.
fragment body associated sequence amino acids. fragment sequence
h 1 amino acids described body length 4h + 3, modeling concatenation
atoms represented regular expression: C 0 O(N C C 0 O)h N . representation first last sequence C 0 atoms coincide front- end-anchor,
respectively, employed process assembling consecutive fragments (i.e.,
used roto-translation).
discretized R3 space represented regular lattice, composed cubic cells
side length equal given parameter k. cell referred 3D voxel
(or, simply, voxel ); assume voxel receives unique identifier. denote
voxel(p, k) identifier voxel contains 3D point p context
discretization space using cubes side length equal k. spatial quantization
allows efficient treatment approximated propagation required
geometric constraints introduced following sections.
966

fiA Constraint Solver Flexible Protein Models

3.2 Variables Domains
Let us define variables adopted describe entities problem fragments.
domain variable V set allowable values V , denoted
DV . deal fragments placements 3D space adopt two distinct types
variables:
Finite Domain Variables (FDVs): domain finite domain variable finite
set non negative integer numbers.
Point Variables (PVs): variables assume coordinates 3D point R3 .
domains are, initially, 3D boxes identified two opposite vertices hmin, maxi,
done discrete solver COLA (Dal Palu, Dovier, & Pontelli, 2005, 2007).
Remark 3.2 (Working Example) Following Remark 3.1, FDVs identifiers
various fragments body, PVs used represent 3D coordinates assigned
various structural points (e.g., atoms, centroids) interest molecule
considered. Clearly, values PVs depend deterministically values FDVs
(and vice-versa).
variable assigned domain contains unique value; case point variables,
happens DV = hmin, maxi min = max.
3.3 Constraints
section, formalize constraints define fragments placement,
used describe Protein Structure problems context fragment assembly.
3.3.1 Distance Constraints
Distance constraints model spatial properties point variables operating 3D space.
Point variables P Q related distance constraint form
kP Qk op

(2)

k k Euclidean norm, R+ op .
built-in global constraint alldistant associates minimal radius di point
variable Pi (i = 1, . . . , n) ensures spheres surrounding pair point variables
intersect:
alldistant(P1 , . . . , Pn , d1 , . . . , dn ),
(3)
constraint equivalent constraints kPi Pj k di +dj i, j {1, . . . , n}, <
j. used avoid steric clashes among different atoms (and centroids),
different volumes. Checking consistency alldistant constraint (given domains
variables Pi ) NP-complete (Dal Palu, Dovier, & Pontelli, 2010)the proof based
encoding bin-packing problem using alldistant constraint, holds true
even particular setting, point variables intervals R3 domains.
Remark 3.3 (Working Example) alldistant constraint introduced avoid clashes
rigid body obtained multi-body S1 , . . . , Sn . distance constraints
967

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Figure 7: Fragments assembled overlapping plane R , described rightmost
C 0 , O, N atoms first fragment (left), plane L , described leftmost
C 0 , O, N atoms second fragment (right), common nitrogen atom
useful extra information known (e.g., one might inferred biological
arguments pair amino acid stay within certain distance).
3.3.2 Fragment Constraint
Fragment constraints relate finite domain variables point variables. Let us assume
database F fragments, F [i] represents i-th fragment database.
Thus, given FDV variable V , F [V ] denotes fragment indexed V V
instantiated. fragments stored F ordered list 3D points.
Given list point variables P~ , constraint:
fragment(V, P~ , F )

(4)

states exists roto-translation Rot P~ = Rot F [V ]namely, V =
list points P~ take form fragment F [i]. simplicity,
omit database F clear context. Intuitively, constraints ensure
fragment choice reproduce correct shape associated 3D point,
regardless space orientation fragment. orientation determined
joined multi-body constraint presented following section.
3.3.3 Centroid Constraint
centroid constraint enforces relation among four PVs. Intuitively, first three
associated atoms N, C , C 0 amino acid fourth related
centroid CG. constraint parametric w.r.t. type amino acid
deterministically establishes position CG depending position points:
centroid(PN , PC , PC 0 , PCG , a)

(5)

Figure 7 centroids displayed along backbone purple circles labeled
CG. constraint used database fragment contains full backbone information. centroid information used place missing full-atom side
chain. side-chain centroid computed taking account average C -side-chain
center mass distance, average bend angle formed side-chain center-of-massC -C 0 , torsional angle formed N -C -C 0 -side-center mass (Fogolari et al.,
968

fiA Constraint Solver Flexible Protein Models

1996). abstraction allows us reduce number fragments consider, removing
fragments would geometrically conflict position CG. Consider
single side chain may 100 main configurations (rotamers).
3.3.4 Table Constraint
constraint used restrict assignments set FDVs (representing fragments)
specific tuples choices. useful modeling specific local collaborative
behavior involves one fragment; example, happens modeling
secondary structure multiple arrangements underlying amino acids and/or specific
approximation strategies employed.
~ k-tuple FDVs. table (or
Let F set k-tuples integer values V
combinatorial) constraint, form
~ ,F)
table(V

(6)

~ assumes values restricted tuples listed F , i.e.,
requires list variables V
~
exists F V [i] = t[i], 0, . . . , k 1.
Remark 3.4 (Working Example) Going back loop-modeling problem, role
fragment constraint evident: relates (IDs the) selected fragments multibody 3D positions various atoms involved. centroid constraint
instead introduced add position centroid represents side chain
5@ representation. table constraint common constraint constraint languages
useful info consecutive fragments rigid body known due external
knowledge.
3.3.5 Joined Multibody Constraint
Joined Multibody (JM) constraint enforces relation list FDVs encoding
multibody. limits spatial domains various fragments composing multibody
order retain fragments assemble properly compenetrate.
~ V
~ , A,
~ E,
~ i, where:
joined-multibody (JM) constraint described tuple: J = hS,
~ = S1 , . . . , Sn multi-body. Let B = {B1 , . . . , Bk } set fragments S,
~

Sn
i.e., B = i=1 Si .
~ = V1 , . . . , Vn list FDVs, domains DVi = {j : Bj Si }.
V
~ = A1 , A2 , A3 , E~ = E1 , . . . , E3n lists sets 3D points that:

A1 A2 A3 set admissible points front(B), B S1 ;
E3i2 E3i1 E3i set admissible points end(B), B Si , = 1, . . . , n;
constant, used express minimal distance constraint different point.
~ {1, . . . , |B|} s.t. exist
solution JM constraint J assignment : V
matrices T1 , . . . , Tn (used ) following properties:
Domain: = 1, . . . , n, (Vi ) DVi .
Joint: = 1, . . . , n 1, let (a1 , a2 , a3 ) = end(B(Vi ) ) (b1 , b2 , b3 ) = front(B(Vi+1 ) ),
holds (for j = 1, 2, 3):
Ti (ajx , ajy , ajz ) = Ti+1 (bjx , bjy , bjz )
969

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Spatial Domain: Let (a1 , a2 , a3 ) = front(B(V1 ) ), T1 aj Aj {1}.3 =
1, . . . , n, let (e1 , e2 , e3 ) = end(B(Vi ) )
Ti (ejx , ejy , ejz ) E3(i1)+j {1}
1 j 3 T2 , . . . , Ti (in Ti ) matrices overlap end(B(Vi1 ) )
front(B(Vi ) )
Minimal Distance: j, ` = 1, . . . , n, j < `, points points(B(Vj ) )
b points(B(V` ) ), holds that:4
kTj (ax , ay , az ) T` (bx , , bz )k
proved establishing consistencyi.e., existence solutionof JM
constraints NP-complete (Campeotto et al., 2012). also proved remains
NP complete even assuming fragments problem three
atoms spatial position, holds last three atoms (of
course fragments allowed contain three atoms otherwise problem
trivial). proof reported www.cs.nmsu.edu/fiasco/.
Remark 3.5 (Working Example) JM constraint contains exactly ingredients
~ corresponding FDs
needed modeling loop problem. multi-body S,
~ , set possible 3D points loop starts
~ set possible 3D
V
~
points loop ends E weak version alldistant constraint pair
~
atoms avoid clashes, solutions (non clashing) rigid bodies starts
~
ends E.
Let us observe JM constraint explicitly forbid spatial positions PVs
variables (save first three last three points loop). However,
additional constraints explicitly required domain definition PVs variables
used encoding.
Remark 3.6 choice using three points overlap resembles method proposed
Kolodny, Guibas, Levitt, Koehl (2005). hand, observe
technical exercise modify JM constraints allow parametric
overlap contiguous fragments.

4. FIASCO Constraint Solver
present overall structure implementation hybrid constraint solver capable
handling classes constraints described previous section.
4.1 Constraint Solving
distinctive feature FIASCO possibility handle continuous domains cost
keeping discrete library choices (finite domain variables). handling fragments
allows us reason spatial properties efficient descriptive way
pure 3D domain modeling adopted previous proposals. Moreover, FIASCO allows
3. product {1} necessary use homogeneous coordinates.
4. Let us observe weak form alldistant constraint different distances
point allowed. is, sense, closer alldifferent constraint.

970

fiA Constraint Solver Flexible Protein Models

solver uniformly sample search space means spatial equivalence relation
used control tradeoff accuracy efficiency. particularly
effective finite domains heavily populated, critical component
model real-world problems.
constraint solver builds classical prop-labeling tree exploration constraint propagation phases interleaved non-deterministic branching phases used
explore different value assignments variables (Apt, 2009). solver able handle
point variables finite domain variablesthis reason refer
hybrid solver. particular, assignments finite domain variables guide search;
values imply assignments point variables, turn may propagate reduce
domains point variables finite domain variables. Moreover, propagation technique implemented JM constraint classical filtering techniqueit
approximated technique describe later.
presence point variables allows, principle, infinite number domain values
R3 . However, noted information carried assembling fragments (encoded
finite domain variables) much informative complex demanding
model 3D continuous space (e.g., Oct-trees, CSG, no-goods). particular, direct
kinematics encoded JM constraint able efficiently identify set admissible
regions point variable fast, approximated, controlled way. Therefore,
point variables seen internal aid propagation. variables updated
JM propagation phase interact JM propagator prune
corresponding fragment variables. Distance constraints point variables included
standard AC3 propagation loop domains updates.
aspect extends classical solver structure capability controlling amount search tree explored. search tree contains large number
branches similar, point view geometric distance
corresponding point variables. goal produce subset feasible solutions
exhibit significant 3D differences themselves. accomplished introducing
possibility explore subtree given depth, enumerating specific limited
number branches, rather following standard recursion propagation expansion. achieve behavior, necessary selectively interfere standard
recursive call solver, implement non-deterministic assignment partial tuples
finite domain variables. resembles implementation table constraint,
dynamically created search. strategy allows us significantly reduce
number branches explored subtree, produces significant results
selection branches controlled adequate partitioning function. work,
propose effective partitioning function based measure 3D similarity point
variables; used direct search along specific branches controlled depth
adequately separated partitioning function. practically realized
introducing form look-ahead, controlled JM propagator, returns set
partial assignments well filtered domains finite domain variables.
4.1.1 Hybrid Solver

971

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

~ , P~ , D,
~ C, `)
Algorithm 1 search(V
~ , P~ , D,
~ C, `
Require: V
~ |
1: ` > |V
2:
output (P~ )
3:
return
4: end
5: fragment index f Dv`
~ , P~ , D)
~
6:
AC-3(C {v` = f }, V
nm
7:

get table JM()
8:
n > 0
9:
Non-deterministically select 1..n
10:
j = 1..m
11:
C C {v`+j = [i][j]}
12:
end
~ , P~ , D,
~ C, ` + m)
13:
search(V
14:
else
~ , P~ , D,
~ C, ` + 1)
15:
search(V
16:
end
17:
end
18: end
general structure solver highlighted Algorithm 1. solver designed
~ = v1 , . . . , vn finite domains variables, together domains
process list V
Dv1 , . . . , Dvn them. Intuitively, domain set indices set fragments.
Moreover, solver receives list P~ = p1 , . . . , p5n 5 n point variables,
variables p4i , . . . , p4i+4 related fragment domain Dvi . point variable
~
pj has, turn, spatial domain Dpj . C represents constraints elements V
P~ . Finally, solver receives also input current level ` exploration
search tree (set 1 first time procedure called). sake simplicity,
choice variables assigned based ordering input list (more
sophisticated selection strategies easily introduced). enter level `,
assume variables v1 , . . . , v`1 already assigned.
~ already assigned
Let us briefly describe algorithm. variables V
(lines 14), search algorithm terminates returns computed solution, represented values assigned variables P~ . Otherwise, non-deterministically select
fragment index domain variable v` assign variable. Lines 67
indicate execution standard constraint propagation step (using AC-3). propagation step fails, assume another non-deterministic choice made, possible.
Every reference non-deterministic choice algorithm corresponds creation
choice-point target backtracking case failure (for simplicity,
assume chronological backtracking). succeeds, leading possible reduction
~ computation proceed. table constraint might produced
domains D,
propagation JM constraint AC-3 procedure (see details).
case (lines 89), (m) variables non-deterministically assigned
values table (lines 912), search continues less variables
972

fiA Constraint Solver Flexible Protein Models

assigned (line 13). case, search continue one less
variable (v` ) assigned (line 15).
peculiar feature constraint solver (not reported abstract algorithm
defined) used avoid search solutions similar others.
Let us assume 3D space partitioned cubic voxels size k A. Then, given list
~ list PVs P~ , user state:
FDVs V
~ , P~ , k)
uniqueseq(V

(7)

constraint forces solver prune search tree following way. Given
~ variable assigned next step
partial assignment , let v V
~
p1 , . . . , ph P PVs consequently instantiated. constraint ensures
two assignments 1 , 2 extending v, p1 , . . . , ph holds exists least
one {1, . . . , h} 1 (pi ) 2 (pi ) belong voxel.
4.2 Constraint Propagation
section, discuss propagation rules associated various constraints introduced Section 3.3; applied within call AC-3 procedure (line 6
Algorithm 1). constraint propagation used reduce domain size PVs
FDVs, ensuring constraint consistency. AC-3 standard implementation fixpoint
propagation loop (Apt, 2009; Rossi et al., 2006).
4.2.1 Joined Multibody Constraint
JM constraint complex constraint triggered leftmost points involved constraint (anchors) instantiated. JM propagation (JMf) based
analysis distribution space points involved. goal propagation reduce domains FDVs identification fragments
cannot contribute generation rigid body compatible corresponding Point Variable domains. viewed form hyper-arc consistency
set fragments. Moreover, due complexity precision considerations,
propagator approximated use spatial equivalence relation (), identifies
classes tuples fragments; classes property spatially different
one another.
allows compact handling combinatorics multi-body, controlled
error threshold allows us select precision filtering. equivalence relation
captures rigid bodies geometrically similar, allowing search compact
small differences among them.
~ V
~ , A,
~ E,
~ i, along
JMf algorithm receives input JM-constraint hS,
set G points available placement bodies,
equivalence relation .
sake readability, assume domain information variables avail~ , Tab). process, algorithm
able. algorithm builds table constraint table (V
makes use function (lines 7 8); function takes input two lists ~a ~b
3D points, computes homogeneous transformation overlap ~b ~a. call
973

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Algorithm 2 JMf algorithm.
~ V
~ , A,
~ E,
~ , G,
Require: S,
Ensure: Tab
~ |; Tab =
1: n |V



T1 start(B) A1 A2 A3






T1 end(B) E1 E2 E3



2: R1 (B, T1 ) B S1 , T1
p points(B).q G. k(T1 p) qk





c C involving p.consistent(c))
3: P1 {T1 end(B) | (B, T1 ) R1 }
4: = 2, . . . , n
5:
Pi = ; Ri = ;
6:
E
Pi1 /

= (E, start(B)) 6= fail






end(B) E3i2 E3i1 E3i
7:
Ri Ri B Si
p points(B).q G. k(T p) qk





c C involving p.consistent(c))
8:
Pi {(E, start(B)) end(B) | B Ri }
9:
end
10:
compute Pi / filter Ri accordingly
11: end
12: representative L Pn /
13:
Tab = Tab (L)
14: end
function fail ~a 6_ ~b. simplicity, fourth component (always 1)
homogeneous transformation explicitly reported algorithm.
~ |, algorithm computes sets Ri Pi , respectively
= 1, . . . , n = |V
contain fragments Si still lead solution, corresponding allowed
3D positions end-points. fragment B Ri+1 denote parent(B)
set fragments B 0 Ri end(B 0 ) _ front(B) via . fragment B,
denote label(B) corresponding FD value associated.
computing/updating Ri Pi , fragments end-anchors contained
bounds E3i2 , E3i1 , E3i kept. Fragments would cause points collapsei.e.,
due distance smaller previously placed pointsare filtered (lines 2
7). Moreover, spatial positions points first fragment validated
(line 2); finally, enforce consistency check constraint c C involving points
points(B) Si retain points potentially reach admissible positions
(lines 2 7).
~ | 1 iterations (lines 411). First Ri Pi computed
algorithm performs |V
basis sets end-anchors previous level Pi1 starting point
selected fragment B, filtering overlapping lead
wrong portions space (lines 78). filtering based applied (line 10).
step, set triples 3D points Pi clustered using . representative
equivalence class chosen (within Pi ) corresponding fragment Ri identified;
(non-identified) fragments filtered Ri . Let us also note

974

fiA Constraint Solver Flexible Protein Models

filtering based clustering performed initial step P1 , typically
already captured restrictions imposed A.
fragments reachable last iteration determined representatives
selected, populate Tab set tuples associated representative L.
~ allows us overlap last point
function (L) returns assignments V
L.
JMf algorithm parametric w.r.t. clustering relation function selecting
representative; express degree approximation rigid bodies
built. proposed clustering relation loop modeling takes account two factors: (a) positions end-anchors 3D space (b) orientation
plane formed fragments anchor L w.r.t. fixed reference system 0 adopted
FIASCO (c.f. Figure 7). combination clusterings allows capture local geometrical
similarities, since spatial rotational features taken account.
spatial clustering (a) used following. Given set fragments, three end
points C 0 (end anchors) cluster considered, centroid triangle
C 0 computed. use three parameters: kmin , kmax N, kmin kmax , r R,
r 0. start selecting set kmin fragments, pairwise distant least 2r.
fragments selected representatives equivalence class fragments fall
within sphere radius r centered centroid representative. clustering
ensures rather even initial distribution clusters, however fragments may fall
within kmin clusters. allow create kmax kmin new clusters,
covering sphere radius r. Remaining fragments assigned closest
cluster. employed technique variant k-means clustering algorithm called
leader clustering algorithm; allows fast implementation acceptable results.
orientation clustering (b) partitions fragments according relative orientation planes R w.r.t. 0 . plane spatial orientation described Euler angles
, , frame w.r.t. 0 . algorithm produces variable number partitions depending . particular, given threshold > 0 3 (360/) possible partitions
describing equal regions sphere though interval ( / 2 , / 2 , / 2 ).
fragment allotted partition determined .
final cluster intersection two partitioning algorithms. defines
equivalence relation depending kmin , kmax , r, . representative selection
function selects fragment partition according preferences (e.g.,
frequent fragment, closest center, etc.).
Note r = 0, = 360, kmax unbounded, clustering performed
would cause combinatorial explosion every possible end-anchor whole
problem. spatial error introduced depends r . = 360, error
introduced step bounded 2r dimension. iteration
errors linearly increased, since new fragment placed initial error gathered
previous iterations, thus resulting 2nr bound last end-anchor. Clearly
bound coarse, average experimental results show better performances.
Similar considerations argued rotational errors, however intersection
two clusterings, provide, general, much tighter bound.

975

fiF. Campeotto et al.

Campeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Figure
8: graphical
representation
propagation
JM constraint
theconstraint
variables Viover
, . . . , Vthe
i+3 .variables Vi , . . . , Vi+
Figure
9: graphical
representation
theofpropagation

JM
(a) simultaneous placement elements domain variable Vi+1 simulated,
(a) simultaneous placement elements domain variable Vi+1 simulated,
overlapping corresponding fragment end-anchor fragment associated element
overlapping
eachsetcorresponding
fragment
end-anchor
fragment
element
domain
Vi .
points Pi+1
computedwith
clustered
using theofrelation
(pointsassociated
within
domain
Vieach
.
set one
points
Pi+1
computed
andchosen
clustered
using fragments
relation (points
dotted
ellipses). ofFor
cluster
fragment
representative
hence
(highlighted
dotted
ellipses).
Forcollection
cluster
one fragment
representative
hence
(highlighted fragmen
filled
rightmost
circle).
representatives
constitutes
set Ri+1
(b) Thechosen
previous
step iswith
performed
circle).
basis
end-anchors
fragmentsconstitutes
representatives
ini+1 (b) previo
filled
rightmost

collectionrelated
representatives
thechosen
set R
previous
filled
box,on
represents

setthe
points
G
available
placement
step islevel.
performed

basis
end-anchors
related
thefor
fragments
representatives chosen
bodies (for instance due distance constraint). fragment falling area discarded.

previous
level.

filled
box,
represents

set

points
G



available
placeme
(c) last iteration JMf algorithm set points Pi+3 clustered,

bodies
(for
instance
due


distance
constraint).


fragment
falling


area
discarde
reach desired position retained, instance front-anchor associated fragment next
(c)


last
iteration


JMf
algorithm

set

points
P


clustered,


th
variable, sequence fragments able lead condition (marked thick
i+3 lines) selected
populate

table
Tab.
reach desired position retained, instance front-anchor associated fragment ne
variable, sequence fragments able lead condition (marked thick lines) select
populate table Tab.
976

fiA Constraint Solver Flexible Protein Models

P
||P-Q||d

Q
Figure 9: effect distance constraint ||P Q|| propagation. Empty boxes
represent original PVs domains full boxes represent reduced PVs domains
effect constraint propagation.
4.2.2 Distance Constraints
propagation distance constraints approximated technique reduces
size box domains. introduce following operations PVs box domains
two variables P Q used describe propagation rule
following subsections:
Domain intersection: DP DQ = hmax(Pmin , Qmin ), min(Pmax , Qmax )i
Domain union: DP DQ = hmin(Pmin , Qmin ), max(Pmax , Qmax )i
Domain dilatation:
DP + = hPmin d, Pmin + di
max(P, Q) = (max(Px , Qx ), max(Py , Qy ), max(Pz , Qz )), (and similarly min),
P + = (Px + d, Py + d, Pz + d).
Given two point variables P Q, domains DP DQ , respectively, simplification rule constraint ||P Q|| updates domains follows:
DP = ((DQ + d) DP )

DQ = ((DP + d) DQ )

(8)

ensures points DP DQ positioned within approximation
sphere radius d. sphere approximated considering box inscribing (a cube
side 2d), illustrated Figure 9.
propagation constraint ||P Q|| harder coarse representation
box domains adopted work model PVs allow description
complex polyhedron. hence apply simple form bound consistency described
following rule:

(DP DQ ) = hl, ui, ||u l|| <
P

||P Q|| :
= , DQ =


establishes unsatisfiability constraint.
977

(9)

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

4.2.3 Fragment Constraint
propagation fragment constraints fragment(V, P~ , ) exploited solution
search enforce assembly process fragment [V ] along point variables
P1 , . . . , Pn P~ . Recall DV domain V containing references {j1 , . . . , jk }
database fragments .
P1

= {p1 }, DP2 = {p2 }, DP3 = {p3 }, DV = {j1 , . . . , jk }
~

fragment(V, P , ) :
jk
n
^

[
{((p1 , p2 , p3 ), [f ]) [f ](i)}
DPi = DPi


i=1

(10)

f =j1

((p1 , p2 , p3 ), [f ]) roto-translation applied overlap first three
points fragment [f ] start-anchor (p1 , p2 , p3 ).
conjunction bottom part rule re-evaluates domains P1 , P2 , P3 ,
may reduce singleton domains empty whenever compatible
selected fragment.
4.2.4 Centroid Constraint
positions atoms N , C C 0 amino acids determined,
propagation algorithm enforces value PV PCG involved centroid constraint.

P
N = {pN }, DC = {pC }, DPC 0 = {pC 0 }

centroid(PN , PC , PC 0 , PCG , a) : P
(11)
CG = (DPCG {cg(pN , pC , pC 0 , a)})
cg(pN , pC , pC 0 , a) support function returns center mass
side chain amino acid considering points pN , pC , pC 0 , described
Sect. 3.3.3.
4.2.5 Implementation Details
proposed solver relies efficient C++ implementation, carefully designed
allow additional tailored solving capability without need reshaping core structures.
internal representation domains finite domain variables abstracted two arrays length size initial domain. One array points
values Boolean bit-mask states whether value still
domain. flags set 0, current partial assignment cannot part solution
overall constraint; exactly one set 1, variable assigned value.
representation implies linear scan domains propagation
justified reasonably small size domains target application (typically less
100 values). internal representation domains point variables simply
pair hmin, maxi uniquely characterizes 3D box R3 . Since variables used
mostly distance constraints, representation expressive enough (Oct-trees
considered significant advantage).
Point Variables propagation described above; variables instantiated
fragment selection.
978

fiA Constraint Solver Flexible Protein Models

management uniqueseq property (7) implemented dedicated data
structure based hash tables. Every time PV assigned, value mapped 3D
voxel fixed size. 3D grid implemented via Hash Table voxel indexes keys
points contained voxels values. operations performed O(1)
(amortized complexity).
4.3 One JM Constraints
briefly describe modeled two problems FIASCO. JM constraint
able model geometrically assembly fragments therefore used every
protein model. single JM covers protein ensures flexibility, however long
proteins computational precision issues arise. beneficial model
protein multiple JM constraints, e.g. JM (i, j) JM (j, k) amino acids
j covered JM constraints overlap common amino acid.
practical choice improves approximate search allows increase number
different solutions produced. practice, protein section handled JM constraint
potentially combined different arrangements sections. Therefore,
expected number solutions found grows exponentially number JM
constraints. JM constraint parameters used control clustering precision
number conformations found.

5. Experimental Results
report experimental results obtained FIASCO system (available
http://www.cs.nmsu.edu/fiasco). Experiments conducted Linux Intel Core i7
860, 2.5 GHz, memory 8 GB, machine. solver implemented C++.
fragment database adopted FREAD database shown effective loop structure prediction (Choi & Deane, 2010). parameters analysis 5.1.4
use database fragments length 1. fragments classified amino
acid class frequency occurrence whole top-500.
set system model two applications described below. particular,
Section 5.1 analyze loop modeling scenario focus performances JM
filtering examining filtering power computational costs. Next, compare
quality loop conformations generated, measuring RMSD proposed loop
respect native conformation. present relationships among
JM parameters control quality efficiency.
Section 5.2 show examples ab-initio protein structure prediction
conclude comparison FIASCO constraint solvers, protein models
described common subset constraints.
5.1 Loop Modeling
loop modeling problem formalized presence two known (large) fragments
fixed space. sequence amino acids length n given connecting
two parts protein. JM constraint defined sequence, particular
attention starting ending points fixed. start first fragment
979

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Figure 10: example loop computed tool
end last fragment, namely sequence C 0 (initial points) coordinates
~a = (a1 , a2 , a3 ), sequence C 0 (final points) coordinates ~e = (e1 , e2 , e3 )
known. one caveat end points: due discrete nature fragment
assembly, unlikely exactly reach final points. accommodate errors,
require JM constraint produces results fall within threshold
corresponding final points.
Figure 10 show Example loop computed tool (the parts protein
connected shown left connecting loop right).
Additional spatial constraints points (e.g. no-good regions determined presence
atoms) given. constant (now = 1.5A) asserts minimum distance
pairs atoms.
5.1.1 Filtered Search Space Performances
selected 30 protein targets set non-redundant X-ray crystallography structures
done Canutescu Dunbrack (2003). partitioned proteins 3 classes
according loop region lengths (n = 4, 8, 12). model CSP uses
fragment assembly model loop, particular using JM constraint loop
region.
assess filtering capabilities FIASCO, perform exhaustive search generating solution protein targets. Using clusterization 0.2A, number
different fragments length 1 found amino acid (see Fig. 11). size
domains corresponding FDVs bound 100this adequate sampling
describe reasonable amino acid flexibility. cases number fragments
exceeds 100, 100 frequent ones kept.
increases likelihood generating loop structure similar native
one. loop length n generates exponential search space size bounded 100n .
selected variable leftmost one. Fragments selected decreasing frequency order.
imposed JM constraint every 4 consecutive amino acids. clustering
parameters set follows: kmin value equal size domains,

980

fi100 120 140
80
60
40
0

20

N. different Fragments

Constraint Solver Flexible Protein Models



C



E

F

G

H



K

L



N

P

Q

R





V

W



Amino acids

Figure 11: Number different fragments (after clustering) per amino acid dataset
used different values kmax based loop lengths. values r set
120 0.5 setting. summary parameters listed Table 1.
Table 1 report average times needed exhaustively explore loop search
space, average number solutions generated.
n
4
8
12

# JM
1
2
3

JM Parameters
kmin kmax

100 1000 120
100
500 120
100
100 120

r
0.5
0.5
0.5

Full JM
# Solutions Time (s)
597
3.13
98507
10.12
328309
28.87

Table 1: Loop Modeling settings average running times (in seconds) number
solutions generated.

5.1.2 JM Approximated Propagator Quality
Even approximated JM produces small set solutions, show
good representation overall variability protein structure. test,
compare solutions means RMSD original structures. experiments
carried 30 protein targets settings described Table 1,
exception kmax loop set size 12, set 500.
Figure 12 show bar chart RMSD predictions protein
loop within group targets analyzed. Precisely, x-axis 30 (10
loop length) protein targets. bar reports best RMSD (dark), average
RMSD (grey), worst RMSD (light grey) found. Numbers bars represent
number loops found (multiplied factors indicated underneath). results
biased fragment database use: excluded fragments belong

981

fi2.1
0.15

1.1

3.7

0.3

0.38

6
5.6

4.7

1.3
0.78

0.59

0.96
1.3

1.3

1.1

0.67

0.65
0.83

0.64

0.68

0.31

0.77

0.71
0.2

0.52

0.19

1.3

6

0.92

8

Best Rmsd
Avg Rmsd
Worst Rmsd

4
0

2

RMSD (Angstrm)

2.9

Campeotto, Dal Palu, Dovier, Fioretto, & Pontelli

. 105

. 103

Length 4

Length 8

. 107

Length 12

Figure 12: RMSD comparison Loop Set (x-axis: 30 protein targets)

deposited protein targets. Therefore, possible reconstruct original
target loop none searches expected reach RMSD equal 0.
loops length 8 12, exploration whole conformational search space
using simple search procedure would result excessively long computation time.
enforces need propagator JM, filtering algorithm successfully removes
redundant conformations allows us cover whole search space short period
time.
Fig. 12 loop predictions calculated using fragments length 1. study
choice affects time accuracy sampling also model loops length
12 using fragment length 3, 6, 9. Best RMSDs reported Figure 13.
experiments kept settings used (kmax = 500). Moreover, JM constraint
imposed fragments order cover whole fragment (e.g, fragments
length 3 set JM constraint every three consecutive amino acids) set time-out
3600 Seconds.
Notice increasing length fragments accuracy decreases due
reduced size domains. Nevertheless, time also reduced since sampling
performed smaller search space JM constraints cover longer sequences amino
acids. average times are: 1580.14, 0.98, 0.74 seconds using fragments length 3,
6, 9 respectively.

982

fi3
2
0

1

RMSD (Angstrm)

4

5

Constraint Solver Flexible Protein Models

Len3

Len6

Len9

Figure 13: RMSD comparison loop sampling loops length 12 using fragments
length 3, 6, 9.

983

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

5.1.3 Comparison State-of-the-art Loop Samplers
section, compare method three state-of-the-art loop samplers: Cyclic
Coordinate Descent (CCD) algorithm (Canutescu & Dunbrack, 2003), Self-Organizing
algorithm (SOS) (Liu, Zhu, Rassokhin, & Agrafiotis, 2009), FALCm method (Lee,
Lee, Park, Coutsias, & Seok, 2010).
Table 2 shows average best RMSD benchmarks length 4, 8 12
computed four programs. report results given Table 2 Canutescu
Dunbrack CCD, Table 1 Liu et al. SOS, Table II Lee et al.
FALCm method, RMSDs obtained adopting settings JMf provided
best results previous section (see also Subsection 5.1.5). noted
results line produced systems.
Loop
Length
4
8
12

Average (best) RMSD
CCD
SOS
FALCm
JMf
0.56
0.20
0.22
0.27
1.59
1.19
0.72
0.93
3.05
2.25
1.81
1.58

Table 2: Comparison loop sampling methods
execution time reported appear competitive (e.g., considered
results reported Soto et al., 2008).
5.1.4 JM Parameters Analysis
section, analyze impact JM parameters quality best
solutions found execution times. particular, aim experiments
shed light relationship JM constraint settings results.
Figure 14, analyze impact kmax execution times (left)
precision (right) filtering JM constraint. top bottom, use
= 60, 120, 360. tests performed protein loops length 4 (see section
above), adopting cluster parameters, r {0.5, 1.0, 3.0, 5.0}, kmin = 100. dot
plots represents average best RMSD found predictions (left)
average execution time (right). RMSD values tend decrease smaller clustering
parameters r number clusters increases, filtering time increases
kmax increases.
Figure 15 study relation RMSD number JMs
cover given target loop protein Voxel-side parameter. experiments
used values {100, 250, 500, 800, 1000} kmax , set r = 1, = 120,
averaged RMSDs values resulting sample set structures. relation
RMSD number JM well average worst computational times
shown Fig. 15 left. use medium-length loop taken protein 1XPC
(res. 216-230) vary number JMs cover loop (the side voxel
set 3A). figure observe increasing number JMs (i.e.
covering less amino acids single JM) RMSD decreases computational
cost higher. Notice best RMSD given loop covered 4 JM

984

fiA Constraint Solver Flexible Protein Models

0.9



50.0


0.8

0.6

10.0


r


0.5

r

Time (s)

)
RMSD (A

0.7

0.5
1.0
3.0
5.0



0.4



0.5
1.0
3.0
5.0

1.0


0.3

0.5


0.2





10000

100

0.1

100

1000

5000

1000

5000

JM kmax

10000

JM kmax

50.0


0.9


0.8
10.0

0.6

r



0.5
1.0
3.0
5.0



0.5

Time (s)

)
RMSD (A

0.7
r





1.0

0.4

0.5
1.0
3.0
5.0

0.5



0.3




0.2

100

1000

5000

10000

100

1000

JM kmax

5000

10000

JM kmax

0.9





0.8



0.6

r




0.5

0.5
1.0
3.0
5.0

Time (s)

)
RMSD (A

0.7

r

1.0



0.5

0.5
1.0
3.0
5.0

0.4



0.3




5000

10000

0.2

100

1000

100

JM kmax

1000

5000

10000

JM kmax

Figure 14: Comparison best RMSD values execution times varying
kmax clustering parameter = 60 (top), 120 (center), 360 (bottom)

985

fi7
RMSD

1LE0
1MXN
1FDF

0

0

1

2

3

2091.72 (3216.94)

1105.63 (2057.83)

194.70 (411.95)

9.53 (19.54)

11.73 (18.42)

4
2

RMSD

4

5

6

6

8

Campeotto, Dal Palu, Dovier, Fioretto, & Pontelli

1

2

3

4

5

0

N.of JM

20

40

60

80

100

VoxelSide

Figure 15: Left: RMSD (best average) Time (average worst) values increasing
number JM constraints completely cover target loop length 15. Right:
Average (dotted line) best (solid line) RMSD targets 1LE0 length 12 (top),
1MXN length 16 (medium), 1FDF length 25 (low). JM-Voxel-side parameter
voxels clustering varies 3 100. JM kmax parameter varies
100 1000. targets completely covered multiple JM-constraints.
constraint (i.e., JM constraint four consecutive amino acids). rule thumb
suggest use JM constraint cover 3 4 consecutive amino acids since
setting produces best results within acceptable time. Fig. 15 right report
best RMSD (solid line) average RMSD (dotted line) structures found using
multiple JM constraints cover sequences 4 consecutive amino acids whole
target proteins. Namely, protein target length n, set JM constraints
+ 3, = 3 j, 0 j < n/3. experiments, considered three proteins
relatively short length, order obtain complete exploration search space
reasonable computational time: 1LE0 (length 12), 1MXN (length 16), 1FDF (length
24). Moreover used values {3, 5, 10, 20, 30, 50, 100} side voxels used
clustering.
Figure 15 observe voxel size (enabled uniqueseq)
impact clustering values lower 30A (recall proteins diameter
less 30A). voxel sides lower 3A observe substantial improvement
terms quality, time required solver compute solutions increases
exponentially.
5.1.5 Results Summary Default Parameters
provide guidelines may helpful tune JM parameters given
protein modeling problem. suggest several levels parametrization might used
according user needs respect running time prediction accuracy. stress

986

fiA Constraint Solver Flexible Protein Models

merely guidelines, outlined empirical evaluations, several
tests done establish desired tuning.
suggest set JM model sequence least 3 amino acids general
longer 8, payoff computational load JM clustering. default choice
kmin set average size variable domains involved JM constraint,
suggest set kmax least kmin greater 10000. latter,
together number consecutive JM constraints, greatest impact
computational cost prediction accuracy. Computational costs grow number
consecutive JM increases, time also produce general higher
accuracy. trend exhibited growing kmax parameter. Table 3 illustrates
five basic settings could used incrementally establish trade running
times prediction accuracy. first level (Lev. 1) associated faster computational
times lower accuracy last one (Lev. 5) slowest also accurate.
second column table indicates length amino acid sequence modeled
single JM.
Lev.
1
2
3
4
5

n.JM
8
8
6
4
4

kmin
|D|
|D|
|D|
|D|
|D|

kmax
500
1000
100
500
1000


120
120
120
120
120

r
5
3
3
3
1

Speed






Accuracy






Table 3: JM default parameters

5.2 Application Protein Structure Prediction
protein structure prediction problem, model generic backbone multiple
JM constraints. principle, unique JM constraint model whole problem.
previous cases, split smaller parts, moreover, presence secondary
structure valid help placement JM constraints handle loops
consecutive pair. simple search generate pool conformations, energy
scoring select best candidate. used statistical energy function developed
5@ model, energy function used instead.
section, study applicability FIASCO protein structure prediction
problem. particular, consider prediction problems secondary structure
elements protein given. Furthermore, order assess potential structure,
introduce energy functionthe adopted previous studies,
precisely described http://www.cs.nmsu.edu/fiasco.
modeling, used information location type
secondary structure elements primary sequence provided directly Protein
Data Bank. imposed sequence JM constraints every consecutive
pair secondary structure elements. number consecutive JM constraints varied
according length unstructured sequence modeled, covering 5
amino acids single JM constraint. addition one JM constraint imposed
first amino acid beginning first secondary structure element another
987

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

end last secondary structure element last amino acid (the tails
protein). domains initial end points JM constraints set
admissible points (a box large enough contain protein). search phase,
first secondary structure deterministically set space. labeling proceeds
JM constraint attached leading next secondary structure on.
Tails instantiated end.
propagation constraints generates set admissible structures, represents possible folds target protein. set, select structure
minimum energy; extract also structure minimum RMSD, order evaluate
quality energy function. tests adopt FREAD database. Table 4
reports best energy values found FIASCO. RMSD columns reported
corresponding RMSD associated conformation best energy found solver.
#JM column reports total number JM used model protein, together
maximum number consecutive JM adopted model contiguous sequence
amino acids (within parentheses).
Protein ID
1ZDD
2GP8
2K9D
1ENH
2IGD
1SN1
1AIL
1B4R
1JHG

Len.
35
40
44
54
60
63
69
79
100

# JM
4(2)
4(2)
5(2)
4(1)
7(2)
7(3)
4(1)
11(2)
7(1)

Energy
100513
138110
204693
309896
295882
358874
411077
313590
572950

RMSD
2.05
6.28
2.52
8.21
10.50
5.55
4.59
6.11
4.51

Time (Min.)
11.42
8.55
2.69
31.67
26.47
14.82
4.46
8.41
4.50

Table 4: Ab initio prediction FIASCO.
results show quality predictions computed FIASCO (6.3 average
RMSD) competitive (and, shown following section, par better
produced methods). results particularly encouraging proteins longer
length, sampling search space aids development admissible structures.
time required FIASCO completely explore search space depends strongly
type mutual arrangement secondary structure elements target.
example, protein 2K9D protein 1ENH length, FIASCO
significantly faster first protein second one. observation
made proteins 2IGD 1SN1. results reported Table 4 promising
suggest feasible approach solve ab initio prediction problem.
future work, explore integration local search techniques (e.g., largeneighboring search), order sample search space decrease time
needed explore it.

988

fiA Constraint Solver Flexible Protein Models

5.3 Comparison FIASCO State-of-the-Art Constraint Solvers
section, motivate choice designing ad-hoc solver instead using
general-purpose constraint solver. particular provide comparison FIASCO
state-of-the-art constraint solving. results justify choice implementing new
solver scratch instead using available constraint programming library constraint programming language. solver chosen comparison Gecode (Gecode
Team, 2013), efficient solver winner recent MiniZinc challenges (Stuckey, Becket, & Fischer, 2010).
Gecode recently introduced (in version 4.0) handling floating point variables.
Nevertheless, since Gecode fastest solver FD variables, first encoded
PSP discretizing fragments positions. particular, multiplied real
number scaling factor (100) obtain integer values. spatial position encoded
triple variables, representing coordinates point. operation (e.g.,
multiplications) applied variables requires re-scaling result; unfortunately
leads ineffective propagation. particularly evident dealing distance
constraints, require implementation Euclidean distance pairs triples
variables.
order understand solvers capabilities propagate constraints placement overlapping fragment implemented three versions code, considered
different number constraints, precisely:
1. implementation uses JM constraint (JM only)
2. implementation adds alldistant constraint
3. implementation adds alldistant centroid constraints
cases use complete search (in particular, clustering tabling constraints
lines 10 1214 Algorithm 2 disabled).
Table 5, report execution times required FIASCO Gecode (with
considered constraints) determine increasing number solutions, 1, 000
1, 000, 000. solutions computed target protein 1ZDD length
35. Table 5 shows execution time solvers increases proportionally
number solutions found. However, FIASCO one order magnitude faster Gecode
unconstrained case, two orders magnitude faster cases. main
reason FIASCO specifically developed handle finite domains 3D point
variables, approximated FD variables Gecode. Constraints
approximations propagate poorly slowly. Moreover, approximation fragments
using finite domain variables introduces approximation errors, grow search
phase (and consequently, less solutions returned constrained cases). errors
may result final structures relatively imprecise coordinates atoms
converted back real values.
Table 6, consider small sequence four amino acids (SER TRP THR TRPthe
first four amino acids protein 1LE0), generate solutions. report
values best average RMSD among structures sets solutions computed using FIASCO Gecode implementation complete enumeration
989

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Number
solutions
1000
10000
100000
1000000

JM
0.030
0.312
3.006
29.859

FIASCO
alldistant alldistant + centroid
0.051
0.059
0.476
0.612
4.794
6.040
47.669
61.385

JM
0.358
2.571
25.407
252.815

Gecode
alldistant alldistant + centroid
2.531
3.807
21.056
35.370
209.569
347.831
2186.83
3632.39

Table 5: Comparison execution times FIASCO Gecode, increasing number
solutions different sets considered constraints.
domains. observe FIASCO significantly faster exploring search space,
moreover, approximation introduces errors leads loss feasible solutions.

JM
alldistant
alldistant + centroid

N. sol.
810000
805322
805322

FIASCO
Time (sec.) RMSD
20.493
0.167
33.493
0.167
38.953
0.167

Avg. RMSD
1.570
1.564
1.564

N. sol
810000
774463
169441

Gecode
Time (sec.) RMSD
181.102
0.190
252.974
0.190
140.644
0.580

Avg. RMSD
1.596
1.591
1.880

Table 6: Number solutions, time, best RMSD, average RMSD set structures
found FIASCO Gecode complete enumeration solution space using
different constraints
encoded constraint satisfaction problem using new version
Gecode allows employ float variables. labeled finite domain variables
allow select fragments, values point variables obtained constraint
propagation. Since constraint propagation float variables based interval arithmetics,
turns amino acids intervals large able reconstructing protein evaluating energy value. instance, complete
assignment variables related fragments protein 1ZDD, domains
float variables associated position first two amino acids singletons,
related tenth amino acids intervals size one two A; even worse,
domains atoms eleventh amino acids unbounded. stage
labeling float variables required computational time orders magnitude higher
reported Table 6 finite domain Gecode implementation.
Constraint solvers like ECLiPSe (Cheadle, Harvey, Sadler, Schimpf, Shen, & Wallace, 2003) Choco (Choco Team, 2008) also support mixed use integer
real variables. ECLiPSe Prolog-based language handles integer real variables together. However, great number matrix operations required application fit well Prolog implementation. Furthermore, current trend
ECLiPSe replace direct constraint solving translation FlatZinc.
case Choco, current support Real Variables still development (c.f.
http://choco.sourceforge.net/userguide.pdfpage 3). Things may change
next releases.
also experimented another constraint solver, implementing multi-body
constraints using JaCoP library (JaCoP Team, 2012), similar way done
Gecode. Eventually, tested protein used results reported Table 5,

990

fiA Constraint Solver Flexible Protein Models

observe substantial difference terms execution time,
Gecode implementation.
terms protein structure prediction, design FIASCO influenced
previous work TUPLES system (Dal Palu et al., 2011). TUPLES also
constraint solver protein structure prediction, based fragments assembly. Figure 16
compares performance TUPLES FIASCO set proteins discussed
Section 5.2. make comparison fair, make use energy function
systems assume secondary structure elements known. Note
important differences two systems. TUPLES implemented using
constraint logic programming techniques, specifically, SICStus Prolog (Swedish Institute
Computer Science, 2012); TUPLES make use floating point variables;
hand, TUPLES introduces heuristic search mechanism based large neighboring
search.
results show quality predictions computed FIASCO (6.3 average
RMSD) better quality predictions computed TUPLES (9.4 average
RMSD). complete sampling search space allows us obtain better results
proteins longer length benchmark ( 63). Instead, shorter proteins,
obtain comparable results. similarity quality depends use
energy function systems. Notice energy function used designed
simpler model adopted TUPLES (C C ). Moreover, TUPLES based Prolog
implementation provide floating point variables hence value must
rounded approximated. aspects explain quality differences
RMSD Best RMSD found FIASCO behavior
proteins (e.g., 1ZDD, 2GP8 ) (energy) RMSD values better FIASCO even
corresponding energy (RMSD) values higher TUPLES. execution times
FIASCO significantly faster TUPLES, spite FIASCOs lack sophisticated
search heuristic.
also performed comparison state-of-the-art online Robetta predictor (Raman, Vernon, Thompson, Tyka, Sadreyev, Pei, Kim, Kellogg, DiMaio, Lange, Kinch, Sheffler, Kim, Das, Grishin, & Baker, 2009) first four proteins Table 6. built
dictionary 3 9 amino acid long peptides Robetta interface,
disabled homology inference, order maintain fair comparison. results are:
1ZDD computed 21s 5.92 RMSD, 2GP8 computed 16s 5.44 RMSD, 2K9D
computed 22s 4.65 RMSD, 1ENH computed 39s 2.74 RMSD. noted
results line Robetta predictor.
Let us conclude section mentioning results reported previous section
(where compared FIASCO TUPLES) provide also implicit comparison another off-the-shelf solver, SICStus Prolog constraint logic programming solver (Swedish
Institute Computer Science, 2012).

6. Conclusions
paper, presented novel constraint (joined-multibody) model rigid bodies
connected joints, constrained degrees freedom 3D space. presented
polynomial time approximated filtering algorithm joined-multibody constraint,
991

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Figure 16: Comparison RMSD Execution Time TUPLES FIASCO

992

fiA Constraint Solver Flexible Protein Models

exploits geometrical features rigid bodies. particular, filtering algorithm
combined search heuristics produce pool admissible solutions
uniformly sampled. allows direct control quality number solutions.
filtering algorithm based 3D clustering procedure able cope
high variability rigid bodies, preserving computational cost. practical
advantages joined-multibody constraint shown extensive set real protein
simulations two main categories: protein loop reconstruction structure prediction
(ab-initio). tests showed parameters constraint able control
effectively quality computational cost search. conclusion, constraint
solver FIASCO able model effectively various common protein case-studies analyses.
future work, applications side, plan explore protein loop closure
problem, use specific databases scoring functions. close problem
protein flexibility, plan use FIASCO solver generate conformational space
long scale movements nuclear receptors (Dal Palu et al., 2012b). Finally, plan use
FIASCO general context protein structure prediction combination local
search methods protein-ligand spatial constraints. constraint side, plan
integrate JM filtering algorithm distance constraints, order generate
accurate clusters; plan integrate spatial constraints inferred bounds
energy terms (e.g., favorable contributions provided pairing secondary structure
elements translate energy bounds distance constraints). plan investigate
use multiple JM constraints model super-secondary structures placement,
useful capture important functional structural protein features. latter
thought imposing several spatial path preferences given chain points. Finally,
intend integrate constraint solver visual interface make easily available
Biologist practitioners porting parts tool within GPU-based
framework recently explored Campeotto, Dovier, Pontelli (2013).

Acknowledgments
thank Federico Fogolari comments several parts work. authors
would like express gratitude JAIR reviewers helped us sensibly improve
presentation.

References
Al-Bluwi, I., Simeon, T., & Cortes, J. (2012). Motion Planning Algorithms Molecular
Simulations: Survey. Computer Science Review, 6 (4), 125143.
Alberts, B., Johnson, A., Lewis, J., Raff, M., Roberts, K., & Walter, P. (2007). Molecular
Biology Cell (5th Edition edition). Garland Science.
Anfinsen, C. B. (1973). Principles Govern Folding Protein Chains. Science, 181,
223230.
Apt, K. (2009). Principles Constraint Programming. Cambridge University Press.
Backofen, R., & Will, S. (2006). Constraint-Based Approach Fast Exact Structure
Prediction 3-Dimensional Protein Models. Constraints, 11 (1), 530.
993

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Backofen, R., Will, S., & Bornberg-Bauer, E. (1999). Application Constraint Programming Techniques Structure Prediction Lattice Proteins Extended Alphabet.
Bioinformatics, 15(3), 234242.
Baker, D., & Sali, A. (2001). Protein Structure Prediction Structual Genomics. Science,
294 (5540), 9396.
Barahona, P., & Krippahl, L. (2008). Constraint Programming Structural Bioinformatics.
Constraints, 13 (1-2), 320.
Ben-David, M., Noivirt-Brik, O., Paz, A., Prilusky, J., Sussman, J. L., & Levy, Y. (2009).
Assessment CASP8 Structure Predictions Template Free Targets. Proteins, 77,
5065.
Bennett, W., & Huber, R. (1984). Structural Functional Aspects Domain Motions
Proteins. Crit. Rev. Biochem., 15, 291384.
Borning, A. (1981). Programming Language Aspects ThingLab, ConstraintOriented Simulation Laboratory. ACM Transactions Programming Languages
Systems, 3 (4), 353387.
Cahill, S., Cahill, M., & Cahill, K. (2003). Kinematics Protein Folding. Journal
Computational Chemistry, 24 (11), 13641370.
Campeotto, F., Dovier, A., & Pontelli, E. (2013). Protein Structure Prediction GPU:
Declarative Approach Multi-agent Framework. International Conference
Parallel Processing (ICPP), pp. 474479. IEEE Computer Society Press.
Campeotto, F., Dal Palu, A., Dovier, A., Fioretto, F., & Pontelli, E. (2012). Filtering
Technique Fragment Assembly-Based Proteins Loop Modeling Constraints.
Milano, M. (Ed.), CP, Vol. 7514 Lecture Notes Computer Science, pp. 850866.
Springer.
Canutescu, A., & Dunbrack, R. (2003). Cyclic coordinate descent: robotics algorithm
protein loop closure. Protein Sci, 12, 963972.
Cheadle, A. M., Harvey, W., Sadler, A. J., Schimpf, J., Shen, K., & Wallace, M. G. (2003).
ECLiPSe: Introduction. Technical report IC-Parc 031, IC-Parc, Imperial College
London.
Chelvanayagam, G., Knecht, L., Jenny, T., Benner, S., & Gonnet, G. (1998). Combinatorial Distance-Constraint Approach Predicting Protein Tertiary Models
Known Secondary Structure. Folding Design, 3, 149160.
Choco Team (2008). Choco: Open Source Java Constraint Programming Library.
Workshop Open-Source Software Integer Constraint Programming. Available http://www.emn.fr/z-info/choco-solver/.
Choi, Y., & Deane, C. M. (2010). FREAD Revisited: Accurate Loop Structure Prediction
Using Database Search Algorithm. Proteins, 78 (6), 143140.
Clementi, C. (2008). Coarse-grained Models Protein Folding: Toy Models Predictive
Tools?. Curr Opin Struct Biol, 18, 1015.
994

fiA Constraint Solver Flexible Protein Models

Corblin, F., Trilling, L., & Fanchon, E. (2005). Constraint Logic Programming Modeling
Biological System Described Logical Network. Workshop ConstraintBased Methods Bioinformatics.
Cortes, J., & Al-Bluwi, I. (2012). Robotics Apporach Enhance Conformational Sampling Proteins. International Design Engineering Technical Conferences
Computers Information Engineering Conference, Vol. 4, pp. 11771186. ASME.
Crescenzi, P., Goldman, D., Papadimitriou, C., Piccolboni, A., & Yannakakis, M. (1998).
Complexity Protein Folding. Proceedings Thirtieth Annual ACM
Symposium Theory Computing, pp. 597603. ACM Press.
Dal Palu, A., Dovier, A., Fogolari, F., & Pontelli, E. (2012a). Protein Structure Analysis
Constraint Programming. Cozzini, P., & Kellogg, G. (Eds.), Computational
Approaches Nuclear Receptors, chap. 3, pp. 4059. Royal Society Chemistry.
Dal Palu, A., Spyrakis, F., & Cozzini, P. (2012b). New Approach Investigating Protein
Flexibility Based Constraint Logic Programming: First Application Case
Estrogen Receptor. European Journal Medicinal Chemistry, 49, 127140.
Dal Palu, A., Dovier, A., & Fogolari, F. (2004). Constraint Logic Programming Approach
Protein Structure Prediction. BMC Bioinformatics, 5, 186.
Dal Palu, A., Dovier, A., Fogolari, F., & Pontelli, E. (2010). CLP-based protein fragment
assembly. Theory Practice Logic Programming, 10 (4-6), 709724.
Dal Palu, A., Dovier, A., Fogolari, F., & Pontelli, E. (2011). Exploring Protein Fragment
Assembly Using CLP. Walsh, T. (Ed.), Proceedings International Joint
Conference Artificial Intelligence, IJCAI, pp. 25902595. IJCAI/AAAI.
Dal Palu, A., Dovier, A., & Pontelli, E. (2005). New Constraint Solver 3D Lattices
Application Protein Folding Problem. International Conference Logic
Programming Artificial Intelligence Reasoning, pp. 4863. Springer Verlag.
Dal Palu, A., Dovier, A., & Pontelli, E. (2007). Constraint Solver Discrete Lattices,
Parallelization, Application Protein Structure Prediction. Software Practice
Experience, 37 (13), 14051449.
Dal Palu, A., Dovier, A., & Pontelli, E. (2010). Computing Approximate Solutions
Protein Structure Determination Problem using Global Constraints Discrete
Crystal Lattices. International Journal Data Mining Bioinformatics, 4 (1),
120.
Deane, C., & Blundell, T. (2001). CODA. Combined Algorithm Predicting Structurally Variable Regions Protein Models. Protein Sci, 10, 599612.
Debartolo, J., Hocky, G., Wilde, M., Xu, J., Freed, K., & Sosnick, T. (2010). Protein
Structure Prediction Enhanced Evolutionary Diversity: SPEED. Protein Science,
19 (3), 520534.
Dotu, I., Cebrian, M., Van Hentenryck, P., & Clote, P. (2011). Lattice Protein Structure
Prediction Revisited. IEEE/ACM Trans. Comput. Biology Bioinform, 8 (6), 1620
1632.
995

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Dunbrack, R. (2002). Rotamer Libraries 21st Century. Curr. Opin. Struct. Biol.,
12 (4), 431440.
Erdem, E. (2011). Applications Answer Set Programming Phylogenetic Systematics.
Logic Programming, Knowledge Representation, Nonmonotonic Reasoning, pp.
415431. Springer Verlag.
Erdem, E., & Ture, F. (2008). Efficient Haplotype Inference Answer Set Programming.
National Conference Artificial Intelligence (AAAI), pp. 436441. AAAI/MIT
Press.
Felts, A., Gallicchio, E., Chekmarev, D., Paris, K., Friesner, R., & Levy, R. (2008). Prediction Protein Loop Conformations using AGBNP Implicit Solvent Model
Torsion Angle Sampling. J Chem Theory Comput, 4, 855868.
Fiser, A., Do, R., & Sali, A. (2000). Modeling Loops Protein Structures. Protein Sci,
9, 17531773.
Fogolari, F., Esposito, G., Viglino, P., & Cattarinussi, S. (1996). Modeling Polypeptide
Chains C Chains, C Chains C , C Chains Ellipsoidal Lateral
Chains. Biophysical Journal, 70, 11831197.
Fogolari, F., Pieri, L., Dovier, A., Bortolussi, L., Giugliarelli, G., Corazza, A., Esposito, G.,
& Viglino, P. (2007). Scoring Predictive Models using Reduced Representation
Proteins: Model Energy Definition. BMC Structural Biology, 7 (15), 117.
Fogolari, F., Corazza, A., Viglino, P., & Esposito, G. (2012). Fast Structure Similarity
Searches among Protein Models: Efficient Clustering Protein Fragments. Algorithms
Molecular Biology, 7, 16.
Fujitsuka, Y., Chikenji, G., & Takada, S. (2006). SimFold Energy Function De Novo
Protein Structure Prediction: Consensus Rosetta. Proteins, 62, 381398.
Gay, S., Fages, F., Martinez, T., & Soliman, S. (2011). Constraint Program Subgraph
Epimorphisms Application Identifying Model Reductions Systems Biology.
Workshop Constraint-Based Methods Bioinformatics.
Gebser, M., Schaub, T., Thiele, S., & Veber, P. (2011). Detecting Inconsistencies Large
Biological Networks Answer Set Programming. Theory Practice Logic
Programming, 11 (2-3), 323360.
Gecode Team (2013). Gecode: Generic Constraint Development Environment. Available
http://www.gecode.org.
Go, N., & Scheraga, H. (1970). Ring Closure Local Conformational Deformations
Chain Molecules. Macromolecules, 3, 178187.
Graca, A., Marques-Silva, J., Lynce, I., & Oliveira, A. (2011). Haplotype Inference
Pseudo-Boolean Optimization. Annals OR, 184 (1), 137162.
Guns, T., Sun, H., Marchal, K., & Nijssen, S. (2010). Cis-regulatory Module Detection Using
Constraint Programming. IEEE International Conference Bioinformatics
Biomedicine (BIBM), pp. 363368.
996

fiA Constraint Solver Flexible Protein Models

Handl, J., Knowles, J., Vernon, R., Baker, D., & Lovell, S. (2012). Dual Role
Fragments Fragment-Assembly Methods De Novo Protein Structure Prediction.
Proteins: Structure, Function Bioinformatics, 80 (2), 490504.
Hartenberg, R., & Denavit, J. (1995). Kinematic Notation Lower Pair Mechanisms
Based Matrices. Journal Applied Mechanics, 77, 215221.
Hegler, J., Latzer, J., Shehu, A., Clementi, C., & Wolynes, P. (2009). Restriction Versus
Guidance Protein Structure Prediction. Proc Natl Acad Sci U.S.A., 106 (36), 15302
15307.
Jacobson, M., Pincus, D., Rapp, C., Day, T., Honig, B., Shaw, D., & Friesner, R. (2004).
Hierarchical Approach All-atom Protein Loop Prediction. Proteins, 55, 351367.
JaCoP Team (2012). JaCoP web page, visited November 2012..
http://www.jacop.eu.

Available

Jamroz, M., & Kolinski, A. (2010). Modeling Loops Proteins: Multi-method Approach. BMC Struct. Biol., 10 (5).
Jauch, R., Yeo, H., Kolatkar, P. R., & Clarke, N. D. (2007). Assessment CASP7 Structure
Predictions Template Free Targets. Proteins, 69, 5767.
Jones, D. (2006). Predicting Novel Protein Folds using FRAGFOLD. Proteins, 45,
127132.
Karplus, K., Karchin, R., Draper, J., Casper, J., Mandel-Gutfreund, Y., Diekhans, M.,
& Source., R. H. (2003). Combining local structure, fold-recognition, new fold
methods protein structure prediction. Proteins, 53 (6), 491497.
Karplus, M., & Shakhnovich, E. (1992). Protein Folding: Theoretical Studies Thermodynamics Dynamics. Protein Folding, pp. 127195. WH Freeman.
Kim, D. E., Blum, B., Bradley, P., & Baker, D. (2009). Sampling Bottlenecks De novo
Protein Structure Prediction. Journal Molecular Biology, 393 (1), 249 260.
Kinch, L., Yong Shi, S., Cong, Q., Cheng, H., Liao, Y., & Grishin, N. V. (2011). CASP9
assessment free modeling target predictions. Proteins, 79, 5973.
Kirillova, S., Cortes, J., Stefaniu, A., & Simeon, T. (2008). NMA-Guided Path Planning Approach Computing Large-Amplitude Conformational Changes Proteins.
Proteins: Structure, Function, Bioinformatics, 70 (1), 131143.
Kolodny, R., Guibas, L., Levitt, M., & Koehl, P. (2005). Inverse Kinematics Biology:
Protein Loop Closure Problem. International Journal Robotics Research,
24 (2-3), 151163.
Krippahl, L., & Barahona, P. (2002). Psico: Solving Protein Structures Constraint
Programming Optimization. Constraints, 7 (4-3), 317331.
Krippahl, L., & Barahona, P. (2005). Applying Constraint Programming Rigid Body
Protein Docking. Principles Practice Constraint Programming, pp. 373
387. Springer Verlag.
Krippahl, L., & Barahona, P. (1999). Applying Constraint Programming Protein Structure Determination. Principles Practice Constraint Programming, pp. 289
302. Springer Verlag.
997

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Larhlimi, A., & Bockmayr, A. (2009). New Constraint-Based Description SteadyState Flux Cone Metabolic Networks. Discrete Applied Mathematics, 157 (10),
22572266.
LaValle, S. (2006). Planning Algorithms. Cambridge University Press.
Lazaridis, T., Archontis, G., & Karplus, M. (1995). Enthalpic Contribution Protein
Stability: Atom-Based Calculations Statistical Mechanics. Adv. Protein Chem.,
47, 231306.
Lee, J., Kim, S., Joo, K., Kim, I., & Lee, J. (2004). Prediction Protein Tertiary Structure
using Profesy, Novel Method Based Fragment Assembly Conformational
Space Annealing. Proteins, 56 (4), 704714.
Lee, J., Lee, D., Park, H., Coutsias, E., & Seok, C. (2010). Protein Loop Modeling Using
Fragment Assembly Analytical Loop Closure. Proteins, 78 (16), 34283436.
Liu, P., Zhu, F., Rassokhin, D., & Agrafiotis, D. (2009). Self-organizing Algorithm
Modeling Protein Loops. PLOS Comput Biol, 5 (8).
Lovell, S., Davis, I., Arendall, W., de Bakker, P., Word, J., Prisant, M., Richardson, J., &
Richardson, D. (2003). Structure Validation C Geometry: , C Deviation.
Proteins, 50, 437450.
Mann, M., & Dal Palu, A. (2010). Lattice Model Refinement Protein Structures.
Workshop Constraint-Based Methods Bioinformatics.
Micheletti, C., Seno, F., & Maritan, A. (2000). Recurrent oligomers proteins: optimal scheme reconciling accurate concise backbone representations automated
folding design studies. proteins, 40 (4), 662674.
Moll, M., Schwarz, D., & Kavraki, L. (2007). Roadmap Methods Protein Folding. Humana Press.
Molloy, K., Saleh, S., & Shehu, A. (2013). Probabilistic Search Energy Guidance
Biased Decoy Sampling Ab-Initio Protein Structure Prediction. IEEE/ACM Trans.
Comput. Biology Bioinform, PrePrint.
Neumaier, A. (1997). Molecular Modeling Proteins Mathematical Prediction
Protein Structure. SIAM Review, 39, 407460.
Noonan, K., OBrien, D., & Snoeyink, J. (2005). Protein Backbone Motion Inverse
Kinematics. International Journal Robotics Research, 24 (11), 971982.
Olson, B. S., Molloy, K., & Shehu, A. (2011). Search Protein Native State
Probabilistic Sampling Approach. J. Bioinformatics Computational Biology,
9 (3), 383398.
Raman, S., Vernon, R., Thompson, J., Tyka, M., Sadreyev, R., Pei, J., Kim, D., Kellogg, E.,
DiMaio, F., Lange, O., Kinch, L., Sheffler, W., Kim, B.-H., Das, R., Grishin, N. V.,
& Baker, D. (2009). Structure Prediction CASP8 All-atom Refinement using
Rosetta. Proteins, 77 (Suppl. 9), 8999.
Rapp, C. S., & Friesner, R. A. (1999). Prediction Loop Geometries using Generalized
Born Model Solvation Effects. Proteins, 35, 173183.
998

fiA Constraint Solver Flexible Protein Models

Ray, O., Soh, T., & Inoue, K. (2010). Analyzing Pathways Using ASP-Based Approaches.
Algebraic Numeric Biology, 4th International Conference, pp. 167183. Springer
Verlag.
Rossi, F., van Beek, P., & Walsh, T. (2006). Handbook Constraint Programming. Elsevier
Science Inc.
Rufino, S., Donate, L., Canard, L., & Blundell, T. (1997). Predicting Conformational
Class Short Medium Size Loops Connecting Regular Secondary Structures:
Application Comparative Modeling. J. Mol. Biol., 267, 352367.
Shehu, A. (2009). Ab-Initio Tree-Based Exploration Enhance Sampling Low-Energy
Protein Conformations. Proceedings Robotics: Science Systems V.
Shehu, A. (2010). Conformational Search Protein Native State, pp. 431452. John
Wiley & Sons. Inc.
Shehu, A., & Kavraki, L. (2012). Modeling Structures Motions Loops Protein
Molecules. Entropy, 14, 252290.
Shen, M., & Sali, A. (2006). Statistical Potential Assessment Prediction Protein
Structures. Protein Sci, 15, 25072524.
Shih, E., & Hwang, M.-J. (2011). Use Distance Constraints Protein-Protein
Docking Computations. Proteins: Structure, Function, Bioinformatics, 80 (1),
194205.
Shmygelska, A., & Hoos, H. (2005). Ant Colony Optimisation Algorithm 2D
3D Hydrophobic Polar Protein Folding Problem. BMC Bioinformatics, 6, 3052.
Shmygelska, A., & Levitt, M. (2009). Generalized Ensemble Methods De Novo Structure
Prediction. Proceedings National Academy Science (USA), 106 (5), 1415
1420.
Simoncini, D., Berenger, F., Shrestha, R., & Zhang, K. (2012). Probabilistic FragmentBased Protein Structure Prediction Algorithm. PLOS One, 7 (7), e38799.
Simons, K., Kooperberg, C., Huang, E., & Baker, D. (1997). Assembly Protein Tertiary
Structures Fragments Similar Local Sequences using Simulated Annealing
Bayesian Scoring Functions. J. Mol. Biol., 268, 209225.
Skolnick, J., Fetrow, J., & Kolinski, A. (2000). Structural Genomics Importance
Gene Function Analysis. Nat. Biotechnology, 18 (3), 283287.
Soto, C., Fasnacht, M., Zhu, J., Forrest, L., & Honig, B. (2008). Loop Modeling: Sampling,
Filtering, Scoring. Proteins: Structure, Function, Bioinformatics, 70, 834
843.
Spassov, V., Flook, P., & Yan, L. (2008). LOOPER: Molecular Mechanics-based Algorithm Protein Loop Prediction. Protein Eng, 21, 91100.
Stuckey, P. J., Becket, R., & Fischer, J. (2010). Philosophy MiniZinc Challenge.
Constraints, 15 (3), 307316.
Sussmann, G., & Steele, G. (1980). CONSTRAINTS: Language Expressing AlmostHierarchical Descriptions. Artificial Intelligence, 14 (1), 139.
999

fiCampeotto, Dal Palu, Dovier, Fioretto, & Pontelli

Sutherland, I. (1963). Sketchpad: Man-Machine Graphical Communication System. Tech.
rep. 296, Lincoln Laboratory, MIT.
Swedish Institute Computer Science (2012). SICStus Prolog Home Page. http://www.
sics.se/sicstus/.
Thebault, P., de Givry, S., Schiex, T., & Gaspin, C. (2005). Combining Constraint Processing Pattern Matching Describe Locate Structured Motifs Genomic
Sequences. Fifth Workshop Modeling Solving Problems Constraints,
pp. 5360.
Tsai, Y., Huang, Y., Yu, C., & Lu, C. (2004). MuSiC: Tool Multiple Sequence
Alignment Constraints. Bioinformatics, 20 (14), 23092311.
Xiang, Z., Soto, C., & Honig, B. (2002). Evaluating Conformal Energies: Colony Energy
Application Problem Loop Prediction. PNAS, 99, 74327437.
Xu, D., & Zhang, Y. (2012). Ab Initio Protein Structure Assembly Using Continuous
Structure Fragments Optimized Knowledge-based Force Field. Proteins, 80 (7),
17151735.
Yang, R. (1998). Multiple Protein/DNA Sequence Alignment Constraints. International Conference Practical Applications Constraint Programming.
Yap, R. (2001). Parametric Sequence Alignment Constraints. Constraints, 6, 157172.
Yap, R., & Chuan, H. (1993). Constraint Logic Programming Framework Constructing
DNA Restriction Maps. Artificial Intelligence Medicine, 5 (5), 447464.
Yue, K., & Dill, K. (2000). Constraint Based Assembly Tertiary Protein Structures
Secondary Structure Elements. Proteins Science, 9 (10), 19351946.
Zhang, M., & Kavraki, L. (2002). New Method Fast Accurate Derivation
Molecular Conformations. Journal Chemical Information Computer Sciences,
42 (1), 6470.
Zhang, Y., & Hauser, K. (2013). Unbiased, Scalable Sampling Protein Loop Conformations Probabilistic Priors. BMC Structural Biology, (to appear http:
// www. indiana. edu/ ~ motion/ slikmc/ papers/ BMC_ Zhang. pdf ).
Zhou, H., & Zhou, Y. (2002). Distance-scaled, Finite Ideal-gas Reference State Improves
Structure-derived Potentials Mean Force Structure Selection Stability Prediction. Protein Sci, 11, 27142726.

1000

fiJournal Artificial Intelligence Research 48 (2013) 841-883

Submitted 06/13; published 11/13

Scalable Efficient Bayes-Adaptive Reinforcement
Learning Based Monte-Carlo Tree Search
Arthur Guez

aguez@gatsby.ucl.ac.uk

Gatsby Computational Neuroscience Unit
University College London
London, WC1N 3AR, UK

David Silver

d.silver@cs.ucl.ac.uk

Dept. Computer Science
University College London
London, WC1E 6BT, UK

Peter Dayan

dayan@gatsby.ucl.ac.uk

Gatsby Computational Neuroscience Unit
University College London
London, WC1N 3AR, UK

Abstract
Bayesian planning formally elegant approach learning optimal behaviour
model uncertainty, trading exploration exploitation ideal way. Unfortunately,
planning optimally face uncertainty notoriously taxing, since search space
enormous. paper introduce tractable, sample-based method approximate
Bayes-optimal planning exploits Monte-Carlo tree search. approach avoids expensive applications Bayes rule within search tree sampling models current
beliefs, furthermore performs sampling lazy manner. enables outperform previous Bayesian model-based reinforcement learning algorithms significant
margin several well-known benchmark problems. show, approach even
work problems infinite state space lie qualitatively reach almost
previous work Bayesian exploration.

1. Introduction
key challenge sequential decision-making understand agents learn
collect rewards avoid costs interactions world. natural way
characterize interactions Markov Decision Process (mdp). mdps consist
set states, set possible actions, transition model stochastically decides
successor state given state action. addition, cost reward associated
state action. problem learning arises aspects
transition model unknown agent, implying uncertainty best strategy
gathering rewards avoiding costs. Exploration therefore necessary reduce
uncertainty ensure appropriate exploitation environment. Weighing benefits
exploring, identify potentially better actions, benefits exploiting known
sources rewards generally referred exploration-exploitation trade-off.
trade-off formalized various different ways. One possible objective
control number suboptimal actions agent ever performs; algorithms high
2013 AI Access Foundation. rights reserved.

fiGuez, Silver, & Dayan

probability bound number suboptimal steps polynomial number
states actions said pac-mdp (Strehl, Li, & Littman, 2009). Instead
focusing suboptimal actions, another objective minimize so-called regret,
expected loss relative optimal policy mdp (Jaksch, Ortner, & Auer,
2010). Lastly, Bayesian decision theory prescribes maximizing expected discounted
sum rewards light prior distribution transition models; one way
achieve solving augmented mdp, called Bayes-Adaptive mdp (bamdp),
corresponding augmented dynamics known (Martin, 1967; Duff, 2002).
augmentation posterior belief distribution dynamics, given data far
observed. agent starts belief state corresponding prior and, executing
greedy policy bamdp whilst updating posterior, acts optimally (with respect
beliefs) original mdp. Bayes-optimal policy optimal policy
bamdp; integrates exploration exploitation ideal manner.
general, different objectives compatible see example work
Kolter Ng (2009) incompatibility pac-mdp Bayes-optimal solution. pac-mdp regret frameworks gained considerable traction recent years,
Bayesian exploration comparatively ignored. However, Bayesian framework attractive structured prior knowledge incorporated solution
principled manner, providing means tackle, least theory, large complex
unknown environments. Methods tailored objectives pac-mdp regret minimization cannot far easily adapted exploit priors. assumption
environment, thus forced explore every state action least once,
hopeless large environments.
Unfortunately, exact Bayesian reinforcement learning (RL) computationally intractable.
Various algorithms devised approximate optimal learning, often rather
large cost. computational barrier restricted Bayesian RL small domains
simple priors. paper, present tractable approach exploits extends
recent advances Monte-Carlo tree search (mcts) (Kocsis & Szepesvari, 2006), notably
partially-observable mdps (Silver & Veness, 2010) bamdp seen
special case. mcts capable tackling large mdp problems dynamics
known (Gelly, Kocsis, Schoenauer, Sebag, Silver, Szepesvari, & Teytaud, 2012), show
naive application mcts bamdp tractable general propose
set principled modifications obtain practical algorithm, called bamcp
Bayes-Adaptive Monte-Carlo Planner.
iteration bamcp, pomcp algorithm (Silver & Veness, 2010), single
mdp sampled agents current beliefs. mdp used simulate single
episode whose outcome used update value node search tree traversed
simulation. integrating many simulations, therefore many sample
mdps, optimal value future sequence obtained respect agents
beliefs. prove process converges Bayes-optimal policy, given infinite
samples. Since many priors appropriate Bayesian RL setting require
form approximate inference, extend convergence proof show bamcp
also converges combined Markov Chain Monte Carlo-based inference scheme.
algorithm efficient previous sparse sampling methods Bayes-adaptive
planning (Wang, Lizotte, Bowling, & Schuurmans, 2005; Castro, 2007; Asmuth & Littman,
842

fiBayes-Adaptive Monte-Carlo Planning

2011), partly update posterior belief state course
simulation. thus avoids repeated applications Bayes rule, expensive
simplest priors mdp. increase computational efficiency further,
introduce additional innovation: lazy sampling scheme samples posterior
distribution states traversed simulation.
applied bamcp representative sample benchmark problems competitive algorithms literature. consistently significantly outperformed existing
Bayesian RL methods, also recent non-Bayesian approaches, thus achieving state-ofthe-art performance.
Further, bamcp particularly well suited support planning large domains
richly structured prior knowledge makes lazy sampling possible effective.
offers prospect applying Bayesian RL realistically complex scale. illustrate
possibility showing bamcp tackle domain infinite number
states structured prior dynamics, challenging, radically intractable,
task existing approaches. example exploits bamcps ability use Markov chain
Monte Carlo methods inference associated posterior distribution models.
paper organized follows. First, formally define Bayesian model-based
RL problem review existing methods; present new algorithm context
previous suggestions; finally report empirical results existing domains
new, infinite, task. results appeared short conference version
paper (Guez, Silver, & Dayan, 2012).

2. Model-Based Reinforcement Learning
first briefly review model-based reinforcement learning search algorithms
case model known. introduce formalism Bayesian model-based
RL provide survey existing approximation algorithms motivate approach.
2.1 Model-Based Reinforcement Learning Known Model
mdp described 5-tuple = hS, A, P, R, i, discrete set states,
finite set actions, P : R state transition probability kernel,
R : R bounded reward function, discount factor (Szepesvari,
2010). deterministic stationary mdp policy defined mapping :
states actions. value function policy state expected return,
defined as:
"
#
X



V (s) E
rt |s0 = ,
(1)
t=0

rt random reward obtained time following policy state
E denotes expectation operator averages possible paths policy
implies. related quantity action-value function policy executing
843

fiGuez, Silver, & Dayan

particular action state executing :
"
#
X
X

0

t1
0
Q (s, a) R(s, a) +
P(s, a, )EM
rt |s1 =
s0

= R(s, a) +

X

(2)

t=1

P(s, a, s0 )V (s0 ),

(3)

s0

implying relation V (s) = Q (s, (s)). optimal action-value function, denoted Q ,
provides maximum expected return Q (s, a) obtained executing action
state s. optimal value function, V , similarly defined related Q
V (s) = maxaA Q (s, a), S. optimal policy achieves maximum expected
return states, obtained Q as: (s) = argmaxaA Q (s, a),
breaking ties arbitrarily.
components mdp tuple known including model P
standard mdp planning algorithms used estimate optimal policy off-line,
Value Iteration Policy Iteration (Bellman, 1954; Ross, 1983; Sutton & Barto,
1998; Szepesvari, 2010). However, always practical find optimal policy
states large mdps one fell swoop. Instead, methods concentrate
searching online best action current state st . particularly common
model-based Bayesian RL algorithms. therefore introduce relevant existing online
search methods mdps used building blocks Bayesian RL algorithms.
2.1.1 Online Search
Online search methods evaluate tree possible future sequences. tree rooted
current state composed state action nodes. state node, including
root, children actions legal state. turn, action
node children successor states resulting action. goal
forward search algorithm recursively estimate value state action node
tree. Ultimately, value possible action root used select
next action, process repeats using new state root.
Online search methods may categorised firstly backup method
value node updated, secondly order nodes tree
traversed backups applied.
2.1.2 Full-Width Search
Classical online search methods based full-width backups, consider legal
actions possible successor states, example using Bellman backup,

V (s) max R(s, a) +
aA

X

P(s, a, s0 )V (s0 )

(4)

s0

Search efficiency largely determined order nodes traversed.
One example best-first, current best usually determined according
optimistic criterion. leads algorithm resembling (Hart, Nilsson, & Raphael,
844

fiBayes-Adaptive Monte-Carlo Planning

1968), applies deterministic case. search tree may also truncated,
using knowledge extreme reward discount factor ensure
provably benign (Davies, Ng, & Moore, 1998). one prepared give guarantees
optimality, approximate value function (typically described online search literature
heuristic function evaluation function) applied leaf nodes substitute
value truncated subtree.
2.1.3 Sample-Based Search
Rather expanding every tree node completely, sample-based search methods overcome
curse dimensionality sampling successor states transition distribution.
generic advantage full-width search expend little effort
unlikely paths tree.
Sparse Sampling
Sparse Sampling (Kearns, Mansour, & Ng, 1999) sample-based online search algorithm.
key idea sample C successor nodes action node, apply Bellman
backup sampled transitions, update value parent state node
values child nodes:
C
X
Vd (s) = max R(s, a) +
Vd+1 (child(s, a, i)).
aA
C

(5)

i=1

search tree traversed depth-first manner, approximate value function
employed truncated leaf nodes, pre-defined depth D. Sparse Sampling
converges near-optimal policy given appropriate choice parameters C D.
FSSS
Although Sparse Sampling concentrates likely transitions, focus search
nodes relatively high values returns. work Walsh, Goschin,
Littman (2010), Forward Search Sparse Sampling (fsss) extends regular Sparse Sampling
maintaining lower upper bounds value node:
Ld (s, a) = R(s, a) +
Ud (s, a) = R(s, a) +


C

C

X

Ld+1 (s0 )Count(s, a, s0 ),

(6)

Ud+1 (s0 )Count(s, a, s0 ),

(7)

s0 Child(s,a)

X
s0 Child(s,a)

Ld (s) = max Ld (s, a),

(8)

Ud (s) = max Ud (s, a),

(9)

aA

aA

Child(s, a) set successor states sampled C draws P(s, a, ),
Count(s, a, s0 ) number times set element sampled. Whenever node
created, lower upper bounds initialized according Ld (s, a) = Vmin
845

fiGuez, Silver, & Dayan

Ud (s, a) = Vmax , i.e., worst best possible returns. tree traversed bestfirst manner according value bounds, starting root simulation
tree. state node, promising action selected maximising
upper bound value. action (or chance) node, successor states selected
sampled set C candidates maximising uncertainty (upper minus lower
bound). effectively prunes branches tree low upper bounds
exhaustively explored, still maintaining theoretical guarantees Sparse
Sampling.
Monte-Carlo Tree Search
Despite theoretical guarantees, practice, sparse sampling fsss suffer
fact truncate search tree particular depth, experience bias
associated approximate value function use leaves. Monte-Carlo Tree
Search (mcts) provides way reducing bias evaluating leaves exactly using
model, employing sub-optimal, rollout policy. formally, mcts, states
evaluated averaging many simulations. simulation starts root
traverses current tree leaf reached, using tree policy (e.g., greedy action
selection) based information far gathered nodes tree.
results (locally) best-first tree traversal, step tree policy selects
best child (best according exploration criterion) given current values tree.
Rather truncating search relying potentially biased value function leaf
nodes, different policy, called rollout policy (e.g., uniform random) employed
leaf node termination search horizon. node traversed simulation
updated Monte-Carlo backup, simply evaluates node mean
outcome simulations passed node. Specifically, Monte-Carlo
backups update value action node follows:
Qd (s, a) Qd (s, a) + (R Qd (s, a))/Nd (s, a),

(10)

R sampled discounted return obtained traversed action node s,
depth Nd (s, a) visitation count action node s, (i.e., update computes
mean sampled returns obtained action node simulations).
particular tree policy mcts received much attention, indeed underlies
algorithm, uct (Upper Confidence bounds applied Trees) policy (Kocsis &
Szepesvari, 2006). uct employs ucb (Upper Confidence Bounds) algorithm (Auer,
Cesa-Bianchi, & Fischer, 2002), designed multi-armed bandit problems, select adaptively actions every state node according to:
p
argmax Qd (s, a) + c log(Nd (s))/Nd (s, a),
(11)
aA

c exploration constant needs set appropriately Nd (s)
visitation count state node s. tree policy treats forward search metaexploration problem, preferring exploit regions tree currently appear better
others, continuing explore unknown less known parts tree. leads
good empirical results even small numbers simulations, effort expended
846

fiBayes-Adaptive Monte-Carlo Planning

search seems fruitful. Nevertheless parts tree eventually visited infinitely
often, therefore algorithm shown converge optimal policy
long run.
Despite negative theoretical results showing uct slow find optimal
policies carefully designed counterexample mdps (Coquelin & Munos, 2007), uct
successful many large mdp domains (Gelly et al., 2012).
2.2 Model-Based Bayesian Reinforcement Learning
methods far discussed depend agent model world, i.e.,
dynamics P. key concern Bayesian RL acting model fully
known. first describe generic Bayesian formulation optimal decision-making
unknown mdp, following Martin (1967) Duff (2002), consider approximations
inspired intractability full problem.
2.2.1 Formalism
Given dynamics P P (coming set possible models)
incompletely known, Bayesian RL treats latent random variable follows
prior distribution P (P). Observations dynamics contained history ht (at
time t) actions states: ht s1 a1 s2 a2 . . . at1 st , duly lead posterior distribution
P via likelihood.
history ht influences posterior distribution. Thus policies integrate
exploration exploitation (called EE policies) Bayesian RL problem generically
take history account, along current state, order specify
action take. is, whereas P known, policy defined mapping
: [0, 1] current state actions probability (of execution),
Bayesian RL, EE policies defined mappings history, current state, action
probability : H [0, 1], H set possible histories.1 denote
set EE policies.
objective EE policy Bayesian formulation maximize
expected return (sum discounted rewards), expectation taken distribution environments P (P) = P (P |), addition taking usual expectation
stochasticity return induced dynamics. Formally, define expected
discounted return v starting state seeing history h following EE
policy as:
"
#
X
v(s, h, ) = E
rt |s0 = s, h0 = h
(12)
t=0

Z
=
P

=

"
dP P (P |h) EM (P)

X
a0


X

#
rt |s0 = s, h0 = h

(13)

t=0

R(s, a0 ) +

X

(s, h, a0 )v(s0 , ha0 s0 , )P(s, a0 , s0 , h),

(14)

s0

1. redundancy state-history notation throughout paper, namely current state could
extracted history, present ensure clarity exposition.

847

fiGuez, Silver, & Dayan

R
P(s, a, s0 , h) P dP P (P |h) P(s, a, s0 ) denotes probability transitioning
state s0 executing distribution dynamics P (P |h), (P) denotes
mdp associated dynamics P.
Definition 1 Given S, A, R, , prior distribution P (P) dynamics
mdp , let
v (s, ) = sup v(s, , ).

(15)



Martin (1967, Thm. 3.2.1) shows exists strategy achieves
expected return (i.e., v(s, , ) = v (s, )). EE strategy called Bayesoptimal policy.2
formulation prescribes natural recipe computing Bayes-optimal policy.
observing history ht mdp, posterior belief P updated using Bayes
rule P (P|ht ) P (ht |P)P (P) (or recursive form P (P|ht ) P(st1 , at1 , st )P (P|ht1 )).
Thus, uncertainty dynamics model transformed certainty
current state inside augmented state space + = H, state
space original problem H set possible histories. dynamics associated
augmented state space described
Z
P + (hs, hi, a, hs0 , h0 i) = 1[h0 = has0 ]
P(s, a, s0 )P (P|h) dP,
(16)
P

reward function simply projected reward function original mdp:
R+ (hs, hi, a) = R(s, a).

(17)

Together, 5-tuple + = hS + , A, P + , R+ , forms Bayes-Adaptive mdp (bamdp)
mdp problem . Since dynamics bamdp known, principle
solved obtain optimal value function associated action:
"
#
X 0



Q (hst , ht i, a) = max EM +

rt0 |at =
(18)


t0 =t

optimal action state readily derived. Optimal actions
bamdp executed greedily real mdp constitute best course action
Bayesian agent respect prior belief P:
Proposition 1 (Silver, 1963; Martin, 1967) optimal policy bamdp
Bayes-optimal policy, defined Definition 1.
obvious expected performance bamdp policy mdp
bounded optimal policy obtained fully-observable model,
2. proof Martin (1967) covers finite state spaces, extended specific kinds
infinite state spaces one consider Section 4.2, see Appendix details.

848

fiBayes-Adaptive Monte-Carlo Planning

equality occurring, example, degenerate case prior support
true model.
Bayes-optimal policy stationary function augmented state, evolves
time observed context original mdp function state
only. Since uncertainty dynamics taken account optimization
return, Bayes-optimal policy integrates exploration exploitation optimally.
useful observe bamdp particular form Partially Observable
mdp (pomdp). state space pomdp P, P set possible
models P. second component state space static hidden, partially
observed experienced transitions. Planning conducted belief space,
equivalently space sufficient statistics belief distribution, allowing decisions
taken light likely outcomes gathering exploitable information
hidden state. case bamdp, actions gather information hidden
model P. However, pomdp discrete pomdp since state space continuous
(with discrete observations). Therefore, pointed Duff (2002), many classical
solutions pomdps cannot directly applied bamdp.
practical perspective, solving bamdp exactly computationally intractable,
even small state spaces. First, augmented state space contains possible histories
therefore infinite. Second, transitions bamdp, described Equation 16,
require integration transition models posterior. Although operation
trivial simple probabilistic models (e.g., independent Dirichlet-Multinomial),
intractable priors interest (see Section 4.2 example). However, certain
special cases bamdp known somewhat tractable. example,
celebrated Gittins indices provide shortcut solution bandit problems (Gittins, Weber,
& Glazebrook, 1989), although calculating indices remains challenge general.
Further, optimal solution least finite-horizon linear-Gaussian control problems computed exactly (Tonk & Kappen, 2010). Nevertheless, appears unlikely
exists tractable exact algorithm solve general bamdps, justifying search
sound efficient approximations.
2.2.2 Approximate Bayes-Adaptive Algorithms
Three coarse classes approximation methods developed, review.
Note analogues solution methods pomdps.
First offline methods toil mightily provide execution policies used
observed augmented state. Second third two sets online methods
concentrate current augmented state. One set methods uses sparse sampling
full tree future states actions associated bamdp, starting
current augmented state. samples solves one mdps current
posterior P, possibly correcting bias towards exploitation typically
leads.
describing classes, highlight currently lack, establish
basis new algorithm, bamcp.
849

fiGuez, Silver, & Dayan

Offline Methods
One idea solve entire bamdp offline, every state belief (or history).
obviates need anything simple value/policy lookup execution.
However, avenue approximation led much practical success presumably
difficulties associated size bamdp, including fact
gargantuan amounts computation may performed find good policies parts
space histories actually sampled practice.
Existing approaches class include actor-critic algorithm (Duff, 2003),
learning, point-based value iteration algorithm, called beetle (Bayesian Exploration Exploitation Tradeoff LEarning) (Poupart, Vlassis, Hoey, & Regan, 2006).
beetle builds approximate policy off-line exploiting facets structure
value functions bamdps, inherit broader, parent, class pomdps.
recently, Wang, Won, Hsu, Lee (2012) propose solve offline pomdp representing latent dynamics discrete partially-observed state component,
value state component corresponds one K possible models sampled
prior. approach fail true model well-represented K sampled
models.
Offline methods particularly poorly suited problems infinite state task
consider section 4.2.
Online Methods: Sparse Sampling
Online methods reduce dependency size bamdp approximating
bamdp solution around current (augmented) state agent running planning
algorithm step.
One idea perform forms forward search current state. Although
methods concentrate current state, search tree still large expensive
evaluate given path tree. partial alleviation problem, approaches
rely form sparse, non-uniform, tree exploration minimize search effort
(but see also Fonteneau, Busoniu, & Munos, 2013). Section 2.1.3 described search
algorithms mdps, present existing extensions bamdp setting. Analogous
methods pomdps reviewed Ross, Pineau, Paquet, Chaib-Draa (2008).
Wang et al. applied Sparse Sampling search online bamdps (Wang et al., 2005),
expanding tree non-uniformly according sampled trajectories. state node,
promising action selected via Thompson sampling (Thompson, 1933; Agrawal & Goyal,
2011) i.e., sampling mdp belief-state, solving mdp taking optimal
action. Sparse Sampling, fails exploit information values nodes
prioritizing sampling process. chance (action) node, successor belief-state
sampled transition dynamics bamdp.
Castro et al. applied Sparse Sampling define relevant region bamdp
current decision step. leads optimization problem solved using Linear
Programming (Castro & Precup, 2007).
Asmuth Littmans bfs3 algorithm (Asmuth & Littman, 2011) adapts Forward
Search Sparse Sampling (Walsh et al., 2010) bamdp (treated particular mdp).
Although bfs3 described Monte-Carlo tree search, fact uses Bellman backup
850

fiBayes-Adaptive Monte-Carlo Planning

rather Monte-Carlo evaluation. fsss, Bellman backup updates lower
upper bounds value node.
Online Methods: Dual Optimism
Instead applying sparse sampling methods tree future states actions,
alternative collection methods derives one simpler mdps posterior
current augmented state, whose solution often computationally straightforward. itself,
leads over-exploitation: corrections thus necessary generate sufficient exploration. Exploration seen coming optimism face uncertainty actions
yet tried sufficiently must look attractive current mean.
Indeed, various heuristic forms exploration bonus (Sutton, 1990; Schmidhuber,
1991; Dayan & Sejnowski, 1996; Kearns et al., 1999; Meuleau & Bourgine, 1999; Brafman
& Tennenholtz, 2003) generalize optimism inherent optimal solutions
Gittins indices.
One approximation first derived work Cozzolino, Gonzalez-Zubieta,
Miller (1965), mean estimate transition probabilities (i.e., mean
posterior) employed certainty equivalence approximation. Solving corresponding mean mdp induces form optimism, always sufficient drive
exploration. idea revisited linked reinforcement learning formulations
Dayan Sejnowski (1996).
Another way induce optimism exploit variance posterior sampling
mdps augmented state. One approaches Bayesian DP algorithm (Strens,
2000). step (or every couple steps), single model sampled
posterior distribution transition models, action optimal model
executed. Although popular approach practice, algorithm known
theoretical guarantee relating Bayes-optimal solution. Bandit case,
reduces Thompson Sampling. Optimism generated solving posterior samples
likely yield optimistic values unknown parts mdp (where posterior
entropy large) force agent visit regions. Best Sampled
Set (boss) algorithm generalizes idea (Asmuth, Li, Littman, Nouri, & Wingate, 2009).
boss samples number models posterior combines optimistically.
drives sufficient exploration guarantee finite-sample performance guarantees,
theoretical guarantees cannot easily related Bayes-optimal solution. boss
quite sensitive parameter governs sampling criterion, unfortunately
difficult select. Castro Precup proposed variant, referred sboss,
provides effective adaptive sampling criterion (Castro & Precup, 2010).
One also see certain non-Bayesian methods light. instance, Bayesian
Exploration Bonus (beb) solves posterior mean mdp, additional reward
bonus depends visitation counts (Kolter & Ng, 2009). bonus tailored
method satisfies so-called pac-bamdp property, generalizes pac-mdp
mentioned introduction, implies limiting polynomial factor number
steps EE policy different Bayes-optimal policy. recent
approach bolt algorithm, merges ideas beb boss, enforces optimism
transitions (temporarily) adding fictitious evidence currently poorly-known
851

fiGuez, Silver, & Dayan

actions lead currently poorly-known states (Araya-Lopez, Buffet, & Thomas, 2012). bolt
also pac-bamdp property.
Discussion Existing Methods
Despite recent progress approximation algorithms, tackling large domains complex structured priors remains computational reach existing Bayesian RL methods.
Unfortunately, exactly structured domains Bayesian methods shine,
since statistical capacity take advantage priors.
Methods tackle bamdp directly forward-search methods, suffer
repeated computation bamdp dynamics inside search tree priors.
is, compute single bamdp transition Equation 16, one needs apply Bayes
rule perform integration possible models. done cheaply
simple priors, rather expensive arbitrary priors.
hand, optimism-based methods attractive appear
tractable since dealing smaller mdps. However, turns hard
translate sophisticated prior knowledge form bonus existing methods
compatible simple Dirichlet-Multinomial models. Moreover, behavior
early steps exploration sensitive precise parameter inducing
optimism.
therefore developed approximation algorithm Bayesian RL compatible
complex priors, maintaining efficiency (Bayesian) soundness, large
EE tasks approached principled way. approach adapts pomcp MonteCarlo tree search algorithm (Silver & Veness, 2010), avoids expensive applications
Bayes rule sampling root tree. also extends approach
introducing novel scheme lazy sampling. makes possible search locally
finite portions large even infinite domains.

3. BAMCP Algorithm
reiterate, goal bamdp planning method find, decision point hs, hi
encountered, action least approximately maximizes future expected return
(i.e., find optimal EE policy (s, h)). algorithm, Bayes-Adaptive Monte-Carlo
Planning (BAMCP), performing forward-search space possible
future histories bamdp using tailored Monte-Carlo tree search.
employ uct algorithm, presented Section 2.1.3, allocate search effort
promising branches state-action tree, use sample-based rollouts provide value
estimates node. clarity, let us denote Bayes-Adaptive uct (ba-uct)
algorithm applies vanilla uct bamdp (i.e., particular mdp dynamics
described Equation 16).3 Sample-based search bamdp using ba-uct requires
generation samples P + every step simulation expensive procedure
simplest generative models P (P). avoid cost sampling
single transition model P posterior root search tree start
3. using uct solve bamdps mentioned Asmuth Littman (2011), aware
published work evaluated performance ba-uct.

852

fiBayes-Adaptive Monte-Carlo Planning

simulation i, using P generate necessary samples simulation.
Sample-based tree search acts filter, ensuring correct distribution state
successors obtained tree nodes, sampled P + . root
sampling method originally introduced pomcp algorithm (Silver & Veness, 2010),
developed solve discrete-state pomdps.
Combining ba-uct version root sampling forms basis proposed
bamcp algorithm; detailed Section 3.1. addition, bamcp also takes advantage
lazy sampling reduce sampling complexity root, detailed Section 3.2.
Finally, bamcp integrates rollout learning improve rollouts online, detailed
Section 3.3. Section 3.4, show bamcp converges Bayes-optimal solution.
Subsequent sections provide empirical results.
3.1 BA-UCT Root Sampling
root node search tree decision point represents current state
bamdp. tree composed state nodes representing belief states hs, hi action
nodes representing effect particular actions parent state node. visit
counts: N (hs, hi) state nodes, N (hs, hi, a) action nodes, initialized 0
updated throughout search. value, Q(hs, hi, a), initialized 0, also maintained
action node.
simulation traverses tree without backtracking
following uct policy
p
state nodes defined argmaxa Q(hs, hi, a) + c log(N (hs, hi))/N (hs, hi, a), c
exploration constant needs set appropriately. Given action, transition
distribution P corresponding current simulation used sample next state.
is, action node (hs, hi, a), s0 sampled P (s, a, ), new state node
set hs0 , has0 i.
simulation reaches leaf, tree expanded attaching new state node
connected action nodes, rollout policy ro used control mdp defined
current P . policy followed fixed total depth (determined using
discount factor). rollout provides estimate value Q(hs, hi, a) leaf
action node. estimate used update value action nodes traversed
simulation: R sampled discounted return obtained traversed
action node (hs, hi, a) given simulation, update value action node
Q(hs, hi, a)+ (R Q(hs, hi, a))/N (hs, hi, a) (i.e., mean sampled returns obtained
action node simulations).
detailed description bamcp algorithm provided Algorithm 1. diagram
example bamcp simulations presented Figure 1. Section 3.4, show bamcp
eventually converges Bayes-optimal policy.
Finally, note history transitions h generally compact sufficient statistic belief fully observable mdps. can, instance, replaced
unordered transition counts , considerably reducing number states bamdp
and, potentially complexity planning. bamcp search reduced search space,
takes form expanding lattice rather tree. found version
bamcp offer marginal improvement. common finding mcts, stemming tendency concentrate search effort one several equivalent paths (up
853

fiGuez, Silver, & Dayan

1.

2.
Past

Past

Planning

Planning

Tree policy

Tree policy

Rollout
policy

Rollout
policy

0

0

0

4.

3.

0

2

Past

Past

Planning

Planning

Tree policy

Tree policy

Rollout
policy

Rollout
policy

0

0

2

0

Figure 1: diagram presents first 4 simulations bamcp mdp 2 actions state
hst , ht i. rollout trajectories represented dotted lines (green current rollouts,
greyed past rollouts). 1. root node expanded two action nodes. Action
a1 chosen root (random tie-breaking) rollout executed P 1 resulting
value estimate 0. Counts N (hst , ht i) N (hst , ht i, a1 ), value Q(hst , ht i, a1 ) get updated.
2. Action a2 chosen root rollout executed value estimate 0. Counts
value get updated. 3. Action a1 chosen (tie-breaking), s0 sampled P 3 (st , a1 , ).
State node hs0 , ht a1 s0 gets expanded action a1 selected, incurring reward 2, followed
rollout. 4. UCB rule selects action a1 top, successor state s0 sampled
P 4 (st , a1 , ). Action a2 chosen internal node hs0 , ht a1 s0 i, followed rollout using
P 4 ro . reward 2 obtained 2 steps tree node. Counts traversed
nodes updated MC backup updates Q(hs0 , ht a1 s0 i, a1 ) R = 0+0+ 2 2+ 3 0+ =
2 2 Q(hst , ht i, a1 ) + 2 3 /3 = 32 ( + 3 ).

854

fiBayes-Adaptive Monte-Carlo Planning

Algorithm 1: BAMCP

procedure Search( hs, hi )
repeat
P P (P|h)
Simulate(hs, hi, P, 0)
Timeout()
return argmax Q(hs, hi, a)


end procedure

procedure Rollout(hs, hi, P, )
Rmax <
return 0
end
ro (hs, hi, )
s0 P(s, a, )
r R(s, a)
return
r+Rollout(hs0 , has0 i, P, d+1)
end procedure

procedure Simulate( hs, hi, P, d)
Rmax < return 0
N (hs, hi) = 0

N (hs, hi, a) 0,
Q(hs, hi, a)) 0
end
ro (hs, hi, )
s0 P(s, a, )
r R(s, a)
R r + Rollout(hs0 , has0 i, P, d)
N (hs, hi) 1, N (hs, hi, a) 1
Q(hs, hi, a) R
return R
end
q
(hs,hi))
argmax Q(hs, hi, b) + c log(N
N (hs,hi,b)
b

s0 P(s, a, )
r R(s, a)
R r + Simulate(hs0 , has0 i, P, d+1)
N (hs, hi) N (hs, hi) + 1
N (hs, hi, a) N (hs, hi, a) + 1
Q(hs, hi, a) Q(hs, hi, a) +

RQ(hs,hi,a)
N (hs,hi,a)

return R
end procedure

transposition), implying limited effect performance reducing number
paths.
Note algorithm similar ba-uct root sampling also appeared
work Vien Ertel (2012), shortly bamcp originally published Guez et al.
(2012).
3.1.1 Root Sampling Work Simple Example
illustrate workings bamcp, particular root sampling, simulated example
showcases crucial component Bayes-adaptivity.
Consider simple prior distribution two mdps (P 0 P 1 ), illustrated Figure 2,
P (P = P 0 ) = P (P = P 1 ) = 21 . mdps episodic stop leaves,
episode starts s0 . state s1 s2 , action expected reward
0 prior distribution mdps. Nevertheless, outcome transition
action a0 state s0 carries information identity mdp, allows Bayesadaptive agent take informed decision state s1 s2 . Using Bayes-rule,
P (P = P 0 |s0 a0 s1 ) P (s0 a0 s1 | P = P 0 )P (P = P 0 ) = 0.8.
855

fiGuez, Silver, & Dayan

s0
a0

a1

s1 p = 0.8 s2 p = 0.2
a0

a1

s3 p = 1
+2

a0
s4 p = 1
2
(a)

s5 p = 1
0

a1

s4 p = 1
2

s3 p = 1
+2

P = P0
s0
a0

a1

s1 p = 0.2 s2 p = 0.8
a0

a1

s4 p = 1
2

a0
s3 p = 1
+2
(b)

s3 p = 1
+2

a1

s5 p = 1
0
s4 p = 1
2

P = P1

Figure 2: two mdps Section 3.1.1, prior probability P (P = P 0 ) = P (P = P 1 ) =
1
2 . Differences two mdps highlighted blue.

therefore compute optimal values:
(
2P (P = P 0 |h) 2P (P = P 1 |h)

V (h = s0 a0 s1 ) = max
2P (P = P 1 |h) 2P (P = P 0 |h)

(19)

= 2 0.8 2 0.2 = 1.2 (= V (h = s0 a0 s2 ))
V (h = s0 ) = max{0, 1.2} = 1.2.
simulate bamcp simple example first decision state s0 .
root sampling, bamcp samples either P 0 P 1 equal probability root
tree, perform explicit posterior update inside tree. Yet, suggested
Lemma 1, expect find correct distribution P (P = P 0 |s0 a0 s1 ) samples
P tree node hs0 a0 s1 i. Moreover, bamcp converge optimal values V
according Theorem 1. observed empirically Figure 3.
second row Figure 3, observe Q(s0 a0 s1 , a1 ) slower converge
compared values. time ticking slowly non-optimal
node (i.e., small fraction simulations reach node) value stays put many
simulations.
3.2 Lazy Sampling
previous work sample-based tree search, indeed including pomcp (Silver & Veness,
2010), complete sample state drawn posterior root search tree.
However, computationally costly. Instead, sample P lazily, generating
856

fiBayes-Adaptive Monte-Carlo Planning

2

2

1.5

1.5

Value

1

V
V
V
V

0.5
0
0.5
1

0

200

400

600

800

1000

1200

1400

( 0a 0s 1)

( 0a 0s 1)
( 0)

( 0)
1600

1800

1
0.5
0
0.5
2000

1

0

5

10
4

x 10
0.5

0.5

Q( 0 0 1 , 1 )
Q ( 0a 0s 1, 1)

Value

1

1.5

2

1

1.5

0

200

400

600

800

1000

1200

1400

1600

1800

2000

2

0

5

10
4

x 10
1

1

P 0 0 s(1 P = P 1 )
P ( P = P 1| 0a 0s 1)

Probability

0.8
0.6

0.8
0.6

P 0 0 s(1 P = P 0 )
P ( P = P 0| 0a 0s 1)

0.4

0.4

0.2
0

0.2
0

200

400

600

800

1000

1200

Number simulations

1400

1600

1800

2000

0

0

5

10
4

x 10

Figure 3: Tracking different internal variables bamcp example Section 3.1.1 = 0.9.
bamcp run starting state number simulations (x-axis) c = 20.
first two rows show evolution values tree nodes corresponding different histories,
along target values computed Equation 20. bottom row shows evolution
Ps0 a1 s1 (P = P 0 ) = 1 Ps0 a1 s1 (P = P 1 ), empirical distribution mdps seen going
PN (hs0 a1 s1 i)
1[P = P 0 ]). (Left) first 2000 simulations
tree node hs0 a1 s1 (i.e., N (hs01a1 s1 i) i=0
(Right) Zoomed view 100,000 simulations, displaying empirical convergence target
values.

857

fiGuez, Silver, & Dayan

particular transition probabilities required simulation traverses
tree, also rollout.
Consider P(s, a, ) parametrized latent variable s,a state action
pair. may depend other, well
R additional set latent variables .
posterior P written P (|h) = P (|, h)P (|h), = {s,a |s
S, A}. Define = {s1 ,a1 , , st ,at } (random) set parameters required
course bamcp simulation starts time 1 ends time t. Using
chain rule, rewrite
P (|, h) =P (s1 ,a1 |, h)
P (s2 ,a2 |1 , , h)
..
.
P (sT ,aT |T 1 , , h)
P ( \ |T , , h)
length simulation \ denotes (random) set parameters
required simulation. simulation i, sample P (|ht )
root lazily sample st ,at parameters required, conditioned
t1 parameters sampled current simulation. process stopped end
simulation, typically long parameters sampled. example,
transition parameters different states actions independent, simply
draw necessary parameters individually state-action pair encountered
simulation. general, transition parameters independent different states,
dependencies likely structured. example, mdp dynamics could arise
mixture model denotes mixture component P (|h) specifies posterior
mixture proportion. Then, transition parameters conditionally independent
given mixture component, sampling root simulation allows us sample
required parameters s,a independently P (s,a |i , h) required
i-th simulation. leads substantial performance improvement, especially
large mdps single simulation requires small subset parameters (see
example domain Section 4.2 concrete illustration). lazy sampling scheme
limited shallow latent variable models; deeper models, also benefit
conditional independencies save sampling operations simulation sampling
necessary latent variables opposed sampling .
3.3 Rollout Policy Learning
choice rollout policy ro important simulations few, especially domain
display substantial locality rewards require carefully selected sequence
actions obtained. Otherwise, simple uniform random policy chosen provide
noisy estimates. work, learn Qro , optimal Q-value real mdp, modelfree manner, using Q-learning, samples (st , , rt , st+1 ) obtained off-policy result
interaction bamcp agent mdp time t. real transition
858

fiBayes-Adaptive Monte-Carlo Planning

(st , , rt , st+1 ) observed, update
Qro (st , ) Qro (st , ) + (rt + max Qro (st+1 , a) Qro (st , )),


(20)

learning rate parameter; standard Q-learning rule (Watkins,
1989). Acting greedily according Qro translates pure exploitation gathered knowledge. rollout policy bamcp following Qro could therefore over-exploit. Instead, similar
work Gelly Silver (2007), select -greedy policy respect Qro
rollout policy ro . words, steps mdp, updated Qro
times use following stochastic rollout policy mcts simulations + 1
decision step:
(

1 + |A|
= argmaxa0 Qro (s, a0 )
ro (s, a) =
(21)

otherwise,
|A|
ro (s, a) probability selecting action mdp state (i.e., history
ignored) rollout. biases rollouts towards observed regions high rewards.
method provides valuable direction rollout policy negligible computational
cost. complex rollout policies considered, example rollout policies
depend sampled model P history ht . However, usually incur computational overhead, may less desired running simulations worse
estimates.
3.4 Theoretical Properties
section, show bamcp converges Bayes-optimal policy. first present
theoretical results case exact posterior inference conducted obtain
posterior samples dynamics (Section 3.4.1), extend convergence guarantee
case approximate inference (MCMC-based) necessary produce posterior
samples (Section 3.4.2).
proof Theorem 1 present supplementary material Guez et al. (2012).
Theorem 2 novel contribution paper.
3.4.1 Exact Inference Case
main step proving root sampling alter behavior ba-uct.
proof adaptation pomcp proof Silver Veness (2010). provide
intuition empirical evidence convergence simple Bandit problems
Bayes-optimal solution known.
Consider ba-uct algorithm: uct applied Bayes-Adaptive mdp (its dynamics
described Equation 16). Let (hT ) rollout distribution ba-uct:
probability history hT generated running ba-uct search hst , ht i,
ht prefix hT , effective horizon search tree, arbitrary EE

policy. Similarly define quantities (hT ): probability history hT generated
running bamcp algorithm, Ph (P): distribution P node h
859

fiGuez, Silver, & Dayan

running bamcp. following lemma shows rollout statistics
bamcp ba-uct.4
Lemma 1 (hT ) = (hT ) EE policies : H A.
Proof Let arbitrary. show induction suffix histories h ht , (a)
(h) = (h); (b) P (P |h) = Ph (P), P (P |h) denotes (as before) posterior
distribution dynamics given h.
Base case: root (h = ht , suffix history size 0), clear Pht (P) = P (P |ht )
since sampling posterior root node (ht ) = (ht ) = 1 since
simulations go root node.
Step case:
Assume proposition true suffices size j. Consider suffix has0 size j + 1,
s0 arbitrary h arbitrary suffix size j ending s.
following relation holds:
Z

0

(has ) = (h)(h, a)
dP P (P |h) P(s, a, s0 )
(22)
ZP

= (h)(h, a)
dP Ph (P) P(s, a, s0 )
(23)
P

= (has0 ),

(24)

second line obtained using induction hypothesis, rest
definitions. addition, match distribution samples P node has0 :
P (P |has0 ) = P (has0 | P)P (P)/P (has0 )

(25)

0

0

(26)

0

0

(27)

= P (h| P)P (P) P(s, a, )/P (has )
= P (P |h)P (h) P(s, a, )/P (has )
0

= ZP (P |h) P(s, a, )
0

(28)

= Z Ph (P) P(s, a, )

(29)

0

= Z Pha (P) P(s, a, )

(30)

= Phas0 (P),

(31)

Equation 29 obtained induction hypothesis, Equation 30 obtained
fact choice action node made independently samples P.
Finally, obtain Equation 31 Equation 30, consider probability sample
P arrives node has0 , first needs traverse node ha (this occurs probability
Pha (P)) then, node ha, state s0 needs sampled (this occurs probability P(s, a, s0 )); therefore, Phas0 (P) Pha (P) P(s, a, s0 ). Z normalization constant:
R
R
Z = 1/( P P P(s, a, s0 )P (P |h)) = 1/( P P P(s, a, s0 )Ph (P)). completes induction.


4. ease notation, refer node history only, opposed state history
rest paper.

860

fiBayes-Adaptive Monte-Carlo Planning

proof Lemma 1 make explicit use lazy sampling, since method
realizing values relevant random variables affect rollout distribution
affect computed, how.
Define V (hs, hi) = max Q(hs, hi, a) hs, hi H. show bamcp conaA

verges Bayes-optimal solution.
Theorem 1 > 0 (the numerical precision, see Algorithm 1) suitably chosen
max
c (e.g. c > R1
), state hst , ht i, bamcp constructs value function root node
p

converges probability 0 -optimal value function, V (hst , ht i) V0 (hst , ht i),

. Moreover, large enough N (hst , ht i), bias V (hst , ht i) decreases
0 = 1
O(log(N (hst , ht i))/N (hst , ht i)).
Proof uct analysis Kocsis Szepesvari (2006) applies ba-uct algorithm,
since vanilla uct applied bamdp (a particular mdp). also applies arbitrary rollout policies, including one developed Section 3.3. Lemma 1, bamcp
simulations equivalent distribution ba-uct simulations. nodes bamcp
therefore evaluated exactly ba-uct, providing result.

Lemma 1 provides intuition belief updates unnecessary search
tree: search tree filters samples root node distribution
samples node equivalent distribution obtained explicitly updating
belief. particular, root sampling pomcp (Silver & Veness, 2010) bamcp
different evaluating tree using posterior mean. illustrated empirically
Figures 4 5 case simple Bandit problems.
3.4.2 Approximate Inference Case
Theorem 1, made implicit assumption bamcp provided true samples
drawn iid posterior. However, sophisticated priors require form
approximate sampling scheme (see, example, task Section 4.2), Markov
Chain Monte Carlo (MCMC), generally deliver correlated posterior samples
chain converges stationary distribution (Neal, 1993). Thus, necessary extend
proof convergence bamcp deal samples nature.
Theorem 2 using approximate sampling procedure based MCMC chain
stationary distribution P (P |ht ) (e.g., Metropolis-Hastings Gibbs sampling) produce
sample sequence P 1 , P 2 , . . . root node bamcp, value V (hst , ht i) found
bamcp converges probability (near-)optimal value function.
Proof Let > 0 chosen numerical accuracy algorithm. choose
finite depth search tree function , rmax , guarantees sum
total return depth amountsPto less . consider leaf Q-node
tree, mean value = n1 nm=1 rm n simulations, rm reward
obtained node m-th simulation going node. Since ucb
used throughout tree, exploration never ceases guarantees n (see
example Kocsis & Szepesvari, 2006, Thm. 3).
861

fiGuez, Silver, & Dayan

80

Discounted sum rewards

Undiscounted sum rewards

260

240

220

200

180

75

70

65

60
160
Bayesoptimal

BAMCP

Posterior Mean

(a)

55

Bayesoptimal

BAMCP

Posterior Mean

(b)

Figure 4: Performance comparison bamcp (50000 simulations, 100 runs) posterior mean
decision 8-armed Bernoulli bandit = 0.99 300 steps. arms success
probabilities 0.6 except one arm success probability 0.9. Bayes-optimal
result obtained 1000 runs Gittins indices (Gittins et al., 1989). a. Mean sum
rewards 300 steps. b. Mean sum discounted rewards 300 steps.

Root sampling filtering (Lemma 1) still holds despite approximate sampling
root node; since statement distribution samples, order
samples arrive. Therefore, distribution dynamics node converges
right stationary distribution P (P |hi ), hi history corresponding node i.
Asymptotic results Markov Chains (Law large numbers Markov Chains) guarantee
us a.s., true expected reward leaf node i.
Given convergence leaves, work way tree backward induction
show values node converge (near-)optimal values. particular
value root converges optimal value.


3.5 Possible Misuse Latent Variable Information: Counter-Example
planning bamdp using sample-based forward-search algorithm bamcp,
could tempting use knowledge available sampler producing samples
(such value latent variables model) take better planning decisions.
example, generating sample P iR dynamics according posterior distribution P (P |h) written P (P |)P (|h), P might generated
sampling P (|h) sampling P P (P |i ). Since value available
contain high-level information, one natural question ask whether search
informed value .
862

fiBayes-Adaptive Monte-Carlo Planning

BAMCP Number simulations: 5000

BAMCP Number simulations: 250000

20

20

15





15

10

5

5

5

10



15

20

5

BAMCP Number simulations: 2500000

20

15





15



20

15

10

5

10

5

5

10



15

20

5

10

15



20

Posterior mean decision

Probability correct decision

15



10

BAMCP Number simulations: 5000000

20

20

10

10

0

0.2

0.4

0.6

0.8

1

5

5

10



15

20

Figure 5: Evaluation bamcp Bayes-optimal policy, case = 0.95, choosing
deterministic arm reward 0.5 stochastic arm reward 1 posterior
probability p Beta(, ). result tabulated range values , , cell value
corresponds probability making correct decision (computed 50 runs)
compared Gittins indices (Gittins et al., 1989) corresponding posterior. first
four tables corresponds different number simulations bamcp last table shows
performance acting according posterior mean. range , values,
Gittins indices stochastic arm larger 0.5 (i.e., selecting stochastic arm
optimal) + 1 also = + 2 6. Acting according posterior mean
different Bayes-optimal decision >= Gittins index larger 0.5.
bamcp guaranteed converge Bayes-optimal decision cases, convergence
slow edge cases Gittins index close 0.5 (e.g., = 17, = 19,
Gittins index 0.5044 implies value 0.5044/(1) = 10.088 stochastic
arm versus value 10 0.5 + 10.088 = 10.0836 deterministic arm).

863

fiGuez, Silver, & Dayan

Here, outline one incorrect way using latent variable value search.
Suppose would want split search tree value (this would occur implicitly
constructing history features based value ), provide simple
counter-example shows valid search approach.
Consider simple prior distribution two 5-state mdps, illustrated Figure 6,
P ( = 0|h0 ) = P ( = 1|h0 ) = 21 , P (P |) delta function illustrated mdp.
s0
a0

s0
a1

a0

s1 p = 1
a0

s2 p = 1
+1

s1 p = 1

a1

s3 p = 1
+2

a0
s4 p = 1
2
(a)

a1
s2 p = 1
+1

a1

s4 p = 1
2

=0

s3 p = 1
+2
(b)

=1

Figure 6: two possible mdps corresponding two settings .

h0 = s0
a0

a1

s1

s2
+1

a0
1
2
s3

1
2

+2

a1
1
2
s4

1
2

2

Figure 7: bamdp, nodes correspond belief(or history)-states.

2 deterministic actions (a0 , a1 ) mdp, episode length 1 2 steps.
difference two mdps outcome taking action a0 a1 state
s1 , illustrated Figure 6, a0 rewarding = 0 costly = 1,
vice-versa a1 . rewards obtained executing action
terminal states (s2 , s3 , s4 ).
Observing first transition informative, implies posterior distribution unchanged first transition: P (P |h0 ) = P (P |h0 a0 s1 ) = P (P |h0 a1 s2 ).
bamdp corresponding problem illustrated Figure 7.
864

fiBayes-Adaptive Monte-Carlo Planning

history-state h0 = s0 , Bayes-optimal Q values easily computed:
Q (h0 , a1 ) = ,

(32)



Q (h0 a0 s1 , a0 ) = 0 + (2 P (s3 |h0 a0 s1 a0 ) 2 P (s4 |h0 a0 s1 a0 ))
= (1 1) = 0,

(33)
(34)



Q (h0 a0 s1 , a1 ) = 0 + (2 P (s3 |h0 a0 s1 a0 ) 2 P (s4 |h0 a0 s1 a0 ))

(35)

= (1 1) = 0,

(36)



(37)



Q (h0 , a0 ) = 0 + max Q (h0 a0 s1 , a) = 0,


implies a1 = (h0 ) . [We used fact P (s3 |h0 a0 s1 a0 ) = P ( =
0|h0 a0 s1 a0 ) P (s3 | = 0, s1 a0 ) + P ( = 1|h0 a0 s1 a0 ) P (s3 | = 1, s1 a0 ) = 21 1 + 12 0 = 12 ,
similarly P (s4 |h0 a0 s1 a0 )].
Note that, since belief updates occur terminal states, forward-search
without root sampling equivalent. would construct search tree
Figure 7 compute right value right decision.
problem comes decide split search tree chance nodes based
value generated samples going tree. example, taking
action a0 state s0 , would using either mdp = 0 w.p 0.5 mdp
= 1 w.p 0.5. Since multiple values go node h0 a0 , would
branch tree illustrated Figure 8. search tree problematic value
computed Q (h0 , a0 ) becomes 2 2 , larger Q (h0 , a1 ) = > 0.5.
Therefore, policy computed root longer Bayes-optimal.

h0 = s0
a0
= 0, s1

a1
s2

= 1, s1

+1
a0
s3

a1

a0
s4

+2

a1

s3
2

s4
+2

2

Figure 8: problematic search tree.

branching latent variable value, creating spurious observations:
implying latent variable past observed future,
case.
summarize, Bayes-adaptive policy optimized must function future
histories (i.e., things well actually observe future), cannot function future
unobserved latent variables. Ignoring causes problems simple domains
one illustrated above, similar scenarios would occur complex latent variable
models reasons.
865

fiGuez, Silver, & Dayan

4. Experiments
first present empirical results bamcp set standard problems comparisons
popular algorithms. showcase bamcps advantages large scale task:
infinite 2D grid complex correlations reward locations.
4.1 Standard Domains
following algorithms run standard domains: bamcp, sboss, beb, bfs3.
Details implementation parametrization found Appendix B.
addition, report results work Strens (2000) several algorithms.
following domains, fix = 0.95.
Double-loop domain 9-state deterministic mdp 2 actions (Dearden,
Friedman, & Russell, 1998), 1000 steps executed domain. illustrated
Figure 9(a).
Grid5 5 5 grid reset state one corner, single reward state
diametrically opposite reset state. Actions cardinal directions executed
small probability failure (pfailure = 0.2) 1000 steps.

expectacan approxias derived fairly

Grid10 10 10 grid designed way Grid5. collect 2000 steps
domain.
b,2
a,10
a,0

a,0

a,0

a,0

1 b,2 Maze
2
3
4
Deardens
264-states
maze
5
3 flags collect (Dearden et al., 1998).
b,2
special state provides
reward
equivalent

number flags collected since
b,2
b,2
last visit. 20000 steps executed domain5 . illustrated Figure 9(b).

ed two possible
first, modate, might
ture update,
ation.

(a) Task 1 [11].
b,2

a,10
1

6
b,0

b,0
a,0

2

b,2
b,2
b,2a,0
b,2

5

a,0

a,0

3

b,0

a,0

1

4

a,b,0
5

2

a,0

a,0

a,0

0

a,b,0

Figure 1. Chain problem

algorithms conces show
es,
case,
-VPI strategies

nvergence proof
tried infinitely
0

earning rate.
shows
-values.
use moment
ect mean.
finitely often
updating,
every state

prove

finitely often

a,b,2
7
6.1 Problem
Descriptions

Figure

a,b,1

3

4 arcs
Figure 1b,0
shows 8
5-state Chain problem.
a,b,0are
labeled actions cause state transition,
associated rewards.
However
agent
Figure 3. Maze problem.
(b) Task
(a) 2 [14].
(b)
abstract actions {1,2} available. Usually abstract action 1
causes real-world action take place, abstract
direction perpendicular intended (with
2 causes real-world action b. probability 0.2,
standard
F domains
Geffect.
9: action
Two

described
Section
4.1: a)
Double-loop
domain,
b) Deardens
probability
0.1).
33 reachable
locations


agent
slips
action

opposite
maze (including goal) 8
maze.
Figures

work
Strens

optimal
behavior
tothe
always
choose
action 1(2000).
(even
combinations status flags time.
though sometimes results transitions labeled
yields 264 discrete states. agent given limited
b). state 5 reached, reward 10 usually
layout information (identifying immediate successors
received several times agent slips, starts

order tothe
reduce
complexity


state performance
1. problemofrequires
quantify
effective
algorithm,
westate)
measured
total
undiscounted
posterior distribution Bayesian DP approach.
exploration accurate estimation discounted reward.


reward many steps. chose measure performance enable fair comparisons
Figure 2 shows Loop problem involves two
6.2 Results
drawn

prior
work.
fact,
Two
areFactions
optimising
different criterion discounted
loops
length
5 joined
single
start state.
available transitions deterministic. Taking

experimental results show accumulated totals

rewardDP
received
overStrens
learning(2000)
phases

5. result
reported
maze

theloop,
Bayesian
alg.
consist
different
action
repeatedly
causes traversal

right
F Deardens
steps Chain Loop, 20000 steps
yielding
reward
1the
maze
every layout
5 actions
taken. 1000
version
task

given
agent.

Maze. Averages taken 256 runs Chain
Conversely,
taking actionproblem.
b repeatedly causesistraversal

(c) Task 3.
navigation
start
state.

Loop,
16 runs Maze. Table 1 summarizes
left loop, yielding reward 2 every 5 actions
comparative
performance 1, 2, 8 phases
agent receives

reward
upon
reaching
based


number
taken. problem requires difficult compromise
866
learning. (Note results pessimistic
exploration exploitation.
flags collected.
show rewards actually received learning
Figure 3 shows Maze problem. agent move

rather rewards could received

instantaneous greedy policy.) Bayesian DP method,

one square


Figureleft,
3: right,
Theupthree
domains
used
maze.
experiments.
new hypothesis (for MDP) drawn time
attempts move wall, action effect.
problem move start (top-left) goal
(top-right) collecting flags way.

system entered starting state. Maze, new
hypothesis also obtained every 24 steps

fiBayes-Adaptive Monte-Carlo Planning

reward start state might expect evaluation unfavourable
algorithm.
Although one major advantage Bayesian RL one specify priors
dynamics, domains, used rather generic priors enable comparisons previous work. Double-loop domain, Bayesian RL algorithms run simple
1
Dirichlet-Multinomial model symmetric Dirichlet parameter = |S|
. grids
maze domain, algorithms run sparse Dirichlet-Multinomial model,
described Friedman Singer (1999). models, efficient collapsed
sampling schemes available; employed ba-uct bfs3 algorithms
experiments compress posterior parameter sampling transition sampling
single transition sampling step. considerably reduces cost belief updates
inside search tree using simple probabilistic models. Unfortunately, efficient collapsed sampling schemes available general (see example model
Section 4.2).
Sum Rewards 1000 steps

90

90

BAMCP

80

80

70

70

60

60

50

50

40

40

30

30

20

20

10

3

10

2

1

10

90

10

0

10

10

80

70

70

60

60

50

50

40

40

30

30

20

20
3

10

2

10

3

10

2

10

90

SBOSS

80

10

BFS3

1

10

0

10

10

1

10

1

10

10

0

BEB

3

10

2

10

Average Time per Step (s)

10

0

Figure 10: Performance algorithm Grid5 domain function planning time.
point corresponds single run algorithm associated setting parameters.
Increasing brightness inside points codes increasing value parameter (bamcp
bfs3: number simulations, beb: bonus parameter , sboss: number samples K).
second dimension variation coded size points (bfs3: branching factor C,
sboss: resampling parameter ). range parameters specified Appendix B.

summary results presented Table 1. Figures 10 11 report planning
time/performance trade-off different algorithms Grid5 Maze domain.
867

fiUndiscounted sum rewards 20000 steps

Guez, Silver, & Dayan

1100
1000
900

BAMCP (BAUCT+RS+LS+RL)
BEB
BFS3
SBOSS

800
700
600
500
400
300
200
100
0

1

0

10

Average Time per Step (s)

10

Figure 11: Performance algorithm, Figure 10 Deardens Maze domain.

BAMCP
BFS3 (Asmuth & Littman, 2011)
SBOSS (Castro & Precup, 2010)
BEB (Kolter & Ng, 2009)
Bayesian DP* (Strens, 2000)
Bayes VPI+MIX* (Dearden et al., 1998)
IEQL+* (Meuleau & Bourgine, 1999)
QL Boltzmann*

Double-loop
387.6 1.5
382.2 1.5
371.5 3
386 0
377 1
326 31
264 1
186 1

Grid5
72.9 3
66 5
59.3 4
67.5 3
-

Grid10
32.7 3
10.4 2
21.8 2
10 1
-

Deardens Maze
965.2 73
240.9 46
671.3 126
184.6 35
817.6 29
269.4 1
195.2 20

Table 1: Experiment results summary. algorithm, report mean sum rewards
confidence interval best performing parameter within reasonable planning time limit
(0.25 s/step Double-loop, 1 s/step Grid5 Grid10, 1.5 s/step Maze).
bamcp, simply corresponds number simulations achieve planning time
imposed limit. * Results Strens (2000) reported without timing information.

domains tested, bamcp performed best. algorithms came close
tasks, parameters tuned specific domain.
particularly evident beb, required different value exploration bonus achieve
maximum performance domain. bamcps performance stable respect
choice exploration constant (c = 3) require fine tuning obtain
results.
bamcps performance scaled well function planning time, evident Figures 10 11. contrast, sboss follows opposite trend. samples employed
build merged model, sboss actually becomes optimistic over-explores, de868

fi(a)

Undiscounted sum rewards 20000 steps

Bayes-Adaptive Monte-Carlo Planning

1100
1000

BAUCT + RL
BAUCT

900
800
700
600
500
400
300
200
100
0

1

10

Average Time per Step (s)

0

10

1100

BAUCT + RS + RL
1000

BAUCT + RS

900
800
700
600

(b)

500
400
300
200
100
0
1

10

1100
1000

0

10

BAUCT + RS + LS + RL (BAMCP)
BAUCT + RS + LS

900
800
700
600

(c)

500
400
300
200
100
0

1

10

0

10

Figure 12: Evolution performance ba-uct bamcp Deardens Maze domain. bamcp present
plots comparison, also displayed Figure 11. a. Performance vanilla ba-uct
without rollout policy learning (RL) presented Section 3.3. b. Performance
ba-uct Root Sampling (RS), presented Section 3.1, without rollout
learning. c. Performance ba-uct Root Sampling Lazy Sampling (LS), presented
Section 3.2. addition rollout policy learning, bamcp algorithm.

869

fiGuez, Silver, & Dayan

grading performance. beb cannot take advantage prolonged planning time all.
performance bfs3 generally improves planning time, given appropriate
choice parameters, obvious trade-off branching factor, depth,
number simulations domain. bamcp greatly benefited lazy sampling
scheme experiments, providing 35 speed improvement naive approach
maze domain example; illustrated Figure 12.
Deardens maze aptly illustrates major drawback forward search sparse sampling
algorithms bfs3. Like many maze problems, rewards zero least k steps,
k solution length. Without prior knowledge optimal solution length,
upper bounds higher true optimal value tree fully
expanded depth k even simulation happens solve maze. contrast,
bamcp discovers successful simulation, Monte-Carlo evaluation immediately bias
search tree towards successful trajectory.
Figure 12 confirms that, even moderate-sized domain simple prior (Independent Sparse Dirichlet-Multinomial), bamcp amply benefits root sampling, lazy
sampling, rollout learning. complex priors, following section, ba-uct
becomes computationally intractable. Root sampling lazy sampling mandatory
components.
4.2 Infinite 2D Grid Task
..
.





..
.
Figure 13: portion infinite 2D grid task generated Beta distribution parameters 1 = 1, 1 = 2
(columns) 2 = 2, 2 = 1 (rows). Black squares location (i,j) indicates reward 1,
circles represent corresponding parameters pi (blue) qj (orange) row
column (area circle proportional parameter value). One way interpret
parameters following column implies collection 2pi /3 reward average (2/3
mean Beta(2, 1) distribution) whereas following row j implies collection qj /3
reward average; high values parameters pi less likely high values parameters
qj . parameters employed results presented Figure 14-c).

870

fiBayes-Adaptive Monte-Carlo Planning

perhaps unfair characterize domains previous section
limited scale. Indeed, seen correct reflection state
art Bayesian RL. However, bamcp, root-based lazy sampling,
applied considerably larger challenging domains. therefore designed new
problem well beyond capabilities prior algorithms since infinite
combinatorially structured state space, even challenging belief space. Although
still abstract, new task illustrates something bamcps power.
4.2.1 Problem Description
new problem class complex mdps infinite grid. draw particular
mdp, column associated latent parameter pi Beta(1 , 1 ) row j
associated latent parameter qj Beta(2 , 2 ). probability grid cell ij
reward 1 pi qj , otherwise reward 0. agent knows grid
always free move four cardinal directions. Rewards consumed
visited; returning location subsequently results reward 0. opposed
independent Dirichlet priors employed standard domains, here, dynamics tightly
correlated across states (i.e., observing state transition provides information
state transitions).
domain illustrated Figure 13. Although uncertainty appears concern
reward function mdp rather dynamics, viewed formally uncertainty dynamics state augmented binary variable indicates
whether reward present.6
Formally, since rewards disappear one visit, description state
mdp needs include information state rewards (for example
form set grid locations previously visited) addition position agent
infinite grid. state therefore combination current agents location
(i, j), unordered set previously visited locations V , binary variable R = rij .
dynamics P deterministically updates position agent visited
locations based agents action, updates R according reward map.
known reward function simply R(s, a) = s(R) (i.e., described before,
agent gets reward position ij rij = 1).
4.2.2 Inference
Posterior inference (of dynamics P) model requires approximation
non-conjugate coupling variables. see this, consider posterior probability
particular grid cell kl reward 1 (denote event rkl = 1),
Z
pk ql P (pk , ql |O) dpk dql ,

P (rkl = 1|O) =

(38)

pk ,ql

= {(i, j)} set observed reward locations, associated observed
reward rij {0, 1}. Sampling rkl straightforward given access posterior samples
6. fact, bamdp framework straightforwardly extended deal general, partiallyobserved, reward functions (Duff, 2002).

871

fiGuez, Silver, & Dayan

pk ql . However, posterior distribution pk ql , P (pk , ql |O), cannot easily
sampled from, given by:
P (pk , ql |O) P (O|pk , ql )P (pk )P (ql )
Z


P (O|PO , QO )
P (p)
P (q)
=
PO \pk ,QO \ql

Z
=

pPO



(40)

qQO

(pi qj )rij (1 pi qj )1rij

PO \pk ,QO \ql (i,j)O

(39)



Beta(p; 1 , 1 )

pPO



Beta(q; 2 , 2 ),

qQO

(41)
PO denotes set parameters pi observed columns (columns least
one observation exists) similarly QO rows. posterior suffers nonconjugacy (because multiplicative interaction two Beta distribution)
also complicated dependence structure (pk ql depend observations outside
column k row l). reasons, inference done approximately via MetropolisHastings (details Appendix C).
4.2.3 Results
Planning algorithms attempt solve mdp based sample(s) (or mean)
posterior (e.g., boss, beb, Bayesian DP) cannot directly handle large combinatorial
state space. Previous forward-search methods (e.g., ba-uct, bfs3) deal state
space, complex belief space: every node search tree must solve
approximate inference problem estimate posterior beliefs. contrast, bamcp
limits posterior inference root search tree directly affected
size state space belief space, allows algorithm perform well even
limited planning time. Note lazy sampling required setup since full
sample dynamics involves infinitely many parameters.
Figure 14 demonstrates planning performance bamcp complex domain.
Performance improves additional planning time. quality prior clearly affects
agents performance, bamcp take advantage correct prior information gain
rewards. addition, behavior agent qualitatively different depending
prior parameters employed.
example, case Figure 14-a, rewards often found relatively dense
blocks map agent exploits fact exploring; explains high
frequency short dwell times. Figure 14-b, good rewards rates obtained
following rare rows high qj parameters, finding good rows expensive
least two reasons: 1) good rows far agents current position 2)
takes longer decide value row observations lack rewards;
entropy posterior larger given observations rewards (which explained
either rows columns poor, time) given observations
rewards (which explained high probability rows columns
good, since rij Bernoulli(pi qj )). Hence, agent might settle sub-optimal rows
large periods time, example gains enough confidence better row likely
found nearby (as Bandit problems Bayes-optimal agent might settle
872

fi60

50

30

1

10

0

10

Planning time (s)

45

40

35

30

25

20

15

1

10

0

10

Planning time (s)

8

6
5
4
2
10

70

60

50

40

30

20

1

10

0

10

Planning time (s)

1

10

1

10

0

10

Planning time (s)

1

10

7

0.05

0

50

100

150

200

50

100

150

200

50

100

150

200

Dwell Time (Horizontal)

0.05

6.5
6
5.5
5
4.5
4

0

3.5
3
2.5
2
2
10

1

80

0

7

10

90

10
2
10

9

1

50

10
2
10

10

10

Discounted sum rewards 200 steps

Undiscounted sum rewards 200 steps

20
2
10

Undiscounted sum rewards 200 steps

BAMCP
BAMCP Wrong prior
Random

11

Frequency

70

BAMCP
BAMCP Wrong prior

12

Frequency

80

0.05

13

1

10

0

10

Planning time (s)

1

10

13

0.05

0

Dwell Time (Horizontal)

0.05

12
11
10

Frequency

90

40

14

Discounted sum rewards 200 steps

100

Discounted sum rewards 200 steps

Undiscounted sum rewards 200 steps

Bayes-Adaptive Monte-Carlo Planning

9
8
7

0

6
5
4
3
2
10

1

10

0

10

Planning time (s)

1

10

0.05

0

Dwell Time (Horizontal)

Figure 14: Performance bamcp function planning time Infinite 2D grid task, = 0.97,
row corresponds different set parameters generating grid. performance first 200 steps environment averaged 50 sampled environments
(5 runs sample) reported terms undiscounted (left) discounted
(center) sum rewards. bamcp run either correct generative model prior (solid
green) incorrect prior (dotted green). performance uniform random policy
also reported (blue). small sample portion grid generated parameters
displayed row, presented Figure 13. frequency histogram dwell times
number consecutive steps agent stays row switching reported
scenario. grids generated Beta parameters a) 1 =0.5, 1 =0.5, 2 =0.5, 2 =0.5,
b) 1 =0.5, 1 =0.5, 2 =1, 2 =3, c) 1 =2, 1 =1, 2 =1, 2 =2. case wrong priors (dot-dashed lines), bamcp given parameters a) 1 =4, 1 =1, 2 =0.5, 2 =0.5, b)
1 =1, 1 =3, 2 =0.5, 2 =0.5, c) 1 =1, 1 =2, 2 =2, 2 =1.

873

fiGuez, Silver, & Dayan

sub-optimal arm believes likely best arm given past data). heavier-tail
distribution dwell times scenario, Figure 14-b, reflects behavior.
case Figure 14-c consists mixture rich poor rows. agent
determine moderately quickly row good enough, given expects find,
switches nearby row. good enough row found, agent stick
large periods time. reflected bimodal nature distribution
dwell times Figure 14-c. many cases, agent satisfied one first rows
visits, since likely agent starts good row. decides stay
entire duration episode, explains peak towards 200.
bamcps prior belief dynamics generative models
distribution (Wrong prior dot-dashed lines Figure 14), maladaptive behavior
observed. instance, Figure 14-a, deluded agent expects columns rich,
rows rich others poor. Hence, good strategy given prior
belief find one good rows exploit travelling horizontally. However,
since lot columns actually poor generative model, agent never encounters
continuous sequence rewards expects find good rows. Given wrong prior,
even actually good row, explains observation row poor
rather column switches different row. behavior reflected
shorter horizontal dwell times plotted Figure 14-a. Similar effects observed
Wrong prior cases Figure 14-b,c.
pointed actual Bayes-optimal strategy domain
known behavior bamcp finite planning time might qualitatively match
Bayes-optimal strategy. Nevertheless, speculate behavior observe
bamcp, including apparently maladaptive behaviors, would also found
Bayes-optimal solution.

5. Discussion
Bayesian model-based reinforcement learning addresses problem optimizing discounted return agent dynamics uncertain. solving augmented
mdp called bamdp, agent optimally trade-off exploration exploitation
maximize expected discounted return according prior beliefs. formally attractive, framework suffers major drawback: computationally intractable
solve bamdp exactly. aware formal complexity analysis
solving bamdps, bamdps mapped continuous-state pomdps discrete observations (Duff, 2002). general, solving discrete pomdps known challenging
(Mundhenk, Goldsmith, Lusena, & Allender, 2000; Madani, Hanks, & Condon, 2003).
approximate Bayes-optimal solution efficiently, suggested sample-based
algorithm Bayesian RL called bamcp significantly surpassed performance
existing algorithms several standard tasks. showed bamcp tackle larger
complex tasks generated structured prior, existing approaches scale
poorly. addition, bamcp provably converges Bayes-optimal solution, even
MCMC-based posterior sampling employed.
main idea employ Monte-Carlo tree search explore augmented Bayesadaptive search space efficiently. naive implementation idea algorithm
874

fiBayes-Adaptive Monte-Carlo Planning

called ba-uct. However, ba-uct cannot cope priors employs
expensive belief updates inside search tree. therefore introduced three modifications
obtain computationally tractable sampled-based algorithm: root sampling,
requires beliefs sampled start simulation (as Silver & Veness, 2010);
model-free RL algorithm learns rollout policy; lazy sampling scheme
enables posterior beliefs sampled cheaply.
5.1 Future Work: Algorithms
Despite excellent empirical performance many domains (Gelly et al., 2012), uct
algorithm known suffer several drawbacks. First, finite-time regret bound.
possible construct malicious environments, example optimal policy
hidden generally low reward region tree, uct misled long
periods (Coquelin & Munos, 2007). course, setting, appropriate prior distributions
might help structure search effectively. But, issue convergence MCMC
chain approximate inference settings may hinder effort get finite-time guarantees.
Second, uct algorithm treats every action node multi-armed bandit problem.
However, actual benefit accruing reward planning,
theory appropriate use pure exploration bandits (Bubeck, Munos, & Stoltz, 2009).
focused learning dynamics (and implicitly rewards infinite grid
task) fully observable mdp. states observed directly, bamcp could
extended maintain beliefs dynamics state. state dynamics
would sampled posterior distribution, start simulation.
setting known Bayes-Adaptive Partially Observable mdp (bapomdp) (Ross, Pineau,
Chaib-draa, & Kreitmann, 2011).
work, limited case discrete-state mdps, since already
present significant challenges Bayesian exploration. bamcp cannot straightforwardly
converted deal continuous-state mdps, remains see whether ingredients make bamcp successful discrete setting could reassembled
continuous-state solution, example using form value function approximation
simulation-based search (Silver, Sutton, & Muller, 2008).
5.2 Future Work: Priors
bamcp able exploit prior knowledge dynamics principled manner.
possible encode many aspects domain knowledge prior distribution,
important avenue future work explore rich, structured priors dynamics
mdp. showed, prior knowledge matches class environments
agent encounter, exploration significantly accelerated. therefore
important understand select learn appropriate priors large real-world
tasks tackled Bayesian RL. One promising category rich priors context
non-parametric priors. example, Doshi-Velez, Wingate, Roy, Tenenbaum (2010)
Wingate, Goodman, Roy, Kaelbling, Tenenbaum (2011) investigated
direction, yet combination myopic planning algorithms, rather
Bayes-Adaptive planning.
875

fiGuez, Silver, & Dayan

5.3 Evaluation Bayesian RL Algorithms
Bayesian RL algorithms traditionally tested small, hand-crafted, domains.
Even though domains contain substantial structure, priors given agent
usually independent Dirichlet distributions generate unstructured random
worlds. mismatch prior distribution domains problematic
evaluate, since perfect Bayesian RL algorithm guaranteed perform well given
incorrect priors. Since tractable compute Bayes-optimal solution exactly,
becomes impossible decide whether algorithm obtains low return compared
algorithms hand-crafted domain better worse approximation
Bayes-optimal solution.
purpose algorithmic evaluation, obvious solution design priors
actually generate tasks solve. Bayesian RL algorithm given
generative model prior also tested many generated tasks,
Bayes-optimal solution guaranteed obtain best discounted return average.
case, higher mean return becomes synonymous better approximation given
goal matching Bayes-optimal solutions performance. employed method
averaging across generated domains Section 4.2 evaluate bamcp algorithm
Infinite Grid task. understand exploration performance proposed algorithmic solutions truly, future comparisons algorithms would likely benefit
evaluation schemes.
5.4 Conclusion
Bayes-adaptive planning conceptually appealing computationally challenging.
enormous computation required prior approaches largely due fact
root values computed expectations and/or maximisations complete tree
possible actions states follows current history. addition,
values must integrate distribution transition potentially reward models
state search tree. result, computation typically grows exponentially
search depth, rate determined action space, successor state space, model
space.
new algorithm, bamcp, builds previous work (Kearns et al., 1999; Kocsis &
Szepesvari, 2006; Silver & Veness, 2010) solves problems systematically sampling expectations, notably pomcp algorithm Silver Veness (2010).
tiny fraction future tree actually explored, chosen sampled actions according likely worth; sampling transitions dynamics. Additionally,
bamcp also solves requirement integrating models: sampling models
belief distribution; root node, avoid need compute
posteriors throughout tree; lazily avoiding realizing random choices
last possible moment.
result efficient algorithm outperforms previous Bayesian model-based
reinforcement learning algorithms significant margin several well-known benchmark
problems, scale problems infinite state space complex prior
structure.
876

fiBayes-Adaptive Monte-Carlo Planning

Acknowledgments
acknowledge support project Gatsby Charitable Foundation (AG, PD),
Natural Sciences Engineering Research Council Canada (AG), Royal Society
(DS), European Communitys Seventh Framework Programme (FP7/20072013) grant agreement n 270327 (DS).

Appendix A: List Acronyms
BAMCP
BAMDP
BA-UCT
BEB
BEETLE
BFS3
BOLT
BOSS
FSSS
IEQL
MCMC
MCTS
MDP
PAC
POMCP
POMDP
RL
SBOSS
UCB1
UCT

Bayes-Adaptive Monte-Carlo Planner (Algorithm name)
Bayes-Adaptive Markov Decision Process
Bayes-Adaptive UCT (Algorithm name)
Bayesian Exploration Bonus (Algorithm name)
Bayesian Exploration Exploitation Tradeoff LEarning (Algorithm name)
Bayesian Forward Search Sparse Sampling (Algorithm name)
Bayesian Optimistic Local Transitions (Algorithm name)
Best Sampled Set (Algorithm name)
Forward Search Sparse Sampling (Algorithm name)
Interval Estimation Q-learning (Algorithm name)
Monte-Carlo Markov Chain
Monte-Carlo Tree Search (Algorithm name)
Markov Decision Process
Probably Approximately Correct
Partially-Observable Monte-Carlo Planner (Algorithm name)
Partially Observable Markov Decision Process
Reinforcement Learning
Smarter Best Sampled Set (Algorithm name)
Upper Confidence Bound 1 (Algorithm name)
Upper Confidence bounds applied Trees (Algorithm name)

Appendix B: Algorithms Implementation
algorithms implemented C++ (code components shared across algorithms much possible):
BAMCP - algorithm presented Section 3, implemented root sampling,
lazy sampling, rollout learning. algorithm run different number
simulations (10 10000) span different planning times. experiments,
set ro -greedy policy = 0.5. uct exploration constant
left unchanged experiments (c = 3), experimented values
c {0.5, 1, 5} similar results.
SBOSS (Castro & Precup, 2010): domain, varied number samples
K {2, 4, 8, 16, 32} resampling threshold parameter {3, 5, 7}.
BEB (Kolter & Ng, 2009): domain, varied bonus parameter
{0.5, 1, 1.5, 2, 2.5, 3, 5, 10, 15, 20}.
877

fiGuez, Silver, & Dayan

BFS3 (Asmuth & Littman, 2011) domain, varied branching factor
C {2, 5, 10, 15} number simulations (10 2000). depth search
set 15 domains except larger grid maze domain
set 50. also tuned Vmax parameter domain Vmin always set
0.
Code paper found online first authors website, directly
following GitHub link https://github.com/acguez/bamcp.

Appendix C: Inference Details Infinite 2D Grid Task
construct Markov Chain using Metropolis-Hastings algorithm sample
posterior distribution row column parameters given observed transitions, following
notation introduced Section 4.2. Let = {(i, j)} set observed reward
locations, associated observed reward rij {0, 1}. proposal distribution
chooses row-column pair (ip , jp ) uniformly random,
Pand samples pip Beta(1 +
m1 , 1 + n1 ) qjp Beta(2 + m2 , 2 + n2 ), m1 = (i,j)O
P 1i=ip rij (i.e., sum

2
rewards observed column) n1 = (1 /2(2 + 2 )) (i,j)O 1i=ip (1 rij ),
similarly m2 , n2 (mutatis mutandis). n1 term proposed column parameter
pi rough correction term, based prior mean failure row parameters,
account observed 0 rewards column due potentially low row parameters.
Since proposal biased respect true conditional distribution (from
cannot sample), also prevent proposal distribution getting peaked. Better
proposals (e.g., taking account sampled row parameters) could devised,
would likely introduce additional computational cost proposal generated
large enough acceptance probabilities (generally 0.5 experiments).
parameters pi , qj j present kept last accepted samples
(i.e., pi = pi qj = pj js), parameters pi , qj linked
observations (lazily) resampled prior influence acceptance
probability. denote Q(p, q p, q) probability proposing set parameters
p q last accepted sample column/row parameters p q. acceptance
probability computed = min(1, A0 ) where:
P (p, q |h)Q(p, q p, q)
P (p, q |h)Q(p, q p, q)
P (p, q)Q(p, q p, q)P (h| p, q)
=
P (p, q)Q(p, q p, q)P (h| p, q)
Q
n1 m2
n2
rij
1rij
1
pm
(i,j)O 1[i = ip j = jp ](pi qj ) (1 pi qj )
ip (1 pip ) qjp (1 qjp )
Q
= m1
2
rij
1rij .
n2
pip (1 pip )n1 qm
(i,j)O 1[i = ip j = jp ](pi qj ) (1 pi qj )
jp (1 qjp )

A0 =

last accepted sampled employed whenever sample rejected. Finally, reward values
Rij resampled lazily based last accepted sample parameters pi , qj ,
observed already. omit implicit deterministic mapping obtain
dynamics P parameters.
878

fiBayes-Adaptive Monte-Carlo Planning

Appendix D: Existence Bayes-Optimal Policy
described Definition 1, Martin (1967) proves following statement mdps
finite state spaces.
Theorem 3 (Martin, 1967, Thm. 3.2.1) Let v(s, h, ) expected discounted return
mdp (with |S| |A| finite) process starts augmented state hs, hi
EE policy used. Let
v (s, h) = sup v(s, h, ).

(42)



policy v (s, h) = v(s, h, ).
proof Theorem 3 consists proving set EE policies mapped
compact subset real line, mapping function v(s, h, )
continuous set. proof requires ordering histories relies
finiteness state space. Let N = |S|, history ordering employed Martin
(1967) is:
t0
X
st N t+1 ,
(43)
z(ht0 )
t=1

z(h) number corresponding history h order. general |S|
finite, scenarios may bound Nt number states agent
time (for example Infinite Grid scenario). scenarios consider
following ordering histories:
0

w(ht0 )


X

st

t=1




Nk1 ,

(44)

k=0

reduces w(h) = z(h) Nt = N t. ordering, proof Theorem 3
carried Martin (1967) (with minimal modifications) prove
statement general mdps - state space infinite possible states
agent grows controlled manner time.
Although reassuring know Bayes-optimal policy exists additional cases, practice satisfied approximation Bayes-optimal policy
existence -Bayes-optimal policies likely guaranteed even general
scenarios.

879

fiGuez, Silver, & Dayan

References
Agrawal, S., & Goyal, N. (2011). Analysis Thompson sampling multi-armed
bandit problem. Arxiv preprint.
Araya-Lopez, M., Buffet, O., & Thomas, V. (2012). Near-optimal BRL using optimistic
local transitions. Proceedings 29th International Conference Machine
Learning.
Asmuth, J., Li, L., Littman, M., Nouri, A., & Wingate, D. (2009). Bayesian sampling
approach exploration reinforcement learning. Proceedings Twenty-Fifth
Conference Uncertainty Artificial Intelligence, pp. 1926.
Asmuth, J., & Littman, M. (2011). Approaching Bayes-optimality using Monte-Carlo tree
search. Proceedings 27th Conference Uncertainty Artificial Intelligence,
pp. 1926.
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis multiarmed
bandit problem. Machine learning, 47 (2), 235256.
Bellman, R. (1954). theory dynamic programming. Bull. Amer. Math. Soc, 60 (6),
503515.
Brafman, R., & Tennenholtz, M. (2003). R-max-a general polynomial time algorithm
near-optimal reinforcement learning. Journal Machine Learning Research, 3,
213231.
Bubeck, S., Munos, R., & Stoltz, G. (2009). Pure exploration multi-armed bandits
problems. Proceedings 20th international conference Algorithmic learning
theory, pp. 2337. Springer-Verlag.
Castro, P., & Precup, D. (2010). Smarter sampling model-based Bayesian reinforcement
learning. Machine Learning Knowledge Discovery Databases, pp. 200214.
Springer.
Castro, P. (2007). Bayesian exploration Markov decision processes. Ph.D. thesis, McGill
University.
Castro, P., & Precup, D. (2007). Using linear programming Bayesian exploration
Markov decision processes. Proceedings 20th International Joint Conference
Artificial Intelligence, pp. 24372442.
Coquelin, P., & Munos, R. (2007). Bandit algorithms tree search. Proceedings
23rd Conference Uncertainty Artificial Intelligence, pp. 6774.
Cozzolino, J., Gonzalez-Zubieta, R., & Miller, R. (1965). Markov decision processes
uncertain transition probabilities. Tech. rep., 11, Operations Research Center, MIT.
Davies, S., Ng, A., & Moore, A. (1998). Applying online search techniques reinforcement
learning. Proceedings National Conference Artificial Intelligence, pp.
753760.
Dayan, P., & Sejnowski, T. (1996). Exploration bonuses dual control. Machine Learning,
25 (1), 522.
880

fiBayes-Adaptive Monte-Carlo Planning

Dearden, R., Friedman, N., & Russell, S. (1998). Bayesian Q-learning. Proceedings
National Conference Artificial Intelligence, pp. 761768.
Doshi-Velez, F., Wingate, D., Roy, N., & Tenenbaum, J. (2010). Nonparametric bayesian
policy priors reinforcement learning. Advances Neural Information Processing
Systems (NIPS).
Duff, M. (2003). Design optimal probe. Proceedings 20th International
Conference Machine Learning, pp. 131138.
Duff, M. (2002). Optimal Learning: Computational Procedures Bayes-Adaptive Markov
Decision Processes. Ph.D. thesis, University Massachusetts Amherst.
Fonteneau, R., Busoniu, L., & Munos, R. (2013). Optimistic planning belief-augmented
Markov decision processes. IEEE International Symposium Adaptive Dynamic
Programming reinforcement Learning (ADPRL 2013).
Friedman, N., & Singer, Y. (1999). Efficient Bayesian parameter estimation large discrete
domains. Advances Neural Information Processing Systems (NIPS), 1 (1), 417423.
Gelly, S., Kocsis, L., Schoenauer, M., Sebag, M., Silver, D., Szepesvari, C., & Teytaud, O.
(2012). grand challenge computer Go: Monte Carlo tree search extensions.
Communications ACM, 55 (3), 106113.
Gelly, S., & Silver, D. (2007). Combining online offline knowledge UCT. Proceedings 24th International Conference Machine learning, pp. 273280.
Gittins, J., Weber, R., & Glazebrook, K. (1989). Multi-armed bandit allocation indices.
Wiley Online Library.
Guez, A., Silver, D., & Dayan, P. (2012). Efficient Bayes-adaptive reinforcement learning
using sample-based search. Advances Neural Information Processing Systems
(NIPS), pp. 10341042.
Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determination
minimum cost paths. Systems Science Cybernetics, IEEE Transactions on,
4 (2), 100107.
Jaksch, T., Ortner, R., & Auer, P. (2010). Near-optimal regret bounds reinforcement
learning. Journal Machine Learning Research, 99, 15631600.
Kearns, M., Mansour, Y., & Ng, A. (1999). sparse sampling algorithm near-optimal
planning large Markov decision processes. Proceedings 16th international
joint conference Artificial intelligence-Volume 2, pp. 13241331.
Kocsis, L., & Szepesvari, C. (2006). Bandit based Monte-Carlo planning. Machine
Learning: ECML, pp. 282293. Springer.
Kolter, J., & Ng, A. (2009). Near-Bayesian exploration polynomial time. Proceedings
26th Annual International Conference Machine Learning, pp. 513520.
Madani, O., Hanks, S., & Condon, A. (2003). undecidability probabilistic planning
related stochastic optimization problems. Artificial Intelligence, 147 (1), 534.
Martin, J. (1967). Bayesian decision problems Markov chains. Wiley.
881

fiGuez, Silver, & Dayan

Meuleau, N., & Bourgine, P. (1999). Exploration multi-state environments: Local measures back-propagation uncertainty. Machine Learning, 35 (2), 117154.
Mundhenk, M., Goldsmith, J., Lusena, C., & Allender, E. (2000). Complexity finitehorizon markov decision process problems. Journal ACM (JACM), 47 (4),
681720.
Neal, R. M. (1993). Probabilistic inference using markov chain monte carlo methods. Tech.
rep., University Toronto.
Poupart, P., Vlassis, N., Hoey, J., & Regan, K. (2006). analytic solution discrete
Bayesian reinforcement learning. Proceedings 23rd international conference
Machine learning, pp. 697704. ACM.
Ross, S., Pineau, J., Chaib-draa, B., & Kreitmann, P. (2011). Bayesian approach
learning planning Partially Observable Markov Decision Processes. Journal
Machine Learning Research, 12, 17291770.
Ross, S., Pineau, J., Paquet, S., & Chaib-Draa, B. (2008). Online planning algorithms
POMDPs. Journal Artificial Intelligence Research, 32 (1), 663704.
Ross, S. (1983). Introduction stochastic dynamic programming: Probability mathematical. Academic Press, Inc.
Schmidhuber, J. (1991). Curious model-building control systems. IEEE International
Joint Conference Neural Networks, pp. 14581463.
Silver, D., & Veness, J. (2010). Monte-Carlo planning large POMDPs. Advances
Neural Information Processing Systems (NIPS), pp. 21642172.
Silver, D., Sutton, R. S., & Muller, M. (2008). Sample-based learning search
permanent transient memories. Proceedings 25th international conference
Machine learning, pp. 968975. ACM.
Silver, E. (1963). Markovian decision processes uncertain transition probabilities
rewards. Tech. rep., DTIC Document.
Strehl, A., Li, L., & Littman, M. (2009). Reinforcement learning finite MDPs: PAC
analysis. Journal Machine Learning Research, 10, 24132444.
Strens, M. (2000). Bayesian framework reinforcement learning. Proceedings
17th International Conference Machine Learning, pp. 943950.
Sutton, R. (1990). Integrated architectures learning, planning, reacting based
approximating dynamic programming. Proceedings Seventh International
Conference Machine Learning, Vol. 216, p. 224. Citeseer.
Sutton, R., & Barto, A. (1998). Reinforcement learning. MIT Press.
Szepesvari, C. (2010). Algorithms reinforcement learning. Synthesis Lectures Artificial Intelligence Machine Learning. Morgan & Claypool Publishers.
Thompson, W. (1933). likelihood one unknown probability exceeds another
view evidence two samples. Biometrika, 25 (3/4), 285294.
Tonk, S., & Kappen, H. (2010). Optimal exploration symmetry breaking phenomenon.
Tech. rep., Radboud University Nijmegen.
882

fiBayes-Adaptive Monte-Carlo Planning

Vien, N. A., & Ertel, W. (2012). Monte carlo tree search bayesian reinforcement learning.
Machine Learning Applications (ICMLA), 2012 11th International Conference
on, Vol. 1, pp. 138143. IEEE.
Walsh, T., Goschin, S., & Littman, M. (2010). Integrating sample-based planning
model-based reinforcement learning. Proceedings 24th Conference Artificial Intelligence (AAAI).
Wang, T., Lizotte, D., Bowling, M., & Schuurmans, D. (2005). Bayesian sparse sampling
on-line reward optimization. Proceedings 22nd International Conference
Machine learning, pp. 956963.
Wang, Y., Won, K., Hsu, D., & Lee, W. (2012). Monte Carlo Bayesian reinforcement
learning. Proceedings 29th International Conference Machine Learning.
Watkins, C. (1989). Learning delayed rewards. Ph.D. thesis, Cambridge.
Wingate, D., Goodman, N., Roy, D., Kaelbling, L., & Tenenbaum, J. (2011). Bayesian policy
search policy priors. Proceedings International Joint Conferences
Artificial Intelligence.

883

fiJournal Articial Intelligence Research 48 (2013) 783-812

Submitted 08/13; published 11/13

Complexity Optimal Monotonic Planning:
Bad, Good, Causal Graph
Carmel Domshlak
Anton Nazarenko

dcarmel@ie.technion.ac.il
anton.nazarenko@gmail.com

Faculty Industrial Engineering & Management,
Technion - Israel Institute Technology,
Haifa, Israel

Abstract
almost two decades, monotonic, delete free, relaxation one
key auxiliary tools practice domain-independent deterministic planning.
particular contexts satiscing optimal planning, underlies state-of-theart heuristic functions. satiscing planning monotonic tasks polynomial-time,
optimal planning monotonic tasks NP-equivalent. establish negative
positive results complexity wide fragments optimal monotonic planning,
fragments dened around causal graph topology. results shed
light link complexity general optimal planning complexity
optimal planning respective monotonic relaxations.

1. Introduction
domain-independent deterministic (or classical) planning, world states represented complete assignments set variables, operators allow deterministic
modications assignments, objective nd sequence operators
sequentially modies given initial assignment assignment satises certain
predened goal property. last two decades, solvers problem made spectacular advances empirical eciency, especially context state-space
heuristic search planning techniques. eciency made possible largely
ability exploit monotonic, delete-free, relaxations planning tasks (McDermott,
1999; Bonet & Gener, 2001; Homann & Nebel, 2001).
high level, monotonic relaxation replaces regular value switching semantics
planning operators value accumulating semantics. is, operator switches
value variable v x y, relaxed version operator extends
value v {x} {x, y}. key point applying operators value
accumulating semantics reduce applicability operators future. Two
properties monotonic relaxation make especially valuable automated planning. First,
deterministic planning PSPACE-complete even rather conservative propositional
formalisms, planning monotonic tasks polynomial-time (Bylander, 1994), thus
exploited deriving heuristic estimates. Second, numerous problems practical
interest, plans monotonic relaxations distant true plans
problems (Homann, 2005; Helmert & Mattmuller, 2007; Helmert & Domshlak, 2009; Bonet
& Helmert, 2010). Hence, starting seminal HSP (Bonet & Gener, 2001)
FF (Homann & Nebel, 2001) planning systems, exploiting, particular, explicitly
c
2013
AI Access Foundation. rights reserved.

fiDomshlak & Nazarenko

planning for, monotonic relaxations became important ingredient systems
domain-independent deterministic planning. (For comprehensive survey, see, e.g., Betz &
Helmert, 2009.)
Ideally, planner reasoning cost plans monotonic relaxations
reason cost optimal plans monotonic relaxations. Unfortunately,
regular planning monotonic tasks polynomial-time, optimal planning tasks
NP-equivalent (Bylander, 1994), constant-factor approximations problem
provably hard well (Betz & Helmert, 2009). Still, admissible heuristic estimates
principle exploit tractable fragments optimal planning monotonic relaxations (Katz
& Domshlak, 2010). However, best knowledge, substantial fragments
tractability revealed monotonic optimal planning date.
Identifying signicant fragments tractability optimal monotonic planning precisely focus here. special interest establishing connections complexity optimal planning optimal planning monotonic relaxations
respective planning tasks. interest motivated new important role
methods combine tractable fragments regular deterministic planning monotonic relaxation heuristics (Helmert, 2004; Keyder & Gener, 2008a; Katz & Domshlak,
2010; Katz, Homann, & Domshlak, 2013a, 2013b; Katz & Homann, 2013). turn,
comparative perspective brought us consider planning tasks terms nite-domain
representations go beyond standard propositional representation formalisms
STRIPS (Fikes & Nilsson, 1971) ADL (Pednault, 1989). explanation,
possibly even justication, choice analysis place here.
Due close relationship rst-order propositional logics, propositional representations dominated area automated planning since early days AI research. instance, propositional PDDL language still de facto standard problem
description language planning community (Fox & Long, 2003). However, propositional languages blur lot important structure present typical planning tasks
interest. discuss later on, nite-domain representations (FDR) go beyond propositional state variables (Backstrom & Klein, 1991; Backstrom & Nebel, 1995; Helmert,
2009) allowed much deeper much discriminative analysis automated planning complexity. turn, formal developments already translated
practical advances planning, allowing introduction eective enhancements
monotonic relaxation heuristics (Fox & Long, 2001; Helmert, 2004; Helmert & Gener,
2008; Keyder & Gener, 2008a; Cai, Homann, & Helmert, 2013; Katz et al., 2013a,
2013b), abstraction heuristics (Edelkamp, 2001; Helmert, Haslum, & Homann, 2007; Katz
& Domshlak, 2010), decomposition-based planning (Nissim, Brafman, & Domshlak, 2010;
Nissim & Brafman, 2012), search-topology analysis (Homann, 2011), many others.
Nonetheless, value accumulating semantics monotonic relaxation, FDR
longer maintains key advantage propositional representations,
longer seems reason prefer explicit representation certain mutual exclusion
relationships propositions. Therefore, principle, results presented
follows phrased, sometimes even extended, context propositional
representations STRIPS. Why, then, chosen view complexity
optimal relaxed planning lens FDR? primary reason interest
comparative complexity analysis optimal planning optimal planning respective
784

fiThe Complexity Optimal Monotonic Planning

monotonic relaxations. Departing previously discovered fragments tractability
satiscing optimal planning, approach following two high-level questions:
1. fragments deterministic planning, any, optimal planning hard
optimal (monotonically) relaxed planning easy?
2. fragments deterministic planning, any, optimal planning easy yet
optimal (monotonically) relaxed planning hard?
regular planning, best classication days worst-case time
complexity exploits properties graphical structures induced planning
tasks, together various properties FDR state variables size
domains. Hence, discussing optimal relaxed planning FDR tasks keeps us direct
relation well-explored complexity map regular deterministic planning. Moreover,
show known links planning complexity graph-topological
properties FDR tasks even stronger case optimal relaxed planning.
second reason choice recent work already revealed interesting
interplays (either complete partial) monotonic relaxation nite-domain variables graphical structures induced FDR tasks. respective results
context computational complexity non-optimal planning (Katz et al., 2013b), heuristic estimates (Keyder & Gener, 2008b; Katz et al., 2013a; Katz & Homann, 2013),
search-topology analysis (Homann, 2011). instance, Homann (2011) showed that,
causal graph FDR task acyclic, every variable transition invertible,
h+ heuristic induced optimal relaxed plans evaluated states local
minima. result particular testies examining monotonic relaxations
lens FDR lead crisp concisely formulated results.
found work here: show results easily reformulated, sometimes even generalized, terms STRIPS, results
conform easily STRIPS reformulation.
Finally, immediate value results work mostly theoretical,
would like see step towards exploiting optimal relaxed planning devise
heuristic functions deterministic planning. results, Theorem 4,
fact directly used within framework implicit abstractions (Katz & Domshlak,
2010), results possibly exploited within various frameworks
partial monotonic relaxation as, e.g., recent red-black planning framework Katz
et al. (2013b, 2013a). extent actually happen remains, course,
seen. However, focus implicit abstractions red-black planning
nite-domain task representations make much easier assess relevance
results frameworks.

2. Formalism, Background, Related Results
use notation [n] refer set {1, . . . , n}. directed graphs, edge x
denoted (x, y), undirected graphs, edge x denoted
{x, y}. ||x|| refer representation size object x, confused |x|,
denotes number elements set x.
785

fiDomshlak & Nazarenko

2.1 FDR, MFDR, Monotonic Relaxation
adopt terminology notation Katz et al. (2013b). planning task
nite-domain representation (FDR) given quintuple = V, A, I, G, cost,
where:
V set state variables, v V associated nite domain
D(v). partial variable assignment p function variable subset V(p) V
assigns v V(p) value p[v] D(v) domain. partial variable assignment
called state V(s) = V .
initial state. goal G partial variable assignment V .
nite set actions. action pair pre(a), e(a) partial variable
assignments V called precondition eect, respectively.
cost : R0+ real-valued, nonnegative action cost function.
Auxiliary notation:
partial assignment p variable subset V V(p), p[V ] denote
assignment provided p V . ease presentation, sometimes also specify
partial assignments sets constructs v d, v V D(v).
variable v V , Av denote actions aecting value v,
is, Av = {a | v V(e(a))}. sequence actions state variable v V ,
v denote restriction actions Av .
semantics FDR tasks follows. action applicable state
s[v] = pre(a)[v] v V(pre(a)). Applying state changes value every v
V(e(a)) e(a)[v]; resulting state denoted sJaK. sJa1 , . . . , ak K denote
state obtained sequential application (respectively applicable) actions a1 , . . . , ak
starting state s. action sequence s-plan sJa1 , . . . , ak K[V(G)] = G,
optimal s-plan sum action costs minimal among s-plans.
computational task (optimal) planning nding (optimal) I-plan. follows,
(optimal) I-plans often referred simply (optimal) plans .
monotonic nite-domain representation (MFDR) planning task given
quintuple = V, A, I, G, cost exactly FDR tasks, semantics dierent.1
Informally, MFDR tasks state variables accumulate values, rather switching
them. specically, MFDR state function assigns v V
non-empty subset s[v] D(v) domain. MFDR action applicable state
pre(a)[v] s[v] v V(pre(a)). Applying MFDR action state changes
value v V(e(a)) s[v] s[v] {e(a)[v]}. Respectively, MFDR action sequence
a1 , . . . , ak applicable state s-plan G[v] sJa1 , . . . , ak K[v] v V(G).
respects, MFDR FDR semantics identical.
1. entirely clear original formulation monotonic relaxation multi-valued variable
domains attributed, traced back least work Helmert (2006)
Fast Downward planning system.

786

fiThe Complexity Optimal Monotonic Planning

FDR planning PSPACE-complete even propositional state variables, planning MFDR tasks polynomial time (Bylander, 1994). Starting HSP (Bonet
& Gener, 2001) FF (Homann & Nebel, 2001) planning systems, exploiting attractive property MFDR deriving heuristic estimates via notion monotonic
relaxation became key ingredient many planning systems. Given FDR planning task
= V, A, I, G, monotonic relaxation MFDR task + = .
state , optimal relaxation heuristic h+ (s) dened cost optimal
plan MFDR task V, A, s, G, (optimal) relaxed planning refer
(optimal) planning + . + plan + , + referred relaxed
plan .
Finally, FDR MFDR, sometimes distinguish planning tasks
terms pair standard graphical structures induced description tasks.
causal graph CG digraph nodes V . arc (v, v ) CG v =
v exists action (v, v ) V(e(a))V(pre(a))V(e(a)).
case, say (v, v ) induced a. succ(v) pred(v) respectively
denote sets immediate successors predecessors v CG .
domain transition graph DTG(v, ) variable v V arc-labeled
digraph nodes D(v) arc (d, ) labeled pre(a)[V \ {v}]
cost(a) belongs graph e(a)[v] = , either pre(a)[v] =
v V(pre(a)).
2.2 Causal Graph Treewidth Planning Complexity
Introduced Halin (1976), tree-width went unnoticed independently rediscovered Robertson Seymour (1984) Arnborg, Cornell, Proskurowski (1987).
since received widespread attention due numerous graph-theoretic algorithmic
applications. Informally, tree-width graph measure close structure
graph tree. example, tree-width tree 1, regardless size,
whereas tree-width complete graph n nodes n 1. Formally, tree-width
graph dened via notion tree decomposition follows.
tree decomposition connected undirected graph G = (V, E) pair T, ,
= (V , E ) tree, i.e., connected acyclic graph, : V 7 2V that:
1. every v V , set {t V | v (t)} non-empty connected.
2. every (v, u) E V {v, u} (t).
width tree decomposition T, G max{|(t)| | V } 1, treewidth G, tw(G), minimum width tree decompositions G. Following
appears standard terminology, tree-width digraph G refer
tree-width undirected graph induced G (Berwanger, Dawar, Hunter, Kreutzer, &
Obdzralek, 2012).
development parametrized complexity analysis (Downey & Fellows, 1999;
Flum & Grohe, 2006), shown many NP-hard problems solved
polynomial time restricted induce certain problem-specic graphical structures
xed tree-width. particular, constraint satisfaction constraint optimization
787

fiDomshlak & Nazarenko

problems nite-domain variables solved time polynomial size
explicit description problems, exponential tree-width induced
constraint graph (Dechter, 2003). Since causal graph captures high-level structure
planning problems, one would expect tree-width play similar role
worst-case time complexity satiscing optimal FDR planning. Unfortunately,
results direction mostly negative.
standard assumption parametric complexity hierarchy W[1] nu-FPT
(Flum & Grohe, 2006), Chen Gimenez (2010) proved that, family
digraphs C, FDR planning tasks inducing causal graphs C polynomial-time
size connected components C bounded constant.
family digraphs tree-width 1 trivially fails satisfy latter condition,
immediate corollary result even satiscing FDR planning restricted
causal graphs tree-width 1 polynomial.
construction proof Chen Gimenez (2010) uses FDR tasks
variable domains parametric size, work Gimenez Jonsson (2009b)
shows negative result causal graphs tree-width 1 holds even
restricted planning tasks xed variable domains. Specically, Gimenez
Jonsson show FDR planning chain causal graphs NP-hard even
restricted variables domains size 5.
negative results role causal graphs tree-width computational
tractability FDR planning strong, apparently tell part story.
shown Brafman Domshlak (2006, 2013), causal graphs tree-width play
role worst-case time complexity FDR planning, tight interplay
another parameter called tasks local depth. Informally, local depth FDR task
captures minmax amount work required single variable order solve . Since
later refer result Brafman Domshlak, precise specication local depth
warranted here: denoting P lans() (possibly innite) set plans FDR
task , local depth
=

min

max {|v |},

P lans() vV

is, maximal number value changes single state variable, along plan
minimizes quantity among plans . Theorem 6 Brafman Domshlak
(2013), FDR tasks solved time polynomial |||| exponential
O(tw(CG ) ). possible stratications based succinct representation
internal variable domain dynamics, suggested Fabre, Jezequel, Haslum,
Thiebaux (2010), appears strongest link discovered far
complexity general FDR planning graph-topological properties causal
graphs. Note also positive result applies satiscing planning; applicable
optimal planning limited settings (Fabre et al., 2010; Brafman & Domshlak,
2013).
788

fiThe Complexity Optimal Monotonic Planning

3. Negative Results: Bottleneck Variable Domains
focus connections worst-case time complexity optimal relaxed
planning structure problems causal graphs. Note causal graphs
FDR tasks trivially invariant monotonic relaxation: since + = ,
CG+ = CG . mentioned, previous works already revealed certain connections structure causal graphs, particular tree-width,
complexity FDR planning. follows, show link even stronger
somewhat intriguing case optimal relaxed planning. said that,
begin set negative results which, least rst glance, suggest
link actually likely.
Denition 1 connected digraph G = (N, E) fork contains exactly one node
r N non-zero out-degree, is, E = {(r, n) | n N \ {r}}. Similarly, G
inverted fork contains exactly one node r N non-zero in-degree, is,
E = {(n, r) | n N \ {r}}. respective special nodes r fork inverted-fork graphs
called roots graphs.
Considering FDR planning tasks fork inverted fork causal graphs, rst
impression might FDR fragments restricted enough allow polynomialtime planning. This, however, case: even non-optimal planning FDR tasks
simple causal graphs hard, even variables roots
restricted binary-valued (Domshlak & Dinitz, 2001). hand, especially
since non-optimal relaxed planning FDR polynomial-time, results direct
inuence complexity optimal relaxed planning respective FDR fragments.
Nonetheless, surprisingly not, problem hard.
Theorem 1 Optimal relaxed planning NP-equivalent even restricted FDR tasks
fork inverted-fork structured causal graphs. Moreover, result holds even
state variables roots restricted binary domains.
Proof: proof polynomial reductions NP-equivalent problems (minimum) Directed Steiner Tree (minimum) Set Cover (Karp, 1972).
Directed Steiner Tree: Given digraph G = (N, E) arc weights w : E R0+ ,
set terminals Z N , root vertex nr , nd minimum weight arborescence
(directed tree) rooted nr N terminals Z included .
Set Cover: Given collection C subsets nite set S, nd minimum cardinality
subset C C every element belongs least one member C .
Fragment I: Given Directed Steiner Tree problem G = (N, E), w, Z, nr , corresponding fork-structured FDR task = V, A, I, G, cost constructed follows.
variable set V contains variable per terminal node G, plus extra variable r,
is, V = {r} VZ VZ = {vz | z Z}. domain r, D(r) = N , corresponds
nodes G, variables binary-valued, D(vz ) = {0, 1}.
initial state, I[r] = nr I[v] = 0 v VZ . goal achieve value 1
v VZ . arc e = (x, y) E, action set contains root-changing action
789

fiDomshlak & Nazarenko

ae pre(ae ) = {r x}, e(ae ) = {r y}, cost(ae ) = w(e). Likewise,
terminal z Z, contains vz -changing action az pre(az ) = {r z, vz 0},
e(az ) = {vz 1}, cost(az ) = 0. construction clearly polynomial, causal
graph forms fork rooted r, variable domains required
theorem. also holds that:
(i) relaxed plan , set arcs {e | ae r } G induces connected
sub-graph G containing nr terminals Z included G (or otherwise
least one leaf variables could changed goal value).
Likewise, directed path G nr every node n G (or
otherwise respective value n root variable r could achieved
along ). Hence, particular, G contains arborescence rooted nr includes
terminals Z.
(ii) Vice versa, let arborescence G rooted nr includes terminals
Z = {z1 , . . . , zm }, let {e1 , . . . , ek } topological ordering arcs .
ae1 , . . . , aek , az1 , . . . , azm relaxed plan , cost precisely
weight .
Hence, optimal relaxed plans induce minimum directed Steiner trees G = (N, E), w, Z, nr ,
vice versa.
Fragment II: Given Set Cover problem S, C = {1, 2, . . . , m} |C| = n,
corresponding inverted-fork structured FDR task = V, A, I, G, cost constructed
follows. variable set V contains variable per member C, plus extra variable r,
is, V = {r}{vc | c C}. domain r D(r) = {0}S, variables
binary-valued, D(vc ) = {0, 1}. initial state, I[v] = 0 v V . goal
achieve value special variable r. S, subset c C c,
action set contains root-changing action ai;c pre(ai;c ) = {r (i 1), vc 1},
e(ai;c ) = {r i}, cost(ai;c ) = 0. Likewise, c C, contains vc -changing
action ac pre(ac ) = {vc 0}, e(ac ) = {vc 1}, cost(ac ) = 1. construction
polynomial, causal graph forms inverted fork rooted r, variable
domains required theorem. Note that, due chain-like structure
domain transition graphs , dierence plans
+ , easy verify plan induces cover cost,
vice versa. Hence, optimal relaxed plans induce minimum set covers S, C,
vice versa.

Corollary 1 Optimal relaxed planning NP-equivalent even restricted FDR tasks
causal-graph tree-width 1.
corollary immediate Theorem 1 undirected graphs induced
forks inverted forks special case trees thus tree-width 1. rst
glance, message Corollary 1 discouraging respect agenda: structure
causal graphs seem play major role complexity optimal
relaxed planning FDR. next result, however, seems even discouraging
respect prospects tractability optimal relaxed planning FDR.
790

fiThe Complexity Optimal Monotonic Planning

Theorem 2 Optimal relaxed planning NP-equivalent even restricted FDR tasks
two state variables.
Proof: proof polynomial reduction (minimum) directed Steiner tree
problem, fact, proof similar fork case Theorem 1. Given
Directed Steiner Tree problem G = (N, E), w, Z, nr Z = {z1 , . . . , zm }, compile
FDR task = {v1 , v2 }, A, I, G, cost follows. domain v1 corresponds
nodes G, domain v2 corresponds terminal nodes root
node, is, D(v1 ) = N D(v2 ) = Z {nr }. initial state, I[v1 ] = nr
I[v2 ] = nr , goal achieve value zm v2 . arc e = (x, y) E,
action set contains v1 -changing action ae pre(ae ) = {v1 x}, e(ae ) = {v1 y},
cost(ae ) = w(e). Likewise, denoting nr z0 , 1 m, contains v2 -changing
action azi pre(azi ) = {v1 zi , v2 zi1 }, e(azi ) = {v2 zi }, cost(azi ) = 0.
construction polynomial, correctness stems analysis identical
proof Theorem 1.

Theorem 2 shows even dimensionality FDR state spaces plays secondary
role, any, complexity optimal relaxed planning. that, however, answers
one two macro-questions agenda:
Corollary 2 exist fragments FDR optimal planning polynomial-time,
optimal relaxed planning NP-equivalent.
corollary immediate Theorem 2 polynomial-time solvability optimal
planning FDR tasks xed number state variables.

4. Positive Results I: State Variables Fixed-Size Domains
Depending readers background intuitions, Theorem 2 either surprise seem
somewhat predictable. case, Theorem 2 led us consider dierent
(and time fruitful) fragmentation optimal relaxed planning.
closer look Theorem 1 Lemma 2 reveals size FDR variable
domains might crucial complexity optimal relaxed planning: proofs
two claims rely heavily parametric domain size state variables,
proofs also imply optimal relaxed planning hard even single FDR
variable comes parametric domain size. Departing observation,
show that, all, topology causal graph play interesting role worstcase time complexity classication optimal relaxed planning FDR tasks. particular,
turns bounding tree-width causal graph constant takes
achieve polynomial-time optimal relaxed planning FDR tasks xed-size variable
domains.
Theorem 3 family directed graphs C, tree-width C bounded
constant, optimal relaxed planning FDR tasks xed-size variable domains
causal graphs C polynomial-time.
791

fiDomshlak & Nazarenko

Proof: proof Theorem 3 inspired closely resembles approach Brafman
Domshlak (2013) discussed Section 2.2. Given FDR task = V, A, I, G, cost,
compile constraint optimization problem COP+ = (X , ) nite-domain
variables X , functions , global objective minimize (X ). + unsolvable, assignments X evaluate objective function . Otherwise,
optimum objective obtained assignments X correspond
optimal plans + , is, optimal relaxed plans .
Let |V | = n, = maxvV |D(v)|, recall pred(v) state variable v V denotes
set vs immediate predecessors causal graph.
state variable v V , X contains variable xv represents choice
subset actions Av participate plan looking for. possible
choices form domain D(xv ) xv , choice represented set
size smaller equal , element set quadruple
(d, id, a, t),
D(v), id {v} pred(v), {1, . . . , n}, and, id = v, {a
Av | e(a )[v] = d} otherwise =. high level, view state variables
active decision makers, value relaxed variable v aims achieve
accumulate time point t, either using action a, delegating
task another variable id. show later on, variable accumulate
values, since accumulated values never lost, optimal plans +
cannot longer n actions.
state variable v V , contains non-negative, extended real-valued function v D(xv ). Likewise, pair state variables {v, w}
causal graph CG contains either arc (v, w) arc (w, v), contains indicator
function v,w : D(xv ) D(xw ) {0, }. simplify specication v,w ,
dene set auxiliary constraints follows.
(S1) [Precondition Constraint] assignment v1 , . . . , vn X satises S1 i,
v V , (d, v, a, t) v implies that, w V(pre(a)),
pre(a)[w] {I[w]} {d | (d , , , ) w , < t}.

(1)

(S2) [Delegation Constraint] assignment v1 , . . . , vn X satises S2 i, v V ,
(d, w, , t) v implies that, Av Aw e(a)[v], (, w, a, t) w .
(S3) [Goal Achievement Constraint] assignment v1 , . . . , vn X satises S3 i,
v V(G),
G[v] {I[v]} {d | (d, , , ) v }.
(2)
Constraint S1 ensures preconditions actions variable committed
provided time. Constraint S2 ensures outsourced value achievements
fullled required time points. Finally, constraint S3 simply veries value
v induced v goal value. Importantly, S3 corresponds set n unary constraints,
792

fiThe Complexity Optimal Monotonic Planning

S1 S2 represented set binary constraints X . Given that,
functions specied

v (v ) =
cost(a),
(,v,a,)v

{
0, {v , w } satises S1(xv , xw ), S2(xv , xw ), S3(xv ) S3(xw )
v,w (v , w ) =
,
, otherwise

(3)

S1(xv , xw ), S2(xv , xw ), S3(xv ) correspond binary unary constraints
induced respectively S1, S2, S3 COP variables xv xw .
Let us take closer look COP+ constructed problems
scope Theorem 3.
(1) constraint network COP+ corresponds undirected graph induced
causal graph CG+ (= CG ). Hence, since tree-width latter bounded
constant scope theorem, tree-width constraint network
COP+ . nding optimal tree decomposition graph G NP-hard,
tree decomposition G width c tw(G) low constant c found
time polynomial size G (Robertson & Seymour, 1991; Becker & Geiger,
1996; Amir, 2010). Hence, COP+ solved time polynomial size
representation using standard message-passing algorithm constraint optimization
trees (Dechter, 2003).
(2) Recall values COP variable xv sets quadruples (d, id, a, t) size
. Then, size xv domain D(xv ) upper-bounded
(|D(v)| |{z}
n (|Av | + 1) (n)) ,
| {z }
| {z } | {z }


id



(4)



since = O(1), |D(xv )| = O(poly(||||)). Together (1), implies
COP+ solved time O(poly(||||)).
prove correctness compilation showing that, + unsolvable,
assignments X evaluate objective function (X ) , otherwise, objective minimized assignments X correspond
optimal plans + .
First, given assignment = v1 , . . . , vn X () < , show
induces valid plan + cost (). Note that, since () < ,
Eq. 3 satisfying constraints
S1-S3.

Consider multi-set Z = vV {(a, t) | (, v, a, t) v } induced , let
Z = {(a1 , t1 ), . . . , (am , tm )}, = |Z|,
arbitrary ordering Z that, 1 j < m, holds tj ti .
1 m, let v variable charge performing action ai , is,
(e(ai )[v], v, ai , ti ) v . w V(pre(ai )), Eq. 1 constraint S1 implies either
pre(ai )[w] = I[w] (pre(ai )[w], id, a, t) w < ti . latter case,
793

fiDomshlak & Nazarenko

id = w, construction , = aj j < i, denition
D(xw ), pre(ai )[w] = e(aj )[w]. Otherwise, id = w w = w,
denition S2, pre(ai )[w] = e(a )[w] (, w , , t) w . Thus, = aj
j < i, pre(ai )[w] = e(aj )[w]. Therefore, action sequence
= a1 , . . . ,
applicable initial state + , given that, satisfying S3 implies
plan + . Finally, immediate construction Eq. 3
cost( ) = ().
show optimal plan = a1 , . . . , + induces assignment
= v1 , . . . , vn X ( ) = cost(). denition MFDR,
1 state variable v V , IJa1 , . . . , ai1 K[v] IJa1 , . . . , ai K[v].
optimality , 1 m, exists least one variable v V
IJa1 , . . . , ai1 K[v] IJa1 , . . . , ai K[v]. particular, implies actions {a1 , . . . , }
dierent, variable v V ,


/v = {ai | IJa1 , . . . , ai1 K[v] IJa1 , . . . , ai K[v]},
then2 |/v| .
Adopting arbitrary ordering {v1 , . . . , vn } state variables V , 1 j n, let
/vj = {aj1 , . . . , ajmj }. turn, 1 l mj , let djl D(vj ) value achieved
accumulated vj action ajl , i.e., IJa1 , . . . , ajl K[vj ] \ IJa1 , . . . , ajl 1 K[vj ] = {djl }.

1 l mj , ajl j1
k=1 /vk , vj set contain (djl , vj , ajl , jl ), otherwise, vj
set contain (djl , vk , , jl ) k = min {k | ajl /vk }.
construction , action present value exactly one
variable xv , Eq. 3, v (v ) sums cost exactly once. Hence, satises
constraints S1-S3 enforced step functions v,w , ( ) =
cost(). former also directly follows construction . 1 j n,
mj
let vj = {(djl , idjl , ajl , tjl )}l=1
. denition values djl above, set
values {dj1 , . . . , djmj } exactly set values vj gets accumulated relaxed
plan I, thus satisfaction S3 follows plan + . Again,
construction , sequence time points {tj1 , . . . , tjmj } corresponds time
points rst achievements {dj1 , . . . , djmj }, respectively, along ,
rst achiever ajl along , captured properly scheduled either vi neighbor
vi causal graph. Hence, constraints S1 S2 satised well.
nalizes proof compilation correctness, thus Theorem 3.

Note Theorem 3 particular answers second macro-question agenda:
Corollary 3 exist fragments FDR (even satiscing) planning NPequivalent, optimal relaxed planning polynomial-time.
2. corresponds
well-known fact that, notation, optimal plans MFDR cannot

longer vV |D(v)| n.

794

fiThe Complexity Optimal Monotonic Planning

corollary immediate Theorem 3 discovery Gimenez Jonsson
(2009b) FDR planning chain causal graphs NP-hard even restricted variables
domains size 5.
Following STRIPS vs. FDR discussion introduction, Theorem 3 trivially
implies optimal relaxed planning STRIPS tasks done time polynomial
|||| exponential tree-width causal graph. actually
example tractability fragment one benet switching
propositional representation: formulation result remains same,
coverage result grows because, size FDR variable domains bounded
= O(1), tree-width causal graph STRIPS representation
times larger FDR. However, also smaller, identical.
Later, however, present results directly benet nite-domain input
representation planning tasks.
discussion remainder section addresses readers familiar
work Brafman Domshlak (2013) detail. discussion skipped without
loss continuity.
rst view, compilation proof Theorem 3 brings mind compilation algorithm
v1
v2
vn
behind proof Theorem 6 Brafman Domshlak (2013). One might thus naturally ask whether
algorithm cannot used directly proof
Theorem 3. stands, however, algorithm
vn+1
Brafman Domshlak yield polynomialtime complexity tasks Theorem 3
concerned. see why, consider FDR task = V, A, I, G, cost V = {v1 , . . . , vn+1 },
where, vi , D(vi ) = {0, 1}, I[vi ] = 0, G[vi ] = 1, = {a1 , . . . , }
V(pre(ai )) = , V(e(ai )) = {vi , vn+1 }, e(ai )[vi ] = e(ai )[vn+1 ] = 1. causal graph
, depicted above, tree-width 1. However, plan + , maxvV {|v |}
n, local depth + n. + cannot solved without applying
n actions least once, actions aects value variable
vn+1 . Hence, nding even non-optimal plan + using algorithm Brafman
Domshlak (2013) take time exponential ||||.
nal note, actually shown algorithm Brafman Domshlak
(2013) optimal polynomial-time sub-class MFDR tasks like Theorem 3
also restricted single-eect operators. multiple-eect actions complicate
matters, require dierent algorithmic approach guarantee planning tractability.

5. Positive Results II: M-unfoldable State Variables
Despite discouraging results Section 3, return consider FDR tasks
parametric-size domains. Recall that, variable values monotonic relaxation +
correspond sets values respective variables , large variable domains
+ given implicitly, via variable domains D(v1 ), . . . , D(vn ) . conciseness
representation, however, hides many aspects problem structure + otherwise
might exploited planning eciency. particular, reasoning domain
795

fiDomshlak & Nazarenko

transition graphs induced FDR tasks successfully exploited complexity
analysis FDR (Jonsson & Backstrom, 1998; Domshlak & Dinitz, 2001; Katz & Domshlak,
2008; Gimenez & Jonsson, 2009a). contrast, monotonic relaxations, true domain
transition graphs + , denoted henceforth DTG(v, + ), cannot always represented
explicitly number nodes graphs exponential ||||. However,
always, always be, case, focus planning
accessible monotonic variables.
Denition 2 Given FDR planning task = V, A, I, G, cost, eective domain
(v) v V + consists value subsets D+ (v) = 2D(v) \ reachable
{I[v]} DTG(v, + ). is, (v)
(i) I[v] ,
(ii) value , directed path I[v] unlabeled digraph
induced DTG(v, ) values along path belong .
elements (v) called eective values v + .
Denition 3 Let innite set FDR tasks, property state variables
that, task , partitions state variables satisfy
(referred -variables), satisfy . say -variables
monotonically unfoldable (M-unfoldable) exists integer k N that,
every task every -variable v , |D (v)| = O(||||k ).
Denition 3 property FDR state variables, particular,
property dened respect tasks causal graphs, root, sink, nodes
whose causal graph in-degrees larger causal graph out-degrees, etc. Informally, -variables set FDR tasks M-unfoldable if, every task
every -variable v , relevant subgraph DTG(v, + ) described explicitly
space polynomial representation size . instance, v trivially M-unfoldable
size domain bounded constant, even O(log(||||)). generally, let DTG (v, ) digraph obtained (labels ignored) domain transition
graph DTG(v, ) unifying parallel edges. hard verify Denitions 2 3
v M-unfoldable number arborescence subgraphs DTG (v, )
rooted I[v], covering G[v] v V(G), O(poly(||||)).
return consider fork-structured FDR tasks. Theorem 1 considered
fork-structured FDR tasks root variables unrestricted, consider
inverse fragment, corresponding fork-structured FDR tasks root variables
restricted. Optimal FDR planning tasks polynomial-time |D(r)| =
2 (Katz & Domshlak, 2010), NP-equivalent |D(r)| > 2 (Katz & Keyder, 2012).
contrast, Theorem 4 shows optimal relaxed planning fork-structured FDR
tasks polynomial-time much wider class root variables. Moreover, simple
observation behind construction proof Theorem 4 later generalized capture
much richer fragment causal graphs.
Theorem 4 Optimal relaxed planning polynomial time set FDR tasks
fork-structured causal graphs M-unfoldable root variables.
796

fiThe Complexity Optimal Monotonic Planning

Proof: Let = V, A, I, G, cost fork-structured FDR task root r Vleafs =
V \ {r} = {v1 , . . . , vn }. assume goal values specied variables Vleafs ;
none leaves Vleafs \ V(G) need change value all, thus
schematically removed problem. Given FDR task , optimal
plan relaxation + constructed follows.
eective values (r) consistent goal (that is, G[r]
r V(G)) processed one one, independently. eective value ,
extract following information.
(1) root variable r, determine cheapest path I[r] DTG(r, + ),
() denoting action sequence inducing path.
(2) leaf variable v Vleafs ,
(a) schematically remove domain transition graph DTG(v, )
arcs labeled actions supported , is, actions
r V(pre(a)) pre(a)[r] ,
(b) determine cheapest path I[v] G[v] arc-reduced domain
transition graph, (G[v]) denoting action sequence inducing
path.
Return concatenation action sequences ( ) (G[v1 ]) . . . (G[vn ])
[
]
n


cost( (G[vi ])) ,
= argmin cost(()) +
(r)

i=1

cost() action sequence sum costs actions along .
algorithm polynomial-time given explicit description eective part
DTG(r, + ), thus polynomial-time r M-unfoldable. Recall that, since
causal graph acyclic, action aects one variable. correctness
algorithm stems simple observation that, relaxed plan fork-structured
FDR task , = r v1 . . . vn also relaxed plan , (trivially) identical
cost. Hence, searching optimal relaxed plan , restrict
plans latter form, immediate description algorithm
nds cheapest plan.

aside, following STRIPS vs. FDR discussion, note Theorem 4 provides
example exploiting value grouping induced FDR representation planning tasks. simple algorithm proof exploits fact actions
change value leaf variable depend value, prevents restrictions
placed either size leaf domains, structure domain
transition graphs. course, Theorem 4 also reformulated terms STRIPS, yet
would require respective partition propositions given/discovered,
essentially equivalent starting something like FDR input rst place.
scope tractability result Theorem 4 fairly limited terms
causal graph structure, nice property sets optimal relaxed plans forkstructured FDR tasks, exploited proof Theorem 4, generalized much
797

fiDomshlak & Nazarenko

wider fragment causal graphs. turn, generalization allows us provide next
tractability result wide fragment optimal relaxed planning FDR.
Lemma 1 Let = V, A, I, G, cost FDR task directed acyclic causal graph,
let {v1 , . . . , vn } arbitrary topological ordering V respect CG . Then,
plan + , = v1 . . . vn also plan + .
Proof: Directed acyclicity causal graph particular implies action aects
one variable. proof lemma stems combining property
(i) fact preserves order actions vi , (ii) core property
monotonic relaxations + that, variable v, value D(v), state
relaxed task + , s[v], [v] reachable + .
Since CG forms DAG state variables ordered according topological
ordering CG , V(pre(a)[v1 ]) {v1 } actions v1 . Thus, order preservation
v1 along respect implies v1 applicable I, IJv1 K[v1 ] = IJK[v1 ],
IJv1 K[vi ] = I[vi ] > 1. Assume that, 1, v1 . . . vi applicable
I,
{
IJK[vj ], j
IJv1 . . . vi K[vj ] =
.
(5)
I[vj ],
j>i
Together topological ordering V order preservation vi +1 along
respect , Eq. 5 implies vi+1 applicable IJv1 . . . vi K,
IJv1

{
IJK[vj ],
j =i+1
.
. . . vi KJvi+1 K[vj ] =
IJv1 . . . vi K[vj ], otherwise

(6)

Putting Eqs. 5 6 together proves induction hypothesis, = n, Eq. 5
boils
IJ K = IJv1 . . . vn K = IJK.

Denition 4 Let innite set FDR tasks. say tasks Munfoldable state variables M-unfoldable.
Theorem 5 Let innite set M-unfoldable FDR tasks directed acyclic causal
graphs. tree-width node in-degree causal graphs bounded
constant, optimal relaxed planning polynomial-time.
Proof: Similarly proof Theorem 3, proof Theorem 5 based planningto-COP compilation. Given FDR task = V, A, I, G, cost, compile monotonic
relaxation + constraint optimization problem COP+ = (X , ) variables X ,
functions , global objective minimize (X ) that, + unsolvable,
assignments X evaluate objective function , otherwise,
optimum objective obtained assignments X correspond
optimal plans + . specic construction COPs relies property
monotonic relaxations DAG-structured FDR tasks expressed Lemma 1.
798

fiThe Complexity Optimal Monotonic Planning

Given FDR task = V, A, I, G, cost theorem, COP+ = (X , ) specied
follows. state variable v V ,
X contains variable xv domain D(xv ) = (v), is, eective domain
v + ,
contains non-negative, extended real-valued function v v immediate
ancestors causal graph, is, {xv } {xw | w pred(v)}.
Assuming arbitrary xed ordering {w1 , . . . , wk } vs immediate ancestors pred(v),
pred (pred(v)) = (w1 ) (wk ), let DTG(v, + |pred ) denote
restriction DTG(v, + ) edges supported pred : edge marked
action remains DTG(v, + |pred ) if, w V(pre(a))\{v}, pre(a)[w]
pred [w]. Given that, eective value (v) pred (pred(v)),
v (, pred ) = reachable I[v] DTG(v, + |pred ), G[v] specied
yet G[v] . Otherwise, v (, pred ) equals cost cheapest path I[v]
DTG(v, + |pred ). follows, action sequence inducing cheapest path
denoted (|pred ).
properties COP+ constructed problems scope Theorem 5 follows.
(1) constraint network COP+ corresponds (the undirected graph induced by)
moral graph causal graph CG . Since in-degree tree-width CG
bounded constant, tree-width COP constraint network.
mentioned before, given graph G, tree decomposition graph G width
c tw(G) low constant c found time polynomial size G
exponential tw(G). Hence, since COP = O(1), COP+ solved time
polynomial size explicit representation.
(2) Since M-unfoldable, domain size COP variable O(poly(||||)). Together (1), implies COP+ solved time O(poly(||||)).
(3) denitions monotonic relaxation domain transition graphs, explicit
description DTG(v, + ) M-unfoldable FDR task polynomial ||||.
Hence, construction functional components , thus entire COP+ ,
done time O(poly(||||)).
(4) construction COP+ Lemma 1, topological ordering {v1 , . . . , vn }
V , every complete assignment COP variables X

v ([v], [pred(v)]) = =
vV

induces relaxed plan
([v1 ]|[pred(v1 )]) . . . ([vn ]|[pred(vn )])
cost , vice versa. Thus, + solvable, then, given assignment
X minimization objective COP+ obtained, derive
(in O(poly(||||)) time) optimal relaxed plan . Otherwise, + unsolvable,
assignments X evaluate objective function .
799

fiDomshlak & Nazarenko

ONML
HIJK
v1


HIJK
ONML
v2


HIJK
ONML
v3

/ d1,2

d1,1
d1,1

d2,1

d1,2
d1,3

d2,2
d2,3

ONML
HIJK
HIJK
HIJK
/ ONML
/ ONML

d1,1 Qo Q
d1,3

BB QQQ | 1,2 BB
mm|


BB Q|Q|Q
B
|


BB|| QQQmmmmBBB|||
|B
mmQQQQ ||BB
|| BBmmm
Q| B
}|||mmmmm BB! }||| QQQQQBB!
mv
(
HIJK
ONML
HIJK
HIJK
/ ONML

/ ONML
d2,1 Qo Q

d2,3

BB QQQ | 2,2 BB


|

BB Q|Q|Q
B
|


BB|| QQQmmmmBBB|||
|B
mmQQQQ ||BB
|| BBmmm
Q| B
}|||mmmmm BB! }||| QQQQQBB!
mv
(
HIJK
ONML
HIJK
HIJK
/ ONML
/ ONML
d3,1
d3,2
d3,3

d1,1

#

/ d2,2
;

d2,1

d3,1

/ d1,3

d1,2
d1,3

#
/ d2,3
;

d2,1

#

/ d3,2
;

d2,2
d2,3

#
/ d3,3
;

(a)

(b)

Figure 1: Illustration example used discussion Theorem 5.
nalizes proof Theorem 5, Corollary 4 generalizes digraphs
almost DAGs.

Note Theorem 5 provides yet another example exploiting value grouping induced FDR representation planning tasks. Consider planning task family
(n) = V, A, I, G, cost V = {v1 , . . . , vn }; 1 n, D(vi ) = {di,1 , . . . , di,n };
I[vi ] = di,1 ; G[vi ] = di,n ; actions

A=
{ai,k,j = {vi di,k , vi1 di1,j }, {vi di,k+1 }}.
|
{z
} |
{z
}
1i,jn
1kn1

pre

e

Figure 1a illustrates causal graph domain transition graphs task (3) .
causal graphs (n) form directed chains, thus tree-width node
in-degree causal graphs (n) equal 1. Likewise, eective domain (vi )
vi +
(n) size n, thus tractability optimal relaxed planning (n)
directly covered Theorem 5. contrast, variable value di,j represented
separate propositional variable, inducing causal graph Figure 1b,
tree-width node in-degree family induced causal graphs order
n, fact, causal graph even acyclic. Therefore, Theorem 5 longer
directly applicable.
Corollary 4 Let innite set M-unfoldable FDR tasks. size strongly
connected components, tree-width, node in-degree causal graphs
bounded constant, optimal relaxed planning polynomial-time.
Proof: FDR task causal graph whose strongly connected components (SCCs)
size k compiled equivalent FDR task directed acyclic
causal graph merging variables SCC single variable (Seipp & Helmert,
2011). compilation done time polynomial |||| exponential k.
causal graph CGm obtained causal graph CG contracting nodes
SCC. Since node contraction decrease tree-width, tw(CGm )
800

fiThe Complexity Optimal Monotonic Planning

tw(CG ), thus tw(CGm ) = O(1). Likewise, maximal node in-degree CG
c, maximal node in-degree CGm ck, thus also O(1). Finally,
domain variable u obtained merging SCC {v1u , . . . , vku },
k k, corresponds cross-product domains SCCs variables.
easy verify Denition 2 (u) (v1u ) (vku ), thus, together
k = O(1), M-unfoldability v1u , . . . , vku implies |D (u)| = O(poly(||||)), tting3
Denition 3.

Returning statement Theorem 5, comments extensions beyond
Corollary 4 place. First, note Theorem 5 generalize Theorem 4
fork-structured FDR tasks latter allows general, M-unfoldable,
leaf variables. However, easy see Theorem 5 stratied allow
generalization. Since variable depends leaf v, care v
achieving G[v]. Thus, optimal relaxed plan , v induces simple path,
general arborescence, DTG(v, ). Hence, using binary-valued (G[v] achieved: yes/no)
COP variables xv DAG leaf v, specifying respective functions v using
procedure proof Theorem 4, scope Theorem 5 extended generalize
Theorem 4.
Second, case DAG-structured causal graphs, Denition 2 eective domains,
notion M-unfoldability based, overly conservative. Instead deriving
eective domains variables isolation, derive topological order
causal graph, given already derived eective domains immediate ancestors.
specic domains, substantially extend scope M-unfoldability FDR tasks
DAG causal graphs.
Finally, Theorem 5 requires tree-width, also in-degree causal
graph bounded constant. extra condition, latter sucient,
necessary. Below, notion prevail decomposability, list two local
properties state variables guarantee polynomial-time optimal relaxed planning
arbitrary acyclic causal graphs xed tree-width. likely
helpful properties exist, thus boundaries prevail decomposability
extended. Nicely, optimal relaxed planning remain polynomial-time even dierent
state variables satisfy dierent properties, even state variables
prevail decomposable, xed in-degree.
Denition 5 Let innite set FDR tasks, property state variables
that, task , partitions state variables satisfy
(referred -variables), satisfy . say -variables
prevail decomposable if, every task every -variable v , either
(i) set PRv = {pre(a)[pred(v)] | Av } preconditions actions Av variables
v size O(log(||||)),
3. Note possible (u) (v1u ) (vku ). instance, V = {x, y}, D(x) = D(y) =
{0, 1}, = {x 0, 0}, G = {x 1, 1}, = {{x 0, 1}, {x 1}, {y 0, x
1}, {y 1}}, (x) = {{0}, {0, 1}}, (y) = {{0}, {0, 1}}, (xy) = {{x 0, 0}}.
fact, example easily extended merged variable xy M-unfoldable,
x not.

801

fiDomshlak & Nazarenko

(ii) set ARBv arborescence subgraphs DTG(v, ) rooted I[v], covering G[v]
v V(G), size O(log(||||)).
say tasks prevail decomposable state variables
prevail decomposable.
Note prevail decomposability type (i) tangential notion M-unfoldability:
neither former imply latter, way around. contrast, prevail
decomposability type (ii) direct stratication M-unfoldability latter
considers compacted version DTG (v, ) DTG(v, ), furthermore, allows
polynomial (rather logarithmic) bound number arborescence subgraphs.
Theorem 6 Let innite set M-unfoldable, prevail decomposable FDR tasks
directed acyclic causal graphs. tree-width causal graphs bounded
constant, optimal relaxed planning polynomial-time.
Proof: well, proof Theorem 6 follows planning-to-COP compilation
methodology. However, compilation prevail decomposability must dier
one proof Theorem 5 since longer rely xed in-degree causal
graphs derive xed tree-width constraint networks xed tree-width
causal graphs. ease presentation, rst specify compilation assuming
state variables satisfy specic condition (i) Denition 5. extend
specication cover alternative condition (ii) Denition 5 well.
construction need establish certain graph-theoretic formalism
respective notation. Let G = (V, E) graph, let N : V 2V node neighborhood function G, is, N (v) = {u | {v, u} E}. splitting v V
support N (v) transforms G adding new vertex v edge {v, v }, and,
u S, removing edge {v, u} adding edge {v , u}. Informally, splitting seen
(non-unique) reverse process edge contraction, nodes added G splittings
called stretch nodes. example, Figure 2b depicts graph obtained graph
Figure 2a splitting node v support {x, y, u} N (v) = {x, y, u, w},
adding stretch node v(1) .
graph G expansion G G transformed G sequence splittings.
example, Figure 2c depicts graph obtained graph Figure 2a rst
splitting node v support {x, y} N (v) = {x, y, u, w}, splitting v
support {u, w} N (v) = {v(1) , u, w}. specically, G = (V , E )
expansion G = (V, E) exist functions f : V V g : E E

(a) v V , subgraph G induced f 1 (v) = {v V | f (v ) = v} tree,
(b) {v, u} E, g({v, u}) = {v , u } f (v ) = v f (u ) = u.
tree subgraph (v) G induced f 1 (v) called stretch tree v.
bijective correspondence leaves (v) neighbors N (v) v via
function g: {v, u} E, exactly one edge E , g({v, u}), directly
connects (v) (u). words, f induces partition V ,
part stretch tree (v) v V , g maps edges G edges
802

fiThe Complexity Optimal Monotonic Planning

x-



-
--
-
v/
///
//



u

w

(a)

x






v(1)





x ==

==

v(1)
v

x(1)

v

w

u(1)

@@
@

u

(b)

v(1)

y(1)

v

v(2)
u



x

w

(c)

v(2)

u

w(1)
w

(d)

Figure 2: Node splitting graph expansions.
G connect parts partition. Finally, node degree G
bounded 3, G called sub-cubic. example, expansion Figure 2c
sub-cubic, expansion Figure 2b not.
terminology mostly adopted Markov Shi (2011). addition,
call expansion G G fully separating stretch trees G connected
stretch nodes, original nodes, G. is, G = (V , E ) fully
separating expansion G = (V, E) exists function : V V that,
v V , holds (v) f 1 (v) and, edge { (v), v } E , v f 1 (v).
example, expansion Figure 2c fully separating, expansion Figure 2d
is.4
Given notion graph expansion, problem solved
graph G, eciency solving problem depends badly, possibly exponentially,
node degree G, try reformulate problem sub-cubic expansion
G G. However, eciency problem question also depends badly
graphs tree-width, tree-width G close possible G.
(The tree-width G cannot smaller tree-width G G G
minor.) numerous ecient schemes sub-cubic graph expansion,
create expansions arbitrarily larger tree-width expandees.
Recently, however, Markov Shi (2011) showed negative side-eect always
eliminated, sometimes even eciently.
Theorem 3.1 Markov Shi (2011) states main result: polynomialtime algorithm that, given graph G tree decomposition width w, computes
sub-cubic expansion G G tw(G ) w + 1. particular, result implies
graph G admits sub-cubic expansion whose tree-width tw(G) + 1,
expansion constructed eciently arbitrary graph families xed
4. Without eective loss generality, one assume (v) = v, is, nodes V never
mapped stretch nodes, mirrors V . However, decided stick
explicit use function avoid confusion nodes V identically named nodes
V .

803

fiDomshlak & Nazarenko

tree-width.5 Moreover, straightforward verify expansion transformed
linear time fully separating expansion, without increasing tree-width node
degrees. Therefore, Theorem 3.1 Markov Shi (2011) holds even request fully
separating sub-cubic expansions.6
COP compilation exploits tree-width friendly expansions causal graphs.
Since focus Theorem 5 digraph families C tree-width C
bounded constant, Theorem 3.1 Markov Shi (2011), digraph G C
eciently associated fully separating sub-cubic expansion G tree-width
tw(G) + 1. Note, however, construction G ignores orientation arcs
G: G digraph, G undirected graph, construction based
tree decomposition undirected graph induced G. Since COP compilation
depend direction arcs causal graph, restore G
relevant bits information G.
rst give auxiliary notation.
Given fully separating expansion G (the undirected graph induced by) digraph
G = (V, E), consider stretch trees (v) rooted respective nodes (v),
Tv (v) denote subtree (v) rooted v (v).
Recalling graphs G interest DAGs, leaves (v)
bijectively associated neighbors N (v) v G, let N (v), N (v)
N (v) partition vs neighbors G immediate ancestors immediate
descendants v, respectively.
(v) denote respective neighbors
Nvin (v) N (v) Nvout
(v) N
v associated leaves stretch subtree Tv (v). is,
(v) u (u),
u Nvin (v) Nvout
(v) if, leaf v
v





G contains edge {v , u } (i.e., g({v, u}) = {v , u }).

proceed specifying COP compilation FDR tasks Theorem 6.
Given task = V, A, I, G, cost, let G = (V , E ) fully separating, sub-cubic
expansion G causal graph CG tree-width tw(CG ) + 1. respective
constraint optimization problem COP+ = (X , ) specied follows.
v V , X contains variable xv schematically associated root
(v) (v), variable xv /v stretch-tree node v (v) \ { (v)}.
domain variable xv
{
{ | (v), G[v] }, v V(G)
D(xv ) =
.
(7)
(v),
otherwise
domain variable xv /v
D(xv /v ) = {0, 1}mv D(xv ),

(8)

5. determining optimal tree decomposition graph NP-hard, done polynomial time
graph families xed tree-width (Bodlaender, 1996).
6. Requiring expansions fully separating luxury need: relying property
simplies compilation scheme described next, scheme also modied require
full separation.

804

fiThe Complexity Optimal Monotonic Planning

mv = |PRv |. is, D(xv /v ) set pairs , , {0, 1}mv
(v). {0, 1}mv , DTG(v, + |) denote restriction DTG(v, + )
edges supported : Assuming arbitrarily xed numbering elements
PRv = {pr1 , . . . , prmv }, edge marked action pre(a)[pred(v)] = pri
PRv remains DTG(v, + |) [i] = 1. (v), c(|) denote
cost cheapest path I[v] DTG(v, + |); case unreachability,
c(|) = .
Similarly way node causal graphs expansion G associated
COP variable, also associated non-negative, extended real-valued function.
state variable v V :
(I) stretch tree root (v) associated function v . scope v
Q(v ) = {xv } {xv /v | v N ( (v))},


N : V 2V node neighborhood function G . Note |N ( (v))|
3 G sub-cubic. D(xv ) assignment = {v , v }v N ( (v))
Q(v ) \ {xv },
{
c(|H() ), v N ( (v)) : v =
v (, ) =
,
(9)
,
otherwise
H() Hadamard, entrywise, product indicator vectors
{v }v N ( (v)) .
(II) leaf stretch node v (v) associated 0/ indicator function v /v ,
scope
Q(v /v ) = {xv /v , xu /u },
u leaf node (u) g({v, u}) = {v , u }. orientation
arcs within causal graph CG matters. Specically, {v, u} represents
causal graph arc u v , v /v zeroes assignments (v , , u , u )

vector v enables preconditions PRv are, passively
actively, supported value u u,
vector u enables preconditions PRu , since v condition
u-changing actions DAG-structured planning task .
is, v , v D(xv /v ) u , u D(xu /u ),


0,

u = 1
v /v (v , v , u , u ) =
1 mv : (v [i] = 0) (u V(pri ) pri [u] u ) .


, otherwise
(10)
Note value v /v independent v component v , v , u , u .
805

fiDomshlak & Nazarenko

Otherwise, {v, u} represents causal graph arc7 v u, conversely,


0, v = 1




v /v (v , v , u , u ) =
1 mu : (u [i] = 0) (v V(pri ) pri [v] v ) .


, otherwise
(11)
(III) internal stretch node v (v) also associated 0/ indicator function
v /v , scope comprises variable xv /v , together variables xv /v
correspond immediate descendants v Tv (v). is,
Q(v /v ) = {xv /v } {xv /v | v N (v ) Tv (v)}.
v , v D(xv /v ) assignment = {v , v } Q(v /v ) \
{xv /v }, v /v zeroes (v , v , ) vector v enables
preconditions PRv (passively actively) supported
immediate ancestors u Nvin (v) via values ancestors commit
respective stretch-tree root COP variables xu . is,
{
0, v = H() v N (v ) Tv (v) : v = v
v /v (v , v , ) =
. (12)
, otherwise
words, starting Eqs. 10 11, support provided Nvin (v) v
communicated v indicator vectors v , aggregated/summarized
Eq. 12 Hadamard vector product H() .
Complexity-wise, properties COP+ constructed follows.
(1) constraint network COP+ obtained expansion G replacing
graph
subgraph G induced nodes Q( ) clique Q( ). Let GQ
nodes Q( ), x X , edges {Q( ), Q( )} (only) pairs ,
isomorphic
Q( ) Q( ) = . construction COP functions , GQ

G . Likewise, since |Q( ) Q( )| 1 pairs functions , , tree-width
) max |Q( )|. Together implies
COP constraint network tw(GQ



COP tw(GQ
)max |Q( )| = tw(G )max |Q( )| 4tw(G ) 4(tw(CG )+1) = O(1).




Hence, since nding constant-factor approximation graphs tree-width polynomial size graph exponential tree-width, COP+
solved time polynomial size representation.
(2) M-unfoldability Eq. 7, v V , domain size COP variable xv
O(poly(||||)). domain stretch node variable xv /v cross-product
two sets. size second set Eq. 8 O(poly(||||)) domain
size respective variable xv . size rst set Eq. 8 2|PRv | and,
Denition 5, 2|PRv | = 2O(log(||||)) = O(poly(||||)). Together (1), implies
COP+ solved time O(poly(||||)).
7. Since Theorem 6 devoted directed acyclic causal graphs, address case
CG contains (v, u) (u, v).

806

fiThe Complexity Optimal Monotonic Planning

(3) denition monotonic relaxation denition domain transition graphs,
explicit description DTG(v, + ) M-unfoldable FDR task polynomial
||||. Hence, construction functional components Eqs. 9-12, thus
entire COP+ , done time O(poly(||||)).

proceed prove correctness COP+ = (X , ). is, prove
+ unsolvable, assignments X evaluate objective function
(X ) , otherwise, objective minimized assignments
X correspond optimal plans + .
First, given assignment X () < , show induces
valid plan + cost (). Eqs. 9 12, () < implies that,
v V xv /v stretch tree (v), [xv /v ] {, [xv ]}. is,
relaxation value v assigned xv consistently propagated nodes
(v), particular, leaves.
Let leaf node xv /v (v) connect (v) (u) causal graph neighbor
u N (v), let [xv /v ] = v , [xv ]. u N (v), Eq. 10, v /v () =
implies v encodes preconditions PRv disabled
relaxation value [xu ] u. Otherwise, u N (v), then, DAG structure CG ,
u nothing preconditions actions aecting v + , Eq. 11, v /v () =
implies v = 1 trivially enables preconditions PRv .
Given that, (leaf internal) stretch node (v), let [xv /v ] = v , [xv ].
conjunctive structure preconditions FDR Hadamard vector product
Eq. 12, v /v () = implies v encodes preconditions PRv
notdisabled values [xu ] u Nvin (v). Finally, denition graph
expansion, v N ( (v)) Nvin (v) = N (v). Thus, Eq. 11, v () = implies [xv ]
reachable I[v] properly restricted domain transition graph DTG(v, + |[Xv ])
Xv = {xu | u pred(v)}, v () equals cost cheapest
path. rest follows DAG structure CG Lemma 1. proof
opposite direction straightforward construction COP+ serialization
Lemma 1.


nal note return denition prevail decomposability, specically,
second sucient condition that, state variables v V , set ARBv arborescence
sugraphs DTG(v, ) rooted I[v], covering G[v] v V(G), size O(log(||||)).
condition addressed COP construction far, switching
rst second sucient condition prevail decomposability requires
semantics indicator vectors changed: Instead encoding support
pred(v) provide individual preconditions actions Av , encode support
pred(v) provide entire arborescences ARBv DTG(v, ). Since condition
requires |ARBv | = O(log(||||)), support encoded reasoned eciently.
Note choice two conditions made variable-by-variable basis,
thus two conditions mutually exclusive complementary.
807

fiDomshlak & Nazarenko


causal graph
extra condition
xed size
= O(1)
|D(v)| = O(1)
= O(1) & DAG |D(v)| = O(1)
= O(1) & DAG in-degree = O(1)
= O(1) & DAG

FDR
P?
Yes





MFDR
P?

Yes
Yes
Yes, M-unfoldable
Yes, M-unfoldable
prevail decomposable

Th.
Th.
Th.
Th.
Th.

2
3
3
5
6

Table 1: summary main results optimal MFDR planning, contrasted
previously established complexity corresponding fragments optimal FDR
planning. table, fragment FDR/MFDR planning, characterized
terms causal graph tree-width , causal graph in-degree, upper
bound |D(v)| size variable domains. M-unfoldability prevail
decomposability two properties MFDR tasks introduced
exploited work.

6. Summary Future Work
took step towards ne-grained classication worst-case time complexity optimal
monotonic planning, focus gets harder gets easier
switching optimal planning optimal relaxed planning, context nite-domain
planning task representations. Along way, established negative positive
results complexity wide fragments problem, negative results
emphasizing role structure state variable domains, positive results
emphasizing role causal graph topology. Table 1 lists main results optimal
monotonic planning, contrasted complexity corresponding fragments
optimal FDR planning. key conclusions follows.
1. Optimal planning monotonic relaxations hard even restricted simple
causal graph structures, complexity stems size state
variable domains.
2. Restricted planning tasks constant-bounded state variable domains, problem becomes solvable time exponential tree-width causal graph,
known much even non-optimal regular planning.
3. tree-width digraphs independent edge directions, exploiting
directed structure causal graph together tree-width allows
computational tractability expanded beyond xed-size state variable domains.
latter conclusion opens interesting venue investigation.
addressed directed acyclic causal graphs, scope tractability perhaps expanded exploiting existing directed notions graph width (Johnson, Robertson,
Seymour, & Thomas, 2001; Hunter & Kreutzer, 2008; Berwanger et al., 2012). might
808

fiThe Complexity Optimal Monotonic Planning

especially appealing tree-width standard planning benchmarks natural FDR encodings appear xed across respective families
tasks.
Another interesting direction would examine results techniques introduced wider context: recently introduced framework red-black
relaxations (Katz et al., 2013b). red-black (RB) planning, variables partitioned
two sets: black set adopts regular, value switching semantics FDR,
red set adopts monotonic, value accumulating semantics MFDR. context
satiscing planning, complexity analysis RB planning complexity lens
causal graph topology already led advances practice heuristic-search
planning (Katz et al., 2013a; Katz & Homann, 2013). take similar step optimal
planning, admissible heuristics based RB relaxations must devised. That,
turn, calls identifying tractable fragments optimal RB planning. cautiously
optimistic results techniques presented paper found valuable context RB planning well. instance, positive result Theorem 4
fork-structured MFDR tasks straightforwardly extended RB tasks
root variables taking monotonic semantics leaves keeping regular, FDR
semantics. Similarly, positive result Theorem 5 DAG-structured MFDR tasks
straightforwardly extended RB tasks black-painted leaf variables. interesting
question respect whether computational tractability optimal RB planning
extended causal graphs internal nodes get keep original FDR
semantics.

Acknowledgments
work partly supported Israel Science Foundation (ISF) grant 1045/12,
EOARD grant FA8655-12-1-2096.

References
Amir, E. (2010). Approximation algorithms treewidth. Algorithmica, 56 (4), 448479.
Arnborg, S., Cornell, D. G., & Proskurowski, A. (1987). Complexity nding embeddings
k-tree. SIAM Journal Algebraic Discrete Methods, 8, 277284.
Backstrom, C., & Klein, I. (1991). Planning polynomial time: SAS-PUBS class.
Computational Intelligence, 7 (3), 181197.
Backstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. Computational
Intelligence, 11 (4), 625655.
Becker, A., & Geiger, D. (1996). suciently fast algorithm nding close optimal
junction trees. Proceedings 12th Conference Uncertainty Articial
Intelligence (UAI), pp. 8189.
Berwanger, D., Dawar, A., Hunter, P., Kreutzer, S., & Obdzralek, J. (2012). DAG-width
directed graphs. Journal Combinatorial Theory, Series B, 102 (4), 900923.
Betz, C., & Helmert, M. (2009). Planning h+ theory practice. Proceedings
32nd Annual German Conference Articial Intelligence (KI), pp. 916.
809

fiDomshlak & Nazarenko

Bodlaender, H. L. (1996). linear-time algorithm nding tree-decompositions small
treewidth. SIAM Journal Computing, 25 (6), 13051317.
Bonet, B., & Gener, H. (2001). Planning heuristic search. Articial Intelligence, 129 (1
2), 533.
Bonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets.
Proceedings 19th European Conference Articial Intelligence, pp. 329334,
Lisbon, Portugal.
Brafman, R. I., & Domshlak, C. (2006). Factored planning: How, when, not.
Proceedings 18th National Conference Articial Intelligence (AAAI), pp.
809814, Boston, MA.
Brafman, R. I., & Domshlak, C. (2013). complexity planning agent teams
implications single agent planning. Articial Intelligence, 198, 5271.
Bylander, T. (1994). computational complexity propositional STRIPS planning.
Articial Intelligence, 69 (1-2), 165204.
Cai, D., Homann, J., & Helmert, M. (2013). Enhancing context-enhanced additive
heuristic precedence constraints. Proceedings 19th International Conference Automated Planning Scheduling (ICAPS), pp. 5057.
Chen, H., & Gimenez, O. (2010). Causal graphs structurally restricted planning. Journal Computer System Sciences, 76 (7), 579592.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
Domshlak, C., & Dinitz, Y. (2001). Multi-agent o-line coordination: Structure complexity. Proceedings Sixth European Conference Planning (ECP), pp. 277288.
Downey, R. G., & Fellows, M. R. (1999). Parameterized Complexity. Springer-Verlag, New
York.
Edelkamp, S. (2001). Planning pattern databases. Proceedings European
Conference Planning (ECP), pp. 1324.
Fabre, E., Jezequel, L., Haslum, P., & Thiebaux, S. (2010). Cost-optimal factored planning:
Promises pitfalls. Proceedings International Conference Automated
Planning Scheduling (ICAPS), pp. 6572.
Fikes, R. E., & Nilsson, N. (1971). STRIPS: new approach application theorem
proving problem solving. Articial Intelligence, 2, 189208.
Flum, J., & Grohe, M. (2006). Parameterized Complexity Theory. Springer-Verlag.
Fox, M., & Long, D. (2001). Stan4: hybrid planning strategy based subproblem
abstraction. AI Magazine, 22 (3), 8184.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal
planning problems. Journal Articial Intelligence Research, 20, 61124.
Gimenez, O., & Jonsson, A. (2009a). inuence k-dependence complexity
planning. Proceedings 19th International Conference Automated Planning
Scheduling (ICAPS), pp. 138145.
810

fiThe Complexity Optimal Monotonic Planning

Gimenez, O., & Jonsson, A. (2009b). Planning chain causal graphs variables
domains size 5 NP-hard. Journal Articial Intelligence Research, 34, 675706.
Halin, R. (1976). s-functions graphs. Journal Geometry, 8, 171186.
Helmert, M. (2004). planning heuristic based causal graph analysis. Proceedings
Fourteenth International Conference Automated Planning Scheduling
(ICAPS), pp. 161170.
Helmert, M. (2006). Fast Downward planning system. Journal Articial Intelligence
Research, 26, 191246.
Helmert, M. (2009). Concise nite-domain representations PDDL planning tasks. Articial Intelligence, 173, 503535.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whats
dierence anyway?. Proceedings 19th International Conference Automated Planning Scheduling (ICAPS), pp. 162169.
Helmert, M., & Gener, H. (2008). Unifying causal graph additive heuristics. Proceedings 18th International Conference Automated Planning Scheduling
(ICAPS), pp. 140147.
Helmert, M., Haslum, P., & Homann, J. (2007). Flexible abstraction heuristics optimal
sequential planning. Proceedings 17th International Conference Automated
Planning Scheduling (ICAPS), pp. 200207.
Helmert, M., & Mattmuller, R. (2007). Accuracy admissible heuristic functions selected planning domains. Proceedings 23rd AAAI Conference Articial
Intelligence, pp. 938943.
Homann, J. (2005). ignoring delete lists works: Local search topology planning
benchmarks. Journal Articial Intelligence Research, 24, 685758.
Homann, J. (2011). Analyzing search topology without running search: connection causal graphs h+ . Journal Articial Intelligence Research, 41,
155229.
Homann, J., & Nebel, B. (2001). FF planning system: Fast plan generation
heuristic search. Journal Articial Intelligence Research, 14, 253302.
Hunter, P., & Kreutzer, S. (2008). Digraph measures: Kelly decompositions, games,
orderings. Theoretical Computer Science, 399 (3), 206219.
Johnson, T., Robertson, N., Seymour, P. D., & Thomas, R. (2001). Directed tree-width.
Journal Combinatorial Theory, Series B, 82 (1), 138154.
Jonsson, P., & Backstrom, C. (1998). State-variable planning structural restrictions:
Algorithms complexity. Articial Intelligence, 100 (12), 125176.
Karp, R. (1972). Reducibility among combinatorial problems. Complexity Computer
Computations, pp. 85103. Plenum Press, New York.
Katz, M., & Domshlak, C. (2008). New islands tractability cost-optimal planning.
Journal Articial Intelligence Research, 32, 203288.
811

fiDomshlak & Nazarenko

Katz, M., & Domshlak, C. (2010). Implicit abstraction heuristics. Journal Articial
Intelligence Research, 39, 51126.
Katz, M., & Homann, J. (2013). Red-black relaxed plan heuristics reloaded. Proceedings
6th Annual Symposium Combinatorial Search (SOCS), pp. 105113.
Katz, M., Homann, J., & Domshlak, C. (2013a). Red-black relaxed plan heuristics.
Proceedings 27th AAAI Conference Articial Intelligence (AAAI), pp. 489
495.
Katz, M., Homann, J., & Domshlak, C. (2013b). said need relax variables?. Proceedings 23rd International Conference Automated Planning
Scheduling (ICAPS), pp. 126134.
Katz, M., & Keyder, E. (2012). Structural patterns beyond forks: Extending complexity
boundaries classical planning. Proceedings 26th AAAI Conference
Articial Intelligence (AAAI), pp. 17791785.
Keyder, E., & Gener, H. (2008a). Heuristics planning action costs revisited.
Proceedings 18th European Conference Articial Intelligence (ECAI), pp.
588592.
Keyder, E., & Gener, H. (2008b). Heuristics planning action costs revisited.
Proceedings 18th European Conference Articial Intelligence, pp. 588592.
Markov, I. L., & Shi, Y. (2011). Constant-degree graph expansions preserve treewidth.
Algorithmica, 59, 461470.
McDermott, D. V. (1999). Using regression-match graphs control search planning.
Articial Intelligence, 109 (1-2), 111159.
Nissim, R., & Brafman, R. I. (2012). Multi-agent parallel distributed systems.
Proceedings 11th International Conference Autonomous Agents Multiagent Systems (AAMAS), pp. 12651266.
Nissim, R., Brafman, R. I., & Domshlak, C. (2010). general, fully distributed multiagent planning algorithm. Proceedings 9th International Conference
Autonomous Agents Multiagent Systems (AAMAS), pp. 13231330.
Pednault, E. (1989). ADL: Exploring middle ground STRIPS situation calculus. Proceedings 1st International Conference Principles
Knowledge Representation Reasoning, pp. 324331.
Robertson, N., & Seymour, P. D. (1984). Graph minors III: Planar tree-width. Journal
Combinatorial Theory, 36, 4963.
Robertson, N., & Seymour, P. D. (1991). Graph minors X: Obstructions tree decomposition. Journal Combinatorial Theory, Series B, 52 (2), 153190.
Seipp, J., & Helmert, M. (2011). Fluent merging classical planning problems. Proceedings ICAPS-2011 Workshop Knowledge Engineering Planning
Scheduling (KEPS), pp. 4753.

812

fiJournal Artificial Intelligence Research 48 (2013) 923-951

Submitted 7/13; published 12/13

Smooth Transition Powerlessness Absolute Power
Elchanan Mossel

mossel@stat.berkeley.edu

University California, Berkeley
Berkeley, CA 94720 USA
Weizmann Institute Science
Rehovot, Israel

Ariel D. Procaccia

arielpro@cs.cmu.edu

Carnegie Mellon University
Pittsburgh, PA 15213 USA

Miklos Z. Racz

racz@stat.berkeley.edu

University California, Berkeley
Berkeley, CA 94720 USA

Abstract
study phase transition coalitional manipulation problem generalized
scoring rules. Previously shown that,
conditions distribution
votes, number manipulators ( n), n number voters,
probability random profile manipulable coalition goes zero

number voters goes infinity, whereas number manipulators ( n),
probability random profile manipulable
goes one. consider

critical window, coalition size c n, show c goes zero
infinity, limiting probability random profile manipulable goes zero
one smooth fashion, i.e., smooth phase transition two regimes.
result analytically validates recent empirical results, suggests deciding
coalitional manipulation problem may limited computational hardness practice.

1. Introduction
Finding good voting systems satisfy natural requirements one main
goals social choice theory. problem increasingly relevant area artificial intelligence computer science broadly, virtual elections
established tool preference aggregation (see, e.g., Caragiannis & Procaccia, 2011).
naturally desirable property voting system strategyproofness (a.k.a. nonmanipulability): voter benefit voting strategically, i.e., voting according
true preferences. However, Gibbard (1973) Satterthwaite (1975) showed
reasonable voting system strategyproof. stating result, let us specify
problem formally.
consider n voters electing single winner among candidates. voters specify
opinion ranking candidates, winner determined according
n [m] voters rankings,
predefined social choice function (SCF) f : Sm

denotes set possible total orderings candidates. call collection
rankings voters ranking profile. say SCF manipulable exists
c
2013
AI Access Foundation. rights reserved.

fiMossel, Procaccia, & Racz

ranking profile voter achieve desirable outcome election according
true preferences voting way reflect true preferences.
Gibbard-Satterthwaite theorem states SCF dictatorship
(i.e., function single voter), allows least three candidates
elected, manipulable. contributed realization unlikely expect
truthfulness voting. Consequently, many branches research devoted
understanding extent manipulability voting systems, finding ways
circumventing negative results.
One approach, introduced Bartholdi, Tovey, Trick (1989), suggests computational
complexity barrier manipulation: SCF may manipulable practice
hard voter compute manipulative vote. significant body work focuses
worst-case complexity manipulation (see survey Faliszewski Procaccia, 2010).
interested specifically coalitional manipulation problem, group
voters change votes unison, goal making given candidate
win. Various variations problem known N P-hard many
common SCFs (Conitzer, Sandholm, & Lang, 2007; Xia, Zuckerman, Procaccia, Conitzer,
& Rosenschein, 2009; Betzler, Niedermeier, & Woeginger, 2011).
Crucially, line work focuses worst-case complexity. worst-case hardness
manipulation desirable property SCF have, tell us much
typical instances problemis usually easy hard manipulate? recent line
research average-case manipulability questioning validity worstcase complexity results. goal alternative line work show
reasonable voting rules computationally hard manipulate average.
Specifically, goal rule following informal statement: good voting
rules hard manipulate average sufficiently rich distribution
votes.
Taking point view, showing easiness manipulation restricted class
distributionssuch i.i.d. votes even uniform votes (the impartial culture assumption)
interesting, even necessarily capture possible real-world elections.
Specifically, show manipulation easy distributions, averagecase hardness result would necessarily make unnatural technical assumptions
avoid distributions. Studying restricted distributions votes indeed
exactly recent papers done.
coalitional manipulation problem, Procaccia Rosenschein (2007a) first suggested trivial determine whether manipulation possible coalitional
manipulation instances, typical-case computational point view; one make
highly informed guess purely based number manipulators. Specifically, studied setting distribution votes (which satisfies conditions),
concentrated family SCFs known positional scoring rules. showed

size coalition ( n), probability converging 1 n , coalition
powerless, i.e., cannot change outcome election. contrast, size

coalition ( n) (and (n)), probability converging 1 n , coalition all-powerful, i.e., elect candidate. Later Xia Conitzer (2008b) proved
analogous result so-called generalized scoring rules, family contains almost
common voting rules. See also related work Peleg (1979), Slinko (2004), Pritchard
924

fiA Smooth Transition Powerlessness Absolute Power

Slinko (2006), Pritchard Wilson (2009). discuss additional related work
Section 1.2.
primary interest paper understand critical window papers

leave open, size coalition ( n). Specifically, interested
phase transition probability coalitional manipulation, size coalition

c n c varies zero infinity, i.e., transition powerlessness absolute
power.
past decades much research connection phase
transitions computationally hard problems (see, e.g., Fu & Anderson, 1986; Cheeseman,
Kanefsky, & Taylor, 1991; Achlioptas, Naor, & Peres, 2005). particular, often
case computationally hardest problems found critical values
sharp phase transition (see, e.g., Gomes & Walsh, 2006, overview).
hand, smooth phase transitions often found connection computationally easy
(polynomial) problems, 2-coloring (Achlioptas, 1999) 1-in-2 SAT (Walsh, 2002).
Thus understanding phase transition critical window may shed light
computationally hardest problems lie.
Recently, Walsh (2011) empirically analyzed two well-known voting rulesveto
single transferable vote (STV)and found smooth phase transition
two regimes. Specifically, Walsh studied coalitional manipulation unweighted votes
STV weighted votes veto, sampled number distributions
experiments, including i.i.d. distributions, correlated distributions, votes sampled
real-world elections. main result complements improves upon Walshs analysis
two ways; Walshs results show phase transition looks like concretely
veto STV, analytically show phase transition indeed smooth
generalized scoring rule (including veto STV) votes i.i.d. suggests
deciding coalitional manipulation problem may computationally hard
practice.
1.1 Results
present results, first let us formally specify setup problem.
n , candidate a, define
denote ranking profile = (1 , . . . , n ) Sm
n | f () = a}, set ranking profiles outcome f a.
Wa = { Sm
setup assumptions following.
Assumption 1. assume number candidates, m, constant.
Assumption 2. assume SCF f anonymous, i.e., treats voter equally.
Assumption 3. assume votes voters i.i.d., according distribution
p Sm . Furthermore, assume exists > 0 every Sm ,
p () (necessarily 1/m!).
assume these, setup would include uninteresting cases,
f constanti.e., matter votes are, specific candidate wins.
Another less interesting case probability given candidate winning vanishes
n essentially forget candidate large n (in sense
925

fiMossel, Procaccia, & Racz

coalition size (n) would necessary make candidate win). exclude
focus interesting cases, make additional assumption concerns
SCF distribution votes.
Assumption 4. assume exists > 0 every n every
candidate [m], probability elected least > 0, i.e., P (Wa )
(necessarily 1/m).
four assumptions satisfied distribution uniform (i.e., impartial culture assumption) SCF close neutral (i.e., neutral
tie-breaking rules); particular, hold commonly used SCFs. assumptions
somewhat general this, although i.i.d. assumption remains restrictive
one. However, discussed earlier, even showing easiness manipulation
restricted class distributions interesting.

mentioned before, interested case coalition size c n
constant c. Define probabilities


q n (c) := P coalition size c n elect candidate ,


q n (c) := P coalition size c n change outcome election ,


rn (c) := P specific coalition size c n elect candidate ,


rn (c) := P specific coalition size c n change outcome election ,
let
q (c) := lim q n (c) ,
n

q (c) := lim q n (c) ,
n

r (c) := lim rn (c) ,
n

r (c) := lim rn (c) ,
n

provided limits exist. Clearly q n (c) q n (c), rn (c) rn (c), rn (c) q n (c),
rn (c) q n (c).
describe results, deal quantities, first explain
relate various variants coalitional manipulation problem. coalitional manipulation problem coalition fixed, thus relevant quantities rn (c)
rn (c). Closely related problem determining margin victory,
minimum number voters need change votes change outcome
election. Also related problem bribery, minimum number voters need
change votes make given candidate win. main difference
problems coalitional manipulation coalition fixed, whereas latter two
problems coalition fixed. Hence relevant quantities studying latter
two q n (c) q n (c). tools also allow us deal related quantities (such
microbribery, Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009), focus
attention four quantities described above.

first result analyzes case size coalition c n large c.

show c large enough, probability close 1, specific coalition size c n
elect candidate. holds SCF satisfies (mild) restrictions.
Theorem 1.1. Assume Assumptions 1, 2, 3, 4 hold. > 0 exists
constant c = c (, , , m) rn (c) 1 every n. particular, choose
hp

p
c = (4/) log (2m!/)
log (2m/) + log (2/) .
926

fiA Smooth Transition Powerlessness Absolute Power

follows
lim lim inf rn (c) = 1.

c

n

result extends previous theorems Procaccia Rosenschein (2007a), Xia
Conitzer (2008b), scoring rules generalized scoring rules, respectively,
anonymous SCFs.

second result deals case size coalition c n small
c, transition c goes 0 . assume additionally f
generalized scoring rule (to defined Section 3.1.1); needed exist
(pathological) anonymous SCFs result hold (see beginning
Section 3 example).
Theorem 1.2. Assume Assumptions 1, 2, 3, 4 hold, furthermore f
generalized scoring rule. Then:
(1) limits q (c), q (c), r (c) r (c) exist.
(2) exists constant K = K (f, ) < q (c) Kc; particular,
limc0 q (c) = 0.
(3) 0 < c < , 0 < q (c) q (c) < 1 0 < r (c) r (c) < 1, furthermore
q (c), q (c), r (c) r (c) continuously differentiable c bounded derivative.
words, Part 2 means c small enough probability close 1

coalition size c n change outcome election, statements
r r Part 3 mean coalitional manipulation problem smooth phase
transition: number manipulators increases, probabilities coalition
power, absolute power, increase smoothly. Parts 1 2 theorem
simply make result Xia Conitzer (2008b) precise, extending analysis

( n) regime. importantly, proofs statements introduce
machinery needed establish Part 3, main result.
Since coalitional manipulation problem sharp phase transition, Theorem 1.2 interpreted suggesting realistic distributions votes likely
yield coalitional manipulation instances tractable practice, even size

coalition concentrates previously elusive ( n) regime; true
generalized scoring rule, particular almost common social choice functions (an
exception Dodgsons rule). interpretation negative flavor strengthening conclusion worst-case complexity poor barrier manipulation.
However, complexity glass fact half empty. probability margin

victory c n captured quantity q n , hence Part 3 Theorem 1.2 also
implies margin victory problem smooth phase transition. recently
pointed Xia (2012a), efficiently solving margin victory problem could help
post-election auditsused determine whether electronic elections resulted
incorrect outcome due software hardware bugsand tractability fact desirable.
methods use flexible, extended various setups interest
directly satisfy assumptions above, instance single-peaked preferences. Consider
one-dimensional political spectrum represented interval [0, 1], fix location
927

fiMossel, Procaccia, & Racz

candidates. Assume voters uniformly distributed interval, independently
other. technical reasons, distribution satisfy assumptions,
since rankings Sm p () = 0; however, tools allow us

setting
well. instance, locations candidates
1deal3

2m1
, results hold (with appropriate quantitative modifications).
2m , 2m , . . . , 2m
Similarly, locations something else, would exist subset candidates
asymptotically nonvanishing probability winning, results hold
restricted subset candidates.
Finally, discuss role tie-breaking setup, since often important
issue studying manipulation. However, since consider manipulation coalitions

size c n, ties exist constant number voters votes
changed appropriately longer tie, relevant. Indeed, tools allow us
extend results Theorem 1.2 class SCFs slightly beyond generalized scoring
rules, and, particular, allow arbitrary tie-breaking rules (see Section 3.2.1
details).
1.2 Additional Related Work
recent line research average-case algorithmic flavor also suggests manipulation indeed typically easy; see, e.g., work Kelly (1993), Conitzer Sandholm (2006), Procaccia Rosenschein (2007b), Zuckerman et al. (2009) results
certain restricted classes SCFs. different approach, initiated Friedgut, Kalai,
Keller Nisan (2008, 2011), studied fraction ranking profiles manipulable, also suggests manipulation easy average; see work Xia
Conitzer (2008a), Dobzinski Procaccia (2008), Isaksson, Kindler Mossel (2012),
Mossel Racz (2012). refer survey Faliszewski Procaccia (2010)
detailed history surrounding literature. See also related literature economics,
e.g., work Good Mayer (1975), Chamberlain Rothschild (1981), Myatt
(2007).
Recent work Xia (2012a) independent from, closely related to, work.
mentioned above, Xias paper concerned computing margin victory
elections. focuses computational complexity questions approximation algorithms,
one results similar Parts 1 2 Theorem 1.2. However, analysis
completely different; approach facilitates proof Part 3 theorem,
main contribution. even recent (and also independent) manuscript Xia (2012b)
considers similar questions generalized scoring rules captures additional types
strategic behavior (such control), again, crucially, work attempt
understand phase transition (nor subsume Theorem 1.1).

2. Large Coalitions
Without ado, prove Theorem 1.1. main idea observe i.i.d. distributions, Hamming distance random ranking profile fixed subset ranking
profiles concentrates around mean. theorem follows standard concentration
inequalities.
928

fiA Smooth Transition Powerlessness Absolute Power

n , define
Proof Theorem 1.1. , 0 Sm

,

0




1X
=
1 6= i0 ,
n
n

i=1

i.e., (, 0 ) 1/n times Hamming distance 0 . U subset ranking
profiles specific ranking profile define dU () = min0 U (, 0 ).
function dU Lipschitz constant 1/n, therefore McDiarmids inequality
following concentration inequality:

P (|dU () EdU | c) 2 exp 2c2 n
(1)
n . Suppose U n measure least , i.e., U
c > 0 U Sm

p


P ( U ) , take 2 exp 2 2 n < , e.g., let = log (2/)/ n.
(1) implies exists U |dU () EdU | , since dU () = 0,
means EdU . set U ,

P (dU () > + c) exp 2c2 n
p

c > 0. Choosing c = B/ n defining B 0 = B + log (2/) get


(2)
P dU () > B 0 / n exp 2B 2 .

language usual Hamming distance, means probability
0 n coordinates U
ranking profile
needs


changed


least
B

exp 2B 2 , made arbitrarily small choosing B large enough.
assumption, P ( Wa ) every a, therefore (2) union bound
get


P : dWa () > B 0 / n exp 2B 2 .
p
choosing B = log (2m/), probability /2.

Consider specific coalition size DB 0 n, = (, m) chosen later.
Using Chernoffs bound union bound, probability close one, every possible

ranking coalition least B 0 n voters ranking :



P Sm : coalition size DB 0 n less B 0 n voters ranking






m!P Bin DB 0 n, < B 0 n m! exp (1 1/D)2 DB 0 n/2


m! exp (1 1/D)2 D/2 ,


Bin (DB 0 n, ) denotes binomial random variable parameters DB 0 n ,
used assumption every voter probability every ranking
least > 0. Choosing = (4/) log (2m!/), probability /2.
anonymity f , outcome depends number voters voting
according ranking. Consequently, distance


B 0 / n away Wa , ranking least B 0 n voters
coalition ranking , coalition able achieve outcome. Using
union bound happens probability least 1 .
929

fiMossel, Procaccia, & Racz

3. Small Coalitions Phase Transition
section almost entirely devoted proof Theorem 1.2, also includes
helpful definitions, examples, intuitions.
Consider following example SCF. [m] let na () denote number
votersPwho ranked candidate top ranking profile . Define SCF f
f () =
a=1 ana () mod m. SCF clearly anonymous (since depends
number voters voting according specific rankings), moreover easy see
single voter always elect candidate.
example shows that, general, cannot matching lower bound

size manipulating coalition order n. However, artificial example
(one would consider voting system real life), expect matching
lower bound holds reasonable SCFs.
Xia Conitzer (2008b) introduced large class SCFs called generalized scoring
rules, include commonly occurring SCFs. following introduce
class SCFs, provide alternative way looking (as so-called hyperplane

rules), show class SCFs coalition size c n small enough
c, probability able change outcome election arbitrarily
close zero. end section prove smooth transition stated
Part 3 Theorem 1.2.
3.1 Generalized Scoring Rules, Hyperplane Rules, Equivalence
section introduce generalized scoring rules hyperplane rules show
equivalence.
3.1.1 Generalized Scoring Rules
define generalized scoring rules.
Definition 1. y, z Rk , say z equivalent, denoted z,
every i, j [k], yi yj zi zj .
Definition 2. function g : Rk [m] compatible z, g (y) = g (z).
is, function g compatible, g (y) completely determined
total preorder {y1 , . . . , yk } (a total preorder ordering ties allowed).
Definition 3 (Generalized scoring rules). Let k N, f : Sm Rk (called generalized
scoring function), g : Rk [m] g compatible (g called decision function).
functions f g determine generalized scoring rule GS (f, g) follows:
n , let
Sm
!
n
X
GS (f, g) () := g
f (i ) .
i=1

definition clear every generalized scoring rule (GSR) anonymous.
930

fiA Smooth Transition Powerlessness Absolute Power

3.1.2 Hyperplane Rules
Preliminaries notation. following, SCF let us write f fn , i.e., let us
explicitly note f function n voters; also let us write n . Since SCF
fn anonymous, outcome depends numbers voters vote according
particular rankings. Let Dn denote set points probability simplex m!
coordinates integer multiples 1/n. Let us denote typical element
probability simplex m! x = {x }Sm . ranking profile n , let us denote
corresponding element probability simplex x ( n ), i.e., Sm ,
1X
1 [i = ] .
n
n

x ( n ) =

i=1

assumptions outcome fn depends x ( n ), abuse notation
may write fn : m! |Dn [m] fn ( n ) = fn (x ( n )).
ready define hyperplane rules.
Definition 4 (Hyperplane rules). Fix finite set affine hyperplanes simplex m! :
H1 , . . . , H` . affine hyperplane partitions simplex three parts: affine hyperplane two open halfspaces either side affine hyperplane. Thus affine
hyperplanes H1 , . . . , H` partition simplex finitely many (at 3` ) regions. Let
F : m! [m] function constant region. sequence
n [m], defined
SCFs {fn }n1 , fn : Sm
fn ( n ) = F (x ( n ))
called hyperplane rule induced affine hyperplanes H1 , . . . , H` function F .
function F : m! [m] naturally partitions simplex m! parts based
outcome F . (For hyperplane rules partition coarser partition m!
induced affine hyperplanes H1 , . . . , H` .) abuse notation denote parts
{Wa }a[m] . following definition useful us.
Definition 5 (Interior boundaries partition induced F ). say x m!
interior point partition {Wa }a[m] induced F exists > 0
m! |x y| , F (x) = F (y). Otherwise, say
x m! boundary partition, denote B.
hyperplane rule boundary B contained union corresponding
affine hyperplanes. Conversely, suppose F : m! [m] arbitrary function
n [m] defined f ( n ) = F (x ( n )).
sequence (anonymous) SCFs {fn }n1 , fn : Sm
n
boundary B F contained union finitely many affine hyperplanes
m! , F necessarily hyperplane rule, exists hyperplane rule F
F F agree everywhere except perhaps union finitely many affine
hyperplanes.
931

fiMossel, Procaccia, & Racz

3.1.3 Equivalence
Xia Conitzer (2009) gave characterization generalized scoring rules: SCF
generalized scoring rule anonymous finitely locally consistent (see Xia
& Conitzer, 2009, Definition 5). characterization related saying generalized
scoring rules hyperplane rules, yet believe spelling explicitly
important, geometric viewpoint hyperplane rules somewhat different,
probabilistic context also flexible.
Lemma 3.1. class generalized scoring rules coincides class hyperplane
rules.
Proof. First let us show every hyperplane rule generalized scoring rule. Let us
consider hyperplane rule induced affine hyperplanes H1 , . . . , H` simplex m! ,
function F : m! [m]. affine hyperplanes m! thought
hyperplanes Rm! go originabusing notation also denote
H1 , . . . , H` . Let u1 , . . . , u` denote unit normal vectors hyperplanes.
n,
need define functions f g every ranking profile n Sm
`+1
`+1
n
n
GS (f, g) ( ) = F (x ( )). f : Sm R
g : R
[m]. Coordinates
1, . . . , ` f correspond hyperplanes H1 , . . . , H` , last coordinate f always
0 (this technical necessity make sure function g compatible). Let us
look coordinate corresponding hyperplane Hj normal vector uj . Sm
define
(f ())j (f ())Hj (f ())uj := (uj ) ,
coordinates Rm! indexed elements Sm .
n

(f ( ))j :=

n
X
i=1

(f (i ))j =

n
X

(uj )i = n (uj x ( n )) .

i=1

sign (f ( n ))j thus tells us side hyperplane Hj point x ( n ) lies
on. define g (y) R`+1 y`+1 = 0; requirement g
compatible defines g R`+1 . x R, define sgn (x) 1 x > 0, 1 x < 0,
0 x = 0.
define g (y1 , . . . , y` , 0), look vector (sgn (y1 ) , . . . , sgn (y` )). vector determines region m! following way: sgn (yj ) = 1, region lies
open halfspace uj , sgn (yj ) = 1 region lies open halfspace
contain uj , finally yj = 0, region lies hyperplane
Hj . define g (y1 , . . . , y` , 0) value F region m! defined
(sgn (y1 ) , . . . , sgn (y` )). value g (y1 , . . . , y` , 0) well-defined since F constant
region. Moreover, take z y`+1 = z`+1 = 0, necessarily
(sgn (y1 ) , . . . , sgn (y` )) = (sgn (z1 ) , . . . , sgn (z` )), thus g (y) = g (z): g compatible
(this used extra coordinate).
let us show every generalized scoring rule hyperplane rule. Suppose
generalized scoring rule given P
functions f : Sm P
Rk g : Rk [m]. ranking
n
n
n
n
profile Sm , define f ( ) := i=1 f (i ) = n Sm f () (x ( n )) ; way
k
view f function mapping Nm!
0 \ {0} R (and hence also view GS (f, g)
932

fiA Smooth Transition Powerlessness Absolute Power

function mapping Nm!
0 \ {0} [m]). Since mapping homogeneous, may extend
domain f (and hence GS (f, g)) Qm!
0 \ {0} natural way.


m!
total preorder O, let RO = x Q0 \ {0} : f (x) . definition, x, RO
g (f (x)) = g (f (y)), i.e., GS (f, g) constant region RO . region RO
Q-convex cone, i.e. x, RO Q [0, 1], x + (1 ) RO ,
furthermore Q>0 , x RO (both properties follow immediately
Definition 1). Thus write Qm!
0 \ {0} disjoint union Q-convex cones
{RO }O . way taking finitely many hyperplanes Rm! cutting
Qm!
0 \ {0} using hyperplanes; precise statement found Appendix A.
essentially follows result Kemperman (1986, Thm. 2)to keep paper selfcontained reproduce Appendix results proof, show statement
follows results. Since function homogeneous, need look
m!
values
GSm!(f,
g) simplex . Bym!the above, simplex divided
regions RO
via affine hyperplanes , function GS (f, g) constant

m!
RO total preorder O, GS (f, g) indeed hyperplane rule.
3.1.4 Examples
commonly used SCFs generalized scoring rules / hyperplane rules, including
positional scoring rules, instant-runoff voting, Coombs method, contingent vote,
Kemeny-Young method, Bucklin voting, Nansons method, Baldwins method, Copelands
method, maximin, ranked pairs. examples already shown Xia
Conitzer (2008b, 2009), nevertheless Appendix B detail explanations many
examples. main reason perspective hyperplane rule
arguably makes explanations simpler clearer. rule fit
framework Dodgsons rule, homogeneous (see, e.g., Brandt, 2009), therefore hyperplane rule.
3.2 Small Coalitions Generalized Scoring Rules

show generalized scoring rules, coalition size c n small enough
c change outcome election small probability. equivalence
above, work framework hyperplane rules.
consider two metrics m! : L1 metric, denoted d1 kk1 , L2
metric, denoted d2 kk2 . L1 metric important setting, since changing
votes voters corresponds moving L1 metric m! ; connection
formalized following lemma.
n . (x ( n ) , x ( n )) 2 ( n , n ),
Lemma 3.2. Let n , n Sm
1
H denotes
n H
P
Hamming distance, i.e., dH ( n , n ) = ni=1 1 [i 6= ]. Furthermore, Dn ,
n x ( n ) = (x ( n ) , y) = 2 ( n , n ).
exists n Sm
1
n H

Proof. Let 0 = n , = 1, . . . , n, define ranking profile
= (1 , . . . , , i+1 , . . . , n ) .
933

fiMossel, Procaccia, & Racz

definition, n = n . desired inequality follows triangle inequality:
n
X



d1 (x ( n ) , x ( n )) = d1 x 0 , x ( n )
d1 x i1 , x
i=1
n
X
2
2
1 [i 6= ] = dH ( n , n ) .
=
n
n
i=1

second part lemma, construct n follows. Sm , let :=
{i [n] : = }. x ( n ) , every , let = . x ( n ) > ,
choose subset indices I0 size |I0 | = ny , every I0 , let = . Finally,
define rest coordinates n x ( n ) = y. construction guarantees
d1 (x ( n ) , y) = n2 dH ( n , n ).
therefore natural define distances boundary B using L1 metric:
Definition 6 (Blowup boundary). > 0, define blowup boundary B

n

B + = m! : x B kx yk1 .
order coalition able change outcome election given
ranking profile, point simplex corresponding ranking profile needs
sufficiently close boundary B; formulated following lemma.
Lemma 3.3. Suppose n voters, coalition size k, ranking profile
n , corresponds point x ( n ) m! probability simplex.
n Sm
necessary condition coalition able change outcome election
position x ( n ) B +2k/n . Conversely, x ( n ) B +(2km!)/n , exists
coalition size k change outcome election.
Proof. ranking profile n coalition reach, dH ( n , n ) k,
n / B +2k/n , every
Lemma 3.2 d1 (x ( n ) , x ( n )) 2k
n . x ( )
ranking profile n coalition reach, x ( n ) x ( n ) region
determined hyperplanes, F (x ( n )) = F (x ( n )), i.e., coalition cannot
change outcome election.
suppose x ( n ) B +(2km!)/n . definition exists B
m!
d1 (x ( n ) , y) 2km!
n . Since B, exists Dn d1 (y, y) n
F (y) 6= F (x ( n )). triangle inequality, d1 (x ( n ) , y) 2k
n , second
n x ( n ) = ( n , n ) k.
part Lemma 3.2 exists n Sm
H
coalition consisting voters indices := {i [n] : 6= } thus change
outcome election.
Corollary 3.4. n voters, probability coalition size k change
outcome election bounded P x ( n ) B +(2km!)/n

P x ( n ) B +2k/n , n drawn according probability distribution
satisfying conditions setup.
934

fiA Smooth Transition Powerlessness Absolute Power

Gaussian limit. Due i.i.d.-ness votes, multinomial random variable x ( n )
concentrates around expectation, rescaled random variable
x ( n ) :=



n (x ( n ) E (x ( n )))

converges normal distribution, zero mean specific covariance structure.
analysis better use Gaussian picture, thus reformulate
preliminaries limiting setting. First, let us determine limiting distribution.
Lemma 3.5. x ( n ) n N (0, ), covariance structure given
= diag (p) ppT , recall p distribution vote.
Proof. clear P
E (x ( n )) = 0. Computing
covariance structure, first
1
2
E x = n2 ni,j=1 P (i = , j = ) = 1 n1 p ()2 + n1 p (),


Var (x ) = n1 p () p ()2 thus Var (x ) = p () p ()2 . similarly 6= 0



n



1 X
1 X
1
0
0
E (x x0 ) = 2
p () p 0 ,
P = , j = = 2
p () p = 1
n
n
n
i,j=1

i6=j

Cov (x , x0 ) = n1 p () p ( 0 ) thus

Cov (x , x0 ) = p () p 0 .
Note: concentration x ( n ) around mean, assumption
every n every candidate [m], P (f ( n ) = a) , necessary every
> 0 every candidate [m] exists m! ky E (x (1 ))k1
F (y) = a.
Denote distribution N (0, ) let X denote random variable distributed
according . Note degenerate multivariate normal distribution, support
concentrates hyperplane H0 coordinates sum zero. (This
P
n
Sm x ( ) = 0.)
underlying function F : m! [m] corresponds function F : Rm! |H0 [m]
Gaussian limit, functionnF partitions
Rm! |H0 parts based

outcome F . denote parts Wa
. need following definitions
a[m]

properties boundaries, analogous above.
Definition 7 (Interior boundaries
partition). say x Rm! |H0
n
interior point partition Wa
induced F exists > 0
a[m]

R |H0 kx yk1 , F (x) = F (y). Otherwise, say
x Rm! |H0 boundary partition, denote B.
m!

Lemma 3.6. boundary B comes hyperplane rule, i.e., B contained
union ` affine hyperplanes m! , B contained union ` hyperplanes
Rm! |H0 , ` `.
935

fiMossel, Procaccia, & Racz

Proof. Two things happen affine hyperplane H m! take Gaussian
limit: (1) E (x ()) H, translation E (x ()) takes H hyperplane H

Rm! |H0 , since H goes origin, scaling (in particular n) move
hyperplane; (2) E (x ())
/ H, translation E (x ()) takes H affine

m!
hyperplane H R |H0 go origin, scaling n moves
H affine hyperplane Rm! |H0 whose L2 distance origin proportional

n, n limit affine hyperplane vanishes.
Definition 8 (Blowup boundary). > 0, define blowup boundary B

n

B + = Rm! |H0 : x B kx yk1 .

Let us focus specifically coalition size c n (small) constant c. Corollary 3.4 implies following.
Corollary 3.7. hyperplane rules limit probability election
n


voters coalition size c n change outcome election X B +2c .
following claim, together Corollary 3.7, tells us hyperplane rules

coalition size c n change outcome election small probability,
given c sufficiently small, proving Part 2 Theorem 1.2.
n oM
Claim 3.8. Suppose SCF hyperplane rule, particular let Hi

i=1

collection hyperplanes Rm! |H0 B
i=1 Hi .

r 2 Mc
+c
.
X B


Proof. condition union bound


X


+c
X B

X Hi+c .
i=1

hyperplane H Rm! |H0 , denote (one of) corresponding unit normal vector(s) (in
hyperplane H0 ) u.
n

H = x Rm! |H0 : u x = 0
since L1 distance always greater L2 distance,
n
n

H +c x Rm! |H0 : H kx yk2 c = x Rm! |H0 : |u x| c .
Since X multidimensional Gaussian r.v., u X one-dimensional Gaussian r.v. (which
centered). Therefore




2c
X H +c u X [c, c] r

.
2 Var u X
936

fiA Smooth Transition Powerlessness Absolute Power






2


Var u X = E u X = E uT X X u = uT u,

remains show
min
u:kuk=1,u1

uT u ,

1 m!-dimensional vector 1 every coordinate.
Let 1 () 2 () m! () denote eigenvalues . Since positive
semidefinite, eigenvalues nonnegative. know 0 eigenvalue (the
corresponding eigenvector 1), m! () = 0. variational characterization
eigenvalues
min
uT u = m!1 () ,
u:kuk=1,u1

need show m!1 () . use Weyls inequalities.
Lemma 3.9 (Weyls inequalities). n n matrix let 1 (M ) 2 (M )
n (M ) denote eigenvalues. C n n symmetric matrices
j (A + C) (A) + ji+1 (C)

j,

j (A + C) (A) + ji+n (C)

j.

use Weyls inequality = diag (p) C = ppT . eigenvalues
{p ()}Sm , less . Since C rank 1, eigenvalues one
zero, single nonzero eigenvalue m! (C) = pT p. Since = diag (p)ppT = A+C,
Weyls inequality tells us

m!1 () m! (diag (p)) + m!1 ppT + 0 = .

implies lower bound ( n) size coalition needed
order change outcome election hyperplane rules. mentioned before,
commonly occurring SCFs class rules: see Appendix B many examples.
3.2.1 Almost Hyperplane Rules
Furthermore, Gaussian limiting setting sensitive small changes
voting rule finite n. Consequently, SCFs almost hyperplane rules (in

sense make precise below), conclusion holds: coalition size ( n) needed
order able change outcome election non-negligible probability.
particular, result holds SCFs arbitrary tie-breaking rules ranking
profiles lie one hyperplanes (e.g., tie-breaking rule depend
number voters n).
Definition 9 (Almost hyperplane rules). Fix finite set affine hyperplanes
simplex m! : H1 , . . . , H` . partition simplex finitely many regions. Let F :
m! [m] function constant region, let B denote
937

fiMossel, Procaccia, & Racz

n [m], called
induced boundary. sequence SCFs {fn }n1 , fn : Sm

almost hyperplane rule every n x ( n )
/ B +o(1/ n) ,

fn ( n ) = F (x ( n )) .
SCF called almost hyperplane rule induced affine hyperplanes H1 , . . . , H`
function F .
n [m], almost
Lemma 3.10. Suppose sequence SCFs {fn }n1 , fn : Sm
hyperplane rule defined ` hyperplanes. Gaussian limiting setting boundary
B contained union ` hyperplanes Rm! |H0 , ` `.


Proof. finite n, induced boundary fn simplex m! contained B +o(1/ n) ,


definition. Since Gaussian limit scale n, blowup (1/ n)
boundary B disappears limit, hence back situation Lemma 3.6.
Consequently, affine hyperplanes corresponding almost hyperplane rule either
disappear infinity become hyperplanes Rm! |H0 .
Corollary 3.11. Corollary 3.7 Claim 3.8 hold almost hyperplane rules well.
3.3 Smoothness Phase Transition
final subsection goal show Parts 1 3 Theorem 1.2. existence
limits Part 1 follows immediately Gaussian limit described above;
detail this, rather give formulas limiting probabilities. imply
properties described Part 3 theorem.
following let hyperplane rule given affine hyperplanes H1 , . . . , H`
m! function F : m! [m]; limiting setting denote H1 , . . . , H`
corresponding hyperplanes Rm! |H0 denote F : Rm! |H0 [m] corresponding
function.
3.3.1 Quantities q q
x Rm! |H0 define
(x) :=

inf

d1 (x, y) ,

(x) := max

inf

a[m] y:F (y)=a

y:F (y)6=F (x)

d1 (x, y) .

previous subsection immediate write



q (c) = X : X 2c ,



q (c) = X : X 2c .
important note boundary B contained union finitely many
hyperplanes, H1 , . . . , H`, thus regions F constant convex cones
intersection finitely many halfspaces.

Consequently (x) either d1 (x,0),

m!
d1 x, Hj =
0 denotes origin R , d1 x, Hj 1 j `,
938

fiA Smooth Transition Powerlessness Absolute Power

inf yHj d1 (x, y). scale x positive constant , distance origin


every hyperplane scales well (i.e., d1 (x, 0) = d1 (x, 0) d1 x, Hj =


d1 x, Hj ), thus every > 0, (x) = (x). Consequently,
write x = kxk2 s, m!1 , m!1 denotes (m! 1)-sphere (not
n , set ranking profiles n voters candidates),
confused Sm
(x) = kxk2 (s).
scaling property holds well, hence




(3)
q (c) = X : X 2c ,
2



q (c) = X : X 2c .
(4)
2

Recall condition every [m], P (f () = a) , implies every
> 0 every [m] exists x Rm! |H0 kxk2 F (x) = a.
Consequently every x Rm! |H0 must
(x) d1(x, 0) (x) d1 (x, 0).

m!1
particular,
d1 (s, 0) m!d2 (s, 0) = m! (s) , (s) m!.
immediately implies every c > 0



2c

q (c) X : X
> 0.
2
m!
show q (c) < 1, note since boundary contained union finitely
many hyperplanes, exists m!1 (s ) > 0. continuity ,
exists neighborhood U m!1 every U , (s) (s ) /2.
4c
x x/ kxk2 U kxk2 > (s
) ,
(x) = kxk2 (x/ kxk2 ) >

4c (s )
= 2c.
(s ) 2

consequently






q (c) 1 X : X/ X U, X >
2

2

4c
(s )


< 1.

Finally, fact q (c) q (c) continuously differentiable follows
formulas (3) (4), since q (c) q (c) written Gaussian volume
subset Rm! |H0 , cases subset grows continuously c increases.
derivative q (c) q (c) bounded zero (by Corollary 3.7 Claim 3.8),
c derivative approaches zero, since derivative continuous, must
bounded constant whole half-line.
3.3.2 Quantities r r


previous setup coalition size c n specified, ranking profile

could changed arbitrarily within Hamming ball radius c n. probability

simplex m! corresponded L1 ball radius 2c/ n, rescaled limiting
939

fiMossel, Procaccia, & Racz

setting corresponded L1 ball Rm! |H0 radius 2c. coalition size

c n specified, things slightly different. particular, look probability
distribution probability simplex m! induced distribution ranking profiles
(or, limiting setting, Gaussian distribution Rm! |H0 ), lost track
votes specific coalition. Nonetheless, Gaussian limiting setting still provides
formulas limiting probabilities r (c) r (c).

first
draw random ranking profile n c n voters
coalition, nc n , voters coalition set votes arbitrarily.
question is, coalition affect outcome vote? particular, (a)
change outcome election, (b)
candidate?
elect

ranking profile nc n corresponds point x nc n probability simplex m! , setting votes
move point probability
thecoalition

nc
n
simplex neighborhood x
. omit calculation finite n
present result limiting setting.
Suppose limiting ranking profile voters coalition corresponds
point x Rm! |H0 . set points coalition reach following:
n

Rc (x) := Rm! |H0 : Sm : x + cp () 0 .
define

n

(x) := inf : R (x) F (y) 6= F (x) ,
n

(x) := inf : [m] R (x) F (y) = ,

follows immediately write



r (c) = X : X c ,



r (c) = X : X c .
way Section 3.3.1 one argue scale: > 0
(x) = (x) (x) = (x). Hence




r (c) = X : X c ,
(5)
2



r (c) = X : X c .
(6)
2

every 0 < c < r (c) q (c) < 1 (using Section 3.3.1). Let us show
also r (c) > 0. claim m!1 |H0 , (s) 2 . follows fact
m!1 |H0 m!1 |H0 R 2 (s), true m!1 |H0


Sm , + 2 p () 1 1 + 2 = 0. Thus



c

r (c) X : X
>0
2
2
claimed.
940

fiA Smooth Transition Powerlessness Absolute Power

Finally, fact r (c) r (c) continuously differentiable follows
formulas (5) (6) using argument given above: r (c) r (c) written
Gaussian volume subsets Rm! |H0 , subsets grow continuously c increases.
derivative r (c) r (c) bounded zero (by Corollary 3.7 Claim 3.8),
c derivative approaches zero, since derivative continuous,
must bounded constant whole half-line.

Acknowledgments
thank anonymous referees helpful comments. E.M. supported NSF (DMS
1106999) DOD ONR grant N000141110140, M.Z.R. supported UC
Berkeley Graduate Fellowship NSF (DMS 0548249).

Appendix A. Decomposing Rd Disjoint Union Finitely Many
Convex Cones: Via Hyperplanes
order paper self-contained, reproduce main definitions
results Kemperman (1986) make precise claim used proof Lemma 3.1
way decompose Qd0 \ {0} disjoint union finitely many Q-convex
cones via hyperplanes. Kempermans paper deals convex sets general,
summarize results convex cones relevant us. Kempermans results
pertain finite dimensional linear spaces state form; end
show results Rd0 follow immediately these, consequence also
obtain claim used proof Lemma 3.1.
Let us start main definitions. following, linear spaces
reals finite dimensional. Let X linear space. convex cone subset K X
x, K > 0 imply x + K x K. (We require
0 K.) set X, denote affine hull aff (A), convex hull cvx (A),
closure cl (A). Note K X convex cone, aff (K) linear subspace
X.
define two special types convex cones: basic convex cones elementary convex
cones.
Definition 10 (Basic convex cone). Let K convex cone finite dimensional linear
space X. say K basic convex cone (in X) K member K = K0
partition
1 . . . K
r
X = K0 K
X finitely many disjoint convex cones {Ki }ri=0 .
Note linear subspace X basic convex cone, immediately
follows K basic convex cone X basic convex cone aff (K).
order define elementary convex cones, need definitions.
Definition 11 (Open polyhedral convex cone). Let K convex cone finite dimensional linear space X. say K open polyhedral convex cone relative X K
941

fiMossel, Procaccia, & Racz

expressed intersection finitely many open halfspaces H1 , . . . , H` X,
origin boundary. whole linear space X open polyhedral
convex cone ` = 0.
Definition 12 (Relatively open polyhedral convex cone). Let K convex cone
finite dimensional linear space X. K relatively open polyhedral convex cone
either K = K open polyhedral convex cone relative aff (K).
Definition 13 (Elementary convex cone). Let K convex cone finite dimensional
linear space X. say K elementary convex cone K represented
disjoint union finitely many relatively open polyhedral convex cones.
main result Kemperman concerning convex cones following (Kemperman,
1986, Thm. 2).
Theorem A.1. Let K convex cone Rd . K basic convex cone
elementary convex cone.
Lemma 3.1 use direction, thus leave proof
direction exercise reader.
Proof direction. Let X finite dimensional linear space let K basic
convex cone X dimension = dim (K) = dim (Y ), = aff (K). prove
induction following:
(i) relative interior K, denoted K 0 , relatively open polyhedral convex cone.
(ii) K 0 6= , denote F1 , . . . , F` (d 1)-dimensional hyperplanes
cor0
responding finitely many faces polyhedron cl (K) = cl K .
convex cones Fi K, = 1, . . . , `, elementary convex cones dimension
1 (but need disjoint).
(iii) convex cone K also elementary convex cone.
K = , properties (i) - (iii) hold. = 0, necessarily K = {0}, since K
convex cone, K satisfies properties (i) - (iii) above.
may assume 1 basic convex cone dimension
1 satisfies properties (i) - (iii) above. Since K basic convex cone, exists
partition
1 . . . K
r
= K0 K
(7)
finitely many disjoint convex cones {Kj }rj=0 , K0 = K. may assume
r 0 minimal, hence Kj non-empty. Note K 0 also non-empty since
dim (K) = dim (Y ).
r = 0 K = K0 = properties (i) - (iii) immediately satisfied,
may assume r 1. j = 1, . . . , r, let Hj hyperplane separates
convex cone K = K0 non-empty interior K 0 non-empty convex cone Kj .
(Such hyperplanes exist hyperplane separation theorem, and, moreover,
hyperplane goes origin, Kj contains least one point every
942

fiA Smooth Transition Powerlessness Absolute Power

open ball around origin, since Kj cone.) Let Hj0 associated open half
space contains interior K 0 K. Let
L0 = H10 Hr0 .
L0 polyhedral convex cone, open relative , contains interior
K 0 K.
claim L0 = K 0 . enough show L0 K, L0 K 0
follows definition K 0 . Suppose contrary exists x L0
x
/ K. partition (7) must exist index 1 j r x Kj .
/ L0 , contradiction. proves (i).
implies x
/ Hj0 thus x
let us show (ii). (7), write linear space Fi disjoint union
convex cones Fi Kj , j = 0, . . . , r, thus Fi K basic convex cone hence,
induction, elementary convex cone.
Finally, let us show K elementary convex cone. Since K 0 polyhedral
convex cone open relative , remains show K \ K 0 written
finite disjoint union relatively open polyhedral convex cones. (ii), write
K \ K 0 finite union elementary convex cones:
K \ K 0 = `i=1 (Fi K) ,
remains show write finite disjoint union relatively
open polyhedral convex cones. may assume w.l.o.g. Fi K 6=
(Fi K) * (Fj K) 6= j (otherwise leave Fi K union).
claim every i,
[
(Fj Fi K) ,
(8)
rel int (Fi K) (Fi K) \
j6=i

immediately follows rel int (Fi K) rel int (Fj K) = 6= j.
show (8), let two open halfspaces either side hyperplane Fj denoted
Fj+ Fj . W.l.o.g. assume K Fj = . Since (Fi K) * (Fj K), must
(Fi K) Fj+ 6= . Let x (Fi K) Fj+ let Fj Fi K. Since Fi K convex,
interval x contained Fi K, (Fi K) Fj = , points
line past point Fi K; hence
/ rel int (Fi K).
Since Fi K basic convex cone, rel int (Fi K) relatively open polyhedral
convex cone induction. Fi K = aff (Fi K) rel int (Fi K) = Fi K. not,
denote Fi,1 , . . . , Fi,`i hyperplanes aff (Fi K) corresponding finitely
many faces polyhedron cl (Fi K). induction, convex cones Fi,j Fi K,
j = 1, . . . , `i , elementary convex cones, write

[

`
`

K \ K 0 = i=1 rel int (Fi K)
i=1 `j=1
(Fi,j Fi K) .

remains shown `i=1 `j=1
(Fi,j Fi K) written finite disjoint
union relatively open polyhedral convex cones; follows iterating previous
argument.

943

fiMossel, Procaccia, & Racz

Let us show Rd0 basic convex cone Rd . = 1, . . . , d, define




closed halfspace Hi0 = x Rd : xi 0 complement Hi<0 = x Rd : xi < 0 ,
define convex cones
0
Ki = H10 Hi1
Hi<0 ,

= 1, . . . , d.

write Rd disjoint union convex cones Rd0 K1 , . . . , Kd , showing
indeed Rd0 basic convex cone. implies write Rd0 disjoint
union convex cones C1 , . . . , Cr , Ci basic convex cone, hence,
Theorem A.1, elementary convex cone.
let us turn claim proof Lemma 3.1. Lemma 3.1, write Qm!
0 \ {0}
m!
1 . . . C
r .
disjoint union finitely many Q-convex cones: Q0 \ {0} = C0 C
m!
= 0, . . . , r, let Ci = cvx (Ci ). known (see, e.g., Young, 1975) Ci = Q Ci .
Ci therefore disjoint convex cones satisfy



C0 C1 . . . Cr Rm!
0

(9)




cl C0 cl C1 cl Cr = Rm!
0 .

(10)

goal show Ci elementary convex cone. Conditions (10) (9)
similar definition basic convex cone; spirit let us introduce
following definition.
Definition 14 (Basic convex cone closure). Let K0 convex cone finite
dimensional linear space X. say K0 basic convex cone closure (in X)
exist disjoint convex cones K1 , . . . , Kr
1 . . . K
rX
K0 K

cl (K0 ) cl (K1 ) cl (Kr ) = X.
Since Rd0 basic convex cone, Ci basic convex cones closure.
fact, every basic convex cone closure elementary convex cone; proof
exactly one shown direction Theorem A.1, one
needs replace basic convex cone basic convex cone closure everywhere
proof, make appropriate changes. Moreover, direction Theorem A.1
implies actually every basic convex cone closure basic convex cone.
Hence Ci elementary convex cones, need Lemma 3.1.

Appendix B. Voting Rules Hyperplane Rules: Examples
following show positional scoring rules, instant-runoff voting, Coombs
method, contingent vote, Kemeny-Young method, Bucklin voting, Nansons method,
Baldwins method, Copelands method hyperplane rules.
944

fiA Smooth Transition Powerlessness Absolute Power

Positional scoring rules. Let w Rm weight vector. Given
ranking profile
P
vector , (normalized) score candidate [m] sa = n1 ni=1 w i1 (a) .
positional scoring rule associated weight vector w elects candidate
highest score. (In case tie, tie-breaking rule,
care here.) denote SCF n voters fnw . Examples include
plurality (with weight vector w = (1, 0, 0, . . . , 0)), Borda count (with weight vector
w = (m 1, 2, . . . , 0)) veto (with weight vector w = (1, 1, . . . , 1, 0)).
sequence SCFs {fnw }n1 associate function F w : m! [m]
followingP
way. candidate
[m] x m! , define (normalized) score

sa (x) = Sm x w 1 (a) , let
F w (x) := arg max sa (x) ,
a[m]

arg max unique, unique, tie-breaking rule.
construction guarantees fnw = F w |Dn . candidates 6= b, define
n

Ha,b := x m! : sa (x) = sb (x) ,
affine hyperplane
probability simplex m! . Clearly boundary


B w contained union 2 affine hyperplanes:
[
Bw
Ha,b .
a6=b[m]

Instant-runoff voting. candidate receives absolute majority first preference
votes, candidate wins. candidate receives absolute majority,
candidate fewest top votes eliminated. next round votes
counted again, ballot counted one vote advancing candidate
ranked highest ballot. repeated winning candidate receives
majority vote remaining candidates.
boundary corresponds two kinds situations: either (1) tie
top end, two candidates remain; (2) tie eliminating
candidate end one rounds. Technically situation (1) also contained
situation (2), since end one view choosing winner eliminating
second placed candidate. One see candidates b tied
elimination candidates C [m] \ {a, b} (where C = allowed)
eliminated, necessarily
X
X
X
X
x =
x .
(11)
C 0 C {(1),...,(|C 0 |)}=C 0 ,
(|C 0 |+1)=a

C 0 C {(1),...,(|C 0 |)}=C 0 ,
(|C 0 |+1)=b

Consequently, denoting sa,C (x) quantity left hand side (11),
boundary B contained union m2 2m affine hyperplanes:
n

[
[
B
x m! : sa,C (x) = sb,C (x) .
a6=b C[m]\{a,b}

945

fiMossel, Procaccia, & Racz

Coombs method. similar IRV, elimination rule different.
candidate receives absolute majority first preference votes, candidate
wins. candidate receives absolute majority, candidate ranked
last voters eliminated. next round votes counted again,
ballot counted one vote advancing candidate ranked highest
ballot. repeated winning candidate receives majority
vote remaining candidates.
boundary corresponds two kinds situations: either (1) tie
top end, two candidates remain; (2) tie eliminating
candidate end one rounds. Technically situation (1) also contained
situation (2), since end one view choosing winner eliminating
second placed candidate. One see candidates b tied
elimination candidates C [m] \ {a, b} (where C = allowed)
eliminated, necessarily
X
X
X
X
x =
x .
(12)
C 0 C {(m),...,(m|C 0 |+1)}=C 0 ,
(m|C 0 |)=a

C 0 C {(m),...,(m|C 0 |+1)}=C 0 ,
(m|C 0 |)=b

Consequently, denoting sa,C (x) quantity left hand side (12),
boundary B contained union m2 2m affine hyperplanes:
B

[

[




x m! : sa,C (x) = sb,C (x) .

a6=b C[m]\{a,b}

Contingent vote. also similar IRV, except two candidates
get eliminated first round. candidate receives absolute majority first
preference votes, he/she wins. candidate receives absolute majority,
top two leading candidates eliminated second count,
votes supported eliminated candidate redistributed among
two remaining candidates. candidate achieves absolute majority
wins.
boundary B corresponds two kinds situations: either (1)
two distinct top candidates, votes voters voted
candidates redistributed, two top candidates dead heat; (2)
two candidates receive equal number votes first
round. situations described subsets affine hyperplanes,
B contained union (m 1) affine hyperplanes:






[
X
X
X
X
m!
B
x :
x +
x =
x +
x





a6=b
:(1)=a
:(1)=b
:(1){a,b},a
/
>b
:(1){a,b},b
/
>a



[
X
X

x m! :
x =
x .


a6=b

:(1)=a

:(1)=b

946

fiA Smooth Transition Powerlessness Absolute Power

Kemeny-Young method. Denote K Kendall tau distance, metric
permutations counts number pairwise disagreements two
permutations, i.e.,
X
K (1 , 2 ) =
1 [a b opposite order 1 2 ] ,
{a,b}

sum unordered pairs distinct candidates. Given ranking
profile n , Kemeny-Young method selects ranking minimizes sum
Kendall tau distances votes:
= arg min

n
X

K (i , ) ,

i=1

winner election declared (1). us convenient
write
X
= arg min
x ( n ) K (, ) .


boundary
B
P
P must exist two rankings 1 2
1 (1) 6= 2 (1) x K (, 1 ) = x K (, 2 ). Thus B contained
union (m!)2 affine hyperplanes:
(
)
X
X
[
m!
x :
x K (, 1 ) =
x K (, 2 ) .
B
1 6=2





Bucklin voting. First every candidate gets point voters ranked
top. candidate majority (i.e., n/2
points), candidate wins. not, every candidate gets point
voters ranked second. candidate
n/2 points this, candidate points wins (there might
multiple candidates n/2 points given round). process
iterated candidate n/2 points.
point boundary B corresponds situation pair candidates number points number rounds. Therefore B
contained union m2 (m 1) /2 affine hyperplanes:



k
k

[ [
X
X
X
X
B
x m! :
x =
x .


a6=b k=1

i=1 :(i)=a

i=1 :(i)=b

Nansons method. Borda count combined variation instantrunoff voting procedure. First, Borda scores candidates computed,
candidates Borda score greater average Borda score
eliminated. Borda scores remaining candidate recomputed,
eliminated candidates ballot. repeated
final candidate left.
947

fiMossel, Procaccia, & Racz

boundary corresponds situations candidates Borda score exactly equals
average score candidates eliminated. C [m], denote
sa,C (x) score candidate exactly candidates C eliminated
(sa,C (x) linear function {x }Sm ), denote sC (x) average score
remaining candidates exactly candidates C eliminated.
boundary B contained union m2m affine hyperplanes:
n

[
[
B
x m! : sa,C (x) = sC (x) .
a[m] C[m]\{a}

Baldwins method. essentially Borda count combined instantrunoff voting procedure. First, Borda scores candidates computed,
candidate lowest score eliminated. Borda scores
remaining candidate recomputed, eliminated candidate
ballot. repeated final candidate left.
boundary corresponds ties eliminating candidate end one
rounds. Borrow notation sa,C (x) previous example. boundary B
thus contained union m2 2m affine hyperplanes:
n

[
[
B
x m! : sa,C (x) = sb,C (x) .
a6=b C[m]\{a,b}

Copelands method. pairwise aggregation method: every candidate gets
1 point candidate beats pairwise majority election, 1/2
point candidate ties pairwise majority election. winner
candidate receives points. method
corresponds cutting

simplex m! finitely many regions via
affine
hyperplanes,
2
region winner candidate points.
previous examples tie-breaking rules issue,
become important. care tie-breaking rules
affine hyperplane two candidates tie pairwise majority election.
However, open regions intersection halfspaces defined affine
hyperplanes candidates tied top scores.
case, order Copeland hyperplane rule, need break ties
favor candidate whole region. (This also ties broken
Copelands method Xia & Conitzer, 2008b.)
Using tie-breaking rule Copelands method indeed hyperplane rule, since
boundary contained union
2 affine hyperplanes:



[
X
X
B
x m! :
x =
x .




a6=b

:a>b

948

:b>a

fiA Smooth Transition Powerlessness Absolute Power

References
Achlioptas, D. (1999). Threshold phenomena random graph colouring satisfiability.
Ph.D. thesis, Department Computer Science, University Toronto.
Achlioptas, D., Naor, A., & Peres, Y. (2005). Rigorous location phase transitions hard
optimization problems. Nature, 435 (7043), 759764.
Bartholdi III, J., Tovey, C., & Trick, M. (1989). Computational Difficulty Manipulating Election. Social Choice Welfare, 6 (3), 227241.
Betzler, N., Niedermeier, R., & Woeginger, G. J. (2011). Unweighted coalitional manipulation Borda rule NP-hard. Proceedings 22nd International Joint
Conference Artificial Intelligence (IJCAI), pp. 5560.
Brandt, F. (2009). remarks Dodgsons voting rule. Mathematical Logic Quarterly,
55 (4), 460463.
Caragiannis, I., & Procaccia, A. D. (2011). Voting almost maximizes social welfare despite
limited communication. Artificial Intelligence, 175 (910), 16551671.
Chamberlain, G., & Rothschild, M. (1981). note probability casting decisive
vote. Journal Economic Theory, 25 (1), 152162.
Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). really hard problems are.
Proceedings 12th International Joint Conference Artificial Intelligence
(IJCAI), pp. 331337.
Conitzer, V., & Sandholm, T. (2006). Nonexistence Voting Rules Usually
Hard Manipulate. Proceedings 21st National Conference Artificial
Intelligence, Vol. 21, pp. 627634.
Conitzer, V., Sandholm, T., & Lang, J. (2007). elections candidates
hard manipulate?. Journal ACM, 54 (3), 133.
Dobzinski, S., & Procaccia, A. (2008). Frequent Manipulability Elections: Case
Two Voters. Proceedings 4th International Workshop Internet
Network Economics, pp. 653664. Springer.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). Llull
Copeland Voting Computationally Resist Bribery Constructive Control. Journal Artificial Intelligence Research, 35, 275341.
Faliszewski, P., & Procaccia, A. (2010). AIs War Manipulation: Winning?. AI
Magazine, 31 (4), 5364.
Friedgut, E., Kalai, G., Keller, N., & Nisan, N. (2011). Quantitative Version
Gibbard-Satterthwaite Theorem Three Alternatives. SIAM J. Comput., 40 (3),
934952.
Friedgut, E., Kalai, G., & Nisan, N. (2008). Elections manipulated often. Proceedings 49th Annual Symposium Foundations Computer Science, pp.
243249. IEEE.
949

fiMossel, Procaccia, & Racz

Fu, Y., & Anderson, P. (1986). Application statistical mechanics NP-complete problems
combinatorial optimisation. Journal Physics A: Mathematical General, 19,
16051620.
Gibbard, A. (1973). Manipulation Voting Schemes: General Result. Econometrica:
Journal Econometric Society, 587601.
Gomes, C., & Walsh, T. (2006). Randomness Structure. Rossi, F., van Beek, P.,
& Walsh, T. (Eds.), Handbook Constraint Programming, Foundations Artificial
Intelligence, pp. 639664. Elsevier.
Good, I., & Mayer, L. (1975). Estimating efficacy vote. Behavioral Science, 20 (1),
2533.
Isaksson, M., Kindler, G., & Mossel, E. (2012). Geometry Manipulation: Quantitative Proof Gibbard-Satterthwaite Theorem. Combinatorica, 32 (2), 221250.
Kelly, J. (1993). Almost social choice rules highly manipulable, arent.
Social Choice Welfare, 10 (2), 161175.
Kemperman, J. (1986). Decomposing Rd finitely many semigroups. Indagationes
Mathematicae (Proceedings), Vol. 89, pp. 7178. Elsevier.
Mossel, E., & Racz, M. (2012). quantitative Gibbard-Satterthwaite theorem without
neutrality. Proceedings 44th ACM Symposium Theory Computing
(STOC), pp. 10411060. ACM. Full version appear Combinatorica, available
arXiv preprint arXiv:1110.5888.
Myatt, D. (2007). theory strategic voting. Review Economic Studies,
74 (1), 255281.
Peleg, B. (1979). note manipulability large voting schemes. Theory Decision,
11 (4), 401412.
Pritchard, G., & Slinko, A. (2006). average minimum size manipulating coalition.
Social Choice Welfare, 27 (2), 263277.
Pritchard, G., & Wilson, M. (2009). Asymptotics minimum manipulating coalition
size positional voting rules impartial culture behaviour. Mathematical Social
Sciences, 58 (1), 3557.
Procaccia, A., & Rosenschein, J. (2007a). Average-case tractability manipulation voting via fraction manipulators. Proceedings 6th International Conference
Autonomous Agents Multi-Agent Systems (AAMAS), pp. 718720.
Procaccia, A., & Rosenschein, J. (2007b). Junta Distributions Average-case Complexity Manipulating Elections. Journal Artificial Intelligence Research, 28,
157181.
Satterthwaite, M. (1975). Strategy-proofness Arrows Conditions: Existence Correspondence Theorems Voting Procedures Social Welfare Functions. Journal
Economic Theory, 10 (2), 187217.
Slinko, A. (2004). large coalition manipulate election?. Mathematical
Social Sciences, 47 (3), 289293.
950

fiA Smooth Transition Powerlessness Absolute Power

Walsh, T. (2002). Interface P NP: COL, XOR, NAE, 1-in-k, Horn
SAT. Proceedings 17th National Conference AI (AAAI 2002), pp. 695
700.
Walsh, T. (2011). Hard Manipulation Problems?. Journal Artifical
Intelligence Research, 42, 129.
Xia, L. (2012a). Computing margin victory various voting rules. Proceedings
13th ACM Conference Electronic Commerce (EC), pp. 982999. ACM.
Xia, L. (2012b). Many Vote Operations Needed Manipulate Voting System?.
Arxiv preprint arXiv:1204.1231.
Xia, L., & Conitzer, V. (2008a). Sufficient Condition Voting Rules Frequently
Manipulable. Proceedings 9th ACM Conference Electronic Commerce
(EC), pp. 99108. ACM.
Xia, L., & Conitzer, V. (2008b). Generalized Scoring Rules Frequency Coalitional
Manipulability. Proceedings 9th ACM Conference Electronic Commerce
(EC), pp. 109118. ACM.
Xia, L., & Conitzer, V. (2009). Finite Local Consistency Characterizes Generalized Scoring
Rules. Proceedings 9th International Joint Conference Artificial Intelligence (IJCAI), pp. 336341.
Xia, L., Zuckerman, M., Procaccia, A. D., Conitzer, V., & Rosenschein, J. S. (2009). Complexity unweighted coalitional manipulation common voting rules.
Proceedings 21st International Joint Conference Artificial Intelligence (IJCAI), pp. 348353.
Young, H. (1975). Social choice scoring functions. SIAM Journal Applied Mathematics,
824838.
Zuckerman, M., Procaccia, A. D., & Rosenschein, J. S. (2009). Algorithms coalitional
manipulation problem. Artificial Intelligence, 173 (2), 392412.

951

fiJournal Artificial Intelligence Research 48 (2013)

Submitted 5/2013; published 12/2013

Exact Query Reformulation Databases
First-order Description Logics Ontologies
Enrico Franconi
Volha Kerhet
Nhung Ngo

franconi@inf.unibz.it
kerhet@inf.unibz.it
ngo@inf.unibz.it

Free University Bozen-Bolzano, Italy

Abstract
study general framework query rewriting presence arbitrary
first-order logic ontology database signature. framework supports deciding
existence safe-range first-order equivalent reformulation query terms
database signature, so, provides effective approach construct reformulation based interpolation using standard theorem proving techniques (e.g., tableau).
Since reformulation safe-range formula, effectively executable SQL query.
end, present non-trivial application framework ontologies
expressive ALCHOIQ description logic, providing effective means compute
safe-range first-order exact reformulations queries.

1. Introduction
address problem query reformulation expressive ontologies databases.
ontology provides conceptual view database composed constraints
vocabulary extending basic vocabulary data. Querying database using
terms richer ontology allows flexibility using basic
vocabulary relational database directly.
paper study develop query rewriting framework applicable knowledge
representation systems data stored classical finite relational database, way
literature called locally-closed world assumption (Etzioni, Golden,
& Weld, 1997), exact views (Marx, 2007; Nash, Segoufin, & Vianu, 2010; Fan, Geerts,
& Zheng, 2012), DBox (Seylan, Franconi, & de Bruijn, 2009; Franconi, Ibanez-Garcia,
& Seylan, 2011). DBox set ground atoms semantically behaves like
database, i.e., interpretation database predicates DBox exactly equal
database relations. DBox predicates closed, i.e., extensions
every interpretation, whereas predicates ontology open, i.e.,
extensions may vary among different interpretations. consider
open interpretation database predicates (also called ABox sound views).
ABox, interpretation database predicates contains database relations possibly
more. notion less faithful representation database semantics since would
allow spurious interpretations database predicates additional unwanted tuples
present original database.
general framework ontology set first-order formulas, queries
(possibly open) first-order formulas. Within setting, framework provides precise
semantic conditions decide existence safe-range first-order equivalent reformulac
2013
AI Access Foundation. rights reserved.

fiFranconi, Kerhet, & Ngo

tion query terms database signature. also provides effective approach
construct reformulation sufficient conditions. interested safe-range reformulations queries range-restricted syntax needed reduce original
query answering problem relational algebra evaluation (e.g., via SQL) original
database (Abiteboul, Hull, & Vianu, 1995). framework points several conditions
ontologies queries guarantee existence safe-range reformulation.
show conditions feasible practice also provide efficient method
ensure validation. Standard theorem proving techniques used compute
reformulation.
order complete, framework applicable ontologies queries expressed
fragment first-order logic enjoying finitely controllable determinacy (Nash et al.,
2010), stronger property finite model property logic. employed logic
enjoy finitely controllable determinacy approach would become sound
incomplete, still effectively implementable using standard theorem proving techniques.
explored non-trivial applications framework complete; paper,
application ALCHOIQ ontologies concept queries discussed. show
(i) check whether answers given query ontology solely determined
extension DBox predicates and, so, (ii) find equivalent rewriting
query terms DBox predicates allow use standard database technology
answering query. means benefit low computational complexity
size data answering queries relational databases. addition, possible
reuse standard techniques description logics reasoning find rewritings,
paper Seylan et al. (2009).
query reformulation problem received strong interest classical relational
database research well modern knowledge representation studies. Differently
mainstream research query reformulation (Halevy, 2001), mostly based
perfect maximally contained rewritings sound views relatively inexpressive constraints (see, e.g., DL-Lite approach Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009), focus exact rewritings exact views, since characterises
precisely query answering problem ontologies databases, case
exact semantics database must preserved. example, consider ground negative query given standard relational database; adding ontology top it,
answer supposed changesince query uses signature database
additional constraints supposed change meaning querywhereas
database treated ABox (sound views) answer may change presence
ontology. may important application perspective: DBox preserves
behaviour legacy application queries relational database. Moreover,
focussing exact reformulations definable queries (as opposed considering certain
answer semantics arbitrary queries, DL-Lite), guarantee answers
queries subsequently composed arbitrary way: may important legacy
database applications.
work extends works exact rewritings exact views Marx (2007)
Nash et al. (2010) focussing safe-range reformulations conditions ensuring
existence, considering general first-order ontologies extending database
signature, rather local view constraints database predicates (Halevy,
886

fiExact Query Reformulation DBs FO DL Ontologies

2001). paper extends papers Franconi, Kerhet, Ngo (2012a, 2012b)
providing precise semantic characterisation existence exact reformulation
(Theorem 4) opposed sufficient conditions, considering much expressive description logic ALCHOIQ, providing proofs.
paper organised follows: section 2 provides necessary formal background
definitions; section 3 introduces notion query determined database; section 4 introduces characterisation query reformulation problem; sections 5 6
conditions allowing effective reformulation analysed, sound complete algorithm compute reformulation introduced. Finally, present case
ALCHOIQ ontologies. proofs presented details Appendix.

2. Preliminaries
Let FOL(C, P) classical function-free first-order language equality signature
= (C, P), C finite set constants P set predicates associated
arities. rest paper refer arbitrary fragment FOL(C, P),
called L.
denote P{1 ,...,n } set predicates occurring formulas 1 , . . . , n ,
C{1 ,...,n } set constants occurring formulas 1 , . . . , n ; sake
brevity, instead P{} (resp. C{} ) write P (resp. C ). denote (1 , . . . , n )
signature formulas 1 , . . . , n , namely union P{1 ,...,n } C{1 ,...,n } .
denote arity predicate P ar(P ). Given formula , denote set
variables appearing var(), set free variables appearing
free(); may use notation [X] , X = free() (possibly empty)
set free variables formula.
database (instance) DB finite set ground atoms form P (c1 , . . . , cn ),
P P, n-ary predicate, ci C (1 n). set predicates appearing
database DB denoted PDB , set constants appearing DB called
active domain DB, denoted CDB . (possibly empty) finite set KB closed
formulas called ontology.
usual, interpretation = hI , includes non-empty setthe domain
interpretation function defined constants predicates signature. say
interpretations = hI , J = hJ , J equal, written = J , = J
= J . interpretation embeds database DB, holds aI = every
database constant CDB (the standard name assumption (SNA), customary databases,
see Abiteboul et al., 1995) (c1 , . . . , cn ) P P (c1 , . . . , cn ) DB.
denote set interpretations embedding database DB E(DB).
words, every interpretation embedding DB interpretation
database predicate always given exactly content database;
is, general, case interpretation non-database predicates.
say database predicates closed, predicates open
may interpreted differently different interpretations. consider open
world assumption (the ABox ) embedding database interpretation. open
world, interpretation soundly embeds database holds (c1 , . . . , cn ) P
(but if) P (c1 , . . . , cn ) DB.
887

fiFranconi, Kerhet, & Ngo

order allow arbitrary database embedded, generalise standard
name assumption constants C; implies domain interpretation
necessarily includes set constants C. finiteness C corresponds
finite ability database system represent distinct constant symbols; C meant
unknown advance, since different database systems may different limits.
see framework introduced depend choice C.
Given interpretation = hI , i, denote I|S interpretation restricted
smaller signature P C, i.e., interpretation domain
interpretation function defined constants predicates set
S. semantic active domain signature 0 P C interpretation I, denoted
adom( 0 , I), set elements domain occurring interpretations
predicates constants 0 I:
adom( 0 , I) :=

[

[

P 0 (a1 ,...,an )P

{a1 , . . . , }

[

{cI }.

c 0

0 PDB C, interpretations J embedding DB have:
adom( 0 , I) = adom( 0 , J ); so, case introduce notation adom( 0 , DB) :=
adom( 0 , I), interpretation embedding database DB. Intuitively
adom( 0 , DB) includes constants 0 DB appearing relations corresponding predicates 0 .
Let X set variable symbols set; substitution total function : X 7
assigning element variable X, including empty substitution
X = . Domain image (range) substitution written dom() rng()
respectively. Given subset set constants C0 C, write formula [X]
true interpretation free variables substituted according substitution
: X 7 C0 (I |= [X/] ). Given interpretation = hI , subset
domain , write formula [X] true free variables
interpreted according substitution : X 7 (I, |= ). extension domain
aSformula [X] respect interpretation defined set domain elements
{rng() | dom() = X, rng() , I, |= [X] }.
usual, interpretation closed formula true called model
formula; set models formula (resp. KB) denoted () (resp.
(KB)). database DB legal ontology KB exists model KB embedding
DB. following, consider consistent non-tautological ontologies legal
databases.
2.1 Queries
query (possibly closed) formula. Given query Q[X] , define certain answer
KB DB follows:
Definition 1 (Certain Answer). (certain) answer query Q[X] database DB
ontology KB set substitutions constants:
{ | dom() = X, rng() C, (KB) E(DB) : |= Q[X/] }.
888

fiExact Query Reformulation DBs FO DL Ontologies

Query answering defined entailment problem, going
(high) complexity entailment.
Note, query Q closed (i.e., Boolean query), certain answer {}
Q true models ontology embedding database, otherwise.
following, assume closed formula Q[X/] neither valid inconsistent
ontology KB, given substitution : X 7 C assigning variables distinct constants
appearing Q, KB, CDB : would lead trivial reformulations.
show weaken standard name assumption constants
assuming unique names, without changing certain answers. said before,
interpretation satisfies standard name assumption cI = c c C. Alternatively, interpretation satisfies unique name assumption (UNA) aI 6= bI
different a, b C. denote set interpretations satisfying standard name
assumption I(SNA). denote set interpretations satisfying unique name
assumption I(UNA). following proposition allows us freely interchange standard name unique name assumptions interpretations embedding databases.
practical advantage, since encode unique name assumption classical
first-order logic reasoners, many description logics reasoners support natively
unique name assumption extension OWL.
Proposition 1 (SNA vs UNA). query Q[X] , ontology KB database DB,
{ | dom() = X, rng() C, I(SNA) (KB) E(DB) : |= Q[X/] } =
{ | dom() = X, rng() C, I(UNA) (KB) E(DB) : |= Q[X/] }.
Since query arbitrary first-order formula, answer may depend
domain, know advance. example, query Q(x) = Student(x)
database Student(a), Student(b), domain {a, b, c} answer {x = c},
domain {a, b, c, d} answer {x = c, x = d}. Therefore, notion
domain independent queries introduced relational databases. adapt
classical definitions (Avron, 2008; Abiteboul et al., 1995) framework: need
general version domain independence, namely domain independence w.r.t
ontology, i.e., restricted models ontology.
Definition 2 (Domain Independence). formula Q[X] domain independent
respect ontology KB iff every two models J KB (i.e., = hI ,
J = hJ , J i) agree interpretation predicates constants (i.e.
= J ), every substitution : X 7 J have:
rng() I, |= Q[X] iff
rng() J J , |= Q[X] .
definition reduces classical definition domain independence whenever
ontology empty.
weaker version domain independencewhich relevant open formulasis
following.
Definition 3 (Ground Domain Independence). formula Q[X] ground domain independent iff Q[X/] domain independent every substitution : X 7 C.
889

fiFranconi, Kerhet, & Ngo

example, formula P (x) ground domain independent, domain independent.
problem checking whether FOL formula domain independent undecidable
(Abiteboul et al., 1995). well known safe-range syntactic fragment FOL introduced
Codd equally expressive language; indeed safe-range formula domain independent, domain independent formula easily transformed logically
equivalent safe-range formula. Intuitively, formula safe-range variables
bounded positive predicates equalities (for full details see Appendix A.3).
example, formula A(x) B(x) safe-range, queries A(x) x. A(x)
not. check whether formula safe-range, formula transformed logically
equivalent safe-range normal form range restriction computed according set
syntax based rules; range restriction formula subset free variables,
coincides free variables formula said safe-range (Abiteboul
et al., 1995). Similar domain independence, formula ground safe-range grounding formula safe-range. ontology KB safe-range (domain independent),
every formula KB safe-range (domain independent).
safe-range fragment first-order logic standard name assumption
equally expressive relational algebra, core SQL (Abiteboul et al.,
1995).

3. Determinacy
certain answer query includes substitutions make query true
models ontology embedding database: so, substitution would make
query true model, would discarded certain answer.
words, may case answer query necessarily among
models ontology embedding database. case, query fully
determined given source data; indeed, answer possible,
certain. Due indeterminacy query respect data, complexity
compute certain answer general increases complexity entailment
logic. paper focus case query answer
models ontology embedding database, namely, information requested
query fully available source data without ambiguity. way,
indeterminacy disappears, complexity process may decrease (see section 4).
determinacy query w.r.t. source database (Nash et al., 2010; Marx, 2007; Fan
et al., 2012) called implicit definability formula (the query) set
predicates (the database predicates) Beth (1953).
Definition 4 (Finite Determinacy Implicit Definability). query Q[X] (finitely)
determined (or implicitly definable from) database predicates PDB KB iff
two models J ontology KBboth finite interpretation
database predicates PDB whenever I|PDB C = J |PDB C every substitution
: X 7 have: I, |= Q[X] iff J , |= Q[X] .
Intuitively, answer implicitly definable query depend interpretation non-database predicates. database domain fixed, never
890

fiExact Query Reformulation DBs FO DL Ontologies

case substitution would make query true model ontology
false others, since truth value implicitly defined query depends
interpretation database predicates constants domain (which
fixed). practice, focussing finite determinacy queries guarantee user
always interpret answers certain, also exactnamely
whatever answer never part answer possible world.
following focus ontologies queries fragments FOL(C, P)
determinacy models finite interpretation database predicates (finite
determinacy) determinacy models unrestricted interpretation database
predicates (unrestricted determinacy) coincide. say fragments finitely
controllable determinacy: require whenever query finitely determined
also determined unrestricted models (the reverse trivially true). Indeed, results
paper would fail finite determinacy unrestricted determinacy coincide:
shown (Gurevich, 1984) Theorem 1 fails consider models
finite interpretation database predicates.
Example 1 (Example database theory). Let P = {P, R, A}, PDB = {P, R},
KB = {x, y, z. R(x, y) R(x, z) = z,
x, y. R(x, y) z. R(z, x),
(x, y. R(x, y) z. R(y, z)) (x. A(x) P (x))}.
formula x, y. R(x, y) z. R(y, z) entailed first two formulas
finite interpretations R. query Q = A(x) finitely determined P (it equivalent
P (x) models finite interpretation R), determined
database predicate models unrestricted interpretation R. knowledge
base enjoy finitely controllable determinacy.
exact reformulation query (Nash et al., 2010) (also called explicit definition
Beth, 1953) formula logically equivalent query makes use database
predicates constants.
Definition 5 (Exact Reformulation Explicit Definability). query Q[X] explicitly definable database predicates PDB ontology KB iff
b[X] FOL(C, P), KB |= X.Q[X] Q
b[X] (Q)
b PDB . call
formula Q
b[X] exact reformulation Q[X] KB PDB .
formula Q
Determinacy query completely characterised existence exact reformulation query: well known first-order query determined database
predicates exists first-order exact reformulation.
Theorem 1 (Projective Beth definability, Beth, 1953). query Q implicitly definable database predicates PDB ontology KB, iff explicitly definable
b FOL(C, P) PDB KB.
formula Q
e formula obtained uniformly replacing
Let Q formula L Q
every occurrence non-database predicate P new predicate Pe. extend
renaming operator e set formulas natural way. One check whether query
implicitly definable using following theorem.
Theorem 2 (Testing Determinacy, Beth, 1953). query Q[X] implicitly definable
g |= X.Q[X] Q
e[X] .
database predicates PDB ontology KB iff KB KB

891

fiFranconi, Kerhet, & Ngo

4. Exact Safe-Range Query Reformulation
section analyse conditions original query answering problem
corresponding entailment problem reduced systematically model checking
problem safe-range formula database (e.g., using database system
SQL). Given database signature PDB , ontology KB, query Q[X] expressed L
determined database predicates, goal find safe-range reformulation
b[X] Q[X] FOL(C, P), evaluated relational algebra expression
Q
legal database instance, gives answer certain answer Q[X] database
KB. reformulated following problem:
Problem 1 (Exact safe-range Query Reformulation). Find exact reformulation
b[X] Q[X] KB safe-range query FOL(C, P) PDB .
Q
Since exact reformulation equivalent ontology original query,
certain answer original query reformulated query identical.
precisely, following proposition holds.
Proposition 2. Given database DB, let Q[X] implicitly definable PDB KB
b[X] exact reformulation Q[X] KB PDB , then:
let Q
{ | dom() = X, rng() C, (KB) E(DB) : |= Q[X/] } =
b[X/] }.
{ | dom() = X, rng() C, (KB) E(DB) : |= Q
equation clear order answer exactly reformulated query,
one may still need consider models ontology embedding database, i.e.,
still entailment problem solve. following theorem states condition
reduce original query answering problembased entailmentto problem
checking validity exact reformulation single model: condition
reformulation domain independent. Indeed one interpretation
(with particular domain) embedding database signature restricted
database predicates.
Theorem 3 (Adequacy Exact safe-range Query Reformulation). Let DB
b[X] exact domain
database legal KB, let Q[X] query. Q
independent (or safe-range) reformulation Q[X] KB PDB , then:
{ | dom() = X, rng() C, (KB) E(DB) : |= Q[X/] } =
b DB), = hC, E(DB) : I|P
{ | dom() = X, rng() adom((Q),

DB C

b[X/] }.
|= Q

safe-range reformulation necessary transform first-order query relational
algebra query evaluated using SQL techniques. theorem
shows addition safe-range also sufficient property exact reformulation correctly evaluated SQL query. Let us see example
cannot reduce problem answering exact reformulation model checking
database, exact reformulation safe-range.
Example 2. Let P = {P, A}, PDB = {P }, C = {a},
DB = {P (a, a)}, KB = {y. P (a, y) A(y)},
b[X] = y. P (x, y) (i.e., X = {x}).
Q[X] = Q
892

fiExact Query Reformulation DBs FO DL Ontologies

C includes active domain CDB (it actually equal).
DB legal KB = h{a}, P = {(a, a)}, AI =
obviously, (KB).
{ | dom() = X, rng() C, (KB) E(DB) : |= Q[X/] } =
one take = h{a, b}, P = {(a, a)}, AI = {b}; (KB)
E(DB), possible substitution {x a} have: 6|= P (a, y).
However,
b DB), = hC, E(DB) : I|P C |= Q
b[X/] } =
{ | dom() = X, rng() adom((Q),
DB

{x a}
seen, answers query reformulation exists contain
constants active domain database query; therefore, ground statements ontology involving non-database predicates non-active domain constants
(for example, ABox statements) play role final evaluation
reformulated query database.

5. Conditions Exact Safe-Range Reformulation
seen importance getting exact safe-range query reformulation.
section going study conditions exact safe-range query
reformulation exists.
First all, focus semantic notion safe-range namely domain independence. implicit definability isas already knowa sufficient condition
existence exact reformulation, guarantee alone existence domain
independent reformulation.
Example 3. Let P = {A, B}, PDB = {A}, KB = {x.B(x) A(x)}, Q = B(x).
Q implicitly definable PDB KB, every exact reformulation Q
PDB KB logically equivalent A(x) domain independent.
looking example, seems reason non domain independent
reformulation lies fact ontology, domain independent, cannot guarantee existence exact domain independent reformulation non domain independent
query. However, let us consider following example:
Example 4. Let PDB = {A, C}, KB = {A(a),
B(y) C(x). easy see KB
implicitly definable PDB KB,
independent reformulation Q.

x. A(x) B(x)} let query Q =
domain independent Q not. Q
b = A(a) C(x) exact domain
Q

obvious spite fact query Q domain independent,
domain independent respect ontology KB. words, case
ontology guarantees existence exact domain independent reformulation.
queries domain independent respect ontology, following
theorem holds, giving semantic requirements existence exact domain
independent reformulation.
893

fiFranconi, Kerhet, & Ngo

Theorem 4 (Semantic Characterisation). Given set database predicates PDB ,
domain independent ontology KB, query Q[X] , domain independent exact reformub[X] Q[X] PDB KB exists Q[X] implicitly definable
lation Q
PDB KB domain independent respect KB.
theorem shows us semantic conditions exact domain independent reformulation query, give us method compute reformulation equivalent safe-range form. following theorem gives us sufficient
conditions existence exact safe-range reformulation decidable fragment
FOL(C, P) finite unrestricted determinacy coincide, gives us constructive
way compute it, exists.
Theorem 5 (Constructive). If:
g |= X. Q[X] Q
e[X] (that is, Q[X] implicitly definable),
1. KB KB
2. Q[X] safe-range (that is, Q[X] domain independent),
3. KB safe-range (that is, KB domain independent),
b[X] Q[X] safe-range query FOL(C, P)
exists exact reformulation Q
PDB KB, obtained constructively.
order constructively compute exact safe-range query reformulation use
b[X]
tableau based method find Craigs interpolant (Fitting, 1996) compute Q
gQ
e[X] ). See Section 6 full details.
validity proof implication (KB Q[X] ) (KB
Let us consider fully worked example, adapted paper Nash et al.
(2010).
Example 5. Given: P = {R, V1 , V2 , V3 , A}, PDB = {V1 , V2 , V3 , Adom} Adom
active domain DB,
KB = { x, y. V1 (x, y) z, v. R(z, x) R(z, v) R(v, y),
x, y. V2 (x, y) z. R(x, z) R(z, y),
x, y. V3 (x, y) z, v. R(x, z) R(z, v) R(v, y),
Q(x, y) = z, v, u. R(z, x) R(z, v) R(v, u) R(u, y)}.
conditions theorem satisfied: Q(x, y) implicitly definable PDB
KB; Q(x, y) safe-range; KB safe-range.
b y)
Therefore, tableau method one finds Craigs interpolant compute Q(x,
g
e
b
validity proof implication (KB Q[X] ) (KB Q[X] ) obtain Q(x, y) =
z. V1 (x, z) v. (V2 (v, z) V3 (v, y))an exact ground safe-range reformulation. Since
b y) Adom(x)
answer Q active domain, also KB |= Q(x,
b
Adom(y). KB |= Q(x, y) Q(x, y) Adom(x) Adom(y). Therefore, z. V1 (x, z)
v. (V2 (v, z) V3 (v, y))Adom(x)Adom(y) exact safe-range reformulation Q(x, y)
PDB KB.
894

fiExact Query Reformulation DBs FO DL Ontologies

6. Constructing Safe-Range Reformulation
section introduce method compute safe-range reformulation implicitly
definable query conditions theorem 5 satisfied. method based
notion interpolant introduced Craig (1957).
Definition 6 (Interpolant). sentence interpolant sentence
FOL(C, P), predicate constant symbols set predicate
constant symbols , valid sentences
FOL(C, P).
Theorem 6 (Craigs interpolation). valid sentence FOL(C, P),
neither valid, exists interpolant.
Note, Beth definability (Theorem 1) Craigs interpolation theorem
hold fragments FOL(C, P): interpolant may always expressed
fragment itself, obviously FOL(C, P) (because Theorem 6).
interpolant used find exact reformulation given implicitly definable
query follows.
Theorem 7 (Interpolant definition). Let Q[X] query n 0 free variables
implicitly definable database predicates PDB ontology KB. Then,
closed formula c1 , ..., cn distinct constant symbols C appearing KB Q[X] :
^
^
g Q
e[X/c ,...,c ] )
(( KB) Q[X/c1 ,...,cn ] ) (( KB)
(1)
n
1
b[c ,...,c /X] exact reformulation Q[X] KB
valid, interpolant Q
n
1
PDB .
Therefore, find exact reformulation implicitly definable query terms
database predicates enough find interpolant implication (1)
substitute constants c1 , . . . , cn back free variables X original query.
interpolant constructed validity proof (1) using automated theorem
proving techniques tableau resolution. order guarantee safe-range
property reformulation, use tableau method book Fitting (1996).
6.1 Tableau-based Method Compute Interpolant
section recall context tableau based method compute interpolant (Fitting, 1996).
Assume valid, therefore unsatisfiable. closed tableau
corresponding . order compute interpolant tableau one needs
modify biased tableau.
Definition 7 (Biased tableau). biased tableau formulas tree = (V, E)
where:
V set nodes, node labelled set biased formulas. biased formula
expression form L() R() formula. node n,
S(n) denotes set biased formulas labelling n.
895

fiFranconi, Kerhet, & Ngo

root tree labelled {L(), R()}
E set edges. Given 2 nodes n1 n2 , (n1 , n2 ) E iff biased
completion rule n1 n2 . say biased completion rule n1
n2
() result applying rule X(), X refer L R
(for rules, two possibilities choosing ()),
S(n2 ) = (S(n1 ) \ {X()}) {Y ()}.
Let C set constants input formulas tableau. C par extends C
infinite set new constants. constant new occur anywhere
tableau. notations, following rules :
Propositional rules
X()
X()

Negation rules
X(>)
X()

rule
X(1 2 )

X()
X(>)

X(1 )
X(2 )

rule
X((1 2 ))
X(1 ) | X(2 )

First order rules
rule
X(x.)

rule
X(x.)

X((t))
C par

X((c))
new constant c

Equality rules
reflexivity rule

replacement rule
X(t = u)
((t))

X()


X(t = t)
occurs

((u))

C par

node tableau closed contains X() (). node closed,
rule applied. words, becomes leaf tree. branch closed
contains closed node tableau closed branches closed. Obviously,
standard tableau FOL closed biased tableau vice versa.
Given closed biased tableau, interpolant computed applying interpolant rules.
int
interpolant rule written I, formula
= {L(1 ), L(2 ), ..., L(n ), R(1 ), R(2 ), ..., R(m )}.
Rules closed branches
int

int

r1. {L(), L()}

r2. {R(), R()} >

int

int

r3. {L()}

r4. {R()} >
int

int

r5. {L(), R()}

r6. {R(), L()}
896

fiExact Query Reformulation DBs FO DL Ontologies

Rules propositional cases
int
{X()}
p1.

p4.

p6.

int

p2.

int

{X()}
int
{X(1 ), X(2 )}

{X(>)}
int

int

p3.

{X()}
int

{X()}
{X(>)}
int
int
{L(1 )} I1 {L(2 )} I2
p5.

int

int

{X(1 2 )}
{L((1 2 ))} I1 I2
int
int
{R(1 )} I1 {R(2 )} I2
int

{R((1 2 ))} I1 I2
Rules first order cases :
int
{X((p))}
f1.

f2.

f3.

f4.

f5.

p parameter occur

int

{X(x.(x))}
int
{L((c))}
int

c occurs {1 , ..., n }

{L(x.(x))}
int
{R((c))}
int

c occurs {1 , ..., }

{R(x.(x))}
int
{L((c))}
int

{L(x.(x))} x.I[c/x]
int
{R((c))}
int

c occur {1 , ..., n }
c occur {1 , ..., }

{R(x.(x))} x.I[c/x]
Rules equality cases
int
{X((p)), X(t = t)}
e1.

e3.

int

int

e2.

{X((p))}
int
{L((u)), R(t = u)}
int

{X((u)), X(t = u)}
int

{X((t)), X(t = u)}
u occurs (t), 1 , ...,

{L((t)), R(t = u)} = u
int
{R((u)), L(t = u)}

u occurs (t), 1 , ...,
int
{R((t)), L(t = u)} = u
int
{L((u)), R(t = u)}
e5.
u occur (t), 1 , ...,
int
{L((t)), R(t = u)} I[u/t]
int
{R((u)), L(t = u)}
e6.
u occur (t), 1 , ...,
int
{R((t)), L(t = u)} I[u/t]
e4.

summary, order compute interpolant , one first need generate
biased tableaux proof unsatisfiability using biased completion rules
apply interpolant rules bottom leaves root.
Let us consider example demonstrate method works.
Example 6. Let P = {S, G, U }, PDB = {S, U },
897

fiFranconi, Kerhet, & Ngo

KB = { x(S(x) (G(x) U (x)))
x(G(x) S(x))
x(U (x) S(x))
x(G(x) U (x))}
Q(x) = G(x)
Obviously, Q implicitly definable U , since ontology states G
U partition S. follow tableau method find exact reformulation.
int
compactness, use notation instead I.
S0 = {L(x(S(x) (G(x) U (x)))),
L(x(G(x) S(x))),
L(x(U (x) S(x))),
L(x(G(x) U (x))),
L(G(c)),
R(x(S(x) (G1 (x) U (x)))),
R(x(G1 (x) S(x))),
R(x(U (x) S(x))),
R(x(G1 (x) U (x))),
R(G1 (c))}
applying rule removing implication, have:
S1 = {L(S(c) G(c) U (c)),
L(G(c) S(c))),
L(U (c) S(c)),
L(G(c) U (c)),
L(G(c)),
R(S(c) G1 (c) U (c)),
R(G1 (c) S(c)),
R(U (c) S(c)),
R(G1 (c) U (c)),
R(G1 (c))}
interpolant S1 computed follows:
S4 {R(S(c)}S(c)

S4 {R(U (c))}U (c)

S4 = S3 {R(S(c) U (c))}(S(c)U (c))



S3 {R(G1 (c))}>

(S(c)U (c))

B.7

S2 {L(G(c))}

S3 = S2 {L(U (c))}

(S(c)U (c))

S2 = S1 {L(S(c))}

(S(c)U (c))

B.5

S1 {L(G(c))}

B.3

S1

b
Therefore, S(c)U (c) interpolant Q(x)
= S(x)U (x) exact reformulation
Q(x).
898

fiExact Query Reformulation DBs FO DL Ontologies

Algorithm 1 Safe-range Reformulation
Input: safe-range KB, safe-range implicitly definable query Q[X] .
Output: exact safe-range reformulation.
b[X] Theorem 7
1: Compute interpolant Q
b[X]
2: free variable x bounded positive predicate Q
b[X] := Q
b[X] Adom b (x)
Q
Q
b[X]
3: Return Q

6.2 Safe-Range Reformulation
want show reformulation computed tableau based method
condition Theorem 5 generates ground safe-range query.
Theorem 8 (Ground safe-range Reformulation). Let KB ontology, let Q
query implicitly definable PDB . KB Q safe-range rewritten
b obtained using tableau method described Section 6.1 ground safe-range.
query Q
words, conditions Theorem 8 guarantee quantified variables
reformulation range-restricted. need consider still unsafe free variables.
theorem help us deal non-range-restricted free variables. Let us first
define active domain predicate query Q safe-range formula:
Adom Q (x) :=
W
W
P PQ z1 , . . . , zar(P )1 . P (x, z1 , . . . , zar(P )1 ) . . . P (z1 , . . . , zar(P )1 , x)
cCQ (x = c).

Theorem 9 (Range query). Let KB domain independent ontology, let
Q[x1 ,...,xn ] query domain independent respect KB.
KB |= x1 , . . . , xn . Q[x1 ,...,xn ] Adom Q (x1 ) . . . Adom Q (xn ).
Given safe-range ontology, safe-range implicitly definable query obviously
domain independent respect ontology. case, Theorem 9 says
answer reformulation include active domain elements. Therefore, active
domain predicate used guard free variables bounded
positive predicate.
Based Theorem 8 Theorem 9, propose complete procedure construct
safe-range reformulation Algorithm 1.

7. Guarded Negation Fragment ALCHOIQ
ALCHOIQ extension description logic ALC role hierarchies, individuals,
inverse roles, qualified cardinality restrictions: corresponds SHOIQ description logic without transitive roles; logic basis OWL. syntax
semantics ALCHOIQ concept expressions summarised Figure 1,
atomic concept, C concepts, individual name, P atomic role,
R either P P . forall qualified unqualified atmost operators
derived using negation atleast operator usual way. TBox ALCHOIQ
899

fiFranconi, Kerhet, & Ngo

Syntax

{o}
P
P
C
C uD
C tD
nR
nR.C

Semantics
AI
{oI }
P
{(y, x)|(x, y) P }
\C
C DI
C DI
{x|#({y|(x, y) RI }) n}
{x|#({y|(x, y) RI } C ) n}

Figure 1: Syntax semantics ALCHOIQ concepts roles
set concept inclusion axioms C v role inclusion axioms R v (where C,
concepts R, roles) usual description logics semantics.
section, present application Theorem 5, introducing ALCHOIQGN
description logic, guarded negation syntactic fragment ALCHOIQ (Figure 2)
happens express exactly domain independent concepts TBoxes ALCHOIQ.
language restricts ALCHOIQ prescribing negated concepts
guarded generalised atom (an atomic concept, nominal, unqualified atleast
number restriction), i.e., absolute negation forbidden. Similarly, derived forall
atmost operators would guarded using standard definition dual
atleast operator, guarded negation. ALCHOIQGN actually intersection GNFO fragment (Barany, ten Cate, & Otto, 2012) ALCHOIQ (see
Appendix A.5 details GNFO).
ALCHOIQGN important property coinciding domain independent fragment ALCHOIQ, therefore providing excellent candidate language
ontologies queries satisfying conditions Theorem 5.
Theorem 10 (Expressive power equivalence). domain independent fragment
ALCHOIQ ALCHOIQGN equally expressive.
words theorem says domain independent TBox axiom
domain independent concept query ALCHOIQ logically equivalent, respectively,
TBox axiom concept query ALCHOIQGN , vice-versa. theorem provides
description logics version Codds theorem. Codds theorem states safe-range
syntactic fragment FOL domain-independent fragment FOL precisely
equivalent expressive power; is, database query formulated one language
expressed other.
R
B
C

::=
::=
::=

P | P
| {o} | nR
B | nR.C | nR.C | B u C | C u | C
Figure 2: Syntax ALCHOIQGN concepts roles
900

fiExact Query Reformulation DBs FO DL Ontologies

7.1 Applying Constructive Theorem
want reformulate concept queries ontology DBox reformulated query evaluated SQL query database represented DBox.
context, database DBox, ontology ALCHOIQGN TBox,
query ALCHOIQGN concept query. concept query either ALCHOIQGN
concept expression denoting open formula one free variable, ALCHOIQGN
ABox concept assertion denoting boolean query. expected, DBox includes ground
atomic statements form A(a) P (a, b) (where atomic concept P
atomic role). Theorem 10 draw following corollary.
Corollary 1. ALCHOIQGN TBoxes concept queries domain independent.
also prove following theorem.
Theorem 11. ALCHOIQGN TBoxes concept queries finitely controllable determinacy.
Therefore, satisfy conditions Theorem 5, language like
expressive ALCHOIQ description logic, guarded negation.
argue non-guarded negation appear cleanly designed ontology,
and, present, fixed. Indeed, use absolute negative informationsuch as,
e.g., non-male female ( male v female)should discouraged clean
design methodology, since subsumer would include sorts objects universe
(but ones subsumee type) without obvious control. guarded negative
information subsumee allowedsuch axiom non-male person
female (person u male v female).
observation suggests fix non-guarded negations: every non-guarded negation users asked replace guarded one, guard may arbitrary
atomic concept, nominal, non-qualified existential. Therefore, user asked make
explicit type concept, way make domain independent; note
type could also fresh new atomic concept. believe fix proposing
ALCHOIQ reasonable one, would make ALCHOIQ ontologies eligible
used framework.
7.2 Complete Procedure
ALCHOIQGN decidable logic feasible application general framework.
Given ALCHOIQGN ontology KB concept query Q, apply procedure
generate safe-range reformulation database concepts roles (based
constructive theorem, conditions satisfied), exists.
Input: ALCHOIQGN TBox KB, concept query Q ALCHOIQGN ,
database signature (database atomic concepts roles).
g |= Q Q
e using
1. Check implicit definability query Q testing KB KB
standard OWL2 reasoner (ALCHOIQGN sublanguage OWL2). Continue
holds.
901

fiFranconi, Kerhet, & Ngo

b tableau proof generated step 1 (see
2. Compute safe-range reformulation Q
Section 6). implemented simple extension standard DL reasoner
even presence important optimisation techniques semantic
branching, absorption, backjumping explained Seylan et al. (2009) ten
Cate, Franconi, Seylan (2011).
b expressed database signature.
Output: safe-range reformulation Q
Note procedure checking determinacy computing reformulation
could run offline mode compile time. Indeed, could run atomic concept
ontology, store persistently outcome reformulation
successful. pre-computation may expensive operation, sinceas
seenit based entailment, complexity involves size ontology
data.
order get idea size reformulations, ALCF description
logic tableau-based algorithm computing explicit definitions double
exponential size (ten Cate et al., 2011; ten Cate, Franconi, & Seylan, 2013); algorithm
optimal also shown smallest explicit definition implicitly defined
concept may double exponentially long size input TBox.
Clearly, similarly DL-Lite reformulations, research needed order optimise
reformulation step order make practical. However, note framework
presented clear advantage point view conceptual modelling since
implicit definitions (that is, queries) general TBoxes double exponentially
succinct acyclic concept definitions (that is, explicit queries database).
also another interesting open problem checking given database
legal respect given ontology. Remember database DB legal
ontology KB exists model KB embedding DB. check involves heavy
computations optimised algorithm still unknown: matter fact,
known method today reduce problem satisfiability problem
database embedded TBox using nominals (Franconi et al., 2011). research
needed order optimise reasoning nominals special case.
Appendix A.5 contains definitions theorems needed prove theorems 10
11.

8. Conclusion
introduced framework compute exact reformulation first-order queries
database ontologies. found exact conditions guarantee
safe-range reformulation exists, show evaluated relational
algebra query database give answer original query
ontology. non-trivial case study presented field description logics,
ALCHOIQ language.
also implemented tool based Prover9 theorem prover (McCune, 2011).
Given arbitrary first-order ontology, database signature, arbitrary first-order
query TPTP syntax, tool performs tests check whether reformulation computed, computes optimal safe-range reformulation.
902

fiExact Query Reformulation DBs FO DL Ontologies

framework useful data exchange-like scenarios, target database
(made determined relations) materialised proper database,
arbitrary queries performed. achieved context non-exact
rewritings preserving certain answers. scenario description logics ontologies,
rewritings concept queries pre-computed offline once. shown framework works theory also case arbitrary safe-range first-order queries, tool
shows possible practice. case description logics, working
extending theoretical framework conjunctive queries: need finitely controllable
determinacy conjunctive queries, seems follow description logic
works Barany, Gottlob, Otto (2010) Rosati (2011).
future work, would like study optimisations reformulations. practical perspective, since might many rewritten queries one original query,
problem selecting optimised query terms query evaluation important.
fact, one take account criteria used optimise, as:
size rewritings, numbers used predicates, priority predicates, number
relational operators, clever usage duplicates. tool, plan evaluate
proposed technique real context.
Concurrently, exploring problem fixing real ontologies order enforce
definability known case (Franconi, Ngo, & Sherkhonov, 2012c).
happens intuitively obvious answer query found
available data (that is, query definable database), mediating
ontology entail definability. introduce novel problem definability
abduction solve completely data exchange scenario.
thank anonymous reviewers useful comments got earlier versions paper. wish thank Alex Borgida, Tommaso Di Noia, Umberto Straccia,
David Toman, Grant Weddell fruitful discussions topics
paper.

Appendix A. Proofs
A.1 Proofs Section 2
Proposition 1
Proof.

Let

Asna = { | dom() = X, rng() C, I(SNA)M (KB)E(DB) : |= Q[X/] }

Auna = { | dom() = X, rng() C, I(UNA)M (KB)E(DB) : |= Q[X/] }
Since SNA stricter UNA, i.e. I(SNA) I(UNA), have: Auna Asna trivially.
Let Asna .
/ Auna interpretation = hI , embedding
DB satisfying UNA (KB) 6|= Q[X/] . Let us construct new


interpretation J = hJ , J embedding DB follows:


J := (I \ {aI | C}) C;
903

fiFranconi, Kerhet, & Ngo



constant C, aJ := a;


every predicate P P, PJ constructed PI replacing element
aI PI , constant, a.
Obviously, J satisfies SNA J isomorphic. Since first-order logic sentences cannot distinguish two isomorphic structures, J 6|= Q[X/] contradicts
assumption Asna . Therefore Auna .
A.2 Proofs Section 4
Proposition 2.
b[X] exact reformulation Q[X] , KB |= X.Q[X] Q
b[X] . Then,
Proof. Since Q

b[X] ,
model (KB) substitution : X 7 have: I, |= Q[X] Q
b[X] ).
equivalent (I, |= Q[X] I, |= Q
Now, let substitution { | dom() = X, rng() = C, (KB)
E(DB) : |= Q[X/] }, = h, model KB embedding DB (if
any). Let := composition substitution interpretation
function (i.e. (x) = iff (x) = c C cI = a). I, |= Q[X]
b[X] |= Q
b
b
|= Q[X/] I, |= Q
[X/] . Summing up: |= Q[X/] |= Q[X/] .
b[X/] }.
Hence, { | dom() = X, rng() = C, (KB) E(DB) : |= Q
inverse inclusion proved similarly.
Theorem 3.
Proof. First recall assume SNA. order prove theorem, one needs
following two propositions.
Proposition 3 (Domain Independence). query Q[X] domain independent iff
every two interpretations = hI , J = hJ , J agree interpretation
predicates PQ (and constants C), every substitution : X 7 J
have:
rng() I, |= Q[X]
iff
rng() J J , |= Q[X] .
Proof. () Obviously, second part proposition holds, query domain
independent.
() Suppose, query domain independent. Let = hI , J = hJ , J
two interpretations, agree interpretation predicates PQ
(and constants C), I|PQ C = J |PQ C . Let us fix substitution : X 7 J
(if query closed, omit everything, concerns substitution
proof) that:
rng() I, |= Q[X] .
(2)
904

fiExact Query Reformulation DBs FO DL Ontologies

0

0

Let us consider interpretations 0 = hI , J 0 = hJ , J i, I|PQ C =
0
0
0
0 |PQ C

= J |PQ C = J |PQ C , P P \ PQ : P = = P J . Let us consider
0 . domain interpret predicates constants, occurring
Q[X] equally. Therefore, since I, |= Q[X] (by (2)), 0 , |= Q[X] .
Let us consider interpretations 0 J 0 . construction, agree interpretation predicates constants. Therefore, apply definition domain
independence them. Then, since
rng() 0 , |= Q[X] ,

(3)

rng() J J 0 , |= Q[X] .

(4)

have,
interpretations J J 0 domain interpret predicates
constants, occurring Q[X] equally. Thus, (4),
rng() J J , |= Q[X] .

(5)

Therefore, (2) = (5). Similarly (5) = (2), proposition proved.

Proposition 4. Q[X] domain independent, interpretation = h,
substitution : X 7 , I, |= Q[X] , following holds:
rng() adom((Q[X] ), I).
Proof. Assume, X = {x}, Q one free variable x (the proof easily
extended general case).
Let us prove contradiction. Suppose, exists substitution {x b}
I, {x b} |= Q(x) b \ adom((Q(x)), I). Let us consider interpretation 0 =
h {a}, i, brand-new element, appear . 0 , {x
b} |= Q(x) domain independence Q(x). Consider another interpretation
00
00 = h {a}, occurrence b interpretation predicate replaced
element a. words, n-ary predicate P P \ (Q(x)), (. . . , a, . . .)
00
0
P iff (. . . , b, . . .) P (since supposition b appear interpretations
predicates query). Interpretations predicates constants
same. 00 satisfies SNA (even b C). Then, since 0 , {x b} |= Q[X] ,
construction 00 have: 00 , {x a} |= Q(x), changed interpretations
predicates, appear query. since 0 00 domain
agree interpretations predicates Q(x) constants, following
holds: 0 , {x a} |= Q(x).
Let us consider interpretations = h, 0 = h {a}, i.
interpretation function. Therefore, since Q(x) domain independent 0 , {x
a} |= Q(x), have: rng({x a}) . . contradiction,
supposition 6 .
905

fiFranconi, Kerhet, & Ngo

prove theorem itself.
L := { | dom() = X, rng() C, (KB) E(DB) : |= Q[X/] };
b DB), = hC, E(DB) : I|P
R := { | dom() = X, rng() adom((Q),

DB C

b[X/] }.
|= Q

b
Let L. (KB) E(DB) have: |= Q[X/] |= Q
[X/] ,
Proposition 2.
Consider J = hC, embedding DB. J agree interpretations C
b P subset PDB .
(since SNA) predicates set (Q)
b[X] domain independent, Proposition 3 have: J |= Q
b
Then, since Q
[X/] . Since
b
b
b
(Q[X] ) PDB C, J |P C |= Q
. Since Q[X] domain independent, Proposition
DB

[X/]

b[X] ), J ). adom((Q
b[X] ), J ) = adom((Q
b[X] ), DB),
4 have: rng() adom((Q
b[X] ) PDB C. Therefore, rng() adom((Q
b[X] ), DB).
assume SNA (Q
R and, hence, L R.
b[X] ), DB)). J = hC, embedding DB
Let R (rng() adom((Q
b
b
have: J |PDB C |= Q
[X/] . J |= Q[X/] . Consider (KB) E(DB). J
b
agree interpretations C (since SNA) PDB . Since (Q
) PDB C
[X/]

b[X] domain independent, Proposition 3 have: |= Q
b
b
Q
[X/] . Since Q[X] exact
reformulation Q[X] KB PDB , Proposition 2 have: |= Q[X/] .
L and, hence, R L.
Theorem 3 proved completely.
A.3 Definitions Proofs Section 5
Proposition 5. Let KB domain independent ontology. interpretation = hI ,
model KB, J = hJ , J i, = J , also model KB.
Proof. Let sentence KB. Then, since model KB, |= . domain
independent, KB domain independent. Hence, since = J , J |= . Thus, J
model sentence KB. means, J model KB.

Proposition 6. Let KB ontology, let Q[X] query domain independent
respect KB. exact reformulation Q[X] KB (over set predicates)
also domain independent respect KB.
b[X] exact reformulation Q[X] KB (over set predicates),
Proof. Let Q


= h , J = hJ , J two models KB = J ,
: X 7 J substitution
b[X] .
rng() I, |= Q
b
Then, since Q[X] exact reformulation Q[X] , have: I, |= Q[X] . Then, since Q[X]
domain independent respect KB, have:
rng() J J , |= Q[X] .
b[X] exact reformulation Q[X] , have: J , |= Q
b[X] . Thus, Q
b[X]
again, since Q
domain independent respect KB definition.

906

fiExact Query Reformulation DBs FO DL Ontologies

Lemma 1. Let KB domain independent ontology, let Q[X] query
domain independent respect KB. = h, model KB
substitution : X 7 I, |= Q[X] following holds:
rng() adom((Q[X] ), I).
Proof. Without loss generality assume, X = {x}, Q one free variable x
(the proof easily extended general case).
Let us prove contradiction. Suppose I, {x b} |= Q(x), b \
adom((Q[X] ), I). Since KB domain independent, brand-new element a,
appear , interpretation = h {a}, also model KB Proposition
5. Then, since Q(x) domain independent respect KB
interpretation function, I, {x b} |= Q(x).
1
Consider new interpretation 1 = h {a}, constructed occurrence b interpretation predicate replaced element a. words,
1
n-ary predicate P P \ PQ , (. . . , a, . . .) P iff (. . . , b, . . .) P (since supposition
b appear interpretations predicates query).
Then, since I, {x b} |= Q(x) construction 1 have: 1 , {x a} |= Q(x)
(since simply replace b, appear neither constant Q(x)
interpretations predicates Q(x), a). Then, since 1 domain
{a} agree interpretations predicates Q(x) constants
(since assume SNA), have: I, {x a} |= Q(x).
Let us consider interpretations = h, = h {a}, i.
models KB interpretation function . So, since Q(x) domain
independent respect KB I, {x a} |= Q(x), have: I, {x
a} |= Q(x) definition domain independence respect ontology.
contradiction, supposition 6 . lemma proved.
Let set formulas. Adom defined similarly Adom Q , Q
query.
Lemma 2. Let KB domain independent ontology, let Q[X] (X = {x1 , ..., xn })
query domain independent respect KB. following holds:
KB |= X.Q[X] Q[X] |Adom KBQ
Q[X] |Adom KBQ Q0 [X] Adom KBQ (x1 ) ... Adom KBQ (xn ), Q0 [X] Q[X]
that:
Every sub-formula Q[X] form x.(x) replaced x.(x)Adom KBQ (x)
Every sub-formula Q[X] form x.(x) replaced x.Adom KBQ (x)
(x)
Proof. Without loss generality, prove lemma n = 1. case,
write Q(x) instead Q[X] . prove contradiction.
Assume model = hI , KB element I, {x
a} |= Q(x) I, {x a} 6|= Q(x)|Adom KBQ .
907

fiFranconi, Kerhet, & Ngo

construct new interpretation J = hAdom IKBQ C, J predicate
P PKBQ , P J := P , predicate P P \ PKBQ , P J := .
Since KB domain independent, J also model KB Proposition 5. Then,
J , {x a} |= Q(x) Q domain independent respect KB. consequence, however, J , {x a} |= Q(x)|Adom KBQ definition Q(x)|Adom KBQ .
Q(x)|Adom KBQ safe-range construction (see Definition 10). Hence, domain
independent. Therefore I, {x a} |= Q(x)|Adom KBQ . Contradiction.
Assume model = hI , KB element I, {x
a} |= Q(x)|Adom KBQ I, {x a} 6|= Q(x). One lead contradiction similarly
above. Therefore, lemma proved.
Theorem 4.
Proof. theorem proved Theorem 5.
direction. Based Lemma 2, one see exact reformulations Q[X]
also exact reformulations Q[X] |Adom KBQ . Since Q[X] |Adom KBQ safe-range
KB always transformed logically equivalent safe-range ontology KB 0 ,
b[X] found Theorem 5 takes
obviously exact safe-range reformulation Q
0
KB Q[X] |Adom KBQ input exact domain independent reformulation
Q[X] .
direction.
b[X] Q[X]
Suppose, exists exact domain independent reformulation Q
PDB KB. domain independent respect KB. Hence,
Proposition 6, Q[X] domain independent respect KB. Since exists
exact reformulation Q[X] , Q[X] implicitly definable PDB KB
Theorem 1.
theorem proved completely.
order help readers follow easier, recall formal definitions safe-range
safe-range normal form (Abiteboul et al., 1995).
Definition 8 (safe-range normal form). denoted SRNF
first order formula transformed SRNF following steps :
Variable substitution: distinct pair quantifiers may employ variable.
Remove universal quantifiers
Remove implications
Push negation
Flatten and/or
908

fiExact Query Reformulation DBs FO DL Ontologies

Definition 9 (Range restriction formula). denoted rr
Input : formula SRNF
Output : subset free()
Case
R(e1 , ..., en ) : rr() = set variables e1 , ..., en
x = = x, constant : rr() = {x}
x = : rr() =
1 2 : rr() = rr(1 ) rr(2 )
1 2 : rr() = rr(1 ) rr(2 )
1 x = : rr() = rr(1 ) {x, y} rr(1 ) = ; rr() = rr(1 ) {x, y} otherwise
1 : rr() = rr(1 )
x1 : rr() = rr(1 )\{x} x rr(1 ); rr() = otherwise
Note : Z = Z = \Z = Z\ =
Definition 10 (safe-range). formula safe-range iff rr(SRNF()) = free().
Definition 11 (ground safe-range). formula ground safe-range iff substitution free variables constants becomes safe-range.
Observation 1.
1. query Q[X] interpretation = h, following holds:
Adom IQ = adom((Q[X] ), I).
2. Adom Q (x) safe-range.
Theorem 5.
Proof. theorem proved Theorem 8 Theorem 9.
use following lemma proof.
Lemma 3. KB ontology, Q[X] (X = {x1 , . . . , xn }) ground safe-range query
KB |= X. Q[X] 1 (x1 ) . . . n (xn ),

(6)

b[X] := Q[X] 1 (x1 ). . .n (xn )
1 , . . . , n n safe-range formulas, query Q
b
safe-range KB |= X. Q[X] Q[X] .
909

fiFranconi, Kerhet, & Ngo

Proof. Let Q0[X] safe-range normal form query Q[X] , i.e. Q0[X] := SRNF(Q[X] ) =
Y. [XY] , [XY] conjunctive normal form (the safe-range normal form
query prenex normal form). Q0[X] ground safe-range, KB |= Q0[X] Q[X] .
Hence, KB |= X. Q0[X] 1 (x1 ) . . . n (xn ). Let Q00[X] := Q0[X] 10 (x1 ) . . . n0 (xn ),
i0 (xi ) = SRNF(i (xi )). 6, KB |= Q0[X] Q00[X] . hand
b[X] Q00 construction. Summing everything, have: KB |= Q
b[X]
KB |= X. Q
[X]

b[X] safe-range.
Q[X] thing need prove Q
00
0
One see, Q[X] Y. ([XY] 1 (x1 ). . .n0 (xn )) safe-range normal
b[X] . Since Q0 = Y. [XY] ground safe-range, rr([XY] ) \ X = Y0 Y,
form Q
[X]

Y\Y0 exists conjunct x = [XY] , x X. Then, since
i0 (xi ) safe-range, definition range restriction rr([XY] 10 (x1 ). . .n0 (xn )) =
X Y, rr(Y. ([XY] 10 (x1 ) . . . n0 (xn ))) = X = free(Q00[X] ). Therefore,
b[X] safeY. ([XY] 10 (x1 ) . . . n0 (xn )) safe-range definition, hence Q
range.
Let us continue prove theorem.
b using
X = , (Q closed) build exact safe-range reformulation Q
Theorem 8.
Suppose now, X = {x1 , . . . , xn }. Since Q[X] safe-range implicitly definable
PDB , apply Theorem 8 Q[X] construct ground safe-range rewriting Q0[X] expressed PDB KB |= X. Q[X] Q0[X] . Since Q[X] domain independent
(since safe-range), also domain independent respect KB. Hence, Proposition 6, Q0[X] also domain independent respect KB. Moreover, KB safe-range
and, hence, domain independent. Theorem 9:
KB |= X. Q0[X] Adom Q (x1 ) . . . Adom Q (xn ).
second item Observation 1 Adom Q (x) safe-range formula. Lemma 3
b[X] := Q0 Adom Q0 (x1 ) . . . Adom Q0 (xn ) safe-range KB |= X. Q0
query Q
[X]
[X]
b[X] . Since KB |= X. Q[X] Q0 , have: KB |= X. Q[X] Q
b[X] . Therefore,
Q
[X]

b[X] one looking for.
constructed query Q
Theorem 5 proved completely.
A.4 Proofs Section 6
Theorem 7.
Proof. First prove Q implicitly definable formula (1) valid.
g |= X.Q[X] Q
g
Applying syntactic definition implicit definability: KB KB
[X] . Therefore,


replace
X


set

constants
c
,
...,
c
,

following
formula
valid
1
n
V
Vg
e[X/c ,...,c ] ). consequence, (1) valid.
( KB KB)
(Q[X/c1 ,...,cn ] Q
n
1
b[c ,...,c /X] ) Q
b[X/c ,...,c ] Craig interNext, prove KB |= (Q[X] Q
n
0
n1
1
b[X/c ,...,c ] interpolant:
polant (1). Since Q
1

n

910

fiExact Query Reformulation DBs FO DL Ontologies

V
b[X/c ,...,c ]
1. (( KB) Q[X/c1 ,...,cn ] ) Q
n
1
b[X/c ,...,c ] )
: KB |= (Q[X/c1 ,...,cn ] Q
n
1
g Q
e[X/c ,...,c ] )
b[X/c ,...,c ] ((V KB)
2. Q
n
n
1
1
g |= (Q
b[X/c ,...,c ] Q
e[X/c ,...,c ] ).
: KB
n
n
1
1
b PDB , relation KB |= (Q
b[X/c ,...,c ] Q[X/c ,...,c ] ) holds well
Since (Q)
n
n
1
1
From(1)(2) expected statement.
b[X/c ,...,c ] ) PDB (Q
b[c ,...,c /X] ) PDB .
Last least, since (Q
n
n
1
1
b[c ,...,c /X] really explicit definition Q
statements, Q
n
1
Theorem 8.
Proof. need following propositions prove theorem.
Proposition 7. 1 2 safe-range closed iff 1 2 safe-range closed.
Proof. have:
rr(1 2 ) = rr(1 ) rr(2 )
free(1 2 ) = free(1 ) free(2 )
rr(1 ) = rr(1 ) free(1 )
rr(2 ) = rr(2 ) free(2 )
1 2 closed iff f ree(1 ) = free(2 ) =
1 closed iff free(1 ) =
2 closed iff free(2 ) =
1 2 safe-range iff rr(1 2 ) = free(1 2 )
1 safe-range iff rr(1 ) = free(1 )
1 safe-range iff rr(2 ) = free(2 )
Therefore:
1 2 closed iff 1 2 closed
1 2 closed, safe-range iff 1 2 closed, safe-range.

Proposition 8. 1 2 safe-range closed iff 1 2 safe-range closed.
Proof. have:
rr(1 2 ) = rr(1 ) rr(2 )
911

fiFranconi, Kerhet, & Ngo

free(1 2 ) = free(1 ) free(2 )
rr(1 ) = rr(1 ) free(1 )
rr(2 ) = rr(2 ) free(2 )
1 2 closed iff free(1 ) = free(2 ) =
1 closed iff free(1 ) =
2 closed iff free(2 ) =
1 2 safe-range iff rr(1 2 ) = free(1 2 )
1 safe-range iff rr(1 ) = free(1 )
1 safe-range iff rr(2 ) = free(2 )
Therefore:
1 2 closed iff 1 2 closed
1 2 closed, safe-range iff 1 2 closed, safe-range.

Proposition 9. ~x(~x) closed safe-range (~t) closed safe-range
~t constants.
Proof. Obviously, ~x(~x) closed (~t) closed.
Assume (~t) safe-range. Since closed rr(SRNF((~t))) =
SRNF((~t)) must contain subformula form ~z0 (~t, ~z)
~z 6 rr(SRNF(0 (~t, ~z)))
SRNF((~x)) must contain subformula form ~z0 (~x, ~z)
~z 6 rr(SRNF(0 (~x, ~z)))
SRNF((~x)) must contain subformula form ~z0 (~x, ~z)
~z 6 rr(SRNF(0 (~x, ~z))) pushing negation effect formula

rr(SRNF((~x))) =
rr(SRNF(~x(~x))) =
rr(SRNF(~x(~x))) =
~x(~x) safe-range
contradiction.
Proposition 10. ~x(~x) closed safe-range (~t) closed safe-range
~t constants.
Proof. Undoubtedly, ~x(~x) closed (~t) closed.
Assume (~t) safe-range. Since closed, rr(SRNF((~t))) =
SRNF((~t)) must contain subformula form ~z0 (~t, ~z)
~z 6 rr(SRNF(0 (~t, ~z)))
912

fiExact Query Reformulation DBs FO DL Ontologies

SRNF((~x)) must contain subformula form ~z0 (~x, ~z)
~z 6 rr(SRNF(0 (~x, ~z)))
rr(SRNF((~x))) =
rr(SRNF(~x(~x))) =
~x(~x) safe-range
contradiction.
Based propositions, prove Theorem 8 follows.
First, show closed safe-range valid
interpolant. Assume biased tableau . Therefore root node
= {L(), R()}. Based tableau expansion rules propositions, every expansion step = {L(1 ), ..., L(n ), R(1 ), ..., R(m )}, 1 , ..., n
1 , ..., safe-range closed(*) .
need prove interpolant step safe-range closed (**)
induction shape proof set rules Section 6.
Rules closed branches: trivial safe-range closed
(*)
Rules propositional case :
rule (p1)(p2)(p3)(p4) nothing changes, one need prove.
rule (p5), apply Proposition 8, (**) holds.
rule (p6), apply Proposition 7,(**) holds.
Rules first order case :
rule (f1) (f2) (f3) nothing changes, one need prove.
rule (f4), since c occur {1 , ..., n } case c
int
contains R((c)). Therefore {L((c))} = (c). Since x.(x)
safe-range (due (*)) x.I[c/x] safe-range
rule (f5), since c occur {1 , ..., } case c
int
contains L((c)). Therefore {R((c))} = (c). Since x.(x)
safe-range (due (*)) x.I[c/x] safe-range
Rules equality : input formulas closed contain
function symbols, equations ground. Therefore, influence
safe-range property interpolant step.
consequence, Q(~c), KB, KB 0 ,Q0 (~c) closed safe-range
b c) KB Q(~c) KB 0 Q0 (~c).
interpolant Q(~
Theorem 9.
Proof. consequence Lemma 1, Theorem 9 holds.
913

fiFranconi, Kerhet, & Ngo

A.5 Definitions Proofs Section 7
safe-range fragment ALCHOIQ. call axiom (concept) ALCHOIQ
(ground) safe-range, corresponding logically equivalent (open) formula FOL(C, P)
(ground) safe-range. concept C denote corresponding logically equivalent
formula FOL(C, P) one free variable x C(x). Unfortunately concept inclusion
axioms ALCHOIQ ontologies may safe-range: example, axiom male v
female safe-range. easy see axiom C v safe-range
C(x) safe-range D(x) safe-range: observe axiom logically
equivalent formula x. C(x) D(x) FOL(C, P) (which actually saferange normal form). following proposition provides recursive rules deciding whether
ALCHOIQ concept safe-range.
Proposition 11. Let atomic concept, let C ALCHOIQ concepts,
let R either atomic role inverse atomic role. Then:
1. A, {o}, nR, nR.C safe-range;
2. C u safe-range C safe-range safe-range;
3. C safe-range C safe-range safe-range;
4. C safe-range C safe-range.
Proof. enough prove proposition atomic roles order
variables binary atoms first-order logic translation ALCHOIQ concept
affect safe-range property translation. Therefore hereafter assume R
atomic role.
Since atomic concept, A(x) safe-range.
{o}(x) = (x = o) - safe-range.
( nR)(x) = x1 , . . . , xn . R(x, x1 ) . . . R(x, xn ) (x1 6= x2 ) . . . (xn1 6= xn ) safe-range.
( nR.C)(x) = x1 , . . . , xn . R(x, x1 ) . . . R(x, xn ) C(x1 ) . . . C(xn ) (x1 6=
x2 ) . . . (xn1 6= xn ) - safe-range.
Let us prove, (C u D)(x) = C(x) D(x) safe-range C(x)
safe-range D(x) safe-range.
) Let C(x) D(x) safe-range let safe-range normal
forms.
C(x) D(x) safe-range definition.
) Let C(x) D(x) safe-range safe-range normal form (i.e. C(x)
D(x) safe-range normal form). Let us prove contradiction. Suppose,
C(x) D(x) safe-range. C(x) D(x) safe-range definition.
contradiction. Therefore, C(x) safe-range D(x) safe-range.
914

fiExact Query Reformulation DBs FO DL Ontologies

Let us prove, (C D)(x) = C(x) D(x) safe-range C(x)
safe-range D(x) safe-range.
) Let C(x) D(x) safe-range safe-range normal forms.
C(x) D(x) safe-range definition.
) Let C(x) D(x) safe-range safe-range normal form (i.e. C(x)
D(x) safe-range normal form). Let us prove contradiction. Suppose, C(x)
D(x) safe-range. C(x) D(x) safe-range definition.
contradiction. Therefore, C(x) safe-range D(x) safe-range.
Let us prove, C(x) safe-range C(x) safe-range.
) Let C(x) safe-range. Let us prove contradiction. Let C(x) also saferange. C(x) C(x) domain independent. one easily see
(looking definition domain independence), impossible. Therefore,
C(x) safe-range. ) need prove, C(x) safe-range,
C(x) safe-range.
Let us prove induction structure formula. Suppose, item true
subformula formula C(x).
Suppose, C(x) safe-range. Let us consider (using already proved items)
possible cases, C(x) safe-range.
C(x) = (R.D)(x) = y. R(x, y) D(y) y.R(x, y)D(y) - safe-range,
(possibly complex) concept. C(x) = y.R(x, y) D(y)
safe-range definition.
Suppose, C(x) = (D u F )(x) safe-range. D(x) safe-range
F (x) safe-range. Since D(x) F (x) subformulas C(x),
applying current item get: D(x) F (x) safe-range. C(x)
(D(x) F (x)) D(x) F (x) - safe-range, D(x) F (x)
safe-range.
Suppose, C(x) = (D F )(x) safe-range. D(x) safe-range
F (x) safe-range. Since D(x) F (x) subformulas C(x),
applying current item get: either D(x) F (x) safe-range. C(x)
(D(x) F (x)) D(x) F (x) - safe-range, either D(x) F (x)
safe-range.
Suppose, C(x) = D(x) safe-range. need prove, C(x) D(x)
safe-range. Let us prove contradiction. Suppose, D(x) safe-range.
Then, since D(x) subformula C(x), applying current item get:
D(x) C(x) safe-range. contradiction. Hence, C(x) safe-range.
item proved completely.
proposition proved completely.
Proposition 12. ALCHOIQ role inclusion axioms safe-range.
915

fiFranconi, Kerhet, & Ngo

Proof. Let v R role inclusion axiom ALCHOIQ. formula x, y. S(x, y)
R(x, y) first-order logic translation axiom, (x, y) stands (x, y)
preceding role atomic (x, y) stands (y, x) preceding role inverse atomic.
formula safe-range.
Guarded negation first-order logic. recall definition guarded negation firstorder logic (GNFO) given paper Barany et al. (2012). GNFO fragment
first-order logic consisting formulas generated following recursive definition:
::= R(t1 , . . . , tn ) | t1 = t2 | 1 2 | 1 2 | x. |

(7)

ti either variable constant, atomic formula (possibly
equality statement) containing free variables .
Guarded negation fragment ALCHOIQ. consider ALCHOIQGN -
guarded negation fragment ALCHOIQ (i.e. intersection GNFO ALCHOIQ).
say,
concept C ALCHOIQGN concept C ALCHOIQ concept
corresponding first-order logic translation C(x) expressed GNFO;
concept inclusion axiom C v ALCHOIQGN concept inclusion axiom
C ALCHOIQ concepts formula x. C(x) D(x) (which
equivalent first-order translation C v D) expressed GNFO;
role inclusion axiom v R ALCHOIQGN role inclusion axiom R
roles (atomic inverse atomic) formula x, y. S(x, y) R(x, y) ,
(x, y) stands (x, y) preceding role atomic (x, y) stands (y, x)
preceding role inverse atomic, expressed GNFO.
easy see, ALCHOIQ role inclusion axiom ALCHOIQGN role inclusion
axiom. Proposition 12 following holds.
Proposition 13.

ALCHOIQGN role inclusion axioms safe-range.

safe-range role inclusion axioms ALCHOIQ ALCHOIQGN .
definition GNFO ALCHOIQ follows, complex concept C
logic ALCHOIQGN recursively defined follows:
B ::= | {o} | nR
C ::= B | nR.C | nR.C | B u C | C u | C

(8)

atomic concept, R atomic role inverse atomic role, C
ALCHOIQGN concepts (possibly complex).
Note, general, according definition (7) GNFO formulas atleast
operator n 2 GNFO non-guarded inequality statements xi 6=
xj . fix assuming inequality relation actually special binary database
predicate. assumption usual databases.
916

fiExact Query Reformulation DBs FO DL Ontologies

Also strictly speaking nR u C GNFO. Indeed, formula
(x1 , . . . , xn . R(x, x1 ) . . . R(x, xn ) (x1 6= x2 ) . . . (xn1 6= xn )) C(x)
GNFO (R(x, y) stands P (x, y) R stands atomic role P , R(x, y) stands
P (y, x) R stands inverse atomic role P ), easily transformed
logically equivalent GNFO one simply shifting parentheses: x1 , . . . , xn . (R(x, x1 )
. . . R(x, xn ) (x1 6= x2 ) . . . (xn1 6= xn ) C(x)). So, assume, formula
nR u C ALCHOIQGN .
Proposition 14. ALCHOIQGN concepts safe-range.
Proof. Let us prove induction structure ALCHOIQGN concepts defined
(8).
1. A, {o}, nR, nR.C, nR.C (C ALCHOIQGN concept) safe-range
item 1 Proposition 11.
2. atomic concept A, individual role R natural number n
concepts u C, {o} u C nR u C safe-range item 3
Proposition 11 since A, {o} nR safe-range first item.
3. Suppose, ALCHOIQGN concepts C safe-range. concepts
C u C safe-range items 2 3 Proposition 11 respectively.
proposition proved.
Lemma 4. safe-range concept C ALCHOIQ following holds:
C v B1 . . . Bn ,
Bi appears subconcept C one following concepts:
atomic concept A;
{o}, individual name;
nR, R atomic role inverse atomic role, n natural number.
Proof. Let us prove proposition induction safe-range concepts ALCHOIQ.
A, {o}, nR, nR.C safe-range Proposition 11. v A, {o} v {o}, nR v
nR, nR.C v nR.
Suppose C complex safe-range concept proposition holds
safe-range subconcepts C.
1. C = C1 u C2 - safe-range. either C1 C2 safe-range. Let C1 safe-range.
Hence, C1 v B1 . . . Bm , Bi concept aforementioned type.
C1 u C2 v C1 v B 1 . . . B .
2. C = C1 C2 - safe-range. C1 C2 safe-range. Hence, C1 v B1 . . . Bk
C2 v Bk+1 . . . Bm , Bi concept aforementioned type.
C1 C2 v (B1 . . . Bk ) (Bk+1 . . . Bm ) v B1 . . . Bm .
917

fiFranconi, Kerhet, & Ngo

3. C = safe-range. Proposition 11 possible saferange. one following cases takes place.
= D1 u D2 . D1 D2 . reduced case item 2.
= D1 D2 . D1 u D2 . reduced case item 1.
= D1 . D1 D1 . Hence, D1 safe-range subconcept D.
proposition holds D1 and, hence, also C, C D1 .
lemma proved completely.
Lemma 5. ALCHOIQ concept C exists ALCHOIQGN concept C 0
either C C 0 C C 0 .
Proof. Suppose lemma holds ALCHOIQ subconcepts ALCHOIQ
concept C. Let us prove C.
1. Base. A, {o}, nR ALCHOIQGN concepts definition ALCHOIQGN
concept (8).
2. C = nR.D D0 ALCHOIQGN concept D0
D0 . C nR.D0 C nR.D0 . nR.D0 nR.D0
ALCHOIQGN concepts. Hence, item proved.
3. C = D0 ALCHOIQGN concept D0 D0 .
C D0 C D0 D0 . item proved.
4. C = C1 u C2 C10 ALCHOIQGN concept C1 C10 C1 C10 , C20
ALCHOIQGN concept C2 C20 C2 C20 . Consider possible
cases.
(a) C1 C10 C2 C20 . C C 0 , C 0 = C10 u C20 ALCHOIQGN
concept (because C10 C20 ALCHOIQGN concepts).
(b) C1 C10 C2 C20 . C C10 u C20 (C10 C20 ) = C 0 ,
C 0 = C10 tC20 ALCHOIQGN concept (because C10 C20 ALCHOIQGN
concepts).
(c) C1 C10 C2 C20 (the case C1 C10 C2 C20 similar
one). C C10 u C20 . Since C10 ALCHOIQGN concept Proposition
14 safe-range and, hence, Lemma 4 C10 v B1 . . . Bn , Bi
either atomic concept {o} R. C10 C10 u (B1 . . . Bn ) and,
hence, C C10 u (B1 . . . Bn ) u C20 C10 u (B1 u C20 . . . Bn u C20 ).
disjunct Bi u C20 ALCHOIQGN concept (because C2 ALCHOIQGN
concept definition (8) ALCHOIQGN concepts). C 0 = C10 u
(B1 u C20 . . . Bn u C20 ) ALCHOIQGN concept. C C 0 . item
proved.
5. C = C1 C2 (C1 u C2 ). case reduced items 3 4.
918

fiExact Query Reformulation DBs FO DL Ontologies

lemma proved completely.
Corollary 2. ALCHOIQ concept C concept B, either atom
{o} nR, concept B u C equivalent ALCHOIQGN concept.
Proof. Lemma 5 exists ALCHOIQGN concept C 0 either C C 0
C C 0 . B u C B u C 0 B u C B u C 0 . B u C 0 B u C 0
ALCHOIQGN concepts (by definition (8) ALCHOIQGN concepts). Hence,
corollary proved.
Proposition 15. safe-range ALCHOIQ concept equivalent ALCHOIQGN
concept.
Proof. Let C safe-range ALCHOIQ concept. Lemma 4 C v B1 . . . Bn ,
Bi either atom {o} nR. C C u (B1 . . . Bn )
B1 uCt. . .tBn uC. corollary 2 disjunct Bi uC exists ALCHOIQGN
concept Di Bi u C Di . C D1 . . . Dn . concept D1 . . . Dn
ALCHOIQGN concept disjunction ALCHOIQGN concepts. Hence, proposition
proved.
Proposition 16. ALCHOIQGN concept inclusion axioms safe-range.
Proof. Let C v concept inclusion axiom ALCHOIQGN . means
corresponding first-order logic translation x. C(x) D(x) GNFO. Hence, C(x)
D(x) GNFO or, same, C u ALCHOIQGN . easy see,
x. C(x) D(x) safe-range formula C(x) D(x) safe-range,
corresponding ALCHOIQGN concept C u safe-range.
Proposition 14 ALCHOIQGN concept safe-range. proposition proved.
Lemma 6. safe-range ALCHOIQ concept C ALCHOIQ concept
concept C u equivalent ALCHOIQGN concept C 0 u D0 , C 0 D0
ALCHOIQGN concepts.
Proof. Since C safe-range Lemma 4 C v B1 . . . Bn , Bi either
atomic concept {o} R. C C u (B1 . . . Bn ) and, hence, C u
C u (B1 . . . Bn ) u C u (B1 u . . . Bn u D). corollary 2 disjunct
Bn uD ALCHOIQGN concept. Hence, D0 := B1 uDt. . .tBn uD ALCHOIQGN
concept. Since C safe-range Proposition 15 exists ALCHOIQGN concept
C 0 C C 0 . C u C 0 u D0 , C 0 u D0 ALCHOIQGN concept,
C 0 D0 ALCHOIQGN concepts.
Proposition 17. safe-range ALCHOIQ concept inclusion axiom C v transformed concept inclusion axiom C 0 v D0 , C 0 D0 ALCHOIQGN .
Proof. Let C v safe-range ALCHOIQ concept inclusion axiom. corresponding formula x. C(x) D(x) safe-range. first-order logic formula
C(x) D(x) safe-range, or, same, ALCHOIQ concept C u saferange. Proposition 11 C safe-range safe-range.
919

fiFranconi, Kerhet, & Ngo

C safe-range. Lemma 6 exist two ALCHOIQGN concepts C 0
D0 C u logically equivalent ALCHOIQGN concept C 0 u D0 .
x. C(x) D(x) logically equivalent x. C 0 (x) D0 (x). Hence, C v
logically equivalent C 0 v D0 (C 0 D0 ALCHOIQGN concepts).
safe-range. proof similar previous item.
proposition proved completely.
Proposition 18. two ALCHOIQGN concepts C axiom C v
ALCHOIQGN concept inclusion axiom.
Proof. axiom C v logically equivalent first-order logic formula x. C(x)
D(x), C(x) D(x) GNFO. x. C(x) D(x) also GNFO. Hence,
definition ALCHOIQGN concept inclusion axiom axiom C v
ALCHOIQGN concept inclusion axiom.
Propositions 17 18 imply following.
Proposition 19. safe-range ALCHOIQ concept inclusion axiom equivalent
ALCHOIQGN concept inclusion axiom.
consider connection safe-range fragment ALCHOIQ guarded negation fragment ALCHOIQ, ALCHOIQGN . say fragment, mean
set TBox assertions (concept role inclusion axioms) concepts (open formulas)
ALCHOIQ satisfying particular property (e.g safe-range guarded negation). Taking
account propositions 14, 15, 16, 19 13, following theorem.
Proposition 20. safe-range fragment ALCHOIQ ALCHOIQGN equally
expressive.
proves Theorem 10:
Theorem 10 (Expressive power equivalence). domain independent fragment
ALCHOIQ ALCHOIQGN equally expressive.
Theorem 11. ALCHOIQGN TBoxes finitely controllable determinacy concept
queries.
Proof. need prove, ALCHOIQGN TBox (ontology), concept
query Q ALCHOIQGN set database predicates PDB , whenever query
finitely determined database predicates ontology also determined
unrestricted models.
Suppose, Q finitely determined PDB . Theorem 2
e |=fin P means entailment models
follows, Te |=fin PDB Q v Q,
DB
e
finite interpretation database predicates. Hence, particular Te |=fin Q v Q,
|=fin means entailment finite models. Hereafter let one sentence,
first-order logic translation conjunction axioms TBox .
aforementioned entailment have:
e
|=fin ( e
) (x. Q(x) Q(x)).
920

(9)

fiExact Query Reformulation DBs FO DL Ontologies

e
Proposition 14 Q(x) safe-range. Hence, Q(x) Q(x)
safe-range, hence
e safe-range and, hence, Proposition 15 exists
ALCHOIQ concept Q u Q
e C 0 . x. Q(x) Q(x)
e
ALCHOIQGN concept C 0 Q u Q
x.C 0 (x)
following holds:
|=fin ( e
) (x.C 0 (x)).
(10)
x.C 0 (x) GNFO, C 0 (x) GNFO. Since axioms ALCHOIQGN
TBox axioms, sentences e GNFO. sentence e
GNFO.
Therefore right hand side entailment (10) GNFO. (( e
)
(x.C 0 (x))) also GNFO entailment (10) finite model.
Then, since GNFO finite model property, (( e
) (x.C 0 (x))) unsatisfiable. Hence, have:
|= ( e
) (x.C 0 (x)).
e
Since x.C 0 (x) x. Q(x) Q(x),
following holds:
e
|= ( e
) (x. Q(x) Q(x)).
e Theorem 2 means, query Q determined
Te |= Q v Q.
unrestricted models database predicates PDB ontology .
proposition proved.

References
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations Databases. Addison-Wesley.
Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). DL-Lite family
relations. J. Artif. Intell. Res. (JAIR), 36, 169.
Avron, A. (2008). Constructibility decidability versus domain independence absoluteness. Theor. Comput. Sci., 394, 144158.
Barany, V., Gottlob, G., & Otto, M. (2010). Querying guarded fragment. Proceedings
25th Annual IEEE Symposium Logic Computer Science (LICS 2010), pp.
110.
Barany, V., ten Cate, B., & Otto, M. (2012). Queries guarded negation (full version).
CoRR, abs/1203.0077.
Beth, E. (1953). Padoas method theory definition. Indagationes Mathematicae,
15, 330339.
Craig, W. (1957). Three uses Herbrand-Gentzen theorem relating model theory
proof theory. J. Symb. Log., 22 (3), 269285.
Etzioni, O., Golden, K., & Weld, D. S. (1997). Sound efficient closed-world reasoning
planning. Artif. Intell., 89, 113148.
Fan, W., Geerts, F., & Zheng, L. (2012). View determinacy preserving selected information data transformations. Inf. Syst., 37, 112.
Fitting, M. (1996). First-order logic automated theorem proving (2nd edition). Springer.
921

fiFranconi, Kerhet, & Ngo

Franconi, E., Ibanez-Garcia, Y. A., & Seylan, Inanc. (2011). Query answering DBoxes
hard. Electronic Notes Theoretical Computer Science, Elsevier, 278, 7184.
Franconi, E., Kerhet, V., & Ngo, N. (2012a). Exact query reformulation SHOQ DBoxes.
Proc. 2012 International workshop Description Logics (DL-2012).
Franconi, E., Kerhet, V., & Ngo, N. (2012b). Exact query reformulation first-order ontologies databases. Logics Artificial Intelligence - 13th European Conference,
JELIA 2012, pp. 202214.
Franconi, E., Ngo, N., & Sherkhonov, E. (2012c). definability abduction problem
data exchange. Web Reasoning Rule Systems - 6th International Conference
RR 2012.
Gurevich, Y. (1984). Toward logic tailored computational complexity. Computation
Proof Theory, Vol. 1104, pp. 175216. Springer.
Halevy, A. Y. (2001). Answering queries using views: survey. VLDB Journal, 10,
270294.
Marx, M. (2007). Queries determined views: pack views. Proceedings 26th
ACM symposium Principles Database Systems, PODS 07, pp. 2330.
McCune, W. (20052011).
prover9.

Prover9 Mace4.

http://www.cs.unm.edu/~mccune/

Nash, A., Segoufin, L., & Vianu, V. (2010). Views queries: Determinacy rewriting.
ACM Trans. Database Syst., 35, 21:121:41.
Rosati, R. (2011). finite controllability conjunctive query answering databases
open-world assumption. J. Comput. Syst. Sci., 77 (3), 572594.
Seylan, Inanc., Franconi, E., & de Bruijn, J. (2009). Effective query rewriting ontologies DBoxes. Proc. 21st International Joint Conference Artificial
Intelligence (IJCAI 2009), pp. 923925.
ten Cate, B., Franconi, E., & Seylan, Inanc. (2011). Beth definability expressive description logics. Proc. 22nd International Joint Conference Artificial
Intelligence (IJCAI 2011), pp. 10991106.
ten Cate, B., Franconi, E., & Seylan, Inanc. (2013). Beth definability expressive description logics. Journal Artificial Intelligence Research (JAIR), 48, 347414.

922

fiJournal Artificial Intelligence Research 48 (2013) 347-414

Submitted 05/13; published 11/13

Beth Definability Expressive Description Logics
Balder ten Cate

btencate@ucsc.edu

UC Santa Cruz

Enrico Franconi

franconi@inf.unibz.it

Free University Bozen-Bolzano

Inanc Seylan

seylan@informatik.uni-bremen.de

University Bremen

Abstract
Beth definability property, well-known property classical logic, investigated context description logics: general L -TBox implicitly defines
L -concept terms given signature, L description logic,
always exist signature explicit definition L concept? property
studied used optimize reasoning description logics. paper
complete classification Beth definability provided extensions basic description logic ALC transitive roles, inverse roles, role hierarchies, and/or functionality
restrictions, arbitrary finite structures. Moreover, present tableaubased algorithm computes explicit definitions double exponential size.
algorithm optimal also shown smallest explicit definition
implicitly defined concept may double exponentially long size input TBox.
Finally, explicit definitions allowed expressed first-order logic, show
compute single exponential time.

1. Introduction
address Beth definability property (Beth, 1953) context description logics
(DLs). Beth definability property relates two notions definability logic L ,
implicit definability explicit definability. Implicit definability semantic notion:
asks whether interpretation given L -formula fully determined universe
discourse interpretation given predicates models theory
. Explicit definability hand syntactic: asks whether
L -formula set predicates equivalent . Clearly, explicit
definability implies implicit definability. converse holds well, logic L
said Beth definability property. Logics property considered
well-balanced terms syntax semantics since connects model-theoretic
notion implicit definability explicit definability.
Beth definability property naturally formulated DLs slightly changing terminology paragraph above: formulas become concepts, theories become
TBoxes, consists unary binary predicates (respectively called concept names
role names).
c
2013
AI Access Foundation. rights reserved.

fiTen Cate, Franconi, & Seylan

Example 1.1. Consider following ALC-TBox .
Parent
Parent
Father
Mother
Man



v
v
v

hasChild.>
Father Mother
Man
Woman
Woman

concept name Mother implicitly definable = {hasChild, Woman} .
Precisely mean clear present Definition 4.1; intuitively,
mean instances Mother model exactly determined know
domain instances I. fact, spell implicit definition
ALC-concept Woman u hasChild.>. concept explicit definition Mother
|= Mother Woman u hasChild.> (cf. Definition 4.5).
Beth definability DLs found applications optimizing reasoning. first application related extracting equivalent acyclic L -terminology general TBox
L (Baader & Nutt, 2003; ten Cate, Conradie, Marx, & Venema, 2006). acyclic
terminology consists acyclic definitions concept names particular
interest reasoning easier general TBoxes. example, satisfiability ALC-terminology PSpace-complete problem whereas problem
general ALC-TBoxes ExpTime-complete (Donini, 2003). second application
related ontology-based data access setting, assumes existence database
instance (also referred DBox context) TBox may speak
predicates database instance (Seylan, Franconi, & de Bruijn, 2009).
setting, user may ask concept queries signature TBox; idea
find equivalent rewriting original query terms predicates appear
DBox. rewriting exists, determining certain answers query
reduced query answering relational databases, known AC0
data complexity contrast general coNP-completeness concept querying ALC
DBoxes (Seylan et al., 2009).
applications involve computing explicit definitions basis implicit
definitions. Here, problem may always possible DLs, i.e.,
DLs may lack Beth definability property.
Example 1.2. example, model scenario cars, owners,
relationships owners cars. Consider following ALCH-TBox
consisting concept inclusion axioms
SportsCar
FuelEfficientCar
SportsCar
proudOwner.Car

v
v
v
v

Car
Car
FuelEfficientCar
(loves.SportsCar u owns.SportsCar))
(loves.FuelEfficientCar u owns.FuelEfficientCar))

role inclusion axioms
proudOwner v owns
proudOwner v loves
348

fiBeth Definability Expressive Description Logics

concept proudOwner.Car implicitly definable = {owns, loves} ,
sense instances proudOwner.Car model exactly determined
know domain instances roles . Indeed, individual
proud owner car individual owns something he/she loves.
fact left-to-right direction equivalence holds every model follows
immediately role inclusion axioms, similarly, fact (contrapositive
the) right-to-left direction holds models follows immediately TBox
axioms. implicit definition made explicit using role conjunction operator
concept (owns u loves).>. However, shown ALCH-concept
explicit definition proudOwner.Car . formally prove
here, see proof Theorem 4.18 Section 4.2 similar example. particular,
shows ALCH lacks Beth definability property.
natural research agenda case identify DLs Beth definability
property. Since property useful computing explicit definitions basis
implicit definitions, vital question complexity task, terms
time needed compute explicit definitions, terms size explicit
definitions obtained. question first studied ten Cate et al. (2006) weaker
Beth definability property, considers concept names signature.
paper interested general Beth definability property takes
account role names signature. believe natural DLs
DL knowledge base, role names considered part signature. present
worst-case optimal algorithm constructing explicit definitions.
Since work Craig (1957), customary establish Beth definability
via interpolation theorem; work exception. particular, obtain
positive results Beth definability worst-case optimal algorithm constructing
interpolants description logics consider.
contributions paper follows.
obtain complete classification Beth definability property extensions
ALC transitive roles, inverse roles, role hierarchies, and/or functionality restrictions, arbitrary structures (BP) finite structures (BPF).
results summarized Table 1. Note finite model property (FMP)
sub-logics SHOQ shown Lutz, Areces, Horrocks, Sattler (2005); FMP
sub-logics SHIO+ Duc Lamolle (2010); failure FMP
ALCF extensions well-known (cf. Calvanese & Giacomo, 2003).
present constructive algorithm based interpolating tableau calculus
compute explicit definitions ALC considered extensions
Beth definability property. algorithm runs double exponential time computes worst case explicit definition double exponential size concept
implicitly definable. respect, algorithm optimal also show
smallest explicit definition implicitly defined concept may double
exponentially long size input TBox DLs.
consider case explicit definitions allowed expressed firstorder logic. particularly relevant use case computing certain answers
349

fiTen Cate, Franconi, & Seylan



H



F









































FMP
+
+
+
+
+
+
+
+
+
+
+
+
-

BP
+
+
+
+
+
+
+
+
-

BPF
+
+
+
+
+
+
-

Table 1: BP BPF ALC SHIF

query given DBox TBox. present algorithm computes firstorder explicit definition implicitly defined concept single exponential time
DLs BP BPF.

1.1 Related Work
Beth definability property, general sense, first shown hold firstorder logic (Beth, 1953). Beth definability comes different flavors one
interested related projective Beth definability. Here, projective refers
ability specify set predicates . projective version known stronger
Beths original formulation (cf. Hoogland, 2001) first shown hold first-order logic
Craig (1957). Since seminal works Beth Craig, Beth definability
studied many logics.
Lang Marquis (2008), also motivated AI, study propositional variant.
modal temporal variants extensively studied (cf. Gabbay & Maksimova,
2005). k-variable fragment first-order logic, k 2, known lack Beth
definability property, whereas Guarded Packed Fragments satisfy non-projective
version Beth property (cf. Hoogland, 2001). guarded-negation fragment
recently shown Beth definability property well (Barany, Benedikt, & ten
Cate, 2013).
Beth definability practical applications relational databases query rewriting
using exact views (Nash, Segoufin, & Vianu, 2010; Afrati, 2011; Marx, 2007; Pasaila, 2011;
Barany et al., 2013). Here, idea decide answers given query inferred
content collection views (that is, whether theory consisting view
definitions implicitly defines query terms view predicates), and, indeed
case, rewrite query query schema consisting view predicates
350

fiBeth Definability Expressive Description Logics

(that is, explicit definition query terms view predicates). View-based
query rewriting naturally arises various settings, including query optimization, querying
access restrictions, data integration, privacy analysis.
Beth definability also studied DL literature. Similarly relational
database case, finds applications computing explicit definitions basis implicit
definitions (Baader & Nutt, 2003; ten Cate et al., 2006; Seylan et al., 2009; Seylan, Franconi, & de Bruijn, 2010). papers also present results size explicit
definitions obtained implicitly defined concepts. Ten Cate et al. establish
single exponential lower bound triple exponential upper bound ALC.
hard see lower bound proof ten Cate et al. carries Beth definability
property consider. matching single exponential upper bound size explicit
definitions claimed established Seylan et al. (2010) Theorem 1; however,
theorem wrong since crucial step proof, namely Lemma 1, erroneous.
paper, improve single exponential lower bound ten Cate et al. double
exponential correct single exponential upper bound Seylan et al. double exponential, thus obtaining tight complexity bounds. bounds DLs sharp contrast
first-order logic since recursive bound minimal number quantifier
alternations explicit definitions first-order logic (Friedman, 1976). BP first
shown hold ALC Seylan et al. (2009) stronger variant studied
ten Cate et al.. Specifically, show DLs consider support role hierarchies
actually lack BP, whereas satisfy variant BP studied ten Cate et al..
respect, Theorem 10 Seylan et al. (2010) claiming DLs BP erroneous.
mistake proof Theorem 9, presents reduction concept
satisfiability problem w.r.t. TBoxes SHI problem ALC, actually
used computing SHI-interpolants.
Since work Craig (1957), customary establish Beth definability
via interpolation lemma; work exception. interpolation lemma
usually established model-theoretic proof-theoretic argument (Hoogland, 2001).
advantage latter former yields procedure construct
interpolant. Several interpolation properties formulated general TBoxes studied
ALC- (ten Cate et al., 2006; Ghilardi, Lutz, & Wolter, 2006; Konev, Lutz, Walther,
& Wolter, 2009a; Seylan et al., 2009; Konev, Lutz, Ponomaryov, & Wolter, 2010; Lutz
& Wolter, 2011) EL-family DLs (Konev, Walther, & Wolter, 2009b; Lutz, Piro,
& Wolter, 2010; Nikitina & Rudolph, 2012; Lutz, Seylan, & Wolter, 2012a). notable
variant uniform interpolation property. uniform interpolant given L -TBox
set predicates another L -TBox 0 0 uses predicates
logical consequences 0 formulated coincide. paper,
consider uniform interpolation right interpolation property
establishing tight bounds size explicit definitions. witnessed
following observations. Deciding existence uniform interpolant given ALCTBox set predicates known 2-ExpTime-complete (Lutz & Wolter, 2011),
whereas problem formulated interpolation property study
ExpTime. simpler DL EL, uniform interpolants also expensive
non-uniform ones. particular, deciding existence uniform interpolants EL
ExpTime-complete (Lutz et al., 2012a); Nikitina Rudolph (2012) establish triple
351

fiTen Cate, Franconi, & Seylan

exponential tight bounds size uniform interpolants. hand, deciding
existence interpolants, consider paper description logic EL,
PTime problem reduced concept subsumption w.r.t. TBox
EL Lemma 3 Lutz, Seylan, Wolter (2012b).
results paper announced ten Cate, Franconi, Seylan
(2011) extended abstract. current paper extends work full proofs
claimed results new material Section 4.4.
1.2 Outline
start introducing Section 2 DLs study BP reasoning
problems relevant us paper. also fix section first-order
notation standard translation DLs first-order logic. using firstorder logic extensively Section 3.3. hammer nail positive
results, i.e., +, columns BP BPF Table 1, worst-case optimal algorithm
constructing interpolants. Section 3 dedicated interpolation result. Finally,
results BP presented Section 4. Since interpolation results used
prove BP, Section 3 naturally comes Section 4, reader less interested
interpolation results may prefer skip Section 3 initially.

2. Preliminaries
section, introduce description logics study. frequently
used logics expressive ALC-family description logics.
2.1 Description Logics
Let NC NR countably infinite mutually disjoint sets concept names
role names, respectively. reasons become clear moment, also assume
countably infinite subset NR , denoted NR+ , NR \ NR+ also countably infinite.
role names NR+ are, intuitively, designated transitive, allowed
used description logics transitive roles. element NC NR also
called predicate, set NC NR concept role names called signature.
ease exposition, first introduce description logic ALCF I,
define description logics study. concept language ALCF defined
follows:
Concepts:
Roles:

C, ::= > | | C | C u | R.C | 1R
R
::= P | P

NC P NR \ NR+ . concept constructors , t, R.C, 2R
defined abbreviations usual way. Also, slight abuse notation,
sometimes write (P ) , P NR , case refers role name P itself.
ALCFI-TBox finite set concept inclusion axioms (CIAs) C v D, C
ALCF I-concepts.
semantics ALCF I-concepts roles given terms interpretations.
interpretation pair = hI , non-empty set called domain
352

fiBeth Definability Expressive Description Logics

>I
(C)I
(C u D)I
(R.C)I
( 1R)I
(P )I

=
=
=
=
=
=

,
\ C ,
C DI ,
{s | exists hs, ti RI C },
{s | t, u , hs, ti RI hs, ui RI = u},
{hs, ti | ht, si P }.

Table 2: Semantics complex ALCF I-concepts roles
I, function maps concept name NC subset AI
role name P NR binary relation P . anticipation discussion
description logics transitive roles below, require also P NR+ ,
relation P transitive. map extended complex concepts roles means
inductive definitions provided Table 2.
interpretation satisfies (or, model of) CIA C v C DI , satisfies
(or, model of) TBox satisfies every CIA . use notation |= C v
|= express satisfies C v D, respectively, satisfies .
description logic ALCF defined member larger family
description logics. basic description logic ALC defined ALCF without inverse
roles (i.e., without roles form P ) without functionality restrictions (i.e., without
concepts form 1R). X {S, H, I, F}, description logic ALCX extends
ALC
1. Functionality restrictions (as ALCF I) F X,
2. Inverse roles (as ALCF I) X,
3. Transitive roles X. this, mean role names NR+ allowed
used.
4. Role hierarchies H X. this, mean TBox may contain role inclusion
axioms (RIAs) form R v S, R roles, satisfied
interpretation RI .
DLs include transitive roles (S) functionality restrictions (F),
syntactic restriction imposed: whenever 1R occurs concept, R required
simple role respect TBox hand (Horrocks, Sattler, & Tobies, 2000).
simple role is, intuitively, role transitive subrole. formal definition
simplicity follows: let us write R vT either R = roles R1 , . . . , Rn
R1 = R, Rn = S, 1 < n, contains either RIA Ri v Ri+1

RIA Ri v Ri+1
. say R simple respect exist
role vT R form P P P NR+ .
motivation standard syntactic restriction that, without it, basic decision
problems satisfiability concept subsumption respect TBox (defined
below) quickly become undecidable (Horrocks et al., 2000).
353

fiTen Cate, Franconi, & Seylan

X {S, H, I, F} X, customary omit prefix ALC notation ALCX. particular, description logic ALCSHIF (which expressive
description logic consider paper) referred simply SHIF. SHIF also
theoretical basis Web Ontology Language OWL-Lite (Horrocks, Patel-Schneider,
& van Harmelen, 2003), makes important DL practical viewpoint.
L -concept C, set sub(C) consists C subconcepts. concept
C TBox , rol(C, ) denotes set roles occurring C ; sig(C, )
denotes set concept names role names occurring C , i.e., signature
C . use sig(C) abbreviation sig(C, ). size L -concept C
(L -role R), written |C| (resp. |R|), number occurrences symbols needed write
C (resp. R). size L -TBox , written |T |, defined analogously. Later on,
Section 3.3 also consider other, succinct, ways representing concepts.
alternative ways represent functionality restrictions transitive roles
DL literature. example, functionality transitivity axioms form funct R
Trans(R) sometimes treated axioms TBox. Although syntactic differences considered minor far standard reasoning tasks (cf. Section 2.2)
concerned, interpolation results sensitive changes language. example,
opting TBox axioms form funct R instead freely allowing 1R construct
concept language would change expressive power languages consider.
Section 3.1, show ALCF interpolation property. interested reader
invited check proof adapted case allow functionality
restrictions TBox axioms.
2.2 Decision Problems
concept C satisfiable respect TBox exists model
C 6= . CIA C v follows TBox (denoted |= C v D), every model
model C v D. write |= C |= C v |= v C hold
true.
following decision problems relevant us:
Concept satisfiability respect TBox :
Given C , determine C satisfiable w.r.t. .
Concept subsumption respect TBox :
Given C v , determine |= C v D.
problems parametrized description logic L , input concept(s)
TBox specified. two problems reducible (or, accurately,
others complement) logics consider, due fact concept
languages closed negation. fact, problems ExpTime-complete
description logics consider (Tobies, 2001).
decision problems also considered restricted class finite
interpretations, i.e., interpretations whose domain finite set. refer
variants decision problems finite concept satisfiability finite concept
subsumption. Thus, finite concept satisfiability respect TBox problem
deciding whether given concept non-empty denotation finite model
354

fiBeth Definability Expressive Description Logics

given TBox. known (Lutz, Sattler, & Tendera, 2005) finite concept satisfiability
finite concept subsumption also ExpTime-complete description logics
consider here.
finite concept satisfiability problem coincides unrestricted satisfiability problem, say description logic question finite model
property.
Definition 2.1 (Finite model property). DL L said finite model property
(FMP) every L -concept C every L -TBox , C satisfiable w.r.t. ,
finite interpretation model C 6= .
well-known ALCF extensions lack finite model property (Calvanese
& Giacomo, 2003).
2.3 First-Order Translation
well-known correspondence theory modal/description logics description
logic concepts can, general, translated first-order logic formulae one free
variable (Sattler, Calvanese, & Molitor, 2003). translation, concept name
viewed unary predicate symbol role name R viewed binary predicate
symbol first-order language. interpretation, then, corresponds first-order
structure.
assume reader familiar basic notation terminology first-order
logic. particular, use notation I, |= express first-order formula
satisfied structure first-order variable assignment . Sometimes,
convenient use different notation express thing: (x1 , . . . , xn )
first-order formula whose free variables x1 , . . . , xn , a1 , . . . , elements
domain structure I, write |= [a1 , . . . , ] express satisfied
variable assignment sends variable xi corresponding element ai .
Note notation implicitly assumes order free variables ,
always clear context.
Definition 2.2. mapping x SHIF-concepts first-order formulae defined
follows:
x (>) = >,
x (A) = A(x),
x (C) = x (C),
x (C u D) = x (C) x (D),
x (P.C) = y[P (x, y) (C)],
x (P .C) = y[P (y, x) (C)],
x ( 1P ) = z1 z2 [P (x, z1 ) P (x, z2 ) z1 = z2 ],
x ( 1P ) = z1 z2 [P (z1 , x) P (z2 , x) z1 = z2 ],
355

fiTen Cate, Franconi, & Seylan

obtained definition replacing
occurrences x
V
vice versa. SHIF-TBox , (T ) defined (),
(C v D) = x[x (C) x (D)]
(R v S) = xy[xy (R) xy (S)]
where, P NR , xy (P ) = P (x, y) xy (P ) = P (y, x).
translation model-preserving, i.e., SHIF-concepts C, interpretations
I, first-order assignments I, (x) C iff I, |= x (C); similarly
CIAs, RIAs, TBoxes.

3. Constructive Interpolation Tableaux
section provides constructive proof interpolation property DLs
interested in. property essential part proof BP DLs (cf.
Definition 4.7). Resorting interpolation show Beth definability property logic
standard technique since seminal work Craig (1957). start defining
interpolation property.
Definition 3.1 (Interpolation property). DL L said interpolation property
L -concepts C1 , C2 L -TBoxes T1 , T2 , T1 T2 |= C1 v C2 ,
L -concept
sig(I) sig(C1 , T1 ) sig(C2 , T2 ),
T1 T2 |= C1 v I,
T1 T2 |= v C2 .
concept called interpolant C1 C2 hT1 , T2 i.
interpolation property consider defined specifically prove BP. Normally,
Craig interpolation property first-order logic stated follows: first-order
formulae , |= , exists first-order formula sig()
sig() sig(), |= , |= . however relate interpolation property
consider first-order Craig interpolation using standard translation Definition 2.2.
Given L -concepts C1 , C2 L -TBoxes T1 , T2 , standard translation
following equivalences:
T1 T2 |= C1 v C2
(T1 ) (T2 ) |= x (C1 ) x (C2 )
(T1 ) x (C1 ) |= (T2 ) x (C2 )
Thus, setting = (T1 ) x (C1 ) = (T2 ) x (C2 ), know Craigs
Interpolation Theorem first-order logic always first-order interpolant
, |= (Craig, 1957). However, know general whether
interpolant expressed L -concept. reason work
356

fiBeth Definability Expressive Description Logics

DL setting instead full first-order. proofs constructive sense
present effective procedures computing interpolants. also allows us establish
upper bounds size interpolants.
section organized follows. Section 3.1, show directly interpolation property holds ALC ALCF using worst-case optimal tableau (plural:
tableaux) algorithm style Gore Nguyen (2007). Section 3.2, show
interpolation property also holds extensions ALC ALCF transitive inverse roles. Instead establishing results directly using tableaux,
make use satisfiability signature preserving reductions ALC ALCF.
main result says interpolants logics computed double exponential time. Section 3.3, study happens interpolants allowed
expressed full first-order logic show first-order interpolants computed
single exponential time.
3.1 Direct Algorithm Computing Interpolants ALCF
section, assume ALCF-concepts defined recursively Section 2.1 using
also , t, R.C, 2R primitives, i.e., assume that, e.g., 2R constructor
concept language abbreviation ( 1R) anymore. Moreover, assume
concepts negation normal form (NNF), i.e., negation occurs front
concept names. well-known every ALCF-concept easily transformed
equivalent one NNF pushing negation inwards using dualities
concept constructors (Tobies, 2001), e.g., R.C R.C. NNF complement
concept C written C.

Another assumption make ALCF-TBoxes
consist axioms form > v C. assumptions make tableau notation
compatible standard tableau notation DLs. precisely, want
separate rule concept constructor language (Horrocks et al., 2000).
main result present section, namely Theorem 3.10, easily shown
hold case make assumptions.
Definition 3.2. Let C ALCF-concept let ALCF-TBox. concept
closure cl(C, ) C smallest set concepts satisfying following conditions:
C cl(C, );
> v , cl(C, );
cl(C, ) E sub(D), E cl(C, );
R.D cl(C, ) R.D cl(C, ).
rest section, fix two ALCF-concepts C0 , D0 two ALCF-TBoxes Tl ,
Tr . denote union Tl Tr . l stands left r right
naming scheme adopted Fitting (1996). allow us identify TBox
(Tl Tr ) concept (C0 D0 ) inference made. biased concept expression
form C , C ALCF-concept {l, r} bias. Two relevant biased
concept closures cll clr defined follows.
cll = {C l | C cl(C0 , Tl )} clr = {C r | C cl(D
0 , Tr )}.
357

fiTen Cate, Franconi, & Seylan

use Greek letters , denote bias.
tableau rules producing subsets cll clr systematic way. aim,
make use metaphor burden relief. Intuitively, subset cll clr
burden satisfiability depends satisfiability one subsets
cll clr call reliefs .
Definition 3.3. Let cll clr.
(C1 u C2 ) u-burden iff (C1 u C2 ) {(C1 ) , (C2 ) } 6 ;
(C1 C2 ) t-burden iff (C1 C2 ) {(C1 ) , (C2 ) } = ;
( 1R) 1-burden iff ( 1R) {(R.C) | (R.C) } 6 ;
(R.C) -burden iff (R.C) ;
( 2R) 2-burden iff ( 2R) .
burden type burden above.
Definition 3.4. Let cll clr, C burden , = {Dl | > v Tl } {Dr |
> v Tr }. cll clr called C -relief
C = (C1 u C2 ) = {(C1 ) , (C2 ) } ;
C = (C1 C2 ) either = {(C1 ) } = {(C2 ) };
C = ( 1R) = {(R.C) | (R.C) };
C = (R.C) = {C } {D | (R.D) } S;
C = ( 2R) = {D | (R.D) } S.
biased hC0 v D0 , i-tableau (hC0 v D0 , i-tableau short) vertex-labeled
directed graph hV, Ei labeling content : V 2cllclr . Intuitively, edges hg, g 0
constructed algorithm, g 0 .content correspond C -relief g.content. Note
tableau neither required tree directed acyclic graph (DAG)
cycles may occur general. say node g tableau contains clash
either one following holds.
g.content,
{A , (A) } g.content,
{( 1R) , ( 2R) } g.content.
tableau expansion rules given Figure 1 expand tableau making use
semantics concepts. rule said applicable node g condition
satisfied g, rule applied g before, g contain clash. order
guarantee finite expansion, use proxies following way. Whenever rule creates
new node g 0 g, attaching edge hg, g 0 E, tableau searched
358

fiBeth Definability Expressive Description Logics

Ru rule
Condition:
Action:
Rt rule
Condition:
Action:

(C1 u C2 ) u-burden g.content.
E E {hg, g 0 i} g 0 .content , (C1 u C2 ) -relief
g.content.
(C1 C2 ) t-burden g.content.
E E {hg, g1 i, hg, g2 i}, g1 .content 1 , g2 .content 2 ,
1 , 2 (C1 C2 ) -reliefs g.content.

R1 rule
Condition:
( 1R) 1-burden g.content.
Action:
E E {hg, g 0 i} g 0 .content , ( 1R) -relief
g.content.
R rule
Condition:
= {(C1 )1 , . . . , (Cn )n } C iff C - 2-burden
g.content.
Action:
E E {hg, gi | 1 n} 1 n,
gi .content , (Ci )i -relief g.content.
Figure 1: Tableau expansion rules ALCF
node g 00 V g 0 .content = g 00 .content. g 00 found, edge hg, g 00
added E g 0 discarded.
interested deciding |= C0 v D0 . tableau algorithm consists two
phases. first phase starts initial hC0 v D0 , i-tableau = h{g0 }, i,
g0 .content = {(C0 )l , (D
0 )r } {E l | > v E Tl } {E r | > v E Tr }.
expanded repeatedly applying tableau expansion rules way
one rule applicable node time, first applicable rule
list [Ru , Rt , R1 , R ] chosen. first phase continues long rule applicable
T. hC0 v D0 , i-tableau called complete output first
phase tableau algorithm.
Lemma 3.5. first phase tableau algorithm terminates time 2O(n) ,
n = |cll clr|. Moreover complete hC0 v D0 , i-tableau = hV, Ei produces,
|V| 2n |E| 2O(n) .
Proof. definition, first phase continues long rule applicable
node tableau. definition applicability, one rule
applied node tableau.
Let n = |cll clr|. definition proxy, |V| 2n since 2n
distinct subsets cll clr. Combining fact one rule
application per node, obtain 2n bound number rule applications. Now,
easy see rule executes time polynomial n, i.e., execution time
rule bounded nk , k constant. whole running
359

fiTen Cate, Franconi, & Seylan

time first phase 2n nk . is,
k

2n nk = 2n+log n

= 2n+klog n
2O(n) .
remains show bound |E|. definition tableau rules, out-degree
node cannot exceed n. Therefore, |E| n 2n , i.e., |E| 2O(n) .
Let complete hC0 v D0 , i-tableau obtained first phase algorithm. purpose second phase tableau algorithm, i.e., Algorithm 1,
construct following functions:
1. status : V {sat, unsat} total function,
2. int partial function V ALCF-concepts.
g V, values assigned g thesedfunctions denoted g.status
int(g). Intuitively, status node g denotes C g.content C satisfiable
w.r.t. ; int(g), defined, interpolant g.content following sense.
Definition 3.6. Let cll clr. concept called interpolant
F

|= C l C v |= v C r C

F
sig(I) sig( C l C) sig( C r C),
definition Algorithm 1, g V, int(g) defined if,
if, g.status = unsat. order compute int(g) node g V g.status = unsat,
Algorithm 1 uses interpolant calculation rules presented Figures 2, 3, 4.
rules Figure 2 compute int(g) based solely g.content; ones Figure 3 take account
g.content successor g 0 g, values g 0 .content int(g 0 ); finally, ones
Figure 4 take account g.content every successor g 0 g, values g 0 .content
int(g 0 ). invite reader verify that, indeed, whenever Algorithm 1 assigns unsat
g.status, node g tableau, interpolant calculation rule
applied compute int(g). Furthermore, interpolant calculation rule easily
seen sound. example, interpolant calculation rule Cu Figure 3 sound
because, successor g 0 g, g 0 .content (C1 u C2 ) -relief g.content int(g 0 )
interpolant g 0 .content (in sense Definition 3.6) necessarily also
interpolant g.content.
Let = hV, Ei complete hC0 v D0 , i-tableau output second
phase. said open g0 .status = sat; said closed
g0 .status = unsat. determined open second phase,
tableau algorithm returns 6|= C0 v D0 , otherwise returns |= C0 v D0 .
next three results establish important properties tableau algorithm
use prove Theorem 3.10. proofs results require introduction
standard substantial amount notation DL modal logic literature.
order present Theorem 3.10 clearly, defer proofs Appendix C.
360

fiBeth Definability Expressive Description Logics

Algorithm 1 Second phase tableau algorithm
Propagate:

done true.
every g V g.status 6= unsat:
g contains clash,
1. g.status unsat,
lr
rl
2. apply one {Cl , Cr , Cll , Crr
, C , C }, one whose condition satisfied,
calculate int(g),
3. done false.
g 0 V hg, g 0 E, g 0 .status = unsat, g 0 .content (C1 u C2 ) -,
( 1R) -, (R.C) , ( 2R) -relief g.content,
1. g.status unsat,
r6R
R
R
l6R
rR
lR
rR
2. apply one {Cu , Cl61
, Cr61
, ClR
1 , C1 , C , C , C , C }, one whose condition satisfied, calculate int(g),
3. done false.
g1 , g2 V g1 6= g2 , hg, g1 i, hg, g2 E, gi .status = unsat
{1, 2}, gi .content (C1 C2 ) -relief g.content {1, 2},
1. g.status unsat,
2. apply one {Clt , Crt }, one whose condition satisfied, calculate int(g),
3. done false.
done = false.
Assign:
every g V g.status 6= unsat, g.status sat.

361

fiTen Cate, Franconi, & Seylan

Cl rule
Condition:
Action:
Cr rule
Condition:
Action:
Cll rule
Condition:
Action:
Crr
rule
Condition:
Action:
Clr
rule
Condition:
Action:
Crl
rule
Condition:
Action:

l g.content.
int(g)
r g.content.
int(g) >
{C l , (C)
l } g.content, C form 1R.
int(g)
{C r , (C)
r } g.content, C form 1R.
int(g) >
{C l , (C)
r } g.content, C form 1R.
int(g) C
{C r , (C)
l } g.content, C form 1R.
int(g) C


Figure 2: Interpolant calculation rules ALCF (content dependent rules)
Lemma 3.7. Let = hV, Ei output second phase. g V, g.status =
unsat,
1. g.content unsatisfiable w.r.t. ;
2. int(g) defined interpolant g.content;
n

3. |int(g)| O(22 ), n = |cll clr|.
next lemma establishes double exponential upper bound runtime Algorithm 1. consequence interpolant calculation double exponential upper
bound size interpolants (cf. Lemma 3.7).
Lemma 3.8. second phase tableau algorithm, i.e., Algorithm 1, runs time
n
O(22 ), n = |cll clr|.
next proposition establishes soundness completeness algorithm
concept subsumption w.r.t. TBoxes ALCF.
Proposition 3.9. closed hC0 v D0 , i-tableau |= C0 v D0 .
tableau algorithm presented section two phases actually
algorithm compute interpolants double exponential size ALCF.
upper bound optimal results establish Section 4 imply smallest
interpolants double exponential size.
362

fiBeth Definability Expressive Description Logics

Cu rule
Condition:
g 0 .content (C1 u C2 ) -relief g.content.
Action:
int(g) int(g 0 ).
R
Cl61
rule
Condition:
g 0 .content ( 1R)l -relief g.content
biased concept form (R.C)r g.content.
Action:
int(g) int(g 0 ).
r6R
C1 rule
Condition:
g 0 .content ( 1R)r -relief g.content
biased concept form (R.C)l g.content.
Action:
int(g) int(g 0 ).
lR
C1 rule
Condition:
g 0 .content ( 1R)l -relief g.content
biased concept form (R.C)r g.content.
Action:
int(g) int(g 0 )u 1R.
CrR
1 rule
Condition:
g 0 .content ( 1R)r -relief g.content
biased concept form (R.C)l g.content.
Action:
int(g) int(g 0 )t 2R.
l6R
C rule
Condition:
g 0 .content (R.C)l - ( 2R)l -relief g.content,
biased concept form (R.D)r g.content.
Action:
int(g) .
Cr6R rule
Condition:
g 0 .content (R.C)r - ( 2R)r -relief g.content,
biased concept form (R.D)l g.content.
Action:
int(g) >.
ClR
rule

Condition:
g 0 .content (R.C)l - ( 2R)l -relief g.content,
biased concept form (R.D)r g.content.
Action:
int(g) R.int(g 0 ).
CrR
rule
Condition:
g 0 .content (R.C)r - ( 2R)r -relief g.content,
biased concept form (R.D)l g.content.
Action:
int(g) R.int(g 0 ).
Figure 3: Interpolant calculation rules ALCF (single successor dependent rules)
Theorem 3.10. ALCF-concepts C, ALCF-TBoxes T1 , T2 T1 T2 |=
C v exists interpolant C hT1 , T2 computed
time double exponential |T1 | + |T2 | + |C| + |D|.
Proof. Suppose C, ALCF-concepts T1 , T2 , ALCF-TBoxes
T1 T2 = |= C v D. Proposition 3.9, closed hC v D, i363

fiTen Cate, Franconi, & Seylan

Clt rule
Condition:
Action:
Crt rule
Condition:
Action:

g1 .content, g2 .content (C1 C2 )l -reliefs g.content.
int(g) int(g1 ) int(g2 ).
g1 .content, g2 .content (C1 C2 )r -reliefs g.content.
int(g) int(g1 ) u int(g2 ).

Figure 4: Interpolant calculation rules ALCF (multiple successor dependent rules)
tableau = hV, Ei. means g0 .status = unsat, thus Lemma 3.7,

ALCF-concept suchFthat int(g0 ) = interpolant g0 .content. Let
X = >vET1 E = >vET2 E. Since interpolant g0 .content,
|= C u X v I, |= v , sig(I) sig(C u X) sig(D ).
fact |= X > |= , obtain |= C v |= v D;
sig(I) sig(C u X) sig(D ), obtain sig(I) sig(C, T1 ) sig(D, T2 ). Hence
interpolant C hT1 , T2 i. Finally Lemma 3.8, computed time
double exponential |T1 | + |T2 | + |C| + |D|.
end section discussion techniques used. tableau algorithm
defined based tableau algorithm Gore Nguyen (2007). extended
algorithm ALCF added machinery compute interpolants. general
interpolation follows corollary cut-free sequent tableau calculus1 logic (e.g.,
see Rautenberg, 1983; Fitting, 1996; Kracht, 2007); corollary give upper
bounds size computation time interpolants unless calculus combined
decision procedure. section, goal obtain tight upper bounds
size computation time interpolants ALCF. traditional tableau algorithms
DLs, e.g., one Horrocks et al. (2000), also used establish similar results
(Seylan et al., 2009). crucial idea tableau algorithm provide
explicit representation tableau rule applications interpolant
calculated induction rule applications. chose non-traditional DL tableau
algorithm purposes based non-labeled2 tableau calculus
calculi actually commonly used proving interpolation results modal logics
(e.g., Rautenberg, 1983).
3.2 Extending Interpolation Transitive Inverse Roles
section, extend Theorem 3.10 logics order obtain main interpolation result Theorem 3.22. aim, present various polynomial reductions
reasoning one DL another. purpose reductions eliminate constructors language. technique use reductions well-known DL
literature called axiom schema instantiation technique (Calvanese, Giacomo,
& Rosati, 1998; Calvanese, Giacomo, Lenzerini, & Nardi, 2001). Similar techniques also
1. tableau calculus defined set tableau rules.
2. non-labeled tableau calculus provides explicit representation individuals interpretations.

364

fiBeth Definability Expressive Description Logics

appear modal logic (Kracht, 2007). idea behind technique summarized
follows.
DLs syntactic variants modal logics. well-known axiom schema
valid modal logic corresponds certain condition accessibility relation
frames logic (Blackburn, de Rijke, & Venema, 2001). example axiom schema
4 : 2 22 defines class transitive frames. axiom schema instantiation
technique based instantiating axiom schema finite number times
concept cl relevant concept closure, adding instances TBox
obtain equi-satisfiable TBox. resulting TBox free constructor
language instantiated axiom schema.
note input reductions normally concept TBox;
interpolation, given pair concepts C1 , C2 pair TBoxes T1 , T2 .
Therefore, require reductions mix signature sig(C1 , T1 )
sig(C2 , T2 ) uncontrolled way. exactly mean clear
Lemma 3.14 Lemma 3.19. Naturally, calls extra notation.
Definition 3.11. injective function : X NR , X finite subset NR
{P | P NR }, called role renaming P NR , {P, P } 6 X. role
renaming called safe signature range() = .
Given L -concept C role renaming , Z (C) concept obtained C
replacing every occurrence every R dom() (R).
Intuitively, use role renamings, name suggests, rename roles concepts.
need make sure renaming operation well-defined thus, avoid mappings
role inverse domain mapping. Safeness mapping
w.r.t. signature property desire following reductions. start
transitive roles thus, instantiate axiom schema 2 22.
Definition 3.12. Let C0 SIF-concept, SIF-TBox, safe role
renaming sig(C0 , ) dom() = sig(C0 , ) NR+ range() NR+ = .
(C0 , , ) defined ALCF I-TBox S1 (C0 , , )S2 (C0 , , ), S1 (C0 , , ) =
{> v Z (C) | > v C }
S2 (C0 , , ) = {Z (R.C) v Z (R.R.C) | R.C cl(C0 , ) {R, R } NR+ 6= }
Note definition above, signature resulting ALCF I-TBox
equal signature original SIF-TBox C0 contains transitive roles.
Introducing new non-transitive role names necessary allowed
use symbols NR+ logics without transitive roles (cf. Section 2.1). Although
formulation following proposition slightly different one Lemma 6.23
Tobies (2001), proof idea same.
Proposition 3.13. SIF-concept C0 satisfiable w.r.t. SIF-TBox
ALCF I-concept Z (C0 ) satisfiable w.r.t. ALCF I-TBox (C0 , , ),
safe role renaming sig(C0 , ) dom() = sig(C0 , ) NR+ range() NR+ = .
reduction (for concept satisfiability w.r.t. TBoxes) Definition 3.12 satisfies
following property essential extending interpolation results logics
365

fiTen Cate, Franconi, & Seylan

transitive roles. respect, also resembles splitting reduction functions
Kracht (2007).
Lemma 3.14. Let T1 , T2 SIF-TBoxes let C1 , C2 SIF-concepts.
T1 T2 |= C1 v C2 iff (C1 , T1 , ) (C
2 , T2 , ) |= Z (C1 ) v Z (C2 )
safe role renaming sig(C1 u C
2 , T1 T2 ) dom() = sig(C1 u C
2 , T1
T2 ) NR+ range() NR+ = .
Proof. Let safe role renaming sig(C1 u C
2 , T1 T2 ) specified lemma.
use following claims proof.
Claim 3.15. (C1 u C
2 , T1 T2 , ) = (C1 u C
2 , T1 , ) (C1 u C
2 , T2 , ).
Proof claim. () Suppose C v (C1 u C
2 , T1 T2 , ). either C v
2
1
2 , T1 T2 , ). former holds,
2 , T1 T2 , ) C v (C1 u C
(C1 u C
immediately obtain desired result; thus, suppose latter holds. C v
form Z (R.C) v Z (R.R.C), R.C cl(C1 u C
2 , T1 T2 ), {R, R } NR+ 6= .
Definition 3.2 R.C cl(C1 u C
2 , T1 T2 ), obtain R.C cl(C1 u C
2 , T1 )
cl(C1 u C
2 , T2 ). Definition 3.12 fact either R NR+ R NR+ ,
Z (R.C) v Z (R.R.C) (C1 u C
2 , T1 , ) (C1 u C
2 , T2 , ),
wanted show.
() rather easy see direction claim holds.



Claim 3.16. (C1 u C
2 , T1 , ) (C1 u C
2 , T2 , ) = (C1 , T1 , ) (C
2 , T2 , ).
Proof claim. () Suppose C v (C1 , T1 , ) (C
2 , T2 , ). desired result
0
follows immediately C v = > v Z (C ), > v C 0 T1 T2 . Otherwise,
Definition 3.12 C v form Z (R.C) v Z (R.R.C), R.C cl(C1 , T1 )
cl(C
2 , T2 ) either R NR+ R NR+ . R.C cl(C1 , T1 ) cl(C
2 , T2 )
cl(C1 , T1 ) cl(C
2 , T2 ) cl(C1 u C
2 , T1 ) cl(C1 u C
2 , T2 ), obtain R.C cl(C1 u
C
2 , T1 ) cl(C1 u C
2 , T2 ). Definition 3.12 fact either R NR+

R NR+ , Z (R.C) v Z (R.R.C) (C1 u C
2 , T1 , ) (C1 u C
2 , T2 , ),
wanted show.
() Suppose C v (C1 u C
2 , T1 , ) (C1 u C
2 , T2 , ). desired result follows
immediately C v = > v Z (C 0 ), > v C 0 T1 T2 . Otherwise,
Definition 3.12 C v form Z (R.C) v Z (R.R.C), R.C cl(C1 u
C
2 , T1 ) cl(C1 u C
2 , T2 ), either R NR+ R NR+ . R.C cl(C1 u
C
2 , T1 ) cl(C1 u C
2 , T2 ), fact C1 u C
2 6= R.C, Definition 3.2, obtain
R.C cl(C1 , T1 ) cl(C
2 , T2 ). Definition 3.12 fact either R NR+
R NR+ , Z (R.C) v Z (R.R.C) (C1 , T1 , ) (C
2 , T2 , ),
wanted show.

lemma shown following way.
T1 T2 |= C1 v C2 , iff
366

fiBeth Definability Expressive Description Logics

C1 u C
2 unsatisfiable w.r.t. T1 T2 , iff
Z (C1 u C
2 ) unsatisfiable w.r.t. (C1 u C
2 , T1 T2 , ) (Proposition 3.13), iff
Z (C1 u C
2 ) unsatisfiable w.r.t. (C1 u C
2 , T1 , ) (C1 u C
2 , T2 , ) (first
claim), iff
Z (C1 u C
2 ) unsatisfiable w.r.t. (C1 , T1 , ) (C
2 , T2 , ) (second claim), iff
(C1 , T1 , ) (C
2 , T2 , ) |= Z (C1 ) v Z (C2 ).

need similar reduction eliminate inverse roles. De Giacomo (1996) presents
method reduce converse-PDL satisfiability PDL satisfiability using axiom schema
instantiation technique. Since DLs notational variants PDLs, technique
easily adapted DLs done Calvanese et al. (1998, 2001). idea instantiate
converse-PDL axiom schemas []h [ ]hi.
Definition 3.17. Let C0 ALCF I-concept, let ALCF I-TBox,
safe role renaming sig(C0 , ) dom() consisting inverse roles appearing
C0 , range() NR+ = . (C0 , , ) defined ALCF-TBox
I1 (C0 , , ) I2 (C0 , , ), I1 (C0 , , ) = {> v Z (C) | > v C }
I2 (C0 , , ) = {Z (C)

v Z (R .R.C)

| R.C cl(C0 , )}
Note definition above, signature resulting ALCF-TBox
equal signature original ALCF I-TBox C0 contains inverse roles.
Proposition 3.18 establishes correctness reduction concept satisfiability w.r.t.
TBoxes. full proof proposition given Seylan (2012).
Proposition 3.18. ALCF I-concept C0 satisfiable w.r.t. ALCF I-TBox
ALCF-concept Z (C0 ) satisfiable w.r.t. ALCF-TBox (C0 , , ),
safe role renaming sig(C0 , ) dom() consisting inverse roles appearing
C0 range() NR+ = .
following property reduction useful interpolation results.
Lemma 3.19. Let T1 , T2 ALCF I-TBoxes let C1 , C2 ALCF I-concepts.
T1 T2 |= C1 v C2 iff (C1 , T1 , ) (C
2 , T2 , ) |= Z (C1 ) v Z (C2 )
safe role renaming sig(C1 u C
2 , T1 T2 ) dom() consisting
inverse roles appearing C1 u C
2 T1 T2 range() NR+ = .
Proof. following claims shown analogously Claim 3.15 Claim 3.16, respectively.
Claim 3.20. (C1 u C
2 , T1 T2 , ) = (C1 u C
2 , T1 , ) (C1 u C
2 , T2 , ).
Claim 3.21. (C1 u C
2 , T1 , ) (C1 u C
2 , T2 , ) = (C1 , T1 , ) (C
2 , T2 , ).
367

fiTen Cate, Franconi, & Seylan

argument last step proof Lemma 3.14.
Theorem 3.22. Let L ALC extensions constructors {S, I, F}.
L -concepts C1 , C2 L -TBoxes T1 , T2 , T1 T2 |= C1 v C2 , exists
interpolant C1 C2 hT1 , T2 computed time double exponential
|T1 | + |T2 | + |C1 | + |C2 |.
Proof. Theorem 3.10 already covers case L = ALCF.
L = ALC. tableau algorithm ALCF (with proved Theorem 3.10)
used without modification decide concept satisfiability w.r.t. TBox ALC.
words, given ALC-concepts C1 , C2 ALC-TBox = T1 T2 , check
|= C1 v C2 using algorithm. Observe execution
algorithm, R1 never applied clashes involving concept
form 1R. algorithm constructs closed hC1 v C2 , i-tableau, interpolant
calculation algorithm calculate interpolant ALCF. Since R1 never applied
first phase clash involving concept form 1R resulting
tableau, interpolant calculation rules producing concepts form 1R 2R,
rR
namely ClR
1 , C1 , ones Figure 2, never applied second phase. Hence
resulting interpolant actually ALC-concept. always interpolant
T1 T2 |= C1 v C2 double exponential upper bound computation time
shown Theorem 3.10.
L {ALCI, ALCF I}. Let C1 , C2 L -concepts T1 , T2 L -TBoxes
T1 T2 |= C1 v C2 . Let role renaming specified Lemma 3.19: role
renaming always exists. Lemma 3.19, (C1 , T1 , ) (C
2 , T2 , ) |= Z (C1 ) v
Z (C2 ), C1 , C2 ALC-concepts (ALCF-concepts) (C1 , T1 , ), (C
2 , T2 , )
ALC-TBoxes (respectively ALCF-TBoxes). compute interpolant Z (C1 )
Z (C2 ) hI (C1 , T1 , ), (C
2 , T2 , )i time double exponential
size input.
1. sig(I) sig(Z (C1 ), (C1 , T1 , )) sig(Z (C2 ), (C
2 , T2 , )),
2. (C1 , T1 , ) (C
2 , T2 , ) |= Z (C1 ) v I,
3. (C1 , T1 , ) (C
2 , T2 , ) |= v Z (C2 ).
Let 1 restriction rol(C1 , T1 ) 2 restriction rol(C
2 , T2 );
set 1 = range(1 ) 2 = range(2 ). Intuitively, 1 2 exactly sets
new role names introduced (C1 , T1 , ) (C
2 , T2 , ), respectively. easy
see sig(Z (C1 ), (C1 , T1 , )) sig(C1 , T1 ) 1 , sig(Z (C2 ), (C
2 , T2 , ))
sig(C2 , T2 ) 2 . item 1 above,
sig(I) (sig(C1 , T1 ) 1 ) (sig(C2 , T2 ) 2 )
simple distributivity argument, obtain
sig(I) (sig(C1 , T1 ) sig(C2 , T2 )) (1 2 )
(sig(C1 , T1 ) 2 ) (sig(C2 , T2 ) 1 )
368

fiBeth Definability Expressive Description Logics

Since sig(C1 , T1 ) 2 = sig(C2 , T2 ) 1 = ,
sig(I) (sig(C1 , T1 ) sig(C2 , T2 )) (1 2 )
let L -concept obtained replacing occurrences
role name P 1 2 role R (R ) = P . Since injective,
well defined. Moreover, Z (D) = I.
claim every P 1 2 , role name R (R ) = P sig(C1 , T1 )
sig(C2 , T2 ). Suppose P 1 2 . P range(1 ) range(2 ). Since 1 2
defined restrictions rol(C1 , T1 ) rol(C2 , T2 ), respectively,
R rol(C1 , T1 ) rol(C2 , T2 ) (R ) = P . R sig(C1 , T1 ) sig(C2 , T2 ).
claim shown, sig(I) (sig(C1 , T1 ) sig(C2 , T2 )) (1 2 ),
construction D, sig(D) sig(C1 , T1 ) sig(C2 , T2 ). Moreover, Z (D) = I,
items 2 3 above, Lemma 3.19, obtain T1 T2 |= C1 v T1 T2 |= v C2 .
Hence interpolant C1 C2 hT1 , T2 i. easy see time
required compute stated theorem.
L {S, SI, SF, SIF}. follows, let L 0 L without transitive role
constructor, e.g., L = SIF, L 0 = ALCF I. know L 0 satisfies
stated theorem. Suppose C1 , C2 L -concepts T1 , T2 L -TBoxes
T1 T2 |= C1 v C2 . proof proceeds analogously inverse role case,
except course use Lemma 3.14.
conclude, shown logic L stated theorem constructive way
compute interpolant, one exists, time double exponential size input.
Hence theorem follows.
3.3 Shorter First-Order Interpolants
show interpolation algorithm adapted compute first-order
interpolants single exponential time. proof proceed along following lines.
First show double exponential size interpolants due
repeated occurrence subformulas algorithm yields single exponential size
interpolants using succinct (DAG-shaped opposed tree-shaped) concept representation. Next apply idea implicit work Avigad (2003), namely succinctly
represented first-order formulas transformed polynomial time equivalent ordinary tree-shaped first-order formulas structures least two elements.
allows us compute single exponential first-order interpolants structures least
two elements. that, show single exponential interpolants structures
one element constructed reduction propositional logic. combining interpolants obtained via two methods, finally obtain desired single exponential
first-order interpolant arbitrary structures.
Step 1: Singly-exponential interpolants via succinct representation start
defining notions allow us represent DAG-shaped concepts.
Definition 3.23. Fix description logic L . axiom form C, NC
C L -concept, called concept definition axiom L (or, L -CDA). Let
369

fiTen Cate, Franconi, & Seylan

signature. acyclic terminology L set L -CDAs
= {A1 C1 , . . . , Cn }
{A1 , . . . , } = sig(Ci ) {A1 , . . . , Ai1 } {1, . . . , n}.
succinct-L -concept pair hA, i, acyclic terminology
L concept name belonging sig(T ) \ . unfolding succinct-L concept hA, L -concept obtained repeatedly applying
CDAs , i.e., replacing occurrences left-hand side right-hand side,
CDA applied.
Note acyclic terminologies well-known DL literature (Baader & Nutt,
2003).
Example 3.24. Let consist following.
Woman Person u Female
Man Person u Male
Human Woman Man
acyclic terminology {Person, Female, Male}. unfolding succinctconcept hHuman,
(Person u Female) (Person u Male).
unfolding succinct-concept general exponentially longer.
Proposition 3.25. Let L description logic. succinct-L -concept hA,
O(1)
unfolding C, |C| 2|T | .
Theorem 3.26. Let L ALC extensions constructors {S, I, F}.
L -concepts C1 , C2 L -TBoxes T1 , T2 , T1 T2 |= C1 v C2 , exists
succinct-L -concept hA, sig(C1 , T1 ) sig(C2 , T2 )
unfolding hA, interpolant C1 C2 hT1 , T2 i,
hA, computed time single exponential |T1 | + |T2 | + |C1 | + |C2 |.
Proof. Let L one DLs mentioned theorem, let T1 T2 |= C1 v C2 ,
T1 , T2 L -TBoxes C1 , C2 L -concepts, let = |T1 | + |T2 | + |C1 | + |C2 |.
proof Theorem 3.22, first reduce T1 T2 |= C1 v C2 T10 T20 |= D1 v D2 ,
T10 , T20 ALC-TBoxes (ALCF-TBoxes) D1 , D2 ALC-concepts (resp. ALCFconcepts).
show interpolant calculation step Algorithm 1 ALCF (and thus ALC,
see Figures 2, 3, 4) modified compute succinct-concept single exponential size
interpolant, instead concept.
associate every node g tableau distinct fresh concept name Xg .
new algorithm still uses interpolation calculation rules instead directly
assigning interpolant every node g g.status = unsat, construct acyclic
terminology 0 sig(D1 , T10 ) sig(D2 , T20 ), acyclic terminology makes use
370

fiBeth Definability Expressive Description Logics

new concept names Xg , unfolding succinct-concept hXg , 0
interpolant g.content whenever g.status = unsat. set 0 initialized empty
set, throughout computation algorithm, 0 extended natural way.
instance, suppose Clt applied g. Clt adds 0 CDA Xg Xg1 Xg2 ,
g1 g2 successors node g tableau. Another example
l
r } g.content. Clr adds
clash rule. Suppose Clr
applied g {C , (C)

0
0
CDA Xg C. Lemma 3.5, follows |T | 2O(m) ; definition
Algorithm 1, follows 0 acyclic terminology sig(D1 , T10 ) sig(D2 , T20 ).
Moreover, T10 T20 |= D1 v D2 , Xg0 C 0 . hXg0 , 0
succinct-concept sig(D1 , T10 ) sig(D2 , T20 ) unfolding easily shown
interpolant D1 D2 T10 T20 .
way similar proof Theorem 3.22, i.e., replacing back newly introduced role names inverse transitive roles 0 originals, obtain new
terminology 00 . unfolding hXg0 , 00 guaranteed interpolant C1
C2 hT1 , T2 i. Moreover, hXg0 , 00 size single exponential m.
rest section, purpose obtain equivalent first-order formula
given succinct-concept polynomial time. make use standard translation
(see Definition 2.2). following, distinguish DL interpretations
first-order structures (we choose unary binary predicates first-order
language symbols NC NR , respectively).
Step 2: Singly-exponential FO interpolants interpretations two elements first-order formula (x) interpretation = hI , ,
write I, |= (x) first-order assignment (x) =
I, |= (x). |=2 , denote restriction relation |= considers
interpretations = hI , |I | 2. Similarly, |==1 , denote restriction
relation |= considers interpretations = hI , |I | = 1.
proof following theorem inspired result Avigad (2003),
states that, structures least two elements, one efficiently eliminate acyclic
definitions proofs. Theorem 3.27 viewed adaptation result
first-order translation succinct-concepts description logic.
Theorem 3.27. Given succinct-SHIF-concept hB, signature , construct polynomial time first-order formula (x) , |=2 (x) x (C),
C unfolding hB, i.
proof Theorem 3.27 based lemma state next. expository
reasons, convenient state lemma terms structures constant
symbols. constant symbols needed Theorem 3.27. used
make statement proof following lemma readable.
Lemma 3.28. Given acyclic terminology = {A1 C1 , . . . , Cn } SHIF,
construct polynomial time first-order formula (x, y1 , . . . , yn , z) additional
constant symbols 0 1, that, interpretations satisfying 0I 6= 1I ,
elements a, ~b, c (where ~b = b1 , . . . , bn ),
371

fiTen Cate, Franconi, & Seylan

|=

(
1I
[a, ~b, c] ~b = k k {1, . . . , n}, c =
0I

CkI
otherwise


0I} 1I |0I {z
k = 0
0I} Ck unfolding succinct-concept hAk , i.
| {z
k1 times

nk times

Proof. define induction number n CDAs . n = 1,
simply define (x, y, z)
(x, y, z) = (y = 1) ((x (C1 ) z = 1) (x (C1 ) z = 0))
Now, let n > 1 let 0 obtained removing last CDA. words,
let = 0 {An Cn }. induction hypothesis, formula 0 (u, ~v , w) satisfying
required conditions w.r.t. 0 (where ~v = v1 , . . . , vn1 ). distinguish following
cases:
1. Cn atomic concept functionality restriction signature . case,
define follows, ~y = y1 . . . yn ~v = v1 . . . vn1 .
(x, ~y , z) = u, ~v , w(T 0 (u, ~v , w) x = u ~y = ~v 0 z = w)
(~y = 0 01 ((x (Cn ) z = 1) (x (Cn ) z = 0))))
V
Here, ~y = ~v 0 shorthand formula
i<n yi = vi yn = 0, and, similarly,
V
~y = 0 01 shorthand formula i<n yi = 0 yn = 1.
2. Cn form Ai < n. case, define follows:
(x, ~y , z) = u, ~v , w 0 (u, ~v , w)
((x = u ~y = ~v 0 z = w)

(~y = 0 01 u = x ~v = ((w = 1 z = 0) (w = 0 z = 1))))
Here, notation conventions apply V
previous item. addition, ~v =
used shorthand formula vi = 1 j6=i vj = 0. notations also
used following items.
3. Cn form Ai uAj i, j < n. first attempt, define (x, ~y , z) follows:
(x, ~y , z) = u, ~v , w u0 , ~v 0 , w0 0 (u, ~v , w) 0 (u0 , ~v 0 , w0 )
((x = u ~y = ~v 0 z = w)
(~y = 0 01 u = u0 = x ~v = ~v 0 = j

(w = w0 = z = 1 ((w = 0 w0 = 0) z = 0))))
works, except fact 0 occurs twice formula, may
result exponential blowup. solve problem replacing conjunction
0 (u, ~v , w) 0 (u0 , ~v 0 , w0 )
u00 , ~v 00 , w00 ((u00 = u~v 00 = vw00 = w)(u00 = u0 ~v 00 = v 0 w00 = w0 ) 0 (u00 , ~v 00 , w00 ))
372

fiBeth Definability Expressive Description Logics

4. Cn form P.Ai < n. difficult case. following
formula expresses required property:
(x, ~y , z) = u, ~v , w 0 (u, ~v , w)
((x = u ~y = ~v 0 z = w)
(~y = 0 01 z = 1 P xu ~v = w = 1)
(~y = 0 01 z = 0

u0 , ~v 0 , w0 (T 0 (u0 , ~v 0 , w0 ) P xu0 ~v 0 = w0 = 0)))
However, before, formula still problem contains two copies
0 . fix two steps. First, bring universal quantifiers front,
transform formula following equivalent formula:
u, ~v , w u0 , ~v 0 , w0 0 (u, ~v , w) 0 (u0 , ~v 0 , w0 )
((x = u ~y = ~v 0 z = w)
(~y = 0 01 z = 1 P xu ~v = w = 1)
(~y = 0 01 z = 0 (P xu0 ~v 0 = w0 = 0)))
Finally, before, replace conjunction 0 (u, ~v , w) 0 (u0 , ~v 00 , w0 )
u00 , ~v 00 , w00 ((u00 = u~v 00 = vw00 = w)(u00 = u0 ~v 00 = v 0 w00 = w0 ) 0 (u00 , ~v 00 , w00 ))
5. Cn form P .Ai < n. case handled like previous one.
Note that, general, Cn could complex concept various Ai < n occur.
However, complex CDAs always decomposed multiple simpler CDAs
kinds, cost polynomial increase size terminology.
clear construction formula obtained satisfies
conditions stated lemma. obtained polynomial-time follows
fact that, inductive definition , previously constructed
formula 0 occurs once.
ready proof Theorem 3.27.
Proof Theorem 3.27. Let succinct-concept hAi , given, = {A1 C1 , . . . ,
Cn }. Let (x) = (x, i, 1) let (x) = u, v(u 6= v 0 (x)), 0 (x)
obtained (x) replacing 0 1 u v, respectively. that,
every interpretation domain least two elements, every ,
following conditions equivalent:
1. I, |= (x)
2. 0 , |= (x), interpretation 0 extends mapping constant
symbols 0 1 distinct elements .
3. 0 , |= C, C unfolding hAi , i.
4. I, |= C, C unfolding hAi , i.
373

fiTen Cate, Franconi, & Seylan

equivalence 1 2 immediate construction . equivalence 2
3 follows Lemma 3.28. equivalence 3 4 immediate, since 0 1
occur C. concludes proof.
Definition 3.29. Let C, L -concepts let T1 , T2 L -TBoxes T1 T2 |=
C v D. first-order formula (x) called FO interpolant C hT1 , T2
following conditions hold:
sig((x)) sig(C, T1 ) sig(D, T2 ),
(T1 ) (T2 ) |= x.x (C) (x),
(T1 ) (T2 ) |= x.(x) x (D).
FO |=2 -interpolant FO |==1 -interpolant defined way above, except
replace occurrences |= |=2 |==1 , respectively.
Proposition 3.30. Let L ALC extensions constructors {S, I, F}.
L -concepts C1 , C2 L -TBoxes T1 , T2 , T1 T2 |= C1 v C2 , exists
FO |=2 -interpolant C1 C2 hT1 , T2 computed time single
exponential |T1 | + |T2 | + |C1 | + |C2 |.
Proof. Suppose T1 T2 |= C1 v C2 . Theorem 3.26, succinct-concept
hA, sig(C1 , T1 ) sig(C2 , T2 ) unfolding hA, interpolant
C1 C2 hT1 , T2 i, hA, computed time single exponential
|T1 |+|T2 |+|C1 |+|C2 |. Theorem 3.27, first-order formula (x)
constructed time polynomial |T | (hence single exponential |T1 |+|T2 |+|C1 |+|C2 |)

sig((x)) sig(I),
|=2 (x) x (I).
follows (x) FO |=2 -interpolant C1 C2 hT1 , T2 whose size single
exponential |T1 | + |T2 | + |C1 | + |C2 |.
Step 3: Singly-exponential FO interpolants interpretations one element
still obtain interpolants structures one element. show
Proposition 3.35. essential idea interpolants structures
singleton domains much different propositional interpolants. First,
give reduction concept subsumption w.r.t. TBoxes interpretations singleton
domains entailment propositional logic.
Definition 3.31. Let C SHIF-concept. mapping PL (C) defined inductively follows.
PL (>) = >,
PL (A) = A,
PL (C) = PL (C),
PL (C u D) = PL (C) u PL (D),
PL (R.C) = AR u PL (C),
PL ( 1R) = >,
374

fiBeth Definability Expressive Description Logics

AP = AP fresh concept name every role name P NR . SHIF-TBox
, define
PL (T ) = {PL (C) v PL (D) | C v } {AR v | R v }.
Here, concept name AP , intuitively, expresses non-emptiness role P .
Note transitive roles ignored translation, semantics
trivially satisfied interpretations whose domain singleton set. SHIF-concept
C, PL (C) ALC-concept without role constructors. view PL (C) propositional
formula (where concept names propositions, identify u
propositional connectives , respectively). Similarly, SHIF-TBox , PL (T )
set ALC CIAs without role constructors, view set propositional
formulae.
Proposition 3.32. Let SHIF-TBox let C, SHIF-concepts.
|==1 C v



PL (T ) |= PL (C) v PL (D).

Proof. () Let |==1 C v D, suppose |= PL (T ), PL (C)I . need
show PL (D)I . Let J obtained restricting domain element
reading interpretation role name P concept name AP .
Formally,
J = {s};
NC , AJ iff AI ;
P NR , P J = {hs, si} AIP , P J = , otherwise.
definition above, trivially follows every P NR+ P J transitive. Moreover, every role R,
hs, si RJ AIR .

(1)

see this, suppose first hs, si RJ . R = P P NR , AIP , i.e., AIR ;
R = P P NR , AIP fact AP = AP
(see Definition 3.31), obtain AIR . Hence AIR . direction, suppose
AIR . R = P P NR , hs, si P J , i.e., hs, si RJ ; R = P
P NR , fact AP = AP , hs, si P J thus, hs, si RJ .
Hence (1) follows.
Claim 3.33. every SHIF-concept C 0 , PL (C 0 )I (C 0 )J .
Proof claim. proof induction structure C 0 . base case,
C 0 = C 0 = >, trivial, boolean cases follow immediately inductive
hypothesis. C 0 = R.D0 , following equivalences:
PL (C 0 )I ;
AIR PL (D0 )I (by semantics);
375

fiTen Cate, Franconi, & Seylan

hs, si RJ (D0 )J (by (1) inductive hypothesis);
(C 0 )J (by semantics).
Finally, C 0 = 1R, since PL (C 0 ) = > , PL (C 0 )I . Moreover,
definition J , (C 0 )J . PL (C 0 )I iff (C 0 )J ,
wanted show.

show J |= , i.e., J satisfies every CIA RIA . J satisfies
every CIA direct consequence previous claim; proceed case
RIAs. Let R v hs, ti RJ . definition J , = t. Hence
w.l.o.g. suppose hs, si RJ . (1), AIR . Since AR v PL (T )
|= PL (T ), AIS . (1) again, implies hs, si J . Hence J satisfies
R v S.
proceed towards goal PL (D)I follows. |= PL (T ), PL (C)I ,
previous claim, obtain C J . Since J |= , follows |= C v
DJ . using previous claim, conclude PL (D)I .
() Let PL (T ) |= PL (C) v PL (D), suppose |= , C , = {s}.
need show DI . Define interpretation J follows:
J = {s};
NC , AJ = AI
P NR , (AP )J = {s} P = {hs, si}, (AP )J = otherwise
first show every role R

AJ
R hs, si R .

(2)


left-to-right, suppose AJ
R . R = P P NR , hs, si P , i.e.,
hs, si RI ; R = P P NR , fact AP = AP ,


AJ
P , implies hs, si R . Hence hs, si R . direction, suppose
J

hs, si RI . R = P P NR , AP , i.e., AJ
R ; R = P
J

P NR , hs, si P , implies AP = AP AR . Hence (2) follows.

Claim 3.34. every SHIF-concept C 0 , (C 0 )I PL (C 0 )J .
Proof claim. proof induction structure C 0 . base case,
C 0 = C 0 = >, trivial, boolean cases follow immediately inductive
hypothesis. C 0 = R.D0 , following equivalences
(C 0 )I ;
hs, si RI (D0 )I (by semantics = {s});
0 J
AJ
R PL (D ) (by (2) inductive hypothesis).

PL (C 0 )J (by semantics).
376

fiBeth Definability Expressive Description Logics

Finally, C 0 = 1R, since = {s}, |= > v 1R, thus, (C 0 )I .
Moreover, PL (C 0 ) = >, PL (C 0 )J . (C 0 )I iff PL (C 0 )J .
show J |= PL (T ). definition, every CIA PL (T ) form (i)
PL (C 0 ) v PL (D0 ), C 0 v D0 ; form (ii) AR v , R v .
J satisfies CIAs form (i) direct consequence previous claim;
focus CIAs form (ii). Let AR v PL (T ) AJ
R . (2),
hs, si RI . Since |= R v , hs, si . (2) again,
AJ
. Hence J satisfies AR v .
proceed towards goal DI follows. C previous claim,
PL (C)J . J |= PL (T ) PL (T ) |= PL (C) v PL (D), obtain
PL (D)J . Using previous claim again, conclude DI .
Proposition 3.35. Let L ALC extensions constructors {S, H, I, F}.
L -concepts C1 , C2 L -TBoxes T1 , T2 , T1 T2 |= C1 v C2 , exists
FO |==1 -interpolant C1 C2 hT1 , T2 computed time single
exponential |T1 | + |T2 | + |C1 | + |C2 |.
Proof. Let L one DLs mentioned theorem let C1 , C2 L -concepts
let T1 , T2 L -TBoxes T1 T2 |= C1 v C2 . immediately follows
T1 T2 |==1 C1 v C2 . Proposition 3.32, T1 T2 |==1 C1 v C2 implies PL (T1 )
PL (T2 ) |= PL (C1 ) v PL (C2 ). Theorem 3.10, interpolant PL (C1 )
PL (C2 ) hPL (T1 ), PL (T2 )i computed time double exponential
|T1 |+|T2 |+|C1 |+|C2 |. However, case dealing propositional formulae
tableau algorithm easily modified construct tree-shaped proof instead
general graph-shaped one eliminating use proxies. fact, described
standard tableau algorithm propositional logic. well-known node
tree polynomial out-degree size input height tree
polynominal size input. inspecting proof Theorem 3.10, one easily
see case computed time single exponential |T1 | + |T2 | + |C1 | + |C2 |.
Finally let concept obtained replacing occurrence concept name
AR R.>. x (D) FO |==1 -interpolant C1 C2 hT1 , T2 i.
easy see time required compute x (D) stated proposition.
Step 4: Putting together result follows, putting
FO |==1 -interpolants FO |=2 -interpolants together:
Theorem 3.36. Let L ALC extensions constructors {S, I, F}.
L -concepts C, L -TBoxes T1 , T2 , T1 T2 |= C v D, exists
FO interpolant (x) C hT1 , T2 (x) computed time single
exponential |T1 | + |T2 | + |C| + |D|.
Proof. Let L one DLs mentioned theorem, let C, L -concepts,
let T1 , T2 L -TBoxes T1 T2 |= C v D. Proposition 3.35,
FO |==1 -interpolant (x) C hT1 , T2 computed time
single exponential |T1 | + |T2 | + |C| + |D|; Proposition 3.30, FO |=2 interpolant (x) C hT1 , T2 computed time single exponential
377

fiTen Cate, Franconi, & Seylan

|T1 | + |T2 | + |C| + |D|. Let
(x) = (yz(y 6= z) (x)) (yz(y = z) (x)).
Claim 3.37. (T1 ) (T2 ) |= x.x (C) (x)



(T1 ) (T2 ) |= x.(x) x (D).

Proof claim. prove first part. proof second part analogous.
Let = hI , model T1 T2 , i.e., (T1 ) (T2 ), first-order
variable assignment I, |= x (C). need show I, |= (x). aim,
show I, |= (yz(y 6= z) (x)) I, |= (yz(y = z) (x)).
First suppose I, |= yz(y 6= z). done prove I, |= (x).
I, |= yz(y 6= z) implies |I | 2. I, |= x (C) (T1 ) (T2 ) |=2
x.x (C) (x), obtain I, |= (x), done.
suppose I, |= yz(y = z). done prove I, |= (x).
I, |= yz(y = z) implies |I | = 1. I, |= x (C) (T1 ) (T2 ) |==1
x.x (C) (x), obtain I, |= (x), done.
Thus, conjuncts (x) satisfied I, . I, |= (x).

assumption sig((x)), sig((x)) sig(C, T1 ) sig(D, T2 ). Since
formulas yz(y 6= z) yz(y = z) introduce new predicates,
sig((x)) sig(C, T1 ) sig(D, T2 ). Therefore, (x) FO interpolant C
hT1 , T2 i. Moreover, since conjuncts computed single exponential time,
(x). Hence theorem follows.

4. Results Beth Definability
section, present main technical contributions paper. first introduce
notions implicit explicit definability concepts define (projective) Beth
definability property, fact primary notions interest paper.
follows, L denotes description logics ALCX X {S, H, I, F}.
Definition 4.1 (Implicit definability). Let C L -concept, L -TBox,
sig(C, ). C implicitly definable if, every two models J
satisfying = J and, P , P = P J , holds C = C J .
words, given TBox, concept C implicitly definable set
instances depends extension predicates domain discourse.
Deciding implicit definability L means, given L -concept C, L -TBox , set
predicates sig(C, ), check whether C implicitly definable .
every predicate P sig(C, ) \ , introduce new predicate P 0 sig(C, ).
e (respectively, Te ) concept (respectively, TBox) obtained replacing every
let C
occurrence predicate P 6 C (respectively, ) P 0 . Lemma 4.2, whose proof
routine adaptation analogous result first-order logic (Boolos, Burgess, & Jeffrey,
2007), provides characterization implicit definability terms entailment. wellknown characterization often used definition implicit definability (Hoogland &
Marx, 2002; Conradie, 2002).
Lemma 4.2. Let C L -concept, L -TBox, sig(C, ). C
e
implicitly definable Te |= C C.
378

fiBeth Definability Expressive Description Logics

particular, Lemma 4.2 reduces implicit definability L concept subsumption
problem L w.r.t. TBoxes. also possible reduce concept subsumption problem
L w.r.t. TBoxes problem deciding implicit definability L .
Lemma 4.3. Let C v L -CIA, L -TBox, = sig(C u D, ),
A0 NC \ . |= C v A0 implicitly definable
{A0 v C u D}.
Proof. () Suppose |= C v D. Let J models {A0 v C u D}
= J P , P = P J . Obviously, J also models .
|= C v D, (C u D)I = (C u D)J = . AI0 = AJ
0 = .
Hence, A0 implicitly definable {A0 v C u D}.
() show contrapositive, i.e., 6|= C v D, A0 implicitly definable
{A0 v C u D}. Suppose 6|= C v D. model
(C u D)I . Let I1 = hI1 , I1 I2 = hI2 , I2

I1 = I2 = ;
AI1 = AI2 = AI , (NC \ A0 );
RI1 = RI2 = RI , R NR ;
AI0 1 = {s} AI0 2 = .
easy see I1 I2 models {A0 v C u D}. Also observe I1
I2 two interpretations domain agree assign
predicates . AI0 1 6= AI0 2 . Hence A0 implicitly definable
{A0 v C u D}.
Using Lemma 4.2 (for upper bound) Lemma 4.3 (for lower bound), following theorem follows immediately, since concept subsumption problem w.r.t. TBoxes
ExpTime-complete description logics question (Tobies, 2001).
Theorem 4.4. ALC extensions constructors {S, H, I, F},
implicit definability ExpTime-complete.
Explicit definability syntactic counterpart implicit definability. Given concept
C, signature , TBox , asks existence concept formulated
C denote set every model .
Definition 4.5 (Explicit definability). Let C L -concept, L -TBox,
sig(C, ). say C explicitly definable L -concept
|= C sig(D) . concept called explicit definition
C .
Proposition 4.6. Let C L -concept, L -TBox, sig(C, ). C
explicitly definable , C implicitly definable .
379

fiTen Cate, Franconi, & Seylan

Proof. Suppose C explicitly definable . concept
e sig(D)
|= C D. implies definition Te C,
e
e
e
e
e
|= C D. |= C |= C monotonicity
e Lemma 4.2, C implicitly definable
|=. yield Te |= C C.
.
Definition 4.7 (Beth definability property). L Beth definability property (BP)
L -concepts C, L -TBoxes , signatures sig(C, ), C implicitly
definable , C explicitly definable .
Observe that, definition, restricts concept names role names
allowed appear explicit definition. obtain weaker version
Beth definability property restricting concept names occurring explicit
definition. called concept-name Beth definability property (CBP). words,
CBP refers existence explicit definitions signatures form NR .
explain later, also reasons interested whether description
logics satisfy Beth definability property restricted class finite interpretations. known Beth definability property, restricted finite structures,
fails first-order logic (see e.g., Hoogland, 2001), spite fact holds
unrestricted case. specifically investigate Beth definability description logics
restricted finite interpretations. call Beth definability property finite
(BPF). Formally, BPF defined way BP, except replace,
definition, occurrences word interpretation model finite interpretation
finite model, replace symbol |= |=f , |=f considers finite interpretations. addition, speak f-implicit definability f-explicit definability.
follows Lemma 4.2 that, L FMP, BP BPF equivalent L .
Hence makes sense specifically study BPF logics without FMP.
4.1 Bounds Size Explicit Definitions
start positive result BP direct application interpolation theorem,
i.e., Theorem 3.22.
Theorem 4.8 (BP). Let L ALC extensions constructors
{S, I, F}. L -concepts C, L -TBoxes , signatures sig(C, ),
C implicitly definable , C explicitly definable ,
explicit definition C computed time double exponential |T | + |C|.
Proof. Let L one DLs stated theorem, C L -concept,
L -TBox, sig(C, ) C implicitly definable .
e (where Te C
e obtained C,
Lemma 4.2, Te |= C C
respectively, replacing occurrences predicates P 6 fresh predicates P 0
e
sig(C, )). Theorem 3.22, interpolant C C
e
e
e
hT , computed time double exponential |T | + |T | + |C| + |C|. Since
e Te ) = , (a) Te |= C v
interpolant, sig(I) sig(C, ) sig(C,
e (b) Te |= C
e v C, Te |= v C,
(b) Te |= v C.
e
|= C follows (a). structure Te , straightforwardly follows
|= C I.
380

fiBeth Definability Expressive Description Logics

e = 2 (|T | + |C|).
time needed compute I, observe |T | + |Te | + |C| + |C|
Hence computed time double exponential |T | + |C|.
proof Theorem 4.8 uses Theorem 3.22. Similarly, use Theorem 3.36 instead, show first-order explicit definitions implicitly defined concepts
computed single exponential time. Note Theorem 4.8 also establishes double
exponential upper bound size explicit definitions considered logics. upper
bound optimal show Theorem 4.11 explicit definitions L
may need double exponentially big.
essential tool proof Theorem 4.11, path-set construction
previously used Lutz (2006) characterize succinctness public announcement
logic compared epistemic logic. path-set construction also used Ghilardi
et al. (2006) establish lower bound size concepts witnessing TBox
conservative extension another TBox.
Definition 4.9. C ALC-concept, path-set PC C defined structural
induction follows, denotes empty sequence denotes concatenation
finite sequences:
P> = PA = {}, NC ;
PC = PC ;
PCuD = PC PD ;
PR.C = {} {R p | p PC }.
Intuitively, PC describes nestings role constructors C. use PC tool
establishing lower bounds size concepts.
Lemma 4.10. every ALC-concept C, |C| |PC |.
Proof. proof induction structure C.
C atomic concept form > (with NC ), then, definition, |C| = 1
|PC | = 1 since PC = {}. Hence |C| |PC |.
Next, let C = D. inductive hypothesis, |D| |PD |. |PD | =
|PD |, obtain |D| |PD |. Finally, fact |D| = |D| + 1, obtain |D|
|PD |. Hence |C| |PC |.
Next, let C = C1 u C2 . inductive hypothesis, |C1 | |PC1 | |C2 |
|PC2 |. implies |C1 |+|C2 | |PC1 |+|PC2 |. fact |C1 uC2 | = |C1 |+|C2 |+1,
obtain |C1 uC2 | |PC1 |+|PC2 |. Finally, |PC1 |+|PC2 | |PC1 uC2 |, |C1 uC2 |
|PC1 uC2 |. Hence |C| |PC |.
Finally, let C = R.D. inductive hypothesis, |D| |PD |. implies
|D| + 2 |PD | + 2. fact |R.D| = |D| + 2, obtain |R.D| |PD | + 2.
Finally, |PD | + 1 = |PR.D |, |R.D| |PR.D |. Hence |C| |PC |.
Theorem 4.11 (Explicit definition lower bound). Let = {R, S} NR . every
n N, ALC-concept Cn ALC-TBox Tn sig(Cn , Tn ), |Tn |
|Cn | polynomial n, Cn implicitly definable Tn , smallest
explicit definition Cn Tn double exponentially long n.
381

fiTen Cate, Franconi, & Seylan

Proof. Fix n N. Let A1 , . . . , pairwise distinct concept names. use
concept names negations represent binary format number {0, . . . , 2n 1}.
precisely, u . . . u A1 represents 0, u An1 . . . u A1 represents 1,
on. Obviously, implies least significant bit position 1. every
{0, . . . , 2n 1}, denote concept represents Ci . Note Ci ,
either Aj Aj conjunct Ci , j {1, . . . , n}.
k {1, . . . , n},
let Xk = A1 u . . . u Ak1 u Ak
let Yk = A1 u . . . u Ak1 u Ak .
Note Xk Yk concept names use abbreviate
CIAs. define Tn ALC-TBox consisting following CIAs.
u . . . u A1 v R. u S.
A1 . . . v R.> S.>
every k {1, . . . , n} ,
Xk v .Yk u
l
((Al u .Al ) (Al u .Al ))
k<ln

Intuitively, last item allows us decrease counter value one flipping
respective bits. Note |Tn | polynomial n Tn satisfiable. fact,
present models Tn Claim 4.14. model Tn , every
every {1, . . . , n}, either AIi (Ai )I virtue interpretation.
Therefore, every exactly one {0, . . . , 2n 1} CiI .
Claim 4.12. Let {1, . . . , 2n 1}.
1. Tn |= Ci v R.Ci1 u S.Ci1
2. Tn |= Ci R.Ci1 S.Ci1
Proof claim. 1, suppose = hI , model Tn , CiI ,
{R, S} = . suffices show (.Ci1 )I .
hs, ti done immediately; therefore, suppose hs, ti . need show
.
Ci1
Ci = Bn u . . . u B1 , Bj = Aj Bj = Aj , j {1, . . . , n}.
Denote B j concept Aj Bj = Aj , else concept Aj Bj = Aj . Since
CiI , exactly one k {1, . . . , n} XkI . CIA
Xk v .Yk u
l
((Al u .Al ) (Al u .Al ))
k<ln

382

fiBeth Definability Expressive Description Logics

Tn , (Bn u . . . u Bk+1 u B k u . . . u B 1 )I . hard see
Ci1 = Bn u . . . u Bk+1 u B k u . . . u B 1 .
, wanted show.
Hence conclude Ci1

2. () Suppose = hI , model Tn CiI . Since 6= 0,
A1 . . . v R.> S.> Tn , Ci (R.>)I Ci (S.>)I . is,

either hs, ti RI hs, ti . cases, Ci1
1. Hence

(R.Ci1 S.Ci1 ) .
() Suppose = hI , model Tn (R.Ci1 S.Ci1 )I .

means Ci1
either hs, ti RI hs, ti .
proceed towards contradiction suppose 6 CiI , i.e., (Ci )I .
definition interpretation, CjI , j 6= j {0, . . . , 2n 1}. j = 0,
u . . . u A1 v R. u S. Tn , immediately get contradiction. j 6= 0,
. Since 6= j, (i 1) 6= (j 1). Thus, binary representation
1, Cj1


1 j 1 must differ least one bit. implies Cj1
Ci1




k {1, . . . , n} Ak 6 Ak . Hence contradiction.
define concepts D0 . . . D2n 1 inductively follows.
D0 = R. u S.
Di = R.Di1 S.Di1
Intuitively, Di shape binary tree (due role names R, S) height
tree O(i). implies |C2n 1 | double exponential n.
Claim 4.13. every {0, . . . , 2n 1}, Tn |= Ci Di .
Proof claim. proof induction i. base case = 0.
axioms Tn , trivially follows Tn |= A1 u . . . R. u S.. words,
Tn |= C0 D0 . Hence claim holds base case.
inductive step, suppose > 0. previous claim, Tn |= Ci R.Ci1
S.Ci1 ; inductive hypothesis, Tn |= Ci1 Di1 . Tn |= Ci
R.Di1 S.Di1 wanted show.

previous claim, {0, . . . , 2n 1}, Di explicit definition
Ci = {R, S} Tn . Proposition 4.6, Ci implicitly definable
Tn . rest proof, show explicit definition Ci
Tn least double exponentially long. aim, introduce interpretations
based elements , denotes set strings symbols
. precisely, every p 0 |p| 2n 1, define interpretation Ip
follows.
Ip = {p0 | p0 prefix p};
NC ,
383

fiTen Cate, Franconi, & Seylan

= Aj j {1, . . . , n},
AIp = {p0 Ip | conjunct C|p||p0 | },
6= Aj j {1, . . . , n}, AIp = ;
NR ,
Ip = {hp1 , p2 Ip Ip | p2 = p1 }, ,
Ip = , NR \ .
following claim easy show.
Claim 4.14. every p 0 |p| 2n 1,
Ip |= Tn ,


C|p|p .
Denote every {0, . . . , 2n 1}, set p |p| = .
Claim 4.15. Let {0, . . . , 2n 1} let C ALC-concept sig(C) =
{R, S} Tn |= Ci C. PC .
Proof claim. Suppose first = 0. = {}. Moreover, definition
PC . Hence PC , wanted show.
suppose > 0. proceed towards contradiction. Suppose
pa \ PC . Let pb prefix pa |pb | = 1. Since > 0, pb well-defined.
claim Ipb Ipa sub(C) {s p | p PD } PC ,
DIpb DIpa .

(3)

proof induction structure D. Since base boolean cases
trivial, treat case = .E, .
(). Suppose DIpb . Ipb hs, ti Ipb E Ipb .
Since Ipa well, former yields hs, ti Ipa . thus remains show
E Ipa . definition, = PD = {} { p | p PE }. Thus,
assumption {s p | p PD } PC yields {s} {t p | p PE } PC . implies
{t p | p PE } PC . induction hypothesis E Ipb , obtain
E Ipa , wanted show direction proof.
(). Suppose DIpa . Ipa hs, ti Ipa
E Ipa . definition, = PD = {} { p | p PE }. Thus,
assumption {s p | p PD } PC yields {s} {t p | p PE } PC . implies
{t p | p PE } PC .

(4)

(4) PE , obtain PC . Since pa 6 PC , means 6= pa .
Ipb hs, ti Ipb . (4), E Ipa , induction hypothesis,
E Ipb . Hence (.E)Ipb .
384

fiBeth Definability Expressive Description Logics

Thus, shown (3) holds. arrive contradiction follows.
Ip

Claim 4.14, Ci pa Ci1b , since |pa | = |pb | = 1, respectively.
Ip

Ip

Ci1b implies definition Ci 6 Ci b . Tn |= Ci C Claim 4.14,
obtain C Ipa 6 C Ipb . contradicts immediate consequence
(3), namely C Ipb iff C Ipa . Hence contradiction. Thus, conclude PC
> 0.

show theorem, argue follows. Suppose C ALC-concept
n
Tn |= C2n 1 C sig(C) . previous claim 2 1 PC .
n
n
n
n
definition, |2 1 | = 22 1 thus, 22 1 |PC |. Lemma 4.10, 22 1 |C|.
Hence theorem follows.
Remark 4.16. role disjunction constructor, present ALC, Cn
would admit single exponentially long explicit definition Tn Theorem 4.11.
Remark 4.17. lower bound argument Theorem 4.11 works CBP well
setting = .
Combined Theorem 4.8, Theorem 4.11 implies implicit definitions using general TBoxes exactly double exponentially succinct explicit definitions using
acyclic terminologies. closes open problem ten Cate et al. (2006) size
explicit definitions. Moreover, theorems establish exact bound size
equivalent rewritings concept queries considered Seylan et al. (2009). Theorem 4.11
also shows Theorem 1 Seylan et al. (2010), claims single exponential upper
bound size explicit definitions ALC, wrong. source problem
proof Theorem 1 Lemma 1, claims single exponential upper bound
size interpolants ALC.
4.2 Failure Beth Definability Presence Role Hierarchies
show BP fails description logics consider include role
hierarchies (H). shows BP indeed stronger property CBP
logics CBP (ten Cate et al., 2006).
Theorem 4.18. Let L ALCH extensions constructors {S, I, F}.
L BP.
Proof. Let = {R1 , R2 } consider ALCH-TBox consists
v R1
v R2
R1 .A u S. v R2 .A
R1 .A u S. v R2 .A
easy see satisfiable. fact, present two models below.
Claim 4.19. S.> implicitly definable .
385

fiTen Cate, Franconi, & Seylan

R1




R1



w



R2

R2

R2
v

b

R1

Figure 5: Interpretations J used disproving BP ALCH
Proof claim. Define XI = {s | .hs, ti R1I R2I }. show that,
whenever |= , (S.>)I = XI . establishes claim, since XI depends
R1I R2I .
First, show (S.>)I XI . Suppose (S.>)I .
hs, ti . RIAs , hs, ti R1I R2I . Hence XI .
Next, show XI (S.>)I . contradiction, suppose XI 6 (S.>)I ,
i.e., (S.)I . hs, ti R1I R2I hs, ti 6 .
definition interpretation, either (i) AI (ii) (A)I . (i),
R1 .A u S. v R2 .A , (A)I , contradiction. (ii),
R1 .A u S. v R2 .A , AI , contradiction. Hence
XI (S.>)I .

Let = hI , interpretation
= {s, t},
R1I = R2I = = {hs, ti};
RI = , R NR \ ( {S});
B = , B NC .
Let J = hJ , J interpretation
J = {w, v, a, b},
R1J = {hw, ai, hv, bi}, R2J = {hw, bi, hv, ai};
RJ = , R NR \ ;
AJ = {a};
B J = , B (NC \ {A}).
interpretations J depicted Figure 5. hard see J
models . Furthermore, two structures indistinguishable concepts
signature , following sense:
Claim 4.20. SHIF-concepts C sig(C) = {R1 , R2 },
1. C w C J ;
386

fiBeth Definability Expressive Description Logics

2. C v C J ;
3. C C J ;
4. C b C J
proof claim straightforward, simultaneous induction structure
concept C (alternatively, bisimulations used establish result).
Since (S.>)I w 6 (S.>)J , follows SHIF-concept C
sig(C) |= S.> C. summary, ALCH-concept S.>
implicitly definable ALCH-TBox , S.> explicitly definable
SHIF. conclude BP fails every description logic
includes ALCH included SHIF.
Theorem 4.18 shows Theorem 10 Seylan et al. (2010), claims ALCH
extensions and/or BP, incorrect. mistake proof
Theorem 9, presents reduction concept satisfiability problem w.r.t.
TBoxes SHI problem ALC, actually used computing
SHI-interpolants.
4.3 Failure Beth Definability Finite
consider BPF (the analogue Beth Definability finite structures).
start, explain motivations. Seylan et al. (2009) consider ontology-based data
access setting, traditional ABoxes replaced DBoxes. Syntactically, DBoxes
defined way ABoxes, semantics different: ABox
merely assumed express true facts, DBox assumed list true facts
specified subset signature (known set data predicates). Thus, example,
= {A(a), R(a, b)} DBox data predicates R, and, definition
semantics DBoxes, that, every model D, AI = {aI } RI = {haI , bI i}.
setting, TBox may contain predicates data predicates
authors use BP determine whether concept query signature TBox
rewritten equivalent first-order query data predicates. possible,
computing certain answers original query reduced computing
answers rewriting DBox, viewed database. setting
described here, DLs without FMP, natural consider BPF BP.
reason that, every interpretation DBox, data predicates are, definition, finite
relations. fact, appropriate analogue BP setting one restricted
interpretations data predicates finite rest signature
unrestricted. variant BP viewed common generalization BP BFP.
study here, negative results present BFP apply
well.
Theorem 4.21 establishes BPF fails L , L DL (among
ones consider) lacking FMP. precisely, show L -TBox , L concept C, signature C f-implicitly definable ,
f-explicit definition L , i.e., L -concept sig(D)
|=f C D. Intuitively, reason failure BPF logics
387

fiTen Cate, Franconi, & Seylan

express transitive closure role (see also discussion
proof Theorem 4.21).
Theorem 4.21. Let L ALCF extensions constructors {S, H}.
L BPF.
Proof. will, fact, prove something stronger: construct implicit definition
corresponding explicit definition even full first-order logic.
Let A, B, X concept names let R role name. Suppose = {R, A}. Consider
ALCF I-TBox consists following.
> v 1R u 1R
B v R.B
v X
R.(A u B) v X
R.X v X
show concept f-implicitly definable
f-explicitly definable . concept question u B. Note
concept finitely satisfiable w.r.t. , i.e., finite model
(A u B)I 6= . fact, provide model below.
interpretation = hI , i. sequence s0 , . . . , sn elements called
finite R-path n > 0 hsi , si+1 RI < n. infinite R-path defined
analogously. R-path start end nodes called
R-cycle. show two claims useful proof theorem.
Claim 4.22. Let finite model . B , hs, si (RI )+ , (RI )+
transitive closure RI .
Proof claim. Suppose B . axiom B v R.B implies existence
following infinite R-path:
p = s0 , s1 , . . .
s0 = 0, si B .
Since finite, 0 n < sn = sm . n = 0, immediately
hs, si (RI )+ . Otherwise, claim pairs hsi , sj sequence
hsn , sm i, hsn1 , sm1 i, . . . , hs0 , smn i, si = sj . base case follows immediately
sn = sm . inductive step, inductive hypothesis sni =
smi = t, . definition p, hsni1 , ti RI hsmi1 , ti
RI , imply axiom > v 1R |= sni1 = smi1 . Hence
= s0 = smn . hs, si (RI )+ , wanted show.
Claim 4.23. u B f-implicitly definable .
Proof claim. interpretations I, define
YI = {s | hs, si (RI )+ AI }.
388

fiBeth Definability Expressive Description Logics

show that, finite models , (A u B)I = YI . implies claim, since
YI depends RI AI .
() Suppose (A u B)I . Claim 4.22, know hs, si (RI )+ .
YI .
() Suppose YI . AI hs, si (RI )+ . Since AI , enough
show B . definition interpretation, either B (B)I .
(B)I , hs, si (RI )+ , (A u B)I , axioms R.(A u B) v X
R.X v X , (X)I would contradiction AI
v X . must B .

rest proof showing f-explicit definition u B
. aim, start defining interpretation , parameterized
natural number n > 0.
= {s0 , . . . , s2n+1 } {t0 , . . . , t2n+1 }
RIn = {hsi , si+1 | 0 2n} {hti , ti+1 | 0 2n} {ht2n+1 , t0 , }i
= {sn , tn }
B = X = {ti | 0 2n + 1}
Claim 4.24. every first-order formula (x) n > 0 |= [sn ]
|= [tn ].
Proof. apply Gaifman locality theorem (cf. Libkin, 2004). present setting,
unary binary relations, concerned formulas
single free variable, Gaifman locality theorem particularly easy state. Given
interpretation elements a, b , say b distance n relative
signature , sequence s0 , . . . , sm 0 n s0 = a, sm = b,
0 < m, pair hsi , si+1 belongs P (P ) binary relation (i.e.,
role name) P . interpretation I, element , natural number n 0,
denote a,n interpretation whose domain consists elements
distance n a, whose relations ones restricted
subset domain. Gaifman locality theorem stated follows:
every first-order formula (x), natural number n > 0 that, structures
elements a, b , a,n isomorphic b,n , via isomorphism maps
b, |= [a] |= [b].
Now, let (x) first-order formula, let n > 0 natural number given
Gaifman locality theorem. Consider instance constructed earlier.
immediately clear construction sn ,n isomorphic tn ,n , via
isomorphism maps sn tn . Therefore, |= [sn ] |= [tn ].
Claim 4.25. SHIF-concept C sig(C) |=f (A u B) C.
Proof claim. proceed towards contradiction. Suppose C ALCF I-concept
sig(C) = {R, A} |=f u B C. Let (x) = x (C). Since sn (A u B)In ,
|=f u B C, fact finite model , 6|= [sn ];
389

fiTen Cate, Franconi, & Seylan

reasoning, |= [tn ]. previous claim, |= [sn ]
|= [tn ], contradiction.

summary, ALCF I-concept u B f-implicitly definable
ALCF I-TBox , u B f-explicitly definable even
SHIF (or first-order logic, matter). follows that, L proper extension
ALCF constructors {S, H}, L BPF.
point specific counterexample BPF described proof Theorem 4.21 actually admits explicit definition one allow use transitive
closure. Specifically, shown u (R.>) u (R+ .R.>) explicit
definition, R+ denotes transitive closure role R.
4.4 Transitive Closure Operator
proof Theorem 4.21 suggests failure BPF considered logics may
caused fact express transitive closure. raises question
whether one regain BPF adding transitive closure constructor ALCF I.
section, show ALCF extended transitive closure constructor still
lacks BPF.
following, denote L+ language obtained L additionally allowing R+ role every role R L . allows us include roles
inductive definition concepts. However, L includes functionality restrictions, then,
usual, forbid use transitive closure inside functionality restrictions.
words, concepts form 1R, R allowed make use transitive closure
constructor.
semantics transitive closure construct expected, namely, (R+ )I
relation
{hs, ti | s1 , . . . , sn (n > 1) s1 = s, sn = t, hsi , si+1 RI 1 < n}.
Theorem 4.26. ALCF + BPF.
Proof. Consider following ALCF + -TBox .
> v 1R
> v R.>
> v R+ .A

v B
R.B v B
R.B v B

easy see finitely satisfiable, i.e., finite model. fact, provide
finite models , n > 0, below. first show concept name B f-implicitly
definable = {R, A} show f-explicit definition
concept language. interpretation I, R-path R-cycle
defined proof Theorem 4.21.
Claim 4.27. Let finite model . , hs, si (R+ )I .
Proof claim. Identical proof Claim 4.22 proof Theorem 4.21.

390



fiBeth Definability Expressive Description Logics

Claim 4.28.
(a) |=f > v R .>,
(b) |=f > v 1R.
Proof claim. Part (a) follows immediately previous claim.
prove part (b), suppose, sake contradiction 6|=f > v 1R.
finite model s, t, u 6= u hs, ti, hs, ui RI .
Since > v 1R |= part (a) claim, (R )I
graph total function . Since 6= u hs, ti, hs, ui RI , also know
cardinality image function must strictly smaller cardinality
domain, i.e., |Y | < |I |. implies ( . contradicts
> v R.> |= . Hence conclude |=f > v 1R.

s, , write odd(s, t) R-path odd length t, is,
R-path s0 , . . . , sn = s0 , = sn , n odd. Note R-path like
s0 , . . . , sn , always n > 0.
Claim 4.29. finite models ,
B = {s | .odd(s, t) AI }.
Proof claim. () Suppose B . first claim, R-cycle p = s0 , . . . , sn ,
s0 = sn = n > 0. Since > v R+ .A |= ,
hs, ti (R+ )I AI . claim = si , {1, . . . , n 1}.
show this, proceed towards contradiction.
Suppose claim hold. R-path t0 , . . . , tm ,
t0 = tm = t. Obviously, path different R-cycle p since
occur p. using |=f > v 1R previous claim, show every
individual ti actually appears p, contradicts fact appear
p. Hence conclude {1, . . . , n 1} si = t.
show odd. v B , |= , AI , 6 B .
using B axioms R.B v B , R.B v B , one easily
show induction odd. implies odd(s, t).
Hence odd(s, t) AI , wanted
show.
() Suppose odd(s, t) AI . implies
R-path s0 , . . . , sn s0 = s, sn = t, n odd. AI ,
v B , |= , 6 B . using fact n odd, axioms
R.B v B R.B v B , one easily show induction B .
Claim 4.29 implies B f-implicitly definable . rest proof
shows f-explicit definition B . n 0, let
following interpretation:
= {s0 , . . . , s2n+3 }
391

fiTen Cate, Franconi, & Seylan

RIn = {hsi , si+1 | 0 < 2n + 3} {hs2n+3 , s0 i}
= {sn+2 },
B = {si | 0 2n + 3 odd(si , sn+2 )}.
Intuitively, R-cycle (even) length 2n + 4, whose elements satisfy
concept name and/or B. Observe , n 0, model . Define function
: N follows.

(n + 2) n + 2
d(si ) =
(n + 2) < n + 2
words, d(s) distance sn+2 .
ALCF + -concept C, denote md(C) modal depth C, is,
maximal nesting depth role constructors C. Formally,
md(A) = md(>) = md( 1R) = 0
md(C) = md(C)
md(C u D) = max{md(C), md(D)}
md(R.C) = md(R+ .C) = md(C) + 1
CA R form P P P NR .
Claim 4.30. {0, . . . , n} s, s0 \ {s | d(s) i},
C iff s0 C
ALCF + -concepts md(C) sig(C) .
Proof claim. Let {0, . . . , n}, s, s0 \ {s | d(s) i}, C
ALCFI + -concept md(C) sig(C) . proof induction i.
= 0. Since md(C) = 0 sig(C) , C obeys following grammar:
C ::= > | | 1S | C | C u C
= R = R (recall forbid use transitive closure inside functionality restrictions).
induction structure C, show C iff s0 C .
C = >. definition interpretation, >In s0 >In . Hence
>In iff s0 >In .
C = (recall concept name ). assumption, 6= sn+2
s0 6= sn+2 . definition , obtain 6 s0 6 . Hence
iff s0 .
C = 1S. definition , |S (t)| = 1.
particular, |S (s)| = |S (s0 )| = 1. Hence ( 1S)In iff s0 ( 1S)In .
392

fiBeth Definability Expressive Description Logics

C = D. Follows easily inductive hypothesis C.
C = C1 u C2 . Follows easily inductive hypothesis C.
Hence conclude C iff s0 C , = 0.
Next, consider case > 0, let md(C) sig(C) .
C obeys following grammar:
C ::= S.E | + .E | C | C u C
md(E) 1, = R = R . induction structure C, show
C iff s0 C .
C = S.E md(E) 1, = R = R . definition ,
exactly one hs, ti exactly one t0
hs0 , t0 . Moreover, t, t0 \ {s | d(s) 1}.
following equivalent:
(S.E)In
E (since individual hs, ti )
t0 E (by inductive hypothesis i)
s0 (S.E)In (since individual hs0 , t0 ).
= + .E md(E) = 1, = R = R . Suppose first (S + .E)In .
hs, ti (S )+ E . distinguish
following cases:
6= s0 . definition , immediately obtain hs0 , ti (S )+ ;
E implies s0 (S + .E)In .
= s0 . definition , hs0 , si (S )+ . Moreover, s, s0
\ {s | d(s) 1}. inductive hypothesis
s0 E , E , implies hs0 , si (S )+ s0 (S + .E)In .
Hence s0 (S + .E)In cases, wanted show. direction
right left shown analogously.
cases shown easily inductive hypothesis C.


Hence claim follows.

Claim 4.31. ALCF + -concept C sig(C) {A, R} |=f B C.
Proof claim. proceed towards contradiction suppose existence
concept C. definition, md(C) = n, n 0; s0 , s1 \ {s | d(s)
n}. previous claim, s0 C iff s1 C . fact
finite model |=f B C, s0 B iff s1 B . implies
definition Claim 4.29 odd(s0 , sn+2 ) iff odd(s1 , sn+2 ), contradiction.
Hence conclude exists ALCF + -concept C sig(C) {A, R}
|= B C.

393

fiTen Cate, Franconi, & Seylan

proof theorem follows. Claim 4.29, B f-implicitly definable
= {A, R} . Claim 4.31, B f-explicitly definable
. Hence ALCF + BPF.

5. Concluding Remarks
paper, studied BP expressive DLs commonly used concept constructors.
constructors appear Web Ontology Language OWL-Lite (Horrocks et al.,
2003). OWL-Lite superseded OWL 2, supports important constructors nominals, denoted language, qualified number restrictions,
denoted Q language. already results available regarding BP
logics Q O.
Q generalization F ten Cate et al. (2006) show via model-theoretic argument
CBP holds ALCQ. believe BP also shown hold ALCQ
ALCQI using model-theoretic argument; although argument gives upper
bound size explicit definitions. Extending upper bound results size
explicit definitions logics appears difficult unavailability
natural optimal tableau algorithm logics.
logics O, besides concept role names, assume set NI = {i, j, . . .}
nominals. Syntactically, nominals treated atomic concepts semantically
nominal interpreted singleton set. presence nominals gives rise two different Beth definability properties. first one, allowed restrict nominals
appearing implicit/explicit definitions making part signature ;
second one, definitions allowed use nominal NI . Obviously, first one
stronger property. Ten Cate et al. (2006) show even second property fails
ALCO. also observe extending ALCO concepts form @i C enough
regain CBP. Intuitively, @i C says point satisfying nominal also satisfies
concept C.
similar way, one try identify extension ALCH BP.
proof Theorem 4.18, argument failure BP considered logics
express role conjunction. remains open ALCH extended role
conjunction constructor BP. Another interesting open question identify minimal
extension ALCF BPF.
Theorem 3.36, know compute first-order explicit definitions single
exponential size, given concept implicitly defined TBox. leave
another open problem existence matching lower bound, i.e., family
TBoxes implicitly defining concept smallest explicit definitions first-order
logic single exponentially big?

Acknowledgments
grateful Carsten Lutz Maarten Marx helpful discussions topic.
substantial part research carried extended visit Inanc Seylan
394

fiBeth Definability Expressive Description Logics

UC Santa Cruz 2010, thank Phokion Kolaitis hospitality. also thank
anonymous reviewers extensive comments.
Balder ten Cate supported NSF grants IIS-0905276 IIS-1217869.

Appendix A. Quasimodels
Decision procedures based semantic tableau construct model given formula/concept, finite representation model model unfolded.
paper, use term quasimodel denote finite representation following Andreka, Nemeti, van Benthem (1998). Various names used
literature, including Hintikka structures (Schwendimann, 1998), model graph (Gore, 1999),
even tableau (Horrocks & Sattler, 2007). Modulo differences, building blocks
structures sets finite concepts subset relevant concept
closure. using definition concept closure cl(C, ) given Section 3.1.
Remark A.1. rest appendix, assume ALCF-concepts defined
recursively Section 2.1 using also , t, R.C, 2R primitives; concepts
NNF; ALCF-TBoxes consist axioms form > v C. discussion
assumptions, refer reader beginning Section 3.1.
every subset concept closure suitable take part quasimodel. Depending logic hand, sets satisfy basic consistency requirements. Following,
e.g., Lutz et al. (2005), use term type denote sets satisfying requirements. Note, however, non-membership concept type imply
membership negation concept type. respect, types
similar Hintikka sets, also called downward-saturated sets (cf. Fitting, 1996).
Definition A.2. Let C0 ALCF-concept let ALCF-TBox.
cl(C0 , ) called hC0 , i-type ALCF A, C, C1 , C2 , R.C,
2R, 1R cl(C0 , ),
(P ) 6 ;
(P ) {A, A} 6 ;
(Pu ) C1 u C2 , C1 C2 ;
(Pt ) C1 C2 , C1 C2 ;
(Pv ) > v C , C ;
(P./ ) { 1R, 2R} 6 ;
(P1 ) { 1R, R.C} , R.C .
type belongs quasimodel, may force type also belong
quasimodel, instance witness existential statement. fact, quasimodel
collection types coherent sense.
395

fiTen Cate, Franconi, & Seylan

Definition A.3. Let C0 ALCF-concept, ALCF-TBox , two hC0 , itypes ALCF.
R.C

write === R.C {C} {C 0 | R.C 0 } .
2R

write === 2R {C 0 | R.C 0 } .
set Q hC0 , i-types ALCF hC0 , i-quasimodel ALCF satisfies:
(a) 0 Q C0 0 ;
R.C

(b) every Q R.C , type Q === ;
2R

(c) every Q 2R , type Q === .
following theorem useful soundness completeness proofs tableau
interpolation algorithms. proof inspired Marx Venema (2007).
Theorem A.4. ALCF-concept C0 satisfiable w.r.t. ALCF-TBox
hC0 , i-quasimodel ALCF.
Proof. () Given model = hI , C0I 6= , carve ,
set concepts L(s) cl(C0 , ) follows.
L(s) = {C cl(C0 , ) | C }.
let Q = {L(s) | }.
Claim A.5. Q hC0 , i-type ALCF.
Proof claim. Suppose Q. = L(s) . verify conditions
Definition A.2.
definition, = thus 6 thus 6 L(s). Hence (P ) satisfied.
virtue interpretation, case AI (A)I .
Hence (P ) satisfied.
C1 u C2 L(s), (C1 u C2 )I . Since interpretation, C1I
C2I . C1 , C2 L(s). Hence (Pu ) satisfied.
C1 C2 L(s), (C1 C2 )I . Since interpretation, C1I C2I .
C1 L(s) C2 L(s). Hence (Pt ) satisfied.
> v C , C thus C . C L(s). Hence (Pv )
satisfied.
Suppose contradiction (P./ ) hold. ( 1R)I (
2R)I . contradiction. Hence (P./ ) satisfied.
Suppose { 1R, R.C} . assumption ( 1R)I (R.C)I .
follows exactly one hs, ti RI C .
(R.C)I . Hence (P1 ) satisfied.
396

fiBeth Definability Expressive Description Logics

Since shown conditions Definition A.2 satisfied, conclude
hC0 , i-type ALCF.

claim Q hC0 , i-quasimodel. Claim A.5, Q, hC0 , i-type
ALCF. Thus remains show condition (a), (b), (c) Definition A.3
satisfied.
(a), since C0I 6= , s0 s0 C0I construction
Q, L(s0 ) Q. Hence, condition (a) satisfied.
condition (b), suppose R.C L(s) . means (R.C)I ,
i.e., individual hs, ti RI C . construction
Q, C L(t). let R.D L(s). construction Q,
(R.D)I . implies hs, ti RI DI . construction Q again,
R.C

obtain L(t). Hence, L(s) === L(t); conclude (b) satisfied.
proof (c) analogous.
() Suppose Q hC0 , i-quasimodel ALCF. idea proof construct
interpretation inductively using Q show |= C0I 6= .
construction, need introduce notation first.
Let interpretation let L : Q. pair hs, Ci
C cl(C0 , ) called defect w.r.t. L, if,
R.C L(s) hs, ti RI C L(t),
2R L(s) |{t | hs, ti RI }| < 2.
Fix map f : cl(C0 , ) N, let linear order Cartesian product N N
order type (recall countably infinite linear order said order type
element order finitely many elements less it;
well known linear orders N N order type ).
ready define induction interpretations Ii = hIi , Ii Ii N
mappings Li : Ii Q, N.
Base case. condition (a) Definition A.3, type 0 Q C0 0 .
Define interpretation I0 follows.
I0 = {s}, N;
NC ,
0 , AI0 = {s},
6 0 , AI0 = ;
R NR , RI0 = .
Set L0 = {s 7 0 }.
Inductive step. defect Ii w.r.t. Li , set Ii+1 = Ii Li+1 = Li ;
otherwise, let hs, Ci least defect Ii w.r.t. Li , i.e., every defect ht, Di Ii w.r.t.
Li , hs, f (C)i ht, f (D)i (using fact order type ). Li (s) Q
C

conditions (b) (c) Definition A.3, Q Li (s) =
.
C = R.D, let N \ Ii define
397

fiTen Cate, Franconi, & Seylan

Ii+1 = Ii {t},
NC ,
, AIi+1 = AIi {t},
6 , AIi+1 = AIi ;
NR ,
= R, Ii+1 = {hs, ti} Ii ,
6= R, Ii+1 = Ii .
Also set Li+1 = Li {t 7 }. C = 2R, let t1 , t2 N \ Ii t1 6= t2 define
Ii+1 = Ii {t1 , t2 },
NC ,
, AIi+1 = AIi {t1 , t2 },
6 , AIi+1 = AIi ;
NR ,
= R, Ii+1 = {hs, t1 i, hs, t2 i} Ii ,
6= R, Ii+1 = Ii .
Also set Li+1 = Li {t1 7 , t2 7 }. finishes inductive construction. define
interpretation follows:

= i0 Ii ,

P NC NR , P = i0 P Ii .

Also set L = i0 Li . Observe L total mapping Q.
Claim A.6. concepts C cl(C0 , ) , C L(s) C .
Proof claim. Let C stated claim. Suppose C L(s).
definition L, N C Li (s); let smallest natural number
satisfying C Li (s), i.e., Ii interpretation introduced s. induction
structure C, show C . Since Q, 6 (P ), follows
C 6= . Hence, consider remaining cases C.
C = >. assumption , i.e., >I .
C = A, NC . definition Ii Li (s), immediately
follows AIi . implies definition AI .
C = A, NC . Since Li (s) Q, Li (s) satisfies (P ). Li (s),
6 Li (s). One easily show induction k i,
6 AIk . implies assumption k N, 6 AIk .
definition I, obtain 6 AI , i.e., (A)I .
398

fiBeth Definability Expressive Description Logics

C = C1 u C2 . Follows easily inductive hypothesis (Pu ).
C = C1 C2 . Follows easily inductive hypothesis (Pt ).
C = R.D. Let hs, ti RI . need show DI .
hs, ti RI , k N hs, ti RIk ; w.l.o.g. assume Ik
interpretation introduced t. follows E cl(C0 , )
E

hs, Ei defect Ik1 w.r.t. Lk1 Lk (s) =
Lk (t). implies Lk (t).
definition L, obtain L(t). inductive hypothesis,
implies DI . Hence, (R.D)I .
C = R.D. assumption i, hs, Ci defect Ii w.r.t. Li .
definition , finitely many pairs ht, Ei N E cl(C0 , )
ht, f (E)i hs, f (C)i. implies k >
fix defect hs, Ci step k. Ik hs, ti RIk
Lk (t). definition I, hs, ti RI L(t).
inductive hypothesis, latter implies DI . Hence, (R.D)I .
C = 1R. Suppose contradiction t1 , t2 t1 6= t2
hs, t1 i, hs, t2 RI . k1 , k2 N hs, ti RIki Iki
interpretation introduced ti , {1, 2}. construction,
implies concepts C1 , C2 cl(C0 , ) hs, Ci defect
C


Iki 1 w.r.t. Lki 1 Lki (s) =
Lki (ti ), {1, 2}. follows
Ci 6= 2R, {1, 2}; otherwise, would obtain contradiction (P./ ).
Thus, C1 = R.D1 C2 = R.D2 . definition cl(C0 , ),
R.D1 , R.D2 cl(C0 , ); (P1 ), implies R.D1 , R.D2 Lk1 (s) =
Lk2 (s). Suppose w.l.o.g. k1 < k2 . D2 Lk1 (t1 ). contradicts
fact hs, R.D2 defect Ik2 1 w.r.t Lk2 1 .

C = 2R. case shown similarly case C = R.D.
Since considered possible cases, conclude claim holds.



Using Claim A.6, direction Theorem shown easily follows.
base case inductive construction, I0 C0 L0 (s).
implies C0 L(s) Claim A.6, obtain C0I . Moreover, Claim A.6
(Pv ), |= . Hence C0 satisfiable w.r.t. .

Appendix B. Useful Lemmas Tableau Correctness Interpolation
cll clr, define
(l) = {C | C l cll} (r) = {C | C r clr}.
() shorthand (l) (r). following
signature set ALCFconcepts concern. define sig(S) = CS sig(C). Let finite set
ALCF-concepts
ALCF-TBox. say satisfiable w.r.t.

satisfiable w.r.t. . Moreover cll clr satisfiable w.r.t.
() satisfiable w.r.t. .
399

fiTen Cate, Franconi, & Seylan

Lemma B.1. Let cll clr satisfiable w.r.t. .
?-burden ? {u, 1, , 2} -relief ,
satisfiable w.r.t. ;
t-burden , -relief satisfiable
w.r.t. .
Proof. Suppose stated Theorem, i.e., satisfiable w.r.t. .
means () satisfiable w.r.t. . Theorem
A.4

hC, i-quasimodel Q ALCF, C = D() D. means Q
() . also use term h, i-quasimodel Q.
Assume (C1 u C2 ) u-burden . (C1 u C2 ) -relief =
{(C1 ) , (C2 ) }. (Pu ), {C1 , C2 } thus () . Hence () satisfiable
w.r.t. .
Assume ( 1R) 1-burden . ( 1R) -relief =
{(R.C) | (R.C) }. (R.C) , R.C (P1 ), R.C .
Hence () satisfiable w.r.t. .
Assume (R.C) -burden . () condition (b) Definition A.3, Q {C} {D | R.D ()}; (Pv ),
{E | > v E } . Let (R.C) -relief . () . Hence
satisfiable w.r.t. .
Assume ( 2R) 2-burden . () condition (b) Definition A.3, Q {D | R.D ()}; (Pv ),
{E | > v E } . Let 2R-relief . () .
Hence satisfiable w.r.t. .
Assume (C1 C2 ) t-burden . (C1 C2 ) -relief ,
() (Pt ). Hence, (C1 C2 ) -relief satisfiable w.r.t.
T.
Proposition B.2. Let ALCF-TBox C0 , C1 , . . . , Cn , ALCF-concepts.
1. |= C0 u C1 u . . . u Cn v D,
|= R.C0 u R.C1 u . . . u R.Cn v R.D.
2. |= v C1 . . . Cn ,
|= R.D v R.C1 . . . R.Cn .
3. |= C1 u . . . u Cn v D,
|= 2R u R.C1 u . . . u R.Cn v R.D.
Proof. 1, proceed towards contradiction. Suppose |= C0 u C1 u . . . u Cn v
6|= R.C0 u R.C1 u . . . u R.Cn v R.D. model
|= C0 u C1 u . . . u Cn v 6|= R.C0 u R.C1 u . . . u R.Cn v R.D. latter
400

fiBeth Definability Expressive Description Logics

(R.C0 u R.C1 u . . . u R.Cn )I 6 (R.D)I .
is, hs, ti RI , (C0 u C1 u . . . u Cn )I , (D)I .
contradicts |= C0 u C1 u . . . u Cn v D.
2, proceed towards contradiction. Suppose |= v C1 . . . Cn
6|= R.D v R.C1 . . . R.Cn . model |= v
C1 . . . Cn 6|= R.D v R.C1 . . . R.Cn . latter
(R.D)I 6 (R.C1 . . . R.Cn )I .
hs, ti RI , DI , (C1 u . . . u Cn )I . contradicts
|= v C1 . . . Cn .
Proposition B.3. Let C ALCF-concept R role name.
|= 1R u R.C u R.C 1R u R.C.
Proof. |= 1R u R.C u R.C v 1R u R.C trivial. direction,
suppose contradiction 6|= 1R u R.C v 1R u R.C u R.C. means
interpretation = hI , 6|= 1R u R.C v 1R u R.C u R.C.
Thus ( 1R)I , (R.C)I , (R.C)I .
last two t1 , t2 hs, t1 i, hs, t2 RI , t1 C , t2 (C)I .
( 1R)I , t1 = t2 contradiction.
Lemma B.4. Let cll clr.
1. l , interpolant ;
2. r , > interpolant ;
3. concept C form 1R,
(a) {C l , (C)
l } , interpolant ;
(b) {C r , (C)
r } , > interpolant ;
(c) {C l , (C)
r } , C interpolant ;
(d) {C r , (C)
l } , C
interpolant ;
4. (C1 u C2 ) -relief interpolant , interpolant
;
5. 1 2 (C1 C2 )l -reliefs , I1 , I2 interpolants 1 , 2 respectively, I1 I2 interpolant ;
6. 1 2 (C1 C2 )r -reliefs , I1 , I2 interpolants 1 , 2 respectively, I1 u I2 interpolant ;
7. ( 1R)l -relief , biased concept form (R.C)r ,
interpolant , interpolant ;
8. ( 1R)r -relief , biased concept form (R.C)l ,
interpolant , interpolant ;
401

fiTen Cate, Franconi, & Seylan

9. ( 1R)l -relief , biased concept form (R.C)r ,
interpolant , Iu 1R interpolant ;
10. ( 1R)r -relief , biased concept form (R.C)l ,
interpolant , 2R interpolant ;
11. (R.C)l -relief , interpolant , biased concept
form (R.D)r , interpolant ;
12. (R.C)r -relief , interpolant , biased concept
form (R.D)l , > interpolant ;
13. (R.C)l -relief , interpolant , biased
concept form (R.D)r , R.I interpolant ;
14. (R.C)r -relief , interpolant , biased
concept form (R.D)l , R.I interpolant ;
15. ( 2R)l -relief , interpolant , biased concept
form (R.D)r , interpolant ;
16. ( 2R)r -relief , interpolant , biased concept
form (R.D)l , > interpolant ;
17. ( 2R)l -relief , interpolant , biased
concept form (R.D)r , R.I interpolant ;
18. ( 2R)r -relief , interpolant , biased
concept form (R.D)l , R.I interpolant .
Proof. 1. Suppose (l) = {X1 , . . . , Xn } {} (r) = {Y1 , . . . , Ym }. |=
u X1 u . . . , Xn v |= v Y1 . . . Ym hold trivially. Since logical
constant, = sig() sig((l)) sig((r)). Hence 1 satisfied.
2. Suppose (r) = {Y1 , . . . , Ym } {} (l) = {X1 , . . . , Xn }. |=
X1 u . . . , Xn v > |= > v Y1 . . . Ym > hold trivially. Since > logical
constant, = sig(>) sig((l)) sig((r)). Hence 2 satisfied.
3a. Suppose (l) = {X1 , . . . , Xn } {C, C}

(r) = {Y1 , . . . , Ym }. |=
X1 u . . . , Xn u C u C
v |= v Y1 . . . Ym hold trivially. Since logical
constant, = sig() sig((l)) sig((r)). Hence 3a satisfied.
argument 3b analogous previous case.
3c. Suppose (l) = {X1 , . . . , Xn } {C} (r) = {Y1 , . . . , Ym } {C}.

|=
X1 u . . . u Xn u C v C, |= C v Y1 . . . Ym (C),

sig(C) sig((l)) sig((r))
hold trivially. Hence 3c satisfied.
argument 3d analogous previous case.
4. Suppose (C1 uC2 )l -relief , interpolant , (l) = {X1 , . . . , Xn }
{C1 uC2 }, (r) = {Y1 , . . . , Ym }. assumption, |= X1 u. . .uXn u(C1 uC2 )uC1 uC2 v
I, i.e., |= X1 u. . .uXn u(C1 uC2 ) v |= v Y1 t. . .tYm . assumption again,
sig((l)) = sig((l)) sig((r)) = sig((r)), thus sig(I) sig((l)) sig((r)).
402

fiBeth Definability Expressive Description Logics

Therefore interpolant . case (C1 u C2 )r -relief
interpolant shown analogously. Hence 4 satisfied.
5. Suppose 1 2 (C1 C2 )l -reliefs , I1 , I2 interpolants 1 , 2
respectively, (l) = {X1 , . . . , Xn } {C1 C2 }, (r) = {Y1 , . . . , Ym }. assumption,
|= X1 u . . . u Xn u (C1 C2 ) u C1 v I1 |= X1 u . . . u Xn u (C1 C2 ) u C2 v I2 .
following.
|= I1 I2 w (X1 u . . . u Xn u (C1 C2 ) u C1 )
(X1 u . . . u Xn u (C1 C2 ) u C2 )
|= I1 I2 w (X1 u . . . u Xn u (C1 C2 )) u (C1 C2 )
|= I1 I2 w X1 u . . . u Xn u (C1 C2 )
half, assumption |= I1 v Y1 . . . Ym |= I2 v Y1 . . . Ym .
|= I1 I2 v Y1 . . . Ym . Clearly, sig(I1 I2 ) sig((l)) sig((r)). Hence
5 satisfied.
argument 6 analogous previous case.
7. Suppose
( 1R)l -relief ,
interpolant ,
(l) = {X1 , . . . , Xn } { 1R} {R.C1 , . . . , R.Ck }, {R.C1 , . . . , R.Ck } =
{R.C (l)},
(r) = {Y1 , . . . , Ym },
biased concept form (R.C)r .
Let E = R.C1 u . . . u R.Ck . assumption,
|= X1 u . . . u Xn u 1R u R.C1 u . . . u R.Ck u E v I.
Proposition B.3
|= X1 u . . . u Xn u 1R u R.C1 u . . . u R.Ck v
wanted show. half, since biased concept
form (R.C)r , (r) = (r).
|= v Y1 . . . Ym
wanted show. assumption sig((l)) = sig((l)) sig((r)) =
sig((r)), thus sig(I) sig((l)) sig((r)). Therefore interpolant . Hence
7 satisfied.
8 shown analogously previous case.
9. Suppose
( 1R)l -relief ,
403

fiTen Cate, Franconi, & Seylan

interpolant ,
(l) = {X1 , . . . , Xn } { 1R} {R.C1 , . . . , R.Ck }, {R.C1 , . . . , R.Ck } =
{R.C (l)},
(r) = {Y1 , . . . , Ym } {R.D1 , . . . , R.Dl }, {R.D1 , . . . , R.Dl } = {R.C
(r)},
biased concept form (R.C)r .
Let E = R.C1 u . . . u R.Ck . assumption,
|= X1 u . . . u Xn u 1R u R.C1 u . . . u R.Ck u E v I.
Also, trivially following.
|= X1 u . . . u Xn u 1R u R.C1 u . . . u R.Ck u E v 1R.
Combining two, get
|= X1 u . . . u Xn u 1R u R.C1 u . . . u R.Ck u E v Iu 1R

wanted show.
half, let F = R.C1 . . . R.Cl . assumption I,
|= v Y1 . . . Ym R.C1 . . . R.Cl F.
this, trivially get
|= v Y1 . . . Ym R.C1 . . . R.Cl F 2R.

Proposition B.3,
|= v Y1 . . . Ym R.C1 . . . R.Cl 2R,
implies
|= Iu 1R v Y1 . . . Ym R.C1 . . . R.Cl ,
wanted show. assumption sig((l)) = sig((l)) sig((r)) =
sig((r)), thus sig(I) sig((l)) sig((r)). Moreover, since biased
concept form (R.C)r , R sig((l)) sig((r)). conclusion, sig(Iu 1R)
sig((l)) sig((r)). Hence 9 satisfied.
10 shown analogously previous case.
11. Suppose following:
(R.C)l -relief ,
interpolant ,

F
E = >vCTl C, F = >vCTr C,
404

fiBeth Definability Expressive Description Logics

(l) = {X1 , . . . , Xn } {R.C} {R.D1 , . . . , R.Dk }, {R.D1 , . . . , R.Dk } =
{R.D (l)},
biased concept form (R.D)r .
last assumption, (r) = {C | > v C Tr }. assumption, |= v F . Since
|= F v , |= thus |= R.I . assumption
|= C u D1 u . . . u Dk u E v I. Since |= > v E, |= C u D1 u . . . u Dk v I.
Proposition B.2
|= R.C u R.D1 u . . . u R.Dk v R.I.
However |= R.I means
|= R.C u R.D1 u . . . u R.Dk v .
Hence,
|= X1 u . . . u Xn u R.C u R.D1 u . . . u R.Dk v
wanted show. half, let (r) = {Y1 , . . . , Ym }. Since
|= , trivially
|= v Y1 . . . Ym
final step, need show
sig() sig((l)) sig((r)).
follows easily since logical constant. Hence 11 satisfied.
12. Suppose following:
(R.C)r -relief ,
interpolant ,

F
E = >vCTl C, F = >vCTr C,
(r) = {Y1 , . . . , Ym } {R.C} {R.D1 , . . . , R.Dk }, {R.D1 , . . . , R.Dk } =
{R.D (r)},
biased concept form (R.D)l .
last assumption, (l) = {C | > v C Tl }. assumption, |= E v I. Since
|= > v E, |= > thus |= R.I >. assumption |=
v CtD1 t. . .tDk tF . Since |= F v , |= v CtD1 t. . .tDk .
Proposition B.2
|= R.I v R.C R.D1 . . . R.Dk .
However |= > R.I, means
|= > v R.C R.D1 . . . R.Dk .
405

fiTen Cate, Franconi, & Seylan

Hence,
|= > v Y1 . . . Ym R.C R.D1 . . . R.Dk
wanted show. half, let (l) = {X1 , . . . , Xn }. Since
|= >, trivially
|= X1 u . . . u Xn v >
final step, need show
sig(>) sig((l)) sig((r)).
follows easily since > logical constant. Hence 12 satisfied.
13. Suppose following:
(R.C)l -relief ,
interpolant ,

F
E = >vCTl C, F = >vCTr C,
(l) = {X1 , . . . , Xn } {R.C} {R.D1 , . . . , R.Dk }, {R.D1 , . . . , R.Dk } =
{R.D (l)},
(r) = {Y1 , . . . , Ym } {R.C1 , . . . , R.Cl }, l 1 {R.C1 , . . . , R.Cl } =
{R.C (r)}.
assumption, |= C u D1 u . . . u Dk u E v I. Since |= > v E,
|= C u D1 u . . . u Dk v I. Proposition B.2
|= R.C u R.D1 u . . . u R.Dk v R.I

(5)

(5),
|= X1 u . . . u Xn u R.C u R.D1 u . . . u R.Dk v R.I
wanted show. argue half. assumption,
|= v C1 . . . Cl F . Since |= F v , |= v C1 . . . Cl .
Proposition B.2
|= R.I v R.C1 . . . R.Cl
(6)
(6),
|= R.I v Y1 . . . Ym R.C1 . . . R.Cl
wanted show. final step, need show
sig(R.I) sig((l)) sig((r)).
follows easily since assumption sig(I) sig((l)) sig((r)) R sig((l))
sig((r)), latter consequence l 1.
argument 14 analogous previous case. Moreover 15, 16, 17, 18
shown similarly 11, 12, 13, 14, respectively.
406

fiBeth Definability Expressive Description Logics

Appendix C. Tableau Correctness, Termination, Interpolation
Lemma C.1. Let = hV, Ei output second phase. every node g V:
1. g.status either sat unsat.
2. g.status = unsat, either one following holds.
g sink node3 containing clash;
exactly one successor g 0 g (C1 uC2 ) g.content,
g 0 .content (C1 u C2 ) -relief g.content g 0 .status = unsat;
exactly one successor g 0 g ( 1R) g.content,
g 0 .content ( 1R) -relief g.content g 0 .status = unsat;
exactly n successors g1 , . . . , gn g, n cardinality
set {(C1 )1 , . . . , (Cn )n } - 2-burdens g.content, gi .content
(Ci )i -relief g.content {1, . . . , n}, {1, . . . , n}
gi .status = unsat;
exactly two successors g1 , g2 g (C1 C2 )
g.content, gi .content (C1 tC2 ) -relief g.content {1, 2}, g1 .content 6=
g2 .content, gi .status = unsat {1, 2}.
3. g.status = sat, either one following holds.
g sink node containing clash,
exactly one successor g 0 g (C1 uC2 ) g.content,
g 0 .content (C1 u C2 ) -relief g.content g 0 .status = sat;
exactly one successor g 0 g ( 1R) g.content,
g 0 .content ( 1R) -relief g.content g 0 .status = sat;
exactly n successors g1 , . . . , gn g, n cardinality
set {(C1 )1 , . . . , (Cn )n } - 2-burdens g.content, gi .content
(Ci )i -relief g.content {1, . . . , n}, {1, . . . , n}
gi .status = sat;
exactly two successors g1 , g2 g (C1 C2 )
g.content, gi .content (C1 tC2 ) -relief g.content {1, 2}, g1 .content 6=
g2 .content, {1, 2} gi .status = sat.
Proof. 1 clearly follows fact every node assigned status unsat
Propagate step Algorithm 1 gets status sat end (Assign) Algorithm 1.
Let g V. definition tableau algorithm, g satisfies exactly one
following structural conditions:
g sink node;
3. node outgoing edges

407

fiTen Cate, Franconi, & Seylan

exactly two successors g1 , g2 g (C1 C2 ) g.content,
gi .content (C1 C2 ) -relief g.content {1, 2};
exactly one successor g 0 g (C1 u C2 ) g.content,
g 0 .content (C1 u C2 ) -relief g.content;
exactly one successor g 0 g ( 1R) g.content,
g 0 .content ( 1R) -relief g.content;
exactly n successors g1 , . . . , gn g, n cardinality set
{(C1 )1 , . . . , (Cn )n } - 2-burdens g.content, gi .content (Ci )n relief g.content {1, . . . , n}.
Suppose first g.status = unsat g clearly respects 2 ways
node get status unsat Propagate. Suppose g.status = sat. g.status
determined Assign Algorithm 1. distinguish structural properties
g above.
Suppose g sink node. means rule applied g. g.status =
sat, immediately obtain g.status contain clash; g.status
contains clash, would g.status = unsat contradicts fact
every node g V, value g.status calculated once.
Suppose exactly two successors g1 , g2 g (C1 C2 )
g.content, gi .content (C1 C2 ) -relief g.content {1, 2}. Since g.status = sat,
g.status undefined right Assign. implies {1, 2}
gi .status undefined otherwise g.status = unsat. Assign,
gi .status = sat. Hence, g satisfies 3.
Suppose exactly one successor g 0 g (C1 u C2 )
g.content, g 0 .content (C1 u C2 ) -relief g.content. Since g.status = sat, g.status
undefined right Assign. implies that, g 0 .status undefined Assign
otherwise g.status = unsat. Assign, g 0 .status = sat. Hence, g satisfies
3.
remaining cases shown analogously. Hence lemma follows.
Proof Lemma 3.7. start observations. Algorithm 1 assigns status
unsat nodes V Propagate phase. status assignment steps induce
sequence 0, 1, 2. . . .. assignment step i, associate set Vi Vi
nodes status unsat far. Observe step step + 1, extend
Vi single node only. induction number status assignment steps, first
show g Vi ,
g.content unsatisfiable w.r.t. ;
ALCF-concept C
int(g) = C,
C interpolant g.content,
|int(g)| 2i+2 1.
408

fiBeth Definability Expressive Description Logics

base case, V0 = {g} sink node g V containing clash.
Obviously, g.content unsatisfiable. interpolant calculation rules Figure 2 cover
cases clash thus, ALCF-concept assigned int(g). Lemma B.4,
int(g) interpolant g.content. claim |int(g)| 2. int(g) form >,
, A, A, clear; int(g) form 1R (or 2R), observe
encoded using one symbol 1 (resp. 2) one symbol R. Hence,
|int(g)| 2 2i+2 1 inductive hypothesis holds base case.
inductive step, let Vi+1 = Vi {g}. inductive hypothesis holds every
g 0 Vi , trivially; thus, consider case g. Lemma C.1, five cases
distinguish:
1. g sink node containing clash. shown analogously base case.
2. exactly one successor g 0 g (C1 u C2 ) g.content,
g 0 .content (C1 u C2 ) -relief g.content g 0 .status = unsat. inductive
hypothesis, g 0 .content unsatisfiable w.r.t. , int(g 0 ) interpolant g 0 ,
|int(g 0 )| 2i+2 1. (the contrapositive version of) Lemma B.1, g.content
unsatisfiable w.r.t. . Moreover, Cu applied calculate int(g) int(g) =
int(g 0 ). Lemma B.4, int(g) interpolant g.content. int(g) =
int(g 0 ) |int(g 0 )| 2i+2 1 |int(g)| 2i+3 1. Hence inductive hypothesis
holds case.
3. exactly two successors g1 , g2 g (C1 C2 ) g.content,
gj .content (C1 C2 ) -relief g.content j {1, 2}, g1 .content 6= g2 .content,
gj .status = unsat j {1, 2}. inductive hypothesis, gj .content
unsatisfiable w.r.t. , int(gj ) interpolant gj , |int(gj )| 2i+2 1, j
{1, 2}. (the contrapositive version of) Lemma B.1, g.content unsatisfiable
w.r.t. . Moreover, depending , either Clt Crt applied calculate int(g).
Lemma B.4, int(g) interpolant g.content. |int(g)| = |int(g1 )|+
|int(g2 )| + 1. inductive hypothesis, obtain
|int(g)| (2i+2 1) + (2i+2 1) + 1 = 2i+3 1.
Thus, inductive hypothesis holds case.
4. cases shown similarly.
Hence, claim follows.
Now, use claim shown prove lemma. Let g V
g.status = unsat. Lemma 3.5, |V| 2n , n = |cll clr|. Thus,
worst case, 2n status assignment steps Propagate V2n = V. Since
g.status = unsat, follows g V2n . claim, g.content unsatisfiable w.r.t.
n
n
, int(g) defined interpolant g.content, |int(g)| 22 +2 1 = 422 1.
n
int(g) O(22 ). Hence lemma follows.
Proof Lemma 3.8. Algorithm 1 consists two stages: Propagate Assign.
Assign, make 2n assignments Lemma 3.5 number nodes
tableau bounded number. Moreover, assignment step takes constant time.
whole Assign stage takes time O(2n ).
409

fiTen Cate, Franconi, & Seylan

Propagate, loop inside do-while loop. loop iterates 2n
nodes assigns, possible, status unsat node checking worst case n
direct successors node. algorithm assigns status unsat node g,
n
also assigns concept int(g). Lemma 3.7, |int(g)| O(22 ). Thus spends
time calculating int(g). Suppose loop finished iteration nodes
tableau. execution, none nodes got status unsat, do-while
loop terminates done = true. worst case, status assigned
one node iteration do-while loop. Hence do-while loop iterates 2n
n
times discussed iteration takes time O(22 ) interpolation
calculation. Since dominates runtime Algorithm 1, lemma follows.
Lemma C.2 (Soundness). closed hC0 v D0 , i-tableau, |= C0 v D0 .
Proof. Let closed hC0 v D0 , i-tableau. Since closed, g0 .content = unsat.
g0 .content = {(C0 )l , (D
0 )r } {E l | > v E Tl } {E r | > v E Tr } Lemma 3.7,
implies
G
l
|= C0 u
E v D0
E
>vETl

>vETr

|= C0 v D0 .
Definition C.3. Let = hV, Ei output second phase. say node
g V saturated
g.status = sat g sink node,
g.status = sat R applied g.
g, g 0 V , g 0 called saturation g g 0 saturated, path
g = g0 , g1 , . . . , gk = g 0 k 0 0 < k, gi .status = sat
edge hgi , gi+1 created application rule {Ru , Rt , R1 }.
Lemma C.4. Let = hV, Ei complete tableau hC0 v D0 , i.
1. g V saturated, g.content() hC0 u
0 , i-type.
2. g V g.status = sat, saturation g 0 g g 0 .content
g.content.
Proof. 1, suppose g saturated. need show g.content() satisfies
Definition A.2. start g.content() cl(C0 u
0 , ). Let C g.content().
follows C g.content, {l, r}. Since g.content cll clr,
C cll clr. implies C cl(C0 , Tl ) cl(D
0 , Tr ). C cl(C0 u
0 , ),
wanted show.
show properties Definition A.2 satisfied. definition, g.status =
sat. g contain clash otherwise g.status = unsat would contradict
assumption. Hence, (P ), (P ), (P./ ) satisfied. definition, g sink node
R applied g. cases, none {Ru , Rt , R1 } applicable
g: former, follows fact rule applicable g;
410

fiBeth Definability Expressive Description Logics

latter, follows rule precedence. Hence, (Pu ), (Pt ), (P1 ) satisfied. Finally,
{C | > v C } g.content() easy consequence definition
tableau algorithm. means (Pv ) satisfied. Hence, conclude 1 holds.
2, suppose g V g.status = sat. saturation g 0 g
g 0 .content g.content follows easily Lemma C.1.
Lemma C.5 (Completeness). open hC0 v D0 , i-tableau, 6|= C0 v D0 .
Proof. Suppose = hV, Ei open hC0 v D0 , i-tableau. Since open,
g0 .status = sat. Lemma C.4, saturation g? g0 g? .content
g0 .content. Since g? saturated, follows Lemma C.4 g? .content() hC0 u

0 , i-type. Let 0 = {C0 u
0 }g? .content(). claim 0 also hC0 u
0 , itype. Suppose contradiction not. Since g? .content() type, follows
(Pu ) violated C0 u
0 0 , i.e., {C0 ,
0 } 6 0 . definition 0 ,
means {C0 ,
0 } 6 g? .content(). know {C0 ,
0 } g0 .content()
g? .content g0 .content, implies {C0 ,
0 } g? .content(), i.e., contradiction.
Hence conclude 0 hC0 u
0 , i-type. Define
Q = {0 } {g.content() | g V saturated}.
show Q hC0 u
0 , i-quasimodel 6|= C0 v D0 follows
Theorem A.4. easy see Q set hC0 u
0 , i-types: already
shown 0 type; g.content() g V saturated, fact
follows immediately Lemma C.4. remains show conditions (a), (b), (c)
Definition A.3 hold.
Condition (a) holds since 0 Q C0 u
0 0 .
Suppose R.C Q. distinguish = 0 =
g.content() saturated g V. first argue latter. Since g saturated
R.C g.content(), R applied g; since g saturated, g.status = sat.
Lemma C.1, successor g 0 g {C} {D | R.D
} g 0 .content() g 0 .status = sat. Lemma C.4, saturation g 00
g 0 g 00 .content g 0 .content. Since g 00 saturated, g 00 .content() Q
R.C

g 00 .content() hC0 u
0 , i-type. === g 00 .content(). case
= 0 follows analogously using fact successor g 0 g?
{C} {D | R.D } g 0 .content() g 0 .status = sat. Hence condition (b)
Definition A.3 satisfied.
condition (c) holds shown similarly previous case; leave
reader verify this. Hence, conclude Q hC0 u
0 , i-quasimodel.
Proposition 3.9 follows immediately Lemma C.2 Lemma C.5.

References
Afrati, F. N. (2011). Determinacy query rewriting conjunctive queries views.
Theoretical Computer Science, 412 (11), 10051021.
Andreka, H., Nemeti, I., & van Benthem, J. (1998). Modal languages bounded fragments
predicate logic. Journal Philosophical Logic, 27, 217274.
411

fiTen Cate, Franconi, & Seylan

Avigad, J. (2003). Eliminating definitions skolem functions first-order logic. ACM
Transactions Computational Logic, 4, 402415.
Baader, F., & Nutt, W. (2003). Basic description logics. Description Logic Handbook,
pp. 4395. Cambridge University Press.
Barany, V., Benedikt, M., & ten Cate, B. (2013). Rewriting guarded negation queries.
MFCS13, pp. 98110.
Beth, E. W. (1953). Padoas methods theory definitions. Indagationes Mathematicae, 15, 330339.
Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal logic. Cambridge University
Press.
Boolos, G. S., Burgess, J. P., & Jeffrey, R. C. (2007). Computability Logic. Cambridge
University Press.
Calvanese, D., & Giacomo, G. D. (2003). Expressive description logics. Description
Logic Handbook, pp. 178218. Cambridge University Press.
Calvanese, D., Giacomo, G. D., Lenzerini, M., & Nardi, D. (2001). Reasoning expressive
description logics. Handbook Automated Reasoning, pp. 15811634.
Calvanese, D., Giacomo, G. D., & Rosati, R. (1998). note encoding inverse roles
functional restrictions ALC knowledge bases. Description Logics, Vol. 11.
CEUR-WS.org.
ten Cate, B., Conradie, W., Marx, M., & Venema, Y. (2006). Definitorially complete description logics. KR, pp. 7989.
ten Cate, B., Franconi, E., & Seylan, I. (2011). Beth definability expressive description
logics. IJCAI, pp. 10991106.
Conradie, W. (2002). Definability changing perspectives: beth property three
extensions modal logic. Masters thesis, University Amsterdam.
Craig, W. (1957). Three uses Herbrand-Gentzen theorem relating model theory
proof theory. Journal Symbolic Logic, 22 (3), 269285.
De Giacomo, G. (1996). Eliminating converse Converse PDL. Journal Logic,
Language Information, 5 (2), 193208.
Donini, F. M. (2003). Complexity reasoning. Description Logic Handbook, pp.
96136. Cambridge University Press.
Duc, C. L., & Lamolle, M. (2010). Decidability description logics transitive closure
roles concept role inclusion axioms. Description Logics, Vol. 573, pp.
372383. CEUR-WS.org.
Fitting, M. (1996). First-order logic automated theorem proving (2nd ed.). SpringerVerlag.
Friedman, H. (1976). complexity explicit definitions. Advances Mathematics,
20 (1), 1829.
Gabbay, D. M., & Maksimova, L. (2005). Interpolation Definability Modal Logics
(Oxford Logic Guides). Clarendon Press.
412

fiBeth Definability Expressive Description Logics

Ghilardi, S., Lutz, C., & Wolter, F. (2006). damage ontology? case conservative extensions description logics. KR, pp. 187197.
Gore, R. (1999). Tableau methods modal temporal logics. Handbook Tableau
Methods, pp. 297396. Kluwer.
Gore, R., & Nguyen, L. A. (2007). Exptime tableaux global caching description
logics transitive roles, inverse roles role hierarchies. TABLEAUX, pp.
133148.
Hoogland, E. (2001). Definability Interpolation: Model-theoretic investigations. Ph.D.
thesis, University Amsterdam.
Hoogland, E., & Marx, M. (2002). Interpolation definability guarded fragments.
Studia Logica, 70 (3), 373409.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDF
OWL: making web ontology language. Journal Web Semantics, 1 (1),
726.
Horrocks, I., & Sattler, U. (2007). tableau decision procedure SHOIQ. Journal
Automated Reasoning, 39 (3), 249276.
Horrocks, I., Sattler, U., & Tobies, S. (2000). Practical reasoning expressive description logics. Logic Journal IGPL, 8 (3), 239264.
Konev, B., Lutz, C., Ponomaryov, D., & Wolter, F. (2010). Decomposing description logic
ontologies. KR, pp. 236246.
Konev, B., Lutz, C., Walther, D., & Wolter, F. (2009a). Formal properties modularisation.
Modular Ontologies, pp. 2566. Springer.
Konev, B., Walther, D., & Wolter, F. (2009b). Forgetting uniform interpolation
large-scale description logic terminologies. IJCAI, pp. 830835.
Kracht, M. (2007). Modal consequence relations. Handbook Modal Logic, pp. 491545.
Elsevier.
Lang, J., & Marquis, P. (2008). propositional definability. Artificial Intelligence, 172,
9911017.
Libkin, L. (2004). Elements Finite Model Theory. Springer.
Lutz, C. (2006). Complexity succinctness public announcement logic. AAMAS,
pp. 137143.
Lutz, C., Areces, C., Horrocks, I., & Sattler, U. (2005). Keys, nominals, concrete
domains. Journal Artificial Intelligence Research, 23, 667726.
Lutz, C., Piro, R., & Wolter, F. (2010). Enriching EL-concepts greatest fixpoints.
ECAI, pp. 4146.
Lutz, C., Sattler, U., & Tendera, L. (2005). complexity finite model reasoning
description logics. Information Computation, 199 (1-2), 132171.
Lutz, C., Seylan, I., & Wolter, F. (2012a). automata-theoretic approach uniform
interpolation approximation description logic EL. KR.
413

fiTen Cate, Franconi, & Seylan

Lutz, C., Seylan, I., & Wolter, F. (2012b). Mixing open closed world assumption
ontology-based data access: Non-uniform data complexity. Description Logics, Vol.
846, pp. 268278. CEUR-WS.org.
Lutz, C., & Wolter, F. (2011). Foundations uniform interpolation forgetting
expressive description logics. IJCAI, pp. 989995.
Marx, M. (2007). Queries determined views: pack views. PODS, pp. 2330.
Marx, M., & Venema, Y. (2007). Local variations loose theme: Modal logic
decidability. Finite Model Theory Applications, pp. 371426. Springer.
Nash, A., Segoufin, L., & Vianu, V. (2010). Views queries: Determinacy rewriting.
ACM Transactions Database Systems, 35 (3).
Nikitina, N., & Rudolph, S. (2012). Expexpexplosion: Uniform interpolation general EL
terminologies. ECAI, pp. 618623.
Pasaila, D. (2011). Conjunctive queries determinacy rewriting. ICDT, pp. 220231.
Rautenberg, W. (1983). Modal tableau calculi interpolation. Journal Philosophical
Logic, 12 (4), 403423.
Sattler, U., Calvanese, D., & Molitor, R. (2003). Relationships formalisms.
Description Logic Handbook, pp. 137177. Cambridge University Press.
Schwendimann, S. (1998). new one-pass tableau calculus PLTL. TABLEAUX, pp.
277292.
Seylan, I. (2012). DBoxes Beth Definability Description Logics. Ph.D. thesis, Free
University Bozen-Bolzano.
Seylan, I., Franconi, E., & de Bruijn, J. (2009). Effective query rewriting ontologies
DBoxes. IJCAI, pp. 923929.
Seylan, I., Franconi, E., & de Bruijn, J. (2010). Optimal rewritings definitorially complete
description logics. Description Logics, Vol. 573, pp. 125136. CEUR-WS.org.
Tobies, S. (2001). Complexity Results Practical Algorithms Logics Knowledge
Representation. Ph.D. thesis, RWTH-Aachen.

414

fiJournal Artificial Intelligence Research 48 (2013) 67-113

Submitted 3/13; published 10/13

Survey Multi-Objective Sequential Decision-Making
Diederik M. Roijers

d.m.roijers@uva.nl

Informatics Institute
University Amsterdam
Amsterdam, Netherlands

Peter Vamplew

p.vamplew@ballarat.edu.au

School Science,
Information Technology Engineering
University Ballarat
Ballarat, Victoria, Australia

Shimon Whiteson

s.a.whiteson@uva.nl

Informatics Institute
University Amsterdam
Amsterdam, Netherlands

Richard Dazeley

r.dazeley@ballarat.edu.au

School Science,
Information Technology Engineering
University Ballarat
Ballarat, Victoria, Australia

Abstract
Sequential decision-making problems multiple objectives arise naturally practice pose unique challenges research decision-theoretic planning learning,
largely focused single-objective settings. article surveys algorithms designed sequential decision-making problems multiple objectives. Though
growing body literature subject, little makes explicit circumstances special methods needed solve multi-objective problems. Therefore,
identify three distinct scenarios converting problem single-objective
one impossible, infeasible, undesirable. Furthermore, propose taxonomy
classifies multi-objective methods according applicable scenario, nature
scalarization function (which projects multi-objective values scalar ones), type
policies considered. show factors determine nature optimal solution, single policy, convex hull, Pareto front. Using taxonomy,
survey literature multi-objective methods planning learning. Finally,
discuss key applications methods outline opportunities future work.

1. Introduction
Sequential decision problems, commonly modeled Markov decision processes (MDPs)
(Bellman, 1957a), occur range real-world tasks robot control (Kober &
Peters, 2012), game playing (Szita, 2012), clinical management patients (Peek, 1999),
military planning (Aberdeen, Thiebaux, & Zhang, 2004), control elevators (Crites
& Barto, 1996), power systems (Ernst, Glavic, & Wehenkel, 2004), water supplies
(Bhattacharya, Lobbrecht, & Solomantine, 2003). Therefore, development algorithms
c
2013
AI Access Foundation. rights reserved.

fiRoijers, Vamplew, Whiteson & Dazeley

automatically solving problems, either planning given model MDP (e.g.,
via dynamic programming methods, Bellman, 1957b) learning interaction
unknown MDP (e.g., via temporal-difference methods, Sutton & Barto, 1998),
important challenge artificial intelligence.
research topics, desirability undesirability actions
effects codified single, scalar reward function. Typically, objective
autonomous agent interacting MDP maximize expected (possibly
discounted) sum rewards time. many tasks, scalar reward function
natural, e.g., financial trading agent could rewarded based monetary gain
loss holdings recent time period. However, also many tasks
naturally described terms multiple, possibly conflicting objectives, e.g.,
traffic control system minimize latency maximize throughput; autonomous
vehicle minimize travel time fuel costs. Multi-objective problems
widely examined many areas decision-making (Zeleny & Cochrane, 1982; Vira &
Haimes, 1983; Stewart, 1992; Diehl & Haimes, 2004; Roijers, Whiteson, & Oliehoek, 2013)
growing, albeit fragmented, literature addressing multi-objective decisionmaking sequential settings.
article, present survey algorithms devised
settings. begin Section 2 formalizing problem multi-objective MDP
(MOMDP). Then, Section 3, motivate multi-objective perspective decisionmaking. Little existing literature multi-objective algorithms makes explicit
multi-objective approach beneficial and, crucially, cases cannot trivially reduced
single-objective problem solved standard algorithms. address this,
describe three motivating scenarios multi-objective algorithms.
Then, Section 4, present novel taxonomy organizes multi-objective problems
terms underlying assumptions nature resulting solutions. key
difficulty existing literature authors considered many different types
problems, often without making explicit assumptions involved, differ
authors, scope applicability resulting methods. taxonomy
aims fill void.
Sections 5 6 survey MOMDP planning learning methods, respectively, organizing according taxonomy identifying key differences
approaches examined planning learning areas. Section 7 surveys applications
methods, covering specific applications general classes problems
MOMDP methods applied. Section 8 discusses future directions field
based gaps literature identified Sections 5 6, Section 9 concludes.

2. Background
finite single-objective Markov decision process (MDP) tuple hS, A, T, R, , where:
finite set states,
finite set actions,
: [0, 1] transition function specifying, state, action,
next state, probability next state occurring,
68

fiA Survey Multi-Objective Sequential Decision-Making

R : reward function, specifying, state, action, next
state, expected immediate reward,
: [0, 1] probability distribution initial states,
[0, 1) discount factor specifying relative importance immediate rewards.
goal agent acts environment maximize expected return
Rt , function rewards received timestep onwards. Typically,
return additive (Boutilier, Dean, & Hanks, 1999), i.e., sum rewards.
infinite horizon MDP, return typically infinite sum, term discounted
according :

X
k rt+k+1 ,
Rt =
k=0

rt reward obtained time t. parameter thus quantifies relative
importance short-term long-term rewards.
contrast, finite horizon MDP, return typically undiscounted finite sum,
i.e., certain number timesteps, process terminates reward
obtained. single- multi-objective methods developed finite horizon,
discounted infinite horizon, average reward settings (Puterman, 1994), sake
brevity formalize infinite horizon discounted reward MDPs article.1
agents policy determines actions selects timestep. broadest
sense, policy condition everything known agent. state-indepedent
value function V specifies expected return following initial state:
V = E[R0 | ].

(1)

policy stationary, i.e., conditions current state,
formalized : [0, 1]: specifies, state action, probability
taking action state. specify state value function policy :
V (s) = E[Rt | , st = s],
st = s. Bellman equation restates expectation recursively
stationary policies:
X
X
(s, a, )[R(s, a, ) + V (s )].
V (s) =
(s, a)




Note Bellman equation, forms heart standard solution algorithms
dynamic programming (Bellman, 1957b) temporal-difference methods (Sutton
& Barto, 1998), explicitly relies assumption additive returns. important
because, explain Section 4.2.2, multi-objective settings interfere
additivity property, making planning learning methods rely Bellman
equation inapplicable.
1. formalizations settings, see example overview Van Otterlo Wiering (2012).

69

fiRoijers, Vamplew, Whiteson & Dazeley

State value functions induce partial ordering policies, i.e., better
equal value greater states:


s, V (s) V (s).
special case stationary policy deterministic stationary policy, one
action chosen probability 1 every state. deterministic stationary policy
seen mapping states actions: : A. single-objective MDPs,
always least one optimal policy , i.e., : , stationary deterministic.
Theorem 1. additive infinite-horizon single-objective MDP, exists deterministic stationary optimal policy (see e.g., Howard, 1960; Boutilier et al., 1999).
one optimal policy exists, share value function, known
optimal value function V (s) = max V (s). Bellman optimality equation defines
optimal value function recursively:
X
(s, a, )[R(s, a, ) + V (s )].
V (s) = max




Note that, maximizes actions, equation makes use fact
optimal deterministic stationary policy. optimal policy maximizes
value every state, policy optimal regardless initial state distribution .
However, state-independent value (Equation 1) may well different different
initial state distributions. Using , state value function translated back
state-independent value function (Equation 1):
X
V =
(s)V (s).
sS

multi-objective MDP (MOMDP)2 MDP reward function R :
n describes vector n rewards, one objective, instead scalar.
Similarly, value function V MOMDP specifies expected cumulative discounted
reward vector:

X
k rk+1 | ],
(2)
V = E[
k=0

rt vector rewards received time t. difference single
objective value (Equation 1) multi-objective value (Equation 2) policy
return, underlying sum rewards, vector rather scalar.
stationary policies, also define multi-objective value state:
V (s) = E[


X

k rt+k+1 | , st = s].

(3)

k=0

single-objective MDP, state value functions impose partial ordering

policies compared different states, e.g., possible V (s) > V (s) V (s ) <
2. Multi-objective MDPs confused mixed-observability MDPs (Ong, Png, Hsu, & Lee,
2010), also sometimes abbreviated MOMDP.

70

fiA Survey Multi-Objective Sequential Decision-Making



V (s ). given state, ordering complete, i.e., V (s) must greater than,

equal to, less V (s). true state-independent value functions.
contrast, MOMDP, presence multiple objectives means value
function V (s) state vector expected cumulative rewards instead scalar.
value functions supply partial ordering, even given state. example,


possible that, state s, Vi (s) > Vi (s) Vj (s) < Vj (s). Similarly,


state-independent value functions, may Vi > Vi Vj < Vj . Consequently,
unlike MDP, longer determine values optimal without additional
information prioritize objectives. information provided
form scalarization function, discuss following sections.
Though focus article, also MOMDP variants constraints
specified objectives (see e.g., Feinberg & Shwartz, 1995; Altman, 1999).
goal agent maximize regular objectives meeting constraints
objectives. Constrained objectives fundamentally different regular
objectives explicitly prioritized regular objectives, i.e., policy
fails meet constraint inferior policy meets constraints, regardless
well policies maximize regular objectives.

3. Motivating Scenarios
MOMDP setting received considerable attention, immediately obvious useful addition standard MDP specialized algorithms
needed. fact, researchers argue modeling problems explicitly multiobjective necessary, scalar reward function adequate sequential
decision-making tasks. direct formulation perspective Suttons reward
hypothesis, states mean goals purposes well
thought maximization expected value cumulative sum received scalar
signal (reward).3
view imply multi-objective problems exist. Indeed,
would difficult claim, since easy think problems naturally possess
multiple objectives. Instead, implication reward hypothesis resulting
MOMDPs always converted single-objective MDPs additive returns.
conversion process would involve two steps. first step specify scalarization
function.
Definition 1. scalarization function f , function projects multi-objective
value V scalar value.
Vw (s) = f (V (s), w),
w weight vector parameterizing f .
example, f may compute linear combination values, case element
w quantifies relative importance corresponding objective (this setting discussed Section 4.2.1). second step define single-objective MDP
3. http://rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html

71

fiRoijers, Vamplew, Whiteson & Dazeley

Figure 1: three motivating scenarios MOMDPs: (a) unknown weights scenario,
(b) decision support scenario, (c) known weights scenario.

additive returns that, s, expected return equals scalarized value
Vw (s).
Though rarely, ever, makes issue explicit, research MOMDPs rests
premise exist tasks one conversion steps impossible,
infeasible, undesirable. section, discuss three scenarios occur
(see Figure 1).
first scenario, call unknown weights scenario (Figure 1a), occurs
w unknown moment planning learning must occur. Consider example
public transport system aims minimize latency (i.e., time commuters
need reach destinations) pollution costs. addition, assume resulting
MOMDP scalarized converting objective monetary cost: economists
compute cost lost productivity due commuting pollution incurs tax
must paid pollution credits purchased given price. Assume also credits
traded open market therefore price constantly fluctuates. transport
system complex, may infeasible compute new plan every day given latest
prices. scenario, preferable use multi-objective planning method
computes set policies that, price, one policies optimal
(see planning learning phase Figure 1a). computationally
expensive computing single optimal policy given price, needs done
done advance, computational resources available.
Then, time select policy, current weights, i.e., price pollution
72

fiA Survey Multi-Objective Sequential Decision-Making

credits, used determine best policy set (the selection phase). Finally,
selected policy employed task (the execution phase).
unknown weights scenario, scalarization impossible planning learning
trivial policy actually needs used w known time.
contrast, second scenario, call decision support scenario (Figure
1b), scalarization infeasible throughout entire decision-making process
difficulty specifying w, even f . example, economists may able accurately
compute cost lost productivity due commuting. user may also fuzzy
preferences defy meaningful quantification. example, transport system could
made efficient building new train line obstructs beautiful view,
human designer may able quantify loss beauty. difficulty specifying
exact scalarization especially apparent designer single person
committee legislative body whose members different preferences agendas.
system, MOMDP method used calculate optimal solution set
respect known constraints f w. Figure 1b shows, decision support
scenario proceeds similarly unknown weights scenario except that, selection
phase, user users select policy set according arbitrary preferences,
rather explicit scalarization according given weights.
cases, one still argue scalarization planning learning
possible principle. example, loss beauty quantified measuring
resulting drop housing prices neighborhoods previously enjoyed unobstructed
view. However, difficulty scalarization may impractical
but, importantly, forces users express preferences way may
inconvenient unnatural. selecting w requires weighing hypothetical
trade-offs, much harder choosing set actual alternatives.
well understood phenomenon field decision analysis (Clemen, 1997),
standard workflow involves presenting alternatives soliciting preferences.
subfields decision analysis multiple criteria decision-making multiattribute utility theory focus multiple objectives (Dyer, Fishburn, Steuer, Wallenius, &
Zionts, 1992). reasons, algorithms MOMDPs provide critical decision
support. Rather forcing users specify w advance, algorithms
prune policies would optimal w. Then, offer users range
alternatives select according preferences whose relative importance
easily quantified.
third scenario, call known weights scenario (Figure 1c), assume
w known time planning learning thus scalarization possible
feasible. However, may undesirable difficulty second step
conversion. particular, f nonlinear, resulting single-objective MDP
may additive returns (see Section 4.2.2). result, optimal policy may
non-stationary (see Section 4.3.2) stochastic (see Section 4.3.3), cannot occur
single-objective, additive, infinite-horizon MDPs (see Theorem 1). Consequently, MDP
difficult solve, standard methods applicable. Converting MDP
one additive returns may help either cause blowup state space,
73

fiRoijers, Vamplew, Whiteson & Dazeley

also leaves problem intractable.4 Therefore, even though scalarization possible
w known, may still preferable use methods specially designed MOMDPs
rather convert problem single-objective MDP. contrast unknown
weights decision support scenarios, known weights scenario, MOMDP
method produces one policy, executed, i.e., separate selection
phase, shown Figure 1c.
Note Figure 1 assumes off-line scenario: planning learning occurs once,
execution. However, multi-objective methods also employed on-line settings
planning learning interleaved execution. on-line version
unknown weights scenario, weights better characterized dynamic, rather
unknown. on-line scenario, agent must already seen weights timesteps
> 1 since prerequisite execution timesteps 1, . . . , 1. However,
weights change time, agent may yet know weights used
timestep planning learning phase timestep.

4. Problem Taxonomy
far, described MOMDP formalism proposed three motivating scenarios
it. section, discuss constitutes optimal solution. Unfortunately,
simple answer question, depends several critical factors. Therefore,
propose problem taxonomy, shown Table 1, categorizes MOMDPs according
factors describes nature optimal solution category.
taxonomy based call utility-based approach, contrast many
multi-objective papers follow axiomatic approach optimality MOMDPs.
utility-based approach rests following premise: execution phases
scenarios Section 3, one policy selected collapsing value vector policy
scalar utility, using scalarization function. application scalarization
function may implicit hidden, e.g., may embedded thought-process
user, nonetheless occurs. scalarization function part notion utility,
i.e., agent maximize. Therefore, find set optimal solution
possible weight setting scalarization function, solved MOMDP.
utility-based approach derives optimal solution set assumptions
made scalarization function, policies user allows, whether need
one multiple policies.
contrast, axiomatic approach begins axiom optimal solution set
Pareto front (see Section 4.2.2).5 approach limiting because, demonstrate
section, settings solution concepts suitable.
Thus, take utility-based approach makes possible derive solution
concept, rather assuming it. Pareto front fact correct solution
4. Since non-additive returns depend agents entire history, immediate reward function
converted MDP may also depend history thus state representation converted
MDP must augmented include it.
5. example axiomatic approach multi-objective reinforcement learning, see survey
Liu, Xu, Hu (2013).

74

fiA Survey Multi-Objective Sequential Decision-Making

single policy
(known weights)
deterministic
linear
scalarization

multiple policies
(unknown weights decision support)

stochastic

one deterministic stationary
policy (1)

monotonically one
increasing
deterministic
scalarization
non-stationary
policy (3)

deterministic

stochastic

convex coverage set
deterministic stationary policies
(2)

one mixture
policy two

deterministic
stationary
policies (4)

Pareto
coverage set
deterministic
non-stationary
policies (5)

convex
coverage set
deterministic
stationary
policies (6)

Table 1: MOMDP problem taxonomy showing critical factors problem
nature resulting optimal solution. columns describe whether
problem necessitates single policy multiple ones, whether policies
must deterministic (by specification) allowed stochastic. rows
describe whether scalarization function linear combination rewards
or, whether cannot assumed scalarization function merely
monotonically increasing function them. contents cell describe
optimal solution given setting looks like.

concept, utility-based approach provides justification it. not, allows
appropriate solution concept derived instead.
taxonomy categorizes problem classes based assumptions scalarization function, policies user allows, whether one multiple policies
required. show leads different solution concepts, underscoring importance
carefully considering choice solution concept based available information.
discuss three factors constitute taxonomy following order.
Section 4.1, discuss first factor: whether one multiple policies sought, choice
follows directly motivating scenario applicable. known weights
scenario (Figure 1c) implies single-policy approach unknown weights decision
support scenarios (Figure 1a 1b) imply multiple-policy approach. Section 4.2,
discuss second factor: whether scalarization function linear combination
rewards merely monotonically increasing function them. Section 4.3, discuss
third factor: whether stochastic deterministic policies permitted.
goal taxonomy cover research MOMDPs remaining simple
intuitive. However, due diversity research MOMDPs, research
fit neatly taxonomy. note discrepancies discussing research
Sections 5 6.
75

fiRoijers, Vamplew, Whiteson & Dazeley

4.1 Single versus Multiple Policies
Following approach Vamplew et al. (2011), first distinguish problems
one policy sought ones multiple policies sought. case holds
depends three motivating scenarios discussed Section 3 applies.
unknown weights decision support scenarios, solution MOMDP
consists multiple policies. Though two scenarios conceptually quite different,
algorithmic perspective identical. reason characterized strict separation decision-making process two phases: planning
learning phase execution phase (though on-line settings, agent may go
back forth two).
planning learning phase, w unavailable. Consequently, planning learning algorithm must return single policy set policies (and corresponding
multi-objective values). set contain policies suboptimal
scalarizations, i.e. interested undominated policies.
Definition 2. MOMDP scalarization function f , set undominated
policies, U (m ), subset possible policies exists w
scalarized value maximal:


U (m ) = { : w( ) Vw Vw }.

(4)

U (m ) sufficient solve m, i.e., w, contains policy optimal
scalarized value. However, may contain redundant policies that, optimal
weights, optimal policy set w. policies removed
still ensuring set contains optimal policy w. fact, order solve
m, need subset undominated policies that, possible w,
least one policy set optimal. sometimes called coverage set (CS) (Becker,
Zilberstein, Lesser, & Goldman, 2003).
Definition 3. MOMDP scalarization function f , set CS(m )
coverage set subset U (m ) if, every w, contains policy maximal
scalarized value, i.e., if:









CS( ) U ( ) (w)() CS( ) ( ) Vw Vw .
(5)
Note U (m ) automatically coverage set. However, U (m ) unique, CS(m )
need be. multiple policies value, U (m ) contains
them, coverage set need contain one. addition, given CS(m ),

may exist policy
/ CS(m ) V different V CS(m )
scalarized value CS(m ) w optimal.
contrast single-objective MDPs, MOMDPs whether policy CS(m )
depend initial state distribution . thus important accurately specify
formulating MOMDP.
Ideally, MOMDP algorithm find smallest CS(m ). However,
might harder finding one smaller U (m ). Section 4.2, specialize
coverage set two classes scalarization functions.
76

fiA Survey Multi-Objective Sequential Decision-Making

execution phase, single policy chosen set returned planning
learning phase executed. unknown weights scenario, assume w revealed
planning learning complete execution begins. Selecting policy
requires maximizing scalarized value policy returned set:
= argmax Vw .
CS(m )

decision support scenario, set manually inspected user(s), select
policy execution informally, making implicit trade-off objectives.
known weights scenario, w known planning learning begins. Therefore,
returning multiple policies unnecessary. However, mentioned Section 3 discussed
Section 4.2.2, scalarization yield single-objective MDP difficult
solve.
4.2 Linear versus Monotonically Increasing Scalarization Functions
second critical factor affecting constitutes optimal solution MOMDP
nature scalarization function. section, discuss two types scalarization
function: linear combinations rewards merely
monotonically increasing functions them.
4.2.1 Linear Scalarization Functions
common assumption scalarization function (e.g., Natarajan & Tadepalli, 2005;
Barrett & Narayanan, 2008), f linear, i.e., computes weighted sum
values objective.
Definition 4. linear scalarization function computes inner product weight vector
w value vector V
Vw = w V .
(6)
element w specifies much one unit value corresponding objective
contributes scalarized value. elements weight vector w positive real
numbers constrained sum 1.
Linear scalarization functions simple intuitive way scalarize. One common
situation applicable rewards easily translated monetary
value. example, consider mining task different policies yield different expected
quantities various minerals. prices per kilo minerals fluctuate daily,
task formulated MOMDP, objective corresponding different
mineral. element V reflects expected number kilos mineral
mined scalarized value Vw corresponds monetary value
everything mined. Vw computed w, corresponding
(normalized) current price per kilo mineral, becomes known.
single-policy setting, w known, presence multiple objectives poses
difficulties given linear f . Instead, f simply applied reward vector
77

fiRoijers, Vamplew, Whiteson & Dazeley

MOMDP. inner product computed f distributes addition, result
single-objective MDP additive returns. infinite horizon setting leads to:
Vw = w V = w E[


X

k rt+k+1 ] = E[


X

k (w rt+k+1 )].

(7)

k=0

k=0

Since single-objective MDP additive returns, solved standard methods, yielding single policy, reflected box labeled (1) Table 1. Due Theorem
1, determinstic stationary policy suffices. However, multi-objective approach still
preferable case, e.g., V may easier estimate Vw large continuous
MOMDPs function approximation required (see Section 6.1).
multiple policy setting, however, know w planning learning
therefore want find coverage set. f linear, U (m ), automatically
coverage set, consists convex hull. Substituting Equation 6 definition
undominated set (Definition 2), obtain definition convex hull:
Definition 5. MOMDP m, convex hull (CH) subset
exists w linearly scalarized value maximal:


CH(m ) = { : w( ) w V w V }.

(8)

Figure 2a illustrates concept convex hull stationary deterministic policies.
point plot represents multi-objective value given policy two-objective
MOMDP. axes represent reward dimensions. convex hull shown set
filled circles, connected lines form convex surface.6 Given linear f ,
scalarized value policy linear function weights. illustrated
Figure 2b, x-axis represents weight dimension 0 (w[1] = 1 w[0]),
y-axis scalarized value policies. select policy, need know
values convex hull policies, form upper surface scalarized value,
illustrated black solid lines, correspond three convex hull policies
Figure 2a. upper surface forms piecewise linear convex function. functions
also well-known literature partially-observable Markov decision processes
(POMDPs), whose relationship MOMDPs discuss Section 5.2.
Like U (m ), CH(m ) contain superfluous policies. However, also define
convex coverage set (CCS) specification coverage set f linear.
reflected box (2) Table 1 (we explain policies set deterministic
stationary Section 4.3.1).
Definition 6. MOMDP m, set CCS(m ) convex coverage set
subset CH(m ) if, every w, contains policy whose linearly scalarized value
maximal, i.e., if:



CCS(m ) CH(m ) (w)() CCS(m ) ( ) w V w V . (9)
6. Note term convex hull slightly different meaning multi-objective literature
standard geometric definition. geometry, convex hull finite set points Euclidean
space minimal subset points expressed convex
combination points convex hull. multi-objective setting, interested
particular subset geometric convex hull; points convex combinations strictly
bigger (in dimensions) point S, i.e., points optimal weight.

78

fiA Survey Multi-Objective Sequential Decision-Making

(a)

(b)

Figure 2: Example convex hull Pareto front. point (a) represents
multi-objective value given policy line (b) represents linearly
scalarized value policy across values w. convex hull shown black
filled circles (a), black lines (b). Pareto front consists filled
points (circles squares) (a), dashed solid black lines
(b). unfilled points (a) (grey lines (b)) dominated.

deterministic stationary policies, difference CH(m ) CCS(m ) may
often small. Therefore, terms often used interchangeably. However, case
non-stationary stochastic policies, difference quite significant, CH
contain infinitely many policies, possible construct finite CCS, show
Section 4.3.1.
4.2.2 Monotonically Increasing Scalarization Functions
linear scalarization functions intuitive simple, always adequate
expressing users preferences. example, suppose mining task mentioned
above, two minerals mined three policies available: 1
sends mining equipment location first mineral mined, 2
location second mineral mined, 3 location
minerals mined. Suppose owner equipment prefers 3 , e.g.,
least partially appeases clients different interests. However, may case that,
location corresponding 3 fewer minerals, convex hull contains
1 2 . Thus, owners preference 3 implies she, implicitly explicitly,
employs nonlinear scalarization function.
Here, consider case f nonlinear, corresponds common
notion relationship reward utility. class possibly nonlinear scalarizations strictly monotonically increasing scalarization functions. functions
adhere constraint policy changed way value increases
79

fiRoijers, Vamplew, Whiteson & Dazeley

one objectives, without decreasing objectives, scalarized
value also increases.
Definition 7. scalarization function f strictly monotonically increasing if:






(i, Vi Vi i, Vi > Vi ) (w, Vw > Vw ).

(10)

Linear scalarization functions (with non-zero positive weights) included class
functions. condition left-hand side Equation 10 commonly known
Pareto dominance (Pareto, 1896).
Definition 8. policy Pareto-dominates another policy value least
high objectives strictly higher least one objective:






V P V i, Vi Vi i, Vi > Vi .

(11)

Demanding f strictly monotonically increasing quite minimal constraint,
requires that, things equal, getting reward certain objective
always better. fact, difficult think f violates constraint without
employing highly unnatural notion reward.7
Three observations order strictly monotonically increasing scalarization
functions related concept Pareto dominance. First, unlike linear case,
necessarily know exact shape f . Instead, know belongs
particular class functions. solution concept follows thus applies strictly
monotonically increasing f . cases stronger assumptions f made,
specific solution concepts possible. However, except linearity, aware
properties f exploited solving MOMDPs.
Second, notions optimality introduced Section 4.2.1 longer appropriate.
reason that, even though vector-valued returns still additive (Equation 2),
scalarized returns may f may longer linear. example, consider
well-known Tchebycheff scalarization function (Perny & Weng, 2010)8 :
X
(V , p, w) = max wi |pi Vi |
wi |pi Vi |,
(12)
i1...n

i1...n

p optimistic reference point, w weights, arbitrarily small positive
constant greater 0. Note sum righthand side makes function
strictly monotonically increasing. Now, p = (3, 3),P = 0.01, r1 = (0, 3), r2 = (3, 0),
k
w = (0.5, 0.5) = 1, f (V , w) = 0 E[
k=0 f (rt+k+1 , w)] = (1.515) +
(1.515) = 3.03. loss additivity scalarized returns applying nonlinear
f important consequences methods applied, show Section 4.3.2.
Third, still identify prune policies optimal w
strictly monotonically increasing f , even though may nonlinear. Consider three
7. addition, f strictly monotonically increasing assumptions made,
policies pruned coverage set. Thus, computing value every policy coverage
set, required selection phase, likely intractable.
8. definition differs slightly Perny Weng (2010): multiplied 1 express
maximization instead minimization, sake consistency rest article.

80

fiA Survey Multi-Objective Sequential Decision-Making

labeled policies Figure 2a (note Figure 2b apply, scalarization
function longer linear). B higher value one objective, lower
value other. therefore cannot tell whether B ought preferred without
knowing w. However, C lower value objectives, thus Paretodominates C: P C. f strictly monotonically increasing, scalarized value
greater C w thus discard C.
now, defer full discussion constitutes optimal solution
MOMDP strictly monotonically increasing scalarization function (i.e., boxes (3)-(6)
Table 1) depends, whether single multiple policy setting
applies, also whether deterministic also stochastic policies considered,
addressed Section 4.3.
However, already observe that, given strictly monotonically increasing f ,
use Pareto front set viable policies. Pareto front consists policies
Pareto dominated.
Definition 9. MOMDP m, Pareto front set policies
Pareto dominated policy :


P F (m ) = { : ( ), V P V }.

(13)

Note P F (m ) set undominated policies U (m ) specific strictly
monotonically increasing f . already seen special case linear f ,
U (m ) = CH(m ), subset P F (m ). (For example, Figure 2, Pareto
front consists convex hull plus B.) However, strictly monotonically increasing
f , know policy P F (m ) dominated respect f , i.e.,
6 P F (m ) 6 U (m ). because, strictly monotonically increasing f

/ P F (m ), cannot exist w optimal, since definition exists

V P V and, since f strictly monotonically increasing, implies

Vw > Vw .
However, know f strictly monotonically increasing, cannot settle
subset P F (m ) either, exist strictly monotonically increasing f
U (m ) = P F (m ). Perny Weng (2010) show U (m ) = P F (m )
Tchebycheff function (Equation 12), strictly monotonically increasing. Therefore,
cannot discard policies P F (m ) retain undominated set U (m )
strictly monotonically increasing f .
Pareto coverage set (PCS) minimal size constructed retaining one
policy policies identical vector values P F (m ). formally define
PCS follows:
Definition 10. MOMDP m, set P CS(m ) Pareto coverage set subset
P F (m ) if, every policy , contains policy either dominates
equal value , i.e., if:




P CS(m ) P F (m ) ( )() P CS(m ) (V P V V = V ) . (14)
Again, deterministic stationary policies difference P CS(m ) P F (m )
may minor. Note P F (m ) automatically P CS(m ). papers
literature therefore take P F (m ) solution.
81

fiRoijers, Vamplew, Whiteson & Dazeley

also slightly relax constraint f , without change policies
P CS(m ). Specifically, define monotonically increasing scalarization

function function following property holds: (i, Vi Vi ) (w, Vw

Vw ). relaxation influences set undominated policies: policies
P F (m ) always dominated strictly monotonically increasing f , need
monotonically increasing f . Consider example f (V , w) = 0,
monotonically increasing strictly monotonically increasing. function
dominated policies, every policy scalarized value. However,
scalarized value policy 6 P F (m ) cannot greater scalarized
function policy P CS(m ), use P CS(m ) (non-strict) monotonically
increasing f . Therefore, article, focus monotonically increasing f ,
broader class functions.
P F (m ), even P CS(m ), may prohibitively large contain
many policies whose values differ negligible amounts, Chatterjee et al. (2006) Brazdil
et al. (2011) introduce slack parameter , use define -approximate Pareto
front, P F (m ). P F (m ) contains values policies every possible policy

policy P F (m ) Vi (s) + Vi (s). weakening
requirements domination, approach yields smaller set calculated
efficiently.
Another option finding smaller set P F (m ) making additional assumptions
scalarization function. example, Perny, Weng, Goldsmith, Hanna (2013)
introduce notion fairness objectives, leading Lorentz optimality.
additional assumption sum values objectives stays same,
making difference two objectives smaller yields higher scalarized value.
course strong assumption apply broadly Pareto optimality. However,
apply, help reduce size optimal solution set.
4.3 Deterministic versus Stochastic Policies
third critical factor affecting constitutes optimal solution MOMDP
whether deterministic polices considered stochastic ones also allowed.
applications reason exclude stochastic policies priori,
cases stochastic policies clearly undesirable even unethical. example,
policy determines clinical treatment patient, e.g., work Lizotte, Bowling,
Murphy (2010) Shortreed, Laber, Lizotte, Stroup, Pineau, Murphy (2011),
flipping coin determine course action may inappropriate. denote

set deterministic policies
set stationary policies . sets subsets




policies: . Finally set policies deterministic


stationary intersection sets, denoted
DS = .
single-objective MDPs, factor critical because, due Theorem 1,
restrict search deterministic stationary policies, i.e. optimal attainable value

V . However,
attainable deterministic stationary policy: maxm V = max



DS

situation complex MOMDPs. section, discuss focus
stochastic deterministic policies affects setting considered taxonomy.
82

fiA Survey Multi-Objective Sequential Decision-Making

4.3.1 Deterministic Stochastic Policies Linear Scalarization
Functions
f linear, result similar Theorem 1 holds MOMDPs due following
corollary:

Corollary 1. MOMDP m, CCS(m
DS ) also CCS( ).

Proof. f linear, translate MOMDP single-objective MDP,
possible w. done treating inner product reward vector w
new rewards, leaving rest problem is. Since inner product distributes
addition, scalarized returns remain additive (Equation 7). Thus, every w
exists translation single-objective MDP, optimal deterministic
stationary policy must exist, due Theorem 1. Hence, w exists optimal
deterministic stationary policy. Therefore, exists CCS(m
DS ) optimal



w. Consequently, cannot exist \ DS w V > w V

thus CCS(m
DS ) also CCS( ).
CCS(m
DS ) thus sufficient solving MOMDPs linear f , even stochastic non-stationary policies allowed. reflected box (2) Table 1. also
applies box (1) since optimal policy case member CCS(m
DS ),
i.e., one best given known w.
Unfortunately, result analogous Corollary 1 holds MOMDPs monotonically increasing f . rest section, discuss consequences
nature optimal MOMDP solution boxes (3)-(6) Table 1.
4.3.2 Multiple Deterministic Policies Monotonically Increasing
Scalarization Functions
multiple-policy setting deterministic policies allowed f nonlinear,
non-stationary policies may better best stationary ones.
Theorem 2. infinite-horizon MOMDPs, deterministic non-stationary policies Paretodominate deterministic stationary policies undominated deterministic stationary policies (White, 1982).
see why, consider following MOMDP, denoted m1, adapted example
White (1982). one state three actions a1 , a2 , a3 , yield rewards
(3, 0), (0, 3), (1, 1), respectively. allow deterministic stationary policies,
three possible policies 1 , 2 , 3 m1
DS , corresponding always taking
one actions, Pareto optimal. policies following
state-independent values (Equation 2): V1 = (3/(1 ), 0), V2 = (0, 3/(1 )),
V3 = (1/(1 ), 1/(1 )). However, consider set possibly non-stationary
m1
m1
policies m1
(including non-stationary ones), construct policy ns \ DS
alternates a1 a2 , starting a1 , whose value Vns = (3/(1
2 ), 3/(1 2 )). Consequently, ns P 3 > 0.5 thus cannot restrict
83

fiRoijers, Vamplew, Whiteson & Dazeley

attention stationary policies.9 Consequently, multiple deterministic policies case
monotonically increasing f , need find P CS(m
), includes non-stationary
policies, shown box (5) Table 1.
addition consider broader class policies, another consequence
defining policy indirectly via value function longer possible. standard
single-objective methods, optimal policy found local action selection
respect value function: i.e., every state, policy selects action
maximizes expected value. However, local selection yield non-stationary policy, value function must also non-stationary, i.e., must condition current
timestep. standard finite-horizon setting, different value function computed timestep, possible infinite-horizon setting.
discuss address difficulty Sections 5 6.
4.3.3 Multiple Stochastic Policies Monotonically Increasing
Scalarization Functions
multiple policy setting stochastic non-stationary policies, i.e., full set ,
allowed, cannot consider deterministic stationary policies. However,
employ stochastic stationary policies instead deterministic non-stationary ones.
particular, employ mixture policy (Vamplew, Dazeley, Barker, & Kelarev, 2009)
takes set N deterministic
policies, selects i-th policy set,
P
probability pi , N
p
=
1.
leads values linear combination
i=0
values constituent policies. previous example, replace ns
policy chooses 1 probability p1 2 otherwise, resulting following
values:


3p1 3(1 p1 )

1
2
V = p1 V + (1 p1 )V =
,
.
1
1
Fortunately, necessary explicitly represent entire P CS(m ) explicitly.
Instead, sufficient compute CCS(m
DS ). necessary stochastic policies
create P CS(m ) easily constructed making mixture policies
policies CCS(m
DS ).
Corollary 2. infinite horizon discounted MOMDP, infinite set mixture policies
PM constructed policies CCS(m
DS ), set PM ,

P CS( ) (Vamplew et al., 2009).
Proof. construct policy value vector convex surface, e.g.,
10 Thereblack lines Figure 2a, mixing policies CCS(m
DS ), e.g., black dots.
fore, always construct mixture policy dominates policy value
surface, e.g., B. show contradiction cannot policy
9. White (1982) shows infinite-horizon discounted setting, arguments hold also
finite-horizon average-reward settings.
10. Note always mix policies adjacent; line pair policies
mix convex surface. E.g. mixing policy represented leftmost black dot
Figure 2a policy represented rightmost black dot lead optimal policies,
line connecting two points convex surface.

84

fiA Survey Multi-Objective Sequential Decision-Making

convex surface. was, would optimal w f linear. Consequently, Corollary 1, would deterministic stationary policy least
equal value. since convex surface spans values CCS(m
DS ), leads
contradiction. Thus, policy Pareto-dominate mixture policy convex
surface.
Thanks Corollary 2, sufficient compute CCS(m
DS ) solve MOMDP,
reflected box (6) Table 1. surprising consequence fact, knowledge
made explicit literature, Pareto optimality, though common
solution concept associated multi-objective problems, actually necessary one
specific problem setting:
Observation 1. multiple policy setting f monotonically increasing
deterministic policies considered (box (5) Table 1), requires computing Pareto coverage set. either f linear stochastic policies allowed, CCS(m
DS ) suffices.
Wakuta (1999) proves sufficiency CCS(m
DS ) monotonically increasing
scalarizations multiple stochastic policies (box (6) Table 1) infinite horizon
MOMDPs, different way. Instead mixture policies Corollary 2, uses stationary randomizations deterministic stationary policies. Wakuta Togawa (1998)
provide similar proof average reward case.
Note that, common consider non-stationary stochastic policies f
nonlinear, policies typically condition current state, current state
time, agents reward history. However, setting, policies condition
reward history dominate not. example, suppose two
objectives take positive values f simply selects smaller two, i.e.,
f (V , w) = mini Vi . Suppose also that, given state, two actions available,
yields rewards (4, 4) (0, 5) respectively. Finally, suppose agent arrive
state one two reward histories, whose discounted sums either (5, 0) (3, 3).
policy conditions discounted reward histories outperform policies
not, i.e., optimal policy selects action yielding (4, 4) reward history sums
(3, 3) action yielding (0, 5) reward history sums (5, 0). So,
single objective MDPs Markov property additive returns sufficient restrict
attention policies ignore history, multi-objective case, scalarized returns
longer additive therefore optimal policy depend history. Examples
methods exploit fact steering approach (Mannor & Shimkin, 2001)
reward-augmented-state thresholded lexicographic ordering method Geibel (2006),
discussed Section 6.1.
4.3.4 Single Deterministic Stochastic Policies Monotonically
Increasing Scalarization Functions
remains address single-policy setting monotonically increasing f .
nature optimal solution case follows directly reasoning given
multiple-policy setting.
deterministic policies considered, single policy sought may
non-stationary, reflected box (3) Table 1, reasons elucidated Whites
85

fiRoijers, Vamplew, Whiteson & Dazeley

example. Again, hard define non-stationary policy local action selection,
due risk circular dependencies Q-values.
stochastic policies allowed, optimal policy may stochastic,
represented mixture policy two deterministic stationary policies,
reflected box (4) Table 1, reasons given Corollary 2. cases,
policies potentially benefit conditioning reward history.

5. Planning MOMDPs
section, survey key approaches planning MOMDPs, i.e., computing
optimal policy coverage set undominated policies given complete model
MOMDP. Following taxonomy presented Section 4, first consider single-policy
methods turn multiple-policy methods linear monotonically increasing
scalarization functions.
5.1 Single-Policy Planning
known weights scenario, w known planning begins, single
policy, optimal w, must discovered. Since MOMDP transformed
single-objective MDP f linear (see Section 4.2.1), focus single-policy
planning nonlinear f .
discussed Section 4.2.2, nonlinear f cause scalarized return nonadditive. Consequently, single-objective dynamic programming linear programming
methods, exploit assumption additive returns employing Bellman equation, applicable. However, different linear programming formulations singlepolicy planning MOMDPs possible. key feature methods
produce stochastic policies, which, discussed Section 4, optimal
scalarization function nonlinear. aware single-policy planning
methods work arbitrary nonlinear f , methods developed two special
cases. particular, Perny Weng (2010) propose linear programming method
MOMDPs scalarized using Tchebycheff function mentioned Section 4.2.2.
Tchebycheff function always w Pareto-optimal policy optimal,
approach find (single) policy Pareto front. addition, Ogryczak, Perny,
Weng (2011) propose analogous method ordered weighted regret metric.
metric calculates regret objective respect estimated ideal reference
point, sorts descending order, calculates weighted sum weights
also descending order.
researchers proposed single-policy methods MOMDPs constraints.
Feinberg Shwartz (1995) consider MOMDPs one regular objective objectives inequality constraints. show feasible policy exists setting,
deterministic stationary finite number timesteps N that,
prior timestep N , random actions must performed. call (M, N )
policy, show Pareto-optimal values achieved (M, N ) policies, propose
linear programming algorithm finds -approximate policies setting.
general MOMDPs constraints also considered. particular, Altman (1999)
proposes several linear programming approaches settings.
86

fiA Survey Multi-Objective Sequential Decision-Making

Furnkranz, Hullermeier, Cheng, Park (2012) propose framework MDPs
qualitative reward signals, related MOMDPs fit neatly
taxonomy. Qualitative reward signals indicate preference policies actions
without directly ascribing numeric value them. Since preferences induce partial
ordering policies, policy iteration method authors propose setting
may applicable MOMDPs nonlinear f , Pareto dominance also induces partial
orderings. However, authors note multi-objective tasks generally numeric
feedback exploited. Thus, suggest quantitative MOMDPs
viewed subset preference-based MDPs, methods designed specifically
MOMDPs may efficient general preference-based methods.
5.2 Multiple-Policy Planning Linear Scalarization Functions
multiple-policy setting linear f , seek CCS(m
DS ). Note however,
distinction convex hull convex coverage set usually made
literature.
One might argue explicitly multi-objective methods necessary setting, one could repeatedly run single-objective methods obtain CCS(m
DS ).
However, since infinitely many possible w, obvious possible values w covered. might possible devise way run single-objective
methods finite number times still guarantee CCS(m
DS ) produced. However,
would nontrivial result corresponding algorithm would essence
multi-objective method happens use single-objective methods subroutines.
One approach attempted find minimally sized CCS(m
), i.e., convex
coverage set deterministic necessarily stationary policies, originally proposed
White Kim (1980), translate MOMDP partially observable Markov
decision process (POMDP) (Sondik, 1971). intuitive way think translation
imagine fact one true objective agent unaware
objectives MOMDP is. modeled POMDP defining state
tuple hs1 , s2 s1 state MOMDP s2 {1 . . . n} indicates
true objective. observations thus identify s1 exactly give information
s2 . Note translation MOMDPs POMDPs one-way only. every
POMDP translated equivalent MOMDP.
Typically, agent interacting POMDP maintains belief, i.e., probability
distribution states. POMDP derived MOMDP, belief decomposed belief s1 belief s2 . former degenerative s1
known. latter vector size n i-th element specifies probability
i-th objective true one. vector analogous w linear f .
fact, reason Figure 2b resembles piecewise linear value functions often
depicted POMDPs; difference whether x-axis interpreted w
belief.
White Kim (1980) show that, finite horizon case, solution every belief
exactly solution w, solutions resulting POMDP
exactly original MOMDP. infinite horizon case difficult
infinite horizon POMDPs undecidable (Madani, Hanks, & Condon, 1999). However,
87

fiRoijers, Vamplew, Whiteson & Dazeley

sufficiently large horizon, solution finite horizon POMDP used
approximate solution infinite horizon MOMDP.
solve resulting POMDP, White Kim (1980) propose combination Sondiks
one-pass algorithm (Smallwood & Sondik, 1973) policy iteration POMDPs (Sondik,
1978). However, POMDP planning method used long (1)
require initial belief POMDP state (which would correspond initializing
MOMDP state also w) (2) computes optimal policy every
possible belief. recently developed exact methods, e.g., Cassandra, Littman,
Zhang (1997) Kaelbling, Littman, Cassandra (1998), meet conditions
could thus employed. Approximate point-based POMDP methods (Spaan & Vlassis,
2005; Pineau, Gordon, & Thrun, 2006) meet conditions (1) (2) could
adapted compute approximate convex hull, choosing prior distribution
weights could sample. Online POMDP planning methods (Ross, Pineau,
Paquet, & Chaib-draa, 2008) applicable plan given belief.
Converting POMDP thus allows use POMDP methods
solving MOMDPs linear f . However, approach inefficient
exploit characteristics distinguish MOMDPs general POMDPs, i.e.,
part state, s1 , known observations give information
s2 . example, methods compute policies trees, e.g., (Kaelbling et al., 1998)
exploit fact deterministic policies stationary functions state
needed MOMDPs linear f . Furthermore, mentioned before, general infinite
horizon POMDPs undecidable, MOMDPs fact possible compute
CCS(m
DS ) exactly.
reasons, researchers also developed specialized planning methods
setting. Viswanathan, Aggarwal, Nair (1977) propose linear programming approach
episodic MOMDPs. Wakuta Togawa (1998) propose policy iteration approach
three phases. first phase uses policy iteration narrow set
possibly optimal policies. second phase uses linear programs check optimality.
Since necessarily give definitive answer, third phase uses another linear
program handle undetermined solutions left second phase.
Barrett Narayanan (2008) propose convex hull value iteration (CHVI), computes CH(m
), every state. CHVI extends conventional value iteration storing
DS
set vectors, Q (s, a) state-action pair, representing convex hull policies involving action. sets vectors correspond Q-values single-objective
setting; contain optimal Q-values possible w. backup operation
performed, Q-hulls next state propagated back s. possible next

state , possible actions considered (i.e. union convex hulls Q (s , )
taken), weighted probability occurring taking action state s.
procedure similar witness algorithm (Kaelbling et al., 1998) POMDPs.
Lizotte et al. (2010) propose value-iteration approach finite-horizon setting
computes different value function timestep. addition, uses piecewise
linear spline representation value functions. authors prove offers asymptotic
time space complexity improvements representation used CHVI also
enables application algorithm MOMDPs continuous states. However,
88

fiA Survey Multi-Objective Sequential Decision-Making

algorithm applicable problems two objectives. limitation addressed
authors subsequent work (Lizotte, Bowling, & Murphy, 2012) extends
algorithm arbitrary number objectives provides detailed implementation
case three objectives.
5.3 Multiple-Policy Planning Monotonically Increasing Scalarization
Functions
section, consider planning MOMDPs monotonically increasing f . discussed Section 4.3, stochastic policies allowed, mixture policies deterministic
stationary policies sufficient. Therefore, focus case deterministic
policies allowed consider methods compute P CS(m
), include
non-stationary policies. distinction P F (m
)

P
CS(m

) usually
made literature.
linear case, scalarizing every w obtaining P CS(m
) singleobjective methods problematic. infinitely many w consider but, unlike
linear case, additional difficulty scalarized returns may longer
additive, make single-objective methods inapplicable.
Daellenbach Kluyver (1980) present algorithm multi-objective routing tasks
(essentially deterministic MOMDPs). approach uses dynamic programming conjunction augmented state space find non-Pareto-dominated policies iteratively,
number iterations equals maximum number steps route. algorithm finds undominated sub-policies parallel. authors use two alternative explicit
scalarization functions, call weighted minsum weighted minmax operators. First, values solutions translated : objective, new value
becomes fractional difference optimal values objective across
solutions. Then, value objective multiplied positive weight. Finally,
either minimum sum (minsum) minimum maximal value (minmax )
new weighted fractional differences chosen scalarization. Note
scalarization functions monotonically increasing objectives, optimal value
objective individually depend scalarization function.
White (1982) extends work proposing dynamic programming method
approximately solves infinite horizon MOMDPs. repeatedly backing according multi-objective version Bellman equation. Since policies
non-stationary, size Pareto front grows rapidly number backups applied.
However, White notes number need large acceptable approximations reached. Nonetheless, approach feasible small MOMDPs.
Wiering De Jong (2007) address difficulty dynamic programming method
called CON-MODP deterministic MOMDPs computes optimal stationary policies.
CON-MODP works enforcing consistency DP updates: policy consistent
suggests action timesteps given state. inconsistent policy
inconsistent one state-action pair, CON-MODP makes consistent forcing
current action taken time current state visited. inconsistency runs
deeper, policy discarded.
89

fiRoijers, Vamplew, Whiteson & Dazeley

contrast, Gong (1992) proposes linear programming approach finds Paretofront stationary policies. However, authors note, approach also suitable
small MOMDPs number constraints decision variables
linear program increase rapidly state space grows.
mentioned Section 4.2.2, one way cope intractably large Pareto fronts
compute instead -approximate Pareto front, much smaller. Chatterjee
et al. (2006) propose linear programming method computes -approximate front
infinite horizon MOMDP, Chatterjee (2007) propose analogous algorithm
average reward setting. cases, stationary stochastic policies shown
sufficient.
Another way improve scalability setting give planning whole
state space instead plan on-line agents current state, using Monte Carlo tree
search approach (Kocsis & Szepesvari, 2006). approaches, proven
successful, e.g., game Go (Gelly & Silver, 2011), increasingly popular
single-objective MDPs. Wang Sebag (2013) propose Monte Carlo tree search method
deterministic MOMDPs. Single-objective tree search methods typically optimistically
explore tree selecting actions maximize upper confidence bound
value estimates. multi-objective variant same, respect scalar
multi-objective value function whose definition based hypervolume indicator induced proposed action together set Pareto optimal policies computed
far. hypervolume indicator (Zitzler, Thiele, Laumanns, Fonseca, & da Fonseca, 2003)
measures hypervolume Pareto-dominated set points. Since Pareto
front maximizes hypervolume indicator, optimistic action selection strategy focuses
tree search branches likely compliment existing archive.

6. Learning MOMDPs
methods reviewed Section 5 assume model transition reward
dynamics MOMDP known. cases model directly available,
multi-objective reinforcement learning (MORL) used instead.
One way carry MORL take model-based approach, i.e., use agents
interaction environment learn model transition reward function
MOMDP apply multi-objective planning methods described
Section 5. Though approach seems well suited MORL, papers
considered it, (e.g., Lizotte et al., 2010, 2012). discuss opportunities future work
model-based MORL Section 8.1. Instead, work MORL focused
model-free methods, model transition reward function never explicitly
learned.
section, survey key MORL approaches. majority
methods single-policy setting, multiple-policy methods also developed. first glance, may seem multiple-policy methods unlikely effective
learning setting, since finding policies would increase sample costs,
computational costs, former typically much scarcer resource. However, modelbased methods obviate issue: enough samples gathered learn
useful model, finding policies optimal weights requires computation. Model90

fiA Survey Multi-Objective Sequential Decision-Making

free methods also practical multiple-policy setting employ off-policy
learning (Sutton & Barto, 1998; Precup, Sutton, & Dasgupta, 2001), makes possible learn one policy using data gathered another. way, policies
multiple weight settings optimized using data.
6.1 Single-Policy Learning Methods
known weights scenario, MORL algorithm aims learn single policy
optimal given weights. discussed Section 5.1, linear scalarization
equivalent learning optimal policy single-objective MDP standard
temporal-difference (TD) methods (Sutton, 1988) Q-learning (Watkins, 1989)
easily applied.
However, even though specialized methods needed address setting,
nonetheless commonly studied setting MORL. Linear scalarization
uniform weights, i.e., elements w equal, forms basis work Karlsson
(1997), Ferreira, Bianchi, Ribeiro (2012), Aissani, Beldjilali, Trentesaux (2008)
Shabani (2009) amongst others, non-uniform weights used authors
Castelletti et al. (2002), Guo et al. (2009) Perez et al. (2009). majority
work uses TD methods, work on-line, although Castelletti et al. (2010) extend off-line
Fitted Q-Iteration (Ernst, Geurts, & Wehenkel, 2005) multiple objectives.
cases, change made underlying RL algorithm that, rather
scalarizing reward function learning scalar value function resulting
single-objective MDP, vector-valued value function learned original MOMDP
scalarized selecting actions. argument approach
values individual objectives may easier learn scalarized value, particularly
function approximation employed (Tesauro et al., 2007). example, function
approximator ignore state variables irrelevant objective, reducing
size state space thereby speeding learning.
discussed Section 4.2.2, linear scalarization may appropriate scenarios. Vamplew, Yearwood, Dazeley, Berry (2008) demonstrate empirically
practical consequences MORL. Therefore, MORL methods work
nonlinear scalarization functions substantial importance. Unfortunately, illustrated
Section 4.2.2, coping setting especially challenging, since algorithms
TD methods based Bellman equation inherently incompatible
nonlinear scalarization functions due non-additive nature scalarized returns.
Four main classes single-policy MORL methods using non-linear scalarization
arisen, differ deal issue. first class simply applies TD
methods without modification. approaches either resign heuristics guaranteed converge impose restrictions environment ensure
convergence. second class modifies either TD algorithm state representation
issue non-additive returns avoided. third class uses TD methods
learn multiple policies using linear scalarization different values w,
forms stochastic non-stationary meta-policy optimal respect
nonlinear scalarization. fourth class uses policy-search methods,
91

fiRoijers, Vamplew, Whiteson & Dazeley

make use Bellman equation hence directly applied combination
nonlinear scalarizations.
first class includes methods model problem multi-agent system,
one agent per objective. agent learns recommends actions basis
return objective. global switch selects winning agent, whose recommended action followed current state. Examples include simple winner-takes-all
approach agent whose recommended action highest Q-value selected,
sophisticated approaches W-learning (Humphrys, 1996) selected
action one incur loss followed. One key weakness
approaches pointed Russell Zimdars (2003): allow
selection actions that, optimal single objective, offer good compromise
multiple objectives. Another key weakness that, since actions selected
different timesteps may recommended different agents, resulting behavior corresponds policy combines elements learned agent. combination
may optimal even single objective, i.e., may Pareto dominated perform
arbitrarily poorly.
TD also used directly nonlinear scalarization functions allow
consideration actions, optimal regards individual
objectives. Scalarization functions based fuzzy logic proposed problems
discrete actions Zhao, Chen, Hu (2010) problems continuous
actions Lin Chung (1999). widely cited approach nonlinear scalarization
Gabor, Kalmar, Szepesvari (1998), designed tasks constraints
must satisfied objectives. lexicographic ordering objectives defined
threshold value specified objectives except last. State-action values
objective exceed corresponding threshold clamped threshold value
prior applying lexicographic ordering. Thus, thresholded lexicographic ordering
(TLO) approach scalarization maximizes performance last objective subject
meeting constraints objectives specified thresholds.
methods combining TD nonlinear scalarization may converge suitable
policy certain conditions, also converge suboptimal policy even fail
converge conditions. example, Issabekov Vamplew (2012) demonstrate empirically TLO fail converge suitable policy episodic tasks
constrained objective receives non-zero rewards timestep end
episode. general, methods based combination TD nonlinear scalarization
must regarded heuristic nature, applicable restricted classes problems.
second class avoids problems caused non-additive scalarized returns modifying either TD algorithm state representation. knowledge, two approaches
proposed Geibel (2006) address limitations TLO members
class. require reward accumulated objective current episode
stored. first algorithm, local decision-making based scalarized value
sum cumulative reward current state-action values. eliminates
problem non-additive returns, yields policy non-stationary respect
observed state, meaning algorithm may converge. second approach augments state representation cumulative reward. approach converges
correct policy learns slowly, due increase size state space.
92

fiA Survey Multi-Objective Sequential Decision-Making

third class uses TD methods learn policies based linear scalarizations.
policy selection mechanism based nonlinear scalarization used form
meta-policy base policies. Multiple Directions Reinforcement Learning
(MDRL) algorithm Mannor Shimkin (2001, 2004) uses approach
context on-line learning non-episodic tasks. user specifies target region within
long-term average reward lie. initial active policy chosen arbitrarily
followed average reward moves outside target region agent
specified reference state. point, direction current average reward
vector closest point target set calculated, policy whose direction best
matches target direction selected active policy. way, average reward
steered towards users specified target region. underlying base policies
utilize linear scalarization, nature policy-selection mechanism means
overall non-stationary policy formed base policies optimal nonlinear
scalarization specified users defined target set. Vamplew et al. (2009) suggest
similar approach episodic tasks, TD used first learn policies optimal
linear scalarization range different w, stochastic mixture policy
constructed optimal regards nonlinear scalarization.
fourth class uses policy-search algorithms directly learn policy without learning value function. single-policy MORL, research policy-search approaches
focused policy-gradient methods (Sutton, McAllester, Singh, & Mansour, 2000; Kohl &
Stone, 2004; Kober & Peters, 2011). methods, policy iteratively adjusted
direction gradient value respect parameters (usually probability
distributions actions per state) policy. Shelton (2001) proposes algorithm
first learns optimal policy individual objective. used base policies form initial mixture policy stochastically selects base policy start
episode. hill-climbing method based weighted convex combination
normalized objective gradients iteratively improves mixture policy. approach
directly fit taxonomy returns never scalarized. Instead,
weights used find step direction relative current policy parameters.
practical perspective, behavior akin single-policy RL using nonlinear
scalarization function, converges single Pareto-optimal policy need lie
convex hull. Uchibe Doya (2009) also propose policy-gradient method
MORL called Constrained Policy Gradient RL (CPGRL) uses gradient projection technique find policies whose average reward satisfies constraints one
objectives. Like Sheltons approach, CPGRL learns stochastic policies works
nonlinear scalarization functions.
6.2 Multiple-Policy Learning Linear Scalarization Functions
unknown weights decision support scenarios, f linear, MORL algorithms
aim learn CCS possible policies. simple inefficient approach used
Castelletti et al. (2002) run TD multiple times different values w.
simplest case, runs conducted sequentially gradually build approximate
CCS. Natarajan Tadepalli (2005) showed approach made efficient
reusing policies learned earlier runs similar w. show
93

fiRoijers, Vamplew, Whiteson & Dazeley

improves greatly sample costs learning policy w similar already
visited previous runs. However, many samples typically still required good
approximate CCS obtained.
sophisticated approach approximating convex coverage set learn multiple policies parallel. Several algorithms proposed achieve within
TD learning framework. approach Hiraoka, Yoshida, Mishima (2009) similar
CHVI planning algorithm Barrett Narayanan (2008) (see Section 5.2)
learns parallel optimal value function w, using convex hull representation.
approach prone infinite growth number vertices convex hull polygons,
threshold margin applied hull representations iteration, eliminating points contribute little hulls hypervolume. Hiraoka et al. (2009) present
algorithm adapt margins learning improve efficiency, note many
parameters must tuned effective performance. Mukai, Kuroe, Iima (2012) present
similar extension CHVI learning context. address problematic growth
number values stored pruning vectors Q-value update: vector
selected random set vectors stored given state-action pair
others lying within threshold distance deleted.
approaches Hiraoka et al. (2009) Mukai et al. (2012) designed
on-line learning. contrast, Multi-Objective Fitted Q-Iteration (MOFQI) (Castelletti,
Pianosi, & Restelli, 2011, 2012) off-line approach learning multiple policies. MOFQI
multi-objective extension Fitted Q-Iteration (FQI) algorithm (Ernst et al., 2005)
uses combination historical data single-step transition dynamics
environment, initial function approximator, Q-learning update rule construct
dataset maps state-action pairs expected return. dataset used
train improved function approximator process repeats values
function approximator converge. MOFQI provides computationally efficient extension
FQI multiple objectives including w input function approximator
constructing expanded training data set containing training instances randomly
generated ws. Since learned function generalizes across weight space addition
state-action space, used construct policy w.
discussed Section 5.3, Lizotte et al. (2010) Lizotte et al. (2012) describe valueiteration algorithm find convex hull policies finite horizon tasks. note
method applied learning context estimating model state transition
probabilities immediate rewards basis experience environment.
approach demonstrated task analyzing randomized drug trial data producing
estimates historical data gathered clinical trials.
6.3 Multiple-Policy Learning Monotonically Increasing Scalarization
Functions
f nonlinear, MORL algorithms unknown weights decision support scenarios aim learn PCS. linear scalarization case, simplest approach
run single-objective algorithms multiple times varying w. Shelton (2001) demonstrates approach policy-gradient algorithm, Vamplew et al. (2011)
TLO method Gabor et al. (1998). approach however, requires
94

fiA Survey Multi-Objective Sequential Decision-Making

f explicitly known learning algorithm, may undesirable decision
support scenario.
knowledge, currently methods learning multiple policies
nonlinear f using value-function approach. might seem possible adapt convex
hull methods CHVI using Pareto-dominance operators place convex-hull
calculations, straightforward. scalarized values policies
certain state non-additive, cannot restrict stationary policies want
find deterministic Pareto-optimal policies (as mentioned Section 4.3.2). However,
Bellman equation CHVI work, additivity, resulting sufficiency
deterministic policies, required. discuss options developing multiple-policy learning
methods nonlinear f Sections 8.1 8.2.
Given extensive research multi-objective evolutionary algorithms (MOEAs)
(Coello Coello, Lamont, & Van Veldhuizen, 2002; Tan, Khor, Lee, & Sathikannan, 2003;
Drugan & Thierens, 2012) evolutionary methods RL (Whiteson, 2012), surprisingly little work evolutionary approaches MORL. methods populationbased, well suited approximating Pareto fronts, would thus seem natural
fit f nonlinear. knowledge, Handa (2009b) first apply MOEAs
MORL, extending Estimation Distribution (EDA) evolutionary algorithms handle multiple objectives. EDA-RL (Handa, 2009a) uses Conditional Random Fields (CRF)
represent probabilistic policies. initial set policies used generate set
episodes. best episodes set selected CRFs likely produce trajectories generated. policies formed CRFs constitute
next generation. Handa (2009b) extends EDA-RL MOMDPs using Paretodominance based fitness metric select best episodes.
Soh Demiris (2011) also apply MOEAs MORL. Policies represented
Stochastic Finite State Controllers (SFSC) optimized using two different MOEAs:
NSGA2, standard evolutionary algorithm, MCMA, EDA. use SFSCs gives
rise large search space, necessitating addition local search operator. local
search generates random w, uses scalarize rewards, performs gradient-based
search SFSC. Empirical comparisons multi-objective variants three POMDP
benchmarks demonstrate evolutionary methods generally superior purely
local-search approach, local search combined evolution usually outperforms
purely evolutionary methods. one papers directly consider partially
observable MOMDPs.

7. MOMDP Applications
Multi-objective methods planning learning employed wide range
applications, simulation real-world settings. section, survey
applications. sake brevity, list comprehensive instead aims
provide illustrative range examples. First, discuss use multi-objective
methods specific applications. Second, discuss research identified broader
classes problems multi-objective methods play useful role.
95

fiRoijers, Vamplew, Whiteson & Dazeley

7.1 Specific Applications
important factor driving interest multi-objective decision-making increasing
social political emphasis environmental concerns. more, decisions must
made trade economic, social, environmental objectives. reflected
fact substantial proportion applications multi-objective methods
environmental component.
Perhaps extensively researched application water reservoir control problem considered Castelletti et al. (2002), Castelletti, Pianosi, Soncini-Sessa (2008),
Castelletti et al. (2011, 2012) Castelletti, Pianosi, Restelli (2013). general
task find control policy releasing water dam balancing multiple uses
reservoir, including hydroelectric production flood mitigation. Management
hydroelectric power production also examined Shabani (2009). Another environmental application forest management balance economic benefits
timber harvesting environmental aesthetic objectives, demonstrated
simulation Gong (1992) Bone Dragicevic (2009).
Several researchers also considered environmentally-motivated applications concerning management energy consumption. SAVES system developed Kwak
et al. (2012) controls various aspects commercial building (lighting, heating, airconditioning, computer systems) provide suitable trade-off energy consumption comfort buildings occupants. Simulation results indicate
SAVES reduce energy consumption approximately 30% compared manual control
system, maintaining slightly improving occupant comfort. Tesauro et al.
(2007) Liu et al. (2010) consider problem controlling computing server,
objectives minimizing response time user requests power consumption.
Guo et al. (2009) apply MORL develop broker agent electricity market.
broker sets caps group agents sit hierarchy manage energy
consumption device level, must balance energy cost system stability.
Shelton (2001) also examines application MORL developing broker agents.
However, case agents task financial rather environmental, acting
market maker sets buy sell prices resources market. aim balance
objectives maximizing profit minimizing spread (the difference buy
sell prices) lead larger volume trades11 .
Computing communications applications also widely considered. Perez
et al. (2009) apply MORL allocation resources jobs cloud computing scenario, objectives maximizing system responsiveness, utilization resources,
fairness amongst different classes user. Comsa et al. (2012) consider maximize
system throughput ensure user equity context Long Term Evolution mobile
communications packet scheduling protocol. Tong Brown (2002) use constraint-based
scalarization address tasks call access control routing broadband multimedia network. system aims maximize profit (a function throughput)
satisfying constraints quality service metrics (capacity constraints fairness constraints), uses methods similar Gabor et al. (1998). Zheng, Li, Qiu, Gong
11. Sheltons model market directly model trading volume, spread used proxy
volume.

96

fiA Survey Multi-Objective Sequential Decision-Making

(2012) also use constrained MORL methods make routing decisions cognitive radio
network, aiming minimize average transmission delay maintaining acceptably
low packet loss rate.
Industrial mechanical control, important application single-objective MDP
methods, also explored MOMDP researchers. Aoki, Kimura, Kobayashi
(2004) apply distributed RL control sewage flow system, exploiting systems hierarchical structure find solution minimizes violation stock levels node
flow system, smoothing variation flow source. Aissani et al. (2008)
apply MORL maintenance scheduling within manufacturing plant minimize time
taken complete maintenance tasks machine downtime. Aissani, Beldjilali,
Trentesaux (2009) build work applying simulation real petroleum refinery demonstrating ability adapt unscheduled corrective maintenance required
due equipment failures. control wet clutch heavy-duty transmission systems
examined Van Vaerenbergh et al. (2012). twin objectives minimizing
engagement time, also making transition smooth.
Robotics also popular application MOMDPs, though work far
simulation rather real robots. Maravall de Lope (2002) consider control
two-limbed brachiating robot, objectives moving desired direction
avoiding collisions12 . Nojima, Kojima, Kubota (2003) also attempt balance
objectives progress target collision avoidance. agent makes use
predefined behavioral modules target tracing, collision avoidance, wall following,
MORL used dynamically adjust weighting modules. Meisner (2009)
identifies social robots promising application MOMDP methods: behavior
inherently multi-objective must carry task without causing anxiety
discomfort humans.
MORL also applied control traffic infrastructure. Yang Wen
(2010) apply control freeway on-ramps vehicle management systems, aiming
maximize throughput equity freeway system. Multiple agents
shared policies used, action selection occurring via negotiation agents.
Similarly, Dusparic Cahill (2009) apply MORL control traffic lights intersections
urban environment minimize waiting time two different classes vehicles. Yin,
Duan, Li, Zhang (2010) Houli, Zhiheng, Yi (2010) also apply MORL traffic
light control. novelty approach lies considering different objectives based
current state road system; minimizing vehicle stops prioritized traffic
free-flowing; minimizing waiting time emphasized system medium load;
minimizing queue length intersections targeted system congested.
Lizotte et al. (2010, 2012) consider medical application: prescribing appropriate
drug regime patient achieve acceptable trade-off drugs effectiveness severity side effects. system learns multiple policies based
12. many robotic applications may ideal avoid collisions completely, environments
may possible (e.g., presence moving obstacles whose velocity faster
robot, difficult predict, may case humans human-controlled vehicles)
reducing likelihood impact collisions may reasonable attempting
find collision-free policy. See example Holenstein Badreddin (1991) Pervez Ryu
(2008).

97

fiRoijers, Vamplew, Whiteson & Dazeley

static data produced randomized controlled drug trials. selection best
treatment specific patient made doctor based patients individual circumstances. application excellent example problem stochastic
approaches like mixture policies inappropriate. policy maximizes symptom relief also side effects one patient minimizes side effects also
symptom relief next patient may appear give excellent results averaged
across episodes. However, experience individual patient likely regarded
undesirable.
7.2 Applications within Broader Planning Learning Tasks
addition specific applications discussed above, several authors identified
general classes tasks multi-objective sequential decision-making applied.
7.2.1 Probabilistic Risk-Aware Planning
Cheng, Subrahmanian, Westerberg (2005) argue decision-making uncertainty inherently multi-objective nature. Even single reward
considered (such profit), environmental uncertainty means expected value
alone insufficient support good decision-making; decision-maker must also consider
variance return. Similarly, Bryce (2008) states probabilistic planning
inherently multi-objective due need optimize cost probability
success plan. criticizes approaches either aggregate factors bound one
optimize other, arguing favor explicitly multi-objective methods.
aptly named Probabilistic Planning Multi-objective! paper Bryce, Cushing,
Kambhampati (2007) demonstrates might achieved, describing method based
multi-objective dynamic programming belief states, multi-objective extension
Looping AO* search algorithm find set Pareto-optimal plans. Recent work
Kolobov, Mausam, Weld (2012) Teichteil-Konigsbuch (2012b) examine extension stochastic shortest path (SSP) methods problems dead-end states exist.
SSP methods assume least one policy exists guaranteed reach goal;
presence dead-ends policy exists, authors propose algorithms
aim maximize probability reaching goal minimize cost
paths found goal.
Bryce (2008) notes probabilistic plan fails environment enters non-goal
absorbing state. Hence, multi-objective probabilistic planning strong parallels
research risk-aware RL carried Geibel (2001) Geibel Wysotzki (2005),
add second reward signal indicating transition environment error
state. Defourny, Ernst, Wehenkel (2008) also provide useful insights incorporation risk-awareness MDP methods. review range criteria proposed
constraining risk, note many nonlinear produce non-additive
scalarized returns incompatible local decision-making methods based
Bellman equation. recommend custom risk-control requirements
mostly enforced heuristically, altering policy optimization procedures checking
compliance policies initial requirements. Multi-policy MOMDP methods treating risk additional objective would satisfy requirement: iden98

fiA Survey Multi-Objective Sequential Decision-Making

tified coverage set, risk-aware metric used select best policy.
However, measures risk may expressed directly discounted cumulative
rewards. example, agent may wish minimize variance expected return particular reward signal rather discounted cumulative value. Methods
based multi-objective probabilistic model checking (Courcoubetis & Yannakakis, 1998;
Forejt, Kwiatkowska, Norman, Parker, & Qu, 2011; Forejt, Kwiatkowska, & Parker, 2012;
Teichteil-Konigsbuch, 2012a), evaluate whether system modelled MDP satisfies multiple, possibly conflicting, properties, may suitable tasks.
7.2.2 Multi-Agent Systems
use MDPs within multi-agent systems widely explored (Bosoniu, Babuska,
& Schutter, 2008), several authors proposed approaches strongly related
MOMDPs. multi-agent system, agent objective, effective
overall performance must also consider actions affect agents.
agents completely self-interested, problem framed MOMDP
treating effects agents additional objectives. example, Mouaddib (2006)
uses multi-objective dynamic programming facilitate cooperation multiple agents
whose underlying goals may conflicting. state-action pair, agent stores
three values: local utility, gain agents receive, penalty inflicts
agents. policy agent established converting vector values
regret ratios applying leximin ordering ratios.
Dusparic Cahill (2010) compare application MORL multi-agent tasks
multi-agent methods evolutionary ant-colony algorithms. Dusparic
Cahill (2009) extend W-Learning algorithm Humphrys (1996). agent learns
local policies (one objectives) remote policies (one
local policy neighboring agents). timestep, local policies
active remote policies agent nominate actions, winning action selected
combining action values across nominating policies. weighting term applied
values remote policies determine level cooperation agent offers neighbor.
Experimental results urban traffic control simulator show substantial improvement
level cooperation non-zero. work similar Schneider, Wong,
Moore, Riedmiller (1999), addresses use multiple agents distributed
network power distribution grid, aim maximize global reward
formed combination agents local reward. demonstrate
agent focuses local reward, policies learned may maximize global
reward, performance improved agent perform linearly scalarized
learning using local reward rewards neighboring agents.
7.2.3 Multi-Objective Optimization using Reinforcement Learning
Reinforcement learning primarily applied sequential decision-making tasks dynamic
environment. However also employed control search mechanisms static optimization tasks scheduling (Carchrae & Beck, 2005). Multi-objective optimization
static tasks design well-established field and, majority work
99

fiRoijers, Vamplew, Whiteson & Dazeley

employed mathematical evolutionary approaches (Coello Coello et al., 2002),
authors explored application reinforcement learning contexts.
Mariano Morales (1999, 2000b, 2000a) investigate use RL methods (Ant-Q
Q-learning) search mechanism optimization multi-objective design tasks.
values decision variables considered current state, actions defined
alter values variables. Multiple agents explore state space parallel.
Agents divided families, family focuses single objective.
end episode, final states found agent evaluated. Undominated solutions kept archive agents discovered solutions rewarded,
increasing likelihood similar policies followed future. method
shown work small number test problems evolutionary multi-objective
optimization literature. Liao, Wu, Jiang (2010) apply RL search static control
settings power generation system objectives reducing fuel usage ensuring voltage stability. propose RL algorithm formulated specifically tasks
high-dimensional state spaces, compare performance evolutionary
multi-objective algorithm, finding RL method discovers fronts
accurate better distributed, also improving speed search.
Note effectively apply RL multi-objective optimization, assumptions usually made nature environment. example, Liao et al. (2010) require
action increases decreases value precisely one state variable. result,
methods likely limited applicability general MORL problems
described earlier.

8. Future Work
section, enumerate possibilities future research multi-objective
planning learning.
8.1 Model-Based Methods
mentioned Section 6, little work model-based approaches
MORL. Given breadth planning methods MOMDPs, could employed
model-based MORL methods subroutines, surprising. knowledge,
work area Lizotte et al. (2010, 2012), model MOMDPs
transition probabilities reward function derived historical data,
spline-based multi-objective value iteration approach applied model. general,
learning models seems negligibly harder single-objective setting, since
estimates reward function learned separately. problem learning
transition function, generally considered hard part model learning, identical
single-objective setting. Especially multiple-policy scenarios, model-based approaches
MORL could greatly reduce sample costs: model learned, entire
CCS PCS computed off-line, without requiring additional samples.
100

fiA Survey Multi-Objective Sequential Decision-Making

8.2 Learning Multiple Policies Monotonically Increasing Scalarization
Functions using Value Functions
mentioned Section 6.3, aware methods use value function
approach learn multiple policies PCS. stochastic policies permitted,
problem easier learn CCS(m
DS ), use either mixture policies
(Vamplew et al., 2009) stationary randomizations (Wakuta, 1999) policies
CCS (see Section 4.3.3). However, deterministic policies permitted,
problem difficult. One option could use finite-horizon approximation
infinite horizon problem. planning backwards planning horizon, expected
reward timesteps go approximates infinite-horizon value better better
. mentioned Section 5.2, similar approaches used POMDP
setting. Another way find good approximations non-stationary policies could
learn stationary policies (perhaps extending CON-MDP (Wiering & De Jong, 2007)
learning setting), prefix timesteps non-stationary policy.
8.3 Many-Objective Sequential Decision-Making
majority research reviewed article, theoretical applied, deals
MOMDPs objectives. mirrors state early evolutionary multiobjective research, focused almost exclusively problems two three
objectives. However, last decade growing interest evolutionary
methods so-called many-objective problems, least four sometimes
fifty objectives (Ishibuchi, Tsukamoto, & Nojima, 2008). research
shown many algorithms perform well objectives scale poorly
number objectives, necessitating special algorithms many-objective setting.
many-objective MDPs received little consideration far, numerous real-world control problems naturally modeled way. example,
Fleming et al. (2005) point many-objective control problems commonly arise
engineering, give example jet engine control system eight objectives.
many-objective problems considered evolutionary computation, seems likely
least methods explored far scale poorly number objectives. example, multi-policy MOMDP planning algorithm described Lizotte
et al. (2010) limited problems two objectives.
key challenge posed many-objective problems number undominated
solutions typically grows exponentially number objectives. particularly
problematic multiple-policy MOMDP methods. Fleming et al. (2005) note one
effective approaches used many-objective evolutionary computation
incorporate user preferences restrict search space small region interest.
particular, recommend interactive preference articulation user interactively steers system towards desirable solution optimization. Vamplew
et al. (2011) raise possibility incorporating approach MORL,
aware research actually done so.
101

fiRoijers, Vamplew, Whiteson & Dazeley

(3,0)

(0,3)

B

C


(1,1)


Figure 3: MOMDP two objectives four states.
8.4 Expectation Scalarized Return
Section 3, defined scalarized value Vw (s) result applying scalarization function f multi-objective value V (s) according w, i.e., Vw (s) = f (V (s), w).
Since V (s) expectation, means scalarization function applied
expectation computed, i.e.,
Vw (s) = f (V (s), w) = f (E[


X

k rk | , s0 = s], w).

k=0

formulation, refer scalarization expected return (SER)
standard literature. However, option. also possible define
Vw (s) expectation scalarized return (ESR):
Vw (s) = E[f (


X

k rk , w) | , s0 = s]

k=0

definition used critically affect policies preferred. example,
consider following MOMDP, illustrated Figure 3. four states (A, B, C,
D) two objectives. agent starts state two possible actions: a1 transits
state B C, probability 0.5, a2 transits state probability 1.
actions lead (0, 0) reward. states B, C one action,
leads deterministic reward (3, 0) B, (0, 3) C, (1, 1) D.
scalarization function multiplies two objectives together. Thus, SER,
Vw (s) = V1 (s)V2 (s),
ESR,
Vw (s) = E[


X
k=0

k rk1


X
k=0

102


k rk2 | , s0 = s],

fiA Survey Multi-Objective Sequential Decision-Making

rki reward i-th objective timestep k (w needed example
since f involves constants). 1 (A) = a1 2 (A) = a2 , multi-objective
values V1 (A) = (1.5/(1 ), 1.5/(1 )) V2 (A) = (/(1 ), /(1 )).
SER, leads scalarized values V 1 (A) = (1.5/(1 ))2 V 2 (A) =
(/(1 ))2 consequently 1 preferred. ESR, however, V 1 (A) = 0
V 2 (A) = (/(1 ))2 thus 2 preferred.
Intuitively, SER formulation appropriate policy used many times
return accumulates across episodes, e.g., user using policy
time. Then, scalarizing expected reward makes sense 1 preferable
expectation accumulate return objectives. However, policy
used times return accumulate across episodes, e.g.,
episode conducted different user, ESR formulation appropriate.
case, expected return scalarization interest 2 preferable
1 always yield zero scalarized return given episode.
knowledge, literature MOMDPs employs ESR formulation,
even though many real-world scenarios seems appropriate.
example, medical application Lizotte et al. (2010) mentioned Section 7,
patient gets one episode treat illness, thus clearly interested
maximizing ESR, SER. Thus, believe developing methods MOMDPs
ESR formulation critical direction future research.

9. Conclusions
article presented survey algorithms designed sequential decision-making problems multiple objectives.
order make explicit circumstances special methods needed
solve multi-objective problems, identified three distinct scenarios converting
problem single-objective one impossible, infeasible, undesirable. well
providing motivation need multi-objective methods, scenarios also represent
three main ways methods applied practice.
proposed taxonomy classifies multi-objective methods according applicable scenario, scalarization function (which projects multi-objective values scalar
ones), type policies considered. showed factors determine
nature optimal solution, single policy, coverage set (convex
Pareto). taxonomy based utility-based approach, sees scalarization
function part utility, thus part problem definition. contrasts
so-called axiomatic approach, usually assumes Pareto front appropriate
solution. showed utility-based approach used justify choice
solution set. Following line thought, observed (Observation 1) computing
Pareto front often necessary, many cases convex coverage set
deterministic stationary policies sufficient.
Using taxonomy, surveyed literature multi-objective methods planning
learning. interesting observation learning methods use modelfree rather model based approach, identifying latter understudied class
103

fiRoijers, Vamplew, Whiteson & Dazeley

methods. Another part taxonomy yet widely studied learning
case monotonically increasing scalarization functions.
discussed key applications MOMDP methods motivation importance
methods. Applications identified diverse range fields including environmental management, financial markets, information communications technology,
control industrial processes, robotic systems traffic infrastructure. addition connections identified multi-objective sequential decision-making broad
areas research probabilistic planning model-checking, multi-agent systems
general multi-objective optimization.
Finally, outlined several opportunities future work, include understudied
areas (model-based methods, learning monotonically increasing scalarization settings,
many-objective sequential decision-making), reformulation objective
MOMDPs Expectation Scalarized Return particularly important
optimize policy executed once.

Acknowledgments
would like thank Matthijs Spaan, Frans Oliehoek, Matthijs Snel, Marie D. Manner
Samy Sa, well anonymous reviewers, valuable feedback. work
supported Netherlands Organisation Scientific Research (NWO): DecisionTheoretic Control Network Capacity Allocation Problems (#612.001.109) project.

References
Aberdeen, D., Thiebaux, S., & Zhang, L. (2004). Decision-theoretic military operations
planning. Proc. ICAPS, Vol. 14, pp. 402411.
Aissani, N., Beldjilali, B., & Trentesaux, D. (2008). Efficient effective reactive scheduling manufacturing system using Sarsa-multi-objective agents. MOSIM08: 7th
Conference Internationale de Modelisation et Simulation, pp. 698707.
Aissani, N., Beldjilali, B., & Trentesaux, D. (2009). Dynamic scheduling maintenance
tasks pretroleum industry: reinforcement approach. Engineering Applications
Artificial Intelligence, 22, 10891103.
Altman, E. (1999). Constrained Markov Decision Processes. Chapman Hall/CRC,
London.
Aoki, K., Kimura, H., & Kobayashi, S. (2004). Distributed reinforcement learning using
bi-directional decision making multi-criteria control multi-stage flow systems.
8th Conference Intelligent Autonomous Systems, Vol. 2004.03, pp. 281290.
Barrett, L., & Narayanan, S. (2008). Learning optimal policies multiple criteria.
Proceedings 25th International Conference Machine Learning, pp. 4147,
New York, NY, USA. ACM.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-Independent
Decentralized Markov Decision Processes. Proc. 2nd Intl Joint Conf.
Autonomous Agents & Multi-Agent Systems.
104

fiA Survey Multi-Objective Sequential Decision-Making

Bellman, R. E. (1957a). Markov decision process. Journal Mathematical Mech., 6,
679684.
Bellman, R. (1957b). Dynamic Programming. Princeton University Press.
Bhattacharya, B., Lobbrecht, A. H., & Solomantine, D. P. (2003). Neural networks reinforcement learning control water systems. Journal Water Resources Planning
Management, 129 (6), 458465.
Bone, C., & Dragicevic, S. (2009). GIS intelligent agents multiobjective natural
resource allocation: reinforcement learning approach. Transactions GIS, 13 (3),
253272.
Bosoniu, L., Babuska, R., & Schutter, B. D. (2008). comprehensive survey multiagent
reinforcement learning. IEEE Transactions Systems, Man, Cybernetics - Part
C: Applications Reviews, 38 (2), 156172.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 194.
Brazdil, T., Brozek, V., Chatterjee, K., Forejt, V., & Kucera, A. (2011). Two views
multiple mean-payoff objectives Markov decision processes. CoRR, abs/1104.3489.
Bryce, D. (2008). value(s) probabilistic plans. Workshop Reality Check
Planning Scheduling Uncertainty, ICAPS-08.
Bryce, D., Cushing, W., & Kambhampati, S. (2007). Probabilistic planning multiobjective!. Technical report 08-006, Arizona State University.
Carchrae, T., & Beck, J. C. (2005). Applying machine learning low-knowledge control
optimization algorithms. Computational Intelligence, 21 (4), 372387.
Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple, fast,
exact method partially observable markov decision processes. Proceedings
Thirteenth conference Uncertainty artificial intelligence, pp. 5461.
Castelletti, A., Pianosi, F., & Restelli, M. (2013). multiobjective reinforcement learning
approach water resources systems operation: Pareto frontier approximation
single run. Water Resources Research.
Castelletti, A., Corani, G., Rizzolli, A., Soncini-Sessa, R., & Weber, E. (2002). Reinforcement learning operational management water system. IFAC Workshop
Modeling Control Environmental Issues, pp. 325330.
Castelletti, A., Galelli, S., Restelli, M., & Soncini-Sessa, R. (2010). Tree-based reinforcement learning optimal water reservoir operation. Water Resources Research,
46 (W09507).
Castelletti, A., Pianosi, F., & Restelli, M. (2011). Multi-objective Fitted Q-Iteration: Pareto
frontier approximation one single run. International Conference Networking,
Sensing Control, pp. 260265.
Castelletti, A., Pianosi, F., & Restelli, M. (2012). Tree-based Fitted Q-iteration multiobjective Markov decision processes. IEEE World Congress Computational
Intelligence.
105

fiRoijers, Vamplew, Whiteson & Dazeley

Castelletti, A., Pianosi, F., & Soncini-Sessa, R. (2008). Water reservoir control economic, social environmental constraints. Automatica, 44, 15951607.
Chatterjee, K. (2007). Markov decision processes multiple long-run average objectives.
FSTTCS, Vol. LNCS 4855, pp. 473484.
Chatterjee, K., Majumdar, R., & Henzinger, T. A. (2006). Markov decision processes
multiple objectives. Proceedings 23rd Annual conference Theoretical
Aspects Computer Science, STACS06, pp. 325336, Berlin, Heidelberg. SpringerVerlag.
Cheng, L., Subrahmanian, E., & Westerberg, A. (2005). Multiobjective decision processes
uncertainty: Applications, formulations solution strategies. Industrial
Engineering Chemistry Research, 44 (8), 24052415.
Clemen, R. T. (1997). Making Hard Decisions: Introduction Decision Analysis (2
edition). South-Western College Pub.
Coello Coello, C. A., Lamont, G. B., & Van Veldhuizen, D. A. (2002). Evolutionary Algorithms Solving Multi-Objective Problems. Kluwer Academic Publishers.
Comsa, I., Aydin, M., Zhang, S., Kuonen, P., & Wagen, J.-F. (2012). Multi objective resource scheduling LTE networks using reinforcement learning. International Journal
Distributed Systems Technologies, 3 (2), 3957.
Courcoubetis, C., & Yannakakis, M. (1998). Markov decision processes regular events.
IEEE Transactions Automatic Control, 43 (10), 13991418.
Crites, R. H., & Barto, A. G. (1996). Improving elevator performance using reinforcement
learning. Touretzky, D. S., Mozer, M. C., & Hasselmo, M. E. (Eds.), Advances
Neural Information Processing Systems 8, pp. 10171023. MIT Press.
Daellenbach, H. G., & Kluyver, C. A. D. (1980). Note multiple objective dynamic
programming. Journal Operational Research Society, 31, 591594.
Defourny, B., Ernst, D., & Wehenkel, L. (2008). Risk-aware decision making dynamic
programming. NIPS 2008 Workshop Model Uncertainty Risk RL.
Diehl, M., & Haimes, Y. Y. (2004). Influence diagrams multiple objectives tradeoff analysis. Systems, Man Cybernetics, Part A: Systems Humans, IEEE
Transactions on, 34 (3), 293304.
Drugan, M. M., & Thierens, D. (2012). Stochastic pareto local search: Pareto neighbourhood
exploration perturbation strategies. Journal Heuristics, 18 (5), 727766.
Dusparic, I., & Cahill, V. (2009). Distributed W-learning: Multi-policy optimization selforganizing systems. Third IEEE International Conference Self-Adaptive
Self-Organizing Systems, pp. 2029.
Dusparic, I., & Cahill, V. (2010). Multi-policy optimization self-organizing systems.
SOAR 2009, LNCS 6090, pp. 101126.
Dyer, J. S., Fishburn, P. C., Steuer, R. E., Wallenius, J., & Zionts, S. (1992). Multiple criteria decision making, multiattribute utility theory: next ten years. Management
Science, 38 (5), 645654.
106

fiA Survey Multi-Objective Sequential Decision-Making

Ernst, D., Geurts, P., & Wehenkel, L. (2005). Tree-based batch mode reinforcement learning.
Journal Machine Learning Research, 6, 503556.
Ernst, D., Glavic, M., & Wehenkel, L. (2004). Power systems stability control: Reinforcement learning framework. IEEE Transactions Power Systems, 19 (1), 427435.
Feinberg, E. A., & Shwartz, A. (1995). Constrained Markov decision models weighted
discounted rewards. Mathematics Operations Research, 20 (2), 302320.
Ferreira, L., Bianchi, R., & Ribeiro, C. (2012). Multi-agent multi-objective reinforcement
learning using heuristically accelerated reinforcement learning. 2012 Brazilian
Robotics Symposium Latin American Robotics Symposium, pp. 1420.
Fleming, P., Purshouse, R., & Lygoe, R. (2005). Many-objective optimization: engineering design perspective. Evolutionary Multi-Criterion Optimization: Lecture Notes
Computer Science, Vol. 3410, pp. 1432.
Forejt, V., Kwiatkowska, M., Norman, G., Parker, D., & Qu, H. (2011). Quantitative
multi-objective verification probabilistic systems. Tools Algorithms
Construction Analysis Systems, pp. 112127. Springer Berlin Heidelberg.
Forejt, V., Kwiatkowska, M., & Parker, D. (2012). Pareto curves probabilistic model
checking. Automated Technology Verification Analysis, pp. 317332.
Springer Berlin Heidelberg.
Furnkranz, J., Hullermeier, E., Cheng, W., & Park, S.-H. (2012). Preference-based reinforcement learning: formal framework policy iteration algorithm. Machine
Learning, 89 (1-2), 123156.
Gabor, Z., Kalmar, Z., & Szepesvari, C. (1998). Multi-criteria reinforcement learning.
Fifteenth International Conference Machine Learning, pp. 197205.
Geibel, P., & Wysotzki, F. (2005). Risk-sensitive reinforcement learning applied control
constraints. Journal Artificial Intelligence Research, 24, 81108.
Geibel, P. (2001). Reinforcement learning bounded risk. Proceeding 18th
International Conference Machine Learning, pp. 162169.
Geibel, P. (2006). Reinforcement learning MDPs constraints. European Conference Machine Learning, Vol. 4212, pp. 646653.
Gelly, S., & Silver, D. (2011). Monte-carlo tree search rapid action value estimation
computer go. Artificial Intelligence, 175 (11), 18561875.
Gong, P. (1992). Multiobjective dynamic programming forest resource management.
Forest Ecology Management, 48, 4354.
Guo, Y., Zeman, A., & Li, R. (2009). reinforcement learning approach setting multiobjective goals energy demand management. International Journal Agent Technologies Systems, 1 (2), 5570.
Handa, H. (2009a). EDA-RL: Estimation distribution algorithms reinforcement learning problems. ACM/SIGEVO Genetic Evolutionary Computation Conference,
pp. 405412.
107

fiRoijers, Vamplew, Whiteson & Dazeley

Handa, H. (2009b). Solving multi-objective reinforcement learning problems EDA-RL acquisition various strategies. Proceedings Ninth Internatonal Conference
Intelligent Sysems Design Applications, pp. 426431.
Hiraoka, K., Yoshida, M., & Mishima, T. (2009). Parallel reinforcement learning weighted
multi-criteria model adaptive margin. Cognitive Neurodynamics, 3, 1724.
Holenstein, A. A., & Badreddin, E. (1991). Collision avoidance behavior-based mobile
robot design. Robotics Automation, 1991. Proceedings., 1991 IEEE International Conference on, pp. 898903. IEEE.
Houli, D., Zhiheng, L., & Yi, Z. (2010). Multiobjective reinforcement learning traffic
signal control using vehicular ad hoc network. EURASIP Journal Advances
Signal Processing.
Howard, R. A. (1960). Dynamic programming Markov decision processes. MIT Press.
Humphrys, M. (1996). Action selection methods using reinforcement learning. Proceedings Fourth International Conference Simulation Adaptive Behavior, pp.
135144.
Ishibuchi, H., Tsukamoto, N., & Nojima, Y. (2008). Evolutionary many-objective optimisation: short review. IEEE Congress Evolutionary Computation, pp. 24192426.
Issabekov, R., & Vamplew, P. (2012). empirical comparison two common multiobjective reinforcement learning algorithms. AI2012: 25th Australasian Joint
Conference Artificial Intelligence, pp. 626636.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting
partially observable stochastic domains. Artificial Intelligence, 101, 99134.
Karlsson, J. (1997). Learning Solve Multiple Goals. Ph.D. thesis, University Rochester.
Kober, J., & Peters, J. (2011). Policy search motor primitives robotics. Machine
Learning, 12, 171203.
Kober, J., & Peters, J. (2012). Reinforcement learning robotics: survey. Wiering,
M., & Otterlo, M. (Eds.), Reinforcement Learning, Vol. 12 Adaptation, Learning,
Optimization, pp. 579610. Springer Berlin Heidelberg.
Kocsis, L., & Szepesvari, C. (2006). Bandit based monte-carlo planning. 17th European
Conference Machine Learning, pp. 282293. Springer.
Kohl, N., & Stone, P. (2004). Policy gradient reinforcement learning fast quadrupedal
locomotion. Proceedings IEEE International Conference Robotics
Automation, pp. 26192624.
Kolobov, A., Mausam, & Weld, D. S. (2012). theory goal-oriented mdps dead
ends. Proceedings Twenty-Eighth Conference Uncertainty Artificial
Intelligence.
Kwak, J., Varakantham, P., Maheswarn, R., Tambe, M., Jazizadeh, F., Kavulya, G., Klein,
L., Becerik-Gerber, B., Hayes, T., & Wood, W. (2012). SAVES: sustainable multiagent application conserve building energy considering occupants. 11th International Conference Autonomous Agents Multiagent Systems, pp. 2128.
108

fiA Survey Multi-Objective Sequential Decision-Making

Liao, H., Wu, Q., & Jiang, L. (2010). Multi-objective optimization reinforcement learning
power system dispatch voltage stability. Innovative Smart Grid Technologies
Conference Europe.
Lin, C.-T., & Chung, I.-F. (1999). reinforcement neuro-fuzzy combiner multiobjective
control. IEEE Transactions Systems, Man Cyberbetics - Part B, 29 (6), 726
744.
Liu, C., Xu, X., & Hu, D. (2013). Multiobjective reinforcement learning: comprehensive
overview. Systems, Man, Cybernetics, Part C: Applications Reviews, IEEE
Transactions on, PP (99), 113.
Liu, W., Tan, Y., & Qiu, Q. (2010). Enhanced q-learning algorithm dynamic power
management performance constraints. DATE10, pp. 602605.
Lizotte, D. J., Bowling, M., & Murphy, S. A. (2010). Efficient reinforcement learning
multiple reward functions randomized clinical trial analysis. 27th International
Conference Machine Learning, pp. 695702.
Lizotte, D. J., Bowling, M., & Murphy, S. A. (2012). Linear fitted-q iteration multiple
reward functions. Journal Machine Learning Research, 13, 32533295.
Madani, O., Hanks, S., & Condon, A. (1999). undecidability probabilistic planning
infinite-horizon partially observable Markov decision problems. Proceedings
National Conference Artificial Intelligence (AAAI), pp. 541548.
Mannor, S., & Shimkin, N. (2001). steering approach multi-criteria reinforcement
learning. Neural Information Processing Systems, pp. 15631570.
Mannor, S., & Shimkin, N. (2004). geometric approach multi-criterion reinforcement
learning. Journal Machine Learning Research, 5, 325360.
Maravall, D., & de Lope, J. (2002). reinforcement learning method dynamic obstacle
avoidance robotic mechanisms. Computational Intelligent Systems Applied
Research: Proceedings 5th International FLINS Conference, pp. 485494, Singapore. World Scientific.
Mariano, C., & Morales, E. (1999). MOAQ Ant-Q algorithm multiple objective
optimization problems. GECCO-99: Proceedings Genetic Evolutionary
Computation Conference, pp. 894901.
Mariano, C., & Morales, E. (2000a). new approach solution multiple objective
optimization problems based reinforcement learning. Advances Artificial
Intelligence, International Joint Conference, 7th Ibero-American Conference AI,
15th Brazilian Symposium. Springer.
Mariano, C., & Morales, E. (2000b). new distributed reinforcement learning algorithm
multiple objective optimisation problems. Lecture Notes AI Vol 1952: Proceedings Mexican International Conference Artficial Intelligence, pp. 212223.
Springer.
Meisner, E. M. (2009). Learning Controllers Human-Robot Interaction. Ph.D. thesis,
Rensselaer Polytechnic Institute.
109

fiRoijers, Vamplew, Whiteson & Dazeley

Mouaddib, A.-I. (2006). Collective multi-objective planning. Proceedings IEEE
Workshop Distributed Intelligent Systems: Collective Intelligence Applications (DIS06), pp. 4348, Washington, DC, USA. IEEE Computer Society.
Mukai, Y., Kuroe, Y., & Iima, H. (2012). Multi-objective reinforcement learning method
acquiring Pareto optimal policies simultaneously. IEEE International Conference Systems, Man Cybernetics, pp. 19171923.
Natarajan, S., & Tadepalli, P. (2005). Dynamic preferences multi-criteria reinforcement
learning. International Conference Machine Learning, pp. 601608.
Nojima, Y., Kojima, F., & Kubota, N. (2003). Local episode-based learning multiobjective behavior coordination mobile robot dynamic environments.
12th IEEE International Conference Fuzzy Systems, Vol. 1, pp. 307312.
Ogryczak, W., Perny, P., & Weng, P. (2011). minimizing ordered weighted regrets
multiobjective Markov decision processes. 2nd International Conference
Algorithmic Decision Theory, pp. 190204.
Ong, S. C., Png, S. W., Hsu, D., & Lee, W. S. (2010). Planning uncertainty robotic
tasks mixed observability. International Journal Robotics Research, 29 (8),
10531068.
Pareto, V. (1896). Manuel dEconomie Politique. Giard, Paris.
Peek, N. B. (1999). Explicit temporal models decisiontheoretic planning clinical
management. Artificial Intelligence Medicine, 15 (2), 135154.
Perez, J., Germain-Renaud, C., Kegl, B., & Loomis, C. (2009). Responsive elastic computing. International Conference Autonomic Computing, pp. 5564.
Perny, P., & Weng, P. (2010). finding compromise solutions multiobjective Markov
decision processes. ECAI Multidisciplinary Workshop Advances Preference
Handling, pp. 5560.
Perny, P., Weng, P., Goldsmith, J., & Hanna, J. P. (2013). Approximation lorenz-optimal
solutions multiobjective markov decision processes. Workshops TwentySeventh AAAI Conference Artificial Intelligence.
Pervez, A., & Ryu, J. (2008). Safe physical human robot interaction-past, present
future. Journal Mechanical Science Technology, 22 (3), 469483.
Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations large
POMDPs. Journal Artificial Intelligence Research, 27 (1), 335380.
Precup, D., Sutton, R. S., & Dasgupta, S. (2001). Off-policy temporal-difference learning
function approximation. Proceedings 18th International Conference
Machine Learning, pp. 417424.
Puterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons, Inc.
Roijers, D. M., Whiteson, S., & Oliehoek, F. A. (2013). Computing convex coverage sets
multi-objective coordination graphs. ADT 2013: Proceedings Third International Conference Algorithmic Decision Theory. appear.
110

fiA Survey Multi-Objective Sequential Decision-Making

Ross, S., Pineau, J., Paquet, S., & Chaib-draa, B. (2008). Online planning algorithms
POMDPs. Journal Artificial Intelligence Research, 32, 663704.
Russell, S., & Zimdars, A. L. (2003). Q-decomposition reinforcement learning agents.
Proceedings 20th International Conference Machine Learning, pp. 656663.
Schneider, J., Wong, W.-K., Moore, A., & Riedmiller, M. (1999). Distributed value functions. Proceedings 16th International Conference Machine Learning, pp.
371378, San Francisco, CA. Morgan Kaufmann.
Shabani, N. (2009). Incorporating flood control rule curves Columbia River hydroelectric system multireservoir reinforcement learning optimization model. Masters
thesis, University British Columbia.
Shelton, C. R. (2001). Importance sampling reinforcement learning multiple objectives. AI Technical Report 2001-003, MIT.
Shortreed, S., Laber, E., Lizotte, D., Stroup, T., Pineau, J., & Murphy, S. (2011). Informing
sequential clinical decision-making reinforcement learning: empirical study.
Machine Learning, 84, 109136.
Smallwood, R., & Sondik, E. (1973). optimal control partially observable Markov
processes finite horizon. Operations Research, 21 (5), 10711088.
Soh, H., & Demiris, Y. (2011). Evolving policies multi-reward partially observable
Markov decision processes (MR-POMDPs). GECCO11 Proceedings 13th
Annual Conference Genetic Evolutionary Computation, pp. 713720.
Sondik, E. (1971). optimal control partially observable processes finite horizon.
Ph.D. thesis, Stanford University, Stanford, California.
Sondik, E. (1978). optimal control partially observable Markov processes
infinite horizon: Discounted costs. Operations Research, 26 (2), 282304.
Spaan, M., & Vlassis, N. (2005). Perseus: Randomized point-based value iteration
POMDPs. Journal Artificial Intelligence Research, 24 (1), 195220.
Stewart, T. J. (1992). critical survey status multiple criteria decision making
theory practice. Omega, 20 (5), 569586.
Sutton, R. S. (1988). Learning predict methods temporal differences. Machine
Learning, 3 (1), 944.
Sutton, R. S., & Barto, A. G. (1998). Introduction Reinforcement Learning (1st edition).
MIT Press, Cambridge, MA, USA.
Sutton, R., McAllester, D., Singh, S., & Mansour, Y. (2000). Policy gradient methods
reinforcement learning function approximation. NIPS, pp. 10571063.
Szita, I. (2012). Reinforcement learning games. Wiering, M., & Otterlo, M. (Eds.),
Reinforcement Learning, Vol. 12 Adaptation, Learning, Optimization, pp. 539
577. Springer Berlin Heidelberg.
Tan, K. C., Khor, E. F., Lee, T. H., & Sathikannan, R. (2003). evolutionary algorithm
advanced goal priority specification multi-objective optimization. Journal
Artificial Intelligence Research, 18, 183215.
111

fiRoijers, Vamplew, Whiteson & Dazeley

Teichteil-Konigsbuch, F. (2012a). Path-constrained markov decision processes: bridging
gap. Proceedings Twentieth European Conference Artificial Intelligence.
Teichteil-Konigsbuch, F. (2012b). Stochastic safest shortest path problems. Proceedings Twenty-Sixth AAAI Conference Artificial Intelligence.
Tesauro, G., Das, R., Chan, H., Kephart, J. O., Lefurgy, C., Levine, D. W., & Rawson, F.
(2007). Managing power consumption performance computing systems using
reinforcement learning. Neural Information Processing Systems.
Tong, H., & Brown, T. X. (2002). Reinforcement learning call admission control
routing quality service constraints multimedia networks. Machine Learning, 49, 111139.
Uchibe, E., & Doya, K. (2009). Constrained Reinforcement Learning Intrinsic
Extrinsic Rewards, pp. 155166. Theory Novel Applications Machine Learning.
I-Tech, Vienna, Austria.
Vamplew, P., Dazeley, R., Barker, E., & Kelarev, A. (2009). Constructing stochastic mixture
policies episodic multiobjective reinforcement learning tasks. AI09: 22nd
Australasian Conference Artificial Intelligence, pp. 340349.
Vamplew, P., Dazeley, R., Berry, A., Dekker, E., & Issabekov, R. (2011). Empirical evaluation methods multiobjective reinforcement learning algorithms. Machine Learning,
84 (1-2), 5180.
Vamplew, P., Yearwood, J., Dazeley, R., & Berry, A. (2008). limitations scalarisation multi-objective reinforcement learning Pareto fronts. AI08: 21st
Australasian Joint Conference Artificial Intelligence, pp. 372378. Springer.
Van Otterlo, M., & Wiering, M. (2012). Reinforcement learning markov decision processes. Reinforcement Learning: State Art, chap. 1, pp. 342. Springer.
Van Vaerenbergh, K., Rodriguez, A., Gagliolo, M., Vrancx, P., Nowe, A., Stoev, J.,
Goossens, S., Pinte, G., & Symens, W. (2012). Improving wet clutch engagement
reinforcement learning. International Joint Conference Neural Networks,
IJCNN 2012.
Vira, C., & Haimes, Y. Y. (1983). Multiobjective decision making: theory methodology.
No. 8. North-Holland.
Viswanathan, B., Aggarwal, V. V., & Nair, K. P. K. (1977). Multiple criteria Markov
decision processes. TIMS Studies Management Science, 6, 263272.
Wakuta, K., & Togawa, K. (1998). Solution procedures Markov decision processes. Optimization: Journal Mathematical Programming Operations Research, 43 (1),
2946.
Wakuta, K. (1999). note structure value spaces vector-valued Markov decision
processes.. Mathematical Methods Operations Research, 49 (1), 7785.
Wang, W., & Sebag, M. (2013). Hypervolume indicator dominance reward based multiobjective monte-carlo tree search. Machine Learning, 127.
Watkins, C. J. C. H. (1989). Learning Delayed Rewards. Ph.D. thesis, Cambridge
University.
112

fiA Survey Multi-Objective Sequential Decision-Making

White, C. C., & Kim, K. M. (1980). Solution procedures solving vector criterion Markov
decision processes. Large Scale Systems, 1, 129140.
White, D. (1982). Multi-objective infinite-horizon discounted Markov decision processes.
Journal Mathematical Analysis Applications, 89 (2), 639 647.
Whiteson, S. (2012). Evolutionary computation reinforcement learning. Wiering,
M. A., & van Otterlo, M. (Eds.), Reinforcement Learning: State Art, chap. 10,
pp. 325352. Springer, Berlin.
Wiering, M., & De Jong, E. (2007). Computing optimal stationary policies multiobjective Markov decision processes. IEEE International Symposium Approximate Dynamic Programming Reinforcement Learning, pp. 158165. IEEE.
Yang, Z., & Wen, K. (2010). Multi-objective optimization freeway traffic flow via
fuzzy reinforcement learning method. 3rd International Conference Advanced
Computer Theory Engineering, Vol. 5, pp. 530534.
Yin, S., Duan, H., Li, Z., & Zhang, Y. (2010). Multi-objective reinforcement learning
traffic signal coordinate control. 1th World Conference Transport Research.
Zeleny, M., & Cochrane, J. L. (1982). Multiple criteria decision making, Vol. 25. McGrawHill New York.
Zhao, Y., Chen, Q., & Hu, W. (2010). Multi-objective reinforcement learning algorithm
MOSDMP unknown environment. Proceedings 8th World Congress
Intelligent Control Automation, pp. 31903194.
Zheng, K., Li, H., Qiu, R. C., & Gong, S. (2012). Multi-objective reinforcement learning
based routing cognitive radio networks: Walking random maze. International
Conference Computing, Networking Communications, pp. 359363.
Zitzler, E., Thiele, L., Laumanns, M., Fonseca, C. M., & da Fonseca, V. G. (2003). Performance assessment multiobjective optimizers: analysis review. Evolutionary
Computation, IEEE Transactions on, 7 (2), 117132.

113

fiJournal Artificial Intelligence Research 48 (2013) 231 252

Submitted 4/2013; published 10/2013

Optimal Implementation Watched Literals
General Techniques
Ian P. Gent

ian.gent@st-andrews.ac.uk

School Computer Science, St Andrews University
St Andrews, Fife KY16 9SX, UK

Abstract
prove implementation technique scanning lists backtracking search
algorithms optimal. result applies simple general framework, present:
applications include watched literal unit propagation SAT number examples
constraint satisfaction. Techniques like watched literals known highly space
efficient effective practice. implemented circular approach described
here, techniques also optimal run time per branch big-O terms amortized
across search tree. also applies multiple list elements must found.
constant factor overhead worst case 2. Replacing existing non-optimal
implementation unit propagation MiniSat speeds propagation 29%, though
enough improve overall run time significantly.

1. Introduction
many backtrack search procedures, given list must contain element satisfying
property. call acceptable element. acceptable element exists list,
relevant action triggered, assigning unit clause SAT (Boolean Satisfiability). paper considers monotonic acceptability properties: i.e. element
unacceptable node search tree, remains unacceptable descendant nodes.
description general, applies vital components modern search techniques.
well unit propagation SAT, typical application CSP (Constraint Satisfaction
Problem) support variable-value pair general purpose arc consistency algorithm,
e.g. MGAC2001/3.1 (Bessiere, Regin, Yap, & Zhang, 2005). acceptable element
(support) available, triggered action variable-value pair must pruned.
common technique maintaining acceptability keep pointer known
acceptable element. element becomes unacceptable search process,
scan list new acceptable element. find one change pointer. not,
trigger necessary action. backtracking, must guarantee pointer
points acceptable element. several methods achieving this. One
reset pointer list scan. second store current value
pointer restore value backtracking. paper focuses third method,
backtrack-stable approach, used watched literals SAT.
backtrack-stable approach simple: changes made pointer except
scanning list new acceptable element. Watched literals SAT,
classic example approach, proven highly effective practice (Moskewicz,
Madigan, Zhao, Zhang, & Malik, 2001). correctness depends acceptability
monotonic. move pointer new acceptable element particular node,
c
2013
AI Access Foundation. rights reserved.

fiGent

new element must acceptable ancestor nodes. backtrack thus still
acceptable element, even different one entered node with.
Unfortunately, branch (i.e. sequence nodes root leaf node),
longer guarantee single pass list enough. given node pointer may
move due moves children nodes, meaning may missed elements. Indeed,
single branch may need check every value list many times.
paper shows simple, circular, implementation backtrack-stable approach optimal big-O terms amortised across branches tree, significantly
better theoretical property previously suggested. amortisation applies
search tree explored depth-first style, independent size search tree.
constant increase worst case complexity 2. Applicability existing code depends
low-level implementation details. discuss cases detail. cases existing implementations shown optimal, others optimal implementation
watched literals used. example show empirically implementing watched
literals optimally speeds unit propagation MiniSat 29%, although effect
lead statistically significant improvement overall MiniSat solution time.
Section 2 describes simple framework scanning lists backtracking search
gives pseudocode three methods compared paper, gives worked example motivate proof circular approach optimal. Section 3 gives proof
related results, including detailed comparisons state restoration method. Section 4 generalises result multiple acceptable elements. Later sections discuss applications constraint satisfaction, watched literal unit propagation SAT, including
experiments MiniSat. Appendices gives proofs omitted main text, detailed
methodology SAT experiments, results another method list scanning,
summary Online Appendix paper.

2. Simple Framework List Scanning Algorithms
list list length N .1 assume boolean function acceptable(list,i)
check list elements, returning true list[i] acceptable false not. Throughout
paper, function required monotonic, following sense.
Definition 1 (Monotonicity Acceptability). Acceptability monotonic if, whenever
acceptable(list,i) fails node, acceptable(list,i) would also fail called
remainder search process node descendant nodes.
Note definition allows elements list moved, long unacceptability index maintained: flexibility important Section 4.
unacceptability detected must find new acceptable element, guarantee none
exists. achieved one three variants function findNewElement (sometimes abbreviated FNE). variant updates value pointer last, either
succeeds acceptable value list[last], fails guarantee acceptable value list exists. execution environment assumed guarantee that,
1. paper assume computational model word size least log N bits,
standard operations words take O(1) time. ideal complexity-theoretic point
view, standard assumption literature, although unfortunately usually clearly stated.

232

fiOptimal Implementation Watched Literals

node search tree, list[last] changes acceptable unacceptable,
findNewElement(list) called next node visited, except backtracking occurs node descendant node visited. purposes
paper, detection unacceptability counted call acceptable: whatever
cost detection approach studied here. assume initialisation phase O(N ) calls acceptable made find initial value last:
calls considered preprocessing charged node search tree.
also assume calls findNewElement made initialisation
unacceptability detected.
variant findNewElement ensure following invariant maintained.
Note invariant means one first two cases holds, value list[last]
unacceptable iff acceptable element list.
Invariant 2. times least one following true:
value list[last] acceptable;
acceptable element list;
initialisation process completed;
value list[last] become unacceptable current node findNewElement yet called completed.
Many calls findNewElement may happen single node, facts
acceptability may become known time. example, case unit
propagations SAT, current watched literal (i.e. list[last]) may become unsatisfiable
(unacceptable), scan new watched literal (value last). later
propagations node make new value unsatisfiable, leading new scans.
paper compares three implementations findNewElement shows good
properties last one. differences arise last dealt
invocations findNewElement. simplest variant shown Procedure 1:
time called previous value last discarded list searched
beginning.
Procedure 1: FNE-NoState(list)
1: last := -1
2: repeat
3:
last := last + 1
4:
acceptable(list,last) return true
5: last = N
6: return false
Procedure 1 simple, certainly maintains Invariant 2, highly space-efficient.
significant disadvantage worst case require (N 2 ) calls acceptable
every leaf node, stated Proposition 3 proved Appendix E. (Appendix E
omitted main text available online, see Appendix D.)
233

fiGent

Proposition 3. procedure FNE-NoState maintains Invariant 2. makes O(N 2 )
calls acceptable per branch search tree, requires (N 2 ) worst case.
(Proof Appendix E online.)
remaining two variants use FNE-NoState initialise last, make different
calls thereafter. second variant based state restoration. call continues search
recent value last found current node ancestors. Since
value last may changed descendent nodes, mechanism backtracking
search solver must exploited restore value last backtracking occurs.
method vary solvers, critical paper. Given this,
method shown Procedure 2.
Procedure 2: FNE-RestoreState(list)
1: repeat
2:
last := last + 1
3:
acceptable(list,last) return true
4: last = N
5: return false
Since acceptability monotonic last always restored value previously,
invariant guaranteed. following, proof necessary.
Proposition 4. procedure FNE-RestoreState maintains Invariant 2. given
branch search tree root leaf node, N calls acceptable made.
final, backtrack-stable, variant, restore former values last
backtracking. problem branch, even single node,
value last moved later nodes restored return current
node. deal correctly, allow every element list checked
even may already checked higher current branch even
node. focus paper circular method checking elements:
end list circle around end list zero, continue checking
hit value last call made. initialisation call
procedure FNE-NoState. subsequent calls following.
Procedure 3: FNE-Circular(list)
1: last-cache := last
2: repeat
3:
last := last + 1
4:
last = N last := 0
5:
acceptable(list,last) return true
6: last = last-cache
7: return false
claim made originality circular method: used Gent, Jefferson,
Miguel (2006b) probably earlier authors. Unlike previous variants,
invariant hold search algorithms. Correctness proved Section 3
below, context downwards-explored search trees, defined Definition 5.
234

fiOptimal Implementation Watched Literals

.
One difference Procedures 2 3 noted. given node
search tree, Procedure 2 make N calls acceptable. However, Procedure
3 can, perhaps counterintuitively, require almost 2N calls single node. example,
suppose given node last = 0. value becomes unacceptable
acceptable value N 1, requires N 1 calls set last = N 1.
propagation may later make N 1 unacceptable also. resulting new call Procedure
3 cannot find acceptable value, must still check values 0 N 2
memory previous call. (The variable last-cache local call Procedure.)
N 1 calls acceptable, total 2N 2 node.
2.1 Worked Example
present worked example Procedure 3, showing amortize count calls
acceptable way form basis optimality results paper.
Figure 1 shows example search using circular approach. example assume
node id = 0 :last=14, cost = 14X
last=17

8 :14, 17X

1 :14, 0
last=19

2 :17, 3X

last=16

5 :17, 18X

9 :16, 21X

12 :14, 18X

last=18

3 :17, 19

P

= 36

0,1,2,3

4 :19, 2X

P
0,1,2,4

= 19

6 :18, 20X

P
0,1,5,6

= 52

last=15

7 :17, 19X

P
0,1,5,7

10 :16, 0

P

= 51

0,8,9,10

= 52

11 :16, 0

P
0,8,9,11

= 52

13 :15, 1X

P
0,8,12,13

= 50

14 :14, 38X

P

= 87

0,8,12,14

Figure 1: search using circular approach. See main text description.
list searched 20 elements, indexed 0 19. initialisation
sets last = 0. box node indicates node number (italics), value last
calls FNE-Circular (bold), cost calls total node
(measured number calls acceptable), including failed call one. X
indicates successful call made, indicates unsuccessful call. occur
node, value last moved successful call, value later
becomes unacceptable, another call FNE-Circular fails. branch annotated
value last search left changed value last. example, node
5 value last 17, left child set 18, meaning changed
branched right. total cost branch listed, e.g. 87 last branch.
example, first 14 elements become unacceptable root, calls FNE
must check 14 elements order set last = 14. restored last maximum cost
235

fiGent

branch would number elements list, case 20. used
FNE-Circular, searched one branch, maximum cost would almost double,
38. could happen eventually settle last = 19, becomes unacceptable
failed call costs 19. contrast, extreme right hand branch costs
14 + 17 + 18 + 38 = 87, twice maximum search took single branch.
Summing number checks FNE branch, get 36 + 19 + 52 + 51 + 52 +
52 + 50 + 87 = 399, almost 50 times number branches, 8. However, counts
many costs twice. summing node, total number checks across tree
14 + 0 + 3 + 19 + 2 + 18 + 20 + 19 + 17 + 21 + 0 + 0 + 18 + 1 + 38 = 190. 20
times number branches, less 40 times number branches. show
latter bound always true: never look list elements 2kN ,
N number list elements k number branches.
left branch segment (LBS) segment branch left branching decisions
ending leaf node. paper, left child given internal node taken mean
whichever child node explored first.2 Figure 2 shows search tree Figure 1,
showing LBSs cost across LBS. Crucial points note are:
left branch segment contains consecutively numbered nodes; cost left branch
segment never 38 = 2N 2; total cost summed left branch
segments (36 + 2 + 38 + 19 + 38 + 0 + 19 + 38 = 190) identical total cost across
tree calculated earlier. observations tree provable general.

3. Formal Results
worked example showed binary branching, necessary. left
hand child node may number later nodes (including zero unary
node.) leaf node simply node children. prove correctness optimality
FNE-Circular search algorithms build trees following type:
Definition 5 (Downwards-Explored Search Tree). search tree Downwards-Explored
if: node n0 tree, nodes descending n0 visited later n0 ,
node n1 descended n0 visited later n0 , nodes tree
descend n0 visited n0 n1 .
Depth-first search certainly defines downwards-explored tree, many variants
conflict-directed backjumping (Prosser, 1993). less obvious case single restart
modern Conflict-Driven Clause Learning (CDCL) SAT solver (Marques-Silva, Lynce, &
Malik, 2009). backtracking, search returns level tree chosen conflictanalysis, guaranteed intermediate parts search tree unsatisfiable,
never visited again, resulting downwards-explored search tree. CDCL solvers
undertake restarts, major algorithms Iterative Deepening (Korf, 1985)
Limited Discrepancy Search (Harvey & Ginsberg, 1995; Prosser & Unsworth, 2011).
three examples iteration considered separately gives downward-explored
tree. results current paper therefore apply single iteration restart
algorithm. also apply multiple restarts iterations one counts separately
2. algorithm may regard first branching choice right branch, e.g. limited discrepancy search
going heuristic, paper left child defined whichever one explored first.

236

fiOptimal Implementation Watched Literals

node id = 0 : last=14, cost = 14X
last=17

8 :14, 17X

1 :14, 0
last=19

2 :17, 3X

last=16

5 :17, 18X

9 :16, 21X

12 :14, 18X

last=18

3 :17, 19

P
0,1,2,3

= 36

4 :19, 2X

P
4

=2

6 :18, 20X

P
5,6

= 38

last=15

7 :17, 19X

P
7

= 19

10 :16, 0

11 :16, 0

P

P

8,9,10

= 38

11

=0

13 :15, 1X

P
12,13

= 19

14 :14, 38X

P

= 38

14

Figure 2: example Figure 1 additional annotations. double line indicates
left hand branch, wavy line right hand branch. sequence double
lines indicates left branch segment. solid box around node indicates
start left branch segment. Finally, sums costs made
left branch segment ending leaf node.

iteration, i.e. counting branch twice even duplicate branch
previous iteration. major algorithms explore downwards: e.g. breadth-first
best-first search. algorithms, consecutive nodes last, contained
search state, moves pointing acceptable unacceptable value.
results paper therefore apply way algorithms.
first result correctness FNE-circular method, slightly generalised
also apply middle-out Procedure 5 Appendix B.
Definition 6 (Locally Correct). findNewElement procedure locally correct iff:
1. element list acceptable procedure sets last point acceptable
value succeeds;
2. element list acceptable procedure fails exits last set
value entry.
Specifically, FNE-Circular locally correct design, therefore global correctness corollary following theorem.
Theorem 7. (Correctness) search algorithm defines downwards-explored search
tree, procedure findNewElement locally correct, Invariant 2 true times.
(Proof Appendix E online.)
Definition 8 (LBS). left branch segment ending leaf node n LBS(n) defined
recursively follows:
237

fiGent

n LBS(n)
m1 LBS(n) m1 left hand child parent node m2 , m2 LBS(n).
convenience, parent node one child call left hand child.
LBS(n) minimal set nodes satisfying properties.
Note leaf node left hand child parent, definitions
trivially give LBS(m) = {m}. Every internal node exactly one left child,
proceed follows.
Lemma 9. Every node tree contained exactly one left branch segment.
downwards-explored search tree, nodes left branch segment visited consecutively
without search visiting nodes. (Proof Appendix E online.)
Theorem 10. downwards-explored search tree, N 1 calls
acceptable made successful calls FNE-Circular left branch segment.
Proof. proof relies monotonicity acceptability tree.
nodes LBS explored sequentially without interruption, Lemma 9, calls
FNE-Circular therefore changes last, consecutive. definition FNECircular means check N 1 elements single LBS, last must
incremented least N times, meaning every value checked least LBS,
including original value last entry LBS. Call value i. Say root LBS
m1 , element checked node m2 either m1 = m2 m1 ancestor
m2 . entry m1 call acceptable(list, i) m2 , every list element
must checked. Furthermore, value j, either check j FNECircular failed, succeeded later value become unacceptable causing
another call FNE-Circular. case value last would
moved j. Therefore, j, list element j cannot acceptable
check made node m2 . Therefore, call FNE-Circular makes
second check must fail. required, shown
N 1 calls acceptable LBS, unsuccessful call FNE-Circular.
Corollary 11. downwards-explored search tree, calls FNE-Circular LBS
make 2N 2 calls acceptable.
Proof. Theorem 10, maximum number list elements checked successful calls
LBS N 1. first unsuccessful call FNE-Circular check N 1:
elements list except list[last]. calls necessary LBS, since
element become acceptable again. total cost bounded 2N 2.
Theorem 12. downwards-explored search tree containing k branches, calls FNECircular make k(2N 2) calls acceptable.
Proof. call FNE-Circular occurs node search tree. node
search tree exactly one LBS. Therefore every call FNE-Circular occurs
exactly one LBS. 2N 2 acceptability checks LBS. tree
k branches exactly k LBSs, meaning total number list elements
checked tree bounded k(2N 2).
238

fiOptimal Implementation Watched Literals

therefore optimality following sense:
Theorem 13. (Optimality) downwards-explored search tree, circular approach
requires space one last pointer worst case O(N ) calls acceptable per
branch tree, algorithm require o(N ) calls per branch. (Proof Appendix E
online.)
results show restoring state circular equivalent worst case
time complexity big-O terms across tree, compare precisely.
Proposition 14. circular, many 2k(N 2) calls acceptable
downwards-explored search tree. state restoration, number calls acceptable
bounded kN k(N 1) calls acceptable tree. (Proof
Appendix E online.)
Thus worst case number list element checks backtrack last twice worst case number list checks backtrack it.
apply instance instance basis, following result shows.
Proposition 15. (Non Dominance) techniques check less across
downwards-explored search tree. circular method take (k) times fewer calls
acceptable across tree state restoration, state restoration need (N )
times fewer calls circular. (Proof Appendix E online.)
language Likitvivatanavong, Zhang, Shannon, Bowen, Freuder (2007),
LBS, circular positive repeats (duplicate successful calls acceptable).
negative repeats (duplicate failed calls) last call FNE-Circular
fails, failed call FNE acceptable element leaf
node. reduces significantly chance N calls LBS. example,
SAT, consider clause r literals. random boolean assignment, 21r
chance literals valid full assignment, 2rr exactly one literal
valid. two valid literals, failed call. chance call
FNE failing r+1
2r LBS. clauses 10 literals random
assignments maximum 1% chance negative repeats LBS.

4. Generalisation Multiple Acceptable Elements
important case need maintain multiple acceptable elements list.
Specifically, must ensure least W different elements list acceptable,
trigger action less W are. classic example watched literals SAT,
W = 2 action unit propagation, examples arise constraints
higher W discussed Section 5. circular approach generalised,
bound number calls acceptable per branch independent W .
implementation technique maintain two lists. first, called watched,
length W 1, second list, unwatched, length N . union two lists
original list list length N + W 1. Initialisation assumed either make
elements watched plus unwatched[last] acceptable, (if possible) trigger
necessary action. solver infrastructure assumed ensure correct notification
239

fiGent

findNewElement: must maintain position element watched,
done O(1) time per move element using O(N + W ) space. assume
element watched changes acceptable unacceptable, single
value unwatched[last] does, FNE-Circular-W called appropriate
parameters next node visited, unless backtracking occurs then. also
assume one event happens node, separate call happens
event. Given assumptions, implementation almost trivial, follows.
FNE-Circular-W(list,elt,i)
1:
// elt value list newly unacceptable
2:
// index elt watched unless elt = unwatched[last ]
3: elt 6= watched[i]
4:
watched[i] := unwatched[last]
5:
unwatched[last] := elt
6: return FNE-Circular(unwatched)
Everything follows depends fact that, despite swapping elements, acceptability monotonic list unwatched.
Procedure 4:

Proposition 16. Acceptability unwatched monotonic acceptability list is.
Proof. acceptability list monotonic, way nonmonotonicity could
occur element unwatched replaced one watched.
happen FNE-Circular-W Line 5. value unwatched[last] replaced
elt, elt becoming unacceptable reason FNE-Circular-W called.
Therefore, call acceptable(list,last) must return false, whether would
succeeded replacement. Definition 1, monotonicity therefore respected.
Proposition 16 Corollary 11 make following immediate. remarkable
bound Corollary 17 independent W , number acceptable elements required.
Initialisation need O(N + W ) calls acceptable, done once.
Corollary 17. downwards-explored search tree, calls FNE-Circular-W
LBS make 2N 2 calls acceptable.
must also show correctness, done revised invariant. first
clause holds, W required acceptable elements, second does,
W 1 acceptable elements trigger necessary action.
Invariant 18. times least one following true:
elements watched {unwatched[last]} acceptable;
acceptable element unwatched;
initialisation process completed;
current node, least one element watched{unwatched[last]} become
unacceptable corresponding call findNewElement yet completed.

240

fiOptimal Implementation Watched Literals

Lemma 19. downwards-explored search tree, call FNE-Circular-W either
returns false reduces one number unacceptable elements set watched
{unwatched[last]}. (Proof Appendix E online.)
Theorem 20. (Correctness) downwards-explored search tree, FNE-Circular-W
maintains Invariant 18. (Proof Appendix E online.)
Chai Kuehlmann (2003) described multiple watches pseudo-boolean solver,
although current results apply number watches varied
search. Chai Kuehlmann give implementation details: implementation
described follows Gent et al. (2006b) sum boolean variables.

5. Application Constraint Satisfaction
first application constraint propagation, specifically maintaining generalised arc
consistency. optimal algorithm GAC2001/3.1 easily turned algorithm
MGAC2001/3.1 maintains GAC search (Bessiere et al., 2005).
Corollary 21. constraint arity r variable domain size d,
downwards-explored search tree circular approach maintaining last pointer
MGAC2001/3.1 achieved using space store O(dr) last pointers (beyond storage
space constraint ), requires time check O(rdr ) tuples per branch. (Proof
Appendix E online.)
reasonable assumption takes time O(r) check tuple since arity
r (Bessiere et al., 2005).3 basis get time O(r2 dr ) per branch. Bessiere et al.
report time complexity O(r2 dr ) GAC2001/3.1 require number
last pointers. shows amortized worst case big-O time per branch
MGAC2001/3.1 needed simply one-off algorithm GAC2001/3.1,
using space. Bessiere (2004) reports using state restoration techniques
implementation MAC2001 (Bessiere & Regin, 2001). therefore used additional
space, since many copies last must stored instead one.
results improve given van Dongen (2004). binary constraints (r = 2),
gives upper bound space complexity O(d min (n, d)) per constraint time
optimal implementation using time O(d2 ) per branch, domain size n
number variables problem. Corollary 21 gives O(d2 ) time improved
O(d) space, although van Dongens results remain valid given upper bounds.
several studies time complexity maintaining arc consistency
branch, suggestion leaving last pointers alone good theoretically.
existing circular implementations seen optimal. example, Gent
et al. (2006b, p. 185) wrote: one general disadvantage mentioned
watched triggers, . . . often possible use propagation algorithm
optimal worst case terms propagation work performed single branch.
3. constraints stored extensionally, true, space requirement store constraint
O(rdr ). However constraints may also stored intensionally procedurally, case
checking time either larger smaller, space requirement arbitrarily small.

241

fiGent

example variant GAC-2001/3.1 below. fact, implementation MGAC2001/3.1 time optimal amortized across branches better space complexity
van Dongen (2004) reports optimal implementations MAC-2001/3.1.
Regin (2005) studied maintaining arc consistency search without backtracking
last pointer. writes: last values restored backtracking
time complexity AC-6 AC-7 algorithms O(d3 ) (Regin, 2005, p. 528),
gives example similar right hand branch Figure 1. context
domain size constraints binary maximum length list N = d2 . Regin
entirely correct. contribution show lack optimality branch
compensated amortization factor d. Regin writes also: Currently [i.e.
paper], MAC version algorithms capable keep optimal time
complexity every branch tree search (O(d2 ) per constraint, size
largest domain), without sacrificing space complexity. method recompute
correct value last backtracking without storing it, comparing current value
last values restored domain backtracking. elegant generalise
obvious way non-binary constraints number combinations restored
values exponential.
Likitvivatanavong et al. (2007) discuss cost Arc Consistency search.
give ACS-resOpt, uses instance list scanning framework given here,
report optimal given node, optimal branch
tree (which call path-forward complexity). reordering domains search,
Adaptive Domain Ordering (ADO) enforces MAC binary constraints optimal
property O(ed2 ) worst-case time complexity branch search tree (Likitvivatanavong, Zhang, Bowen, & Freuder, 2005). Unfortunately perform well
empirical tests (Likitvivatanavong et al., 2007), combined reordering domains search, militates widespread adoption constraint solvers.
applications constraints. Nightingale, Gent, Jefferson, Miguel
(2013) use circular technique avoid restoring state GAC algorithms exploiting
short supports. Jefferson, Moore, Nightingale, Petrie (2010) use propagator
generalised constraint. Gent et al. (2006b) used circular approach propagating
element constraint. also used generalisation W literals sum booleans
constraint. examples seen code Minion constraint solver
(Gent, Jefferson, & Miguel, 2006a), version 0.15, described papers
constraint litsumgeq. seen excellent theoretical properties.

6. Application Satisfiability
second application watched literal unit propagation SAT (Moskewicz et al., 2001).
clause list literals. acceptable element one represents either unassigned
satisfied literal. standard two-literal watching must maintain two acceptable elements. Modern CDCL SAT solvers quickly learn large numbers large clauses, thus
benefit greatly maintain two pointers clause. approach Section 4 applied easily. However, normally done SAT solvers.4 Successful
solvers often implement search new watches non-optimal way. see this,
4. grateful reviewer paper pointing fact me.

242

fiOptimal Implementation Watched Literals

must examine code, necessary level implementation detail given
papers. example MiniSat (Een & Sorensson, 2003) tinisat (Huang, 2007) implement watched literals following non-optimal way. core implementation
shown Procedure 5: key change watched elements list watched
instead one Procedure 4. details MiniSats implementation
Procedure 5: important maintenance blocked literals,
discuss below. Proposition 3 technically apply, nevertheless state
without proof approach leads (N 2 ) calls acceptable per branch
worst case.
FNE-NoState-W(list,elt,i)
// elt value list newly unacceptable
// index elt watched
result = FNE-NoState(unwatched)
result
watched[i] := unwatched[last]
unwatched[last] := elt
return result

Procedure 5:
1:
2:
3:
4:
5:
6:
7:

compare practical performance, adapted MiniSat version 2.2.0 optimal unit
propagation using Procedure 4. call Circular MiniSat original Stock
MiniSat. Appendix gives full methodology detailed results. key results
follows. First, Circular unit propagates notably faster Stock features
MiniSat related unit propagation removed. searching 594 instances
10 million conflicts, Circulars mean time 141.6s, compared mean 182.6s Stock,
Stock takes 29.1% time. Median performance much closer, 73.1s Circular
compared 78.1s Stock. larger disparity mean Stock never
15% faster, Circular much 9.5 times faster Stock. Second,
features MiniSat restored, improved propagation speed Circular
translate improved performance. 330 instances, statistical support
reject null hypothesis two solvers equivalent performance.
interesting look many watched literal scans blocked, term used
MiniSats code. clause scan blocked watched literal (at time watch
set up) valid literal now, clause satisfied new watch needed.
saves accessing memory relating clause. instance measured ratio
b/u blocked unblocked watched scans. higher b/u better since results
less watched scans. computed ratio b/u values obtained Circular
Stock MiniSat, = (bc /uc )/(bs /us ). Figure 3 shows plotted obtained speedup.
Behaviour different two regions. 1.2 (123 instances) median speedup
1.48 mean 1.86. high correlation speedup, r2 = 0.88.
< 1.2 (471 instances) get correlation speedup, r2 = 0.01. Compared
Stock MiniSat median speedup 0.97 mean 0.98, i.e. slight slowdowns.
analysis indicates speedup occurs instances Circular much
better Stock MiniSat setting watches literals likely valid later
search thus block watched literal scans. would interesting investigate
result theoretically, since follow results paper. may related
243

fiGent

10
8
6
5
4
3
2

1
0.8
Circular:Stock
0.548x + 0.523

0.6
0.5
0.5 0.6

0.8

1

1.2

2

3

4

5

6

8

10

20

Figure 3: Scatterplot (x-axis) speedup ratio conflicts per second Circular
Stock MiniSat (y-axis). vertical line shows = 1.2. line 0.548x +
0.523 best-fit line region 1.2.

fact Circular sets watches arbitrary literals clause, Stock MiniSat
tend set watches literals appearing early clause. Therefore good blocking
literal late clause, Circular chance watching it.
Head-tail lists important advance implementation unit propagation
SAT (Zhang & Stickel, 2000). Pointers first unassigned literal clause (head)
last (tail) maintained state-restoration. Head-tail lists led watched literals
(Moskewicz et al., 2001), two pointers arbitrary (but different) unassigned literals. Watched literals (or variants thereof) become standard technique
efficient implementation unit propagation SAT solvers. implemented described paper, theoretical properties watched literals seen
nearly good head-tail lists time, much reduced space overheads.
variant implementation watched literals JQuest (Lynce & Marques-Silva,
2005). Scans go last end list, restart last backwards 0.
middle-out search big-O optimal provided current direction search
persistent calls. However, direction search always initially one direction, require (N 2 ) checks per branch, case JQuest. Full algorithmic
details proofs statements Appendix B. Lynce Marques-Silva (2005)
also introduced literal sifting attempt get best worlds: literals clause
reordered search avoid repeating checks backtracking pointers. Lynce
Marques-Silva report slightly better empirical performance literal sifting (although
noted comparison non-optimal watched literal implementation). Generalising literal sifting arbitrary number acceptable elements performing
theoretical experimental comparisons circular approach open future work.
244

fiOptimal Implementation Watched Literals

Another application seen optimal, implemented circular style,
Van Gelders (2002) three literal watching detect binary clauses.

7. Conclusions
shown circular approach scanning lists backtracking search desirable
theoretical properties. big-O optimal time (measured number acceptability
checks) amortized across search tree. worst case constant factor
two. results average case apply every search tree number
branches. results generalise maintaining multiple acceptable elements single list,
complexity independent number elements required. result relevant
practically important algorithms applications SAT CSP. Techniques like
watched literals SAT known successful practice, certainly reduced
space overheads compared state restoration methods. implemented appropriately,
newly understood essentially theoretical disadvantages time
either. existing implementations seen optimal even though
realised implementers. implementations unit propagations real world
SAT solvers optimal, e.g. MiniSat. Replacing optimal implementation
MiniSat improve propagation speed mean 29%. Experiments suggested
circular approach better able find watched literals likely true future nodes.
However, improved propagation speed result improved speed full solver.

Acknowledgments
thank Chris Jefferson Peter Nightingale help paper many ways,
example C++ coding advice suggestions implement variants watched
literals MiniSat. thank JAIR editor paper, Holger Hoos, anonymous
reviewers suggestions leading comparison MiniSat study middleout approach, requiring much precise presentation results. thank
authors MiniSat, tinisat, JQuest making code available study.

Appendix A. Experiments MiniSat
appendix describes methodology gives detailed results experiments watched
literal implementation MiniSat version 2.2.0. Two variants MiniSat implemented.
first, Circular, implements two literal watching algorithm using approach described
Section 4. second, TwoPointer, variant two independent pointers
maintained, based method preprint paper. Since worse properties
practice theory, TwoPointer described Appendix C.
Timings reported performed single Apple MacPro (MacPro4,1),
two Quad-Core Intel Xeon chips 2.26GHz, L2 Cache 256KB per core, L3 Cache 8MB per
processor, 32GB DDR3 RAM 1066MHz, 7200 RPM hard drive, MacOS 10.6. MiniSat 2.2.0
used codebase, compile time flags distribution. Instances
245

fiGent

100

Circular ratio
TwoPointer ratio

10

1

0.1

0.01
0.01

0.1

1

10

100

1000

Figure 4: Scatterplot relative performance Circular TwoPointer variants compared
Stock MiniSat. x-axis gives run time Stock MiniSat seconds.
y-axis gives ratio Stock time Circular/TwoPointer time. Ratios 1
mean alternative faster, 1 Stock MiniSat faster.

SAT 2005 competition (Le Berre & Simon, 2006) used.5 reasons choice
MiniSat well competition, unlikely straw man,
provides large manageable set benchmarks: using entire available
set chance selection bias. significant advances SAT
solvers since 2005, aware major changes propagation, focus
current paper. Code results available online, see Appendix D.
first experiment, features MiniSat cut out, e.g. clause learning,
conflict analysis, heuristics. eliminating aspects solver, variant
searches identical spaces differences speed must due differing speeds
propagation. optimisations applied exploit cut-down solver,
propagators tested used second experiment. However,
since must propagated running full MiniSat, set learnt clauses
included. this, standard MiniSat run instance 60s (on different
Linux machine). 594 instances unsolved 60s, clause set saved give
realistic static instance cut-down versions MiniSat. Two versions
propagator created: one instrumentation switched off, maximise
speed; one several additional counters added provide metrics
nature search watched literals. reporting cpu times first version
used (with times median three runs). Search performed limit
107 conflicts reached (excepting one instance solved 7.75 106 conflicts).
tests unrestricted MiniSat, three runs performed algorithm-instance
combination. MiniSat default settings used cpu timeout 1200s memory5. http://www.lri.fr/~simon/contest/results/download/distrib-benchs-random-sat2005.tar.bz2,
distrib-benchs-crafted-sat2005.tar.bz2, distrib-benchs-industrial-sat2005.tar.bz2.

246

fiOptimal Implementation Watched Literals

1GB. 87 instances, taking 0.01s max-to-min deviation
algorithm 10% median, another 18 runs performed algorithm,
median 21 runs used. Instances algorithm solved within timeout,
less 0.01s, discarded. 330 instances remained. Results shown
Circular TwoPointer Figure 4. huge variation runtimes
instance, almost 100 times. propagation method find different
conflicting clauses, leading different sets learnt clauses heuristics. instances
vertically diagonally x = 1200 one method timed-out
not. paired t-test performed Circular Stock MiniSat,
null hypothesis distributions mean. gave = 0.127,
p = 0.899, i.e. highly insignificant result. assumption normality invalid,
t-test randomised 100,000 times (Cohen, 1995). these, 48.3% gave lower tvalue 51.7% higher value. Similar results obtained TwoPointer Stock
MiniSat. conclusion must statistical evidence either Circular
TwoPointer either better worse Stock MiniSat fully featured solver.

Appendix B. Middle-Out List Scanning
appendix gives formal presentation algorithms proofs middle-out scanning
watched literals discussed Section 6.
Procedure 5: FNE-MiddleOut-Helper(list,delta)
Require: delta equals 1 +1
1: last-cache := last
2: repeat
3:
last := last + delta
4:
acceptable(list,last) return true
5: last = 0 last = N
6: last:= last-cache
7: return false
Procedure 6: FNE-MiddleOut(list)
Require: delta persistent calls equals 1 +1, initialised either
1: FNE-MiddleOut-Helper(delta)
2:
return true
3: else
4:
delta := delta
5:
FNE-Helper(delta)
6:
return true
7:
else
8:
return false
First, note FNE-MiddleOut locally correct (Definition 6). Therefore Theorem 7, FNE-MiddleOut maintains Invariant 2 times. FNE-MiddleOut turns
optimal big-O terms, follows analogue Theorem 10.

247

fiGent

Theorem 22. downwards-explored search tree, total number calls acceptable made successful calls FNE-MiddleOut LBS 2N . (Proof
Appendix E online.)
result follow similar development circular, analogous results,
omit results except important, state without proof.
Theorem 23. (Optimality) downwards-explored search tree, Middle-Out approach requires space one last pointer worst case O(N ) calls acceptable
per branch tree.
persistence delta executions critical. add line 0 : delta = +1
Procedure 6 give FNE-MiddleOut-Fixed, get following worse result.
Proposition 24. downwards-explored search tree, total number calls acceptable made FNE-MiddleOut-Fixed (N 2 ) per branch search tree.
(Proof Appendix E online.)
solver JQuest Lynce Marques-Silva (2005) implements watched literals
style FNE-MiddleOut-Fixed, non-optimal. cannot deduced
cited paper seen http://sat.inesc.pt/sat/soft/jquest/jquest-src.
tgz file ClauseSCImplWL.java: flag controls direction move first in,
swapped search first watch clause never second.

Appendix C. Maintaining Multiple Pointers: Theory Experiment
Compared described Section 4, naive approach implementing multiple
watches separate last pointer one. unit propagate correctly SAT
two watched literals. Crucially, cannot allow two pointers settle
element. correct method achieve unit propagation follows. pointer
becomes unacceptable, store current value call FNE-Circular. fails
clause entirely false. succeeds different value pointer nothing.
succeeds value pointer call FNE-Circular again.
second call fails must reset value first pointer stored value
unit propagate literal represented second pointer. prove optimality
approach need general version Theorem 10.
Theorem 25. Suppose W pointers last1 , last2 , . . . lastW list maintained
simultaneously, definition acceptability, calls FNE pointer
lasti made points unacceptable element value another
pointer currently has. Then: cN calls acceptable made LBS
downwards-explored search tree, either least one calls FNE-Circular fails
least two pointers take value. (Proof Appendix E online.)
Theorem 25 leads correctness unit propagation procedure described above.
guarantees one satisfiable literal remains clause, pointers
settle unit propagation performed. space requirement O(1) per
last pointer. Detection unacceptability also done O(1) time maintaining
list occurrences literals, consulted literal set false. gives:
248

fiOptimal Implementation Watched Literals

Corollary 26. Unit propagation using watched literals clause N literals
implemented O(1) space using O(N ) time per branch search tree.

10
8
6
5
4
3
2

1
0.8
TwoPointer:Stock
0.437x + 0.572

0.6
0.5
0.5 0.6

0.8

1

1.2

2

3

4

5

6

8

10

20

Figure 5: Scatterplot (x-axis) speedup ratio conflicts per second TwoPointer Stock MiniSat (y-axis). line 0.437x + 0.572 best-fit line
region 1.2. vertical line shows = 1.2.

development following Theorem 10 follows before. state without proof:
Theorem 27. conditions Theorem 25, search tree containing k branches,
calls FNE-Circular make k((c + 1)N 1) calls acceptable.
TwoPointer unit propagates faster Stock. searching 594 instances cutdown MiniSat 10 million conflicts, TwoPointer took mean 155.9s 182.6s
Stock, Stock takes 17.1% time. 10% slower Circulars
mean time 141.6s. Circular never 13% slower TwoPointer
35% faster. mean median speedups Circular TwoPointer 1.10.
median performance Stock slightly better TwoPointer (78.1s 81.1s)
Stock never 22% faster TwoPointer much 7.7 times faster.
see similar results effect blocked watches Circular. Results
Circular definition given main paper Section 6. 1.2 (122
instances), correlates strongly speedup conflicts per second, correlation
coefficient r2 = 0.86. Median speedup region 1.32 mean 1.65. best fit
line shown Figure 5. < 1.2 (472 instances), correlation
speedup, r2 = 0.07. Median mean speedups 0.88 0.90 (so slowdowns
speedups.) regions, extremely high correlation TwoPointer
Circular, r2 > 0.996. Circular, analysis indicates speedup
occurs instances TwoPointer much better Stock MiniSat setting watches
literals likely valid later search thus block watched literal search.
249

fiGent

Results TwoPointer full version MiniSat similar Circular.
methodology described Appendix A, raw t-value 1.54, p = 0.124.
Randomisation 100,000 times gave 26.2% lower t-values 73.8% higher values.

Appendix D. Description Online Appendices
Two Online Appendices available. first textual Appendix E proofs omitted
main text (Gent13a-appendix1.pdf).6 second contains results tables, full
MiniSat outputs, graphs used paper (Gent13a-appendix2.tgz).7 fuller version
appendix, including code variant MiniSat scripts run analyse
experiments, available separately.8 file 4MB unpacks 12MB.
Separately, 2.4GB compressed tar file available containing clausesets written
60s failed search.9

References
Bessiere, C., Regin, J.-C., Yap, R., & Zhang, Y. (2005). optimal coarse-grained arc
consistency algorithm. Artificial Intelligence, 165, 165185.
Bessiere, C. (2004). Personal communication Marc van Dongen.. Described (van
Dongen, 2004).
Bessiere, C., & Regin, J.-C. (2001). Refining basic constraint propagation algorithm.
Nebel, B. (Ed.), Proceedings Seventeenth International Joint Conference
Artificial Intelligence, IJCAI 2001, Seattle, Washington, USA, August 4-10, 2001, pp.
309315. Morgan Kaufmann.
Chai, D., & Kuehlmann, A. (2003). fast pseudo-boolean constraint solver. Proceedings
40th Design Automation Conference, DAC 2003, Anaheim, CA, USA, June
2-6, 2003, pp. 830835. ACM.
Cohen, P. R. (1995). Empirical methods artificial intelligence. MIT Press.
Een, N., & Sorensson, N. (2003). extensible SAT-solver. Giunchiglia, E., & Tacchella, A. (Eds.), SAT, Vol. 2919 Lecture Notes Computer Science, pp. 502518.
Springer.
Gent, I. P., Jefferson, C., & Miguel, I. (2006a). Minion: fast scalable constraint solver.
Brewka, G., Coradeschi, S., Perini, A., & Traverso, P. (Eds.), ECAI, Vol. 141
Frontiers Artificial Intelligence Applications, pp. 98102. IOS Press.
Gent, I. P., Jefferson, C., & Miguel, I. (2006b). Watched literals constraint propagation
Minion. Benhamou, F. (Ed.), CP, Vol. 4204 Lecture Notes Computer
Science, pp. 182197. Springer.
Harvey, W. D., & Ginsberg, M. L. (1995). Limited discrepancy search. Proceedings
Fourteenth International Joint Conference Artificial Intelligence, IJCAI 95,
6.
7.
8.
9.

Also available http://ipg.host.cs.st-andrews.ac.uk/JAIR/Gent13a-appendix1.pdf
Also available http://ipg.host.cs.st-andrews.ac.uk/JAIR/Gent13a-appendix2.tgz
http://ipg.host.cs.st-andrews.ac.uk/JAIR/Gent13a-appendix2-full.tgz
http://ipg.host.cs.st-andrews.ac.uk/JAIR/writtenclausesets.tgz

250

fiOptimal Implementation Watched Literals

Montreal Quebec, Canada, August 20-25 1995, 2 Volumes, Vol. 1, pp. 607615. Morgan
Kaufmann.
Huang, J. (2007). case simple SAT solvers. Bessiere, C. (Ed.), Principles
Practice Constraint Programming - CP 2007, 13th International Conference, CP
2007, Providence, RI, USA, September 23-27, 2007, Proceedings, Vol. 4741 Lecture
Notes Computer Science, pp. 839846. Springer.
Jefferson, C., Moore, N. C. A., Nightingale, P., & Petrie, K. E. (2010). Implementing logical
connectives constraint programming. Artificial Intelligence, 174 (16-17), 14071429.
Korf, R. E. (1985). Depth-first iterative-deepening: optimal admissible tree search.
Artificial Intelligence, 27 (1), 97109.
Le Berre, D., & Simon, L. (2006). Special volume SAT 2005 competitions
evaluations. JSAT, 2 (1-4).
Likitvivatanavong, C., Zhang, Y., Bowen, J., & Freuder, E. C. (2005). Maintaining arc consistency using adaptive domain ordering. Kaelbling, L. P., & Saffiotti, A. (Eds.),
IJCAI-05, Proceedings Nineteenth International Joint Conference Artificial Intelligence, Edinburgh, Scotland, UK, July 30-August 5, 2005, pp. 15271528.
Professional Book Center.
Likitvivatanavong, C., Zhang, Y., Shannon, S., Bowen, J., & Freuder, E. C. (2007). Arc
consistency search. Veloso, M. M. (Ed.), IJCAI 2007, Proceedings 20th
International Joint Conference Artificial Intelligence, Hyderabad, India, January
6-12, 2007, pp. 137142.
Lynce, I., & Marques-Silva, J. P. (2005). Efficient data structures backtrack search SAT
solvers. Ann. Math. Artif. Intell., 43 (1), 137152.
Marques-Silva, J. P., Lynce, I., & Malik, S. (2009). Conflict-driven clause learning sat
solvers. Biere, A., Heule, M., van Maaren, H., & Walsh, T. (Eds.), Handbook
Satisfiability, Vol. 185 Frontiers Artificial Intelligence Applications, pp.
131153. IOS Press.
Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: engineering efficient SAT solver. Proceedings 38th annual Design Automation
Conference, DAC 01, pp. 530535, New York, NY, USA. ACM.
Nightingale, P., Gent, I. P., Jefferson, C., & Miguel, I. (2013). Short long supports
constraint propagation. J. Artif. Intell. Res. (JAIR), 46, 145.
Prosser, P. (1993). Hybrid algorithms constraint satisfaction problem. Computational
Intelligence, 9(3), 268299.
Prosser, P., & Unsworth, C. (2011). Limited discrepancy search revisited. J. Exp. Algorithmics, 16, 1.6:1.11.6:1.18.
Regin, J.-C. (2005). MAC algorithms search without additional space cost.
Proc. 11th Principles Practice Constraint Programming (CP 2005), pp. 520
533.
van Dongen, M. R. C. (2004). Saving support-checks always save time. Artif.
Intell. Rev., 21 (3-4), 317334.
251

fiGent

Van Gelder, A. (2002). Generalizations watched literals backtracking search.
Seventh Intl Symposium AI Mathematics.
Zhang, H., & Stickel, M. E. (2000). Implementing Davis-Putnam method. J. Autom.
Reasoning, 24 (1/2), 277296.

252

fiJournal Artificial Intelligence Research 48 (2013) 513-582

Submitted 12/12; published 11/13

AI Methods Algorithmic Composition:
Comprehensive Survey
Jose David Fernndez
Francisco Vico

josedavid@geb.uma.es
fjv@geb.uma.es

Universidad de Mlaga, Calle Severo Ochoa, 4, 119
Campanillas, Mlaga, 29590 Spain

Abstract
Algorithmic composition partial total automation process music composition using computers. Since 1950s, different computational techniques related
Artificial Intelligence used algorithmic composition, including grammatical
representations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming evolutionary algorithms. survey aims comprehensive
account research algorithmic composition, presenting thorough view field
researchers Artificial Intelligence.

1. Introduction
Many overly optimistic, ultimately unfulfilled predictions made early days
Artificial Intelligence, computers able pass Turing test seemed decades
away. However, field Artificial Intelligence grown got matured, developing
academic research reaching many industrial applications. time, key
projects challenges captivated public attention, driverless cars, natural
language speech processing, computer players board games.
introduction formal methods instrumental consolidation many
areas Artificial Intelligence. However, presents disadvantage areas whose subject
matter difficult define formal terms, naturally tend become marginalized.
case Computational Creativity (also known Artificial Creativity),
loosely defined computational analysis and/or synthesis works art,
partially fully automated way. Compounding problem marginalization, two
communities naturally interested field (AI arts) speak different languages
(sometimes different!) different methods goals1 , creating great difficulties collaboration exchange ideas them. spite this, small
sometimes fragmented communities active research different aspects
Computational Creativity.
purpose survey review bring together existing research specific
style Computational Creativity: algorithmic composition. Interpreted literally, algorithmic composition self-explanatory term: use algorithms compose music.
broad definition, centuries musicians proposing methods
considered algorithmic sense, even human creativity plays key
1. Related problem, uncommon engineering concepts become bent strange ways
interpreted artists. See Footnote 28 page 550 particularly remarkable example.
c
2013
AI Access Foundation. rights reserved.

fiFernndez & Vico

role. commonly cited examples include dArezzos Micrologus, species counterpoint,
Mozarts dice games, Schoenbergs twelve-tone technique, Cages aleatoric music. Readers interested pre-computer examples algorithmic composition
referred introductory chapters almost thesis book subject,
Daz-Jerezs (2000), Aschauers (2008) Nierhauss (2009). survey, use
term algorithmic composition restricted way, partial total automation
music composition formal, computational means. course, pre-computer examples
algorithmic composition implemented computer, approaches
reviewed survey implement classical methodology. general, focus
AI techniques, self-similarity cellular automata also reviewed modern
computational techniques used generating music material without creative
human input.
1.1 Motivation
useful starting points researching past present computer music
Computer Music Journal, International Computer Music Conference 2 annually organized International Computer Music Association 3 , books Machine
Models Music (Schwanauer & Levitt, 1993), Understanding music AI (Balaban
et al., 1992), Music Connectionism (Todd & Loy, 1991), anthologies selected
articles Computer Music Journal (Roads & Strawn, 1985; Roads, 1992). However,
resources algorithmic composition, computer music general.
specific information algorithmic composition, surveys better option.
many surveys reviewing work algorithmic composition. review
analysis composition computer AI methods (Roads, 1985), others discuss
algorithmic composition point view related music theory artistic considerations (Collins, 2009), personal perspective composer (Langston, 1989;
Dobrian, 1993; Pope, 1995; Maurer, 1999). provide depth comprehensive view specific technique algorithmic composition, Anders Miranda
(2011) constraint programming, Ames (1989) Markov chains, Santos
et al. (2000) evolutionary techniques, others specialized comparison paradigms computational research music, Toiviainen (2000). Others
offer wide-angle (but relatively shallow) panoramic field (Papadopoulos & Wiggins,
1999), review early history field (Loy & Abbott, 1985; Ames, 1987; Burns, 1994),
analyze methodologies motivations algorithmic composition (Pearce et al., 2002).
also works combine depth comprehensive reviews wide range
methods algorithmic composition, Nierhauss (2009) book.
context, natural question arises: yet another survey? answer
existing survey article fulfills following criteria: (a) cover methods
comprehensive way, point view primarily focused AI research,
(b) centered algorithmic composition.4 Nierhauss (2009) book algorithmic
2. archives available http://quod.lib.umich.edu/i/icmc/
3. http://www.computermusic.org/
4. Many surveys conflate discussion algorithmic composition (synthesis music) computational analysis music, work Roads (1985), Nettheim (1997) Toiviainen (2000).
become somewhat distracting reader interested algorithmic composition.

514

fiAI Methods Algorithmic Composition

composition comes close fulfilling criteria long, detailed expositions
method comprehensive reviews state art. contrast, survey
intended reasonably short article, without lengthy descriptions: reference
guide AI researchers. aims mind, survey primarily structured
around methods used implement algorithmic composition systems, though early
systems also reviewed separately.
second, practical motivation accessibility. Since Computational Creativity
balances edge AI arts, relevant literature scattered across
many different journals scholarly books, broad spectrum topics computer
science music theory. unfortunate consequence, many different
paywalls researchers relevant content, translating sometimes lot hassle,
partially mitigated relatively recent trends like self-archiving. survey brings
together substantial body research algorithmic composition, intention
conveying effectively AI researchers.

2. Introducing Algorithmic Composition
Traditionally, composing music involved series activities, definition
melody rhythm, harmonization, writing counterpoint voice-leading, arrangement
orchestration, engraving (notation). Obviously, list intended exhaustive
readily applicable every form music, reasonable starting point, especially
classical music. activities automated computer varying degrees,
techniques languages suitable others (Loy &
Abbott, 1985; Pope, 1993).
relatively small degrees automation, focus languages, frameworks
graphical tools provide support specific and/or monotone tasks composition process, provide raw material composers, order bootstrap composition
process, source inspiration. commonly known computer-aided algorithmic composition (CAAC), constitutes active area research commercial
software development: many software packages programming environments
adapted purpose, SuperCollider (McCartney, 2002), Csound (Boulanger,
2000), MAX/MSP (Puckette, 2002), Kyma (Scaletti, 2002), Nyquist (Simoni & Dannenberg, 2013) AC Toolbox (Berg, 2011). development experimental CAAC
systems IRCAM5 (such PatchWork, OpenMusic various extensions)
also emphasized (Assayag et al., 1999). Arizas comprehensive repository software tools research resources algorithmic composition6 constitutes good starting
point (Ariza, 2005a) explore ecosystem, well algorithmic composition general.
Earlier surveys (such Pennycook, 1985 Pope, 1986) also useful understanding
evolution field, especially evolution graphical tools aid composers.
survey, hand, concerned algorithmic composition
higher degrees automation compositional activities, rather typical CAAC.
words, focus techniques, languages tools computationally encode human
musical creativity automatically carry creative compositional tasks minimal
5. http://www.ircam.fr/
6. http://www.flexatone.net/algoNet/

515

fiFernndez & Vico

human intervention, instead languages tools whose primary aim aid human
composers creative processes.
Obviously, divide ends spectrum automation (CAAC representing low degree automation, algorithmic composition high degree automation)
clear, method automates generation creative works used
tool aid composers, systems higher degrees automation custombuilt top many CAAC frameworks.7 Furthermore, human composer naturally
include computer languages tools integral part composition process,
Brian Enos concept generative music (Eno, 1996). conclude considerations,
survey computer systems automating compositional tasks user
expected main source creativity (at most, user expected set
parameters creative process, encode knowledge compose, provide examples music composed humans processed computer). also
includes real-time automatic systems music improvisation, jazz performance,
experimental musical instruments automate certain extent improvisation
music.
Finally, considerations, describe survey about:
Although music defined organized sound, composition written traditional staff notation fully specify music actually sounds:
piece music performed, musicians add patterns small deviations nuances
pitch, timing musical parameters. patterns account musical
concept expressiveness gesture, necessary music sound
natural. problem automatically generating expressive music important
itself, involves creativity, clearly within boundaries algorithmic
composition reviewed survey. reader referred Kirke Mirandas
(2009) review area information.
computational synthesis musical sounds, algorithmic sound synthesis,
understood logical extension algorithmic composition small timescales;
involves use languages tools specifying synthesizing sound waveforms,
rather abstract specification music associated traditional staff
notation. line algorithmic composition algorithmic sound synthesis
blurred previously mentioned CAAC systems, survey
concerned sound synthesis; interested readers may refer Roadss (2004) book
subject.
computer games (and interactive settings), music frequently required
gracefully adapt state game, according rules. kind
music commonly referred non-linear music (Buttram, 2003) procedural
audio (Farnell, 2007). Composing non-linear music presents challenges own,
specifically related problem algorithmic composition, review
literature kind music.
7. case many systems algorithmic composition described here. example,
PWConstraints (described Section 3.2.3) built top PatchWork, described Assayag et al.
(1999).

516

fiAI Methods Algorithmic Composition

three scenarios (automated expressiveness, algorithmic sound synthesis nonlinear music) sparingly mentioned survey, mentioned innovative
(or otherwise notable) techniques involved.
2.1 Early Years
section, review early research published algorithmic composition
computers, clear computational approach. references might
discussed methodology following sections, useful group together
here, since difficult find survey discussing them.
earliest use computers compose music dates back mid-1950s, roughly
time concept Artificial Intelligence coined Darmouth
Conference, though two fields converge time later. Computers
expensive slow, also difficult use, operated batch mode.
One commonly cited examples Hiller Isaacsons (1958) Illiac Suite,
composition generated using rule systems Markov chains, late 1956.
designed series experiments formal music composition. following
decade, Hillers work inspired colleagues university experiment
algorithmic composition, using library computer subroutines algorithmic composition written Baker (also collaborator Hiller), MUSICOMP (Ames, 1987).
library provided standard implementation various methods used Hiller
others.
Iannis Xenakis, renowned avant-garde composer, profusely used stochastic algorithms
generate raw material compositions, using computers since early 1960s
automate methods (Ames, 1987). Though work better described CAAC,
still deserves mentioned pioneer. Koenig, well known
Xenakis, also composer 1964 implemented algorithm (PROJECT1) using
serial composition (a musical theory) techniques (as Markov chains) automate
generation music (Ames, 1987).
However, also several early examples algorithmic composition, though
profusely cited Hiller Xenakiss. Push Button Bertha, composed 1956
(Ames, 1987) around time Hillers Illiac Suite, perhaps third
cited example: song whose music algorithmically composed publicity stunt
Burroughs (an early computer company), generating music similar previously analyzed
corpus. However, least one earlier, unpublished work Caplin Prinz 1955
(Ariza, 2011), used two approaches: implementation Mozarts dice dame
generator melodic lines using stochastic transitional probabilities various aspects
composition. Another commonly cited example Brooks et al. (1957) explored
potential Markoff 8 chain method.
Several early examples also notable. Olsons (1961) dedicated computer
able compose new melodies related previously fed ones, using Markov processes.
work submitted publication 1960, claimed built machine
early 1950s. Also interest Gills (1963) algorithm, implemented request
8. Markov Markoff alternative transliterations Russian surname . spelling
Markov prevalent decades, many older papers used Markoff.

517

fiFernndez & Vico

BBC, represents hallmark application classical AI techniques algorithmic
composition: used hierarchical search backtracking guide compositional process inspired Schoenbergs twelve-tone technique. Finally, worth mentioning
may represent first dissertation algorithmic composition: Padbergs (1964) Ph.D.
thesis implemented compositional framework (based formal music theory) computer
code. work unusual that, instead using random number generators, used
raw text input drive procedural techniques order generate parameters
composition system.
Non-scholarly early examples also exist, though difficult assess
sparsity published material, fact mostly peer-reviewed.
example, Pinkerton (1956) described Scientific American Banal Tune-Maker,
simple Markov chain created several tens nursery tunes, Sowa (1956) used
GENIAC machine9 implement idea (Cohen, 1962), Raymond Kurzweil
implemented 1965 (Rennie, 2010) custom-made device generated music style
classical composers. Another example, unfortunately shrouded mystery, Raymond
Scotts Electronium (Chusid, 1999), electronic device whose development spanned
several decades, reportedly able generate abstract compositions. Unfortunately, Scott
never published otherwise explained work.
machines became less expensive, powerful cases interactive, algorithmic composition slowly took off. However, aside researchers Urbana (Hillers
university), little continuity research, reinventing wheel algorithmic
composition techniques common. problem compounded fact initiatives algorithmic composition often came artists, tended develop ad hoc
solutions, communication computer scientists difficult many cases.

3. Methods
range methodological approaches used implement algorithmic composition
notably wide, encompassing many, different methods Artificial Intelligence,
also borrowing mathematical models Complex Systems even Artificial Life.
survey structured methodology, devoting subsection one:
3.1

Grammars

3.2

Symbolic, Knowledge-Based Systems

3.3

Markov Chains

3.4

Artificial Neural Networks

3.5

Evolutionary Population-Based Methods

3.6

Self-Similarity Cellular Automata

Figure 1 summarizes taxonomy methods reviewed survey. Together,
Sections 3.1 3.2 describe work using symbolic techniques characterized
classical good old-fashioned AI. Although grammars (Section 3.1) symbolic
9. GENIAC Electric Brain, electric-mechanic machine promoted educational toy. Despite
marketed computer device, computing performed human operator.

518

fiAI Methods Algorithmic Composition

Artificial intelligence
Symbolic AI

Optimization

(Knowledge-based, Rule-based)
Sections 3.1, 3.2

Computational methods
automatic generation
music material
(not based models
human creativity)

Population-based methods
Grammars

Rule learning

Section 3.1

Section 3.2.1

Evolutionary algorithms

L-systems

Sections 3.1.2, 3.2.2, 3.4.1, 3.5

Section 3.1.1

Constraint
satisfaction

Related
methods

Automatic

Interactive

Section 3.5.1

Section 3.5.2

Section 3.2.3

Section 3.1.3

Complex systems
Case-based
reasoning

Concurrency
models

Section 3.2.4

Section 3.2.5

population-based methods
Section 3.5.3

Self-similarity
Section 3.6

Machine learning
Markov chains
Related statistical methods
Section 3.3

Cellular automata
Artificial neural networks

Section 3.6.1

Section 3.4

Figure 1: Taxonomy methods reviewed survey
knowledge-based, thus included part Section 3.2, segregated separate subsection relative historical importance algorithmic
composition. Sections 3.3 3.4 describe work using various methodologies machine
learning, Section 3.5 evolutionary algorithms populationbased optimization methods. Although methodologies described Section 3.6
really form Artificial Intelligence, included importance algorithmic composition automatic sources music material (i.e.,
depend model human creativity generating music material).
attempts systematize algorithmic composition,
taxonomies Papadopoulos Wiggins (1999) Nierhaus (2009). taxonomy
roughly similar Nierhauss, differences, including L-systems
grammars instead self-similar systems. reader may surprised find many
methods machine learning optimization missing taxonomy.
several reasons this. cases, methods subsumed others. example,
machine learning, many different methods formulated mathematical
framework artificial neural networks. cases, method used rarely,
almost always together methods. example, optimization, case
tabu search, used times context constraint satisfaction
problems (Section 3.2.3), simulated annealing, occasionally combined
constraint satisfaction, Markov processes artificial neural networks.
difficult neatly categorize existing literature algorithmic composition
hierarchical taxonomy, methods frequently hybridized, giving rise
many possible combinations. specially true evolutionary methods,
519

fiFernndez & Vico

combined almost every method. Additionally, papers considered belong different methodologies, depending selected theoretical framework10 , others unique approaches11 , complicating issue. Finally, lines methods (as rule systems, grammars Markov chains)
frequently blurred: cases, ascribing work one becomes, end,
largely arbitrary exercise depending terminology, intentions domain
researchers. method presented separately (but also presenting existing hybridizations methods), describing state art mostly chronological
order method.
Although classification fully comprehensive, found one (arguably
remote) example using method related ones listed above: Amiot et al.
(2006), applied Discrete Fourier Transform (DFT) generate variations musical
rhythms. Given rhythm sequence numerical symbols, represented
frequency domain computing DFT. Variations rhythm generated
slightly perturbing coefficients transform converting back time domain.
3.1 Grammars Related Methods
broad terms, formal grammar may defined set rules expand high-level
symbols detailed sequences symbols (words) representing elements formal
languages. Words generated repeatedly applying rewriting rules, sequence
so-called derivation steps. way, grammars suited represent systems hierarchical structure, reflected recursive application rules. hierarchical
structures recognized styles music, hardly surprising formal
grammar theory applied analyze compose music long time12 , despite recurring concerns grammars fail capture internal coherency subtleties
required music composition (Moorer, 1972).
compose music using formal grammars, important step define set
rules grammar, drive generative process. rules traditionally
multi-layered, defining several subsets (maybe even separated distinct grammars) rules
different phases composition process: general themes composition,
arrangement individual notes. early authors derived rules hand
principles grounded music theory, methods possible, like examining corpus pre-existing musical compositions distill grammar able generate compositions
general style corpus, using evolutionary algorithms. Another important
aspect mapping formal grammar musical objects generates, usually relates symbols derived sequences elements music
composition, notes, chords melodic lines. However, mappings possible,
using derivation tree define different aspects musical composition. Another
important aspect automatic composition process election grammatical
10. example, Markov chains formulated stochastic grammars; self-similar systems
characterized L-system grammars; rule learning case-based reasoning also machine learning
methods; etc.
11. example, Kohonens method (Kohonen et al., 1991), neither grammatical neural
Markovian, framed either way, according creator.
12. See, e.g., survey Roads (1979).

520

fiAI Methods Algorithmic Composition

Reference

Composition task

Comments

Lidov & Gabura, 1973

melody

early proposal

Rader, 1974

melody

early proposal,
detailed grammar

Ulrich, 1977

jazz chord identification

integrated ad hoc system
(to produce jazz improvisations)

Baroni & Jacoboni, 1978

grammar Bach chorales

early proposal

Leach & Fitch, 1995
(XComposer)

structure, rhythm melody

uses chaotic non-linear systems
(self-similarity)

Hamanaka et al., 2008

generate variations two melodies
(by altering derivation tree)

inspired Lerdahl et al.s (1983)
GTTM

Roads, 1977

structure, rhythm melody

grammar compiler

Holtzman, 1981

structure, rhythm melody

grammar compiler

Jones, 1980

structure

space grammars
(uses derivation tree)

Bel, 1992
(Bol Processor)

improvisation tabla rhythms

tool field research

Kippen & Bel, 1989

improvisation tabla rhythms

grammatical inference

Cruz-Alczar & Vidal-Ruiz,
1998

melody

grammatical inference

Gillick et al., 2009

jazz improvisation

grammatical inference.
Implemented extension
Keller Morrisons (2007)
ImprovGenerator

Kitani & Koike, 2010
(ImprovGenerator)

real-time drum rhythm improvisation

online grammatical inference

Keller & Morrison, 2007
(Impro-Visor)

jazz improvisation

sophisticated GUI interface

Quick, 2010

classical three-voice counterpoint

integrated Schenkerian
framework

Chemillier, 2004

jazz chord sequences

implemented OpenMusic
MAX

Table 1: References Section 3.1 (algorithmic composition grammars), order
appearance.

rules applied. many approaches possible, use activation probabilities
rules (stochastic grammars) common. process compiling information
survey, noted almost research done regular
context-free grammars, context-sensitive general grammars seem
difficult implement effectively, except simple toy systems.
Lidov Gabura (1973) implemented early example formal grammar compose simple rhythms. Another early example implemented Rader (1974): defined
grammar hand rather simple music concepts, enriching rules grammar
activation probabilities. early examples used grammars driven rules
music theories, either small part synthesis engine, Ulrichs (1977) grammar
521

fiFernndez & Vico

enumerating jazz chords, inferring rules classical works, Baroni
Jacobonis (1978) grammar generate melodies. Generative Theory Tonal Music
(Lerdahl et al., 1983), book presenting grammatical analysis tonal music, relatively early theoretical work said influenced use grammars
algorithmic composition, though directly concerned algorithmic composition,
grammatical approach analysis music. book widely popular, lasting impact field high citation rates. Examples later
work inspired book include Popes (1991) T-R Trees, Leach Fitchs (1995)
event trees, Hamanaka et al.s (2008) melody morphing.
1980s, proposed approaches line computer science, abstracting
process generate grammars instead codifying hand, though cost
producing less interesting compositions. Roads (1977) proposed framework define,
process use grammars compose music, Holtzman (1981) described language
define music grammars automatically compose music them. Meanwhile, Jones
(1980) proposed concept space grammars, conjunction novel mapping
technique: instead using terminal symbols building blocks composition,
used derivation tree terminal sequence define characteristics
composition. approach unfortunately developed far enough yield significant
results. spite early efforts, research grammatical representations
music focused analysis rather synthesis. instances, Steedmans
(1984) influential grammar analysis jazz chord progressions, later adapted
synthesis (see below).
problem grammatical approach algorithmic composition difficulty
manually define set grammatical rules produce good compositions. problem
solved generating rules grammar (and way applied) automatically. example, although Bel (1992) implemented BOL processor facilitate
creation hand less sophisticated music grammars13 , also explored
automated inference regular grammars (Kippen & Bel, 1989). Later, Cruz-Alczar
Vidal-Ruiz (1998) implemented several methods grammatical inference: analyze corpus pre-existing classical music compositions, represented suitable set symbols,
inducing stochastic regular grammars (Markov chains) able parse compositions
corpus, finally applying grammars generate new compositions
similar style compositions corpus. Gillick et al. (2009) used similar
approach (also Markovian) synthesize jazz solos, elaborated synthesis
phase. Kitani Koike (2010) provide another example grammatical inference,
case used real-time improvised accompaniment.
However, others still designed grammars hand, carefully choosing mapping terminal symbols musical objects, Keller Morrison (2007)
jazz improvisations. Another approach take pre-existing music theory
strong hierarchical methodology, designing grammar inspired Schenkerian analysis
(Quick, 2010), using Lerdhals grammatical analysis derive new compositions two
previously existing ones altering derivation tree (Hamanaka et al., 2008), even de13. Initially represent analyze informal knowledge Indian tabla drumming, later also
represent music styles.

522

fiAI Methods Algorithmic Composition

Reference

Composition task

Comments

Prusinkiewicz, 1986

melody

mapping turtle graphics music scores

Nelson, 1996

melody

mapping turtle graphics music scores

Mason & Saffle, 1994

melody (counterpoint suggested)

mapping turtle graphics music scores

Soddell & Soddell, 2000

aural representations
biological data

L-system modulates pitch intervals

Morgan, 2007

composition large
instrumental ensemble

ad hoc symbolic mapping

Langston, 1989

melody

L-system interpreted arrange
pre-specified fragments

Worth & Stepney, 2005

melody

several mappings L-system types

Manousakis, 2006

melody (sound synthesis)

complex, multi-dimensional mapping.
Implemented MAX.

McCormack, 1996

melody, polyphonies

contex-sensitive L-systems

DuBois, 2003

real-time accompaniment

implemented MAX

Wilson, 2009

melody

mapping turtle graphics music scores

McGuire, 2006

arpeggiator

simple symbolic mapping

Watson, 2008

base chord progression

L-systems used context
larger, multi-stage system

Gogins, 2006

voice leading

Musical theory (pitch spaces).
Implemented Csound

Bulley & Jones, 2011

arpeggiator

part real-time art installation.
Implemented MAX

Pestana, 2012

real-time accompaniment

implemented MAX

Table 2: References Section 3.1.1, order appearance.

veloping jazz on-the-fly improviser (Chemillier, 2004) adapting Steedmans grammar,
previously implemented analysis purposes.
3.1.1 L-Systems
Lindenmayer Systems, commonly abbreviated L-systems, specific variant formal
grammar, whose distinctive feature parallel rewriting, i.e., derivation step,
one possible rewriting rules applied once. successfully
applied different scenarios, specially model microbial, fungi plant growth
shapes, particularly well-suited represent hierarchical self-similarity
characteristic organisms. ability represent self-similar structures, together
fact L-systems easier understand apply traditional formal
grammars, made L-systems fairly popular algorithmic composition.
Arguably, visually stunning way use L-systems synthesis
2D 3D renderings plants, using mapping sequences symbols graphics
based turtle graphics (Prusinkiewicz & Lindenmayer, 1990). natural
first application L-systems algorithmic composition used turtle graphics render
image interpreted musical score (Prusinkiewicz, 1986), mapping
523

fiFernndez & Vico

coordinates, angles edge lengths musical objects. approach used
music composers, Nelsons (1996) Summer Song Mason Saffles (1994) idea
using different rotations stretchings image implement counterpoint.
funny side note, Soddell Soddell (2000) generated aural renditions biological
L-system models, explore new ways understand them. Additionally, composers
used new approaches dependent upon graphical interpretation L-systems,
Morgans (2007) symbolic mapping. One popular pre-generate collection
short fragments and/or musical objects, define algorithm interpret
final sequence symbols instructions transform arrange fragments
composition. approach used Langston (1989) Kyburz (Supper, 2001),
Edwards (2011) used convoluted ultimately similar mapping.
However, two approaches (the graphics-to-music pre-generated sequences)
scratch surface technical possibilities generate music L-systems; many
mappings possible (Worth & Stepney, 2005). cases, mappings
become exceedingly complex, implementation Manousakis (2006), whose
L-systems drove multidimensional automata whose trajectory interpreted
music. composers researchers experimented context-free L-systems,
McCormack (1996, 2003a) used context-sensitive, parametric L-systems increase
expressiveness compositions enable implementation polyphony. also
used rich comprehensive mapping symbol sequence musical score,
interpreting symbols sequence instructions modulate parameters
automata driving MIDI synthesizer, though grammars ultimately specified
hand. DuBois (2003) used simpler also rich approach, mapping symbols
elemental musical objects (as notes instruments) simple transformations applied
them, using brackets encode polyphony. also used L-systems drive real-time
synthetic accompaniment, extracting features audio signal performer (as
pitch loudness notes), encoding symbols expanded L-system
rules, using resulting symbol sequences drive MIDI synthesizers. spite
developments, new mappings based images rendered turtle method still
investigated (Wilson, 2009).
L-systems also used implement tools assist compositional process
solving part it, generating complex arpeggios off-the-shelf arpeggiators
(McGuire, 2006), providing base chord progression composition (Watson,
2008), sometimes applying elements music theory implement rules (Gogins, 2006).
Another area research implementation real-time improvisers, either limited
parts composition process (Bulley & Jones, 2011), accompaniment (Pestana,
2012).
3.1.2 Grammars Evolutionary Algorithms
Evolutionary methods also used together grammars. case, common
approach evolve grammatical rules, GeNotator (Thywissen, 1999),
genomes grammars specified GUI fitness function interactive
(the user assigns fitness grammars). exotic example Khalifa et al.
524

fiAI Methods Algorithmic Composition

Reference

Composition task

Comments

Thywissen, 1999
(GeNotator)

structure

grammar genotype
interactive evolutionary algorithm

Khalifa et al., 2007

melody

grammar part fitness function

Ortega et al., 2002

melody

grammatical evolution

Reddin et al., 2009

melody

grammatical evolution

Shao et al., 2010
(Jive)

melody

interactive grammatical evolution

Bryden, 2006

melody

interactive evolutionary algorithm L-systems

Fox, 2006

melody

interactive evolutionary algorithm L-systems

Peck, 2011

melody

evolutionary algorithm L-systems

Dalhoum et al., 2008

melody

grammatical evolution L-systems

Table 3: References Section 3.1.2, order appearance.
(2007) uses grammar part fitness function instead generation
compositions.
evolutionary methods specifically adapted handle grammars.
case grammatical evolution, method genomes sequences numbers
symbols controlling application rules pre-defined (and possibly stochastic)
grammar. common approach represent music output
grammar, range general specific given music style. Several
instances method developed: early, bare-bones implementation
(Ortega et al., 2002) elaborated one using simple fitness function based general concepts music theory (Reddin et al., 2009). However, approaches,
system implemented Shao et al. (2010), whose grammar used produce
intermediate code, used generate music.
general case formal grammars, evolutionary algorithms
used create L-systems. However, examples use interactive fitness function (the
fitness assigned human), like basic implementation Bryden (2006)
approach based genetic programming used Fox (2006). Others use simplistic
fitness functions, modest results (Peck, 2011). sophisticated approach
used Dalhoum et al. (2008), using grammatical evolution fitness function based
distance metric synthesized compositions pre-specified corpus compositions.
3.1.3 Related Methods
Finally, subsection presents examples exactly use grammars,
utilize similar borderline approaches.
first one application Kohonens Dynamically Expanding Context (DEC)
method algorithmic composition (Kohonen et al., 1991). DEC, set music examples
fed algorithm, infers model structure examples may
construed stochastic context-sensitive grammar. model parsimonious
possible, is, rules little contextual information possible. Then,
inferred grammar used generate new compositions. Drewes Hgbergs (2007)
525

fiFernndez & Vico

Reference

Composition task

Comments

Kohonen et al., 1991

melody

Uses Kohonens Dynamically Expanding Context

Drewes & Hgberg, 2007

generate variations
melody

applies tree-based algebraic transformations

Cope, 1992 (EMI),
2000 (SARA, ALICE),
2005 (Emily Howell)

melody

EMI uses Augmented Transition Networks

Table 4: References Section 3.1.3, order appearance.
work also borderline, using regular tree grammars generate basic scaffold
modified algebraic operations generate final music composition.
famous example category Copes (1992) Experiments Musical
Intelligence (EMI), software application able analyze set musical compositions
specific style (for example, Bachs) derive Augmented Transition Network (ATN),
i.e., finite state automaton able parse relatively complex languages. EMI applies
pattern-matching algorithms extract signatures short musical sequences characteristic
style set examples analyzed, determining use
signatures compositions style. analysis, synthesis phase generates
new music compositions comply specifications encoded inferred ATN,
quite impressive results. iterated EMIs design applications, like SARA
ALICE (Cope, 2000), ultimately tried new approach yet another application,
Emily Howell. Cope (2005) reported Emily Howell developed unique style
process trial error guided human input; however, researchers (Wiggins,
2008) disputed validity methodology.
3.2 Symbolic, Knowledge-Based Systems Related Methods
Here, knowledge-based system used umbrella term encompassing various rule-based
systems several different paradigms, common denominator representing
knowledge less structured symbols. Since knowledge musical composition
traditionally structured sets less formalized rules manipulating
musical symbols (Anders & Miranda, 2011), knowledge-based rule systems come
natural way implement algorithmic composition. fact, extremely common
algorithmic composition systems include kind composition rules point
workflow. known early work algorithmic composition example:
classical rules counterpoint used generation first second movements
Illiac Suite (Hiller & Isaacson, 1958). this, subsection mostly
confined description systems strong foundations AI (as expert systems),
sidestepping certain degree works composers difficult categorize,
ad hoc nature approaches different language use.
Starting exposition early work, Gills (1963) paper, already cited Section 2.1, presented first application classical AI heuristics algorithmic composition: used hierarchical search backtracking guide set compositional rules
Schoenbergs twelve-tone technique. Another notable example Rothgebs (1968)
Ph.D. thesis: encoded SNOBOL set rules extracted eighteenth century
526

fiAI Methods Algorithmic Composition

Reference

Composition task

Comments

Gill, 1963

Schoenbergs twelve-tone
technique

hierarchical search backtracking

Rothgeb, 1968

unfigured bass

implemented SNOBOL

Thomas, 1985
(Vivace)

four-part harmonization

implemented LISP

Thomas et al., 1989
(Cantabile)

Indian raga style

implemented LISP

Steels, 1986

four-part harmonization

uses Minskys frames

Riecken, 1998
(Wolfgang)

melody

uses Minskys SOM

Horowitz, 1995

jazz improvisation

uses Minskys SOM

Fry, 1984
(Flavors Band)

jazz improvisation
styles

phrase processing networks (networks
agents encoding musical knowledge)

Gjerdingen, 1988
(Praeneste)

species counterpoint

implements theory composers work

Schottstaedt, 1989

species counterpoint

constraint-based search backtracking

Lthe, 1999

piano minuets

set rules extracted classical textbook

Ulrich, 1977

jazz improvisation

also uses grammar (for jazz chords)

Levitt, 1981

jazz improvisation

Criticized Horowitz (1995)
overly primitive

Hirata & Aoyagi, 1988

jazz improvisation

uses logic programming

Rowe, 1992
(Cypher)

interactive jazz
improvisation

uses Minskys SOM

Walker, 1994
(ImprovisationBuilder)

interactive jazz
improvisation

implemented SmallTalk

Ames & Domino, 1992
(Cybernetic Composer)

jazz, rock

also uses Markov chains rhythm

Table 5: References Section 3.2, order appearance.
music treatises harmonize unfigured bass, say, determine adequate chords
sequence bass notes.14 discovered classical rules incomplete
incoherent certain extent.
recurring problems many others implementing rules composition
straight musical theory. example, Thomas (1985) designed rule-based system
four-part chorale harmonization implemented Lisp15 , intent clarifying
musical rules taught students. Later, designed another rule system (Thomas
et al., 1989) simple melody generation Indian raga style. Another example
harmonization use Minskys paradigm frames one students encode
set constraints solve relatively simple problem tonal harmony, finding passing chord two others (Steels, 1979), later tackle problem four-part
harmonization (Steels, 1986). Minsky developed paradigms, K-lines
14. noted stems practice completely specifying harmonization,
problem performers expected solve improvisation.
15. systems discussed paragraph implemented Lisp.

527

fiFernndez & Vico

Reference

Composition task

Comments

Schwanauer, 1993
(MUSE)

four-part
harmonization

presents learning techniques similar way
Roads (1985, sect. 8.2)

Widmer, 1992

harmonization

based user evaluations training corpus

Spangler, 1999

real-time four-part
harmonization

prioritizes harmonic errors severity,
order refine results

Morales & Morales, 1995

species counterpoint

uses logic programming

Table 6: References Section 3.2.1, order appearance.
Society Mind (SOM), also influenced work algorithmic composition two
students, Riecken Horowitz. Riecken (1998) used system composed monophonic melodies according user-specified emotional criteria, Horowitz
(1995) used system improvised jazz solos. Frys (1984) phrase processing
networks, directly based SOM, specialized procedural representations
networks agents implementing musical transformations encode knowledge jazz
improvisation styles.
researchers also explored different ways generate species counterpoint
rule-based systems: Gjerdingen (1988) implemented system based use
several pre-specified musical schemata, implementing theory composers work,
Schottstaedt (1989) used formal approach: constraint-based search
backtracking. followed classical rulebook species counterpoint, point
bending rules creating new ones order get close possible scores
serving examples book. Also formal side, Lthe (1999) extracted set
rules classical textbook composing minuets.
music styles demanded different approaches: jazz performances improvisations existing melodies, knowledge-based systems jazz structured
less sophisticated analysis-synthesis engines. example, work Ulrich (1977):
system analyzed melody fitted harmonic structure. Another student Minsky
(Levitt, 1981) implemented rule-based jazz improviser formulating rules
constraints, Hirata Aoyagi (1988) encoded rules logic programming, trying
design flexible system. Rowe (1992) used SOM architecture16 Cypher,
analysis-synthesis engine able play jazz interactively human performer, notable
flexibility musical knowledge encoded it. Also, Walker (1994) implemented
object-oriented analysis-synthesis engine able play jazz interactively human
performer, Ames Domino (1992) implemented hybrid system (using rules
Markov chains) generation music several popular genres.
3.2.1 Rule Learning
knowledge implemented rule-based systems usually static, part knowledge may dynamically changed learned. natural term concept machine
learning, meaning unfortunately vague, used catch-all many
methods, including neural networks Markov chains.
16. student Minsky, though.

528

fiAI Methods Algorithmic Composition

examples rule-based learning systems developed. example,
Schwanauer (1993) implemented MUSE, rule-based system solving several tasks fourpart harmonization. core ruleset static, series constraints directives
composition process also built system, application also
used dynamically change rule priorities. Additionally, system successfully
solved task, able deduce new composite rules extracting patterns rule
application. Widmer (1992) implemented another example: system harmonization
simple melodies. based user evaluations training corpus: hierarchical
analysis training melodies evaluations, extracted rules harmonization.
Spangler (1999) implemented system generating rule systems harmonizing fourpart chorales style Bach, constraint harmonization real
time. rulesets generated analyzing databases examples algorithms
applied formal concepts information theory distilling rules, violations
harmonic rules prioritized order refine results. Using framework logic
programming, Morales Morales (1995) designed system learned rules classical
counterpoint musical examples rule templates.
3.2.2 Rule-Based Methods Evolutionary Algorithms
intuitive way hybridize rule-based knowledge systems evolutionary algorithms craft fitness function ruleset. done efficiently domains
whose rules adequately codified, compliance rules expressed
graduated scale, instead binary (yes/no) compliance.
good example four-part baroque harmonization pre-specified melody,
lends particularly well approach. McIntyre (1994) extracted set rules
performing harmonization classical works, codified set scoring
functions. fitness weighted sum scores, tiered structure:
scores added unless specific scores values thresholds (because critical prerequisites produce good harmonizations). slightly
different approach used Horner Ayers (1995): defined two classes rules:
one defining acceptable voicings individual chords, used enumerate possible
voicings, another defining voices allowed change successive chords. evolutionary algorithm used find music compositions, whose search
space constructed enumeration voicings (first class rules). fitness
candidate solution simply amount violated rules second class.
Phon-Amnuaisuk et al. (1999) also four-part harmonization using set rules build
fitness function musical knowledge design genotype mutation
crossover operators, lack global considerations fitness function led modest results. contrast, Maddox Otten (2000) got good results implementing system
similar McIntyres (1994), using flexible representation, resulting
larger search space possible individuals, without tiered structure fitness
function, enabling less constrained search process.
Another good example species counterpoint: Polito et al. (1997) extracted rules
species counterpoint classic eighteenth century music treatise, using define
fitness functions multi-agent genetic programming system: agent performed
529

fiFernndez & Vico

Reference

Composition task

Comments

McIntyre, 1994

four-part harmonization

explores several schemes combine
rules fitness function

Horner & Ayers, 1995

four-part harmonization

two stages: enumeration possible
chord voicings, evolutionary algorithm
voice-leading rules

Phon-Amnuaisuk et al., 1999

four-part harmonization

criticizes vanilla evolutionary algorithms
generating unstructured
harmonizations

Maddox & Otten, 2000

four-part harmonization

similar McIntyres (1994)

Polito et al., 1997

species counterpoint

multi-agent genetic programming system

Gwee, 2002

species counterpoint

fuzzy rules

Table 7: References Section 3.2.2, order appearance.
set composition transformation operations given melody specified seed,
cooperated produce composition. Gwee (2002) exhaustively studied
computational complexity problems related generation species counterpoint
rulesets, implemented evolutionary algorithm whose fitness function based
set fuzzy rules (although also experimented trained artificial neural networks
fitness functions).
3.2.3 Constraint Satisfaction
Gradually (in process spanned 1980s 1990s), researchers algorithmic
composition rule-based systems adopted formal techniques based logic programming. example, Boenn et al. (2008) used answer set programming encode rules
melodic composition harmonization. However, work logic programming
different paradigm: formulation algorithmic composition tasks
constraint satisfaction problems (CSPs). Previously referenced work, Steelss (1979),
Levitts (1981), Schottstaedts (1989) Lthes (1999) seen part gradual
trend towards formulation musical problems CSPs17 , although constraint logic
programming (CLP) came tool choice solve CSPs. Good surveys CLP
algorithmic composition written Pachet Roy (2001) Anders
Miranda (2011).
Ebciolu worked many years area, achieving notable results. first work
implemented Lisp (Ebciolu, 1980), translated rules fifth-species strict counterpoint
composable Boolean functions (he add rules bring system
producing acceptable results, though), used algorithm produced exhaustive
enumeration compositions satisfying previously arranged set rules: basically,
implemented custom engine logic programming Lisp. next decade,
tackled problem writing four-part chorales style J. S. Bach. Finally,
produced CHORAL, monumental expert system (Ebciolu, 1988), distilling 350
rules guide harmonization process melody generation. keep problem
17. Gills (1963) implementation formulated CSP, somewhat primitive later standards.

530

fiAI Methods Algorithmic Composition

tractable, designed custom logic language (BSL) optimizations standard logic
languages, backjumping. system received substantial publicity, supposed
reach level talented music student, words.
Following Ebciolus work, many constraint systems implemented harmonization counterpoint. Tsang Aitken (1991) implemented CLP system using Prolog
harmonize four-part chorales. However, system grossly inefficient.18 Ovans
Davison (1992) described interactive CSP system first-species counterpoint,
human user drove search process, system constrained possible outputs
(according counterpoint rules) search progressed. took care efficiency
using arc-consistency resolution constraints. Ramrez Peralta (1998)
solved different problem: given monophonic melody, CLP system generated
chord sequence harmonize it. Phon-Amnuaisuk (2002) implemented constraint system
harmonizing chorales style J. S. Bach, innovation previous
systems: add knowledge system apply rules control
harmonization process explicitly, thus modulating search process explicit
flexible way. Anders Miranda (2009) analyzed Schoenbergs textbook theory
harmony, programming system Strasheela (see below) produce self-contained harmonic progressions, instead harmonizing pre-existing melodies, constraint
systems do.
many CLP systems implemented solve classical problems harmonization counterpoint, researchers studied application CLP techniques
different problems. simple application, Wiggins (1998) used CLP system generate short fragments serial music. Zimmermann (2001) described two-stage method,
stages used CLP: first stage (AARON) took input storyboard specify mood composition function time, generated harmonic progression
sequence directives. second (COMPOzE) generated four-part harmonization
according previously arranged progression directives; result intended
background music. Laurson Kuuskankare (2000) studied constraints instrumentation19 guitars trumpets (i.e., constraints composing music easily playable
instruments). Chemillier Truchet (2001) analyzed two CSPs: style Central African harp music, Ligeti textures. used heuristic search analyzes
instead backtracking, heralding OMClouds approach constraint programming (see
below). Sandred (2004) proposed application constraint programming rhythm.
Several general-purpose constraint programming systems algorithmic composition
proposed (i.e., languages environments program constraints). One
earliest examples Courtots (1990) CARLA, CLP system generating polyphonies visual front-end rich, extendable type system designed represent
relationships different musical concepts. Pachet Roy (1995) implemented another general-purpose musical CLP (Backtalk) object-oriented framework (MusES),
designing generator four-part harmonizations top it. key contribution
hierarchical arrangement constraints notes chords, dramatically decreasing (both cognitive computational) complexity resulting constraint system.
18. spite using 20 rules, required 70 megabytes memory harmonize phrase 11
notes.
19. say, take account way instrument played composing part.

531

fiFernndez & Vico

Reference

Composition task

Comments

Boenn et al., 2008

melody harmonization

answer set programming

Ebciolu, 1980

species counterpoint

implemented LISP

Ebciolu, 1988
(CHORAL)

four-part harmonization

implemented custom logic language (BSL)

Tsang & Aitken, 1991

four-part harmonization

inefficient

Ovans & Davison, 1992

species counterpoint

interactive search

Ramrez & Peralta, 1998

melody harmonization

simpler constraint solver

Phon-Amnuaisuk, 2002

four-part harmonization

explicit control search process

Anders & Miranda, 2009

Schoenbergs Theory
Harmony

implemented Strasheela

Wiggins, 1998

Schoenbergs twelve-tone
technique

simple demonstration

Zimmermann, 2001
(Coppelia)

structure, melody,
harmonization, rhythm

two stages: harmonic plan (Aaron)
execution (Compoze)

Laurson & Kuuskankare,
2000

guitar trumpet
instrumentation

implemented PWConstraints

Chemillier & Truchet, 2001

African harp Ligeti
textures

implemented OpenMusic

Sandred, 2004

rhythm

implemented OpenMusic

Courtot, 1990
(CARLA)

polyphony, general purpose

early general-purpose system

Pachet & Roy, 1995
(BackTalk)

four-part harmonization

implemented MusEs

Rueda et al., 1998

polyphony, general purpose

describes PWConstraints (implemented
PatchWork) Situation (implemented
OpenMusic)

Rueda et al., 2001

general purpose

describes PiCO
describes ntcc

Olarte et al., 2009

general purpose

Allombert et al., 2006

interactive improvisation

uses ntcc

Rueda et al., 2006

interactive improvisation

uses ntcc Markovian models

Pachet et al., 2011

melody

integrates Markovian models constraints

Truchet et al., 2003

general purpose

describes OMClouds

Anders, 2007

general purpose

describes Strasheela

Sandred, 2010

general purpose

describes PWMC.
Implemented PatchWork

Carpentier & Bresson, 2010

orchestration

uses multi-objective optimization
discover candidate solutions.
Interfaces OpenMusic MAX

Yilmaz & Telatar, 2010

harmonization

fuzzy logic

Aguilera et al., 2010

species counterpoint

probabilistic logic

Geis & Middendorf, 2008

four-part harmonization

multi-objective Ant Colony Optimization

Herremans & Sorensena,
2012

species counterpoint

variable neighborhood tabu search

Davismoon & Eccles, 2010

melody, rhythm

uses simulated annealing combine
constraints Markov processes

Martin et al., 2012

interactive improvisation

implemented MAX

Table 8: References Section 3.2.3, order appearance.

532

fiAI Methods Algorithmic Composition

Rueda et al. (1998) reviewed two early general-purpose systems, PWConstraints
Situation. PWConstraints able (relatively easily) handle problems polyphonic
composition subsystem (score-PMC), Situation flexible implemented optimizations search procedures. PiCO (Rueda et al., 2001)
experimental language music composition seamlessly integrated constraints,
object-oriented programming calculus concurrent processes. idea use
constraint programming specify voices composition, use concurrent
calculus harmonize them. authors also implemented visual front-end PiCO
ease use, Cordial. similar way PiCO, ntcc another language constraint
programming implemented primitives defining concurrent systems, although
specifically designed algorithmic composition. ntcc proposed generate
rhythm patterns expressive alternative PiCO (Olarte et al., 2009),
mainly used machine improvisation: Allombert et al. (2006) used
improvisation stage two-stage system (the first stage used temporal logic system
compose abstract temporal relationships musical objects, ntcc stage
generated concrete music realizations), Rueda et al. (2006) used ntcc implement
real-time system learned Markovian model (using Factor Oracle) musicians concurrently applied generate improvisations. related ntcc, Pachet
et al. (2011) also proposed framework combine constraint satisfaction Markov
processes.
OMClouds (Truchet et al., 2003) another general-purpose (but purely visual) constraint system composition, implementation set apart formal
systems: internally, constraints translated cost functions. Instead optimized tree search backtracking usual CLP, adaptive tabu search performed,
seeking minimize solution minimal cost. avoids problems inherent
constraint programming, overconstraining, cannot guaranteed completely navigate search space. Anders (2007) implemented Strasheela, system
expressly designed highly flexible programmable, aiming overcome perceived limitation previous general-purpose systems: difficulty implement complex
constraints related multiple aspects compositions process. Finally, another
purely visual constraint system, PWMC, proposed Sandred (2010) overcome perceived limitations score-PMC. able handle constraints concerning pitch
structure score-PMC, also rhythm metric structure.
stressed that, CLP become tool choice solve CSPs,
approaches also used. Previously cited OMClouds one these. Carpentier
Bresson (2010) implemented mixed system orchestration worked curious way: user fed system target sound set symbolic constraints;
multi-objective evolutionary algorithm found set orchestration solutions matching
target sound, local search algorithm filtered solutions complying
constraints. Yilmaz Telatar (2010) implemented system simple constraint
harmonization fuzzy logic, Aguilera et al. (2010) used probabilistic logic solve
first-species counterpoint. exotic solutions proposed, use Ant
Colony Optimization multi-objective approach solve constraints Baroque
harmonization (Geis & Middendorf, 2008), variable neighborhood tabu search solve
soft constraints first-species counterpoint (Herremans & Sorensena, 2012), simulated
533

fiFernndez & Vico

Reference

Composition task

Comments

Pereira et al., 1997

Baroque music

hierarchical analysis representation

Ribeiro et al., 2001
(MuzaCazUza)

Baroque music

generates melody harmonic line

Ramalho & Ganascia, 1994

jazz improvisation

uses rule-based system analysis

Parikh, 2003

jazz improvisation

also uses rule system analize music

Eigenfeldt & Pasquier, 2010

jazz chord progressions

also uses Markovian models

Sabater et al., 1998

harmonization

also uses rule system

Table 9: References Section 3.2.4, order appearance.
annealing combine constraints Markov processes (Davismoon & Eccles, 2010). Finally, Martin et al. (2012) presented even exotic approach: real-time music
performer reacted environment. aspects music
controlled Markov chains, others expressed CSP. solve CSP real
time, solution calculated random (but quickly) using binary decision diagrams.
3.2.4 Case-Based Reasoning
Case-based reasoning (CBR) another formal framework rule-based systems.
CBR paradigm, system database cases, defined instances
problem corresponding solutions. Usually, case also contains structured
knowledge problem solved case. faced new problem,
system matches case database. Unless new problem identical
one recorded case database, system select case similar new
problem, adapt corresponding solution new problem. new solution
deemed appropriate, new case (recording new problem along new solution)
may included database.
Several researchers used CBR algorithmic composition. Pereira et al. (1997)
implemented system generated case database three Baroque music
pieces, analyzed hierarchical structures; cases nodes.
system composed soprano melodic line piece, searching similar cases
case database. results comparable output first-year student, according
music experts consulted authors. intersecting set researchers implemented
simpler CBR composing system (Ribeiro et al., 2001) generated melody
harmonic line, time case database generated six Baroque pieces. Cases
represented different way, tough: case represented rhythm, melody
attributes associated chord given context. generate new music piece,
harmonic line specified, system fleshed music piece matching
cases harmonic line.
Hybrid systems also proposed. Ramalho Ganascia (1994) proposed jazz
improviser used rule system analyze incoming events (for example, ongoing
sequence chords) CBR engine improvise. case database assembled
extracting patterns transcriptions jazz recordings, consisted descriptions
contexts play contexts. improvisation, current context
534

fiAI Methods Algorithmic Composition

Reference

Composition task

Comments

Haus & Sametti, 1991
(Scoresynth)

melody

Petri nets

Lyon, 1995

melody

Petri nets encode Markov chains

Holm, 1990

melody, sound synthesis

inspired CSP process algebra

Ross, 1995
(MWSCCS)

melody

custom process algebra

Rueda et al., 2001

general purpose

describes PiCO

Allombert et al. (2006)

interactive improvisation

uses ntcc

Rueda et al., 2006

interactive improvisation

uses ntcc Markovian models

(Olarte et al., 2009)

general purpose, rhythms

describes ntcc

Table 10: References Section 3.2.5, order appearance.

analyzed rule system, cases applying current context
extracted database combined determine output improviser. Considering Ramalho Ganascias system inflexible, Parikh (2003) implemented
another jazz improviser, intending use large case database containing jazz fragments
various sources, order get system style own. Eigenfeldt
Pasquier (2010) used case-based system generate variable-order Markov models
jazz chord progressions.
Outside jazz domain, hybrid system harmonizing melodies popular songs
implemented Sabater et al. (1998): given melody, system sequentially decided
chords harmonize it. CBR module failed match case, system fell back
simple heuristic rule system select appropriate chord. harmonized output
added case database, CBR module gradually learned time rule
system.
3.2.5 Concurrency Models
Concurrency models described formal languages specify, model and/or reason
distributed systems. provide primitives precisely define semantics
interaction synchronization several entities. main application
modeling designing distributed concurrent computer systems, also
used languages partially fully model composition process, music
composition formulated endeavor carefully synchronize streams music
events produced several interacting entities. Concerning algorithmic composition,
used concurrency models Petri nets several kinds process algebras,
also known process calculi. Detailed descriptions models beyond scope
survey; see example Reisigs (1998) book Petri nets Baetens (2005) survey
process algebras information.
Petri nets used basis Scoresynth (Haus & Sametti, 1991), visual
framework algorithmic composition Petri nets used describe transformations musical objects (sequences notes musical attributes), synchronization
535

fiFernndez & Vico

musical objects implicit structure net. Petri nets also
used efficient compact way implement Markov chains (Lyon, 1995).
Process algebras first used algorithmic composition Holm (1990), although
model (inspired Hoares algebra, CSP) geared towards sound synthesis
music composition. proper example Rosss (1995) MWSCCS, extension (adding concepts music composition) previously existing algebra (WSCCS).
Specifications algorithmic composition written MWSCCS meant resemble
grammatical specifications, richer expressive power. Later examples also
cited Section 3.2.3, PiCO language (Rueda et al., 2001), integrated
logical constraints process algebra. Also cited Section, ntcc process algebra used implement machine improvisation (Allombert et al., 2006)
drive Markovian model (using Factor Oracle) real-time machine learning
improvisation (Rueda et al., 2006). also proposed generate rhythm patterns,
expressive alternative PiCO (Olarte et al., 2009).
3.3 Markov Chains Related Methods
Conceptually, Markov chain simple idea: stochastic process, transiting discrete
time steps finite (or countable) set states, without memory: next
state depends current state, sequence states preceded
time step. simplest incarnations, Markov chains represented
labeled directed graphs: nodes represent states, edges represent possible transitions,
edge weights represent probability transition states. However, Markov chains
commonly represented probability matrices.
Markov chains applied music composition, probability matrices may
either induced corpus pre-existing compositions (training), derived hand
music theory trial-and-error. former common way use
research, latter used software tools composers. important design
decision map states Markov chain musical objects. simplest
(but fairly common) mapping assigns sequential group notes state,
choice one note (instead larger sequence) fairly common.
also common extend consideration current state: n-th order
Markov chain, next state depends last n states, last one.
consequence, probability matrix n + 1 dimensions. algorithmic composition,
Markov chains mostly used generative devices (generating sequence states),
also used analysis tools (evaluating probability sequence states).
latter case, term n-gram also used, though strictly speaking refers
sequence N states.
Markov chains popular method early years algorithmic composition.
Early examples already reviewed Section 2.1; additionally, Ames (1989) also
provides good survey. However, Markov chains generated corpus pre-existing
compositions captured local statistical similarities, limitations soon became
apparent (Moorer, 1972): low-n Markov chains produced strange, unmusical compositions
wandered aimlessly, high-n ones essentially rehashed musical segments
corpus also computationally expensive train.
536

fiAI Methods Algorithmic Composition

Reference

Composition task

Comments

Tipei, 1975

melody

Markov chains part larger,
ad hoc system

Jones, 1981

melody

simple introduction composers

Langston, 1989

melody

dynamic weights

North, 1991

melody

Markov chains part larger,
ad hoc system

Ames & Domino, 1992
(Cybernetic Composer)

jazz, rock

uses Markov chains rhythms

Visell, 2004

real-time generative art
installation

Liberal use concept HMM.
Implemented MAX

Zicarelli, 1987
(M Jam Factory)

interactive improvisation

commercial GUI applications
alternative representation transition
matrices. Implemented athenaCL

Ariza, 2006
Ponsford et al., 1999

composing sarabande pieces

adds symbols expose structure
pieces

Lyon, 1995

melody

implements Markov chains
Petri nets

Verbeurgt et al., 2004

melody

two stages: Markovian model
artificial neural network

Thom, 2000
(BoB)

interactive jazz improvisation

statistical machine learning

Lo & Lucas, 2006

melody

evolutionary algorithm, Markov chains
used fitness function

Werner & Todd, 1997

melody

co-evolutionary algorithm, Markov
chains used evolvable fitness functions

Thornton, 2009

melody

grammar-like hierarchy
Markov models

Cruz-Alczar Vidal-Ruiz
(1998)

melody

analysis grammatical inference,
generation Markovian models

Gillick et al. (2009)

jazz improvisation

analysis grammatical inference,
generation Markovian models

Eigenfeldt & Pasquier, 2010

jazz chord progressions

uses case-based reasoning

Davismoon & Eccles, 2010

melody, rhythm

uses simulated annealing combine
constraints Markov processes

Pachet et al., 2011

melody

integrates Markovian models
constraints

Grachten, 2001

jazz improvisation

integrates Markovian models
constraints

Manaris et al., 2011

interactive melody improvisation

Markov chains generate candidates
evolutionary algorithm

Wooller & Brown, 2005

transitioning two melodies

alternates Markov chains
two melodies

Table 11: References Section 3.3, order appearance.
this, Markov chains came seen source raw material, instead
method truly compose music automated way, except specialized tasks
537

fiFernndez & Vico

rhythm selection (McAlpine et al., 1999). Therefore, research interest Markov
chains receded subsequent years limitations became apparent methods
developed, remained popular among composers. However, citing even relevant
subset works composers use Markov chains part compositional
process would inflate reference list beyond reasonable length. typical examples
Markov chains used composers (sometimes part larger automatic compositional
framework software system) papers Tipei (1975), Jones (1981), Langston (1989,
although used dynamically computed weights), North (1991) Ames Domino
(1992). noted composers sometimes deconstruct formal methods adapt
purposes, Visell (2004) used concept Hidden Markov
Model (described below) implement manually-tuned real-time generative art system.
addition, many software suites use Markov chains provide musical ideas composers, even probabilities specified hand instead generated corpus.
Ariza (2006) gives compact list software suites experimental programs using Markov
chains, thesis (Ariza, 2005b) provides comprehensive view field. Much
done usability field, using GUI interfaces (Zicarelli, 1987), also
developing effective ways encode probability matrices, example compact
string specifications (Ariza, 2006).
However, novel research Markov chains algorithmic compositions still
carried several ways. example, Ponsford et al. (1999) used corpus sarabande
pieces (relatively simple dance music) generate new compositions using Markov models20 ,
pre-processing stage automatically annotate compositions corpus
symbols make explicit structure, post-processing stage using template
constrain structure synthesized composition, order generate minimally
acceptable results. Another way hybridization Markov chains methods.
example, Lyon (1995) used Petri nets efficient compact way implement
Markov chains, Verbeurgt et al. (2004) used Markov chains21 generate basic
pattern melody, refined artificial neural network.
BoB system (Thom, 2000), Markov chains trained statistical learning: set
jazz solos, statistical signatures extracted pitches, melodic intervals contours.
Then, signatures used define transition probabilities Markov chain
whose output sampled generate acceptable solos. Lo Lucas (2006) trained
Markov chains classic music pieces, but, instead generating compositions them,
used fitness evaluators evolutionary algorithm evolve melodies encoded
sequences pitches. Werner Todd (1997) also used Markov chains evaluate
simple (32-note) melodies, particularity chains also
subject evolution, investigate sexual evolutionary dynamics. Thornton (2009) defined
set grammar-like rules existing composition, inferring hierarchy Markov
models use statistical patterns analyzed composition multiple levels. already
mentioned Section 3.1, Cruz-Alczar Vidal-Ruiz (1998) Gillick et al. (2009) used
grammatical inference Markovian models. Regarding symbolic methods, Eigenfeldt
Pasquier (2010) used case-based system generate Markov processes jazz chord
20. work commonly cited literature grammatical, methodology thoroughly
statistical.
21. Also reviewed Section 3.4.

538

fiAI Methods Algorithmic Composition

progressions, (Davismoon & Eccles, 2010) used simulated annealing combine constraints
Markov processes, Pachet et al. (2011) proposed framework combine Markovian
generation music rules (constraints) produce better results.
Additionally, Markov chains remained feasible option restricted problems (for example, real-time performances, jazz improvisation), limitations less apparent
cases generation whole compositions. example, Grachten (2001)
developed jazz improviser Markov chains generated duration pitches,
system constraints refined output, pre-defined licks (short musical patterns)
inserted appropriate times. Manaris et al. (2011) also implemented improviser, using
Markov model trained user input generate population candidate melodies, feeding evolutionary algorithm, whose fitness function rewarded melodies whose
metrics similar user inputs metrics. different (but also restricted) problem
studied Wooller Brown (2005): applying Markov chains generate musical
transitions (morphings) two different pieces simple application non-linear
music, stochastically alternating two Markov chains, one trained one
pieces.
3.3.1 Related Methods
sophisticated Markovian models (and related statistical methods; see survey
Conklin, 2003) also applied algorithmic composition, Pachets (2002)
Continuator, real-time interactive music system. Continuator departs common
Markov chain implementations uses variable-order (also known mixed-order)
Markov chains22 , constrained fixed n value, used get best
low high-n chains. Conklin Witten (1995) implemented sophisticated variableorder scheme23 , whose main feature consideration parallel multiple viewpoints
sequences events compositions (for example, pitches, durations, contours, etc.),
instead integrating unique sequence symbols, common
implementations Markov chains. Variable-order Markov chains also used
part larger real-time music accompaniment system (Martin et al., 2012). variableorder schemes used algorithmic composition, formulated machine learning framework,
Prediction Suffix Trees (PSTs, Dubnov et al., 2003), space-efficient structures like
Factor Oracles 24 (Assayag & Dubnov, 2004), Multiattribute Prediction Suffix Graphs
(MPSGs, Trivio Rodrguez & Morales-Bueno, 2001), considered extension
PSTs consider multiple viewpoints Conklin Wittens work. Sastry (2011)
also used multiple viewpoints PSTs modelize Indian tabla compositions, though
model could also used generate new compositions.
Hidden Markov Models (HMMs) also generalizations Markov chains
used algorithmic composition. HMM Markov chain whose state unobservable, state-dependent output visible. Training HMM involves
22. noted Kohonens method (Kohonen et al., 1991), reviewed Section 3.1.3, similar
(in ways) variable-order chains.
23. Conklin Wittens method also described grammatical, included
emphasis formal statistical analysis.
24. Also implemented concurrent constraint paradigm Rueda et al. (2006). See Section 3.2.3
Section 3.2.5 details.

539

fiFernndez & Vico

Reference

Composition task

Comments

Pachet, 2002
(Continuator)

interactive improvisation

variable-order

Conklin & Witten, 1995

Bach chorales

multiple viewpoint systems

Martin et al., 2012

interactive improvisation

variable-order; implemented MAX

Dubnov et al., 2003

melody

Prediction Suffix Trees.
Implemented OpenMusic

Rueda et al., 2006

interactive improvisation

uses ntcc Factor Oracles

Assayag & Dubnov, 2004

melody

Factor Oracles.
Implemented OpenMusic

Trivio Rodrguez &
Morales-Bueno, 2001

melody

Multiattribute Prediction Suffix Graphs

Sastry, 2011

improvisation tabla rhythms

multiple viewpoints Prediction
Suffix Trees.
Implemented MAX

Farbood & Schoner, 2001

species counterpoint

Hidden Markov Models

Biyikoglu, 2003

four-part harmonization

Hidden Markov Models

Allan, 2002

four-part harmonization

Hidden Markov Models

Morris et al., 2008
(SongSmith)

melody harmonization

Hidden Markov Models

Schulze, 2009
(SuperWillow)

melody, rhythm, two-voice
harmonization

Hidden Markov Models
Prediction Suffix Trees

Yi & Goldsmith, 2007

four-part harmonization

Markov Decision Processes

Martin et al., 2010

interactive improvisation

Partially Observable
Markov Decision Processes

Table 12: References Section 3.3.1, order appearance.
determining matrix transition probabilities, also matrix output probabilities
(that is, state, probability possible output). Then, given sequence
outputs, possible compute likely sequence states produce sequence outputs, using Viterbi dynamic programming algorithm. way, HMMs
find globally optimized sequence states, simpler Markov methods perform local optimization. applied algorithmic composition, HMMs appropriate add
elements existing composition (most commonly, counterpoint harmonization),
given set pre-existing examples: composition modeled sequence outputs
HMM, additions computed likely sequence states
HMM.
Farbood Schoner (2001) implemented earliest example HMM algorithmic
composition: trained second-order HMM generate Palestrina-style first-species
counterpoint (the simplest way write counterpoint), defining training set rules
used teach counterpoint. related problem train HMMs set chorale
harmonizations style J.S. Bach order get Bach-like harmonizations.
problem researched Biyikoglu (2003) Allan (2002); latter divided
problem harmonization three subtasks HARMONET (Hild et al.,
1992). Microsofts SongSmith software, Morris et al. (2008) trained HMM
540

fiAI Methods Algorithmic Composition

300 lead sheets (specifications song melodies) generate chords accompany userspecified vocal melody, parametrizing resulting system intuitive interface
non-technical users. Schulze (2009) generated music several styles using mixed-order
Markov chains generate melodies, HMMs harmonize them.
Markov Decision Processes (MDPs) another generalization Markov models,
agent maximizes utility function taking actions probabilistically influence next state, Partially Observable MDPs (POMDPs) represent corresponding
generalization HMMs. Experimental systems algorithmic composition implemented MDPs (Yi & Goldsmith, 2007) POMDPs (Martin et al., 2010), though
clear sophisticated models offer definitive advantages simpler ones.
3.4 Artificial Neural Networks Related Methods
Artificial Neural Networks (ANNs) computational models inspired biological neural
networks, consisting interconnected sets artificial neurons: simple computational
devices aggregate numeric inputs single numeric output using (generally)
simple nonlinear function. neurons connections set externally
(input connections), output signals intended read result
networks computation (output connections). Typically, neurons organized
recurrent networks (some neurons inputs come neurons)
several interconnected layers, many variations found literature. ANNs
typically used machine learning method, using set examples (input patterns)
train network (i.e., set weights connections neurons), order
use recognize generate similar patterns. Effectively, means neural
networks need pre-existing corpus music compositions (all similar
style, generally); therefore imitate style training examples.
papers use supervised learning approach, meaning examples training set
associated signal, ANN learns association. important aspect
ANN design modelization musical composition, is, mapping
music music notation inputs outputs network. Another important
aspect way compositions fed ANNs: may presented
temporal patterns network inputs, usually windowed segments,
cases fed (as wholes) ANNs (these implementations
scale well, though, big ANNs needed model long compositions).
ANNs first used 1970s 1980s analyze musical compositions, creating artificial models cognitive theories music (Todd & Loy, 1991), later
adapted music composition. first example implemented Todd (1989),
used three-layered recurrent ANN designed produce temporal sequence outputs
encoding monophonic melody, output signal network representing absolute
pitch. Given set one composition examples, ANN trained associate
single input configuration output temporal sequence corresponding composition. Then, feeding input configurations different ones used training
created melodies interpolated ones used training. one melody
used training, result extrapolation it. Later year, Duff
(1989) published another early example, using different approach, encoding relative
541

fiFernndez & Vico

Reference

Composition task

Comments

Todd, 1989

melody

three layers, recurrent

Duff, 1989

melody

two layers, recurrent

Mozer, 1991
(CONCERT)

melody

psychologically-grounded representation
pitch
feedforward model, used fitness
function optimization algorithm

Lewis, 1991
Shibata, 1991

harmonization

feedforward model

Bellgard & Tsang, 1992

harmonization

effective Boltzmann machine

Melo, 1998

harmonization

ANN trained
model music tension

Toiviainen, 1995

jazz improvisation

recurrent model

Nishijimi & Watanabe, 1993

jazz improvisation

feedforward model

Franklin, 2001

jazz improvisation

recurrent model

Hild et al., 1992
(HARMONET)

four-part harmonization

three-layered architecture
(two ANNs constraint system)

Feulner & Hrnel, 1994
(MELONET)

four-part harmonization

uses HARMONET another ANN
melodic variations

Goldman et al., 1996
(NETNEG)

species counterpoint

ANN basic melody, ensemble
agents refine melody

Verbeurgt et al., 2004

melody

two stages: Markovian model
ANN

Adiloglu & Alpaslan, 2007

species counterpoint

feedforward model

Browne & Fox, 2009

melody

simulated annealing ANN
measure musical tension

Coca et al., 2011

melody

recurrent model, uses chaotic non-linear
systems introduce variation

Table 13: References Section 3.4, order appearance.

instead absolute pitches (as Todds work) mapping, composing music Bachs
style.
machine learning paradigm, ANNs used many different ways, Todds
approach possible; indeed, early papers provide different examples.
example, Mozer (1991) developed recurrent ANN training program devised capture local global patterns set training examples. model also featured
output mapping sophisticated multidimensional space pitch representation,
capture formal psychological notion similarity different pitches. way,
similar output signals mapped similar pitches, order facilitate learning phase
improve composition phase. Lewis (1991) proposed another ANN framework: creation refinement, feedforward ANN trained set patterns ranging
random good music, associating pattern (possibly) multidimensional musicality score. way, training phase generated mapping function
patterns musicality scores. Then, create new compositions, mapping inverted:
starting purely random pattern, gradient-descent algorithm used ANN
542

fiAI Methods Algorithmic Composition

critique, reshaping random pattern maximize musicality score hope
finally producing pleasant composition. Unfortunately, paradigm prohibitive
computational cost, tested fairly simple short compositions.
early examples described experiments composing less
full-fledged monophonic compositions. However, ANNs also used automate
tasks music composition, harmonization pre-existing melodies. Shibata (1991) implemented early example: feedforward ANN represented chords using component tones, trained harmonizing simple MIDI music, whose performance measured
human listeners. sophisticated ANN used harmonization, effective Boltzmann machine (EBM), also provided measure quality output relative
training set (Bellgard & Tsang, 1992). Melo (1998) also harmonized classical music,
notable twist: order model tension25 music harmonized,
measured tension curve reported several human subjects listening music,
used averaged tension curve train ANN, chord progressions
generated ANN matched tension level suggested curve. seen,
harmonization popular test case, problems also tried. example,
Toiviainen (1995) used ANNs generated jazz improvisations based set training
examples. ANNs able create new jazz melodic patterns based training
set. similar way, Nishijimi Watanabe (1993) trained set feedforward ANNs
produce jazz improvisations jam session, modeling several music features jazz
using examples modeled jazz improvisations train ANNs. Franklin (2001)
used recurrent ANNs improvise jazz (trade four solos jazz performer), trained
two phases: first phase training ANN set pre-specified examples,
second phase ANN reconfigured trained reinforcement learning,
reinforcement values obtained applying set heuristic rules.
researchers came use hybrid systems, combining ANNs methods.
One first examples HARMONET (Hild et al., 1992): model designed solve
complex task: four-part choral harmonization Bachs style. HARMONET
three-layered architecture: first component feedforward ANN sophisticated encoding musical information (optimized harmonization functions instead
individual pitches), used extract harmonization information. output
fed second component, rule-based constraint satisfaction algorithm generate
chords, final component another ANN designed add quaver ornaments
previously generated chords. evolution HARMONET, MELONET (Feulner &
Hrnel, 1994; improved Hrnel & Degenhardt, 1997) harmonized chorales,
also generated melodic variations voices, using HARMONET first processing
stage harmonization, used another neural network generate melodic
variations.
NETNEG (Goldman et al., 1996) another hybrid system used ANN trained
sixteenth century classical music compositions. ANN generated basic melody
segments. segment created, ensemble agents generated polyphonic
elaboration segment. agents rule-based systems crafted music theoret25. important property music, rather difficult define. point time, tension related
interplay structure uncertainty perceived listener flow music.
Informally, defined unfinishedness music stopped point.

543

fiFernndez & Vico

ical considerations, coordinated maintain coherent global output. ANNs also
combined probabilistic methods: work Verbeurgt et al. (2004), set
training sequences decomposed musical motifs, encoded relative pitch. Then,
Markov chain constructed, whose states motifs. New compositions generated Markov chain, assign absolute pitches motifs resulting
composition, trained ANN. Adiloglu Alpaslan (2007) used feedforward ANNs
generate two-voice counterpoint, applying notions music theory representation
musical information networks. Browne Foxs (2009) system, simulated
annealing used arrange small fragments (motifs) classical music, trying get
profile musical tension (the metric Melo, 1998) similar profile
pre-specified composition, measured using ANN specialized music perception. Finally,
another hybrid system implemented Coca et al. (2011), used ANNs trained
pre-existing compositions together pseudo-random musical input generated
chaotic system, order generate complex compositions synthesis phase.
3.4.1 ANNs Evolutionary Algorithms
Among hybrid systems, combining ANNs evolutionary algorithms quickly became popular. Usually, ANN trained act fitness function
evolutionary algorithm. case earliest example hybrid systems,
NEUROGEN (Gibson & Byrne, 1991). fitness function composed result two
ANNs, one judging intervals pitches overall structure. genetic algorithm rather rigid, classical binary representation used,
severely limiting applicability whole implementation. However, also
inverted frameworks evolving individuals ANNs. example, Hrnel
Ragg (1996) evolved HARMONET networks, fitness network performance
training harmonization. another example (Chen & Miikkulainen, 2001), recurrent
three-layered ANNs evolved compose music, fitness computed set
rules music theory.
Given modular nature evolutionary algorithms perceived complexity
ANNs, uncommon evolutionary framework laid first research
work, subsequent developments ANNs used replace original fitness
function.26 example, Spector Alpern (1994) developed genetic programming (GP)
framework jazz improvisations: individuals programs composed collections
transformations produced improvisations upon fed previously existing jazz
melodies, fitness function aggregated several simple principles jazz music
theory. following year (Spector & Alpern, 1995), updated model train
ANN used fitness function. However, scheme always fare well,
specially initial framework uses interactive fitness. case GenJam (Biles,
1994), evolutionary algorithm generating jazz melodies interactive fitness
function. Later (Biles et al., 1996), interactive fitness evaluation represented
severe fitness bottleneck, ANNs tried partially offload evaluation human users,
success, ANNs failed satisfactorily generalize evaluations
26. approach risky, though, evolutionary algorithms tend find exploit unexpected
undesired quirks fitness evaluation function; evaluator ANN.

544

fiAI Methods Algorithmic Composition

Reference

Composition task

Comments

Gibson & Byrne, 1991

melody, rhythm

Two ANNs define fitness function

Hrnel & Ragg, 1996

melody harmonization

evolves HARMONET, fitness ANNs
performance training harmonization

Chen & Miikkulainen, 2001

melody

evolves recurrent networks,
fitness computed set rules

Spector & Alpern, 1994

melody

genetic programming, ANNs fitness functions

Biles et al., 1996

jazz improvisation

ANNs fitness functions

Johanson & Poli, 1998
(GP-Music)

melody

genetic programming, ANNs fitness functions

Klinger & Rudolph, 2006

melody

ANNs decision trees fitness functions

Manaris et al., 2007

melody

genetic programming, ANNs fitness functions

Burton, 1998

drum rhythms

Adaptive Resonance Theory
(unsupervised learning) fitness function

Phon-Amnuaisuk et al., 2007

melody

genetic programming, self-organinzing map
(unsupervised learning) fitness function

Table 14: References Section 3.4.1, order appearance.

training sets. case GP-Music System (Johanson & Poli, 1998), used
GP procedural representations short melodies, trained ANNs failure,
decidedly par respect performance algorithm interactive
fitness. Klinger Rudolph (2006) compared performance feedforward ANNs
learned decision trees, finding latter performed better ratings easier
understand. spite examples, successful instances exist: Manaris et al.
(2007) extracted several statistical metrics music compositions trained ANN
recognize compositions whose metrics distributions featured Zipfs law, used
fitness function GP framework whose individuals procedural representations
polyphonic compositions. results validated aesthetically pleasing human
testers.
research described point uses ANNs supervised learning. However,
methods using unsupervised learning also exist. Burton (1998) proposed genetic algorithm
classical binary representation generating multi-voice percussion rhythms, whose
fitness function presented unconventional feature mix. used Adaptive Resonance Theory (ART), ANN unsupervised learning, initially trained perform automatic
clustering set drum rhythms. Then, execution genetic algorithm,
unsupervised learning continued. fitness measure near individual cluster. individual represented brand new rhythm different add
new cluster, rhythm presented user decide musically acceptable.
way, Burton tried get best interactive automatic fitness evaluation.
different example unsupervised learning presented Phon-Amnuaisuk et al.
(2007), able generate variations pre-specified composition. used GP whose individuals procedural representations melodies, fitness similarity
pre-specified composition, measured self-organizing map (SOM) previously
trained musical elements pre-specified composition.
545

fiFernndez & Vico

Reference

Composition task

Comments

Baggi, 1991

jazz improvisation

ad hoc connectionist expert system

Laine, 2000

simple rhytms

uses central pattern generators

Dorin, 2000

poly-rhythmic musical patterns

uses Boolean networks

Hoover et al., 2012

acompaniment

uses CPPNs

Table 15: References Section 3.4.2, order appearance.

3.4.2 Related Methods
Finally, subsection presents methods may considered roughly similar ANNs,
generally, connectionist. example, Neurswing (Baggi, 1991) may described
ad hoc expert system jazz improvisation crafted connectionist framework.
Surprisingly, many research papers cite Neurswing ANN framework, despite Baggis
disclaimer: [Neurswing], though vaguely resembling neural net connectionist
system, [. . . ] (Baggi, 1991). Laine (2000) used simple ANNs implement Central
Pattern Generators, whose output patterns interpreted less simple
rhythms (motion patterns terminology).
Boolean networks another connectionist paradigm node binary
state edges nodes directed; nodes state changes discrete steps
according (usually randomly chosen) Boolean function whose inputs states
nodes connections node. may considered generalizations
cellular automata, depending wiring distribution Boolean functions,
states change complex patterns, potentially complex responses external forcing. properties, Dorin (2000) used Boolean networks generate
complex poly-rhythmic musical patterns, modulable real time user.
Hoover et al. (2012) proposed another connectionist approach: compositional pattern
producing networks (CPPNs). feedforward networks neuron may
use different, arbitrary function, instead classical sigmoid function. usually
designed interactive evolutionary methods, used generate modulate
highly complex patterns. cited paper, fed pre-existing simple composition
input, order generate accompaniment it.
3.5 Evolutionary Population-Based Methods
evolutionary algorithms (EAs) approximately follow common pattern: changing
set candidate solutions (a population individuals) undergoes repeated cycle evaluation, selection reproduction variation. first step generate candidate
solutions initial set, either user-specified examples less random
way. candidate evaluated using fitness function, heuristic rule measure
quality. next phase selection: new set candidate solutions generated
copying candidate solutions old one; candidate solution copied number
times probabilistically proportional fitness. step decreases diversity
population, restored applying (to fraction candidate solutions)
operators designed increase variation (for example, mutation recombination
546

fiAI Methods Algorithmic Composition

operators). steps applied iteratively; result, best mean fitness gradually
tend increase.
algorithmic pattern common EAs, exist many different algorithms using different sets selection rules, variation operators solution encoding.
EA, encoded form candidate solution genotype, phenotype
translation coded form solution. Hollands original formulation genetic algorithms strongly associated plain direct encoding genotypes
binary strings, case papers using EAs. this,
term evolutionary preferred genetic paper. Another popular variant, Kozas
genetic programming, represents genotypes tree structures, often encoding expressions
programming language, phenotype result evaluating expressions.
Since EAs particularly prone hybridized methods, also
reviewed sections: grammars Section 3.1.2, ANNs Section 3.4.1,
Markov chains Section 3.3 , rule-based systems Section 3.2.2,
cellular automata Section 3.6.1 . general, papers cited sections
discussed here. also recommend literature learn evolutionary
computer music: Burton Vladimirova (1999) wrote survey long, thorough descriptions referenced papers, survey Santos et al. (2000) similar
style, packed brief descriptions. Miranda Biless (2007) book
recent, also contains work optimization methods (as swarm optimization)
algorithmic composition techniques (as cellular automata).
3.5.1 Evolution Automatic Fitness Functions
difficulty define automatic fitness functions constant issue, frequently
limiting application evolutionary methods well-defined restricted problems
composition. Horner Goldberg (1991) provided one first examples:
implemented EA thematic bridging, composition technique consisting defining
sequence (with pre-specified preferred length) small musical patterns
first last ones pre-specified, pattern result applying simple
transformation previous one. Naturally, individuals EA defined
lists operations applied initial pattern generate sequence patterns.
fitness measured close final pattern (generated operations)
pre-specified final pattern, plus difference actual preferred lengths
sequence. Essentially work (with differences underlying representation
operation set) reported Ricanek et al. (1993).
common way implement fitness function weighted sum features
composition (although tuning weights optimize EA prove difficult except
toy problems). example, Marques et al. (2000) composed short polyphonic melodies
using direct representation genotypes also simple fitness function,
rather simple, ad hoc evaluation harmony melodic value. results
reportedly acceptable. Johnson et al. (2004) also composed short melodies using EA
. examples evolutionary algorithms described section, warranting dedicated
subsection them.
. case previous note.

547

fiFernndez & Vico

Reference

Composition task

Comments

Horner & Goldberg, 1991

thematic bridging

fitness: distance original melodies

Ricanek et al., 1993

thematic bridging

fitness: distance original melodies

Marques et al., 2000

polyphony

fitness: combination features

Johnson et al., 2004

melody

fitness: combination features

Papadopoulos & Wiggins,
1998

jazz improvisation

fitness: combination features

Harris, 2008
(JazzGen)

jazz improvisator

fitness: combination features

Towsey et al., 2001

melodic extension

fitness: combination features
(only fitness, evolutionary algorithm)

Birchfield, 2003

melody, rhythm

fitness: combination features

Garay Acevedo, 2004

species counterpoint

fitness: combination features

Lozano et al., 2009

melody
harmonization

fitness: combination features

De Prisco et al., 2010

unfigured bass

multi-objective optimization

Freitas & Guimares, 2011

melody
harmonization

multi-objective optimization

Gartland-Jones, 2002

generate variations
two melodies

fitness: distance melody

Alfonseca et al., 2005

melody

fitness: distance corpus melodies

zcan & Eral, 2008
(AMUSE)

generate variations
melody

fitness: combination features

Wolkowicz et al., 2009

melody

fitness: distance corpus melodies

Laine & Kuuskankare, 1994

melody

fitness: distance melody. Genetic programming

Spector & Alpern, 1994

jazz improvisation

fitness: combination features. Genetic programming

Dahlstedt, 2007

contemporary
classical music

fitness: combination features. Genetic programming

Esp et al., 2007

melody

fitness: combination features extracted fropm
corpus melodies. Genetic programming

Jensen, 2011

melody

fitness: distance corpus melodies.
Genetic programming

Daz-Jerez, 2011;
Snchez-Quintana et al.,
2013

contemporary
classical music,
genres

sophisticated indirect encoding

Table 16: References Section 3.5.1, order appearance.
fitness function weighted sum series basic, local features
melody. Papadopoulos Wiggins (1998) implemented system that, given
chord progression, evolved jazz melodies relative pitch encoding, using fitness function
weighted sum eight evaluations characteristics melody, ranging
simple heuristics speed position notes user-specified contour
similarity user-specified music fragments. similar approach implemented Harris
(2008), modest (if promising) results. Towsey et al. (2001) actually
implement EA, discussed build fitness function melodic extension
548

fiAI Methods Algorithmic Composition

(given composition, extend bars): proposed extract 21 statistical
characteristics corpus pre-specified compositions, defining fitness function
weighted sum distance individual mean characteristic.
similar vein, Birchfield (2003) implemented fitness function giant weighted sum
many features hierarchical EA, multiple population levels, individual
population composed individuals lower populations (similar model
described Biles, 1994; see Section 3.5.2). used output EA material
arrange long composition ten instruments. Garay Acevedo (2004) implemented
simple EA compose first species counterpoint, features weighted fitness
function simplistic, leading modest results. Lozano et al. (2009) generated chord
sequences harmonize pre-specified melody two steps: first simple EA generated
set possible solutions according simple local considerations (appropriateness
chord corresponding part melody) chords notes
melody, variable neighborhood search used establish chord progression
according global considerations.
alternative implementing fitness function weighted sum musical features use multi-objective evolutionary algorithms (MOEAs). However,
rarely used, probably harder implement, conceptually practice. MOEAs used De Prisco et al. (2010) harmonize
unfigured bass27 Freitas Guimares (2011) harmonization.
Another approach consists measuring fitness distance target composition corpus compositions. example, Gartland-Jones (2002) implemented EA
compose hybrids two pre-specified compositions (a goal similar Hamanaka
et al.s, 2008, cited Section 3.1) using one seed initial population,
distance (sum difference pitch note) fitness function. Alfonseca et al. (2005) used sophisticated evaluation: fitness composition
population sum distances corpus pre-specified target compositions. metric normalized compression distance, measure different two
symbol strings are, based compressed lengths (both concatenated separated).
zcan Eral (2008) used simple genetic representation fitness function based
weighted sum long list simple musical characteristics, reportedly able
generate improvisations pre-specified melody given harmonic context,
specify evolved melodies related pre-specified music.
EA implemented Wolkowicz et al. (2009), individuals encoded using relative
pitches, sophisticated statistical analysis n-gram sequences pre-specified corpus
compositions used implement fitness function.
Genetic programming also used fitness functions based comparisons
pre-specified music. work Laine Kuuskankare (1994), individuals
discrete functions time whose output (phenotype) interpreted sequence
pitches. fitness simply sum differences pitches pre-specified
target composition phenotype individual, similar way GartlandJones (2002). However, frequent find fitness functions based analysis
characteristics compositions evolving algorithm, often comparing
27. description unfigured bass, see discussion Rothgebs (1968) work Section 3.2.

549

fiFernndez & Vico

characteristics pre-specified training set compositions. Spector Alpern (1994)
used genetic programming jazz improvisation, trading fours: individuals
functions took input four produced another one applying series
transformations it. fitness determined applying series score functions
measured rhythm, tonality features produced four, comparing
database high-quality examples renowned artists.
Also using genetic programming, Dahlstedt (2007) composed relatively short contemporary classical music pieces, simple fitness function: set target values
assigned several statistics compositions (as note density pitch standard deviation, among others), fitness weighted sum differences
target values values individual. individuals trees whose nodes
represented notes different operations musical sequences (using developmental
process genotype phenotype). Reportedly, generated pieces acceptable
quality, genotype model especially well suited contemporary classical
music. Esp et al. (2007) used simple tree representation compositions (but without
indirect encoding), defining fitness function weighted sum sophisticated statistical models melody description (measuring distance values set
pre-specified compositions) several relatively simple characteristics composition.
Jensen (2011) also used simple tree representation compositions; fitness calculated measuring frequency distributions simple events compositions, rating
according Zipfs law similarity pre-specified compositions.
One latest successful results evolutionary computer music follows
evo-devo strategy. Iamus computer cluster hybridizes bioinspired techniques:
compositions evolve environment ruled formal constraints aesthetic principles (Daz-Jerez, 2011). compositions also develop genomic encodings way
resembles embryological development (hence evo-devo), providing high structural
complexity relatively low computational cost. composition result
evolutionary process instruments involved preferred duration
specified, included fitness function. Iamus write professional scores
contemporary classical music, published debut album September 2012
(Ball, 2012; Coghlan, 2012), ten works interpreted first-class musicians (including LSO orchestra piece). Melomics, technology behind avant-garde
computer-composer, also mastering genres transferring result industry
(Snchez-Quintana et al., 2013). compiling myriad musical fragments
essential styles browsable, web-based repository (Stieler, 2012). first time,
Melomics offering music real commodity (priced size MIDI representation),
ownership piece directly transferred buyer.
3.5.2 Musical IGAs
previous exposition, apparent designing objective convenient
fitness function evaluating music compositions difficult problem.28 music
evaluated terms subjective aesthetic quality, may become impractical
28. point artists sometimes find unconventional ways around problem: Waschka (1999)
argued solved problem assigning purely random fitness value individuals.

550

fiAI Methods Algorithmic Composition

Reference

Composition task

Comments

Hartmann, 1990

melody

inspired Dawkins biomorphs

Nelson, 1993

melody

inspired Dawkins biomorphs

Horowitz, 1994

rhythms

inspired Dawkins biomorphs

Pazos et al., 1999

rhythms

binary genotype

Degazio, 1996

melody

binary genotype; graphical representation

Biles, 1994
(GenJam)

jazz improvisation

two hierarchically structured populations
(measures jazz phrases)

Tokui & Iba, 2000

rhythms

two hierarchically structured populations
(short long rhythmic patterns).
Genetic programming

Jacob, 1995

melody

user trains critics act fitness functions

Schmidl, 2008

melody

user trains critics act fitness functions

Putnam, 1994

melody

genetic programming

Ando & Iba, 2007

melody

genetic programming

MacCallum et al., 2012
(DarwinTunes)

melody

genetic programming

Kaliakatsos-Papakostas
et al., 2012

melody, 8-bit sound
synthesis

genetic programming

Hoover et al., 2012

acompaniment

uses CPPNs

McDermott & OReilly, 2011

interactive generative
music

similar CPPNs

Ralley, 1995

melody

minimizes user input clustering candidates

Unehara & Onisawa, 2001

melody

minimizes user input elitism

Daz-Jerez, 2011

contemporary classical
music

minimizes user fatigue producing small, good
compositions

Beyls, 2003

melody

uses cellular automata; graphical representation

Moroni et al., 2000
(Vox Populi)

melody

complex graphical representation

Ventrella, 2008

melody

whole population comprises melody

Marques et al., 2010

melody

minimizes evolutionary iterations

Table 17: References Section 3.5.2, order appearance.
directly impossible define formal fitness function. inconveniences,
many researchers resorted implement fitness function human evaluators.
common term describing class EAs musical IGA (interactive genetic algorithm29 , MIGA short). MIGAs represent substantial percentage total body
work EAs algorithmic composition, subsection devoted them.
first MIGAs implemented composers intrigued concept evolutionary computing, resulting less peculiar architectures perspective
common practice evolutionary computing, also computer scientists exploring
29. research tagged IGA use binary genotypes commonly associated term
genetic algorithm, term common.

551

fiFernndez & Vico

field. Hartmann (1990), inspired Dawkins biomorphs, presented one first
applications evolutionary computing composition, MIGA unfortunately described notoriously laconic obscure language, resulting low citation rate
work. Also inspired biomorphs, Nelson (1993) described toy MIGA
evolving short rhythms fixed melodic structure, simple binary genotypes (each
bit simply denoted presence absence sound). formal models similar scope
Nelsons designed Horowitz (1994) Pazos et al. (1999). implemented
rhythm generators multiple instruments, one independent rhythm
pattern encoded genotype (Horowitzs genotypes parametric, Pazos et
al. used direct binary encodings). Degazio (1996) implemented system
genotype set parameters (in later iterations, mini-language describe
parameters) instruct CAAC software generate melodies.
best known MIGA may GenJam, system generating jazz solos, developed
several years. first incarnation (Biles, 1994), formulated MIGA
two hierarchically structured populations: one measures, jazz phrases, constructed sequences measures. Given chord progression several parameters,
jazz solos emerged concatenating selected phrases evolutionary process,
fitness integrated time accumulating fixed increments decrements
simple good/bad indications evaluator. iterations system included
already discussed use ANNs fitness functions (Biles et al., 1996) possibility
trade fours human performer, dynamically introducing population
music performed human (Biles, 1998). Tokui Iba (2000) used similar solution
creating rhythms multiple instruments: population short sequences specified
list notes, another population tree structures representing functions simple
macro language used short sequences building blocks. Another example hierarchical structuring MIGA Jacobs (1995) system general-purpose composition,
three inter-dependent evolutionary processes: one involving human user train
ears evaluate short musical sequences, another one compose musical phrases using
ears (filters) fitness functions, another also involving human user train
arranger reorders resulting phrases final output system. Schmidl
(2008) implemented similar system, without high-level arranger module,
ears automatically trained set examples, order minimize user interaction
enable real-time composition.
Genetic programming interactive evaluation used several times. Putnam
(1994) implemented early example: individual coded set functions
generated melody result iterated function system. Tokui Ibas (2000)
example already cited. Ando Iba (2007) implemented fully interactive
system (not selection, also reproduction mutation user-guided),
genotype model similar Dahlstedts (2007). MacCallum et al. (2012) used
trees encode Perl expressions generated polyphonic short loops, concentrated
analyzing interactive evolution point view theoretical biology. KaliakatsosPapakostas et al. (2012) used rather different approach generate 8-bit melodies:
individual function composed bitwise operators generated waveform
iterating function. fact, work might described sound synthesis large
time scales, rather music composition.
552

fiAI Methods Algorithmic Composition

MIGAs graph-based genetic representations also exist, implementation
based CPPNs (Hoover et al., 2012) already cited Section 3.4.2. McDermott
OReilly (2011) used similar paradigm: genotypes sequences integers
indirectly encoded graphs, whose nodes represented functions, connections
compositions functions. output nodes generated musical output one
voices, modulated user inputs.
problem common MIGAs user fatigue: candidate solution evaluation
comparatively slow monotone task rapidly leads user fatigue. Even small
population sizes small numbers generations, remains significant problem
solved many researchers different ways. example, Ralley (1995) used binary
representation relative pitch encoding genotypes, classified population
clustering algorithm, deriving similarity metrics rudimentary spectral analysis
scores. user simply required evaluate closest composition centroid
cluster. exotic solution employed previously cited Tokui Iba
(2000): training neural network filter candidates low fitness, thus presenting
user individuals acceptable quality. Unehara Onisawa (2001) presented
10% candidate melodies human user, parts genomes best
rated ones dispersed population horizontal gene transfer. McDermott
OReilly (2011) also limited number candidates exposed user rating, filtering
worst ones heuristic functions. Another option minimize user fatigue
produce small compositions already reasonably good. Melomics system (see
last paragraph Section 3.5.1) used way (Daz-Jerez, 2011).
common ways manage problem include low population sizes and/or hierarchical structuring algorithm (Biles, 1994), providing statistical information and/or
rendering graphical representations compositions order make possible
evaluation without actually listening (Degazio, 1996). Graphical representations
also particularly useful using generative methods L-systems cellular automata
(Beyls, 2003). Putnam (1994) used web interface reach volunteers. Moroni
et al. (2000) tried solve problem using sophisticated GUI abstractions, complex
non-linear mappings graphic controls parameters fitness function
aspects evolutionary process, produce highly modulable system
real-time interactive composition melodies. mitigate user fatigue, Ventrella (2008)
presented population short melodies continuous stream sound; fitness obtained binary signal set user. Marques et al. (2010) limited user fatigue
generation short, simple melodies severely limiting number generations
algorithm, generating reasonably good starting population drawing notes
using Zipfs law.
3.5.3 Population-Based Methods
Finally, subsection presents methods also population-based. example, metaheuristic method Harmony Search inspired improvisation process
musicians, though practice framed evolutionary method specific way
structure candidate solutions perform selection, crossover mutation operations.
Geem Choi (2007) used method harmonize Gregorian chants (i.e., write or553

fiFernndez & Vico

Reference

Composition task

Comments

Geem & Choi, 2007

harmonize
Gregorian chants

harmony search

Geis & Middendorf, 2008

four-part
harmonization

multi-objective Ant Colony Optimization

Tominaga & Setomoto, 2008

polyphony,
counterpoint

artificial chemistry

Werner & Todd, 1997

melody

co-evolutionary algorithm, Markov chains used
evolvable fitness functions

Bown & Wiggins, 2005

melody

individuals Markov chains
compose evaluate music

Miranda, 2002

melody

individuals agree common set
intonation patterns

Miranda et al., 2003,
sect. IV

melody

individuals grammars compose music

McCormack, 2003b

interactive soundscape

individuals indirectly compete users attention

Dahlstedt & Nordahl, 2001

soundscape

music emerges collective interactions

Beyls, 2007

soundscape

music emerges collective interactions

Blackwell & Bentley, 2002

soundscape

music emerges collective interactions

Eldridge & Dorin, 2009

soundscape,
sound synthesis

music emerges collective interactions,
individuals exist frequency domain

Bown & McCormack, 2010

interactive soundscape,
sound synthesis

music emerges collective interactions,
individuals exist frequency domain

Table 18: References Section 3.5.3, order appearance.

ganum lines chants). methods also population-based properly
evolutionary. example use Ant Colony Optimization (ACO) solve constraint
harmonization problems (Geis & Middendorf, 2008), already mentioned Section 3.2.3.
ACO, candidate solutions represented paths graph, population
agents (ants) traverse graph, cooperating find optimal path.
exotic example, Artificial Chemistry generative system consisting
multiset strings symbols. strings (analogues molecules) react according
pre-specified set rules (analogues chemical reactions), generating new strings
existing ones. Tominaga Setomoto (2008) used method, encoding polyphonic
compositions strings musical rules counterpoint reaction rules
artificial chemistry: starting set simple strings, system generated progressively
complex ones, though aesthetical value resulting compositions varied widely.
popular method based populations individuals Artificial Ecosystem.
artificial ecosystem, compositions emerge interaction individuals
simulation evolutionary and/or cultural interactions, taking inspiration evolutionary origins music humans (Wallin & Merker, 2001). Frequently, complexity
simulations severely limited available computational power,
cases goal music composition per se, study evolutionary dynamics,
emergence shared cultural traits avant-garde artistic experimentation.
554

fiAI Methods Algorithmic Composition

early example Werner Todd (1997) investigated sexual evolutionary dynamics
population males (small compositions) females (Markov chains initially generated
corpus songs) evaluated much males deviated expectations.
However, studies use one kind agent, work Bown Wiggins (2005),
whose agents used Markov chains compose analyze music. Miranda (2002)
Miranda, Kirby, Todd (2003, sect. IV) implemented models similar goal:
study emergence common structures (shared cultural knowledge). first case
population agents strove imitate others intonation patterns (short sequences
pitches); second case agents learned compose music inferring musical grammars
agents songs. Bosma (2005) extended Mirandas (2002) model, using neural
networks agents learn compose music, although tiny population sizes.
part art installation, McCormack (2003b) proposed virtual ecosystem evolving
agents able compose music using rule-based system, competing resources
indirectly determined interest human observers.
alternative music composed agents, emerges
epiphenomenon whole ecosystem. models Dahlstedt Nordahl (2001)
Beyls (2007) used simple organisms two-dimensional space whose collective behavior
mapped complex compositions, Blackwell Bentleys (2002) model
similar three-dimensional, dynamics agents inspired swarm
flocking simulations. examples use either homogeneous spatially
structured ecosystems, recent trend use sound environment itself.
Eldridge Dorins (2009) model, agents dwelt one-dimensional space
Fourier transform sample ambient sound, feeding moving energy across
frequencies. Bown McCormack (2010) implemented similar model,
agents neural networks generated sound competed room space
frequencies ambient sound.
3.6 Self-Similarity Cellular Automata
late 1970s, two notable results music reported Voss Clarke (1978).
first that, music many different styles, spectral density audio
signal (approximately) inversely proportional frequency; words, approximately follows 1/f distribution. surprising: many different data series
follow property, meteorological data stock market prices; usually referred
1/f noise pink noise. second result random compositions seemed
musical pleasing (for wide range evaluators, unskilled people professional musicians composers) pitches determined source 1/f noise,
rather common random processes white (uncorrelated) noise Brownian motion (random walks). Although first result since challenged30 , second one
used composers source raw material. Bolognesi (1983) implemented
early example influenced Voss Clarkes results, composers used data series
1/f noise raw material even results, early 1970s (Doornbusch, 2002).
30. main criticism data samples used Voss Clarke hours long, merging
sample many different compositions (and even non-musical sounds radio station). view

555

fiFernndez & Vico

Reference

Composition task

Comments

Voss & Clarke, 1978

melody

first reference 1/f noise music

Bolognesi, 1983

melody

early deliberate use 1/f noise music

Doornbusch, 2002

melody

reference early non-deliberate use 1/f noise music

Gogins, 1991

melody

iterated function systems

Pressing, 1988

melody,
sound synthesis

chaotic non-linear maps

Herman, 1993

melody

chaotic non-linear dynamical systems

Langston, 1989

melody

fractional Brownian motion

Daz-Jerez, 2000

melody

fractals self-similar systems

Bidlack, 1992

melody

various fractal chaotic systems

Leach & Fitch, 1995
(XComposer)

melody, rhythm

uses various fractal chaotic systems

Hinojosa-Chapel, 2003

melody

uses various fractal chaotic systems fill

Coca et al., 2011

melody

uses chaotic systems add variation

Table 19: References Section 3.6, order appearance.
remains question 1/f noise produces musical results
random processes. consensus research artistic communities self-similarity:
structure 1/f noise statistically similar across several orders magnitude (Farrell
et al., 2006). Self-similarity common feature classical music compositions (Hs &
Hs, 1991), also one defining features fractals (in fact, 1/f noise also
fractal characteristics). this, fractals extensively used source
inspiration raw material compositions CAAC software. general, selfsimilar musical patterns multiple levels structure, pleasing regularities
also dotted sudden changes. characteristics also present
output chaotic systems (whose attractors also fractal structures), also used
generate musical patterns. Commonly used techniques generate self-similar musical
patterns include chaotic systems iterated function systems (Gogins, 1991), nonlinear maps (Pressing, 1988) non-linear dynamical systems (Herman, 1993), also
fractional Brownian motion (Langston, 1989), cellular automata (discussed below) Lsystems (already discussed Section 3.1.1). exotic methods also possible,
musical renderings fractal images number sequences fractal characteristics (DazJerez, 2000). methods widely regarded suitable produce melodies
compositions right, source inspiration raw material (Bidlack,
1992). this, extensive review provided here.31 However, full-fledged
algorithmic composition methods use part creative process, Leach
Fitchs (1995) XComposer, chaotic systems used fill structures laid
critics, Nettheim (1992), samples could possibly representative single musical
pieces (see also discussion Daz-Jerez, 2000, pp. 136138).
31. Good (if somewhat outdated) reviews found work Jones (1989), Daz-Jerez (2000)
Nierhaus (2009). list composers using fractals chaotic systems CAAC long
impractical consistently describe existing relevant work.

556

fiAI Methods Algorithmic Composition

hierarchical model, Hinojosa-Chapels (2003) paradigm interactive
systems, also used source musical material. also used
add complexity compositions generated means.32
3.6.1 Cellular Automata
cellular automaton (CA) discrete (in time, space state) dynamic system composed
simple computational units (cells) usually arranged ordered n-dimensional (and
potentially unbounded) grid (or regular tiling). cell one finite
number states. discrete time step, cells state deterministically updated,
using set transition rules take account state neighbors states.
Although definition generalized multiple ways, represents good first
approximation. Cellular automata used many disciplines across Science
Humanities dynamical models complex spatial temporal patterns emerging
local interaction many simple units; music composition one disciplines.
Cellular automata used generate fractal patterns discrete versions chaotic
dynamical systems, also represent alternative computational paradigm realize
algorithmic composition.33 Unfortunately, like fractals chaotic systems, CA also
tend produce interesting somewhat unmusical patterns used inspiration
raw material rather directly music compositions. Although CA argued
better suited sound synthesis algorithmic composition (Miranda, 2007),
latter application reviewed here.
Xenakis known deeply interested application CA music.
orchestral composition Horos, released 1986, widely regarded used CA
configure structure composition, though heavily edited hand (Hoffmann, 2002). Early, better documented explorations CA music composition include
implementations Beyls (1989), Millen (1990), Hunt et al. (1991), mapped
patterns generated user-defined CA MIDI output. Beyls (1989) presented CA
generative system real-time composition avant-garde music, exploring several ways
complexify generated musical patterns (as changing transition rules according
meta-rules), Millen (1990) presented minimalist CAAC system. Hunt et al. (1991)
implemented another CAAC system designed give composer control
composition process. Echoing Beylss (1989) early work, Ariza (2007) proposed bend
transition rules order increase space parameters available composer experimentation, either randomly changing state isolated cells dynamically
changing transition rules one generation next.
CAMUS (Miranda, 1993) known CA system algorithmic composition
innovative design using two bidimensional CA: Conways Game Life (used determine
musical sequences) Griffeaths Crystalline Growths (used determine instrumentation notes generated first CA). activated cell Game Life
mapped sequence three notes, whose instrument selected according corresponding cell second CA (the Crystalline Growths system). Unfortunately, according
creator (Miranda, 2007), CAMUS produce musical results: out32. See, e.g., description work Coca et al. (2011) Section 3.4.
33. detailed survey, see e.g. work Burraston Edmonds (2005).

557

fiFernndez & Vico

Reference

Composition task

Comments

Hoffmann, 2002

structure

reference early use CA music (Xenakiss Horos)

Beyls, 1989

melody

early use CA music

Millen, 1990

melody

early use CA music

Hunt et al., 1991

melody

early use CA music

Ariza, 2007

melody

dynamically changing CA rules

Miranda, 1993
(CAMUS)

melody,
instrumentation

two CA: one melody,
instrumentation

McAlpine et al., 1999
(CAMUS 3D)

melody, rhythm,
instrumentation

above, plus Markov chains select rhythm

Bilotta & Pantano, 2001

melody

explores several mappings CA music events

Dorin, 2002
(Liquiprism)

rhythmic patterns

several interacting CA

Ball, 2005, Miljkovic, 2007

melody, rhythm

references WolframTones

Phon-Amnuaisuk, 2010

melody

uses ANNs learn CA rules

Beyls, 2003

melody

interactive evolutionary algorithm

Bilotta & Pantano, 2002

melody

extends Bilotta Pantanos (2001) work
evolutionary algorithm

Lo, 2012

melody

evolutionary algorithm,
Markov chains used fitness function

Table 20: References Section 3.6.1, order appearance.
put properly considered raw material edited hand. CAMUS later
generalized, using Markov chain determine note durations three-dimensional
versions Game Life Crystalline Growths (McAlpine et al., 1999).
recently, Bilotta Pantano (2001) explored several different mappings generate music CA: local codes (mapping cells pitches, usual mapping
papers), global codes (mapping entropy whole pattern generation musical events) mixed codes (mapping groups cells musical events). Dorin (2002) used
six bidimensional finite CA arranged cube (their edges connected), running different speeds, generate complex poly-rhythmic patterns.34 Finally, WolframTones35 (Ball,
2005) commercial application CA music composition, using database four
billions transition rules one-dimensional CA (all possible transition rules taking
account five neighbors). WolframTones searches rules produce chaotic complex
patterns. patterns mapped musical events, system able search
patterns whose musical mapping resembles one set pre-defined musical styles
(Miljkovic, 2007).
Although CA commonly used generate musical material uncontrolled way
(i.e., composer tunes parameters CA hand), possible use
methods design CA (states, transition rules, etc.). example, Phon-Amnuaisuk
34. Section 3.4.2, similar work (Dorin, 2000) Boolean networks (a connectionist paradigm usually
seen generalization CA) mentioned.
35. http://tones.wolfram.com/

558

fiAI Methods Algorithmic Composition

(2010) used artificial neural networks trained learn transition rules CA: given
melody, piano-roll notation interpreted temporal pattern CA,
network trained learn transition rules temporal pattern. Then, given
initial conditions, network produced new compositions piano-roll notation.
Evolutionary algorithms also used design parameters (transition rules, states,
etc.) CA. cases, previous work hand-designed CA adapted use
evolutionary algorithm. case Beyls (2003), used interactive evolutionary algorithm evolve parameters CA, Bilotta Pantano (2002),
adapted previously discussed work (Bilotta & Pantano, 2001) use evolutionary
algorithm, although fitness function poorly described. Lo (2012) applied evolutionary algorithms generate CA algorithmic composition comprehensive way,
experimenting various fitness functions based extracting statistical models
corpus pre-existing compositions, including metrics based Markov models Zipfs
law.

4. Conclusions
survey, several hundreds papers algorithmic composition briefly reviewed. Obviously, none described detail. Rather, survey
intended short reference guide various methods commonly used algorithmic
composition. Pearce et al. (2002) noted, papers algorithmic composition
adequately (a) specify precise practical theoretical aims research; (b) use
methodology achieve aims; (c) evaluate results controlled, measurable
repeatable way. Researchers algorithmic composition diverse backgrounds,
and, many cases, present work way enables comparison
others. considerations, presented literature narrative
style, classifying existing work several broad categories, providing brief descriptions papers approximately chronological order category.
4.1 Creativity
Algorithmic composition automates (to varying degrees) various tasks associated
music composition, generation melodies rhythms, harmonization, counterpoint orchestration. tasks applied two ways: (a) generate music
imitating corpus compositions specific style, (b) automate composition
tasks varying degrees, designing mere tools human composers, generating
compositions without human intervention:
Generating music imitating corpus compositions specific style.
instances kind problem (including real-time improvisation systems elaborate input human musicians) considered solved: imitation problems
tackled many different methods, many cases reasonable success
(such Copes EMI, 1992, Pachets Continuator, 2002). fact, since origins
computational algorithmic composition, bias research community towards imitation problems (Nierhaus, 2009). may attributed
559

fiFernndez & Vico

difficulty merge mindset computer science (clear-cut definitions, precise algorithms, straight methodologies) mindset artistic work (intuition, vagueness,
cultural heritage artistic influences). two mindsets may compared
common cultural divide Artificial Intelligence neats scruffies.
Unfortunately, neats reign supreme Artificial Intelligence, yet
gain upper hand Artificial Creativity.
Automating composition tasks varying degrees. case automated
systems algorithmic composition intended reproduce human creativity
way, problem evaluating performance: concept artistic
creativity eludes formal, unambiguous effective definition. makes difficult
evaluate systems completely rigorous way. Certainly, many frameworks
proposed assessing computational creativity36 , one easily
uniformly applied computers humans alike, way spark
controversy. may seem simple measure computational creativity human
standards: simply ask people listen human machine compositions,
declare algorithmic composition system creative people cannot tell
apart compositions human ones. Ariza (2009) noted, kind musical
Turing Test performed many different researchers trying validate
systems, valid algorithmic composition system aspires imitate,
truly creative create truly innovative work art.
also view systems algorithmic composition cannot attain true creativity, even principle. fact, suggested (Kugel, 1990) Turing-equivalent
formalism truly simulate human creativity, i.e., musical creativity effectively computable, thus preventing computer systems completely imitating human composers,
even theory. argument without merits, open debate, precisely
lacks rigorous, unambiguous definition creativity.
4.2 Methods
Regardless (more less abstract) considerations true creativity, survey
presented existing work algorithmic composition, organized several categories.
described beginning Section 3, categories grouped classes:
Symbolic AI (grammars rule-based systems). umbrella,
grouped different techniques. techniques used imitation (be
style specific composer, generally musical style) automation
composition tasks. proved effective, popular (at least,
sheer volume reviewed work), cases, labor-intensive,
require musical knowledge encoded maintained symbolic
framework choice. also clear trend towards formal
systems, gradually moving ad hoc rule systems constraint satisfaction
various formalisms.
36. example, Geros (2000), Pearce Wigginss (2001), Ritchies (2007) Bodens (2009).

560

fiAI Methods Algorithmic Composition

Machine learning (Markov chains artificial neural networks).
nature, machine learning techniques used primarily imitation, although
Markov chains (and related statistical methods) artificial neural networks
also used automate composition tasks (such harmonization).
noted techniques described symbolic AI also machine learning
(like Copes ATNs, rule learning case-based reasoning).
Optimization techniques (evolutionary algorithms). case machine
learning, optimization techniques (mostly evolutionary algorithms) profusely
used imitation, since natural express objective optimization (the
fitness function) distance musical style imitated. However,
automation composition tasks also explored, case
machine learning techniques.
Self-similarity cellular automata. Strictly speaking, techniques
form AI. explained beginning Section 3, represent convenient
way generate novel musical material without resorting human musical knowledge,
problem musical material generated way rough;
commonly used human composers raw material build upon.
reviewing literature, becomes apparent silver bullet: except
strict, limited imitation specific musical styles real-time improvisation systems
elaborate input human musicians, almost approaches algorithmic composition
seem unable produce content deemed par professional
human composers, even without taking account problem creativity discussed
Section 4.1. examples stand out, niche applications,
contemporary classical music composed Iamus (Ball, 2012).
silver bullet, one obvious way forward hybridization two
methods. fact, review existing work, seems apparent many
researchers already following route, hybridizations
rarely explored. example, music material produced systems based
self-similarity CA commonly regarded mere source inspiration human
composers, rather proper way automate composition music,
music material generally lacks structure. However, used first stage
process algorithmic composition, modified refined subsequent stages,
probably based form machine learning goal produce music specific
musical style, knowledge-based system (such Leach Fitchs XComposer,
1995). case evolutionary algorithms, self-similarity systems may used seed
initial population, introduce variety avoid premature convergence, CA may
used individuals evolved (such work Lo, 2012, also features
machine learning techniques). research required explore potential
kind approach, combining self-similarity CA-based systems methods.
case optimization techniques, multi-objective paradigm rarely
used, least comparison traditional single-objective approach. Composing
music usually requires balancing set many different, sometimes conflicting objectives,
configure various aspects music, multi-objective optimization seems natural
561

fiFernndez & Vico

way tackle problem. often, researchers use weighted sum parameters
conflate objectives single fitness function. researchers use
multi-objective optimization, Geis Middendorf (2008), Carpentier Bresson
(2010), De Prisco et al. (2010), Freitas Guimares (2011). multi-objective
optimization harder (both conceptually practice), represents natural way
dealing complexity many different objectives, explored
research community.
Finally, specific case evolutionary algorithms, issue encoding individuals also examined. Looking algorithmic composition optimization
problem, search spaces musical compositions tend huge high-dimensional.
Direct encodings (such directly representing music sequence pitches) make
difficult explore search space effective way, problems scalability
(the performance degrades significantly size problem increases) solution
structure (the solutions generated algorithm tend unstructured, hard adapt
fragile). problem mitigated indirect encodings, genotype
directly represent phenotype, rather list instructions build it. Many
different types indirect encoding used algorithmic composition,
L-systems, types grammars, various encoding styles used genetic programming. However, advanced techniques indirect encoding rarely
applied algorithmic composition, order overcome aforementioned problems
scalability solution structure, related artificial embryogenies (Stanley
& Miikkulainen, 2003), inspired biological developmental processes.
Adding evolutionary toolkit may way enable complex
compositional tasks tackled.
4.3 Final Thoughts
Computers come stay: use CAAC software prevalent among many composers, artistic scenes (as generative music) embrace computer-generated music
part identity. However, creativity still hands composers part.
argued Section 4.1, creativity inherently subjective concept, arguably
debatable point computational system may become truly creative. However,
even precise definition cannot agreed upon, easy see development
algorithmic composition systems capable independent creativity radically change
process music composition, consequently market music.
seen yet another case computers replacing humans ever sophisticated
activity, potentially radical disruption way composers perform work:
like pedagogical expert system supersedes role human teachers, enable
new ways work.
music one arts stronger mathematical background, surprising
debate whether machines make original creative works
centered subfield computational creativity. Hybridization different techniques,
bioinspiration, use high performance computing might bring new realms
(computer-) creativity. science writer Philip Ball put analysis Melomics
562

fiAI Methods Algorithmic Composition

music composition technology: . . . unfolding complex structure mutable core
enabled kind dramatic invention found biological evolution (Ball, 2012).

Acknowledgments
authors wish thank Ilias Bergstrom comments preliminary version
manuscript. Also, critical review anonymous referees greatly improved
final version. study partially supported grant MELOMICS project
(IPT-300000-2010-010) Spanish Ministerio de Ciencia e Innovacin, grant
CAUCE project (TSI-090302-2011-8) Spanish Ministerio de Industria, Turismo
Comercio. first author supported grant GENEX project (P09-TIC5123) Consejera de Innovacin Ciencia de Andaluca. first author also
wishes thank wife Elisa daughter Isabel day day, spite
long hours spent writing manuscript, family invaluable support
provided.

References
Adiloglu, K., & Alpaslan, F. N. (2007). machine learning approach two-voice counterpoint composition. Knowledge-Based Systems, 20 (3), 300309.
Aguilera, G., Galn, J. L., Madrid, R., Martnez, A. M., Padilla, Y., & Rodrguez, P. (2010).
Automated generation contrapuntal musical compositions using probabilistic logic
Derive. Mathematics Computers Simulation, 80 (6), 12001211.
Alfonseca, M., Cebrin, M., & Ortega, A. (2005). Evolving computer-generated music
means normalized compression distance. Proceedings WSEAS
International Conference Simulation, Modelling Optimization, pp. 343348,
Stevens Point, Wisconsin, USA.
Allan, M. (2002). Harmonising chorales style Johann Sebastian Bach. Masters
thesis, University Edinburgh.
Allombert, A., Assayag, G., Desainte-Catherine, M., & Rueda, C. (2006). Concurrent constraints models interactive scores. Proceedings Sound Music Computing Conference.
Ames, C. (1987). Automated composition retrospect: 1956-1986. Leonardo, 20 (2), 169
185.
Ames, C. (1989). Markov process compositional model: survey tutorial.
Leonardo, 22 (2), 175187.
Ames, C., & Domino, M. (1992). Understanding music AI, chap. Cybernetic composer:
overview, pp. 186205. MIT Press, Cambridge.
Amiot, E., Noll, T., Agon, C., & Andreatta, M. (2006). Fourier oracles computer-aided
improvisation. Proceedings International Computer Music Conference.
Anders, T. (2007). Composing Music Composing Rules: Design Usage Generic
Music Constraint System. Ph.D. thesis, Queens University Belfast.
563

fiFernndez & Vico

Anders, T., & Miranda, E. R. (2009). computational model generalises Schoenbergs
guidelines favourable chord progressions. Proceedings Sound Music
Computing Conference.
Anders, T., & Miranda, E. R. (2011). Constraint programming systems modeling music
theories composition. ACM Computing Surveys, 43 (4), 30:130:38.
Ando, D., & Iba, H. (2007). Interactive composition aid system means tree representation musical phrase. Proceedings IEEE Conference Evolutionary
Computation, pp. 42584265.
Ariza, C. (2005a). Navigating landscape computer aided algorithmic composition
systems: definition, seven descriptors, lexicon systems research.
Proceedings International Computer Music Conference.
Ariza, C. (2005b). Open Design Computer-Aided Algorithmic Music Composition:
athenaCL. Ph.D. thesis, New York University.
Ariza, C. (2006). Beyond transition matrix: language-independent, string-based input
notation incomplete, multiple-order, static Markov transition values. Unpublished
manuscript.
Ariza, C. (2007). Automata bending: Applications dynamic mutation dynamic rules
modular One-Dimensional cellular automata. Computer Music Journal, 31 (1),
2949.
Ariza, C. (2009). interrogator critic: Turing test evaluation generative
music systems. Computer Music Journal, 33 (2), 4870.
Ariza, C. (2011). Two pioneering projects early history computer-aided algorithmic composition. Computer Music Journal, 35 (3), 4056.
Aschauer, D. (2008). Algorithmic composition. Masters thesis, Vienna University Technology.
Assayag, G., & Dubnov, S. (2004). Using factor oracles machine improvisation. Soft
Computing - Fusion Foundations, Methodologies Applications, 8 (9), 604610.
Assayag, G., Rueda, C., Laurson, M., Agon, C., & Delerue, O. (1999). Computer-Assisted
composition IRCAM: PatchWork OpenMusic. Computer Music Journal,
23 (3), 5972.
Baeten, J. C. M. (2005). brief history process algebra. Theoretical Computer Science,
335 (23), 131146.
Baggi, D. L. (1991). Neurswing: intelligent workbench investigation swing
jazz. Computer, 24 (7), 6064.
Balaban, M., Ebciolu, K., & Laske, O. E. (1992). Understanding music AI : perspectives music cognition. MIT Press, Cambridge.
Ball, P. (2005). Making music numbers online. Nature News Online
(http://dx.doi.org/10.1038/050919-14).
Ball, P. (2012). Computer science: Algorithmic rapture. Nature, 488 (7412), 458.
564

fiAI Methods Algorithmic Composition

Baroni, M., & Jacoboni, C. (1978). Proposal grammar melody : Bach chorales.
Les Presses de lUniversit de Montral.
Bel, B. (1992). Modelling improvisatory compositional processes. Languages Design,
Formalisms Word, Image Sound, 1, 1126.
Bellgard, M. I., & Tsang, C. P. (1992). Harmonizing music using network Boltzmann
machines. Proceedings Annual Conference Artificial Neural Networks
Applications, pp. 321332, France.
Berg, P. (2011). Using AC Toolbox. Institute Sonology, Royal Conservatory,
Hague.
Beyls, P. (1989). musical universe cellular automata. Proceedings International Computer Music Conference, pp. 3441.
Beyls, P. (2003). Selectionist musical automata: Integrating explicit instruction evolutionary algorithms. Proceedings Brazilian Symposium Computer Music.
Beyls, P. (2007). Interaction self-organisation society musical agents. Proceedings European Conference Artificial Life.
Bidlack, R. (1992). Chaotic systems simple (but complex) compositional algorithms.
Computer Music Journal, 16 (3), 3347.
Biles, J. A. (1994). GenJam: genetic algorithm generating jazz solos. Proceedings
International Computer Music Conference.
Biles, J. A. (1998). Interactive GenJam: Integrating real-time performance genetic
algorithm. Proceedings International Computer Music Conference.
Biles, J. A., Anderson, P., & Loggi, L. (1996). Neural network fitness functions
musical IGA. Proceedings International Symposium Intelligent Industrial
Automation Soft Computing.
Bilotta, E., & Pantano, P. (2001). Artificial life music tells complexity. Proceedings
European Conference Artificial Life.
Bilotta, E., & Pantano, P. (2002). Synthetic harmonies: approach musical semiosis
means cellular automata. Leonardo, 35 (2), 153159.
Birchfield, D. A. (2003). Evolving intelligent musical materials. Ph.D. thesis, Columbia
University, New York.
Biyikoglu, K. M. (2003). Markov model chorale harmonization. Proceedings
Triennial ESCOM Conference.
Blackwell, T. M., & Bentley, P. (2002). Improvised music swarms. Proceedings
IEEE Conference Evolutionary Computation, pp. 14621467.
Boden, M. A. (2009). Computer models creativity. AI Magazine, 30 (3), 2334.
Boenn, G., Brain, M., De Vos, M., & Fitch, J. (2008). Automatic composition melodic
harmonic music Answer Set Programming. Proceedings International
Conference Logic Programming, pp. 160174.
Bolognesi, T. (1983). Automatic composition: Experiments self-similar music. Computer Music Journal, 7 (1), 2536.
565

fiFernndez & Vico

Bosma, M. (2005). Musicology virtual world: bottom approach study
musical evolution. Masters thesis, University Groningen University Plymouth.
Boulanger, R. C. (Ed.). (2000). Csound Book: Perspectives Software Synthesis,
Sound Design, Signal Processing, Programming. MIT Press.
Bown, O., & McCormack, J. (2010). Taming nature: tapping creative potential
ecosystem models arts. Digital Creativity, 21 (4), 215231.
Bown, O., & Wiggins, G. A. (2005). Modelling musical behaviour cultural-evolutionary
system. Proceedings International Joint Conference Artificial Inteligence.
Brooks, F. P., Hopkins, A. L., Neumann, P. G., & Wright, W. V. (1957). experiment
musical composition. IRE Transactions Electronic Computers, EC-6 (3), 175182.
Browne, T. M., & Fox, C. (2009). Global Expectation-Violation fitness function evolutionary composition. Proceedings Conference Applications Evolutionary
Computation, pp. 538546.
Bryden, K. (2006). Using Human-in-the-Loop evolutionary algorithm create DataDriven music. Proceedings IEEE Conference Evolutionary Computation,
pp. 20652071.
Bulley, J., & Jones, D. (2011). Variable 4: dynamical composition weather systems.
Proceedings International Computer Music Conference.
Burns, K. (1994). History Development Algorithms Music Composition,
1957-1993. Ph.D. thesis, Ball State University.
Burraston, D., & Edmonds, E. (2005). Cellular automata generative electronic music
sonic art: historical technical review. Digital Creativity, 16 (3), 165185.
Burton, A. R. (1998). Hybrid Neuro-Genetic Pattern Evolution System Applied Musical
Composition. Ph.D. thesis, University Surrey.
Burton, A. R., & Vladimirova, T. (1999). Generation musical sequences genetic
techniques. Computer Music Journal, 23 (4), 5973.
Buttram, T. (2003). DirectX 9 Audio Exposed: Interactive Audio Development, chap. Beyond Games: Bringing DirectMusic Living Room. Wordware Publishing Inc.
Carpentier, G., & Bresson, J. (2010). Interacting symbol, sound, feature spaces
orchide, computer-aided orchestration environment. Computer Music Journal,
34 (1), 1027.
Chemillier, M. (2004). Toward formal study jazz chord sequences generated Steedmans grammar. Soft Computing - Fusion Foundations, Methodologies Applications, 8 (9), 617622.
Chemillier, M., & Truchet, C. (2001). Two musical CSPs. Proceedings International
Conference Principles Practice Constraint Programming.
Chen, C. C. J., & Miikkulainen, R. (2001). Creating melodies evolving recurrent neural
networks. Proceedings International Joint Conference Neural Networks,
pp. 22412246.
566

fiAI Methods Algorithmic Composition

Chusid, I. (1999). Beethoven-in-a-box: Raymond scotts electronium. Contemporary Music
Review, 18 (3), 914.
Coca, A. E., Romero, R. A. F., & Zhao, L. (2011). Generation composed musical structures recurrent neural networks based chaotic inspiration. Proceedings
International Joint Conference Neural Networks, pp. 32203226.
Coghlan, A. (2012).
215 (2872), 7.

Computer composer honours Turings centenary.

New Scientist,

Cohen, J. E. (1962). Information theory music. Behavioral Science, 7 (2), 137163.
Collins, N. (2009). Musical form algorithmic composition. Contemporary Music Review,
28 (1), 103114.
Conklin, D. (2003). Music generation statistical models. Proceedings Symposium Artificial Intelligence Creativity Arts Science.
Conklin, D., & Witten, I. (1995). Multiple viewpoint systems music prediction. Journal
New Music Research, 24 (1), 5173.
Cope, D. (1992). Computer modeling musical intelligence EMI. Computer Music
Journal, 16 (2), 6983.
Cope, D. (2000). Algorithmic Composer. A-R Editions.
Cope, D. (2005). Computer Models Musical Creativity. MIT Press, Cambridge.
Courtot, F. (1990). constraint-based logic program generating polyphonies. Proceedings International Computer Music Conference, pp. 292294.
Cruz-Alczar, P. P., & Vidal-Ruiz, E. (1998). Learning regular grammars model musical style: Comparing different coding schemes. Proceedings International
Colloquium Grammatical Inference, pp. 211222.
Dahlstedt, P. (2007). Autonomous evolution complete piano pieces performances.
Proceedings European Conference Artificial Life.
Dahlstedt, P., & Nordahl, M. G. (2001). Living melodies: Coevolution sonic communication. Leonardo, 34 (3), 243248.
Dalhoum, A. A., Alfonseca, M., Cebrin, M., Snchez-Alfonso, R., & Ortega, A. (2008).
Computer-generated music using grammatical evolution. Proceedings MiddleEast Simulation Multiconference, pp. 5560.
Davismoon, S., & Eccles, J. (2010). Combining musical constraints Markov transition
probabilities improve generation creative musical structures. Proceedings
European Conference Applications Evolutionary Computation.
De Prisco, R., Zaccagnino, G., & Zaccagnino, R. (2010). EvoBassComposer: multiobjective genetic algorithm 4-voice compositions. Proceedings Genetic
Evolutionary Computation Conference.
Degazio, B. (1996). evolution musical organisms. Leonardo Music Journal, 7, 2733.
Daz-Jerez, G. (2000). Algorithmic Music Using Mathematical Models. Ph.D. thesis, Manhattan School Music.
567

fiFernndez & Vico

Daz-Jerez, G. (2011). Composing Melomics: Delving computational world
musical inspiration. Leonardo Music Journal, 21, 1314.
Dobrian, C. (1993). Music artificial intelligence. Unpublished manuscript. Available
http://music.arts.uci.edu/dobrian/CD.music.ai.htm.
Doornbusch, P. (2002). brief survey mapping algorithmic composition. Proceedings
International Computer Music Conference.
Dorin, A. (2000). Boolean networks generation rhythmic structure. Proceedings
Australasian Computer Music Conference, pp. 3845.
Dorin, A. (2002). Liquiprism : Generating polyrhythms cellular automata. Proceedings International Conference Auditory Display.
Drewes, F., & Hgberg, J. (2007). algebra tree-based music generation. Proceedings
International Conference Algebraic Informatics, pp. 172188.
Dubnov, S., Assayag, G., Lartillot, O., & Bejerano, G. (2003). Using machine-learning
methods musical style modeling. Computer, 36 (10), 7380.
DuBois, R. L. (2003). Applications Generative String-Substitution Systems Computer
Music. Ph.D. thesis, Columbia University.
Duff, M. O. (1989). Backpropagation Bachs 5th cello suite (Sarabande). Proceedings
International Joint Conference Neural Networks, p. 575.
Ebciolu, K. (1980). Computer counterpoint. Proceedings International Computer
Music Conference.
Ebciolu, K. (1988). expert system harmonizing four-part chorales. Computer Music
Journal, 12 (3), 4351.
Edwards, M. (2011). Algorithmic composition: computational thinking music. Communications ACM, 54 (7), 5867.
Eigenfeldt, A., & Pasquier, P. (2010). Realtime generation harmonic progressions using controlled Markov selection. Proceedings International Conference
Computational Creativity.
Eldridge, A., & Dorin, A. (2009). Filterscape: Energy recycling creative ecosystem.
Proceedings Conference Applications Evolutionary Computation.
Eno, B. (1996). Generative Music, speech Imagination Conference. Available
http://www.inmotionmagazine.com/eno1.html.
Esp, D., Ponce de Len, P. J., Prez-Sancho, C., Rizo, D., nesta, J. I., Moreno-Seco, F., &
Pertusa, A. (2007). cooperative approach style-oriented music composition.
Proceedings International Joint Conference Artificial Inteligence.
Farbood, M., & Schoner, B. (2001). Analysis synthesis Palestrina-style counterpoint
using Markov chains. Proceedings International Computer Music Conference.
Farnell, A. (2007). introduction procedural audio itsapplication computer
games. Proceedings Audio Mostly Conference.
Farrell, S., Jan Wagenmakers, E., & Ratcliff, R. (2006). 1/f noise human cognition:
ubiquitous, mean?. Psychonomic Bulletin & Review, 13 (4), 737741.
568

fiAI Methods Algorithmic Composition

Feulner, J., & Hrnel, D. (1994). MELONET: neural networks learn harmony-based
melodic variations. Proceedings International Computer Music Conference,
pp. 121124, San Francisco.
Fox, C. W. (2006). Genetic hierarchical music structures. Proceedings International
Florida Artificial Research Society Conference.
Franklin, J. (2001). Multi-phase learning jazz improvisation interaction. Proceedings Biennial Symposium Arts Technology.
Freitas, A., & Guimares, F. (2011). Melody harmonization evolutionary music using
multiobjective genetic algorithms. Proceedings Sound Music Computing
Conference.
Fry, C. (1984). Flavors band: language specifying musical style. Computer Music
Journal, 8 (4), 2034.
Garay Acevedo, A. (2004). Fugue composition counterpoint melody generation using
genetic algorithms. Proceedings International Conference Computer Music
Modeling Retrieval, pp. 96106.
Gartland-Jones, A. (2002). genetic algorithm think like composer?. Proceedings
Generative Art Conference.
Geem, Z. W., & Choi, J. Y. (2007). Music composition using harmony search algorithm.
Proceedings Conference Applications Evolutionary Computation, pp.
593600.
Geis, M., & Middendorf, M. (2008). Creating melodies baroque harmonies ant
colony optimization. International Journal Intelligent Computing Cybernetics,
1 (2), 213218.
Gero, J. S. (2000). Computational models innovative creative design processes.
Technological Forecasting Social Change, 64 (23), 183196.
Gibson, P. M., & Byrne, J. A. (1991). NEUROGEN, musical composition using genetic
algorithms cooperating neural networks. Proceedings International Conference Artificial Neural Networks, pp. 309313.
Gill, S. (1963). technique composition music computer. Computer
Journal, 6 (2), 129133.
Gillick, J., Tang, K., & Keller, R. M. (2009). Learning jazz grammars. Proceedings
Sound Music Computing Conference, pp. 125130.
Gjerdingen, R. (1988). Explorations Music, Arts, Ideas: Essays Honor
Leonard B. Meyer, chap. Concrete musical knowledge computer program
species counterpoint, pp. 199228. Pendragon Press.
Gogins, M. (1991). Iterated functions systems music. Computer Music Journal, 15 (1),
4048.
Gogins, M. (2006). Score generation voice-leading chord spaces. Proceedings
International Computer Music Conference.
569

fiFernndez & Vico

Goldman, C., Gang, D., Rosenschein, J., & Lehmann, D. (1996). NETNEG: hybrid
interactive architecture composing polyphonic music real time. Proceedings
International Computer Music Conference, pp. 133140.
Grachten, M. (2001). JIG : jazz improvisation generator. Proceedings Workshop
Current Research Directions Computer Music, pp. 16. Audiovisual Institute-UPF.
Gwee, N. (2002). Complexity Heuristics Rule-Based Algorithmic Music Composition.
Ph.D. thesis, Louisiana State University.
Hamanaka, M., Hirata, K., & Tojo, S. (2008). Melody morphing method based GTTM.
Proceedings International Computer Music Conference, pp. 155158.
Harris, R. (2008). Algorithmic composition jazz. Masters thesis, University Bath.
Hartmann, P. (1990). Natural selection musical identities. Proceedings International Computer Music Conference.
Haus, G., & Sametti, A. (1991). Scoresynth: system synthesis music scores
based Petri nets music algebra. IEEE Computer, 24 (7), 5660.
Herman, M. (1993). Deterministic chaos, iterative models, dynamical systems
application algorithmic composition. Proceedings International Computer
Music Conference.
Herremans, D., & Sorensena, K. (2012). Composing first species counterpoint variable
neighbourhood search algorithm. Journal Mathematics Arts, 6 (4), 169189.
Hild, H., Feulner, J., & Menzel, D. (1992). HARMONET: neural net harmonising
chorales style J.S. Bach. Proceedings Conference Neural Information Processing Systems.
Hiller, L. A., & Isaacson, L. M. (1958). Musical composition High-Speed digital
computer. Journal Audio Engineering Society, 6 (3), 154160.
Hinojosa-Chapel, R. (2003). Realtime algorithmic music systems fractals chaotic
functions: Toward active musical instrument. Masters thesis, Universitat Pompeu
Fabra.
Hirata, K., & Aoyagi, T. (1988). realize jazz feelings: logic programming approach. Proceedings International Conference Fifth Generation Computer
Systems.
Hoffmann, P. (2002). Towards "automated art": Algorithmic processes xenakis compositions. Contemporary Music Review, 21 (2-3), 121131.
Holm, F. (1990). CESAM: concept engine synthesis audio music. Proceedings
International Computer Music Conference.
Holtzman, S. R. (1981). Using generative grammars music composition. Computer
Music Journal, 5 (1), 5164.
Hoover, A. K., Szerlip, P. A., Norton, M. E., Brindle, T. A., Merritt, Z., & Stanley, K. O.
(2012). Generating complete multipart musical composition single monophonic melody functional scaffolding. Proceedings International Conference Computational Creativity.
570

fiAI Methods Algorithmic Composition

Hrnel, D., & Degenhardt, P. (1997). neural organist improvising baroque-style melodic
variations. Proceedings International Computer Music Conference, pp. 430
433.
Hrnel, D., & Ragg, T. (1996). connectionist model evolution styles harmonization. Proceedings International Conference Music Perception
Cognition.
Horner, A., & Ayers, L. (1995). Harmonization musical progressions genetic algorithms. Proceedings International Computer Music Conference.
Horner, A., & Goldberg, D. E. (1991). Genetic algorithms computer-assisted music
composition. Proceedings International Conference Genetic Algorithms,
pp. 337441.
Horowitz, M. D. (1994). Generating rhythms genetic algorithms. Proceedings
AAAI National Conference Artificial intelligence, Menlo Park.
Horowitz, M. D. (1995). Representing musical knowledge. Ph.D. thesis, Columbia University.
Hs, K. J., & Hs, A. (1991). Self-similarity "1/f noise" called music. Proceedings
National Academy Sciences United States America, 88 (8), 35073509.
Hunt, A., Kirk, R., & Orton, R. (1991). Musical applications cellular automata workstation. Proceedings International Computer Music Conference.
Jacob, B. L. (1995). Composing genetic algorithms. Proceedings International
Computer Music Conference.
Jensen, J. H. (2011). Evolutionary music composition: quantitative approach. Masters
thesis, Norwegian University Science Technology.
Johanson, B., & Poli, R. (1998). GP-music: interactice genetic programming system
music generation automated fitness raters. Proceedings Annual
Conference Genetic Programming, pp. 181186.
Johnson, M., Tauritz, D. R., & Wilkerson, R. (2004). Evolutionary computation applied
melody generation. Proceedings Artificial Neural Networks Engineering
(ANNIE) Conference.
Jones, K. (1980). space grammar stochastic generation Multi-Dimensional
structures. Proceedings International Computer Music Conference.
Jones, K. (1981). Compositional applications stochastic processes. Computer Music
Journal, 5 (2), 4561.
Jones, K. (1989). Generative models computer-assisted musical composition. Contemporary Music Review, 3 (1), 177196.
Kaliakatsos-Papakostas, M. A., Epitropakis, M. G., Floros, A., & Vrahatis, M. N. (2012).
Interactive evolution 8-Bit melodies genetic programming towards finding aesthetic measures sound evolutionary biologically inspired music, sound, art
design. Proceedings International Conference Evolutionary Biologically Inspired Music, Sound, Art Design, pp. 141152.
571

fiFernndez & Vico

Keller, R. M., & Morrison, D. R. (2007). grammatical approach automatic improvisation. Proceedings Sound Music Computing Conference, pp. 330337.
Khalifa, Y. M. A., Khan, B. K., Begovic, J., Wisdom, A., & Wheeler, A. M. (2007). Evolutionary music composer integrating formal grammar. Proceedings Genetic
Evolutionary Computation Conference, pp. 25192526, New York.
Kippen, J., & Bel, B. (1989). identification modelling percussion language,
emergence musical concepts machine-learning experimental set-up.
Computers Humanities, 23 (3), 199214.
Kirke, A., & Miranda, E. (2009). survey computer systems expressive music
performance. ACM Computing Surveys, 42 (1), 3:13:41.
Kitani, K. M., & Koike, H. (2010). ImprovGenerator: Online grammatical induction onthe-fly improvisation accompaniment. Proceedings International Conference
New Interfaces Musical Expression.
Klinger, R., & Rudolph, G. (2006). Evolutionary composition music learned melody
evaluation. Proceedings WSEAS International Conference Computational
Intelligence, Man-Machine Systems Cybernetics, pp. 234239, Stevens Point, Wisconsin, USA.
Kohonen, T., Laine, P., Tiits, K., & Torkkola, K. (1991). Music Connectionism, chap.
Nonheuristic Automatic Composing Method, pp. 229242. MIT Press, Cambridge.
Kugel, P. (1990). Myhills Thesis: Theres computing musical thinking. Computer Music Journal, 14 (3), 1225.
Laine, P. (2000). Method Generating Musical Motion Patterns. Ph.D. thesis, University Helsinki.
Laine, P., & Kuuskankare, M. (1994). Genetic algorithms musical style oriented generation. Proceedings IEEE Conference Evolutionary Computation, pp.
858862.
Langston, P. (1989). Six techniques algorithmic music composition. Proceedings
International Computer Music Conference.
Laurson, M., & Kuuskankare, M. (2000). Towards idiomatic instrumental writing: constraint based approach. Proceedings Annual Symposium Systems Research
Arts.
Leach, J., & Fitch, J. (1995). Nature, music, algorithmic composition. Computer Music
Journal, 19 (2), 2333.
Lerdahl, F., Jackendoff, R., & Jackendoff, R. S. (1983). Generative Theory Tonal
Music. MIT Press, Cambridge.
Levitt, D. A. (1981). melody description system jazz improvisation. Masters thesis,
Massachusetts Institute Technology.
Lewis, J. P. (1991). Music Connectionism, chap. Creation refinement problem algorithmic music composition. MIT Press, Cambridge.
572

fiAI Methods Algorithmic Composition

Lidov, D., & Gabura, J. (1973). melody writing algorithm using formal language model.
Computer Studies Humanities Verbal Behavior, 4 (34), 138148.
Lo, M. Y. (2012). Evolving Cellular Automata Music Composition Trainable Fitness
Functions. Ph.D. thesis, University Essex.
Lo, M., & Lucas, S. M. (2006). Evolving musical sequences N-Gram based trainable fitness functions. Proceedings IEEE Conference Evolutionary Computation,
pp. 601608.
Lthe, M. (1999). Knowledge based automatic composition variation melodies
minuets early classical style. Proceedings Annual German Conference
Artificial Intelligence, pp. 159170.
Loy, G., & Abbott, C. (1985). Programming languages computer music synthesis, performance, composition. ACM Computing Surveys, 17 (2), 235265.
Lozano, L., Medaglia, A. L., & Velasco, N. (2009). Generation Pop-Rock chord sequences
using genetic algorithms variable neighborhood search. Proceedings
Conference Applications Evolutionary Computation, pp. 573578.
Lyon, D. (1995). Using stochastic Petri nets real-time Nth-order stochastic composition.
Computer Music Journal, 19 (4), 1322.
MacCallum, R. M., Mauch, M., Burt, A., & Leroi, A. M. (2012). Evolution music
public choice. Proceedings National Academy Sciences United States
America, 109 (30), 1208112086.
Maddox, T., & Otten, J. (2000). Using evolutionary algorithm generate Four-Part
18th century harmony. Proceedings WSEAS International Conference
Mathematics Computers Business Economics.
Manaris, B., Hughes, D., & Vassilandonakis, Y. (2011). Monterey mirror: Combining
Markov models, genetic algorithms, power laws. Proceedings IEEE
Conference Evolutionary Computation.
Manaris, B., Roos, P., Machado, P., Krehbiel, D., Pellicoro, L., & Romero, J. (2007).
corpus-based hybrid approach music analysis composition. Proceedings
AAAI National Conference Artificial intelligence, pp. 839845.
Manousakis, S. (2006). Musical L-Systems. Masters thesis, Royal Conservatory,
Hague.
Marques, V. M., Oliveira, V., Vieira, S., & Rosa, A. C. (2000). Music composition using genetic evolutionary algorithms. Proceedings IEEE Conference Evolutionary
Computation, pp. 714719.
Marques, V. M., Reis, C., & Machado, J. A. T. (2010). Interactive evolutionary computation
music. Proceedings IEEE International Conference Systems, Man
Cybernetics, pp. 35013507.
Martin, A., Jin, C. T., & Bown, O. (2012). Implementation real-time musical decisionmaker. Proceedings Australasian Computer Music Conference.
573

fiFernndez & Vico

Martin, A., Jin, C. T., van Schaik, A., & Martens, W. L. (2010). Partially observable Markov
decision processes interactive music systems. Proceedings International
Computer Music Conference.
Mason, S., & Saffle, M. (1994). L-Systems, melodies musical structure. Leonardo Music
Journal, 4, 3138.
Maurer, J. (1999). brief history algorithmic composition. Unpublished manuscript.
Available https://ccrma.stanford.edu/~blackrse/algorithm.html.
McAlpine, K., Miranda, E., & Hoggar, S. (1999). Making music algorithms: CaseStudy system. Computer Music Journal, 23 (2), 1930.
McCartney, J. (2002). Rethinking computer music language: SuperCollider. Computer
Music Journal, 26 (4), 6168.
McCormack, J. (1996). Grammar-Based music composition. Complexity International, 3,
320336.
McCormack, J. (2003a). Application L-systems Developmental Models Computer Art, Animation Music Synthesis. Ph.D. thesis, Monash University.
McCormack, J. (2003b). Evolving sonic ecosystems. Kybernetes, 32 (12), 184202.
McDermott, J., & OReilly, U. M. (2011). executable graph representation evolutionary generative music. Proceedings Genetic Evolutionary Computation
Conference, pp. 403410, New York.
McGuire, K. (2006). ArpEgg: rewriting grammar complex arpeggios. Proceedings
Generative Art Conference.
McIntyre, R. A. (1994). Bach box: evolution four part baroque harmony using genetic algorithm. Proceedings IEEE Conference Evolutionary
Computation, pp. 852857.
Melo, A. F. (1998). connectionist model tension chord progressions. Masters thesis,
University Edinburgh.
Miljkovic, K. (2007). Mathematica live performance: Mapping simple programs
music. Proceedings International Conference Mathematics Computation Music.
Millen, D. (1990). Cellular automata music. Proceedings International Computer
Music Conference.
Miranda, E. R. (1993). Cellular automata music: interdisciplinary project. Journal
New Music Research, 22 (1), 321.
Miranda, E. R. (2002). Mimetic development intonation. Proceedings International Conference Music Artificial Intelligence.
Miranda, E. R. (2007). Evolutionary Computer Music, chap. Cellular Automata Music:
Sound Synthesis Musical Forms, pp. 170193. Springer-Verlag London.
Miranda, E. R., & Biles, J. A. (Eds.). (2007). Evolutionary computer music. Springer-Verlag
London.
574

fiAI Methods Algorithmic Composition

Miranda, E. R., Kirby, S., & Todd, P. M. (2003). computational models evolution music: origins musical taste emergence grammars.
Contemporary Music Review, 22 (3), 91111.
Moorer, J. A. (1972). Music computer composition. Communications ACM,
15 (2), 104113.
Morales, E., & Morales, R. (1995). Learning musical rules. Proceedings International Joint Conference Artificial Inteligence.
Morgan, N. (2007). Transformation mapping L-Systems data composition
large-scale instrumental work. Proceedings European Conference Artificial
Life.
Moroni, A., Manzolli, J., Zuben, F. V., & Gudwin, R. (2000). Vox Populi: interactive
evolutionary system algorithmic music composition. Leonardo Music Journal, 10,
4954.
Morris, D., Simon, I., & Basu, S. (2008). Exposing parameters trained dynamic model
interactive music creation. Proceedings AAAI National Conference
Artificial intelligence, pp. 784791.
Mozer, M. (1991). Music Connectionism, chap. Connectionist music composition based
melodic, stylistic, psychophysical constraints, pp. 195211. MIT Press,
Cambridge.
Nelson, G. L. (1993). Sonomorphs: application genetic algorithms growth
development musical organisms. Proceedings Biennial Symposium Arts
Technology, pp. 155169.
Nelson, G. L. (1996). Real time transformation musical material fractal algorithms.
Computers & Mathematics Applications, 1, 109116.
Nettheim, N. (1992). spectral analysis melody. Journal New Music Research,
21 (2), 135148.
Nettheim, N. (1997). bibliography statistical applications musicology. Musicology
Australia, 20 (1), 94106.
Nierhaus, G. (2009). Algorithmic Composition: Paradigms Automated Music Generation.
Springer Berlin / Heidelberg.
Nishijimi, M., & Watanabe, K. (1993). Interactive music composer based neural networks. Fujitsu Scientific Technical Journal, 29 (2), 189192.
North, T. (1991). technical explanation theme variations: computer music work
utilizing network compositional algorithms. Ex Tempore, 5 (2).
Olarte, C., Rueda, C., & Valencia, F. D. (2009). New Computational Paradigms Computer Music, chap. Concurrent Constraint Calculi: Declarative Paradigm Modeling Music Systems. Editions Delatour France.
Olson, H. F. (1961). Aid music composition employing random probability system.
Journal Acoustical Society America, 33, 11631170.
575

fiFernndez & Vico

Ortega, A., Snchez, R., & Alfonseca, M. (2002). Automatic composition music means
grammatical evolution. Proceedings Conference APL.
Ovans, R., & Davison, R. (1992). interactive Constraint-Based expert assistant music
composition. Proceedings Canadian Conference Artificial Intelligence, pp.
7681.
zcan, E., & Eral, T. (2008). genetic algorithm generating improvised music.
Proceedings International Conference Artificial Evolution, pp. 266277.
Pachet, F. (2002). Interacting musical learning system: Continuator. Proceedings International Conference Music Artificial Intelligence, pp. 103108.
Pachet, F., & Roy, P. (1995). Mixing constraints objects: case study automatic
harmonization. Proceedings Conference Technology Object-Oriented
Languages Systems.
Pachet, F., & Roy, P. (2001). Musical harmonization constraints: survey. Constraints,
6 (1), 719.
Pachet, F., Roy, P., & Barbieri, G. (2011). Finite-length Markov processes constraints.
Proceedings International Joint Conference Artificial Inteligence.
Padberg, H. A. (1964). Computer-composed canon free-fugue. Ph.D. thesis, Saint Louis
University, St. Louis.
Papadopoulos, G., & Wiggins, G. (1998). genetic algorithm generation jazz
melodies. Proceedings Finnish Conference Artificial Intelligence (STeP).
Papadopoulos, G., & Wiggins, G. (1999). AI methods algorithmic composition: survey,
critical view future prospects. Proceedings Symposium Musical
Creativity, pp. 110117.
Parikh, T. (2003). Iris: artificially intelligent real-time improvisation system. Masters
thesis, Emory University.
Pazos, A., Santos del Riego, A., Dorado, J., & Romero Caldalda, J. J. (1999). Genetic music
compositor. Proceedings IEEE Conference Evolutionary Computation, pp.
885890.
Pearce, M., Meredith, D., & Wiggins, G. (2002). Motivations methodologies automation compositional process. Music Scienti, 6 (2), 119147.
Pearce, M., & Wiggins, G. (2001). Towards framework evaluation machine
compositions. Proceedings Symposium Artificial Intelligence Creativity
Arts Science, pp. 2232.
Peck, J. M. (2011). Explorations algorithmic composition: Systems composition
examination several original works. Masters thesis, State University New York,
College Oswego.
Pennycook, B. (1985). Computer-music interfaces: survey. ACM Computing Surveys,
17 (2), 267289.
Pereira, F., Grilo, C., Macedo, L., & Cardoso, A. (1997). Composing music case-based
reasoning. Proceedings Conference Computational Models Creative
Cognition.
576

fiAI Methods Algorithmic Composition

Pestana, P. (2012). Lindenmayer systems harmony fractals. Chaotic Modeling
Simulation, 1 (1), 9199.
Phon-Amnuaisuk, S. (2002). Control language harmonisation process. Proceedings
International Conference Music Artificial Intelligence, pp. 155167.
Phon-Amnuaisuk, S. (2010). Investigating music pattern formations heterogeneous
cellular automata. Journal New Music Research, 39 (3), 253267.
Phon-Amnuaisuk, S., Law, E. H., & Kuan, H. C. (2007). Evolving music generation
SOM-fitness genetic programming. Proceedings Conference Applications
Evolutionary Computation, pp. 557566.
Phon-Amnuaisuk, S., Tuson, A., & Wiggins, G. (1999). Evolving musical harmonisation.
Proceedings International Conference Artificial Neural Nets Genetic
Algorithms.
Pinkerton, R. C. (1956). Information theory melody. Scientific American, 194 (2),
7787.
Polito, J., Daida, J. M., & Bersano Begey, T. F. (1997). Musica ex machina: Composing
16th-Century counterpoint genetic programming symbiosis. Proceedings
International Conference Evolutionary Programming, pp. 113124.
Ponsford, D., Wiggins, G., & Mellish, C. (1999). Statistical learning harmonic movement.
Journal New Music Research, 28 (2), 150177.
Pope, S. T. (1986). Music notations representation musical structure knowledge. Perspectives New Music, 24 (2), 156189.
Pope, S. T. (1991). tool manipulating expressive structural hierarchies music
(or: "T-R trees MODE: tree editor based loosely Freds theory").
Proceedings International Computer Music Conference.
Pope, S. T. (1993). Music Processing, chap. Music composition editing computer,
pp. 2572. Oxford University Press.
Pope, S. T. (1995). Fifteen years computer-assisted composition. Proceedings
Brazilian Symposium Computer Music.
Pressing, J. (1988). Nonlinear maps generators musical design. Computer Music
Journal, 12 (2), 3546.
Prusinkiewicz, P. (1986). Score generation L-systems. Proceedings International Computer Music Conference, pp. 455457.
Prusinkiewicz, P., & Lindenmayer, A. (1990). algorithmic beauty plants. SpringerVerlag New York.
Puckette, M. (2002). Max Seventeen. Computer Music Journal, 26 (4), 3143.
Putnam, J. (1994). Genetic programming music. Tech. rep., New mexico institute
mining technology.
Quick, D. (2010). Generating music using concepts Schenkerian analysis chord
spaces. Tech. rep., Yale University.
577

fiFernndez & Vico

Rader, G. M. (1974). method composing simple traditional music computer.
Communications ACM, 17 (11), 631638.
Ralley, D. (1995). Genetic algorithms tool melodic development. Proceedings
International Computer Music Conference, pp. 501502.
Ramalho, G., & Ganascia, J.-G. (1994). Simulating creativity jazz performance.
Proceedings AAAI National Conference Artificial intelligence, pp. 108113,
Menlo Park.
Ramrez, R., & Peralta, J. (1998). constraint-based melody harmonizer. Proceedings
Workshop Constraints Artistic Applications.
Reddin, J., McDermott, J., & ONeill, M. (2009). Elevated pitch: Automated grammatical
evolution short compositions applications evolutionary computing. Proceedings
Conference Applications Evolutionary Computation, pp. 579584.
Reisig, W. (1998). Elements Distributed Algorithms: Modeling Analysis Petri
Nets. Springer.
Rennie, J. (2010). Ray Kurzweils slippery futurism. IEEE Spectrum, 47 (12), 2428.
Ribeiro, P., Pereira, F. C., Ferrand, M., & Cardoso, A. (2001). Case-based melody generation
MuzaCazUza. Proceedings Symposium Artificial Intelligence
Creativity Arts Science, pp. 6774.
Ricanek, K., Homaifar, A., & Lebby, G. (1993). Genetic algorithm composes music.
Proceedings Southeastern Symposium System Theory, pp. 223227.
Riecken, D. (1998). WOLFGANG: "emotions" architecture enable learning
compose music. Proceedings International Conference Society
Adaptive Behavior.
Ritchie, G. (2007). empirical criteria attributing creativity computer program.
Journal Artificial Intelligence, Philosophy Cognitive Science, 17 (1), 6799.
Roads, C. (1977). Composing grammars. Proceedings International Computer
Music Conference.
Roads, C. (1979). Grammars representations music. Computer Music Journal, 3 (1),
4855.
Roads, C. (1985). Research music artificial intelligence. ACM Computing Surveys,
17 (2), 163190.
Roads, C. (Ed.). (1992). Music Machine: Selected Readings Computer Music
Journal. MIT Press.
Roads, C. (2004). Microsound. MIT Press.
Roads, C., & Strawn, J. (Eds.). (1985). Foundations computer music. MIT Press.
Ross, B. J. (1995). process algebra stochastic music composition. Proceedings
International Computer Music Conference.
Rothgeb, J. (1968). Harmonizing unfigured bass: computational Study. Ph.D. thesis,
Yale University.
578

fiAI Methods Algorithmic Composition

Rowe, R. (1992). Interactive Music Systems: Machine Listening Composing. MIT
Press, Cambridge.
Rueda, C., lvarez, G., Quesada, L. O., Tamura, G., Valencia, F., Daz, J. F., & Assayag,
G. (2001). Integrating constraints concurrent objects musical applications:
calculus visual language. Constraints, 6 (1), 2152.
Rueda, C., Assayag, G., & Dubnov, S. (2006). concurrent constraints factor oracle model
music improvisation. Proceedings Latin American Informatics Conference.
Rueda, C., Lindberg, M., Laurson, M., Bloch, G., & Assayag, G. (1998). Integrating constraint programming visual musical composition languages. Proceedings
Workshop Constraints Artistic Applications.
Sabater, J., Arcos, J., & Lpez de Mntaras, R. (1998). Using rules support case-based
reasoning harmonizing melodies. Proceedings AAAI Spring Symposium
Multimodal Reasoning, pp. 147151.
Snchez-Quintana, C., Moreno-Arcas, F., Albarracn-Molina, D., Fernndez, J. D., & Vico,
F. (2013). Melomics: case-study AI Spain. AI Magazine, 34 (3), 99103.
Sandred, O. (2004). Interpretation everyday gestures composing rules. Proceedings Music Music Science Conference.
Sandred, O. (2010). PWMC, constraint-solving system generating music scores. Computer Music Journal, 34 (2), 824.
Santos, A., Arcay, B., Dorado, J., Romero, J. J., & Rodrguez, J. A. (2000). Evolutionary
computation systems musical composition. Proceedings International
Conference Acoustic Music: Theory Applications.
Sastry, A. (2011). N-gram modeling tabla sequences using variable-length hidden Markov
models improvisation composition. Masters thesis, Georgia Institute Technology.
Scaletti, C. (2002). Computer music languages, Kyma, future. Computer Music
Journal, 26 (4), 6982.
Schmidl, H. (2008). Pseudo-Genetic algorithmic composition. Proceedings International Conference Genetic Evolutionary Methods.
Schottstaedt, W. (1989). Current directions computer music research, chap. Automatic
Counterpoint, pp. 199214. MIT Press, Cambridge.
Schulze, W. (2009). Formal Language Theory Approach Music Generation. Ph.D.
thesis, Stellenbosch University.
Schwanauer, S. (1993). Machine models music, chap. learning machine tonal composition, pp. 511532. MIT Press, Cambridge.
Schwanauer, S. M., & Levitt, D. A. (1993). Machine Models Music. MIT Press,
Cambridge.
Shao, J., McDermott, J., ONeill, M., & Brabazon, A. (2010). Jive: generative, interactive, virtual, evolutionary music system applications evolutionary computation.
Proceedings Conference Applications Evolutionary Computation, pp.
341350.
579

fiFernndez & Vico

Shibata, N. (1991). neural network-based method chord/note scale association
melodies. NEC Research Development, 32 (3), 453459.
Simoni, M., & Dannenberg, R. B. (2013). Algorithmic Composition: Guide Composing
Music Nyquist. University Michigan Press.
Soddell, F., & Soddell, J. (2000). Microbes music. Proceedings Pacific Rim
International Conference Artificial Intelligence.
Sowa, J. F. (1956). machine compose music. Instruction manual GENIAC, Oliver
Garfield Company, Inc.
Spangler, R. R. (1999). Rule-Based Analysis Generation Music. Ph.D. thesis, California Institute Technology.
Spector, L., & Alpern, A. (1994). Criticism, culture, automatic generation
artworks. Proceedings AAAI National Conference Artificial intelligence,
pp. 38, Menlo Park.
Spector, L., & Alpern, A. (1995). Induction recapitulation deep musical structure.
Proceedings International Joint Conference Artificial Inteligence, pp. 4148.
Stanley, K., & Miikkulainen, R. (2003). taxonomy artificial embryogeny. Artificial
Life, 9 (2), 93130.
Steedman, M. J. (1984). generative grammar jazz chord sequences. Music Perception:
Interdisciplinary Journal, 2 (1), 5277.
Steels, L. (1979). Reasoning modeled society communicating experts. Masters
thesis, Massachusetts Institute Technology, Cambridge.
Steels, L. (1986). Learning craft musical composition. Proceedings International Computer Music Conference.
Stieler, W. (2012). Die mozart-Maschine. Technology Review (German edition), 12/2012,
2634.
Supper, M. (2001). remarks algorithmic composition. Computer Music Journal,
25 (1), 4853.
Thom, B. (2000). BoB: interactive improvisational music companion. Proceedings
International Conference Autonomous Agents, pp. 309316, New York.
Thomas, M. T. (1985). Vivace: rule based AI system composition. Proceedings
International Computer Music Conference, pp. 267274.
Thomas, M. T., Chatterjee, S., & Maimone, M. W. (1989). Cantabile: rule-based system
composing melody. Proceedings International Computer Music Conference.
Thornton, C. (2009). Hierarchical Markov modelling generative music. Proceedings
International Computer Music Conference.
Thywissen, K. (1999). GeNotator: environment exploring application evolutionary techniques computer-assisted composition. Organised Sound, 4 (2), 127133.
Tipei, S. (1975). MP1: computer program music composition. Proceedings
Annual Music Computation Conference.
580

fiAI Methods Algorithmic Composition

Todd, P. M. (1989). connectionist approach algorithmic composition. Computer Music
Journal, 13 (4), 2743.
Todd, P. M., & Loy, D. G. (1991). Music Connectionism. MIT Press, Cambridge.
Toiviainen, P. (1995). Modeling target-note technique bebop-style jazz improvisation:
artificial neural network approach. Music Perception: Interdisciplinary Journal,
12 (4), 399413.
Toiviainen, P. (2000). Readings Music Artificial Intelligence, chap. Symbolic AI
versus Connectionism Music Research, pp. 4767. Harwood Academic Publishers.
Tokui, N., & Iba, H. (2000). Music composition interactive evolutionary computation.
Proceedings Generative Art Conference.
Tominaga, K., & Setomoto, M. (2008). artificial-chemistry approach generating
polyphonic musical phrases. Proceedings Conference Applications
Evolutionary Computation, pp. 463472.
Towsey, M. W., Brown, A. R., Wright, S. K., & Diederich, J. (2001). Towards melodic
extension using genetic algorithms. Educational Technology & Society, 4 (2), 5465.
Trivio Rodrguez, J. L., & Morales-Bueno, R. (2001). Using multiattribute prediction
suffix graphs predict generate music. Computer Music Journal, 25 (3), 6279.
Truchet, C., Assayag, G., & Codognet, P. (2003). OMClouds, heuristic solver musical
constraints. Proceedings International Conference Metaheuristics.
Tsang, C. P., & Aitken, M. (1991). Harmonizing music discipline constraint logic
programming. Proceedings International Computer Music Conference.
Ulrich, J. W. (1977). analysis synthesis jazz computer. Proceedings
International Joint Conference Artificial Inteligence, pp. 865872.
Unehara, M., & Onisawa, T. (2001). Composition music using human evaluation.
Proceedings IEEE International Conference Fuzzy Systems, pp. 12031206.
Ventrella, J. J. (2008). Art Artificial Evolution, chap. Evolving Structure Liquid
Music, pp. 269288. Springer Berlin / Heidelberg.
Verbeurgt, K., Fayer, M., & Dinolfo, M. (2004). hybrid Neural-Markov approach
learning compose music example. Proceedings Canadian Conference
Advances Artificial Intelligence, pp. 480484.
Visell, Y. (2004). Spontaneous organisation, pattern models, music. Organised Sound,
9 (02), 151165.
Voss, R. F., & Clarke, J. (1978). 1/f noise music: Music 1/f noise. Journal
Acoustical Society America, 63, 258263.
Walker, W. F. (1994). conversation-based framework musical improvisation. Ph.D.
thesis, University Illinois.
Wallin, N. L., & Merker, B. (2001). Origins Music. MIT Press.
Waschka, R. (1999). Avoiding fitness bottleneck: Using genetic algorithms compose
orchestral music. Proceedings International Computer Music Conference, pp.
201203.
581

fiFernndez & Vico

Watson, L. A. (2008). Algorithmic composition flute accompaniment. Masters
thesis, University Bath.
Werner, M., & Todd, P. M. (1997). many love songs: Sexual selection evolution
communication. Proceedings European Conference Artificial Life.
Widmer, G. (1992). Qualitative perception modeling intelligent musical learning. Computer Music Journal, 16 (2), 5168.
Wiggins, G. A. (1998). use constraint systems musical composition. Proceedings
Workshop Constraints Artistic Applications.
Wiggins, G. A. (2008). Computer models musical creativity: review computer models
musical creativity David Cope. Literary Linguistic Computing, 23 (1), 109
116.
Wilson, A. J. (2009). symbolic sonification L-systems. Proceedings International Computer Music Conference, pp. 203206.
Wolkowicz, J., Heywood, M., & Keselj, V. (2009). Evolving indirectly represented melodies
corpus-based fitness evaluation. Proceedings Conference Applications
Evolutionary Computation, pp. 603608.
Wooller, R., & Brown, A. R. (2005). Investigating morphing algorithms generative
music. Proceedings International Conference Generative Systems
Electronic Arts.
Worth, P., & Stepney, S. (2005). Growing music: Musical interpretations L-Systems.
Proceedings Conference Applications Evolutionary Computation, pp.
545550.
Yi, L., & Goldsmith, J. (2007). Automatic generation four-part harmony. Proceedings
Conference Uncertainty Artificial Intelligence.
Yilmaz, A. E., & Telatar, Z. (2010). Note-against-note two-voice counterpoint means
fuzzy logic. Knowledge-Based Systems, 23 (3), 256266.
Zicarelli, D. (1987). Jam factory. Computer Music Journal, 11 (4), 1329.
Zimmermann, D. (2001). Modelling musical structures. Constraints, 6 (1), 5383.

582

fiJournal Artificial Intelligence Research 48 (2013) 305-346

Submitted 04/13; published 10/13

Global Model Concept-to-Text Generation
Ioannis Konstas
Mirella Lapata

IKONSTAS @ INF. ED . AC . UK
MLAP @ INF. ED . AC . UK

Institute Language, Cognition Computation,
School Informatics, University Edinburgh,
10 Crichton Street, EH8 9AB, Edinburgh UK

Abstract
Concept-to-text generation refers task automatically producing textual output
non-linguistic input. present joint model captures content selection (what say)
surface realization (how say) unsupervised domain-independent fashion. Rather
breaking generation process sequence local decisions, define probabilistic context-free grammar globally describes inherent structure input (a corpus
database records text describing them). recast generation task finding
best derivation tree set database records describe algorithm decoding
framework allows intersect grammar additional information capturing fluency
syntactic well-formedness constraints. Experimental evaluation several domains achieves results competitive state-of-the-art systems use domain specific constraints, explicit feature
engineering labeled data.

1. Introduction
Concept-to-text generation broadly refers task automatically producing textual output
non-linguistic input (Reiter & Dale, 2000). Depending application domain hand,
input may assume various representations including databases records, expert system knowledge bases, simulations physical systems on. Figure 1 shows input examples
corresponding text three domains: air travel, sportscasting weather forecast generation.
typical concept-to-text generation system implements pipeline architecture consisting
three core stages, namely content planning (selecting appropriate content input
determining structure target text), sentence planning (determining structure lexical content individual sentences), surface realization (rendering specification chosen
sentence planner surface string). Traditionally, components hand-engineered
order generate high quality text, expense portability scalability. thus surprise recent years witnessed growing interest automatic methods creating trainable
generation components. Examples include learning database records present
text (Duboue & McKeown, 2002; Barzilay & Lapata, 2005) verbalized
(Liang, Jordan, & Klein, 2009). Besides concentrating isolated components, approaches
emerged tackle concept-to-text generation end-to-end. Due complexity task,
models simplify generation process, e.g., creating output consists sentences, thus obviating need content planning, treating sentence planning surface
realization one component. common modeling strategy break generation process
sequence local decisions, learned separately (Reiter, Sripada, Hunter, & Davy, 2005a;
Belz, 2008; Chen & Mooney, 2008; Angeli, Liang, & Klein, 2010; Kim & Mooney, 2010).
c
2013
AI Access Foundation. rights reserved.

fiKONSTAS & L APATA

Pass

Bad Pass

Turn

Database:



pink3 pink7



pink7 purple3



pink7 purple3

Text:

pink3 passes ball pink7
(a) ROBO C

Database:

Temperature

Cloud Sky Cover

time
min mean max
06:00-21:00 9 15 21

time
percent (%)
06:00-09:00
25-50
09:00-12:00
50-75

Wind Speed

Wind Direction

time
min mean max
06:00-21:00 15 20 30
Text:

time
mode
06:00-21:00


Cloudy, temperatures 10 20 degrees. South wind around 20 mph.
(b) W EATHER G OV

Database:

Text:

Flight

Day Number

Month



denver boston

number dep/ar
9
departure

month
dep/ar
august departure

Condition

Search

arg1
arg2 type
arrival time 16:00 <

type
query flight

Give flights leaving Denver August ninth coming back Boston 4pm.
(c) ATIS

Figure 1: Input-output examples (a) sportscasting, (b) weather forecast generation, (c) query
generation air travel domain.

paper focus problem generating text database describe
end-to-end generation model performs content selection surface realization jointly.
specifically, input model set database records collocated textual descriptions.
Consider example Figure 1b. Here, records provide structured representation
weather specific time interval (e.g., temperature, wind speed direction)
text renders information natural language. formulate task creating text
corresponding database following generative process: database consists
306

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

set typed tuples (record, field, value), aim choose subset talk about.
naturally decomposes selecting sequence records, sequence fields within
record. Finally, field generate sequence words according value
field. Central approach jointly optimize process, rather breaking various
decisions local problems greedily trying solve one them.
this, define probabilistic context-free grammar (PCFG) captures structure
database verbalized. Generation boils finding best string
output captured best derivation tree licensed grammar. order ensure
generation output coherent, intersect grammar additional information capturing
fluency syntactic well-formedness constraints. Specifically, experiment n-gram language model dependency model based work Klein Manning (2004). follow
Chiangs (2007) integration framework show extended intersecting CFG
grammar arbitrary number models (see Huang, 2008 similar proposal). work
closest Liang et al. (2009) learn align database records text segments
using hierarchical hidden semi-Markov generative model (see Section 3.1 details). recast
model PCFG develop decoding algorithm allows us go beyond alignments,
i.e., generate multi-sentence text corresponding database input.
model conceptually simpler previous approaches (e.g., Angeli et al., 2010; Kim &
Mooney, 2010); encodes information domain structure globally, considering
input space simultaneously generation. thus need train single model (on given
domain) without separately optimize different content selection surface realization components. importantly, recasting generation parsing allows us optimize joint
objective (hence finding likely grammar derivation also yields grammatical output
text) principled manner, rather approximating greedy search local decisions. assumption input must set records essentially corresponding
database-like tables whose columns describe fields certain type. Experimental evaluation
three domains obtains results competitive state-of-the-art without using domain specific
constraints, explicit feature engineering labeled data.1
remainder paper structured follows. Section 2 provides overview related
work. Section 3 presents generation model; defines PCFG used experiments
presents decoding algorithm Section 4 discusses experimental set-up Section 5 presents
results. Discussion future work concludes paper.

2. Related Work
literature reveals many examples generation systems produce high quality text, almost
indistinguishable human writing (Dale, Geldof, & Prost, 2003; Reiter, Sripada, Hunter, Yu,
& Davy, 2005b; Green, 2006; Turner, Sripada, & Reiter, 2009). systems often implement
pipeline architecture involve great deal manual effort. instance, typical content
selection module involves manually engineered rules based analysis large number
texts domain-relevant corpus, consultation domain experts. Analogously, surface
1. preliminary version work published proceedings NAACL 2012. current article presents
general model, formulates explicitly decoding algorithm shows intersect PCFG arbitrary number external knowledge sources. addition, present several novel experiments, comprehensive
error analysis.

307

fiKONSTAS & L APATA

realization often based grammar written hand cover syntactic constructs
vocabulary domain.
One earliest systems exemplifies approach FOG (Goldberg, Driedger, & Kittredge, 1994), weather forecast generator used Environment Canada, Canadian weather
service. FOG takes input numerical simulations meteorological maps uses expert
system decide structure document optional human intervention via
graphical interface. sentence planning surface realization, generator uses grammar
specific weather domain, well canned syntactic structures written expert linguists
encoded Backus Naur Form (BNF). recently, Reiter et al. (2005a) developed
UM IME -M OUSAM, text generator produces marine weather forecasts offshore oilrig applications. content planner system based linear segmentation input
(i.e., time series data) informed pragmatic (Gricean) analysis communicated weather forecasts (Sripada, Reiter, Hunter, & Yu, 2003). Sentence planning relies rules
select appropriate time phrases, based empirical study human-written forecasts. Surface realization relies special grammar rules emulate weather sub-language interest,
based corpus analysis.
existing generation systems engineered obtain good performance particular
domains, often difficult adapt across different domains. alternative adopt
data-driven approach try automatically learn individual generation components even
end-to-end system. example class methods described work Barzilay
Lapata (2005) view content selection instance collective classification. Given
corpus database records texts describing them, first use simple anchor-based
alignment technique obtain records-to-text alignments. Then, use alignments training
data (records present text positive labels, records negative) learn content
selection model simultaneously optimizes local label assignments pairwise relations.
Building work, Liang et al. (2009) present hierarchical hidden semi-Markov generative
model first determines facts discuss generates words predicates
arguments chosen facts. model decomposed three tiers HMMs correspond
chains records, fields words. use Expectation Maximization (EM) training
dynamic programming inference (see Section 3.1 thorough description).
approaches emerged recently combine content selection surface realization. Kim Mooney (2010) present generator two-stage pipeline architecture: using
generative model similar model work Liang et al. (2009), first decide
say verbalize selected input WASP1 , existing generation system (Wong &
Mooney, 2007). contrast, Angeli et al. (2010) propose unified content selection surface
realization model also operates alignment output produced model Liang
et al.. model decomposes sequence discriminative local decisions. first determine records database talk about, fields records mention,
finally words use describe chosen fields. decisions implemented
log-linear model features learned training data. surface realization component
performs decisions based automatically extracted templates filtered domain-specific
constraints order guarantee fluent output.
related work focused mapping meaning representations (e.g., logical form
numeric weather data) natural language, using explicitly aligned sentence/meaning pairs
training data. example, Wong Mooney (2007) learn mapping using synchronous
308

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

context-free grammar (SCFG). also integrate language model SCFG decode
meaning representation input text, using left-to-right Early chart generator. Belz (2008)
creates CFG hand (using set template-based domain-specific rules) estimates probabilities rule application automatically development corpus. Ratnaparkhi (2002) uses
dependency-style grammar phrase fragments context dialogue system, incorporating
among others long-range dependencies. recently, Lu Ng (2011) propose work
model performs joint surface realization lexical acquisition input represented
typed lambda calculus. present novel SCFG forest-to-string generation algorithm,
captures correspondence natural language logical form represented hybrid
trees.
Similar work Angeli et al. (2010), also present end-to-end system performs
content selection surface realization. However, rather breaking generation task
sequence local decisions, optimize say say simultaneously.
learn mappings logical form, rather focus input less structured possibly
noisy. key insight convert set database records serving input generator
PCFG neither hand crafted domain specific simply describes structure
input. training, estimate weights grammar rules using EM algorithm
dynamic program similar inside-outside algorithm (Li & Eisner, 2009). testing
given set database search best derivation tree licensed grammar.
searching, intersect grammar external linguistically motivated models create
k-best lists derivations, thus optimizing say say time.

3. Problem Formulation
assume generator takes input set database record tuples (r, f , v) outputs
text g verbalizes records. record token ri , 1 |d|, type ri .t,
thought name table relational database schema. Note
total number records |d| vary examples. Figure 1b illustrates instances record
types Temperature, Wind Speed, Wind Direction. record token also set
fields ri .f associated it. example, record type Wind Direction two fields, namely
windDir1 .time windDir1 .mode. henceforth abbreviate fields names (e.g., time
mode) record type apparent context. Fields different values fk .v;
Figure 1b value field mode S. Fields also associated type fk .t, defines
range possible values take; model supports integer categorical value types.
example, top right table Figure 1b named Cloud Sky Cover (sc short), corresponds four
database record tuples: (sc1 , time, 06:00-09:00), (sc1 , percent, 25-50), (sc2 , time, 09:00-12:00)
(sc2 , percent, 50-75). time percent categorical type.
training corpus consists several scenarios, i.e., database records paired texts w2
like shown Figure 1. weather forecast domain, scenario corresponds weatherrelated measurements temperature, wind, speed, collected specific day time
(e.g., day night). sportscasting, scenarios describe individual events soccer game
(e.g., passing kicking ball). air travel domain, scenarios comprise flight-related
details (e.g., origin, destination, day, time).
2. use w denote gold-standard text g refer string words system generates.

309

fiKONSTAS & L APATA



...

r1

...

r1 . f 1

w1

...

w

...

ri . f 1

w

...

ri

...

w

...

ri . f|f|

w

...

r|r|

w

r|r| . f|f|

w

...

wN

Figure 2: Graphical model representation generative alignment model Liang et al. (2009).
Shaded nodes represent observed variables (i.e., database collocated text w), unshaded
nodes indicate latent variables. Arrows indicate conditional dependencies variables. Starting database d, model emits sequence records; record emits
sequence fields, specific type particular record. Finally, record uniformly
selects number c emits words w1 . . . wc .

goal first define model naturally captures (hidden) relations
database records observed text w. trained, use model generate text g
corresponding new records d. model extension hierarchical hidden semi-Markov
model Liang et al. (2009) describe detail next section. key idea recast
model probabilistic context-free grammar, therefore reducing tasks content selection
surface realization common parsing problem.3 Arguably, could implemented
model using finite-state representation. However, conceptualization generation parsing,
allows us use well-known CYK algorithm (Kasami, 1965; Younger, 1967) order find
best g licensed grammar. also affords wider range extensions go beyond
expressivity cascade HMMs model Liang et al. furthermore ensure
resulting text fluent intersecting grammar externally trained surface level models,
namely n-gram language model dependency model. Thus, model generate parse
importantly text deemed likely grammar surface models.
following, first describe approach Liang et al. move describe grammar
decoding algorithm, i.e., procedure finding best g given input d.
310

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

3.1 Model Inducing Alignments
Liang et al. (2009) present generative semi-hidden Markov model learns correspondence
world state unsegmented string text without, however, generating output
string words g describing world state. case, world state represented set
database records, associated fields values. model defined generative
process summarized three steps:
1. Record choice. Choose sequence records r describe. Consecutive records selected
basis types.
2. Field choice. record ri emit sequence fields ri .f.
3. Word choice. chosen field ri . fk generate number words c, c > 0 chosen
uniformly.
process implemented hierarchy Markov chains correspond records, fields,
values input database. captured Markov chain records conditioned record
types; given record type, record chosen uniformly set records type.
way, model essentially captures rudimentary notions local coherence salience,
respectively. formally:
|r|

p(r | d) = p(ri .t | ri1 .t)


1
|s(ri .t)|

(1)

s(t) defined function returns set records type t: = {r : r.t = t},
r0 .t START record type. Liang et al. (2009) also include special null record type,
accounts words particularly align record present database. Field
choice modeled analogously Markov chain fields given record choice ri type t:
|ri .f|

p(f | ri .t) = p(ri . fk | ri . fk1 )

(2)

k

also implement special start stop fields model transitions boundaries
corresponding phrase. Finally, chosen record ri , field fk uniformly chosen number c,
0 < c < N, emit words independently given field value type. Note since
model always observes words, simplistic representation surface level adequate
(however, relaxing independence assumption, e.g., additionally conditioning previous
word(s), could potentially yield powerful model):
|w|

p(w |ri , ri . fk , ri . fk .t) = p(w j | ri .t, ri . fk .v)

(3)

j

model supports three different types fields, namely string, categorical integer.
adopt specific generation strategy word level. string-typed fields,
3. alternative would learn SFCG database input accompanying text. However, would
involve considerable overhead terms alignment (as database text together constitute clean
parallel corpus, rather noisy comparable corpus), well grammar training decoding using state-of-the
art statistical machine translation (SMT) methods, manage avoid simpler approach.

311

fiKONSTAS & L APATA

Events:
Fields:
Text:

skyCover1
percent=0-25
cloudy ,

k
N
withg

temperature1
time=6am-9pm
temperatures

min=9
10gand

max=21
20 degrees .

kwindDir1
mode=S
N
southg
windg

kwindSpeed1
N
mean=20
aroundg
20 mph .

Figure 3: Example alignment output model Liang et al. (2009) weather domain.
Subscripts refer record tokens (e.g., skyCover1 first record type Cloud Sky Cover).
emit single word (possibly) multi-word value, chosen uniformly. categorical fields,
maintain separate multinomial distribution words field value. Finally, integer
fields, wish capture intuition numeric quantity database rendered
text word possibly numerical value due stylistic factors.
allow several ways generating word given field value. include generating exact value,
rounding rounding multiple 5, rounding closest multiple 5,
adding subtracting unexplained noise + , respectively. noise modeled
geometric distribution, parameters trained given value ri . fk .v.
example models output weather domain shown Figure 3. top
row contains database records selected model (subscripts correspond record tokens;
e.g., temperature1 refers first record type temperature Figure 1b). second row contains selected fields record associated values. special field null aligns
words directly refer values database records, with, wind
around. Finally, last row shows segmentation alignment original text w produced
model.
stands, Liang et al.s (2009) model generates alignment sequences words
facts database, falling short creating meaningful sentence document. Kim Mooney
(2010) address problem interfacing alignments WASP1 (Wong & Mooney, 2007).
latter publicly available generation system takes alignment input finds
likely string using widely popular noisy-channel model. Angeli et al. (2010) propose
model different spirit nevertheless also operates alignments Liang et al. Using
template extraction method, post-process alignments order obtain sequence
records, fields, words spanned chosen records fields. generation
process modeled series local decisions, arranged hierarchically trained
discriminatively. record choose talk about, choose subset fields,
finally suitable template render chosen content. process repeats decides
generate special STOP record.
treat model Liang et al. (2009) black box order obtain alignments.
Rather, demonstrate generation seamlessly integrated semi-hidden Markov
model re-interpreting CFG rewrite rules providing appropriate decoding algorithm.
model simultaneously learns records fields talk about, textual units
correspond to, creatively rearrange coherent document.
3.2 Grammar Definition
mentioned earlier, recast model Liang et al. (2009) series CFG rewrite rules,
corresponding first two layers HMMs Figure 2. also include set grammar
rules emit chains words, rather words isolation. viewed additional
312

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

1. R(start)

GCS

GSURF

2. R(ri .t) FS(r j , start) R(r j .t)

[Pr = 1]

h
P(r j .t | ri .t) |s(r1i .t)|

3. R(ri .t) FS(r j , start)

h

P(r j .t | ri .t) |s(r1i .t)|

4. FS(r, r. fi ) F(r, r. f j ) FS(r, r. f j )

[P( f j | fi )]

5. FS(r, r. fi ) F(r, r. f j )

[P( f j | fi )]

6. F(r, r. f ) W(r, r. f ) F(r, r. f )

[P(w | w1 , r, r. f )]

7. F(r, r. f ) W(r, r. f )

[P(w | w1 , r, r. f )]

8. W(r, r. f )

[P( | r, r. f , f .t, f .v, f .t = {cat, null})]

9. W(r, r. f ) gen( f .v)

[P(gen( f .v).mode | r, r. f , f .t = int)
P( f .v | gen( f .v).mode)]

Table 1: Grammar rules GGEN weights shown square brackets.

HMM words field original model. modification important generation; since observe set database records d, need better informed model
decoding captures word-to-word dependencies directly. also point
PCFG extend underlying expressivity model presented Liang et al., namely
also describes regular language.
grammar GGEN defined Table 1 (rules (1)(9)) contains two types rules. GCS
rules perform content selection, whereas GSURF rules perform surface realization. types
rules purely syntactic (describing intuitive relationship records, records fields,
fields corresponding words), could apply database similar structure irrespectively semantics domain. Rule weights governed underlying multinomial
distribution shown square brackets. Non-terminal symbols capitals denote intermediate states; terminal symbol corresponds words seen training set, gen( f .v)
function generating integer numbers given value field f . non-terminals, save
start symbol S, one features (shown parentheses) act constraints, similar number gender agreement constraints augmented syntactic rules. Figure 4 shows two
derivation trees licensed grammar sentence Cloudy, temperatures 10
20 degrees. (see example Figure 1b).
first rule grammar denotes expansion start symbol record R,
special start record type (hence notation R(start)). Rule (2) defines chain
two consecutive records, i.e., going record ri r j . Here, FS(r j , start) represents set
fields record r j following record R(ri ). example, Figure 4a, top branching rule
R(start) FS(sc2 , start)R(sc2 .t) (sc stands Cloud Sky Cover) interpreted follows.
Given beginning document, hence record R(start), talk
313

fiKONSTAS & L APATA


R(start)
R(sc2 .t)

FS(sc2 ,start)
F(sc2 ,%)

FS(sc2 ,%)

W(sc2 ,%)

F(sc2 ,null)

R(t1 .t)

FS(t1 ,start)
FS(t1 ,min)

F(t1 ,min)

W(sc2 ,null) W(t1 ,min)

F(t1 ,min)

W(t1 ,min)

FS(t1 ,max)

F(t1 ,max)

F(t1 ,min)

W(t1 ,min)

W(t1 ,max)

F(t1 ,null)

F(t1 ,max)

W(t1 ,max) W(t1 ,null)

F(t1 ,min)

W(t1 ,null)

W(t1 ,min)

Cloudy

,

10



temperatures



F(t1 ,null)



.

degrees

20

...

(a)

R(start)
R(sc2 .t)

FS(sc2 ,start)
F(sc2 ,%)

FS(sc2 ,%)

W(sc2 ,%)

F(sc2 ,null)

R(t1 .t)

FS(t1 ,start)
FS(t1 ,null)

F(t1 ,null)

W(sc2 ,null) W(t1 ,null)

F(t1 ,null)
W(t1 ,null)

FS(t1 ,min)

F(t1 ,min)
W(t1 ,min)

F(t1 ,max)

F(t1 ,min)

F(t1 ,max)

W(t1 ,min) W(t1 ,max)

W(t1 ,max)

F(t1 ,max)
W(t1 ,max)

F(t1 ,max)
W(t1 ,max)

Cloudy

,



temperatures

10





20

degrees

.

...

(b)

Figure 4: Two derivation trees using grammar Table 1 sentence Cloudy, temperatures 10 20 degrees.. use sc shorthand record type Cloud Sky
Cover, Temperature. Subscripts refer record tokens (e.g., sc2 second Cloud Sky
Cover record, t1 first Temperature record, on).

314

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

part forecast refers Cloud Sky Cover, i.e., emit set fields spanned
non-terminal FS(sc2 , start). field start FS acts special boundary consecutive
records. Note input database example 1b, two records type Cloud Sky
Cover (see second box example). Given value percent (%) field
second record 50-75, likely lexicalize phrase Cloudy ,. different
scenario, equivalent phrase Mostly sunny , first record value 25-50 would
appropriate. Rule R(sc2 .t) FS(t1 , start)R(t1 .t) (t stands Temperature)
interpreted similarly: talk sky coverage forecast move
describe temperature outlook, via field set spanned non-terminal FS(t1 , start) (see
second sub-tree Figure 4a). weight rule bigram probability two records
conditioned record type, multiplied normalization factor |s(r1i .t)| , s(t)
function returns set records type (Liang et al., 2009). also defined null
record type i.e., record fields acts smoother words may correspond
particular record. Rule (3) simply escape rule, parsing process (on record
level) finish.
Rule (4) equivalent rule (2) field level, i.e., describes chaining two
consecutive fields fi f j . Non-terminal F(r, r. f ) refers field f record r. example,
tree Figure 4a, rule FS(t1 , min) F(t1 , max) FS(t1 , max) specifies talk
field max record t1 (i.e., temperature record), talking field min. Analogously
record level, also included special null field type emission words
correspond specific record field (e.g., see emission two last tokens degrees .
end phrase derivation tree. Rule (6) defines expansion field F sequence
(binarized) words W, weight equal bigram probability current word given
previous word, current record, field. See consecutive application rule
derivation tree emission phrase temperatures 10 .
Rules (8) (9) responsible surface generation; define emission words
integers W , given field type value, thus regarded lexical rules
grammar (see pre-terminal expansions derivation tree Figure 4a examples).
Rule (8) emits single word vocabulary training set. weight defines multinomial
distribution seen words, every value field f , given field type categorical
(denoted cat grammar) special null field. Rule (9) identical fields whose
type integer. Function gen( f .v) generates integer number given field value, using either
following six ways (Liang et al., 2009): identical field value, rounding rounding
multiple 5, rounding closest multiple 5 finally adding subtracting
unexplained noise + respectively. noise modeled geometric distribution,
parameters trained given value f .v. weight multinomial six
integer generation function choices, given record field f , times P( f .v | gen( f .v).mode),
set geometric distribution noise + , 1 otherwise.
Naturally, grammar yield several derivation trees given input string. Notice
difference Figure 4a Figure 4b emitting phrases temperatures 10
20 degrees .. Figure 4a, field min (whose record Temperature) spans entire
phrase, whereas Figure 4b phrase split two parts. null field emits temperatures
min field emits 10 . Analogously, derivation tree Figure 4a, field max
emits first three words, 20 degrees, null emits full-stop null field
315

fiKONSTAS & L APATA

record (very common situation case punctuation marks). derivation tree
Figure 4b, however, whole phrase spanned field max.
3.3 Generation
far defined probabilistic grammar captures structure database
records fields intermediate non-terminals, words w (from associated text) terminals. mapping w unknown thus intermediate multinomials (see rule
weights GGEN Table 1) define distribution hidden correspondences h records,
fields values. Given input scenario database generate corresponding
text using grammar Table 1.
high-level generation procedure described follows. first select length
N output text (we defer discussion achieve Section 4.3). Then, apply
grammar empty document building derivation trees bottom-up fashion, starting
lexical rules r GSURF . word position document emit k-best list
candidate words drawn corresponding distributions, given values fields
records d; then, apply rest rules r GCS , keeping list k-best partial derivations
partially generated text node4 , reach root symbol spanning whole
document. Finally, reconstruct top-scoring generated string root tree, following pointers best derivation, lexical rules emit words final
document. order guarantee grammaticality final output text, rescore k-best
lists node applying external linguistic knowledge, n-gram language models
head dependency-style models, partially generated substrings.
analogy parsing, procedure amounts finding likely derivation, i.e., sequence
rewrite rules given input. Note, subtle difference syntactic parsing
generation. former case, observe string words goal find
probable syntactic structure, i.e., hidden correspondence h. generation, however, described
above, string observed; instead, must thus find best text g, maximizing
h g (the latter achieved use external linguistic knowledge via rescoring),
g = g1 . . . gN sequence words licensed GCS GSURF . formally:


g = f arg max P (g, h)
(4)
g,h

f function takes input derivation tree (g, h) returns g. use modified
version CYK parser (Kasami, 1965; Younger, 1967) find g. Optimizing h g
intractable, approximate f pruning search space explain Section 3.5.
following, use framework deductive proof systems (Shieber, Schabes,
& Pereira, 1995) order describe decoder. first present basic adaptation CYK
algorithm task give concrete decoding procedure generates text, using chart data
structure (Section 3.4). extend basic decoder k-best decoder, integrating external linguistic knowledge attempt improve quality output. basic decoder
naively optimizes function f h, whereas extended version maximizes h g,
approximately. Note framework deductive proof systems used convenience.
4. use efficient method compresses stored substrings considerably, following work Chiang (2007);
see equation (12) Section 3.6.

316

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

Items:

[A, i, j]
R(A B)
R(A BC)

Axioms:

[W, i, + 1] :

W gi+1 , gi+1 {, gen()}

Inference rules:
(1)
(2)
Goal:

R(A B) : [B, i, j] : s1
[A, i, j] : s1
R(A B C) : [B, i, k] : s1 [C, k, j] : s2
[A, i, j] : s1 s2

[S, 0, N]

Figure 5: basic decoder deductive system. Productions B B C
GCS rules Figure 1; features grammar non-terminals omitted sake clarity.
provides level abstract generalization number algorithms. Examples include recogntion sentence according grammar, learning inside outside weights, Viterbi search,
case generating text (see Goodman, 1999 details).
3.4 Basic Decoder
Analogously parser, decoder generally defined set weighted items (some
designated axioms others goals, i.e., items proven) set inference
rules form:
I1 : s1 . . . Ik : sk

I:s
interpreted follows: items Ii (i.e., antecedents) first proven
weight (or score) si , item (i.e., consequent) provable, weight provided side
condition holds. decoding process begins set axioms, progressively applies
inference rules, order prove items reaches one designated goals.
basic decoder specified Figure 5 consists four components, class items,
set axioms, set inference rules subclass items, namely goal items. Following
work Goodman (1999), items system take two forms: [A, i, j] indicates generated span
j, rooted non-terminal A; R(A B) R(A B C) corresponds content
selection production rules GCS one two non-terminals right hand side. Axioms
correspond individual word generated surface realization grammar rules GSURF (see
(8) (9) Table 1). inference rules follow two forms, one grammar production rules
one non-terminal right hand side, another one rules two non-terminals.
example, inference rule (1) Figure 5 combines two items, namely rule form B
weight generated span [B, i, j] weight s1 rooted B, results new generated
span [A, i, j] weight s1 , rooted A. Finally, system one goal, [S, 0, N],
root node grammar N (predicted) length generated text. time complexity
317

fiKONSTAS & L APATA

O(n3 ), case CYK algorithm. could converted grammar rules Chomsky
normal form (CNF) implemented original CYK algorithm. Note grammar
CNF, since contains unary productions type B, i.e., non-terminal symbols
right-hand side well. chose directly implement inference rules (1) (2) instead (see
Figure 5), since know arity grammar 2 thus able avoid
blow-up number derived rules.
defined parsing strategy, need way find likely derivation;
pseudocode Figure 6 gives generation algorithm basic decoder. uses array
chart[A, i, j], cells get filled sets weights items. also uses identical array
bp[A, i, j] stores back-pointers antecedents item rooted A, well actual
generated words processing lexical rules r GSURF (abusing somewhat traditional
interpretation back-pointer array, storage pointers antecedent chart items). size
chart back-pointer array set pre-defined number N words want
generate (Section 4.3). procedure begins first filling diagonal cells chart
unary spans rooted W , weights lexical rules r GSURF . Equivalently,
back-pointers array takes corresponding generated word. Note conventionial parsing
procedure, always assume diagonal cells chart already filled
actual words underlying sentence. case, assume fixed-size chart
empty diagonal, gets filled top scoring words emitted lexical rules
grammar. Next, items visited combined order, i.e., smaller spans come larger
spans. Given way grammar constructed, items rooted F (corresponding fields)
come items rooted R (records) ultimately S. particular point chart,
algorithm considers antecedent items proven given rules GCS stores
highest scoring combination. Finally, construct resulting string g recursively
visiting bp[S, 0, N]. trace back-pointers item antecedents words gi
emitted axioms.
3.5 k-best Decoding
basic decoder described far produce best derivation tree input given
grammar GGEN unfortunately may correspond best generated text. fact,
output often poor model notion constitutes fluent language. grammar encodes little knowledge regard syntactic well-formedness grammatical coherence.
Essentially, surface realization boils word bigram rules (6) (7) lexical rules
GSURF . word bigram rules inject knowledge word combinations model,
kind information usually sparse cannot capture longer range dependencies.
generation process Figure 6 picks top scoring words emitted lexical production
rules (lines 35), order produce best derivation root node S. Instead, would
preferable added chart list top k words (as well list top k items [B, i, j],
[C, j, k] production rule r GCS ), thus produced k-best list derivations (with
associated strings) root node. done efficiently using lazy algorithm found
work Huang Chiang (2005). Then, generation process finished,
use language model higher order n-grams, head dependency-style rules rescore
k-best lists generated strings directly (see also Charniak & Johnson, 2005 Liang, BouchardCote, Klein, & Taskar, 2006 application similar idea parsing machine translation,
318

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

function ECODE(GGEN ,d,N)
0 . . . N
r : W gi+1 GSURF
chart[W, i, + 1] [W, i, + 1] :
bp[W, i, + 1] gi+1
. store actual word gi+1
end
end
l 2 . . . N
i, k, j j = l < k < j
items [B, i, j] [B, i, k], [C, k, j] inferable chart rules r GCS
r form B
chart[A, i, j] max ([B, i, j] : s1 P(r))
bp[A, i, j] argmax ([B, i, j] : s1 P(r))
end
r form B C
chart[A, i, j] max (chart[B, i, k] chart[C, k, j] P(r))
bp[A, i, j] argmax ([B, i, k] : s1 [C, k, j] : s2 P(r))
end
end
end
end
return chart[S, 0, N], bp[S, 0, N]
end function
Figure 6: Generation procedure basic decoder.

respectively). Although method fast, i.e., linear k, would practically set k
high search among exponentially many possible generations given input.
better solution, common practice machine translation, rescore derivation
trees online. Chiang (2007) intersects PCFG grammar weighted finite state automaton
(FSA), represents n-gram language model; states FSA correspond n 1 terminal symbols. resulting grammar also PCFG incorporates FSA. Similarly,
intersect grammar ensemble external probabilistic models, provided express
regular language. probable generation g calculated as:


g = f arg max p(g) p( g, h | d)
(5)
g,h

p(g, h | d) decoding likelihood sequence words g = g1 . . . gN length N
hidden correspondence h emits it, i.e., likelihood grammar given database
input scenario d. p(g) measure quality output could instance provided
language model (see Section 4.2 details estimate p(g, h | d) p(g)). theory,
function f optimize h g jointly, thus admitting search errors. practice,
however, resulting grammar intersection prohibitively large, calls pruning
search space. following show extend basic generation decoder Figure 5
intersecting (linearly) ensemble external probabilistic models.
319

fiKONSTAS & L APATA


PP

ADVP
RB

NP



PP

NP

NP



NNS

NNS

QP
CD CC CD
Cloudy temperatures

10



20

CD
10

CC


degrees

(a)
ROOT

RB
Cloudy




NNS
temperatures




CD
20

NNS
degrees

(b)

Figure 7: Phrase structure tree dependency graph sentence.

addition n-gram language models routinely used means ensuring lexical fluency rudimentary grammaticality, also inject syntactic knowledge
generator. represent syntactic information form directed dependencies could
potentially capture long range relationships beyond horizon language model. Figure 7
shows dependency-style representation sentence Cloudy temperatures 10
20 degrees corresponding phrase structure. dependency graph Figure 7b captures
grammatical relations words via directed edges syntactic heads dependents
(e.g., verb subject noun modifying adjective). Edges labeled
indicate type head-dependent relationship (e.g., subject object) unlabeled shown
figure. Formally, dependency structure set dependency pairs hwh , wa head wh
argument word wa , respectively. general, argument modifier, object complement; head times determines behavior pair. Figure 7b, cloudy
head with, head temperature, on. D(wh ) returns set dependency pairs
whose head wh , e.g., D(10) = {and, 20}.
Previous work (Ratnaparkhi, 2002) incorporated dependency information surface realization directly generating syntactic dependency tree rather word sequence.
underlying probabilistic model predicts word conditioning syntactically related words
320

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

(i.e., parent, grandparent, siblings). Importantly, approach requires corpus
annotated dependency tree structures. obviate need manual annotation considering dependency structures induced automatically unsupervised fashion.
this, use Dependency Model Valence (DMV; Klein & Manning, 2004), however,
nothing inherent formulation restricts us model. unsupervised model
learns dependency structures broadly similar fashion (e.g., captures attachment likelihood argument head) could used instead proviso operates
structures isomorphic derivation trees generated grammar. necessary
intersecting dependency model expresses (up to) context-free language, since formulate
model also CFG5 .
Finally, note although work two external information sources (i.e., language models
dependencies), framework propose applies arbitrary number models expressing
regular language. instance, could incorporate models capture dependencies relating
content selection field n-grams, however leave future work.
3.6 Extended Decoder
begin introducing notation. define two functions p q operate
surface-level models strings = a1 . . . al , length l, ai V {?}. V vocabulary
observed text w (obtained training corpus), ? symbol represents elided
part string. Recall k-best decoder needs keep list generated sub-strings
node, rescoring purposes. Note sub-strings (potentially) different
observed text w; top-scoring string root node essentially collapses final generated
text g. Storing lists whole sub-strings generated far node, would require considerable
amounts memory. avoid define function q(a) stores essential minimum
string information needed surface-level models (the ? symbol stands omitted
parts string) step, order correctly compute rescoring weight. Function p(a)
essentially calculates rescoring weight given string, linearly interpolating scores
individual model mi weight . Therefore applying p(a) bottom-up fashion (see
extended decoder Figure 8) output q(a) allows us correctly compute rescoring
weight model whole document incrementally. formally:




p(a) = pmi (a)


s.t.

= 1

(6)



setting, make use language model (pm1 ) dependency model (pm2 ):
pm1 (a1 . . . al ) =

PLM (ai |ain+1 . . . ai1 )



(7)

nil
?{a
/ in+1 ,...,ai }


pm2 (a1 . . . al ) = PDEP D(ah ) , ah {a1 , . . . , al }

(8)

function pm1 computes LM probabilities complete n-grams string; PLM returns
probability observing word given previous n1 words. pm2 returns probability
5. Intersecting two CFGs undecidable, PSPACE-complete one CFG finite (Nederhof & Satta, 2004).

321

fiKONSTAS & L APATA

a1 . . . al
mostly cloudy ,

mostly cloudy ? cloudy ,

pm1 (a1 . . . al )
PLM (,|mostly cloudy)
1
PLM (with|cloudy ,) PLM (a|, with)

qm1 (a1 . . . al )
mostly cloudy ? cloudy ,

mostly cloudy ?

Table 2: Example values functions pm1 qm1 phrase mostly cloudy, a.
assume 3-gram language model.
dependency model dependency structure headed word ah . dependency structure D,
word ah dependants depsD (ah , le f t) attach left dependents depsD (ah , right)
attach right. Equation (9) recursively defines probability dependency D(ah )
rooted ah (Klein & Manning, 2004):


PDEP D(ah ) =

PSTOP (STOP|ah , dir, ad j)
dir[le f t,right] depsD (ah ,dir)

(9)


PCHOOSE (aa |ah , dir)PDEP D(aa )
PSTOP (STOP|ah , dir, ad j)

PSTOP binary multinomial indicating whether stop attaching arguments head word ah
given direction, i.e., left right, adjacency, i.e., whether directly adjacent
ah not. PCHOOSE multinomial possible argument words given ah direction
attachment. next define function q(a) returns set strings, one model mi
(we use shortly expand lexical items [A, i, j] basic decoder Figure 5).

q(a) = hqm1 (a), . . . , qmM (a)i

(10)
(11)

(
a1 . . . an1 ? aln+2 . . . al
qm1 (a1 . . . al ) =
a1 . . . al

l n
otherwise

(12)
(13)



al
l = 1



q (a . . . )
pm2 (a1 . . . ak )
m2 1
k
qm2 (a1 . . . ak ak+1 . . . al ) =

pm2 (ak+1 . . . al )
1kl



q (a . . . ) otherwise
m2 k+1
l

(14)

Function qm1 (a) compresses string a, eliding words n-grams
recognized. thus avoid storing whole sub-generation string, produced decoder far,
mentioned earlier. Table 2 gives example values pm1 (a) qm1 (a) phrase mostly
cloudy, a. Function qm2 (a) returns head string a. progressively combine substrings (a1 . . . ak ) (ak+1 . . . al ) together, 1 k l, head words ah1 {a1 , . . . , ak }
ah2 {ak+1 , . . . , al }, function qm2 (a) returns either ah1 ah2 . probability PDEP decides
whether ah1 attaches ah2 vice versa, thus augmenting D(ah1 ) pair hah1 , ah2 D(ah2 )
hah2 , ah1 i, respectively.
322

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

j

Items:

[A, i, j; q(gi )]
R(A B)
R(A BC)

Axioms:

i+1
[W, i, + 1; q(gi+1
)] : p(gi )

W gi+1 , gi+1 {, gen()}

Inference rules:
j

(1)

R(A B) : [B, i, j; q(gi )] : s1
j
j
[A, i, j; q(gi )] : s1 p(gi )

(2)

R(A B C) : [B, i, k; q(gki )] : s1 [C, k, j; q(gk )] : s2
j
j
[A, i, j; q(gi )] : s1 s2 p(gi )

j

Goal:

[S, 0, N; q(hsin1 gN0 h/si)]

Figure 8: Extended decoder using rescoring function p(g). Productions B B C
GCS rules Figure 1; features grammar non-terminals omitted sake
clarity.
Note equation (14) evaluates whether every word attach left right every
head word, therefore essentially collapses to:

Pmdep = PDEP D(ah ) = PSTOP (STOP|ah , dir, ad j)PCHOOSE (aa |ah , dir)
(15)
PSTOP (STOP|ah , dir, ad j)
example, case pm2 (a1 . . . ak ), ah becomes one a1 . . . ak , aa one ak+1 . . . al ,
dir = right ad j true ah = ak aa = ak+1 .
ready extend basic decoder Figure 5, includes rescoring funcj
tion p(gi ) generated sub-string gi . . . g j . new deduction system specified Figure 8.
j
Items [A, i, j] become [A, i, j; q(gi )]; represent derivations gi g j rooted nonterminal augmented model-specific strings defined above; words, include
compressed sub-generations
elidedN parts head word. Analogously, goal item
n1
N
includes q hsi g0 h/si . Note g0 augmented (n 1) start symbols hsi end
symbol h/si. necessary correctly computing n-gram probabilities beginning
end sentence. Figure 9 shows example instantiations inference rules extended
decoder.
generation procedure identical procedure described basic decoder Figure 6, save exponential items need deducted. Recall chart Figure 6
stores cell chart[A, i, j] set combined weights cells correspond proved antecedents item [A, i, j]. new chart 0 extended decoder equivalently stores set lists
j
weights cell position chart 0 [A, i, j]. list contains items [A, i, j; q(gi )]
j
root non-terminal span j, different set q(gi ), sorted best-first.
running time integrating LM DMV models (N 3 |V |4(n1) |P|), V output
vocabulary P vocabulary used DMV. using lexicalized dependency model,
323

fiKONSTAS & L APATA

R (R(skyCover1 .t) FS(temp1 , start) R(temp1 .t)) :
[FS(temp1 , start), 1, 2; hwith, INi] : s1 [R(temp1 .t), 2, 8; ha low ? 15 degrees, JJi] : s2
[R(skyCover1 .t), 1, 8; hwith ? 15 degrees, JJi] : s1 s2 p(hwith ? 15 degrees, JJi)
R (FS(windSpeed1 , min) F(windSpeed1 , max) FS(windSpeed1 , max)) :
[F(windSpeed1 , max), 3, 4; hhigh, JJi] : s1 [FS(windSpeed1 , max), 4, 5; h15, CDi] : s2
[FS(windSpeed1 , min), 3, 5; hhigh 15, JJi] : s1 s2
R (F(windDir1 , mode) W(windDir1 , mode)) : [W(windDir1 , mode), 3, 4; hsoutheast, JJi] : s1
[F(windDir1 , mode), 3, 4; hsoutheast, JJi] : s1
Figure 9: Inference rules extended decoder productions (2), (4), (7) Table 1
(W EATHER G OV domain). strings h. . .i, correspond output functions qmlm
qmdep . adopt unlexicalized dependency model, trained POS tags derived
Penn Treebank project (Marcus et al., 1993). first example corresponds word
JJ word low, second example JJ corresponds word high CD
number 15, whereas third example JJ corresponds word southeast.
P collapses V , otherwise contains part-of-speech (POS) tags every gi V . Notice
rule (2) Figure 8 combines two items contain 2(n 1) words, hence exponent
4(n 2). running time slow use practice, explain must adopt
form pruning order able explore search space efficiently.
3.7 Approximate Search
j

j

Consider task deriving k-best list items L([A, i, j; q(gi )]) deducted item [A, i, j; q(gi )]
j
rule (2) extended decoder Figure 8. item Lm ([A, i, j; q(gi )]) position list,
j
1 k, takes form [A, i, j; q(gm |i )]. example procedure shown Figure 10.
j
grid depicts possible combinations items [B, i, k; q(gki )] [C, k, j; q(gk )] inferred
rule form R(A B C) corresponding weights. k2 combinations
used create resulting k-best list shown bottom figure, store cell
chart 0 [A, i, j]. However, want keep k items, going pruned
away. fact, grid example worst case cube, i.e., hold two three
dimensions, one rules B C left hand-side non-terminal A, two
corresponding items rooted B C6 ; calls calculation k3 combinations.
better approach apply cube pruning (Chiang, 2007; Huang & Chiang, 2005), i.e., compute
small corner grid prune items fly, thus obviating costly computation
k3 combinations.
6. deducted item [R(skyCover1 .t); q(g81 )] Figure 10 also inferred rule R(R(skyCover1 .t)
R(windSpeed1 .t) FS(windSpeed1 , start)) (and corresponding antecedent items) rule R(R(skyCover1 .t)
R(rainChance1 .t) FS(rainChance1 , start)), on. illustrate slice cube, depicting enumeration k-best lists fixed grammar rule, sake clarity.

324

fi[FS(temp1 , start), 1, 2; hwith, INi]

[FS(temp1 , start), 1, 2; ha, DTi]

[FS(temp1 , start), 1, 2; haround, RBi]

G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

.95

.93

.91

[R(temp1 .t), 2, 8; ha low ? 15 degrees, JJi] .56

.40

.25

.20

[R(temp1 .t), 2, 8; hlow around ? 15 degrees, JJi] .54

.35

.30

.17

[R(temp1 .t), 2, 8; ha low ? around 17, RBi] .44

.15

.08

.10












[R(skyCover1 .t), 1, 8; hwith ? 15 degrees, JJi
[R(skyCover1 .t), 1, 8; hwith low ? 15 degrees, JJi]
[R(skyCover1 .t), 1, 8; ha ? 15 degrees, JJi]
[R(skyCover1 .t), 1, 8; haround low ? 15 degrees, RBi]
[R(skyCover1 .t), 1, 8; hwith ? around 17, RBi]


: .40
: .35
: .25
: .17
: .15










Figure 10: Computing exhaustive list deducted item [R(skyCover1 .t); q(g81 )] via application inference rule (2) extended decoder Figure 9. antecedent items
rule R (R(skyCover1 .t) R(temp1 .t) FS(temp1 , start)) items [R(temp1 .t), 2, 8; q(g82 )],
FS(temp1 , start), 1, 2; q(g21 )]. figure shows slice cube, particular rule;
side grid lists top three candidate items antecedent item, sorted bestfirst. Numbers grid represent total score combination.

Consider Figure 11 example. side grid shows lists top three items
antecedent item. Numbers grid represent total score combination.
Figures 11b11d illustrate enumeration top three combinations best-first order. Cells
gray represent frontiers iteration; cells black resulting top three items.
basic intuition behind cube pruning pair antecedent items u1 = [B, i, k; q(gki )], u2 =
j
[C, k, j; q(gk )] sorted k-best lists L(u1 ), L(u2 ), best combinations lie close
upper-left corner grid. example, 3-best list nodes u1 = [R(temp1 .t), 2, 8; q(g82 )]
325

fi[FS(temp1 , start), 1, 2; hwith, INi]

[FS(temp1 , start), 1, 2; ha, DTi]

[FS(temp1 , start), 1, 2; haround, RBi]

[FS(temp1 , start), 1, 2; hwith, INi]

[FS(temp1 , start), 1, 2; ha, DTi]

[FS(temp1 , start), 1, 2; haround, RBi]

[FS(temp1 , start), 1, 2; hwith, INi]

[FS(temp1 , start), 1, 2; ha, DTi]

[FS(temp1 , start), 1, 2; haround, RBi]

KONSTAS & L APATA

.95

.93

.91

.95

.93

.91

.95

.93

.91

[R(temp1 .t), 2, 8; ha low ? 15 degrees, JJi] .56

.40

.25

.20

.40

.25

.20

.40

.25

.20

[R(temp1 .t), 2, 8; hlow around ? 15 degrees, JJi] .54

.35

.30

.17

.35

.30

.17

.35

.30

.17

[R(temp1 .t), 2, 8; ha low ? around 17, RBi] .44

.15

.08

.10

.15

.08

.10

.15

.08

.10

(a)

(b)

(c)

Figure 11:
Computing item combinations u1 = [R(temp1 .t), 2, 8; q(g82 )]
u2 = [FS(temp1 , start), 1, 2; q(g21 )] using cube pruning. (a)(c) enumerate combinations items order construct resulting k-best list described text.
u2 = [FS(temp1 , start), 1, 2; q(g21 )] are:
h

L(u1 ) = ha low ? 15 degrees, JJi, hlow around ? 15 degrees, JJi, ha low ? around 17, RBi
h

L(u2 ) = hwith, INi, ha, DTi, haround, RBi
intuitively best combination derivation top left corner7 :



L1 (u1 ), L1 (u2 ) = ha low ? 15 degrees, JJi, hwith, INi = hwith ? 15 degrees, INi
cases combination cost, i.e., score grammar rule multiplied
rescoring weight p(g), negligible, could start enumerating item combinations order shown Figures 11b11c, starting (L1 (u1 ), L1 (u2 )) stopping k. Since two
lists sorted guaranteed L2 (u1 ), i.e., second item k-best list u1 either
(L1 (u1 ), L2 (u2 )) (L2 (u1 ), L1 (u2 )) (in example Figure 11b latter). thus select
move compute neighboring combinations, on.8 computation
k-best lists axioms [W, i, + 1; q(gii+1 )], enumerate top-k terminal symbols gi+1 .
take account combination cost, grid non-monotonic, therefore bestfirst guarantee longer holds enumerate neighbors fashion described. Huang
Chiang (2007) argue loss incurred search error insignificant compared
speedup gained. case, overcome this, compute resulting k-best list, first adding
7. Note head sub-generation fragment shifted head L2 .
8. Contrary Huang Chiang (2007) use probabilities instead log scores computation item
combinations, hence select biggest scoring combinations.

326

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

computed item combinations temporary buffer, resort enumerated
total k combinations.
3.8 Learning
represent grammar input scenario weighted hypergraph (Gallo, Longo, Pallottino, & Nguyen, 1993). follow procedure proposed Klein Manning (2001)
allows transform CFG hypergraph. order learn weights grammar rules
directly estimate hypergraph representation using EM algorithm. Formally,
objective trying optimize factorizes into:
P(r, ri .f, w|d) = P(ri |d) P(ri . f j |ri ) P(w j |ri . f j , ri )


j

(16)

k

r set record tokens ri . Given training set scenarios database records
observed text w maximize marginal likelihood data, summing record
tokens ri fields ri .f, regarded latent variables:
arg max


p(r, ri .f, w|d; ),

(17)

(w,d) r,ri .f

multinomial distributions weights GGEN . EM algorithm alternates
E-step M-step. E-step compute expected counts rules using
dynamic program similar inside-outside algorithm (Li & Eisner, 2009). M-step,
optimise normalising counts computed E-step. initialise EM uniform
distribution multinomial distribution applied add-0.001 smoothing multinomial
M-step. average, EM converged datasets 15 iterations. Note n-gram
language model dependency model trained externally, hence parameters
optimized alongside model. generation procedure extended decoder Figure 8
implemented using dynamic programming. choice hypergraph representation merely
one several alternatives. example, could adopted representation based weighted
finite state transducers (de Gispert, Iglesias, Blackwood, Banga, & Byrne, 2010) since model
describes regular language terms PCFG surface level models intersect
with. also possible represent grammar pushdown automaton (Iglesias, Allauzen,
Byrne, de Gispert, & Riley, 2011) intersect finite automata representing language model
dependency-related information, respectively. choice hypergraph representation
motivated compactness9 fact allows future extensions PCFG
rules capture global aspects generation problem (e.g., document planning)
unavoidably result context-free languages.

4. Experimental Design
section present experimental setup assessing performance model.
give details datasets used, explain model trained, describe models
used comparison approach, discuss system output evaluated.
9. Hypergraphs commonly used machine translation literature allow compact encoding SCFGs
even though cases also describe regular languages. example, true SCFGs employed
hierarchical phrase-based SMT (Chiang, 2007) assume finite input language permit infinite
recursions.

327

fiKONSTAS & L APATA

4.1 Data
used system generate soccer commentaries, weather forecasts, spontaneous utterances relevant air travel domain (examples given Figure 1). first domain
used dataset described work Chen Mooney (2008), consists 1,539 scenarios 20012004 Robocup game finals (henceforth ROBO C UP). scenario contains
average |d| = 2.4 records, paired short sentence (5.7 words). domain
small vocabulary (214 words) simple syntax (e.g., transitive verb subject object).
Records dataset aligned manually corresponding sentences (Chen & Mooney,
2008). Given relatively small size dataset, performed cross-validation following previous work (Chen & Mooney, 2008; Angeli et al., 2010). trained system three ROBO C
games tested fourth, averaging four train/test splits.
weather forecast generation, used dataset presented work Liang et al. (2009),
consists 29,528 weather scenarios 3,753 major US cities (collected four days).
vocabulary domain (henceforth W EATHER G OV) comparable ROBO C (345 words),
however, texts longer (N = 29.3) varied. average, forecast 4 sentences
content selection problem challenging; 5.8 36 records per scenario
mentioned text roughly corresponds 1.4 records per sentence. used 25,000
scenarios W EATHER G OV training, 1,000 scenarios development 3,528 scenarios
testing. partition used work Angeli et al. (2010).
air travel domain used ATIS dataset (Dahl, Bates, Brown, Fisher, Hunicke-Smith,
Pallett, Pao, Rudnicky, & Shriberg, 1994), consisting 5,426 scenarios. transcriptions
spontaneous utterances users interacting hypothetical online flight booking system.
used dataset introduced work Zettlemoyer Collins (2007)10 automatically converted lambda-calculus expressions attribute-value pairs following conventions adopted study Liang et al. (2009).11 Figure 1c shows output conversion process original lambda expression x. f light(x) f rom(x, denver) to(x, boston)
day number departure(x, 9) month departure(x, august) < (arrival time(x), 16:00). Given
expression, first create record variable (e.g., x). assign record types
according corresponding class types (e.g., variable x class type flight). Next, fields
values added predicates two arguments class type first argument matching record type. name predicate denotes field, second argument
denotes value (e.g., f rom(x, denver) used fill record type Flight, since type
first argument also f light). name function becomes field name, (i.e., from)
second argument set value, (i.e., denver). Note functions names
month departure, month arrival, day number arrival, day number departure on. order
reduce resulting number record types, created aggregate record types embed
common information (i.e., departure, arrival) special field. example, function
day number departure split value departure field dep/ar record Day,
field number value 9. also defined special record types, Condition
Search. latter introduced every lambda operator assigned categorical field
value flight refers record type variable x.
10. original corpus contains user utterances single dialogue turns would result trivial scenarios. Zettlemoyer Collins (2007) concatenate user utterances referring dialogue act, (e.g., book flight), thus
yielding complex scenarios longer sentences.
11. See Konstas (2013) resulting dataset.

328

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

contrast two previous datasets, ATIS much richer vocabulary (927 words);
scenario corresponds single sentence (average length 11.2 words) 2.65 19 record
types mentioned average. Note original lambda expressions created based
utterance, thus contain necessary information conveyed meaning text.
result, converted records scenario mentioned corresponding text.
Following work Zettlemoyer Collins (2007), trained 4,962 scenarios tested
ATIS NOV93 contains 448 examples.
4.2 Model Training
Generation model amounts finding best derivation (g, h) maximizes product
two likelihoods, namely p(g, h | d) p(g) (see equation (5)). p(g, h | d) corresponds
rules GGEN generate word sequence g, whereas p(g) likelihood g independently
d. estimate p(g, h | d) described Section 3.8. Examples top scoring items
multinomial distributions grammar rules GGEN given Table 3. obtain
estimate p(g) linearly interpolating score language model DMV (Klein &
Manning, 2004).
Specifically, language models trained SRI toolkit (Stolcke, 2002) using add-1
smoothing.12 ROBO C domain, used bigram language model given average
text length relatively small. W EATHER G OV ATIS, used trigram language model.
obtained unlexicalized version DMV13 domains. datasets
tagged automatically using Stanford POS tagger (Toutanova, Klein, Manning, & Singer, 2003)
words augmented part speech, e.g., low becomes low/JJ, around becomes
around/RB on; words several parts speech duplicated many times
number different POS tags assigned tagger. example, gust may act
noun verb, given context, hence keep augmented forms, i.e., gust/NNS
gust/VBS. initialized EM uniform distributions small amount noise14 added
multinomials (i.e., PSTOP PCHOOSE ) break initial symmetry. Klein Manning (2004)
use harmonic distribution instead, probability one word heading another higher
appear closer one another. Preliminary results development set showed
former initialization scheme robust across datasets.
model two hyperparameters: number k-best derivations considered decoder vector weights model integration. Given interpolate two models whose weights sum one, need modulate single interpolation parameter 0 LM 1. LM 0, decoder influenced DMV conversely
LM 1 decoder influenced language model. general case, could learn
interpolation parameters using minimum error rate training (Och, 2003), however
necessary experiments. performed grid search k LM held-out data taken
12. Adopting complex smoothing technique Good-Turing (Good, 1953) usually applicable
small vocabularies. statistics computing called count-of-counts, i.e., number words occurring once,
twice on, sufficient lead poor smoothing estimates.
13. trained WSJ-10 corpus, implementation DMV obtained accuracy reported
work Klein Manning (2004). WSJ-10 consists 7,422 sentences 10 words removing
punctuation.
14. Repeated runs different random noise WSJ-10 corpus yielded results; accuracy stabilized around
60th iteration (out 100).

329

fiKONSTAS & L APATA

Weight Distribution
P( | pass, from, purple2)
P( | steal, null, NULL)
P( | turnover, null, NULL)

Top-5 scoring items
purple2, a, makes, pink10, short
ball, the, steals, from, purple8
to, the, ball, kicks, loses

(a) ROBO C

Weight Distribution
P(ri .t | temperature)
P(ri .t | windSpeed)
P(ri .t | skyCover)
P( fi | temperature.time)
P( fi | windSpeed.min)
P( fi | gust.max)
P( | skyCover, percent, 0-25)
P( | skyCover, percent, 25-50)
P( | rainChance, mode, Definitely)

Top-5 scoring items
windDir, sleetChance, windSpeed,
freezingRainChance, windChill
gust, null, precipPotential,
windSpeed, snowChance
temperature, skyCover, thunderChance,
null, rainChance
min, max, mean, null, time
max, time, percent, mean, null
min, mean, null, time, max
,, clear, mostly, sunny, mid
,, cloudy, partly, clouds, increasing
rain, of, and, the, storms

(b) W EATHER G OV

Weight Distribution
P(ri .t | search)
P(ri .t | flight)
P(ri .t | day)
P( | flight, to, mke)
P( | search, what, flight)
P( | search, type, query)

Top-5 scoring items
flight, search, when, day, condition
search, day, flight, month, condition
when, search, flight, month, condition
mitchell, general, international, takeoffs, depart
I, a, like, to, flight
list, the, me, please, show
(c) ATIS

Table 3: Top-5 scoring items multinomial distributions record rules, field rules
categorical word rewrite rule GGEN (see rules (2), (4), (8) Table 1, respectively). first
column table shows underlying multinomial distribution corresponding rule.
example P( | pass, from, purple2), corresponds distribution emitting word given
value purple2 field record type pass.

W EATHER G OV, ROBO C UP, ATIS, respectively. optimal values k LM
three domains (when evaluating system performance BLEU-4) shown Table 4.
conducted two different tuning runs, one version model takes LM
account (k- BEST- LM; LM = 1) another one LM DMV integrated
(k- BEST- LM - DMV). seen, optimal values k generally larger k- BEST- LM - DMV.
probably due noise introduced DMV; result, decoder explore
search space thoroughly. effort investigate impact DMV further, fixed
330

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

k- BEST- LM
ROBO C
W EATHER G OV
ATIS

k
25
15
40

(a) Interpolation LM

k- BEST- LM - DMV
ROBO C
W EATHER G OV
ATIS

k
85
65
40

LM
0.9
0.3
0.6

(b) Interpolation LM DMV

Table 4: Optimal values parameters k LM calculated performing grid search
BLEU-4 development set. LM Table (a) set 1.

LM = 0 development set performed grid search DMV own. Model
performance dropped significantly (by 58% BLEU points) entirely surprising given
DMV alone cannot guarantee fluent output. contribution rather rests capturing
global dependencies outwith local horizon language model.
4.3 Determining Output Length
Unlike generation systems operate surface realization level word templates,
emit word individually bottom-up fashion. Therefore, need decide number
words N wish generate beginning decoding process. common approach
fix N average text length training set (Banko, Mittal, & Witbrock, 2000). However,
would good choice case, since text length follow normal distribution.
shown Figure 12 distribution N across domains mostly skewed.
avoid making unwarranted assumptions output, trained linear regression
model determines text length individually scenario. input model,
used flattened version database, features record-field pairs. underlying idea
scenario contains many records fields, use words express
them. contrast, number records fields small, likely output
laconic. attempt capture number words needed communicate specific record-field
pairs, experimented different types feature values, e.g., setting feature actual
value (categorical numerical) frequency training data. former scheme worked
better denser datasets, W EATHER G OV ROBO C whereas latter adopted
ATIS sparser database, means smooth infrequent values. trained
training set tested development set regression model obtained correlation
coefficient 0.64 ROBO C UP, 0.84 W EATHER G OV, 0.73 ATIS (using Pearsons r).
4.4 System Comparison
evaluated three configurations system. baseline uses top scoring derivation
subgeneration (1- BEST) two versions model make better use decoding
algorithm. One version integrates k-best derivations LM (k- BEST- LM), version additionally takes DMV account (k- BEST- LM - DMV). Preliminary experiments
model integrates k-best derivations DMV exhibit satisfactory results (see
Section 4.2) omit sake brevity. compared output models
331

fiKONSTAS & L APATA

6000

700
600
500
400
300
200
100

Frequency

Frequency

7000
5000
4000
3000
200
1000
3 5 7 9 11 13 15 17

9

(a) Text length N ROBO C

21 33 45 57 69 81

(b) Text length N W EATHER G OV

Frequency

2400
2000
1600
1200
800
400
2

6 10 14 18 22 26 30 34 38 44 48
(c) Text length N ATIS

Figure 12: Text length distribution ROBO C UP, W EATHER G OV, ATIS (training set).
Angeli et al. (2010) whose approach closest state-of-the-art W EATHER G OV.15
ROBO C UP, also compared best-published results (Kim & Mooney, 2010).
4.5 Evaluation
evaluated system output automatically, using BLEU-4 modified precision score (Papineni,
Roukos, Ward, & Zhu, 2002) human-written text reference. addition, evaluated
generated text via judgment elicitation study. Participants presented scenario
corresponding verbalization asked rate latter along two dimensions: fluency
(is text grammatical overall understandable?) semantic correctness (does meaning
conveyed text correspond database input?). subjects used five point rating scale
high number indicates better performance. randomly selected 12 documents
test set (for domain) generated output models (1- BEST k- BEST- LM - DMV)
Angeli et al.s (2010) model. also included original text (H UMAN) gold standard. thus
15. grateful Gabor Angeli providing us code system.

332

fiF IXED

J OINT

G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

System
1- BEST
k- BEST- LM
k- BEST- LM - DMV
1- BEST
k- BEST- LM
k- BEST- LM - DMV
NGELI
K IM -M OONEY

BLEU
8.01.
24.88
23.14
10.79.
30.90
29.73
28.70
47.27.

System
1- BEST
k- BEST- LM
k- BEST- LM - DMV
NGELI

BLEU
8.64.
33.70
34.18.
38.40.

(b) W EATHER G OV

System
1- BEST
k- BEST- LM
k- BEST- LM - DMV
NGELI

BLEU
11.85.
29.30
30.37
28.70

(c) ATIS

(a) ROBO C

Table 5: BLEU-4 scores ROBO C UP, W EATHER G OV, ATIS ( : significantly different
1- BEST; : significantly different NGELI; . significantly different k- BEST- LM; : significantly different k- BEST- LM - DMV; : significantly different K IM -M OONEY.
obtained ratings 48 (12 4) scenario-text pairs domain. study conducted
Internet using Amazon Mechanical Turkand involved 305 volunteers (104 ROBO C UP, 101
W EATHER G OV, 100 ATIS), self reported native English speakers. experimental
instructions given Appendix A.

5. Results
conducted two experiments ROBO C domain. first assessed performance
generator joint content selection surface realization obtained results shown
upper half Table 5a (see J OINT). second experiment forced generator use
gold-standard records database. necessary order compare previous
work (Angeli et al., 2010; Kim & Mooney, 2010).16 results summarized lower half
Table 5a (see F IXED).
Overall, generator performs better 1- BEST baseline comparably Angeli et al.
(2010). k- BEST- LM - DMV slightly worse k- BEST- LM. due fact sentences
ROBO C short (their average length 5.7 words) result model cannot
recover meaningful dependencies. Using Wilcoxon signed-rank test find differences
BLEU scores among k- BEST- LM - DMV, k- BEST- LM NGELI statistically significant. Kim Mooney (2010) significantly outperform three models 1- BEST baseline
(p < 0.01). entirely surprising, however, model requires considerable
supervision (e.g., parameter initialization) includes post-hoc re-ordering component.
Finally, also observe substantial increase performance compared joint content selection surface realization setting. expected generator faced easier task
less scope error.
regard W EATHER G OV, model (k- BEST- LM k- BEST- LM - DMV) significantly improves 1- BEST baseline (p < 0.01) lags behind Angeli et al. (2010) difference
16. Angeli et al. (2010) Kim Mooney (2010) fix content selection record field level. let
generator select appropriate fields, since two per record type level complexity
easily tackled decoding.

333

fiKONSTAS & L APATA

F1 (%)

BLEU-4 (%)

100
90
80
70
60
50
40
30
20
10
5 000 10 000 15 000 20 000 25 000
Number training scenarios

50
45
40
35
30
25
20
15
10
5
5 000 10 000 15 000 20 000 25 000
Number training scenarios

(a) Alignment

(b) Generation output

Figure 13: Learning curves displaying quality alignments generated output vary
function size training data.

statistically significant (p < 0.01). Since system emits words based language model rather
template, displays freedom word order lexical choice, thus likelier
produce creative output, sometimes even overly distinct compared reference. Dependencies seem play important role here, yielding overall better performance.17 Interestingly,
k- BEST- LM - DMV significantly better k- BEST- LM domain (p < 0.01). Sentences
W EATHER G OV longer ROBO C allows k- BEST- LM - DMV learn dependencies capture information complementary language model.
ATIS, k- BEST- LM - DMV model significantly outperforms 1- BEST (p < 0.01)
NGELI (p < 0.05), whereas k- BEST- LM performs comparably. Furthermore, k- BEST- LM - DMV
significantly better k- BEST- LM (p < 0.01). ATIS domain challenging
respect surface realization. vocabulary larger ROBO C factor 4.3
W EATHER G OV factor 2.7. increased vocabulary model learns richer
dependencies improve fluency overall performance.
also examined amount training data required model. performed learning
experiments W EATHER G OV since contains training scenarios ROBO C ATIS
challenging regard content selection. Figures 13(a) (b) show number training instances influnces quality alignment generation output, respectively.
measure alignment F-score following methodology outlined work Liang et al.
(2009) using gold alignments. graphs show 5,000 scenarios enough obtaining
reasonable alignments generation output. small upward trend detected increasing training instances, however seems considerably larger amounts would required
obtain noticeable improvements.
17. DMV commonly trained sentence-by-sentence basis. ROBO C ATIS datasets, scenario-text
pair corresponds single sentence. W EATHER G OV, however, text may include multiple sentences.
latter case trained DMV multi-sentence text without presegmenting individual sentences.
non-standard training regime seem pose difficulty domain, safely assume
examples elided root head, namely weather (e.g., weather mostly cloudy, low
around 30).

334

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

System
1- BEST
k- BEST- LM - DMV
NGELI
H UMAN

ROBO C
F
SC

2.14
2.09

4.05
3.55

4.01
3.47
4.17
3.97

W EATHER G OV
F
SC

2.25
2.53

3.89
3.54

3.82
3.72
4.01
3.58

ATIS
F
2.40
3.96
3.86
4.16

SC
2.49
3.82
3.31
3.96

Table 6: Mean ratings fluency (F) semantic correctness (SC) system output elicited
humans ROBO C UP, W EATHER G OV, ATIS ( : significantly different 1- BEST; : significantly different NGELI; : significantly different k- BEST- LM - DMV; : significantly
different H UMAN).
results human evaluation study shown Table 6. report mean ratings
system gold-standard human authored text. experimental participants rated output
two dimensions, namely fluency (F) semantic correctness (SC). elicited judgments
k- BEST- LM - DMV generally performed better k- BEST- LM automatic evaluation
(see Table 5). carried Analysis Variance (A NOVA) examine effect system
type (1- BEST, k- BEST- LM - DMV, NGELI, H UMAN) fluency semantic correctness
ratings. used Tukeys Honestly Significant differences (HSD) test, explained Yandell
(1997) assess whether means differences statistically significant.
three domains system (k- BEST- LM - DMV) significantly better 1- BEST baseline (a < 0.01) terms fluency. output indistinguishable gold-standard (H UMAN)
NGELI (pair-wise differences among k- BEST- LM - DMV, NGELI H UMAN statistically significant). respect semantic correctness, ROBO C UP, k- BEST- LM - DMV significantly better 1- BEST (a < 0.01) significantly worse H UMAN (a < 0.01). Although
ratings k- BEST- LM - DMV numerically higher NGELI, difference statistically significant. NGELI also significantly worse H UMAN (a < 0.01). W EATHER G OV,
semantic correctness k- BEST- LM - DMV NGELI significantly different. two
systems also indistinguishable H UMAN. ATIS, k- BEST- LM - DMV best performing model respect semantic correctness. significantly better 1- BEST NGELI
(a < 0.01) significantly different H UMAN.
sum, observe performance improves k-best derivations taken account
(the 1- BEST system consistently worse). results also show taking dependency-based
information account boosts model performance achieved
language model. model par NGELI ROBO C W EATHER G OV performs
better ATIS evaluated automatically humans. Error analysis suggests
reason NGELIs poorer performance ATIS might inability create good quality
surface templates. due lack sufficient data fact templates cannot
fully express database configurations many different ways. especially true
ATIS consists transcriptions spontaneous spoken utterances meaning
rendered many different ways. example, phrases show flights,
flights, flights, please give flights, convey exact
meaning stemming Search record.
model learns domain specific conventions say say data,
without hand-engineering manual annotation. Porting system different domain
335

fiKONSTAS & L APATA

Bad Pass
Input:
1- BEST:
k- BEST- LM - DMV:



pink11 purple5
pink11 pass purple5 purple5 pink11 pass purple5 purple5 purple5
pink11 made pass intercepted purple5

NGELI:

pink11 made bad pass missed target picked
purple5

H UMAN:

pink11 tries pass intercepted purple5
(a) ROBO C

Input:

1- BEST:
k- BEST- LM - DMV:

Temperature

Cloud Sky Cover

time
min mean max
06:00-21:00 32 39 46

time
percent (%)
06:00-21:00
75-100

Wind Speed

Wind Direction

time
min mean max
06:00-21:00 6
7
10

time
mode
06:00-21:00 SE

Near 46. Near 46. Near 46. Near 46. Near 46. near 46. Southeast wind.
Mostly cloudy, high near 46. South southeast wind 6 10 mph.

NGELI :

chance rain drizzle, high near 46. Southeast wind 6
10 mph. mph. Chance precipitation 60%.

H UMAN:

Mostly cloudy, high near 46. South southeast wind 6 10 mph.
(b) W EATHER G OV

Input:
1- BEST:
k- BEST- LM - DMV:

Flight

Day

Search



milwaukee phoenix

day
dep/ar/ret
saturday departure

type
query flight

Milwaukee Phoenix Saturday Saturday Saturday Saturday
Show flights Milwuakee Phoenix Saturday

NGELI :

Show flights Milwuakee Phoenix Saturday

H UMAN:

Milwuakee Phoenix Saturday
(c) ATIS

Figure 14: Example output (a) sportscasting, (b) weather forecasting, (c) air travel domains
correct content selection.

straightforward, assuming database corresponding (unaligned) text. long database
obeys structure grammar GGEN , need retrain model obtain weights
grammar rules; addition, system requires domain specific language model optionally
336

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

Gold:

Output:

Temperature

Cloud Sky Cover

Chance Rain

time
min mean max
06:00-21:00 30 38 44

time
percent (%)
06:00-21:00
75-100

Time
Mode
06:00-21:00 Slight Chance

Wind Speed

Wind Direction

Precipitation Potential (%)

time
min mean max
06:00-21:00 6
6
7

time
mode
06:00-21:00 ENE

time
min mean max
06:00-21:00 9 20 35

40 percent chance showers 10am. Mostly cloudy, high
near 44. East northeast wind around 7 mph.
(a) Gold standard content selection verbalization

Content
Selection:

Output:

Temperature

Cloud Sky Cover

Chance Rain

time
min mean max
06:00-21:00 30 38 44

time
percent (%)
06:00-21:00
75-100

time
mode
06:00-09:00 Chance

Wind Speed

Wind Direction

Chance Thunderstorm

time
min mean max
06:00-21:00 6
6
7

time
mode
06:00-21:00 ENE

time
mode
06:00-13:00 -13:00-21:00 --

chance showers. Patchy fog noon. Mostly cloudy,
high near 44. East wind 6 7 mph.
(b) k- BEST- LM - DMV content selection

Content
Selection:

Output:

Temperature

Precipitation Potential (%)

Chance Rain

time
min mean max
06:00-21:00 30 38 44

time
min mean max
06:00-21:00 9 20 35

time
mode
06:00-09:00 Chance

Wind Speed

Wind Direction

Chance Thunderstorm

time
min mean max
06:00-21:00 6
6
7

time
mode
06:00-21:00 ENE

time
mode
06:00-21:00 --

chance showers. Patchy fog noon. Mostly cloudy, high
near 44. East wind 6 7 mph. Chance precipitation 35%
(c) NGELI content selection

Figure 15: Example output W EATHER G OV domain incorrect content selection (in gray).

337

fiKONSTAS & L APATA

ROOT



show








Phoenix
Phoenix

show flights Milwaukee Phoenix Saturday
Figure 16: Dependency structure sentence Show flights Milwaukee Phoenix
Sunday generated k- BEST- LM - DMV (see Figure 14c). Intermediate nodes tree denote
head words subtree.

information heads dependents DMV learns unsupervised fashion.
latter case, also need tune hyperparameter LM , cases k. Note,
fine-tuning k becomes less important integrating language model only. explain
Section 4.2, DMV possibly introduces noise, therefore modulate k carefully
allow decoder search bigger space.
Examples system output correct content selection record level given Figure 14. Note case ROBO C UP, content selection fixed gold standard.
seen, generated text close human authored text. Also note output
system improves considerably taking k-best derivations account (compare 1- BEST
k- BEST- LM - DMV figure). Figure 15a shows examples incorrect content selection
record level W EATHER G OV domain. Figure 15a shows gold standard content selection
corresponding verbalization. Figures 15b 15c show output k- BEST- LM - DMV
system NGELI. Tables black denote record selection identical gold standard, whereas
tables grey denote false positive recall. k- BEST- LM - DMV identifies incorrect value
Mode field Chance Rain record; addition, fails select Precipitation Potential (%) record altogether. former mistake affect correctness generators
output, whereas latter (i.e., fails mention exact likelihood rain, 40% gold
standard 35% NGELIs output). Finally, Figure 16 shows dependency structure
model produced sentence Show flights Milwaukee Phoenix Saturday
Figure 14c; notice long range dependency flights on, would otherwise
inaccessible language model.
338

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

6. Conclusions
presented end-to-end generation system performs content selection surface
realization simultaneously. Central approach encoding generation parsing problem. reformulate input (a set database records text describing them)
PCFG show approximately find best generated string licensed grammar.
evaluated model three domains (ROBO C UP, W EATHER G OV, ATIS) showed
able obtain performance comparable superior state-of-the-art. experiments
also designed assess several aspects proposed framework use k-best decoding intersection grammar multiple information sources. observed k-best
decoding essential producing good quality output. Across domains, performance increases
factor least two multiple derivations taken account. addition, intersecting
grammar dependency-based information seems capture syntactic information complementary language model. argue approach computationally efficient viable
practical applications. Outwith generation, hope work described might
relevance fields summarization machine translation.
Future extensions many varied. obvious extension concerns porting framework
challenging domains richer vocabulary longer texts (e.g., product descriptions,
user manuals, sports summaries). related question extend PCFG-based approach
advocated capture discourse-level document structure. future directions involve
exploiting information available database directly. model takes account
k-best derivations decoding time, however inspection indicates often fails
select best one. Initial work (Konstas & Lapata, 2012) shows model presented
adapted use forest reranking, technique approximately reranks packed forest
exponentially many derivations (Huang, 2008). reranker essentially structured perceptron
(Collins, 2002) enriched local non-local features. therefore allows explicitly model
dependencies across fields, records, interactions.
Finally, although focus paper, worth pointing model described
also perform semantic parsing, i.e., convert text formal meaning representation.
done trivially modifying grammar Table 1. Instead observing words terminals
(rules (8) (9)), observe values fields, given particular word w, field, record:
W(r, r. f ) f .v

[P( f .v | r, r. f , f .t, w)]

W(r, r. f ) gen(w)

[P(gen(w).mode | r, r. f , f .t=int)]

decoding, prior p(g) equation (4) becomes p( f .v) naively obtained
creating n-gram language model alignments meaning representations
text. alignments principle hidden could estimated using model Liang
et al. (2009).

Acknowledgments
grateful anonymous referees whose feedback helped substantially improve
present paper. Thanks Luke Zettlemoyer Tom Kwiatkowski help ATIS
dataset well Giorgio Satta Frank Keller helpful comments suggestions. also
339

fiKONSTAS & L APATA

thank members Probabilistic Models reading group University Edinburgh
feedback. preliminary version work published proceedings NAACL 2012.

Appendix A. Experimental Instructions
A.1 Instructions
experiment given tables contain facts weather (e.g., Temperature, Chance Rain, Wind Direction, Cloud Coverage on) translation
natural language. Example 1 tabulates weather related information translation
Rainy high near 47. Windy, east wind 5 15 mph.
Example 1
Category

Temperature

Fields

time: 17.0006.00(+1 day) min: 30

mean: 40 max: 47

Wind Direction time: 17.0006.00(+1 day) mode: SE
Cloud Sky Cover time: 17.0006.00(+1 day) percent: 2550
Chance Rain time: 17.0021.00

mode: Likely

Rainy high near 47. Windy , east wind 5 15 mph.
row table contains different weather-related event. first row talks temperature, second one wind direction, etc. Different event types instantiate different fields.
example, Temperature four fields, time, min, mean, max. Fields turn values,
either numbers (e.g., 47 degrees Fahrenheit event Temperature), words (e.g.,
Likely Slight Chance event Chance Rain).
specifically, read table follows. Temperature, field time
value 17.00-06.00(+1 day) refers temperatures measured 5pm 6am
following day. minimum temperature recorded time period 30 degrees Fahrenheit
(field min), maximum 47 degrees (field max) average temperature 40 degrees
(field mean). time period, wind blow south east direction (the mode
Wind Direction SE). 2550% sky covered clouds (see field percent value
25-50 Cloud Sky Cover), may interpreted slightly cloudy outlook. Finally,
5pm 9pm likely rain, indicated mode field value Likely Chance
Rain event.
Note temperature values Fahrenheit scale. Fahrenheit scale alternative
temperature scale Celsius, proposed 1724 physicist Daniel Gabriel Fahrenheit.
formula converts Fahrenheit degrees Celsius [F] = [C] 59 + 32. So, instance, 1 C =
30 F. Also note, measure speed used throughout experiment miles per hour, mph
short.
natural language translations generated computer program. task
rate translations two dimensions, namely Fluency Semantic Correctness scale
340

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

1 5. far Fluency concerned, judge whether translation grammatical
well-formed English gibberish. translation grammatical, rate
high terms fluency. lot repetition translation seems like word salad,
give low number.
Semantic Correctness refers meaning conveyed translation whether corresponds reported tabular data. words, translation convey
content table not? translation nothing categories, fields values
described table, probably give low number Semantic Correctness.
translation captures information listed table, give high number. Bear mind slight numerical deviations normal penalized (e.g.,
common weather forecasters round wind speed values closest 5, i.e., 50 mph instead
47 mph).
A.2 Rating Examples
Example 1, would probably give translation high score Fluency (e.g., 4 5), since
coherent contain grammatical errors. However, give low
score Semantic Correctness (e.g., 13), conveys information table.
example, windy wind 5 15 mph relate wind speed
mentioned table. Let us consider following example:
Example 2
Category

Fields

Temperature

time: 17.0006.00(+1 day) min: 40

mean: 45 max: 50

Wind Direction time: 17.0006.00(+1 day) mode:
Wind Speed

time: 17.00 06.00(+1 day) min: 5

mean: 7 max: 15

Cloud Sky Cover time: 17.0006.00(+1 day) percent: 025
Sunny, low around 40. South wind 5 15 mph.
Here, give translation high scores dimensions, namely Fluency Semantic Correctness. text grammatical succinctly describes content table.
example, 4 5 would appropriate numbers.
Example 3
Category

Fields

Temperature

time: 17.0006.00(+1 day) min: 30

mean: 40 max:47

Wind Direction time: 17.0006.00(+1 day) mode: ESE
Around 40. Around 40. Around 40. East wind.
341

fiKONSTAS & L APATA

example 3, translation scores poorly Fluency Semantic Correctness. text
many repetitions clear correspondence translation table. around
40 probably refers temperature, clear context text. east
wind refers wind direction, missing verb preposition would relate
weather outlook. Appropriate scores dimensions would 1 2.
Finally, judging translation pay attention values fields table
addition event categories. example, may event Chance Rain value
None mode field. means likely rain, penalize mention
rain text, unless another event Chance Rain different time period
different value mode field.
A.3 Rating Procedure
start experiment asked enter personal details. Next,
presented 15 table-translation pairs evaluate manner described above.
shown one pair time. finish rating, click button bottom right
advance next response.
Things remember:
unsure rate translation, click top right window Help
link. may also leave open course experiment reference.
Higher numbers represent positive opinion translation lower numbers negative
one.
spend long analyzing translations; able rate
read first time.
right wrong answer, use judgment rating translation.
A.4 Personal Details
part experiment ask couple personal details. information
treated confidentially made available third party. addition, none
responses associated name way. ask supply following
information.
name email address.
age sex.
specify, Language Region, place (city, region/state/province, country)
learnt first language.
enter code provided end experiment Mechanical Turk HIT.
342

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

References
Amazon Mechanical Turk (2012). Retrieved https://www.mturk.com..
Angeli, G., Liang, P., & Klein, D. (2010). simple domain-independent probabilistic approach
generation. Proceedings 2010 Conference Empirical Methods Natural
Language Processing, pp. 502512, Cambridge, MA.
Banko, M., Mittal, V. O., & Witbrock, M. J. (2000). Headline generation based statistical translation. Proceedings Association Computational Linguistics, pp. 318325, Hong Kong.
Barzilay, R., & Lapata, M. (2005). Collective content selection concept-to-text generation.
Proceedings Human Language Technology Empirical Methods Natural Language
Processing, pp. 331338, Vancouver, British Columbia.
Belz, A. (2008). Automatic generation weather forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engineering, 14(4), 431455.
Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-best parsing maxent discriminative
reranking. Proceedings 43rd Annual Meeting Association Computational
Linguistics, pp. 173180, Ann Arbor, Michigan.
Chen, D. L., & Mooney, R. J. (2008). Learning sportscast: test grounded language acquisition. Proceedings International Conference Machine Learning, pp. 128135,
Helsinki, Finland.
Chiang, D. (2007). Hierarchical phrase-based translation. Computational Linguistics, 33(2), 201
228.
Collins, M. (2002). Discriminative training methods hidden Markov models: Theory experiments perceptron algorithms. Proceedings 2002 Conference Empirical
Methods Natural Language Processing, pp. 18, Philadelphia, Pennsylvania.
Dahl, D. A., Bates, M., Brown, M., Fisher, W., Hunicke-Smith, K., Pallett, D., Pao, C., Rudnicky,
A., & Shriberg, E. (1994). Expanding scope ATIS task: ATIS-3 corpus.
Proceedings Workshop Human Language Technology, pp. 4348, Plainsboro, New
Jersey.
Dale, R., Geldof, S., & Prost, J.-P. (2003). Coral: Using natural language generation navigational
assistance. Proceedings 26th Australasian Computer Science Conference, pp. 3544,
Adelaide, Australia.
de Gispert, A., Iglesias, G., Blackwood, G., Banga, E. R., & Byrne, W. (2010). Hierarchical phrasebased translation weighted finite-state transducers shallow-n grammars. Computational Linguistics, 36(3), 505533.
Duboue, P. A., & McKeown, K. R. (2002). Content planner construction via evolutionary algorithms
corpus-based fitness function. Proceedings International Natural Language Generation, pp. 8996, Ramapo Mountains, NY.
Gallo, G., Longo, G., Pallottino, S., & Nguyen, S. (1993). Directed hypergraphs applications.
Discrete Applied Mathematics, 42, 177201.
Goldberg, E., Driedger, N., & Kittredge, R. (1994). Using natural-language processing produce
weather forecasts. IEEE Expert, 9(2), 4553.
343

fiKONSTAS & L APATA

Good, I. J. (1953). population frequencies species estimation population parameters. Biometrika, 40(3/4), pp. 237264.
Goodman, J. (1999). Semiring parsing. Computational Linguistics, 25(4), 573605.
Green, N. (2006). Generation biomedical arguments lay readers. Proceedings 5th
International Natural Language Generation Conference, pp. 114121, Sydney, Australia.
Huang, L. (2008). Forest reranking: Discriminative parsing non-local features. Proceedings
ACL-08: HLT, pp. 586594, Columbus, Ohio.
Huang, L., & Chiang, D. (2005). Better k-best parsing. Proceedings 9th International
Workshop Parsing Technology, pp. 5364, Vancouver, British Columbia.
Huang, L., & Chiang, D. (2007). Forest rescoring: Faster decoding integrated language models.
Proceedings 45th Annual Meeting Association Computational Linguistics,
pp. 144151, Prague, Czech Republic.
Iglesias, G., Allauzen, C., Byrne, W., de Gispert, A., & Riley, M. (2011). Hierarchical phrase-based
translation representations. Proceedings 2011 Conference Empirical Methods
Natural Language Processing, pp. 13731383, Edinburgh, Scotland, UK. Association
Computational Linguistics.
Kasami, T. (1965). efficient recognition syntax analysis algorithm context-free languages. Tech. rep. AFCRL-65-758, Air Force Cambridge Research Lab, Bedford, Massachusetts.
Kim, J., & Mooney, R. (2010). Generative alignment semantic parsing learning ambiguous supervision. Proceedings 23rd Conference Computational Linguistics,
pp. 543551, Beijing, China.
Klein, D., & Manning, C. (2004). Corpus-based induction syntactic structure: Models dependency constituency. Proceedings 42nd Meeting Association Computational Linguistics, pp. 478485, Barcelona, Spain.
Klein, D., & Manning, C. D. (2001). Parsing hypergraphs. Proceedings 7th International Workshop Parsing Technologies, pp. 123134, Beijing, China.
Konstas, I. (2013). ATIS dataset retrieved http://homepages.inf.ed.ac.uk/ikonstas/
index.php?page=resources..
Konstas, I., & Lapata, M. (2012). Concept-to-text generation via discriminative reranking.
Proceedings 50th Annual Meeting Association Computational Linguistics:
Human Language Technologies, pp. 369378, Jeju, South Korea.
Li, Z., & Eisner, J. (2009). First- second-order expectation semirings applications
minimum-risk training translation forests. Proceedings 2009 Conference
Empirical Methods Natural Language Processing, pp. 4051, Suntec, Singapore.
Liang, P., Bouchard-Cote, A., Klein, D., & Taskar, B. (2006). end-to-end discriminative approach machine translation. Proceedings 21st International Conference Computational Linguistics 44th Annual Meeting Association Computational
Linguistics, pp. 761768, Sydney, Australia.
344

fiA G LOBAL ODEL C ONCEPT- -T EXT G ENERATION

Liang, P., Jordan, M., & Klein, D. (2009). Learning semantic correspondences less supervision.
roceedings Joint Conference 47th Annual Meeting ACL 4th
International Joint Conference Natural Language Processing AFNLP, pp. 9199,
Suntec, Singapore.
Lu, W., & Ng, H. T. (2011). probabilistic forest-to-string model language generation
typed lambda calculus expressions. Proceedings 2011 Conference Empirical
Methods Natural Language Processing, pp. 16111622, Edinburgh, Scotland, UK.
Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building large annotated corpus
English: Penn treebank. Comput. Linguist., 19(2), 313330.
Nederhof, M.-J., & Satta, G. (2004). language intersection problem non-recursive contextfree grammars. Information Computation, 192(2), 172 184.
Och, F. J. (2003). Minimum error rate training statistical machine translation. Proceedings
Annual Meeting Association Computational Linguistics, pp. 160167, Sapporo,
Japan.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: method automatic evaluation machine translation. Proceedings 40th Annual Meeting Association
Computational Linguistics, pp. 311318, Philadelphia, Pennsylvania.
Ratnaparkhi, A. (2002). Trainable approaches surface natural language generation
application conversational dialog systems. Computer Speech & Language, 16(3-4), 435
455.
Reiter, E., & Dale, R. (2000). Building natural language generation systems. Cambridge University
Press, New York, NY.
Reiter, E., Sripada, S., Hunter, J., & Davy, I. (2005a). Choosing words computer-generated
weather forecasts. Artificial Intelligence, 167, 137169.
Reiter, E., Sripada, S., Hunter, J., Yu, J., & Davy, I. (2005b). Choosing words computer-generated
weather forecasts. Artificial Intelligence, 167, 137169.
Shieber, S. M., Schabes, Y., & Pereira, F. C. N. (1995). Principles implementation deductive
parsing. Logic Programming, 24, 336.
Sripada, S. G., Reiter, E., Hunter, J., & Yu, J. (2003). Generating English summaries time series
data using gricean maxims. Proceedings Ninth ACM SIGKDD International
Conference Knowledge Discovery Data Mining, pp. 187196. ACM Press.
Stolcke, A. (2002). SRILM extensible language modeling toolkit. Hansen, J. H. L., &
Pellom, B. L. (Eds.), Proceedings 7th International Conference Spoken Language
Processing, pp. 901904, Denver, Colorado. ISCA.
Toutanova, K., Klein, D., Manning, C. D., & Singer, Y. (2003). Feature-rich part-of-speech tagging
cyclic dependency network. Proceedings 2003 Conference North
American Chapter Association Computational Linguistics Human Language
Technology - Volume 1, pp. 173180, Edmonton, Canada.
Turner, R., Sripada, Y., & Reiter, E. (2009). Generating approximate geographic descriptions.
Proceedings 12th European Workshop Natural Language Generation, pp. 4249,
Athens, Greece.
345

fiKONSTAS & L APATA

Wong, Y. W., & Mooney, R. (2007). Generation inverting semantic parser uses statistical
machine translation. Proceedings Human Language Technology Conference
North American Chapter Association Computational Linguistics, pp. 172
179, Rochester, NY.
Yandell, B. S. (1997). Practical Data Analysis Designed Experiments. Chapman & Hall/CRC.
Younger, D. H. (1967). Recognition parsing context-free languages time n3 . Information
Control, 10(2), 189208.
Zettlemoyer, L., & Collins, M. (2007). Online learning relaxed CCG grammars parsing
logical form. Proceedings 2007 Joint Conference Empirical Methods Natural
Language Processing Computational Natural Language Learning, pp. 678687, Prague,
Czech Republic.

346

fiJournal Artificial Intelligence Research 48 (2013) 115174

Submitted 11/12; published 10/13

Taming Infinite Chase: Query Answering
Expressive Relational Constraints
Andrea Cal

andrea@dcs.bbk.ac.uk

Department Computer Science Information Systems
University London, Birkbeck College, UK

Georg Gottlob

georg.gottlob@cs.ox.ac.uk

Department Computer Science
University Oxford, UK

Michael Kifer

kifer@cs.stonybrook.edu

Department Computer Science
Stony Brook University, USA

Abstract
chase algorithm fundamental tool query evaluation testing query
containment tuple-generating dependencies (TGDs) equality-generating dependencies (EGDs). far, research topic focused cases
chase procedure terminates. paper introduces expressive classes TGDs defined via
syntactic restrictions: guarded TGDs (GTGDs) weakly guarded sets TGDs (WGTGDs). classes, chase procedure guaranteed terminate thus may
infinite outcome. Nevertheless, prove problems conjunctive-query
answering query containment TGDs decidable. provide decision
procedures tight complexity bounds problems. show EGDs
incorporated results providing conditions EGDs
harmfully interact TGDs affect decidability complexity query
answering. show applications aforesaid classes constraints problem
answering conjunctive queries F-Logic Lite, object-oriented ontology language,
tractable Description Logics.

1. Introduction
paper studies simple yet fundamental rule-based language ontological reasoning
query answering: language tuple-generating dependencies (TGDs). formalism captures wide variety logics far considered unrelated other:
OWL-based languages EL (Baader, Brandt, & Lutz, 2005) DL-Lite (Calvanese,
De Giacomo, Lembo, Lenzerini, & Rosati, 2007; Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009) one hand object-based languages like F-Logic Lite (Cal &
Kifer, 2006) other. present paper significant extension earlier work
(Cal, Gottlob, & Kifer, 2008), since applied contexts gave rise
Datalog family (Cal, Gottlob, & Pieris, 2011) ontology languages. present
paper focuses fundamental complexity results underlying one key fragments
family. Subsequent work focused study various special cases
formalism (Cal, Gottlob, & Lukasiewicz, 2012a), complexity, extensions based
paradigms (Cal, Gottlob, & Pieris, 2012b).
c
2013
AI Access Foundation. rights reserved.

fiCal, Gottlob & Kifer

work also closely related work query answering query containment (Chandra & Merlin, 1977), central problems database theory knowledge representation and, cases, reducible other. especially
interesting presence integrity constraintsor dependencies, database parlance.
databases, query containment used query optimization schema integration (Aho, Sagiv, & Ullman, 1979; Johnson & Klug, 1984; Millstein, Levy, & Friedman,
2000), knowledge representation often used object classification, schema
integration, service discovery, (Calvanese, De Giacomo, & Lenzerini, 2002; Li &
Horrocks, 2003).
practically relevant instance containment problem first studied Johnson Klug (1984) functional inclusion dependencies later Calvanese,
De Giacomo, Lenzerini (1998). Several additional decidability results obtained
focusing concrete applications. instance, work Cal Martinenghi (2010)
considers constraints arising Entity-Relationship diagrams, Cal
Kifer (2006) considers constraints derived relevant subset F-logic (Kifer, Lausen,
& Wu, 1995), called F-Logic Lite.
literature studies variants subclasses tuple-generating dependencies (TGDs)
purpose reasoning query answering. TGD Horn-like rule existentially-quantified variables head. early works subject dubbed resulting
language Datalog value invention (Mailharrow, 1998; Cabibbo, 1998). formally,
TGD XY(X, Y) Z(X, Z) first-order formula, (X, Y) (X, Z)
conjunctions atoms, called body head TGD, respectively. TGD satisfied relational instance B whenever body TGD satisfied B
B also satisfies head TGD. possible enforce TGD satisfied
adding new facts B head, thus TGD itself, become satisfied.
new facts contain labeled null values (short: nulls) positions corresponding
variables Z. nulls similar Skolem constants. chase database
presence set TGDs process iterative enforcement dependencies
, fixpoint reached. result process, also call chase,
infinite and, case, procedure cannot used without modifications
decision algorithms. Nevertheless, result chase serves fundamental theoretical
tool answering queries presence TGDs (Cal, Lembo, & Rosati, 2003a; Fagin,
Kolaitis, Miller, & Popa, 2005) representative models .
present paper, focus specific logical theory. Instead, tackle
common issue possibly non-terminating chase underlying several earlier
studies, including works Johnson Klug (1984), Cal Martinenghi (2010),
Cal Kifer (2006). works study constraints language TGDs
equality-generating dependencies (EGDs) using chase technique, face
problem chase procedure might generate infinite result. deal
problem much general way carving large class constraints
infinite chase tamed, i.e., modified would become decision
procedure query answering.
Section 3, define notions sets guarded TGDs (GTGDs) weakly
guarded sets TGDs (WGTGDs). TGD guarded body contains atom called
116

fiTaming Infinite Chase

guard covers variables occurring body. WGTGDs generalize guarded TGDs
requiring guards cover variables occurring so-called affected positions
(predicate positions may contain labeled nulls generated chase). Note
inclusion dependencies (or IDs) viewed trivially guarded TGDs. importance guards lies Theorem 3.5, shows fixed set u GTGDs plus
single non-guarded TGD, query evaluation u undecidable. However,
show WGTGDs (possibly infinite) result chase finite treewidth
(Theorem 3.14). use result together well-known results generalized tree-model property (Goncalves & Gradel, 2000; Gradel, 1999) show evaluating
Boolean conjunctive queries decidable WGTGDs (and thus also GTGDs). Unfortunately, result directly provide useful complexity bounds.
Section 4, show lower complexity bounds conjunctive query answering
weakly guarded sets TGDs. prove, Turing machine simulations, query evaluation weakly guarded sets TGDs exptime-hard case fixed set TGDs,
2exptime-hard case TGDs part input.
Section 5, address upper complexity bounds query answering weakly
guarded sets TGDs. Let us first remark showing |= Q equivalent
showing theory = {Q} unsatisfiable. Unfortunately, general
guarded Q WGTGDs generally non-guarded first-order
sentences (while GTGDs are). Therefore, cannot (as one might think first glance)
directly use known results guarded logics (Goncalves & Gradel, 2000; Gradel, 1999)
derive complexity results query evaluation. thus develop completely new algorithms
prove problem question exptime-complete case bounded
predicate arities and, even case TGDs fixed, 2exptime-complete general.
Section 6, derive complexity results reasoning GTGDs. general
case, complexity WGTGDs but, interestingly, reasoning fixed set
dependencies (which usual setting data exchange description logics),
get much better results: evaluating Boolean queries np-complete ptime case
query atomic. Recall Boolean query evaluation np-hard even case
simple database without integrity constraints (Chandra & Merlin, 1977). Therefore,
np upper bound general Boolean queries optimal, i.e., class TGDs
query evaluation (or query containment) efficient.
Section 7, describe semantic condition weakly guarded sets TGDs.
prove whenever set WGTGDs fulfills condition, answering Boolean queries
np, answering atomic queries, well queries bounded treewidth, ptime.
Section 8 extends results case TGDs multiple-atom heads.
extension trivial cases except case bounded predicate arity.
Section 9 deals equality generating dependencies (EGDs), generalization functional dependencies. Unfortunately, shown works Chandra Vardi (1985),
Mitchell (1983), Johnson Klug (1984), Koch (2002), Cal et al. (2003a), query answering many problems become undecidable case admit TGDs
EGDs. remains undecidable even mix simplest class guarded TGDs, namely,
inclusion dependencies, simplest type EGDs, namely functional dependencies,
even key dependencies (Chandra & Vardi, 1985; Mitchell, 1983; Johnson & Klug, 1984;
Cal et al., 2003a). Section 9, present sufficient semantic condition decidabil117

fiCal, Gottlob & Kifer

BCQ type
GTGDs WGTGDs
general
2exptime 2exptime
atomic fixed 2exptime 2exptime
Query answering variable TGDs.
BCQ type
GTGDs
general
np
atomic fixed
ptime
Query answering fixed

WGTGDs
exptime
exptime
TGDs.

BCQ type
GTGDs WGTGDs
general
exptime
exptime
atomic fixed exptime
exptime
Query answering fixed predicate arity.
Figure 1: Summary results. complexity bounds tight.
ity query-answering sets TGDs general EGDs. call EGDs innocuous
when, roughly speaking, application (i.e., enforcement) introduce new atoms,
eliminates atoms. show innocuous EGDs essentially ignored
conjunctive query evaluation query containment testing.
TGD-based ontology languages paper part larger family ontology
languages called Datalog (Cal et al., 2011). results subsume main decidability
np-complexity result Johnson Klug (1984), decidability complexity
results F-Logic Lite Cal Kifer (2006), DL-Lite special cases.
fact, Section 10 shows results even general that.
complexity results paper, together immediate consequences, summarized Figure 1, complexity bounds tight. Notice
complexity case fixed queries fixed TGDs so-called data complexity,
i.e., complexity respect data only, particular interest database
applications. complexity variable Boolean conjunctive queries (BCQs) variable
TGDs called combined complexity. easy see (but prove formally
classes) complexity results atomic fixed queries extend queries
bounded width, width mean treewidth even hypertree width (Gottlob,
Leone, & Scarcello, 2002)see also works Adler, Gottlob, Grohe (2007),
Gottlob, Leone, Scarcello (2001).

2. Preliminaries
section define basic notions use throughout paper.
2.1 Relations, Instances Queries
relational schema R set relational predicates, aritya non-negative
integer represents number arguments predicate takes. write r/n say
118

fiTaming Infinite Chase

relational predicate r arity n. Given n-ary predicate r R, position r[k],
1 6 k 6 n, refers k-th argument r. assume underlying relational
schema R postulate queries constraints use predicates R.
schema R sometimes omitted clear context immaterial.
introduce following pairwise disjoint sets symbols: (i) (possibly infinite) set
data constants, constitute normal domain databases schema
R; (ii) set N labeled nulls, i.e., fresh Skolem constants; (iii) infinite set V
variables, used queries constraints. Different constants represent different
values (unique name assumption), different nulls may represent value.
also assume lexicographic order N , every labeled null N following
constant symbols . Sets variables (or sequences, order relevant)
denoted X, i.e., X = X1 , . . . , Xk , k. notation X shorthand
X1 . . . Xk , similarly X.
instance relational predicate r/n (possibly infinite) set atomic formulas (atoms) form r(c1 , . . . , cn ), {c1 , . . . , cn } N . atoms also
called facts. fact r(c1 , . . . , cn ) true, say tuple hc1 , . . . , cn belongs
instance r (or r, confusion arise). instance
relational schema R = {r1 , . . . , rm } set comprised instances r1 , . . . , rm .
instances treated first-order formulas, labeled null viewed existential variable name, relational instances nulls correspond
conjunction atoms preceded existential quantification nulls. instance, {r(a, z1 , z2 , z1 ), s(b, z2 , z3 )}, {z1 , z2 , z3 } N {a, b} , expressed
z1 z2 z3 r(a, z1 , z2 , z1 ) s(b, z2 , z3 ). following, omit quantifiers.
fact r(c1 , . . . , cn ) said ground ci {1, . . . , n}. case,
also tuple hc1 , . . . , cn said ground. relation schema instance whose
facts ground said ground, ground instance R also called database.
sequence atoms ha1 , . . . , ak conjunction atoms a1 . . . ak , use
atoms(A) denote set atoms A: atoms(A) = {a1 , . . . , ak }. Given (ground
non-ground) atom a, domain a, denoted dom(a), set values (variables,
constants orSlabeled nulls) appear arguments a. set atoms, define
dom(A) = aA dom(a). sequence conjunction atoms define
dom(A) = dom(atoms(A)). atom, set, sequence, conjunction atoms,
write vars(A) denote set variables A.
Given instance B relational schema R, Herbrand Base B, denoted HB (B),
set atoms formed using predicate symbols R arguments
dom(B). Notice extension classical notion Herbrand Base,
includes ground atoms only.
n-ary conjunctive query (CQ) R formula form q(X1 , . . . , Xn ) (X),
q predicate appearing R, variables X1 , . . . , Xn appear X,
(X), called body query, conjunction atoms constructed predicates
R. arity query arity head predicate q. q arity 0,
conjunctive query called Boolean (BCQ). BCQs, convenient drop head
predicate simply view query set atoms (X). stated otherwise,
assume queries contain constants, since constants eliminated queries
simple polynomial time transformation. also sometimes refer conjunctive
119

fiCal, Gottlob & Kifer

queries queries. size conjunctive query Q denoted |Q|; represents
number atoms Q.
2.2 Homomorphisms
mapping set symbols S1 another set symbols S2 seen function
: S1 S2 defined follows: (i) (the empty mapping) mapping; (ii)
mapping, {X }, X S1 S2 mapping already
contain X 6= . X mapping , write (X) = .
notion mapping naturally extended atoms follows. = r(c1 , . . . , cn )
atom mapping, define (a) = r((c1 ), . . . , (cn )). set atoms,
= {a1 , . . . , }, (A) = {(a1 ), . . . , (am )}. set atoms (A) also called image
respect . conjunction atoms C = a1 . . . , (C) shorthand
(atoms(C)), is, (C) = {(a1 ), . . . , (am )}.
homomorphism set atoms A1 another set atoms A2 , dom(A1
A2 ) N V mapping dom(A1 ) dom(A2 ) following
conditions hold: (1) c (c) = c; (2) (A1 ) A2 , i.e., atom, a, A1 ,
atom (a) A2 . case, say A1 maps A2 via .
answer conjunctive query Q form q(X1 , . . . , Xn ) (X) instance
B R, denoted Q(B), defined follows: tuple ( N )n , Q(B) iff
homomorphism maps (X) atoms B, hX1 , . . . , Xn t. case,
abuse notation, also write q(t) Q(B). Boolean conjunctive query Q
positive answer B iff hi (the tuple elements) Q(B); otherwise, said
negative answer.
2.3 Relational Dependencies
define main type dependencies used paper, tuple-generating
dependencies, TGDs.
Definition 2.1. Given relational schema R, TGD R first-order formula
form XY(X, Y) Z(X, Z), (X, Y) (X, Z) conjunctions
atoms R, called body head TGD, respectively; denoted body()
head (). dependency satisfied instance B R if, whenever
homomorphism h maps atoms (X, Y) atoms B, exists extension
h2 h (i.e., h2 h) maps atoms (X, Z) atoms B.
simplify notation, usually omit universal quantifiers TGDs.
also sometimes call TGDs rules implication symbol them. Notice
that, general, constants appear body, also heads
TGDs. simplicity without loss generality, assume constants
appear head TGDs also appear body TGD.
symbol |= used henceforth usual logical entailment, sets
atoms TGDs viewed first-order theories. theories, restrict
finite models: consider arbitrary models could finite infinite.
aspect discussed Section 11.
120

fiTaming Infinite Chase

2.4 Query Answering Containment TGDs
define notion query answering TGDs. similar notion used data
exchange (Fagin et al., 2005; Gottlob & Nash, 2006) query answering incomplete
data (Cal et al., 2003a). Given database satisfy constraints ,
first define set completions (or repairssee Arenas, Bertossi, & Chomicki, 1999)
database, call solutions.
Definition 2.2. Consider relational schema R, set TGDs , database
R. set instances {B | B |= } called set solutions given ,
denoted sol(D, ).
following definition problem, denote CQAns, answering
conjunctive queries TGDs. answers defined also referred certain
answers (see Fagin et al., 2005).
Definition 2.3. Consider relational schema R, set TGDs , database R,
conjunctive query Q R. answer conjunctive query Q given ,
denoted ans(Q, D, ), set tuples every B sol(D, ), Q(B)
holds.
Notice components definition necessarily constants
. ans(Q, D, ), also write {Q} |= q(t), Q represented
rule body(Q) q(X).
Containment queries relational databases long considered fundamental
problem query optimization, especially query containment constraints
TGDs. formally define problem, call CQCont.
Definition 2.4. Consider relational schema R, set TGDs R, two conjunctive
queries Q1 , Q2 expressed R. say Q1 contained Q2 , denoted
Q1 Q2 , every instance B R B |= Q1 (B) subset
Q2 (B).
2.5 Chase
chase introduced procedure testing implication dependencies (Maier,
Mendelzon, & Sagiv, 1979), later also employed checking query containment (Johnson & Klug, 1984) query answering incomplete data relational dependencies (Cal et al., 2003a). Informally, chase procedure process repairing database
respect set dependencies, result chase satisfies dependencies. chase may refer either chase procedure output. chase
works database so-called TGD chase rule, defines result
applications TGD comes two flavors: oblivious restricted.
Definition 2.5. [Oblivious Applicability] Consider instance B schema R,
TGD = (X, Y) Z (X, Z) R. say obliviously applicable B
exists homomorphism h h((X, Y)) B.
121

fiCal, Gottlob & Kifer

Definition 2.6. [Restricted Applicability] Consider instance B schema R,
TGD = (X, Y) Z (X, Z) R. say restrictively applicable B
exists homomorphism h h((X, Y)) B, extension h
h|X h ((X, Z)) B.1
oblivious form applicability called way forgets check
whether TGD already satisfied. contrast, TGD restrictively applicable
already satisfied.
Definition 2.7. [TGD Chase Rule] Let TGD form (X, Y) Z (X, Z)
suppose obliviously (resp., restrictively) applicable instance B via
homomorphism h. Let h extension h|X that, Z Z, h (Z) fresh
labeled null N occurring B, following lexicographically B.
result oblivious (resp., restricted ) application B h B = Bh ((X, Z)).
,h

,h

write B B (resp., B R B ) denote B obtained B
single oblivious (resp., restricted) chase step.
TGD chase rule, defined above, basic building block construct chase
database set TGDs. Depending notion applicability useoblivious
restrictedwe get oblivious restricted chase. formal definition
chase given below.
Definition 2.8. [Oblivious Restricted Chase] Let database set
TGDs. oblivious (resp., restricted) chase sequence respect sequence
,hi

instances B0 , B1 , B2 , . . . B0 = and, > 0, Bi Bi+1 (resp.,
,hi

Bi R Bi+1 ) . also assume chase sequence pair
hi , hi never applied once. oblivious (resp., restricted) chase
respect , denoted Ochase(D, ) (resp., Rchase(D, )), defined follows:
finite oblivious (resp., restricted) chase respect finite oblivious
,hi

,hi

(resp., restricted) chase sequence B0 , . . . , Bm Bi Bi+1 (resp., Bi R
Bi+1 ) 0 6 < m, application yields
instance B 6= Bm . define Ochase(D, ) = Bm (resp., Rchase(D, ) = Bm ).
,hi

infinite oblivious (resp., restricted) chase sequence B0 , B1 , . . ., Bi Bi+1
,hi

(resp., Bi R Bi+1 ) > 0, fair whenever TGD = (X, Y)
Z (X, Z) obliviously (resp., restrictedly) applicable Bi homomorphism h, exists extension h h|X k > 0 h (head ())
Bk . infinite oblivious chase respect fair infinite chase sequence
,hi

,hi

B0 , B1 , . . . Bi Bi+1 (resp., Bi R Bi+1 ) > 0. case,
define Ochase(D, ) = limi Bi (resp., Rchase(D, ) = limi Bi ).
easy see chase infinite, sequence applications chase
rule infinite. remark chase defined databases ground tuples. However, definition straightforwardly applies also arbitrary instances, possibly containing
1. h|X denotes restriction h set variables X.

122

fiTaming Infinite Chase

labeled nulls. assume fair deterministic strategy constructing chase sequences.
use Ochase [i] (D, ) (resp., Rchase [i] (D, )) denote result i-th step oblivious (resp., restricted) chase respect . Notice Ochase [i] (D, ) (resp.,
Rchase [i] (D, )) called oblivious (resp., restricted) chase respect
derivation level i, work Cal et al. (2012a).
Example 2.9. example, show oblivious chase procedure. Consider following set = {1 , 2 , 3 , 4 } TGDs.
1 :
2 :
3 :
4 :

r3 (X, )
r1 (X, )
r1 (X, ), r2 (Y )
r1 (X, )






r2 (X)
Z r3 (Y, Z)
Z r1 (Y, Z)
r2 (Y )

let = {r1 (a, b)}. chase procedure adds following sequence atoms:
r3 (b, z1 ) via 2 , r2 (b) via 4 , r1 (b, z2 ) via 3 , r3 (z2 , z3 ) via 2 , r2 (z2 ) via 4 , on.
2.6 Query Answering Chase
problems query containment answering TGDs closely related
notion chase, explained below.
Theorem 2.10 (see Nash, Deutsch, & Remmel, 2006). Consider relational schema R,
database R, set TGDs R, n-ary conjunctive query Q head-predicate
q, n-ary ground tuple (with values ). ans(Q, D, ) iff exists
homomorphism h h(body(Q)) Rchase(D, ) h(head (Q)) = q(t).
Notice fact h(body(Q)) Rchase(D, ) h(head (Q)) = q(t) equivalent saying q(t) Q(Rchase(D, )), Rchase(D, ) {Q} |= q(t).
result Theorem 2.10 important, holds (possibly infinite) restricted
chase universal solution (Fagin et al., 2005), i.e., representative instances
sol(D, ). formally, universal solution (possibly infinite) instance
U that, every instance B sol(D, ), exists homomorphism maps U
B. work Nash et al. (2006) shown chase constructed respect
TGDs universal solution.
freezing homomorphism query homomorphism maps every distinct
variable query distinct labeled null N . following well known result
slight extension result Chandra Merlin (1977).
Theorem 2.11. Consider relational schema R, set TGDs R, two conjunctive queries Q1 , Q2 R. Q1 Q2 iff (head (Q1 )) Q2 (Rchase((body(Q1 )), )
freezing homomorphism Q1 .
results Johnson Klug (1984) Nash et al. (2006),
easily obtain following result, considered folklore.
Corollary 2.12. problems CQAns CQCont mutually logspace-reducible.
123

fiCal, Gottlob & Kifer

2.7 Oblivious vs. Restricted Chase
observed Johnson Klug (1984) case functional inclusion dependencies, things complicated restricted chase used instead oblivious
one, since applicability TGD depends presence atoms previously added
database chase. technically easier use oblivious chase
used lieu restricted chase because, shall prove now, result similar
Theorem 2.10 holds oblivious chase, i.e., also universal. result, best
knowledge, never explicitly stated before. sake completeness,
present full proof here.
Theorem 2.13. Consider set TGDs relational schema R, let
database R. exists homomorphism (Ochase(D, ))
Rchase(D, ).
Proof. proof induction number applications TGD chase rule
construction oblivious chase Ochase(D, ). want prove that,
> 0, homomorphism Ochase [m] (D, ) Rchase(D, ).
Base case. base case, = 0, TGD rule yet applied,
Ochase [0] (D, ) = Rchase(D, ) required homomorphism simply identity homomorphism 0 .
Inductive case. Assume applied TGD chase rule times obtained
Ochase [m] (D, ). induction hypothesis, exists homomorphism maps
Ochase [m] (D, ) Rchase(D, ). Consider (m + 1)-th application TGD chase
rule, TGD form (X, Y) Z(X, Z). definition applicability TGDs,
homomorphism maps (X, Y) atoms Ochase(D, )
suitably extended another homomorphism, , maps variables
Z fresh null N already present Ochase [m] (D, ). result application TGD, atoms ((X, Z)) added Ochase [m] (D, ), thus obtaining
Ochase [m+1] (D, ). Consider homomorphism R = , maps (X, Y)
atoms Rchase(D, ). Since Rchase(D, ) satisfies dependencies (and
Ochase(D, )), extension R R maps (X, Z) tuples Rchase(D, ).
Denoting Z = Z1 , . . . , Zk , define m+1 = {O (Zi ) R (Zi )}16i6k . complete proof, need show m+1 indeed homomorphism. addition (Zi ) R (Zi ), 1 6 6 k, compatible none
(Zi ) appears . Therefore m+1 well-defined mapping. Now, consider atom
r(X, Z) (X, Z). atom (r(X, Z)) added Ochase(D, ) (m+1)-th
step m+1 (r(X, Y)) = m+1 (r(O (X), (Z))) = r(m+1 (O (X), m+1 (O (Z)). Notice m+1 (O (X)) = m+1 (O (X)) = R (X) = R (X), m+1 (O (Z)) = R (Z).
Therefore, m+1 (r(X, Z)) = r(R (X), R (Z)) = R (r(X, Z)), Rchase(D, ),
construction.
desired homomorphism Ochase(D, ) Rchase(D, ) therefore

=

.
i=0
Corollary 2.14. Given set TGDs relational schema R database
R, Ochase(D, ) universal solution .
124

fiTaming Infinite Chase

Corollary 2.15. Given Boolean query Q schema R, database R,
set TGDs , Ochase(D, ) |= Q Rchase(D, ) |= Q.
following, unless explicitly stated otherwise, chase mean oblivious chase,
chase(D, ) stand Ochase(D, ).
2.8 Decision Problems
Recall that, Theorem 2.10, |= Q iff chase(D, ) |= Q. Based this, define
two relevant decision problems prove logspace-equivalence.
Definition 2.16. conjunctive query evaluation decision problem CQeval defined
follows. Given conjunctive query Q n-ary head predicate q, set TGDs ,
database ground n-tuple t, decide whether ans(Q, D, ) or, equivalently,
whether chase(D, ) {Q} |= q(t).
Definition 2.17. Boolean conjunctive query evaluation problem BCQeval defined
follows. Given Boolean conjunctive query Q, set TGDs , database D, decide
whether chase(D, ) |= Q.
following result implicit work Chandra Merlin (1977).
Lemma 2.18. problems CQeval BCQeval logspace-equivalent.
Proof. Notice BCQeval trivially made special instance CQeval, e.g.,
adding propositional atom head atom. thus suffices show CQeval polynomially
reduces BCQeval. Let hQ, D, , q(t)i instance CQeval, q/n head
predicate Q ground n-tuple. Assume head atom Q q(X1 , . . . , Xn )
= hc1 , . . . , cn i. define Q Boolean conjunctive query whose body
body(Q) q (X1 , . . . , Xn ), q fresh predicate symbol occurring D, Q,
easy see q(t) Q(chase(D, )) iff chase(D {q (c1 , . . . , cn )}, ) |= Q .
lemma well-known equivalence problem query containment TGDs CQeval problem (Corollary 2.12), three following problems
logspace-equivalent: (1) CQ-eval TGDs, (2) BCQeval TGDs, (3) query
containment TGDs. Henceforth, consider one problems,
BCQ-eval problem. above, complexity results carry problems.
Dealing multiple head-atoms. turns dealing multiple atoms
TGD heads complicates proof techniques, assume TGDs single
atom head. proving results single-headed TGDs, extend
results case multiple-atom heads Section 8.
2.9 Tree Decomposition Related Notions
introduce required notions tree decompositions. hypergraph pair
H = hV, Hi, V set nodes H 2V . elements H thus subsets
V ; called hyperedges. Gaifman graph hypergraph H = hV, Hi, denoted
125

fiCal, Gottlob & Kifer

GH , undirected graph V set nodes edge (v1 , v2 )
graph v1 v2 jointly occur hyperedge H.
Given graph G = hV, Ei, tree decomposition G pair hT, i, = hN, Ai
tree, labeling function : N 2V that:

(i) v V n N v (n); is, (N ) = nN (n) = V ;
(ii) every edge e = (v1 , v2 ) E n N (n) {v1 , v2 };
(iii) every v V , set {n N | v (n)} induces connected subtree .
width tree decomposition hT, integer value max{|(n)| 1 | n N }.
treewidth graph G = hV, Ei, denoted tw(G), minimum width tree
decompositions G. Given hypergraph H, treewidth tw(H) defined treewidth
Gaifman graph: tw(H) = tw(GH ). Notice notion treewidth immediately
extends relational structures.

3. Guarded Weakly-Guarded TGDs: Decidability Issues
section introduces guarded TGDs (GTGDs) weakly guarded sets TGDs (WGTGDs), enjoy several useful properties. particular, show query answering
TGDs decidable.
Definition 3.1. Given TGD form (X, Y) (X, Z), say (fully)
guarded TGD (GTGD) exists atom body, called guard, contains
universally quantified variables , i.e., variables X, occur (X, Y).
define weakly guarded sets TGDs, first give notion affected position
predicate relational schema, given set TGDs . Intuitively, position
affected set TGDs exists database labeled null appears
atom chase(D, ) position . importance affected positions
definitions labeled null appear non-affected positions. define notion
below.
Definition 3.2. Given relational schema R set TGDs R, position
predicate p R affected respect either:
(base case) , existentially quantified variable appears head (),

(inductive case) , variable appearing position head () also
appears body(), affected positions.
Example 3.3. Consider following set TGDs:
1 : p1 (X, ), p2 (X, ) Z p2 (Y, Z)
2 : p2 (X, ), p2 (W, X) p1 (Y, X)
Notice p2 [2] affected since Z existentially quantified 1 . variable 1
appears p2 [2] (which affected position) also p1 [2] (which affected
position). Therefore 1 make position p2 [1] affected one. Similarly,
126

fiTaming Infinite Chase

2 , X appears affected position p2 [2] also non-affected position p2 [1].
Therefore, p1 [2] affected. hand, 2 appears p2 [2] nowhere
else. Since already established p2 [2] affected position, makes p1 [1]
also affected position.
Definition 3.4. Consider set TGDs schema R. TGD form
(X, Y) (X, Z) said weakly guarded respect (W GT GD)
atom body(), called weak guard, contains universally quantified
variables appear affected positions respect also appear
non-affected positions respect . set said weakly guarded set
TGDs TGD weakly guarded respect .
GTGD WGTGD may one guard. case, pick
lexicographically first guard use criterion fixing guard rule.
actual choice affect proofs results.
following theorem shows undecidability conjunctive query answering
TGDs. result, general form, follows undecidability results TGD implication (see Beeri & Vardi, 1981; Chandra, Lewis, & Makowsky, 1981b). show
CQ answering problem remains undecidable even case fixed set singleheaded TGDs single non-guarded rule, ground atom query. proof
first principles reduces well-known halting problem Turing machines
query-answering TGDs. recently, Baget, Leclere, Mugnier, Salvat (2011a)
showed CQ answering undecidable also case contains single TGD, which,
however, contains multiple atoms head.
Theorem 3.5. exists fixed atomic BCQ Q fixed set TGDs u ,
TGDs u guarded except one, undecidable determine whether
database D, u |= Q or, equivalently, whether chase(D, u ) |= Q.
Proof. proof hinges observation that, appropriate input facts D, using
fixed set TGDs consists guarded TGDs single unguarded TGD, possible
force infinite grid appear chase(D, u ). set guarded rules, one
easily simulate deterministic universal Turing machine (TM) M, executes
every deterministic TM empty input tape, whose transition table specified
database D. done using infinite grid, i-th horizontal line
grid represents tape content instant i. assume transitions Turing
machine encoded relation trans D, example, ground atom
trans(s1 , a1 , s2 , a2 , right) means current state s1 symbol a1 read, switch
state s2 , write a2 , move right.
show infinite grid defined. Let contain (among initialization
atoms specify initial configuration M) atom index (0), defines
initial point grid. Also, make use three constants right, left, stay encoding
three types moves. Consider following TGDs:
index (X) next(X, )
next(X, ) index (Y )
trans(T), next(X1 , X2 ), next(Y1 , Y2 ) grid (T, X1 , Y1 , X2 , Y2 )
127

fiCal, Gottlob & Kifer

stands sequence argument variables S1 , A1 , S2 , A2 , , appropriate
predicate trans. Note last three TGDs non-guarded.
TGDs define infinite grid whose points co-ordinates X (horizontal
vertical, respectively) point horizontal vertical successors
also encoded. addition, point appears together possible transition
rule. hard see simulate progress Turing machine using
suitable initialization atoms guarded TGDs. end, need additional
predicates cursor (Y, X), meaning cursor position X time , state(Y, S),
expressing state time , content(X, Y, A), expressing time
, content position X tape A. following rule encodes behavior
transition rules move cursor right:
grid (S1 , A1 , S2 , A2 , right, X1 , Y1 , X2 , Y2 ),
cursor (Y1 , X1 ), state(Y1 , S1 ), content(X1 , Y1 , A1 )
cursor (Y2 , X2 ), content(X1 , Y2 , A2 ), state(Y2 , S2 ), mark (Y1 , X1 )
rule also obvious sibling rules left stay moves. sake
brevity only, rule contains multiple atoms head. problem,
rules existentially quantified variables head. Therefore, TGD
multiple head-atoms replaced equivalent set TGDs single-atom heads
identical bodies.
Notice mark predicate head marks tape cell modified
instant Y1 . need additional inertia rules, ensure positions
tape modified Y1 following time instant Y2 . end,
use two different markings: keep f tape positions follow one marked
mark , keep p preceding tape positions. way, able, making
use guarded rules only, ensure that, every instant Y1 , every tape cell X,
keep p (Y1 , X) keep f (Y1 , X) true, keeps symbol instant Y2 following
Y1 . rules propagate aforementioned markings forward backwards,
respectively, starting marked tape positions.
mark (Y1 , X1 ), grid (T, X1 , Y1 , X2 , Y2 ) keep f (Y1 , X2 )
keep f (Y1 , X1 ), grid (T, X1 , Y1 , X2 , Y2 ) keep f (Y1 , X2 )
mark (Y1 , X2 ), grid (T, X1 , Y1 , X2 , Y2 ) keep p (Y1 , X1 )
keep p (Y1 , X2 ), grid (T, X1 , Y1 , X2 , Y2 ) keep p (Y1 , X1 )
also inertia rules {a1 , . . . , , }, {a1 , . . . , , } tape alphabet:
keep f (Y1 , X1 ), grid (T, X1 , Y1 , X2 , Y2 ), content(X1 , Y1 , a) content(X1 , Y2 , a)
keep p (Y1 , X1 ), grid (T, X1 , Y1 , X2 , Y2 ), content(X1 , Y1 , a) content(X1 , Y2 , a)
Notice use constant instead variable rules order
guardedness property. therefore need two rules every tape symbol,
is, 2 + 2 inertia rules altogether.
Finally, assume, without loss generality, Turing machine single
halting state s0 encoded atom halt(s0 ) D. add guarded
128

fiTaming Infinite Chase

rule state(Y, S), halt(S) stop. clear machine halts iff chase(D, u ) |=
stop, i.e., iff u |= stop. thus reduced halting problem problem
answering atomic queries database u . latter problem therefore
undecidable.
Definition 3.6. [Guarded chase forest, restricted GCF] Given set WGTGDs
database D, guarded chase forest (GCF) , denoted gcf(D, ), constructed
follows.
(a) atom (fact) D, add node labeled d.
(b) every node v labeled chase(D, ) every atom b obtained
(and possibly atoms) one-step application TGD ,
image guard add one node v labeled b arc going v
v .
Assuming chase forest gcf(D, ) built inductively, following precisely strategy
fixed deterministic chase procedure, set non-root nodes chase forest
totally ordered relation reflects order generation. restricted GCF
, denoted rgcf(D, ), obtained gcf(D, ) eliminating subtree
rooted node w whose label duplicate earlier generated node. Thus, v
w nodes labeled atom, v w, w nodes subtree rooted
w eliminated gcf(D, ) obtain rgcf(D, ). Note rgcf(D, )
label occurs once, therefore identify nodes labels say,
instance, node instead node v labeled a.
Example 3.7. Consider Example 2.9 page 123. corresponding (infinite)
guarded chase forest shown Figure 2. Every edge a-node b-node labeled
TGD whose application causes introduction b. Notice atoms (e.g.,
r2 (b) r2 (z2 )) label one node forest. nodes belonging also
restricted GCF shaded figure.
r1 (a, b)
2
r3 (b, z1 )

4

3

r2 (b)

r1 (b, z2 )
2

1
r2 (b)

3

4

r3 (z2 , z3 )

r2 (z2 )

r1 (z2 , z4 )
2

1


r2 (z2 )

4


3


Figure 2: Chase forest Example 3.7.
goal following material show that, weakly guarded sets TGDs,
possibly infinite set atoms chase(D, ) finite treewidth (Lemma 3.13).
used show decidability query-answering WGTGDs (Theorem 3.14).
first step towards proving chase(D, ) finite treewidth, generalize notion
129

fiCal, Gottlob & Kifer

acyclicity instance, point relationship notion
treewidth. show chase(D, ) enjoys (a specific version of) generalized
form acyclicity (Lemma 3.11), finite treewidth result immediately follows.
Definition 3.8. Let B (possibly infinite) instance schema R let dom(B).
[S]-join forest hF, B undirected labeled forest F = hV, Ei (finite
infinite), whose labeling function : V B that:
(1) epimorphism, i.e., (V ) = B;
(2) F [S]-connected, i.e., c dom(B) S, set {v V
dom((v))} induces connected subtree F .

|

c

say B [S]-acyclic B [S]-join forest.
Notice dealing relational instance, definition works
relational structure, including queries. Definition 3.8 generalizes classical notion
hypergraph acyclicity (Beeri, Fagin, Maier, Mendelzon, Ullman, & Yannakakis, 1981)
instance query: instance query, seen hypergraph, hypergraph-acyclic
(which -acyclic according Fagin, 1983) []-acyclic .
following Lemma follows definitions [S]-acyclicity.
Lemma 3.9. Given instance B schema R, set dom(B), B [S]acyclic, tw(B) 6 |S| + w, w maximum predicate arity R tw(B)
treewidth B.
Proof. hypothesis, B [S]-acyclic therefore [S]-join forest hF, i,
F = hV, Ei. tree decomposition hT, = hN, Ai, constructed follows. First,
take N = V {n0 }, n0 auxiliary node. Let Vr V set nodes
roots [S]-join forest F let Ar set edges n0 node Vr .
define = E Ar . labeling function defined follows: (n0 ) = S,
nodes v 6= n0 , (v) = dom((v)) S. show hT, tree decomposition.
Recalling definition tree decompositions Section 2.9, (i) holds trivially F
join forest (V ) = B. (ii), notice edges Gaifman graph B
atom = r(c1 , . . . , cm ) B clique among nodes c1 , . . . , cm .
Since atom exists v V (v) = (v) dom((v)), (ii)
holds immediately. Finally consider connectedness. Let us take value c appearing
B argument. c S, set {v N | c (v)} entire N , construction,
connectedness holds. c 6 S, set {v N | c (v)} induces connected subtree F
therefore , since (v) = (v) S. Therefore, (iii) holds. Notice also width
tree decomposition |S| + w construction.
Definition 3.10. Let database schema R, HB (D) Herbrand Base
defined Section 2. define:
chase (D, ) = chase(D, ) HB (D),
chase + (D, ) = chase(D, ) chase (D, )
130

fiTaming Infinite Chase

plain words, chase (D, ) finite set null-free atoms chase(D, ).
contrast, chase + (D, ) may infinite; set atoms chase(D, )
least one null argument. Note chase (D, ) chase + (D, ) = chase(D, )
chase (D, ) chase + (D, ) = .
Lemma 3.11. weakly guarded set TGDs database, chase + (D, )
[dom(D)]-acyclic, therefore chase(D, ).
order prove result, resort auxiliary lemma.
Lemma 3.12. Let database weakly guarded set TGDs. Let node
rgcf(D, ) null value N first introduced, let af descendant
node rgcf(D, ) argument. Then, appears every node (=atom)
(unique) path af .
Proof. Let a1 = , a2 , . . . , = af path af . definition affected
positions, appears affected positions atoms chase. Suppose,
contrary, appear intermediate atom path. Then,
i, 2 6 6 n 1, appear ai , appears ai+1 . Since
appears affected positions, order ai+1 must either appear ai
invented ai+1 added. first case ruled assumption,
second impossible first introduced a1 , ai+1 contradiction.
come back proof Lemma 3.11.
Proof. proof constructive, exhibiting [dom(D)]-join forest F = hV, Ei
chase(D, ). take F rgcf(D, ) define, atom rgcf(D, ),
labeling function F (d) = d. Since every atom chase(D, ) covered
corresponding node F , remains show chase(D, ) [dom(D)]-connected.
Take pair distinct atoms a1 , a2 rgcf(D, ) value c N
argument. atoms a1 a2 must common ancestor rgcf(D, ) c
first invented: not, value c would introduced twice chase(D, ).
Lemma 3.12, c appears atoms paths a1 a2 . thus
follows set {v V | c (v)} induces connected subtree F .
Lemma 3.13. weakly guarded set TGDs database schema R,
tw(chase(D, )) 6 |dom(D)| + w, w maximum predicate arity R.
Proof. claim follows Lemmas 3.9 3.11.
Theorem 3.14. Given relational schema R, weakly guarded set TGDs , Boolean
conjunctive query Q, database R, problem checking whether |= Q,
equivalently chase(D, ) |= Q, decidable.
Proof (sketch). first remind key result Courcelle (1990), generalizes earlier
result Rabin (1969). Courcelles result states satisfiability problem decidable
classes first-order theories (more generally, theories monadic second-order logic)
131

fiCal, Gottlob & Kifer

enjoy finite treewidth model property. class C theories finite-treewidth
model property satisfiable theory C possible compute integer f (T )
model treewidth f (T )see also works Goncalves
Gradel (2000) Gradel (1999), general property, called generalized
tree-model property, discussed. apply prove theorem.
Let Q universal sentence obtained negating existentially quantified conjunctive query Q. classes TGDs, |= Q iff chase(D, ) |= Q iff Q
unsatisfiable. Trivially, deciding whether |= Q equivalent Turing reductions deciding whether 6|= Q. latter holds iff {Q} satisfiable
or, equivalently, iff chase(D, ) model Q which, turn, holds iff chase(D, )
model {Q}. Lemma 3.13, WGTGDs, chase(D, ) finite treewidth.
decision problem thus amounts checking whether theory belonging class C
first-order theories (of form {Q}) satisfiable, guaranteed
whenever theory class satisfiable, model finite treewidth
(namely, chase(D, )), C therefore enjoys finite treewidth model property.
Decidability thus follows Courcelles result.
Determining precise complexity query answering sets guarded weakly
guarded sets TGDs require new techniques, subject next sections.

4. Complexity: Lower Bounds
section prove several lower bounds complexity decision problem
answering Boolean conjunctive queries guarded weakly guarded sets TGDs.
Theorem 4.1. problem BCQeval WGTGDs exptime-hard case TGDs
fixed. problem 2exptime-hard predicate arity bounded.
hardness results also hold fixed atomic ground queries.
Proof. start exptime-hardness result fixed WGTGD sets . wellknown apspace (alternating pspace, see Chandra, Kozen, & Stockmeyer, 1981a)
equals exptime. Notice apspace-hard languages accepted alternating polynomial-space machines use n worktape cells, n input
size, input initially placed worktape. (This well-known
shown trivial padding arguments). prove claim, thus suffices simulate
behavior restricted linear space (linspace) Alternating Turing Machine (ATM)
input bit string means weakly guarded set TGDs database
D. Actually, show stronger result: fixed set WGTGDs simulate
universal ATM turn simulates every linspace ATM uses n tape
cells every input. ATM transition table ATM input string
stored database D. |= Q atomic ground query Q iff ATM
accepts given input.2
Without loss generality, assume ATM exactly one accepting
state sa . also assume never tries read beyond tape boundaries. Let
2. technique proposed Cal et al. (2008). similar technique later described Hernich,
Libkin, Schweikardt (2011) proof undecidability existence so-called CWA (closedworld assumption) universal solutions data exchange.

132

fiTaming Infinite Chase

defined
= (S, , , , s0 , {sa })
set states, = {0, 1, } tape alphabet, blank tape symbol,
transition function, defined : (S {, r, })2 ( denotes stay
head move, r denote left right, respectively), s0 initial state,
{sa } singleton-set accepting states. Since alternating Turing machine
(ATM), set states partitioned two sets: (universal existential
states, respectively). general idea encoding configurations (except
initial configuration ) represented fresh nulls vi , > 1, generated
chase.
relational schema. describe predicates schema
use reduction. Notice schema fixed depend particular
ATM encode. schema predicates follows.
(1) Tape. ternary predicate symbol (a, c, v) denotes configuration v cell
c contains symbol a, . Also, binary predicate succ(c1 , c2 ) denotes
fact cell c1 follows cell c2 tape. Finally, neq(c1 , c2 ) says two cells
distinct.
(2) States. binary predicate state(s, v) says configuration v ATM
state s. use three additional unary predicates: existential , universal , accept.
atom existential (s) (resp., universal (s)) denotes state existential
(resp., universal), accept(c) says c accepting configuration, is,
one whose state accepting state.
(3) Configurations. unary predicate config(v) expresses fact value v
identifies configuration. ternary predicate next(v, v1 , v2 ) used say
configurations v1 v2 derived v. Similarly, use follows(v, v ) say
configuration v derived v. Finally, unary predicate init(v) states
configuration v initial.
(4) Head (cursor). use fact cursor (c, v) say head (cursor)
ATM cell c configuration v.
(5) Marking. Similarly done proof Theorem 3.5, use mark (c, v)
say cell c marked configuration v. TGDs ensure
non-marked cells keep symbols transition one configuration another.
(6) Transition function. represent transition function M, use single
8-ary predicate transition: every transition rule (s, a) = ((s1 , a1 , m1 ), (s2 , a2 , m2 ))
transition(s, a, s1 , a1 , m1 , s2 , a2 , m2 ).
database D. data constants database used identify cells,
configurations, states on. particular, use accepting state sa
initial state s0 plus special initial configuration . database describes initial
configuration ATM technicalities.
(a) assume, without loss generality, n symbols input occupy
cells numbered 1 n, i.e., c1 , . . . , cn . technical reasons, order obtain
simpler TGD set below, also use dummy cell constants c0 cn+1 ,
intuitively represent border cells without symbols. i-th symbol ai I,
database fact symbol (a, ci , ), {1, . . . , n}.
133

fiCal, Gottlob & Kifer

(b) atom state(s0 , ) specifies state s0 initial configuration .
(c) every existential state sE universal state sU , facts existential (sE )
universal (sU ). accepting state, database fact accept(sa ).
(d) atom cursor (c1 , ) indicates that, initial configuration, cursor points
first cell.
(e) atoms succ(c1 , c2 ), . . . , succ(cn1 , cn ) encode fact cells c1 , . . . , cn
tape (beyond ATM operate) adjacent. technical
reasons, also use analogous facts succ(c0 , c1 ) succ(cn , cn+1 ). Also, atoms
form neq(ci , cj ), i, j 1 6 6 n, 1 6 j 6 n 6= j, denote
fact cells c1 , . . . , cn pairwise distinct.
(f ) atom config() says valid configuration.
(g) database atoms form transition(s, a, s1 , a1 , m1 , s2 , a2 , m2 ),
encode transition function , described above.
TGDs. describe TGDs define transitions accepting
configurations ATM.
(a) Configuration generation. following TGDs say that, every configuration
(halting non haltingwe mind configurations derived
halting one), two configurations follow it, configuration
follows another configurations also valid configuration:
config(V ), V1 V2 next(V, V1 , V2 )
next(V, V1 , V2 ) config(V1 ), config(V2 )
next(V, V1 , V2 ) follows(V, V1 )
next(V, V1 , V2 ) follows(V, V2 )
(b) Configuration transition. following TGD encodes transition
ATM starts existential state, moves right first configuration left
second. C denotes current cell, C1 C2 new cells first
second configuration (on right left C, respectively),
constants r, , represent right, left, stay moves,
respectively.
transition(S, A, S1 , A1 , r, S2 , A2 , ), next(V, V1 , V2 ),
state(S, V ), cursor (C, V ), symbol (A, C, V ), succ(C1 , C), succ(C, C2 )
state(S1 , V1 ), state(S2 , V2 ), symbol (A1 , C1 , V1 ), symbol (A2 , C2 , V2 ),
cursor (C1 , V1 ), cursor (C2 , V2 ), mark (C, V ),
nine rules like one, corresponding possible moves
head child configurations C1 C2 . moves encoded via
similar TGDs. rules suitably mark cells written transition
means predicate mark . cells involved transition
must retain symbols, specified following TGD:
config(V ), follows(V, V1 ), mark (C, V ), symbol (C1 , A, V ), neq(C1 , C) symbol (C1 , A, V1 )
134

fiTaming Infinite Chase

(c) Termination. rule state(sa , V ) accept(V ) defines configuration V
accepting state accepting state. following TGDs state that,
existential state, least one configuration derived must accepting.
universal states, configurations must accepting.
next(V, V1 , V2 ), state(S, V ), existential (S), accept(V1 ) accept(V )
next(V, V1 , V2 ), state(S, V ), existential (S), accept(V2 ) accept(V )
next(V, V1 , V2 ), state(S, V ), universal (S), accept(V1 ), accept(V2 ) accept(V )
Note that, brevity, TGDs used multiple atoms head.
However, heads existentially quantified variables, multi-headed TGDs
replaced sets TGDs one head-atom. Note also
database constants (r, , , sa ) appearing rules eliminated
introducing additional predicate symbols database atoms. example, add
predicate acceptstate signature fact acceptstate(sa ) database D,
rule state(sa , V ) accept(V ) replaced equivalent constant-free rule
acceptstate(X), state(X, V ) accept(V ).
hard show encoding described sound complete. is,
accepts input chase(D, ) |= accept(). also easy verify
set TGDs used weakly guardedthis done checking
variable appearing affected positions also appears guard atom. instance,
take rule next(V, V1 , V2 ), state(S, V ), existential (S), accept(V1 ) accept(V ).
immediate see state[1] existential [1] non-affected (the TGDs never invent
new states), variables appearing affected positions only, namely V, V1 , V2 ,
appear guard atom next(V, V1 , V2 ). proves claim.
turn case fixed unbounded predicate arities.
obtaining 2exptime lower bound, sufficient adapt proof
simulate ATM 2n worktape cells, i.e., aexpspace machine whose space
restricted 2n tape cells. Actually, accommodate two dummy cells left right
2n effective tape cells, used technical reasons, feature 2n+1 tape
cells instead 2n .
make sure input string put cells 1, . . . , n worktape. Given
many n worktape cells, fill cells right
input string blank symbol .
time, WGTGD set fixed, depend n. Since much
stronger result shown Section 6 (Theorem 6.2), belabor details
follows, explain proof fixed sets TGDs needs
changed.
Rather representing tape cell data constant, tape cell represented vector (b0 , b1 , b2 , . . . , bn ) Boolean values {0, 1}. database
before, except following changes:
contains additional facts bool (0), bool (1), zero(0), one(1).
fact symbol (a, ci , ) replaced fact symbol (a, b0 , b1 , b2 , . . . , bn , ),
(b0 , b1 , b2 , . . . , bn ) Boolean vector length n representing integer i,
0 6 6 n 6 2n+1 .
135

fiCal, Gottlob & Kifer

fact cursor (c1 , ) replaced (n + 2)-ary fact cursor (0, 0, , 0, 1, ).
succ neq facts described item (e) eliminated. (Vectorized versions
predicates defined via Datalog rulessee below).
TGD set changed follows. rules, cell-variable C
replaced vector C n variables. example, atom succ(C1 , C) becomes
succ(C1 , C) = succ(C10 , C11 , . . . C1n , C 0 , C 1 , . . . , C n ).
add Datalog rules n-ary succ neq predicates. example, n-ary predicate succ implemented following rules:
bool (X0 ), . . . , bool (Xn1 ) succ(X0 , . . . , Xn1 , 0 , X0 , . . . , Xn1 , 1),
bool (X0 ), . . . , bool (Xn2 ) succ(X0 , . . . , Xn2 , 0, 1 , X0 , . . . , Xn2 , 1, 0),
..
.
bool (X0 ), . . . , bool (Xni ) succ(X0 , . . . , Xni , 0, 1 . . . 1 , X0 , . . . , Xni , 1, 0, . . . , 0),
..
.
succ(0, 1, . . . , 1 , 1, 0, . . . , 0)
rules contain constants easily eliminated use zero
one predicates, extensional database (EDB) predicates. add simple Datalog rules use vectorized succ predicate define vectorized versions
less neq predicates. Using less than, add single rule that,
initial configuration , puts blanks tape cells beyond last cell n input:
less than(n, C) symbol (, C, ), n n-ary binary vector representing number n (i.e., input size).
resulting set rules weakly guarded correctly simulates aexpspace (alternating exponential space) Turing machine whose transition table stored database
D. reduction polynomial time. Since aexpspace=2exptime, immediately follows arity bounded problem 2exptime-hard.

5. Complexity: Upper Bounds
section present upper bounds query answering weakly guarded TGDs.
5.1 Squid Decompositions
define notion squid decomposition, prove lemma called Squid
Lemma useful tool proving complexity results following
sub-sections.
Definition 5.1. Let Q Boolean conjunctive query n body atoms schema
R. R-cover Q Boolean conjunctive query Q+ R contains body
body atoms Q. addition, Q+ may contain n R-atoms.
Example 5.2. Let R = {r/2, s/3, t/3} Q Boolean conjunctive query
body atoms {r(X, ), r(Y, Z), t(Z, X, X)}. following query Q+ R-cover Q:
Q+ = {r(X, ), r(Y, Z), t(Z, X, X), t(Y, Z, Z), s(Z, U, U )}.
136

fiTaming Infinite Chase

Lemma 5.3. Let B instance schema R Q Boolean conjunctive query
B. B |= Q iff exists R-cover Q+ Q B |= Q+ .
Proof. only-if direction follows trivially fact Q R-cover itself.
direction follows straightforwardly fact whenever homomorphism
h : vars(Q+ ) dom(B), h(Q+ ) B, then, given Q subset Q+ ,
restriction h h vars(Q) homomorphism vars(Q) dom(B) h (Q) =
h(Q) B. Therefore B |= Q+ implies B |= Q.
Definition 5.4. Let Q Boolean conjunctive query schema R. squid decomposition = (Q+ , h, H, ) Q consists R-cover Q+ Q, mapping h : vars(Q+ )
vars(Q+ ), partition h(Q+ ) two sets H , = h(Q+ ) H,
exists set variables V h(vars(Q+ )) that: (i) H = {a h(Q+ ) |
vars(a) V }, (ii) [V ]-acyclic. appropriate set V given together
squid decomposition = (Q+ , h, H, ), then, slight terminology overloading, may
speak squid decomposition (Q+ , h, H, T, V ).
Note squid decomposition = (Q+ , h, H, ) Q necessarily define
query folding (Chandra & Merlin, 1977; Qian, 1996) Q+ , h need
endomorphism Q+ ; terms, require h(Q+ ) Q+ . However, h
trivially homomorphism Q+ h(Q+ ).
Intuitively, squid decomposition = (Q+ , h, H, T, V ) describes way query
Q may mapped homomorphically chase(D, ). First, instead mapping Q
chase(D, ), equivalently map h(Q+ ) = H chase(D, ). set V specifies variables h(Q+ ) ought mapped constants, i.e., elements
dom(D). atoms set H thus mapped ground atoms, is, elements finite set chase (D, ), may highly cyclic. [V ]-acyclic atom set shall
mapped possibly infinite set chase + (D, ) which, however, [dom(D)]-acyclic.
acyclicities chase + (D, ) exploited designing appropriate decision
procedures determining whether chase(D, ) |= Q. made formal
sequel.
One think set H squid decomposition = (Q+ , h, H, T, V ) head
squid, set join-forest tentacles attached head. become
clear following example associated Figure 3.
Example 5.5. Consider following Boolean conjunctive query:
Q = {r(X, ), r(X, Z), r(Y, Z),
r(Z, V1 ), r(V1 , V2 ), r(V2 , V3 ), r(V3 , V4 ), r(V4 , V5 ),
r(V1 , V6 ), r(V6 , V5 ), r(V5 , V7 ), r(Z, U1 ), s(U1 , U2 , U3 ),
r(U3 , U4 ), r(U3 , U5 ), r(U4 , U5 )}.
Let Q+ following Boolean query: Q+ = Q {s(U3 , U4 , U5 )}. possible squid
decomposition (Q+ , h, H, T, V ) based homomorphism h, defined follows:
h(V6 ) = V2 , h(V4 ) = h(V5 ) = h(V7 ) = V3 , h(X) = X variable X Q+ .
result squid decomposition V = {X, Y, Z} query shown Figure 3.
137

fiCal, Gottlob & Kifer

cyclic head H (encircled oval) represented join graph,3 [V ]acyclic tentacle set depicted [V ]-join forest. Moreover, forest representing
rooted bag H-atoms, entire decomposition takes shape
squid. Note eliminated additional atom s(U3 , U4 , U5 ), original set
atoms {r(U3 , U4 ), r(U3 , U5 ), r(U4 , U5 )} would form non-[V ]-acyclic cycle, therefore
would part tentacles.

r(X, )

r(X, Z)
head
r(Y, Z)

r(Z, V1 )

r(Z, U1 )

r(V1 , V2 )

s(U1 , U2 , U3 )

r(V2 , V3 )

s(U3 , U4 , U5 )

tentacles

r(V3 , V3 )

r(U3 , U4 )

r(U3 , U5 )

r(U4 , U5 )

Figure 3: Squid decomposition Example 5.5. Atoms h(Q+ ) shown.
following two lemmas auxiliary technical results.
Lemma 5.6. Let Q Boolean conjunctive query let U (possibly infinite) [A]acyclic instance, dom(U ). Assume U |= Q, i.e., homomorphism
f : dom(Q) dom(U ) f (Q) U . Then:
(1) [A]-acyclic subset W U that: (i) f (Q) W (ii) |W | < 2|Q|.
(2) cover Q+ Q |Q+ | < 2|Q|, homomorphism g
extends f g(Q+ ) = W .
Proof.
Part (1). assumption,4 U [A]-acyclic f : dom(Q) dom(U ) homomorphism f (Q) U . Since U [A]-acyclic, (possibly infinite) [A]-join
forest = hhV, Ei, i. assume, without loss generality, distinct vertices u, v
different labels, i.e., (u) 6= (v). assumption made removing
subforests rooted nodes labeled duplicate atoms. Let TQ finite subforest
3. join graph H atoms nodes. edge two atoms exists iff atoms share
least one variable.
4. One may tempted conjecture W = f (Q), work acyclicity (and thus
also [A]-acyclicity) hereditary property: may well case U acyclic,
subset f (Q) U not. However, taking W = f (Q) works case arities 2.

138

fiTaming Infinite Chase

contains ancestors nodes (s) f (Q). Let F = hhV , E i,
forest obtained follows.
V = {v V | (v) f (Q)} K, K set vertices TQ
least two children.
v, w V edge v w E iff w descendant v ,
unique shortest path v w contain node
V .
Finally, v V , (v) = (v).
Let us define W = (V ). claim forest F [A]-join forest W . Since
Condition (1) Definition 3.8 ([S]-join forest) immediately satisfied, suffices show
Condition (2), is, F satisfies [A]-connectedness condition. Assume,
pair distinct vertices v1 v2 F , value b dom(U ) holds
b dom( (v1 )) dom( (v2 )). order prove aforementioned [A]-connectedness
condition, need show least one path F v1 v2 (here
view F undirected graph), every node v V lying
path b dom( (v)). construction F , v1 v2 connected v
lies (unique) path v1 v2 . Since [A]-join forest,
b dom((v)) = dom( (v)). Thus F [A]-join forest W .
Moreover, construction F , number children inner vertex F
least 2, F |Q| leaves. follows F 2|Q| 1 vertices.
Therefore W [A]-acyclic set atoms |W | 6 2|Q| W f (Q).
Part (2). Q extended Q+ follows. atom r(t1 , . . . , tk ) W f (Q),
add Q new query atom r(1 , . . . , k ) 1 6 6 k, newly invented
variable. Obviously, W |= Q+ thus homomorphism g extending f
g(Q+ ) = W . Moreover, construction |Q+ | < 2|Q|.
Lemma 5.7. Let G [A]-acyclic instance let G instance obtained G
eliminating set atoms dom(S) A. G [A]-acyclic.
Proof. = hhV, Ei, [A]-join forest G [A]-join forest G
obtained G repeatedly eliminating vertex v (v) S.
construction, atom e eliminated G way property dom(e) A.
Hence, every value b dom(G) A, node u V (u) = e cannot belong
induced (connected) subtree {v V | b dom((v))}. thus get G enjoys
[A]-connectedness property.
following Lemma used main tool subsequent complexity analysis.
Lemma 5.8 (Squid Lemma). Let weakly guarded set TGDs schema R,
database R, Q Boolean conjunctive query. chase(D, ) |= Q iff
squid decomposition = (Q+ , h, H, ) homomorphism : dom(h(Q+ ))
dom(chase(D, )) that: (i) (H) chase (D, ), (ii) (T ) chase + (D, ).
Proof. If. squid decomposition = (Q+ , h, H, ) Q homomorphism
described, composition h homomorphism ( h)(Q+ ) =
139

fiCal, Gottlob & Kifer

(h(Q+ )) chase(D, ). Hence, chase(D, ) |= Q+ and, Lemma 5.3, chase(D, ) |=
Q.
if. Assume U = chase(D, ) |= Q. Then, exists homomorphism f :
vars(Q) dom(U ) f (Q) chase(D, ). Lemma 3.11, chase + (D, ) [dom(D)]acyclic. Lemma 5.6, follows Boolean query Q+ < 2|Q| atoms,
atoms Q contained Q+ , homomorphism g : dom(Q+ )
dom(U ) g(Q+ ) U , g(Q+ ) [dom(D)]-acyclic.
Partition vars(Q+ ) two sets vars (Q+ ) vars + (Q+ ) follows:
vars (Q+ ) = {X vars(Q+ ) | g(X) dom(D)}
vars + (Q+ ) = vars(Q+ ) vars (Q+ ).
Define mapping h : vars(Q+ ) vars(Q+ ) follows. X vars(Q+ ), let h(X)
lexicographically first variable set {Y vars(Q+ ) | g(Y ) = g(X)}. Let
us define V V = h(vars (Q+ )). Moreover, let H set atoms
h(Q+ ), vars(a) V = h(vars (Q+ )), let = h(Q+ ) H. Note that,
definition H, g(H) chase (D, ) and, definition , g(T ) chase + (D, ). Let
restriction g dom(h(Q+ )). Clearly, , h, H, fulfill conditions (i)
(ii) statement lemma. thus remains prove = (Q+ , h, H, )
actually squid decomposition Q. this, need show [V ]-acyclic.
prove this, first observe pair variables X, vars(Q+ )
g(X) = g(Y ) h(X) = h(Y ). Therefore is, construction, bijection
h(dom(Q+ )) dom((Q+ )). particular, h(Q+ ) isomorphic (T ) via
restriction dom(T ). Since (T ) = (T ) obtained [dom(D)]-acyclic
instance (Q+ ) eliminating atoms whose arguments dom(D) (namely
atoms (H)), Lemma 5.7, (T ) [dom(D)]-acyclic and, therefore, trivially
also [dom(D) dom((T ))]-acyclic. Now, since every X dom(T ) holds X V
iff (X) dom(D), immediately follows that, since (T ) [dom(D)]-acyclic,
[V ]-acyclic.
5.2 Clouds Complexity Query Answering WGTGDs
goal subsection prove following theorem:
Theorem 5.9. Let weakly guarded set TGDs, database schema R,
Q Boolean conjunctive query. problem determining whether |= Q or,
equivalently, whether chase(D, ) |= Q exptime case bounded arity,
2exptime general.
general case (of unbounded arities), first outline short high-level proof
aimed specialists Computational Logic. proof makes sophisticated use previous
results. give much longer, self-contained proof, works
general case case bounded arities. self-contained proof also introduces
concepts used following sections.
High Level Proof Sketch Theorem 5.9 (General Case). transform original problem
instance (D, , Q) guarded first-order theory = (D, , Q) chase(D, ) |=
140

fiTaming Infinite Chase

Q iff unsatisfiable. signature uses set constants plus constant
element dom(D). Moreover, includes predicate symbols occurring D, ,
Q, plus special nullary (i.e., propositional) predicate symbol q.
contains ground facts D, plus instances rule r obtained
r replacing variables r occur non-affected positions constants. Note
resulting rules universally quantified guarded sentences. Moreover,
squid decomposition = (Q+ , h, H, T, V ), possible replacement set
variables V constants signature , contains guarded sentence obtained
follows. Notice Q := (H) Boolean acyclic conjunctive query. results
Gottlob, Leone, Scarcello (2003),5 Q thus rewritten (in polynomial time)
equivalent guarded sentence . define ( q), obviously
guarded, too. Let denote sentences mentioned far. construction
Squid Lemma (Lemma 5.8), follows chase(D, ) |= Q iff |= q. let
= {q}. Obviously, unsatisfiable iff chase(D, ) |= Q.
Note reduction arity-preserving exptime-reduction. Let exponential upper bound runtime required reduction (and thus also size
(D, , Q)). deterministic version algorithm work Gradel (1999)
deciding whether guarded theory unbounded arity satisfiable unsatisfiable runs
w
double-exponential time O(2O(sw ) ), size theory w maximum
predicate arity. Therefore, overall runtime first computing = (D, , Q) input (D, , Q) size n maximum arity w, checking whether unsatisfiable
w
O(2O(t(n)w ) ), still double-exponential. Deciding |= Q thus
2exptime.

Note case bounded w, similar proof provide exptime bound,
w
2t(n)w would still doubly exponential due exponential term t(n), even w
constant. Actually, noted Barany, Gottlob, Otto (2010), evaluating non-atomic
conjunctive queries guarded first-order theories bounded predicate arity fact
2exptime-complete. Surprisingly, remains true even guarded theories
(variable) database fixed guarded theory simple form involving
disjunctions (Bourhis, Morak, & Pieris, 2013). therefore needed develop different
proof ideas.
rest section present independent self-contained proof Theorem 5.9 developing tools analyzing complexity query answering WGTGDs. end introduce notion cloud atom chase database
set WGTGDs. Intuitively, cloud set atoms chase(D, )
whose arguments belong dom(a) dom(D). words, atoms cloud
cannot nulls appear a. cloud important show
subtree gcf(D, ) rooted depends cloud.
Definition 5.10. Let weakly guarded set TGDs schema R
database R. every atom chase(D, ) cloud respect
following set: cloud (D, , a) = {b chase(D, ) | dom(b) dom(a) dom(D)}. Notice
5. See Theorem 3 paper, proof, remark proof, Corollary 3.

141

fiCal, Gottlob & Kifer

every atom chase(D, ) cloud (D, , a). Moreover, define
clouds(D, ) = {cloud (D, , a) | chase(D, )}
clouds + (D, ) = {(a, cloud (D, , a)) | chase(D, )}
subset cloud (D, , a) called subcloud (with respect D).
set subclouds atom denoted subclouds(D, , a). Finally, define
subclouds + (D, ) = {(a, C) | chase(D, ) C cloud (D, , a)}.
Definition 5.11. Let B instance (possibly nulls) schema, R,
database R. Let atoms Herbrand Base HB (B). say
D-isomorphic, denoted , simply case understood,
bijective homomorphism6 f : dom() dom() f () = (and thus
also f 1 () = ). definition extends directly cases sets
atoms atom-set pairs (in similar fashion clouds + (D, )).
Example 5.12. {a, b} dom(D) {1 , 2 , 3 , 4 } N , have: p(a, 1 , 2 )
p(a, 3 , 4 ) (p(a, 3 ), {q(a, 3 ), q(3 , 3 ), r(3 )}) (p(a, 1 ), {q(a, 1 ), q(1 , 1 ), r(1 )}).
hand, p(a, 1 , 2 ) 6 p(a, 1 , 1 ) p(a, 1 , 2 ) 6 p(3 , 1 , 2 ).
Lemma 5.13. Given database schema R instance B R, Disomorphism relation HB (B) (resp., 2HB (B) HB (B) 2HB (B) , Definition 5.11) equivalence relation.
lemma follows directly definitions; lets us define, every set
atoms HB (B), quotiont set A/D respect defined equivalence
relation . notion quotient set naturally extends sets sets atoms
clouds(D, ), sequences (pairs, particular) thereof.
Lemma 5.14. Let weakly guarded set TGDs let database schema
R. Let |R| denote number predicate symbols R, w maximum arity
symbols R. Then:
(1) every atom chase(D, ), |cloud (D, , a)| 6 |R| (|dom(D)| + w)w .
Thus, cloud (D, , a) polynomial size database arity w
bounded exponential otherwise.
w
(2) atom chase(D, ), |subclouds(D, , a)| 6 2|R|(|dom(D)|+w) .
w
(3) |clouds(D, )/ | 6 2|R|(|dom(D)|+w) , i.e., areup isomorphismat
exponentially many possible clouds subclouds chase, arity w bounded,
otherwise doubly exponentially many.
w
(4) |clouds + (D, )/ | 6 |subclouds + (D, )/ | 6 |R|(|dom(D)|+w)w 2|R|(|dom(D)|+w) .
Proof. claims proved combinatorial arguments follows.
(1) distinct atoms cloud obtained placing symbols a, plus possibly
symbols dom(D), w arguments predicate symbol R.
predicate R, number symbols thus placed |dom(D)| + w.
6. Recall that, definition, restriction homomorphism dom(D) identity mapping.

142

fiTaming Infinite Chase

(2) different ways choose subclouds(D, , a) clearly determines set
subsets cloud (D, , a).
(3) easy see size set non-pairwise-isomorphic clouds
chase bounded number possible subclouds fixed atom.
(4) Here, counting number possible subclouds, associated
generating atom. inequality holds because, choose non-pairwiseisomorphic clouds, possible generating atoms arguments
|dom(D)| + w symbols construct subclouds.
Definition 5.15. Given database set WGTGDs, let atom
chase(D, ). define following notions:
set atoms label nodes subtrees gcf(D, ) rooted a;
= cloud (D, , a);
subset atoms gcf(D, ), gcf[a, S]7 inductively defined follows:
(i) {a} gcf[a, S];
(ii) b gcf[a, S] b , b obtained via chase rule applied using TGD
body head-atom , homomorphism , () = b
() gcf[a, S].
Theorem 5.16. database schema R, weakly guarded set TGDs,
chase(D, ), = gcf[a, cloud (D, , a)].
Proof. definitions gcf[a, cloud (D, , a)], gcf[a, cloud (D, , a)]
a. remains show converse inclusion: gcf[a, cloud (D, , a)]. Define
levela (a) = 0 fact b cloud (D, , a) also define levela (b) = 0.
every atom c , levela (c) defined distance (i.e., length path)
c gcf(D, ).
first show following facts parallel induction levela (b):
(1) b cloud (D, , b) gcf[a, cloud (D, , a)].
(2) b b gcf[a, cloud (D, , a)].
Statement (2) converse inclusion after.
Induction basis. levela (b) = 0, either (a) b cloud (D, , a) {a},
(b) b = a. case (a), cloud (D, , a) gcf[a, cloud (D, , a)] therefore b
gcf[a, cloud (D, , a)], proves (1). Moreover, since b cloud (D, , a), b cannot contain labeled nulls a, dom(b) dom(D) dom(a) dom(D). Therefore
cloud (D, , b) cloud (D, , a) gcf[a, cloud (D, , a)], proves (2). case (b),
b = thus cloud (D, , a) = cloud (D, , b) gcf[a, cloud (D, , a)], proves (1).
Since b = gcf[a, cloud (D, , a)], (2) follows well.
Induction step. Assume (1) (2) satisfied c
levela (c) 6 assume levela (b) = + 1, > 0. atom b produced TGD
whose guard g matches atom b level i, is, induction hypothesis,
gcf[a, cloud (D, , a)]. body atoms TGD match atoms whose arguments
7. implicit here, avoid clutter.

143

fiCal, Gottlob & Kifer

must cloud (D, , b) thus also gcf[a, cloud (D, , a)], induction hypothesis. Therefore, (2) holds b. show (1), consider atom b cloud (D, , b).
case dom(b ) dom(b ), cloud (D, , b ) cloud (D, , b ) gcf[a, cloud (D, , a)].
Otherwise, b contains least one new labeled null introduced generation b. Given weakly guarded set labeled null N introduced
chase, must path b b gcf(D, ) (and therefore also
b). simple additional induction levelb (b ) shows applications TGDs
path must fired elements gcf[a, cloud (D, , a)] only. Therefore,
b gcf[a, cloud (D, , a)], proves (1).
corollary follows directly theorem.
Corollary 5.17. database schema R, weakly guarded set TGDs,
a, b chase(D, ), (a, cloud (D, , a)) (b, cloud (D, , b)), b.
Definition 5.18. Let database atom. canonical renaming :
dom(a) dom(D) dom(D), = {1 , . . . , h } N set labeled nulls
appearing a, 1-1 substitution maps element dom(D)
null-argument first unused element . cloud (D, , a)
(S) well-defined pair (can (a), (S)) denoted can(a, S).
Example 5.19. Let = g(d, 1 , 2 , 1 ) = {p(1 ), r(2 , 2 ), s(1 , 2 , b)}, {d, b}
dom(D) {1 , 2 } N . (a) = g(d, 1 , 2 , 1 ), (S) = {p(1 ), r(2 , 2 ),
s(1 , 2 , b)}.
Definition 5.20. database schema R, weakly guarded set TGDs
R, set atoms S, write (D, , a, S) |= Q iff exists
homomorphism (Q) .
following result straightforwardly follows Theorem 5.16 previous definitions.
Corollary 5.21. database schema R, weakly guarded set TGDs,
chase(D, ), Q Boolean conjunctive query, following statements
equivalent:
(1)
(2)
(3)
(4)

|= Q
(D, , a, cloud (D, , a)) |= Q
(D, , (a), cana (cloud (D, , a))) |= Q
subset cloud (D, , a) (D, , (a), (S )) |= Q.

use pair can(a, cloud (D, , a)) unique canonical representative
equivalence class {(b, cloud (D, , b)) | (b, cloud (D, , b)) (a, cloud (D, , a))}
clouds + (D, ). Therefore, set {can(a, cloud (D, , a)) | chase(D, )}
quotient set clouds + (D, )/ isomorphic. Note that, Lemma 5.14, sets
finite size exponential |D| + || schema fixed (and double exponential
otherwise).
Now, given database schema R, weakly guarded set TGDs R,
atomic Boolean conjunctive query Q, describe alternating algorithm Acheck(D, , Q)
144

fiTaming Infinite Chase

decides whether |= Q. assume Q form Y1 , . . . , , q(t1 , t2 , . . . , tr ),
t1 , . . . , tr , r > , terms (constants variables) dom(D){Y1 , Y2 , . . . , }.
algorithm Acheck returns true accepts configuration, according
criteria explained below; otherwise, returns false. Acheck uses tuples form
(a, S, , , b) basic data structures (configurations). Intuitively, configuration corresponds atom derived step chase computation together
set already derived atoms belonging cloud a. informal meaning
parameters configuration follows.
(1) root atom chase subtree consideration.
(2) cloud (D, , a); intuitively subcloud containing set atoms cloud (D, , a)
that, computing chase(D, ), originally derived outside subtree
guarded chase forest rooted (and thus outside subtree rooted
rgcf(D, )). expect atoms serve side atoms (i.e., atoms matching
non-guard atoms TGD) deriving desired atom b starting a.
(3) contains, every step computation, subset cloud (D, , a)
computed far, assumed valid, verified another
branch computation.
(4) total ordering atoms consistent order atoms
proved algorithm (by simulating chase procedure).
(5) b atom needs derived. cases (namely, main path
proof tree developed Acheck), algorithm try derive specific
atom, match query atom q(t1 , . . . , tr ) atoms path.
case, use symbol place b.
ready describe algorithm Acheck sufficiently detailed level. However,
omit many low-level details.
Acheck first checks |= Q. so, Acheck returns true halts. Otherwise,
algorithm attempts guess path, called main branch, contains atom q
instance Q. done follows.
Initialization. algorithm Acheck starts guesses atom D,
expand main branch eventually lead atom q matching
query Q. end, algorithm guesses set cloud (D, , a) total order
S, generates configuration c0 = (a, S, , , ). set initialized
= S.
Form configurationadditional specifications. configuration,
set set implicitly partitioned two sets + , + =
{a1 , a2 , . . . , ak } disjoint D. total order elements precede
+ . + , defined a1 a2 ak .
Summary tasks Acheck performs configuration. Assume
Acheck algorithm generates configuration c = (a, S, , , b), b might . Acheck
performs following tasks c:
Acheck verifies guessed set c actually subset cloud (D, , a).
achieved massive universal branching described
145

fiCal, Gottlob & Kifer

heading Universal Branching. Let us, however, anticipate works,
may contribute understanding steps. Acheck verify
atoms a1 , . . . , ak chase(D, ), where, {1, . . . , k},
proof ai chase(D, ) use premises atoms precede ai ,
according . algorithm thus finds suitable atoms d1 , . . . , dk builds
proof trees a1 , . . . , ak . 1 6 6 k, generates configurations
form (di , S, {a1 , a2 , . . . , ai1 }, , ai ). configuration used
starting point proof ai chase(D, ) assuming a1 , . . . , ai1 chase(D, )
already established. Acheck thus simulates sequential proof atoms
cloud (D, , a) via parallel universal branching c.
Acheck tests whether c final configuration (i.e., accepting rejecting one).
described heading Test final Configuration below.
c final configuration Acheck, means first component
yet one matched b (or query, b = ). Acheck
moves chase tree one step replacing child a. step
described heading Existential Branching.
following, let c = (a, S, , , b) configuration, b may .
Test final configuration. b D, Acheck accepts configuration,
expand further. b = , Acheck checks (via simple subroutine)
whether Q matches a, i.e., homomorphic image query atom q(t1 , . . . , tr ).
so, Acheck accepts c (and thus returns true) expand further. b 6= ,
Acheck checks whether = b. true, Acheck accepts configuration c
expand further. Otherwise, configuration tree expanded described next.
Existential Branching. Acheck guesses TGD body headatom , whose guard g matches via substitution (that is, (g) = a)
() . () corresponds newly generated atom (possibly containing
fresh labeled nulls N ). Note that, guess made, existential
branching automatically fails Acheck returns false. define configuration c1
Acheck creates c, first introduce intermediate auxiliary configuration
b), where:
c = (a, S, , ,
(a) = () new atom generated application substitution .
(b) contains atom dom(d) dom(a) dom(D). Thus,
addition new atom a, inherits atoms subcloud
parent configuration c compatible a. addition, includes set
newatoms(c) new atoms guessed Acheck algorithm. arguments
atom newatoms(c) must elements set dom(a) dom(D).
(c) = S.
total order obtained eliminating atoms
(d)
ordering atoms newatoms(c) atoms set oldproven(c) =
(these assumed already proven parent configuration c).
(e) b defined b = b.
Next, Acheck constructs configuration c1 c canonicalization: c1 = (c),
(b)), ()
total
c1 = (can (a), (S), (S ), (),


order atoms (S ) derived .
146

fiTaming Infinite Chase

Intuitively, c1 main child c way deriving query atom q(t1 , . . . , tr )
assuming atoms guessed subcloud derivable.
Universal Branching. generated configuration c, set equal
S. already said, means assumed configuration set
atoms derivable. verify indeed case, Acheck generates parallel,
using universal computation branching, set auxiliary configurations proving
guessed atoms (newatoms(c)) indeed derivable chase
respect .
concateLet (newatoms(c)) = {n1 , . . . , nm } let linear order

n
.
nation order , restricted oldproven(c), order n1 n2
(i)
1 6 6 m, Acheck generates configuration c2 defined
(i)
ni ).
c2 = (can (a), (S), (oldproven(c)) {n1 , . . . , ni1 }, (),

completes description Acheck algorithm.
Theorem 5.22. Acheck algorithm correct runs exponential time case
bounded arities, double exponential time otherwise.
Proof.
Soundness. easy see algorithm sound respect standard
chase, i.e., Acheck(D, , Q) returns true, chase(D, ) |= Q. fact, modulo
variable renaming, preserves soundness according Corollary 5.21, algorithm
nothing chasing respect , even chase steps necessarily
order standard chase. Thus, atom derived Acheck occurs
chase. Since every chase computes universal solution complete respect
conjunctive query answering, whenever Acheck returns true, Q entailed
chase, thus also standard chase, chase(D, ).
Completeness. completeness Acheck respect chase(D, ) shown
follows. Whenever chase(D, ) |= Q, finite proof Q, i.e., finite sequence
proof Q generated atoms ends atom q, instance Q.
proof simulated alternating computation Acheck follows: (i) steer main
branch Acheck towards (a variant of) q choosing successively TGDs
substitutions (modulo appropriate variable renamings) used standard
chase branch q; (ii) whenever subcloud chosen atom
Acheck, choose set atoms cloud (D, , a) (D atoms(proof Q )), modulo appropriate
variable renaming; (iii) ordering , always choose one given proof Q .
fact Q-instance lost replacing configurations canonical versions
guaranteed Corollary 5.21.
Computational cost. case bounded arity, size configuration c
polynomial . Thus, Acheck describes alternating pspace (i.e., apspace)
computation. well-known apspace = exptime. case arity bounded,
configuration requires exponential space. algorithm describes
computation Alternating expspace, equal 2exptime.

147

fiCal, Gottlob & Kifer

Corollary 5.23. Let weakly guarded set TGDs, let database
schema R. Then, computing chase (D, ) done exponential time case
bounded arity, double exponential time otherwise.
Proof. sufficient start empty set cycle ground atoms b
Herbrand base HB (D) checking whether chase(D, ) |= b. holds, add
b A. result chase (D, ). claimed time bounds follow straightforwardly.
finally state independent proof Theorem 5.9.
Proof Theorem 5.9. construct algorithm Qcheck Qcheck(D, , Q) outputs
true iff |= Q (i.e., iff chase(D, ) |= Q). algorithm relies notion squid
decompositions, Lemma 5.8; works follows.
(1) Qcheck starts computing chase (D, ).
(2) Qcheck nondeterministically guesses squid decomposition = (Q+ , h, H, ) Q
based set V vars(h(Q+ )), H = {a h(Q+ ) | vars(a) V }
[V ]-acyclic. Additionally, Qcheck guesses substitution 0 : V dom(D)
0 (H) chase (D, ). Note np guess, number atoms
Q+ twice number atoms Q.
(3) Qcheck checks whether 0 extended homomorphism (T )
chase + (D, ). Lemma 5.8, equivalent check chase(D, ) |= Q.
exists iff connected subgraph 0 (T ), homomorphism
(t) chase + (D, ). Qcheck algorithm thus identifies connected
components 0 (T ). component [dom(D)]-acyclic conjunctive query,
whose arguments may contain constants dom(D). component
thus represented [dom(D)]-join tree t. join tree t, Qcheck
tests whether exists homomorphism (t) chase + (D, ).
done subroutine Tcheck, takes TGD set , database D,
connected subgraph (i.e., subtree) 0 (T ) input. inner workings
Tcheck(D, , t) described below.
(4) Qcheck outputs true iff check (3) gives positive result.
correctness Qcheck follows Lemma 5.8. Given step (2) nondeterministic, complexity Qcheck npX , i.e., np oracle X, X
complexity class sufficiently powerful for: (i) computing chase (D, ), (ii)
performing tests Tcheck(D, , t).
describe Tcheck subroutine.
General notions. Tcheck(D, , t) obtained Acheck via following modifications. configuration Tcheck maintains pointer Tpoint vertex (an atom
aq ). Intuitively, provides link root subtree still needs
matched descendant configurations c. addition data structures carried
configuration Acheck, configuration Tcheck also maintains array subst
length w, w maximum predicate arity R. Informally, subst encodes
substitution maps current atom (the canonicalized version of) current
atom chase(D, ).
148

fiTaming Infinite Chase

Tcheck works like Acheck, instead nondeterministically constructing main configuration path configuration tree eventually atom matches query,
nondeterministically constructs main configuration (sub)tree configuration tree,
eventually atoms join tree get consistently translated vertices
. important component main configuration c Tcheck current atom a.
Initially, nondeterministically chosen atom D. subsequent configurations
alternating computation tree, take nodes gcf(D, ).
Initialization. Similarly Acheck, computation starts generating initial
configuration (a, S, S, , , Tpoint, subst), nondeterministically chosen
database D, Tpoint points root r t, subst homomorphic substitution
subst(r) = a, r homomorphically mappable a; otherwise subst empty.
configuration root main configuration tree.
general, pointer Tpoint main configuration c = (a, S, , , , Tpoint, subst)
points atom aq t, yet matched. algorithm attempts
expand configuration successively guessing subtree configurations, mimicking
suitable subtree gcf(D, ) satisfies subquery rooted aq .
Whenever Tcheck generates configuration, Acheck, Tcheck generates
via universal branching number configurations whose joint task verify
elements indeed provable. (We provide details branching
done.)
Expansion. expansion main configuration c = (a, S, , , , Tpoint, subst)
works follows. configuration c, Tcheck first checks whether exists homomorphism (subst(aq )) = a.
1. ( exists.) exists, two cases:
1.1. aq leaf t, current configuration turns accepting one.
1.2. aq leaf t, Tcheck nondeterministically guesses whether
good match, i.e., one contributes global query answer
expanded map entire tree gcf(D, ).
1.2.1. (Good match). case good match, Tcheck branches universally
following child aqs aq t. nondeterministically (i.e.,
via existential branching) creates new configuration
cs = (as , Ss , Ss , , , Tpoints , substs )
Tpoints points aqs , substs encodes composition
substs . atom guessed, analogously done Acheck,
guessing TGD body head atom ,
guard atom g matches via homomorphism (that is, (g) = a)
() . cloud subsets Ss Ss chosen
Acheck. Intuitively, Tcheck, found good match aq a, tries
match children aq children (and, eventually, descendants)
gcf(D, ). Finally, function indicates appropriate
canonizations made obtain cs c (we omit tedious details).
149

fiCal, Gottlob & Kifer

1.2.2. (No good match). case good match exists, child configuration
cnew = cananew (anew , S, , , , Tpoint, subst)
c nondeterministically created, whose first component represents child
anew a, cnew inherits remaining components c.
Intuitively, failed matching aq (to which, remind, Tpoint
points) a, Tcheck attempts matching aq child
gcf(D, ). analogy previous case, anew obtained guessing
TGD body head atom , guard
atom g matches via homomorphism (that is, (g) = a), () ,
anew := (). Again, function term cananew indicates
appropriate canonizations applied (which describe detail).
2. ( exist.) case, Tcheck proceeds exactly case 1.2.2, namely,
attempts matching aq child (or eventually descendant)
gcf(D, ).
Correctness. correctness Tcheck shown along similar lines
Acheck. important additional point consider Tcheck that, given query
acyclic, actually sufficient remember configuration c latest
atom substitution subst. correctness Qcheck follows correctness
Tcheck Lemma 5.8.
Computational cost. complexity Qcheck, note case arity
bounded, Tcheck runs apspace = exptime, computing chase (D, ) exptime Corollary 5.23. Thus, Qcheck runs time npexptime = exptime. case
unbounded arities, computing chase (D, ) running Tcheck 2exptime,
therefore Qcheck runs time np2exptime = 2exptime.

combining Theorems 4.1 5.9 immediately get following characterization
complexity reasoning weakly guarded sets TGDs.
Theorem 5.24. Let weakly guarded set TGDs schema R, database
R, Q Boolean conjunctive query. Determining whether |= Q or, equivalently,
whether chase(D, ) |= Q exptime-complete case bounded predicate arities, even
fixed Q atomic. general case unbounded predicate arities,
problem 2exptime-complete. completeness results hold problem
query containment weakly guarded sets TGDs.
Generalization. definition WGTGDs generalized classes TGDs whose
unguarded positions guaranteed contain controlled finite number null-values
only. Let f computable integer function two variables. Call predicate position
TGD set f -bounded f (|D|, ||) null values appear chase(D, )
arguments position ; otherwise call f -unbounded. set TGDs f -weakly guarded
rule contains atom body covers variables occur
within rule f -unbounded positions only. minor adaptation proof
Theorem 3.14, seen CQ-answering class f -weakly guarded TGDs
decidable. Moreover, simple modification Qcheck Tcheck procedures,
150

fiTaming Infinite Chase

allowing polynomial number nulls enter unguarded positions, shown
CQ-answering fixed sets WGTGDs exptime-complete worst case,
class WGTGD sets defined follows. set TGDs belongs class
f -weakly guarded function f exists function g,
f (|D|, ||)| 6 |D|g(||) .

6. Guarded TGDs
turn attention GTGDs. first consider case variable database
input. Later, prove part complexity bounds stronger condition
fixed database.
6.1 ComplexityVariable Database
Theorem 6.1. Let set GTGDs schema R database R. Let,
before, w denote maximum predicate arity R |R| total number predicate
symbols R. Then:
(1) Computing chase (D, ) done polynomial time w |R|
bounded and, thus, also case fixed set . problem exptime
(and thus exptime-complete) w bounded, 2exptime otherwise.
(2) Q atomic fixed Boolean query checking whether chase(D, ) |= Q
ptime-complete w |R| bounded. problem remains ptimecomplete even case fixed. problem exptime-complete w bounded
2exptime-complete general. remains 2exptime-complete even |R|
bounded.
(3) Q general conjunctive query, checking chase(D, ) |= Q np-complete
case w |R| bounded and, thus, also case fixed set . Checking
chase(D, ) |= Q exptime-complete w bounded 2exptime-complete
general. remains 2exptime-complete even |R| bounded.
(4) BCQ answering GTGDs np-complete w |R| bounded, even
case set GTGDs fixed.
(5) BCQ answering GTGDs exptime-complete w bounded 2exptimecomplete general. remains 2exptime-complete even |R| bounded.
Proof. First, note items (4) (5) immediately follow first three items,
given chase(D, ) universal model. therefore need prove items (1)-(3).
first explain hardness results obtained, deal matching
membership results.
Hardness Results. ptime-hardness checking chase(D, ) |= Q atomic (and
thus also fixed) queries Q fixed follows fact ground atom inference
fixed fully guarded Datalog program variable databases ptime-hard. fact,
proof Theorem 4.4 work Dantsin, Eiter, Gottlob, Voronkov (2001)
shown fact inference single-rule Datalog program whose body guard
atom contains variables ptime-hard. np-hardness item (3) immediately
derived hardness CQ containment (which turn polynomially equivalent
151

fiCal, Gottlob & Kifer

query answering) without constraints (Chandra & Merlin, 1977). hardness results
exptime 2exptime derived via minor variations proof Theorem 4.1.
example, |R| unbounded w bounded, tape cells polynomial
worktape simulated using polynomially many predicate symbols. example,
fact configuration v cell 5 contains symbol 1 encoded S51 (v). omit
details, given much stronger hardness result established via full
proof Theorem 6.2.
Membership results. membership results proved exactly weakly
guarded sets TGDs, except instead using concept cloud, use
similar concept restricted cloud, coincides type atom
work Cal et al. (2012a). restricted cloud rcloud (D, , a) atom chase(D, )
set atoms b chase(D, ) dom(b) dom(a). proof
almost identical one Theorem 5.16, show database,
set GTGDs, chase(D, ), r = gcf[a, rcloud (D, , a)], r
defined r = {a } rcloud (D, , a). follows that, main computational
tasks, use algorithms rAcheck, rQcheck, rTcheck, differ already
familiar Acheck, Qcheck, Tcheck restricted clouds instead ordinary
clouds used. However, unlike case |R| w bounded cloud
(or subcloud) polynomial size |D |, restricted cloud rcloud (D, , a)
constant number atoms, storing canonical version (rcloud (D, , a)) thus
requires logarithmic space only. total, case |R| w bounded, due
use restricted clouds (and subsets thereof) configuration c rAcheck rTcheck
requires logarithmic space. Since alogspace = ptime, ptime-results atomic
queries items (1) (2) follow. Moreover, |R| w bounded, general
(non-atomic non-fixed) queries, rQcheck algorithm decides chase(D, ) |= Q
np guessing squid decomposition (in nondeterministic polynomial time) checking
(in alogspace=ptime) homomorphism squid decomposition
chase(D, ). Thus, case, rQcheck runs npptime = np, proves np upper
bound Item (3). If, addition, Q fixed, Q constant number squid
decompositions, therefore rQcheck runs ptimeptime = ptime, proves ptime
upper bound fixed queries mentioned item (2). exptime 2exptime upper
bounds inherited upper bounds WGTGDs.
Note one main results Johnson Klug (1984), namely, query
containment inclusion dependencies bounded arities np-complete, special
case Item (3) Theorem 6.1.
6.2 ComplexityFixed Database
next result tightens parts Theorem 6.1 showing exptime
2exptime-completeness results hold even case fixed input database.
Theorem 6.2. Let set GTGDs schema R. before, let w denote
maximum arity predicate R |R| total number predicate symbols. Then,
fixed databases D, checking whether chase(D, ) |= Q exptime-complete w
152

fiTaming Infinite Chase

bounded 2exptime-complete unbounded w. unbounded w, problem remains
2exptime-complete even |R| bounded.
Proof. First, observe upper bounds (i.e., membership results exptime
2exptime) inherited Theorem 6.1, suffices prove hardness results
cases Q fixed atomic query.
start proving checking chase(D, ) |= Q exptime-hard w bounded.
well-known apspace (alternating pspace) equals exptime.
already noted proof Theorem 4.1, sufficient simulate linspace
alternating Turing machine (ATM) uses n worktape cells every input
(bit string) size n, input string initially present worktape.
particular, show accepts input iff chase(D, ) |= Q.
Without loss generality, assume (i) ATM exactly one accepting state,
a, also halting state; (ii) initial state existential state; (iii)
alternates transition existential universal states; (iv) never
tries read beyond tape boundaries.
Let defined = (S, , , q0 , {sa }), set states, = {0, 1, }
tape alphabet, blank tape symbol, : (S {, r, })2
transition function ( denotes stay head move, r denote left
right respectively), q0 initial state, {sa } singleton set final
(accepting) states. Since alternating TM, set states partitioned
two sets, universal existential states, respectively. general idea
encoding different configurations input length n represented
fresh nulls generated construction chase.
Let us describe schema R. First, integer 1 6 6 n, R contains
predicate head /1, head (c) true iff configuration c head
tape cell i. R also predicates zero /1, one /1, blank /1, zero (c),
one (c), blank (c) true configuration c tape cell contains symbol 0, 1,
, respectively. Furthermore, state S, R predicate state /1,
state (c) true iff state configuration c s. R also contains: predicate start/1,
start(c) true iff c starting configuration; predicate config/1, true
iff argument identifies configuration; predicate next/3, next(c, c1 , c2 )
true c1 c2 two successor configurations c. also predicates
universal /1 existential /1, universal (c) existential (c) true c
universal (respectively, existential) configuration. Finally, predicate accepting/1,
accepting(c) true accepting configurations c, propositional symbol
accept, true iff Turing Machine accepts input I.
describe set (M, I) GTGDs simulates behavior input
I. rules (M, I) follows.
1. Initial configuration generation rules. following rule creates initial state:
X init(X). also add rule init(X) config(X), says initial
configuration is, fact, configuration.
2. Initial configuration rules. following set rules encodes tape content
initial configuration, is, input string I. 1 6 6 n, i-th cell
tape contains 0, add rule init(X) zero (X); contains 1,
153

fiCal, Gottlob & Kifer

add init(X) one (X). also add rule init(X) existential (X) order
say, without loss generality, initial configuration existential one.
Moreover, add rules init(X) head 1 (X) init(X) state s0 (X) define
initial values state head position input I.
3. Configuration generation rules. add rule creates two successor configuration
identifiers configuration identifier. Moreover, add rules stating
new configuration identifiers indeed identify configurations:
config(X) X1 ,X2 next(X, X1 , X2 ),
next(X, Y, Z) config(Y ),
next(X, Y, Z) config(Z).
4. Transition rules. show example transition rules generated
transition finite control. Assume, instance, transition table contains
specific transition form: (s, 0) ( (s1, 1, r) , (s2, 0, ) ). assert
following rules, 1 6 6 n:
head (X), zero (X), state (X), next(X, X1 , X2 ) state s1 (X1 )
head (X), zero (X), state (X), next(X, X1 , X2 ) state s2 (X2 ).
Moreover, 1 6 < n rules:
head (X), zero (X), state (X), next(X, X1 , X2 ) one (X1 )
head (X), zero (X), state (X), next(X, X1 , X2 ) head i+1 (X1 ),
1 < 6 n add rules:
head (X), zero (X), state (X), next(X, X1 , X2 ) zero (X2 )
head (X), zero (X), state (X), next(X, X1 , X2 ) head i1 (X2 )
types transition rules constructed analogously. Note total
number rules added 6n times number transition rules. Hence linearly
bounded size n input string M.
5. Inertia rules. rules state tape cells positions head keep
values. Thus, 1 6 i, j 6 n 6= j add rules:
head (X), zero j (X), next(X, X1 , X2 ) zero j (X1 )
head (X), one j (X), next(X, X1 , X2 ) one j (X1 )
head (X), blank j (X), next(X, X1 , X2 ) blank j (X1 ),
6. Configuration-type rules. rules say immediate successor configurations
existential configuration universal, vice-versa:
existential (X), next(X, X1 , X2 ) universal (X1 )
existential (X), next(X, X1 , X2 ) universal(X2 )
universal (X), next(X, X1 , X2 ) existential (X1 )
universal (X), next(X, X1 , X2 ) existential (X2 ).
154

fiTaming Infinite Chase

7. Acceptance rules. recursive rules state configuration accepting:
state sa (X) accepting(X)
existential (X), next(X, X1 , X2 ), accepting(X1 ) accepting(X)
existential (X), next(X, X1 , X2 ), accepting(X2 ) accepting(X)
universal (X), next(X, X1 , X2 ), accepting(X1 ), accepting(X2 ) accepting(X)
init(X), accepting(X) accept.
completes description set TGDs (M, I). Note set guarded,
maximum predicate arity 3, obtained logarithmic space
constant machine description M. faithfully simulates behavior alternating
linear space machine input I. follows (M, I) |= accept iff accepts input I.
Let D0 denote empty database, let Q0 ground-atom query accept.
(M, I) D0 |= Q0 iff accepts input I. shows answering ground
atom queries fixed databases constrained bounded arity GTGDs exptime-hard.
Let us illustrate obtain 2exptime hardness result guarded TGDs
arities unbounded, number |R| predicate symbols schema R
bounded constant. Given aexpspace=2exptime (aexpspace alternating
aexpspace), aim simulate aexpspace Turing machine. sufficient
simulate one uses 2n worktape cells, since acceptance problem
machines already 2exptime-hard. fact, trivial padding arguments,
acceptance problem every aexpspace machine transformed polynomial time
acceptance problem one using 2n worktape cells.
problem is, however, longer construct polynomial number
rules explicitly address worktape cell i, pair cells i, j, since
exponential number worktape cells. idea encode tape cell indexes
vectors symbols (v1 , . . . , vk ) vi {0, 1}. proof Theorem 4.1,
could define, polynomial number rules, successor relation succ stores pairs
indexes succ(v1 , . . . , vk , w1 , . . . , wk ). However, difficulty:
two different types variables: variables Vi , Wj range bits vi , wi
above-described bit vectors, variables X, Y, Z range configurations.
major difficulty that, given rules guarded, must make sure
two types variables, whenever occur elsewhere rule body, also occur
guard. end, use fixed database D01 contains single fact
zeroone(0, 1), construct guard relation g vector v
n bits binary successor w, configuration x two successor
configurations z, relation g contains tuple g(v, w, x, y, z). use several
auxiliary relations construct g.
technical reasons, first two arguments atoms dummy
variables T0 T1 always forced take values 0 1, respectively.
way, convenient, values 0 1 available implicitly form
variables, need use constants explicitly rules.
Given database non-empty, need create initial configuration identifier via existential rule before. simply take 0 identifier
155

fiCal, Gottlob & Kifer

initial configuration: zeroone(T0 , T1 ) init(T0 , T1 , T0 ). (Here, first two arguments init(T0 , T1 , T0 ) serve, explained, carry values 0 1 along.)
also add: init(T0 , T1 , T0 ) config(T0 , T1 , T0 ) assert 0 identifier initial
configuration. Next present new configuration generation rules.
config(T0 , T1 , X) Y, Z next(T0 , T1 , X, Y, Z),
next(T0 , T1 , X, Y, Z) config(T0 , T1 , ),
next(T0 , T1 , X, Y, Z) config(T0 , T1 , Z).
use rules create relation b atom b(0, 1, v, x, y, z) contains
tuple vector v n bits, configuration x. better readability,
whenever useful, use superscripts indicating arity vector variables:
instance, V(n) denotes V1 , . . . , Vn . Moreover, 0(j) denotes vector j zeros 1(j)
vector j ones. start rule next(T0 , T1 , X, Y, Z) b(T0 , T1 , T0 (n) , X, Y, Z),
defines atom b(0, 1, 0(n) , x, y, z), configuration x next-successors
z.
following n rules, 1 6 6 n, generate exponential number new atoms,
triple X, Y, Z, swapping 0s 1s possible ways. Eventually, chase
generate possible prefixes n bits.
b(T0 , T1 , U1 , . . . , Ui1 , T0 , Ui+1 , . . . , Un , X, Y, Z)
b(T0 , T1 , U1 , . . . , Ui1 , T1 , Ui+1 , . . . , Un , X, Y, Z).
ready define guard-relation g another group guarded rules.
0 6 r < n, add:
b(T0 , T1 , U(r) , T0 , T1 (nr1) , X, Y, Z) g(U(r) , T0 , T1 (nr1) , U(r) , T1 , T0 (nr1) , X, Y, Z).
n rules define exponential number cell-successor pairs triple
configuration identifiers X, Y, Z, Z next configurations following
X. particular, relation g contains precisely tuples g(v, w, x, y, z), v
n-ary bit vector, w binary successor, x configuration identifier, first
successor via next relation, z second successor via next relation.
ready simulate aexpspace Turing machine input string
set GTGDs (M , I). Since simulation similar one presented
first part proof, sketch point main differences.
simulation, use (in addition aforementioned auxiliary predicates)
predicates similar ones used earlier simulation exptime Turing machine
M. However, use constant number predicates. So, rather using, atoms
head (x), zero (x) on, use vectorized versions head (v, x), zero(v, x)
on, v bit vector length n takes role exponential index. Thus,
example, equivalent earlier rule
head (X), zero (X), state (X), next(X, X1 , X2 ) one (X1 )
g(V, W, X, X1 , X2 ), head (V, X), zero(V, X), state(X, s) one(V, X1 ). earlier rule
head (X), zero (X), state (X), next(X, X1 , X2 ) head i1 (X2 )
156

fiTaming Infinite Chase

becomes g(V, W, X, X1 , X2 ), head (W, X), zero(W, X), state(X, s) head (V, X2 ).
straightforward see initialization rules written. Informally,
copying input string worktape, place n input bits tape
writing rule bit. add rules fill positions n + 1 2n
blanks. done similar way second part proof
Theorem 4.1, omit details.
remaining issue specification inertia rules. rules deal
pairs i, j different, necessary adjacent, tape cell positions earlier simulation.
adjacent cell positions available far. problem solved
different ways. One possibility described below.
simply modify definition predicate b adding second vector
n bits b-atoms b-atoms actually form b(T0 , T1 , v, u, x, y, z),
v u range possible distinct pairs bit vectors length n. u vector
carried g-atoms. thus assume g-atoms form
g(v, w, u, x, y, z). former inertia rule head (X), zero j (X), next(X, X1 , X2 ) zero j (X1 )
would become g(V, W, U, X, X1 , X2 ), head (W, X), zero(U, X) zero(U, X1 ).
remains defined configuration acceptance rules. configuration rules similar ones used previous reduction, hence leave
exercise. acceptance rules follows:
state(X, sa ) accepting(X)
existential (X), g(V, W, X, X1 , X2 ), accepting(X1 ) accepting(X)
existential (X), g(V, W, X, X1 , X2 ), accepting(X2 ) accepting(X)
universal (X), g(V, W, X, X1 , X2 ), accepting(X1 ), accepting(X2 ) accepting(X)
zeroone(T0 , T1 ), accepting(T0 ) accept.
completes description set TGDs (M , I). Note set
guarded constant number predicates. obtained logspace
constant machine description M. also faithfully simulates behavior
alternating exponential space machine input I. follows (M , I) |= accept
iff accepts input I. Let Q0 BCQ defined Q0 = {accept}.
D01 (M , I) |= Q0 iff accepts input I. shows answering ground atomic
queries fixed databases guarded TGDs fixed number predicate symbols
(but unbounded arity) 2exptime-hard.

7. Polynomial Clouds Criterion
previous section seen that, case bounded arity, query answering
weakly guarded sets TGDs exptime-complete, query answering GTGDs
np-complete. Note that, unrestricted queries databases, np-completeness
best obtain. fact, even absence constraints, BCQ answering problem
np-complete (Chandra & Merlin, 1977).
section, establish criterion used tool recognizing relevant
cases query answering np even weakly guarded sets TGDs
fully guarded. Note consider setting weakly guarded set
157

fiCal, Gottlob & Kifer

TGDs fixed setting classes TGD sets considered. classes,
require uniform polynomial bounds.
Definition 7.1. [Polynomial Clouds Criterion] fixed weakly guarded set TGDs
satisfies Polynomial Clouds Criterion (PCC ) following conditions hold:
1. exists polynomial () database D, |clouds(D, )/ | 6
(|D|). words, isomorphism, polynomially many
clouds.
2. polynomial () that, database atom a:
cloud (D, , a) computed time (|D|, ||),
6 cloud (D, , a) computed time (|D|, ||) starting
D, a, cloud (D, , b), b predecessor gcf(D, ).
also say satisfies PCC respect . Note
above, || constant omitted. However, use || justified
following. class C weakly guarded TGD sets satisfies PCC fixed
polynomials TGD set C satisfies PCC uniformly
respect (i.e., TGD set class , bound).
Theorem 7.2. Let fixed weakly guarded set TGDs schema R,
enjoys Polynomial Clouds Criterion. Then:
Deciding database atomic fixed Boolean conjunctive query Q whether
|= Q (equivalently, whether chase(D, ) |= Q) ptime.
Deciding database general Boolean conjunctive query Q whether |=
Q (equivalently, chase(D, ) |= Q) np.
Proof. polynomial algorithm Acheck2 atomic queries Q works follows. start
produce chase forest gcf(D, ) using standard chase. addition, immediately
generating node cloud cloud (D, , a) (in polynomial time), store
(a, cloud (D, , a)) buffer, call cloud-store. Whenever branch
forest reaches vertex b b (cloud (D, , b)) already cloud-store,
expansion branch b blocked. Since polynomial number
pairs (a, cloud (D, , a)), algorithm stops polynomial number chase steps,
step requiring polynomial time. Now, Corollary 5.17, cloud-store already
contains possible atoms chase(D, ) clouds, isomorphism. check
whether chase(D, ) |= Q holds atomic query Q, thus sufficient test whether
every atom c occurs cloud-store matches Q. summary, Acheck2 runs ptime.
algorithm Qcheck2 conjunctive queries works like Qcheck, except
calls algorithm Tcheck2 subroutine instead Tcheck. input Tcheck2 D,
Q, also cloud-store computed Acheck2. assume cloud-store
identifies entry e = (a, cloud (D, , a)) unique integer e# using O(log n) bits
only. Tcheck2 alternating algorithm works essentially like Tcheck, except
following modifications:
Tcheck always guesses full cloud = cloud (D, , a), instead possibly guessing
subcloud. contrast, Tcheck2 guesses entry number e# corresponding
entry (a, cloud (D, , a)) cloud-store.
158

fiTaming Infinite Chase

Tcheck2 verifies correctness cloud guess alogspace using D, a, e# , well
b e# , b main atom predecessor configuration e
entry cloud-store featuring b (b, cloud (D, , b)). Note verification
effectively possible due condition (2) Definition 7.1.
Tcheck2 needs compute main configuration treethe one whose configurations contain . algorithm compute auxiliary branches, since
longer necessary, correctness check done different way.
configurations Tcheck2 need guess memorize linear orders
set + .
Given Tcheck2 alogspace algorithm, Qcheck2 npalogspace procedure. Since
npalogspace = npptime = np, query answering np. case fixed conjunctive query
Q, since Q constant number squid decompositions, Qcheck2 runs ptimeptime =
ptime.
Note Polynomial Clouds Criterion syntactic. Nevertheless, useful
proving query answering weakly guarded sets TGDs np, even
polynomial time atomic queries. application criterion illustrated
Section 10.
following direct corollary Theorem 6.1.
Theorem 7.3. (1) Every set GTGDs satisfies PCC. (2) constant k,
class GTGD sets arity bounded k satisfies PCC.
following result obtained minor adaptation proof Theorem 7.2.
Theorem 7.4. Let fixed weakly guarded set TGDs enjoys Polynomial
Clouds Criterion, let k constant. Then:
(1) database Boolean conjunctive query treewidth 6 k, deciding
whether |= Q (equivalently, chase(D, ) |= Q) ptime.
(2) tractability result holds acyclic Boolean conjunctive queries.
analogy PCC, one may define various criteria based bounds.
particular, define Exponential Clouds Criterion (ECC) classes TGD
sets, use next section, follows:
Definition 7.5. [Exponential Clouds Criterion] Let C class weakly guarded TGD
sets. C satisfies Exponential Clouds Criterion (ECC) following conditions
satisfied:
1. polynomial () every database set TGDs
C size n, |clouds(D, )/ | 6 2(|D|+n) .
2. exists polynomial () every database D, set TGDs
C size n, atom a:


D, cloud (D, , a) computed time 2 (|D|+n) ,

6 D, cloud (D, , a) computed time 2 (|D|+n) D, a,
cloud (D, , b), b predecessor gcf(D, ).
159

fiCal, Gottlob & Kifer

following result sets TGDs enjoying ECC:
Theorem 7.6. weakly guarded set TGDs class C enjoys Exponential Clouds Criterion, deciding database Boolean conjunctive query
Q (atomic not) whether |= Q exptime.
Proof (sketch). proof similar Theorem 7.2. main difference
ptime alogspace replaced exptime apspace, respectively.
get query answering atomic queries apspace = exptime, answering
non-atomic queries npapspace = npexptime = exptime. Thus, case,
difference atomic non-atomic query answering: exptime.

8. TGDs Multiple-Atom Heads
mentioned Section 2, complexity results proved far single-headed TGDs also
carry general case, multiple atoms may appear rule heads. make
claim formal here.
Theorem 8.1. complexity results derived paper sets TGDs whose heads
single-atoms equally valid sets multi-atom head TGDs.
Proof (sketch). suffices show upper bounds carry setting TGDs
multiple-atom heads. exhibit transformation arbitrary set TGDs
schema R set single-headed TGDs schema R extends R
auxiliary predicate symbols.
TGD set obtained replacing rule form r : body(X)
head 1 (Y), head 2 (Y), . . . , head k (Y), k > 1 set variables
appear head, following set rules:
body(X) V (Y)
V (Y) head 1 (Y)
V (Y) head 2 (Y)
..
.
V (Y) head k (Y),
V fresh predicate symbol, arity number variables
Y. Note that, general, neither contained X way around.
easy see that, except atoms form V (Y), chase(D, ) chase( , D)
coincide. atoms form V (Y) completely new predicates thus
match predicate symbol conjunctive query Q. Therefore, chase(D, ) |= Q iff
chase( , D) |= Q.
Obviously, constructed logspace . Therefore, extension
complexity results general case immediate, except case bounded arity.
Notice arity auxiliary predicate construction depends
number head-variables corresponding transformed TGD, which, general,
bounded.
160

fiTaming Infinite Chase

case bounded-arity WGTGDs, exptime upper bound still derived
transformation showing class TGD sets obtained
transformation satisfies Exponential Clouds Criterion Section 7. see
database exponential number clouds, notice
every large atom V (Y) derived rule small weak guard g body,
i.e., weak guard g bounded arity. cloud cloud (D, , g) weak guard g
clearly determines everything g guarded chase forest; particular, cloud
V (Y). Thus set clouds(D, ) clouds atoms determined
clouds atoms bounded arity. immediately verifiable combinatorial reasons,
singly-exponentially many clouds. shows |clouds(D, )/ |
singly-exponentially bounded. Therefore, first condition Definition 7.5 satisfied.
hard verify second condition Definition 7.5, too. Thus, query-answering
based bounded-arity WGTGDs exptime. Given GTGDs subclass
WGTGDs, exptime bound holds bounded-arity GTGDs, well.
completely different proof theorem follows directly results
Gottlob, Manna, Pieris (2013a) class GTGDs, Gottlob,
Manna, Pieris (2013b) class WGTGDs.

9. EGDs
section deal equality generating dependencies (EGDs), generalization
functional dependencies, which, turn, generalize key dependencies (Abiteboul, Hull, &
Vianu, 1995).
Definition 9.1. Given relational schema R, EGD first-order formula form
X(X) X = Xk , (X) conjunction atoms R, X , Xk X.
dependency satisfied instance B if, whenever homomorphism h
maps atoms (X) atoms B, h(X ) = h(Xk ).
possible repair, chase, instance according EGDs analogy
chase based TGDs. start defining EGD chase rule.
Definition 9.2. [EGD Applicability] Consider instance B schema R, EGD
form (X) Xi = Xj R. say applicable B
homomorphism h h((X)) B h(Xi ) 6= h(Xj ).
Definition 9.3. [EGD Chase Rule] Let EGD form (X) Xi = Xj
suppose applicable instance B via homomorphism h. result
application B h failure {h(Xi ), h(Xj )} (because unique name
assumption). Otherwise, result application instance B obtained B
replacing occurrence h(Xj ) h(Xi ) h(Xi ) precedes h(Xj ) lexicographical
order. h(Xj ) precedes h(Xi ) occurrences h(Xi ) replaced h(Xj )
,h

instead. write B B say B obtained B via single EGD chase step.
Definition 9.4. [Chase sequence respect TGDs EGDs] Let database
= E , set TGDs E set EGDs. (possibly
infinite) chase sequence respect sequence instances B0 , B1 , . . .
161

fiCal, Gottlob & Kifer

,hi

Bi Bi+1 , B0 = E > 0. chase sequence said
failing last step failure. chase sequence said fair every TGD
EGD applicable certain step eventually applied.
case fair chase sequence happens finite, B0 , . . . , Bm , rule application change Bm , chase well defined Bm , denoted chase(D, ).
purposes, order application TGDs EGDs irrelevant. following
therefore, saying fair chase sequence, refer fair chase sequence,
chosen according order application dependencies.
well-known (see Johnson & Klug, 1984) EGDs cause problems combined
TGDs, even simple types EGDs, plain key constraints,
implication problem EGDs plus TGDs query answering problem undecidable.
remains true even EGDs together GTGDs. fact, even though inclusion
dependencies fully guarded TGDs, implication problem, query answering, query
containment undecidable keys used EGDs inclusion dependencies
TGDs (Chandra & Vardi, 1985; Mitchell, 1983; Cal et al., 2003a).
Moreover, result infinite chase using TGDs well-defined limit
infinite, monotonically increasing sequence (or, equivalently, least fixed-point
monotonic operator), sequence sets obtained infinite chase database
TGDs EGDs is, general, neither monotonic convergent. Thus, even though
define chase procedure TGDs plus EGDs, clear result
infinite chase involving TGDs EGDs defined.
reasons, cannot hope extend positive results weakly guarded
sets TGDs, even GTGDs, previous sections include arbitrary EGDs.
Therefore, looking suitable restrictions EGDs, would allow us to: (i)
use (possibly infinite) chase procedure obtain query-answering algorithm,
(ii) transfer decidability results upper complexity bounds derived previous
sections extended formalism.
class fulfills desiderata subclass EGDs, call innocuous
relative set TGDs. EGDs enjoy property query answering insensitive
them, provided chase fail. words, = E ,
set TGDs, E set EGDs, E innocuous relative , simply
ignore EGDs non-failing chase sequence. possible because, intuitively,
non-failing sequence generate atom entailed chase(D, ).
specifically, start notion innocuous application EGD. Intuitively, making two symbols equal, innocuous EGD application makes atom
equal existing atom a0 ; way, consequence EGD
application, original atom lost, new atom whatsoever introduced.
concept innocuous EGD application formally defined follows.
Definition 9.5. [Innocuous EGD application] Consider (possibly infinite) non-failing
chase sequence = B0 , B1 , . . ., starting database D, respect set =
E , set TGDs E set EGDs. say EGD
,h

application Bi Bi+1 , E > 0, innocuous Bi+1 Bi .
162

fiTaming Infinite Chase

Notice innocuousness semantic, syntactic, property. desirable
innocuous EGD applications applications cannot trigger new TGD applications, i.e., TGD applications possible EGD applied.
Given might undecidable whether set dependencies certain class
guarantees innocuousness EGD applications, one either give direct proof
innocuousness concrete set dependencies, Section 10.2, define
sufficient syntactic conditions guarantee innocuousness EGD applications
entire class dependencies, done, e.g., Cal et al. (2012a).
Definition 9.6. Let = E , set TGDs E set EGDs,
= E . E innocuous if, every database fair
chase sequence respect non-failing, application EGD
sequence respect innocuous.
Theorem 9.7. Let = E , set TGDs E set EGDs
innocuous . Let database fair chase sequence respect
non-failing. |= Q iff chase(D, ) |= Q.
Proof. Consider fair chase sequence B0 , B1 , . . . = B0 presence ,
,hi

Bi Bi+1 > 0 E . Let us define modified chase procedure
call blocking chase, denoted blockchase(D, ). blocking chase uses two sets:
set C blocked atoms set (unblocked) atoms A. started database
|= E (the case 6|= E possible implies immediate chase
failure), C initialized empty set (C = ) initialized D.
initialization, blocking chase attempts apply dependencies E exactly
way standard fair chase sequence, following caveats.
trying application hi , hi i:
TGD, hi (body(i )) C = , apply hi , hi add new atom
generated application A.
TGD hi (body(i )) C 6= , application hi , hi blocked,
nothing done.
EGD, application hi , hi proceeds follows. Add C
facts standard chase disappear step (because Bi Bi1 , due
innocuousness), i.e., add C set Bi Bi1 . Thus, instead eliminating tuples
A, blocking chase simply bans used putting C.
Note that, construction blockchase(D, ), whenever block chase encounters
EGD , hi , hi actually applicable, blockchase(D, ) well-defined. Let us use Ci
Ai denote values C step i, respectively. Initially, C0 = A0 =
explained before. Observe = C0 C1 C2 = A0 A1 A2
monotonically increasing sequences least upper bounds C = Ci = Ai ,
respectively. Clearly, (C , ) least fixpoint transformation performed
blockchase(D, ) (with respect component-wise set inclusion).
Now, let defined = C . definition S, have: |= .
Moreover, homomorphism h maps chase(D, ) S. Note h
limit homomorphism sequence h1 , h2 , h3 , . . . (these hi homomorphisms
163

fiCal, Gottlob & Kifer

used computing block chase), defined set pairs (x, y)
exists > 0 hi (hi1 ( h1 (x))) = altered
homomorphism hj j > i. Note every instance B contains D,
B |= D. particular, |= D. Putting everything together, conclude |= .
also well-known (see Nash et al., 2006) set atoms
|= , homomorphism hM hM (chase(D, )) .
assume |= Q. |= Q and, chase(D, ), also
chase(D, ) |= Q. Conversely, chase(D, ) |= Q, homomorphism g,
g(Q) chase(D, ). Therefore, set atoms |= ,
since hM (chase(D, )) , hM (g(Q)) . latter means |= Q.
come problem checking, given database set = E ,
set WGTGDs E EGDs innocuous , whether fair chase
,h

sequence (denoted B0 , B1 , . . .) respect fails. Consider application Bi
Bi+1 , E form (X) X = Xk . application causes chase
fail, h(X ) h(Xk ) distinct values dom(D). Notice Bj exists
j 6 i, exist j > i.
Lemma 9.8. Consider database set dependencies = E ,
weakly guarded set TGDs E EGDs innocuous .
fair chase sequence respect fails iff EGD E
form (X) X = Xk homomorphism h h((X)) chase(D, ),
h(X ) 6= h(Xk ), {h(X ), h(Xk )} dom(D).
Proof (sketch).
If. Let B0 , B1 , . . . fair chase sequence respect . First,
difficult show that, since E innocuous relative , failure occurs step
,hi

EGD applications Bi Bi+1 , E < 1, innocuous
(see similar proof Cal, Console, & Frosini, 2013) sequence B0 , . . . , B1 .
this, direction follows straightforwardly.
if. assumption, fails Bk , k > 1. Since applications innocuous
EGDs remove tuples chase, easily seen that, applicable Bk via
homomorphism h, also applicable chase(D, ) via homomorphism
h, settles only-if part.
Theorem 9.9. Consider database set dependencies = E ,
GTGDs (resp., WGTGDs) E EGDs innocuous . Checking
whether fair chase sequence respect fails decidable,
complexity query answering GTGDs (resp., WGTGDs) alone.
Proof (sketch). Let neq new binary predicate, serve inequality.
extension neq defined dom(D) dom(D) {(d, d) | dom(D)}
constructed time quadratic |dom(D)|. Now, every EGD form (X)
X1 = X2 , X1 , X2 X, define following Boolean conjunctive query (expressed
set atoms): Q = (X) {neq(X1 , X2 )}. Since, construction, new facts
form neq(1 , 2 ) introduced chase, immediate see, Lemma 9.8,
164

fiTaming Infinite Chase

least one Q positive answer fair chase sequence
respect fails. Theorem 9.7, answering query Q done
respect chase alone, decidable.
Let = E theorem, database, let Q query.
theorem, check |= Q help following algorithm:
1. check whether fair chase sequence respect fails algorithm
described Theorem 9.9;
2. fair chase sequence respect fails, return true halt;
3. |= Q return true; otherwise return false.
gives us following corollary:
Corollary 9.10. Answering general conjunctive queries weakly guarded sets TGDs
innocuous EGDs ptime reducible answering queries class
weakly guarded sets TGDs alone, thus complexity.

10. Applications
section discuss applications results weakly guarded sets TGDs
Description Logic languages object-oriented logic languages.
10.1 DL-Lite
DL-Lite (Calvanese et al., 2007; Artale et al., 2009) prominent family ontology
languages tractable query answering. Interestingly, restriction GTGDs called
linear TGDs (which exactly one body-atom one head-atom) properly extends
DL-Lite languages, shown Cal et al. (2012a). complexity query answering
linear TGDs lower GTGDs, refer reader work Cal
et al. (2012a) details.
Furthermore, Cal et al. (2012a) also show language GTGDs properly extends
description logic EL well extension ELf , allows inverse functional
roles. fact TGDs capture important DL-based ontology languages confirms
TGDs useful tools ontology modeling querying.
10.2 F-Logic Lite
F-Logic Lite expressive subset F-logic (Kifer et al., 1995), well-known formalism
introduced object-oriented deductive languages. refer reader work Cal
Kifer (2006) details F-Logic Lite. Roughly speaking, compared full FLogic, F-Logic Lite excludes negation default inheritance, allows limited
form cardinality constraints. F-Logic Lite encoded set twelve TGDs
EGDs, below, denote FLL :
1 : type(O, A, ), data(O, A, V ) member(V, ).
2 : sub(C1 , C3 ), sub(C3 , C2 ) sub(C1 , C2 ).
3 : member(O, C), sub(C, C1 ) member(O, C1 ).
165

fiCal, Gottlob & Kifer

4 : data(O, A, V ), data(O, A, W ), funct(A, O) V = W .
Note EGD axiomatization.
5 : mandatory(A, O) V data(O, A, V ).
Note TGD existentially quantified variable head.
6 : member(O, C), type(C, A, ) type(O, A, ).
7 : sub(C, C1 ), type(C1 , A, ) type(C, A, ).
8 : type(C, A, T1 ), sub(T1 , ) type(C, A, ).
9 : sub(C, C1 ), mandatory(A, C1 ) mandatory(A, C).
10 : member(O, C), mandatory(A, C) mandatory(A, O).
11 : sub(C, C1 ), funct(A, C1 ) funct(A, C).
12 : member(O, C), funct(A, C) funct(A, O).
results paper apply set constraints, since FLL weakly
guarded set, single EGD 4 innocuous. innocuousness 4 shown
observing that, whenever EGD applied, turns one atom another; moreover,
new data atoms created chase (see rule 5 ) new labeled nulls exactly
position data[3], symbols equated also reside.
prove relevant complexity results. start showing BCQ answering
F-Logic Lite np-complete.
Theorem 10.1. Conjunctive query answering F-Logic Lite rules np-hard.
Proof (sketch). proof reduction 3-colorability problem. Encode
graph G = (V, E) conjunctive query Q which, edge (vi , vj ) E, two atoms
data(X, Vi , Vj ) data(X, Vj , Vi ), X unique variable. Let database
= {data(o, r, g), data(o, g, r), data(o, r, b), data(o, b, r), data(o, g, b), data(o, b, g)}. Then,
G three-colorable iff |= Q, case iff FLL |= Q. transformation
G (Q, D) obviously polynomial, proves claim.
Theorem 10.2. Conjunctive query answering F-Logic Lite rules np.
Proof (sketch). mentioned before, ignore EGD FLL since,
innocuous, interfere query answering. Let FLL denote set TGDs
resulting FLL eliminating rule 4 , i.e., let FLL = FLL {4 }. establish
membership np, sufficient show that: (1) FLL weakly guarded; (2) FLL enjoys
PCC (see Definition 7.1). condition, membership np
proved exhibiting following. (i) algorithm, analogous Acheck, constructs
canonical versions atoms chase clouds (which stored
cloud store), polynomial time. algorithm check whether atomic
(Boolean) query satisfied atom cloud store. (ii) algorithm, analogous
Qcheck, guesses (by calling analogous version Tcheck) entire clouds
guessing cloud index (a unique integer) cloud store. algorithm
check, alternating logarithmic space (alogspace), correctness cloud guess.
check, use cloud main atom predecessor configuration.
complexity running algorithm shown npalogspace = np.
(1) easy: affected positions data[3], member[1], type[1], mandatory[2], funct[2]
data[1]. easy see every rule FLL weakly guarded, thus FLL
weakly guarded.
166

fiTaming Infinite Chase

let us sketch (2 ). need show FLL satisfies two conditions
Definition 7.1. prove first condition holds FLL follows. Let full
FLL =
FLL {5 }. full TGDs (no existentially-quantified variables) appli
cation alter domain. chase(D, FLL ) = chase(chase(D, full
FLL ), FLL ).
full
Let us closer look D+ = chase(D, FLL ). Clearly, dom(D+ ) = dom(D).
predicate symbol p, let Rel (p) denote relation consisting p-atoms D+ .
Let family relations obtained relations Rel (p)
performing arbitrary selection followed projection (we forbid disjunctions
selection predicate). example, let c, dom(D). contain relations
1,2 ({1=c} Rel (data)), 2 ({1=d3=c} Rel (data)), on, numbers represent
attributes selection applied. Given D+ size polynomial
maximum arity relation Rel (p) 3, set size polynomial D+
thus polynomial D. shown preserved precise sense,
going final result chase(D+ , FLL ): relation Rel (p) corresponding predicate p final chase result, performing selection values outside dom(D)
projecting columns used selection, set tuples dom(D)elements result relation . example, v5 labeled null, set
dom(D), member(v5 , ) element final result, relation
. Similarly, v7 v8 new values, set values A, data(v7 , A, v8 )
chase, relation . follows FLL satisfies (2). fact,
possible clouds determined polynomially many ways choosing three
elements predicate. proof preservation property done
induction i-th new labeled null added. Roughly, labeled null, created
rule 5 , analyze sets values (or tuples) attached via rules 4 ,
6 , 7 , 8 , 10 , on, conclude sets already present
next lower level, thus, induction hypothesis, .
second condition Definition 7.1 proved similar arguments.
Theorems 10.1 10.2 immediately get following result.
Corollary 10.3. Conjunctive query answering F-Logic Lite rules np-complete
general conjunctive queries, ptime fixed-size atomic conjunctive queries.

11. Conclusions Related Work
paper identified large non-trivial class tuple-generating equalitygenerating dependencies problems conjunctive query containment answering decidable, provided relevant complexity results. Applications
results span databases knowledge representation. particular, shown
class constraints subsumes classical work Johnson Klug (1984) well
recent results Cal Kifer (2006). Moreover, able capture relevant
ontology formalisms Description Logics (DL) family, particular DL-Lite EL.
problem query containment non-terminating chase addressed
database context Johnson Klug (1984), ontological theory contains inclusion dependencies key dependencies particular form. introduction
DL-Lite family description logics works Calvanese et al. (2007) Artale et al.
167

fiCal, Gottlob & Kifer

(2009) significant leap forward ontological query answering due expressiveness DL-Lite languages tractable data complexity. Conjunctive query answering
DL-Lite advantage first-order rewritable, i.e., pair hQ, i, Q
CQ DL-Lite ontology (TBox), rewritten first-order query Q
that, every database (ABox) D, answer Q logical theory
coincides answer Q D. Since first-order query written
SQL, practical terms means pair hQ, rewritten SQL query
original database D.
Rewritability widely adopted ontology querying. works Cal, Calvanese,
De Giacomo, Lenzerini (2001), Cal, Lembo, Rosati (2003b) present query
rewriting techniques deal Entity-Relationship schemata inclusion dependencies, respectively. work Perez-Urbina, Motik, Horrocks (2010) presents Datalog rewriting algorithm expressive DL ELHIO , comprises limited form
concept role negation, role inclusion, inverse roles, nominals, i.e., concepts
interpreted singletons. Conjunctive query answering ELHIO ptime-complete
data complexity, proposed algorithm also optimal ontology languages
DL-Lite. Optimizations rewriting linear TGDs (TGDs exactly one
atom body) presented Gottlob, Orsi, Pieris (2011), Orsi Pieris
(2011). Gottlob Schwentick (2012) showed rewriting conjunctive query
set linear TGDs polynomial size query TGD set.
rewriting techniques ptime-complete languages (in data complexity)
proposed description logic EL (Rosati, 2007; Lutz, Toman, & Wolter, 2009; Krotzsch
& Rudolph, 2007). Another approach worth mentioning combination rewriting
chase (see Kontchakov, Lutz, Toman, Wolter, & Zakharyaschev, 2010); technique
introduced DL-Lite order tackle performance problems arise
rewriting according ontology large.
Recent works concentrate semantic characterization sets TGDs
query answering decidable (Baget et al., 2011a). notion first-order rewritability
tightly connected finite unification set (FUS). FUS semantically characterized
set TGDs enjoy following property: every conjunctive query Q,
rewriting Q Q obtained backward-chaining unification, according
rules , terminates. Another semantic characterization TGDs bounded
treewidth set (BTS), i.e., set TGDs chase TGDs bounded
treewidth. seen Section 3, every weakly guarded set TGDs BTS. finite
expansion set (FES) set TGDs guarantees, every database, termination
restricted chase, therefore decidability query answering.
Datalog family (Cal et al., 2011) proposed purpose providing
tractable query answering algorithms general ontology languages. Datalog ,
fundamental constraints TGDs EGDs. Clearly, TGDs extension Datalog
rules. absence value invention (existential quantification head), thoroughly
discussed Patel-Schneider Horrocks (2007), main shortcoming plain Datalog
modeling ontologies even conceptual data formalisms Entity-Relationship
model (Chen, 1976). Sets GTGDs WGTGDs Datalog ontologies. Datalog
languages easily extend common tractable ontology languages; particular,
168

fiTaming Infinite Chase

main DL-Lite languages (see Cal et al., 2012a). fundamental decidability paradigms
Datalog family following:
Chase termination. chase terminates, finite instance produced; obviously, Theorem 2.10, query answering case decidable. notable
syntactic restriction guaranteeing chase termination weak acyclicity TGDs,
refer reader milestone paper Fagin et al. (2005). general
syntactic restrictions studied Deutsch, Nash, Remmel (2008), Marnette
(2009), Greco, Spezzano, Trubitsyna (2011), Baget et al. (2011a), Grau,
Horrocks, Krotzsch, Kupke, Magka, Motik, Wang (2012). semantic property
TGDs, called parsimony, introduced Leone, Manna, Terracina, Veltri
(2012). Parsimony ensures decidability query answering termination special
version chase, called parsimonious chase.
Guardedness. paradigm studied paper. thorough study
data complexity query answering GTGDs linear TGDs, subset
guarded class, found work Cal et al. (2012a). interesting
classes frontier guarded (FGTGDs) weakly frontier-guarded TGDs (WFGTGDs) considered studied Baget et al. (2011a), Baget, Mugnier, Rudolph,
Thomazo (2011b), Krotzsch Rudolph (2011). idea underlying
classes that, obtain decidability, sufficient guard frontier variables,
is, variables occur body head rule.8 WFGTGDs syntactically liberal succinct WGTGDs, conjunctive query answering WFGTGDs computationally expensive case
bounded arities. seen querying WFGTGDs expressive
querying WGTGDs. fact, every WFGTGD set CQ Q,
exists WGTGD set CQ Q every database D, |= Q iff
|= Q . generalization WFGTGDs, called greedy bounded-treewidth TGDs,
proposed Baget et al. (2011b), together complexity analysis. guardedness paradigm combined acyclicity Krotzsch Rudolph (2011),
generalization WFGTGDs weakly acyclic TGDs proposed.
Stickiness. class sticky sets TGDs (or sticky Datalog , see Cal et al., 2012b)
defined means syntactic restriction rule bodies, ensure
sticky set TGDs first-order rewritable, FUS, according Baget et al.
(2011a). Civili Rosati (2012) proposed extension sticky sets TGDs.
interaction equality generating dependencies TGDs subject several works, starting work Johnson Klug (1984), deals
functional inclusion dependencies, proposing class inclusion dependencies called
key-based, which, intuitively, interaction key dependencies thanks syntactic
restrictions. absence interaction EGDs TGDs captured notion
separability, first introduced Cal et al. (2003a) key inclusion dependencies,
also adopted, though sometimes explicitly stated, instance, Cal, Gottlob,
Pieris (2012a), Artale et al. (2009) Calvanese et al. (2007)see work Cal,
Gottlob, Orsi, Pieris (2012b) survey topic.
8. FGTGDs independently discovered Mantas Simkus working doctoral thesis.

169

fiCal, Gottlob & Kifer

shown Cal et al. (2012a), stratified negation added straightforwardly
Datalog . recently, guarded Datalog extended two versions well-founded
negation (see Gottlob, Hernich, Kupke, & Lukasiewicz, 2012; Hernich, Kupke, Lukasiewicz,
& Gottlob, 2013).
ontological query answering, normally finite infinite models theories
considered. cases, restricting attention finite solutions (models)
always equivalent general approach. property equivalence query
answering finite models query answering arbitrary models (finite
infinite) called finite controllability, proved restricted classes functional
inclusion dependencies Johnson Klug (1984). Finite controllability proved
class arbitrary inclusion dependencies pioneering work Rosati (2011).
even general result appears work Barany et al. (2010), shown
finite controllability holds guarded theories.
related previous approach guarded logic programming guarded open answer set
programming (Heymans, Nieuwenborgh, & Vermeir, 2005). easy see set
GTGDs interpreted guarded answer set program, defined Heymans et al.
(2005), guarded answer set programs expressive GTGDs
allow negation.
Implementations ontology-based data access systems take advantage query answering techniques tractable ontologies; particular, mention DLV (Leone et al., 2012),
Mastro (Savo, Lembo, Lenzerini, Poggi, Rodriguez-Muro, Romagnoli, Ruzzi, & Stella, 2010)
NYAYA (De Virgilio, Orsi, Tanca, & Torlone, 2012).
Acknowledgments
extended version results authors, published KR 2008 Conference DL 2008 Workshop. Andrea Cal Georg Gottlob also affiliated
Oxford-Man Institute Quantitative Finance, University Oxford, UK. Andrea
Cal acknowledges support EPSRC project Logic-based Integration Querying
Unindexed Data (EP/E010865/1). Georg Gottlob acknowledges funding European Research Council European Communitys Seventh Framework Program
(FP7/2007-2013) / ERC grant agreement DIADEM no. 246858. Michael Kifer partially supported NSF grant 0964196. authors grateful Andreas Pieris,
Marco Manna, Michael Morak anonymous reviewers valuable comments
suggestions improve paper.

References
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations Databases. Addison-Wesley.
Adler, I., Gottlob, G., & Grohe, M. (2007). Hypertree width related hypergraph invariants. Eur. Journal Combinatorics, 28 (8), 21672181.
Aho, A., Sagiv, Y., & Ullman, J. D. (1979). Equivalence relational expressions. SIAM
Journal Computing, 8 (2), 218246.
Arenas, M., Bertossi, L. E., & Chomicki, J. (1999). Consistent query answers inconsistent
databases. Proc PODS 1999, pp. 6879.
170

fiTaming Infinite Chase

Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). DL-lite family
relations. J. Artif. Intell. Res., 36, 169.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Proc. IJCAI 2005,
pp. 364369.
Baget, J.-F., Leclere, M., Mugnier, M.-L., & Salvat, E. (2011a). rules existential
variables: Walking decidability line. Artif. Intell., 175 (910), 16201654.
Baget, J.-F., Mugnier, M.-L., Rudolph, S., & Thomazo, M. (2011b). Walking complexity
lines generalized guarded existential rules. Proc. IJCAI 2011, pp. 712717.
Barany, V., Gottlob, G., & Otto, M. (2010). Querying guarded fragment. Proc.
LICS 2010, pp. 110.
Beeri, C., Fagin, R., Maier, D., Mendelzon, A. O., Ullman, J. D., & Yannakakis, M. (1981).
Properties acyclic database schemes. Proc. STOC 1981, pp. 355362.
Beeri, C., & Vardi, M. Y. (1981). implication problem data dependencies.
Proc. ICALP 1981, pp. 7385.
Bourhis, P., Morak, M., & Pieris, A. (2013). impact disjunction query answering
guarded-based existential rules. Proc. IJCAI 2013.
Cabibbo, L. (1998). expressive power stratified logic programs value invention.
Inf. Comput., 147 (1), 2256.
Cal, A., Calvanese, D., De Giacomo, G., & Lenzerini, M. (2001). Accessing data integration
systems conceptual schemas. Proc. ER 2001, pp. 270284.
Cal, A., Console, M., & Frosini, R. (2013). separability ontological constraints.
Forthcoming.
Cal, A., Gottlob, G., & Kifer, M. (2008). Taming infinite chase: Query answering
expressive relational constraints. Proc. KR 2008, pp. 7080.
Cal, A., Gottlob, G., & Lukasiewicz, T. (2009). general datalog-based framework
tractable query answering ontologies. Proc. PODS 2009, pp. 7786.
Cal, A., Gottlob, G., & Lukasiewicz, T. (2012a). general datalog-based framework
tractable query answering ontologies. J. Web Semantics, 14, 5783. Extended
version (Cal, Gottlob, & Lukasiewicz, 2009).
Cal, A., Gottlob, G., Orsi, G., & Pieris, A. (2012b). interaction existential rules
equality constraints ontology querying. Proc. Correct Reasoning 2012,
pp. 117133.
Cal, A., Gottlob, G., & Pieris, A. (2011). New expressive languages ontological query
answering. Proc. AAAI 2011.
Cal, A., Gottlob, G., & Pieris, A. (2012a). Ontological query answering expressive
entity-relationship schemata. Inf. Syst., 37 (4), 320335.
Cal, A., Gottlob, G., & Pieris, A. (2012b). Towards expressive ontology languages:
query answering problem. Artif. Intell., 193, 87128.
Cal, A., & Kifer, M. (2006). Containment conjunctive object meta-queries. Proc.
VLDB 2006, pp. 942952.
171

fiCal, Gottlob & Kifer

Cal, A., Lembo, D., & Rosati, R. (2003a). decidability complexity query
answering inconsistent incomplete databases. PODS 2003, pp. 260271.
Cal, A., Lembo, D., & Rosati, R. (2003b). Query rewriting answering constraints
data integration systems. Proc. IJCAI 2003, pp. 1621.
Cal, A., & Martinenghi, D. (2010). Querying incomplete data extended er schemata.
TPLP, 10 (3), 291329.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning efficient query answering description logics: DL-lite family. J.
Autom. Reasoning, 39 (3), 385429.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (2002). Description logics information
integration. Computational Logic: Logic Programming Beyond, Vol. 2408
LNCS, pp. 4160. Springer.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998). decidability query
containment constraints. Proc. PODS 1998, pp. 149158.
Chandra, A. K., Kozen, D., & Stockmeyer, L. J. (1981a). Alternation. J. ACM,
28 (1), 114133.
Chandra, A. K., Lewis, H. R., & Makowsky, J. A. (1981b). Embedded implicational dependencies inference problem. Proc. STOC 1981, pp. 342354.
Chandra, A. K., & Merlin, P. M. (1977). Optimal implementation conjunctive queries
relational data bases. Proc. STOC 1977, pp. 7790.
Chandra, A. K., & Vardi, M. Y. (1985). implication problem functional inclusion
dependencies undecidable. SIAM J. Comput., 14, 671677.
Chen, P. P. (1976). entity-relationship model - toward unified view data. Trans.
Database Syst., 1 (1), 936.
Civili, C., & Rosati, R. (2012). broad class first-order rewritable tuple-generating
dependencies. Proc. Datalog 2.0 2012, pp. 6880.
Courcelle, B. (1990). monadic second-order logic graphs. I. recognizable sets finite
graphs. Information Computation, 85 (1), 1275.
Dantsin, E., Eiter, T., Gottlob, G., & Voronkov, A. (2001). Complexity expressive
power logic programming. ACM Computing Surveys, 33 (3), 374425.
De Virgilio, R., Orsi, G., Tanca, L., & Torlone, R. (2012). NYAYA: system supporting
uniform management large sets semantic data. Proc. ICDE 2012, pp.
13091312.
Deutsch, A., Nash, A., & Remmel, J. B. (2008). chase revisited. Proc. PODS 2008,
pp. 149158.
Fagin, R. (1983). Degrees acyclicity hypergraphs relational database schemes.
J. ACM, 30 (3), 514550.
Fagin, R., Kolaitis, P. G., Miller, R. J., & Popa, L. (2005). Data exchange: semantics
query answering. Theor. Comput. Sci., 336 (1), 89124.
172

fiTaming Infinite Chase

Goncalves, M. E., & Gradel, E. (2000). Decidability issues action guarded logics.
Proc. DL 2000, pp. 123132.
Gottlob, G., Hernich, A., Kupke, C., & Lukasiewicz, T. (2012). Equality-friendly wellfounded semantics applications description logics. Proc. AAAI 2012.
Gottlob, G., Leone, N., & Scarcello, F. (2001). Hypertree decompositions: survey.
Proc. MFCS 2001, pp. 3757.
Gottlob, G., Leone, N., & Scarcello, F. (2002). Hypertree decompositions tractable
queries. J. Comp. Syst. Sci., 64 (3).
Gottlob, G., Leone, N., & Scarcello, F. (2003). Robbers, marshals, guards: game theoretic logical characterizations hypertree width. J. Comput. Syst. Sci., 66 (4),
775808.
Gottlob, G., Manna, M., & Pieris, A. (2013a). Combining decidability paradigms existential rules. appear TPLP.
Gottlob, G., Manna, M., & Pieris, A. (2013b). Querying hybrid fragments existential
rules. Forthcoming.
Gottlob, G., & Nash, A. (2006). Data exchange: computing cores polynomial time.
Proc. PODS 2006, pp. 4049.
Gottlob, G., Orsi, G., & Pieris, A. (2011). Ontological queries: Rewriting optimization.
Proc. ICDE 2011, pp. 213.
Gottlob, G., & Schwentick, T. (2012). Rewriting ontological queries small nonrecursive
datalog programs. Proc. KR 2012.
Gradel, E. (1999). restraining power guards. J. Symb. Log., 64 (4), 17191742.
Grau, B. C., Horrocks, I., Krotzsch, M., Kupke, C., Magka, D., Motik, B., & Wang, Z.
(2012). Acyclicity conditions application query answering description
logics. Proc. KR 2012.
Greco, S., Spezzano, F., & Trubitsyna, I. (2011). Stratification criteria rewriting techniques checking chase termination. PVLDB, 4 (11), 11581168.
Hernich, A., Kupke, C., Lukasiewicz, T., & Gottlob, G. (2013). Well-founded semantics
extended datalog ontological reasoning. Proc. PODS 2013, pp. 225236.
Hernich, A., Libkin, L., & Schweikardt, N. (2011). Closed world data exchange. ACM
Trans. Database Syst., 36 (2), 1453.
Heymans, S., Nieuwenborgh, D. V., & Vermeir, D. (2005). Guarded open answer set programming. Proc. LPNMR 2005.
Johnson, D. S., & Klug, A. (1984). Testing containment conjunctive queries
functional inclusion dependencies. J. Comp. Syst. Sci., 28, 167189.
Kifer, M., Lausen, G., & Wu, J. (1995). Logical foundations object-oriented framebased languages. J. ACM, 42, 741843.
Koch, C. (2002). Query rewriting symmetric constraints. Proc. FoIKS 2002, pp.
130147.
173

fiCal, Gottlob & Kifer

Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2010). combined approach query answering dl-lite. Proc. KR 2010.
Krotzsch, M., & Rudolph, S. (2007). Conjunctive queries EL composition roles.
Proc. DL 2007.
Krotzsch, M., & Rudolph, S. (2011). Extending decidable existential rules joining acyclicity guardedness. Proc. IJCAI 2011, pp. 963968.
Leone, N., Manna, M., Terracina, G., & Veltri, P. (2012). Efficiently computable datalog;
programs. Proc. KR 2012.
Li, L., & Horrocks, I. (2003). software framework matchmaking based semantic
web technology. Proc. WWW 2003.
Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering description
logic EL using relational database system. Proc. IJCAI 2009, pp. 20702075.
Maier, D., Mendelzon, A. O., & Sagiv, Y. (1979). Testing implications data dependencies.
Trans. Database Syst., 4 (4), 455469.
Mailharrow, D. (1998). classification constraint-based framework configuration.
Artif. Intell. Eng. Design, Anal. Manuf., 12 (4), 383397.
Marnette, B. (2009). Generalized schema-mappings: termination tractability.
Proc. PODS 2009, pp. 1322.
Millstein, T., Levy, A., & Friedman, M. (2000). Query containment data integration
systems. PODS 2000.
Mitchell, J. C. (1983). implication problem functional inclusion dependencies.
Inf. Control, 56, 154173.
Nash, A., Deutsch, A., & Remmel, J. (2006). Data exchange, data integration, chase.
Tech. rep. CS2006-0859, UCSD.
Orsi, G., & Pieris, A. (2011). Optimizing query answering ontological constraints.
PVLDB, 4 (11), 10041015.
Patel-Schneider, P. F., & Horrocks, I. (2007). comparison two modelling paradigms
semantic web. J. Web Semantics, 5 (4), 240250.
Perez-Urbina, H., Motik, B., & Horrocks, I. (2010). Tractable query answering rewriting
description logic constraints. J. Appl. Logic, 8 (2), 186209.
Qian, X. (1996). Query folding. Proc. ICDE 1996, pp. 4855.
Rabin, M. O. (1969). Decidability second-order theories automata infinite trees.
Trans. Am. Math. Soc., 141 (135), 4.
Rosati, R. (2007). conjunctive query answering EL. Proc. DL 2007.
Rosati, R. (2011). finite controllability conjunctive query answering databases
open-world assumption. J. Comput. Syst. Sci., 77 (3), 572594.
Savo, D. F., Lembo, D., Lenzerini, M., Poggi, A., Rodriguez-Muro, M., Romagnoli, V.,
Ruzzi, M., & Stella, G. (2010). Mastro work: Experiences ontology-based data
access. Proc. Description Logics.

174

fiJournal Artificial Intelligence Research 48 (2013) 23-65

Submitted 04/13; published 10/13

Learning Optimal Bayesian Networks:
Shortest Path Perspective
Changhe Yuan

changhe.yuan@qc.cuny.edu

Department Computer Science
Queens College/City University New York
Queens, NY 11367 USA

Brandon Malone

brandon.malone@cs.helsinki.fi

Department Computer Science
Helsinki Institute Information Technology
Fin-00014 University Helsinki, Finland

Abstract
paper, learning Bayesian network structure optimizes scoring function
given dataset viewed shortest path problem implicit state-space search
graph. perspective highlights importance two research issues: development
search strategies solving shortest path problem, design heuristic functions guiding search. paper introduces several techniques addressing
issues. One A* search algorithm learns optimal Bayesian network structure
searching promising part solution space. others mainly
two heuristic functions. first heuristic function represents simple relaxation
acyclicity constraint Bayesian network. Although admissible consistent, heuristic may introduce much relaxation result loose bound. second heuristic
function reduces amount relaxation avoiding directed cycles within groups
variables. Empirical results show methods constitute promising approach
learning optimal Bayesian network structures.

1. Introduction
Bayesian networks graphical models represent uncertain relations
random variables domain compactly intuitively. Bayesian network directed
acyclic graph nodes represent random variables, arcs lack
represent dependence/conditional independence relations variables.
relations quantified set conditional probability distributions, one
variable conditioning parents. Overall, Bayesian network represents joint
probability distribution variables.
Applying Bayesian networks real-world problems typically requires building graphical
representations problems. One popular approach use score-based methods
find high-scoring structures given dataset (Cooper & Herskovits, 1992; Heckerman,
1998). Score-based learning shown NP-hard, however (Chickering, 1996).
Due complexity, early research area mainly focused developing approximation algorithms greedy hill climbing approaches (Heckerman, 1998; Bouckaert,
1994; Chickering, 1995; Friedman, Nachman, & Peer, 1999). Unfortunately solutions
found methods unknown quality. recent years, several exact learning algoc
2013
AI Access Foundation. rights reserved.

fiYuan & Malone

rithms developed based dynamic programming (Koivisto & Sood, 2004; Ott,
Imoto, & Miyano, 2004; Silander & Myllymaki, 2006; Singh & Moore, 2005), branch
bound (de Campos & Ji, 2011), integer linear programming (Cussens, 2011; Jaakkola,
Sontag, Globerson, & Meila, 2010; Hemmecke, Lindner, & Studeny, 2012). methods
guaranteed find optimal solutions able finish successfully. However,
efficiency scalability leave much room improvement.
paper, view problem learning Bayesian network structure optimizes scoring function given dataset shortest path problem. idea
represent solution space learning problem implicit state-space search graph,
shortest path start goal nodes graph corresponds
optimal Bayesian network. perspective highlights importance two orthogonal
research issues: development search strategies solving shortest path problem,
design admissible heuristic functions guiding search. present several
techniques address issues. Firstly, A* search algorithm developed learn
optimal Bayesian network focusing searching promising parts solution
space. Secondly, two heuristic functions introduced guide search. tightness
heuristic determines efficiency search algorithm. first heuristic represents simple relaxation acyclicity constraint Bayesian networks
variable chooses optimal parents independently. result, heuristic estimate may
contain many directed cycles result loose bound. second heuristic, named
k-cycle conflict heuristic, based form relaxation tightens bound
avoiding directed cycles within groups variables. Finally, traversing
search graph, need calculate cost arc visited, corresponds
selecting optimal parents variable candidate set. present two data
structures storing querying costs candidate parent sets. One set
full exponential-size data structures called parent graphs stored hash tables
answer query constant time. sparse representation parent
graph stores optimal parent sets improve space efficiency.
empirically evaluated A* algorithm empowered different combinations
heuristic functions parent graph representations set UCI machine learning
datasets. results show even simple heuristic full parent graph representation, A* often achieve better efficiency and/or scalability existing approaches
learning optimal Bayesian networks. k-cycle conflict heuristic sparse parent
graph representation enabled algorithm achieve even greater efficiency
scalability. results indicate proposed methods constitute promising approach
learning optimal Bayesian network structures.
remainder paper structured follows. Section 2 reviews problem
learning optimal Bayesian networks reviews related work. Section 3 introduces
shortest path perspective learning problem. formulation search graph
discussed detail. Section 4 introduces two data structures developed compute
store optimal parent sets pairs variables candidate sets. data structures used query cost arc search graph. Section 5 presents
A* search algorithm. developed two heuristic functions guiding algorithm
studied theoretical properties. Section 6 presents empirical results evaluating
algorithm several existing approaches. Finally, Section 7 concludes paper.
24

fiLearning Optimal Bayesian Networks

2. Background
first provide brief summary related work learning Bayesian networks.
2.1 Learning Bayesian Network Structures
Bayesian network directed acyclic graph (DAG) G represents joint probability
distribution set random variables V = {X1 , X2 , ..., Xn }. directed arc Xi
Xj represents dependence two variables; say Xi parent Xj .
use PAj stand parent set Xj . dependence relation Xj PAj
quantified using conditional probability distribution, P (Xj |PAj ). joint probability
distribution represented G factorized product
Q conditional probability
distributions network, i.e., P (X1 , ..., Xn ) = ni=1 P (Xi |PAi ). addition
compact representation, Bayesian networks also provide principled approaches solving
various inference tasks, including belief updating, probable explanation, maximum
Posteriori assignment (Pearl, 1988), relevant explanation (Yuan, Liu, Lu, & Lim,
2009; Yuan, Lim, & Littman, 2011a; Yuan, Lim, & Lu, 2011b).
Given dataset = {D1 , ..., DN }, data point Di vector values
variables V, learning Bayesian network task finding network structure
best fits D. work, assume variable discrete finite number
possible values, data point missing values.
roughly three main approaches learning problem: score-based learning,
constraint-based learning, hybrid methods. Score-based learning methods evaluate
quality Bayesian network structures using scoring function selects one
best score (Cooper & Herskovits, 1992; Heckerman, 1998). methods basically
formulate learning problem combinatorial optimization problem. work well
datasets many variables, may fail find optimal solutions large
datasets. discuss approach detail next section,
approach take. Constraint-based learning methods typically use statistical testings
identify conditional independence relations data build Bayesian network
structure best fits independence relations (Pearl, 1988; Spirtes, Glymour, &
Scheines, 2000; Cheng, Greiner, Kelly, Bell, & Liu, 2002; de Campos & Huete, 2000; Xie &
Geng, 2008). Constraint-based methods mostly rely results local statistical testings,
often scale large datasets. However, sensitive accuracy
statistical testings may work well insufficient noisy data.
comparison, score-based methods work well even datasets relatively data
points. Hybrid methods aim integrate advantages previous two approaches
use combinations constraint-based and/or score-based methods solving learning
problem (Dash & Druzdzel, 1999; Acid & de Campos, 2001; Tsamardinos, Brown, & Aliferis,
2006; Perrier, Imoto, & Miyano, 2008). One popular strategy use constraint-based
learning create skeleton graph use score-based learning find high-scoring
network structure subgraph skeleton (Tsamardinos et al., 2006; Perrier et al.,
2008). work, consider Bayesian model averaging methods aim
estimate posterior probabilities structural features edges rather model
selection (Heckerman, 1998; Friedman & Koller, 2003; Dash & Cooper, 2004).
25

fiYuan & Malone

2.2 Score-Based Learning
Score-based learning methods rely scoring function Score(.) evaluating quality
Bayesian network structure. search strategy used find structure G optimizes
score. Therefore, score-based methods two major elements, scoring functions
search strategies.
2.2.1 Scoring Functions
Many scoring functions used measure quality network structure.
Bayesian scoring functions define posterior probability distribution
network structures conditioning data, structure highest
posterior probability presumably best structure. scoring functions best
represented Bayesian Dirichlet score (BD) (Heckerman, Geiger, & Chickering, 1995)
variations, e.g., K2 (Cooper & Herskovits, 1992), Bayesian Dirichlet score
score equivalence (BDe) (Heckerman et al., 1995), Bayesian Dirichlet score score
equivalence uniform priors (BDeu) (Buntine, 1991). scoring functions often
form trading goodness fit structure data complexity
structure. goodness fit measured likelihood structure given
data amount information compressed structure data.
Scoring functions belonging category include minimum description length (MDL)
(or equivalently Bayesian information criterion, BIC) (Rissanen, 1978; Suzuki, 1996; Lam
& Bacchus, 1994), Akaike information criterion (AIC) (Akaike, 1973; Bozdogan, 1987),
(factorized) normalized maximum likelihood function (NML/fNML) (Silander, Roos, Kontkanen, & Myllymaki, 2008), mutual information tests score (MIT) (de Campos,
2006). scoring functions decomposable, is, score network
decomposed sum node scores (Heckerman, 1998).
optimal structure G may unique multiple Bayesian network structures may share optimal score1 . Two network structures said belong
equivalence class (Chickering, 1995) represent set probability distributions possible parameterizations. Score-equivalent scoring functions assign
score structures equivalence class. scoring functions
score equivalent.
mainly use MDL score work. Let ri number states Xi , Npai
number data points consistent PAi = pai , Nxi ,pai number data
points constrained Xi = xi . MDL defined follows (Lam & Bacchus, 1994).

DL(G) =

X

DL(Xi |PAi ),



1. often use optimal instead optimal throughout paper.

26

(1)

fiLearning Optimal Bayesian Networks


log N
K(Xi |PAi ),
2
X
Nxi ,pai
H(Xi |PAi ) =
Nxi ,pai log
,
Npai
xi ,pai

K(Xi |PAi ) = (ri 1)
rl .
Xl PAi

DL(Xi |PAi ) = H(Xi |PAi ) +

(2)
(3)
(4)

goal find Bayesian network minimum MDL score. However,
methods means restricted MDL; decomposable scoring function,
BIC, BDeu, fNML, used instead without affecting search strategy.
demonstrate that, test BDeu experimental section. One slight difference
MDL scoring functions latter scores need maximized
order find optimal solution. rather straightforward translate
maximization minimization problems simply changing sign scores. Also,
sometimes use costs refer scores, also represent distances
nodes search graph.
2.2.2 Local Search Strategies
Given n variables, O(n2n(n1) ) directed acyclic graphs (DAGs). size
solution space grows exponentially number variables. surprising
score-based structure learning shown NP-hard (Chickering, 1996). Due
complexity, early research focused mainly developing approximation algorithms (Heckerman, 1998; Bouckaert, 1994). Popular search strategies used include greedy hill
climbing, stochastic search, genetic algorithm, etc..
Greedy hill climbing methods typically begin initial network, e.g., empty
network randomly generated structure, repeatedly apply single edge operations,
including addition, deletion, reversal, finding locally optimal network. Extensions approach include tabu search random restarts (Glover, 1990), limiting
number parents parameters variable (Friedman et al., 1999), searching
space equivalence classes (Chickering, 2002), searching space variable
orderings (Teyssier & Koller, 2005), searching constraints extracted
data (Tsamardinos et al., 2006). optimal reinsertion algorithm (OR) (Moore & Wong,
2003) adds different operator: variable removed network, optimal parents
selected, variable reinserted network parents.
parents selected ensure new network still valid Bayesian network.
Stochastic search methods Markov Chain Monte Carlo simulated annealing
also applied find high-scoring structure (Heckerman, 1998; de Campos &
Puerta, 2001; Myers, Laskey, & Levitt, 1999). methods explore solution space
using non-deterministic transitions neighboring network structures favoring
better solutions. stochastic moves used hope escape local optima find
better solutions.
optimization methods genetic algorithms (Hsu, Guo, Perry, & Stilson,
2002; Larranaga, Kuijpers, Murga, & Yurramendi, 1996) ant colony optimization meth27

fiYuan & Malone

ods (de Campos, Fernndez-Luna, Gmez, & Puerta, 2002; Daly & Shen, 2009)
applied learning Bayesian network structures well. Unlike previous methods
work one solution time, population-based methods maintain set candidate solutions throughout search. step, create next generation
solutions randomly reassembling current solutions genetic algorithms,
generating new solutions based information collected incumbent solutions
ant colony optimization. hope obtain increasingly better populations solutions
eventually find good network structure.
local search methods quite robust face large learning problems
many variables. However, guarantee find optimal solution. worse,
quality solutions typically unknown.
2.2.3 Optimal Search Strategies
Recently multiple exact algorithms developed learning optimal Bayesian networks. Several dynamic programming algorithms proposed based observation
Bayesian network least one leaf (Ott et al., 2004; Singh & Moore, 2005).
leaf variable child variables Bayesian network. order find optimal
Bayesian network set variables V, sufficient find best leaf. leaf
choice X, best possible Bayesian network constructed letting X choose optimal
parent set PAX V\{X} letting V\{X} form optimal subnetwork.
best leaf choice one minimizes sum Score(X, PAX ) Score(V\{X})
scoring function Score(.). formally, have:
Score(V) = min {Score(V \ {X}) + BestScore(X, V \ {X})},
XV

(5)


BestScore(X, V \ {X}) =

min
Score(X, PAX ).
PAX V\{X}

(6)

Given recurrence relation, dynamic programming algorithm works follows. first finds optimal structures single variables, trivial. Starting
base cases, algorithm builds optimal subnetworks increasingly larger variable
sets optimal network found V. dynamic programming algorithms
find optimal Bayesian network O(n2n ) time space (Koivisto & Sood, 2004; Ott
et al., 2004; Silander & Myllymaki, 2006; Singh & Moore, 2005). Recent algorithms
improved memory complexity either trading longer running times reduced memory consumption (Parviainen & Koivisto, 2009) taking advantage layered structure
present within dynamic programming lattice (Malone, Yuan, & Hansen, 2011b; Malone,
Yuan, Hansen, & Bridges, 2011a).
branch bound algorithm (BB) proposed de Campos Ji (2011)
learning Bayesian networks. algorithm first creates cyclic graph allowing
variable obtain optimal parents variables. best-first search strategy
used break cycles removing one edge time. algorithm uses
approximation algorithm estimate initial upper bound solution pruning.
algorithm also occasionally expands worst nodes search frontier hope find
28

fiLearning Optimal Bayesian Networks

Figure 1: order graph four variables.
better networks update upper bound. completion, algorithm finds optimal
network structure subgraph initial cyclic graph. algorithm ran
memory finding solution, switch using depth-first search strategy
find suboptimal solution.
Integer linear programming (ILP) also used learn optimal Bayesian network
structures (Cussens, 2011; Jaakkola et al., 2010). learning problem cast integer
linear program polytope exponential number facets. outer bound
approximation polytope solved. solution relaxed problem
integral, guaranteed optimal structure. Otherwise, cutting planes branch
bound algorithms subsequently applied find optimal structure. Recently
similar method proposed find optimal structure searching space
equivalence classes (Hemmecke et al., 2012).
Several methods considered optimal constraints enforce
network structure. example, optimal parents selected variable, K2
finds optimal network structure particular variable ordering (Cooper & Herskovits,
1992). methods developed (Ordyniak & Szeider, 2010; Kojima, Perrier, Imoto, &
Miyano, 2010) find optimal network structure must subgraph given super
graph.

3. Shortest Path Perspective
section introduces shortest path perspective problem learning Bayesian
network structure given dataset.
3.1 Order Graph
state space graph learning Bayesian networks basically Hasse diagram containing
subsets variables domain. Figure 1 visualizes state space graph
learning problem four variables. top-most node empty set layer
29

fiYuan & Malone

0 start search node, bottom-most node complete set layer n
goal node, n number variables domain. arc U U {X}
represents generating successor node adding new variable {X} existing set
variables U; U called predecessor U {X}. cost arc equal score
selecting optimal parent set X U, i.e., BestScore(X, U). example, arc
{X1 , X2 } {X1 , X2 , X3 } cost equal BestScore(X3 , {X1 , X2 }). node layer
ni successors many ways add new variable, predecessors
many leaf choices. define expanding node U generating successors
nodes U.
search graph thus defined, path start node goal node defined
sequence nodes arc nodes next node
sequence. path also corresponds ordering variables order
appearance. example, path traversing nodes , {X1 }, {X1 , X2 }, {X1 , X2 , X3 },
{X1 , X2 , X3 , X4 } stands variable ordering X1 , X2 , X3 , X4 . also call
search graph order graph. cost path defined sum costs
arcs path. shortest path path minimum total cost
order graph.
Given shortest path, reconstruct Bayesian network structure noting
arc path encodes choice optimal parents one variables
preceding variables, complete path represents ordering
variables. Therefore, putting together optimal parent choices generates valid
Bayesian network. construction, Bayesian network structure optimal.
3.2 Finding Shortest Path
Various methods applied solve shortest path problem. Dynamic programming
considered evaluate order graph using top sweep order graph (Silander
& Myllymaki, 2006; Malone et al., 2011b). Layer layer, dynamic programming finds
optimal subnetwork variables contained node order graph based
results previous layers. example, three ways construct Bayesian
network node {X1 , X2 , X3 }: using {X2 , X3 } subnetwork X1 leaf, using
{X1 , X3 } subnetwork X2 leaf, using {X1 , X2 } subnetwork X3
leaf. top-down sweep makes sure optimal subnetworks already found
{X2 , X3 }, {X1 , X3 }, {X1 , X2 }. need select optimal parents
leaves identify leaf produces optimal network {X1 , X2 , X3 }.
evaluation reaches node last layer, shortest path and, equivalently, optimal
Bayesian network found global variable set.
drawback dynamic programming approach need compute
BestScore(.) candidate parent sets variable. n variables,
2n nodes order graph, also 2n1 parent scores computed
variable, totally n2n1 scores. number variables increases, computing storing
order parent graphs quickly becomes infeasible.
paper, propose apply A* algorithm (Hart, Nilsson, & Raphael, 1968)
solve shortest path problem. A* uses heuristic function evaluate quality
search nodes expand promising search node search step.
30

fiLearning Optimal Bayesian Networks

guidance heuristic functions, A* needs explore part search
graph finding optimal solution. However, comparison dynamic programming,
A* overhead calculating heuristic values maintaining priority queue.
actual relative performance dynamic programming A* thus depends
efficiency calculating heuristic values tightness values (Felzenszwalb
& McAllester, 2007; Klein & Manning, 2003).

4. Finding Optimal Parent Sets
introducing algorithm solving shortest path problem, first discuss
obtain cost BestScore(X, U) arc U U {X} visit
order graph. Recall arc involves selecting optimal parents variable
candidate set. need consider subsets candidate set finding subset
best score. section, introduce two data structures related methods
computing storing optimal parent sets scores pairs variable candidate
parent set.
exact algorithms learning Bayesian network structures need calculate
optimal parent sets scores. present reasonable approach calculation
paper. Note, however, approach applicable algorithms, vice versa.
4.1 Parent Graph
use data structure called parent graph compute costs arcs order graph.
variable parent graph. parent graph variable X Hasse diagram
consisting subsets variables V \ {X}. node U stores optimal parent
set PAX U minimizes Score(X, PA X ) well BestScore(X, U) itself.
example, Figure 2(b) shows sample parent graph X1 contains best scores
subsets {X2 , X3 , X4 }. obtain Figure 2(b), however, first need calculate
preliminary graph Figure 2(a) contains raw score subset U parent
set X1 , i.e., Score(X1 , U). Equation 3 shows, scores calculated based
counts particular instantiations parent child variables.
use AD-tree (Moore & Lee, 1998) collect counts dataset
compute scores. AD-tree unbalanced tree structure contains two types
nodes, AD-tree nodes varying nodes. AD-tree node stores number data points
consistent particular variable instantiation; varying node used instantiate
state variable. full AD-tree stores counts data points consistent
partial instantiations variables. sample AD-tree two variables shown
Figure 3. n variables states each, number AD-tree nodes AD-tree
(d+1)n . grows even faster size order parent graph. Moore Lee (1998)
also described sparse AD-tree significantly reduces space complexity. Readers
referred paper details. pseudo code assumes sparse AD-tree
used.
Given AD-tree, ready calculate raw scores Score(X1 , .) Figure 2(a).
exponential number scores parent graph. However, parent
sets possibly optimal Bayesian network; certain parent sets discarded
without ever calculating values according following theorems Tian (2000).
31

fiYuan & Malone

Figure 2: sample parent graph variable X1 . (a) raw scores Score(X1 , .)
parent sets. first line node gives parent set, second
line gives score using set parents X1 . (b) optimal
scores BestScore(X1 , .) candidate parent set. second line
node gives optimal score using subset variables first line
parents X1 . (c) optimal parent sets scores. pruned parent
sets shown gray. parent set pruned predecessors
better score.

X1 = *
X2 = *
C = 50
Vary
V
X1

Vary
V
X2

X1 = 0
X2 = *

X1 = 1
X2 = *

X1 = *
X2 = 0

X1 = *
X2 = 1

C = 20

C = 30

C = 25

C = 25

Vary
X2

Vary
X2

X1 = 0
X2 = 0

X1 = 0
X2 = 1

X1 = 1
X2 = 0

X1 = 1
X2 = 1

C = 15

C=5

C = 10

C = 20

Figure 3: AD-tree.
use theorems compute necessary MDL scores. scoring functions
BDeu also similar pruning rules (de Campos & Ji, 2011). Algorithm 1 provides
pseudo code calculating raw scores.
Theorem 1 optimal Bayesian network based MDL scoring function,
2N
variable log( log
N ) parents, N number data points.
32

fiLearning Optimal Bayesian Networks

Algorithm 1 Score Calculation Algorithm
Input: AD sparse AD-tree input data; V input variables.
Output: Score(X, U) pair X V U V \ {X}
1: function calculateMDLScores(AD, V)
2:
Xi V
3:
calculateScores(Xi , AD)
4:
end
5: end function
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:

function calculateScores(Xi, AD)
2N
k 0 log( log
Prune due Theorem 1
N )
U U V \ {X}& |U| == k
parent sets size k
prune f alse
U
K(Xi |U) - Score(Xi , U \ {Y }) > 0
prune true
Prune due Theorem 2
break
end
end
prune ! = true
Score(Xi , U) log2 N K(Xi |U)
Complexity term
instantiation xi , u Xi , U
Log likelihood term
cF amily GetCount({xi } u,AD)
cP arents GetCount(u, AD)
Score(Xi , U) Score(Xi , U) - cF amily log cF amily
Score(Xi , U) Score(Xi , U) + cF amily log cP arents
end
end
end
end
end function

Theorem 2 Let U two candidate parent sets X, U S, K(Xi |S)
DL(Xi |U) > 0. supersets cannot possibly optimal parent sets
X.
computing raw scores, compute parent graph according following
theorem appeared many earlier papers, e.g., see work Teyssier
Koller (2005), de Campos Ji (2010). theorem simply means parent set
optimal subset better score.
Theorem 3 Let U two candidate parent sets X U S,
Score(X, U) Score(X, S). optimal parent set X candidate set.
33

fiYuan & Malone

Algorithm 2 Computing parent graphs
Input: necessary Score(X, U), X V&U V \ {X}
Output: Full parent graphs containing BestScore(X, U)
1: function calculateFullParentGraphs(V, Score(., .))
2:
X V
3:
layer 0 n
Propagate best scores graph
4:
U U V \ {X}& |U| == layer
5:
calculateBestScore(X, U, Score(., .))
6:
end
7:
end
8:
end
9: end function
10:
11:
12:
13:
14:
15:
16:
17:

function calculateBestScore(X, U, Score(., .))
BestScore(X, U) Score(X, U)
U
BestScore(X, U \ {Y }) < BestScore(X, U)
BestScore(X, U) BestScore(X, U \ {Y })
end
end
end function

function getBestScore(X, U)
19:
return BestScore(X, U)
20: end function

Propagate best scores

Query BestScore(X, U)

18:

Therefore, generate successor node U{Y } U parent graph X,
check whether Score(X, U {Y }) smaller BestScore(X, U). so, let parent
graph node U{Y } record optimal parent set. Otherwise BestScore(X, U)
smaller, propagate optimal parent set U U{Y }. propagation,
must following (Teyssier & Koller, 2005).
Theorem 4 Let U two candidate parent sets X U S. must
BestScore(X, S) BestScore(X, U).
pseudo code propagating scores computing parent graph outlined
Algorithm 2. Figure 2(b) shows parent graph optimal scores propagating
best scores top bottom.
search order graph, whenever visit new arc U U {X},
find score looking parent graph variable X. example, need find
optimal parents X1 {X2 , X3 }, look node {X2 , X3 } X1 parent graph
find optimal parent set score. make look-ups efficient, use hash
tables organize parent graphs query answered constant time.
34

fiLearning Optimal Bayesian Networks

parentsX1
scoresX1

{X2 , X3 }
5

{X3 }
6

{X2 }
8

{}
10

Table 1: Sorted scores parent sets X1 pruning parent sets
possibly optimal.
parentsX1
2
parentsX
X1
X3
parentsX1
4
parentsX
X1

{X2 , X3 }
1
1
0

{X3 }
0
1
0

{X2 }
1
0
0

{}
0
0
0

Table 2: parentsX (Xi ) bit vectors X1 . 1 line Xi indicates corresponding parent set includes variable Xi , 0 indicates otherwise. Note
that, pruning, none optimal parent sets include X4 .

4.2 Sparse Parent Graphs
full parent graph variable X exhaustively enumerates subsets V \ {X}
stores BestScore(X, U) subsets. Naively, approach requires storing
n2n1 scores parent sets (Silander & Myllymaki, 2006). Theorem 3, however,
number optimal parent sets often far smaller full size. Figure 2(b) shows
optimal parent set may shared several candidate parent sets. full parent
graph representation allocate space repetitive information candidate sets,
resulting waste time space.
address limitations, introduce sparse representation parent graphs
related scanning techniques querying optimal parent sets. full parent
graphs, begin calculating pruning scores described last Section. Due
Theorems 1 2, parent sets pruned without evaluated.
Therefore, create full parent graphs. Also, instead creating
Hasse diagrams, sort optimal parent scores variable X list, also
maintain parallel list stores associated optimal parent sets. call sorted
lists scoresX parentsX . Table 1 shows sorted lists optimal scores
parent graph Figure 2(b). essence, allows us store efficiently process
scores Figure 2(c).
find optimal parent set X candidate set U, simply scan
list X starting beginning. soon find first parent set subset
U, find optimal parent score BestScore(X, U). trivially true due
following theorem.
Theorem 5 first subset U parentsX optimal parent set X U.
Scanning lists find optimal parent sets inefficient done properly.
Since scanning arc visited order graph, inefficiency
scanning large impact search algorithm.
35

fiYuan & Malone

parentsX1
validX1
3
parentsX
X1
new
validX1

{X2 , X3 }
1
0
0

{X3 }
1
0
0

{X2 }
1
1
1

{}
1
1
1

Table 3: result performing bitwise operation exclude parent sets
include X3 . 1 validX1 bit vector means parent set
include X3 used selecting optimal parents. first set bit
indicates best possible score parent set.

parentsX1
validX1
3
parentsX
X1
new
validX1

{X2 , X3 }
0
0
0

{X3 }
0
1
0

{X2 }
1
0
0

{}
1
1
1

Table 4: result performing bitwise operation exclude parent sets
include either X3 X2 . 1 validnew
X1 bit vector means parent
set includes neither X2 X3 . initial validX1 bit vector already excluded
X3 , finding validnew
X1 required excluding X2 .

ensure efficiency, propose following scanning technique. variable
X, first initialize working bit vector length kscoresX k called validX 1s.
indicates parent scores scoresX usable. Then, create n 1 bit vectors
also length kscoresX k, one variable V \ {X}. bit vector variable
denoted parentsYX contains 1s parent sets contain 0s others.
Table 2 shows bit vectors example Table 1. Then, exclude variable
candidate parent, perform bit operation validnew
validX & parentsYX . new
X
validX bit vector contains 1s parent sets subsets V \ {Y }.
first set bit corresponds BestScore(X, V \ {Y }). Table 3 shows example excluding
X3 set possible parents X1 , first set bit new bit vector
corresponds BestScore(X1 , V \ {X3 }). want exclude X2 candidate
parent, new bit vector last step becomes current bit vector step,
2
bit operation applied: validnew
validX & parentsX
X
X1 . first set
bit result corresponds BestScore(X1 , V \ {X2 , X3 }). Table 4 demonstrates
operation. Also, important note exclude one variable time. example,
if, excluding X3 , wanted exclude X4 rather X2 , could take validnew

X
4
validX & parentsX
.

operations

described


createSparseParentGraph

X
getBestScore functions Algorithm 3.
pruning duplicate scores, sparse representation requires much less
memory storing possible parent sets scores. long kscores(X)k <
C(n 1, n2 ), also requires less memory memory-efficient dynamic programming
algorithm (Malone et al., 2011b).
Experimentally, show kscoresX k almost
36

fiLearning Optimal Bayesian Networks

Algorithm 3 Sparse Parent Graph Algorithms
Input: necessary Score(X, U), X V&U V \ {X}
Output: Sparse parent graphs containing optimal parent sets scores
1: function createSparseParentGraph(X, Score(., .))
2:
X V
3:
scorest , parentst sort(Score(X, ))
Sort scores, preferring low cardinality
4:
scoresX , parentsX
Initialize possibly optimal scores
5:
= 0 |scorest |
6:
prune f alse
7:
j = 0 |scoresX |
Check better subset pattern exists
8:
contains(parentst(i), parentsX (j))&scoresX (i) scorest (i)
9:
prune true
10:
Break
11:
end
12:
end
13:
prune ! = true
14:
Append scoresX , parentsX parentst(i), parentst (i)
15:
end
16:
end
17:
= 0 |scoresX |
Set bit vectors efficient querying
18:
parentsX (i)
19:
set(parentsYX (i))
20:
end
21:
end
22:
end
23: end function
24:
25:
26:
27:
28:
29:
30:
31:

function getBestScore(X, U)
valid allScoresX
V \ U
valid valid& parentsYX
end
f sb f irstSetBit(valid)
return scoresX [f sb]
end function

Query BestScore(X, U)

Return first score set bit

always smaller C(n 1, n2 ) several orders magnitude. approach offers
(usually substantial) memory savings compared previous best approaches.
sparse representation extra benefit improving time efficiency well.
full representation, create complete exponential-size parent graphs,
even though many nodes parent graph share optimal parent choices.
sparse representation, avoid creating nodes, makes creating sparse
parent graphs much efficient.
37

fiYuan & Malone

5. A* Search Algorithm
ready tackle shortest path problem order graph. section
presents search algorithm well two admissible heuristic functions guiding
algorithm.
5.1 Algorithm
apply well known state space search method, A* algorithm (Hart et al., 1968),
solve shortest path problem order graph. main idea algorithm
use evaluation function f measure quality search nodes always expand
one lowest f cost exploration order graph. node U,
f (U) decomposed sum exact past cost, g(U), estimated future cost,
h(U). g(U) cost measures shortest distance start node U,
h(U) cost estimates far away U goal node. Therefore, f cost provides
estimated total cost best possible path passes U.
A* uses open list (usually priority queue) store search frontier,
closed list store expanded nodes. Initially open list contains start node,
closed list empty. search step, node lowest f -cost
open list, say U, selected expansion generate successor nodes. expanding
U, however, need first check whether goal node. yes, shortest path
goal found; construct Bayesian network path terminate
search.
U goal, expand generate successor nodes. successor
considers one possible way adding new variable, say X, leaf existing
subnetwork variables U, = U {X}. g cost calculated
sum g-cost U cost arc U S. arc cost well
optimal parent set PAX X U retrieved Xs parent graph. h cost
computed heuristic function describe shortly. record
following information2 : g cost, h cost, X, PAX .
clear order graph multiple paths node.
perform duplicate detection see whether node representing set variables
already generated before. check duplicates, search space blows
order graph size 2n order tree size n!. first check whether
duplicate already exists closed list. so, check whether duplicate
better g cost S. yes, discard immediately, represents worse path.
Otherwise, remove duplicate closed list, place open list.
happens found better path lower g cost, reopen node future
search.
duplicate found closed list, also need check open list.
duplicate found, simply add open list. Otherwise, compare
g costs duplicate S. duplicate lower g cost, discarded.
Otherwise, replace duplicate S. Again, lower g cost means better
path found.
2. also delay calculation h duplicate detection avoid unnecessary calculations
nodes pruned.

38

fiLearning Optimal Bayesian Networks

Algorithm 4 A* Search Algorithm
Input: full sparse parent graphs containing BestScore(X, U)
Output: optimal Bayesian network G
1: function main(D)
2:
start
3:
Score(start) 0 P
4:
push(open, start, V BestScore(Y, V \ {Y })
5:
!isEmpty(open)
6:
U pop(open)
7:
U goal
shortest path found
8:
print(The best score + Score(V))
9:
G construct network shortest path
10:
return G
11:
end
12:
put(closed, U)
13:
X V \ U
Generate successors
14:
g BestScore(X, U) + Score(U)
15:
contains(closed, U {X})
Closed list DD
16:
g < Score(U {X})
reopen node
17:
delete(closed, U {X})
18:
push (open, U {X}, g + h)
19:
Score(U {X}) g
20:
end
21:
else
22:
contains(open, U {X}) & g < Score(U {X}) Open list DD
23:
update(open, U {X}, g + h)
24:
Score(U {X}) g
25:
end
26:
end
27:
end
28:
end
29: end function

successor nodes generated, place node U closed
list, indicates node already expanded. Expanding top node open
list called one search step. A* algorithm performs step repeatedly goal
node selected expansion. moment shortest path start state
goal state found.
shortest path found, reconstruct optimal Bayesian network
structure starting goal node tracing back shortest path reaching
start node. Since node path stores leaf variable optimal parent set,
putting optimal parent sets together generates valid Bayesian network structure.
pseudo code A* algorithm shown Algorithm 4.
39

fiYuan & Malone

5.2 Simple Heuristic Function
A* algorithm provides different theoretical guarantees depending properties
heuristic function h. function h admissible h cost never greater
true cost goal; words, optimistic. Given admissible heuristic
function, A* algorithm guaranteed find shortest path goal node
selected expansion (Pearl, 1984). Let U node order graph. first consider
following simple heuristic function h.
Definition 1
h(U) =

X

BestScore(X, V\{X}).

(7)

XV\U

heuristic function allows remaining variable choose optimal parents
variables. design reflects principle exact cost relaxed problem
used admissible bound original problem (Pearl, 1984). case,
original problem learn Bayesian network directed acyclic graph. Equation 7
relaxes problem ignoring acyclicity constraint, directed cyclic graphs
allowed. heuristic function easily proven admissible following theorem.
proofs theorems paper found Appendix A.
Theorem 6 h admissible.
turns h even nicer property. heuristic function consistent if,
node U successor S, h(U) h(S) + c(U, S), c(U, S) stands cost
arc U S. Given consistent heuristic, f cost monotonically non-decreasing
following path order graph. result, f cost node less
equal f cost goal node. follows immediately consistent heuristic
guaranteed admissible. consistent heuristic, A* algorithm guaranteed
find shortest path node U U selected expansion. duplicate
found closed list, duplicate must optimal g cost, new node
discarded immediately. show following simple heuristic Equation 7
also consistent.
Theorem 7 h consistent.
heuristic may seem expensive compute requires computing BestScore(X, V\
{X}) variable X. However, scores easily found querying parent
graphs stored array repeated use. takes linear time calculate
heuristic start node. subsequent computation h, however, takes constant
time simply subtract best score newly added variable
heuristic value parent node.
5.3 Improved Admissible Heuristic
simple heuristic function defined Equation 7, referred hsimple hereafter, relaxes
acyclicity constraint Bayesian networks completely. result, hsimple may introduce
many directed cycles result loose bound. introduce another heuristic
section tighten heuristic. first use toy example motivate new heuristic,
describe two specific approaches computing heuristic.
40

fiLearning Optimal Bayesian Networks

X1

X2

X3

X4

Figure 4: directed graph representing heuristic estimate start search node.

5.3.1 Motivating Example
hsimple , heuristic estimate start node order graph allows variable
choose optimal parents variables. Suppose optimal parent sets
X1 , X2 , X3 , X4 {X2 , X3 , X4 }, {X1 , X4 }, {X2 }, {X2 , X3 } respectively. parent
choices shown directed graph Figure 4. Since acyclicity constraint
ignored, directed cycles introduced, e.g., X1 X2 . However, know
final solution cannot cycles; three cases possible X1 X2 : (1) X2
parent X1 (so X1 cannot parent X2 ), (2) X1 parent X2 , (3) neither
true. Based Theorem 4, third case cannot provide better value
first two cases one variables must fewer candidate parents.
(1) (2), unclear one better, take minimum
get lower bound. Consider case (1). delete arc X1 X2 rule
X1 parent X2 . let X2 rechoose optimal parents remaining
variables {X3 , X4 }, is, must check parent sets including X1 . deletion
arc alone cannot produce new bound best parent set X2
{X3 , X4 } necessarily {X4 }. total bound X1 X2 computed summing
together original bound X1 new bound X2 . call total bound
b1 . Case (2) handled similarly; call total bound b2 . joint cost
X1 X2 , c(X1 , X2 ), must optimistic, compute minimum b1 b2 .
Effectively considered possible ways break cycle obtained tighter
heuristic value. new heuristic clearly admissible, still allow cycles among
variables.
Often, hsimple introduces multiple cycles heuristic estimate. Figure 4 also
cycle X2 X4 . cycle shares X2 earlier cycle X1 X2 ;
say cycles overlap. One way break cycles set parent set X2
{X3 }; however, introduces new cycle X2 X3 . described detail
shortly, partition variables exclusive groups break cycles within
group. example, X2 X3 different groups, break cycle.
41

fiYuan & Malone

5.3.2 K-Cycle Conflict Heuristic
idea generalized compute joint cost variable group
size k avoiding cycles within group. node U order graph,
calculate heuristic value partitioning variables V \ U several exclusive
groups sum costs together. name resulting technique k-cycle conflict
heuristic. Note simple heuristic hsimple special case new heuristic,
simply contains costs individual variables (k=1).
new heuristic application additive pattern database technique (Felner,
Korf, & Hanan, 2004). Pattern databases (Culberson & Schaeffer, 1998) approach
computing admissible heuristic problem solving relaxed problem. Consider
15-puzzle problem. 15 square tiles numbered 1 15 randomly placed 4
4 box one position left empty. configuration tiles called state.
goal slide tiles one time destination configuration. tile slide
empty position beside position. 15 puzzle relaxed
contain tiles 1-8 tiles removed. relaxation, multiple
states original problem map one state abstract state space relaxed
problem share positions remaining tiles. abstract state called
pattern; cost pattern equal smallest cost sliding remaining
tiles destination positions. cost provides lower bound state
original state space maps pattern. costs patterns stored
pattern database.
relax problem different ways obtain multiple pattern databases.
solutions several relaxed problems independent, problems said
exclusive. 15-puzzle, also relax contain tiles 9-15. relaxation
solved independently previous one share puzzle
movements. concrete state original state space, positions tiles 1-8
map pattern first pattern database, positions tiles 9-15 map
different pattern second pattern database. costs patterns added
together obtain admissible heuristic, hence name additive pattern databases.
learning problem, pattern defined group variables, cost
optimal joint cost variables avoiding directed cycles them.
decomposability scoring function implies costs two exclusive patterns
added together obtain admissible heuristic.
explicitly break cycles computing cost pattern.
following theorem offers straightforward approach so.
Theorem 8 cost pattern U, c(U), equal shortest distance V \ U
goal node order graph.
consider example Figure 4. cost pattern {X1 , X2 } equal
shortest distance {X3 , X4 } goal order graph Figure 1.
Furthermore, difference c(U) sum simple heuristic values
variables U indicates amount improvement brought avoiding cycles within
pattern. differential score, called h , thus used quality measure
ordering patterns choosing patterns likely result tighter
heuristic.
42

fiLearning Optimal Bayesian Networks

5.3.3 Dynamic K-Cycle Conflict Heuristic
two slightly different versions k-cycle conflict heuristic. first version
named dynamic k-cycle conflict heuristic, compute costs groups variables
size k store single pattern database. According Theorem 8,
heuristic computed finding shortest distances nodes
last k layers order graph goal.
compute heuristic using breadth-first search backward search
order graph k layers. search starts goal node expands order
graph backward layer layer. reverse arc U {X} U cost arc
U U {X}, i.e., BestScore(X, U). reverse g cost U updated whenever new
path lower cost found. Breadth-first search ensures node U obtain
exact reverse g cost previous layer expanded. g cost cost pattern
V \ U. also compute differential score, h , pattern time.
pattern better differential score subset patterns
discarded. pruning significantly reduce size pattern database improve
query efficiency. algorithm computing dynamic k-cycle conflict heuristic
shown Algorithm 5.
heuristic created, calculate heuristic value search node
follows. node U, partition remaining variables V \ U set exclusive
patterns, sum costs together heuristic value. Since prune superset
patterns, always find partition. However, potentially many ways
partition. Ideally want find one highest total cost, represents
tightest heuristic value. problem finding optimal partition formulated
maximum weighted matching problem (Felner et al., 2004). k = 2, define
undirected graph vertex represents variable, edge two
variables represents pattern containing variables weight equal
cost pattern. goal select set edges graph two
edges share vertex total weight edges maximized. matching problem
solved O(n3 ) time, n number vertices (Papadimitriou & Steiglitz,
1982).
k > 2, add hyperedges matching graph connecting
k vertices represent larger patterns. goal becomes select set edges
hyperedges maximize total weight. However, three-dimensional higher-order
maximum weighted matching problem NP-hard (Garey & Johnson, 1979). means
solve NP-hard problem calculating heuristic value.
alleviate potential inefficiency, greedily select patterns based quality.
Consider node U unsearched variables V \ U. choose pattern highest
differential cost patterns subsets V \ U. repeat step
remaining variables variables covered. total cost chosen patterns
used heuristic value U. hdynamic function Algorithm 5 gives pseudocode
computing heuristic value.
dynamic k-cycle conflict heuristic introduced example dynamically partitioned pattern database (Felner et al., 2004) patterns dynamically
selected search algorithm. refer dynamic pattern database short.
43

fiYuan & Malone

Algorithm 5 Dynamic k-cycle Conflict Heuristic
Input: full sparse parent graphs containing BestScore(X, U)
Output: pattern database P patterns size k
1: function createDynamicPD(k)
2:
P D0 (V) 0
3:
h (V) 0
4:
l = 1 k
Perform BFS k levels
5:
U P Dl1
6:
expand(U, l)
7:
checkSave(U)
8:
P D(V \ U) P Dl1 (U)
9:
end
10:
end
11:
X P \ save
Remove superset patterns improvement
12:
delete P D(X)
13:
end
14:
sort(P : h )
Sort patterns decreasing costs
15: end function
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:

function expand(U, l)
X U
g P Dl1 (U) + BestScore(X, U \ {X})
g < P Dl (U \ {X}) P Dl (U \ {X}) g
end
end function

Duplicate detection

function checkSave(U)
P
h (U) g V\U BestScore(Y, V \ {Y })
X V \ U
Check improvement subset patterns
h (U) > h (U {X}) save(U)
end
end function
function hdynamic (U)
h0
RU
P
R
RR\S
h h + P D(S)
end
end
return h
end function

Calculate heuristic value U

Greedily find best subset pattern R

44

fiLearning Optimal Bayesian Networks

potential drawback dynamic pattern databases that, even greedy
method, computing heuristic value still much expensive simple heuristic
Equation 7. Consequently, search time longer even though tighter pattern
database heuristic results pruning fewer expanded nodes.
5.3.4 Static K-Cycle Conflict Heuristic
address inefficiency dynamic pattern database computing heuristic values,
introduce another version named static k-cycle conflict heuristic based statically
partitioned pattern database technique (Felner et al., 2004). idea partition
variables several static exclusive groups, create separate pattern database
group. Consider problem variables {X1 , ..., X8 }. divide variables
two groups, {X1 , ..., X4 } {X5 , ..., X8 }. group, say {X1 , ..., X4 }, create
pattern database contains costs subsets {X1 , ..., X4 } store
hash table. refer heuristic static pattern database short.S
create static pattern databases follows. static grouping V = Vi , need
compute pattern database group Vi resembles order graph containing
subsets Vi . use breadth first search create graph starting
node
Vi . cost arc U{X} U graph equal BestScore(X, ( j6=i Vj )U),
means variables groups valid candidate parents. ensure
efficient retrieval, static pattern databases stored hashtables; nothing pruned
them. Algorithm 6 gives pseudocode creating static pattern databases.
much simpler use static pattern databases compute heuristic value. Consider
search node {X1 , X4 , X8 }; unsearched variables {X2 , X3 , X5 , X6 , X7 }. simply
divide variables two patterns {X2 , X3 } {X5 , X6 , X7 } according static
grouping, look respective pattern databases, sum costs together
heuristic value. Moreover, since search step processes one variable,
one pattern affected requires new score lookup. Therefore, heuristic value
calculated incrementally. hstatic function Algorithm 6 provides pseudocode
naively calculating heuristic value.
5.3.5 Properties K-Cycle Conflict Heuristic
versions k-cycle conflict heuristic remain admissible. Although avoid
cycles within pattern, cannot prevent cycles across different patterns. following theorem proves result.
Theorem 9 k-cycle conflict heuristic admissible.
Understanding consistency new heuristic slightly complex. first
look static pattern database involve selecting patterns dynamically.
following theorem shows static pattern database still consistent.
Theorem 10 static pattern database version k-cycle conflict heuristic remains
consistent.
dynamic pattern database, search step needs solve maximum weighted
matching problem select set patterns compute heuristic value.
45

fiYuan & Malone

Algorithm 6 Static k-cycle Conflict Heuristics

Input: full sparse parent graphs containing BestScore(X, U), Vi partition V
Output: full pattern database P Vi
1: function createStaticPD(Vi )
2:
P D0i () 0 fi fi
3:
l = 1 fiVi fi
Perform BFS Vi

4:
U P Dl1
5:
expand(U, l, Vi )
(U)
6:
P (U) P Dl1
7:
end
8:
end
9: end function
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

function expand(U, l, Vi )
X Vi \ U

(U) + BestScore(X, U
g P Dl1
j6=i Vj )


g < P Dl (U X) P Dl (U X) g
end
end function
function hstatic (U)
h0
Vi V
h h + P (U Vi )
end
return h
end function

Duplicate detection

Sum P separately

following, show dynamic k-cycle conflict heuristic also consistent closely
following Theorem 4.1 work Edelkamp Schrodl (2012).
Theorem 11 dynamic pattern database version k-cycle conflict heuristic remains consistent.
However, theorem assumes use shortest distances nodes
abstract space. use greedy method solve maximum weighted
matching problem, longer guarantee find shortest paths. result,
may lose consistency property dynamic pattern database. thus necessary
A* reopen duplicate node closed list better path found.

6. Experiments
evaluated A* search algorithm set benchmark datasets UCI repository (Bache & Lichman, 2013). datasets 29 variables 30, 162 data
points. discretized variables two states using mean values deleted
46

fiLearning Optimal Bayesian Networks

1.00E+10
1.00E+09

Full

Largest Layer

Sparse

1.00E+08
1.00E+07

Size

1.00E+06
1.00E+05
1.00E+04
1.00E+03
1.00E+02
1.00E+01
1.00E+00

Figure 5: number parent sets scores stored full parent graphs
(Full), largest layer parent graphs memory-efficient dynamic programming (Largest Layer), sparse representation (Sparse).

data points missing values. A* search algorithm implemented Java3 .
compared algorithm branch bound (BB)4 (de Campos & Ji, 2011), dynamic programming (DP)5 (Silander & Myllymaki, 2006), integer linear programming
(GOBNILP) algorithms6 (Cussens, 2011). used latest versions software
source code time experiments well default parameter settings;
version 1.1 GOBNILP 2.1.1 SCIP. BB DP calculate MDL,
use BIC score, uses equivalent calculation MDL. results confirmed
algorithms found Bayesian networks either belong
equivalence class. experiments performed 2.66 GHz Intel Xeon 16GB
RAM running SUSE Linux Enterprise Server version 10.
6.1 Full vs Sparse Parent Graphs
first evaluated memory savings made possible sparse parent graphs comparison full parent graphs. particular, compared maximum number
scores stored variables algorithm. typical dynamic programming algorithm stores scores possible parent sets variables.
memory-efficient dynamic programming (Malone et al., 2011b) stores possible parent
sets one layer parent graphs variables, size largest layer
3. software package source code named URLearning (You Learning) implementing A*
algorithm downloaded http://url.cs.qc.cuny.edu/software/URLearning.html.
4. http://www.ecse.rpi.edu/cvrl/structlearning.html
5. http://b-course.hiit.fi/bene
6. http://www.cs.york.ac.uk/aig/sw/gobnilp/

47

fiYuan & Malone

parent graphs indication space requirement. sparse representation
stores optimal parent sets variables.
Figure 5 shows memory savings sparse representation benchmark
datasets. clear number optimal parent scores stored sparse representation typically several orders magnitude smaller full representation.
Furthermore, due Theorem 1, increasing number data points increases maximum number candidate parents. Therefore, number candidate parent sets increases
number data points increases; however, many new parent sets pruned
sparse representation Theorem 3. number variables also affects
number candidate parent sets. Consequently, number optimal parent scores
increases function number data points number variables.
results show, amount pruning data-dependent, though, easily predictable.
practice, find number data points affect number unique scores much
number variables.
6.2 Pattern Database Heuristics
new pattern database heuristic two versions: static dynamic pattern databases;
parameterized different ways. tested various parameterizations
new heuristics A* algorithm two datasets named Autos Flag. chose
two datasets large enough number variables better
demonstrate effect pattern database heuristics. dynamic pattern database,
varied k 2 4. static pattern databases, tried groupings 9-9-8 13-13
Autos dataset groupings 10-10-9 15-14 Flag dataset. obtained
groupings simply dividing variables datasets several consecutive blocks.
results based sparse parent graphs shown Figure 6. show
results full parent graphs A* ran memory datasets
full parent graphs used. sparse representations, A* achieved much better
scalability, able solve Autos heuristic Flag
best heuristics using sparse parent graphs. Hereafter experiments results
assume use sparse parent graphs.
Also, pattern database heuristics improved efficiency scalability A* significantly. A* either simple heuristic static pattern database grouping
10-10-9 ran memory Flag dataset. pattern database heuristics enabled A* finish successfully. dynamic pattern database k = 2 helped reduce
number expanded nodes significantly datasets. Setting k = 3 helped even
more. However, increasing k 4 resulted increased search time, sometimes
even increased number expanded nodes (not shown). believe larger k always
results better pattern database; occasional increase expanded nodes
greedy strategy used choose patterns fully utilize better heuristic.
longer search time understandable though, less efficient compute
heuristic value larger pattern databases, inefficiency gradually overtook
benefit. Therefore, k = 3 seems best parametrization dynamic pattern
database general. static pattern databases, able test much larger
48

fiLearning Optimal Bayesian Networks

1.00E+04

Running Time

Size Pattern Database

1.00E+05

1.00E+03
1.00E+02
1.00E+01
1.00E+00

500
450
400
350
300
250
200
150
100
50
0

Autos

1.00E+04

Running Time

Size Pattern Database

1.00E+05

1.00E+03
1.00E+02
1.00E+01
1.00E+00

500
450
400
350
300
250
200
150
100
50
0

X

X

F lag
Figure 6: comparison A* enhanced different heuristics (hsimple , hdynamic k =
2, 3, 4, hstatic groupings 9-9-8 13-13 Autos dataset
groupings 10-10-9 15-14 Flag dataset). Size Pattern Database
means number patterns stored. Running Time means search time
(in seconds) using indicated pattern database strategy. X means
memory.

groups need enumerate groups certain size. results
suggest fewer larger groups tend result tighter heuristic.
sizes static pattern databases typically much larger dynamic
pattern databases. However, time needed create pattern databases still negligible comparison search time cases. thus cost effective try compute
larger affordable-size static pattern databases achieve better search efficiency.
results show best static pattern databases typically helped A* achieve better
efficiency dynamic pattern databases, even number expanded nodes
larger. reason calculating heuristic values much efficient
using static pattern databases.
49

fiYuan & Malone

10000
BB Scoring

DP Scoring

A* Scoring

Scoring Time

1000

100

10

1

0.1

Figure 7: comparison scoring time BB, DP, A* algorithms. label
X-axis consists dataset name, number variables, number
data points.

6.3 A* Simple Heuristic
first tested A* hsimple heuristic. competing algorithm roughly two
phases, computing optimal parent sets/scores (scoring phase) searching Bayesian
network structure (searching phase). therefore compare algorithms based two
parts running time: scoring time search time. Figure 7 shows scoring times
BB, DP, A*. GOBNILP included assumes optimal scores
provided input. label horizontal axis shows dataset, number
variables, number data points. results show AD-tree method used
A* algorithm seems efficient approach computing parent scores.
scoring part DP often order magnitude slower others.
result somewhat misleading, however. scoring searching parts DP
tightly integrated algorithms. result, work DP done
scoring part; little work left search. show shortly, search time
DP typically short.
Figure 8(a) reports search time algorithms. benchmark
datasets difficult algorithms take long even fail find optimal
solutions. therefore terminate algorithm early runs 7,200 seconds
dataset. results show BB succeeded two datasets, Voting
Hepatitis, within time limit. datasets, A* algorithm several orders
magnitude faster BB. major difference A* BB formulation
search space. BB searches space directed cyclic graphs, A* always
maintains directed acyclic graph search. results indicate better
search space directed acyclic graphs.
results also show search time needed DP algorithm often shorter
A*. explained earlier, reason heavy lifting DP done
50

fiLearning Optimal Bayesian Networks

10000
BB

DP

GOBNILP

A*

Search Time

1000

100

10

1

X X

X

X

X

X

X

X

X X

X

(a)
10000

Total Running Time

DP Total Time

A* Total Time

1000

100

10

1

(b)
Figure 8: comparison (a) search time (in seconds) BB, DP, GOBNILP, A*
(b) total running time DP A*. X means corresponding
algorithm finish within time limit (7,200 seconds) ran memory
case A*.
.
scoring part. add scoring search time together, shown Figure 8(b),
A* several times faster DP datasets except Adult Voting (Again,
GOBNILP left search part). main difference A*
DP A* explores part order graph, dynamic programming fully
evaluates graph. However, step A* search algorithm overhead
cost computing heuristic function maintaining priority queue. One step
51

fiYuan & Malone

A* expensive similar dynamic programming step. pruning
outweigh overhead, A* slower dynamic programming. Adult
Voting large number data points, makes pruning technique
Theorem 1 less effective. Although DP algorithm perform pruning, due
simplicity, algorithm highly streamlined optimized performing
calculations. DP algorithm faster A* search two
datasets. However, A* algorithm efficient DP datasets.
datasets, number data points large comparison number
variables. pruning significantly outweighs overhead A*. example,
A* runs faster Mushroom dataset comparing total running time even though
Mushroom 8,000 data points.
comparison GOBNILP A* shows advantages. A* able find optimal Bayesian networks datasets well within
time limit. GOBNILP failed learn optimal Bayesian networks three datasets,
including Letter, Image, Mushroom. reason GOBNILP formulates
learning problem integer linear program whose variables correspond optimal
parent sets variables. Even though datasets many variables,
many optimal parent sets, integer programs many variables
solvable within time limit. hand, results also show GOBNILP
quite efficient many datasets. Even though dataset may many
variables, GOBNILP solve efficiently long number optimal parent sets
small. much efficient A* datasets Hepatitis Heart, although
opposite true datasets Adult Statlog.
6.4 A* Pattern Database Heuristics
Since static pattern databases seem work better dynamic pattern databases
cases, tested A* static pattern database (A*,SP) A*, DP, GOBNILP
datasets used Figure 8 well several larger datasets. used simple
static grouping n2 n2 datasets, n number variables.
results BB excluded solve additional dataset. results
shown Figure 9.
benefits brought pattern databases A* rather obvious.
datasets A* able finish, A*,SP typically order magnitude
faster. addition, A*,SP able solve three larger datasets: Sensor, Autos, Flag,
A* failed them. running time datasets pretty short,
indicates memory consumption parent graphs reduced, A*
able use memory order graph solve search problems rather
easily.
DP able solve one dataset, Autos, A* able solve.
somewhat surprising given A* pruning capability. explanation A*
stores search information RAM, fail RAM exhausted. DP
algorithm described Silander Myllymaki (2006) stores intermediate results
computer files hard disks, able scale larger datasets A*.
52

fiLearning Optimal Bayesian Networks

1000

Search Time

DP

GOBNILP

A*

A*, SP

100

10

1

X

X

X

XXX

X

X XX X X

Figure 9: comparison search time (in seconds) DP, GOBNILP, A*, A*,SP.
X means corresponding algorithm finish within time
limit (7,200 seconds) ran memory case A*.

GOBNILP able solve Autos, Horse, Flag, failed Sensors. Sensors
dataset 5, 456 data points. number optimal parent sets large, almost
106 shown Figure 5. GOBNILP begins difficulty solving datasets
8, 000 optimal parent scores particular computing environment. again,
GOBNILP quite efficient datasets able solve Autos Flag.
algorithm solve Horse dataset. Figure 5, clear
reason number optimal parent sets small dataset.
6.5 Pruning A*
gain insight performance A*, also looked amount pruning
A* different layers order graph. plot Figure 10 detailed numbers
expanded nodes versus numbers unexpanded nodes layer order
graph two datasets: Mushroom Parkinsons. use datasets
largest datasets solved A* A*,SP, manifest different
pruning behaviors. top two figures show results A* simple heuristic,
bottom two show A*,SP algorithm.
Mushroom, plain A* needed expand small portion search nodes
layer, indicates heuristic function quite tight dataset. effective
pruning started early 6th layer. Parkinsons, however, plain A*
successful pruning nodes. first 13 layers, heuristic function appeared
loose. A* expand nodes layers. heuristic function became
tighter latter layers enabled A* prune increasing percentage search
nodes. help pattern database heuristic, however, A*,SP helped prune many
53

fi1.60E+06
Expanded

1.40E+06

Unexpanded

ExpandedvsUnexpandedNodes

ExpandedvsUnexpandedNodes

Yuan & Malone

1.20E+06
1.00E+06
8.00E+05
6.00E+05
4.00E+05
2.00E+05
0.00E+00
0

2

4

6

8

10

12 14
Layer

16

18

20

1.60E+06
Expanded

1.40E+06
1.20E+06
1.00E+06
8.00E+05
6.00E+05
4.00E+05
2.00E+05
0.00E+00
0

22

(a) A* Mushroom

2

4

6

8

10 12 14
Layer

16

18

20

22

18

20

22

(b) A* Parkinsons
1.60E+06

1.60E+06
Expanded

Unexpanded

ExpandedvsUnexpandedNodes

ExpandedvsUnexpandedNodes

Unexpanded

1.40E+06
1.20E+06
1.00E+06
8.00E+05
6.00E+05
4.00E+05
2.00E+05
0.00E+00
0

2

4

6

8

10 12
Layer

14

16

18

20

22

(c) A*,SP Mushroom

Expanded

Unexpanded

1.40E+06
1.20E+06
1.00E+06
8.00E+05
6.00E+05
4.00E+05
2.00E+05
0.00E+00
0

2

4

6

8

10 12 14
Layer

16

(d) A*,SP Parkinsons

Figure 10: number expanded unexpanded nodes A* layer order
graph Mushroom Parkinsons using different heuristics.

search nodes Parkinsons; pruning became effective early 6th layer.
A*,SP also helped prune nodes Mushroom, although benefit clear
A* already quite effective dataset.
6.6 Factors Affecting Learning Difficulty
Several factors may affect difficulty dataset Bayesian network learning
algorithms, including number variables, number data points, number
optimal parent sets. analyzed correlation factors search
times algorithms. replaced occurrence time 7,200 order
make analysis possible (we caution though may results underestimation).
Figure 11 shows results. excluded results BB finished two
datasets. DP, A*, A*,SP, important factor determining efficiency
number variables, correlations search time numbers
variables greater 0.58. However, seems negative correlation
search time number data points. Intuitively, increasing number
data points make dataset difficult. explanation preexisting negative correlation number data points number variables
datasets tested; analysis shows correlation 0.61.
54

fiLearning Optimal Bayesian Networks

1

Variables

Data!Records

Optimal!Parent!Sets

0.8

Correlation

0.6
0.4
0.2
0
0.2

DP

GOBNILP

A*

A*,!SP

0.4
0.6

Figure 11: correlation search time algorithms several factors
may affect difficulty learning problem, including number
variables, number data points dataset, number optimal
parent sets.

Since search time strong positive correlation number variables,
seemingly negative correlation search time number data points
becomes less surprising.
comparison, efficiency GOBNILP affected number optimal
parent sets; correlation high close 0.8. Also, positive correlation
number data points efficiency. because, explained earlier,
data points often leads optimal parent sets. Finally, correlation
number variables almost zero, means difficulty dataset GOBNILP
determined number variables.
insights quite important, provide guideline choosing suitable
algorithm given characteristic dataset. many optimal parent sets
many variables, A* better algorithm; way around true, GOBNILP
better.
6.7 Effect Scoring Functions
analyses far based mainly MDL score. decomposable scoring
functions also used A* algorithm, correctness search strategies
heuristic functions affected scoring function. However, different scoring
functions may different properties. example, Theorem 1 property MDL
score. cannot use pruning technique scoring functions. Consequently,
number optimal parent sets, tightness heuristic, practical performance
various algorithms may affected.
verify hypothesis, also tested BDeu scoring function (Heckerman, 1998)
equivalent sample size set 1.0. Since scoring phase common
exact algorithms, focus experiment comparing number optimal parent
sets resulted scoring functions, search time A*,SP GOBNILP
55

fiYuan & Malone

Optimal PS, MDL

Optimal PS, Bdeu

Size

10000000
1000000
100000
10000
1000
100
10
1

(a)
10000
GOBNILP, MDL

GOBNILP, BDeu

A*, MDL

A*, BDeu

Search Time

1000

100

10

1

XX

XX

X

XX

XX

X

(b)
Figure 12: comparison (a) number optimal parent sets, (b) search time
A*,SP GOBNILP various datasets two scoring functions, MDL
BDeu.

datasets; Horse Flag included optimal parent sets
unavailable. Figure 12 shows results.
main observation number optimal parent sets differ MDL
BDeu. BDeu score tends allow larger parent sets MDL results
larger number optimal parent sets datasets. difference around
order magnitude datasets Imports Autos.
comparison search time shows A*,SP affected much GOBNILP. increase number optimal parent sets, efficiency finding
optimal parent set affected, A*,SP slowed slightly
datasets. significant change Mushroom dataset. took A*,SP 2
seconds solve dataset using MDL, 115 seconds using BDeu. comparison,
GOBNILP affected much more. able solve datasets Imports Autos effi56

fiLearning Optimal Bayesian Networks

ciently using MDL, failed solve within 3 hours using BDeu. remained
unable solve Letter, Image, Mushroom, Sensors within time limit.

7. Discussions Conclusions
paper presents shortest-path perspective problem learning optimal Bayesian
networks optimize given scoring function. uses implicit order graph represent
solution space learning problem shortest path start
goal nodes graph corresponds optimal Bayesian network. perspective
highlights importance two orthogonal directions research. One direction
develop search algorithms solving shortest path problem. main contribution
made line A* algorithm solving shortest path problem learning
optimal Bayesian network. Guided heuristic functions, A* algorithm focuses
searching promising parts solution space finding optimal Bayesian
network.
second equally important research direction development search heuristics.
introduced two admissible heuristics shortest path problem. first heuristic
estimates future cost completely relaxing acyclicity constraint Bayesian networks. shown admissible also consistent. second heuristic,
k-cycle conflict heuristic, developed based additive pattern database technique.
Unlike simple heuristic variable allowed choose optimal parents independently, new heuristic tightens estimation enforcing acyclicity constraint
within small groups variables. two specific approaches computing
new heuristic. One approach named dynamic k-cycle conflict heuristic computes costs
groups variables size k. search, dynamically partition
remaining variables exclusive patterns calculating heuristic value.
approach named static k-cycle conflict heuristic partitions variables several static
exclusive groups, computes separate pattern database group. sum
costs static pattern databases obtain admissible heuristic. heuristics
remain admissible consistent, although consistency dynamic k-cycle conflict
may sacrificed due greedy method used select patterns.
tested A* algorithm empowered different search heuristics set UCI
machine learning datasets. results show pattern database heuristics
contributed significant improvements efficiency scalability A* algorithm. results also show A* algorithm typically efficient dynamic
programming shares similar formulation. comparison GOBNILP, integer
programming algorithm, A* less sensitive number optimal parent sets, number
data points, scoring functions, sensitive number variables
datasets. advantages, believe methods represent promising approach
learning optimal Bayesian network structures.
Exact algorithms learning optimal Bayesian networks still limited relatively
small problems. scaling learning needed, e.g., incorporating domain
expert knowledge learning. also means approximation methods still useful
domains many variables. Nevertheless, exact algorithms valuable
serve basis evaluate different approximation methods
57

fiYuan & Malone

quality assurance. Also, promising research direction develop algorithms
best properties approximation exact algorithms, is,
find good solutions quickly and, given enough resources, converge optimal
solution (Malone & Yuan, 2013).

Acknowledgments
research supported NSF grants IIS-0953723, EPS-0903787, IIS-1219114
Academy Finland (Finnish Centre Excellence Computational Inference Research
COIN, 251170). Part research previously presented IJCAI-11 (Yuan,
Malone, & Wu, 2011) UAI-12 (Yuan & Malone, 2012).

Appendix A. Proofs
following proofs theorems paper.
A.1 Proof Theorem 5
Proof: Note optimal parent set X U subset U,
subset best score. Sorting unique parent scores makes sure
first found subset must satisfy requirements stated theorem.

A.2 Proof Theorem 6
Proof: Heuristic function h clearly admissible, allows remaining variable
choose optimal parents variables V. chosen parent set must
superset parent set variable optimal directed acyclic graph
consisting remaining variables. Due Theorem 4, heuristic results lower
bound cost.

A.3 Proof Theorem 7
Proof: successor node U, let \ U.
X
h(U) =
BestScore(X, V\{X})
XV\U



X

BestScore(X, V\{X})

XV\U,X6=Y

+BestScore(Y, U)
= h(S) + c(U, S).
inequality holds fewer variables used select optimal parents . Hence,
h consistent.

A.4 Proof Theorem 8
Proof: theorem proven noting avoiding cycles variables
U equivalent finding optimal ordering variables best joint score.
58

fiLearning Optimal Bayesian Networks

different paths V \ U goal node correspond different orderings
variables, among shortest path hence corresponds optimal ordering.
A.5 Proof Theorem 9
Proof: node U, assume remaining variables V \ U partitioned exclusive
sets V1 , ..., Vp . decomposability scoring function, h(U) =
p
P
c(Vi ). computing c(Vi ), allow directed cycles within Vi .
i=1

variables V \ Vi valid candidate parents, however. cost pattern, c(Vi ),
must optimal definition pattern databases. argument used
proof Theorem 6, h(U) cost cannot worse total cost V \ U, is,
cost optimal directed acyclic graph consisting variables (with U allowable
parents also). Otherwise, simply arrange variables patterns
order optimal directed acyclic graph get cost. Therefore, heuristic
still admissible.
Note previous argument relies optimality pattern costs,
patterns chosen. greedy strategy used dynamic pattern database
affects patterns selected. Therefore, theorem holds dynamic
static pattern databases.

A.6 Proof Theorem 10
Proof: Recall using static pattern databases node partitions V = Vi ,
heuristic value node U follows.
h(U) =

X

c((V \ U) Vi ),



(V \U) Vi pattern ith static pattern database. Then, successor
node U, let \ U. Without lost generality, let (V \ U) Vj . heuristic
value node
h(S) =

X

c((V \ U) Vi ) + c((V \ U) (Vj \ {Y })).

i6=j

Also, cost U
c(U, S) = BestScore(Y, U).
definition pattern database, know c((V\U)Vj ) best possible
joint score variables pattern U searched. Therefore,
c((V \ U) Vj ) c(V \ U) Vj \ {Y }) + BestScore(Y, (i6=j Vi ) (Vj \ (V \ U))
c((V \ U) (Vj \ {Y })) + BestScore(Y, U).
last inequality holds U (i6=j Vi ) (Vj \ (V \ U)). following
immediately follows.
h(U) h(S) + c(U, S).
59

fiYuan & Malone

Hence, static k-cycle conflict heuristic consistent.

A.7 Proof Theorem 11
Proof: heuristic values calculated dynamic pattern database considered shortest distances nodes abstract space. abstract space consists
set nodes, i.e., subsets V. However, additional arcs added
node nodes k additional variables.
Consider shortest path p two nodes U goal V original solution
space. path remains valid path, may longer shortest path U
V additional arcs.
Let g (U, V) shortest distance U V abstract space.
successor node U, must following.
g (U, V) g (U, S) + g (S, V).

(8)

Now, recall g (U, V) g (S, V) heuristic values original solution
space, g (U, S) equal arc cost c(U, S) original space. therefore
following.
h(U) c(U, S) + h(S).
(9)
Hence, dynamic k-cycle conflict heuristic consistent.



References
Acid, S., & de Campos, L. M. (2001). hybrid methodology learning belief networks:
BENEDICT. International Journal Approximate Reasoning, 27 (3), 235262.
Akaike, H. (1973). Information theory extension maximum likelihood principle.
Proceedings Second International Symposium Information Theory, pp.
267281.
Bache, K., & Lichman, M.
http://archive.ics.uci.edu/ml.

(2013).

UCI

machine

learning

repository.

Bouckaert, R. R. (1994). Properties Bayesian belief network learning algorithms.
Proceedings Tenth Conference Uncertainty Artificial Intelligence, pp.
102109, Seattle, WA. Morgan Kaufmann.
Bozdogan, H. (1987). Model selection Akaikes information criterion (AIC): general
theory analytical extensions. Psychometrika, 52, 345370.
Buntine, W. (1991). Theory refinement Bayesian networks. Proceedings seventh
conference (1991) Uncertainty artificial intelligence, pp. 5260, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Cheng, J., Greiner, R., Kelly, J., Bell, D., & Liu, W. (2002). Learning Bayesian networks
data: information-theory based approach. Artificial Intelligence, 137 (1-2),
4390.
60

fiLearning Optimal Bayesian Networks

Chickering, D. (1995). transformational characterization equivalent Bayesian network
structures. Proceedings 11th annual conference uncertainty artificial
intelligence (UAI-95), pp. 8798, San Francisco, CA. Morgan Kaufmann Publishers.
Chickering, D. M. (1996). Learning Bayesian networks NP-complete. Learning
Data: Artificial Intelligence Statistics V, pp. 121130. Springer-Verlag.
Chickering, D. M. (2002). Learning equivalence classes Bayesian-network structures.
Journal Machine Learning Research, 2, 445498.
Cooper, G. F., & Herskovits, E. (1992). Bayesian method induction probabilistic
networks data. Machine Learning, 9, 309347.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,
14, 318334.
Cussens, J. (2011). Bayesian network learning cutting planes. Proceedings
Twenty-Seventh Conference Annual Conference Uncertainty Artificial Intelligence (UAI-11), pp. 153160, Corvallis, Oregon. AUAI Press.
Daly, R., & Shen, Q. (2009). Learning Bayesian network equivalence classes ant colony
optimization. Journal Artificial Intelligence Research, 35, 391447.
Dash, D., & Cooper, G. (2004). Model averaging prediction discrete Bayesian
networks. Journal Machine Learning Research, 5, 11771203.
Dash, D. H., & Druzdzel, M. J. (1999). hybrid anytime algorithm construction
causal models sparse data. Proceedings Fifteenth Annual Conference
Uncertainty Artificial Intelligence (UAI99), pp. 142149, San Francisco, CA.
Morgan Kaufmann Publishers, Inc.
de Campos, C. P., & Ji, Q. (2011). Efficient learning Bayesian networks using constraints.
Journal Machine Learning Research, 12, 663689.
de Campos, C. P., & Ji, Q. (2010). Properties Bayesian Dirichlet scores learn Bayesian
network structures. Fox, M., & Poole, D. (Eds.), AAAI, pp. 431436. AAAI Press.
de Campos, L. M. (2006). scoring function learning Bayesian networks based
mutual information conditional independence tests. Journal Machine Learning
Research, 7, 21492187.
de Campos, L. M., Fernndez-Luna, J. M., Gmez, J. A., & Puerta, J. M. (2002). Ant colony
optimization learning Bayesian networks. International Journal Approximate
Reasoning, 31 (3), 291311.
de Campos, L. M., & Huete, J. F. (2000). new approach learning belief networks
using independence criteria. International Journal Approximate Reasoning, 24 (1),
11 37.
61

fiYuan & Malone

de Campos, L. M., & Puerta, J. M. (2001). Stochastic local algorithms learning belief
networks: Searching space orderings. Benferhat, S., & Besnard, P.
(Eds.), ECSQARU, Vol. 2143 Lecture Notes Computer Science, pp. 228239.
Springer.
Edelkamp, S., & Schrodl, S. (2012). Heuristic Search - Theory Applications. Morgan
Kaufmann.
Felner, A., Korf, R., & Hanan, S. (2004). Additive pattern database heuristics. Journal
Artificial Intelligence Research, 22, 279318.
Felzenszwalb, P. F., & McAllester, D. A. (2007). generalized A* architecture. Journal
Artificial Intelligence Research, 29, 153190.
Friedman, N., & Koller, D. (2003). Bayesian network structure: Bayesian
approach structure discovery Bayesian networks. Machine Learning, 50 (1-2),
95125.
Friedman, N., Nachman, I., & Peer, D. (1999). Learning Bayesian network structure
massive datasets: sparse candidate algorithm. Laskey, K. B., & Prade, H.
(Eds.), Proceedings Fifteenth Conference Conference Uncertainty Artificial
Intelligence (UAI-99), pp. 206215. Morgan Kaufmann.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide
Theory NP-Completeness. W. H. Freeman & Co., New York, NY, USA.
Glover, F. (1990). Tabu search: tutorial. Interfaces, 20 (4), 7494.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determination minimum cost paths. IEEE Trans. Systems Science Cybernetics, 4 (2),
100107.
Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks:
combination knowledge statistical data. Machine Learning, 20, 197243.
Heckerman, D. (1998). tutorial learning Bayesian networks. Holmes, D., & Jain,
L. (Eds.), Innovations Bayesian Networks, Vol. 156 Studies Computational
Intelligence, pp. 3382. Springer Berlin / Heidelberg.
Hemmecke, R., Lindner, S., & Studeny, M. (2012). Characteristic imsets learning
Bayesian network structure. International Journal Approximate Reasoning, 53 (9),
13361349.
Hsu, W. H., Guo, H., Perry, B. B., & Stilson, J. A. (2002). permutation genetic algorithm
variable ordering learning Bayesian networks data. Langdon, W. B.,
Cant-Paz, E., Mathias, K. E., Roy, R., Davis, D., Poli, R., Balakrishnan, K., Honavar,
V., Rudolph, G., Wegener, J., Bull, L., Potter, M. A., Schultz, A. C., Miller, J. F.,
Burke, E. K., & Jonoska, N. (Eds.), GECCO, pp. 383390. Morgan Kaufmann.
62

fiLearning Optimal Bayesian Networks

Jaakkola, T., Sontag, D., Globerson, A., & Meila, M. (2010). Learning Bayesian network
structure using LP relaxations. Proceedings 13th International Conference
Artificial Intelligence Statistics (AISTATS), pp. 358365, Chia Laguna Resort,
Sardinia, Italy.
Klein, D., & Manning, C. D. (2003). A* parsing: Fast exact Viterbi parse selection.
Proceedings Human Language Conference North American Association
Computational Linguistics (HLT-NAACL), pp. 119126.
Koivisto, M., & Sood, K. (2004). Exact Bayesian structure discovery Bayesian networks.
Journal Machine Learning Research, 5, 549573.
Kojima, K., Perrier, E., Imoto, S., & Miyano, S. (2010). Optimal search clustered
structural constraint learning Bayesian network structure. Journal Machine
Learning Research, 11, 285310.
Lam, W., & Bacchus, F. (1994). Learning Bayesian belief networks: approach based
MDL principle. Computational Intelligence, 10, 269293.
Larranaga, P., Kuijpers, C. M. H., Murga, R. H., & Yurramendi, Y. (1996). Learning
Bayesian network structures searching best ordering genetic algorithms. IEEE Transactions Systems, Man, Cybernetics, Part A, 26 (4), 487
493.
Malone, B., & Yuan, C. (2013). Evaluating anytime algorithms learning optimal Bayesian
networks. Proceedings 29th Conference Uncertainty Artificial Intelligence (UAI-13), pp. 381390, Seattle, Washington.
Malone, B., Yuan, C., Hansen, E., & Bridges, S. (2011a). Improving scalability optimal Bayesian network learning frontier breadth-first branch bound search.
Proceedings 27th Conference Uncertainty Artificial Intelligence (UAI-11),
pp. 479488, Barcelona, Catalonia, Spain.
Malone, B., Yuan, C., & Hansen, E. A. (2011b). Memory-efficient dynamic programming
learning optimal Bayesian networks. Proceedings 25th AAAI Conference
Artificial Intelligence (AAAI-11), pp. 10571062, San Francisco, CA.
Moore, A., & Lee, M. S. (1998). Cached sufficient statistics efficient machine learning
large datasets. Journal Artificial Intelligence Research, 8, 6791.
Moore, A., & Wong, W.-K. (2003). Optimal reinsertion: new search operator accelerated accurate Bayesian network structure learning. International
Conference Machine Learning, pp. 552559.
Myers, J. W., Laskey, K. B., & Levitt, T. S. (1999). Learning Bayesian networks
incomplete data stochastic search algorithms. Laskey, K. B., & Prade, H.
(Eds.), Proceedings Fifteenth Conference Conference Uncertainty Artificial
Intelligence (UAI-99), pp. 476485. Morgan Kaufmann.
63

fiYuan & Malone

Ordyniak, S., & Szeider, S. (2010). Algorithms complexity results exact Bayesian
structure learning. Gruwald, P., & Spirtes, P. (Eds.), Proceedings 26th
Conference Conference Uncertainty Artificial Intelligence (UAI-10), pp. 401
408. AUAI Press.
Ott, S., Imoto, S., & Miyano, S. (2004). Finding optimal models small gene networks.
Pacific Symposium Biocomputing, pp. 557567.
Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial optimization: algorithms
complexity. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Parviainen, P., & Koivisto, M. (2009). Exact structure discovery Bayesian networks
less space. Proceedings Twenty-Fifth Conference Uncertainty Artificial
Intelligence, Montreal, Quebec, Canada. AUAI Press.
Pearl, J. (1984). Heuristics: intelligent search strategies computer problem solving.
Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.
Pearl, J. (1988). Probabilistic reasoning intelligent systems: networks plausible inference. Morgan Kaufmann Publishers Inc.
Perrier, E., Imoto, S., & Miyano, S. (2008). Finding optimal Bayesian network given
super-structure. Journal Machine Learning Research, 9, 22512286.
Rissanen, J. (1978). Modeling shortest data description. Automatica, 14, 465471.
Silander, T., & Myllymaki, P. (2006). simple approach finding globally optimal Bayesian network structure. Proceedings 22nd Annual Conference
Uncertainty Artificial Intelligence (UAI-06), pp. 445452. AUAI Press.
Silander, T., Roos, T., Kontkanen, P., & Myllymaki, P. (2008). Factorized normalized
maximum likelihood criterion learning Bayesian network structures. Proceedings
4th European Workshop Probabilistic Graphical Models (PGM-08), pp. 257
272.
Singh, A., & Moore, A. W. (2005). Finding optimal Bayesian networks dynamic programming. Tech. rep. CMU-CALD-05-106, Carnegie Mellon University.
Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, prediction, search (second
edition). MIT Press.
Suzuki, J. (1996). Learning Bayesian belief networks based minimum description
length principle: efficient algorithm using B&B technique. International
Conference Machine Learning, pp. 462470.
Teyssier, M., & Koller, D. (2005). Ordering-based search: simple effective algorithm
learning Bayesian networks. Proceedings Twenty-First Annual Conference
Uncertainty Artificial Intelligence (UAI-05), pp. 584590. AUAI Press.
64

fiLearning Optimal Bayesian Networks

Tian, J. (2000). branch-and-bound algorithm MDL learning Bayesian networks.
UAI 00: Proceedings 16th Conference Uncertainty Artificial Intelligence,
pp. 580588, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Tsamardinos, I., Brown, L., & Aliferis, C. (2006). max-min hill-climbing Bayesian
network structure learning algorithm. Machine Learning, 65, 3178.
Xie, X., & Geng, Z. (2008). recursive method structural learning directed acyclic
graphs. Journal Machine Learning Research, 9, 459483.
Yuan, C., Lim, H., & Littman, M. L. (2011a). relevant explanation: Computational
complexity approximation methods. Annals Mathematics Artificial Intelligence, 61, 159183.
Yuan, C., Lim, H., & Lu, T.-C. (2011b). relevant explanation Bayesian networks.
Journal Artificial Intelligence Research (JAIR), 42, 309352.
Yuan, C., Liu, X., Lu, T.-C., & Lim, H. (2009). Relevant Explanation: Properties,
algorithms, evaluations. Proceedings 25th Conference Uncertainty
Artificial Intelligence (UAI-09), pp. 631638, Montreal, Canada.
Yuan, C., & Malone, B. (2012). improved admissible heuristic learning optimal
Bayesian networks. Proceedings 28th Conference Uncertainty Artificial
Intelligence (UAI-12), pp. 924933, Catalina Island, CA.
Yuan, C., Malone, B., & Wu, X. (2011). Learning optimal Bayesian networks using A*
search. Proceedings 22nd International Joint Conference Artificial Intelligence (IJCAI-11), pp. 21862191, Helsinki, Finland.

65

fi
